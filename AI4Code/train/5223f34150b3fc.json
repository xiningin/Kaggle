{"cell_type":{"fc892727":"code","51beb28d":"code","222425ba":"code","c98778bb":"code","a4a6a03f":"code","cc362db2":"code","6a96503f":"code","a6664cfb":"code","286165af":"code","1cb9dd26":"code","e5ef78d8":"code","bfd36e77":"code","f447b11e":"code","9ebfb1a9":"code","29ecbd7d":"code","7ea78218":"code","de301913":"code","44f1a180":"code","1955d30c":"code","666365ee":"code","c58eb38c":"code","4e1751ff":"code","3b8c51d3":"code","8bdd9b0b":"code","58c4465c":"code","1a1e7dec":"code","73dc1c99":"code","f13b02ae":"code","5390ee8e":"code","fce578ea":"code","2830c198":"code","a8228df7":"code","fc4978ce":"code","f5514f46":"code","d6bc86e6":"code","a54d3ff1":"code","6ae05f1b":"code","503691a4":"code","c90aeaed":"code","dd97c099":"code","5a464cb0":"code","f5b53237":"code","fe1d0f23":"code","10d974a3":"code","36c0555d":"code","d9a1dc18":"code","b488ad6b":"code","be2f5a3e":"code","5ae1fb06":"code","8c539a2b":"code","3eab56fc":"code","6cd1ee6b":"code","f8ef9397":"code","fa250e2a":"code","ab4dc433":"code","efb67fea":"code","6a705f5b":"code","07638641":"code","4348e8b8":"code","670be748":"code","d239d5a6":"code","7ba4df18":"code","456b0394":"code","ea973fb8":"markdown","91726a9d":"markdown","8e900c4d":"markdown","765af258":"markdown","bbf227e5":"markdown","9a9b5a0c":"markdown","7280e325":"markdown","da5dfef9":"markdown"},"source":{"fc892727":"\nfrom glob import glob\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom skimage.io import imread\nfrom skimage.color import rgb2grey\nfrom sklearn.feature_extraction import image\nfrom sklearn.cluster import KMeans\nfrom skimage.filters import rank, threshold_otsu\nfrom skimage.morphology import closing, square, disk\nfrom skimage import exposure as hist, data, img_as_float\nfrom skimage.segmentation import chan_vese\nfrom skimage.feature import canny\nfrom skimage.color import rgb2gray\nfrom scipy import ndimage as ndi ","51beb28d":"mal_images = glob('..\/input\/skin-cancer-malignant-vs-benign\/data\/train\/malignant\/*')\nben_images = glob('..\/input\/skin-cancer-malignant-vs-benign\/data\/train\/benign\/*')","222425ba":"mal_images_1 = glob('..\/input\/skin-cancer-malignant-vs-benign\/data\/train\/malignant\/*')[:5]\nben_images_1 = glob('..\/input\/skin-cancer-malignant-vs-benign\/data\/train\/benign\/*')[:5]","c98778bb":"len(mal_images)\n","a4a6a03f":"len(ben_images)","cc362db2":"#https:\/\/www.sciencedirect.com\/topics\/engineering\/binary-image\ndef binary(image):\n    return image > threshold_otsu(image)\n\n#https:\/\/campus.datacamp.com\/courses\/introduction-to-data-visualization-in-python\/analyzing-time-series-and-images?ex=13\ndef equalize(image):\n    return hist.equalize_hist(image)\n\n#https:\/\/homepages.inf.ed.ac.uk\/rbf\/HIPR2\/median.htm\ndef mean_filter(image, raio_disk):\n    return rank.mean_percentile(image, selem = disk(raio_disk))\n\ndef preenche_bords(image):\n    return ndi.binary_fill_holes(image)\n\n#https:\/\/www.unioviedo.es\/compnum\/labs\/PYTHON\/intro_image.html\n\ndef load_images(paths):\n    tmp = []\n    for path in paths:\n        tmp.append(imread(path))\n    return tmp\n    \ndef plot_any(arr, title = ''):\n    plt.figure(figsize = (15, 25))\n    for i in range(len(arr)):\n        plt.subplot(1,len(arr),i + 1)\n        plt.title(title)\n        plt.imshow(arr[i]);\n\n        \ndef plot_camadas(img):\n    plt.figure(figsize = (15, 25))\n    for i in range(3):\n        plt.subplot(1, 3, i + 1)\n        plt.imshow(img[:,:,i], cmap = 'gray');\n        \ndef d2Kmeans(img, k):\n    return KMeans(n_jobs=-1, \n                  random_state=1, \n                  n_clusters = k, \n                  init='k-means++'\n    ).fit(img.reshape((-1,1))).labels_.reshape(img.shape)\n\ndef merge_segmented_mask_ROI(uri_img, img_kluster):\n    new_img = uri_img.copy()\n    for ch in range(3):\n        new_img[:,:, ch] *= img_kluster\n    return new_img\n\n\ndef elbow(img, k):\n    hist = []\n    for kclusters in  range(1, k):\n        Km = KMeans(n_jobs=-1, random_state=1, n_clusters = kclusters, init='k-means++').fit(img.reshape((-1,1)))  \n        hist.append(Km.inertia_)\n        \n    plt.figure(figsize = (15, 8))\n    plt.grid()\n    plt.plot(range(1, k), hist, 'o-')\n    plt.ylabel('Soma das dist\u00e2ncias quadradas')\n    plt.xlabel('k clusters')\n    plt.title('Elbow')\n    plt.show();\n    \n    ","6a96503f":"mal = load_images(mal_images)\nben = load_images(ben_images)\n\nmal_1 = load_images(mal_images_1)\nben_1 = load_images(ben_images_1)","a6664cfb":"plot_any(mal_1)\nplot_any(ben_1)\n","286165af":"img_selected = mal[1]","1cb9dd26":"elbow(img_selected, 6)","e5ef78d8":"k_klusters = 2","bfd36e77":"result_gray = d2Kmeans(rgb2grey(img_selected), k_klusters)\nresult_img = d2Kmeans(img_selected, k_klusters)","f447b11e":"klusters_gray = [result_gray == i for i in range(k_klusters)]\nplot_any(klusters_gray)","9ebfb1a9":"def select_cluster_index(clusters):\n    minx = clusters[0].mean()\n    index = 0\n    for i in clusters:\n        if i.mean() < minx:\n            minx = i.mean()\n            index += 1\n    return index","29ecbd7d":"index_kluster = select_cluster_index(klusters_gray)\nprint(index_kluster)\nselecionado = klusters_gray[index_kluster]","7ea78218":" for ch in range(3):\n    img_k = []\n    for K in range(k_klusters):\n         img_k.append(result_img[:, :, ch] == K)\n    plot_any(img_k)","de301913":"clusters = [(result_img[:,:,1] == K) for K in range(k_klusters)]","44f1a180":"clusters","1955d30c":"new_img = merge_segmented_mask_ROI(img_selected, selecionado)","666365ee":"plot_any([new_img])","c58eb38c":"image_mean_filter = mean_filter(selecionado, 20)\ntest_binary = binary(image_mean_filter)","4e1751ff":"plot_any([selecionado, image_mean_filter, test_binary])","3b8c51d3":"final_result = merge_segmented_mask_ROI(img_selected ,test_binary)","8bdd9b0b":"final_result.shape","58c4465c":"plot_any([test_binary, new_img, final_result])","1a1e7dec":" max_mean = 0\nimg_gray = rgb2gray(final_result)\n img_bin  = binary(img_gray)\nx, y = img_bin.shape\n\n limits_before = []\nfor i in range(x):\n    for j in range(y):\n        if  img_bin[i, j]:\n            limits_before.append(j)\n            \nstop_before = ( sum(limits_before) \/\/ len(limits_before) ) \/\/ 2\nimg_copy = img_bin.copy()\nfor i in range(x):\n    for j in range(stop_before):\n        img_copy[i, j] = 0\n        limits_after = []\nfor i in range(x):\n     for j in range(y - 1, 0, -1):\n        if  img_copy[i, j]:\n            limits_after.append(j)\n            \nstop_after = sum(limits_after) \/\/ len(limits_after) + min(limits_after)\nfor i in range(x):\n    for j in range(stop_after, y):\n        img_copy[i, j] = 0\n\nmean_result = mean_filter(img_copy, 15)\nmean_result = binary(mean_result)\nfinal_result = merge_segmented_mask_ROI(img_selected , mean_result)\n\n\n plot_any([mean_result, final_result])","73dc1c99":"import matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport cv2\nimg = cv2.imread(mal_images[1]) \nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nr, g, b = cv2.split(img)\nr = r.flatten()\ng = g.flatten()\nb = b.flatten()\n#plotting \nfig = plt.figure()\nax = Axes3D(fig)\nax.scatter(r, g, b)\nplt.show()","f13b02ae":"data_mal = list() \nfor img in mal :\n    img = merge_segmented_mask_ROI(img ,test_binary)\n    data_mal.append(img)","5390ee8e":"data_ben = list() \nfor img in ben :\n    img = merge_segmented_mask_ROI(img ,test_binary)\n    data_ben.append(img)","fce578ea":"len(data_ben)\nlen(data_mal)","2830c198":"import cv2\nimport os\nimport io\nimport skimage\n# create a directory in which to store cropped images\nout_dir = \"\/kaggle\/working\/skin_seg\"\nif not os.path.exists(out_dir):\n    os.makedirs(out_dir)\n\n# save each cropped image by its index number\nfor k,im in enumerate(data_ben):\n    skimage.io.imsave( 'ben' + str(k) + \".jpg\", im)       ","a8228df7":"# create a directory in which to store cropped images\nout_dir = \"\u202a\/kaggle\/working\/skin_seg\"\nif not os.path.exists(out_dir):\n    os.makedirs(out_dir)\n\n# save each cropped image by its index number\nfor c,image in enumerate(data_mal):\n    skimage.io.imsave( 'mal' + str(c) + \".jpg\", image)","fc4978ce":"plt.imshow(data_ben[1],cmap = 'hot')","f5514f46":"\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn import svm, metrics, datasets\nfrom sklearn.utils import Bunch\nfrom sklearn.model_selection import GridSearchCV, train_test_split\n\nfrom skimage.io import imread\nfrom skimage.transform import resize","d6bc86e6":"def load_image_files(container_path, dimension=(28, 28)):\n    \"\"\"\n    Load image files with categories as subfolder names \n    which performs like scikit-learn sample dataset\n    \n    Parameters\n    ----------\n    container_path : string or unicode\n        Path to the main folder holding one subfolder per category\n    dimension : tuple\n        size to which image are adjusted to\n        \n    Returns\n    -------\n    Bunch\n    \"\"\"\n    image_dir = Path(container_path)\n    folders = [directory for directory in image_dir.iterdir() if directory.is_dir()]\n    categories = [fo.name for fo in folders]\n\n    descr = \"A image classification dataset\"\n    images = []\n    flat_data = []\n    target = []\n    for i, direc in enumerate(folders):\n        for file in direc.iterdir():\n            img = skimage.io.imread(file)\n            img_resized = resize(img, dimension, anti_aliasing=True, mode='reflect')\n            flat_data.append(img_resized.flatten()) \n            images.append(img_resized)\n            target.append(i)\n    flat_data = np.array(flat_data)\n    target = np.array(target)\n    images = np.array(images)\n    images.reshape((-1, 1, 28, 28))\n\n    return Bunch(data=flat_data,\n                 target=target,\n                 target_names=categories,\n                 images=images,\n                 DESCR=descr)","a54d3ff1":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.utils import to_categorical\nfrom keras.preprocessing import image\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import to_categorical\nfrom tqdm import tqdm\nimport numpy\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D\nfrom keras.constraints import maxnorm\nfrom keras.utils import np_utils\nimport sklearn","6ae05f1b":"image_dataset = load_image_files(\"..\/input\/segmented-images-of-the-skin-cancer-dataset\/\")","503691a4":"\nX_train, X_test, y_train, y_test = train_test_split(\n    image_dataset.data, image_dataset.target, test_size=0.3,random_state=109)","c90aeaed":"from sklearn.utils import shuffle\n\nX_train, y_train = shuffle(X_train, y_train, random_state=42)\n","dd97c099":"from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier()\nprint(clf)","5a464cb0":"RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n           max_depth=None, max_features='auto', max_leaf_nodes=None,\n           min_impurity_split=1e-07, min_samples_leaf=1,\n           min_samples_split=2, min_weight_fraction_leaf=0.0,\n           n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n           verbose=0, warm_start=False)","f5b53237":"from sklearn.model_selection import train_test_split\n\nclf.fit(X_train, y_train)","fe1d0f23":"from sklearn.metrics import accuracy_score\npreds = clf.predict(X_test)\nprint(\"Accuracy:\", accuracy_score(y_test,preds))","10d974a3":"print(X_train.shape)\nprint(X_test.shape )\nprint(y_train.shape )\nprint(y_test.shape) ","36c0555d":"x_train, x_test, y_train_v, y_test_v = train_test_split(X_train,y_train, test_size = 0.3, random_state = 2)","d9a1dc18":"rf = RandomForestClassifier()\n\nrf.fit(x_train,y_train_v)\n# Predictions on training and validation\ny_pred_train = rf.predict(x_train)\n    # predictions for test\ny_pred_test = rf.predict(x_test)\n    # training metrics\nprint(\"Training metrics:\")\nprint(sklearn.metrics.classification_report(y_true= y_train_v, y_pred= y_pred_train))\n    \n    # test data metrics\nprint(\"Test data metrics:\")\nprint(sklearn.metrics.classification_report(y_true= y_test_v, y_pred= y_pred_test))","b488ad6b":"print(rf.classes_)\n","be2f5a3e":"from PIL import Image\nimport time\nfrom torch.autograd import Variable\nimport json\nimport copy\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport PIL\nfrom PIL import Image\nfrom collections import OrderedDict\nimport torch\nfrom torch import nn, optim\nfrom torch.optim import lr_scheduler\nfrom torch.autograd import Variable\nimport torchvision\nfrom torchvision import datasets, models, transforms\nfrom torch.utils.data.sampler import SubsetRandomSampler\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom skimage.transform import resize","5ae1fb06":"img_path=\"..\/input\/segmented-images-of-the-skin-cancer-dataset\/benign_segmented\/ben1.jpg\"\nimg_ = cv2.imread(img_path, cv2.IMREAD_COLOR)\nimg_ = cv2.resize(img_, (28, 28))\nimg_ = cv2.cvtColor(img_, cv2.COLOR_RGB2BGR)\nplt.imshow(img_)\n\n","8c539a2b":"!pip install mahotas\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport numpy as np\nimport mahotas\nimport cv2\nimport os\nimport h5py\nimport glob\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier","3eab56fc":"# make a fix file size\nfixed_size  = tuple((1000,1000))\n\n#train path \ntrain_path = \"..\/input\/segmented-images-of-the-skin-cancer-dataset\/\"\n\n# no of trees for Random Forests\nnum_tree = 100\n\n# bins for histograms \nbins = 8\n\n# train_test_split size\ntest_size = 0.10\n\n# seed for reproducing same result \nseed = 9","6cd1ee6b":"def fd_hu_moments(image):\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    feature = cv2.HuMoments(cv2.moments(image)).flatten()\n    return feature","f8ef9397":"\n# feature-descriptor -2 Haralick Texture \n\ndef fd_haralick(image):\n    # conver the image to grayscale\n    gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n    # Ccompute the haralick texture fetature ve tor \n    haralic = mahotas.features.haralick(gray).mean(axis=0)\n    return haralic","fa250e2a":"# feature-description -3 Color Histogram\n\ndef fd_histogram(image, mask=None):\n    # conver the image to HSV colors-space\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n    #COPUTE THE COLOR HISTPGRAM\n    hist  = cv2.calcHist([image],[0,1,2],None,[bins,bins,bins], [0, 256, 0, 256, 0, 256])\n    # normalize the histogram\n    cv2.normalize(hist,hist)\n    # return the histog....\n    return hist.flatten()","ab4dc433":"# get the training data labels \ntrain_labels = os.listdir(train_path)\n\n# sort the training labesl \ntrain_labels.sort()\nprint(train_labels)\n\n# empty list to hold feature vectors and labels \nglobal_features = []\nlabels = []\n\ni, j = 0, 0 \nk = 0\n\n# num of images per class \nimages_per_class = 80","efb67fea":"# ittirate the folder to get the image label name\n\n%time\n# lop over the training data sub folder \n\nfor training_name in train_labels:\n    # join the training data path and each species training folder\n    dir = os.path.join(train_path, training_name)\n\n    # get the current training label\n    current_label = training_name\n\n    k = 1\n    # loop over the images in each sub-folder\n        \n    for file in os.listdir(dir):\n\n        file = dir + \"\/\" + os.fsdecode(file)\n       \n        # read the image and resize it to a fixed-size\n        image = cv2.imread(file) \n        \n        if image is not None:\n            image = cv2.resize(image,fixed_size)\n            fv_hu_moments = fd_hu_moments(image)\n            fv_haralick   = fd_haralick(image)\n            fv_histogram  = fd_histogram(image)\n        #else:\n            #print(\"image not loaded\")\n                \n        #image = cv2.imread(file)        \n        #image = cv2.resize(image,fixed_size)\n\n        # Concatenate global features\n        global_feature = np.hstack([fv_histogram, fv_haralick, fv_hu_moments])\n\n        # update the list of labels and feature vectors\n        labels.append(current_label)\n        global_features.append(global_feature)\n\n        i += 1\n        k += 1\n    print(\"[STATUS] processed folder: {}\".format(current_label))\n    j += 1\n\nprint(\"[STATUS] completed Global Feature Extraction...\")\n","6a705f5b":"%time\n# get the overall feature vector size\nprint(\"[STATUS] feature vector size {}\".format(np.array(global_features).shape))\n\n# get the overall training label size\nprint(\"[STATUS] training Labels {}\".format(np.array(labels).shape))\n\n# encode the target labels\ntargetNames = np.unique(labels)\nle = LabelEncoder()\ntarget = le.fit_transform(labels)\nprint(\"[STATUS] training labels encoded...{}\")\n# normalize the feature vector in the range (0-1)\nscaler = MinMaxScaler(feature_range=(0, 1))\nrescaled_features = scaler.fit_transform(global_features)\nprint(\"[STATUS] feature vector normalized...\")\n\nprint(\"[STATUS] target labels: {}\".format(target))\nprint(\"[STATUS] target labels shape: {}\".format(target.shape))\n","07638641":"\nglobal_features = np.array(rescaled_features)\nglobal_labels = np.array(target)","4348e8b8":"# split the training and testing data\n(trainDataGlobal, testDataGlobal, trainLabelsGlobal, testLabelsGlobal) = train_test_split(np.array(global_features),\n                                                                                          np.array(global_labels),\n                                                                                          test_size=test_size,\n                                                                                          random_state=seed)","670be748":"import sklearn\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n# create the model - Random Forests\nclf  = RandomForestClassifier(n_estimators=100)\n\n# fit the training data to the model\nclf.fit(trainDataGlobal, trainLabelsGlobal)\n\n#print(clf.fit(trainDataGlobal, trainLabelsGlobal))\n\nclf_pred = clf.predict(trainDataGlobal)\n#clf_pred = clf.predict(global_feature.reshape(1,-1))[0]\nprint(classification_report(trainLabelsGlobal,clf_pred))\ny_pred_test = clf.predict(testDataGlobal)\n\nprint(classification_report(testLabelsGlobal,y_pred_test))\n#print(confusion_matrix(trainLabelsGlobal,clf_pred))\n\n#print(clf.predict(trainDataGlobal))\n\n#print(clf.predict(global_feature.reshape(1,-1))[0])","d239d5a6":"# path to test data\ntest_path = \"..\/input\/segmented-images-of-the-skin-cancer-dataset\/benign_segmented\/ben1.jpg\"\n\n\n    #print(file)\n    \n    # read the image\nimage = cv2.imread(test_path)\n\n    # resize the image\nimage = cv2.resize(image, fixed_size)\n\n    # Global Feature extraction\nfv_hu_moments = fd_hu_moments(image)\nfv_haralick   = fd_haralick(image)\nfv_histogram  = fd_histogram(image)\n\n    # Concatenate global features\n\nglobal_feature = np.hstack([fv_histogram, fv_haralick, fv_hu_moments])\n\n    # predict label of test image\nprediction = clf.predict(global_feature.reshape(1,-1))[0]\n\n    # show predicted label on image\ncv2.putText(image, train_labels[prediction], (20,30), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0,255,255), 3)\n\n    # display the output image\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n","7ba4df18":"data.to_csv(r'df_name.csv')","456b0394":"from IPython.display import FileLink\nFileLink(r'df_name.csv')","ea973fb8":"Image segmentation ","91726a9d":"Image segmentation is the process of partitioning an image into multiple different regions (or segments). The goal is to change the representation of the image into an easier and more meaningful image.\n\nIt is an important step in image processing, as real world images doesn't always contain only one object that we wanna classify. For instance, for self driving cars, the image would contain the road, cars, pedestrians, etc. So we may need to use segmentation here to separate objects and analyze each object individually (i.e image classification) to check what it is.\n\nK-Means clustering is unsupervised machine learning algorithm that aims to partition N observations into K clusters in which each observation belongs to the cluster with the nearest mean. A cluster refers to a collection of data points aggregated together because of certain similarities. For image segmentation, clusters here are different image colors.\n\n\nImage segmentation is an essential topic in an image processing framework. It is the process to classify an image into different groups. There are many different methods, and k-means is one of the most popular methods.","8e900c4d":"Image segmentation is the classification of an image into different groups. Many researches have been done in the area of image segmentation using clustering. There are different methods and one of the most popular methods is k-means clustering algorithm. K -means clustering algorithm is an unsupervised algorithm and it is used to segment the interest area from the background. But before applying K -means algorithm, first partial stretching enhancement is applied to the image to improve the quality of the image. Subtractive clustering method is data clustering method where it generates the centroid based on the potential value of the data points. So subtractive cluster is used to generate the initial centers and these centers are used in k-means algorithm for the segmentation of image. Then finally medial filter is applied to the segmented image to remove any unwanted region from the image...","765af258":"The glob module finds all the pathnames matching a specified pattern according to the rules used by the Unix shell, although results are returned in arbitrary order. No tilde expansion is done, but *, ?, and character ranges expressed with [] will be correctly matched. This is done by using the os.scandir() and fnmatch.fnmatch() functions in concert, and not by actually invoking a subshell. Note that unlike fnmatch.fnmatch(), glob treats filenames beginning with a dot (.) as special cases. (For tilde and shell variable expansion, use os.path.expanduser() and os.path.expandvars().)\n\nFor a literal match, wrap the meta-characters in brackets. For example, '[?]' matches the character '?'.\n\nlink : https:\/\/docs.python.org\/3\/library\/glob.html","bbf227e5":"Starting the classification with segemented images","9a9b5a0c":"2 method ","7280e325":"<font size=\"+3\" color=Red><b> <center><u> Image Segmentation and Classification <\/u><\/center><\/b><\/font>","da5dfef9":"![image.png](attachment:image.png)"}}