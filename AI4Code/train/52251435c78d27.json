{"cell_type":{"42242172":"code","74ca4689":"code","aee0c431":"code","e7d7f76f":"code","ee7c2d67":"code","186ad769":"code","34ac9717":"code","4b221979":"code","9f7ec4a3":"code","8cba8757":"code","abc0467a":"code","4fa0dc99":"code","8ddcdd2e":"code","1dd1ccd0":"code","d19fd704":"code","51158d3f":"code","0f34f858":"code","2843e8c2":"code","17bfe974":"code","9b1ea66a":"code","7f5cc886":"code","0d487759":"code","4763c0bc":"code","7811d80c":"code","a75299cc":"code","3487f953":"code","b77c6295":"code","1eaa1be8":"code","30398e0d":"code","d44a418b":"code","c3e86367":"code","ddc4a1e1":"code","8cfbf73b":"code","b206eed3":"code","9ac92807":"code","7b1ca679":"code","b15f896f":"code","6c558f49":"code","48097ed8":"code","01d530a2":"code","1ba62ec1":"code","d74090dc":"code","f7cb76b7":"code","46165074":"code","02c79013":"code","88c326e2":"code","69f7560e":"code","2dd6b4b8":"code","317a4ece":"code","c420657a":"code","a757ec10":"code","7916d0dd":"code","1c41fb08":"code","2a4c5a1f":"code","a5b59cdd":"code","1144f0d5":"code","ae038036":"code","94fd07dc":"code","4e22b197":"code","4c878123":"code","32786f49":"code","b4ab7725":"code","b3e82348":"code","0b763988":"code","7035fa81":"code","4a4ba081":"code","ee594a83":"code","80a9af73":"markdown","f275e51e":"markdown","1dde7128":"markdown","133a51f3":"markdown","cc33048d":"markdown","d3e1a80f":"markdown","11626205":"markdown","7fdde0ba":"markdown","22ec4e28":"markdown","75010932":"markdown","f9466ef9":"markdown","3f186db9":"markdown","ec63a65b":"markdown","595ffecf":"markdown","e0ff5441":"markdown","9b71a7dd":"markdown","4f77a3d3":"markdown","a10ea56b":"markdown","9fa1c2de":"markdown","4f388eff":"markdown","a2300bfe":"markdown","5206f275":"markdown","73e4a34d":"markdown","766b8cea":"markdown","81b6a258":"markdown","6baf5372":"markdown","63efc572":"markdown","c2ccc607":"markdown","0767dc0a":"markdown","a01a05eb":"markdown","420848eb":"markdown","cdb992b6":"markdown","0b14d41c":"markdown","fcd8cf9e":"markdown","f9572d48":"markdown","f378a102":"markdown","9b3e68ea":"markdown","f0a6df4c":"markdown","4a0ae5a0":"markdown","ecc0fd86":"markdown","cc1ba120":"markdown"},"source":{"42242172":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","74ca4689":"import pandas as pd\n#pick microsoft share price \n# load the data\ndata_MS = pd.read_csv('..\/input\/microsoft-share-price\/MSFT_data.csv')\ndata_MS.head()\n","aee0c431":"data_MS.isnull().sum() #check if we have zero values ","e7d7f76f":"data_MS.dtypes #check the types of data","ee7c2d67":"import pandas_profiling as pp\npp.ProfileReport(data_MS)   ","186ad769":"data_MS['Date'] = pd.to_datetime(data_MS['Date'], format = \"%Y\/%m\/%d\") #convert argument to datetime\ndata_MS","34ac9717":"data_MS.drop('Name',1,inplace =True)","4b221979":"#### print(data_MS.loc[data_MS['Date'] =='2016-06-13']) # find index 0\n#print(data_MS.loc[data_MS['Date'] =='2017-06-13']) # find index 252 \n#504 #755 # 1008\n#print(data_MS.loc[data_MS['Date'] =='2020-06-12']) # 1008\n\n#specify rows and columns by their integer index\nclose1 = data_MS.iloc[252]['Close'] #take the first closing price 2016\/06\/13\n#print(close1)\nclose2 = data_MS.iloc[504]['Close']  #take the 2020 closing price at the same day and month 06\/13\n#print(close2)\n\nYears = 1\nCARG = (((close2\/close1)**(1\/Years))-1)*100\nprint(f'Annual Return for {Years} years:',CARG)","9f7ec4a3":"import matplotlib.pyplot as plt\n#take closing price on 2016-06-13 and 2017-06-13 and subtract them to find annual return \n#The formula for percentage return begins by dividing the current price by the prior price (can be yearly daily or monthly).\n#The number 1 is then subtracted from \n#this result before multiplying the resulting figure by 100 to convert it from decimal to percentage format.\n\nclose1 = data_MS.iloc[0]['Close']\nclose2 = data_MS.iloc[252]['Close']\n\n#create a for loop to iterate\n#ignore 2020 until 2021 returns for now, we havent reached 2021\/06\/13 yet to find the % return\nlist_percent_year =[]\nfor i in range(0,1260,252): #iterate by jumping over 252 values 2016 -> 2017 -> 2018 etc\n    \n    a = data_MS.iloc[i]['Close']   #pick the first closing value \n    if i == 1008: #when we reach 2020, stop working\n        break\n    b = data_MS.iloc[i+252]['Close'] #pick the next years closing value\n    \n    \n    return1 = (((b-a)\/a))*100\n    list_percent_year.append(return1)\n    print(return1)\n#list_percent_year #these numbers are wierd","8cba8757":"#plot the yearly return \ndata_MS['year'] = pd.DatetimeIndex(data_MS['Date']).year  #transform the year part of the datetime into an index\ndf_year = data_MS['year'].unique()\n\ndf_year = list(df_year)\n\nprint(df_year)\ndf_year.pop(0)\ndf_year.pop(-1)\ndf_year\nplt.plot(df_year,list_percent_year)","abc0467a":"#CREDIT: https:\/\/plotly.com\/python\/ohlc-charts\/\nimport plotly.graph_objects as go\nfig = go.Figure(data=go.Ohlc(x=data_MS['Date'],\n                    open=data_MS['Open'],\n                    high= data_MS['High'],\n                    low=data_MS['Low'],\n                    close=data_MS['Close']))\nfig.show() ","4fa0dc99":"import numpy as np\nfrom scipy import stats # used for used mathematical and numerical analysis\nimport matplotlib.pyplot as plt\nimport seaborn as sns   #used for statistical graphs\n\n\nplt.figure(figsize=(15, 7))\nplt.plot(data_MS['Date'],data_MS['Adj Close'],color='red')\nplt.legend(['Adj. Close'])\nplt.xlabel('Date')\nplt.ylabel('Share price')","8ddcdd2e":"#This is the SMA (Simple Moving Average)\n#function provides the feature of rolling window calculations\n#use the mean function to find the mean over each window\n#rolling operation takes the cell and k values before it, where k in the windows parameter\n\n#windows = 5 means 5 day moving average.\n#min_periods will not allow NaN values but intsead will apply the mean rule on as many previous numbers as possible\n\n#The bigger the windows value, the more it underfits\n#The smaller the windows value, the more it overfits\ndata_MS['SMA'] = data_MS['Adj Close'].rolling(window=5, min_periods= 1).mean()\ndata_MS['SMA 50'] = data_MS['Adj Close'].rolling(window=50, min_periods= 1).mean()\ndata_MS","1dd1ccd0":"from matplotlib.pyplot import figure\n\n\nplt.figure(figsize=(15, 7))\nplt.plot(data_MS['Date'][1148:1257],data_MS['Adj Close'][1148:1257],color='red',linewidth=3)\nplt.plot(data_MS['Date'][1148:1257], data_MS['SMA'][1148:1257],linewidth=3)\nplt.plot(data_MS['Date'][1148:1257], data_MS['SMA 50'][1148:1257],linewidth=3)\n\n#Long term 50, 200 day windows value\n\nplt.legend(['Adj. Close','SMA', 'SMA 50'])\nplt.xlabel('Date')\nplt.ylabel('Share price in $')\n","d19fd704":"data_MS['SMA 50'] = data_MS['Adj Close'].rolling(window=50, min_periods= 1).mean()\ndata_MS['SMA 200'] = data_MS['Adj Close'].rolling(window=200, min_periods= 1).mean()\n\nplt.figure(figsize=(15, 7))\n\nplt.plot(data_MS['Date'][1008:1257],data_MS['Adj Close'][1008:1257],color='red',linewidth=3)\nplt.plot(data_MS['Date'][1008:1257], data_MS['SMA'][1008:1257],linewidth=3)\nplt.plot(data_MS['Date'][1008:1257], data_MS['SMA 200'][1008:1257],linewidth=3)\n\nplt.legend(['Adj. Close','SMA','SMA 200'])\nplt.xlabel('Date')\nplt.ylabel('Share price in $')","51158d3f":"data_MS['EMA_0.1'] = data_MS['Adj Close'].ewm(alpha = 0.1,adjust=False).mean()\ndata_MS['EMA_0.3'] = data_MS['Adj Close'].ewm(alpha = 0.3,adjust=False).mean()\n#data_MS = data_MS.drop(columns=['EMA','CMA'])\n#The closer alpha is to 1, the more it overfits, it responds to changes more quickly\n#The smaller alpha is, the higer degree of smoothing\ndata_MS","0f34f858":"plt.figure(figsize=(15, 7))\n\nplt.plot(data_MS['Date'][1148:1257],data_MS['Adj Close'][1148:1257],color='red',linewidth = 3)\nplt.plot(data_MS['Date'][1148:1257], data_MS['EMA_0.1'][1148:1257], linewidth = 2)\nplt.plot(data_MS['Date'][1148:1257],data_MS['EMA_0.3'][1148:1257],color = 'green',linewidth = 2)\n\nplt.legend(['Adj. Close','Alpha =  0.1', 'Alpha = 0.3'])\nplt.xlabel('Date')\nplt.ylabel('Share price')","2843e8c2":"plt.figure(figsize=(15, 7))\n\nplt.plot(data_MS['Date'],data_MS['Adj Close'],color='red')\nplt.plot(data_MS['Date'], data_MS['EMA_0.1'])\nplt.legend(['Adj. Close','EMA'])\nplt.xlabel('Date')\nplt.ylabel('Share price')","17bfe974":"plt.figure(figsize=(15, 7))\n\n\nplt.plot(data_MS['Date'][1148:1257],data_MS['Adj Close'][1148:1257],color='red',linewidth = 3)\nplt.plot(data_MS['Date'][1148:1257], data_MS['EMA_0.1'][1148:1257], linewidth = 3)\nplt.plot(data_MS['Date'][1148:1257], data_MS['SMA 50'][1148:1257],linewidth=3)\n\nplt.legend(['Adj. Close','Alpha =  0.1', 'SMA 50 day'])\nplt.xlabel('Date')\nplt.ylabel('Share price')","9b1ea66a":"# Try to make an exponential moving average over that first volume plot\nimport matplotlib.pyplot as plt\nimport numpy as np\n#data_MS['EMA_Volume'] = data_MS['Volume'].ewm(span=40,adjust=False).mean()\ndata_MS['VMA'] = data_MS['Volume'].rolling(window=50, min_periods= 1).mean()\ndata_MS","7f5cc886":"\nplt.figure(figsize=(15, 7))\nplt.plot(data_MS['Date'],data_MS['Volume'],linewidth = 3)\nplt.plot(data_MS['Date'],data_MS['VMA'],color='red',linewidth = 2)\nplt.xlabel('Dates')\nplt.ylabel('Volume traded')\nplt.tight_layout()","0d487759":"#sns.displot(data=data_MS, x=\"Volume\", col=\"year\", kde=True)\n#which histogram has the highest mean?\ndata_MS['VMA'] = data_MS['Volume'].rolling(window=5, min_periods= 1).mean()\nplt.figure(figsize=(13, 6))\nplt.plot(data_MS['Date'][1148:1257],data_MS['Volume'][1148:1257],linewidth = 3)\nplt.plot(data_MS['Date'][1148:1257],data_MS['VMA'][1148:1257],color='orange',linewidth = 2)\n\nplt.figure(figsize=(13, 6))\n\n\nplt.plot(data_MS['Date'][1148:1257],data_MS['Adj Close'][1148:1257],color='red',linewidth = 3)\nplt.plot(data_MS['Date'][1148:1257], data_MS['EMA_0.1'][1148:1257], linewidth = 3)\nplt.plot(data_MS['Date'][1148:1257], data_MS['SMA 50'][1148:1257],linewidth=3)\n\nplt.legend(['Adj. Close','Alpha =  0.1', 'SMA 50 day'])\nplt.xlabel('Date')\nplt.ylabel('Share price')","4763c0bc":"#find the index of only 2016 #that a half year only though\ndata_MS['Date'].index[data_MS['Date']== '2021-01-04']\n#141:391 for 2017\n#392:642 for 2018\n#643:894 for 2019\n#895:1147 for 2020\n#no plot them","7811d80c":"\nfig, (ax1, ax2,ax3,ax4) = plt.subplots(4,1,figsize=(15,15))\nax1.plot(data_MS['Date'][141:391],data_MS['Volume'][141:391])\nax1.plot(data_MS['Date'][141:391],data_MS['EMA_Volume'][141:391],color='red')\n\nax2.plot(data_MS['Date'][392:642],data_MS['Volume'][392:642])\nax2.plot(data_MS['Date'][392:642],data_MS['EMA_Volume'][392:642],color='red')\n\nax3.plot(data_MS['Date'][643:894],data_MS['Volume'][643:894])\nax3.plot(data_MS['Date'][643:894],data_MS['EMA_Volume'][643:894],color='red')\n\n\nax4.plot(data_MS['Date'][895:1147],data_MS['Volume'][895:1147])\nax4.plot(data_MS['Date'][895:1147],data_MS['EMA_Volume'][895:1147],color='red')\n\n#plt.plot(data_MS['Date'][1148:1257],data_MS['EMA_Volume'][1148:1257],color='red')\na =data_MS['Volume'][141:391]\nb = data_MS['Volume'][392:642]\nc = data_MS['Volume'][643:894]\nd = data_MS['Volume'][895:1147]\n#print(np.mean(a))\nfig.tight_layout()\nprint('Mean Volume values')\nprint(f'For 2017: {np.mean(a)}')\nprint(f'For 2018: {np.mean(b)}')\nprint(f'For 2019: {np.mean(c)}')\nprint(f'For 2020: {np.mean(d)}') # 2020 has the highest volume traded during March April and May\n#What is the reason for that? People were selling due to COVID","a75299cc":"\n#What day had the highest volumes traded\ndata_MS.iloc[data_MS['Volume'].argmax()]","3487f953":"def df_to_plotly(df):\n    return {'z': df.values.tolist(),\n            'x': df.columns.tolist(),\n            'y': df.index.tolist() }\nimport plotly.graph_objects as go\ndata_MS_New = data_MS.corr()\nfig = go.Figure(data=go.Heatmap(df_to_plotly(data_MS_New)))\nfig.show()","b77c6295":"# Now look at the things that have a positive correlation and plot them\n#Plot for example the high with the low to see their relation\nimport plotly.express as px\nfig = px.scatter(data_MS, x= 'High',y = 'Low')\nfig.update_layout(title_text = 'High vs Low')\nfig.show()\n#CONCLUSION: the high and low are lineraly dependent\n#makes sense since the higher a price of a stock goes, it can reach a new and bigger low every day. ","1eaa1be8":"mean_close = data_MS['Close'].mean()\n\ndeviation = []\nfor i in data_MS['Close']:\n    value = i - mean_close\n    deviation.append(value)\ndeviation = np.array(deviation)\n\n\ndeviation_squared = deviation**2\n\ndeviation_squared_sum = sum(deviation_squared)\n#deviation_squared_mean = np.mean(deviation_squared)\nprint(deviation_squared_sum)\n\n\nvariance = deviation_squared_sum \/ len(data_MS)\n\nprint(variance)\n\nstd = np.sqrt(variance)\nstd\n#This is a measure of risk and shows how values \n#are spread out around the average price. \n#It gives traders an idea of how far the price may deviate from the average.","30398e0d":"#percent change, I took it from this video: https:\/\/www.youtube.com\/watch?v=d2kXmWzfS0w&ab_channel=DerrickSherrill\nclose_data = data_MS['Close']\npercent_change = close_data.pct_change()\nprint(percent_change)\n","d44a418b":"#Try and plot volatility, using this article: https:\/\/blog.quantinsti.com\/volatility-and-measures-of-risk-adjusted-return-based-on-volatility\/\n#But apparently it is the same numbers as above for the percenage change \n#so percent change up is the same as log_ret here\n#.shift will shift the value of the index by 1 in this case \n#print(data_MS['Close'].shift(1))\n#print(data_MS['Close'] \/ data_MS['Close'].shift(1))\n\ndata_MS['Log_Ret'] = np.log(data_MS['Close'] \/ data_MS['Close'].shift(1))\n# Compute Volatility using the pandas rolling standard deviation function\ndata_MS['Volatility'] = data_MS['Log_Ret'].rolling(window=252).std() * np.sqrt(252)\ndata_MS.head()","c3e86367":"#Just the Volatility index plot\nplt.plot(data_MS['Date'],data_MS['Volatility'])\nplt.xticks(rotation=45)\nplt.xlabel('Date')\nplt.ylabel('Volatility index')\n#figure.tight_layout()","ddc4a1e1":"#Compare the two plots\nfigure, ax = plt.subplots(2)\n\nax[0].plot(data_MS['Date'],data_MS['Close'],color='red',label = 'Close')\nax[0].set(ylabel = 'Close price')\nax[0].legend()\nax[1].plot(data_MS['Date'],data_MS['Volatility'],color='orange',label = 'Volatility')\nax[1].legend()\nplt.xticks(rotation=45)\nplt.xlabel('Date')\nplt.ylabel('Volatility index')\n\nfig = plt.gcf()\nfig.set_size_inches(15.5, 8)","8cfbf73b":"data_MS.set_index('Date', inplace=True)\n#plt.plot(data_MS['Date'],data_MS['Close'])\ndata_MS['Close'][1000:].plot(label = 'Close',figsize=[15,7])\ndata_MS['Open'][1000:].plot(label = 'Open')\ndata_MS['High'][1000:].plot(label = 'High')\ndata_MS['Low'][1000:].plot(label = 'Low')\nplt.xticks(rotation=45)\nplt.legend()\nplt.xlabel('Date')\nplt.ylabel('Share price')","b206eed3":"#data_MS = data_MS.set_index('Date')\ndata_MS['Volume'][1000:].plot()\nplt.xticks(rotation=45)","9ac92807":"import pandas as pd #imoort all the data\nms = pd.read_csv('..\/input\/microsoft-share-price\/MSFT_data.csv')\ngoogle = pd.read_csv('..\/input\/google-data\/GOOGL_data.csv')\napple = pd.read_csv('..\/input\/AAPL-data\/AAPL_data.csv')","7b1ca679":"apple = apple.set_index('Date')\nms = ms.set_index('Date')\ngoogle = google.set_index('Date')","b15f896f":"import matplotlib.pyplot as plt \nms['Open'].plot(label = 'MSFT',figsize=(15,7))\napple['Open'].plot(label = 'AAPL')\ngoogle['Open'].plot(label = 'GOOGL')\nplt.xticks(rotation= 45)\nplt.legend()\nplt.xlabel('Date')\nplt.ylabel('Share price in $')","6c558f49":"google['Volume'].plot(label = 'GOOGL',figsize=(15,7))\napple['Volume'].plot(label = 'AAPL')\nms['Volume'].plot(label = 'MSFT')\nplt.legend()","48097ed8":"\n#What is the highest volume number in the apple data?\napple.iloc[apple['Volume'].argmax()]","01d530a2":"#This high jump explains the volume\napple['Open'][100:200].plot()\nplt.xticks(rotation=45)","1ba62ec1":"#Market cap = Open price * Volume, not a true representation\napple['Total traded'] = apple['Open'] * apple['Volume']\ngoogle['Total traded'] = google['Open'] * google['Volume']\nms['Total traded'] = ms['Open'] * ms['Volume']","d74090dc":"google['Total traded'].plot(label = 'GOOGL',figsize=(15,7))\napple['Total traded'].plot(label = 'AAPL')\nms['Total traded'].plot(label = 'MSFT')\nplt.legend()","f7cb76b7":"#What is the highest total traded, 2 trillion dollars market cap is reached\napple.iloc[apple['Total traded'].argmax()]","46165074":"# Moving averages,\n#in my previous attempt, SMA and CMA will not be suitable, but EMA seems very good,\n#Here I will try the example from the video on apple\n\napple['Open'].plot(figsize=(15,7))\napple['MA50'] = apple['Open'].rolling(50).mean()\napple['MA200'] = apple['Open'].rolling(200).mean()\n\napple['MA50'].plot()\napple['MA200'].plot()\n","02c79013":"# find correlation between these companies \n\nfrom pandas.plotting import scatter_matrix\n#concatenate all the data on the columns side, axis = 1 and call it tech\ntech = pd.concat([apple['Open'],ms['Open'],google['Open']],axis=1)\ntech.columns = ['Apple Open', 'MSFT Open', 'Google Open']\ntech.head()","88c326e2":"scatter_matrix(tech,figsize = (15,7),hist_kwds = {'bins':50})\n#between all of them there is a positive correlation\n#first enter the frame and change the histogram keyword to have 50 bins","69f7560e":"#daily percentage change\n\napple['return'] = (apple['Close']\/ apple['Close'].shift(1))-1\nms['return'] = (ms['Close']\/ ms['Close'].shift(1))-1\ngoogle['return'] = (google['Close']\/ google['Close'].shift(1))-1","2dd6b4b8":"#Plot the histogram of returns, alpha decides how transparent the image will be\n#Volatility, the bigger the dispersion the more volatile the plot is \n\n\nms['return'].hist(bins=100,label = 'MSFT',alpha= 0.5)\napple['return'].hist(bins=100,label = 'APPL',alpha= 0.5,figsize=(13,4))\ngoogle['return'].hist(bins=100,label = 'GOOGL',alpha= 0.5)\nplt.legend()","317a4ece":"apple['return'].plot(kind='kde',figsize=(13,6),label = 'APPL')\ngoogle['return'].plot(kind='kde',label = 'GOOGL')\nms['return'].plot(kind='kde',label = 'MSFT')\nplt.legend() #Kernel Density Estimation plot","c420657a":"#create a new data witht the\nbox = pd.concat([apple['return'],ms['return'],google['return']],axis=1)\nbox.columns = ['AAPL', 'MSFT', 'GOOGL']\nbox.plot(kind='box',figsize=(15,6))\nbox\n#Boxplots tell different information. We can see the outliers, maximum and minimum vlaues,\n#upper and lower quartile, median","a757ec10":"scatter_matrix(box,figsize = (10,8),hist_kwds = {'bins':50},alpha = 0.5)","7916d0dd":"#Cumulative return has \napple['Cumulative return'] = (1+ apple['return']).cumprod() #cumulative product\nms['Cumulative return'] = (1+ ms['return']).cumprod()\ngoogle['Cumulative return'] = (1+ google['return']).cumprod()\ngoogle.head()","1c41fb08":"apple['Cumulative return'].plot(figsize=(15,6),label = 'AAPL')\nms['Cumulative return'].plot(label = 'MSFT')\ngoogle['Cumulative return'].plot(label = 'GOOGL')\nplt.title('Cumulative return vs time')\nplt.legend()\n#apple.tail() # 5.182248\n#ms.tail() #5.130435\n#google.tail() #3.327226","2a4c5a1f":"%matplotlib inline\n#from matplotlib.pylab import rcParams\n#rcParams['figure.figsize']=20,10\nfrom keras.models import Sequential  \nfrom keras.layers import LSTM,Dropout,Dense\nfrom sklearn.preprocessing import MinMaxScaler \n\n#Sequential for initializing the neural network\n#Dense for adding a densely connected neural network layer\n#LSTM for adding the Long Short-Term Memory layer\n#Dropout for adding dropout layers that prevent overfitting","a5b59cdd":"training_set = data_MS.iloc[:,3:4].values #Take the Open price column and all the values on it\ntraining_set","1144f0d5":"from sklearn.preprocessing import MinMaxScaler # first scale the training set\nsc = MinMaxScaler()\ntraining_set_scaled = sc.fit_transform(training_set)\ntraining_set_scaled #This is the scaled training data","ae038036":"len(training_set_scaled[0:60, 0])","94fd07dc":"X_train = [] #Create data in 60 timesteps and convert it to an numpy array.\ny_train = []\nfor i in range(60, 1258):    #1258 - 60 = 1198 #Go from 60 until 1258, excluding the last element\n    #print(i)\n    X_train.append(training_set_scaled[i-60:i]) #Take 60 elements [0:60], [1:61], [2:62] from the training scaled data \n    y_train.append(training_set_scaled[i])   #takes a single number, starting from the 60th index\n    \n#print(len(X_train))\n#print(len(y_train))\nX_train, y_train = np.array(X_train), np.array(y_train)\n\n#print(X_train)\nX_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1)) \n#print(X_train[0])              #1198 rows          #60 elements    #1 column\n#reshpae a into 1198 rows, each with 60 elements under 1 column","4e22b197":"#### a = np.arange(6).reshape((3, 2,))\nprint(a) \na = np.reshape(a, (a.shape[0], a.shape[1], 1)) #reshpae a into 3 rows, each with 2 elements under 1 column\na                      #3          #2","4c878123":"#The input is a 3D tensor with shape [batch, timesteps, feature].\n\nregressor = Sequential()\n#50 dimensionality of the output space.| \n#Whether to return the last output. in the output sequence, or the full sequence. \n#Shape of training set = (60,1)\nregressor.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\nregressor.add(Dropout(0.2)) #to prevent overfitting, drop 20% of the layers\n\nregressor.add(LSTM(units = 50, return_sequences = True))\nregressor.add(Dropout(0.2))\n\nregressor.add(LSTM(units = 50, return_sequences = True))\nregressor.add(Dropout(0.2))\n\nregressor.add(LSTM(units = 50))\nregressor.add(Dropout(0.2))\n\nregressor.add(Dense(units = 1))\n\nregressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n\nregressor.fit(X_train, y_train, epochs = 100, batch_size = 32)","32786f49":"import pandas as pd    #insert the test data         #Intesr the latest data of MSFT\ndataset_test = pd.read_csv('..\/input\/msft-data-from-1106-until-0207\/MSFT (1).csv')\nreal_stock_price = dataset_test.iloc[:,1:2].values","b4ab7725":"dataset_test.tail()","b3e82348":"real_stock_price","0b763988":"dataset_total = pd.concat((data_MS['Open'], dataset_test['Open']), axis = 0) #combine both datasets over the rows\n#print(len(dataset_test))#1273                  #15  \n\n#print(dataset_total[1198:]) just take that fraction and save it on inputs\ninputs = dataset_total[len(dataset_total) - len(dataset_test) - 60:].values   #1273 - 15 = 1258 - 60 = 1198:\n\n#print(inputs)\ninputs = inputs.reshape(-1,1) #a single line\n#print(len(inputs))\ninputs = sc.transform(inputs) #standarize the data with Min Max Scalar, seperately form the train set\n#print(inputs)\n\nX_test = []     \n\nfor i in range(60, 76): #the length of inputs is 75\n    X_test.append(inputs[i-60:i, 0])  #[0:60], [1:61], [2:62], \nX_test = np.array(X_test)\nprint(X_test.shape[1]) #16\n\n\n\n\n                                     #16                 60                           \nX_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1)) #same as before, same shape as X_train\n#print(X_test)\n\npredicted_stock_price = regressor.predict(X_test)\n#print(predicted_stock_price)\npredicted_stock_price = sc.inverse_transform(predicted_stock_price) #get back the normal numbers \nprint(len(predicted_stock_price))\nprint(len(dataset_test['Open']))","7035fa81":"import matplotlib.pyplot as plt\nplt.figure(figsize=(15, 7))\n\nplt.plot(dataset_test['Date'],real_stock_price, color = 'black', label = 'MSFT Stock Price')\nplt.plot(predicted_stock_price, color = 'green', label = 'Predicted MSFT Stock Price')\nplt.title('MSFT Stock Price Prediction')\nplt.xlabel('Time')\nplt.ylabel('MSFT stock price')\nplt.xticks(rotation = 45)\nplt.legend()\nplt.show()","4a4ba081":"#predicted_stock_price.pop(-1)\npredicted_stock_price= np.resize(predicted_stock_price, predicted_stock_price.size - 1)\n","ee594a83":"from sklearn.metrics import mean_absolute_error, mean_squared_error\nprint(mean_absolute_error(real_stock_price, predicted_stock_price))\nprint(mean_squared_error(real_stock_price, predicted_stock_price,squared = False))","80a9af73":"# Video Help","f275e51e":"### OHLC Chart","1dde7128":"so if you bought on 2016\/06\/13 and held until 2020\/06\/13, you will have grown by 39.32%\n\\\nThis is based on the closing price","133a51f3":"### Insight\n\nIt seems all three shares are correlated","cc33048d":"### Insight\nWhen the market is in a strong and sustained uptrend, the EMA indicator line will also show an uptrend and vice-versa for a downtrend. When the EMA crosses the SMA downwards, it goes into a downwards trend. But if EMA crosses SMA upwards, it is a upwards\/ buy trend.","d3e1a80f":"# Annual Return Calculation","11626205":"# Mcirosoft share price analysis","7fdde0ba":"### Insight\nAll values for cumulative return begin with 1 \\\nbut end with different values at the tail. \\\nSo we can answer the question, which stock has the highest return for 1$?? \\\nBoth AAPL and MSFT perform well, but AAPL is the winner (5.182248 vs 5.130435)","22ec4e28":"Any code used will be thanks to this video: https:\/\/www.youtube.com\/watch?v=57qAxRV577c&ab_channel=ahmadbazzi","75010932":"Source: https:\/\/www.investopedia.com\/terms\/a\/annual-return.asp","f9466ef9":"More information about LSTM: https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/LSTM","3f186db9":"# Predict MSFT stock price","ec63a65b":"# Yearly Rate of Return ","595ffecf":"### Cumulative return\n>A cumulative return on an investment is the aggregate amount that the investment has gained or lost over time, independent of the amount of time involved. The cumulative return is expressed as a percentage, and it is the raw mathematical return of the following calculation:\n$$\ni_t = (1+r_t) i_{t-1} = (1 + \\frac{p_t}{p_{t -1}}-1) i_{t-1} = \\frac{p_t}{p_{t -1}}i_{t-1}\n$$\n\nWe are accumulating the returns. Takes into account daily returns. Doesnt include divident","e0ff5441":"Often, the simple moving average is used to show a security\u2019s price trend. If the simple moving average is trending upwards, for example, this indicates its price is rising. The opposite is true if a security\u2019s price trend is declining. ","9b71a7dd":"# Pandas Profiling Report","4f77a3d3":"The adjusted closing price will give you a better idea of the overall value of the stock and help you make informed decisions about buying and selling, while the closing stock price will tell you the exact cash value of a share of stock at the end of the trading day.","a10ea56b":"Vidoe withe valuable information: https:\/\/youtu.be\/8HyCNIVRbSU","9fa1c2de":"https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.Series.ewm.html","4f388eff":"![rate of return.JPG](attachment:89aef839-4fa2-498e-8543-332488356a1e.JPG)","a2300bfe":"![equation.PNG](attachment:478fe526-4c7d-4891-841f-384af4a3a606.PNG)","5206f275":"# Google vs Microsoft vs Apple","73e4a34d":"This article explains the sharp rise in volume: https:\/\/9to5mac.com\/2017\/02\/01\/aapl-stock-opens-up-5-after-company-reports-record-revenue-beats-analyst-expectations\/","766b8cea":"# Additional research","81b6a258":"### Insight\nTHe SMA has an upwards trend so the share price is bound to increase. \n### Popular Trading Patterns\nTwo popular trading patterns that use simple moving averages include the death cross and a golden cross. A death cross occurs when the 50-day SMA crosses below the 200-day SMA. This is considered a bearish signal, that further losses are in store. The golden cross occurs when a short-term SMA breaks above a long-term SMA.","6baf5372":"Reference followed: https:\/\/www.kdnuggets.com\/2018\/11\/keras-long-short-term-memory-lstm-model-predict-stock-prices.html","63efc572":"* I used this code https:\/\/github.com\/CNuge\/kaggle-code\/blob\/master\/stock_data\/getSandP.py to update the MSFT share price details","c2ccc607":"# Volatility Index","0767dc0a":"### Daily percentage change  \n$$\nr_t = \\frac{p_t}{p_{t -1}} -1\n$$\nreturn at time t = (price at time t \/ price at time t-1 ) -1\\\nThe value reported is percentage gain (or loss). The larger the ratio, the better the returns. \\\nGood to use it to analyze the volatility of the stock (wider distribution) \n","a01a05eb":"![image.png](attachment:41e12197-8e1e-4583-94f3-9d20e4f12bf4.png)(http:\/\/)","420848eb":"Source: https:\/\/www.investopedia.com\/terms\/y\/yearly-rate-of-return-method.asp\n\\\nIf you hold it for one year between 2016\/06\/13 and 2017\/06\/13 you will get 40.9% return, etc.\n\\\nThe numbers below and the plot show how your money will have grown if you held year by year. Dividend is excluded","cdb992b6":">While variance captures the dispersion of returns \\\naround the mean of an asset in general, volatility is a\\\nmeasure of that variance bounded by a specific period of \\\ntime. Thus, we can report daily volatility, weekly, monthly,\\\nor annualized volatility. It is, therefore, useful to think of\\\nvolatility as the annualized standard deviation.\n\nArticle Followed: https:\/\/www.investopedia.com\/terms\/v\/volatility.asp","0b14d41c":"### EMA \nWhat is the difference between a simple moving average and an exponential moving average?\nWhile a simple moving average gives an equal weight to each of the values within a time period, an exponential moving average places greater weight on recent prices. Exponential moving averages are typically seen as a more timely indicator of a price trend. Compared to the simple moving average, the exponential moving average reacts faster to changes, since is more sensitive to recent movements.","fcd8cf9e":"![wall.jpg](attachment:556d07f0-c891-4b5e-a6f3-98df0558342e.jpg)","f9572d48":"# The 3 Moving averages","f378a102":"This article could explain that big movement: https:\/\/www.macrumors.com\/2020\/08\/24\/apple-stock-500\/","9b3e68ea":"# Trading Volume","f0a6df4c":"Maybe becasue MSFT overpayed for Linkdin, This article could explain why that date had highest volume: https:\/\/www.fool.com\/investing\/2016\/06\/29\/why-microsoft-stock-is-down-13-in-2016.aspx","4a0ae5a0":"### Insight\n>Trading volume can help an investor identify momentum in a security and confirm a trend. If trading volume increases, prices generally move in the same direction. That is, if a security is continuing higher in an uptrend, the volume of the security should also increase and vice versa.\n\nref:https:\/\/www.investopedia.com\/ask\/answers\/041015\/why-trading-volume-important-investors.asp","ecc0fd86":"In this case, the resulting variance is 3476.70. The square root is taken to get the standard deviation.\n\\\nThis equals 58.9. This is a measure of risk and shows how values are \n\\\nspread out around the average price. It gives traders an idea of how far \n\\\nthe price may deviate from the average. ","cc1ba120":"### Insight\nThe higher the variance, the higher the standard deviation -> higer volatility.\\\nMSFT seems to be a bit more volatile compared to the rest"}}