{"cell_type":{"21c05214":"code","3152bd18":"code","451f1455":"code","528c4039":"code","70c0609b":"code","dcc10f9b":"code","2bc8e666":"code","30a04775":"code","3bfe7394":"code","803bf218":"code","ff7f3dcb":"code","5eded599":"code","ee69baff":"code","30c51227":"code","fbf21929":"code","b4d61b9f":"code","0d289dab":"code","e1dd2d5c":"code","08385077":"code","869ec617":"code","0ab5087d":"code","6c3e86ce":"code","bd6f6ac0":"code","9529e621":"code","e93ddacd":"code","99616152":"code","29fabae0":"code","a1f4f773":"code","5fd5096e":"code","3989ea0a":"code","e622b6f6":"code","7f2fbebd":"code","9a378024":"code","f8a51ce6":"code","8735873b":"code","bb548473":"code","e4e39f4f":"code","15c1f168":"code","4ac83aee":"code","a5209b45":"code","ae65f3e5":"code","ba10e7d5":"code","29d9a6cd":"code","069ddda5":"code","697afb45":"code","73cf676c":"code","71a8ba78":"code","4138b345":"code","7dddc0b6":"code","5ec39fae":"code","174a94f6":"code","f7c13a6f":"code","30bc0418":"code","15e82d96":"code","4e2c8245":"code","d53349d1":"code","8fca4f20":"code","52c5f7d2":"code","828f8538":"code","db8feff7":"code","0358fd37":"code","950e77d4":"code","177c12e8":"code","526454c9":"code","a2b29c3a":"code","d5c0a151":"code","8dfa2057":"code","c8aa2ea5":"code","9cdeead8":"code","4f023611":"code","6910d87e":"code","98e44bdd":"code","310e2094":"code","c018b8d6":"code","c0281fa7":"code","b3320bd3":"code","c73f29fc":"code","23630ffc":"code","af972e7a":"code","bac9b9e1":"code","e1aec903":"code","eb0129b2":"code","917f7f1b":"code","53dca6be":"code","5a3484fa":"code","98f254f4":"markdown","ea72b7b2":"markdown","c3af46ac":"markdown","c70eed4c":"markdown","8b854c5d":"markdown","e7caf66b":"markdown","394d4562":"markdown","399af849":"markdown","a0b8e85b":"markdown","35781020":"markdown","1999ef15":"markdown","eb0fb8d3":"markdown","c12589d4":"markdown","9dae5c16":"markdown","6f632603":"markdown","3e2c4000":"markdown","166eac09":"markdown","3b58197d":"markdown","3bc7c0fe":"markdown","d50e5b07":"markdown","a07c11fb":"markdown","bdc8b999":"markdown","962145d8":"markdown","469a731b":"markdown","ddd97ce3":"markdown","6dbd1d1d":"markdown"},"source":{"21c05214":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os","3152bd18":"os.getcwd()","451f1455":"test=pd.read_csv('..\/input\/titanic\/test.csv')\ntrain=pd.read_csv('..\/input\/titanic\/train.csv')\n# train=pd.read_csv('train.csv')\n# train=pd.read_csv('test.csv')","528c4039":"train.head()","70c0609b":"test.head()","dcc10f9b":"train.info()","2bc8e666":"survived = 'survived'\nnot_survived = 'not survived'\nfig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))\nwomen = train[train['Sex']=='female']\nmen = train[train['Sex']=='male']\nax = sns.distplot(women[women['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[0], kde =False)\nax = sns.distplot(women[women['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[0], kde =False)\nax.legend()\nax.set_title('Female')\nax = sns.distplot(men[men['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[1], kde = False)\nax = sns.distplot(men[men['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[1], kde = False)\nax.legend()\n_ = ax.set_title('Male')","30a04775":"fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize = (13, 5))\nsns.boxplot(x = 'Age', data = train, orient = 'v', ax = ax1)\nax1.set_xlabel('People Age', fontsize=15)\nax1.set_ylabel('Age', fontsize=15)\nax1.set_title('Age Distribution', fontsize=15)\nax1.tick_params(labelsize=15)\n\n# sns.distplot(train['Age'], ax = ax2)\n# sns.despine(ax = ax2)\n# ax2.set_xlabel('Age', fontsize=15)\n# ax2.set_ylabel('Occurence', fontsize=15)\n# ax2.set_title('Age x Ocucurence', fontsize=15)\n# ax2.tick_params(labelsize=15)\n\nplt.subplots_adjust(wspace=0.5)\nplt.tight_layout()","3bfe7394":"FacetGrid = sns.FacetGrid(train, row='Embarked', size=4.5, aspect=1.6)\nFacetGrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette=None,  order=None, hue_order=None )\nFacetGrid.add_legend()","803bf218":"sns.barplot(x='Pclass', y='Survived', data=train)","ff7f3dcb":"grid = sns.FacetGrid(train, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();","5eded599":"data = [train, test]\nfor dataset in data:\n    dataset['relatives'] = dataset['SibSp'] + dataset['Parch']\n    dataset.loc[dataset['relatives'] > 0, 'not_alone'] = 0\n    dataset.loc[dataset['relatives'] == 0, 'not_alone'] = 1\n    dataset['not_alone'] = dataset['not_alone'].astype(int)\ntrain['not_alone'].value_counts()","ee69baff":"axes = sns.factorplot('relatives','Survived', \n                      data=train, aspect = 2.5, )\n","30c51227":"train=train.drop(['PassengerId'],axis=1)\ntest=test.drop(['PassengerId'],axis=1)","fbf21929":"train.head()","b4d61b9f":"test.head()","0d289dab":"def Check_duplicate(df):\n    duplicate_row=df.duplicated().sum()\n    null_values=df.isnull().sum()\n    Total_null_values=sum(null_values)\n    if(duplicate_row>0):\n        print(\"Please remove duplicates Row=\",duplicate_row)\n    elif(Total_null_values>0):\n            print(\"Please deal with Missing Values\",null_values)\n    else:\n        print(duplicate_row,\"duplicated and null\/Missing vlues\",Total_null_values,\" in this dataFrame\")","e1dd2d5c":"Check_duplicate(train)","08385077":"Check_duplicate(test)","869ec617":"# Quartiles\ndef deal_with_outlier(column,df_name,col_name):\n    Q1=column.quantile(q = 0.25)\n    Q2=column.quantile(q = 0.50)\n    Q3=column.quantile(q = 0.75)\n    Q4=column.quantile(q = 1.00)\n    print('1\u00ba Quartile: ', Q1)\n    print('2\u00ba Quartile: ', Q2)\n    print('3\u00ba Quartile: ', Q3)\n    print('4\u00ba Quartile: ', Q4)\n    #Calculate the outliers:\n    IQR = Q3 - Q1  # Interquartile range, \n    Lower=Q1 - 1.5 * IQR\n    Upper=Q3 + 1.5 * IQR\n    print(\"Lower bound\",Lower)\n    print(\"Upper bound\",Upper)  \n    out=column.quantile(q = 0.75) + 1.5*(column.quantile(q = 0.75) - column.quantile(q = 0.25))\n    print(' above: ',out , 'are outliers')\n   \n    \n#      show the percentage of outlier for upper\n    print('Number of outliers in upper: ', df_name[column > Upper][col_name].count())\n    print('Number of clients: ', len(df_name))\n#Outliers in %\n    print('Outliers are:', round(df_name[column > Upper][col_name].count()*100\/len(df_name),2), '%')\n    \n    #     show the percentage of outlier for lower\n    print('Number of outliers in Lower: ', df_name[column > Lower][col_name].count())\n    print('Number of clients: ', len(df_name))\n#Outliers in %\n    print('Outliers are:', round(df_name[column > Lower][col_name].count()*100\/len(df_name),2), '%')\n    \n#     Deal with outlier\n\n    ## Flooring\n    df_name.loc[column < (Q1 - 1.5 * IQR),col_name] = column.quantile(0.05)\n    ## Capping \n    df_name.loc[column > (Q3 + 1.5 * IQR),col_name] = column.quantile(0.95)\n    \n    Boxplot=df_name.boxplot(column=[col_name])\n    \n#     After deal with outlier\n    \n#     show the percentage of outlier for upper \n    print('Number of outliers in upper Afer deal: ', df_name[column > Upper][col_name].count())\n    print('Number of clients: ', len(df_name))\n#Outliers in %\n    print('Outliers are Afer deal:', round(df_name[column > Upper][col_name].count()*100\/len(df_name),2), '%')\n    \n    #     show the percentage of outlier for lower\n    print('Number of outliers in Lower Afer deal: ', df_name[column > Lower][col_name].count())\n    print('Number of clients: ', len(df_name))\n#Outliers in %\n    print('Outliers are Afer deal:', round(df_name[column > Lower][col_name].count()*100\/len(df_name),2), '%')\n    return Boxplot","0ab5087d":"train.boxplot(column=['Age'])\nplt.title('we have see outlier in AGE Variable')","6c3e86ce":"deal_with_outlier(train['Age'],train,'Age')\n\n# train.boxplot(column=['Age'])\n# plt.title('we have see outlier in AGE Variable')","bd6f6ac0":"# Calculating some values to evaluete this independent variable\nprint('MEAN:', round(train['Age'].mean(), 1))\n# A low standard deviation indicates that the data points tend to be close to the mean or expected value\n# A high standard deviation indicates that the data points are scattered\nprint('STD :', round(train['Age'].std(), 1))\n\nprint('Median',round(train['Age'].median(),1))\n# I thing the best way to give a precisly insight abou dispersion is using the CV (coefficient variation) (STD\/MEAN)*100\n#    cv < 15%, low dispersion\n#    cv > 30%, high dispersion\nprint('CV  :',round(train['Age'].std()*100\/train['Age'].mean(), 1), ', High middle dispersion')","9529e621":"test.boxplot(column=['Age'])\nplt.title('We have see outlier in AGE Variable(test dataset)')","e93ddacd":"deal_with_outlier(test['Age'],test,'Age')\n\n# train.boxplot(column=['Age'])\n# plt.title('we have see outlier in AGE Variable')","99616152":"train['Age']=train['Age'].fillna(train['Age'].mean())\ntest['Age']=test['Age'].fillna(test['Age'].mean())","29fabae0":"Check_duplicate(train)","a1f4f773":"Check_duplicate(test)","5fd5096e":"train['Cabin'].mode()","3989ea0a":"# import re\n# deck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\n# data = [train, test]\n\n# for dataset in data:\n#     dataset['Cabin'] = train['Cabin'].fillna(\"U0\")\n#     dataset['Deck'] = train['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n#     dataset['Deck'] = train['Deck'].map(deck)\n#     dataset['Deck'] = train['Deck'].fillna(0)\n#     dataset['Deck'] = train['Deck'].astype(int)\n# we can now drop the cabin feature\ntrain = train.drop(['Cabin'], axis=1)\ntest= test.drop(['Cabin'], axis=1)","e622b6f6":"Check_duplicate(train)","7f2fbebd":"train = train.dropna()","9a378024":"train=train.reset_index(drop=True)","f8a51ce6":"Check_duplicate(train)","8735873b":"Check_duplicate(test)","bb548473":"train.boxplot(column=['Fare'])\nplt.title('we have see outlier in Fare Variable')","e4e39f4f":"deal_with_outlier(train['Fare'],train,'Fare')\n\n# train.boxplot(column=['Fare'])\n# plt.title('we have see outlier in Fare Variable')","15c1f168":"test.boxplot(column=['Fare'])\nplt.title('we have see outlier in Fare Variable(test dataset)')","4ac83aee":"deal_with_outlier(test['Fare'],test,'Fare')\n\n# train.boxplot(column=['Fare'])\n# plt.title('we have see outlier in Fare Variable')","a5209b45":"train['Fare']=train['Fare'].fillna(train['Fare'].mean())\ntest['Fare']=test['Fare'].fillna(test['Fare'].mean())","ae65f3e5":"Check_duplicate(train)","ba10e7d5":"Check_duplicate(test)","29d9a6cd":"train.info()","069ddda5":"train.head()","697afb45":"Check_duplicate(test)  # now age have no duplicte and no outlier","73cf676c":"data = [train, test]\n\nfor dataset in data:\n#     dataset['Fare'] = dataset['Fare'].fillna(0)\n    dataset['Fare'] = dataset['Fare'].astype(int)","71a8ba78":"train.info()","4138b345":"test.info()","7dddc0b6":"data = [train, test]\ntitles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n\nfor dataset in data:\n    # extract titles\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    # replace titles with a more common title or as Rare\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\\\n                                            'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    # convert titles into numbers\n    dataset['Title'] = dataset['Title'].map(titles)\n    # filling NaN with 0, to get safe\n    dataset['Title'] = dataset['Title'].fillna(0)\ntrain= train.drop(['Name'], axis=1)\ntest= test.drop(['Name'], axis=1)","5ec39fae":"genders = {\"male\": 0, \"female\": 1}\ndata = [train, test]\n\nfor dataset in data:\n    dataset['Sex'] = dataset['Sex'].map(genders)\n","174a94f6":"train['Ticket'].describe()","f7c13a6f":"train= train.drop(['Ticket'], axis=1)\ntest= test.drop(['Ticket'], axis=1)","30bc0418":"ports = {\"S\": 0, \"C\": 1, \"Q\": 2}\ndata = [train, test]\n\nfor dataset in data:\n    dataset['Embarked'] = dataset['Embarked'].map(ports)","15e82d96":"data = [train, test]\nfor dataset in data:\n    dataset['Age'] = dataset['Age'].astype(int)\n    dataset.loc[ dataset['Age'] <= 11, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 11) & (dataset['Age'] <= 18), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 18) & (dataset['Age'] <= 22), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 22) & (dataset['Age'] <= 27), 'Age'] = 3\n    dataset.loc[(dataset['Age'] > 27) & (dataset['Age'] <= 33), 'Age'] = 4\n    dataset.loc[(dataset['Age'] > 33) & (dataset['Age'] <= 40), 'Age'] = 5\n    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 66), 'Age'] = 6\n    dataset.loc[ dataset['Age'] > 66, 'Age'] = 6\n\n# let's see how it's distributed train_df['Age'].value_counts()","4e2c8245":"train.head()","d53349d1":"test.head()","8fca4f20":"y=train.Survived","52c5f7d2":"y.head()","828f8538":"X=train.drop(['Survived'], axis=1)","db8feff7":"X.head()","0358fd37":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n","950e77d4":"print('Train Dataset',X_train.shape,y_train.shape)\nprint('Test Dataset',X_test.shape,y_test.shape)","177c12e8":"from sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport itertools\nfrom sklearn.model_selection import KFold\n","526454c9":"from sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX_train_f = sc_X.fit_transform(X_train)\nX_test_f = sc_X.transform(X_test)\nk_fold = KFold(n_splits=10, shuffle=True, random_state=0)","a2b29c3a":"logmodel = LogisticRegression() \nlogmodel.fit(X_train,y_train)\nlogpred = logmodel.predict(X_test)\n\n# predict our train dataset\n\ncnf_matrix=confusion_matrix(y_test, logpred)\nprint(\"Confusion Matrix on Train Dataset:\")\nprint(confusion_matrix(y_test, logpred))\nprint(round(accuracy_score(y_test, logpred),2)*100)\n\n","d5c0a151":"# Stratified cross validation\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())\nprint('strtified cross validation accuracy',LOGCV)","8dfa2057":"from sklearn.metrics import classification_report, confusion_matrix\nimport itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\nprint(confusion_matrix(y_test, logpred))\nprint(round(accuracy_score(y_test, logpred),2)*100)","c8aa2ea5":"plt.figure()\nplot_confusion_matrix(cnf_matrix, classes=['YES=1','NO=0'],normalize= False,  title='Confusion matrix')","9cdeead8":"logpred = logmodel.predict(test)\n\n# confusion matrix\n# cnf_matrix=confusion_matrix(y_train, logpred)\n# print(confusion_matrix(y_train, logpred))\n# print(round(accuracy_score(y_train, logpred),2)*100)\n","4f023611":"!pip install pydotplus","6910d87e":"import numpy as np, pandas as pd, matplotlib.pyplot as plt, pydotplus\nfrom sklearn import tree, metrics, model_selection, preprocessing\nfrom IPython.display import Image, display\nfrom sklearn.tree import export_graphviz","98e44bdd":"# train the decision tree\ndtree = tree.DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=0)\ndtree.fit(X_train,y_train)","310e2094":"# use the model to make predictions with the test data\ny_pred_test = dtree.predict(X_test)\ny_pred_test","c018b8d6":"# how did our model perform?\n# count_misclassified =(y_test!= y_pred).sum()\n# print('Misclassified samples: {}'.format(count_misclassified))\naccuracy = metrics.accuracy_score(y_test, y_pred_test)\nprint('Accuracy: {:.2f}'.format(accuracy))","c0281fa7":"# use the model to make predictions with the test data\ny_pred = dtree.predict(test)\ny_pred","b3320bd3":"# from sklearn.cross_validation import KFold\n\n# cv = KFold(n=len(bank_final),  # Number of elements\n#            n_folds=10,            # Desired number of cv folds\n#            random_state=12) \ncv = KFold(n_splits=12, shuffle=True, random_state=0)","c73f29fc":"fold_accuracy = []\n\n# titanic_train[\"Sex\"] = encoded_sex\n\nfor train_fold, valid_fold in cv.split(X):\n    train = X.loc[train_fold] # Extract train data with cv indices\n    valid = X.loc[valid_fold] # Extract valid data with cv indices\n    \n    train_y = y.loc[train_fold]\n    valid_y = y.loc[valid_fold]\n    \n    model = dtree.fit(X = train, \n                           y = train_y)\n    valid_acc = model.score(X = valid, \n                            y = valid_y)\n    fold_accuracy.append(valid_acc)    \n\nprint(\"Accuracy per fold: \", fold_accuracy, \"\\n\")\nprint(\"Average accuracy: \", sum(fold_accuracy)\/len(fold_accuracy))","23630ffc":"import pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import roc_auc_score\n# from sklearn.cross_validation import train_test_split, cross_val_score\nfrom sklearn.model_selection import train_test_split, cross_val_score","af972e7a":"from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, BaggingRegressor, RandomForestRegressor\n\n# we can achieve the above two tasks using the following codes\n# Bagging: using all features\nrfc1 = RandomForestClassifier(max_features=8, random_state=1)\nrfc1.fit(X_train, y_train)\npred1 = rfc1.predict(X_test)\nprint(roc_auc_score(y_test, pred1))\n\n# play around with the setting for max_features\nrfc2 = RandomForestClassifier(max_features=6, random_state=1)\nrfc2.fit(X_train, y_train)\npred2 = rfc2.predict(X_test)\nprint(roc_auc_score(y_test, pred2))\n\n\n\n\n# applyin on test dataset------------------------------------------\n\n# we can achieve the above two tasks using the following codes\n# Bagging: using all features\n\n\n\n# rfc1 = RandomForestClassifier(max_features=8, random_state=1)\n# rfc1.fit(X_train, y_train)\npred_x = rfc1.predict(X_test)\nprint(roc_auc_score(y_test, pred_x))\n\n# # play around with the setting for max_features\n# rfc2 = RandomForestClassifier(max_features=6, random_state=1)\n# rfc2.fit(X_train, y_train)\n# pred2 = rfc2.predict(X_train)\n# print(roc_auc_score(y_train, pred2))\n","bac9b9e1":"y_pred_final = dtree.predict(test)","e1aec903":"submission=pd.read_csv('..\/input\/titanic\/test.csv')","eb0129b2":"submission['Survived']=y_pred_final","917f7f1b":"submission= submission.drop(['Pclass','Name','Sex','Age','SibSp','Parch','Ticket','Fare','Cabin','Embarked'], axis=1)","53dca6be":"submission.head()","5a3484fa":"submission.to_csv('submission',index=False)","98f254f4":"### Model applying on Test dataset","ea72b7b2":"### Name:\nWe will use the Name feature to extract the Titles from the Name, so that we can build a new feature out of that.","c3af46ac":"### Fare\nfirst deal with outlier & missing then Converting \u201cFare\u201d from float to int64, using the \u201castype()\u201d function pandas provides:\n","c70eed4c":"### Cabin","8b854c5d":"### Deal with missing and duplicate values","e7caf66b":"## Selecting Random Features\nWith the bagging example, we gained some accuracy over a single decision tree. We achieved an AUC score of around 0.851 with bagging.\n`","394d4562":"### AGE variable","399af849":"### Now we are select the feature","a0b8e85b":"### Ticket:","35781020":"## Bagging\n\nThe ensemble method we will be using today is called **bagging**, which is short for **bootstrap aggregating**.\n\nBagging builds multiple base models with **resampled training data with replacement.** We train $k$ base classifiers on $k$ different samples of training data. Using random subsets of the data to train base models promotes more differences between the base models.\n\nRandom Forests, which \"bag\" Multiple decision trees, can achieve very high classification accuracy.","1999ef15":"### DECESION TREE","eb0fb8d3":"### LOGISTIC REGRESSION","c12589d4":"### Evaluate the model's performance on Train Datasets\n\nIncluding the tree's axis-parallel decision boundaries and how the tree splits","9dae5c16":"# We are getting more accuracy with Decesion tree. so we predict our final test dataset with decesion tree.","6f632603":"## Data preprocessing","3e2c4000":"### Creating Categories:\nWe will now create categories within the following features:","166eac09":"### Standarized Our variable","3b58197d":"## A Classification Problem Statement using Ensemble technique(Random Forest)","3bc7c0fe":"### Cross Validation\n\nCross Validation is a technique which involves reserving a particular sample of a data set on which you do not train the model. Later, you test the model on this sample before finalizing the model.","d50e5b07":"### SibSp and Parch:","a07c11fb":"### Embarked, Pclass and Sex:","bdc8b999":"##### Since the Ticket attribute has 681 unique tickets, it will be a bit tricky to convert them into useful categories. So we will drop it from the dataset.\n","962145d8":"### Sex:\nConvert \u2018Sex\u2019 feature into numeric.\n","469a731b":"##### we are droping the Passengerid because its everry are uniuqe.","ddd97ce3":"### Embarked:\nConvert \u2018Embarked\u2019 feature into numeric.\n","6dbd1d1d":"### Stratified cross-validation\nStratification is a technique where we rearrange the data in a way that each fold has a good representation of the whole dataset. It forces each fold to have at least m instances of each class"}}