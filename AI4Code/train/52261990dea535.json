{"cell_type":{"f2083756":"code","3fa18c2e":"code","a27f1a39":"markdown","96bd7bf1":"markdown","56f2385d":"markdown","fbb73035":"markdown","fe8dd809":"markdown"},"source":{"f2083756":"from sklearn.decomposition import PCA\nfrom sklearn.datasets import load_iris\n\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nn_components = 2\npca = PCA(n_components=n_components)\nX_pca = pca.fit_transform(X)","3fa18c2e":"import matplotlib.pyplot as plt\n\n\ncolors = ['yellow', 'lime', 'red']\n\nplt.figure(figsize=(8, 8))\nfor color, i, target_name in zip(colors, [0, 1, 2], iris.target_names):\n    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1],\n                color=color, lw=2, label=target_name)\n\nplt.title('PCA - Iris dataset')\nplt.legend(loc='best', shadow=False, scatterpoints=1)\nplt.axis([-4, 4, -1.5, 1.5])\n\nplt.show()","a27f1a39":"\n\n\nReferences:\n\n- FACELI, K. et al. Intelig\u00eancia Artificial: Uma Abordagem de Aprendizado de M\u00e1quina. [S.l.]: GEN, 2011.\n- WITTEN, I. H.; FRANK, E.; HALL, M. A. Data Mining Practical Machine Learning Tools and Techniques. [S.l.]: Elsevier, 2011.\n- CESTNIK, B. I. K.; BRATKO, I. Assistant 86: A knowledge-elicitation tool for sophis-ticated users. Progress in Machine Learning-Proc. of EWSL 87: 2nd European Working Session on Learning, p. 31, 1987.","96bd7bf1":"The PCA decomposition creates main (new) components based on desired number. Two components were specified in code above.","56f2385d":"### Feature selection\n\nThere also more techniques to reduce dimensionallity including feature selection. How often features seems useless in datasets? It could be frequent. To help this scenario, feature selection technique provides remove these worst features from datasets applying selection methods. Let's see them.\n\n#### Embedded Approach\n\nWhitebox algorithms provides analyse the impact of dataset features on final model. Analysing the tree output from Decision Tree algorithm will help identify relevant features, for example.\n\n#### Filtering\n\nFiltering analysis subsets of features with better correlation independent of machine learning algorithm that will be used\n\n#### Wrapper\n\nFor each feature subset, analyse it with desired machine learning algorithm\n\n\nThe `sklearn` library coverage these techniques and can be include in your model pipeline to improve analysis. This post describes the dimensionallity reduction in a general way, its recommended to search deepth about these techniques.\n\n\n\n","fbb73035":"### Aggregation\n\nThe aggregation approach provides replace features with new features (aggregated). Most commom technique in this field is PCA (Principal Component Analysis)\n\n#### PCA\n\nPCA uses an ortogonal transformation to convert a set of correlated observations in a set of main components (uncorrelated linear observations). It provides reduce features dimensionallity and redundancy, noisy, data compression and could prepare features for other techniques.\n\nLet's see an example using `sklearn`:","fe8dd809":"### Introduction\n\nSometimes, and it occurs at the most of the time, data scientists work on datasets with too many features or some useless\/poor ones. It could impact your model metrics along analysis process and frustate you. So, how you can handle it? For these cases, the pre-process will be your best friend.\n\nIn Machine Learning, this problem could be solve with **dimensionality reduction**. Many techniques are used to treat datasets with a high number of features, for example, these problems illustrates this scenario: Image Processing with pixels as features, Gene Expression Data with genes as features and Text Mining with words as features.\n\nThis post describes two techniques: aggregation and feature selection. Both are not rules where you must use but consider as resources to be relevant in your analysis."}}