{"cell_type":{"38e03f86":"code","39c5fb5f":"code","d1894b7e":"code","b9827919":"code","ae0ab36a":"code","2b1dde38":"code","96b35704":"code","dabf6313":"code","8ec8744d":"code","a8880395":"code","1e572032":"code","d2ce0fc5":"code","2ab42611":"code","f576f883":"code","54a0b663":"code","ea7b4e1e":"code","57a117f8":"code","a92a20a7":"code","d868c112":"code","6a279cf6":"code","f6104c72":"code","ba517f5a":"code","0771df6e":"code","83f27789":"code","822134cb":"code","622f7f24":"markdown","32b60739":"markdown","dbb30aae":"markdown","350ae70c":"markdown","3844506d":"markdown","62a6148f":"markdown","726c7c6d":"markdown"},"source":{"38e03f86":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","39c5fb5f":"import pandas as pd\nimport numpy as np\nimport statsmodels.formula.api as smf\nimport matplotlib.pylab as plt\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom sklearn import ensemble\nfrom sklearn.ensemble import RandomForestClassifier\n\n%matplotlib inline\npd.options.display.max_columns = None","d1894b7e":"test_raw = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntrain_raw = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")","b9827919":"train_raw.head()","ae0ab36a":"# Let's check for missing values, we will have to address these missing values later \nnull_cols = set(train_raw.columns[train_raw.isna().any()].tolist())\nnull_cols.update(test_raw.columns[test_raw.isna().any()].tolist())\nprint(null_cols)","2b1dde38":"# Let's first check the easier label columns \nprint(train_raw['Survived'].groupby(train_raw['Pclass']).mean(), '\\n')\nprint(train_raw['Survived'].groupby(train_raw['Sex']).mean(), '\\n')\nprint(train_raw['Survived'].groupby(train_raw['Embarked']).mean())\n\n# We can see that Pclass, Sex, and Embarking Port were very important\n# for determining survival ","96b35704":"# Now let's look at numeric columns, but first we must cut them\ndef get_age_range(data):\n    bins = [0, 18, 35, 50, np.inf]\n    names = ['0-18', '18-35', '35-50', '50+']\n    return pd.cut(data['Age'], bins, labels=names)\n\ntrain_raw['age_range'] = get_age_range(train_raw)\nprint(train_raw[['Survived','age_range']].groupby(['age_range']).mean())\n# print(train_raw[['Survived','age_range','Pclass']].groupby(['age_range','Pclass']).mean())\n\ndel train_raw['age_range'] # let's clean that up \n\n# Looks like being young is an advantage to survive the titanic ","dabf6313":"# What about your family size?\n\ndef get_family_size(data):\n    return data.apply(lambda row: \n                      \"alone\" if (row[\"SibSp\"] + row[\"Parch\"]) == 0 else \"small\" \n                      if (row[\"SibSp\"] + row[\"Parch\"]) <= 3 \n                      else \"large\", axis=1 )\n\ntrain_raw[\"family_size\"] = get_family_size(train_raw)\n        \n\nprint(train_raw[['Survived','family_size']].groupby(['family_size']).mean())\ndel train_raw['family_size']\n\n# Small families had higher survival rates, followed by solo travelers, and finally large families ","8ec8744d":"# What about Fare paid?\n\ntrain_raw['fare_range'] = pd.qcut(train_raw['Fare'], 6)\nprint(train_raw[['Survived','fare_range']].groupby(['fare_range']).mean())\ndel train_raw['fare_range']\n\n# Of course the more you paid, the richer you were, the more likely you survived","a8880395":"# For cabin, let's separate the number and letter\n\ndef get_cabin_letter(data):\n    return data.apply(lambda row: \"Missing\" if str(row['Cabin'])[0] == \"n\" else str(row['Cabin'])[0], axis=1)\n\ndef get_cabin_num(data):\n    raw_num = data['Cabin'].apply(lambda x: str(x).split(' ')[-1][1:])\n    raw_num.replace('an', np.NaN, inplace = True)\n    return raw_num.apply(lambda x: int(x) if not pd.isnull(x) and x != '' else np.NaN)\n\ntrain_raw['cabin_letter'] = get_cabin_letter(train_raw)\nprint(train_raw[['Survived','cabin_letter']].groupby(['cabin_letter']).mean().sort_values(by='Survived',ascending=False)\n      , '\\n')\ndel train_raw['cabin_letter']\n\n# We see that D, E, B cabins had higher survival rates, and those with missing cabins had the lowest\n# But what about cabin numbers?\n\ntrain_raw['cabin_num'] = pd.qcut(get_cabin_num(train_raw), 5)\nprint(train_raw[['Survived','cabin_num']].groupby(['cabin_num']).mean().sort_values(by='Survived',ascending=False))\ndel train_raw['cabin_num']\n\n# Looks like certain cabin numbers also had higher survival rates","1e572032":"# Let's do something similar to ticket\ndef get_ticket_len(data):\n    return data.apply(lambda row: len(row['Ticket']), axis=1)\n\ndef get_ticket_letter(data):\n    return data.apply(lambda row: str(str(row['Ticket'])[0]), axis=1)\n\ndef map_ticket_letter(letter):\n    if letter in ['3','2','1','S','P','C','A']:\n        return letter\n    elif letter in ['W','4','7','F','6','L','5','8','9']:\n        return \"uncommon\" \n    else:\n        return \"unknown\"\n\n# Does the ticket length tell us anything?\ntrain_raw['ticket_len'] = get_ticket_len(train_raw)\nprint(train_raw[['Survived','ticket_len']].groupby(['ticket_len']).mean(), '\\n')\ndel train_raw['ticket_len']\n#Looking at the results, a bit\n\n#What about the first ticket letter?\ntrain_raw['ticket_letter'] = get_ticket_letter(train_raw)\nprint(train_raw[['Survived','ticket_letter']].groupby(['ticket_letter']).mean().sort_values(by='Survived',ascending=False), '\\n')\n#There's definitely some info here, but we may have to cut down on which ticket letter matter, by looking at the counts\nprint(train_raw[['Survived','ticket_letter']].groupby(['ticket_letter']).count().sort_values(by='Survived',ascending=False), '\\n')\n\ndel train_raw['ticket_letter']","d2ce0fc5":"# Let's examine the info we can extract from Name\n\ndef get_title(data):\n    return data.apply(lambda row: row['Name'].split(\",\")[1].strip().split(\" \")[0], axis=1)\n\ndef get_name_len(data):\n    return data.apply(lambda row: len(row['Name']), axis=1)\n\n# We can get the title from Name\ntrain_raw['title'] = get_title(train_raw)\nprint(train_raw[['Survived','title']].groupby(['title']).mean().sort_values(by='Survived',ascending=False))\ndel train_raw['title']\n# We can see certain nobility titles seem to have way better odds of survival\n\n# What about how long the name is?\ntrain_raw['name_len'] = pd.qcut(get_name_len(train_raw), 5)\nprint(train_raw[['Survived','name_len']].groupby(['name_len']).mean().sort_values(by='Survived',ascending=False))\ndel train_raw['name_len']\n# We see that long names have higher survival rates, maybe people with longer names were also richer? ","2ab42611":"# These functions transform our columns into the final form we need them in \ndef trans_name(train, test):\n    for data in [train, test]:\n            data[\"title\"] = get_title(data)\n            data[\"name_len\"] = get_name_len(data)\n            del data[\"Name\"]\n    return train,test\n\ndef trans_age(train, test):\n    for data in [train, test]:\n        data[\"age_missing\"] = data.apply(lambda row: 1 if row['Age'] != row['Age'] else 0, axis=1)\n        newAges = train.groupby(['title', 'Pclass'])['Age']\n        data['Age'] = newAges.transform(lambda x: x.fillna(x.mean()))\n    return train,test\n    \ndef trans_ticket(train, test):\n    for data in [train, test]:\n        data[\"ticket_len\"] = get_ticket_len(data)\n        data[\"ticket_letter\"] = get_ticket_letter(data)\n        data[\"ticket_letter\"] = data.apply(lambda row: map_ticket_letter(row['ticket_letter']), axis=1)\n        del data['Ticket']\n    return train, test\n\ndef trans_family(train, test):\n    for data in [train, test]:\n        data[\"family_size\"] = get_family_size(data)\n        del data[\"SibSp\"]\n        del data[\"Parch\"]\n    return train,test\n\ndef trans_fare(train, test):\n    mean = train['Fare'].mean()    \n    train['Fare'].fillna(mean, inplace=True) \n    test['Fare'].fillna(mean, inplace=True) \n    return train,test\n\ndef trans_embarked(train, test):\n    train['Embarked'] = train['Embarked'].fillna(\"S\") \n    test['Embarked'] = test['Embarked'].fillna(\"S\") \n    return train,test\n\ndef trans_cabin(train, test):\n    for data in [train, test]:\n        data[\"cabin_missing\"] = data.apply(lambda row: 0 if row['Cabin'] == row['Cabin'] else 1, axis=1)\n        data[\"cabin_letter\"] = get_cabin_letter(data)\n        data[\"cabin_num1\"] = get_cabin_num(data)\n        data['cabin_num'] = pd.qcut(train['cabin_num1'],5)\n    \n    train = pd.concat((train, pd.get_dummies(train['cabin_num'], prefix = 'cabin_num')), axis = 1)\n    test = pd.concat((test, pd.get_dummies(test['cabin_num'], prefix = 'cabin_num')), axis = 1)\n    del train['cabin_num']\n    del test['cabin_num']\n    del train['cabin_num1']\n    del test['cabin_num1']\n    del test['Cabin']\n    del train['Cabin']\n    return train, test\n\ndef get_dummies(train, test):\n    cols_to_expand = ['Sex','Embarked', 'Pclass', 'title', 'cabin_letter','family_size', 'ticket_letter']\n    for column in cols_to_expand:\n        vals = set(train[column].unique())\n        vals = vals.intersection(set(test[column].unique()))\n        new_cols = [column + \"_\" + str(val) for val in vals]\n\n        train = pd.concat((train, pd.get_dummies(train[column], prefix = column)[new_cols]), axis = 1)\n        test = pd.concat((test, pd.get_dummies(test[column], prefix = column)[new_cols]), axis = 1)\n    \n        del train[column]\n        del test[column]\n\n    return train, test","f576f883":"def get_cols_by_type(data):\n    colsbytype = {}\n    for idx, val in zip(data.dtypes.index, data.dtypes.values):\n        if idx == 'Survived':\n            continue\n        val = str(val)\n        curr = colsbytype.get(val, set())\n        curr.add(idx)\n        colsbytype[val] = curr\n    for key in colsbytype.keys():\n        columns = list(colsbytype[key])\n        columns.sort()\n        colsbytype[key] = columns\n    return colsbytype\n\ndef scale_data(train, test, cols):\n    cols = list(train.columns)\n    cols.remove(\"PassengerId\")\n    cols.remove(\"Survived\")\n    all_cols = get_cols_by_type(test[cols])\n    num_cols = all_cols['int64']\n    num_cols.extend(all_cols['float64'])\n\n    scaler = MinMaxScaler()\n    X_train = train.copy()\n    X_test = test.copy()\n    X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n    X_test[num_cols] = scaler.transform(X_test[num_cols])\n    return X_train, X_test\n","54a0b663":"train = train_raw.copy()\ntest = test_raw.copy()\n\ntrain, test = trans_name(train, test)\ntrain, test = trans_age(train, test)  \ntrain, test = trans_ticket(train, test)\ntrain, test = trans_family(train, test)\ntrain, test = trans_fare(train, test)\ntrain, test = trans_embarked(train, test)\ntrain, test = trans_cabin(train, test)\ntrain, test = get_dummies(train, test)\ntrain, test = scale_data(train, test, list(train.columns))\n\nprint(\"Columns: \", len(train.columns))\ncols = list(train.columns[2:]) # features relevant to our ML model\n\nprint(\"Any Nulls: \", train.isnull().values.any(), test.isnull().values.any())\ntrain.head()","ea7b4e1e":"# We will use these functions to select the best features from our data\ndef select_cols(feature_cols, target, data, k):\n    from sklearn.feature_selection import SelectKBest, f_classif\n\n    selector = SelectKBest(f_classif, k=k)\n    X_new = selector.fit_transform(data[feature_cols], data[target]) \n\n    selected_features = pd.DataFrame(selector.inverse_transform(X_new), index=data.index, columns=feature_cols)\n    selected_columns = selected_features.columns[selected_features.var() != 0]\n    return selected_columns\n    \ndef find_best_cols(cols, target, data):\n    import warnings\n    warnings.filterwarnings(\"ignore\")\n    state = 1993  \n    size = 0.30 \n   \n    X_train = data[:500]\n    X_valid = data[500:]\n    \n    lr = ensemble.GradientBoostingRegressor() #Base Model\n    lr.fit(X_train[cols], X_train[target].values.ravel())\n    print (\"Base Score: \", lr.score(X_train[cols], X_train[target].values.ravel())) \n    best_score = 0\n    best_cols = cols\n    for k in range(len(cols)\/\/4, len(cols)):\n        lr = ensemble.RandomForestClassifier() # NOTE: using different classifier yields diff results \n        curr_cols = select_cols(cols, target, X_train, k)\n        lr.fit(X_train[curr_cols], X_train[target].values.ravel())\n        os_score = lr.score(X_valid[curr_cols], X_valid[target].values.ravel())\n        if os_score > best_score:\n            is_score = lr.score(X_train[curr_cols], X_train[target].values.ravel())\n            print (\"K= \", k, \", IS score: \", is_score, \", OS score: \", os_score)\n            best_score = os_score\n            best_cols = curr_cols\n            \n    return best_cols\n\nbest_cols = find_best_cols(cols, \"Survived\", train)\nbest_cols","57a117f8":"print(\"Previous Columns: \", len(best_cols))\nprint(\"Selected Columns: \", len(cols))\ncols = best_cols","a92a20a7":"# We'll use this function to save our results to CSV\ndef save_results(model, data):\n    pred_test = model.predict(data)\n\n    #PassengerId,Survived\n    test_res = test[[\"PassengerId\"]].copy()\n    test_res[\"Survived\"] = pred_test\n    test_res.to_csv(\"\/kaggle\/working\/my_predictions.csv\", index=False)\n    return test_res","d868c112":"# We use this function to tune our model\ndef get_tuned_model(estimator, param_grid, scoring, X_train, Y_train):\n    from sklearn.model_selection import GridSearchCV\n\n    grid = GridSearchCV(estimator = estimator, \n                       param_grid = param_grid,\n                       scoring = scoring,\n                       cv=3,\n                       n_jobs= -1\n                      )\n\n    tuned = grid.fit(X_train, Y_train)\n\n    print (\"Best score: \", tuned.best_score_) \n    print (\"Best params: \", tuned.best_params_)\n    print (\"IS Score: \", tuned.score(X_train, Y_train)) \n    \n    return tuned","6a279cf6":"param_grid = { \n    \"learning_rate\":  [0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1],\n    \"n_estimators\": [32, 64, 100, 200, 400, 500],\n}\n\ngbc = ensemble.GradientBoostingClassifier()\ngbc_tuned = get_tuned_model(gbc, param_grid, \"accuracy\", train[cols], train[['Survived']].values.ravel())","f6104c72":"# save_results(gbc_tuned, test[cols]) # Uncomment whichever model you want to use","ba517f5a":"param_grid = { \n    \"criterion\" : [\"gini\", \"entropy\"], \n    \"min_samples_leaf\" : [1, 5, 10], \n#     \"min_samples_split\" : [2, 4, 10, 12, 16],\n#     \"n_estimators\": n_estimators,\n    'max_leaf_nodes': range(4,20)\n}\n\nforest = RandomForestClassifier()\nft_tuned = get_tuned_model(forest, param_grid, \"accuracy\", train[cols], train[['Survived']].values.ravel())\n# Best score:  0.8293955181721173\n# Best params:  {'criterion': 'gini', 'max_leaf_nodes': 14, 'min_samples_leaf': 1}\n# IS Score:  0.856341189674523","0771df6e":"save_results(ft_tuned, test[cols])","83f27789":"logit = LogisticRegression()\n\nparam_grid = {'penalty': ['l1','l2'], \n              'C': [0.001,0.01,0.1,1,10,100],\n              'max_iter': [100,200,300,500]\n             }\n\nlog_tuned = get_tuned_model(logit, param_grid, \"accuracy\", train[cols], train[['Survived']].values.ravel())","822134cb":"# save_results(log_tuned, test[cols])","622f7f24":"### Random Forest Classifier","32b60739":"### Logistic Regression ","dbb30aae":"## Data Exploration\nWhat does the data look like? How does each feature relate to survivorship?","350ae70c":"## Data Preprocessing\n","3844506d":"## Modelling","62a6148f":"### Gradient Boost ","726c7c6d":"### Feature Selection"}}