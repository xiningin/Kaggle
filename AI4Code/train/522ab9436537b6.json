{"cell_type":{"3cbc5256":"code","7ed44953":"code","1c06c5c4":"code","e318fe94":"code","6247fbac":"code","bb9eb7b4":"code","9ddcad28":"code","fb21cd1c":"code","916ce7ba":"code","3f875392":"code","4341bf9f":"code","eab02fd5":"code","dc490c48":"code","8b70a391":"code","5dc234ae":"code","ff366d8d":"code","4213c183":"code","4991a141":"code","70030cca":"code","49e6798a":"code","5dad7620":"code","4710687f":"code","e8c8a92f":"code","cc3ffc86":"code","0f88781e":"code","0becd831":"code","fda83f31":"code","51ca6fc6":"code","b4b4a657":"markdown","f0deff57":"markdown","3b6ff892":"markdown","2fd8dcc6":"markdown","66440c72":"markdown","d9dd63e7":"markdown","09717c47":"markdown","6d55b100":"markdown","de80e461":"markdown","31106dee":"markdown"},"source":{"3cbc5256":"from __future__ import print_function, division\nimport os\nimport torch\nimport pandas as pd\nfrom skimage import io, transform\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport rasterio as rio\nfrom rasterio import features\nfrom pathlib import Path\nimport pathlib\nimport geopandas as gpd\nfrom descartes import PolygonPatch\nfrom PIL import Image\nimport itertools\nimport re\nfrom tqdm.notebook import tqdm\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\n\nimport imgaug\nimport random\n\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nplt.ion()   # interactive mode\n\n%matplotlib inline","7ed44953":"im_path = Path('..\/input\/spacenet-7-multitemporal-urban-development\/SN7_buildings_train_sample\/sample\/L15-0506E-1204N_2027_3374_13\/images\/global_monthly_2018_01_mosaic_L15-0506E-1204N_2027_3374_13.tif')","1c06c5c4":"with rio.open(im_path) as r:\n    print(r.read().shape)","e318fe94":"def set_seed(seed=0):\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    imgaug.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)","6247fbac":"random.randint(0,100)","bb9eb7b4":"class MultiTemporalSatelliteDataset(Dataset):\n    \"\"\"SpaceNet 7 Multi-Temporal Satellite Imagery Dataset\"\"\"\n    \n    def __init__(self,csv_file, root_dir, no_udm=True, transform=None, chip_dimension=None):\n        \"\"\"\n        Args:\n            csv_file (Path): Path to the csv file with annotations\n            root_dir (Path): Parent directory containing all other directories.\n            no_udm (bool): Specifies whether the dataset will load UDM images or not.\n            transform (callable, optional): Optional transform to be applied on a sample.\n            chip_dimension (int, optional): Specifies the dimensions of the chip being generated.\n        \"\"\"\n        self.annotations = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.no_udm = no_udm\n        self.transform = transform\n        self.idx_combinations = self.__get_all_idx_combinations()\n        self.chip_dimension = chip_dimension\n        if self.chip_dimension is not None:\n            self.chip_generator = self.__ChipGenerator(chip_dimension = self.chip_dimension)\n            # this will be replaced later with an abstracted version\n            # returns number of chips per image assuming all images are 1024\n            self.n_chips = ((1024 - 1) \/\/ self.chip_dimension + 1)**2\n            \n    def __len__(self):\n        if self.chip_dimension is not None:\n            return len(self.idx_combinations)*self.n_chips\n        else:\n            return len(self.idx_combinations)\n    \n    def __getitem__(self,idx):\n        if self.chip_dimension is not None:\n            raster_idx = idx\/\/self.n_chips\n            chip_idx = idx%self.n_chips\n        else:\n            raster_idx = idx\n            \n        if torch.is_tensor(raster_idx):\n            raster_idx = raster_idx.tolist()\n        # get the indices of the 2 images\n        idx1,idx2 = self.idx_combinations[raster_idx]\n        # paths where the images are stored\n        img1_path = self.root_dir\/self.annotations.loc[idx1,'images_masked']\n        img2_path = self.root_dir\/self.annotations.loc[idx2,'images_masked']\n        # paths where the corresponding true building footprints \n        labels1_path = self.root_dir\/self.annotations.loc[idx1,'labels_match_pix']\n        labels2_path = self.root_dir\/self.annotations.loc[idx2,'labels_match_pix']\n        # read rasters using imported rasterio library\n        with rio.open(img1_path) as r1, rio.open(img2_path) as r2:\n            raster1 = r1.read()[0:3]  \n            raster2 = r2.read()[0:3]\n        # get the concatenated array of the 2 images that will be fed into the neural_net\n        raster_diff = np.concatenate((raster1,raster2),axis=0)\n        # get the dates for the images\n        date1 = tuple(self.annotations.loc[idx1,['month','year']])\n        date2 = tuple(self.annotations.loc[idx2,['month','year']])\n        # read geojson files for each of the satellite images into a geodataframe\n        gdf1 = gpd.read_file(labels1_path).set_index('Id').sort_index()\n        gdf2 = gpd.read_file(labels2_path).set_index('Id').sort_index()\n        # get the change between the 2 satellite images by comparing their polygons\n        gdf_diff = self.__geo_difference(labels1_path,labels2_path)\n        # get the corresponding rasterized image of the geodataframes\n        mask_diff = self.__rasterize_gdf(gdf_diff,out_shape=raster1.shape[1:3])\n        \n        if self.chip_dimension:\n            raster_diff_dict = self.chip_generator(raster_diff)\n            mask_diff_dict = self.chip_generator(mask_diff)\n\n            sample = {'raster_diff':raster_diff_dict['chip'][chip_idx],'date1':date1,'date2':date2,\n          'mask_diff':mask_diff_dict['chip'][chip_idx],'im_dir':str(img1_path.parent.parent),'blank_label':mask_diff_dict['blank'][chip_idx]}\n        \n        else:\n            sample = {'raster_diff':raster_diff,'date1':date1,'date2':date2,'mask_diff':mask_diff,'im_dir':str(img1_path.parent.parent)}\n        \n        if self.transform is not None:\n            # get the individual images and mask from the output sample\n            raster1 = np.moveaxis(np.uint8(sample['raster_diff'][:3]),0,-1)\n            raster2 = np.moveaxis(np.uint8(sample['raster_diff'][3:6]),0,-1)\n            mask = np.moveaxis(np.uint8(sample['mask_diff']),0,-1)\n            \n            seed = random.randint(0,1000)\n            set_seed(seed)\n            \n            # apply transform on first image and mask\n            transformed = self.transform(image=raster1,mask=mask)\n            raster1 = transformed['image']\n            mask_diff = transformed['mask']\n            \n            set_seed(seed)\n            \n            # apply transform on second image\n            raster2 = self.transform(image=raster2)['image']\n            # concatenate input images\n            raster_diff = torch.cat((raster1,raster2))\n            # update sample dictionary paramters after transformation\n            if not isinstance(raster_diff,np.ndarray):\n                sample['raster_diff'] = raster_diff\n                mask_diff = mask_diff.permute(2,0,1)\n                sample['mask_diff'] = mask_diff\n            else:\n                sample['raster_diff'] = raster_diff\n                mask_diff = np.moveaxis(mask_diff,-1,0)\n                sample['mask_diff'] = mask_diff\n            \n        return sample\n    \n    def __get_all_idx_combinations(self):\n        all_combinations = []\n        # group by satellite image location\n        location_groups = self.annotations.groupby('image_dir_name')\n        # loop through the groups and get the different index combinations\n        for i,location in enumerate(location_groups):\n            # get the dataframe in the group\n            loc_frame = location[1]\n            # make sure that list does not contain images with unidentified masks\n            condition = (loc_frame['has_udm'] == False)\n            # return a list of the indices in the location dataframe\n            l = list(loc_frame[condition].index)\n            # use itertools to get all the different combinations between 2 in the list\n            combinations = list(itertools.combinations(l,2))\n            all_combinations.extend(combinations)\n        return all_combinations\n        \n    def __geo_difference(self,geojson1,geojson2):\n        # read geojson into geodataframes\n        gdf1 = gpd.read_file(geojson1).set_index('Id').sort_index()\n        gdf2 = gpd.read_file(geojson2).set_index('Id').sort_index()\n\n        # get geodataframe lengths\n        len_1 = len(gdf1)\n        len_2 = len(gdf2)\n        # check which gdf is longer\n        len_diff = abs(len_2-len_1)\n\n        if len_2 > len_1:\n            start_index = len_2-len_diff\n            diff_gdf = gdf2.iloc[start_index:].copy()\n        else:\n            start_index = len_1-len_diff\n            diff_gdf = gdf1.iloc[start_index:].copy()\n\n        # reset the index\n        diff_gdf.reset_index(inplace=True,drop=True)\n\n        return diff_gdf\n\n    \n    def __rasterize_gdf(self,gdf,out_shape):\n        # if geodataframe is empty return empty mask\n        if len(gdf)==0:\n            return np.zeros((1,*out_shape))\n            \n        mask = features.rasterize(((polygon, 255) for polygon in gdf['geometry']),out_shape=out_shape)\n        \n        return np.expand_dims(mask,axis=0)\n    \n    class __ChipGenerator():   \n        def __init__(self, chip_dimension=256,return_raster=False):  \n            self.chip_dimension = chip_dimension\n            self.return_raster = return_raster\n            self.chip_dict = {'chip':[],'x':[],'y':[], 'blank':[]}\n\n        def __call__(self,raster):\n            np_array = self.__read_raster(raster)\n            # get number of chips per colomn\n            n_rows = (np_array.shape[1] - 1) \/\/ self.chip_dimension + 1\n            # get number of chips per row\n            n_cols = (np_array.shape[2] - 1) \/\/ self.chip_dimension + 1\n            # segment image into chips and return dict of chips and metadata\n            chip_dict = {'chip':[],'x':[],'y':[], 'blank':[]}\n\n            for r in range(n_rows):\n                for c in range(n_cols):\n                    start_r_idx = r*self.chip_dimension\n                    end_r_idx = start_r_idx + self.chip_dimension\n\n                    start_c_idx = c*self.chip_dimension\n                    end_c_idx = start_c_idx + self.chip_dimension\n                    \n                    chip = np_array[:,start_r_idx:end_r_idx,start_c_idx:end_c_idx]\n\n                    chip_dict['chip'].append(chip)\n                    chip_dict['x'].append(start_r_idx)\n                    chip_dict['y'].append(start_c_idx)\n                    \n                    # Check if the chip is an empty chip\n                    if chip.mean() == 0 and chip.sum() == 0:\n                        chip_dict['blank'].append(1)\n                    else:\n                        chip_dict['blank'].append(0)\n\n            return chip_dict\n\n        def __read_raster(self,raster):\n            # check whether raster is a path or array\n            if isinstance(raster,(pathlib.PurePath,str)):\n                    with rio.open(raster) as r:\n                        # convert raster into np array\n                        np_array = r.read()\n                    return np_array\n\n            elif isinstance(raster,np.ndarray):\n                return raster\n            else:\n                raise ValueError(f\"Expected Path or Numpy array received: {type(raster)}\")  ","9ddcad28":"''' ToDo:\nNeed to update function below to make it more similar to torchvision.utils.make_grid() method\nThe final output will be 3 images:\nThe stacked before images\nThe stacked after images\nThe stacked difference images\n'''\ndef plot_sample(d,dpi=300,show=True):\n    # convert torch tensor to numpy array\n    if isinstance(d['raster_diff'],torch.Tensor):\n        raster_diff = d['raster_diff'].numpy()\n        mask_diff = d['mask_diff'].numpy()\n    else:\n        raster_diff = d['raster_diff']\n        mask_diff = d['mask_diff']\n        \n    # make sure channels are in the correct order for plotting\n    if d['raster_diff'].shape[0] <= 6:\n        image1 = np.moveaxis(raster_diff[0:3],0,-1)\n        image2 = np.moveaxis(raster_diff[3:6],0,-1)\n        mask_diff = np.moveaxis(mask_diff,0,-1).squeeze()\n    else:\n        image1 = d['raster_diff'][0:3]\n        image2 = d['raster_diff'][3:6]\n        \n    images = [image1,image2,mask_diff]\n    \n    mpl.rcParams['figure.dpi'] = dpi\n    ncols = 3\n    fig,axs = plt.subplots(1,ncols,figsize=(10,10))\n    fig.tight_layout()\n    plt.tick_params(axis = 'both', which = None, bottom = None, top = None) \n    \n    date1 = d['date1']\n    date2 = d['date2']\n    titles = [d['date1'],d['date2'],'difference']\n    \n    \n\n    for n in range(ncols):\n        axs[n].set_title(titles[n])\n        axs[n].imshow(images[n])\n    \n    if show:\n        plt.show()\n    else:\n        return axs","fb21cd1c":"root_dir = Path('..\/input\/spacenet-7-multitemporal-urban-development\/SN7_buildings_train\/train')\ncsv_file = Path('..\/input\/spacenet-7-directory-metadata-extraction\/output_csvs\/df_train_untidy.csv')","916ce7ba":"df = pd.read_csv(csv_file)","3f875392":"df.head()","4341bf9f":"train_set = MultiTemporalSatelliteDataset(root_dir=root_dir,csv_file=csv_file)","eab02fd5":"plot_sample(train_set[6])","dc490c48":"train_set_512 = MultiTemporalSatelliteDataset(root_dir=root_dir,csv_file=csv_file,chip_dimension=512)","8b70a391":"plot_sample(train_set_512[24])","5dc234ae":"plot_sample(train_set_512[25])","ff366d8d":"class ToTensor(object):\n    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n\n    def __call__(self, sample):\n        raster_diff, mask_diff = sample['raster_diff'], sample['mask_diff']\n        sample['raster_diff'] = torch.from_numpy(raster_diff)\n        sample['mask_diff'] = torch.from_numpy(mask_diff)\n        \n        return sample","4213c183":"class AsImage(object):\n    \"\"\"Convert shape of image from (Channels,Rows,Columns) to (Rows,Column,Channels).\"\"\"\n\n    def __call__(self, sample):\n        raster_diff, mask_diff = sample['raster_diff'], sample['mask_diff']\n        sample['raster_diff'] = torch.from_numpy(raster_diff)\n        sample['mask_diff'] = torch.from_numpy(mask_diff)\n        \n        return sample","4991a141":"import albumentations as A\nfrom albumentations.pytorch import ToTensorV2","70030cca":"chip_dimension = 64","49e6798a":"transform = A.Compose(\n    [\n        A.PadIfNeeded(min_height=chip_dimension,min_width=chip_dimension,value=0,p=1),\n        A.RandomRotate90(p=1.0),\n        ToTensorV2()\n    ]\n)","5dad7620":"train_set_64 = MultiTemporalSatelliteDataset(root_dir=root_dir,csv_file=csv_file,chip_dimension=chip_dimension,transform=transform)","4710687f":"plot_sample(train_set_64[328])","e8c8a92f":"plot_sample(train_set_64[328])","cc3ffc86":"plot_sample(train_set_64[328])","0f88781e":"dataloader = DataLoader(train_set_64, batch_size=8,\n                        shuffle=True, num_workers=0)","0becd831":" batch = next(iter(dataloader))","fda83f31":"batch['raster_diff'].shape","51ca6fc6":"batch","b4b4a657":"### Paths to Directories and Files\nThe csv file that we are going to be using to load the data into our dataset was [created in one of our previous notebooks](https:\/\/www.kaggle.com\/amerii\/spacenet-7-directory-metadata-extraction). The data is formatted in such a way that we can easily access the files we want.","f0deff57":"## Importing Dependencies","3b6ff892":"# Transforms (Work in Progress)","2fd8dcc6":"## Utilizing our Dataset Object","66440c72":"# DataLoader (Work in Progress)","d9dd63e7":"```\nA.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\nA.Rotate(limit=(-360, 360), interpolation=4, border_mode=4,p=0),\n```","09717c47":"# [Dataset class](https:\/\/pytorch.org\/tutorials\/beginner\/data_loading_tutorial.html#dataset-class)\n\n## (Work in Progress)\n\n`torch.utils.data.Dataset` is an abstract class representing a dataset. Your custom dataset should inherit Dataset and override the following methods:\n\n* `__len__` so that `len(dataset)` returns the size of the dataset.\n* `__getitem__` to support the indexing such that `dataset[i]` can be used to get *ith* sample\n\nLet\u2019s create a dataset class for our dataset. We will read the [csv that we created in our previous notebook](https:\/\/www.kaggle.com\/amerii\/spacenet-7-directory-metadata-extraction), in `__init__` but leave the reading of images to `__getitem__`. This is memory efficient because all the images are not stored in the memory at once but read as required.\n\n### Retrieving the Satellite Image Samples\n\nThe sample that we are going to retrieve from our dataset will be retrieved in one of the 2 following ways:\n\n* image_diff: The concatenated array of image1 and image2 that will be fed into our neural network\n* date1 and date2: The dates that each of the satellite images were captured on\n* im_dir: The parent image directory name\n* mask_diff: The rasterized image of the change between the two ground truth polygons from both images\n* blank_label: Boolean, indicating whether the output mask has a change or not. \n\n\n\n```sample = {'image_diff':image_diff_dict['chip'][chip_idx],\n             'mask_diff':mask_diff_dict['chip'][chip_idx]\n             'date1':date1,'date2':date2,\n             'im_dir':img1_path.parent.parent,\n             'blank_label':mask_diff_dict['blank'][chip_idx]}```\n\nOur dataset will take an optional argument `transform` so that any required processing can be applied on the sample.\n\nOur dataset will also take another optional argument `chip_dimension` which specifies the dimensions of the output chips incase we want to segment our satellite image into smaller chips.\nThe chip dimension will be fed into our `ChipGenerator` Class, which creates chips out of the input satellite image.","6d55b100":"# Schedular (Work in Progress)","de80e461":"In our [previous notebook](https:\/\/www.kaggle.com\/amerii\/spacenet-7-helper-functions) we programmed a few helper functions that made our lives much easier. Let's see how we are going to use those functions now to create our dataset class.","31106dee":"# Sampler (Work in Progress)"}}