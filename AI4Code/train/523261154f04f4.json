{"cell_type":{"f7948b8e":"code","abe451c3":"code","3bc1762b":"code","64fa4987":"code","8c66f201":"code","299b20e2":"code","e90fecda":"code","048f5e71":"code","0cb227bc":"code","27234642":"code","c77d2ea1":"code","383dff4e":"code","c24dd3af":"code","8efa60c4":"code","87830ab0":"code","62dc69c3":"code","11661188":"code","16f47110":"code","53dbbda2":"code","b39f7202":"code","e9a5418c":"code","89d9adf4":"code","0615d834":"markdown","60960641":"markdown","bfd7de1d":"markdown","428437d5":"markdown","bffbe6f7":"markdown","b5824989":"markdown","dfc53aec":"markdown","d96fb30e":"markdown","ea2a1c7f":"markdown","84976a31":"markdown","82073e5b":"markdown","0e71bc60":"markdown","7fd65063":"markdown","37175bf9":"markdown","06258a5c":"markdown","92e4b4ef":"markdown","1d93d67f":"markdown","4f5c2c03":"markdown","39046c15":"markdown","e73e9d69":"markdown","409082d3":"markdown","1bad95b6":"markdown","a412bf26":"markdown","1f728986":"markdown","03030b06":"markdown","2e2db40f":"markdown","24c5da1c":"markdown","e32db3c3":"markdown","c99250ef":"markdown","4fcd6e09":"markdown","b644dec2":"markdown","8b0c8f47":"markdown","7e6277a5":"markdown","2030f77c":"markdown","c325c621":"markdown","53ba570a":"markdown"},"source":{"f7948b8e":"# Importing standard packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport copy\nfrom typing import Callable, List, Dict, Tuple\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.utils.validation import check_X_y, check_array\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report","abe451c3":"# Import dataset\ndf = pd.read_csv('..\/input\/sms-spam-collection-dataset\/spam.csv')\n# Display dataframe\ndf","3bc1762b":"# Remove unneccesary columns\ndf = df.iloc[:,:2]","64fa4987":"df","8c66f201":"#\u00a0To make dataset binary, change ham 0 and spam to 1\ndf = df.replace(\"ham\", 0)\ndf = df.replace(\"spam\", 1)","299b20e2":"# Check for Nulls\ndf.info()","e90fecda":"# Check for NaNs \ndf.isna().sum()","048f5e71":"# Check for duplicates\ndf.duplicated().sum()","0cb227bc":"# Drop duplicates\ndf = df.drop_duplicates().reset_index(drop=True)","27234642":"df","c77d2ea1":"# Bar chart of class ratio \ntarget_pd = pd.DataFrame(index = [\"Not Spam\",\"Spam\"], columns= [\"Quantity\", \"Percentage\"])\n# Not spam\ntarget_pd.loc[\"Not Spam\"][\"Quantity\"] = len(df[df.columns[0]][df[df.columns[0]]==0].dropna())\ntarget_pd.loc[\"Not Spam\"][\"Percentage\"] = target_pd.iloc[0,0]\/len(df[df.columns[0]])*100\n# Spam\ntarget_pd.loc[\"Spam\"][\"Quantity\"] = len(df[df.columns[0]][df[df.columns[0]]==1].dropna())\ntarget_pd.loc[\"Spam\"][\"Percentage\"] = target_pd.iloc[1,0]\/len(df[df.columns[0]])*100\n# Plot barchart\nfig = plt.figure(figsize = (10, 5))\nplt.bar(list(target_pd.index), target_pd.iloc[:,0], color =[\"maroon\", \"blue\"], width = 0.4)\nplt.ylabel(\"Number of cases\")\nplt.title(\"Distribution of disease and non-disease cases\");\n# Print the dataframe\ntarget_pd","383dff4e":"#\u00a0Create X (emails) and y (binary target) dataset\nX = df[df.columns[-1]]\ny = df[df.columns[:-1]]","c24dd3af":"# Split the dataset into training and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1\/4, random_state=42, stratify=y)\n# Re-index\nX_train = X_train.reset_index(drop=True) \ny_train = y_train.reset_index(drop=True) \nX_test = X_test.reset_index(drop=True) \ny_test = y_test.reset_index(drop=True) ","8efa60c4":"def vocab_list(X: pd.Series) -> List[str]:\n    \n    \"\"\" Returns every word in the dataset on it's own as a string in a list. \"\"\"\n    \n    #\u00a0Create copy to not override X dataframe\n    X_copy = X.copy(deep=True)\n    for i in range(len(X_copy)):\n        X_copy[i] = X[i].split()\n    #\u00a0Flatten the list of lists \n    vocabs = [vocab for sublist in X_copy.tolist() for vocab in sublist]\n    # Remove duplicates \n    vocabs = [i for n, i in enumerate(vocabs) if i not in vocabs[:n]]\n    return vocabs","87830ab0":"def word_count(X: pd.Series, y: pd.Series, vocab_list: List, show_X: bool) -> Tuple:\n    \n    \"\"\" Return word count of email dataset. \"\"\"\n    \n    #\u00a0Convert a collection of text documents to a matrix of token counts\n    vectorizer = CountVectorizer(vocabulary=vocab_list)\n    word_counts = vectorizer.fit_transform(X.tolist()).toarray()\n    df = pd.DataFrame(word_counts, columns=vocab_list)\n    #\u00a0Function to transform new test data into word count matrix\n    msg_tx_func = lambda x: vectorizer.transform(x).toarray()\n    if show_X:\n        display(df)\n    return df.to_numpy(), np.array(y), msg_tx_func","62dc69c3":"def prior_lh(X: pd.Series, y: pd.Series) -> Tuple:\n    \n    \"\"\" Use training data for Naive Bayes classifier. \"\"\"\n\n    n = X.shape[0]\n    # Re-order X as a 2-dimensional array; each dimension contains data examples of only one of our two classes i.e. X_by_class[0] = non-spam and X_by_class[1] = spam\n    X_by_class = np.array([X[y==c] for c in np.unique(y)])\n    # Define prior\n    prior = np.array([len(X_class)\/n for X_class in X_by_class]) \n    # Count words in each class\n    word_counts = np.array([sub_arr.sum(axis=0) for sub_arr in X_by_class])\n    # Define likelihood\n    lh_word = word_counts \/ word_counts.sum(axis=1).reshape(-1, 1)\n    return prior, lh_word","11661188":"def posterior(X: pd.DataFrame, prior: np.array, lh_word: np.array) -> np.array:\n    \n    \"\"\" Predict probability of class. \"\"\"\n    \n    # Loop over each observation to calculate conditional probabilities\n    class_numerators = np.zeros(shape=(X.shape[0], prior.shape[0]))\n    for i, x in enumerate(X):\n        # Count how often words appear in each email\n        word_exists = x.astype(bool)\n        # Compute likelihoods of words (probability of data appearing in any class)\n        lh_words_present = lh_word[:, word_exists] ** x[word_exists]\n        # Compute likelihood of entire message with likelihoods of words\n        lh_message = (lh_words_present).prod(axis=1)\n        # Combine likelihood and prior to numerator\n        class_numerators[i] = lh_message * prior ## \n    normalize_term = class_numerators.sum(axis=1).reshape(-1, 1)\n    posteriors = class_numerators \/ normalize_term\n    if not (posteriors.sum(axis=1) - 1 < 1e-5).all():\n        raise ValueError('Rows should sum to 1')\n    return posteriors","16f47110":"def predict(X: np.array, y: np.array) -> np.array:\n    \n    \"\"\" Predict class with highest probability. \"\"\"\n    \n    y_pred  = posterior(X, y).argmax(axis=1)\n    return y_pred","53dbbda2":"class NaiveBayesClassifier():\n    \n    def __init__(self):\n    \n        \"\"\" Initialise parameters. \"\"\"\n       \n        self._vocab_list = None\n        self._word_count = None\n        self._prior = None\n        self._lh = None\n        self._posteriors = None\n        \n    def fit(self, X: pd.Series, y: pd.Series) -> np.array:\n    \n        \"\"\" Fit Naive bayes model. \"\"\"\n\n        #\u00a0Allocate initialised parameters\n        self._vocabs = self.vocab_list(X)\n        self._word_matrix = self.word_count(X, y, self._vocabs, show_X=False)[0]\n        self._prior = self.prior_lh(self._word_matrix, y)[0]\n        self._lh = self.prior_lh(self._word_matrix, y)[1]\n        self._posteriors = self.posterior(self._word_matrix, y, self._prior, self._lh)\n    \n    def predict(self) -> np.array:\n    \n        \"\"\" Predict class with highest probability. \"\"\"\n\n        y_pred = self._posteriors.argmax(axis=1)\n        return y_pred\n    \n        \n    def vocab_list(self, X: pd.Series) -> List[str]:\n    \n        \"\"\" Returns every word in the dataset on it's own as a string in a list. \"\"\"\n\n        #\u00a0Create copy to not override X dataframe\n        X_copy = X.copy(deep=True)\n        for i in range(len(X_copy)):\n            X_copy[i] = X[i].split()\n        #\u00a0Flatten the list of lists \n        vocabs = [vocab for sublist in X_copy.tolist() for vocab in sublist]\n        # Remove duplicates \n        vocabs = [i for n, i in enumerate(vocabs) if i not in vocabs[:n]]\n        return vocabs\n\n    def word_count(self, X: pd.Series, y: pd.Series, vocab_list: List, show_X: bool) -> Tuple:\n    \n        \"\"\" Return word count of email dataset. \"\"\"\n\n        #\u00a0Convert a collection of text documents to a matrix of token counts\n        vectorizer = CountVectorizer(vocabulary=vocab_list)\n        word_counts = vectorizer.fit_transform(X.tolist()).toarray()\n        df = pd.DataFrame(word_counts, columns=vocab_list)\n        #\u00a0Function to transform new test data into word count matrix\n        msg_tx_func = lambda x: vectorizer.transform(x).toarray()\n        if show_X:\n            display(df)\n        return df.to_numpy(), np.array(y), msg_tx_func\n    \n    def prior_lh(self, word_matrix: pd.DataFrame, y: pd.Series) -> Tuple:\n\n        \"\"\" Use training data for Naive Bayes classifier. \"\"\"\n        \n        n = X.shape[0]\n        # Re-order X as a 2-dimensional array; each dimension contains data examples of only one of our two classes i.e. X_by_class[0] = non-spam and X_by_class[1] = spam\n        X_no_spam = np.array(word_matrix[y[y==0].dropna().index.tolist(),:])\n        X_spam = np.array(word_matrix[y[y==1].dropna().index.tolist(),:])\n        X_by_class = np.array([X_no_spam, X_spam], dtype= \"object\")\n        # Define prior\n        prior = np.array([len(X_class)\/n for X_class in X_by_class]) \n        # Count words in each class\n        word_counts = np.array([sub_arr.sum(axis=0) for sub_arr in X_by_class])\n        # Define likelihood\n        lh_word = word_counts \/ word_counts.sum(axis=1).reshape(-1, 1)\n        return prior, lh_word\n    \n    def posterior(self, word_matrix: pd.DataFrame, y: pd.Series, prior: np.array, lh_word: np.array) -> np.array:\n    \n        \"\"\" Predict probability of class. \"\"\"\n\n         # Loop over each observation to calculate conditional probabilities\n        class_numerators = np.zeros(shape=(word_matrix.shape[0], prior.shape[0]))\n        for i, x in enumerate(word_matrix):\n            # Count how often words appear in each email\n            word_exists = x.astype(bool)\n            # Compute likelihoods of words (probability of data appearing in any class)\n            lh_words_present = lh_word[:, word_exists] ** x[word_exists]\n            # Compute likelihood of entire message with likelihoods of words\n            lh_message = (lh_words_present).prod(axis=1)\n            # Combine likelihood and prior to numerator\n            class_numerators[i] = lh_message * prior ## \n        normalize_term = class_numerators.sum(axis=1).reshape(-1, 1)\n        posteriors = class_numerators \/ normalize_term\n        if not (posteriors.sum(axis=1) - 1 < 1e-3).all():\n            raise ValueError('Rows should sum to 1')\n        return posteriors","b39f7202":"# Instantiate training model\nspam_model_train = NaiveBayesClassifier()\n# Fit model to training dataset to obtain Bayes Theorem components\nspam_model_train.fit(X_train, y_train)\n# Return training predictions\ny_pred_train = spam_model_train.predict()","e9a5418c":"# Print train confusion matrix\ncm_train = confusion_matrix(y_train, y_pred_train)\nax = sns.heatmap(cm_train, annot=True, fmt=\".1f\")\nax.set(xlabel=\"Predicted Labels\", ylabel=\"True Labels\");","89d9adf4":"# Print train metric report\npd.DataFrame(classification_report(y_train, y_pred_train, output_dict=True))","0615d834":"<p> <center> This notebook is in <span style=\"color: green\"> <b> Active <\/b> <\/span> state of development! <\/center> <\/p>  \n<p> <center> Be sure to checkout my other notebooks for <span style=\"color: blue\"> <b> knowledge, insight and laughter <\/b> <\/span>! \ud83e\udde0\ud83d\udca1\ud83d\ude02<\/center> <\/p> ","60960641":"The first job is to create a vocabulary list of all the possible words that are contained in the dataset.","bfd7de1d":"# Classification Prior & Likelihood","428437d5":"<hr style=\"height:2px;border-width:0;color:gray;background-color:gray\">","bffbe6f7":"# Classification Posterior","b5824989":"<hr style=\"height:2px;border-width:0;color:gray;background-color:gray\">","dfc53aec":"# Word Count","d96fb30e":"<center> <img src=\"https:\/\/www2.isye.gatech.edu\/isyebayes\/bank\/bayesfun.jpg\" width=\"625\" height=\"625\" \/> <\/center>","ea2a1c7f":"<center> <h1>\ud83d\udce9 Naive Bayes \ud83d\udce9<\/h1> <\/center>","84976a31":"# Vocabulary List","82073e5b":"# Model Testing and Results","0e71bc60":"Again, like most of the other datasets we have been dealing with in other notebooks, we have a highly imbalanced dataset, with more \"no spam\" emails than \"spam\". We will continue with our models, without using imbalanced classification techniques such as SMOTE, ADASYN etc...","7fd65063":"<hr style=\"height:2px;border-width:0;color:gray;background-color:gray\">","37175bf9":"# Summary","06258a5c":"For most machine learning models, we would like them to have low bias and low variance - that is, the model should perform well on the training set (low bias) and also the test set, alongside with other new random test sets (low variance). Therefore, to test for bias and variance of our model, we shall split the dataset into training and test set. We will not be tuning any hyperparameters (and thus do not need a validation set).  We will not be tuning any hyperparameters (and thus do not need a validation set). \n\nFor these functions, the $X$ dataset (of features) should have a column 1's as the first column to account for the bias term\/intercept co-efficient. Before this occurs, one should check the order of magnitude of the features - if they differ hugely, one must apply feature scaling. Having looked at the data however, we can see that since we have merely categorical features, there is no need for feature scaling. ","92e4b4ef":"Putting all of this together, we can now predict in a binary fashion by asserting any data points to the class with the highest probability. Here, we take our emails we trained our Naive Bayes classifier on also to evaluate it, but the evaluation normally happens on unseen emails.","1d93d67f":"# Aim","4f5c2c03":"<hr style=\"height:2px;border-width:0;color:gray;background-color:gray\">","39046c15":"- It is easy and fast to predict class of test data set. \n- When assumption of independence holds, a Naive Bayes classifier performs better compare to other models like logistic regression and you need less training data.\n- A limitation of Naive Bayes is the assumption of independent predictors. In real life, it is almost impossible that we get a set of predictors which are completely independent.\n- Another limitation is that if there is a category in the test data set that never appeared in the training set, then the probability output will always be 0 (which is not accurate).","e73e9d69":"# Data Processing","409082d3":"## Extra","1bad95b6":"Some comments about the code implementations:\n\n1. If dealing with arrays rather than dataframes, some of the functions may need altering to account for dimension\/shape issues e.g. the _prior lh_ and _posterior_ functions.  \n2. To debug this, it is important to print out the _word count_ and _X by class_ so you can check if you are obtaining the correct outputs. ","a412bf26":"Unlike our other datasets in the other notebooks, the target variable is in the first column (and not in the last column) and so our data processing will be slightly different. If you want, you can just switch the columns and then use code from the other notebooks to proceed with data processing. ","1f728986":"Now that we have the vocabulary list, we can now calculate the number of occurences of each word per sentence. We will use the help of **one** in-built sk-learn function for now (till we work out how to create our own one). ","03030b06":"<hr style=\"height:2px;border-width:0;color:gray;background-color:gray\">","2e2db40f":"## Import Modules","24c5da1c":"# Background","e32db3c3":"Naive Bayes is a classification algorithm based on Bayes' theorem. Bayes\u2019 theorem provides a way to calculate the probability of a data point belonging to a given class, given our prior knowledge. It is defined as\n\n$$\n\\mathbb P (class|data) = \\frac{\\mathbb P (data|class) \\ \\mathbb P (class)}{\\mathbb P (data)} ,\n$$\n\nwhere $\\mathbb P (class | data)$ is the probability over the potential classes given the provided data. The different probabilities $\\mathbb P$ you see in the equations above are commonly called prior, likelihood, evidence, and posterior as follows.\n\n$$\n\\overbrace{\\mathbb P (class|data)}^{\\text{posterior}} = \\frac{\\overbrace{\\mathbb P (data|class)}^{\\text{likelihood}} \\ \\overbrace{\\mathbb P (class)}^{\\text{prior}}}{\\underbrace{\\mathbb P (data)}_{\\text{evidence}}}\n$$\n\nThe algorithm is 'naive', because of its assumption that features of data are independent given the class label. This idea helps us simplify the likelihood event. Let us call the data features $x_1, \\dots, x_i, \\dots, x_n$ and the class label $y$, and rewrite Bayes theorem in these terms:\n\n$$\n\\mathbb P (y|x_1, \\dots, x_n) = \\frac{\\mathbb P (x_1, \\dots, x_n|y) * \\mathbb P (y)}{\\mathbb P (x_1, \\dots, x_n)} \\, . \n$$\n\nThen, the naive assumption of conditional independence between any two features given the class label can be expressed as\n\n$$\n\\mathbb P(x_i | y, x_1, \\dots, x_{i-1}, x_{i+1}, \\dots, x_n) = \\mathbb P (x_i | y) \\, .\n$$\n\nFor all $i$, we can simply write Bayes' theorem as:\n\n$$\n\\mathbb P (y | x_1, \\dots, x_n) = \\frac{\\mathbb P (y) \\prod_{i=1}^n \\mathbb P(x_i | y)}{\\mathbb P (x_1, \\dots, x_n)} \\, .\n$$\n\nSince $\\mathbb P (x_1, \\dots, x_n)$ is the constant input, we can define the following proportional relationship\n\n$$\n\\mathbb P (y|x_1, \\dots, x_n) \\propto \\mathbb P (y) \\prod_{i=1}^n \\mathbb P(x_i | y) \\, ,\n$$\n\nand can use it to classify any data point as\n\n$$\n\\hat y = \\underset{y}{\\text{arg max}} \\ \\mathbb P (y) \\prod_{i=1}^n \\mathbb P(x_i | y) \\, .\n$$\n\n**Note:** Naive Bayes can indeed be used for multiclass classification, however we use it here as a binary classifier._ ","c99250ef":"We compute the last part of the Bayes Theorem component, which is the posterior.","4fcd6e09":"Next, we train the Naive Bayes classifier, where we define the prior and likelihood. The prior is the probability distribution incorporating our knowledge of the data. Consequently, we use the available training set to define it.","b644dec2":"The aim is to provide, from scratch, code implementations for linear regression problems. This will involve both the main functions needed to solve a linear regression and some additional utility functions as well.\n\n**Note**: We will not be diving into in-depth exploratory data analysis, feature engineering etc... in these notebooks and so will not be commenting extensively on things such as skewness, kurtosis, homoscedasticity etc...","8b0c8f47":"# Full Naive Bayes Classifier Model","7e6277a5":"# Classification Prediction","2030f77c":"# Data Collection","c325c621":"## Splitting dataset","53ba570a":"Thanks for reading this notebook. If there are any mistakes or things that need more clarity, feel free to respond in the comment section and I will be happy to reply.\n\nAs always, please leave an upvote - it would also be helpful if you cite this documentation if you are going to use any of the code. \ud83d\ude0a\n\n#CodeWithSid"}}