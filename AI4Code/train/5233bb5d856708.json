{"cell_type":{"30d55005":"code","c538d4bb":"code","b4ba6752":"code","24f22c42":"code","acb76094":"code","e53ed824":"code","94a5b0c6":"code","c0890e53":"code","0a4ce36b":"code","9f453fe1":"code","a7ac5bf5":"code","759408df":"code","6c76c3b1":"code","e8afab5d":"code","bc3f411a":"code","c5dc614d":"code","2f6d39cf":"code","ff8190e3":"code","13410a37":"code","75d5f69b":"code","449c4e3b":"code","77ba6107":"code","a8df7ac9":"code","bf1eadf5":"code","f3958751":"code","90da2af6":"code","cb8c3521":"code","3b1ecc4b":"code","2cfa1b85":"code","4fee2e7f":"code","c9413d22":"code","c1c12aa8":"code","d053941a":"code","0b1a89cf":"code","c0e12bb5":"code","d9cbb84b":"code","419911e5":"code","caf685ae":"code","4488edf3":"code","74a706eb":"code","2419613f":"code","64037848":"markdown","4504604b":"markdown","365042ff":"markdown","0ba29a44":"markdown","e5944bd4":"markdown","e65a2ec4":"markdown","f655ea74":"markdown","1ac5a2c4":"markdown","8a43e878":"markdown"},"source":{"30d55005":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c538d4bb":"df=pd.read_csv(\"\/kaggle\/input\/heart-disease-uci\/heart.csv\")\ndf.head(2)","b4ba6752":"df.isna().sum()","24f22c42":"df.target.value_counts()","acb76094":"df.target.value_counts().plot(kind='bar',color=['orange','green'])","e53ed824":"df.info()","94a5b0c6":"df.describe()","c0890e53":"#Heart Disease with respect to Sex\npd.crosstab(df.sex,df.target).plot(kind=\"bar\",figsize=(10,6))\nplt.title(\"Heart Disease vs Sex\")\nplt.xlabel(\"0=No Disease & 1=Disease\")\nplt.ylabel(\"Count\")\nplt.legend([\"female\",\"Male\"]);","0a4ce36b":"#Age vs Max Heart rate(thalach)\nplt.figure(figsize=(10,6))\n\nplt.scatter(df.age[df.target==1],\n           df.thalach[df.target==1])\n\nplt.scatter(df.age[df.target==0],\n           df.thalach[df.target==0])\n\nplt.legend([\"Disease\",\"No Disease\"])\nplt.xlabel(\"Age\")\nplt.ylabel(\"Heart Rate\")\nplt.title(\"Age vs Heart Rate\");","9f453fe1":"pd.crosstab(df.cp,df.target).plot(kind=\"bar\",figsize=(10,6))\nplt.xlabel(\"Chest Pain Type\")\nplt.title(\"Chest Pain vs Target\")\nplt.ylabel(\"Count\")\nplt.legend([\" No Disease\",\"Disease\"])\nplt.xticks(rotation = 0);","a7ac5bf5":"df.corr()","759408df":"#Plot Correlation matrix using seaborn \ncorr_mat=df.corr()\nplt.figure(figsize=(15,10))\nsns.heatmap(corr_mat,\n           annot=True,\n           linewidth=0.5,\n           fmt= \".2f\");","6c76c3b1":"cp=pd.get_dummies(df['cp'],prefix='cp',drop_first=True)\nthal=pd.get_dummies(df['thal'],prefix='thal',drop_first=True)\nslope=pd.get_dummies(df['slope'],prefix='slope',drop_first=True)","e8afab5d":"new_df=pd.concat([df,cp,thal,slope],axis=1)\nnew_df=new_df.drop(['cp','thal','slope'],axis=1)\nnew_df.head()","bc3f411a":"X = new_df.drop(\"target\", axis=1)\n\n# Target variable\ny = new_df['target']\n\n#Normalize X\nX = (X - X.min())\/(X.max()-X.min())\n\n#Split data into training and test data\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=2)","c5dc614d":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score,RandomizedSearchCV,GridSearchCV\n","2f6d39cf":"#Store all models in a Dictionary\nnp.random.seed(42)\nmodels={'Logistic Regression':LogisticRegression(max_iter=1000),\n       'RandomForestClassifier':RandomForestClassifier(),\n        \"GradientBoostingClassifier\":GradientBoostingClassifier(),\n       \"Knn\":KNeighborsClassifier(),\n        \"GaussianNB\":GaussianNB(),\n       \"SVC\":SVC(kernel='linear')}\n#Store result in an empty dictionary\nscores={}\n\n#Write a function to fit,train and test a model and store in result dictionary\ndef fit_and_score(models,X_train,X_test,y_train,y_test):\n    for model_name,model in models.items():\n        model.fit(X_train,y_train)\n        scores[model_name]=model.score(X_test,y_test)\n    return scores","ff8190e3":"fit_and_score(models,X_train,X_test,y_train,y_test)","13410a37":"scores['Knn']","75d5f69b":"pd.DataFrame(scores,index=['accuracy']).T.plot(kind='bar',figsize=(10,6))\nplt.yticks(np.arange(0,1.1,0.05))\nplt.grid();","449c4e3b":"# 1. Let us tune our KNN model wuth different values of n_neighbors\ntrain_scores = []\n\n# Create a list of test scores\ntest_scores = []\n\n# Create a list of different values for n_neighbors\nneighbors = range(1, 21) # 1 to 20\n\n# Setup algorithm\nknn = KNeighborsClassifier()\n\n# Loop through different neighbors values\nfor i in neighbors:\n    knn.set_params(n_neighbors = i) # set neighbors value\n    \n    # Fit the algorithm\n    knn.fit(X_train, y_train)\n    \n    # Update the training scores\n    train_scores.append(knn.score(X_train, y_train))\n    \n    # Update the test scores\n    test_scores.append(knn.score(X_test, y_test))\n    \nplt.plot(neighbors, train_scores, label=\"Train score\")\nplt.plot(neighbors, test_scores, label=\"Test score\")\nplt.xticks(np.arange(1, 21, 1))\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Model score\")\nplt.legend()\n \nknn_score=max(test_scores)\nprint(f\"Maximum KNN score on the test data: {knn_score*100:.2f}%\")","77ba6107":"# 2. Let us tune our Logistic Regression and Random Forest model using RandomizedSearchCV\n#Logistic Regression hyperparameter grid\nlog_reg_grid={\"C\":np.logspace(-4,4,20),\n             \"solver\":['newton-cg','liblinear','sag','saga']}\n#RandomForest hyerparameter grid\nrf_grid={\"n_estimators\":np.arange(10,1000,50),\n        \"max_depth\":[None,1,2,3,5,10,15,22,25],\n        \"min_samples_split\":np.arange(2,40,2),\n        \"min_samples_leaf\":np.arange(1,40,2)}","a8df7ac9":"#Set up random hyperparameter search for Logistic Regression\nnp.random.seed(42)\nrs_log_reg=RandomizedSearchCV(LogisticRegression(),param_distributions=log_reg_grid,n_iter=100,cv=5,verbose=True)\n\nrs_log_reg.fit(X_train,y_train);","bf1eadf5":"#Best parameters for LogisticRegression Model\nrs_log_reg.best_params_","f3958751":"#Accuracy score for LogisticRegression Model\nrs_log_reg_score=rs_log_reg.score(X_test,y_test)\nrs_log_reg_score","90da2af6":"np.random.seed(42)\nrs_rf=RandomizedSearchCV(RandomForestClassifier(),param_distributions=rf_grid,n_iter=20,cv=5,verbose=True)\n\nrs_rf.fit(X_train,y_train)","cb8c3521":"rs_rf.best_params_","3b1ecc4b":"rs_rf_score=rs_rf.score(X_test,y_test)\nrs_rf_score","2cfa1b85":"clf_model=['Logistic Regression','RandomForestClassifier','GradientBoostingClassifier','Knn','GaussianNB','SVC']\naccuracy=[rs_log_reg_score,rs_rf_score,scores['GradientBoostingClassifier'],knn_score,scores['GaussianNB'],scores['SVC']]\nplt.bar(x=clf_model,height=accuracy)\nplt.title('Model Accuracy')\nplt.grid()\nplt.xticks(rotation=90)\nplt.yticks(np.arange(0,1.1,0.1));","4fee2e7f":"#Predicted values for y_test\ngnb=GaussianNB()\ngnb.fit(X_train,y_train)\ny_preds = gnb.predict(X_test)\ny_preds","c9413d22":"y_test","c1c12aa8":"#Plot ROC Curve\nfrom sklearn.metrics import plot_roc_curve,confusion_matrix,plot_confusion_matrix,classification_report\nplot_roc_curve(gnb,X_test,y_test)","d053941a":"#Plot confusion matrix\nconfusion_matrix(y_test,y_preds)","0b1a89cf":"plot_confusion_matrix(gnb,X_test,y_test)","c0e12bb5":"#Print Calssification Report\nprint(classification_report(y_test,y_preds))","d9cbb84b":"from sklearn.model_selection import cross_val_score\ncv_acc= cross_val_score(gnb,\n                        X,\n                        y,\n                        cv=5,  #5-fold cross validation\n                       scoring='accuracy') # scoring parameter as accuracy\n\ncv_acc","419911e5":"#Take mean of these values\ncv_acc=np.mean(cv_acc)\ncv_acc","caf685ae":"#Calculate rest of paramters\ncv_precision=np.mean(cross_val_score(gnb,\n                        X,\n                        y,\n                        cv=5,  \n                       scoring='precision'))\n\ncv_precision ","4488edf3":"cv_recall=np.mean(cross_val_score(gnb,\n                        X,\n                        y,\n                        cv=5,  \n                       scoring='recall'))\n\ncv_recall ","74a706eb":"cv_f1=np.mean(cross_val_score(gnb,\n                        X,\n                        y,\n                        cv=5,  \n                       scoring='f1'))\n\ncv_f1 ","2419613f":"# Let us visualize them\ncv_metrics=pd.DataFrame({'Accuracy':cv_acc,\n                        'Precision':cv_precision,\n                        'Recall':cv_recall,\n                        'F1-score':cv_f1},index=[0])\ncv_metrics.T.plot(kind='bar',legend=False)\nplt.title(\"Classification Metrics for GaussianNB\")\nplt.yticks(np.arange(0,1.1,0.1))\nplt.grid();","64037848":"# Compare Different Models","4504604b":"# Evaluating our Model\nWe will evaluate our model using \n * ROC Curve\n * Confusion Matrix\n * Classification Matrix\n * Precisin \n * Recall \n * F1-Score","365042ff":"# Let us try to tune our Models","0ba29a44":"# Introduction\n\nThe given dataset has data about patients if they have heart disease or not with respect to some given features. Our goal is to predict whether a person has heart disease or not based on given feature values. We will follow the following process to achieve this:\n* Import Data\n* Exploratory data analysis\n* Data Preprocessing\n* Model our Data\n* Compare different Models\n* Tune our Model(Hyperparameter tuning)\n* Evaluate our Data\n\nIf you find this helpful kindly do **Upvote**","e5944bd4":"# Dataset Columns (Features)\n\n *   age - age in years\n *   sex - (1 = male; 0 = female)\n *   cp - chest pain type\n      *  0: Typical angina: chest pain related decrease blood supply to the heart\n      *  1: Atypical angina: chest pain not related to heart\n      *  2: Non-anginal pain: typically esophageal spasms (non heart related)\n      *  3: Asymptomatic: chest pain not showing signs of disease\n *   trestbps - resting blood pressure (in mm Hg on admission to the hospital)\n        anything above 130-140 is typically cause for concern\n *   chol - serum cholestoral in mg\/dl\n        serum = LDL + HDL + .2 * triglycerides\n        above 200 is cause for concern\n *   fbs - (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n        '>126' mg\/dL signals diabetes\n *  restecg - resting electrocardiographic results\n       * 0: Nothing to note\n       * 1: ST-T Wave abnormality\n            can range from mild symptoms to severe problems\n            signals non-normal heart beat\n       * 2: Possible or definite left ventricular hypertrophy\n            Enlarged heart's main pumping chamber\n *   thalach - maximum heart rate achieved\n *   exang - exercise induced angina (1 = yes; 0 = no)\n *   oldpeak - ST depression induced by exercise relative to rest\n        looks at stress of heart during excercise\n        unhealthy heart will stress more\n *   slope - the slope of the peak exercise ST segment\n      *  0: Upsloping: better heart rate with excercise (uncommon)\n      *  1: Flatsloping: minimal change (typical healthy heart)\n      *  2: Downslopins: signs of unhealthy heart\n *   ca - number of major vessels (0-3) colored by flourosopy\n        colored vessel means the doctor can see the blood passing through\n        the more blood movement the better (no clots)\n *   thal - thalium stress result\n      *  1,3: normal\n      *  6: fixed defect: used to be defect but ok now\n      *  7: reversable defect: no proper blood movement when excercising\n *   target - have disease or not (1=yes, 0=no) (= the predicted attribute)\n\n","e65a2ec4":"# Lets Model our Data","f655ea74":"# Lets Do Exploratory Data Analysis","1ac5a2c4":"## Data Preprocessing\nData consist of categorical features. We need to convert them to dummy variables ","8a43e878":"From above graph we can see that **GaussianNB** performs best with **93.44%** accuracy"}}