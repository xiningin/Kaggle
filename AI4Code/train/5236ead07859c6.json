{"cell_type":{"8b44f6fe":"code","6a702ee9":"code","7baac0df":"code","e9d4029f":"code","36736ffd":"code","b2d04c4a":"code","63e50d79":"code","d746df8d":"code","40e92716":"code","4e70ae78":"code","4b39a0bc":"code","cdea58f4":"code","c606fc96":"code","936b6f3a":"code","ad70ee32":"code","61813bf0":"code","cdaeac26":"code","295b9336":"code","29d60652":"code","b4e28cc2":"code","f31596c3":"code","f2bcd228":"code","8328ef1a":"code","ea143388":"code","9b319d19":"code","6dd41369":"code","1d2b2fdc":"code","b827174e":"code","b4a05f9c":"code","d076debc":"code","3b03306d":"code","2d81b32e":"code","c776f7d6":"code","0eb7173f":"code","943c60f2":"code","c68aad11":"code","78bc55c3":"code","0adde2aa":"code","44628015":"code","cc22883c":"code","1d941ea5":"code","ed3c66d3":"code","6c9da6d5":"code","94c36c80":"code","24a0a81c":"code","b0a66ac6":"code","e9d408b5":"code","0f053119":"code","430ea864":"code","a7f3ede3":"code","cfc01020":"code","d4ed0477":"code","b95060b8":"code","ba9daa7d":"code","dc880a5b":"code","afa2a717":"code","4d0b557c":"code","2f30de03":"code","43ca4106":"code","a014ee62":"code","fb6901a1":"code","f7adcb7a":"markdown","e69bfa34":"markdown","29dbe41a":"markdown","a528283d":"markdown","de1292fd":"markdown","d8044037":"markdown","8ae9dbe3":"markdown","3b86c8c9":"markdown","e06c5b45":"markdown","037773bd":"markdown","5d6e3df7":"markdown","8c66ae52":"markdown","fc0675cb":"markdown","a67eaffe":"markdown","bb54dff6":"markdown","1c6e3956":"markdown","8e94a9be":"markdown","19e44061":"markdown","d2f4916b":"markdown","dd29ba97":"markdown","07d5d4c9":"markdown","60268d25":"markdown","660786f7":"markdown","b6e8315f":"markdown","637f7450":"markdown","39a7bd6d":"markdown","c33ab029":"markdown","72d88f3f":"markdown","536a8695":"markdown","7ae9b604":"markdown","6aa3e69f":"markdown","2d30f4b1":"markdown","fef12c96":"markdown","ff7aef8f":"markdown","fee71164":"markdown","0a1022ff":"markdown","83f31ffa":"markdown","cdb8a63b":"markdown","6afbff8c":"markdown","39993909":"markdown","bdcde162":"markdown","cd4b159b":"markdown","a8b62b43":"markdown","56c36c21":"markdown","04d1cf13":"markdown","34e10203":"markdown","1b386ce6":"markdown","89c1a29a":"markdown","58185bdd":"markdown","c164086e":"markdown","1217aae9":"markdown","a57359f7":"markdown","181b6d5e":"markdown"},"source":{"8b44f6fe":"import pandas as pd \nimport numpy as np\nimport lightgbm as lgb\nimport sklearn\nimport sys, os\nfrom sklearn.metrics import r2_score\nfrom catboost import Pool, CatBoostRegressor, cv\nfrom sklearn.model_selection import KFold \nfrom tqdm import tqdm_notebook\nfrom  sklearn.preprocessing import LabelEncoder\nfrom calendar import monthrange\nfrom itertools import product, chain \nimport gc\n\nimport re\nfrom bayes_opt import BayesianOptimization\nfrom bayes_opt.util import Colours\n\nimport matplotlib.pyplot as plt\npd.set_option('display.max_columns',500)\npd.set_option('display.max_rows',100)","6a702ee9":"sales = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\nshops = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/shops.csv')\nitems = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/items.csv')\nitem_cats = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\ntest = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv')","7baac0df":"plt.subplot(1,2,2)  ### outliers\nplt.plot(sales['item_price'])\nplt.legend('price')\n\nplt.subplot(1,2,1)\nplt.plot(sales['item_cnt_day'])\nplt.legend(('count'))","e9d4029f":"sales = sales[sales['item_price'] < 250000]\nsales = sales[sales['item_cnt_day'] < 1000]","36736ffd":"# \u042f\u043a\u0443\u0442\u0441\u043a \u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435, 56                            ## thanks to Denis Larionov ##\nsales.loc[sales.shop_id == 0, 'shop_id'] = 57\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\n# \u042f\u043a\u0443\u0442\u0441\u043a \u0422\u0426 \"\u0426\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u0439\"\nsales.loc[sales.shop_id == 1, 'shop_id'] = 58\ntest.loc[test.shop_id == 1, 'shop_id'] = 58\n# \u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439 \u0443\u043b. \u0427\u043a\u0430\u043b\u043e\u0432\u0430 39\u043c\u00b2\nsales.loc[sales.shop_id == 10, 'shop_id'] = 11\ntest.loc[test.shop_id == 10, 'shop_id'] = 11\n\nsales.loc[sales.shop_id == 39, 'shop_id'] = 40\ntest.loc[test.shop_id == 39, 'shop_id'] = 40","b2d04c4a":"sales['date'] = pd.to_datetime(sales['date'], format='%d.%m.%Y')\nsales['month'] = sales['date'].dt.month\nsales['year'] = sales['date'].dt.year\n\nsales = sales.drop('date', axis = 1)","63e50d79":"to_append = test[['shop_id', 'item_id']].copy()\n\nto_append['date_block_num'] = sales['date_block_num'].max() + 1\nto_append['year'] = 2015\nto_append['month'] = 11\nto_append['item_cnt_day'] = 0\nto_append['item_price'] = 0\n\nsales = pd.concat([sales, to_append], ignore_index=True, sort=False)","d746df8d":"holiday_dict = {\n    1: 7,\n    2: 4,\n    3: 3,\n    4: 9,\n    5: 4,\n    6: 4,\n    7: 3,\n    8: 9,\n    9: 5,\n    10: 9,\n    11: 6,\n    12: 5,\n}\n\nsales['holidays_in_month'] = sales['month'].map(holiday_dict)","40e92716":"period = sales[['date_block_num','year','month','holidays_in_month']].drop_duplicates().reset_index(drop=True)\nperiod['days'] = period.apply(lambda r: monthrange(r.year, r.month)[1], axis=1)","4e70ae78":"cities = shops['shop_name'].str.split(' ').map(lambda row : row[0])\ncities.unique()","4b39a0bc":"shops['shop_city'] = cities\nshops.loc[shops['shop_city'] == '!\u042f\u043a\u0443\u0442\u0441\u043a', 'shop_city' ] = '\u042f\u043a\u0443\u0442\u0441\u043a'\nls = LabelEncoder()\nshops['shop_city'] = ls.fit_transform(shops['shop_city'])\nshops['shop_type'] = shops['shop_name'].apply(lambda x: '\u041c\u0422\u0420\u0426' if '\u041c\u0422\u0420\u0426' in x \n                                              else '\u0422\u0420\u0426' if '\u0422\u0420\u0426' in x \n                                              else '\u0422\u0420\u041a' if '\u0422\u0420\u041a' in x \n                                              else '\u0422\u0426' if '\u0422\u0426' in x \n                                              else '\u0422\u041a' if '\u0422\u041a' in x \n                                              else 'NO_DATA')\nls1  = LabelEncoder()\nshops['shop_type'] = ls1.fit_transform(shops['shop_type'])","cdea58f4":"main_cat = item_cats['item_category_name'].str.split('-').map(lambda row : row[0].strip())\nmain_cat.unique()","c606fc96":"ls = LabelEncoder()\nitem_cats['main_cat'] = ls.fit_transform(main_cat)\n\nsub_cat = item_cats['item_category_name'].str.split('-').map(lambda row : row[1].strip()\n                                                             if len(row) > 1 \n                                                             else row[0].strip()) \nitem_cats['sub_cat'] = ls.fit_transform(sub_cat)","936b6f3a":"items['name_1'], items['name_2'] = items['item_name'].str.split('[', 1).str\nitems['name_1'], items['name_3'] = items['item_name'].str.split('(', 1).str\n\nitems['name_2'] = items['name_2'].str.replace('[^A-Za-z0-9\u0410-\u042f\u0430-\u044f]+', ' ').str.lower()\nitems['name_3'] = items['name_3'].str.replace('[^A-Za-z0-9\u0410-\u042f\u0430-\u044f]+', ' ').str.lower()\nitems.drop('name_1', axis = 1 , inplace=True)\nitems = items.fillna('0')\n\nls = LabelEncoder()\nitems['name_2'] = ls.fit_transform(items['name_2'])\nitems['name_3'] = ls.fit_transform(items['name_3'])","ad70ee32":"def name_correction(x):\n    x = x.lower()\n    x = x.partition('[')[0]\n    x = x.partition('(')[0]\n    x = re.sub('[^A-Za-z0-9\u0410-\u042f\u0430-\u044f]+', ' ', x)\n    x = x.replace('  ', ' ')\n    x = x.strip()\n    return x\n\nitems['item_name'] = items['item_name'].apply(lambda x: name_correction(x))\nitems.head()\nprint('Unique item names after correction:', len(items['item_name'].unique()))","61813bf0":"items = items.merge(item_cats.drop('item_category_name', axis=1),\n                    on = ['item_category_id'], how = 'left')\nitems = items[['item_name','item_id','item_category_id','main_cat','sub_cat','name_2','name_3']]","cdaeac26":"## helper functions ##\n\ndef downcast_dtypes(df):\n    '''\n        Changes column types in the dataframe: \n                \n                `float64` type to `float32`\n                `int64`   type to `int32`\n    '''\n    \n    # Select columns to downcast\n    float_cols = [c for c in df if df[c].dtype in [\"float64\"]]\n    int_cols =   [c for c in df if df[c].dtype in [\"int64\"]]\n    \n    # Downcast\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols]   = df[int_cols].astype(np.int32)\n    \n    return df\n\ndef position_colum(df,column_name):\n    '''\n        return the position of the column in the dataset \n    '''\n    assert column_name in df, \"Column {} not in the dataset\".format(column_name)\n    colums = df.columns\n    for i in range(len(colums)) :\n        if (column_name == colums[i]):\n            position = i\n        \n    return(position)\n\ndef mean_encoding_reg(df, col_to_enc,target_col, nb_split):\n    '''\n        return a new dataset where the categorical feature has been \n        encoded and replaced with KFOLD_mean_encoding\n        col_to_enc : name of the colums to encode\n    '''\n    col = [col_to_enc,target_col]\n    df_small = df[col]\n\n    target_mean_fold_enc = []\n\n    kf = KFold(n_splits=nb_split, shuffle=False)\n\n    for ind_tr, ind_val in kf.split(df_small):\n        mat_tr, mat_val = df_small.iloc[ind_tr], df_small.iloc[ind_val]\n    \n    \n        target_tr_mean_estimate = mat_tr.groupby(col_to_enc)[target_col].mean()\n        target_val_mean = df_small.loc[ind_val,col_to_enc].map(target_tr_mean_estimate)\n    \n        for mean in target_val_mean :\n            target_mean_fold_enc.append(mean)\n    \n    pos_categorical_feat = position_colum(df,col_to_enc)\n    new_df = df.copy()\n    new_df.insert(pos_categorical_feat,col_to_enc +'_mean_enc',pd.Series(target_mean_fold_enc).fillna(0))\n    \n    return ((downcast_dtypes(new_df)))","295b9336":"## GRID DATA ###\n\nfrom itertools import product, chain \nimport gc\n\nindex_cols = ['shop_id', 'item_id', 'date_block_num']\n\n# For every month we create a grid from all shops\/items combinations from that month\ngrid = [] \nfor block_num in sales['date_block_num'].unique():\n    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n\n# Turn the grid into a dataframe\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n\n# Groupby data to get shop-item-month aggregates == Target\ngb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'item_cnt_month':'sum'},\n                                                  'item_price' : {'item_price_month' : 'mean'}})\n# Fix column names\ngb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values] \n# Join it to the grid\nall_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)\n\n\nall_data['item_cnt_month'] = all_data['item_cnt_month'].clip(0,20)  ## CLIP TARGET VALUE ##","29d60652":"all_data = all_data.merge(period, on = ['date_block_num'], how = 'left')\nall_data = all_data.merge(shops.drop('shop_name', axis=1), on = ['shop_id'], how = 'left')\nall_data = all_data.merge(items.drop('item_name', axis=1), on = ['item_id'], how ='left')","b4e28cc2":"all_data = downcast_dtypes(all_data)\n\nall_data.to_pickle('all_data.pkl')\n\ndel grid, gb , cur_items\ngc.collect();","f31596c3":"#all_data = pd.read_pickle('all_data.pkl')","f2bcd228":"def aggregate(data, col_groupby = ['shop_id'],target_col = 'item_cnt_day',\n              new_col_name='', agg_function = 'mean'):\n    '''\n    groupy data to get an aggregate of columns\n    '''\n    \n    gb = data.groupby(col_groupby, as_index=False).agg({target_col:{new_col_name : agg_function}})\n    gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\n    return (pd.merge(data, gb, how='left', on=col_groupby).fillna(0))\n\ndef create_lags(data, index_cols, column_to_lag, shift_range = [1]):\n    '''\n    make feature lags for a columns for a dataset groupby index_cols\n    '''\n    for month_shift in tqdm_notebook(shift_range) :\n        train_shift = data[index_cols + [column_to_lag]].copy()\n        train_shift = train_shift.drop_duplicates().reset_index(drop=True)\n        \n        train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n        \n        foo = lambda x : '{}_lag_{}'.format(x, month_shift) if x == column_to_lag else x\n        train_shift = train_shift.rename(columns = foo)\n             \n        data = pd.merge(data, train_shift, on=index_cols, how='left').fillna(0)\n        \n      \n        \n    return data\n\ndef agg_and_lag(data, col_groupby, target_col, new_col_name, agg_function, shift_range):\n    ''' \n    wrapper for aggregate and create_lags fonctions\n    '''\n    \n    df_agg = aggregate(data=data, col_groupby = col_groupby,target_col=target_col,\n                      new_col_name=new_col_name, agg_function=agg_function)\n    \n    df_lags = create_lags(data=df_agg, index_cols=col_groupby, \n                          column_to_lag=new_col_name,shift_range=shift_range)\n    \n    df_lags = df_lags.drop(new_col_name, axis = 1) \n    \n    return df_lags","8328ef1a":"all_data = agg_and_lag(all_data,col_groupby=['date_block_num'],\n                       target_col='item_price_month',\n                       new_col_name='date_price_mean', \n                       agg_function='mean',shift_range=[1])\n\nall_data = agg_and_lag(all_data,col_groupby=['item_id','date_block_num'],\n                       target_col='item_price_month',\n                       new_col_name='date_item_price_mean', \n                       agg_function='mean',shift_range=[1,2])\n\nall_data = agg_and_lag(all_data,col_groupby=['shop_id','date_block_num'],\n                       target_col='item_price_month', \n                       new_col_name='date_shop_price_mean', \n                       agg_function='mean',shift_range=[1,2])\n\nall_data = agg_and_lag(all_data,col_groupby=['sub_cat','shop_id','date_block_num'],\n                       target_col='item_price_month',\n                       new_col_name='date_shop_subcat_price_mean',\n                       agg_function='mean',shift_range=[1])\n\nall_data = agg_and_lag(all_data,col_groupby=['item_category_id','date_block_num'],\n                       target_col='item_price_month', \n                       new_col_name='date_itemcat_price_mean',\n                       agg_function='mean',shift_range=[1])\n\nall_data = agg_and_lag(all_data,col_groupby=['main_cat','date_block_num'],\n                       target_col='item_price_month',\n                       new_col_name='date_maincat_price_mean',\n                       agg_function='mean',shift_range=[1])\n\nall_data = agg_and_lag(all_data,col_groupby=['shop_type','date_block_num'],\n                       target_col='item_price_month',\n                       new_col_name='date_shop_type_price_mean',\n                       agg_function='mean',shift_range=[1])\n\nall_data = agg_and_lag(all_data,col_groupby=['item_id','shop_city','date_block_num'],\n                       target_col='item_price_month',\n                       new_col_name='date_item_shopcity_price_mean',\n                       agg_function='mean',shift_range=[1])","ea143388":"all_data = agg_and_lag(all_data,col_groupby=['item_id','date_block_num'],\n                       target_col='item_cnt_month',\n                       new_col_name='date_item_month_mean',\n                       agg_function='mean', shift_range=[1,2])\n\n\nall_data = agg_and_lag(all_data,col_groupby=['shop_id','date_block_num'],\n                       target_col='item_cnt_month',\n                       new_col_name='date_shop_month_mean', \n                       agg_function='mean', shift_range=[1,2])\n\nall_data = agg_and_lag(all_data,col_groupby=['item_category_id','date_block_num'],\n                       target_col='item_cnt_month',\n                       new_col_name='date_itemcat_month_mean', \n                       agg_function='mean', shift_range=[1])\n\nall_data = agg_and_lag(all_data,col_groupby=['shop_id','item_id','date_block_num'],\n                       target_col='item_cnt_month',\n                       new_col_name='date_item_shop_month_mean', \n                       agg_function='mean',shift_range=[1])\n\nall_data = agg_and_lag(all_data,col_groupby=['sub_cat','shop_id','date_block_num'],\n                       target_col='item_cnt_month',\n                       new_col_name='date_item_subcat_month_mean', \n                       agg_function='mean', shift_range=[1])\n\nall_data = agg_and_lag(all_data,col_groupby=['main_cat','shop_id','date_block_num'],\n                       target_col='item_cnt_month',\n                       new_col_name='date_item_maincat_month_mean',\n                       agg_function='mean', shift_range=[1])\n\nall_data = agg_and_lag(all_data,col_groupby=['item_id','shop_city','date_block_num'],\n                       target_col='item_cnt_month',\n                       new_col_name='date_item_shopcity_month_mean',\n                       agg_function='mean',shift_range=[1])\n\nall_data = agg_and_lag(all_data,col_groupby=['date_block_num'],\n                       target_col='item_cnt_month',\n                       new_col_name='date_month_mean',\n                       agg_function='mean', shift_range=[1])","9b319d19":"index_cols = ['shop_id', 'item_id', 'date_block_num']\n\nall_data = create_lags(all_data, index_cols=index_cols,\n                       column_to_lag='item_cnt_month', shift_range=[1,2,3,12])\nall_data = create_lags(all_data, index_cols=index_cols,\n                       column_to_lag='item_price_month', shift_range=[1,2])","6dd41369":"del sales, item_cats, items","1d2b2fdc":"all_data = all_data[all_data['date_block_num'] >= 3]  \n\ngc.collect();","b827174e":"all_data.to_pickle('all_data_lags.pkl')","b4a05f9c":"#all_data = pd.read_pickle('all_data_lags.pkl')","d076debc":"#all_data_enc = mean_encoding_reg(all_data,'shop_city','item_cnt_month', 5)\n#all_data_enc = mean_encoding_reg(all_data_enc,'main_cat','item_cnt_month', 5)\n#all_data_enc = mean_encoding_reg(all_data_enc,'month','item_cnt_month', 5)\n#all_data_enc = mean_encoding_reg(all_data_enc,'item_category_id','item_cnt_month', 5)\n\n#all_data_enc.to_pickle('all_data_enc.pkl')","3b03306d":"#all_data_enc = pd.read_pickle('all_data_enc.pkl')","2d81b32e":"date = all_data['date_block_num']\ntarget = all_data['item_cnt_month']","c776f7d6":"all_data = all_data.drop('item_cnt_month', axis = 1)\nall_data = all_data.drop('item_price_month', axis = 1 )","0eb7173f":" def correlatedDropper( df_data, thresh = 0.95):\n\n        \"\"\"\n        Remove columns too correlated from a ``pandas.DataFrame\n\n        \"\"\"\n        df = df_data.copy()\n        corr_matrix = df.corr()\n\n        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n        cols_to_drop = [column for column in upper.columns if any(upper[column] > thresh)]\n\n        df = df.drop(cols_to_drop, axis=1)\n        return df, cols_to_drop","943c60f2":"test, col_drop = correlatedDropper(all_data, 0.95)","c68aad11":"col_drop","78bc55c3":"last_block_num = 33\n\nX_test = all_data[date == 34]\nX_valid = all_data[date == last_block_num]\nX_train = all_data[date < last_block_num]\n\ny_train = target.loc[date < last_block_num].values\ny_valid = target.loc[date == last_block_num].values\n\n\ncategorical_feat_ind = np.where(X_train.dtypes != np.float32)[0]\nprint(categorical_feat_ind)\n\n#Pool1 = Pool(X_train, y_train, categorical_feat_ind)","0adde2aa":"del all_data, target, date\n\ngc.collect()","44628015":"def catboost_rmse(X_train,y_train,\n                  X_valid, y_valid,\n                  loss_function='RMSE',\n                  iterations=400,\n                  random_seed=0,\n                  learning_rate=0.15,\n                  depth=5,\n                  l2_leaf_reg=1,\n                  one_hot_max_size=200,\n                  min_data_in_leaf = 1,\n                  bagging_temperature = 1,\n                  border_count = 32,\n                  max_ctr_complexity = 2\n               ) :\n    \n    ''' \n    fonction to optimize, here the catboost RMSE \n    '''\n    \n    catmodel = CatBoostRegressor( \n    loss_function= loss_function,\n    iterations=iterations,\n    random_seed=random_seed,\n    learning_rate=learning_rate,\n    depth=depth,\n    l2_leaf_reg=l2_leaf_reg,\n    one_hot_max_size=one_hot_max_size,\n    min_data_in_leaf = min_data_in_leaf,\n    bagging_temperature= bagging_temperature,        \n    border_count=border_count,\n    max_ctr_complexity=max_ctr_complexity,\n    thread_count=-1,\n    od_type='Iter',\n    od_wait= 20,\n    task_type='GPU',\n    devices='0',  \n    verbose = 100\n    )\n\n    catmodel.fit(X_train,\n                 y_train,\n                 eval_set=(X_valid,y_valid),\n                 plot=False\n            )\n\n    predict = catmodel.predict(X_valid).clip(0,20)\n    \n    best_rmse_train = catmodel.best_score_['learn']['RMSE']\n    best_rmse_valid = catmodel.best_score_['validation']['RMSE']\n        \n        \n    print('train_rmse : {:.4f}, test_rmse : {:.4f}'.format(best_rmse_train, best_rmse_valid))\n        \n        \n    r2 = r2_score(y_valid,predict)\n        \n    print('the R2 score is {:.4f}'.format(r2))\n    \n    del predict\n   \n    \n    return - best_rmse_valid","cc22883c":"def optimize_catboost(X_train,y_train,X_valid,y_valid, param_probe=None) :\n    '''\n    return the optimizer  and output the best hyperparameters found\n    param_probe : dict of hyperparameters in order to guide the optimization\n    '''\n    def catboost_wrapper(learning_rate, depth, one_hot_max_size, \n                         min_data_in_leaf, bagging_temperature,\n                         border_count, max_ctr_complexity,\n                         loss_function = 'RMSE', l2_leaf_reg = 1, \n                         random_seed = 0,  iterations = 1000) :\n        '''\n        wrapper for the function to optimize\n        '''\n        \n        return catboost_rmse(X_train=X_train, y_train=y_train ,\n                             X_valid=X_valid ,y_valid=y_valid, \n                             iterations = int(iterations),\n                             random_seed = random_seed, \n                             learning_rate=learning_rate, \n                             depth=int(depth),\n                             l2_leaf_reg=l2_leaf_reg,\n                             one_hot_max_size= int(one_hot_max_size), \n                             min_data_in_leaf= int(min_data_in_leaf),\n                             bagging_temperature = bagging_temperature,\n                             border_count = int(border_count), \n                             max_ctr_complexity = int(max_ctr_complexity)\n              )\n    \n    optimizer = BayesianOptimization(\n        f = catboost_wrapper,\n        pbounds= {\n            \"learning_rate\" : (0.01,0.50),\n            \"depth\" : (2,10),\n            \"l2_leaf_reg\" : (1,50),\n            \"one_hot_max_size\" : (2,230),\n            \"min_data_in_leaf\" : (1,10),\n            \"bagging_temperature\" : (0,100),\n            \"border_count\" : (30, 250),\n            \"max_ctr_complexity\" : (1,4)          \n        },\n        random_state = 63,\n        verbose = 30,\n    )\n    if param_probe != None :\n        optimizer.probe(\n        params= param_probe,\n        lazy=True,\n        )\n    \n    optimizer.maximize(n_iter=3,\n                      init_points=5)\n    \n    print(\"Final result:\", optimizer.max)\n    return optimizer","1d941ea5":"params_probe = {'bagging_temperature': 0.28300538794137076, \n                'border_count': 247.51179202909668, \n                'depth': 8.478936311173683, \n                'l2_leaf_reg': 7.73213368476749,\n                'learning_rate': 0.10290800942292017,\n                'max_ctr_complexity': 2.9452851946784433, \n                'min_data_in_leaf': 2.383458113416781, \n                'one_hot_max_size': 209.4502766887087}","ed3c66d3":"print(Colours.green(\"\"\" -- Optimizing Catboost Parameters -- \"\"\"))\noptimizer_cat = optimize_catboost(X_train=X_train, y_train=y_train, \n                                  X_valid=X_valid, y_valid=y_valid,\n                                  param_probe=params_probe)","6c9da6d5":"params_catboost =  {'bagging_temperature': 0.28300538794137076,\n                    'border_count': 247, \n                    'depth': 8, \n                    'l2_leaf_reg': 7.73213368476749,\n                    'learning_rate': 0.10290800942292017,\n                    'max_ctr_complexity': 2, \n                    'min_data_in_leaf': 2.383458113416781, \n                    'one_hot_max_size': 209,\n                    'random_seed' : 0,\n                    'thread_count' : -1,\n                    'task_type' : 'GPU',\n                    'devices' : '0', \n                    'od_type' : 'Iter',\n                    'od_wait' :  20,\n                    'verbose' : 50\n                   }","94c36c80":"catmodel = CatBoostRegressor(**params_catboost)\n\ncatmodel.fit(X_train,y_train,\n             eval_set=(X_valid,y_valid),\n             plot=False\n            )\n\npredict = catmodel.predict(X_valid).clip(0,20)\nr2 = r2_score(y_valid,predict)\nprint( 'r2_score est {}'.format(r2))","24a0a81c":"def get_feature_imp(catmodel, method, X_train, y_train, X_test, y_test, plot = False):\n    '''\n    return the importance of the feature based on a specific method\n    method : - \"PredictionValuesChange\"\n             - \"LossFunctionChange\"\n    '''\n    \n    fi = catmodel.get_feature_importance(Pool(X_test, label=y_test), type=method)\n        \n    feature_score = pd.DataFrame(list(zip(X_test.dtypes.index, fi )),\n                                        columns=['Feature','Score'])\n\n    feature_score = feature_score.sort_values(by='Score', ascending=False, inplace=False,\n                                              kind='quicksort', na_position='last')\n    if plot : \n        plt.rcParams[\"figure.figsize\"] = (12,7)\n        ax = feature_score.plot('Feature', 'Score', kind='bar', color='c')\n        ax.set_title(\"Feature Importance using {}\".format(method), fontsize = 14)\n        ax.set_xlabel(\"features\")\n        plt.show()   \n    return feature_score","b0a66ac6":"feature_pred = get_feature_imp(catmodel=catmodel, method='PredictionValuesChange', \n                               X_train=X_train, y_train=y_train,\n                               X_test=X_valid, y_test=y_valid,\n                               plot=True)","e9d408b5":"feature_useless_pred = feature_pred[feature_pred['Score'] < 0.01]   #0.1\nfeature_useless_pred.Feature","0f053119":"X_train_pred = X_train.drop(feature_useless_pred.Feature[:-1], axis = 1)\nX_valid_pred = X_valid.drop(feature_useless_pred.Feature[:-1], axis = 1)\nX_test_pred = X_test.drop(feature_useless_pred.Feature[:-1], axis = 1) \n### I kept date_block_num after tests","430ea864":"feature_loss = get_feature_imp(catmodel,'LossFunctionChange',\n                               X_train, y_train, \n                               X_valid, y_valid,\n                               plot=True)","a7f3ede3":"feature_useless_loss = feature_loss[feature_loss['Score'] < 0.0]\nfeature_useless_loss.Feature","cfc01020":"catmodel_pred = CatBoostRegressor(**params_catboost)\n\ncatmodel_pred.fit(X_train_pred,y_train,\n             eval_set=(X_valid_pred,y_valid),\n             plot=False\n            )\n\npredict = catmodel_pred.predict(X_valid_pred).clip(0,20)\nr2 = r2_score(y_valid,predict)\nprint( 'r2_score est {}'.format(r2))","d4ed0477":"print(Colours.green(\"\"\" -- Optimizing Catboost Parameters -- \"\"\"))\noptimizer_cat_loss = optimize_catboost(X_train=X_train_pred, y_train=y_train,\n                                       X_valid=X_valid_pred, y_valid=y_valid,\n                                       param_probe=params_probe)","b95060b8":"del X_train, X_valid","ba9daa7d":"X_train_pred.to_pickle('X_train_pred.pkl')\nX_valid_pred.to_pickle('X_valid_pred.pkl')","dc880a5b":"#X_train_pred = pd.read_pickle('X_train_pred.pkl')\n#X_valid_pred = pd.read_pickle('X_valid_pred.pkl')","afa2a717":"def find_top_seeds(X_train,y_train,X_valid,y_valid,params, range_min = 0, range_max = 10):\n    \n    \n    metric_dict = pd.DataFrame(columns=['state','rmse'])\n                                  \n    for i in range(range_min,range_max) :\n        print('--- random state : {} ---\\n'.format(i))\n        \n        params['random_seed'] = i \n        catmodel = CatBoostRegressor(**params)\n        catmodel.fit(X_train,y_train,\n             eval_set=(X_valid,y_valid),\n             plot=False\n            )\n        \n        best_rmse_valid = catmodel.best_score_['validation']['RMSE']\n        \n        metric_dict.loc[i-range_min] = [int(i)] + [best_rmse_valid]\n        metric_dict['state'] = metric_dict['state'].astype(np.int32)\n        \n    return metric_dict.sort_values(by='rmse', ascending = 'False')","4d0b557c":"metric = find_top_seeds(X_train_pred,y_train,X_valid_pred, y_valid,\n                        params=params_catboost, range_min=0, range_max=10)","2f30de03":"plt.rcParams[\"figure.figsize\"] = (7,7)\nax = metric.plot('state', 'rmse', kind='bar', color='c')\nax.set_title(\"rmse en fonction du shuffling\", fontsize = 10)\nax.set_xlabel(\"state\")\nplt.show()  ","43ca4106":"predictions_test = []   # 0.92949 for 2,4,6,8\n   \nfor i in [4,8,6]:\n    print('random state {}'.format(i))\n    \n    params_catboost['random_seed'] = i\n    \n    catmodel_bagging_pred = CatBoostRegressor(**params_catboost)\n    catmodel_bagging_pred.fit(X_train_pred,y_train,\n             eval_set=(X_valid_pred,y_valid),\n             plot=False\n            )\n\n    predict = catmodel_bagging_pred.predict(X_valid_pred).clip(0,20)\n    r2 = r2_score(y_valid,predict)\n    print( 'r2_score est {}'.format(r2))\n    \n    predictions_test.append(catmodel_bagging_pred.predict(X_test_pred).clip(0,20))","a014ee62":"predictions =  np.mean(np.array(predictions_test), axis = 0)","fb6901a1":"dt_test_opt = pd.DataFrame()\ndt_test_opt['item_cnt_month'] = predictions\ndt_test_opt.index.name='ID'\ndt_test_opt.to_csv('submission_featurev11_bagging_[4_6_8].csv')","f7adcb7a":"ITEMS FEATURES ","e69bfa34":"PREDICTION VALUE CHANGE","29dbe41a":"For emsembling, I use the simple Bagging where models are the same except for the random seed.\nAfter using 10 differents seeds, the overall prediction performed worse so I decided to only choose the best seeds.","a528283d":"Now, let's remove the old X and save the new one.","de1292fd":"I merge the test data with the sales data.","d8044037":"Then, the aggregates based on the item_cnt_month feature :","8ae9dbe3":"Since I ran this a lot, I found thoses parameters bellow. Let's probe it to the optimizer in order to guide the optimization and see if we can find better parameters. ","3b86c8c9":"![](http:\/\/)To check the size of your variables, if you have memory issue.","e06c5b45":"Thanks to the name of shops, we can extract the city and the type of the shop   and then label encode it. ","037773bd":"ITEM_CNT_MONTH AGGREGATE ","5d6e3df7":"PRICE AGGREGATE ","8c66ae52":"We don't need to use data from the first 3 months since we created lags.","fc0675cb":"We can remove the duplicate names by doing the following :","a67eaffe":"I decided to remove those outliers.","bb54dff6":"SHOP FEATURES","1c6e3956":"We store the target and date and then delete from all_data.","8e94a9be":"Let's add a holiday feature.","19e44061":"First, some shops are the same. Let's remove it. ","d2f4916b":"After submitting to kaggle several times, I only kept seeds  4,6,8.","dd29ba97":" # **Splitting the dataset in train \/ validation \/ test set**","07d5d4c9":"# **Ensembling and final prediction**","60268d25":"After test, I decided to keep it.","660786f7":"I tried mean encoding for categorical variables but I ended up not using it because of more overfitting.","b6e8315f":"We can extract time data.","637f7450":"import sys\n\n# These are the usual ipython objects, including this one you are creating\nipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n\n# Get a sorted list of the objects and their sizes\nsorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)","39a7bd6d":"As item_price_month and item_cnt_month were made from data only available from the train data, we cannot use them for the prediction, but we can still count on generated features of past data. this means we can for example use the last 2 or 3 months of an item_id price of any feature combination( item_id and shop_city).\n\nSo in order to do this, let's define some functions to generate the lag of a column created by any kind of combination for the  month we want.","c33ab029":"I found this idea thanks to Mazoub parpanchi who implemented it for lgb  in his great kernel : https:\/\/www.kaggle.com\/masoudmzb\/bayesian-optimization-for-lgb\nGive it a look !        ","72d88f3f":"REMOVING FEATURE TOO CORRELATED \/ USELESS ONE","536a8695":"Let's run a model based on this shrunken X.","7ae9b604":"# **Feature engineering**","6aa3e69f":"I decided to remove the feature with virtually no impact on the prediction.","2d30f4b1":"First, let's make aggregate based on price :","fef12c96":"At this step, the all_data dataset contains all the basic information (train and test data) summarized per month.","ff7aef8f":"Now we can expand the sales dataset. We first create a grid from all shops\/items combinations from every month\nFor item_cnt_day we sum them to generate item_cnt_month values, the target.\nFor item_price we average it's values to calculate the item_price_month.\n\nAlso, we clip the target value between 0 & 20 as recommended on Kaggle.","fee71164":"I make a small dataframe containing only time data.","0a1022ff":"Bellow another method in order to perform feature selection, but I didn't use it eventually as I performed worse in the LB with this one.","83f31ffa":"### MERGING ###","cdb8a63b":"We start with a little bit of EDA.","6afbff8c":" # **Feature Selection **","39993909":"Since X has a lot of features, we can reduce them in order to reduce overfitting.","bdcde162":"We can finally merge every datasets together and save it all.","cd4b159b":"Again, let's extract a main and a sub category from the name of the category.","a8b62b43":"# **Bayesian optimization for hyperparameters**","56c36c21":"Not for this run, so let's troncate the int parameters and add the constant ones. We stop the training after 8 iterations without test RMSE improving. You can of course increase the number of iterations for a better mapping of the objective function.","04d1cf13":"Finally, we create lags for the target itself and item_price_month :","34e10203":"Different names features can be extracted from the items data.","1b386ce6":"SALES FEATURES ","89c1a29a":"ITEM_CAT FEATURES","58185bdd":"The RMSE test decreased and the train RMSE increased. So that's exactly what we wanted since there is less overfitting. My LB improved after this.\nWe can try to improve it by running an another optimization. For me, even if the RMSE decreases, it did not improve in the LB but you can try it by yourself.","c164086e":"Since I don't speak russian , I found help in several kernels in Kaggle in order to extract valuables informations from sales,shop and items data by looking at the names. This one especially : https:\/\/www.kaggle.com\/kyakovlev\/1st-place-solution-part-1-hands-on-data","1217aae9":"The last month 34 correspond to the test set. We'll be using the last month - 1 as the validation set since it's time data.","a57359f7":"Instead of using gridSearchCV , I decided to use Bayesian Optimization. Bayesian Optimization enables to find rapidly good hyperparameters in less than 10 iterations using Gausian processes. For this, we have to define a function , and the algorithm will try to find the best maxima of this function out of several parameters.\nHere, the function is basically the catboost model which output the RMSE and the parameters are the hyperparameters ( like the bagging temperature, l2_reg..).\nThe library is only defined to maximize the output so here the output will be -RMSE since we want to minimize it !.\nBellow is the link of the repo for a more detailled explanation : \nhttps:\/\/github.com\/fmfn\/BayesianOptimization.\nUsing GPU is of course the best choice.\n                                                                                                                        ","181b6d5e":"We first merge item and item_category for simplicity first. "}}