{"cell_type":{"7bd8a6ff":"code","e75ead69":"code","7f1a6197":"code","59c30b63":"code","dfa203a0":"code","f9762d4e":"code","621ff7da":"code","4abbb0b6":"code","7c839476":"code","013bc140":"code","a03d00e8":"code","3523a494":"code","dc1ef634":"code","f6e9cc4e":"code","920a024b":"code","8ab6fa3b":"markdown","81b19207":"markdown","1b2a0b53":"markdown","25c12f99":"markdown","9a6aa9cb":"markdown","c533a5bb":"markdown","7dce4ec7":"markdown","e21ec9c2":"markdown","3b640d7a":"markdown","50c1c6b6":"markdown","b3fe848f":"markdown","fe983127":"markdown"},"source":{"7bd8a6ff":"import numpy as np \nimport pandas as pd\nimport os\n\n# CSV loaderfunction\ndef load_data(path, file_name):\n    csv_path = os.path.join(path, file_name)\n    return pd.read_csv(csv_path)\n\n# Extract X_test\nX_test = load_data('\/kaggle\/input\/titanic-machine-learning-from-disaster','test.csv')\n\n# y_test\ny_test = load_data('\/kaggle\/input\/y-test-titanic','y_test.csv')\n\n# Extract X_train\nX_train = load_data('\/kaggle\/input\/titanic-machine-learning-from-disaster','train.csv')\n\n# Extract predictions (y_train) from X_train\ny_train = X_train[['Survived']]\ny_train.append(y_test[['Survived']])\n\n# Drop the predictions from the training set\nX_train.drop('Survived', axis=1, inplace=True)\n\n# Merge x_train and x_test\nX_merged = X_train.append(X_test)","e75ead69":"from sklearn.pipeline import Pipeline \nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\n\n\n# COMBINE AND CLEAN UP DATA\n\nclass Modifier():\n    \n    def transform(self, df=X_merged, \n                  company=True, title=True, \n                  cabin_letter=True, ticket_num=True,\n                  age=True, pipeline=True):\n        if company:\n            df['Company'] = df['SibSp'] + df['Parch']\n        if title:\n            df['Title'] = df.Name.str.extract(' ([A-Za-z]+)\\.')\n            \n            df['Title'] = df['Title'].replace(\n                                            ['Lady','Countess',\n                                             'Capt','Col',\n                                             'Don','Dr',\n                                             'Major','Rev',\n                                             'Sir','Jonkheer',\n                                             'Dona'],\n                                                    'Unusual')\n            \n            df['Title'] = df['Title'].replace('Mlle','Miss')\n            df['Title'] = df['Title'].replace('Ms','Miss')\n            df['Title'] = df['Title'].replace('Mme','Mrs')\n        if cabin_letter:\n            df['Cabin_letter'] = df.Cabin.str.extract('([A-Z])') #Takes the letter of the Cabin\n        if ticket_num:\n            df['Ticket_num'] = df.Ticket.str.extract('([0-9]+)') #Takes the number on the ticket\n        if age:\n            df['Age_strat'] = pd.cut(df['Age'],\n                                     bins=[\n                                             0, 11, \n                                             18, 22,\n                                             27, 33,\n                                             40, 66,\n                                             np.inf\n                                                     ],\n                                     \n                                     labels=[i for i in range(1,9)]\n                                    )\n            \n        return df\n    \n# ATTRIBUTES\n\n# Numeric\nnum_attr = ['Fare']\n\n# Categorical Alphabetic\ncat_attr = ['Embarked', 'Title',\n            'Cabin_letter','Sex']\n\n# Categorical Numeric\nord_attr = ['Pclass','Company','Age_strat']\n\n\n#PIPELINE TO CLEAN AND TRANSFORM THE DATA \n\n# For numeric attributes\nnum_pipeline = Pipeline(\n    [\n        ('imputer', SimpleImputer(strategy='median')),\n        ('StdScaler', StandardScaler())\n    ]\n)\n\n# For categorical alphabetic attributes\ncat_pipeline = Pipeline(\n    [\n        ('imputer', SimpleImputer(strategy='constant', fill_value = 'Z')),\n        ('OneVeryHot', OneHotEncoder())\n    ]\n)\n\n# For categorical alphabetic numeric\nord_pipeline = Pipeline(\n    [\n        ('imputer', SimpleImputer(strategy='constant',\n                                  fill_value = 0)),\n        ('OneVeryHot', OneHotEncoder())\n    ]    \n)\n\n\n# MERGE THE THREE PIPELINES IN ONE\n\nfull_pipeline = ColumnTransformer(\n    [('num', num_pipeline, num_attr),\n    ('cat', cat_pipeline, cat_attr),\n    ('ord', ord_pipeline, ord_attr)]\n)\n\n# Instanciateing\nattr_mod = Modifier()\nmod_df = attr_mod.transform(ticket_num=False) #ticket_num set to false since it is probably not useful for the algorithm\n\n# Our dataframe transformed and output as a spare matrix --- READY TO BE TO PASS IT TO THE ALGORITHM \nX = full_pipeline.fit_transform(mod_df)","7f1a6197":"# Split into training data and test data \n\nX, X_test = X[:891], X[891:]","59c30b63":"# Function to nicely print cross val results\ndef cross_val_results(cross_val_score):\n    print('---------CROSS VALIDATION - THREE-FOLD---------\\n')\n    print('Scores: \\t', [round(i,3) for i in cross_val_score])\n    print('Mean:   \\t', round(cross_val_score.mean(),2))\n    print('Std dev:\\t', round(cross_val_score.std(),6))","dfa203a0":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\n# List of attributes\nlist_of_attr = [\n                    'Fare','Embarked_S','Embarked_C',\n                    'Embarked_Q','Embarked_Nan' ,'Mr',\n                    'Mrs','Miss','Master','Unusual',\n                    'Cabin_NaN','Cabin_C','Cabin_E', \n                    'Cabin_G','Cabin_D','Cabin_A',\n                    'Cabin_B','Cabin_F','Cabin_T',\n                    'male','female','Class_3',\n                    'Class_1','Class_2','Company_1',\n                    'Company_0','Company_4','Company_2',\n                    'Company_6','Company_5','Company_3',\n                    'Company_7','Company_10','Age_3',\n                    'Age_6', 'Age_4', 'Age_NaN','Age_7',\n                    'Age_1','Age_2','Age_5','Age_8'\n                                                   ]\n\n# Create a function that returns a data frame of which its columns are: attributes, coeficients and Odds Ratio\ndef coef(model, list_of_attr):\n    list_of_coef = list(model.coef_[0,:])\n    \n    coef_df = pd.DataFrame(\n            {\n            'Attributes':list_of_attr, \n            'Coefficients': list_of_coef, \n            'Odds_Ratio': [np.exp(i) for i in list_of_coef]\n            }\n        )\n    \n    return coef_df\n\nlog_reg = LogisticRegression()\nlog_reg_cross_val = cross_val_score(\n                                log_reg, \n                                X,\n                                np.ravel(y_train),\n                                cv=3\n                                    )\n\n# Print cross val results\ncross_val_results(log_reg_cross_val)\n\nlog_reg.fit(X, np.ravel(y_train))\n\ncoef_df = coef(log_reg, list_of_attr)\n\n# Print positive coefficients, in descending order\ncoef_df.loc[coef_df['Coefficients'] < 0].sort_values(\n                                                    ascending=False,\n                                                    by='Coefficients'\n                                                            ).head()","f9762d4e":"from sklearn.tree import DecisionTreeClassifier\n\ntree_clf = DecisionTreeClassifier(max_depth=4, max_leaf_nodes=7)\ntree_clf.fit(X, np.ravel(y_train))\n\ncross_val_results(\n    cross_val_score(\n        tree_clf,\n        X,\n        np.ravel(y_train),\n        cv=3\n    )\n)\n\n# Extract decision tree as .dot file\n\nfrom sklearn.tree import export_graphviz\n\nexport_graphviz(\n    tree_clf,\n    out_file='\/kaggle\/working\/tree_clf.dot',\n    feature_names=list_of_attr,\n    class_names= ['Died','Survived'],\n    rounded=True,\n    filled=True\n)\n\n# Convert the decision tree .dot file into an easy-to-read image .png\n\nimport pydot\n\n(graph,) = pydot.graph_from_dot_file('tree_clf.dot')\ngraph.write_png('tree_clf.png')","621ff7da":"from sklearn.ensemble import RandomForestClassifier\nfrom  sklearn.ensemble import VotingClassifier\nfrom sklearn.svm import SVC\n\n# Instanciateing\nlog_clf = LogisticRegression()\nrnd_clf = RandomForestClassifier(n_estimators=300,\n                                 max_leaf_nodes=15,\n                                 n_jobs=-1)\nsvm_clf = SVC(probability=True)\n\nvoting_clf = VotingClassifier(\n                    estimators=[\n                                    ('lr',log_clf),\n                                    ('rf',rnd_clf),\n                                    ('svc',svm_clf)                         \n                                ],\n                    voting='soft'\n)\n\ncross_val_results(\n    cross_val_score(\n                    voting_clf,\n                    X,\n                    np.ravel(y_train),\n                    cv=3)\n                            )","4abbb0b6":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    {\n     'n_estimators': [200, 300, 500],\n     'max_leaf_nodes': [15,17,23]\n    }\n]\n\nrnd_clf = RandomForestClassifier(n_jobs=-1, oob_score=True) #All available CPU's\n\n\ngrid_search = GridSearchCV(rnd_clf, param_grid, cv=3, return_train_score=True)\n\ngrid_search.fit(X, np.ravel(y_train))\ngrid_search.best_params_\n\ncvres = grid_search.cv_results_\n\nfor scores, params in zip(cvres['mean_test_score'],cvres['params']):\n    print(scores, params)\n\ngrid_search.best_params_","7c839476":"rfc_2 = RandomForestClassifier(n_estimators=300, max_leaf_nodes=15, n_jobs=-1) \n\ncross_val_results(\n    cross_val_score(\n                    rfc_2,\n                    X,\n                    np.ravel(y_train),\n                    cv=3)\n                            )","013bc140":"from sklearn.ensemble import ExtraTreesClassifier\n\netc_clf = ExtraTreesClassifier(n_jobs=-1, oob_score=True) #All available CPU's\n\n\ngrid_search_2 = GridSearchCV(\n                             etc_clf,\n                             param_grid,\n                             cv=3,\n                             return_train_score=True\n                                )\n\ngrid_search.fit(X, np.ravel(y_train))\n\ncvres = grid_search.cv_results_\n\nfor scores, params in zip(cvres['mean_test_score'],cvres['params']):\n    print(scores, params)","a03d00e8":"# RANDOM FOREST CLASSIFIER\n\nrandom_forest = RandomForestClassifier(\n                                        n_estimators=300,\n                                        max_leaf_nodes=15,\n                                        n_jobs=-1\n                                                    )\nrandom_forest.fit(X, np.ravel(y_train))\ny_pred = random_forest.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\n\naccuracy_score(y_pred, y_test['Survived'])","3523a494":"from sklearn.metrics import precision_score, recall_score\n\nprecision = precision_score(y_pred, y_test['Survived'])\nrecall = recall_score(y_pred, y_test['Survived'])\n\nprint('Precision: {}\\nRecall: {}'.format(round(precision,2),\n                                         round(recall,2)))","dc1ef634":"# EXTREMELY RANDOMIZED TREES \n\netc = ExtraTreesClassifier(max_leaf_nodes=17, n_estimators=200, n_jobs=-1)\netc = etc.fit(X, np.ravel(y_train))\nex_tr_cl_pred = etc.predict(X_test)\n\naccuracy_score(ex_tr_cl_pred, y_test['Survived'])","f6e9cc4e":"# SOFT VOTING CLASSIFIER \n\nvoting_clf.fit(X, np.ravel(y_train))\np = voting_clf.predict(X_test)\n\naccuracy_score(p, y_test['Survived'])","920a024b":"# SUBMISSION\n\nsubmission = pd.DataFrame(\n                    {\n                        'PassengerId': y_test['PassengerId'],\n                        \"Survived\": y_pred\n                    }\n                            )\n\nsubmission.to_csv('\/kaggle\/working\/submission.csv', index=False)\n","8ab6fa3b":"# Data Preparation","81b19207":"# Getting the feet wet on ML with the Titanc Dataset, by *Pau Orti*","1b2a0b53":"# RANDOM FOREST CLASSIFIER","25c12f99":"# DECISION TREE CLASSIFIER","9a6aa9cb":"# TESTING MODEL'S ACCURRACY ON TEST SET","c533a5bb":"# BEST MODEL (RandomForestClassifier)","7dce4ec7":"# Ensamble Methods\n\n# SOFT VOTING CLASSIFIER - Logit, RandomForest, SVC, SVC","e21ec9c2":"# LOGISTIC REGRESSION CLASSIFIER","3b640d7a":"# References\nThis notebook has been created based on great work done solving the Titanic competition and other sources.\n\n* [Predicting the Survival of Titanic Passengers, by Niklas Donges](http:\/\/towardsdatascience.com\/predicting-the-survival-of-titanic-passengers-30870ccc7e8)\n* [Titanic Data Science Solutions, by Manav Sehgal](https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions)\n* [Hands-on machine learning with scikit-learn keras and tensorflow, by Aur\u00e9lien G\u00e9ron](https:\/\/www.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/)\n\n","50c1c6b6":"### **Titanic Decision Tree**\n![sklearn.tree.export_graphviz](tree_clf.png)","b3fe848f":"# Building Machine Learning Models","fe983127":"# EXTREMELY RANDOMIZED TREES"}}