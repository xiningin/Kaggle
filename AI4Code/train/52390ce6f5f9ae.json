{"cell_type":{"306e1e6e":"code","3175a6ff":"code","3257a9d8":"code","caf55d9c":"code","c718948e":"code","3a79a214":"code","66e9d169":"code","f15acc17":"code","992c1ed4":"code","0391f9c9":"code","a597bd40":"code","357702d0":"code","3d0106ae":"code","68b59050":"code","81028e3e":"code","03f3614a":"code","dcde39e4":"markdown","2c7353e2":"markdown","6a477c00":"markdown","2d3265e1":"markdown","eaf408c9":"markdown","1a4f9745":"markdown","4bee8922":"markdown","fcdf70a5":"markdown","e6a10f8b":"markdown"},"source":{"306e1e6e":"# !pip install geopy # in case kaggle image doesn't have it","3175a6ff":"import re\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom geopy.geocoders import ArcGIS","3257a9d8":"# check what data do we have\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","caf55d9c":"# import data\ntest = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\n\n# glue datasets together, for convenience\ntrain['is_train'] = True\ntest['is_train'] = False\ndf = pd.concat(\n    [train, test], \n    sort=False, ignore_index=True\n).set_index('id').sort_index()\n\nprint(train.shape, test.shape, df.shape)\ndf.head()","c718948e":"# define 'suspicious' locations (fictional, worldwide and\/or aren't close to location pattern)\n\nsuspicious_locations = '(?:' + '|'.join([\n    'worldwide',\n    'world',\n    'earth',\n    'global',\n    'narnia',\n    'the universe',\n    'does it really matter',\n    'upstairs',\n    'heaven',\n    'azeroth',\n    'location'  \n])\n\nsuspicious_locations += '|@|\\?+|^missing$|(?:any|else|some|no|every)where|^[0-9]+$|http[s]?|#)'\nsuspicious_locations","3a79a214":"col = 'location'\n\nmissing_cnt = df[col].isnull().sum()\nprint(f'Missing values before cleaning: {missing_cnt}')\n\n# fill in with NaNs some strange values (emails, question marks, etc.)\ndf[col] = df[col].fillna('missing').astype(str)\ndf.loc[\n    (\n        df[col].str.contains(suspicious_locations, flags=re.I)\n        | (df[col].str.len() < 2) # strange 1-char outliers\n    ),\n    col\n] = np.nan\nprint(f'Missing values after  cleaning: {df.location.isnull().sum()}')\n\nloc_cnt = df[col].str.lower().value_counts()\n\nmin_cnt = 3\n\nprint(\n    f'Unique locations: \\t\\t{len(loc_cnt)}'\n    f'\\nPopular locations (>{min_cnt}): \\t{(loc_cnt > min_cnt).sum()}\\n',\n    f'Popular locations (>{min_cnt}) %: \\t{loc_cnt[loc_cnt > min_cnt].sum()\/df[col].notnull().sum() * 100 :.2f}'\n)\n\n# well, we have common ones (like USA), as well as tweet-unique locations\n# as well as fake places like `Narnia` or `Azeroth` :)\n# as well as unspecified like `worldwide`\npd.concat([loc_cnt.head(10), loc_cnt.tail(10)])","66e9d169":"latlong_regex = (\n    '(?P<lat>[-+]?(?:[1-8]?\\d(?:\\.\\d+)?|90(?:\\.0+)?)),'\n    '\\s*(?P<lon>[-+]?(?:180(?:\\.0+)?|(?:(?:1[0-7]\\d)|(?:[1-9]?\\d))(?:\\.\\d+)?))$'\n)\n\nvalid_coordinates_ind = df[col].fillna('missing').str.contains(latlong_regex)\n# check extracted data\nprint(f'Coordinates found: {valid_coordinates_ind.sum()}\/{df[col].notnull().sum()}')\ndf.loc[valid_coordinates_ind, col].head()","f15acc17":"# extract lat, long using Pandas functionality\nvalid_coordinates = df[col].fillna('missing').str.extract(latlong_regex)\\\n[valid_coordinates_ind].astype(np.float32)\n\nvalid_coordinates.head()","992c1ed4":"from tqdm.notebook import tqdm\nfrom time import sleep\n\n\ndef geocode_address(query: str, geocoder=ArcGIS(), **kwgs):\n    \"\"\"\n    get lat\/lon and other attributes \n    from query string (address)\n    \"\"\"\n    # https:\/\/geopy.readthedocs.io\/en\/stable\/#arcgis\n    sleep(0.1)\n    res = geocoder.geocode(query=query, **kwgs)\n    try:\n        return {\n            'lat': res.point.latitude,\n            'lon': res.point.longitude,\n            'country': res.raw['attributes'].get('Country', np.nan),\n            'city': res.raw['attributes'].get('City', np.nan),\n            'match_score': res.raw['score'],\n        }\n    except (KeyError, AttributeError):\n        return {\n            'lat': np.nan,\n            'lon': np.nan,\n            'country': np.nan,\n            'city': np.nan,\n            'match_score': 0,\n        }\n    \ndef geocode_address_rev(query: str, geocoder=ArcGIS(), **kwgs):\n    \"\"\"\n    get address attributes \n    from query string (lat\/lon pair)\n    \"\"\"\n    sleep(0.1)\n    lat, lon = [float(c) for c in query.split(',')]\n    # https:\/\/geopy.readthedocs.io\/en\/stable\/#arcgis\n    res = geocoder.reverse(query=query, **kwgs)\n    try:\n        return {\n            'lat': lat,\n            'lon': lon,\n            'country': res.raw['CountryCode'],\n            'city': res.raw.get('City', np.nan),\n            'match_score': 100,\n        }\n    except KeyError:\n        return {\n            'lat': lat,\n            'lon': lon,\n            'country': np.nan,\n            'city': np.nan,\n            'match_score': 0,\n        }\n    \n\ngeocoder = ArcGIS(\n    timeout=100,\n    user_agent='kaggle_twitter_geodata',\n    # or create free account at https:\/\/www.arcgis.com\/index.html\n    # and get up to 1kk requests per month and specify default referer\n    # username='your_username', \n    # password='your_password', \n    # scheme='https', \n    # referer='https:\/\/www.example.com'\n)","0391f9c9":"# extract info using reverse geocoding (full)\ngeodata_inv = {\n        ind: geocode_address_rev(query=coords, geocoder=geocoder)\n        for (ind, coords) in tqdm(\n            valid_coordinates.astype(str)\\\n            .apply(lambda x: ', '.join(x), axis=1).to_dict().items()\n        )\n    }\n\n# geodata\ndf_geo_inv = pd.DataFrame(geodata_inv).T\ndf_geo_inv.head()","a597bd40":"# DDoS geo-API a little o_0\nfrom joblib import Parallel, delayed\nfrom os import cpu_count\n\n# let's try to extract popular locations (set min_cnt to zero to allow all locations parsing)\n# however, you'd probably get 403 error for brute force :)\nmin_cnt = 15\naddress_to_code = loc_cnt[loc_cnt > min_cnt].index.tolist()\n# comment next 3 lines, it adds least frequent locations to see imperfect matches\nprint(len(address_to_code))\naddress_to_code += loc_cnt.tail(10).index.tolist() \nprint(len(address_to_code))\n\n# extract info using direct geocoding (partial)\ngeodata = dict(\n    zip(\n        address_to_code,\n        Parallel(n_jobs=cpu_count(), backend='threading')(\n            delayed(geocode_address)(\n                query=location, \n                geocoder=geocoder, \n                out_fields=['x', 'y', 'Country', 'Score', 'City']\n            )\n        for location in tqdm(address_to_code)\n        )\n    )\n)\n\n# geodata\ndf_geo = pd.DataFrame(geodata).T\ndf_geo.head()","357702d0":"# take a look at imperfect matches (if there are any)...\nmin_match_score = 90\ndf_geo[df_geo.match_score < min_match_score].sort_values(by='match_score', ascending=True)\n# see, what results you can obtain from imperfect matching :)","3d0106ae":"# ...and clean those outliers\nprint(df_geo.shape)\ndf_geo = df_geo[df_geo.match_score >= min_match_score]\nprint(df_geo.shape)","68b59050":"# map obtained results to initial tweet data and merge with inverse geocoded\ngeodata = pd.concat([\n    # direct geocoding\n    df[[col]].astype(str).applymap(str.lower).merge(\n        df_geo,\n        left_on='location',\n        right_index=True,\n        how='inner'\n    ).drop(columns=[col]),\n    # inverse geocoding\n    df_geo_inv\n]).drop(columns=['match_score'])\n\n# drop possible duplicates\nprint(df_geo.shape)\ngeodata = geodata[~geodata.index.duplicated(keep='first')]\nprint(df_geo.shape)\n\n# save for the latter usage (this was performed on full dataset)\n# geodata.reset_index().rename(columns={'index': 'id'})\\\n# .to_csv('geodata.csv', index=False, encoding='utf-8')\n\nprint(geodata.shape)\ngeodata.head()","81028e3e":"# append geodata to the initial dataframe\nprint(df.shape)\ndf = df.merge(geodata, left_index=True, right_index=True, how='left')\n# drop possible duplicates\ndf = df[~df.index.duplicated(keep='first')]\nprint(df.shape)\ndf.sample(10, random_state=911)","03f3614a":"# print location target (by country)\n# this is only the subset, as it's based on sampled geodata\ndf[df.is_train].groupby(df.country.replace('', np.nan)).agg({'target': 'mean', col: 'count'})\\\n.sort_values(by=[col, 'target'], ascending=[False, False]).head(20)","dcde39e4":"### 'Suspicious' locations\n---\nDumb regex to filter out email-like, hashtag-like, garbage-like, etc. location candidates","2c7353e2":"Let's map those addresses back to tweet's dataset","6a477c00":"Please unfold cells below to see code for data downloading","2d3265e1":"### Direct geocoding\n---\nGiven address string, extract geoinfo","eaf408c9":"While browsing `location` field I noticed some of the rows contain direct (lat,long) pairs within raw string, let's try to find them and use for reverse geocoding\n<br>We'll be using [this modified regex](https:\/\/stackoverflow.com\/questions\/3518504\/regular-expression-for-matching-latitude-longitude-coordinates)","1a4f9745":"### Reverse geocoding\n---\nGiven (lat,lon) pairs, extract geoinfo","4bee8922":"#### Let's see how to append this data as features\n---","fcdf70a5":"## Introduction\n---\nHi guys!\n<br>Please find below the code, dedicated to the data collection process","e6a10f8b":"<br>**Hint!**\n<br>You're also given `match_score` field (from 0 to 100) that indicates the matching quality of a result given initial request"}}