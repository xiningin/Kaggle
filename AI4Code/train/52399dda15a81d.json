{"cell_type":{"5f03eef6":"code","be060a66":"code","c01b1f98":"code","433c1c0c":"code","10436fb8":"code","c456fd30":"code","ee60905c":"code","44806598":"code","b231f99c":"code","7f425069":"code","4316c623":"code","d570f5ac":"code","9f4a4255":"code","11d17d99":"code","41dc81fb":"code","af4149d2":"code","854e5ae7":"code","e14dea44":"code","5001177f":"code","b1bfc43f":"code","dc5d76ba":"code","eb2bb0a4":"code","45013129":"code","42f7d2b0":"code","7c43a4db":"code","e0d92606":"code","bbc9cc13":"code","01c3d532":"code","bb7e4f37":"code","d63f4393":"code","22cdd2ab":"code","6caa75e2":"code","13213ce6":"code","00bbe9fa":"code","f5a64d82":"code","af7f08a0":"code","aa581a5d":"code","c1f0c6a5":"code","a888a11f":"code","79d623cf":"code","5df819ec":"code","e0c64561":"code","923174e9":"code","aed6a029":"code","3683fa76":"code","4387af23":"code","8ac5f19b":"code","606e96ab":"code","587af098":"code","d6a4eead":"code","c30dda04":"markdown","3a31e02e":"markdown","110c5476":"markdown","27a76e6e":"markdown","2515104c":"markdown","8e6dc0bc":"markdown","ef0a4718":"markdown","7c8f50c2":"markdown","80db81b7":"markdown","baf3a009":"markdown","2f02a867":"markdown","86c20a09":"markdown","859bbf17":"markdown","d73c93e6":"markdown","86519568":"markdown","f5ff393f":"markdown","080a60fe":"markdown","f4c78c1c":"markdown","c8f64b06":"markdown","fce8b087":"markdown"},"source":{"5f03eef6":"!pip install jcopml","be060a66":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport json\nimport pprint\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c01b1f98":"df = pd.read_csv(\"..\/input\/ndsc-beginner\/train.csv\")\ndf.tail(20)","433c1c0c":"df.info()","10436fb8":"with open(\"..\/input\/ndsc-beginner\/categories.json\", 'rb') as handle:\n    category_details = json.load(handle)","c456fd30":"pprint.pprint(category_details)","ee60905c":"category_mapper = {}\nproduct_type_mapper = {}\n\nfor cat in category_details.keys():\n    for key, value in category_details[cat].items():\n#         print(key)\n#         print(value)\n        category_mapper[value] = key\n        product_type_mapper[value] = cat","44806598":"category_mapper, product_type_mapper","b231f99c":"df['category_name'] = df['Category'].map(category_mapper)\ndf['product_type'] = df['Category'].map(product_type_mapper)","7f425069":"df.head()","4316c623":"df.groupby(['Category', 'category_name', 'product_type']).sum()","d570f5ac":"# df.category_name.value_counts()","9f4a4255":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\nfrom jcopml.pipeline import num_pipe, cat_pipe\nfrom jcopml.plot import plot_missing_value\nfrom jcopml.feature_importance import mean_score_decrease","11d17d99":"from nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom string import punctuation\n\n# sw is list of stopwords (ID + EN) and punctuation\nsw = stopwords.words(\"indonesian\") + stopwords.words(\"english\") + list(punctuation)","41dc81fb":"df.to_csv(\"df_complete_with_cat_prod.csv\")\ndf.head()","af4149d2":"# check dataset of category\n# print('length of category: ', len(df.Category.unique()))\n# print(df.Category.value_counts(normalize=True))","854e5ae7":"import torch\nimport os\n\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\nfrom nltk.tokenize import word_tokenize\nfrom gensim.models import FastText\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","e14dea44":"sentences = [word_tokenize(text.lower()) for text in tqdm(df.title)]","5001177f":"sentences[:5]","b1bfc43f":"model = FastText(sentences, size=128, window=5, min_count=3, workers=4, iter=100, sg=0, hs=0)","dc5d76ba":"from jcopml.utils import save_model\nsave_model(model, \"product_detection_title.fasttext\")","eb2bb0a4":"# save\nos.makedirs(\"model\/fasttext\/\", exist_ok=True)\nmodel.save(\"model\/fasttext\/product_detection_title.fasttext\")","45013129":"# load\nmodel = FastText.load(\"..\/input\/ndsc-2019-product-detection-title-fasttext\/product_detection_title.fasttext\")","42f7d2b0":"# text = [\n#     ['missha', 'line', 'fighting', 'foundation', 'moisture', '2', 'refills'],\n#     ['cushion', 'moisture','chafing', 'relief', 'gel', '2', 'refills']\n# ]","7c43a4db":"# model.train(text, total_examples=len(text), epochs=2)\n# model.save(\"model\/fasttext\/product_detection_title.fasttext\")","e0d92606":"w2v = model.wv","bbc9cc13":"# w2v.index2word\n# w2v.vectors\nw2v.vector_size","01c3d532":"# similar word\nw2v.similar_by_word('foundation', topn=5)","bb7e4f37":"from umap import UMAP\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px","d63f4393":"X = UMAP().fit_transform(w2v.vectors)\ndf = pd.DataFrame(X, columns=[\"umap1\", \"umap2\"])\ndf[\"title\"] = w2v.index2word","22cdd2ab":"fig = px.scatter(df, x=\"umap1\", y=\"umap2\", text=\"text\")\nfig.update_traces(textposition='top center')\nfig.update_layout(\n    height=800,\n    title_text='Reduced FastText Visualization'\n)\nfig.show()","6caa75e2":"w2v = FastText.load(\"..\/input\/ndsc-2019-product-detection-title-fasttext\/product_detection_title.fasttext\").wv","13213ce6":"def simple_encode_sentence(sentence, w2v, stopwords=None):\n    if stopwords is None:\n        vecs = [w2v[word] for word in word_tokenize(sentence)]\n    else:\n        vecs = [w2v[word] for word in word_tokenize(sentence) if word not in stopwords]\n        \n    sentence_vec = np.mean(vecs, axis=0) #setiap kalimat dicari rata2 dari vektor per kata\n    return sentence_vec\n\ndef better_encode_sentence(sentence, w2v, stopwords=None):\n    if stopwords is None:\n        vecs = [w2v[word] for word in word_tokenize(sentence)]\n    else:\n        vecs = [w2v[word] for word in word_tokenize(sentence) if word not in stopwords]\n    \n    vecs = [vec \/ np.linalg.norm(vec) for vec in vecs if np.linalg.norm(vec) > 0]\n    sentence_vec = np.mean(vecs, axis=0) #setiap kalimat dicari rata2 dari vektor per kata\n    return sentence_vec","00bbe9fa":"vecs = [better_encode_sentence(sentence, w2v, stopwords=sw) for sentence in tqdm(df.title)]\nvecs = np.array(vecs)\nvecs","f5a64d82":"df_vecs = pd.DataFrame(vecs)\ndf_vecs.to_csv('vectorized_sentences.csv')","af7f08a0":"X = vecs\ny = df.Category\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","aa581a5d":"X_train, y_train","c1f0c6a5":"from sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom jcopml.tuning import random_search_params as rsp\n\nfrom sklearn.feature_extraction.text import CountVectorizer","a888a11f":"pipeline = Pipeline([\n    ('algo', LogisticRegression(solver='lbfgs', n_jobs=-1, random_state=42, class_weight=\"balanced\"))\n])\n\nmodel_logreg = RandomizedSearchCV(pipeline, rsp.logreg_params, cv=4, scoring='f1_micro', n_iter=50, n_jobs=-1, verbose=1, random_state=42)\nmodel_logreg.fit(X_train, y_train)\n\n\nprint(model_logreg.best_params_)\nprint(model_logreg.score(X_train, y_train), model_logreg.best_score_, model_logreg.score(X_test, y_test))","79d623cf":"pipeline = Pipeline([\n#     ('prep', CountVectorizer(tokenizer=word_tokenize, stop_words=sw_indo)), # this step has been done in Encoding process with w2v, train set has been encoded\n    ('algo', SGDClassifier(random_state=42, tol=None))\n])\n\nparameter = {\n    'algo__loss': ['hinge', 'log'],\n    'algo__penalty': ['l2', 'l1'],\n    'algo__alpha': [0.0001, 0.0002, 0.0003], \n    'algo__max_iter': [5, 6, 7, 8, 9, 10],\n    'algo__tol': [0.0001, 0.0002, 0.0003]\n}\n\nmodel_sgd = RandomizedSearchCV(pipeline, parameter, cv=2, scoring='f1_micro', n_iter=20, n_jobs=-1, verbose=1, random_state=42)\nmodel_sgd.fit(X_train, y_train)\n\nprint(model_sgd.best_params_)\nprint(model_sgd.score(X_train, y_train), model_sgd.best_score_, model_sgd.score(X_test, y_test))\n\n# Fitting 2 folds for each of 20 candidates, totalling 40 fits\n# [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n# [Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed: 24.8min finished\n# \/opt\/conda\/lib\/python3.7\/site-packages\/sklearn\/linear_model\/_stochastic_gradient.py:573: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n#   ConvergenceWarning)\n# {'algo__tol': 0.0001, 'algo__penalty': 'l2', 'algo__max_iter': 10, 'algo__loss': 'hinge', 'algo__alpha': 0.0001}\n# 0.6196323965107295 0.6242902574949558 0.6201405608934693\n\n\n# pipeline = Pipeline([\n# #     ('prep', CountVectorizer(tokenizer=word_tokenize, stop_words=sw_indo)), # this step has been done in Encoding process with w2v, train set has been encoded\n#     ('algo', SGDClassifier(random_state=42, tol=None))\n# ])\n# ----------------------------------------------------------------------------------------------------------\n# Training based on the best parameter.\n\n# pipeline = Pipeline([\n# #     ('prep', CountVectorizer(tokenizer=word_tokenize, stop_words=sw_indo)), # this step has been done in Encoding process with w2v, train set has been encoded\n#     ('algo', SGDClassifier(tol=0.0001, penalty='l2', max_iter=10, loss='hinge', alpha=0.0001, random_state=42))\n# ])\n\n# model_sgd = pipeline\n# model_sgd.fit(X_train, y_train)\n\n# print(model_sgd.best_params_)\n# print(model_sgd.score(X_train, y_train), model_sgd.best_score_, model_sgd.score(X_test, y_test))","5df819ec":"# import os\n# from jcopml.utils import save_model","e0c64561":"os.makedirs(\"model\/linear_svm\/\", exist_ok=True)\nmodel_sgd.save(\"model\/linear_svm\/linear_svm.pkl\")","923174e9":"# model_logreg.save(\"model\/fasttext\/product_detection_title_model_logreg.pkl\")\nmodel_sgd.save(\"model\/fasttext\/product_detection_title_model_sgd.pkl\")","aed6a029":"df_submit = pd.read_csv(\"..\/input\/ndsc-beginner\/test.csv\")\ndf_submit.head()","3683fa76":"df.head()","4387af23":"submit_vecs = [better_encode_sentence(sentence, w2v, stopwords=sw) for sentence in tqdm(df_submit.title)]\nsubmit_vecs = np.array(submit_vecs)\nsubmit_vecs","8ac5f19b":"target = model_sgd.predict(submit_vecs)\ntarget","606e96ab":"df_submit_final = pd.DataFrame({\n    \"itemid\": df_submit.itemid,\n    \"Category\": target\n})\n\n# set itemid as index\ndf_submit_final.set_index('itemid', inplace=True)","587af098":"df_submit_final","d6a4eead":"df_submit_final.to_csv('product_detection.csv')","c30dda04":"## Dataset Splitting","3a31e02e":"# 1) CATEGORY CLASSIFICATION BASED ON TEXT (TITLE FEATURE)","110c5476":"#### Train FastText Model","27a76e6e":"#### ENCODING TEXT","2515104c":"#### join the dictionary into dataframe","8e6dc0bc":"#### Sanity check","ef0a4718":"#### Model information","7c8f50c2":"## Submit Prediction","80db81b7":"## Encoding with FastText","baf3a009":"rsp.logreg_params\n??model.fit()\nPipeline.fit does not accept the average parameter. You can pass parameters to specific steps of your pipeline using the stepname__parameter format, e.g. `Pipeline.fit(X, y, logisticregression__sample_weight=sample_weight)`.","2f02a867":"## Save Model","86c20a09":"# Add Information to df","859bbf17":"## Training","d73c93e6":"# Import Data","86519568":"# 2) CATEGORY CLASSIFICATION BASED ON IMAGE","f5ff393f":"#### create dictionary of category type & product type","080a60fe":"#### Continue training","f4c78c1c":"#### Prepare Corpus","c8f64b06":"#### Higher order visualization","fce8b087":"## Import Data"}}