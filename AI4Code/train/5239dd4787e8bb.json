{"cell_type":{"9fbc4ebc":"code","6049819d":"code","26e76a33":"code","48201d7c":"code","7e76b4f4":"code","21f5e694":"code","5850300a":"code","8c26b6f2":"code","7ed6e84b":"code","d14c4095":"code","19a99da0":"code","346e648d":"code","8017dd1a":"code","bf579cc1":"code","d86642ba":"code","1b66c10d":"code","983d351f":"code","c3704e69":"code","f789d113":"code","7496264c":"code","0a5197a5":"code","e308b5d0":"code","30217670":"code","101272e2":"code","a770b69f":"markdown","662151d6":"markdown","e04c36cb":"markdown","258f4ef0":"markdown","33ef1aef":"markdown","211d6cd7":"markdown","d65f1f31":"markdown","0f135818":"markdown","a24f7558":"markdown","554e4777":"markdown","85cead70":"markdown","53dff10d":"markdown","8882eee8":"markdown"},"source":{"9fbc4ebc":"import tensorflow as tf\nimport os\nimport numpy as np \nimport pandas as pd \nfrom sklearn.model_selection import KFold\nimport cv2\nimport seaborn as sns\nimport matplotlib.pylab as plt\nimport random\nfrom skimage.transform import rescale, resize\nfrom scipy.ndimage import rotate\nfrom skimage import data, img_as_float\nfrom skimage import exposure\nfrom sklearn.model_selection import KFold\nfrom sklearn.utils.class_weight import compute_class_weight\nimport warnings\nwarnings.filterwarnings('ignore')\nsns.set_style(\"white\")","6049819d":"dev_path = \"..\/input\/dont-stop-until-you-drop\/images\/train_images\/\"","26e76a33":"test_path = \"..\/input\/dont-stop-until-you-drop\/images\/test_images\/\"","48201d7c":"target = pd.read_csv(\"..\/input\/dont-stop-until-you-drop\/train.csv\")","7e76b4f4":"dev_data_list = np.asarray(os.listdir(dev_path))","21f5e694":"test_data_list = np.asarray(os.listdir(test_path))","5850300a":"target.head()","8c26b6f2":"plt.figure(figsize=(20,10))\nm = sns.countplot(x=\"class_6\", data=target)\nm.set_xticklabels([\"pose 1\",\"pose 2\", \"pose 3\",\"pose 4\", \"pose 5\", \"pose 6\"])\nm.set_xlabel(\"Yoga poses\")\nplt.show()","7ed6e84b":"for i in range(6):\n    pose = cv2.imread(dev_path + target.iloc[np.where(target[\"class_6\"]==i)[0][0]][0], cv2.IMREAD_COLOR )\n    plt.imshow(pose)\n    plt.show()\n","d14c4095":"width_dev = []\nheight_dev = []\nfor i in dev_data_list:\n    data = cv2.imread(dev_path + i, cv2.IMREAD_COLOR )\n    height_dev.append(data.shape[0])\n    width_dev.append(data.shape[1]) \n    \nwidth_test = []\nheight_test = []\nfor i in test_data_list:\n    data = cv2.imread(test_path + i, cv2.IMREAD_COLOR )\n    height_test.append(data.shape[0])\n    width_test.append(data.shape[1])    \n    \nplt.figure(figsize=(20,10))\nplt.title(\"Variation in image width and height\")\nplt.violinplot([width_dev,width_dev],positions = np.arange(2)*2.0)\nplt.violinplot([width_test,height_test],positions = np.arange(2)*2.0+0.5)\nplt.xticks(ticks = [0,0.5,2,2.5], labels = [\"Width development\",\"Width test\",\"Height development\",\"Height test\"], rotation=90)\n#plt.boxplot(width_dev)\nplt.show()","19a99da0":"def batch_generator(batch_size, gen_x): \n    batch_features = np.zeros((batch_size,256,256,3))\n    batch_labels = np.zeros((batch_size,6)) \n    while True:\n        for i in range(batch_size):\n            batch_features[i] , batch_labels[i] = next(gen_x)\n        yield batch_features, batch_labels","346e648d":"def generate_data(filelist, img_path, target):\n    while True:\n        for i in filelist:\n            y_train = np.array([0,0,0,0,0,0])\n            X_train = cv2.imread(img_path + i, cv2.IMREAD_COLOR )\n            X_train = cv2.resize(X_train, (256,256), interpolation= cv2.INTER_LINEAR )\n            y_train_num = target.iloc[int(np.where(target[\"image_id\"] == i)[0]),:][1]\n            y_train[y_train_num] = 1\n            yield X_train, y_train","8017dd1a":"def generate_data_augment(filelist, img_path, target):\n    while True:\n        for i in filelist:\n            y_train = np.array([0,0,0,0,0,0])\n            X_train = cv2.imread(img_path + i, cv2.IMREAD_COLOR )\n            X_train = cv2.resize(X_train, (256,256), interpolation= cv2.INTER_LINEAR )\n            y_train_num = target.iloc[int(np.where(target[\"image_id\"] == i)[0]),:][1]\n            y_train[y_train_num] = 1\n            # returns a random integer used to select augmentataion techniques for a given sample\n            augment_num = np.random.randint(0,6)\n            #augment_num = 0 \n            if augment_num == 0:\n                # do nothing\n                X_train = X_train\n            elif augment_num == 1:\n                X_train = X_train + np.random.rand(X_train.shape[0], X_train.shape[1], X_train.shape[2])*np.random.randint(-100,100)\n            elif augment_num == 2:\n                X_train = cv2.GaussianBlur(X_train,(random.randrange(1,50,2),random.randrange(1,50,2)), 0)\n            elif augment_num == 3:\n                rot = np.random.randint(-15,15)\n                X_train = rotate(X_train,rot, reshape=False)\n            elif augment_num == 4:\n                X_train = brightness(X_train,0.5,3)\n            elif augment_num == 5:\n                X_train = np.fliplr(X_train)\n            elif augment_num == 6:\n                X_train = np.flipud(X_train)\n            yield X_train, y_train","bf579cc1":"def gaussian_noise(img, mean=0, sigma=0.03):\n    img = img.copy()\n    noise = np.random.normal(mean, sigma, img.shape)\n    mask_overflow_upper = img+noise >= 1.0\n    mask_overflow_lower = img+noise < 0\n    noise[mask_overflow_upper] = 1.0\n    noise[mask_overflow_lower] = 0\n    img += noise\n    return img\n\n#https:\/\/towardsdatascience.com\/complete-image-augmentation-in-opencv-31a6b02694f5\ndef brightness(img, low, high):\n    value = random.uniform(low, high)\n    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n    hsv = np.array(hsv, dtype = np.float64)\n    hsv[:,:,1] = hsv[:,:,1]*value\n    hsv[:,:,1][hsv[:,:,1]>255]  = 255\n    hsv[:,:,2] = hsv[:,:,2]*value \n    hsv[:,:,2][hsv[:,:,2]>255]  = 255\n    hsv = np.array(hsv, dtype = np.uint8)\n    img = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n    return img\n\n#https:\/\/towardsdatascience.com\/complete-image-augmentation-in-opencv-31a6b02694f5\ndef fill(img, h, w):\n    img = img.astype('float32')\n    img = cv2.resize(img, (h, w), cv2.INTER_CUBIC)\n    return img\ndef zoom(img, mask, value):\n    if value > 1 or value < 0:\n        print('Value for zoom should be less than 1 and greater than 0')\n        return img, mask\n    value = random.uniform(value, 1)\n    h, w = img.shape[:2]\n    h_taken = int(value*h)\n    w_taken = int(value*w)\n    h_start = random.randint(0, h-h_taken)\n    w_start = random.randint(0, w-w_taken)\n    img = img[h_start:h_start+h_taken, w_start:w_start+w_taken, :]\n    img = fill(img, h, w)\n    mask = mask[h_start:h_start+h_taken, w_start:w_start+w_taken, :]\n    mask = fill(mask, h, w)\n    return img, mask\n\n# https:\/\/towardsdatascience.com\/complete-image-augmentation-in-opencv-31a6b02694f5\ndef vertical_shift(img, mask, ratio=0.0):\n    if ratio > 1 or ratio < 0:\n        print('Value should be less than 1 and greater than 0')\n        return img, mask\n    ratio = random.uniform(-ratio, ratio)\n    h, w = img.shape[:2]\n    to_shift = h*ratio\n    if ratio > 0:\n        img = img[:, :int(w-to_shift), :]\n        mask = mask[:, :int(w-to_shift), :]\n    if ratio < 0:\n        img = img[:, int(-1*to_shift):, :]\n        mask = mask[:, int(-1*to_shift):, :]\n    img = fill(img, h, w)\n    mask = fill(mask, h, w)\n    return img, mask\n\n# https:\/\/towardsdatascience.com\/complete-image-augmentation-in-opencv-31a6b02694f5 \ndef horizontal_shift(img, mask, ratio=0.0):\n    if ratio > 1 or ratio < 0:\n        print('Value should be less than 1 and greater than 0')\n        return img, mask\n    ratio = random.uniform(-ratio, ratio)\n    h, w = img.shape[:2]\n    to_shift = w*ratio\n    if ratio > 0:\n        img = img[:, :int(w-to_shift), :]\n        mask = mask[:, :int(w-to_shift), :]\n    if ratio < 0:\n        img = img[:, int(-1*to_shift):, :]\n        mask = mask[:, int(-1*to_shift):, :]\n    img = fill(img, h, w)\n    mask = fill(mask, h, w)\n    return img,mask","d86642ba":"labels = np.asarray(pd.get_dummies(target[\"class_6\"]))\n\ndef calculating_class_weights(y_true):\n    number_dim = np.shape(y_true)[1]\n    weights = np.empty([number_dim, 2])\n    for i in range(number_dim):\n        weights[i] = compute_class_weight('balanced', [0.,1.], y_true[:, i])\n    return weights\n\nnew_weights=calculating_class_weights(labels)\n\nkeys = np.arange(0,6,1)\nweight_dictionary = dict(zip(keys, new_weights.T[1]))\nweight_dictionary","1b66c10d":"base_model = tf.keras.applications.EfficientNetB1(\nweights= \"imagenet\", include_top=False, input_shape= (256,256,3)\n)","983d351f":"def create_model():\n    base_model = tf.keras.applications.EfficientNetB1(\n    weights= \"imagenet\", include_top=False, input_shape= (256,256,3)\n    )\n    num_classes=6\n\n    x = base_model.output\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    predictions = tf.keras.layers.Dense(num_classes, activation= 'softmax')(x)\n    model = tf.keras.Model(inputs = base_model.input, outputs = predictions)\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=\"categorical_crossentropy\", metrics=['acc'])\n    return model","c3704e69":"reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_acc', factor=0.1, patience=1, verbose=0, mode='max',\n    min_delta=0.0001, cooldown=2, min_lr=0\n)","f789d113":"num_epoch = 15\nbatch_size = 30\ndata_size = len(dev_data_list)\ndata_num = np.arange(data_size)\nsplits = 3\nkf = KFold(n_splits=splits)\nvalsize = data_size \/\/ splits\ntrainsize = data_size - valsize\nfor train_index, val_index in kf.split(data_num):\n    model = create_model()\n    history = model.fit(x=batch_generator(batch_size, generate_data_augment(dev_data_list[train_index], dev_path, target)), epochs=num_epoch, \n                            steps_per_epoch=(trainsize\/batch_size), \n                            validation_steps=(valsize\/batch_size),\n                            validation_data=batch_generator(batch_size, generate_data(dev_data_list[val_index], dev_path, target)), \n                            validation_freq=1, \n                            verbose = 0, \n                            callbacks=[reduce_lr],\n                            #class_weight=weight_dictionary\n                            )\n    print(\"---------------------------\")\n    print(\"#     Result CV-fold      #\")\n    print(\"---------------------------\")\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    fig.set_figheight(10)\n    fig.set_figwidth(30)\n    ax1.plot(history.history[\"val_loss\"])\n    ax1.plot(history.history[\"loss\"])\n    ax1.set_title(\"Loss\")\n    ax2.plot(history.history[\"acc\"])\n    ax2.plot(history.history[\"val_acc\"])\n    ax2.set_title(\"Accuracy\")\n    plt.show()\n    ","7496264c":"def scheduler(epoch, lr):\n    if epoch > 7:\n        return lr * 0.1\n    else:\n        return lr","0a5197a5":"lr_schedule = tf.keras.callbacks.LearningRateScheduler(scheduler,verbose=0)","e308b5d0":"num_epoch = 15\nbatch_size = 30\ndata_size = len(dev_data_list)\ntrainsize = data_size\nmodel = create_model()\nhistory = model.fit(x=batch_generator(batch_size, generate_data_augment(dev_data_list, dev_path, target)), epochs=num_epoch, \n                        steps_per_epoch=(trainsize\/batch_size),  \n                        verbose = 0, \n                        callbacks=[lr_schedule],\n                        )\nfig, (ax1, ax2) = plt.subplots(1, 2)\nfig.set_figheight(10)\nfig.set_figwidth(30)\nax1.plot(history.history[\"loss\"])\nax1.set_title(\"Loss\")\nax2.plot(history.history[\"acc\"])\nax2.set_title(\"Accuracy\")\nplt.show()","30217670":"df_sub = pd.read_csv(\"..\/input\/dont-stop-until-you-drop\/sample_submission.csv\")\ndf_sub = df_sub[0:0]\nfor i in os.listdir(test_path):\n    X_test = cv2.imread(test_path + i, cv2.IMREAD_COLOR )\n    X_test = cv2.resize(X_test, (256,256), interpolation= cv2.INTER_LINEAR )\n    y_hat = model.predict(np.expand_dims(X_test,0))\n    #print(np.argmax(y_hat))\n    df_sub = df_sub.append({\"image_id\":i,\"class_6\": np.argmax(y_hat)}, ignore_index=True)\n    ","101272e2":"df_sub.to_csv('submission.csv',index=False)","a770b69f":"## \ud83d\udca1 We tried to make weigth adjusted learning by compensating for the slightly imbalanced dataset \n(this was not a huge success so we did not use it in our final submission)","662151d6":"## \ud83d\udcc9 Define a learning rate reduction using accuarcy on the validation data to decrease the learning rate","e04c36cb":"# <center> Kaggle Days Shanghai 2021 - Don't stop until you drop!<\/center> \n## <center> Predicting 6 different yoga poses from images<\/center> \nThis notebook shows the best model developed by our team, *Overfitted*, in classifying yoga poses from images [1]. This was part of a challenge, organized by [Kaggle Days](https:\/\/kaggledays.com\/championship\/). We achieved an accuracy of 88.07% on the test set.","258f4ef0":"## \ud83d\udcca Show the distribution of widths and hights in the training and test data","33ef1aef":"# \u23f3 Final training\nIn the final training procedure we want to use all the data (and no validation data), therefore we have to find another way to decrease the learning rate. We therefore, manually define a learning rate scheduler (we try to decrease the learning rate at the same time as in the cross-validation above)","211d6cd7":"## \ud83d\udd27 Making batch generators for importing data to the CNN-model\n* We choose to resize all images to 256 x 256 pixels \n* In *generate_data_augment* we add 6 different augmentation techniques which we use only on the training data to make a more diverse and versatile data set","d65f1f31":"# \ud83d\udcbe Data exploration","0f135818":"## \ud83d\udcf7 Image examples of each of the 6 poses","a24f7558":"# \ud83d\udcda References\n[1] M. Verma, S. Kumawat, Y. Nakashima, and S. Raman, \u2018Yoga-82: A New Dataset for Fine-grained Classification of Human Poses\u2019, in IEEE\/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2020, pp. 4472\u20134479.\n","554e4777":"## \ud83d\udd29 Augmentation techniques","85cead70":"# \ud83d\udce5 Import EfficientNetB1","53dff10d":"## \ud83d\udd2e Do final prediction and make submission file based on the sample file","8882eee8":"## \ud83d\udcca Distribution of poses in the training data"}}