{"cell_type":{"80b658d8":"code","212f9a94":"code","dcfe9929":"code","4791e783":"code","e4d3ba8c":"code","d74a986e":"code","20d79bed":"code","ad6647ba":"code","e30980a6":"code","fb86c110":"code","7a6f189c":"markdown","e1d193ca":"markdown","76758556":"markdown","ee7e32c8":"markdown","f5b2cb7a":"markdown","cb76f626":"markdown","df956768":"markdown","1771535d":"markdown","241ff8e7":"markdown","733916cd":"markdown","9f2ab767":"markdown","efeeb44f":"markdown","c0f59171":"markdown","f44a078c":"markdown"},"source":{"80b658d8":"import os, sys\nimport time, math\nimport argparse, random\nfrom math import exp\nimport numpy as np\n\nimport torch\nfrom torch import nn, optim\nimport torch.nn.functional as F\nimport torch.utils.data as data\nfrom torch.utils.data import DataLoader\nfrom torch.backends import cudnn\nfrom torch.autograd import Variable\n\nimport torchvision\nimport torchvision.transforms as tfs\nfrom torchvision.transforms import ToPILImage\nfrom torchvision.transforms import functional as FF\nimport torchvision.utils as vutils\nfrom torchvision.utils import make_grid\nfrom torchvision.models import vgg16\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')","212f9a94":"# number of training steps\nsteps = 20000\n# Device name\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n# resume Training\nresume = False\n# number of evaluation steps\neval_step = 5000\n# learning rate\nlearning_rate = 0.0001\n# pre-trained model directory\npretrained_model_dir = '..\/input\/ffa-net-for-single-image-dehazing-pytorch\/trained_models\/'\n# directory to save models to\nmodel_dir = '.\/trained_models\/'\n# train data\ntrainset = 'its_train'\n# test data\ntestset = 'its_test'\n# model to be used\nnetwork = 'ffa'\n# residual_groups\ngps = 3\n# residual_blocks\nblocks = 12\n# batch size\nbs = 1\n# crop image\ncrop = True\n# Takes effect when crop = True\ncrop_size = 240\n# No lr cos schedule\nno_lr_sche = True\n# perceptual loss\nperloss = True\n\nmodel_name = trainset + '_' + network.split('.')[0] + '_' + str(gps) + '_' + str(blocks)\npretrained_model_dir = pretrained_model_dir + model_name + '.pk'\nmodel_dir = model_dir + model_name + '.pk'\nlog_dir = 'logs\/' + model_name\n\nif not os.path.exists('trained_models'):\n    os.mkdir('trained_models')\nif not os.path.exists('numpy_files'):\n    os.mkdir('numpy_files')\nif not os.path.exists('logs'):\n    os.mkdir('logs')\nif not os.path.exists('samples'):\n    os.mkdir('samples')\nif not os.path.exists(f\"samples\/{model_name}\"):\n    os.mkdir(f'samples\/{model_name}')\nif not os.path.exists(log_dir):\n    os.mkdir(log_dir)\n    \ncrop_size='whole_img'\nif crop:\n    crop_size = crop_size\n","dcfe9929":"def tensorShow(tensors,titles=None):\n    '''t:BCWH'''\n    fig=plt.figure()\n    for tensor, title, i in zip(tensors, titles, range(len(tensors))):\n        img = make_grid(tensor)\n        npimg = img.numpy()\n        ax = fig.add_subplot(211+i)\n        ax.imshow(np.transpose(npimg, (1, 2, 0)))\n        ax.set_title(title)\n    plt.show()\n    \ndef lr_schedule_cosdecay(t, T, init_lr=learning_rate):\n    lr=0.5*(1+math.cos(t*math.pi\/T))*init_lr\n    return lr","4791e783":"def default_conv(in_channels, out_channels, kernel_size, bias=True):\n    return nn.Conv2d(in_channels, out_channels, kernel_size, padding=(kernel_size\/\/2), bias=bias)\n    \n    \nclass PALayer(nn.Module):\n    def __init__(self, channel):\n        super(PALayer, self).__init__()\n        self.pa = nn.Sequential(\n                nn.Conv2d(channel, channel \/\/ 8, 1, padding=0, bias=True),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(channel \/\/ 8, 1, 1, padding=0, bias=True),\n                nn.Sigmoid()\n        )\n    def forward(self, x):\n        y = self.pa(x)\n        return x * y\n\n    \nclass CALayer(nn.Module):\n    def __init__(self, channel):\n        super(CALayer, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.ca = nn.Sequential(\n                nn.Conv2d(channel, channel \/\/ 8, 1, padding=0, bias=True),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(channel \/\/ 8, channel, 1, padding=0, bias=True),\n                nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        y = self.avg_pool(x)\n        y = self.ca(y)\n        return x * y\n\n    \nclass Block(nn.Module):\n    def __init__(self, conv, dim, kernel_size,):\n        super(Block, self).__init__()\n        self.conv1 = conv(dim, dim, kernel_size, bias=True)\n        self.act1 = nn.ReLU(inplace=True)\n        self.conv2 = conv(dim, dim, kernel_size, bias=True)\n        self.calayer = CALayer(dim)\n        self.palayer = PALayer(dim)\n\n    def forward(self, x):\n        res = self.act1(self.conv1(x))\n        res = res+x \n        res = self.conv2(res)\n        res = self.calayer(res)\n        res = self.palayer(res)\n        res += x \n        return res\n\n    \nclass Group(nn.Module):\n    def __init__(self, conv, dim, kernel_size, blocks):\n        super(Group, self).__init__()\n        modules = [Block(conv, dim, kernel_size)  for _ in range(blocks)]\n        modules.append(conv(dim, dim, kernel_size))\n        self.gp = nn.Sequential(*modules)\n\n    def forward(self, x):\n        res = self.gp(x)\n        res += x\n        return res\n\n    \nclass FFA(nn.Module):\n    def __init__(self,gps,blocks,conv=default_conv):\n        super(FFA, self).__init__()\n        self.gps = gps\n        self.dim = 64\n        kernel_size = 3\n        pre_process = [conv(3, self.dim, kernel_size)]\n        assert self.gps==3\n        self.g1 = Group(conv, self.dim, kernel_size,blocks=blocks)\n        self.g2 = Group(conv, self.dim, kernel_size,blocks=blocks)\n        self.g3 = Group(conv, self.dim, kernel_size,blocks=blocks)\n        self.ca = nn.Sequential(*[\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(self.dim*self.gps,self.dim\/\/16,1,padding=0),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(self.dim\/\/16, self.dim*self.gps, 1, padding=0, bias=True),\n            nn.Sigmoid()\n            ])\n        self.palayer = PALayer(self.dim)\n\n        post_process = [\n            conv(self.dim, self.dim, kernel_size),\n            conv(self.dim, 3, kernel_size)]\n\n        self.pre = nn.Sequential(*pre_process)\n        self.post = nn.Sequential(*post_process)\n\n    def forward(self, x1):\n        x = self.pre(x1)\n        res1 = self.g1(x)\n        res2 = self.g2(res1)\n        res3 = self.g3(res2)\n        w = self.ca(torch.cat([res1,res2,res3],dim=1))\n        w = w.view(-1,self.gps, self.dim)[:,:,:,None,None]\n        out = w[:,0,::] * res1 + w[:,1,::] * res2+w[:,2,::] * res3\n        out = self.palayer(out)\n        x = self.post(out)\n        return x + x1","e4d3ba8c":"# --- Perceptual loss network  --- #\nclass PerLoss(torch.nn.Module):\n    def __init__(self, vgg_model):\n        super(PerLoss, self).__init__()\n        self.vgg_layers = vgg_model\n        self.layer_name_mapping = {\n            '3': \"relu1_2\",\n            '8': \"relu2_2\",\n            '15': \"relu3_3\"\n        }\n\n    def output_features(self, x):\n        output = {}\n        for name, module in self.vgg_layers._modules.items():\n            x = module(x)\n            if name in self.layer_name_mapping:\n                output[self.layer_name_mapping[name]] = x\n        return list(output.values())\n\n    def forward(self, dehaze, gt):\n        loss = []\n        dehaze_features = self.output_features(dehaze)\n        gt_features = self.output_features(gt)\n        for dehaze_feature, gt_feature in zip(dehaze_features, gt_features):\n            loss.append(F.mse_loss(dehaze_feature, gt_feature))\n\n        return sum(loss)\/len(loss)","d74a986e":"def gaussian(window_size, sigma):\n    gauss = torch.Tensor([exp(-(x - window_size \/\/ 2) ** 2 \/ float(2 * sigma ** 2)) for x in range(window_size)])\n    return gauss \/ gauss.sum()\n\ndef create_window(window_size, channel):\n    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n    return window\n\ndef _ssim(img1, img2, window, window_size, channel, size_average=True):\n    mu1 = F.conv2d(img1, window, padding=window_size \/\/ 2, groups=channel)\n    mu2 = F.conv2d(img2, window, padding=window_size \/\/ 2, groups=channel)\n    mu1_sq = mu1.pow(2)\n    mu2_sq = mu2.pow(2)\n    mu1_mu2 = mu1 * mu2\n    sigma1_sq = F.conv2d(img1 * img1, window, padding=window_size \/\/ 2, groups=channel) - mu1_sq\n    sigma2_sq = F.conv2d(img2 * img2, window, padding=window_size \/\/ 2, groups=channel) - mu2_sq\n    sigma12 = F.conv2d(img1 * img2, window, padding=window_size \/\/ 2, groups=channel) - mu1_mu2\n    C1 = 0.01 ** 2\n    C2 = 0.03 ** 2\n    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) \/ ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n\n    if size_average:\n        return ssim_map.mean()\n    else:\n        return ssim_map.mean(1).mean(1).mean(1)\n\ndef ssim(img1, img2, window_size=11, size_average=True):\n    img1=torch.clamp(img1,min=0,max=1)\n    img2=torch.clamp(img2,min=0,max=1)\n    (_, channel, _, _) = img1.size()\n    window = create_window(window_size, channel)\n    if img1.is_cuda:\n        window = window.cuda(img1.get_device())\n    window = window.type_as(img1)\n    return _ssim(img1, img2, window, window_size, channel, size_average)\n\ndef psnr(pred, gt):\n    pred=pred.clamp(0,1).cpu().numpy()\n    gt=gt.clamp(0,1).cpu().numpy()\n    imdff = pred - gt\n    rmse = math.sqrt(np.mean(imdff ** 2))\n    if rmse == 0:\n        return 100\n    return 20 * math.log10( 1.0 \/ rmse)","20d79bed":"class RESIDE_Dataset(data.Dataset):\n    def __init__(self, path, train, size=crop_size, format='.png'):\n        super(RESIDE_Dataset, self).__init__()\n        self.size = size\n        self.train = train\n        self.format = format\n        self.haze_imgs_dir = os.listdir(os.path.join(path,'hazy'))\n        self.haze_imgs = [os.path.join(path, 'hazy', img) for img in self.haze_imgs_dir]\n        self.clear_dir = os.path.join(path,'clear')\n        \n    def __getitem__(self, index):\n        haze = Image.open(self.haze_imgs[index])\n        if isinstance(self.size, int):\n            while haze.size[0] < self.size or haze.size[1] < self.size :\n                index = random.randint(0, 20000)\n                haze = Image.open(self.haze_imgs[index])\n        img = self.haze_imgs[index]\n        id = img.split('\/')[-1].split('_')[0]\n        clear_name = id + self.format\n        clear = Image.open(os.path.join(self.clear_dir, clear_name))\n        clear = tfs.CenterCrop(haze.size[::-1])(clear)\n        if not isinstance(self.size, str):\n            i, j, h, w = tfs.RandomCrop.get_params(haze, output_size=(self.size, self.size))\n            haze = FF.crop(haze, i, j, h, w)\n            clear = FF.crop(clear, i, j, h, w)\n        haze, clear = self.augData(haze.convert(\"RGB\"), clear.convert(\"RGB\") )\n        return haze, clear\n    \n    def augData(self, data, target):\n        if self.train:\n            rand_hor = random.randint(0,1)\n            rand_rot = random.randint(0,3)\n            data = tfs.RandomHorizontalFlip(rand_hor)(data)\n            target = tfs.RandomHorizontalFlip(rand_hor)(target)\n            if rand_rot:\n                data = FF.rotate(data, 90*rand_rot)\n                target = FF.rotate(target, 90*rand_rot)\n        data = tfs.ToTensor()(data)\n        data = tfs.Normalize(mean=[0.64,0.6,0.58], std=[0.14,0.15,0.152])(data)\n        target = tfs.ToTensor()(target)\n        return data, target\n\n    def __len__(self):\n        return len(self.haze_imgs)\n\n\n# path to your 'data' folder\nits_train_path = '..\/input\/indoor-training-set-its-residestandard'\nits_test_path = '..\/input\/synthetic-objective-testing-set-sots-reside\/indoor'\n\nITS_train_loader = DataLoader(dataset=RESIDE_Dataset(its_train_path, train=True, size=crop_size), batch_size=bs, shuffle=True)\nITS_test_loader = DataLoader(dataset=RESIDE_Dataset(its_test_path, train=False, size='whole img'), batch_size=1, shuffle=False)","ad6647ba":"print('log_dir :', log_dir)\nprint('model_name:', model_name)\n\nmodels_ = {'ffa': FFA(gps = gps, blocks = blocks)}\nloaders_ = {'its_train': ITS_train_loader, 'its_test': ITS_test_loader}\n# loaders_ = {'its_train': ITS_train_loader, 'its_test': ITS_test_loader, 'ots_train': OTS_train_loader, 'ots_test': OTS_test_loader}\nstart_time = time.time()\nT = steps\n\ndef train(net, loader_train, loader_test, optim, criterion):\n    losses = []\n    start_step = 0\n    max_ssim = max_psnr = 0\n    ssims, psnrs = [], []\n    if resume and os.path.exists(pretrained_model_dir):\n        print(f'resume from {pretrained_model_dir}')\n        ckp = torch.load(pretrained_model_dir)\n        losses = ckp['losses']\n        net.load_state_dict(ckp['model'])\n        start_step = ckp['step']\n        max_ssim = ckp['max_ssim']\n        max_psnr = ckp['max_psnr']\n        psnrs = ckp['psnrs']\n        ssims = ckp['ssims']\n        print(f'Resuming training from step: {start_step} ***')\n    else :\n        print('Training from scratch *** ')\n    for step in range(start_step+1, steps+1):\n        net.train()\n        lr = learning_rate\n        if not no_lr_sche:\n            lr = lr_schedule_cosdecay(step,T)\n            for param_group in optim.param_groups:\n                param_group[\"lr\"] = lr\n        x, y = next(iter(loader_train))\n        x = x.to(device); y = y.to(device)\n        out = net(x)\n        loss = criterion[0](out,y)\n        if perloss:\n            loss2 = criterion[1](out,y)\n            loss = loss + 0.04*loss2\n\n        loss.backward()\n\n        optim.step()\n        optim.zero_grad()\n        losses.append(loss.item())\n        print(f'\\rtrain loss: {loss.item():.5f} | step: {step}\/{steps} | lr: {lr :.7f} | time_used: {(time.time()-start_time)\/60 :.1f}',end='',flush=True)\n\n        if step % eval_step ==0 :\n            with torch.no_grad():\n                ssim_eval, psnr_eval = test(net, loader_test, max_psnr, max_ssim, step)\n            print(f'\\nstep: {step} | ssim: {ssim_eval:.4f} | psnr: {psnr_eval:.4f}')\n\n            ssims.append(ssim_eval)\n            psnrs.append(psnr_eval)\n            if ssim_eval > max_ssim and psnr_eval > max_psnr :\n                max_ssim = max(max_ssim,ssim_eval)\n                max_psnr = max(max_psnr,psnr_eval)\n                torch.save({\n                            'step': step,\n                            'max_psnr': max_psnr,\n                            'max_ssim': max_ssim,\n                            'ssims': ssims,\n                            'psnrs': psnrs,\n                            'losses': losses,\n                            'model': net.state_dict()\n                }, model_dir)\n                print(f'\\n model saved at step : {step} | max_psnr: {max_psnr:.4f} | max_ssim: {max_ssim:.4f}')\n\n    np.save(f'.\/numpy_files\/{model_name}_{steps}_losses.npy',losses)\n    np.save(f'.\/numpy_files\/{model_name}_{steps}_ssims.npy',ssims)\n    np.save(f'.\/numpy_files\/{model_name}_{steps}_psnrs.npy',psnrs)\n\ndef test(net, loader_test, max_psnr, max_ssim, step):\n    net.eval()\n    torch.cuda.empty_cache()\n    ssims, psnrs = [], []\n    for i, (inputs, targets) in enumerate(loader_test):\n        inputs = inputs.to(device); targets = targets.to(device)\n        pred = net(inputs)\n        # # print(pred)\n        # tfs.ToPILImage()(torch.squeeze(targets.cpu())).save('111.png')\n        # vutils.save_image(targets.cpu(),'target.png')\n        # vutils.save_image(pred.cpu(),'pred.png')\n        ssim1 = ssim(pred, targets).item()\n        psnr1 = psnr(pred, targets)\n        ssims.append(ssim1)\n        psnrs.append(psnr1)\n        #if (psnr1>max_psnr or ssim1 > max_ssim) and s :\n#             ts=vutils.make_grid([torch.squeeze(inputs.cpu()),torch.squeeze(targets.cpu()),torch.squeeze(pred.clamp(0,1).cpu())])\n#             vutils.save_image(ts,f'samples\/{model_name}\/{step}_{psnr1:.4}_{ssim1:.4}.png')\n#             s=False\n    return np.mean(ssims) ,np.mean(psnrs)\n","e30980a6":"%%time\n\nloader_train = loaders_[trainset]\nloader_test = loaders_[testset]\nnet = models_[network]\nnet = net.to(device)\nif device == 'cuda':\n    net = torch.nn.DataParallel(net)\n    cudnn.benchmark = True\ncriterion = []\ncriterion.append(nn.L1Loss().to(device))\nif perloss:\n    vgg_model = vgg16(pretrained=True).features[:16]\n    vgg_model = vgg_model.to(device)\n    for param in vgg_model.parameters():\n        param.requires_grad = False\n    criterion.append(PerLoss(vgg_model).to(device))\noptimizer = optim.Adam(params = filter(lambda x: x.requires_grad, net.parameters()), lr=learning_rate, betas=(0.9,0.999), eps=1e-08)\noptimizer.zero_grad()\ntrain(net, loader_train, loader_test, optimizer, criterion)","fb86c110":"# its or ots\ntask = 'its'\n# test imgs folder\ntest_imgs = '..\/input\/synthetic-objective-testing-set-sots-reside\/indoor\/hazy\/'\n\ndataset = task\nimg_dir = test_imgs\n\noutput_dir = f'pred_FFA_{dataset}\/'\nprint(\"pred_dir:\",output_dir)\n\nif not os.path.exists(output_dir):\n    os.mkdir(output_dir)\n\nckp = torch.load(model_dir, map_location=device)\nnet = FFA(gps=gps, blocks=blocks)\nnet = nn.DataParallel(net)\nnet.load_state_dict(ckp['model'])\nnet.eval()\n\nfor im in os.listdir(img_dir):\n    haze = Image.open(img_dir+im)\n    haze1 = tfs.Compose([\n        tfs.ToTensor(),\n        tfs.Normalize(mean=[0.64, 0.6, 0.58],std=[0.14,0.15, 0.152])\n    ])(haze)[None,::]\n    haze_no = tfs.ToTensor()(haze)[None,::]\n    with torch.no_grad():\n        pred = net(haze1)\n    ts = torch.squeeze(pred.clamp(0,1).cpu())\n    # tensorShow([haze_no, pred.clamp(0,1).cpu()],['haze', 'pred'])\n    \n    haze_no = make_grid(haze_no, nrow=1, normalize=True)\n    ts = make_grid(ts, nrow=1, normalize=True)\n    image_grid = torch.cat((haze_no, ts), -1)\n    vutils.save_image(image_grid, output_dir+im.split('.')[0]+'_FFA.png')","7a6f189c":"### Get Dataloaders","e1d193ca":"### Utility Functions","76758556":"## Acknowledgements\n\n### This work was inspired by and borrows code from the authors' [original FFA-Net implementation](https:\/\/github.com\/zhilin007\/FFA-Net).","ee7e32c8":"### Define Train \/ Test Functions","f5b2cb7a":"### Model Definition","cb76f626":"<h3><center>FFA-Net Model Architecture<\/center><\/h3>\n<img src=\"https:\/\/storage.googleapis.com\/groundai-web-prod\/media%2Fusers%2Fuser_297673%2Fproject_398618%2Fimages%2Ffig2.jpg\" width=\"750\" height=\"750\"\/>\n<h4><\/h4>\n<h4><center><a href=\"https:\/\/arxiv.org\/abs\/1911.07559v2\">Source: FFA-Net [Xu Qin & Zhilin Wang et. al.]<\/a><\/center><\/h4>","df956768":"<h3><center>Image Dehazing with FFA-Net<\/center><\/h3>\n<img src=\"https:\/\/storage.googleapis.com\/groundai-web-prod\/media%2Fusers%2Fuser_297673%2Fproject_398618%2Fimages%2Ffig1.jpg\" width=\"500\" height=\"500\"\/>\n<h4><\/h4>\n<h4><center><a href=\"https:\/\/arxiv.org\/abs\/1911.07559v2\">Source: FFA-Net [Xu Qin & Zhilin Wang et. al.]<\/a><\/center><\/h4>","1771535d":"## Introduction\n\n### In this notebook we use [Feature Fusion Attention Network (FFA-Net)](https:\/\/arxiv.org\/abs\/1911.07559v2) to perform Single Image Dehazing on [RESIDE Dataset](https:\/\/sites.google.com\/view\/reside-dehaze-datasets\/reside-standard?authuser=0).","241ff8e7":"### SSIM \/ PSNR Metrics","733916cd":"### Test FFA-Net","9f2ab767":"### Perceptual Loss","efeeb44f":"### Train FFA-Net","c0f59171":"### Libraries \ud83d\udcda\u2b07","f44a078c":"### Settings \u2699\ufe0f"}}