{"cell_type":{"22c4f0ff":"code","e7d6998a":"code","0bb1c9e0":"code","d49fc1e4":"code","7ba2a38f":"code","96f918e1":"code","540b1fac":"code","5ff4fbd5":"code","8b1672d0":"code","7095c613":"code","bb18e5df":"code","8545527f":"code","d755aecd":"code","bf381fe6":"code","7f4988bb":"code","b5d60c87":"code","ce98192f":"code","8a1e68f3":"code","5d3a9f73":"code","bb7dd400":"code","5f7da4cf":"code","35aa0801":"code","7b4a9268":"code","9ac2ae62":"code","b4b598d1":"code","e16903bd":"code","25b7c839":"code","6bc695e3":"code","2ad039bc":"code","eea36c3d":"code","649ed785":"code","95d76790":"code","96bb277c":"code","26633c7d":"code","630eb272":"code","e520042d":"code","a68ef50b":"markdown","4c9e9b83":"markdown","f414ab0a":"markdown","2730e206":"markdown","1dc7cec2":"markdown"},"source":{"22c4f0ff":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","e7d6998a":"df = pd.read_csv(\"..\/input\/train.csv\",index_col=  0)","0bb1c9e0":"df.columns","d49fc1e4":"df.size","7ba2a38f":"# To change the names of DataFrame's Columns\ndf.columns = [\"label\",\"message\"]","96f918e1":"df.head()","540b1fac":"# Change all the messages to string format\ndf['message'] = df['message'].apply(lambda x : str(x))","5ff4fbd5":"df[\"message\"]","8b1672d0":"import nltk","7095c613":"df[\"length of message\"] = df[\"message\"].apply(len)","bb18e5df":"df.head()","8545527f":"# To plot a countplot to view how many SPAM messages are there and how many HAM\nsns.countplot(\"label\", data = df)","d755aecd":"# To visualize the length of the messages based on the label\nasdf = sns.FacetGrid(data = df, col = 'label')\nasdf.map(sns.distplot, 'length of message', kde = False, hist_kws = dict(edgecolor = \"k\"))","bf381fe6":"from nltk.corpus import stopwords\nimport string","7f4988bb":"# These are the most common word which we have to remove from text messages\nstopwords.words(\"french\")","b5d60c87":"# We need to remove punctuations too\nstring.punctuation","ce98192f":"# function for preprocessing\ndef all_words(msg):\n    no_punctuation = [char for char in msg if char not in string.punctuation]\n    no_punctuation = \"\".join(no_punctuation)\n    word = [word for word in no_punctuation.split() if word.lower() not in stopwords.words(\"english\")]\n    return word","8a1e68f3":"word=all_words(df[\"message\"])","5d3a9f73":"from collections import Counter","bb7dd400":"a=Counter(word)","5f7da4cf":"len(a)","35aa0801":"a.transform(df[\"message\"])","7b4a9268":"print(bag_of_words_transformer)","9ac2ae62":"len(bag_of_words_transformer)","b4b598d1":"# This will create the sparse matrix of all the messages based on the frequecy of words in that message\nmessage_bow = bag_of_words_transformer.transform(df['message'])","e16903bd":"# This is the shape of sparse matrix\n# 310 is no. of message\n# 1406 is the no. of words after preprocessing\nmessage_bow.shape","25b7c839":"tfid_transformer = TfidfTransformer().fit(message_bow)","6bc695e3":"message_tfid = tfid_transformer.transform(message_bow)","2ad039bc":"# Here Naive bayes has been used for traning\nfrom sklearn.naive_bayes import MultinomialNB\nspam_detection_model = MultinomialNB().fit(message_tfid,df['label'])","eea36c3d":"test = pd.read_csv(\"..\/input\/test.csv\", index_col = 0)","649ed785":"test.head()","95d76790":"test[\"'text'\"] = test[\"'text'\"].apply(lambda x: str(x))","96bb277c":"test_message_bow = bag_of_words_transformer.transform(test[\"'text'\"])","26633c7d":"test_message_tfid = tfid_transformer.transform(test_message_bow)","630eb272":"# Prediction\ntest[\"'label'\"] = spam_detection_model.predict(test_message_tfid)","e520042d":"sns.countplot(test[\"'label'\"])","a68ef50b":"*For testing we need to get the Tfidf of all the messages*","4c9e9b83":"# Traning","f414ab0a":"*So we need to do following things for preprocessing <br>*\n*1. Remove the Punctuations <br>*\n*2. Remove the most common words*","2730e206":"# Testing","1dc7cec2":"# Preprocessing"}}