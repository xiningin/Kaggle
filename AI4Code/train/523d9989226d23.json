{"cell_type":{"839c524c":"code","97a88e5f":"code","3bf59d55":"code","4ca30847":"code","0afbd2f6":"code","5fe3bcd2":"code","6d353ab3":"code","0084f255":"code","1399535e":"code","b05fb6fe":"code","69d43ce0":"code","671ed676":"code","bcbe2118":"code","1c43c8ed":"code","7b22cb65":"code","b20269e9":"code","fa409e4d":"code","1f84511a":"code","be838fc8":"code","c0ade0db":"code","c024ea3f":"code","ccac67f9":"code","18adfed1":"code","4f65946e":"code","25a33701":"code","f2d6a591":"code","9c036aba":"code","f5dc6e50":"code","80ef9d82":"code","def8bc83":"code","534b531c":"code","eba21cb9":"code","781ddae2":"code","c448b709":"code","869ef31e":"code","f3099b82":"code","ef3cff15":"code","c4c84a6b":"code","f558c282":"code","d386a93a":"code","98b23540":"code","f2a53472":"code","212e1e5d":"code","a5b8cc1e":"code","92aa06a8":"code","f01ec1ba":"code","9059a1cd":"code","23c1acbe":"code","4c6fb60b":"code","3b2a813b":"code","53b18879":"code","0a91537d":"code","a8e77fb6":"code","e4bc82dd":"code","f22e6656":"markdown","32a90a0b":"markdown","df0d46d4":"markdown","02605579":"markdown","72bae119":"markdown","1c497603":"markdown","66baaa78":"markdown","baa66861":"markdown","e8f2ea0d":"markdown","3268fca7":"markdown","1ea495ca":"markdown","eb49ad5b":"markdown","96d1329f":"markdown","91d1e337":"markdown","3b817bbe":"markdown","0d49cb39":"markdown","f808203e":"markdown","7817512f":"markdown","2eb63fc9":"markdown","28d23ea9":"markdown","8557ee78":"markdown","a26bc29e":"markdown"},"source":{"839c524c":"#Load the librarys\nimport pandas as pd #To work with dataset\nimport numpy as np #Math library\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns #Graph library that use matplot in background\nimport matplotlib.pyplot as plt #to plot some parameters in seaborn\nimport warnings\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import PowerTransformer, StandardScaler,Normalizer,RobustScaler,MaxAbsScaler,MinMaxScaler,QuantileTransformer\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.neighbors import KNeighborsClassifier\n# Import StandardScaler from scikit-learn\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.impute import KNNImputer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom matplotlib import pyplot\nfrom sklearn.manifold import TSNE\n# Import train_test_split()\n# Metrics\nfrom sklearn.metrics import roc_auc_score, average_precision_score\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import roc_curve\nfrom datetime import datetime, date\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.linear_model import LinearRegression, RidgeCV\n\nimport lightgbm as lgbm\nfrom catboost import CatBoostRegressor\nimport tensorflow as tf \nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import LearningRateScheduler\n#import smogn\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor\n# For training random forest model\nimport lightgbm as lgb\nfrom scipy import sparse\nfrom sklearn.neighbors import KNeighborsRegressor \nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans \n# Model selection\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression,f_classif\nfrom sklearn.feature_selection import mutual_info_regression\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\n\nfrom itertools import combinations\n#import smong \n# Plotlty : \nimport pprint\nfrom plotly.offline import iplot, init_notebook_mode\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly import tools\nimport plotly.io as pio\npp = pprint.PrettyPrinter(indent=4)\npio.templates.default = \"plotly_white\"\n\nfrom sklearn.metrics import mean_squared_log_error\nimport category_encoders as ce\nimport warnings\nimport optuna \nwarnings.filterwarnings('ignore')","97a88e5f":"%%time\npath = '\/kaggle\/input\/store-sales-time-series-forecasting\/'\noil_data = pd.read_csv(path+'oil.csv')\ntrain = pd.read_csv(path+'train.csv', parse_dates = True, low_memory = False)\ntest = pd.read_csv(path+'test.csv')\nsubmission_sample = pd.read_csv(path+'sample_submission.csv')\nholidays_data = pd.read_csv(path+'holidays_events.csv',parse_dates = True, low_memory = False)\nstore_data =  pd.read_csv(path+'stores.csv')\ntransaction_data = pd.read_csv(path+'transactions.csv', parse_dates = True, low_memory = False)","3bf59d55":"def reduce_mem_usage(df):\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n    if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.uint8).min and c_max < np.iinfo(np.uint8).max:\n                    df[col] = df[col].astype(np.uint8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.uint16).min and c_max < np.iinfo(np.uint16).max:\n                    df[col] = df[col].astype(np.uint16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.uint32).min and c_max < np.iinfo(np.uint32).max:\n                    df[col] = df[col].astype(np.uint32)                    \n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n                elif c_min > np.iinfo(np.uint64).min and c_max < np.iinfo(np.uint64).max:\n                    df[col] = df[col].astype(np.uint64)\n            elif str(col_type)[:5] == 'float':\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    return df","4ca30847":"train['year'] = pd.DatetimeIndex(train['date']).year\ntrain['month'] = pd.DatetimeIndex(train['date']).month\ntrain['day'] = pd.DatetimeIndex(train['date']).day\ntrain['day_of_week'] = pd.DatetimeIndex(train['date']).weekday\ntrain['day_name'] = pd.DatetimeIndex(train['date']).day_name()\ntrain['week_of_year'] = pd.DatetimeIndex(train['date']).weekofyear\ntrain['quarter'] = pd.DatetimeIndex(train['date']).quarter\ntrain['season'] = train.month%12 \/\/ 3 + 1\ntrain1=reduce_mem_usage(train)\n\nholidays_data['month'] = pd.DatetimeIndex(holidays_data['date']).month\nholidays_data['week_of_year'] = pd.DatetimeIndex(holidays_data['date']).weekofyear\nholidays_data['quarter'] = pd.DatetimeIndex(holidays_data['date']).quarter\nholidays_data['season'] = holidays_data.month%12 \/\/ 3 + 1\nholidays_data=holidays_data.drop(['date'], axis=1).head()\nholidays_data=reduce_mem_usage(holidays_data)\ntrain1 = pd.merge(train1, holidays_data,  how='left', left_on=['month','week_of_year','quarter','season'], right_on = ['month','week_of_year','quarter','season'])\noil_data=reduce_mem_usage(oil_data)\ntrain1 = pd.merge(train1, oil_data,  how='left', left_on=['date'], right_on = ['date'])\nstore_data=reduce_mem_usage(store_data)\ntrain1 = pd.merge(train1, store_data,  how='left', left_on=['store_nbr'], right_on = ['store_nbr'])\ntransaction_data=reduce_mem_usage(transaction_data)\ntrain1 = pd.merge(train1, transaction_data,  how='left', left_on=['store_nbr', 'date'], right_on = ['store_nbr','date'])\ntrain1 = train1.rename(columns = {\"type_x\" : \"holiday_type\", \"type_y\" : \"store_type\"})\ndel train \ndel holidays_data\ndel transaction_data\ndel oil_data\ndel store_data ","0afbd2f6":" train1=reduce_mem_usage(train1)","5fe3bcd2":"train1[['holiday_type', 'locale', 'locale_name', 'description', 'transferred']]=train1[['holiday_type', 'locale', 'locale_name', 'description', 'transferred']].fillna('NoEvent')\ntrain1[['dcoilwtico', 'transactions']]=train1[['dcoilwtico', 'transactions']].fillna(np.nan)\ntrain1[train1.select_dtypes(['float64','float16']).columns] = train1[train1.select_dtypes(['float64','float16']).columns].apply(pd.to_numeric)\ntrain1[train1.select_dtypes(['object','int64','int8']).columns] = train1.select_dtypes(['object','int64','int8']).apply(lambda x: x.astype('category'))","6d353ab3":"# summarize the number of rows with missing values for each column\nfor i in range(train1.shape[1]):\n    # count number of rows with missing values\n    n_miss = train1.iloc[:,i].isnull().sum()\n    perc = n_miss \/ train1.shape[0] * 100\n    print('> %d, Missing: %d (%.1f%%)' % (i, n_miss, perc))","0084f255":"cat_columns = train1.drop(['id','date','store_nbr'], axis=1).select_dtypes(exclude=['float64','float16']).columns\nnum_columns = train1.drop(['id','date','store_nbr'], axis=1).select_dtypes(exclude=['int64','category','int8']).columns","1399535e":"cat_columns","b05fb6fe":"sns.countplot(x='cluster', data=train1)\nplt.xticks(rotation=45) ","69d43ce0":"sns.countplot(x='store_type', data=train1)\nplt.xticks(rotation=45) ","671ed676":"sns.countplot(x='state', data=train1)\nplt.xticks(rotation=45) ","bcbe2118":"sns.countplot(x='city', data=train1)\nplt.xticks(rotation=45) ","1c43c8ed":"sns.countplot(x='description', data=train1)\nplt.xticks(rotation=45) ","7b22cb65":"sns.countplot(x='transferred', data=train1)\nplt.xticks(rotation=45) ","b20269e9":"sns.countplot(x='locale_name', data=train1)\nplt.xticks(rotation=45) ","fa409e4d":"sns.countplot(x='locale', data=train1)\nplt.xticks(rotation=45) ","1f84511a":"sns.countplot(x='holiday_type', data=train1)\nplt.xticks(rotation=45) ","be838fc8":"sns.countplot(x='day_of_week', data=train1)\nplt.xticks(rotation=45) ","c0ade0db":"sns.countplot(x='month', data=train1)\nplt.xticks(rotation=45) ","c024ea3f":"sns.countplot(x='year', data=train1)\nplt.xticks(rotation=45)  ","ccac67f9":"sns.countplot(x='family', data=train1)\nplt.xticks(rotation=80)  ","18adfed1":"num_columns","4f65946e":"\nfig = px.line(train1.iloc[0:8000], x='id', y=\"sales\")\nfig.show()","25a33701":"fig = px.scatter(train1.iloc[0:8000], x='id', y=\"sales\")\nfig.show()","f2d6a591":"groups = train1.iloc[0:8000].groupby(train1.iloc[0:8000]['store_nbr']).agg(['sum', 'mean', 'max','count'])\ngroups.head()","9c036aba":"train1.iloc[0:8000].groupby(train1.iloc[0:8000]['store_nbr']).sales.plot(figsize=(10, 6))","f5dc6e50":"list_store=train1.groupby(['store_nbr'])\nlist_store=[list_store.get_group(x) for x in list_store.groups]","80ef9d82":"for store in list_store:\n    store_groups=store.groupby(['family'])\n    list_item=[store_groups.get_group(x) for x in store_groups.groups]\n    for list_store_item in list_item:\n        list_store_item=pd.DataFrame(list_store_item).reset_index(drop=True)\n        store_id=list_store_item[\"store_nbr\"][0]\n        item_id=list_store_item[\"family\"][0]\n        list_store_item.to_csv(f\"file_{store_id}_{item_id}.csv\",index=None)\n        break","def8bc83":"list_store_item[\"family\"][0]","534b531c":"store1_family1= pd.read_csv('.\/file_1_AUTOMOTIVE.csv')\nstore1_family1.head()","eba21cb9":"store1_family1.sales.plot()\npyplot.show()","781ddae2":"# Define plot space\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Define x and y axes\nax.plot(store1_family1.date.iloc[0:10], \n        store1_family1.sales.iloc[0:10])\nplt.show()","c448b709":"groups = store1_family1.groupby('year')[ 'sales', 'onpromotion', \n       'holiday_type', 'locale', 'locale_name', 'description', 'transferred',\n       'dcoilwtico', 'city' ,'store_type', 'cluster', 'transactions'].agg(['sum', 'mean', 'max'])\ngroups","869ef31e":"store1_family1.groupby('year').sales.plot(figsize=(10, 6))","f3099b82":"\nstore1_family1.sales.hist()","ef3cff15":"store1_family1.sales.plot(kind='kde')\npyplot.show()","c4c84a6b":"series1=store1_family1.loc[:,['year','sales']]\nseries2=store1_family1.loc[:,['month','sales']]\nseries1.groupby('year').boxplot(figsize=(30, 30))","f558c282":"sns.boxplot(x = 'year', y = 'sales', data = series1)","d386a93a":"sns.boxplot(x = 'month', y = 'sales', data = store1_family1)","98b23540":"sns.boxplot(x = 'day_of_week', y = 'sales', data = store1_family1)","f2a53472":"plt.figure(figsize=(20,5))\n#series=store1_family1[store1_family1.year==2016]\nseriesh= store1_family1[[\"month\", \"day\", \"sales\"]].pivot_table(\"sales\", \"month\", \"day\", aggfunc='mean')\n\nax = sns.heatmap(seriesh, annot=True,linewidths=.5)","212e1e5d":"plt.figure(figsize=(20,5))\n#series=store1_family1[store1_family1.year==2016]\nseriesh= store1_family1[[\"year\", \"day\", \"sales\"]].pivot_table(\"sales\", \"year\", \"day\", aggfunc='mean')\n\nax = sns.heatmap(seriesh, annot=True,linewidths=.5)","a5b8cc1e":"plt.figure(figsize=(20,5))\n#series=store1_family1[store1_family1.year==2016]\n#days = {0:'Mon',1:'Tues',2:'Weds',3:'Thurs',4:'Fri',5:'Sat',6:'Sun'}\n#store1_family1['day_of_week'] = store1_family1['day_of_week'].apply(lambda x: days[x])\nstore1_family1['day_name'] = pd.DatetimeIndex(store1_family1['date']).day_name()\nseriesh= store1_family1[[\"year\", \"day_name\", \"sales\"]].pivot_table(\"sales\", \"year\", \"day_name\", aggfunc='mean')\n\nax = sns.heatmap(seriesh, annot=True,linewidths=.5)","92aa06a8":"from matplotlib import pyplot\nfrom pandas.plotting import lag_plot\nseries3=store1_family1.sales\nlag_plot(series3)\npyplot.show()","f01ec1ba":"from pandas import DataFrame\nfrom pandas import concat\nfrom pandas.plotting import scatter_matrix\nplt.figure(figsize=(20,10))\nvalues = DataFrame(series3.values)\nlags = 7\ncolumns = [values]\nfor i in range(1,(lags + 1)):\n\tcolumns.append(values.shift(i))\ndataframe = concat(columns, axis=1)\ncolumns = ['t+1']\nfor i in range(1,(lags + 1)):\n\tcolumns.append('t-' + str(i))\ndataframe.columns = columns\npyplot.figure(1)\nfor i in range(1,(lags + 1)):\n\tax = pyplot.subplot(240 + i)\n\tax.set_title('t+1 vs t-' + str(i))\n\tpyplot.scatter(x=dataframe['t+1'].values, y=dataframe['t-'+str(i)].values)\npyplot.show()","9059a1cd":"from pandas.plotting import autocorrelation_plot\nautocorrelation_plot(series3)\npyplot.show()","23c1acbe":"from statsmodels.graphics.tsaplots import plot_acf\nplot_acf(series3, lags=31)\npyplot.show()","4c6fb60b":"train_EDA = store1_family1\nfig = px.histogram(\n    train_EDA, \n    x=\"sales\",\n    marginal=\"box\",\n    color=\"locale\",\n    hover_data=train_EDA.columns,\n    nbins = 50\n)\n\nfig.update_layout(\n    title=\"sales distribution\"\n)\n\nfig.show()","3b2a813b":"agg = train1.groupby([\"year\", \"store_type\"]).agg({\"sales\"  :\"mean\", \"transactions\" : \"mean\"}).reset_index()\nfig = px.box(agg, y=\"sales\", facet_col=\"store_type\", color=\"store_type\",\n             boxmode=\"overlay\", points='all')\nfig.update_layout(title = \"Average Sales Distribution by Store Type\")\nfig.show()","53b18879":"# split into train and test sets\n# Create lagged dataset\nvalues = DataFrame(store1_family1.sales.values)\ndataframe = concat([values.shift(1), values], axis=1)\ndataframe.columns = ['t-1', 't+1']\nprint(dataframe.head(5))\nX = dataframe.values\ntrain_size = int(len(X) * 0.90)\ntrain, test1 = X[1:train_size], X[train_size:]\ntrain_X, train_y = train[:,0], train[:,1]\ntest_X, test_y = test1[:,0], test1[:,1]\n \n# persistence model\ndef model_persistence(x):\n\treturn x\n \n# walk-forward validation\npredictions = list()\nfor x in test_X:\n\tyhat = model_persistence(x)\n\tpredictions.append(yhat)\ntest_score = mean_squared_error(test_y, predictions)\ntest_RMSLE= mean_squared_log_error(test_y, predictions)\nprint('Test MSE: %.3f' % test_score)\nprint('Test RMSLE{}'.format(test_RMSLE))\n# plot predictions and expected results\npyplot.plot(train_y)\n#pyplot.plot(test_y)\npyplot.plot([None for i in train_y] + [x for x in test_y])\npyplot.plot([None for i in train_y] + [x for x in predictions])\npyplot.show()","0a91537d":"from statsmodels.tsa.ar_model import AR\n# split dataset\nX = store1_family1.sales.values\ntrain, test = X[0:-15], X[-15:]\n# train autoregression\nmodel = AR(train)\nmodel_fit = model.fit()\nprint('Lag: %s' % model_fit.k_ar)\nprint('Coefficients: %s' % model_fit.params)\n# make predictions\npredictions = model_fit.predict(start=len(train), end=len(train)+len(test)-1, dynamic=False)\nfor i in range(len(predictions)):\n\tprint('predicted=%f, expected=%f' % (predictions[i], test[i]))\ntest_RMSLE= mean_squared_log_error(test, predictions)    \nerror = mean_squared_error(test, predictions)\nprint('Test MSE: %.3f' % error)\nprint('Test RMSLE{}'.format(test_RMSLE))\n# plot results\npyplot.plot(test)\npyplot.plot(predictions, color='red')\npyplot.show()","a8e77fb6":"from statsmodels.tsa.arima_model import ARIMA\n\nX = store1_family1.sales.values\ntrain, test = X[0:-15], X[-15:]\n# fit model\nmodel = ARIMA(train\n              , order=(5,1,0))\nmodel_fit = model.fit()\nprint('Lag: %s' % model_fit.k_ar)\nprint('Coefficients: %s' % model_fit.params)\n# make predictions\npredictions = model_fit.predict(start=len(train), end=len(train)+len(test)-1, dynamic=False)\nfor i in range(len(predictions)):\n\tprint('predicted=%f, expected=%f' % (predictions[i], test[i]))\ntest_RMSLE= mean_squared_log_error(test, abs(predictions))    \nerror = mean_squared_error(test, predictions)\nprint('Test MSE: %.3f' % error)\nprint('Test RMSLE{}'.format(test_RMSLE))\n# plot results\npyplot.plot(test)\npyplot.plot(predictions, color='red')\npyplot.show()","e4bc82dd":"submitions=pd.read_csv('..\/input\/store-sales-time-series-forecasting\/sample_submission.csv')\nbasline=pd.read_csv('..\/input\/baseline\/BaselineSubmition.csv')\nsubmitions['sales']=basline.sales\n\nsubmitions.to_csv('BaselineSubmition.csv', index=False)\nsubmitions","f22e6656":"### Visual Exploratory \nThe first, and perhaps most popular, visualization for time series is the line plot.","32a90a0b":"## Time Series Lag Scatter Plots\nTime series modeling assumes a relationship between an observation and the previous observation.\n\nPrevious observations in a time series are called lags, with the observation at the previous time step called lag1, the observation at two time steps ago lag2, and so on.\n\nA useful type of plot to explore the relationship between each observation and a lag of that observation is called the scatter plot.\n\nPandas has a built-in function for exactly this called the lag plot. It plots the observation at time t on the x-axis and the lag1 observation (t-1) on the y-axis.\n\nIf the points cluster along a diagonal line from the bottom-left to the top-right of the plot, it suggests a positive correlation relationship.\nIf the points cluster along a diagonal line from the top-left to the bottom-right, it suggests a negative correlation relationship.\nEither relationship is good as they can be modeled.\n\nMore points tighter in to the diagonal line suggests a stronger relationship and more spread from the line suggests a weaker relationship.\n\nA ball in the middle or a spread across the plot suggests a weak or no relationship.\n\nBelow is an example of a lag plot for pressure  dataset.","df0d46d4":"We can quantify the strength and type of relationship between observations and their lags.\n\nIn statistics, this is called correlation, and when calculated against lag values in time series, it is called autocorrelation (self-correlation).\n\nA correlation value calculated between two groups of numbers, such as observations and their lag1 values, results in a number between -1 and 1. The sign of this number indicates a negative or positive correlation respectively. A value close to zero suggests a weak correlation, whereas a value closer to -1 or 1 indicates a strong correlation.\n\nCorrelation values, called correlation coefficients, can be calculated for each observation and different lag values. Once calculated, a plot can be created to help better understand how this relationship changes over the lag.\n\nThis type of plot is called an autocorrelation plot and Pandas provides this capability built in, called the autocorrelation_plot() function.\n\nThe example below creates an autocorrelation plot for the Minimum Daily Temperatures dataset","02605579":"<a id=\"top\"><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\"><center>Data Preparation<\/center><\/h3>\n\n## Data preprocessing\n\nData preprocessing comes after you've cleaned up your data and after you've done some exploratory analysis to understand your dataset. Once you understand your dataset, you'll probably have some idea about how you want to model your data. Machine learning models in Python require numerical input, so if your dataset has categorical variables, you'll need to transform them. Think of data preprocessing as a prerequisite for modeling:\n\nOutlier Handling\n\nScaling\n\nFeature Engineering\n\nFeature Selection \n\n\n","72bae119":"# Time series EDA ","1c497603":"* Grouping data based on item for each store\n\n* creating separate file for each item of each store","66baaa78":"#  ARIMA Forecast Model\nThe ARIMA is a classical linear model for time series forecasting. It combines the autoregressivemodel (AR), differencing to remove trends and seasonality, called integrated (I) and the movingaverage model (MA) which is an old name given to a model that forecasts the error, used tocorrect predictions. The Statsmodels Python library provides the ARIMA class2. In this lesson,you will develop an ARIMA model for a standard time series dataset.\n\nAutoregressive Integrated Moving Average Model\nAn ARIMA model is a class of statistical models for analyzing and forecasting time series data.\n\nIt explicitly caters to a suite of standard structures in time series data, and as such provides a simple yet powerful method for making skillful time series forecasts.\n\nARIMA is an acronym that stands for AutoRegressive Integrated Moving Average. It is a generalization of the simpler AutoRegressive Moving Average and adds the notion of integration.\n\nThis acronym is descriptive, capturing the key aspects of the model itself. Briefly, they are:\n\nAR: Autoregression. A model that uses the dependent relationship between an observation and some number of lagged observations.\nI: Integrated. The use of differencing of raw observations (e.g. subtracting an observation from an observation at the previous time step) in order to make the time series stationary.\nMA: Moving Average. A model that uses the dependency between an observation and a residual error from a moving average model applied to lagged observations.\n\nEach of these components are explicitly specified in the model as a parameter. A standard notation is used of ARIMA(p,d,q) where the parameters are substituted with integer values to quickly indicate the specific ARIMA model being used.\n\nThe parameters of the ARIMA model are defined as follows:\n\np: The number of lag observations included in the model, also called the lag order.\nd: The number of times that the raw observations are differenced, also called the degree of differencing.\nq: The size of the moving average window, also called the order of moving average.\n\nA linear regression model is constructed including the specified number and type of terms, and the data is prepared by a degree of differencing in order to make it stationary, i.e. to remove trend and seasonal structures that negatively affect the regression model.\n\nA value of 0 can be used for a parameter, which indicates to not use that element of the model. This way, the ARIMA model can be configured to perform the function of an ARMA model, and even a simple AR, I, or MA model.\n\nAdopting an ARIMA model for a time series assumes that the underlying process that generated the observations is an ARIMA process. This may seem obvious, but helps to motivate the need to confirm the assumptions of the model in the raw observations and in the residual errors of forecasts from the model.","baa66861":"#  Time Series Line Plot\nThe first, and perhaps most popular, visualization for time series is the line plot.\n\nIn this plot, time is shown on the x-axis with observation values along the y-axis","e8f2ea0d":"#  Time Series Histogram and Density Plots\nAnother important visualization is of the distribution of observations themselves.\n\nThis means a plot of the values without the temporal ordering.\n\nSome linear time series forecasting methods assume a well-behaved distribution of observations (i.e. a bell curve or normal distribution). This can be explicitly checked using tools like statistical hypothesis tests. But plots can provide a useful first check of the distribution of observations both on raw observations and after any type of data transform has been performed.","3268fca7":"# Data collection +Data Curation\n","1ea495ca":"<a id=\"top\"><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\"><center>Data Modeling<\/center><\/h3>\n\nModeling is the part of the Cross-Industry Standard Process for Data Mining (CRISP-DM) process model that i like best. Our data is already in good shape, and now we can search for useful patterns in our data.\n\n # Persistence Forecast Model\nIt is important to establish a baseline forecast. The simplest forecast you can make is to use thecurrent observation (t) to predict the observation at the next time step (t+1). This is calledthe naive forecast or the persistence forecast and may be the best possible model on some timeseries forecast problems.\n\n","eb49ad5b":"# Grouped TimeSeries :\n","96d1329f":"# Autoregressive ForecastModel\nAutoregression means developing a linear model that uses observations at previous time steps topredict observations at future time step (automeans self in ancient Greek). Autoregression is aquick and powerful time series forecasting method. The Statsmodels Python library providesthe autoregression model in the AR class1","91d1e337":"# Grouping data according to store","3b817bbe":"We can get a better idea of the shape of the distribution of observations by using a density plot.\n\nThis is like the histogram, except a function is used to fit the distribution of observations and a nice, smooth line is used to summarize this distribution.","0d49cb39":"\n## Step 2: Load the data\n\nNext, we'll load the training and test data.\n\nWe set index_col=0 in the code cell below to use the id column to index the DataFrame. (If you're not sure how this works, try temporarily removing index_col=0 and see how it changes the result.)\n","f808203e":" # Time Series Box and Whisker Plots by Interval\nHistograms and density plots provide insight into the distribution of all observations, but we may be interested in the distribution of values by time interval.\n\nAnother type of plot that is useful to summarize the distribution of observations is the box and whisker plot. This plot draws a box around the 25th and 75th percentiles of the data that captures the middle 50% of observations. A line is drawn at the 50th percentile (the median) and whiskers are drawn above and below the box to summarize the general extents of the observations. Dots are drawn for outliers outside the whiskers or extents of the data.\n\nBox and whisker plots can be created and compared for each interval in a time series, such as years, months, or days.","7817512f":"## Target Plot  as complet time series ","2eb63fc9":"The plot created from running the example shows a relatively strong positive correlation between observations and their lag1 values.\n\nWe can repeat this process for an observation and any lag values. Perhaps with the observation at the same time last week, last month, or last year, or any other domain-specific knowledge we may wish to explore.\n\n\nFirst, a new DataFrame is created with the lag values as new columns. The columns are named appropriately. Then a new subplot is created that plots each observation with a different lag value","28d23ea9":"\n<a id=\"top\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\"><center>CRISP-DM Methodology<\/center><\/h3>\n\n* [Buissness Understanding](#1)\n* [Data Understanding](#2)\n* [Data Preparation](#3)\n* [Data Modeling](#4)   \n* [Data Evaluation](#5)\n\nIn this section we overview our selected method for engineering our solution. CRISP-DM stands for Cross-Industry Standard Process for Data Mining. It is an open standard guide that describes common approaches that are used by data mining experts. CRISP-DM includes descriptions of the typical phases of a project, including tasks details and provides an overview of the data mining lifecycle. The lifecycle model consists of six phases with arrows indicating the most important and frequent dependencies between phases. The sequence of the phases is not strict. In fact, most projects move back and forth between phases as necessary. It starts with business understanding, and then moves to data understanding, data preparation, modelling, evaluation, and deployment. The CRISP-DM model is flexible and can be customized easily.\n## Buissness Understanding\n\n    Tasks:\n\n    1.Determine business objectives\n\n    2.Assess situation\n\n    3.Determine data mining goals\n\n    4.Produce project plan\n\n## Data Understanding\n     Tasks:\n\n    1.Collect data\n\n    2.Describe data\n\n    3.Explore data    \n\n## Data Preparation\n    Tasks\n    1.Data selection\n\n    2.Data preprocessing\n\n    3.Feature engineering\n\n    4.Dimensionality reduction\n\n            Steps:\n\n            Data cleaning\n\n            Data integration\n\n            Data sampling\n\n            Data dimensionality reduction\n\n            Data formatting\n\n            Data transformation\n\n            Scaling\n\n            Aggregation\n\n            Decomposition\n\n## Data Modeling :\n\nModeling is the part of the Cross-Industry Standard Process for Data Mining (CRISP-DM) process model that i like best. Our data is already in good shape, and now we can search for useful patterns in our data.\n\n    Tasks\n    1. Select modeling technique Select technique\n\n    2. Generate test design\n\n    3. Build model\n\n    4. Assess model\n\n## Data Evaluation :\n    Tasks\n\n    1.Evaluate Result\n\n    2.Review Process\n\n    3.Determine next steps\n\n<a id=\"top\"><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\">\n<center>Buissness Understanding<\/center><\/h3>\n\n**Goal of the Competition** In this \u201cgetting started\u201d competition, you\u2019ll use time-series forecasting to forecast store sales on data from Corporaci\u00f3n Favorita, a large Ecuadorian-based grocery retailer.\n\nSpecifically, you'll build a model that more accurately predicts the unit sales for thousands of items sold at different Favorita stores. You'll practice your machine learning skills with an approachable training dataset of dates, store, and item information, promotions, and unit sales.\n    \n<a id=\"top\"><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\">\n<center>Data Understanding<\/center><\/h3>\n    \nThis is part is done here : \n    \nhttps:\/\/www.kaggle.com\/bannourchaker\/sales-crisp-dm-dataunderstanding-part1\n    \n## Step 1: Import helpful libraries","8557ee78":"\nThe statsmodels library also provides a version of the plot in the plot_acf() function as a line plot.","a26bc29e":"<a id=\"top\"><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\"><center>Data Evaluation  <\/center><\/h3>\n\n\n\n\n\n\nRegression predictive modeling is the task of approximating a mapping function (f) from input variables (X) to a continuous output variable (y).\n\nRegression is different from classification, which involves predicting a category or class label.\n\nEvaluating Regression Models\n\nA common question by beginners to regression predictive modeling projects is:\n\n    How do I calculate accuracy for my regression model?\n\nAccuracy (e.g. classification accuracy) is a measure for classification, not regression.\n\nWe cannot calculate accuracy for a regression model.\n\nThe skill or performance of a regression model must be reported as an error in those predictions.\n\nThis makes sense if you think about it. If you are predicting a numeric value like a height or a dollar amount, you don\u2019t want to know if the model predicted the value exactly (this might be intractably difficult in practice); instead, we want to know how close the predictions were to the expected values.\n\nError addresses exactly this and summarizes on average how close predictions were to their expected values.\n\nThere are three error metrics that are commonly used for evaluating and reporting the performance of a regression model; they are:\n\n    Mean Squared Error (MSE).\n    Root Mean Squared Error (RMSE).\n    Mean Absolute Error (MAE)\n\n**Mean Absolute Error**, or MAE, is a popular metric because, like RMSE, the units of the error score match the units of the target value that is being predicted.\n\nUnlike the RMSE, the changes in MAE are linear and therefore intuitive.\n\nThat is, MSE and RMSE punish larger errors more than smaller errors, inflating or magnifying the mean error score. This is due to the square of the error value. The MAE does not give more or less weight to different types of errors and instead the scores increase linearly with increases in error.\n\nAs its name suggests, the MAE score is calculated as the average of the absolute error values. Absolute or abs() is a mathematical function that simply makes a number positive. Therefore, the difference between an expected and predicted value may be positive or negative and is forced to be positive when calculating the MAE.\n\nThe MAE can be calculated as follows:\n\n    MAE = 1 \/ N * sum for i to N abs(y_i \u2013 yhat_i)\n\nWhere y_i is the i\u2019th expected value in the dataset, yhat_i is the i\u2019th predicted value and abs() is the absolute function.\nThe evaluation metric for this competition is Root Mean Squared Logarithmic Error.\n\n**The RMSLE is calculated as:**\n\n![image.png](attachment:8cc8be90-d78d-4028-99d8-3f50e05563e1.png)\n\nhttps:\/\/medium.com\/analytics-vidhya\/root-mean-square-log-error-rmse-vs-rmlse-935c6cc1802a\n\n**we have done all EDA needed to chose the best preprocessing steps and begin modeling .\nWork is in progress to go more in depth with statistical models ..I will post the next notebook soon**  \n\n ***Upvote if you find it useful*** .\nreference : \nGrouped Time Series Data :\n\nprepra data : \nhttps:\/\/www.kaggle.com\/manishkumar77\/preparing-dataset\nfeature engnieer :\nhttps:\/\/www.kaggle.com\/hikmetsezen\/base-model-with-lightgbm-on-demand-forecasting\n\nhttps:\/\/www.kaggle.com\/haticeebraralc\/demand-forecasting\n\n\nhttps:\/\/www.kaggle.com\/ekrembayar\/store-item-demand-forecasting-with-lgbm"}}