{"cell_type":{"7b9ca8ec":"code","33e3b300":"code","a610554e":"code","e7b845ae":"code","bb58db85":"code","2da2bc96":"code","1daba8ee":"code","4e9ae49d":"code","6edc3771":"code","3d676374":"code","2bb74353":"code","c21dc381":"code","854af49c":"code","c02342f7":"code","ddbf969c":"code","6816ddc9":"code","afaf0b33":"code","f9e04245":"code","352817d9":"code","4e899a57":"code","1905dc02":"code","3508553f":"code","c1ddefe1":"code","bf209597":"code","1495bb83":"code","c70bbb0a":"code","923dd9c0":"code","3465729d":"code","b7ccb4ae":"code","99adbf82":"code","efa831af":"code","481333e9":"code","f67a343e":"code","3322232c":"code","507f06b3":"markdown","c4392593":"markdown","dd8762b1":"markdown","94698997":"markdown","7dfff81b":"markdown","87c24d26":"markdown","6a9ac379":"markdown","af33d877":"markdown","07ef2ff9":"markdown","ff845f16":"markdown","d1457680":"markdown","43eda1d9":"markdown","80f45227":"markdown","afcfafa3":"markdown","2b4116a5":"markdown","cc429793":"markdown","b76bb032":"markdown","ec6f9a54":"markdown","e2e91ce6":"markdown","777ae93d":"markdown","7191c391":"markdown","8400a065":"markdown","de78e1e6":"markdown","e164f640":"markdown","0230da68":"markdown","a76d399b":"markdown","c319f5ee":"markdown","795b1baf":"markdown","9e4a3773":"markdown","78d770b4":"markdown","125265df":"markdown"},"source":{"7b9ca8ec":"import pandas as pd\nimport numpy as np\nimport os,cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\nfrom keras.utils import plot_model\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.optimizers import Adam,SGD\nfrom keras.callbacks import ModelCheckpoint,EarlyStopping\nfrom keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D, Dropout\nfrom keras.models import Sequential,Input,Model\n\nfrom keras.initializers import *","33e3b300":"datasets = ['..\/input\/intel-image-classification\/seg_train\/seg_train','..\/input\/intel-image-classification\/seg_test\/seg_test']\n\noutput = []\n\nclass_names = ['buildings','forest','glacier','mountain','sea','street']\n\nclass_name_labels = {class_name:i for i,class_name in enumerate(class_names)}\n\nnb_classes = len(class_names)\nclass_name_labels","a610554e":"from tqdm import tqdm\n\ndef load_data():\n    for dataset in datasets:\n        print(\"Loading {}\".format(dataset))\n\n        images,labels = [],[]\n\n        for folder in os.listdir(dataset):\n            label = class_name_labels[folder]\n            \n            for file in tqdm(os.listdir(os.path.join(dataset,folder))):\n            \n                img_path = os.path.join(os.path.join(dataset,folder),file)\n    #             print(img_path)\n                img = cv2.imread(img_path,cv2.IMREAD_COLOR)\n                img = cv2.resize(img,(150,150))\n\n                images.append(img)\n                labels.append(label)\n                pass\n            pass\n        \n        images = np.array(images,dtype=np.float32)\n        labels = np.array(labels,dtype=np.float32)\n\n        output.append((images,labels))\n        pass\n\n    return output\n    pass","e7b845ae":"(train_images,train_labels),(test_images,test_labels) = load_data()","bb58db85":"n_train = train_labels.shape[0]\nn_test = test_labels.shape[0]\n\n_, train_count = np.unique(train_labels, return_counts=True)\n_, test_count = np.unique(test_labels, return_counts=True)\n\ndf = pd.DataFrame(data = (train_count,test_count))\ndf = df.T\ndf['Index']=['buildings','forest','glacier','mountain','sea','street']\ndf.columns = ['Train','Test','Name']\ndf","2da2bc96":"plt.figure()\ndf.set_index('Name').plot.bar(rot=0)\n# plt.xticks(df['Name'])","1daba8ee":"plt.pie(train_count,\n       explode=(0,0,0,0,0,0),\n       labels = class_names,\n       autopct = '%1.1f%%')\nplt.axis('equal')\nplt.title('Proportion of each observed quantity in train dataset')\nplt.show()","4e9ae49d":"plt.pie(test_count,\n       explode=(0,0,0,0,0,0),\n       labels = class_names,\n       autopct = '%1.1f%%')\nplt.axis('equal')\nplt.title('Proportion of each observed quantity in test dataset')\nplt.show()","6edc3771":"def show_final_history(history):\n    fig, ax = plt.subplots(1,2,figsize=(15,5))\n    ax[0].set_title('Loss')\n    ax[1].set_title(\"Accuracy\")\n    ax[0].plot(history.history['loss'],label='Train Loss')\n    ax[0].plot(history.history['val_loss'],label='Test loss')\n    ax[1].plot(history.history['accuracy'],label='Train Accuracy')\n    ax[1].plot(history.history['val_accuracy'],label='Test Accuracy')\n    \n#     ax.set_ylim(20)\n    \n    ax[0].legend(loc='upper right')\n    ax[1].legend(loc='lower right')","3d676374":"train_dir = '\/kaggle\/input\/intel-image-classification\/seg_train\/seg_train'\ntest_dir = '\/kaggle\/input\/intel-image-classification\/seg_test\/seg_test'\nbatch_size = 128\n\nIGD = ImageDataGenerator(rescale=1.\/255)\ntrain_generator = IGD.flow_from_directory(train_dir,\n                                         target_size=(150,150),\n                                         color_mode='rgb',\n                                         batch_size=batch_size,\n                                         class_mode='categorical',\n                                         shuffle=True,\n                                         seed = 42)\ntest_generator = IGD.flow_from_directory(test_dir,\n                                        target_size=(150,150),\n                                        color_mode='rgb',\n                                        batch_size=batch_size,\n                                        class_mode='categorical',\n                                        shuffle=True,\n                                        seed = 42)","2bb74353":"nb_classes = len(train_generator.class_indices)\nnb_classes","c21dc381":"def identity_block(X,f,filters,stage,block):\n    \n    conv_name_base = 'res_'+str(stage)+block+'_branch'\n    bn_name_base = 'bn_'+str(stage)+block+'_branch'\n    \n    F1,F2,F3 = filters\n    \n    X_shortcut = X\n    \n    # First Component of Main Path\n    X = Conv2D(filters=F1,kernel_size=(3,3),strides=(1,1),\n               padding='same',name=conv_name_base+'2a',\n               kernel_initializer = glorot_uniform(seed=0))(X)\n    X = BatchNormalization(name=bn_name_base+'2a')(X)\n    X = Activation('relu')(X)\n    \n    # Second Component of Main Path\n    X = Conv2D(filters=F2,kernel_size=(f,f),strides=(1,1),\n              padding='same',name=conv_name_base+'2b',\n              kernel_initializer=glorot_uniform(seed=0))(X)\n    X = BatchNormalization(name=bn_name_base+'2b')(X)\n    X = Activation('relu')(X)\n    \n    # Third Component of Main Path\n    X = Conv2D(filters=F3,kernel_size=(3,3),strides=(1,1),\n              padding='same',name=conv_name_base+'2c',\n              kernel_initializer=glorot_uniform(seed=0))(X)\n    X = BatchNormalization(name=bn_name_base+'2c')(X)\n    \n    X = Add()([X,X_shortcut])\n    X = Activation('relu')(X)\n    \n    return X\n    pass","854af49c":"def convolutional_block(X,f,filters,stage,block,s=2):\n    \n    conv_base_name = 'res_' + str(stage) + block + '_branch'\n    bn_base_name = 'bn_' + str(stage) + block + '_branch'\n    \n    F1,F2,F3 = filters\n    \n    X_shortcut = X\n    \n    ### MAIN PATH ###\n    # First component of main path\n    X = Conv2D(filters=F1,kernel_size=(3,3),strides=(s,s),\n              padding='same',name=conv_base_name+'2a',\n              kernel_initializer=glorot_uniform(seed=0))(X)\n    X = BatchNormalization(name=bn_base_name+'2a')(X)\n    X = Activation('relu')(X)\n    \n    # Second Component of main path\n    X = Conv2D(filters=F2,kernel_size=(f,f),strides=(1,1),\n              padding='same',name=conv_base_name+'2b',\n              kernel_initializer=glorot_uniform(seed=0))(X)\n    X = BatchNormalization(name=bn_base_name+'2b')(X)\n    X = Activation('relu')(X)\n    \n    # Third Component of main path\n    X = Conv2D(filters=F3,kernel_size=(3,3),strides=(1,1),\n              padding='same',name=conv_base_name+'2c',\n              kernel_initializer=glorot_uniform(seed=0))(X)\n    X = BatchNormalization(name=bn_base_name+'2c')(X)\n    \n    # Shortcut path\n    X_shortcut = Conv2D(filters=F3,kernel_size=(1,1),strides=(s,s),\n                       padding='same',name=conv_base_name+'1',\n                       kernel_initializer=glorot_uniform(seed=0))(X_shortcut)\n    X_shortcut = BatchNormalization(name=bn_base_name+'1')(X_shortcut)\n    \n    X = Add()([X,X_shortcut])\n    X = Activation('relu')(X)\n    \n    return X\n    pass","c02342f7":"def ResNet(input_shape,classes):\n    \n    X_input = Input(input_shape)\n    \n    # Zero Padding\n    X = ZeroPadding2D((3,3))(X_input)\n    \n    # Stage 1\n    X = Conv2D(64,(7,7),strides=(2,2),name='conv1',kernel_initializer=glorot_uniform(seed=0))(X)\n    X = BatchNormalization(name='bn_conv1')(X)\n    X = Activation('relu')(X)\n    X = MaxPooling2D((3,3),strides=(2,2))(X)\n    X = Dropout(0.25)(X)\n    \n    # Stage 2\n    X = convolutional_block(X,f=3,filters=[64,64,128],stage=2,block='A',s=1)\n    X = identity_block(X,3,[64,64,128],stage=2,block='B')\n    X = identity_block(X,3,[64,64,128],stage=2,block='C')\n    X = Dropout(0.25)(X)\n    \n    # Stage 3\n    X = convolutional_block(X,f=3,filters=[128,128,256],stage=3,block='A',s=2)\n    X = identity_block(X,f=3,filters=[128,128,256],stage=3,block='B')\n    X = identity_block(X,f=3,filters=[128,128,256],stage=3,block='C')\n    X = identity_block(X,f=3,filters=[128,128,256],stage=3,block='D')\n    X = Dropout(0.25)(X)\n    \n    # Stage 4\n    X = convolutional_block(X,f=3,filters=[256,256,512],stage=4,block='A',s=2)\n    X = identity_block(X,f=3,filters=[256,256,512],stage=4,block='B')\n    X = identity_block(X,f=3,filters=[256,256,512],stage=4,block='C')\n    X = identity_block(X,f=3,filters=[256,256,512],stage=4,block='D')\n    X = identity_block(X,f=3,filters=[256,256,512],stage=4,block='E')\n    X = identity_block(X,f=3,filters=[256,256,512],stage=4,block='F')\n    X = Dropout(0.25)(X)\n    \n    # Stage 5\n    X = convolutional_block(X,f=3,filters=[512,512,1024],stage=5,block='A',s=1)\n    X = identity_block(X,f=3,filters=[512,512,1024],stage=5,block='B')\n    X = identity_block(X,f=3,filters=[512,512,1024],stage=5,block='C')\n    X = Dropout(0.25)(X)\n    \n    # Stage 6\n    X = convolutional_block(X,f=3,filters=[1024,1024,2048],stage=6,block='A',s=2)\n    X = identity_block(X,f=3,filters=[1024,1024,2048],stage=6,block='B')\n    X = identity_block(X,f=3,filters=[1024,1024,2048],stage=6,block='C')\n    X = identity_block(X,f=3,filters=[1024,1024,2048],stage=6,block='D')\n    X = Dropout(0.25)(X)\n    \n    # Average Pool Layer\n    X = AveragePooling2D((2,2),name=\"avg_pool\")(X)\n    \n    # Output layer\n    X = Flatten()(X)\n    X = Dense(classes,activation='softmax',name='fc'+str(classes),\n              kernel_initializer=glorot_uniform(seed=0))(X)\n    \n    model = Model(inputs=X_input,outputs=X,name='ResNet')\n    \n    return model\n    pass","ddbf969c":"model = ResNet(input_shape=(150,150,3),classes=nb_classes)","6816ddc9":"plot_model(model, to_file='model.png')\nSVG(model_to_dot(model).create(prog='dot', format='svg'))\n\nmodel.summary()","afaf0b33":"opt = SGD(lr=0.01,momentum=0.7)\nmodel.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy'])","f9e04245":"checkpoint = ModelCheckpoint(\"model_weights.h5\",monitor='val_accuracy',verbose=1,\n                              save_best_only=True,mode='max')\ncallbacks_list=[checkpoint]","352817d9":"epochs = 100\n\nhistory = model.fit_generator(generator = train_generator,\n                              steps_per_epoch = train_generator.n\/\/batch_size,\n                              epochs = epochs,\n                              validation_data = test_generator,\n                              validation_steps = test_generator.n\/\/batch_size,\n                              callbacks = callbacks_list,\n                              verbose = 1)","4e899a57":"test_evaluate = model.evaluate_generator(test_generator)\nprint(\"Loss: {} Accuracy: {}\".format(test_evaluate[0], test_evaluate[1]*100))","1905dc02":"show_final_history(history)","3508553f":"from sklearn.metrics import confusion_matrix\nimport itertools\n\ntest_pred = model.predict_generator(generator=test_generator)\ny_pred = [np.argmax(probas) for probas in test_pred]\ny_test = test_generator.classes\nclass_names = test_generator.class_indices.keys()\n\ndef plot_confusion_matrix(cm,classes,title='Confusion Matrix',cmap=plt.cm.Blues):\n    \n    cm = cm.astype('float')\/cm.sum(axis=1)[:,np.newaxis]\n    plt.figure(figsize=(10,10))\n    plt.imshow(cm,interpolation='nearest',cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    \n    fmt = '.2f'\n    thresh = cm.max()\/2.\n    for i,j in itertools.product(range(cm.shape[0]),range(cm.shape[1])):\n        plt.text(j,i,format(cm[i,j],fmt),\n                horizontalalignment=\"center\",\n                color=\"white\" if cm[i,j] > thresh else \"black\")\n        pass\n    \n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    pass\n\ncnf_mat = confusion_matrix(y_test,y_pred)\nnp.set_printoptions(precision=2)\n\n\nplt.figure()\nplot_confusion_matrix(cnf_mat,classes=class_names)\nplt.show()","c1ddefe1":"pred_dir = '..\/input\/intel-image-classification\/seg_pred\/'\n\npred_generator = IGD.flow_from_directory(pred_dir,\n                                         target_size=(150,150),\n                                         color_mode='rgb',\n                                         batch_size=batch_size,\n                                         class_mode=None,\n                                         shuffle=False)","bf209597":"pred_generator.reset()\npred_result = model.predict_generator(pred_generator,verbose=1,steps=len(pred_generator))","1495bb83":"predicted_class = np.argmax(pred_result,axis=1)\n\nlabels = (train_generator.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npred_class = [labels[k] for k in predicted_class]\n\nfilenames = pred_generator.filenames\n# actual_class = test_generator_1.classes\nresults = pd.DataFrame({\"Filename\":filenames,\n                        \"Predictions\":pred_class})\nresults.head()","c70bbb0a":"results_class = results.groupby('Predictions').count()\n\nplt.figure()\nresults_visualisation = results_class.plot(kind='bar')\nresults_visualisation.legend_.remove()\n\nplt.title(\"Number of Images per Class\")\nplt.xlabel(\"Class label\")\nplt.ylabel(\"Total number of predictions\");","923dd9c0":"_,pred_count = np.unique(predicted_class,return_counts=True)\n\nplt.pie(pred_count,\n       explode=(0,0,0,0,0,0),\n       labels = class_names,\n       autopct = '%1.1f%%')\nplt.axis('equal')\nplt.title('Proportion of labels in validation dataset')\nplt.show()","3465729d":"from random import randint\n\nbase_path = '..\/input\/intel-image-classification\/seg_pred\/'\nl = len(os.listdir(base_path+'seg_pred\/'))\n# print(l)\nfor i in range(10):\n    \n    rnd_number = randint(0,l-1)\n#     print(rnd_number)\n    filename,pred_class = results.loc[rnd_number]\n    img_path = os.path.join(base_path,filename)\n#     print(img_path)\n    img = cv2.imread(img_path)\n    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    plt.imshow(img)\n    plt.title(pred_class)\n    plt.show()\n    pass","b7ccb4ae":"test_generator_1 = IGD.flow_from_directory(test_dir,\n                                          target_size=(150,150),\n                                          color_mode='rgb',\n                                          batch_size=batch_size,\n                                          class_mode='categorical',\n                                          shuffle=False)","99adbf82":"test_generator_1.reset()\ntest_result = model.predict_generator(test_generator_1,verbose=1,steps=len(test_generator))","efa831af":"test_class = np.argmax(test_result,axis=1)\n\ntest_class = [labels[k] for k in test_class]\n\nfilenames = test_generator_1.filenames\nactual_class = [labels[h] for h in test_generator_1.classes]\ntest_results = pd.DataFrame({\"Filename\":filenames,\n                        \"Predictions\":test_class,\n                        \"Actual Values\":actual_class})\ntest_results.head()","481333e9":"test_class_pred = test_results[\"Filename\"].groupby(test_results[\"Predictions\"]).count()\n\nplt.figure()\nplt.figure()\ntest_results_visualisation = test_class_pred.plot(kind='bar')\n# test_results_visualisation.legend_.remove()\n\nplt.title(\"Number of Images per Class\")\nplt.xlabel(\"Class label\")\nplt.ylabel(\"Total number of predictions\");","f67a343e":"_,train_count = np.unique(test_class,return_counts=True)\n\nplt.pie(train_count,\n       explode=(0,0,0,0,0,0),\n       labels = class_names,\n       autopct = '%1.1f%%')\nplt.axis('equal')\nplt.title('Proportion of labels in validation dataset')\nplt.show()","3322232c":"l = len(filenames)\n\nfor i in range(10):\n    \n    rnd_number = randint(0,l-1)\n#     print(rnd_number)\n    filename,pred_class,actual_class = test_results.loc[rnd_number]\n    img_path = os.path.join(test_dir,filename)\n#     print(img_path)\n    img = cv2.imread(img_path)\n    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    plt.imshow(img)\n    plt.title(\"Predicted class: {} {} Actual Class: {}\".format(pred_class,'\\n',actual_class))\n    plt.show()\n#     break\n    pass","507f06b3":"### Defining the Model.\n\n* Input shape = (150,150,3)\n\n* Classes = 6\n\n* The method ResNet is called which initializes the model defined by the user.","c4392593":"In simple terms, the model overfits. Maybe by removing the last stage or reducing the size of the filters the model may not overfit.\n\nOf course transfer learning with `ResNet50` module, which is available in Keras, will give an accuracy approaching 90% or higher.","dd8762b1":"# Residual Networks(50 Layers)\n\nThis model is made using residual networks with identity blocks and convolutional blocks","94698997":"Importing the necessary libraries","7dfff81b":"#### Pie chart of test data\n\nEven here there is no class imbalance with more `Glacier` images compared to `Mountain`, the opposite of the train data.\n\n`Buildings` in both datasets occupies the lowest position.","87c24d26":"Displaying a summary of the model and storing a flowchart of the model in `model.png`.","6a9ac379":"### Convolutional Block\n\nThis block consists of 4 Convolutional layers, with three in the main path and the fourth in the residual(secondary) path.\n\nThe two paths meet before the final activation layer.\n\nRelu is used for activation with padding `same` so as to maintain the input shape.","af33d877":"### Evaluating the Model\n\nThe model is evaluated using the evaluate_generator method of keras on the test dataset. The method outputs two values, the loss and accuracy.\n\nThe loss and accuracy will be different compared to the one obtained through the epochs. It will be close to the best accuracy obtained through the model.","07ef2ff9":"\n\nThe model is good but can be improved by changing the hyperparameters of the model such as learning rate.","ff845f16":"Running the model for 100 epochs.","d1457680":"Bar chart of the train and test data.","43eda1d9":"Padding is maintaned `same` as it allows for greater variations in the model as defined by a user.","80f45227":"### ResNet model\n\nThis a 50 layer(convolutional layers) model.\n\nIt consists of 5 stages:\n    \n    Stage 1: Taking the input with kernel size = (7,7) and number of filters = 64.\n    Stage 2: Consists of 1 convolutional block and 2 identity blocks\n    Stage 3: Consists of 1 convolutional block and 3 identity blocks\n    Stage 4: Consists of 1 convolutional block and 5 identity blocks\n    Stage 5: Consists of 1 convolutional block and 2 identity blocks\n    Stage 6: Consists of 1 convolutional block and 3 identity blocks\n    \nEach stage has a Dropout layer with rate set at 0.25(This is introduced in version 6, not in the previous versions)\n\nVersion 6: 66 layers(counting only convolutional layers), previous versions 50.","afcfafa3":"### Plotting a Confusion Matrix\n\nA confusion matrix is plotted to obtain how the model fared with respect to the test dataset. This gives a more accurate representation as compared to accuracy, since one gets to know which label the model can easily predict and one in which there is difficulty.\n\nA threshold is set at half of the maximum correlation. Above that values are shown in white while below that are shown in black.\n\nBluer the shade of the block stronger the correlation between the predicted value and actual value. This type of matrix can also be shown for train data to show how the model fared there.\n\nThe model can be changed using this confusion matrix as a base calculation.","2b4116a5":"Saving the weights of the model to `model_weights.h5`.","cc429793":"Converting the probabilities into class labels, namely, buildings, glaciers, mountains, etc. ","b76bb032":"Using Stochastic Gradient Descent instead of Adam as there is no erratic changes in the test accuracy and loss values. \n\nLearning rate is set at 0.003 and momentum to prevent the optimizer from settling in a local minimum is set at 0.85.\n\nIn version 4 learning rate was set at 0.0001 and momemtum at 0.9, which gave a train accuracy of 99.82% and test accuracy of 76.80%.\n\nVersion 6: learning rate = 0.01 and momentum = 0.7.","ec6f9a54":"The accuracy of the model over 100 epochs increased to 84.767% as compared to 76.8% in version 4.\n\nVersion 6: Accuracy increased to 81.766%","e2e91ce6":"Finding to which class the images belong to.","777ae93d":"### References\n\n[ResNet Keras by Priya Dwivedi](https:\/\/github.com\/priya-dwivedi\/Deep-Learning\/tree\/master\/resnet_keras).\nShe has explained the model in quite good detail along with the necessary visualisations along for an easier learning.\n\nPlease feel free to comment and fork this notebook. ","7191c391":"### Visualising predictions of validation dataset\n\n10 random images from the validation dataset along with the predicted classes are displayed to show how well the model works on unseen data.\n\nThe results dataframe provides the filename and predicted class.","8400a065":"#### Pie chart of train data\n\nIt can be observed from the pie chart that there is very little class imbalance, which is good for the model. Even then, data augmentation is advised as it helps in making the model more robust\n\nThe maximum number of images are present in the `Mountain` folder with `Glacier` second and `Street` a close third","de78e1e6":"As it can be seen from the above image `Glacier` is easily predicted with `Street` and `Sea` being a close second","e164f640":"### Loading Train and Test Images\n\nkeras.ImageDataGenerator is used for for making the train and test datasets. It allows for augmentation, dividing into batch size,shuffling without going through numpy and making large methods.","0230da68":"The model is confusing `sea` with `forest` for an unknown reason.\n\nBut it is evident that the model overfits and takes the noise as features too. This is evudent when using the test dataset by comparing actual class label and predicted class label.","a76d399b":"### Comparing Predicted labels with Test Data\n\nThe test dataset is also used to evaluate how the model predicts and what it is predicting. \n\nThe test dataset already has labels which helps the user in determining the actual label of an image.\n\nThe code is similar to the one used in the validation dataset.","c319f5ee":"### Visualising the loss and accuracy of the model (train and test)\n\nTwo plots are plotted, one for the loss and the other for the accuracy.\n\nIt allows for a user to determine whether the model overfitted or underfitted.\n\nAdvised to check the loss as compared to checking accuracy.","795b1baf":"### Visualisation of Dataset\n\nTo get a feel of the data, exploratory data analysis is done.\n\nThe total number of train and test images from each subdirectory are stored and used in the bar and pie charts","9e4a3773":"### Testing Model on Validation Dataset\n\nTo test how good the model runs in the real world, it is tested on a validation dataset which os not used by the model while training. The images are loaded using the ImageDataGenerator module and the model predicts their classes.","78d770b4":"### Identity Block\n\nThis block consists of three Convolutional blocks with the residual block same as the input joining just before the final Activation layer.\n\nRelu is used for activation with padding `same` so as to maintain the same shape as the input image.","125265df":"Converting the probabilities into class labels, namely, buildings, forest, sea, glacier."}}