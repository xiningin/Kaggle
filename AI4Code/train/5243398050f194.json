{"cell_type":{"e28f01d6":"code","69937f14":"code","44f22290":"code","ef7e5538":"code","384e0223":"code","a89b9757":"code","252ec40a":"code","fdaa8b37":"code","087203f9":"code","e3ec18ef":"code","f06b1fbb":"code","be9d80a6":"code","967a7adc":"code","e98837d8":"code","b7d1f4e5":"code","00134ee5":"code","979d181e":"code","752bf918":"code","18c2f231":"code","3b17fbc5":"code","32554c0e":"code","564f90e9":"code","12a63a9c":"code","07571e3c":"code","3ff155c7":"code","c7a7e644":"code","03632b16":"code","c0050d6a":"code","5123cfa6":"code","e2146440":"code","665e3b1a":"code","0f06b286":"markdown","6eeccb07":"markdown","9ba71de3":"markdown"},"source":{"e28f01d6":"import pandas as pd\nimport os\nfrom IPython import display\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.pyplot as plt\n#import seaborn as sns\n%matplotlib inline","69937f14":"def read_metadata(file):\n    return pd.read_csv(file, index_col=0, header=None)\n    \ndef read_data(file):\n    return pd.read_csv(file, index_col=[0, 1, 2], header=[0,1], na_values=['(X)', '-', '**'])\n\ndef read_prepped(file):\n    return pd.read_csv(file, header=[0,1], na_values=['-'])\n\ndef ingnore_DS_Store(directory):\n    return filter(lambda f: f != '_DS_Store', os.listdir(directory))\n\ndef collect_info_for_dep (dept_dir):\n    \"\"\"\n    This function collects the '.csv' files into pandas dataframes.\n    The return value is a hash where the keys refer to the original file names.\n    \"\"\"\n    base_dir = \"..\/input\/cpe-data\/{}\".format(dept_dir)\n    data_directories = list(filter(lambda f: f.endswith(\"_data\"), os.listdir(base_dir)))\n    info = {'dept' : dept_dir}\n    assert len(data_directories) == 1, \"found {} data directories\".format(len(data_directories))\n    for dd in data_directories:\n        directory = \"{}\/{}\".format(base_dir, dd)\n        dd_directories = ingnore_DS_Store(directory)\n        #print(dd_directories)\n        for ddd in dd_directories:\n            ddd_directory = \"{}\/{}\".format(directory, ddd)\n            files = list(ingnore_DS_Store(ddd_directory))\n            #print(files)\n            assert len(files) == 2, \"found {} files in {}\".format(len(files), directory)\n            full_file_names = [\"{}\/{}\".format(ddd_directory, file) for file in files]\n            dataframes = [read_metadata(file) if file.endswith('_metadata.csv') else read_data(file) for file in full_file_names]\n            info[ddd] = dict(zip(files, dataframes))\n    prepped_files = list(filter(lambda f: f.endswith(\"_prepped.csv\"), os.listdir(base_dir)))\n    for pf in prepped_files:\n        info[pf] = read_prepped(\"{}\/{}\".format(base_dir, pf))\n    return info","44f22290":"department_names = [\n#    'Dept_11-00091',\n#    'Dept_23-00089',\n    'Dept_35-00103',\n    'Dept_37-00027',\n    'Dept_37-00049',\n#    'Dept_49-00009',\n]\n\ndepartments = {dep: collect_info_for_dep(dep) for dep in department_names}","ef7e5538":"def investigate_dept(dept):\n    print(dept['dept'])\n    print('=' * 20)\n    print(dept.keys())","384e0223":"for dep in departments.keys():\n    investigate_dept(departments[dep])\n    print()","a89b9757":"prepped_dfs = [departments['Dept_35-00103']['35-00103_UOF-OIS-P_prepped.csv'], departments['Dept_37-00027']['37-00027_UOF-P_2014-2016_prepped.csv'], departments['Dept_37-00049']['37-00049_UOF-P_2016_prepped.csv']]","252ec40a":"print(prepped_dfs[0].shape)\nprint(prepped_dfs[1].shape)\nprint(prepped_dfs[2].shape)","fdaa8b37":"from functools import reduce\n\ncolumns = [list(zip(*pre.columns))[0] for pre in prepped_dfs]\ncommon_columns = reduce(lambda x, y: list(set(x).intersection(y)), columns)","087203f9":"common_columns","e3ec18ef":"# rearrange a little\n\ncommon_columns = [\n 'INCIDENT_DATE',\n 'LOCATION_LONGITUDE',\n 'LOCATION_LATITUDE',\n 'LOCATION_FULL_STREET_ADDRESS_OR_INTERSECTION',\n 'SUBJECT_GENDER',\n 'SUBJECT_RACE',\n 'SUBJECT_INJURY_TYPE',\n]","f06b1fbb":"prepped_dfs[1].head()","be9d80a6":"class TransformAndSelectForDept_35_00103(BaseEstimator, TransformerMixin):\n    \n    def __init__(self):\n        pass\n        \n    def fit(self, X, y = None):\n        #print(\"TransformAndSelectForDept_35_00103\")\n        return self\n    \n    def transform(self, X, y = None):\n        columns = common_columns\n        ret_df = X[columns].copy()\n        ret_df.columns = columns\n        ret_df['SUBJECT_GENDER'] = ret_df['SUBJECT_GENDER'].map({'Male': 'M', 'Female': 'F'})\n        return ret_df","967a7adc":"class TransformAndSelectForDept_37_00027(BaseEstimator, TransformerMixin):\n    \n    def __init__(self):\n        pass\n        \n    def fit(self, X, y = None):\n        #print(\"TransformAndSelectForDept_37_00027\")\n        return self\n    \n    def transform(self, X, y = None):\n        columns1 = [\n         'INCIDENT_DATE',\n        ]\n        ret_df = pd.DataFrame()\n        ret_df[('Y_COORDINATE', 'Y-Coordinate')] = - X[('Y_COORDINATE', 'Y-Coordinate')] \/ 100000.0        \n        ret_df[('Y_COORDINATE', 'X-Coordinate')] = X[('Y_COORDINATE', 'X-Coordinate')] \/ 100000.0\n        columns2 = [\n         'LOCATION_FULL_STREET_ADDRESS_OR_INTERSECTION',\n         'SUBJECT_GENDER',\n         'SUBJECT_RACE',\n         'SUBJECT_INJURY_TYPE',\n        ]\n        ret_df = pd.concat([X[columns1], ret_df, X[columns2]], axis=1)\n        ret_df.columns = common_columns\n        return ret_df","e98837d8":"class TransformAndSelectForDept_37_00049(BaseEstimator, TransformerMixin):\n    \n    def __init__(self):\n        pass\n        \n    def fit(self, X, y = None):\n        #print(\"TransformAndSelectForDept_37_00049\")\n        return self\n    \n    def transform(self, X, y = None):\n        columns = common_columns\n        ret_df = X[columns].copy()\n        ret_df.columns = columns\n        ret_df['SUBJECT_GENDER'] = ret_df['SUBJECT_GENDER'].map({'Male': 'M', 'Female': 'F'})\n        return ret_df","b7d1f4e5":"transformations = [\n    TransformAndSelectForDept_35_00103(),\n    TransformAndSelectForDept_37_00027(),\n    TransformAndSelectForDept_37_00049()\n]\n\ndfs = [trans.fit_transform(df1) for df1, trans in zip(prepped_dfs, transformations)]","00134ee5":"for d in dfs:\n    print(d.shape)","979d181e":"df = pd.concat(dfs)","752bf918":"df.shape","18c2f231":"df.info()","3b17fbc5":"df['INCIDENT_DATE'] = pd.to_datetime(df['INCIDENT_DATE'])\ndti = pd.DatetimeIndex(df['INCIDENT_DATE'])\ndf['year'] = dti.year\ndf['month'] = dti.month\ndf['dayofweek'] = dti.dayofweek\ndf['dayofyear'] = dti.dayofyear","32554c0e":"df = df.dropna()\n\nlb_gender = LabelEncoder()\nlb_race = LabelEncoder()\nlb_injury_type = LabelEncoder()\n\ndf[\"subject_gender_code\"] = lb_gender.fit_transform(df[\"SUBJECT_GENDER\"])\ndf['subject_race_code'] = lb_race.fit_transform(df[\"SUBJECT_RACE\"]) \ndf['subject_injury_type_code'] = lb_injury_type.fit_transform(df['SUBJECT_INJURY_TYPE'])","564f90e9":"df.info()","12a63a9c":"columns_for_prediction = [\n    'LOCATION_LONGITUDE',\n    'LOCATION_LATITUDE',\n    'year',              \n    'month',             \n    'dayofweek',         \n    'dayofyear',         \n    'subject_gender_code',\n    'subject_race_code',  \n    'subject_injury_type_code',    \n]","07571e3c":"len(columns_for_prediction)","3ff155c7":"# display large dataframes in an html iframe\ndef ldf_display(df, lines=500):\n    txt = (\"<iframe \" +\n           \"srcdoc='\" + df.head(lines).to_html() + \"' \" +\n           \"width=1000 height=500>\" +\n           \"<\/iframe>\")\n\n    return display.HTML(txt)","c7a7e644":"ldf_display(df, lines=20)","03632b16":"for col in df.columns:\n    print(col)\n    print('=' * 20)\n    print(df[col].value_counts())","c0050d6a":"prepped_dfs[0]['SUBJECT_GENDER']['INDIVIDUAL_GENDER'].value_counts()","5123cfa6":"prepped_dfs[1]['SUBJECT_GENDER']['Subject Sex'].value_counts()","e2146440":"prepped_dfs[2]['SUBJECT_GENDER']['CitSex'].value_counts()","665e3b1a":"#from pandas.tools.plotting import scatter_matrix\nfrom pandas.plotting import scatter_matrix\n\nscatter_matrix(df[columns_for_prediction], figsize=(20, 20), alpha=0.2, diagonal='kde')\nplt.show()","0f06b286":"Status as of this point:\n\n* I note only few main \"subject_injury_type\"s. Maybe that can be used as a good candidate for \"classification\" (collect all other types to \"OTHER\").\n* I handled missing values very lightly (removed them). Maybe worth looking into those (re: LON\/LAT). Also need to verify how I have obtained some of the LON\/LAT, is it valid (divided by 10000 and took counter intuitively Y as LON, and X as LAT?).\n* Still did not take values from the statistic \/ demographics of the locations.\n* Another type of model that I can consider, is to ignore the \"subject_injury_type\" and just focus on what is the likelihood of the subject to be of a specific race\/age, given the population there. Ex. if a region has 55% bright skin people and 45% dark skin people, why 80% of the cases are with dark skin subjects? TODO: think if direction interesting.\n..","6eeccb07":"Predicting Incidences\n=================\n\nLet's turn this into a classification\/regression task. Let's predict insidents based on demographic attributes. Once we manage to achieve a reasonably useful model, we can ask more questions like, what does the model actually do? What are the important factor for the decision? And hence we can investigate better fairness.\n\nIn order to get to classification\/regression, we'll need to collect data about locations, circumstances, demographic attributes to the neighbourhood, etc. Then we'll use Machince Learning and Data Mining, and see where this takes us.","9ba71de3":"Let's first try to build a model on the commmon fields. We can later consider other models."}}