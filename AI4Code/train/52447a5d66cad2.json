{"cell_type":{"8325f5d3":"code","11121d80":"code","ec7207ca":"code","c4728ec5":"code","627874ac":"code","c13f6b27":"markdown","b2a1b76f":"markdown","eeca67b7":"markdown","90de2c25":"markdown"},"source":{"8325f5d3":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pydicom as dicom\nimport glob\nimport os\nfrom tqdm import tqdm\nfrom PIL import Image\n\ntraindir = \"\/kaggle\/input\/rsna-intracranial-hemorrhage-detection\/stage_1_train_images\"\ntestdir = \"\/kaggle\/input\/rsna-intracranial-hemorrhage-detection\/stage_1_test_images\"\n\ntrain_dicom_files = glob.glob(f\"{traindir}\/*.dcm\")\nprint(f\"Number of train files: {len(train_dicom_files)}\")\n\ntest_dicom_files = glob.glob(f\"{testdir}\/*.dcm\")\nprint(f\"Number of test files: {len(test_dicom_files)}\")","11121d80":"# Helper function adapted from: \n# https:\/\/www.raddq.com\/dicom-processing-segmentation-visualization-in-python\/\n\ndef get_pixels_hu(scan):\n    image = np.stack(scan.pixel_array)\n    \n    # Convert to int16 (from sometimes int16), \n    # should be possible as values should always be low enough (<32k)\n    image = image.astype(np.int16)\n\n    # Set outside-of-scan pixels to 1\n    # The intercept is usually -1024, so air is approximately 0\n    image[image == -2000] = 0\n    \n    # Convert to Hounsfield units (HU)\n    intercept = scan.RescaleIntercept\n    slope = scan.RescaleSlope\n    \n    if slope != 1:\n        image = slope * image.astype(np.float64)\n        image = image.astype(np.int16)\n        \n    image += np.int16(intercept)\n    \n    return np.array(image, dtype=np.int16)\n\ndef hu2pil(img):\n    # as we want to detect hemorrages (HU between 0 to 100), we can normalizeas 0\n    # -1000: air\n    # -500: lung\n    # (-100, -50): Fat\n    # 0: Water\n    # (+30, +70): Blood\n    # (+10, +40): Muscle\n    # (+40, +60): Liver\n    # (+600, +3000): Bone\n    \n    # remove values greater than 3000 (out of scale)\n    img = np.less_equal(img, 3000)*img\n    # remove values greater lower than -1000 (out of scale)\n    img = np.greater_equal(img, -1000)*img\n    \n    # convert to positive values (0 to 4000 range)\n    img = img + 1000\n    \n    img = img\/4000 # (0 to 1 scaling)\n    \n    img = Image.fromarray(np.uint8(img*255.0))\n    \n    return img","ec7207ca":"outputdir = \".\/output\"\nos.system(f\"mkdir {outputdir} {outputdir}\/train {outputdir}\/test\")\n\nfor file in tqdm(train_dicom_files):\n    id = os.path.splitext(os.path.basename(file))[0]\n    patient = dicom.read_file(file)\n    img = get_pixels_hu(patient)\n    img = hu2pil(img)\n    img.save(f\"{outputdir}\/train\/{id}.jpg\")\n    break # REMOVE WHEN USING IT (IN THIS SCRIPT THE LOOP IS DISABLED)\n    \nfor file in tqdm(test_dicom_files):\n    id = os.path.splitext(os.path.basename(file))[0]\n    patient = dicom.read_file(file)\n    img = get_pixels_hu(patient)\n    img = hu2pil(img)\n    img.save(f\"{outputdir}\/test\/{id}.jpg\")\n    break # REMOVE WHEN USING IT (IN THIS SCRIPT THE LOOP IS DISABLED)","c4728ec5":"import hashlib, os\n\nunique = dict()\nduplicate = []\nfor filename in glob.glob(f\"{outputdir}\/*\/*.jpg\"):\n    if os.path.isfile(filename):\n        filehash = hashlib.md5(open(filename, 'rb').read()).hexdigest()\n\n        if filehash not in unique: \n            unique[filehash] = filename\n        else:\n            print (f\"{filename} is a duplicate of {unique[filehash]}\")\n            duplicate.append(filename)\nprint(f\"Number of duplicates: {len(duplicate)}\")\n","627874ac":"for file in duplicate:\n    print(f\"{file}\")\n    os.system(f\"rm {file}\")","c13f6b27":"## Detecting duplicate files","b2a1b76f":"> ## DICOM to JPEG converter\n\nThis script allows the conversion of the DICOM data files to JPEG format. \n\nPersonally I prefer to handle images, because you can benefit from the image compression formats and from the ease of visualization.","eeca67b7":"After running this script, you obtain an easier to handle BW 512x512 JPEG image dataset of about 30GB.","90de2c25":"## Removing duplicate files"}}