{"cell_type":{"1868ed45":"code","8284f5a5":"code","4f0a948e":"code","f7c9180b":"code","c8636505":"code","a35c1d75":"code","c91f0c34":"code","1335c75f":"code","359d303e":"code","48f6b8db":"code","ec36a15d":"code","858d9f5b":"code","0ef97dcf":"code","5552de18":"code","44957620":"code","029b7388":"code","b0e62358":"code","1738f352":"code","42c1fa1b":"code","458b0eee":"code","4008f1e5":"code","b157238a":"code","e697ea19":"code","bd21c112":"markdown","202e8887":"markdown","00f98e0c":"markdown","8623d21d":"markdown","ae60dbd5":"markdown","29e0fa95":"markdown","445fa15b":"markdown","99edeea9":"markdown","e529ba35":"markdown","98b8e762":"markdown","2ca1b7bb":"markdown","3859e58d":"markdown","edbf7f89":"markdown","44efa051":"markdown","f121609f":"markdown","2ee5bbca":"markdown","68aa16ad":"markdown","64d72389":"markdown","01e16935":"markdown","1b5076dc":"markdown","9828b6a2":"markdown","9a6e0dd9":"markdown","a23445df":"markdown","9d43adf5":"markdown","d7945e68":"markdown","398589c3":"markdown","78264d68":"markdown"},"source":{"1868ed45":"import os\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport warnings\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib import ticker\nimport seaborn as sns\n\n# setting up options\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('float_format', '{:f}'.format)\nwarnings.filterwarnings('ignore')\n\n# import datasets\ntrain_df = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv')\ntest_df = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/test.csv')\nsubmission = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv')","8284f5a5":"print(\"Shape: \",train_df.shape)\nprint(\"NULL values: \",sum(train_df.isna().sum()))\ntrain_df.head()","4f0a948e":"train_df.describe()","f7c9180b":"print(\"Shape: \",test_df.shape)\nprint(\"NULL values: \",sum(test_df.isna().sum()))\ntest_df.head()","c8636505":"test_df.describe()","a35c1d75":"submission.head()","c91f0c34":"missing_train_df = pd.DataFrame(train_df.isna().sum())\nmissing_train_df = missing_train_df.drop(['id', 'target']).reset_index()\nmissing_train_df.columns = ['feature', 'count']\n\nmissing_train_percent_df = missing_train_df.copy()\nmissing_train_percent_df['count'] = missing_train_df['count']\/train_df.shape[0]\n\nmissing_test_df = pd.DataFrame(test_df.isna().sum())\nmissing_test_df = missing_test_df.drop(['id']).reset_index()\nmissing_test_df.columns = ['feature', 'count']\n\nmissing_test_percent_df = missing_test_df.copy()\nmissing_test_percent_df['count'] = missing_test_df['count']\/test_df.shape[0]\n\nfeatures = [feature for feature in train_df.columns if feature not in ['id', 'target']]\nmissing_train_row = train_df[features].isna().sum(axis=1)\nmissing_train_row = pd.DataFrame(missing_train_row.value_counts()\/train_df.shape[0]).reset_index()\nmissing_train_row.columns = ['no', 'count']\n\nmissing_test_row = test_df[features].isna().sum(axis=1)\nmissing_test_row = pd.DataFrame(missing_test_row.value_counts()\/test_df.shape[0]).reset_index()\nmissing_test_row.columns = ['no', 'count']","1335c75f":"background_color = \"#f6f5f5\"\nplt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(10, 10), facecolor='#f6f5f5')\ngs = fig.add_gridspec(5, 5)\ngs.update(wspace=0.3, hspace=0.3)\n\nrun_no = 0\nfor row in range(0, 5):\n    for col in range(0, 5):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        for s in [\"top\",\"right\"]:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1  \n\nfeatures = list(train_df.columns[0:25])\n\nbackground_color = \"#f6f5f5\"\nrun_no = 0\nfor col in features:\n    sns.kdeplot(ax=locals()[\"ax\"+str(run_no)], x=train_df[col], zorder=2, alpha=1, linewidth=1, color='#ffd514')\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=4, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=4, width=0.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n    run_no += 1\n\nrun_no = 0\nfor col in features:\n    sns.kdeplot(ax=locals()[\"ax\"+str(run_no)], x=test_df[col], zorder=2, alpha=1, linewidth=1, color='#ff355d')\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=4, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=4, width=0.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n    run_no += 1\n\nplt.show()","359d303e":"background_color = \"#f6f5f5\"\nplt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(10, 10), facecolor='#f6f5f5')\ngs = fig.add_gridspec(5, 5)\ngs.update(wspace=0.3, hspace=0.3)\n\nrun_no = 0\nfor row in range(0, 5):\n    for col in range(0, 5):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        for s in [\"top\",\"right\"]:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1  \n\nfeatures = list(train_df.columns[26:51])\n\nbackground_color = \"#f6f5f5\"\nrun_no = 0\nfor col in features:\n    sns.kdeplot(ax=locals()[\"ax\"+str(run_no)], x=train_df[col], zorder=2, alpha=1, linewidth=1, color='#ffd514')\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=4, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=4, width=0.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n    run_no += 1\n\nrun_no = 0\nfor col in features:\n    sns.kdeplot(ax=locals()[\"ax\"+str(run_no)], x=test_df[col], zorder=2, alpha=1, linewidth=1, color='#ff355d')\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=4, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=4, width=0.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n    run_no += 1\n\nplt.show()","48f6b8db":"background_color = \"#f6f5f5\"\nplt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(10, 10), facecolor='#f6f5f5')\ngs = fig.add_gridspec(5, 5)\ngs.update(wspace=0.3, hspace=0.3)\n\nrun_no = 0\nfor row in range(0, 5):\n    for col in range(0, 5):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        for s in [\"top\",\"right\"]:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1  \n\nfeatures = list(train_df.columns[52:76])\n\nbackground_color = \"#f6f5f5\"\nrun_no = 0\nfor col in features:\n    sns.kdeplot(ax=locals()[\"ax\"+str(run_no)], x=train_df[col], zorder=2, alpha=1, linewidth=1, color='#ffd514')\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=4, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=4, width=0.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n    run_no += 1\n\nrun_no = 0\nfor col in features:\n    sns.kdeplot(ax=locals()[\"ax\"+str(run_no)], x=test_df[col], zorder=2, alpha=1, linewidth=1, color='#ff355d')\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=4, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=4, width=0.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n    run_no += 1\n\nplt.show()","ec36a15d":"background_color = \"#f6f5f5\"\nplt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(10, 10), facecolor='#f6f5f5')\ngs = fig.add_gridspec(5, 5)\ngs.update(wspace=0.3, hspace=0.3)\n\nrun_no = 0\nfor row in range(0, 5):\n    for col in range(0, 5):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        for s in [\"top\",\"right\"]:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1  \n\nfeatures = list(train_df.columns[77:99])\n\nbackground_color = \"#f6f5f5\"\nrun_no = 0\nfor col in features:\n    sns.kdeplot(ax=locals()[\"ax\"+str(run_no)], x=train_df[col], zorder=2, alpha=1, linewidth=1, color='#ffd514')\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=4, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=4, width=0.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n    run_no += 1\n\nrun_no = 0\nfor col in features:\n    sns.kdeplot(ax=locals()[\"ax\"+str(run_no)], x=test_df[col], zorder=2, alpha=1, linewidth=1, color='#ff355d')\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=4, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=4, width=0.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n    run_no += 1\n\nplt.show()","858d9f5b":"claim_df = pd.DataFrame(train_df['target'].value_counts()).reset_index()\nclaim_df.columns = ['target', 'count']\n\nclaim_percent_df = pd.DataFrame(train_df['target'].value_counts()\/train_df.shape[0]).reset_index()\nclaim_percent_df.columns = ['target', 'count']\n\nplt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(5, 1), facecolor='#f6f5f5')\ngs = fig.add_gridspec(1, 2)\ngs.update(wspace=0.3, hspace=0.05)\n\nbackground_color = \"#f6f5f5\"\nsns.set_palette(['#ffd514']*120)\n\nax0 = fig.add_subplot(gs[0, 0])\nfor s in [\"right\", \"top\"]:\n    ax0.spines[s].set_visible(False)\nax0.set_facecolor(background_color)\nax0_sns = sns.barplot(ax=ax0, y=claim_df['target'], x=claim_df['count'], \n                      zorder=2, linewidth=0, orient='h', saturation=1, alpha=1)\nax0_sns.set_xlabel(\"count\",fontsize=3, weight='bold')\nax0_sns.set_ylabel(\"\",fontsize=3, weight='bold')\nax0_sns.tick_params(labelsize=3, width=0.5, length=1.5)\nax0_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\nax0_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\nax0.text(0, -0.8, 'Claim', fontsize=4, ha='left', va='top', weight='bold')\nax0.text(0, -0.65, 'Both of 0 and 1 has almost the same numbers', fontsize=2.5, ha='left', va='top')\nax0.get_xaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n# data label\nfor p in ax0.patches:\n    value = f'{p.get_width():,.0f}'\n    x = p.get_x() + p.get_width() + 10000\n    y = p.get_y() + p.get_height() \/ 2 \n    ax0.text(x, y, value, ha='left', va='center', fontsize=2, \n            bbox=dict(facecolor='none', edgecolor='black', boxstyle='round', linewidth=0.2))\n    \nax1 = fig.add_subplot(gs[0, 1])\nfor s in [\"right\", \"top\"]:\n    ax1.spines[s].set_visible(False)\nax1.set_facecolor(background_color)\nax1_sns = sns.barplot(ax=ax1, y=claim_percent_df['target'], x=claim_percent_df['count'], \n                      zorder=2, linewidth=0, orient='h', saturation=1, alpha=1)\nax1_sns.set_xlabel(\"percentage\",fontsize=3, weight='bold')\nax1_sns.set_ylabel(\"\",fontsize=3, weight='bold')\nax1_sns.tick_params(labelsize=3, width=0.5, length=1.5)\nax1_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\nax1_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\nax1.text(0, -0.8, 'Target in %', fontsize=4, ha='left', va='top', weight='bold')\nax1.text(0, -0.65, 'Both of 0 and 1 distributrion are alomost the same of 50%', fontsize=2.5, ha='left', va='top')\n# data label\nfor p in ax1.patches:\n    value = f'{p.get_width():.2f}'\n    x = p.get_x() + p.get_width() + 0.01\n    y = p.get_y() + p.get_height() \/ 2 \n    ax1.text(x, y, value, ha='left', va='center', fontsize=2, \n            bbox=dict(facecolor='none', edgecolor='black', boxstyle='round', linewidth=0.2))","0ef97dcf":"from sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LinearRegression\nfrom scipy.stats import boxcox\nfrom xgboost import XGBClassifier \nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\n\n# import datasets\ntrain_df = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv')\n\nfolds = 5\nfeatures = list(train_df.columns[1:101])","5552de18":"train_oof = np.zeros((600000,))\nskf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train_df[features], train_df['target'])):\n    X_train, X_valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n    y_train = X_train['target']\n    y_valid = X_valid['target']\n    X_train = X_train.drop('target', axis=1)\n    X_valid = X_valid.drop('target', axis=1)\n\n    model = XGBClassifier(random_state=42, verbosity=0, tree_method='gpu_hist')\n\n    model =  model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_valid)[:, 1]\n    train_oof[valid_idx] = temp_oof\n    print(f'Fold {fold} AUC: ', roc_auc_score(y_valid, temp_oof))\n    \nprint(f'OOF AUC: ', roc_auc_score(train_df['target'], train_oof))","44957620":"train_oof = np.zeros((600000,))\nskf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train_df[features], train_df['target'])):\n    X_train, X_valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n    y_train = X_train['target']\n    y_valid = X_valid['target']\n    X_train = X_train.drop('target', axis=1)\n    X_valid = X_valid.drop('target', axis=1)\n\n    model = LGBMClassifier(random_state=42)\n\n    model =  model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_valid)[:, 1]\n    train_oof[valid_idx] = temp_oof\n    print(f'Fold {fold} AUC: ', roc_auc_score(y_valid, temp_oof))\nprint(f'OOF AUC: ', roc_auc_score(train_df['target'], train_oof))","029b7388":"train_oof = np.zeros((600000,))\nskf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train_df[features], train_df['target'])):\n    X_train, X_valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n    y_train = X_train['target']\n    y_valid = X_valid['target']\n    X_train = X_train.drop('target', axis=1)\n    X_valid = X_valid.drop('target', axis=1)\n\n    model = CatBoostClassifier(random_state=42)\n\n    model =  model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_valid)[:, 1]\n    train_oof[valid_idx] = temp_oof\n    print(f'Fold {fold} AUC: ', roc_auc_score(y_valid, temp_oof))\nprint(f'OOF AUC: ', roc_auc_score(train_df['target'], train_oof))","b0e62358":"train_df = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv')\ntrain_oof = np.zeros((600000,))\nskf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train_df[features], train_df['target'])):\n    X_train, X_valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n    y_train = X_train['target']\n    y_valid = X_valid['target']\n    X_train = X_train.drop('target', axis=1)\n    X_valid = X_valid.drop('target', axis=1)\n    \n    X_train = np.log(X_train)\n    X_valid = np.log(X_valid)\n    \n    model = LGBMClassifier(random_state=42)\n    \n    model =  model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_valid)[:, 1]\n    train_oof[valid_idx] = temp_oof\n    print(f'Fold {fold} AUC: ', roc_auc_score(y_valid, temp_oof))\n    \nprint(f'OOF AUC: ', roc_auc_score(train_df['target'], train_oof))","1738f352":"train_df = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv')\ntrain_oof = np.zeros((600000,))\nskf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train_df[features], train_df['target'])):\n    X_train, X_valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n    y_train = X_train['target']\n    y_valid = X_valid['target']\n    X_train = X_train.drop('target', axis=1)\n    X_valid = X_valid.drop('target', axis=1)\n    \n    X_train['min'] = X_train.min(axis=1)\n    X_valid['min'] = X_valid.min(axis=1)\n    \n    model = LGBMClassifier(random_state=42)\n    \n    model =  model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_valid)[:, 1]\n    train_oof[valid_idx] = temp_oof\n    print(f'Fold {fold} AUC: ', roc_auc_score(y_valid, temp_oof))\n    \nprint(f'OOF AUC: ', roc_auc_score(train_df['target'], train_oof))","42c1fa1b":"train_df = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv')\ntrain_oof = np.zeros((600000,))\nskf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train_df[features], train_df['target'])):\n    X_train, X_valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n    y_train = X_train['target']\n    y_valid = X_valid['target']\n    X_train = X_train.drop('target', axis=1)\n    X_valid = X_valid.drop('target', axis=1)\n    \n    X_train['max'] = X_train.max(axis=1)\n    X_valid['max'] = X_valid.max(axis=1)\n    \n    model = LGBMClassifier(random_state=42)\n    \n    model =  model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_valid)[:, 1]\n    train_oof[valid_idx] = temp_oof\n    print(f'Fold {fold} AUC: ', roc_auc_score(y_valid, temp_oof))\n    \nprint(f'OOF AUC: ', roc_auc_score(train_df['target'], train_oof))","458b0eee":"train_df = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv')\ntrain_oof = np.zeros((600000,))\nskf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train_df[features], train_df['target'])):\n    X_train, X_valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n    y_train = X_train['target']\n    y_valid = X_valid['target']\n    X_train = X_train.drop('target', axis=1)\n    X_valid = X_valid.drop('target', axis=1)\n    \n    X_train['sum'] = X_train.sum(axis=1)\n    X_valid['sum'] = X_valid.sum(axis=1)\n    \n    model = LGBMClassifier(random_state=42)\n    \n    model =  model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_valid)[:, 1]\n    train_oof[valid_idx] = temp_oof\n    print(f'Fold {fold} AUC: ', roc_auc_score(y_valid, temp_oof))\n    \nprint(f'OOF AUC: ', roc_auc_score(train_df['target'], train_oof))","4008f1e5":"train_df = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv')\ntrain_oof = np.zeros((600000,))\nskf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train_df[features], train_df['target'])):\n    X_train, X_valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n    y_train = X_train['target']\n    y_valid = X_valid['target']\n    X_train = X_train.drop('target', axis=1)\n    X_valid = X_valid.drop('target', axis=1)\n    \n    X_train['multiply'] = 1\n    X_valid['multiply'] = 1\n    for feature in features:\n        X_train['multiply'] = X_train[feature] * X_train['multiply']\n        X_valid['multiply'] = X_valid[feature] * X_valid['multiply']\n    \n    model = LGBMClassifier(random_state=42)\n    \n    model =  model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_valid)[:, 1]\n    train_oof[valid_idx] = temp_oof\n    print(f'Fold {fold} AUC: ', roc_auc_score(y_valid, temp_oof))\n    \nprint(f'OOF AUC: ', roc_auc_score(train_df['target'], train_oof))","b157238a":"train_df = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv')\ntrain_oof = np.zeros((600000,))\nskf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train_df[features], train_df['target'])):\n    X_train, X_valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n    y_train = X_train['target']\n    y_valid = X_valid['target']\n    X_train = X_train.drop('target', axis=1)\n    X_valid = X_valid.drop('target', axis=1)\n    \n    X_train['sum'] = X_train[features].sum(axis=1)\n    X_valid['sum'] = X_valid[features].sum(axis=1)\n    for feature in features:\n        X_train[feature+'_prorate'] = X_train[feature] \/ X_train['sum']\n        X_valid[feature+'_prorate'] = X_valid[feature] \/ X_valid['sum']\n    X_train = X_train.drop('sum', axis=1)\n    X_valid = X_valid.drop('sum', axis=1)\n    model = LGBMClassifier(random_state=42)\n    \n    model =  model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_valid)[:, 1]\n    train_oof[valid_idx] = temp_oof\n    print(f'Fold {fold} AUC: ', roc_auc_score(y_valid, temp_oof))\n    \nprint(f'OOF AUC: ', roc_auc_score(train_df['target'], train_oof))","e697ea19":"train_df = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv')\ntrain_oof = np.zeros((600000,))\nskf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train_df[features], train_df['target'])):\n    X_train, X_valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n    y_train = X_train['target']\n    y_valid = X_valid['target']\n    X_train = X_train.drop('target', axis=1)\n    X_valid = X_valid.drop('target', axis=1)\n    \n    X_train = np.exp(X_train)\n    X_valid = np.exp(X_valid)\n    model = LGBMClassifier(random_state=42)\n    \n    model =  model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_valid)[:, 1]\n    train_oof[valid_idx] = temp_oof\n    print(f'Fold {fold} AUC: ', roc_auc_score(y_valid, temp_oof))\n    \nprint(f'OOF AUC: ', roc_auc_score(train_df['target'], train_oof))","bd21c112":"# CatBoost Classifier","202e8887":"# Minimum:","00f98e0c":"# Importing Librabies and Loading datasets","8623d21d":"# Sum:","ae60dbd5":"# Dataset Overview\nThe intend of the overview is to get a feel of the data and its structure in train, test and submission file. An overview on train and test datasets will include a quick analysis on missing values and basic statistics, while sample submission will be loaded to see the expected submission.","29e0fa95":"# Distribution\nShowing distribution on each feature that are available in train and test dataset. As there are 118 features, it will be broken down into 25 features for each sections. Yellow represents train dataset while pink will represent test dataset\n\n# Observations:\n\nAll features distribution on train and test dataset are almost similar.\n# **Features f1 - f25**","445fa15b":"# Train dataset\nAs stated before, train dataset is mainly used to train predictive model as there is an available target variable in this set. This dataset is also used to explore more on the data itself including find a relation between each predictors and the target variable.\n\n# Observations:\n\n* Column named target is the target variable which is only available in the train dataset.\n* There are 102 columns: 100 features, 1 target variable claim and 1 column of id.\n* train dataset contain 600,000 observation with 0 missing values which need to be treated carefully.","99edeea9":"# Basic statistics for train\nBelow is the basic statistics for each variables which contain information on count, mean, standard deviation, minimum, 1st quartile, median, 3rd quartile and maximum for train dataset.","e529ba35":"# Multiply:","98b8e762":"# Target\n\n# Distribution\nTarget variable has a value of 0 to 1 which indicate people that not claim and claim the insurance. Let's see how the distribution of the claim variable.\n\n# Observations:\n\nThe number of people that not target and target (0 and 1) are almost the same of 296,394 and 303,606 respectively.\nIn term of percentage both of people that claim and not claim are around 50%.","2ca1b7bb":"# XGBoost Classifier","3859e58d":"# LGBM Classifier","edbf7f89":"# Base model\nModels that will be evaluated are XGBoost Classifier, LGBM Classifier and Catboost Classifier.\n\n# Observations:\n\nAll 3 models have quite a same AUC result at around 0.8. The differences are very small among the models.\n* Catboost Classifier has AUC of 0.803.\n* XGBoost Classifier has AUC 0.799.\n* LGBM Classifier has AUC of 0.801","44efa051":"# Exponential:","f121609f":"# **Features F77-F99**","2ee5bbca":"# Base model & feature engineering\nThis section will blindly try feature engineering, to see if there are any new features that are useful. This section will use LGBM Classifier as the base model.\n\n# Observations:\nWith the following feature engineering attempts:\n* Log\n* Minimum \n* Maximum \n* Sum \n* Multiplication \n* Prorate\n* Exponential\n\nIt is observed that AUC in all the attempts are quite similar.","68aa16ad":"# **Features F52-F76**","64d72389":"# Features\nNumber of features available to be used to create a prediction model are 100.\n#  Missing values\nCounting number of missing value and it's relative with their respective observations between train & test dataset.\n#  Preparation\nPrepare train and test dataset for data analysis and visualization.","01e16935":"# Test dataset\nTest dataset is used to make a prediction based on the model that has previously trained. Exploration in this dataset is also needed to see how the data is structured and especially on it\u2019s similiarity with the train dataset.\n\n# Observations:\n\nThere are 101 columns: 100 features and 1 column of id.\ntrain dataset contain 540,000 observation with 0 missing values which need to be treated carefully.","1b5076dc":"# **Features F26-F51**","9828b6a2":"# Prorate:","9a6e0dd9":"# Basic statistics for test\nBelow is the basic statistics for each variables which contain information on count, mean, standard deviation, minimum, 1st quartile, median, 3rd quartile and maximum for test dataset.","a23445df":"# Model\nEvaluate the performance of base model. Models will be evaluated using five cross validation without any hyperparameters tuning. (to see the packages used, please expand)","9d43adf5":"# Dataset Preparation Details: \nPreparing packages and data that will be used in the analysis process. Packages that will be loaded are mainly for data manipulation, data visualization and modeling. There are 2 datasets that are used in the analysis, they are train and test dataset. The main use of train dataset is to train models and use it to predict test dataset. While sample submission file is used to informed participants on the expected submission for the competition. (to see the details, please expand)","d7945e68":"# Maximum:","398589c3":"# Work in progress, Upvote and Spread Love..!!\n# Introduction\nKaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, Kaggle have launched many Playground competitions that are more approachable than Featured competition, and thus more beginner-friendly.\n\nThe goal of these competitions is to provide a fun, but less challenging, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition.\n\nThe dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting whether a claim will be made on an insurance policy. Although the features are anonymized, they have properties relating to real-world features.\n\nThis competition will asked to predict whether a customer made a claim upon an insurance policy. The ground truth claim is binary valued, but a prediction may be any number from 0.0 to 1.0, representing the probability of a claim. The features in this dataset have been anonymized and may contain missing values.\n\nSubmissions are evaluated on area under the ROC curve between the predicted probability and the observed target.","78264d68":"# Log:"}}