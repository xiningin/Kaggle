{"cell_type":{"fa2f7fb0":"code","3c2fa4bf":"code","29f0313d":"code","1e59c7df":"code","5236d6c4":"code","e9ff4a01":"code","0319bf1f":"code","a04a56d5":"code","c3e565ec":"code","bad65962":"code","ec1178f3":"code","dbeb3c9c":"code","91677558":"code","09d7bbf2":"code","1b4521c8":"code","ea8dfe8f":"code","15117f60":"code","7fd61674":"code","c75fb144":"code","7f5b1c18":"code","c08d34a3":"code","a15b7f36":"code","4a5b1401":"code","8c43eb7c":"code","072a102f":"code","fe03a95d":"code","78135898":"code","14b9377d":"code","b5f36d1d":"code","5f2762de":"code","73b6c4e1":"code","7bd2d2b2":"code","f3d78ea4":"code","5c71032d":"code","60321999":"code","e0d74c44":"code","ac9acbcf":"code","9df24cfe":"code","c6815ea3":"code","10e8b7e1":"code","5bab7115":"code","f0d3f5f0":"code","68c19aac":"code","01e4b143":"code","10926c02":"code","99eec569":"code","30efec8f":"code","618f97ef":"code","f0ce4c39":"code","7416b4bc":"code","d512c24d":"code","1b97883e":"code","3081e34b":"code","6845f292":"code","61e3227f":"code","c5de2f12":"code","f0262e00":"code","7bc114d3":"code","3194f5d0":"code","5bb84b54":"code","5511b964":"code","89af0dcb":"code","09268541":"code","a0d041c7":"code","24ae9998":"code","5e52ed90":"code","064da00d":"code","301031c9":"code","59d8c141":"code","bae360b5":"code","bad16fd7":"code","92a6350a":"code","79dd5ca3":"code","adb876df":"code","c37830f5":"code","08e4a4c6":"code","adbf9742":"code","33f540ab":"code","a402e765":"code","ccf928a2":"code","7bdd66a4":"code","55bb991d":"code","69681d4d":"code","4763cc51":"code","f497f2f7":"code","68af91cb":"markdown","693733d8":"markdown","37525d7e":"markdown","2ae9b0ab":"markdown","4c309609":"markdown","cdd760c7":"markdown","d1c17946":"markdown","4d61ea17":"markdown","e1b9b296":"markdown","8ef2bb53":"markdown","8fce3444":"markdown"},"source":{"fa2f7fb0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.metrics import accuracy_score\n## for data\nimport pandas as pd\n\nimport re\nimport nltk## for language detection\n\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3c2fa4bf":"df = pd.read_csv('..\/input\/supreme-court-judgment-prediction\/justice.csv', delimiter=',', encoding = \"utf8\")\ndf.dropna(inplace=True)\ndf.head()","29f0313d":"df1 = df.copy()","1e59c7df":"df1.drop(columns=['Unnamed: 0', 'docket','name','first_party','second_party', 'issue_area', \n                 'facts_len', 'majority_vote', 'minority_vote', 'href', 'ID','term'], inplace=True)","5236d6c4":"df_cat = df1[['decision_type', 'disposition']]","e9ff4a01":"df_target = df1['first_party_winner']","0319bf1f":"df_nlp = df1['facts']","a04a56d5":"df_cat.reset_index(drop=True, inplace=True)\ndf_target.reset_index(drop=True, inplace=True)\ndf_nlp.reset_index(drop=True, inplace=True)","c3e565ec":"from sklearn import preprocessing\nlabel_encoder = preprocessing.LabelEncoder()\ndf_target= label_encoder.fit_transform(df_target)","bad65962":"df_target1 = pd.DataFrame(df_target, columns=['first_party_winner'])\ndf_target1","ec1178f3":"frames = [df_cat, df_target1]\ndf_concat = pd.concat(frames, axis=1, join='inner')\ndf_concat","dbeb3c9c":"df_nlp1 = pd.DataFrame(df_nlp, columns=['facts'])","91677558":"df_nlp1['facts'] = df_nlp1['facts'].str.replace(r'<[^<>]*>', '', regex=True)\ndf_nlp1","09d7bbf2":"corpus = df_nlp1[\"facts\"]\nlst_tokens = nltk.tokenize.word_tokenize(corpus.str.cat(sep=\" \"))","1b4521c8":"ps = nltk.stem.porter.PorterStemmer()\nlem = nltk.stem.wordnet.WordNetLemmatizer()","ea8dfe8f":"\ndef utils_preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):\n    ## clean (convert to lowercase and remove punctuations and characters and then strip)\n    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n            \n    ## Tokenize (convert from string to list)\n    lst_text = text.split()    ## remove Stopwords\n    if lst_stopwords is not None:\n        lst_text = [word for word in lst_text if word not in \n                    lst_stopwords]\n                \n    ## Stemming (remove -ing, -ly, ...)\n    if flg_stemm == True:\n        ps = nltk.stem.porter.PorterStemmer()\n        lst_text = [ps.stem(word) for word in lst_text]\n                \n    ## Lemmatisation (convert the word into root word)\n    if flg_lemm == True:\n        lem = nltk.stem.wordnet.WordNetLemmatizer()\n        lst_text = [lem.lemmatize(word) for word in lst_text]\n            \n    ## back to string from list\n    text = \" \".join(lst_text)\n    return text","15117f60":"df_nlp1[\"facts_clean\"] = df_nlp1[\"facts\"].apply(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=True))","7fd61674":"df_nlp1","c75fb144":"from sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer","7f5b1c18":"vectorizer = CountVectorizer()\nx = vectorizer.fit_transform(df_nlp1['facts_clean'])","c08d34a3":"x1 = x.toarray()","a15b7f36":"df.reset_index(drop=True, inplace=True)","4a5b1401":"df_nlp2 = pd.concat([df_nlp1,df_target1['first_party_winner']],axis=1, join='inner')","8c43eb7c":"xfeatures = df_nlp2['facts_clean']\nylabel = df_nlp2['first_party_winner']","072a102f":"X_train, X_test, y_train, y_test = train_test_split(xfeatures,ylabel, test_size=0.25)","fe03a95d":"pipe = Pipeline(steps=[('cv',CountVectorizer()),('lr',LogisticRegression(solver='liblinear'))])","78135898":"pipe.fit(X_train,y_train)","14b9377d":"pipe.score(X_test,y_test)","b5f36d1d":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier","5f2762de":"pipe1= Pipeline(steps=[('cv',CountVectorizer()),('rf',RandomForestClassifier())])","73b6c4e1":"pipe1.fit(X_train,y_train)","7bd2d2b2":"pipe1.score(X_test,y_test)","f3d78ea4":"pipe2= Pipeline(steps=[('cv',CountVectorizer()),('rf',KNeighborsClassifier(n_neighbors=3))])","5c71032d":"pipe2.fit(X_train,y_train)","60321999":"pipe2.score(X_test,y_test)","e0d74c44":"df_nl1=pd.concat([df_nlp2,df_cat],axis=1,join='inner')","ac9acbcf":"df_nl1","9df24cfe":"xfeatures11 = df_nl1[['facts_clean','decision_type','disposition']]\nylabel11 = df_nl1['first_party_winner']","c6815ea3":"df_cat1=pd.get_dummies(df_cat['decision_type'])","10e8b7e1":"df_cat2=pd.get_dummies(df_cat['disposition'])","5bab7115":"df_cat3=pd.concat([df_cat2,df_cat1],axis=1,join='inner')","f0d3f5f0":"df_cat3=pd.concat([df_cat3,df_nl1['first_party_winner']],axis=1,join='inner')","68c19aac":"vectorize=CountVectorizer()","01e4b143":"count_matrix = vectorize.fit_transform(df_nl1['facts_clean'])\ncount_array = count_matrix.toarray()\ndata_hello = pd.DataFrame(data=count_array,columns = vectorize.get_feature_names())","10926c02":"data_hello=pd.concat([data_hello,df_cat3],axis=1,join='inner')","99eec569":"data_hello","30efec8f":"X=data_hello.drop(columns=['first_party_winner'])\nY=data_hello['first_party_winner']","618f97ef":"X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.3,random_state=10)","f0ce4c39":"#XX=final_data.drop(columns=['first_party_winner'])\n#YY=final_data['first_party_winner']","7416b4bc":"#from sklearn.preprocessing import MinMaxScaler\n#scaler = MinMaxScaler()\n#X_train = scaler.fit_transform(X_train)","d512c24d":"from sklearn.decomposition import PCA\npca = PCA(n_components=400)\npca_fit = pca.fit_transform(X_train)","1b97883e":"import matplotlib.pyplot as plt\nimport numpy as np\n\nPC_values = np.arange(pca.n_components_) + 1\nplt.plot(PC_values, pca.explained_variance_ratio_, 'o-', linewidth=2, color='blue')\nplt.title('Scree Plot')\nplt.xlabel('Principal Component')\nplt.ylabel('Variance Explained')\nplt.show()","3081e34b":"print(pca.explained_variance_ratio_.sum())","6845f292":"pca_df = pd.DataFrame(data = pca_fit)\npca_df.head(3)","61e3227f":"final_data=pd.concat([pca_df,Y],axis=1,join='inner')","c5de2f12":"final_data","f0262e00":"XX=final_data.drop(columns=['first_party_winner'])\nYY=final_data[['first_party_winner']]","7bc114d3":"rand=RandomForestClassifier()","3194f5d0":"rand.fit(XX,YY)","5bb84b54":"rand.score(XX,YY)","5511b964":"pca = PCA(n_components=400)\npca_fit = pca.fit_transform(X_test)\nX_test = pd.DataFrame(data = pca_fit)","89af0dcb":"rand.score(X_test,y_test)","09268541":"from sklearn.model_selection import GridSearchCV\nparam_grid = {  'bootstrap': [True], 'max_depth': [5, 10, None], 'max_features': ['auto', 'log2'], 'n_estimators': [5, 6, 7, 8, 9, 10, 11, 12, 13, 15]}","a0d041c7":"rand = RandomForestClassifier(random_state = 1)\ng_search = GridSearchCV(estimator = rand, param_grid = param_grid,\n                        cv = 3, n_jobs = 1, verbose = 0, return_train_score=True)","24ae9998":"g_search.fit(XX, YY);\nprint(g_search.best_params_)","5e52ed90":"print(g_search.score(X_test, y_test))","064da00d":"rand=RandomForestClassifier(bootstrap= True, max_depth= 5, max_features= 'log2', n_estimators= 15)","301031c9":"rand.fit(XX,YY)","59d8c141":"y_pred = rand.predict(X_test)\npredictions = [round(value) for value in y_pred]\n# evaluate predictions\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","bae360b5":"from sklearn.metrics import f1_score","bad16fd7":"f1_score(y_test, y_pred)","92a6350a":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\nmatrix = confusion_matrix(y_test,y_pred, labels=[1,0])\nprint('Confusion matrix : \\n',matrix)\n\n# outcome values order in sklearn\ntp, fn, fp, tn = confusion_matrix(y_test,y_pred,labels=[1,0]).reshape(-1)\nprint('Outcome values : \\n', tp, fn, fp, tn)\n\n# classification report for precision, recall f1-score and accuracy\nmatrix = classification_report(y_test,y_pred,labels=[1,0])\nprint('Classification report : \\n',matrix)","79dd5ca3":"y_pred1 = rand.predict(XX)\nf1_score(YY, y_pred1)","adb876df":"model = XGBClassifier()\nmodel.fit(XX, YY)\ny_pred1 = model.predict(X_test)\npredictions = [round(value) for value in y_pred]\n# evaluate predictions\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\nf1_score(y_test, y_pred1)","c37830f5":"matrix = confusion_matrix(y_test,y_pred1, labels=[1,0])\nprint('Confusion matrix : \\n',matrix)\n\n# outcome values order in sklearn\ntp, fn, fp, tn = confusion_matrix(y_test,y_pred1,labels=[1,0]).reshape(-1)\nprint('Outcome values : \\n', tp, fn, fp, tn)\n\n# classification report for precision, recall f1-score and accuracy\nmatrix = classification_report(y_test,y_pred1,labels=[1,0])\nprint('Classification report : \\n',matrix)","08e4a4c6":"knn=KNeighborsClassifier(n_neighbors=3)","adbf9742":"knn.fit(XX,YY)","33f540ab":"knn.score(X_test,y_test)","a402e765":"y_pred = knn.predict(X_test)\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\nf1_score(y_test, y_pred)","ccf928a2":"import numpy as np\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional","7bdd66a4":"n_unique_words = 10000 # cut texts after this number of words\nmaxlen = 2000\nbatch_size = 32","55bb991d":"model = Sequential()\nmodel.add(Embedding(n_unique_words, 128, input_length=maxlen))\nmodel.add(LSTM(64))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","69681d4d":"history=model.fit(XX, YY,\n           batch_size=batch_size,\n           epochs=200,\n           validation_data=[X_test, y_test])","4763cc51":"from sklearn import svm","f497f2f7":"clf = svm.SVC(kernel='linear') # Linear Kernel\n#Train the model using the training sets\nclf.fit(XX, YY)\n#Predict the response for test dataset\ny_pred = clf.predict(X_test)\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\nf1_score(y_test, y_pred1)","68af91cb":"# 5. Performing initial cleaning and tokenizing the corpus, introducing a function to perform further cleaning and Lemmatization upon the data.","693733d8":"# 6. Introducing Count Vectorizer to derive features from textual data.","37525d7e":"# **Thank you for taking the time to go through our submission. Any feedback is always welcome.**","2ae9b0ab":"# 7. Using sklearn train_test_split and Pipeline to fit and score the model on Logistic Regression, RandomForest and K-NearestNeighbors on the newly engineered features","4c309609":"# 1. **Importing libraries that are planned to be used in our process**","cdd760c7":"# 3. Seperating the dataset into target variables and two groups of independent variables, one (df_cat) which requires one-hot encoding to be machine readable and the other (df_nlp) which is text data which needs to be cleaned for features to be engineered from it.","d1c17946":"# 9. Using Principal Component Analysis to perform dimensionality reduction and measure the accuracy trade-off","4d61ea17":"# 4. Resetting indices to avoid NaNs during concatenation and performing one-hot encoding","e1b9b296":"# 2. Reading the dataset with pandas, dropping null values (calculated earlier at less than 5%) and creating a copy of the dataframe for the modelling process","8ef2bb53":"# 8. Now including the one-hot encoded features within our model.","8fce3444":"# 10. Deploying a LSTM Model to achieve higher accuracy"}}