{"cell_type":{"bd06ee5c":"code","4f7ec0ce":"code","b04ee2bc":"code","f8b2f38a":"code","81a2af52":"code","249eace4":"code","a461291d":"code","d5c47264":"code","a5c2b6c6":"code","6dbf76bb":"code","967a7f65":"code","c5237e40":"code","5a22ccd3":"code","0075efce":"code","8cb95d84":"code","4f405423":"code","3f1d24d0":"code","6fb378cf":"code","25c9cd9e":"code","c025dae5":"code","8aa3919b":"code","db30075f":"code","4536bc63":"code","fa1e29dd":"code","3846ed30":"code","6a4c22e4":"code","9238e085":"code","90ab7b69":"code","d61cca97":"code","fb079df8":"code","0e2f0af9":"code","71f8512c":"code","ecc8b5f2":"code","1db4fba4":"code","4a558094":"code","49b90c19":"code","fb7c2336":"code","23fec143":"code","546a12ba":"code","eb69ddf1":"code","e5031a88":"code","a9b07a1a":"code","b353413a":"code","a5b664e5":"code","1d0bb325":"code","6bef486c":"code","be31a2a6":"code","445a9bd4":"code","378c954b":"code","0dd1c8cd":"code","ba6a705f":"code","b41b1e6f":"code","b1a24fed":"code","d75fff2f":"code","de58c03c":"code","2840e7ab":"code","9421f152":"code","2beb3f3b":"code","78497081":"code","47be0ec6":"code","14c57ff8":"code","a8e0b018":"code","72f6a3bc":"code","92cd9bd3":"code","0cd9ed26":"code","d8f2ca88":"code","b4d566ae":"code","b9a74ce7":"markdown","18f7b91f":"markdown","5a603210":"markdown","522aa6ca":"markdown","f48b3e9f":"markdown","a83ca138":"markdown","35b67337":"markdown","e1707f82":"markdown","8ef4d706":"markdown","b756b404":"markdown","0c7c6a2c":"markdown","4b39e1bb":"markdown","2bc88175":"markdown","6cb0d0f3":"markdown","780e0ebb":"markdown","bf44edd8":"markdown","1d475361":"markdown","0d9b8e87":"markdown","c47c9aa3":"markdown","6ac16f95":"markdown","1062f72e":"markdown","f4800df9":"markdown","99f60cfc":"markdown","c24dd6f3":"markdown","0a941354":"markdown","745b62e2":"markdown","48889381":"markdown","f9c0892d":"markdown","794cf0d9":"markdown","f3aacb9f":"markdown","3183d647":"markdown","237dc025":"markdown","c088d466":"markdown","270e9efd":"markdown"},"source":{"bd06ee5c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\nimport seaborn as sns\nplt.style.use(\"fivethirtyeight\")\n\nimport pandas_profiling as pp\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","4f7ec0ce":"#Allow for full tables to be shown\npd.options.display.max_columns = None\npd.options.display.max_rows = None","b04ee2bc":"iris=pd.read_csv(\"\/kaggle\/input\/iris\/Iris.csv\")\n#iris.head(5)\niris.sample(5)","f8b2f38a":"# drop the \"Id\" column from the iris data\niris=iris.drop(\"Id\",axis=1)","81a2af52":"##Split the Species names by the '-', and use the second one. i.e remove iris from the Species names.\niris['Species'] = iris['Species'].str.split('-', expand=True)[1]\niris.head()","249eace4":"# iris data shape\niris.shape","a461291d":"# data type of columns of iris dataframe.\niris.dtypes","d5c47264":"iris.columns  ## iris.keys()","a5c2b6c6":"iris.info()","6dbf76bb":"import pandas_profiling as pp\ndf=iris\nreport=pp.ProfileReport(df,title = \"Pandas Profile Report\")\n\n","967a7f65":"#report.to_widgets()\nreport.to_notebook_iframe()","c5237e40":"#report.to_file(\"your_report.html\")","5a22ccd3":"iris.isnull().values.any()","0075efce":"iris.duplicated().value_counts()","8cb95d84":"iris[iris.duplicated(keep=\"first\")]","4f405423":"iris.drop_duplicates(keep=\"first\",inplace=True)","3f1d24d0":"iris.describe(include=\"all\")","6fb378cf":"iris.corr()","25c9cd9e":"fig=plt.figure()\nsns.heatmap(iris.corr(),annot= True)\nplt.show()","c025dae5":"iris[\"Species\"].value_counts()","8aa3919b":"\nfig=plt.figure()\niris[\"Species\"].value_counts().plot.bar()\nplt.show()","db30075f":"fig=plt.figure()\nsns.countplot(\"Species\",data=iris)\nplt.show()","4536bc63":"#!pip install squarify\nimport squarify\nplt.figure(figsize=(8,8))\nsquarify.plot(sizes=iris.Species.value_counts(), label=iris['Species'], alpha=.5 ,color=['r','g','b'])\nplt.axis('off')\nplt.show()","fa1e29dd":"iris.groupby(\"Species\").size().plot.bar()\nplt.show()","3846ed30":"iris.Species.value_counts().plot.pie(explode=(0.1,0.1,0.1),autopct='%1.1f%%',shadow=True,figsize=(8,8))\nplt.tight_layout()","6a4c22e4":"fig,ax=plt.subplots(2,2,figsize=(10,10))\niris.SepalLengthCm.plot.line(ax=ax[0][0])\nax[0][0].set_title(\"Sepal Length\")\niris.SepalWidthCm.plot.line(ax=ax[0][1])\nax[0][1].set_title(\"Sepal Width\")\niris.PetalLengthCm.plot.line(ax=ax[1][0])\nax[1][0].set_title(\"Petal Length\")\niris.PetalWidthCm.plot.line(ax=ax[1][1])\nax[1][1].set_title(\"Petal Width\")\nplt.show()","9238e085":"iris.hist(edgecolor=\"black\",figsize=(10,10))\nplt.show()","90ab7b69":"iris.boxplot(figsize=(10,10))\nplt.show()","d61cca97":"iris.plot(kind='box', subplots=True, layout=(2,2),sharex=False,  sharey=False,figsize=(8,8))\nplt.show()","fb079df8":"iris.plot(figsize=(12,12))\nplt.show()","0e2f0af9":"iris.boxplot(by=\"Species\",figsize=(12,12))\nplt.show()","71f8512c":"sns.pairplot(iris,hue=\"Species\")\nplt.show()","ecc8b5f2":"sns.scatterplot(x=\"SepalLengthCm\",y=\"SepalWidthCm\",data=iris,hue=\"Species\")\nplt.show()","1db4fba4":"from IPython.display import Image\nImage(\"http:\/\/scikit-learn.org\/dev\/_static\/ml_map.png\", width=800)","4a558094":"\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score,confusion_matrix, classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder","49b90c19":"X=iris.iloc[:,:-1]\nY=iris.iloc[:,-1:]\n","fb7c2336":"#scaling of the feature\nohe=OneHotEncoder()\nLE=LabelEncoder()\nY=LE.fit_transform(Y)","23fec143":"x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.25,random_state=2)\nprint(\"Train data size\",x_train.shape,y_train.shape)\nprint(\"Test data size\",x_test.shape,y_test.shape)","546a12ba":"model=LogisticRegression()\nmodel.fit(x_train,y_train)\npred=model.predict(x_test)\n\n\n\nprint(\"Confusion Matrix:\\n\",confusion_matrix(pred,y_test))\nprint(\"=================================================================\")\nprint(\"Classification Report:\\n\",classification_report(pred,y_test))\nprint(\"=================================================================\")\n# Accuracy Score\nprint(\"Accuracy Score:\\n\",accuracy_score(pred,y_test))","eb69ddf1":"model1=SVC()\nmodel1.fit(x_train,y_train)\npred1=model1.predict(x_test)\n\n\nprint(\"Confusion Matrix:\\n\",confusion_matrix(pred,y_test))\nprint(\"=================================================================\")\nprint(\"Classification Report:\\n\",classification_report(pred,y_test))\nprint(\"=================================================================\")\n#Accuracy Score\nprint(\"Accuracy Score:\\n\",accuracy_score(pred,y_test))\n","e5031a88":"# Decision Tree Classifier\nmodel_dt=DecisionTreeClassifier()\nmodel_dt.fit(x_train,y_train)\npred_dt=model_dt.predict(x_test)\n\n\n\nprint(\"Confusion Matrix:\\n\",confusion_matrix(pred_dt,y_test))\nprint(\"=================================================================\")\nprint(\"Classification Report:\\n\",classification_report(pred_dt,y_test))\nprint(\"=================================================================\")\n\n# Accuracy Score\nprint(\"Accuracy Score:\\n\",accuracy_score(pred_dt,y_test))","a9b07a1a":"# Random Forest Classifier\nmodel_rf=RandomForestClassifier(n_jobs=3,max_depth=3)\nmodel_rf.fit(x_train,y_train)\npred_rf=model_rf.predict(x_test)\n\n\n# Summary of the predictions made by the classifier\nprint(\"Confusion Matrix:\\n\",confusion_matrix(pred_rf,y_test))\nprint(\"=================================================================\")\nprint(\"Classification Report:\\n\",classification_report(pred_rf,y_test))\nprint(\"=================================================================\")\n#Accuracy Score\nprint(\"accuracy Score:\\n\",accuracy_score(pred_rf,y_test))","b353413a":"# K Nearest Neighbors \nmodel_knn=KNeighborsClassifier(n_neighbors=3)\nmodel_knn.fit(x_train,y_train)\npred_knn=model_knn.predict(x_test)\n\n# Summary of the predictions made by the classifier\nprint(\"Confusion Matrix:\\n\",confusion_matrix(pred_knn,y_test))\nprint(\"=================================================================\")\nprint(\"Classification Report:\\n\",classification_report(pred,y_test))\nprint(\"=================================================================\")\n# Accuracy Score\nprint(\"Accuracy Score:\\n\",accuracy_score(pred_knn,y_test))\n","a5b664e5":"param={'C':[0.1,0.8,0.9,1,1.1,1.2,1.3,1.4,1.5],\n      'kernel': [\"linear\",\"rbf\"],\n      \"gamma\":[0.1,0.8,0.9,1,1.1,1.2,1.3,1.4,1.5]}\n               \ngrid_svc=GridSearchCV(model1,param_grid=param,scoring=\"accuracy\",cv=10)\ngrid_svc.fit(x_train,y_train)","1d0bb325":"grid_svc.best_params_","6bef486c":"gridsearch_svc=SVC(C=0.8,gamma=0.1,kernel='linear')\ngridsearch_svc.fit(x_train,y_train)\npred_grid=gridsearch_svc.predict(x_test)\n\nprint(\"Confusion Matrix:\\n\",confusion_matrix(pred_grid,y_test))\nprint(\"=================================================================\")\nprint(\"Classification Report:\\n\",classification_report(pred_grid,y_test))\nprint(\"=================================================================\")\n\n# Accuracy Score\nprint(\"Accuracy Score:\\n\",accuracy_score(pred_grid,y_test))","be31a2a6":"models=[model,model1,gridsearch_svc,model_dt,model_rf,model_knn]\naccuracy_scores=[]\nfor i in models:\n    pred=i.predict(x_test)\n    accuracy=accuracy_score(pred,y_test)\n    accuracy_scores.append(accuracy)\nprint(accuracy_scores)    \nplt.bar(['LogReg','SVM','GridSVC','DT','RF','KNN'],accuracy_scores)\nplt.ylim(0.90,1.01)\nplt.title(\"Accuracy comparision for various models\",fontsize=15,color='r')\nplt.xlabel(\"Models\",fontsize=18,color='g')\nplt.ylabel(\"Accuracy Score\",fontsize=18,color='g')\nplt.show()\n    ","445a9bd4":"#converting categorical data  into int data type using labelEncoder for Linear reagration.\n\nx = iris.iloc[:,:-1].values    #   X -> Feature Variables\ny = iris.iloc[:,-1].values #   y ->  Target\n\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ny = le.fit_transform(y)\n\nprint(y)  # this is y categotical to numerical","378c954b":"from sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 42)\n\nmodel_LR = LinearRegression()\nmodel_LR.fit(X_train, y_train)\ny_pred = model_LR.predict(X_test)\n\n\nprint('y-intercept             :' , model_LR.intercept_)\nprint('beta coefficients       :' , model_LR.coef_)\nprint('Mean Abs Error MAE      :' ,metrics.mean_absolute_error(y_test,y_pred))\nprint('Mean Sqrt Error MSE     :' ,metrics.mean_squared_error(y_test,y_pred))\nprint('Root Mean Sqrt Error RMSE:' ,np.sqrt(metrics.mean_squared_error(y_test,y_pred)))\nprint('r2 value                :' ,metrics.r2_score(y_test,y_pred))\n","0dd1c8cd":"# Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nModel = GaussianNB()\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(\"Confusion Matrix:\\n\",confusion_matrix(y_test, y_pred))\nprint(\"=================================================================\")\nprint(\"Classification Report:\\n\",classification_report(y_test, y_pred))\nprint(\"=================================================================\")\n# Accuracy score\nprint(\"Accuracy Score:\\n\",accuracy_score(y_pred,y_test))","ba6a705f":"from sklearn.svm import NuSVC\n\nModel = NuSVC()\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(\"Confusion Matrix:\\n\",confusion_matrix(y_test, y_pred))\nprint(\"=================================================================\")\nprint(\"Classification Report:\\n\",classification_report(y_test, y_pred))\nprint(\"=================================================================\")\n\n# Accuracy score\nprint(\"Accuracy Score:\\n\",accuracy_score(y_pred,y_test))","b41b1e6f":"# Linear Support Vector Classification\nfrom sklearn.svm import LinearSVC\n\nModel = LinearSVC()\nModel.fit(X_train, y_train)\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(\"Confusion Matrix:\\n\",confusion_matrix(y_test, y_pred))\nprint(\"=================================================================\")\nprint(\"Classification Report:\\n\",classification_report(y_test, y_pred))\nprint(\"=================================================================\")\n\n# Accuracy score\nprint(\"Accuracy Score:\\n\",accuracy_score(y_pred,y_test))","b1a24fed":"from sklearn.neighbors import  RadiusNeighborsClassifier\nModel=RadiusNeighborsClassifier(radius=2.0)\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(\"Confusion Matrix:\\n\",confusion_matrix(y_test, y_pred))\nprint(\"=================================================================\")\nprint(\"Classification Report:\\n\",classification_report(y_test, y_pred))\nprint(\"=================================================================\")\n\n# Accuracy score\nprint(\"Accuracy Score:\\n\",accuracy_score(y_pred,y_test))","d75fff2f":"# BernoulliNB\nfrom sklearn.naive_bayes import BernoulliNB\nModel = BernoulliNB()\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(\"Confusion Matrix:\\n\",confusion_matrix(y_test, y_pred))\nprint(\"=================================================================\")\nprint(\"Classification Report:\\n\",classification_report(y_test, y_pred))\nprint(\"=================================================================\")\n\n# Accuracy score\nprint(\"Accuracy Score:\\n\",accuracy_score(y_pred,y_test))","de58c03c":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nModel=LinearDiscriminantAnalysis()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(\"Confusion Matrix:\\n\",confusion_matrix(y_test, y_pred))\nprint(\"=================================================================\")\nprint(\"Classification Report:\\n\",classification_report(y_test, y_pred))\nprint(\"=================================================================\")\n\n# Accuracy score\nprint(\"Accuracy Score:\\n\",accuracy_score(y_pred,y_test))","2840e7ab":"from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nModel=QuadraticDiscriminantAnalysis()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(\"Confusion Matrix:\\n\",confusion_matrix(y_test, y_pred))\nprint(\"=================================================================\")\nprint(\"Classification Report:\\n\",classification_report(y_test, y_pred))\nprint(\"=================================================================\")\n\n# Accuracy score\nprint(\"Accuracy Score:\\n\",accuracy_score(y_pred,y_test))","9421f152":"from sklearn.linear_model import PassiveAggressiveClassifier\nModel = PassiveAggressiveClassifier()\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(\"Confusion Matrix:\\n\",confusion_matrix(y_test, y_pred))\nprint(\"=================================================================\")\nprint(\"Classification Report:\\n\",classification_report(y_test, y_pred))\nprint(\"=================================================================\")\n\n# Accuracy score\nprint(\"Accuracy Score:\\n\",accuracy_score(y_pred,y_test))","2beb3f3b":"# BernoulliNB\nfrom sklearn.naive_bayes import BernoulliNB\nModel = BernoulliNB()\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(\"Confusion Matrix:\\n\",confusion_matrix(y_test, y_pred))\nprint(\"=================================================================\")\nprint(\"Classification Report:\\n\",classification_report(y_test, y_pred))\nprint(\"=================================================================\")\n\n# Accuracy score\nprint(\"Accuracy Score:\\n\",accuracy_score(y_pred,y_test))","78497081":"# ExtraTreeClassifier\nfrom sklearn.tree import ExtraTreeClassifier\nModel = ExtraTreeClassifier()\nModel.fit(X_train, y_train)\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(\"Confusion Matrix:\\n\",confusion_matrix(y_test, y_pred))\nprint(\"=================================================================\")\nprint(\"Classification Report:\\n\",classification_report(y_test, y_pred))\nprint(\"=================================================================\")\n\n# Accuracy score\nprint(\"Accuracy Score:\\n\",accuracy_score(y_pred,y_test))","47be0ec6":"from sklearn.ensemble import BaggingClassifier\nModel=BaggingClassifier()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(\"Confusion Matrix:\\n\",confusion_matrix(y_test, y_pred))\nprint(\"=================================================================\")\nprint(\"Classification Report:\\n\",classification_report(y_test, y_pred))\nprint(\"=================================================================\")\n\n# Accuracy score\nprint(\"Accuracy Score:\\n\",accuracy_score(y_pred,y_test))","14c57ff8":"from sklearn.ensemble import AdaBoostClassifier\nModel=AdaBoostClassifier()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(\"Confusion Matrix:\\n\",confusion_matrix(y_test, y_pred))\nprint(\"=================================================================\")\nprint(\"Classification Report:\\n\",classification_report(y_test, y_pred))\nprint(\"=================================================================\")\n\n# Accuracy score\nprint(\"Accuracy Score:\\n\",accuracy_score(y_pred,y_test))","a8e0b018":"from sklearn.ensemble import GradientBoostingClassifier\nModel=GradientBoostingClassifier()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(\"Confusion Matrix:\\n\",confusion_matrix(y_test, y_pred))\nprint(\"=================================================================\")\nprint(\"Classification Report:\\n\",classification_report(y_test, y_pred))\nprint(\"=================================================================\")\n\n# Accuracy score\nprint(\"Accuracy Score:\\n\",accuracy_score(y_pred,y_test))","72f6a3bc":"\n#Finding the optimum number of clusters for k-means classification\nfrom sklearn.cluster import KMeans\nwcss = []\n\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 100, n_init = 5, random_state = 42)\n    kmeans.fit(X_train,y_train)\n    wcss.append(kmeans.inertia_)\n    \n#Plotting the results onto a line graph, allowing us to observe 'The elbow'\nplt.plot(range(1, 11), wcss)\nplt.title('The elbow method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS') # within cluster sum of squares\nplt.show()","92cd9bd3":"#Applying kmeans to the dataset \/ Creating the kmeans classifier\nkmeans = KMeans(n_clusters = 3, init = 'k-means++', max_iter = 100, n_init = 5, random_state = 42)\ny_preds = kmeans.fit_predict(X)\n\n\n\n# Summary of the predictions made by the classifier\nprint(\"Confusion Matrix:\\n\",confusion_matrix(y, y_preds))\nprint(\"=================================================================\")\nprint(\"Classification Report:\\n\",classification_report(y, y_preds))\nprint(\"=================================================================\")\n\n# Accuracy score\nprint(\"Accuracy Score:\\n\",accuracy_score(y_preds,y))\n\n","0cd9ed26":"#Visualising the clusters\nplt.scatter(x[y_preds == 0, 0], x[y_preds == 0, 1], c = 'g', label = 'Setosa')\nplt.scatter(x[y_preds == 1, 0], x[y_preds == 1, 1], c = 'b', label = 'Versicolour')\nplt.scatter(x[y_preds == 2, 0], x[y_preds == 2, 1], c = 'y', label = 'Virginica')\n\n#Plotting the centroids of the clusters\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:,1], s = 100, c = 'r', label = 'Centroids',marker='*')\n\nplt.legend()","d8f2ca88":"from scipy.cluster.hierarchy import linkage,dendrogram\n\nplt.figure(figsize=(18,8))\nmerg=linkage(X,method=\"ward\")\ndendrogram(merg,leaf_rotation=90)\nplt.xlabel(\"data points\")\nplt.ylabel(\"euclidian distance\")\nplt.show() ","b4d566ae":"from sklearn.cluster import AgglomerativeClustering\n\nac=AgglomerativeClustering(n_clusters=3,affinity=\"euclidean\",linkage=\"ward\")\ny_prediction=ac.fit_predict(X) \n\nplt.scatter(X.loc[:,\"PetalLengthCm\"],X.loc[:,\"PetalWidthCm\"],c=y_prediction,cmap=\"rainbow\") # G\u00f6rselle\u015ftirme\nplt.xlabel(\"PetalLength\")\nplt.ylabel(\"PetalWidth\")\nplt.show()","b9a74ce7":"### 2 Naive Bayes\nIt is a classification technique based on Bayes\u2019 theorem with an assumption of independence between predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. For example, a fruit may be considered to be an apple if it is red, round, and about 3 inches in diameter. Even if these features depend on each other or upon the existence of the other features, a naive Bayes classifier would consider all of these properties to independently contribute to the probability that this fruit is an apple.","18f7b91f":"### Accuracy comparision for various models.","5a603210":"**6. Grid SearchCV**\n\nto increase accuracy of the model.","522aa6ca":"**Import the Libraries**","f48b3e9f":"**4. Random Forest Classifier**\n\nRandom Forest is an ensembles of decision trees. In Random Forest, we\u2019ve collection of decision trees (so known as \u201cForest\u201d). To classify a new object based on attributes, each tree gives a classification and we say the tree \u201cvotes\u201d for that class. The forest chooses the classification having the most votes (over all the trees in the forest).","a83ca138":"### 3 Nu-Support Vector Classification\nSimilar to SVC but uses a parameter to control the number of support vectors.\n","35b67337":"## Scikit-learn Algorithms","e1707f82":"### 4 Linear Support Vector Classification\nSimilar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples.\nThis class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme","8ef4d706":"### 9 BernoulliNB\u00b6\nLike MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary\/boolean features.","b756b404":"### 10 ExtraTreeClassifier\nExtraTreesClassifier is an ensemble learning method fundamentally based on decision trees. ExtraTreesClassifier, like RandomForest, randomizes certain decisions and subsets of data to minimize over-learning from the data and overfitting. Let\u2019s look at some ensemble methods ordered from high to low variance, ending in ExtraTreesClassifier.","0c7c6a2c":"## Unsupervised Machine Learning Algorithms","4b39e1bb":"### 16.Hierarchical Clustering","2bc88175":"### Classification Models","6cb0d0f3":"### 13 Gradient Boosting Classifier\u00b6\nGBM is a boosting algorithm used when we deal with plenty of data to make a prediction with high prediction power. Boosting is actually an ensemble of learning algorithms which combines the prediction of several base estimators in order to improve robustness over a single estimator. It combines multiple weak or average predictors to a build strong predictor.","780e0ebb":"**Thankyou for visit the kernel. If you have any suggustion please comment.if you feel the kernel helpful,please upvote.**","bf44edd8":"### 12 AdaBoost classifier\nAn AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.","1d475361":"### 14 K- means\nIt is a type of unsupervised algorithm which solves the clustering problem. Its procedure follows a simple and easy way to classify a given data set through a certain number of clusters (assume k clusters). Data points inside a cluster are homogeneous and heterogeneous to peer groups.\n\nRemember figuring out shapes from ink blots? k means is somewhat similar this activity. You look at the shape and spread to decipher how many different clusters \/ population are present!","0d9b8e87":"### 11 Bagging classifier\nBagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.","c47c9aa3":"**3. Decision Tree Classifier**\n\nIt is a supervised learning algorithm that is mostly used for classification problems. it works for both categorical and continuous dependent variables\n\n","6ac16f95":"### 6 Linear Discriminant Analysis\nA classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule.\n\nThe model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix.\n\nThe fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions.","1062f72e":"> Updating.....","f4800df9":"## SOME MORE MACHINE LEARNING ALGORITHMS","99f60cfc":"### 8 Passive Aggressive Classifier\u00b6\nPA algorithm is a margin based online learning algorithm for binary classification. Unlike PA algorithm, which is a hard-margin based method, PA-I algorithm is a soft margin based method and robuster to noise.","c24dd6f3":"### 7 Quadratic Discriminant Analysis\nA classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule.\n\nThe model fits a Gaussian density to each class.","0a941354":"### Estimators\nGiven a scikit-learn estimator object named model, the following methods are available:\n\n- Available in all Estimators\n - model.fit() : fit training data. For supervised learning applications, this accepts two arguments: the data X and the labels y (e.g. model.fit(X, y)). For unsupervised learning applications, this accepts only a single argument, the data X (e.g. model.fit(X)).\n- Available in supervised estimators\n - model.predict() : given a trained model, predict the label of a new set of data. This method accepts one argument, the new data X_new (e.g. model.predict(X_new)), and returns the learned label for each object in the array.\n - model.predict_proba() : For classification problems, some estimators also provide this method, which returns the probability that a new observation has each categorical label. In this case, the label with the highest probability is returned by model.predict().\n - model.score() : for classification or regression problems, most (all?) estimators implement a score method. Scores are between 0 and 1, with a larger score indicating a better fit.\n- Available in unsupervised estimators\n - model.predict() : predict labels in clustering algorithms.\n - model.transform() : given an unsupervised model, transform new data into the new basis. This also accepts one argument X_new, and returns the new representation of the data based on the unsupervised model.\n - model.fit_transform() : some estimators implement this method, which more efficiently performs a fit and a transform on the same input data","745b62e2":"### Pandas Profile Report","48889381":"### 5 BernoulliNB\nLike MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary\/boolean features.","f9c0892d":"**2. Support Vector Classifier**\n\nIt is a classification method. In this algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate","794cf0d9":"### 1. Linear Regression\nIt is used to estimate real values based on continuous variable(s). Here, we establish relationship between independent and dependent variables by fitting a best line. This best fit line is known as regression line and represented by a linear equation `Y= a X + b`","f3aacb9f":"\n## Statistical Summary\nNow we can take a look at a summary of each attribute.\n\nThis includes the count, mean, the min and max values as well as some percentiles","3183d647":"**1. Logistic Regression**\n\nDon\u2019t get confused by its name! It is a classification not a regression algorithm. It is used to estimate discrete values ( Binary values like 0\/1, yes\/no, true\/false ) based on given set of independent variables. In simple words, it predicts the probability of occurrence of an event by fitting data to a logit function. Hence, it is also known as logit regression. Since, it predicts the probability, its output values lies between 0 and 1.","237dc025":"**5. K Nearest Neighbors**\n\nIt can be used for both classification and regression problems. However, it is more widely used in classification problems in the industry. K nearest neighbors is a simple algorithm that stores all available cases and classifies new cases by a majority vote of its k neighbors. The case being assigned to the class is most common amongst its K nearest neighbors measured by a distance function.","c088d466":"**Data Loading**","270e9efd":"### 4 Radius Neighbors Classifier\nIn scikit-learn RadiusNeighborsClassifier is very similar to KNeighborsClassifier with the exception of two parameters. First, in RadiusNeighborsClassifier we need to specify the radius of the fixed area used to determine if an observation is a neighbor using radius. Unless there is some substantive reason for setting radius to some value, it is best to treat it like any other hyperparameter and tune it during model selection. The second useful parameter is outlier_label, which indicates what label to give an observation that has no observations within the radius - which itself can often be a useful tool for identifying outliers"}}