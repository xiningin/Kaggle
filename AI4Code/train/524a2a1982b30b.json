{"cell_type":{"60f2a38b":"code","6d915e40":"code","07111d64":"code","2f4e3a09":"code","decce6bd":"code","06141589":"code","4f98f512":"code","9ccd190d":"code","da9e5b8c":"code","07604e41":"code","0f5715de":"code","07842e0e":"code","460d5bcf":"code","34859a98":"markdown","f5e3d52c":"markdown","5737fc2e":"markdown","9663b6df":"markdown","95f84000":"markdown","bd02d446":"markdown","c8e806ec":"markdown","46af9fda":"markdown"},"source":{"60f2a38b":"!git clone https:\/\/github.com\/sidml\/Self-Supervised-Learning-for-Gravitational-Waves.git","6d915e40":"cd Self-Supervised-Learning-for-Gravitational-Waves","07111d64":"%load_ext autoreload\n%autoreload 2\n\nimport numpy as np\nimport pandas as pd\nimport pdb\npd.set_option(\"display.max_columns\", None)\nimport cv2\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nmatplotlib.use(\"Agg\")\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport random\nimport torchvision.transforms as T\nimport pdb\n\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning import seed_everything\n\nimport random\nfrom collections import OrderedDict\nimport os, gc, time\nfrom glob import glob\nfrom functools import partial\nfrom tqdm.auto import tqdm\n\nfrom sklearn.metrics import (\n    roc_auc_score,\n    accuracy_score,\n    average_precision_score,\n    f1_score,\n    precision_score,\n    recall_score,\n)\n\nfrom pl_model import G2Net, G2NetEval\nfrom cnn1d_models import NetEval\nfrom dataset import GWDatasetBandpass\n\nfrom utils import average_model, get_file_path\n\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")","2f4e3a09":"class Config:\n    batch_size = 456\n    weight_decay = 1e-8\n    lr = 5e-5\n    num_workers = 4\n    epochs = 4\n    model_name = 'cnn1d_aug'\n    suffix = \"barlow\"\n    ROOT = \"\/kaggle\/input\/g2net-gravitational-wave-detection\"\n    TRAIN_ROOT = f\"{ROOT}\/train\/\"\n    NOISE_DIR = f\"\/kaggle\/input\/gwdet-noise\"\n    TEST_ROOT = f\"{ROOT}\/test\/\"\n    SUB_DIR = \".\/submissions\/\"\n    \nconfig = Config()","decce6bd":"seed_everything(42, workers=True)\nprint(\"PL_SEED_WORKERS=\" + os.environ[\"PL_SEED_WORKERS\"])\nconfig = Config()\ntrain_labels = pd.read_csv(f\"{config.ROOT}\/training_labels.csv\")\n# bce loss requires labels to be of float type\ntrain_labels[\"target\"] = train_labels[\"target\"].astype(np.float32)\n\nget_path = partial(get_file_path, config.TRAIN_ROOT)\ntrain_labels[\"file_path\"] = train_labels[\"id\"].apply(get_path)\nprint(train_labels.head())","06141589":"trn_idx = np.random.randint(0, len(train_labels), (int(len(train_labels)*0.9),))\nval_idx = np.array(list(set(range(len(train_labels))) - set(trn_idx)))\ntrain_df = train_labels.loc[trn_idx].reset_index(drop=True)\nval_df = train_labels.loc[val_idx].reset_index(drop=True)\n\nprint(f'Training with {len(trn_idx)}, validation with {len(val_df)}')","4f98f512":"model = G2Net(config=config, train_df=train_df, val_df=val_df)\nckpt_dir = \"\/kaggle\/input\/self-supervised-method-for-gravitation-wave-det\/Self-Supervised-Learning-for-Gravitational-Waves\/cnn1d_aug_barlow\"\npaths = glob(f\"{ckpt_dir}\/lightning_logs\/version_0\/checkpoints\/*.ckpt\")\naveraged_w = average_model(paths)\nmodel.model.load_state_dict(averaged_w, strict=True)\ndel averaged_w; torch.cuda.empty_cache(); gc.collect()\n\ncheckpoint_callback = ModelCheckpoint(\n    monitor=\"val_loss\",\n    filename=\"{epoch:02d}-{val_loss_epoch:.3f}\",\n    mode=\"min\",\n    save_top_k=5,\n    save_weights_only=True,\n)\nearly_stopping = EarlyStopping(\n    monitor=\"val_loss\", mode=\"min\", patience=10, verbose=True\n)\ntrainer = Trainer(\n    max_epochs=config.epochs,\n    progress_bar_refresh_rate=10,\n    limit_val_batches=0.2,\n    val_check_interval=0.5,\n    amp_level='O2',\n    precision=16,\n    gpus=1,\n    default_root_dir=f\"{config.model_name}_{config.suffix}\",\n    num_sanity_val_steps=0,\n    callbacks=[checkpoint_callback, early_stopping],\n)\ntrainer.fit(model)","9ccd190d":"config.epochs = 8\nconfig.batch_size = 512\nconfig.model_name = 'cnn1d_aug'\nconfig.suffix = \"eval\"\nckpt_dir = \".\/cnn1d_aug_barlow\/lightning_logs\/version_0\/checkpoints\/\"\nconfig.weight_paths = glob(f\"{ckpt_dir}\/*.ckpt\")\nmodel = G2NetEval(config=config, train_df=train_df, val_df=val_df)","da9e5b8c":"checkpoint_callback = ModelCheckpoint(\n    monitor=\"val_loss\",\n    filename=\"{epoch:02d}-{val_loss_epoch:.3f}\",\n    mode=\"min\",\n    save_top_k=5,\n    save_weights_only=True,\n)\nearly_stopping = EarlyStopping(\n    monitor=\"val_loss\", mode=\"min\", patience=5, verbose=True\n)\ntrainer = Trainer(\n    max_epochs=config.epochs,\n    progress_bar_refresh_rate=10,\n    limit_train_batches=0.25,\n    limit_val_batches=0.2,\n    gpus=1,\n    default_root_dir=f\"{config.model_name}_{config.suffix}\",\n    num_sanity_val_steps=0,\n    callbacks=[checkpoint_callback, early_stopping],\n)\ntrainer.fit(model)\n","07604e41":"root_dir = (\n    f\".\/{config.model_name}_{config.suffix}\/lightning_logs\"\n)\npaths = list(glob(f\"{root_dir}\/version_0\/checkpoints\/*.ckpt\"))\n\nprint(paths)\nmodel = NetEval(config.weight_paths)\naveraged_w = average_model(paths)\nmodel.load_state_dict(averaged_w)\nmodel.eval()\ndevice = 'cuda'\nmodel.to(device)\nval_idx = val_idx[:40000]\nval_df = train_labels.loc[val_idx].reset_index(drop=True)\ndataset = GWDatasetBandpass(val_df, mode=\"val\")\ntest_loader = DataLoader(\n    dataset,\n    batch_size=int(config.batch_size),\n    num_workers=config.num_workers,\n    shuffle=False,\n    drop_last=False,\n)\ntk = tqdm(test_loader, total=len(test_loader))\nsub_index = val_df.id.values\nidx = 0\ncv_preds = train_labels.copy(deep=True)\ncv_preds[\"preds\"] = None\ncv_preds = cv_preds.set_index(\"id\")\n\nwith torch.no_grad():\n    for i, (im, _) in enumerate(tk):\n        im = im.to(device)\n        preds = model(im, mode='val').reshape(\n            -1,\n        )\n        o = preds.sigmoid().cpu().numpy()\n        for offset, val in enumerate(o):\n            cv_preds.loc[sub_index[idx], \"preds\"] = val\n            idx += 1\n\nt = cv_preds.loc[sub_index]\nauc = roc_auc_score(t[\"target\"], t[\"preds\"])\nprint(f\"\\nAUC:{auc:.4f}\")\nprint(t[\"preds\"].astype(np.float32).describe())\ntorch.cuda.empty_cache()\ngc.collect()\ntime.sleep(2)\n\ncv_preds = cv_preds.dropna()\ncv_preds[\"preds\"] = cv_preds[\"preds\"].astype(np.float32)\ncv_preds.to_csv(f\"cv_{config.model_name}_{config.suffix}.csv\", index=True)\nprint(cv_preds.head())\nprint(cv_preds[\"preds\"].describe())\nauc = roc_auc_score(cv_preds.loc[:, \"target\"], cv_preds.loc[:, \"preds\"])\nprint(\"auc score\", auc)","0f5715de":"os.makedirs(config.SUB_DIR, exist_ok=True)\n# # Test predictions\nsub = pd.read_csv(f\"{config.ROOT}\/sample_submission.csv\")\nsub.loc[:, \"target\"] = 0.0  # init to 0\nget_path = partial(get_file_path, config.TEST_ROOT)\nsub[\"file_path\"] = sub[\"id\"].apply(get_path)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nroot_dir = f\".\/{config.model_name}_{config.suffix}\/lightning_logs\"\npaths = list(glob(f\"{root_dir}\/version_0\/checkpoints\/*.ckpt\"))\nprint(paths)\nmodel = NetEval(paths, mode='val').to(device)\naveraged_w = average_model(paths)\nmodel.load_state_dict(averaged_w)\nmodel.eval();","07842e0e":"dataset = GWDatasetBandpass(sub, mode=\"test\")\ntest_loader = DataLoader(\n    dataset,\n    batch_size=int(config.batch_size),\n    num_workers=4,\n    shuffle=False,\n    drop_last=False,\n)\ntk = tqdm(test_loader, total=len(test_loader))\nsub_index = 0\nwith torch.no_grad():\n    for i, (waves, _) in enumerate(tk):\n        waves = waves.to(device)\n        preds = model(waves,\n                      mode=\"test\").reshape(-1,)\n        o = preds.sigmoid().cpu().numpy()\n        for val in o:\n            sub.loc[sub_index, \"target\"] = val\n            sub_index += 1\nsub = sub.drop(\"file_path\", axis=1)\nsub_path = (\n    f\"{config.SUB_DIR}\/submission_{config.model_name}_{config.suffix}.csv\"\n)\nsub.to_csv(sub_path, index=False)\nprint(sub.head())\nprint(sub.loc[:, \"target\"].describe())","460d5bcf":"!rm *.py\n!find . | grep -E \"(__pycache__|\\.pyc|\\.pyo$)\" | xargs rm -rf\n!rm -rf .\/.git","34859a98":"# Useful links:\n- [Project Github Repo](https:\/\/github.com\/sidml\/Self-Supervised-Learning-for-Gravitational-Waves)\n- [Barlow Twins Paper](https:\/\/arxiv.org\/pdf\/2103.03230.pdf)\n- [Gravitational Wave Dataset](https:\/\/www.kaggle.com\/c\/g2net-gravitational-wave-detection\/data)\n- [1D CNN trained using supervised learning](https:\/\/www.kaggle.com\/scaomath\/g2net-1d-cnn-gem-pool-pytorch-train-inference)","f5e3d52c":"# Freeze backbone layers and finetune FC\n\nAfter training the 1D CNN based model, we are able to get embeddings of the input waveform. Now, we need to train a Fully connected (FC) layer to make final prediction. For this we obtain embeddings for all the three detectors. Embeddings for Hanford\/Livingston are obtained from Net 1 and Net 2 provides the Virgo embedding. The concatenation of all the embeddings is used as input to the FC layer.\n\nDuring the FC layer training, we freeze the backbone layer weights (net 1 & 2) and only train the FC layer on a subset of training dataset (i used 25% of the dataset). It is sufficient to train FC layer for 7-8 epochs. Once the FC training is complete, we can use the model to detect GW.","5737fc2e":"# Make Test set predictions for LB evaluation","9663b6df":"![barlow.jpg](attachment:e7c8e015-acb2-4806-88aa-f23cdf3a26d9.jpg)\n\nI used the recently proposed Barlow Twins method for semi-supervised training. I would highly recommend going through the paper to understand the details. The basic idea is to have two models which see different versions of data (in our case the gravitational wave signal) and use the barlow twin's objective function to learn embeddings. While training on imagenet, people generally use cropping, flipping, blurring, random contrast etc\nto create different versions of the same image. \n\nIn our case, we can feed the data from Hanford\/Livingston into Net 1 and data from the Virgo detector into Net 2. Since the noise in Virgo detector is quite different from Hanford\/Livingston, we can imagine it as an augmented sample of the same underlying signal.","95f84000":"# Possible future work:\n\n- Try this semi-supervised approach with 2D CNN model. In this case, instead of using direct waveforms we can use the extracted CQT features.\n- Different 1d CNN architectures.\n- New Augmentations\n- Tuning the lambda parameter in barlow twins loss\n","bd02d446":"It is often hard to find a large collection of correctly labelled data. This scarcity of labelled real data exists for gravitational waves as well. One way to combat this shortage is to train on simulated data and hope the trained model is useful for the real dataset as well. The success of such an approach is dependant on the quality of simulation. However, astronomy doesn't have a shortage of raw data. So, I felt the application of semi-supervised training approach is particularly relevant for this task. One of the main components of semi-supervised training is the augmentation scheme. However in my experiments I found that it extremely difficult to come up with a good augmentation for this dataset.\n\nWhile thinking on how to augment the data such that the signal characteristic is preserved, I realized that we already have an augmented dataset! My main insight was that whenever there is a gravitational wave, all three detectors must detect it. So, we can imagine that we have access to augmented versions of the same signal. The different geographical position and noise characteristic already takes care of the augmentation :-)","c8e806ec":"# Validation set result","46af9fda":"# Train barlow twins using self-supervised learning\n\nCurrently most semi-supervised work is focused on images. So, usually inputs are images and 2D CNN is used as model. However, for faster training I decided to train using waveforms. I trained a 1D CNN model inspired by [this post](https:\/\/www.kaggle.com\/scaomath\/g2net-1d-cnn-gem-pool-pytorch-train-inference). I observed that adding a GRU layer and reducing the original pooling size led to better results. I also removed the final fully connected layers since we require the models to produce embeddings. The bandpassed waveforms are fed as input to the model and we get embeddings of size 2048 as output.\n\nThe output embeddings produced by both the networks are normalized along the batch dimension and we calculate the invariance and redundancy reduction terms. Using these terms, we get the final loss used for training the networks. In the original paper, they used LARS optimizer but I found that AdamW with an initial learning rate of 1e-4 also works. (I haven't tried LARS optimizer yet, so maybe it will work even better.) Currently, I am only using pycbc generated noise as an additional augmentation. "}}