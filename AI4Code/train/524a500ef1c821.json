{"cell_type":{"2cc42e39":"code","6298bf4c":"code","0cfadd1e":"code","0b811a08":"code","a66718d9":"code","7aed5080":"code","3d9ebb91":"code","f981162e":"code","2144d815":"code","86c32ebc":"code","2c2c52ea":"code","cecea103":"code","7678cb29":"code","9a57b441":"code","f25fd4e9":"code","c0df8368":"code","a665f7a8":"code","e807448c":"code","70462d90":"code","63c8d035":"code","ff3172b7":"code","5c249b41":"code","59f38e3b":"code","9afff486":"code","8b0eda17":"code","ed8418ab":"code","54a3d556":"code","c95c183b":"code","a53dde1d":"code","15db2a99":"code","33a8050e":"code","a2c98180":"code","4ab5ebef":"code","8363abd9":"code","a9efd4ac":"code","177f73a9":"code","3cdf4250":"code","20045c24":"code","28518943":"code","55a864ca":"code","d463e08f":"code","804eb5a2":"code","5081ef22":"code","f21c26b8":"code","a87a46ae":"code","33862cf9":"code","f30f8edd":"code","a9cbe722":"code","e1f18ae2":"code","73c81c81":"code","1711b443":"code","4e17d4fd":"code","509b8a86":"code","ce47fd99":"code","15c3bccf":"code","f7f27581":"code","b17b7a99":"code","2017a00b":"code","ee6fc1a1":"code","e7ad091e":"code","57a69786":"code","d2438c9b":"code","679ee539":"code","02c330c9":"code","918a969b":"code","448520f2":"code","c9539565":"code","5e210d23":"code","b23b883e":"code","0046b793":"code","e484615e":"code","300aa786":"code","8f33b917":"code","021170a9":"code","04e20709":"code","6f6751ca":"markdown","17bab9fc":"markdown","e1ba0ccd":"markdown","6f234430":"markdown","e3d8a510":"markdown","c46f7e49":"markdown","a729ddef":"markdown","bd12bc62":"markdown","fbb775ea":"markdown","d7fba67d":"markdown","ab8cd592":"markdown","2eba7bcb":"markdown","c93fbd29":"markdown","dc2916af":"markdown","7f9ef6f8":"markdown","0e7b9ec7":"markdown","8a9c1e3c":"markdown","6b607b44":"markdown","477cce47":"markdown","34fa63da":"markdown","7b5ae223":"markdown","22500ad5":"markdown","3f0afdb2":"markdown","ea742d2e":"markdown","1d117469":"markdown","98cf9a84":"markdown","3ce8f927":"markdown","5fdd1644":"markdown","0014c2de":"markdown","baa6d50a":"markdown","c7a95d6b":"markdown","9cb99cef":"markdown","8d6f4174":"markdown","2a09198a":"markdown","f6c29374":"markdown","381b8d10":"markdown","9c972e15":"markdown","2c20138f":"markdown","e0152681":"markdown","ddce3a35":"markdown","9df6ea89":"markdown","ca963d8d":"markdown","e4b0e24c":"markdown","65360a70":"markdown","50a9e50a":"markdown","4ab46cc7":"markdown","c7ed6e62":"markdown","e3c7beed":"markdown","87b5c1f3":"markdown","976652ad":"markdown","18dbba8a":"markdown","508a828b":"markdown","605a35c3":"markdown","2201b2ae":"markdown","09051adb":"markdown","7820bd83":"markdown","bcf73e00":"markdown","b1f5a671":"markdown","0ae709c8":"markdown","2abe7276":"markdown","a93e7b13":"markdown","dded7944":"markdown","3bf1d11c":"markdown","0205d4e3":"markdown","1e994394":"markdown","90e4b7b0":"markdown","4b52ee10":"markdown","a19996f4":"markdown","1238862e":"markdown","f156ae01":"markdown","4d7bd488":"markdown"},"source":{"2cc42e39":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import train_test_split\n\nimport os\niowa_file_path1 = '..\/input\/home-data-for-ml-course\/train.csv'\niowa_file_path2 =\"..\/input\/home-data-for-ml-course\/test.csv\"\n\nhome_data = pd.read_csv(iowa_file_path1)\ntest_data=pd.read_csv(iowa_file_path2)\n\nprint(home_data.isnull().sum().sum())","6298bf4c":"\ntrain_data=home_data.drop(['SalePrice'], axis=1)\ntarget= home_data['SalePrice']\nId=test_data['Id']","0cfadd1e":"\nfig = plt.figure(figsize = (15,10))\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.hist(home_data['SalePrice'])\nhome_data['SalePrice'].describe()","0b811a08":"from matplotlib import pyplot as plt\ninputs= pd.concat([train_data,target], axis=1)\ncorr = inputs[inputs.SalePrice>1].corr()\ntop_corr_cols = corr[abs((corr.SalePrice)>=0.5)].SalePrice.sort_values(ascending=False).keys()\n\n# print(top_corr_cols)\n\ntop_corr = corr.loc[top_corr_cols, top_corr_cols]\ndropSelf = np.zeros_like(top_corr)\n# print(dropSelf)\n\ndropSelf[np.triu_indices_from(dropSelf)] = True\nplt.figure(figsize=(20,20))\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nsns.heatmap(top_corr, cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            annot=True, fmt=\".2f\", mask=dropSelf)\nsns.set(font_scale=0.5)\nplt.show()\ndel corr, dropSelf, top_corr","a66718d9":"plt.figure(figsize=(20,10))\nplt.xlabel('OverallQual', fontsize=20)\nplt.ylabel('SalePrice', fontsize=20)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\n\nsns.boxplot(x= home_data['OverallQual'], y= target)\n\n","7aed5080":"plt.figure(figsize=(20,10))\nplt.xlabel('GrLivArea', fontsize=20)\nplt.ylabel('SalePrice', fontsize=20)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.scatter(x= home_data['GrLivArea'], y= target)","3d9ebb91":"data= home_data.loc[(home_data['GrLivArea']>4000) & (home_data['SalePrice']< 200000)]\nprint(data['Id'])            \n           ","f981162e":"# print(home_data.describe())\nprint('Info of input:\\n',home_data.info())\nprint('Info of test data:\\n',test_data.info())","2144d815":"# Quality and condition columns are to be numerically encoded \n\ncleanup_nums = {\"ExterQual\":     {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1},\n            \"ExterCond\":     {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1},\n            \"HeatingQC\":       {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1},\n            \"KitchenQual\":     {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1},\n            \"CentralAir\":     {\"Y\": 1, \"N\": 0},\n# below features also have missing values that can be replaced by 0\n    \"BsmtQual\":      {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1, \"NA\": 0},\n    \"BsmtCond\":      {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1, \"NA\": 0},\n    \"BsmtExposure\":    {\"Gd\":4, \"Av\":3 , \"Mn\":2, \"No\":1, \"NA\":0},\n    \"BsmtFinType1\":    {\"None\":0, \"Unf\":1, \"LwQ\":2, \"Rec\":3, \"BLQ\":4, \"ALQ\":5, \"GLQ\":6},\n    \"BsmtFinType2\":    {\"None\":0, \"Unf\":1, \"LwQ\":2, \"Rec\":3, \"BLQ\":4, \"ALQ\":5, \"GLQ\":6},\n    \"FireplaceQu\":     {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1, \"NA\": 0},\n    \"GarageQual\":      {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1, \"NA\": 0},\n    \"GarageCond\":      {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1, \"NA\": 0},\n    'GarageFinish':    {'Fin':3,'RFn':2, 'Unf':1, 'None':0},\n    \"PoolQC\":     {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1, \"NA\": 0}}\n","86c32ebc":"train_data.replace(cleanup_nums, inplace=True)\ntest_data.replace(cleanup_nums, inplace=True)\n\ntrain_data[['PoolQC','FireplaceQu','GarageQual','GarageCond',\n            'GarageFinish','BsmtQual','BsmtCond',\"BsmtExposure\",\"BsmtFinType1\",\n            \"BsmtFinType2\"]]=train_data[['PoolQC','FireplaceQu', 'GarageQual',\n                        'GarageCond','GarageFinish','BsmtQual',\n                        'BsmtCond', \"BsmtExposure\",\"BsmtFinType1\",\n                                         \"BsmtFinType2\"]].fillna(0)\n\ntest_data[['PoolQC','FireplaceQu','GarageQual','GarageCond',\n           'GarageFinish','BsmtQual','BsmtCond', \"BsmtExposure\",\"BsmtFinType1\",\n           \"BsmtFinType2\"]]=test_data[['PoolQC','FireplaceQu', 'GarageQual',\n                                'GarageCond','GarageFinish',\n                                'BsmtQual','BsmtCond',\"BsmtExposure\",\n                                \"BsmtFinType1\",\"BsmtFinType2\"]].fillna(0)\n\nprint(train_data.info())\nprint(test_data.info())","2c2c52ea":"print(train_data['Alley'].unique(),'\\n',train_data['Alley'].describe(),'\\n')\n\nprint(train_data['Utilities'].unique(),'\\n',train_data['Utilities'].describe(),'\\n')\n\nprint(train_data['MasVnrType'].unique(),'\\n',train_data['MasVnrType'].describe(),'\\n')\n\nprint(train_data['Fence'].unique(),'\\n',train_data['Fence'].describe(),'\\n')\n\nprint(train_data['MiscFeature'].unique(),'\\n',train_data['MiscFeature'].describe(),'\\n')\n\nprint(train_data['Functional'].unique(),'\\n',train_data['Functional'].describe(),'\\n')\n\nprint(train_data[\"GarageType\"].unique(),'\\n',train_data[\"GarageType\"].describe(),'\\n')\n\n\n","cecea103":"train_data.drop(['Utilities'], axis=1, inplace=True)\ntest_data.drop(['Utilities'], axis=1, inplace=True)\n","7678cb29":"# print(train_data['LotFrontage'].head())\n\ntrain_data[['Alley','MiscFeature','Fence','MasVnrType',\"GarageType\"]]=train_data[['Alley', 'MiscFeature',\n                        'Fence', 'MasVnrType',\"GarageType\"]].fillna(\"None\")\n\ntest_data[['Alley', 'MiscFeature','Fence', 'MasVnrType',\"GarageType\"]]=test_data[['Alley', 'MiscFeature',\n                     'Fence', 'MasVnrType',\"GarageType\"]].fillna(\"None\")\n\n\ndata=train_data[['Alley', 'MiscFeature','Fence', 'MasVnrType', 'GarageFinish']]\nj=1\nfig=plt.figure(figsize=(15,10))\nfor i in data.columns:\n    if j<=5:\n        ax1 = fig.add_subplot(2,3,j)\n        sns.barplot(data= data, x=i, y=target, ax = ax1, label=i)\n        plt.xlabel(i,fontsize=14)\n        ax1.xaxis.set_ticks_position('top')\n        plt.xticks(rotation='vertical', fontsize=12) \n        plt.yticks(fontsize=8)\n        \n    j+=1\nax1 = fig.add_subplot(2,3,6)  \nsns.boxplot(x= home_data['GarageFinish'], y= target)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=8)\nplt.xlabel('Barplot of GarageFinish', fontsize=14)\n\n\n","9a57b441":"print(test_data['Exterior1st'].unique(),'\\n',test_data['Exterior1st'].describe(),'\\n')\n\nprint(test_data['Exterior2nd'].unique(),'\\n',test_data['Exterior2nd'].describe(),'\\n')\n\nprint(test_data['SaleType'].unique(),'\\n',test_data['SaleType'].describe())","f25fd4e9":"train_data.loc[train_data['GarageYrBlt'].isnull(),\n               'GarageYrBlt']=train_data['YearBuilt']\n\ntest_data.loc[test_data['GarageYrBlt'].isnull(),\n              'GarageYrBlt']=test_data['YearBuilt']","c0df8368":"# looking for id with no garage\ndata1= test_data.loc[(test_data['GarageArea'].isnull())]\nprint(data1['Id'])\n","a665f7a8":"test_data.loc[1116,'GarageCars']=0\nprint(test_data.loc[1116,'GarageType'],'\\n')\n","e807448c":"#looking for id with no basement\ndata2= test_data.loc[((test_data['BsmtFinSF1'].isnull())&(test_data['BsmtFinSF2'].isnull())&\n                      (test_data['BsmtFullBath'].isnull())&(test_data['BsmtHalfBath'].isnull()))]\n\nprint(data2[['Id','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1',\n        'BsmtFinType2','BsmtFinSF1','BsmtFinSF2','TotalBsmtSF','BsmtUnfSF',\n             'BsmtFullBath','BsmtHalfBath']],'\\n')\n\n","70462d90":"## updating values for no basement condition\ntest_data.loc[660 ,'BsmtFullBath']=0\ntest_data.loc[660 ,'BsmtHalfBath']=0\ntest_data.loc[660 ,'BsmtFinSF1']=0\ntest_data.loc[660 ,'BsmtFinSF2']=0\ntest_data.loc[660 ,'TotalBsmtSF']=0\ntest_data.loc[660 ,'BsmtUnfSF']=0","63c8d035":"data3= test_data.loc[(test_data['MasVnrType']=='None')]\nprint(data3['MasVnrArea'].isnull().sum(),data3[['MasVnrType','MasVnrArea']].shape )","ff3172b7":"\ntest_data.loc[test_data['MasVnrType']=='None', 'MasVnrArea']=0\n","5c249b41":"test_data['BsmtFullBath']=test_data['BsmtFullBath'].fillna(0)\ntest_data['BsmtHalfBath']=test_data['BsmtHalfBath'].fillna(0)","59f38e3b":"print(train_data.info())\nprint(test_data.info())","9afff486":"#collecting categorical data columns\n\ncategory=(train_data.select_dtypes(include=['object']).copy()).columns\nnon_category=list( set(train_data.columns) - set(category))\n\nprint('non_categorical columns: \\n',non_category)\nprint('categorical columns: \\n',category )\n\n","8b0eda17":"train_data[category]=train_data[category].fillna(train_data[category].mode().iloc[0])\ntest_data[category]=test_data[category].fillna(test_data[category].mode().iloc[0])\n\ntrain_data[non_category]= train_data[non_category].fillna(train_data[non_category].mean().iloc[0])\ntest_data[non_category]= test_data[non_category].fillna(test_data[non_category].mean().iloc[0])\n\nprint(train_data.info())\nprint(test_data.info())","ed8418ab":"data1 = train_data[['MSZoning', 'Street', 'LotShape', 'LandContour', 'LotConfig',\n       'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n        'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n            'Foundation', 'Heating', 'Electrical', 'PavedDrive',\n                'SaleType', 'SaleCondition','HouseStyle','GarageType','Functional']]\n\ndata1=pd.concat([data1,target],axis=1)\n# print(data.info())\n\ni = 1\nfig = plt.figure(figsize = (20,50))\nfor c in list(data1.columns):\n    if i <= 3:\n        if c != 'SalePrice':\n            ax1 = fig.add_subplot(8,3,i)\n            sns.countplot(data = data1, x=c, ax = ax1)\n            plt.xlabel(c,fontsize=12)\n            ax1.xaxis.set_ticks_position('top')\n            plt.xticks(rotation='vertical',fontsize=14)\n            plt.yticks(fontsize=14)\n\n            \n            ax2 = fig.add_subplot(8,3,i+3)\n            sns.boxplot(data=data1, x=c, y='SalePrice', ax=ax2)\n            plt.xlabel(c,fontsize=12)\n            ax1.xaxis.set_ticks_position('top')\n            plt.xticks(rotation='vertical',fontsize=14)\n            plt.yticks(fontsize=14)\n\n       \n    i = i +1\n    if i == 4: \n        fig = plt.figure(figsize = (20, 50))\n        i =1","54a3d556":"train_data[['MSSubClass_o','YrSold_o','MoSold_o']]=train_data[['MSSubClass',\n                                             'YrSold','MoSold']].astype('O')\ntest_data[['MSSubClass_o','YrSold_o','MoSold_o']]=test_data[['MSSubClass',\n                                            'YrSold','MoSold']].astype('O')\n\nfinal_categorical_cols=list(category) +['MSSubClass_o','YrSold_o','MoSold_o']\nprint(final_categorical_cols)\n\nprint(train_data.info())\nprint(test_data.info())","c95c183b":"print(train_data['MSSubClass_o'].unique(),'\\n',train_data['MSSubClass_o'].describe())\nprint(train_data['YrSold_o'].unique(),'\\n',train_data['YrSold_o'].describe())\nprint(train_data['MoSold_o'].unique(),'\\n',train_data['MoSold_o'].describe())\n","a53dde1d":"total_data =pd.concat([train_data,test_data],axis=0)\nlen=train_data.shape[0]\nprint('total length:',len)\nnon_object_data=pd.get_dummies(total_data, columns=final_categorical_cols,drop_first=True)\n\ntrain_data_dummy= non_object_data[0:len]\ntest_data_dummy=non_object_data[len:]\nprint('all final columns after OneHotEncoding:\\n',train_data.columns)\n\n# data.head()\n\nprint(train_data_dummy.shape,test_data_dummy.shape)\ntest_data.isnull().sum().sum()","15db2a99":"from matplotlib import pyplot as plt\n\ninputs=pd.concat([home_data['SalePrice'],train_data_dummy],axis=1)\ncorr = inputs[inputs.SalePrice>1].corr()\ntop_corr_cols = corr[abs((corr.SalePrice)>=0.5)].SalePrice.sort_values(ascending=False).keys()\n\n# print(top_corr_cols)\n\ntop_corr = corr.loc[top_corr_cols, top_corr_cols]\ndropSelf = np.zeros_like(top_corr)\n# print(dropSelf)\n\ndropSelf[np.triu_indices_from(dropSelf)] = True\nplt.figure(figsize=(20,20))\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nsns.heatmap(top_corr, cmap=sns.diverging_palette(220, 10, as_cmap=True), annot=True, fmt=\".2f\", mask=dropSelf)\nsns.set(font_scale=1)\nplt.show()\ndel corr, dropSelf, top_corr","33a8050e":"from sklearn.ensemble import RandomForestRegressor\n\ninput_data= train_data_dummy.drop(['Id'], axis=1)\ntest_data_dummy=test_data_dummy.drop(['Id'], axis=1)\n\nrfr_imp= RandomForestRegressor(n_estimators=100)\nrfr_imp.fit(input_data, target)\n\nimportance =pd.Series(rfr_imp.feature_importances_, index=input_data.columns)\nimportance.nlargest(15).plot(kind='bar')","a2c98180":"area=['GrLivArea','GarageArea','GarageCars','LotArea','MasVnrArea','TotalBsmtSF','1stFlrSF',\n     '2ndFlrSF','FullBath','HalfBath','WoodDeckSF','OpenPorchSF','EnclosedPorch','Fireplaces',\n      'LowQualFinSF','TotRmsAbvGrd']\ncorr = train_data[area].corr()\nplt.figure(figsize=(10,10))\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nsns.heatmap(corr, cmap=sns.diverging_palette(220, 10, as_cmap=True), annot=True, fmt=\".2f\")\nsns.set(font_scale=1)\nplt.show()\n","4ab5ebef":"plt.figure(figsize= (100, 50))\ntrain_data['LowQualFinSF'].hist(bins= int(180\/5),grid=True,alpha=1,color='gray',label='LowQualFinSF')\ntrain_data['WoodDeckSF'].hist(bins =int(180\/5), grid=True,alpha=1,color= 'k',label='WoodDeckSF')\ntrain_data['OpenPorchSF'].hist(bins =int(180\/5),grid=True,alpha=1,color='purple',label= 'OpenPorchSF')\ntrain_data['MasVnrArea'].hist(bins =int(180\/5),grid=True,alpha=0.5,color='Yellow',label='MasVnrArea')\ntrain_data['GrLivArea'].hist(bins = int(180\/5), grid=True, alpha=1, color= 'Red', label= 'GrLivArea')\ntrain_data['1stFlrSF'].hist(bins = int(180\/5),grid=True, alpha=0.5, color= 'Blue', label= '1stFlrSF')\ntrain_data['2ndFlrSF'].hist(bins = int(180\/5),grid=True, alpha=0.7, color= 'green', label= '2ndFlrSF')\nplt.legend(fontsize= 80)\nplt.xticks(fontsize= 80)\nplt.yticks(fontsize= 80)\n","8363abd9":"train_data.loc[train_data['LowQualFinSF']>0,['GrLivArea','1stFlrSF','2ndFlrSF','LowQualFinSF']].head()","a9efd4ac":"train_data['2ndFlr']=np.where(train_data['2ndFlrSF']==0,0,1)\ntest_data['2ndFlr']=np.where(test_data['2ndFlrSF']==0,0,1)\nprint(train_data[['2ndFlr','2ndFlrSF']])","177f73a9":"train_data=train_data.drop(['1stFlrSF','2ndFlrSF','LowQualFinSF'],axis=1)\n\ntest_data=test_data.drop(['1stFlrSF','2ndFlrSF','LowQualFinSF'],axis=1)","3cdf4250":"train_data=train_data.loc[(train_data['GrLivArea'] < 4500)]\ntarget1=home_data.loc[(home_data['GrLivArea'] < 4500)]\ntarget=target1['SalePrice']\nprint(train_data.shape,target.shape)","20045c24":"quality=['OverallQual','OverallCond','YearBuilt','ExterQual','BsmtQual',\n        'KitchenQual','FireplaceQu','GarageQual','PoolQC']\ncorr = (pd.concat([train_data[quality], target],axis=1)).corr()\nplt.figure(figsize=(10,10))\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nsns.heatmap(corr, cmap=sns.diverging_palette(220, 10, as_cmap=True), annot=True, fmt=\".2f\")\nsns.set(font_scale=1)\nplt.show()\n\ndata1=train_data[quality]\ni = 1\nfig = plt.figure(figsize = (20,10))\nfor c in list(quality):\n    if c!='YearBuilt':\n        if i <= 4:\n            ax1 = fig.add_subplot(2,4,i)\n            sns.countplot(data = data1, x=c, ax = ax1, label= c)\n            plt.xticks(rotation='vertical')            \n        i = i +1\n        if i == 5: \n            fig = plt.figure(figsize = (20, 10))\n            i =1","28518943":"train_data['Pool']=np.where(train_data['PoolArea']==0, 0,1)\ntest_data['Pool']=np.where(test_data['PoolArea']==0, 0,1)\ntrain_data=train_data.drop(['PoolQC'],axis=1)\ntest_data=test_data.drop(['PoolQC'],axis=1)","55a864ca":"basement=['BsmtHalfBath','BsmtFullBath','TotalBsmtSF','BsmtUnfSF','BsmtFinSF2','BsmtFinType2',\n          'BsmtFinSF1','BsmtFinType1','BsmtExposure','BsmtCond','BsmtQual']\ncorr = (pd.concat([train_data[basement],home_data['SalePrice']], axis=1)).corr()\nplt.figure(figsize=(10,10))\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nsns.heatmap(corr, cmap=sns.diverging_palette(220, 10, as_cmap=True), annot=True, fmt=\".2f\")\nsns.set(font_scale=1)\nplt.show()\n\ndata1=train_data[basement]\ni = 1\nfig = plt.figure(figsize = (20,10))\nfor c in list(basement):\n    if c not in list(['TotalBsmtSF','BsmtUnfSF','BsmtFinSF2','BsmtFinSF1']):\n        if i <= 4:\n            ax1 = fig.add_subplot(2,4,i)\n            sns.countplot(data = data1, x=c, ax = ax1, label= c)\n            plt.xticks(rotation='vertical')            \n        i = i +1\n        if i == 5: \n            fig = plt.figure(figsize = (20, 10))\n            i =1","d463e08f":"train_data[['TotalBsmtSF','BsmtUnfSF','BsmtFinSF2','BsmtFinSF1']].head()","804eb5a2":"train_data['BsmtFin2']=np.where(train_data['BsmtFinSF2']==0,0,1)\ntest_data['BsmtFin2']=np.where(test_data['BsmtFinSF2']==0,0,1)\n\n\ntrain_data['BsmtFin1']=np.where(train_data['BsmtFinSF1']==0,0,1)\ntest_data['BsmtFin1']=np.where(test_data['BsmtFinSF1']==0,0,1)\n\nprint(train_data[['BsmtFin2','BsmtFinSF2','BsmtFin1','BsmtFinSF1']].head())","5081ef22":"train_data=train_data.drop(['BsmtUnfSF', 'BsmtFinSF2','BsmtFinSF1'],axis=1)\ntest_data=test_data.drop(['BsmtUnfSF', 'BsmtFinSF2','BsmtFinSF1'],axis=1)","f21c26b8":"garage= ['GarageFinish','GarageCars','GarageArea','GarageQual','GarageCond','GarageYrBlt']\ndata1=pd.concat([train_data[garage],train_data['GarageType'],home_data['SalePrice']],axis=1)\ncorr = data1.corr()\nplt.figure(figsize=(10,10))\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nsns.heatmap(corr, cmap=sns.diverging_palette(220, 10, as_cmap=True), annot=True, fmt=\".2f\")\nsns.set(font_scale=1)\nplt.show()\n\ni = 1\nfig = plt.figure(figsize = (20,5))\nfor c in data1.columns:\n    if c not in list(['GarageArea','GarageYrBlt']):\n        if i <= 5:\n            ax1 = fig.add_subplot(1,5,i)\n            sns.countplot(data = data1, x=c, ax = ax1, label= c)\n            plt.xticks(rotation='vertical')  \n        i = i +1\n","a87a46ae":"train_data=train_data.drop(['GarageCond', 'GarageArea'],axis=1)\ntest_data=test_data.drop(['GarageCond', 'GarageArea'],axis=1)","33862cf9":"train_data['TotalBathrooms']= (train_data['FullBath']+0.5*train_data['HalfBath']+\n                                train_data['BsmtFullBath']+0.5*train_data['BsmtHalfBath'])\nprint(train_data['TotalBathrooms'].head())\nplt.scatter(x= train_data['TotalBathrooms'], y= target )\nplt.show()","f30f8edd":"test_data['TotalBathrooms']= (test_data['FullBath']+0.5*test_data['HalfBath']+\n                    test_data['BsmtFullBath']+0.5*test_data['BsmtHalfBath'])\n","a9cbe722":"train_data['Age']= train_data['YrSold']-train_data['YearRemodAdd']\ntrain_data['Remodeled']= np.where(train_data['YearRemodAdd']-train_data['YearBuilt']==0, 0, 1)\ntrain_data['Isnew']=np.where(train_data['YrSold']-train_data['YearBuilt']==0, 1, 0)\n# print(input_data[['Remodeled','Age']].head())\n\ncorrelation=pd.concat([train_data['Age'],target],axis=1).corr()\nprint(correlation)\n\nplt.figure(figsize=(15,10))\nplt.subplot(221)\nplt.scatter(x=train_data['Age'], y= target,s=1)\nplt.subplot(222)\nsns.barplot(x=train_data['Remodeled'],y= target)\nplt.axhline(y=178000,linewidth=1, color='k')\nplt.subplot(223)\nsns.barplot(x=train_data['Isnew'],y= target)\nplt.axhline(y=178000,linewidth=1, color='k')\nplt.show()","e1f18ae2":"test_data['Age']= test_data['YrSold']-test_data['YearRemodAdd']\ntest_data['Remodeled']= np.where(test_data['YearRemodAdd']-test_data['YearBuilt']==0, 0, 1)\ntest_data['Isnew']=np.where(test_data['YrSold']-test_data['YearBuilt']==0, 1, 0)\n\nprint(test_data[['Remodeled','Age']])","73c81c81":"train_data=train_data.drop(['YearBuilt','YearRemodAdd'],axis=1)\ntest_data=test_data.drop(['YearBuilt','YearRemodAdd'],axis=1)","1711b443":"data4=pd.concat([target,train_data['Neighborhood']],axis=1)\n\ndata_41 = data4.groupby('Neighborhood', as_index=False)['SalePrice'].mean()\ndata_41=data_41.sort_values(['SalePrice','Neighborhood'], ascending=[1,0])\n\ndata_42 = data4.groupby('Neighborhood', as_index=False)['SalePrice'].median()\ndata_42=data_42.sort_values(['SalePrice','Neighborhood'], ascending=[1,0])\n\n# print(data_41.head(),data_42.head()) \nplt.figure(figsize=(30,20))\nplt.subplot(211)\nplt.xlabel('mean sale price for each neighborhood', fontsize=18)\nsns.barplot(x=data_41['Neighborhood'],y= data_41['SalePrice'])\nplt.axhline(y=110000,linewidth=1, color='k')\nplt.axhline(y=240000,linewidth=1, color='k')\nplt.xticks(rotation='vertical', fontsize=18) \nplt.yticks(fontsize=18) \n\nplt.subplot(212)\nsns.barplot(x=data_42['Neighborhood'],y= data_42['SalePrice'])\nplt.axhline(y=110000,linewidth=1, color='k')\nplt.axhline(y=240000,linewidth=1, color='k')\nplt.xticks(rotation='vertical', fontsize=18) \nplt.xlabel('median sale price', fontsize=18)\nplt.yticks(fontsize=18) \nplt.show()\nprint(data_42['Neighborhood'].unique(),'\\n',train_data['Neighborhood'].unique())","4e17d4fd":"mapping= {'MeadowV':0, 'IDOTRR':0, 'BrDale':0,'StoneBr':2, 'NoRidge':2, 'NridgHt':2}\ntrain_data['Neighborhood_bin']=train_data['Neighborhood'].map(mapping)\ntrain_data['Neighborhood_bin']=train_data['Neighborhood_bin'].fillna('1')\n\ntest_data['Neighborhood_bin']=test_data['Neighborhood'].map(mapping)\ntest_data['Neighborhood_bin']=test_data['Neighborhood_bin'].fillna('1')\nprint(train_data['Neighborhood_bin'].value_counts(),'\\n', test_data['Neighborhood_bin'].value_counts())","509b8a86":"train_data['TotalPorchArea']= (train_data['WoodDeckSF']+train_data['OpenPorchSF']+\n         train_data['EnclosedPorch']+ train_data['3SsnPorch']+train_data['ScreenPorch'])\nplt.figure(figsize=(10,10))\nplt.scatter(x=train_data['TotalPorchArea'], y= target,s=3)\nprint(train_data[['TotalPorchArea','WoodDeckSF','OpenPorchSF','EnclosedPorch','3SsnPorch',\n                 'ScreenPorch']].head()) \ncorrelation=(pd.concat([train_data[['TotalPorchArea','WoodDeckSF','OpenPorchSF',\n            'EnclosedPorch','3SsnPorch','ScreenPorch']],home_data['SalePrice']],\n                       axis=1)).corr()\nprint(correlation)\n# x=train_data.loc[(train_data['TotalPorchArea'])==0]\n\n","ce47fd99":"test_data['TotalPorchArea']=(test_data['WoodDeckSF']+test_data['OpenPorchSF']+\n  test_data['EnclosedPorch']+ test_data['3SsnPorch']+test_data['ScreenPorch'])","15c3bccf":"\ntrain_data['Has3SSNPorch']=np.where(train_data['3SsnPorch']==0,0,1)\ntest_data['Has3SSNPorch']=np.where(test_data['3SsnPorch']==0,0,1)\n\n# print(train_data[['TotalPorchArea','Has3SSNPorch','3SsnPorch']].head())\n\ntrain_data.drop(['3SsnPorch'],axis=1)\ntest_data.drop(['3SsnPorch'],axis=1)","f7f27581":"train_data['TotalArea']=train_data['GrLivArea']+train_data['TotalBsmtSF']\nplt.figure(figsize=(10,5))\nplt.scatter(x=train_data['TotalArea'],y=target,s=3)","b17b7a99":" \ncorrelation=(pd.concat([train_data['TotalArea'],target],axis=1)).corr()\nprint(correlation)","2017a00b":"test_data['TotalArea']=test_data['GrLivArea']+test_data['TotalBsmtSF']\n","ee6fc1a1":"\ndrop=['YrSold','MoSold','MSSubClass', 'GarageYrBlt','TotalBsmtSF',\n      'TotRmsAbvGrd']\ntrain_data=train_data.drop(drop,axis=1)\ntest_data=test_data.drop(drop,axis=1)\nprint(train_data.columns, train_data.shape)","e7ad091e":"category=(train_data.select_dtypes(include=['object']).copy()).columns\ncategory=list(category)+['OverallQual', 'OverallCond']\ndummy=list(set(category)-set(['MoSold','YrSold']))\nnon_category=list( set(train_data.columns) - set(category))\n\nprint((category))","57a69786":"skewValue = train_data[non_category].skew(axis=0)\ndata=[]\nfor i,index in enumerate(skewValue):\n    if abs(index)>0.8:\n        data.append(non_category[i])\nprint(print(skewValue.head()),data)\nprint(train_data.isnull().sum().sum(),test_data.isnull().sum().sum())\n\ntrain_data[data]=np.log1p(train_data[data])\ntarget=np.log1p(target)\ntarget.hist(bins = 40)\nprint(train_data[data].head())","d2438c9b":"test_data[data]=np.log1p(test_data[data])","679ee539":"total_data =pd.concat([train_data,test_data],axis=0)\nlen=train_data.shape[0]\nprint('total length:',len)\ntotal_dummy=pd.get_dummies(total_data, columns=dummy,drop_first=True)\n\ntrain_data_final= total_dummy[0:len]\ntest_data_final=total_dummy[len:]\nprint(train_data_final.shape, target.shape)","02c330c9":"\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import Ridge, Lasso,ElasticNet\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error, make_scorer  \nfrom sklearn import metrics \nfrom sklearn.model_selection import cross_val_score,KFold\n \n\nkfolds = KFold(n_splits=5, shuffle=True, random_state=42)\n\ndef rmse(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model, X,y):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kfolds))\n    return (rmse)\nscaler = RobustScaler()\nrobust_scaled_df = scaler.fit_transform(train_data_final)\nrobust_scaled_test = scaler.fit_transform(test_data_final)","918a969b":"# # [1e-5, 1e-4, 1e-3, 1e-2, 1e-1,1,10,20,50,100,150,200]\n# param_rcv={'alpha':np.arange(1,5,1)}\n# gs_rcv=GridSearchCV(Ridge(),param_rcv,cv=kfolds,\n#                     scoring='neg_mean_squared_error',verbose=3)\n# gs_rcv.fit(robust_scaled_df,target)  \n# print('optimal estimator: %s' % gs_rcv.best_estimator_)\n# rmse1 = cv_rmse(gs_rcv.best_estimator_, robust_scaled_df, target)\n# print(\"Ridge score: {:.4f} ({:.4f})\\n\".format(rmse1.mean(),rmse1.std()))\n\n\n","448520f2":"# # [1e-5, 1e-4, 1e-3, 1e-2, 1e-1,1,10,20,50,100,150,200],\n# param_lcv={'max_iter':np.arange(12000,15000,1000),\n#            'alpha':[1e-5, 1e-4, 1e-3, 1e-2]}\n# gs_lcv=GridSearchCV(Lasso(),param_lcv,scoring='neg_mean_squared_error',\n#                     verbose=3,cv=kfolds)\n# gs_lcv.fit(robust_scaled_df,target)     \n# print('optimal estimator: %s' % gs_lcv.best_estimator_)\n# rmse2 = cv_rmse(gs_lcv.best_estimator_, robust_scaled_df, target)\n# print(\"Lasso score: {:.4f} ({:.4f})\\n\".format(rmse2.mean(),rmse2.std()))\n","c9539565":"# # [1e-5, 1e-4, 1e-3, 1e-2, 1e-1,1,10,20,50,100,150,200]\n# param_encv={'max_iter':[5e5,1e6,5e6,1e7,5e7,1e8],\n#             'alpha':[1e-5, 1e-4, 1e-3, 1e-2],\n#             'l1_ratio':np.arange(0.1,0.5,0.1)}\n# gs_encv=GridSearchCV(ElasticNet(),param_encv,scoring='neg_mean_squared_error',\n#                      verbose=3,cv=kfolds)\n# gs_encv.fit(robust_scaled_df,target)  \n# print('optimal estimator: %s' % gs_encv.best_estimator_)\n# rmse3 = cv_rmse(gs_encv.best_estimator_, robust_scaled_df, target)\n# print(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(rmse3.mean(),rmse3.std()))","5e210d23":"# param_svr={'C':np.arange(20,30,1),\n#            'epsilon':[1e-5,1e-4,1e-3,5e-3,1e-2,5e-2,1e-1,5e-1],\n#             'gamma':[1e-6,1e-5,1e-4,1e-3,1e-2, 1e-1]}\n# gs_svr=GridSearchCV(SVR(),param_svr,scoring='neg_mean_squared_error',\n#                     cv=kfolds,verbose=3)\n# gs_svr.fit(robust_scaled_df,target)  \n# print('optimal estimator: %s' % gs_svr.best_estimator_)\n# rmse4 = cv_rmse(gs_svr.best_estimator_, robust_scaled_df, target)\n# print(\"svr score: {:.4f} ({:.4f})\\n\".format(rmse4.mean(),rmse4.std()))","b23b883e":"# # 'n_estimators':np.arange(100,2000,200),'l_rate':[1e-5,1e-4,1e-3,1e-2,1e-1],\n# # 'colsample_bytree':np.arange(0.1,1, 0.2)}\n# param_xgb={'n_estimators':[5000],'booster': [\"gbtree\"],\n#            'objective':['reg:squarederror'],           \n#            'learning_rate':[0.01],'max_depth':[7],\n#           'min_child_weight':[5],'subsample':[0.7],'reg_alpha':[0],\n#           'colsample_bytree':[0.7],'gamma':[0],\n#            'reg_lambda':[0.8]}\n# #            \n# gs_xgb=GridSearchCV(XGBRegressor(),param_grid=param_xgb,\n#                     scoring='neg_mean_squared_error',cv=3,verbose=3)\n# gs_xgb.fit(robust_scaled_df,target)  \n# print('optimal estimator: %s' % gs_xgb.best_estimator_)\n# rmse5 = cv_rmse(gs_xgb.best_estimator_, robust_scaled_df, target)\n# print(\"xgb score: {:.4f} ({:.4f})\\n\".format(rmse5.mean(),rmse5.std()))","0046b793":"# param_gbr = {\n#              'learning_rate': [0.1],'n_estimators': [400],'verbose':[0],\n#             'min_samples_split' :[41],'min_samples_leaf': [8],\n#             'subsample': [0.6],'max_depth':[4],\n#             'max_features':[9]  \n#              }\n# #  \n\n# gs_gbr=GridSearchCV(GradientBoostingRegressor(),param_grid=param_gbr,\n#                     scoring='neg_mean_squared_error',cv=5,verbose=3)\n# gs_gbr.fit(robust_scaled_df,target)  \n# print('optimal estimator: %s' % gs_gbr.best_estimator_)\n\n# rmse6 = cv_rmse(gs_gbr.best_estimator_, robust_scaled_df, target)\n# print(\"gbr score: {:.4f} ({:.4f})\\n\".format(rmse6.mean(),rmse6.std()))","e484615e":"ridge=Ridge(alpha=4.0)\nlasso=Lasso(alpha=0.0001, max_iter=12000)\nelasticnet=ElasticNet(alpha=0.001, l1_ratio=0.2, max_iter=500000.0)\nsvr=SVR(C=21, epsilon=0.01, gamma=0.001)\n\nxgboost=XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n        colsample_bynode=1,validate_parameters=1, verbosity=None,\n            colsample_bytree=0.7, gamma=0, gpu_id=-1,importance_type='gain',\n        learning_rate=0.01, max_delta_step=0, max_depth=7,min_child_weight=5,\nn_estimators=5000, n_jobs=0, num_parallel_tree=1, random_state=0,reg_alpha=0,\n    reg_lambda=0.8,scale_pos_weight=1, subsample=0.7,tree_method='exact')\n\ngbr=GradientBoostingRegressor(max_depth=4, max_features=9, min_samples_leaf=8,\n                        min_samples_split=41, n_estimators=400,subsample=0.6)\n","300aa786":"elastic_model = elasticnet.fit(robust_scaled_df,target)\n\nlasso_model = lasso.fit(robust_scaled_df,target)\n\nridge_model = ridge.fit(robust_scaled_df,target)\n\nsvr_model = svr.fit(robust_scaled_df,target)\n\nxgb_model = xgboost.fit(robust_scaled_df,target)\n\ngbr_model=gbr.fit(robust_scaled_df,target)\nprint(target)","8f33b917":"from mlxtend.regressor import StackingCVRegressor\nstack_gen=StackingCVRegressor(regressors=[ridge,lasso,elasticnet,\n                                          xgboost,svr,gbr],\n                 meta_regressor=xgboost,use_features_in_secondary=True)\n\nstack_model=stack_gen.fit(robust_scaled_df,target)\n\nscores = cross_val_score(stack_gen,robust_scaled_df,target, cv=10)\n\nprint(scores.mean(), scores.std())","021170a9":"average_output= (0.1*elastic_model.predict(robust_scaled_test)+\n                 0.02*lasso_model.predict(robust_scaled_test)+\n                    0.01*ridge_model.predict(robust_scaled_test)+\n                       0.25* svr_model.predict(robust_scaled_test)+\n                         0.6* stack_model.predict(robust_scaled_test)+\n                            0.01*xgb_model.predict(robust_scaled_test)+\n                            0.01*gbr_model.predict(robust_scaled_test))\nfinal_output= np.expm1(average_output)\nprint(final_output,average_output)","04e20709":"test_preds=np.around(final_output,1)\nprint(test_preds)\n\noutput = pd.DataFrame({'Id':Id.astype('Int32'),\n                      'SalePrice': test_preds})\noutput.to_csv('house_pred.csv', index=False)\n","6f6751ca":"> *the correlation with total porch area is highest with sales price compared to other porch variables, but not significant*","17bab9fc":"**heat map**","e1ba0ccd":"<a id='step2'><\/a>\n\n* Visualizing SalesPrice and heatmap of top correlations","6f234430":"> **pool quality has least effect and least information, hence it can be replaced by a column representing presence of pool or not**","e3d8a510":"    GroundLiving Area","c46f7e49":"<a id='step3'><\/a>\n\n3. Analyzing Features having Object data type","a729ddef":"*correlation between total area and sales price is very high and higher than ground living area, total basement separately*\n*looking at correlation after removing outliers from train data,by considering only the examples in which groung area less than 4500*","bd12bc62":"<a id='step4.1.a'><\/a>\n\n         4.1.a. Garage Year Built","fbb775ea":"<a id='step3.2'><\/a>\n\n    3.2.Handling missing Features and analysing ","d7fba67d":"<a id='step5.6.1'><\/a>\n\n    5.6.1. Observing correlation between all Areas(Since many area features have high importance)","ab8cd592":"<a id='step5.13'><\/a>\n\n\n    5.13. One hot encoding","2eba7bcb":"> the above missing values of features are handled by filling with most frequent characteristic","c93fbd29":"     Obtaining outliers from plot ","dc2916af":"<a id='step4.1.b'><\/a>\n\n    4.1.b. Garage Cars:\n    Setting car count zero for examples having no garage","7f9ef6f8":"Lasso model has convergence issues below 12000 iterations\n   >optimal estimator: Lasso(alpha=0.0001, max_iter=12000)\n       \n    Lasso score: 0.1179 (0.0082)\n","0e7b9ec7":"<a id='step5.7'><\/a>\n\n    5.7. Handling year features to obtain new feature and remove the columns year built, year remodelled","8a9c1e3c":"     OverallQuality","6b607b44":"<a id='step6.7'><\/a>\n\n    6.7. Saving models and training them.","477cce47":"> optimal estimator: Ridge(alpha=4)\n    \n    Ridge score: 0.1180 (0.0092)\n\n","34fa63da":"\n    optimal estimator: SVR(C=21, epsilon=0.01, gamma=0.001)\n    svr score: 0.1126 (0.0094)\n","7b5ae223":">  RMSE summary:\n    1. > xgb score:  0.1200 (0.0088)\n    2. > svr score:  0.1126 (0.0094)\n    3. > ElasticNet score:  0.1169 (0.0092)\n    4. > Lasso score:  0.1179 (0.0082)\n    5. > Ridge score: 0.1180 (0.0092)\n    6. >gbr score: 0.1244 (0.0096)","22500ad5":"> *avoiding overbinng, we can see that neighbourhood mean and median have same areas in each range, 3 very poor and 3 very rich neighbourhoods are seen clearly*","3f0afdb2":"> the correlation between TotalBsmtSF and (BsmtUnfSF+ BsmtFinSF2 +BsmtFinSF1) is exactly 1 , hence they can be dropped and replaced by whether the types of basement are present or not","ea742d2e":"> 3SSNPorch has least correlation with sales price and its area can be ignored, hence creating a new feature representing if it is present or not","1d117469":"<a id='step5.8'><\/a>\n\n    5.8. Neighbourhood has many categories and hece binning them\n","98cf9a84":"<a id='step6.1'><\/a>\n\n    6.1.  Ridge Regression","3ce8f927":" meta_regressor=xgboost:  \n \n     scores.mean()=0.91751898492917 ,scores.std=0.02215314970159242","5fdd1644":"<a id='step2'><\/a>\n\n2. **Cleanup of data and Handling missing values**","0014c2de":"<a id='step5.11'><\/a>\n\n    5.11. Handling Collinearity (Dropping highly correlated variables)\n\n","baa6d50a":"    optimal estimator: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=-1,importance_type='gain',learning_rate=0.01, max_delta_step=0, max_depth=7,min_child_weight=5,n_estimators=5000, n_jobs=0, num_parallel_tree=1, random_state=0,reg_alpha=0, reg_lambda=0.8, scale_pos_weight=1, subsample=0.7,tree_method='exact', validate_parameters=1, verbosity=None)\n        xgb score: 0.1200 (0.0088)\n\n","c7a95d6b":"<a id='step4.1.e'><\/a>\n\n    4.1.e. Bathroom features:\n    Filling missing bathroom values with 0","9cb99cef":"<a id='step5.6.2'><\/a>\n\n    5.6.2. Obserivng correlation between Quality variables(Since highest importance value is obtained for overall quality)","8d6f4174":"<a id='step3.1'><\/a>\n\n     3.1. Dropping Categorical Features with most missing values","2a09198a":"<a id='step6.5'><\/a>\n\n    6.5. XGBoost","f6c29374":"> Sales price is right skewed, log transformation is applied to attain less skewness for better model training and better accuracy of predicting at end of EDA","381b8d10":"correlation between 'GrLivArea' and ('1stFlrSF'+'2ndFlrSF'+'LowQualFinSF') is clearly 1, hence they can be dropped also ground living area greater than 4500 appears as outliers\n> a new feature can be made representing if 2nd floor is present or not","9c972e15":"    optimal estimator:GradientBoostingRegressor(max_depth=4, max_features=9,min_samples_leaf=8, min_samples_split=41,\n        n_estimators=400, subsample=0.6)\ngbr score: 0.1244 (0.0096)","2c20138f":"<a id='step5.6'><\/a> \n\n    5.6. Observing variable importances using Random Forest","e0152681":"<a id='step5.6.4'><\/a>\n\n    5.6.4. Coorelation between Garage variables","ddce3a35":"<a id='step6.8'><\/a>\n\n    6.8. Stacking Regressor\n        Stacking improves accuracy","9df6ea89":"<a id='step5.2'><\/a>\n\n    5.2. Looking for skewness, outliers and distribution type of each categorical feature","ca963d8d":"> 1. <a href='#1. Loading and Exploring Data'>Loading and Exploring Data<\/a>\n     *  <a href='#step1'>Loading Libraries and gathering data sets, target(SalesPrice) and index of each target<\/a>  \n     *  <a href='#step2'>Visualizing SalesPrice and heatmap of top correlations<\/a> \n     *  <a href='#step3'>Analysing highly correlated feature with sales price<\/a>    \n>2. <a href='#step2'>Cleanup of data and Handling missing values<\/a>\n     *   <a href='#step2.1'>Viewing information about all features <\/a>     \n     *   <a href='#step2.2'>Assigning cleanup numbers for each characteristic of each feature<\/a>\n >3. <a href='#step3'>Analyzing Features having Object data type<\/a>\n     * <a href='#step3.1'>Dropping Categorical Features with most missing values<\/a>\n     * <a href='#step3.2'>Handling missing Features<\/a>              \n >4. <a href='#step4'>Analyzing non-Categorical Features<\/a>\n     * <a href='#step4.1'>Handling missing values in Features <\/a>\n         * <a href='#step4.1.a'>GarageYearBuilt <\/a>\n         * <a href='#step4.1.b'>Garage Cars<\/a>\n         * <a href='#step4.1.c'>Basement related features <\/a>\n         * <a href='#step4.1.d'>Veneer <\/a>\n         * <a href='#step4.1.e'>Bathrooms <\/a>\n         * <a href='#step4.1.e'>Viewing information of Features after filling missing values logically<\/a> \n> 5. <a href='#step5'>Remaining Feature Engineering and EDA\n    * <a href='#step5.1'>Impute Remaining missing values<\/a>\n    * <a href='#step5.2'> Looking for skewness, outliers and distribution type of each categorical feature<\/a>\n    * <a href='#step5.3'> Converting some non object data types to object, since its numerical value is not to significant<\/a>\n    * <a href='#step5.4'>One Hot Encoding<\/a>\n    * <a href='#step5.5'>Heatmap of correlations<\/a>\n    * <a href='#step5.6'>Observing variable importances using Random Forest<\/a>\n        * <a href='#step5.6.1'>correlation between all Areas<\/a>\n        * <a href='#step5.6.2'>correlation between Quality variables<\/a>\n        * <a href='#step5.6.3'>correlation between Basement variables<\/a>\n        * <a href='#step5.6.4'>correlation between Garage variables<\/a>\n        * <a href='#step5.6.5'>correlation between Bathroom variables<\/a>\n    * <a href='#step5.7'>Handling features related to years<\/a>\n    * <a href='#step5.8'>Handling feature Neighbourhood<\/a>\n    * <a href='#step5.9'>Handling features related to Porch Area<\/a>\n    * <a href='#step5.10'>Creating feature Total Area<\/a>\n    * <a href='#step5.11'>Handling Collinearity<\/a>\n    * <a href='#step5.12'>Handling Skewness<\/a>\n    * <a href='#step5.13'>One Hot Encoding<\/a>\n6. <a href='#step6'>Creating Models with Grid Search<\/a>\n    * <a href='#step6.1'>Ridge Regression<\/a>\n    * <a href='#step6.2'>Lasso Regression<\/a>\n    * <a href='#step6.3'>Elastic Net<\/a>\n    * <a href='#step6.4'>SVR<\/a>\n    * <a href='#step6.5'>XGBoost<\/a>\n    * <a href='#step6.6'>Gradient Boost Regressor<\/a>\n    * <a href='#step6.7'>Saving Models and training them<\/a>\n    * <a href='#step6.8'>Stacking all models<\/a>\n    \n7. <a href='#step7'>Ensembling and Obtaining result<\/a>\n8. <a href='#step8'>References<\/a>","e4b0e24c":"<a id='step5.4'><\/a>    \n    \n    5.4. One Hot Encoding\n> Since the test data has few categorical labels that are missing in train data,both data frames are concatinated and labels are found ","65360a70":"<a id='step5.3'><\/a>\n\n    5.3. Converting some non object data types to object, since its numerical value is not to significant","50a9e50a":"<a id='step4.1.f'><\/a>\n\n        4.1.f. Viewing information of Features after filling missing values logically","4ab46cc7":"<a id='step5.6.5'><\/a>\n\n    5.6.5. Dealing with all Bathroom features\n","c7ed6e62":"Iterations below 1e5 have convergence issues and greater than 1e9 has overflow error.\n  >optimal estimator: ElasticNet(alpha=0.001, l1_ratio=0.2, max_iter=500000.0)\n             \n    ElasticNet score: 0.1169 (0.0092)\n\n","e3c7beed":"<a id='step8'><\/a>\n\n\n**8. References**\n\n* https:\/\/www.kaggle.com\/erikbruin\/house-prices-lasso-xgboost-and-a-detailed-eda\n* https:\/\/www.kaggle.com\/itslek\/blend-stack-lr-gb-0-10649-house-prices-v57\n","87b5c1f3":"<a id='step3'><\/a>\n\n* **Analysing highly correlated feature with sales price**","976652ad":"> majority of Ground living area is less than 4500 and rest are considered as outliers, since GrLivArea has highest priority, accuracy can be improved by eliminating those outliers","18dbba8a":"<a id='step5.5'><\/a> \n\n    5.5. Heatmap\n> Visualizing correlation between columns after OHE","508a828b":"<a id='step5.10'><\/a>\n\n    5.10. Creating new feature Total Area:\n>After removing highly correlated variables to GrLivArea, we see that a new variable can be made out of summing GrLivArea and TotalBsmtSF as total area since they both are highly correlated with Sale Price**","605a35c3":"<a id='step4.1.c'><\/a>\n\n    4.1.c. Basement related Features:\n    Handling missing values in basement features for houses with no basement and veneer","2201b2ae":"<a id='step5.6.3'><\/a>\n\n    5.6.3. Correlation between Basement variables","09051adb":"<a id='step4.1.d'><\/a>\n\n       4.1.d. Veneer:\n       Setting Area of veneer as 0 for examples with no veneer","7820bd83":"<a id='step5.12'><\/a>\n\n    5.12. Handling skewness in each feature","bcf73e00":"1. > garage condition and garage quality are highly correlated, garage quality is more correlated with sales price. Hence garage condition is to be dropped\n2. garage area is highlyt correlated with garage carsand garage cars is more correlated with sales price. Hence garage areais to be dropped","b1f5a671":"<a id='step7'><\/a>\n\n**7. Ensembling and obtaining output**","0ae709c8":"<a id='step6'><\/a>\n\n6. Creating Models and grid search for each model for finding best hyper parameters","2abe7276":"<a id='step4'><\/a>\n\n4.Analyzing non-Categorical Features<\/a>\n\n    4.1.Handling missing values in Features","a93e7b13":"<a id='step6.2'><\/a>\n   \n    6.2. Lasso","dded7944":"<a id='step5'><\/a>\n**Remaining Feature Engineering and EDA**\n<a id='step5.1'><\/a>\n\n    5.1. Impute remaining missing values","3bf1d11c":"<a id='step5.9'><\/a>\n\n    5.9. Handling Porch Area features\n> creating a new porch feature consisting of total porch area","0205d4e3":"> There is a majority of each category in each feature and outliers are present for most of the categories for each feature. Neighbourhood feature has the highes number of categories","1e994394":"<a id='1. Loading and Exploring Data'><\/a>\n1. Loading and Exploring Data\n    <a id='step1'><\/a>\n    *     Loading Libraries and gathering data sets, target(SalesPrice) and index of each target ","90e4b7b0":"<a id='step2.1'><\/a>\n>      2.1. Viewing information about all features","4b52ee10":"<a id='step6.4'><\/a>\n\n    6.4. SVR","a19996f4":"<a id='step6.6'><\/a>\n\n    6.6. GradientBoostingRegressor","1238862e":"1. > GrLiv area has high correlation with sales price compared to TotalBsmtSF, since Total area feature is present, TotalBsmtSF can be dropped.\n2. > 'TotRmsAbvGrd' and GrLiv area have high correlation with each other but GrLiv area has higher coorelation  with sales price\n3. > garage year built has high correlation with year built(see 1st heat map)and year built has high correlation with sale price\n","f156ae01":"<a id='step6.3'><\/a>\n\n    6.3. ElasticNet","4d7bd488":"<a id='step2.2'><\/a>\n>     2.2 Assigning cleanup numbers for each characteristic of each feature"}}