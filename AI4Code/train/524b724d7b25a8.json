{"cell_type":{"0a381e89":"code","e5b6e7a4":"code","717e5f27":"code","b222edc6":"code","a62f24c5":"code","0460dea9":"code","eb078598":"code","8183c543":"code","9804417f":"code","5433eb86":"code","5c48e6bb":"code","5ee2cc58":"code","9aa97d4d":"code","0ba88579":"code","b449c90c":"code","591d762d":"code","94d2e079":"code","3def8813":"code","51fcd26d":"code","94133323":"code","a537f318":"code","75b941f7":"code","af4d3103":"code","e1c68d10":"code","8b7422b0":"code","47ceac56":"code","c4fd9556":"code","9e445871":"code","b44617cd":"code","1e6244dd":"code","7f4809a2":"code","a850a08a":"code","40e0ca00":"code","9d148a0a":"code","af7e9fb2":"code","e9128450":"code","4636cef1":"code","65d60edc":"code","8753d198":"code","786faf70":"code","43f5b186":"code","c2d4af34":"code","4d26abf5":"code","969459a7":"code","5c19cd4b":"code","de31c4d3":"code","8269dc30":"code","32753027":"code","4a15f43a":"code","34f21858":"code","5248ddda":"code","f6f4d396":"code","d738b368":"code","e1b790a5":"code","4f85f437":"code","e1939364":"code","51c9cbab":"markdown","3ad96680":"markdown","653ad0d5":"markdown","9211832a":"markdown","2ae68744":"markdown","ed842f3d":"markdown","5eba9283":"markdown","bfd6a599":"markdown","661a29ae":"markdown","518951dd":"markdown","0151bd74":"markdown","8ebc4b01":"markdown","1523e6ba":"markdown","a24fff36":"markdown","e8a64e79":"markdown","4fcace21":"markdown","69ef6dd2":"markdown","f358527b":"markdown","b2a85225":"markdown","14b56898":"markdown","0aa4c2d8":"markdown","f182daaa":"markdown","9a6f032d":"markdown"},"source":{"0a381e89":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport torch\nfrom torchvision import datasets, transforms, models\nfrom torch.autograd import Variable\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","e5b6e7a4":"#!wget https:\/\/bin.equinox.io\/c\/4VmDzA7iaHb\/ngrok-stable-linux-amd64.zip\n#!unzip ngrok-stable-linux-amd64.zip","717e5f27":"import os\nLOG_DIR = 'runs'\n#os.makedirs(LOG_DIR, exist_ok=True)\n#get_ipython().system_raw(\n#    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n#    .format(LOG_DIR)\n#)","b222edc6":"#get_ipython().system_raw('.\/ngrok http 6006 &')","a62f24c5":"#! curl -s http:\/\/localhost:4040\/api\/tunnels | python3 -c \\\n#    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"","0460dea9":"from PIL import Image\ndata_dir = '..\/input\/training-a'\nname = os.listdir(data_dir)[10]\nImage.open(data_dir+\"\/\"+name)","eb078598":"data_dir = '..\/input\/training-c'\nprint(os.listdir(data_dir)[:5])\nlen(os.listdir(data_dir))\n#10908+19702+24298","8183c543":"print(\"Final Size:\", str(10908+19702+24298))","9804417f":"a_csv = pd.read_csv('..\/input\/training-a.csv')\na_csv.columns","5433eb86":"a_csv = a_csv.drop(columns=['original filename', 'scanid',\n       'database name original', 'contributing team', 'database name'])\na_csv.iloc[:10, 0:]","5c48e6bb":"c_csv = pd.read_csv('..\/input\/training-c.csv')\nc_csv.columns","5ee2cc58":"c_csv = c_csv.drop(columns=['original filename', 'scanid',\n       'database name original', 'contributing team', 'database name'])\nc_csv.iloc[:10, 0:]","9aa97d4d":"d_csv = pd.read_csv('..\/input\/training-d.csv')\nd_csv.columns","0ba88579":"d_csv = d_csv.drop(columns=['original filename', 'scanid', 'num', 'database name original',\n       'database name'])\nd_csv.iloc[:10, 0:]","b449c90c":"frames = [a_csv, c_csv, d_csv]\nlabel_csv = pd.concat(frames)","591d762d":"# almost minist\nlen(label_csv)","94d2e079":"path = 'train'\nos.mkdir(path)","3def8813":"import os\nimport shutil\nsrc = '..\/input\/training-a\/'\nsrc_files = os.listdir(src)\nfor file_name in src_files:\n    full_file_name = os.path.join(src, file_name)\n    if os.path.isfile(full_file_name):\n        shutil.copy(full_file_name, path)\n\nprint(\"A Done\")","51fcd26d":"src = '..\/input\/training-c\/'\nsrc_files = os.listdir(src)\nfor file_name in src_files:\n    full_file_name = os.path.join(src, file_name)\n    if os.path.isfile(full_file_name):\n        shutil.copy(full_file_name, path)\n\nprint(\"C Done\")","94133323":"src = '..\/input\/training-d\/'\nsrc_files = os.listdir(src)\nfor file_name in src_files:\n    full_file_name = os.path.join(src, file_name)\n    if os.path.isfile(full_file_name):\n        shutil.copy(full_file_name, path)\n\nprint(\"D Done\")","a537f318":"print(len(os.listdir(path)))","75b941f7":"t = label_csv.iloc[1000]\nprint(t)\nprint(\"Label: \", t[0])\nImage.open(path+\"\/\"+t[1])","af4d3103":"import torch\nfrom torch.utils.data import Dataset\n\nclass Dataset(Dataset):\n    def __init__(self, df, root, transform=None):\n        self.data = df\n        self.root = root\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        item = self.data.iloc[index]\n        \n        path = self.root + \"\/\" + item[1]\n        image = Image.open(path).convert('L')\n        label = item[0]\n        \n        if self.transform is not None:\n            image = self.transform(image)\n            \n        return image, label","e1c68d10":"mean = [0.5,]\nstd = [0.5, ]\n\ntrain_transform = transforms.Compose([\n    transforms.Resize(180),\n    transforms.RandomRotation(30),\n    #transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    #transforms.Normalize(mean, std)\n])\n\ntest_transform = transforms.Compose([\n        transforms.Resize(180),\n        transforms.ToTensor(),\n        #transforms.Normalize(mean, std)\n])\n\ntrain_data  = Dataset(label_csv, path, train_transform)\ntest_data = Dataset(label_csv, path, test_transform)\n\nprint(\"Trainig Samples: \",len(train_data))","8b7422b0":"from torch.utils.data.sampler import SubsetRandomSampler\n\n#batch size\nbatch_size=128\n\n# split data 20% for testing\ntest_size = 0.2\n# obtain training indices that will be used for validation\nnum_train = len(train_data)\n\n# mix data\n# index of num of train\nindices = list(range(num_train))\n# random the index\nnp.random.shuffle(indices)\nsplit = int(np.floor(test_size * num_train))\n# divied into two part\ntrain_idx, test_idx = indices[split:], indices[:split]\n\n# define the sampler\ntrain_sampler = SubsetRandomSampler(train_idx)\ntest_sampler = SubsetRandomSampler(test_idx)\n\n# prepare loaders\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=batch_size,\n    sampler=train_sampler)\n\ntest_loader = torch.utils.data.DataLoader(\n    test_data, batch_size=batch_size,\n    sampler=test_sampler)\n\nprint(\"Train dataloader:{}\".format(len(train_loader)))\nprint(\"Test dataloader:{}\".format(len(test_loader)))","47ceac56":"classes = list()\nfor i in range(10):\n    classes.append(str(i))\nclasses","c4fd9556":"!wget https:\/\/raw.githubusercontent.com\/Iamsdt\/DLProjects\/master\/utils\/Helper.py\nimport Helper","9e445871":"import Helper\nfrom torch.utils.tensorboard import SummaryWriter\n\ndata_iter = iter(train_loader)\nimages, labels = data_iter.next()\n\n# Write images\ntb = SummaryWriter()\ntb.add_images(\"Train Images\", images)\ntb.close()\n\nfig = plt.figure(figsize=(25, 10))\nfor idx in range(5):\n        ax = fig.add_subplot(1, 10, idx + 1, xticks=[], yticks=[])\n        ax.imshow(np.squeeze(images[idx]), cmap='gray')\n        ax.set_title(classes[labels[idx]])\n","b44617cd":"#model = models.densenet161(pretrained=True)\n#model.classifier","1e6244dd":"#model = Helper.freeze_parameters(model)","7f4809a2":"import torch.nn as nn\nfrom collections import OrderedDict\n\nclassifier = nn.Sequential(\n  nn.Linear(in_features=2208, out_features=1024),\n  nn.ReLU(),\n  nn.Dropout(p=0.3),\n  nn.Linear(in_features=1024, out_features=10),\n  nn.LogSoftmax(dim=1)  \n)\n    \n#model.classifier = classifier\n#model.classifier","a850a08a":"import torch.nn as nn\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        \n        self.conv1 = nn.Sequential(\n            nn.Conv2d(1, 32, 3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(32),\n            nn.Conv2d(32, 32, 3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(32),\n            nn.MaxPool2d(2, 2),\n            nn.Dropout(0.25)\n        )\n        \n        self.conv2 = nn.Sequential(\n            nn.Conv2d(32, 64, 3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(64),\n            nn.Conv2d(64, 64, 3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(64),\n            nn.MaxPool2d(2, 2),\n            nn.Dropout(0.25)\n        )\n        \n        self.conv3 = nn.Sequential(\n            nn.Conv2d(64, 128, 3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(128),\n            nn.Conv2d(128, 128, 3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(128),\n            nn.MaxPool2d(2, 2),\n            nn.Dropout(0.25)\n        )\n        \n        self.conv4 = nn.Sequential(\n            nn.Conv2d(128, 256, 3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(256),\n            nn.MaxPool2d(2, 2),\n            nn.Dropout(0.25)\n        )\n        \n        self.fc = nn.Sequential(\n            nn.Linear(256, 128),\n            nn.Dropout(0.4),\n            nn.ReLU(),\n            nn.Linear(128, 10),\n            nn.Softmax(dim=1)\n        )\n                \n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n        # flaten tensor\n        x = x.view(x.size(0), -1)\n        return self.fc(x)","40e0ca00":"from torch import optim\nmodel = Net()\n\n# Gpu\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n#move tensor to default device\nmodel.to(device)\n\ncriterion = nn.NLLLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)","9d148a0a":"optimizer","af7e9fb2":"global_epoch = 0","e9128450":"epoch = 25","4636cef1":"import time\nfrom torch.utils.tensorboard import SummaryWriter\n\ndef train(model, train_loader, test_loader,\n          epochs, optimizer, criterion, scheduler=None, global_epoch = 0,\n          name=\"model.pt\", path=None):\n  \n    global_epoch = global_epoch + 1\n\n    # Write images\n    tb = SummaryWriter()\n\n    # compare overfitted\n    train_loss_data, valid_loss_data = [], []\n    # check for validation loss\n    valid_loss_min = np.Inf\n    # calculate time\n    since = time.time()\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    for epoch in range(epochs):\n        print(\"Epoch: {}\/{}\".format(epoch + 1, epochs))\n        # monitor training loss\n        train_loss = 0.0\n        valid_loss = 0.0\n        total = 0\n        correct = 0\n        e_since = time.time()\n\n        ###################\n        # train the model #\n        ###################\n        model.train()  # prep model for training\n        if scheduler is not None:\n            scheduler.step()  # step up scheduler\n\n        for images, labels in train_loader:\n            # Move input and label tensors to the default device\n            images, labels = images.to(device), labels.to(device)\n            # clear the gradients of all optimized variables\n            optimizer.zero_grad()\n            # forward pass: compute predicted outputs by passing inputs to the model\n            log_ps = model(images)\n            # calculate the loss\n            loss = criterion(log_ps, labels)\n            # backward pass: compute gradient of the loss with respect to model parameters\n            loss.backward()\n            # perform a single optimization step (parameter update)\n            optimizer.step()\n            # update running training loss\n            train_loss += loss.item() * images.size(0)\n            # Write on tensorbroad\n            tb.add_scalar(\"Loss\", loss.item(), global_epoch)\n\n        ######################\n        # validate the model #\n        ######################\n        print(\"\\t\\tGoing for validation\")\n        model.eval()  # prep model for evaluation\n        for data, target in test_loader:\n            # Move input and label tensors to the default device\n            data, target = data.to(device), target.to(device)\n            # forward pass: compute predicted outputs by passing inputs to the model\n            output = model(data)\n            # calculate the loss\n            loss_p = criterion(output, target)\n            # update running validation loss\n            valid_loss += loss_p.item() * data.size(0)\n            # calculate accuracy\n            proba = torch.exp(output)\n            top_p, top_class = proba.topk(1, dim=1)\n            equals = top_class == target.view(*top_class.shape)\n            # accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n\n            _, predicted = torch.max(output.data, 1)\n            total += target.size(0)\n            correct += (predicted == target).sum().item()\n            # Write on tensorbroad\n            tb.add_scalar(\"Loss\", loss.item(), global_epoch)\n\n        # print training\/validation statistics\n        # calculate average loss over an epoch\n        train_loss = train_loss \/ len(train_loader.dataset)\n        valid_loss = valid_loss \/ len(test_loader.dataset)\n\n        # calculate train loss and running loss\n        train_loss_data.append(train_loss * 100)\n        valid_loss_data.append(valid_loss * 100)\n\n        accuracy = (correct \/ total) * 100\n\n        print(\"\\tTrain loss:{:.6f}..\".format(train_loss),\n              \"\\tValid Loss:{:.6f}..\".format(valid_loss),\n              \"\\tAccuracy: {:.4f}\".format(accuracy))\n        \n        tb.add_scalar(\"Global Loss\", train_loss, global_epoch)\n        tb.add_scalar(\"Global Loss\", valid_loss, global_epoch)\n        tb.add_scalar(\"Global Accuracy Loss\", accuracy, global_epoch)\n        tb.close()\n\n        # Update global epoch\n        global_epoch += 1\n\n        # save model if validation loss has decreased\n        if valid_loss <= valid_loss_min:\n            print('\\tValidation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n                valid_loss_min,\n                valid_loss))\n            torch.save(model.state_dict(), name)\n            valid_loss_min = valid_loss\n            # save to google drive\n            if path is not None:\n                torch.save(model.state_dict(), path)\n\n        # Time take for one epoch\n        time_elapsed = time.time() - e_since\n        print('\\tEpoch:{} completed in {:.0f}m {:.0f}s'.format(\n            epoch + 1, time_elapsed \/\/ 60, time_elapsed % 60))\n\n    # compare total time\n    time_elapsed = time.time() - since\n    print('Training completed in {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 60, time_elapsed % 60))\n\n    # load best model\n    #model = load_latest_model(model, name)\n    \n    # close writer\n    #tb.close()\n\n    # return the model\n    return [model, train_loss_data, valid_loss_data, global_epoch]","65d60edc":"model, train_loss, test_loss, global_epoch = train(\n    model, train_loader, test_loader, epoch, optimizer, criterion, scheduler, global_epoch=global_epoch)","8753d198":"Helper.check_overfitted(train_loss, test_loss)","786faf70":"Helper.test(model, test_loader)","43f5b186":"Helper.test_per_class(model, test_loader, criterion, classes)","c2d4af34":"from PIL import Image\n\ndef test(file):\n    file = Image.open(file).convert('L')\n    img = test_transform(file).unsqueeze(0)\n    with torch.no_grad():\n        out = model(img.to(device))\n        proba = torch.exp(out)\n        top_p, top_class = proba.topk(1, dim=1)\n        print(f\"Predicted Label: {top_class.item()}\")\n        plt.imshow(np.array(file))\n        plt.show()","4d26abf5":"from PIL import Image\nfrom matplotlib import pyplot as plt\ndata_dir = '..\/input\/testing-d'\nname = os.listdir(data_dir)[4]\nfile = data_dir+\"\/\"+name\nprint(file)\n\ntest(file)","969459a7":"data_dir = '..\/input\/testing-c'\nname = os.listdir(data_dir)[4]\nfile = data_dir+\"\/\"+name\nprint(file)\n\ntest(file)","5c19cd4b":"data_dir = '..\/input\/testing-a'\nname = os.listdir(data_dir)[4]\nfile = data_dir+\"\/\"+name\nprint(file)\n\ntest(file)","de31c4d3":"data_dir = '..\/input\/testing-b'\nname = os.listdir(data_dir)[4]\nfile = data_dir+\"\/\"+name\nprint(file)\n\ntest(file)","8269dc30":"data_dir = '..\/input\/testing-b'\nname = os.listdir(data_dir)[15]\nfile = data_dir+\"\/\"+name\nprint(file)\n\ntest(file)","32753027":"!wget https:\/\/i.imgur.com\/jJz1GUB.png","4a15f43a":"test('jJz1GUB.png')","34f21858":"!wget https:\/\/i.imgur.com\/Pd5P7C3.png","5248ddda":"test('Pd5P7C3.png')","f6f4d396":"import os\nimport shutil\n\ndef remove_contents(path):\n    for c in os.listdir(path):\n        full_path = os.path.join(path, c)\n        if os.path.isfile(full_path):\n            os.remove(full_path)\n        else:\n            shutil.rmtree(full_path)","d738b368":"remove_contents(path)","e1b790a5":"os.listdir(path)","4f85f437":"def save_check_point(model, epoch, classes, optimizer, scheduler=None,\n                     path=None, name='model.pt'):\n    try:\n        classifier = model.classifier\n    except AttributeError:\n        classifier = model.fc\n        \n    \n    print(classifier)\n\n    checkpoint = {\n        'class_to_name': classes,\n        'epochs': epoch,\n        'classifier': classifier,\n        'state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict()\n    }\n\n    if scheduler is not None:\n        checkpoint['scheduler_state_dict'] = scheduler.state_dict()\n\n    if path is None:\n        d = name\n    else:\n        d = path + \"\/\" + name\n\n    torch.save(checkpoint, d)\n    print(f\"Model saved at {d}\")","e1939364":"save_check_point(model, epoch, classes, optimizer, scheduler=scheduler,\n                     path=None, name='saved_model.pt')","51c9cbab":"# Now combine Image","3ad96680":"# Show a single Image","653ad0d5":"### Prepare loader\n\nBatch Size: 128\n\nSplit percentage: 20%","9211832a":"# A","2ae68744":"# Visualize Data","ed842f3d":"# Prepare Tensorboard","5eba9283":"# Check again","bfd6a599":"### Contain RGB and Greyscale\nconvert to greyscale","661a29ae":"# Testing","518951dd":"# Define loss and optimizer","0151bd74":"# Create model","8ebc4b01":"# Data loader\nprepare datasets first","1523e6ba":"# Training","a24fff36":"# Saved Model","e8a64e79":"# Check","4fcace21":"#### Check for overfitting","69ef6dd2":"# Now combine","f358527b":"# Delete all files","b2a85225":"# D","14b56898":"prepare data","0aa4c2d8":"# Lets combine A C D\ncsv first","f182daaa":"# Test some single Image","9a6f032d":"# C"}}