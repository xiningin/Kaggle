{"cell_type":{"b5e4f5d7":"code","e56babc7":"code","4aeb0b6e":"code","ab76be3d":"code","b3ec3055":"code","1a67a46e":"code","343804aa":"code","1f325e2d":"code","880f0d07":"code","422eb92c":"code","caad8cae":"code","23e482e6":"code","f4596a65":"code","691e1d34":"code","22007257":"code","d1b701f5":"code","2ebaf592":"code","e089eff8":"code","758ffb0b":"code","10effd02":"code","92a8e3ca":"code","4396c948":"code","1869061e":"code","b353c007":"code","4c2ed357":"code","57aaf21e":"markdown","d408a275":"markdown","c549b431":"markdown","704108b9":"markdown","fba152e0":"markdown","b5a8c48f":"markdown","fb1698b6":"markdown","b5cb115c":"markdown","9c98fb69":"markdown"},"source":{"b5e4f5d7":"# Import packages\nimport numpy as np # Handling matrices\nimport pandas as pd # Data processing\nimport matplotlib.pyplot as plt # Plotting\nimport seaborn as sns # Plotting \nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler, LabelEncoder # Handling categorical data and normalization\nfrom sklearn.model_selection import train_test_split # Split data in train and test\nfrom sklearn.metrics import roc_auc_score,precision_score,confusion_matrix, accuracy_score, roc_curve, f1_score # Several useful metrics\nimport lightgbm as lgb # LightGBM\n\n# Set matplotlib configuration\n%matplotlib inline\nplt.style.use('seaborn')","e56babc7":"# Import data\ndata = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\nprint(\"This dataset contains: {} rows and {} columns\".format(data.shape[0],data.shape[1]))\ndata.head()","4aeb0b6e":"# Review the type of each feature\ndata.dtypes","ab76be3d":"# Our target feature \"stroke\" is numeric, however, we need to change it to object. Also, we need to change the features \"heart_disease\" and \"hypertension\"\ndata[\"stroke\"] = data[\"stroke\"].astype(\"object\")\ndata[\"heart_disease\"] = data[\"heart_disease\"].astype(\"object\")\ndata[\"hypertension\"] = data[\"hypertension\"].astype(\"object\")","b3ec3055":"# Remove id\ndata.drop(\"id\",axis = 1,inplace = True)\n\n# Analyse missing values\ndata.isna().sum()\n\n# We can see tha only bmi (body mass index) is missing.This is a numerical feature and can be difficult to imputate.","1a67a46e":"# We can note the distribution of bmi values\ndata[\"bmi\"].describe()","343804aa":"# The number of missing values in bmi are low, thereby we can remove them. First, we go to review the relation between bmi and stroke\n\n# Create the boxplot \nplt.figure(figsize=(20,6))\nsns.boxplot(x='stroke', y=\"bmi\", data=data);\nplt.title(\"Relation between BMI and stroke\", fontsize=24)\nplt.xlabel('Stroke')\nplt.ylabel('BMI');\n\n# We observe that bmi is a feature that is not so relevant. In this case, patients wihtout stroke tend to have a less bmi, however, some cases with a high bmi are detected\n\n# Remove missing values\ndata.dropna(subset = [\"bmi\"], inplace=True)","1f325e2d":"# Identify categorical features\ncat = (data.dtypes == 'object')\ncat_cols = list(cat[cat].index)\nprint(cat_cols)\n\n# Create a handful of plots\nfor cols in cat_cols:\n    plt.figure(figsize=(8,4));\n    sns.countplot(x = data[cols]);","880f0d07":"# We can see closer some particular features\ndata[\"gender\"].value_counts()\n\n# The value \"Other\" is valid, however we have only a one case. Additionally, in the feature \"smoking_status\", we have the class \"Unknown\". \n# We can mantain this class to avoid create missing values.","422eb92c":"# An important point here, is that we have an unbalanced dataset, \n# because we count with a little number of patients with stroke.","caad8cae":"# Create a list of numerical_cols\nnumerical_cols = [cname for cname in data.columns if data[cname].dtype in ['int64', 'float64']]\n\n# Also, we can see how numerical features are related with the target\ndata[numerical_cols].hist(bins=15, figsize=(15, 6), layout=(2, 3));","23e482e6":"# We can create a model using these features.\n\n# Separate target from predictors \ny = data[[\"stroke\"]].copy()\n\n# LightGBM need numerical data\ny[\"stroke\"] = y[\"stroke\"].astype(\"int64\")\n\nX = data.drop(['stroke'], axis=1).copy()\n\n# Divide data into training and validation subsets\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, \n                                                                test_size=0.2,random_state = 0)\n","f4596a65":"# Preprocessing data\n\n# We go to use Label Encoding, One-Hot encoding and MinMaxScaler to different features\n\n# Apply Label encoding to ordinal feature. Make copy to avoid changing original data \nord_cat = [\"heart_disease\",\"hypertension\"]\nlabel_train = X_train[ord_cat].copy()\nlabel_valid = X_valid[ord_cat].copy()\nlabel_encoder = LabelEncoder()\nfor col in ord_cat:\n    label_train[col] = label_encoder.fit_transform(X_train[col])\n    label_valid[col] = label_encoder.transform(X_valid[col])\n\n# Apply one-hot encoder to each column with categorical data\nnom_cat = ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']\ndummy_train = pd.get_dummies(X_train[nom_cat], prefix=nom_cat)\ndummy_valid = pd.get_dummies(X_valid[nom_cat], prefix=nom_cat)\n\n# Apply normalization to numerical data. We use this normalization because our data does not have a normal distribution\nscaler = MinMaxScaler()\ns_cols_train = pd.DataFrame(scaler.fit_transform(X_train[numerical_cols]))\ns_cols_valid = pd.DataFrame(scaler.transform(X_valid[numerical_cols]))\n\n# Rename columns\ns_cols_train.columns = numerical_cols\ns_cols_valid.columns= numerical_cols\n\n# Encoding removed index; put it back\ns_cols_train.index = X_train.index\ns_cols_valid.index = X_valid.index\n\n# Add one-hot encoded columns to numerical features\nX_train_modified = pd.concat([s_cols_train, dummy_train, label_train], axis=1)\nX_valid_modified = pd.concat([s_cols_valid, dummy_valid, label_valid], axis=1)","691e1d34":"# Create the model \nd_train=lgb.Dataset(X_train_modified, label=y_train) \nparams={} #Specifying the parameter\nparams['learning_rate']=0.015 # Learning rate \nparams['boosting_type']='gbdt' # GradientBoostingDecisionTree\nparams['objective']='binary' # Binary target feature\nparams['metric']='auc',# Metric for binary classification\nparams['max_depth']=200, # Set depth\nparams['bagging_fraction'] = 0.8,\nparams['force_row_wise'] = True, # Need to the model\nparams['unbalance'] =True, # To consider an unbalanced dataset\nclf=lgb.train(params,d_train,200) # Train the model on 100 epocs","22007257":"# Prediction on the test set\ny_pred=clf.predict(X_valid_modified, predict_disable_shape_check=True)\ny_pred.shape","d1b701f5":"# Function to plot ROC curve\ndef plot_roc_curve(fpr, tpr):\n    plt.plot(fpr, tpr, color='orange', label='ROC')\n    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend()\n    plt.show()\n\nfpr, tpr, thresholds = roc_curve(y_valid, y_pred)\nplot_roc_curve(fpr, tpr)","2ebaf592":"# Create Confusion Matrix\npred_class = y_pred > 0.5\npred_class = pred_class.astype(int)\ncm = confusion_matrix(y_valid, pred_class)\nprint(cm)\n\n# Get accuracy\naccuracy = round(accuracy_score(y_valid,pred_class),4)\nprint(\"Accuracy: {}\".format(accuracy),\"\\n\")\n\n# Get f1 score (it is required on the Task 1 of this dataset)\nf1 = f1_score(y_valid,pred_class)\nprint(\"F1: {}\".format(f1),\"\\n\")","e089eff8":"# See the feature importance\nimportance_feature = pd.DataFrame({'Value':clf.feature_importance(),'Feature':clf.feature_name()}).sort_values(by=\"Value\", ascending=False)\n\n# Create a plot\nplt.figure(figsize=(20, 10))\nsns.barplot(x = 'Value',y = 'Feature',data = importance_feature);\nplt.title(\"Importance feature\");","758ffb0b":"# LightGBM also can handle categorical data directly We go to probe its inner method\n\n# Transform categorical features into the appropriate type that is expected by LightGBM\nfor c in X_train.columns:\n    col_type = X_train[c].dtype\n    if col_type == 'object' or col_type.name == 'category':\n        X_train[c] = X_train[c].astype('category')\n        \nfor c in X_valid.columns:\n    col_type = X_valid[c].dtype\n    if col_type == 'object' or col_type.name == 'category':\n        X_valid[c] = X_valid[c].astype('category')","10effd02":"# Create the model \nd_train=lgb.Dataset(X_train, label=y_train)#Specifying the parameter\nparams={}\nparams['learning_rate']=0.015 # Learning rate \nparams['boosting_type']='gbdt' # GradientBoostingDecisionTree\nparams['objective']='binary' # Binary target feature\nparams['metric']='auc' # Metric for binary classification\nparams['max_depth']=200, # Set depth\nparams['bagging_fraction'] = 0.8,\nparams['force_row_wise'] = True, # Need to the model\nparams['unbalance'] = True, # To consider an unbalanced dataset\nclf=lgb.train(params,d_train,200) # Train the model on 100 epocs\n\n# Prediction on the test set\ny_pred2=clf.predict(X_valid, predict_disable_shape_check=True)","92a8e3ca":"# We can review the params of our model. We can see if the categorical columns are correclty considered.\nclf.params","4396c948":"# Function to plot ROC curve\ndef plot_roc_curve(fpr, tpr):\n    plt.plot(fpr, tpr, color='orange', label='ROC')\n    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend()\n    plt.show()\n\nfpr, tpr, thresholds = roc_curve(y_valid, y_pred2)\nplot_roc_curve(fpr, tpr)","1869061e":"# Create Confusion Matrix\npred_class = y_pred2 > 0.5\npred_class = pred_class.astype(int)\ncm2 = confusion_matrix(y_valid, pred_class)\nprint(cm2)\n\n# Get accuracy\naccuracy2 = round(accuracy_score(y_valid,pred_class),4)\nprint(\"Accuracy: {}\".format(accuracy2),\"\\n\")\n\n# Get f1 score (it is required on the Task 1 of this dataset)\nf1_2 = f1_score(y_valid,pred_class)\nprint(\"F1: {}\".format(f1_2),\"\\n\")\n","b353c007":"# See the feature importance\nimportance_feature = pd.DataFrame({'Value':clf.feature_importance(),'Feature':clf.feature_name()}).sort_values(by=\"Value\", ascending=False)\n\n# Create a plot\nplt.figure(figsize=(20, 10))\nsns.barplot(x = 'Value',y = 'Feature',data = importance_feature);\nplt.title(\"Importance feature\");","4c2ed357":"# We can summarize the results\nprint(\"Accuracy modifying categorical features manually: {}\".format(accuracy))\nprint(\"Accuracy using LightGBM to handle categorical features: {}\".format(accuracy2),\"\\n\")\n\n# We compare matrix\nprint(\"Confusion matrix using manual handling: \\n\", cm,\"\\n\")\nprint(\"Confusion matrix LightGBM: \\n\", cm2, \"\\n\")\n\n# Besides, we add the F1_score that is required  in the Task 1 of this dataset\nprint(\"F1 modifying categorical features manually: {}\".format(f1))\nprint(\"F1 using LightGBM to handle categorical features: {}\".format(f1_2),\"\\n\")","57aaf21e":"# References\n\n* https:\/\/nitin9809.medium.com\/lightgbm-binary-classification-multi-class-classification-regression-using-python-4f22032b36a2\n* https:\/\/medium.com\/@pushkarmandot\/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc\n* https:\/\/www.kaggle.com\/alexisbcook\/categorical-variables\n* https:\/\/www.kaggle.com\/ashishpatel26\/feature-importance-of-lightgbm\n* https:\/\/medium.com\/swlh\/dealing-with-categorical-variables-in-machine-learning-4401b949b093","d408a275":"# 3) Use LightGBM with modified data","c549b431":"# 1) Review and analysis of data","704108b9":"# 5) Compare results","fba152e0":"# Stroke prediction with LightGBM\n\nIn this activity we will use a [dataset available in Kaggle](https:\/\/www.kaggle.com\/fedesoriano\/stroke-prediction-dataset), which has a handful of features of people and a target column `stroke` where is indicated wheter a patient has stroke or not. LightGBM is an powerful ensembling model capable to predict very fast and accurate. In the end of this Notebook, some useful references are mentioned. We address the handling of categorical data using LightGBM and comparing with manual preprocessing. An important point of the data used here is that it is unbalanced. \n<img src=\"https:\/\/webstockreview.net\/images\/pain-clipart-heart-failure.jpg\">\n","b5a8c48f":"We can note that we have 19 right predictions of patients with stroke out of 43 patients. We can use another configuration of options to get a better result, however it is likely that we increase importantly the amount of mistakes in our prediction (false positive). In the last plot, we see that the main features are the glucose level, bmi and age. Hypertension and smoking status play a key role as the most important categorical features. ","fb1698b6":"* Carrying out this new model, we obtain a better result using the inner method of LightGBM to handle categorical data. Our accuracy improved and also we predict correctly two patients more, without trade-off. In [this page](https:\/\/medium.com\/swlh\/dealing-with-categorical-variables-in-machine-learning-4401b949b093), you can see more about the use of LightGBM to handle categorical data. Any comment is welcome!","b5cb115c":"# 2) Manual preprocessing of categorical features\n\nNow, we can modify categorical data to transform to numerical values. For this, we can use different approaches such as Label encoding and One-Hot encoding. More information abou the use of these methods is available in [this section of ML Kaggle course](https:\/\/www.kaggle.com\/alexisbcook\/categorical-variables). First, reviewing our categorical dataset, we can distinguish ordinal and nominal features. Unlike nominal categories, ordinal categories follow an order. For example, heart_disease can be classified as a ordinal value, because we know that class \"1\" or having a heart disease is worst than class 0 or not having a heart disease. In other hand, we can order the feature gender, because neither of its classes (\"Male\", \"Female\" and \"Other\") is better than the other.","9c98fb69":"# 4) Use LightGBM and its method to handle categorical feature"}}