{"cell_type":{"9fba635d":"code","0d857b6a":"code","196a7e2d":"code","d7c42198":"code","e54a2f74":"code","4846bdaa":"code","e821719f":"code","97aa94b3":"code","6e9f8893":"code","eff82b4a":"code","f42acd78":"markdown","980f071a":"markdown","64c2f2cd":"markdown","2a8a174a":"markdown","3cfae8a5":"markdown","61b7bdda":"markdown","f35722b4":"markdown"},"source":{"9fba635d":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom joblib import dump, load\nimport pickle\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import log_loss\nfrom datetime import date\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA","0d857b6a":"\ndata_folder = '..\/input\/lish-moa\/'\n\n# fix the random seed \nxseed = 43\n\n# number of folds for cv\nnfolds = 5\n\n# number of components to retain from PCA decomposition\nnof_comp = 250\n\nmodel_name = 'lr'","196a7e2d":"xtrain = pd.read_csv(data_folder + 'train_features.csv')\nxtest = pd.read_csv(data_folder + 'test_features.csv')\nytrain = pd.read_csv(data_folder + 'train_targets_scored.csv')","d7c42198":"# due to small cardinality of all values, it's faster to handle categoricals that way,\n\nprint(set(xtrain['cp_time']), set(xtest['cp_time']) )\n\n# cp_time\nxtrain['cp_time_24'] = (xtrain['cp_time'] == 24) + 0\nxtrain['cp_time_48'] = (xtrain['cp_time'] == 48) + 0\nxtest['cp_time_24'] = (xtest['cp_time'] == 24) + 0\nxtest['cp_time_48'] = (xtest['cp_time'] == 48) + 0\nxtrain.drop('cp_time', axis = 1, inplace = True)\nxtest.drop('cp_time', axis = 1, inplace = True)\n\n# cp_dose\nprint(set(xtrain['cp_dose']), set(xtest['cp_dose']) )\nxtrain['cp_dose_D1'] = (xtrain['cp_dose'] == 'D1') + 0\nxtest['cp_dose_D1'] = (xtest['cp_dose'] == 'D1') + 0\nxtrain.drop('cp_dose', axis = 1, inplace = True)\nxtest.drop('cp_dose', axis = 1, inplace = True)\n\n# cp_type\nxtrain['cp_type_control'] = (xtrain['cp_type'] == 'ctl_vehicle') + 0\nxtest['cp_type_control'] = (xtest['cp_type'] == 'ctl_vehicle') + 0\nxtrain.drop('cp_type', axis = 1, inplace = True)\nxtest.drop('cp_type', axis = 1, inplace = True)","e54a2f74":"# prepare split\nkf = KFold(n_splits = nfolds)\n\n# separation\nid_train = xtrain['sig_id']; id_test = xtest['sig_id']\nytrain.drop('sig_id', axis = 1, inplace = True) \nxtrain.drop('sig_id', axis = 1, inplace = True)\nxtest.drop('sig_id', axis = 1, inplace = True)\n\n# storage matrices for OOF \/ test predictions\nprval = np.zeros(ytrain.shape)\nprfull = np.zeros((xtest.shape[0], ytrain.shape[1]))","4846bdaa":"# base model definition throught sklearn Pipeline\npca = PCA(n_components = nof_comp)\nlogistic = LogisticRegression(max_iter=10000, tol=0.1, C = 0.5)\nbase_model = Pipeline(steps=[('pca', pca), ('logistic', logistic)])\n\n# a pipeline can be fed into MultiOutputClassifier just like a regular estimator would\nmo_base = MultiOutputClassifier(base_model, n_jobs=-1)","e821719f":"for (ff, (id0, id1)) in enumerate(kf.split(xtrain)):\n     \n    x0, x1 = xtrain.loc[id0], xtrain.loc[id1]\n    y0, y1 = np.array(ytrain.loc[id0]), np.array(ytrain.loc[id1])\n    \n    # stupid fix for empty columns - LogisticRegression blows up otherwise \n    # (the problem occurs for two folds only, each time for a single column)\n    # yes, i know it's ugly\n    check_for_empty_cols = np.where(y0.sum(axis = 0) == 0)[0]\n    if len(check_for_empty_cols):\n        y0[0,check_for_empty_cols] = 1\n    \n    # fit model\n    mo_base.fit(x0,y0)\n    \n    # generate the prediction\n    vpred = mo_base.predict_proba(x1)\n    fpred = mo_base.predict_proba(xtest)\n    \n    for ii in range(0,ytrain.shape[1]):\n        \n        prval[id1,ii] = vpred[ii][:,1]\n        prfull[:,ii] += fpred[ii][:,1]\/nfolds        \n    ","97aa94b3":"prval = pd.DataFrame(prval); prval.columns = ytrain.columns\nprval['sig_id'] = id_train\n\nprfull = pd.DataFrame(prfull); prfull.columns = ytrain.columns\nprfull['sig_id'] = id_test","6e9f8893":"metrics = []\nfor _target in ytrain.columns:\n    metrics.append(log_loss(ytrain.loc[:, _target], prval.loc[:, _target]))\nprint(f'OOF Metric: {np.round(np.mean(metrics),4)}')","eff82b4a":"xcols = list(ytrain.columns); xcols.insert(0, 'sig_id')\nprval = prval[xcols]; prfull = prfull[xcols]\n\n\n# actual submission\nprfull.to_csv('submission.csv', index = False)","f42acd78":"Create OOF forecasts + test ones averaged across folds","980f071a":"# Eval and sub","64c2f2cd":"Check performance - in line with what others have reported in the forum, there is ~ 0.004 gap between CV and LB","2a8a174a":"settings and hyperparameters","3cfae8a5":"# FE","61b7bdda":"# Data","f35722b4":"# Model"}}