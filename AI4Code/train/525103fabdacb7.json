{"cell_type":{"cd693803":"code","c7204b56":"code","6c88dafb":"code","2b338dfc":"code","a12f0a59":"code","2ee18bdb":"code","b831c057":"code","04af35fe":"code","b3831627":"code","6559f986":"code","aeee6334":"code","405f037f":"code","e08bdabc":"code","9fb52f08":"code","79e4794d":"code","3d638bd7":"code","2b19fc03":"code","4ac0808b":"code","2788707b":"code","61151b8d":"code","f156f5c9":"code","cfcd58f3":"code","816cd1a5":"code","070039d7":"code","5dfd889d":"code","ac77cc75":"code","c2948aa6":"code","54491d93":"code","ffa4d279":"code","b643cc50":"code","d87e8107":"markdown","296c48ce":"markdown","0ad1ab32":"markdown","7fb0d40e":"markdown","103992eb":"markdown"},"source":{"cd693803":"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","c7204b56":"## To access tha Dataframe\ndf =pd.read_csv('\/kaggle\/input\/fake-news\/train.csv')\ndf.head()","6c88dafb":"### to get the dataframe size i,e: how many rows and columns\ndf.shape","2b338dfc":"df.isnull().sum()","a12f0a59":"df=df.dropna()","2ee18bdb":"df.shape","b831c057":"## get the independent Features\nx = df.drop('label',axis=1)\nx.head()","04af35fe":"## get the dependent Features\ny = df.label\ny.head()","b3831627":"x.shape,y.shape","6559f986":"import tensorflow as tf\nfrom tensorflow.keras.layers import Embedding,Dense,LSTM\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import one_hot","aeee6334":"### vocabulary size =5000\nvoc_size = 5000\nmessages = x.copy()\nmessages.reset_index(inplace=True)","405f037f":"import nltk\nimport re\nfrom nltk.corpus import stopwords","e08bdabc":"### Dataset Preprocessing\nfrom nltk.stem.porter import PorterStemmer\nps = PorterStemmer()\ncorpus=[]\nfor i in range(0,len(messages)):\n    print(i)\n    review = re.sub('[^a-zA-Z]', ' ', messages['title'][i])\n    review = review.lower()\n    review = review.split()\n    print(review)\n    \n    review = [ps.stem(words) for words in review if not words in stopwords.words('english')]\n    review = ' '.join(review)\n    corpus.append(review)\n                        ","9fb52f08":"corpus","79e4794d":"onehot_rep = [one_hot(words,voc_size) for words in corpus]","3d638bd7":"sent_length = 20\nembedded_docs = pad_sequences(onehot_rep,padding ='pre',maxlen = sent_length)\nprint(embedded_docs)","2b19fc03":"embedded_docs[0]","4ac0808b":"### creating the model\nembedding_vector_features = 40\nmodel = Sequential()\nmodel.add(Embedding(voc_size,embedding_vector_features,input_length = sent_length))\nmodel.add(LSTM(100))\nmodel.add(Dense(1,activation = 'sigmoid'))\nmodel.compile(optimizer ='adam',loss = 'binary_crossentropy',metrics = ['accuracy'])\nprint(model.summary())","2788707b":"len(embedded_docs),y.shape","61151b8d":"import numpy as np\nx_final = np.array(embedded_docs)\ny_final = np.array(y)","f156f5c9":"x_final\n","cfcd58f3":"x_final.shape,y_final.shape","816cd1a5":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(x_final,y_final,test_size = 0.33,random_state=0)","070039d7":"y_train","5dfd889d":"model.fit(X_train,y_train,validation_data = (X_test,y_test),epochs = 10,batch_size = 64)","ac77cc75":"y_pred  = model.predict_classes(X_test)","c2948aa6":"y_pred","54491d93":"from sklearn.metrics import confusion_matrix,accuracy_score","ffa4d279":"confusion_matrix(y_test,y_pred)","b643cc50":"accuracy_score(y_pred,y_test)","d87e8107":"### Model Tranning","296c48ce":"### Embedding Representation","0ad1ab32":"### one hot representation","7fb0d40e":"### performance Metrics and Accuracy","103992eb":"### Dropping the NAN vlaues"}}