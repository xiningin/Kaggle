{"cell_type":{"36517ead":"code","22f267b0":"code","bc47f1d3":"code","d940263a":"code","0217e4a8":"code","476fb6a9":"code","dfc79b06":"code","e1a60d46":"code","73f3eddc":"code","95d436d7":"code","7c19a5f0":"code","7dfb1dff":"code","8a89e940":"code","b411d1a2":"code","48eee5c1":"code","3d60af87":"code","f72b1303":"code","635f2b6b":"code","7f14ecd0":"code","e53d0e71":"code","9d169d1f":"code","cf5fff61":"markdown","597c3bf7":"markdown","54974fc8":"markdown","2ecf2928":"markdown","9566d66a":"markdown","9da94700":"markdown","cc6b87e2":"markdown","a700016b":"markdown","9f917530":"markdown","666ddc5d":"markdown","5aa9509a":"markdown","c8eefab9":"markdown","8c15b114":"markdown","2c6b8aa5":"markdown","28fede68":"markdown","c2d4d338":"markdown","767e38c8":"markdown","4a65b9e6":"markdown","8bbb1f9f":"markdown"},"source":{"36517ead":"import pandas as pd \n\ndata = pd.read_csv(\"..\/input\/wisconsin-breast-cancer-cytology\/BreastCancer2.csv\") # reading data from input file\ndata= data.drop([\"id\"],axis=1) # remove useless feature ","22f267b0":"data.head() #data features and class","bc47f1d3":"x = data.drop([\"class\"],axis = 1) # x consist only features\ny = data.loc[:,\"class\"] # y consist only class\n\nprint(x.iloc[0:5])\nprint(y.iloc[0:5])","d940263a":"# AttributeError: 'tuple' object has no attribute 'fit' Therefore we have to transform numpy array\nx = data.drop([\"class\"],axis = 1).values \ny = data.loc[:,\"class\"].values","0217e4a8":"from sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.33,random_state=42) # create a %33 test data from orginal data\n\nprint(\"x train shape:\",x_train.shape)\nprint(\"x test shape:\",x_test.shape)\nprint(\"y train shape:\",y_train.shape)\nprint(\"y test shape:\",y_test.shape)\n\n","476fb6a9":"from IPython.display import Image\nImage(\"..\/input\/logistic-shematic\/lrs.png\")","dfc79b06":"from IPython.display import Image\nImage(\"..\/input\/function\/sigmoid.png\")","e1a60d46":"from sklearn.linear_model import LogisticRegressionCV # Use the sklearn module \nlrc = LogisticRegressionCV()\nlrc.fit(x_train,y_train) \nprint(\"logistic score\",lrc.score(x_test,y_test)*100)","73f3eddc":"from IPython.display import Image\nImage(\"..\/input\/knnclass\/KNN circle.png\")","95d436d7":"from  sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=5) # K values \nknn.fit(x_train,y_train)\nprint(\"KNN score\",knn.score(x_test,y_test)*100)","7c19a5f0":"neighbors_list = []\n\nfor i in range(1,15):\n    knn1 = KNeighborsClassifier(n_neighbors = i)\n    knn1.fit(x_train,y_train)\n    neighbors_list.append(knn1.score(x_test,y_test))\nprint(neighbors_list) # find finest k value for classification","7dfb1dff":"from IPython.display import Image\nImage(\"..\/input\/support-vector\/SVM.png\")","8a89e940":"from sklearn.svm import SVC\nsvc = SVC(random_state=1,gamma=0.22)\nsvc.fit(x_train,y_train)\nprint(\"SVC score\",svc.score(x_test,y_test))","b411d1a2":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\nprint(\"Naive Boyes score\",nb.score(x_test,y_test))","48eee5c1":"from IPython.display import Image\nImage(\"..\/input\/treedesi\/decision tree_LI.jpg\")","3d60af87":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train,y_train)\nprint(\"Decision Tree Score\",dt.score(x_test,y_test))","f72b1303":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators = 10, random_state=1)\nrf.fit(x_train,y_train)\nprint(\"Random Forest Classification Score\",rf.score(x_test,y_test))","635f2b6b":"from IPython.display import Image\nImage(\"..\/input\/confuison\/matrix.png\")","7f14ecd0":"from sklearn.metrics import confusion_matrix\ny_prediction = rf.predict(x_test) \ncm = confusion_matrix(y_true=y_test,y_pred=y_prediction)#actual value -->y_true\nprint(\"Confusion Matrix \",cm)","e53d0e71":"# Vizualition of Confusion Matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nf,ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot = True,fmt=\".0f\")\nplt.xlabel(\"Prediction Value\")\nplt.ylabel(\"True Value\")\nplt.show()","9d169d1f":"print(\"Logistic Regression Classification score :\",lrc.score(x_test,y_test)*100)\nprint(\"KNN score :\",knn.score(x_test,y_test)*100)\nprint(\"SVC score :\",svc.score(x_test,y_test)*100)\nprint(\"Naive Boyes score :\",nb.score(x_test,y_test)*100)\nprint(\"Decision Tree Score :\",dt.score(x_test,y_test)*100)\nprint(\"Random Forest Classification Score :\",rf.score(x_test,y_test)*100)","cf5fff61":"**Chapter 7 : Confusion Matrix**\n* Confusion matrix create a table which present visualization of performance of an algorithm","597c3bf7":"*  x : Features of data - Train part\n*  w : weight - coefficient \n*  net input fuction : z = w0x0 + w1x1 + ..... + wnxn\n*  Activation function : Sigmoid fuction","54974fc8":"Image take from [github](https:\/\/rasbt.github.io\/mlxtend\/user_guide\/classifier\/LogisticRegression\/)","2ecf2928":"Image take from [Codeproject](http:\/\/lh3.ggpht.com\/_qIDcOEX659I\/SzjW6wGbmyI\/AAAAAAAAAtY\/Nls9tSN6DgU\/contingency_thumb%5B3%5D.png?imgmax=800)","9566d66a":"I hope this kernel help you to understand this consept.\n\nPlease comment if something wrong\n\nThank you for effort.\n\n\n\n","9da94700":"H1 does not separate the classes. H2 does, but only with a small margin. H3 separates them with the maximum margin.","cc6b87e2":"**Chapter 1 : Logistic Classification **\n","a700016b":"**Chapter 3 : Support Vector Machine**\n* Support vector machine(SVM) use for regression and classification, but mostly use for classification.  Datas are ploted as a point in n-dimensional space. Points positioning the particular coordinate due to their value.Then, draw line or plane(depens on dimension) for separate the two classes. Choose the line or plane which has maximum margin(distance called as margin). ","9f917530":"**Chapter 5 : Decision Tree Classification**\n* Decision Tree Classification split the plot into subplot which incluede tree models where points can take values. Each split create leaves which present in the trees represent class label.\n","666ddc5d":"**Chapter 0 : Data Arrange and Train,Test**","5aa9509a":"**Chapter 2 : KKN classification**\nKNN Algorithm is relying on feature similarity: \n\n","c8eefab9":"**Chapter 6 : Random Forest Classification**\n* This classification alghorithm is ensemble learning model. It take sum of the decision tree to one algorithmic process.","8c15b114":"The green solid circle is test sample should be classified either to the first class of blue squares or to the second class of red triangles. If k = 3 (solid line circle) it is assigned to the second class because there are 2 triangles and only 1 square inside the inner circle. If k = 5 (dashed line circle) it is assigned to the first class (3 squares vs. 2 triangles inside the outer circle).","2c6b8aa5":"Image take from [github](https:\/\/rasbt.github.io\/mlxtend\/user_guide\/classifier\/LogisticRegression\/)","28fede68":"**Chapter 4 : Naive Bayes**\n* Navie Bayes algorithm  predict unknown data to classificate and do this process faster than other algorithms. It works on Bayes theorem of probability","c2d4d338":"**Content Of Kernel**\n* Data Arrange and Train,Test\n* Logistic Regression Classification\n* KKN Classification\n* Support Vector Machine\n* Naive Bayes\n* Decision Tree Classification\n* Random Forest Classification\n* Confusion Matrix\n* Comparison of Algorithm Performance","767e38c8":"* Unit Step function : Binary output\n* Error : Backward propagation\n* More detail  in the my previous kernel : [Machine learning Review](https:\/\/www.kaggle.com\/zayon5\/machine-learning-review?scriptVersionId=5819035)","4a65b9e6":"Image taken from [Slideshare](https:\/\/www.slideshare.net\/marinasantini1\/lecture02-machine-learning)","8bbb1f9f":"**Comparison of Algorithm Performance**"}}