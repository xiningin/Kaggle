{"cell_type":{"0ca53f12":"code","256e2f9b":"code","8f9ec92e":"code","887ca9f4":"code","1ee93ad7":"code","56b5f3b9":"code","daa19064":"code","c35b3b7f":"code","0af360ea":"code","680cdb41":"code","f0a3f64c":"code","4f60adfb":"code","473054d7":"code","00f95672":"code","f212271a":"code","a6ae443a":"code","b713dcfb":"code","384b5f35":"code","c941a1d7":"code","3e0de40a":"code","60104f02":"code","32d64dee":"code","0e55cbbe":"code","da459bad":"code","ee715da0":"code","79cce2ba":"code","ad51bdc8":"code","83ab1be5":"code","ccb53c75":"code","4589a36f":"code","de1157aa":"code","08f0db50":"markdown","7c2acf20":"markdown","86888dda":"markdown","72a09027":"markdown","f727a023":"markdown","6cd04800":"markdown","f1ee718a":"markdown","ddb73a37":"markdown","e5905b34":"markdown","f3fb70b3":"markdown","cdaa98c6":"markdown","624ca009":"markdown","a5b97125":"markdown","bc8ae164":"markdown","74fc5205":"markdown","0e294d0d":"markdown","3eaef81c":"markdown","c17dd6e4":"markdown","0ed37f5b":"markdown","8aba4172":"markdown","c8a9b59f":"markdown","9064df5e":"markdown","a33aefd8":"markdown","750a3b8f":"markdown","3d2c8524":"markdown","7516a4ea":"markdown","bf985509":"markdown","d1f65e04":"markdown","1b454c71":"markdown","4b28c85d":"markdown","c9aeb4a1":"markdown"},"source":{"0ca53f12":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import datasets, linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.feature_selection import RFE\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import IncrementalPCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier\nimport random\nfrom sklearn import metrics\nfrom imblearn.over_sampling import SMOTE\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","256e2f9b":"train = pd.read_csv('..\/input\/santander-customer-satisfaction\/train.csv')\ntest = pd.read_csv('..\/input\/santander-customer-satisfaction\/test.csv')","8f9ec92e":"train.head()","887ca9f4":"train.describe()","1ee93ad7":"train.isna().sum().sort_values(ascending=False)","56b5f3b9":"train['imp_ent_var16_ult1'].value_counts()","daa19064":"# Checking outliers at 25%,50%,75%,90%,95% and 99%\ntrain.describe(percentiles=[.25,.5,.75,.90,.95, .975,.99,.999])","c35b3b7f":"high = .99\nfirst_quartile = 0.25\nthird_quartile = 0.75\nquant_df = train.quantile([high, first_quartile, third_quartile])","0af360ea":"quant_df","680cdb41":"train_df = train.drop(['ID', 'TARGET'], axis = 1)","f0a3f64c":"train_df = train_df.apply(lambda x: x[(x <= quant_df.loc[high,x.name])], axis=0)","4f60adfb":"train_df.describe(include='all')","473054d7":"train_df.shape","00f95672":"train_df.head()","f212271a":"train_df = pd.concat([train.loc[:,'ID'], train_df], axis=1)\n\ntrain_df = pd.concat([train.loc[:,'TARGET'], train_df], axis=1)","a6ae443a":"train_df.describe()","b713dcfb":"train_df.isnull().sum().sort_values(ascending=False)","384b5f35":"new_train_df = train_df\nfor col in new_train_df.columns:\n    min_val = min(new_train_df[col])\n    max_val = max(new_train_df[col])\n    new_train_df[col].fillna(round(random.uniform(min_val, max_val), 2), inplace =True)","c941a1d7":"new_train_df.isna().sum().sort_values(ascending=False)","3e0de40a":"y = new_train_df['TARGET']\nX = new_train_df.drop(['TARGET','ID'], axis=1)","60104f02":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=99)","32d64dee":"lr = LogisticRegression()\nlr.fit(X_train,y_train)\n","0e55cbbe":"preds = lr.predict(X_test)\nprint(\"Accuracy with Logistic = \", metrics.accuracy_score(y_test, preds))","da459bad":"new_train_df['TARGET'].value_counts()","ee715da0":"sm = SMOTE(kind = \"regular\")\nX_tr,y_tr = sm.fit_sample(X_train,y_train)","79cce2ba":"print(X_tr.shape)\nprint(y_tr.shape)\n","ad51bdc8":"lr.fit(X_tr,y_tr)\n\nlr_preds = lr.predict(X_test)\nprint(\"Accuracy with Logistic = \", metrics.accuracy_score(y_test, lr_preds))","83ab1be5":"dt1 = DecisionTreeClassifier(max_depth=5)\ndt1.fit(X_tr, y_tr)\n\ndt_preds = dt1.predict(X_test)\nprint(\"Accuracy with Decision Tree = \", metrics.accuracy_score(y_test, dt_preds))","ccb53c75":"rft = RandomForestClassifier(n_jobs=-1)\nrft.fit(X_tr, y_tr)\n\nrft_preds = rft.predict(X_test)\nprint(\"Accuracy with Random Forest = \", metrics.accuracy_score(y_test, rft_preds))","4589a36f":"x_test_final = test.drop(['ID'], axis=1)\nfinal_prediction = rft.predict(x_test_final)","de1157aa":"submission = pd.DataFrame({\n        \"ID\": test[\"ID\"],\n        \"TARGET\": final_prediction\n    })\nsubmission.to_csv('RandomForect.csv',header=True, index=False)","08f0db50":"# Final Prediction","7c2acf20":"Yes there it is . We can clearly see there is only 1 value of 17595.15 which definitely serve as an outlier. To clean this dataset outlier ommission is needed.","86888dda":"First we will see the values of different quartiles","72a09027":"# Import important libraries","f727a023":"**Santander Customer Satisfaction**\n\n**Problem statement:-**\n\nFrom frontline support teams to C-suites, customer satisfaction is a key measure of success. Unhappy customers don't stick around. What's more, unhappy customers rarely voice their dissatisfaction before leaving.\n\nThis kernel will help Santander Bank to identify dissatisfied customers early in their relationship. Doing so would allow Santander to take proactive steps to improve a customer's happiness before it's too late.\n\nWe are provided with an anonymized dataset containing a large number of numeric variables. The \"TARGET\" column is the variable to predict. It equals 1 for unsatisfied customers and 0 for satisfied customers.\n\nThe task is to predict the probability that each customer in the test set is an unsatisfied customer.\n\n![Customer Satisfaction](https:\/\/cdn.dribbble.com\/users\/489445\/screenshots\/1819359\/pyxl_blog_creating_customer_experience_2.gif)\n\n","6cd04800":"Now we can see our dataset is free of null values :)","f1ee718a":"# Handling null values","ddb73a37":"We have got multiple models with all their accuracies. We can choose anyone of them to make further predictions.","e5905b34":"Now lets go to model testing","f3fb70b3":"We saw there are only numerical columns in this dataset. However looking at the range it seems there are many outliers here","cdaa98c6":"The much needed train test split for cross validation","624ca009":"## Decision tree","a5b97125":"# Analysing dataset","bc8ae164":"#### Lets take 99% as threshold for outlier. Drop all values above 0.99 percentile","74fc5205":"So we can see that almost 96% of dataset is having value 0. We need to balance this dataset to get better results","0e294d0d":"Lets load","3eaef81c":"As RFT has maximum accuracy lets use that.","c17dd6e4":"To handle null values we will take a random value between minimum and maximum value of each column and use that random value for imputation","0ed37f5b":"### Logistic Regression","8aba4172":"# Model building","c8a9b59f":"Now we have our quartile dataframe","9064df5e":"Lets now prepare and clean our training dataset.\n\nFirst we need to remove TARGET and ID column from our training dataset.","a33aefd8":"### Dataset balancing\nWe will use **SMOTE** technique to balance the dataset","750a3b8f":"## Random Forest","3d2c8524":"Lets now fit the data","7516a4ea":"Accuracy with Logistic is quite good. But hold on do we have a balanced dataset. Maybe not. Lets check if imbalance is the root cause of this high accuracy.","bf985509":"By analysing this visually it seems that ranges 0.99, 0.25, 0.75 seems important. Lets save their value somewhere","d1f65e04":"# Outlier handling","1b454c71":"We can see that we didnt have that much data loss and also our data is now free of outliers.\n\nNow lets get back our ID and TARGET columns","4b28c85d":"After dropping outliers we can see that we have now encountered some null values in our dataset. We need to cater this issue.\n","c9aeb4a1":"There are no null values present either\n\nLets check one of the column to confirm our theory about outliers"}}