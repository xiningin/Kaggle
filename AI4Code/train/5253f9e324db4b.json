{"cell_type":{"d80328ff":"code","bd4126f8":"code","4d4259d5":"code","446ba5d3":"code","26bd0578":"code","cdf64b81":"code","87c13cf7":"code","7044bd65":"code","5767c80a":"code","b378f4f2":"code","740b83fc":"code","f9c094db":"code","e48b7034":"code","c2652870":"code","b3a6101d":"code","8885d514":"markdown","edf67403":"markdown"},"source":{"d80328ff":"import pandas as pd\nfrom importlib import reload\nimport sys\nfrom imp import reload\nimport warnings\nwarnings.filterwarnings('ignore')\nif sys.version[0] == '2':\n    reload(sys)\n    sys.setdefaultencoding(\"utf-8\")","bd4126f8":"df = pd.read_csv('..\/input\/word2vec-nlp-tutorial\/labeledTrainData.tsv',delimiter=\"\\t\")\ndf.head()","4d4259d5":"df = df.drop(['id'],axis =1)\ndf.head()","446ba5d3":"df2 = pd.read_csv('..\/input\/imdb-review-dataset\/imdb_master.csv',encoding=\"latin-1\")\ndf2.head()\n","26bd0578":"# dropping columns - Unnamed: 0','type' and 'file'\ndf2 = df2.drop(['Unnamed: 0','type','file'],axis=1)\ndf2.columns = [\"review\",\"sentiment\"]\ndf2.head()","cdf64b81":"df2 = df2[df2.sentiment != 'unsup']\ndf2['sentiment'] = df2['sentiment'].map({'pos': 1, 'neg': 0})\ndf2.head()","87c13cf7":"df = pd.concat([df, df2]).reset_index(drop=True)\ndf.head()","7044bd65":"import re\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\nstop_words = set(stopwords.words(\"english\")) \nlemmatizer = WordNetLemmatizer()\n\n","5767c80a":"def clean_text(text):\n    text = re.sub(r'[^\\w\\s]','',text, re.UNICODE)\n    text = text.lower()\n    text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n    text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n    text = [word for word in text if not word in stop_words]\n    text = \" \".join(text)\n    return text\n","b378f4f2":"df['Processed_Reviews'] = df.review.apply(lambda x: clean_text(x))","740b83fc":"df.head()","f9c094db":"df.Processed_Reviews.apply(lambda x: len(x.split(\" \"))).mean()","e48b7034":"# tensorflow for improving accuracy","c2652870":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model, Sequential\nfrom keras.layers import Convolution1D\nfrom keras import initializers, regularizers, constraints, optimizers, layers\n\nmax_features = 6000\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(df['Processed_Reviews'])\nlist_tokenized_train = tokenizer.texts_to_sequences(df['Processed_Reviews'])\n\nmaxlen = 130\nX_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\ny = df['sentiment']\n\nembed_size = 128\nmodel = Sequential()\nmodel.add(Embedding(max_features, embed_size))\nmodel.add(Bidirectional(LSTM(32, return_sequences = True)))\nmodel.add(GlobalMaxPool1D())\nmodel.add(Dense(20, activation=\"relu\"))\nmodel.add(Dropout(0.05))\nmodel.add(Dense(1, activation=\"sigmoid\"))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nbatch_size = 100\nepochs = 3\nmodel.fit(X_t,y, batch_size=batch_size, epochs=epochs, validation_split=0.2)","b3a6101d":"df_test=pd.read_csv(\"..\/input\/word2vec-nlp-tutorial\/testData.tsv\",header=0, delimiter=\"\\t\", quoting=3)\ndf_test.head()\ndf_test[\"review\"]=df_test.review.apply(lambda x: clean_text(x))\ndf_test[\"sentiment\"] = df_test[\"id\"].map(lambda x: 1 if int(x.strip('\"').split(\"_\")[1]) >= 5 else 0)\ny_test = df_test[\"sentiment\"]\nlist_sentences_test = df_test[\"review\"]\nlist_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\nX_te = pad_sequences(list_tokenized_test, maxlen=maxlen)\nprediction = model.predict(X_te)\ny_pred = (prediction > 0.5)\nfrom sklearn.metrics import f1_score, confusion_matrix\nprint('F1-score: {0}'.format(f1_score(y_pred, y_test)))\nprint('Confusion matrix:')\nconfusion_matrix(y_pred, y_test)\n","8885d514":"### function for Cleaning text","edf67403":"## Cleaning review file and saving the processed review"}}