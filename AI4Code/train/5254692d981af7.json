{"cell_type":{"a55d5b1e":"code","770c9105":"code","1e904af3":"code","46191595":"code","f63749fa":"code","68417cdd":"code","fc991196":"code","a62a1cd7":"code","5760f2ef":"code","4b3545dd":"code","f35fcaf5":"code","fe39d2cd":"code","ba82e68d":"code","0b917f1f":"code","c200ccde":"code","5ff785eb":"code","564ab525":"code","c907ad03":"code","98fc1244":"code","ff7ead50":"code","c29e4b46":"code","cf77b627":"markdown","54f24d44":"markdown","0e97e390":"markdown","dc9fe13c":"markdown","c97fb89d":"markdown","670477b5":"markdown","232a0ac5":"markdown","3a0dbb65":"markdown","7ba88e6c":"markdown","e0b12aa1":"markdown","0062c6c3":"markdown","7ff8c19b":"markdown","9f3333fc":"markdown","60cbc952":"markdown","624399cc":"markdown","d22a7cc6":"markdown"},"source":{"a55d5b1e":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nimport xgboost as xgb\n\nbold = \"\\033[1m\"","770c9105":"train = pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/train.csv\")\nprint(bold + \"Training Set :\\n\")\ndisplay(train.head())\nprint(bold + str(train.shape))\n\nprint(bold + \"\\nTest Set :\\n\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/test.csv\")\ndisplay(test.head())\nprint(bold + str(test.shape))","1e904af3":"target = \"loss\"\npredictors = [x for x in train.columns if x not in [\"id\", \"loss\"]]","46191595":"X = train[predictors]\ny = train[target]\ntest = test[predictors]\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.25, random_state = 42)","f63749fa":"d_train = xgb.DMatrix(data = X_train.values, label = y_train.values, silent = True, feature_names = X_train.columns)\n\nd_val = xgb.DMatrix(data = X_val.values, label = y_val.values, silent = True, feature_names = X_val.columns)","68417cdd":"params = {\"n_jobs\": -1,\n          \"random_state\": 42}","fc991196":"start = datetime.now()\n\nxgb.train(params, d_train, num_boost_round = 1000, evals = [(d_val, \"eval\")], early_stopping_rounds = 10, verbose_eval = 10)\n\nprint(datetime.now() - start)","a62a1cd7":"xgb_sklearn = xgb.XGBRegressor(n_estimators = 1000, **params)","5760f2ef":"start = datetime.now()\n\nxgb_sklearn.fit(X_train.values, y_train.values,\n                eval_set=[(X_val.values, y_val.values)],\n                eval_metric = \"rmse\",\n                early_stopping_rounds = 10,\n                verbose = 10)\n\nprint(datetime.now() - start)","4b3545dd":"initial_params = {\"n_jobs\": -1,\n                  \"random_state\": 42,\n                  \"gamma\": 0.25,\n                  \"max_depth\": 12,\n                  \"min_child_weight\": 8,\n                  \"subsample\": 0.8,\n                  \"colsample_bytree\": 0.7,\n                  }","f35fcaf5":"start = datetime.now()\n\nxgb.train(initial_params, d_train, num_boost_round = 1000, evals = [(d_val, \"eval\")], early_stopping_rounds = 50, verbose_eval = 25)\n\nprint(datetime.now() - start)","fe39d2cd":"xgb_initial_sklearn = xgb.XGBRegressor(n_estimators = 1000, **initial_params)","ba82e68d":"start = datetime.now()\n\nxgb_initial_sklearn.fit(X_train.values, y_train.values,\n                        eval_set=[(X_val.values, y_val.values)],\n                        eval_metric = \"rmse\",\n                        early_stopping_rounds = 50,\n                        verbose = 25)\n\nprint(datetime.now() - start)","0b917f1f":"params[\"tree_method\"] = \"gpu_hist\"","c200ccde":"start = datetime.now()\n\nxgb.train(params, d_train, num_boost_round = 1000, evals = [(d_val, \"eval\")], early_stopping_rounds = 500, verbose_eval = 50)\n\nprint(datetime.now() - start)","5ff785eb":"xgb_gpu_sklearn = xgb.XGBRegressor(n_estimators = 1000, **params)","564ab525":"start = datetime.now()\n\nxgb_gpu_sklearn.fit(X_train.values, y_train.values,\n                    eval_set=[(X_val.values, y_val.values)],\n                    eval_metric = \"rmse\",\n                    early_stopping_rounds = 500,\n                    verbose = 50)\n\nprint(datetime.now() - start)","c907ad03":"initial_params[\"tree_method\"] = \"gpu_hist\"\ninitial_params[\"eta\"] = 0.1","98fc1244":"start = datetime.now()\n\nxgb.train(initial_params, d_train, num_boost_round = 1000, evals = [(d_val, \"eval\")], early_stopping_rounds = 500, verbose_eval = 100)\n\nprint(datetime.now() - start)","ff7ead50":"xgb_initial_gpu_sklearn = xgb.XGBRegressor(n_estimators = 1000, **initial_params)","c29e4b46":"start = datetime.now()\n\nxgb_initial_gpu_sklearn.fit(X_train.values, y_train.values,\n                            eval_set=[(X_val.values, y_val.values)],\n                            eval_metric = \"rmse\",\n                            early_stopping_rounds = 500,\n                            verbose = 100)\n\nprint(datetime.now() - start)","cf77b627":"We have same eval scores and almost same time with 2.07 (This could be change after rerun)","54f24d44":"This process took five seconds. Again, everything looks same.","0e97e390":"Again, we have same eval scores and almost same time.","dc9fe13c":"To speed up xgboost model, using GPU is the most effective option. \n\nTo use GPU, we only need to set tree method to \"gpu_hist\"\n\nI set early stopping to 500 to see the performance of GPU.","c97fb89d":"# 2) Initial Parameters","670477b5":"# Data Preparation","232a0ac5":"First comparison is training with default parameters. I only set n_jobs and random_state parameters. \n\nDefault learning rate, or eta is 0.3 in xgboost. It is a little bit large number, so that I set early stopping rounds to 10.","3a0dbb65":"# Takeaways","7ba88e6c":"It looks like there is no significant difference between **train** and **fit**.\n\n**We get same results at the same time.**\n\nOnly difference is, we need DMatrix data form for using xgb.train.","e0b12aa1":"Second, I set parameters initially and using early stopping rounds as 50.","0062c6c3":"Initial parameters with using GPU, again we have same results.","7ff8c19b":"For using xgb.train, we need to convert data to DMatrix form.","9f3333fc":"# 4) Initial Parameters with using GPU","60cbc952":"# 1) Default Parameters","624399cc":"XGBoost is a slow algorithm, especially you have a little bit large data. \n\nI just think to speed up xgboost model. I read a post on internet about XGBoost with using train is faster than XGBoost's sklearn api, or using fit method.\n\nIn this notebook, I want to compare this two method.","d22a7cc6":"# 3) Default Parameters with using GPU"}}