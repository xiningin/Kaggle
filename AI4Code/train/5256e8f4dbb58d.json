{"cell_type":{"9b4caf2f":"code","db8e49cf":"code","93735d54":"code","f7dc7497":"code","2aea0490":"code","40b17104":"code","811855ee":"code","aa35bb92":"code","47682790":"code","56bef38f":"code","479d33eb":"code","ff86042e":"code","ee866f8a":"code","34a18e60":"code","0c42f199":"code","e083b4a3":"code","d7ff4924":"code","6c265b17":"code","08aea017":"code","2b96d232":"code","38d7ab54":"code","2b505a6d":"code","312c8b8b":"code","b50ec385":"code","3d8e41f2":"code","f8b8e610":"code","46b58eaa":"code","9ca3e581":"code","98474d20":"code","1fc930e8":"code","fd39b305":"code","2afdc1eb":"markdown","d2117a3d":"markdown","5ba2f92d":"markdown","59ae12a8":"markdown","75c85fb5":"markdown","994acaad":"markdown","53609d43":"markdown","e721a2a7":"markdown"},"source":{"9b4caf2f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","db8e49cf":"#importing dataset\ndf=pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndf.head()","93735d54":"df.shape","f7dc7497":"df.info()","2aea0490":"df.describe()","40b17104":"sns.pairplot(df,hue='Outcome')\n","811855ee":"corr = df.corr()\ncorr.style.background_gradient(cmap='coolwarm')","aa35bb92":"sns.set_theme(style=\"whitegrid\")\nsns.boxplot(x=\"Age\", data=df, palette=\"Set3\")\nplt.title(\"Age Distribution\")","47682790":"fig = plt.figure(figsize = (15,20))\nax = fig.gca()\ndf.hist(ax = ax)","56bef38f":"df.Outcome.value_counts().plot(kind='bar')\nplt.xlabel(\"Diabetes or Not\")\nplt.ylabel(\"Count\")\nplt.title(\"Outcome \")\n#Here we can see that dataset is not much imbalanced so there is no need to balance.","479d33eb":"X=df.drop('Outcome',axis=1)\nX.head()","ff86042e":"y=df['Outcome']\ny.head()","ee866f8a":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2,random_state=0)\n","34a18e60":"X_train.shape","0c42f199":"from sklearn.preprocessing import StandardScaler","e083b4a3":"sc_x=StandardScaler()\nX_train=sc_x.fit_transform(X_train)\nX_test=sc_x.transform(X_test)","d7ff4924":"from sklearn.neighbors import KNeighborsClassifier","6c265b17":"knn=KNeighborsClassifier(n_neighbors=5,metric='euclidean',p=2)\nknn.fit(X_train,y_train)","08aea017":"y_pred=knn.predict(X_test)\ny_pred","2b96d232":"knn.score(X_test,y_test)","38d7ab54":"from sklearn.metrics import accuracy_score\nfrom sklearn import metrics","2b505a6d":"metrics.accuracy_score(y_test,y_pred)","312c8b8b":"from sklearn.metrics import confusion_matrix\nmat = confusion_matrix(y_test, y_pred)\nmat","b50ec385":"from sklearn.metrics import classification_report\ntarget_names = ['Diabetes', 'Normal']\nprint(classification_report(y_test, y_pred, target_names=target_names))","3d8e41f2":"#For selecting K value\nerror_rate = []\n\n# Will take some time\nfor i in range(1,40):\n    \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))","f8b8e610":"import matplotlib.pyplot as plt\nplt.figure(figsize=(10,6))\nplt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","46b58eaa":"#From graph we can see that optimize k value is 16,17,18\n# Now we will train our KNN classifier with this k values\n\nknn=KNeighborsClassifier(n_neighbors=18,metric='euclidean',p=2)\nknn.fit(X_train,y_train)","9ca3e581":"y_pred=knn.predict(X_test)\ny_pred","98474d20":"knn.score(X_test,y_test)","1fc930e8":"from sklearn.metrics import confusion_matrix\nmat = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(10, 8))\nsns.heatmap(mat, annot=True)","fd39b305":"from sklearn.metrics import classification_report\ntarget_names = ['Diabetes', 'Normal']\nprint(classification_report(y_test, y_pred, target_names=target_names))","2afdc1eb":" 2. How many total observations in data?\n    \n    There are total 768 observations are there\n 3. How many independent variables?\n\n    There are total 8 independent varibales are there\n 4. Which is dependent variable?\n     \n    Outcome is the dependent variable\n\n","d2117a3d":"\n Now we can plot the distribution of data wrt dependent variable i.e outcome","5ba2f92d":"Here, from correlation matrix we can see that age is highly correlated with outcome.","59ae12a8":"1. Why you want to apply classification on selected dataset? Discuss full story behind dataset\n   \n    Here, the independent variable which is 'Outcome' having data as class value so we can apply classification to classify the test data belongs to which class.\n","75c85fb5":"# KNN","994acaad":"To select optimize k value we will use elbow method","53609d43":"6. Quantify goodness of your model and discuss steps taken for improvement.\n\n    For this dataset KNN had archive 81% accuracy. We can further improve accuracy by using bagging and boosting techniques.\n\n7. Can we use KNN for regression also? Why \/ Why not?\n\n    KNN algorithm can be used for both classification and regression problems. The KNN algorithm uses \u2018feature similarity\u2019 to predict the values of any new data points. This means that the new point is assigned a value based on how closely it resembles the points in the training set.\n\n8. Discuss drawbacks of algorithms such as KNN\n  \n    -> It does not work well with large dataset and high dimensional dataset.\n\n    -> Knn is noise sensitive dataset, we need to do feature engineering like outlier removal, handling missing value,etc.\n\n    -> Require high memory \u2013 need to store all of the training data\n\n    -> Given that it stores all of the training, it can be computationally expensive\n","e721a2a7":"  5. Which are most useful variable in classification? Prove using correlation."}}