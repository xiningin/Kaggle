{"cell_type":{"bc3c1fa3":"code","8047c1cb":"code","7dc23e7a":"code","bbd17fb9":"code","ba8f736e":"code","0220ad20":"code","79318b30":"code","e0f8aed5":"code","993896f3":"code","9ff53a88":"code","20ca6785":"code","454c67ca":"code","798018cc":"code","ed1baa74":"code","3d8458cc":"code","fa515ff6":"code","5673d41c":"code","14720244":"code","b2469fae":"code","667a1d6f":"code","eac94390":"code","119fed55":"code","2e16ba87":"markdown","36396735":"markdown","9eb17e9d":"markdown","005868de":"markdown","dc1bbab1":"markdown","ce039707":"markdown","aaa8eae7":"markdown","652a0428":"markdown","08e6707a":"markdown","6deffc93":"markdown","e758a118":"markdown","ccb22f1b":"markdown","7d71a732":"markdown","32b324af":"markdown"},"source":{"bc3c1fa3":"!pip install pytorch-tabnet","8047c1cb":"FLAG_LOCAL = False # Flag to run in kaggle notebook or in jupyter server :)\n\nKAGGLE_PATH = \"\/kaggle\/input\/tabular-playground-series-mar-2021\/\"\nLOCAL_PATH = \"\/home\/rapela\/Downloads\/kaggle\/tps_mar\/input\/\"\n\nPATH = (LOCAL_PATH if FLAG_LOCAL else KAGGLE_PATH)\n\nTRAIN_PATH = PATH + \"train.csv\"\nTEST_PATH = PATH + \"test.csv\"\nSUBMISSION = PATH + \"sample_submission.csv\"\nSUBMISSION_OUTPUT = \"submission.csv\"\n\nprint(TRAIN_PATH)\nprint(TEST_PATH)\nprint(SUBMISSION)\n\nNUM_FOLDS = 2\nSEED = 42","7dc23e7a":"## TabNet Parameters\nMAX_EPOCH = 500\nN_D = 2 \nN_A = 2 \nN_STEPS = 3\nGAMMA = 1.3\nLAMBDA_SPARSE = 0\nOPT_LR = 1e-2\nOPT_WEIGHT_DECAY = 1e-5\nOPT_MOMENTUM = 0.9\nMASK_TYPE = \"entmax\"\nSCHEDULER_MIN_LR = 1e-6\nSCHEDULER_FACTOR = 0.9\nDEVICE_NAME = \"cuda\"\n\nBATCH_SIZE = 1024*4","bbd17fb9":"import torch\nfrom torch import nn\nfrom pytorch_tabnet.tab_model import TabNetClassifier\nfrom pytorch_tabnet.pretraining import TabNetPretrainer\n\nfrom tqdm.notebook import tqdm\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\n\nfrom sklearn.metrics import roc_auc_score\n\nimport numpy as np\nimport pandas as pd \n\nimport os\nimport random\nimport sys\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n\n\ndef seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        \nseed_everything(SEED)","ba8f736e":"train = pd.read_csv(TRAIN_PATH, index_col='id')\ntest = pd.read_csv(TEST_PATH, index_col='id')\nsubmission = pd.read_csv(SUBMISSION, index_col='id')\ntarget = train.pop(\"target\")\ntarget = target.values","0220ad20":"train.shape","79318b30":"train.head()","e0f8aed5":"test.shape","993896f3":"test.head()","9ff53a88":"types = train.dtypes\n\ncategorical_columns = []\ncategorical_dims =  {}\nfor col in train.columns:\n    if types[col] == 'object':\n        lbl = LabelEncoder()\n        lbl.fit(list(train[col].values) + list(test[col].values))\n        \n        train[col] = lbl.transform(train[col].values)\n        test[col] = lbl.transform(test[col].values)\n\n        categorical_columns.append(col)\n        categorical_dims[col] = len(lbl.classes_)        ","20ca6785":"features = [ col for col in train.columns] \n\ncat_idxs = [ i for i, f in enumerate(features) if f in categorical_columns]\n\ncat_dims = [ categorical_dims[f] for i, f in enumerate(features) if f in categorical_columns]","454c67ca":"train.head()","798018cc":"test.head()","ed1baa74":"columns = test.columns","3d8458cc":"tabnet_params = dict(n_d=N_D, n_a=N_A, n_steps=N_STEPS, gamma=GAMMA,\n                    lambda_sparse=LAMBDA_SPARSE, \n                    optimizer_fn=torch.optim.Adam,\n                    optimizer_params=dict(lr=OPT_LR, weight_decay=OPT_WEIGHT_DECAY),#, \\momentum=OPT_MOMENTUM),\n                    mask_type=MASK_TYPE,\n                     \n                    scheduler_fn=torch.optim.lr_scheduler.OneCycleLR,\n                    scheduler_params={\n                                    \"is_batch_level\": True,\n                                    \"max_lr\": 5e-2,\n                                    \"steps_per_epoch\":int(train.shape[0] \/ BATCH_SIZE) + 1,\n                                    \"epochs\": MAX_EPOCH\n                    },\n                     \n                    #scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n                    #scheduler_params=dict(mode=\"min\",\n                    #                   patience=200,\n                    #                   min_lr=SCHEDULER_MIN_LR,\n                    #                   factor=SCHEDULER_FACTOR,),\n                    verbose=10,\n                    device_name=DEVICE_NAME,\n                    seed=SEED\n                    )","fa515ff6":"# train = train[0:3000]\n# test = test[0:3000]\n# target = target[0:3000]","5673d41c":"print(train.shape)","14720244":"print(test.shape)","b2469fae":"print(target.shape)","667a1d6f":"#     TabNetPretrainer\nunsupervised_model = TabNetPretrainer(\n    optimizer_fn=torch.optim.Adam,\n    optimizer_params=dict(lr=2e-2),\n    mask_type='entmax', \n    device_name=DEVICE_NAME,\n)\n\n\nunsupervised_model.fit(\n    X_train=np.asarray(train.values.tolist() + test.values.tolist()),\n    eval_set=[np.asarray(train.values.tolist() + test.values.tolist())],\n    patience=100,\n    max_epochs=500, \n    batch_size=2048, virtual_batch_size=128,\n    num_workers=0,\n    drop_last=False,\n    pretraining_ratio=0.9,\n)    ","eac94390":"train_oof = np.zeros((len(train)))\ntest_preds = 0\n\nkf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=SEED)\n\nfor f, (train_ind, val_ind) in tqdm(enumerate(kf.split(train, target))):\n\n    print(f'Fold {f}')\n    train_df, val_df = train.iloc[train_ind][columns], train.iloc[val_ind][columns]\n\n    train_target, val_target = target[train_ind], target[val_ind]\n\n    train_target = train_target.reshape(-1,1)\n    val_target = val_target.reshape(-1,1)\n\n    train_df      = train_df.to_numpy()\n    train_target = np.squeeze(train_target)\n    \n    val_df = val_df.to_numpy()\n    val_target = np.squeeze(val_target)\n    \n\n    model = TabNetClassifier(**tabnet_params)\n\n    model.fit(X_train=train_df,\n          y_train=train_target,\n          eval_set=[(val_df, val_target)],\n          eval_name = [\"val\"],\n          eval_metric = ['auc'],\n          max_epochs=MAX_EPOCH,\n          patience=20, \n          batch_size=BATCH_SIZE, virtual_batch_size=128,\n          num_workers=1, \n          drop_last=False,\n          from_unsupervised=unsupervised_model    \n         )#,\n    \n        #         X_train=X_train, y_train=y_train,\n        #         eval_set=[(X_train, y_train), (X_valid, y_valid)],\n        #         eval_name=['train', 'valid'],\n        #         eval_metric=['auc'],\n        #         max_epochs=max_epochs , patience=20,\n        #         batch_size=1024, virtual_batch_size=128,\n        #         num_workers=0,\n        #         weights=1,\n        #         drop_last=False,\n        #         from_unsupervised=loaded_pretrain\n    \n    temp_oof = model.predict_proba(val_df)[:,1]\n    train_oof[val_ind] = temp_oof.reshape(-1)\n    temp_test = model.predict_proba(test.to_numpy())[:, 1]\n\n    test_preds += temp_test\/NUM_FOLDS     \n    \n    print(f'Fold {f}: {roc_auc_score(val_target, temp_oof)}')        ","119fed55":"submission['target'] = test_preds\nsubmission.to_csv(SUBMISSION_OUTPUT)","2e16ba87":"\n## TabNet Parameters","36396735":"## Run Kfold with TabNet Classifier","9eb17e9d":"\n## Parameters\n","005868de":"Hello Kagglers, in this Notebook we will try to use TabNet Classifier to solve TPS Playground problem! I added some credits in the final of this notebook :D\n\n* This notebook is an update version of my previous work (TabNet Regression) for TPS 02-21\n\n> For this competition, you will be predicting a binary target based on a number of feature columns given in the data. All of the feature columns, cat0 - cat18 are categorical, and the feature columns cont0 - cont10 are continuous.","dc1bbab1":"\n## Install PyTorch TabNet","ce039707":"## Label Encoding\n","aaa8eae7":"\n## Imports Libs","652a0428":"\n## Submit your output csv","08e6707a":"## Unsupervised Pre-training","6deffc93":"\n## Create TabNet Params Dictionary","e758a118":"\n## Import Data","ccb22f1b":"## Credits\n\n* [TabNet Paper](https:\/\/arxiv.org\/pdf\/1908.07442.pdf)\n* [TabNet PyTorch GitHub](https:\/\/github.com\/dreamquark-ai\/tabnet)\n* [Kaggle Notebook TabNet Regressor](https:\/\/www.kaggle.com\/optimo\/tabnetregressor-2-0-train-infer?scriptVersionId=44853427)\n* [Tunguz CV Notebook](https:\/\/www.kaggle.com\/tunguz\/tps-02-21-feature-importance-with-xgboost-and-shap)\n* [Rank Gauss](https:\/\/www.kaggle.com\/tottenham\/10-fold-simple-dnn-with-rank-gauss)\n\nIf it was useful for you please comment! Your feedback is really important","7d71a732":"\n## TabNet: Attentive Interpretable Tabular Learning\n\n\"We propose a novel high-performance and interpretable canonical deep tabular data learning architecture, TabNet. TabNet uses sequential attention to choose which features to reason from at each decision step, enabling interpretability and more efficient learning as the learning capacity is used for the most salient features. We demonstrate that TabNet outperforms other neural network and decision tree variants on a wide range of non-performance-saturated tabular datasets and yields interpretable feature attributions plus insights into the global model behavior. Finally, for the first time to our knowledge, we demonstrate self-supervised learning for tabular data, significantly improving performance with unsupervised representation learning when unlabeled data is abundant.\"\n\n[TabNet Paper](https:\/\/arxiv.org\/abs\/1908.07442)\n","32b324af":"# <p style=\"background-color:#33ff99; font-family:newtimeroman; font-size:170%; text-align:center\">[TPS 03-21] TabNet Classifier \ud83d\udd25\ud83d\udd25 <\/p>"}}