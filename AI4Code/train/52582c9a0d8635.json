{"cell_type":{"1c1e5bf7":"code","8cf27cd9":"code","3d1136a3":"code","5ec77dba":"code","06ec9f29":"code","cf814296":"code","45f7af29":"code","f77ff6be":"code","476cb5e2":"code","aebe81ad":"code","896811d0":"markdown","9c062933":"markdown","779352af":"markdown","b44f903a":"markdown","9dbd2a5c":"markdown","246c3592":"markdown","925196b6":"markdown"},"source":{"1c1e5bf7":"import tensorflow as tf\nimport tensorflow_addons as tfa\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'svg'","8cf27cd9":"def plot_scheduler(step, schedulers):\n    if not isinstance(schedulers, list):\n        schedulers = [schedulers]\n\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=[10, 3])\n    for scheduler in schedulers:\n        ax1.plot(range(step), scheduler(range(step)), label=scheduler.name)\n        ax1.set_xlabel('Epoch')\n        ax1.set_ylabel('Learning Rate')\n        ax1.legend()\n\n        ax2.plot(range(step), scheduler(range(step)), label=scheduler.name)\n        ax2.set_xlabel('Epoch')\n        ax2.set_ylabel('Learning Rate')\n        ax2.set_yscale('log')\n        ax2.legend()\n    plt.show()","3d1136a3":"init_lr = 1e-2\ndecay_steps = 50\nalpha = 1e-2\ncos_dec1 = tf.keras.experimental.CosineDecay(init_lr, decay_steps, alpha=alpha, name='Cosine Decay 1')\n\ninit_lr = 1e-2\ndecay_steps = 50\nalpha = 0\ncos_dec2 = tf.keras.experimental.CosineDecay(init_lr, decay_steps, alpha=alpha, name='Cosine Decay 2')\n\ninit_lr = 1e-3\ndecay_steps = 50\nalpha = 1e-2\ncos_dec3 = tf.keras.experimental.CosineDecay(init_lr, decay_steps, alpha=alpha, name='Cosine Decay 3')\n\nplot_scheduler(60, [cos_dec1, cos_dec2, cos_dec3])","5ec77dba":"init_lr = 1e-2\ndecay_steps = 50\nalpha = 0\nbeta = 1e-3\nnum_periods=1\nlin_cos_dec1 = tf.keras.experimental.LinearCosineDecay(init_lr,\n                                                       decay_steps,\n                                                       num_periods=num_periods, alpha=alpha,\n                                                       beta=beta, name='LinCosDec 1')\n\ninit_lr = 1e-3\ndecay_steps = 50\nalpha = 0\nbeta = 1e-3\nnum_periods=2\nlin_cos_dec2 = tf.keras.experimental.LinearCosineDecay(init_lr,\n                                                       decay_steps,\n                                                       num_periods=num_periods, alpha=alpha,\n                                                       beta=beta, name='LinCosDec 2')\n\ninit_lr = 4e-3\ndecay_steps = 50\nalpha = 1e-5\nbeta = 1e-8\nnum_periods=4\nlin_cos_dec3 = tf.keras.experimental.LinearCosineDecay(init_lr,\n                                                       decay_steps,\n                                                       num_periods=num_periods, alpha=alpha,\n                                                       beta=beta, name='LinCosDec 3')\n\nplot_scheduler(100, [lin_cos_dec1, lin_cos_dec2, lin_cos_dec3])","06ec9f29":"init_lr = 1e-2\ndecay_steps = 50\n\nnoisy_lin_cos_dec = tf.keras.experimental.NoisyLinearCosineDecay(\n    init_lr, decay_steps, initial_variance=0.002, variance_decay=0.001,\n    num_periods=2, alpha=0.0, beta=0.01, name=None)\n\nplot_scheduler(100, noisy_lin_cos_dec)","cf814296":"clr = tfa.optimizers.CyclicalLearningRate(1e-3, 1e-2,\n    step_size=25,\n    scale_fn=lambda x: 1,\n    scale_mode= 'cycle',\n    name= 'CyclicalLearningRate'\n)\n\nplt.plot(range(100), clr(range(100)).numpy())\nplt.xlabel('Epoch')\nplt.ylabel('Learning Rate')\nplt.show()","45f7af29":"def plot_scheduler2(step, schedulers):\n    if not isinstance(schedulers, list):\n        schedulers = [schedulers]\n\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=[10, 3])\n    for scheduler in schedulers:\n        x = range(step)\n        y = [scheduler(i).numpy() for i in x]\n        ax1.plot(x, y, label=scheduler.name)\n        ax1.set_xlabel('Epoch')\n        ax1.set_ylabel('Learning Rate')\n        ax1.legend()\n\n        ax2.plot(x, y, label=scheduler.name)\n        ax2.set_xlabel('Epoch')\n        ax2.set_ylabel('Learning Rate')\n        ax2.set_yscale('log')\n        ax2.legend()\n    plt.show()","f77ff6be":"exp_clr = tfa.optimizers.ExponentialCyclicalLearningRate(\n    initial_learning_rate=1e-3,\n    maximal_learning_rate=1e-2,\n    step_size=5,\n    scale_mode='iterations',\n    gamma=0.8,\n    name='ExponentialCyclicalLearningRate'\n)\n\nplot_scheduler2(100, exp_clr)","476cb5e2":"def plot_scheduler3(step, schedulers):\n    if not isinstance(schedulers, list):\n        schedulers = [schedulers]\n\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=[10, 3])\n    for scheduler in schedulers:\n        x = range(step)\n        y = [scheduler(i) for i in x]\n        ax1.plot(x, y)\n        ax1.set_xlabel('Epoch')\n        ax1.set_ylabel('Learning Rate')\n\n        ax2.plot(x, y)\n        ax2.set_xlabel('Epoch')\n        ax2.set_ylabel('Learning Rate')\n        ax2.set_yscale('log')\n    plt.show()","aebe81ad":"# This is directly copied from a notebook of Chris Deotte.\n\nLR_START = 1e-5\nLR_MAX = 1e-2\nLR_RAMPUP_EPOCHS = 2\nLR_SUSTAIN_EPOCHS = 1\nLR_STEP_DECAY = 0.75\n\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) \/ LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = LR_MAX * LR_STEP_DECAY**((epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS)\/\/2)\n    return lr\n\nplot_scheduler3(100, lrfn)","896811d0":"## Noisy Linear Cosine Decay","9c062933":"## Cosine Decay\n\nScheduler = $\\dfrac{1}{2} \\left(1- \\alpha\\right)  \\left[1 + cos\\left(\\frac{\\pi Step}{Decay Steps}\\right)\\right] + \\alpha$","779352af":"## Linear Cosine Decay","b44f903a":"## CyclicalLearningRate","9dbd2a5c":"# TF\/Keras Learning Rate & Schedulers\n\nNeural networks are optimized with an algorithm called back-propagation. The model parameters start from some random initial guesses, the data is feed forward, and the error of the outcome is propagated backwards in the model as a function of a parameter called the **learning rate**.\n\nThe learning rate is an important hyper-parameter for model training. For instance, if it is too low, the training of the model can take too long; if it is too large, the model may not converge. Furthermore, the model can converge to a sub-optimal solution (a local minimum).\n\nLearning rate schedulers provide a dynamic solution to these issues. They are just some functions of a training epoch (or step) that multiply the learning rate. Here, I will plot the readily available learning rate schedulers in Tensorflow\/Keras as well as one popular scheduler that is probably not available in Tensorflow\/Keras. Seeing these functions is extremely helpful for training. Most of the plots are shown in both linear and logarithmic scales. \n\nInterested readers may benefit from playing with the code by changing the parameters of schedulers to see how they affect the learning rate.","246c3592":"## Exponential Cyclical Learning Rate","925196b6":"## Custom Learning Scheduler"}}