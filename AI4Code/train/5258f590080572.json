{"cell_type":{"a5479935":"code","2770b114":"code","83458ab5":"code","11903c5f":"code","d4464aa6":"code","7846a525":"code","31cb95ad":"code","46eeba2e":"code","6afe7d72":"code","c6d08766":"code","c8de1257":"code","299bda26":"code","861aee0a":"code","e8185c5d":"code","fc867241":"code","1f9cd5e3":"code","d41a5184":"code","8bab2752":"code","351c277c":"code","db02fbe8":"code","5bb00860":"code","74d0b113":"code","69271255":"code","da66a3a9":"code","9ca9cfe9":"code","c047d290":"code","6a7fd5a7":"code","f4e45e03":"code","785b1895":"code","7ba5e7e8":"code","39d29563":"code","d05ab852":"code","25ad17ae":"code","418c9638":"code","6c67ca98":"code","a2402245":"code","ffdab8d8":"code","ad8c2314":"code","45336fab":"code","72472f8a":"code","896ca79e":"code","0e98a56d":"code","8393ad97":"code","297bcda4":"code","4ef5e826":"code","9c634897":"code","97bd5188":"code","5ddb798d":"code","736b4fa5":"code","4f3f5afe":"code","36e5fcdf":"code","50cc1970":"code","3e318e9c":"code","230e31b4":"code","8ada9f1f":"code","7d4f3a8a":"code","60fac3f7":"code","8a6a4a52":"code","2f65580b":"code","b91fbc05":"code","d83515b4":"code","7e95bb36":"code","d741a02f":"code","30153120":"code","f65eac1f":"code","040e410a":"code","df590b1d":"code","3b291fec":"code","3d0ec06d":"code","3a84974d":"code","35b2ef46":"code","da920aa0":"code","0465abd6":"code","d039ad38":"code","ec089f0c":"code","54b7d4da":"code","977a585e":"code","1673c657":"code","d6a2e909":"code","168753c9":"code","05b074da":"code","808e1b9e":"code","87138788":"code","78c3e284":"code","b936eff6":"code","a4bf2dd2":"code","9c2481bd":"code","c8a0935c":"code","2b789792":"code","d234e0d1":"code","9740a70a":"code","ba5abc19":"code","60a72ef8":"code","ea6204f6":"code","0d827318":"code","974a6a9c":"code","b1035b9f":"code","1875a15a":"code","1dfb2995":"code","cc4c1fc5":"code","4df64bfb":"code","7b0e0275":"code","a7b94488":"code","04fda033":"code","7513e627":"code","581d6703":"code","21ecf377":"code","70eabb4d":"code","f6d999f4":"code","422bb9f7":"code","168862b1":"code","b0bbd53b":"code","93f79b8c":"code","354ab3ba":"code","665557a3":"markdown","59054013":"markdown","79a96a9b":"markdown","7594f609":"markdown","0765f5de":"markdown","488b4ba6":"markdown","87b421f5":"markdown","35390d00":"markdown","5e0b29e1":"markdown","2a3f1083":"markdown","f8614575":"markdown","428abc4d":"markdown","a287af29":"markdown","59c6397c":"markdown","d7f0ce26":"markdown","4f55689e":"markdown","47cf21c3":"markdown","6d8105e9":"markdown","7ffd5f69":"markdown","913ab949":"markdown","4d6e0b69":"markdown","0c956a9c":"markdown","76304279":"markdown","47cf50c4":"markdown","ba76e715":"markdown","f8b507f2":"markdown","5815db02":"markdown","6ece65d1":"markdown","92ec8687":"markdown","e88bf832":"markdown","c3711e3a":"markdown","de1b5c85":"markdown","293119f7":"markdown","db0d5c6c":"markdown","bc0d3172":"markdown","bf3f4e1b":"markdown","f71bb0b9":"markdown","0ef7eee7":"markdown","bc94073e":"markdown","259758d6":"markdown","975ef323":"markdown","c7238458":"markdown","d554dfd9":"markdown"},"source":{"a5479935":"from IPython.display import Image\nImage(url= \"https:\/\/static1.squarespace.com\/static\/5006453fe4b09ef2252ba068\/5095eabce4b06cb305058603\/5095eabce4b02d37bef4c24c\/1352002236895\/100_anniversary_titanic_sinking_by_esai8mellows-d4xbme8.jpg\")","2770b114":"import pandas as pd\nimport pandas_profiling # library for automatic EDA\n%pip install autoviz # installing and importing autoviz, another library for automatic data visualization\nfrom autoviz.AutoViz_Class import AutoViz_Class\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\n# data visualization\nimport seaborn as sns\n\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import linear_model","83458ab5":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")","11903c5f":"print(\"Shape train data :\",train.shape)\nprint(\"Shape test data :\",test.shape)","d4464aa6":"def concat_df(train_data,test_data):\n    return pd.concat([train_data,test_data],sort=True).reset_index(drop=True)","7846a525":"df_all = concat_df(train,test)","31cb95ad":"df_all.head()","46eeba2e":"df_all.shape","6afe7d72":"list(df_all.columns)","c6d08766":"list(train.columns)","c8de1257":"list(test.columns)","299bda26":"# The pandas profiling library is really useful on helping us understand the data we're working on.\n# It saves us some precious time on the EDA process.\nreport = pandas_profiling.ProfileReport(train)","861aee0a":"# Let's now visualize the report generated by pandas_profiling.\ndisplay(report)","e8185c5d":"# Also, there is an option to generate an .HTML file containing all the information generated by the report.\n# report.to_file(output_file='report.html')","fc867241":"# Another great library for automatic EDA is AutoViz.\n# With this library, several plots are generated with only 1 line of code.\n# When combined with pandas_profiling, we obtain lots of information in a\n# matter of seconds, using less then 5 lines of code.\nAV = AutoViz_Class()\n\n# Let's now visualize the plots generated by AutoViz.\nreport_2 = AV.AutoViz(\"..\/input\/titanic\/train.csv\")","1f9cd5e3":"print(\"Missings in the train data : \")\ndisplay(train.isnull().sum())\nprint(\"=============================\")\nprint(\"Missings in the test data : \")\ndisplay(test.isnull().sum())","d41a5184":"train.Cabin.unique()","8bab2752":"def process_age(df,cut_points,label_names):\n    df[\"Age\"] = df[\"Age\"].fillna(-0.5)\n    df[\"Age_categories\"] = pd.cut(df[\"Age\"],cut_points,labels=label_names)\n    return df\n\ncut_points = [-1,0,18,100]\nlabel_names = [\"Missing\",\"Child\",\"Adult\"]\n\ntrain = process_age(train,cut_points,label_names)\ntest = process_age(test,cut_points,label_names)","351c277c":"def process_age(df,cut_points,label_names):\n    df[\"Age\"] = df[\"Age\"].fillna(-0.5)\n    df[\"Age_categories\"] = pd.cut(df[\"Age\"],cut_points,labels=label_names)\n    return df\n\ncut_points = [-1,0, 5, 12, 18, 35, 60, 100]\nlabel_names = [\"Missing\", 'Infant', \"Child\", 'Teenager', \"Young Adult\", 'Adult', 'Senior']\n\ntrain = process_age(train,cut_points,label_names)\ntest = process_age(test,cut_points,label_names)\n\nage_cat_pivot = train.pivot_table(index=\"Age_categories\",values=\"Survived\")\nage_cat_pivot.plot.bar()\nplt.show()","db02fbe8":"list(train.columns)","5bb00860":"train.Pclass.unique()","74d0b113":"train.Pclass.value_counts()","69271255":"train.Pclass.value_counts().plot(kind='bar',color='red')","da66a3a9":"train.Sex.value_counts()","9ca9cfe9":"train.Sex.value_counts().plot(kind='bar',color='green')","c047d290":"column_name = \"Pclass\"\ndf = train\ndummies = pd.get_dummies(df[column_name],prefix=column_name,drop_first=True)\ndummies.head()","6a7fd5a7":"def create_dummies(df,column_name):\n    dummies = pd.get_dummies(df[column_name],prefix=column_name)\n    df = pd.concat([df,dummies],axis=1)\n    return df\n\ntrain = create_dummies(train,\"Pclass\")\ntest = create_dummies(test,\"Pclass\")\ntrain.head()","f4e45e03":"train = create_dummies(train,\"Sex\")\ntest = create_dummies(test,\"Sex\")\ntrain = create_dummies(train,\"Age_categories\")\ntest = create_dummies(test,\"Age_categories\")","785b1895":"train_copy = train.copy()\ntest_copy = test.copy()","7ba5e7e8":"train = train.drop(['Sex','Age','Age_categories'], axis=1)\ntest = test.drop(['Sex','Age','Age_categories'], axis=1)","39d29563":"train.head()\n","d05ab852":"#from sklearn.linear_model import LogisticRegression\n#lr = LogisticRegression()\n","25ad17ae":"list(train.columns)","418c9638":"columns1 = ['Pclass_1', 'Pclass_2', 'Pclass_3', 'Sex_female', 'Sex_male',\n       'Age_categories_Missing','Age_categories_Infant',\n       'Age_categories_Child', 'Age_categories_Teenager',\n       'Age_categories_Young Adult', 'Age_categories_Adult',\n       'Age_categories_Senior']","6c67ca98":"#lr.fit(train[columns], train['Survived'])\n","a2402245":"train['Survived']","ffdab8d8":"#from sklearn.model_selection import train_test_split\n\n#all_X = train[columns]\n#all_y = train['Survived']\n\n#train_X, test_X, train_y, test_y = train_test_split(\n#    all_X, all_y, test_size=0.2,random_state=0)","ad8c2314":"#holdout = test # from now on we will refer to this\n               # dataframe as the holdout data","45336fab":"#train_X.shape\n","72472f8a":"#lr = LogisticRegression()\n#lr.fit(train_X, train_y)\n#predictions = lr.predict(test_X)","896ca79e":"#lr = LogisticRegression()\n#lr.fit(train_X, train_y)\n#predictions = lr.predict(test_X)\n#accuracy = accuracy_score(test_y, predictions)\n#accuracy","0e98a56d":"#from sklearn.metrics import confusion_matrix\n\n#conf_matrix = confusion_matrix(test_y, predictions)\n#pd.DataFrame(conf_matrix, columns=['Survived', 'Died'], index=[['Survived', 'Died']])","8393ad97":"#holdout.head()\n","297bcda4":"#Making Predictions on Unseen Data","4ef5e826":"#lr = LogisticRegression()\n#lr.fit(all_X, all_y)\n#holdout_predictions = lr.predict(holdout[columns])\n#holdout_predictions","9c634897":"#holdout_ids = holdout[\"PassengerId\"]\n#submission_df = {\"PassengerId\": holdout_ids,\n#                 \"Survived\": holdout_predictions}\n#submission = pd.DataFrame(submission_df)","97bd5188":"#holdout_ids = holdout[\"PassengerId\"]\n#submission_df = {\"PassengerId\": holdout_ids,\n#                 \"Survived\": holdout_predictions}\n#submission = pd.DataFrame(submission_df)\n\n#submission.to_csv('titanic_submission.csv', index=False)","5ddb798d":"from sklearn.ensemble import RandomForestClassifier\n#rf = RandomForestClassifier()\n#rf.fit(train_X, train_y)\n#predictions = rf.predict(test_X)","736b4fa5":"#predictions_rf = rf.predict(test_X)\n#accuracy_rf = accuracy_score(test_y, predictions_rf)\n#accuracy_rf","4f3f5afe":"train.isnull().sum()","36e5fcdf":"test.isnull().sum()","50cc1970":"test.loc[test['Fare'].isnull()]","3e318e9c":"mr_thomas = test.loc[(test['Pclass']==3) & (test['SibSp']==0) & (test['Embarked'] =='S')]['Fare'].median()","230e31b4":"test.loc[test['Fare'].isnull(),'Fare'] = mr_thomas","8ada9f1f":"test.isnull().sum()","7d4f3a8a":"target = train['Survived']","60fac3f7":"train.head()","8a6a4a52":"test.head()","2f65580b":"columns2 = ['Pclass_1', 'Pclass_2', 'Pclass_3', 'Sex_female', 'Sex_male',\n       'Age_categories_Missing','Age_categories_Infant',\n       'Age_categories_Child', 'Age_categories_Teenager',\n       'Age_categories_Young Adult', 'Age_categories_Adult',\n       'Age_categories_Senior','Embarked_C','Embarked_Q','Embarked_S','Fare']","b91fbc05":"columns3 = ['Pclass_1', 'Pclass_2', 'Pclass_3', 'Sex_female', 'Sex_male',\n       'Age_categories_Missing','Age_categories_Infant',\n       'Age_categories_Child', 'Age_categories_Teenager',\n       'Age_categories_Young Adult', 'Age_categories_Adult',\n       'Age_categories_Senior','Fare']","d83515b4":"from sklearn.model_selection import train_test_split\n\nall_X = train[columns1]\nall_y = train['Survived']\n\ntrain_X, test_X, train_y, test_y = train_test_split(\n    all_X, all_y, test_size=0.2,random_state=0)","7e95bb36":"train_X.shape","d741a02f":"train_y.shape","30153120":"test_y.shape","f65eac1f":"test_X.shape","040e410a":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\nrf.fit(train_X, train_y)\npredictions = rf.predict(test_X)","df590b1d":"predictions_rf2 = rf.predict(test_X)\naccuracy_rf2 = accuracy_score(test_y, predictions_rf2)\naccuracy_rf2","3b291fec":"from sklearn.metrics import confusion_matrix","3d0ec06d":"from sklearn.metrics import accuracy_score","3a84974d":"\naccuracy = accuracy_score(test_y, predictions_rf2)\naccuracy","35b2ef46":"conf_matrix = confusion_matrix(test_y, predictions_rf2)\npd.DataFrame(conf_matrix, columns=['Survived', 'Died'], index=[['Survived', 'Died']])","da920aa0":"train.isnull().sum()","0465abd6":"train.loc[train['Embarked'].isnull()]","d039ad38":"#check for passengers wo were in passenger class 1, on deck abc and paid 80 or less for the tickets\ntrain.loc[(train['Pclass']==1) & (train['Fare']<=80)]['Embarked'].value_counts()   ","ec089f0c":"train.loc[train['Embarked'].isnull(),'Embarked'] ='S'","54b7d4da":"from sklearn.preprocessing import OneHotEncoder ","977a585e":"train.Embarked.unique()","1673c657":"a = pd.get_dummies(train.Embarked, prefix='Embarked')\n","d6a2e909":"a.head()","168753c9":"b = pd.get_dummies(test.Embarked, prefix='Embarked')\n","05b074da":"target = train['Survived']","808e1b9e":"train.Embarked.shape","87138788":"test.Embarked.shape","78c3e284":"frames = [train, a]\nframes_1 = [test, b]\n","b936eff6":"train = pd.concat(frames,axis=1)\ntest = pd.concat(frames_1,axis=1)","a4bf2dd2":"holdout = test # from now on we will refer to this\n               # dataframe as the holdout data","9c2481bd":"train.Embarked_C.shape","c8a0935c":"train.shape","2b789792":"test.shape","d234e0d1":"test.Embarked.shape","9740a70a":"from sklearn.model_selection import train_test_split\n\nall_X = train[columns2]\nall_y = target\n\ntrain_X, test_X, train_y, test_y = train_test_split(\n    all_X, all_y, test_size=0.2,random_state=0)","ba5abc19":"\nfrom xgboost import XGBClassifier","60a72ef8":"\n# fit model no training data\nmodel = XGBClassifier()\nmodel.fit(train_X, train_y)","ea6204f6":"predictions_xgb = model.predict(test_X)\naccuracy_xgb = accuracy_score(test_y, predictions_xgb)\naccuracy_xgb","0d827318":"conf_matrix3 = confusion_matrix(test_y, predictions_xgb)\npd.DataFrame(conf_matrix, columns=['Survived', 'Died'], index=[['Survived', 'Died']])","974a6a9c":"holdout_predictions = model.predict(holdout[columns2])\nholdout_predictions","b1035b9f":"holdout_ids = holdout[\"PassengerId\"]\nsubmission_df = {\"PassengerId\": holdout_ids,\n                 \"Survived\": holdout_predictions}\nsubmission = pd.DataFrame(submission_df)\n\nsubmission.to_csv('titanic_submission2.csv', index=False)","1875a15a":"data = [train, test]\nfor dataset in data:\n    dataset['relatives'] = dataset['SibSp'] + dataset['Parch']\n    dataset.loc[dataset['relatives'] > 0, 'not_alone'] = 0\n    dataset.loc[dataset['relatives'] == 0, 'not_alone'] = 1\n    dataset['not_alone'] = dataset['not_alone'].astype(int)\ntrain['not_alone'].value_counts()","1dfb2995":"axes = sns.factorplot('relatives','Survived', \n                      data=train, aspect = 2.5, )","cc4c1fc5":"X_train = train[columns2]\nY_train = train[\"Survived\"]\nX_test  = test.drop(\"PassengerId\", axis=1).copy()","4df64bfb":"sgd = linear_model.SGDClassifier(max_iter=5, tol=None)\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(test_X)\n\nsgd.score(train_X, train_y)\n\nacc_sgd = round(sgd.score(train_X, train_y) * 100, 2)","7b0e0275":"random_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(train_X, train_y)\n\nY_prediction_rf = random_forest.predict(test_X)\n\nrandom_forest.score(train_X, train_y)\nacc_random_forest = round(random_forest.score(train_X, train_y) * 100, 2)\n","a7b94488":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(train_X, train_y)\nY_pred = knn.predict(test_X)\nacc_knn = round(knn.score(train_X, train_y) * 100, 2)","04fda033":"logreg = LogisticRegression()\nlogreg.fit(train_X, train_y)\n\nY_pred = logreg.predict(test_X)\n\nacc_log = round(logreg.score(train_X, train_y) * 100, 2)","7513e627":"gaussian = GaussianNB()\ngaussian.fit(train_X, train_y)\nY_pred = gaussian.predict(test_X)\nacc_gaussian = round(gaussian.score(train_X, train_y) * 100, 2)","581d6703":"perceptron = Perceptron(max_iter=5)\nperceptron.fit(train_X, train_y)\n\nY_pred = perceptron.predict(test_X)\n\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)","21ecf377":"linear_svc = LinearSVC()\nlinear_svc.fit(train_X, train_y)\n\nY_pred = linear_svc.predict(test_X)\n\nacc_linear_svc = round(linear_svc.score(train_X, train_y) * 100, 2)","70eabb4d":"decision_tree = DecisionTreeClassifier()\ndecision_tree.fit(train_X, train_y)\nY_pred = decision_tree.predict(test_X)\nacc_decision_tree = round(decision_tree.score(train_X, train_y) * 100, 2)","f6d999f4":"results = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', \n              'Decision Tree'],\n    'Score': [acc_linear_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_decision_tree]})\nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Score')\nresult_df.head(9)","422bb9f7":"from sklearn.model_selection import cross_val_score\nrf = RandomForestClassifier(n_estimators=100)\nscores = cross_val_score(rf, train_X, train_y, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","168862b1":"importances = pd.DataFrame({'feature':train_X.columns,'importance':np.round(random_forest.feature_importances_,3)})\nimportances = importances.sort_values('importance',ascending=False).set_index('feature')\nimportances.head(15)","b0bbd53b":"importances.plot.bar()","93f79b8c":"\"\"\"holdout_predictions = random_forest.predict(holdout[columns2])\nholdout_predictions\"\"\"","354ab3ba":"\"\"\" holdout_ids = holdout[\"PassengerId\"]\nsubmission_df = {\"PassengerId\": holdout_ids,\n                 \"Survived\": holdout_predictions}\nsubmission = pd.DataFrame(submission_df)\n\nsubmission.to_csv('titanic_submission_3.csv', index=False)\"\"\"","665557a3":"#### Logistic Regression:","59054013":"**Pclass**","79a96a9b":"To give us a better understanding of the real performance of our model, we can use a technique called cross validation to train and test our model on different splits of our data, and then average the accuracy scores.","7594f609":"The result of our K-Fold Cross Validation example would be an array that contains 4 different scores. We then need to compute the mean and the standard deviation for these scores.\nThe code below perform K-Fold Cross Validation on our random forest model, using 10 folds (K = 10). Therefore it outputs an array with 10 different scores.","0765f5de":"## Exploratory data analysis","488b4ba6":"#### Linear Support Vector Machine:","87b421f5":"name and passagerId are not necessary. Lets's create a model now without :\n- Sibsp\n- Parch\n- Ticket\n- Fare\n- Cabin\n- Embarked","35390d00":"### Which is the best Model ?","5e0b29e1":"Create the cut_points and label_names lists to split the Age column into six categories:\n\n- Missing, from -1 to 0\n- Infant, from 0 to 5\n- Teenager, from 12 to 18\n- nager, from 12 to 18\n- Young Adult, from 18 to 35\n- Adult, from 35 to 60\n- Senior from from 60 to 100","2a3f1083":"### Let's improve our model","f8614575":"##### Submission","428abc4d":"#### Info about our data\n- pclass: A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n- sibsp: The dataset defines family relations in this way\u2026\n- Sibling = brother, sister, stepbrother, stepsister\n- Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n- parch: The dataset defines family relations in this way\u2026\n- Parent = mother, father\n- Child = daughter, son, stepdaughter, stepson\nSome children travelled only with a nanny, therefore parch=0 for them.","a287af29":"This means in our case that the accuracy of our model can differ + \u2014 2%.\nI think the accuracy is still really good and since random forest is an easy to use model, we will try to increase it\u2019s performance even further in the following section.","59c6397c":"Using the power of both automatic EDA libraries listed above, we can observe each variable's behaviour individually, with plots that goes from Histograms to Boxplots, Correlation Matrix and much more. It speeds up time and minimizes the effort spent on the initial process of our work.\n\nWe can gather some really useful information from both reports. Let's now point some of them out:\n\n- Our classes are not that much disbalanced. We have ~38% of the passengers into class \"1\" (survived) and ~62% of the passengers into class \"0\" (didn't survive).\n- The \"Pclass\" column, that informs us about the passenger's ticket class, shows us that ~55% of them are on class 3, ~24% of them are on class 2 and ~21% on class 1.\n- Most of the passengers into this dataset are male: ~35% of the passengers are female, and ~65% are male.\n- Almost 20% of the values in the \"Age\" column are missing. We can fill out these nulls with various techniques, such as filling them with the distribution's mean. The ages distribution is a little bit skewed, with it's mean being around 30 years old, and it's standard deviation being close to 15. The oldest passenger we have in this dataset is 80 years old.\n- According to the \"SibSP\" column, most of the passengers (~68%) didn't have any spouses or siblings aboard the ship. That is also applied when we check out the \"Parch\" column.\n- The distribution of Fares is much more skewed. It's mean value is around 32, with it's standard deviation being close to 50. It's minimum value is 0, and it's maximum value is 512.3292. That means that we're going to have to deal with this column carefully if we plan to use models such as SVMs.\nWhen ckecking the \"Embarked\" column, it shows us that 72.3% of the passengers embarked at Southampton port, 18.9% of the passengers at Cherbourg port and 8.6% of the passengers at Queenstown port.\n- \"Fare\" values are higher for passengers with \"Pclass\" = 1, lower for passengers with \"Pclass\" = 2 and even lower for passengers with \"Pclass\" = 3. Logically, it looks like the classification of \"Pclass\" is defined by the value of the passenger's fare.","d7f0ce26":"## Building Machine Learning Models","4f55689e":"We handled the missing values of Fare","47cf21c3":"#### Stochastic Gradient Descent (SGD):","6d8105e9":"\nFinally, we'll use the DataFrame.to_csv() method to save the dataframe to a CSV file. We need to make sure the index parameter is set to False, otherwise we will add an extra column to our CSV.","7ffd5f69":"### K-Fold Cross Validation:\nK-Fold Cross Validation randomly splits the training data into K subsets called folds. Let\u2019s image we would split our data into 4 folds (K = 4). Our random forest model would be trained and evaluated 4 times, using a different fold for evaluation everytime, while it would be trained on the remaining 3 folds.","913ab949":"**Age**","4d6e0b69":"This is the legendary Titanic ML competition \u2013 the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\n\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.","0c956a9c":"#### Random Forest:","76304279":"### Adding Now SibSp and Parch:","47cf50c4":"**Sexe**","ba76e715":"### Creating our first machine learning model","f8b507f2":"#### Perceptron:","5815db02":"SibSp and Parch would make more sense as a combined feature, that shows the total number of relatives, a person has on the Titanic. I will create it below and also a feature that sows if someone is not alone.","6ece65d1":"# Titanic: Machine Learning from Disaster","92ec8687":"Another great quality of random forest is that they make it very easy to measure the relative importance of each feature. Sklearn measure a features importance by looking at how much the treee nodes, that use that feature, reduce impurity on average (across all trees in the forest). It computes this score automaticall for each feature after training and scales the results so that the sum of all importances is equal to 1. We will acces this below:","e88bf832":"As we can see, the Random Forest classifier goes on the first place. But first, let us check, how random-forest performs, when we use cross validation.","c3711e3a":"Since the Ticket attribute has 681 unique tickets, it will be a bit tricky to convert them into useful categories. So we will drop it from the dataset.","de1b5c85":"**Import libraries**","293119f7":"**Embarked**","db0d5c6c":"### Data cleaning","bc0d3172":"### Let's analyse the different unique values","bf3f4e1b":"**Fare**","f71bb0b9":"#### Decision Tree","0ef7eee7":"#### K Nearest Neighbor:","bc94073e":"Here we can see that you had a high probabilty of survival with 1 to 3 realitves, but a lower one if you had less than 1 or more than 3 (except for some cases with 6 relatives).","259758d6":"We will try a new model with Fare handled and embarked encoded","975ef323":"Conclusion:\nage_cat_senionr and teenager doesn\u2019t play a significant role in our random forest classifiers prediction process. Because of that I will drop them from the dataset and train the classifier again. ","c7238458":"#### Gaussian Naive Bayes:","d554dfd9":"### Let's have a look at the feature importance"}}