{"cell_type":{"89be79df":"code","01d60390":"code","e1a294ee":"code","dc449bf4":"code","25b1b984":"code","70319229":"code","b4120011":"code","e608bde1":"code","95b869bf":"code","cee86f51":"code","7fe15601":"code","6fac5fde":"code","11516297":"code","b8adb59e":"code","210cf34b":"code","47068ecf":"code","53b025f9":"code","ca4122ec":"code","970df943":"code","b6c780a8":"code","595b43d6":"code","9ad1e58e":"code","ef94579b":"code","0c29274d":"code","27c9551b":"code","c16d0e26":"code","a1f19ec4":"code","a36f1eb2":"code","bba90a1a":"code","afdc9e6c":"code","eb6e3574":"code","eb899b0a":"code","57e77439":"code","c9d2ff21":"code","f6f2cc61":"code","cf3f5d01":"code","cc9c5887":"code","9b2479ee":"code","63ef4389":"code","e11683f0":"code","afb57f52":"markdown","b0ffeace":"markdown","b103f6b2":"markdown","00e0246d":"markdown","42470eb7":"markdown","cabf1fb6":"markdown","5a2b7b24":"markdown","96034186":"markdown","fc17a41c":"markdown","c6be42e8":"markdown","f885ae91":"markdown","09678b39":"markdown"},"source":{"89be79df":"# loading libraries - must have internet on\n\n# Overall tools\nimport numpy as np\nimport pandas as pd\nimport json\nimport glob\nfrom scipy.spatial.distance import cdist\n\n# Progress bar for the loops\nimport time\nimport sys\nimport tqdm\n\n# Text tools\nimport re, nltk, spacy, gensim\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\nnltk.download(\"punkt\")\nnltk.download(\"stopwords\")\n\n# Sklearn\nfrom sklearn.feature_extraction.text import HashingVectorizer # Vectorizor for the words in the abstract\nfrom sklearn.feature_extraction.text import TfidfVectorizer # Vectorizor for the text in the abstract (tf-idf)\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.cluster import KMeans\nfrom sklearn import metrics\n\n# Plotting tools\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n# Bokeh\nfrom bokeh.models import ColumnDataSource, HoverTool, LinearColorMapper, CustomJS\nfrom bokeh.palettes import Category20\nfrom bokeh.transform import linear_cmap\nfrom bokeh.io import output_file, show\nfrom bokeh.transform import transform\nfrom bokeh.io import output_notebook\nfrom bokeh.plotting import figure\nfrom bokeh.layouts import column\nfrom bokeh.models import RadioButtonGroup\nfrom bokeh.models import TextInput\nfrom bokeh.layouts import gridplot\nfrom bokeh.models import Div\nfrom bokeh.models import Paragraph\nfrom bokeh.layouts import column, widgetbox","01d60390":"#loading metadata file\n\nroot_path = '\/kaggle\/input\/CORD-19-research-challenge\/'\nmetadata_path = f'{root_path}\/metadata.csv'\nmeta_df = pd.read_csv(metadata_path, dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str\n})\nmeta_df.head()","e1a294ee":"# importing all json files\n\nall_json = glob.glob(f'{root_path}\/**\/*.json', recursive=True)\nlen(all_json)","dc449bf4":"# File Reader class\n\nclass FileReader:\n    def __init__(self, file_path):\n        with open(file_path) as file:\n            content = json.load(file)\n            self.paper_id = content['paper_id']\n            self.abstract = []\n            self.body_text = []\n            # Abstract\n            for entry in content['abstract']:\n                self.abstract.append(entry['text'])\n            # Body text\n            for entry in content['body_text']:\n                self.body_text.append(entry['text'])\n            self.abstract = '\\n'.join(self.abstract)\n            self.body_text = '\\n'.join(self.body_text)\n    def __repr__(self):\n        return f'{self.paper_id}: {self.abstract[:200]}... {self.body_text[:200]}...'","25b1b984":"# Checking if the File Reader Class worked\n\nprint(FileReader(all_json[0]))","70319229":"# Function to add break every length characters\n\ndef get_breaks(content, length):\n    data = \"\"\n    words = content.split(' ')\n    total_chars = 0\n\n    for i in range(len(words)):\n        total_chars += len(words[i])\n        if total_chars > length:\n            data = data + \"<br>\" + words[i]\n            total_chars = 0\n        else:\n            data = data + \" \" + words[i]\n    return data","b4120011":"# Input the research papers into a DataFrame\n\ndict_ = {'paper_id': [], 'abstract': [], 'body_text': [], 'authors': [], 'title': [], 'journal': [], 'abstract_summary': []}\nfor idx, entry in enumerate(all_json):\n    if idx % (len(all_json) \/\/ 10) == 0:\n        print(f'Processing index: {idx} of {len(all_json)}')\n    content = FileReader(entry)\n    \n    # get metadata information\n    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n    # no metadata, skip this paper\n    if len(meta_data) == 0:\n        continue\n    \n    dict_['paper_id'].append(content.paper_id)\n    dict_['abstract'].append(content.abstract)\n    dict_['body_text'].append(content.body_text)\n    \n    # also create a column for the summary of abstract to be used in a plot\n    if len(content.abstract) == 0: \n        # no abstract provided, we input the title\n        dict_['abstract_summary'].append(meta_data['title'].values[0])\n    else:\n        dict_['abstract_summary'].append(content.abstract)\n        \n    # get metadata information\n    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n    \n    # if more than one author\n    try:\n        authors = str(meta_data['authors'].values[0]).split(';')\n        authors1 = [i.split(',') for i in authors]    \n        dict_['authors'].append(\". \".join(authors))\n    except Exception as e:\n        dict_['authors'].append(\". \".join(authors))\n    \n    # add the title information, add breaks when needed\n    dict_['title'].append(meta_data['title'].values[0])\n    \n    # add the journal information\n    dict_['journal'].append(meta_data['journal'].values[0])\n    \ndf_covid = pd.DataFrame(dict_, columns=['paper_id', 'abstract', 'body_text', 'authors', 'title', 'journal', 'abstract_summary'])\ndf_covid.head()","e608bde1":"dict_ = None","95b869bf":"# Adding word count column\n\ndf_covid['abstract_word_count'] = df_covid['abstract'].apply(lambda x: len(x.strip().split()))\ndf_covid['body_word_count'] = df_covid['body_text'].apply(lambda x: len(x.strip().split()))\ndf_covid.head()","cee86f51":"# We will remove the duplicated papers\ndf_covid.shape","7fe15601":"# Removing duplicated papers\n\nduplicate_paper = ~(df_covid.title.isnull() | df_covid.abstract.isnull()) & (df_covid.duplicated(subset=['title', 'abstract']))\ndf_covid = df_covid[~duplicate_paper].reset_index(drop=True)\ndf_covid.shape","6fac5fde":"# Creating a list of stopwords in english\n\nenglish_stopwords = list(set(stopwords.words('english')))","11516297":"# Creating a lemmatizing function\n\nlmtzr = WordNetLemmatizer()","b8adb59e":"# Creating a stem function\n\nporter = PorterStemmer()","210cf34b":"# Creating a function that cleans text of special characters\n\ndef strip_characters(text):\n    t = re.sub('\\(|\\)|:|,|;|\\.|\u2019|\u201d|\u201c|\\?|%|>|<', '', text)\n    t = re.sub('\/', ' ', t)\n    t = t.replace(\"'\",'')\n    return t","47068ecf":"# Creating a function that makes text lowercase and uses the function created above\n\ndef clean(text):\n    t = text.lower()\n    t = strip_characters(t)\n    return t","53b025f9":"# Tokenize into individual tokens - words mostly\n\ndef tokenize(text):\n    words = nltk.word_tokenize(text)\n    return list(set([word for word in words \n                     if len(word) > 1\n                     and not word in english_stopwords\n                     and not (word.isnumeric() and len(word) is not 4)\n                     and (not word.isnumeric() or word.isalpha())] )\n               )","ca4122ec":"# Creating a function that cleans, lemmatize and tokenize texts\n\ndef preprocess(text):\n    t = clean(text)\n    tokens = tokenize(t)\n    l = [lmtzr.lemmatize(word) for word in tokens]\n    return tokens","970df943":"def stemming(text):\n    stem_sentence=[]\n    for word in text:\n        stem_sentence.append(porter.stem(word))\n    return \"\".join(stem_sentence)","b6c780a8":"# Preprocessing all the strings inside the column abstract. It will make them lowercase, remove special characters, stopwords and tokenize them.\ndf_covid['abstract_processed'] = df_covid['abstract'].apply(lambda x: preprocess(x))","595b43d6":"# Preprocessing all the strings inside the column abstract. It will make stem them.\ndf_covid['abstract'] = df_covid['abstract'].apply(lambda x: stemming(x))","9ad1e58e":"abstract = df_covid['abstract_processed'].tolist()\nlen(abstract)","ef94579b":"# Creating vectors for each word\nn_gram_all = []\n\nfor word in abstract:\n    n_gram = []\n    for i in range(len(word)-2+1):\n        n_gram.append(\"\".join(word[i:i+2]))\n    n_gram_all.append(n_gram)","0c29274d":"# hash vectorizer instance\n\nhvec = HashingVectorizer(lowercase=False, analyzer=lambda l:l, n_features=2**12)","27c9551b":"# Fit and Transforming hash vectorizer\n\nX = hvec.fit_transform(n_gram_all)\nX.shape","c16d0e26":"# THIS WILL TAKE A LONG, LONG TIME. Go watch some series. Go call your family. Catch up with your friends.\n\n# Building the clustering model and calculating the values of the Distortion and Inertia\n\ndistortions = [] \ninertias = [] \nmapping1 = {} \nmapping2 = {} \nK = range(1,26) \n\nfor k in tqdm.tqdm(K): \n    #Building and fitting the model \n    kmeanModel = KMeans(n_clusters=k).fit(X.toarray()) \n    kmeanModel.fit(X.toarray())     \n\n    distortions.append(sum(np.min(cdist(X.toarray(), kmeanModel.cluster_centers_, \n                          'euclidean'),axis=1)) \/ X.toarray().shape[0]) \n    inertias.append(kmeanModel.inertia_)\n\n    mapping1[k] = sum(np.min(cdist(X.toarray(), kmeanModel.cluster_centers_, \n                     'euclidean'),axis=1)) \/ X.toarray().shape[0] \n    mapping2[k] = kmeanModel.inertia_ \n    time.sleep(0.1)","a1f19ec4":"# List of number of clusters and the decrease of value, this helps to see exactly where the elbow is flexing\n\nfor key,val in mapping1.items(): \n    print(str(key)+' : '+str(val.round(4)))","a36f1eb2":"# Plotting the elbow graph\n\nplt.plot(K, distortions, 'bx-') \nplt.xlabel('Values of K') \nplt.ylabel('Distortion') \nplt.title('The Elbow Method using Distortion') \nplt.show()","bba90a1a":"# Dimensionality Reduction with t-SNE\n\ntsne = TSNE(verbose = 1, perplexity = 10, metric = 'cosine', early_exaggeration = 20, learning_rate = 300, random_state = 42)\nX_embedded = tsne.fit_transform(X)","afdc9e6c":"# sns settings\nsns.set(rc={'figure.figsize':(15,15)})\n\n# colors\npalette = sns.color_palette(\"bright\", 1)\n\n# plot\nsns.scatterplot(X_embedded[:,0], X_embedded[:,1], palette=palette)\n\nplt.title(\"t-SNE Covid-19 Articles\")\n# plt.savefig(\"plots\/t-sne_covid19.png\")\nplt.show()","eb6e3574":"# determining the best number of clusters\n\nk = 21\nkmeans = MiniBatchKMeans(n_clusters=k)\ny_pred = kmeans.fit_predict(X)\ny = y_pred","eb899b0a":"\noutput_notebook()\ny_labels = y_pred\n\n# data sources\nsource = ColumnDataSource(data=dict(\n    x= X_embedded[:,0], \n    y= X_embedded[:,1],\n    x_backup = X_embedded[:,0],\n    y_backup = X_embedded[:,1],\n    desc= y_labels, \n    titles= df_covid['title'],\n    authors = df_covid['authors'],\n    journal = df_covid['journal'],\n    abstract = df_covid['abstract_summary'],\n    labels = [\"C-\" + str(x) for x in y_labels]\n    ))\n\n# hover over information\nhover = HoverTool(tooltips=[\n    (\"Title\", \"@titles{safe}\"),\n    (\"Author(s)\", \"@authors\"),\n    (\"Journal\", \"@journal\"),\n    (\"Abstract\", \"@abstract{safe}\"),\n],\n                 point_policy=\"follow_mouse\")\n\n# map colors\nmapper = linear_cmap(field_name='desc', \n                     palette=Category20[20],\n                     low=min(y_labels) ,high=max(y_labels))\n\n# prepare the figure\np = figure(plot_width=800, plot_height=800, \n           tools=[hover, 'pan', 'wheel_zoom', 'box_zoom', 'reset'], \n           title=\"t-SNE Covid-19 Articles, Clustered(K-Means), Abstracts Hash Vectorized\", \n           toolbar_location=\"right\")\n\n# plot\np.scatter('x', 'y', size=5, \n          source=source,\n          fill_color=mapper,\n          line_alpha=0.3,\n          line_color=\"black\",\n          legend = 'labels')\n\n# add callback to control \ncallback = CustomJS(args=dict(p=p, source=source), code=\"\"\"\n            \n            var radio_value = cb_obj.active;\n            var data = source.data; \n            \n            x = data['x'];\n            y = data['y'];\n            \n            x_backup = data['x_backup'];\n            y_backup = data['y_backup'];\n            \n            labels = data['desc'];\n            \n            if (radio_value == '20') {\n                for (i = 0; i < x.length; i++) {\n                    x[i] = x_backup[i];\n                    y[i] = y_backup[i];\n                }\n            }\n            else {\n                for (i = 0; i < x.length; i++) {\n                    if(labels[i] == radio_value) {\n                        x[i] = x_backup[i];\n                        y[i] = y_backup[i];\n                    } else {\n                        x[i] = undefined;\n                        y[i] = undefined;\n                    }\n                }\n            }\n\n\n        source.change.emit();\n        \"\"\")\n\n# callback for searchbar\nkeyword_callback = CustomJS(args=dict(p=p, source=source), code=\"\"\"\n            \n            var text_value = cb_obj.value;\n            var data = source.data; \n            \n            x = data['x'];\n            y = data['y'];\n            \n            x_backup = data['x_backup'];\n            y_backup = data['y_backup'];\n            \n            abstract = data['abstract'];\n            titles = data['titles'];\n            authors = data['authors'];\n            journal = data['journal'];\n\n            for (i = 0; i < x.length; i++) {\n                if(abstract[i].includes(text_value) || \n                   titles[i].includes(text_value) || \n                   authors[i].includes(text_value) || \n                   journal[i].includes(text_value)) {\n                    x[i] = x_backup[i];\n                    y[i] = y_backup[i];\n                } else {\n                    x[i] = undefined;\n                    y[i] = undefined;\n                }\n            }\n            \n\n\n        source.change.emit();\n        \"\"\")\n\n# option\noption = RadioButtonGroup(labels=[\"C-0\", \"C-1\", \"C-2\",\n                                  \"C-3\", \"C-4\", \"C-5\",\n                                  \"C-6\", \"C-7\", \"C-8\",\n                                  \"C-9\", \"C-10\", \"C-11\",\n                                  \"C-12\", \"C-13\", \"C-14\",\n                                  \"C-15\", \"C-16\", \"C-17\",\n                                  \"C-18\", \"C-19\", \"C-20\", \"C-21\",\n                                  \"C-22\", \"C-22\", \"C-23\", \"C-24\",\n                                  \"C-25\", \"C-26\", \"C-27\", \"C-28\",\n                                  \"C-20\", \"C-29\", \"C-30\", \"C-31\",\n                                  \"C-32\", \"C-33\", \"C-34\", \"C-35\",\n                                  \"C-36\", \"C-37\", \"C-38\", \"C-39\",\n                                  \"C-40\", \"All\"], \n                          active=40, callback=callback)\n\n# search box\nkeyword = TextInput(title=\"Search:\", callback=keyword_callback)\n\n#header\nheader = Div(text=\"\"\"<h1>COVID-19 Research Papers Interactive Cluster Map<\/h1>\"\"\")\n\n# show\nshow(column(header, widgetbox(option, keyword),p))\n","57e77439":"# Vectorizing with plain text and TD-IDF\n\nvectorizer = TfidfVectorizer(max_features=2**12)\nX1 = vectorizer.fit_transform(df_covid['abstract'].values)","c9d2ff21":"# Dimension reduction\n\ntsne = TSNE(verbose=1, perplexity = 10, metric = 'cosine', early_exaggeration = 20, learning_rate = 300, random_state = 42)\nX_embedded1 = tsne.fit_transform(X1.toarray())","f6f2cc61":"# THIS WILL TAKE A LONG, LONG TIME. Go watch some series. Go call your family. Catch up with your friends.\n\n# Building the clustering model and calculating the values of the Distortion and Inertia\n\ndistortions = [] \ninertias = [] \nmapping1 = {} \nmapping2 = {} \nK = range(1,26) \n\nfor k in tqdm.tqdm(K): \n    #Building and fitting the model \n    kmeanModel = KMeans(n_clusters=k).fit(X1.toarray()) \n    kmeanModel.fit(X1.toarray())     \n\n    distortions.append(sum(np.min(cdist(X1.toarray(), kmeanModel.cluster_centers_, \n                          'euclidean'),axis=1)) \/ X1.toarray().shape[0]) \n    inertias.append(kmeanModel.inertia_)\n\n    mapping1[k] = sum(np.min(cdist(X1.toarray(), kmeanModel.cluster_centers_, \n                     'euclidean'),axis=1)) \/ X1.toarray().shape[0] \n    mapping2[k] = kmeanModel.inertia_ \n    time.sleep(0.1)","cf3f5d01":"# List of number of clusters and the decrease of value, this helps to see exactly where the elbow is flexing\n\nfor key,val in mapping1.items(): \n    print(str(key)+' : '+str(val.round(4)))","cc9c5887":"# Plotting the elbow graph\n\nplt.plot(K, distortions, 'bx-') \nplt.xlabel('Values of K') \nplt.ylabel('Distortion') \nplt.title('The Elbow Method using Distortion') \nplt.show()","9b2479ee":"# determining the best number of clusters for TD IDF\n\nk = 21\nkmeans = MiniBatchKMeans(n_clusters=k)\ny_pred1 = kmeans.fit_predict(X1)\ny1 = y_pred1","63ef4389":"# sns settings\nsns.set(rc={'figure.figsize':(15,15)})\n\n# colors\npalette = sns.color_palette(\"bright\", len(set(y1)))\n\n# plot\nsns.scatterplot(X_embedded1[:,0], X_embedded1[:,1], hue=y1, legend='full', palette=palette)\nplt.title(\"t-SNE Covid-19 Articles - Clustered(K-Means) - Tf-idf with Plain Text\")\n# plt.savefig(\"plots\/t-sne_covid19_label_TFID.png\")\nplt.show()","e11683f0":"\noutput_notebook()\ny_labels = y_pred1\n\n# data sources\nsource = ColumnDataSource(data=dict(\n    x= X_embedded1[:,0], \n    y= X_embedded1[:,1],\n    x_backup = X_embedded1[:,0],\n    y_backup = X_embedded1[:,1],\n    desc= y_labels, \n    titles= df_covid['title'],\n    authors = df_covid['authors'],\n    journal = df_covid['journal'],\n    abstract = df_covid['abstract_summary'],\n    labels = [\"C-\" + str(x) for x in y_labels]\n    ))\n\n# hover over information\nhover = HoverTool(tooltips=[\n    (\"Title\", \"@titles{safe}\"),\n    (\"Author(s)\", \"@authors\"),\n    (\"Journal\", \"@journal\"),\n    (\"Abstract\", \"@abstract{safe}\"),\n],\n                 point_policy=\"follow_mouse\")\n\n# map colors\nmapper = linear_cmap(field_name='desc', \n                     palette=Category20[20],\n                     low=min(y_labels) ,high=max(y_labels))\n\n# prepare the figure\np = figure(plot_width=800, plot_height=800, \n           tools=[hover, 'pan', 'wheel_zoom', 'box_zoom', 'reset'], \n           title=\"t-SNE Covid-19 Articles, Clustered(K-Means), Tf-idf with Plain Text\", \n           toolbar_location=\"right\")\n\n# plot\np.scatter('x', 'y', size=5, \n          source=source,\n          fill_color=mapper,\n          line_alpha=0.3,\n          line_color=\"black\",\n          legend = 'labels')\n\n# add callback to control \ncallback = CustomJS(args=dict(p=p, source=source), code=\"\"\"\n            \n            var radio_value = cb_obj.active;\n            var data = source.data; \n            \n            x = data['x'];\n            y = data['y'];\n            \n            x_backup = data['x_backup'];\n            y_backup = data['y_backup'];\n            \n            labels = data['desc'];\n            \n            if (radio_value == '20') {\n                for (i = 0; i < x.length; i++) {\n                    x[i] = x_backup[i];\n                    y[i] = y_backup[i];\n                }\n            }\n            else {\n                for (i = 0; i < x.length; i++) {\n                    if(labels[i] == radio_value) {\n                        x[i] = x_backup[i];\n                        y[i] = y_backup[i];\n                    } else {\n                        x[i] = undefined;\n                        y[i] = undefined;\n                    }\n                }\n            }\n\n\n        source.change.emit();\n        \"\"\")\n\n# callback for searchbar\nkeyword_callback = CustomJS(args=dict(p=p, source=source), code=\"\"\"\n            \n            var text_value = cb_obj.value;\n            var data = source.data; \n            \n            x = data['x'];\n            y = data['y'];\n            \n            x_backup = data['x_backup'];\n            y_backup = data['y_backup'];\n            \n            abstract = data['abstract'];\n            titles = data['titles'];\n            authors = data['authors'];\n            journal = data['journal'];\n\n            for (i = 0; i < x.length; i++) {\n                if(abstract[i].includes(text_value) || \n                   titles[i].includes(text_value) || \n                   authors[i].includes(text_value) || \n                   journal[i].includes(text_value)) {\n                    x[i] = x_backup[i];\n                    y[i] = y_backup[i];\n                } else {\n                    x[i] = undefined;\n                    y[i] = undefined;\n                }\n            }\n            \n\n\n        source.change.emit();\n        \"\"\")\n\n# option\noption = RadioButtonGroup(labels=[\"C-0\", \"C-1\", \"C-2\",\n                                  \"C-3\", \"C-4\", \"C-5\",\n                                  \"C-6\", \"C-7\", \"C-8\",\n                                  \"C-9\", \"C-10\", \"C-11\",\n                                  \"C-12\", \"C-13\", \"C-14\",\n                                  \"C-15\", \"C-16\", \"C-17\",\n                                  \"C-18\", \"C-19\", \"C-20\", \"C-21\",\n                                  \"C-22\", \"C-22\", \"C-23\", \"C-24\",\n                                  \"C-25\", \"C-26\", \"C-27\", \"C-28\",\n                                  \"C-20\", \"C-29\", \"C-30\", \"C-31\",\n                                  \"C-32\", \"C-33\", \"C-34\", \"C-35\",\n                                  \"C-36\", \"C-37\", \"C-38\", \"C-39\",\n                                  \"C-40\", \"All\"], \n                          active=40, callback=callback)\n\n# search box\nkeyword = TextInput(title=\"Search:\", callback=keyword_callback)\n\n#header\nheader = Div(text=\"\"\"<h1>COVID-19 Research Papers Interactive Cluster Map<\/h1>\"\"\")\n\n# show\nshow(column(header, widgetbox(option, keyword),p))","afb57f52":"<h2>Goal<\/h2><br>\n    This is my second response to the call to action to the artificial intelligence experts (if I can be called one) to ddevelop text and data mining tools that can help the medical community develop answers to high priority scientific questions. For that I will use the CORD-19 dataset, which represents the most extensive machine-readable coronavirus literature collection available for data mining to date. Bellow are the current tasks for this challenge, which will be completed by the creation of an interactive custer graph, with a search bar search for the all_sources_metadata file. There are around 29500 papers in the dataset. These are listed in the all_sources_metadata file. Some of the papers in the metadata are also in JSON files. The eventual goal is to connect the metadata with the JSON data.<br>\n    <h2>Tasks<\/h2>\n    <ul>\n    <li>What is known about transmission, incubation, and environmental stability?<\/li>\n    <li>What do we know about COVID-19 risk factors?<\/li>\n    <li>What do we know about virus genetics, origin, and evolution?<\/li>\n    <li>Sample task with sample submission<\/li>\n    <li>What do we know about vaccines and therapeutics?<\/li>\n    <li>What do we know about non-pharmaceutical interventions?<\/li>\n    <li>What has been published about ethical and social science considerations?<\/li>\n    <li>What do we know about diagnostics and surveillance?<\/li>\n    <li>What has been published about medical care?<\/li>\n    <li>What has been published about information sharing and inter-sectoral collaboration?<\/li>\n    <\/ul>\n    <h2>Citations, ups and downs<\/h2><br>\n    I used the <a href='https:\/\/www.kaggle.com\/maksimeren\/covid-19-literature-clustering'>COVID-19 Literature Clustering<\/a> as a reference for making this notebook. I was still looking forward to testing different NLP solutions (check my other one <a href='https:\/\/www.kaggle.com\/beatrizyumi\/covid-19-autocomplete-search-bar'>here<\/a> and the ones presented on this notebook were very interesting, showing the different results on many different modeling choices for the clustering of the research papers. I opted for the t-SNE, because it is very visual and it would give me a good output for an interactive graph. I also tried to make the graph on plotly, simply because it is one of my favorite libraries out there. On the plus side, it offers a very dynamic, interactive and visual way to look at the research papers, being able to sort them by clusters or by searching them through the search bar. On the down side, it needs you to have your internet working, and depending on the size of the dataframe, it can take a while to load (mine took a few hours). Besides that I am still working on a way to split authors when they are separated by comma or by semicolon.<br>\n    <h2>Features of this notebook<\/h2>\n<ol><li>Viewing the papers in the metdata csv as a dataframe<\/li>\n    <li>Viewing the papers in the interactive graph, which has hover information<\/li>\n    <li>Search using a simple search index inside the interactive graph<\/li><\/ol>\n    <h2>Turn your internet on!<\/h2><br>\n    For this notebook to work your internet must be on.","b0ffeace":"Now that the functions are ready, we can start trating our text. However, I will create the tokens in another column because I want to vectorize my X in two different ways.","b103f6b2":"We then input all the json files into a DataFrame. (This might take a while, since there are a lot of files). A lot of things will happen here.<br><br>\n<b>First<\/b> we will only work with papers with meta data.<br><br>\n<b>Second<\/b> if there is no abstract provided, we will use the title of the research paper as the abstract, as the analysis will use the abstract as a basis.<br><br>\n<b>Third<\/b> all the other information is included on it's field, separated if there is more than one of each, in the case of authors, for example.\n","00e0246d":"# It is time for some modeling\n![](https:\/\/media0.giphy.com\/media\/Mvm1XBC8O48EM\/giphy.gif?cid=790b76119bdd3e0bcedc198f7b26c76abecbb392aba651cf&rid=giphy.gif)","42470eb7":"We will now start treating the text inside the DataFrame. Before doing anything, we have to define a lot of functions to do that.","cabf1fb6":"# Importing libraries and datasets\n![](https:\/\/media1.giphy.com\/media\/2yqYwtl5MUa6V405Ib\/giphy.gif?cid=790b7611312b8226ba98603f316f4a2740c6e4a11ef228fc&rid=giphy.gif)","5a2b7b24":"From the graph we can see that ther are a few inflexions we could use, but I will go for 21 because of the large number of data.","96034186":"# COVID-19 Open Research Dataset Challenge (CORD-19)\n![](https:\/\/altaonline.typepad.com\/.a\/6a0192ac343706970d025d9b3673bb200c-800wi)","fc17a41c":"First we create a class that will read the json files in a humane readable way.","c6be42e8":"# Treating the data\n![](https:\/\/media2.giphy.com\/media\/l41YtBXZvSRdgqq7m\/giphy.gif?cid=790b76115c16e4f50dc6c6525b7b74b67a0ef53415ae01c0&rid=giphy.gif)","f885ae91":"There may be papers that were inputted from more than one source, so we should check and remove duplicated inputs.","09678b39":"Before we go any further, let's determine the ideal number of clusters for our analysis. We will do that with the elbow method in kmeans. This takes a long time, so I put the tqdm library on the for loop, so we are sure the kernel is working. Seriously, it takes a ridiculous amount of time, even with a good computer, and since we are dealing with a great number of entries, we have to test a lot of possibilities of clusters. Run it, go watch a series, call your family, catch up with some friends."}}