{"cell_type":{"b4bfbb28":"code","559069c2":"code","399b399d":"code","105c4d08":"code","6ea71b9e":"code","a26af80a":"code","c5b571db":"code","ac307cb6":"code","49830c1d":"code","b1b5e0f9":"code","40c15d56":"code","73b205bb":"code","c4cd6d07":"code","788790e8":"code","73658f30":"code","4c5864f7":"code","032ca908":"code","b49735af":"code","3cd93055":"code","8c835ef1":"code","33a349ec":"code","041a247a":"code","54a3be9f":"code","9779e6f9":"code","6f0c58f0":"code","d2764234":"code","17807ea9":"code","4c1a4339":"code","f9533db6":"code","34777446":"code","3d503e1c":"code","6d3211f6":"code","238ca1cb":"code","7db3004b":"code","9d1a3fa2":"code","40d0c372":"code","2d95284b":"code","e2a419e4":"code","bbdc59b3":"code","b3606e11":"code","0f038aa7":"code","9e028159":"code","fcdaf8a1":"code","c634af41":"code","b5214a76":"code","b5f5658a":"markdown","bede86fa":"markdown","fd2f48cb":"markdown","1c72f506":"markdown","116e21a0":"markdown","1103fa8f":"markdown","bbeff817":"markdown"},"source":{"b4bfbb28":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","559069c2":"from sklearn.datasets import fetch_openml\nfrom matplotlib import pyplot as plt","399b399d":"data=fetch_openml(\"mnist_784\",version=1)","105c4d08":"X,y=data.data,data.target","6ea71b9e":"X.shape,y.shape","a26af80a":"some_digit=X[0]\nsome_digit1=some_digit.reshape(28,28)","c5b571db":"plt.matshow(some_digit1,interpolation=\"nearest\")\nplt.axis(\"off\")","ac307cb6":"from sklearn.model_selection import train_test_split\ny=y.astype(np.uint16)\nX_train,x_test,y_train,y_test=train_test_split(X,y,random_state=42)\ny_train_5=(y_train==5)","49830c1d":"from sklearn.linear_model import SGDClassifier ","b1b5e0f9":"sgd_classifier=SGDClassifier()\nsgd_classifier.fit(X_train[:10000],y_train_5[:10000])","40c15d56":"sgd_classifier.predict([some_digit])","73b205bb":"from sklearn.model_selection import cross_val_predict","c4cd6d07":"y_predicted= cross_val_predict(SGDClassifier(),X_train[:10000],y_train_5[:10000],cv=3)","788790e8":"y_predicted","73658f30":"from sklearn.metrics import confusion_matrix,precision_score,recall_score\nprint(confusion_matrix(y_train_5[:10000],y_predicted))","4c5864f7":"precision=(601)\/(601+134)##true positive \/ (true postive + false positive)\nrecall=(601)\/(601+301   ) ## true positive \/ (ture postive + false negative )\nprecision,recall","032ca908":"print(precision_score(y_train_5[:10000],y_predicted))\nprint(recall_score(y_train_5[:10000],y_predicted))","b49735af":"score=sgd_classifier.decision_function([some_digit])\npredicted_for_some_digit=sgd_classifier.predict([some_digit])\nprint(score,predicted_for_some_digit)","3cd93055":"let_suppose_threshold=0\nthreshold_predicted_value=(score<let_suppose_threshold)\nthreshold_predicted_value","8c835ef1":"## first it was giving the true value and not its giving false just because we reduce the threshold","33a349ec":"scores=cross_val_predict(SGDClassifier(),X_train[:10000],y_train_5[:10000],cv=3,method=\"decision_function\")","041a247a":"from sklearn.metrics import precision_recall_curve\nprecisions,recalls,thresholds=precision_recall_curve(y_train_5[:10000],scores)\nthreshold_90=thresholds[np.argmax(precisions>=0.90)]\nrecall_90=recalls[np.argmax(precisions>=0.9)]","54a3be9f":"plt.plot(thresholds,precisions[:-1],\"g--\",label=\"precision\")\nplt.plot(thresholds,recalls[:-1],\"b-\",label=\"recall\")\nplt.grid(True)\nplt.axis([-500000,500000,0,1])\nplt.plot([-600000,threshold_90],[0.9,0.9],\"r--\")\nplt.plot([-600000,threshold_90],[recall_90,recall_90],\"r--\")\nplt.plot([threshold_90,threshold_90],[0.,0.9],\"r--\")\nplt.plot(threshold_90,0.9,\"or\")\nplt.plot(threshold_90,recall_90,\"or\")\nplt.show()","9779e6f9":"# when we fix the precision on 90% , at that point the recall rate in 50.88","6f0c58f0":"plt.plot(precisions,recalls)\nplt.xlabel(\"precision\")\nplt.ylabel(\"recall\")\nplt.show()","d2764234":"from sklearn.metrics import roc_curve","17807ea9":"fpr,tpr,threshold_roc=roc_curve(y_train_5[:10000],scores)\nfpr_90=fpr[np.argmax(tpr>=recall_90)]","4c1a4339":"plt.plot(fpr,tpr)\nplt.plot([0,1],\"k--\")\nfpr_90=fpr[np.argmax(tpr>=recall_90)]\nplt.plot([fpr_90,fpr_90],[0,recall_90],\"r:\")\nplt.plot([0,fpr_90],[recall_90,recall_90],'r:')\nplt.plot(fpr_90,recall_90,\"or\")\nplt.show()","f9533db6":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV","34777446":"params=[\n    {\"n_estimators\":[2,4,6,8],\"criterion\":[\"entropy\"],\"bootstrap\":[False]},\n    {\"n_estimators\":[2,4,6,8],\"criterion\":[\"gini\"]},\n]","3d503e1c":"forest_cls=GridSearchCV(RandomForestClassifier(),param_grid=params,cv=2,return_train_score=True)","6d3211f6":"forest_cls.fit(X_train[:10000],y_train_5[:10000])","238ca1cb":"forest_cls.best_estimator_\nforest_cls.best_params_","7db3004b":"grid_random_forest=RandomForestClassifier(bootstrap= False, criterion= \"entropy\", n_estimators=8)","9d1a3fa2":"y_forest_classifer=cross_val_predict(grid_random_forest,X_train[:10000],y_train_5[:10000],cv=3,method=\"predict_proba\")","40d0c372":"scores_for_forest=y_forest_classifer[:,1]\n","2d95284b":"fpr_forest,tpr_forest,threshold_forest=roc_curve(y_train_5[:10000],scores_for_forest)\nrecall_for_forest = tpr_forest[np.argmax(fpr_forest >= fpr_90)]","e2a419e4":"plt.plot(fpr_forest,tpr_forest,label=\"random_forest\")\nplt.plot(fpr,tpr,label=\"Sgd_classifier\")\nplt.plot(fpr_90,recall_for_forest,\"or\")\nplt.plot(fpr_90,recall_90,\"or\")\nplt.legend();plt.show()","bbdc59b3":"from sklearn.metrics import roc_auc_score","b3606e11":"print(f\"{roc_auc_score(y_train_5[:10000],scores_for_forest)} auc for random_forest\")\nprint(f\"{roc_auc_score(y_train_5[:10000],scores)} auc for sgd classifier\")","0f038aa7":"from sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier()","9e028159":"knn.fit(X_train[:10000],y_train[:10000])","fcdaf8a1":"knn.score(X_train[10000:20000],y_train[10000:20000])","c634af41":"y_predicted_all=knn.predict(X_train[10000:11000])","b5214a76":"print(confusion_matrix(y_train[10000:11000],y_predicted_all))","b5f5658a":"now we have seen the Random_forest perform very well than sgd classifer , another way to find the area under the curve is auc ","bede86fa":"**ROC CURVE (reciever operating characteristics)**\n    the roc curve is used to plot the curve of true positive rate(i.e recall, sensitivity) in against of false positive ratre (1-specificity)","fd2f48cb":"##### now we have gain many insight of single binary classification now we will using all classes\n","1c72f506":"look there is the trade-off between true positive(recall) and false positve(fpr), the higher the recall it produces more false postive  ","116e21a0":"now we wil using different classifer and see the roc curve","1103fa8f":"#### so there is the tradeoff between precision and recall one's go high another goes down , its totally depend upon the situation like if you are preparing a model for shoplifter the recall should be high,precision is not so much important \n","bbeff817":"you can control the precision and recall by the threshold "}}