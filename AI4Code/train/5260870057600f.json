{"cell_type":{"36962da1":"code","c8913a6c":"code","37a6f8e7":"code","c5fef7cc":"code","f37e6bac":"code","96329d59":"code","b2633fa1":"code","361cbaeb":"code","5d648aa0":"code","435c1d3f":"code","c5be2f8b":"code","cf810211":"code","217b1cc3":"code","988c75dd":"code","1267785d":"code","cea0f880":"code","7cca2301":"code","cee1531c":"code","c86936fa":"code","ac7c8826":"code","45a90e98":"code","bbdbb6cd":"code","3b6cbc33":"code","8ac19c11":"code","3e770758":"code","25a55f02":"code","a88e9f14":"code","4f1c1d2f":"code","308cf72e":"code","5a10ce91":"code","9812c67c":"code","480c4146":"code","20759080":"code","b551defc":"code","3297a6b0":"code","149cf99b":"code","221f63f4":"code","f936cfe2":"code","050685e5":"code","005e3fc5":"code","6498767a":"code","4706d799":"code","e3456b72":"code","bd8904b3":"code","f41aae99":"code","39986d0f":"code","a2178ce1":"code","0ae53a9c":"code","8264b960":"code","db8a704d":"code","d4f640c1":"code","53566abe":"markdown","db7a7443":"markdown","180ac9f7":"markdown","38dfd67c":"markdown","94ca7d6b":"markdown","822228be":"markdown","25dda72d":"markdown","fa57a7d4":"markdown","8a826ae0":"markdown","b5438cd7":"markdown","6689db41":"markdown","b2669763":"markdown","88dbbd42":"markdown","845d941b":"markdown","5eb07e6d":"markdown","7cfc39e0":"markdown","edf7b12f":"markdown","33784686":"markdown","766c02ad":"markdown","1cec1906":"markdown"},"source":{"36962da1":"# importing pandas for reading the datasets\nimport pandas as pd","c8913a6c":"# reading the training dataset with a ';' delimiter\nbdata=pd.read_csv('..\/input\/mlworkshop\/bank-full.csv',delimiter=';')","37a6f8e7":"# displaying first 10 observations from the dataset\nbdata.head(10)","c5fef7cc":"# describing the pandas dataframe bdata\nbdata.describe()","f37e6bac":"# getting details of no of attributes and observations\nprint('No of observations :',bdata.shape[0])\nprint('No of attributes :',bdata.shape[1])\nprint('No of numerical attributes :',bdata.describe().shape[1])\nprint('No of categorical attributes :',bdata.shape[1]-bdata.describe().shape[1])","96329d59":"# getting list of attributes\nbdata.columns.tolist()","b2633fa1":"# importing matplotlib for plotting the graphs\nimport matplotlib.pyplot as plt","361cbaeb":"bdata['y'].value_counts().plot(kind='bar')\nplt.title('Subscriptions')\nplt.xlabel('Term Deposit')\nplt.ylabel('No of Subscriptions')\nplt.show()","5d648aa0":"pd.crosstab(bdata.job,bdata.y).plot(kind='bar')\nplt.title('Subscriptions based on Job')\nplt.xlabel('Job')\nplt.ylabel('No of Subscriptions')\nplt.show()","435c1d3f":"pd.crosstab(bdata.marital,bdata.y).plot(kind='bar')\nplt.title('Subscriptions based on Marital Status')\nplt.xlabel('Marital Status')\nplt.ylabel('No of Subscriptions')\nplt.show()","c5be2f8b":"pd.crosstab(bdata.education,bdata.y).plot(kind='bar')\nplt.title('Subscriptions based on Education')\nplt.xlabel('Education')\nplt.ylabel('No of Subscriptions')\nplt.show()","cf810211":"pd.crosstab(bdata.housing,bdata.y).plot(kind='bar')\nplt.title('Subscriptions based on Housing Credit')\nplt.xlabel('Housing Credit')\nplt.ylabel('No of Subscriptions')\nplt.show()","217b1cc3":"pd.crosstab(bdata.loan,bdata.y).plot(kind='bar')\nplt.title('Subscriptions based on Personal Loan')\nplt.xlabel('Personal Loan')\nplt.ylabel('No of Subscriptions')\nplt.show()","988c75dd":"pd.crosstab(bdata.poutcome,bdata.y).plot(kind='bar')\nplt.title('Subscriptions based on Outcome of Previous Campaign')\nplt.xlabel('Outcome of Previous Campaign')\nplt.ylabel('No of Subscriptions')\nplt.show()","1267785d":"pd.crosstab(bdata.month,bdata.y).plot(kind='bar')\nplt.title('Monthly Subscriptions')\nplt.xlabel('Month')\nplt.ylabel('No of Subscriptions')\nplt.show()","cea0f880":"# creating dummy variables for categorical variables\n\n# creating a list of categorical variables to be transformed into dummy variables\ncategory=['job','marital','education','default','housing','loan','contact',\n          'month','poutcome']\n\n# creating a backup\nbdata_new = bdata\n\n# creating dummy variables and joining it to the training set\nfor c in category:\n    new_column = pd.get_dummies(bdata_new[c], prefix=c)\n    bdata_dummy=bdata_new.join(new_column)\n    bdata_new=bdata_dummy","7cca2301":"bdata_new.head(10)","cee1531c":"# see the dummy setup of one categorical variable\nbdata_new[[col for col in bdata_new if col.startswith('education')]].head(10)","c86936fa":"# drop the initial categorical variable\nbdata_final=bdata_new.drop(category,axis=1)","ac7c8826":"bdata_final.head(10)","45a90e98":"# coding no as '0' and yes as '1'\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nlabels = le.fit_transform(bdata_final['y'])\nbdata_final['y'] = labels","bbdbb6cd":"bdata_final.y.value_counts()","3b6cbc33":"bdata_final.head(10)","8ac19c11":"# feature selection to reduce dimensionality\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import MinMaxScaler\n\n# creating dataframe of features\nX=bdata_final.drop(['y'],axis=1)\n# creating dataframe of output variable\ny=bdata_final['y']\n\n# standard scaling\nX_norm = MinMaxScaler().fit_transform(X)\n\nrfe_selector = RFE(estimator=LogisticRegression(solver='liblinear',max_iter=100,multi_class='ovr',n_jobs=1), n_features_to_select=30, step=10, verbose=5)\nrfe_selector.fit(X_norm, y)\nrfe_support = rfe_selector.get_support()\nrfe_feature = X.loc[:,rfe_support].columns.tolist()\nprint(str(len(rfe_feature)), 'selected features')","3e770758":"rfe_feature","25a55f02":"# dropping age and pdays\nbdata_final=bdata_final.drop(['age','pdays'],axis=1)","a88e9f14":"bdata_final.head(10)","4f1c1d2f":"cat=[col for col in bdata_final if col.startswith('job')]\nmar_cat=[col for col in bdata_final if col.startswith('marital')]\nedu_cat=[col for col in bdata_final if col.startswith('education')]\nloan_cat=[col for col in bdata_final if col.startswith('loan')]\ncat.extend(mar_cat)\ncat.extend(edu_cat)\ncat.extend(loan_cat)","308cf72e":"cat","5a10ce91":"# creating a dataframe with lesser dimension\nbdata_dr=bdata_final.drop(cat,axis=1)","9812c67c":"bdata_dr.head(10)","480c4146":"# importing sklearn for train test split\nfrom sklearn.model_selection import train_test_split","20759080":"# creating training set of features\nX=bdata_final.drop(['y'],axis=1)\n# creating training set of output variable\ny=pd.DataFrame(bdata_final['y'])","b551defc":"# splitting the dataset into train and test for both input and output variable\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25)","3297a6b0":"X_train.head(10)","149cf99b":"y_train.head(10)","221f63f4":"X_test.head(10)","f936cfe2":"y_test.head(10)","050685e5":"# importing the Standard Scaler from sklearn\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\ny_train = y_train.values.ravel()\ny_test = y_test.values.ravel()","005e3fc5":"X_train","6498767a":"X_test","4706d799":"y_train","e3456b72":"y_test","bd8904b3":"# importing imblearn for Synthetic Minority Over Sampling Technique\n# NOTE : SMOTE technique needs the dataset to be numpy array\n\n# from imblearn.over_sampling import SMOTE\n# sm = SMOTE(sampling_strategy='auto', k_neighbors=1, random_state=0)\n# X_res, y_res = sm.fit_resample(X_train, y_train)\n# import numpy as np\n# np.savetxt('xres.txt', X_res, fmt='%f')\n# np.savetxt('yres.txt', y_res, fmt='%d')\n\n# SMOTE applied dataset\nimport numpy as np\nX_res = np.loadtxt('..\/input\/smotedata\/xres.txt', dtype=float)\ny_res = np.loadtxt('..\/input\/smotedata\/yres.txt', dtype=int)","f41aae99":"print('No 0f 0 case :',y_res[y_res==0].shape[0])\nprint('No of 1 case :',y_res[y_res==1].shape[0])","39986d0f":"# importing keras library\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense","a2178ce1":"# initializing the ANN\nclassifier = Sequential()\n# Adding the input layer and the hidden layer\nclassifier.add(Dense(units = 25 , kernel_initializer = 'uniform',\n                     activation = 'relu', input_dim = 49))\n\n# Adding a second hidden layer\nclassifier.add(Dense(units = 18, kernel_initializer = 'uniform', activation = 'sigmoid'))\n\n# Adding the output layer\nclassifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n\n# Compiling the ANN\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])","0ae53a9c":"# Fitting the ANN to the Training set\nclassifier.fit(X_res, y_res, batch_size = 100, epochs = 10)","8264b960":"# predicting the testing set results\ny_pred = classifier.predict(X_test)\ny_pred = (y_pred > 0.50)","db8a704d":"# importing confusion matrix and roc_auc_score from sklearn\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score\n\n# importing seaborn for plotting the heatmap\nimport seaborn as sn\n\ncm = confusion_matrix(y_test, y_pred) # rows = truth, cols = prediction\ndf_cm = pd.DataFrame(cm, index = ('no', 'yes'), columns = ('predicted no',\n                                                           'predicted yes'))\nplt.figure(figsize = (5,4))\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nsn.set(font_scale=1.4)\nsn.heatmap(df_cm, annot=True, fmt='g')\nplt.show()\nprint(\"Test Data Accuracy: %0.4f\" % roc_auc_score(y_test, y_pred))","d4f640c1":"# importing roc curve and metrics from sklearn\nfrom sklearn.metrics import roc_curve\nimport sklearn.metrics as metrics\n\nfpr, tpr, threshold = metrics.roc_curve(y_test, y_pred)\nroc_auc=roc_auc_score(y_test, y_pred)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, label='AUC = %0.2f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.001, 1])\nplt.ylim([0, 1.001])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","53566abe":"__Personal loan vs Subscription__","db7a7443":"<h4>UNDERSTANDING FEATURES OF DATASET<\/h4>\n\n__default__: has credit in default?\n\n__housing__: has housing loan? \n\n__loan__: has personal loan?\n\n__day__: last contact day of the week\n\n__month__: last contact month of year \n\n__duration__: last contact duration, in seconds \n\n__campaign__: number of contacts performed during this campaign and for this client \n\n__pdays__: number of days that passed by after the client was last contacted from a previous campaign\n\n__previous__: number of contacts performed before this campaign and for this client\n\n__poutcome__: outcome of the previous marketing campaign","180ac9f7":"__Outcome variable__","38dfd67c":"__Outcome of Previous Campaign vs Subscription__","94ca7d6b":"<h4>READING DATASET<\/h4>","822228be":"__Education vs Subscription__","25dda72d":"__Artificial Neural Network__","fa57a7d4":"__Job vs Subscription__","8a826ae0":"__Housing Credit vs Subscription__","b5438cd7":"<p>features to be eliminated : age, pdays (30 selected features)<\/p>\n<p>features that may be eliminated : job, marital, education, loan (20 selected features)<\/p>","6689db41":"<h4>STANDARDIZING TRAINING AND TESTING SET<\/h4>","b2669763":"Since the data preprocessing steps are same for both testing and training dataset, we first perform the data preprocessing and then divide the data into training data and testing data.","88dbbd42":"<h4>DATA PREPROCESSING<\/h4>","845d941b":"__Marital Status vs Subscription__","5eb07e6d":"<h4>VISUALIZATION<\/h4>","7cfc39e0":"**BALANCING THE DATASET**","edf7b12f":"__Month vs Subscription__","33784686":"<h4>FITTING MODEL<\/h4>","766c02ad":"We observe that the data is highly imbalanced, however we need a balanced data only for training.","1cec1906":"<h4>TRAIN TEST SPLIT<\/h4>"}}