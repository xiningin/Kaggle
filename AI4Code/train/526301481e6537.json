{"cell_type":{"347992e9":"code","c4c2edca":"code","17ddbcea":"code","ec8fb50c":"code","6663f644":"code","8b092930":"code","655d020b":"code","1263fa45":"code","27a708ad":"code","211ccfd2":"code","7684c863":"code","559a57a9":"code","0189ba34":"code","c611ee8b":"code","5232a8f3":"code","6e5c601a":"code","b0a64720":"code","1c870db2":"code","2482514f":"code","feab8862":"code","7e139ef8":"code","108c8cf9":"code","8a40daec":"code","c4b572ec":"code","2eeaf692":"code","8d95fe1b":"code","5c726540":"code","6b0cf5a8":"code","707a6eab":"markdown","344ab511":"markdown","cdaf8978":"markdown","1e73181d":"markdown","d63c18f0":"markdown","d7c6fa57":"markdown","041af985":"markdown","be4795dc":"markdown","0f673663":"markdown","0e629ec2":"markdown","eb1bcdb8":"markdown","ca2a935a":"markdown","fd5a867d":"markdown","c4f9108e":"markdown","88ed334f":"markdown","9c807c1d":"markdown"},"source":{"347992e9":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix","c4c2edca":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ntrain = pd.read_csv(os.path.join(dirname, filename))\ntrain.head()","17ddbcea":"train = train.drop([\"Unnamed: 32\"], axis=1)\ntrain.shape","ec8fb50c":"# Heatmap to mark all the missing values in white colour.\nsns.heatmap(train.isnull(), cbar=False)","6663f644":"train[\"diagnosis\"].replace({\"M\":2, \"B\":1}, inplace=True)\ntrain.head()","8b092930":"train_corr = train.corr()","655d020b":"plt.subplots(figsize=(30, 30))\nax = sns.heatmap(\n    train_corr,\n    vmin = -1.0, vmax = 1.0, center=0,\n    cmap = sns.diverging_palette(20, 220),\n    annot = True,\n    square = True\n)\n\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    horizontalalignment='right'\n)","1263fa45":"corr_target = abs(train_corr[\"diagnosis\"])\nfeatures = corr_target[corr_target>=0.5]\nfeatures = features.keys()\nfeatures = features.delete(0)\nfeatures = features.tolist()\nfeatures","27a708ad":"X = train[features]\nX.head()","211ccfd2":"y = train[\"diagnosis\"]\ny.head()","7684c863":"# 80% - 20% Split\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.20, random_state=42, shuffle=False)\n\n# 70% - 30% Split\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.30, random_state=42)\n\nalgo = [\"XGBoost Classifier\", \"Random Forest Classifier\", \"Logistic Regression\", \"AdaBoost Classifier\", \"Gradient Boosting\", \"Bagging Classifier\", \"CatBoost Classifier\"]\naccuracy1=[]\nprecision1 = []\nrecall1 = []\nf1_score1 = []\n\naccuracy2=[]\nprecision2 = []\nrecall2 = []\nf1_score2 = []","559a57a9":"# Model 1 with 80% - 20% split\nxg_model1 = XGBClassifier()\nxg_model1.fit(X_train1, y_train1)\ny_pred_xg1 = xg_model1.predict(X_test1)\n\n# Model 2 with 70% - 30% split\nxg_model2 = XGBClassifier()\nxg_model2.fit(X_train2, y_train2)\ny_pred_xg2 = xg_model2.predict(X_test2)\n\n# The statement below is meant to be used for continuous target variable not categorical\n# predictions = [round(value) for value in y_pred]\n\n# Calculating Evaluation Metrics for the Model 1\nxg_accuracy1 = accuracy_score(y_test1, y_pred_xg1) * 100\nxg_confusion1 = confusion_matrix(y_test1, y_pred_xg1)\nxg_precision1 = xg_confusion1[0][0]\/(xg_confusion1[0][0] + xg_confusion1[1][0]) * 100\nxg_recall1 = xg_confusion1[0][0]\/(xg_confusion1[0][0] + xg_confusion1[0][1]) * 100\nxg_f1_score1 = ((2 * xg_precision1 * xg_recall1) \/ (xg_precision1 + xg_recall1)) \/ 100\n\n# Calculating Evaluation Metrics for the Model 2\nxg_accuracy2 = accuracy_score(y_test2, y_pred_xg2) * 100\nxg_confusion2 = confusion_matrix(y_test2, y_pred_xg2)\nxg_precision2 = xg_confusion2[0][0]\/(xg_confusion2[0][0] + xg_confusion2[1][0]) * 100\nxg_recall2 = xg_confusion2[0][0]\/(xg_confusion2[0][0] + xg_confusion2[0][1]) * 100\nxg_f1_score2 = ((2 * xg_precision2 * xg_recall2) \/ (xg_precision2 + xg_recall2)) \/ 100\n\n# Storing all the metrics in values for Model 1 in common lists\naccuracy1.append(round(xg_accuracy1, 2))\nprecision1.append(round(xg_precision1, 2))\nrecall1.append(round(xg_recall1, 2))\nf1_score1.append(round(xg_f1_score1, 4))\n\n# Storing all the metrics in values for Model 2 in common lists\naccuracy2.append(round(xg_accuracy2, 2))\nprecision2.append(round(xg_precision2, 2))\nrecall2.append(round(xg_recall2, 2))\nf1_score2.append(round(xg_f1_score2, 4))","0189ba34":"# Evaluation Metrics for the Model 1\nprint(\"Results for the 80 - 20 split\")\nprint(\"Accuracy:\", xg_accuracy1)\nprint(\"Precision:\", xg_precision1)\nprint(\"Recall:\", xg_recall1)\nprint(\"F1 Score:\", xg_f1_score1)\n\nprint(\"----------------------------------\")\nprint(\"----------------------------------\")\n\n# Evaluation Metrics for the Model 2\nprint(\"Results for the 70 - 30 split\")\nprint(\"Accuracy:\", xg_accuracy2)\nprint(\"Precision:\", xg_precision2)\nprint(\"Recall:\", xg_recall2)\nprint(\"F1 Score:\", xg_f1_score2)\n","c611ee8b":"# Model 1 with 80% - 20% split\nrfc_model1 = RandomForestClassifier()\nrfc_model1.fit(X_train1, y_train1)\ny_pred_rf1 = rfc_model1.predict(X_test1)\n\n# Model 2 with 70% - 30% split\nrfc_model2 = RandomForestClassifier()\nrfc_model2.fit(X_train2, y_train2)\ny_pred_rf2 = rfc_model2.predict(X_test2)\n\n# Calculating Evaluation Metrics for the Model 1\nrf_accuracy1 = accuracy_score(y_test1, y_pred_rf1) * 100\nrf_confusion1 = confusion_matrix(y_test1, y_pred_rf1)\nrf_precision1 = xg_confusion1[0][0]\/(rf_confusion1[0][0] + rf_confusion1[1][0]) * 100\nrf_recall1 = xg_confusion1[0][0]\/(rf_confusion1[0][0] + rf_confusion1[0][1]) * 100\nrf_f1_score1 = ((2 * rf_precision1 * rf_recall1) \/ (rf_precision1 + rf_recall1)) \/ 100\n\n# Calculating Evaluation Metrics for the Model 2\nrf_accuracy2 = accuracy_score(y_test2, y_pred_rf2) * 100\nrf_confusion2 = confusion_matrix(y_test2, y_pred_rf2)\nrf_precision2 = xg_confusion2[0][0]\/(rf_confusion2[0][0] + rf_confusion2[1][0]) * 100\nrf_recall2 = xg_confusion2[0][0]\/(rf_confusion2[0][0] + rf_confusion2[0][1]) * 100\nrf_f1_score2 = ((2 * rf_precision2 * rf_recall2) \/ (rf_precision2 + rf_recall2)) \/ 100\n\n# Storing all the metrics in values for Model 1 in common lists\naccuracy1.append(round(rf_accuracy1, 2))\nprecision1.append(round(rf_precision1, 2))\nrecall1.append(round(rf_recall1, 2))\nf1_score1.append(round(rf_f1_score1, 4))\n\n# Storing all the metrics in values for Model 2 in common lists\naccuracy2.append(round(rf_accuracy2, 2))\nprecision2.append(round(rf_precision2, 2))\nrecall2.append(round(rf_recall2, 2))\nf1_score2.append(round(rf_f1_score2, 4))","5232a8f3":"# Evaluation Metrics for the Model 1\nprint(\"Results for the 80% - 20% split\")\nprint(\"Accuracy:\", rf_accuracy1)\nprint(\"Precision:\", rf_precision1)\nprint(\"Recall:\", rf_recall1)\nprint(\"F1 Score:\", rf_f1_score1)\n\nprint(\"----------------------------------\")\nprint(\"----------------------------------\")\n\n# Evaluation Metrics for the Model 2\nprint(\"Results for the 70% - 30% split\")\nprint(\"Accuracy:\", rf_accuracy2)\nprint(\"Precision:\", rf_precision2)\nprint(\"Recall:\", rf_recall2)\nprint(\"F1 Score:\", rf_f1_score2)","6e5c601a":"# Model 1 with 80% - 20% split\nlr_model1 = LogisticRegression(class_weight='dict', max_iter=500, random_state=42)\nlr_model1.fit(X_train1, y_train1)\ny_pred_lr1 = lr_model1.predict(X_test1)\n\n# Model 2 with 70% - 30% split\nlr_model2 = LogisticRegression(solver = 'liblinear', max_iter=300, random_state=42)\nlr_model2.fit(X_train2, y_train2)\ny_pred_lr2 = lr_model2.predict(X_test2)\n\n# Calculating Evaluation Metrics for the Model 1\nlr_accuracy1 = accuracy_score(y_test1, y_pred_lr1) * 100\nlr_confusion1 = confusion_matrix(y_test1, y_pred_lr1)\nlr_precision1 = lr_confusion1[0][0]\/(lr_confusion1[0][0] + lr_confusion1[1][0]) * 100\nlr_recall1 = lr_confusion1[0][0]\/(lr_confusion1[0][0] + lr_confusion1[0][1]) * 100\nlr_f1_score1 = ((2 * lr_precision1 * lr_recall1) \/ (lr_precision1 + lr_recall1)) \/ 100\n\n# Calculating Evaluation Metrics for the Model 2\nlr_accuracy2 = accuracy_score(y_test2, y_pred_lr2) * 100\nlr_confusion2 = confusion_matrix(y_test2, y_pred_lr2)\nlr_precision2 = lr_confusion2[0][0]\/(lr_confusion2[0][0] + lr_confusion2[1][0]) * 100\nlr_recall2 = lr_confusion2[0][0]\/(lr_confusion2[0][0] + lr_confusion2[0][1]) * 100\nlr_f1_score2 = ((2 * lr_precision2 * lr_recall2) \/ (lr_precision2 + lr_recall2)) \/ 100\n\n# Storing all the metrics in values for Model 1 in common lists\naccuracy1.append(round(lr_accuracy1, 2))\nprecision1.append(round(lr_precision1, 2))\nrecall1.append(round(lr_recall1, 2))\nf1_score1.append(round(lr_f1_score1, 4))\n\n# Storing all the metrics in values for Model 2 in common lists\naccuracy2.append(round(lr_accuracy2, 2))\nprecision2.append(round(lr_precision2, 2))\nrecall2.append(round(lr_recall2, 2))\nf1_score2.append(round(lr_f1_score2, 4))","b0a64720":"# Evaluation Metrics for the Model 1\nprint(\"Results for the 80% - 20% split\")\nprint(\"Accuracy:\", lr_accuracy1)\nprint(\"Precision:\", lr_precision1)\nprint(\"Recall:\", lr_recall1)\nprint(\"F1 Score:\", lr_f1_score1)\n\nprint(\"----------------------------------\")\nprint(\"----------------------------------\")\n\n# Evaluation Metrics for the Model 2\nprint(\"Results for the 70% - 30% split\")\nprint(\"Accuracy:\", lr_accuracy2)\nprint(\"Precision:\", lr_precision2)\nprint(\"Recall:\", lr_recall2)\nprint(\"F1 Score:\", lr_f1_score2)","1c870db2":"# Model 1 with 80% - 20% split\nab_model1 = AdaBoostClassifier(n_estimators=500, learning_rate=0.1, random_state=42)\nab_model1.fit(X_train1, y_train1)\ny_pred_ab1 = ab_model1.predict(X_test1)\n\n# Model 2 with 70% - 30% split\nab_model2 = AdaBoostClassifier(n_estimators=500, learning_rate=0.1, random_state=42)\nab_model2.fit(X_train2, y_train2)\ny_pred_ab2 = ab_model2.predict(X_test2)\n\n# Calculating Evaluation Metrics for the Model 1\nab_accuracy1 = accuracy_score(y_test1, y_pred_ab1) * 100\nab_confusion1 = confusion_matrix(y_test1, y_pred_ab1)\nab_precision1 = ab_confusion1[0][0]\/(ab_confusion1[0][0] + ab_confusion1[1][0]) * 100\nab_recall1 = ab_confusion1[0][0]\/(ab_confusion1[0][0] + ab_confusion1[0][1]) * 100\nab_f1_score1 = ((2 * ab_precision1 * ab_recall1) \/ (ab_precision1 + ab_recall1)) \/ 100\n\n# Calculating Evaluation Metrics for the Model 2\nab_accuracy2 = accuracy_score(y_test2, y_pred_ab2) * 100\nab_confusion2 = confusion_matrix(y_test2, y_pred_ab2)\nab_precision2 = ab_confusion2[0][0]\/(ab_confusion2[0][0] + ab_confusion2[1][0]) * 100\nab_recall2 = ab_confusion2[0][0]\/(ab_confusion2[0][0] + ab_confusion2[0][1]) * 100\nab_f1_score2 = ((2 * ab_precision2 * ab_recall2) \/ (ab_precision2 + ab_recall2)) \/ 100\n\n# Storing all the metrics in values for Model 1 in common lists\naccuracy1.append(round(ab_accuracy1, 2))\nprecision1.append(round(ab_precision1, 2))\nrecall1.append(round(ab_recall1, 2))\nf1_score1.append(round(ab_f1_score1, 4))\n\n# Storing all the metrics in values for Model 2 in common lists\naccuracy2.append(round(ab_accuracy2, 2))\nprecision2.append(round(ab_precision2, 2))\nrecall2.append(round(ab_recall2, 2))\nf1_score2.append(round(ab_f1_score2, 4))","2482514f":"# Evaluation Metrics for the Model 1\nprint(\"Results for the 80% - 20% split\")\nprint(\"Accuracy:\", ab_accuracy1)\nprint(\"Precision:\", ab_precision1)\nprint(\"Recall:\", ab_recall1)\nprint(\"F1 Score:\", ab_f1_score1)\n\nprint(\"----------------------------------\")\nprint(\"----------------------------------\")\n\n# Evaluation Metrics for the Model 2\nprint(\"Results for the 70% - 30% split\")\nprint(\"Accuracy:\", ab_accuracy2)\nprint(\"Precision:\", ab_precision2)\nprint(\"Recall:\", ab_recall2)\nprint(\"F1 Score:\", ab_f1_score2)","feab8862":"# Model 1 with 80% - 20% split\ngb_model1 = GradientBoostingClassifier()\ngb_model1.fit(X_train1, y_train1)\ny_pred_gb1 = gb_model1.predict(X_test1)\n\n# Model 2 with 70% - 30% split\ngb_model2 = GradientBoostingClassifier()\ngb_model2.fit(X_train2, y_train2)\ny_pred_gb2 = gb_model2.predict(X_test2)\n\n# Calculating Evaluation Metrics for the Model 1\ngb_accuracy1 = accuracy_score(y_test1, y_pred_gb1) * 100\ngb_confusion1 = confusion_matrix(y_test1, y_pred_gb1)\ngb_precision1 = gb_confusion1[0][0]\/(gb_confusion1[0][0] + gb_confusion1[1][0]) * 100\ngb_recall1 = gb_confusion1[0][0]\/(gb_confusion1[0][0] + gb_confusion1[0][1]) * 100\ngb_f1_score1 = ((2 * gb_precision1 * gb_recall1) \/ (gb_precision1 + gb_recall1)) \/ 100\n\n# Calculating Evaluation Metrics for the Model 2\ngb_accuracy2 = accuracy_score(y_test2, y_pred_gb2) * 100\ngb_confusion2 = confusion_matrix(y_test2, y_pred_gb2)\ngb_precision2 = gb_confusion2[0][0]\/(gb_confusion2[0][0] + gb_confusion2[1][0]) * 100\ngb_recall2 = gb_confusion2[0][0]\/(gb_confusion2[0][0] + gb_confusion2[0][1]) * 100\ngb_f1_score2 = ((2 * gb_precision2 * gb_recall2) \/ (gb_precision2 + gb_recall2)) \/ 100\n\n# Storing all the metrics in values for Model 1 in common lists\naccuracy1.append(round(gb_accuracy1, 2))\nprecision1.append(round(gb_precision1, 2))\nrecall1.append(round(gb_recall1, 2))\nf1_score1.append(round(gb_f1_score1, 4))\n\n# Storing all the metrics in values for Model 2 in common lists\naccuracy2.append(round(gb_accuracy2, 2))\nprecision2.append(round(gb_precision2, 2))\nrecall2.append(round(gb_recall2, 2))\nf1_score2.append(round(gb_f1_score2, 4))","7e139ef8":"# Evaluation Metrics for the Model 1\nprint(\"Results for the 80% - 20% split\")\nprint(\"Accuracy:\", gb_accuracy1)\nprint(\"Precision:\", gb_precision1)\nprint(\"Recall:\", gb_recall1)\nprint(\"F1 Score:\", gb_f1_score1)\n\nprint(\"----------------------------------\")\nprint(\"----------------------------------\")\n\n# Evaluation Metrics for the Model 2\nprint(\"Results for the 70% - 30% split\")\nprint(\"Accuracy:\", gb_accuracy2)\nprint(\"Precision:\", gb_precision2)\nprint(\"Recall:\", gb_recall2)\nprint(\"F1 Score:\", gb_f1_score2)","108c8cf9":"# Model 1 with 80% - 20% split\nbc_model1 = BaggingClassifier(n_estimators=200, random_state=42)\nbc_model1.fit(X_train1, y_train1)\ny_pred_bc1 = bc_model1.predict(X_test1)\n\n# Model 2 with 70% - 30% split\nbc_model2 = BaggingClassifier(n_estimators=200, random_state=42)\nbc_model2.fit(X_train2, y_train2)\ny_pred_bc2 = bc_model2.predict(X_test2)\n\n\n# Calculating Evaluation Metrics for the Model 1\nbc_accuracy1 = accuracy_score(y_test1, y_pred_bc1) * 100\nbc_confusion1 = confusion_matrix(y_test1, y_pred_bc1)\nbc_precision1 = bc_confusion1[0][0]\/(bc_confusion1[0][0] + bc_confusion1[1][0]) * 100\nbc_recall1 = bc_confusion1[0][0]\/(bc_confusion1[0][0] + bc_confusion1[0][1]) * 100\nbc_f1_score1 = ((2 * bc_precision1 * bc_recall1) \/ (bc_precision1 + bc_recall1)) \/ 100\n\n# Calculating Evaluation Metrics for the Model 2\nbc_accuracy2 = accuracy_score(y_test2, y_pred_bc2) * 100\nbc_confusion2 = confusion_matrix(y_test2, y_pred_bc2)\nbc_precision2 = bc_confusion2[0][0]\/(bc_confusion2[0][0] + bc_confusion2[1][0]) * 100\nbc_recall2 = bc_confusion2[0][0]\/(bc_confusion2[0][0] + bc_confusion2[0][1]) * 100\nbc_f1_score2 = ((2 * bc_precision2 * bc_recall2) \/ (bc_precision2 + bc_recall2)) \/ 100\n\n# Storing all the metrics in values for Model 1 in common lists\naccuracy1.append(round(bc_accuracy1, 2))\nprecision1.append(round(bc_precision1, 2))\nrecall1.append(round(bc_recall1, 2))\nf1_score1.append(round(bc_f1_score1, 4))\n\n# Storing all the metrics in values for Model 2 in common lists\naccuracy2.append(round(bc_accuracy2, 2))\nprecision2.append(round(bc_precision2, 2))\nrecall2.append(round(bc_recall2, 2))\nf1_score2.append(round(bc_f1_score2, 4))","8a40daec":"# Evaluation Metrics for the Model 1\nprint(\"Results for the 80% - 20% split\")\nprint(\"Accuracy:\", bc_accuracy1)\nprint(\"Precision:\", bc_precision1)\nprint(\"Recall:\", bc_recall1)\nprint(\"F1 Score:\", bc_f1_score1)\n\nprint(\"----------------------------------\")\nprint(\"----------------------------------\")\n\n# Evaluation Metrics for the Model 2\nprint(\"Results for the 70% - 30% split\")\nprint(\"Accuracy:\", bc_accuracy2)\nprint(\"Precision:\", bc_precision2)\nprint(\"Recall:\", bc_recall2)\nprint(\"F1 Score:\", bc_f1_score2)","c4b572ec":"# Model 1 with 80% - 20% split\ncb_model1 = CatBoostClassifier(iterations=200, logging_level='Silent')\ncb_model1.fit(X_train1, y_train1)\ny_pred_cb1 = cb_model1.predict(X_test1)\n\n# Model 2 with 70% - 30% split\ncb_model2 = CatBoostClassifier(iterations=200, logging_level='Silent')\ncb_model2.fit(X_train2, y_train2)\ny_pred_cb2 = cb_model2.predict(X_test2)\n\n# Calculating Evaluation Metrics for the Model 1\ncb_accuracy1 = accuracy_score(y_test1, y_pred_cb1) * 100\ncb_confusion1 = confusion_matrix(y_test1, y_pred_cb1)\ncb_precision1 = cb_confusion1[0][0]\/(cb_confusion1[0][0] + cb_confusion1[1][0]) * 100\ncb_recall1 = cb_confusion1[0][0]\/(cb_confusion1[0][0] + cb_confusion1[0][1]) * 100\ncb_f1_score1 = ((2 * cb_precision1 * cb_recall1) \/ (cb_precision1 + cb_recall1)) \/ 100\n\n# Calculating Evaluation Metrics for the Model 2\ncb_accuracy2 = accuracy_score(y_test2, y_pred_cb2) * 100\ncb_confusion2 = confusion_matrix(y_test2, y_pred_cb2)\ncb_precision2 = cb_confusion2[0][0]\/(cb_confusion2[0][0] + cb_confusion2[1][0]) * 100\ncb_recall2 = cb_confusion2[0][0]\/(cb_confusion2[0][0] + cb_confusion2[0][1]) * 100\ncb_f1_score2 = ((2 * cb_precision2 * cb_recall2) \/ (cb_precision2 + cb_recall2)) \/ 100\n\n# Storing all the metrics in values for Model 1 in common lists\naccuracy1.append(round(cb_accuracy1, 2))\nprecision1.append(round(cb_precision1, 2))\nrecall1.append(round(cb_recall1, 2))\nf1_score1.append(round(cb_f1_score1, 4))\n\n# Storing all the metrics in values for Model 2 in common lists\naccuracy2.append(round(cb_accuracy2, 2))\nprecision2.append(round(cb_precision2, 2))\nrecall2.append(round(cb_recall2, 2))\nf1_score2.append(round(cb_f1_score2, 4))\n\n# To see the hyper-parameters for this model use \"cb_model.get_all_params()\"","2eeaf692":"# Evaluation Metrics for the Model 1\nprint(\"Results for the 80% - 20% split\")\nprint(\"Accuracy Score:\", cb_accuracy1)\nprint(\"Precision:\", cb_precision1)\nprint(\"Recall:\", cb_recall1)\nprint(\"F1 Score:\", cb_f1_score1)\n\nprint(\"----------------------------------\")\nprint(\"----------------------------------\")\n\n# Evaluation Metrics for the Model 2\nprint(\"Results for the 70% - 30% split\")\nprint(\"Accuracy Score:\", cb_accuracy2)\nprint(\"Precision:\", cb_precision2)\nprint(\"Recall:\", cb_recall2)\nprint(\"F1 Score:\", cb_f1_score2)","8d95fe1b":"metric1 = pd.DataFrame({\n    'Alogrithms':algo,\n    'Accuracy':accuracy1,\n    'Precision':precision1,\n    'Recall':recall1,\n    'F1 Score':f1_score1\n})\n\nmetric2 = pd.DataFrame({\n    'Alogrithms':algo,\n    'Accuracy':accuracy2,\n    'Precision':precision2,\n    'Recall':recall2,\n    'F1 Score':f1_score2\n})","5c726540":"metric1","6b0cf5a8":"metric2","707a6eab":"#### Metrics in the 80% - 20% setting","344ab511":"## Creating and training models\nThese are the following algorithms that are used in this exercise.\n1. XGBoost Classifier\n2. Random Forest Classifier\n3. Logistic Regression \n4. AdaBoost Classifier\n5. Gradient Boosting\n6. Bagging Classifier\n7. CatBoost Classifier","cdaf8978":"### Conclusion\n<p>In my opinion I think that choosing the 80-20 setting is always a better in small or medium sized datasets. But I have observed that in 70-30 setting Logistic Regression has outperformed all the algorithms irrespective of the settings by a very narrow margin.<\/p>\nOverall it would be a good idea to choose XGBoost Classifier in 80-20 setting.","1e73181d":"### Splitting the dataset into train and test dataset.\n- The dataset is split into Train and Test dataset in 2 ways:\n - First is in a ratio of 80% - 20%.\n - Second is in a ratio of 70% - 20%.","d63c18f0":"### 6) Bagging Classifier","d7c6fa57":"### 1) XGBoost Classifier","041af985":"### 7) CatBoost Classifier","be4795dc":"## Breast Cancer Diagnostic Classification Models and their performace comparison\n- The following notebook contains exploratory data analysis and ML classification models to classify if a cancer is benign or malignant \n- Dataset used: Breast Cancer Wisconsin (Diagnostic) Data Set (https:\/\/www.kaggle.com\/uciml\/breast-cancer-wisconsin-data).\n- In this notebook we are going to compare the various ML classification algorithms using metrics like:\n - Classification Accuracy\n - Confusion Matrix\n - Precision and Recall\n - F1 score","0f673663":"### 5) Gradient Boosting Classifier","0e629ec2":"#### Metrics in the 70% - 30% setting","eb1bcdb8":"### Heatmap to show the correlation matrix of the features.","ca2a935a":"### 3) Logistic Regression","fd5a867d":"### Selecting features\nThe approach is to choose the features that the correlation of >=0.5 w.r.t. diagnosis","c4f9108e":"### 4) AdaBoost Classifier","88ed334f":"### 2) Random Forest Classifier","9c807c1d":"### Tabular Performance Comparison of all the algorithm w.r.t. the train-test data split"}}