{"cell_type":{"5aff12c9":"code","f7567e1b":"code","3bd25e78":"code","2a34eec9":"code","c32b9ed5":"code","db92771d":"code","58725181":"code","7b946b93":"code","6fe80d53":"code","f2bd749c":"code","77f16672":"code","541cd645":"code","e6b61dca":"code","060c41b1":"code","4c8da889":"code","395cc874":"code","3b4812f1":"code","ec325c33":"code","bcf9583b":"code","1e7df246":"code","6e16824a":"code","43432935":"code","f7e53263":"code","3d7e2017":"code","9422ae93":"code","5cae6387":"code","61f71b13":"code","9b049577":"code","e84b2d27":"code","05b48b15":"code","33664b62":"markdown","b314709c":"markdown","7cd7e9a1":"markdown","ff14e4df":"markdown"},"source":{"5aff12c9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import confusion_matrix # We'll use a lot of times it!\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","f7567e1b":"Cancer = pd.read_csv('..\/input\/Prostate_Cancer.csv')","3bd25e78":"Cancer.info()","2a34eec9":"Cancer.head(10)","c32b9ed5":"Cancer.tail()","db92771d":"Cancer.describe()","58725181":"Cancer.columns","7b946b93":"# We don't care id of the columns. So, we drop that!\nCancer.drop(['id'],axis=1,inplace=True)","6fe80d53":"Cancer.head()","f2bd749c":"# diagnosis_result is the most important column for us. Because we'll classify datas depend on this column.\n# We have to integers for classification. Therefore, we must convert them from object to integer.\nCancer.diagnosis_result = [1 if each == 'M' else 0 for each in Cancer.diagnosis_result]","77f16672":"# Let's check it.\nCancer.diagnosis_result.value_counts()\n# And then, we assigned 1 and 0 to M and B. Let's some classification!","541cd645":"# We should assign x and y values for test-train datas split.\ny = Cancer.diagnosis_result.values\nx_data = Cancer.drop(['diagnosis_result'],axis=1)","e6b61dca":"# See our values\ny","060c41b1":"x_data.head()","4c8da889":"# Normalization: Normalization means all of the values of data, scale between 0 and 1.\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range=(0,1))\nx = scaler.fit_transform(x_data)","395cc874":"x","3b4812f1":"# We are ready to split datas as train and test.\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)\n#%40 data will assign as 'Test Datas'\nmethod_names=[] # In Conclusion part, I'll try to show you which method gave the best result.\nmethod_scores=[]","ec325c33":"# Let's look at new values.\nx_train","bcf9583b":"# Firstly, we start with Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nlog_reg = LogisticRegression()\nlog_reg.fit(x_train, y_train) #Fitting\nprint(\"Logistic Regression Classification Test Accuracy {}\".format(log_reg.score(x_test,y_test)))\nmethod_names.append(\"Logistic Reg.\")\nmethod_scores.append(log_reg.score(x_test,y_test))\n\n#Confusion Matrix\ny_pred = log_reg.predict(x_test)\nconf_mat = confusion_matrix(y_test,y_pred)\n#Visualization Confusion Matrix\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(conf_mat,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"True Values\")\nplt.show()","1e7df246":"# Continue with; KNN Classification!\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=5)  # 5 is optional.\nknn.fit(x_train,y_train)\nprint(\"Score for Number of Neighbors = 5: {}\".format(knn.score(x_test,y_test)))\nmethod_names.append(\"KNN\")\nmethod_scores.append(knn.score(x_test,y_test))\n\n#Confusion Matrix\ny_pred = knn.predict(x_test)\nconf_mat = confusion_matrix(y_test,y_pred)\n#Visualization Confusion Matrix\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(conf_mat,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"True Values\")\nplt.show()","6e16824a":"# SVM!\nfrom sklearn.svm import SVC\nsvm = SVC(random_state=42)\nsvm.fit(x_train,y_train)\nprint(\"SVM Classification Score is: {}\".format(svm.score(x_test,y_test)))\nmethod_names.append(\"SVM\")\nmethod_scores.append(svm.score(x_test,y_test))\n\n#Confusion Matrix\ny_pred = svm.predict(x_test)\nconf_mat = confusion_matrix(y_test,y_pred)\n#Visualization Confusion Matrix\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(conf_mat,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"True Values\")\nplt.show()","43432935":"# Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nnaive_bayes = GaussianNB()\nnaive_bayes.fit(x_test,y_test)\nprint(\"Naive Bayes Classification Score: {}\".format(naive_bayes.score(x_test,y_test)))\nmethod_names.append(\"Naive Bayes\")\nmethod_scores.append(naive_bayes.score(x_test,y_test))\n\n#Confusion Matrix\ny_pred = naive_bayes.predict(x_test)\nconf_mat = confusion_matrix(y_test,y_pred)\n#Visualization Confusion Matrix\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(conf_mat,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"True Values\")\nplt.show()","f7e53263":"# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\ndec_tree = DecisionTreeClassifier()\ndec_tree.fit(x_train,y_train)\nprint(\"Decision Tree Classification Score: \",dec_tree.score(x_test,y_test))\nmethod_names.append(\"Decision Tree\")\nmethod_scores.append(dec_tree.score(x_test,y_test))\n\n#Confusion Matrix\ny_pred = dec_tree.predict(x_test)\nconf_mat = confusion_matrix(y_test,y_pred)\n#Visualization Confusion Matrix\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(conf_mat,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"True Values\")\nplt.show()","3d7e2017":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nrand_forest = RandomForestClassifier(n_estimators=100, random_state=42)\nrand_forest.fit(x_train,y_train)\nprint(\"Random Forest Classification Score: \",rand_forest.score(x_test,y_test))\nmethod_names.append(\"Random Forest\")\nmethod_scores.append(rand_forest.score(x_test,y_test))\n\n#Confusion Matrix\ny_pred = rand_forest.predict(x_test)\nconf_mat = confusion_matrix(y_test,y_pred)\n#Visualization Confusion Matrix\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(conf_mat,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"True Values\")\nplt.show()","9422ae93":"# ANN!\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom keras.models import Sequential # initialize neural network library\nfrom keras.layers import Dense # build our layers library\ndef build_classifier():\n    classifier = Sequential() # initialize neural network\n    classifier.add(Dense(units = 50, kernel_initializer = 'uniform', activation = 'relu', input_dim = x_train.shape[1]))\n    classifier.add(Dense(units = 10, kernel_initializer = 'uniform', activation = 'relu'))\n    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    return classifier\nclassifier = KerasClassifier(build_fn = build_classifier, epochs = 200)\naccuracies = cross_val_score(estimator = classifier, X = x_train, y = y_train, cv = 3)\nmean = accuracies.mean()\nvariance = accuracies.std()\nprint(\"Accuracy mean: \"+ str(mean))\nprint(\"Accuracy variance: \"+ str(variance))","5cae6387":"method_names.append(\"ANN\")\nmethod_scores.append(0.851)","61f71b13":"trainX = np.reshape(x_train, (x_train.shape[0], x_train.shape[1],1))\ntestX = np.reshape(x_test, (x_test.shape[0],x_test.shape[1],1))\n# Print and check shapes\nprint(\"Shape of trainX is {}\".format(trainX.shape))\nprint(\"Shape of testX is {}\".format(testX.shape))","9b049577":"from keras.layers import Dense, SimpleRNN, Dropout\nfrom keras.metrics import mean_squared_error\nfrom keras.models import Sequential\nmodel = Sequential()\n# Add the first layer and Dropout regularization\nmodel.add(SimpleRNN(units=100,activation='tanh',return_sequences=True, \n                    input_shape=(trainX.shape[1],1)))\nmodel.add(Dropout(0.20))\n# Second layer and Dropout regularization\nmodel.add(SimpleRNN(units = 100, activation='tanh',return_sequences=True))\nmodel.add(Dropout(0.20))\n# Third layer and Dropout regularization\nmodel.add(SimpleRNN(units = 70, activation='tanh', return_sequences= True))\nmodel.add(Dropout(0.20))\n# Fourth layer and Dropout regularization\nmodel.add(SimpleRNN(units = 50))\nmodel.add(Dropout(0.20))\n# Add final or output layer\nmodel.add(Dense(units=1))\n\n# Compile our RNN model\nmodel.compile(optimizer = 'adam', loss = 'mean_squared_error',metrics = ['accuracy'])\n# Fitting the RNN to the training set\nmodel.fit(trainX, y_train, epochs = 200, batch_size=32)\n# Remember; epochs, batch_size etc. are just some of hyper parameters. \n# You can change these parameters whatever you want\nmean = accuracies.mean()\nvariance = accuracies.std()\nprint(\"Accuracy mean: \"+ str(mean))\nprint(\"Accuracy variance: \"+ str(variance))","e84b2d27":"method_names.append(\"RNN\")\nmethod_scores.append(0.887)","05b48b15":"plt.figure(figsize=(15,10))\nplt.ylim([0.60,0.90])\nplt.bar(method_names,method_scores,width=0.5)\nplt.xlabel('Method Name')\nplt.ylabel('Method Score')","33664b62":"**And now time to classification!**\n","b314709c":"**CONCLUSION**\n\nWe've already completed to train our data with a lot of different method. Let's look which method is given the best result to us!\n","7cd7e9a1":"As we can see easily; RNN gave us the best result! I hope you learned something like I did. Please comment me!","ff14e4df":"**INTRODUCTION**\n\nHello! I'll train Prostate Cancer datas with some machine learning and deep learning methods. I'm believe in this course will learn a lot of thing to us. You'll see this kernel:\n\n* EDA (Exploratory Data Analysis)\n* Data Preprocessing (Scaling, Reshaping)\n* Test-Train Datas Split\n* Logistic Regression Classification\n* KNN Classification\n* Support Vector Machine (SVM) Classification\n* Naive Bayes Classification\n* Desicion Tree Classification\n* Random Forest Classification\n* Artificial Neural Network\n* Recurrent Neural Network\n* Compare all of these Classification Models\n* Conclusion"}}