{"cell_type":{"82e98e1c":"code","8cc1be34":"code","1f08510e":"code","66b979f8":"code","d74ee055":"code","01d6bc6b":"code","f65f2959":"code","79291dc3":"code","b4bb02c1":"code","56c70cb8":"code","6a0874c7":"code","1a7986ad":"code","82c0982b":"code","2d318865":"code","b28cea78":"code","802ccb8d":"code","cc1a8e12":"code","87b87678":"code","10b8d350":"code","86ee1d7f":"code","2aced087":"code","03d2b405":"code","c865545f":"markdown","38fd5251":"markdown","467c15e4":"markdown","0607ea43":"markdown","900ce2cc":"markdown","47f39bad":"markdown","ebf5e9ba":"markdown","91aa0ec3":"markdown","95a038a8":"markdown","119a3ce1":"markdown","a861141a":"markdown","134f7635":"markdown","44354531":"markdown","58c91185":"markdown","1bf22b89":"markdown","5795c771":"markdown"},"source":{"82e98e1c":"import os\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport torch, torchvision\n\nfrom PIL import Image\nfrom IPython.display import clear_output\nfrom torchvision import transforms as T\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, accuracy_score\n","8cc1be34":"np.random.seed(0)\ntorch.manual_seed(0)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n\nPATH = '\/kaggle\/input\/avito-auto-moderation\/'\nTRAIN_FILE = 'train_v2.csv'\nSUB_FILE = 'sample_submission_v2.csv'\nBATCH_SIZE = 32\nLR1 = 0.001\nLR2 = 0.0001\n\ntrain = pd.read_csv(os.path.join(PATH, TRAIN_FILE))\nsubmission = pd.read_csv(os.path.join(PATH, SUB_FILE))","1f08510e":"train.head()","66b979f8":"train.label.mean()","d74ee055":"image_file = train.image[0]\nimg = plt.imread(os.path.join(PATH, image_file))\nplt.imshow(img)","01d6bc6b":"image_file = train.image[1]\nimg = plt.imread(os.path.join(PATH, image_file))\nplt.imshow(img)","f65f2959":"MEAN = np.array([0.485, 0.456, 0.406])\nSTD = np.array([0.229, 0.224, 0.225])\nDEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n\ntrain_transforms = T.Compose([\n    T.ToPILImage(),\n    T.Resize(size=224),\n    T.CenterCrop(size=(224, 224)),\n    \n    # \u043c\u0435\u0441\u0442\u043e \u0434\u043b\u044f \u0442\u043e\u0433\u043e \u0447\u0442\u043e\u0431\u044b \u0434\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438\n    \n    T.ToTensor(),     \n    T.Normalize(MEAN, STD)\n])\n\nval_transforms = T.Compose([\n    T.ToPILImage(),\n    T.Resize(size=224),\n    T.CenterCrop(size=(224, 224)),\n    \n    # \u043d\u0443\u0436\u043d\u043e \u043b\u0438 \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0442\u044c \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u043d\u0430 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438?\n    \n    T.ToTensor(),\n    T.Normalize(MEAN, STD)\n])\n\n","79291dc3":"img = plt.imread(os.path.join(PATH, image_file))\nimg_transformed = train_transforms(img)","b4bb02c1":"plt.imshow(img_transformed.permute(1, 2, 0).numpy() * STD + MEAN)","56c70cb8":"class ImageDataset(torch.utils.data.Dataset):\n    \n    def __init__(self, images, labels, path, mode='fit'):\n        \n        super(ImageDataset).__init__()\n        self.images = images\n        self.labels = labels\n        self.path = path\n        self.mode = mode\n        \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, index):\n        \n        if self.mode == 'fit':\n            x = self.read_image(file=self.images[index])\n            y = torch.tensor([self.labels[index]])\n            return x, y\n        \n        else:\n            x = self.read_image(file=self.images[index])\n            return x\n        \n    def preprocess_image(self, img):\n        \n        if self.mode == 'fit':\n            img = train_transforms(img)\n        else:\n            img = val_transforms(img)\n        \n        return img    \n    \n    def read_image(self, file):\n        img = plt.imread(os.path.join(self.path, file))\n        img = self.preprocess_image(img)\n        return img","6a0874c7":"train_images, val_images, train_labels, val_labels = train_test_split(train.image.values, train.label.values, random_state=1)\n\ntrain_dataset = ImageDataset(train_images, train_labels, path=PATH)\nval_dataset = ImageDataset(val_images, val_labels, path=PATH)\ntest_dataset = ImageDataset(images=submission.image, labels=None, path=PATH, mode='predict')\n\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE)\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)","1a7986ad":"model = torchvision.models.resnet18(pretrained=True)\nmodel.fc = torch.nn.Sequential(torch.nn.Linear(512, 1))\nmodel = model.to(DEVICE)\nmodel.eval()\n\nfor param in model.parameters():\n    param.requires_grad = False\n\nfor param in model.layer2.parameters():\n    param.requires_grad = False\n\nfor param in model.layer3.parameters():\n    param.requires_grad = False\n\nfor param in model.layer4.parameters():\n    param.requires_grad = False\n    \nfor param in model.fc.parameters():\n    param.requires_grad = True","82c0982b":"\ndef train_model(train_dataloader, val_dataloader, model, n_epochs=5, lr=0.001):\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    criterion = torch.nn.BCEWithLogitsLoss()\n\n    losses = []\n    val_losses = []\n\n    for epoch in tqdm(range(n_epochs)):\n        \n        # eval\n        model.eval()\n        val_epoch_loss = estimate_val_loss(val_dataloader, model, criterion)\n        val_losses.append(val_epoch_loss)\n        \n        # train\n        model.train()\n        for inputs, labels in train_dataloader:\n            inputs = inputs.to(DEVICE)\n            labels = labels.to(DEVICE)\n            optimizer.zero_grad()\n\n            outputs = model(inputs)\n            preds = outputs.round()\n            loss = criterion(outputs, labels.float())\n\n            loss.backward()\n            optimizer.step()\n\n            curr_loss = loss.item() * inputs.size(0)\n            losses.append(curr_loss)\n            \n            plot_progress(losses, val_losses, epoch, len(train_dataloader))\n    \n    return model\n\n\ndef estimate_val_loss(dataloader, model, criterion):\n    \n    val_epoch_loss = 0\n    \n    for inputs, labels in dataloader:\n        with torch.no_grad():\n            inputs = inputs.to(DEVICE)\n            labels = labels.to(DEVICE)\n\n            outputs = model(inputs)\n            preds = outputs.round()\n            loss = criterion(outputs, labels.float())\n\n            val_epoch_loss += loss.item() * inputs.size(0)\n    return val_epoch_loss \/ len(dataloader)\n\n\ndef get_model_predict(dataloader, model):\n    \n    preds = []\n    \n    mode = dataloader.dataset.mode\n    dataloader.dataset.mode = 'predict'\n    \n    model.eval()\n    \n    for inputs in tqdm(dataloader):\n        \n        with torch.no_grad():\n            \n            inputs = inputs.to(DEVICE)\n\n            outputs = model(inputs)\n            batch_preds = torch.sigmoid(outputs.round())\n            preds.extend(list(batch_preds.cpu().detach().numpy()))\n            \n    dataloader.dataset.mode = mode\n    preds = np.vstack(preds)\n    return preds\n\n\ndef plot_progress(train_losses, val_losses, epoch, train_dataloader_len):\n    clear_output(True)\n    plt.figure(figsize=(12, 8))\n    plt.plot(np.arange(len(train_losses)), train_losses, label='train_loss')\n    plt.plot([i*train_dataloader_len for i in range(epoch+1)], val_losses, label='val_loss')\n    plt.legend()\n    plt.ylabel('Loss')\n    plt.xlabel('Batch number')\n    plt.show()\n","2d318865":"model = train_model(train_dataloader, val_dataloader, model, lr=LR1, n_epochs=5)","b28cea78":"val_preds = get_model_predict(val_dataloader, model)\nroc_auc_score(val_labels, val_preds)","802ccb8d":"accuracy_score(val_labels, val_preds > 0.5)","cc1a8e12":"for param in model.parameters():\n    param.requires_grad = False\n\nfor param in model.layer2.parameters():\n    param.requires_grad = True\n\nfor param in model.layer3.parameters():\n    param.requires_grad = True\n\nfor param in model.layer4.parameters():\n    param.requires_grad = True\n    \nfor param in model.fc.parameters():\n    param.requires_grad = True\n","87b87678":"model = train_model(train_dataloader, val_dataloader, model, n_epochs=2, lr=LR2)","10b8d350":"val_preds = get_model_predict(val_dataloader, model)\nroc_auc_score(val_labels, val_preds)","86ee1d7f":"accuracy_score(val_labels, val_preds>0.5)","2aced087":"test_preds = get_model_predict(test_dataloader, model)","03d2b405":"submission.score = test_preds\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","c865545f":"<img src=\"http:\/\/labelimages.avito.ru\/mlcourse_week4_augmentation.png\" style=\"width: 1200px;\">","38fd5251":"1) \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u0442\u0435 \u043a\u0430\u043a\u0438\u0435 \u0441\u043f\u043e\u0441\u043e\u0431\u044b \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u0435\u0441\u0442\u044c \u0432 torchvision.transforms, \u043f\u043e\u0445\u043e\u0434\u044f\u0442 \u043b\u0438 \u0434\u0430\u043d\u043d\u044b\u0435 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u0434\u043b\u044f \u043d\u0430\u0448\u0435\u0439 \u0437\u0430\u0434\u0430\u0447\u0438? \u0411\u044b\u0432\u0430\u044e\u0442 \u043b\u0438 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043d\u0435 \u043f\u043e\u0434\u0445\u043e\u0434\u044f\u0442 \u0434\u043b\u044f \u043e\u0434\u043d\u043e\u0439 \u0437\u0430\u0434\u0430\u0447\u0438 \u0438 \u043f\u043e\u0434\u0445\u043e\u0434\u044f\u0442 \u0434\u043b\u044f \u0434\u0440\u0443\u0433\u043e\u0439?\n\n2) \u0421\u0447\u0438\u0442\u0430\u0435\u0442\u0441\u044f, \u0447\u0442\u043e SGD \u0432 \u0441\u0440\u0435\u0434\u043d\u0435\u043c \u043d\u0430\u0445\u043e\u0434\u0438\u0442 \u0431\u043e\u043b\u0435\u0435 \u043e\u043f\u0442\u0438\u043c\u0430\u043b\u044c\u043d\u0443\u044e \u0442\u043e\u0447\u043a\u0443 \u043b\u043e\u0441\u0441 \u0444\u0443\u043d\u043a\u0446\u0438\u0438, \u0447\u0435\u043c \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u044b \u0441 \u0430\u0434\u0430\u043f\u0442\u0438\u0432\u043d\u044b\u043c \u0448\u0430\u0433\u043e\u043c (RMSprop, Adam). \u041f\u0440\u043e\u0432\u0435\u0440\u044c\u0442\u0435 \u044d\u0442\u043e \u0443\u0442\u0432\u0435\u0440\u0436\u0434\u0435\u043d\u0438\u0435 \u0432 \u043d\u0430\u0448\u0435\u0439 \u0437\u0430\u0434\u0430\u0447\u0435.","467c15e4":"## Predict in test","0607ea43":"## TRANSFORMS","900ce2cc":"<img src=\"http:\/\/labelimages.avito.ru\/mlcourse_week4_transferlearing.png\" style=\"width: 1200px;\">","47f39bad":"## SUBMIT","ebf5e9ba":"## Class examples","91aa0ec3":"### \u0420\u0430\u0431\u043e\u0442\u0430 \u043d\u0430 \u0441\u0435\u043c\u0438\u043d\u0430\u0440\u0435","95a038a8":"## MODEL (unfreeze)","119a3ce1":"### \u0414\u043e\u043c\u0430\u0448\u043d\u0435\u0435 \u0437\u0430\u0434\u0430\u043d\u0438\u0435\n\n1) \u0420\u0430\u0437\u0431\u0435\u0440\u0438\u0442\u0435 \u043c\u0435\u0442\u0440\u0438\u043a\u0443 AUC ROC","a861141a":"## Image dataset","134f7635":"## TRAIN MODEL","44354531":"## TRAIN\/VAL\/TEST","58c91185":"2) \u041f\u043e\u0441\u0442\u0430\u0440\u0430\u0439\u0442\u0435\u0441\u044c \u043a\u0430\u043a \u043c\u043e\u0436\u043d\u043e \u0441\u0438\u043b\u044c\u043d\u0435\u0435 \u0443\u043b\u0443\u0447\u0448\u0438\u0442\u044c \u0431\u0435\u0439\u0437\u043b\u0430\u0439\u043d, \u0434\u043b\u044f \u044d\u0442\u043e\u0433\u043e \u043c\u043e\u0436\u043d\u043e \u043d\u0430\u0447\u0430\u0442\u044c \u0441: \n\n* \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u0434\u0440\u0443\u0433\u043e\u0439 \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u044b: \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c resnet18 \u0441\u043b\u0438\u0448\u043a\u043e\u043c \u043f\u0440\u043e\u0441\u0442\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c?;\n* \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u0434\u0440\u0443\u0433\u043e\u0439 \u0441\u0445\u0435\u043c\u044b \u0440\u0430\u0437\u043c\u043e\u0440\u043e\u0437\u043a\u0438 \u0432\u043d\u0443\u0442\u0440\u0435\u043d\u043d\u0438\u0445 \u0441\u043b\u043e\u0451\u0432 \u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u0438;\n* \u0414\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u0438: weight_decay \/ dropout;\n* \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u0431\u043e\u043b\u0435\u0435 \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043d\u044b\u0445 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0439: \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u0442\u0435 \u0432 \u0441\u0442\u043e\u0440\u043e\u043d\u0443 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 albumentations;\n* \u041f\u043e\u0438\u0441\u043a\u0430 \u0432\u0434\u043e\u0445\u043d\u043e\u0432\u0435\u043d\u0438\u044f \u043d\u0430 \u0444\u043e\u0440\u0443\u043c\u0430\u0445 \u0434\u0440\u0443\u0433\u0438\u0445 \u0441\u043e\u0440\u0435\u0432\u043d\u043e\u0432\u0430\u043d\u0438\u0438\u0439 \u0441 \u0437\u0430\u0434\u0430\u0447\u0435\u0439 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439.\n\n3) \u0421\u0434\u0435\u043b\u0430\u0439\u0442\u0435 submit","1bf22b89":"### \u0410\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u0434\u0430\u043d\u043d\u044b\u0445","5795c771":"## MODEL (freeze all except fc)"}}