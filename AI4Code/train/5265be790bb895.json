{"cell_type":{"cf9219e1":"code","26abed96":"code","fe45e2ba":"code","543ba6c8":"code","c15dd714":"code","9c5981e6":"code","4113cb68":"code","2f5f275e":"code","405bce89":"code","af1a70f7":"code","9656e4d8":"code","7d3dcc78":"code","7b0214d6":"code","8c9f3867":"code","d9dfc0ea":"code","b04eb738":"code","bf324027":"code","fd7b6fd6":"code","6f0435a2":"code","cca6fb99":"code","077312e3":"code","eb51a18c":"code","1f8e92af":"code","e60f4286":"code","95ac7f2d":"code","d83b2320":"code","f96833cc":"code","71b3eb60":"code","62c806a5":"code","71933fa8":"code","c3a61798":"code","9c2fa7d6":"code","66c6d209":"code","fa8f50af":"markdown"},"source":{"cf9219e1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","26abed96":"import tensorflow as tf\nfrom tensorflow import keras\nfrom keras.utils import plot_model\n\nfrom nltk.corpus import stopwords\nimport re\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nimport urllib.request\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"darkgrid\")","fe45e2ba":"n_rows = 100000\n#\uc6d0\ub798\ub294 200000\ndf = pd.read_csv(\"..\/input\/amazon-fine-food-reviews\/Reviews.csv\",nrows=n_rows)\nprint(df.shape)\ndf.head(5)","543ba6c8":"df = df[['Text','Summary']]\ndf.sample(5)","c15dd714":"print(f\"Text Column unique Rows >> {df['Text'].nunique()}\")\nprint(f\"Summary Column unique Rows >> {df['Summary'].nunique()}\")","9c5981e6":"df.drop_duplicates(subset=['Text'],inplace=True)\nprint(\"Dropped duplicate sentences in Summary Column!!\\n\")\nprint(f\"Shape after dropping duplicates >> {df.shape[0]}\")","4113cb68":"print(df.isnull().sum())\ndf.dropna(inplace=True)\nprint('\\n')\nprint(df.isnull().sum())","2f5f275e":"stop_words = set(stopwords.words('english'))\ncontractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\", \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n\ndef preprocess_text(sentence,drop_stop):\n    sentence = sentence.lower()\n    sentence = re.sub(r'<[^>]*>','',sentence)\n    sentence = re.sub(r'[()]','',sentence)\n    sentence = re.sub('\"','',sentence)\n    sentence = re.sub(\"'\",'',sentence)\n    sentence = re.sub(r\"'s\\b\",' ',sentence)\n    sentence = re.sub(r\"[^a-zA-Z0-9]\",' ',sentence)\n    \n    sentence = ' '.join(contractions[word] if word in contractions else word for word in sentence.split())\n    \n    if drop_stop==True:\n        sentence = ' '.join([word for word in sentence.split() if word not in stop_words])\n    \n    #\ub2e4\uc911\uacf5\ubc31 \uc81c\uac70\n    sentence = ' '.join([word for word in sentence.split()])\n    \n    return sentence","405bce89":"cleaned_review=[]\ncleaned_summary=[]\n\nfor sentence in df.Text:\n    cleaned_review.append(preprocess_text(sentence,drop_stop=True))\n    \nfor sentence in df.Summary:\n    cleaned_summary.append(preprocess_text(sentence,drop_stop=False))\n    \nprint(cleaned_review[:5])\nprint(\"\\n\\n\")\nprint(cleaned_summary[:5])","af1a70f7":"df.Text = cleaned_review\ndf.Summary = cleaned_summary","9656e4d8":"df.Text = df.Text.replace('',np.nan)\ndf.Summary = df.Summary.replace('',np.nan)\ndf.isnull().sum()","7d3dcc78":"df.dropna(axis=0,inplace=True)\nprint(df.isnull().sum())\nprint(\"\\n\\n\")\nprint(df.shape)","7b0214d6":"#encoder_input, decoder_input, decoder_label\n\nencoder_input = df.Text\ndecoder_input = df.Summary.apply(lambda x : 'SOS'+' '+x)\ndecoder_label = df.Summary.apply(lambda x : x+' '+'EOS')\n\nencoder_input = np.array(encoder_input)\ndecoder_input = np.array(decoder_input)\ndecoder_label = np.array(decoder_label)\n\nprint(len(encoder_input))\nprint(len(decoder_input))\nprint(len(decoder_label))","8c9f3867":"indices = np.arange(len(encoder_input))\nnp.random.shuffle(indices)\n\nencoder_input = encoder_input[indices]\ndecoder_input = decoder_input[indices]\ndecoder_label = decoder_label[indices]","d9dfc0ea":"# train_encoder_input\n# train_decoder_input\n# train_decoder_label\n\n# test_encoder_input\n# test_decoder_input\n# test_decoder_label\n\nrows = len(encoder_input)\ntrain_size=0.8\n\ntrain_encoder_input = encoder_input[:int(train_size*rows)]\ntrain_decoder_input = decoder_input[:int(train_size*rows)]\ntrain_decoder_label = decoder_label[:int(train_size*rows)]\n\ntest_encoder_input = encoder_input[int(train_size*rows):]\ntest_decoder_input = decoder_input[int(train_size*rows):]\ntest_decoder_label = decoder_label[int(train_size*rows):]","b04eb738":"review_tok = Tokenizer()\nreview_tok.fit_on_texts(train_encoder_input)\nprint(f\"{len(review_tok.index_word)} words are used in train review data\\n\")\n\nminimum = 5\nrare_count=0\nrare_occupation=0\ntotal_count=0\ntotal_occupation=0\n\nfor key,value in review_tok.word_counts.items():\n    if value <= minimum:\n        rare_count +=1\n        rare_occupation +=value\n    total_count+=1\n    total_occupation += value\n\nprint(f\"Number of Words used less than {minimum+1} times >> {rare_count}\")\nprint(f\"which is {np.round(rare_count\/total_count*100,2)}% of total words used\")\nprint(f\"And they occupy for only {np.round(rare_occupation\/total_occupation*100)}% of total frequency\")","bf324027":"summary_tok = Tokenizer()\nsummary_tok.fit_on_texts(train_decoder_input)\nsummary_tok.fit_on_texts(train_decoder_label)\nprint(f\"{len(summary_tok.index_word)} words are used in train review data\\n\")\n\nminimum = 2\nrare_count=0\nrare_occupation=0\ntotal_count=0\ntotal_occupation=0\n\nfor key,value in summary_tok.word_counts.items():\n    if value <= minimum:\n        rare_count +=1\n        rare_occupation +=value\n    total_count+=1\n    total_occupation += value\n\nprint(f\"Number of Words used less than {minimum+1} times >> {rare_count}\")\nprint(f\"which is {np.round(rare_count\/total_count*100)}% of total words used\")\nprint(f\"And they occupy for only {np.round(rare_occupation\/total_occupation*100,2)}% of total frequency\")","fd7b6fd6":"review_word_size = 20000\nsummary_word_size = 10000\n\nreview_vocab_size = review_word_size+1\nsummary_vocab_size = summary_word_size+1\n\nreview_tok = Tokenizer(num_words=review_word_size)\nsummary_tok = Tokenizer(num_words=summary_word_size)\n\nreview_tok.fit_on_texts(train_encoder_input)\nsummary_tok.fit_on_texts(train_decoder_input)\nsummary_tok.fit_on_texts(train_decoder_label)\n\ntrain_encoder_input = review_tok.texts_to_sequences(train_encoder_input)\ntest_encoder_input = review_tok.texts_to_sequences(test_encoder_input)\n\ntrain_decoder_input = summary_tok.texts_to_sequences(train_decoder_input)\ntest_decoder_input = summary_tok.texts_to_sequences(test_decoder_input)\n\ntrain_decoder_label = summary_tok.texts_to_sequences(train_decoder_label)\ntest_decoder_label = summary_tok.texts_to_sequences(test_decoder_label)","6f0435a2":"print(len(train_encoder_input))\nprint(len(train_decoder_input))\nprint(len(train_decoder_label),'\\n')\n\nprint(len(test_encoder_input))\nprint(len(test_decoder_input))\nprint(len(test_decoder_label))","cca6fb99":"train_delete_indices = [i for i,sequence in enumerate(train_decoder_input) if len(sequence) ==1]\ntest_delete_indices = [i for i,sequence in enumerate(test_decoder_input) if len(sequence)==1]\n\nprint(len(train_delete_indices))\nprint(len(test_delete_indices))\n\ntrain_encoder_input = np.delete(train_encoder_input,train_delete_indices,axis=0)\ntrain_decoder_input = np.delete(train_decoder_input,train_delete_indices,axis=0)\ntrain_decoder_label = np.delete(train_decoder_label,train_delete_indices,axis=0)\n\ntest_encoder_input = np.delete(test_encoder_input,test_delete_indices,axis=0)\ntest_decoder_input = np.delete(test_decoder_input,test_delete_indices,axis=0)\ntest_decoder_label = np.delete(test_decoder_label,test_delete_indices,axis=0)","077312e3":"reviews_len = [len(s) for s in train_encoder_input]\nsummarys_len  = [len(s) for s in train_decoder_input]\n\nplt.subplot(2,1,1)\nplt.hist(reviews_len,bins=200)\nplt.title(\"Dirstribution of lengths of Review sentences\")\nplt.show()\n\nplt.subplot(2,1,2)\nplt.hist(summarys_len,bins=50)\nplt.title(\"Dirstribution of lengths of Summary sentences\")\nplt.show()","eb51a18c":"review_sequence_size = 100\nsummary_sequence_size = 15\n\n#\uc0dd\uac01\ud574\ubcf4\ub2c8 \uc774\ub7ec\uba74 eos \ud1a0\ud070 \uc0ac\ub77c\uc9c0\ub294 \uc0d8\ud50c\ub4e4 \uc788\uc74c\ntrain_encoder_input = pad_sequences(train_encoder_input,maxlen=review_sequence_size,padding='post',truncating='post')\ntrain_decoder_input = pad_sequences(train_decoder_input,maxlen=summary_sequence_size,padding='post',truncating='post')\ntrain_decoder_label = pad_sequences(train_decoder_label,maxlen=summary_sequence_size,padding='post',truncating='post')\n\ntest_encoder_input = pad_sequences(test_encoder_input,maxlen=review_sequence_size,padding='post',truncating='post')\ntest_decoder_input = pad_sequences(test_decoder_input,maxlen=summary_sequence_size,padding='post',truncating='post')\ntest_decoder_label = pad_sequences(test_decoder_label,maxlen=summary_sequence_size,padding='post',truncating='post')","1f8e92af":"print(train_encoder_input.shape)\nprint(train_decoder_input.shape)\nprint(train_decoder_label.shape,'\\n\\n')\n\nprint(test_encoder_input.shape)\nprint(test_decoder_input.shape)\nprint(test_decoder_label.shape)","e60f4286":"urllib.request.urlretrieve(\"https:\/\/raw.githubusercontent.com\/thushv89\/attention_keras\/master\/src\/layers\/attention.py\",filename=\"attention.py\")","95ac7f2d":"#train model(Model for training)\nfrom keras.layers import Input,Embedding,Bidirectional,LSTM,Dense,Concatenate\nfrom keras.models import Model\nfrom attention import AttentionLayer\n\n#hyperparameters\nembedding_size = 128\nhidden_size = 256\n\n#encoder side\nencoder_input = Input(shape=[review_sequence_size],name=\"Encoder_Input\")\nencoder_embedding = Embedding(review_vocab_size,embedding_size,mask_zero=True,name=\"Encoder_Embedding\")(encoder_input)\n\nencoder_lstm1 = LSTM(hidden_size,return_sequences=True,return_state=True,dropout=0.2,recurrent_dropout=0.2,name=\"Encoder_LSTM1\")\nencoder_output1, encoder_h1, encoder_c1 = encoder_lstm1(encoder_embedding)\n\nencoder_lstm2 = LSTM(hidden_size,return_sequences=True,return_state=True,dropout=0.2,recurrent_dropout=0.2,name=\"Encoder_LSTM2\")\nencoder_output2, encoder_h2, encoder_c2 = encoder_lstm2(encoder_output1)\n\nencoder_lstm3 = LSTM(hidden_size,return_sequences=True,return_state=True,dropout=0.2,recurrent_dropout=0.2,name=\"Encoder_LSTM3\")\nencoder_output3, encoder_h3, encoder_c3 = encoder_lstm2(encoder_output2)\n\n#encoder_output -> Decoder \ub85c\n\n#decoder side\ndecoder_input = Input(shape=(None,),name=\"Decoder_Input\")\ndecoder_embedding = Embedding(summary_vocab_size,embedding_size,mask_zero=True,name=\"Decoder_Embedding\")\ndecoder_embedded =  decoder_embedding(decoder_input)\n\ndecoder_lstm = LSTM(hidden_size,return_sequences=True,return_state=True,name=\"Decoder_LSTM\")\ndecoder_output,_,_ = decoder_lstm(decoder_embedded,initial_state=[encoder_h3,encoder_c3])\n\nattention_layer = AttentionLayer(name=\"Decoder_Attention\")\nattention_output,attention_state = attention_layer([encoder_output3,decoder_output])\nconcatenated_output = Concatenate(axis=-1,name=\"Decoder_Concatenate\")([decoder_output,attention_output])\n\ndecoder_softmax = Dense(summary_vocab_size,activation='softmax',name=\"Decoder_Softmax\")\ndecoder_output = decoder_softmax(concatenated_output)\n\ntraining_model = Model([encoder_input,decoder_input],decoder_output)\ntraining_model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n\nprint(training_model.summary())\nplot_model(training_model)","d83b2320":"train_hist = training_model.fit([train_encoder_input,train_decoder_input],train_decoder_label,epochs=10,batch_size=256,validation_split=0.2)\ntest_result = training_model.evaluate([test_encoder_input,test_decoder_input],test_decoder_label)#\ubcc4 \uc758\ubbf8 \uc5c6\uc74c","f96833cc":"print(train_encoder_input.shape)\nprint(train_decoder_input.shape)\nprint(train_decoder_label.shape)","71b3eb60":"train_loss = train_hist.history['loss']\nval_loss = train_hist.history['val_loss']\ntrain_acc = train_hist.history['accuracy']\nval_acc = train_hist.history['val_accuracy']\n\nplt.subplot(1,2,1)\nplt.plot(range(1,len(train_loss)+1),train_loss,color='b',label='training loss')\nplt.plot(range(1,len(val_loss)+1),val_loss,color='r',label='validation loss')\nplt.legend()\nplt.show()\n\nplt.subplot(1,2,2)\nplt.plot(range(1,len(train_acc)+1),train_acc,color='b',label='training accuracy')\nplt.plot(range(1,len(val_acc)+1),val_acc,color='r',label='validation accuracy')\nplt.legend()\nplt.show()","62c806a5":"#test model(Model for generating)\ntest_encoder = Model(encoder_input,[encoder_output3,encoder_h3,encoder_c3])\n\ntest_decoder_initial_h_input = Input(shape=[hidden_size])\ntest_decoder_initial_c_input = Input(shape=[hidden_size])\n\ntest_decoder_embedded = decoder_embedding(decoder_input)\ntest_decoder_output,test_decoder_h,test_decoder_c =  decoder_lstm(test_decoder_embedded,initial_state=[test_decoder_initial_h_input,test_decoder_initial_c_input])\n\n#attention\ntest_decoder_hidden_state_input = Input(shape=[review_sequence_size,hidden_size])\ntest_attention_output,test_attention_state = attention_layer([test_decoder_hidden_state_input,test_decoder_output])\ntest_decoder_concatenated = Concatenate(axis=-1,name=\"Test_Decoder_Concatenate\")([test_decoder_output,test_attention_output])\n\ntest_decoder_output = decoder_softmax(test_decoder_concatenated)\n\ntest_decoder = Model([decoder_input]+[test_decoder_hidden_state_input,test_decoder_initial_h_input,test_decoder_initial_c_input],[test_decoder_output]+[test_decoder_h,test_decoder_c])","71933fa8":"review_index_word = review_tok.index_word\nsummary_word_index = summary_tok.word_index\nsummary_index_word = summary_tok.index_word\n\ndef decode_sequence(input_seq):\n    e_hs,e_h,e_c = test_encoder.predict(input_seq)\n    \n    decoder_input = np.zeros((1,1))\n    decoder_input[0,0] = summary_word_index['sos']\n    \n    stop_condition=False\n    decoded_sentence = ''\n    \n    while not stop_condition:\n        d_o,d_h,d_c = test_decoder.predict([decoder_input]+[e_hs,e_h,e_c])\n        sampled_index = np.argmax(d_o[0,-1,:])\n        sampled_word = summary_index_word[sampled_index]\n        \n        if sampled_word != 'eos':\n            decoded_sentence = decoded_sentence + ' ' + sampled_word\n        \n        #escape \ud560\uc9c0 \ub9d0\uc9c0 \ud310\ub2e8\n        if sampled_word == 'eos' or len(decoded_sentence.split()) >= summary_sequence_size:\n            stop_condition=True\n        \n        decoder_input = np.zeros((1,1))\n        decoder_input[0,0] = sampled_index\n        e_h,e_c = d_h,d_c\n     \n    #\uc774\uac74 \ub0b4\uac00 \ucd94\uac00(decoded sentence \uc911\uac04\uc911\uac04\uc5d0 0\uc788\uc744 \uc218 \ub3c4 \uc788\uc73c\ub2c8\uae4c)   \n#     decoded_sentence = ' '.join([word in word for decoded_sentence.split()])\n    return decoded_sentence","c3a61798":"def seq2review(sequence):\n    ret = ''\n    for seq in sequence:\n        if seq != 0:\n            ret = ret + review_index_word[seq] + ' '\n    return ret.strip()\n    \ndef seq2summary(sequence):\n    ret = ''\n    for seq in sequence:\n        if (seq != 0 and summary_index_word[seq] != 'sos') and summary_index_word[seq] != 'eos':\n            ret = ret + summary_index_word[seq] + ' '\n    return ret.strip()","9c2fa7d6":"for i in range(700,710):\n    print(\"Review : \",seq2review(test_encoder_input[i]))\n    print(\"Summray Label : \",seq2summary(test_decoder_label[i]))\n    print(\"Summary predicted : \", decode_sequence(test_encoder_input[i].reshape(1,review_sequence_size)))\n    print(\"\\n\")","66c6d209":"decode_sequence(train_encoder_input[11].reshape(1,-1))","fa8f50af":"seq2seq model (encoder + decoder) + Attention"}}