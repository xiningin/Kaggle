{"cell_type":{"130f248d":"code","b93eecdb":"code","2b09337f":"code","4c8fdd0e":"code","ccbef74b":"code","154b503f":"code","61621a08":"code","c6f92571":"code","4b2052fb":"code","024912b1":"code","b648dba3":"code","829ebc97":"code","a9bea94f":"code","0895ae8d":"code","087701ad":"code","ead86256":"code","1a78e3d0":"code","698d63c7":"code","2acf7027":"code","431e9422":"code","ef32f82b":"code","d30406e6":"code","8cd99957":"code","f2678fe4":"code","531d149d":"code","1c3b4379":"code","5060a474":"code","8ee3faf9":"code","6edf062a":"code","4a8939d1":"markdown","97bcb88e":"markdown","df6307aa":"markdown","6ef72efd":"markdown","d78e92b1":"markdown","252730ba":"markdown","cffa68b2":"markdown","f8593548":"markdown","7eae57ee":"markdown","3ebb0a9f":"markdown","7b34249d":"markdown","15bcac24":"markdown","bc9b4859":"markdown","0134333f":"markdown","21b62628":"markdown","e2715a79":"markdown","6839e2bb":"markdown","22f8323c":"markdown","621e4cf2":"markdown","3210bb1b":"markdown","797f7b4c":"markdown"},"source":{"130f248d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns # graphical display\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error as MSE\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b93eecdb":"all_data = pd.read_csv('..\/input\/real-estate-dataset\/data.csv')\nall_data.head()","2b09337f":"train_df, test_df, y_train, y_test = train_test_split(all_data.drop(columns='CRIM'), all_data['CRIM'], test_size=0.21, random_state=0)\nprint(train_df.shape)","4c8fdd0e":"train_df.head()","ccbef74b":"#Print Dataframe Infos\ntrain_df.info()","154b503f":"print('Dataset is composed of {} rows and {} columns'.format(train_df.shape[0], train_df.shape[1]))","61621a08":"fig, axes = plt.subplots(nrows=2, ncols=7, figsize=(20,7))\n\nfor col, ax in zip(train_df.columns, axes.flatten()):\n    print(col)\n    sns.distplot(train_df[col], ax=ax,kde_kws = {'bw' : 10}) #kde_kws = {'bw' : 10} manually added to prevent \"Selected KDE bandwidth is 0.\" error\n    ax.set_title(col)\n    \nplt.tight_layout()\nplt.show()","c6f92571":"lr = LinearRegression()\nlinMod = []\n\nfor col in train_df.columns.drop(['CHAS']):\n    \n    #Simple model\n    X = train_df[col].fillna(value=train_df[col].median()).values\n    lr.fit(X[:,np.newaxis],y_train)\n    score_s=lr.score(X[:,np.newaxis], y_train)\n    \n    #only logarithm\n    X_log = np.log1p(X)\n    lr.fit(X_log[:,np.newaxis], y_train)\n    score_l=lr.score(X_log[:,np.newaxis], y_train)\n    \n    linMod.append({\n        'simple': score_s,\n        'log': score_l,\n    })\n    \nlinMod = pd.DataFrame(linMod)\nlinMod['features'] = train_df.columns.drop(['CHAS'])\nlinMod.sort_values(by='simple', ascending=False, inplace=True)","4b2052fb":"plt.scatter(np.arange(linMod.shape[0]), linMod['simple'], color='C0', alpha =0.5, s=75, label='Simple model')\nplt.scatter(np.arange(linMod.shape[0]), linMod['log'], color='C1',alpha =0.5, s=75,label='log1p(feature)')\nplt.xticks(np.arange(linMod.shape[0]), linMod['features'], rotation=90)\nplt.ylabel('R^2 score')\nplt.legend()\nplt.title('Comparison between [x vs. y] and [log1p(x) vs. y]')\nplt.show()","024912b1":"#Here I extract the features that seem to benefit from a log transformation.\n\ncol_lg = linMod.loc[linMod['log']>linMod['simple'], 'features']\ncol_lg","b648dba3":"#Let's try, in fact, to calculate the MAE of a LinearModel\n\n# Take the columns \"as they are\"\ndf_simple = train_df.drop(columns=['CHAS']).copy() \nfor c in train_df.columns.drop(['CHAS']):\n    df_simple[c].fillna(df_simple[c].median(), inplace=True)\n    \n# Take the log1p(x) only for the columns where the log1p(x) has a higher R^2 score\ndf_log = train_df.drop(columns=['CHAS']).copy() \nfor c in train_df.columns.drop(['CHAS']):\n    df_log[c].fillna(df_log[c].median(), inplace=True)\nfor c in col_lg:\n    df_log[c] = np.log1p(df_log[c])\n    \n# Take the log1p(x) for all columns\ndf_alllog = train_df.drop(columns=['CHAS']).copy() \nfor c in train_df.columns.drop(['CHAS']):\n    df_alllog[c].fillna(df_alllog[c].median(), inplace=True)\n    df_alllog[c] = np.log1p(df_alllog[c])\n    \n\n# Scale the features\nscaler = StandardScaler()\nX_s = scaler.fit_transform(df_simple.values)\nX_l = scaler.fit_transform(df_log.values)\nX_al = scaler.fit_transform(df_alllog.values)\n#y = scaler.fit_transform(y)","829ebc97":"lr = LinearRegression()\n\n#simple model\nlr.fit(X_s, y_train)\nprint('MSE for simple model: {:.2f}'.format(MSE(y_train, lr.predict(X_s))))\n\n#log model\nlr.fit(X_l, y_train)\nprint('MSE for log model: {:.2f}'.format(MSE(y_train, lr.predict(X_l))))\n\n#ALL log model\nlr.fit(X_al, y_train)\nprint('MSE for ALL log model: {:.2f}'.format(MSE(y_train, lr.predict(X_al))))\n","a9bea94f":"# I add a binary column for the features RAD, ZN and TAX\n\ncont_col = train_df.drop(columns=['CHAS']).columns\n\n# Take the log1p(x) only for the columns where the log1p(x) has a higher R^2 score\ndf_log = train_df.drop(columns=['CHAS']).copy() \nfor c in train_df.columns.drop(['CHAS']):\n    df_log[c].fillna(df_log[c].median(), inplace=True)\nfor c in col_lg:\n    df_log[c] = np.log1p(df_log[c])\n    \ndf_log['ZN_binary'] = [1 if x>0 else 0 for x in df_log['ZN']]\ndf_log['RAD_binary'] = [1 if x>20 else 0 for x in df_log['RAD']]\ndf_log['TAX_binary'] = [1 if x>600 else 0 for x in df_log['TAX']]\n\nX_l = scaler.fit_transform(df_log.values)\n\n#log model\nlr.fit(X_l, y_train)\nprint('MSE for log model after binary addition: {:.2f}'.format(MSE(y_train, lr.predict(X_l))))","0895ae8d":"# Add polynomial features to continuous columns\n\nfor c in cont_col:\n    for d in [0.5, 2, 3]:\n        name = '{}**{}'.format(c, d)\n        df_log[name] = df_log[c]**d\n        \nX_l = scaler.fit_transform(df_log.values)\n\n#log model\nlr.fit(X_l, y_train)\nprint('MSE for log model after polynomial feature: {:.2f}'.format(MSE(y_train, lr.predict(X_l))))","087701ad":"def preprocess(df1, df2): # df1 is the dataframe to preprocess, based on df2 informations\n    \n    df1 = df1.copy() #work on a copy\n    \n    #set column names\n    cont_col = df1.drop(columns=['CHAS']).columns\n    col_lg = ['MEDV', 'NOX', 'DIS', 'RM', 'ZN']\n    \n    #compute log transform\n    for c in cont_col:\n        df1[c].fillna(df2[c].median(), inplace=True)\n    for c in col_lg:\n        df1[c] = np.log1p(df1[c])\n        \n    #Feature engineering\n    df1['ZN_binary'] = [1 if x>0 else 0 for x in df1['ZN']]\n    df1['RAD_binary'] = [1 if x>20 else 0 for x in df1['RAD']]\n    df1['TAX_binary'] = [1 if x>600 else 0 for x in df1['TAX']]\n    \n    #Polynomial features\n    for c in cont_col:\n        for d in [0.5, 2, 3]:\n            name = '{}**{}'.format(c, d)\n            df1[name] = df1[c]**d\n            \n    #One-Hot Encoding\n    df1 = pd.get_dummies(df1, dummy_na=False)\n    \n    return df1","ead86256":"train_df_preprocessed = preprocess(train_df, train_df)\ntest_df_preprocessed = preprocess(test_df, train_df)\n\n#ensure same columns\ntest_df_preprocessed = test_df_preprocessed.reindex(columns=train_df_preprocessed.columns, fill_value=0) #Ensure same columns","1a78e3d0":"scaler = StandardScaler()\ntrain_df_preproc_scaled = scaler.fit_transform(train_df_preprocessed)\ntest_df_preproc_scaled = scaler.transform(test_df_preprocessed)","698d63c7":"linreg = LinearRegression() #creates the object\nlinreg.fit(train_df_preproc_scaled, y_train) #fit the model using the train data rescaled\nmae_model1 = MSE(y_test, linreg.predict(test_df_preproc_scaled))\nprint('MSE Linear Regression: {:.6f}'.format(mae_model1))","2acf7027":"#Create pipeline object\npipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('logreg', LinearRegression())#, ))\n])\n\n#Create Cross-Validation object\ngrid = {}\n\n#Create shufflesplit cross-validation\ngrid_cv = GridSearchCV(pipe, grid, cv=KFold(n_splits=5, shuffle=True), return_train_score=True, n_jobs=-1)","431e9422":"#Fit the model and get the results in a DataFrame\ngrid_cv.fit(train_df_preprocessed, y_train)","ef32f82b":"linreg_predictions = grid_cv.predict(test_df_preprocessed)\nprint('MSE on test set using Linear Regression: {:.2f}'.format(MSE(y_test, linreg_predictions)))","d30406e6":"#Create pipeline object\npipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('ridge', Ridge(alpha=1))#, ))\n])\n\n#Create Cross-Validation object\ngrid = {'ridge__alpha': np.logspace(-1,5, num=100)}\n\n#Create shufflesplit cross-validation\ngrid_cv = GridSearchCV(pipe, grid, cv=KFold(n_splits=5), return_train_score=True, n_jobs=-1)","8cd99957":"#Fit the model\ngrid_cv.fit(train_df_preprocessed, y_train)","f2678fe4":"#Compute predicitons on the test set\nridge_predictions = grid_cv.predict(test_df_preprocessed)\nprint('MSE on test set using Ridge Regression: {:.2f}'.format(MSE(y_test, ridge_predictions)))","531d149d":"best_alpha = grid_cv.best_params_\nbest_alpha['ridge__alpha']","1c3b4379":"target = all_data['CRIM'].values\nX = all_data.drop(columns='CRIM')\n\nX_proc = preprocess(X, X)\nscaler = StandardScaler()\nX_proc_scaled = scaler.fit_transform(X_proc.values)","5060a474":"ridge = Ridge(alpha=best_alpha['ridge__alpha'])\nridge.fit(X_proc_scaled, target)\nridge_predictions = ridge.predict(X_proc_scaled)\nprint('MSE on test set using Ridge Regression: {:.2f}'.format(MSE(target, ridge_predictions)))","8ee3faf9":"pred_df = pd.DataFrame()\npred_df[\"CRIM\"] = all_data[\"CRIM\"]\npred_df[\"abs_err\"] = abs(all_data[\"CRIM\"] - ridge_predictions)\npred_df.head()","6edf062a":"pred_df.to_csv('Predictions_ridge.csv')","4a8939d1":"## 1.0 - Import dataset","97bcb88e":"## 4.0 Create Preprocessing function","df6307aa":"### 3.2 - Feature engineering","6ef72efd":"### 1.1 - Separate train and test set. Test set will be used to compute predictions.","d78e92b1":"#### 3.2.2 - Add Polynomial Features","252730ba":"#### 5.3.2 Extract best parameters and compute predictions on full dataset","cffa68b2":"Or alternatively:","f8593548":"## 2.0 - Investigate quality and content of data","7eae57ee":"## 3.0 - Exploratory Data Analysis","3ebb0a9f":"#### 3.1.3 - Confirm results","7b34249d":"#### 5.3.4 Retrain the model using the full dataset","15bcac24":"#### 3.2.1 - Column ZN contains several zeros","bc9b4859":"Column RM contains missing values. This will be addressed later","0134333f":"### 3.1 - Normal Distribution","21b62628":"#### 2.2 - Data Content","e2715a79":"#### 3.1.2 - Graphically show R^2 results","6839e2bb":"#### 2.1 - Data Quality","22f8323c":"### 5.2 Linear Regression Model","621e4cf2":"### 5.3 Ridge Regression\n#### 5.3.1 Combine Grid search and Cross validation","3210bb1b":"## 5.0 - Models\n\n### 5.1 Preprocess the data","797f7b4c":"#### 3.1.1 - Let's first evaluate the R^2 value between a given column and the CRIM target variable. Do a log transform provide better relationship?"}}