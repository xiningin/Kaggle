{"cell_type":{"fbb8b23f":"code","6ba183ee":"code","35d49db7":"code","651f9736":"code","bebabd48":"code","506f13c4":"code","8e727bd9":"code","e705d69e":"code","077a0b6f":"code","9a7da28f":"code","e99bf94f":"code","01159ed1":"code","cd324afe":"code","62cc3251":"code","73870cbe":"code","fb966c16":"code","6904b841":"code","5cabf8da":"code","01de73ff":"code","e5332cfc":"code","82bc1a70":"code","adde22b4":"code","b4876e04":"code","e9290e68":"code","4762a91e":"code","5350d0bf":"code","e40eec18":"code","6dd2fbd9":"code","210044e2":"code","eaf628fb":"code","9a851c65":"code","8bb44e3a":"code","a8d62680":"code","a4432713":"code","294691db":"code","87cc51eb":"code","2506962c":"code","b48b215b":"code","62726058":"code","705ab84b":"code","bf3d339e":"code","cd632053":"code","d7e77bd9":"code","0677adf0":"code","c990a41a":"code","a4832126":"code","77bca922":"code","3066d550":"code","fee544b6":"code","dcd4dce4":"code","ad102627":"code","2ab1adac":"code","6145aca3":"code","72957573":"code","e29fabf1":"code","0c3bf498":"code","481b5904":"code","13928b8a":"code","0a495362":"code","78610fa5":"code","d317efa6":"code","98b1d294":"code","ea22dcfd":"code","5ec9b707":"code","8a92d87e":"code","9bc3780b":"code","e102802a":"code","283d62a4":"code","e253f7e5":"markdown","63238c5d":"markdown","ab17cc52":"markdown","9419a8c9":"markdown","0256bcb9":"markdown","e9b9ef51":"markdown","6b7ee90d":"markdown","9c20e48a":"markdown","ea754128":"markdown","4b1da0db":"markdown","10a7185d":"markdown","ed402f95":"markdown","0b37268d":"markdown","00a75730":"markdown","b7944da7":"markdown","dd57a21b":"markdown","d535d776":"markdown","60328f37":"markdown","1c5637ff":"markdown"},"source":{"fbb8b23f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\nplt.style.use(\"ggplot\")\n\nprint(\"Libraries loaded successfully!!!\")","6ba183ee":"data = pd.read_csv(\"..\/input\/labourearnings\/LabourTrainingEvaluationData.csv\")\ndata.head(2)","35d49db7":"df = data.copy(deep = True)","651f9736":"print(\"Shape of the given dataset is {} rows and {} columns.\".format(*data.shape))","bebabd48":"# Missing values\ndef missing_values_table(df):\n        mis_val = df.isnull().sum()\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        mis_val_table_ren_columns = mis_val_table.rename(columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        mis_val_table_ren_columns = mis_val_table_ren_columns[mis_val_table_ren_columns.iloc[:,1] != 0].sort_values('% of Total Values', ascending=False).round(1)\n        print (\"Your selected dataframe has \" + str(df.shape[1]))   \n        print(\"There are \" + str(mis_val_table_ren_columns.shape[0])+\" columns that have missing values.\")\n        return mis_val_table_ren_columns","506f13c4":"missing_values_table(data).style.background_gradient(cmap='vlag_r')","8e727bd9":"data.dtypes","e705d69e":"data.describe()","077a0b6f":"l = ['Eduacation', 'Race', 'Hisp', 'MaritalStatus', 'Nodeg']\n\nfor i in l:\n    print(\"Count of unique values in {} features are {}.\".format(i, data[i].nunique()))\n    print(\"Unique values are {}\\n\".format(data[i].unique()))","9a7da28f":"l = ['Eduacation', 'Race', 'Hisp', 'MaritalStatus', 'Nodeg']\nj = 1\n\nplt.figure(figsize=(20,10))\nfor i in l:\n    plt.subplot(3, 2, j)\n    sns.countplot(data[i])\n    j= j+1\n    plt.title(i, fontdict={'fontsize': '20'})\n    plt.tight_layout()","e99bf94f":"l = ['Age', 'Earnings_1974', 'Earnings_1975', 'Earnings_1978']\nj = 1\n\nplt.figure(figsize=(20,7))\nfor i in l:\n    plt.subplot(2, 2, j)\n    sns.boxplot(data[i])\n    plt.title(i, fontdict={'fontsize': '20'})\n    j = j+1\nplt.tight_layout()","01159ed1":"l = ['Age','Earnings_1974', 'Earnings_1975', 'Earnings_1978']\nj = 1\n\nplt.figure(figsize=(20,10))\nfor i in l:\n    plt.subplot(2,2, j)\n    sns.distplot(data[i])\n    plt.title(i, fontdict={'fontsize': '20'})\n    j = j+1\nplt.tight_layout()","cd324afe":"l = ['Age','Earnings_1974', 'Earnings_1975', 'Earnings_1978']\nj = 1\n\nplt.figure(figsize=(20,10))\nfor i in l:\n    plt.subplot(2, 2, j)\n    sns.boxplot(x = data[i], y = data['Eduacation'])\n    plt.title(i, fontdict={'fontsize': '20'})\n    j = j+1\n    plt.tight_layout()","62cc3251":"l = ['Age','Earnings_1974', 'Earnings_1975', 'Earnings_1978']\nj = 1\n\nplt.figure(figsize=(20,7))\nfor i in l:\n    plt.subplot(2, 2, j)\n    sns.boxplot(x = data[i], y = data['Race'])\n    plt.title(i, fontdict={'fontsize': '20'})\n    j = j+1\n    plt.tight_layout()","73870cbe":"l = ['Age','Earnings_1974', 'Earnings_1975', 'Earnings_1978']\nj = 1\n\nplt.figure(figsize=(20,7))\nfor i in l:\n    plt.subplot(2, 2, j)\n    sns.boxplot(x = data[i], y = data['Hisp'])\n    plt.title(i, fontdict={'fontsize': '20'})\n    j = j+1\n    plt.tight_layout()","fb966c16":"l = ['Age','Earnings_1974', 'Earnings_1975', 'Earnings_1978']\nj = 1\n\nplt.figure(figsize=(20,7))\nfor i in l:\n    plt.subplot(2, 2, j)\n    sns.boxplot(x = data[i], y = data['MaritalStatus'])\n    plt.title(i, fontdict={'fontsize': '20'})\n    j = j+1\n    plt.tight_layout()","6904b841":"l = ['Age','Earnings_1974', 'Earnings_1975', 'Earnings_1978']\nj = 1\n\nplt.figure(figsize=(20,7))\nfor i in l:\n    plt.subplot(2, 2, j)\n    sns.violinplot(x=data[\"Hisp\"], y=data[i] )\n    plt.title(i, fontdict={'fontsize': '20'})\n    j = j+1\n    plt.tight_layout()","5cabf8da":"l = ['Age','Earnings_1974', 'Earnings_1975', 'Earnings_1978']\nj = 1\n\nplt.figure(figsize=(20,7))\nfor i in l:\n    plt.subplot(2, 2, j)\n    sns.violinplot(x=data[\"Eduacation\"], y=data[i] )\n    plt.title(i, fontdict={'fontsize': '20'})\n    j = j+1\n    plt.tight_layout()","01de73ff":"sns.pairplot(data)","e5332cfc":"sns.heatmap(data.corr(), annot=True, fmt='.2f', linewidths=0.3, linecolor='white')","82bc1a70":"cat_dummies = pd.get_dummies(data[['Race', 'Hisp', 'MaritalStatus', 'Eduacation']])\ncat_dummies.head()","adde22b4":"data = pd.concat([data, cat_dummies], axis = 1)\ndata.head(2)","b4876e04":"data.drop(['Eduacation', 'Race', 'Hisp', 'MaritalStatus'], axis = 1, inplace = True)\ndata.head(2)","e9290e68":"plt.figure(figsize=(20,10))\nsns.heatmap(data.corr(), annot=True, fmt='.2f', linewidths=0.3, linecolor='white', square = True)","4762a91e":"from sklearn.preprocessing import LabelEncoder","5350d0bf":"le = LabelEncoder()\ndf['Eduacation']= le.fit_transform(df['Eduacation'])\ndf['Race']= le.fit_transform(df['Race'])\ndf['Hisp']= le.fit_transform(df['Hisp'])\ndf['MaritalStatus']= le.fit_transform(df['MaritalStatus'])","e40eec18":"l = ['Eduacation', 'Race', 'Hisp', 'MaritalStatus', 'Nodeg']\n\nfor i in l:\n    print(\"Unique values are {}\\n\".format(df[i].unique()))","6dd2fbd9":"plt.figure(figsize=(20,10))\nsns.heatmap(df.corr(), annot=True, fmt='.2f', linewidths=0.3, linecolor='white', square = True)","210044e2":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score","eaf628fb":"X = data.drop('Earnings_1978', axis = 1)\ny = data['Earnings_1978']","9a851c65":"rs = RobustScaler()\nrs.fit(X)\nX_scaled = rs.transform(X)","8bb44e3a":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)","a8d62680":"lr = LinearRegression()\nlr.fit(X_train, y_train)\ny_pred = lr.predict(X_test)","a4432713":"print(\"Score for linear regression is {}\".format(r2_score(y_test, y_pred)))\nprint(\"MSE for linear regression is {}\".format(mean_squared_error(y_test, y_pred)))","294691db":"Xe = df.drop('Earnings_1978', axis = 1)\nye = df['Earnings_1978']","87cc51eb":"rs = RobustScaler()\nrs.fit(Xe)\nXe_scaled = rs.transform(Xe)","2506962c":"Xe_train, Xe_test, ye_train, ye_test = train_test_split(Xe, ye, test_size = 0.3, random_state = 42)","b48b215b":"lre = LinearRegression()\nlre.fit(Xe_train, ye_train)\nye_pred = lre.predict(Xe_test)","62726058":"print(\"Score for linear regression is {}\".format(r2_score(ye_test, ye_pred)))\nprint(\"MSE for linear regression is {}\".format(mean_squared_error(ye_test, ye_pred)))","705ab84b":"from sklearn.model_selection import GridSearchCV","bf3d339e":"fit_intercept=[True,False]\nnormalize=[False,True]\ncopy_X=[True,False]\nn_jobs=[1, 2, 3, 4, 5, 10,20,50,100]","cd632053":"param_grid = {'fit_intercept': fit_intercept,\n              'normalize': normalize,\n              'copy_X': copy_X,\n              'n_jobs': n_jobs}","d7e77bd9":"lin_reg = LinearRegression()","0677adf0":"grid_lr = GridSearchCV(lin_reg, param_grid = param_grid)","c990a41a":"grid_lr.fit(X_train, y_train)","a4832126":"grid_lr.best_score_","77bca922":"grid_lr.best_params_","3066d550":"grid_lr = GridSearchCV(lin_reg, param_grid = param_grid)","fee544b6":"grid_lr.fit(Xe_train, ye_train)","dcd4dce4":"grid_lr.best_score_","ad102627":"grid_lr.best_params_","2ab1adac":"from statsmodels.stats.outliers_influence import variance_inflation_factor","6145aca3":"vif = pd.DataFrame()\nvif[\"VIF Factor\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif[\"features\"] = X.columns","72957573":"vif.round(2).sort_values(by = 'VIF Factor')","e29fabf1":"x = data[['Age', 'Earnings_1975', 'Earnings_1974']]\ny = data['Earnings_1978']","0c3bf498":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 42)\nlr = LinearRegression()\nlr.fit(x_train, y_train)\ny_cap = lr.predict(x_test)","481b5904":"print(\"Score for linear regression is {}\".format(r2_score(y_test, y_cap)))\nprint(\"MSE for linear regression is {}\".format(mean_squared_error(y_test, y_cap)))","13928b8a":"fit_intercept=[True,False]\nnormalize=[False,True]\ncopy_X=[True,False]\nn_jobs=[1, 2, 3, 4, 5, 10,20,50,100]","0a495362":"param_grid = {'fit_intercept': fit_intercept,\n              'normalize': normalize,\n              'copy_X': copy_X,\n              'n_jobs': n_jobs}","78610fa5":"grid_lr = GridSearchCV(LinearRegression(), param_grid = param_grid)\ngrid_lr.fit(x_train, y_train)","d317efa6":"grid_lr.best_score_","98b1d294":"grid_lr.best_params_","ea22dcfd":"print(\"Score for linear regression is {}\".format(r2_score(y_test, grid_lr.predict(x_test))))\nprint(\"MSE for linear regression is {}\".format(mean_squared_error(y_test, grid_lr.predict(x_test))))","5ec9b707":"import statsmodels.api as sm\nX_train_sm = x_train\n\n#Unlike SKLearn, statsmodels don't automatically fit a constant, \n#so we need to use the method sm.add_constant(X) in order to add a constant. \nX_train_sm = sm.add_constant(X_train_sm)\n# create a fitted model in one line\nlm_1 = sm.OLS(y_train,X_train_sm).fit()\n\n# print the coefficients\nlm_1.params","8a92d87e":"print(lm_1.summary())","9bc3780b":"#Actual vs Predicted\n# Check for the forst 150 observations\n\nc = [i for i in range(1,150,1)]\nfig = plt.figure()\nplt.plot(c,y_test[1:150], color=\"blue\", linewidth=2.5, linestyle=\"-\")\nplt.plot(c,y_cap[1:150], color=\"red\",  linewidth=2.5, linestyle=\"-\")\nfig.suptitle('Actual and Predicted', fontsize=20)              # Plot heading \nplt.xlabel('Labour', fontsize=18)                               # X-label\nplt.ylabel('Earnings_1978', fontsize=16) ","e102802a":"# Error terms\nc = [i for i in range(1,4799,1)]\nfig = plt.figure()\nplt.plot(c,y_test-y_cap, color=\"blue\", linewidth=2.5, linestyle=\"-\")\nfig.suptitle('Error Terms', fontsize=20)              # Plot heading \nplt.xlabel('Index', fontsize=18)                      # X-label\nplt.ylabel('Actual - Predicted', fontsize=16) ","283d62a4":"X_train_final = X_train\n#Unlike SKLearn, statsmodels don't automatically fit a constant, \n#so you need to use the method sm.add_constant(X) in order to add a constant. \nX_train_final = sm.add_constant(X_train_final)\n# create a fitted model in one line\nlm_final = sm.OLS(y_train,X_train_final).fit()\n\nprint(lm_final.summary())","e253f7e5":"- Education for most of the people is LessThanHighSchool.\n- People who are post graduate and graduate have similar earning pattern for all years. ","63238c5d":"### Implementing the results","ab17cc52":"## Variance Inflation Factor","9419a8c9":"#### Encoded Data","0256bcb9":"- People who have non-black race tends to have more earnings as compare to the black people.","e9b9ef51":"## Model Building - Encoded Data","6b7ee90d":"- Likewise race, people who are non-hispanic have more earnings as compare to the hispanic people.","9c20e48a":"##### Median Earning for Married labour is higher than Unmarried labours","ea754128":"- With the distribution graph, we can see the spread of the earnings for all years is consistant.","4b1da0db":"### Grid Search CV","10a7185d":"#### Dummy Data","ed402f95":"- Since it is clearly shown that only 3 factors are affecting the most our target variable hence, we will do model evaluation using these feature as well to see we have any result improvements","0b37268d":"## Model Building - Dummy Data","00a75730":"## The END!!!","b7944da7":"- Married people are earning as compare to unmarried people.\n- Unmarried people contains outliers.","dd57a21b":"### Using GridSearch CV","d535d776":"## Checking for P-value Using STATSMODELS","60328f37":"- The value of R2 is 0.476 then this suggests that 47.6% of the variation in Y can be explained with the help of given explanatory variables in that model. In other words, it explains the proportion of variation in the dependent variable that is explained by the independent variables.","1c5637ff":"# Business Problem\n\"The labor problem\" is the economics term widely used toward the turn of the twentieth century with various applications.It has been defined in many ways, such as \"the problem of improving the conditions of employment of the wage-earning classes.\" It encompasses the difficulties faced by wage-earners and employers who began to cut wages for various reasons including increased\ntechnology, desire for lower costs or to stay in business.\n\nNow the laor organization, https:\/\/labour.gov.in\/lcandilasdivision\/india-ilo the indian version of this recently published the data for the 1974 and 1975 and wanted to demo of data science such that the data of 1978 labor can be predicted. Now the problem is the data is a copy of USA version and have race in it.\n\nData Contains Age, Race, Educational detail and Labour earning for 1974, 1975. The problem we are solving is the prediction of the future labours earning. The earning can be dependant on many of the variables. We have data for following:\n\n- Age of the person.\n- Race : Is he\/she is black or not black.\n- Education Details : How qualified the person is?\n- Hispanic : Is that person is Hispanic or not?\n- Married : Does marriage affect the earnings. And other informations.\n\n### Solution Approach:\nAs we need to predict Labour earning for 1978 which is continuous in nature, Linear Regression can be used for prediction.\n\n### Multiple Linear Regression\nNow you know how to build a model with one X (feature variable) and Y (response variable). But what if you have three feature variables, or may be 10 or 100? Building a separate model for each of them, combining them, and then understanding them will be a very difficult and next to impossible task. By using multiple linear regression, you can build models between a response variable and many feature variables.\n\nLet's see how to do that."}}