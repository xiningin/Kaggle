{"cell_type":{"7708ab76":"code","1a51196b":"code","90a09375":"code","b0948625":"code","b6a04f5a":"code","4ae2196f":"code","3069940d":"code","6b7c83c5":"code","9a1648bf":"code","bdb71833":"code","9d6d97ca":"code","9076b595":"code","e6c5b709":"code","2c44d266":"code","f0db210d":"code","7ca684f8":"code","04493e8c":"code","edec88a5":"code","37394373":"code","9898e77b":"code","06ccf910":"code","dc771899":"code","42070fb5":"code","72d8c544":"code","1484f52f":"code","edee50c2":"markdown","a21bb727":"markdown","9929f5d7":"markdown","dfa54ee5":"markdown","91c49cd7":"markdown","9611857a":"markdown","da75f37d":"markdown","06406a5a":"markdown"},"source":{"7708ab76":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1a51196b":"df = pd.read_csv('..\/input\/frenchfakenewsdetector\/datafake_train.csv',delimiter=';', encoding='utf-8')\ndf.head()","90a09375":"# keras\nimport keras\n\n# matplotlib\nimport matplotlib\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nnp.random.seed(42)","b0948625":"#Remove one row where the email is null. There is NO nulls.\n\n#df.dropna(inplace=True)","b6a04f5a":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\n\ndef tokenizer_sequences(num_words, X):\n    \n    # when calling the texts_to_sequences method, only the top num_words are considered while\n    # the Tokenizer stores everything in the word_index during fit_on_texts\n    tokenizer = Tokenizer(num_words=num_words)\n    # From doc: By default, all punctuation is removed, turning the texts into space-separated sequences of words\n    tokenizer.fit_on_texts(X)\n    sequences = tokenizer.texts_to_sequences(X)\n    \n    return tokenizer, sequences","4ae2196f":"max_words = 10000 \n# for the tokenizer, we configure it to only take into account the 1000 most common words when calling the texts_to_sequences method.\n\nmaxlen = 300\n# maxlen is the dimension that each email will have a fixed word sequence, in this case each email will be of a 1-d tensor (300,).","3069940d":"#Use a Tokenizer class of Keras to convert Messages text to sequences consistently\n\ntokenizer, sequences = tokenizer_sequences(max_words, df.post.copy())","6b7c83c5":"word_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\n# # We will pad all input sequences to have the length of 300. Each email will be the same length of sequence.\nX = pad_sequences(sequences, maxlen=maxlen)\n\ny = df.fake.copy()\n\nprint('Shape of data tensor:', X.shape)\nprint('Shape of label tensor:', y.shape)","9a1648bf":"max_words = len(tokenizer.word_index) + 1 # 33672 + 1\n# 0 is reserved for padding \/no data. The word indexes (i.e. tokenizer.word_index) are 1-offset.\n# max_words is the size of the vocabulary, you can think of a book is of max_words pages\n\nembedding_dim = 100 # the dimension of the word dictinory, i.e. this will be 100-dimensional word vector\n# you can think of a book that each page has embedding_dim words.","bdb71833":"from keras.models import Sequential\nfrom keras.layers import Flatten, Dense, Embedding\n\nmodel = Sequential()\n\n# embedding dictionary = 33673 * 100 = 3_367_300 parameters\n# we have a 33673 x 100 word vector, Embedding accepts 2D input and returns 3D output as shown in the summary\n# input_length = the length of input sequences (i.e. e-mails)\nmodel.add(Embedding(max_words, embedding_dim, input_length=maxlen)) # 33673, 100, input_length=300 = (None, 300,100)\n# the activations have shape of (33673, 300, embedding_dim=100)\n\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()","9d6d97ca":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)","9076b595":"X_train.shape, y_train.shape","e6c5b709":"model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n\nhistory = model.fit(X_train, y_train, epochs=3, batch_size=32, validation_split=0.2)","2c44d266":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', color='red', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend();","f0db210d":"model.evaluate(X_test, y_test)\n# loss value & acc metrics","7ca684f8":"#save the trained word vector and use it later.  I didn't get this part\n\nmodel.save_weights('pre_trained_model_100D.h5')","04493e8c":"# just curiosity, let's explore the shape of the trained word embedding\n\nmodel.layers[0].get_weights()[0].shape\n\n# (vocabulary len x dimension of word vector) = word vector!","edec88a5":"model2 = Sequential()\n\nmodel2.add(Embedding(max_words, embedding_dim, input_length=maxlen))\nmodel2.add(Flatten())\nmodel2.add(Dense(32, activation='relu'))\nmodel2.add(Dense(1, activation='sigmoid'))\nmodel2.summary()","37394373":"model2.layers[0].set_weights(model.layers[0].get_weights()) # load\nmodel2.layers[0].trainable = False # freeze","9898e77b":"model2.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n\nhistory2 = model2.fit(X_train, y_train, epochs=2, batch_size=32, validation_split=0.2)","06ccf910":"from sklearn.metrics import precision_score, recall_score\n\ny_test_pred = np.where(model2.predict(X_test) > .5, 1, 0).reshape(1, -1)[0]\nprint(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_test_pred)))\nprint(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_test_pred)))","dc771899":"acc = history2.history['acc']\nval_acc = history2.history['val_acc']\nloss = history2.history['loss']\nval_loss = history2.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', color='red', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend();","42070fb5":"# predict the test score\nmodel2.evaluate(X_test, y_test)\n# loss value & acc metrics","72d8c544":"model2.save_weights('pre_trained_model2100D_dense.h5')","1484f52f":"#Code by Olga Belitskaya https:\/\/www.kaggle.com\/olgabelitskaya\/sequential-data\/comments\nfrom IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#eb3434','#eb3446','Akronim','Smokum',30,15\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https:\/\/fonts.googleapis.com\/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';<\/style>\n    <h1 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s<\/h1>\"\"\"%string))\n    \n    \ndhtml('Thanks to Hakan Ozler, all code by Hakan.')","edee50c2":"#Create a new model and add the pre-trained word vector is of shape (57565, 100).\n\nThose numbers are on the snippet above.","a21bb727":"#Save the model2 along with the fully connected layers and word vector.","9929f5d7":"#Script by Hakan Ozler https:\/\/www.kaggle.com\/ozlerhakan\/neural-network-word-embedding-using-keras","dfa54ee5":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcQRygaVWDHG13ukn9jIHYr_eBMaA_-UZLgoPfjW87XjOpjfQUiyq6W_KDyxj59QAQhsd9c&usqp=CAU)glendon.yorku.ca","91c49cd7":"#https:\/\/www.kaggle.com\/ozlerhakan\/neural-network-word-embedding-using-keras","9611857a":"#The Keras Embedding Layer\n\nIn the first example we use the embedding layer to train the word embedding alone so that we can save and use it in another model later.","da75f37d":"#I didn't get the snippet above. Where can I find that model and its number (100D.h5)?","06406a5a":"You should see the architecture, it uses 6,716,565 parameters, of which 57,565,100 (the word embeddings) are non-trainable, and the remaining are 960,032 + 33. Because our vocabulary size has 57,565 words (with valid indices from 0 to 57,565) there are 57,565*100 = 57,565,100 non-trainable parameters.\n\nThe model2 assumes that the pre-trained vocabulary is of shape (57565, 100), otherwise we receive the not compatible with provided weight shape error. Since we load the embedding from outside, and do not want it to be trained, we freeze the first layer as follows:"}}