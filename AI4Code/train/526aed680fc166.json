{"cell_type":{"82341c7b":"code","4639a4da":"code","69ec2190":"code","6056855a":"code","cb3f6e7e":"code","fed6336f":"code","639cf278":"code","abd53d33":"code","550c23c5":"code","1a041098":"code","48a12477":"code","a86bc799":"code","b431be37":"code","f1c65483":"code","c4e1601d":"code","b2167cc4":"code","4ddc7bbf":"code","2831819c":"code","f6cd2bc7":"code","f7aaf5a2":"code","077c8f5a":"code","dfe6bfc0":"code","8aa118bb":"code","31af8d7d":"code","a0a0a192":"code","bd8c978f":"code","e91045c5":"code","c01f1778":"code","859bc80d":"code","6a4f5f60":"code","a264772c":"code","5d48b878":"code","46e5a2cf":"code","87881726":"code","17f20393":"code","7b8600cc":"code","dc9b70a7":"code","e61d051e":"code","71689993":"code","8d7a82d5":"code","83842de8":"code","5de30a9a":"markdown"},"source":{"82341c7b":"pip install skompiler","4639a4da":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport missingno as msno\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import scale\nfrom scipy.stats import levene\nfrom scipy.stats import shapiro\nfrom scipy.stats.stats import pearsonr\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\nfrom sklearn.preprocessing import scale\nfrom sklearn.model_selection import ShuffleSplit, GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn import model_selection\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.neural_network import MLPClassifier\nfrom skompiler import skompile\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.cross_decomposition import PLSRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import ElasticNetCV\nfrom sklearn import linear_model\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\nimport xgboost as xgb\nfrom xgboost import XGBRegressor, XGBClassifier\nfrom lightgbm import LGBMRegressor, LGBMClassifier\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn import tree\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_auc_score, roc_curve","69ec2190":"Asteroid = pd.read_csv(\"..\/input\/asteroid-classification-for-hazardous-prediction\/Asteroid_Updated.csv\",\n                       low_memory=False)\nAsteroid = Asteroid.drop([\"name\", \"condition_code\",\n                          \"extent\", \"IR\", \"diameter\", \"data_arc\", \"n_obs_used\",\n                          \"spec_B\", \"spec_T\", \"class\",\n                          \"rot_per\", \"GM\", \"BV\", \"UB\", \"G\", \"albedo\"], axis=1)\n\ndata = Asteroid.copy()  # for VISUALIZATION\ndf = data.select_dtypes(include=[\"float64\", \"int64\"])  # for OBSERVATION","6056855a":"print(data.shape)\nprint(\"-\" * 8)\nprint(data.columns)\nprint(\"-\" * 8)\nprint(data.info())\nprint(\"-\" * 8)\nprint(data.describe().T)\nprint(\"-\" * 8)\nprint(df.corr())\nprint(\"-\" * 8)\nprint(data.groupby([\"neo\", \"pha\"])[\"ma\"].mean())\nprint(\"-\" * 8)\nprint(data.groupby([\"neo\", \"pha\"])[\"per_y\"].mean())\nprint(\"-\" * 8)\nprint(data.groupby([\"neo\", \"pha\"])[\"H\"].mean())\nprint(\"-\" * 8)\nprint(data.groupby([\"neo\", \"pha\"])[\"moid\"].mean())\nprint(\"-\" * 8)\nprint(data.isnull().sum())\nprint(\"-\" * 8)\nprint(data[\"neo\"].value_counts())\nprint(\"-\" * 8)\nprint(data[\"pha\"].value_counts())\nprint(\"-\" * 8)","cb3f6e7e":"sns.barplot(x=\"neo\", y=\"a\", data=data)\nplt.show()\n# N+\n\nsns.barplot(x=\"neo\", y=\"per_y\", data=data)\nplt.show()\n# N+\n\nsns.barplot(x=\"neo\", y=\"n\", data=data)\nplt.show()\n# Y+\n\nsns.barplot(x=\"neo\", y=\"moid\", data=data)\nplt.show()\n# N+\n\nsns.barplot(x=\"pha\", y=\"a\", data=data)\nplt.show()\n# N+\n\nsns.barplot(x=\"pha\", y=\"per_y\", data=data)\nplt.show()\n# N+\n\nsns.barplot(x=\"pha\", y=\"n\", data=data)\nplt.show()\n# Y+\n\nsns.barplot(x=\"pha\", y=\"moid\", data=data)\nplt.show()\n# N+","fed6336f":"sns.relplot(x=\"neo\", y=\"a\", sizes=(40, 400), alpha=.5,\n            palette=\"muted\",\n            height=6, data=data)\nplt.show()\n\n\nsns.relplot(x=\"neo\", y=\"per_y\", sizes=(40, 400), alpha=.5,\n            palette=\"muted\",\n            height=6, data=data)\nplt.show()\n\n\nsns.relplot(x=\"neo\", y=\"n\", sizes=(40, 400), alpha=.5,\n            palette=\"muted\",\n            height=6, data=data)\nplt.show()\n\n\nsns.relplot(x=\"neo\", y=\"moid\", sizes=(40, 400), alpha=.5,\n            palette=\"muted\",\n            height=6, data=data)\nplt.show()\n\n\nsns.relplot(x=\"pha\", y=\"a\", sizes=(40, 400), alpha=.5,\n            palette=\"muted\",\n            height=6, data=data)\nplt.show()\n\n\nsns.relplot(x=\"pha\", y=\"per_y\", sizes=(40, 400), alpha=.5,\n            palette=\"muted\",\n            height=6, data=data)\nplt.show()\n\n\nsns.relplot(x=\"pha\", y=\"n\", sizes=(40, 400), alpha=.5,\n            palette=\"muted\",\n            height=6, data=data)\nplt.show()\n\n\nsns.relplot(x=\"pha\", y=\"moid\", sizes=(40, 400), alpha=.5,\n            palette=\"muted\",\n            height=6, data=data)\nplt.show()","639cf278":"sns.lineplot(x=\"a\", y=\"per_y\", hue=\"neo\", data=data)\nplt.show()\n\nsns.lineplot(x=\"n\", y=\"moid\", hue=\"neo\", data=data)\nplt.show()\n\nsns.lineplot(x=\"a\", y=\"per_y\", hue=\"pha\", data=data)\nplt.show()\n\nsns.lineplot(x=\"n\", y=\"moid\", hue=\"pha\", data=data)\nplt.show()","abd53d33":"msno.bar(data)\nmsno.matrix(data)\nmsno.heatmap(data)\nplt.show()\n\n# PHA - MOID MISSING CORR 1\n# AD - NEO MISSING CORR 1\n# AD - PER MISSING CORR 1\n# A - N MISSING CORR 1","550c23c5":"print(shapiro(df[\"a\"]))\n# p > 0.5\nprint(shapiro(df[\"e\"]))\n# NOT ACCEPTABLE\nprint(shapiro(df[\"i\"]))\n# NOT ACCEPTABLE\nprint(shapiro(df[\"om\"]))\n# NOT ACCEPTABLE\nprint(shapiro(df[\"w\"]))\n# NOT ACCEPTABLE\nprint(shapiro(df[\"q\"]))\n# NOT ACCEPTABLE\nprint(shapiro(df[\"ad\"]))\n# p > 0.5 ACCEPTABLE\nprint(shapiro(df[\"per_y\"]))\n# p > 0.5 ACCEPTABLE\nprint(shapiro(df[\"H\"]))\n# p > 0.5 ACCEPTABLE\nprint(shapiro(df[\"moid\"]))\n# p > 0.5 ACCEPTABLE\nprint(shapiro(df[\"n\"]))\n# p > 0.5 ACCEPTABLE\nprint(shapiro(df[\"per\"]))\n# p > 0.5 ACCEPTABLE\nprint(shapiro(df[\"ma\"]))\n# p > 0.5 ACCEPTABLE\nprint(\"-\" * 8)","1a041098":"encode = LabelEncoder()\n\ndata[\"pha\"] = encode.fit_transform(data[\"pha\"])\ndata[\"neo\"] = encode.fit_transform(data[\"pha\"])\ndfCorr = data.select_dtypes(include=[\"float64\", \"int64\", \"int32\"])\n\n# CORR FOR NORMALITY ACCEPTABLE\nprint(dfCorr[\"neo\"].corr(dfCorr[\"a\"]))\n# - \nprint(dfCorr[\"neo\"].corr(dfCorr[\"ad\"]))\n# -\nprint(dfCorr[\"neo\"].corr(dfCorr[\"per_y\"]))\n# -\nprint(dfCorr[\"neo\"].corr(dfCorr[\"H\"]))\n# -\nprint(dfCorr[\"neo\"].corr(dfCorr[\"moid\"]))\n# -\nprint(dfCorr[\"neo\"].corr(dfCorr[\"n\"]))\n# -\nprint(dfCorr[\"neo\"].corr(dfCorr[\"per\"]))\n# -\nprint(dfCorr[\"neo\"].corr(dfCorr[\"ma\"]))\n# -\nprint(\"-\" * 8)\n\nprint(dfCorr[\"pha\"].corr(dfCorr[\"a\"]))\n# -\nprint(dfCorr[\"pha\"].corr(dfCorr[\"ad\"]))\n# -\nprint(dfCorr[\"pha\"].corr(dfCorr[\"per_y\"]))\n# -\nprint(dfCorr[\"pha\"].corr(dfCorr[\"H\"]))\n# +\nprint(dfCorr[\"pha\"].corr(dfCorr[\"moid\"]))\n# -\nprint(dfCorr[\"pha\"].corr(dfCorr[\"n\"]))\n# +\nprint(dfCorr[\"pha\"].corr(dfCorr[\"per\"]))\n# -\nprint(dfCorr[\"pha\"].corr(dfCorr[\"ma\"]))\n# -\nprint(\"-\" * 8)\n\n# CORR FOR NORMALITY NOT ACCEPTABLE\nprint(dfCorr[\"neo\"].corr(dfCorr[\"e\"],method=\"spearman\"))\n# +\nprint(dfCorr[\"neo\"].corr(dfCorr[\"i\"],method=\"spearman\"))\n# +\nprint(dfCorr[\"neo\"].corr(dfCorr[\"om\"],method=\"spearman\"))\n# -\nprint(dfCorr[\"neo\"].corr(dfCorr[\"w\"],method=\"spearman\"))\n# +\nprint(dfCorr[\"neo\"].corr(dfCorr[\"q\"],method=\"spearman\"))\n# -\nprint(\"-\" * 8)\n\nprint(dfCorr[\"pha\"].corr(dfCorr[\"e\"]))\n# +\nprint(dfCorr[\"pha\"].corr(dfCorr[\"i\"]))\n# +\nprint(dfCorr[\"pha\"].corr(dfCorr[\"om\"]))\n# -\nprint(dfCorr[\"pha\"].corr(dfCorr[\"w\"]))\n# +\nprint(dfCorr[\"pha\"].corr(dfCorr[\"q\"]))\n# -\nprint(\"-\" * 8)","48a12477":"# HOMOGENEITY TEST FOR NOT ACCEPTABLE\nprint(levene(df[\"e\"], df[\"i\"], df[\"om\"], df[\"w\"], df[\"q\"]))\n# HOMOGENEITY TEST FOR ACCEPTABLE\nprint(levene(df[\"a\"], df[\"ad\"], df[\"per_y\"], df[\"H\"], df[\"moid\"], df[\"n\"], df[\"per\"], df[\"ma\"]))\n# HOMOGENEITY TEST FOR ALL\nprint(levene(df[\"e\"], df[\"i\"], df[\"om\"], df[\"w\"], df[\"q\"],\n             df[\"a\"], df[\"ad\"], df[\"per_y\"], df[\"H\"], df[\"moid\"], df[\"n\"], df[\"per\"], df[\"ma\"]))\nprint(\"-\" * 8)","a86bc799":"data.dropna(how=\"all\", inplace=True)\n\nlistColumnsNumeric = [\"a\", \"ad\", \"per_y\", \"n\", \"per\", \"ma\", \"H\"]\n\nfor i in listColumnsNumeric:\n    data[i].fillna(data[i].mean(), inplace=True)\n\ndata.dropna(inplace=True)","b431be37":"# NEO N - 0 , Y - 1  \/ # PHA N - 0 , Y - 1 \n\nx = data.drop(\"pha\", axis=1)\ny = data[\"pha\"]\nprint(x)\nprint(\"-\"*30)\nprint(y)\n\nxTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size=0.20, random_state=42)","f1c65483":"\nolsmodel = sm.OLS(yTrain, xTrain).fit()\npredict = olsmodel.predict(xTest)\n\nprint(olsmodel.summary())","c4e1601d":"linearmodel = LinearRegression().fit(xTrain, yTrain)\npredict = linearmodel.predict(xTest)\n\nprint(r2_score(yTest, predict))\n\nerror = mean_squared_error(yTest, predict)\nprint(np.sqrt(error))\n\nR2CV = cross_val_score(linearmodel, xTest, yTest, cv=10, scoring=\"r2\").mean()\nprint(R2CV)\nerrorCV = -cross_val_score(linearmodel, xTest, yTest, cv=10, scoring=\"neg_mean_squared_error\").mean()\nprint(np.sqrt(errorCV))","b2167cc4":"pca = PCA()\n\nxRTrain = pca.fit_transform(scale(xTrain))\nlinearmodel = LinearRegression().fit(xRTrain, yTrain)\npredict = linearmodel.predict(xTest)\n\nprint(r2_score(yTest, predict))\n\nerror = mean_squared_error(yTest, predict)\nprint(np.sqrt(error))\n\nR2CV = cross_val_score(linearmodel, xTest, yTest, cv=10, scoring=\"r2\").mean()\nprint(R2CV)\nerrorCV = -cross_val_score(linearmodel, xTest, yTest, cv=10, scoring=\"neg_mean_squared_error\").mean()\nprint(np.sqrt(errorCV))","4ddc7bbf":"plsmodel = PLSRegression().fit(xTrain, yTrain)\npredict = plsmodel.predict(xTest)\n\nprint(r2_score(yTest, predict))\n\nerror = mean_squared_error(yTest, predict)\nprint(np.sqrt(error))\n\n#for i in range(1, 30):\n    #print(f\"{i}\", \"-\" * 10)\n    #tuned = PLSRegression(n_components=i).fit(xTrain, yTrain)  # BEST 12\n    #predicttuned = tuned.predict(xTest)\n    #print(r2_score(yTest, predicttuned))\n    #errortuned = mean_squared_error(yTest, predicttuned)\n    #print(np.sqrt(errortuned))\n\nplsmodeltuned = PLSRegression(n_components=12).fit(xTrain, yTrain)\npredicttuned = plsmodeltuned.predict(xTest)\n\nR2CV = cross_val_score(plsmodeltuned, xTest, yTest, cv=10, scoring=\"r2\").mean()\nprint(R2CV)\n\nerrorCV = -cross_val_score(plsmodeltuned, xTest, yTest,\n                           cv=10, scoring=\"neg_mean_squared_error\").mean()\nprint(np.sqrt(errorCV))","2831819c":"ridgemodel = Ridge().fit(xTrain, yTrain)\npredict = ridgemodel.predict(xTest)\n\nprint(r2_score(yTest, predict))\n\nerror = mean_squared_error(yTest, predict)\nprint(np.sqrt(error))\n\nalpha = np.random.uniform(0.01, 10, 100)\nprint(alpha)\n\n#cv = RidgeCV(alphas=alpha, normalize=True, cv=10, scoring=\"r2\").fit(xTrain, yTrain)\n# process report = 0.14874893890570207\nbest = 0.14874893890570207\n\nridgetuned = Ridge(alpha=best).fit(xTrain, yTrain)\npredicttuned = ridgetuned.predict(xTest)\n\nR2CV = cross_val_score(ridgetuned, xTest, yTest, cv=10, scoring=\"r2\").mean()\nprint(R2CV)\n\nerrorCV = -cross_val_score(ridgetuned, xTest, yTest, scoring=\"neg_mean_squared_error\").mean()\nprint(np.sqrt(errorCV))","f6cd2bc7":"lassomodel = Lasso().fit(xTrain, yTrain)\npredict = lassomodel.predict(xTest)\n\nprint(r2_score(yTest, predict))\n\nerror = mean_squared_error(yTest, predict)\nprint(np.sqrt(error))\n\ncv = LassoCV(alphas=None, cv=10, max_iter=100000, normalize=True).fit(xTrain, yTrain)\nprint(cv.alpha_)\nlassomodeltuned = Lasso(alpha=cv.alpha_).fit(xTrain, yTrain)\npredicttuned = lassomodeltuned.predict(xTest)\n\nR2CV = cross_val_score(lassomodeltuned, xTest, yTest, cv=10, scoring=\"r2\").mean()\nprint(R2CV)\n\nerrorCV = -cross_val_score(lassomodel, xTest, yTest, cv=10, scoring=\"neg_mean_squared_error\").mean()\nprint(np.sqrt(errorCV))","f7aaf5a2":"enetmodel = ElasticNet().fit(xTrain, yTrain)\npredict = enetmodel.predict(xTest)\n\nprint(r2_score(yTest, predict))\n\nerror = mean_squared_error(yTest, predict)\nprint(np.sqrt(error))\n\nalphas = np.random.uniform(0.01, 10, 100)\n\n# cv = ElasticNetCV(alphas=alphas, cv=10, random_state=0).fit(xTrain, yTrain)\n# print(cv.alpha_)\n# process report -- 5.20214504131188\n\nbest = 5.20214504131188\n\nenetmodeltuned = ElasticNet(alpha=best).fit(xTrain, yTrain)\npredicttuned = enetmodeltuned.predict(xTest)\n\nR2CV = cross_val_score(enetmodeltuned, xTest, yTest, cv=10, scoring=\"r2\").mean()\nprint(int(R2CV))\n\nerrorCV = -cross_val_score(enetmodeltuned, xTest, yTest, cv=10, scoring=\"neg_mean_squared_error\").mean()\nprint(np.sqrt(errorCV))","077c8f5a":"knnmodel = KNeighborsRegressor().fit(xTrain, yTrain)\npredict = knnmodel.predict(xTest)\n\nprint(r2_score(yTest, predict))\n\nerror = mean_squared_error(yTest, predict)\nprint(np.sqrt(error))\n\nneighbor = {\"n_neighbors\": np.arange(1, 10)}\n\n# cv = GridSearchCV(knnmodel, neighbor, cv=10, n_jobs=-1).fit(xTrain, yTrain)\n# print(cv.best_params_)\n# process report -- 9\n\nknnmodeltuned = KNeighborsRegressor(n_neighbors=9).fit(xTrain, yTrain)\npredicttuned = knnmodeltuned.predict(xTest)\n\nR2CV = cross_val_score(knnmodeltuned, xTest, yTest, cv=10, scoring=\"r2\").mean()\nprint(R2CV)\n\nerrorCV = -cross_val_score(knnmodeltuned, xTest, yTest, cv=10, scoring=\"neg_mean_squared_error\").mean()\nprint(np.sqrt(errorCV))","dfe6bfc0":"scaler = StandardScaler().fit(xTrain, yTrain)\n\nxRTrain = scaler.transform(xTrain)\n\nmlpmodel = MLPRegressor().fit(xRTrain, yTrain)\n\npredict = mlpmodel.predict(xTest)\n\nprint(r2_score(yTest, predict))\n\nerror = mean_squared_error(yTest, predict)\nprint(np.sqrt(error))\n\nparams = {\"alpha\": [0.001, 0.01, 0.1, 0.2],\n          \"hidden_layer_sizes\": [(20, 20), (10, 10), (100, 200, 150), (300, 200, 250)],\n          \"activation\": [\"relu\", \"logistic\"]}\n\n#cv = GridSearchCV(mlpmodel, params, cv=10, n_jobs=-1).fit(xRTrain, yTrain)\n#print(cv.best_params_)\n\nmlptuned = MLPRegressor(activation=\"logistic\", alpha=0.1, hidden_layer_sizes=(100, 200, 150)).fit(xRTrain, yTrain)\npredicttuned = mlptuned.predict(xTest)\n\nR2CV = cross_val_score(mlptuned, xTest, yTest, cv=10, scoring=\"r2\").mean()\nprint(R2CV)\nerrorCV = -cross_val_score(mlptuned, xTest, yTest, cv=10, scoring=\"neg_mean_squared_error\").mean()\nprint(np.sqrt(errorCV))","8aa118bb":"cart = DecisionTreeRegressor().fit(xTrain, yTrain)\npredict = cart.predict(xTest)\n\nprint(r2_score(yTest, predict))\n\nerror = mean_squared_error(yTest, predict)\nprint(np.sqrt(error))\n\nparams = {\"min_samples_split\": range(2, 100),\n          \"max_leaf_nodes\": range(2, 10)}\n\n#cv = GridSearchCV(cart, params, cv=10, verbose=2, n_jobs=-1).fit(xTrain, yTrain)\n#print(cv.best_params_)\n\ncarttuned = DecisionTreeRegressor(max_leaf_nodes=3, min_samples_split=2).fit(xTrain, yTrain)\npredicttuned = carttuned.predict(xTest)\n\nR2CV = cross_val_score(carttuned, xTest, yTest, cv=10, scoring=\"r2\").mean()\nprint(R2CV)\n\nerrorCV = -cross_val_score(carttuned, xTest, yTest, cv=10, scoring=\"neg_mean_squared_error\").mean()\nprint(np.sqrt(errorCV))","31af8d7d":"bagg = BaggingRegressor(bootstrap_features=True).fit(xTrain, yTrain)\npredict = bagg.predict(xTest)\n\nprint(r2_score(yTest, predict))\n\nerror = mean_squared_error(yTest, predict)\nprint(np.sqrt(error))\n\nestimator = {\"n_estimators\": range(2, 30)}\n\ncv = GridSearchCV(bagg, estimator, cv=10, verbose=2, n_jobs=-1).fit(xTrain, yTrain)\nprint(cv.best_params_)\n\nbaggtuned = BaggingRegressor(n_estimators=27, bootstrap_features=True).fit(xTrain, yTrain)\npredicttuned = baggtuned.predict(xTest)\n\nR2CV = cross_val_score(baggtuned, xTest, yTest, cv=10, scoring=\"r2\").mean()\nprint(R2CV)\n\nerrorCV = -cross_val_score(baggtuned, xTest, yTest, cv=10, scoring=\"neg_mean_squared_error\").mean()\nprint(np.sqrt(errorCV))","a0a0a192":"rf = RandomForestRegressor(random_state=42).fit(xTrain, yTrain)\npredict = rf.predict(xTest)\n\nprint(r2_score(yTest, predict))\n\nerror = mean_squared_error(yTest, predict)\nprint(np.sqrt(error))\n\nparams = {\"max_depth\": list(range(1, 20)),\n          \"max_features\": [3, 5, 10, 15],\n          \"n_estimators\": [200, 500, 1000, 2000]}\n\n# cv = GridSearchCV(rf, params, cv=10, verbose=2, n_jobs=-1).fit(xTrain, yTrain)\n# print(cv.best_params_)\n\nrftuned = RandomForestRegressor(max_depth=5, max_features=10, n_estimators=1000, random_state=42).fit(xTrain, yTrain)\npredicttuned = rftuned.predict(xTest)\n\nR2CV = cross_val_score(rftuned, xTest, yTest, cv=10, scoring=\"r2\").mean()\nprint(R2CV)\n\nerrorCV = -cross_val_score(rftuned, xTest, yTest, cv=10, scoring=\"neg_mean_squared_error\").mean()\nprint(np.sqrt(errorCV))","bd8c978f":"gbm = GradientBoostingRegressor().fit(xTrain, yTrain)\npredict = gbm.predict(xTest)\n\nprint(r2_score(yTest, predict))\n\nerror = mean_squared_error(yTest, predict)\nprint(np.sqrt(error))\n\nparams = {\"learning_rate\": [0.001, 0.01, 0.1, 0.2],\n          \"max_depth\": [3, 5, 8, 10],\n          \"n_estimators\": [200, 500, 1000],\n          \"subsample\": [1, 0.5, 0.75]}\n\n# cv = GridSearchCV(gbm, params, cv=10, n_jobs=-1, verbose=2).fit(xTrain,yTrain)\n# print(cv.best_params_)\n\ngbmtuned = GradientBoostingRegressor(learning_rate=0.1,\n                                     max_depth=5,\n                                     n_estimators=200,\n                                     subsample=0.5).fit(xTrain, yTrain)\n\npredicttuned = gbmtuned.predict(xTest)\n\nR2CV = cross_val_score(gbmtuned, xTest, yTest, cv=10, scoring=\"r2\").mean()\nprint(R2CV)\n\nerrorCV = -cross_val_score(gbmtuned, xTest, yTest, cv=10, scoring=\"neg_mean_squared_error\").mean()\nprint(np.sqrt(errorCV))","e91045c5":"xgb = XGBRegressor().fit(xTrain, yTrain)\npredict = xgb.predict(xTest)\n\nprint(r2_score(yTest, predict))\n\nerror = mean_squared_error(yTest, predict)\nprint(np.sqrt(error))\n\nparams = {\"colsample_bytree\": [0.4, 0.5, 0.6, 0.9, 1],\n          \"n_estimators\": [100, 200, 500, 1000],\n          \"max_depth\": [2, 3, 4, 5, 6],\n          \"learning_rate\": [0.1, 0.01, 0.5]}\n\n#cv = GridSearchCV(xgb, params, cv=10, verbose=2, n_jobs=-1).fit(xTrain, yTrain)\n#print(cv.best_params_)\n\nxgbtuned = XGBRegressor(colsample_bytree=0.9,\n                        n_estimators=100, learning_rate=0.1, max_depth=2).fit(xTrain, yTrain)\n\npredicttuned = xgbtuned.predict(xTest)\n\nR2CV = cross_val_score(xgbtuned, xTest, yTest, cv=10, scoring=\"r2\").mean()\nprint(R2CV)\n\nerrorCV = -cross_val_score(xgbtuned, xTest, yTest, cv=10, scoring=\"neg_mean_squared_error\").mean()\nprint(np.sqrt(errorCV))","c01f1778":"lgbm = LGBMRegressor().fit(xTrain, yTrain)\npredict = lgbm.predict(xTest)\n\nprint(r2_score(yTest, predict))\n\nerror = mean_squared_error(yTest, predict)\nprint(np.sqrt(error))\n\nparams = {\n    \"n_estimators\": [100, 200, 500, 1000],\n    \"max_depth\": [2, 3, 4, 5, 6],\n    \"learning_rate\": [0.1, 0.01, 0.5]\n}\n\n#cv = GridSearchCV(lgbm, params, cv=10, verbose=2, n_jobs=-1).fit(xTrain, yTrain)\n#print(cv.best_params_)\n\nlgbmtuned = LGBMRegressor(learning_rate=0.01, max_depth=2, n_estimators=1000).fit(xTrain, yTrain)\npredicttuned = lgbmtuned.predict(xTest)\n\nR2CV = cross_val_score(lgbmtuned, xTest, yTest, cv=10, scoring=\"r2\").mean()\nprint(R2CV)\n\nerrorCV = -cross_val_score(lgbmtuned, xTest, yTest, cv=10, scoring=\"neg_mean_squared_error\").mean()\nprint(np.sqrt(errorCV))","859bc80d":"catboost = CatBoostRegressor().fit(xTrain, yTrain)\npredict = catboost.predict(xTest)\n\nprint(r2_score(yTest, predict))\n\nerror = mean_squared_error(yTest, predict)\nprint(np.sqrt(error))\n\nparams = {\n    \"depth\": [2, 3, 4, 5, 6],\n    \"learning_rate\": [0.1, 0.01, 0.5]\n}\n\n# cv = GridSearchCV(catboost, params, cv=10, verbose=2, n_jobs=-1).fit(xTrain, yTrain)\n# print(cv.best_params_)\n\ncatboosttuned = CatBoostRegressor(depth=2, learning_rate=0.01).fit(xTrain, yTrain)\npredicttuned = catboosttuned.predict(xTest)\n\nR2CV = cross_val_score(catboosttuned, xTest, yTest, cv=10, scoring=\"r2\").mean()\nprint(R2CV)\nerrorCV = -cross_val_score(catboosttuned, xTest, yTest, cv=10, scoring=\"neg_mean_squared_error\").mean()\nprint(np.sqrt(errorCV))","6a4f5f60":"loj = LogisticRegression(solver=\"liblinear\").fit(xTrain,yTrain)\npredict = loj.predict(xTest)\n\nprint(accuracy_score(yTest,predict))\n\nR2CV = cross_val_score(loj,xTest,yTest,cv=10).mean()\nprint(R2CV)\n\nerror = mean_squared_error(yTest,predict)\nprint(np.sqrt(error))\n\nprint(classification_report(yTest,predict))\nprint(roc_curve(yTest,predict))","a264772c":"gaussian = GaussianNB().fit(xTrain,yTrain)\npredict = gaussian.predict(xTest)\n\nprint(accuracy_score(yTest,predict))\nR2CV = cross_val_score(gaussian,xTest,yTest,cv=10).mean()\nprint(R2CV)\n\nerror = mean_squared_error(yTest,predict)\nprint(np.sqrt(error))\n\nprint(classification_report(yTest,predict))\nprint(roc_curve(yTest,predict))\n","5d48b878":"knn = KNeighborsClassifier().fit(xTrain,yTrain)\npredict = knn.predict(xTest)\n\nprint(accuracy_score(yTest,predict))\nR2CV = cross_val_score(knn,xTest,yTest,cv=10).mean()\nprint(R2CV)\n\nerror = mean_squared_error(yTest,predict)\nprint(np.sqrt(error))\n\nprint(classification_report(yTest,predict))\nprint(roc_curve(yTest,predict))\n\nparams = {\"n_neighbors\": np.arange(1, 50)}\n\n#cv = GridSearchCV(knn,params,cv=10).fit(xTrain,yTrain)\n#print(cv.best_params_)\n\nknntuned = KNeighborsClassifier(n_neighbors=11).fit(xTrain,yTrain)\npredicttuned = knntuned.predict(xTest)\n\nprint(accuracy_score(yTest,predicttuned))\n\nR2CVtunned = cross_val_score(knntuned,xTest,yTest,cv=10).mean()\nprint(R2CVtunned)\n\nerrortuned = mean_squared_error(yTest,predicttuned)\nprint(np.sqrt(errortuned))\n\nprint(classification_report(yTest,predicttuned))\nprint(roc_curve(yTest,predicttuned))","46e5a2cf":"svc = SVC(kernel=\"linear\").fit(xTrain,yTrain)\npredict = svc.predict(xTest)\n\nprint(accuracy_score(yTest,predict))\nR2CV = cross_val_score(svc,xTest,yTest,cv=10).mean()\nprint(R2CV)\n\nerror = mean_squared_error(yTest,predict)\nprint(np.sqrt(error))\n\nprint(classification_report(yTest,predict))\nprint(roc_curve(yTest,predict))\n\nparams = {\"C\": np.arange(1, 10)}\n\ncv = GridSearchCV(svc,params,cv=10,n_jobs=-1).fit(xTrain,yTrain)\n\nprint(cv.best_params_)","87881726":"scaler = StandardScaler().fit(xTrain,yTrain)\n\nxRTrain = scaler.transform(xTrain)\n\nmlpmodel = MLPClassifier().fit(xRTrain,yTrain)\npredict = mlpmodel.predict(xTest)\n\nprint(accuracy_score(yTest,predict))\n\nR2CV = cross_val_score(mlpmodel,xTest,yTest,cv=10).mean()\nprint(R2CV)\n\nerror = mean_squared_error(yTest,predict)\nprint(np.sqrt(error))\n\nprint(classification_report(yTest,predict))\nprint(roc_curve(yTest,predict))","17f20393":"cart = DecisionTreeClassifier().fit(xTrain,yTrain)\npredict = cart.predict(xTest)\n\nprint(accuracy_score(yTest,predict))\n\nR2CV = cross_val_score(cart,xTest,yTest,cv=10).mean()\nprint(R2CV)\n\nerror = mean_squared_error(yTest,predict)\nprint(np.sqrt(error))\n\nprint(classification_report(yTest,predict))\nprint(roc_curve(yTest,predict))","7b8600cc":"rf = RandomForestClassifier().fit(xTrain,yTrain)\npredict = rf.predict(xTest)\n\nprint(accuracy_score(yTest,predict))\n\nR2CV = cross_val_score(rf,xTest,yTest,cv=10).mean()\nprint(R2CV)\n\nerror = mean_squared_error(yTest,predict)\nprint(np.sqrt(error))\n\nprint(classification_report(yTest,predict))\nprint(roc_curve(yTest,predict))","dc9b70a7":"gbm = GradientBoostingClassifier().fit(xTrain,yTrain)\npredict = gbm.predict(xTest)\n\nprint(accuracy_score(yTest,predict))\n\nR2CV = cross_val_score(gbm,xTest,yTest,cv=10).mean()\nprint(R2CV)\n\nerror = mean_squared_error(yTest,predict)\nprint(np.sqrt(error))\n\nprint(classification_report(yTest,predict))\nprint(roc_curve(yTest,predict))","e61d051e":"xgbmodel = XGBClassifier().fit(xTrain, yTrain)\npredict = xgbmodel.predict(xTest)\n\nprint(accuracy_score(yTest, predict))\nR2CV = cross_val_score(xgb,xTest,yTest,cv=10).mean()\nprint(R2CV)\n\nerror = mean_squared_error(yTest,predict)\nprint(np.sqrt(error))\n\nprint(classification_report(yTest,predict))\nprint(roc_curve(yTest,predict))","71689993":"lgbm = LGBMClassifier().fit(xTrain, yTrain)\npredict = lgbm.predict(xTest)\n\nprint(accuracy_score(yTest, predict))\nR2CV = cross_val_score(lgbm,xTest,yTest,cv=10).mean()\nprint(R2CV)\n\nerror = mean_squared_error(yTest,predict)\nprint(np.sqrt(error))\n\nprint(classification_report(yTest,predict))\nprint(roc_curve(yTest,predict))","8d7a82d5":"catb = CatBoostClassifier().fit(xTrain, yTrain)\n\npredict = catb.predict(xTest)\n\nprint(accuracy_score(yTest, predict))\nR2CV = cross_val_score(catb,xTest,yTest,cv=10).mean()\nprint(R2CV)\n\nerror = mean_squared_error(yTest,predict)\nprint(np.sqrt(error))\n\nprint(classification_report(yTest,predict))\nprint(roc_curve(yTest,predict))","83842de8":"Importance = pd.DataFrame({\"Importance\": rf.feature_importances_ * 100},\n                          index=xTrain.columns)\n\nImportance.sort_values(by=\"Importance\", axis=0, ascending=True).plot(kind=\"barh\", color=\"r\")\nplt.show()","5de30a9a":"* All decision structures, regression types and prediction models have been tested.\n\n* Many of them have a high rate of accuracy.\n\n* Used to select a general observation and model."}}