{"cell_type":{"c411997c":"code","cb4f3a98":"code","8a5e0ead":"code","1d453cc5":"code","0e37ac7b":"code","56e3350b":"code","aaccbe9b":"code","0cfe5266":"code","421a7b14":"code","3aa52457":"code","049f5a59":"code","8325a166":"code","b4dfdc9d":"code","6d6d411c":"code","c8a884e9":"code","585f0dc5":"code","eca78129":"code","a6cba7ba":"code","61dfef0f":"markdown","5d16a8d6":"markdown","d7cbdace":"markdown","87f00cb4":"markdown","5691a865":"markdown","ec2513da":"markdown","fd1a536c":"markdown","911b7d3e":"markdown","14d86b9c":"markdown","f5ca7250":"markdown","b631272f":"markdown","911c2e72":"markdown","5d8d03b6":"markdown","e05c1d54":"markdown","7b5c8388":"markdown","a195fd45":"markdown"},"source":{"c411997c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pickle\nimport time\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nimport os\nprint('-------------------------')\nprint('all files:')\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nprint('-------------------------')","cb4f3a98":"results_filename = '..\/input\/bigger-is-better\/FCN_LReLU_03__I-_DxW-_O__10-_1x3-_1__n_models_63__n_inits_30__n_iter_1200__seed_1234.pickle'\nwith open(results_filename, \"rb\") as f:\n    training_results = pickle.load(f)\n    \nprint('-----------------------------')\nall_model_names = list(training_results.keys())\nprint('all model names:')\nfor model_name in all_model_names:\n    print('  ' + model_name)\nprint('-----------------------------')\n\nsingle_model_results = training_results[all_model_names[9]]\nall_results_dict_keys = list(single_model_results.keys())\nprint('single model results dict keys = %s' %(all_results_dict_keys))\n\n\nprint('single model hyperparams = %s' %(single_model_results['model_hyperparams_dict']))\n\nprint('single model y_GT_stats = ' %(single_model_results['y_GT_stats_dict']))\n[print(x, single_model_results['y_GT_stats_dict'][x]) for x in single_model_results['y_GT_stats_dict'].keys()]\n\nprint('-----------------------------')\n\n# collect all learning curves for the same model in the same matrix\nlearning_curve_matrix = np.zeros((len(single_model_results['all learning curves']), single_model_results['all learning curves'][0]['valid_1_loss'].shape[0]))\nfor k, curr_learning_curves in enumerate(single_model_results['all learning curves']):\n    learning_curve_matrix[k, :] = curr_learning_curves['valid_1_loss']\n    \nnum_batches_vec = single_model_results['all learning curves'][0]['num_batches']\nnum_samples_vec = single_model_results['all learning curves'][0]['num_samples']\n\nprint('single model learning_curve_matrix.shape = %s' %(str(learning_curve_matrix.shape)))\nprint('max number of batches (training steps\/iterations) is %d' %(num_batches_vec[-1]))\nprint('max number training samples is %d' %(num_samples_vec[-1]))\n\nprint('-----------------------------')\nprint('single model all start losses = %s' %(single_model_results['all start losses']))\n\nprint('single model all final losses = %s' %(single_model_results['all final losses']))\n\nprint('single model key outcomes = %s' %(single_model_results['key outcomes']))\nprint('-----------------------------')","8a5e0ead":"plt.figure(figsize=(15,7))\nplt.subplots_adjust(left=0.06, bottom=0.06, right=0.94, top=0.94, hspace=0.1, wspace=0.1)\n\nplt.plot(num_batches_vec, learning_curve_matrix.T, color='b', alpha=0.6)\nplt.plot(num_batches_vec, learning_curve_matrix.mean(axis=0), color='k', label='average')\nplt.plot(num_batches_vec, learning_curve_matrix.max(axis=0), color='r', label='worst')\nplt.plot(num_batches_vec, learning_curve_matrix.min(axis=0), color='g', label='best')\nplt.xlabel('batch index', fontsize=20)\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=20)","1d453cc5":"\ndef get_learning_curve_matrix(model_name, valid_index=1):\n    single_model_results = training_results[model_name]\n\n    valid_loss = 'valid_%d_loss' %(valid_index)\n    \n    # collect all learning curves for the same model in the same matrix\n    learning_curve_matrix = np.zeros((len(single_model_results['all learning curves']), single_model_results['all learning curves'][0][valid_loss].shape[0]))\n    for k, curr_learning_curves in enumerate(single_model_results['all learning curves']):\n        learning_curve_matrix[k, :] = curr_learning_curves[valid_loss]\n\n    return learning_curve_matrix\n\n\ndef get_learning_curves(model_name, valid_index=1):\n    single_model_results = training_results[model_name]\n\n    valid_loss = 'valid_%d_loss' %(valid_index)\n    \n    # collect all learning curves for the same model in the same matrix\n    learning_curves = np.zeros((len(single_model_results['all learning curves']), single_model_results['all learning curves'][0][valid_loss].shape[0]))\n    for k, curr_learning_curves in enumerate(single_model_results['all learning curves']):\n        learning_curves[k, :] = curr_learning_curves[valid_loss]\n\n    training_steps = single_model_results['all learning curves'][0]['num_batches']\n\n    return training_steps, learning_curves\n\n\ndef get_depth_x_width(model_name):\n    single_model_results = training_results[model_name]\n    \n    nn_depth = single_model_results['model_hyperparams_dict']['student_nn_depth']\n    nn_width = single_model_results['model_hyperparams_dict']['student_nn_width']\n    \n    return nn_depth, nn_width\n\n\ndef get_key_outcomes(model_name):\n    single_model_results = training_results[model_name]\n\n    return single_model_results['key outcomes']\n","0e37ac7b":"plt.figure(figsize=(16,12))\nplt.subplots_adjust(left=0.06, bottom=0.06, right=0.94, top=0.93, hspace=0.1, wspace=0.1)\nplt.suptitle('Average Learning Curves', fontsize=24)\n\nmax_depth_log2 = 6\nmax_width_log2 = 7\n\nall_model_names = list(training_results.keys())\nax0 = plt.subplot(2,1,1)\nax1 = plt.subplot(2,1,2)\n\nfor model_name in all_model_names:\n    model_label = model_name.split('_stu')[0]\n    learning_curve_matrix = get_learning_curve_matrix(model_name)\n    nn_depth, nn_width = get_depth_x_width(model_name)\n    curve_color = (np.log2(nn_depth) \/ max_depth_log2, 0.35, np.log2(nn_width) \/ max_width_log2)\n    ax0.plot(num_batches_vec, learning_curve_matrix.mean(axis=0), color=curve_color, label=model_label)\n    ax1.semilogy(num_batches_vec, learning_curve_matrix.mean(axis=0), color=curve_color, label=model_label)\n\nax0.set_ylabel('MSE', fontsize=20)\nax1.set_ylabel('MSE (log scale)', fontsize=20)\nax0.legend(fontsize=9, ncol=7)\nplt.xlabel('training steps', fontsize=20);","56e3350b":"plt.figure(figsize=(16,12))\nplt.subplots_adjust(left=0.06, bottom=0.06, right=0.94, top=0.93, hspace=0.1, wspace=0.1)\nplt.suptitle('Best Learning Curves', fontsize=24)\n\nall_model_names = list(training_results.keys())\nax0 = plt.subplot(2,1,1)\nax1 = plt.subplot(2,1,2)\n\nfor model_name in all_model_names:\n    model_label = model_name.split('_stu')[0]\n    learning_curve_matrix = get_learning_curve_matrix(model_name)\n    nn_depth, nn_width = get_depth_x_width(model_name)\n    curve_color = (np.log2(nn_depth) \/ max_depth_log2, 0.35, np.log2(nn_width) \/ max_width_log2)\n    ax0.plot(num_batches_vec, learning_curve_matrix.min(axis=0), color=curve_color, label=model_label)\n    ax1.semilogy(num_batches_vec, learning_curve_matrix.min(axis=0), color=curve_color, label=model_label)\n\nax0.set_ylabel('MSE', fontsize=20)\nax1.set_ylabel('MSE (log scale)', fontsize=20)\nax0.legend(fontsize=9, ncol=7)\nplt.xlabel('training steps', fontsize=20);","aaccbe9b":"all_model_names = list(training_results.keys())\n\nstart_mean = []\nstart_std = []\nfinal_mean = []\nfinal_std = []\ndepth = []\nwidth = []\n\nfor model_name in all_model_names:\n    key_outcomes = get_key_outcomes(model_name)\n    nn_depth, nn_width = get_depth_x_width(model_name)\n\n    start_mean.append(key_outcomes['mean start point'])\n    start_std.append(key_outcomes['std start point'])\n    final_mean.append(key_outcomes['mean final point'])\n    final_std.append(key_outcomes['std final point'])\n    \n    depth.append(nn_depth)\n    width.append(nn_width)\n\nshort_model_names = [x.split('_stu')[0] for x in all_model_names]\n\nstart_mean = np.array(start_mean)\nstart_std = np.array(start_std)\nfinal_mean = np.array(final_mean)\nfinal_std = np.array(final_std)\n    \nx_axis = range(len(short_model_names))\n\nplt.figure(figsize=(18,20));\nplt.subplots_adjust(left=0.06, bottom=0.06, right=0.94, top=0.93, hspace=0.33, wspace=0.1)\nplt.subplot(2,1,1);\nplt.bar(x_axis, final_mean, yerr=np.minimum(final_mean, final_std))\nplt.xticks(x_axis, short_model_names, rotation=90, fontsize=18);\nplt.ylabel('final MSE', fontsize=20)\nplt.subplot(2,1,2);\nplt.bar(x_axis, final_mean, yerr=np.minimum(final_mean, final_std))\nplt.xticks(x_axis, short_model_names, rotation=90, fontsize=18);\nplt.ylabel('final MSE (log scale)', fontsize=20)\nplt.yscale('log')","0cfe5266":"all_model_names = list(training_results.keys())\n\nstart_mean = []\nstart_std = []\nfinal_mean = []\nfinal_std = []\ndepth = []\nwidth = []\n\nfor model_name in all_model_names:\n    key_outcomes = get_key_outcomes(model_name)\n    nn_depth, nn_width = get_depth_x_width(model_name)\n\n    start_mean.append(key_outcomes['mean start point'])\n    start_std.append(key_outcomes['std start point'])\n    final_mean.append(key_outcomes['mean final point'])\n    final_std.append(key_outcomes['std final point'])\n    \n    depth.append(nn_depth)\n    width.append(nn_width)\n\nshort_model_names = [x.split('_stu')[0] for x in all_model_names]\n\nstart_mean = np.array(start_mean)\nstart_std = np.array(start_std)\nfinal_mean = np.array(final_mean)\nfinal_std = np.array(final_std)\n    \nx_axis = range(len(short_model_names))\n\nmse_0 = single_model_results['y_GT_stats_dict']['valid_1']['mse_0']\nfinal_mean = 100 * final_mean \/ mse_0\nfinal_std  = 100 * final_std \/ mse_0\n\n\nplt.figure(figsize=(18,20));\nplt.subplots_adjust(left=0.06, bottom=0.06, right=0.94, top=0.93, hspace=0.33, wspace=0.1)\nplt.subplot(2,1,1);\nplt.bar(x_axis, final_mean, yerr=np.minimum(final_mean, final_std))\nplt.xticks(x_axis, short_model_names, rotation=90, fontsize=18);\nplt.ylabel('variance remaining (%)', fontsize=20)\nplt.subplot(2,1,2);\nplt.bar(x_axis, final_mean, yerr=np.minimum(final_mean, final_std))\nplt.xticks(x_axis, short_model_names, rotation=90, fontsize=18);\nplt.ylabel('variance remaining (%) (log scale)', fontsize=20)\nplt.yscale('log')","421a7b14":"# build a dataframe\ndataframe_cols = ['full_model_name', 'short_name', 'depth', 'width', 'final_MSE_mean', 'final_MSE_std', 'start_MSE_mean', 'start_MSE_std']\nresults_summary_df = pd.DataFrame(index=range(len(all_model_names)), columns=dataframe_cols)\n\nresults_summary_df.loc[:,'full_model_name'] = all_model_names\nresults_summary_df.loc[:,'short_name'] = short_model_names\nresults_summary_df.loc[:,'depth'] = depth\nresults_summary_df.loc[:,'width'] = width\nresults_summary_df.loc[:,'final_MSE_mean'] = final_mean\nresults_summary_df.loc[:,'final_MSE_std'] = final_std\nresults_summary_df.loc[:,'start_MSE_mean'] = start_mean\nresults_summary_df.loc[:,'start_MSE_std'] = start_std\nresults_summary_df","3aa52457":"plt.figure(figsize=(20,10))\n\nunique_depth = sorted(results_summary_df['depth'].unique())\n\nmax_depth = 16\nunique_depth = [x for x in unique_depth if x <= max_depth]\n\nplt.subplot(2,1,1);\nfor depth in unique_depth:\n    const_depth_curve_x   = results_summary_df.loc[results_summary_df.loc[:,'depth'] == depth, 'width']\n    const_depth_curve     = results_summary_df.loc[results_summary_df.loc[:,'depth'] == depth, 'final_MSE_mean']\n    const_depth_curve_std = results_summary_df.loc[results_summary_df.loc[:,'depth'] == depth, 'final_MSE_std']\n    const_depth_curve_std = np.minimum(const_depth_curve_std, const_depth_curve)\n    plt.errorbar(const_depth_curve_x, const_depth_curve, yerr=const_depth_curve_std, label='%d_layers' %(depth))\n\nplt.xlabel('Width', fontsize=20)\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=16, ncol=1)\n\n\nplt.subplot(2,1,2);\nfor depth in unique_depth:\n    const_depth_curve_x   = results_summary_df.loc[results_summary_df.loc[:,'depth'] == depth, 'width']\n    const_depth_curve     = results_summary_df.loc[results_summary_df.loc[:,'depth'] == depth, 'final_MSE_mean']\n    const_depth_curve_std = results_summary_df.loc[results_summary_df.loc[:,'depth'] == depth, 'final_MSE_std']\n    const_depth_curve_std = np.minimum(const_depth_curve_std, const_depth_curve)\n    plt.errorbar(const_depth_curve_x, const_depth_curve, yerr=const_depth_curve_std, label='%d_layers' %(depth))\n    \nplt.xlabel('Width', fontsize=20)\nplt.ylabel('MSE (log scale)', fontsize=20)\nplt.yscale('log')","049f5a59":"plt.figure(figsize=(20,10))\n\nmax_depth = 16\n\nunique_width = sorted(results_summary_df['width'].unique())\n\nplt.subplot(2,1,1);\nfor width in unique_width:\n    const_width_curve_x   = results_summary_df.loc[results_summary_df.loc[:,'width'] == width, 'depth']\n    const_width_curve     = results_summary_df.loc[results_summary_df.loc[:,'width'] == width, 'final_MSE_mean']\n    const_width_curve_std = results_summary_df.loc[results_summary_df.loc[:,'width'] == width, 'final_MSE_std']\n    const_width_curve_std = np.minimum(const_width_curve_std, const_width_curve)\n    \n    selected_inds = const_width_curve_x <= max_depth\n    const_width_curve_x   = const_width_curve_x[selected_inds]\n    const_width_curve     = const_width_curve[selected_inds]\n    const_width_curve_std = const_width_curve_std[selected_inds]\n    \n    plt.errorbar(const_width_curve_x, const_width_curve, yerr=const_width_curve_std, label='%d_units' %(width))\n\nplt.xlabel('Depth', fontsize=20)\nplt.ylabel('MSE', fontsize=20)\nplt.legend(fontsize=16, ncol=3)\n\n\nplt.subplot(2,1,2);\nfor width in unique_width:\n    const_width_curve_x   = results_summary_df.loc[results_summary_df.loc[:,'width'] == width, 'depth']\n    const_width_curve     = results_summary_df.loc[results_summary_df.loc[:,'width'] == width, 'final_MSE_mean']\n    const_width_curve_std = results_summary_df.loc[results_summary_df.loc[:,'width'] == width, 'final_MSE_std']\n    const_width_curve_std = np.minimum(const_width_curve_std, const_width_curve)\n    \n    selected_inds = const_width_curve_x <= max_depth\n    const_width_curve_x   = const_width_curve_x[selected_inds]\n    const_width_curve     = const_width_curve[selected_inds]\n    const_width_curve_std = const_width_curve_std[selected_inds]\n\n    plt.errorbar(const_width_curve_x, const_width_curve, yerr=const_width_curve_std, label='%d_units' %(width))\n    \nplt.xlabel('Depth', fontsize=20)\nplt.ylabel('MSE (log scale)', fontsize=20)\nplt.yscale('log')","8325a166":"requested_valid_index = 1\nrequested_train_step = 30\n\ndepth_lims = [2,16]\nwidth_lims = [2,128]\n\nall_model_names = list(training_results.keys())\nshort_model_names = [x.split('_stu')[0] for x in all_model_names]\n\ntraining_steps, _ = get_learning_curves(model_name)\ntraining_step_ind = np.argmin(np.abs(training_steps - requested_train_step))\nrequested_train_step = training_steps[training_step_ind]\n\ndepth_list = []\nwidth_list = []\n\nresult_mean = []\nresult_std = []\nresult_best = []\nresult_90th_percentile = []\n\nall_short_model_names = []\n\nfor model_name in all_model_names:\n    nn_depth, nn_width = get_depth_x_width(model_name)\n    depth_OK = nn_depth >= depth_lims[0] and nn_depth <= depth_lims[1]\n    width_OK = nn_width >= width_lims[0] and nn_width <= width_lims[1]\n    \n    if depth_OK and width_OK:\n        _, learning_curves = get_learning_curves(model_name, valid_index=requested_valid_index)\n        result_vec = learning_curves[:,training_step_ind]\n        \n        key_outcomes = get_key_outcomes(model_name)\n\n        depth_list.append(nn_depth)\n        width_list.append(nn_width)\n        result_mean.append(result_vec.mean())\n        result_std.append(result_vec.std())\n        result_best.append(result_vec.min())\n        result_90th_percentile.append(np.percentile(result_vec, 90))\n        all_short_model_names.append(model_name.split('_stu')[0])\n\nresult_mean = np.array(result_mean)\nresult_std = np.array(result_std)\nresult_best = np.array(result_best)\nresult_90th_percentile = np.array(result_90th_percentile)\n\nx_axis = range(len(all_short_model_names))\n\nplt.figure(figsize=(18,20));\nplt.subplots_adjust(left=0.06, bottom=0.06, right=0.94, top=0.96, hspace=0.33, wspace=0.1)\nplt.suptitle('\"valid_%d\" after %d training steps' %(requested_valid_index, requested_train_step), fontsize=24)\nplt.subplot(2,1,1);\nplt.bar(x_axis, result_mean, yerr=result_std)\nplt.xticks(x_axis, all_short_model_names, rotation=90, fontsize=18);\nplt.ylabel('MSE', fontsize=20)\nplt.ylim([0, 1.3 * result_mean.max()])\nplt.subplot(2,1,2);\nplt.bar(x_axis, result_mean, yerr=result_std)\nplt.xticks(x_axis, all_short_model_names, rotation=90, fontsize=18);\nplt.ylabel('MSE (log scale)', fontsize=20)\nplt.yscale('log')\n","b4dfdc9d":"requested_valid_index = 1\nrequested_train_step = 140\n\ndepth_lims = [2,16]\nwidth_lims = [2,128]\n\nall_model_names = list(training_results.keys())\nshort_model_names = [x.split('_stu')[0] for x in all_model_names]\n\ntraining_steps, _ = get_learning_curves(model_name)\ntraining_step_ind = np.argmin(np.abs(training_steps - requested_train_step))\nrequested_train_step = training_steps[training_step_ind]\n\ndepth_list = []\nwidth_list = []\n\nresult_mean = []\nresult_std = []\nresult_best = []\nresult_90th_percentile = []\n\nall_short_model_names = []\n\nfor model_name in all_model_names:\n    nn_depth, nn_width = get_depth_x_width(model_name)\n    depth_OK = nn_depth >= depth_lims[0] and nn_depth <= depth_lims[1]\n    width_OK = nn_width >= width_lims[0] and nn_width <= width_lims[1]\n    \n    if depth_OK and width_OK:\n        _, learning_curves = get_learning_curves(model_name, valid_index=requested_valid_index)\n        result_vec = learning_curves[:,training_step_ind]\n        \n        key_outcomes = get_key_outcomes(model_name)\n\n        depth_list.append(nn_depth)\n        width_list.append(nn_width)\n        result_mean.append(result_vec.mean())\n        result_std.append(result_vec.std())\n        result_best.append(result_vec.min())\n        result_90th_percentile.append(np.percentile(result_vec, 90))\n        all_short_model_names.append(model_name.split('_stu')[0])\n\nresult_mean = np.array(result_mean)\nresult_std = np.array(result_std)\nresult_best = np.array(result_best)\nresult_90th_percentile = np.array(result_90th_percentile)\n\nx_axis = range(len(all_short_model_names))\n\nplt.figure(figsize=(18,20));\nplt.subplots_adjust(left=0.06, bottom=0.06, right=0.94, top=0.96, hspace=0.33, wspace=0.1)\nplt.suptitle('\"valid_%d\" after %d training steps' %(requested_valid_index, requested_train_step), fontsize=24)\nplt.subplot(2,1,1);\nplt.bar(x_axis, result_mean, yerr=result_std)\nplt.xticks(x_axis, all_short_model_names, rotation=90, fontsize=18);\nplt.ylabel('MSE', fontsize=20)\nplt.ylim([0, 1.3 * result_mean.max()])\nplt.subplot(2,1,2);\nplt.bar(x_axis, result_mean, yerr=result_std)\nplt.xticks(x_axis, all_short_model_names, rotation=90, fontsize=18);\nplt.ylabel('MSE (log scale)', fontsize=20)\nplt.yscale('log')","6d6d411c":"requested_valid_index = 1\nrequested_train_step = 1200\n\ndepth_lims = [2,16]\nwidth_lims = [2,128]\n\nall_model_names = list(training_results.keys())\nshort_model_names = [x.split('_stu')[0] for x in all_model_names]\n\ntraining_steps, _ = get_learning_curves(model_name)\ntraining_step_ind = np.argmin(np.abs(training_steps - requested_train_step))\nrequested_train_step = training_steps[training_step_ind]\n\ndepth_list = []\nwidth_list = []\n\nresult_mean = []\nresult_std = []\nresult_best = []\nresult_90th_percentile = []\n\nall_short_model_names = []\n\nfor model_name in all_model_names:\n    nn_depth, nn_width = get_depth_x_width(model_name)\n    depth_OK = nn_depth >= depth_lims[0] and nn_depth <= depth_lims[1]\n    width_OK = nn_width >= width_lims[0] and nn_width <= width_lims[1]\n    \n    if depth_OK and width_OK:\n        _, learning_curves = get_learning_curves(model_name, valid_index=requested_valid_index)\n        result_vec = learning_curves[:,training_step_ind]\n        \n        key_outcomes = get_key_outcomes(model_name)\n\n        depth_list.append(nn_depth)\n        width_list.append(nn_width)\n        result_mean.append(result_vec.mean())\n        result_std.append(result_vec.std())\n        result_best.append(result_vec.min())\n        result_90th_percentile.append(np.percentile(result_vec, 90))\n        all_short_model_names.append(model_name.split('_stu')[0])\n\nresult_mean = np.array(result_mean)\nresult_std = np.array(result_std)\nresult_best = np.array(result_best)\nresult_90th_percentile = np.array(result_90th_percentile)\n\nx_axis = range(len(all_short_model_names))\n\nplt.figure(figsize=(18,20));\nplt.subplots_adjust(left=0.06, bottom=0.06, right=0.94, top=0.96, hspace=0.33, wspace=0.1)\nplt.suptitle('\"valid_%d\" after %d training steps' %(requested_valid_index, requested_train_step), fontsize=24)\nplt.subplot(2,1,1);\nplt.bar(x_axis, result_mean, yerr=result_std)\nplt.xticks(x_axis, all_short_model_names, rotation=90, fontsize=18);\nplt.ylabel('MSE', fontsize=20)\nplt.ylim([0, 1.3 * result_mean.max()])\nplt.subplot(2,1,2);\nplt.bar(x_axis, result_mean, yerr=result_std)\nplt.xticks(x_axis, all_short_model_names, rotation=90, fontsize=18);\nplt.ylabel('MSE (log scale)', fontsize=20)\nplt.yscale('log')","c8a884e9":"requested_valid_index = 1\nrequested_train_step = 30\n\napply_normalization = False\n\ndepth_lims = [2,16]\nwidth_lims = [2,128]\n\nall_model_names = list(training_results.keys())\nshort_model_names = [x.split('_stu')[0] for x in all_model_names]\n\ntraining_steps, _ = get_learning_curves(model_name)\ntraining_step_ind = np.argmin(np.abs(training_steps - requested_train_step))\nrequested_train_step = training_steps[training_step_ind]\n\ndepth_list = []\nwidth_list = []\n\nresult_mean = []\nresult_std = []\nresult_best = []\nresult_90th_percentile = []\n\nall_short_model_names = []\n\nfor model_name in all_model_names:\n    nn_depth, nn_width = get_depth_x_width(model_name)\n    depth_OK = nn_depth >= depth_lims[0] and nn_depth <= depth_lims[1]\n    width_OK = nn_width >= width_lims[0] and nn_width <= width_lims[1]\n    \n    if depth_OK and width_OK:\n        _, learning_curves = get_learning_curves(model_name, valid_index=requested_valid_index)\n        result_vec = learning_curves[:,training_step_ind]\n        \n        key_outcomes = get_key_outcomes(model_name)\n\n        depth_list.append(nn_depth)\n        width_list.append(nn_width)\n        result_mean.append(result_vec.mean())\n        result_std.append(result_vec.std())\n        result_best.append(result_vec.min())\n        result_90th_percentile.append(np.percentile(result_vec, 90))\n        all_short_model_names.append(model_name.split('_stu')[0])\n\nresult_mean = np.array(result_mean)\nresult_std = np.array(result_std)\nresult_best = np.array(result_best)\nresult_90th_percentile = np.array(result_90th_percentile)\nresult_zeros = np.zeros(result_90th_percentile.shape)\n\nif apply_normalization:\n    mean_norm = result_mean[0]\n    best_norm = result_best[0]\n    result_mean \/= mean_norm\n    result_std  \/= mean_norm\n    result_best \/= best_norm\n    result_90th_percentile \/= best_norm\nx_axis = range(len(all_short_model_names))\n\nplt.figure(figsize=(18,20));\nplt.subplots_adjust(left=0.06, bottom=0.06, right=0.94, top=0.96, hspace=0.33, wspace=0.1)\nplt.suptitle('\"valid_%d\" after %d training steps' %(requested_valid_index, requested_train_step), fontsize=24)\nplt.subplot(2,1,1);\nplt.bar(x_axis, result_best, yerr=[result_zeros, result_90th_percentile-result_best])\nplt.xticks(x_axis, all_short_model_names, rotation=90, fontsize=18);\nplt.ylabel('MSE', fontsize=20)\nplt.subplot(2,1,2);\nplt.bar(x_axis, result_best, yerr=[result_zeros, result_90th_percentile-result_best])\nplt.xticks(x_axis, all_short_model_names, rotation=90, fontsize=18);\nplt.ylabel('MSE (log scale)', fontsize=20)\nplt.yscale('log')","585f0dc5":"requested_valid_index = 1\nrequested_train_step = 1200\n\napply_normalization = False\n\ndepth_lims = [2,16]\nwidth_lims = [2,128]\n\nall_model_names = list(training_results.keys())\nshort_model_names = [x.split('_stu')[0] for x in all_model_names]\n\ntraining_steps, _ = get_learning_curves(model_name)\ntraining_step_ind = np.argmin(np.abs(training_steps - requested_train_step))\nrequested_train_step = training_steps[training_step_ind]\n\ndepth_list = []\nwidth_list = []\n\nresult_mean = []\nresult_std = []\nresult_best = []\nresult_90th_percentile = []\n\nall_short_model_names = []\n\nfor model_name in all_model_names:\n    nn_depth, nn_width = get_depth_x_width(model_name)\n    depth_OK = nn_depth >= depth_lims[0] and nn_depth <= depth_lims[1]\n    width_OK = nn_width >= width_lims[0] and nn_width <= width_lims[1]\n    \n    if depth_OK and width_OK:\n        _, learning_curves = get_learning_curves(model_name, valid_index=requested_valid_index)\n        result_vec = learning_curves[:,training_step_ind]\n        \n        key_outcomes = get_key_outcomes(model_name)\n\n        depth_list.append(nn_depth)\n        width_list.append(nn_width)\n        result_mean.append(result_vec.mean())\n        result_std.append(result_vec.std())\n        result_best.append(result_vec.min())\n        result_90th_percentile.append(np.percentile(result_vec, 90))\n        all_short_model_names.append(model_name.split('_stu')[0])\n\nresult_mean = np.array(result_mean)\nresult_std = np.array(result_std)\nresult_best = np.array(result_best)\nresult_90th_percentile = np.array(result_90th_percentile)\nresult_zeros = np.zeros(result_90th_percentile.shape)\n\nif apply_normalization:\n    mean_norm = result_mean[0]\n    best_norm = result_best[0]\n    result_mean \/= mean_norm\n    result_std  \/= mean_norm\n    result_best \/= best_norm\n    result_90th_percentile \/= best_norm\nx_axis = range(len(all_short_model_names))\n\nplt.figure(figsize=(18,20));\nplt.subplots_adjust(left=0.06, bottom=0.06, right=0.94, top=0.96, hspace=0.33, wspace=0.1)\nplt.suptitle('\"valid_%d\" after %d training steps' %(requested_valid_index, requested_train_step), fontsize=24)\nplt.subplot(2,1,1);\nplt.bar(x_axis, result_best, yerr=[result_zeros, result_90th_percentile-result_best])\nplt.xticks(x_axis, all_short_model_names, rotation=90, fontsize=18);\nplt.ylabel('MSE', fontsize=20)\nplt.subplot(2,1,2);\nplt.bar(x_axis, result_best, yerr=[result_zeros, result_90th_percentile-result_best])\nplt.xticks(x_axis, all_short_model_names, rotation=90, fontsize=18);\nplt.ylabel('MSE (log scale)', fontsize=20)\nplt.yscale('log')","eca78129":"requested_valid_index = 3\nrequested_train_step = 1200\n\napply_normalization = False\n\ndepth_lims = [2,16]\nwidth_lims = [2,128]\n\nall_model_names = list(training_results.keys())\nshort_model_names = [x.split('_stu')[0] for x in all_model_names]\n\ntraining_steps, _ = get_learning_curves(model_name)\ntraining_step_ind = np.argmin(np.abs(training_steps - requested_train_step))\nrequested_train_step = training_steps[training_step_ind]\n\ndepth_list = []\nwidth_list = []\n\nresult_mean = []\nresult_std = []\nresult_best = []\nresult_90th_percentile = []\n\nall_short_model_names = []\n\nfor model_name in all_model_names:\n    nn_depth, nn_width = get_depth_x_width(model_name)\n    depth_OK = nn_depth >= depth_lims[0] and nn_depth <= depth_lims[1]\n    width_OK = nn_width >= width_lims[0] and nn_width <= width_lims[1]\n    \n    if depth_OK and width_OK:\n        _, learning_curves = get_learning_curves(model_name, valid_index=requested_valid_index)\n        result_vec = learning_curves[:,training_step_ind]\n        \n        key_outcomes = get_key_outcomes(model_name)\n\n        depth_list.append(nn_depth)\n        width_list.append(nn_width)\n        result_mean.append(result_vec.mean())\n        result_std.append(result_vec.std())\n        result_best.append(result_vec.min())\n        result_90th_percentile.append(np.percentile(result_vec, 90))\n        all_short_model_names.append(model_name.split('_stu')[0])\n\nresult_mean = np.array(result_mean)\nresult_std = np.array(result_std)\nresult_best = np.array(result_best)\nresult_90th_percentile = np.array(result_90th_percentile)\n\nif apply_normalization:\n    mean_norm = result_mean[0]\n    best_norm = result_best[0]\n    result_mean \/= mean_norm\n    result_std  \/= mean_norm\n    result_best \/= best_norm\n    result_90th_percentile \/= best_norm\n\nx_axis = range(len(all_short_model_names))\n\nplt.figure(figsize=(18,20));\nplt.subplots_adjust(left=0.06, bottom=0.06, right=0.94, top=0.96, hspace=0.33, wspace=0.1)\nplt.suptitle('\"valid_%d\" after %d training steps' %(requested_valid_index, requested_train_step), fontsize=24)\nplt.subplot(2,1,1);\nplt.bar(x_axis, result_mean, yerr=result_std)\nplt.xticks(x_axis, all_short_model_names, rotation=90, fontsize=18);\nplt.ylabel('MSE', fontsize=20)\nplt.ylim([0, 1.3 * result_mean.max()])\nplt.subplot(2,1,2);\nplt.bar(x_axis, result_mean, yerr=result_std)\nplt.xticks(x_axis, all_short_model_names, rotation=90, fontsize=18);\nplt.ylabel('MSE (log scale)', fontsize=20)\nplt.yscale('log')","a6cba7ba":"requested_valid_index = 4\nrequested_train_step = 1200\n\napply_normalization = False\n\ndepth_lims = [2,16]\nwidth_lims = [2,128]\n\nall_model_names = list(training_results.keys())\nshort_model_names = [x.split('_stu')[0] for x in all_model_names]\n\ntraining_steps, _ = get_learning_curves(model_name)\ntraining_step_ind = np.argmin(np.abs(training_steps - requested_train_step))\nrequested_train_step = training_steps[training_step_ind]\n\ndepth_list = []\nwidth_list = []\n\nresult_mean = []\nresult_std = []\nresult_best = []\nresult_90th_percentile = []\n\nall_short_model_names = []\n\nfor model_name in all_model_names:\n    nn_depth, nn_width = get_depth_x_width(model_name)\n    depth_OK = nn_depth >= depth_lims[0] and nn_depth <= depth_lims[1]\n    width_OK = nn_width >= width_lims[0] and nn_width <= width_lims[1]\n    \n    if depth_OK and width_OK:\n        _, learning_curves = get_learning_curves(model_name, valid_index=requested_valid_index)\n        result_vec = learning_curves[:,training_step_ind]\n        \n        key_outcomes = get_key_outcomes(model_name)\n\n        depth_list.append(nn_depth)\n        width_list.append(nn_width)\n        result_mean.append(result_vec.mean())\n        result_std.append(result_vec.std())\n        result_best.append(result_vec.min())\n        result_90th_percentile.append(np.percentile(result_vec, 90))\n        all_short_model_names.append(model_name.split('_stu')[0])\n\nresult_mean = np.array(result_mean)\nresult_std = np.array(result_std)\nresult_best = np.array(result_best)\nresult_90th_percentile = np.array(result_90th_percentile)\n\nif apply_normalization:\n    mean_norm = result_mean[0]\n    best_norm = result_best[0]\n    result_mean \/= mean_norm\n    result_std  \/= mean_norm\n    result_best \/= best_norm\n    result_90th_percentile \/= best_norm\n\nx_axis = range(len(all_short_model_names))\n\nplt.figure(figsize=(18,20));\nplt.subplots_adjust(left=0.06, bottom=0.06, right=0.94, top=0.96, hspace=0.33, wspace=0.1)\nplt.suptitle('\"valid_%d\" after %d training steps' %(requested_valid_index, requested_train_step), fontsize=24)\nplt.subplot(2,1,1);\nplt.bar(x_axis, result_mean, yerr=result_std)\nplt.xticks(x_axis, all_short_model_names, rotation=90, fontsize=18);\nplt.ylabel('MSE', fontsize=20)\nplt.ylim([0, 1.3 * result_mean.max()])\nplt.subplot(2,1,2);\nplt.bar(x_axis, result_mean, yerr=result_std)\nplt.xticks(x_axis, all_short_model_names, rotation=90, fontsize=18);\nplt.ylabel('MSE (log scale)', fontsize=20)\nplt.yscale('log')","61dfef0f":"# Show best and almost worst (90th percentile) models at start of training (30 iterations)","5d16a8d6":"# Display all learning curves of the first model","d7cbdace":"# Display Best learning curves for all models","87f00cb4":"# show a bar plot with all models starting and final accuracy","5691a865":"# Display mean accuracy at start of training (30 iterations)","ec2513da":"# Data acess helper functions","fd1a536c":"# Show best and almost worst (90th percentile) models at end of training","911b7d3e":"# Display OOD4 (laplace) mean results at end of training","14d86b9c":"# Show model accuracy vs model complexity","f5ca7250":"# Display mean accuracy at middle of training (140 iterations)","b631272f":"# Display mean accuracy at end of training (1200 iterations)","911c2e72":"# Display OOD3 (uniform) mean results at end of training","5d8d03b6":"# Load the previously saved results and access the dictionary","e05c1d54":"# collect results into a dataframe","7b5c8388":"# Display average learning curves of all models","a195fd45":"# show the same bar plot only with variance remaining"}}