{"cell_type":{"986afc56":"code","ff065d0a":"code","bb788b2d":"code","7b595af2":"code","e18249b0":"code","ff2c25fc":"code","eb598f78":"code","f6ffe54c":"code","9624d936":"code","cb1e3447":"code","5c60ed18":"code","9dac2790":"code","647beb8d":"code","3e556e63":"code","f65192e6":"code","505731c5":"code","16cbf186":"code","4164d58d":"code","fdd83884":"code","4ebe126f":"code","3c94889d":"code","fd550b45":"code","93291bdf":"code","4e566e95":"code","d4c3be8a":"code","6cbd6f3a":"markdown","7b0194fc":"markdown","f0ad543c":"markdown","1b18be8e":"markdown","abc6c178":"markdown","53b2d174":"markdown","42245028":"markdown"},"source":{"986afc56":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ff065d0a":"train = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")","bb788b2d":"train.head()","7b595af2":"y_train = train.iloc[:,-1] ","e18249b0":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nps=PorterStemmer()\nfrom nltk.stem import WordNetLemmatizer\nwnl=WordNetLemmatizer()","ff2c25fc":"corpus=[]\ntext=[]\nfor comment in train.loc[:,'text']:\n    comment = re.sub(r'(.)1+', r'1', comment)\n    comment = re.sub('((http:\\.+)|(www.[^s]+))','',comment) \n    comment=re.sub('[^a-zA-Z]',' ',comment)\n    comment=comment.lower()\n    comment=comment.split()\n    text=text+comment\n    comment=[wnl.lemmatize(word) for word in comment if word not in set(stopwords.words('english'))]\n    comment=' '.join(comment)\n    corpus.append(comment)\ncorpus[0]","eb598f78":"from nltk.probability import FreqDist\nfqd = FreqDist()\nfor word in text:\n    fqd[word]+=1\nfqd ","f6ffe54c":"fqd_values = list(fqd.values())\nfqd_values.sort(reverse=True)","9624d936":"len(fqd_values)","cb1e3447":"s=0\ns1=sum(fqd_values)\niteration =1 \nl=[]\nfor i in range(0,10000,500): \n    s=s+sum(fqd_values[i:i+500])\n    print(f'for top {i+500} words {(s\/s1)*100}% of data')\n    l.append((s\/s1)*100) \n    iteration+=1 ","5c60ed18":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(max_features=3500) #we choose 2500 features from the ditributin plot\ntrain = tfidf.fit_transform(corpus).toarray()","9dac2790":"train.shape","647beb8d":"tweets1 = []\nfor s in test.loc[:,'text']:  \n    s = re.sub(r'(.)1+', r'1', s)\n    s = re.sub('((http:\\.+)|(www.[^s]+))','',s) \n    s = re.sub('[^a-zA-Z]',' ',s) \n    s = s.split(' ')\n    s = [wnl.lemmatize(i) for i in s if i not in set(stopwords.words('english'))] \n    s = ' '.join(s) \n    tweets1.append(s)  \ntweets1[:10] ","3e556e63":"test = tfidf.transform(tweets1) \ntest = test.toarray()","f65192e6":"from sklearn.naive_bayes import GaussianNB\ngnb=GaussianNB()\ngnb.fit(train,y_train)","505731c5":"from sklearn.model_selection import cross_val_score\ncross_val_score(gnb,train,y_train,cv=10,scoring='accuracy').mean() ","16cbf186":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(max_iter=3000)","4164d58d":"from sklearn.model_selection import cross_val_score\ncross_val_score(lr,train,y_train,cv=10,scoring='accuracy').mean() ","fdd83884":"#from sklearn.svm import SVC\n#svc = SVC(kernel='linear')","4ebe126f":"#from sklearn.model_selection import cross_val_score\n#cross_val_score(svc,train,y_train,cv=10,scoring='accuracy').mean() ","3c94889d":"lr.fit(train,y_train)\ny_pred = lr.predict(test)  ","fd550b45":"sample_submission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")","93291bdf":"sample_submission['target']=y_pred","4e566e95":"sample_submission.head()","d4c3be8a":"sample_submission.to_csv(\"submission.csv\", index=False)","6cbd6f3a":"### training and testing the model","7b0194fc":"#### 1.naive bayes model","f0ad543c":"### we choose 3500 features for our feature vecotor","1b18be8e":"#### 3.svm model","abc6c178":"#### 2.logistic regression model","53b2d174":"#### svm model is too slow. not able to complete even after so much time.","42245028":"#### so i'm choosing the logistic regression model"}}