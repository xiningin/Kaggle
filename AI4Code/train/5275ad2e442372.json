{"cell_type":{"b8dc40d5":"code","203a681a":"code","f220bda7":"code","88dc4857":"code","9bc45c5e":"code","a2aa2e6d":"code","6a965b7a":"code","8342a7ef":"code","c74de128":"code","34437297":"code","f0ff0692":"code","c2b21e4a":"code","399b1d19":"code","1e814a63":"code","4d020fa3":"code","b45e4ad3":"code","481f8e77":"code","1fb2856e":"code","ef108088":"code","1ff9fc42":"code","1bec9c50":"code","fd53314b":"code","e9718c82":"code","f5b127a7":"code","96e79428":"code","09cae073":"code","8e054734":"code","cbd9eb22":"code","28a0af11":"code","6221326e":"code","f0eda594":"code","0adec1cd":"code","1611b442":"code","82143821":"code","09b4aa78":"code","d581c9cd":"code","fd5980e4":"code","7a867dc7":"code","c165c226":"code","687acc96":"code","45721ee5":"code","7ec77ddc":"markdown","2ab9bbea":"markdown","3a37b4d3":"markdown","8870c4ee":"markdown","9c7581ba":"markdown","ec5df83b":"markdown","6c60e3d8":"markdown","4dc5b344":"markdown","dd3efa38":"markdown","a1277467":"markdown","d6665780":"markdown","87beb54b":"markdown","65b69299":"markdown","fefd3223":"markdown","1e6b892d":"markdown","53a404e2":"markdown","ab82eab6":"markdown","25e3e0da":"markdown","1b7a74a5":"markdown","57a33ce8":"markdown","e38f03c1":"markdown","44b54e06":"markdown","5a80968a":"markdown"},"source":{"b8dc40d5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","203a681a":"\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import validation_curve\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nimport matplotlib.pyplot as plt\nimport seaborn as sns","f220bda7":"# load the data\nemail_rec = pd.read_csv(\"..\/input\/spambase\/realspambase.data\",  sep = ',', header= None )\nprint(email_rec.head())","88dc4857":"# renaming the columns\nemail_rec.columns  = [\"word_freq_make\", \"word_freq_address\", \"word_freq_all\", \"word_freq_3d\", \n                      \"word_freq_our\", \"word_freq_over\", \"word_freq_remove\", \"word_freq_internet\", \n                      \"word_freq_order\", \"word_freq_mail\", \"word_freq_receive\", \"word_freq_will\", \n                      \"word_freq_people\", \"word_freq_report\", \"word_freq_addresses\", \"word_freq_free\", \n                      \"word_freq_business\", \"word_freq_email\", \"word_freq_you\", \"word_freq_credit\", \n                      \"word_freq_your\", \"word_freq_font\", \"word_freq_000\", \"word_freq_money\", \"word_freq_hp\", \n                      \"word_freq_hpl\", \"word_freq_george\", \"word_freq_650\", \"word_freq_lab\", \"word_freq_labs\", \n                      \"word_freq_telnet\", \"word_freq_857\", \"word_freq_data\", \"word_freq_415\", \"word_freq_85\", \n                      \"word_freq_technology\", \"word_freq_1999\", \"word_freq_parts\", \"word_freq_pm\", \"word_freq_direct\",\n                      \"word_freq_cs\", \"word_freq_meeting\", \"word_freq_original\", \"word_freq_project\", \"word_freq_re\", \n                      \"word_freq_edu\", \"word_freq_table\", \"word_freq_conference\", \"char_freq_;\", \"char_freq_(\", \n                      \"char_freq_[\", \"char_freq_!\", \"char_freq_$\", \"char_freq_hash\", \"capital_run_length_average\", \n                      \"capital_run_length_longest\", \"capital_run_length_total\", \"spam\"]\nprint(email_rec.head())","9bc45c5e":"# look at dimensions of the df\nprint(email_rec.shape)","a2aa2e6d":"# ensure that data type are correct\nemail_rec.info()","6a965b7a":"# there are no missing values in the dataset \nemail_rec.isnull().sum()","8342a7ef":"# look at fraction of spam emails \n# 39.4% spams\nemail_rec['spam'].describe()","c74de128":"email_rec.describe()","34437297":"# splitting into X and y\nX = email_rec.drop(\"spam\", axis = 1)\ny = email_rec.spam.values.astype(int)","f0ff0692":"# scaling the features\n# note that the scale function standardises each column, i.e.\n# x = x-mean(x)\/std(x)\n\nfrom sklearn.preprocessing import scale\nX = scale(X)","c2b21e4a":"# split into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 4)","399b1d19":"# confirm that splitting also has similar distribution of spam and ham \n# emails\nprint(y_train.mean())\nprint(y_test.mean())","1e814a63":"# Model building\n\n# instantiate an object of class SVC()\n# note that we are using cost C=1\nmodel = SVC(C = 1)\n\n# fit\nmodel.fit(X_train, y_train)\n\n# predict\ny_pred = model.predict(X_test)","4d020fa3":"# Evaluate the model using confusion matrix \nfrom sklearn import metrics\nmetrics.confusion_matrix(y_true=y_test, y_pred=y_pred)","b45e4ad3":"# print other metrics\n\n# accuracy\nprint(\"accuracy\", metrics.accuracy_score(y_test, y_pred))\n\n# precision\nprint(\"precision\", metrics.precision_score(y_test, y_pred))\n\n# recall\/sensitivity\nprint(\"recall\", metrics.recall_score(y_test, y_pred))\n","481f8e77":"# specificity (% of hams correctly classified)\nprint(\"specificity\", 811\/(811+38))","1fb2856e":"# creating a KFold object with 5 splits \nfolds = KFold(n_splits = 5, shuffle = True, random_state = 4)\n\n# instantiating a model with cost=1\nmodel = SVC(C = 1)","ef108088":"# computing the cross-validation scores \n# note that the argument cv takes the 'folds' object, and\n# we have specified 'accuracy' as the metric\n\ncv_results = cross_val_score(model, X_train, y_train, cv = folds, scoring = 'accuracy') ","1ff9fc42":"# print 5 accuracies obtained from the 5 folds\nprint(cv_results)\nprint(\"mean accuracy = {}\".format(cv_results.mean()))","1bec9c50":"# specify range of parameters (C) as a list\nparams = {\"C\": [0.1, 1, 10, 100, 1000]}\n\nmodel = SVC()\n\n# set up grid search scheme\n# note that we are still using the 5 fold CV scheme we set up earlier\nmodel_cv = GridSearchCV(estimator = model, param_grid = params, \n                        scoring= 'accuracy', \n                        cv = folds, \n                        verbose = 1,\n                       return_train_score=True)      ","fd53314b":"# fit the model - it will fit 5 folds across all values of C\nmodel_cv.fit(X_train, y_train)  ","e9718c82":"# results of grid search CV\ncv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results","f5b127a7":"# plot of C versus train and test scores\n\nplt.figure(figsize=(8, 6))\nplt.plot(cv_results['param_C'], cv_results['mean_test_score'])\nplt.plot(cv_results['param_C'], cv_results['mean_train_score'])\nplt.xlabel('C')\nplt.ylabel('Accuracy')\nplt.legend(['test accuracy', 'train accuracy'], loc='upper left')\nplt.xscale('log')","96e79428":"best_score = model_cv.best_score_\nbest_C = model_cv.best_params_['C']\n\nprint(\" The highest test accuracy is {0} at C = {1}\".format(best_score, best_C))","09cae073":"# model with the best value of C\nmodel = SVC(C=best_C)\n\n# fit\nmodel.fit(X_train, y_train)\n\n# predict\ny_pred = model.predict(X_test)","8e054734":"# metrics\n# print other metrics\n\n# accuracy\nprint(\"accuracy\", metrics.accuracy_score(y_test, y_pred))\n\n# precision\nprint(\"precision\", metrics.precision_score(y_test, y_pred))\n\n# recall\/sensitivity\nprint(\"recall\", metrics.recall_score(y_test, y_pred))\n","cbd9eb22":"# specify params\nparams = {\"C\": [0.1, 1, 10, 100, 1000]}\n\n# specify scores\/metrics in an iterable\nscores = ['accuracy', 'precision', 'recall']\n\nfor score in scores:\n    print(\"# Tuning hyper-parameters for {}\".format(score))\n    \n    # set up GridSearch for score metric\n    clf = GridSearchCV(SVC(), \n                       params, \n                       cv=folds,\n                       scoring=score,\n                       return_train_score=True)\n    # fit\n    clf.fit(X_train, y_train)\n\n    print(\" The highest {0} score is {1} at C = {2}\".format(score, clf.best_score_, clf.best_params_))\n    print(\"\\n\")","28a0af11":"import pandas as pd\nimport numpy as np\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import scale","6221326e":"email_rec = pd.read_csv(\"..\/input\/spambase\/realspambase.data\",  sep = ',', header= None )\nemail_rec.head()","f0eda594":"# renaming the columns\nemail_rec.columns  = [\"word_freq_make\", \"word_freq_address\", \"word_freq_all\", \"word_freq_3d\", \n                      \"word_freq_our\", \"word_freq_over\", \"word_freq_remove\", \"word_freq_internet\", \n                      \"word_freq_order\", \"word_freq_mail\", \"word_freq_receive\", \"word_freq_will\", \n                      \"word_freq_people\", \"word_freq_report\", \"word_freq_addresses\", \"word_freq_free\", \n                      \"word_freq_business\", \"word_freq_email\", \"word_freq_you\", \"word_freq_credit\", \n                      \"word_freq_your\", \"word_freq_font\", \"word_freq_000\", \"word_freq_money\", \"word_freq_hp\", \n                      \"word_freq_hpl\", \"word_freq_george\", \"word_freq_650\", \"word_freq_lab\", \"word_freq_labs\", \n                      \"word_freq_telnet\", \"word_freq_857\", \"word_freq_data\", \"word_freq_415\", \"word_freq_85\", \n                      \"word_freq_technology\", \"word_freq_1999\", \"word_freq_parts\", \"word_freq_pm\", \"word_freq_direct\",\n                      \"word_freq_cs\", \"word_freq_meeting\", \"word_freq_original\", \"word_freq_project\", \"word_freq_re\", \n                      \"word_freq_edu\", \"word_freq_table\", \"word_freq_conference\", \"char_freq_;\", \"char_freq_(\", \n                      \"char_freq_[\", \"char_freq_!\", \"char_freq_$\", \"char_freq_hash\", \"capital_run_length_average\", \n                      \"capital_run_length_longest\", \"capital_run_length_total\", \"spam\"]\nprint(email_rec.head())","0adec1cd":"# splitting into X and y\nX = email_rec.drop(\"spam\", axis = 1)\ny = email_rec.spam.values.astype(int)","1611b442":"# scaling the features\nX_scaled = scale(X)\n\n# train test split\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.3, random_state = 4)","82143821":"# using rbf kernel, C=1, default value of gamma\n\nmodel = SVC(C = 1, kernel='rbf')\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)","09b4aa78":"# confusion matrix\nconfusion_matrix(y_true=y_test, y_pred=y_pred)","d581c9cd":"# accuracy\nprint(\"accuracy\", metrics.accuracy_score(y_test, y_pred))\n\n# precision\nprint(\"precision\", metrics.precision_score(y_test, y_pred))\n\n# recall\/sensitivity\nprint(\"recall\", metrics.recall_score(y_test, y_pred))","fd5980e4":"# creating a KFold object with 5 splits \nfolds = KFold(n_splits = 5, shuffle = True, random_state = 4)\n\n# specify range of hyperparameters\n# Set the parameters by cross-validation\nhyper_params = [ {'gamma': [1e-2, 1e-3, 1e-4],\n                     'C': [1, 10, 100, 1000]}]\n\n\n# specify model\nmodel = SVC(kernel=\"rbf\")\n\n# set up GridSearchCV()\nmodel_cv = GridSearchCV(estimator = model, \n                        param_grid = hyper_params, \n                        scoring= 'accuracy', \n                        cv = folds, \n                        verbose = 1,\n                        return_train_score=True)      \n\n# fit the model\nmodel_cv.fit(X_train, y_train)                  \n","7a867dc7":"# cv results\ncv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results","c165c226":"# converting C to numeric type for plotting on x-axis\ncv_results['param_C'] = cv_results['param_C'].astype('int')\n\n# # plotting\nplt.figure(figsize=(16,6))\n\n# subplot 1\/3\nplt.subplot(131)\ngamma_01 = cv_results[cv_results['param_gamma']==0.01]\n\nplt.plot(gamma_01[\"param_C\"], gamma_01[\"mean_test_score\"])\nplt.plot(gamma_01[\"param_C\"], gamma_01[\"mean_train_score\"])\nplt.xlabel('C')\nplt.ylabel('Accuracy')\nplt.title(\"Gamma=0.01\")\nplt.ylim([0.80, 1])\nplt.legend(['test accuracy', 'train accuracy'], loc='upper left')\nplt.xscale('log')\n\n# subplot 2\/3\nplt.subplot(132)\ngamma_001 = cv_results[cv_results['param_gamma']==0.001]\n\nplt.plot(gamma_001[\"param_C\"], gamma_001[\"mean_test_score\"])\nplt.plot(gamma_001[\"param_C\"], gamma_001[\"mean_train_score\"])\nplt.xlabel('C')\nplt.ylabel('Accuracy')\nplt.title(\"Gamma=0.001\")\nplt.ylim([0.80, 1])\nplt.legend(['test accuracy', 'train accuracy'], loc='upper left')\nplt.xscale('log')\n\n\n# subplot 3\/3\nplt.subplot(133)\ngamma_0001 = cv_results[cv_results['param_gamma']==0.0001]\n\nplt.plot(gamma_0001[\"param_C\"], gamma_0001[\"mean_test_score\"])\nplt.plot(gamma_0001[\"param_C\"], gamma_0001[\"mean_train_score\"])\nplt.xlabel('C')\nplt.ylabel('Accuracy')\nplt.title(\"Gamma=0.0001\")\nplt.ylim([0.80, 1])\nplt.legend(['test accuracy', 'train accuracy'], loc='upper left')\nplt.xscale('log')\n","687acc96":"# printing the optimal accuracy score and hyperparameters\nbest_score = model_cv.best_score_\nbest_hyperparams = model_cv.best_params_\n\nprint(\"The best test score is {0} corresponding to hyperparameters {1}\".format(best_score, best_hyperparams))","45721ee5":"# specify optimal hyperparameters\nbest_params = {\"C\": 100, \"gamma\": 0.0001, \"kernel\":\"rbf\"}\n\n# model\nmodel = SVC(C=100, gamma=0.0001, kernel=\"rbf\")\n\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\n# metrics\nprint(metrics.confusion_matrix(y_test, y_pred), \"\\n\")\nprint(\"accuracy\", metrics.accuracy_score(y_test, y_pred))\nprint(\"precision\", metrics.precision_score(y_test, y_pred))\nprint(\"sensitivity\/recall\", metrics.recall_score(y_test, y_pred))","7ec77ddc":"## Model Building\n\nLet's build a linear SVM mode now. The ```SVC()``` class does that in sklearn. We highly recommend reading the documentation at least once.\n","2ab9bbea":"## Grid Search to Find Optimal Hyperparameter C\n\nK-fold CV helps us compute average metrics over multiple folds, and that is the best indication of the 'test accuracy\/other metric scores' we can have. \n\nBut we want to use CV to compute the optimal values of hyperparameters (in this case, the cost C is a hyperparameter). This is done using the ```GridSearchCV()``` method, which computes metrics (such as accuracy, recall etc.) \n\nIn this case, we have only one hyperparameter, though you can have multiple, such as C and gamma in non-linear SVMs. In that case, you need to search through a *grid* of multiple values of C and gamma to find the optimal combination, and hence the name GridSearchCV.","3a37b4d3":"# Non-Linear SVM","8870c4ee":"### K-Fold Cross Validation\n\nLet's first run a simple k-fold cross validation to get a sense of the **average metrics** as computed over multiple *folds*. the easiest way to do cross-validation is to use the ```cross_val_score()``` function.\n\n","9c7581ba":"Though the training accuracy monotonically increases with C, the test accuracy gradually reduces. Thus, we can conclude that higher values of C tend to **overfit** the model. This is because a high C value aims to classify all training examples correctly (since C is the *cost of misclassification* - if you impose a high cost on the model, it will avoid misclassifying any points by overfitting the data). \n\n\n\nLet's finally look at the optimal C values found by GridSearchCV.","ec5df83b":"As of now, the columns are named as integers. Let's manually name the columns appropriately (column names are available at the UCI website here: https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/spambase\/spambase.names)","6c60e3d8":"The SVM we have built so far gives decently good results - an accuracy of 92%, sensitivity\/recall (TNR) of 88%. \n\n### Interpretation of Results\n\nIn the confusion matrix, the elements at (0, 0) and (1,1) correspond to the more frequently occurring class, i.e. ham emails. Thus, it implies that:\n- 92% of all emails are classified correctly\n- 88.5% of spams are identified correctly (sensitivity\/recall)\n- Specificity, or % of hams classified correctly, is 95%\n\n\n\n## Hyperparameter Tuning","4dc5b344":"# Linear SVM","dd3efa38":"Let's also look at the fraction of spam and ham emails in the dataset.","a1277467":"## Hyperparameter Tuning \n\nNow, we have multiple hyperparameters to optimise - \n- The choice of kernel (linear, rbf etc.)\n- C\n- gamma\n\nWe'll use the ```GridSearchCV()``` method to tune the hyperparameters. \n\n## Grid Search to Find Optimal Hyperparameters\n\nLet's first use the RBF kernel to find the optimal C and gamma (we can consider the kernel as a hyperparameter as well, though training the model will take an exorbitant amount of time). ","d6665780":"Let's now look at the metrics corresponding to C=10.","87beb54b":"## Data Preparation","65b69299":"Build a linear SVM classifier to classify emails into spam and ham.","fefd3223":"## Conclusion\n\nThe accuracy achieved using a non-linear kernel is comparable to that of a linear one. Thus, it turns out that for this problem, **you do not really need a non-linear kernel**.","1e6b892d":"## Model Evaluation Metrics","53a404e2":"## Data Preparation\n\nLet's now conduct some prelimininary data preparation steps, i.e. rescaling the variables, splitting into train and test etc. To understand why rescaling is required, let's print the summary stats of all columns - you'll notice that the columns at the end (capital_run_length_longest, capital_run_length_total etc.) have much higher values (means = 52, 283 etc.) than most other columns which represent fraction of word occurrences (no. of times word appears in email\/total no. of words in email).\n","ab82eab6":"## Optimising for Other Evaluation Metrics\n\nIn this case, we had optimised (tuned) the model based on overall accuracy, though that may not always be the best metric to optimise. For example, if you are concerned more about catching all spams (positives), you may want to maximise TPR or sensitivity\/recall. If, on the other hand, you want to avoid classifying hams as spams (so that any important mails don't get into the spam box), you would maximise the TNR or specificity.","25e3e0da":"A non-linear model (using non-linear kernels) and then finding the optimal hyperparameters (the choice of kernel, C, gamma).","1b7a74a5":"Though sklearn suggests the optimal scores mentioned above (gamma=0.001, C=100), one could argue that it is better to choose a simpler, more non-linear model with gamma=0.0001. This is because the optimal values mentioned here are calculated based on the average test accuracy (but not considering subjective parameters such as model complexity).\n\nWe can achieve comparable average test accuracy (~92.5%) with gamma=0.0001 as well, though we'll have to increase the cost C for that. So to achieve high accuracy, there's a tradeoff between:\n- High gamma (i.e. high non-linearity) and average value of C\n- Low gamma (i.e. less non-linearity) and high value of C\n\nWe argue that the model will be simpler if it has as less non-linearity as possible, so we choose gamma=0.0001 and a high C=100.\n\n### Building and Evaluating the Final Model\n\nLet's now build and evaluate the final model, i.e. the model with highest test accuracy.","57a33ce8":"## Model Building","e38f03c1":"This plot reveals some interesting insights:\n- **High values of gamma** lead to **overfitting** (especially at high values of C); note that the training accuracy at gamma=0.01 and C=1000 reaches almost 99% \n- The **training score increases with higher gamma**, though the **test scores are comparable** (at sufficiently high cost, i.e. C > 10)\n- The least amount of overfitting (i.e. difference between train and test accuracy) occurs at low gamma, i.e. a quite *simple non-linear model*\n","44b54e06":"## Data Understanding\n\nLet's first load the data and understand the attributes meanings, shape of the dataset etc.\n","5a80968a":"Renaming the column names"}}