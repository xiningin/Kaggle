{"cell_type":{"46b1c512":"code","c6a510b6":"code","71100d34":"code","6d149ba5":"code","5ff6cef0":"code","8046e9e8":"code","fe781f83":"code","d30e601a":"code","70fec4e0":"code","1591b8b4":"code","eccad9ec":"code","2ed3722c":"code","2bb4299a":"code","967325b5":"code","5ba55061":"code","24f12c81":"code","2a19b96b":"code","d9c156db":"code","4acce2f0":"code","3c84e516":"code","6c0d7cb0":"code","e17d55c2":"code","50f227df":"code","60dbd72c":"code","2f9ee030":"code","94920abb":"code","5d8f5e84":"code","b60f6a18":"code","6c81d1c1":"code","a04949db":"code","d0e3cf97":"code","724eef8c":"code","18f6c79a":"code","8fe09d06":"code","d0a81dad":"code","38b00caf":"code","d694a455":"code","15a4aa64":"code","c6ef9ce7":"code","7943a448":"code","c083aeea":"code","c6a19ded":"code","a230581b":"code","c0b11f1f":"code","c7c02b09":"code","9955b5d2":"code","8fc7d83e":"code","8ee79d06":"code","c856fbda":"code","2b2f370e":"code","3568ed62":"code","e71395cd":"code","5142e764":"code","19e91cd7":"code","076acee4":"code","f1f4a96d":"code","174c22e9":"code","8cc9cc69":"code","f9fc9124":"code","5a533f7d":"code","185dad3f":"code","62275b66":"code","c3a25a00":"code","226aa1f5":"code","125e8cdc":"markdown","6b0c9bd2":"markdown","ba91891a":"markdown","eb1aee9e":"markdown","4a974e99":"markdown"},"source":{"46b1c512":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","c6a510b6":"from sklearn import datasets\nboston= datasets.load_boston()\nboston.keys()","71100d34":"data= pd.DataFrame(boston.data, columns=boston.feature_names)","6d149ba5":"data['MEDV']=boston.target","5ff6cef0":"data.head()","8046e9e8":"#data Wrangling\ndata.info()","fe781f83":"data.dtypes","d30e601a":"data.describe()","70fec4e0":"data.isna().any","1591b8b4":"data.isna().sum()","eccad9ec":"for columns in data:\n    plt.figure()\n    sns.distplot(data[columns], color='red')","2ed3722c":"for columns in data:\n    plt.figure()\n    sns.boxplot(y=data[columns], color='blue')","2bb4299a":"#Bivariate Analysis\ndata.corr()","967325b5":"#Correlation of MEDV with other features\ndata.corr()['MEDV'].sort_values(ascending=False)","5ba55061":"#Heatmap\nplt.figure(figsize=(10,8))\nsns.heatmap(data.corr(), annot=True, linecolor='white', linewidths='0.1', square=True)\nplt.xticks(rotation=90)","24f12c81":"#PairPlot\nsns.pairplot(data)","2a19b96b":"#Regplot\nfor columns in data:\n    plt.figure()\n    sns.regplot(x=columns, y='MEDV' , data=data)","d9c156db":"#Splitting the dataset\nX= data.iloc[:,:-1]\nY= data.iloc[:,-1]","4acce2f0":"#Data Preprocessing\n#1 - No missing value is there\ndata.isna().sum()","3c84e516":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test= train_test_split(X, Y, test_size=0.25, random_state=1)","6c0d7cb0":"#Linear Regression- taking all the variables\nfrom sklearn.linear_model import LinearRegression\nlr_reg= LinearRegression(normalize=True)\n\nlr_reg.fit(X_train, Y_train)","e17d55c2":"#coffecient values\ncoff= pd.DataFrame(lr_reg.coef_, index=X_train.columns).sort_values(by=[0], ascending=False)\ncoff.rename(columns={0:'coff'})\n\nprint('linear intercept is {}'.format(lr_reg.intercept_))","50f227df":"Y_pred= lr_reg.predict(X_test)\nplt.scatter(Y_test, Y_pred)\n#less scattered it will be more better it is","60dbd72c":"#Evaluating metrics for linear regression\nfrom sklearn import metrics\nprint('R2 score for linear reg is {}'.format(metrics.r2_score(Y_test, Y_pred)))\nprint('MSE for linear reg is {}'.format(metrics.mean_squared_error(Y_test, Y_pred)))\nprint('RMSE for linear reg is {}'.format(np.sqrt(metrics.mean_squared_error(Y_test, Y_pred))))","2f9ee030":"#Lets use backward elimination to build model\nfrom scipy.special import factorial\nimport statsmodels.formula.api as sm\n\nones= pd.Series(np.ones(shape=(506,1), dtype=int).ravel())\nX1= pd.concat([ones, X], axis=1, )\n\nX1_opt=X1.iloc[:,[0,1,2,4,5,6,8,9,10,11,12,13]]","94920abb":"reg_OLS= sm.OLS(endog=Y, exog=X1_opt)\nreg_OLS= reg_OLS.fit()\nreg_OLS.summary()\n#thus 'Age' and 'Indus' has very high p value. So they are statistically insignificant--thus removing them","5d8f5e84":"#Decision tree\nfrom sklearn.tree import DecisionTreeRegressor\ndtree= DecisionTreeRegressor(criterion='mse', random_state=1)\n\ndtree.fit(X_train, Y_train)","b60f6a18":"print('R2 score for decision tree is {}'.format(metrics.r2_score(Y_test, dtree.predict(X_test))))\nprint('RMSE for decision tree is {}'.format(np.sqrt(metrics.mean_squared_error(Y_test, dtree.predict(X_test)))))","6c81d1c1":"pd.Series(dtree.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n#Looks like only two columns has high dependency - LSTAT, RM\n\nsns.barplot(x=X_train.columns, y=dtree.feature_importances_)\nplt.xticks(rotation=90)","a04949db":"#lets draw the tree to analyse better\nfrom sklearn.externals.six import StringIO\nfrom IPython.display import Image\nfrom sklearn.tree import export_graphviz\nfrom sklearn import tree\nimport pydot","d0e3cf97":"#Create DOT data\ndot_data= StringIO()\nexport_graphviz(dtree, out_file=dot_data, feature_names=X_train.columns.tolist(), rounded=True, filled=True)\n\n#Draw Graph\ngraph= pydot.graph_from_dot_data(dot_data.getvalue())\n\n#Show Graph\nImage(graph[0].create_png())","724eef8c":"#Using grid search CV for best parameter for decision tree\nfrom sklearn.model_selection import GridSearchCV\ndt= DecisionTreeRegressor(random_state=5)\nparams= [{'max_depth':[4,8,12,16,20], \"max_leaf_nodes\":range(2,20)}]\ngrid= GridSearchCV(dt, param_grid=params, refit=True)","18f6c79a":"grid.fit(X_train, Y_train)\ngrid_predictions= grid.predict(X_test)\n\nprint('Accuracy Score:{}'.format(metrics.r2_score(Y_test, grid_predictions)))","8fe09d06":"print('Best hyperparameter is {}'.format(grid.best_params_))\nprint('\\n')\nprint('Best Estimator is :')\ngrid.best_estimator_","d0a81dad":"#Using hyperparameter- max-depth=4, max_leaf_nodes=7\ndtree1= DecisionTreeRegressor(criterion='mse', max_depth= 4, max_leaf_nodes= 7, random_state=1)\ndtree1.fit(X_train, Y_train)\nmetrics.r2_score(Y_test, dtree1.predict(X_test))\nnp.sqrt(metrics.mean_squared_error(Y_test, dtree1.predict(X_test)))","38b00caf":"#Create DOT data\ndot_data= StringIO()\nexport_graphviz(grid.best_estimator_, out_file=dot_data, feature_names=X_train.columns.tolist(),class_names=['MEDV'], rounded=True, filled=True)\n\n#Draw Graph\ngraph= pydot.graph_from_dot_data(dot_data.getvalue())\n\n#Show Graph\nImage(graph[0].create_png())","d694a455":"#Important features in Decision tree\npd.DataFrame(grid.best_estimator_.feature_importances_, index=X_train.columns, columns=['Importance'])","15a4aa64":"#using only LSTAT and RM to predict the MEDV value\nX_train_dt= X_train[['LSTAT', 'RM']]\nX_test_dt= X_test[['LSTAT', 'RM']]\n\ndtr= DecisionTreeRegressor(criterion='mse', max_depth=4, max_leaf_nodes=7, random_state=1)\ndtr.fit(X_train_dt, Y_train)\n\n\nprint('The R2 score is {}'.format(metrics.r2_score(Y_test, dtr.predict(X_test_dt))))\nprint('The RMSE is {}'.format(np.sqrt(metrics.mean_squared_error(Y_test, dtr.predict(X_test_dt)))))","c6ef9ce7":"#Random Forest\n#Taking max_depth = 4 using Decision tree\n\nfrom sklearn.ensemble import RandomForestRegressor\nrf= RandomForestRegressor(random_state=1,max_depth=4)\nparams1= [{'n_estimators': range(10,100)}]\ngrid1= GridSearchCV(rf, param_grid=params1, refit=True)","7943a448":"grid1.fit(X_train, Y_train)","c083aeea":"grid_predictions1= grid1.predict(X_test)\n\nprint('Accuracy Score: {}'.format(metrics.r2_score(Y_test, grid_predictions1)))","c6a19ded":"print('The best value for n_estimator is {}'.format(grid1.best_params_))\npd.DataFrame(grid1.best_estimator_.feature_importances_, index=X_train.columns, columns=['Importance'])","a230581b":"print('One of the estimator is {}'.format(grid1.best_estimator_.estimators_[0]))","c0b11f1f":"#Create DOT data\ndot_data= StringIO()\nexport_graphviz(grid1.best_estimator_.estimators_[1], out_file=dot_data, feature_names=X_train.columns.tolist(),class_names=['MEDV'], rounded=True, filled=True)\n\n#Draw Graph\ngraph= pydot.graph_from_dot_data(dot_data.getvalue())\n\n#Show Graph\nImage(graph[0].create_png())","c7c02b09":"#Regularization\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso","9955b5d2":"lr= LinearRegression()\nlr.fit(X_train, Y_train)","8fc7d83e":"print(lr.coef_)\nprint('\\n')\nprint('The R2 score for train data is {}'.format(lr.score(X_train, Y_train)))\nprint('The R2 for test data is {}'.format(lr.score(X_test, Y_test)))","8ee79d06":"#Ridge Regression\nrr1= Ridge(alpha=0.01)\nrr1.fit(X_train, Y_train)\nprint(rr1.coef_)\nprint('\\n')\nprint('The R2 score for train data is {}'.format(rr1.score(X_train, Y_train)))\nprint('The R2 score for train data is {}'.format(rr1.score(X_test, Y_test)))\n\nsns.barplot(X_train.columns, rr1.coef_)\nplt.xticks(rotation=90)\n\n#looks like there is no change in coff wrt Linear Reg as alpha value is very small","c856fbda":"rr100= Ridge(alpha=100)\nrr100.fit(X_train, Y_train)\nprint(rr100.coef_)\nprint('\\n')\nprint('The R2 score for train data is {}'.format(rr100.score(X_train, Y_train)))\nprint('The R2 score for train data is {}'.format(rr100.score(X_test, Y_test)))\n\nsns.barplot(X_train.columns, rr100.coef_)\nplt.xticks(rotation=90)\n\n#looks like there is no change in coff wrt Linear Reg as alpha value is very small","2b2f370e":"#Plotting the coff values for LR, Ridge(0.01), Ridge(100)\nplt.plot(lr.coef_ ,linestyle='none' , marker='+', color='blue', markersize=10, label='lin reg')\nplt.plot(rr1.coef_ ,linestyle='none' , marker='o', color='green', label=' Ridge(0.01)')\nplt.plot(rr100.coef_ ,linestyle='none' , marker='*', color='red', label= 'Ridge(100)')\nplt.xlabel(boston.feature_names)\nplt.legend()\nplt.show()","3568ed62":"#Zipping the coff values\nlist(zip(X_train.columns, rr100.coef_))","e71395cd":"#Lasso regression\nlasso1= Lasso(alpha=0.01)\nlasso1.fit(X_train, Y_train)\n\nlasso1.coef_\nprint('The R2 score for train data is {}'.format(lasso1.score(X_train, Y_train)))\nprint('The R2 score for test data is {}'.format(lasso1.score(X_test, Y_test)))\n\nsns.barplot(X_train.columns, lasso1.coef_)\nplt.xticks(rotation=90)","5142e764":"lasso2= Lasso(alpha=1)\nlasso2.fit(X_train, Y_train)\n\nlasso2.coef_\nprint('The R2 score for train data is {}'.format(lasso2.score(X_train, Y_train)))\nprint('The R2 score for test data is {}'.format(lasso2.score(X_test, Y_test)))\n\nsns.barplot(X_train.columns, lasso2.coef_)\nplt.xticks(rotation=90)","19e91cd7":"list(zip(X_train.columns, lasso2.coef_))","076acee4":"plt.plot(lr.coef_, linestyle='none', marker='*', color='blue', label='lin reg')\nplt.plot(lasso1.coef_, linestyle='none', marker='o',color='red', label='lasso(0.01)')\nplt.plot(lasso2.coef_, linestyle='none', marker='+', color='green', label='lasso(1)')\nplt.xlabel(X_train.columns)\nplt.legend()\nplt.show()","f1f4a96d":"print('no of features where coff is not zero: {}'.format(sum((lasso2.coef_!=0))))","174c22e9":"train_score=[]\ntest_score= []\n\nfor i in [0.01,0.1,1,2,3,4,5,6,7,8,9,10]:\n    lasso= Lasso(alpha=i)\n    lasso.fit(X_train, Y_train)\n    \n    train_sc= lasso.score(X_train, Y_train)\n    test_sc= lasso.score(X_test, Y_test)\n    \n    train_score.append(train_sc)\n    test_score.append(test_sc)\n    \nprint(train_score)\nprint(test_score)","8cc9cc69":"plt.figure()\nplt.plot([0.01,0.1,1,2,3,4,5,6,7,8,9,10], train_score, marker='*', color='green')\nplt.plot([0.01,0.1,1,2,3,4,5,6,7,8,9,10], test_score, marker='+', color='red')\nplt.xlabel('x-values')\nplt.ylabel('accuracy score')\nplt.plot()\n\n#at x=1 the accuracy value is almost sme for train and test data","f9fc9124":"#Elastic net Regression\nfrom sklearn.linear_model import ElasticNet\nElastic1= ElasticNet(alpha=1)\n\nElastic1.fit(X_train, Y_train)\ntrain_score= Elastic1.score(X_train, Y_train)\ntest_score= Elastic1.score(X_test, Y_test)\n\nprint('Train score={}'.format(train_score))\nprint('Test score={}'.format(test_score))\nprint('No of features used={}'.format(np.sum(Elastic1.coef_!=0)))","5a533f7d":"train_score=[]\ntest_score= []\n\nfor i in [0.01,0.1,1,2,3,4,5,6,7,8,9,10]:\n    elastic= ElasticNet(alpha=i)\n    elastic.fit(X_train, Y_train)\n    \n    train_sc= elastic.score(X_train, Y_train)\n    test_sc= elastic.score(X_test, Y_test)\n    \n    train_score.append(train_sc)\n    test_score.append(test_sc)\n    \nprint(train_score)\nprint(test_score)","185dad3f":"plt.figure()\nplt.plot([0.01,0.1,1,2,3,4,5,6,7,8,9,10], train_score, marker='*', color='green')\nplt.plot([0.01,0.1,1,2,3,4,5,6,7,8,9,10], test_score, marker='+', color='red')\nplt.xlabel('x-values')\nplt.ylabel('accuracy score')\nplt.plot()\n\n#at x=2 the accuracy value is almost sme for train and test data","62275b66":"#Using Grid search CV for Elatic net regression\nfrom sklearn.model_selection import GridSearchCV\nelasticnet= ElasticNet()\nparam_elastic={'alpha':[0.01,0.1,1,2,3,4,5,6,7,8,9,10,100]}\ngrid_elastic= GridSearchCV(estimator=elasticnet, param_grid=param_elastic, cv=3, refit= True)","c3a25a00":"grid_elastic.fit(X_train, Y_train)","226aa1f5":"print(grid_elastic.best_params_)\nmetrics.r2_score(Y_test, grid_elastic.best_estimator_.predict(X_test))","125e8cdc":"The Boston Housing Dataset\n\nThe Boston Housing Dataset is a derived from information collected by the U.S. Census Service concerning housing in the area of Boston MA. The following describes the dataset columns:\n\nCRIM - per capita crime rate by town\nZN - proportion of residential land zoned for lots over 25,000 sq.ft.\nINDUS - proportion of non-retail business acres per town.\nCHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\nNOX - nitric oxides concentration (parts per 10 million)\nRM - average number of rooms per dwelling\nAGE - proportion of owner-occupied units built prior to 1940\nDIS - weighted distances to five Boston employment centres\nRAD - index of accessibility to radial highways\nTAX - full-value property-tax rate per $10,000\nPTRATIO - pupil-teacher ratio by town\nB - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\nLSTAT - % lower status of the population\nMEDV - Median value of owner-occupied homes in $1000's","6b0c9bd2":"=====================================================================================================================","ba91891a":"=============================================================================================================================","eb1aee9e":"==========================================================================================","4a974e99":"Thus using Backward elimination the R2 value is 0.741 and adj R2 is 0.735 "}}