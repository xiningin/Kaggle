{"cell_type":{"cd55b1ef":"code","c9bef6d2":"code","9ca86d62":"code","3fa41d66":"code","35c4c2b5":"code","e528aaaf":"code","fc8f9163":"code","1680c68e":"code","0acbf181":"code","76bc43f2":"code","026a3a0c":"code","a662b098":"markdown","dfe47fe6":"markdown","15176a90":"markdown"},"source":{"cd55b1ef":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c9bef6d2":"# Reading in the training data into 2 lists: X_train containg the images and Y the labels\n\nX_train = []\nY_train = []\n\nwith open('\/kaggle\/input\/mnist-digit-recognition-using-knn\/train_sample.csv') as train_file:\n    for line in train_file.readlines()[1:]:\n        content = line.strip().split(',')\n        label, image_raw_data = content[0], content[1:]\n        image_np = np.array_split(np.array(image_raw_data), 28)\n        X_train.append(image_np)\n        Y_train.append(label)\n\nX_train = np.array(X_train).astype(float)\nX_train = np.expand_dims(X_train, axis=3)\nY_train = np.array(Y_train).astype(float)\n\nprint(X_train.shape)\nprint(Y_train.shape)","9ca86d62":"# Reading in the testing data into 1 list: X_test containg the images\n\nX_test = []\n\nwith open('\/kaggle\/input\/mnist-digit-recognition-using-knn\/test_sample.csv') as test_file:\n    for line in test_file.readlines()[1:]:\n        content = line.strip().split(',')\n        image_raw_data = content[:]\n        image_np = np.array_split(np.array(image_raw_data), 28)\n        X_test.append(image_np)\n\nX_test = np.array(X_test).astype(float)\nX_test = np.expand_dims(X_test, axis=3)\n\nprint(X_test.shape)","3fa41d66":"# Downloading fonts from Github\n\n\n!wget --recursive --no-parent 'https:\/\/github.com\/google\/fonts\/raw\/master\/apache\/opensans\/OpenSans-Regular.ttf' -P \/usr\/local\/lib\/python3.6\/dist-packages\/matplotlib\/mpl-data\/fonts\/ttf\n!wget --recursive --no-parent 'https:\/\/github.com\/google\/fonts\/raw\/master\/apache\/opensans\/OpenSans-Light.ttf' -P \/usr\/local\/lib\/python3.6\/dist-packages\/matplotlib\/mpl-data\/fonts\/ttf\n!wget --recursive --no-parent 'https:\/\/github.com\/google\/fonts\/raw\/master\/apache\/opensans\/OpenSans-SemiBold.ttf' -P \/usr\/local\/lib\/python3.6\/dist-packages\/matplotlib\/mpl-data\/fonts\/ttf\n!wget --recursive --no-parent 'https:\/\/github.com\/google\/fonts\/raw\/master\/apache\/opensans\/OpenSans-Bold.ttf' -P \/usr\/local\/lib\/python3.6\/dist-packages\/matplotlib\/mpl-data\/fonts\/ttf","35c4c2b5":"#Codes by Joseph Assaker https:\/\/www.kaggle.com\/josephassaker\/cnn-mnist-digit-classification\/notebook?select=test.csv\n\n# Plotting the output labels distribution\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\n\nmpl.font_manager._rebuild()\n\nplt.rc('font', family='Open Sans')\n\nfig, ax = plt.subplots(figsize=(10, 5), dpi=240)\n\nbars = ax.bar(\n    x=np.unique(Y_train, return_counts=True)[0],\n    height=np.unique(Y_train, return_counts=True)[1],\n    tick_label=np.unique(Y_train, return_counts=True)[0].astype(int)\n)\n\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.spines['left'].set_visible(False)\nax.spines['bottom'].set_color('#DDDDDD')\nax.tick_params(bottom=False, left=False)\nax.set_axisbelow(True)\nax.yaxis.grid(True, color='#EEEEEE')\nax.xaxis.grid(False)\n\nbar_color = bars[0].get_facecolor()\nfor bar in bars:\n    ax.text(\n        bar.get_x() + bar.get_width() \/ 2,\n        bar.get_height() + 50,\n        round(bar.get_height(), 1),\n        horizontalalignment='center',\n        color=bar_color,\n        weight='bold'\n    )\n\nax.set_xlabel('Label', labelpad=20, color='#333333', fontsize=12)\nax.set_ylabel('Number of Training Samples', labelpad=16, color='#333333', fontsize=12)\nax.set_title('Label Counts Distribution', pad=14, color='#333333', fontsize=18, weight='bold')\n\nxl, yl, xh, yh=np.array(ax.get_position()).ravel()\nw=xh-xl\nh=yh-yl\nsize=0.075\n\nfor i in range(10):\n    xp=xl+w*(0.06 + (0.106 * i))\n    ax1=fig.add_axes([xp-size*0.5, 0.075, size, size])\n    ax1.axison = False\n    imgplot = ax1.imshow(X_train[np.where(Y_train==i)[0][0]].reshape((28,28)),cmap='Blues', vmin=0, vmax=255)\n\nfig.tight_layout","e528aaaf":"# Plotting 10 examples from each output class\n\nfrom collections import defaultdict\n\nf, axes = plt.subplots(10, 10, figsize=(16, 18))\n\ncounter = defaultdict(int)\n\nfor i in range(1000):\n    label = int(Y_train[i])\n    if counter[label] >= 10: continue\n    img = X_train[i].reshape((28,28))\n    axes[counter[label], label].imshow(img)\n    axes[counter[label], label].set_title(label)\n    counter[label] += 1\n        \n[ax.set_axis_off() for ax in axes.ravel()]\nplt.show()","fc8f9163":"# Defining the model, compiling and fitting it to the data\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.Input(shape=(28, 28, 1)),\n    tf.keras.layers.Dropout(0.25),\n    tf.keras.layers.Conv2D(32, (5,5), activation='relu', kernel_initializer = 'he_uniform', padding=\"SAME\"),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Conv2D(32, (5,5), activation='relu', kernel_initializer = 'he_uniform', padding=\"SAME\"),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu', kernel_initializer = 'he_uniform', padding=\"SAME\"),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu', kernel_initializer = 'he_uniform', padding=\"SAME\"),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Conv2D(128, (3,3), activation='relu', kernel_initializer = 'he_uniform', padding=\"SAME\"),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Conv2D(128, (3,3), activation='relu', kernel_initializer = 'he_uniform', padding=\"SAME\"),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(512, activation='relu', kernel_initializer = 'he_uniform'),\n    tf.keras.layers.Dropout(rate=0.5),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(256, activation='relu', kernel_initializer = 'he_uniform'),\n    tf.keras.layers.Dropout(rate=0.5),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(128, activation='relu', kernel_initializer = 'he_uniform'),\n    tf.keras.layers.Dropout(rate=0.5),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(64, activation='relu', kernel_initializer = 'he_uniform'),\n    tf.keras.layers.Dropout(rate=0.5),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(10, activation='softmax')\n])\n\nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nmodel.summary()\n\nearly_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor=\"accuracy\", mode='max', patience=25)\n\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint('.\/best_model.hdf5', monitor='accuracy',\n                                                               mode='max', verbose=1, save_best_model=True)\n\nreduce_lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='accuracy', mode='max', patience=15,\n                                                          verbose=1, factor=0.5, min_lr=0.0001)\n\nhistory = model.fit(\n    X_train \/ 255,\n    tf.keras.utils.to_categorical(Y_train, num_classes=10),\n    epochs=1000,\n    shuffle=True,\n    callbacks=[early_stopping_callback, model_checkpoint_callback,reduce_lr_callback]\n)","1680c68e":"# Plotting the training and pseudo-validation accuracies\n\nacc = history.history['accuracy']\nloss = history.history['loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.title('Training Accuracy')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, loss, 'b', label='Training Loss')\nplt.title('Training Loss')\nplt.legend()\n\nplt.show()","0acbf181":"# Predicting on the test set and saving it into a CSV submission file\n\npredictions = model.predict(X_test \/ 255)\ny_pred = np.argmax(predictions, axis=-1)\n\nsubmission_df = pd.DataFrame(data={\n    'ImageId': np.arange(1, X_test.shape[0] + 1),\n    'label': y_pred\n})\nsubmission_df.to_csv('train_sample.csv', index=False)\nsubmission_df","76bc43f2":"# Plotting 100 prediction examples\n\nf, axes = plt.subplots(10, 10, figsize=(16, 18))\n\ncounter = defaultdict(int)\n\nfor i in range(100):\n    label = int(y_pred[i])\n    img = X_test[i].reshape((28,28))\n    axes[i % 10, i \/\/ 10].imshow(img)\n    axes[i % 10, i \/\/ 10].set_title(label)\n    counter[label] += 1\n        \n[ax.set_axis_off() for ax in axes.ravel()]\nplt.show()","026a3a0c":"#Code by Olga Belitskaya https:\/\/www.kaggle.com\/olgabelitskaya\/sequential-data\/comments\nfrom IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#eb3434','#eb3446','Akronim','Smokum',30,15\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https:\/\/fonts.googleapis.com\/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';<\/style>\n    <h1 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s<\/h1>\"\"\"%string))\n    \n    \ndhtml('Thanks to Joseph Assaker  Mar\u00edlia Prata, @mpwolke was Here.' )","a662b098":"We have no Submission file, so I changed to `train_sample`.","dfe47fe6":"#Codes by Joseph Assaker https:\/\/www.kaggle.com\/josephassaker\/cnn-mnist-digit-classification\/notebook?select=test.csv","15176a90":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcT7bj7W8opeaMiuq2Nrfj_OydY9jFP0_9h7dQ&usqp=CAU)m.youtube.com"}}