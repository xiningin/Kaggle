{"cell_type":{"474c1b4f":"code","b39eadad":"code","f8b7cc97":"code","bf5b5f6c":"code","1c0f81a3":"code","cae6e30c":"code","f47b9d71":"code","84661a07":"code","54cf22d1":"code","0abf4587":"code","b0071983":"code","314cb4b8":"code","1b1403d3":"code","531969a4":"code","8593c4a0":"markdown","a91204fb":"markdown","f1d74547":"markdown","4f180968":"markdown","faadf24e":"markdown","33d3668f":"markdown","1be0d76f":"markdown","3fb132bf":"markdown"},"source":{"474c1b4f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#import os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b39eadad":"import os\nimport zipfile\nimport random\nimport shutil\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom shutil import copyfile\nfrom os import getcwd","f8b7cc97":"base_dir = '\/kaggle\/input\/cat-individuals\/cat_individuals_dataset\/'\n\nselection = [6, 7, 15, 18, 19, 29, 55, 57, 82, 152]\n#selection = range(1, 11)\nlabels = []\nfor i in selection:\n    label = '000' + str(i)\n    label = label[-4:]\n    labels.append(label)\n    \n\nif os. path. exists('\/tmp'): \n    shutil.rmtree('\/tmp')\ntry:\n    os.mkdir('\/tmp')\n    os.mkdir('\/tmp\/cats')\n    os.mkdir('\/tmp\/cats\/training')\n    os.mkdir('\/tmp\/cats\/validation')\n    os.mkdir('\/tmp\/cats\/testing')\n    for i in labels:\n        os.mkdir('\/tmp\/cats\/training\/' + i)\n        os.mkdir('\/tmp\/cats\/validation\/' + i)\n        os.mkdir('\/tmp\/cats\/testing\/' + i)\nexcept OSError:\n    pass","bf5b5f6c":"def split_data(SOURCE, TRAINING, VAL, TESTING, TRAIN_SIZE, VAL_SIZE):\n    files = []\n    for filename in os.listdir(SOURCE):\n        file = SOURCE + filename\n        if os.path.getsize(file) > 0:\n            files.append(filename)\n        else:\n            print(filename + \" is zero length, so ignoring.\")\n            \n    training_length = int(len(files) * TRAIN_SIZE)\n    val_length = int(len(files) * VAL_SIZE)\n    testing_length = int(len(files) - training_length - val_length)\n    shuffled_set = random.sample(files, len(files))\n    training_set = shuffled_set[:training_length]\n    val_set = shuffled_set[training_length:(training_length+val_length)]\n    testing_set = shuffled_set[-testing_length:]\n    \n    for filename in training_set:\n        this_file = SOURCE + filename\n        destination = TRAINING + filename\n        copyfile(this_file, destination)\n    \n    for filename in val_set:\n        this_file = SOURCE + filename\n        destination = VAL + filename\n        copyfile(this_file, destination)\n        \n    for filename in testing_set:\n        this_file = SOURCE + filename\n        destination = TESTING + filename\n        copyfile(this_file, destination)","1c0f81a3":"TRAIN_SIZE = 0.7\nVAL_SIZE = 0.2\n\nfor i in labels:\n    SOURCE = base_dir + i + '\/'\n    TRAINING = '\/tmp\/cats\/training\/' + i + '\/'\n    VAL = '\/tmp\/cats\/validation\/' + i + '\/'\n    TESTING = '\/tmp\/cats\/testing\/' + i + '\/'\n    split_data(SOURCE, TRAINING, VAL, TESTING, TRAIN_SIZE, VAL_SIZE)","cae6e30c":"for i in labels:\n    TRAINING_DIR = '\/tmp\/cats\/training\/' + i + '\/'\n    VAL_DIR = '\/tmp\/cats\/validation\/' + i + '\/'\n    TESTING_DIR = '\/tmp\/cats\/testing\/' + i + '\/'\n    print('train_size of {}: {}'.format(i, len(os.listdir(TRAINING_DIR))))\n    print('val_size of {}: {}'.format(i, len(os.listdir(VAL_DIR))))\n    print('test_size of {}: {}'.format(i, len(os.listdir(TESTING_DIR))))\n    print('\\n')","f47b9d71":"base_model = tf.keras.applications.vgg16.VGG16(include_top=False,\n                  input_shape = (150,150,3),\n                  weights = 'imagenet')\n\n# freeze all the layers of VGG, so they won't be trained.\nfor layer in base_model.layers:\n    layer.trainable = False\n    \n#for layer in base_model.layers:\n#    print(layer,layer.trainable)\n    \nmodel = tf.keras.models.Sequential([\n    base_model,\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(units=1024, activation='relu'),\n    tf.keras.layers.Dense(units=len(selection), activation='softmax')\n])\n\nmodel.summary()","84661a07":"model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])","54cf22d1":"TRAINING_DIR = \"\/tmp\/cats\/training\"\ntrain_datagen = ImageDataGenerator(\n        rescale = 1\/255.,\n        rotation_range = 40,\n        width_shift_range = .2,\n        height_shift_range = .2,\n        shear_range = .2,\n        zoom_range = .2,\n        horizontal_flip = True,\n        fill_mode = 'nearest'\n)\n\n# TRAIN GENERATOR.\ntrain_generator = train_datagen.flow_from_directory(\n        TRAINING_DIR,\n        target_size = (150, 150),\n        batch_size = 16,\n        class_mode = 'categorical'\n)\n\nVALIDATION_DIR = \"\/tmp\/cats\/validation\"\nvalidation_datagen = ImageDataGenerator(\n        rescale = 1\/255.,\n)\n\n# VALIDATION GENERATOR.\nvalidation_generator = validation_datagen.flow_from_directory(\n        VALIDATION_DIR,\n        target_size = (150, 150),\n        batch_size = 16,\n        class_mode = 'categorical'\n)","0abf4587":"history = model.fit(train_generator,\n                    epochs=20,\n                    verbose=1,\n                    validation_data=validation_generator)","b0071983":"import matplotlib.pyplot as plt\n\ndef plt_metric(history, metric, title, has_valid=True):\n    \"\"\"Plots the given 'metric' from 'history'.\n\n    Arguments:\n        history: history attribute of History object returned from Model.fit.\n        metric: Metric to plot, a string value present as key in 'history'.\n        title: A string to be used as title of plot.\n        has_valid: Boolean, true if valid data was passed to Model.fit else false.\n\n    Returns:\n        None.\n    \"\"\"\n    plt.plot(history[metric])\n    if has_valid:\n        plt.plot(history[\"val_\" + metric])\n        plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n    plt.title(title)\n    plt.ylabel(metric)\n    plt.xlabel(\"epoch\")\n    plt.show()\n\n\n# Plot the accuracy\nplt_metric(history=history.history, metric=\"acc\", title=\"Model accuracy\")\n\n# Plot the loss\nplt_metric(history=history.history, metric=\"loss\", title=\"Categorical_Crossentropy loss\")","314cb4b8":"TESTING_DIR = \"\/tmp\/cats\/testing\"\n\ntest_generator = validation_datagen.flow_from_directory(\n        TESTING_DIR,\n        target_size = (150, 150),\n        class_mode = 'categorical'\n)\n\nloss, acc = model.evaluate(test_generator)\nprint('Test loss:', loss)\nprint('Test accuracy:', acc)","1b1403d3":"import cv2\ndef re_id(test_img_path):\n    test_img = cv2.imread(test_img_path)\n    test_img = cv2.resize(test_img, (150, 150))\n    test_img = test_img\/255.\n    test_img = test_img.reshape(1, 150, 150, 3)\n    result = labels[np.argmax(model.predict(test_img))]\n    print('The true label of this cat individual is ID:{}'.format(test_label))\n    print('This cat is ID:{}'.format(result))\n    ","531969a4":"label_idx = 0\ntest_label = labels[label_idx]\ntest_img_path = os.path.join(TESTING_DIR, test_label, os.listdir(TESTING_DIR + '\/' + test_label)[0])\nre_id(test_img_path)","8593c4a0":"## Plot of accuracy and loss for each epoach","a91204fb":"## Additional Packages","f1d74547":"## Finally, evaluating our test set","4f180968":"## Compiling the model\n- optimizer function (Adam - adaptive moment estimation\n- loss function(categorical cross entropy) - softmax","faadf24e":"## Example - different image from existing cat indivduals ","33d3668f":"## Create training\/validation\/testing sets from directory","1be0d76f":"## Create model with VGG16 as base_model","3fb132bf":"## Data Augmentation"}}