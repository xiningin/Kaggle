{"cell_type":{"48a4dd53":"code","a245ddf9":"code","c158bb96":"code","2d9d271b":"code","e1155d9f":"code","aef3b8df":"code","fd3c8254":"code","8580156a":"code","51f758b8":"code","aacaeb75":"code","f71da926":"code","3cbc8523":"code","bce31864":"code","b3afe955":"code","8ab49252":"code","006d624c":"code","161e252f":"code","7c6006a7":"markdown","b3617062":"markdown","17fb0389":"markdown","4a0197db":"markdown","e268a67a":"markdown","5bd30d2c":"markdown","eac68506":"markdown","c4405b19":"markdown","ac8f02a5":"markdown","943b575f":"markdown","1db05111":"markdown","e40a0a6d":"markdown"},"source":{"48a4dd53":"# Importing Required Variables\nimport sys,math, copy, time, os\nimport re\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport numpy as np\nfrom scipy.spatial.distance import cosine\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n# print(os.listdir(\"..\/input\/womens-ecommerce-clothing-reviews\"))\n\n\nnp.set_printoptions(threshold=np.nan)\n# Reading the Data\noriginal_data = clothing_review = pd.read_csv(\"..\/input\/Womens Clothing E-Commerce Reviews.csv\")\nclothing_review = clothing_review.dropna(subset=['Review Text'])\nclothing_review = clothing_review.dropna(subset=['Clothing ID'])\nclothing_review = clothing_review.loc[clothing_review['Department Name'].isin([\"Tops\",\"Bottoms\"])]\ngrouped_review = clothing_review.groupby([\"Clothing ID\"])['Review Text'].apply(' ::: '.join).reset_index()\n\n\n#Getting Keywords\ntopWear = [ \"top\",\"blouse\",\"shirt\",\"upper\",\"dress\",\"torso\",\"tank\",\"sleeve\",\"body\",\"sweater\"]\nbottomWear = [\"pant\",\"jean\",\"slack\",\"skirt\",\"leg\",\"waist\",\"lower\",\"thigh\",\"trouser\",\"flare\"]\n\n\n# positivewords = []\n# with open(\"..\/input\/positive-and-negetive-words\/positive_words\", 'r') as readfile :\n#     temp = readfile.readline().strip()\n#     while temp != \"\" :\n#         positivewords.append(temp)\n#         temp = readfile.readline().strip()\n\n# negetivewords = []\n# with open(\"..\/input\/positive-and-negetive-words\/negetive_words\", 'r') as readfile :\n#     temp = readfile.readline().strip()\n#     while temp != \"\" :\n#         positivewords.append(temp)\n#         temp = readfile.readline().strip()\n\nwordnet_lemmatizer = WordNetLemmatizer()\n\nkeyWords = topWear + bottomWear# + positivewords + negetivewords\n\nfor i in range(len(keyWords)) :\n    keyWords[i] = wordnet_lemmatizer.lemmatize(keyWords[i])\n    \n# Clearing the data from extra characters\ndata = []\nactual_labels = []\nfor i in range(len(grouped_review[\"Review Text\"])):\n    j = grouped_review[\"Review Text\"][i].lower()\n    j = re.sub(r'[^A-Za-z ]', '', j)\n    data.append(j)\n    compare = grouped_review.iloc[i][\"Clothing ID\"]\n    temp = clothing_review.loc[clothing_review['Clothing ID'] == compare]\n    id_cloth = temp.iloc[0].loc['Department Name']\n    actual_labels.append(id_cloth)\n    \n# Tokenising the data\ntokenizer = RegexpTokenizer(r'\\w+')\nfor i in range(len(data)) :\n    data[i] = tokenizer.tokenize(data[i])\n\n# Getting the list of stop words\nstopWords = list(stopwords.words('english'))\nstopWords = [re.sub(r'[^A-Za-z ]', '', j) for j in stopWords]\n\n\n# Lemmatizing and removing stop words\nwordnet_lemmatizer = WordNetLemmatizer()\ndataFiltered = []\nfor each_review in data :\n    temp = []\n    for word in each_review : \n        if not word in stopWords and word in keyWords:\n            temp.append(wordnet_lemmatizer.lemmatize(word))\n    dataFiltered.append(temp)\n\n\n# dataFiltered.append(keyWords)\n\n# Creating the word list\n\nwordList = list(keyWords)\nwordList.sort()\n\nnumber_of_reviews = len(dataFiltered)\nwordListIndex = { wordList[i]: i for i in range(len(wordList))}\nnDocsPerWord = {i : 0 for i in wordList}\n\nfor i in range(len(actual_labels)) :\n    if actual_labels[i] == \"Tops\" :\n        actual_labels[i] = 0\n    else :\n        actual_labels[i] = 1\n\n\n\n","a245ddf9":"tf = np.zeros(shape=(number_of_reviews,len(wordList)))\n\nfor i in range(len(dataFiltered)):\n    this_doc_accounted = []\n    for j in dataFiltered[i] :\n        print(j)\n        if j in topWear :\n            tf[i][wordListIndex[j]] = 1\n        elif j in bottomWear :\n            tf[i][wordListIndex[j]] = -1\n        elif j in keyWords :\n            tf[i][wordListIndex[j]]\n        if not j in this_doc_accounted :\n            this_doc_accounted.append(j)\n            print(j in nDocsPerWord)\n            nDocsPerWord[j] += 1\n            \ntf_normalized = copy.deepcopy(tf)\ntf_normalized = tf_normalized \/ tf_normalized.max(axis=0)\n","c158bb96":"tfIdf = copy.deepcopy(tf)\n\nfor i in range(number_of_reviews) :\n    for k in dataFiltered[i]:\n        j = wordListIndex[k]\n        if tfIdf[i][j] != 0 :\n            tfIdf[i][j] = tfIdf[i][j]*math.log(number_of_reviews\/nDocsPerWord[wordList[j]])\n\n\ntfIdf_normalized = copy.deepcopy(tfIdf)\ntfIdf_normalized = tfIdf_normalized \/ tfIdf_normalized.max(axis=0)\n","2d9d271b":"temprow = np.zeros(len(wordList))\nfor i in range(len(temprow)) :\n    if wordList[i] in topWear :\n        temprow[i] = 1\n\n\ntfIdf = np.vstack([tfIdf, temprow])\ntf = np.vstack([tf, temprow])\ntfIdf_normalized = np.vstack([tfIdf_normalized, temprow])\ntf_normalized = np.vstack([tf_normalized, temprow])\n\nprint(np.isnan(np.min(tfIdf)))\nprint(np.isnan(np.min(tf)))\nprint(np.isnan(np.min(tfIdf_normalized)))\nprint(np.isnan(np.min(tf_normalized)))","e1155d9f":"# K-means Clustering\nfrom sklearn.cluster import KMeans\nfrom sklearn.mixture import GaussianMixture\n\nkmeans_clothing = KMeans(n_clusters=2,random_state=0).fit(tf)\nkmeans_centroids = kmeans_clothing.cluster_centers_\n\nkmeans_labels= kmeans_clothing.labels_\ntop_label = kmeans_labels[-1]\ncorrect = 0\nfor i in range(len(kmeans_labels) -1 ) :\n    compare = grouped_review.iloc[i][\"Clothing ID\"]\n    temp = clothing_review.loc[clothing_review['Clothing ID'] == compare]\n    id_cloth = temp.iloc[0].loc['Department Name']\n    if kmeans_labels[i] == top_label:\n        if id_cloth == \"Tops\" :\n            correct += 1\n    else :\n        if id_cloth == \"Bottoms\" :\n            correct +=1\n\nprint (correct\/ (len(kmeans_labels) -1))\n\n# Agg Clustering ---------------------------------\nfrom sklearn.cluster import AgglomerativeClustering\n\nprint((~tf.any(axis=1)).any())\n# temp = np.append(tf, np.ones((len(tf),1)),axis=1)\nagg_iris = AgglomerativeClustering(n_clusters= 2,linkage=\"average\",affinity=\"manhattan\").fit(tf)\n#Getting labels\nagg_labels = agg_iris.labels_\n\ntop_label = agg_labels[-1]\ncorrect = 0\nfor i in range(len(agg_labels) -1 ) :\n    compare = grouped_review.iloc[i][\"Clothing ID\"]\n    temp = clothing_review.loc[clothing_review['Clothing ID'] == compare]\n    id_cloth = temp.iloc[0].loc['Department Name']\n    if agg_labels[i] == top_label:\n        if id_cloth == \"Tops\" :\n            correct += 1\n    else :\n        if id_cloth == \"Bottoms\" :\n            correct +=1\n\nprint (correct\/ (len(agg_labels) -1))\n\n# GMM Clustering ---------------------------------\n\ngm_labels = GaussianMixture(2).fit_predict(tf)\n\nprint(gm_labels)\ntop_label = gm_labels[-1]\ncorrect = 0\nfor i in range(len(gm_labels) -1 ) :\n    compare = grouped_review.iloc[i][\"Clothing ID\"]\n    temp = clothing_review.loc[clothing_review['Clothing ID'] == compare]\n    id_cloth = temp.iloc[0].loc['Department Name']\n    if gm_labels[i] == top_label:\n        if id_cloth == \"Tops\" :\n            correct += 1\n    else :\n        if id_cloth == \"Bottoms\" :\n            correct +=1\n\nprint (correct\/ (len(gm_labels) -1))\n","aef3b8df":"\nkmeans_clothing = KMeans(n_clusters=2,random_state=0).fit(tfIdf)\nkmeans_centroids = kmeans_clothing.cluster_centers_\n\nkmeans_labels= kmeans_clothing.labels_\ntop_label = kmeans_labels[-1]\ncorrect = 0\nfor i in range(len(kmeans_labels) -1 ) :\n    compare = grouped_review.iloc[i][\"Clothing ID\"]\n    temp = clothing_review.loc[clothing_review['Clothing ID'] == compare]\n    id_cloth = temp.iloc[0].loc['Department Name']\n    if kmeans_labels[i] == top_label:\n        if id_cloth == \"Tops\" :\n            correct += 1\n    else :\n        if id_cloth == \"Bottoms\" :\n            correct += 1\n\nprint (correct\/ (len(kmeans_labels) -1))\n\n# Agg Clustering ---------------------------------\nfrom sklearn.cluster import AgglomerativeClustering\n\n\nagg_iris = AgglomerativeClustering(n_clusters= 2,linkage=\"average\",affinity=\"manhattan\").fit(tfIdf)\n#Getting labels\nagg_labels = agg_iris.labels_\n\ntop_label = agg_labels[-1]\ncorrect = 0\nfor i in range(len(agg_labels) -1 ) :\n    compare = grouped_review.iloc[i][\"Clothing ID\"]\n    temp = clothing_review.loc[clothing_review['Clothing ID'] == compare]\n    id_cloth = temp.iloc[0].loc['Department Name']\n    if agg_labels[i] == top_label:\n        if id_cloth == \"Tops\" :\n            correct += 1\n    else :\n        if id_cloth == \"Bottoms\" :\n            correct +=1\n\nprint (correct\/ (len(agg_labels) -1))","fd3c8254":"tf_matrix = tf # D x V matrix \nA = tf_matrix.T \n\nU, s, V = np.linalg.svd(A, full_matrices=1, compute_uv=1)\n\nK =  2 # number of components\n\nA_reduced = np.dot(U[:,:K], np.dot(np.diag(s[:K]), V[:K, :])) # D x V matrix \n\ndocs_rep = np.dot(np.diag(s[:K]), V[:K, :]).T # D x K matrix \nterms_rep = np.dot(U[:,:K], np.diag(s[:K])) # V x K matrix \n\n# print (A_reduced)\n# print (docs_rep)\n# print (terms_rep)\n\nkey_word_indices = [wordList.index(key_word) for key_word in keyWords] # vocabulary indices \n\nkey_words_rep = terms_rep[key_word_indices,:]     \nquery_rep = np.sum(key_words_rep, axis = 0)\n\n\nsvd_start = time.time()\nquery_doc_cos_dist = [cosine(query_rep, doc_rep) for doc_rep in docs_rep]\nsvd_end = time.time()\nquery_doc_sort_index = np.argsort(np.array(query_doc_cos_dist))\n\n\nmax_iter = 5\nfor rank, sort_index in enumerate(query_doc_sort_index):\n    print(rank + 1, \") Cosine value : \", float(query_doc_cos_dist[sort_index]) ,\"\\n\", clothing_review[\"Review Text\"].iloc[sort_index],\"\\n\")\n    max_iter -= 1\n    if max_iter == 0 :\n        break\n\n;","8580156a":"\nkmeans_clothing = KMeans(n_clusters=2,random_state=0).fit(docs_rep)\n\n\nkmeans_labels= kmeans_clothing.labels_\ntop_label = kmeans_labels[-1]\ncorrect = 0\nfor i in range(len(kmeans_labels) -1 ) :\n    compare = grouped_review.iloc[i][\"Clothing ID\"]\n    temp = clothing_review.loc[clothing_review['Clothing ID'] == compare]\n    id_cloth = temp.iloc[0].loc['Department Name']\n    if kmeans_labels[i] == top_label:\n        if id_cloth == \"Tops\" :\n            correct += 1\n    else :\n        if id_cloth == \"Bottoms\" :\n            correct += 1\n\nprint (correct\/ (len(kmeans_labels) -1))","51f758b8":"from sklearn.cluster import AgglomerativeClustering\n\nprint(np.isnan(np.min(docs_rep)))\n\n\nagg_iris = AgglomerativeClustering(n_clusters= 2,linkage=\"average\",affinity=\"manhattan\").fit(docs_rep)\n\n\n#Getting labels\nagg_labels = agg_iris.labels_\n\ntop_label = agg_labels[-1]\ncorrect = 0\nfor i in range(len(agg_labels) -1 ) :\n    compare = grouped_review.iloc[i][\"Clothing ID\"]\n    temp = clothing_review.loc[clothing_review['Clothing ID'] == compare]\n    id_cloth = temp.iloc[0].loc['Department Name']\n    if agg_labels[i] == top_label:\n        if id_cloth == \"Tops\" :\n            correct += 1\n    else :\n        if id_cloth == \"Bottoms\" :\n            correct +=1\n\nprint (correct\/ (len(agg_labels) -1))\n","aacaeb75":"gm_labels = GaussianMixture(2).fit_predict(docs_rep)\n\nprint(gm_labels)\ntop_label = gm_labels[-1]\ncorrect = 0\nfor i in range(len(gm_labels) -1 ) :\n    compare = grouped_review.iloc[i][\"Clothing ID\"]\n    temp = clothing_review.loc[clothing_review['Clothing ID'] == compare]\n    id_cloth = temp.iloc[0].loc['Department Name']\n    if gm_labels[i] == top_label:\n        if id_cloth == \"Tops\" :\n            correct += 1\n    else :\n        if id_cloth == \"Bottoms\" :\n            correct +=1\n\nprint (correct\/ (len(gm_labels) -1))\n","f71da926":"import itertools\n\nii = itertools.count(docs_rep.shape[0])\ntree = [{'node_id': next(ii), 'left': x[0], 'right':x[1]} for x in agg_iris.children_]\n\n# print(tree)\n","3cbc8523":"%matplotlib inline\nimport matplotlib.pyplot as plt\nprint(len(docs_rep[:,0]),len(actual_labels))\nplt.scatter(docs_rep[:,0][:-1], docs_rep[:,1][:-1], c=actual_labels) # all documents \nplt.scatter(docs_rep[:,0][-1], docs_rep[:,1][-1], marker='+', c=agg_labels[-1]) # the query \nplt.xlabel(\"Component 1\")\nplt.ylabel(\"Component 2\")\nplt.show()\nplt.plot()\nplt.scatter(docs_rep[:,0][:-1], docs_rep[:,1][:-1], c=agg_labels[:-1]) # all documents \nplt.scatter(docs_rep[:,0][-1], docs_rep[:,1][-1], marker='+', c=agg_labels[-1]) # the query \nplt.xlabel(\"Component 1\")\nplt.ylabel(\"Component 2\")\nplt.show()\nplt.scatter(docs_rep[:,0][:-1], docs_rep[:,1][:-1], c=kmeans_labels[:-1]) # all documents \nplt.scatter(docs_rep[:,0][-1], docs_rep[:,1][-1], marker='+', c=kmeans_labels[-1]) # the query \nplt.show()\nplt.scatter(docs_rep[:,0][:-1], docs_rep[:,1][:-1], c=gm_labels[:-1]) # all documents \nplt.scatter(docs_rep[:,0][-1], docs_rep[:,1][-1], marker='+', c=gm_labels[-1]) # the query \nplt.show()","bce31864":"all_data = original_data.dropna()\n\ntops = []\nfor i in range(1,len(agg_labels) -1) :\n    if agg_labels[i] == 1 :\n        tops.append( clothing_review.iloc[i].loc[\"Clothing ID\"] )\n\n\ntop_data_pandas = all_data[all_data['Clothing ID'].isin(tops)]\ntop_data_pandas = top_data_pandas.reset_index(drop=True)\nprint(top_data_pandas.shape)\n\ntop_data = np.zeros(shape=(top_data_pandas.shape[0],3))\nprint(top_data.shape)\n\nfor  index, row in top_data_pandas.iterrows() :\n    top_data[index][0] = int(row[\"Age\"])\n    top_data[index][1] = int(row[\"Rating\"])\n    top_data[index][2] = int(row[\"Recommended IND\"])\n\n\nagg_tops = AgglomerativeClustering(n_clusters= 2,linkage=\"average\",affinity=\"manhattan\").fit(top_data)\nagg_tops_labels= agg_tops.labels_\nprint(top_data.shape)\n","b3afe955":"data = []\nprint(top_data_pandas.shape)\nfor i in top_data_pandas[\"Review Text\"]:\n    j = i.lower()\n    j = re.sub(r'[^A-Za-z ]', '', j)\n    data.append(j)\n    \n# Tokenising the data\ntokenizer = RegexpTokenizer(r'\\w+')\nfor i in range(len(data)) :\n    data[i] = tokenizer.tokenize(data[i])\n\n\n# Getting the list of stop words\nstopWords = list(stopwords.words('english'))\nstopWords = [re.sub(r'[^A-Za-z ]', '', j) for j in stopWords]\n\n# Lemmatizing and removing stop words\nwordnet_lemmatizer = WordNetLemmatizer()\ndataFiltered = []\nfor each_review in data :\n    temp = []\n    for word in each_review : \n        if not word in stopWords :\n            temp.append(wordnet_lemmatizer.lemmatize(word))\n    dataFiltered.append(temp)\n\n\n\n# Creating the word list\nwordList = np.array(dataFiltered)\nwordList = np.hstack(wordList)\nwordList = list(set(wordList))\nwordList.sort()\nnumber_of_reviews = len(dataFiltered)\nwordListIndex = { wordList[i]: i for i in range(len(wordList))}\nnDocsPerWord = {i : 0 for i in wordList}\n\ntf_top = np.zeros(shape=(number_of_reviews,len(wordList)))\nprint(tf_top.shape, top_data.shape)\n\nfor i in range(len(dataFiltered)):\n    this_doc_accounted = []\n    for j in dataFiltered[i] :\n        tf_top[i][wordListIndex[j]] += 1\n        if not j in this_doc_accounted :\n            this_doc_accounted.append(j)\n            nDocsPerWord[j] += 1\n\ntfIdf_top = copy.deepcopy(tf_top)\n\nprint(tfIdf_top.shape)\nfor i in range(number_of_reviews) :\n    for k in dataFiltered[i]:\n        j = wordListIndex[k]\n        if tfIdf_top[i][j] != 0 :\n            tfIdf_top[i][j] = tfIdf_top[i][j]*math.log(number_of_reviews\/nDocsPerWord[wordList[j]])\n            \n#\nprint(top_data.shape, tfIdf_top.shape)\ntop_data = np.concatenate((top_data, tfIdf_top), axis=1)","8ab49252":"from sklearn.preprocessing import normalize\n\ntop_data = top_data \/ top_data.max(axis=0)\n\ntf_matrix = top_data # D x V matrix \nA = tf_matrix.T \n\nU, s, V = np.linalg.svd(A, full_matrices=1, compute_uv=1)\n\nK =  2 # number of components\n\nA_reduced = np.dot(U[:,:K], np.dot(np.diag(s[:K]), V[:K, :])) # D x V matrix \n\ndocs_rep = np.dot(np.diag(s[:K]), V[:K, :]).T # D x K matrix \nterms_rep = np.dot(U[:,:K], np.diag(s[:K])) # V x K matrix \n\n# print (A_reduced)\n# print (docs_rep)\n# print (terms_rep)\n\nkey_word_indices = [wordList.index(key_word) for key_word in keyWords] # vocabulary indices \n\nkey_words_rep = terms_rep[key_word_indices,:]     \nquery_rep = np.sum(key_words_rep, axis = 0)\n\ndef removeOutliers(x):\n    to_ret = []\n    for i in x :\n        print(i)\n        if abs(i[0] - i[1]) < 500:\n            print(\"oh ya\")\n            to_ret.append(i)\n    print (to_ret)\n    return np.array(to_ret)\n    \n\n# docs_rep = removeOutliers(docs_rep)\nprint(docs_rep.shape)\n\nagg_tops = AgglomerativeClustering(n_clusters= 2,linkage=\"average\",affinity=\"manhattan\").fit(docs_rep)\nnew_labels = agg_tops.labels_","006d624c":"print(np.where(new_labels==1))\nprint(docs_rep[177])","161e252f":"%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.scatter(top_data[:,0], top_data[:,1],c= agg_tops_labels) # all documents \nplt.show()\n\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nx =top_data[:,0]\ny =top_data[:,1]\nz =top_data[:,2]\n\nax.scatter(x, y, z, c=agg_tops_labels, marker='o')\nax.set_xlabel('Age')\nax.set_ylabel('Rating')\nax.set_zlabel('Recommended IND')\nplt.show()\n\nplt.scatter(docs_rep[:,0][:], docs_rep[:,1][:], c=new_labels[:]) # all documents \nplt.show()\n\n\nplt.scatter(docs_rep[:,0][:], docs_rep[:,1][:], c=top_data[:,2]) # all documents \nplt.show()","7c6006a7":"## Text Analysis\n\n#### Dataset : Women's E-Commerce Clothing Reviews\n\n### 1. Pre-Processing of Data\n   * Remove all extra charecters such as punctuations, non charecters, etc\n   * Tokenisation\n   * Lametisation of data. (preffered over stemming as stemming can corrupt data in some cases)","b3617062":"### 5. Plotting","17fb0389":"### 2. Creation of TF matrix","4a0197db":"### 4. Adding Keyword as a dummy to detect clusters","e268a67a":"#### Agglomorative clustering to find relevance","5bd30d2c":"### 3. Creation of TF-IDF matrix from calculated TF matrix","eac68506":"### SVD Clustering","c4405b19":"* #### From LSA using TF matrix","ac8f02a5":"### Clustering on Tfidf","943b575f":"## Finding heirarical patterns","1db05111":"#### Top data TFIDF","e40a0a6d":"### Clustering on tf"}}