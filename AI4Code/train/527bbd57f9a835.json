{"cell_type":{"e502fa46":"code","cfe4a4bd":"code","266fdd07":"code","c5ad9500":"code","33fbac30":"code","28672c14":"code","4316f1a5":"code","a02d0caa":"code","d75c9c32":"code","30cd5d12":"markdown"},"source":{"e502fa46":"# ====== decorators.py ======\nimport functools\nimport time\n\n\nclass DecoratorInputOutputExceptionTime(object):\n    def __call__(self, fn):\n        @functools.wraps(fn)\n        def decorated(*args, **kwargs):\n            try:\n                print(\"{} ({} - {})\".format(fn.__name__, args, kwargs))\n                tic = time.time()\n                result = fn(*args, **kwargs)\n                toc = time.time()\n                print(\"Result: {} [in {:.4f}s]\".format(result, toc-tic))\n                return result\n            except Exception as ex:\n                print(\"Exception {0}\".format(ex))\n                raise ex\n        return decorated\n","cfe4a4bd":"# ====== io.py ======\n\n\nimport pathlib\nimport os\nimport pandas as pd\nimport pickle\n\ntrain_file_path = '\/kaggle\/input\/covid19-global-forecasting-week-3\/train.csv'\ntest_file_path = '\/kaggle\/input\/covid19-global-forecasting-week-3\/test.csv'\nsubmission_file_path = '\/kaggle\/input\/covid19-global-forecasting-week-3\/submission.csv'\nmy_submission_file_path = 'submission.csv'\nenrichment_file_path = '\/kaggle\/input\/covid-19-enriched-dataset-week-2\/enriched_covid_19_week_2.csv'\n\n\ndef cast_columns(df: pd.DataFrame) -> pd.DataFrame:\n    df.Province_State = df.Province_State.astype('category')\n    df.Country_Region = df.Country_Region.astype('category')\n    df.Date = pd.to_datetime(df.Date)\n    return df\n\n\ndef read_enrichment_file() -> pd.DataFrame:\n    df = pd.read_csv(enrichment_file_path, na_filter=False)\n    # Fix the representation of Country_Region colunn\n    df['Country_Region'] = df['Country_Region'].apply(lambda val: val.split('_')[0])\n\n    grouped_cols = ['Country_Region', 'Province_State']\n    cast_columns(df)\n    enriched_df = df.sort_values('Date').groupby(grouped_cols).last().reset_index().dropna()\n    enriched_df = enriched_df.drop(columns=[\"Id\", \"ConfirmedCases\", \"Fatalities\", \"Date\"])\n\n    # Fill the missing rows with avg values\n    avg_row = enriched_df.mean().to_dict()\n    train_df = read_train_file()\n    train_df_locations = train_df.groupby(grouped_cols).last().reset_index().dropna()[grouped_cols]\n\n    joined_df = pd.merge(train_df_locations, enriched_df[grouped_cols], indicator=True, on=grouped_cols, how='left')\n    for _, row in joined_df[joined_df._merge != 'both'].iterrows():\n        avg_row['Country_Region'] = row.Country_Region\n        avg_row['Province_State'] = row.Province_State\n        enriched_df = enriched_df.append(pd.Series(avg_row), ignore_index=True)\n    assert len(enriched_df) == len(train_df_locations)\n    return enriched_df\n\n\ndef read_train_file() -> pd.DataFrame:\n    df = pd.read_csv(train_file_path, na_filter=False)\n    cast_columns(df)\n    return df\n\n\ndef read_test_file() -> pd.DataFrame:\n    df = pd.read_csv(test_file_path, na_filter=False)\n    cast_columns(df)\n    return df\n\n\ndef get_train_subset(pc: float) -> pd.DataFrame:\n    df = read_train_file().sample(frac=pc).reset_index(drop=True).copy(deep=True)\n    return df\n\n\ndef save_pickle(obj: object, file_path: str) -> None:\n    with open(file_path, \"wb\") as pk:\n        pickle.dump(obj, pk)\n\n\ndef load_pickle(file_path: str) -> object:\n    # May rise IOError if the file is not there\n    with open(file_path, \"rb\") as pk:\n        return pickle.load(pk)\n\n\ndef write_submission_csv(submission_df: pd.DataFrame) -> None:\n    df = submission_df[['ForecastId', 'ConfirmedCases', 'Fatalities']]\n    df.to_csv(my_submission_file_path, header=True, index=False)\n","266fdd07":"# ====== featurization.py ======\nimport random\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nfrom typing import List, Optional, Tuple\n\n\ndef get_empty_point(point: pd.Series, date: datetime) -> pd.Series:\n    new_point = point.copy(deep=True)\n    new_point.Date = date\n    new_point.Fatalities = 0.0\n    new_point.ConfirmedCases = 0.0\n    new_point.Id = -1\n    return new_point\n\n\ndef get_points_in_the_past(point: pd.Series, max_lookback_days: int, train_points: pd.DataFrame) -> List[pd.Series]:\n    history = []\n    for i in range(1, max_lookback_days+1):\n        date = point.Date - pd.DateOffset(days=i)\n        past_point = train_points[(train_points.Province_State == point.Province_State) & \\\n                                  (train_points.Country_Region == point.Country_Region) & \\\n                                  (train_points.Date == date)]\n        if len(past_point) == 1:\n            history.append(past_point.iloc[0])\n        else:\n            history.append(get_empty_point(point, date))\n\n    assert len(history) == max_lookback_days\n    return history[::-1]\n\n\ndef randomize_the_points_and_censor_the_last_n(points: List[pd.Series],\n                                               n_points_to_keep: int,\n                                               censor_days: Optional[int],) -> List[Optional[pd.Series]]:\n    if censor_days is None:\n        censor_days = random.randint(0, 14)  # up to 30 days of censorship\n    points = points[:len(points)-censor_days]\n    if len(points) < n_points_to_keep:\n        raise Exception('There are not enough points to choose from (after censorship)')\n\n    sampled_points = random.sample(points, n_points_to_keep)\n    assert len(sampled_points) == n_points_to_keep\n    return sorted(sampled_points, key=lambda el: el.Date)\n\n\ndef to_features(point: pd.Series, history: List[pd.Series], is_test_set: bool) -> Tuple[str, pd.DataFrame, Tuple[float, float]]:\n    province_state = point.Province_State\n    country_region = point.Country_Region\n    date = point.Date\n    feats = []\n    for p in history:\n        point_dict = {\n            'days_back': -1 * (p.Date - date).days,\n            'confirmed_cases': p.ConfirmedCases,\n            'fatalities': p.Fatalities\n        }\n        feats.append(point_dict)\n    df = pd.DataFrame.from_records(feats)\n    labels = (None, None) if is_test_set else (point.ConfirmedCases, point.Fatalities)\n    return f\"{province_state}{country_region}\", df, labels\n\n\ndef get_enriched_row(point: pd.Series, enrichment_df: pd.DataFrame) -> pd.Series:\n    row = enrichment_df[(enrichment_df.Province_State == point.Province_State) & \\\n                        (enrichment_df.Country_Region == point.Country_Region)].iloc[0]\n    return row\n\n\ndef to_feat_row(feat: Tuple[str, pd.DataFrame, Optional[Tuple[float, float]]],\n                enriched_row: pd.Series, row_id: Optional[int] = None) -> pd.Series:\n\n    loc, df, labels = feat\n    feat_row = {'location': loc, 'label_confirmed_cases': labels[0], 'label_fatalities': labels[1]}\n    if row_id:\n        feat_row['ForecastId'] = row_id\n\n    for idx, row in df.iterrows():\n        feat_row[f'days_back_{idx}'] = row.days_back\n        feat_row[f'confirmed_cases_{idx}'] = row.confirmed_cases\n        feat_row[f'fatalities_{idx}'] = row.fatalities\n\n    for col in enriched_row.to_dict().keys():\n        if col in ('Province_State', 'Country_Region'):\n            continue\n        feat_row[col] = enriched_row[col]\n\n    return pd.Series(feat_row)\n\n\ndef generate_features(train_df: pd.DataFrame, validation_df: pd.DataFrame, test_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    numerical_columns = [col for col in train_df.columns if not col[:5] in ('label', 'locat')]\n    for df in [train_df, validation_df, test_df]:\n        for col in numerical_columns:\n            df[col + '_sqrt'] = df[col].pow(1.\/2)\n            df[col + '_log2'] = np.log2(df[col].values + 1)\n    return train_df, validation_df, test_df\n\n","c5ad9500":"# ====== modeling.py ======\n\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Embedding, Dense, Input, Concatenate, Dropout, Reshape, BatchNormalization\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom typing import List, Optional, Tuple, Dict\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\n\n\n\ndef label_encode_columns(cols: List[str], df_train: pd.DataFrame, df_val: pd.DataFrame, df_test: pd.DataFrame) -> \\\n                            Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, Dict[str, LabelEncoder]]:\n    les = {}\n    for col in cols:\n        le = LabelEncoder()\n        df_train[col + '_encoded'] = le.fit_transform(df_train[col])\n        df_val[col + '_encoded'] = le.transform(df_val[col])\n        df_test[col + '_encoded'] = le.transform(df_test[col])\n        les[col] = le\n    return df_train, df_val, df_test, les\n\n\ndef create_model(cat_encoded_cols: List[str], numerical_cols: List[str], labels: List[str],\n                 df_train: pd.DataFrame, df_val: pd.DataFrame, model_path: str) -> Tuple[keras.Model, StandardScaler]:\n\n    X_train_cat = df_train[cat_encoded_cols].values\n    X_train_num = df_train[numerical_cols].values\n    Y_train = df_train[labels].values\n\n    X_val_cat = df_val[cat_encoded_cols].values\n    X_val_num = df_val[numerical_cols].values\n    Y_val = df_val[labels].values\n\n    # scaling numerical inputs\n    ss = StandardScaler()\n    X_train_num = ss.fit_transform(X_train_num)\n    X_val_num = ss.transform(X_val_num)\n\n    cat_distinct_values = len(df_train.groupby(cat_encoded_cols))\n\n    cat_inputs = Input(shape=(len(cat_encoded_cols)))\n    cat_embeddings = Embedding(cat_distinct_values, 64)(cat_inputs)\n    modifier_cat_embeddings = Reshape(target_shape=(64,))(cat_embeddings)\n    cat_output = Dense(128, activation='relu')(modifier_cat_embeddings)\n\n    num_inputs = Input(shape=(len(numerical_cols)))\n    num_dense = Dense(1024, activation='relu')(num_inputs)\n    num_normalized = BatchNormalization()(num_dense)\n    num_dropout = Dropout(0.2)(num_normalized)\n    num_output = Dense(256, activation='relu')(num_dropout)\n\n    merge_layer = Concatenate(axis=-1)([cat_output, num_output])\n    normalization_layer = BatchNormalization()(merge_layer)\n    merge_dropout_layer = Dropout(0.2)(normalization_layer)\n    dense_layer = Dense(256, activation='relu')(merge_dropout_layer)\n    dropout_layer = Dropout(0.2)(dense_layer)\n\n    cases = Dense(1, activation='relu')(dropout_layer)\n    fatalities = Dense(1, activation='relu')(dropout_layer)\n\n    model = Model(inputs=[cat_inputs, num_inputs], outputs=[cases, fatalities])\n    model.compile(loss=[tf.keras.losses.MeanSquaredLogarithmicError(),\n                        tf.keras.losses.MeanSquaredLogarithmicError()],\n                  optimizer=Adam())\n\n    print(model.summary())\n    callbacks = [ReduceLROnPlateau(monitor='val_loss', patience=20, verbose=1, factor=0.8),\n                 EarlyStopping(monitor='val_loss', patience=50),\n                 ModelCheckpoint(filepath=model_path+'_best_model.h5', monitor='val_loss', save_best_only=True)]\n\n    train_history = model.fit([X_train_cat, X_train_num], [Y_train[:,0], Y_train[:,1]],\n                              epochs=500,\n                              batch_size=2048,  # with BatchNormalization, should be large enough\n                              validation_data=([X_val_cat, X_val_num], [Y_val[:, 0], Y_val[:, 1]]),\n                              callbacks=callbacks)\n    plt.plot(train_history.history['loss'])\n    plt.plot(train_history.history['val_loss'])\n    plt.title('loss per epoch')\n    plt.ylabel('overall loss')\n    plt.xlabel('# epochs')\n    plt.legend(['train', 'val'])\n    plt.show()\n    return model, ss\n\n\ndef predict_test_cases(model: keras.Model, scaler: StandardScaler, df_test: pd.DataFrame, cat_encoded_cols: List[str], numerical_cols: List[str]) -> pd.DataFrame:\n    X_test_cat = df_test[cat_encoded_cols].values\n    X_test_num = scaler.transform(df_test[numerical_cols].values)\n    Y_pred = model.predict([X_test_cat, X_test_num])\n    return Y_pred\n","33fbac30":"# ====== main.py ======\nimport os\nimport pandas as pd\nfrom typing import List, Optional, Tuple\nfrom multiprocessing import Pool\nfrom functools import partial\n\n# from lib.io import get_train_subset, read_test_file, save_pickle, load_pickle, write_submission_csv, read_enrichment_file\n# from lib.featurization import (get_points_in_the_past, randomize_the_points_and_censor_the_last_n, to_features,\n#                                to_feat_row, generate_features, get_enriched_row)\n# from lib.modeling import label_encode_columns, create_model, predict_test_cases\n\nCACHE_PATH = '\/tmp\/covid_'\nMAX_LOOKBACK = 40\nN_POINTS_TO_KEEP = 24\nCENSOR = None  # random between 1 and 10\nCPU_COUNT = os.cpu_count()\n\n\ndef row_to_feat_row(row: pd.Series, train_df: pd.DataFrame, enrichment_df: pd.DataFrame, keep_id_col=False) -> pd.Series:\n    history = get_points_in_the_past(row, MAX_LOOKBACK, train_df)\n    history = randomize_the_points_and_censor_the_last_n(history, N_POINTS_TO_KEEP, CENSOR)\n    enriched_row = get_enriched_row(row, enrichment_df)\n    feat = to_features(row, history, keep_id_col)\n    feat_row = to_feat_row(feat, enriched_row, row.ForecastId if keep_id_col else None)\n    return feat_row\n\n\ndef get_train_validation_sets(pc_train: float) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    train_df = get_train_subset(1.0)\n    enrichment_df = read_enrichment_file()\n    oversampling_factor = 2\n    train_len = int(pc_train * oversampling_factor * len(train_df))\n    validation_len = len(train_df) - train_len\n\n    train_rows = [el[1] for el in train_df.iterrows() for _ in range(oversampling_factor)]\n    with Pool(CPU_COUNT * 2) as p:\n        trains = p.map(partial(row_to_feat_row, train_df=train_df, enrichment_df=enrichment_df), train_rows[:train_len])\n    with Pool(CPU_COUNT * 2) as p:\n        validations = p.map(partial(row_to_feat_row, train_df=train_df, enrichment_df=enrichment_df), train_rows[-validation_len:])\n\n    return pd.DataFrame(trains), pd.DataFrame(validations)\n\n\ndef get_test_set() -> pd.DataFrame:\n    train_df = get_train_subset(1.0)\n    test_df = read_test_file()\n    enrichment_df = read_enrichment_file()\n\n    test_rows = [el[1] for el in test_df.iterrows()]\n    with Pool(CPU_COUNT * 2) as p:\n        tests = p.map(partial(row_to_feat_row, train_df=train_df, enrichment_df=enrichment_df, keep_id_col=True), test_rows)\n\n    return pd.DataFrame(tests)\n\n\ndef clip_and_fix_submission(submission_df: pd.DataFrame) -> pd.DataFrame:\n    for idx in submission_df.index:\n        max_cases = submission_df.loc[idx][f'confirmed_cases_{N_POINTS_TO_KEEP-1}']\n        max_fatalities = submission_df.loc[idx][f'fatalities_{N_POINTS_TO_KEEP-1}']\n        if submission_df.loc[idx]['ConfirmedCases'] < max_cases:\n            submission_df.at[idx, 'ConfirmedCases'] = max_cases\n        if submission_df.loc[idx]['Fatalities'] < max_fatalities:\n            submission_df.at[idx, 'Fatalities'] = max_fatalities\n    return submission_df\n\n","28672c14":"try:\n    train_df, validation_df, test_df = load_pickle(CACHE_PATH + 'datasets.pk')\nexcept IOError:\n    print(f'Creating datasets with {CPU_COUNT} CPUs')\n    print('Train and validation ...')\n    train_df, validation_df = get_train_validation_sets(pc_train=0.8)\n    print('Test ...')\n    test_df = get_test_set()\n    save_pickle([train_df, validation_df, test_df], CACHE_PATH + 'datasets.pk')\n\ntrain_df.head(5)","4316f1a5":"\ntrain_df, validation_df, test_df, label_encoder_dict = label_encode_columns(['location'], train_df, validation_df, test_df)\ntrain_df, validation_df, test_df = generate_features(train_df, validation_df, test_df)\n\ntrain_df.head(5)","a02d0caa":"categorical_columns = ['location_encoded']\nnumerical_columns = [col for col in train_df.columns if not col[:5] in ('label', 'locat')]\nlabel_columns = ['label_confirmed_cases', 'label_fatalities']\nmodel, scaler = create_model(cat_encoded_cols=categorical_columns,\n                             numerical_cols=numerical_columns,\n                             labels=label_columns,\n                             df_train=train_df,\n                             df_val=validation_df,\n                             model_path=CACHE_PATH)\nlabels_pred = predict_test_cases(model, scaler, test_df, categorical_columns, numerical_columns)\nfor i in range(len(label_columns)):\n    test_df[label_columns[i]] = labels_pred[i]\ntest_df = test_df.rename(columns={'label_confirmed_cases': 'ConfirmedCases',\n                                  'label_fatalities': 'Fatalities'})\n","d75c9c32":"submission_df = clip_and_fix_submission(test_df)[['ForecastId', 'ConfirmedCases', 'Fatalities']].astype('int')\nwrite_submission_csv(submission_df)","30cd5d12":"#### > Inspired by: https:\/\/www.kaggle.com\/frlemarchand\/covid-19-forecasting-with-an-rnn\n> Requires an extra dataset: https:\/\/www.kaggle.com\/optimo\/covid19-enriched-dataset-week-2\n> \n\n## Preprocessing\n\n1. Traininig data is read and put into 2 tables at random: train (80%) and validation (20%).\n2. For each point, 40 (variable MAX_LOOKBACK) points of the past are collected (ConfirmedCases and Fatalities features). If data is not available, ConfirmedCases and Fatalities are both set to 0.\n3. At random, between 0 and 14 points (variable CENSOR) of the most recent datapoints are deleted\/censored. This is to make the classifier able to predict up to 14 days in the future.\n4. Of the remanining datapoints, only 24 (variable N_POINTS_TO_KEEP) of them are kept ar random. This is to train the model to deal with missing information of the past.\n5. Those 14 points are reshaped into an array, containing (days_in_the_past, confirmed_cases and fatalities)\n * The feature vector looks like this:\n * `[days_in_the_past_0=22, confirmed_cases_0=0, fatalities_0=0,`\n    `days_in_the_past_1=20, confirmed_cases_1=5, fatalities_1=0,`\n    `...`\n    `days_in_the_past_23=4, confirmed_cases_23=36, fatalities_23=5]`\n6. The information is then joined with the location (i.e. the union of the features Country_Region and Province_State)\n7. And the array is finally joined with the information coming from the enrichment table. This table is idexed by location, and provides a set of info connected to the location (age split, smokers percentage,...) see the enrichment dataset above for more info.\n8. This process is done for all the points in the training set, validation set and testing set.\n\n\n## Modeling (using TF\/Keras)\n\n1. The only categorical feature is the location. It's label-encoded\n2. The numerical features are all the others. To each numerical column, is created its sqrt and log2 version.\n3. Numerical data goes into a StandardScaler, trained on the train dataset, and applied on the training, validation and test.\n4. The Deep model is the following:\n * The label-encoded categorical feature goes through a Embedding(out_size=64) and Dense(128)\n * The numerical features goes into a Dense(1024), BatchNormalization, Dropout(0.2), Dense(256)\n * The two output layers are Concatenate-d. Then it's applied BatchNormalization, Dropout(0.2), Dense(256) and a Dropout(0.2).\n * Finally, the two predicted labels are two Dense(1) layers.\n * All the Dense layers uses a relu activation. All of them.\n * Optimiser=Adam, batch_size is 2048 (due to the BatchNormalization layers)\n * Callbacks: ReduceLROnPlateau, EarlyStopping\n\n## Postprocessing\n1. The prediction must be above the historical data (see point 3, preprocessing), so both predictions goes into a max(prediction, max(historical_data))\n2. Predictions are written as csv\n"}}