{"cell_type":{"b8212413":"code","1971c13e":"code","6a598de6":"code","82c0b888":"code","0c8a64fd":"code","efdae64b":"code","2ea05342":"code","80728668":"code","7ff38a99":"code","782155c9":"code","6723c393":"code","08a9f624":"code","5aa6cac6":"code","ae0935c5":"code","c6e89cda":"code","6a569901":"code","4aee67ad":"code","43c5e154":"code","32775841":"code","766738fa":"code","87f8d7c1":"code","4e2d9897":"code","e83e678d":"code","d9c00fb7":"code","4d09f747":"code","b090c900":"code","d51fa9d9":"code","5cb4392e":"code","597336cb":"code","160e00a3":"markdown","c4cf749c":"markdown","61e41948":"markdown","c0aa3f0e":"markdown","6218418b":"markdown","b95e162d":"markdown","1d380e8c":"markdown","b379da61":"markdown","c5c27fc7":"markdown","9894afab":"markdown","906a14a8":"markdown","f849c3fc":"markdown","adcce5c4":"markdown","a5af4b23":"markdown","927bd25d":"markdown","9ab7e64e":"markdown","a3b777c2":"markdown","aab3cd2c":"markdown","10db7860":"markdown","cf826901":"markdown","058e0baf":"markdown","3a9e5c36":"markdown","a03a3ff7":"markdown","743f4b04":"markdown","fe810d07":"markdown","a3613c15":"markdown","3cad8433":"markdown","55b77de3":"markdown","28bca9fc":"markdown","30a31b65":"markdown"},"source":{"b8212413":"!pip install unidecode\n!pip install transformers\n!pip install emoji","1971c13e":"CONTRACTIONS = {\n    \"I'm\": \"I am\",\n    \"I'm'a\": \"I am about to\",\n    \"I'm'o\": \"I am going to\",\n    \"I've\": \"I have\",\n    \"I'll\": \"I will\",\n    \"I'll've\": \"I will have\",\n    \"I'd\": \"I would\",\n    \"I'd've\": \"I would have\",\n    \"Whatcha\": \"What are you\",\n    \"amn't\": \"am not\",\n    \"ain't\": \"are not\",\n    \"aren't\": \"are not\",\n    \"'cause\": \"because\",\n    \"can't\": \"cannot\",\n    \"can't've\": \"cannot have\",\n    \"could've\": \"could have\",\n    \"couldn't\": \"could not\",\n    \"couldn't've\": \"could not have\",\n    \"daren't\": \"dare not\",\n    \"daresn't\": \"dare not\",\n    \"dasn't\": \"dare not\",\n    \"didn't\": \"did not\",\n    \"didn\u2019t\": \"did not\",\n    \"don't\": \"do not\",\n    \"don\u2019t\": \"do not\",\n    \"doesn't\": \"does not\",\n    \"e'er\": \"ever\",\n    \"everyone's\": \"everyone is\",\n    \"finna\": \"fixing to\",\n    \"gimme\": \"give me\",\n    \"gon't\": \"go not\",\n    \"gonna\": \"going to\",\n    \"gotta\": \"got to\",\n    \"hadn't\": \"had not\",\n    \"hadn't've\": \"had not have\",\n    \"hasn't\": \"has not\",\n    \"haven't\": \"have not\",\n    \"he've\": \"he have\",\n    \"he's\": \"he is\",\n    \"he'll\": \"he will\",\n    \"he'll've\": \"he will have\",\n    \"he'd\": \"he would\",\n    \"he'd've\": \"he would have\",\n    \"here's\": \"here is\",\n    \"how're\": \"how are\",\n    \"how'd\": \"how did\",\n    \"how'd'y\": \"how do you\",\n    \"how's\": \"how is\",\n    \"how'll\": \"how will\",\n    \"isn't\": \"is not\",\n    \"it's\": \"it is\",\n    \"'tis\": \"it is\",\n    \"'twas\": \"it was\",\n    \"it'll\": \"it will\",\n    \"it'll've\": \"it will have\",\n    \"it'd\": \"it would\",\n    \"it'd've\": \"it would have\",\n    \"kinda\": \"kind of\",\n    \"let's\": \"let us\",\n    \"luv\": \"love\",\n    \"ma'am\": \"madam\",\n    \"may've\": \"may have\",\n    \"mayn't\": \"may not\",\n    \"might've\": \"might have\",\n    \"mightn't\": \"might not\",\n    \"mightn't've\": \"might not have\",\n    \"must've\": \"must have\",\n    \"mustn't\": \"must not\",\n    \"mustn't've\": \"must not have\",\n    \"needn't\": \"need not\",\n    \"needn't've\": \"need not have\",\n    \"ne'er\": \"never\",\n    \"o'\": \"of\",\n    \"o'clock\": \"of the clock\",\n    \"ol'\": \"old\",\n    \"oughtn't\": \"ought not\",\n    \"oughtn't've\": \"ought not have\",\n    \"o'er\": \"over\",\n    \"shan't\": \"shall not\",\n    \"sha'n't\": \"shall not\",\n    \"shalln't\": \"shall not\",\n    \"shan't've\": \"shall not have\",\n    \"she's\": \"she is\",\n    \"she'll\": \"she will\",\n    \"she'd\": \"she would\",\n    \"she'd've\": \"she would have\",\n    \"should've\": \"should have\",\n    \"shouldn't\": \"should not\",\n    \"shouldn't've\": \"should not have\",\n    \"so've\": \"so have\",\n    \"so's\": \"so is\",\n    \"somebody's\": \"somebody is\",\n    \"someone's\": \"someone is\",\n    \"something's\": \"something is\",\n    \"sux\": \"sucks\",\n    \"that're\": \"that are\",\n    \"that's\": \"that is\",\n    \"that'll\": \"that will\",\n    \"that'd\": \"that would\",\n    \"that'd've\": \"that would have\",\n    \"em\": \"them\",\n    \"there're\": \"there are\",\n    \"there's\": \"there is\",\n    \"there'll\": \"there will\",\n    \"there'd\": \"there would\",\n    \"there'd've\": \"there would have\",\n    \"these're\": \"these are\",\n    \"they're\": \"they are\",\n    \"they've\": \"they have\",\n    \"they'll\": \"they will\",\n    \"they'll've\": \"they will have\",\n    \"they'd\": \"they would\",\n    \"they'd've\": \"they would have\",\n    \"this's\": \"this is\",\n    \"those're\": \"those are\",\n    \"to've\": \"to have\",\n    \"wanna\": \"want to\",\n    \"wasn't\": \"was not\",\n    \"we're\": \"we are\",\n    \"we've\": \"we have\",\n    \"we'll\": \"we will\",\n    \"we'll've\": \"we will have\",\n    \"we'd\": \"we would\",\n    \"we'd've\": \"we would have\",\n    \"weren't\": \"were not\",\n    \"what're\": \"what are\",\n    \"what'd\": \"what did\",\n    \"what've\": \"what have\",\n    \"what's\": \"what is\",\n    \"what'll\": \"what will\",\n    \"what'll've\": \"what will have\",\n    \"when've\": \"when have\",\n    \"when's\": \"when is\",\n    \"where're\": \"where are\",\n    \"where'd\": \"where did\",\n    \"where've\": \"where have\",\n    \"where's\": \"where is\",\n    \"which's\": \"which is\",\n    \"who're\": \"who are\",\n    \"who've\": \"who have\",\n    \"who's\": \"who is\",\n    \"who'll\": \"who will\",\n    \"who'll've\": \"who will have\",\n    \"who'd\": \"who would\",\n    \"who'd've\": \"who would have\",\n    \"why're\": \"why are\",\n    \"why'd\": \"why did\",\n    \"why've\": \"why have\",\n    \"why's\": \"why is\",\n    \"will've\": \"will have\",\n    \"won't\": \"will not\",\n    \"won't've\": \"will not have\",\n    \"would've\": \"would have\",\n    \"wouldn't\": \"would not\",\n    \"wouldn't've\": \"would not have\",\n    \"y'all\": \"you all\",\n    \"y'all're\": \"you all are\",\n    \"y'all've\": \"you all have\",\n    \"y'all'd\": \"you all would\",\n    \"y'all'd've\": \"you all would have\",\n    \"you're\": \"you are\",\n    \"you've\": \"you have\",\n    \"you'll've\": \"you shall have\",\n    \"you'll\": \"you will\",\n    \"you'd\": \"you would\",\n    \"you'd've\": \"you would have\"\n}","6a598de6":"import os\nimport shutil\nimport string\nfrom pathlib import Path\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport tensorflow as tf\nimport pandas as pd\nfrom pandas import DataFrame\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\nfrom unidecode import unidecode","82c0b888":"nltk.download('stopwords')\nnltk.download('punkt')","0c8a64fd":"TRAIN_PATH = Path('..\/input\/nlp-getting-started\/train.csv')\nTEST_PATH = Path('..\/input\/nlp-getting-started\/test.csv')\n\ntweets_train = pd.read_csv(TRAIN_PATH)\ntweets_test = pd.read_csv(TEST_PATH)\n\ntweets_train_raw = tweets_train.copy()\ntweets_test_raw = tweets_test.copy()","efdae64b":"def get_copy(df: DataFrame) -> DataFrame:\n    return df.copy()\n\n\ndef remove_links(df: DataFrame, column_name: str) -> DataFrame:\n\n    df[column_name] = df[column_name].str.replace(r'https?:\\\/\\\/\\S*', '', regex=True)\n    return df\n\n\ndef remove_whitespace_newline_tabs(df: DataFrame, column_name: str) -> DataFrame:\n\n    df[column_name] = df[column_name].str.replace(r'\\s+', ' ', regex=True)\n    return df\n\ndef remove_mentions(df: DataFrame, column_name: str) -> DataFrame:\n\n    df[column_name] = df[column_name].str.replace(r'@\\S+', '', regex=True)\n    return df\n\n\ndef remove_punctuation(df: DataFrame, column_name: str) -> DataFrame:\n\n    #punct = '!\"$%&()*+,-.\/:;<=>?[\\]^_`{|}~'\n    df[column_name] = df[column_name].str.replace(f'[{string.punctuation}]', ' ', regex=True)\n    #df[column_name] = df[column_name].str.replace(f'[{punct}]', ' ', regex=True)\n    return df\n\n\ndef to_lower(df: DataFrame, column_name: str) -> DataFrame:\n\n    df[column_name] = df[column_name].str.lower()\n    return df\n\n\ndef remove_accents(df: DataFrame, column_name: str) -> DataFrame:\n\n    df[column_name] = df[column_name].apply(unidecode)\n    return df\n\ndef remove_unicode(df: DataFrame, column_name: str) -> DataFrame:\n    \n    func = lambda text: text.encode('ascii', 'ignore').decode()\n\n    df[column_name] = df[column_name].apply(func)\n    return df\n\ndef expand_contractions(df: DataFrame, column_name: str) -> DataFrame:\n\n    def expand(text):\n        for word in text.split():\n            text = text.replace(word, CONTRACTIONS.get(word, word))\n        return text\n\n    df[column_name] = df[column_name].apply(expand)\n    return df\n\ndef remove_numbers(df: DataFrame, column_name: str) -> DataFrame:\n\n    df[column_name] = df[column_name].str.replace(r'[0-9]+', '', regex=True)\n    return df\n\ndef text_strip(df: DataFrame, column_name: str) -> DataFrame:\n\n    df[column_name] = df[column_name].str.strip()\n    return df\n\ndef remove_stop_words(df: DataFrame, column_name: str) -> DataFrame:\n\n    def remove(text):\n        filter = []\n        stopWords = set(stopwords.words('english'))\n        nltk_words = word_tokenize(text)\n        for w in nltk_words:\n            if w.lower() not in stopWords:\n                filter.append(w)\n\n        resp = ' '.join([str(s) for s in filter])\n        return resp\n\n    df[column_name] = df[column_name].apply(remove)\n    return df\n\ndef clean_text(df: DataFrame, column_name: str) -> DataFrame:\n\n    df_cleaned = (df.pipe(get_copy)\n                    .pipe(remove_unicode, column_name)\n                    .pipe(remove_links, column_name)\n                    .pipe(remove_mentions, column_name)\n                    .pipe(expand_contractions, column_name)\n                    .pipe(remove_punctuation, column_name)\n                    .pipe(remove_whitespace_newline_tabs, column_name)\n                    .pipe(remove_numbers, column_name)\n                    .pipe(to_lower, column_name)\n                    .pipe(remove_stop_words, column_name)\n                    .pipe(text_strip, column_name))\n    return df_cleaned\n","2ea05342":"train_cleaned = clean_text(tweets_train, 'text')\ntest_cleaned = clean_text(tweets_test, 'text')","80728668":"x = train_cleaned[['text']]\ny = train_cleaned['target']\ntest_final = test_cleaned[['text']]\n\nx_train_val, x_test, y_train_val, y_test = train_test_split(x,y,test_size=0.2, stratify=y, random_state=42)\nx_train, x_val, y_train, y_val = train_test_split(x_train_val,y_train_val,test_size=0.2, stratify=y_train_val, random_state=42)","7ff38a99":"from transformers import DistilBertTokenizerFast\n\ndbert_tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')","782155c9":"from transformers import BertTokenizer\n\nbert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","6723c393":"from transformers import BertweetTokenizer\n\ntweet_tokenizer = BertweetTokenizer.from_pretrained(\"vinai\/bertweet-base\", normalization=True)","08a9f624":"def batch_encode(tokenizer, texts, batch_size=256, max_length=50):\n    \n    input_ids = []\n    attention_mask = []\n    \n    for i in range(0, len(texts), batch_size):\n        batch = texts[i:i+batch_size]\n        inputs = tokenizer.batch_encode_plus(batch,\n                                             max_length=max_length,\n                                             padding='max_length',\n                                             truncation=True,\n                                             return_attention_mask=True,\n                                             return_token_type_ids=False\n                                             )\n        input_ids.extend(inputs['input_ids'])\n        attention_mask.extend(inputs['attention_mask'])\n    \n    \n    return tf.convert_to_tensor(input_ids), tf.convert_to_tensor(attention_mask)","5aa6cac6":"tokenizer = bert_tokenizer\n\nX_train_ids, X_train_attention = batch_encode(tokenizer, list(x_train['text']))\n\nX_valid_ids, X_valid_attention = batch_encode(tokenizer, list(x_val['text']))\n\nX_test_ids, X_test_attention = batch_encode(tokenizer, list(x_test['text']))\n\nX_id, X_attention = batch_encode(tokenizer, list(x['text']))\n\ntest_final_id, test_final_attention = batch_encode(tokenizer, list(test_final['text']))","ae0935c5":"params = {'MAX_LENGTH': 50,\n          'EPOCHS': 22,\n          'LEARNING_RATE': 5e-5,\n          'BATCH_SIZE': 64,\n          'STEPS': len(x_train.index) \/\/ 64,\n          'DROPOUT': 0.2,\n          'THRESHOLD': 0.5,          \n          'RANDOM_STATE':42\n          }","c6e89cda":"from transformers import TFDistilBertModel, DistilBertConfig\n\ndistil_config = DistilBertConfig(dropout=params['DROPOUT'], \n                          attention_dropout=params['DROPOUT'], \n                          output_hidden_states=True)\n\ndistil_bert = TFDistilBertModel.from_pretrained('distilbert-base-uncased', config=distil_config)\n\nfor layer in distil_bert.layers:\n    layer.trainable = False","6a569901":"from transformers import TFBertModel, BertConfig\n\nbert_config = BertConfig(dropout=params['DROPOUT'], \n                          attention_dropout=params['DROPOUT'], \n                          output_hidden_states=True)\n\nbert = TFBertModel.from_pretrained(\"bert-base-uncased\", config=bert_config)\n\nfor layer in bert.layers:\n    layer.trainable = False","4aee67ad":"from transformers import TFAutoModel\n\nbert_tweet = TFAutoModel.from_pretrained(\"vinai\/bertweet-base\")\n\nfor layer in bert_tweet.layers:\n    layer.trainable = False","43c5e154":"def build_model(transformer, max_length=params['MAX_LENGTH']):\n    \n    weight_initializer = tf.keras.initializers.GlorotNormal(seed=params['RANDOM_STATE']) \n    \n    input_ids_layer = tf.keras.layers.Input(shape=(max_length,), name='input_ids', dtype='int32')\n    input_attention_layer = tf.keras.layers.Input(shape=(max_length,), name='input_attention', dtype='int32')\n    \n    # output do transformer\n    last_hidden_state = transformer([input_ids_layer, input_attention_layer])[0]\n    cls_token = last_hidden_state[:, 0, :]\n    \n    # aplicacao da camada de classifica\u00e7\u00e3o\n    D1 = tf.keras.layers.Dropout(0.2,seed=42)(cls_token)\n    X = tf.keras.layers.Dense(256,activation='relu',kernel_initializer=weight_initializer,bias_initializer='zeros')(D1)\n    D2 = tf.keras.layers.Dropout(0.2,seed=42)(X)\n    X = tf.keras.layers.Dense(32,activation='relu',kernel_initializer=weight_initializer,bias_initializer='zeros')(D2)\n    D3 = tf.keras.layers.Dropout(0.2,seed=42)(X)\n    \n    output = tf.keras.layers.Dense(1, \n                                   activation='sigmoid',\n                                   kernel_initializer=weight_initializer,  \n                                   kernel_constraint=None,\n                                   bias_initializer='zeros')(D3)\n    \n    # cria\u00e7\u00e3o do modelo\n    model = tf.keras.Model([input_ids_layer, input_attention_layer], output)\n    \n    # compila\u00e7\u00e3o do modelo\n    model.compile(tf.keras.optimizers.Adam(lr=params['LEARNING_RATE']), \n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n    \n    return model","32775841":"model_distil = build_model(distil_bert)\nmodel_bert = build_model(bert)\nmodel_tweet = build_model(bert_tweet)","766738fa":"train_history_distil_1 = model_bert.fit(\n    x = [X_train_ids, X_train_attention],\n    y = y_train.to_numpy(),\n    epochs = params['EPOCHS'],\n    batch_size = params['BATCH_SIZE'],\n    steps_per_epoch = params['STEPS'],\n    validation_data = ([X_valid_ids, X_valid_attention], y_val.to_numpy()),\n    verbose=2\n)","87f8d7c1":"train_history_bert_1 = model_bert.fit(\n    x = [X_train_ids, X_train_attention],\n    y = y_train.to_numpy(),\n    epochs = params['EPOCHS'],\n    batch_size = params['BATCH_SIZE'],\n    steps_per_epoch = params['STEPS'],\n    validation_data = ([X_valid_ids, X_valid_attention], y_val.to_numpy()),\n    verbose=2\n)","4e2d9897":"train_history_tweet_1 = model_tweet.fit(\n    x = [X_train_ids, X_train_attention],\n    y = y_train.to_numpy(),\n    epochs = params['EPOCHS'],\n    batch_size = params['BATCH_SIZE'],\n    steps_per_epoch = params['STEPS'],\n    validation_data = ([X_valid_ids, X_valid_attention], y_val.to_numpy()),\n    verbose=2\n)","e83e678d":"for layer in distil_bert.layers:\n    layer.trainable = True","d9c00fb7":"for layer in bert.layers:\n    layer.trainable = True","4d09f747":"for layer in bert_tweet.layers:\n    layer.trainable = True","b090c900":"optimizer = tf.keras.optimizers.Adam(lr=params['LEARNING_RATE'])\n\nmodel_bert.compile(optimizer=optimizer, \n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss',\n                                                  mode='min',\n                                                  min_delta=0,\n                                                  patience=0,\n                                                  restore_best_weights=True)\n\ntrain_history_bert_2 = model_bert.fit(\n    x = [X_train_ids, X_train_attention],\n    y = y_train.to_numpy(),\n    epochs = params['EPOCHS'],\n    batch_size = params['BATCH_SIZE'],\n    steps_per_epoch = params['STEPS'],\n    validation_data = ([X_valid_ids, X_valid_attention], y_val.to_numpy()),\n    callbacks=[early_stopping],\n    verbose=2\n)","d51fa9d9":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import f1_score\nimport numpy as np\ny_pred = model_bert.predict([X_test_ids, X_test_attention])\ny_pred_thresh = np.where(y_pred >= params['THRESHOLD'], 1, 0)\nauc_roc = roc_auc_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred_thresh)\n\nprint(f'f1: {f1}')\nprint(f'ROC-AUC: {auc_roc}')","5cb4392e":"y_final = model_bert.predict([test_final_id, test_final_attention])\ny_final_thresh = np.where(y_final >= params['THRESHOLD'], 1, 0)","597336cb":"result = pd.DataFrame({'target':y_final_thresh[:,0]}, index=tweets_test_raw['id'])\ny_final.to_csv('..\/output\/result.csv')","160e00a3":"## Result","c4cf749c":"We then need to transform the words into something that the machine can understand. Tokenization achieves this by indexing the words.","61e41948":"## Text pre-processing","c0aa3f0e":"Training the three models.","6218418b":"We then normalyze the train and test datasets.","b95e162d":"We instantiate the transformers and use them to build our model, which is composed of transformers and classification layers.","1d380e8c":"We achieved a 0.83328 score and the 64th position at the time of writing.","b379da61":"Finally, we used the model to predict on the test set and post the results on Kaggle.","c5c27fc7":"## Data","9894afab":"## Train, test, validation split","906a14a8":"# Kaggle Competition - Natural Language Processing with Disaster Tweets","f849c3fc":"## Contractions","adcce5c4":"As usual, we create three sets for training, validating, and testing. This is critical to avoid bias on the results.","a5af4b23":"##### Text Normalization","927bd25d":"## Imports","9ab7e64e":"## Setup","a3b777c2":"##### Functions","aab3cd2c":"We create different function for each processing type. Note some common operations, such as removing links, ponctuation, and stop words.","10db7860":"## Tokenization","cf826901":"Finally, we create a model and train it on the training set.","058e0baf":"We used f1_score to validate our results, this is a competition requirement.","3a9e5c36":"For our approach we decided to experiment with BERT to analyse the tweets and predict disaster related tweets.\nhttps:\/\/www.kaggle.com\/c\/nlp-getting-started\/","a03a3ff7":"## Model","743f4b04":"We used a pre-trained model, leveraging the concept of **tranfer learning** to reduce training times. We used three versions of BERT to evaluate for effectiveness: **standard BERT**, **distil BERT** (a more compact version), and **BERTweet** (similar to RoBERTa but pre-trained with tweets).","fe810d07":"## Metrics","a3613c15":"Since we downloaded the data we just have to read it into a daframe.","3cad8433":"Apply the enconder on each set. BERT's tokenizator returns the ids and an attention mask , this is used to correlate which ids are to be used.","55b77de3":"Usually the first step is to perform text pre-processing. We remove links, mentions, ponctuation, UTF-8 characters, and other types of text that do not carry semantic value.","28bca9fc":"Instantiating each transformer model.","30a31b65":"Prepare the batch encoder:"}}