{"cell_type":{"74d720ce":"code","a9439873":"code","f8f59697":"code","a922db49":"code","7092be82":"code","c32a34f7":"code","05dca83b":"code","8decb21e":"code","c68be68c":"code","30b8dbdc":"code","62d9aa55":"code","158cccba":"code","8030389e":"code","e642288d":"code","d51a28bb":"code","85966457":"code","affa242d":"code","60c062fe":"code","18324be0":"code","ddb2a053":"code","a6f0899b":"code","52030404":"code","086fcec8":"code","e8810999":"code","318f5371":"code","1852e59c":"code","3772280a":"code","f75820cd":"code","af7ae110":"code","bf2cf680":"markdown","b6fd1666":"markdown","b401a5ca":"markdown"},"source":{"74d720ce":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","a9439873":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv') \ntrain.head(4)","f8f59697":"train.shape","a922db49":"train.describe()","7092be82":"train.describe(include=['O'])","c32a34f7":"train.info()","05dca83b":"train.isnull().sum()","8decb21e":"test.isnull().sum()","c68be68c":"survived = train[train['Survived'] == 1]\nnot_survived = train[train['Survived'] == 0]\n\nprint (\"Survived: %i (%.1f%%)\"%(len(survived), float(len(survived))\/len(train)*100.0))\nprint (\"Not Survived: %i (%.1f%%)\"%(len(not_survived), float(len(not_survived))\/len(train)*100.0))\nprint (\"Total: %i\"%len(train))","30b8dbdc":"train.Pclass.value_counts()\ntrain.groupby('Pclass').Survived.value_counts()","62d9aa55":"train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean()","158cccba":"sns.barplot(x='Pclass', y='Survived', data=train)","8030389e":"train.groupby('Sex').Survived.value_counts()\nsns.barplot(x='Sex', y='Survived', data=train)","e642288d":"tab = pd.crosstab(train['Pclass'], train['Sex'])\nprint (tab)\n\ntab.div(tab.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True)\nplt.xlabel('Pclass')\nplt.ylabel('Percentage')","d51a28bb":"sns.factorplot('Sex', 'Survived', hue='Pclass', size=4, aspect=2, data=train)","85966457":"sns.factorplot(x='Pclass', y='Survived', hue='Sex', col='Embarked', data=train)","affa242d":"fig = plt.figure(figsize=(15,5))\nax1 = fig.add_subplot(131)\nax2 = fig.add_subplot(132)\nax3 = fig.add_subplot(133)\n\nsns.violinplot(x=\"Embarked\", y=\"Age\", hue=\"Survived\", data=train, split=True, ax=ax1)\nsns.violinplot(x=\"Pclass\", y=\"Age\", hue=\"Survived\", data=train, split=True, ax=ax2)\nsns.violinplot(x=\"Sex\", y=\"Age\", hue=\"Survived\", data=train, split=True, ax=ax3)","60c062fe":"total_survived = train[train['Survived']==1]\ntotal_not_survived = train[train['Survived']==0]\nmale_survived = train[(train['Survived']==1) & (train['Sex']==\"male\")]\nfemale_survived = train[(train['Survived']==1) & (train['Sex']==\"female\")]\nmale_not_survived = train[(train['Survived']==0) & (train['Sex']==\"male\")]\nfemale_not_survived = train[(train['Survived']==0) & (train['Sex']==\"female\")]\n\nplt.figure(figsize=[15,5])\nplt.subplot(111)\nsns.distplot(total_survived['Age'].dropna().values, bins=range(0, 81, 1), kde=False, color='blue')\nsns.distplot(total_not_survived['Age'].dropna().values, bins=range(0, 81, 1), kde=False, color='red', axlabel='Age')\n\nplt.figure(figsize=[15,5])\n\nplt.subplot(121)\nsns.distplot(female_survived['Age'].dropna().values, bins=range(0, 81, 1), kde=False, color='blue')\nsns.distplot(female_not_survived['Age'].dropna().values, bins=range(0, 81, 1), kde=False, color='red', axlabel='Female Age')\n\nplt.subplot(122)\nsns.distplot(male_survived['Age'].dropna().values, bins=range(0, 81, 1), kde=False, color='blue')\nsns.distplot(male_not_survived['Age'].dropna().values, bins=range(0, 81, 1), kde=False, color='red', axlabel='Male Age')","18324be0":"plt.figure(figsize=(15,6))\nsns.heatmap(train.drop('PassengerId',axis=1).corr(), vmax=0.6, square=True, annot=True)","ddb2a053":"train.head(4)","a6f0899b":"def substrings_in_string(big_string, substrings):\n    for substring in substrings:\n        #print (substring,big_string)\n        if not pd.isnull(big_string) and big_string.find( substring) == 0:\n            return substring\n    #print (big_string)\n    return np.nan\n\ntrain['Family_Size']=train['SibSp']+train['Parch']\ntrain['Fare_Per_Person']=train['Fare']\/(train['Family_Size']+1)\n#Turning cabin number into Deck\ncabin_list = ['A', 'B', 'C', 'D', 'E', 'F', 'T', 'G', 'Unknown']\ntrain['Deck']=train['Cabin'].map(lambda x: substrings_in_string(x, cabin_list))","52030404":"import matplotlib.pyplot as plt\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\nX =  train[['Pclass', 'Sex', 'Age', 'Embarked','Fare','Age','Family_Size','Deck' ]]\nY = train['Survived']\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, shuffle= True)\n\nimport sklearn as sk\nfrom sklearn.ensemble import RandomForestClassifier\n\nnumeric_features = ['Age', 'Fare','Family_Size']\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())])\n\ncategorical_features = ['Embarked', 'Sex', 'Pclass','Deck']\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n         ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)])\n\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('classifier', RandomForestClassifier(n_estimators=1000, criterion='gini', max_depth=8, random_state=0,\n                                                            max_features =5,oob_score=True,n_jobs=-1, verbose=0,\n                                                            min_samples_leaf=1,  min_samples_split=5\n                                                           ))])\n \nclf.fit(X_train, y_train)\nprint('Score: ', clf.score(X_train, y_train)*100)\ny_pred_random_forest_training_set = clf.predict(X_test)\nfrom sklearn.metrics import accuracy_score\nprint (\"Random Forest Accuracy \" , accuracy_score(y_pred_random_forest_training_set,y_test)*100 ,\"%\")\n\n","086fcec8":"test['Family_Size']=test['SibSp']+test['Parch']\ntest['Fare_Per_Person']=test['Fare']\/(test['Family_Size']+1)\n#Turning cabin number into Deck\ncabin_list = ['A', 'B', 'C', 'D', 'E', 'F', 'T', 'G', 'Unknown']\ntest['Deck']=test['Cabin'].map(lambda x: substrings_in_string(x, cabin_list))\n\nX_test =  test[['Pclass', 'Sex', 'Age', 'Embarked','Fare','Age','Family_Size','Deck']]\ny_pred_random_forest = clf.predict(X_test)\n#submission = pd.DataFrame({\n#        \"PassengerId\": test[\"PassengerId\"],\n#        \"Survived\": y_pred_random_forest\n#    })","e8810999":"clf_xg = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('classifier', XGBClassifier(learning_rate=0.003,n_estimators=1900,max_depth=6, min_child_weight=0,\n                                gamma=0, subs1ample=0.7,\n                                colsample_bytree=0.7,\n                                scale_pos_weight=1, seed=20,\n                                reg_alpha=0.00005))]) \n\nclf_xg.fit(X_train, y_train)\nprint('Score: ', clf_xg.score(X_train, y_train)*100)\ny_xboost = clf_xg.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\nprint (\"Accuracy XGB\" , accuracy_score(y_xboost,y_test)*100 ,\"%\")","318f5371":"y_xgboost = clf_xg.predict(X_test)\nsubmission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": y_xgboost\n    })","1852e59c":"from sklearn.metrics import confusion_matrix\nimport itertools\n\ny_pred_random_forest_training_set = clf.predict(X_test)\ncnf_matrix = confusion_matrix(y_test, y_pred_random_forest_training_set)\nnp.set_printoptions(precision=2)\n\nprint ('Confusion Matrix in Numbers')\nprint (cnf_matrix)\nprint ('')\ntrue_class_names = ['True Survived', 'True Not Survived']\npredicted_class_names = ['Predicted Survived', 'Predicted Not Survived']\ndf_cnf_matrix = pd.DataFrame(cnf_matrix, \n                             index = true_class_names,\n                             columns = predicted_class_names)\nsns.heatmap(df_cnf_matrix, annot=True, fmt='d')\n \n\n\n","3772280a":"y_pred_xgb = clf_xg.predict(X_test)\ncnf_matrix_cgb = confusion_matrix(y_test, y_pred_xgb)\nnp.set_printoptions(precision=2)\nprint ('Confusion Matrix in Numbers XGB' )\nprint (cnf_matrix_cgb)\nprint ('')\ntrue_class_names = ['True Survived', 'True Not Survived']\npredicted_class_names = ['Predicted Survived', 'Predicted Not Survived']\ndf_cnf_matrix = pd.DataFrame(cnf_matrix_cgb, \n                             index = true_class_names,\n                             columns = predicted_class_names)\nsns.heatmap(df_cnf_matrix, annot=True, fmt='d') ","f75820cd":"submission.head(2)\n","af7ae110":"submission.to_csv('submission.csv', index=False)","bf2cf680":"Upper class survive more","b6fd1666":"Feature selection","b401a5ca":"Confusion matrix"}}