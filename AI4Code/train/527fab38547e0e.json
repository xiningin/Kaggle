{"cell_type":{"3975220f":"code","162d868d":"code","7f2777c0":"code","ab2a4cdd":"code","85258639":"code","e8ed3bb8":"code","5128aa66":"code","b0ec3f02":"code","6c7842c3":"code","601c9689":"code","3d062072":"code","f59a1ebe":"code","17faa0c8":"markdown","8746c8ba":"markdown"},"source":{"3975220f":"!pip install jax jaxlib","162d868d":"!pip install gym","7f2777c0":"import random\n\nrandom.seed(0)","ab2a4cdd":"# We need 4 inputs and 2 softmax outputs for our policy network.\n\nimport jax.numpy as jnp\nimport jax.tree_util as tree_util\nfrom jax import grad, jit, vmap\nfrom jax import random as jrandom\n\nfrom jax.experimental import stax\nfrom jax.experimental.stax import Dense, Relu, Softmax, Tanh\n\npolicy_init_fun, policy_net = stax.serial(Dense(256), Relu,\n                                          Dense(2), Softmax)\n\nbaseline_init_fun, baseline_net = stax.serial(Dense(256), Relu,\n                                              Dense(128),\n                                              Dense(1))","85258639":"def select_action(policy_params, state):\n    \"\"\"\n    Select an action according to a sample from the policy distribution.\n    \"\"\"\n    \n    # Select an action.\n    policy_dist = policy_net(policy_params, jnp.array(state))\n    action = random.choices([0, 1], weights=policy_dist)[0]\n    \n    return action","e8ed3bb8":"def value_estimate(baseline_params, state):\n    \"\"\"\n    Produce a value estimate from the baseline network.\n    \"\"\"\n    \n    return baseline_net(baseline_params, jnp.array(state))[0]","5128aa66":"policy_step_size = 1e-3\nbaseline_step_size = 3e-2 # This should be greater than policy_step_size.\nconsecutive_solutions_required = 3 # How many times should we have to solve the task in a row to stop training?\n\nfrom jax.experimental.optimizers import adam\n\npolicy_opt_init, policy_opt_update, policy_opt_get_params = adam(policy_step_size)\nbaseline_opt_init, baseline_opt_update, baseline_opt_get_params = adam(baseline_step_size)","b0ec3f02":"import gym\n\n# Initialize the environment.\nenv = gym.make(\"CartPole-v1\")\nobservation = env.reset()\n\n# Initialize parameters.\npolicy_output_shape, policy_params = policy_init_fun(jrandom.PRNGKey(0), (1, 4))\nbaseline_output_shape, baseline_params = baseline_init_fun(jrandom.PRNGKey(0), (1, 4))\n\n# Initialize optimizers.\npolicy_opt_state = policy_opt_init(policy_params)\nbaseline_opt_state = baseline_opt_init(baseline_params)\n\n# This accumulator speeds up the parameter update computation.\ntotal_reward = 0\n# This is a list of tuples containing (state, action, reward).\nepisode_SARs = []\n\n# Used for plotting.\ntotal_rewards = []\n\nconsecutive_solutions = 0\nepisode = 1\nprint(f\"Training until reward threshold {env.spec.reward_threshold} is attained {consecutive_solutions_required} times in a row.\")\nwhile True:\n    action = select_action(policy_params, observation)\n    previous_observation = observation\n    observation, reward, done, info = env.step(action)\n    \n    # Update episode training data.\n    total_reward += reward\n    episode_SARs.append((previous_observation, action, reward))\n    \n    if done:\n        total_rewards.append(total_reward)\n        print(f\"Episode {episode} reward achieved: {int(total_reward)}.\")\n        \n        if total_reward >= env.spec.reward_threshold:\n            consecutive_solutions += 1\n            if consecutive_solutions == consecutive_solutions_required:\n                # Training complete.\n                print(\"Task solved!\")\n                break\n        else:\n            consecutive_solutions = 0\n        \n        # Reset the state to initial conditions.\n        observation = env.reset()\n        \n        # Update the parameters.\n        value_estimates = [value_estimate(baseline_params, state) for state, _, _ in episode_SARs]\n        \n        def policy_loss(policy_params, value_estimates, total_reward):\n            loss = 0\n            i = 0\n            for state, action, reward in episode_SARs:\n                value_error = total_reward - value_estimates[i]\n                # Increase the loss.\n                loss -= value_error * jnp.log(policy_net(policy_params, state)[action])\n                # Update the return for the next step.\n                total_reward -= reward\n                i += 1\n            \n            return loss\n        \n        def baseline_loss(baseline_params, value_estimates, total_reward):\n            loss = 0\n            i = 0\n            for state, action, reward in episode_SARs:\n                value_error = total_reward - value_estimates[i]\n                # Increase the loss.\n                loss -= value_error * value_estimate(baseline_params, state)\n                # Update the return for the next step.\n                total_reward -= reward\n                i += 1\n            \n            return loss\n                \n        policy_grad = grad(policy_loss)(policy_params, value_estimates, total_reward)\n        baseline_grad = grad(baseline_loss)(baseline_params, value_estimates, total_reward)\n        \n        policy_opt_state = policy_opt_update(episode, policy_grad, policy_opt_state)\n        baseline_opt_state = baseline_opt_update(episode, baseline_grad, baseline_opt_state)\n        \n        # Grab the updated parameters.\n        policy_params = policy_opt_get_params(policy_opt_state)\n        baseline_params = baseline_opt_get_params(baseline_opt_state)\n        \n        episode_SARs = []\n        episode += 1\n        total_reward = 0","6c7842c3":"from matplotlib import pyplot as plt\n\nplt.plot(total_rewards)\nplt.xlabel(\"Episode\")\nplt.ylabel(\"Total reward\")\nplt.show()","601c9689":"!apt-get install python-opengl -y\n!pip install pyvirtualdisplay","3d062072":"from pyvirtualdisplay import Display\nimport os\n\ndisplay = Display(visible=0, size=(1400, 900))\ndisplay.start()\nos.environ[\"DISPLAY\"] = \":\" + str(display.display) + \".\" + str(display._obj._screen)","f59a1ebe":"from matplotlib import animation, rc\n\nfig = plt.figure()\n\nframe = []\n\ntotal_reward = 0\nobservation = env.reset()\n\nwhile True:\n    action = select_action(policy_params, observation)\n    observation, reward, done, info = env.step(action)\n    \n    total_reward += reward\n    \n    img = plt.imshow(env.render(\"rgb_array\"))\n    frame.append([img])\n    if done:\n        break\n\nanim = animation.ArtistAnimation(fig, frame, interval=100, repeat_delay=1000, blit=True)\nrc(\"animation\", html=\"jshtml\")\n\nprint(f\"Final reward: {total_reward}.\")\n\nanim","17faa0c8":"This is my first attempt at an OpenAI Gym environment. I am using vanilla REINFORCE to learn a policy for the CartPole environment.","8746c8ba":"Update rule for REINFORCE:\n\n$\\theta \\leftarrow \\theta + \\alpha G_t \\nabla \\ln \\pi(A_t \\mid S_t, \\theta)$"}}