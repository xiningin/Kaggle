{"cell_type":{"4ba6780d":"code","28a0ed5d":"code","d26a3244":"code","aad485bb":"code","180e0fe7":"code","68a0f155":"code","ce815e9d":"code","f7dd1b5e":"code","a55e88a3":"code","d04cf720":"code","a6694f4c":"code","12111d33":"code","a13121fd":"code","6ea8d476":"code","a2594b77":"code","b8886bd9":"code","5d76d7ea":"code","21d8113b":"code","cdd028e1":"code","4bb6d524":"code","585cc60e":"code","9a490e8e":"code","2461734a":"markdown","6c4b0c00":"markdown","31d58661":"markdown","a7a48970":"markdown","d83a449b":"markdown","38cd0f3c":"markdown","98809a06":"markdown","533de6cf":"markdown","dca60960":"markdown","81dd5f92":"markdown","480fc624":"markdown","61643090":"markdown","2a3ccfc9":"markdown","33ff47f4":"markdown","fe1c220a":"markdown","65918583":"markdown","1d4c4b3e":"markdown","1c886967":"markdown","c35b6118":"markdown","b203aca1":"markdown","baaefda2":"markdown","885a6ea8":"markdown"},"source":{"4ba6780d":"# Loading a few important modules\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n%matplotlib inline\nsns.set() #sets a style for the seaborn plots.\nnp.random.seed(64)","28a0ed5d":"columns_names = ['Target', 'Alcohol', 'Malic_acid', 'Ash', 'Alcalinity_of_ash', 'Magnesium', 'Total_phenols', 'Flavanoids',\n                 'Nonflavanoid_phenols', 'Proanthocyanins', 'Color_intensity', 'Hue', 'OD280\/OD315_of_diluted_wines', 'Proline']","d26a3244":"# Loading the data from it's csv,\n# and converting the 'Target' column to be a string so pandas won't infer it as a numeric value\ndata = pd.read_csv(os.path.join('..', 'input', 'Wine.csv'), header=None)\ndata.columns = columns_names\ndata['Target'] = data['Target'].astype(str)\ndata.head() # print the data's top five instances","aad485bb":"data.info() # prints out a basic information about the data.","180e0fe7":"sns.countplot(data['Target']);","68a0f155":"# This method prints us some summary statistics for each column in our data.\ndata.describe()","ce815e9d":"# box plots are best for plotting summary statistics.\nsns.boxplot(data=data);","f7dd1b5e":"data_to_plot = data.iloc[:, 1:]\nfig, ax = plt.subplots(ncols=len(data_to_plot.columns))\nplt.subplots_adjust(right=3, wspace=1)\nfor i, col in enumerate(data_to_plot.columns):\n    sns.boxplot(y=data_to_plot[col], ax = ax[i]);","a55e88a3":"columns_to_plot = list(data.columns)\ncolumns_to_plot.remove('Target')\nsns.pairplot(data, hue='Target', vars=columns_to_plot);\n# the hue parameter colors data instances baces on their value in the 'Target' column.","d04cf720":"sns.lmplot(x='Proline', y='Flavanoids', hue='Target', data=data, fit_reg=False);","a6694f4c":"sns.lmplot(x='Hue', y='Flavanoids', hue='Target', data=data, fit_reg=False);","12111d33":"# This is a good feature comination to separate the red ones (label 3)\nsns.lmplot(x='Color_intensity', y='Flavanoids', hue='Target', data=data, fit_reg=False);","a13121fd":"sns.boxplot(x=data['Target'], y=data['OD280\/OD315_of_diluted_wines']);\n# this is a vey good feature to separate label 1 and 3","6ea8d476":"plt.figure(figsize=(18,15))\nsns.heatmap(data.corr(), annot=True, fmt=\".1f\");","a2594b77":"sns.lmplot(x='Total_phenols', y ='Flavanoids', data=data, fit_reg=True);","b8886bd9":"from sklearn.model_selection import train_test_split\nnp.random.seed(64) # initialize a random seed, this will help us make the random stuff reproducible.\n\nX = data.drop('Target', axis=1)\ny = data['Target']\nx_train, x_test, y_train, y_test = train_test_split(X, y, stratify=y)\n","5d76d7ea":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\nnp.random.seed(64) # initialize a random seed, this will help us make the random stuff reproducible.\n\nmodel = KNeighborsClassifier()\nmodel.fit(x_train, y_train)\npred = model.predict(x_test)\nprint('score on training set:', model.score(x_train, y_train))\nprint('score on test set:', model.score(x_test, y_test))\nprint(metrics.classification_report(y_true=y_test, y_pred=pred))","21d8113b":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nnp.random.seed(64) # initialize a random seed, this will help us make the random stuff reproducible.\n\nmodel = Pipeline(\n    [\n        ('scaler', StandardScaler()), # mean normalization\n        ('knn', KNeighborsClassifier(n_neighbors=1))\n    ]\n)\nmodel.fit(x_train, y_train)\npred = model.predict(x_test)\nprint('score on training set:', model.score(x_train, y_train))\nprint('score on test set:', model.score(x_test, y_test))\nprint(metrics.classification_report(y_true=y_test, y_pred=pred))","cdd028e1":"from sklearn.model_selection import learning_curve\nnp.random.seed(64) # initialize a random seed, this will help us make the random stuff reproducible.\n\ndef over_underfit_plot(model, X, y):\n    plt.figure()\n\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(model, X, y, cv=5, n_jobs=-1)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    plt.grid()\n\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\");\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n    plt.legend(loc=\"best\")\n    plt.yticks(sorted(set(np.append(train_scores_mean, test_scores_mean))))\n    \nover_underfit_plot(model, x_train, y_train)","4bb6d524":"np.random.seed(64) # initialize a random seed, this will help us make the random stuff reproducible.\n\nX.drop('Total_phenols', axis=1, inplace =True) # delete one of the correlating features\nx_train, x_test, y_train, y_test = train_test_split(X, y, stratify=y) # split the data again\n\n#fit the same model again and print the scores\nmodel.fit(x_train, y_train)\npred = model.predict(x_test)\nprint('score on training set:', model.score(x_train, y_train))\nprint('score on test set:', model.score(x_test, y_test))\nprint(metrics.classification_report(y_true=y_test, y_pred=pred))","585cc60e":"from sklearn.ensemble import RandomForestClassifier\nnp.random.seed(64) # initialize a random seed, this will help us make the random stuff reproducible.\n\nmodel_feature_importance = RandomForestClassifier(n_estimators=1000).fit(x_train, y_train).feature_importances_\nfeature_scores = pd.DataFrame({'score':model_feature_importance}, index=list(x_train.columns)).sort_values('score')\nsns.barplot(feature_scores['score'], feature_scores.index)","9a490e8e":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import SelectFromModel, RFECV\nfrom sklearn.ensemble import RandomForestClassifier\nnp.random.seed(64) # initialize a random seed, this will help us make the random stuff reproducible.\n\nmodel = Pipeline(\n    [\n        ('select', SelectFromModel(RandomForestClassifier(n_estimators=1000), threshold=0.06)),\n        ('scaler', StandardScaler()),\n        ('knn', KNeighborsClassifier(n_neighbors=1))\n    ]\n)\n\nmodel.fit(x_train, y_train)\npred = model.predict(x_test)\nprint('score on training set:', model.score(x_train, y_train))\nprint('score on test set:', model.score(x_test, y_test))\nprint(metrics.classification_report(y_true=y_test, y_pred=pred))","2461734a":"This is probably only informative to people who have some experience in statistics.\nLet's try to plot this information and see if it helps us understand.","6c4b0c00":"# Better Predicting Wine Cultivar with Feature Selection\n\nIn supervised machine learning (ML) the goal is to have an accurate model.\nThis which based on previously tagged data provides predictions for new data.\n\nThe number one question when it comes to modeling is:\n**\"How can I improve my results?\"**\n\nThere are several basic ways to improve your prediction model:\n1. Hyperparameters optimization\n2. Feature extraction\n3. Selecting another model\n4. Adding more data\n5. Feature selection\n\nIn this blog post, I'll walk you through how I used **Feature Selection** to improve my model.\nFor the demonstration I'll use the ['Wine' dataset from UCI ML repository](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Wine), which was also available [here at kaggle](https:\/\/www.kaggle.com\/brynja\/wineuci)\n\nMost of the functions are from the [sklearn (scikit-learn) module](http:\/\/scikit-learn.org\/).\n\nFor the plotting functions make sure to read about [matplotlib](https:\/\/matplotlib.org\/) and [seaborn](https:\/\/seaborn.pydata.org).\nBoth are great plotting modules with great documentation.\n\nBefore we jump into the ML model and prediction we need to understand our data.\nThe process of understanding the data is called EDA - exploratory data analysis.\n\n### EDA - Exploratory Data Analysis.\nUCI kindly gave us some basic information about the data set.\nI'll quote some of the more important info given:\n\"These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines ... All attributes are continuous ... 1st attribute is class identifier (1-3)\"\n\nBased on this, it seems like a classification problem with 3 class labels and 13 numeric attributes.\nA classification problem with the goal of predicting the specific cultivar the wine was derived from.","31d58661":"As we can see we have 178 entries (instances).\nas we know from UCI's description of the data, we have 13 numeric attributes and one 'object' type attribute (which is the target column).\nall the columns of all the rows have data, therefore we see \"178 non-null\" next to every column description.","a7a48970":"Great improvment!\n\nWe can use a \"Pipeline\" in this part. \"Pipeline\" is a function in sklearn that combines several other functions and enables us to use other sklearn functions with only one fit command. More on this you can read [here](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html) and [here](http:\/\/scikit-learn.org\/stable\/modules\/pipeline.html) and in a future blog post.\n\n\nSo as I mentioned before, we need to understand if our mode is over or under fitting.","d83a449b":"## Feature selection\nOne of the ways to avoid overfitting is by selecting a subset of features from the data.\nThere are a lot of ways to do feature selection. The most basic one in my opinion is removing correlating features.\n\nAs we checked before 'Total_phenols' and 'Flavanoids' are closely correlating features. Let's see what happens if we drop one of them!","38cd0f3c":"Let's load the 'train_test_split' function, and separate our data into only the feature vectors and the target vector.\nWe will split our data into 25% test and 75% train.\n\nThe 'stratify' parameter will ensure equal distribution of subgroups.\nIt will keep the ration between the classes in the train and test data as they were in the actual full data.","98809a06":"As we can see from the plot there are features that are more important than others and we can see 5-7 features which stand out.\nI'll use the feature important scores to put a threshold for my model feature selection function.\n\n[\"SelectFromModel\"](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.SelectFromModel.html) is a sklearn function which takes an estimator and a threshold, extracts from the estimator the feature importance scores and returns only the features with a score above the given threshold.\n","533de6cf":"It's important to check the amount of instances in each class. There is difference between the class labels but It isn't a huge difference.\nIf the difference was bigger we would be in an imbalanced problem.\nThat would require a lot of other things to do, but this is for another post.","dca60960":"I named the first columns as 'Target'.\nThis is the target attribute - what we are trying to predict.\nThis is a classification problem, so the class label ('Target') is not a numeric but a nominal value. that's why I'm telling Pandas this columns dtype is 'str'.","81dd5f92":"An improvment of 8-9% in this high scores is super great and difficult.\nThis is a very good score just by itself! Now we can improve our score in other ways.\n\n\nMy point was to show how I improved my score with this very simple KNN model.\nI also wanted to show you how a few simple steps can improve your model and get a high score.","480fc624":"As we can see from the plot and from the accuracy scores, we are in an overfitting situation - We have a good score on the train, but a low score on the test and by adding data we improve the model's results.\nThis means the model is bad for unseen data and very good with the data it was trained on.\n\nIn an overfitting case there are a few things which need and can be done to improve our model:\n1. Add more data - not possible in this case.\n2. Remove unimportant features in order to make the model less complex - aka. feature selection.\n3. Add regulization.","61643090":"## Modelling and Predicting","2a3ccfc9":"I'll start with a very simple classifier called [knn (k-nearest neighbors)](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html).\n\nKnn classifies an object (a new data point) by the majority vote of its k nearest neighbors in the feature space.","33ff47f4":"The diagonal line from the top left side to the right buttom side of the pair plot are histograms of the columns.\n\nGood feature combination for me is a feature combination that separates some of the class labels.\n'Flavanoids' in row 7 looks like a good feature combined with the other ones. Same goes for 'Proline' in the last row.\n\nOn the other hand 'Malic_acid' (2nd row) does not look like a good feature at all.\n\nWe can have a further look at some features:","fe1c220a":"Pair plot is a great way to see a scatter plot of all the data, of course only for two features at a time.\nPair plot is good for small amout of features and for first glance at the columns (features), afterwords in my opinion a simple scatterplot with the relevant columns is better.","65918583":"Unfortunately this is not a very informative plot becasue the data is not in the same value range.\nWe can resolve the problem by plotting each column side by side.","1d4c4b3e":"Another thing that is good to check is the feature correlation. We don't like features that correlate with each other so much.\n\nWe will use the [Pearson correlation](https:\/\/en.wikipedia.org\/wiki\/Pearson_correlation_coefficient) to compute pairwise correlation of columns in our data.\nIt's worth to emphasize that the Pearson correlation is only good for linear correlation, but as we saw from the pair plot, our data dosen't seem to correlate in any other way.","1c886967":"As I mentioned before there are several ways in which you can improve your ML model.\n\nToday I'll focus on **feature selection**, a very basic way to improve your model's score.","c35b6118":"As we can see this is a pretty bad result.\n\nThe key question which will help us decide in which way we should improve our model is \"whether our model is [overfitting or underfitting](https:\/\/en.wikipedia.org\/wiki\/Overfitting)?\"\n\n\n\n\nBefore I'll answer this question, there is something we didn't do and it's essential in this case:\nWhen we are using KNN or any other algorithm which is based on distances (like the [Euclidean distance](https:\/\/en.wikipedia.org\/wiki\/Euclidean_distance) in our case), normalization of the data is necessery.\nI'll be using the mean normaliztion method, which is subtracting the feature's mean and dividing by it's standard deviation. This is basically converting each data point into it's [Z-score](https:\/\/en.wikipedia.org\/wiki\/Standard_score).","b203aca1":"There is a correlation between Total_phenols - Flavanoids => 0.9 which is strong.\nTypically I would delete one of the correlting features, but for now I won't do it.\nLet's plot these feautes to see the correlation.","baaefda2":"Truly as we expected this step improved our model's score, but we are still overfitting.\n\nAnother feature selection method (and my favourite) is by using another algorithm's feature importance.\nMany algorithms rank the importance of the features in the data, based on which feature helped the most to distinguish between the target labels.\nFrom this ranking we can learn which features were more and less important and select just the one's which contribute the most.\n\nLet's fit the train data in a [Random Forest classifier](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html) and print the feature importance scores.\n\nRandom Forest is an ensamble that fits a number of decision tree classifiers. Ensamble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of its learning algorithms alone, in our case from a simple Decision tree.","885a6ea8":"This is a better way to plot the data.\n\nWe can see that we have some outliers (based on the [IQR calculation](https:\/\/en.wikipedia.org\/wiki\/Interquartile_range)) in almost all the feaures.\nThese outliers deserve a second look, but we won't deal with them right now."}}