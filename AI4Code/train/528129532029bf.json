{"cell_type":{"c26ac49e":"code","8019bd31":"code","868431f3":"code","dfb217b3":"code","c97169d1":"code","3e8c3180":"code","aba92576":"code","5aaf7341":"code","9188883a":"code","d579a252":"code","4fdcdc50":"code","da19af57":"code","cf5be769":"code","cafdbc2f":"code","88d7796e":"code","b7f96ffd":"code","8d2d2dda":"code","26868406":"code","7a956044":"code","1ce95b14":"code","57cb12c3":"code","5b66e4ae":"code","62270a96":"code","1a1e3810":"code","d817d14c":"code","2a2742cb":"code","5dcbe741":"code","1fbda14e":"code","da45f5e2":"code","a11ebde2":"code","1d97e194":"code","5bb78546":"code","e3415c1f":"code","d44dfea2":"code","791b4c43":"code","236b6415":"code","455d8ea6":"code","e339e8db":"code","f210a777":"code","5cf3b81c":"code","455e900b":"code","ad99009b":"code","7be51d54":"code","13dec3ca":"code","b389a173":"code","9de55a3f":"code","6baccc98":"code","ebb548c7":"code","b93933ef":"code","f67629e5":"code","1e42ab01":"code","3ab5c33f":"code","0ef4873f":"code","8d8186e2":"code","e628f4b3":"code","1ddec34e":"markdown","a1b359c0":"markdown","a4f5f5c2":"markdown","a092e4dd":"markdown","cbc86ec1":"markdown","a4c60612":"markdown","f804e6aa":"markdown","75b810e1":"markdown","021fe466":"markdown","c40409c7":"markdown","248db11f":"markdown","386a8153":"markdown","431bb553":"markdown","1d04c971":"markdown","d2e03ebe":"markdown","3062794f":"markdown"},"source":{"c26ac49e":"import sqlite3\nimport pandas as pd\n\nconn = sqlite3.connect(r'..\/input\/imdb-movie-reviews-2021\/IMDB_Movies_2021.db')\nquery = 'SELECT AUTHOR,TITLE,REVIEW,RATING FROM REVIEWS'\n\ndf = pd.read_sql_query(query,conn)\n\n# Removing unnecessary newline characters\ndf = df.replace('\\n','', regex=True)\n\n# Visualizing 10 samples\ndf.head(10)","8019bd31":"df.info()","868431f3":"# check missing value\ndf.isnull().sum()\n","dfb217b3":"# understand the numeric feature\ndf['RATING'].describe()","c97169d1":"# drop missing values from RATING\ndf.dropna(subset=['RATING'], inplace = True)\ndf.reset_index(inplace=True, drop=True)\n\ndf.info()","3e8c3180":"import wordcloud\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors\n\n# Creating histogram\nfig, axs = plt.subplots(1, 1,\n                        figsize =(8, 4), \n                        tight_layout = True)\naxs.xaxis.set_ticks(range(0,11))  \n\nN, bins, patches = axs.hist(df['RATING'])\n  \ncolors_ = ['red', 'orange', 'yellow', 'green', 'blue', 'purple', 'violet', 'pink', 'brown', 'black']\n\nfor colnum, thispatch in enumerate(patches):\n    thispatch.set_facecolor(colors_[colnum])\n  \n# Adding extra features    \nplt.xlabel(\"Rating\")\nplt.ylabel(\"Number of Reviews\")\n  \n# Show plot\nplt.show()","aba92576":"# use the first 4000 rows as training data\ndata = df.loc[:3999, 'REVIEW'].tolist()","5aaf7341":"len(data)","9188883a":"# import libraries\n\nimport nltk\n# import gensim\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport matplotlib.pyplot as plt\n\nnltk.download('punkt')\nnltk.download('stopwords')","d579a252":"# Use nltk's English stopwords.\nstopwords = nltk.corpus.stopwords.words('english')\nstopwords.append(\"'s\")\nstopwords.append(\"'m\")\nstopwords.append(\"n't\")\nstopwords.append(\"br\")\n\n# this is a movie review dataset, so the word \"movie\" or \"film \"really doesn't give much sight\nstopwords.append(\"movie\")\nstopwords.append(\"film\")\n\nprint (\"We use \" + str(len(stopwords)) + \" stop-words from nltk library.\")\nprint (stopwords[:11])","4fdcdc50":"from nltk.stem.snowball import SnowballStemmer\n\nstemmer = SnowballStemmer(\"english\")\n\n# tokenization and stemming\ndef tokenization_and_stemming(text):\n    tokens = []\n    # exclude stop words and tokenize the document, generate a list of string \n    for word in nltk.word_tokenize(text):\n        if word.lower() not in stopwords:\n            tokens.append(word.lower())\n\n    filtered_tokens = []\n    \n    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n    for token in tokens:\n        if token.isalpha():\n            filtered_tokens.append(token)\n    \n    #exclude stopwords from stemmed words\n    stems = [stemmer.stem(t) for t in filtered_tokens if word not in stopwords]\n    return stems","da19af57":"# test our function on data[0]\n\ntokenization_and_stemming(data[0])\n","cf5be769":"data[0]","cafdbc2f":"from sklearn.feature_extraction.text import TfidfVectorizer\n# define vectorizer parameters\n# TfidfVectorizer will help us to create tf-idf matrix\n# max_df : maximum document frequency for the given word\n# min_df : minimum document frequency for the given word\n# max_features: maximum number of words\n# use_idf: if not true, we only calculate tf\n# stop_words : built-in stop words\n# tokenizer: how to tokenize the document\n# ngram_range: (min_value, max_value), eg. (1, 3) means the result will include 1-gram, 2-gram, 3-gram\ntfidf_model = TfidfVectorizer(max_df=0.99, max_features=1000,\n                                 min_df=0.01, use_idf=True, stop_words = stopwords,\n                              tokenizer=tokenization_and_stemming, ngram_range=(1,1))\n\ntfidf_matrix = tfidf_model.fit_transform(data) #fit the vectorizer to synopses\n\nprint (\"In total, there are \" + str(tfidf_matrix.shape[0]) + \\\n      \" reviews and \" + str(tfidf_matrix.shape[1]) + \" terms.\")","88d7796e":"# check the parameters\ntfidf_model.get_params()","b7f96ffd":"tfidf_matrix.todense()","8d2d2dda":"# words\ntf_selected_words = tfidf_model.get_feature_names()","26868406":"# print out words\ntf_selected_words","7a956044":"# k-means clustering\nfrom sklearn.cluster import KMeans\n\nnum_clusters = 5\n\n# number of clusters\nkm = KMeans(n_clusters=num_clusters)\nkm.fit(tfidf_matrix)\n\nclusters = km.labels_.tolist()","1ce95b14":"# create DataFrame films from all of the input files.\nproduct = { 'review': df[:4000].REVIEW, 'cluster': clusters}\nframe = pd.DataFrame(product, columns = ['review', 'cluster'])","57cb12c3":"frame.head(10)","5b66e4ae":"print (\"Number of reviews included in each cluster:\")\nframe['cluster'].value_counts().to_frame()","62270a96":"km.cluster_centers_","1a1e3810":"km.cluster_centers_.shape","d817d14c":"print (\"<Document clustering result by K-means>\")\n\n#km.cluster_centers_ denotes the importances of each items in centroid.\n#We need to sort it in decreasing-order and get the top k items.\norder_centroids = km.cluster_centers_.argsort()[:, ::-1] \n\nCluster_keywords_summary = {}\nfor i in range(num_clusters):\n    print (\"Cluster \" + str(i) + \" words:\", end='')\n    Cluster_keywords_summary[i] = []\n    for ind in order_centroids[i, :5]: #5 words per cluster\n        Cluster_keywords_summary[i].append(tf_selected_words[ind])\n        print (tf_selected_words[ind] + \",\", end='')\n    print ()\n    \n    cluster_reviews = frame[frame.cluster==i].review.tolist()\n    print (\"Cluster \" + str(i) + \" reviews (\" + str(len(cluster_reviews)) + \" reviews): \")\n   ","2a2742cb":"num_clusters_7 = 7\n\n# number of clusters\nkm2 = KMeans(n_clusters=num_clusters_7)\nkm2.fit(tfidf_matrix)\n\nclusters_7 = km2.labels_.tolist()","5dcbe741":"# create another DataFrame films from all of the input files for 7 clusters\nproduct2 = { 'review': df[:4000].REVIEW, 'cluster': clusters_7}\nframe2 = pd.DataFrame(product2, columns = ['review', 'cluster'])","1fbda14e":"frame2.head(10)","da45f5e2":"print (\"Number of reviews included in each cluster:\")\nframe2['cluster'].value_counts().to_frame()","a11ebde2":"km2.cluster_centers_","1d97e194":"km2.cluster_centers_.shape","5bb78546":"print (\"<Document clustering result by K-means>\")\n\n#km.cluster_centers_ denotes the importances of each items in centroid.\n#We need to sort it in decreasing-order and get the top k items.\norder_centroids_2 = km2.cluster_centers_.argsort()[:, ::-1] \n\nCluster_keywords_summary_2 = {}\nfor i in range(num_clusters_7):\n    print (\"Cluster \" + str(i) + \" words:\", end='')\n    Cluster_keywords_summary_2[i] = []\n    for ind in order_centroids_2[i, :5]: #5 words per cluster\n        Cluster_keywords_summary_2[i].append(tf_selected_words[ind])\n        print (tf_selected_words[ind] + \",\", end='')\n    print ()\n    \n    cluster_reviews_2 = frame2[frame2.cluster==i].review.tolist()\n    print (\"Cluster \" + str(i) + \" reviews (\" + str(len(cluster_reviews_2)) + \" reviews): \")\n   ","e3415c1f":"# Use LDA for clustering\nfrom sklearn.decomposition import LatentDirichletAllocation\nlda = LatentDirichletAllocation(n_components=5)","d44dfea2":"# document topic matrix for tfidf_matrix_lda\nlda_output = lda.fit_transform(tfidf_matrix)\nprint(lda_output.shape)\nprint(lda_output)","791b4c43":"# topics and words matrix\ntopic_word = lda.components_\nprint(topic_word.shape)\nprint(topic_word)","236b6415":"import numpy as np","455d8ea6":"# column names\ntopic_names = [\"Topic\" + str(i) for i in range(lda.n_components)]\n\n# index names\ndoc_names = [\"Doc\" + str(i) for i in range(len(data))]\n\ndf_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topic_names, index=doc_names)\n\n# get dominant topic for each document\ntopic = np.argmax(df_document_topic.values, axis=1)\ndf_document_topic['topic'] = topic\n\ndf_document_topic.head(10)","e339e8db":"df_document_topic['topic'].value_counts().to_frame()","f210a777":"# topic word matrix\nprint(lda.components_)\n# topic-word matrix\ndf_topic_words = pd.DataFrame(lda.components_)\n\n# column and index\ndf_topic_words.columns = tfidf_model.get_feature_names()\ndf_topic_words.index = topic_names\n\ndf_topic_words.head()","5cf3b81c":"# print top n keywords for each topic\ndef print_topic_words(tfidf_model, lda_model, n_words):\n    words = np.array(tfidf_model.get_feature_names())\n    topic_words = []\n    # for each topic, we have words weight\n    for topic_words_weights in lda_model.components_:\n        top_words = topic_words_weights.argsort()[::-1][:n_words]\n        topic_words.append(words.take(top_words))\n    return topic_words\n\ntopic_keywords = print_topic_words(tfidf_model=tfidf_model, lda_model=lda, n_words=15)        \n\ndf_topic_words = pd.DataFrame(topic_keywords)\ndf_topic_words.columns = ['Word '+str(i) for i in range(df_topic_words.shape[1])]\ndf_topic_words.index = ['Topic '+str(i) for i in range(df_topic_words.shape[0])]\ndf_topic_words","455e900b":"!pip install twython","ad99009b":"from nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.sentiment.util import *\nfrom textblob import TextBlob","7be51d54":"# double check if the original df has duplicates\ndf.drop_duplicates(subset =\"REVIEW\", inplace = True)","13dec3ca":"df.info()","b389a173":"df['REVIEW'] = df['REVIEW'].astype('str')\ndef get_polarity(text):\n    return TextBlob(text).sentiment.polarity\ndf['Polarity'] = df['REVIEW'].apply(get_polarity)","9de55a3f":"df.head(10)","6baccc98":"df['Sentiment_Type']=''\ndf.loc[df.Polarity>0.1,'Sentiment_Type']='POSITIVE'\ndf.loc[(df.Polarity < 0.1) & (df.Polarity >= 0),'Sentiment_Type']='NEUTRAL'\ndf.loc[df.Polarity<0,'Sentiment_Type']='NEGATIVE'","ebb548c7":"df.head(10)","b93933ef":"df.Sentiment_Type.value_counts().plot(kind='bar',title=\"Sentiment Analysis\")","f67629e5":"nltk.download('vader_lexicon')\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nsid = SentimentIntensityAnalyzer()","1e42ab01":"df['VADER_scores'] = df['REVIEW'].apply(lambda REVIEW: sid.polarity_scores(REVIEW))\ndf.head(10)","3ab5c33f":"df['compound'] = df['VADER_scores'].apply(lambda score_dict: score_dict['compound'])\ndf.head(5)","0ef4873f":"df['sentiment_VADER']=''\ndf.loc[df.compound>0,'sentiment_VADER']='POSITIVE'\ndf.loc[df.compound==0,'sentiment_VADER']='NEUTRAL'\ndf.loc[df.compound<0,'sentiment_VADER']='NEGATIVE'","8d8186e2":"df.head(5)","e628f4b3":"df.sentiment_VADER.value_counts().plot(kind='bar',title=\"sentiment analysis_VADER\")","1ddec34e":"From the Polarity scores above in the preview we can see that the difference between a rating of 4.0 and a rating of 8.0 is marginally small, we should change our Polarity threshold accordingly to reflect that. However, notice how row 9, the review has a rating of 7.0 but the Polarity is higher than that of a review with 9.0. This should be noted to uncover how is Polarity score calculated mathematically.","a1b359c0":"# Part 4-1: Sentiment Analysis with Textblob","a4f5f5c2":"Credit: same with the Textblob section above (description of the method are borrowed from the same blog)\n\nVADER (Valence Aware Dictionary and sEntiment Reasoner) is a rule\/lexicon-based, open-source sentiment analyzer pre-built library, protected under the MIT license.","a092e4dd":"As we can see, VADER is not perfect by any means. It labeled row 4 review as positive when the raw rating is only 4.0, and row 0 review a negative when the raw review was 5.0. \n\nIt was able to correctly label row 3 review as negative. I'm curious to learn if we should also set a customized threshold for the compound score. Because the calculation formulas for Textblob polarity is different from VADER compound, it's hard to evaluate thresholds.\n\n","cbc86ec1":"Use defined functions to analyze (i.e. tokenize, stem) our reviews.","a4c60612":"With VADER, using the sid.polarity_scores(Description)), to generate sentiment polarity.","f804e6aa":"# Part 1: Tokenizing and Stemming","75b810e1":"# Part 4-2: Sentiment Analysis with VADER","021fe466":"Load stopwords and stemmer function from NLTK library. Stop words are words like \"a\", \"the\", or \"in\" which don't convey significant meaning. Stemming is the process of breaking a word down into its root","c40409c7":"Save the terms identified by TF-IDF.","248db11f":"**I feel like 5 clusters didn't provide a whole lot of insight in terms of sentiment, so let's try increase k to 7.**","386a8153":"**3.1. Analyze K-means Result**","431bb553":"Credit: awesome demo from https:\/\/www.analyticsvidhya.com\/blog\/2021\/01\/sentiment-analysis-vader-or-textblob\/","1d04c971":"# Part 4: Topic Modeling - Latent Dirichlet Allocation","d2e03ebe":"# Part 3: K-Means Clustering","3062794f":"# Part 2: TF-IDF"}}