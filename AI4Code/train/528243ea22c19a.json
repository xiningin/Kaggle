{"cell_type":{"17642488":"code","c1fc017c":"code","65c37f1d":"code","c5b91d21":"code","4108e2c0":"code","e6b5ca24":"code","8d80351a":"code","6d9dca01":"code","eb18e9f5":"code","ab6e4f83":"code","53703d26":"code","a682209a":"code","a61fd243":"code","1c67d927":"code","8cec784c":"code","ec2cd93d":"code","4130478e":"code","fdd86408":"code","cfc29e74":"code","26ceb3f6":"code","cd2e2ec2":"code","b9964d08":"code","f5704cd6":"code","a4087cfe":"markdown","2a474082":"markdown","77fecbeb":"markdown","ec839fb6":"markdown","48eb399c":"markdown"},"source":{"17642488":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c1fc017c":"import lightgbm as lgb\n#import optuna.integration.lightgbm as lgb\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error # \u5e73\u5747\u4e8c\u4e57\u8aa4\u5dee\n#from sklearn.metrics import mean_squared_log_error # \u5bfe\u6570\u5e73\u5747\u4e8c\u4e57\u8aa4\u5dee\nfrom sklearn.metrics import r2_score # \u6c7a\u5b9a\u4fc2\u6570\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\nimport missingno as msno\nimport plotly.express as px\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","65c37f1d":"sample_submission = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-aug-2021\/sample_submission.csv\")\ntrain = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-aug-2021\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-aug-2021\/test.csv\")","c5b91d21":"# Pandas setting to display more dataset rows and columns\npd.set_option('display.max_rows', 150)\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_colwidth', None)\npd.set_option('display.float_format', lambda x: '%.5f' % x)","4108e2c0":"sample_submission.head()","e6b5ca24":"sample_submission.info()","8d80351a":"train.head()","6d9dca01":"train.describe()","eb18e9f5":"test.head()","ab6e4f83":"test.describe()","53703d26":"# Colors to be used for plots\ncolors = [\"lightcoral\", \"sandybrown\", \"darkorange\", \"mediumseagreen\", \"lightseagreen\",\n          \"cornflowerblue\", \"mediumpurple\", \"palevioletred\", \"lightskyblue\", \"sandybrown\",\n          \"yellowgreen\", \"indianred\", \"lightsteelblue\", \"mediumorchid\", \"deepskyblue\"]","a682209a":"\n# Search for missing data\n\nmsno.matrix(df=train, figsize=(10,6), color=(0,.3,.3))\n","a61fd243":"\nmsno.matrix(df=test, figsize=(10,6), color=(0,.3,.3))\n","1c67d927":"\n# Concat train and test\nall = pd.concat([train,test],ignore_index=True)\n\nfor i in range(100):\n    colName = \"f\" + str(i)\n    f = all[colName]\n    f = np.array(f)\n    all[colName] = preprocessing.minmax_scale(f[:])\n\nall\n","8cec784c":"\nall.drop(columns=['id', 'loss']).describe().T#\\\n#        .style.bar(subset=['mean'], color=px.colors.qualitative.G10[0])\\\n#        .background_gradient(subset=['std'], cmap='Greens')\\\n#        .background_gradient(subset=['50%'], cmap='BuGn')\n","ec2cd93d":"\n%%time\ncorr = train.drop(columns=[\"id\"]).corr()\nplt.figure(figsize=(20,10))\nsns.heatmap(corr, vmin=-0.05, vmax=0.05, center=0, square=False, annot=False, cmap='coolwarm')\nplt.show()\n","4130478e":"train[\"loss\"].value_counts()","fdd86408":"\nfig, ax = plt.subplots(figsize=(16, 8))\n\nbars = ax.bar(train[\"loss\"].value_counts().sort_index().index,\n              train[\"loss\"].value_counts().sort_index().values,\n              color=colors,\n              edgecolor=\"black\")\nax.set_title(\"Loss distribution\", fontsize=20, pad=15)\nax.set_ylabel(\"Count\", fontsize=14, labelpad=15)\nax.set_xlabel(\"Loss value\", fontsize=14, labelpad=10)\nax.bar_label(bars, [f\"{x:2.2f}%\" for x in train[\"loss\"].value_counts().sort_index().values\/(len(train)\/100)],\n                 padding=5, fontsize=10, rotation=90)\nax.margins(0.025, 0.12)\nax.grid(axis=\"y\")\n\nplt.show();\n","cfc29e74":"\n%%time\ndf = pd.concat([train.drop([\"id\", \"loss\"], axis=1), test.drop(\"id\", axis=1)], axis=0)\ncolumns = df.columns.values\n\ncols = 5\nrows = len(columns) \/\/ cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(30,100), sharex=False)\n\nplt.subplots_adjust(hspace = 0.3)\ni=0\n\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(columns):\n            axs[r, c].set_visible(False)\n        else:\n            hist1 = axs[r, c].hist(train[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"cornflowerblue\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Train\")\n            hist2 = axs[r, c].hist(test[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"darkorange\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Test\")\n            axs[r, c].set_title(columns[i], fontsize=14, pad=5)\n            axs[r, c].tick_params(axis=\"y\", labelsize=13)\n            axs[r, c].tick_params(axis=\"x\", labelsize=13)\n            axs[r, c].grid(axis=\"y\")\n            axs[r, c].legend(fontsize=13)\n                                  \n        i+=1\n\nplt.show();\n","26ceb3f6":"X = train.drop(columns=[\"id\", \"loss\"])\nvalue = train[\"loss\"]","cd2e2ec2":"X_train, X_test, t_train, t_test = train_test_split(X, value, test_size=0.2, random_state=0)\n\nlgb_train = lgb.Dataset(X_train, t_train)\nlgb_eval = lgb.Dataset(X_test, t_test, reference=lgb_train)\n\nparams = {\n        'task': 'train',\n        'boosting_type': 'gbdt',\n        'objective': 'regression',\n        'metric': 'rmse',#l2\n        'learning_rate': 0.1,\n        'num_leaves': 3,\n        'max_bin': 234,#234\n        'num_iterations': 20000,\n        'verbosity': -1\n}\n\nmodel = lgb.train(\n    params,\n    train_set=lgb_train,\n    valid_sets=lgb_eval,\n    early_stopping_rounds=100,\n    verbose_eval=100\n)\n\n\n# Verification\n\npred = model.predict(X_test)\n\nmse = mean_squared_error(t_test, pred) # MSE(\u5e73\u5747\u4e8c\u4e57\u8aa4\u5dee)\u306e\u7b97\u51fa\nrmse = np.sqrt(mse) # \u221aMSE\u306e\u7b97\u51fa\nprint('RMSE : {}'.format(rmse))\n\n#r2 = r2_score(t_test,pred)\n#print('R2    : {}'.format(r2))","b9964d08":"X_test = test.drop(columns=[\"id\"])\nsample_submission['loss'] = model.predict(X_test)\nsample_submission","f5704cd6":"sample_submission.to_csv('submission.csv', index=False)","a4087cfe":"# 2.EDA","2a474082":"# 3.Modeling","77fecbeb":"# 5.Make submission file","ec839fb6":"# 1. Import data","48eb399c":"# 4.Prediction"}}