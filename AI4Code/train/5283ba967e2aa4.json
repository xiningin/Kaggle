{"cell_type":{"8a833069":"code","4ebf0427":"code","b8f871f8":"code","e8490269":"code","24b10eed":"code","636e9d5a":"code","10ff7d63":"code","36ecbf98":"code","14f4801c":"code","94cd4a22":"code","0879b717":"code","5b2ab029":"code","bd249941":"code","4e536b26":"code","ef00c485":"code","a667af0b":"code","51a7f1ee":"code","9831a60e":"code","144ebe8d":"code","d4b689c0":"code","c952155c":"markdown","75562439":"markdown","1a70c63f":"markdown","447e38d2":"markdown","acd7fbdb":"markdown","1ae06192":"markdown","ab41d90e":"markdown","a3fbc529":"markdown","37166cbb":"markdown","25f46373":"markdown","decd8173":"markdown","da29ea3a":"markdown","0ebf99e3":"markdown"},"source":{"8a833069":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM, Embedding, SpatialDropout1D, Dropout, Bidirectional, Lambda, Input\nfrom tensorflow.keras.optimizers import Adam\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\n# set random seeds for reproducibility\ntf.random.set_seed(51)\nnp.random.seed(51)","4ebf0427":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n            path = os.path.join(dirname, filename)","b8f871f8":"df = pd.read_csv(path, names=['index', 'date', 'sunspot'], header=0)\ndf['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\nprint(df.head(10))\nmax_sunspot = df.sunspot.values.max()\nprint('Max sunspot recorded: ', max_sunspot)","e8490269":"df.plot('date', 'sunspot', figsize=(15,10))\ndf.plot('index', 'sunspot', figsize=(15,10))","24b10eed":"df.groupby(df.date.dt.year)['sunspot'].mean().plot(figsize=(15,10))","636e9d5a":"scaler = MinMaxScaler().fit(df.sunspot.values.reshape(-1, 1))\nsunspot_values = scaler.transform(df.sunspot.values.reshape(-1, 1))","10ff7d63":"def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n    # create dataset from series\n    ds = tf.data.Dataset.from_tensor_slices(series)\n    # slice data into appropiate windows\n    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n    # flatten data into batches\n    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n    # shuffle to avoid overfitting\n    ds = ds.shuffle(shuffle_buffer)\n    # separate features and label\n    ds = ds.map(lambda w: (w[:-1], w[-1:]))\n    # batch for training\n    ds = ds.batch(batch_size).prefetch(1)\n    return ds","36ecbf98":"split_time = 12*11*20 # 12 months * 11 years * 20 cycles\nwindow_size = 50\nbatch_size = 10\n\nx_train = sunspot_values[:split_time]\ntrain_set = windowed_dataset(x_train, window_size=window_size, batch_size=batch_size, shuffle_buffer=len(x_train))\nx_val = sunspot_values[split_time:]\nval_set = windowed_dataset(x_val, window_size=window_size, batch_size=batch_size, shuffle_buffer=len(x_val))\ntime_train = df.index.values[:split_time]\ntime_val = df.index.values[split_time:]\nprint('Train size: ', len(x_train))\nprint('Val size: ', len(x_val))","14f4801c":"tf.keras.backend.clear_session()\nmodel = Sequential()\nmodel.add(Input(shape=[None, 1], name='sunspot'))\nmodel.add(Bidirectional(LSTM(50, dropout=0.05, return_sequences=True)))\nmodel.add(Bidirectional(LSTM(50, dropout=0.05, return_sequences=True)))\nmodel.add(Dense(30, activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(1))\nmodel.summary()","94cd4a22":"lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-7 * 10**(epoch \/ 20))\nmodel.compile(Adam(lr=1e-7), loss=tf.keras.losses.Huber(), metrics=[\"mae\"])\ntrain_history = model.fit(train_set,\n                epochs=100, \n                batch_size=batch_size, \n                verbose=2, \n                validation_data=val_set,\n                callbacks=[lr_schedule])","0879b717":"plt.semilogx(train_history.history[\"lr\"], train_history.history[\"loss\"])\nplt.semilogx(train_history.history[\"lr\"], train_history.history[\"val_loss\"])\nplt.axis([1e-8, 1e-3, 0, 0.034])","5b2ab029":"tf.keras.backend.clear_session()\nmodel = Sequential()\nmodel.add(Input(shape=[None, 1], name='sunspot'))\nmodel.add(Bidirectional(LSTM(50, dropout=0.05, return_sequences=True)))\nmodel.add(Bidirectional(LSTM(50, dropout=0.05, return_sequences=True)))\nmodel.add(Dense(30, activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(1))\nmodel.summary()\n\nmodel.compile(Adam(lr=9e-6), loss=tf.keras.losses.Huber(), metrics=[\"mae\"])\ntrain_history = model.fit(train_set,\n                epochs=500, \n                batch_size=batch_size, \n                verbose=2, \n                validation_data=val_set\n                         )","bd249941":"train_loss = train_history.history['loss'][100:]\nval_loss = train_history.history['val_loss'][100:]\nepochs = range(len(train_loss))\nplt.figure(figsize=(20, 10))\nplt.plot(epochs, train_loss, 'r')\nplt.plot(epochs, val_loss, 'b')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend([\"Train Loss\", \"Val Loss\"])","4e536b26":"val_loss = train_history.history['val_loss'][100:]\nepochs = range(len(val_loss))\nplt.figure(figsize=(20, 10))\nplt.plot(epochs, val_loss, 'b')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend([\"Val Loss\"])","ef00c485":"forecast = []\nfor time in range(len(sunspot_values) - window_size):\n    forecast.append(model.predict(sunspot_values[time:time + window_size][np.newaxis]))","a667af0b":"train_preds = np.array(forecast[: split_time - window_size])[:, 0, 0]\ntrain_preds = scaler.inverse_transform(train_preds.reshape(-1,1))\nx_train = scaler.inverse_transform(x_train.reshape(-1,1))\nplt.figure(figsize=(15, 10))\nplt.plot(time_train[window_size:], x_train[window_size:], 'r')\nplt.plot(time_train[window_size:], train_preds, 'b')\nplt.title('Train predictions')","51a7f1ee":"print('Train mean absolute error (mae): ')\nprint(tf.keras.metrics.mean_absolute_error(x_train[window_size:].ravel(), train_preds.ravel()).numpy())","9831a60e":"val_preds = np.array(forecast[split_time:])[:, 0, 0]\nval_preds = scaler.inverse_transform(val_preds.reshape(-1,1))\nx_val = scaler.inverse_transform(x_val.reshape(-1,1))","144ebe8d":"plt.figure(figsize=(15, 10))\nplt.plot(time_val[:-window_size], x_val[window_size:], 'r')\nplt.plot(time_val[:-window_size], val_preds, 'b')\nplt.title('Validation predictions')","d4b689c0":"print('Validation mean absolute error (mae): ')\nprint(tf.keras.metrics.mean_absolute_error(x_val[window_size:].ravel(), val_preds.ravel()).numpy())","c952155c":"# Sunspots - Time series problem","75562439":"<a name='0'><\/a>\n# 0. Overview\nSunspots are temporary phenomena on the Sun's photosphere that appear as spots darker than the surrounding areas. They are regions of reduced surface temperature caused by concentrations of magnetic field flux that inhibit convection. Sunspots usually appear in pairs of opposite magnetic polarity. Their number varies according to the approximately 11-year solar cycle.\nIn this notebook, we attempt to model and predict the number of sunspots given historical data dating back to 1749. This is a [Kaggle](https:\/\/www.kaggle.com\/robervalt\/sunspots) dataset.","1a70c63f":"![s%20%281%29.jpg](attachment:s%20%281%29.jpg)","447e38d2":"We can also plot the average per year to average out the month-to-month variance and see the pattern more clearly.","acd7fbdb":"<a name='5'><\/a>\n# 4. TO DO\n\nThe mean absolute error is as high but we can do better (specially near the peaks), the following are to be studied:\n\n- Add recurrent dropout on each LSTM model\n- Fine-tune the batch size and window size\n- Explore other model architectures (number of LSTM\/Dense layers)\n- Add convolutional layers","1ae06192":"<a name='4'><\/a>\n# 4. Predict\nUsing our trained model, we can now predict and compare our predictions using the mean absolute error.","ab41d90e":"<a name='1'><\/a>\n# 1. Read and explore dataset\nEach sample has the following:\n\n*   Index\n*   Date\n*   Monthly mean total sunspot number","a3fbc529":"Plot the learning rate versus the loss to choose the ideal learning rate. According to this [study](https:\/\/arxiv.org\/abs\/1506.01186), we need to choose the learning rate that has the greatest slope. Looking at the image below, we will choose ~9e-6.","37166cbb":"<a name='2'><\/a>\n# 2. Prepare data\nTurn pandas dataframe into arrays for training, specifically, splitting features and labels and batching","25f46373":"By plotting the validation loss alone, we can see that there is a constant decrease and indicates that the model is not overfitting. Further fine-tune of hyperparameters is needed to smooth out the noise.","decd8173":"## Outline\n- [0. Overview](#0)\n- [1. Read and explore dataset](#1)\n- [2. Prepare data](#2)\n- [3. Train model](#3)\n- [4. Predict](#4)\n- [5. TO DO](#5)","da29ea3a":"### Visualizing forecasting data","0ebf99e3":"<a name='3'><\/a>\n# 3. Train model\nSince we know that the seasonality of this phenomenom is about ~11 years, we can split the data such that the training set takes a multiple of it."}}