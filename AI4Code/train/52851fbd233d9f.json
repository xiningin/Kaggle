{"cell_type":{"4cdb6536":"code","6c0ddfb8":"code","4b8ec7b6":"code","2216a54d":"code","276c32d6":"code","f7c85d67":"code","20f35c3c":"code","1a5d4347":"code","1891ca3d":"code","c710c6e7":"code","ae080d1e":"code","5adb26bc":"code","1abe0671":"code","4255e103":"code","cf54cefb":"code","6227e5d8":"code","9abdbe4a":"code","8cf9ea1a":"code","f4870da7":"code","8246e8f4":"code","ece54269":"code","e6bdcc68":"markdown","ec69d51e":"markdown","d8ee4a7f":"markdown","3d913d2a":"markdown","41bd3eca":"markdown","038bfa68":"markdown","5c0e3369":"markdown","f6f37ed1":"markdown","c1bcfdfa":"markdown","f7e5d7e1":"markdown","1d61c01d":"markdown","08a7a180":"markdown","dbaab169":"markdown","56514df3":"markdown","91a08083":"markdown","4ffb2a0b":"markdown","5ba90ba4":"markdown","dd780618":"markdown","b8181174":"markdown","d2f324cb":"markdown","8f2a3dd5":"markdown","39b096cf":"markdown","44fff623":"markdown","1eb41d00":"markdown","04febbae":"markdown","863396a4":"markdown","e97f195f":"markdown","8758eecd":"markdown","484b7a0f":"markdown","06b87043":"markdown"},"source":{"4cdb6536":"# Import all required modules for the analysis(make sure that you installed all these modules prior to importing)\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport matplotlib.pyplot as plt\nimport math\nfrom IPython.display import display,HTML\nfrom patsy import dmatrices\nimport seaborn as sns; sns.set()\n\nimport warnings\nwarnings.filterwarnings('ignore')\n%pylab inline","6c0ddfb8":"# Reading the train data\ntrain_df = pd.read_csv('..\/input\/Train_rev1.csv')","4b8ec7b6":"# Checking the data types in the train data\nprint (train_df.info())\n\n# Let's look at the unique values present in the data frame to have a general understanding of the data\nnames = train_df.columns.values\nuniq_vals = {}\nfor name in names:\n    uniq_vals[name] = train_df.loc[:,name].unique()\n    print(\"Count of %s : %d\" %(name,uniq_vals[name].shape[0]))","2216a54d":"# Distribution of salaries based on the train data\npylab.rcParams['figure.figsize'] = (20,10)\nplt.hist(train_df['SalaryNormalized'], bins='auto')\nplt.xlabel('Salaries')\nplt.ylabel('Number of postings')\nplt.title('Histogram of Salaries')","276c32d6":"# Randomly selecting 2500 rows to train the classifier\nimport random\nrandom.seed(1)\nindices = list(train_df.index.values)\nrandom_2500 = random.sample(indices,2500)\n\n# Subsetting the train data based on the random indices\ntrain_df1 = train_df.loc[random_2500].reset_index()","f7c85d67":"pylab.rcParams['figure.figsize'] = (20,10)\nplt.hist(train_df1['SalaryNormalized'], bins='auto')\nplt.xlabel('Salaries')\nplt.ylabel('Number of postings')\nplt.title('Histogram of Salaries')","20f35c3c":"import re\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize, sent_tokenize \nstop_words = set(stopwords.words('english')) \nfrom string import punctuation\nimport collections","1a5d4347":"# To obtain the full width of a cell in a dataframe\npd.set_option('display.max_colwidth', -1)\ndesc = train_df1.loc[1,'FullDescription']\n\n# Creating a list of words from all the job descriptions in train_df1 data\nall_desc = []\nfor i in range(0,train_df1.shape[0]):\n    desc = train_df1.loc[i,'FullDescription']\n    desc1 = desc.lower()\n    # Removing numbers, *** and www links from the data\n    desc2 = re.sub('[0-9]+\\S+|\\s\\d+\\s|\\w+[0-9]+|\\w+[\\*]+.*|\\s[\\*]+\\s|www\\.[^\\s]+','',desc1)\n    # Removing punctuation\n    for p in punctuation:\n        desc2 = desc2.replace(p,'')\n    all_desc.append(desc2)\n","1891ca3d":"# Creating word tokens for all the descriptions\nfinal_list = []\nfor desc in all_desc:\n    word_list = word_tokenize(desc)\n    final_list.extend(word_list)","c710c6e7":"# 3. Tagging parts of speech\npos_tagged = nltk.pos_tag(final_list)\n\n# 4. Identifying the most common parts of speech\ntag_fd = nltk.FreqDist(tag for (word, tag) in pos_tagged)\ntag_fd.most_common()[:5]","ae080d1e":"# Excluding stopwords from the analysis\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english')) \n\nlist_wo_stopwords = []\nfor w in final_list:\n    if w not in stop_words:\n        list_wo_stopwords.append(w)\n        \n# 3. Tagging parts of speech\npos_tagged_wo_sw = nltk.pos_tag(list_wo_stopwords)\n\n# 4. Identifying the most common parts of speech\ntag_fd_wo_sw = nltk.FreqDist(tag for (word, tag) in pos_tagged_wo_sw)\ntag_fd_wo_sw.most_common()[:5]","5adb26bc":"from nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\n\n# Lemmatization without specifying parts of speech\nlist_lemmatized = []\nfor word in list_wo_stopwords:\n    list_lemmatized.append(lemmatizer.lemmatize(word))\n\nword_freq_lem = dict(collections.Counter(list_lemmatized))\nkeys = list(word_freq_lem.keys())\nvalues = list(word_freq_lem.values())\ndf_lem = pd.DataFrame({'words':keys,'freq':values})\ndisplay(df_lem.sort_values(by = 'freq',ascending = False)[:10])\n\nfrom wordcloud import WordCloud\nfrom collections import Counter\nword_could_dict=Counter(word_freq_lem)\nwordcloud = WordCloud(width = 1000, height = 500).generate_from_frequencies(word_could_dict)\n\nplt.figure(figsize=(15,8))\nplt.imshow(wordcloud)\n","1abe0671":"p_75 = np.percentile(train_df1['SalaryNormalized'], 75)\ntrain_df1['target'] = train_df1['SalaryNormalized'].apply(lambda x: 1 if x>=p_75 else 0)","4255e103":"costly_cities = ['London','Brighton','Edinburgh','Bristol','Southampton','Portsmouth','Exeter','Cardiff','Manchester',\n                 'Birmingham','Leeds','Aberdeen','Glasgow','Newcastle','Sheffield','Liverpool']\ncostly_cities_lower = [x.lower() for x in costly_cities]\n\n# More robust if lower() is applied\ntrain_df1['location_flag'] = train_df1['LocationNormalized'].apply(lambda x: 1 if x in costly_cities else 0)","cf54cefb":"# Dropping job description column from the dataset\ntrain_x = train_df1.drop(['FullDescription','index','Id','LocationRaw','Title','Company','LocationNormalized','SalaryRaw','SalaryNormalized',\n                    'target'],axis=1)\n\ntrain_x1 = pd.get_dummies(train_x,drop_first=True)\nX_n = np.array(train_x1)\ny_n = np.array(train_df1['target'])\n\nfrom sklearn.model_selection import train_test_split\nX_train_num, X_val_num, y_train_num, y_val_num = train_test_split(X_n, y_n, test_size=0.3, random_state=1)\n","6227e5d8":"# Bernoulli\nfrom sklearn.naive_bayes import BernoulliNB\nclf = BernoulliNB()\nclf.fit(X_train_num, y_train_num)\n\nfrom sklearn import metrics\nprediction_train = clf.predict(X_val_num)\nmat_n = metrics.confusion_matrix(y_val_num, prediction_train)\nmat_n\nprint (metrics.accuracy_score(y_val_num, prediction_train))","9abdbe4a":"# Baseline accuracy\n1-(sum(y_val_num)\/len(y_val_num))\n# sum(prediction_train)","8cf9ea1a":"def models(l):\n    # Counting the occurence of each word in the corpus\n    from sklearn.feature_extraction.text import CountVectorizer\n    count_vect = CountVectorizer()\n    X_train_counts = count_vect.fit_transform(l)\n    count_vect.get_feature_names()\n    X_matrix= X_train_counts.todense()\n\n    y = np.array(train_df1['target'])\n\n    # Creating the train and test split\n    from sklearn.model_selection import train_test_split\n    X_train_m, X_val_m, y_train_m, y_val_m = train_test_split(X_train_counts, y, test_size=0.3, random_state=1)\n\n    #Multinomial\n\n    from sklearn.naive_bayes import MultinomialNB\n    clf = MultinomialNB().fit(X_train_m, y_train_m)\n    labels_m = clf.predict(X_val_m)\n\n    from sklearn.metrics import confusion_matrix\n    from sklearn.metrics import accuracy_score\n    mat_m = confusion_matrix(y_val_m, labels_m)\n\n    # Bernoulli\n    # Changing the data to binary to input BernoulliNB\n    x_train_b1 = X_train_counts.todense()\n    X_train_counts_ber = np.where(x_train_b1 >=1 ,1,0)\n\n    # Creating the train and test split for bernoulli\n    from sklearn.model_selection import train_test_split\n    X_train_b, X_val_b, y_train_b, y_val_b = train_test_split(X_train_counts_ber, y, test_size=0.3, random_state=1)\n\n    from sklearn.naive_bayes import BernoulliNB\n    clf = BernoulliNB().fit(X_train_b, y_train_b)\n    labels_b = clf.predict(X_val_b)\n\n    from sklearn.metrics import confusion_matrix\n    from sklearn.metrics import accuracy_score\n    mat_b = confusion_matrix(y_val_b, labels_b)\n    print ('Confusion matrix:',mat_b)\n    print ('Accuracy using BernoulliNB:',accuracy_score(y_val_b, labels_b))\n\n\n    print ('Confusion matrix:',mat_m)\n    print ('Accuracy using MultinomialNB:',accuracy_score(y_val_m, labels_m))","f4870da7":"models(all_desc)","8246e8f4":"# Removing stopwords\ndef remove_stopwords(s):\n    big_regex = re.compile(r'\\b%s\\b' % r'\\b|\\b'.join(map(re.escape, stop_words)))\n    return big_regex.sub('',s)\n\nall_desc_wo_sw = [remove_stopwords(s) for s in all_desc]\nmodels(all_desc_wo_sw)","ece54269":"from nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\n\nfrom nltk.corpus import wordnet\n\ndef get_wordnet_pos(treebank_tag):\n\n    if treebank_tag.startswith('J'):\n        return wordnet.ADJ\n    elif treebank_tag.startswith('V'):\n        return wordnet.VERB\n    elif treebank_tag.startswith('N'):\n        return wordnet.NOUN\n    elif treebank_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return None \n\n# Lemmatizing the data\nall_desc_lemm = []\nfor i in range(0,len(all_desc_wo_sw)):\n    desc = all_desc_wo_sw[i]\n    desc2 = re.sub('[0-9]+\\S+|\\s\\d+\\s|\\w+[0-9]+|\\w+[\\*]+.*|\\s[\\*]+\\s|www\\.[^\\s]+','',desc)\n    for p in punctuation:\n        desc2 = desc2.replace(p,'')\n    tagged = nltk.pos_tag(word_tokenize(desc2))\n    list_lemmatized = []\n    for word, tag in tagged:\n        wntag = get_wordnet_pos(tag)\n        if wntag is None:# not supply tag in case of None\n            list_lemmatized.append(lemmatizer.lemmatize(word)) \n        else:\n            list_lemmatized.append(lemmatizer.lemmatize(word, pos=wntag))\n    k = ' '.join(list_lemmatized)   \n    all_desc_lemm.append(k)\n\nmodels(all_desc_lemm)","e6bdcc68":"**3. After lemmatizing the data**\n\nLemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.([Click to know more](https:\/\/nlp.stanford.edu\/IR-book\/html\/htmledition\/stemming-and-lemmatization-1.html))\n\nWe will lemmatize and see if there is any improvement in the result","ec69d51e":"## 5. Conclusion","d8ee4a7f":"## Objective\n\n* To predict the salary of a job based on the data posted on the company's website or on any job portal\n* To compare how addition of text(job description) into the models increases the prediction power","3d913d2a":"**Technical limitation**:  \nAs you can see, there are about ~240 k rows in the dataset. As i am currently running the analysis on my personal system with 8 GB, i will randomly take a small number of rows to train my classifier and incrementally add data to train untill its feasible.","41bd3eca":"We can observe that the job descriptions are skewed mostly towards the lower end(mostly < 50,000) showing that most of the jobs are on the lower end of the job salary spectrum. This distribution might be useful as we move further into the analysis, as this might help us detect if there is any bias in our final analysis.","038bfa68":"## 2. Exploratory Data analysis - Job descriptions\n\nLet's look into the job descriptions and try to answer some of the questions to have more clarity\n1. What are the top 5 parts of speech in the job descriptions? How frequently do they appear?\n2. How do these numbers change if you exclude stopwords?\n3. What are the 10 most common words after removing stopwords and lemmatization?","5c0e3369":"## About data\n\nI have used the job salary prediction dataset from Kaggle to perform the analysis and achieve the objective that i have mentioned above.\nFollowing is the link for the data description [Data Description](https:\/\/www.kaggle.com\/c\/job-salary-prediction\/data)","f6f37ed1":"### Creation of features\n**Creating a proxy variable for location**\nHere as there are so many locations, creating dummy variable will bloat the dataset a lot. So, we will create a proxy variable\nfor the location by taking the cities with high cost of living under one group(1) and the others in a separate group(0).\n\nWe have considered 17 cities as cities with high cost of living [Source](https:\/\/www.thisismoney.co.uk\/money\/mortgageshome\/article-5283699\/The-cheapest-expensive-cities-live-in.html)","c1bcfdfa":"**2.How do these numbers change if you exclude stopwords?**  \nIn english,for that matter in any language, there will be a lot of stop words that repeat a lot of times in the sentence but do not actually carry a lot of information.  \nFor examples: the, a , an ,and etc.  \nnltk library has a repository for stop words. Let's remove those stopwords and check how the above results will change.","f7e5d7e1":"Now next step after cleaning the descriptions is to tokenize them. Here for simplicity purpose, i have just considered word tokenize. We can also tokenize the descriptions by sentences but that is not suitable for our problem here.\n[Refer this link](https:\/\/textminingonline.com\/dive-into-nltk-part-ii-sentence-tokenize-and-word-tokenize) to know more about word tokenizer and sentence tokenizer","1d61c01d":"**Creating the target column by splitting the salary normalized into high(above 75th percentile) and low(below 75th percentile)**","08a7a180":"You can see that we did not very well with just numeric values in the model as the accuracy is almost equal to the baseline accuracy. Now let's look at the model with just the job descriptions and see if we can predict the salaries with some higher accuracies","dbaab169":"**Creating dummy variables for all the columns except for job descriptions and splitting the data into train and validation set**  ","56514df3":"You can see that Noun(NN) , adjective(JJ), preposition(IN), determiner(DT), plural nouns(NNS)  are the most common parts of speech from the job descriptions.\n[Refer this link](https:\/\/www.ling.upenn.edu\/courses\/Fall_2003\/ling001\/penn_treebank_pos.html) to look at the descriptions of the parts of speech for the above result","91a08083":"You can see the accuracy has increased from 80% in the first step to 82% in the lemmatization while using text data. There is a still a lot of scope for this analysis. We can use SVM algorithm to predict the salary based on the text data. In the next version of this analysis, i will try more models and compare them with the results from the current analysis. Hope you enjoyed the analysis. Please share your thoughts.","4ffb2a0b":"**1. What are the top 5 parts of speech in the job descriptions? How frequently do they appear?** ","5ba90ba4":"While looking at the data, you can observe that the numbers are masked as *** and they turn out to be of no value for us in the analysis. In addition to that, there are a few data cleaning steps that i have performed in the below code\n1. Remove website links from the data\n2. Remove punctuations\n3. Removing numbers\n\nBy running these steps, we can achieve a higher accuracy as the data becomes more cleaned and the predictive power of the algorithm increases because of that.","dd780618":"After removing stopwords, there are two important observations in comparison to the previous result\n1. Prepositions and determiners disappeared from the top 5 set as most of these are present in the stopwords imported from NLTK \n2. The counts of nouns and plural nouns have decreased and the adjectives have increased.\n3. Verb, gerund or present participle(VBG) and Verb, non-3rd person singular present(VBP) moved to the top 5 list","b8181174":"Now before finding the most common parts of speech in the job description, we first need to tag each word with the relevant parts of speech. I have used pos_tag from nltk library to tag the parts of speech. This uses the pos tags from treebank pos repository.   \nAfter creating the pos tags, the next would be to find the frequent occurence of each parts of speech and find the most common parts of speech","d2f324cb":"**3. What are the 10 most common words after removing stopwords and lemmatization?**\n\nHere is some backgrond information about [lemmatization](https:\/\/textminingonline.com\/dive-into-nltk-part-iv-stemming-and-lemmatization)\n\nAs we have already removed stopwords and create a dataframe list_wo_stopwords earlier, our first step here would be to perform lemmatization and then identify the 10 most common words.  \nI have also plotted the wordcloud of all the words to visualize these words.  ","8f2a3dd5":"The distribution seemingly holds good and is comparably similar to the orginial distribution. With this, let's move forward with the actual analysis of cleaning the data and creating the features for the analysis.","39b096cf":"## 4. Models using text as variables\n\nNow let's run the Naive Bayes using the words in the job description as variables. You will notice that text in itself has a higher prediction power than the non-text variables\n\nFor this model, we will run both Bernoulli and Multinomial Naive Bayes([click to know more](https:\/\/syncedreview.com\/2017\/07\/17\/applying-multinomial-naive-bayes-to-nlp-problems-a-practical-explanation\/)), to check the performance of both the models\n\nWe will run both the models in multiple steps\n1. Without removing stopwords from the data\n2. After removing stopwords from the data\n3. After lemmatizing the data\n\nThis will help us understand the effect of each step on the accuracy of the result\n\nHere i have created a function to make it easy to perform the above 3 steps","44fff623":"**1. Without removing stopwords**","1eb41d00":"**2. After removing stopwords from the data**","04febbae":"**Using test data to predict the salary**","863396a4":"### 1.Exploratory data analysis - general","e97f195f":"## 3. Model without text variables\n\nAs i have told, we will perform the analysis with 2 types of variables\n1. Non-text variables\n2. Text variables\n\nJust to revisit, our objective is to predict the salaries based on the information posted on the website including variables such as location, company and job description.\n\nWith this in mind, let's proceed with the analysis\n\nFor this analysis, let's define the target variable based on the salary normalized. This converts the problem into a classfication problem and reduces the complexity. Further, based on the requirement we can perform a regression analysis to predict a number for the salary","8758eecd":"**Let's run a Bernoulli Naive Bayes algorithm to predict the salary of a job using the non-text variables. I have selected this algorithm because most often Naive bayes gives good results in simplistic scenarios.  \nIt also acts as a good starting point to compare the effect of non-text and text variables on prediction.**","484b7a0f":"## Approach\n\nRunning a classfier on the data is never the first step of any analysis be it on numerical data or text data. We will have to first understand the data, clean it, perform exploratory data analysis following which we will perform feature extraction and only then think of training a classifier on the training data.\n\nSo as a logical first step, i started with importing the data and performing exploratory data analysis on the data. For easy understanding of the entire analysis, i have listed all the steps that i have followed below and divide my entire analysis based in the same chronological order.\n\n1. Exploratory Data analysis\n2. Exploratory Data analysis - Job descriptions\n3. Models without text variables\n4. Models with text variables\n5. Conclusion","06b87043":"Now let's see the salary distribution in this data and compare it with the original data"}}