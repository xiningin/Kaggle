{"cell_type":{"596e0503":"code","b5d7e427":"code","c7d374de":"code","617a3262":"code","890deb6a":"code","e4851c56":"code","ca0cfa39":"code","72835534":"code","3eb0dfd6":"code","f997c992":"code","df64b006":"code","f962ba77":"code","2af96972":"code","f660eb6f":"code","27951af3":"code","4aa00cd9":"markdown","e265fe6b":"markdown","c2707bab":"markdown","5d2f61cb":"markdown","ceb25ee7":"markdown","fefb65b9":"markdown","e949cbfd":"markdown","9f0f0036":"markdown","8f902579":"markdown","16cd9674":"markdown","ae5546be":"markdown","580c7f9f":"markdown","370232fd":"markdown","b3bc4437":"markdown","d638de08":"markdown","c08d36e4":"markdown","a6f03b9a":"markdown"},"source":{"596e0503":"import numpy as np\nimport pandas as pd        \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport gc\nimport os\nimport logging\nimport datetime\nimport warnings\nfrom tqdm import tqdm_notebook\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('max_columns', 310)\npd.set_option('max_rows', 200)","b5d7e427":"PATH=\"..\/input\/ubiquant-market-prediction\/\"\nos.listdir(PATH)","c7d374de":"data_types_dict = {\n    'time_id': 'int32',\n    'investment_id': 'int16',\n    \"target\": 'float16'}\n\nfeatures = [f'f_{i}' for i in range(300)]\n\nfor f in features:\n    data_types_dict[f] = 'float16'\n    \ntarget = 'target'","617a3262":"%%time\ntrain_df = pd.read_csv('\/kaggle\/input\/ubiquant-market-prediction\/train.csv', \n                       usecols = data_types_dict.keys(),\n                       dtype=data_types_dict, \n                       index_col = 0)","890deb6a":"train_df.shape","e4851c56":"train_df.head()","ca0cfa39":"def missing_data(data):\n    \n    total = data.isnull().sum()\n    percent = (data.isnull().sum()\/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    \n    return(np.transpose(tt))\n\nmissing_data(train_df)","72835534":"train_df.describe()","3eb0dfd6":"feats = []\nfor i in range(0,300):\n    if train_df['f_'+str(i)].std()!=0:\n        feats.append('f_'+str(i))\n\nprint('Features with Non Zero Standard Deviation: {}'.format(feats))","f997c992":"train_df['target'].hist(bins = 100, figsize = (10,6))","df64b006":"for f in np.random.choice(train_df['investment_id'].unique(), 10):\n    train_df[train_df['investment_id'] == f]['target'].hist(bins=100, alpha=0.2,figsize=(10,6))","f962ba77":"def plot_feature_distribution(df1, df2, label1, label2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(10,10,figsize=(20,24))\n\n    for feature in features:\n        i += 1\n        plt.subplot(10,10,i)\n        sns.distplot(df1[feature], hist=False,label=label1)\n        sns.distplot(df2[feature], hist=False,label=label2)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n        plt.tick_params(axis='y', which='major', labelsize=6)\n    plt.show();","2af96972":"t0 = train_df.loc[train_df['target'] > 0]\nt1 = train_df.loc[train_df['target'] < 0]\nfeatures = train_df.columns.values[2:102]\nplot_feature_distribution(t0, t1, '>0', '<0', features)","f660eb6f":"features = train_df.columns.values[102:202]\nplot_feature_distribution(t0, t1, '>0', '<0', features)","27951af3":"features = train_df.columns.values[202:]\nplot_feature_distribution(t0, t1, '>0', '<0', features)","4aa00cd9":"Let's check the **target** distribution in train dataset.","e265fe6b":"### Data exploration\nLet's check the train set.","c2707bab":"It's interesting to see here that:\n- Majority of features have 0 standard deviations, seems like they are deidentified by some transformation.\n- Only `f_124` have a non-zero standard deviations (std = 0.04837)","5d2f61cb":"There is no missing data present in the train data. Let's check the numerical value distribution.","ceb25ee7":"### Feature plots with Target\nLet's see the density plot of variables in train dataset.","fefb65b9":"#### Load data\n- Let's see what all files are provided by Ubiquant","e949cbfd":"Our dataset contains 300 anonymous features that don't have any description, `investment_id,` and target that is also some anonymous float value.\n\n**Train contains:**\n- time_id (int)\n- investment_id (int)\n- target (float)\n- 300 numerical features (float), from f_0 to f_299","9f0f0036":"On a high-level target for each investment_id also looks ok.","8f902579":"- This notebook contains an initial EDA of train dataset for the [Ubiquant Market Prediction competition](https:\/\/www.kaggle.com\/c\/ubiquant-market-prediction) built on https:\/\/www.kaggle.com\/ilialar\/ubiquant-eda-and-baseline and https:\/\/www.kaggle.com\/gpreda\/santander-eda-and-prediction\n- Main objective is to find some Features of interest and group features based on similarity","16cd9674":"Let's load the train file.","ae5546be":"- `example_sample_submission.csv` sample submission file (Not for EDA)\n- `ubiquant` API wheel for loading test data (Not for EDA)\n- `example_test.csv` sample test file (Not for EDA)\n- `train.csv` This is our little treasure - TRAINING DATA","580c7f9f":"#### Some Observations based on the First 100 features density plot:\n- Features with significant spikes: `['f_23','f_39','f_42','f_45','f_55','f_64','f_68','f_71','f_77','f_79','f_80','f_92']`\n- Features with High Skewness: `['f_63','f_87','f_59','f_3','f_67','f_34','f_7','f_57','f_98','f_74','f_56','f_26','f_33','f_30','f_8','f_58']`\n\nWe can use this information to group features and do feature engineering using them for our prediction model.","370232fd":"Let's check if we have any missing data.","b3bc4437":"### More to be Added later...","d638de08":"#### Load packages ","c08d36e4":"Let's have a glimpse of train dataset.","a6f03b9a":"Target values look quite normal without any outliers or long tails. Let's also plot distributions of targets with few random features."}}