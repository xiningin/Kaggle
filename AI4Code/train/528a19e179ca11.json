{"cell_type":{"f72960dc":"code","1656e69d":"code","abbb278f":"code","ba5e89a6":"code","609100c1":"code","c1144226":"code","30857828":"code","2600edbb":"code","8b3bf451":"code","f2f8e585":"code","cef59159":"code","60a1f948":"code","12061bfd":"code","c019a63b":"code","b72fa043":"code","18c8e422":"code","60d6140d":"code","b297f0ed":"code","458e469f":"code","8cb388ab":"code","15fea4af":"code","e75cdbab":"code","5f5abfc4":"code","3d166628":"code","febf3a97":"code","c55ed210":"code","66bfdd0c":"code","844d7fcf":"code","571aff4f":"code","891b6a20":"code","3e92834a":"code","8f6eeb8c":"code","ed21e457":"code","ec03f3be":"code","d0061119":"code","962f07ee":"code","93f52381":"code","fd9b7845":"code","3a858c64":"code","6e5ac0f6":"code","383655a6":"code","b2ddf77a":"code","5f1f49d9":"code","c316a935":"code","67078dca":"code","8f71ac17":"code","eafb1722":"code","ebcf2c4d":"code","887b9685":"code","6258c2cb":"code","ab660296":"code","c5b8a55f":"code","8ec6c126":"code","ff1bbb39":"code","70dea5f7":"code","3aa9df40":"code","f029681c":"code","a61f484d":"code","6fdce15b":"code","3451b249":"code","6ed727bf":"code","7cf1e0b4":"code","a40e599f":"code","723f65eb":"code","2b7a708e":"code","c8a0408d":"code","f72e6e60":"code","16117a99":"code","353bda50":"code","9864eff8":"code","3942afbc":"code","c17f7a7b":"code","3497528d":"code","9f048a3a":"code","62297e57":"code","8d934cf3":"code","ecfa0c39":"code","852542b1":"code","ebe282d4":"code","d9f27193":"code","dac722d8":"code","9949a1c7":"code","162fd660":"code","d3ac25eb":"code","24e4c190":"code","a57eafb0":"code","56b174c3":"code","099b1295":"code","b07f5e09":"code","ec8a4ca1":"code","9744f495":"code","00a1bf90":"code","4968926f":"code","68f113cc":"code","bf90a4cd":"code","c5ac551a":"code","e634632c":"code","3bf7c333":"code","ddcd77d6":"code","4a2f0629":"code","11eb493e":"code","c6d875ea":"code","c15d1891":"code","37ee3848":"code","63804a18":"code","25c6b326":"code","fa59fb93":"code","882c9947":"code","983f7880":"code","a51c96c3":"code","2fbc25ef":"code","e4cf2493":"code","7febcaf0":"code","d59c19e8":"code","7a2e80a3":"code","a9fb9b7c":"code","2f1c8ae4":"code","28ffb22d":"code","70f750db":"code","ac6d4d6e":"code","b3af6a41":"code","7f17f677":"code","067eda22":"code","3d302e2b":"code","4b88cbde":"code","93fbf83b":"code","df13c51d":"code","626c17f1":"code","0ac2cb23":"markdown","9adf0c98":"markdown","bb8640bb":"markdown","4b794815":"markdown","784a0045":"markdown","9901212d":"markdown","38825392":"markdown","7c4e4468":"markdown","6f991202":"markdown","d44d4f3a":"markdown","3157fc22":"markdown","3890c74a":"markdown","043bf296":"markdown","587657e6":"markdown","ab1e89b3":"markdown","5c8a17de":"markdown","ddcead58":"markdown","0cbd3332":"markdown","cad5aa70":"markdown","5717ebe8":"markdown","55cdba35":"markdown","7cca7272":"markdown","10a00de4":"markdown","ae501d40":"markdown","3075debd":"markdown","884fcdd4":"markdown","c963c6ca":"markdown","e8a422a4":"markdown","633aa8a9":"markdown","d68a102e":"markdown","c2eb8376":"markdown","f4181dff":"markdown","10ec91e5":"markdown","102082a2":"markdown","1c1e144d":"markdown","c7ad1d08":"markdown","1bf7923d":"markdown","a3e54584":"markdown","84a00d43":"markdown","d4cd8b44":"markdown","cf6ac579":"markdown","0c7068bc":"markdown","d4d53aaa":"markdown","8de28982":"markdown"},"source":{"f72960dc":"# Ignore harmless warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#Importing libraries for data analysis and cleaning\nimport numpy as np\nimport pandas as pd\n\n#importing visualisation libraries for data visualisation\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\n\n#load datasets\ntrain = pd.read_csv(\"..\/input\/analytics-vidhya-loan-prediction\/train.csv\")\ntest = pd.read_csv(\"..\/input\/analytics-vidhya-loan-prediction\/test.csv\")\n\n#per describtion, loan amount is in 1000's\ntrain['LoanAmount'] = train['LoanAmount'] *1000\ntest['LoanAmount'] = test['LoanAmount'] *1000","1656e69d":"#Observing the first five rows of the dataset for training\ntrain.head()","abbb278f":"# Function to calculate missing values by column#\ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","ba5e89a6":"missing_values_table(train)","609100c1":"#check for duplicated data\ntrain.duplicated().sum()","c1144226":"#checking the distribution of the target column (Loan Status)\nplt.figure(figsize=(8,5))\nplt.title('Loan Status Count')\nsns.countplot(data=train,x='Loan_Status');","30857828":"#checking the distribution of the target column (Loan Status)\npx.pie(data_frame=train,names='Loan_Status',title='Distribution of Loan Status')","2600edbb":"#Checking the statistical info of the dataset(train)\ntrain.describe()","8b3bf451":"#Check for the distribution of loan amount\ntrain['LoanAmount'].plot(kind='hist',figsize=(13,8),bins=50,edgecolor='k',\n                         title='Distribution of Loan Amount').autoscale(axis='x',tight=True)","f2f8e585":"train.boxplot(column='LoanAmount',figsize=(4,7));","cef59159":"#Check for the distribution of loan amount\ntrain['ApplicantIncome'].plot(kind='hist',figsize=(13,8),bins=50,edgecolor='k',\n                              title='Applicant Income Distribution').autoscale(axis='x',tight=True)","60a1f948":"#median of loan amounts applied for\ntrain['ApplicantIncome'].median()","12061bfd":"#Check for the distribution of loan amount\ntrain['CoapplicantIncome'].plot(kind='hist',figsize=(13,8),bins=50,edgecolor='k',\n                                title='Distribution of Coapplicant Income').autoscale(axis='x',tight=True)","c019a63b":"#Median of coapplicant income\ntrain['CoapplicantIncome'].median()","b72fa043":"pd.DataFrame(train.groupby(['Property_Area','Loan_Status'])['Loan_Status'].count())","18c8e422":"plt.figure(figsize=(8,5))\nsns.countplot(data=train,x='Property_Area',hue='Loan_Status');","60d6140d":"pd.DataFrame(train.groupby(['Property_Area','Education'])['Loan_Status'].count())","b297f0ed":"a = pd.crosstab(train['Property_Area'],[train['Education'],train['Loan_Status']])\na.plot(kind='bar',stacked=True,figsize=(12,7),legend=True,title='Loan status based on education and property type').legend(loc=3, bbox_to_anchor=(1.0,0.1));","458e469f":"#Checking the barchart distribution of loan status in relation with educational status\npd.crosstab(train['Education'],train['Loan_Status']).plot(kind='bar',figsize=(12,6));","8cb388ab":"pd.DataFrame(train.groupby(['Education','Credit_History','Loan_Status'])['Loan_Status'].count())","15fea4af":"b = pd.crosstab(train['Education'],[train['Credit_History'],train['Loan_Status']])\nb.plot(kind='bar',stacked=True,figsize=(10,5),title='Distribution of education in relation with credit history and loan status');","e75cdbab":"#Grouping loan_status by credit history\npd.DataFrame(train.groupby(['Credit_History','Loan_Status'])['Loan_Status'].count())","5f5abfc4":"#Graphical representaion\nc = pd.crosstab(train['Credit_History'],train['Loan_Status'])\nc.plot(kind='bar',figsize=(10,6));","3d166628":"#Grouping loan status by education,self employed and credit history\npd.DataFrame(train.groupby(['Education','Self_Employed','Credit_History'])['Loan_Status'].count())","febf3a97":"#Barchart represnting eduaction,self-employment,and credit historys impact on loan status\nd = pd.crosstab(train['Education'],[train['Self_Employed'],train['Credit_History'],train['Loan_Status']])\nd.plot(kind='bar',stacked=True,figsize=(10,6),legend=True);","c55ed210":"f = pd.crosstab(train['Gender'],[train['Loan_Status'],train['Credit_History']])\nf.plot(kind='bar',stacked=True,figsize=(12,6));","66bfdd0c":"#Grouping loan status by Gender, and credit history\npd.DataFrame(train.groupby(['Gender','Credit_History','Loan_Status'])['Loan_Status'].count())","844d7fcf":"print('Total count of male applicants are', len(train[train['Gender'] == 'Male']))\nprint('Total count of female applicants are', len(train[train['Gender'] == 'Female']))","571aff4f":"sns.countplot(data=train,x='Married',hue='Loan_Status');","891b6a20":"round(100 * train.groupby('Married')['Loan_Status'].value_counts(normalize=True))","3e92834a":"print('Total count of married applicants are', len(train[train['Married'] == 'Yes']))\nprint('Total count of single applicants are', len(train[train['Married'] == 'No']))","8f6eeb8c":"plt.figure(figsize=(12,5))\nsns.scatterplot(data=train,x='ApplicantIncome',y='LoanAmount');","ed21e457":"plt.figure(figsize=(12,5))\nsns.scatterplot(data=train,x='CoapplicantIncome',y='LoanAmount');","ec03f3be":"px.scatter(train,x='ApplicantIncome',y='LoanAmount',color='Loan_Status',title='LOAN STATUS BASED ON APPLICANT INCOME')","d0061119":"px.scatter(train,x='ApplicantIncome',y='LoanAmount',color='Credit_History',facet_col='Loan_Status',\n           color_continuous_scale=[\"red\", \"green\", \"blue\"],title='LOAN STATUS BASED ON APPLICANTINCOME AND CREDIT HISTORY')","962f07ee":"px.scatter(train,y='LoanAmount',x='CoapplicantIncome',color='Loan_Status',title='LOAN STATUS BASED ON COAPPLICANT INCOME')","93f52381":"px.scatter(train,x='CoapplicantIncome',y='LoanAmount',color='Credit_History',facet_col='Loan_Status',\n           color_continuous_scale=[\"red\", \"green\", \"blue\"],title='LOAN STATUS BASED ON COAPPLICANTINCOME AND CREDIT HISTORY')","fd9b7845":"train['Dependents'].value_counts()","3a858c64":"temp = train.dropna()","6e5ac0f6":"from sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\ntemp['Loan_Status'] = encoder.fit_transform(temp['Loan_Status'])\ntemp['Property_Area'] = encoder.fit_transform(temp['Property_Area'])\ntemp['Education'] = encoder.fit_transform(temp['Education'])","383655a6":"temp['Dependents'] = encoder.fit_transform(temp['Dependents'])\ntemp['Married'] = encoder.fit_transform(temp['Married'])\ntemp['Self_Employed'] = encoder.fit_transform(temp['Self_Employed'])\ntemp['Gender'] = encoder.fit_transform(temp['Gender'])","b2ddf77a":"#Correlations in data\nplt.figure(figsize=(12,7))\nax = sns.heatmap(temp.corr(),cmap='coolwarm',annot=True,vmax=1,vmin=-1);\n# fix for mpl bug that cuts off top\/bottom of seaborn viz\nb, t = plt.ylim() # discover the values for bottom and top\nb += 0.5 # Add 0.5 to the bottom\nt -= 0.5 # Subtract 0.5 from the top\nplt.ylim(b, t) # update the ylim(bottom, top) values\nplt.show() ","5f1f49d9":"missing_values_table(train)","c316a935":"#Save loanID\ntrain_ID= train['Loan_ID']\ntrain = train.drop('Loan_ID',axis=1)","67078dca":"#filling n\/a with the most occuring for genders\ntrain['Gender'] = train['Gender'].fillna(train['Gender'].mode()[0])","8f71ac17":"#filling values for married\ntrain['Married'] = train['Married'].fillna(train['Married'].mode()[0])","eafb1722":"#filling values for dependents\ntrain['Dependents'] = train['Dependents'].fillna(train['Dependents'].mode()[0])","ebcf2c4d":"#filling values for self_employed\ntrain['Self_Employed'] = train['Self_Employed'].fillna(train['Self_Employed'].mode()[0])","887b9685":"avg_loans = train.pivot_table(values='LoanAmount', index='Self_Employed' ,columns='Education', aggfunc=np.median)\navg_loans","6258c2cb":"avg_loans.columns","ab660296":"def values(x):\n    return avg_loans.loc[x['Self_Employed'],x['Education']]","c5b8a55f":"train['LoanAmount'] = train['LoanAmount'].fillna(train[train['LoanAmount'].isnull()].apply(values, axis=1))","8ec6c126":"#filling based on the most frequent\ntrain['Loan_Amount_Term'] = train['Loan_Amount_Term'].fillna(train['Loan_Amount_Term'].mode()[0])","ff1bbb39":"from sklearn.preprocessing import LabelBinarizer\nencoder = LabelBinarizer()\ntrain['Loan_Status'] = encoder.fit_transform(train['Loan_Status'])","70dea5f7":"train[train['Credit_History'].isnull()].head()","3aa9df40":"#filling credit_history where loan status was approved\ntrain['Credit_History'] = np.where(((train['Credit_History'].isnull()) & (train['Loan_Status'] ==1)),\n                                   1,train['Credit_History'])\n\n#filling credit_history based on where loan status was declined\ntrain['Credit_History'] = np.where(((train['Credit_History'].isnull()) & (train['Loan_Status'] ==0)),\n                                   0,train['Credit_History'])","f029681c":"#Log transfromations\ntrain['LoanAmount'] = np.log1p(train['LoanAmount'])","a61f484d":"#Log transforming features\ntrain['ApplicantIncome'] = np.log1p(train['ApplicantIncome'])\ntrain['CoapplicantIncome'] = np.log1p(train['CoapplicantIncome'])","6fdce15b":"#coapplicant income and applicant income both serves as determinants for loan status\n#log transformation\n\ntrain['total_income'] = train['ApplicantIncome'] + train['CoapplicantIncome']\ntrain['total_income'] = np.log1p(train['total_income'])","3451b249":"#Log transformation \ntrain['Ratio of LoanAmt :Total_Income'] = train['LoanAmount'] \/ train['total_income']\ntrain['Ratio of LoanAmt :Total_Income'] = np.log1p(train['Ratio of LoanAmt :Total_Income'])","6ed727bf":"#checking the categorical variables in the dataset\ntrain.select_dtypes(['object']).columns","7cf1e0b4":"#One hot encoding variables\nDummies = pd.get_dummies(train[['Gender','Married', 'Dependents', 'Education', 'Self_Employed', 'Property_Area']],drop_first=True)\n\n#Dropping the columns which got one hot-encoded\ntrain = train.drop(['Gender','Married', 'Dependents', 'Education', 'Self_Employed', 'Property_Area'],axis=1)\n\n#Combining the one-hot encoded variables into the actual dataset to make it as one\ntrain = pd.concat([train,Dummies],axis=1)","a40e599f":"#Viewing the combined dataset with one-hot enocoded variables\ntrain.head()","723f65eb":"#Defining the variables X and y Where; \n\n#X are the features for training \nX = train.drop('Loan_Status',axis=1)\n\n#y is the target(Loan_Status) to be predicted\ny = train['Loan_Status']","2b7a708e":"#Splitting the train data into train and test purposes.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)","c8a0408d":"#Scaling features. \nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","f72e6e60":"from sklearn.metrics import classification_report,confusion_matrix","16117a99":"from sklearn.model_selection import RandomizedSearchCV,GridSearchCV\nfrom sklearn.metrics import accuracy_score","353bda50":"from sklearn.linear_model import LogisticRegression","9864eff8":"model_LR = LogisticRegression()\nmodel_LR.fit(X_train,y_train)","3942afbc":"pred_LR = model_LR.predict(X_test)\nprint(classification_report(y_test,pred_LR))\nprint('\\n')\nprint(confusion_matrix(y_test,pred_LR))","c17f7a7b":"print(accuracy_score(pred_LR,y_test))","3497528d":"model_LR2 = LogisticRegression()\ntuned_parameters = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] ,\n              'penalty':['l1','l2']\n                   }\n\nLR = GridSearchCV(estimator=model_LR2,\n                  param_grid=tuned_parameters,\n                  cv=10,\n                 scoring='accuracy',n_jobs=-1)\n\nLR.fit(X_train,y_train)","9f048a3a":"LR.best_estimator_","62297e57":"pred_LR2 = LR.predict(X_test)\nprint(classification_report(y_test,pred_LR2))\nprint('\\n')\nprint(confusion_matrix(y_test,pred_LR2))","8d934cf3":"print(accuracy_score(pred_LR2,y_test))","ecfa0c39":"import xgboost","852542b1":"params = {\n    'learning_rate'   : [0.05,0.3,0.10,0.15,0.20],\n    'max_depth'       : [3,4,5,6,8,10],\n    'gamma'           : [0.0,0.1,0.2,0.3,0.4],\n    'n_estimators'    : range(100,1000,100),\n    'colsample_bytree': [0.3,0.4,0.5,0.7]\n}\n\n\nmodel_xg2 = xgboost.XGBClassifier()\nxgb_rand_cv = RandomizedSearchCV(estimator=model_xg2,\n                             param_distributions=params,n_iter=5,\n                            scoring='accuracy',cv=5,n_jobs=-1)\n\nxgb_rand_cv.fit(X_train,y_train)","ebe282d4":"pred_xgb = xgb_rand_cv.predict(X_test)\nprint(classification_report(y_test,pred_xgb))\nprint('\\n')\nprint(confusion_matrix(y_test,pred_xgb))","d9f27193":"print(accuracy_score(pred_xgb,y_test))","dac722d8":"from sklearn.svm import SVC\nmodel_svc = SVC()\nmodel_svc.fit(X_train,y_train)","9949a1c7":"pred_svc = model_svc.predict(X_test)\nprint(classification_report(y_test,pred_svc))\nprint('\\n')\nprint(confusion_matrix(y_test,pred_svc))","162fd660":"print(accuracy_score(pred_svc,y_test))","d3ac25eb":"# Applying Grid Search to find the best model and the best parameters\nmodel_svc2 = SVC()\nparameters = [{'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n              {'C': [1, 10, 100, 1000], 'kernel': ['rbf'], 'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}]\nsvm = RandomizedSearchCV(estimator = model_svc2,\n                         param_distributions=parameters,\n                           scoring = 'accuracy',\n                           cv = 5,\n                           n_jobs = -1)\nsvm.fit(X_train, y_train)","24e4c190":"svm.best_estimator_","a57eafb0":"pred_svc2 = svm.predict(X_test)\nprint(classification_report(y_test,pred_svc2))\nprint('\\n')\nprint(confusion_matrix(y_test,pred_svc2))","56b174c3":"print(accuracy_score(pred_svc2,y_test))","099b1295":"from sklearn.ensemble import RandomForestClassifier\nmodel_RR = RandomForestClassifier()\nmodel_RR.fit(X_train,y_train)","b07f5e09":"pred_rr = model_RR.predict(X_test)\nprint(classification_report(y_test,pred_rr))\nprint('\\n')\nprint(confusion_matrix(y_test,pred_rr))","ec8a4ca1":"print(accuracy_score(pred_rr,y_test))","9744f495":"model_RR2 = RandomForestClassifier()\n\ntuned_parameters = {'min_samples_leaf': range(2,100,10), \n                    'n_estimators' : range(100,550,50),\n                    'max_features':['auto','sqrt','log2'],\n                    'max_depth' : range(0,100,10)\n                    }\n\nrr = RandomizedSearchCV(estimator = model_RR2,\n                        param_distributions= tuned_parameters,\n                           scoring = 'accuracy',\n                           cv = 5,\n                           n_jobs = -1)\n\nrr.fit(X_train,y_train)","00a1bf90":"rr.best_params_","4968926f":"pred_rr2 = rr.predict(X_test)\nprint(classification_report(y_test,pred_rr2))\nprint('\\n')\nprint(confusion_matrix(y_test,pred_rr2))","68f113cc":"print(accuracy_score(y_test,pred_rr2))","bf90a4cd":"X = train.drop('Loan_Status', axis = 1).values\ny = train['Loan_Status'].values","c5ac551a":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)","e634632c":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","3bf7c333":"X_train.shape","ddcd77d6":"y_train.shape","4a2f0629":"#Importing libraries for neural network\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation,Dropout\nfrom tensorflow.keras.constraints import max_norm","11eb493e":"#Creating a neural network\nmodel = Sequential()\n\n# input layer\nmodel.add(Dense(40, activation='relu'))\nmodel.add(Dropout(0.2))\n\n# hidden layer\nmodel.add(Dense(20, activation='relu'))\nmodel.add(Dropout(0.2))\n\n# hidden layer\nmodel.add(Dense(10, activation='relu'))\nmodel.add(Dropout(0.2))\n\n# hidden layer\nmodel.add(Dense(5, activation='relu'))\nmodel.add(Dropout(0.2))\n\n# output layer\nmodel.add(Dense(units=1,activation='sigmoid'))\n\n# Compile model\nmodel.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])","c6d875ea":"from tensorflow.keras.callbacks import EarlyStopping\n\nearly_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=25)","c15d1891":"model.fit(x=X_train, \n          y=y_train, \n          epochs=600,\n          validation_data=(X_test, y_test), verbose=1,\n          callbacks=[early_stop]\n          )","37ee3848":"metrics = pd.DataFrame(model.history.history)\nmetrics[['loss','val_loss']].plot()","63804a18":"metrics[['accuracy','val_accuracy']].plot()","25c6b326":"pred_ANN = model.predict_classes(X_test)","fa59fb93":"from sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(y_test,pred_ANN))\nprint('\\n')\nprint(confusion_matrix(y_test,pred_ANN))","882c9947":"print(accuracy_score(pred_ANN,y_test))","983f7880":"#Training all features and target column on train dataset for final use\nX = train.drop('Loan_Status',axis=1)  #------> features\ny = train['Loan_Status'] #-------> target(loan_status prediction)","a51c96c3":"#scaling train features\nfull_scaler = StandardScaler()\nX = full_scaler.fit_transform(X)","2fbc25ef":"#shape of train (features and target)\nX.shape,y.shape","e4cf2493":"LR.fit(X,y)","7febcaf0":"#Checking first 5 rows of the test data\ntest.head()","d59c19e8":"#Checking for null values\ntest.isnull().sum()","7a2e80a3":"#filling null_values\ntest['Gender'] = test['Gender'].fillna(test['Gender'].mode()[0])\ntest['Dependents'] = test['Dependents'].fillna(test['Dependents'].mode()[0])\ntest['Self_Employed'] = test['Self_Employed'].fillna(test['Self_Employed'].mode()[0])","a9fb9b7c":"#fiiling loan amt based on the defined function used on train\ntest['LoanAmount'] = test['LoanAmount'].fillna(test[test['LoanAmount'].isnull()].apply(values, axis=1))","2f1c8ae4":"#filling based on the most frequent\ntest['Loan_Amount_Term'] = test['Loan_Amount_Term'].fillna(test['Loan_Amount_Term'].mode()[0])","28ffb22d":"test['Credit_History'].value_counts()\n#filling based on the most frequent\ntest['Credit_History'] = test['Credit_History'].fillna(test['Credit_History'].mode()[0])","70f750db":"#Log_transformations\ntest['ApplicantIncome'] = np.log1p(test['ApplicantIncome'])\ntest['CoapplicantIncome'] = np.log1p(test['CoapplicantIncome'])\ntest['LoanAmount'] = np.log1p(test['LoanAmount'])\ntest['total_income'] = test['ApplicantIncome'] + test['CoapplicantIncome']\ntest['total_income'] = np.log1p(test['total_income'])\ntest['Ratio of LoanAmt:Total_income'] = test['LoanAmount']\/test['total_income']\ntest['Ratio of LoanAmt:Total_income'] = np.log1p(test['Ratio of LoanAmt:Total_income'])","ac6d4d6e":"test.select_dtypes(['object']).columns","b3af6a41":"#Save loanID\ntest_ID= test['Loan_ID']\ntest = test.drop('Loan_ID',axis=1)","7f17f677":"dummies = pd.get_dummies(test[['Gender','Married', 'Dependents', 'Education', 'Self_Employed', 'Property_Area']],drop_first=True)\ntest = test.drop(['Gender','Married', 'Dependents', 'Education', 'Self_Employed', 'Property_Area'],axis=1)\ntest = pd.concat([test,dummies],axis=1)","067eda22":"#scaling test features\nscaled_test = full_scaler.transform(test)","3d302e2b":"result = LR.predict(scaled_test)","4b88cbde":"#reassigning target names Y and N\nresult = np.where(result ==1, 'Y', 'N')\nresult = pd.Series(result,name='Loan_Status')","93fbf83b":"test_predictions = pd.concat([test_ID,result],axis=1)\ntest_predictions['Loan_Status'].value_counts()","df13c51d":"test_predictions['Loan_Status'].value_counts()","626c17f1":"test_predictions.to_csv('submission_1.csv')","0ac2cb23":"**Tuned hyperparameters**","9adf0c98":"### Test Data\n\nPreparing the actual test data for predictions.","bb8640bb":"**Correlations**\n\nWe can calculate the Pearson correlation coefficient between every variable and the target using the .corr dataframe method.\n\nThe correlation coefficient gives us an idea of possible relationships within the data. Some general interpretations of the absolute value of the correlation coefficent are:\n\n1. .00 -.19 \u201cvery weak\u201d\n2. .20 -.39 \u201cweak\u201d\n3. .40 -.59 \u201cmoderate\u201d\n4. .60 -.79 \u201cstrong\u201d\n5. .80 -1.0 \u201cvery strong\u201d","4b794815":"### Artificial Neural Networks\n\nPrediction accuracy  88.7 %","784a0045":"**Default Parameters**","9901212d":"The graphical representation shows that the loan amount is skewed to the right in the histogram and with outliers showed clearly in the box plot. This would be tackled later on in this analysis","38825392":"#### Examining depedants","7c4e4468":"## Categorical Variables and Dummy Variables\n\nMachine learning algorithims cannot take in words\/categorical variables. Therefore, they need to be encoded(represented) in the form of numbers before the models can handle them. A process called One-Hot encoding is used and this creates categorical variables into 0's and 1's giving a unique representation of each as and when they occur in the dataset.\n\nA Pandas function called 'pd.get_dummies' is used in this scenario to one hot encode (represent) these categorical variables.","6f991202":"### Logistic Regression \n\nPrediction accuracy  88.7%","d44d4f3a":"## Conclusion\n\nSeems the Logistic Regression, ANN,Random Forest,Support vector machine classifiers are all achieving an **88.7 % prediction accuracy**.\n\nLets settle on using the logistic regression model.","3157fc22":"### Support vector machine classifier \n\nPrediction accuracy  88.7 %","3890c74a":"# About Company\nDream Housing Finance company deals in all home loans. They have presence across all urban, semi urban and rural areas. Customers first apply for home loans and after that, the company validates the customer eligibility for loan.\n\n## Problem\nCompany wants to automate the loan eligibility process (real time) based on customers detail provided while filling the online application form. These details are Gender, Marital Status, Education, Number of Dependents, Income, Loan Amount, Credit History and others. To automate this process, they have given a problem to identify the customers segments for those are eligible for loan amount so that they can specifically target these customers. Here they have provided a partial data set.","043bf296":"**Default parameters**","587657e6":"From the observations above, credit history is a high determinant for loan approval (correlation of +0.5) and for that reason, null values for credit history would be filled based on where loan status was approved or declined.\n\n#### Convert Loan_Status to numerical\nwhere Y = 1\nwhere N = 0","ab1e89b3":"Married couples have a slight possibility of having their loans approved.","5c8a17de":"**Further analysis**\n\nAnalysing credit history and its impact on loan_status and collectively, the impact of credit history, education,and employment.\n\n\n**Conclusion**\n\nFrom this analysis, the barplot shows that the two major factors contributing to loan status approval or decline are **credit_history** and being a **graduate**. Where **credit history is the main contributing factor here**.","ddcead58":"Zero here indicated that the applicants didnt have coapplicants for the loan application","0cbd3332":"**Parameter Tuuning**","cad5aa70":"**Further Analysis**\n\nLets see if loans are fairly approved among males and females\n\n**Conclusion**\n\nMost of the applicants are males with a number of 489 whiles females are 112. \nThe distribution shows that females are rarely approved loans if there is no credit history and about 17.8% of females with credit history are denied loans.","5717ebe8":"From the observation some customers are likely to apply for larger loans and this is the reason for the loan amount being rightly skewed. Lets log the loan amount","55cdba35":"### XGBOOST","7cca7272":"The histograms below for the applicant income and coapplicant income respectively, show a distribution of income skewed to the right indicating that majority of applicants apply for loans from 2000 - 7000 with a few applying for loans above 20,000 whiles majority of coapplicant incomes range from 0 - 2000 whiles a few ranging from 5000 and above.","10a00de4":"**Further analysis**\n\nLets analyse further to see if being a graduate with credit history has an influnence of loan status.\n\n**Conclusion**\n\nThe barchart shows that graduates are most likely to have their loans approved if they have a credit history.","ae501d40":"1. Loan amount is skewed to the right. This is because there are a few customers applying for large loans whiles the majority are applying for loans within the median (60,000 - 200,000). This resulted in the outliers seen in the boxplot\n\n2. Loans are mostly approved for property areas in the semiurban areas followed by urban areas where majority are graduates.\n\n3. Credit history is a high determinant for the status of loan approval. There is a positive correlation of 0.5\n\n4. Married couples have a slight possibility of having their loans approved\n\n5. There is also a slight correlation with coapplicant income and the loan amount.\n\n6. There are 7 columns in the training dataset with missing values that need to be filled. The machine learning algorthim cannot process data that has missing values.\n","3075debd":"There are more with no dependants","884fcdd4":"### Random Forest Classifier\n\nDefault parameters (Prediction accuracy  87 %)\n\nAdjusted Parameters (Prediction accuracy 88.7%)","c963c6ca":"The scatterplot shows that most people without coapplicant incomes had their loan status declined.","e8a422a4":"### COLUMN DESCRIPTION OF DATASET\n\n**Loan_ID**         : Unique Loan ID\n\n**Gender**\t        : Male\/ Female\n\n**Married**         : Applicant married (Y\/N)\n\n**Dependents**      : Number of dependents\n\n**Education**       :Applicant Education (Graduate\/ Under Graduate)\n\n**Self_Employed**   :Self employed (Y\/N)\n\n**ApplicantIncome** :Applicant income\n\n**CoapplicantIncome**:Coapplicant income\n\n**LoanAmount**\t     :Loan amount in thousands\n\n**Loan_Amount_Term** :Term of loan in months\n\n**Credit_History**   :Credit history meets guidelines\n\n**Property_Area**    :Urban\/ Semi Urban\/ Rural\n\n**Loan_Status**      :Loan approved (Y\/N)\n\n","633aa8a9":"**Default parameters**","d68a102e":"The barplot below shows that the loan status are approved mostly for **Graduates**. Those under the 'Semiurban' property have a higher probabilty of getting their loans approved if they are graduates.","c2eb8376":"## Models for predictions\n\n1. Logistic Regression\n\n2. XGBOOST\n\n3. Support Vector Machines Classifier\n\n4. Random Forest Classifiers\n\n5. Neural Networks (Simple Artificial Neural Network)\n","f4181dff":"**Default parameters**","10ec91e5":"#### Filling out missing values based on observations","102082a2":"### DATA PREPROCESSING AND VISUALISATION","1c1e144d":"### Final results ","c7ad1d08":"#### Feauture scaling\n\nThis is done to normalise or standardise the features of a dataset. Since each column has data in different magnitudes or units of measurement, if the data is not scaled, the machine learning algorithm would consider some features as higher than the others. \nExample: 1000km and 6kgs are different units of different magnitudes, if these are not scaled, it could output wrong predictions.","1bf7923d":"## Objective\nThe objective is to use historical loan application data to predict whether or not an applicant will be eligible for a loan or not based on named features. This is a standard supervised classification task:\n\n1. Supervised: The labels are included in the training data and the goal is to train a model to learn to predict the labels from the features\n\n\n\n\n2. Classification: The label is a binary variable, **Y** (Eligible), **N** (Not Eligible)","a3e54584":"The target column is distributed between 69% and 31% for YES and NO respectively. The data seems to be fairely balanced","84a00d43":"**Adjusted hyperparameter**","d4cd8b44":"The countplot below shows most loans are approved for people in the **Semiurban areas** with a count of 179 for Y. The less approved being the rural property areas, have a count of 110 for Y.","cf6ac579":"#### Creating Features","0c7068bc":"## Train\/Test Split\n\nThe train dataset would be split into two. One for training and the other for testing the model to see how well it will perform against the true lables. This enables the data scientist to see the actual performance of the model on data it hasnt seen before(test data). When satisfied with the performance, the model can then be deployed for use on the actual test dataset provided for predictions.\n\nIn this scenario, the train data would be split into a 90% (for training) and the 10% for testing to measure performance.","d4d53aaa":"**Tuned hyper-parameters**","8de28982":"## Observations"}}