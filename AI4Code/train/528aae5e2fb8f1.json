{"cell_type":{"7286fc22":"code","28a54309":"code","128f1f35":"code","3db42d11":"code","2245fbc1":"code","62e81272":"code","13b4b70e":"code","bfb2d202":"code","895d2ec0":"code","e6ff7fa2":"code","f6f846a5":"code","75174b05":"code","99597741":"code","68d60c7e":"code","567f5ac6":"code","599237de":"code","4a5ad831":"code","54d08f01":"code","9cc2c639":"code","a92b9c3f":"code","535012f2":"code","5ca00ab8":"code","7149a09a":"code","93bd69c4":"code","b69d8de0":"code","7bb5ed14":"code","e1a7251f":"code","0055537a":"code","1db67404":"code","d637db8b":"code","1413e0f3":"code","021799f2":"code","579a0213":"code","c78179d5":"code","491f2cb3":"code","19616f96":"code","ceb6b393":"markdown","395b2c68":"markdown","85c01b1f":"markdown","d34c85bc":"markdown","0bb481d7":"markdown","e6d03936":"markdown","51727ad9":"markdown","64abfe8e":"markdown","11824790":"markdown","2395d52d":"markdown","da508ea0":"markdown","1b0ca793":"markdown"},"source":{"7286fc22":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n#import os\n#print(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","28a54309":"import seaborn as sns \nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold,train_test_split,cross_val_predict,GridSearchCV,cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import preprocessing\nfrom scipy.stats import skew  # for some statistics\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax","128f1f35":"Train_DF = pd.read_csv(\"..\/input\/train.csv\")\nTest_DF = pd.read_csv(\"..\/input\/test.csv\")","3db42d11":"Train_DF.drop(['Id'],axis=1,inplace=True)\nTest_DF.drop(['Id'],axis=1,inplace=True)\nTrain_DF.describe()","2245fbc1":"_Analysis_trainDF = Train_DF\n_Analysis_trainDF.head()","62e81272":"#Funtion to identify columns which contains NaN values & those which are having more than 80% NaN values\n_EmptyColList = list()\n_EmptyColwithNAN = list()\n_Maxcount = len(_Analysis_trainDF.iloc[:,0])\nfor column in _Analysis_trainDF.columns:\n    if (_Analysis_trainDF[column].isna().any()):\n        _EmptyColList.append(column)\n        _TempVar = len(_Analysis_trainDF[_Analysis_trainDF[column].isna()])\n        count = (_TempVar\/_Maxcount)*100\n        if count > 80:\n            _EmptyColwithNAN.append(column)","13b4b70e":"_NullColums = _Analysis_trainDF[_EmptyColList].isna().sum()\n_NullColums.sort_values().plot.bar()","bfb2d202":"Train_DF.drop(_EmptyColwithNAN,axis=1,inplace=True)\nTrain_DF.reset_index(drop=True, inplace=True)\nTrain_DF[\"SalePrice\"] = np.log1p(Train_DF[\"SalePrice\"])","895d2ec0":"_Analysis_trainDF.plot.scatter(x='OverallQual',y='SalePrice'),_Analysis_trainDF.plot.scatter(x='GrLivArea',y='SalePrice')\n_Analysis_trainDF.plot.scatter(x='GarageCars',y='SalePrice'),_Analysis_trainDF.plot.scatter(x='GarageArea',y='SalePrice')\n_Analysis_trainDF.plot.scatter(x='TotalBsmtSF',y='SalePrice'),_Analysis_trainDF.plot.scatter(x='1stFlrSF',y='SalePrice')\n_Analysis_trainDF.plot.scatter(x='FullBath',y='SalePrice'),_Analysis_trainDF.plot.scatter(x='TotRmsAbvGrd',y='SalePrice')\n_Analysis_trainDF.plot.scatter(x='YearBuilt',y='SalePrice'),_Analysis_trainDF.plot.scatter(x='GarageYrBlt',y='SalePrice'),","e6ff7fa2":"# Removing outliers\nTrain_DF = Train_DF[(Train_DF['GrLivArea'] < 4500) & (Train_DF['LotArea'] < 50000) & (Train_DF['GarageArea'] < 1220) & (Train_DF['TotalBsmtSF'] < 3000) & (Train_DF['1stFlrSF'] < 4000) & (Train_DF['TotRmsAbvGrd'] < 13) & (Train_DF['YearBuilt'] > 1895)]  \nTrain_DF.reset_index(drop=True, inplace=True)\nTrain_DF.shape","f6f846a5":"y = Train_DF['SalePrice'].reset_index(drop=True)\ny.shape","75174b05":"Train_Features = Train_DF.drop(['SalePrice'],axis = 1)\nTest_Features = Test_DF\nCombined_FeaturesDF = pd.concat([Train_Features, Test_Features]).reset_index(drop=True)\nTrain_Features.shape, Test_Features.shape, Combined_FeaturesDF.shape","99597741":"# 2nd floor surface area should be 0 incase of 1 story building\nCombined_FeaturesDF[(Combined_FeaturesDF['HouseStyle'] == '1Story') & (Combined_FeaturesDF['2ndFlrSF'] > 0)]['2ndFlrSF'] = 0\n\n# filling below features with most frequent Attribure\/values\nCombined_FeaturesDF['Electrical'].fillna(Combined_FeaturesDF['Electrical'].mode()[0],inplace=True)\nCombined_FeaturesDF['Exterior1st'].fillna(Combined_FeaturesDF['Exterior1st'].mode()[0],inplace=True)\nCombined_FeaturesDF['Exterior2nd'].fillna(Combined_FeaturesDF['Exterior2nd'].mode()[0],inplace=True)\nCombined_FeaturesDF['Functional'].fillna(Combined_FeaturesDF['Functional'].mode()[0],inplace=True)\nCombined_FeaturesDF['KitchenQual'].fillna(Combined_FeaturesDF['KitchenQual'].mode()[0],inplace=True)\nCombined_FeaturesDF['MasVnrType'].fillna(Combined_FeaturesDF['MasVnrType'].mode()[0],inplace=True)\nCombined_FeaturesDF['SaleType'].fillna(Combined_FeaturesDF['SaleType'].mode()[0],inplace=True)\nCombined_FeaturesDF['Utilities'].fillna(Combined_FeaturesDF['Utilities'].mode()[0],inplace=True)\nCombined_FeaturesDF['BsmtExposure'].fillna(Combined_FeaturesDF['BsmtExposure'].mode()[0],inplace=True)\nCombined_FeaturesDF['FireplaceQu'].fillna(0,inplace=True)\n\n#Idea is that similar MSSubClasses will have similar MSZoning\nCombined_FeaturesDF['MSZoning'] = Combined_FeaturesDF.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n\n# For those which are having no Bsmt area we can fill it with None.\nfor col in ('BsmtCond','BsmtFinType1','BsmtFinType2','BsmtQual'):\n    Combined_FeaturesDF[col] = Combined_FeaturesDF[col].fillna('None')\n# For those which are having no garage area we can fill it with None then we will encode it with some value\nfor col in ('GarageCond','GarageFinish','GarageQual','GarageType'):\n    Combined_FeaturesDF[col] = Combined_FeaturesDF[col].fillna('None')","68d60c7e":"Combined_FeaturesDF['LotFrontage'] = Combined_FeaturesDF.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\n#below we are setting numeric features having Nan to 0\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics = []\nfor i in Combined_FeaturesDF.columns:\n    if Combined_FeaturesDF[i].dtype in numeric_dtypes:\n        numerics.append(i)\nCombined_FeaturesDF.update(Combined_FeaturesDF[numerics].fillna(0))\nnumerics[1:100]","567f5ac6":"# Adding new features.\nCombined_FeaturesDF['YrBltAndRemod']=Combined_FeaturesDF['YearBuilt']+Combined_FeaturesDF['YearRemodAdd']\nCombined_FeaturesDF['TotalSF']=Combined_FeaturesDF['TotalBsmtSF'] + Combined_FeaturesDF['1stFlrSF'] + Combined_FeaturesDF['2ndFlrSF']\n\nCombined_FeaturesDF['Total_sqr_footage'] = (Combined_FeaturesDF['BsmtFinSF1'] + Combined_FeaturesDF['BsmtFinSF2'] +\n                                 Combined_FeaturesDF['1stFlrSF'] + Combined_FeaturesDF['2ndFlrSF'])\n\nCombined_FeaturesDF['Total_Bathrooms'] = (Combined_FeaturesDF['FullBath'] + (0.5 * Combined_FeaturesDF['HalfBath']) +\n                               Combined_FeaturesDF['BsmtFullBath'] + (0.5 * Combined_FeaturesDF['BsmtHalfBath']))\n\nCombined_FeaturesDF['Total_porch_sf'] = (Combined_FeaturesDF['OpenPorchSF'] + Combined_FeaturesDF['3SsnPorch'] +\n                              Combined_FeaturesDF['EnclosedPorch'] + Combined_FeaturesDF['ScreenPorch'] +\n                              Combined_FeaturesDF['WoodDeckSF'])","599237de":"#Adding few more features.\nCombined_FeaturesDF['haspool'] = Combined_FeaturesDF['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nCombined_FeaturesDF['has2ndfloor'] = Combined_FeaturesDF['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nCombined_FeaturesDF['hasgarage'] = Combined_FeaturesDF['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nCombined_FeaturesDF['hasbsmt'] = Combined_FeaturesDF['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nCombined_FeaturesDF['hasfireplace'] = Combined_FeaturesDF['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","4a5ad831":" ### Fill the remaining columns as **None**\nobjects = []\nfor i in Combined_FeaturesDF.columns:\n    if Combined_FeaturesDF[i].dtype == object:\n        objects.append(i)\nCombined_FeaturesDF.update(Combined_FeaturesDF[objects].fillna('None'))\nprint(objects)","54d08f01":"#Removing Skewness of numeric columns in dataset\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics2 = []\nfor i in Combined_FeaturesDF.columns:\n    if Combined_FeaturesDF[i].dtype in numeric_dtypes:\n        numerics2.append(i)\nskew_features = Combined_FeaturesDF[numerics2].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\n\nfor i in skew_index:\n    Combined_FeaturesDF[i] = boxcox1p(Combined_FeaturesDF[i], boxcox_normmax(Combined_FeaturesDF[i] + 1))","9cc2c639":"# We are putting categorical features in list.\n\ncols = Combined_FeaturesDF.columns\nnum_cols = Combined_FeaturesDF._get_numeric_data().columns\nnum_cols\n\n_ListOf_Cat_column = list(set(cols) - set(num_cols))\n_ListOf_Cat_column\n","a92b9c3f":"#Comment out for now , let see what is the when we are trying using get_dummies\n'''le = preprocessing.LabelEncoder()\n# apply le on categorical feature columns\nCombined_FeaturesDF[_ListOf_Cat_column] = Combined_FeaturesDF[_ListOf_Cat_column].apply(lambda col: le.fit_transform(col))\nCombined_FeaturesDF.head(10)'''\n\n# One Hot Encoding for categorical features\nfinal_features = pd.get_dummies(Combined_FeaturesDF).reset_index(drop=True)\nfinal_features.shape","535012f2":"final_features.head()","5ca00ab8":"C_mat = _Analysis_trainDF.corr()\nfig = plt.figure(figsize = (15,15))\n\nsns.heatmap(C_mat, vmax = .8, square = True, annot=True)\nplt.show()\nC_mat.OverallQual.sort_values(ascending=False)","7149a09a":"C_mat.SalePrice.sort_values(ascending=False),C_mat.SalePrice.sort_values(ascending=False).plot.hist()","93bd69c4":"# creating data for fitting model\nX = final_features.iloc[:len(y), :]\nX_sub = final_features.iloc[len(y):, :]\nX.shape, y.shape, X_sub.shape","b69d8de0":"import xgboost as xgb\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nimport matplotlib.pyplot as pyplot\nfrom mlxtend.regressor import StackingCVRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.svm import SVR","7bb5ed14":"# defining error functions for handy use. \nkfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model, T=X):\n    rmse = np.sqrt(-cross_val_score(model, T, y, scoring=\"neg_mean_squared_error\", cv=kfolds))\n    return (rmse)","e1a7251f":"alphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]","0055537a":"ridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alphas_alt, cv=kfolds))\nlasso = make_pipeline(RobustScaler(), LassoCV(max_iter=1e7, alphas=alphas2, random_state=42, cv=kfolds))\nelasticnet = make_pipeline(RobustScaler(), ElasticNetCV(max_iter=1e7, alphas=e_alphas, cv=kfolds, l1_ratio=e_l1ratio))                                ","1db67404":"gbr = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10, loss='huber', random_state =42)                             ","d637db8b":"lightgbm = LGBMRegressor(objective='regression', \n                                       num_leaves=4,\n                                       learning_rate=0.01, \n                                       n_estimators=5000,\n                                       max_bin=200, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.2,\n                                       feature_fraction_seed=7,\n                                       verbose=-1,\n                                       )","1413e0f3":"xgboost = XGBRegressor(learning_rate=0.01,n_estimators=3000,\n                                     max_depth=3, min_child_weight=0,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     objective='reg:linear', nthread=-1,\n                                     scale_pos_weight=1, seed=27,\n                                     reg_alpha=0.00006)","021799f2":"score = cv_rmse(ridge)\nprint(score)\nscore = cv_rmse(lasso)\nprint(score)\nscore = cv_rmse(elasticnet)\nprint(score)\nscore = cv_rmse(gbr)\nprint(score)\nscore = cv_rmse(lightgbm)\nprint(score)\nscore = cv_rmse(xgboost)\nprint(score)","579a0213":"#In order to avoid overfitting. we are using StackingCVRegressor\nstack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, gbr, xgboost, lightgbm),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)","c78179d5":"#Start Fitting\nprint('Start fitting models')\n#stack_gen.fit(X_train,y_train)\nprint('elasticnet')\nelastic_model_full_data = elasticnet.fit(X, y)\n\nprint('Lasso')\nlasso_model_full_data = lasso.fit(X, y)\n\nprint('Ridge')\nridge_model_full_data = ridge.fit(X, y)\n\nstack_gen_model = stack_gen.fit(np.array(X), np.array(y))\nprint('stack_gen_model',stack_gen_model)\n\nGradiantBoosting_model = gbr.fit(X, y)\nprint('GradiantBoosting_model',GradiantBoosting_model)\n\nXGBoosting_model = xgboost.fit(X, y)\nprint('XGBoosting_model',XGBoosting_model)\n\nLightGBM_model = lightgbm.fit(X, y)\nprint('LightGBM_model',LightGBM_model)","491f2cb3":"# Assigning different weight of models in order to create final one depending on scores of each model\ndef combine_models_predict(X):\n    return ((0.15 * GradiantBoosting_model.predict(X)) + \\\n            (0.2 * XGBoosting_model.predict(X)) + \\\n            (0.1 * ridge.predict(X)) + \\\n            (0.05 * lasso.predict(X)) + \\\n            (0.1 * elasticnet.predict(X)) + \\\n            (0.1 * LightGBM_model.predict(X)) + \\\n            (0.3 * stack_gen_model.predict(np.array(X))))","19616f96":"print('Predict submission')\nsubmission = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsubmission['SalePrice'] = (np.expm1(combine_models_predict(X_sub)))\nsubmission.to_csv(\"sample_submission.csv\",index=False)","ceb6b393":"Below we can see Heatmap in order to see correlation of features with respect to target variable","395b2c68":"Below we can plot graphs in order to find out outliers","85c01b1f":"* We can see above Nan Value count in Train Data. In few features we can see that more than 80% of Data is having Nan Values.\nFor instance Fence,Alley,MiscFeature & PoolQC.\n* We can remove these features as part of data cleanup\/Feature engineering.","d34c85bc":"StackingCVRegressor- combining multiple models into a single one.\nhttp:\/\/www.programmersought.com\/article\/5645254916\/","0bb481d7":"Below we are doing normalization on skewed numerical features. This has a very good impact on performance of model","e6d03936":"1 **Importing below Libraries **","51727ad9":"As Shown in Above Graph we can see few columns are having more than 80% Nan values\nWe can remove features which are containing more than 80% NaN Values from Dataframes.","64abfe8e":"Below we are creating a seprate Dataframe in order to analyis Train Data, latter on we will try to merger both Train and Test data & perform the required feature enginieering.","11824790":"## Using Boosting Technique ##","2395d52d":"**Creating Train & Test Data from CSV files**","da508ea0":"**Data Ananlysis**\n* We can see with describe method there are column whose count less than number of rows, it's means they are having null\/Nan Values","1b0ca793":"**Feature Analysis\/Engineering **"}}