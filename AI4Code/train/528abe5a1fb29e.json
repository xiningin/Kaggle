{"cell_type":{"f6cdc03e":"code","a6184e72":"code","04bb422e":"code","b52e4af0":"code","ec875c61":"code","1143dd81":"code","74568965":"code","bd2c1847":"code","13c0c4b1":"code","210c2b9a":"code","ab9e6d56":"code","b3c19e85":"code","ee38342d":"code","d65a20bc":"code","4f03a8a9":"code","c2fa5a32":"code","144e999b":"code","62f1ade1":"code","0a36a4fe":"code","b8d40a70":"code","555a87bb":"code","324cd456":"code","a7383741":"code","8a14a27a":"code","8c314f52":"code","135357be":"code","132ea504":"code","954b835e":"code","96c28eec":"code","11dff188":"code","2ce23a86":"code","85d030ab":"code","6642d7e0":"code","c0f5f0c4":"code","7279ed36":"code","fa180fc3":"code","028c211a":"code","7c0b9887":"code","f690f1cb":"code","598b5c6d":"code","86ef98a5":"code","6a168e1a":"markdown","9dd9428a":"markdown","c813897d":"markdown","9588ed37":"markdown","1973fffd":"markdown","aefee12a":"markdown","0ba27e97":"markdown","097871ac":"markdown","8fd37439":"markdown","5e720c93":"markdown","1bb8fd25":"markdown","8e56fbe5":"markdown","422949cc":"markdown","2b7c7ec7":"markdown","be951951":"markdown"},"source":{"f6cdc03e":"import rkn_module_benford_law as rkn_benford\n\nimport sys\nimport csv\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport matplotlib.image as mpimg","a6184e72":"path = \"..\/input\/inputbenfordcovid19\/time_series_covid19_recovered_global.csv\"\n\ndata = pd.read_csv(path, encoding='ISO-8859-1', delimiter=',' , low_memory=False)\n\ndata_mod = data.copy()\n\ndf = pd.DataFrame(data_mod)","04bb422e":"# df to store the analysis\ndf_analysis = pd.DataFrame(columns=['var_name', 'var_NaN', 'var_not_NaN', \"var_min\", \"var_max\" , \"var_mean\" , 'var_type', 'var_categ'])","b52e4af0":"# Column names in dataSet\ncoluna_name_list = list(df.columns.values)","ec875c61":"# This routine will store on df_analise the number os NaN each variable from dataset has. Alto the variable type and its categories.\nfor i in coluna_name_list:\n    if(df[i].dtypes == \"object\"):\n        lista=[i, df[i].isna().sum(), df[i].count(), \"NA\", \"NA\", \"NA\", df[i].dtypes, \"numerical variable\"]\n        df_length = len(df_analysis)\n        df_analysis.loc[df_length] = lista\n    else:\n        lista=[i, df[i].isna().sum(), df[i].count(), df[i].min(), df[i].max(), df[i].mean(), df[i].dtypes, \"numerical variable\"]\n        df_length = len(df_analysis)\n        df_analysis.loc[df_length] = lista","1143dd81":"# Set var_name as index\ndf_analysis.set_index('var_name', inplace=True)","74568965":"# For each non numerical variable assign its possible values\nfor i in coluna_name_list:\n    if(df_analysis.loc[i, \"var_type\"] == \"object\"):\n        df_analysis.loc[i, \"var_categ\"] = list(df[i].unique())\n    else:\n        pass","bd2c1847":"df_analysis.sort_values(\"var_NaN\", ascending=False)","13c0c4b1":"# Create a new column with country names. If a country has no provinces\/state desaggregation, so it will show the term: Single Unity\ndf[\"Province\/State\"] = df[\"Province\/State\"].replace(np.NaN, \"Single Unity\")\ndf[\"Country_State\"] = df[\"Country\/Region\"] + \"_\" + df[\"Province\/State\"]","210c2b9a":"# Remove unecessary columns\ndel df[\"Province\/State\"]\ndel df[\"Country\/Region\"]\ndel df[\"Lat\"]\ndel df[\"Long\"]","ab9e6d56":"df","b3c19e85":"# Send columns to rows\ndf_melt = df.melt(id_vars=[\"Country_State\"],\n       var_name=\"Date\",\n       value_name=\"Recovered_Accumulated\")","ee38342d":"# Assign date type\ndf_melt['Date'] = pd.to_datetime(df_melt['Date'])","d65a20bc":"# Sort by country and date\ndf_melt = df_melt.sort_values([\"Country_State\", \"Date\"], ascending = (True, True))","4f03a8a9":"# New column to receive the new cases for each day\ndf_melt[\"Recovered_New_Day\"] = 0\n# This function will assign to each country\/day the number of new cases confirmed, based on the difference among accumulated cases of the actual and last day.\ncountry_before = df_melt.iloc[0,0]\n\nfor row in range(1, df_melt.shape[0], 1):\n    country_actual = df_melt.iloc[row,0]\n    if(country_actual == country_before):\n        df_melt.iloc[row,3] = df_melt.iloc[row,2] - df_melt.iloc[row-1,2]\n    else:\n        df_melt.iloc[row,3] = df_melt.iloc[row,2]\n    country_before = country_actual","c2fa5a32":"# Reset index: drop = False\ndf_melt.reset_index(inplace = True, drop = True)","144e999b":"# Create column to store de number of the week\ndf_melt['Date_week'] = pd.DatetimeIndex(df_melt['Date']).week","62f1ade1":"# New dataFrame grouped by country and week. The column value represents the number of new cases in each week per country.\ndf_agg_week = (df_melt.groupby(['Country_State', 'Date_week']).sum()).copy()","0a36a4fe":"# Remove columns && rename columns\ndel df_agg_week[\"Recovered_Accumulated\"]\ndf_agg_week = df_agg_week.rename(columns = {\"Recovered_New_Day\": \"Recovered_New_Week\"})","b8d40a70":"# Set index to columns: drop = False\ndf_agg_week.reset_index(inplace = True, drop = False)","555a87bb":"# IMPORTANT: This is the data that will be used on our analysis. However, will keep on the code to create a more robust database\ndf_week = df_agg_week.copy()\ndel df_week[\"Date_week\"]\ndf_week.to_excel(\"df_week_recovered.xlsx\")","324cd456":"df_week","a7383741":"# Create key-column (Country_week) to join both dataFrames\ndf_melt[\"Country_week\"] = df_melt[\"Country_State\"] + \"-\" + df_melt[\"Date_week\"].astype(str)\ndf_agg_week[\"Country_week\"] = df_agg_week[\"Country_State\"] + \"-\" + df_agg_week[\"Date_week\"].astype(str)","8a14a27a":"# Reorder column\ndf_agg_week = df_agg_week[['Country_week', 'Recovered_New_Week']]\ndf_melt = df_melt[[\"Country_week\", 'Country_State', 'Date', \"Date_week\", \"Recovered_Accumulated\", \"Recovered_New_Day\"]]","8c314f52":"# Create the final dataFrame with number of accumulated cases, daily cases and weekly cases. Key-column: Country_week\ndf_merge = df_melt.merge(df_agg_week, on=\"Country_week\")","135357be":"# Remove key column\ndel df_merge[\"Country_week\"]","132ea504":"# Show final data and send to excell\ndf_merge.to_excel(\"df_merge_recovered.xlsx\")","954b835e":"# Note that we have few weeks of information per country. Insufficient for an analysis ungrouped per country.\ndf_merge","96c28eec":"# Note that it is possible to have a negative new number of cases. Meaning that in such a week the government corrected the numbers informed in the previous week.\ndf_week.describe()","11dff188":"# Some deeper analysis\ndf_analysis_desc = (df_week.groupby(['Country_State']).describe()).copy()\ndf_analysis_desc","2ce23a86":"# Getting hints (1 for aggregated analysis)\nrkn_benford.hints(df_week, 1)","85d030ab":"# df_week: data set with the values to be analyzed\n# 1: Aggregated analysis (Since the sample per country is very small, we are only interested to analyze frequencies of the entire data set as a whole.)\n# 5: Number of rounds we will run the code in order to produced an averaged chi-squared value.\n# 500: Sample size for the first digit analysis.\n# 500: Sample size for the second digit analysis.\n# 184: Sample size for the third digit analysis.\n# 1: Number of graphs to produce with the best chi-sq values.\n# 1: Number of graphs to produce with the worst chi-sq values. Same as the graph before.\ntable_app = rkn_benford.benford(df_week, 1, 5, 500, 500, 184, 1, 1, \"output_recovered.xlsx\", \"\")","6642d7e0":"# Order by city name\ntable_app[0].sort_values(by=['units'], inplace=True)\n# Format table values\nresults_d1 = table_app[0].style.format({\n    'N0': '{:,.2%}'.format, 'N1': '{:,.2%}'.format, 'N2': '{:,.2%}'.format, 'N3': '{:,.2%}'.format, 'N4': '{:,.2%}'.format, 'N5': '{:,.2%}'.format,\n    'N6': '{:,.2%}'.format, 'N7': '{:,.2%}'.format, 'N8': '{:,.2%}'.format, 'N8': '{:,.2%}'.format, 'N9': '{:,.2%}'.format, \n    'chi_sq': '{:,.2f}'.format, 'chi_sq 10 rounds': '{:,.2f}'.format,\n    })","c0f5f0c4":"fig = plt.figure(figsize=(8,6), dpi=250)\n\na = fig.add_subplot(1, 1, 1)\nimgplot = plt.imshow(mpimg.imread('..\/input\/inputbenfordcovid19\/D1__Aggregated_recovered_table.png'))\n\nplt.setp(plt.gcf().get_axes(), xticks=[], yticks=[]);","7279ed36":"fig = plt.figure(figsize=(8,6), dpi=250)\n\na = fig.add_subplot(1, 1, 1)\nimgplot = plt.imshow(mpimg.imread('..\/input\/inputbenfordcovid19\/D1__Aggregated_recovered.png'))\n\nplt.setp(plt.gcf().get_axes(), xticks=[], yticks=[]);","fa180fc3":"# Order by city name\ntable_app[1].sort_values(by=['units'], inplace=True)\n# Format table values\nresults_d2 = table_app[1].style.format({\n    'N0': '{:,.2%}'.format, 'N1': '{:,.2%}'.format, 'N2': '{:,.2%}'.format, 'N3': '{:,.2%}'.format, 'N4': '{:,.2%}'.format, 'N5': '{:,.2%}'.format,\n    'N6': '{:,.2%}'.format, 'N7': '{:,.2%}'.format, 'N8': '{:,.2%}'.format, 'N8': '{:,.2%}'.format, 'N9': '{:,.2%}'.format, \n    'chi_sq': '{:,.2f}'.format, 'chi_sq 10 rounds': '{:,.2f}'.format,\n    })","028c211a":"fig = plt.figure(figsize=(8,6), dpi=250)\n\na = fig.add_subplot(1, 1, 1)\nimgplot = plt.imshow(mpimg.imread('..\/input\/inputbenfordcovid19\/D2__Aggregated_recovered_table.png'))\n\nplt.setp(plt.gcf().get_axes(), xticks=[], yticks=[]);","7c0b9887":"fig = plt.figure(figsize=(8,6), dpi=250)\n\na = fig.add_subplot(1, 1, 1)\nimgplot = plt.imshow(mpimg.imread('..\/input\/inputbenfordcovid19\/D2__Aggregated_recovered.png'))\n\nplt.setp(plt.gcf().get_axes(), xticks=[], yticks=[]);","f690f1cb":"# Order by city name\ntable_app[2].sort_values(by=['units'], inplace=True)\n# Format table values\nresults_d3 = table_app[2].style.format({\n    'N0': '{:,.2%}'.format, 'N1': '{:,.2%}'.format, 'N2': '{:,.2%}'.format, 'N3': '{:,.2%}'.format, 'N4': '{:,.2%}'.format, 'N5': '{:,.2%}'.format,\n    'N6': '{:,.2%}'.format, 'N7': '{:,.2%}'.format, 'N8': '{:,.2%}'.format, 'N8': '{:,.2%}'.format, 'N9': '{:,.2%}'.format, \n    'chi_sq': '{:,.2f}'.format, 'chi_sq 10 rounds': '{:,.2f}'.format,\n    })","598b5c6d":"fig = plt.figure(figsize=(8,6), dpi=250)\n\na = fig.add_subplot(1, 1, 1)\nimgplot = plt.imshow(mpimg.imread('..\/input\/inputbenfordcovid19\/D3__Aggregated_recovered_table.png'))\n\nplt.setp(plt.gcf().get_axes(), xticks=[], yticks=[]);","86ef98a5":"fig = plt.figure(figsize=(8,6), dpi=250)\n\na = fig.add_subplot(1, 1, 1)\nimgplot = plt.imshow(mpimg.imread('..\/input\/inputbenfordcovid19\/D3__Aggregated_recovered.png'))\n\nplt.setp(plt.gcf().get_axes(), xticks=[], yticks=[]);","6a168e1a":"# 4. Creating modified data sets\n\nFor this section we want to aggregate the daily numbers into weekly numbers. This aggregation is important since many cases that happen in one day may only be informed some days later. \n\nWe also gonna concatenate the country name and the country state into a unique column named: Country_State\n\nThen, we will produce to excel files:\n\n* **df_merge_recovered.xlsx**: This file contains the number of new cases per day, per week and the accumulated up to the date. These numbers are shown per country and per day (from 22\/jan\/2020 to 11\/apr\/2020). This file will not be used on the work, but may be useful for researchers.\n\n* **df_week_recovered.xlsx**: This file contains in each row the name of the Country\/State plus the number of confirmed new cases of Covid-19 for each week from 22\/jan\/2020 to 11\/apr\/2020. This is the data that will be used further for the Benford's analysis.","9dd9428a":"# THANKS FOR READING!","c813897d":"## Usefull Data for researchers","9588ed37":"# 5. Application: Recovered cases of Covid-19 per week\n\nFor this analysis we are taking our treated data set with only the number of new cases per week. \n\nThe idea is to select a sample size from this data set and analyze the frequency in which appears numbers from 1 to 9 in the first position of the values selected. Then we compare these frequencies to those predicted by Benford's theory. We will be using a chi-squared test for statistical significance. \n\nSince the sample per country is very small (only 15 weeks of information per country), we will be only interested to analyze frequencies of the entire data set as a whole. Later, in the end of the year when we have more data per country, then we may analyze frequencies per country too.","1973fffd":"## 5.1. Analysis of the data set","aefee12a":"## Data to be used further on our analysis","0ba27e97":"### 5.2.1. First digit results\n\nIn this application we will use a sample size of 500 out of 951 possibles. The chi-squared obtained 18.19 means that we should not reject the hypothesis that this distribution is equal to the one predicted by Benford. In such case there is no evidence of data manipulation.\n\nHowever, our analysis focused only on the entire data. Maybe an analysis per country would reveal that some countries follow and others do not follow Benford's law. Unfortunately there is not enough data to make this investigation at the present moment.","097871ac":"## 5.2 Running the script\n\n* This work uses the following utility script: \"RKN Module - Benford Law\". For more on how to use this script, please access:\nhttps:\/\/www.kaggle.com\/rafaelknunes\/rkn-module-benford-law-tutorial\/notebook","8fd37439":"# 1. Notebook Goals\n\n* 1) Using Benford's Law theory, test whether the numbers of recovered Covid-19 cases informed by governments are experiencing some kind of manipulation.\n* 2) This work uses the following utility script: \"RKN Module - Benford Law\". For more on how to use this script, please access:\nhttps:\/\/www.kaggle.com\/rafaelknunes\/rkn-module-benford-law-tutorial\/notebook","5e720c93":"# 6. Final Remarks\n\nAlong this work we tested the values of new recovered cases of Covid-19 per week informed by governments around the world. We took data from 22\/jan\/2020 to 11\/apr\/2020.\n\nFor the first, second and third digits the conclusion was the same: No evidence of data manipulation. However, we still need to make an specific investigation per country. Nevertheless, this will only be possible after we have more weeks of data, meaning a sufficient sample size.","1bb8fd25":"### 5.2.2. Second digit results\n\nAgain we find a low chi-squared value (6.59) which means that for the second digit the values informed by governments around the world adhere to the Benford's Law.\n\nFurther analysis per country still needed.","8e56fbe5":"# 2. Loading the DataSet","422949cc":"# 3. Analysis on the Original Data Set\n\nIn this section we want to analyze the data for recovered cases took from [Data Repository by Johns Hopkins CSSE](https:\/\/github.com\/CSSEGISandData\/COVID-19). This dataset shows the number of recovered cases of Covid-19 per country and per country's state when possible. The period of analysis goes from 22\/jan\/2020 to 11\/apr\/2020.\n\nThen, let's check for:\n\n* Number of NaN in each variable\n* Variable types\n* Variable categories","2b7c7ec7":"### 5.2.3. Third digit results\n\n\nFinally, we find a very low chi-squared value (12.30) for the third digit. Which means that the values informed by governments around the world adhere to the Benford's Law.\n\nFurther analysis per country still needed.","be951951":"<span style='color:Red ; font-size: 250%'> Benford's Law - A tool to detect data manipulation on recovered cases of Covid-19 (Part 2 of 3) <\/span> \n#### Author: [Rafael Klanfer Nunes](https:\/\/www.linkedin.com\/in\/rafaelknunes\/)\n#### **Date**: 12\/apr\/2020\n#### **Data Source**: [Data Repository by Johns Hopkins CSSE](https:\/\/github.com\/CSSEGISandData\/COVID-19\/tree\/master\/csse_covid_19_data\/csse_covid_19_time_series)\n#### **Disclaimer**: In this first part we will be analyzing the numbers of confirmed cases of Covid-19. Look for parts 1 and 3 for an investigation on the number of deaths and confirmed cases of Covid-19.\n#### **KAGGLE Notebook (Part 1: confirmed)**: https:\/\/www.kaggle.com\/rafaelknunes\/benford-law-to-detect-covid-19-manipulation-1of3\n#### **KAGGLE Notebook (Part 2: recovered)**: https:\/\/www.kaggle.com\/rafaelknunes\/benford-law-to-detect-covid-19-manipulation-2of3\n#### **KAGGLE Notebook (Part 3: deaths)**: https:\/\/www.kaggle.com\/rafaelknunes\/benford-law-to-detect-covid-19-manipulation-3of3"}}