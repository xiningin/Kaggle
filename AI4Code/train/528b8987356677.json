{"cell_type":{"9a065452":"code","a6a1b62f":"code","838fedb5":"code","e0122358":"code","14fbbb83":"code","e0a51ca3":"code","b6d02d50":"code","b46dfd40":"code","0fe83e19":"code","ebaa200d":"code","c03e683e":"code","8f9a7f48":"code","3a60da65":"code","3c5e6036":"code","1b04fc78":"code","499da59b":"code","0f48a6c1":"code","4c2cffc6":"code","205ecf56":"code","473aa20f":"code","ef69dab2":"code","58b839ea":"code","2ee802bc":"code","0f9d0679":"code","c99f1710":"code","cdc7218b":"code","fe6e51bd":"code","e7f34371":"code","73d01929":"code","4be4b5ff":"code","5d64267a":"code","e7550ac8":"code","43462b53":"code","38f0fc02":"code","3f37e1e0":"code","33ae91d4":"code","a23737a6":"code","58f10153":"code","7817e79f":"code","f7ecc29f":"code","320d07b2":"code","16fc28ad":"code","42da155f":"code","755d7451":"code","b6cbecfe":"code","cf42d915":"code","ea7842ee":"code","b05dcce4":"code","a23aa454":"code","a9323838":"code","9503d2eb":"code","c23f8dda":"code","049470e2":"code","d5fcb22a":"code","00f75ec7":"code","558a3d62":"code","92871cf8":"markdown","fdc0ed23":"markdown","40768ecf":"markdown","6477907a":"markdown","37b4e157":"markdown","fd678db4":"markdown","22edd439":"markdown","7982b007":"markdown","07dcae47":"markdown","0bd2c96d":"markdown","11d0dd35":"markdown","0d7f4cb9":"markdown","61ee8f7b":"markdown","5c398965":"markdown","31f1f577":"markdown","d49bfbc3":"markdown","96f46efb":"markdown","0875e673":"markdown","13ee44b9":"markdown"},"source":{"9a065452":"# Import all the necessary libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n#This is for data exploration and viualization\nimport seaborn as sns \nfrom scipy import stats\nimport matplotlib.pyplot as plt \n\n# The following is basically for building and training our models\nimport torch \nfrom torch import nn \nimport torch.optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom sklearn.ensemble import RandomForestClassifier","a6a1b62f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","838fedb5":"train_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntrain_data.head()","e0122358":"test_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntest_data.head()","14fbbb83":"# Check the length and columns\nlen(train_data.index)","e0a51ca3":"len(train_data.columns)","b6d02d50":"# Find survival rate for women\nwomen = train_data.loc[train_data.Sex=='female'][\"Survived\"]\nrate_women = sum(women) \/ len(women)\nF\"% of women who survived: {rate_women}\"","b46dfd40":"men = train_data.loc[train_data.Sex=='male']['Survived']\nmen_rate = sum(men) \/ len(men)\nF\"% of men who survived: {men_rate}\"","0fe83e19":"# Find the survival rate in the different classes\nfare_mean_1st = train_data[train_data[\"Pclass\"]==1].Fare.mean()\nfare_mean_2nd = train_data[train_data[\"Pclass\"]==2].Fare.mean()\nfare_mean_3rd = train_data[train_data[\"Pclass\"]==3].Fare.mean()\nF\"Average cost of tickets for 1st, snd, 3rd classes: \\\n{fare_mean_1st} || {fare_mean_2nd} || {fare_mean_3rd}\"\n","ebaa200d":"woman_survived_1st = len(train_data[(train_data[\"Sex\"]==\"female\") & (train_data[\"Survived\"]==1) & (train_data[\"Pclass\"]==1)].index) \/ len(train_data[(train_data[\"Sex\"]==\"female\") & (train_data[\"Pclass\"]==1)].index)\nwoman_survived_2nd = len(train_data[(train_data[\"Sex\"]==\"female\") & (train_data[\"Survived\"]==1) & (train_data[\"Pclass\"]==2)].index) \/ len(train_data[(train_data[\"Sex\"]==\"female\") & (train_data[\"Pclass\"]==2)].index)\nwoman_survived_3rd = len(train_data[(train_data[\"Sex\"]==\"female\") & (train_data[\"Survived\"]==1) & (train_data[\"Pclass\"]==3)].index) \/ len(train_data[(train_data[\"Sex\"]==\"female\") & (train_data[\"Pclass\"]==3)].index)\n\nF\"Rate of Survival for women in different classes: {woman_survived_1st} || {woman_survived_2nd} || {woman_survived_3rd}\"","c03e683e":"# Let's find out what other factors could effect the rate of survival\nwoman_survived_1st = len(train_data[(train_data[\"Sex\"]==\"male\") & (train_data[\"Survived\"]==1) & (train_data[\"Pclass\"]==1)].index) \/ len(train_data[(train_data[\"Sex\"]==\"male\") & (train_data[\"Pclass\"]==1)].index)\nwoman_survived_2nd = len(train_data[(train_data[\"Sex\"]==\"male\") & (train_data[\"Survived\"]==1) & (train_data[\"Pclass\"]==2)].index) \/ len(train_data[(train_data[\"Sex\"]==\"male\") & (train_data[\"Pclass\"]==2)].index)\nwoman_survived_3rd = len(train_data[(train_data[\"Sex\"]==\"male\") & (train_data[\"Survived\"]==1) & (train_data[\"Pclass\"]==3)].index) \/ len(train_data[(train_data[\"Sex\"]==\"male\") & (train_data[\"Pclass\"]==3)].index)\n\nF\"Rate of Survival for men in different classes: {woman_survived_1st} || {woman_survived_2nd} || {woman_survived_3rd}\"","8f9a7f48":"# The verage Age \nage_mean = train_data.Age.mean()\nsurvived_age_mean = train_data[(train_data[\"Survived\"]==1)].Age.mean()\nsurvived_age_std = train_data[(train_data[\"Survived\"]==1)].Age.std()\nsurvived_min_age = train_data[(train_data[\"Survived\"]==1)].Age.min()\nsurvived_max_age = train_data[(train_data[\"Survived\"]==1)].Age.max()\n\nprint(\"The average of survivals age \", survived_age_mean)\nprint(\"The STD of survivals age \", survived_age_std)\nprint(\"The min age of survivals \", survived_min_age)\nprint(\"The max age of survivals \", survived_max_age)\n\n# Let's see the other side\ndeceased_age_mean = train_data[(train_data[\"Survived\"]==0)].Age.mean()\ndeceased_age_std = train_data[(train_data[\"Survived\"]==0)].Age.std()\ndeceased_min_age = train_data[(train_data[\"Survived\"]==0)].Age.min()\ndeceased_max_age = train_data[(train_data[\"Survived\"]==0)].Age.max()\n\nprint()\nprint(\"The average of deceased age\", deceased_age_mean)\nprint(\"The STD of deceased age\", deceased_age_std)\nprint(\"The min age of deceased \", deceased_min_age)\nprint(\"The max age of deceased \", deceased_max_age)","3a60da65":"# Show Any Null values in the data\ntrain_data.isnull().sum()","3c5e6036":"test_data.isnull().sum()","1b04fc78":"sns.boxplot(x=train_data['Age'])","499da59b":"sns.boxplot(x=train_data['SibSp'])","0f48a6c1":"fig= plt.figure(figsize=(10,5))\ntrain_data.groupby('Sex')['PassengerId'].nunique().plot(kind='bar')\nplt.xlabel('Sex')\nplt.title('Number of records by Sex')\nplt.show()","4c2cffc6":"fig= plt.figure(figsize=(20,10))\ntrain_data.groupby('Age')['PassengerId'].nunique().plot(kind='bar')\nplt.xlabel('Age')\nplt.title('Number of records by Age')\nplt.show()","205ecf56":"# Before ficing our data let's drop any undesired columns\nX = train_data.drop(['PassengerId', 'Name', 'Ticket', 'Cabin', 'Embarked'], axis=1)\nX_test = test_data.drop(['PassengerId','Name', 'Ticket', 'Cabin', 'Embarked'], axis=1)","473aa20f":"X = pd.get_dummies(X)\nX_test = pd.get_dummies(X_test)\nX.fillna(X.mean(),inplace=True)\nX_test.fillna(X_test.mean(),inplace=True)","ef69dab2":"X.isnull().sum()","58b839ea":"X_test.isnull().sum()","2ee802bc":"# Splitting data into labels and targets\nfeatures = [\"Pclass\", \"Sex_female\", \"Age\", \"Fare\", \"SibSp\", \"Parch\"]\n\n# Dividing the data into features and labels\ny= X['Survived']\n\nX = pd.DataFrame(X, columns = features) \nX_test = pd.DataFrame(X_test, columns = features)\n\n# standardize and Normalizing the data\n# This will also help minimize the effect of outliers \nfor col in features:\n    X[col] = (X[col] - X[col].mean()) \/ X[col].std()\n    X_test[col] = (X_test[col] - X_test[col].mean()) \/ X_test[col].std()\n    \nfor col in features:    \n    X[col] = (X[col] - X[col].min()) \/ (X[col].max() - X[col].min())\n    X_test[col] = (X_test[col] - X_test[col].min()) \/ (X_test[col].max() - X_test[col].min())\n       ","0f9d0679":"from sklearn.model_selection import cross_val_score\n\nmodel = RandomForestClassifier(n_estimators = 100, max_features='auto', criterion='entropy',max_depth=10)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('random_forest_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n\ncv_scores = cross_val_score(model, X, y, \n                            cv=5,\n                            scoring='accuracy')\n\nprint(\"Cross-validation accuracy: %f\" % cv_scores.mean())","c99f1710":"df = pd.read_csv(\"random_forest_submission.csv\")\ndf","cdc7218b":"# convert to numpy array \nX = X.to_numpy()\ny = y.to_numpy().reshape(-1, 1)\nX_test = X_test.to_numpy()","fe6e51bd":"F\"Length of train data: {len(X)}  Length of test data: {len(X_test)} \"","e7f34371":"# Chaeck the types of the data and shapes\nprint(\"The type of our data:\\n\", type(X))\nprint(type(X_test))\n\n\n\n# Print the shapes\nprint(\"\\nThe shape of our training data: \\n\", X.shape)\nprint(\"\\nThe shape of our targets: \\n\", y.shape)\nprint(\"\\nThe shape of our test data: \\n\", X_test.shape)\n\n\n","73d01929":"# Batch the data for the linear regression\ndef batch_data(batch_size, input_data, target, test_data, train_type = \"regression\", val_size=0.1):\n    '''\n    This function batches the data for our model to train on\n    batch_size: number of batches to perform backpropagation on\n    input_data: numpy array with our input features\n    target: numpy array with our target\n    test_data: numpy array of our test data (doesn't contain targets)\n    train_type: some small differences in batches between regression vs classification\n    '''\n    if train_type == \"regression\":\n         target_tensor = torch.FloatTensor(target)\n            \n    elif train_type == \"classification\":\n        target_tensor = torch.LongTensor(target)\n        target_tensor = target_tensor.squeeze()\n        \n    input_tensor = torch.FloatTensor(input_data)\n    test_tensor = torch.FloatTensor(test_data)\n    \n     # Create our custom dataset with input and corresponding targets\n    train_dataset = TensorDataset(input_tensor, target_tensor)\n    \n    # Split training set into validation and training\n    num_train = len(train_dataset)\n    indicies = list(range(num_train))\n    np.random.shuffle(indicies)\n    val_split = int(np.floor(val_size * num_train))\n    \n    train_idx, val_idx = indicies[val_split:], indicies[:val_split]\n    \n    train_sampler = SubsetRandomSampler(train_idx)\n    val_sampler = SubsetRandomSampler(val_idx)\n    \n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler = train_sampler)\n    val_loader = DataLoader(train_dataset, batch_size=batch_size, sampler = val_sampler)\n    # No need to create multiple batches for the test loader\n    test_loader = DataLoader(test_tensor, batch_size=len(test_data))\n    \n    return train_loader, val_loader, test_loader\n    \n    \n    ","4be4b5ff":"batch_size = 32\ntrain_loader, val_loader, test_loader = batch_data(batch_size, X, y, X_test)","5d64267a":"# Check our data loader\ndata_iter = iter(train_loader)\nsample_x, sample_y = data_iter.next()\n\nprint(sample_x.shape)\nprint(sample_x)\nprint()\nprint(sample_y.shape)\nprint(sample_y)","e7550ac8":"\nclass LinearRegression(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(6, 20)     \n        self.fc2 = nn.Linear(20, 1)\n        self.sigmoid  = nn.Sigmoid()\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.sigmoid(x)\n        x = self.fc2(x)\n        return x\n    ","43462b53":"# Initializing our network weights\ndef init_weights(m):\n    if type(m) == nn.Linear:\n        torch.nn.init.xavier_uniform(m.weight)\n        m.bias.data.fill_(0.01)","38f0fc02":"linear_regression_model = LinearRegression()\nlinear_regression_model.apply(init_weights)\nprint(linear_regression_model)","3f37e1e0":"# Initialize the loss and opimization functions\nlr = 0.01\ncriterion = nn.MSELoss() # mean square error\noptimizer = torch.optim.SGD(linear_regression_model.parameters(), lr=lr, momentum=0.9)\nbatch_size = 32","33ae91d4":"def train_model(model, batch_size, epochs, cost_function, print_every = 100):\n    val_loss_min = np.Inf\n    \n    for e in range(epochs):\n        val_loss = 0.0\n        train_loss = 0.0\n        \n        model.train()\n        for inputs, labels in train_loader:\n        \n            optimizer.zero_grad()\n            output = model(inputs)\n            loss = cost_function(output, labels)\n            \n             # Perform the backpropagation and the optimization step\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item() * batch_size\n        \n        # Evaluating our model performance\n        model.eval()\n        for inputs, labels in val_loader:\n            output = model(inputs)\n            loss = cost_function(output, labels)\n            \n            val_loss += loss.item() * batch_size\n                    \n        train_loss = train_loss \/ len(train_loader.sampler)\n        val_loss = val_loss \/ len(val_loader.sampler)\n        \n        # save model if validation loss has decreased\n        if val_loss <= val_loss_min:\n            # print the decremnet in the validation\n            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n            val_loss_min, val_loss))\n            val_loss_min = val_loss\n            torch.save(model.state_dict(), 'model_linear.pt')\n            \n        if epochs % print_every == 0:\n            print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(e, train_loss, val_loss))\n        \n    print(\"Best model with validation loss: {}\". format(val_loss_min))\n\n \n    ","a23737a6":"train_model(linear_regression_model, batch_size, epochs=3000, cost_function=criterion)","58f10153":"linear_regression_model.load_state_dict(torch.load('model_linear.pt'))","7817e79f":"# getting a batch from testing data\nwith torch.no_grad():\n    for data in test_loader:\n        output = linear_regression_model(data)\n        preds = torch.round(output)\n    preds = preds.squeeze()\n    survived = preds.numpy()\n        ","f7ecc29f":"survived = survived.astype('int')","320d07b2":"submission = pd.DataFrame({'PassengerId': test_data['PassengerId'], 'Survived': survived})\nsubmission.to_csv('submission_regression.csv', index=False)\nprint(\"Your submission was successfully saved!\")","16fc28ad":"df = pd.read_csv(\"submission_regression.csv\")\ndf","42da155f":"train_loader, val_loader, test_loader = batch_data(batch_size, X, y, X_test, train_type=\"classification\", val_size=0.2)","755d7451":"\nclass Clasification(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(6, 20)\n        self.fc2 = nn.Linear(20, 2)\n        \n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n    ","b6cbecfe":"classification_model = Clasification()\nclassification_model.apply(init_weights)\nclassification_model","cf42d915":"# Initialize the loss and opimization functions\nlr = 0.01\ncriterion = nn.CrossEntropyLoss() # mean square error\noptimizer = torch.optim.SGD(classification_model.parameters(), lr=lr, momentum=0.9)\nbatch_size = 64","ea7842ee":"train_model(classification_model, batch_size, 3000, criterion)","b05dcce4":"# getting a batch from testing data\nwith torch.no_grad():\n    for data in test_loader:\n        output = classification_model(data.float())\n        _, preds = torch.max(output.data, 1)\nsurvived = preds.numpy()  ","a23aa454":"survived","a9323838":"submission = pd.DataFrame({'PassengerId': test_data['PassengerId'], 'Survived': survived})\nsubmission.to_csv('submission_classification.csv', index=False)\nprint(\"Your submission was successfully saved!\")","9503d2eb":"df = pd.read_csv(\"submission_classification.csv\")\ndf.head()","c23f8dda":"X_valid  = [next(iter(val_loader))[0].numpy()]\ny_valid  = next(iter(val_loader))[1].numpy()","049470e2":"from xgboost import XGBClassifier\nfrom sklearn.metrics import mean_absolute_error","d5fcb22a":"xg_model = XGBClassifier(learning_rate=0.05, n_estimators=800)\nxg_model.fit(X, y)","00f75ec7":"\npredictions = xg_model.predict(X_test)\npredictions","558a3d62":"output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('xg_boost_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","92871cf8":"## Testing","fdc0ed23":"References: \n- https:\/\/www.kaggle.com\/frtgnn\/introduction-to-pytorch-a-very-gentle-start\n- https:\/\/www.kaggle.com\/kiranscaria\/titanic-pytorch\n- https:\/\/www.kaggle.com\/alexisbcook\/getting-started-with-titanic\n- https:\/\/www.analyticsvidhya.com\/blog\/2015\/12\/improve-machine-learning-results\/\n- https:\/\/androidkt.com\/detect-and-remove-outliers-from-pandas-dataframe\/","40768ecf":"Just to have a better visualization for our data let's have a better look with some histograms","6477907a":"> Box Plot is a good way to detect outlier values. They are the invisual dots away from the quartiles.","37b4e157":"## Building the Classification  Model\n- The number of output here will change into 2\n- We'll use CrossEntropyLoss instead of MSELoss","fd678db4":"### Preprocessing Data\nBut first make sure that we convert categorical data into one hot encoding and fix any NAN values in the train data","22edd439":"As a side note deep learning technique is more suitable for large data, in fact this is why it is widely used and the main purpose it was invented which is to handle learning from large data. So it is understandable why it might not perform perfectly here.","7982b007":"## Testing & Submision\nHere I'll test the classification model and save the submission","07dcae47":"## Exploring the data","0bd2c96d":"## Batch The Data for the XG-Booster","11d0dd35":"## Training the Model","0d7f4cb9":"## Building the Linear Regression Model\n\nNow that everything is looking good, let's build our training model!","61ee8f7b":"## Investigating Deeper into the data\nI actually added this part after unsuccessfully triec to increase the accuracy for my model both linear regression and classification. So I decided to explore the data more and see if there is any Null values or\/and outliers that could effect the model results.\n","5c398965":"Awesome! No more null values","31f1f577":"## Training the Model with Random Forest","d49bfbc3":"Now let's see how were the effect of other factors in the rate of survival.\nThe code below might not be the easiest to read, but if we take a second good look it will be clear tp us that we are dividing the numbers of survivals (men\/women) by the number of passangers (survived or not) in the specific class.","96f46efb":"## Batching the Data","0875e673":"# Titanic with Pytorch & XG-Boost\nHello there! We'll try to solve the Titanic survival model first as a Linear Regression problem and second as classification problem.\n\n- We'll take a look at our data.\n- Choose the features (Featur selection)\n- Creating our custom dataset and batches using Pytorch DataLoader which will make it an easy task.\n\nLet's dive in!","13ee44b9":"## Building XG-Boost"}}