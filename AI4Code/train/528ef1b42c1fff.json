{"cell_type":{"2c5f7e67":"code","f2c7b192":"code","0baaf8e6":"code","ba6d5e21":"code","977231e3":"code","74da07d0":"code","73b0b4d9":"code","b6588c84":"code","e1a2cd21":"code","d34cc891":"code","68be8a48":"code","26d25af3":"code","e4eea070":"code","67e233b2":"code","fc1f4845":"code","12b5f576":"code","1989aa84":"code","0b15438b":"code","23324c9e":"code","ad151ac5":"code","441daeed":"code","24e36961":"code","4dc6eafa":"code","d6c1b806":"code","2a058b6d":"code","2c5f53c1":"code","14847d4b":"code","da80647c":"code","f3eda8dd":"code","0ab6eabf":"code","d272abc2":"code","09f639a8":"code","fdf047c7":"code","a64a747a":"code","09c9a9ea":"code","1022380b":"markdown","80c87651":"markdown","0a9fdfdd":"markdown","f5656561":"markdown","1d03457a":"markdown","81edd0b2":"markdown","5d2b87bf":"markdown","1a1de99b":"markdown","920865d9":"markdown","b1d19868":"markdown","46fa7ce3":"markdown","967ef9de":"markdown","14f58243":"markdown","0a9ebffa":"markdown","ba6c5407":"markdown","806be271":"markdown","8043e9b3":"markdown"},"source":{"2c5f7e67":"# Let's install Feature-engine\n# this package will allow us to quickly remove \n# non-predictive variables\n\n!pip install feature-engine","f2c7b192":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# I use GBM because it usually out-performs other off-the-shelf \n# classifiers\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# metric to optimize for the competition\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.model_selection import train_test_split\n\n# to assemble various procedures in sequence\nfrom sklearn.pipeline import Pipeline\n\n# some methods to work with imbalanced data are based in nearest neighbours\n# and nearest neighbours are sensitive to the magnitude of the features\n# so we need to scale the data\nfrom sklearn.preprocessing import MinMaxScaler\n\n# import selection classes from Feature-engine\n# to reduce the number of features\nfrom feature_engine.selection import (\n    DropDuplicateFeatures,\n    DropConstantFeatures,\n)\n\n# over-sampling techniques for imbalanced data\nfrom imblearn.over_sampling import (\n    RandomOverSampler,\n    SMOTENC,\n)\n\n# under-sampling techniques for imbalanced data\nfrom imblearn.under_sampling import (\n    InstanceHardnessThreshold,\n    RandomUnderSampler,\n)\n\n# special ensemble methods to work with imbalanced data\n# we will use those based on boosting, which tend to work better\nfrom imblearn.ensemble import (\n    RUSBoostClassifier,\n    EasyEnsembleClassifier,\n)","0baaf8e6":"# load the Santander Customer Satisfaction dataset\n\ndata = pd.read_csv('\/kaggle\/input\/santander-customer-satisfaction\/train.csv')","ba6d5e21":"# separate dataset into train and test sets\n# I split 20:80 mostly to reduce the size of the train set\n# so that this notebook does not run out of memory :_(\n\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop(labels=['ID','TARGET'], axis=1),\n    data['TARGET'],\n    test_size=0.8,\n    random_state=0)\n\nX_train.shape, X_test.shape","977231e3":"# check class imbalance\n\ny_train.value_counts(normalize=True), y_train.value_counts()","74da07d0":"# check also the test set\ny_test.value_counts(normalize=True)","73b0b4d9":"# to remove constant and duplicated features, we use the transformers from Feature-engine\n\npipe = Pipeline([\n    ('constant', DropConstantFeatures(tol=1)), # drops constant features\n    ('duplicated', DropDuplicateFeatures()), # drops duplicates\n])\n\n# find features to remove\npipe.fit(X_train, y_train)","b6588c84":"# how many constant features are there in the dataset?\n\nlen(pipe.named_steps['constant'].features_to_drop_)","e1a2cd21":"# how many duplicated features are there in the dataset?\n\nlen(pipe.named_steps['duplicated'].features_to_drop_)","d34cc891":"print('Number of original variables: ', X_train.shape[1])\n\n# see how with the pipeline we can apply all transformers in sequence\n# with one line of code, for each data set\nX_train = pipe.transform(X_train)\nX_test = pipe.transform(X_test)\n\nprint('Number of variables after selection: ', X_train.shape[1])","68be8a48":"# Let's find out how many variables we have with 2, or less than 10 or 20 distinct values\n\nfor max_unique in [2, 10, 20]:\n    vars_ = [x for x in X_train.columns if X_train[x].nunique()<= max_unique]\n    vars_ = len(vars_)\n    print(f'{vars_} variables with less than or equal to {max_unique} values')","26d25af3":"# set up the gradient boosting classifier\ngbm = GradientBoostingClassifier(\n    loss = 'exponential',\n    max_depth = 1,\n    min_samples_split = 0.80,\n    n_estimators = 100,\n)\n\n# fit\ngbm.fit(X_train, y_train)","e4eea070":"# Now let's get the benchmark performance on train and test\n\nX_train_preds = gbm.predict_proba(X_train)[:,1]\nX_test_preds = gbm.predict_proba(X_test)[:,1]\n\nprint('Train roc_auc: ', roc_auc_score(y_train, X_train_preds))\nprint('Test roc_auc: ', roc_auc_score(y_test, X_test_preds))","67e233b2":"# set up instance hardness threshold\n# the instance hardness is determined based on a gradient boosting machine\n# trained on the entire dataset\n\niht = InstanceHardnessThreshold(\n    estimator=gbm, # we pass the model we set up earlier\n    sampling_strategy='auto',  # undersamples only the majority class\n    random_state=1,\n    cv=2,  # cross validation fold, 2 to speed things up.\n)\n\n# resample\nX_resampled, y_resampled = iht.fit_resample(X_train, y_train)\n\n# shape of original data and data after resampling\nX_train.shape, X_resampled.shape","fc1f4845":"# check the resampled target\n# instance hardness treshold is a fixed undersampling method\n# so it aims for 50:50 observations from majority and minority class\n\n# let's see\ny_resampled.value_counts(normalize=True)","12b5f576":"# train model on resampled data\n\ngbm.fit(X_resampled, y_resampled)","1989aa84":"# Now let's get the performance on train and test\n\nX_train_preds = gbm.predict_proba(X_resampled)[:,1]\nX_test_preds = gbm.predict_proba(X_test)[:,1]\n\nprint('Train roc_auc: ', roc_auc_score(y_resampled, X_train_preds))\nprint('Test roc_auc: ', roc_auc_score(y_test, X_test_preds))","0b15438b":"rus = RandomUnderSampler(\n    sampling_strategy='auto',  # undersamples only the majority class\n    random_state=0,\n)\n\n# resample\nX_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n\n# shape of original data and data after resampling\n# we see that the data was reduced quite a bit\n\nX_train.shape, X_resampled.shape","23324c9e":"# check the resampled target\n\ny_resampled.value_counts(normalize=True)","ad151ac5":"# train model\n\ngbm.fit(X_resampled, y_resampled)","441daeed":"# Now let's get the performance on train and test\n\nX_train_preds = gbm.predict_proba(X_resampled)[:,1]\nX_test_preds = gbm.predict_proba(X_test)[:,1]\n\nprint('Train roc_auc: ', roc_auc_score(y_resampled, X_train_preds))\nprint('Test roc_auc: ', roc_auc_score(y_test, X_test_preds))","24e36961":"ros = RandomOverSampler(\n    sampling_strategy='auto',  # undersamples only the majority class\n    random_state=0,\n)\n\n# resample\nX_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n\n# we would have a lot more observations from the majority class now\nX_train.shape, X_resampled.shape","4dc6eafa":"# check the resampled target\n\ny_resampled.value_counts(normalize=True)","d6c1b806":"# Now let's get the performance on train and test\n\nX_train_preds = gbm.predict_proba(X_resampled)[:,1]\nX_test_preds = gbm.predict_proba(X_test)[:,1]\n\nprint('Train roc_auc: ', roc_auc_score(y_resampled, X_train_preds))\nprint('Test roc_auc: ', roc_auc_score(y_test, X_test_preds))","2a058b6d":"# we need to capture the index of the discrete variables\n\n# make list of discrete variables\ncat_vars = [var for var in X_train.columns if X_train[var].nunique() <= 10]\n\n# capture the index in the dataframe columns\ncat_vars_index = [cat_vars.index(x) for x in cat_vars]\n\ncat_vars_index[0:6]","2c5f53c1":"smnc = SMOTENC(\n    sampling_strategy='auto', # samples only the minority class\n    random_state=0,  # for reproducibility\n    k_neighbors=3,\n    categorical_features=cat_vars_index # indeces of the columns of discrete variables\n)  \n\n# because SMOTE uses KNN, and KNN is sensible to variable magnitude, we re-scale the data\n\n# this procedure will take a while\nX_resampled, y_resampled = smnc.fit_resample(MinMaxScaler().fit_transform(X_train), y_train)\n\nX_train.shape, X_resampled.shape","14847d4b":"# check the distribution of the resampled target\n# we should have 50:50 now\n\ny_resampled.value_counts(normalize=True)","da80647c":"# train the model \n\ngbm.fit(X_resampled, y_resampled)","f3eda8dd":"# Now let's get the performance on train and test\n\nX_train_preds = gbm.predict_proba(X_resampled)[:,1]\nX_test_preds = gbm.predict_proba(X_test)[:,1]\n\nprint('Train roc_auc: ', roc_auc_score(y_resampled, X_train_preds))\nprint('Test roc_auc: ', roc_auc_score(y_test, X_test_preds))","0ab6eabf":"# set up the RUSBoost ensemble model\n\nrusboost = RUSBoostClassifier(\n        base_estimator=None,\n        n_estimators=20,\n        learning_rate=1.0,\n        sampling_strategy='auto',\n        random_state=2909,\n    )\n\n\n# train model\nrusboost.fit(X_train, y_train)","d272abc2":"# Now let's get the performance on train and test\n\nX_train_preds = rusboost.predict_proba(X_train)[:,1]\nX_test_preds = rusboost.predict_proba(X_test)[:,1]\n\nprint('Train roc_auc: ', roc_auc_score(y_train, X_train_preds))\nprint('Test roc_auc: ', roc_auc_score(y_test, X_test_preds))","09f639a8":"easy = EasyEnsembleClassifier(\n        n_estimators=10,\n        sampling_strategy='auto',\n        random_state=2909,\n    )\n\n\n# train model\neasy.fit(X_train, y_train)","fdf047c7":"# Now let's get the performance on train and test\n\nX_train_preds = easy.predict_proba(X_train)[:,1]\nX_test_preds = easy.predict_proba(X_test)[:,1]\n\nprint('Train roc_auc: ', roc_auc_score(y_train, X_train_preds))\nprint('Test roc_auc: ', roc_auc_score(y_test, X_test_preds))","a64a747a":"# we have an imbalance of 95 to 5, so we use those as weights\nsample_weight = np.where(y_train==1, 95, 5)\n\n# train model\ngbm.fit(X_train, y_train, sample_weight)","09c9a9ea":"# Now let's get the performance on train and test\n\nX_train_preds = gbm.predict_proba(X_train)[:,1]\nX_test_preds = gbm.predict_proba(X_test)[:,1]\n\nprint('Train roc_auc: ', roc_auc_score(y_train, X_train_preds))\nprint('Test roc_auc: ', roc_auc_score(y_test, X_test_preds))","1022380b":"Now, we reduced the dataset a bit. Let's hope that that helps speed things up!\n\n## Variable exploration\n\nFrom previous analysis we know that this data set does not contain missing values and that all variables are numerical.\n\nWe also know from previous analysis that most variables in this dataset are binary and discrete, with very few continuous variables.","80c87651":"The ensemble methods did not improve the performance either. Another thing we can investigate is cost-sensitive learning.\n\n## Cost-sensitive learning\n\nTo finish with, we will implement cost-sensitive learning. That is, we will modify the penalization cost of the minority class. We can do this directly from the sklearn Gradient Boosting Classifier as follows:","0a9fdfdd":"This SMOTE method was not useful in this dataset\n\n## Ensemble methods for imbalanced data\n\nWe will implement RUSBOOSt and Easy Ensemble. Both are based on boosting methods thus, tend to return better model performance.","f5656561":"## Target\n\nThe target class is imbalanced. The value 1 refers to un-satisfied customers and 0 to satisfied. So most of Santander's customers are satisfied.","1d03457a":"We observe a tiny improvement, but probably within the error of the model. We would have to use cross-validation and get a measure of the error dispersion to be sure. But I can't do that on Kaggle kernels for every technique. It runs out of memory.","81edd0b2":"Let's see if we can improve the performance a bit by incorporating methods designed to work with imbalanced datasets.\n\n# Methods for Imbalanced data\n\n# Under-sampling\n\n## Instance Hardness Threshold\n\nAmong the under-sampling methods, we can perform random under-sampling, where we extract samples form the majority class at random. We extract normally as many samples as those we have in the minority. \n\nThen we have cleaning methods, but all of them depend on nearest neighbours, so I would argue that are not suitable given that we have a mix of discrete and continuous variables. \n\nWe can use the InstanceHardness treshold which will remove observations from the majority class that are hard to classify correctly. \n\nInstance hardness is a measure of how difficult an observation is to classify correclty, and it is inversely correlated to the probability of its class.\n\nSo to keep things simple, let's just implement the instance hardness treshold to under-sample our data.","5d2b87bf":"The model is over-fit to the train set. The instance hardness threshold is not improving the model performance. On the contrary.\n\n## Random Undersampling\n\nIn random undersampling, we would select at random as many observations from the majority as those we have in the minority. This method is often neglected because it tends to reduce the size of the train set quite dramatically.\n\nLet's try it in any case.","1a1de99b":"From all the techniques that we tested in this notebook, the benchmark model trained on the entire dataset and the 1 with cost-sensitive learning seem to be the ones that perform the best. So to follow up, we could optimize parameters on these to see if this improves model performance.\n\nThat will be all for this notebook.\n\nFor more details on feature engineering and feature selection, hyperparameter optimization or working with imbalanced datasets, feel free to check my [online courses](https:\/\/www.trainindata.com\/).","920865d9":"Interesting, even with under-sampling, reducing the dataset quite a bit, we obtain quite a similar performance to that obtain using the entire dataset.\n\nIf we want to train a model repeatedly in a live system, this could be a nice alternative, as smaller datasets allow faster training times, and we would not be sacrificing performance.","b1d19868":"## Load Data","46fa7ce3":"## SMOTENC\n\nSMOTE methos create new, synthetic data, using original samples as templates. They interpolate the synthetic data within the range of a sample used as template and any og its 5 closest neighbours.\n\nThere is 1 variant that is suitable for datasets with continuous and discrete variables, which is SMOTE-NC, so we will implement that method.","967ef9de":"We see that we have 95 binary variables, and a few more that are also discrete.\n\nWhy is this important?\n\n* Some under- and over- sampling methods for imbalanced datasets are based of Nearest neighbours\n* Nearest neighbours depend on distance metrics\n* in theory, distance metrics for continuous variables are not appropriate for discrete variables and vice-versa.\n\nSo, if we are strict, we would exclude most of the under- and smote based algorithms, or alternatively, we should create distance matrices manually to accomodate the different metrics. But that is a lot of work.\n\nSo, in this notebook, we will use only algorithms that we can use off-the-shelf.\n\nFor now, let's train a gradient boosting machine with these variables to determine the benchmark performance.\n\n## Train Gradient Boosting Model\n\nWe know that for classification Gradient Boosting Machines out-perform all other models, so we will implement directly that model.","14f58243":"## Drop constant and duplicated features\n\nThis dataset contains constant and duplicated features. I know this from previous analysis so I will quickly remove these features to reduce the data size.\n\nMore insight about feature selection for this dataset here:\nhttps:\/\/www.kaggle.com\/solegalli\/feature-selection-with-feature-engine","0a9ebffa":"# Predicting Customer Satisfaction with Imbalanced Data\n\nIn this notebook, I'll show different techniques suitable for imabalanced datses to try and improve model performance.\n\n## In this notebook\n\n* Remove redundant features\n* Train a classifier to predict customer satisfaction\n* Improve performance with different techniques for imbalanced data\n\n## Feature selection\n\nTo begin with, I will remove duplicated and constant features. These are in essence redundant or non-predictive. Next, I will find quasi-constant features, and evaluate the distribution of its values across satisfied and un-satisfied customers, to determine if I can remove them.\n\n* Remove constant features\n* Remove duplicated features\n\n\n## The Machine Learning Model\n\nThis is a classification problem. I want to predict if a customer is unsatisfied (1 in the target). From the off-the-shelf algorithms we know that **Gradient Boosting Machines** out-perform all other models. So in this notebook I will train a Gradient Boosting Classifier from Scikit-learn.\n\n\n## Imbalanced data\n\nThere are a number of techniques that we can use to try and improve the performance of models trained on imbalanced datasets. We can under- or over-sample the dataset. Within the under-sampling techniques we have cleaning techniques that allow us to remove noisy observations instead of observations just at random. Within the over-sampling techniques we have methods to create new, \"synthetic\", data using existing observations as templates. This way, we do not just \"duplicate\" the data as we would do with over-sampling.\n\nWe can also implement cost-sensitive learning, where we modify the optimization function to account for the cost of miss-classification. Miss-classifying an observation from the minority class tends to be more costly in real situations. And finally we have special ensemble algorithms that were designed specifically to work with imbalanced datasets.\n\nSo in summary, we could try:\n\n* undersampling at random or based on cleaning criteria\n* oversampling at random or create synthethic new data\n* introduce cost sensitive learning\n* train a special algorithm for imbalanced datasets\n\nFor more details on feature engineering and feature selection, hyperparameter optimization or working with imbalanced datasets, visit my [online courses](https:\/\/www.trainindata.com\/).","ba6c5407":"# Oversampling\n\n## Random Over-sampling\n\nAmong the over-sampling methods we have, random over-sampling, which bootstraps observations from the minority class to increase their number. This technique in essence duplicates data, so sometimes leads to over-fitting. \n\nLet's try it in any case.","806be271":"Let's go ahead and remove them from the datasets.","8043e9b3":"We see that ~ 4% of the customers are not satisfied, that is around ~ customers 2700."}}