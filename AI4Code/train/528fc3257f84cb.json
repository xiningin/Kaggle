{"cell_type":{"fb772a90":"code","88fb7e09":"code","58a3dc5f":"code","bb64fa8b":"code","0224c92d":"code","cf1b8fb6":"code","1d096755":"code","54885911":"code","03582b70":"code","f3dd10eb":"code","8d4d0207":"code","4624cee7":"code","7d419595":"code","d9a5d5f1":"code","2aca4064":"code","6612db98":"code","4c3da0de":"code","44cc123c":"code","70a90aea":"code","5d3fd40c":"code","4fe8885d":"code","7897ab88":"code","aac5e951":"code","d575b104":"code","afa78d25":"code","4420ca56":"code","4959786e":"code","d8aaaf61":"code","345601db":"code","00b7e92f":"code","5aac5bc6":"code","b8ee127c":"code","89ebe3e8":"markdown","99d74761":"markdown","e9dba567":"markdown","3c4610ca":"markdown","78005afd":"markdown","c1ec5e37":"markdown","467cee6d":"markdown","25b6b4f3":"markdown","485060c6":"markdown","0466d625":"markdown","b5250b3d":"markdown","fda11e4d":"markdown","db01189c":"markdown"},"source":{"fb772a90":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nimport seaborn as sns","88fb7e09":"que = pd.read_csv('..\/input\/data-science-for-good-careervillage\/questions.csv')\nans = pd.read_csv('..\/input\/data-science-for-good-careervillage\/answers.csv')\nprof = pd.read_csv('..\/input\/data-science-for-good-careervillage\/professionals.csv')\nstud = pd.read_csv('..\/input\/data-science-for-good-careervillage\/students.csv')","58a3dc5f":"ans.shape","bb64fa8b":"que.shape","0224c92d":"prof.shape","cf1b8fb6":"que.head()","1d096755":"stud.shape","54885911":"stud.isnull().sum()","03582b70":"stud.head()","f3dd10eb":"ans.head()","8d4d0207":"prof.head()","4624cee7":"stud.students_location.unique()","7d419595":"n_locations = 20\n\nusers = [\n    ('students', stud),\n    ('professionals', prof)\n]\n\nfor user, df in users:\n    locations = df['{}_location'.format(user)].value_counts().sort_values(ascending=True).tail(n_locations)\n    \n    ax = locations.plot(kind='barh',figsize=(14, 10),width=0.8, fontsize=14) \n    ax.set_title('Top %s {} locations'.format(user) % n_locations, fontsize=20)\n    ax.set_xlabel('Number of {}'.format(user), fontsize=14)\n    for p in ax.patches:\n        ax.annotate(str(p.get_width()), (p.get_width(), p.get_y()), color='w', fontsize=14)\n    plt.show()","d9a5d5f1":"stud.isnull().sum()","2aca4064":"ans.isnull().sum()","6612db98":"que.isnull().sum()","4c3da0de":"ans['answers_date_added'] = pd.to_datetime(ans['answers_date_added'], infer_datetime_format=True)\nprof['professionals_date_joined'] = pd.to_datetime(prof['professionals_date_joined'], infer_datetime_format=True)\nque['questions_date_added'] = pd.to_datetime(que['questions_date_added'], infer_datetime_format=True)\nstud['students_date_joined'] = pd.to_datetime(stud['students_date_joined'], infer_datetime_format=True)","44cc123c":"users = [\n    ('students', stud),\n    ('professionals', prof)\n]\n\ncolors = {'students' : 'cyan', 'professionals' : 'mediumvioletred'}\n\nfor user, df in users:\n    \n    years = df['{}_date_joined'.format(user)].dt.year.unique()\n    years.sort()\n    \n    min_date = df['{}_date_joined'.format(user)].min()\n    min_date = min_date.strftime(\"%B %Y\")\n    \n    max_date = df['{}_date_joined'.format(user)].max()\n    max_date = max_date.strftime(\"%B %Y\")\n    \n    \n    amounts = [len(df[df['{}_date_joined'.format(user)].dt.year == y]) for y in years]\n    \n    for i in range(len(amounts)):\n        if i > 0:\n            amounts[i] += amounts[i - 1]\n    to_plot = pd.DataFrame({'years': years, 'users': amounts})\n    plt.figure(figsize=(14, 5))\n    \n    plt.plot('years', 'users', data=to_plot, marker='o', color=colors[user])\n    x = to_plot['years']\n    y = to_plot['users']\n    plt.fill_between(x, y, color=colors[user], alpha = 0.4)\n    \n    plt.ylabel('Users', fontsize=14)\n    plt.title('Growth of {}'.format(user), fontsize=20)\n    plt.show()","70a90aea":"entities = [\n    ('questions', que),\n    ('answers', ans)\n]\n\ncolors = {'questions' : 'cyan', 'answers' : 'mediumvioletred'}\n\nfor entity, df in entities:\n    min_date = df['{}_date_added'.format(entity)].min().strftime(\"%B %Y\")\n    max_date = df['{}_date_added'.format(entity)].max().strftime(\"%B %Y\")\n\n    df['year'] = df['{}_date_added'.format(entity)].dt.year\n    plt_data = df.groupby('year').size()\n    plt_data.plot(figsize=(14, 5), color=colors[entity],  marker='o')\n\n    x = plt_data.reset_index()['year']\n    y = plt_data.reset_index()[0]\n    plt.fill_between(x, y, color=colors[entity], alpha = 0.4)\n\n    plt.xlabel('Year', fontsize=15)\n    plt.ylabel('{} Count'.format(entity.capitalize()), fontsize=15)\n    plt.title('Number of {} asked per year ({}-{})'.format(entity.capitalize(), min_date, max_date), fontsize=20)\n    plt.show()","5d3fd40c":"from collections import Counter \n\nll = [\n    ('students', 'questions', stud, que),\n    ('professionals', 'answers', prof, ans)\n]\n\ncolors = {'students' : 'cyan', 'professionals' : 'mediumvioletred'}\n\nfor user, entity, user_df, entity_df in ll:\n    tm = dict(sorted(Counter(pd.merge(user_df, entity_df, left_on='{}_id'.format(user), right_on='{}_author_id'.format(entity), how='inner').groupby('{}_id'.format(user)).size().values).items())) \n    t_d = {}\n    t_d['{}_amount'.format(entity)] = list(tm.keys())\n    t_d['{}_amount'.format(user)] = list(tm.values())\n\n    plt_data = pd.DataFrame(t_d)\n\n    plt_data.plot(x='{}_amount'.format(entity), y='{}_amount'.format(user), kind='bar', figsize=(14, 5), color=colors[user])\n    plt.xlim(-1, 30)\n    plt.xlabel('{} Count'.format(entity.capitalize()), fontsize=15)\n    plt.ylabel('{} Count'.format(user.capitalize()), fontsize=15)\n    plt.title('{} {} Diagram'.format(user.capitalize(), entity.capitalize()), fontsize=20)\n    plt.show()","4fe8885d":"n = 10\n\nll = [\n    ('students', 'questions', stud, que),\n    ('professionals', 'answers', prof, ans)\n]\n\ncolors = {'students' : '#549da8', 'professionals' : '#852ab2'} \n\nfor user, entity, user_df, entity_df in ll:\n    top_n = pd.DataFrame(pd.merge(user_df, entity_df, left_on='{}_id'.format(user), right_on='{}_author_id'.format(entity), how='inner').groupby('{}_id'.format(user)).size().reset_index())\n    plt_data = top_n.rename(index=str, columns={0: '{}_amount'.format(entity)}).sort_values(by=['{}_amount'.format(entity)], ascending=False)[:n]\n\n    plt_data.plot(kind='bar', figsize=(14, 5), color=colors[user])\n    plt.xticks(np.arange(len(plt_data)), tuple(plt_data['{}_id'.format(user)]), rotation=90)\n    plt.xlabel('{} Ids'.format(user.capitalize()), fontsize=15)\n    plt.ylabel('{} Count'.format(entity.capitalize()), fontsize=15)\n    plt.title('Top {} {} with most {}'.format(n, user.capitalize(), entity), fontsize=20)\n    leg = plt.legend(loc='best', fontsize=15)\n    for text in leg.get_texts():\n        plt.setp(text, color = 'w')\n    plt.show()","7897ab88":"answers = pd.read_csv('..\/input\/data-science-for-good-careervillage\/answers.csv')\nanswer_scores = pd.read_csv('..\/input\/data-science-for-good-careervillage\/answer_scores.csv')\ncomments = pd.read_csv('..\/input\/data-science-for-good-careervillage\/comments.csv')\nemails = pd.read_csv('..\/input\/data-science-for-good-careervillage\/emails.csv')\ngroups = pd.read_csv('..\/input\/data-science-for-good-careervillage\/groups.csv')\ngroup_memberships = pd.read_csv('..\/input\/data-science-for-good-careervillage\/group_memberships.csv')\nmatches = pd.read_csv('..\/input\/data-science-for-good-careervillage\/matches.csv')\nprofessionals = pd.read_csv('..\/input\/data-science-for-good-careervillage\/professionals.csv')\nquestions = pd.read_csv('..\/input\/data-science-for-good-careervillage\/questions.csv')\nquestion_scores = pd.read_csv('..\/input\/data-science-for-good-careervillage\/question_scores.csv')\nschool_memberships = pd.read_csv('..\/input\/data-science-for-good-careervillage\/school_memberships.csv')\nstudents = pd.read_csv('..\/input\/data-science-for-good-careervillage\/students.csv')\ntags = pd.read_csv('..\/input\/data-science-for-good-careervillage\/tags.csv')\ntag_questions = pd.read_csv('..\/input\/data-science-for-good-careervillage\/tag_questions.csv')\ntag_users = pd.read_csv('..\/input\/data-science-for-good-careervillage\/tag_users.csv')","aac5e951":"answers['answers_date_added'] = pd.to_datetime(answers['answers_date_added'], infer_datetime_format=True)\ncomments['comments_date_added'] = pd.to_datetime(comments['comments_date_added'], infer_datetime_format=True)\nemails['emails_date_sent'] = pd.to_datetime(emails['emails_date_sent'], infer_datetime_format=True)\nprofessionals['professionals_date_joined'] = pd.to_datetime(professionals['professionals_date_joined'], infer_datetime_format=True)\nquestions['questions_date_added'] = pd.to_datetime(questions['questions_date_added'], infer_datetime_format=True)\nstudents['students_date_joined'] = pd.to_datetime(students['students_date_joined'], infer_datetime_format=True)","d575b104":"# Last Answer\ntemp = answers.groupby('answers_author_id')['answers_date_added'].max()\nprofessionals['date_last_answer'] = pd.merge(professionals, pd.DataFrame(temp.rename('last_answer')), left_on='professionals_id', right_index=True, how='left')['last_answer']\n# First Answer\ntemp = answers.groupby('answers_author_id')['answers_date_added'].min()\nprofessionals['date_first_answer'] = pd.merge(professionals, pd.DataFrame(temp.rename('first_answer')), left_on='professionals_id', right_index=True, how='left')['first_answer']\n# Last Comment\ntemp = comments.groupby('comments_author_id')['comments_date_added'].max()\nprofessionals['date_last_comment'] = pd.merge(professionals, pd.DataFrame(temp.rename('last_comment')), left_on='professionals_id', right_index=True, how='left')['last_comment']\n# First Comment\ntemp = comments.groupby('comments_author_id')['comments_date_added'].min()\nprofessionals['date_first_comment'] = pd.merge(professionals, pd.DataFrame(temp.rename('first_comment')), left_on='professionals_id', right_index=True, how='left')['first_comment']\n# Last Activity\nprofessionals['date_last_activity'] = professionals[['date_last_answer', 'date_last_comment']].max(axis=1)\n# First Activity\nprofessionals['date_first_activity'] = professionals[['date_first_answer', 'date_first_comment']].min(axis=1)\n# Last activity (Question)\ntemp = questions.groupby('questions_author_id')['questions_date_added'].max()\nstudents['date_last_question'] = pd.merge(students, pd.DataFrame(temp.rename('last_question')), left_on='students_id', right_index=True, how='left')['last_question']\n# First activity (Question)\ntemp = questions.groupby('questions_author_id')['questions_date_added'].min()\nstudents['date_first_question'] = pd.merge(students, pd.DataFrame(temp.rename('first_question')), left_on='students_id', right_index=True, how='left')['first_question']\n# Last activity (Comment)\ntemp = comments.groupby('comments_author_id')['comments_date_added'].max()\nstudents['date_last_comment'] = pd.merge(students, pd.DataFrame(temp.rename('last_comment')), left_on='students_id', right_index=True, how='left')['last_comment']\n# First activity (Comment)\ntemp = comments.groupby('comments_author_id')['comments_date_added'].min()\nstudents['date_first_comment'] = pd.merge(students, pd.DataFrame(temp.rename('first_comment')), left_on='students_id', right_index=True, how='left')['first_comment']\n# Last activity (Total)\nstudents['date_last_activity'] = students[['date_last_question', 'date_last_comment']].max(axis=1)\n# First activity (Total)\nstudents['date_first_activity'] = students[['date_first_question', 'date_first_comment']].min(axis=1)","afa78d25":"pro_emails = pd.merge(professionals, emails, how='inner', left_on='professionals_id', right_on='emails_recipient_id')\npro_emails = pro_emails[pro_emails['emails_frequency_level'] == 'email_notification_immediate']\npro_emails = pro_emails[['professionals_id', 'emails_id', 'emails_date_sent']]\n\npro_email_ques = pro_emails.merge(matches, left_on='emails_id', right_on='matches_email_id')\npro_email_ques = pro_email_ques.drop(columns=['emails_id', 'matches_email_id']) \\\n                 .set_index('professionals_id').rename(columns={'matches_question_id': 'questions_id'})","4420ca56":"users = [\n    ('students', students),\n    ('professionals', professionals)\n]\n\nmin_rel_date = '01-01-2016'\nmax_rel_date = '01-01-2019'\n\nplt_data = {}\n\nfor user, df in users:\n    df = df[(df['{}_date_joined'.format(user)] >= min_rel_date) & (df['{}_date_joined'.format(user)] <= max_rel_date)]\n    df = (df['date_first_activity'] - df['{}_date_joined'.format(user)]).dt.days.fillna(10000).astype(int)\n    df = df.groupby(df).size()\/len(df)\n    df = df.rename(lambda x: 0 if x < 0 else x)\n    df = df.rename(lambda x: x if x <= 1 or x == 10000 else '> 1')\n    df = df.rename({10000: 'NaN'})\n    df = df.groupby(level=0).sum()\n\n    plt_data[user] = df\n\nplt_data = pd.DataFrame(plt_data)\n\nplt_data.plot(kind='bar', figsize=(14, 5), colors=('#852ab2', '#21b7f2'))\nplt.xlabel('Days', fontsize=15)\nplt.ylabel('Ration', fontsize=15)\nplt.title('Days before first activity after registration', fontsize=20)\nleg = plt.legend(bbox_to_anchor=(1, 0.5), fontsize=15)\nfor text in leg.get_texts():\n    plt.setp(text, color = 'w')\nplt.show()","4959786e":"import datetime\nfrom datetime import datetime","d8aaaf61":"# Date of export\ncurrent_date = datetime(2019, 2 ,1)\n\nusers = [\n    ('students', students),\n    ('professionals', professionals)\n]\n\nplt_data = {}\n\nfor user, df in users:\n    df = ((current_date - df['date_last_activity']).dt.days\/30).dropna().astype(int)\n    df = df.groupby(df).size()\/len(df)\n    df = df.rename(lambda x: 0 if x < 0 else x).rename(lambda x: x if x <= 30 or x == 10000 else '> 30').rename({10000:'NaN'})\n    df = df.groupby(level=0).sum()\n\n    plt_data[user] = df\n\nplt_data = pd.DataFrame(plt_data)\n\nplt_data.plot(kind='bar', figsize=(14, 5), colors=('#852ab2', '#21b7f2'))\nplt.xlabel('Months', fontsize=15)\nplt.ylabel('Ratio', fontsize=15)\nplt.title('Last activity by Month', fontsize=20)\nleg = plt.legend(loc='best', fontsize=15)\nfor text in leg.get_texts():\n    plt.setp(text, color = 'w')\nplt.show()","345601db":"min_date = emails['emails_date_sent'].min().strftime(\"%B %Y\")\nmax_date = emails['emails_date_sent'].max().strftime(\"%B %Y\")\n\nemails['year'] = emails['emails_date_sent'].dt.year\nplt_data = emails.groupby('year').size()\n\nplt_data.plot(figsize=(14, 5), color='#f4d641',  marker='o')\n\nx = plt_data.reset_index()['year']\ny = plt_data.reset_index()[0]\nplt.fill_between(x, y, color='#f4d641', alpha = 0.4)\n\nplt.xlabel('Year', fontsize=20)\nplt.ylabel('Emails Amount', fontsize=20)\nplt.title('Number of Emails sent per year ({0}, {1})'.format(min_date, max_date), fontsize=20)\nleg = plt.legend(loc='best', fontsize=15)\nfor text in leg.get_texts():\n    plt.setp(text, color = 'w')\nplt.show()","00b7e92f":"e_m = pd.DataFrame(pd.merge(emails, matches, how='inner', left_on='emails_id', right_on='matches_email_id').groupby('emails_id').size().reset_index()).rename(index=str, columns={0: \"questions_amount\"}).sort_values(by=['questions_amount'], ascending=False)\nplt_data = e_m.groupby('questions_amount').size().reset_index().rename(index=str, columns={0: \"emails_amount\"})\n\nmapping = {\n    1: '1',\n    2: '2',\n    3: '3',\n    4: '4 - 7',\n    8: '8 - 10',\n}\n\ndef get_key(x):\n    for i in range(x, 0, -1):\n        if i in mapping:\n            return mapping[i]\n\n\nplt_data['groups'] = plt_data['questions_amount'].apply(lambda x: '>10' if x >= 11 else get_key(x))\nplt_data = pd.DataFrame({'groups' :['0'], 'emails_amount' : [len(emails) - len(e_m)]}).append(plt_data.groupby('groups').sum().reset_index()[['groups', 'emails_amount']])\n\nplt_data.plot(kind='bar', figsize=(14, 5), color='#57c6b1')\n\nplt.xticks(np.arange(len(plt_data)), tuple(plt_data['groups']))\nplt.xlabel('Questions Count', fontsize=15)\nplt.ylabel('Emails Count', fontsize=15)\nplt.title('Questions contained in each email', fontsize=20)\nleg = plt.legend(loc='best', fontsize=15)\nfor text in leg.get_texts():\n    plt.setp(text, color = 'w')\nplt.show()","5aac5bc6":"from wordcloud import WordCloud","b8ee127c":"entities = [\n    ('students', students),\n    ('professionals', professionals),\n    ('questions', questions)\n]\n\ndfs = []\n\nfor entity, df in entities:\n    if entity == 'questions':\n        df = tag_questions\n        df = pd.merge(df, tags, left_on='tag_questions_tag_id', right_on='tags_tag_id')\n    else:\n        df = tag_users[tag_users['tag_users_user_id'].isin(df['{}_id'.format(entity)])]\n        df = pd.merge(df, tags, left_on='tag_users_tag_id', right_on='tags_tag_id')\n\n    df['entity_type'] = entity\n\n    dfs.append(df)\n\n\nplt_data = pd.concat(dfs)\n\nplt_data = plt_data[['tags_tag_name', 'entity_type']].pivot_table(index='tags_tag_name', columns='entity_type', aggfunc=len, fill_value=0)\n\nfor entity, df in entities:\n    plt_data[entity] = plt_data[entity] \/ len(df)\n\nplt_data['sum'] = (plt_data['professionals'] + plt_data['students'] + plt_data['questions'])\nplt_data = plt_data.sort_values(by='sum', ascending=False).drop(['sum'], axis=1).head(100)\n\n\n# Wordcloud\nplt.figure(figsize=(20, 20))\nwordloud_values = ['students', 'professionals', 'questions']\naxisNum = 1\nfor wordcloud_value in wordloud_values:\n    wordcloud = WordCloud(margin=0, max_words=20, random_state=42).generate_from_frequencies(plt_data[wordcloud_value])\n    ax = plt.subplot(1, 3, axisNum)\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.title(wordcloud_value)\n    plt.axis(\"off\")\n    axisNum += 1\nplt.show()    ","89ebe3e8":"# Top n Professionals with most Answers & Students with most Questions","99d74761":"# Datetime feature preprocessing","e9dba567":"# Users Growth","3c4610ca":"# Look into DataSet","78005afd":"# How many questions are contained in each email\nMost emails contain 1-3 questions, so an average amount of questions per email is 2.33.\n\nAccessing questions is also possible directly from the Career Village website, so professionals are not restricted to answering emails, so contact method should not be assumed. However, we need inferred links between questions and professionals to build a recommender.","c1ec5e37":"# Last activity \nDepending on the last comment, question or answer of a user, we have extracted the last activity date. On the previous plot we have seen, that many users haven't done any activity yet. For the 'last activity' plot we take a look only on users with already have one activity (dropna).","467cee6d":"# Import libraries","25b6b4f3":"# Number of Answers & Questions added per year","485060c6":"# Import Dataset","0466d625":"# Tags Wordclouds \nIn most of the cases, students are not using tags. Student tags are similar to questions tags. The current system is recommending questions tags, and they are not that similar to those which professionals are following.\n\nTags of questions and students and more generalized comparing to professionals tags. It means that even if we apply some processing and modeling techniques and deriving similarities out of it, there still be unmatched student and professionals using tags due to generalized vs. specialized tags problem.\n\nOur model also solves this issue.","b5250b3d":"# First activity after registration \n### There are two general types of users:\n\n- Activity right after registration\n- No activity at all","fda11e4d":"# Professionals Answers & Students Questions amounts","db01189c":"# Number of Emails sent per year\nThe number of emails sent yearly tends to grow each year."}}