{"cell_type":{"6b94eb22":"code","5b5b3a0c":"code","2285a835":"code","cb9c7789":"code","91e60021":"code","8387d33d":"code","e393645a":"code","81ca4f3f":"code","bd0290e4":"code","e3c0e8be":"code","725c524e":"code","33482a60":"code","8e4400e7":"code","21eb5064":"code","2e718b77":"code","97bdadf0":"code","602b8d96":"code","87309856":"code","3daeac16":"code","86b5de8e":"code","849e3c47":"code","affd59e0":"code","b85a443a":"code","54a7a1d0":"code","8f2ac4d8":"code","cbd15265":"code","be0b4de7":"code","99e31d55":"code","d857c0eb":"code","d1913413":"code","5a897a88":"code","8325509e":"code","e47967bd":"code","0de06833":"code","f815bee8":"code","b7c4ad7e":"code","cca3c831":"code","f3d21edb":"code","de3672a8":"code","d8c2047b":"code","311cc0cb":"markdown","e944c1f2":"markdown","5f2c79e5":"markdown","a0f39f31":"markdown","76237f96":"markdown","4dd36529":"markdown","f125c22a":"markdown","a9664f6a":"markdown","95600fd2":"markdown","1e23d8f6":"markdown","807cd1fb":"markdown","315f525c":"markdown","53cb64a6":"markdown","f20ae38f":"markdown","165a4280":"markdown","6bcde226":"markdown","e92a8814":"markdown"},"source":{"6b94eb22":"!pip install pydicom","5b5b3a0c":"import os\nimport pydicom as dicom\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport cv2\n#from google.colab.patches import cv2_imshow","2285a835":"from tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision.datasets as datasets\nfrom torchvision.transforms import transforms as transform","cb9c7789":"if torch.cuda.is_available():\n    device = torch.device(\"cuda:0\")\n    print(\"Running on GPU ->\",device)\nelse:\n    device = torch.device(\"cpu\")\n    print(\"Running on CPU ->\",device)","91e60021":"'''\nfrom google.colab import drive\ndrive_location = \"\/content\/drive\/\"\ndrive.mount(drive_location)\nos.chdir(\"\/content\/drive\/MyDrive\/SPIE_AAPM\")\nspied_aapm = os.getcwd()\n'''","8387d33d":"def prep_data(img_data):\n    p_data = []\n    for j in range(len(img_data)):\n        tmp = np.array(img_data[j])\n        tmp = tmp.reshape(512,1024)\n        tmp = cv2.resize(tmp, (256,256),interpolation = cv2.INTER_NEAREST)\n        #cv2_imshow(tmp)\n        p_data.append(tmp)\n        if j>=320-1:\n            break\n    if(len(p_data)<320):\n        l = len(p_data)\n        l = 320-l\n        for k in range(l):\n            p_data.append(np.ones((256,256)))\n    return np.array(p_data)\n","e393645a":"def sliding_window(x):\n    p,q,r = x.shape\n    #print(p,q,r)\n    count = 0\n    slid_value = []\n    for k in range(0,p,32):\n        for i in range(0,q,32):\n            for j in range(0,r,32):\n                tmp = 0\n                for f in range(k,k+32):\n                    for g in range(i,i+32):\n                        for h in range(j,j+32):\n                            tmp += x[f][g][h]**2\n\n                tmp = np.sqrt(tmp)\n                slid_value.append([tmp,[i,j,k]])\n    slid_value.sort(reverse=True)\n    top_8 = slid_value[:8]\n    top_8_value = []\n    for t in top_8:\n        _,pos = t\n        i,j,k = pos[0],pos[1],pos[2]\n        value = []\n        for p in range(k,k+32):\n            for q in range(i,i+32):\n                for r in range(j,j+32):\n                    value.append(x[k][j][i])\n        top_8_value.append(np.array(value).reshape(32,32,32))\n    result = np.zeros((64,64,64),dtype=np.double)\n    result[0:32,0:32,0:32] = top_8_value[0]\n    result[0:32,0:32,32:64] = top_8_value[1]\n    result[0:32,32:64,0:32] = top_8_value[2]\n    result[0:32,32:64,32:64] = top_8_value[3]\n    \n    result[32:64,0:32,0:32] = top_8_value[4]\n    result[32:64,0:32,32:64] = top_8_value[5]\n    result[32:64,32:64,0:32] = top_8_value[6]\n    result[32:64,32:64,32:64] = top_8_value[7]\n    return result","81ca4f3f":"'''data = []\nfolder=[]\ni=0\nfor dir in os.listdir():\n    person_data = []\n    path = os.path.join(spied_aapm,dir)\n    print(path)\n    folder.append(dir)\n    for dir1 in os.listdir(path):\n        path1 = os.path.join(path,dir1)\n        for dir2 in os.listdir(path1):\n            path2 = os.path.join(path1,dir2)\n            \n            for d in os.listdir(path2):\n                final_path = os.path.join(path2,d)\n                image_data = dicom.dcmread(final_path)\n                #plt.imshow(image_data.pixel_array, cmap=plt.cm.bone)\n                #plt.show()\n                person_data.append(list(image_data.PixelData))\n                #np_data = image_data.pixel_array\n    p_data = prep_data(person_data)\n    p_data = sliding_window(p_data)\n    #print(p_data.shape)\n    data.append(p_data)\n    i +=1\n    #if(i==NUM_OF_PATIENT):\n        #break\n    print(\"next :\",i+1)'''\n\n'''np.save('\/content\/drive\/MyDrive\/Lung_cancer_data.npy',np.array(data))'''\n","bd0290e4":"ydata = [0,0,1,0,0,1,\n         1,1,0,1,\n         0,0,1,0,1,1,0,0,\n         1,0,0,0,1,1,1,0,\n         1,0,1,0,1,0,1,0,\n         1,0,0,0,1,0,0,1,\n         0,0,0,1,1,0,1,0,\n         0,1,1,0,1,1,1,1,\n         1,1,0,1,0,1,1,1,\n         0,0,0,1\n         ]","e3c0e8be":"!pip install gdown","725c524e":"!gdown https:\/\/drive.google.com\/uc?id=1-WrfioKW1aHGvlMoVRaGirb0M3E7Aqxh","33482a60":"data = np.load(\"Lung_cancer_data.npy\",allow_pickle=True)","8e4400e7":"data[0].shape\nxdata = np.array(data)\nxdata.shape","21eb5064":"xdata = torch.tensor(xdata)\nxdata.shape","2e718b77":"xdata = xdata.reshape(70,1,64,64,64)\nxdata.shape","97bdadf0":"ydata=np.array(ydata)\nydata= torch.from_numpy(ydata)\nydata.shape","602b8d96":"train_xdata = xdata[0:56]\ntrain_ydata = ydata[0:56]\ntest_xdata = xdata[56:]\ntest_ydata = ydata[56:]","87309856":"test_ydata","3daeac16":"class Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv3d(1,256 ,5,stride=2)\n        self.dropout1 = nn.Dropout(0.5)\n        self.conv2 = nn.Conv3d(256,128,5,stride=2)\n        self.dropout2 = nn.Dropout(0.5)\n        self.conv3 = nn.Conv3d(128,64,5,stride=2)\n        self.dropout3 = nn.Dropout(0.5)\n        self.conv4 = nn.Conv3d(64,32,5,stride=2)\n        self.dropout4 = nn.Dropout(0.5)\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(32,2)\n    \n    \n    def forward(self,x):\n        x = F.leaky_relu(self.conv1(x))\n        x = self.dropout1(x)\n        #print(x.shape)\n        x = F.leaky_relu(self.conv2(x))\n        x = self.dropout2(x)\n        #print(x.shape)\n        x = F.leaky_relu(self.conv3(x))\n        x = self.dropout3(x)\n        #print(x.shape)\n        x = F.leaky_relu(self.conv4(x))\n        x = self.dropout4(x)\n        #print(x.shape)\n        x = self.flatten(x)\n        #print(x.shape)\n        #x = F.relu(self.fc1(x))\n        x = F.softmax(self.fc1(x),1)\n        return x","86b5de8e":"model = Model()\nprint(model)\nmodel = model.double()\nmodel.to(device)\nprint()","849e3c47":"optimizerCNN = optim.Adam(model.parameters(),lr=0.1)\nloss_functionCNN = nn.CrossEntropyLoss()","affd59e0":"def trainCNN(train_xdata,train_ydata,model,optimizer,loss_function,EPOCHS=3,batch_size=8):\n    loss_list=[]\n    for epoch in tqdm(range(EPOCHS)):\n        for i in range(0,train_xdata.shape[0],batch_size):\n            x = train_xdata[i:i+batch_size]\n            y = train_ydata[i:i+batch_size]\n            \n            \n            model.zero_grad()\n            outputs = model(x)\n            \n            loss = loss_function(outputs,y.to(device))\n            \n            loss.backward()\n            optimizer.step()\n        if epoch%5==0:\n            print(\"  loss ->\",loss.item())    \n        loss_list.append(loss.item())\n    return loss_list","b85a443a":"loss_list = trainCNN(train_xdata.to(device),train_ydata,model,optimizerCNN,loss_functionCNN,EPOCHS=100)","54a7a1d0":"plt.plot(loss_list)","8f2ac4d8":"model.eval()","cbd15265":"def accuracy(model,test_xdata,test_ydata):\n    count = 0\n    for i in range(test_xdata.shape[0]):\n        out = model(test_xdata[i:i+1].to(device))\n        #print(out[0])\n        if(torch.argmax(out[0]).item()==test_ydata[i].item()):\n            count += 1\n    return count\/test_xdata.shape[0]\n\n","be0b4de7":"print(\"Accuracy ->\",accuracy(model,test_xdata,test_ydata)*100,\" %\")\n","99e31d55":"class conv_block(nn.Module):\n    def __init__(self, in_channels, out_channels, **kwargs):\n        super(conv_block, self).__init__()\n        self.relu = nn.ReLU()\n        self.conv = nn.Conv3d(in_channels, out_channels, **kwargs)\n        self.batchnorm = nn.BatchNorm3d(out_channels)\n\n    def forward(self, x):\n        return self.relu(self.batchnorm(self.conv(x)))","d857c0eb":"class Inception_block(nn.Module):\n    def __init__(self, in_channels, out_1x1, red_3x3, out_3x3, red_5x5, out_5x5, out_1x1pool):\n        super(Inception_block, self).__init__()\n        self.branch1 = conv_block(in_channels, out_1x1, kernel_size=(1, 1, 1))\n        self.branch2 = nn.Sequential(conv_block(in_channels, red_3x3, kernel_size=(1, 1, 1)),conv_block(red_3x3, out_3x3, kernel_size=(3, 3, 3), padding=(1, 1, 1)),)\n        self.branch3 = nn.Sequential(conv_block(in_channels, red_5x5, kernel_size=(1, 1, 1)),conv_block(red_5x5, out_5x5, kernel_size=(5, 5, 5), padding=(2, 2, 2)),)\n        self.branch4 = nn.Sequential(nn.MaxPool3d(kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1)),conv_block(in_channels, out_1x1pool, kernel_size=(1, 1, 1)),)\n\n    def forward(self, x):\n        return torch.cat([self.branch1(x), self.branch2(x), self.branch3(x), self.branch4(x)], 1)","d1913413":"class GoogleNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv3d(1,64,7)\n        self.batch_norm1 = nn.BatchNorm3d(64)\n\n        self.max_pool2 = nn.MaxPool3d(2,2,1)\n\n        self.conv3 = nn.Conv3d(64,192,3)\n        self.batch_norm3 = nn.BatchNorm3d(192)\n\n        self.inception3a = Inception_block(192, 64, 96, 128, 16, 32, 32)\n\n        self.inception3b = Inception_block(256, 128, 128, 192, 32, 96, 64)\n\n        self.maxpool3 = nn.MaxPool3d(3,2,1)\n\n        self.inception4a = Inception_block(480, 192, 96, 208, 16, 48, 64)\n\n        self.inception4b = Inception_block(512, 160, 112, 224, 24, 64, 64)\n\n        self.inception4c = Inception_block(512, 128, 128, 256, 24, 64, 64)\n\n        self.inception4d = Inception_block(512, 112, 144, 288, 32, 64, 64)\n\n        self.inception4e = Inception_block(528, 256, 160, 320, 32, 128, 128)\n\n        self.maxpool4 = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n\n        self.inception5a = Inception_block(832, 256, 160, 320, 32, 128, 128)\n\n        self.inception5b = Inception_block(832, 384, 192, 384, 48, 128, 128)\n\n        self.avgpool = nn.AvgPool3d(kernel_size=7, stride=1)\n\n        self.dropout = nn.Dropout(p=0.8)\n\n        self.flatten = nn.Flatten()\n        \n        self.fc1 = nn.Linear(1024,2)\n\n\n    def forward(self,x):\n        x = F.relu(self.batch_norm1(self.conv1(x)))\n        x = self.max_pool2(x)\n        x = F.relu(self.batch_norm3(self.conv3(x)))\n        x = self.inception3a(x)\n        x = self.inception3b(x)\n        x = self.maxpool3(x)\n        x = self.inception4a(x)\n        x = self.inception4b(x)\n        x = self.inception4c(x)\n        x = self.inception4d(x)\n        x = self.inception4e(x)\n        x = self.maxpool4(x)\n        x = self.inception5a(x)\n        x = self.inception5b(x)\n        x = self.avgpool(x)\n        x = self.dropout(x)\n        x = self.flatten(x)\n        x = self.fc1(x)\n\n        return F.softmax(x,1)\n\n","5a897a88":"google_net = GoogleNet()\ngoogle_net.double()\ngoogle_net.to(device)","8325509e":"inception_optimizer = optim.Adam(google_net.parameters(),lr=0.005)\ninception_loss_function = nn.CrossEntropyLoss()","e47967bd":"#inception_loss_function(out.to(device),train_ydata[0:2].to(device))","0de06833":"\ndef train(train_xdata,train_ydata,model,optimizer,loss_function,EPOCHS=3,batch_size=2):\n    loss_list=[]\n    for epoch in tqdm(range(EPOCHS)):\n        for i in range(0,train_xdata.shape[0],batch_size):\n            x = train_xdata[i:i+batch_size]\n            y = train_ydata[i:i+batch_size]\n            \n            \n            model.zero_grad()\n            outputs = model(x)\n            \n            loss = loss_function(outputs,y.to(device))\n            \n            loss.backward()\n            optimizer.step()\n        if epoch%5==0:\n            print(\"  loss ->\",loss.item())    \n        loss_list.append(loss.item())\n    return loss_list","f815bee8":"train_xdata.shape\ntrain_ydata.shape","b7c4ad7e":"loss_list = train(train_xdata.to(device),train_ydata,google_net,inception_optimizer,inception_loss_function,EPOCHS=25)","cca3c831":"google_net.eval()\nprint()","f3d21edb":"plt.plot(loss_list)","de3672a8":"def accuracy(model,test_xdata,test_ydata):\n    count = 0\n    for i in range(test_xdata.shape[0]):\n        out = model(test_xdata[i:i+1].to(device))\n        if(torch.argmax(out[0]).item()==test_ydata[i].item()):\n            count += 1\n    return count\/test_xdata.shape[0]","d8c2047b":"print(\"Accuracy ->\",accuracy(google_net,test_xdata,test_ydata)*100,\" %\")","311cc0cb":"# Accuracy 3D GoogleNet","e944c1f2":"# Traning 3D GoogleNet","5f2c79e5":"# **Lung Cancer detection using 3D-CNN**","a0f39f31":"# Setting optimizer parameters, loss function","76237f96":"# Traning 3D-CNN","4dd36529":"# Loss function for 3D-CNN","f125c22a":"# `**GoogleNet Architecture**","a9664f6a":"# Input for sliding window : 256x256x320 \n# Output for sliding window : 64x64x64 \n\n# Input of 3dGoogle Net\/ 3dCNN : 64x64x64 \n# Output of 3dGoogle Net\/3dCNN : 2 classes (cancerous\/ not cancerous)\n","95600fd2":"## setting y_data","1e23d8f6":"### Loading required packages","807cd1fb":"# **Sliding Window**","315f525c":"# Simple 3D CNN Architecture :","53cb64a6":"# Accuracy for 3D-CNN","f20ae38f":"# DATASET : SPIE_AAPM\n# 70 patient CT scans (56 for Traning and 14 for testing)","165a4280":"# Setting Optimizer and Loss function for 3D GoogleNet","6bcde226":"# Project Members \n### 1. AMIT KUSHWAHA (MIT2020031) \n### 2. Majithia Tejas Vinodbhai (MIT2020058)  \n### 3. Ranjith Kalingeri (MIT2020017) \n### 4. Tarun Phate (MIT2020080) ","e92a8814":"# Preprocessing data"}}