{"cell_type":{"c6af9ecc":"code","02a7bd98":"code","6270ea52":"code","7032ad0c":"code","20d681a3":"code","92508a56":"code","77592218":"code","32a8c451":"code","d83aac18":"code","c5ddb5f7":"code","21c390ff":"code","b1603c4c":"code","6c149c2c":"code","0286f7bb":"code","d303b50c":"code","71ac9025":"code","2f72ae98":"code","23f68057":"code","32acc4e1":"code","bf2949d3":"code","c2e93f2d":"code","55136d33":"code","143d80af":"code","4a9abf13":"code","20f54e41":"code","cfb34a55":"code","40e40952":"code","3497bb5f":"code","c2936046":"code","5a98848f":"code","f2c5cedb":"code","61ec8ca0":"code","f3dfbd21":"code","40b8629f":"code","50be535d":"code","ed43706f":"code","b793fe0b":"code","5791cc8a":"code","d5ab0c90":"code","20e11495":"markdown","af1243d1":"markdown","b7008625":"markdown","c4ae9956":"markdown","9a3a954e":"markdown","2c903f6c":"markdown","7da1f2ef":"markdown","d110b5f2":"markdown","190b2cff":"markdown","2ea11ed8":"markdown","1f2f26aa":"markdown","d868035d":"markdown","b3bd61e8":"markdown","156e4bd6":"markdown","b38c580c":"markdown","305fc74e":"markdown","bc4a2d13":"markdown","73f262ca":"markdown","7d1ce2ea":"markdown","3ff2849a":"markdown","e0cb746f":"markdown","6d563b84":"markdown","adee4de3":"markdown","791734ad":"markdown"},"source":{"c6af9ecc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","02a7bd98":"df_train = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ndf_train.head()","6270ea52":"X_train = df_train.drop('label',axis=1)\ny_train = df_train['label']\n\ny_train.shape, y_train[0].shape","7032ad0c":"X_train.head(2)","20d681a3":"X_train.shape, X_train.loc[0].shape, y_train.shape","92508a56":"df_test = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\ndf_test.head()\n","77592218":"X_test = df_test","32a8c451":"import matplotlib.pyplot as plt\nimport random\n\n\nplt.figure(figsize=(7,7))\n\nfor i in range(4):\n    ax = plt.subplot(2,2,i+1) # Create a matrix of 2 by 2 (4) images\n    random_index = random.choice(range(len(X_train))) # get a random index value \n    plt.imshow(np.array(X_train.loc[random_index]).reshape(28,28)) # to visualise the digit the image has to reshaped to 28*28 size \n    plt.title(y_train[random_index]) # put the label at the top of the image\n    plt.axis(False)\n\n","d83aac18":"X_train = X_train\/255.\nX_test = X_test\/255.","c5ddb5f7":"# Randomly check some values\n\nX_train.loc[0].max(), X_train.loc[279].max(), X_test.loc[2].max()","21c390ff":"from sklearn.model_selection import train_test_split","b1603c4c":"X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2)\nprint(f\"X_train shape: {X_train.shape}\")\nprint(f\"X_valid shape: {X_valid.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\nprint(f\"y_valid shape: {y_valid.shape}\")","6c149c2c":"import tensorflow as tf","0286f7bb":"import warnings\nwarnings.filterwarnings('ignore')","d303b50c":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import Adam","71ac9025":"# Build the model\nmodel_1 = Sequential([\n          Dense(20,activation='relu'),\n          Dense(20, activation='relu'),\n          Dense(10,activation='softmax')   # Softmax activation for the output layer for multiclass classification\n])\n\n# Compile the model\nmodel_1.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), # we use SparseCategorical Entropy in the \n                                                                        # case of the output labels being in integer form\n               optimizer = Adam(),\n               metrics=['accuracy'])\n\n# Fit the model\nhistory_1 = model_1.fit(X_train, y_train, \n                        epochs=50,\n                        validation_data = (X_valid, y_valid)\n                        )","2f72ae98":"model_1.summary()","23f68057":"model_1.evaluate(X_valid, y_valid)","32acc4e1":"# define a function to plot loss and accuracy curves\n\ndef plot_loss_curves(history):\n    \n    '''\n    Plot the accuracy and loss curves using the history obtained\n    from fitting the model\n    '''\n    \n    # Get the accuracy and loss values from the history \n    tr_loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    \n    tr_acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n    \n    epochs = range(len(tr_loss)) # get the epochs as a list to plot along the x-axis\n    \n    \n\n    # We will separate the accuracy and the loss curves into separate plots\n\n    plt.figure(figsize=(8,12))\n    \n    # Loss curves\n    ax = plt.subplot(2,1,1)\n    plt.plot(epochs, tr_loss, label='Training Loss')\n    plt.plot(epochs, val_loss, label='Validation Loss')\n    plt.xlabel('epochs')\n    plt.ylabel('loss')\n    plt.title('Loss Curves')\n    plt.legend();\n    \n    # Accuracy curves\n    ax = plt.subplot(2,1,2)\n    plt.plot(epochs, tr_acc, label='Training Accuracy')\n    plt.plot(epochs, val_acc, label='Validation Accuracy')\n    plt.xlabel('epochs')\n    plt.title('Accuracy Curves')\n    plt.legend();\n\n","bf2949d3":"plot_loss_curves(history_1)","c2e93f2d":"# Build and compile the model\nmodel_2 = Sequential([\n            Dense(10, activation='relu'),\n            Dense(5, activation='relu'),\n            Dense(10, activation='softmax')]\n)\n\nmodel_2.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n               optimizer=Adam(),\n               metrics=['accuracy'])","55136d33":"# Fit the model \nhistory_2 = model_2.fit(X_train, y_train,\n                       epochs=50,\n                       validation_data=(X_valid, y_valid)\n                       )","143d80af":"model_2.evaluate(X_valid, y_valid)","4a9abf13":"plot_loss_curves(history_2)","20f54e41":"# Build and compile the model\nmodel_3 = Sequential([\n            Dense(20,activation='relu'),\n            Dense(20,activation='relu'),\n            Dense(10,activation='softmax')\n])\n\nmodel_3.compile(loss= tf.keras.losses.SparseCategoricalCrossentropy(),\n               optimizer = Adam(),\n               metrics=['accuracy'])","cfb34a55":"# define the LearningRateScheduler Callback\nlr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4 * 10**(epoch\/20))","40e40952":"1e-4","3497bb5f":"history_3 = model_3.fit(X_train, y_train,\n                       epochs=100,\n                       validation_data = (X_valid, y_valid),\n                       callbacks =[lr_scheduler]\n                       )","c2936046":"learning_rate = history_3.history['lr']\n","5a98848f":"max(learning_rate), min(learning_rate)","f2c5cedb":"np.log(1e-4 * 10*100\/20)","61ec8ca0":"# Plot the learning rate versus the loss\nlrs = 1e-4 * (10 ** (np.arange(100)\/20))\nplt.figure(figsize=(10, 7))\nplt.semilogx(lrs, history_3.history[\"loss\"]) # we want the x-axis (learning rate) to be log scale\nplt.xlabel(\"Learning Rate\")\nplt.ylabel(\"Loss\")\nplt.title(\"Learning rate vs. loss\");\n","f3dfbd21":" # Build and compile the model\nmodel_4 = Sequential([\n            Dense(20,activation='relu'),\n            Dense(20,activation='relu'),\n            Dense(10,activation='softmax')\n])\n\nmodel_4.compile(loss= tf.keras.losses.SparseCategoricalCrossentropy(),\n               optimizer = Adam(learning_rate=0.005),\n               metrics=['accuracy'])","40b8629f":"history_4 = model_4.fit(X_train, y_train,\n                       epochs=100,\n                       validation_data = (X_valid, y_valid),\n                       callbacks =[lr_scheduler]\n                       )","50be535d":" # Build and compile the model\nmodel_5 = Sequential([\n            Dense(20,activation='relu'),\n            Dense(20,activation='relu'),\n            Dense(10,activation='softmax')\n])\n\nmodel_5.compile(loss= tf.keras.losses.SparseCategoricalCrossentropy(),\n               optimizer = Adam(learning_rate=0.005),\n               metrics=['accuracy'])\n\nhistory_5 = model_5.fit(X_train, y_train,\n                       epochs=30,\n                       validation_data = (X_valid, y_valid),\n                       callbacks =[lr_scheduler]\n                       )","ed43706f":"plot_loss_curves(history_5)","b793fe0b":"model_5.evaluate(X_valid, y_valid)","5791cc8a":"final_pred = model_5.predict(X_test)\nfinal_pred.shape\n\n","d5ab0c90":"y_pred = final_pred.argmax(axis=1)\nImageID = np.arange(len(y_pred))+1\nOut = pd.DataFrame([ImageID,y_pred]).T\nOut.rename(columns = {0:'ImageId', 1:'Label'})\n#Out\nOut.to_csv('submission.csv', header =  ['ImageId', 'Label' ], index = None)","20e11495":"#### Load and explore the data","af1243d1":"It seems that with this learning rate we, achieve the maximum accuracy in only about 30 epochs. So lets train another model with 30 epochs, all other parameters same as model 4","b7008625":"The test data does not have the **label** column, so we can assign this to X_test","c4ae9956":"# Model_2","9a3a954e":"# Baseline Model \n\nNow we are ready to build our baseline model. Do note that I am building a Neural Network for classification, rather than a CNN ","2c903f6c":"#### Test Data","7da1f2ef":"####  Train Data","d110b5f2":"Lets evaluate the model ","190b2cff":"# Data Transformation","2ea11ed8":"# Model 4","1f2f26aa":"# Model 5","d868035d":"# Data Visualisation\n**Visualize some random images alongwith their labels from the train dataset**","b3bd61e8":"From the above curve we can see that when learning rate is about .005, the loss is still decreasing but is still not flattened out","156e4bd6":"Lets take 0.005 as our learning and retrain our model","b38c580c":"#  Data Extraction & Exploration","305fc74e":"The model_2 seems to performing worse than our baseline model. Lets train it for some more epochs to see if it is overfitting","bc4a2d13":"**Divide the training data into train and validation data**","73f262ca":"We can see that we are getting quite a good training and validation accuracy even with our baseline model. Lets evaluate our model further","7d1ce2ea":"****The data is in the form of image labels and image pixels. Lets separate it into features X and target Y****","3ff2849a":"# Model Evaluation","e0cb746f":"#### We can see that the model is overfitting. There are many ways this can be addressed, some of which are :\n* Reducing the number of neurons in hidden layers\n* Reducing the number of hidden layers\n* Changing the learning rate\n\nI will start by the first option, by reducing the number of neurons from 20 to 10 ","6d563b84":"**Standardize the input data**\nAs this is a image data, we can obtain values between 0 and 1 by dividing by 255, which is the max value of pixel","adee4de3":"**From these curves above we can see that though the overfitting has reduced as compared to model_1, it is still there. Lets introducing LearningRate Scheduler callback in our model. This changes the leraning rate as the number of epochs change, thus helping the model fit better.**","791734ad":"# Model_3"}}