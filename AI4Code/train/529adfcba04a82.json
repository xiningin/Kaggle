{"cell_type":{"94691cc5":"code","aadcd538":"code","12957d37":"code","2b3af1f7":"code","e10eb7c9":"code","6ce1009a":"code","40d347dd":"code","c03bab5e":"code","87c7a7ae":"code","698ffb36":"code","5e3a87f9":"code","7b36365d":"code","7b2ce4e3":"code","51ed36bd":"code","5ff776d2":"code","271ad727":"code","aee8a1a7":"code","ff757354":"code","39b863c1":"code","f8dcde69":"code","5475be2d":"code","f4ed281d":"code","d1609e59":"code","7df629bc":"code","37071514":"code","e149ae2e":"code","95fce175":"code","9e4bd0c4":"code","ac96e9f7":"code","6b9a347e":"code","cbdf0512":"code","410d9d18":"code","83dc12f6":"code","ba8dd8f4":"code","241016ea":"code","42fc9b7f":"code","2040477f":"code","a16bc32a":"code","c7f520c7":"code","61dbfbd1":"code","a4fc9875":"code","43014404":"code","bcde289a":"code","78228379":"code","7dca0626":"code","cadfd9a1":"code","ddb62d42":"markdown","cf790a15":"markdown","a253aaab":"markdown","4181175d":"markdown","783378c5":"markdown","cbbb5233":"markdown","2cc9d9c7":"markdown","2158ede2":"markdown","eda18fdc":"markdown","ccb6d3b6":"markdown","26854c19":"markdown"},"source":{"94691cc5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow import keras\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","aadcd538":"train = pd.read_csv('..\/input\/mockdatadec\/MOCK_DATA_DEC.csv')\ntest = pd.read_csv('..\/input\/mockedtest\/MOCK_TEST.csv')","12957d37":"train[train['x1'] == 0.01] = None\ntrain[train['x2'] == 0.01] = None\ntest[test['x1'] == 0.01] = None\ntest[test['x2'] == 0.01] = None\ntrain[train['x1'] == 0.00] = None\ntrain[train['x2'] == 0.00] = None\ntest[test['x1'] == 0.00] = None\ntest[test['x2'] == 0.00] = None\ntrain[train['x1'] == -0.01] = None\ntrain[train['x2'] == -0.01] = None\ntest[test['x1'] == -0.01] = None\ntest[test['x2'] == -0.01] = None","2b3af1f7":"#train['id'] = train.id.astype('int')","e10eb7c9":"train.loc[(train.x1 > 0) & (train.x2 < 0), 'target'] = 1\ntrain.loc[(train.x1 < 0) & (train.x2 > 0), 'target'] = 1\ntrain.loc[train.target != 1, 'target'] = 0","6ce1009a":"train.head(10)","40d347dd":"train.describe()","c03bab5e":"x = train.x1\ny = train.x2","87c7a7ae":"target = train.target","698ffb36":"x1 = train.loc[train.target > 0].x1\ny1 = train.loc[train.target > 0].x2\nx2 = train.loc[train.target == 0].x1\ny2 = train.loc[train.target == 0].x2\n","5e3a87f9":"plt.plot(x1,y1,'o',color='orange')\nplt.plot(x2,y2,'bo')\nplt.show()","7b36365d":"test.head(10)","7b2ce4e3":"x_test = test.x1\ny_test = test.x2","51ed36bd":"plt.plot(x_test,y_test, 'ro')\nplt.show()","5ff776d2":"train.isna().sum()","271ad727":"test.isna().sum()","aee8a1a7":"train.dropna(0, inplace=True)\ntest.dropna(0, inplace=True)\nprint('done')","ff757354":"test.isna().sum()","39b863c1":"test['id'] = test.id.astype('int')","f8dcde69":"target = train.target","5475be2d":"del train['target'], train['id']","f4ed281d":"x_train, x_validation, y_train, y_validation = train_test_split(train.values, target.values, test_size=0.2, random_state=42)","d1609e59":"optimAdam = keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-8)","7df629bc":"model = Sequential()\nmodel.add(Dense(4,activation='relu'))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer=optimAdam, metrics=['accuracy'])","37071514":"bst_model_path = 'model.h5'\nmodel_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)","e149ae2e":"history = model.fit(x_train, y_train, validation_data=([x_validation], y_validation), epochs=50, batch_size=50, callbacks=[model_checkpoint], verbose=2)\n","95fce175":"plt.plot(history.history['val_loss'])\nplt.plot(history.history['loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","9e4bd0c4":"model.load_weights('.\/model.h5')","ac96e9f7":"test_ids = test.id","6b9a347e":"del test['id']","cbdf0512":"preds = model.predict_classes(test, batch_size=10, verbose=2)","410d9d18":"test['target'] = preds","83dc12f6":"print(test.head(10))","ba8dd8f4":"x1_test = test.loc[test.target > 0].x1\ny1_test = test.loc[test.target > 0].x2\nx2_test = test.loc[test.target == 0].x1\ny2_test = test.loc[test.target == 0].x2","241016ea":"old_x1 = x1_test\nold_y1 = y1_test\nold_x2 = x2_test\nold_y2 = y2_test","42fc9b7f":"plt.figure(figsize=(9, 4))\nplt.subplot(121)\nplt.title('Predicted test')\nplt.plot(x1_test,y1_test,'o',color='orange')\nplt.plot(x2_test,y2_test,'bo')\nplt.subplot(122)\nplt.title('Train dataset')\nplt.plot(x1,y1,'o',color='orange')\nplt.plot(x2,y2,'bo')\nplt.show()","2040477f":"train['x1x2'] = train['x1'] * train['x2']\ntest['x1x2'] = test['x1'] * test['x2']","a16bc32a":"train.head(10)","c7f520c7":"x_train, x_validation, y_train, y_validation = train_test_split(train.values, target.values, test_size=0.2, random_state=4)","61dbfbd1":"model = Sequential()\nmodel.add(Dense(4,activation='relu'))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer=optimAdam, metrics=['accuracy'])\nbst_model_path = 'model.h5'\nmodel_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)","a4fc9875":"history = model.fit(x_train, y_train, validation_data=([x_validation], y_validation), epochs=50, batch_size=50, callbacks=[model_checkpoint], verbose=2)","43014404":"plt.plot(history.history['val_loss'])\nplt.plot(history.history['loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","bcde289a":"del test['target']","78228379":"model.load_weights('.\/model.h5')\npreds = model.predict_classes(test, batch_size=10, verbose=2)\ntest['target'] = preds","7dca0626":"x1_test = test.loc[test.target > 0].x1\ny1_test = test.loc[test.target > 0].x2\nx2_test = test.loc[test.target == 0].x1\ny2_test = test.loc[test.target == 0].x2","cadfd9a1":"plt.figure(figsize=(9, 4))\nplt.subplot(121)\nplt.title('New Results')\nplt.plot(x1_test,y1_test,'o',color='orange')\nplt.plot(x2_test,y2_test,'bo')\nplt.subplot(122)\nplt.title('Old results')\nplt.plot(old_x1,old_y1,'o',color='orange')\nplt.plot(old_x2,old_y2,'bo')\nplt.show()","ddb62d42":"<h1> 1. EXPLORE THE DATA <\/h1>\n<h2> a) Load the data <\/h2>","cf790a15":"<h2>b) Print first ten rows<\/h2>","a253aaab":"<h1>7. Simple feature engineering<\/h1>","4181175d":"<h1> 2. Clean it <\/h1>","783378c5":"<h1> 5. Create a neural network model <\/h1>","cbbb5233":"<h2> d) Explore test dataset <\/h2>","2cc9d9c7":"<h1> 4. Split the data on training set and validation set<\/h1>","2158ede2":"<h1> 6. Everything is ready - let's test it<\/h1>","eda18fdc":"<h2>c) Statistic details<\/h2>","ccb6d3b6":"<h1>New and old results<\/h1>","26854c19":"<h1> 3. Select target <\/h1>"}}