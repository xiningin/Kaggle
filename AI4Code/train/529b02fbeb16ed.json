{"cell_type":{"53c2424c":"code","dfe6265f":"code","c07dcce0":"code","f53a0de8":"code","eac0f49f":"code","368832e4":"code","cd263af9":"code","7720fd38":"code","8c974f76":"code","34c7756f":"code","b156166f":"code","48a4e184":"code","9973ca24":"code","c0b96ae7":"code","0f819147":"code","277a7689":"code","9c88a52d":"code","0bf9f1f0":"code","f3303d98":"code","752939f6":"code","89321249":"code","cd2b24df":"code","b5fe09c1":"markdown","d8f70c31":"markdown","f9198223":"markdown","0c44a029":"markdown","a42a9de8":"markdown","335c37f0":"markdown","4180edf2":"markdown","dcb70aed":"markdown","ef719c18":"markdown","a8fdff71":"markdown","1a3dec2d":"markdown","72f547e9":"markdown","03cb06d1":"markdown","1ce17f47":"markdown","c96bf69f":"markdown","1b65a9bf":"markdown","5a972e8b":"markdown","5a6852bf":"markdown","9747b8c2":"markdown","6daa8cd2":"markdown","d107f5a7":"markdown","d80d6bc6":"markdown","f70b50b9":"markdown","119488f2":"markdown","a4a44e0a":"markdown","de101364":"markdown","1f234edd":"markdown","b489c0d4":"markdown"},"source":{"53c2424c":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport re\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport random\nimport nltk\n\nnltk.download('punkt')\nnltk.download('stopwords')","dfe6265f":"movies = pd.read_csv('\/kaggle\/input\/movie-lens-small-latest-dataset\/movies.csv')\nmovies['title'] = movies['title'].str.strip().str[:-7]\nmovies['genres'] = movies['genres'].str.replace('|', ' ')\nuser_ratings = pd.read_csv('\/kaggle\/input\/movie-lens-small-latest-dataset\/ratings.csv')\nuser_ratings = user_ratings.drop(columns=['timestamp'])","c07dcce0":"top_N = 25\n\ntxt = movies.title.str.lower().str.cat(sep=' ')\n\nwords = nltk.tokenize.word_tokenize(txt)\nword_dist = nltk.FreqDist(words)\n\nstopwords = nltk.corpus.stopwords.words('english')\nstopwords = stopwords + [')', '(', ',', ':', \"'s\", '.', '!', '&', '?']\nwords_except_stop_dist = nltk.FreqDist(w for w in words if w not in stopwords) \n\nprint('All frequencies:')\nprint('=' * 60)\nrslt = pd.DataFrame(words_except_stop_dist.most_common(top_N),\n                    columns=['Word', 'Frequency']).set_index('Word').head(10)\nprint(rslt)\nprint('=' * 60)\n\nrslt = pd.DataFrame(words_except_stop_dist.most_common(top_N),\n                    columns=['Word', 'Frequency']).set_index('Word')\n\nmatplotlib.style.use('ggplot')\nrslt.plot.bar(rot=0, figsize=(18, 10), fontsize=12)","f53a0de8":"top_N = 25\n\ntxt = movies.genres.str.lower().str.cat(sep=' ')\nwords = nltk.tokenize.word_tokenize(txt)\nword_dist = nltk.FreqDist(words)\n\nstopwords = nltk.corpus.stopwords.words('english')\nstopwords = stopwords + [')', '(', ',', ':', \"'s\", '.', '!', '&', '?']\nwords_except_stop_dist = nltk.FreqDist(w for w in words if w not in stopwords) \n\nprint('All frequencies:')\nprint('=' * 60)\nrslt = pd.DataFrame(words_except_stop_dist.most_common(top_N),\n                    columns=['Word', 'Frequency']).set_index('Word').head(25)\nprint(rslt)\nprint('=' * 60)\n\nrslt = pd.DataFrame(words_except_stop_dist.most_common(top_N),\n                    columns=['Word', 'Frequency']).set_index('Word')\n\nmatplotlib.style.use('ggplot')\nrslt.plot.bar(rot=0, figsize=(16, 8), fontsize=10)","eac0f49f":"movies.head()","368832e4":"vectorizer = CountVectorizer()\nx = vectorizer.fit_transform(movies['genres'].values)\nfeature_names = vectorizer.get_feature_names()","cd263af9":"genres_bow = pd.DataFrame(x.toarray(), columns=feature_names)\ngenres_bow['combined']= genres_bow.values.tolist()","7720fd38":"movies['genres'] = genres_bow['combined']","8c974f76":"movies.head()","34c7756f":"def get_cossim(movieid, top):\n    # Creating dataframe with only IDs and genres\n    movies_to_search = movies[['movieId', 'genres']]\n    # Remove the ID of the movie we are measuring distance to\n    movies_to_search = movies_to_search[movies_to_search.movieId != movieid]\n    # Saving distances to new column\n    movies_to_search['dist'] = movies_to_search['genres'].apply(lambda x: cosine_similarity(np.array(x).reshape(1, -1), np.array(movies.loc[movies['movieId'] == movieid]['genres'].values[0]).reshape(1, -1)))\n    # Remove the genres column\n    movies_to_search = movies_to_search.drop(columns=['genres'])\n    # Distance value is in the list inside of the list so we need to unpack it\n    movies_to_search = movies_to_search.explode('dist').explode('dist')\n    # Sort the data and return top values\n    return movies_to_search.sort_values(by=['dist'], ascending=False)['movieId'].head(top).values","b156166f":"def get_similar(userid):\n    # Take all the movies watched by user\n    movies_watched_by_user = user_ratings[user_ratings.userId == user_id]\n    # Only 4.5 or higher rating filtered\n    movies_watched_by_user = movies_watched_by_user[movies_watched_by_user['rating'] > 4.5]\n    # Taking top 20 with highest ratings\n    top_movies_user = (movies_watched_by_user.sort_values(by=\"rating\", ascending=False).head(20))\n    top_movies_user['watched_movieId'] = top_movies_user['movieId']\n    top_movies_user = top_movies_user[['userId', 'watched_movieId']]\n    # Find 5 similar movies for each of the selected above\n    top_movies_user['similar'] = top_movies_user['watched_movieId'].apply(lambda x: (get_cossim(x, 5)))\n    # Remove movies that user have already watched from recommendations\n    result = [x for x in np.concatenate(top_movies_user['similar'].values, axis=0).tolist() if x not in top_movies_user.watched_movieId.values.tolist()]\n    return result","48a4e184":"def get_top(id, top):\n    # taking movies that user may like\n    smlr = get_similar(id)    \n    # Calculating mean rationg for every movie\n    movie_data = pd.merge(user_ratings, movies, on='movieId')\n    ratings_mean_count = pd.DataFrame(movie_data.groupby('movieId')['rating'].mean())\n    ratings_mean_count['rating_counts'] = pd.DataFrame(movie_data.groupby('movieId')['rating'].count())\n    # Sorting movies with 10 or more ratings by users\n    ratings_mean_count = ratings_mean_count[ratings_mean_count['rating_counts'] > 10]\n    # Returning top N movies sorted by rating\n    return ratings_mean_count[ratings_mean_count.index.isin(smlr)].sort_values(by=['rating'], ascending=False).head(top)","9973ca24":"df = user_ratings","c0b96ae7":"user_ids = df[\"userId\"].unique().tolist()\n# Reassign user IDs\nuser2user_encoded = {x: i for i, x in enumerate(user_ids)}\nuserencoded2user = {i: x for i, x in enumerate(user_ids)}\n\nmovie_ids = df[\"movieId\"].unique().tolist()\n# Reassign movie IDs\nmovie2movie_encoded = {x: i for i, x in enumerate(movie_ids)}\nmovie_encoded2movie = {i: x for i, x in enumerate(movie_ids)}\n\ndf[\"user\"] = df[\"userId\"].map(user2user_encoded)\ndf[\"movie\"] = df[\"movieId\"].map(movie2movie_encoded)\n\nnum_users = len(user2user_encoded)\nnum_movies = len(movie_encoded2movie)\ndf[\"rating\"] = df[\"rating\"].values.astype(np.float32)\n# min and max ratings will be used to normalize the ratings later\nmin_rating = min(df[\"rating\"])\nmax_rating = max(df[\"rating\"])\n\nprint(\n    \"Number of users: {}, Number of Movies: {}, Min rating: {}, Max rating: {}\".format(\n        num_users, num_movies, min_rating, max_rating\n    )\n)","0f819147":"df = df.sample(frac=1, random_state=42)\nx = df[[\"user\", \"movie\"]].values\n# Normalize the targets between 0 and 1. Makes it easy to train.\ny = df[\"rating\"].apply(lambda x: (x - min_rating) \/ (max_rating - min_rating)).values\n# Assuming training on 90% of the data and validating on 10%.\ntrain_indices = int(0.9 * df.shape[0])\nx_train, x_val, y_train, y_val = (\n    x[:train_indices],\n    x[train_indices:],\n    y[:train_indices],\n    y[train_indices:],\n)","277a7689":"EMBEDDING_SIZE = 50\n\nclass RecommenderNet(keras.Model):\n    def __init__(self, num_users, num_movies, embedding_size, **kwargs):\n        super(RecommenderNet, self).__init__(**kwargs)\n        self.num_users = num_users\n        self.num_movies = num_movies\n        self.embedding_size = embedding_size\n        self.user_embedding = layers.Embedding(\n            num_users,\n            embedding_size,\n            embeddings_initializer=\"he_normal\",\n            embeddings_regularizer=keras.regularizers.l2(1e-6),\n        )\n        self.user_bias = layers.Embedding(num_users, 1)\n        self.movie_embedding = layers.Embedding(\n            num_movies,\n            embedding_size,\n            embeddings_initializer=\"he_normal\",\n            embeddings_regularizer=keras.regularizers.l2(1e-6),\n        )\n        self.movie_bias = layers.Embedding(num_movies, 1)\n\n    def call(self, inputs):\n        user_vector = self.user_embedding(inputs[:, 0])\n        user_bias = self.user_bias(inputs[:, 0])\n        movie_vector = self.movie_embedding(inputs[:, 1])\n        movie_bias = self.movie_bias(inputs[:, 1])\n        dot_user_movie = tf.tensordot(user_vector, movie_vector, 2)\n        # Add all the components (including bias)\n        x = dot_user_movie + user_bias + movie_bias\n        # The sigmoid activation forces the rating to between 0 and 1\n        return tf.nn.sigmoid(x)\n\nmodel = RecommenderNet(num_users, num_movies, EMBEDDING_SIZE)\nmodel.compile(\n    loss=tf.keras.losses.BinaryCrossentropy(), optimizer=keras.optimizers.Adam(lr=0.0005)\n)","9c88a52d":"history = model.fit(\n    x=x_train,\n    y=y_train,\n    batch_size=64,\n    epochs=15,\n    verbose=1,\n    validation_data=(x_val, y_val),\n)","0bf9f1f0":"plt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])\nplt.title(\"model loss\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"epoch\")\nplt.legend([\"train\", \"test\"], loc=\"upper left\")\nplt.show()","f3303d98":"user_id = 10","752939f6":"top = get_top(user_id, 20)\ncontent_rec = top.index.values.tolist()","89321249":"movie_df = pd.read_csv('\/kaggle\/input\/movie-lens-small-latest-dataset\/movies.csv') \n\n# Searching for movies that user already watched\nmovies_watched_by_user = df[df.userId == user_id]\n# Searching for movies that user haven't watched yet\nmovies_not_watched = movie_df[\n    ~movie_df[\"movieId\"].isin(movies_watched_by_user.movieId.values)\n][\"movieId\"]\n\nmovies_not_watched = list(\n    set(movies_not_watched).intersection(set(movie2movie_encoded.keys()))\n)\n\nmovies_not_watched = [[movie2movie_encoded.get(x)] for x in movies_not_watched]\nuser_encoder = user2user_encoded.get(user_id)\nuser_movie_array = np.hstack(\n    ([[user_encoder]] * len(movies_not_watched), movies_not_watched)\n)\n# Predicting ratings for movies\nratings = model.predict(user_movie_array).flatten()\n# Sorting predicted ratings and taking top 20\ntop_ratings_indices = ratings.argsort()[-20:][::-1]\n# Getting the actual IDs for movies\nrecommended_movie_ids = [\n    movie_encoded2movie.get(movies_not_watched[x][0]) for x in top_ratings_indices\n]","cd2b24df":"print(\"Showing recommendations for user: {}\".format(user_id))\nprint(\"====\" * 10)\nprint(\"Movies with high ratings from user\")\nprint(\"----\" * 10)\ntop_movies_user = (\n    movies_watched_by_user.sort_values(by=\"rating\", ascending=False)\n    .head(10)\n    .movieId.values\n)\nmovie_df_rows = movie_df[movie_df[\"movieId\"].isin(top_movies_user)]\nfor row in movie_df_rows.itertuples():\n    print(row.title, \":\", row.genres)\n\nprint(\"----\" * 10)\nprint(\"Top 10 movie recommendations\")\nprint(\"----\" * 10)\n# \u0421\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c \u0432\u044b\u0431\u0438\u0440\u0430\u0435\u043c 10 \u0444\u0438\u043b\u044c\u043c\u043e\u0432 \u0438\u0437 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0439\nto_recommend = random.sample((content_rec + recommended_movie_ids), 10)\nrecommended_movies = movie_df[movie_df[\"movieId\"].isin(to_recommend)]\nfor row in recommended_movies.itertuples():\n    print(row.title, \":\", row.genres)","b5fe09c1":"## Collaborative filtering","d8f70c31":"Mixing all recommendations in one list and randomly taking 10 of them to recommend.","f9198223":"Creating a dataframe for genre indicators and combining them in one list for each movie.","0c44a029":"## Loading libraries and data","a42a9de8":"Some movies from content-based recommendations may repeat with each other and with collaborative filtering recommendations but it will only give them a higher chance of appearing in recommended movies, which is good in this example.","335c37f0":"Let's have a look at the top 25 words used in movie titles.","4180edf2":"# Hybrid movie recommendation system","dcb70aed":"Let's look at our data again.","ef719c18":"## Results","a8fdff71":"Get the top 20 recommendations based on similar users' preferences.","1a3dec2d":"To measure the similarity between movies, we need to present the genre line in a more formalized form. To do so, we will use a bag of words model. So for each movie, we will get a vector of 21 values indicating which genres it belongs to.","72f547e9":"## Content based recommendations","03cb06d1":"Set the ID of a user that we will be recommending movies to.","1ce17f47":"The next function takes 20 top-rated movies by a selected user and returns 5 similar movies for each of those.","c96bf69f":"## Visualization","1b65a9bf":"Let's see the results.","5a972e8b":"Import the 'movies.csv' file and 'ratings.csv' file. We will get rid of release year in movie titles for future title analysis and of timestamps in ratings because we are not using them in this example.","5a6852bf":"Training the model in 15 epoches with batch size of 64 and learning rate of 0.0005.","9747b8c2":"Let's take a look at the genres data. It contains strings with names of genres of the movies. There are only 20 unique that that appear in the string in alphabet order.","6daa8cd2":"Normalizing ratings and splitting data.","d107f5a7":"For content-based recommendations, we will be looking at movie genres and searching for similar movies. To find the similarity between the two movies we will be using the cosine similarity metric.","d80d6bc6":"'get_top' function returns top N recommended movies sorted by mean user rating. (only movies with 10 or more ratings are used).","f70b50b9":"The function below returns IDs for top N movies similar to the given one.","119488f2":"Defining model structure.","a4a44e0a":"The code was retrieved from example from keras.io. The model was tuned a little bit by me.","de101364":"Replacing genres line with a bag of words representation.","1f234edd":"It does not look representative. Numerical methods won't be really effective with this kind of data. The pre-trained word2vec model could help in this case but movie title should have a weak correlation with the movie itself and the user's preferences, so we will just ignore titles in this example.","b489c0d4":"Get top 20 recommendations based on previously watched movies."}}