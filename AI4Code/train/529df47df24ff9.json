{"cell_type":{"49537093":"code","47491b0b":"code","8cce2dd3":"code","7d405323":"code","fc760320":"code","e995f29c":"code","a4960fc7":"code","000ca8af":"code","922a1e11":"code","1e109807":"code","87dbbeea":"code","8276a81f":"code","858f8c8d":"code","8d4c41b0":"code","b0e0a804":"code","03ca45e0":"code","b1e6a538":"code","61bdddae":"code","55bfbe8c":"code","3be5723e":"code","9d85e247":"code","1131b776":"code","0e351b77":"code","19276376":"code","868bcffc":"code","bc9678d5":"code","65ffc1b9":"code","8310e3e7":"code","7bc1910e":"code","c548acd6":"code","520c9a92":"code","285338a1":"code","2960d6ae":"code","2bf20178":"code","7e6114ee":"code","557cc1d4":"code","15573a32":"code","ccc91d02":"code","c0b83701":"code","ab5cf832":"code","b930e6f5":"code","a591c49e":"code","895f3c67":"code","e46ea8db":"code","54465471":"code","d668fef1":"code","287426aa":"code","f7e8330f":"code","fbea3b86":"code","9ef27741":"code","db09bca7":"code","c9005d94":"markdown","55095774":"markdown","b794bcc8":"markdown","ee412877":"markdown","059881f1":"markdown","3d1f7556":"markdown","1f84cd30":"markdown","02577e28":"markdown","6b357d04":"markdown","e709193e":"markdown","fa5bde9e":"markdown","5d162d67":"markdown"},"source":{"49537093":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","47491b0b":"# Importing the Dataset\ndf=pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')\ndf.head()","8cce2dd3":"#Number of rows and features\ndf.shape","7d405323":"# Checking for the balance of categories in the target variable.\n# The feature is highly imbalanced\ndf['Class'].value_counts()","fc760320":"# Splittig the data into majority and minority based on the target variable. \nfrom sklearn.utils import resample\ndf_minor=df[df['Class']==1]\ndf_major=df[df['Class']==0]\n\n# Upsampling the minority class to the number of rows in the majority class by using the\n# sampling with replacement technique.\ndf_minority_upsampled=resample(df_minor,replace=True,n_samples=len(df_major),random_state=42)\n\n# Concatenating the majority data and the upsampled data\ndf_sampled=pd.concat([df_major,df_minority_upsampled])\n","e995f29c":"df_sampled['Class'].shape","a4960fc7":"# Checking for the co-relation of independent features with the target variable \ndf_sampled.corr()['Class']","000ca8af":"# Extracting features that are highly co-related with the dependent variable as these are the\n# features responsible for the variation of values in the dependent variable.\ncorr_cols=df_sampled.corr()['Class'][((df_sampled.corr()['Class']>0.5) | (df_sampled.corr()['Class']<-0.5))].index","922a1e11":"# Apart from the features above, the duration of the transaction and the amount used for the\n# transaction also plays a role in predicting the transaction as fraudulent or not.\ndf1=df_sampled[corr_cols]\ncol=df_sampled[['Time','Amount']]\ndf1=pd.concat([df1,col],1)\ndf1.head()","1e109807":"# Scaling the time and amount variable to bring all the independent variables to the same scale.\nfrom sklearn.preprocessing import StandardScaler\nstd=StandardScaler().fit_transform(df1[['Time','Amount']])\ndf1[['Time','Amount']]=std","87dbbeea":"df1.head()","8276a81f":"# Splitting X and y\nfrom sklearn.model_selection import train_test_split\nX=df1.drop('Class',1)\ny=df1['Class']\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=42)","858f8c8d":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nimport warnings\nwarnings.filterwarnings('ignore')\nfor i in [0.000001,0.00001,0.0001,0.001,0.01,0.1,1,10,100]:\n    lr=LogisticRegression(C=i).fit(X_train,y_train)\n    y_pred_lr=lr.predict(X_test)\n    print(accuracy_score(y_test,y_pred_lr))\n    ","8d4c41b0":"lr=LogisticRegression(C=0.00001).fit(X_train,y_train)\ny_pred_lr=lr.predict(X_test)\nprint(accuracy_score(y_test,y_pred_lr))","b0e0a804":"# Varying the threshold value of the probability to check for the least misclassified values.\nfrom sklearn.preprocessing import binarize\nfrom sklearn.metrics import f1_score,confusion_matrix\nfor i in range(1,11):\n    y_pred2=lr.predict_proba(X_test)\n    bina=binarize(y_pred2,threshold=i\/10)[:,1]\n    cm2=confusion_matrix(y_test,bina)\n    print ('With',i\/10,'threshold the Confusion Matrix is ','\\n',cm2,'\\n',\n            'with',cm2[0,0]+cm2[1,1],'correct predictions and',cm2[1,0],'Type II errors( False Negatives)','\\n\\n',\n          'Sensitivity: ',cm2[1,1]\/(float(cm2[1,1]+cm2[1,0])),'Specificity: ',cm2[0,0]\/(float(cm2[0,0]+cm2[0,1])),'\\n\\n\\n')\n    print('f1 score: ',f1_score(y_test,bina))\n    print('accuracy score: ',accuracy_score(y_test,bina))\n    print('\\n')","03ca45e0":"from sklearn.metrics import confusion_matrix\nprint('Confusion matrix for Logistic Regression: ',confusion_matrix(y_test,y_pred_lr))","b1e6a538":"from sklearn.tree import DecisionTreeClassifier\ndt=DecisionTreeClassifier().fit(X_train,y_train)\ny_pred_dt=dt.predict(X_test)\nprint(accuracy_score(y_test,y_pred_dt))\nprint(confusion_matrix(y_test,y_pred_dt))","61bdddae":"from sklearn.ensemble import RandomForestClassifier\nrf=RandomForestClassifier().fit(X_train,y_train)\ny_pred_rf=rf.predict(X_test)\nprint('Accuracy score for Random Forest: ',accuracy_score(y_test,y_pred_rf))\nprint('Confusion matrix for Random Forest: ',confusion_matrix(y_test,y_pred_rf))","55bfbe8c":"from sklearn.ensemble import AdaBoostClassifier,GradientBoostingClassifier\nadb=AdaBoostClassifier().fit(X_train,y_train)\ny_pred_adb=adb.predict(X_test)\nprint(\"Adaboost Classifier's accuracy: \",accuracy_score(y_test,y_pred_adb))\nprint(\"Adaboost Classifier's confusion matrix: \",confusion_matrix(y_test,y_pred_adb))","3be5723e":"grb=GradientBoostingClassifier().fit(X_train,y_train)\ny_pred_grb=grb.predict(X_test)\nprint(\"Gradient boost Classifier's accuracy: \",accuracy_score(y_test,y_pred_grb))\nprint(\"Gradient boost Classifier's confusion matrix: \",confusion_matrix(y_test,y_pred_grb))","9d85e247":"from sklearn.utils import resample\ndf_minor=df[df['Class']==1]\ndf_major=df[df['Class']==0]\n# Reducing the number of data in the majority class to the number of observations in the minority class.\ndf_majority_downsampled=resample(df_major,replace=False,n_samples=len(df_minor),random_state=42)\n\ndf_downsampled=pd.concat([df_minor,df_majority_downsampled])","1131b776":"df_downsampled.shape","0e351b77":"corr_col=df_downsampled.corr()['Class'][((df_downsampled.corr()['Class']>0.5) | (df_downsampled.corr()['Class']<-0.5))].index\ndf2=df_downsampled[corr_col]","19276376":"cols=df_downsampled[['Time','Amount']]\ndf2=pd.concat([df2,cols],1)\ndf2.head()","868bcffc":"df2.reset_index(drop=True,inplace=True)\ndf2.head()","bc9678d5":"df2.shape","65ffc1b9":"std_us=StandardScaler().fit_transform(df2[['Time','Amount']])\ndf2[['Time','Amount']]=std_us\ndf2.head()","8310e3e7":"from sklearn.model_selection import train_test_split\nX_us=df2.drop('Class',1)\ny_us=df2['Class']\n\nX_train1,X_test1,y_train1,y_test1=train_test_split(X_us,y_us,test_size=0.3,random_state=42)","7bc1910e":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nimport warnings\nwarnings.filterwarnings('ignore')\nfor i in [0.000001,0.00001,0.0001,0.001,0.01,0.1,1,10,100]:\n    lr_us=LogisticRegression(C=i).fit(X_train1,y_train1)\n    y_pred_lr_us=lr_us.predict(X_test1)\n    print(accuracy_score(y_test1,y_pred_lr_us))","c548acd6":"lr_us=LogisticRegression(C=0.1).fit(X_train1,y_train1)\ny_pred_lr_us=lr_us.predict(X_test1)\nprint(accuracy_score(y_test1,y_pred_lr_us))","520c9a92":"for i in range(1,11):\n    y_pred2_us=lr_us.predict_proba(X_test1)\n    bina_us=binarize(y_pred2_us,threshold=i\/10)[:,1]\n    cm2=confusion_matrix(y_test1,bina_us)\n    print ('With',i\/10,'threshold the Confusion Matrix is ','\\n',cm2,'\\n',\n            'with',cm2[0,0]+cm2[1,1],'correct predictions and',cm2[1,0],'Type II errors( False Negatives)','\\n\\n',\n          'Sensitivity: ',cm2[1,1]\/(float(cm2[1,1]+cm2[1,0])),'Specificity: ',cm2[0,0]\/(float(cm2[0,0]+cm2[0,1])),'\\n\\n\\n')\n    print('f1 score: ',f1_score(y_test1,bina_us))\n    print('accuracy score: ',accuracy_score(y_test1,bina_us))\n    print('\\n')","285338a1":"# Taking 0.5 as the threshold as it gives the least misclassified values\ny_pred2_us=lr_us.predict_proba(X_test1)\nbina_us=binarize(y_pred2_us,threshold=0.5)[:,1]\ncm2=confusion_matrix(y_test1,bina_us)\nprint ('With 0.5 threshold the Confusion Matrix is ','\\n',cm2,'\\n',\n            'with',cm2[0,0]+cm2[1,1],'correct predictions and',cm2[1,0],'Type II errors( False Negatives)','\\n\\n',\n          'Sensitivity: ',cm2[1,1]\/(float(cm2[1,1]+cm2[1,0])),'Specificity: ',cm2[0,0]\/(float(cm2[0,0]+cm2[0,1])),'\\n\\n\\n')\nprint('f1 score: ',f1_score(y_test1,bina_us))\nprint('accuracy score: ',accuracy_score(y_test1,bina_us))\nprint('\\n')","2960d6ae":"from sklearn.tree import DecisionTreeClassifier\ndt_us=DecisionTreeClassifier().fit(X_train1,y_train1)\ny_pred_dt_us=dt_us.predict(X_test1)\nprint(accuracy_score(y_test1,y_pred_dt_us))\nprint(confusion_matrix(y_test1,y_pred_dt_us))","2bf20178":"# Grid Search CV for hyperparameter tuning\nfrom sklearn.model_selection import GridSearchCV\ndt_us=DecisionTreeClassifier()\nparam_grid = {\n\n    'criterion': ['gini','entropy'],\n    'max_depth': [4,6,8,10],\n    'min_samples_split' : [5,10,15,20,25,30],\n    'min_samples_leaf': [2,5,7],\n    'random_state': [42,135,777],\n}\n\ndt_grid_us=GridSearchCV(estimator=dt_us,param_grid=param_grid,n_jobs=-1,return_train_score=True)\n\ndt_grid_us.fit(X_train1,y_train1)","7e6114ee":"# Checking for the best parameters of the grid search cv that gives the highest result.\ndt_grid_us.best_params_","557cc1d4":"# The results of the cross validated grid search \ncv_res_df_us=pd.DataFrame(dt_grid_us.cv_results_)","15573a32":"# Plotting the mean test score and mean train score and extracting the point where the test score\n# is high and also the difference in the train and test score is minimum.\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(20,5))\nplt.plot(cv_res_df_us['mean_train_score'])\nplt.plot(cv_res_df_us['mean_test_score'])\nplt.xticks(rotation=90)\nplt.show()","ccc91d02":"cv_res_df_us[cv_res_df_us['mean_test_score']==cv_res_df_us['mean_test_score'].max()]","c0b83701":"# Using the parameters that yielded the best results.\ndt_us=DecisionTreeClassifier(max_depth=4,min_samples_leaf=5,min_samples_split=15,random_state=135).fit(X_train1,y_train1)\ny_pred_dt_us=dt_us.predict(X_test1)\nprint(accuracy_score(y_test1,y_pred_dt_us))\nprint(confusion_matrix(y_test1,y_pred_dt_us))","ab5cf832":"from sklearn.ensemble import RandomForestClassifier\nrf_us=RandomForestClassifier().fit(X_train1,y_train1)\ny_pred_rf_us=rf.predict(X_test1)\nprint(accuracy_score(y_test1,y_pred_rf_us))\nprint(confusion_matrix(y_test1,y_pred_rf_us))","b930e6f5":"# Hyper parameter tuning using GridSearchCV\nrf_us=RandomForestClassifier()\nparam_grid = {\n    'n_estimators': [8,10,20,30],\n    'criterion': ['gini','entropy'],\n    'max_depth': [4,6,8,10],\n    'min_samples_split' : [5,10,15,20,25,30],\n    'min_samples_leaf': [2,5,7],\n    'random_state': [42,135,777],\n}\n\nrf_grid=GridSearchCV(estimator=rf_us,param_grid=param_grid,n_jobs=-1,return_train_score=True)\n\nrf_grid.fit(X_train1,y_train1)","a591c49e":"rf_grid.best_params_","895f3c67":"cv_res_df_rf_us=pd.DataFrame(rf_grid.cv_results_)","e46ea8db":"import matplotlib.pyplot as plt\nplt.figure(figsize=(20,5))\nplt.plot(cv_res_df_rf_us['mean_train_score'])\nplt.plot(cv_res_df_rf_us['mean_test_score'])\nplt.xticks(rotation=90)\nplt.show()","54465471":"min(cv_res_df_rf_us['mean_train_score']-cv_res_df_rf_us['mean_test_score'])","d668fef1":"cv_res_df_rf_us[(cv_res_df_rf_us['mean_test_score']==cv_res_df_rf_us['mean_test_score'].max())]","287426aa":"rf_us=RandomForestClassifier(**rf_grid.best_params_).fit(X_train1,y_train1)\ny_pred_rf_us=rf.predict(X_test1)\nprint(\" Random Forest Classifier's accuracy: \",accuracy_score(y_test1,y_pred_rf_us))\nprint(\" Random Forest Classifier's confusion matrix: \",confusion_matrix(y_test1,y_pred_rf_us))","f7e8330f":"adbc=AdaBoostClassifier().fit(X_train1,y_train1)\ny_pred_adbc=adbc.predict(X_test1)\nprint(\"Adaboost Classifier's accuracy: \",accuracy_score(y_test1,y_pred_adbc))\nprint(\" Adaboost Classifier's confusion matrix: \",confusion_matrix(y_test1,y_pred_adbc))","fbea3b86":"gra_bc=GradientBoostingClassifier().fit(X_train1,y_train1)\ny_pred_gra_bc=gra_bc.predict(X_test1)\nprint(\" Gradient boost Classifier's accuracy: \",accuracy_score(y_test1,y_pred_gra_bc))\nprint(\" Gradient boost Classifier's confusion matrix: \",confusion_matrix(y_test1,y_pred_gra_bc))","9ef27741":"y_pred_rf=rf.predict(X_test)\nprint('Accuracy score for Random Forest: ',accuracy_score(y_test,y_pred_rf))\nprint('Confusion matrix for Random Forest: \\n ',confusion_matrix(y_test,y_pred_rf))","db09bca7":"sub=pd.DataFrame(y_test,columns=['Actual Class'])\nsub['Predicted Class']=y_pred_rf\nsub.to_csv('submission.csv')","c9005d94":"### RANDOM FOREST CLASSIFIER","55095774":"<h2> UPSAMPLING <\/h2>\n- Upsampling is the technique of increasing the number of rows of the minority class (the category with the least number of data) to the number of rows of the majority class (the category with the most data) in order to prevent the model from being biased towards the majority class.\n- Sampling with replacement technique is used so that there are duplicate rows and the number of rows of minority is equal to the majority class. ","b794bcc8":"<h4> CLASSIFICATION MODELS <\/h4>\n\n<h5> LOGISTIC REGRESSION <\/h5>","ee412877":"<h4> Decision Tree Classifier","059881f1":"<h3> DATASET <\/h3>\n- This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n- It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'.\n- Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset.\n- The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n\n<h4> APPROACH <\/h4>\n- As the dataset is highly imbalanced, we have to perform sampling techniques to bring the proportion of Non-fraudulent and Fraudulent transactions.\n- We will build classification models to check which of them works the best for the dataset.","3d1f7556":"#### The best obtained result is for the Decision Tree with upsampling\n- The obtained scores for upsampling and under sampling shows us that in under sampling, there is a drop in the scores and this is because of the fact that the models missed out some information as most of the rows were dropped.\n- Whereas in upsampled models, the models have learned better and yielded better results.","1f84cd30":"<h3> CLASSIFICATION MODELS <\/h3>\n\n<h4> LOGISTIC REGRESSION","02577e28":"#### DECISION TREE CLASSIFIER","6b357d04":"#### GRADIENT BOOST CLASSIFIER","e709193e":"#### ADA BOOST CLASSIFIER","fa5bde9e":"### RANDOM FOREST WITH UPSAMPLED DATA","5d162d67":"<h2> UNDER SAMPLING <\/h2>\n- Under sampling is the technique of reducing the number of rows of the majority class (the category with the most number of data) to the number of rows of the minority class (the category with the least data) in order to prevent the model from being biased towards the majority class.\n- Sampling without replacement technique is used so that there are no duplicate rows."}}