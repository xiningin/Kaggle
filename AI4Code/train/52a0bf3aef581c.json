{"cell_type":{"526a0116":"code","7ea09e62":"code","f14d80ae":"code","a158e62d":"code","4bb96ebe":"code","067eef0d":"code","f0dfec37":"markdown","eea701a7":"markdown","4042dc3d":"markdown","2f1e8989":"markdown","7b2fc2e7":"markdown","4ca30c8a":"markdown"},"source":{"526a0116":"import pandas as pd\nimport numpy as np\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\nfrom sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\nprint(\"Setup Complete\")\n","7ea09e62":"# Read the Housing Prices data\nX = pd.read_csv('..\/input\/train.csv', index_col='Id')\n\n# Remove rows with missing target, separate target from predictors\nX.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X.SalePrice              \nX.drop(['SalePrice'], axis=1, inplace=True)\n\n# Select only the numeric columns\nnumericial_cols = [cname for cname in X.columns if X[cname].dtype in ['int64', 'float64']]\nX_num = X[numericial_cols].copy()\n\n\n# Preprocessed features\n# Imputation to fill in the missing data\nfinal_imputer = SimpleImputer(strategy='most_frequent')\nfinal_X_num = pd.DataFrame(final_imputer.fit_transform(X_num))\n\n# Imputation removed column names; put them back\nfinal_X_num.columns = X_num.columns\n\nfinal_X_num.head()","f14d80ae":"final_X_num.shape","a158e62d":"#apply SelectKBest class to extract top 10 best features\nbestfeatures = SelectKBest(score_func=chi2, k=20)\nfit = bestfeatures.fit(final_X_num,y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(final_X_num.columns)\n\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(10,'Score'))  #print 10 best features\n","4bb96ebe":"# number of features to select\nnum_select = 10\n\nmodel = ExtraTreesClassifier()\nmodel.fit(final_X_num,y)\nprint(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=final_X_num.columns)\nfeat_importances.nlargest(num_select).plot(kind='barh')\nplt.show()\n\nfeat_importances.nlargest(num_select)","067eef0d":"#get correlations of each features in dataset\ncorrmat = final_X_num.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(25,25))\n\n#plot heat map\ng=sns.heatmap(final_X_num[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","f0dfec37":"\n### **3. Correlation Matrix with Heatmap**","eea701a7":"# [**Feature Selection Techniques in Machine Learning with Python**](https:\/\/towardsdatascience.com\/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e)\n\nThis notebook is an implementation of three feature selection techniques detailed in a Medium artice by [Raheel Shaikh](https:\/\/towardsdatascience.com\/@srhussain99). The artilce is only a 5 min read, so I won't reproduce it here, and I hope you'll give Raheel some applause for his very useful insights.\n\nThese techniques have been applied to the [**Housing Prices Competition for Kaggle Learn Users**](https:\/\/www.kaggle.com\/c\/home-data-for-ml-course) to help decide which of the numerical fields to keep or discard. \n\nAgain... to fully understand this notebook you will need to read [Raheel's article](https:\/\/towardsdatascience.com\/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e) in conjunction with this notebook.\n\nPersonally, I found the *Feature Importance* technique the most useful for this project.","4042dc3d":"Read in the data, select the fields containing numericial data and then impute any missing data in those fields. \nFor this data, it doesn't seem to matter if you use *median* or *most_frequent* for this selection process.","2f1e8989":"### **2. Feature Importance**","7b2fc2e7":"\n### **1. Univariate Selection**","4ca30c8a":"**Import the required libraries**"}}