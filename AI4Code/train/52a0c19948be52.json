{"cell_type":{"20d39730":"code","9672faac":"code","f4dc246f":"code","a5f03d7c":"code","653060c3":"code","5e183b31":"code","af370b4c":"code","23d90127":"code","571d3672":"code","bf555c92":"code","7cfa882d":"code","53c5ffa1":"code","6d428eea":"code","032c5b40":"code","b737abc0":"code","5297e57d":"code","0f8279f7":"code","5e60c527":"code","188c4999":"code","7b1758de":"code","43353714":"code","f8f6f819":"code","b06d96f8":"code","c86e7378":"markdown","a46c3352":"markdown","73ebb5e8":"markdown","5ca3b59e":"markdown","52b8cdcb":"markdown","b055320e":"markdown","b25d6346":"markdown","7b3e0614":"markdown","dd149f3b":"markdown","0fed87f2":"markdown","6138cbed":"markdown","966faa2d":"markdown","312ed92e":"markdown","7f07f07c":"markdown","449c0269":"markdown","41eb943c":"markdown","708990f4":"markdown","6f0cd4b6":"markdown","f200e31e":"markdown","8a001aaf":"markdown","0a71a046":"markdown","ad3ac4f3":"markdown","6d13aada":"markdown","9e9be122":"markdown","bc91dee9":"markdown","697ab419":"markdown","c4014c9e":"markdown","ff1308df":"markdown","288db94e":"markdown"},"source":{"20d39730":"import numpy as np\nimport matplotlib.pyplot as plt\nX = 2*np.random.rand(100, 1)\ny = 4 + 3*X + np.random.randn(100, 1)\nplt.scatter(X, y)\nplt.show()","9672faac":"# Stack 1-D arrays as columns into a 2-D array.\nX_b = np.c_[np.ones((100, 1)), X]\ntheta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\ntheta_best","f4dc246f":"X_new = np.array([[0], [2]])\nX_new_b = np.c_[np.ones((2, 1)), X_new] # add x0 = 1 to each instance\ny_predict = X_new_b.dot(theta_best) # vectorized form \ny_predict","a5f03d7c":"plt.plot(X_new, y_predict, \"r-\")\nplt.plot(X, y, \"b.\")\nplt.axis([0, 2, 0, 15]) # X-axis from 0 to 2, Y-axis from 0 to 15\nplt.show()","653060c3":"from sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(X, y)\nlin_reg.intercept_, lin_reg.coef_","5e183b31":"## Gradient Descent\nfrom sklearn.linear_model import SGDRegressor\nsgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1)\nsgd_reg.fit(X, y.ravel())","af370b4c":"sgd_reg.intercept_, sgd_reg.coef_","23d90127":"m = 100\nX = 6 * np.random.rand(m, 1) - 3\ny = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\ngraph = plt.scatter(X, y)\nplt.show()","571d3672":"from sklearn.preprocessing import PolynomialFeatures\npoly_features = PolynomialFeatures(degree=2, include_bias=False)\nX_poly = poly_features.fit_transform(X)\n\nmodel = LinearRegression()\nmodel.fit(X_poly, y)\n","bf555c92":"from sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\ndef plot_learning_curves(model, X, y):\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n    train_errors, val_errors = [], []\n    for m in range(1, len(X_train)):\n        model.fit(X_train[:m], y_train[:m])\n        y_train_predict = model.predict(X_train[:m])\n        y_val_predict = model.predict(X_val)\n        train_errors.append(mean_squared_error(y_train[:m], y_train_predict[:m]))\n        val_errors.append(mean_squared_error(y_val, y_val_predict))\n    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")\n    plt.axis([0, 80, 0, 3])\n    plt.xlabel(\"Training set size\")\n    plt.ylabel(\"RMSE\")\n    plt.legend(loc='best')\n    plt.show()","7cfa882d":"lin_reg = LinearRegression()\nplot_learning_curves(lin_reg, X, y)","53c5ffa1":"## If we want to see Polynoial Regression..\nfrom sklearn.pipeline import Pipeline\n\npolynomial_regression = Pipeline([\n    (\"poly_features\", PolynomialFeatures(degree=10, include_bias=False)),\n    (\"lin_reg\", LinearRegression()),\n])\n\nplot_learning_curves(polynomial_regression, X, y)","6d428eea":"#Heres is how to perform Ridge Regression with Scikit-learn\nfrom sklearn.linear_model import Ridge\nridge_reg = Ridge(alpha=1, solver=\"cholesky\")\nridge_reg.fit(X, y)\nridge_reg.predict([[1.5]])","032c5b40":"## And using Stochastic Gradient Descent\nsgd_reg = SGDRegressor(penalty='l2')\nsgd_reg.fit(X, y.ravel())\nsgd_reg.predict([[1.5]])","b737abc0":"from sklearn.linear_model import Lasso\nlasso_reg = Lasso(alpha=0.1)\nlasso_reg.fit(X, y)\nlasso_reg.predict([[1.5]])\n\n# Note that you could instead use SGDRegressor(penalty=\"l1\")","5297e57d":"from sklearn.linear_model import ElasticNet\nelastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\nelastic_net.fit(X, y)\nelastic_net.predict([[1.5]])\n","0f8279f7":"from sklearn.base import clone\nfrom sklearn.preprocessing import StandardScaler\n\nX_train, X_val, y_train, y_val = train_test_split(X, y.ravel(), test_size=0.2)\n\n#prepare the data\npoly_scaler = Pipeline([\n    (\"poly_features\", PolynomialFeatures(degree=90, include_bias=False)),\n    (\"std_scaler\", StandardScaler())\n])\n\nX_train_poly_scaled = poly_scaler.fit_transform(X_train)\nX_val_poly_scaled = poly_scaler.transform(X_val)\n\nsgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True,\n                      penalty=None, learning_rate=\"constant\", eta0=0.0005)\n\nminimum_val_error = float(\"inf\")\nbest_epoch = None\nbest_model = None\nfor epoch in range(1000):\n    sgd_reg.fit(X_train_poly_scaled, y_train) # continues where it left off\n    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n    val_error = mean_squared_error(y_val, y_val_predict)\n    if val_error < minimum_val_error:\n        minimum_val_error = val_error\n        best_epoch = epoch\n        best_model = clone(sgd_reg)\n        \n#Note with warm_start = True, when the fit method is called, it continues training where it left off instead of starting from beginning","5e60c527":"from sklearn import datasets\niris = datasets.load_iris()\nlist(iris.keys())","188c4999":"X = iris[\"data\"][:, 3:] #petal width\ny = (iris[\"target\"] == 2).astype(np.int) # if Iris virginica then 1, else 0\n","7b1758de":"from sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression()\nlog_reg.fit(X, y)\n\nX_new = np.linspace(0, 3, 1000).reshape(-1, 1)\ny_proba = log_reg.predict_proba(X_new)\nplt.plot(X_new, y_proba[:, 1], \"g-\", label=\"Iris virginica\")\nplt.plot(X_new, y_proba[:, 0], \"b--\", label=\"Not Iris virginica\")\nplt.legend(loc='best')\n","43353714":"log_reg.predict([[1.7], [1.5]])","f8f6f819":"X = iris[\"data\"][:, (2,3)] # petal length, petal width\ny = iris[\"target\"]\n\nsoftmax_reg = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", C=10)\nsoftmax_reg.fit(X, y)\nsoftmax_reg.predict([[5, 2]])\n","b06d96f8":"softmax_reg.predict_proba([[5, 2]])","c86e7378":"Now we can make predictions using theta","a46c3352":"$$ \\hat{y} = \\theta^{T}x $$ \n\nVectors are often represented as a column vectors, which are 2D arrays with a single column. If they are column vectors, prediction is dot product of transpose of theta and x ","73ebb5e8":"## Normal Equation\n\nTo find the value of theta that minimizes the cost function, there is a closed-form solution that gives the result directly\n\n$$ \\hat{\\theta }=\\left(X^{T}X\\right) ^{-1}X^{T}y $$ \n\n<br> Note: Normal Equation does not need scaling required, but algorithm is slow when number of features are large. <\/br>\n\n<br> Let's generate some linear-looking data to test this equation <\/br>","5ca3b59e":"Logistic model predicts 1 if input is positive and 0 if input is negative\n\n<br> Logistic Regression Cost Function (log loss) <\/br>\n$$ J\\left( \\theta \\right) = -\\dfrac{1}{m}\\sum ^{m}_{i=1}\\left[ y^{\\left( i\\right) }\\log \\left(\\hat{p}^{\\left( i\\right) }\\right) + \\left( 1-y^{\\left( i\\right) }\\right) \\log\\left( 1-\\hat{p}^{\\left( i\\right) }\\right) \\right] $$ \n\n<br> Logistic cost function partial derivatives <\/br> \n$$ \\dfrac{\\partial }{\\partial \\theta _{j}}J\\left( \\theta \\right) =\\dfrac{1}{m}\\sum ^{m}_{i=1}\\left( \\sigma \\left( \\theta ^{T}x^{\\left( i\\right) }\\right) -y^{\\left( i\\right) }\\right) x_{j}^{\\left( i\\right) } $$\n\n<br> We will use iris dataset to demonstrate Logistic Regression. <\/br>","52b8cdcb":"## Gradient Descent\n![](https:\/\/miro.medium.com\/max\/1200\/1*iNPHcCxIvcm7RwkRaMTx1g.jpeg)\nGradient Descent is to tweak parameters iteratively in order to minimize a cost function. ","b055320e":"# Linear Regression","b25d6346":"Another way to regularize learning algorithms is to stop training as soon as validation error reaches a minimum.","7b3e0614":"Let's compute theta using the Normal Equation. We will use the inv() function from np.linalg to compute the inverse of a matrix, and the dot() method for\nmatrix multiplication","dd149f3b":"# Polynomial Regression\n\nWhat if my data is more complex than a straight line? We can just add powers of feature as new features.\n","0fed87f2":"# Elastic Net\n\nSimply put, Elastic Net is mix of Lasso Regression and Ridge Regression\n$$ J\\left( \\theta \\right) = MSE\\left( \\theta \\right) + r\\alpha \\sum ^{n}_{i=1}\\left| \\theta_{i}\\right| + \\dfrac{1-r}{2}\\alpha \\sum ^{n}_{i=1}\\theta_{i}^{2} $$","6138cbed":"softmax score for class k\n$$ S_{k}\\left( x\\right) =x^{T}\\theta ^{\\left( k\\right) } $$ \n\n<br> Then you can estimate the probability base on the score...<\/br>\n\n$$ \\hat{p}_{k} =\\dfrac{\\exp \\left(S_{k}\\left( x\\right) \\right) }{\\sum ^{k}_{j=1}\\exp \\left( S_{j}\\left( x\\right) \\right) } $$ ","966faa2d":"Let's plot this model's prediction","312ed92e":"In order to train our Linear model, we need to compute the error between actual and predicted values\n\n<br> $$ \\dfrac{1}{m}\\sum ^{m}_{i=1}\\left(\\theta ^{T}x^{\\left(i\\right) }-y^{\\left(i\\right) }\\right) ^{2} $$<\/br>","7f07f07c":"$$\\hat{Y}_i = {\\theta}_0 + {\\theta}_1 X_1 + {\\theta}_2 X_2 + ... + {\\theta}_n X_n $$\n\nLinear model makes a prediction by simply computing a weighted sum of the input features plus a constant called the bias(intercept)\n\n<br> $$\\hat{y} $$ -- predict value <\/br>\n\n<br> n -- number of features\/columns <\/br>\n\n<br> $$x_i $$ -- ith feature value <\/br>\n\n<br> $$ {\\theta}_j $$ -- jth model parameter ( including bias term and feature weight)\n\nThis can be written more concisely using a vectorized form, (easier to debug + code efficient)","449c0269":"if r = 0, Elastic Net is identical as Ridge Regression\n\n<br> if r = 1, Elastic Net is identical as Lasso Regression <\/br>\n\n<br> If you suspect that only a few features are useful, you should prefer Lasso or Elastic Net because they tend to reduce the useless features' weights down to zero <\/br>","41eb943c":"# Logistic Regression\n\nThe main idea of logistic Regression is to classify data. (binary-classifer)\n\n<br> Logistic Regression model estimated probability(vectorized form) <\/br>\n\n$$ \\hat{p} = h_{\\theta }\\left( x\\right) =\\sigma \\left( x^{T}\\theta\\right) $$ \n\n<br> Logistic Function <\/br>\n$$ \\sigma \\left( t\\right) =\\dfrac{1}{1+\\exp \\left( -t\\right) } $$ \n![](http:\/\/miro.medium.com\/max\/1838\/1*JHWL_71qml0kP_Imyx4zBg.png)\n","708990f4":"Normal Equation is slow when there are a large number of features or too many training instances to fit in memory","6f0cd4b6":"# Softmax Regression\n\nMulti-class Classification but not multi-output classification\n\n<br> Given an instance x, softmax regression computes score for each class k, then estimate the probability of each class by applying softmax function<\/br>","f200e31e":"## Training Linear Model","8a001aaf":"## Performing Linear Regression using Scikit-Learn is simple","0a71a046":"## Lasso Regression\n\nThis is another regularized version of Linear Regression. It tends to set the weight of least important features to 0. \n\n$$ J\\left( \\theta \\right) = MSE\\left( \\theta \\right) + \\alpha \\sum ^{n}_{i=1}\\left| \\theta_{i}\\right| $$","ad3ac4f3":"### Cross-Entropy cost function\n\nCross entropy is frequently used to measure how well a set of estimated class probability matches the target classes \n\n$$ J\\left( \\Theta \\right) = -\\dfrac{1}{m}\\sum ^{m}_{i=1}\\sum ^{k}_{k=1}y_{k}^{\\left( i\\right) }\\log \\left( \\hat{p}_{k}^{\\left( i\\right) }\\right) $$ \n\n<br> Big theta is a parameter matrix that contains all theta vectors.. <\/br>\n\n<br> y is the target probability that the ith instance belongs to class k. <\/br>","6d13aada":"# EarlyStopping","9e9be122":"Close form solution of Ridge Regression\n$$ \\hat{\\theta } =\\left( X^{T}X+\\alpha A\\right) ^{-1}X^{T}y $$ ","bc91dee9":"## Ridge Regression\n\n<br> All we did is to add regularization term to the cost function. This reduces all weights close to 0 in order to prevent overfitting<\/br> \n$$ J\\left(\\theta \\right) = MSE\\left( \\theta \\right) + \\alpha \\dfrac{1}{2}\\sum^{n}_{i=1}\\theta_{i}^{2} $$ \n\n<br> Note: bias term(intercept) is not regularized, so the sum starts at i = 1, not 0 <\/br>","697ab419":"# Regularized Linear Models\n\nA good way to reduce overfitting is to regularize the model to prevent overfitting data.","c4014c9e":"![](http:\/\/miro.medium.com\/max\/973\/1*nhmPdWSGh3ziatQKOmVq0Q.png)","ff1308df":"## Visualizing Learning Curves","288db94e":"This notebook consists of ...\n1. Linear Regression\n2. Training Linear Model\n3. Normal Equation\n4. Polynomial Regression\n5. Visualizing Learning curves\n6. Ridge Regression\n7. Lasso Regression\n8. Elastic Net\n9. EarlyStopping\n10. LogisticRegression\n11. SoftMaxRegression\n<br><\/br>\n<br>Credit to Hands-on Machine Learning with Scikit-learn, keras, tensorflow by Aurelien Geron <\/br>"}}