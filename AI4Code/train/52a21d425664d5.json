{"cell_type":{"30b285da":"code","7a839e22":"code","6dc39bc9":"code","43107559":"code","8781deb4":"code","aa9157c0":"code","b44dcbab":"code","e62afb36":"code","950cd19b":"code","34f8a6a1":"code","469bbf8f":"code","79f9ca25":"code","19af1b20":"code","33337213":"code","fe0b2e9d":"code","eaa59933":"code","2ad1d537":"code","00e1f19c":"markdown","60a81c71":"markdown","e81cbe8d":"markdown","2a311680":"markdown","f250fb82":"markdown","064c8ae4":"markdown","8f6590c5":"markdown","f210e14c":"markdown","27ae9bb0":"markdown","00bacfa6":"markdown","a46bff63":"markdown","890735d0":"markdown","4d6b4ad8":"markdown","88022801":"markdown","54afe0d3":"markdown"},"source":{"30b285da":"import re\nimport keras\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.manifold import TSNE\nfrom keras.optimizers import RMSprop\nfrom keras.models import Sequential\nfrom keras.callbacks import ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\nfrom keras.layers import Dropout, Dense, BatchNormalization\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\n\ndf = pd.read_csv(\"\/kaggle\/input\/bertembedded-spam-messages\/spam_encoded.csv\")\ndf.head()","7a839e22":"plt.subplots(figsize=(10,8))\nax = sns.countplot(x=\"spam\", data=df)","6dc39bc9":"df[\"spam\"].value_counts()","43107559":"# Count the number of words in the message\ndf[\"num_words\"] = df[\"original_message\"].apply(lambda s: len(re.findall(r'\\w+', s)))\n\n# Get the length of the text message\ndf[\"message_len\"] = df[\"original_message\"].apply(len)\n\n# Count the number of uppercased characters\ndf[\"num_uppercase_chars\"] = df[\"original_message\"].apply(lambda s: sum(1 for c in s if c.isupper())) \n\n# Count the numbe rof uppercased words\ndf[\"num_uppercase_words\"] = df[\"original_message\"].apply(lambda s: len(re.findall(r\"\\b[A-Z][A-Z]+\\b\", s)))\n\n# Check if the message contains the word \"free\" or \"win\"\ndf[\"contains_free_or_win\"] = df[\"original_message\"].apply(lambda s: int(\"free\" in s.lower() or \"win\" in s.lower()))","8781deb4":"# Initialize StandardScaler\nscaler = preprocessing.StandardScaler()\n\n# Dont standerdize binary columns and the text column\nfeats_to_scale = df.drop([\"spam\", \"original_message\", \"contains_free_or_win\"], axis=1)\n\n# Create a new dataframe with the standardized features\nscaled_features = pd.DataFrame(scaler.fit_transform(feats_to_scale))\nscaled_features.rename(\n    {768: \"num_words\", 769:\"message_len\", 770: \"num_uppercase_chars\", 771: \"num_uppercase_words\"},\n    axis=1,\n    inplace=True\n)\n\n# Update the dataset with the new standerdized features\nscaled_df = df.copy()\nscaled_df.update(scaled_features)","aa9157c0":"X = scaled_df.drop([\"spam\", \"original_message\"], axis=1)\ny = scaled_df[\"spam\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)","b44dcbab":"X_embedded = TSNE(n_components=2, random_state=42).fit_transform(X_train)","e62afb36":"# creating the dataframe for plotting\ndef creat_plotting_data(data, labels=y_train, rename=False):\n    \"\"\"Creates a dataframe from the given data, used for plotting\"\"\"\n    \n    df = pd.DataFrame(data)\n    df[\"spam\"] = labels.to_numpy()\n    \n    if rename:\n        df.rename({0:\"v1\", 1:\"v2\"}, axis=1, inplace=True)\n        \n    return df\n\n# creating the dataframes for plotting\nplotting_data_embedded = creat_plotting_data(X_embedded, rename=True)","950cd19b":"plt.figure(figsize=(16, 10))\nax = sns.scatterplot(x=\"v1\", y=\"v2\", hue=\"spam\", data=plotting_data_embedded)\nax.set(title = \"Spam messages are generally closer together due to the BERT embeddings\")\nplt.show()","34f8a6a1":"_,ax = plt.subplots(figsize=(16,10))\nsns.kdeplot(df.loc[df.spam == 0, \"num_words\"], shade=True, label=\"Ham\", clip=(0, 35)) # removing observations with message length above 35 because there is an outlier\nsns.kdeplot(df.loc[df.spam == 1, \"num_words\"], shade=True, label=\"Spam\")\nax.set(xlabel = \"Number of words\", ylabel = \"Density\",title = \"Spam messages have more words than ham messages\")\nplt.show()","469bbf8f":"_,ax = plt.subplots(figsize=(16,10))\nsns.kdeplot(df.loc[df.spam == 0, \"message_len\"], shade=True, label=\"Ham\", clip=(0, 250)) # removing observations with message length above 250 because there is an outlier\nsns.kdeplot(df.loc[df.spam == 1, \"message_len\"], shade=True, label=\"Spam\")\nax.set(xlabel = \"Message length\", ylabel = \"Density\",title = \"Spam messages are longer than ham messages, concentrated on 150 characters\")\nplt.show()","79f9ca25":"_,ax = plt.subplots(figsize=(16,10))\nsns.kdeplot(df.loc[df.spam == 0, \"num_uppercase_words\"], shade=True, label=\"Ham\", clip=(0, 250)) # removing observations with message length above 250 because there is an outlier\nsns.kdeplot(df.loc[df.spam == 1, \"num_uppercase_words\"], shade=True, label=\"Spam\")\nax.set(xlabel = \"Number of uppercased words\", ylabel = \"Density\",title = \"Number of uppercased words don't seem to have any patter with spam\")\nplt.show()","19af1b20":"plt.subplots(figsize=(10,8))\n\n# Get the proportion of the genders grouped by the attrition status\ngrouped_data = df.groupby(\"spam\")[\"contains_free_or_win\"].value_counts(normalize = True).rename(\"Percentage of group\").reset_index()\nprint(grouped_data)\n\n# Plot the result\nax = sns.barplot(x=\"spam\", y=\"Percentage of group\", hue=\"contains_free_or_win\", data=grouped_data)","33337213":"plt.subplots(figsize=(16,10))\nax = sns.scatterplot(x=\"message_len\", y=\"num_uppercase_chars\", hue=\"spam\", data=df)\nax.set(\n    xlabel=\"Number of characters in the message\",\n    ylabel=\"Number of uppercase characters in the message\",\n    title=\"Spam messages are clustered together. There is a strong linear pattern for ham messages with high number of uppercase characters\")\nplt.show()","fe0b2e9d":"model = Sequential()\n\nmodel.add(Dense(1000, input_shape=(773,), activation=\"relu\"))\nmodel.add(BatchNormalization(axis=-1))\n\nmodel.add(Dense(256, activation=\"relu\"))\nmodel.add(BatchNormalization(axis=-1))\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(256, activation=\"relu\"))\nmodel.add(BatchNormalization(axis=-1))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(128, activation=\"relu\"))\nmodel.add(BatchNormalization(axis=-1))\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(10, activation=\"relu\"))\nmodel.add(BatchNormalization(axis=-1))\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(1, activation=\"sigmoid\"))","eaa59933":"# defining the learning rate, the number of epochs and the batch size\nINIT_LR = 0.001\nNUM_EPOCHS = 30\nBS = 64\nopt = RMSprop(lr = INIT_LR)\n\n# This is just a necessary step to compile the model, we don't actually need it because we're not using the old model\nmodel.compile(optimizer=opt, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n\n# Reduce the learning rate by half if validation accuracy has not increased in the last 3 epochs\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience=3, verbose=1, factor=0.5, min_lr=0.00001)\n\nfitted_network = model.fit(X_train, y_train, validation_split=0.2, batch_size=BS, epochs=NUM_EPOCHS, callbacks=[learning_rate_reduction])","2ad1d537":"# predict results\npreds = np.round(model.predict(X_test)).flatten()\n\n# Plot confusion matrix\nplt.figure(figsize=(10,4))\nheatmap = sns.heatmap(data = pd.DataFrame(confusion_matrix(y_test, preds)), annot = True, fmt = \"d\", cmap=sns.color_palette(\"Reds\", 50))\nheatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=14)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=14)\nplt.ylabel('Ground Truth')\nplt.xlabel('Prediction')\nplt.show()\n\n# Print accuracy, ROC and classification report for the test-set\nprint(f\"\"\"Accuray: {round(accuracy_score(y_test, preds), 5) * 100}%\nROC-AUC: {round(roc_auc_score(y_test, preds), 5)}\"\"\")\nprint(classification_report(y_test, preds))","00e1f19c":"We clearly see that the BERT embeddings together with the additional engineered features we get a training set where spam messages are quite close together with some outliers amongst ham messages.\n\n# Do spam messages have more words than ham messages?","60a81c71":"This plot shows us that yes **spam** messages, generally, have **more words** than ham messages. \n\n**Spam** messages seem to be concentrated around 30 words while **ham** messages are concentrated around 10 words\n\n# Is there any pattern in the length of the messages?","e81cbe8d":"# T-SNE visualization of SMS messages\n\nWe can see how the messages are distributed in manifold space. Since we have the BERT embedings for every message it should produce interesting results","2a311680":"# Run predictions on the test set\n\nOur network achieved pretty good results in the training and validation data, let's see if that is the case for the test set as well","f250fb82":"## Compiling the network\n\nWe'll use a RMSProp optimizer with 0.001 learning rate but we'll reduce this learning rate by half if the validation accuracy has not increased in the last 3 epochs (we should not be using accuracy since its a skewed dataset but it will do for now)","064c8ae4":"# Exploratory data analysis\n\nHere the idea is to get a sense of the data.\n\nFirst we can see the manifold distribution of the data using T-SNE, where we can see how the embeddings encode the messages","8f6590c5":"So the number of uppercased words does not seem to have any particular pattern with Spam\n\n## Is having the word \"free\" or \"win\"  more associated with spam messages?","f210e14c":"# Train-Test split\n\nWe'll leave a held out test set and perform our analysis on the training set","27ae9bb0":"Creating additional dataframes for plotting","00bacfa6":"We can see that there is the same amout of class imbalance as in the original dataset: **13.41%** of messages are spam\n\n# Basic feature engineering\n\nAdding just some counts based on the original messages (number of words, number of characters, etc.)","a46bff63":"- **36.95% of spam** messages contain the word \"free\" or \"win\"\n- Only **2.69% of ham** messages contain the word \"free\" or \"win\"","890735d0":"This plots shows two things:\n\n- Spam messages are, generally, longer than ham messages (which is to be expected since they have more words)\n- There seems to be a common value of around 150 characters for spam messages\n\n# Do spam messages have more uppercased words?","4d6b4ad8":"# Results\n\nIt is clear that our network with the BERT embeddings and the engineered features achieved great results\n\n- Retrieved 95% of all spam messages\n- Of all those messages the network said were spam, it was right 97% of the times\n- Only 5 ham messages were missclassified\n\nFor comparison in my [first analysis](https:\/\/www.kaggle.com\/mrlucasfischer\/bert-the-spam-detector-that-uses-just-10-words) I created a baseline random forest (with no hyperparameter tunning) that achieved 0.8806 ROC-AUC\n\nHope this helps get started using this dataset, and I hope this motivates with the usefulness of the BERT embeddings\n\nCheers! :)","88022801":"# Standardizing features\n\nFor better performance in algorithms that optimize a loss-function it is best if every feature is in the same scale. To this end lets standardize all features except the binary features and the original text column","54afe0d3":"- **Spam** messages are really clustered together in terms of message length with some spam messages having more uppercase characters than others\n\n- There is a strong linear pattern for **ham** messages with higher number of uppercased characters\n\n# Simple neural net\n\nTo demonstrate the efficiency of the embeddings we'll create a simple neural network\n\n## Defining the architecture of the network\n\nLets go for a 1000-256-256-128-10-1 layered network, not particular reason for it.\n\nWe'll add dropout to help prevent overfitting and batchnorm for better convergence"}}