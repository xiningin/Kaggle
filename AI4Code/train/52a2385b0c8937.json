{"cell_type":{"cc330c55":"code","9297c0fc":"code","371bd6c9":"code","327c9671":"code","94e678a1":"code","fd3c4265":"code","ca838e82":"code","dcd6fba6":"markdown","089c4818":"markdown","65cb365e":"markdown","bc3dfd87":"markdown","f46f7301":"markdown","0c47d413":"markdown"},"source":{"cc330c55":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OrdinalEncoder, StandardScaler\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score","9297c0fc":"df_train = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/train.csv\", index_col='id')\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/test.csv\", index_col='id')\n\nFEATURES = list(df_train.columns[:-1])\nTARGET = df_train.columns[-1]\n\ndf_train.head()","371bd6c9":"df_train.info()\ndf_train.describe()","327c9671":"df_train['n_missing'] = df_train[FEATURES].isna().sum(axis=1)\ntest['n_missing'] = test[FEATURES].isna().sum(axis=1)\n\ndf_train['std'] = df_train[FEATURES].std(axis=1)\ntest['std'] = test[FEATURES].std(axis=1)\n\nFEATURES += ['n_missing', 'std']\nn_missing = df_train['n_missing'].copy()","94e678a1":"df_train[FEATURES] = df_train[FEATURES].fillna(df_train[FEATURES].mean())\ntest[FEATURES] = test[FEATURES].fillna(test[FEATURES].mean())","fd3c4265":"from lightgbm import LGBMClassifier\n\nX = df_train.loc[:, FEATURES]\ny = df_train.loc[:, TARGET]\n\nfinal_predictions = []\nkf = KFold(n_splits=10, shuffle=True, random_state=42)\nfor fold, (train_indicies, valid_indicies) in enumerate(kf.split(X=X)):\n    X_train = X.loc[train_indicies]\n    X_valid = X.loc[valid_indicies]\n    X_test = test.copy()\n    \n    y_train = y.loc[train_indicies]\n    y_valid = y.loc[valid_indicies]\n    \n    \n    scaler = StandardScaler()\n    X_train[FEATURES] = scaler.fit_transform(X_train[FEATURES])\n    X_valid[FEATURES] = scaler.transform(X_valid[FEATURES])\n    X_test[FEATURES] = scaler.transform(X_test[FEATURES])\n    \n    model = LGBMClassifier(\n        max_depth = 3,\n        num_leaves = 7,\n        n_estimators = 10000,\n        colsample_bytree = 0.3,\n        subsample = 0.5,\n        random_state = 42,\n        reg_alpha=18,\n        reg_lambda=17,\n        learning_rate = 0.095,\n        device = 'gpu',\n        objective= 'binary'\n    )\n    \n    model.fit(X_train, y_train,\n             verbose = False,\n             eval_set = [(X_train, y_train), (X_valid, y_valid)],\n             eval_metric = \"auc\",\n             early_stopping_rounds = 200)\n    \n    preds_valid = model.predict_proba(X_valid)[:,1]\n    preds_test = model.predict_proba(X_test)[:,1]\n    final_predictions.append(preds_test)\n    print(fold, roc_auc_score(y_valid, preds_valid))\n\n","ca838e82":"preds = np.mean(np.column_stack(final_predictions), axis=1)\n\n# Make predictions\ny_pred = pd.Series(\n    preds,\n    index=X_test.index,\n    name=TARGET,\n)\n\n# Create submission file\ny_pred.to_csv(\"submission.csv\")","dcd6fba6":"# Welcome to the September 2021 Tabular Playground Competition! #\n\nIn this competition, we predict whether a customer will make an insurance claim.\n\n# Step1: Import Helpful Libraries #","089c4818":"The target `'claim'` has binary outcomes: `0` for no claim and `1` for claim.","65cb365e":"# Step3: Train Model #\n\nLet's try out a simple XGBoost model. This algorithm can handle missing values, but you could try imputing them instead.  We use `XGBClassifier` (instead of `XGBRegressor`, for instance), since this is a classification problem.","bc3dfd87":"# Make Submission #\n\nOur predictions are binary 0 and 1, but you're allowed to submit probabilities instead. In scikit-learn, you would use the `predict_proba` method instead of `predict`.\n","f46f7301":"# Missing Value\nRefer to [TPS Sep 2021 single LGBM](https:\/\/www.kaggle.com\/hiro5299834\/tps-sep-2021-single-lgbm\/notebook) by [@hiro5299834](https:\/\/www.kaggle.com\/hiro5299834)","0c47d413":"# Step2: Load Data"}}