{"cell_type":{"acb6415f":"code","43666428":"code","e51f0617":"code","991ff79a":"code","c8974416":"code","4230aee1":"code","58761579":"code","bfdc369a":"code","446a32a4":"code","75ecb065":"code","11878cbf":"code","6b82ce22":"code","722cd0f1":"code","d72b9271":"markdown","e6c42e35":"markdown","63985397":"markdown","1bb16838":"markdown","2388a428":"markdown","9c2f25de":"markdown","c5786a31":"markdown","d9d257a0":"markdown","729eb96b":"markdown","7fae9dc8":"markdown","be7dc132":"markdown"},"source":{"acb6415f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","43666428":"nRowsRead = 1000 # specify 'None' if want to read whole file\ndf = pd.read_csv('..\/input\/cusersmarildownloadsgermancsv\/german.csv', delimiter=';', encoding = \"ISO-8859-2\", nrows = nRowsRead)\ndf.dataframeName = 'german.csv'\nnRow, nCol = df.shape\nprint(f'There are {nRow} rows and {nCol} columns')\ndf.head()","e51f0617":"from sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, fbeta_score\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import GaussianNB","991ff79a":"X = df.drop(['Creditability', 'Account_Balance'], axis=1).values\ny = df['Creditability'].values\n\n# train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=42)","c8974416":"log = LogisticRegression()\nlog.fit(X_train, y_train)\ny_pred_log = log.predict(X_test)\nprint(accuracy_score(y_pred_log, y_test))\nprint(confusion_matrix(y_test, y_pred_log))\nprint(classification_report(y_test, y_pred_log))","4230aee1":"\"\"\"knn = KNeighborsClassifier()\ngrid = GridSearchCV(knn, param_grid={'n_neighbors':range(1,20)}, scoring='recall')\ngrid.fit(X_train, y_train)\n\ngrid.best_params_\n\nfor i in range(0, len(grid.cv_results_['mean_test_score'])):\n    print('N_Neighbors {}: {} '.format(i+1, grid.cv_results_['mean_test_score'][i]*100))\"\"\"\n    \n# recall peaks at k = 1\n\nknn = KNeighborsClassifier(n_neighbors = 1)\nknn.fit(X_train, y_train)\ny_pred_knn = knn.predict(X_test)\nprint(accuracy_score(y_pred_knn, y_test))\nprint(confusion_matrix(y_test, y_pred_knn))\nprint(classification_report(y_test, y_pred_knn))","58761579":"\"\"\"param_grid_svc = {\"gamma\": [0.1,0.5,1,5,10,50,100],\n                  \"C\": [0.1,0.5,1,5,10,50,100]}\n\nsvc = SVC(kernel='linear')\n\ngs_svc = GridSearchCV(svc, param_grid = param_grid_svc, cv=5, scoring='recall', verbose=4)\ngs_svc.fit(X_train, y_train)\n\ngs_svc.best_params_ # gamma = , C = \"\"\"\n\nsvc = SVC(kernel='linear', gamma=10, C=0.8)\nsvc.fit(X_train, y_train)\ny_pred_svc = svc.predict(X_test)\nprint(accuracy_score(y_pred_svc, y_test))\nprint(confusion_matrix(y_test, y_pred_svc))\nprint(classification_report(y_test, y_pred_svc))","bfdc369a":"\"\"\"param_grid_rf = {\"max_depth\": range(3,10),\n                  \"max_features\": [3,5,7,9,11,13,15,17,20],\n                  \"min_samples_leaf\": [5,10,15,20,25,30],\n                  \"n_estimators\": [3,5,10,25,50,150]}\n\nrf = RandomForestClassifier()\ngs_rf = GridSearchCV(rf, param_grid=param_grid_rf, cv=5, scoring=\"recall\", verbose=4)\ngs_rf.fit(X_train, y_train)\n\ngs_rf.best_params_\n\"\"\"\n# {'max_depth': 9, 'max_features': 15, 'min_samples_leaf': 5, 'n_estimators': 25}\n\n\nrf = RandomForestClassifier(max_depth=9, max_features=15, min_samples_leaf=5, n_estimators=25)\nrf.fit(X_train, y_train)\ny_pred_rf = rf.predict(X_test)\nprint(accuracy_score(y_pred_rf, y_test))\nprint(confusion_matrix(y_test, y_pred_rf))\nprint(classification_report(y_test, y_pred_rf))","446a32a4":"nb = GaussianNB()\nnb.fit(X_train, y_train)\ny_pred_nb = nb.predict(X_test)\nprint(accuracy_score(y_pred_nb, y_test))\nprint(confusion_matrix(y_test, y_pred_nb))\nprint(classification_report(y_test, y_pred_nb))","75ecb065":"\"\"\"param_grid_xgb = {\"max_depth\": range(3,10),\n                  \"subsample\": [0.5,0.6,0.7,0.8,0.9,1],\n                  \"eta\": [0.01,0.03,0.05,0.07,0.09,0.14,0.19],\n                  \"colsample_bytree\": [0.5,0.6,0.7,0.8,0.9,1],\n                  \"n_estimators\": [3,5,10,25,50,150]}\n\nxgb = XGBClassifier()\ngs_xgb = GridSearchCV(xgb, param_grid=param_grid_xgb, cv=5, scoring=\"recall\", verbose=4)\ngs_xgb.fit(X_train, y_train)\n\ngs_xgb.best_params_ \"\"\"\n\n\"\"\"{'colsample_bytree': 1,\n 'eta': 0.19,\n 'max_depth': 8,\n 'n_estimators': 150,\n 'subsample': 0.8}\"\"\"\n\nxgb = XGBClassifier(eta=0.19, max_depth=8, n_estimators=150, subsample=0.8, colsample_bytree=1)\nxgb.fit(X_train, y_train)\ny_pred_xgb = xgb.predict(X_test)\nprint(accuracy_score(y_pred_xgb, y_test))\nprint(confusion_matrix(y_test, y_pred_xgb))\nprint(classification_report(y_test, y_pred_xgb))","11878cbf":"x_training, x_valid, y_training, y_valid = train_test_split(X_train, y_train,\n                                                            test_size=0.5,\n                                                            random_state=42)\n#specify models\nmodel1 = LogisticRegression()\nmodel2 = SVC(kernel='linear', gamma=10, C=0.8)\nmodel3 = GaussianNB()\nmodel4 = XGBClassifier(eta=0.19, max_depth=8, n_estimators=150, subsample=0.8, colsample_bytree=1)\n#fit models\nmodel1.fit(x_training, y_training)\nmodel2.fit(x_training, y_training)\nmodel3.fit(x_training, y_training)\nmodel4.fit(x_training, y_training)\n#make pred on validation\npreds1 = model1.predict(x_valid)\npreds2 = model2.predict(x_valid)\npreds3 = model3.predict(x_valid)\npreds4 = model4.predict(x_valid)\n#make pred on test\ntestpreds1 = model1.predict(X_test)\ntestpreds2 = model2.predict(X_test)\ntestpreds3 = model3.predict(X_test)\ntestpreds4 = model4.predict(X_test)\n#form new dataset from valid and test\nstackedpredictions = np.column_stack((preds1, preds2, preds3, preds4))\nstackedtestpredictions = np.column_stack((testpreds1, testpreds2,\n                                              testpreds3, testpreds4))\n#make meta model\nmetamodel = LogisticRegression()\nmetamodel.fit(stackedpredictions, y_valid)\nfinal_predictions = metamodel.predict(stackedtestpredictions)\n    \nprint(accuracy_score(final_predictions, y_test))\nprint(confusion_matrix(y_test, final_predictions))\nprint(classification_report(y_test, final_predictions))","6b82ce22":"from sklearn.metrics import roc_curve, roc_auc_score\n\nresults_table = pd.DataFrame(columns = ['models', 'fpr','tpr','auc'])\n\npredictions = {'LR': y_pred_log, 'SVC': y_pred_svc, 'NB': y_pred_nb, 'XGB': y_pred_xgb, 'Stacked': final_predictions}\n\nfor key in predictions:\n    fpr, tpr, _ = roc_curve(y_test, predictions[key])\n    auc = roc_auc_score(y_test, predictions[key])\n    \n    results_table = results_table.append({'models': key,\n                                         'fpr' : fpr,\n                                         'tpr' : tpr,\n                                         'auc' : auc}, ignore_index=True)\n    \nresults_table.set_index('models', inplace=True)\n\nprint(results_table)\n\nfig = plt.figure(figsize = (8,6))\n\nfor i in results_table.index:\n    plt.plot(results_table.loc[i]['fpr'], \n             results_table.loc[i]['tpr'], \n             label = \"{}, AUC={:.3f}\".format(i, results_table.loc[i]['auc']))\n    \nplt.plot([0,1], [0,1], color = 'black', linestyle = '--')\n\nplt.xticks(np.arange(0.0, 1.1, step=0.1))\nplt.xlabel(\"False Positive Rate\", fontsize=15)\n\nplt.yticks(np.arange(0.0, 1.1, step=0.1))\nplt.ylabel(\"True Positive Rate\", fontsize=15)\n\nplt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\nplt.legend(prop = {'size':13}, loc = 'lower right')\n\nplt.show()","722cd0f1":"#Code by Olga Belitskaya https:\/\/www.kaggle.com\/olgabelitskaya\/sequential-data\/comments\nfrom IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#B784A7','#eb3446','Akronim','Smokum',30,15\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https:\/\/fonts.googleapis.com\/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';<\/style>\n    <h1 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s<\/h1>\"\"\"%string))\n    \n    \ndhtml('Thanks to Shoaib Mehmood for all the Code and Jabeen for the html Markdown cells Layout' )","d72b9271":"<h1 style=\"background-color: #B784A7; font-family:'Brush Script MT',cursive;color:white;font-size:200%; text-align:center;border-radius: 50% 20% \/ 10% 40%\">XGBoost<\/h1>","e6c42e35":"<h1 style=\"background-color: #B784A7; font-family:'Brush Script MT',cursive;color:white;font-size:200%; text-align:center;border-radius: 50% 20% \/ 10% 40%\"> Stacked Model<\/h1>","63985397":"![](https:\/\/mlfromscratch.com\/content\/images\/2020\/01\/model_stacking_overview-4.png)mlfromscratch.com","1bb16838":"<h1 style=\"background-color: #B784A7; font-family:'Brush Script MT',cursive;color:white;font-size:200%; text-align:center;border-radius: 50% 20% \/ 10% 40%\">Support Vector Classification<\/h1>","2388a428":"<h1 style=\"background-color: #B784A7; font-family:'Brush Script MT',cursive;color:white;font-size:200%; text-align:center;border-radius: 50% 20% \/ 10% 40%\">K-Nearest Neighbors<\/h1>","9c2f25de":"<h1 style=\"background-color: #B784A7; font-family:'Brush Script MT',cursive;color:white;font-size:200%; text-align:center;border-radius: 50% 20% \/ 10% 40%\">Conclusion<\/h1>","c5786a31":"<h1 style=\"background-color: #B784A7; font-family:'Brush Script MT',cursive;color:white;font-size:200%; text-align:center;border-radius: 50% 20% \/ 10% 40%\">Logistic Regression<\/h1>","d9d257a0":"<h1 style=\"background-color: #B784A7; font-family:'Brush Script MT',cursive;color:white;font-size:200%; text-align:center;border-radius: 50% 20% \/ 10% 40%\">Naive-Bayes<\/h1>","729eb96b":"<h1 style=\"background-color: #B784A7; font-family:'Brush Script MT',cursive;color:white;font-size:200%; text-align:center;border-radius: 50% 20% \/ 10% 40%\">Random Forest<\/h1>","7fae9dc8":"I'll try to interpret those results. \n\nNaive-Bayes classifier had the higher Accuracy:\n                               0.75      \n\n\nAnd while plotting ROC curves and calculate the AUC for all our models too with AUC=0.694\n\n\nThe Stacked model:\n    accuracy                           0.73      \n\n\n\nAccording to that Code, to approve the loan based on the Creditability of an applicant,  Naive-Bayes Classifier should be the choice.  Though in my personal experience of the \"facts of life\", an individual analysis of each case will conduct to better results in any field. ","be7dc132":"#Code by Shoaib Mehmood https:\/\/www.kaggle.com\/shoaibmnagi\/credit-risk-modelling-german-data\/notebook\n\n#Layout of Markdown cells by Jabeen https:\/\/www.kaggle.com\/jabeen12\/stroke-prediction-99-data-visualization"}}