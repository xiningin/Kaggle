{"cell_type":{"6bbc3fcf":"code","5699a3c0":"code","9603f2b3":"code","e1de6996":"code","064704ab":"code","bdf2135f":"code","61020cfb":"code","aaeed0f4":"code","8ba0295d":"code","3c0e699a":"code","47825c29":"code","e397bd97":"code","44bb4268":"code","facb02f7":"code","14859c6d":"code","dfc3f8f3":"code","30a10402":"code","86c96f6d":"code","d18a2a53":"code","96702a46":"code","edf67ac5":"code","0a024c5d":"code","f86d30b8":"code","a8183fc8":"code","a8ff5704":"code","9accff50":"code","764ae790":"code","285f1d90":"code","6c24c988":"code","aaf79d4f":"code","6c60e717":"code","e5f78884":"code","ab207724":"code","0272e8c2":"code","675fe3ac":"code","44e8afbd":"markdown","db188539":"markdown","c6bc5910":"markdown","1ea6d9a6":"markdown","2bc52d65":"markdown"},"source":{"6bbc3fcf":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.layers import Dense\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.tree import DecisionTreeClassifier\nfrom tensorflow.keras.models import Sequential\nimport seaborn as sns","5699a3c0":"loans = pd.read_csv('..\/input\/should-this-loan-be-approved-or-denied\/SBAnational.csv')\nloans.head()","9603f2b3":"loans.shape","e1de6996":"loans.isnull().sum()","064704ab":"cor_fig, cor_ax = plt.subplots(figsize=(15, 10))\n# we nneded to make MIS_Status binary in order to display it in the correlation matrix\n# it is a part of the data preparation\nloans['Defaulted'] = [1 if app == 'CHGOFF' else 0 for app in loans.MIS_Status.values]\ncorr_matrix = loans.corr()\ncor_ax = sns.heatmap(corr_matrix, annot=True)\nplt.xticks(rotation=30, horizontalalignment='right', fontsize=22)\nplt.yticks(fontsize=22)\n\nplt.savefig('correlation.png')\nplt.show()","bdf2135f":"fig2 = plt.figure(figsize=(20,8))\nax1 = fig2.add_subplot(1, 2, 1)\nplt.rcParams.update({'font.size': 14})\nsns.countplot(x=\"NewExist\", hue=\"Defaulted\", data=loans)\nplt.legend()\n\nlabels = (\"NULL\", \"NEW BUSINESS\", \"EXISTING BUSINESS\")\npositions = (0, 1, 2)\nplt.xticks(positions, labels)\nplt.savefig('new_exist.png')\nplt.show()","61020cfb":"fig3 = plt.figure(figsize=(50, 10))\n\nax1 = fig3.add_subplot(1, 2, 1)\n\ndf = loans.copy()\n\ndf['Default'] = np.where(df['MIS_Status'] == 'P I F', 0, 1)\ndf['Default'].value_counts()\n\ndata = df.groupby(['State', 'Default'])['State'].count().unstack('Default')\ndata.fillna(0)\n\nax1.bar(data.index, data[1], label='Default')\nax1.bar(data.index, data[0], bottom=data[1], label='Paid in full')\n\nax1.set_title('Number of PIF\/Defaulted Loans by State from 1984-2010', fontsize=15)\nax1.set_xlabel('State')\nax1.set_ylabel('Number of PIF\/Defaulted Loans')\nax1.legend()\n\nplt.tight_layout()\n\nplt.savefig('states.png')\nplt.show()","aaeed0f4":"loans.columns","8ba0295d":"# we want to make a binary value stating if the loan was defaulted or not\nloans = loans.drop(columns=['MIS_Status'])\n\n# we only take the loans created before 2007\nloans['ApprovalFY'] = loans['ApprovalFY'].replace({'A':'','B':''}, regex = True).astype(int)\nloans.drop(loans[loans['ApprovalFY']>2007].index, axis = 0, inplace = True)\n\nloans = loans.drop(columns=['ApprovalFY','ApprovalDate','DisbursementDate','ChgOffDate','LoanNr_ChkDgt','Name','Zip', 'City', 'Bank'])","3c0e699a":"loans.head()","47825c29":"loans.shape","e397bd97":"def amount_to_float(number):\n    num = number.replace(\"$\", \"\")\n    num = num.replace(\",\",\"\")\n    num = num.replace(\" \",\"\")\n    return float(num)\n\n#instead of dealing with money as a string we will deal with it as a number\nloans['BalanceGross'] = loans['BalanceGross'].apply(lambda x: amount_to_float(x))\nloans['DisbursementGross'] = loans['DisbursementGross'].apply(lambda x: amount_to_float(x))\nloans['ChgOffPrinGr'] = loans['ChgOffPrinGr'].apply(lambda x: amount_to_float(x))\nloans['GrAppv'] = loans['GrAppv'].apply(lambda x: amount_to_float(x))\nloans['SBA_Appv'] = loans['SBA_Appv'].apply(lambda x: amount_to_float(x))","44bb4268":"# we replace it to be a binary where 1 to be a new business and 0 an old business\nloans['NewExist'] = loans['NewExist'].replace(1,0)\nloans['NewExist'] = loans['NewExist'].replace(2,1)","facb02f7":"# we replace the LowDoc values to be binary instead of Y and N\nloans['LowDoc'] = loans['LowDoc'].replace({'Y':'1', 'N':'0'}, regex=True)\nvalid = ['1', '0']\nloans = loans.loc[loans['LowDoc'].isin(valid)]\nloans['LowDoc'] = loans['LowDoc'].astype(int)","14859c6d":"# we replace the RevLineCr values to be binary instead of Y and N\nloans['RevLineCr'] = loans['RevLineCr'].replace({'Y':'1', 'N':'0'}, regex=True)\nloans = loans.loc[loans['RevLineCr'].isin(valid)]\nloans['RevLineCr'] = loans['RevLineCr'].astype(int)","dfc3f8f3":"# we make FranchiseCode a binary column named Franchise where 1 is true and 0 is false\nloans['FranchiseCode'] = loans['FranchiseCode'].replace(1,0)\nloans['FranchiseCode'] = np.where((loans.FranchiseCode != 0),1,loans.FranchiseCode)\nloans.rename(columns={\"FranchiseCode\":\"Franchise\"},inplace=True)","30a10402":"loans.shape","86c96f6d":"# we get dummies for the catagorical columns BankState and State\nloans = pd.get_dummies(loans,columns=['BankState','State'],prefix=['BankState','State'])","d18a2a53":"# only take the first 2 digits from the NAICS values\ndef first_n_digits(number, n):\n    return int(str(number)[:n])\nloans['NAICS'] = loans['NAICS'].apply(lambda x: first_n_digits(x,2))","96702a46":"loans.head()","edf67ac5":"loans['NAICS'].unique()","0a024c5d":"loans.columns","f86d30b8":"loans.dtypes","a8183fc8":"# we seperate the values we want to classify\ny = loans['Defaulted']\nX = loans.drop(columns=['Defaulted'])\ny_backup = y\nX = X.to_numpy()\ny = pd.get_dummies(y)\ny = y.to_numpy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, stratify=y)","a8ff5704":"# the ration between the 0s and 1s is about 1:5\ny_backup.value_counts()","9accff50":"# we create the encoder layers of the auto-encoder neuron network\nn_inputs = X.shape[1]\ne = Sequential()\ne.add(Dense(100, activation = 'tanh', input_shape = [n_inputs]))\ne.add(Dense(50, activation = 'tanh'))\ne.add(Dense(16, activation = 'tanh'))\ne.add(Dense(2, activation = 'tanh'))","764ae790":"# we create the decoder layers of the auto-encoder neuron network\nd = Sequential()\nd.add(Dense(16, activation = 'tanh', input_shape = [2]))\nd.add(Dense(50, activation = 'tanh'))\nd.add(Dense(100, activation = 'tanh'))\nd.add(Dense(n_inputs, activation = 'tanh'))","285f1d90":"# define auto-encoder model\nautoencoder = Sequential([e, d])","6c24c988":"# compile auto-encoder model\nautoencoder.compile(optimizer='adam', loss='mse')\n\nn_epochs = 10\nbatch_size = 256\n\nX_train = np.asarray(X_train).astype('float32')\n\n# we train the auto-encoder with the X_train\nfit_time = autoencoder.fit(X_train, X_train, epochs=n_epochs, batch_size=batch_size,verbose=0)","aaf79d4f":"# we predict the output values of the encoder for the train and test via compression\nenc_X_train = e.predict(X_train)\nenc_X_test = e.predict(X_test)","6c60e717":"# we use a Decision Tree Classifier to evaluate our model\nclf = DecisionTreeClassifier()\n\ny_train_labels = np.argmax(y_train, axis = 1)\ny_test_labels = np.argmax(y_test, axis = 1)","e5f78884":"clf.fit(enc_X_train, y_train_labels)","ab207724":"y_pred = clf.predict(enc_X_test)\n\nprint(classification_report(y_test_labels, y_pred))","0272e8c2":"from sklearn.metrics import plot_confusion_matrix\n\nplot_confusion_matrix(clf,enc_X_test,y_test_labels, normalize = 'true')","675fe3ac":"cm = confusion_matrix(y_test_labels, y_pred)\nTN = cm[0][0]\nFN = cm[1][0]\nTP = cm[1][1]\nFP = cm[0][1]\n\n# we use the WACC matric from the article\n# http:\/\/store.ectap.ro\/articole\/1421.pdf\n\nWACC = 0.25*(TP\/(TP+FN))+0.75*(TN\/(TN+FP)) \nprint('The WACC rank of our model is: '+str(WACC))","44e8afbd":"# **Data Preparation**","db188539":"**Mark Oulitin 208283291**\n\n**Itay Cohen 211896261**","c6bc5910":"# **Evaluation**","1ea6d9a6":"# **Data Understanding**","2bc52d65":"# **Modeling**"}}