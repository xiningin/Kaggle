{"cell_type":{"e170ac31":"code","b62862ba":"code","0d834f7e":"code","f1c2dcad":"code","6c5a8910":"code","ceaa7dd5":"code","69a1fcea":"code","ae3466c1":"code","fca85b5a":"code","2c4e8916":"code","c7ac37e6":"code","65012b5f":"code","16254b82":"code","08512b86":"code","97cad080":"code","d6474177":"code","e46a9618":"code","2461638b":"code","80758273":"code","0d51d0bb":"code","a069050d":"code","082149fa":"code","a8644ee4":"code","c939e2a4":"code","5794ddea":"code","a1fdd760":"code","733b7693":"code","b4334262":"code","86bab913":"code","607abcfc":"code","a646417b":"markdown","b702ba96":"markdown","9a53e51f":"markdown","c5563501":"markdown","36d0a6ab":"markdown","48519a2a":"markdown","2a2f94d2":"markdown","5598796b":"markdown","39c6551b":"markdown","5e2b2f37":"markdown"},"source":{"e170ac31":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing import text, sequence\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D\nfrom sklearn.model_selection import train_test_split\nprint(tf.__version__)","b62862ba":"train_df = pd.read_csv('\/content\/train.csv').fillna(' ')\ntrain_df.sample(10, random_state=1)","0d834f7e":"x = train_df['comment_text'].values\nprint(x)","f1c2dcad":"# View few toxic comments\ntrain_df.loc[train_df['toxic']==1].sample(10, random_state=10)","6c5a8910":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\ntext = train_df['comment_text'].loc[train_df['toxic']==1].values\nwordcloud = WordCloud(\n    width = 640,\n    height = 640,\n    background_color = 'black',\n    stopwords = STOPWORDS).generate(str(text))\nfig = plt.figure(\n    figsize = (12, 8),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","ceaa7dd5":"y = train_df['toxic'].values\nprint(y)","69a1fcea":"# Plot frequency of toxic comments\ntrain_df['toxic'].plot(kind='hist', title='Distribution of Toxic Comments');","ae3466c1":"train_df['toxic'].value_counts()","fca85b5a":"max_features = 20000\nmax_text_length = 400","2c4e8916":"x_tokenizer = text.Tokenizer(max_features)\nx_tokenizer.fit_on_texts(list(x))\nx_tokenized = x_tokenizer.texts_to_sequences(x) #list of lists(containing numbers), so basically a list of sequences, not a numpy array\n#pad_sequences:transform a list of num_samples sequences (lists of scalars) into a 2D Numpy array of shape \nx_train_val = sequence.pad_sequences(x_tokenized, maxlen=max_text_length)","c7ac37e6":"# Download and extract GloVe embeddings\n!wget http:\/\/nlp.stanford.edu\/data\/glove.6B.zip\n!unzip glove.6B.zip","65012b5f":"embedding_dims = 100\nembeddings_index = dict()\nf = open('glove.6B.100d.txt')\nfor line in f:\n  values = line.split()\n  word = values[0]\n  coefs = np.asarray(values[1:], dtype='float32')\n  embeddings_index[word] = coefs\nf.close()\n\nembedding_matrix = np.zeros((max_features, embedding_dims))\nfor word, index in x_tokenizer.word_index.items():\n  if index > max_features -1:\n    break\n  else:\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n      embedding_matrix[index] = embedding_vector","16254b82":"print('Build model...')\nmodel = Sequential()\n# we start off with an efficient embedding layer which maps\n# our vocab indices into embedding_dims dimensions\n#load pre-trained word embeddings into an Embedding layer\n# note that we set trainable = False so as to keep the embeddings fixed\n#(we don't want to update them during training).\nmodel.add(Embedding(max_features,\n                    embedding_dims,\n                    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n                    trainable=False))\nmodel.add(Dropout(0.2))","08512b86":"filters = 250\nkernel_size = 3\nhidden_dims = 250","97cad080":"# we add a Convolution1D, which will learn filters\n# word group filters of size filter_length:\nmodel.add(Conv1D(filters,\n                 kernel_size,\n                 padding='valid',\n                 activation='relu'))\nmodel.add(MaxPooling1D())\nmodel.add(Conv1D(filters,\n                 5,\n                 padding='valid',\n                 activation='relu'))\n# we use max pooling:\nmodel.add(GlobalMaxPooling1D())\n# We add a vanilla hidden layer:\nmodel.add(Dense(hidden_dims, activation='relu'))\nmodel.add(Dropout(0.2))\n\n# We project onto 6 output layers, and squash it with a sigmoid:\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()","d6474177":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","e46a9618":"x_train, x_val, y_train, y_val = train_test_split(x_train_val, y, test_size=0.15, random_state=1)","2461638b":"batch_size = 32\nepochs = 3","80758273":"model.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          validation_data=(x_val, y_val))","0d51d0bb":"model.evaluate(x_val, y_val, batch_size=128)","a069050d":"test_df = pd.read_csv('.\/test.csv')","082149fa":"x_test = test_df['comment_text'].values","a8644ee4":"x_test_tokenized = x_tokenizer.texts_to_sequences(x_test)\nx_testing = sequence.pad_sequences(x_test_tokenized, maxlen=max_text_length)","c939e2a4":"y_testing = model.predict(x_testing, verbose = 1, batch_size=32)","5794ddea":"y_testing.shape","a1fdd760":"y_testing[0]","733b7693":"test_df['Toxic'] = ['not toxic' if x < .5 else 'toxic' for x in y_testing]\ntest_df[['comment_text', 'Toxic']].head(20)#.sample(20, random_state=1)","b4334262":"word_index = x_tokenizer.word_index\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])","86bab913":"e = model.layers[0]\nweights = e.get_weights()[0]\nprint(weights.shape)","607abcfc":"import io\n\nout_v = io.open('vecs.tsv', 'w', encoding='utf-8')\nout_m = io.open('meta.tsv', 'w', encoding='utf-8')\n\nfor word_num in range(max_features):\n  word = reverse_word_index[word_num+1]\n  embeddings = weights[word_num]\n  out_m.write(word + \"\\n\")\n  out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\nout_v.close()\nout_m.close()","a646417b":"## (Optional) Task 8: Visualize Embeddings","b702ba96":"## Task 2: Load and Explore Data","9a53e51f":"## Task 3: Data Prep \u2014 Tokenize and Pad Text Data","c5563501":"<h2 align=center> Toxic Comments Classification using 1D CNN with Keras<\/h2>","36d0a6ab":"## Task 1: Import Packages and Functions","48519a2a":"## Task 7: Evaluate Model","2a2f94d2":"## Task 5: Create Embedding Layer","5598796b":"### Task 6: Build the Model","39c6551b":"## Task 6: Train Model","5e2b2f37":"## Task 4: Prepare Embedding Matrix with Pre-trained GloVe Embeddings"}}