{"cell_type":{"cff0350f":"code","d4abf054":"code","8d65a91f":"code","54b53265":"code","bf39501d":"code","37cce018":"code","9f22b3f2":"code","6e47cab0":"code","46df4df4":"code","092223ae":"code","9eea505f":"code","ce3500c2":"code","6972dd6f":"code","57123bcf":"code","60cd59d8":"code","555f1dce":"code","a51c6098":"code","abbf7c97":"code","739858f7":"code","c961d5d6":"code","fa364d3a":"code","a0d5189e":"code","b5575477":"code","300cf014":"code","510e01b4":"code","ead5629e":"code","a313b839":"code","da6cbe98":"code","e418358d":"code","6b7e5bd3":"code","74f1d902":"code","5f7455ef":"code","f4a9a96a":"code","6bcf2abf":"code","4ef3227c":"code","ee9743e6":"code","4125f00b":"code","afba457b":"code","ca2e7024":"code","fa0b5bd7":"markdown","ec50debb":"markdown","a6d61ce3":"markdown","f2fecfb3":"markdown","1438f051":"markdown","12e595ce":"markdown","276dc7a2":"markdown","c4207388":"markdown","2eaa23dc":"markdown","a4484b15":"markdown","09bc1415":"markdown","41c3190d":"markdown","2b4bb99b":"markdown","58278e6c":"markdown","0c920574":"markdown","6a20cda4":"markdown","fac0cc81":"markdown","975b7444":"markdown","66d3c794":"markdown"},"source":{"cff0350f":"import pandas as pd  # data analysis library\nimport numpy as np  # library linear algebra, Fourier transform and random numbers\n\n# sklearn - \u0430 set of python modules for machine learning and data mining\nfrom sklearn.ensemble import RandomForestRegressor  # using the Random Forest Regressor\nfrom sklearn.feature_extraction.text import TfidfVectorizer  # for convert a collection of raw documents to a matrix of TF-IDF features\nfrom sklearn.linear_model import Ridge, LinearRegression  # Ridge - Linear least squares with l2 regularization, Linear Regression - ordinary least squares\nfrom sklearn.pipeline import Pipeline, FeatureUnion  # module implements utilities to build a composite estimator, as a chain of transforms and estimators\nfrom sklearn.base import TransformerMixin, BaseEstimator # TransformerMixin - Mixin class for all transformers in scikit-learn.\n\n\nimport re  # module for working with regular expressions\nimport scipy  # library is built to work with NumPy arrays, and provides efficient numerical routines such as routines for numerical integration and optimization\nfrom scipy import sparse  # SciPy 2-D sparse matrix package for numeric data\nimport gc # Garbage Collector - module provides the ability to disable the collector, tune the collection frequency, and set debugging options\nfrom IPython.display import display, HTML  # Jupyter kernel to work with Python code in Jupyter notebooks and other interactive frontends\nfrom pprint import pprint  # module provides a capability to \u201cpretty-print\u201d arbitrary Python data structures in a form which can be used as input to the interpreter\nimport warnings  # Warning messages are typically issued in situations where it is useful to alert the user of some condition in a program\n\nwarnings.filterwarnings(\"ignore\")  # This is the base class of all warning category classes. It is a subclass of Exception. \n# The warnings filter controls whether warnings are ignored, displayed, or turned into errors (raising an exception) \n# \"ignore\" - never print matching warnings\n\npd.options.display.max_colwidth=300  # The maximum width in characters of a column in the repr of a pandas data structure. \n#Wen the column overflows, a \u201c\u2026\u201d placeholder is embedded in the output","d4abf054":"# this block is needed only for understanding what data we are working with\n#  We use data from the 2017 competition \"The problem of classification of toxic comments\"\ndf = pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv\")  # read the data for training and put it in the date frame 'df'\nprint(df.shape)  # display information about the size of the table, the size of the table is (159571 lines, 8 columns)\n# of 8 columns, one column is the comment number, the second is the comment text, \n# and another 6 columns are the relationship to the degree of toxicity: 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'\n\nfor col in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']:  # we iterate over each taxation column in the table\n    print(f'****** {col} *******')  # display the name of the processed column\n    display(df.loc[df[col]==1,['comment_text',col]].sample(10))  \n    # we will display 10 examples (rows) of the table each in which the column of the value of the given taxation category is equal to one","8d65a91f":"\n# Give more weight to severe toxic \ndf['severe_toxic'] = df.severe_toxic * 2  # multiply the highly toxic value of the column by 2. While the remaining toxicity columns remain at one.\ndf['y'] = (df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) ).astype(int)\n# Let's add one more column \"y\" to our dataframe - equal to the sum of all toxicity values.\n# Since we have 6 degrees of toxicity with maximum values in the column:\n# 'toxic' = 1\n# 'severe_toxic' = 2\n# 'obscene' = 1\n# 'threat' = 1\n# 'insult' = 1\n# 'identity_hate' = 1\n# the most toxic comment will collect all levels of toxicity 1 + 2 + 1 + 1 + 1 + 1 = 7\n\ndf['y'] = df['y']\/df['y'].max()  # Let's normalize the values, not from 0 to 7, but from 0 to 1.\n# Where 0 is a non-toxic comment, 1 - corresponds to the presence of all signs of toxicity\n\ndf = df[['comment_text', 'y']].rename(columns={'comment_text': 'text'})  # rename column 'comment_text' in 'text'\ndf.sample(5)  # we will display 5 examples (rows)","54b53265":"df['y'].value_counts()  # we will display statistics for the entire table, how many comments correspond to one of the 8 degrees of toxicity","bf39501d":"# Divide the resulting dataframe into 7 and save each in a separate csv output file. \n# It should be noted that the division into 7 folders is not linear, so we minimize the skew in the number of values, although it will not play a special role here.\nn_folds = 7  # number of folders\n\nfrac_1 = 0.7\nfrac_1_factor = 1.5\n\nfor fld in range(n_folds):  # iterate over each of the 7 folders in turn\n    print(f'Fold: {fld}')  # display the name of the currently formed folder\n    tmp_df = pd.concat([df[df.y>0].sample(frac=frac_1, random_state = 10*(fld+1)) , \n                        df[df.y==0].sample(n=int(len(df[df.y>0])*frac_1*frac_1_factor) , \n                                            random_state = 10*(fld+1))], axis=0).sample(frac=1, random_state = 10*(fld+1))\n    # use handling of joining pandas objects along a specific axis with optional setup logic\n\n    tmp_df.to_csv(f'\/kaggle\/working\/df_fld{fld}.csv', index=False)  # save the resulting folder dataframe to a csv file and mark it in a folder '\/kaggle\/working\/'\n    print(tmp_df.shape)  # display statistics for in this file, how many comments correspond to one of the 8 degrees of toxicity \n    print(tmp_df['y'].value_counts())  # display statistics in this file. As we can see, all files will contain the same number of lines.","37cce018":"\ndef clean(data, col):  # Replace each occurrence of pattern\/regex in the Series\/Index\n\n    # Clean some punctutations\n    data[col] = data[col].str.replace('\\n', ' \\n ')  \n    data[col] = data[col].str.replace(r'([a-zA-Z]+)([\/!?.])([a-zA-Z]+)',r'\\1 \\2 \\3')\n    # Replace repeating characters more than 3 times to length of 3\n    data[col] = data[col].str.replace(r'([*!?\\'])\\1\\1{2,}',r'\\1\\1\\1')    \n    # Add space around repeating characters\n    data[col] = data[col].str.replace(r'([*!?\\']+)',r' \\1 ')    \n    # patterns with repeating characters \n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1{2,}\\b',r'\\1\\1')\n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1\\1{2,}\\B',r'\\1\\1\\1')\n    data[col] = data[col].str.replace(r'[ ]{2,}',' ').str.strip()   \n    \n    return data  # the function returns the processed value","9f22b3f2":"# Test clean function\ntest_clean_df = pd.DataFrame({\"text\":\n                              [\"heyy\\n\\nkkdsfj\",\n                               \"hi   how\/are\/you ???\",\n                               \"hey?????\",\n                               \"noooo!!!!!!!!!   comeone !! \",\n                              \"cooooooooool     brooooooooooo  coool brooo\",\n                              \"naaaahhhhhhh\"]})\ndisplay(test_clean_df)  # display the test function before transformation\nclean(test_clean_df,'text')  # display the test function after transformation","6e47cab0":"df = clean(df,'text')  # clear the whole date frame","46df4df4":"# # Divide the resulting cleared dataframe by 7 and save each in a separate output csv file.\n# It should be noted that, as before, the separation rule is respected. In this way, we created 7 cleared and not cleared data files.\nn_folds = 7  # number of folders\n\nfrac_1 = 0.7\nfrac_1_factor = 1.5\n\nfor fld in range(n_folds):  # iterate over each of the 7 folders in turn\n    print(f'Fold: {fld}')  # display the name of the currently formed folder\n    tmp_df = pd.concat([df[df.y>0].sample(frac=frac_1, random_state = 10*(fld+1)) , \n                        df[df.y==0].sample(n=int(len(df[df.y>0])*frac_1*frac_1_factor) , \n                                            random_state = 10*(fld+1))], axis=0).sample(frac=1, random_state = 10*(fld+1))\n     # use handling of joining pandas objects along a specific axis with optional setup logic\n        \n        \n    tmp_df.to_csv(f'\/kaggle\/working\/df_clean_fld{fld}.csv', index=False)  # save the resulting folder dataframe to a csv file and mark it in a folder '\/kaggle\/working\/'\n    print(tmp_df.shape)  # display statistics for in this file, how many comments correspond to one of the 8 degrees of toxicity\n    print(tmp_df['y'].value_counts())  # display statistics in this file. As we can see, all files will contain the same number of lines.","092223ae":"del df,tmp_df  # remove the applied date frames\ngc.collect()  # With no arguments, run a full collection, ","9eea505f":"df_ = pd.read_csv(\"..\/input\/ruddit-jigsaw-dataset\/Dataset\/ruddit_with_text.csv\")  # create a dateframe based on a file\n\nprint(df_.shape)  # display its size\n\ndf_ = df_[['txt', 'offensiveness_score']].rename(columns={'txt': 'text',\n                                                                'offensiveness_score':'y'})  # change columns\n\ndf_['y'] = (df_['y'] - df_.y.min()) \/ (df_.y.max() - df_.y.min())  # converting all toxicity values from 0 to 1\ndf_.y.hist()  # display all values on the histogram","ce3500c2":"# Divide the resulting cleared dataframe by 7 and save each in a separate output csv file.\nn_folds = 7  # number of folders\n\nfrac_1 = 0.7  # for all categories we take 70% of the original amount\n\nfor fld in range(n_folds):  # iterate over each of the 7 folders in turn\n    print(f'Fold: {fld}')  # display the name of the currently formed folder\n    tmp_df = df_.sample(frac=frac_1, random_state = 10*(fld+1))  # use handling of joining pandas objects along a specific axis with optional setup logic\n    tmp_df.to_csv(f'\/kaggle\/working\/df2_fld{fld}.csv', index=False)  # save the resulting folder dataframe to a csv file and mark it in a folder '\/kaggle\/working\/'\n    print(tmp_df.shape)  # display statistics for in this file\n    print(tmp_df['y'].value_counts())  # display statistics in this file. As we can see, all files will contain the same number of lines.\n","6972dd6f":"del tmp_df, df_;  # remove the applied date frames\ngc.collect()  # With no arguments, run a full collection","57123bcf":"\n# Validation data \n\ndf_val = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\")  # create a variable dataframe containing data from the original competition data file\nprint(df_val.shape)  # display statistics for in this file\nprint(df_val.head())  # display the first 5 rows of the dataframe table","60cd59d8":"# Test data\n\ndf_sub = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")  # create a variable dataframe containing data from the original competition data file\nprint(df_sub.shape)  # display statistics for in this file\nprint(df_sub.head())\n","555f1dce":"# NOT USED \n# class LengthTransformer(BaseEstimator, TransformerMixin):\n\n#     def fit(self, X, y=None):\n#         return self\n#     def transform(self, X):\n#         return sparse.csr_matrix([[(len(x)-360)\/550] for x in X])\n#     def get_feature_names(self):\n#         return [\"lngth\"]\n\nclass LengthUpperTransformer(BaseEstimator, TransformerMixin):\n\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return sparse.csr_matrix([[sum([1 for y in x if y.isupper()])\/len(x)] for x in X])\n    def get_feature_names(self):\n        return [\"lngth_uppercase\"]","a51c6098":"\ndf_val['upper_1'] = np.array(LengthUpperTransformer().transform(df_val['less_toxic']).todense()).reshape(-1,1)\ndf_val['upper_2'] = np.array(LengthUpperTransformer().transform(df_val['more_toxic']).todense()).reshape(-1,1)\n\nprint(df_val['upper_1'].mean(), df_val['upper_1'].std())\nprint(df_val['upper_2'].mean(), df_val['upper_2'].std())\n\ndf_val['upper_1'].hist(bins=100)\ndf_val['upper_2'].hist(bins=100)","abbf7c97":"val_preds_arr1 = np.zeros((df_val.shape[0], n_folds))\nval_preds_arr2 = np.zeros((df_val.shape[0], n_folds))\ntest_preds_arr = np.zeros((df_sub.shape[0], n_folds))\n\nfor fld in range(n_folds):\n    print(\"\\n\\n\")\n    print(f' ****************************** FOLD: {fld} ******************************')\n    df = pd.read_csv(f'\/kaggle\/working\/df_fld{fld}.csv')\n    print(df.shape)\n\n    features = FeatureUnion([\n        #('vect1', LengthTransformer()),\n        #('vect2', LengthUpperTransformer()),\n        (\"vect3\", TfidfVectorizer(min_df= 3, max_df=0.5, analyzer = 'char_wb', ngram_range = (3,5))),\n        #(\"vect4\", TfidfVectorizer(min_df= 5, max_df=0.5, analyzer = 'word', token_pattern=r'(?u)\\b\\w{8,}\\b')),\n\n    ])\n    pipeline = Pipeline(\n        [\n            (\"features\", features),\n            #(\"clf\", RandomForestRegressor(n_estimators = 5, min_sample_leaf=3)),\n            (\"clf\", Ridge()),\n            #(\"clf\",LinearRegression())\n        ]\n    )\n    print(\"\\nTrain:\")\n    # Train the pipeline\n    pipeline.fit(df['text'], df['y'])\n    \n    # What are the important features for toxicity\n\n    print('\\nTotal number of features:', len(pipeline['features'].get_feature_names()) )\n\n    feature_wts = sorted(list(zip(pipeline['features'].get_feature_names(), \n                                  np.round(pipeline['clf'].coef_,2) )), \n                         key = lambda x:x[1], \n                         reverse=True)\n\n    pprint(feature_wts[:30])\n    \n    print(\"\\npredict validation data \")\n    val_preds_arr1[:,fld] = pipeline.predict(df_val['less_toxic'])\n    val_preds_arr2[:,fld] = pipeline.predict(df_val['more_toxic'])\n\n    print(\"\\npredict test data \")\n    test_preds_arr[:,fld] = pipeline.predict(df_sub['text'])","739858f7":"val_preds_arr1c = np.zeros((df_val.shape[0], n_folds))\nval_preds_arr2c = np.zeros((df_val.shape[0], n_folds))\ntest_preds_arrc = np.zeros((df_sub.shape[0], n_folds))\n\nfor fld in range(n_folds):\n    print(\"\\n\\n\")\n    print(f' ****************************** FOLD: {fld} ******************************')\n    df = pd.read_csv(f'\/kaggle\/working\/df_clean_fld{fld}.csv')\n    print(df.shape)\n\n    features = FeatureUnion([\n        #('vect1', LengthTransformer()),\n        #('vect2', LengthUpperTransformer()),\n        (\"vect3\", TfidfVectorizer(min_df= 3, max_df=0.5, analyzer = 'char_wb', ngram_range = (3,5))),\n        #(\"vect4\", TfidfVectorizer(min_df= 5, max_df=0.5, analyzer = 'word', token_pattern=r'(?u)\\b\\w{8,}\\b')),\n\n    ])\n    pipeline = Pipeline(\n        [\n            (\"features\", features),\n            #(\"clf\", RandomForestRegressor(n_estimators = 5, min_sample_leaf=3)),\n            (\"clf\", Ridge()),\n            #(\"clf\",LinearRegression())\n        ]\n    )\n    print(\"\\nTrain:\")\n    # Train the pipeline\n    pipeline.fit(df['text'], df['y'])\n    \n    # What are the important features for toxicity\n\n    print('\\nTotal number of features:', len(pipeline['features'].get_feature_names()) )\n\n    feature_wts = sorted(list(zip(pipeline['features'].get_feature_names(), \n                                  np.round(pipeline['clf'].coef_,2) )), \n                         key = lambda x:x[1], \n                         reverse=True)\n\n    pprint(feature_wts[:30])\n    \n    print(\"\\npredict validation data \")\n    val_preds_arr1c[:,fld] = pipeline.predict(df_val['less_toxic'])\n    val_preds_arr2c[:,fld] = pipeline.predict(df_val['more_toxic'])\n\n    print(\"\\npredict test data \")\n    test_preds_arrc[:,fld] = pipeline.predict(df_sub['text'])","c961d5d6":"val_preds_arr1_ = np.zeros((df_val.shape[0], n_folds))\nval_preds_arr2_ = np.zeros((df_val.shape[0], n_folds))\ntest_preds_arr_ = np.zeros((df_sub.shape[0], n_folds))\n\nfor fld in range(n_folds):\n    print(\"\\n\\n\")\n    print(f' ****************************** FOLD: {fld} ******************************')\n    df = pd.read_csv(f'\/kaggle\/working\/df2_fld{fld}.csv')\n    print(df.shape)\n\n    features = FeatureUnion([\n        #('vect1', LengthTransformer()),\n        #('vect2', LengthUpperTransformer()),\n        (\"vect3\", TfidfVectorizer(min_df= 3, max_df=0.5, analyzer = 'char_wb', ngram_range = (3,5))),\n        #(\"vect4\", TfidfVectorizer(min_df= 5, max_df=0.5, analyzer = 'word', token_pattern=r'(?u)\\b\\w{8,}\\b')),\n\n    ])\n    pipeline = Pipeline(\n        [\n            (\"features\", features),\n            #(\"clf\", RandomForestRegressor(n_estimators = 5, min_sample_leaf=3)),\n            (\"clf\", Ridge()),\n            #(\"clf\",LinearRegression())\n        ]\n    )\n    print(\"\\nTrain:\")\n    # Train the pipeline\n    pipeline.fit(df['text'], df['y'])\n    \n    # What are the important features for toxicity\n\n    print('\\nTotal number of features:', len(pipeline['features'].get_feature_names()) )\n\n    feature_wts = sorted(list(zip(pipeline['features'].get_feature_names(), \n                                  np.round(pipeline['clf'].coef_,2) )), \n                         key = lambda x:x[1], \n                         reverse=True)\n\n    pprint(feature_wts[:30])\n    \n    print(\"\\npredict validation data \")\n    val_preds_arr1_[:,fld] = pipeline.predict(df_val['less_toxic'])\n    val_preds_arr2_[:,fld] = pipeline.predict(df_val['more_toxic'])\n\n    print(\"\\npredict test data \")\n    test_preds_arr_[:,fld] = pipeline.predict(df_sub['text'])","fa364d3a":"del df, pipeline, feature_wts\ngc.collect()","a0d5189e":"print(\" Toxic data \")\np1 = val_preds_arr1.mean(axis=1)\np2 = val_preds_arr2.mean(axis=1)\n\nprint(f'Validation Accuracy is { np.round((p1 < p2).mean() * 100,2)}')\n\nprint(\" Ruddit data \")\np3 = val_preds_arr1_.mean(axis=1)\np4 = val_preds_arr2_.mean(axis=1)\n\nprint(f'Validation Accuracy is { np.round((p3 < p4).mean() * 100,2)}')\n\nprint(\" Toxic CLEAN data \")\np5 = val_preds_arr1c.mean(axis=1)\np6 = val_preds_arr2c.mean(axis=1)\n\nprint(f'Validation Accuracy is { np.round((p5 < p6).mean() * 100,2)}')\n","b5575477":"print(\"Find right weight\")\n\nwts_acc = []\nfor i in range(30,70,1):\n    for j in range(0,20,1):\n        w1 = i\/100\n        w2 = (100 - i - j)\/100\n        w3 = (1 - w1 - w2 )\n        p1_wt = w1*p1 + w2*p3 + w3*p5\n        p2_wt = w1*p2 + w2*p4 + w3*p6\n        wts_acc.append( (w1,w2,w3, \n                         np.round((p1_wt < p2_wt).mean() * 100,2))\n                      )\nsorted(wts_acc, key=lambda x:x[3], reverse=True)[:5]","300cf014":"w1,w2,w3,_ = sorted(wts_acc, key=lambda x:x[2], reverse=True)[0]\n#print(best_wts)\n\np1_wt = w1*p1 + w2*p3 + w3*p5\np2_wt = w1*p2 + w2*p4 + w3*p6\n","510e01b4":"df_val['p1'] = p1_wt\ndf_val['p2'] = p2_wt\ndf_val['diff'] = np.abs(p2_wt - p1_wt)\n\ndf_val['correct'] = (p1_wt < p2_wt).astype('int')\n","ead5629e":"\n### Incorrect predictions with similar scores\n\ndf_val[df_val.correct == 0].sort_values('diff', ascending=True).head(20)","a313b839":"### Incorrect predictions with dis-similar scores\n\n\ndf_val[df_val.correct == 0].sort_values('diff', ascending=False).head(20)","da6cbe98":"# Predict using pipeline\n\ndf_sub['score'] = w1*test_preds_arr.mean(axis=1) + w2*test_preds_arr_.mean(axis=1) + w3*test_preds_arrc.mean(axis=1)","e418358d":"#test_preds_arr","6b7e5bd3":"# Cases with duplicates scores\n\ndf_sub['score'].count() - df_sub['score'].nunique()","74f1d902":"same_score = df_sub['score'].value_counts().reset_index()[:10]\nsame_score","5f7455ef":"df_sub[df_sub['score'].isin(same_score['index'].tolist())]","f4a9a96a":"# Same comments have same score - which is ok ","6bcf2abf":"# # Rank the predictions \n\n# df_sub['score']  = scipy.stats.rankdata(df_sub['score'], method='ordinal')\n\n# print(df_sub['score'].rank().nunique())","4ef3227c":"%%time\n# connect libraries only for this task\nimport os  # operating system library\nimport gc  # Garbage Collector - module provides the ability to disable the collector, tune the collection frequency, and set debugging options\nimport cv2  # open source computer vision and machine learning library\nimport copy  # The assignment operation does not copy the object, it only creates a reference to the object. \n# For mutable collections, or for collections containing mutable items, a copy is often needed so that it can be modified without changing the original. \n# This module provides general (shallow and deep) copy operations. \n\nimport time  # time library\nimport random  # library for working with random values\n\n# For data manipulation\nimport pandas as pd  # data analysis library\nimport numpy as np  # library linear algebra, Fourier transform and random numbers\n\n# Pytorch Imports\nimport torch  #  a Tensor library like NumPy, with strong GPU support\nimport torch.nn as nn  # a neural networks library deeply integrated with autograd designed for maximum flexibility\nfrom torch.utils.data import Dataset, DataLoader  # DataLoader and other utility functions for convenience\n\n\n# For Transformer Models\nfrom transformers import AutoTokenizer, AutoModel  # In many cases, the architecture you want to use can be guessed from the name or the path of the \n# pretrained model you are supplying to the from_pretrained() method. AutoClasses are here to do this job for you so that you automatically retrieve the \n# relevant model given the name\/path to the pretrained weights\/config\/vocabulary.\n#Instantiating one of AutoConfig, AutoModel, and AutoTokenizer will directly create a class of the relevant architecture.\n\n# Utils\nfrom tqdm import tqdm  # tqdm derives from the Arabic word taqaddum  which can mean \"progress,\" and is an abbreviation for \"I love you so much\" in Spanish \n# (te quiero demasiado).  this library show a smart progress meter - just wrap any iterable with tqdm(iterable)\n\n# For descriptive error messages\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n\nCONFIG = dict(\n    seed = 42,\n    model_name = '..\/input\/roberta-base',\n    test_batch_size = 64,\n    max_length = 128,\n    num_classes = 1,\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n)\n\nCONFIG[\"tokenizer\"] = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n\n\n\nMODEL_PATHS = [\n    '..\/input\/forkofpytorchwbmyjigsawstarter\/Loss-Fold-0.bin',\n    '..\/input\/forkofpytorchwbmyjigsawstarter\/Loss-Fold-1.bin',\n    '..\/input\/forkofpytorchwbmyjigsawstarter\/Loss-Fold-2.bin',\n    '..\/input\/forkofpytorchwbmyjigsawstarter\/Loss-Fold-3.bin',\n    '..\/input\/forkofpytorchwbmyjigsawstarter\/Loss-Fold-4.bin',\n    '..\/input\/forkofpytorchwbmyjigsawstarter\/Loss-Fold-5.bin',\n    '..\/input\/forkofpytorchwbmyjigsawstarter\/Loss-Fold-6.bin',\n    '..\/input\/forkofpytorchwbmyjigsawstarter\/Loss-Fold-7.bin',\n    '..\/input\/forkofpytorchwbmyjigsawstarter\/Loss-Fold-8.bin',\n    '..\/input\/forkofpytorchwbmyjigsawstarter\/Loss-Fold-9.bin'\n]\n\n\n\ndef set_seed(seed = 42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \n    \nclass JigsawDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length):\n        self.df = df\n        self.max_len = max_length\n        self.tokenizer = tokenizer\n        self.text = df['text'].values\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        text = self.text[index]\n        inputs = self.tokenizer.encode_plus(\n                        text,\n                        truncation=True,\n                        add_special_tokens=True,\n                        max_length=self.max_len,\n                        padding='max_length'\n                    )\n        \n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']        \n        \n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long)\n        }    \n\n    \nclass JigsawModel(nn.Module):\n    def __init__(self, model_name):\n        super(JigsawModel, self).__init__()\n        self.model = AutoModel.from_pretrained(model_name)\n        self.drop = nn.Dropout(p=0.2)\n        self.fc = nn.Linear(768, CONFIG['num_classes'])\n        \n    def forward(self, ids, mask):        \n        out = self.model(input_ids=ids,attention_mask=mask,\n                         output_hidden_states=False)\n        out = self.drop(out[1])\n        outputs = self.fc(out)\n        return outputs\n    \n@torch.no_grad()\ndef valid_fn(model, dataloader, device):\n    model.eval()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    PREDS = []\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:\n        ids = data['ids'].to(device, dtype = torch.long)\n        mask = data['mask'].to(device, dtype = torch.long)\n        \n        outputs = model(ids, mask)\n        PREDS.append(outputs.view(-1).cpu().detach().numpy()) \n    \n    PREDS = np.concatenate(PREDS)\n    gc.collect()\n    \n    return PREDS\n\n\ndef inference(model_paths, dataloader, device):\n    final_preds = []\n    for i, path in enumerate(model_paths):\n        model = JigsawModel(CONFIG['model_name'])\n        model.to(CONFIG['device'])\n#         model.load_state_dict(torch.load(path, map_location=torch.device('cpu')))  # for cpu\n        model.load_state_dict(torch.load(path))  # for gpu\n       \n        \n        print(f\"Getting predictions for model {i+1}\")\n        preds = valid_fn(model, dataloader, device)\n        final_preds.append(preds)\n    \n    final_preds = np.array(final_preds)\n    final_preds = np.mean(final_preds, axis=0)\n    return final_preds\n\n\nset_seed(CONFIG['seed'])\ndf = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\ndf.head()\n\ntest_dataset = JigsawDataset(df, CONFIG['tokenizer'], max_length=CONFIG['max_length'])\ntest_loader = DataLoader(test_dataset, batch_size=CONFIG['test_batch_size'],\n                         num_workers=2, shuffle=False, pin_memory=True)\n\npreds1 = inference(MODEL_PATHS, test_loader, CONFIG['device'])","ee9743e6":"preds = (preds1-preds1.min())\/(preds1.max()-preds1.min())","4125f00b":"df_sub['score'] = df_sub['score']*0.85+preds*0.15  # preparation of the output, we select the coefficient empirically 0.82->0.85->0.90, 0.17->0.15->0.10","afba457b":"df_sub[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)  # we form an output file for evaluation in the competition","ca2e7024":"!nvidia-smi","fa0b5bd7":"# Toxic __clean__ data","ec50debb":"## Ruddit data pipeline","a6d61ce3":"#### Some of these just look incorrectly tagged \n","f2fecfb3":"# Create Sklearn Pipeline with \n## TFIDF - Take 'char_wb' as analyzer to capture subwords well\n## Ridge - Ridge is a simple regression algorithm that will reduce overfitting \n\n### model Ridge - Linear least squares with l2 regularization.\n\nMinimizes the objective function:\n\n||y - Xw||^2_2 + alpha * ||w||^2_2\n\nThis model solves a regression model where the loss function is the linear least squares function and regularization is given by the l2-norm. Also known as Ridge Regression or Tikhonov regularization. This estimator has built-in support for multi-variate regression (i.e., when y is a 2d-array of shape (n_samples, n_targets)).\n\n### Linear Regression - ordinary least squares .\n\nLinear Regression fits a linear model with coefficients w = (w1, \u2026, wp) to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation.","1438f051":"### Does % of uppercase characters have effect on toxicity\n","12e595ce":"## Train pipeline\n\n- Load folds data\n- train pipeline\n- Predict on validation data\n- Predict on test data\n\npipeline module implements utilities to build a composite estimator, as a chain of transforms and estimators.","276dc7a2":"# Create 3 versions of data","c4207388":"## Create 3 versions of the data","2eaa23dc":"# Validate the pipeline ","a4484b15":"## Correct the rank ordering","09bc1415":"# Predict on test data ","41c3190d":"# Training data \n\n## Convert the label to SUM of all toxic labels (This might help with maintaining toxicity order of comments)","2b4bb99b":"# Create 3 versions of __clean__ data","58278e6c":"\n\nThis notebook can be run without a graphics card (GPU) or TPU enough and CPU. (I have already spent my entire GPU resource, but there is a way out.)\n\n### Ensemble [TFIDF+BERT]\nPay attention to the input libraries (databases):\n- ..\/input\/jigsaw-toxic-severity-rating  - current competition data\n- ..\/input\/jigsaw-toxic-comment-classification-challenge - 2017 competition data \"The problem of classification of toxic comments\"\n- ..\/input\/roberta-base - model data roberta base\n- ..\/input\/ruddit-jigsaw-dataset - Norms of Offensiveness for English Reddit Comments is a dataset of English language Reddit comments\n- ..\/input\/0-824-jigsaw-inference - output of the corresponding notepad\n\n### Very important!!\nThis notebook uses data from my other two notebooks, I will leave links. You can, on the basis of their variants and already modified data, get your own results, perhaps even better than mine.\n\n- ..\/input\/fork-of-pytorch-w-b-my-jigsaw-starter - output of the corresponding notepad https:\/\/www.kaggle.com\/andrej0marinchenko\/my-jigsaw-starter-for-beginners\n\n# Imports modules","0c920574":"## Load Validation and Test data  \nnow we read the data file of the current competition and transfer it to the dataframe","6a20cda4":"## Ruddit data\n\nRuddit: Norms of Offensiveness for English Reddit Comments is a dataset of English language Reddit comments that has fine-grained, real-valued scores between -1 (maximally supportive) and 1 (maximally offensive). Data sampling procedure, annotation, and analysis have been discussed in detail in the accompanying paper. Authors have provided the comment IDs, post IDs and not the bodies, in accordance to the GDPR regulations. They have suggested that the comments and post bodies can be extracted from any Reddit API using the IDs provided.\n\nThe original paper can be found here: Ruddit: Norms of Offensiveness for English Reddit Comments\n\nThe source github repo can be found here: https:\/\/github.com\/hadarishav\/Ruddit","fac0cc81":"## Analyze bad predictions \n### Incorrect predictions with similar scores\n### Incorrect predictions with different scores","975b7444":"# Bert Ensemble","66d3c794":"### Toxic data"}}