{"cell_type":{"4412ee0a":"code","d880d434":"code","1956ff61":"code","37ac9916":"code","a44eb98d":"code","0362b31b":"code","9a0d5a05":"code","4016771d":"code","1a00e5a3":"code","f6ef4ff2":"code","832deb24":"code","e379182d":"code","4d079593":"code","aac5d6a2":"code","ca5e270e":"code","29a269a2":"code","b92b714e":"code","64211418":"code","94c7fe86":"code","3224ac07":"code","b684f344":"code","7208df78":"code","e37e38d6":"code","b171c88a":"code","fecbe8fc":"code","180a7634":"code","a09f111f":"code","71e1f665":"code","36b6e4f8":"code","2b7bd3f7":"code","2158691a":"code","cd228082":"code","024cea86":"code","5bb495fd":"code","49ef49f0":"code","91043015":"code","29857bbf":"code","16ff861c":"code","763dc10f":"code","5e9988d6":"code","5b8e101f":"code","9411cb94":"code","1208fbad":"code","751ada07":"code","0bca395d":"code","e452aeff":"code","d7721776":"code","d0ce310e":"code","92f0c5e8":"code","5a9f8254":"code","428f77f2":"code","f04204f7":"code","04789b42":"code","1c01cd15":"code","dd678ca8":"code","62ccd600":"code","72cd4d26":"code","fe738cdf":"code","0d530041":"code","afb616b8":"code","33d05487":"code","677c7824":"code","53ef5851":"code","18b098ee":"code","7f167494":"code","ea31cf1b":"code","455ea36e":"code","a2613682":"code","fda04c2d":"code","a32b84c2":"code","33330e96":"code","d4fddbd7":"code","38d585c7":"code","91df9624":"code","01ce1852":"code","ba151cb1":"code","4d1328e8":"markdown","32bbe536":"markdown","b82e4efe":"markdown","c9c3f5f9":"markdown","d49af3ec":"markdown","c591d294":"markdown","9f6acf6b":"markdown","994571da":"markdown","68f245b9":"markdown","50e083d4":"markdown","8e85681b":"markdown","22f75100":"markdown","4aff1386":"markdown","f635d2bf":"markdown","b981570d":"markdown","0c396011":"markdown","c959f350":"markdown","89d946b6":"markdown","f0257cba":"markdown","f6aac772":"markdown","75f5a4f5":"markdown","bb339f9a":"markdown","13366324":"markdown","f5f8b9ab":"markdown","76287c83":"markdown","b6c018ad":"markdown","74d1e9ba":"markdown","71f96121":"markdown","00a03862":"markdown","16f258c2":"markdown"},"source":{"4412ee0a":"# Suppressing Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","d880d434":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","1956ff61":"# Importing Pandas and NumPy\nimport pandas as pd, numpy as np, seaborn as sns,matplotlib.pyplot as plt","37ac9916":"pd.set_option('display.max_columns', None)","a44eb98d":"# Importing all datasets\nchurn_data = pd.read_csv(\"\/kaggle\/input\/telecom-churn-data-sets\/churn_data.csv\")\nchurn_data.head()","0362b31b":"customer_data = pd.read_csv(\"\/kaggle\/input\/telecom-churn-data-sets\/customer_data.csv\")\ncustomer_data.head()","9a0d5a05":"internet_data = pd.read_csv(\"\/kaggle\/input\/telecom-churn-data-sets\/internet_data.csv\")\ninternet_data.head()","4016771d":"# Merging on 'customerID'\ndf_1 = pd.merge(churn_data, customer_data, how='inner', on='customerID')","1a00e5a3":"# Final dataframe with all predictor variables\ntelecom = pd.merge(df_1, internet_data, how='inner', on='customerID')","f6ef4ff2":"# Let's see the head of our master dataset\ntelecom.head()","832deb24":"# Let's check the dimensions of the dataframe\ntelecom.shape","e379182d":"# let's look at the statistical aspects of the dataframe\ntelecom.describe()","4d079593":"# Let's see the type of each column\ntelecom.info()","aac5d6a2":"#The varaible was imported as a string we need to convert it to float\n# telecom['TotalCharges'] = telecom['TotalCharges'].astype(float) \ntelecom.TotalCharges = pd.to_numeric(telecom.TotalCharges, errors='coerce')","ca5e270e":"telecom.info()","29a269a2":"\nplt.figure(figsize=(20,40))\nplt.subplot(10,2,1)\nax = sns.distplot(telecom['tenure'], hist=True, kde=False, \n             bins=int(180\/5), color = 'darkblue', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 4})\nax.set_ylabel('# of Customers')\nax.set_xlabel('Tenure (months)')\nplt.subplot(10,2,2)\nax = sns.countplot(x='PhoneService', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,3)\nax =sns.countplot(x='Contract', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,3)\nax =sns.countplot(x='Contract', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,4)\nax =sns.countplot(x='PaperlessBilling', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,5)\nax =sns.countplot(x='PaymentMethod', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,6)\nax =sns.countplot(x='Churn', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,7)\nax =sns.countplot(x='gender', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,8)\nax =sns.countplot(x='SeniorCitizen', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,9)\nax =sns.countplot(x='Partner', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,10)\nax =sns.countplot(x='Dependents', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,11)\nax =sns.countplot(x='MultipleLines', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,12)\nax =sns.countplot(x='InternetService', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,13)\nax =sns.countplot(x='OnlineSecurity', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,14)\nax =sns.countplot(x='OnlineBackup', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,15)\nax =sns.countplot(x='DeviceProtection', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,16)\nax =sns.countplot(x='TechSupport', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,17)\nax =sns.countplot(x='StreamingTV', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,18)\nax =sns.countplot(x='StreamingMovies', data=telecom)\nax.set_ylabel('# of Customers')\nplt.subplot(10,2,19)\nax = sns.distplot(telecom['MonthlyCharges'], hist=True, kde=False, \n             bins=int(180\/5), color = 'darkblue', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 4})\nax.set_ylabel('# of Customers')\nax.set_xlabel('MonthlyCharges')\nplt.subplot(10,2,20)\nax = sns.distplot(telecom['TotalCharges'], hist=True, kde=False, \n             bins=int(180\/5), color = 'darkblue', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 4})\nax.set_ylabel('# of Customers')\nax.set_xlabel('TotalCharges');","b92b714e":"sns.pairplot(telecom)\nplt.show()","64211418":"plt.figure(figsize=(25, 10))\nplt.subplot(1,3,1)\nsns.boxplot(x = 'tenure', y = 'Churn', data=telecom)\nplt.subplot(1,3,2)\nsns.boxplot(x = 'MonthlyCharges', y = 'Churn', data=telecom)\nplt.subplot(1,3,3)\nsns.boxplot(x = 'TotalCharges', y = 'Churn', data=telecom)\nplt.show()","94c7fe86":"# List of variables to map\n\nvarlist =  ['PhoneService', 'PaperlessBilling', 'Churn', 'Partner', 'Dependents']\n\n# Defining the map function\ndef binary_map(x):\n    return x.map({'Yes': 1, \"No\": 0})\n\n# Applying the function to the housing list\ntelecom[varlist] = telecom[varlist].apply(binary_map)","3224ac07":"telecom.head()","b684f344":"# Creating a dummy variable for some of the categorical variables and dropping the first one.\ndummy1 = pd.get_dummies(telecom[['Contract', 'PaymentMethod', 'gender', 'InternetService']], drop_first=True)\n\n# Adding the results to the master dataframe\ntelecom = pd.concat([telecom, dummy1], axis=1)","7208df78":"telecom.head()","e37e38d6":"# Creating dummy variables for the remaining categorical variables and dropping the level with big names.\n\n# Creating dummy variables for the variable 'MultipleLines'\nml = pd.get_dummies(telecom['MultipleLines'], prefix='MultipleLines')\n# Dropping MultipleLines_No phone service column\nml1 = ml.drop(['MultipleLines_No phone service'], 1)\n#Adding the results to the master dataframe\ntelecom = pd.concat([telecom,ml1], axis=1)\n\n# Creating dummy variables for the variable 'OnlineSecurity'.\nos = pd.get_dummies(telecom['OnlineSecurity'], prefix='OnlineSecurity')\nos1 = os.drop(['OnlineSecurity_No internet service'], 1)\n# Adding the results to the master dataframe\ntelecom = pd.concat([telecom,os1], axis=1)\n\n# Creating dummy variables for the variable 'OnlineBackup'.\nob = pd.get_dummies(telecom['OnlineBackup'], prefix='OnlineBackup')\nob1 = ob.drop(['OnlineBackup_No internet service'], 1)\n# Adding the results to the master dataframe\ntelecom = pd.concat([telecom,ob1], axis=1)\n\n# Creating dummy variables for the variable 'DeviceProtection'. \ndp = pd.get_dummies(telecom['DeviceProtection'], prefix='DeviceProtection')\ndp1 = dp.drop(['DeviceProtection_No internet service'], 1)\n# Adding the results to the master dataframe\ntelecom = pd.concat([telecom,dp1], axis=1)\n\n# Creating dummy variables for the variable 'TechSupport'. \nts = pd.get_dummies(telecom['TechSupport'], prefix='TechSupport')\nts1 = ts.drop(['TechSupport_No internet service'], 1)\n# Adding the results to the master dataframe\ntelecom = pd.concat([telecom,ts1], axis=1)\n\n# Creating dummy variables for the variable 'StreamingTV'.\nst =pd.get_dummies(telecom['StreamingTV'], prefix='StreamingTV')\nst1 = st.drop(['StreamingTV_No internet service'], 1)\n# Adding the results to the master dataframe\ntelecom = pd.concat([telecom,st1], axis=1)\n\n# Creating dummy variables for the variable 'StreamingMovies'. \nsm = pd.get_dummies(telecom['StreamingMovies'], prefix='StreamingMovies')\nsm1 = sm.drop(['StreamingMovies_No internet service'], 1)\n# Adding the results to the master dataframe\ntelecom = pd.concat([telecom,sm1], axis=1)","b171c88a":"telecom.head()","fecbe8fc":"# We have created dummies for the below variables, so we can drop them\ntelecom = telecom.drop(['Contract','PaymentMethod','gender','MultipleLines','InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',\n       'TechSupport', 'StreamingTV', 'StreamingMovies'], 1)","180a7634":"# Checking for outliers in the continuous variables\nnum_telecom = telecom[['tenure','MonthlyCharges','SeniorCitizen','TotalCharges']]","a09f111f":"# Checking outliers at 25%, 50%, 75%, 90%, 95% and 99%\nnum_telecom.describe(percentiles=[.25, .5, .75, .90, .95, .99])","71e1f665":"# Adding up the missing values (column-wise)\ntelecom.isnull().sum()","36b6e4f8":"print('No. of Null Records for TotalCharges:',telecom.TotalCharges.isnull().sum())","2b7bd3f7":"print('No. of Records for TotalCharges:',len(telecom))","2158691a":"print('No. of non Records for TotalCharges:',len(telecom)-telecom.TotalCharges.isnull().sum())","cd228082":"# Checking the percentage of missing values\nround(100*(telecom.isnull().sum()\/len(telecom.index)), 2)","024cea86":"telecom = telecom.dropna()\ntelecom = telecom.reset_index(drop=True)\n\n","5bb495fd":"# Checking percentage of missing values after removing the missing values\nround(100*(telecom.isnull().sum()\/len(telecom.index)), 2)","49ef49f0":"from sklearn.model_selection import train_test_split","91043015":"# Putting feature variable to X\nX = telecom.drop(['Churn','customerID'], axis=1)\n\nX.head()","29857bbf":"# Putting response variable to y\ny = telecom['Churn']\n\ny.head()","16ff861c":"# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","763dc10f":"from sklearn.preprocessing import StandardScaler","5e9988d6":"scaler = StandardScaler()\n\nX_train[['tenure','MonthlyCharges','TotalCharges']] = scaler.fit_transform(X_train[['tenure','MonthlyCharges','TotalCharges']])\n\nX_train.head()","5b8e101f":"X_test[['tenure','MonthlyCharges','TotalCharges']] = scaler.transform(X_test[['tenure','MonthlyCharges','TotalCharges']])\n\nX_test.head()","9411cb94":"### Checking the Churn Rate\nchurn = (sum(telecom['Churn'])\/len(telecom['Churn'].index))*100\nchurn","1208fbad":"# Importing matplotlib and seaborn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","751ada07":"# Let's see the correlation matrix \nplt.figure(figsize = (25,25))        # Size of the figure\nsns.heatmap(telecom.corr(),annot = True,cmap=\"tab20c\")\nplt.show()","0bca395d":"plt.figure(figsize=(10,8))\ntelecom.corr()['Churn'].sort_values(ascending = False).plot(kind='bar');","e452aeff":"X_test = X_test.drop(['MultipleLines_No','OnlineSecurity_No','OnlineBackup_No','DeviceProtection_No','TechSupport_No',\n                       'StreamingTV_No','StreamingMovies_No'], 1)\nX_train = X_train.drop(['MultipleLines_No','OnlineSecurity_No','OnlineBackup_No','DeviceProtection_No','TechSupport_No',\n                         'StreamingTV_No','StreamingMovies_No'], 1)","d7721776":"plt.figure(figsize = (25,25))\nsns.heatmap(X_train.corr(),annot = True,cmap=\"tab20c\")\nplt.show()","d0ce310e":"from sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nmodel = SVC()","92f0c5e8":"# fit the model with the training data\nmodel.fit(X_train,y_train)","5a9f8254":"# predict the target on the train dataset\npredict_train = model.predict(X_train)\npredict_train","428f77f2":"trainaccuracy = accuracy_score(y_train,predict_train)\nprint('accuracy_score on train dataset : ', trainaccuracy)","f04204f7":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif.tail()","04789b42":"features_to_remove = vif.loc[vif['VIF'] >= 4.99,'Features'].values\nfeatures_to_remove = list(features_to_remove)\nprint(features_to_remove)","1c01cd15":"X_train = X_train.drop(columns=features_to_remove, axis = 1)\nX_train.head()","dd678ca8":"X_test = X_test.drop(columns=features_to_remove, axis = 1)\nX_test.head()","62ccd600":"# fit the model with the training data\nmodel.fit(X_train,y_train)","72cd4d26":"# predict the target on the train dataset\npredict_train = model.predict(X_train)\npredict_train","fe738cdf":"trainaccuracy = accuracy_score(y_train,predict_train)\nprint('accuracy_score on train dataset : ', trainaccuracy)","0d530041":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","afb616b8":"from sklearn import metrics\n# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train, predict_train )\nprint(confusion)\n","33d05487":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","677c7824":"# Let's see the sensitivity of our model\ntrainsensitivity= TP \/ float(TP+FN)\ntrainsensitivity","53ef5851":"# Let us calculate specificity\ntrainspecificity= TN \/ float(TN+FP)\ntrainspecificity","18b098ee":"# Calculate false postive rate - predicting churn when customer does not have churned\nprint(FP\/ float(TN+FP))","7f167494":"# Positive predictive value \nprint (TP \/ float(TP+FP))","ea31cf1b":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","455ea36e":"draw_roc(y_train,predict_train)","a2613682":"#Looking at the confusion matrix again","fda04c2d":"from sklearn.metrics import precision_score, recall_score\nprecision_score(y_train,predict_train)","a32b84c2":"recall_score(y_train,predict_train)","33330e96":"# predict the target on the test dataset\npredict_test = model.predict(X_test)\nprint('Target on test data\\n\\n',predict_test)","d4fddbd7":"confusion2 = metrics.confusion_matrix(y_test, predict_test )\nprint(confusion2)","38d585c7":"# Let's check the overall accuracy.\ntestaccuracy= accuracy_score(y_test,predict_test)\ntestaccuracy","91df9624":"# Let's see the sensitivity of our lmodel\ntestsensitivity=TP \/ float(TP+FN)\ntestsensitivity","01ce1852":"# Let us calculate specificity\ntestspecificity= TN \/ float(TN+FP)\ntestspecificity","ba151cb1":"# Let us compare the values obtained for Train & Test:\nprint(\"Train Data Accuracy    :{} %\".format(round((trainaccuracy*100),2)))\nprint(\"Train Data Sensitivity :{} %\".format(round((trainsensitivity*100),2)))\nprint(\"Train Data Specificity :{} %\".format(round((trainspecificity*100),2)))\nprint(\"Test Data Accuracy     :{} %\".format(round((testaccuracy*100),2)))\nprint(\"Test Data Sensitivity  :{} %\".format(round((testsensitivity*100),2)))\nprint(\"Test Data Specificity  :{} %\".format(round((testspecificity*100),2)))","4d1328e8":"### Step 5: Feature Scaling","32bbe536":"### Step 11: Making predictions on the test set","b82e4efe":"### Step 1: Importing and Merging Data","c9c3f5f9":"# VIF","d49af3ec":"# EDA","c591d294":"# Final Observation:","9f6acf6b":"We have almost 27% churn rate","994571da":"It is a classification method. In this algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate.","68f245b9":"It means that 11 * 100\/7043 = 0.1561834%, best is to remove these observations from the analysis","50e083d4":"From the distribution shown above, you can see that there no outliers in your data. The numbers are gradually increasing.","8e85681b":"Now we don't have any missing values","22f75100":"# VIF","4aff1386":"#### Combining all data files into one consolidated dataframe","f635d2bf":"#### Dropping the repeated variables","b981570d":"## Telecom Churn Case Study\nWith 21 predictor variables we need to predict whether a particular customer will switch to another telecom provider or not. In telecom terminology, this is referred to as churning and not churning, respectively.","0c396011":"#### Checking the Correlation Matrix","c959f350":"### Step 6: Looking at Correlations","89d946b6":"Now you can see that you have all variables as numeric.","f0257cba":"## Precision and Recall","f6aac772":"#### Converting some binary variables (Yes\/No) to 0\/1","75f5a4f5":"# Plotting the ROC Curve","bb339f9a":"### Step 4: Test-Train Split","13366324":"After dropping highly correlated variables now let's check the correlation matrix again.","f5f8b9ab":"# SVM (Support Vector Machine)","76287c83":"### Step 7: Model Building\nLet's start by splitting our data into a training set and a test set.","b6c018ad":"#### Checking for Outliers","74d1e9ba":"### Step 2: Inspecting the Dataframe","71f96121":"#### Dropping highly correlated dummy variables","00a03862":"#### Checking for Missing Values and Inputing Them","16f258c2":"#### For categorical variables with multiple levels, create dummy features (one-hot encoded)"}}