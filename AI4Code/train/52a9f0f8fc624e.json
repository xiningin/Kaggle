{"cell_type":{"baaaab03":"code","3b34cfcf":"code","ea494a5f":"code","ecbe465c":"code","19b812ab":"code","7f936f52":"code","bf6558b4":"code","58f6efc6":"code","d27c780f":"code","a0bbf63a":"code","939f545f":"code","598d3aa9":"code","69c0bfd1":"code","d329428c":"code","29128594":"code","23d5a01e":"code","f99451ab":"code","cd1ae4bf":"code","978ade73":"code","5c425627":"code","ba44100f":"code","de5dd4c5":"code","fcae0c7a":"code","fb57f51a":"code","da4e17d7":"code","192ff46d":"code","9d9539e4":"code","3d46391c":"code","5ee2ce8c":"code","07924b90":"code","24489d8f":"code","71754c50":"code","f5b4f9d1":"code","a3428c03":"code","56e49578":"code","fd34d90d":"code","11c4b343":"code","7581ccac":"code","6e3bd412":"code","f00adeb4":"code","d3b02bce":"code","80411d46":"code","d44c5ac3":"code","4af67cf0":"code","8eec5074":"code","5b440512":"code","2dec28a7":"code","c2f12bc1":"code","9feae884":"code","210e4339":"code","fc9e19a1":"code","d8aed6ce":"code","eaa37291":"code","e64fc88c":"code","1269ae0b":"code","177122c8":"code","42790207":"code","b62df53c":"code","59da80a3":"code","0ecc0a29":"code","0e373704":"code","425b1fa4":"code","9ab762b8":"markdown","122985da":"markdown","37c1f419":"markdown"},"source":{"baaaab03":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3b34cfcf":"!pip install dash-html-components\n!pip install dash-core-components\n!pip install dash\n!pip install tweepy \n!pip install spacy\n!pip install fugashi[unidic-lite]\n!pip install nltk\n!pip install stemmer\n!pip install langdetect\n!pip3 uninstall googletrans -y\n!pip3 install googletrans==3.1.0a0\n!pip install TextBlob \n!pip install gensim\n!pip install gensim-evaluations\n!pip install tmplot\n#!pip install tmplot","ea494a5f":"%matplotlib inline\nimport dash\nimport dash_core_components as dcc\nimport dash_html_components as html\nfrom matplotlib import pyplot as plt\nimport pandas as pd \nimport numpy as np \nimport tweepy as tw\nfrom tweepy.streaming import StreamListener\nfrom tweepy import OAuthHandler\nfrom tweepy import Stream \nfrom langdetect import detect\nimport string \nimport dash\n\nimport tmplot as tmp\nfrom googletrans import Translator\ntranslator = Translator()\n\nimport seaborn as sns\nimport re\nfrom tqdm import tqdm\nfrom textblob import TextBlob\n\nimport nltk\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem import WordNetLemmatizer,PorterStemmer\nfrom nltk.corpus import stopwords\nstopit = stopwords.words('english')\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk import corpus\nfrom nltk import word_tokenize\nfrom nltk import trigrams\nfrom nltk.util import ngrams\n\nfrom wordcloud import WordCloud\n\n# Gensim\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\nfrom gensim.models import Phrases\nfrom gensim.corpora import Dictionary\n\n# spacy for lemmatization\nimport spacy\n\n# Plotting tools\n#import tmplot as tmp\n\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\npd.set_option('display.width', 10000)\npd.set_option('display.max_columns', 10000)\npd.set_option('display.width', 10000)\npd.set_option('display.max_columns', 10000)","ecbe465c":"userIDJPHelp = \"AmazonHelp\"\n# your Twitter API key and API secret\nmy_api_key = \"Hzbb0sXPSUX5yFXRozdmdeqSX\"\nmy_api_secret = \"j2kIHynH5tqE4RV6lMC602K4HQKbTlVbTiguFj7R4UgeAHVpVV\"\n# authenticate\nauth = tw.OAuthHandler(my_api_key, my_api_secret)\napi = tw.API(auth, wait_on_rate_limit=True)\n","19b812ab":"streamingme = StreamListener()\nstream =tw.Stream(auth=api.auth, listener= streamingme, tweet_mode='extended')","7f936f52":"class StreamListener(tw.StreamListener):\n    def on_status(self, status):\n        print(status.id_str)\n    def on_error(self, status_code):\n        sys.exit \n","bf6558b4":"ahtweet = api.user_timeline(screen_name=userIDJPHelp, \n                               count=200,\n                               include_rts = False,\n                               tweet_mode = 'extended')","58f6efc6":"amazon_help_df = pd.DataFrame()\n\nfor tweet in ahtweet:\n    hashtags = []\n    try:\n        for hashtag in tweet.entities[\"hashtags\"]:\n            hashtags.append(hashtag[\"text\"])\n        text = api.get_status(id=tweet.id, tweet_mode='extended').full_text\n    except:\n        pass\n    amazon_help_df = amazon_help_df.append(pd.DataFrame({'user_name': tweet.user.name, \n                                               'user_location': tweet.user.location,\\\n                                               'user_description': tweet.user.description,\n                                               'user_verified': tweet.user.verified,\n                                               'date': tweet.created_at,\n                                               'text': text, \n                                               'hashtags': [hashtags if hashtags else None],\n                                               'source': tweet.source}))\namazon_help_df = amazon_help_df.reset_index(drop=True)","d27c780f":"amazon_help_df['users'] = \"\"\namazon_help_df.loc[amazon_help_df.users == '', 'users'] = amazon_help_df.text.str.split().str.get(0)","a0bbf63a":"amazon_help_df['language'] = amazon_help_df['text'].apply(detect)","939f545f":"translator.translate(text, dest='en', src='auto')\namazon_help_df['english_text'] = amazon_help_df['text'].apply(lambda x: translator.translate(x,dest='en').text )","598d3aa9":"def preprocess(word):\n    word=str(word)\n    word = word.lower()\n    word=word.replace('{html}',\"\") \n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, '', word)\n    rem_url=re.sub(r'http\\S+', '',cleantext)\n    rem_num = re.sub('[0-9]+', '', rem_url)\n    tokenizer = RegexpTokenizer(r'\\w+')\n    tokens = tokenizer.tokenize(rem_num)  \n    filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n    #stem_words=[stemmer.stem(w) for w in filtered_words]\n    #lemma_words=[lemmatizer.lemmatize(w) for w in filtered_words]#\n    return \" \".join(filtered_words)\n\n#helpdesk['text'] = helpdesk['text'].map(lambda s:preprocess(s))\namazon_help_df['english_text'] = amazon_help_df['english_text'].map(lambda s:preprocess(s))","69c0bfd1":"amazon_help_df['english_text'] = amazon_help_df['english_text'].str.split(n=1).str[1]","d329428c":"%%time\ntokens = []\nlemma = []\npos = []\ndep = []\n\nfor doc in nlp.pipe(amazon_help_df['english_text'].astype('unicode').values, batch_size=100,\n                        n_threads=4):\n    if doc.is_parsed:\n        tokens.append([n.text for n in doc])\n        lemma.append([n.lemma_ for n in doc])\n        pos.append([n.pos_ for n in doc])\n        dep.append([n.dep_ for n in doc])\n        \n        \n        \n    else:\n        # We want to make sure that the lists of parsed results have the\n        # same number of entries of the original Dataframe, so add some blanks in case the parse fails\n        tokens.append(None)\n        lemma.append(None)\n        pos.append(None)\n        dep.append(None)\n        \namazon_help_df['tokens_text'] = tokens\namazon_help_df['lemma_text'] = lemma\namazon_help_df['pos_text'] = pos\namazon_help_df['dep_text'] = dep       ","29128594":"amazon_help_df.head(2)","23d5a01e":"amazon_help_df.to_csv (r'C:\\Users\\Naeemah\\Desktop\\cleanhelpdesktweet.csv', index = False, header=True)","f99451ab":"helpdeskexplain = amazon_help_df","cd1ae4bf":"train = pd.DataFrame()","978ade73":"train['words'] = helpdeskexplain['english_text']","5c425627":"train['word_count'] = train['words'].apply(lambda x: len(str(x).split(\" \")))","ba44100f":"train['char_count'] = train['words'].str.len() ## this also includes spaces\ntrain.head()","de5dd4c5":"def avg_word(sentence):\n    words = sentence.split()\n    return (sum(len(word) for word in words)\/len(words))","fcae0c7a":"train['avg_word'] = train['words'].apply(lambda x: avg_word(x))\ntrain.head()","fb57f51a":"train['stopwords'] = train['words'].apply(lambda x: len([x for x in x.split() if x in stopit]))\ntrain.head()","da4e17d7":"train['hastags'] = train['words'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\ntrain.head()","192ff46d":"train['numerics'] = train['words'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\ntrain.head()","9d9539e4":"train['upper'] = train['words'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\ntrain.head()\n","3d46391c":"freq = pd.Series(' '.join(train['words']).split()).value_counts()[:10]\nfreq ","5ee2ce8c":"freq = list(freq.index)\ntrain['words'] = train['words'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\ntrain['words'].head()","07924b90":"freq = pd.Series(' '.join(train['words']).split()).value_counts()[-30:]\nfreq","24489d8f":"freq = list(freq.index)\ntrain['words'] = train['words'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\ntrain['words'].head()","71754c50":"TextBlob(train['words'][1]).words","f5b4f9d1":"from textblob import Word\ntrain['words'] = train['words'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\ntrain['words'].head()","a3428c03":"TextBlob(train['words'][0]).ngrams(2)","56e49578":"tf1 = (train['words'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\ntf1.columns = ['words','tf']\ntf1","fd34d90d":"for i,word in enumerate(tf1['words']):\n    tf1.loc[i, 'idf'] = np.log(train.shape[0]\/(len(train[train['words'].str.contains(word)])))\n    \nprint(tf1)   ","11c4b343":"tf1['tfidf'] = tf1['tf'] * tf1['idf']\nprint(tf1)","7581ccac":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(max_features=1000, lowercase=True, analyzer='word',\n stop_words= 'english',ngram_range=(1,1))\ntrain_vect = tfidf.fit_transform(train['words'])\ntrain_vect","6e3bd412":"from sklearn.feature_extraction.text import CountVectorizer\nbow = CountVectorizer(max_features=1000, lowercase=True, ngram_range=(1,1),analyzer = \"word\")\ntrain_bow = bow.fit_transform(train['words'])\ntrain_bow","f00adeb4":"train['words'][:5].apply(lambda x: TextBlob(x).sentiment)","d3b02bce":"train['sentiment'] = train['words'].apply(lambda x: TextBlob(x).sentiment[0] )\ntrain[['words','sentiment']].head()","80411d46":"train['sentiment'].unique()","d44c5ac3":"train.info()","4af67cf0":"train['sentiment'].describe()","8eec5074":"train['sentiment'].plot(kind='hist')","5b440512":"conditions = [\n    (train['sentiment'] == 0.0),\n    (train['sentiment'] <= -0.1),\n    (train['sentiment'] >= -0.1)]\n   \nvalues = ['Netural', 'Negative', 'Positive']\n\ntrain['results'] = np.select(conditions, values)\ntrain.head()","2dec28a7":"ax = sns.catplot(y=\"results\", data=train,kind=\"count\", palette=\"hls\")","c2f12bc1":"train.to_csv (r'C:\\Users\\Naeemah\\Desktop\\emotions_data.csv', index = False, header=True)","9feae884":"topmodel = helpdeskexplain","210e4339":"from sklearn.feature_extraction.text import CountVectorizer\n\n# the vectorizer object will be used to transform text to vector form\nvectorizer = CountVectorizer(max_df=0.9, min_df=25, token_pattern='\\w+|\\$[\\d\\.]+|\\S+')\n\n# apply transformation\ntf = vectorizer.fit_transform(topmodel['english_text']).toarray()\n\n# tf_feature_names tells us what word each column in the matric represents\ntf_feature_names = vectorizer.get_feature_names()","fc9e19a1":"from sklearn.decomposition import LatentDirichletAllocation\nnumber_of_topics = 10\nmodel = LatentDirichletAllocation(n_components=number_of_topics, random_state=0)","d8aed6ce":"model.fit(tf)","eaa37291":"def display_topics(model, feature_names, no_top_words):\n    topic_dict = {}\n    for topic_idx, topic in enumerate(model.components_):\n        topic_dict[\"Topic %d words\" % (topic_idx)]= ['{}'.format(feature_names[i])\n                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n        topic_dict[\"Topic %d weights\" % (topic_idx)]= ['{:.1f}'.format(topic[i])\n                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n    return pd.DataFrame(topic_dict)","e64fc88c":"no_top_words = 6\ntopwords = display_topics(model, tf_feature_names, no_top_words)","1269ae0b":"topwords","177122c8":"topicwords = pd.DataFrame(topwords)","42790207":"topwords.to_csv (r'C:\\Users\\Naeemah\\Desktop\\topwords_data.csv', index = False, header=True)","b62df53c":"topicwords.to_csv (r'C:\\Users\\Naeemah\\Desktop\\topicwordsnew_data.csv', index = False, header=True)","59da80a3":"topicwords.head()","0ecc0a29":"df1_transposed = topicwords.T","0e373704":"df1_transposed","425b1fa4":"df1_transposed.to_csv (r'C:\\Users\\Naeemah\\Desktop\\topwordstrans_data.csv', index = False, header=True)","9ab762b8":"Emotions","122985da":"Rare words removal","37c1f419":"Common word removal"}}