{"cell_type":{"32f2f220":"code","f368f847":"code","031fad53":"code","982cb46c":"code","779a080a":"code","66a3d1ec":"code","322b15f5":"code","0a8d8f4e":"code","a03928f3":"code","280542a2":"code","df607dbc":"code","f323641c":"code","05f92f6a":"code","e03b9738":"code","26979891":"code","ed90f00a":"code","621d22a9":"code","1415c2b1":"code","6388fd72":"code","51d2b9c9":"code","1c0e51f2":"code","c10cd9a8":"code","d3a85757":"code","3077f40d":"code","c5484ab0":"code","16353018":"code","2c2ab2dc":"code","8962093c":"code","895dbe7d":"code","12e7785f":"markdown","13eae0ff":"markdown","675f57fe":"markdown","5c6d0ee4":"markdown","11ff0c3c":"markdown","aa47f182":"markdown","a3aa250c":"markdown","f088c746":"markdown","12a59bad":"markdown","c2720d1a":"markdown","5a2fbbb6":"markdown","7f69f8c9":"markdown","d7e86ac8":"markdown","b481b6a9":"markdown","6b76851f":"markdown","2430bcbe":"markdown","bc4733ff":"markdown","f88ef269":"markdown","7292cbd8":"markdown","29409a85":"markdown","70daaa26":"markdown","60fa4349":"markdown","c009ab5b":"markdown","c3af5171":"markdown","6e05cc00":"markdown","476e2c2d":"markdown","8237ebeb":"markdown","cee182ab":"markdown","7d27b176":"markdown","c89c48cf":"markdown","d72f8186":"markdown","a535e7b5":"markdown","1a740730":"markdown","9625254d":"markdown","1b55f53b":"markdown","399f3ae2":"markdown","30b2e42d":"markdown","357f7763":"markdown","3d1b0cea":"markdown","2a8f87c2":"markdown","cda8cab0":"markdown","666e73df":"markdown","378c3b84":"markdown","2abbef1a":"markdown"},"source":{"32f2f220":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","f368f847":"sns.set_style('darkgrid')\ndf = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')","031fad53":"df.head()","982cb46c":"df.info()","779a080a":"df.describe()","66a3d1ec":"sns.distplot(df['Time'],bins=30)\nplt.title('Histogram of Time feature')\nplt.show()","322b15f5":"sns.distplot(df['Amount'],bins=10)\nplt.title('Histogram of Amount feature')\nplt.show()","0a8d8f4e":"plt.figure(figsize=(10,10))\nsns.heatmap(df.corr())\nplt.show()","a03928f3":"sns.countplot(df['Class'])\nplt.show()","280542a2":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler","df607dbc":"scaler = StandardScaler()\n\ndf['Time'] = scaler.fit_transform(df['Time'].values.reshape(-1,1))\ndf['Amount'] = scaler.fit_transform(df['Amount'].values.reshape(-1,1))","f323641c":"X = df.drop('Class',axis=1)\ny = df['Class']","05f92f6a":"kf = StratifiedKFold(n_splits=10,random_state=42,shuffle=True)","e03b9738":"for train,test in kf.split(X,y):\n    X_train,X_test = X.loc[train],X.loc[test],\n    y_train,y_test = y.loc[train],y.loc[test]","26979891":"from xgboost import XGBClassifier","ed90f00a":"xgb = XGBClassifier(n_estimators=200,learning_rate=0.3,gamma=0.2)","621d22a9":"xgb.fit(X_train,y_train)","1415c2b1":"pred = xgb.predict(X_test)","6388fd72":"from sklearn.metrics import classification_report,confusion_matrix,roc_auc_score,roc_curve\nfrom imblearn.metrics import classification_report_imbalanced","51d2b9c9":"def evaluate(prediction,test,imb=False):\n    sns.heatmap(confusion_matrix(prediction,test),annot=True)\n    plt.show()\n    if imb == True:\n        print(classification_report(prediction,test))\n    else:\n        print(classification_report_imbalanced(test,prediction))\n    score = roc_auc_score(test,prediction)\n    print('AUC Score: ' + str(score))\n    fp,tp,_ = roc_curve(test,prediction)\n\n    plt.figure(figsize=(8,8))\n    plt.plot(fp,tp,linestyle='dashed',c='orange',label='XGBoostClassifier')\n    plt.plot([0,1],[0,1],'r--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Visualisation')\n    plt.legend()\n    plt.show()","1c0e51f2":"evaluate(pred,y_test)","c10cd9a8":"from imblearn.under_sampling import RandomUnderSampler","d3a85757":"rus = RandomUnderSampler(sampling_strategy=1)\n\nX_rus,y_rus = rus.fit_sample(X,y)","3077f40d":"rus_df = pd.concat([X_rus,y_rus],axis=1)\nsns.countplot(rus_df['Class'])\nplt.show()","c5484ab0":"undersample_X = rus_df.drop('Class',axis=1)\nundersample_y = rus_df['Class']\n\nfor train,test in kf.split(undersample_X,undersample_y):\n    X_train_rus,X_test_rus = undersample_X.loc[train],undersample_X.loc[test],\n    y_train_rus,y_test_rus = undersample_y.loc[train],undersample_y.loc[test]","16353018":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=200,max_features=2)","2c2ab2dc":"rf.fit(undersample_X,undersample_y)","8962093c":"pred_rf = rf.predict(X_test)","895dbe7d":"evaluate(pred_rf,y_test,imb=True)","12e7785f":"## Predict on test set","13eae0ff":"# Evaluating the models\n\n## I used the following to evalute my models:\n### 1. Classification Report\n### 2. Confusion Matrix\n### 3. ROC & AUC","675f57fe":"## Histogram of Amount feature","5c6d0ee4":"## Evaluation function","11ff0c3c":"It is a good habit to set a `random_state` as this ensures repeatability in your data. If you do not do so, each time you split will result in different train and test values, which is not ideal ","aa47f182":"Well, looking at the code here we can see that our `XGBClassifier` did a pretty good job, reaching 93% recall. We will now use Undersampling and the `RandomForestClassifier` to view its performance","a3aa250c":"### Here we can see that some features did have correlations.\n\nWe can see that some features were indeed correlated with the Class feature, but the correlations were insignificant because of two primary factors:\n\n1. The data was transformed using PCA\n2. There is a high imbalance in the data, which might reduce correlations with the Class feature","f088c746":"## Import Evaluation metrics","12a59bad":"## Import the usual libraries","c2720d1a":"## Initialize StandardScaler","5a2fbbb6":"## Instantiate RandomUnderSampler and fit it to the data","7f69f8c9":"## Describe the Data to get a sense of the data we are working with","d7e86ac8":"## Import the library","b481b6a9":"## Some info on the data to check if there is any missing values","6b76851f":"Now we have removed the imbalance by randomly selecting samples to match the minority class. Thus, we can begin modelling!","2430bcbe":"## Evaluate Model","bc4733ff":"## Import the Model","f88ef269":"I am going to use `StandardScaler` to scale out the `Time` and `Amount` features. It is important to have all your features at a relatively similiar scale and close to normal distribution.","7292cbd8":"I decided to use Random Forests as I believe that they perform slightly better than `XGBoosterClassifier` for this problem.","29409a85":"## Countplot of class labels","70daaa26":"## Concactinate the two DataFrames together and visualize using CountPlot","60fa4349":"## Define our features and target","c009ab5b":"The model I am going to be using is `XGBClassifier`. This is a very popular classification model used by many Kagglers to win many Data Science Competitions. It is essnetially a gradient booster that builds off the erros of previous models. Most of its parameters are best optimized using a Grid Search or Randomized Search. I myself ran 3 instances of `RandomizedSearchCV` to find the optimal parameters","c3af5171":"## Define Features and Target","6e05cc00":"I successfully used imblearn's `RandomUnderSampler` to undersample the majority class.\nNote the `sampling_strategy` was set to 1. of I was to set the strategy to 0.9, the minority class was still slightly smaller than the majority class, but the imbalance is practically gone. Now that the imbalance is gone,we can start training the data.","476e2c2d":"I have created a function for evaluation that includes all of the evaluation methods listed above. I did not use `accuracy_score` here as accuracy tends to be very misleading and is not a good evaluation when deealing with imbalanced datasets","8237ebeb":"# The Model","cee182ab":"# Exploratory Data Analysis ","7d27b176":"# Welcome to my Notebook!","c89c48cf":"## Grab the Head of the Data","d72f8186":"## Heatmap to check for any correlation","a535e7b5":"## Split data with StratifiedKFold","1a740730":"## Objective","9625254d":"Now, since we have imbalanced data, we have a few options that we can use at hand:\n\n1. Undersampling\n2. Oversampling\n3. SMOTE\n\nI will be using Imblearn's `RandomUnderSampler` to undersample the majority class(Non Fraud) and make it match with the minority class","1b55f53b":"# Preparing the Model","399f3ae2":"Given this dataset, the task is to classify whether a credit card transaction is fraud or not, based on features that were transformed using PCA and hidden for confidentiality purposes. Our objective is to have a fast,reliable and unbiased model that can accurately classify transactions as being fraud or not fraud.\n\nThis data gives many challenges, as there is a high imbalance between non-fraud and fraud transactions. I will use two technique in this notebook to try and get the best results:\n\n1. XGBoostClassifier\n2. RandomForestClassifier using the `RandomUnderSampler` from the  `imblearn` module","30b2e42d":"For this model, I will split the data using Sklearn's `StratisfiedKfold`, as it effectively keeps the distribution of the data when performing cross validations splits","357f7763":"## Fit the model","3d1b0cea":"## Read the CSV File and adjust grid style with seaborn","2a8f87c2":"It is clearly evident from this plot that an imbalance is evident. I will try different methods to reduce this imbalance. Training this without evening out the balance will result in your model heavily overfitting, therefore we will have to use an imbalance technique to even out the balance","cda8cab0":"## Histogram of the Time feature with KDE graph","666e73df":"# Undersampling with RF","378c3b84":"## Import RandomForestClassifier and initialise","2abbef1a":"## Call the function"}}