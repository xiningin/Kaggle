{"cell_type":{"49f5aa40":"code","f06a209f":"code","aa79499f":"code","980a9eb8":"code","caf2a304":"code","b6341664":"code","bfcb49d9":"code","a763796c":"code","b4d6b23d":"code","5fec188c":"code","9fea6c15":"code","9e3faa82":"code","76ed8127":"code","9680bbe6":"code","ed8f5624":"code","2c4b64ed":"code","956dd135":"code","c8925b11":"code","73fae425":"code","5d595eb0":"code","b4dadafa":"code","19075c96":"code","1507f76c":"code","3a0f6252":"code","9b06d6c3":"code","19aa5e2e":"code","f6393622":"markdown","b0fd1c0c":"markdown","86b0c43b":"markdown","f06ec8f9":"markdown","db8da3dc":"markdown","8d6dce2d":"markdown","3e71f083":"markdown","43729b25":"markdown","e542058c":"markdown","3a404d03":"markdown","998ffad9":"markdown","95dced4f":"markdown","47ff7b57":"markdown","050497a0":"markdown","442196f5":"markdown","6d2d8b95":"markdown","33f202ce":"markdown","e25943e9":"markdown","99c90b82":"markdown","7d985700":"markdown","ef28ff66":"markdown","b40c895a":"markdown"},"source":{"49f5aa40":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Defining plotting style\nsns.set(style=\"ticks\", color_codes=True)","f06a209f":"df = pd.read_csv('\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv')","aa79499f":"df.describe()","980a9eb8":"if not df.isnull().values.any():\n    print(\"No missing values in the data.\")\nelse: \n    print(\"There is missing values in the data, you need to preprocess those values.\")","caf2a304":"sns.boxplot(x=\"Pregnancies\", y=\"Insulin\", data=df)","b6341664":"sns.pairplot(df, hue=\"Outcome\", markers=[\"o\", \"s\"], corner=True);","bfcb49d9":"sns.distplot(df.BloodPressure.dropna());","a763796c":"df[df['BloodPressure'] == 0].describe()","b4d6b23d":"fig, axs = plt.subplots(ncols=3, figsize=(20,10))\nsns.distplot(df.Glucose, ax = axs[0])\nsns.distplot(df.BMI, ax = axs[1])\nsns.distplot(df.Insulin, ax = axs[2])\nplt.show()","5fec188c":"df_clean = df[df['BloodPressure'] != 0]\ndf_clean = df_clean[df_clean['BMI'] != 0]\ndf_clean = df_clean[df_clean['Glucose'] != 0]\ndf_clean.describe()","9fea6c15":"x = df_clean.drop(\"Outcome\", axis=1)\ny = df_clean[\"Outcome\"]","9e3faa82":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)","76ed8127":"from sklearn import datasets, linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Create linear regression object\nregr = linear_model.LinearRegression()\n\n# Train the model using the training sets\nregr.fit(X_train, y_train)\n\n# Make predictions using the testing set\ndiabetes_y_pred = regr.predict(X_test)","9680bbe6":"# The coefficients\nprint('Coefficients: \\n', regr.coef_)\n# The mean squared error\nprint('Mean squared error: %.2f'\n      % mean_squared_error(y_test, diabetes_y_pred))\n# The coefficient of determination: 1 is perfect prediction\nprint('Coefficient of determination: %.2f'\n      % r2_score(y_test, diabetes_y_pred))","ed8f5624":"y_pred = (diabetes_y_pred > 0.5).astype(int)","2c4b64ed":"print(f\"Accuracy: {np.around(sum(y_pred == y_test)\/len(y_test)*100,1)}%\")","956dd135":"from sklearn.metrics import roc_curve, auc\n\ndef roc(y_test, y_pred, model_name, title=\"ROC\"):\n    \"\"\"Creates and plots the roc for a model.\n    \"\"\"\n    \n    fpr, tpr, _ = roc_curve(y_test, y_pred)\n    roc_auc = auc(fpr, tpr)\n    lw = 2\n    plt.plot(fpr, tpr,\n             lw=lw, label=f'{model_name} ROC curve area = {roc_auc:0.2f}')\n    plt.plot([0, 1], [0, 1], color='red', lw=lw, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title(title)\n    plt.legend(loc=\"lower right\")","c8925b11":"from sklearn.ensemble import RandomForestClassifier\nrandomforest = RandomForestClassifier(n_estimators=100, n_jobs=1, random_state=0)\nrandomforest.fit(X_train,y_train)","73fae425":"y_pred_RF = randomforest.predict_proba(X_test)\nprint(f\"Accuracy: {np.around(sum(np.argmax(y_pred_RF, axis=1) == y_test)\/len(y_test)*100,1)}%\")\nroc(y_test, y_pred_RF[:,1], \"Random Forest\")","5d595eb0":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import preprocessing\n\nModel = LogisticRegression();\n\n# Let's rescale the data\nX_scaled = preprocessing.scale(X_train)\nModel.fit(X_scaled, y_train);","b4dadafa":"X_scale_test = preprocessing.scale(X_test)\ny_pred_Log = Model.predict_proba(X_scale_test)\nprint(f\"Accuracy: {np.around(sum(np.argmax(y_pred_Log, axis=1) == y_test)\/len(y_test)*100,1)}%\")\nroc(y_test, y_pred_Log[:, 1], \"Logistic regression\")","19075c96":"from sklearn.ensemble import GradientBoostingClassifier\n\nGB = GradientBoostingClassifier(n_estimators=100, learning_rate = 0.05, max_features=3, max_depth = 10, random_state = 0)\nGB.fit(X_train,y_train)","1507f76c":"y_pred_GB = GB.predict_proba(X_test)\nprint(f\"Accuracy: {np.around(sum(np.argmax(y_pred_GB, axis=1) == y_test)\/len(y_test)*100,1)}%\")\nroc(y_test, y_pred_GB[:,1], \"Gradient Boost\")","3a0f6252":"from xgboost import XGBRegressor, XGBClassifier\n\n# Define the model\nXGBR = XGBRegressor(n_estimators=1000, learning_rate=0.05) # Your code here\nXGBC = XGBClassifier(n_estimators=1000, learning_rate=0.05)\n\n# Fit the model\nXGBR.fit(X_train, y_train,\n               early_stopping_rounds=5,\n              eval_set=[(X_test, y_test)],\n              verbose=0)\n\nXGBC.fit(X_train, y_train,\n               early_stopping_rounds=5,\n              eval_set=[(X_test, y_test)],\n              verbose=0)","9b06d6c3":"y_pred_XGBC = XGBC.predict_proba(X_test)\nprint(f\"Classifier Accuracy: {np.around(sum(np.argmax(y_pred_XGBC, axis=1) == y_test)\/len(y_test)*100,1)}%\")\nroc(y_test, y_pred_XGBC[:, 1], \"Gradient Boost Classifier\")\n\ny_pred_XGBR = XGBR.predict(X_test)\ny_pred = (y_pred_XGBR > 0.5).astype(int)\nprint(f\"Regressor Accuracy: {np.around(sum(y_pred == y_test)\/len(y_test)*100,1)}%\")\nroc(y_test, y_pred_XGBR, \"Gradient Boost Regressor\")","19aa5e2e":"roc(y_test, diabetes_y_pred, \"Linear regression\")\nroc(y_test, y_pred_RF[:,1], \"Random Forest\")\nroc(y_test, y_pred_Log[:, 1], \"Logistic regression\")\nroc(y_test, y_pred_GB[:,1], \"Gradient Boost\")\nroc(y_test, y_pred_XGBR, \"XGBoost Regressor\")\nroc(y_test, y_pred_XGBC[:, 1], \"XGBoost Classifier\")","f6393622":"Looking at the pairplot, we see that there is some data with Blood Pressure = 0, which seems odd... Let's look it further.","b0fd1c0c":"# EDA and simple baselines \n\nIn this notebook I will do the Exploratory Data Analysis of this dataset and create some baselines with Linear Regression, Random Forest, Logistic regression, Gradient Boost, and XGBoost.","86b0c43b":"## 1.2 Checking statistics and correlations of the data","f06ec8f9":"## 2.2 Random Forest","db8da3dc":"# 1. EDA","8d6dce2d":"### If you like this notebook, please upvote! :)","3e71f083":"## 2.1 Linear Regression","43729b25":"Let's consider y_pred > 0.5 as pacients with diabetes and y_pred < 0.5 as pacients without diabetes and evaluate our accuracy.","e542058c":"### 2.1.2 Evaluating the model using ROC Curve","3a404d03":"## 1.1 Getting the data\nFirst, let's create a dataframe from the `.csv` file.","998ffad9":"# 0. Importing dependencies","95dced4f":"So analysing those plots, we see that we can discart the 0 values for Glucose and BMI, because they are probably null values.","47ff7b57":"## 2.3 Logistic Regression","050497a0":"In order to visualize the correlation of variables, let's do a pair plot using seaborn! I choose to do only the lower triangular ","442196f5":"## 2.4 Gradient Boost","6d2d8b95":"Analysing this, we see that the it is higly likely that null values are written as 0, thus we can remove them.","33f202ce":"We see that there is 35 counts that doesn't have data for insulin and blood pressure, since those are important factors for diabetes, I choose to delete this data since they seem to be out of place. Also we shall delete other data points which have 0 as the value, such as BMI and Glucose. Let's analyze the distributions:","e25943e9":"## 1.3 Creating a training\/test split\n\nNow we create a training\/test split in order to see how our model works for unseen data.","99c90b82":"# Analysing all models\n\nWe see that all the models have similar ROC curves and the one that have the best accuracy and ROC curve area is the XGBoost Regressor.","7d985700":"# 2.5 XGBoost","ef28ff66":"# 2. Creating models\n\nLet's create some models to see which one works better.","b40c895a":"Let's check the statistics and see if there is any null values in the data."}}