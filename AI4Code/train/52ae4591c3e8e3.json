{"cell_type":{"b8186827":"code","f8f414ac":"code","0a77a15f":"code","68bd4551":"code","0e215276":"code","adfcb695":"code","5e2260c3":"code","8e193724":"code","709af353":"code","9998bbb1":"code","87ce2771":"code","24b60f03":"code","e5c7e8ad":"code","6964bbac":"code","457fe6dd":"code","7c41e223":"code","a5727e45":"code","9f26d745":"code","e15ce3bd":"code","e8873831":"code","2748192c":"code","a7cb7072":"code","26761f07":"code","84035f8a":"code","214b64f3":"code","3d421611":"code","3dfc222d":"code","0eb9d982":"code","20316cd6":"code","dbccf13c":"code","f7a0054b":"code","b80d6609":"code","9e345eff":"code","1e5ef11b":"code","d38dbc05":"code","cc078b6d":"code","0f413770":"code","19bd5b91":"code","7fb2eb17":"code","7ffdf815":"markdown","59c95f65":"markdown","a647a262":"markdown","07436fdc":"markdown","ec81ea9f":"markdown","2b4ab44b":"markdown","b0ec72c0":"markdown","df7cab11":"markdown","f39c43f6":"markdown","a5878796":"markdown","f21f12ae":"markdown","116e5ad7":"markdown","6d0ae683":"markdown","0a8e9499":"markdown","0bd844e7":"markdown","e39ce357":"markdown","8c9b25af":"markdown","1db83437":"markdown","c9eda3a5":"markdown","0d715baf":"markdown","05aca92d":"markdown","a8cc75dc":"markdown","105a2264":"markdown","396957fc":"markdown","5fd58d09":"markdown","641b4341":"markdown","109b130c":"markdown","ee2ece01":"markdown","a0f20c0a":"markdown","d44701b7":"markdown","c57bbb91":"markdown","8b6a3730":"markdown","34d3cb8a":"markdown","4f2d6364":"markdown","57ef0cfb":"markdown","73ad24bb":"markdown","ed5651bb":"markdown","1330aec5":"markdown","c4c40980":"markdown","c9f1f88a":"markdown","365a692b":"markdown","fe2e975e":"markdown","0d769bf4":"markdown","f2b476c5":"markdown","44306cbb":"markdown"},"source":{"b8186827":"# @title Tutorial slides\n\n# @markdown These are the slides for the videos in this tutorial\n\n# @markdown If you want to locally download the slides, click [here](https:\/\/osf.io\/rd7ng\/download)\nfrom IPython.display import IFrame\nIFrame(src=f\"https:\/\/mfr.ca-1.osf.io\/render?url=https:\/\/osf.io\/rd7ng\/?direct%26mode=render%26action=download%26mode=render\", width=854, height=480)","f8f414ac":"# @title Install dependencies\n!pip install torchvision --quiet\n\n# @markdown Install *Huggingface BigGAN* library\n\n# @markdown Please ignore `Errors`\/`Warnings` during installation.\n!pip install pytorch-pretrained-biggan --quiet\n!pip install Pillow libsixel-python --quiet\n\n!pip install git+https:\/\/github.com\/NeuromatchAcademy\/evaltools --quiet\nfrom evaltools.airtable import AirtableForm\n\n# generate airtable form\natform = AirtableForm('appn7VdPRseSoMXEG','W2D5_T1','https:\/\/portal.neuromatchacademy.org\/api\/redirect\/to\/9c55f6cb-cdf9-4429-ac1c-ec44fe64c303')","0a77a15f":"# Imports\nimport torch\nimport random\n\nimport numpy as np\nimport matplotlib.pylab as plt\nfrom sklearn.decomposition import PCA\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\n\nimport torchvision\nfrom torchvision import datasets, transforms\n\nfrom pytorch_pretrained_biggan import BigGAN\nfrom pytorch_pretrained_biggan import one_hot_from_names\nfrom pytorch_pretrained_biggan import truncated_noise_sample\n\nfrom tqdm.notebook import tqdm, trange","68bd4551":"# @title Figure settings\nimport ipywidgets as widgets       # interactive display\nfrom ipywidgets import interact, IntSlider, FloatSlider, interact_manual, fixed\nfrom ipywidgets import FloatLogSlider, HBox, Layout, VBox, interactive, Label\nfrom ipywidgets import interactive_output, Dropdown\n\n%config InlineBackend.figure_format = 'retina'\nplt.style.use(\"https:\/\/raw.githubusercontent.com\/NeuromatchAcademy\/content-creation\/main\/nma.mplstyle\")","0e215276":"# @title Helper functions\n\n#@title Helper functions\n\ndef image_moments(image_batches, n_batches=None):\n  \"\"\"\n  Compute mean an covariance of all pixels from batches of images\n  \"\"\"\n  m1, m2 = torch.zeros((), device=DEVICE), torch.zeros((), device=DEVICE)\n  n = 0\n  for im in tqdm(image_batches, total=n_batches, leave=False,\n                 desc='Computing pixel mean and covariance...'):\n    im = im.to(DEVICE)\n    b = im.size()[0]\n    im = im.view(b, -1)\n    m1 = m1 + im.sum(dim=0)\n    m2 = m2 + (im.view(b,-1,1) * im.view(b,1,-1)).sum(dim=0)\n    n += b\n  m1, m2 = m1\/n, m2\/n\n  cov = m2 - m1.view(-1,1)*m1.view(1,-1)\n  return m1.cpu(), cov.cpu()\n\n\ndef interpolate(A, B, num_interps):\n  if A.shape != B.shape:\n    raise ValueError('A and B must have the same shape to interpolate.')\n  alphas = np.linspace(0, 1, num_interps)\n  return np.array([(1-a)*A + a*B for a in alphas])\n\n\ndef kl_q_p(zs, phi):\n  \"\"\"Given [b,n,k] samples of z drawn from q, compute estimate of KL(q||p).\n  phi must be size [b,k+1]\n\n  This uses mu_p = 0 and sigma_p = 1, which simplifies the log(p(zs)) term to\n  just -1\/2*(zs**2)\n  \"\"\"\n  b, n, k = zs.size()\n  mu_q, log_sig_q = phi[:,:-1], phi[:,-1]\n  log_p = -0.5*(zs**2)\n  log_q = -0.5*(zs - mu_q.view(b,1,k))**2 \/ log_sig_q.exp().view(b,1,1)**2 - log_sig_q.view(b,1,-1)\n  # Size of log_q and log_p is [b,n,k]. Sum along [k] but mean along [b,n]\n  return (log_q - log_p).sum(dim=2).mean(dim=(0,1))\n\n\ndef log_p_x(x, mu_xs, sig_x):\n  \"\"\"Given [batch, ...] input x and [batch, n, ...] reconstructions, compute\n  pixel-wise log Gaussian probability\n\n  Sum over pixel dimensions, but mean over batch and samples.\n  \"\"\"\n  b, n = mu_xs.size()[:2]\n  # Flatten out pixels and add a singleton dimension [1] so that x will be\n  # implicitly expanded when combined with mu_xs\n  x = x.reshape(b, 1, -1)\n  _, _, p = x.size()\n  squared_error = (x - mu_xs.view(b, n, -1))**2 \/ (2*sig_x**2)\n\n  # Size of squared_error is [b,n,p]. log prob is by definition sum over [p].\n  # Expected value requires mean over [n]. Handling different size batches\n  # requires mean over [b].\n  return -(squared_error + torch.log(sig_x)).sum(dim=2).mean(dim=(0,1))\n\n\ndef pca_encoder_decoder(mu, cov, k):\n  \"\"\"\n  Compute encoder and decoder matrices for PCA dimensionality reduction\n  \"\"\"\n  mu = mu.view(1,-1)\n  u, s, v = torch.svd_lowrank(cov, q=k)\n  W_encode = v \/ torch.sqrt(s)\n  W_decode = u * torch.sqrt(s)\n\n  def pca_encode(x):\n    # Encoder: subtract mean image and project onto top K eigenvectors of\n    # the data covariance\n    return (x.view(-1,mu.numel()) - mu) @ W_encode\n\n  def pca_decode(h):\n    # Decoder: un-project then add back in the mean\n    return (h @ W_decode.T) + mu\n\n  return pca_encode, pca_decode\n\n\ndef cout(x, layer):\n  \"\"\"Unnecessarily complicated but complete way to\n  calculate the output depth, height and width size for a Conv2D layer\n\n  Args:\n    x (tuple): input size (depth, height, width)\n    layer (nn.Conv2d): the Conv2D layer\n\n  returns:\n    (int): output shape as given in [Ref]\n\n  Ref:\n    https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.Conv2d.html\n  \"\"\"\n  assert isinstance(layer, nn.Conv2d)\n  p = layer.padding if isinstance(layer.padding, tuple) else (layer.padding,)\n  k = layer.kernel_size if isinstance(layer.kernel_size, tuple) else (layer.kernel_size,)\n  d = layer.dilation if isinstance(layer.dilation, tuple) else (layer.dilation,)\n  s = layer.stride if isinstance(layer.stride, tuple) else (layer.stride,)\n  in_depth, in_height, in_width = x\n  out_depth = layer.out_channels\n  out_height = 1 + (in_height + 2 * p[0] - (k[0] - 1) * d[0] - 1) \/\/ s[0]\n  out_width = 1 + (in_width + 2 * p[-1] - (k[-1] - 1) * d[-1] - 1) \/\/ s[-1]\n  return (out_depth, out_height, out_width)","adfcb695":"# @title Plotting functions\n\ndef plot_gen_samples_ppca(therm1, therm2, therm_data_sim):\n  plt.plot(therm1, therm2, '.', c='c', label='training data')\n  plt.plot(therm_data_sim[0], therm_data_sim[1], '.', c='m', label='\"generated\" data')\n  plt.axis('equal')\n  plt.xlabel('Thermometer 1 ($^\\circ$C)')\n  plt.ylabel('Thermometer 2 ($^\\circ$C)')\n  plt.legend()\n  plt.show()\n\n\ndef plot_linear_ae(lin_losses):\n  plt.figure()\n  plt.plot(lin_losses)\n  plt.ylim([0, 2*torch.as_tensor(lin_losses).median()])\n  plt.xlabel('Training batch')\n  plt.ylabel('MSE Loss')\n  plt.show()\n\n\ndef plot_conv_ae(lin_losses, conv_losses):\n  plt.figure()\n  plt.plot(lin_losses)\n  plt.plot(conv_losses)\n  plt.legend(['Lin AE', 'Conv AE'])\n  plt.xlabel('Training batch')\n  plt.ylabel('MSE Loss')\n  plt.ylim([0,\n            2*max(torch.as_tensor(conv_losses).median(),\n                  torch.as_tensor(lin_losses).median())])\n  plt.show()\n\n\ndef plot_images(images, h=3, w=3, plt_title=''):\n  plt.figure(figsize=(h*2, w*2))\n  plt.suptitle(plt_title, y=1.03)\n  for i in range(h*w):\n    plt.subplot(h, w, i + 1)\n    plot_torch_image(images[i])\n  plt.axis('off')\n  plt.show()\n\n\ndef plot_phi(phi, num=4):\n  plt.figure(figsize=(12, 3))\n  for i in range(num):\n    plt.subplot(1, num, i + 1)\n    plt.scatter(zs[i, :, 0], zs[i, :, 1], marker='.')\n    th = torch.linspace(0, 6.28318, 100)\n    x, y = torch.cos(th), torch.sin(th)\n    # Draw 2-sigma contours\n    plt.plot(\n        2*x*phi[i, 2].exp().item() + phi[i, 0].item(),\n        2*y*phi[i, 2].exp().item() + phi[i, 1].item()\n        )\n    plt.xlim(-5, 5)\n    plt.ylim(-5, 5)\n    plt.grid()\n    plt.axis('equal')\n  plt.suptitle('If rsample() is correct, then most but not all points should lie in the circles')\n  plt.show()\n\n\ndef plot_torch_image(image, ax=None):\n  ax = ax if ax is not None else plt.gca()\n  c, h, w = image.size()\n  if c==1:\n    cm = 'gray'\n  else:\n    cm = None\n\n  # Torch images have shape (channels, height, width) but matplotlib expects\n  # (height, width, channels) or just (height,width) when grayscale\n  im_plt = torch.clip(image.detach().cpu().permute(1,2,0).squeeze(), 0.0, 1.0)\n  ax.imshow(im_plt, cmap=cm)\n  ax.set_xticks([])\n  ax.set_yticks([])\n  ax.spines['right'].set_visible(False)\n  ax.spines['top'].set_visible(False)","5e2260c3":"# @title Set random seed\n\n# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n\n# for DL its critical to set the random seed so that students can have a\n# baseline to compare their results to expected results.\n# Read more here: https:\/\/pytorch.org\/docs\/stable\/notes\/randomness.html\n\n# Call `set_seed` function in the exercises to ensure reproducibility.\nimport random\nimport torch\n\ndef set_seed(seed=None, seed_torch=True):\n  if seed is None:\n    seed = np.random.choice(2 ** 32)\n  random.seed(seed)\n  np.random.seed(seed)\n  if seed_torch:\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\n  print(f'Random seed {seed} has been set.')\n\n\n# In case that `DataLoader` is used\ndef seed_worker(worker_id):\n  worker_seed = torch.initial_seed() % 2**32\n  np.random.seed(worker_seed)\n  random.seed(worker_seed)","8e193724":"# @title Set device (GPU or CPU). Execute `set_device()`\n# especially if torch modules used.\n\n# inform the user if the notebook uses GPU or CPU.\n\ndef set_device():\n  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n  if device != \"cuda\":\n    print(\"WARNING: For this notebook to perform best, \"\n        \"if possible, in the menu under `Runtime` -> \"\n        \"`Change runtime type.`  select `GPU` \")\n  else:\n    print(\"GPU is enabled in this notebook.\")\n\n  return device","709af353":"SEED = 2021\nset_seed(seed=SEED)\nDEVICE = set_device()","9998bbb1":"# @title Download `wordnet` dataset\n\n# import nltk\n# nltk.download('wordnet')\n\nimport requests, zipfile\n\nfname = 'wordnet.zip'\nurl = 'https:\/\/osf.io\/ekjxy\/download'\nr = requests.get(url, allow_redirects=True)\n\nwith open('wordnet.zip', 'wb') as fd:\n  fd.write(r.content)\n\nwith zipfile.ZipFile(fname, 'r') as zip_ref:\n  zip_ref.extractall('\/root\/nltk_data\/corpora')","87ce2771":"# @title Video 1: Generative Modeling\nfrom ipywidgets import widgets\n\nout2 = widgets.Output()\nwith out2:\n  from IPython.display import IFrame\n  class BiliVideo(IFrame):\n    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n      self.id=id\n      src = \"https:\/\/player.bilibili.com\/player.html?bvid={0}&page={1}\".format(id, page)\n      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n\n  video = BiliVideo(id=f\"BV1Vy4y1j7cN\", width=854, height=480, fs=1)\n  print(\"Video available at https:\/\/www.bilibili.com\/video\/{0}\".format(video.id))\n  display(video)\n\nout1 = widgets.Output()\nwith out1:\n  from IPython.display import YouTubeVideo\n  video = YouTubeVideo(id=f\"5EEx0sdyR_U\", width=854, height=480, fs=1, rel=0)\n  print(\"Video available at https:\/\/youtube.com\/watch?v=\" + video.id)\n  display(video)\n\nout = widgets.Tab([out1, out2])\nout.set_title(0, 'Youtube')\nout.set_title(1, 'Bilibili')\n\n# add event to airtable\natform.add_event('Video 1: Generative Modeling')\n\n\ndisplay(out)","24b60f03":"# @markdown Download BigGAN (a generative model) and a few standard image datasets\n\n## Initially was downloaded directly\n# biggan_model = BigGAN.from_pretrained('biggan-deep-256')\n\nurl = \"https:\/\/osf.io\/3yvhw\/download\"\nfname = \"biggan_deep_256\"\nr = requests.get(url, allow_redirects=True)\nwith open(fname, 'wb') as fd:\n  fd.write(r.content)\n\nbiggan_model = torch.load(fname)","e5c7e8ad":"# @title Student Response\nfrom ipywidgets import widgets\n\n\ntext=widgets.Textarea(\n   value='Type your answer here and click on `Submit!`',\n   placeholder='Type something',\n   description='',\n   disabled=False\n)\n\nbutton = widgets.Button(description=\"Submit!\")\n\ndisplay(text,button)\n\ndef on_button_clicked(b):\n   atform.add_answer('q1' , text.value)\n   print(\"Submission successful!\")\n\n\nbutton.on_click(on_button_clicked)","6964bbac":"# @markdown BigGAN Image Generator (the updates may take a few seconds)\n\n# category = 'German shepherd' # @param ['tench', 'magpie', 'jellyfish', 'German shepherd', 'bee', 'acoustic guitar', 'coffee mug', 'minibus', 'monitor']\n# z_magnitude = -16 # @param {type:\"slider\", min:-50, max:50, step:1}\n\ndef sample_from_biggan(category, z_magnitude):\n  unit_vector = np.ones((1, 128))\/np.sqrt(128)\n  z = z_magnitude * unit_vector\n  y = one_hot_from_names(category, batch_size=1)\n\n  z = torch.from_numpy(z)\n  z = z.float()\n  y = torch.from_numpy(y)\n\n  # Move to GPU\n  z = z.to(device=set_device())\n  y = y.to(device=set_device())\n  biggan_model.to(device=set_device())\n\n  with torch.no_grad():\n    output = biggan_model(z, y, 1)\n\n  # Back to CPU\n  output = output.to('cpu')\n\n  # The output layer of BigGAN has a tanh layer, resulting the range of [-1, 1] for the output image\n  # Therefore, we normalize the images properly to [0, 1] range.\n  # Clipping is only in case of numerical instability problems\n\n  output = torch.clip(((output.detach().clone() + 1) \/ 2.0), 0, 1)\n\n  plt.imshow(output.squeeze().moveaxis(0,-1))\n  plt.axis('off')\n\nz_slider = IntSlider(min=-15, max=15, step=1, value=0,\n                     continuous_update=False,\n                     description='Z Magnitude',\n                     layout=Layout(width='440px'))\n\ncategory_dropdown = Dropdown(\n    options=['tench', 'magpie', 'jellyfish', 'German shepherd', 'bee',\n             'acoustic guitar', 'coffee mug', 'minibus', 'monitor'],\n             value=\"German shepherd\",\n             description=\"Category: \")\n\nwidgets_ui = VBox([category_dropdown, z_slider])\n\nwidgets_out = interactive_output(sample_from_biggan,\n                                 {\n                                     'z_magnitude': z_slider,\n                                  'category': category_dropdown\n                                  }\n                                 )\n\ndisplay(widgets_ui, widgets_out)","457fe6dd":"# @title Student Response\nfrom ipywidgets import widgets\n\n\ntext=widgets.Textarea(\n   value='Type your answer here and click on `Submit!`',\n   placeholder='Type something',\n   description='',\n   disabled=False\n)\n\nbutton = widgets.Button(description=\"Submit!\")\n\ndisplay(text,button)\n\ndef on_button_clicked(b):\n   atform.add_answer('q2' , text.value)\n   print(\"Submission successful!\")\n\n\nbutton.on_click(on_button_clicked)","7c41e223":"# @markdown BigGAN Interpolation Widget (the updates may take a few seconds)\n\ndef interpolate_biggan(category_A, z_magnitude_A, category_B, z_magnitude_B):\n  num_interps = 16\n\n  # category_A = 'jellyfish' #@param ['tench', 'magpie', 'jellyfish', 'German shepherd', 'bee', 'acoustic guitar', 'coffee mug', 'minibus', 'monitor']\n  # z_magnitude_A = 0 #@param {type:\"slider\", min:-10, max:10, step:1}\n\n  # category_B = 'German shepherd' #@param ['tench', 'magpie', 'jellyfish', 'German shepherd', 'bee', 'acoustic guitar', 'coffee mug', 'minibus', 'monitor']\n  # z_magnitude_B = 0 #@param {type:\"slider\", min:-10, max:10, step:1}\n\n\n  def interpolate_and_shape(A, B, num_interps):\n    interps = interpolate(A, B, num_interps)\n    return (interps.transpose(1, 0, *range(2, len(interps.shape)))\n                  .reshape(num_interps, *interps.shape[2:]))\n\n  unit_vector = np.ones((1, 128))\/np.sqrt(128)\n  z_A = z_magnitude_A * unit_vector\n  z_B = z_magnitude_B * unit_vector\n  y_A = one_hot_from_names(category_A, batch_size=1)\n  y_B = one_hot_from_names(category_B, batch_size=1)\n\n  z_interp = interpolate_and_shape(z_A, z_B, num_interps)\n  y_interp = interpolate_and_shape(y_A, y_B, num_interps)\n\n  # Convert to tensor\n  z_interp = torch.from_numpy(z_interp)\n  z_interp = z_interp.float()\n  y_interp = torch.from_numpy(y_interp)\n\n  # Move to GPU\n  z_interp = z_interp.to(DEVICE)\n  y_interp = y_interp.to(DEVICE)\n  biggan_model.to(DEVICE)\n\n  with torch.no_grad():\n    output = biggan_model(z_interp, y_interp, 1)\n\n  # Back to CPU\n  output = output.to('cpu')\n\n  # The output layer of BigGAN has a tanh layer, resulting the range of [-1, 1] for the output image\n  # Therefore, we normalize the images properly to [0, 1] range.\n  # Clipping is only in case of numerical instability problems\n\n  output = torch.clip(((output.detach().clone() + 1) \/ 2.0), 0, 1)\n  output = output\n\n  # Make grid and show generated samples\n  output_grid = torchvision.utils.make_grid(output,\n                                            nrow=min(4, output.shape[0]),\n                                            padding=5)\n  plt.axis('off');\n  plt.imshow(output_grid.permute(1, 2, 0))\n  plt.show()\n\n\nz_A_slider = IntSlider(min=-10, max=10, step=1, value=0,\n                        continuous_update=False, description='Z Magnitude A',\n                        layout=Layout(width='440px'), style={'description_width': 'initial'})\n\nz_B_slider = IntSlider(min=-10, max=10, step=1, value=0,\n                        continuous_update=False, description='Z Magntude B',\n                        layout=Layout(width='440px'), style={'description_width': 'initial'})\n\ncategory_A_dropdown = Dropdown(\n    options=['tench', 'magpie', 'jellyfish', 'German shepherd', 'bee',\n             'acoustic guitar', 'coffee mug', 'minibus', 'monitor'],\n                      value=\"German shepherd\", description=\"Category A: \")\n\ncategory_B_dropdown = Dropdown(\n    options=['tench', 'magpie', 'jellyfish', 'German shepherd', 'bee',\n             'acoustic guitar', 'coffee mug', 'minibus', 'monitor'],\n                      value=\"jellyfish\", description=\"Category B: \")\n\n\n\nwidgets_ui = VBox([HBox([category_A_dropdown, z_A_slider]),\n                   HBox([category_B_dropdown, z_B_slider])])\n\nwidgets_out = interactive_output(interpolate_biggan,\n                                 {'category_A': category_A_dropdown,\n                                  'z_magnitude_A': z_A_slider,\n                                  'category_B': category_B_dropdown,\n                                  'z_magnitude_B': z_B_slider})\n\ndisplay(widgets_ui, widgets_out)","a5727e45":"# @title Video 2: Latent Variable Models\nfrom ipywidgets import widgets\n\nout2 = widgets.Output()\nwith out2:\n  from IPython.display import IFrame\n  class BiliVideo(IFrame):\n    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n      self.id=id\n      src = \"https:\/\/player.bilibili.com\/player.html?bvid={0}&page={1}\".format(id, page)\n      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n\n  video = BiliVideo(id=f\"BV1Db4y167Ys\", width=854, height=480, fs=1)\n  print(\"Video available at https:\/\/www.bilibili.com\/video\/{0}\".format(video.id))\n  display(video)\n\nout1 = widgets.Output()\nwith out1:\n  from IPython.display import YouTubeVideo\n  video = YouTubeVideo(id=f\"_e0nKUeBDFo\", width=854, height=480, fs=1, rel=0)\n  print(\"Video available at https:\/\/youtube.com\/watch?v=\" + video.id)\n  display(video)\n\nout = widgets.Tab([out1, out2])\nout.set_title(0, 'Youtube')\nout.set_title(1, 'Bilibili')\n\n# add event to airtable\natform.add_event('Video 2: Latent Variable Models')\n\ndisplay(out)","9f26d745":"# @markdown Generate example datapoints from the two thermometers\n\ndef generate_data(n_samples, mean_of_temps, cov_of_temps, seed):\n  \"\"\"\n  Generate random data, normally distributed\n\n  Args:\n    n_samples : int\n      The number of samples to be generated\n    mean_of_temps : numpy.ndarray\n      1D array with the mean of temparatures, Kx1\n    cov_of_temps : numpy.ndarray\n      2D array with the covariance, , KxK\n    seed : int\n      Set random seed for the psudo random generator\n  Returns:\n    therm1 : numpy.ndarray\n    therm2 : numpy.ndarray\n  \"\"\"\n\n  np.random.seed(seed)\n  therm1, therm2 = np.random.multivariate_normal(mean_of_temps,\n                                                cov_of_temps,\n                                                n_samples).T\n  return therm1, therm2\n\n\nn_samples = 2000\nmean_of_temps = np.array([25, 25])\ncov_of_temps = np.array([[10, 5], [5, 10]])\ntherm1, therm2 = generate_data(n_samples, mean_of_temps, cov_of_temps, seed=SEED)\n\nplt.plot(therm1, therm2, '.')\nplt.axis('equal')\nplt.xlabel('Thermometer 1 ($^\\circ$C)')\nplt.ylabel('Thermometer 2 ($^\\circ$C)')\nplt.show()","e15ce3bd":"# @markdown Add first PC axes to the plot\n\nplt.plot(therm1, therm2, '.')\nplt.axis('equal')\nplt.xlabel('Thermometer 1 ($^\\circ$C)')\nplt.ylabel('Thermometer 2 ($^\\circ$C)')\nplt.plot([plt.axis()[0], plt.axis()[1]],\n         [plt.axis()[0], plt.axis()[1]])\nplt.show()","e8873831":"# Project Data onto the principal component axes.\n# We could have \"learned\" this from the data by applying PCA,\n# but we \"know\" the value from the problem definition.\npc_axes = np.array([1.0, 1.0]) \/ np.sqrt(2.0)\n\n# thermometers data\ntherm_data = np.array([therm1, therm2])\n\n# Zero center the data\ntherm_data_mean = np.mean(therm_data, 1)\ntherm_data_center = np.outer(therm_data_mean, np.ones(therm_data.shape[1]))\ntherm_data_zero_centered = therm_data - therm_data_center\n\n# Calculate the variance of the projection on the PC axes\npc_projection = np.matmul(pc_axes, therm_data_zero_centered);\npc_axes_variance = np.var(pc_projection)\n\n# Calculate noise\n# Calculate the residual variance (variance not accounted for by projection on the PC axes)\nsensor_noise_std = np.mean(np.linalg.norm(therm_data_zero_centered - np.outer(pc_axes, pc_projection), axis=0, ord=2))\nsensor_noise_var = sensor_noise_std **2","2748192c":"def gen_from_pPCA(noise_var, data_mean, pc_axes, pc_variance):\n  \"\"\"\n  Args:\n    noise_var (np.ndarray): sensor noise variance\n    data_mean (np.ndarray): thermometer data mean\n    pc_axes (np.ndarray): principal component axes\n    pc_variance (np.ndarray): the variance of the projection on the PC axes (variance along PC axis)\n  \"\"\"\n  # We are matching this value to the thermometer data so the visualizations look similar\n  n_samples = 1000\n\n  # Randomly sample from z (latent space value)\n  z = np.random.normal(0.0, np.sqrt(pc_variance), n_samples)\n\n  # sensor noise covariance matrix (\u2211)\n  epsilon_cov = [[noise_var, 0.0], [0.0, noise_var]]\n\n  # data mean reshaped for the generation\n  sim_mean = np.outer(data_mean, np.ones(n_samples))\n\n  ####################################################################\n  # Fill in all missing code below (...),\n  # then remove or comment the line below to test your class\n  # raise NotImplementedError(\"Please complete the `gen_from_pPCA` function\")\n  ####################################################################\n  # draw `n_samples` from `np.random.multivariate_normal`\n  rand_eps = np.random.multivariate_normal(mean =[0,0], cov = epsilon_cov,size=n_samples)\n  rand_eps = rand_eps.T\n\n  # generate (simulate, draw) `n_samples` from pPCA model\n  therm_data_sim = sim_mean + np.outer(pc_axes,z) + rand_eps\n\n  return therm_data_sim\n\n\n# add event to airtable\natform.add_event('Coding Exercise 2: pPCA')\n\n## Uncomment to test your code\ntherm_data_sim = gen_from_pPCA(sensor_noise_var, therm_data_mean, pc_axes, pc_axes_variance)\nplot_gen_samples_ppca(therm1, therm2, therm_data_sim)","a7cb7072":"# @title Video 3: Autoenconders\nfrom ipywidgets import widgets\n\nout2 = widgets.Output()\nwith out2:\n  from IPython.display import IFrame\n  class BiliVideo(IFrame):\n    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n      self.id=id\n    \n      src = \"https:\/\/player.bilibili.com\/player.html?bvid={0}&page={1}\".format(id, page)\n      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n\n  video = BiliVideo(id=f\"BV16b4y167Z2\", width=854, height=480, fs=1)\n  print(\"Video available at https:\/\/www.bilibili.com\/video\/{0}\".format(video.id))\n  display(video)\n\nout1 = widgets.Output()\nwith out1:\n  from IPython.display import YouTubeVideo\n  video = YouTubeVideo(id=f\"MlyIL1PmDCA\", width=854, height=480, fs=1, rel=0)\n  print(\"Video available at https:\/\/youtube.com\/watch?v=\" + video.id)\n  display(video)\n\nout = widgets.Tab([out1, out2])\nout.set_title(0, 'Youtube')\nout.set_title(1, 'Bilibili')\n\n# add event to airtable\natform.add_event('Video 3: Autoenconders')\n\ndisplay(out)","26761f07":"# @markdown Download MNIST and CIFAR10 datasets\nimport tarfile, requests, os\n\nfname = 'MNIST.tar.gz'\nname = 'mnist'\nurl = 'https:\/\/osf.io\/y2fj6\/download'\n\nif not os.path.exists(name):\n  print('\\nDownloading MNIST dataset...')\n  r = requests.get(url, allow_redirects=True)\n  with open(fname, 'wb') as fh:\n    fh.write(r.content)\n  print('\\nDownloading MNIST completed..\\n')\n\nif not os.path.exists(name):\n  with tarfile.open(fname) as tar:\n    tar.extractall(name)\n    os.remove(fname)\nelse:\n  print('MNIST dataset has been dowloaded.\\n')\n\n\nfname = 'cifar-10-python.tar.gz'\nname = 'cifar10'\nurl = 'https:\/\/osf.io\/jbpme\/download'\n\nif not os.path.exists(name):\n  print('\\nDownloading CIFAR10 dataset...')\n  r = requests.get(url, allow_redirects=True)\n  with open(fname, 'wb') as fh:\n    fh.write(r.content)\n  print('\\nDownloading CIFAR10 completed.')\n\nif not os.path.exists(name):\n  with tarfile.open(fname) as tar:\n    tar.extractall(name)\n    os.remove(fname)\nelse:\n  print('CIFAR10 dataset has been dowloaded.')","84035f8a":"# @markdown Load MNIST and CIFAR10 image datasets\n# See https:\/\/pytorch.org\/docs\/stable\/torchvision\/datasets.html\n\n# MNIST\nmnist = datasets.MNIST('.\/mnist\/',\n                       train=True,\n                       transform=transforms.ToTensor(),\n                       download=False)\nmnist_val = datasets.MNIST('.\/mnist\/',\n                           train=False,\n                           transform=transforms.ToTensor(),\n                           download=False)\n\n# CIFAR 10\ncifar10 = datasets.CIFAR10('.\/cifar10\/',\n                           train=True,\n                           transform=transforms.ToTensor(),\n                           download=False)\ncifar10_val = datasets.CIFAR10('.\/cifar10\/',\n                               train=False,\n                               transform=transforms.ToTensor(),\n                               download=False)","214b64f3":"def get_data(name='mnist'):\n  if name == 'mnist':\n    my_dataset_name = \"MNIST\"\n    my_dataset = mnist\n    my_valset = mnist_val\n    my_dataset_shape = (1, 28, 28)\n    my_dataset_size = 28 * 28\n  elif name == 'cifar10':\n    my_dataset_name = \"CIFAR10\"\n    my_dataset = cifar10\n    my_valset = cifar10_val\n    my_dataset_shape = (3, 32, 32)\n    my_dataset_size = 3 * 32 * 32\n\n  return my_dataset, my_dataset_name, my_dataset_shape, my_dataset_size, my_valset\n\n\ntrain_set, dataset_name, data_shape, data_size, valid_set = get_data(name='mnist')","3d421611":"# @markdown #### Run to define the `train_autoencoder` function.\n# @markdown Feel free to inspect the training function if the time allows.\n\n# @markdown `train_autoencoder(autoencoder, dataset, device, epochs=20, batch_size=250, seed=0)`\n\n\ndef train_autoencoder(autoencoder, dataset, device, epochs=20, batch_size=250,\n                      seed=0):\n  autoencoder.to(device)\n  optim = torch.optim.Adam(autoencoder.parameters(),\n                           lr=1e-3,\n                           weight_decay=1e-5)\n  loss_fn = nn.MSELoss()\n  g_seed = torch.Generator()\n  g_seed.manual_seed(seed)\n  loader = DataLoader(dataset,\n                      batch_size=batch_size,\n                      shuffle=True,\n                      pin_memory=True,\n                      num_workers=2,\n                      worker_init_fn=seed_worker,\n                      generator=g_seed)\n\n  mse_loss = torch.zeros(epochs * len(dataset) \/\/ batch_size, device=device)\n  i = 0\n  for epoch in trange(epochs, desc='Epoch'):\n    for im_batch, _ in loader:\n      im_batch = im_batch.to(device)\n      optim.zero_grad()\n      reconstruction = autoencoder(im_batch)\n      # write the loss calculation\n      loss = loss_fn(reconstruction.view(batch_size, -1),\n                     target=im_batch.view(batch_size, -1))\n      loss.backward()\n      optim.step()\n\n      mse_loss[i] = loss.detach()\n      i += 1\n  # After training completes, make sure the model is on CPU so we can easily\n  # do more visualizations and demos.\n  autoencoder.to('cpu')\n  return mse_loss.cpu()","3dfc222d":"class LinearAutoEncoder(nn.Module):\n  def __init__(self, x_dim, h_dim):\n    \"\"\"A Linear AutoEncoder\n\n    Args:\n      x_dim (int): input dimension\n      h_dim (int): hidden dimension, bottleneck dimension, K\n    \"\"\"\n    super().__init__()\n    ####################################################################\n    # Fill in all missing code below (...),\n    # then remove or comment the line below to test your class\n    # raise NotImplementedError(\"Please complete the LinearAutoEncoder class!\")\n    ####################################################################\n    # encoder layer (a linear mapping from x_dim to K)\n    self.enc_lin = nn.Linear(x_dim, h_dim)\n    # decoder layer (a linear mapping from K to x_dim)\n    self.dec_lin = nn.Linear(h_dim, x_dim)\n\n  def encode(self, x):\n    ####################################################################\n    # Fill in all missing code below (...),\n    #raise NotImplementedError(\"Please complete the `encode` function!\")\n    ####################################################################\n    h = self.enc_lin(x)\n    return h\n\n  def decode(self, h):\n    ####################################################################\n    # Fill in all missing code below (...),\n    #raise NotImplementedError(\"Please complete the `decode` function!\")\n    ####################################################################\n    x_prime = self.dec_lin(h)\n    return x_prime\n\n  def forward(self, x):\n    flat_x = x.view(x.size(0), -1)\n    h = self.encode(flat_x)\n    return self.decode(h).view(x.size())\n\n\n# add event to airtable\natform.add_event('Coding Exercise 3.1: Linear AutoEncoder Architecture')\n\n# Pick your own K\nK = 20\nset_seed(seed=SEED)\n## Uncomment to test your code\nlin_ae = LinearAutoEncoder(data_size, K)\nlin_losses = train_autoencoder(lin_ae, train_set, device=DEVICE, seed=SEED)\nplot_linear_ae(lin_losses)","0eb9d982":"# PCA requires finding the top K eigenvectors of the data covariance. Start by\n# finding the mean and covariance of the pixels in our dataset\ng_seed = torch.Generator()\ng_seed.manual_seed(SEED)\n\nloader = DataLoader(train_set,\n                    batch_size=32,\n                    pin_memory=True,\n                    num_workers=2,\n                    worker_init_fn=seed_worker,\n                    generator=g_seed)\n\nmu, cov = image_moments((im for im, _ in loader),\n                        n_batches=len(train_set) \/\/ 32)\n\npca_encode, pca_decode = pca_encoder_decoder(mu, cov, K)","20316cd6":"# @markdown Visualize the reconstructions $\\mathbf{x}'$, run this code a few times to see different examples.\n\nn_plot = 7\nplt.figure(figsize=(10, 4.5))\nfor i in range(n_plot):\n  idx = torch.randint(len(train_set), size=())\n  image, _ = train_set[idx]\n  # Get reconstructed image from autoencoder\n  with torch.no_grad():\n    reconstruction = lin_ae(image.unsqueeze(0)).reshape(image.size())\n\n  # Get reconstruction from PCA dimensionality reduction\n  h_pca = pca_encode(image)\n  recon_pca = pca_decode(h_pca).reshape(image.size())\n\n  plt.subplot(3, n_plot, i + 1)\n  plot_torch_image(image)\n  if i == 0:\n    plt.ylabel('Original\\nImage')\n\n  plt.subplot(3, n_plot, i + 1 + n_plot)\n  plot_torch_image(reconstruction)\n  if i == 0:\n    plt.ylabel(f'Lin AE\\n(K={K})')\n\n  plt.subplot(3, n_plot, i + 1 + 2*n_plot)\n  plot_torch_image(recon_pca)\n  if i == 0:\n    plt.ylabel(f'PCA\\n(K={K})')\nplt.show()","dbccf13c":"# @title Student Response\nfrom ipywidgets import widgets\n\n\ntext=widgets.Textarea(\n   value='Type your answer here and click on `Submit!`',\n   placeholder='Type something',\n   description='',\n   disabled=False\n)\n\nbutton = widgets.Button(description=\"Submit!\")\n\ndisplay(text,button)\n\ndef on_button_clicked(b):\n   atform.add_answer('q3' , text.value)\n   print(\"Submission successful!\")\n\n\nbutton.on_click(on_button_clicked)","f7a0054b":"class BiasLayer(nn.Module):\n  def __init__(self, shape):\n    super(BiasLayer, self).__init__()\n    init_bias = torch.zeros(shape)\n    self.bias = nn.Parameter(init_bias, requires_grad=True)\n\n  def forward(self, x):\n    return x + self.bias","b80d6609":"dummy_image = torch.rand(data_shape).unsqueeze(0)\nin_channels = data_shape[0]\nout_channels = 7\n\ndummy_conv = nn.Conv2d(in_channels=in_channels,\n                       out_channels=out_channels,\n                       kernel_size=5)\n\ndummy_deconv = nn.ConvTranspose2d(in_channels=out_channels,\n                                  out_channels=in_channels,\n                                  kernel_size=5)\n\nprint(f'Size of image is {dummy_image.shape}')\nprint(f'Size of Conv2D(image) {dummy_conv(dummy_image).shape}')\nprint(f'Size of ConvTranspose2D(Conv2D(image)) {dummy_deconv(dummy_conv(dummy_image)).shape}')","9e345eff":"class ConvAutoEncoder(nn.Module):\n  def __init__(self, x_dim, h_dim, n_filters=32, filter_size=5):\n    \"\"\"A Convolutional AutoEncoder\n\n    Args:\n      x_dim (tuple): input dimensions (channels, height, widths)\n      h_dim (int): hidden dimension, bottleneck dimension, K\n      n_filters (int): number of filters (number of output channels)\n      filter_size (int): kernel size\n    \"\"\"\n    super().__init__()\n    channels, height, widths = x_dim\n\n    # encoder input bias layer\n    self.enc_bias = BiasLayer(x_dim)\n\n    # first encoder conv2d layer (n_filters in output number of channels)\n    self.enc_conv_1 = nn.Conv2d(channels, n_filters, filter_size)\n\n    # output shape of the first encoder conv2d layer given x_dim input\n    conv_1_shape = cout(x_dim, self.enc_conv_1)\n\n    # second encoder conv2d layer\n    self.enc_conv_2 = nn.Conv2d(n_filters, n_filters, filter_size)\n\n    # output shape of the second encoder conv2d layer given conv_1_shape input\n    conv_2_shape = cout(conv_1_shape, self.enc_conv_2)\n\n    # The bottleneck is a dense layer, therefore we need a flattenning layer\n    self.enc_flatten = nn.Flatten()\n\n    # conv output shape is (depth, height, width), so the flatten size is:\n    flat_after_conv = conv_2_shape[0] * conv_2_shape[1] * conv_2_shape[2]\n\n    # encoder Linear layer\n    self.enc_lin = nn.Linear(flat_after_conv, h_dim)\n\n    ####################################################################\n    # Fill in all missing code below (...),\n    # then remove or comment the line below to test your class\n    # Remember that decoder is \"undo\"-ing what the encoder has done!\n    # raise NotImplementedError(\"Please complete the `ConvAutoEncoder` class!\")\n    ####################################################################\n    # decoder Linear layer\n    self.dec_lin = nn.Linear(h_dim, flat_after_conv)\n\n    # unflatten data to (depth, height, width) shape\n    self.dec_unflatten = nn.Unflatten(dim=-1, unflattened_size=conv_2_shape)\n\n    # first \"deconvolution\" layer\n    self.dec_deconv_1 = nn.ConvTranspose2d(n_filters, n_filters, filter_size)\n\n    # second \"deconvolution\" layer\n    self.dec_deconv_2 = nn.ConvTranspose2d(n_filters, channels, filter_size)\n\n    # decoder output bias layer\n    self.dec_bias = BiasLayer(x_dim)\n\n  def encode(self, x):\n    s = self.enc_bias(x)\n    s = F.relu(self.enc_conv_1(s))\n    s = F.relu(self.enc_conv_2(s))\n    s = self.enc_flatten(s) # flatten\n    h = self.enc_lin(s)\n    return h\n\n  def decode(self, h):\n    s = F.relu(self.dec_lin(h))\n    s = self.dec_unflatten(s)\n    s = F.relu(self.dec_deconv_1(s))\n    s = self.dec_deconv_2(s)\n    x_prime = self.dec_bias(s)\n    return x_prime\n\n  def forward(self, x):\n    return self.decode(self.encode(x))\n\n\n# add event to airtable\natform.add_event('Coding Exercise 3.2: Fill in code for the ConvAutoEncoder module')\n\nK = 20\nset_seed(seed=SEED)\n## Uncomment to test your solution\ntrained_conv_AE = ConvAutoEncoder(data_shape, K)\nassert trained_conv_AE.encode(train_set[0][0].unsqueeze(0)).numel() == K, \"Encoder output size should be K!\"\nconv_losses = train_autoencoder(trained_conv_AE, train_set, device=DEVICE, seed=SEED)\nplot_conv_ae(lin_losses, conv_losses)","1e5ef11b":"# @markdown Visualize the linear and nonlinear AE outputs\nn_plot = 7\nplt.figure(figsize=(10, 4.5))\nfor i in range(n_plot):\n  idx = torch.randint(len(train_set), size=())\n  image, _ = train_set[idx]\n  with torch.no_grad():\n    # Get reconstructed image from linear autoencoder\n    lin_recon = lin_ae(image.unsqueeze(0))[0]\n\n    # Get reconstruction from deep (nonlinear) autoencoder\n    nonlin_recon = trained_conv_AE(image.unsqueeze(0))[0]\n\n  plt.subplot(3, n_plot, i+1)\n  plot_torch_image(image)\n  if i == 0:\n    plt.ylabel('Original\\nImage')\n\n  plt.subplot(3, n_plot, i + 1 + n_plot)\n  plot_torch_image(lin_recon)\n  if i == 0:\n    plt.ylabel(f'Lin AE\\n(K={K})')\n\n  plt.subplot(3, n_plot, i + 1 + 2*n_plot)\n  plot_torch_image(nonlin_recon)\n  if i == 0:\n    plt.ylabel(f'NonLin AE\\n(K={K})')\nplt.show()","d38dbc05":"# @title Video 4: Variational Autoencoders\nfrom ipywidgets import widgets\n\nout2 = widgets.Output()\nwith out2:\n  from IPython.display import IFrame\n  class BiliVideo(IFrame):\n    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n      self.id=id\n      src = \"https:\/\/player.bilibili.com\/player.html?bvid={0}&page={1}\".format(id, page)\n      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n\n  video = BiliVideo(id=f\"BV17v411E7ye\", width=854, height=480, fs=1)\n  print(\"Video available at https:\/\/www.bilibili.com\/video\/{0}\".format(video.id))\n  display(video)\n\nout1 = widgets.Output()\nwith out1:\n  from IPython.display import YouTubeVideo\n  video = YouTubeVideo(id=f\"srWb_Gp6OGA\", width=854, height=480, fs=1, rel=0)\n  print(\"Video available at https:\/\/youtube.com\/watch?v=\" + video.id)\n  display(video)\n\nout = widgets.Tab([out1, out2])\nout.set_title(0, 'Youtube')\nout.set_title(1, 'Bilibili')\n\n# add event to airtable\natform.add_event('Video 4: Variational Autoencoders')\n\ndisplay(out)","cc078b6d":"# @markdown Train a VAE for MNIST while watching the video. (Note: this VAE has a 2D latent space. If you are feeling ambitious, edit the code and modify the latent space dimensionality and see what happens.)\nK_VAE = 2\n\n\nclass ConvVAE(nn.Module):\n  def __init__(self, K, num_filters=32, filter_size=5):\n    super(ConvVAE, self).__init__()\n\n    # With padding=0, the number of pixels cut off from each image dimension\n    # is filter_size \/\/ 2. Double it to get the amount of pixels lost in\n    # width and height per Conv2D layer, or added back in per\n    # ConvTranspose2D layer.\n    filter_reduction = 2 * (filter_size \/\/ 2)\n\n    # After passing input through two Conv2d layers, the shape will be\n    # 'shape_after_conv'. This is also the shape that will go into the first\n    # deconvolution layer in the decoder\n    self.shape_after_conv = (num_filters,\n                              data_shape[1]-2*filter_reduction,\n                              data_shape[2]-2*filter_reduction)\n    flat_size_after_conv = self.shape_after_conv[0] \\\n        * self.shape_after_conv[1] \\\n        * self.shape_after_conv[2]\n\n    # Define the recognition model (encoder or q) part\n    self.q_bias = BiasLayer(data_shape)\n    self.q_conv_1 = nn.Conv2d(data_shape[0], num_filters, 5)\n    self.q_conv_2 = nn.Conv2d(num_filters, num_filters, 5)\n    self.q_flatten = nn.Flatten()\n    self.q_fc_phi = nn.Linear(flat_size_after_conv, K+1)\n\n    # Define the generative model (decoder or p) part\n    self.p_fc_upsample = nn.Linear(K, flat_size_after_conv)\n    self.p_unflatten = nn.Unflatten(-1, self.shape_after_conv)\n    self.p_deconv_1 = nn.ConvTranspose2d(num_filters, num_filters, 5)\n    self.p_deconv_2 = nn.ConvTranspose2d(num_filters, data_shape[0], 5)\n    self.p_bias = BiasLayer(data_shape)\n\n    # Define a special extra parameter to learn scalar sig_x for all pixels\n    self.log_sig_x = nn.Parameter(torch.zeros(()))\n\n  def infer(self, x):\n    \"\"\"Map (batch of) x to (batch of) phi which can then be passed to\n    rsample to get z (like an encoder)\n    \"\"\"\n    s = self.q_bias(x)\n    s = F.relu(self.q_conv_1(s))\n    s = F.relu(self.q_conv_2(s))\n    flat_s = s.view(s.size()[0], -1)\n    phi = self.q_fc_phi(flat_s)\n    return phi\n\n  def generate(self, zs):\n    \"\"\"Map [b,n,k] sized samples of z to [b,n,p] sized images\n    \"\"\"\n    # Note that for the purposes of passing through the generator, we need\n    # to reshape zs to be size [b*n,k]\n    b, n, k = zs.size()\n    s = zs.view(b*n, -1)\n    s = F.relu(self.p_fc_upsample(s)).view((b*n,) + self.shape_after_conv)\n    s = F.relu(self.p_deconv_1(s))\n    s = self.p_deconv_2(s)\n    s = self.p_bias(s)\n    mu_xs = s.view(b, n, -1)\n    return mu_xs\n\n  def decode(self, zs):\n    # Included for compatability with conv-AE code\n    return self.generate(zs.unsqueeze(0))\n\n  def forward(self, x):\n    # VAE.forward() is not used for training, but we'll treat it like a\n    # classic autoencoder by taking a single sample of z ~ q\n    phi = self.infer(x)\n    zs = rsample(phi, 1)\n    return self.generate(zs).view(x.size())\n\n  def elbo(self, x, n=1):\n    \"\"\"\n    Run input end to end through the VAE and compute the ELBO using n\n    samples of z\n    \"\"\"\n    phi = self.infer(x)\n    zs = rsample(phi, n)\n    mu_xs = self.generate(zs)\n    return log_p_x(x, mu_xs, self.log_sig_x.exp()) - kl_q_p(zs, phi) # P(z|x)?!\n\n\ndef expected_z(phi):\n  return phi[:, :-1]\n\n\ndef rsample(phi, n_samples):\n  \"\"\"\n  Sample z ~ q(z;phi)\n  Ouput z is size [b,n_samples,K] given phi with shape [b,K+1]. The first K\n  entries of each row of phi are the mean of q, and phi[:,-1] is the log\n  standard deviation\n  \"\"\"\n  b, kplus1 = phi.size()\n  k = kplus1-1\n  mu, sig = phi[:, :-1], phi[:,-1].exp()\n  eps = torch.randn(b, n_samples, k, device=phi.device)\n  return eps*sig.view(b,1,1) + mu.view(b,1,k)\n\n\ndef train_vae(vae, dataset, epochs=10, n_samples=1000):\n  opt = torch.optim.Adam(vae.parameters(), lr=1e-3, weight_decay=0)\n  elbo_vals = []\n  vae.to(DEVICE)\n  vae.train()\n  loader = DataLoader(dataset, batch_size=250, shuffle=True, pin_memory=True)\n  for epoch in trange(epochs, desc='Epochs'):\n    for im, _ in tqdm(loader, total=len(dataset) \/\/ 250, desc='Batches', leave=False):\n      im = im.to(DEVICE)\n      opt.zero_grad()\n      loss = -vae.elbo(im)\n      loss.backward()\n      opt.step()\n\n      elbo_vals.append(-loss.item())\n  vae.to('cpu')\n  vae.eval()\n  return elbo_vals\n\n\ntrained_conv_VarAE = ConvVAE(K=K_VAE)\nelbo_vals = train_vae(trained_conv_VarAE, train_set, n_samples=10000)\n\nprint(f'Learned sigma_x is {torch.exp(trained_conv_VarAE.log_sig_x)}')\n\n# Uncomment below if you'd like to see the the training\n# curve of the evaluated ELBO loss function\n# ELBO is the loss function used to train VAEs (see lecture!)\nplt.figure()\nplt.plot(elbo_vals)\nplt.xlabel('Batch #')\nplt.ylabel('ELBO')\nplt.show()","0f413770":"def generate_images(autoencoder, K, n_images=1):\n  \"\"\"\n  Generate n_images 'new' images from the decoder part of the given\n  autoencoder.\n\n  returns (n_images, channels, height, width) tensor of images\n  \"\"\"\n  # Concatenate tuples to get (n_images, channels, height, width)\n  output_shape = (n_images,) + data_shape\n  with torch.no_grad():\n    ####################################################################\n    # Fill in all missing code below (...),\n    # then remove or comment the line below to test your function\n    # raise NotImplementedError(\"Please complete the `generate_images` function!\")\n    ####################################################################\n    # sample z from a unit gaussian, pass through autoencoder.decode()\n    z = torch.randn(n_images, K)\n    x = autoencoder.decode(z) \n    return x.reshape(output_shape)\n\n\n# add event to airtable\natform.add_event('Coding Exercise 4.2: Generating images')\n\nset_seed(seed=SEED)\n## Uncomment to test your solution\nimages = generate_images(trained_conv_AE, K, n_images=9)\nplot_images(images, plt_title='Images Generated from the Conv-AE')\nimages = generate_images(trained_conv_VarAE, K_VAE, n_images=9*2)\nplot_images(images, plt_title='Images Generated from a Conv-Variational-AE')","19bd5b91":"# @title Video 5: State-Of-The-Art VAEs\nfrom ipywidgets import widgets\n\nout2 = widgets.Output()\nwith out2:\n  from IPython.display import IFrame\n  class BiliVideo(IFrame):\n    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n      self.id=id\n      src = \"https:\/\/player.bilibili.com\/player.html?bvid={0}&page={1}\".format(id, page)\n      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n\n  video = BiliVideo(id=f\"BV1hg411M7KY\", width=854, height=480, fs=1)\n  print(\"Video available at https:\/\/www.bilibili.com\/video\/{0}\".format(video.id))\n  display(video)\n\nout1 = widgets.Output()\nwith out1:\n  from IPython.display import YouTubeVideo\n  video = YouTubeVideo(id=f\"PXBl3KwRfh4\", width=854, height=480, fs=1, rel=0)\n  print(\"Video available at https:\/\/youtube.com\/watch?v=\" + video.id)\n  display(video)\n\nout = widgets.Tab([out1, out2])\nout.set_title(0, 'Youtube')\nout.set_title(1, 'Bilibili')\n\n# add event to airtable\natform.add_event('Video 5: State-Of-The-Art VAEs')\n\ndisplay(out)","7fb2eb17":"# @title Airtable Submission Link\nfrom IPython import display as IPydisplay\nIPydisplay.HTML(\n   f\"\"\"\n <div>\n   <a href= \"{atform.url()}\" target=\"_blank\">\n   <img src=\"https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/blob\/main\/tutorials\/static\/AirtableSubmissionButton.png?raw=1\"\n alt=\"button link to Airtable\" style=\"width:410px\"><\/a>\n   <\/div>\"\"\" )","7ffdf815":"[`torch.nn.ConvTranspose2d`](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.ConvTranspose2d.html) module can be seen as the gradient of `Conv2d` with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation). The following code demonstrates this change in sizes:","59c95f65":"## Section 1.1: Generating Images from BigGAN\n\nTo demonstrate the power of generative models, we are giving you a sneak peek of a fully trained generative model called BigGAN. You\u2019ll see it again (with more background under your belt) later today. For now, let\u2019s just focus on BigGAN as a generative model. Specifically, BigGAN is a class conditional generative model for 128 x 128 images. The classes are based on categorical labels that describe the images and images are generated based upon a vector (z from the video lecture) and the probability that the image comes from a specific discrete category.\n\nFor now, don\u2019t worry about the specifics of the model other than the fact that it generates images based on the vector and the category label.\n\nTo explore the space of generated images, we\u2019ve provided you with a widget that allows you to select a category label and to alter the value of the vector. The vector is a 128-D, which may seem high dimensional, but is much lower-dimensional than a 128 x 128 image! To simplify usability the widget limits the magnitude of the vector and constrains all entries to be equal (so you are only exploring a subset of the possible images that can be generated).","a647a262":"### Select a dataset\n\nWe've built today's tutorial to be flexible. It should work more-or-less out of the box with both MNIST and CIFAR (and other image datasets). MNIST is in many ways simpler, and the results will likely look better and run a bit faster if using MNIST. But we are leaving it up to you to pick which one you want to experiment with!\n\nWe encourage pods to coordinate so that some members use MNIST and others use CIFAR10. Keep in mind that the CIFAR dataset may require more learning epochs (longer training required).","07436fdc":"### Coding Exercise 3.1: Linear AutoEncoder Architecture\n\nComplete the missing parts of the `LinearAutoEncoder` class.\n\nThe `LinearAutoEncoder` as two stages: an `encoder` which linearly maps from inputs of size `x_dim = my_dataset_dim` to a hidden layer of size `h_dim = K` (with no nonlinearity), and a `decoder` which maps back from `K` up to the number of pixels in each image.","ec81ea9f":"## Section 3.2: Building a nonlinear convolutional autoencoder","2b4ab44b":"[*Click for solution*](https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/tree\/main\/\/tutorials\/W2D5_GenerativeModels\/solutions\/W2D5_Tutorial1_Solution_c78a3327.py)\n\n*Example output:*\n\n<img alt='Solution hint' align='left' width=1119.0 height=832.0 src=https:\/\/raw.githubusercontent.com\/NeuromatchAcademy\/course-content-dl\/main\/tutorials\/W2D5_GenerativeModels\/static\/W2D5_Tutorial1_Solution_c78a3327_3.png>\n\n","b0ec72c0":"---\n# Setup","df7cab11":"### Think! 4.2: AutoEncoders vs. Variational AutoEncoders\n\nCompare the images generated by the AutoEncoder to the images generated by the Variational AutoEncoder. You can run the code a few times to see a variety of examples.\n\nDoes one set look more like the training set (handwritten digits) than the other? What is driving this difference?","f39c43f6":"### Comparison to PCA\n\nOne way to think about AutoEncoders is as a form of dimensionality-reduction. The dimensionality of $\\mathbf{h}$ is much smaller than the dimensionality of $\\mathbf{x}$.\n\nAnother common technique for dimensionality reduction is to project data onto the top $K$ **principal components** (Principal Component Analysis or PCA). For comparison, let's also apply PCA for dimensionality reduction.","a5878796":"**Please** run the cell after the video to train a VAE for MNIST while watching it.","f21f12ae":"The first choice to make is the dimensionality of $\\mathbf{h}$. We'll see more on this below, but For MNIST, 5 to 20 is plenty. For CIFAR, we need more like 50 to 100 dimensions.\n\nCoordinate with your pod to try a variety of values for $K$ in each dataset so you can compare results.","116e5ad7":"# Tutorial 1: Variational Autoencoders (VAEs)\n\n**Week 2, Day 5: Generative Models**\n\n**By Neuromatch Academy**\n\n__Content creators:__ Saeed Salehi, Spiros Chavlis, Vikash Gilja\n\n__Content reviewers:__ Diptodip Deb, Kelson Shilling-Scrivo\n\n__Content editor:__ Charles J Edelson, Spiros Chavlis \n\n__Production editors:__ Saeed Salehi, Spiros Chavlis \n\n*Inspired from UPenn course*:\n__Instructor:__ Konrad Kording, __Original Content creators:__ Richard Lange, Arash Ash","6d0ae683":"### Think! 3.1: PCA vs. Linear autoenconder\n\nCompare the PCA-based reconstructions to those from the linear autoencoder. Is one better than the other? Are they equally good? Equally bad? How does the choice of $K$ impact reconstruction quality?","0a8e9499":"---\n# Section 5: State of the art VAEs and Wrap-up (Bonus)","0bd844e7":"Let's visualize some of the reconstructions ($\\mathbf{x'}$) side-by-side with the input images ($\\mathbf{x}$).","e39ce357":"[*Click for solution*](https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/tree\/main\/\/tutorials\/W2D5_GenerativeModels\/solutions\/W2D5_Tutorial1_Solution_2fa496ef.py)\n\n*Example output:*\n\n<img alt='Solution hint' align='left' width=1120.0 height=832.0 src=https:\/\/raw.githubusercontent.com\/NeuromatchAcademy\/course-content-dl\/main\/tutorials\/W2D5_GenerativeModels\/static\/W2D5_Tutorial1_Solution_2fa496ef_0.png>\n\n","8c9b25af":"## Coding Exercise 2: pPCA (Bonus)\n\nAssume we have two noisy thermometers measuring the temperature of the same room. They both make noisy measurements.  The room tends to be around 25&deg;C (that's 77&deg;F), but can vary around that temperature.  If we take lots of readings from the two thermometers over time and plot the paired readings, we might see something like the plot generated below:","1db83437":"---\n# Section 3: Autoencoders\n\n*Time estimate: ~30mins*","c9eda3a5":"With that out of the way, we will next define a **nonlinear** and **convolutional** autoencoder. Here's a quick tour of the architecture:\n\n1. The **encoder** once again maps from images to $\\mathbf{h}\\in\\mathbb{R}^K$. This will use a `BiasLayer` followed by two convolutional layers (`nn.Conv2D`), followed by flattening and linearly projecting down to $K$ dimensions. The convolutional layers will have `ReLU` nonlinearities on their outputs. \n1. The **decoder** inverts this process, taking in vectors of length $K$ and outputting images. Roughly speaking, its architecture is a \"mirror image\" of the encoder: the first decoder layer is linear, followed by two **deconvolution** layers ([`ConvTranspose2d`](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.ConvTranspose2d.html)). The `ConvTranspose2d`layers will have `ReLU` nonlinearities on their _inputs_. This \"mirror image\" between the encoder and decoder is a useful and near-ubiquitous convention. The idea is that the decoder can then learn to approximately invert the encoder, but it is not a strict requirement (and it does not guarantee the decoder will be an exact inverse of the encoder!).\n\nBelow is a schematic of the architecture for MNIST. Notice that the width and height dimensions of the image planes reduce after each `nn.Conv2d` and increase after each `nn.ConvTranspose2d`. With CIFAR10, the architecture is the same but the exact sizes will differ.\n\n<img src=\"https:\/\/raw.githubusercontent.com\/NeuromatchAcademy\/course-content-dl\/main\/tutorials\/W2D5_GenerativeModels\/static\/conv_sizes.png\">","0d715baf":"You should see that the `ConvAutoEncoder` achieved lower MSE loss than the linear one. If not, you may need to retrain it (or run another few training epochs from where it left off). We make fewer guarantees on this working with CIFAR10, but it should definitely work with MNIST.\n\nNow let's visually compare the reconstructed images from the linear and nonlinear autoencoders. Keep in mind that both have the same dimensionality for $\\mathbf{h}$!","05aca92d":"**Step 2**: \"Generate\" from the pPCA model of the thermometer data.\n\nComplete the code so it properly generates from the pPCA model. At present we aren't accounting for the \"sensor noise\" the sensor noise is the variance that the PC axes isn't accounting for. Thus, you'll note that the current output sits on the PC axes and doesn't look like the original data distribution!!\n\nHere is the equation for sampling from the pPCA model:\n\n\\begin{equation}\nx = \\mu + W z + \\epsilon, \\,\\text{where}\\,~~ \\epsilon \\sim \\mathcal{N}(0,~\\sigma^2 \\mathbf{I})\n\\end{equation}","a8cc75dc":"---\n# Section 1: Generative models\n\n*Time estimate: ~15mins*","105a2264":"**Step 1:** Calculate the parameters of the pPCA model\n\nThis part is completed already, so you don't need to make any edits:","396957fc":"Let\u2019s model these data with a single principal component. Given that the thermometers are measuring the same actual temperature, the principal component axes will be the identity line. The direction of this axes can be indicated by the unit vector $[1 ~~ 1]~\/~\\sqrt2$.  We could estimate this axes by applying PCA. We can plot this axes, it tells us something about the data, but we can\u2019t generate from it:","5fd58d09":"Now we'll create our first autoencoder. It will reduce images down to $K$ dimensions. The architecture will be quite simple: the input will be linearly mapped to a single hidden (or latent) layer $\\mathbf{h}$ with $K$ units, which will then be linearly mapped back to an output that is the same size as the input:\n\n\\begin{equation}\n\\mathbf{x} \\longrightarrow \\mathbf{h} \\longrightarrow \\mathbf{x'}\n\\end{equation}\n\nThe loss function we'll use will simply be mean squared error (MSE) quantifying how well the reconstruction ($\\mathbf{x'}$) matches the original image ($\\mathbf{x}$):\n\n\\begin{equation}\n\\text{MSE Loss} = \\sum_{i=1}^{N} ||\\mathbf{x}_i - \\mathbf{x'}_i||^2_2\n\\end{equation}\n\nIf all goes well, then the AutoEncoder will learn, **end to end**, a good \"encoding\" or \"compression\" of inputs to a latent representation ($\\mathbf{x \\longrightarrow h}$) as well as a good \"decoding\" of that latent representation to a reconstruction of the original input ($\\mathbf{h \\longrightarrow x'}$).","641b4341":"[*Click for solution*](https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/tree\/main\/\/tutorials\/W2D5_GenerativeModels\/solutions\/W2D5_Tutorial1_Solution_1624cb07.py)\n\n*Example output:*\n\n<img alt='Solution hint' align='left' width=1119.0 height=832.0 src=https:\/\/raw.githubusercontent.com\/NeuromatchAcademy\/course-content-dl\/main\/tutorials\/W2D5_GenerativeModels\/static\/W2D5_Tutorial1_Solution_1624cb07_3.png>\n\n","109b130c":"**Please** run the cell after the video to download MNIST and CIFAR10 image datasets while the video plays.","ee2ece01":"[*Click for solution*](https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/tree\/main\/\/tutorials\/W2D5_GenerativeModels\/solutions\/W2D5_Tutorial1_Solution_1f5f1241.py)\n\n*Example output:*\n\n<img alt='Solution hint' align='left' width=826.0 height=886.0 src=https:\/\/raw.githubusercontent.com\/NeuromatchAcademy\/course-content-dl\/main\/tutorials\/W2D5_GenerativeModels\/static\/W2D5_Tutorial1_Solution_1f5f1241_1.png>\n\n<img alt='Solution hint' align='left' width=855.0 height=886.0 src=https:\/\/raw.githubusercontent.com\/NeuromatchAcademy\/course-content-dl\/main\/tutorials\/W2D5_GenerativeModels\/static\/W2D5_Tutorial1_Solution_1f5f1241_2.png>\n\n","a0f20c0a":"---\n# Section 2: Latent Variable Models\n\n*Time estimate: ~15mins* excluding the Bonus","d44701b7":"---\n# Summary\nThrough this tutorial, we have learned\n- What a generative model is and why we are interested in them.\n- How latent variable models relate to generative models with the example of pPCA.\n- What a basic AutoEncoder is and how they relate to other latent variable models.\n- The basics of Variational AutoEncoders and how they function as generative models.\n- An introduction to the broad applications of VAEs.\n\nIn the next two tutorials we will cover GANs and how to train them.","c57bbb91":"---\n# Tutorial Objectives\nIn the first tutorial of the *Generative Models* day, we are going to\n\n- Think about unsupervised learning \/ Generative Models and get a bird's eye view of why it is useful\n- Build intuition about latent variables\n- See the connection between AutoEncoders and PCA\n- Start thinking about neural networks as generative models by contrasting AutoEncoders and Variational AutoEncoders","8b6a3730":"<a href=\"https:\/\/colab.research.google.com\/github\/NeuromatchAcademy\/course-content-dl\/blob\/main\/tutorials\/W2D5_GenerativeModels\/student\/W2D5_Tutorial1.ipynb\" target=\"_parent\"><img src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\" alt=\"Open In Colab\"\/><\/a> &nbsp; <a href=\"https:\/\/kaggle.com\/kernels\/welcome?src=https:\/\/raw.githubusercontent.com\/NeuromatchAcademy\/course-content-dl\/main\/tutorials\/W2D5_GenerativeModels\/student\/W2D5_Tutorial1.ipynb\" target=\"_parent\"><img src=\"https:\/\/kaggle.com\/static\/images\/open-in-kaggle.svg\" alt=\"Open in Kaggle\"\/><\/a>","34d3cb8a":"## Section 3.1: Conceptual introduction to AutoEncoders","4f2d6364":"## Section 4.2: Generating novel images from the decoder\n\nIf we isolate the decoder part of the AutoEncoder, what we have is a neural network that takes as input a vector of size $K$ and produces as output an image that looks something like our training data. Recall that in our earlier notation, we had an input $\\mathbf{x}$ that was mapped to a low-dimensional hidden representation $\\mathbf{h}$ which was then decoded into a reconstruction of the input, $\\mathbf{x'}$:\n\n\\begin{equation}\n\\mathbf{x} \\overset{\\text{encode}}{\\longrightarrow} \\mathbf{h} \\overset{\\text{decode}}{\\longrightarrow} \\mathbf{x'}\\, .\n\\end{equation}\n\nPartly as a matter of convention, and partly to distinguish where we are going next from the previous section, we're going to introduce a new variable, $\\mathbf{z} \\in \\mathbb{R}^K$, which will take the place of $\\mathbf{h}$. The key difference is that while $\\mathbf{h}$ is produced by the encoder for a particular $\\mathbf{x}$, $\\mathbf{z}$ will be drawn out of thin air from a prior of our choosing:\n\n\\begin{equation}\n\\mathbf{z} \\sim p(\\mathbf{z})\\\\ \\mathbf{z} \\overset{\\text{decode}}{\\longrightarrow} \\mathbf{x}\\, .\n\\end{equation}\n\n(Note that it is also common convention to drop the \"prime\" on $\\mathbf{x}$ when it is no longer being thought of as a \"reconstruction\").","57ef0cfb":"## Section 4.1: Components of a VAE\n\n*Recognition models and density networks*\n\n\nVariational AutoEncoders (VAEs) are a lot like the classic AutoEncoders (AEs), but where we explicitly think about probability distributions. In the language of VAEs, the __encoder__ is replaced with a __recognition model__, and the __decoder__ is replaced with a __density network__.\n\nWhere in a classic autoencoder the encoder maps from images to a single hidden vector,\n\n\\begin{equation}\n\\mathbf{x} \\overset{\\text{AE}}{\\longrightarrow} \\mathbf{h} \\, ,\n\\end{equation}\n\nin a VAE we would say that a recognition model maps from inputs to entire __distributions__ over hidden vectors,\n\n\\begin{equation}\n\\mathbf{x} \\overset{\\text{VAE}}{\\longrightarrow} q_{\\mathbf{w_e}}(\\mathbf{z}) \\, ,\n\\end{equation}\n\nwhich we will then sample from. Here $\\mathbf{w_e}$ refers to the weights of the recognition model, which parametarize our distribution generating network. We'll say more in a moment about what kind of distribution $q_{\\mathbf{w_e}}(\\mathbf{z})$ is.\nPart of what makes VAEs work is that the loss function will require good reconstructions of the input not just for a single $\\mathbf{z}$, but _on average_ from samples of $\\mathbf{z} \\sim q_{\\mathbf{w_e}}(\\mathbf{z})$.\n\nIn the classic autoencoder, we had a decoder which maps from hidden vectors to reconstructions of the input:\n\n\\begin{equation}\n\\mathbf{h} \\overset{\\text{AE}}{\\longrightarrow} \\mathbf{x'} \\, .\n\\end{equation}\n\nIn a density network, reconstructions are expressed in terms of a distribution:\n\n\\begin{equation}\n\\mathbf{z} \\overset{\\text{VAE}}{\\longrightarrow} p_{\\mathbf{w_d}}(\\mathbf{x}|\\mathbf{z})\n\\end{equation}\n\nwhere, as above, $p_{\\mathbf{w_d}}(\\mathbf{x}|\\mathbf{z})$ is defined by mapping $\\mathbf{z}$ through a density network then treating the resulting $f(\\mathbf{z};\\mathbf{w_d})$ as the mean of a (Gaussian) distribution over $\\mathbf{x}$. Similarly, our reconstruction distribution is parametarized by the weights of the density network.","73ad24bb":"## Section 1.2: Interpolating Images with BigGAN\nThis next widget allows you to interpolate between two generated images. It does this by linearly interpolating between the probability of each category you select and linearly interpolating between the latent vector values.","ed5651bb":"### Think! 1.1: Generated images\n\nAs you alter the magnitude of the vector, what do you note about the relationship between the different generated images? What do you note about the relationship between the image and the category label as you increase the magnitude of the vector?","1330aec5":"**Our 2021 Sponsors, including Presenting Sponsor Facebook Reality Labs**\n\n<p align='center'><img src='https:\/\/github.com\/NeuromatchAcademy\/widgets\/blob\/master\/sponsors.png?raw=True'\/><\/p>","c4c40980":"### Coding Exercise 3.2: Fill in code for the `ConvAutoEncoder` module\n\nComplete the `ConvAutoEncoder` class. We use the helper function `cout(torch.Tensor, nn.Conv2D)` to calculate the output shape of a [`nn.Conv2D`](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.Conv2d.html) layer given a tensor with shape (channels, height, width).","c9f1f88a":"In the video, the concept of a latent variable model was introduced. We saw how PCA (principal component analysis) can be extended into a generative model with latent variables called pPCA. For pPCA the latent variables (z in the video) are the projections onto the principal component axes. \n\nThe dimensionality of the principal components is typically set to be substantially lower-dimensional than the original data. Thus, the latent variables (the projection onto the principal component axes) are a lower-dimensional representation of the original data (dimensionality reduction!). With pPCA we can estimate the original distribution of the high dimensional data. This allows us to generate data with a distribution that \u201clooks\u201d more like the original data than if we were to only use PCA to generate data from the latent variables. Let\u2019s see how that might look with a simple example. ","365a692b":"---\n# Section 4: Variational Auto-Encoders (VAEs)\n\n*Time estimate: ~25mins*","fe2e975e":"### Coding Exercise 4.2: Generating images","0d769bf4":"**Nonlinear:** We'd like to apply autoencoders to learn a more flexible nonlinear mapping between the latent space and the images. Such a mapping can provide a more \"expressive\" model that better describes the image data than a linear mapping. This can be achieved by adding nonlinear activation functions to our encoder and decoder!\n\n**Convolutional:** As you saw on the day dedicated to RNNs and CNNs, parameter sharing is often a good idea for images! It's quite common to use convolutional layers in autoencoders to share parameters across locations in the image.\n\n**Side Note:** The `nn.Linear` layer (used in the linear autoencoder above) has a \"bias\" term, which is a learnable offset parameter separate for each output unit. Just like PCA \"centers\" the data by subtracting off the mean image (`mu`) before encoding and adds the average back in during decoding, a bias term in the decoder can effectively account for the first moment (mean) of the data (i.e. the average of all images in the training set). Convolution layers do have bias parameters, but the bias is applied per filter rather than per pixel location. If we're generating grayscale images (like those in MNIST), then `Conv2d` will learn only one bias across the entire image.\n\nFor some conceptual continuity with both PCA and the `nn.Linear` layers above, the next block defines a custom `BiasLayer` for adding a learnable per-pixel offset. This custom layer will be used twice: as the first stage of the encoder and as the final stage of the decoder. Ideally, this means that the rest of the neural net can focus on fitting more interesting fine-grained structure.","f2b476c5":"**Please** run the cell after the video to download BigGAN (a generative model) and a few standard image datasets while the video plays.","44306cbb":"### Think! 1.2: Interpolating samples from the same category\n\nTry interpolating between samples from the same category, samples from similar categories, and samples from very different categories. Do you notice any trends? What does this suggest about the representations of images in the latent space?"}}