{"cell_type":{"5340e357":"code","73a10bb9":"code","80550c88":"code","afd112f0":"code","05ae6de2":"code","5d1912ae":"code","eb5dca30":"code","5bf46ee5":"code","758e7330":"code","3f538e98":"code","f8fa3558":"code","373a6552":"code","a1188290":"code","cd17fa14":"code","119d4c5e":"code","7b32af33":"code","44661a89":"code","48d111e1":"code","193f2ada":"code","3d612897":"code","390f7979":"code","768fa989":"code","bdac3a65":"code","cc702b36":"code","657c6092":"code","d433432b":"code","9041bfa1":"code","c942fdc1":"code","a67d24c7":"markdown","6f4e9d5a":"markdown","7c8a92a1":"markdown","2eeaf721":"markdown","084e0cb0":"markdown","55a72e75":"markdown","9e4f619b":"markdown","196c451c":"markdown","1f143c32":"markdown","f7851269":"markdown","4c3ae504":"markdown","201b88aa":"markdown","eef5ddad":"markdown","6ad29e1d":"markdown","656514e2":"markdown","2a134aaa":"markdown","718ee5aa":"markdown"},"source":{"5340e357":"import pandas as pd","73a10bb9":"train = pd.read_csv(\"..\/input\/uk-housing-prices-paid\/price_paid_records.csv\")","80550c88":"train.info(memory_usage=\"deep\")","afd112f0":"del train","05ae6de2":"dtypes = {\n    \"Transaction unique identifier\": \"category\",\n    \"Price\": \"int32\",\n    \"Property Type\": \"category\",\n    \"Old\/New\": \"category\",\n    \"Duration\": \"category\",\n    \"Town\/City\": \"category\",\n    \"District\": \"category\",\n    \"Country\": \"category\",\n    \"County\": \"category\",\n    \"PPDCategory Type\": \"category\",\n    \"Record Status - monthly file only\": \"category\"\n}","5d1912ae":"\ntrain = pd.read_csv(\"..\/input\/uk-housing-prices-paid\/price_paid_records.csv\", dtype=dtypes)\ntrain[\"Date of Transfer\"] = pd.to_datetime(train[\"Date of Transfer\"])","eb5dca30":"train.info(memory_usage=\"deep\")","5bf46ee5":"def downcast_dtypes(df):\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols = [c for c in df if df[c].dtype in [\"int64\", \"int32\"]]\n    object_cols = [ c for c in df if df[c].dtype =='object']\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols] = df[int_cols].astype(np.int16)\n    df[object_cols] = df[object_cols].astype(\"category\")\n    return df","758e7330":"del train","3f538e98":"data_chunk=pd.read_csv('..\/input\/uk-housing-prices-paid\/price_paid_records.csv',chunksize=100000)\ndata_chunk","f8fa3558":"#storing the chunks after iterating from chunk object\nchunk_data=[chunk for chunk in data_chunk]\n\n#concatnating dataframes to make it a complete dataset\ntrain=pd.concat(chunk_data)","373a6552":"train.info()","a1188290":"del train","cd17fa14":"# import gc\n# gc.collect()","119d4c5e":"import dask.dataframe as dd","7b32af33":"train = dd.read_csv(\"..\/input\/uk-housing-prices-paid\/price_paid_records.csv\", dtype=dtypes)","44661a89":"train.info(memory_usage=\"deep\")","48d111e1":"train.Price.mean().compute()","193f2ada":"train.Price[train.Price>10000].compute()","3d612897":"# Install datatable\n\n# !pip install datatable","390f7979":"import datatable as dt","768fa989":"train = dt.fread(\"..\/input\/uk-housing-prices-paid\/price_paid_records.csv\")","bdac3a65":"# !pip3 install cudf","cc702b36":"%%time\nimport cudf\ntrain = cudf.read_csv('..\/input\/uk-housing-prices-paid\/price_paid_records.csv')","657c6092":"train.info()","d433432b":"import gc \ngc.collect()","9041bfa1":" debug = True\nif debug:\n    train = pd.read_csv('..\/input\/uk-housing-prices-paid\/price_paid_records.csv',nrows=10000 , dtype=dtypes)\nelse:\n    train = pd.read_csv('..\/input\/uk-housing-prices-paid\/price_paid_records.csv', dtype=dtypes) ","c942fdc1":"train.to_pickle(\"train.pkl\")","a67d24c7":"Today , we have to deal with large amount of data ,you may have seen that most of the kaggle competitions dataset size is huge ( some are of even 20GB or 50 GB), and obviously we don't have that much computation power and reading data with normal  ```read_csv``` using padas is not feasible .So, many *AI-researchers\/Kagglers\/Data-scientists* come up with different techniques to dealing with large datasets.\n\nSo today , we are gonna discuss these techniques.\nLets get started!!","6f4e9d5a":"![image.png](attachment:121b64cc-1dc9-4d73-b85a-5f10711b60cd.png)!","7c8a92a1":"For more information - ","2eeaf721":"# How to deal with large dataset (tabular data)?\n\n","084e0cb0":"## Method 5 : Using cuDF","55a72e75":"## References - \n1. https:\/\/www.kaggle.com\/elvinagammed\/deal-with-large-dfs-reduce-the-size-of-df-to-36?scriptVersionId=43054943\n2. https:\/\/www.kaggle.com\/rohanrao\/tutorial-on-reading-large-datasets","9e4f619b":"Use when - \n* Manipulating large datasets, even when those datasets don\u2019t fit in memory\n* Accelerating long computations by using many cores\n* Distributed computing on large datasets with standard Pandas operations like groupby, join, and time series computations\n","196c451c":"## Method 3 - Use Dask","1f143c32":"## Method 6: Always delete unused variables ( this will save lot of space )","f7851269":"## Method 4 : Using Datatable ","4c3ae504":"## Method 1 - Changing the size of datatypes","201b88aa":"## Method 7: Use Debug mode when doing feature engineering","eef5ddad":"## Method 2 - Reading data in chunks ","6ad29e1d":"\nDatatable is a python library for manipulating tabular data. It supports out-of-memory datasets, multi-threaded data processing, and flexible API. \n\n- Similar to the R\u2019s data.table\n- It is a toolkit for performing big data (up to 100GB) operations on a single-node machine, at the maximum possible speed.\n\n","656514e2":"cuDf is GPU equivalent to pandas . cuDF is a package within the RAPIDS ecosystem that allows data scientists to easily migrate their existing Pandas workflows from CPU to GPU, where computations can leverage the immense parallelization that GPUs provide.","2a134aaa":"## Method 8: Saving dataframes\/objects as pickle files for faster loading","718ee5aa":"\n##### - **Dask** is a flexible library for parallel computing in Python .\n##### - **Dask** = Features of pandas +  ( performance and scalability) .\n\n"}}