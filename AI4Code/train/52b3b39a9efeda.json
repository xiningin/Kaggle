{"cell_type":{"e88d991f":"code","4b6dfa5c":"code","792f8871":"code","25486472":"code","54a47607":"code","6bbe7c64":"code","0cd2c6b5":"code","109ca207":"code","a8582ec7":"code","2040743f":"code","10246912":"code","3e7d53a9":"code","cda81074":"code","2fdce195":"code","3ec7b2be":"code","e0582318":"markdown","e0732307":"markdown","5c69b4e5":"markdown","8ed9adc2":"markdown","63387371":"markdown","538db4b9":"markdown","9e842275":"markdown","e9cc9ade":"markdown","9ec40eee":"markdown","7f0fcd2c":"markdown","043df05b":"markdown","af7946bf":"markdown","dd0b729f":"markdown"},"source":{"e88d991f":"import pandas as pd\npd.set_option('display.max_columns', None)\n\nimport numpy as np\nfrom datetime import datetime as dt\nimport seaborn as sns","4b6dfa5c":"path='..\/input\/ble-rssi-dataset\/iBeacon_RSSI_Labeled.csv'\ndata = pd.read_csv(path, index_col=None)\ndata.head(5)","792f8871":"plots = data.hist(bins=15, figsize=(20,20))\n\nfor ax in plots.flatten():\n    ax.set_xlabel(\"Signal Strength\")\n    ax.set_ylabel(\"count\")","25486472":"import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10,7))\nsns.heatmap(data.corr(method='kendall'), ax=ax)","54a47607":"data.max()","6bbe7c64":"label = [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\"]\nfor col in data.select_dtypes(include=\"number\").columns:\n    data[col] = pd.cut(data[col], bins = 15, labels = label)\n    \n# Splitting the location:\ndata['x'] = data['location'].str[0]\ndata['y'] = data['location'].str[1:]\n\n# Label Encoding\nfrom sklearn.preprocessing import LabelEncoder\ndata['x'] = LabelEncoder().fit_transform(data['x'])\ndata['y'] = LabelEncoder().fit_transform(data['y'])\n\n# Dropping the columns\ndata = data.drop(columns=[\"date\",\"location\"])\n\ndata.head(5)","0cd2c6b5":"data = pd.get_dummies(data, columns=data.columns[0:-2])\ndata.head(5)","109ca207":"target_x = data['x']\ntarget_y = data['y']\ndata.drop(columns=['x','y'], inplace=True)\ndata.head(5)","a8582ec7":"from sklearn.model_selection import train_test_split\n\nD_train, D_test, t_train_x, t_test_x = train_test_split(data, \n                                                    target_x, \n                                                    test_size = 0.3,\n                                                    random_state=999)\n\nD_train, D_test, t_train_y, t_test_y = train_test_split(data, \n                                                    target_y, \n                                                    test_size = 0.3,\n                                                    random_state=999)","2040743f":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import RepeatedStratifiedKFold, GridSearchCV\n\nk_fold_method = RepeatedStratifiedKFold(n_splits=5, \n                                    n_repeats=3, \n                                    random_state=8)\n\n################################## KNN #####################################################\n\nparameters_knn = {'n_neighbors': [1, 2, 3, 4, 5, 6, 7,8,9,10,11,12,13,14,15], \n              'p': [1, 2, 5]}\n\nknn = KNeighborsClassifier()\n\ngs_knn_x = GridSearchCV(estimator=knn, \n                      param_grid=parameters_knn, \n                      cv=k_fold_method,\n                      verbose=1, \n                      n_jobs=-2,\n                      scoring='accuracy',\n                      return_train_score=True)\n\ngs_knn_y = GridSearchCV(estimator=knn, \n                      param_grid=parameters_knn, \n                      cv=k_fold_method,\n                      verbose=1, \n                      n_jobs=-2,\n                      scoring='accuracy',\n                      return_train_score=True)\n\n################################### DT ########################################################\n\nparameters_dt = {'criterion':['gini','entropy'],'max_depth':[2,3,4]}\n\ndt = DecisionTreeClassifier()\n\ngs_dt_y = GridSearchCV(estimator=dt,\n                    param_grid=parameters_dt,\n                    cv = k_fold_method,\n                    verbose=1,\n                    n_jobs=-2,\n                    scoring='accuracy',\n                    return_train_score=True)\n\ngs_dt_x = GridSearchCV(estimator=dt,\n                    param_grid=parameters_dt,\n                    cv = k_fold_method,\n                    verbose=1,\n                    n_jobs=-2,\n                    scoring='accuracy',\n                    return_train_score=True)\n\n####################################### SVC ####################################################\n\nparameters_svc = {'C': [0.1, 1, 10, 100, 1000],  \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n              'kernel': ['rbf']}\n\nsvc = SVC()\n\ngs_svc_x = GridSearchCV(estimator=svc,\n                    param_grid=parameters_svc,\n                    cv = k_fold_method,\n                    verbose=1,\n                    n_jobs=-2,\n                    scoring='accuracy',\n                    return_train_score=True)\n\ngs_svc_y = GridSearchCV(estimator=svc,\n                    param_grid=parameters_svc,\n                    cv = k_fold_method,\n                    verbose=1,\n                    n_jobs=-2,\n                    scoring='accuracy',\n                    return_train_score=True)","10246912":"gs_dt_y.fit(D_train, t_train_y)\ngs_dt_x.fit(D_train, t_train_x)\n\ngs_knn_y.fit(D_train, t_train_y)\ngs_knn_x.fit(D_train, t_train_x)\n\ngs_svc_y.fit(D_train, t_train_y)\ngs_svc_x.fit(D_train, t_train_x)","3e7d53a9":"#function to return an array with distances between the actual and predicted points\ndef distance(x_actual, y_actual, x_predicted, y_predicted):\n    d_x = x_actual - x_predicted\n    d_y = y_actual - y_predicted\n    dist = d_x**2 + d_y**2\n    dist = np.sqrt(dist)\n    #dist = np.sort(dist)\n    return dist","cda81074":"models_predictions_x = {'KNN_x': gs_knn_x.predict(D_test), 'DT_x': gs_dt_x.predict(D_test), 'SVC_x': gs_svc_x.predict(D_test)}\n\nmodels_predictions_y = {'KNN_y': gs_knn_y.predict(D_test), 'DT_y': gs_dt_y.predict(D_test), 'SVC_y': gs_svc_y.predict(D_test)}","2fdce195":"fig, axs = plt.subplots(3, 3, figsize=(15,15))\n(ax1, ax2, ax3), (ax4, ax5, ax6), (ax7, ax8, ax9) = axs\n\nfor ax, px in zip(axs,models_predictions_x):\n    for axes, py in zip(ax,models_predictions_y):\n        distances = distance(t_test_x, t_test_y, models_predictions_x[px], models_predictions_y[py])\n        sorted_distances = np.sort(distances)\n        probabilites = 1. * np.arange(len(sorted_distances))\/(len(sorted_distances) - 1)\n        axes.plot(sorted_distances, probabilites)\n        axes.set_title(f'CDF: Euclidean dist. error: {px}|{py}')\n        axes.set(xlabel = 'Distance (m)', ylabel = 'Probability')\n        axes.text(2,0.05,f\"Mean Error dist.: {np.mean(distances)}\")\n        axes.grid(True)\n        gridlines = axes.get_xgridlines() + axes.get_ygridlines()\n        for line in gridlines:\n            line.set_linestyle(':')\n\nfig.tight_layout()\nplt.show()\nplt.close()","3ec7b2be":"from plotly.offline import init_notebook_mode, iplot\nfrom IPython.display import display, HTML\nimport numpy as np\nfrom PIL import Image\n\nimage = Image.open(\"..\/input\/ble-rssi-dataset\/iBeacon_Layout.jpg\")\ninit_notebook_mode(connected=True)\n\nxm=np.min(t_test_x)-1.5\nxM=np.max(t_test_x)+1.5\nym=np.min(t_test_y)-1.5\nyM=np.max(t_test_y)+1.5\n\ndata=[dict(x=[0], y=[0], \n           mode=\"markers\", name = \"Predictions\",\n           line=dict(width=2, color='green')\n          ),\n      dict(x=[0], y=[0], \n           mode=\"markers\", name = \"Actual\",\n           line=dict(width=2, color='blue')\n          )\n      \n    ]\n\nlayout=dict(xaxis=dict(range=[xm, 24], autorange=False, zeroline=False),\n            yaxis=dict(range=[ym, 21], autorange=False, zeroline=False),\n            title='Predictions for SVC', hovermode='closest',\n            images= [dict(\n                  source= image,\n                  xref= \"x\",\n                  yref= \"y\",\n                  x= -3.5,\n                  y= 22,\n                  sizex= 36,\n                  sizey=25,\n                  sizing= \"stretch\",\n                  opacity= 0.5,\n                  layer= \"below\")]\n            )\n\nframes=[dict(data=[dict(x=[models_predictions_x['SVC_x'][k]], \n                        y=[models_predictions_y['SVC_y'][k]], \n                        mode='markers',\n                        \n                        marker=dict(color='red', size=10)\n                        ),\n                   dict(x=[t_test_x.iloc[k]], \n                        y=[t_test_y.iloc[k]], \n                        mode='markers',\n                        \n                        marker=dict(color='blue', size=10)\n                        )\n                  ]) for k in range(int(len(t_test_x))) \n       ]    \n          \nfigure1=dict(data=data, layout=layout, frames=frames)          \niplot(figure1)","e0582318":"From the above plots, it can be concluded that using Decision Trees to predict y target feature was not a good idea. The single best model to use is SVC to predict both the x-target feature and y-target feature. It can predict with 100% probability within ~6m radius of the actual location while having only ~1.69m mean error distance. Whereas, KNN predicts with a 100% probability within a radius of ~7m and having a mean error distance of ~1.9m There is no need to use DT at all.<br>\nThe worst option is to choose only DT to predict both the features.\n\nBelow is the plotting of the actual and predicted points on the image using SVC. (Taken and modified from ryches notebook)","e0732307":"From the above co-relation heat-map, we can observe that the difference in the colors is high on the top left and on the bottom right while the bottom left and the top right are fairly the same. If we observe the layout of the library (supplied) where the data was collected, we can see that the overlap of the signals is directly proportional to the distance by which they are kept apart. \n![title](..\/input\/ble-rssi-dataset\/iBeacon_Layout.jpg)\n\n## Data Preparation","5c69b4e5":"Defining a function to return us the array containing Euclidean distances between the actual and predicted points and a dictionary to hold our fitted models' predictions.","8ed9adc2":"This is my first kernel on Kaggle. All comments welcome :)\n\nSome parts of the code (modfied and used as is) has been taken or was inspired from the original work by Dr Mehdi Mohammadi and improvisation work by ryches. Links below.\n\nhttps:\/\/www.kaggle.com\/mehdimka\/localization-in-waldo-library-using-keras-dnn\n\nhttps:\/\/www.kaggle.com\/ryches\/better-indoor-localization-wip\n\n\n## About\nFrom source:\nThe dataset was created using the RSSI readings of an array of 13 ibeacons in the first floor of Waldo Library, Western Michigan University. Data was collected using iPhone 6S. The dataset contains two sub-datasets: a labeled dataset (1420 instances) and an unlabeled dataset (5191 instances). The recording was performed during the operational hours of the library. For the labeled dataset, the input data contains the location (label column), a timestamp, followed by RSSI readings of 13 iBeacons. RSSI measurements are negative values. Bigger RSSI values indicate closer proximity to a given iBeacon (e.g., RSSI of -65dBm represent a closer distance to a given iBeacon compared to RSSI of -85dBm). For out-of-range iBeacons, the RSSI is indicated by -200dBm. The locations related to RSSI readings are combined in one column consisting a letter for the column and a number for the row of the position.\n\n## Importing Data","63387371":"From the above plot, we can understand that the strength of signal from each of the beacons is on either ends of the strength spectrum i.e. either good signal or no signal at all. This can be confirmed by the values of the signal strengths. -200 means no signal at all while the higher numbers mean good signal. \n\nI realized as a matter of fact that a signal strength can not be 0 after I tested the signal strength from my wifi on my phone and touching my phone to the wifi router gave me a strength of -49dBm. \n\nEdit: After further research I can't say for sure that the signal strengths can not be 0. Some people argue to have achieved signal strengths in positive values. I'll leave that for now.\n\nSource: https:\/\/www.metageek.com\/training\/resources\/wifi-signal-strength-basics.html\n\nThough the above link talks about Wi-Fi signals and not Bluetooth as in the case with the dataset but it gives us an understanding of the signal strengths.\n\nIn the dataset, the max signal strength observed is -55 dBm from Beacon 9 which is a strength good enough of majority of the real world applications and usages.","538db4b9":"## Modeling\n\nFor modeling, since we have two target features, a model each has to be chosen for each of the target features. Since it is classification, KNN and Decision Tree classifiers have been selected. The models are fit with the best hyper parameters using GridSearchCV from scikit learn for both the target features.\n\nThen the accuracies can be plotted by mixing and matching these models on the data and selecting the best model(s) for the data.\n\nFirst, split the data set into test and train set for both the target features.","9e842275":"Next, import the classifiers, k-fold method and GridSearchCV for fitting the model with the best params and training with k-folds.","e9cc9ade":"Plotting the prediction probabilities","9ec40eee":"Max signal strength has been observed from beacon #9 with -55dBm. It is to be noted that in real life, the signal strength might not be a constant and depends on various factors like temperature, humidity, etc. in the medium. \n\nFor the preparation part, we shall do the following:\n\n1) Bin the data between -50dBm and -200dBm in bin ranges of 10. It is to be noted that there we will no values for some bins as observed from the plots in the data exploration phase.<br>\n2) The location feature is a combination of both the x-axis and y-axis information. i.e. The alphabet in the location is an axis point on the x-coordinate and the number an axis point in the y-coordinate. We split them into two features: x and y. Then, drop the location feature.<br>\n3) Label encode the split location features (x and y) using sklearn's LabelEncoder thus transforming the values between 0 and n_classes-1.<br>\n4) Drop the date feature.","7f0fcd2c":"One-hot encoding the descriptive features and separating the target features","043df05b":"## Data Exploration","af7946bf":"Predictions for each model for both x and y","dd0b729f":"Fitting the data to the model."}}