{"cell_type":{"1da893d0":"code","0f3c5d2e":"code","fc1297c1":"code","b446fdbf":"code","4651cda7":"code","3b15e83b":"code","54eaee38":"code","e2570dc3":"code","4d26575e":"code","443fab4e":"code","3018d85b":"code","d5a96ad0":"code","0e9584b7":"code","c6e27e95":"code","3773e37c":"code","b612676f":"code","ce7b8371":"code","2885c465":"code","361ff4b8":"code","1c59ab04":"code","7c64b739":"code","e7112b1d":"code","c32dab63":"code","24ae69b7":"code","6e26d6c2":"code","66bd7af5":"code","3068ada3":"code","c43677dd":"code","7516faa8":"code","54c90f1a":"code","14d9d22c":"code","05b6ba69":"code","1528fe50":"code","070bc8a8":"code","8d9dde80":"code","a36e04eb":"code","dbb73f1d":"code","3d3dd5f6":"code","1e347a2a":"code","e80ca536":"code","b16e3c50":"code","30f081b4":"code","1e347696":"code","7acd6eca":"code","79366d77":"code","e2a08f90":"code","e50ed7ba":"code","0f94d4ab":"code","6ccd2c34":"code","07455add":"code","7b8e29fc":"code","cbef8539":"code","7a996672":"code","26a83f43":"code","eb646983":"code","695ee4be":"code","3181c2ff":"code","82652842":"code","ca5bb44a":"code","d8227db1":"code","5a51ced1":"code","688ee341":"code","7ce228cc":"code","ad11789a":"markdown","6a57a2c1":"markdown","046b91a5":"markdown","45fd485e":"markdown","81c046b5":"markdown","378b19d1":"markdown","48905880":"markdown","7eedd144":"markdown","146fcc89":"markdown","cd88b95f":"markdown","6b2ee7fa":"markdown","e21a483c":"markdown","109890e9":"markdown","adf3de3c":"markdown","ca5bf064":"markdown","0a1c4fb4":"markdown","a01b0fea":"markdown","7856f93f":"markdown","b2ea4983":"markdown","cd13f6c2":"markdown","1593d140":"markdown","4afb339f":"markdown"},"source":{"1da893d0":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.model_selection import train_test_split","0f3c5d2e":"hp = pd.read_csv('..\/input\/train.csv')\n#pd.options.display.max_columns = None\nhp.info()","fc1297c1":"NULL_Data = pd.DataFrame({'Columns':hp.columns, \n                          'Null_Values' : hp.isnull().sum(),\n                          'Null_Perc' : hp.isnull().sum()*100\/hp.shape[0]})\nNULL_Data['DType'] = [hp.iloc[:,i].dtype for i in range(hp.shape[1])]","b446fdbf":"NULL_Data[NULL_Data.Null_Values!=0]","4651cda7":"# Summarizing the missing values >>>\nNULL_Data[NULL_Data.Null_Values!=0].DType.value_counts()","3b15e83b":"# Looking at the Missing values for Numerical attributes >>>\nNULL_Data[NULL_Data.DType=='float64']","54eaee38":"pd.options.display.max_columns=None\nhp_temp1 = hp._get_numeric_data()\nhp_temp1 = hp_temp1[hp_temp1.LotFrontage.notnull() & hp_temp1.MasVnrArea.notnull() & hp_temp1.GarageYrBlt.notnull()].loc[:,:]\nhp_temp1.drop('Id',axis=1,inplace=True)\nprint(hp_temp1.shape)\nhp_temp1.head(2)\n# This data has no null values and only numerical variables. \n# We have created this dataset to take a look at the correlation coefficients and VIF values before >>>\n# we move ahead to fill the missing data. ","e2570dc3":"#Lets create and see the correlation Coeff for the three vars which have missing data >>>\n\ncorr_coeff = hp_temp1.corr()\ncorr_coeff.loc['SalePrice',['LotFrontage','MasVnrArea','GarageYrBlt']]","4d26575e":"# Lets also calculate the VIF : Variation Inflation factor\n# Step 1 > Divide the data into Dep and Ind vars. \n# Step 2 > Insert \"Intercept\" into Ind var dataframe\n# Step 3 > Apply VIF algorithm on the dataset\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor \n\nInd = hp_temp1.drop('SalePrice',axis=1)  # Step1\nDep = hp_temp1[['SalePrice']]            # Step1\nInd.insert(0,'Intercept',1)              # Step2\nvif = pd.DataFrame({'Attributes':hp_temp1.columns})\nvif['VIF'] = [variance_inflation_factor(Ind.values,i) for i in range(Ind.shape[1])]\nvif.iloc[[1,7,24],:]\n\n# P.S. >> the division of dataset into Dep and Ind vars using dmatrices algo as well. ","443fab4e":"# We need to see how LotFrontage data is spread >>> \nplt.figure(figsize=(10,7))\nplt.hist(hp[hp.LotFrontage.notnull()].LotFrontage,bins=50,width=3)\nplt.xticks(np.arange(20,320,10))\nplt.yticks(np.arange(0,190,5))\nplt.show()","3018d85b":"print('Mean',hp.LotFrontage.mean())\nprint('Median',hp.LotFrontage.median())\nprint('Mode',hp.LotFrontage.mode())\nprint('Std Deviation',hp.LotFrontage.std())\nprint('Min',hp.LotFrontage.min())\nprint('Max',hp.LotFrontage.max())","d5a96ad0":"# As the dataset is completely skewed towards the mean value hence we can safely fill the missing >>>\n# ...values with the mean value. \n\nhp.LotFrontage.replace(hp.LotFrontage[7],hp.LotFrontage.mean(),inplace=True)","0e9584b7":"# Lets check if any missing values are left in LotFrontage \nhp.LotFrontage.isnull().sum() \n# zero value means no null values left. ","c6e27e95":"# Lets see the spread of LotFrontage values again >>> \nsns.distplot(hp.LotFrontage)\nplt.xticks(np.arange(0,313,20))\nplt.show()","3773e37c":"print('Mean',hp.MasVnrArea.mean())\nprint('Median',hp.MasVnrArea.median())\nprint('Mode',hp.MasVnrArea.mode())\nprint('Std Deviation',hp.MasVnrArea.std())\nprint('Min',hp.MasVnrArea.min())\nprint('Max',hp.MasVnrArea.max())","b612676f":"# Lets see the spread of \"MasVnrArea\" values again >>> \nplt.figure(figsize=(15,4))\nsns.distplot(hp[hp.MasVnrArea.notnull()].MasVnrArea)\nplt.xticks(np.arange(0,1600,50))\nplt.show()","ce7b8371":"# As the data is complete skewed towards value '0' hence we can not impute missing values with the mean value. \n# Filling the missing data with '0' value. \n# Run this code before running the below code to find out the index for any NaN inside this column ==> hp[hp.MasVnrArea.isnull()].MasVnrArea\n\nhp.MasVnrArea.replace(hp.MasVnrArea[234],0.00,inplace=True)","2885c465":"# Lets see the missing values again if left >>>\nhp.MasVnrArea.isnull().sum() \n# zero value means no null values left. ","361ff4b8":"# Comparing the GarageYrBlt with YearBuilt to find if there is any corelation between them >>>\nplt.figure(figsize=(10,5))\nplt.subplot(1,2,1)\nsns.distplot(hp[hp.GarageYrBlt.notnull()].GarageYrBlt)\nplt.xlabel('GarageYrBlt',size=15)\nplt.subplot(1,2,2)\nsns.distplot(hp[hp.YearBuilt.notnull()].YearBuilt)\nplt.xlabel('YearBuilt',size=15)\nplt.show()","1c59ab04":"# Showing below the correlation between YearBuilt and GarageYrBlt\ncorr_coeff.loc['YearBuilt','GarageYrBlt']","7c64b739":"# Hence we are dropping this column from the dataset to avoid multicolineariy >>> \nhp.drop('GarageYrBlt',axis=1,inplace=True)","e7112b1d":"hp.drop('Id',axis=1,inplace=True)  # We do not need Id for regression model. ","c32dab63":"hp._get_numeric_data().isnull().sum()\n#Checking if any numerical vars are left with any missing values >>>","24ae69b7":"hp_temp2 = hp._get_numeric_data()","6e26d6c2":"#Creating correlation coefficient matrix again >>>\ncorr_coeff2 = hp_temp2.corr()","66bd7af5":"# Creating VIF and Corr-Coeff table >>>\nDep = hp_temp2.drop('SalePrice',axis=1)\nDep.insert(0,'Intercept',1)\nInd = hp_temp2[['SalePrice']]\nvif = pd.DataFrame({'Attribute':Dep.columns})\nvif['VIF_Value'] = [variance_inflation_factor(Dep.values,i) for i in range(Dep.shape[1])]\nvif.set_index('Attribute',inplace=True)  # setting index as \"Attribute\" to insert Corr_coeff values in the dataset\nvif['Corr_coeff'] = corr_coeff2.SalePrice  # Adding Corr_coeff values\nvif","3068ada3":"# Filtering above data for corr-coeff less than (+ve)0.25 >>>\nvif[vif.Corr_coeff<=0.25].sort_values(by='Corr_coeff',ascending = True).T","c43677dd":"# With the help of above, dropping below columns which have corr_coeff between (+ve)0.25 to (-ve)0.25 >>>\nhp_temp3 = hp_temp2.drop(['KitchenAbvGr','EnclosedPorch','MSSubClass','OverallCond','YrSold',\n                          'LowQualFinSF','MiscVal','BsmtHalfBath','BsmtFinSF2','3SsnPorch',\n                          'MoSold','PoolArea','ScreenPorch','BedroomAbvGr','BsmtUnfSF','BsmtFullBath'],\n                           axis=1)","7516faa8":"# Again running Correlation and VIF >>>\ncorr_coeff3 = hp_temp3.corr()\nDep = hp_temp3.drop('SalePrice',axis=1)\nDep.insert(0,'Intercept',1)\nInd = hp_temp3[['SalePrice']]\nvif = pd.DataFrame({'Attribute':Dep.columns})\nvif['VIF_Value'] = [variance_inflation_factor(Dep.values,i) for i in range(Dep.shape[1])]\nvif.set_index('Attribute',inplace=True)  # setting index as \"Attribute\" to insert Corr_coeff values in the dataset\nvif['Corr_coeff'] = corr_coeff3.SalePrice  # Adding Corr_coeff values\nvif","54c90f1a":"# Lets visualize the Correlation Matrix >>>\nplt.figure(figsize=(18,18))\nsns.heatmap(corr_coeff3,annot=True,cmap='Blues')\nplt.show()","14d9d22c":"# Based on the VIF table and Correlation Matirx below attributes are selected\n# I have selected vars which have high corr-coeff and low VIF\n# Also to avoid multi-colinearity I have selected columns by looking at their corr-coeff with other vars as well.\n\nhp_temp4 = hp_temp3[['LotArea','YearBuilt','MasVnrArea', 'TotalBsmtSF', \n                     'FullBath', 'TotRmsAbvGrd', 'GarageArea','Fireplaces' ,'SalePrice']]","05b6ba69":"Dep.shape[1]","1528fe50":"# Again running VIF and Correlation >>>\ncorr_coeff4 = hp_temp4.corr()\nDep = hp_temp4.drop('SalePrice',axis=1)\nDep.insert(0,'Intercept',1.0)\nInd = hp_temp4[['SalePrice']]\nvif = pd.DataFrame({'Attribute':Dep.columns})\nvif['VIF_Value'] = [variance_inflation_factor(Dep.values,i) for i in range(Dep.shape[1])]\nvif.set_index('Attribute',inplace=True)  # setting index as \"Attribute\" to insert Corr_coeff values in the dataset\nvif['Corr_coeff'] = corr_coeff4.SalePrice  # Adding Corr_coeff values\nvif","070bc8a8":"plt.figure(figsize=(8,5))\nsns.heatmap(corr_coeff4,annot=True,cmap='coolwarm_r')\nplt.show()","8d9dde80":"from statsmodels.formula.api import ols\nfrom sklearn.model_selection import train_test_split","a36e04eb":"train,test = train_test_split(hp_temp4,test_size=0.30,random_state=123)","dbb73f1d":"regressor = ols(formula='SalePrice~LotArea+YearBuilt+MasVnrArea+TotalBsmtSF+FullBath+TotRmsAbvGrd+GarageArea+Fireplaces',\n                data=train).fit()\nregressor.summary()","3d3dd5f6":"test['Pred'] = round(regressor.predict(test[['LotArea','YearBuilt', 'MasVnrArea', 'TotalBsmtSF',\n       'FullBath', 'TotRmsAbvGrd', 'GarageArea','Fireplaces']]),2)","1e347a2a":"test.head()","e80ca536":"def MAPE(PP,AP):\n    '''\n    PP = Predicted Price\n    AP = Actual Price\n    '''\n    Dif = (PP-AP)\/AP\n    ABS = np.abs(Dif)\n    MEAN = np.mean(ABS)*100\n    return MEAN\n# MAPE : Mean Absolute Percentage Error","b16e3c50":"MAPE(test.SalePrice,test.Pred)    ","30f081b4":"#Plotting Predicted vs Actual prices ===> \nplt.scatter(test.index,test.SalePrice)\nplt.scatter(test.index,test.Pred)\nplt.legend(loc='upper right',\n          bbox_to_anchor=(1.35,1))\nplt.title('Actual vs Predicted Price',size=15)\nplt.xlabel('House Id',size=15)\nplt.ylabel('Price',size=15)\nplt.show()","1e347696":"# Script to filter dataframe for object variables only ==> \ntemp = hp.columns\nhp_cat = pd.DataFrame()\nx = 0\nfor i in temp:\n    if hp[i].dtype == 'O':\n        hp_cat.insert(x,i,hp[i])\n        x=x+1\nhp_cat.head().T","7acd6eca":"def encoder(in_df,out_df,Columns=[]):\n    '''\n    in_df = Input dataframe whose columns are coded\n    out_df = output dataframe where the coded columns are inserted\n    Columns = list of column headers of input dataframe which are to be coded\n    ''' \n    for c in Columns:\n        A = []\n        x = 0\n        temp = in_df[c].sort_values().unique()\n        for i in temp:\n            for B in np.arange(in_df[c].count()):\n                if in_df[c][B] == i:\n                    A.append(1)\n                else:\n                    A.append(0)\n            out_df.insert(x,i,A)\n            if i == temp[len(temp)-2]:\n                break\n            x = x+1\n            A = []\n\n# This encoder is soley defined by me and works on numpy and pandas only\n# This works for both LabelEncoder and OneHotEncoder\n# Using this, we can code the values of categorical vars as 0 & 1 and it will insert the values as >>>\n# new columns in the dataset of your choice. \n# Besides this also takes care of creating only columns equal to values n-1. Hence for example >>>\n# if there are 10 unique values in a column, this encoder will only create 10-1 = 9 columns. ","79366d77":"hp_reg = hp_temp4.copy()  # deep copy so that original dataset wont change","e2a08f90":"# As 'HouseStyle' column has some values which are starting with numbers hence we will need to code those values otherwise >>>\n# our regression model will not run as it understands column title example 123Sample ==> 123 as one column and Sample as >>>\n# another column.\n\nhp_cat['HouseStyle'] = ('A'+hp_cat['HouseStyle'])","e50ed7ba":"hp_cat.HouseStyle.value_counts()","0f94d4ab":"# Using below script, we can run Linear Regression for all the categorical variables one-by-one by cumulating the already >>>\n# added columns. Hence first round will run LR for 'CentralAir' and then in second round, the dataset will contain the coded >>>\n# values for CentralAir and will run LR with adding coded values from next column i.e. 'SaleType'. \n# This script can be proved very useful to analyze the impact of each categorical variable on the LR as we are getting error >>>\n# after adding every column. \n# Hence if error is increased hence we should be careful while predicting values finally using that variable. \n# With this script, we can run LR for any number of categorical variables, and it will simply give us error for the impact >>> \n# relative to each variable. \n# THIS IS AMAZING !!!\n\nhp_reg = hp_temp4.copy()\nColumns = ['MSZoning','CentralAir','SaleType','Foundation','HouseStyle']\nfor D in Columns:\n    encoder(hp_cat,hp_reg,Columns=[D])   # Encoding the values of the columns and inserting the coded values in target dataset. \n    train,test = train_test_split(hp_reg,test_size=0.30,random_state=123)   # Splitting dataset into train and test. \n    train.columns = train.columns.str.replace('[.,(,), ]', '')     # any special characters which are present in the values.\n    test.columns = test.columns.str.replace('[.,(,), ]', '')       # performing same step on test dataset as well. \n    features = \"+\".join(train.drop('SalePrice',axis=1).columns)    # Collecting all the columns as features to use in LR. \n    regressor = ols(formula='SalePrice~'+features,data=train).fit()# Creating model and fitting the model\n    test['Pred'] = round(regressor.predict(test[train.drop('SalePrice',axis=1).columns]),2) # predicting the values\n    print(D,'-->',MAPE(test.SalePrice,test.Pred))                  # calculating MeanAbsolutePercentageError (MAPE).\n    \n","6ccd2c34":"# In the above, we see that MSZoning, Foundation and HouseStyle has decreased the error. Hence we will use them again >>>\n# and create the model >>> \nhp_reg = hp_temp4.copy()\n\nColumns = ['MSZoning','Foundation','HouseStyle']\nfor D in Columns:\n    encoder(hp_cat,hp_reg,Columns=[D])   \n    train,test = train_test_split(hp_reg,test_size=0.30,random_state=123)   \n    train.columns = train.columns.str.replace('[.,(,), ]', '')     \n    test.columns = test.columns.str.replace('[.,(,), ]', '')     \n    features = \"+\".join(train.drop('SalePrice',axis=1).columns)     \n    regressor = ols(formula='SalePrice~'+features,data=train).fit()\n    test['Pred'] = round(regressor.predict(test[train.drop('SalePrice',axis=1).columns]),2) \n    print(D,'-->',MAPE(test.SalePrice,test.Pred)) ","07455add":"hp_reg = hp_temp4.copy()\n\nColumns = ['MSZoning','Foundation']\nfor D in Columns:\n    encoder(hp_cat,hp_reg,Columns=[D])   \n    train,test = train_test_split(hp_reg,test_size=0.30,random_state=123)   \n    train.columns = train.columns.str.replace('[.,(,), ]', '')     \n    test.columns = test.columns.str.replace('[.,(,), ]', '')     \n    features = \"+\".join(train.drop('SalePrice',axis=1).columns)     \n    regressor = ols(formula='SalePrice~'+features,data=train).fit()\n    test['Pred'] = round(regressor.predict(test[train.drop('SalePrice',axis=1).columns]),2) \n    print(D,'-->',MAPE(test.SalePrice,test.Pred)) ","7b8e29fc":"regressor.summary()","cbef8539":"#Plotting Predicted vs Actual prices ===> \nplt.scatter(test.index,test.SalePrice)\nplt.scatter(test.index,test.Pred)\nplt.legend(loc='upper right',\n          bbox_to_anchor=(1.35,1))\nplt.title('Actual vs Predicted Price',size=15)\nplt.xlabel('House Id',size=15)\nplt.ylabel('Price',size=15)\nplt.show()","7a996672":"test_kaggle = pd.read_csv('..\/input\/test.csv')","26a83f43":"# encoder(test_kaggle,test_kaggle,Columns = ['MSZoning','HouseStyle'])\n# While running the above encoder, I got the error hence I further checked the data and I found >>>\n# that the MSZoning has the missing values\ntest_kaggle.MSZoning.isnull().sum()","eb646983":"test_kaggle[test_kaggle.Foundation.isnull()].Foundation\n# To find if there is any missing values in \"Foundation\"","695ee4be":"# In order to fill in the missing values, lets summarize the MSZoning data >>>\ntest_kaggle.MSZoning.value_counts() ","3181c2ff":"# Replacing missing values with \"RL\" values as it has the highest frequency in the dataset. \ntest_kaggle.MSZoning.replace(test_kaggle.MSZoning[455],'RL',inplace=True)\ntest_kaggle.MSZoning.isnull().sum()  # To check if there is any missing value left. \n\n# I have used \"455\" as the row number to input the NaN value in the code because otherwise NaN value >>>\n# is empty value and it cant be referenced as NaN itself in this code. \n# Instead of 455, we can provide any other row number as well wherever MSZoning has missing value >>>\n# Using this code, we can find such a rownumber >>> test_kaggle[test_kaggle.MSZoning.isnull()].MSZoning","82652842":"# Further I am checking below for any numerical vars in test data if there is any missing values >>>\ntest_kaggle[hp_temp4.drop(\"SalePrice\",axis=1).columns].isnull().sum()","ca5bb44a":"# Filling out the missing values using the same as used earlier in this project to handle the missing values >>>\ntest_kaggle.MasVnrArea.replace(test_kaggle.MasVnrArea[231],0.00,inplace=True)\ntest_kaggle.TotalBsmtSF.replace(test_kaggle.TotalBsmtSF[660],test_kaggle.TotalBsmtSF.mean(),inplace=True)\ntest_kaggle.GarageArea.replace(test_kaggle.GarageArea[1116],test_kaggle.GarageArea.mean(),inplace=True)","d8227db1":"# Checking again if there is any missing values left in our numerical vars >>>\ntest_kaggle[hp_temp4.drop(\"SalePrice\",axis=1).columns].isnull().sum()","5a51ced1":"# In order to predict the SalePrice in this \"test\" dataset, we need to encode the columns of this >>>\n# dataset as well >>>\nencoder(test_kaggle,test_kaggle,Columns = ['MSZoning','Foundation'])\ntest_kaggle.columns = test_kaggle.columns.str.replace('[.,(,), ]', '')","688ee341":"# Lets predict SalePrice using the regressor we have created earlier >>>\ntest_kaggle['SalePrice'] = round(regressor.predict(test_kaggle[train.drop('SalePrice',axis=1).columns]),2) ","7ce228cc":"test_kaggle[['Id','SalePrice']].head()","ad11789a":"# Creating model with Numeric and Categorical Vars","6a57a2c1":"<font color = 'grey'>\n** Till now I used only numerical vars and created model with error of `15.006 %`. Now I will also include categorical vars into our regression model.**\n    <\/font>","046b91a5":"### Final Step\n<font color = 'grey'>\n    **Predicting the SalePrice for values in the `test` dataset included in the dataset of this competition**\n    <\/font>\n","45fd485e":"#### Handling missing values for \"GarageYrBlt\"","81c046b5":"# I will add the categorical vars' coded values in the numerical dataset we have created earlier so that we can run >>>\n# regression model for all the variables. ","378b19d1":"Kaggle-Project-House_Price_Prediction","48905880":"`As we can see that the correlation coefficient is quite higher hence we can safely drop GarageYrBlt and retain YearBlt for our regression model. `","7eedd144":"** Lets now create the prediction model for Numerical values **    ","146fcc89":"**Post Note :** MAPE and Encoder are my customized functions solely written by me. I have here attempted to work on this project with best of my current understanding of Python, EDA, statistics and Machine Learning. \nI am practicing Data Scientist and looking for career opportunities in Data Science world currently. \n\nIf you liked my work, please give me an upvote. If you have some feedback, please give me your valuable comments. This will help and encourage me a lot in my journey of data science . ","cd88b95f":"## Creating model based on Numerical Variables\n`We will create the model with categorical variables after this step`","6b2ee7fa":"### Feature Selection Process \n***We will use VIF and Correlation to perform this process***","e21a483c":"No missing values above","109890e9":"As we notice the above table no longer has \"inf\" values for VIF after we have removed columns with less corr-coeff","adf3de3c":"#### Handling missing values for \"LotFrontage\"","ca5bf064":"### Handling Missing Data  ","0a1c4fb4":"<font color = 'blue'>\n    **Lets understand this data**\n    <\/font>   \n    \n**Independent Vars :** This data has **43 Categorical** and **37 Numerical** independent variables.    \n**Dependent Vars :** It has **SalePrice** variable which is **Numerica** type. \n\nIn the process of creating the prediction model, we will go step-by-step :::  \n`~> 1 )`  Handle the numerical data --> Handle missing values --> Attribute selection --> Create the model   \n`~> 2 )`  Hanlde the categorical data --> Attribute selection --> Handle missing values --> Create the model with `Numerical` + `Categorical data`.","a01b0fea":"R-Squared - 0.709 --> this is good number as long as it is higher than 0.50\nF-Value - 101.5 --> This is also good number though not as high as the number we received in case numerical vars only. \nP-Values ---> We cant consider P-vales for categorical values as they are always interdependent. However there are no numerical \nvalues for which the p-value>0.05","7856f93f":"This is wow! We see that the error has actually decreased. But this a lot of times will depend on business situation as well >> \n>> and can not just remove the variables based on the increased\/decreased errors. ","b2ea4983":"# But now we see that \"HouseStyle\" has actually increased the error. We can exclude this variable as well though this is quite important variable for houseprice. Still we can check the impact >>> \n","cd13f6c2":"#### Handling missing values for \"MasVnrArea\"","1593d140":"`As we can see that all the three missing attributes have good corr-coeff besides the VIF is only more than 2 for MasVnrArea hence we will now first fill-in the missing data for all these three numerical ind variables and then will run VIF and correlation again. `","4afb339f":"# Instead of filling out the missing values for all the categorical vars one-by-one, lets use only those categorical >>>\n# variables which are most relevant to the SalePrice. \n# Hence  I am actually referring to  \"FORWARD SELECTION PROCESS\" \n\n--------------List of Most Relevant Categorical vars-------------------\n-----------------------------------------------------------------------\nMSZoning: Identifies the general zoning classification of the sale.\nUtilities: Type of utilities available\nCentralAir: Central air conditioning\nSaleType: Type of sale\nHouseStyle: Style of dwelling\nFoundation: Type of foundation\n-----------------------------------------------------------------------\n# I have selected these vars manually by reading out their description, past experience and searching on inernet for relevant \n# factors for house prices. \n# I have however derived a method below how we can select vars based on MAPE. \n# I will now try to add the above features one-by-one. \n# Also I noticed that their is no missing values for the above variables. "}}