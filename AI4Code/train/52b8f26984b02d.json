{"cell_type":{"5837bc24":"code","ff7501a2":"code","e1194f9d":"code","27c9911e":"code","29232ed9":"code","1b1a2b5a":"code","2242dd1d":"code","03b2eb3f":"code","a3f725e4":"code","c78f6f86":"code","df58d0f4":"code","3c0a49ef":"code","42fea0cd":"code","6d8949e5":"code","a1aec7fd":"code","2bbde8cc":"code","dd55ac1a":"code","52bf9964":"code","13bf0998":"code","c0a98430":"code","aea37c56":"code","f16392ab":"code","ee9bcf9c":"markdown","a1fadbac":"markdown","84edc4be":"markdown","6abe9a92":"markdown","82d270a0":"markdown","970854fc":"markdown","69831bb1":"markdown","f42988e3":"markdown","9477d7aa":"markdown","1e06314d":"markdown","2e570043":"markdown","672479b9":"markdown","bfed1ec0":"markdown","4d80ccc2":"markdown","997c7c82":"markdown","e77a31ac":"markdown","e2ec3020":"markdown","9d6e5311":"markdown","492355de":"markdown","08b1b9e4":"markdown"},"source":{"5837bc24":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","ff7501a2":"data=pd.read_csv(\"..\/input\/Pokemon.csv\")","e1194f9d":"data.head()","27c9911e":"data.drop([\"#\",\"Name\",\"Type 1\",\"Type 2\"],axis=1,inplace=True)\ndata.head()","29232ed9":"data.info()","1b1a2b5a":"data.Legendary=[1 if each==True else 0 for each in data.Legendary]","2242dd1d":"y=data.Legendary.values #class\nx_data=data.drop([\"Legendary\"],axis=1)","03b2eb3f":"data.describe()","a3f725e4":"x=(x_data-np.min(x_data))\/(np.max(x_data)-np.min(x_data))","c78f6f86":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)","df58d0f4":"alg_acc={} # to keep accuracies","3c0a49ef":"from sklearn.neighbors import KNeighborsClassifier\nscores=[]\nfor each in range(1,10):\n    knn_t=KNeighborsClassifier(n_neighbors=each)\n    knn_t.fit(x_train,y_train)\n    scores.append(knn_t.score(x_test,y_test))\nplt.plot(range(1,10),scores)\n","42fea0cd":"knn=KNeighborsClassifier(n_neighbors=2)\nknn.fit(x_train,y_train)\nprint(\"accuracy is\",knn.score(x_test,y_test))\nalg_acc[\"knn\"]=knn.score(x_test,y_test)","6d8949e5":"from sklearn.metrics import confusion_matrix\ny_pre=knn.predict(x_test)\ny_true=y_test\ncm=confusion_matrix(y_true,y_pre)","a1aec7fd":"import seaborn as sns\nsns.heatmap(cm,annot=True,fmt=\".0f\")","2bbde8cc":"np.count_nonzero(y_train)","dd55ac1a":"from sklearn.svm import SVC\nsvm=SVC(random_state=42)\nsvm.fit(x_train,y_train)\nalg_acc[\"svm\"]=svm.score(x_test,y_test)\nprint(\"Support Vector Machine test accuracy is:\",svm.score(x_test,y_test))","52bf9964":"from sklearn.naive_bayes import GaussianNB\nnb=GaussianNB()\nnb.fit(x_train,y_train)\nalg_acc[\"nb\"]=nb.score(x_test,y_test)\nprint(\"Naive Bayes test accuracy is:\",nb.score(x_test,y_test))","13bf0998":"from sklearn.tree import DecisionTreeClassifier\ndt=DecisionTreeClassifier()\ndt.fit(x_train,y_train)\nalg_acc[\"dt\"]=dt.score(x_test,y_test)\nprint(\"Decision Tree test accuracy is:\",dt.score(x_test,y_test))","c0a98430":"from sklearn.ensemble import RandomForestClassifier\nscores2=[]\nfor each in range(100,1000,100):\n    rf_t=RandomForestClassifier(n_estimators=each,random_state=42)\n    rf_t.fit(x_train,y_train)\n    scores2.append(rf_t.score(x_test,y_test))\nplt.plot(range(100,1000,100),scores2)","aea37c56":"rf=RandomForestClassifier(n_estimators=100,random_state=42)\nrf.fit(x_train,y_train)\nalg_acc[\"rf\"]=rf.score(x_test,y_test)\nprint(\"Random Forest test accuracy is:\",rf.score(x_test,y_test))","f16392ab":"label=alg_acc.keys()\nscores=alg_acc.values()\nplt.plot(label,scores)","ee9bcf9c":"   Let visulization confusion matrix","a1fadbac":"We will use some algorithm to create model and evaluate accuracy of them.","84edc4be":"**Naive Bayes**","6abe9a92":"100 is a good value for n_estimators hyperparamater.","82d270a0":"**Conclusion**","970854fc":"According to confusion matrix heatmap. We predict 149  of non-legendary pokemon correctly, only 1 wrong. It's good accuracy. However, there are 10 legendary pokemon, our model predict 5 of them correctly, 5 wrong. 50% accuracy. The reason of low accuracy is we dont have enough legendary pokemon at data.","69831bb1":"K=2 gives best accuracy so i will use it to create my model.","f42988e3":"**Random Forest**","9477d7aa":"**Decision Tree**","1e06314d":"**Support Vector Machine**","2e570043":"We used 55 legendary pokemon at data to train our model. not enough.","672479b9":"Random forest has hyperparameter called as n_estimators. It means tree number. To find best hyperparameter value, we can use for loop.","bfed1ec0":"Before fitting my data i want to find best K value, so i will make a for loop.","4d80ccc2":"Because of the numeric differences i will normalize data.","997c7c82":"   True =1 , False=0. This is how i want to use my class.","e77a31ac":"As you see; Knn and random forest gave best accuracies at our data.","e2ec3020":"I will use numeric featues therefore drop them.","9d6e5311":"To understand and evaluate accuracy i will use confusion matrix.","492355de":"**KNearestNeighbor**","08b1b9e4":"I will split my train and test data."}}