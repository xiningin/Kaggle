{"cell_type":{"f8784dfd":"code","d2c1eacc":"code","6ad479dd":"code","c25fc6aa":"code","6620cd60":"code","8f826cb9":"code","c74493c9":"code","fac71776":"code","33fcd289":"code","669b75ca":"code","3e23e9be":"code","49fc7bdc":"code","7851c9ca":"code","40804e13":"code","71caa0f2":"code","372f7d2b":"code","ec12bb13":"code","ad2077ab":"code","7482880f":"code","46c68eb0":"code","ea1c9eeb":"code","67a91414":"code","85db7ba3":"code","47b9c05a":"code","2babd8c4":"code","f2a2cca5":"code","0525e056":"code","9de2cb2f":"code","465299c6":"code","3273cced":"code","95c560e4":"code","31237cb3":"code","6fa38c5e":"code","dd28e1e4":"code","f1bcd0a5":"code","b44dd3c7":"code","f0b66d1d":"code","4a44225b":"code","27246f0e":"code","cc1dbcd4":"code","31890c59":"code","4ca9ff4b":"code","46fc76d9":"code","e03fb160":"code","0d81c2c3":"code","2ddefbf5":"markdown","2406ac13":"markdown","6a8b6582":"markdown","6b4fe062":"markdown","a7b75087":"markdown","fdd3c48b":"markdown","0381c26f":"markdown","4b0948f5":"markdown","b4714033":"markdown","6a2d3236":"markdown","3831269e":"markdown"},"source":{"f8784dfd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d2c1eacc":"df_feature = pd.read_csv('..\/input\/credit-card-approval-prediction\/application_record.csv',header=0)\ndf_feature","6ad479dd":"df_feature.describe()","c25fc6aa":"df_feature.isnull().any()","6620cd60":"df_feature.groupby('OCCUPATION_TYPE')['OCCUPATION_TYPE'].count()","8f826cb9":"df_feature_null = df_feature.dropna(axis=0)\ndf_feature_null.isnull().any()","c74493c9":"df_feature_drop = df_feature_null.drop(['CODE_GENDER','DAYS_BIRTH','DAYS_EMPLOYED','FLAG_MOBIL','FLAG_WORK_PHONE','FLAG_EMAIL'],axis=1)\ndf_feature_drop.isnull().any()","fac71776":"df_feature_clean = df_feature_drop\ndf_feature_clean.dtypes","33fcd289":"df_feature_clean['FLAG_OWN_CAR'] = pd.factorize(df_feature_clean['FLAG_OWN_CAR'])[0]\ndf_feature_clean['FLAG_OWN_REALTY'] = pd.factorize(df_feature_clean['FLAG_OWN_REALTY'])[0]\ndf_feature_clean['NAME_INCOME_TYPE'] = pd.factorize(df_feature_clean['NAME_INCOME_TYPE'])[0]\ndf_feature_clean['NAME_EDUCATION_TYPE'] = pd.factorize(df_feature_clean['NAME_EDUCATION_TYPE'])[0]\ndf_feature_clean['NAME_FAMILY_STATUS'] = pd.factorize(df_feature_clean['NAME_FAMILY_STATUS'])[0]\ndf_feature_clean['NAME_HOUSING_TYPE'] = pd.factorize(df_feature_clean['NAME_HOUSING_TYPE'])[0]\ndf_feature_clean['OCCUPATION_TYPE'] = pd.factorize(df_feature_clean['OCCUPATION_TYPE'])[0]\ndf_feature_clean.dtypes","669b75ca":"#\u0e14\u0e36\u0e07 ID \u0e2d\u0e2d\u0e01\u0e21\u0e32\u0e1e\u0e37\u0e48\u0e2d\u0e40\u0e2d\u0e32\u0e44\u0e1b\u0e40\u0e0a\u0e37\u0e48\u0e2d\u0e21\u0e01\u0e31\u0e1a\u0e02\u0e2d\u0e07 label\nfeature_id = df_feature_clean['ID'].values\ndf_feature_clean = df_feature_clean.drop(['ID'],axis=1)","3e23e9be":"feature_id","49fc7bdc":"df_label = pd.read_csv('..\/input\/credit-card-approval-prediction\/credit_record.csv',header=0)\ndf_label","7851c9ca":"df_label.groupby('ID')['STATUS'].count() #\u0e41\u0e15\u0e48\u0e25\u0e30\u0e04\u0e19\u0e21\u0e35\u0e01\u0e35\u0e48 record","40804e13":"df_label['STATUS'], labels = pd.factorize(df_label['STATUS']) #\u0e40\u0e1b\u0e25\u0e35\u0e48\u0e22\u0e19 status \u0e43\u0e2b\u0e49\u0e40\u0e1b\u0e47\u0e19\u0e40\u0e25\u0e02 \u0e41\u0e25\u0e49\u0e27\u0e40\u0e01\u0e47\u0e1a\u0e04\u0e48\u0e32\u0e40\u0e14\u0e34\u0e21\u0e44\u0e27\u0e49\u0e43\u0e19\u0e15\u0e31\u0e27\u0e41\u0e1b\u0e23\u0e0a\u0e37\u0e48\u0e2d labels\ndf_label","71caa0f2":"labels #\u0e40\u0e23\u0e35\u0e22\u0e07\u0e15\u0e32\u0e21 index \u0e40\u0e1b\u0e47\u0e19\u0e04\u0e48\u0e32\u0e19\u0e35\u0e49\u0e43\u0e19 column \u0e02\u0e2d\u0e07 STATUS","372f7d2b":"df_label_clean_records = df_label[df_label['STATUS'] != 0] #\u0e15\u0e31\u0e14\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25 X \u0e04\u0e37\u0e2d \u0e40\u0e14\u0e37\u0e2d\u0e19\u0e19\u0e31\u0e49\u0e19\u0e44\u0e21\u0e48\u0e44\u0e14\u0e49\u0e01\u0e39\u0e49\u0e2d\u0e2d\u0e01 (index0) \u0e40\u0e1e\u0e23\u0e32\u0e30\u0e44\u0e21\u0e48\u0e21\u0e35\u0e1c\u0e25\u0e43\u0e19\u0e01\u0e32\u0e23\u0e27\u0e34\u0e40\u0e04\u0e23\u0e32\u0e30\u0e2b\u0e4c\ndf_label_clean_records","ec12bb13":"df_label_clean_records['STATUS'][df_label_clean_records['STATUS']>5] = 10 #\u0e1c\u0e39\u0e49\u0e17\u0e35\u0e48\u0e08\u0e48\u0e32\u0e22\u0e40\u0e07\u0e34\u0e19\u0e40\u0e01\u0e34\u0e19\u0e01\u0e33\u0e2b\u0e19\u0e14\u0e43\u0e19\u0e41\u0e15\u0e48\u0e25\u0e30 record = 10\ndf_label_clean_records['STATUS'][df_label_clean_records['STATUS']<=5] = 0 #\u0e1c\u0e39\u0e49\u0e17\u0e35\u0e48\u0e08\u0e48\u0e32\u0e22\u0e40\u0e07\u0e34\u0e19\u0e15\u0e23\u0e07\u0e41\u0e25\u0e30\u0e40\u0e01\u0e34\u0e19\u0e40\u0e1e\u0e35\u0e22\u0e07 1 \u0e40\u0e14\u0e37\u0e2d\u0e19\u0e43\u0e19\u0e41\u0e15\u0e48\u0e25\u0e30 record = 0     \ndf_label_clean_records","ad2077ab":"df_label_clean_records.groupby('STATUS')['STATUS'].count()","7482880f":"df_label_clean = df_label_clean_records.groupby('ID').mean() #\u0e2b\u0e32\u0e04\u0e48\u0e32 avg status \u0e02\u0e2d\u0e07\u0e41\u0e15\u0e48\u0e25\u0e30 ID\ndf_label_clean","46c68eb0":"df_label_clean['STATUS'][df_label_clean['STATUS'] > 0 ] = 1 #ID \u0e17\u0e35\u0e48\u0e08\u0e48\u0e32\u0e22\u0e44\u0e21\u0e48\u0e15\u0e23\u0e07 bad debt = 1\ndf_label_clean['STATUS'][df_label_clean['STATUS'] <= 0 ] = 0 #ID \u0e17\u0e35\u0e48\u0e08\u0e48\u0e32\u0e22\u0e15\u0e23\u0e07\u0e15\u0e25\u0e2d\u0e14 good debt = 0\ndf_label_clean","ea1c9eeb":"df_label_clean.groupby('ID')['STATUS'].count() #Length: 41449","67a91414":"df_label_clean.groupby('STATUS')['STATUS'].count() #41118+331 = 41449","85db7ba3":"#\u0e14\u0e36\u0e07\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e17\u0e35\u0e48\u0e15\u0e49\u0e2d\u0e07\u0e43\u0e0a\u0e49\u0e43\u0e19 label \u0e2d\u0e2d\u0e01\u0e21\u0e32\nlabel_id = df_label_clean.index.values #\u0e40\u0e2d\u0e32 ID (\u0e17\u0e35\u0e48\u0e16\u0e39\u0e01 set \u0e43\u0e2b\u0e49\u0e40\u0e1b\u0e47\u0e19 index) \u0e43\u0e19 label \u0e2d\u0e2d\u0e01\u0e21\u0e32 \u0e40\u0e1e\u0e37\u0e48\u0e2d \u0e44\u0e1b\u0e40\u0e0a\u0e37\u0e48\u0e2d\u0e21\u0e01\u0e31\u0e1a feature\nlabel_status = df_label_clean['STATUS'].values #\u0e40\u0e2d\u0e32 status \u0e43\u0e19 label \u0e2d\u0e2d\u0e01\u0e21\u0e32 \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e43\u0e0a\u0e49\u0e40\u0e1b\u0e47\u0e19 label \u0e02\u0e2d\u0e07 model","47b9c05a":"label_id","2babd8c4":"label_status","f2a2cca5":"ID = [ item for item in feature_id if item in label_id] #\u0e40\u0e0a\u0e47\u0e04 id \u0e08\u0e32\u0e01 feature \u0e41\u0e25\u0e30 label\u0e17\u0e35\u0e48\u0e40\u0e2b\u0e21\u0e37\u0e2d\u0e19\u0e01\u0e31\u0e19\nlen(ID)","0525e056":"#feature \u0e02\u0e2d\u0e07 model\nfeatures = df_feature_clean.values\nX_list = [ features[np.where(feature_id == item), :][0] for item in ID ] #\u0e40\u0e2d\u0e32 feature \u0e08\u0e23\u0e34\u0e07\u0e46 \u0e15\u0e23\u0e07\u0e01\u0e31\u0e1a ID \u0e17\u0e35\u0e48\u0e40\u0e0a\u0e37\u0e48\u0e2d\u0e21\u0e01\u0e31\u0e19\u0e2d\u0e2d\u0e01\u0e21\u0e32\nX = np.vstack(X_list) #\u0e40\u0e2d\u0e32 list \u0e21\u0e32\u0e15\u0e48\u0e2d\u0e01\u0e31\u0e19\u0e43\u0e2b\u0e49\u0e40\u0e1b\u0e47\u0e19 np.array\nX.shape","9de2cb2f":"#label \u0e02\u0e2d\u0e07 model\ny = np.array([ label_status[np.where(label_id == item)][0] for item in ID ]) #\u0e40\u0e2d\u0e32 Label \u0e08\u0e23\u0e34\u0e07\u0e46 \u0e15\u0e23\u0e07\u0e01\u0e31\u0e1a ID \u0e17\u0e35\u0e48\u0e40\u0e0a\u0e37\u0e48\u0e2d\u0e21\u0e01\u0e31\u0e19\u0e2d\u0e2d\u0e01\u0e21\u0e32\ny.shape","465299c6":"sum(y==0)","3273cced":"sum(y==1)","95c560e4":"from sklearn.model_selection import train_test_split \nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2,random_state=0)","31237cb3":"#\u0e41\u0e01\u0e49 imblance \u0e42\u0e14\u0e22\u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49 undersample \u0e21\u0e32\u0e40\u0e17\u0e48\u0e32\u0e01\u0e31\u0e1a 3089(y=1), \u0e43\u0e0a\u0e49 undersampling \u0e40\u0e1e\u0e23\u0e32\u0e30\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e40\u0e22\u0e2d\u0e30\u0e21\u0e32\u0e01\nfrom imblearn.under_sampling import RandomUnderSampler\nundersample = RandomUnderSampler(sampling_strategy='majority', random_state=1) #resample of imbalanced label\nX_res, y_res = undersample.fit_resample(X_train, y_train)","6fa38c5e":"X_train, X_val, y_train, y_val = train_test_split(X_res, y_res,test_size=0.2,random_state=0)","dd28e1e4":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaler.fit(X_train)\nX_train_norm =  scaler.transform(X_train)\nX_val_norm = scaler.transform(X_val)\nX_test_norm = scaler.transform(X_test)","f1bcd0a5":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\nparams = {'C': [1, 2, 4, 8, 16, 32, 64, 128]}\nclf = GridSearchCV(LogisticRegression(random_state=0, solver='liblinear'),params, cv=10)\nclf.fit(X_train_norm, y_train)\nprint(\"Best params : \" + str(clf.best_params_))\nprint(\"10CV accuracy : \"+str(clf.best_score_*100))","b44dd3c7":"y_predict = clf.predict(X_test_norm)\nprint(\"Test accuracy : \"+str(sum(y_test == y_predict)\/len(y_test)*100))","f0b66d1d":"from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ntarget_names = ['Good debt', 'Bad debt']\nC = confusion_matrix(y_test,y_predict) \nC = C \/ C.astype(np.float).sum(axis=1)*100\nsns.heatmap(C,annot=True, fmt=\".2f\",cmap=\"GnBu\",xticklabels=target_names, yticklabels=target_names)\nplt.ylabel(\"True Label\")\nplt.xlabel(\"Predicted Label\")\nplt.show()","4a44225b":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,y_predict,target_names=target_names))","27246f0e":"import tensorflow as tf\ndef create_model():\n    tf.random.set_seed(0)\n    tf.compat.v1.reset_default_graph() # Clear Model\n    model = tf.keras.models.Sequential([\n      tf.keras.layers.Dense(8, activation='relu', input_shape=(11,)),\n      tf.keras.layers.Dense(4, activation='relu'),\n      tf.keras.layers.Dense(2, activation='relu'),\n      tf.keras.layers.Dense(1, activation='sigmoid')  \n    ])\n    return model","cc1dbcd4":"model = create_model()\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) \n\n#\u0e43\u0e0a\u0e49 algorithm \u0e0a\u0e37\u0e48\u0e2d adam\n#\u0e43\u0e0a\u0e49 binary_crossentropy \u0e40\u0e1e\u0e23\u0e32\u0e30\u0e04\u0e33\u0e15\u0e2d\u0e1a\u0e02\u0e2d\u0e07 label \u0e21\u0e35\u0e41\u0e04\u0e48 2 \u0e04\u0e48\u0e32","31890c59":"train_acc = list()\nval_acc = list()\nfor i in range(0,300):\n  history = model.fit(X_train_norm, y_train, epochs= 1, batch_size = 300, validation_data= (X_val_norm, y_val))\n  tmp_avg = np.mean(history.history['accuracy'])\n  tmp_avg_val = np.mean(history.history['val_accuracy'])\n  train_acc.append(tmp_avg)\n  val_acc.append(tmp_avg_val)\n\n#batch_size \u0e04\u0e37\u0e2d \u0e40\u0e2d\u0e32\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e40\u0e02\u0e49\u0e32\u0e44\u0e1b train \u0e17\u0e35\u0e25\u0e30\u0e40\u0e17\u0e48\u0e32\u0e44\u0e2b\u0e23\u0e48","4ca9ff4b":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nplt.figure(num=None, figsize=(16, 8), dpi=90, facecolor='w', edgecolor='k')\nplt.plot()\nplt.plot(train_acc)\nplt.plot(val_acc)\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","46fc76d9":"y_predict = np.round(model.predict(X_test_norm))\ny_predict = [i[0] for i in y_predict.tolist()]\nsum(y_predict == y_test)\/len(y_test)","e03fb160":"from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ntarget_names = ['Good debt', 'Bad debt']\nC = confusion_matrix(y_test,y_predict) \nC = C \/ C.astype(np.float).sum(axis=1)*100\nsns.heatmap(C, annot=True, fmt=\".2f\",cmap=\"GnBu\",xticklabels=target_names, yticklabels=target_names)\nplt.ylabel(\"True Label\")\nplt.xlabel(\"Predicted Label\")\nplt.show()","0d81c2c3":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,y_predict,target_names=target_names))","2ddefbf5":"**feature**","2406ac13":"<p> index 0 = X: No loan for the month\n<p> index 1 = 0: 1-29 days past due \n<p> index 2 = C: paid off that month \n<p> index 3 = 1: 30-59 days past due \n<p> index 4 = 2: 60-89 days overdue \n<p> index 5 = 3: 90-119 days overdue  \n<p> index 6 = 4: 120-149 days overdue \n<p> index7 = 5: Overdue or bad debts, write-offs for more than 150 days ","6a8b6582":"Testing","6b4fe062":"**Label**","a7b75087":"# **Model**","fdd3c48b":"**Deep learning**","0381c26f":"**Logistic Regression**","4b0948f5":"\u0e40\u0e1b\u0e47\u0e19 0 \u0e41\u0e1b\u0e25\u0e27\u0e48\u0e32\u0e08\u0e48\u0e32\u0e22\u0e15\u0e23\u0e07\u0e17\u0e38\u0e01\u0e04\u0e23\u0e31\u0e49\u0e07 \u0e40\u0e1e\u0e23\u0e32\u0e30 \u0e40\u0e09\u0e25\u0e35\u0e48\u0e22 = 0","b4714033":"Training","6a2d3236":"**check feature & label**","3831269e":"# **Preprocessing**"}}