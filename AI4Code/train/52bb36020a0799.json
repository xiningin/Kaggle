{"cell_type":{"d88780c8":"code","fe16b393":"code","24ad0663":"code","fc936d0d":"code","501d5177":"code","d8e92e61":"code","5e350796":"code","f4eb77ae":"code","d8f2bdbb":"code","4c076c27":"code","9e534995":"code","d043b82c":"code","bf9d36de":"code","d8162255":"code","ccc8913a":"code","cb4dbe39":"code","c3cf34a0":"code","261e0f1f":"code","ae6f21be":"code","e3e1b7c8":"code","2ecf3576":"code","515f85dc":"code","214bf323":"code","4e75f5ce":"code","54c432e3":"code","e7876012":"code","ebf41d05":"code","ab3575af":"code","f8e0e898":"code","28881ecc":"code","47841099":"code","603db918":"code","a0e269d7":"code","d9095657":"code","84641ebc":"code","8c1e26b6":"code","8a58c80f":"code","0303182a":"code","f1563700":"code","dc9d0b91":"code","54e94ba1":"code","9bff6477":"code","0d1e5951":"code","aff387a7":"code","afe70d8d":"code","8b5461ee":"code","986736a7":"code","09a940c8":"code","f8838769":"code","7e707f97":"code","1601c00f":"code","baa05337":"code","3c9ca5a0":"code","a4585742":"code","45eab0e2":"code","9599768d":"code","4f5c5cde":"code","e9aced0e":"code","97553dea":"code","6ebebf00":"code","bb907e14":"markdown","1672f85a":"markdown","9da2cd9c":"markdown","a3d78107":"markdown","67891ad2":"markdown","e6087c20":"markdown","36f1fa1f":"markdown","4dbe07f5":"markdown","2b3a0a4e":"markdown","cbe587a4":"markdown","d75d8b8c":"markdown","c1bf8665":"markdown","fd9a7c58":"markdown","d33ae06a":"markdown","92ff2581":"markdown","6c9d85a8":"markdown","da4a6649":"markdown","8e455bf1":"markdown","48c07ef7":"markdown","a353a8e8":"markdown","13810a9d":"markdown","52a62234":"markdown","b83427b4":"markdown","fac8fd79":"markdown","cf32cd6f":"markdown","2e9371e0":"markdown","64f78e21":"markdown","c708934d":"markdown","b65e9491":"markdown","3e133706":"markdown","76142292":"markdown","388c4033":"markdown","2af18787":"markdown","c9155861":"markdown","d1a3a065":"markdown","564c300f":"markdown","6609f05e":"markdown","9658e6f2":"markdown","26aa331a":"markdown","4cb788bd":"markdown","3c126049":"markdown","e71648f7":"markdown","26fc36c3":"markdown","d4bb249f":"markdown","cde1f583":"markdown","7f6d9df8":"markdown","97b5af2d":"markdown","1bb35f4f":"markdown","71194c0e":"markdown","2fb338ba":"markdown","5e9205c7":"markdown","74f654af":"markdown","3fcf4004":"markdown","b7de1270":"markdown","4fb99f0a":"markdown","590c354f":"markdown","55839e85":"markdown","1abdc95b":"markdown","f5cb2f02":"markdown","436a4243":"markdown","7d862d9c":"markdown","a863d0a0":"markdown"},"source":{"d88780c8":"#import of the necessary libaries and define their names \n\nimport numpy as np \n\nimport seaborn as sns \nsns.set_style(\"whitegrid\")\n\nimport statsmodels.api as sm \n\n\nimport pandas as pd \n\nimport matplotlib.pyplot as plt\n#to have the graphs in jupyter\n%matplotlib inline\nimport matplotlib.image as mpimg # only neccesary for the picture \n\n\n#libaries for ML model (predictive analysis)\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\n\n\n\n#upload a happy picture! \nimg= mpimg.imread(\"..\/input\/happiness\/HappinessImage.jpeg\")\nplt.figure(figsize=(15,11)) #make picture larger \nimgplot = plt.imshow(img) \nplt.axis (\"off\") #remove the axis and tickers \nplt.show()","fe16b393":"#import csv datasets and safe as pandas DataFrame\nhappy15_df = pd.read_csv (\"..\/input\/world-happiness\/2015.csv\")\nhappy16_df = pd.read_csv (\"..\/input\/world-happiness\/2016.csv\")\nhappy17_df = pd.read_csv (\"..\/input\/world-happiness\/2017.csv\")\nhappy18_df = pd.read_csv (\"..\/input\/world-happiness\/2018.csv\")\nhappy19_df = pd.read_csv (\"..\/input\/world-happiness\/2019.csv\")\n\n#and we have a look at the last one (head is by default with the first 5 rows)\nhappy19_df.head()\n\n","24ad0663":"#let's see the shape \nprint (\" Shape15:\" , happy15_df.shape ,\"\\n\",\n       \"Shape16:\" , happy16_df.shape ,\"\\n\",\n       \"Shape17:\" , happy17_df.shape ,\"\\n\",\n       \"Shape18:\" , happy18_df.shape ,\"\\n\",\n       \"Shape19:\" , happy19_df.shape  )\n\n\n","fc936d0d":"#values and data types of the 5 datasets \nprint ( happy15_df.info() \n       , happy16_df.info() \n       , happy17_df.info()\n       , happy18_df.info() \n       , happy19_df.info())\n\nprint (\"different data labels, and confirmation of different variables\")","501d5177":"\"\"\"\nrenaming of year 2018 and 2019 \nlabeling as the base: 2015\nNote: very important otherwise we \nwill create new colums as soon as we merge the data\n\"\"\"\n\n#Country \n\nhappy18_df = happy18_df.rename(columns= {\"Country or region\": \"Country\"})\n\nhappy18_df\n\nhappy19_df = happy18_df.rename(columns= {\"Country or region\": \"Country\"})\n\nhappy19_df\n\n\n#Happiness Rank \n\nhappy18_df = happy18_df.rename(columns= {\"Overall rank\": \"Happiness Rank\"})\n\nhappy18_df\n\nhappy19_df = happy18_df.rename(columns= {\"Overall rank\": \"Happiness Rank\"})\n\nhappy19_df\n\n\n\n#Economy (GDP per Capita)\n\nhappy18_df = happy18_df.rename(columns= {\"GDP per capita\": \"Economy (GDP per Capita)\"})\n\nhappy18_df\n\nhappy19_df = happy18_df.rename(columns= {\"GDP per capita\": \"Economy (GDP per Capita)\"})\n\nhappy19_df\n\n\n#Health (Life Expectancy)\n\nhappy18_df = happy18_df.rename(columns= {\"Healthy life expectancy\": \"Health (Life Expectancy)\"})\n\nhappy18_df\n\nhappy19_df = happy18_df.rename(columns= {\"Healthy life expectancy\": \"Health (Life Expectancy)\"})\n\nhappy19_df\n\n\n#Freedom\n\nhappy18_df = happy18_df.rename(columns= {\"Freedom to make life choices\": \"Freedom\"})\n\nhappy18_df\n\nhappy19_df = happy18_df.rename(columns= {\"Freedom to make life choices\": \"Freedom\"})\n\nhappy19_df\n\n\n#Trust (Government Corruption)\n\nhappy18_df = happy18_df.rename(columns= {\"Perceptions of corruption\": \"Trust (Government Corruption)\"})\n\nhappy18_df\n\nhappy19_df = happy18_df.rename(columns= {\"Perceptions of corruption\": \"Trust (Government Corruption)\"})\n\nhappy19_df\n\n\n#Family \n\nhappy18_df = happy18_df.rename(columns= {\"Social support\": \"Family\"})\n\nhappy18_df\n\nhappy19_df = happy18_df.rename(columns= {\"Social support\": \"Family\"})\n\nhappy19_df\n\n\n#Happiness Score \n\nhappy18_df = happy18_df.rename(columns= {\"Score\": \"Happiness Score\"})\n\nhappy18_df\n\nhappy19_df = happy18_df.rename(columns= {\"Score\": \"Happiness Score\"})\n\nhappy15_df\n\n","d8e92e61":"\"\"\"\nrenaming of year 2017\nbase label: 2015\n\"\"\"\n\nhappy17_df = happy17_df.rename(columns = {\"Happiness.Rank\": \"Happiness Rank\"})\n\nhappy17_df = happy17_df.rename(columns = {\"Happiness.Score\": \"Happiness Score\"})\n\nhappy17_df = happy17_df.rename(columns = {\"Economy..GDP.per.Capita.\": \"Economy (GDP per Capita)\"})\n\nhappy17_df = happy17_df.rename(columns = {\"Health..Life.Expectancy.\": \"Health (Life Expectancy)\"})\n\nhappy17_df = happy17_df.rename(columns = {\"Trust..Government.Corruption.\": \"Trust (Government Corruption)\"})\n\nhappy17_df = happy17_df.rename(columns = {\"Dystopia.Residual\": \"Dystopia Residual\"})\n\n#because we already know that we want to delete thouse filds \n\nhappy17_df = happy17_df.drop(columns = \"Whisker.high\") \n\nhappy17_df = happy17_df.drop(columns = \"Whisker.low\")\n\nhappy17_df","5e350796":"#insert year column at first position (index 0)\n\n#2015\nhappy15_df.insert(0, \"Year\",value = \"2015\")\n\n\n#2016\nhappy16_df.insert(0, \"Year\",value = \"2016\")\n\n\n#2017\nhappy17_df.insert(0, \"Year\",value = \"2017\")\n\n\n#2018\nhappy18_df.insert(0, \"Year\",value = \"2018\")\n\n#2019\nhappy19_df.insert(0, \"Year\",value = \"2019\")\n\n#check if it worked\nhappy18_df.head()","f4eb77ae":"#creating empty dict \nregion_dict ={}\n\n#filling with values from DataFame happy15_df \n\n#index to have each row with both data, region and country \nregion_dict = happy15_df [[\"Country\",\"Region\"]].to_dict(\"index\")\n\n\n\n[(key, value) for key, value in region_dict.items()]","d8f2bdbb":"#concatenating objects \n\n#defintion of all the sets we want to bring together\nframes = [happy15_df, happy16_df,  happy17_df, happy18_df, happy19_df]\n\nhappiness = pd.concat (frames)\nhappiness.info()\n\n#what we see is that \"Trust\" is not in one line yet","4c076c27":"happiness = happiness.drop ([\"Lower Confidence Interval\",\"Dystopia Residual\", \"Upper Confidence Interval\", \"Standard Error\"], axis = 1)\nhappiness.head()\n\n","9e534995":"happiness.info()","d043b82c":"#conversion of type str to float64 (alternatively: int)\nhappiness [\"Year\"] = happiness [\"Year\"].astype(int)\n\n#convert object data to category data \nhappiness [\"Country\"] = happiness [\"Country\"].astype(\"category\")\n#happiness [\"Region\"] = happiness [\"Region\"].astype(\"category\")\n\nhappiness.info()","bf9d36de":"#yellow is what is missig and we see that is substential \n\nplt.figure (figsize=(10,7))\n\nsns.heatmap(happiness.isnull(),yticklabels=False, cbar = False, cmap = \"plasma\")\nplt.xlabel(xlabel = \"variable names\", rotation= 0, fontsize= 20)\nplt.ylabel (ylabel= \"missing data\", fontsize = 20)\nplt.title (label = \"missing data per variable\",  fontsize = 25)\nplt.show()\n\n\n","d8162255":"#if we would not have the data \n\nhappiness[\"Region\"].fillna(\"unknown\", inplace = True)\n\nhappiness.info()","ccc8913a":"try:\n    for country in happiness.Country.unique():\n        happiness.loc[happiness['Country']==str(country),\n                              'Region']=happiness[happiness['Country']==str(country)].Region.mode()[0]\nexcept IndexError:\n    pass\n\nhappiness.info()\n","cb4dbe39":"\nplt.figure (figsize=(10,7))\n\nsns.heatmap(happiness.isnull(),yticklabels=False, cbar = False, cmap = \"plasma\")\nplt.xlabel(xlabel = \"variable names\", rotation= 0, fontsize= 20)\nplt.ylabel (ylabel= \"missing data\", fontsize = 20)\nplt.title (label = \"missing data per variable\",  fontsize = 25)\nplt.show()\n\n\n","c3cf34a0":"#lets look for the \"Region\" that are still missing\n\nhappiness [happiness[\"Region\"].isna()]\n\n#we see that we only have 6 missing values and will therefore, assign the regions manually ","261e0f1f":"happiness.loc[[32,70], \"Region\"] =  \"Eastern Asia\"\nhappiness [happiness[\"Region\"].isna()]","ae6f21be":"#lets look for the missing regions in the rest of the data \nhappiness [happiness.Region == \"Latin America and Caribbean\"]\n\n#replacing of missing values: \nhappiness.loc[[37], \"Region\"] =  \"Latin America and Caribbean\"\nhappiness [happiness[\"Region\"].isna()]","e3e1b7c8":"#lets look for the missing regions in the rest of the data \nhappiness [happiness.Region == \"Western Europe\"]\n\n\n#replacing of missing values: \nhappiness.loc[[57], \"Region\"] =  \"Western Europe\"\nhappiness [happiness[\"Region\"].isna()]\n\n\n#we see that there are no missing variables in the region category! \n\n\n","2ecf3576":"\nplt.figure (figsize=(10,7))\n\nsns.heatmap(happiness.isnull(),yticklabels=False, cbar = False, cmap = \"plasma\")\nplt.xlabel(xlabel = \"variable names\", rotation= 0, fontsize= 20)\nplt.ylabel (ylabel= \"missing data\", fontsize = 20)\nplt.title (label = \"missing data per variable\",  fontsize = 25)\nplt.show()\n\n","515f85dc":"#finding the missing trust variable \nhappiness [happiness[\"Trust (Government Corruption)\"].isna()]","214bf323":"\"\"\"\nWe indeed see that the variable missed two trust data in year 2018 and 2019\n\nReplacing the data can be made in different ways, but because we are now \n    dealing with a numeric value we have to have a better understanding of \n    the data \n\"\"\"\n\nround (happiness.describe(), 3)","4e75f5ce":"#we find out a few variables to decisde with what we want to replace the missing values\n\n#average of United Arab Emirates\n\nyear_trust = happiness.loc [:,\"Trust (Government Corruption)\":]\n\n\nmean = year_trust.mean (axis=1)\nprint (\"Mean\",\"\\n\",mean[19])\n\nmedian = year_trust.median (axis = 1)\nprint (\"\\n\",\"Median\",\"\\n\",median[19])\n\n\n#so ahappiness.loc[[57], \"Region\"] =  \"Western Europe\"\nhappiness [happiness[\"Trust (Government Corruption)\"].isna()]\n","54c432e3":"# Replacing the is.na values \n\nhappiness.loc[[19], \"Trust (Government Corruption)\"] =  0.18600\n\n#we check if we have deleted all the null vales (worked!)\nhappiness [happiness[\"Trust (Government Corruption)\"].isna()]\n","e7876012":"#one last time (hopefully)\nplt.figure (figsize=(10,7))\n\nsns.heatmap(happiness.isnull(),yticklabels=False, cbar = False, cmap = \"plasma\")\nplt.xlabel(xlabel = \"variable names\", rotation= 0, fontsize= 20)\nplt.ylabel (ylabel= \"missing data\", fontsize = 20)\nplt.title (label = \"missing data per variable\",  fontsize = 25)\nplt.show()\n\n#we see that all the values have been replaced! ","ebf41d05":"\n#correlation heatmap\n\n#definition of the correlation of all varialbe in DataFrame\ncorr = happiness.corr()\n\nfig, ax = plt.subplots(figsize = (8.5,8.5))\nax = sns.heatmap(\n    corr, \n    vmin = -1, vmax = 1, center= 0, \n    cmap= sns.cubehelix_palette (20), # insert any number larger than the correlation we want to observe! \n    square = True\n)\n\nax.set_title (\"Correlation Heatmap\", fontsize = 20)\n\n\nax.set_yticklabels (\n    ax.get_yticklabels(), \n    fontsize = 12\n)\n\nax.set_xticklabels (\n    ax.get_xticklabels(),\n    rotation = 45,\n    horizontalalignment = \"right\",\n    fontsize = 12\n)\n\n\n#if you dont iclude this line you will have the chart but will all data on top...\nplt.show()","ab3575af":"\n#correlation heatmap without mirrowing\n\n\ncorr = happiness.corr()\ndropself= np.zeros_like (corr)\ndropself [np.triu_indices_from(dropself)] = True\n\nfig, ax = plt.subplots(figsize = (9,9))\nax = sns.heatmap(\n    corr, \n    vmin = -1, vmax = 1, center= 0, \n    cmap= sns.light_palette(\"purple\"),\n    square = True, \n    mask= dropself\n)\n\nax.set_title (\"Correlation Heatmap\", fontsize = 20)\n\nax.set_yticklabels (\n    ax.get_yticklabels(), \n    fontsize = 12\n)\n\nax.set_xticklabels (\n    ax.get_xticklabels(),\n    rotation = 45,\n    horizontalalignment = \"right\",\n    fontsize = 12\n)\n\nplt.show()","f8e0e898":"\n\n#correlation heatmap with correlation values included \n\ndef halfheatmap (df, mirrow, title ): \n    corr = df.corr()\n    fig, ax = plt.subplots(figsize = (10,10))\n    cmap= sns.light_palette(\"purple\")\n    ax.set_title(title, fontsize = 20)\n    \n    if mirrow == True:\n        #Generate Heat Map, allow annotations and place floats in map\n        sns.heatmap(corr, cmap=cmap, annot=True, fmt=\".2f\")\n      #Apply xticks\n        plt.xticks(range(len(corr.columns)), corr.columns);\n      #Apply yticks\n        plt.yticks(range(len(corr.columns)), corr.columns)\n      #show plot\n    \n    else: \n        #drop selcorrelation\n        dropself= np.zeros_like (corr)\n        dropself [np.triu_indices_from(dropself)] = True\n        colormap = sns.diverging_palette (200,10,as_cmap = True)\n        sns.heatmap(corr, cmap=cmap, annot=True, fmt=\".2f\", mask=dropself)\n      # Apply xticks\n        plt.xticks(range(len(corr.columns)), corr.columns, rotation = 45, fontsize= 12);\n      # Apply yticks\n        plt.yticks(range(len(corr.columns)), corr.columns, fontsize= 12)\n        \n   # show plot\n    plt.show()\n    \nhalfheatmap(df = happiness, mirrow = False, title = \"Correlation Heatmap\")","28881ecc":"\n\n#correlation heatmap with other colorcode\n\ndef halfheatmap (df, mirrow, title ): \n    corr = df.corr()\n    fig, ax = plt.subplots(figsize = (10,10))\n    colormap = sns.diverging_palette (220,10, as_cmap = True)\n    ax.set_title(title, fontsize = 20)\n    \n    if mirrow == True:\n        #Generate Heat Map, allow annotations and place floats in map\n        sns.heatmap(corr, cmap=colormap, annot=True, fmt=\".2f\")\n      #Apply xticks\n        plt.xticks(range(len(corr.columns)), corr.columns);\n      #Apply yticks\n        plt.yticks(range(len(corr.columns)), corr.columns)\n      #show plot\n    \n    else: \n        #drop selcorrelation\n        dropself= np.zeros_like (corr)\n        dropself [np.triu_indices_from(dropself)] = True\n        colormap = sns.diverging_palette (200,10,as_cmap = True)\n        sns.heatmap(corr, cmap=colormap, annot=True, fmt=\".2g\", mask=dropself)\n         # Apply xticks\n        plt.xticks(range(len(corr.columns)), corr.columns, rotation = 45);\n        # Apply yticks\n        plt.yticks(range(len(corr.columns)), corr.columns)\n        \n    # show plot and save it \n    plt.savefig(\"Happiness.Correlation.Heatmap.png\")\n    plt.show()\n    \nhalfheatmap(df = happiness, mirrow = False, title = \"Correlation Heatmap\")","47841099":"#overview:\nsns.pairplot(happiness)\nplt.show()","603db918":"happiness.info()\n","a0e269d7":"#we make the analysis with statsmodels.api as sm \n\nimport statsmodels.api as sm \n\nx = happiness.iloc [:,[0,5,6,7,8,9,10]]\n\ny = happiness[\"Happiness Score\"]\n\nX = sm.add_constant(x)\nmodel = sm.OLS(y, X)\nest = model.fit()\n\nprint(est.summary())","d9095657":"happiness_dummy = pd.get_dummies (happiness, columns = [\"Region\"])\nhappiness_dummy.head()\nhappiness_dummy.info()","84641ebc":"\nimport statsmodels.api as sm \n\nx = happiness_dummy.iloc [:,[0,4,5,6,7,8,9,10,11,12,13,14,15,16]]\n\ny = happiness_dummy[\"Happiness Score\"]\n\nX = sm.add_constant(x)\nmodel = sm.OLS(y, X)\nest = model.fit()\n\nprint(est.summary())","8c1e26b6":"#first sorting the scores based on the regions! \n\ngrouped_happiness = happiness.groupby ([\"Region\"])[[\"Happiness Score\", \"Year\"]].aggregate(np.median).reset_index().sort_values (\"Happiness Score\")\ngrouped_happiness.info()","8a58c80f":"#plotting hte results (x and y are given like that to better allows to read the ocuntries)\nchart = sns.barplot (x= grouped_happiness [\"Happiness Score\"],y = grouped_happiness [\"Region\"],saturation = 1.2, palette = \"vlag\"  )\n\nchart.figure.set_size_inches (10,10)\nplt.title (label = \"Happiness Score by Region \\n(2015 - 2019)\", fontsize = 20)\nplt.show()","0303182a":"#plotting hte results (x and y are given like that to better allows to read the ocuntries)\nplt.figure (figsize=(10,10))\nchart = sns.barplot (x= happiness [\"Happiness Score\"],y = happiness [\"Region\"], palette = (\"BrBG\"), hue=happiness [\"Year\"], saturation = 1.2)\n                 \nplt.title (label = \"Happiness Score by Region \\n(2015 - 2019)\", fontsize = 20)\nplt.show()","f1563700":" \nplt.figure (figsize=(10,10))\n\nchart = sns.distplot (a= happiness [\"Happiness Score\"],bins= happiness [\"Year\"], color = \"Red\")\n                 \nplt.title (label = \"Happiness Score distribution \\n(2015 - 2019)\", fontsize = 20)\nplt.show()","dc9d0b91":"plt.figure(figsize=(15,8))\n\nsns.scatterplot(x='Happiness Score', y='Economy (GDP per Capita)', sizes = ((\"Economy (GDP per Capita)\")*100), hue='Region',data=happiness)\nplt.title (\"Relationship Happiness Score and GDP per Capita \\n(divided by Regions between 2015-2019)\", fontsize = 20)\nplt.xlabel('Happiness Score',size=12)\nplt.ylim(0,2.5)\nplt.xlim (2,8)\nplt.ylabel('Economy (GDP per Capita)', size =12)\nplt.show()","54e94ba1":"plt.figure(figsize=(12,10))\nsns.scatterplot(x='Happiness Score', y='Economy (GDP per Capita)',data=happiness, sizes = ((\"Economy (GDP per Capita)\")*100), alpha =0.8, color = \"lime\")\n\nplt.title (\"Relationship Happiness Score and GDP per Capita \\n(between 2015-2019)\", fontsize = 20)\n\nplt.xlabel('Happiness Score',size=12)\nplt.ylim(0,2.5)\nplt.xlim (2,8)\nplt.ylabel('Economy (GDP per Capita)', size =12)\nplt.figure(figsize=(15,8))\nplt.show()","9bff6477":"\n\nplt.figure(figsize=(12,8))\nsns.regplot (happiness [\"Happiness Score\"], happiness [\"Economy (GDP per Capita)\"], x_estimator=np.mean, ci = 80, color = \"darkred\")           \nplt.title (\"Relationship Happiness Score and GDP per Capita \\n(between 2015-2019)\", fontsize = 20)\n\nplt.xlabel('Happiness Score',size=12)\nplt.ylim(0,2.5)\nplt.xlim (2,8)\nplt.ylabel('Economy (GDP per Capita)', size =12)\nplt.show()","0d1e5951":"#for the following we need at least version 3.4+ (so we have a look)\n\nimport sys \n\nprint (sys.version)","aff387a7":"#import additional libaries \n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score ","afe70d8d":"#definition of dependend (y= target) and indipendend variables (xns)\n#note: we are not including everyhting (only the dummy and numerical values)\n\nX= happiness_dummy.iloc [:,[0,4,5,6,7,8,9,10,11,12,13,15,16]]\ny = happiness_dummy.iloc [:, 3] #target, what we are interested in\nX.head() #check if it worked ","8b5461ee":"#establish training and test sets \n\nX_train, X_test, y_train, y_test = train_test_split (X,y, test_size =0.2)","986736a7":"#create linear regression\n\nreg = LinearRegression()\n","09a940c8":"reg.fit(X_train, y_train)","f8838769":"#predict the test data (80% of dataset)\ny_pred = reg.predict (X_test)","7e707f97":"#comute the root mean squared error (RMSE)\nrmse = np.sqrt (mean_squared_error(y_test, y_pred))\nprint (\"RMSE: {}\". format(rmse))\n\n#note between 0.45 and 0.52 that is a very high fluctratin rate! ","1601c00f":"def regression_mode_cv( model, k = 8): \n    scores = cross_val_score (model, X, y, scoring = \"neg_mean_squared_error\", cv = k)\n    \n    rmse= np.sqrt(-scores)\n    print (\"Reg rmse:\", rmse)\n    print(\"Reg mean: \", rmse.mean())\n    print(\"Reg mean:\", rmse.mean())","baa05337":"#prints the 8 rsme and then the mean\nregression_mode_cv(LinearRegression())","3c9ca5a0":"    \nimg= mpimg.imread(\"..\/input\/overunderfitting\/overfitting underfitting.png\")\nplt.figure(figsize=(15,11)) #make picture larger \nimgplot = plt.imshow(img) \nplt.axis (\"off\") #remove the axis and tickers \nplt.show()","a4585742":"#Ridge model\n\nfrom sklearn.linear_model import Ridge\nregression_mode_cv(Ridge())","45eab0e2":"#Lasso model \n\nfrom sklearn.linear_model import Lasso\nregression_mode_cv(Lasso())","9599768d":"#barplot for economy \nfigure = sns.violinplot(happiness_dummy[\"Year\"], happiness_dummy[\"Happiness Score\"])\nfigure.figure.set_size_inches (10,7)\n#plt.savefig(\"Age_Attrition.png\")\nplt.show()\n","4f5c5cde":"plt.title (\"Infuence of time on Happiness Score\", fontsize= 20)\n\nfigure = sns.regplot(happiness_dummy[\"Year\"], happiness_dummy[\"Happiness Score\"], x_estimator=np.mean, x_bins = 15, color = \"darkred\", ci = 90)\nfigure.figure.set_size_inches (10,7)\nplt.xlabel(\"Year\", fontsize = 15)\nplt.ylabel(\"Happiness Score\", fontsize = 15)\nplt.show()","e9aced0e":"plt.title (\"Infuence of Freedom on Happiness Score\", fontsize= 20)\n\nfigure = sns.regplot(happiness_dummy[\"Freedom\" ], happiness_dummy[\"Happiness Score\"], x_estimator=np.mean, x_bins = 15, color = \"darkred\", ci = 90)\nfigure.figure.set_size_inches (10,7)\nplt.xlabel(\"Freedom Score\", fontsize = 15)\nplt.ylabel(\"Happiness Score\", fontsize = 15)\nplt.show()","97553dea":"figure = sns.scatterplot(happiness_dummy[\"Family\"],happiness_dummy[\"Happiness Score\"], color = \"darkred\")\n\nfigure.figure.set_size_inches (10,7)\n\nplt.show()","6ebebf00":"figure = sns.lineplot(happiness_dummy[\"Family\"],happiness_dummy[\"Happiness Score\"], color = \"navy\")\n\nfigure.figure.set_size_inches (10,7)\n\nplt.show()","bb907e14":"Correlation heatmap interpretation: \n\nDefinition: \n- ether dark red or dark turquoise indicates a strong correlation (red = positive, turquoise = negative) \n- light collors show correlations coloser to 0 \n- the correlations with the respective variable itself has been deleted \n\nObservations: \n- what we are interested is the \"happiness ranking\" and the \"happiness sccore\" \n- the next point of interest which values are influening this variables\n- we observe that \"Economy (GDP per Capita\", \"Family\", and \"Health (..)\" seam to have the strongest influence (darkest collors) and thouse variables are futher strongly correlated which one another (which has to be taken into account for the regression discussion below) \n- the time of the observation does not seam to play a huge rule (very light colors for all other variables) >> that could indicate that the relationships have not changed over time (but caution is being required because it could also be observed that the happiness score has shifted from one country to another) \n\nNote: \n- the correlation heatmap only shows an overview and only (as the name suggests) the correlations and not if they are significatn to further inspect the data we need t values (below is an OLS-regression discussion) \n- I personly like a dual color code because you can if you just want to see the strengh of a relationship observe the darkness of a square and if you want to know the DIRECTION of the relationship you know it as well ","1672f85a":"The very nice feature in the follwing graph is that we can zoom in and get a clearer picture, in addition, similairly to the correlation heatmap, we see what is worth spending our attention on. Mittle line shows eachvarialbe distribution ad is helpful to if we have screwed data or exteme outliers ","9da2cd9c":"desision for value to replace NaN: \n    \n    - We see that the mean and median are not to close with one another\n    - just using one of the values whould ignore the country specific sources and also the development over the time\n    - replacing it with 0 would be completely disadvisably, as even with the std in account the value is more than likely not 0 \n    - because of the above mentioned we go back to the inital year data and replace the values manually","a3d78107":"# Visualization of observations","67891ad2":"# Analysis includes: ","e6087c20":"****Comment on 'Freedom' Regression: We observe that the score of freedom has indeed an positive influence on the happiness score (altough only significant on the 10% sig. level)","36f1fa1f":"# Visual overview ","4dbe07f5":"Note: \n- if you want me to write a post about the differnt models avaialbe and\/ or over and underfitting, let me know! \n- the reason why I am not going into detail here is that is would easily take the same space than the rest of the code! \n","2b3a0a4e":"# From OLS to Machine Learning (ML) ","cbe587a4":"options: pandas.DataFrame.\n- fillna()\n- replace()\n- interpolate() \n- map() \n    ","d75d8b8c":"However, we see that in \"Trust\" some data points are still missing. Generally speaking it is always good to replace missig variables with an non-NA \/ non-NaN value as that will also incrase the predictabilty of our model. However, somethimes it is worth dismissing certain rows as the values is likely to be an outlier. Other options are to manually try to figure out what the value is. ","c1bf8665":"# Preperation for cleaning \n","fd9a7c58":"so we have the years 2018 and 2019 means and median and we are going to use them therefore !\nnamely 0.18600 ad 18600 >> hence, no change between the years which means we are dont have to account for that for the change between 2018 and 2019 but we have a change from 2017 to 2018 (decrease)","d33ae06a":"to include the regions \/ or countries we have to introduce dummy variables\n- we include regions, as we have less of them and still want to be able to analyze the data \n- we can always zoom into one specific country later on ","92ff2581":"- one aspect of looking at data is to explain what happend and what is happening at the moment\n- antoher factor is to predict what the future might hold and what the influencing factors are \n- in order to get there ML is a powerful tool to help us to see how the a specific area of life might look like and what factors have which influence on them. \n- there are differnent types of machine learning (supervised, unsupervised, reinforcement,  have a look at the other forms and a neat explaination: https:\/\/machinelearningmastery.com\/types-of-learning-in-machine-learning\/) \n- we are going to supervised learning, as we have a clear target variable (Y = \"Happiness Score\") and labeled input data, we are specifically interested how the different influending factors (like GDP, Generosity,...) are influencing the depended variable (Y) and how that has changed over time and between countries. \n","6c9d85a8":"# Filling missing values \n","da4a6649":"Comment on Regression year: we bassically see, that the inlfuence of the years was slightly negative but not significant on the happiness score ","8e455bf1":"# \"What makes people in a country happy?\"","48c07ef7":"Note: I just googled the regions and found out where they are and looked if we have a similar varialbe in the preperation for the replacement of the null values ","a353a8e8":"prior we made a dictionary with the regional data categories that we not fil in ","13810a9d":"Comment: \n- We see a positive relationship betwen the GDP per capita and the Happiness Score\n- Some regions are subsequentially lower in both measurments","52a62234":"## Region and Happiness Score by year","b83427b4":"# OLS analysis (for all numerical indipended variables) ","fac8fd79":"use of concatenate formula:\n    axis = 0, python stacks the data under the data \n    axis = 1, python stacks the data right to the data \n    \n- imortant: if labeles are the same the data can is automatically put in the right column\/ row ","cf32cd6f":"Note: year is a object type and we have to convert it ","2e9371e0":"a more visual way of presenting the null values: ","64f78e21":"\"\"\"Western Europe as the Iseeland offically belongs to Turkey and \nTurkey is being classified as Western Europe in the base Dataset \nof 2015 (arguable that is not entirely true...)\"\"\"","c708934d":" Delete unnessesary columns:  \n","b65e9491":"# inspect datasets","3e133706":"comment: shows the distribtion on the average of the years for the happiness Scores","76142292":"NOTE(!): everytime you run the code you expect a different error. The reason for that is that the dataset is randomly split into parts of 0.8 and 0.2 and trains and test on different datapoints every time. By setting a sead you could prevent that, but we don\u00b4t want that as we dont know if we then have to deal with a high or low error. The following shows a more reliable solution. ","388c4033":"# OLS with categorical data ","2af18787":"Very briefly but we have to say about why ML model do perform badly:   \n- underfitting: \n    - a model that is not able to capture the variety of the data as it is not enough complex (performs bad on test and training dataset) \n- overfitting: \n    - a model that is to complex and captures the \"noise\", this model fits the training data perfectly well but does not recognize the underlying pattern of the data (and that what is ultimelelty of interest) \n    - nowerdays, overfitting is the bigger problem as often data is readily availabe\n    \n- optimum: \n    - directly inbetween the two (the polynom degree and the error of the predicton is at the lowest) \n    \nPlease see the explanatory picture below: \n","c9155861":"we still have a few missing variables ","d1a3a065":"# Distplot (to bin regions)","564c300f":"interpretation of OLS with region variable: \n- we see an imporvement in the predictability of the model (R^2 form 77% to 82%)\n- the variables that have been included in the old model are still signficiant (only freedom does is only sign. on an apha of 10%)\n- the region variables are all signficiant ofn the 5% alpha level \n- not surprisingly the constant is lower (this is as we have included another variable that adds to each prediction value)\n- The most relevant factors are: \n    - The GDP per capiata\n    - The Year\n    - Freedom\n    - Trust\n    - Generosity \n    - the Region \n- it is important to note that the different values are very likely correlated with one another, we have also seen that in the correlation heatmap. \n- While interpreting one induencing factor (e.g. Economy) one has to pay attention to the units and model (here linear) \n- For instance: we expect the  'Happiness Score' to go up by 0.8158 if the Trust measurment score goes up by 1 point. this is true if everything else is being held constant, this is on a 5%level significance level. ","6609f05e":"Comment to the shape:  \n- more criteria in the first years while merging we have to keep that in mind. \n- different number of countries is an additional concern... -  - the dystopia.residual (which is the hypothetically worst-case state and delivers the baseline with which you can compare the other states).","9658e6f2":"In order to avoid the fluctration of the model and have a more reliable error we create various folds (dependend on the dataset, we use eight). The ML algorithm is tested on one fold and afterwards fitted on the remainig data. The mean of the fold scores is being usded and presents a more accurate and less fluctrative model estimator. \n\n- cv = cross validation","26aa331a":"Note to Lasso: \n- does not perform better than  linear regression. \n- this is because the default alpha is not set accurately\n    ","4cb788bd":"We observe an positive correlation between more family members and the happiness score, interestingly enough we do not seem to see a tipping point, it just seems the more family members the better ","3c126049":"regulaized alternatives to linear regression: ","e71648f7":"lineplot can be good to show the variation in data and simultanously show a trend ","26fc36c3":"# Continue Data Visiualization","d4bb249f":"Note: in most cases data is incomplete. Although we have a very complete and well labeled dataset some values are not given. The missing values are of categorial data type and, therefore, have to be replaced with categories rather than simply with fillna() (fill in NaN (Not A Number) valus with like the mean, with previous once, with next one,  the most frequent value, meadian, 0, or any other numerical data )\n\n\nAlteration: we can also solve the issue with the blow stated replacement oportunity! ","cde1f583":"# Dicovery visualization II","7f6d9df8":"Note: We see the regional happiness score in most regions slightly increasing \n    - however, in Astrilia and New Zealand stayed amost unchanged \n    - in Latin America and Caribean the happiness score declined of the time slightly\n    ","97b5af2d":"We are not looking at single factors that we have identified as highly relevant for the happiness score","1bb35f4f":"Note to ridge model: slightly better score than the linear regression. Partly overcomes the overfitting problem.\n    ","71194c0e":"\n\ncreate an dictonary of countries and regions using DataFrame.to_dict()","2fb338ba":"# Cross Validation","5e9205c7":"The question now is, which models should be use: clearly the model with the higher R^2 and the region dummies. \n","74f654af":"prior to presenting an OLS we have to classify our target variable (also indipended or y variable)\n- as happiness score and happiness rank are strongly correlated (seen in heatmap (very dark colors) and above discribtive statistic (strong non-random relationship), and inuitively it makes sence too ) we have to leave it out of our OLS model, otherwise we have a confounding variable included\n- country and region are both not numberical but are likely to have an influence, we will come to that later\n- Note to OSL: by default the confidence interal is 95%","3fcf4004":"___","b7de1270":"# Merging the datasets ","4fb99f0a":"So: Why are we happy? \n    ","590c354f":"We need a test dataset as we are aiming to predict data the computer\/machine has never seen and we have to \"test\" if our trainng data set has helped to produce a model that is able to predict accurate results. ","55839e85":"- explore and discribe data \n- merge data from 2015-2019 \n- clean data (replace NaN values and delete redudant variables) \n- visualize data (and safe charts) \n- prediction analyzis (with ML)\n- interpretation and answer reseach quesiton (what makes people in a country happy?)\n- data cleaning (replace values, check NA and NaN) \n- merging of yearly datasets \n- statistical discribtion of data \n- upload picture \n- correlation heatmap (with and without mirrow) \n- OLS analysis (with & and without dummy variables for countries) \n- sorted barplots (by region, ... ) ","1abdc95b":"I love unconventional economical data and the world happiness report does fit into that category quit well. I gave it a try and visualized a few findings (after cleaning the data). \n\nHappy reading! \n\n\nAnna-Marie \n\n\n\nPS: I am more than happy to recive any feedback and nudges for future work. If you want me to elaborate on any points, just write me. ","f5cb2f02":"- Context and data source (kaggle.com) \nhttps:\/\/www.kaggle.com\/unsdsn\/world-happiness\n\n- for help with the graphical representation I used this (amazing) webside: http:\/\/python-graph-gallery.com\n","436a4243":"Unfortunately, I can not answer this question. But I can say that with this dataset a clear correlation of the observable factors like the wealth of a country (GDP), the number of family members, and non curuption are adding to happiness. Furhter, self reported factors like Generosity influence postively as well. All that is can be observed, and further there seems to be a constant difference between the regions in our world (constant in that sense that the relationships don\u00b4t change over time). Very likely variables like the environment, the future outlook and the friendliness for doing business also add to the happiness of people. Whatsoever, the dataset gives a interesting glimes into what happiness is made of. And we should keep in mind, the data is accumulated and does not present individuals subjective stance about happiness or anything else. ","7d862d9c":"Comment: \n- over the years the average happiness scorers have been different different evaluated regions\n- Australia and New Zealand are leading the list, directly followed by North America on the bottom of the list wee see Sub-Saharan Africa","a863d0a0":"Note to OLS results: We have an R^2 of approx. 77% which indicates that 77% of the distribution of y (happiness score) can be explained by the underlying OLS model \n - all included variables are significant on a 5% significantce level \n - other than year, all variables are having a positive influence on y (happiness score)\n - if we change y to happiness rank we get almost the same results (not surprisingly as they are strongly correlated) \n - again: please do not include happiness rank and happiness score as they would confuse the model (although it seems to be a better fit on the first glance) \n - I advice to include more and more explainatory variables to see the contribution to the R^2 measurement "}}