{"cell_type":{"b5582a5c":"code","7def22ab":"code","78c15cc7":"code","49576814":"code","7b7cd068":"code","3319c008":"code","127b867a":"code","cfd620a7":"code","7d363487":"code","2e0cb207":"code","046e7d58":"code","ae31e744":"code","04c19354":"code","2c696418":"code","c927242e":"code","7fc3514d":"code","bfed646a":"code","6a6a53bf":"code","797bc9d8":"code","a80053e2":"code","c677f620":"code","663c16fa":"code","1094f62d":"code","88d1ad12":"code","512c4aa7":"code","e1646b05":"code","d268c83d":"code","897f61fa":"code","a5d8f507":"code","b2d128d2":"code","4f079826":"code","e71ea5d3":"code","34c0dda1":"code","f1fd9262":"code","6b31ae42":"code","44c0af20":"code","7eeb56bb":"code","98455990":"code","24fb45af":"code","f08983ae":"code","d7a22761":"code","d364bd7b":"code","49172316":"code","979ea68d":"code","c08ba6b8":"code","1e7998f8":"code","c3a7f287":"code","05e0031f":"code","15e17c41":"code","8161c986":"code","070658c3":"code","7abc020b":"code","e1266969":"code","37e8e7ce":"code","7df0202f":"code","7d748511":"code","46a2d4fa":"code","de8db313":"code","b5ed9663":"code","ecf198de":"code","815c6bcc":"code","d355d246":"code","3d7a1b88":"code","49bde14a":"code","c3d4a553":"code","6a8ad6b5":"code","1d56a8b6":"code","45686f7d":"code","05fb94b1":"code","c7a93faf":"code","42965e57":"code","ba0ce1c7":"code","b023e531":"code","15ebe388":"code","d6f667fc":"code","a56e6d08":"markdown","7919c843":"markdown"},"source":{"b5582a5c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","7def22ab":"#Importing Libraries\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","78c15cc7":"#Data Acquistion\ntrain_data=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest_data=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","49576814":"# First few rows of train data\ntrain_data.head()","7b7cd068":"# First few rows of test data\ntest_data.head()","3319c008":"#Shape of train and test data\ntrain_data.shape,test_data.shape","127b867a":"#Count of null values of individual feature of train data\ntrain_data.isnull().sum()","cfd620a7":"#Count of null values of individual feature of test data\ntest_data.isnull().sum()","7d363487":"#Creating a column named target for test data with NA values\ntest_data['target']=np.nan","2e0cb207":"#Checking head of test data\ntest_data.head()","046e7d58":"#Stacking train and test data set \ndataframe=pd.concat([train_data,test_data],ignore_index=True)","ae31e744":"#Checking head of dataframe\ndataframe.head()","04c19354":"#Shape of dataframe\ndataframe.shape","2c696418":"dataframe.info()","c927242e":"#Checking Null Values for each column if any \ndataframe.isnull().any()","7fc3514d":"#Counting Null values of each column\nnull=dataframe.isnull().sum()\nnull","bfed646a":"#Bar Plot of Features and their Missing Values\nplt.figure(figsize=(16,4))\nnull.plot(kind='bar',color='blue')\nplt.title('Features of Data and there Missing Values Count')","6a6a53bf":"#top 10 keywords from tweets\ndataframe['keyword'].value_counts(dropna=False).head(10)","797bc9d8":"#Top 20 keywords from Tweets that are not actually caused disaster\ntemp1=dataframe.groupby('target')['keyword'].value_counts()[0][:20]\ntemp1","a80053e2":"#Top 20 keywords from tweets that have caused disaster\ntemp2=dataframe.groupby('target')['keyword'].value_counts()[1][:20]\ntemp2","c677f620":"fig,axes=plt.subplots(2,1,figsize=(18,16))\ntemp1.plot(kind='bar',color='red',ax=axes[0]).set_title('Top 20 Fake Disaster Keywords')\ntemp2.plot(kind='bar',color='violet',ax=axes[1]).set_title('Top 20 Real Disaster Keywords')","663c16fa":"#Top 10 Locations of the Tweets\ndataframe.location.value_counts(dropna=False).head(10)","1094f62d":"#Top 10 locations from where fake Disaster tweets have received\ntemp3=dataframe.groupby('target')['location'].value_counts()[0][:10]\ntemp3","88d1ad12":"#Top 10 locations from where real Disaster tweets have received\ntemp4=dataframe.groupby('target')['location'].value_counts()[1][:10]\ntemp4","512c4aa7":"fig,axes=plt.subplots(2,1,figsize=(18,12))\ntemp3.plot(kind='bar',color='red',ax=axes[0]).set_title('Top 10 Fake Disaster Locations')\ntemp4.plot(kind='bar',color='violet',ax=axes[1]).set_title('Top 10 Real Disaster Locations')","e1646b05":"#Checking Target Variable\ndataframe.target.value_counts(dropna=False)","d268c83d":"sns.set_style('darkgrid')\nplt.figure(figsize=(8,4))\nsns.countplot(dataframe['target'])","897f61fa":"#Clear Report of th data using Pandas_Profile_Report \nfrom pandas_profiling import ProfileReport\nprofile = ProfileReport(dataframe)\nprofile","a5d8f507":"#Removing leading and ending spaces of string\ndataframe['text']=dataframe['text'].apply(lambda x: x.strip())","b2d128d2":"#Text Lowercasing\ndataframe['text']=dataframe['text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\ndataframe.head()","4f079826":"#Removing Punctuations\nimport string\npunct_dict=dict((ord(punct),None) for punct in string.punctuation)\nprint(string.punctuation)\nprint(punct_dict)","e71ea5d3":"for i in range(0,dataframe.shape[0]):\n    dataframe['text'][i]=dataframe['text'][i].translate(punct_dict)\ndataframe['text'].head()    ","34c0dda1":"#Removing Stop Words\nfrom nltk.corpus import stopwords\nstop=stopwords.words('english')\nlen(stop)","f1fd9262":"dataframe['text']=dataframe['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\ndataframe['text'].head()","6b31ae42":"#Removing Numbers\ndataframe['text']=dataframe['text'].apply(lambda x: ''.join([x for x in x if not x.isdigit()]))\ndataframe['text']","44c0af20":"#Tokenization\nfrom nltk.tokenize import word_tokenize\nfor i in range(0,len(dataframe['text'])):\n    dataframe['text'][i]=word_tokenize(dataframe['text'][i])\ndataframe['text']    ","7eeb56bb":"#Lemmatization\nfrom nltk.stem import WordNetLemmatizer\nlem=WordNetLemmatizer()\ndataframe['text']=dataframe['text'].apply(lambda x: ' '.join(lem.lemmatize(term) for term in x))\ndataframe['text']","98455990":"#Converting datatype of target variable from float to int\ndataframe['target']=dataframe['target'][:7613].astype('int')\ndataframe.head()","24fb45af":"#Removing URL's\ndataframe['text'] = dataframe['text'].str.replace('http\\S+|www.\\S+','',case=False)\ndataframe['text']","f08983ae":"#List of Tokens\nall_words=[]\nfor msg in dataframe['text']:\n    words=word_tokenize(msg)\n    for w in words:\n        all_words.append(w)        ","d7a22761":"#Frequency of Most Common Words\nimport nltk\nfrequency_dist=nltk.FreqDist(all_words)\nprint('Length of the words',len(frequency_dist))\nprint('Most Common Words',frequency_dist.most_common(100))","d364bd7b":"#Frequency Plot for first 100 most frequently occuring words\nplt.figure(figsize=(20,8))\nfrequency_dist.plot(100,cumulative=False)","49172316":"#Disaster Tweets\ndisaster_tweets=dataframe[dataframe['target']==1]['text']\ndisaster_tweets","979ea68d":"fake_disaster_tweets=dataframe[dataframe['target']==0]['text']\nfake_disaster_tweets","c08ba6b8":"#Word Cloud for Real Disaster Tweets\nfrom wordcloud import WordCloud\nplt.figure(figsize=(16,10))\nwordcloud1=WordCloud(width=600,height=400).generate(' '.join(disaster_tweets))\nplt.imshow(wordcloud1)\nplt.axis('off')\nplt.title('Disaster Tweets',fontsize=40)","1e7998f8":"#Word Cloud for Fake Disaster Tweets\nplt.figure(figsize=(16,10))\nwordcloud2=WordCloud(width=600,height=400).generate(' '.join(fake_disaster_tweets))\nplt.imshow(wordcloud2)\nplt.axis('off')\nplt.title(\"Fake Disaster Tweets\",fontsize=40)","c3a7f287":"#Feature extraction using Tfidf Vectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidfVect=TfidfVectorizer(max_features=5000,stop_words='english')\n\ntfidf=tfidfVect.fit_transform(dataframe['text'])","05e0031f":"#Shape\ntfidf.shape","15e17c41":"#Splitting data into train and test\ntrain=tfidf[:7613]\ntest=tfidf[7613:]","8161c986":"#splitting train data into training and validation sets \nX_train=train[:5330]\nX_valid=train[5330:]\ny_train=dataframe['target'][:5330]\ny_valid=dataframe['target'][5330:7613]\n","070658c3":"#Importing Classification algorithms\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier","7abc020b":"#Naive Bayes Model\nnaive=MultinomialNB()\nnaive_model=naive.fit(X_train,y_train)\nnaive_model","e1266969":"#Prediction\npred=naive_model.predict(X_valid)\npred","37e8e7ce":"#Important Metrics to know the Performance of Model\nfrom sklearn.metrics import classification_report,confusion_matrix,precision_score,recall_score,f1_score,accuracy_score\nprint('Classification Report',classification_report(y_valid,pred))\nprint('Confusion Matrix',confusion_matrix(y_valid,pred))\nprint('Accuracy Score',accuracy_score(y_valid,pred))\nprint('Precision Score',precision_score(y_valid,pred))\nprint('Recall Score',recall_score(y_valid,pred))\nprint('F1 Score',f1_score(y_valid,pred))","7df0202f":"#Cross Validation\nfrom sklearn.model_selection import cross_val_score\ncross_val_score(naive_model,X=X_train,y=y_train,cv=5)","7d748511":"#Predicting final test data\nfinal_naive_pred=naive_model.predict(test)\nfinal_naive_pred=pd.Series(final_naive_pred)\nfinal_naive_pred.value_counts()","46a2d4fa":"#Logistic Regression\nlog=LogisticRegression()\nlog_model=log.fit(X_train,y_train)\nlog_model","de8db313":"#Prediction\nfrom sklearn.preprocessing import binarize\npred2=log_model.predict_proba(X_valid)\npred3=log_model.predict(X_valid)","b5ed9663":"#Important Metrics used to know the performance of the model\nprint('Classification Report',classification_report(y_valid,pred3))\nprint('Confusion Matrix',confusion_matrix(y_valid,pred3))\nprint('Accuracy Score',accuracy_score(y_valid,pred3))\nprint('Precision Score',precision_score(y_valid,pred3))\nprint('Recall Score',recall_score(y_valid,pred3))\nprint('F1 Score',f1_score(y_valid,pred3))","ecf198de":"#Cross Validation\nfrom sklearn.model_selection import cross_val_score\ncross_val_score(log_model,X=X_train,y=y_train,cv=5)","815c6bcc":"#ROC_AUC Curve \nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\n\n\n\nfpr,tpr,thresholds=roc_curve(y_valid,pred2[:,1])\nplt.figure(figsize=(10,8))\nplt.plot(fpr,tpr)\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate ')\nplt.legend()\nplt.title('ROC Curve')","d355d246":"#Final Prediction of test data \nfinal_log_pred=log_model.predict(test)\nfinal_log_pred=pd.Series(final_log_pred)\nfinal_log_pred.value_counts()","3d7a1b88":"# Fitting Decision Tree calssifier Model to the Training set\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(random_state=101)\ndt_model=dt.fit(X_train,y_train)\ndt_model","49bde14a":"#prediction\npred4=dt_model.predict(X_valid)\npred4","c3d4a553":"#Cross Validation\nfrom sklearn.model_selection import cross_val_score\ncross_val_score(dt_model,X=X_train,y=y_train,cv=5)","6a8ad6b5":"#Important Metrics to know the Performance of Model\nfrom sklearn.metrics import classification_report,confusion_matrix,precision_score,recall_score,f1_score,accuracy_score\nprint('Classification Report',classification_report(y_valid,pred4))\nprint('Confusion Matrix',confusion_matrix(y_valid,pred4))\nprint('Accuracy Score',accuracy_score(y_valid,pred4))\nprint('Precision Score',precision_score(y_valid,pred4))\nprint('Recall Score',recall_score(y_valid,pred4))\nprint('F1 Score',f1_score(y_valid,pred4))","1d56a8b6":"#Predicting final test data\nfinal_dt_pred=dt_model.predict(test)\nfinal_dt_pred=pd.Series(final_dt_pred)\nfinal_dt_pred.value_counts()","45686f7d":"#Random Forest Model\nrf=RandomForestClassifier(n_estimators=100,max_features='sqrt')\nrf_model=rf.fit(X_train,y_train)\nrf_model","05fb94b1":"#Prediction\npred5=rf_model.predict(X_valid)\npred5","c7a93faf":"#Important Metrics used to know the Performance of Model\nprint('Classification Report',classification_report(y_valid,pred5))\nprint('Confusion Matrix',confusion_matrix(y_valid,pred5))\nprint('Accuracy Score',accuracy_score(y_valid,pred5))\nprint('Precision Score',precision_score(y_valid,pred5))\nprint('Recall Score',recall_score(y_valid,pred5))\nprint('F1 Score',f1_score(y_valid,pred5))","42965e57":"#Predicting Final test data\nfinal_rf_pred=rf_model.predict(test)\nfinal_rf_pred=pd.Series(final_rf_pred)\nfinal_rf_pred.value_counts()","ba0ce1c7":"#Final sumbmission\nnaive_pred=pd.DataFrame(final_naive_pred, columns=['target'])\nnaive_pred\ntest_data1=pd.concat([test_data['id'],naive_pred], axis=1)\nfinal_sub1=test_data1.to_csv('final_naive_submission.csv', index=False, header=True)","b023e531":"#Final sumbmission\nlog_pred=pd.DataFrame(final_log_pred, columns=['target'])\nlog_pred\ntest_data2=pd.concat([test_data['id'],log_pred], axis=1)\nfinal_sub2=test_data2.to_csv('final_log_submission.csv', index=False, header=True)","15ebe388":"#Final sumbmission\ndt_pred=pd.DataFrame(final_dt_pred, columns=['target'])\ndt_pred\ntest_data3=pd.concat([test_data['id'],dt_pred], axis=1)\nfinal_sub3=test_data3.to_csv('final_dt_submission.csv', index=False, header=True)","d6f667fc":"#Final sumbmission\nrf_pred=pd.DataFrame(final_rf_pred, columns=['target'])\nrf_pred\ntest_data4=pd.concat([test_data['id'],rf_pred], axis=1)\nfinal_sub4=test_data4.to_csv('final_rf_submission.csv', index=False, header=True)","a56e6d08":"Out of all the models, the best model is naive bayes with accuracy of 0.76","7919c843":"**Text Preprocessing**"}}