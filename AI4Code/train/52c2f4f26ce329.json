{"cell_type":{"d8359986":"code","d603fc7c":"code","7082476b":"code","1fcdbdd7":"code","560c2972":"code","372669d7":"code","f8d9ba69":"code","29ca32a9":"code","4c3378bf":"code","af5d15fa":"code","d3a4d3c5":"code","8b7ec4f2":"code","ca2b22ce":"code","d303ec0d":"code","b770f07c":"code","676d3ee4":"code","5a99815d":"code","8cf687fc":"code","13d8c4a5":"code","0d4eab47":"code","9334cb83":"code","6dbb8236":"markdown","c99f8407":"markdown","9222e572":"markdown","d811f0ea":"markdown","9865c0d6":"markdown","fd65367f":"markdown","4a6e9e56":"markdown","cf82546a":"markdown","732f1b25":"markdown","0591ee69":"markdown"},"source":{"d8359986":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","d603fc7c":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nimport gc\ngc.enable()","7082476b":"df_train = pd.read_csv('..\/input\/train.csv')","1fcdbdd7":"df_test = pd.read_csv('..\/input\/test.csv')","560c2972":"df_sub =  pd.read_csv('..\/input\/sample_submission.csv')","372669d7":"df_train.shape,df_test.shape","f8d9ba69":"df_train.head()","29ca32a9":"df_test.head()","4c3378bf":"total = df_train.isnull().sum().sort_values(ascending=False)\npercent = (df_train.isnull().sum()\/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head()","af5d15fa":"def plotCorr(df,col,K=10,ascending = False):\n    fig, ax = plt.subplots()\n    fig.set_size_inches(10, 10)\n    K=10\n    corrmat = df.corr()\n    cols = corrmat.nsmallest(K,'target')['target'].index if ascending else corrmat.nlargest(K,'target')['target'].index\n    cm = np.corrcoef(df[cols].values.T)\n    sns.heatmap(cm,cbar=True,annot=True,yticklabels=cols.values,xticklabels=cols.values,annot_kws={'size':10})\n    plt.show()","d3a4d3c5":"plotCorr(df_train,'target')","8b7ec4f2":"def featureEngineering(df,cols,col_name):\n    df[col_name] = 0\n    for c in cols:\n        df[col_name] += df[c]\n    return df","ca2b22ce":"corr = df_train.corr()\ncols = corr.nlargest(12,'target').index.values[1:]","d303ec0d":"df_train = featureEngineering(df_train,cols, 'posCols')","b770f07c":"plotCorr(df_train,'target')","676d3ee4":"df_test = featureEngineering(df_test,cols, 'posCols')","5a99815d":"val = np.zeros(df_train.shape[0])\npred = np.zeros(df_test.shape[0])\nx = df_train.drop(['ID_code','target'],axis=1).values\ny = df_train['target'].values\nfolds = StratifiedKFold(n_splits=3, shuffle=True, random_state=1234)","8cf687fc":"model_xgb =  xgb.XGBClassifier(max_depth=2,\n                              colsample_bytree=0.7,\n                              n_estimators=20000,\n                              scale_pos_weight = 9,\n                              learning_rate=0.02,\n                              objective='binary:logistic', \n                              verbosity =1,\n                              eval_metric  = 'auc',\n                              tree_method='gpu_hist',\n                              n_jobs=-1)","13d8c4a5":"for fold_index, (train_index,val_index) in enumerate(folds.split(x,y)):\n    print('Batch {} started...'.format(fold_index))\n    gc.collect()\n    bst = model_xgb.fit(x[train_index],y[train_index],\n              eval_set = [(x[val_index],y[val_index])],\n              early_stopping_rounds=200,\n              verbose= 200, \n              eval_metric ='auc'\n              )\n    val[val_index] = model_xgb.predict_proba(x[val_index])[:,1]\n    print('auc of this val set is {}'.format(roc_auc_score(y[val_index],val[val_index])))\n    pred += model_xgb.predict_proba(df_test.drop(['ID_code'],axis=1).values)[:,1]\/folds.n_splits","0d4eab47":"df_sub.target = pred","9334cb83":"df_sub.to_csv('r_prob.csv',index=False)","6dbb8236":"My machine is slow.. so I just split data into three parts","c99f8407":"Huh...the higest correlation rate is only 0.067!... Thats a bit low..","9222e572":"Basic check","d811f0ea":"Can we simply add them up?","9865c0d6":"Load Data","fd65367f":"Hmm.... ID_code is just a simple index, maybe we should exclude it?","4a6e9e56":"Next, let's have a look at correlation of columns and our target. This is very first and important for feature engineering.","cf82546a":"Good! nothing is missed so we can skip the fillNA part. :D","732f1b25":"OK, it appears in test data as well. We will remove it then...\nNow we need to check if any missing data here...","0591ee69":"Now the highest corr rate is 0.16.. It might help or might not... \nLet's just do the same thing to test data"}}