{"cell_type":{"e3aa53df":"code","2468adb3":"code","0cb65467":"code","f65b24a6":"code","6433bb1b":"code","248a53ea":"code","853987b8":"code","1cfdad59":"code","791ed59f":"code","6dc1670e":"code","345c193b":"code","69af002b":"code","874aca72":"code","20768ecd":"code","59fab48e":"code","bcedbf62":"code","674ba933":"code","7725e89b":"code","5fddd78e":"code","738c882a":"code","4d1fe787":"code","cce60f14":"code","7d3eca17":"code","e474f98e":"code","3c4860ec":"code","76d237c4":"code","9602a18c":"code","9634a569":"code","baea1eb8":"code","54de0eba":"code","46120be9":"code","dd5e144a":"code","6ab8b980":"markdown","8cb80c09":"markdown","7fa858d3":"markdown","e62c739d":"markdown","921adb39":"markdown"},"source":{"e3aa53df":"import os\nfrom time import time\nfrom random import randint\nimport json\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"device={device}\")\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    print(dirname)\n    \nfrom pathlib import Path\n\n","2468adb3":"data_path = Path('\/kaggle\/input\/abstraction-and-reasoning-challenge\/')\ntraining_path = data_path \/ 'training'\nevaluation_path = data_path \/ 'evaluation'\ntest_path = data_path \/ 'test'\n\ntraining_tasks = sorted(os.listdir(training_path))\nevaluation_tasks = sorted(os.listdir(evaluation_path))\ntest_tasks = sorted(os.listdir(test_path))\nprint(len(training_tasks), len(evaluation_tasks), len(test_tasks))","0cb65467":"cmap = colors.ListedColormap(\n    ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n     '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\nnorm = colors.Normalize(vmin=0, vmax=9)\n# 0:black, 1:blue, 2:red, 3:greed, 4:yellow,\n# 5:gray, 6:magenta, 7:orange, 8:sky, 9:brown\nplt.figure(figsize=(5, 2), dpi=200)\nplt.imshow([list(range(10))], cmap=cmap, norm=norm)\nplt.xticks(list(range(10)))\nplt.yticks([])\nplt.show()\n\ndef plot_task(task):\n    n = len(task[\"train\"]) + len(task[\"test\"])\n    fig, axs = plt.subplots(2, n, figsize=(4*n,8), dpi=50)\n    plt.subplots_adjust(wspace=0, hspace=0)\n    fig_num = 0\n    for i, t in enumerate(task[\"train\"]):\n        t_in, t_out = np.array(t[\"input\"]), np.array(t[\"output\"])\n        axs[0][fig_num].imshow(t_in, cmap=cmap, norm=norm)\n        axs[0][fig_num].set_title(f'Train-{i} in')\n        axs[0][fig_num].set_yticks(list(range(t_in.shape[0])))\n        axs[0][fig_num].set_xticks(list(range(t_in.shape[1])))\n        axs[1][fig_num].imshow(t_out, cmap=cmap, norm=norm)\n        axs[1][fig_num].set_title(f'Train-{i} out')\n        axs[1][fig_num].set_yticks(list(range(t_out.shape[0])))\n        axs[1][fig_num].set_xticks(list(range(t_out.shape[1])))\n        fig_num += 1\n    for i, t in enumerate(task[\"test\"]):\n        t_in, t_out = np.array(t[\"input\"]), np.array(t[\"output\"])\n        axs[0][fig_num].imshow(t_in, cmap=cmap, norm=norm)\n        axs[0][fig_num].set_title(f'Test-{i} in')\n        axs[0][fig_num].set_yticks(list(range(t_in.shape[0])))\n        axs[0][fig_num].set_xticks(list(range(t_in.shape[1])))\n        axs[1][fig_num].imshow(t_out, cmap=cmap, norm=norm)\n        axs[1][fig_num].set_title(f'Test-{i} out')\n        axs[1][fig_num].set_yticks(list(range(t_out.shape[0])))\n        axs[1][fig_num].set_xticks(list(range(t_out.shape[1])))\n        fig_num += 1\n    \n    plt.tight_layout()\n    plt.show()\n    \n","f65b24a6":"def get_data(task_filename):\n    with open(task_filename, 'r') as f:\n        task = json.load(f)\n    return task\n\nnum2color = [\"black\", \"blue\", \"red\", \"green\", \"yellow\", \"gray\", \"magenta\", \"orange\", \"sky\", \"brown\"]\ncolor2num = {c: n for n, c in enumerate(num2color)}","6433bb1b":"def check(task, pred_func):\n    n = len(task[\"train\"]) + len(task[\"test\"])\n    fig, axs = plt.subplots(3, n, figsize=(4*n,12), dpi=50)\n    plt.subplots_adjust(wspace=0.3, hspace=0.3)\n    fig_num = 0\n    for i, t in enumerate(task[\"train\"]):\n        t_in, t_out = np.array(t[\"input\"]), np.array(t[\"output\"])\n        t_pred = pred_func(t_in)\n        axs[0][fig_num].imshow(t_in, cmap=cmap, norm=norm)\n        axs[0][fig_num].set_title(f'Train-{i} in')\n        axs[0][fig_num].set_yticks(list(range(t_in.shape[0])))\n        axs[0][fig_num].set_xticks(list(range(t_in.shape[1])))\n        axs[1][fig_num].imshow(t_out, cmap=cmap, norm=norm)\n        axs[1][fig_num].set_title(f'Train-{i} out')\n        axs[1][fig_num].set_yticks(list(range(t_out.shape[0])))\n        axs[1][fig_num].set_xticks(list(range(t_out.shape[1])))\n        axs[2][fig_num].imshow(t_pred, cmap=cmap, norm=norm)\n        axs[2][fig_num].set_title(f'Train-{i} pred')\n        axs[2][fig_num].set_yticks(list(range(t_pred.shape[0])))\n        axs[2][fig_num].set_xticks(list(range(t_pred.shape[1])))\n        fig_num += 1\n    for i, t in enumerate(task[\"test\"]):\n        t_in, t_out = np.array(t[\"input\"]), np.array(t[\"output\"])\n        t_pred = pred_func(t_in)\n        axs[0][fig_num].imshow(t_in, cmap=cmap, norm=norm)\n        axs[0][fig_num].set_title(f'Test-{i} in')\n        axs[0][fig_num].set_yticks(list(range(t_in.shape[0])))\n        axs[0][fig_num].set_xticks(list(range(t_in.shape[1])))\n        axs[1][fig_num].imshow(t_out, cmap=cmap, norm=norm)\n        axs[1][fig_num].set_title(f'Test-{i} out')\n        axs[1][fig_num].set_yticks(list(range(t_out.shape[0])))\n        axs[1][fig_num].set_xticks(list(range(t_out.shape[1])))\n        axs[2][fig_num].imshow(t_pred, cmap=cmap, norm=norm)\n        axs[2][fig_num].set_title(f'Test-{i} pred')\n        axs[2][fig_num].set_yticks(list(range(t_pred.shape[0])))\n        axs[2][fig_num].set_xticks(list(range(t_pred.shape[1])))\n        fig_num += 1","248a53ea":"class ArcDataset(torch.utils.data.Dataset):\n    def __init__(self, task=None, mode=\"train\", augment=False):\n        if task is not None:\n            assert mode in [\"train\", \"test\"]\n            self.mode = mode\n            self.task = task[mode]\n        self.augment = augment\n\n    def __len__(self):\n        return len(self.task)\n    \n    def __getitem__(self, index):\n        t = self.task[index]\n        t_in = torch.tensor(t[\"input\"])\n        t_out = torch.tensor(t[\"output\"])\n        t_in, t_out = self.preprocess(t_in, t_out)\n        return t_in, t_out\n    \n    def preprocess(self, t_in, t_out):\n        if self.augment:\n            t_in, t_out = self._random_rotate(t_in, t_out)\n        t_in = self._one_hot_encode(t_in)\n        t_out = self._one_hot_encode(t_out)\n        return t_in, t_out\n    \n    def _one_hot_encode(self, x):\n        return torch.eye(10)[x].permute(2, 0, 1)\n    \n    def _random_rotate(self, t_in, t_out):\n        t_in_shape = t_in.shape\n        t_out_shape = t_out.shape\n        t_in = t_in.reshape(-1, *t_in_shape[-2:])\n        t_out = t_out.reshape(-1, *t_out_shape[-2:])\n        r = randint(0, 7)\n        if r%2 == 0:\n            t_in = t_in.permute(0, 2, 1)\n            t_out = t_out.permute(0, 2, 1)\n        r \/\/= 2\n        if r%2 == 0:\n            t_in = t_in[:, :, torch.arange(t_in.shape[-1]-1, -1, -1)]\n            t_out = t_out[:, :, torch.arange(t_out.shape[-1]-1, -1, -1)]\n        r \/\/= 2\n        if r%2 == 0:\n            t_in = t_in[:, torch.arange(t_in.shape[-2]-1, -1, -1), :]\n            t_out = t_out[:, torch.arange(t_out.shape[-2]-1, -1, -1), :]\n        t_in = t_in.reshape(*t_in_shape[:-2], *t_in.shape[-2:])\n        t_out = t_out.reshape(*t_out_shape[:-2], *t_out.shape[-2:])\n        return t_in, t_out\n    \ndef device_collate(batch):\n    return tuple(map(lambda x: torch.stack(x).to(device), zip(*batch)))","853987b8":"def hinge_loss(y_pred, y_true):\n    loss = y_pred.clone()\n    loss[y_true>0.5] = 1-loss[y_true>0.5]\n    loss[loss<0] = 0\n    return loss.sum(0).mean()","1cfdad59":"task = get_data(str(training_path \/ training_tasks[330]))\nplot_task(task)","791ed59f":"class Task330Net(nn.Module):\n    def __init__(self):\n        super(Task330Net, self).__init__()\n        siz = 16\n        self.conv1 = nn.Conv2d(10, siz, 3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n        self.dropout = torch.nn.Dropout2d(p=0.1)\n        self.conv1x1 = nn.Conv2d(siz, 10, 1)\n\n    def forward(self, x):\n        x2 = self.conv1(x)\n        x2 = self.relu(x2)\n        x2 = self.dropout(x2)\n        x2 = self.conv1x1(x2)\n        x = x + x2  # skip connection\n        return x","6dc1670e":"train_dataset = ArcDataset(task, mode=\"train\", augment=False)\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=device_collate)\nvalid_dataset = ArcDataset(task, mode=\"test\", augment=False)\nvalid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=1, shuffle=False, collate_fn=device_collate)","345c193b":"net = Task330Net().to(device)\ncriterion = hinge_loss\n#optimizer = torch.optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\noptimizer = torch.optim.Adam(net.parameters(), lr=1e-3, weight_decay=1e-4)\nt0 = time()\n\nfor param in net.named_parameters():\n    print(f\"{param[0]:>15} {list(param[1].shape)}\")\n\nfor epoch in range(5000):\n    train_loss = valid_loss = 0.0\n    train_loss_denom = valid_loss_denom = 0\n    \n    ####################\n    # train\n    ####################\n    net.train()\n    for i, (feature, target) in enumerate(train_dataloader):\n        outputs = net(feature)\n        loss = criterion(outputs, target)\n\n        # backprop\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    \n        # record\n        train_loss += loss.item()\n        train_loss_denom += feature.shape[0]\n\n    train_loss \/= train_loss_denom\n\n    ####################\n    # eval\n    ####################\n    net.eval()\n    with torch.no_grad():\n        for i, (feature, target) in enumerate(valid_dataloader):\n            feature = feature.to(device)\n            target = target.to(device)\n            \n            outputs = net(feature)\n            loss = criterion(outputs, target)\n\n            # record\n            valid_loss += loss.item()\n            valid_loss_denom += feature.shape[0]\n\n    valid_loss \/= valid_loss_denom\n\n\n    if epoch%100==0:\n        print(f\"epoch {epoch:4d}  |  train_loss: {train_loss:5.6f}  valid_loss: {valid_loss:5.6f}  |  time: {time()-t0:7.1f} sec\")\n\n#         if best_loss > valid_loss:\n#             best_loss = valid_loss\n#             filename = f\".\/work\/trained_weight\/{MODEL_NAME}_epoch{epoch:03d}_loss{valid_loss:.3f}.pth\"\n#             torch.save(net.state_dict(), filename)","69af002b":"def task_train330(x, net):\n    def one_hot_decode(x):\n        return x.argmax(0)\n    net.eval()\n    with torch.no_grad():\n        x = torch.tensor(x).to(device)\n        y_dummy = x.clone()\n        dataset = ArcDataset(augment=False)\n        x = dataset.preprocess(x, y_dummy)[0].unsqueeze(0)\n        y = net(x).detach()\n        y = one_hot_decode(y.squeeze(0))\n    y = y.to(\"cpu\").numpy()\n    return y\n    \n\ncheck(task, lambda x: task_train330(x, net))","874aca72":"task = get_data(str(training_path \/ training_tasks[301]))\nplot_task(task)","20768ecd":"class Task301Net(nn.Module):\n    def __init__(self):\n        super(Task301Net, self).__init__()\n        siz = 16\n        self.conv1 = nn.Conv2d(10, siz, 3, padding=1)\n        self.conv2 = nn.Conv2d(siz, siz, 3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n        self.dropout = torch.nn.Dropout2d(p=0.1)\n        self.conv1x1 = nn.Conv2d(siz, 10, 1)\n\n    def forward(self, x):\n        x2 = self.conv1(x)\n        x2 = self.relu(x2)\n        x2 = self.conv2(x2)\n        x2 = self.relu(x2)\n        x2 = self.dropout(x2)\n        x2 = self.conv1x1(x2)\n        x = x + x2  # skip connection\n        return x","59fab48e":"train_dataset = ArcDataset(task, mode=\"train\", augment=True)\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=device_collate)\nvalid_dataset = ArcDataset(task, mode=\"test\", augment=False)\nvalid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=1, shuffle=False, collate_fn=device_collate)","bcedbf62":"net = Task301Net().to(device)\ncriterion = hinge_loss\n#optimizer = torch.optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\noptimizer = torch.optim.Adam(net.parameters(), lr=1e-3, weight_decay=1e-4)\n\nt0 = time()\n\nfor param in net.named_parameters():\n    print(f\"{param[0]:>15} {list(param[1].shape)}\")\n\nfor epoch in range(5000):\n    train_loss = valid_loss = 0.0\n    train_loss_denom = valid_loss_denom = 0\n    \n    ####################\n    # train\n    ####################\n    net.train()\n    for i, (feature, target) in enumerate(train_dataloader):\n        outputs = net(feature)\n        loss = criterion(outputs, target)\n\n        # backprop\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    \n        # record\n        train_loss += loss.item()\n        train_loss_denom += feature.shape[0]\n\n    train_loss \/= train_loss_denom\n\n    ####################\n    # eval\n    ####################\n    net.eval()\n    with torch.no_grad():\n        for i, (feature, target) in enumerate(valid_dataloader):\n            feature = feature.to(device)\n            target = target.to(device)\n            \n            outputs = net(feature)\n            loss = criterion(outputs, target)\n\n            # record\n            valid_loss += loss.item()\n            valid_loss_denom += feature.shape[0]\n\n    valid_loss \/= valid_loss_denom\n\n\n    if epoch%100==0:\n        print(f\"epoch {epoch:4d}  |  train_loss: {train_loss:5.6f}  valid_loss: {valid_loss:5.6f}  |  time: {time()-t0:7.1f} sec\")\n\n#         if best_loss > valid_loss:\n#             best_loss = valid_loss\n#             filename = f\".\/work\/trained_weight\/{MODEL_NAME}_epoch{epoch:03d}_loss{valid_loss:.3f}.pth\"\n#             torch.save(net.state_dict(), filename)","674ba933":"def task_train301(x, net):\n    def one_hot_decode(x):\n        return x.argmax(0)\n    net.eval()\n    with torch.no_grad():\n        x = torch.tensor(x).to(device)\n        y_dummy = x.clone()\n        dataset = ArcDataset(augment=False)\n        x = dataset.preprocess(x, y_dummy)[0].unsqueeze(0)\n        y = net(x).detach()\n        y = one_hot_decode(y.squeeze(0))\n    y = y.to(\"cpu\").numpy()\n    return y\n    \n\ncheck(task, lambda x: task_train301(x, net))","7725e89b":"task = get_data(str(training_path \/ training_tasks[343]))\nplot_task(task)","5fddd78e":"class Task343Net(nn.Module):\n    def __init__(self):\n        super(Task343Net, self).__init__()\n        siz = 16\n        self.conv1 = nn.Conv2d(10, siz, 3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n        self.dropout = torch.nn.Dropout2d(p=0.1)\n        self.conv1x1 = nn.Conv2d(siz, 10, 1)\n\n    def forward(self, x):\n        x2 = self.conv1(x)\n        x2 = self.relu(x2)\n        x2 = self.dropout(x2)\n        x2 = self.conv1x1(x2)\n        x = x + x2  # skip connection\n        return x","738c882a":"train_dataset = ArcDataset(task, mode=\"train\", augment=True)\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=device_collate)\nvalid_dataset = ArcDataset(task, mode=\"test\", augment=False)\nvalid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=1, shuffle=False, collate_fn=device_collate)","4d1fe787":"net = Task343Net().to(device)\ncriterion = hinge_loss\n#optimizer = torch.optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\noptimizer = torch.optim.Adam(net.parameters(), lr=1e-3, weight_decay=1e-4)\nt0 = time()\n\nfor param in net.named_parameters():\n    print(f\"{param[0]:>15} {list(param[1].shape)}\")\n\nfor epoch in range(5000):\n    train_loss = valid_loss = 0.0\n    train_loss_denom = valid_loss_denom = 0\n    \n    ####################\n    # train\n    ####################\n    net.train()\n    for i, (feature, target) in enumerate(train_dataloader):\n        outputs = net(feature)\n        loss = criterion(outputs, target)\n\n        # backprop\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    \n        # record\n        train_loss += loss.item()\n        train_loss_denom += feature.shape[0]\n\n    train_loss \/= train_loss_denom\n\n    ####################\n    # eval\n    ####################\n    net.eval()\n    with torch.no_grad():\n        for i, (feature, target) in enumerate(valid_dataloader):\n            feature = feature.to(device)\n            target = target.to(device)\n            \n            outputs = net(feature)\n            loss = criterion(outputs, target)\n\n            # record\n            valid_loss += loss.item()\n            valid_loss_denom += feature.shape[0]\n\n    valid_loss \/= valid_loss_denom\n\n\n    if epoch%100==0:\n        print(f\"epoch {epoch:4d}  |  train_loss: {train_loss:5.6f}  valid_loss: {valid_loss:5.6f}  |  time: {time()-t0:7.1f} sec\")\n\n#         if best_loss > valid_loss:\n#             best_loss = valid_loss\n#             filename = f\".\/work\/trained_weight\/{MODEL_NAME}_epoch{epoch:03d}_loss{valid_loss:.3f}.pth\"\n#             torch.save(net.state_dict(), filename)","cce60f14":"def task_train343(x, net):\n    def one_hot_decode(x):\n        return x.argmax(0)\n    net.eval()\n    with torch.no_grad():\n        x = torch.tensor(x).to(device)\n        y_dummy = x.clone()\n        dataset = ArcDataset(augment=False)\n        x = dataset.preprocess(x, y_dummy)[0].unsqueeze(0)\n        y = net(x).detach()\n        y = one_hot_decode(y.squeeze(0))\n    y = y.to(\"cpu\").numpy()\n    return y\n    \n\ncheck(task, lambda x: task_train343(x, net))","7d3eca17":"task = get_data(str(training_path \/ training_tasks[368]))\nplot_task(task)","e474f98e":"class Task368Net(nn.Module):\n    def __init__(self):\n        super(Task368Net, self).__init__()\n        siz = 16\n        self.conv1 = nn.Conv2d(10, siz, 2, padding=1)\n        self.conv2 = nn.Conv2d(siz, siz, 2, padding=0)\n        self.conv3 = nn.Conv2d(siz, siz, 3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n        self.dropout = torch.nn.Dropout2d(p=0.1)\n        self.conv4 = nn.Conv2d(siz, 10, 1)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.relu(x)\n        x = self.conv3(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.conv4(x)\n        return x\n    \n# class Task368Net(nn.Module):  # does not work\n#     def __init__(self):\n#         super(Task368Net, self).__init__()\n#         siz = 16\n#         self.conv1 = nn.Conv2d(10, siz, 3, padding=1)\n#         self.relu = nn.ReLU(inplace=True)\n#         self.dropout = torch.nn.Dropout2d(p=0.1)\n#         self.conv1x1 = nn.Conv2d(siz, 10, 1)\n\n#     def forward(self, x):\n#         x2 = self.conv1(x)\n#         x2 = self.relu(x2)\n#         x2 = self.dropout(x2)\n#         x2 = self.conv1x1(x2)\n#         x = x + x2  # skip connection\n#         return x","3c4860ec":"train_dataset = ArcDataset(task, mode=\"train\", augment=True)\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=device_collate)\nvalid_dataset = ArcDataset(task, mode=\"test\", augment=False)\nvalid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=1, shuffle=False, collate_fn=device_collate)","76d237c4":"net = Task368Net().to(device)\ncriterion = hinge_loss\n#optimizer = torch.optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\noptimizer = torch.optim.Adam(net.parameters(), lr=1e-3, weight_decay=1e-4)\nt0 = time()\n\nfor param in net.named_parameters():\n    print(f\"{param[0]:>15} {list(param[1].shape)}\")\n\nfor epoch in range(5000):\n    train_loss = valid_loss = 0.0\n    train_loss_denom = valid_loss_denom = 0\n    \n    ####################\n    # train\n    ####################\n    net.train()\n    for i, (feature, target) in enumerate(train_dataloader):\n        outputs = net(feature)\n        loss = criterion(outputs, target)\n\n        # backprop\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    \n        # record\n        train_loss += loss.item()\n        train_loss_denom += feature.shape[0]\n\n    train_loss \/= train_loss_denom\n\n    ####################\n    # eval\n    ####################\n    net.eval()\n    with torch.no_grad():\n        for i, (feature, target) in enumerate(valid_dataloader):\n            feature = feature.to(device)\n            target = target.to(device)\n            \n            outputs = net(feature)\n            loss = criterion(outputs, target)\n\n            # record\n            valid_loss += loss.item()\n            valid_loss_denom += feature.shape[0]\n\n    valid_loss \/= valid_loss_denom\n\n\n    if epoch%100==0:\n        print(f\"epoch {epoch:4d}  |  train_loss: {train_loss:5.6f}  valid_loss: {valid_loss:5.6f}  |  time: {time()-t0:7.1f} sec\")\n\n#         if best_loss > valid_loss:\n#             best_loss = valid_loss\n#             filename = f\".\/work\/trained_weight\/{MODEL_NAME}_epoch{epoch:03d}_loss{valid_loss:.3f}.pth\"\n#             torch.save(net.state_dict(), filename)","9602a18c":"def task_train368(x, net):\n    def one_hot_decode(x):\n        return x.argmax(0)\n    net.eval()\n    with torch.no_grad():\n        x = torch.tensor(x).to(device)\n        y_dummy = x.clone()\n        dataset = ArcDataset(augment=False)\n        x = dataset.preprocess(x, y_dummy)[0].unsqueeze(0)\n        y = net(x).detach()\n        y = one_hot_decode(y.squeeze(0))\n    y = y.to(\"cpu\").numpy()\n    return y\n    \n\ncheck(task, lambda x: task_train368(x, net))","9634a569":"task = get_data(str(training_path \/ training_tasks[351]))\nplot_task(task)","baea1eb8":"class Task351Net(nn.Module):\n    def __init__(self):\n        super(Task351Net, self).__init__()\n        siz = 16\n        self.conv1 = nn.Conv2d(10, siz, 3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n        self.dropout = torch.nn.Dropout2d(p=0.1)\n        self.conv1x1 = nn.Conv2d(siz, 10, 1)\n\n    def forward(self, x):\n        x2 = self.conv1(x)\n        x2 = self.relu(x2)\n        x2 = self.dropout(x2)\n        x2 = self.conv1x1(x2)\n        x = x + x2  # skip connection\n        return x\n    \n","54de0eba":"train_dataset = ArcDataset(task, mode=\"train\", augment=True)\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=device_collate)\nvalid_dataset = ArcDataset(task, mode=\"test\", augment=False)\nvalid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=1, shuffle=False, collate_fn=device_collate)","46120be9":"net = Task351Net().to(device)\ncriterion = hinge_loss\n#optimizer = torch.optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\noptimizer = torch.optim.Adam(net.parameters(), lr=1e-3, weight_decay=1e-4)\nt0 = time()\n\nfor param in net.named_parameters():\n    print(f\"{param[0]:>15} {list(param[1].shape)}\")\n\nfor epoch in range(5000):\n    train_loss = valid_loss = 0.0\n    train_loss_denom = valid_loss_denom = 0\n    \n    ####################\n    # train\n    ####################\n    net.train()\n    for i, (feature, target) in enumerate(train_dataloader):\n        outputs = net(feature)\n        loss = criterion(outputs, target)\n\n        # backprop\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    \n        # record\n        train_loss += loss.item()\n        train_loss_denom += feature.shape[0]\n\n    train_loss \/= train_loss_denom\n\n    ####################\n    # eval\n    ####################\n    net.eval()\n    with torch.no_grad():\n        for i, (feature, target) in enumerate(valid_dataloader):\n            feature = feature.to(device)\n            target = target.to(device)\n            \n            outputs = net(feature)\n            loss = criterion(outputs, target)\n\n            # record\n            valid_loss += loss.item()\n            valid_loss_denom += feature.shape[0]\n\n    valid_loss \/= valid_loss_denom\n\n\n    if epoch%100==0:\n        print(f\"epoch {epoch:4d}  |  train_loss: {train_loss:5.6f}  valid_loss: {valid_loss:5.6f}  |  time: {time()-t0:7.1f} sec\")\n","dd5e144a":"def task_train351(x, net):\n    def one_hot_decode(x):\n        return x.argmax(0)\n    net.eval()\n    with torch.no_grad():\n        x = torch.tensor(x).to(device)\n        y_dummy = x.clone()\n        dataset = ArcDataset(augment=False)\n        x = dataset.preprocess(x, y_dummy)[0].unsqueeze(0)\n        y = net(x).detach()\n        y = one_hot_decode(y.squeeze(0))\n    y = y.to(\"cpu\").numpy()\n    return y\n    \n\ncheck(task, lambda x: task_train351(x, net))","6ab8b980":"# Task Train330","8cb80c09":"# Task Train368","7fa858d3":"# Task Train351","e62c739d":"# Task Train343","921adb39":"# Task Train301"}}