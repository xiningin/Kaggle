{"cell_type":{"3284387c":"code","91c97193":"code","7659532d":"code","e1257c49":"code","f0b210aa":"code","fcb0c25b":"code","7d18a2cf":"code","38bf0bd7":"code","cc1cd417":"code","ae7d66ac":"code","7780ff60":"code","08b838eb":"code","b4ee46bb":"code","f0af1a6b":"code","80045293":"code","28d28531":"code","6f6faa82":"code","8b2092a4":"code","44ea0c24":"code","4b90e270":"code","8f41bf36":"code","481474fa":"code","25e3499d":"code","b9ce69d5":"code","87b3981f":"code","3e2f7598":"code","0d675388":"code","310dec2c":"code","a916f1cf":"code","73d5b562":"code","b9dc8451":"code","3fb26938":"code","b04ceed3":"code","10d99779":"code","034654ff":"code","8c18fd41":"code","f0e28bfc":"code","db3fc91d":"code","a720da42":"code","0bd67c10":"code","5c5ee209":"code","d1c12508":"code","6fd31d96":"code","89b14014":"code","73868267":"code","42a36ede":"code","df3b3089":"code","6b18404a":"code","52b9609b":"code","185b9cc5":"code","9c02c12f":"code","26e69e6c":"code","8da7e87d":"code","f328ae56":"code","d047f718":"code","57de05b9":"code","767cac8c":"code","1fcbecbe":"code","73aab70c":"code","59e326f8":"code","0a611d1d":"code","357a21d8":"code","47823486":"code","76d2f1d4":"code","d85803da":"code","9e40556b":"code","b8baa20b":"code","b85920aa":"code","286d55a8":"code","29224383":"code","fb0dffa5":"code","e9bdb203":"code","d0532535":"code","d5455daa":"markdown","0ea35290":"markdown","5793317c":"markdown","79d9a509":"markdown","b55ba422":"markdown","2c06337d":"markdown","5d243c64":"markdown","5559fe87":"markdown","5b67c940":"markdown","53f81212":"markdown","be7e77d9":"markdown","8357ed8b":"markdown","d1c72543":"markdown","32614754":"markdown","9adb344f":"markdown","a8f07278":"markdown","8047ff0d":"markdown","d2a1eefd":"markdown","c224df26":"markdown","bd12e7b3":"markdown","9191efb2":"markdown","aabe6ce5":"markdown","ff87e5de":"markdown","b6284ef3":"markdown","e9a637e7":"markdown","9efdcbb6":"markdown","d3beb058":"markdown","626de5f7":"markdown","1bb11398":"markdown","77b2f548":"markdown","4982d020":"markdown","30c1fcf1":"markdown","f9faf696":"markdown","f5bf6d0b":"markdown","b30c2dcf":"markdown","c1285f86":"markdown","4e3846f2":"markdown","2260ee2f":"markdown","272859c0":"markdown","55b87feb":"markdown","f387d61e":"markdown","610d9463":"markdown","8f3dfcfb":"markdown"},"source":{"3284387c":"## Switch from Kaggle to Colab easily\nenvironment='Kaggle' ## Kaggle \/ Colab\n\n## Using Cleaned dataset? This can be useful when running into crashes or Cudf incompatibility, just add this to the kaggle input: https:\/\/www.kaggle.com\/aiswaryaramachandran\/baseline-model-xgboost-3-03-rmse\/data?select=train_cleaned.csv\ncleaned_dataset=True ## Change to True or False\n\n## when True only 50.000 rows are used for debugging purpose. Set to False when doing real training\ndebug_mode=False ## Change to True or False\n\n## choose how many rows of the training data sample you would like to use (only works when debug_mode=False ), max is 55423480\nrows_datasample=5542348","91c97193":"if environment == 'Kaggle':\n  env_submission_path='.\/'\n  env_path='..\/input\/new-york-city-taxi-fare-prediction\/'\n  print('The environment and paths were successfully setup for Kaggle')\nelif environment == 'Colab':\n  env_submission_path='\/content\/drive\/My Drive\/Colab Notebooks\/'\n  env_path='\/content\/drive\/My Drive\/Colab Notebooks\/'\n\n  from google.colab import drive\n  drive.mount('\/content\/drive')\n\n  print('The environment and paths were successfully setup for Colab')\n\nelse:\n  print('Something went wrong here, please choose one of the options for path completion: Kaggle or Colab (or implement your own thing)')\n","7659532d":"!nvidia-smi","e1257c49":"!lscpu | grep \"Model name:\"\n!lscpu | grep \"CPU(s)\"","f0b210aa":"%%time\nif environment == 'Kaggle':\n  import sys\n  !cp ..\/input\/rapids\/rapids.0.14.0 \/opt\/conda\/envs\/rapids.tar.gz\n  !cd \/opt\/conda\/envs\/ && tar -xzvf rapids.tar.gz > \/dev\/null\n  sys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\/site-packages\"] + sys.path\n  sys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\"] + sys.path\n  sys.path = [\"\/opt\/conda\/envs\/rapids\/lib\"] + sys.path \n  !cp \/opt\/conda\/envs\/rapids\/lib\/libxgboost.so \/opt\/conda\/lib\/\n  print('You are all set for the kaggle rapids environment')\n\nelif environment == 'Colab':\n  # Install RAPIDS and Dask_ml\n  !pip install dask_ml\n  !pip install dask_cuda\n  !git clone https:\/\/github.com\/rapidsai\/rapidsai-csp-utils.git\n  !bash rapidsai-csp-utils\/colab\/rapids-colab.sh stable\n\n  import sys, os\n  dist_package_index = sys.path.index('\/usr\/local\/lib\/python3.6\/dist-packages')\n  sys.path = sys.path[:dist_package_index] + ['\/usr\/local\/lib\/python3.6\/site-packages'] + sys.path[dist_package_index:]\n  sys.path\n  exec(open('rapidsai-csp-utils\/colab\/update_modules.py').read(), globals())\n  print('You are all set for the colab rapids environment')\n\nelse:\n  print('Something went wrong here, please choose one of the options for path completion: Kaggle or Colab (or implement your own thing). If Kaggle failed please make sure you added the RAPIDS file on Kaggle to your Input!')","fcb0c25b":"import nvstrings\nimport numpy as np\nimport cudf, cuml\nimport dask_cudf\nimport io, requests\nimport math\nimport gc\nimport cupy as cp\nimport pandas as pd\n\nimport time \nimport itertools\n\n#Plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\n#Learning\nfrom cuml.preprocessing.model_selection import train_test_split\nfrom scipy.stats import uniform\n\n# Linear Models https:\/\/github.com\/rapidsai\/cuml\/tree\/branch-0.13\/notebooks\nfrom cuml.linear_model import LinearRegression # Linear\nfrom cuml.linear_model import LogisticRegression # Logisitc\nfrom cuml.linear_model import ElasticNet # Elastic\nfrom cuml.linear_model import Ridge # Ridge\nfrom cuml.linear_model import Lasso # Lasso\nfrom cuml.linear_model import MBSGDRegressor as cumlMBSGDRegressor # Mini Batch SGD Regressor\n\nfrom cuml.solvers import SGD as cumlSGD # Stochastic Gradient Descent\nfrom cuml.ensemble import RandomForestRegressor as cuRF # Random Forest\nfrom cuml.dask.ensemble import RandomForestClassifier as cumlDaskRF # RandomForest\n\nfrom cuml.neighbors import KNeighborsRegressor as cumlKNR # Nearest Neighbours\nfrom cuml.svm import SVC # Support Vector Machines\n\nfrom cuml import ForestInference\nimport xgboost as xgb\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\nfrom cuml.metrics.regression import r2_score\nfrom cuml.metrics.accuracy import accuracy_score\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import accuracy_score as sk_acc\nfrom sklearn.utils.fixes import loguniform","7d18a2cf":"#!pip install -U xgboost\n!pip install dask-xgboost\nimport dask_xgboost as dxgb\n\nimport dask; print('Dask Version:', dask.__version__)\n\nfrom dask.distributed import Client\nfrom dask_cuda import LocalCUDACluster\n\n# create a local cluster with 4 workers\nn_workers = 1\n# create a local CUDA cluster\ncluster = LocalCUDACluster()\nclient = Client(cluster)\nclient\n\nfrom dask.delayed import delayed\n\nimport dask.dataframe as dd\nimport dask.array as da\n\nimport cudf; print('cuDF Version:', cudf.__version__)\nimport numpy as np; print('NumPy Version:', np.__version__)","38bf0bd7":"def save_model_csv(y_pred, csv_name):\n  save_submission = submission\n  save_submission['fare_amount']=y_pred\n  save_submission.to_csv(env_submission_path+csv_name, index=False)\n  return save_submission.head(2)","cc1cd417":"def calc_R2(y_pred, model_name):\n  score = r2_score(y_pred, y_test)\n  print(\"R2 Score for the \"+model_name+\" model is: \",score)\n  return score","ae7d66ac":"## initializing the r2 scores so there will be no errors at the end when comparing them even if we do not run all models\nscore_linear_reg=0\nscore_ridge_reg=0\nscore_KNN=0\nscore_Rand_For=0\nscore_XGB_GPU=0\nscore_LGBM=0","7780ff60":"cudf.set_allocator(\"managed\")\n\n## Check if using cleaned dataset or not\nif cleaned_dataset==False:\n    dtype = {     \n        'key':'str',\n        'fare_amount': 'float32',\n        'pickup_datetime':'str',\n        'pickup_longitude': 'float32',\n        'pickup_latitude': 'float32',\n        'dropoff_longitude': 'float32',\n        'dropoff_latitude': 'float32',\n        'passenger_count': 'int8'}\nelse:\n    dtype = {     \n        'key':'str',\n        'fare_amount': 'float32',\n        'pickup_datetime':'str',\n        'pickup_longitude': 'float32',\n        'pickup_latitude': 'float32',\n        'dropoff_longitude': 'float32',\n        'dropoff_latitude': 'float32',\n        'passenger_count': 'int8',\n        'pickup_latitude_round3': 'float32',\n        'pickup_longitude_round3': 'float32',\n        'dropoff_latitude_round3': 'float32',\n        'dropoff_longitude_round3': 'float32',\n        'is_pickup_JFK': 'int8',\n        'is_dropoff_JFK': 'int8',\n        'is_pickup_EWR': 'int8',\n        'is_dropoff_EWR': 'int8',\n        'is_pickup_la_guardia': 'int8',\n        'is_dropoff_la_guardia': 'int8',\n        'is_pickup_lower_manhattan': 'int8',\n        'is_dropoff_lower_manhattan': 'int8'\n    }\n\nusecols = list(dtype.keys())","08b838eb":"%%time\n# use a subset with 50.000 rows, max is nrows = 55423480\n\nif debug_mode == True:\n  ## using 1% (or how much you like)\n  nrows = 50000\n    \n#del X_train, X_test, y_train, y_test\n#gc.collect()\n\n## Check if using cleaned dataset or not and adjusting the links accordingly\nif cleaned_dataset==False:\n    test = cudf.read_csv(env_path+'test.csv', usecols=usecols, dtype=dtype)\n    train = cudf.read_csv(env_path+'train.csv', nrows=nrows, usecols=usecols, dtype=dtype)\nelse:\n    nrows=55000000\n    test = cudf.read_csv('..\/input\/eda-and-feature-engineering\/test_cleaned.csv', usecols=usecols, dtype=dtype)\n    train = cudf.read_csv('..\/input\/eda-and-feature-engineering\/train_cleaned.csv', nrows=nrows, usecols=usecols, dtype=dtype)\n    \nsubmission = cudf.read_csv(env_path+'sample_submission.csv', usecols=usecols, dtype=dtype)","b4ee46bb":"train.head(5)","f0af1a6b":"test.head(5)","80045293":"#Drop Nan Values\ntrain.nans_to_nulls()\ntrain = train.dropna()","28d28531":"#Checking shape of the data\nprint(\"Train: \" + str(train.shape))\nprint(\"Test: \" + str(test.shape))","6f6faa82":"#Changing the data format of pickup_datetime and adding additional information about pickup time\ntrain['pickup_datetime'] = train['pickup_datetime'].astype('datetime64[ns]')\n\ntrain[\"hour\"] = train.pickup_datetime.dt.hour\ntrain[\"weekday\"] = train.pickup_datetime.dt.weekday\ntrain[\"month\"] = train.pickup_datetime.dt.month\ntrain[\"year\"] = train.pickup_datetime.dt.year\n\n\ntest['pickup_datetime'] = test['pickup_datetime'].astype('datetime64[ns]')\n\ntest[\"hour\"] = test.pickup_datetime.dt.hour\ntest[\"weekday\"] = test.pickup_datetime.dt.weekday\ntest[\"month\"] = test.pickup_datetime.dt.month\ntest[\"year\"] = test.pickup_datetime.dt.year","8b2092a4":"def dist(pickup_lat, pickup_long, dropoff_lat, dropoff_long):  \n    distance = np.abs(dropoff_lat - 40.7141667) + np.abs(dropoff_long - -74.0063889) ## not even working when putting the coordinates right in\n    return distance\n\n#calculate trip distance in miles\ndef calc_distance(data):\n\n    # Distances to nearby airports, and city center\n    # By reporting distances to these points, the model can somewhat triangulate other locations of interest\n    nyc = (-74.0063889, 40.7141667)\n    jfk = (-73.7822222222, 40.6441666667)\n    ewr = (-74.175, 40.69)\n    lgr = (-73.87, 40.77)\n    data['distance_to_center'] = dist(40.7141667, -74.0063889, data['pickup_latitude'].astype(float, copy=False, errors='ignore'), data['pickup_longitude'].astype(float, copy=False, errors='ignore'))\n    #data['pickup_distance_to_jfk'] = dist(jfk[1], jfk[0], data['pickup_latitude'], data['pickup_longitude'])\n    #data['dropoff_distance_to_jfk'] = dist(jfk[1], jfk[0], data['dropoff_latitude'], data['dropoff_longitude'])\n    #data['pickup_distance_to_ewr'] = dist(ewr[1], ewr[0],  data['pickup_latitude'], data['pickup_longitude'])\n    #data['dropoff_distance_to_ewr'] = dist(ewr[1], ewr[0], data['dropoff_latitude'], data['dropoff_longitude'])\n    #data['pickup_distance_to_lgr'] = dist(lgr[1], lgr[0],  data['pickup_latitude'], data['pickup_longitude'])\n    #data['dropoff_distance_to_lgr'] = dist(lgr[1], lgr[0], data['dropoff_latitude'], data['dropoff_longitude'])\n    \n    data['long_dist'] = data['pickup_longitude'] - data['dropoff_longitude']\n    data['lat_dist'] = data['pickup_latitude'] - data['dropoff_latitude']\n    \n    data['dist'] = dist(data['pickup_latitude'], data['pickup_longitude'],\n                        data['dropoff_latitude'], data['dropoff_longitude'])\n    \n    return data\n\n#calc_distance(train)\n#train.head(2)","44ea0c24":"#calculate trip distance in miles\ndef distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295 # Pi\/180\n    a = 0.5 - np.cos((lat2 - lat1) * p)\/2 + np.cos(lat1 * p) * np.cos(lat2 * p) * (1 - np.cos((lon2 - lon1) * p)) \/ 2\n    return 0.6213712 * 12742 * np.arcsin(np.sqrt(a))","4b90e270":"train['distance'] = distance(train['pickup_latitude'], train['pickup_longitude'], train['dropoff_latitude'], train['dropoff_longitude'] )\ntest['distance'] = distance(test['pickup_latitude'], test['pickup_longitude'], test['dropoff_latitude'], test['dropoff_longitude'] )\ntrain['distance'].describe()","8f41bf36":"#check if everything worked\ntrain.head(10)","481474fa":"test.head(2)","25e3499d":"print(\"Ararage fare amount: \" + str(train['fare_amount'].mean()))\nprint(\"Standard deviation fare amount: \" + str(train['fare_amount'].std()))\nprint(\"Ararage distance: \" + str(train['distance'].mean()) + \" miles\")\nprint(\"Standard deviation distance: \" + str(train['distance'].std()) + \" miles\")","b9ce69d5":"train.describe()","87b3981f":"train = train[train.fare_amount>=0]\ntrain = train[(train['distance'] < 30) & (train['distance'] >=0 )]","3e2f7598":"fare_amount = train['fare_amount'].to_array()\npassenger_count = train['passenger_count'].to_array()\ndistance = train['distance'].to_array()","0d675388":"plt.figure(figsize=(8,5))\nsns.kdeplot(fare_amount).set_title(\"Verteilung des Fahrpreises\")","310dec2c":"plt.figure(figsize=(8,5))\nsns.kdeplot(distance).set_title(\"Distanz\")","a916f1cf":"#check max latitude und max longitude of test data\nprint(\"Max lat pickup: \" + str(test['pickup_latitude'].max()))\nprint(\"Max lat dropoff: \" + str(test['dropoff_latitude'].max()))\nprint(\"Max lon pickup: \" + str(test['pickup_longitude'].max()))\nprint(\"Max lon dropoff: \" + str(test['dropoff_longitude'].max()))\nprint(\"\")\nprint(\"Min lat pickup: \" + str(test['pickup_latitude'].min()))\nprint(\"Min lat dropoff: \" + str(test['dropoff_latitude'].min()))\nprint(\"Min lon pickup: \" + str(test['pickup_longitude'].min()))\nprint(\"Min lon dropoff: \" + str(test['dropoff_longitude'].min()))","73d5b562":"train.head(2)","b9dc8451":"#Parts of train data are too far away, so they can be dropped\ntrain = train[(train['pickup_longitude'] > -74.25) & (train['pickup_longitude'] < -72.98)]\ntrain = train[(train['pickup_latitude'] > 40.57) & (train['pickup_latitude'] < 41.70)]\ntrain = train[(train['dropoff_longitude'] < -72.99) & (train['dropoff_longitude'] > -74.26)]\ntrain = train[(train['dropoff_latitude'] > 40.56) & (train['dropoff_latitude'] < 41.69)]","3fb26938":"dropoff_longitude = train['dropoff_longitude'].to_array()\ndropoff_latitude = train['dropoff_latitude'].to_array()\n\ncity_long_border = (-74.03, -73.75)\ncity_lat_border = (40.63, 40.85)\n\nplt.figure(figsize=(10,6))\nplt.scatter(dropoff_longitude, dropoff_latitude,\n                color='green', \n                s=.02, alpha=.6)\nplt.title(\"Dropoffs\")\n\nplt.ylim(city_lat_border)\nplt.xlim(city_long_border)","b04ceed3":"## Kicking out unrelevant columns\nunnecessary_columns=['key','pickup_datetime']\ntrain=train.drop(unnecessary_columns,axis=1)\ntest=test.drop(unnecessary_columns,axis=1)","10d99779":"train.head(2)","034654ff":"test.head(2)","8c18fd41":"X=train.drop(['fare_amount'],axis=1)\ny=train['fare_amount']\n\n## getting the columns into the same order since XGB and LGBM seems to have a problem if they are not aligned\ntest = test[train.columns.drop(['fare_amount'])]\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\nprint(\"Number of records in training data \",X_train.shape[0])\nprint(\"Number of records in validation data \",X_test.shape[0])\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\n\n## free some storage by cleaning up the previous unneeded variables\ndel X, y\ngc.collect()","f0e28bfc":"lm = LinearRegression(fit_intercept = True, \n                      normalize = False,\n                      algorithm = \"eig\")\nlm.fit(X_train,y_train)","db3fc91d":"y_test_pred=lm.predict(X_test)\nscore_linear_reg = calc_R2(y_test_pred, 'Linear Regression')","a720da42":"y_pred_LR=lm.predict(test)\n\nsave_model_csv(y_pred_LR, 'submission_LinearReg')","0bd67c10":"params_ridge = {\n    \"alpha\": loguniform(1e-5, 1e0), # default 1.0\n    \"solver\": ['eig', 'cd'], \n}\nridge = Ridge()\nclf = RandomizedSearchCV(ridge, params_ridge, random_state=1, n_iter=100, cv=5, verbose=0, n_jobs=1)\nbest_model = clf.fit(X_train,y_train)","5c5ee209":"best_model.best_estimator_.get_params()","d1c12508":"ridge_params = {\n 'alpha': 0.240960447726532,\n 'fit_intercept': True,\n 'normalize': False,\n 'solver': 'eig'\n}\n\nridge = Ridge(**ridge_params)\nresult_ridge = ridge.fit(X_train,y_train)","6fd31d96":"y_test_pred = result_ridge.predict(X_test)\nscore_ridge_reg =calc_R2(y_test_pred, 'Ridge Regression')","89b14014":"y_pred_RR = result_ridge.predict(test)\nsave_model_csv(y_pred_RR, 'submission_RidgeReg')","73868267":"## params\nn_neighbors=4","42a36ede":"%%time\nknn_cuml = cumlKNR(n_neighbors=n_neighbors)\nknn_cuml.fit(X_train, y_train)","df3b3089":"y_test_pred = knn_cuml.predict(X_test)\nscore_KNN = calc_R2(y_test_pred, 'K-nearest Neigbors Regression')","6b18404a":"y_pred_KNN = knn_cuml.predict(test)\n\nsave_model_csv(y_pred_KNN, 'submission_KNN')","52b9609b":"## params\nn_estimators=10\nn_jobs=-1","185b9cc5":"%%time\nrf_cuml = cuRF(n_estimators=n_estimators)\nrf_cuml.fit(X_train, y_train)","9c02c12f":"y_test_pred = rf_cuml.predict(X_test)\nscore_Rand_For =calc_R2(y_test_pred, 'Random Forest')","26e69e6c":"y_pred_RF = rf_cuml.predict(test)\nsave_model_csv(y_pred_RF, 'submission_RandomForest')","8da7e87d":"params_gpu = {\n    'n_estimators':350,  \n    'max_depth':7,    \n    'learning_rate':0.003, \n    'subsample':0.9,    \n    'colsample_bytree':0.9,    \n    'missing':-999,    \n    'random_state':2020,    \n    'objective':'reg:linear',    \n    'gamma':0.3,    \n    'reg_alpha':0.01,    \n    'tree_method':'gpu_hist'  # THE MAGICAL PARAMETER\n}\n\n## if you would like to compare runtime on CPU choose this one:\nparams_cpu = {\n    'n_estimators':500,\n    'max_depth':7,\n    'learning_rate':0.0005,\n    'subsample':0.9,\n    'colsample_bytree':0.9,\n    'missing':-999,\n    'random_state':2020,\n    'objective':'reg:linear',\n    'gamma':0.3,\n    'reg_alpha':0.01,\n}","f328ae56":"start_time=time.time()\ntimetaken_gpu_xgb=time.time()-start_time\ntimetaken_gpu_xgb","d047f718":"%%time\ndef XGBmodel(X_train,X_test,y_train,y_test,params):\n    matrix_train = xgb.DMatrix(X_train,label=y_train)\n    matrix_test = xgb.DMatrix(X_test,label=y_test)\n    model=xgb.train(params=params,\n                    dtrain=matrix_train,num_boost_round=1000, \n                    early_stopping_rounds=100,evals=[(matrix_test,'test')])\n    return model\n\nXGB_model = XGBmodel(X_train,X_test,y_train,y_test,params_gpu)","57de05b9":"timetaken_gpu=time.time()-start_time\nprint(\"GPU time taken for XG Boost model: \",timetaken_gpu_xgb)","767cac8c":"y_test_pred = XGB_model.predict(xgb.DMatrix(X_test))\nscore_XGB_GPU = calc_R2(y_test_pred, 'XGBoost GPU')\n\ny_test_pred_xgb = y_test_pred","1fcbecbe":"y_pred_XGB = XGB_model.predict(xgb.DMatrix(test), ntree_limit = XGB_model.best_ntree_limit).tolist()\n\nsave_model_csv(y_pred_XGB, 'submission_XGboost_GPU')","73aab70c":"%%time\ndef dask_XGBmodel(X_train,X_test,y_train,y_test,params):\n    #matrix_test = xgb.DMatrix(data=dd.from_pandas(X_test.to_pandas(),1), label=dd.from_pandas(y_test.to_pandas(), 1))\n\n    #Drop Nan Values\n    X_test.nans_to_nulls()\n    X_test = X_test.dropna()\n\n    matrix_test = xgb.DMatrix(X_test,label=y_test)\n    model = dxgb.train(client, params=params, \n                       data=dd.from_pandas(X_train, 1), \n                       labels=dd.from_pandas(y_train, 1), \n                       num_boost_round=5000,\n                       early_stopping_rounds=10,\n                       evals=[(matrix_test,'test')]\n                      )\n    return model\n\ndask_XGB_model = dask_XGBmodel(X_train,X_test,y_train,y_test,params)\n\n#y_pred = dask_XGB_model.predict(testDGX)","59e326f8":"!rm -r \/opt\/conda\/lib\/python3.6\/site-packages\/lightgbm\n!git clone --recursive https:\/\/github.com\/Microsoft\/LightGBM\n!apt-get install -y -qq libboost-all-dev","0a611d1d":"%%bash\ncd LightGBM\nrm -r build\nmkdir build\ncd build\ncmake -DUSE_GPU=1 -DOpenCL_LIBRARY=\/usr\/local\/cuda\/lib64\/libOpenCL.so -DOpenCL_INCLUDE_DIR=\/usr\/local\/cuda\/include\/ ..\nmake -j$(nproc)","357a21d8":"!cd LightGBM\/python-package\/;python3 setup.py install --precompile\n!mkdir -p \/etc\/OpenCL\/vendors && echo \"libnvidia-opencl.so.1\" > \/etc\/OpenCL\/vendors\/nvidia.icd\n!rm -r LightGBM","47823486":"%%bash\ncd ..","76d2f1d4":"#!pip install lightgbm --install-option=--gpu --force-reinstall","d85803da":"import lightgbm as lgbm","9e40556b":"## Inspiration from https:\/\/www.kaggle.com\/dsaichand3\/lgbm-gpu\nparams = {\n        'boosting_type':'gbdt',\n        'objective': 'regression',\n        'nthread': 4,\n        'num_leaves': 31,\n        'learning_rate': 0.15,\n        'max_depth': -1,\n        'subsample': 0.8,\n        'bagging_fraction' : 1,\n        'max_bin' : 15,\n        'bagging_freq': 20,\n        'colsample_bytree': 0.6,\n        'metric': 'rmse',\n        'min_split_gain': 0.5,\n        'min_child_weight': 1,\n        'min_child_samples': 10,\n        'scale_pos_weight':1,\n        'zero_as_missing': True,\n        'seed':0,\n        'num_rounds':50000,\n        'device': 'gpu',\n        'gpu_platform_id': 0,\n        'gpu_device_id': 0\n    }","b8baa20b":"col_cat = X_train.select_dtypes('object').columns.tolist()\n\nd_train = lgbm.Dataset(X_train.to_pandas(), label = y_train.to_pandas(), categorical_feature = col_cat, free_raw_data=False)\nd_valid = lgbm.Dataset(X_test.to_pandas(), label = y_test.to_pandas(), categorical_feature = col_cat, free_raw_data=False)","b85920aa":"%%time\nmodel = lgbm.train(params_1, train_set = d_train, num_boost_round=10000, early_stopping_rounds=100, verbose_eval=10, valid_sets=d_valid)","286d55a8":"y_test_pred = model.predict(X_test, num_iteration = model.best_iteration)\n\nscore_LGBM = calc_R2(y_test_pred, 'LGBM GPU')\n\ny_test_pred_lgbm = y_test_pred","29224383":"y_pred_LGBM = model.predict(test, num_iteration = model.best_iteration)\n\nsave_model_csv(y_pred_LGBM, 'submission_LGBM_GPU')","fb0dffa5":"## ensembled prediction over splitted test data, 50 \/ 50 split between XGB and LGBM\nensembled_prediction = (0.5*np.expm1(y_test_pred_xgb))+(0.5*np.expm1(y_test_pred_lgbm))\n\nscore_LGBM_XGB = =calc_R2(ensembled_prediction, 'LGBM XGB ensemble')","e9bdb203":"## submitting the combined prediction, weighted 50 \/ 50\nensembled_prediction = (0.5*y_pred_XGB)+(0.5*y_pred_LGBM)\n\nsave_model_csv(ensembled_prediction, 'submission_ensemble.csv')","d0532535":"r2_score=[\n    score_linear_reg,\n    score_ridge_reg,\n    score_KNN,\n    score_Rand_For,\n    score_XGB_GPU,\n    score_LGBM,\n    #score_LGBM_XGB\n]\n\ncol={'R2 score':r2_score}\nmodels=[\n    'Linear Regression',\n    'Ridge Regression',\n    'KNN',\n    'Random Forest',\n    'XGB',\n    'LGBM',\n    #'LGBM & XGB'\n]\ndf=pd.DataFrame(data=col,index=models)\ndf\n\ndf.plot(kind='bar')","d5455daa":"Strange thing is, \"GPU Memory consumption\" is almost at limit with 41 Mio rows (15.1 GB, max. 15.9 GB) but \"Cumulative GPU usage\" still shows 0.00 %\".","0ea35290":"When running XG Boost distributed on several GPUs you need to install Dask_xgboost. Did this for some benchmarking, although it did not run on one GPU. You can spare this installation if you only have one GPU.","5793317c":"https:\/\/github.com\/rapidsai\/cuml\/blob\/branch-0.13\/notebooks\/kneighbors_regressor_demo.ipynb\n\n","79d9a509":"# 3. Data Cleaning\n\nBefore being able to start with the predictions, the data should be cleaned up and extended\/enriched by relevant columns to achieve better results. For this, we are splitting the key (date & time) into seperate columns and add new features like the driven distance. This approch showed the best results so far. There are some limitations given that cudf and pandas are partly incompatible so I decided to test it out with a precleaned dataset as well to see the improvements later.","b55ba422":"# 5. Ridge Regression\n\nThis model is a little buggy and fails every second time to load. The results compared to the linear regression are not really an improvement here with R2 values and submission score being pretty much identical and still having longer training time.","2c06337d":"Check for GPU","5d243c64":"# 10. Stacked Ensemble XGB and LGBM","5559fe87":"> # 9. LGBM model (only runs on Kaggle and not really on GPU)","5b67c940":"## XG Boost Dask model for distributed clients (Not running with one client)\n\n- https:\/\/www.kaggle.com\/sandeepkumar121995\/eda-data-cleaning-xg-boost\n- https:\/\/www.kaggle.com\/gunbl4d3\/xgboost-ing-taxi-fares\n- https:\/\/www.kaggle.com\/aerdem4\/m5-lofo-importance-on-gpu-via-rapids-xgboost\n- https:\/\/www.kaggle.com\/xhlulu\/ieee-fraud-xgboost-with-gpu-fit-in-40s\n\n","53f81212":"Error when using evals, turning it off results in Value Error. Not possible to convert matrix_test to something running, tested with pandas df, cudf, numpy. NaN cleanup again, still...\n- https:\/\/github.com\/dmlc\/xgboost\/issues\/2274\n- https:\/\/github.com\/rapidsai\/cudf\/issues\/2892","be7e77d9":"Running into several problems with XG Boost. Python Kernel stops witout error message \t\n- what(): parallel_for failed: cudaErrorNoKernelImageForDevice: no kernel image is available for execution on the device\n- https:\/\/www.kaggle.com\/c\/otto-group-product-classification-challenge\/discussion\/13308\n- https:\/\/github.com\/CannyLab\/tsne-cuda\/issues\/18\n\nHypothesis: There could be an error when loading a cudf dataset in XGBoost. Seems to not happen when using pandas","8357ed8b":"- https:\/\/docs.dask.org\/en\/latest\/dataframe-api.html#dask.dataframe.DataFrame.to_delayed\n\n- https:\/\/github.com\/dask\/dask-ml\/issues\/521","d1c72543":"# NYC fare prediction using Rapids and XG Boost running on GPU\n\n[This notebook](TBD) shows my approach for predicting the fare amount for a taxi ride in NYC when given the pickup and dropoff locations of the passangers regarding the [New York City Taxi Fare Prediction Challange]( https:\/\/www.kaggle.com\/c\/new-york-city-taxi-fare-prediction).\n\n\n\n---\n\nThis notebook is seperated into different sections, relating to the common data science workflow (except the hypothesis and data collection where already done). I tested different models to get used to Rapids and Cudf by starting with simple linear models and ended up with XG Boost and Light GBM. The best performant results are deffinitely the XG Boost ones. Different other Kernels state that they achieve similar results with LGBM hence I had problems with running the model on the GPU although I installed the right version (some bug reports also suggest there is a problem with the model).\n\n\n0.   Previous Commits\n1.   Setup and Check Infrastructure\n2.   Having a first look at the Data (EDA)\n3.   Data Cleaning (Feature Engineering)\n4.   Linear Regression GPU\n5.   Ridge Regression GPU\n6.   K-Nearest Neighbor Regression GPU\n7.   Random Forest GPU\n8.   XG Boost on GPU\n9.   Light GBM on GPU (not running on Colab)\n10.   Stacked Ensemble XGB and LGBM\n11.   Evaulation\n\n---\n\n## Approach:\n\nI started with this project by getting used to the Rapids environment and the Cuda librarys and went on for more advanced models (XGB and LGBM) with testing them on common parameters mentioned in different notebooks on kaggle (see inspiration in first section) to get a feeling what influences what and how does it generelly work. After gaining some experience I combined several data cleaning approaches from differnt notebooks and general information from the internet. The most promising score yet was 3.19347 which would result in rank 432 of the public leaderboard (if the competition would still run). My kaggle notebook can be found [here, user handle: AlexS2020](https:\/\/)\n\n\n## Evaluation:\nAs will be mentioned in the last section, I used to compare the different RMSE and R2 score values of different models as well as uploading the submission files to get a direct comperision via the score. Last step to evaluate are the confusion_matrices to see how many false positive\/negative and true postivie\/negative there are.\n\n## Conclusion:\nOverall it was an interesting project \/ competition which enabled me to learn new stuff and try out several tools. It is interesting to see, that the same code running on the GPU is so much faster compared to CPU implementations when looking at XGB for example. Interesting to me was also that relatively simple models like the KNN or Random Forest performed quite well in relation to the training runtime and ressource consumption. It would have been nice to directly compare the LBGM model against XGB since I read quite a bit about both. Unfortunately this was not possible within the desired scope due to runtime issues mostly. \n\nAnother interesting aspect was to see the huge difference a properly cleaned dataset can make when using the same model.\n\n\n## Outlook:\nThere are several ways how to improve these scores from my point of view. Some of them can be seen in other notebooks already (especially the larger ones).\n\n1.   Run the code on several GPUs to figure out how well Rapids really performs in combination with DASK compared to one GPU or CPU.\n2.   Ensemble different models based on their R2 score to test if the gap can be reduced this way (depending on the strength of the used models) An initial implementation for this is given in section 10. but I could not test it yet,\n3.   Use larger grid searches to find better parameters for the given models\n4.   An adaptive learning rate could also help with optimizing the results\n\n## Limitations:\n- It was possible to install the LBGM Version for GPU and the memory was consumed by the data but it did not run on the GPU somehow. So it was quite slow to do some training on this model and I could not compare it directly to XGB (or combine these two as intended in section 10)\n- Some of the points mentioned in Outlook were not possible since running code in browser + only 36h GPU-time per week is hardly enough\n- Automation of parameter search on large scale not feasible (not even with the GPU)\n","32614754":"Load the data using cudf. Path variables are adjusted accordingly ","9adb344f":"# 8. XG Boost","a8f07278":"## XG Boost model GPU","8047ff0d":"Before we can train something which predicts the taxi fares we first have to take a look what data we have available. We start by loading the data into cudf dataframes by selecting the relevant columns first. \n\n---\nIt happens that the system crashes if you choose a row number which is too large so that the data overflows your RAM \/ GPU (especially on colab, max 5 Mio rows). On Kaggle you can go up to max. 41 Mio. rows. If you choose to use the precleaned dataset you can use all the available data.","d2a1eefd":"Visualization of the data <br>\n\nThe following things were noticed (while using 500k datapoints):\n*   The minimal fare_amount is negative. As this does not seem to be realistic I will drop them from the dataset.\n*   Some of the minimum and maximum longitude\/lattitude coordinates are way off. These  will also be remove from the dataset. (bounding box will be defined)\n*   The average fare_amount is about 9.79 USD with a standard deviation of 7.48 USD. When building a predictive model we want to be better than 7.48 USD.\n\n","c224df26":"This section is just for showing that it runs. Due to the runtime bug that the training is not really running on the GPU it takes forever to train 1000 rounds and there is still a huge RMSE difference to XGB so I decided to focus more on XGB.\n\n- https:\/\/medium.com\/@pushkarmandot\/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc\n\n- https:\/\/www.kaggle.com\/nicapotato\/taxi-rides-time-analysis-and-oof-lgbm\n\n- https:\/\/www.kaggle.com\/dsaichand3\/lgbm-gpu\n\n- https:\/\/www.kaggle.com\/aerdem4\/rapids-svm-on-trends-neuroimaging\n\n\nCompared CPU vs GPU:\n- https:\/\/www.kaggle.com\/ishivinal\/sklearn-rapids-pandas\n","bd12e7b3":"Not available in Cudf datetime which could have improved results:\n- Dayofyear, Weekofyear, Weekday, Quarter, day of month","9191efb2":"Predictions are done when using the whole data set\n\n### Linear Regression\n\n**Commit 30 (Baseline) Score: 5.15497**\n  - R2 score = 0.7166260480880737\n\n### Ridge Regression\n\n**Commit 31 Score: 5.15489**\n\nparams_ridge\n  - alpha = loguniform(1e-5, 1e0)\n  - solver = ['eig', 'cd']\n  - n_iter = 100\n  - cv = 5\n  - verbose = 0\n  - n_jobs = 1\n\nridge_params\n  - alpha = 0.240960447726532\n  - fit_intercept = True\n  - normalize = False\n  - solver = 'eig'\n\n  - R2 score = 0.7166244983673096\n\n### K-Nearest Neighbor \n**Commit 33 Score: 4.67805**\n  - n_neighbors = 4\n  - data_size = 55 Mio rows\n  - R2 score = 0.7047221660614014\n\n### Random Forest \n**Commit 34 Score: 3.96958**\n  - n_estimators=10\n  - n_jobs=-1\n  - data_size = 55 Mio rows\n  - R2 score = 0.6802817583084106\n\n### XG Boost (GPU)\n**Commit 5 Score: 3.98546**\n  - n_estimators':500,\n  - max_depth':7,\n  - learning_rate':0.0005,\n  - subsample':0.9,\n  - colsample_bytree':0.9,\n  - missing':-999,\n  - random_state':2020,\n  - objective':'reg:linear',\n  - gamma':0.3,\n  - reg_alpha':0.01,\n  - tree_method':'gpu_hist'\n\n  - data_size = 7 Mio. rows\n\n### XG Boost (GPU) BEST\n**Commit 23 Score: 3.19116**\n  - n_estimators':350,\n  - max_depth':7,\n  - learning_rate':0.003,\n  - subsample':0.9,\n  - colsample_bytree':0.9,\n  - missing':-999,\n  - random_state':2020,\n  - objective':'reg:linear',\n  - gamma':0.3,\n  - reg_alpha':0.01,\n  - tree_method':'gpu_hist'  \n\n  - rounds = 80000\n  - early stopping = 250\n  - R2 score = 0.832559\n  - Test-rmse = 3.64371\n  - Wall time = 49 min\n  - data_size = 55 Mio. rows, cleaned dataset","aabe6ce5":"Adding distances to airports does not work using cudf due to data type error (Series, Float) although the normal distance function (as below) works well. Tried different converting approaches including .astype(...) as you can see below at one example.","ff87e5de":"##References:\nInspirations for implementation and design:\n\n- https:\/\/github.com\/rapidsai\/cuml\/tree\/branch-0.13\/notebooks \n- https:\/\/www.kaggle.com\/beniel\/rapids-cudf-tutorial\n- https:\/\/www.kaggle.com\/aerdem4\/rapids-svm-on-trends-neuroimaging\n- https:\/\/www.kaggle.com\/confirm\/xfeat-cudf-lightgbm-catboost-wip\n- https:\/\/www.kaggle.com\/aiswaryaramachandran\/baseline-model-xgboost-3-03-rmse\/data?select=train_cleaned.csv\n- https:\/\/www.kaggle.com\/aiswaryaramachandran\/eda-and-feature-engineering\/comments\n- https:\/\/www.kaggle.com\/btyuhas\/bayesian-optimization-with-xgboost\n- https:\/\/www.kaggle.com\/pnprabakaran\/ny-city-taxi-fare-exploration-with-dask-and-dxgb\n- https:\/\/www.kaggle.com\/dsaichand3\/lgbm-gpu\n- https:\/\/www.kaggle.com\/amar09\/fare-prediction-stacked-ensemble-xgboost-lgbm\/log\n- https:\/\/www.kaggle.com\/nicapotato\/taxi-rides-time-analysis-and-oof-lgbm\n\nThanks to anyone whos code I have used!\n\n\n","b6284ef3":"# 6. K-Nearest Neighbors Regression\n\n","e9a637e7":"Depending on the environment you might want to install different packages (or just link to them)","9efdcbb6":"Library import, important to notice here is we are using mostly cuml librarys for GPU access","d3beb058":"# 4. Linear Regression\nThis counts as our baseline model and showcases the most simple model here.","626de5f7":"In my approach to further improve the predictions I tried to combine the two most advanced models as suggested by https:\/\/www.kaggle.com\/amar09\/fare-prediction-stacked-ensemble-xgboost-lgbm\/log.","1bb11398":"## Setup LGBM with GPU Support (only working on Kaggle, not Colab)\n\nHow to setup LGBM GPU Beta:\n- https:\/\/www.kaggle.com\/vinhnguyen\/gpu-acceleration-for-lightgbm\n\n","77b2f548":"If you find yourself running these notebooks on Colab as well as on Kaggle you might find this placeholder thing helpful. Only thing to touch is the environment you are running on (Kaggle or Colab).\n\nOptional parameters:\n- cleaned_dataset when you are running on Kaggle an use a precleaned one\n- debug_mode for enabling a minimal row number\n- rows_datasample defining how many rows will be used in the training (irrelevant when debug_mode=True)","4982d020":"### Installing all necessary librarys and tools","30c1fcf1":"inspiration: https:\/\/www.kaggle.com\/cdeotte\/rapids-knn-30-seconds-0-938\/notebook","f9faf696":"# 1. Setup and Check Infrastructure\nThe first step is to get the infrastructure up and running. Important to mention here is that it is strongly recommended to use a GPU for calculation purpose.\n","f5bf6d0b":"Running nicely in Kaggle but not in Colab\n- TypeError: can not initialize DMatrix from DataFrame","b30c2dcf":"# 7. Random Forest GPU\n","c1285f86":"Bounding Box New York\n<table>\n  <tr>\n    <th><\/th>\n    <th>Dropoff<\/th>\n    <th>Pickup<\/th>\n  <\/tr>\n  <tr>\n    <td>Max Long<\/td>\n    <td>-72.99096<\/td>\n    <td>-72.986534<\/td>\n   <\/tr>\n   <tr>\n    <td>Max Lat<\/td>\n    <td>41.696682<\/td>\n    <td>41.709553<\/td>\n   <\/tr>\n   <tr>\n    <td>Min Long<\/td>\n    <td>-74.26323<\/td>\n    <td>-74.25219<\/td>\n    <\/tr>\n   <tr>\n    <td>Min Lat<\/td>\n    <td>40.568974<\/td>\n    <td>40.57314<\/td>\n   <\/tr>\n<\/table>\n\n","4e3846f2":"Splitting the train data here","2260ee2f":"Tested different columns to see which have the most impact. Pickup_datetime seems to be irrelevant for the predictions so dropping it together with key.","272859c0":"# 11. Evaluation\n\nShow R2 Scores of the different models, makes it more convenient to compare in one graph.\n","55b87feb":"# 2. First look at the Data (EDA)\n\nThe inital step to getting used to the data is having a look at it and loading it in a useable structure.","f387d61e":"# 0. Previous Commits - Comparison of different Models and Scores\n\n","610d9463":"Setting up the environment path accordingly to your choice of environment","8f3dfcfb":"### Helper Functions\n\nSince the save to CSV file and the calculation of the R2 score show up quite often during this kernel I decided to place them in small functions for an easy and reusable approach."}}