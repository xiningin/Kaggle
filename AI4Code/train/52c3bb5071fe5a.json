{"cell_type":{"104c9afa":"code","6f96b370":"code","5096811a":"code","52a76034":"code","f3a670b6":"code","2b375aca":"code","66ee67dd":"markdown","e952e88f":"markdown","bba0cddd":"markdown","ac28635c":"markdown","d94ea2a1":"markdown","4522dc15":"markdown"},"source":{"104c9afa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# for min_max scaling\nfrom sklearn.preprocessing import MinMaxScaler\n\n# plotting modules\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6f96b370":"scaler=MinMaxScaler()\n# Let's create a data for purpose of explaining.\n# generate 1000 data points randomly drawn from an exponential distribution\noriginal_data = np.random.exponential(size = 1000)\n\n# mix-max scale the data between 0 and 1\nscaled_data = scaler.fit_transform(original_data.reshape(-1,1))\n\n# plot both together to compare\nfig, ax=plt.subplots(1,2)\nsns.distplot(original_data, ax=ax[0])\nax[0].set_title(\"Original Data\")\nsns.distplot(scaled_data, ax=ax[1])\nax[1].set_title(\"Scaled data\")","5096811a":"# for Box-Cox Transformation\nfrom scipy import stats\n# normalize the exponential data with boxcox\nnormalized_data = stats.boxcox(original_data)\n\n# plot both together to compare\nfig, ax=plt.subplots(1,2)\nsns.distplot(original_data, ax=ax[0])\nax[0].set_title(\"Original Data\")\nsns.distplot(normalized_data[0], ax=ax[1])\nax[1].set_title(\"Normalized data\")","52a76034":"data = pd.read_csv(\"..\/input\/new-data\/data.csv\")\ndata.head()","f3a670b6":"# select the usd_goal_real column\nusd_goal = data.usd_goal_real\n# scale the goals from 0 to 1\nscaled_data_2 = scaler.fit_transform(original_data.reshape(-1,1))\n\n# plot the original & scaled data together to compare\nfig, ax=plt.subplots(1,2)\nsns.distplot(usd_goal, ax=ax[0])\nax[0].set_title(\"Original Data\")\nsns.distplot(scaled_data_2, ax=ax[1])\nax[1].set_title(\"Scaled data\")\n","2b375aca":"# get the index of all positive pledges (Box-Cox only takes postive values)\nindex_of_positive_pledges = data.usd_pledged_real > 0\n\n# get only positive pledges (using their indexes)\npositive_pledges = data.usd_pledged_real.loc[index_of_positive_pledges]\n\n# normalize the pledges (w\/ Box-Cox)\nnormalized_pledges = stats.boxcox(positive_pledges)[0]\n\n# plot both together to compare\nfig, ax=plt.subplots(1,2)\nsns.distplot(positive_pledges, ax=ax[0])\nax[0].set_title(\"Original Data\")\nsns.distplot(normalized_pledges, ax=ax[1])\nax[1].set_title(\"Normalized data\")","66ee67dd":"### Normalization","e952e88f":"### Scaling ","bba0cddd":"## Scaling:\nThis means that you\u2019re transforming your data so that it fits within a specific scale, like 0\u2013100 or 0\u20131. We want to scale data when using methods based on measures of the spread of data, like support vector machines, or SVM or k-nearest neighbors, or KNN. With these algorithms, a change of \u201c1\u201d in any numeric feature is given the same importance.\nwhat about if we\u2019re looking at something like height and weight???\n\nSo, by scaling your variables, we can help compare different variables on equal footing.","ac28635c":"## Normalization\nNormalization is a more radical transformation than Scaling. The point of normalization is to change your observations so that they can be described as a normal distribution.\nNormal distribution, also known as the \u201cbell curve\u201d,is a specific statistical distribution where a roughly equal observations fall above and below the mean, the mean and the median are the same, and there are more observations closer to the mean. The normal distribution is also known as the Gaussian distribution.\nIn general, we want to normalize the data if using a machine learning or statistics technique that assumes the data is normally distributed. Some examples of these include t-tests, ANOVAs, linear regression, linear discriminant analysis (LDA) and Gaussian naive Bayes. (Pro tip: any method with \u201cGaussian\u201d in the name probably assumes normality.)","d94ea2a1":"## **Time to apply what we learned on the a real data: -->**","4522dc15":"# Scaling vs. Normalization: What\u2019s the difference?\nOne of the reasons that it\u2019s easy to get confused between scaling and normalization is because the terms are sometimes used interchangeably. \n* **Scaling**, you\u2019re changing the range of your data. \n* **Normalization** you\u2019re changing the shape of the distribution of your data. each of these options.\n\nBelow we are going to show how to scale a data-->"}}