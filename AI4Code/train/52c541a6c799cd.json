{"cell_type":{"ec1b9849":"code","ca1f911f":"code","e9b4ee6d":"code","b82955bc":"code","e2bbe6e3":"code","82217282":"code","fda3c5bb":"code","b96da985":"code","4375d3d7":"code","e2830c71":"code","54aa3b9e":"code","0056b0fb":"code","e8e6f104":"code","93a3aa43":"code","956f11e8":"code","8baefc27":"code","18e435a1":"code","06723dea":"code","1905252a":"code","8c7f34ff":"code","cd8ebac1":"code","fa5668a6":"code","e8a22603":"code","273ec382":"code","af3fdf46":"code","702afab5":"code","a9c973b7":"code","45e11511":"code","8460c4fb":"code","7b0146f9":"code","17d36256":"code","8c7a1ee7":"code","6f973a82":"code","e00f1359":"code","64231577":"code","ef6d597f":"code","4be2dcb7":"code","3d79cda8":"code","1e6f2e0a":"code","41d40de1":"code","8b9691b9":"code","5f9a250b":"code","6ecd5460":"code","04f27de9":"code","91fd03b4":"code","8f89a9f3":"code","fa40128c":"code","8b34e2a5":"code","929eb36d":"code","426ebb68":"code","f349d230":"code","4bc2ec77":"code","2fa5e044":"code","7fc9858e":"code","cd18aab1":"code","1ebc1d98":"code","31b639ba":"code","1a356974":"code","027d4bf1":"code","5938364d":"code","3e9b0943":"code","17345b60":"code","dfa82b7f":"code","f519e211":"code","0eadc17d":"code","1f947a81":"code","65632853":"code","6435bf7d":"code","79ac3ad5":"code","543ae2e2":"code","f31b30dc":"code","9de2e30f":"code","3851243c":"code","01528640":"code","9d0c6a20":"code","eff04307":"code","7b1e9c71":"code","bee8e26f":"code","b9827ea7":"code","1b226c0f":"code","77207ab4":"code","8fc7da0f":"code","d6dd7f64":"code","b9c22c97":"code","6ea289da":"code","c19587dc":"code","1faa4603":"code","8ce54fda":"code","6fddbb3a":"code","ace8b49b":"code","8428ce87":"code","80d7ae49":"code","d6aa0e37":"code","6a7affb4":"markdown","545fd9fb":"markdown","a6a9784c":"markdown","5894c3a7":"markdown","9798188d":"markdown","c962eed6":"markdown","365e8aaa":"markdown","3be2fd6f":"markdown","5ab18bab":"markdown","bf5e0a38":"markdown","15fd6bd6":"markdown","455f5f38":"markdown","57a35309":"markdown","91a09371":"markdown","f7b9307c":"markdown","0a4cec6f":"markdown","e962f95e":"markdown","861b7c61":"markdown","d99cb35c":"markdown","ba4d2389":"markdown","c126be73":"markdown","0b1cf80f":"markdown","342c7cad":"markdown","8bb5437c":"markdown","ae29b9b5":"markdown","ddbd450c":"markdown","a0453a5e":"markdown","2f7944f7":"markdown","7f4bb1bc":"markdown","efc6cf01":"markdown","dd47c00d":"markdown","9046fedf":"markdown"},"source":{"ec1b9849":"import glob\nimport os\nimport time\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nprint(tf.__version__)\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.applications.mobilenet_v2 import preprocess_input\nfrom IPython import display","ca1f911f":"# asign some paths\ntrain_csv_path = '..\/input\/siim-isic-melanoma-classification\/train.csv'\ntest_csv_path = '..\/input\/siim-isic-melanoma-classification\/test.csv'\nimage_path = '..\/input\/siim-isic-melanoma-classification\/jpeg\/train\/'\n\n# read the csv data using pandas\ntrain_df = pd.read_csv(train_csv_path)\ntest_df = pd.read_csv(test_csv_path)\n\nprint(\"unique values in column 'target': {}\".format(list(train_df['target'].unique())))\ntarget_dis = list(train_df['target'].value_counts())\nbenign_per = target_dis[0]\/sum(target_dis)\nprint(\"target count distribution: {}\".format(target_dis))\nprint(\"benign percentage: {:.2f}% vs malignant: {:.2f}%\".format(benign_per*100, (1-benign_per)*100))","e9b4ee6d":"path_tfrec = '..\/input\/melanoma-128x128\/'\npath_jpg = '..\/input\/jpeg-melanoma-128x128\/train\/'\nIMAGE_SIZE = [128, 128]\n\nmalignant = train_df[train_df[\"target\"] == 1]  # list of malignant images\n\ndef preprocess_X():  # load the images into memory\n    X = []\n    for img in malignant.image_name.values:\n        img_name = path_jpg + img + '.jpg'\n        i = tf.keras.preprocessing.image.load_img(img_name) #color_mode='grayscale')\n        i = tf.keras.preprocessing.image.img_to_array(i)\n        i = preprocess_input(i)  # preprocessing fits the pixel value from -127.5 to 127.5\n        X.append(i)\n    return np.array(X)  # convert to numpy array","b82955bc":"X = preprocess_X()\nX.shape","e2bbe6e3":"def display_img(arr):\n    i = tf.keras.preprocessing.image.array_to_img(arr)\n    plt.imshow(i, cmap='gray')\n\nplt.figure(figsize=(7,7))\nfor i in range(9):\n    plt.subplot(3,3, i+1)\n    display_img(X[i])  ","82217282":"BUFFER_SIZE = 584\nBATCH_SIZE = 32  # from 128\nEPOCHS = 50  # from 50\nnoise_dim = 200  # from 100\nnum_examples_to_generate = 9\n\n# We will reuse this seed overtime (so it's easier to visualize progress in the animated GIF)\nseed = tf.random.normal([num_examples_to_generate, noise_dim])","fda3c5bb":"def augmentation_pipeline(image):\n    image = tf.image.random_flip_left_right(image)\n#     image = tf.image.resize(image, IMAGE_RESIZE)\n    return image","b96da985":"# Simple dataset processing with batch and shuffle\ndef get_dataset():\n    ds = tf.data.Dataset.from_tensor_slices(X)\n#     ds = ds.map(augmentation_pipeline)\n    ds = ds.shuffle(BUFFER_SIZE)\n    ds = ds.batch(BATCH_SIZE)\n    return ds\n    \ntrain_dataset = get_dataset()\n# inspect a batch\nn_batch = 0\nfor i in train_dataset:\n    n_batch += 1\nprint(f\"num of batch: {n_batch}, shape of each batch: {i.shape}\")","4375d3d7":"def make_generator_model():\n    model = tf.keras.Sequential()   # dense unit is configured to match soon tobe reshaped layer\n    model.add(layers.Dense(32*32*256, use_bias=False, input_shape=(noise_dim,)))  # starts with 1D array, input is noise array of 100\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    # 32x32 bcz there's 2 conv2D. 128\/2\/2=32\n    model.add(layers.Reshape((32, 32, 256)))\n\n    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n\n    return model\n\n# create the generator\ngenerator = make_generator_model()\ngenerator.summary()","e2830c71":"noise = tf.random.normal([1, noise_dim])  # outputs random values from normal dist. to a certain array shape\ngenerated_image = generator(noise, training=False)  # interesting, doesn't need .fit .predict or anything\n\nplt.imshow(generated_image[0, :, :, :]*255)#, cmap='gray')","54aa3b9e":"def make_discriminator_model():\n    model = tf.keras.Sequential()   # basic binary classification model\n    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n                                     input_shape=[128, 128, 3]))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n\n    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1))\n\n    return model\n\n# create D\ndiscriminator = make_discriminator_model()\nprint(discriminator.summary())","0056b0fb":"decision = discriminator(generated_image)\nprint(decision)","e8e6f104":"# This method returns a helper function to compute cross entropy loss (prob between 0 and 1)\ncross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)","93a3aa43":"def discriminator_loss(real_output, fake_output):\n    # ones_like creates array of ones with similar shape as the input array\n    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n    total_loss = real_loss + fake_loss\n    return total_loss","956f11e8":"def generator_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output), fake_output)","8baefc27":"generator_optimizer = tf.keras.optimizers.Adam(1e-4)  # but here they use the same Adam anyway\ndiscriminator_optimizer = tf.keras.optimizers.Adam(1e-4)","18e435a1":"checkpoint_dir = '.\/training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n                                 discriminator_optimizer=discriminator_optimizer,\n                                 generator=generator,\n                                 discriminator=discriminator)","06723dea":"# Notice the use of `tf.function`\n# This annotation causes the function to be \"compiled\".\n@tf.function\ndef train_step(images):\n    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        generated_images = generator(noise, training=True)\n        real_output = discriminator(images, training=True)\n        fake_output = discriminator(generated_images, training=True)\n\n        gen_loss = generator_loss(fake_output)\n        disc_loss = discriminator_loss(real_output, fake_output)\n\n    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n\n    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))","1905252a":"def generate_and_save_images(model, epoch, test_input):\n    # Notice `training` is set to False.\n    # This is so all layers run in inference mode (batchnorm).\n    predictions = model(test_input, training=False)  # same as num_examples_to_generate\n    fig = plt.figure(figsize=(12,12))\n\n    for i in range(predictions.shape[0]):\n        plt.subplot(3, 3, i+1)\n        plt.imshow(predictions[i, :, :, :] * 127.5 + 127.5, cmap='gray')\n        plt.axis('off')\n\n    #plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n    plt.show()","8c7f34ff":"def train(dataset, epochs):\n    for epoch in range(epochs):\n        start = time.time()\n\n        for image_batch in dataset:\n            train_step(image_batch)\n\n        # Produce images for the GIF as we go\n        display.clear_output(wait=True)\n        generate_and_save_images(generator,\n                                 epoch + 1,\n                                 seed)\n\n        # Save the model every 15 epochs\n        if (epoch + 1) % 15 == 0:\n            checkpoint.save(file_prefix = checkpoint_prefix)\n\n        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n\n        # Generate after the final epoch\n        display.clear_output(wait=True)\n        generate_and_save_images(generator, epochs, seed)","cd8ebac1":"train(train_dataset, EPOCHS)","fa5668a6":"benign = train_df[train_df[\"target\"] == 0]\nmalignant = train_df[train_df[\"target\"] == 1]","e8a22603":"def show_img(target, n=13):\n    img_name = target.image_name.values\n    ex_img = np.random.choice(img_name, n)  # grab n number of images\n    plt.figure(figsize=(15,15))\n    for i in range(n):\n        plt.plot(4,4  , i + 1)\n        img = plt.imread(image_path + ex_img[i]+'.jpg')\n        plt.savefig('\/kaggle\/working\/benign_gans_generated\/GAN_img_{:04d}.jpg'.format(i))\n        plt.imshow(img, cmap='gray')\n        plt.axis('off')\n    plt.tight_layout()\n               \n        # plt.savefig(\"result\/outputt.jpg\")\n","273ec382":"\n#benign = train_df[train_df[\"target\"] == 0]\n#malignant = train_df[train_df[\"target\"] == 1]\n","af3fdf46":"#os.mkdir('\/kaggle\/working\/benign_gans_generated\/')","702afab5":"show_img(benign)","a9c973b7":"#show_img(malignant)","45e11511":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    print(os.path.join(dirname))","8460c4fb":"import re\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly\nimport plotly.graph_objects as go\nimport cv2\nimport tensorflow as tf\nfrom kaggle_datasets import KaggleDatasets\nfrom functools import partial\nimport sklearn\nfrom tqdm import tqdm_notebook as tqdm\nimport gc\n%matplotlib inline","7b0146f9":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()","17d36256":"print('Device:', tpu.master())\nprint('Number of replicas:', strategy.num_replicas_in_sync)\nprint(\"Version of Tensorflow used : \", tf.__version__)","8c7a1ee7":"# import os\n# AUTOTUNE = tf.data.experimental.AUTOTUNE\n# GCS_PATH = os.listdir(\"..\/input\/siim-isic-melanoma-classification\/\")\n# BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n# IMAGE_SIZE = [1024, 1024]\n# SHAPE = [256, 256]","6f973a82":"AUTOTUNE = tf.data.experimental.AUTOTUNE\nGCS_PATH = KaggleDatasets().get_gcs_path(\"siim-isic-melanoma-classification\")\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nIMAGE_SIZE = [1024, 1024]\nSHAPE = [256, 256]","e00f1359":"print(\"Batch Size = \", BATCH_SIZE)\nprint(\"GCS Path = \", GCS_PATH)","64231577":"train = pd.DataFrame(pd.read_csv(\"..\/input\/siim-isic-melanoma-classification\/train.csv\"))\ntrain.head()","ef6d597f":"test = pd.DataFrame(pd.read_csv(\"..\/input\/siim-isic-melanoma-classification\/test.csv\"))\ntest.head()","4be2dcb7":"train.info()","3d79cda8":"test.info()","1e6f2e0a":"train_dir = \"\/kaggle\/input\/siim-isic-melanoma-classification\/jpeg\/train\/\"","41d40de1":"image_names = train[\"image_name\"].values + \".jpg\"\nrandom_images = [np.random.choice(image_names) for i in range(4)] # Generates a random sample from a given 1-D array\nrandom_images","8b9691b9":"sample_images = []","5f9a250b":"plt.figure(figsize = (12, 8))\nfor i in range(4) : \n    plt.subplot(2, 2, i + 1) \n    image = cv2.imread(os.path.join(train_dir, random_images[i]))\n    # cv2 reads images in BGR format. Hence we convert it to RGB\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    sample_images.append(image)\n    plt.imshow(image, cmap = \"gray\")\n    plt.grid(True)\n# Automatically adjust subplot parameters to give specified padding.\nplt.tight_layout()","6ecd5460":"def non_local_means_denoising(image) : \n    denoised_image = cv2.fastNlMeansDenoisingColored(image, None, 10, 10, 7, 21)\n    return denoised_image","04f27de9":"sample_image = cv2.imread(os.path.join(train_dir, random_images[0]))\n# cv2 reads images in BGR format. Hence we convert it to RGB\nsample_image = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)\ndenoised_image = non_local_means_denoising(sample_image)\n\n\nplt.figure(figsize = (12, 8))\nplt.subplot(1,2,1)\nplt.imshow(sample_image, cmap = \"gray\")\nplt.grid(False)\nplt.title(\"Normal Image\")\n\nplt.subplot(1,2,2)  \nplt.imshow(denoised_image, cmap = \"gray\")\nplt.grid(False)\nplt.title(\"Denoised image\")    \n# Automatically adjust subplot parameters to give specified padding.\nplt.tight_layout() ","91fd03b4":"#import shutil\n#shutil.rmtree(\"\/kaggle\/working\/\")","8f89a9f3":"def histogram_equalization(image) : \n    image_ycrcb = cv2.cvtColor(image, cv2.COLOR_RGB2YCR_CB)\n    y_channel = image_ycrcb[:,:,0] # apply local histogram processing on this channel\n    cr_channel = image_ycrcb[:,:,1]\n    cb_channel = image_ycrcb[:,:,2]\n    \n    # Local histogram equalization\n    clahe = cv2.createCLAHE(clipLimit = 2.0, tileGridSize=(8,8))\n    equalized = clahe.apply(y_channel)\n    equalized_image = cv2.merge([equalized, cr_channel, cb_channel])\n    equalized_image = cv2.cvtColor(equalized_image, cv2.COLOR_YCR_CB2RGB)\n    return equalized_image","fa40128c":"equalized_image = histogram_equalization(denoised_image)","8b34e2a5":"plt.figure(figsize = (12, 8))\nplt.subplot(1,3,1)\nplt.imshow(sample_image, cmap = \"gray\")\nplt.grid(False)\nplt.title(\"Normal Image\", fontsize = 14)\n\nplt.subplot(1,3,2)  \nplt.imshow(denoised_image, cmap = \"gray\")\nplt.grid(False)\nplt.title(\"denoised image after histogram processing\", fontsize = 14)\n\nplt.subplot(1,3,3)  \nplt.imshow(equalized_image, cmap = \"gray\")\nplt.grid(False)\nplt.title(\"Histogram equalized image\", fontsize = 14)\n# Automatically adjust subplot parameters to give specified padding.\nplt.tight_layout()","929eb36d":"def segmentation(image, k, attempts) : \n    vectorized = np.float32(image.reshape((-1, 3)))\n    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 20, 1.0)\n    res , label , center = cv2.kmeans(vectorized, k, None, criteria, attempts, cv2.KMEANS_PP_CENTERS)\n    center = np.uint8(center)\n    res = center[label.flatten()]\n    segmented_image = res.reshape((image.shape))\n    return segmented_image","426ebb68":"plt.figure(figsize = (12, 8))\nplt.subplot(1,1,1)\nplt.imshow(denoised_image, cmap = \"gray\")\nplt.grid(False)\nplt.title(\"de Noised Image\")","f349d230":"plt.figure(figsize = (12, 8))\nsegmented_image = segmentation(denoised_image, 3, 10) # k = 3, attempt = 10\nplt.subplot(1,3,1)\nplt.imshow(segmented_image, cmap = \"gray\")\nplt.grid(False)\nplt.title(\"Segmented Image with k = 3\")\n\nsegmented_image = segmentation(denoised_image, 4, 10) # k = 4, attempt = 10\nplt.subplot(1,3,2)\nplt.imshow(segmented_image, cmap = \"gray\")\nplt.grid(False)\nplt.title(\"Segmented Image with k = 4\")\n\nsegmented_image = segmentation(denoised_image, 5, 10) # k = 5, attempt = 10\nplt.subplot(1,3,3)\nplt.imshow(segmented_image, cmap = \"gray\")\nplt.grid(False)\nplt.title(\"Segmented Image with k = 5\")","4bc2ec77":"from sklearn.model_selection import train_test_split \ntraining_files, validation_files = train_test_split(tf.io.gfile.glob(GCS_PATH + \"\/tfrecords\/train*.tfrec\"),\n                                                   test_size = 0.1, random_state = 42)\n\ntesting_files = tf.io.gfile.glob(GCS_PATH + \"\/tfrecords\/test*.tfrec\")\n\nprint(\"Number of training files = \", len(training_files))\nprint(\"Number of validation files = \", len(validation_files))\nprint(\"Number of test files = \", len(testing_files))","2fa5e044":"def decode_image(image) : \n    image = tf.image.decode_jpeg(image, channels = 3)\n    image = tf.cast(image, tf.float32)\n    image = image \/ 255.0\n    return image","7fc9858e":"sample_images[0].shape","cd18aab1":"training_files","1ebc1d98":"sample_picked = training_files[0]\nsample_picked","31b639ba":"file = tf.data.TFRecordDataset(sample_picked)\nfile","1a356974":"feature_description = {\"image\" : tf.io.FixedLenFeature([], tf.string), \n                      \"target\" : tf.io.FixedLenFeature([], tf.int64)}","027d4bf1":"def parse_function(example) : \n    # The example supplied is parsed based on the feature_description above.\n    return tf.io.parse_single_example(example, feature_description)","5938364d":"parsed_dataset = file.map(parse_function)\nparsed_dataset","3e9b0943":"def read_tfrecord(example, labeled) : \n    if labeled == True : \n        tfrecord_format = {\"image\" : tf.io.FixedLenFeature([], tf.string),\n                           \"target\" : tf.io.FixedLenFeature([], tf.int64)}\n    else:\n        tfrecord_format = {\"image\" : tf.io.FixedLenFeature([], tf.string),\n                          \"image_name\" : tf.io.FixedLenFeature([], tf.string)}\n    \n    example = tf.io.parse_single_example(example, tfrecord_format)\n    image = decode_image(example[\"image\"])\n    if labeled == True : \n        label = tf.cast(example[\"target\"], tf.int32)\n        return image, label\n    else:\n        image_name = example[\"image_name\"]\n        return image, image_name     ","17345b60":"def load_dataset(filenames, labeled, ordered):\n    ignore_order = tf.data.Options()\n    if ordered == False: # dataset is unordered, so we ignore the order to load data quickly.\n        ignore_order.experimental_deterministic = False # This disables the order and enhances the speed\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTOTUNE) \n    dataset = dataset.with_options(ignore_order) \n    dataset = dataset.map(partial(read_tfrecord, labeled=labeled), num_parallel_calls=AUTOTUNE)\n    return dataset","dfa82b7f":"def image_augmentation(image, label) :     \n    image = tf.image.resize(image, SHAPE)\n    image = tf.image.random_flip_left_right(image)\n    return image, label","f519e211":"def get_training_dataset() : \n    dataset = load_dataset(training_files, labeled = True, ordered = False)\n    dataset = dataset.map(image_augmentation, num_parallel_calls=AUTOTUNE)\n    dataset = dataset.repeat()\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTOTUNE) \n    return dataset","0eadc17d":"def get_validation_dataset() : \n    dataset = load_dataset(validation_files, labeled = True, ordered = False)\n    dataset = dataset.map(image_augmentation, num_parallel_calls=AUTOTUNE)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.cache()\n    dataset = dataset.prefetch(AUTOTUNE) \n    return dataset","1f947a81":"def get_test_dataset() : \n    dataset = load_dataset(testing_files, labeled = False, ordered = True)\n    dataset = dataset.map(image_augmentation, num_parallel_calls=AUTOTUNE)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.cache()\n    dataset = dataset.prefetch(AUTOTUNE) \n    return dataset","65632853":"training_dataset = get_training_dataset()","6435bf7d":"validation_dataset = get_validation_dataset()","79ac3ad5":"def count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\n\nnum_training_images = count_data_items(training_files)\nnum_validation_images = count_data_items(validation_files)\nnum_testing_images = count_data_items(testing_files)\n\nSTEPS_PER_EPOCH_TRAIN = num_training_images \/\/ BATCH_SIZE\nSTEPS_PER_EPOCH_VAL = num_validation_images \/\/ BATCH_SIZE\n\nprint(\"Number of Training Images = \", num_training_images)\nprint(\"Number of Validation Images = \", num_validation_images)\nprint(\"Number of Testing Images = \", num_testing_images)\nprint(\"\\n\")\nprint(\"Numer of steps per epoch in Train = \", STEPS_PER_EPOCH_TRAIN)\nprint(\"Numer of steps per epoch in Validation = \", STEPS_PER_EPOCH_VAL)","543ae2e2":"image_batch, label_batch = next(iter(training_dataset))","f31b30dc":"def show_batch(image_batch, label_batch) :\n    plt.figure(figsize = (20, 20))\n    for n in range(8) : \n        ax = plt.subplot(2,4,n+1)\n        plt.imshow(image_batch[n])\n        if label_batch[n] == 0 : \n            plt.title(\"BENIGN\")\n        else:\n            plt.title(\"MALIGNANT\")\n    plt.grid(False)\n    plt.tight_layout()       ","9de2e30f":"show_batch(image_batch.numpy(), label_batch.numpy())","3851243c":"del image_batch\ndel label_batch\ngc.collect()","01528640":"malignant = len(train[train[\"target\"] == 1])\nbenign = len(train[train[\"target\"] == 0 ])\ntotal = len(train) \n\nprint(\"Malignant Cases in Train Data = \", malignant)\nprint(\"Benign Cases In Train Dataset = \",benign)\nprint(\"Total Cases In Train Dataset = \",total)\nprint(\"Ratio of Malignant to Benign = \",malignant\/benign)","9d0c6a20":"weight_malignant = (total\/malignant)\/2.0\nweight_benign = (total\/benign)\/2.0\n\nclass_weight = {0 : weight_benign , 1 : weight_malignant}\n\nprint(\"Weight for benign cases = \", class_weight[0])\nprint(\"Weight for malignant cases = \", class_weight[1])","eff04307":"callback_early_stopping = tf.keras.callbacks.EarlyStopping(patience = 15, verbose = 0, restore_best_weights = True)\n\ncallbacks_lr_reduce = tf.keras.callbacks.ReduceLROnPlateau(monitor = \"val_auc\", factor = 0.1, patience = 10, \n                                                          verbose = 0, min_lr = 1e-6)\n\ncallback_checkpoint = tf.keras.callbacks.ModelCheckpoint(\"melanoma_weights.h5\",\n                                                         save_weights_only=True, monitor='val_auc',\n                                                         mode='max', save_best_only = True)","7b1e9c71":"## Setting Evaluation Metric: \n\n# *This is taken from Amy Jang's notebook as mentioned earlier.*\n\n# *Notebook : https:\/\/www.kaggle.com\/amyjang\/tensorflow-transfer-learning-melanoma*\n\n# When we compile our model, we do not want our metric to be accuracy. If we run the model, with an accuracy metric, it will give us false confidence in our model. If we look at the dataset, we see that 98% of the images are classifed as benign, 0. Now, if accuracy was the sole determinant of our model, a model that always outputs 0 will achieve a high accuracy although the model is not good.\n\n# The competition scores the model by finding the area under the ROC curve, which is why our metric will be set to keras.metrics.AUC.","bee8e26f":"with strategy.scope() : \n    bias = np.log(malignant\/benign)\n    bias = tf.keras.initializers.Constant(bias)\n    base_model = tf.keras.applications.MobileNetV2(\n    input_shape=(SHAPE[0], SHAPE[1], 3), alpha=1.0,include_top=False,\n    weights='imagenet', input_tensor=None, classes=1000, pooling=None,\n    classifier_activation='softmax')\n    base_model.trainable = False\n    model = tf.keras.Sequential([base_model,\n                                 tf.keras.layers.GlobalAveragePooling2D(),\n                                 tf.keras.layers.Dense(20, activation = \"relu\"),\n                                 tf.keras.layers.Dropout(0.4),\n                                 tf.keras.layers.Dense(10, activation = \"relu\"),\n                                 tf.keras.layers.Dropout(0.3),\n                                 tf.keras.layers.Dense(1, activation = \"sigmoid\", bias_initializer = bias)                                     \n                                ])\n    model.compile(optimizer = tf.keras.optimizers.Adam(lr = 1e-2), loss = \"binary_crossentropy\", metrics = [tf.keras.metrics.AUC(name = 'auc')])\n    model.summary()\n    \n    EPOCHS = 5\n    history = model.fit(training_dataset, epochs = EPOCHS, steps_per_epoch = STEPS_PER_EPOCH_TRAIN,\n                       validation_data = validation_dataset, validation_steps = STEPS_PER_EPOCH_VAL,\n                       callbacks = [callback_early_stopping, callbacks_lr_reduce, callback_checkpoint],\n                       class_weight = class_weight)\n\n","b9827ea7":"n_epochs_it_ran_for = len(history.history['loss'])\nn_epochs_it_ran_for","1b226c0f":"X = np.arange(0,n_epochs_it_ran_for,1)\nplt.figure(1, figsize = (20, 12))\nplt.subplot(1,2,1)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.plot(X, history.history[\"loss\"], label = \"Training Loss\")\nplt.plot(X, history.history[\"val_loss\"], label = \"Validation Loss\")\nplt.grid(True)\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.plot(X, history.history[\"auc\"], label = \"Training Accuracy\")\nplt.plot(X, history.history[\"val_auc\"], label = \"Validation Accuracy\")\nplt.grid(True)\nplt.legend()","77207ab4":"testing_dataset = get_test_dataset()\ntesting_dataset_images = testing_dataset.map(lambda image, image_name : image)\ntesting_image_names = testing_dataset.map(lambda image, image_name : image_name)","8fc7da0f":"resulting_probabilities = model.predict(testing_dataset_images, verbose = 1)","d6dd7f64":"len(resulting_probabilities)","b9c22c97":"sample_submission_file = pd.read_csv(\"..\/input\/siim-isic-melanoma-classification\/sample_submission.csv\")\nsample_submission_file.head()","6ea289da":"del sample_submission_file[\"target\"]\nsample_submission_file.head()","c19587dc":"testing_image_names","1faa4603":"testing_image_names = np.concatenate([x for x in testing_image_names], axis=0)\ntesting_image_names = np.array(testing_image_names)","8ce54fda":"decoded_test_names = []\nfor names in testing_image_names : \n    names = names.decode('utf-8')\n    decoded_test_names.append(names)\ndecoded_test_names = np.array(decoded_test_names)\ndel testing_image_names","6fddbb3a":"len(decoded_test_names), type(decoded_test_names), decoded_test_names.shape","ace8b49b":"decoded_test_names","8428ce87":"testing_image_names = pd.DataFrame(decoded_test_names, columns=[\"image_name\"])\ntesting_image_names.head()","80d7ae49":"pred_dataframe = pd.DataFrame({\"image_name\" : decoded_test_names, \n                               \"target\" : np.concatenate(resulting_probabilities)})\npred_dataframe","d6aa0e37":"sample_submission_file = sample_submission_file.merge(pred_dataframe, on = \"image_name\")\nsample_submission_file.to_csv(\"submission.csv\", index = False)\nsample_submission_file.head()","6a7affb4":"## Local Histogram Pre-Processing\n\nFirst of all, why can't we apply histogram equalization directly to an RGB image?\nHistogram equalization is a non-linear process. Channel splitting and equalizing each channel separately is incorrect. *Equalization involves intensity values of the image, not the color components*. \n\nSo for a simple RGB color image, histogram equalization cannot be applied directly on the channels.*It needs to be applied in such a way that the intensity values are equalized without disturbing the color balance of the image. So, the first step is to convert the color space of the image from RGB into one of the color spaces that separates intensity values from color components. Some of the possible options are HSV\/HLS, YUV, YCbCr, etc. YCbCr is preferred as it is designed for digital images. Perform histogram equalization on the intensity plane Y. Now convert the resultant YCbCr image back to RGB.*\n\n(Excerpt taken from :\n\nhttps:\/\/prateekvjoshi.com\/2013\/11\/22\/histogram-equalization-of-rgb-images\/ )\n\nAn illustration of histogram equalization : **Observe the intensity difference**\n![image.png](attachment:image.png) \n\nHere the third one is actually local histogram equalization, where we equalize intensities inside a rolling window of certain dimension instead of the whole image at once.","545fd9fb":"## Model Design : MobileNetV2\n\nA supercool resource : **https:\/\/machinethink.net\/blog\/mobilenet-v2\/**","a6a9784c":"* At this point the dataset contains serialized **tf.train.Example** messages. When iterated over it returns these as scalar string tensors. ","5894c3a7":"## Let's look at the libraries we need : \n\n* Numpy : For working on ndarrays. It's a base library in any Machine Learning application.\n* Pandas : Handling csvs'. It's dataframe data structure allows us to quickly derieve insights from our csv data.\n* Matplotlib, Seaborn and Plotly : Plotting libraries for Python3.\n* cv2 : Open source library for Image Processing and Computer Vision.\n* Tensorflow : A deep learning framework for designing models and input data pipelines.\n* partial : Basically used when there are cases of optional arguments. This keeps the pipeline intact. \n* sklearn : This is for yet another beautifully optimized ML library having off-the-shelf ML algorithms implemented. \n* tqdm : For visualizing progress during a loop or any iteration for that matters.\n* gc : Garbage collection. This is for freeing the underlying memory from temporary variables and references.","9798188d":"Let's initialize our TPU !!","c962eed6":"## Image Segmentation :\n\nIs the technique of dividing or partitioning an image into parts, called segments. It is mostly useful for applications like image compression or object recognition, because for these types of applications, it is inefficient to process the whole image.\n\nWe will use **K-means clustering algorithm** to segment the images.\n\nK-Means Segmentation Approach Using OpenCV\n\n* `samples` : It should be of np.float32 data type, and each feature should be put in a single column. Here we have 3 channels, so every channel features have to be in one column. So, total columns we have are 3, while we don't care about the number of rows, hence -1. So, shape : (-1, 3).\n\n* `nclusters(K)` : Number of clusters required at end.\n\n* `criteria` : It is the iteration termination criteria. When this criteria is satisfied, algorithm iteration stops. Actually, it should be a tuple of 3 parameters. They are `( type, max_iter, epsilon )`:\n\nType of termination criteria. It has 3 flags as below:\n\n1. `cv.TERM_CRITERIA_EPS` - stop the algorithm iteration if specified accuracy, epsilon, is reached.\n2. `cv.TERM_CRITERIA_MAX_ITER` - stop the algorithm after the specified number of iterations, max_iter.\n3. `cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER` - stop the iteration when any of the above condition is met.\n\n*max_iter - An integer specifying maximum number of iterations. epsilon - Required accuracy*\n\n* `attempts` : Flag to specify the number of times the algorithm is executed using different initial labellings. The algorithm returns the labels that yield the best compactness. This compactness is returned as output.\n\n* `flags` : This flag is used to specify how initial centers are taken. Normally two flags are used for this : cv.KMEANS_PP_CENTERS and cv.KMEANS_RANDOM_CENTERS.\n\n****\n\nOutput parameters :\n\n* `compactness` : It is the sum of squared distance from each point to their corresponding centers.\n* `labels` : This is the label array (same as 'code' in previous article) where each element marked '0','1'.....\n* `centers` : This is array of centers of clusters.","365e8aaa":"## Malignant VS Benign Imbalance Analysis ","3be2fd6f":"TFRecord files of training and testing are mixed. Therefore we don't know whether the file being input has a label of malignant\/benign associated with it or not! Hence, we will supply both example and label to it, and simply set the label to False in the function call stack; in case label is absent. ","5ab18bab":"## Class Weights Initialization : \n\nWe saw earlier that malignant cases happen to be far less than the benign ones. Hence, we ought to set relatively heavy weights for malignant cases compared to the benign ones. This will encourage the model to pay more attention to malignant ones.\n\n`According to official Tensorflow documentation : `\n\n*Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class*.","bf5e0a38":"We observe our training_files object stores all tfrecord files. Let's pick one to analyze. ","15fd6bd6":"Let's free up some memory","455f5f38":"## The journey continues!!\n\nThe pre-processing we did earlier, I'll soon share the dataset of denoised, histogram equalized images. Then using the approach here in this notebook, we will use tf.data API to directly train on them!!\n\nStay Tuned!~~\n\nIn the meantime, you can view my other works in the SIIM ISIC Melanoma Challenge : \n\n* **https:\/\/www.kaggle.com\/fireheart7\/melanoma-a-story-in-3-parts-part-one**\n* **https:\/\/www.kaggle.com\/fireheart7\/melanoma-a-story-in-3-parts-part-two**","57a35309":"## TFRecord = Array of Examples : \n\nA TFRecord file contains an array of `Examples`. `Example` is a data structure for representing a record, like an observation in a training or test dataset. A record is represented as a set of features, each of which has a name and can be an array of bytes, floats, or 64-bit integers. \n\nTo summarize:\n\n* An Example contains Features.\n* Features is a mapping from the feature names stored as strings to Features.\n\nThese relations are defined in `example.proto` and `feature.proto` in the TensorFlow's source code, along with extensive comments. As the extension `.proto` suggests, these definitions are based on `protocol buffers`.\n\n## Why Protocol Buffers?\n\n![image.png](attachment:image.png) \n\nGoogle\u2019s Protocol buffers are a serialization scheme for structured data. In other words, protocol buffers are used for serializing structured data into a byte array, so that they can be sent over the network or stored as a file. In this sense, it is similar to JSON, XML.\n\n**Protocol buffers can offer a lot faster processing speed compared to text-based formats like JSON or XML.**","91a09371":"\nLet's observe the number of epochs our model ran for before callbacks stopped the execution due to no further significant improvement in validation_accuracy.","f7b9307c":"## Image Denoising :\n\nMany image smoothing techniques like Gaussian Blurring, Median Blurring etc were good to some extent in removing small quantities of noise. In those techniques, we took a small neighbourhood around a pixel and performed some operations like gaussian weighted average, median of the values etc to replace the central element. In short, noise removal at a pixel was local to its neighbourhood.\n\nThere is a property of noise. **Noise is generally considered to be a random variable with zero mean.**\n\nSuppose we hold a static camera to a certain location for a couple of seconds. This will give us plenty of frames, or a lot of images of the same scene. Then averaging all the frames, we compare the final result and first frame. Reduction in noise would be easily observed.\n\nSo idea is simple, we need a set of similar images to average out the noise. Considering a small window (say 5x5 window) in the image, chance is large that the same patch may be somewhere else in the image. Sometimes in a small neighbourhood around it. Hence, using these similar patches together averaging them can lead to an efficient denoised image.\n\nThis method is **Non-Local Means Denoising. It takes more time compared to blurring techniques, but the result are very satisfying.**\n\nDenoising illustration :\n![image.png](attachment:image.png) ","0a4cec6f":"Note below we have a condition for disabling the order. This will come in handy, as for training and validation purposes, we don't need to take care of the order. However, for test images we have to submit the predictions and they have to be in order. Hence, for them `experimental_deterministic will remain true`. The role of `experimental_deterministic` setting is to `disable data order reinforcement.`\n\nMore detail can be found at : **https:\/\/www.kaggle.com\/docs\/tpu**","e962f95e":"## Defining CallBacks : \n\nA callback is a powerful tool to customize the behavior of a Keras model during training, evaluation, or inference. Callbacks are useful to get a view on internal states and statistics of the model during training.\n\nThis can be used to stop predictions when there is no change in the desired metric over a certain epoch range. This is amazingly useful in order to avoid `overfitting`. \n\nHere we use : \n\n* ModelCheckPoint : Callback to save the Keras model or model weights at some frequency.\n* EarlyStopping : Stop training when a monitored metric has stopped improving.","861b7c61":"A TFRecord file can be read using **tf.data.TFRecordDataset** class.","d99cb35c":"## Bias Initialization : \n\nSince the dataset is heavily imbalanced, we may want to assign different weights to different classes. Setting an initial bias is important in such cases.","ba4d2389":"For test images, we will return the image name instead of the label.","c126be73":"## Viewing Our Training Images : \n\nLet's plot some of our training images : ","0b1cf80f":"# Melanoma Classification : Preprocessing cum Model Development\n\n![image.png](attachment:image.png)\n\n## Notebook III of III\n\nThis notebook is the III one in my series of work in this competition. As always shout out to amazing kernel authors present here at kaggle!! I got loads of inspiration from them. I believe this is the best thing about the ML community. The extent of collaboration and guidance one can seek here is inexplicable!!\n\n## Previous works : \n* Dataset preparation(Notebook I) : \n**https:\/\/www.kaggle.com\/fireheart7\/melanoma-a-story-in-3-parts-part-one**\n\n* Exploratory Data Analysis(Notebook II) : \n**https:\/\/www.kaggle.com\/fireheart7\/melanoma-a-story-in-3-parts-part-two?scriptVersionId=38737733**","342c7cad":"*This means that it is composed of a tensors and we need to parse it in order to make some meaning of it. For that very purpose it becomes mandatory to define a feature_description as tensorflow TFRecord datasets use Tensorflow's graph execution instead of Eager execution**. \n\nBasically : \n\n* Eager execution is what has been done so far. You write a statement and execute it. The results appear instantaneously. This imperative way of programming is inspired from Python3.\n\n* On the other hand, Graph execution is another way of defining control flow where we construct a structure in which we define how a tensor flows( hence the name tensorflow, I guess). This means say at the first node there has to be addition of two tensors, at the second rescaling, third divison and in the final node an activation function is there to return the final answer. \n\n* Graph execution is extremely useful in areas where Python interpreter is absent like in Android applications, and so on. Here, due to computational graphs, tensorflow models can still be deployed. This makes tensorflow an amazing tool for model deployment. However, that's a story for another day.","8bb5437c":"Due to callbacks, best weights are automatically restored!","ae29b9b5":"# Load The Datasets : ","ddbd450c":"# Model Construction : ","a0453a5e":"* **map( )** : This transformation applies map_func to each element of this dataset, and returns a new dataset containing the transformed elements\n\n* **repeat( )** : Repeats this dataset so each original value is seen count times.\n\n* **shuffle( )** : Randomly shuffles the elements of this dataset.\n\n* **batch( )** : Combines consecutive elements of this dataset into batches.\n\n* **prefetch( )** : Most dataset input pipelines should end with a call to prefetch. This allows later elements to be prepared while the current element is being processed. prefetch( ) doesn\u2019t allow CPU stand idle. When model is training prefetch continue prepare data while GPU is busy.\n\n* **cache( )** : The first time the dataset is iterated over, its elements will be cached either in the specified file or in memory. Subsequent iterations will use the cached data. When caching to a file, the cached data will persist across runs. Even the first iteration through the data will read from the cache file. ","2f7944f7":"Upload the updated training and test csv obtained in the EDA notebook. There we filled the missing values and saved them in our custom dataset folder.\n\n**Dataset : https:\/\/www.kaggle.com\/fireheart7\/melanoma-image-insights**","7f4bb1bc":"Some key modules used :\n\n* **tf.data.Options()** : An Options object can be, for instance, used to control which graph optimizations to apply. \n\n* **.experiemental_deterministic** : `experiemntal_deterministic` refers to whether the outputs need to be produced in deterministic order. If None, defaults to True. Here, the data is unordered, hence we don't need to process it in an order which may slow down our speed. \n\n* **TFRecordDataset** : A Dataset comprising records from one or more TFRecord files.\n\n* **num_parallel_reads** argument in TFRecordDataset : A tf.int64 scalar representing the number of files to read in parallel.","efc6cf01":"## OpenCV implementation of the aforementioned approach :\ncv2.fastNlMeansDenoisingColored() - Works on Colored images cv2.fastNlMeansDenoising() - Works on graysacle images\n\nCommon arguments are:\n\n* h : parameter deciding filter strength. Higher h value removes noise better, but removes details of image also. (10 is ok)\n* hForColorComponents : same as h, but for color images only. (normally same as h)\n* templateWindowSize : should be odd. (recommended 7)\n* searchWindowSize : should be odd. (recommended 21)","dd47c00d":"## Tensorflow Records : \n\nMany folks don't really understand this! So let's have some insight before we actually dive into creating our model.\n\n**What is Tensorflow Record?**\n\nIt's Tensorflow's binary storage format for your data. \n\n![image.png](attachment:image.png)\n\n**Okay, so what's the endgame? Aren't JPEG, PNG also storage formats? What makes this one special ?**\n\nWell, short answer : It's Google's.\nUmmm.. a little longer answer would be that binary storage data takes up relatively low space on your disk, and hence it takes less time to copy and can be read much more efficiently! Moreover, the *tensorflow* framework is optimized to handle tfrecords amazingly well. \n\nThe datasets that are too large to be stored fully in memory, this is an advantage as only the data that is required at the time (e.g. a batch) is loaded from disk and then processed. \n\nAnother major advantage of TFRecords is that it is possible to store sequence data \u2014 for instance, a time series or word encodings \u2014 in a way that allows for very efficient and (from a coding perspective) convenient import of this type of data.\n\n*In a nutshell, it's cool !*","9046fedf":"**Please consider upvoting if you find the overall series of notebooks useful! I have tried explaining the steps performed in profound details.**\n\nThank you! \nLet's begin!!~~"}}