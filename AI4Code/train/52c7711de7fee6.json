{"cell_type":{"2743f9fe":"code","21442f96":"code","3144ae5b":"code","4246938d":"code","315ad37e":"code","7cb867bc":"code","f5f1b53c":"code","3114d97f":"code","0d835e5a":"code","5306949a":"code","a096f62e":"code","a2cee2ff":"code","9e5ae956":"code","38d6281c":"code","de1b2140":"code","3289a3ac":"code","b7fa8133":"code","7e8be177":"code","1f594f43":"code","91465b5c":"code","bf457294":"code","2090df48":"code","09b01904":"code","71edb907":"code","2a1d2dad":"code","1586e6ce":"code","6888f59f":"code","e03df690":"code","5b42da55":"code","f5fd3e1d":"code","680c41e5":"code","e6c8e63d":"code","ee1a47d2":"code","244b6baf":"code","8835ffa3":"code","a810e1ea":"code","ae67f108":"code","a0a237dc":"code","6385f0f0":"code","b604887d":"code","5cdf9842":"code","bdc270e2":"code","40d5b71a":"code","ec7ed9d5":"code","04a27c20":"code","cf5d95f7":"code","222f7f71":"code","69ef6911":"code","2942852a":"code","d43babdf":"markdown","8d56bfb8":"markdown","37f40158":"markdown","de5596bb":"markdown","6da67c97":"markdown","281716c2":"markdown","160ea574":"markdown","46613b14":"markdown","45730868":"markdown","56fbf162":"markdown","9c264c00":"markdown","a1007295":"markdown","76a7d6cc":"markdown","b374e542":"markdown","d2a9dda0":"markdown"},"source":{"2743f9fe":"# import pandas as pd\nfrom pandas_profiling import ProfileReport\n\n# import matplotlib.pyplot as plt\n# import numpy as mp \nfrom sklearn.metrics import mean_absolute_error\n\nfrom pdpbox.pdp import pdp_isolate, pdp_plot, pdp_interact, pdp_interact_plot\n\nimport shap\n\n\n# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# %matplotlib inline\n\n# validation\nfrom sklearn.model_selection import train_test_split\n\n# encoders\nfrom category_encoders import OneHotEncoder, OrdinalEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n# metrics\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, roc_auc_score\n\n# pipeline\nfrom sklearn.pipeline import make_pipeline\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.linear_model import LinearRegression\n\n\n# Boosted Models\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfrom xgboost import XGBRegressor\n\n# Displaying images in code cell output\n# from IPython.display import Image\n# from IPython.core.display import HTML \n\n# Permutation Importance\nfrom sklearn.inspection import permutation_importance","21442f96":"df = pd.read_csv('..\/input\/world-university-rankings\/cwurData.csv')","3144ae5b":"df.head()","4246938d":"df.shape","315ad37e":"report  = ProfileReport(df).to_notebook_iframe()","7cb867bc":"#Checking for outlier on score feature, scatter plot\n \n\nplt.scatter(df['institution'], df['score'])\nplt.xlabel('Institution')\nplt.ylabel('Score')\nplt.show()","f5f1b53c":"df.head(1)","3114d97f":"#Wrangle function\ndef wrangle (df):\n    df = pd.read_csv('..\/input\/world-university-rankings\/cwurData.csv',\n                     parse_dates=['year'], index_col='year' )\n    \n    #Drop columns due to high-cardinality\n    df = df.drop(columns=['institution'])\n    \n    #Drop column with to many Nans; broad_impact has 200 Nans\n    df = df.drop(columns=['broad_impact'])\n                          \n    #Encoding the country featrure, checking for USA\n    df['USA'] = df['country'].apply(lambda x:1 if x=='USA' else 0)\n    \n    #Drop country feature due to high-cardinality and it was already encoded \n    #in the previous step\n    df = df.drop(columns=['country'])\n   \n    #Drop world_rank feature due to leakage\n    df = df.drop(columns=['world_rank'])\n    \n    \n    return df\n\ndf = wrangle(df)","0d835e5a":"df.head(1)","5306949a":"df.shape","a096f62e":"df.shape","a2cee2ff":"df.head(1)","9e5ae956":"#Shuffle rows randomly because of the Numpy array slicing below. \ndf = df.sample(random_state=42, frac=1)","38d6281c":"#Setting up a target\ntarget = 'score'\ny = df[target]\nX = df.drop(columns=target)","de1b2140":"y.shape","3289a3ac":"X.shape","b7fa8133":"#Numpy array slicing \nX_train, y_train = X[:-600], y[:-600]\nX_val, y_val = X[-600:-300], y[-600:-300]\nX_test, y_test = X[-300:], y[-300:]","7e8be177":"print(X_train.shape, X_val.shape, X_test.shape)","1f594f43":"print(X_train.shape, X_val.shape, X_test.shape)","91465b5c":"#Baseline\nprint('Average World University Ranking:', y_train.mean())\nbaseline_pred = [y_train.mean()] * len(y_train)\nprint('Baseline MAE:', mean_absolute_error(y_train, baseline_pred))","bf457294":"model_lr = LinearRegression()\nmodel_lr.fit(X_val, y_val)","2090df48":"model_rf = RandomForestRegressor(random_state=42, n_jobs=-1)\nmodel_rf.fit(X_train, y_train)","09b01904":"model_xgbregressor = XGBRegressor(random_state=42, n_jobs=-1)\nmodel_xgbregressor.fit(X_train, y_train)","71edb907":"#Metric Linear Regression\ntraining_mae = mean_absolute_error(y, model_lr.predict(X))\nprint('Training MAE:', training_mae)\nprint('Validation MAE:', mean_absolute_error(y_val, model_lr.predict(X_val)))","2a1d2dad":"#Metrics Random Forest\nprint('Training MAE', mean_absolute_error(y_train, model_rf.predict(X_train)))\nprint('Validation MAE', mean_absolute_error(y_val, model_rf.predict(X_val)))","1586e6ce":"#Metrics XGBRegressor\nprint('Training MAE', mean_absolute_error(y_train, model_xgbregressor.predict(X_train)))\nprint('Validation MAE', mean_absolute_error(y_val, model_xgbregressor.predict(X_val)))","6888f59f":"df.head(1)","e03df690":"perm_imp = permutation_importance(model_xgbregressor, X_val, y_val, n_jobs=-1, random_state=42)","5b42da55":"perm_imp","f5fd3e1d":"data = {'imp_mean': perm_imp['importances_mean'],\n       'imp_std': perm_imp['importances_std']}\n\nimportances = pd.DataFrame(data, index=X_val.columns).sort_values(by='imp_mean')\n\nimportances.tail()","680c41e5":"importances['imp_mean'].tail(10).plot(kind='barh')\nplt.show()","e6c8e63d":"#Partial depence plot\nfeature = 'publications'\nisolate = pdp_isolate(model_xgbregressor, \n                     dataset=X_val,\n                     model_features=X_val.columns,\n                     feature=feature)\n\npdp_plot(isolate, feature_name=feature)","ee1a47d2":"#Partial dependece plot\nfeature = 'patents'\nisolate = pdp_isolate(model_xgbregressor, \n                     dataset=X_val,\n                     model_features=X_val.columns,\n                     feature=feature)\n\npdp_plot(isolate, feature_name=feature)","244b6baf":"#Partial dependece plot; two features\nfeatures = ['publications', 'patents']\ninteract = pdp_interact(model_xgbregressor, \n                     dataset=X_val,\n                     model_features=X_val.columns,\n                     features=features)\n\npdp_interact_plot(interact, plot_type='grid', feature_names=features);","8835ffa3":"df.head(1)","a810e1ea":"X_val.head(1)","ae67f108":"#Shapley Plots\nrow = X_val.head(1)\n\nexplainer = shap.TreeExplainer(model_rf)\nshap_values = explainer.shap_values(row)\nshap_values","a0a237dc":"shap_df = pd.DataFrame(data=shap_values[0], index=X_val.columns, columns=['shap_value'])","6385f0f0":"shap_df['shap_value'].sum()","b604887d":"shap.initjs()\nshap.force_plot(\n                base_value=explainer.expected_value,\n                shap_values=shap_values,\n                features=row)","5cdf9842":"number1_world_ranking = df[df['score'] == 100]","bdc270e2":"number1_world_ranking","40d5b71a":"harvard = number1_world_ranking[2:-1]","ec7ed9d5":"harvard","04a27c20":"row = harvard\n\nexplainer = shap.TreeExplainer(model_rf)\nshap_values2 = explainer.shap_values(harvard)\nshap_values2","cf5d95f7":"shap_df2 = pd.DataFrame(data=shap_values2[0], index=number1_world_ranking.columns, columns=['shap_value'])","222f7f71":"shap_df2['shap_value'].sum()","69ef6911":"shap.initjs()\nshap.force_plot(\n                base_value=explainer.expected_value,\n                shap_values=shap_values2,\n                features=harvard)","2942852a":"shap_values2 = explainer.shap_values(X_val) \nshap.summary_plot(shap_values2, X_val)","d43babdf":"**Shapley Plot for Harvard University 2012**","8d56bfb8":"**Check Metrics Linear Regression, Random forest, and XGBRegressor**","37f40158":"**Establish Baseline**","de5596bb":"**Shapley Summary Plot**","6da67c97":"**Linear Regression Model**","281716c2":"**Random Forest Model**","160ea574":"**Partial Dependence Plots**","46613b14":"**EDA**","45730868":"**Split Data**","56fbf162":"**Permutation Importance**","9c264c00":"**WRANGLE DATA**","a1007295":"**Shapley Plots**","76a7d6cc":"**Linear Regression, Random Forest,and XGBoost**","b374e542":"In this notebook, you'll find World University Rankings and what makes a university rank number one. At first, I thought that endowment size would play a significant role; after all, having the most prominent research budget increases the chances of making discoveries. Another of my initial biases was that the number of patents and publications would also significantly impact the overall ranking. However, much to my surprise, none of the previously mentioned are the most important in the World Rankings. The most important feature, it's just going back to the basis, is the quality of faculty. \nI started with some EDA, wrangled the data, dropped some features due to high-cardinality or leakage, established a baseline, and fit three models: Linear Regression, Random Forest, and XGBoost -afterward, I plotted some partial dependence plots, Shapley plots, and a Shapley summary plot. ","d2a9dda0":"**XGBoost Model**"}}