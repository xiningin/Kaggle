{"cell_type":{"e956dd96":"code","f94ba482":"code","30960dc1":"code","9d61425c":"code","2509d5ed":"code","3834c4e9":"code","42f64a86":"code","ec24c849":"code","eb994d49":"code","2760b628":"code","10a1f36d":"code","953488f3":"code","cabc5bc8":"code","396fc499":"code","29bafbd4":"code","6e51bcb1":"code","64d12ac0":"code","9119b394":"code","7709a5f4":"code","95406bce":"markdown","32ef5888":"markdown","92d7355b":"markdown","e15cc2aa":"markdown","7379b79d":"markdown","b326928a":"markdown","89cbe965":"markdown","c29b403f":"markdown","cc80379c":"markdown","ee5f500f":"markdown","71c7661b":"markdown"},"source":{"e956dd96":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f94ba482":"data=pd.read_csv(\"..\/input\/knearest-neighbour-knn-classification\/data.csv\")\ndf = data.copy()\ndf.head()","30960dc1":"df.columns","9d61425c":"df.info()","2509d5ed":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\ndf.diagnosis = le.fit_transform(df['diagnosis'])","3834c4e9":"df.drop([\"Unnamed: 32\"], axis=1, inplace=True)","42f64a86":"df.head()","ec24c849":"df.isnull().sum()","eb994d49":"# Load libraries\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\nfrom sklearn.model_selection import train_test_split # Import train_test_split function\nfrom sklearn import metrics #Import scikit-learn metrics module for accuracy calculation","2760b628":"X = df.drop([\"diagnosis\"], axis=1) # Features\ny = df[\"diagnosis\"] # Target variable","10a1f36d":"# Split dataset into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test","953488f3":"# Create Decision Tree classifer object\nclf = DecisionTreeClassifier()\n\n# Train Decision Tree Classifer\nclf = clf.fit(X_train,y_train)\n\n#Predict the response for test dataset\ny_pred = clf.predict(X_test)","cabc5bc8":"# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","396fc499":"predictors = df.drop([\"diagnosis\"], axis=1)\ntarget = df[\"diagnosis\"]\nfrom sklearn.model_selection import train_test_split # Import train_test_split function\nX_train, X_test, y_train, y_test = train_test_split(predictors, target, test_size = 0.25, random_state = 0)\n\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nmodels = []\nmodels.append(('Logistic Regression', LogisticRegression()))\nmodels.append(('Naive Bayes', GaussianNB()))\nmodels.append(('Decision Tree (CART)',DecisionTreeClassifier())) \nmodels.append(('K-NN', KNeighborsClassifier()))\nmodels.append(('SVM', SVC()))\nmodels.append(('RandomForestClassifier', RandomForestClassifier()))\n\n\nfor name, model in models:\n    model = model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    from sklearn import metrics\n\n    print(\"%s -> ACC: %%%.2f\" % (name,metrics.accuracy_score(y_test, y_pred)*100))","29bafbd4":"from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(y_test, y_pred,labels=[1,0]))\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.heatmap(confusion_matrix(y_test, y_pred),annot=True,lw =2,cbar=False)\nplt.ylabel(\"True Values\")\nplt.xlabel(\"Predicted Values\")\nplt.title(\"CONFUSION MATRIX VISUALIZATION\")\nplt.show()","6e51bcb1":"from sklearn.metrics import f1_score\nf1_score(y_test,y_pred)","64d12ac0":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred))","9119b394":"import sklearn.metrics as metrics\n# calculate the fpr and tpr for all thresholds of the classification\nprobs = model.predict_proba(X_test)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_test, y_pred)\nroc_auc = metrics.auc(fpr, tpr)\n\n# method I: plt\nimport matplotlib.pyplot as plt\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","7709a5f4":"from sklearn.metrics import log_loss\nlog_loss(y_test,y_pred)","95406bce":"![](http:\/\/images.slideplayer.com\/32\/9873965\/slides\/slide_18.jpg)","32ef5888":"## 1.Confusion Matrix\nConfusion Matrix is a performance measurement for machine learning classification. It is a table with 4 different combinations of predicted and actual values.","92d7355b":"## 4.Log loss\nLog loss measures the performance of a model where the predicted outcome is a probability value between 0 and 1.\n\n![](https:\/\/miro.medium.com\/max\/393\/0*tSxNOVxHQObQPby3)\n\nThe equation simply measures how far each predicted probability is from the actual label. ","e15cc2aa":"## Evaluation Metrics for Classification Models\nChoosing the right metric is crucial while evaluating machine learning (ML) models. The next step after implementing a machine learning algorithm is to find out how effective is the model based on metric and datasets. Different performance metrics are used to evaluate different Machine Learning Algorithms. \n\n![](https:\/\/cdn-images-1.medium.com\/max\/800\/1*1WPbfzztdv50V22TpA6njw.png)","7379b79d":"## 2.Recall, Precision, F-Score\n\n* Recall :\nRecall can be defined as the ratio of the total number of correctly classified positive examples divide to the total number of positive examples. High Recall indicates the class is correctly recognized (small number of FN).\n\n* Precision:\nTo get the value of precision we divide the total number of correctly classified positive examples by the total number of predicted positive examples. High Precision indicates an example labeled as positive is indeed positive (small number of FP).\n\n![](http:\/\/nlpforhackers.io\/wp-content\/uploads\/2017\/01\/Precision-Recall.png)\n\n* F-measure\nIt is difficult to compare two models with low precision and high recall or vice versa. So to make them comparable, we use F-Score. F-score helps to measure Recall and Precision at the same time. It uses Harmonic Mean in place of Arithmetic Mean by punishing the extreme values more.\n\n![](https:\/\/miro.medium.com\/max\/2488\/1*7miM0UtyLaYh_1oMjgUXWw.png)\n","b326928a":"**Decision Tree Classification **","89cbe965":"![](https:\/\/miro.medium.com\/max\/712\/1*Z54JgbS4DUwWSknhDCvNTQ.png)","c29b403f":"Definition of the Terms:\n* Positive (P) : Observation is positive (for example: is an apple).\n* Negative (N) : Observation is not positive (for example: is not an apple).\n* True Positive (TP) : Observation is positive, and is predicted to be positive.\n* False Negative (FN) : Observation is positive, but is predicted negative.\n* True Negative (TN) : Observation is negative, and is predicted to be negative.\n* False Positive (FP) : Observation is negative, but is predicted positive.","cc80379c":"## 3.AUC - ROC Curve\n\nIn Machine Learning, performance measurement is an essential task. So when it comes to a classification problem, we can count on an AUC - ROC Curve. When we need to check or visualize the performance of the multi - class classification problem, we use AUC (Area Under The Curve) ROC (Receiver Operating Characteristics) curve. It is one of the most important evaluation metrics for checking any classification model\u2019s performance. \n\nAUC - ROC curve is a performance measurement for classification problem at various thresholds settings. ROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes. \n\nThe ROC curve is plotted with TPR against the FPR where TPR is on y-axis and FPR is on the x-axis.","ee5f500f":"****If you see incorrect or missing places you can specify as a comment.****","71c7661b":"**F1 SCORE**"}}