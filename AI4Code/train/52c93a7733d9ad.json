{"cell_type":{"fc2926ec":"code","2bcef8e5":"code","fccf6805":"code","367850fa":"code","56e95e90":"code","1aa0d68f":"code","7f1e64e2":"code","e1a604ef":"code","acfe4ce9":"code","7e65fc28":"code","0f00ed2d":"code","726ad0a6":"code","f994e125":"code","f97bc213":"code","70d53e00":"code","1d39cf8b":"code","fa1a4ac6":"code","4a404615":"code","1970eeaa":"code","2168d3a9":"code","64b8b944":"code","5b1af561":"code","807aefcd":"code","22a02650":"code","6aa341d5":"code","79bc73fb":"code","b95b2dfe":"code","6080e2b7":"code","9d983509":"code","1198fe12":"code","135990e9":"code","a80063a8":"code","e2838b2d":"code","9c9f0fea":"markdown","0bc7024a":"markdown","b91258f8":"markdown","4218ef8b":"markdown","6dc894ee":"markdown","44bbefc8":"markdown","77132320":"markdown","7ff49554":"markdown","dd197dfc":"markdown","3a35d954":"markdown","90ab09f7":"markdown","a682ea10":"markdown","3187b3b1":"markdown","cfb3b7b6":"markdown"},"source":{"fc2926ec":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score,confusion_matrix\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndata = pd.read_csv(\"\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv\")\n\n# Any results you write to the current directory are saved as output.","2bcef8e5":"data.head()","fccf6805":"data.info()","367850fa":"pd.set_option('float_format', '{:f}'.format)\ndata.describe().T\n","56e95e90":"# Since Unnamed: 32 column doesn't have any value we are dropping it\ndata.drop(['Unnamed: 32', 'id'],axis = 1 ,inplace = True)","1aa0d68f":"data_quality_report = pd.DataFrame(columns = ['feature','count', 'missing %','unique values' ,'mean', 'std', 'min', 'Q1', 'median', 'Q3', 'max', 'IQR'])\n\ni = 0\nfor f, ser in data._get_numeric_data().iteritems():\n    \n    Q1 = ser.quantile(0.25)\n    Q3 = ser.quantile(0.75)\n    \n    data_quality_report.at[i, :] = [f, ser.count(), (ser.isnull().sum()\/ser.size)*100, ser.unique().size, ser.mean(), ser.std(), ser.min(), Q1, ser.median(), Q3, ser.max(), Q3 - Q1]\n    i = i + 1\ndata_quality_report","7f1e64e2":"data.isnull().sum().sum()","e1a604ef":"data['diagnosis'].value_counts().plot(kind = 'bar')","acfe4ce9":"corr = data.corr()\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nf, ax = plt.subplots(figsize=(21, 19))\nsns.heatmap(corr, cmap=cmap, center=0,annot = True,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5});","7e65fc28":"corr.head()","0f00ed2d":"#It is taking a lot of time to execute this function as there are 30 features. If you are running this on TPU or something then uncomment and run\n\n#ns.pairplot(data);\n","726ad0a6":"class_mapping = {label:idx for idx,label in enumerate(np.unique(data['diagnosis']))}\ndata['diagnosis'] = data['diagnosis'].map(class_mapping)\ndata['diagnosis'].value_counts()","f994e125":"#data = data[selected_columns]\nlen(data.columns)\n","f97bc213":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n# find best scored k features\nselect_feature = SelectKBest(chi2, k=10).fit(data.drop('diagnosis',axis = 1 ), data['diagnosis'])\n\nprint('Score list:', select_feature.scores_)\nprint('Feature list:', data.columns)\nselect_feature","70d53e00":"select_feature.transform(data.drop('diagnosis',axis = 1))","1d39cf8b":"selected_columns = np.array(data.drop('diagnosis',axis = 1).columns)[select_feature.get_support()]\nselected_columns","fa1a4ac6":"ABT = pd.DataFrame(select_feature.transform(data.drop('diagnosis',axis = 1)),columns=selected_columns)\ny = data.diagnosis","4a404615":"ABT.head()","1970eeaa":"ABT.describe().T","2168d3a9":"compare = pd.DataFrame(index=['RandomForest', 'SVM', 'LogisticRegression', 'kNN', 'Naive Bayes'], \n                      columns=['Accuracy', 'f1 score', 'Precision', 'Recall'])\ncompare","64b8b944":"X_train, X_test, y_train, y_test = train_test_split(ABT, y, test_size=0.2, random_state=0)\nX_train, X_v, y_train, y_v = train_test_split(X_train, y_train, test_size=0.2, random_state=0)","5b1af561":"rf = RandomForestClassifier(bootstrap= False, criterion='entropy', max_features = 'sqrt', min_samples_leaf =1, n_estimators= 300)      \nrf = rf.fit(X_train,y_train)\n\ny_pred = rf.predict(X_v)\nprint(f'Accuracy on validation set is {accuracy_score(y_v,y_pred)}')\nprint(f'f1 score on validation set is {f1_score(y_true=y_v, y_pred=y_pred)}')\nprint(f'Precision on validation set is {precision_score(y_true=y_v, y_pred=y_pred)}')\nprint(f'Recall on validation set is {recall_score(y_true=y_v, y_pred=y_pred)}')\nconfmat = confusion_matrix(y_v,y_pred)\nsns.heatmap(confmat,annot=True,fmt=\"d\")\n","807aefcd":"y_pred = rf.predict(X_test)\n\naccuracy = accuracy_score(y_test,y_pred)\nf1 = f1_score(y_true=y_test, y_pred=y_pred)\nprecision = precision_score(y_true=y_test, y_pred=y_pred)\nrecall = recall_score(y_true=y_test, y_pred=y_pred)\n\n\ncompare.at['RandomForest', :] = (accuracy, f1, precision, recall)\n\nprint(f'Accuracy on test set is {accuracy}')\nprint(f'f1 score on test set is {f1}')\nprint(f'Precision on test set is {precision}')\nprint(f'Recall on test set is {recall}')\n\n\nconfmat = confusion_matrix(y_test,y_pred)\nsns.heatmap(confmat,annot=True,fmt=\"d\")\n","22a02650":"from sklearn import preprocessing\n\nmin_max_scaler = preprocessing.MinMaxScaler()\nNormalizedABT = min_max_scaler.fit_transform(ABT)\nNormalizedABT=pd.DataFrame(NormalizedABT, columns=selected_columns)\nX_train, X_test, y_train, y_test = train_test_split(NormalizedABT, y, test_size=0.2, random_state=0)\nX_train, X_v, y_train, y_v = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\nNormalizedABT.describe().T\n","6aa341d5":"from sklearn import svm\n\nclf = svm.SVC().fit(X_train, y_train)\ny_pred = clf.predict(X_v)\n\nprint(f'Accuracy on validation set is {accuracy_score(y_v,y_pred)}')\nprint(f'f1 score on validation set is {f1_score(y_true=y_v, y_pred=y_pred)}')\nprint(f'Precision on validation set is {precision_score(y_true=y_v, y_pred=y_pred)}')\nprint(f'Recall on validation set is {recall_score(y_true=y_v, y_pred=y_pred)}')\n\nconfmat = confusion_matrix(y_v,y_pred)\nsns.heatmap(confmat,annot=True,fmt=\"d\")\n\n","79bc73fb":"y_pred = clf.predict(X_test)\n\naccuracy = accuracy_score(y_test,y_pred)\nf1 = f1_score(y_true=y_test, y_pred=y_pred)\nprecision = precision_score(y_true=y_test, y_pred=y_pred)\nrecall = recall_score(y_true=y_test, y_pred=y_pred)\n\ncompare.at['SVM', :] = (accuracy, f1, precision, recall)\n\nprint(f'Accuracy on test set is {accuracy}')\nprint(f'f1 score on test set is {f1}')\nprint(f'Precision on test set is {precision}')\nprint(f'Recall on test set is {recall}')\n\nconfmat = confusion_matrix(y_test,y_pred)\nsns.heatmap(confmat,annot=True,fmt=\"d\")\n","b95b2dfe":"from sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(random_state=0).fit(X_train, y_train)\ny_pred = clf.predict(X_v)\n\nprint(f'Accuracy on validation set is {accuracy_score(y_v,y_pred)}')\nprint(f'f1 score on validation set is {f1_score(y_true=y_v, y_pred=y_pred)}')\nprint(f'Precision on validation set is {precision_score(y_true=y_v, y_pred=y_pred)}')\nprint(f'Recall on validation set is {recall_score(y_true=y_v, y_pred=y_pred)}')\n\nconfmat = confusion_matrix(y_v,y_pred)\nsns.heatmap(confmat,annot=True,fmt=\"d\")\n","6080e2b7":"y_pred = clf.predict(X_test)\n\naccuracy = accuracy_score(y_test,y_pred)\nf1 = f1_score(y_true=y_test, y_pred=y_pred)\nprecision = precision_score(y_true=y_test, y_pred=y_pred)\nrecall = recall_score(y_true=y_test, y_pred=y_pred)\n\ncompare.at['LogisticRegression', :] = (accuracy, f1, precision, recall)\n\nprint(f'Accuracy on test set is {accuracy}')\nprint(f'f1 score on test set is {f1}')\nprint(f'Precision on test set is {precision}')\nprint(f'Recall on test set is {recall}')\n\n\nconfmat = confusion_matrix(y_test,y_pred)\nsns.heatmap(confmat,annot=True,fmt=\"d\")\n","9d983509":"from sklearn.neighbors import KNeighborsClassifier\nclf = KNeighborsClassifier(n_neighbors=19).fit(X_train, y_train)\n\ny_pred = clf.predict(X_v)\n\nprint(f'Accuracy on validation set is {accuracy_score(y_v,y_pred)}')\nprint(f'f1 score on validation set is {f1_score(y_true=y_v, y_pred=y_pred)}')\nprint(f'Precision on validation set is {precision_score(y_true=y_v, y_pred=y_pred)}')\nprint(f'Recall on validation set is {recall_score(y_true=y_v, y_pred=y_pred)}')\n\nconfmat = confusion_matrix(y_v,y_pred)\nsns.heatmap(confmat,annot=True,fmt=\"d\")\n","1198fe12":"y_pred = clf.predict(X_test)\n\naccuracy = accuracy_score(y_test,y_pred)\nf1 = f1_score(y_true=y_test, y_pred=y_pred)\nprecision = precision_score(y_true=y_test, y_pred=y_pred)\nrecall = recall_score(y_true=y_test, y_pred=y_pred)\n\ncompare.at['kNN', :] = (accuracy, f1, precision, recall)\n\nprint(f'Accuracy on test set is {accuracy}')\nprint(f'f1 score on test set is {f1}')\nprint(f'Precision on test set is {precision}')\nprint(f'Recall on test set is {recall}')\n\nconfmat = confusion_matrix(y_test,y_pred)\nsns.heatmap(confmat,annot=True,fmt=\"d\")\n","135990e9":"from sklearn.naive_bayes import GaussianNB\n\nclf = GaussianNB().fit(X_train, y_train)\n\ny_pred = clf.predict(X_v)\n\nprint(f'Accuracy on validation set is {accuracy_score(y_v,y_pred)}')\nprint(f'f1 score on validation set is {f1_score(y_true=y_v, y_pred=y_pred)}')\nprint(f'Precision on validation set is {precision_score(y_true=y_v, y_pred=y_pred)}')\nprint(f'Recall on validation set is {recall_score(y_true=y_v, y_pred=y_pred)}')\n\nconfmat = confusion_matrix(y_v,y_pred)\nsns.heatmap(confmat,annot=True,fmt=\"d\")\n","a80063a8":"y_pred = clf.predict(X_test)\n\naccuracy = accuracy_score(y_test,y_pred)\nf1 = f1_score(y_true=y_test, y_pred=y_pred)\nprecision = precision_score(y_true=y_test, y_pred=y_pred)\nrecall = recall_score(y_true=y_test, y_pred=y_pred)\n\ncompare.at['Naive Bayes', :] = (accuracy, f1, precision, recall)\n\nprint(f'Accuracy on test set is {accuracy}')\nprint(f'f1 score on test set is {f1}')\nprint(f'Precision on test set is {precision}')\nprint(f'Recall on test set is {recall}')\n\nconfmat = confusion_matrix(y_test,y_pred)\nsns.heatmap(confmat,annot=True,fmt=\"d\")\n","e2838b2d":"compare","9c9f0fea":"**There are no missing values in the data set.**","0bc7024a":"### Logistic Regression Model","b91258f8":"## Similarity Based Learning ##\n### kNN classifier ###\n","4218ef8b":"Analysis Base Table","6dc894ee":"## Information Based Learning ##\n### **Random Forest Classifier** ###\n","44bbefc8":"# Data Preprocessing","77132320":"## Probability Based Learning ##\n### Naive Bayes classifier ###\n","7ff49554":"### **Normalization** ### \nPerforming normalization for further models.<br \/> \nRandomForest didn't require it.<br \/> \n\nAfter normalization we will get NormalizedABT, so will split this into 3 sets for training, cross-validation and testing.","dd197dfc":"### Feature Selection###\nUsing chi2 test for selecting k best features from the data set to prevent the overfitting caused by the curse of dimensionality.","3a35d954":"### SVM - Support Vector Machine classifier\n","90ab09f7":"### Missing Values###","a682ea10":"# Exploratory Data Analysis ","3187b3b1":"## Error Based Learning ##\n> ### **1. SVM **###\n> ### **2. Logistic Regression**###\n","cfb3b7b6":"# Modelling\n1. **Information based learning** - Random Forest classifer\n2. **Error based learning** - SVM, Logistic Regression\n3. **Similarity based learning **- kNN classifier\n3. **Probability based learning** - Naive Bayes classifier"}}