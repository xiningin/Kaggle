{"cell_type":{"71cf910c":"code","89ea1eb3":"code","236b70c4":"code","2f8f70b7":"code","3061109c":"code","246c95ce":"code","404eb126":"code","817b5ab4":"code","daae622d":"code","1d847ca4":"code","73cb8e6e":"code","35ac1490":"code","8e884544":"code","3a1fb488":"code","7e474ec6":"code","51da2df2":"code","e8f8ceb0":"code","01c5ce9b":"code","b8c976be":"code","9957f389":"code","0f3327f1":"code","e720ccc4":"code","2e1529d5":"markdown","cfd4b491":"markdown","ae6eb158":"markdown","3d9aca09":"markdown","4d88696a":"markdown","a13e1112":"markdown","583cf614":"markdown"},"source":{"71cf910c":"import os\nimport cv2\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt","89ea1eb3":"data_path = '..\/input\/us-license-plates'","236b70c4":"print(f'Data len: {sum([len(os.listdir(f\"{data_path}\/{dir_name}\/\")) for dir_name in os.listdir(data_path)])}')","2f8f70b7":"## visualization functions\n\ndef barh_plot(data_path, ylabel):\n    dir_names = []\n    value_counts = []\n    for dir_name in os.listdir(data_path):\n        dir_names.append(dir_name)\n        value_counts.append(len(os.listdir(f\"{data_path}\/{dir_name}\/\")))\n    plt.figure(figsize=(18,13))\n    plt.style.use('seaborn')\n    plt.barh(dir_names, \n            value_counts,\n            color='green', \n            linewidth=0,\n            alpha=1.0)\n    plt.tick_params(labelsize=15)\n    plt.xlabel('value counts', fontsize=15)\n    plt.ylabel(ylabel, fontsize=15)\n    plt.show()\n    \n\ndef image_plot(data_path, n=5, random_state=42):\n    random.seed(random_state)\n    labels = random.sample(os.listdir(data_path), n*n)\n    images = {}\n    for label in labels:\n        file_name = random.sample(os.listdir(f'{data_path}\/{label}'), 1)[0]\n        image = cv2.imread(f'{data_path}\/{label}\/{file_name}')\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        images[label] = image\n\n    f, ax = plt.subplots(n, n, figsize=(15,15))\n    for i, (label, image) in enumerate(images.items()):\n        ax[i\/\/n, i%n].imshow(image)\n        ax[i\/\/n, i%n].axis('off')\n        ax[i\/\/n, i%n].set_title(f'label: {label}')\n    plt.show()","3061109c":"barh_plot(data_path, 'state')","246c95ce":"image_plot(data_path)","404eb126":"from sklearn.preprocessing import LabelEncoder\n\ndef create_metadata(data_path):\n    le = LabelEncoder()\n    image_paths = []\n    labels = []\n    for dir_name in os.listdir(data_path):\n        file_names = os.listdir(f'{data_path}\/{dir_name}\/')\n        for file_name in file_names:  \n            image_paths.append(f'{data_path}\/{dir_name}\/{file_name}')\n            labels.append(dir_name)\n    meta_data = pd.DataFrame(\n        {\n            'image_paths': image_paths, \n            'labels': labels\n        }, dtype=object\n    )\n    meta_data['labels'] = le.fit_transform(meta_data['labels'].values)\n    return meta_data\n\ndata = create_metadata(data_path)\ndata.sample(5)","817b5ab4":"!pip install pytorch-lightning==1.4.5 -q\n!pip install timm==0.4.12 -q","daae622d":"# using SAM\n!git clone https:\/\/github.com\/davda54\/sam.git","1d847ca4":"import sys\nsys.path.append('.\/sam\/')","73cb8e6e":"import gc\nimport multiprocessing\nimport numpy as np\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nimport pytorch_lightning as pl\nfrom pytorch_lightning.utilities.seed import seed_everything\nfrom sam import SAM\nimport timm\nimport albumentations as A","35ac1490":"class Cfg:\n    debug = False\n    seed = 42\n    lr = 1e-4 \n    epochs = 10\n    train_batch_size = 16\n    val_batch_size = 256\n    num_classes = 51\n    height = 128\n    width = 256\n    scheduler_t_max = 6\n    scheduler_eta_min = 1e-6\n    model_name = 'dm_nfnet_f1'\n    n_gpus = torch.cuda.device_count()\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","8e884544":"seed_everything(Cfg.seed, workers=True)","3a1fb488":"if Cfg.debug == True:\n    data = data.sample(1000, random_state=Cfg.seed)\nprint(f'Data: {len(data)}')","7e474ec6":"train, val = train_test_split(\n    data, \n    test_size=0.15, \n    stratify=data['labels'], \n    random_state=Cfg.seed\n)\n\ntrain.reset_index(drop=True, inplace=True)\nval.reset_index(drop=True, inplace=True)\n\nprint(f'Train: {len(train)}')\nprint(f'Validation: {len(val)}')","51da2df2":"class LicensePlatesDataset(Dataset):\n    def __init__(self, data, transform=None):\n        super().__init__()\n        self.data = data\n        self.image_paths = data['image_paths']\n        self.labels = data['labels']\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.data)\n        \n    def __getitem__(self, index):\n        image_path = self.image_paths[index]\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        if self.transform is not None:\n            aug_image = self.transform(image=image)\n            image = aug_image['image']\n        image = image.astype(np.float32)\n        image = image.transpose(2, 0, 1)\n        return {\n            'inputs': torch.tensor(image).float(),\n            'labels': torch.tensor(self.labels[index]).long()\n        }","e8f8ceb0":"transforms_train = A.Compose([\n    A.Resize(Cfg.height, Cfg.width),\n    A.HorizontalFlip(p=0.5),\n    A.ShiftScaleRotate(p=0.5),\n    A.OneOf([\n        A.OpticalDistortion(distort_limit=1.0),\n        A.GridDistortion(num_steps=5, distort_limit=1.0),\n        A.GaussianBlur(),\n        A.MedianBlur(),\n    ], p=0.2),\n    \n    A.Normalize()\n])\n\ntransforms_val = A.Compose([\n    A.Resize(Cfg.height, Cfg.width),\n    A.Normalize()\n])","01c5ce9b":"dataset = LicensePlatesDataset(train, transform=transforms_train)\n\nf, ax = plt.subplots(5, 5, figsize=(15,15))\nfor i in range(5*5):\n    image = dataset[i]['inputs']\n    image = image.transpose(0, 1).transpose(1,2).squeeze()\n    ax[i\/\/5, i%5].imshow(image)\n    ax[i\/\/5, i%5].axis('off')\nplt.suptitle('After applying data augmentation', fontsize=15)\nplt.show()","b8c976be":"class CustomModel(pl.LightningModule):\n    def __init__(self, config, pretrained=True):\n        super().__init__()\n        self.config = config\n        # for using SAM\n        self.automatic_optimization = False\n        \n        self.model = timm.create_model(self.config.model_name, \n                                       pretrained=pretrained)\n        n_features = self.model.head.fc.in_features\n        self.model.fc = nn.Linear(n_features, self.config.num_classes)\n        self.criterion = nn.CrossEntropyLoss()\n    \n    def forward(self, inputs, labels=None):\n        preds = self.model(inputs)\n        if labels is not None:\n            loss = self.criterion(preds, labels)\n            return loss\n        else:\n            return preds\n        \n    def training_step(self, batch, batch_idx):\n        optimizer = self.optimizers()\n        # SAM first step\n        first_loss = self.forward(inputs=batch['inputs'], \n                                  labels=batch['labels'])\n        self.manual_backward(first_loss)\n        optimizer.first_step(zero_grad=True)\n        # SAM second step\n        second_loss = self.forward(inputs=batch['inputs'], \n                                   labels=batch['labels'])\n        self.manual_backward(second_loss)\n        optimizer.second_step(zero_grad=True)\n        \n        self.log('train_loss', first_loss)\n        return first_loss\n    \n    def validation_step(self, batch, batch_idx):\n        val_loss = self.forward(inputs=batch['inputs'], \n                                labels=batch['labels'])\n        self.log('val_loss', val_loss)\n        return val_loss\n    \n    def test_step(self, batch, batch_idx):\n        return self.validation_step(batch, batch_idx)\n    \n    def configure_optimizers(self):\n        base_opt = torch.optim.AdamW\n        optimizer = SAM(self.parameters(), base_opt, lr=self.config.lr)\n        scheduler = CosineAnnealingLR(\n            optimizer, \n            T_max=self.config.scheduler_t_max, \n            eta_min=self.config.scheduler_eta_min\n        )\n        return [optimizer], [scheduler]","9957f389":"def inference(model, data_loader, config):\n    all_outputs = []\n    all_labels = []\n    \n    for batch_idx, batch in enumerate(data_loader):\n        inputs = batch['inputs']\n        labels = batch['labels']\n        \n        inputs = inputs.to(config.device)\n        labels = labels.to(config.device)\n        \n        model.to(config.device)\n        model.eval()\n        with torch.no_grad():\n            outputs = model(inputs=inputs, labels=None)\n            \n        outputs = outputs.cpu().detach().numpy().tolist()\n        labels = labels.cpu().detach().numpy().tolist() \n        all_outputs.extend(outputs)\n        all_labels.extend(labels)\n    \n    return all_outputs, all_labels","0f3327f1":"def run_train(train, val, config):\n    train_ds = LicensePlatesDataset(train, transform=transforms_train)\n    val_ds = LicensePlatesDataset(val, transform=transforms_val)\n    \n    train_loader = DataLoader(train_ds, \n                              batch_size=config.train_batch_size, \n                              num_workers=multiprocessing.cpu_count(), \n                              pin_memory=True, \n                              drop_last=True,\n                              shuffle=True)\n    \n    val_loader = DataLoader(val_ds, \n                            batch_size=config.val_batch_size, \n                            num_workers=multiprocessing.cpu_count(), \n                            pin_memory=True, \n                            drop_last=False,\n                            shuffle=False)\n    \n    checkpoint = pl.callbacks.ModelCheckpoint(monitor='val_loss', \n                                              mode='min', \n                                              save_top_k=1,  \n                                              save_weights_only=True, \n                                              dirpath=f'model\/')\n    \n    tb_logger = pl.loggers.TensorBoardLogger(f'model_logs\/')\n\n    trainer = pl.Trainer(max_epochs=config.epochs,\n                         gpus=config.n_gpus,\n                         logger=tb_logger,\n                         callbacks=[checkpoint])\n    \n    model = CustomModel(config)\n    model.to(config.device)\n    trainer.fit(model, train_loader, val_loader)\n    \n    model.load_state_dict(torch.load(checkpoint.best_model_path)['state_dict'])\n    outputs, labels = inference(model, val_loader, config)\n\n    del train_ds, val_ds, train_loader, val_loader, model\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    return outputs, labels","e720ccc4":"outputs, labels = run_train(train, val, Cfg)\noutputs = np.array(outputs)\nlabels = np.array(labels)\n\noutputs = outputs.argmax(1)\n\nscore = f1_score(labels, outputs, average='macro')\nprint(f'VALIDATION DATA MACRO-F1 SCORE: {score:.5f}')","2e1529d5":"### Data Augmentation","cfd4b491":"## Check The Dataset","ae6eb158":"## Create MetaData","3d9aca09":"# US License Plates Recognition\n\n## Introduction\n\nThis notebook challenges the recognition of license plate images from 51 states in the United States.\n\nWe'll use an interesting dataset published by [Tolga](https:\/\/www.kaggle.com\/tolgadincer).","4d88696a":"### Split Dataset","a13e1112":"## Model Training","583cf614":"### Metrics: Macro-F1\n\nMacro-F1 calculates the F1 score for each class and takes the average of the scores."}}