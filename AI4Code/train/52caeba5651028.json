{"cell_type":{"007a0475":"code","34706972":"code","f4b6ed5a":"code","c1436164":"code","58a74726":"code","02d8ff84":"code","0c526c3e":"code","35eff617":"code","afce5598":"code","5a4423a9":"code","b1c428ff":"code","29439a9b":"code","ad094acd":"code","f686b98b":"code","54ad1e08":"code","d07759fc":"code","6af790cb":"code","c9951050":"code","9d7988e9":"code","f3e4faec":"code","cce765eb":"code","2fb8b614":"code","dccd1190":"code","6c21bd69":"code","bb28b45d":"code","ae519d96":"code","6f6699b5":"code","377dfa1d":"code","eef6414d":"code","7248ecc9":"code","c02ecf1a":"code","503b21f0":"code","6f93cd89":"code","d0207fe4":"code","55385b1a":"code","97e08dba":"code","9733509c":"code","2c3c13c0":"code","a43aec9a":"code","9c4c2af9":"code","6bdd3fe0":"code","56e5072b":"code","f25e4004":"code","d8499e8e":"code","d27321b9":"code","88256004":"code","946aa281":"code","01b124b1":"code","9fd7d99e":"code","2a53fc07":"code","15456f6c":"code","c9e7d3a4":"code","1a7aeb0e":"markdown","3eb8fe67":"markdown","e3f2a231":"markdown","b39c7cd1":"markdown","23fe8aac":"markdown","fb644251":"markdown","f78f58f0":"markdown","a0c20d63":"markdown","686a518b":"markdown","59f32b67":"markdown","aaf22d60":"markdown","0a943170":"markdown","5797eca3":"markdown","58df108a":"markdown","ab99904c":"markdown","612554b8":"markdown","ed8dfb2b":"markdown","6e29ceec":"markdown","1d1bf361":"markdown","435552c4":"markdown","87b8e648":"markdown","1867c280":"markdown","952a37f9":"markdown","9ee36289":"markdown","01007dbe":"markdown","bbe2b60a":"markdown","430df032":"markdown","c035a8b6":"markdown","2a75b144":"markdown","aeb7ad41":"markdown","b1244db2":"markdown","34850d08":"markdown","90a5e438":"markdown","f2febe33":"markdown","6c87fca5":"markdown"},"source":{"007a0475":"import numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set(context='notebook', style='white', palette='colorblind')","34706972":"from sklearn.preprocessing import StandardScaler, LabelEncoder\n\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV, learning_curve, cross_val_score","f4b6ed5a":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ndf = pd.concat([train, test], axis=0, ignore_index=True)\n\nprint(f'Train:{train.shape}\\nTest:{test.shape}\\nDf:{df.shape}')","c1436164":"df.sample(3)","58a74726":"df.info()","02d8ff84":"#columns with missing values\ndf.isna().sum()[df.isna().sum()>0]","0c526c3e":"df.describe()","35eff617":"df.describe(exclude='number')","afce5598":"df.sample(3)","5a4423a9":"# replacing values in male for 0 and female for 1 in 'Sex' column\ndf.Sex=df.Sex.map({'male':0, 'female':1}).astype('int')","b1c428ff":"# correlation between features\nplt.figure(figsize=(10,4))\nsns.heatmap(df.drop('PassengerId', axis=1).corr(), annot=True, center=0)","29439a9b":"sns.FacetGrid(train, col='Survived').map(sns.distplot, \"Age\", hist=False, kde=True, rug=False, kde_kws={'shade':True})","ad094acd":"sns.catplot(x=\"Pclass\", y=\"Survived\", data=train, kind=\"bar\", height=3, aspect=2)","f686b98b":"sns.catplot(x=\"Sex\", y=\"Survived\", hue=\"Pclass\", data=train, kind=\"bar\", height=3, aspect=2)","54ad1e08":"f,ax = plt.subplots(1, 2, figsize=(15,4))\n\nsns.violinplot(\"Pclass\",\"Age\", hue=\"Sex\", data=train, split=True, ax=ax[0])\nax[0].set_title('Pclass vs Age')\nax[0].set_yticks(range(0,110,10))\n\nsns.violinplot(\"Sex\", \"Age\", hue=\"Survived\", data=train, split=True, ax=ax[1])\nax[1].set_title('Sex vs Age')\nax[1].set_yticks(range(0,110,10))\nplt.show()","d07759fc":"sns.catplot(x=\"Parch\", y=\"Survived\", data=train, kind=\"bar\", height=4, aspect=2)","6af790cb":"plt.figure(figsize=(3,3))\nsns.barplot(x=\"Sex\", y=\"Survived\", data=train)\nsns.despine()","c9951050":"sns.catplot(x=\"Embarked\", y=\"Survived\", data=train, kind=\"bar\", height=3, aspect=2)","9d7988e9":"# creating a new feature using a linear function and dropping the old features to avoid redundancy and overfitting\ndf['Family_Size']=df.SibSp + df.Parch\ndf.groupby('Family_Size')['Survived'].mean()","f3e4faec":"df['Title'] = df['Name']\n\nfor name_string in df['Name']:\n    df['Title'] = df['Name'].str.extract('([A-Za-z]+)\\.', expand=True)\n\nmapping = {'Mlle': 'Miss', 'Major': 'Mr', 'Col': 'Mr', 'Sir': 'Mr', 'Don': 'Mr', 'Mme': 'Miss',\n          'Jonkheer': 'Mr', 'Lady': 'Mrs', 'Capt': 'Mr', 'Countess': 'Mrs', 'Ms': 'Miss', 'Dona': 'Mrs'}\n\ndf.replace({'Title': mapping}, inplace=True)\n    \ngr = sns.countplot(x=\"Title\", data=df)\ngr.set_xticklabels(gr.get_xticklabels(), rotation=0)\nplt.show(gr)\n\nsns.catplot(x=\"Title\",y=\"Survived\",data=df.iloc[:len(train)],kind=\"bar\")\nplt.show(sns)    ","cce765eb":"df.Title.value_counts()","2fb8b614":"plt.figure(figsize=(5,4))\nsns.countplot(x='Title', data=df)\n\nplt.ylabel('')\nplt.xlabel('')\nplt.title('Count Plot - Titles')","dccd1190":"sns.catplot(x=\"Title\", y=\"Survived\", data=df, kind=\"bar\")","6c21bd69":"df['Family_Size'] = df['Parch'] + df['SibSp']","bb28b45d":"df['Last_Name'] = df['Name'].apply(lambda x: str.split(x, \",\")[0])\n\ndf['Fare'].fillna(df['Fare'].mean(), inplace=True)\n\nDEFAULT_SURVIVAL_VALUE = 0.5\n\ndf['Family_Survival'] = DEFAULT_SURVIVAL_VALUE\n\nfor grp, grp_df in df[['Survived','Name', 'Last_Name', 'Fare', 'Ticket', 'PassengerId',\n                           'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['Last_Name', 'Fare']):\n    \n    if (len(grp_df) != 1):\n        # A Family group is found.\n        for ind, row in grp_df.iterrows():\n            smax = grp_df.drop(ind)['Survived'].max()\n            smin = grp_df.drop(ind)['Survived'].min()\n            passID = row['PassengerId']\n            if (smax == 1.0):\n                df.loc[df['PassengerId'] == passID, 'Family_Survival'] = 1\n            elif (smin==0.0):\n                df.loc[df['PassengerId'] == passID, 'Family_Survival'] = 0\n                \nfor _, grp_df in df.groupby('Ticket'):\n    if (len(grp_df) != 1):\n        for ind, row in grp_df.iterrows():\n            if (row['Family_Survival'] == 0) | (row['Family_Survival']== 0.5):\n                smax = grp_df.drop(ind)['Survived'].max()\n                smin = grp_df.drop(ind)['Survived'].min()\n                passID = row['PassengerId']\n                if (smax == 1.0):\n                    df.loc[df['PassengerId'] == passID, 'Family_Survival'] = 1\n                elif (smin==0.0):\n                    df.loc[df['PassengerId'] == passID, 'Family_Survival'] = 0","ae519d96":"sns.catplot(x=\"Family_Size\", y=\"Survived\", data = df.iloc[:len(train)], kind=\"bar\")\nplt.title('Survival Prediction per Family Size')\nplt.ylabel('')","6f6699b5":"facet = sns.FacetGrid(train, hue='Sex', aspect=3)\nfacet.map(sns.kdeplot,'Age', shade= True)\nfacet.set(xlim=(0, train['Age'].max()))\nfacet.add_legend()","377dfa1d":"df['Family_Survival'].value_counts()","eef6414d":"df['Fare'].fillna(df['Fare'].median(), inplace = True)\n\ndf['FareBin'] = pd.qcut(df['Fare'], 5)\n\nlabel = LabelEncoder()\ndf['FareBin_Code'] = label.fit_transform(df['FareBin'])\n\ndf.drop(['Fare'], 1, inplace=True)","7248ecc9":"df['FareBin_Code'].value_counts()","c02ecf1a":"# filling missing values in 'age' column\ntitles = ['Dr', 'Master', 'Miss', 'Mr', 'Mrs', 'Rev']\n\nfor title in titles:\n    age_to_impute = df.groupby('Title')['Age'].median()[titles.index(title)]\n    df.loc[(df['Age'].isnull()) & (df['Title'] == title), 'Age'] = age_to_impute","503b21f0":"df['AgeBin'] = pd.qcut(df['Age'], 4)\n\nlabel = LabelEncoder()\ndf['AgeBin_Code'] = label.fit_transform(df['AgeBin'])","6f93cd89":"df['AgeBin_Code'].value_counts()","d0207fe4":"sns.FacetGrid(data=df, hue = \"Title\", height=4, aspect=2).map(sns.kdeplot, \"Age\", shade=True)\nplt.legend()","55385b1a":"df.drop(['Name', 'PassengerId', 'SibSp', 'Parch', 'Ticket', 'Cabin',\n               'Embarked', 'Last_Name', 'FareBin', 'AgeBin', 'Survived', 'Title', 'Age'], axis = 1, inplace = True)","97e08dba":"df.sample(2)","9733509c":"X_train = df[:len(train)]\nX_test = df[len(train):]\n\ny_train = train['Survived']","2c3c13c0":"scaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","a43aec9a":"kfold = StratifiedKFold(n_splits=8)","9c4c2af9":"RFC = RandomForestClassifier()\n\nrf_param_grid = {\"max_depth\": [None],\n              \"max_features\": [3,\"sqrt\", \"log2\"],\n              \"min_samples_split\": [n for n in range(1, 9)],\n              \"min_samples_leaf\": [5, 7],\n              \"bootstrap\": [False, True],\n              \"n_estimators\" :[200, 500],\n              \"criterion\": [\"gini\", \"entropy\"]}\n\nrf_param_grid_best = {\"max_depth\": [None],\n              \"max_features\": [3],\n              \"min_samples_split\": [4],\n              \"min_samples_leaf\": [5],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[200],\n              \"criterion\": [\"gini\"]}\n\ngs_rf = GridSearchCV(RFC, param_grid = rf_param_grid_best, cv=kfold, scoring=\"roc_auc\", n_jobs= 4, verbose = 1)\n\ngs_rf.fit(X_train, y_train)\n\nrf_best = gs_rf.best_estimator_\nRFC.fit(X_train, y_train)","6bdd3fe0":"print(f'RandomForest GridSearch best params: {gs_rf.best_params_}\\n')\nprint(f'RandomForest GridSearch best score: {gs_rf.best_score_}')\nprint(f'RandomForest score:                 {RFC.score(X_train,y_train)}')","56e5072b":"KNN = KNeighborsClassifier()\n\nknn_param_grid = {'algorithm': ['auto'],\n                 'weights': ['uniform', 'distance'], \n                 'leaf_size': [20, 25, 30], \n                 'n_neighbors': [12, 14, 16]}\n\nknn_best_param_grid = {'algorithm': ['auto'],\n                 'weights': ['uniform'], \n                 'leaf_size': [25], \n                 'n_neighbors': [14]}\n\ngs_knn = GridSearchCV(KNN, param_grid = knn_best_param_grid, cv=kfold, scoring = \"roc_auc\", n_jobs= 4, verbose = 1)\n\ngs_knn.fit(X_train, y_train)\nKNN.fit(X_train, y_train)\n\nknn_best = gs_knn.best_estimator_","f25e4004":"print(f'KNN GridSearch best params: {gs_knn.best_params_}')\nprint()\nprint(f'KNN GridSearch best score: {gs_knn.best_score_}')\nprint(f'KNN score:                 {KNN.score(X_train,y_train)}')","d8499e8e":"knn1 = KNeighborsClassifier(algorithm='auto', leaf_size=26, metric='minkowski', \n                           metric_params=None, n_jobs=1, n_neighbors=6, p=2, \n                           weights='uniform')\n\nknn1.fit(X_train, y_train)","d27321b9":"print(f'KNN score - 2nd model:           {knn1.score(X_train, y_train)}')","88256004":"GB = GradientBoostingClassifier()\n\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [1000],\n              'learning_rate': [0.02, 0.05],\n              'min_samples_split': [15, 20, 25],\n              'max_depth': [4, 6],\n              'min_samples_leaf': [50, 60],\n              'max_features': [\"sqrt\"] \n              }\n\ngb_param_grid_best = {'loss' : [\"deviance\"],\n              'n_estimators' : [1000],\n              'learning_rate': [0.02],\n              'min_samples_split': [25],\n              'max_depth': [4],\n              'min_samples_leaf': [60],\n              'max_features': [\"sqrt\"] \n              }\n\ngs_gb = GridSearchCV(GB, param_grid = gb_param_grid_best, cv=kfold, scoring=\"roc_auc\", n_jobs= 4, verbose = 1)\n\ngs_gb.fit(X_train,y_train)\nGB.fit(X_train, y_train)\n\ngb_best = gs_gb.best_estimator_","946aa281":"print(f'GradienBoost GridSearch best params: {gs_gb.best_params_}')\nprint()\nprint(f'GradienBoost GridSearch best score: {gs_gb.best_score_}')\nprint(f'GradienBoost score:                 {GB.score(X_train, y_train)}')","01b124b1":"XGB = XGBClassifier()\n\nxgb_param_grid = {'learning_rate':[0.05, 0.1], \n                  'reg_lambda':[0.3, 0.5],\n                  'gamma': [0.8, 1],\n                  'subsample': [0.8, 1],\n                  'max_depth': [2, 3],\n                  'n_estimators': [200, 300]\n              }\n\nxgb_param_grid_best = {'learning_rate':[0.1], \n                  'reg_lambda':[0.3],\n                  'gamma': [1],\n                  'subsample': [0.8],\n                  'max_depth': [2],\n                  'n_estimators': [300]\n              }\n\ngs_xgb = GridSearchCV(XGB, param_grid = xgb_param_grid_best, cv=kfold, scoring=\"roc_auc\", n_jobs= 4, verbose = 1)\n\ngs_xgb.fit(X_train,y_train)\nXGB.fit(X_train, y_train)\n\nxgb_best = gs_xgb.best_estimator_","9fd7d99e":"print(f'XGB GridSearch best params: {gs_xgb.best_params_}')\nprint()\nprint(f'XGB GridSearch best score: {gs_xgb.best_score_}')\nprint(f'XGB score:                 {XGB.score(X_train, y_train)}')","2a53fc07":"def CVScore(classifiers):\n    \n    cv_score = []\n    names = []\n    \n    for n_classifier in range(len(classifiers)):\n        name = classifiers[n_classifier][0]\n        model = classifiers[n_classifier][1]\n        cv_score.append(cross_val_score(model, X_train, y_train, scoring = \"roc_auc\", cv = kfold, n_jobs=4))\n        names.append(name)\n        \n    cv_means = []\n    \n    for cv_result in cv_score:\n        cv_means.append(cv_result.mean())\n        \n    cv_res = pd.DataFrame({\"Model\":names,\"CVMeans\":cv_means})\n    cv_res=cv_res.sort_values(\"CVMeans\", axis = 0, ascending = False, inplace = False).reset_index(drop=True)\n    print('\\n-------------------------CrossVal Training scores-------------------------\\n\\n', cv_res)\n\nclf_list = [(\"BestRandomForest\", rf_best), (\"BestGradientBoost\", gb_best), (\"BestKNN\", knn_best), (\"BestXGB\", xgb_best), (\"RandomForest\", RFC), (\"GradientBoost\", GB), (\"KNN Model 1\", KNN), (\"XGB\", XGB), (\"Best Model: KNN\", knn1)]\n\nCVScore(clf_list)","15456f6c":"results=pd.DataFrame({'PassengerId':test['PassengerId'], 'Survived':knn1.predict(X_test)})\nresults.to_csv(\"Titanic_prediction.csv\", index=False)\n\nprint('Done!')","c9e7d3a4":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    plt.figure()\n    plt.title(title)\n    \n    if ylim is not None:\n        plt.ylim(*ylim)\n        \n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    \n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    \n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    \n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1)\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1)\n\n    plt.plot(train_sizes, train_scores_mean, 'o-',\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-',\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    plt.show()\n\nplot_learning_curve(rf_best,\"RF learning curves\", X_train, y_train, cv=kfold)\nplot_learning_curve(xgb_best,\"XGB learning curves\", X_train, y_train, cv=kfold)\nplot_learning_curve(gb_best,\"Best GradientBoosting learning curves\", X_train, y_train, cv=kfold)\nplot_learning_curve(GB,\"GradientBoosting learning curves\", X_train, y_train, cv=kfold)\nplot_learning_curve(knn1,\"KNN: Winning Model learning curves\", X_train, y_train, cv=kfold)","1a7aeb0e":"# Training","3eb8fe67":"# Model Comparison\n\nIn the course of work, there were many models with roc_auc of about 0.9-0.93, but when testing they mostly showed lower results. This does not mean that they are worse. Perhaps the dataset is not well formed.","e3f2a231":"##### Model 1","b39c7cd1":"#### Sex","23fe8aac":"**GradientBoostingClassifier**","fb644251":"# Is this your first Machine Learning model?\n\nThis kernel is highly recommended: [Alexis Cook\u2019s Titanic Tutorial](https:\/\/www.kaggle.com\/alexisbcook\/titanic-tutorial) that walks you through making your very first submission step by step.\nThen come back here and enjoy climbing the leaderboard!","f78f58f0":"## Age","a0c20d63":"**K-Nearest Neighbors**","686a518b":"# EDA","59f32b67":"# Data Wrangling - Part I","aaf22d60":"## Title","0a943170":"#### Pclass","5797eca3":"# Please share and upvote!\ud83d\udc4b\ud83d\udef3\ufe0f","58df108a":"So the survival chances do differ with the family size of the passenger. This is an important feature which helped me increase the prediction score substantially.","ab99904c":"# Titanic: Machine Learning from Disaster\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.","612554b8":"Small families have more chance to survive, more than single (Parch 0), medium (Parch 3,4) and large families (Parch 5,6).","ed8dfb2b":"**Last_Name**\nThe 'Last_Name' column groups families and people with the same tickets and explores the information.\n\n**Spoiler:** \nthis feature that helped improve the current result quite well. It is quite complicated in ideological understanding, but it is worth it.","6e29ceec":"#### Embarked","1d1bf361":"A passenger arriving from Cherbourg (C) seems to be more likely to survive. But I think that this will not be a key function for our prediction.","435552c4":"**The target variable 'Survived' shows high correlation with 'Fare', 'Parch', 'SibSp' and 'Pclass'.\nHence, These features should be analysed first.**","87b8e648":"### Please Note\nWhen I started doing this analysis my main goal was getting experience. I'm still learning and trying to improve my skills, so there might be some areas can be improved.\nThank you!","1867c280":"### Parch","952a37f9":"# Data Wrangling - II","9ee36289":"There is no missing value on this feature and already a numerical value. So let's check it's impact on our train set.","01007dbe":"**RandomForest**","bbe2b60a":"#### Heatmap","430df032":"##### Model 2\nThis is the winning model which I achieved after spending hours on hyperparameter tuning. Despite not very high training score, this model performed very well on the test dataset.","c035a8b6":"Men are less likely to survive than women. This is quite logical, because women were also allowed to go forward on rescue boats. Perhaps this will play a key role later.","2a75b144":"This is one of the most important engineered features. It helps in segregating passengers according to their ranks which obviously played role in their survival.","aeb7ad41":"**Plot learning curves**\n\nLearning curves are a good way to see the overfitting effect on the training set and the effect of the training size on the accuracy.","b1244db2":"**XGBoost**","34850d08":"### Classifiers\n\nI compared several popular classifiers and estimated the average accuracy of each of them using cross-validation. For each model, I selected the parameters with the help of GridSearch.\n\n* RandomForest\n* GradientBoosting\n* XGB\n* KNN","90a5e438":"The age distribution is probably a Gaussian distribution.\nThe graphs show a peak in survival among elders. Unfortunately, young people had less chances to survive. \nA jump in survival in children is also visible, which is quite logical. Most likely they were saved in the first place.","f2febe33":"The passenger survival is not the same in the 3 classes. First class passengers have more chance to survive than second class and third class passengers.","6c87fca5":"### Introduction\n\nI decided to share my first notebook after I noticed beginners in the discussion forum being overwhelmed with a plethora of public kernels claiming to give extraordinary results. To be honest, most of these kerels have badly trained models which underperform and output poor results on the leaderboard. This makes them nothing more than 'click-baits'. \n\nThese substandard kernels also contain unnecessary code which is not at all required to analyse the dataset ad are rather misleading. You can spend countless hours data wrangling which does not contribute to the prediction or plot innumerable graphs without deriving any meaningful insights. \n\nYour aim should be to start with a quick EDA of the dataset and train a simple model.\n\nThis kernel is an extremely simple solution to your first Kaggle competetion - 'Titanic: Machine Learning from Disaster' to put you in top 2%.\n\n**If this kernel helped you in any way, kindly upvote and share so that others can learn too.\nThank you!**\n\n**-----------------------------------------------------------------------------------------------------------**"}}