{"cell_type":{"f64b86e7":"code","40169be2":"code","2f61c697":"code","b85595ca":"code","262a8d47":"code","600af7f1":"code","97bd9470":"code","2f80bfa1":"code","37724c77":"code","8f21e5b4":"code","6cd02f75":"code","6e5924ac":"code","093093c3":"code","507da37e":"code","ee9e1462":"code","c76ed2bf":"code","55f4aaa3":"code","28bca4bb":"code","3df888b3":"code","09942edb":"code","eef52cbc":"code","c4d155a9":"code","63bd368d":"code","4a13999d":"code","084b84e8":"code","8c2227f0":"code","44331b4d":"code","2796a22c":"code","2e262500":"code","c47e78de":"code","6d4b591a":"code","84418569":"code","c37fc962":"code","da2c2b3f":"code","589f2517":"code","41149dd3":"code","2fbb04dd":"code","97560e13":"code","1898323a":"code","e40b42cf":"code","bc3b5ef2":"code","de14fd67":"code","b0689e3e":"markdown","b0c8d084":"markdown","4addd450":"markdown","a5cac008":"markdown","f1bcbc97":"markdown","db5cda0d":"markdown","93d9ed59":"markdown","0884471d":"markdown","3f1f877c":"markdown","6f315c3c":"markdown","dccaf86c":"markdown","b34d9035":"markdown","f56c95bc":"markdown","220a5c85":"markdown","84b17d96":"markdown","84ed5670":"markdown","42a59a4a":"markdown","98ef237b":"markdown","2d71cb44":"markdown","64f44ba6":"markdown","5caaed15":"markdown","ed93b745":"markdown","570caae6":"markdown","869faf34":"markdown","c3f25f3a":"markdown","6096fa29":"markdown","3a641af6":"markdown","09a1b007":"markdown","78c45db3":"markdown","eceb6074":"markdown","21d29d06":"markdown","5256efd9":"markdown","8ab74491":"markdown","796f6645":"markdown"},"source":{"f64b86e7":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport altair as alt\nfrom itertools import product\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.cluster import KMeans\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_log_error","40169be2":"kaggle_train = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/train.csv')\nkaggle_test = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/test.csv')\ndf_submit = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/sample_submission.csv')","2f61c697":"def make_new_features(df):\n    df[\"date_time\"] = pd.to_datetime(df['date_time'], format = \"%Y-%m-%d %H:%M:%S\")   \n    df[\"day_of_week\"] = df[\"date_time\"].dt.dayofweek\n    df[\"day_of_year\"] = df[\"date_time\"].dt.dayofyear\n    df[\"hour\"] = df[\"date_time\"].dt.hour\n    df[\"quarter\"] = df[\"date_time\"].dt.quarter\n    df[\"week_of_year\"] = df[\"date_time\"].dt.isocalendar().week.astype(\"int\")\n    df[\"month\"] = df[\"date_time\"].dt.month\n    df[\"is_winter\"] = df[\"month\"].isin([1, 2, 12])\n    df[\"is_spring\"] = df[\"month\"].isin([3, 4, 5])\n    df[\"is_summer\"] = df[\"month\"].isin([6, 7, 8])\n    df[\"is_autumn\"] = df[\"month\"].isin([9, 10, 11])\n    #df = df.drop(\"month\",axis=1).copy()\n    df[\"working_hours\"] =  df[\"hour\"].isin(np.arange(8, 21, 1)).astype(\"int\")\n    df[\"is_weekend\"] = (df[\"date_time\"].dt.dayofweek >= 5).astype(\"int\")\n    df[\"date_time\"] = df['date_time'].astype('datetime64[ns]').astype(np.int64)\/10**9\n    return df","b85595ca":"kaggle_train = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/train.csv')\nkaggle_test = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/test.csv')\ndf_submit = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/sample_submission.csv')\n\nkaggle_train = make_new_features(kaggle_train).copy()\nkaggle_test = make_new_features(kaggle_test).copy()","262a8d47":"df_train, df_test = train_test_split(kaggle_train,train_size=5000)","600af7f1":"# Get the target columns and predictor columns\ntar_cols = [x for x in kaggle_train.columns if x.startswith(\"target\")]\npred_cols = [x for x in kaggle_train.columns if x not in tar_cols]","97bd9470":"alt.Chart(df_train).mark_point().encode(\n    x = 'sensor_3',\n    y = 'target_carbon_monoxide'\n)","2f80bfa1":"alt.Chart(df_train.iloc[:400]).mark_circle(size=100).encode(\n    x = 'date_time:T',\n    y = 'target_carbon_monoxide:Q',\n    color = alt.Color(\"target_benzene:Q\",scale=alt.Scale(scheme=\"yelloworangered\"))\n).properties(width=700,height=500)","37724c77":"\nalt.Chart(df_train.iloc[:1000]).mark_circle(size=100).encode(\n    x = 'absolute_humidity',\n    y = 'target_carbon_monoxide:Q',\n    tooltip = list(df_train.columns),\n    color = alt.condition((alt.datum.hour == 3) | (alt.datum.hour == 4), alt.value(\"red\"), alt.value(\"blue\")),\n    opacity = alt.condition((alt.datum.hour == 3) | (alt.datum.hour == 4), alt.value(0.7), alt.value(0.1))\n).properties(width=700,height=500)","8f21e5b4":"# Here is a short way to display multiple plots in a grid.\nalt.Chart(df_train.iloc[:500]).mark_point().encode(\n    alt.X(alt.repeat(\"column\"), type='quantitative',scale=alt.Scale(zero=False)),\n    alt.Y(alt.repeat(\"row\"), type='quantitative')\n).properties(\n    width=200,\n    height=200\n).repeat(\n    row=tar_cols,\n    column=pred_cols[:]\n)","6cd02f75":"target = 'target_nitrogen_oxides'\n\nx_bounds = (250,450)\ny_bounds = (70,1400)\n\nbrush = alt.selection_interval(empty='none',init={'x':x_bounds,'y':y_bounds}) \nbase = alt.Chart(df_train.iloc[:1000]).mark_point().encode(\n    color=alt.condition(brush,alt.value('red'),alt.value('blue')),\n    opacity=alt.condition(brush,alt.value(0.7),alt.value(0.1))\n).add_selection(brush).properties(\n    width=200,\n    height=200\n)\n\ncharts = {(i,j):base.encode(x = 'sensor_'+str(i), y=j) for i in range(1,6) for j in tar_cols}\ndate_charts = [base.encode(x = 'date_time:T', y = t+':Q') for t in tar_cols]\n\ntext_chart = alt.Chart(df_train.iloc[:1000]).transform_filter(brush).mark_text(size=24, baseline = 'top').encode(\n        text=alt.Text('count():Q')\n    ).properties(\n        width = 200,\n        height = 200,\n        title=\"Number of points selected\"\n    )\n\nalt.vconcat(charts[(2,target)],\n            alt.hconcat(text_chart, base.encode(x = 'date_time:T', y = target),\n                        *[base.encode(x = p, y=target) for p in ['deg_C','relative_humidity','absolute_humidity']]\n                       ),\n            alt.hconcat(*[charts[(i,target)] for i in range(1,6)])\n           )","6e5924ac":"target = 'target_nitrogen_oxides'\n\nx_bounds = (250,450)\ny_bounds = (70,1400)\n\ndf_sub = df_train.query('sensor_2 < @x_bounds[1]')\n\nbrush = alt.selection_interval(empty='none',init={'x':x_bounds,'y':y_bounds}) \nbase = alt.Chart(df_sub).mark_circle().encode(\n    color=alt.condition(brush,alt.value('red'),alt.value('blue'))\n).add_selection(brush).properties(\n    width=250,\n    height=220\n)\n\nfull_charts = [alt.Chart(pd.concat([df_sub,df_train.iloc[:500]])).mark_point().encode(\n                x = \"sensor_2\",\n                y = t,\n                color=alt.condition(brush,alt.value('red'),alt.value('blue'))\n                ).properties(\n                    width=250,\n                    height=220\n                ).add_selection(brush) for t in tar_cols[::-1]]\n\ncharts = {(i,j):base.encode(x = 'sensor_'+str(i), y=j) for i in range(1,6) for j in tar_cols}\n\n\nalt.vconcat(alt.hconcat(charts[(2,target)], *full_charts),\n                alt.hconcat(*[base.encode(x=\"sensor_2:Q\", y=p) \n                                          for p in [\"date_time\",\"deg_C\",\"relative_humidity\",\"absolute_humidity\"]]),\n                alt.hconcat(*[base.encode(x = 'sensor_2:Q', y=p) for p in [\"sensor_1\",\"sensor_3\",\"sensor_4\",\"sensor_5\"]])\n            )","093093c3":"brush = alt.selection_interval(empty='none',init={'x':x_bounds,'y':(20,30)}) \nbase = alt.Chart(df_sub).mark_circle().encode(\n    color=alt.condition(brush,alt.value('red'),alt.value('blue'))\n).add_selection(brush).properties(\n    width=250,\n    height=220\n)\n\nfull_charts = [alt.Chart(pd.concat([df_sub,df_train.iloc[:500]])).mark_point().encode(\n                x = \"sensor_2\",\n                y = t,\n                color=alt.condition(brush,alt.value('red'),alt.value('blue'))\n                ).properties(\n                    width=250,\n                    height=220\n                ).add_selection(brush) for t in tar_cols[::-1]]\n\nalt.hconcat(base.encode(x=\"sensor_2:Q\", y = \"deg_C\"), *full_charts)","507da37e":"prec = 10\nm0 = int(np.floor(min(df_train[\"sensor_2\"])\/prec) * prec)\nm1 = int(np.floor(max(df_train[\"sensor_2\"])\/prec) * prec)\n\ndef find_max_in_range(df,var0,var1,lbound,rbound):\n    try:\n        return max(df.loc[df[var0].between(lbound,rbound),var1])\n    except:\n        return np.nan\n\ndf_max = pd.DataFrame({\"sensor_start\" : list(range(m0,m1,prec)), \n                       \"max_nitrogen\":[find_max_in_range(df_train,\"sensor_2\",\"target_nitrogen_oxides\",i,i+prec) for i in range(m0,m1,prec)]})\ndf_max.dropna(inplace=True)","ee9e1462":"chart_all = alt.Chart(df_train).mark_circle(size=100).encode(\n                x = \"sensor_2\",\n                y = \"target_nitrogen_oxides\",\n                tooltip = [\"sensor_2\"],\n).properties(width=500)\n\nchart_max = alt.Chart(df_max).mark_circle(size=100).encode(\n    x = \"sensor_start\",\n    y = \"max_nitrogen\",\n    tooltip = [\"sensor_start\"],\n).properties(width=500)\n\nchart_all | chart_max","c76ed2bf":"xq = df_max[\"sensor_start\"].quantile(.2)\n\nrule_x = alt.Chart(df_max).mark_rule(color='red').encode(\n    x='a:Q'\n).transform_calculate(\n    a=str(xq)\n)\n\nrule_y = alt.Chart(df_max).mark_rule(color='red').encode(\n    y = 'median(max_nitrogen):Q'\n)\n\nchart_max+rule_x+rule_y","55f4aaa3":"# Here are the corresponding values.\ndf_upper_left = df_max[(df_max[\"sensor_start\"] < xq) & (df_max[\"max_nitrogen\"] > np.median(df_max[\"max_nitrogen\"]))]\ndf_upper_left","28bca4bb":"sensor_2_cutoff = max(df_upper_left[\"sensor_start\"]) + 3*prec\ndf_temp = df_max[(df_max[\"sensor_start\"] < sensor_2_cutoff)]\ndf_temp","3df888b3":"df_cluster = df_train.iloc[:,1:].copy() #ignore the date_time column.\n\nkmeans = KMeans(n_clusters=2).fit(df_cluster)\ndf_cluster[\"cluster\"] = kmeans.predict(df_cluster)\n\nalt.Chart(df_cluster.iloc[:1000]).mark_point().encode(\n    x='sensor_2',\n    y='target_nitrogen_oxides',\n    color='cluster:N'\n).properties(\n    width=800,\n    height=400\n)","09942edb":"df_cluster = df_train.copy() # make a copy because I'm going to include a new cluster column.\ncols = [\"sensor_2\",\"deg_C\",\"sensor_1\",\"sensor_5\"]\nmask = (df_cluster[\"sensor_2\"] < sensor_2_cutoff)\nkmeans = KMeans(n_clusters=2).fit(df_cluster.loc[mask, cols])\ndf_cluster.loc[mask, \"cluster\"] = kmeans.predict(df_cluster.loc[mask, cols])","eef52cbc":"def make_row(df,ycols):\n    return alt.hconcat(*[alt.Chart(df).mark_circle().encode(\n            alt.X(\"sensor_2:Q\",        \n                ),\n            y=c,\n            color = alt.Color(\"cluster:N\",scale=alt.Scale(scheme=\"accent\"))\n            ).properties(\n                width=240,\n                height=240\n            ) for c in ycols])\n\ntarget = \"target_nitrogen_oxides\"\nalt.vconcat(make_row(df_cluster.iloc[:1500],[target]+cols[1:]),make_row(df_cluster.loc[mask],[target]+cols[1:]))","c4d155a9":"print(df_cluster[\"cluster\"].value_counts())\nc_outlier = df_cluster[\"cluster\"].value_counts().index[0]\nc_main = 1-c_outlier","63bd368d":"def write_cluster(df):\n    temp_series = pd.Series(dict(zip(df.index,kmeans.predict(df.loc[:,cols]))))\n    temp_series.loc[df[\"sensor_2\"] >= sensor_2_cutoff] = c_main\n    return temp_series","4a13999d":"df_chart = df_train.copy()\ndf_chart[\"cluster\"] = write_cluster(df_chart)\n\nalt.Chart(df_chart.iloc[:1000]).mark_circle().encode(\n    alt.X(alt.repeat(\"column\"), type='quantitative',scale=alt.Scale(zero=False)),\n    alt.Y(alt.repeat(\"row\"), type='quantitative'),\n    color = alt.condition(alt.datum.cluster == c_main, alt.value(\"blue\"), alt.value(\"red\")),\n    opacity = alt.condition(alt.datum.cluster == c_main, alt.value(0.1),alt.value(0.7))\n).properties(\n    width=200,\n    height=200\n).repeat(\n    row=tar_cols,\n    column=pred_cols\n)","084b84e8":"df_chart = df_test.copy()\ndf_chart[\"cluster\"] = write_cluster(df_chart)\n\nalt.Chart(df_chart.iloc[:1000]).mark_circle().encode(\n    alt.X(alt.repeat(\"column\"), type='quantitative',scale=alt.Scale(zero=False)),\n    alt.Y(alt.repeat(\"row\"), type='quantitative'),\n    color = alt.condition(alt.datum.cluster == c_main, alt.value(\"blue\"), alt.value(\"red\")),\n    opacity = alt.condition(alt.datum.cluster == c_main, alt.value(0.1),alt.value(0.7))\n).properties(\n    width=200,\n    height=200\n).repeat(\n    row=tar_cols,\n    column=pred_cols\n)","8c2227f0":"alt.Chart(df_chart.iloc[:1000]).mark_circle().encode(\n    alt.X(alt.repeat(\"column\"), type='quantitative'),\n    alt.Y(alt.repeat(\"row\"), type='quantitative'),\n    color = alt.condition(alt.datum.cluster == c_main, alt.value(\"blue\"), alt.value(\"red\")),\n    opacity = alt.condition(alt.datum.cluster == c_main, alt.value(0.1),alt.value(0.7))\n).properties(\n    width=80,\n    height=80\n).repeat(\n    row=pred_cols[1:9],\n    column=pred_cols[1:9]\n)","44331b4d":"df_train_label = df_train.copy()\ndf_train_label[\"cluster\"] = write_cluster(df_train_label)\nX_train = df_train_label.drop(tar_cols, axis=1).copy()\ny_train = df_train_label[tar_cols].copy()\n\ndf_test_label = df_test.copy()\ndf_test_label[\"cluster\"] = write_cluster(df_test_label)\nX_test = df_test_label.drop(tar_cols, axis=1).copy()\ny_test = df_test_label[tar_cols].copy()\n\nX_train_main = X_train.query(\"cluster==@c_main\").copy()\ny_train_main = y_train.loc[X_train_main.index].copy()\nX_train_outlier = X_train.query(\"cluster==@c_outlier\").copy()\ny_train_outlier = y_train.loc[X_train_outlier.index].copy()\n\nX_test_main = X_test.query(\"cluster==@c_main\").copy()\ny_test_main = y_test.loc[X_test_main.index].copy()\nX_test_outlier = X_test.query(\"cluster==@c_outlier\").copy()\ny_test_outlier = y_test.loc[X_test_outlier.index].copy()","2796a22c":"loss_dictionary = {(i,j):{} for i in [\"main\",\"outlier\"] for j in [\"with\",\"without\"]}\n\nfor t in tar_cols:\n    regr = GradientBoostingRegressor()\n    regr.fit(X_train_main, y_train_main[t])\n    pred = regr.predict(X_test_main)\n    pred[pred < 0] = 0\n    loss_dictionary[(\"main\",\"with\")][t] = np.sqrt(mean_squared_log_error(pred.reshape(-1,1), y_test_main[t].to_numpy().reshape(-1,1)))\n    regr = GradientBoostingRegressor()\n    regr.fit(X_train_outlier, y_train_outlier[t])\n    pred = regr.predict(X_test_outlier)\n    pred[pred < 0] = 0\n    loss_dictionary[(\"outlier\",\"with\")][t] = np.sqrt(mean_squared_log_error(pred.reshape(-1,1), y_test_outlier[t].to_numpy().reshape(-1,1)))\n    regr = GradientBoostingRegressor()\n    regr.fit(X_train.drop(\"cluster\",axis=1).copy(), y_train[t])\n    pred = regr.predict(X_test_main.drop(\"cluster\",axis=1).copy())\n    pred[pred < 0] = 0\n    loss_dictionary[(\"main\",\"without\")][t] = np.sqrt(mean_squared_log_error(pred.reshape(-1,1), y_test_main[t].to_numpy().reshape(-1,1)))\n    pred = regr.predict(X_test_outlier.drop(\"cluster\",axis=1).copy())\n    pred[pred < 0] = 0\n    loss_dictionary[(\"outlier\",\"without\")][t] = np.sqrt(mean_squared_log_error(pred.reshape(-1,1), y_test_outlier[t].to_numpy().reshape(-1,1)))","2e262500":"triples = product([\"main\",\"outlier\"],[\"with\",\"without\"],tar_cols)\ndf_results = pd.DataFrame([{\"data\":t[0],\"method\":t[1]+\" cluster labels\",\"target\":t[2],\"loss\":loss_dictionary[(t[0],t[1])][t[2]]} for t in triples])","c47e78de":"def show_losses(label):\n    c = alt.Chart(df_results[df_results[\"data\"] == label]).mark_bar().encode(\n        x='method:N',\n        y='loss:Q',\n        color='method:N',\n        column=alt.Column('target:N',spacing=90, title=\"\"),\n        tooltip='loss'\n    ).properties(\n        width = 50,\n        title = alt.TitleParams(text = [\"Comparison of losses on the \" + label + \" data\",\"GradientBoostingRegressor\"],anchor=\"middle\",offset=10),\n    )\n    return c\n\n\nalt.hconcat(show_losses(\"main\"), show_losses(\"outlier\")).properties(spacing=100).configure_title(\n    align='center'\n)","6d4b591a":"loss_dictionary = {(i,j):{} for i in [\"main\",\"outlier\"] for j in [\"with\",\"without\"]}\n\nfor t in tar_cols:\n    regr = RandomForestRegressor()\n    regr.fit(X_train_main, y_train_main[t])\n    pred = regr.predict(X_test_main)\n    pred[pred < 0] = 0\n    loss_dictionary[(\"main\",\"with\")][t] = np.sqrt(mean_squared_log_error(pred.reshape(-1,1), y_test_main[t].to_numpy().reshape(-1,1)))\n    regr = RandomForestRegressor()\n    regr.fit(X_train_outlier, y_train_outlier[t])\n    pred = regr.predict(X_test_outlier)\n    pred[pred < 0] = 0\n    loss_dictionary[(\"outlier\",\"with\")][t] = np.sqrt(mean_squared_log_error(pred.reshape(-1,1), y_test_outlier[t].to_numpy().reshape(-1,1)))\n    regr = RandomForestRegressor()\n    regr.fit(X_train.drop(\"cluster\",axis=1).copy(), y_train[t])\n    pred = regr.predict(X_test_main.drop(\"cluster\",axis=1).copy())\n    pred[pred < 0] = 0\n    loss_dictionary[(\"main\",\"without\")][t] = np.sqrt(mean_squared_log_error(pred.reshape(-1,1), y_test_main[t].to_numpy().reshape(-1,1)))\n    pred = regr.predict(X_test_outlier.drop(\"cluster\",axis=1).copy())\n    pred[pred < 0] = 0\n    loss_dictionary[(\"outlier\",\"without\")][t] = np.sqrt(mean_squared_log_error(pred.reshape(-1,1), y_test_outlier[t].to_numpy().reshape(-1,1)))","84418569":"triples = product([\"main\",\"outlier\"],[\"with\",\"without\"],tar_cols)\ndf_results = pd.DataFrame([{\"data\":t[0],\"method\":t[1]+\" cluster labels\",\"target\":t[2],\"loss\":loss_dictionary[(t[0],t[1])][t[2]]} for t in triples])","c37fc962":"def show_losses(label):\n    c = alt.Chart(df_results[df_results[\"data\"] == label]).mark_bar().encode(\n        x='method:N',\n        y='loss:Q',\n        color='method:N',\n        column=alt.Column('target:N',spacing=90, title=\"\"),\n        tooltip='loss'\n    ).properties(\n        width = 50,\n        title = alt.TitleParams(text = [\"Comparison of losses on the \" + label + \" data\",\"RandomForestRegressor\"],anchor=\"middle\",offset=10),\n    )\n    return c\n\n\nalt.hconcat(show_losses(\"main\"), show_losses(\"outlier\")).properties(spacing=100).configure_title(\n    align='center'\n)","da2c2b3f":"# Same as above, except using the entire train.csv file.\ndf_train_label = kaggle_train.copy()\ndf_train_label[\"cluster\"] = write_cluster(df_train_label)\nX_train = df_train_label.drop(tar_cols, axis=1).copy()\ny_train = df_train_label[tar_cols].copy()\n\nX_test = kaggle_test.copy()\nX_test[\"cluster\"] = write_cluster(X_test)\n\nX_train_main = X_train.query(\"cluster==@c_main\").copy()\ny_train_main = y_train.loc[X_train_main.index].copy()\nX_train_outlier = X_train.query(\"cluster==@c_outlier\").copy()\ny_train_outlier = y_train.loc[X_train_outlier.index].copy()\n\nX_test_main = X_test.query(\"cluster==@c_main\").copy()\nX_test_outlier = X_test.query(\"cluster==@c_outlier\").copy()","589f2517":"# From the training data\nmask = (X_train[\"sensor_2\"] < sensor_2_cutoff)\nalt.vconcat(make_row(X_train.iloc[:1500],[\"deg_C\",\"sensor_1\",\"sensor_5\"]),\n            make_row(X_train.loc[mask],[\"deg_C\",\"sensor_1\",\"sensor_5\"]))","41149dd3":"# On the Kaggle test.csv data\nmask = (X_test[\"sensor_2\"] < sensor_2_cutoff)\nalt.vconcat(make_row(X_test.iloc[:1500],[\"deg_C\",\"sensor_1\",\"sensor_5\"]),\n            make_row(X_test.loc[mask],[\"deg_C\",\"sensor_1\",\"sensor_5\"]))","2fbb04dd":"alt.Chart(X_test.iloc[:1000]).mark_circle().encode(\n    alt.X(alt.repeat(\"column\"), type='quantitative'),\n    alt.Y(alt.repeat(\"row\"), type='quantitative'),\n    color = alt.condition(alt.datum.cluster == c_main, alt.value(\"blue\"), alt.value(\"red\")),\n    opacity = alt.condition(alt.datum.cluster == c_main, alt.value(0.1),alt.value(0.7))\n).properties(\n    width=80,\n    height=80\n).repeat(\n    row=pred_cols[1:9],\n    column=pred_cols[1:9]\n)","97560e13":"df_submit_baseline = df_submit.copy()\n\nfor t in tar_cols:\n    regr = GradientBoostingRegressor()\n    regr.fit(X_train.drop(\"cluster\",axis=1), y_train[t])\n    pred = regr.predict(X_test.drop(\"cluster\",axis=1))\n    pred[pred < 0] = 0\n    df_submit_baseline[t] = pred","1898323a":"df_submit_cluster = df_submit.copy()\n\nfor t in tar_cols:\n    regr = GradientBoostingRegressor()\n    regr.fit(X_train, y_train[t])\n    pred = regr.predict(X_test)\n    pred[pred < 0] = 0\n    df_submit_cluster[t] = pred","e40b42cf":"df_submit_GBR = df_submit.copy()\n\ndrop_cols = [f\"sensor_{i}\" for i in range(1,6)] + [\"absolute_humidity\"]\n\nfor t in tar_cols:\n    regr = GradientBoostingRegressor()\n    regr.fit(X_train_main, y_train_main[t])\n    pred = regr.predict(X_test_main)\n    pred[pred < 0] = 0\n    df_submit_GBR.loc[X_test_main.index,t] = pred\n    regr = GradientBoostingRegressor()\n    regr.fit(X_train.drop(drop_cols,axis=1), y_train[t])\n    pred = regr.predict(X_test_outlier.drop(drop_cols,axis=1))\n    pred[pred < 0] = 0\n    df_submit_GBR.loc[X_test_outlier.index,t] = pred","bc3b5ef2":"df_submit_baseline.to_csv(\"submit_baseline.csv\", index=False)\ndf_submit_cluster.to_csv(\"submit_cluster.csv\", index=False)\ndf_submit_GBR.to_csv(\"submit_GBR.csv\", index=False)","de14fd67":"df0 = df_submit_baseline.drop(\"date_time\",axis=1).copy()\ndf1 = df_submit_GBR.drop(\"date_time\",axis=1).copy()\n\ndf0.columns = [\"GBR Baseline \" + t for t in tar_cols]\ndf1.columns = [\"GBR with clusters \" + t for t in tar_cols]\n\ndf_merged = pd.merge(df0,df1,left_index=True,right_index=True)\ndf_merged = df_merged.reset_index()\n\nalt.vconcat(*[\nalt.Chart(df_merged.iloc[::17]).transform_fold(\n    [c for c in df_merged.columns if t in c],\n).mark_circle().encode(\n    x='index:Q',\n    y='value:Q',\n    color='key:N',\n    tooltip=['key:N','value:Q','index:Q']\n).properties(\n    title=t,\n    width=1000\n) for t in tar_cols]).resolve_scale(color='independent')","b0689e3e":"It seems like choosing 250 < sensor_2 < 450 and 20 < deg_C < 30 does a good job of identifying these outliers.  Below we will see if we can pick the outlier points out in a more automated manner.\n![visualization3.png](attachment:e98683bb-d1dc-47d0-b1ec-cb9ff76a6f24.png)","b0c8d084":"### What's one way to find the sensor_2 bounds for these outlier points?\nThis is kind of a weird section.  I wanted to find a \"robust\" way to get the < 450 bound I was using above.  I'm not satisfied with my method, but at least it shows an example of drawing horizontal and vertical lines (called \"rules\") in Altair.\n\nHow can we locate these outliers, at least their positions along the sensor_2 axis?  What stands out is that many of the target_nitrogen_oxides values are so much higher than you would expect.  (Of course we can't use that quantity on the test.csv file, since we will not know the target values.)  Let's collect some data on how these nitrogen oxides values relate to the sensor_2 values.","4addd450":"Let's write a function that provides a cluster label for a dataset using the above K-Means. It only works with the specific columns, etc chosen, so this is not a very robust function.\n\nWe artificially put everything outside of our sensor_2 interval into the \"main\" cluster.  Put everything outside that interval in the \"main\" cluster\n    \n","a5cac008":"Here is another view, still on the test data.","f1bcbc97":"Recall that our cluster stood out when zoomed in on our training data:","db5cda0d":"Do our points labelled as outliers also stand out in the test data?  (The part we split off from kaggle_train.)","93d9ed59":"### RandomForestRegressor\nThere's minimal or no noticeable improvement using our clusters with RandomForestRegressor","0884471d":"It's reassuring that our supposed cluster also stands out when plotted on the data from test.csv.","3f1f877c":"By default, Altair prefers dataframes with 5000 rows or less, so that's what we'll use for a training set.  (All of this comes from the Kaggle training set; so far this has nothing to do with the Kaggle test set.)\n","6f315c3c":"## Some visualization practice using Altair\n\nI'm a Kaggle beginner (bottom 13% from the June TPS competition!) and recently discovered the visualization library Altair.  I instantly felt more comfortable with Altair than I've ever felt with Matplotlib.  This notebook will be some practice with the Altair library.\n\nThe basic idea of this notebook is to use some visualizations to try to identify outliers.\n\nOnce outliers were identified, I copied the gradient boosting method from [Amitesh Gangrade](https:\/\/www.kaggle.com\/amiteshgangrade)'s notebook [https:\/\/www.kaggle.com\/amiteshgangrade\/average-of-random-forest-and-gradient-boosting\/](https:\/\/www.kaggle.com\/amiteshgangrade\/average-of-random-forest-and-gradient-boosting\/) and made two submission files, one which used the same method on the entire test set, and another method which treated the outliers differently.  I had originally hoped that simply including an \"outlier\" label when training would lead to a significant improvement, but it actually made the score worse.\n\n* Baseline submission file loss score: 0.23936\n* Submission file with cluster label loss score: 0.24136\n* Main submission file loss score: 0.20728\n\nI initially divided the training csv file from Kaggle into a training set and a test set (both coming from the train.csv file).  The methods which performed best on that test set did not seem to perform best on the Kaggle submissions.  I am still a little unclear on the cause of that discrepancy.\n\nOne aspect of Altair I don't understand at all yet is the [data transformations](https:\/\/altair-viz.github.io\/user_guide\/transform\/index.html).  That is the next aspect of Altair I would like to learn about.\n\nCorrections or suggestions for improvement are very welcome!","dccaf86c":"Here we show two charts:  \n* the first shows all data points\n* the second shows only the max target_nitrogen_oxides values for width-10 intervals.","b34d9035":"There are two main things to notice in the next pictures:\n* The clusters aren't at all obvious in the full pictures (nor in the zoomed in nitrogen_oxides chart)\n* The clusters are pretty obvious if we zoom in to our small sensor_2 range and look at the correct axes.","f56c95bc":"### Examples of some visualization using Altair\n\nFirst we give a basic example of plotting in Altair using the values in df_train.\nWe give the column names, and the :T and :Q tell Altair what type of values are in those columns: [encoding data types](https:\/\/altair-viz.github.io\/user_guide\/encoding.html#encoding-data-types)","220a5c85":"### Gradient boosting application\nI don't know much about this technique.  I saw it in [Amitesh Gangrade](https:\/\/www.kaggle.com\/amiteshgangrade)'s notebook [https:\/\/www.kaggle.com\/amiteshgangrade\/average-of-random-forest-and-gradient-boosting\/](https:\/\/www.kaggle.com\/amiteshgangrade\/average-of-random-forest-and-gradient-boosting\/).\n\nWe investigate the question: how does the method perform if we remove the \"outliers\" before fitting?","84b17d96":"The next cell is almost entirely copied from [Amitesh Gangrade's notebook](https:\/\/www.kaggle.com\/amiteshgangrade\/average-of-random-forest-and-gradient-boosting\/)","84ed5670":"In the small region where we labeled clusters, almost all the points would be considered \"outliers\" in the full region. The zeroth entry in the index is the corresponding outlier label.","42a59a4a":"We add a tooltip showing all of the data for these points.  We color hours 3 and 4 red and make the other hours less opaque.","98ef237b":"Here are some similar pictures without using the target values on the y-axes.  (We wouldn't be able to make the above pictures on the test.csv file data, but we can make the following pictures on the test.csv data.  We will do that later.)","2d71cb44":"Here we evaluate our main test data using only the main training data.  For the outlier data, looking back at our above images, it seemed like the outliers were most extreme with regards to the sensor columns and the absolute humidity column, so we remove those before training.  (The performance seems to be worse if we train only using the outliers from the training set.)\n\n* Loss score: 0.20728","64f44ba6":"I saw many nice similar plots made in Seaborn and Matplotlib in this notebook: [https:\/\/www.kaggle.com\/docxian\/tabular-playground-7-visualization-baseline](https:\/\/www.kaggle.com\/docxian\/tabular-playground-7-visualization-baseline)","5caaed15":"### File creation\nWe generate two files, both using GradientBoostingRegressor: A baseline file without cluster labels, and a second file with cluster labels.  I tried miscellaneous alterations hoping one of them would perform especially well.  (I didn't notice much correlation between what performed well on my personal test set (from train.csv) and what performed well when submitted to Kaggle.)","ed93b745":"Midway file, where we make no changes other than to include the cluster label when training.  It performs slightly worse!  (Whenever I get such results, I wonder if I made a mistake somewhere.)\n\n* Loss score: 0.24136","570caae6":"These next pictures look very similar to what we had above, but notice that the coloring here was done without ever using the target values.","869faf34":"The red points seem to follow a quite different pattern from the blue points.  The sensor_2 values for these points are all in the range 250 to 450, but that information isn't enough to identify the red points, because if we select all the points with sensor_2 in the range from 250 to 450, we will also select many \"non-outlier\" points.<br><br>In the next cell, we zoom in on the range 250 < sensor_2 < 450.<br><br>You can click and drag to choose a new rectangular region.  These next interactions are faster because we use fewer points.","c3f25f3a":"We'll use a corresponding value as our cutoff.  Recall that the sensor_start values correspond to width-10 intervals.  It feels pretty ad hoc to me.<br><br>Notice how much the maximum drops as we approach this cutoff.","6096fa29":"### Can we identify these same points using K-Means clustering?\n\nThe main point of this section is to produce similar pictures to what we had above, but without using any of the target values.  This will give a method for identifying the similar outliers on the test data.\n\nMy first attempt, without using anything that we learned above, did not work.\nProbably there is some clustering algorithm other than K-Means which would work better.","3a641af6":"Here you can see the output differences for our two methods.  I only show part of the data to make it easier to read.","09a1b007":"The next pictures suggest this method performs great on the outliers, but in my file submissions that has not seemed to be the case.","78c45db3":"Baseline file, without using the cluster label.\n* Loss score: 0.23936","eceb6074":"Here we use only 400 points and add color.  (Since train_test_split already shuffles the rows, there's no need to choose a random sample.) The :T and :Q tell Altair that these are dates and quantitative values [encoding data types](https:\/\/altair-viz.github.io\/user_guide\/encoding.html#encoding-data-types).  Here is a list of some different [color schemes](https:\/\/vega.github.io\/vega\/docs\/schemes\/).","21d29d06":"What if we try to restrict our attention to a smaller amount of the data, using the sensor_2 bound we found above and a subsest of the columns.  I chose these four columns by looking at the above images and asking which images most clearly related to the outlier points. Overall I think I did these steps just to practice with K-Means.","5256efd9":"### Making plots interactive\n\nWhat first caught my eye about Altair is the interactive features in charts.  There is a lot I don't understand, but we will give some examples based on the documentation here: [https:\/\/altair-viz.github.io\/user_guide\/interactions.html](https:\/\/altair-viz.github.io\/user_guide\/interactions.html)<br><br>I looked through those above plots and decided to focus on this one, and in particular. I want to consider the question, of how to identify the vertical clump on the left side.\n\n![sensor2_nitrogen.png](attachment:23456450-8258-47f1-b460-a815258af123.png)\n\nIn the next plots, you can click and drag (within any of the plots) to choose a new rectangular region.  We're only showing 1000 of our 5000 training points to keep the image file size a little smaller.","8ab74491":"Here is the code for the above images.  You can click and drag to choose a new region.","796f6645":"Here is one visual way to identify the upper-left cluster:  \nThe sensor_start values are in the lowest quantile and the max_nitrogen values are above the median. (I checked and using median for both would not work.)  Having to convert the quantile value to a string feels weird; did I do something wrong?"}}