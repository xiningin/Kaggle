{"cell_type":{"d5db52f2":"code","a1c58ec3":"code","3a626356":"code","ad2c7620":"code","99177e42":"code","27fcdbcb":"code","99128601":"code","60c766b8":"code","82e2a449":"code","8ef1fad4":"code","9ab34cac":"code","4c505377":"code","9311e240":"code","2365371f":"code","e01c001c":"code","afa80a1a":"code","49590104":"code","aeab9d34":"code","cbc09a84":"markdown","7105c66f":"markdown","f136ffed":"markdown","f83d3e67":"markdown","195a3696":"markdown","16ffa9f8":"markdown","53e775c8":"markdown","e2caca59":"markdown","6fea13b2":"markdown","f7a937cb":"markdown","cd7669cb":"markdown","2bed7938":"markdown","1ff4f396":"markdown","41d3f1b6":"markdown","9127f53d":"markdown","3988c0f3":"markdown","1a7fbcd1":"markdown","39eb5d5c":"markdown","d6c1871e":"markdown","25c84aba":"markdown","7fe9ced6":"markdown","d2cef130":"markdown","3941e922":"markdown","85083bb9":"markdown"},"source":{"d5db52f2":"# read in the iris data\nfrom sklearn.datasets import load_iris\niris = load_iris()\n\n# create X (features) and y (response)\nX = iris.data\n\ny = iris.target\nprint(y)","a1c58ec3":"print(X)","3a626356":"# import the class\nfrom sklearn.linear_model import LogisticRegression\n\n# instantiate the model (using the default parameters)\nlogreg = LogisticRegression()\n\n# fit the model with data\nlogreg.fit(X, y)\n\n# predict the response values for the observations in X\nlogreg.predict(X)","ad2c7620":"# store the predicted response values\ny_pred = logreg.predict(X)\n\n# check how many predictions were generated\nlen(y_pred)","99177e42":"# compute classification accuracy for the logistic regression model\nfrom sklearn import metrics\nprint(metrics.accuracy_score(y, y_pred))","27fcdbcb":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X, y)\ny_pred = knn.predict(X)\nprint(metrics.accuracy_score(y, y_pred))","99128601":"knn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(X, y)\ny_pred = knn.predict(X)\nprint(metrics.accuracy_score(y, y_pred))","60c766b8":"# print the shapes of X and y\nprint(X.shape)\nprint(y.shape)","82e2a449":"# STEP 1: split X and y into training and testing sets\n#from sklearn.cross_validation import train_test_split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=4)","8ef1fad4":"# print the shapes of the new X objects\nprint(X_train.shape)\nprint(X_test.shape)","9ab34cac":"# print the shapes of the new y objects\nprint(y_train.shape)\nprint(y_test.shape)","4c505377":"# STEP 2: train the model on the training set\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)","9311e240":"# STEP 3: make predictions on the testing set\ny_pred = logreg.predict(X_test)\n\n# compare actual response values (y_test) with predicted response values (y_pred)\nprint(metrics.accuracy_score(y_test, y_pred))","2365371f":"knn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\nprint(metrics.accuracy_score(y_test, y_pred))","e01c001c":"knn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\nprint(metrics.accuracy_score(y_test, y_pred))","afa80a1a":"# try K=1 through K=25 and record testing accuracy\nk_range = list(range(1, 26))\nscores = []\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    y_pred = knn.predict(X_test)\n    scores.append(metrics.accuracy_score(y_test, y_pred))","49590104":"# import Matplotlib (scientific plotting library)\nimport matplotlib.pyplot as plt\n\n# allow plots to appear within the notebook\n%matplotlib inline\n\n# plot the relationship between K and testing accuracy\nplt.plot(k_range, scores)\nplt.xlabel('Value of K for KNN')\nplt.ylabel('Testing Accuracy')","aeab9d34":"# instantiate the model with the best known parameters\nknn = KNeighborsClassifier(n_neighbors=11)\n\n# train the model with X and y (not X_train and y_train)\nknn.fit(X, y)\n\n# make a prediction for an out-of-sample observation\nknn.predict([[3, 5, 4, 2]])","cbc09a84":"## Review\n\n- Classification task: Predicting the species of an unknown iris\n- Used three classification models: KNN (K=1), KNN (K=5), logistic regression\n- Need a way to choose between the models\n\n**Solution:** Model evaluation procedures","7105c66f":"### KNN (K=5)","f136ffed":"[Train\/test split](https:\/\/imgur.com\/L0kDwMB)","f83d3e67":"- **Training accuracy** rises as model complexity increases\n- **Testing accuracy** penalizes models that are too complex or not complex enough\n- For KNN models, complexity is determined by the **value of K** (lower value = more complex)","195a3696":"## Downsides of train\/test split?","16ffa9f8":"## Agenda\n\n- How do I choose **which model to use** for my supervised learning task?\n- How do I choose the **best tuning parameters** for that model?\n- How do I estimate the **likely performance of my model** on out-of-sample data?","53e775c8":"## Making predictions on out-of-sample data","e2caca59":"Classification accuracy:\n\n- **Proportion** of correct predictions\n- Common **evaluation metric** for classification problems","6fea13b2":"# Comparing machine learning models in scikit-learn\n****","f7a937cb":"Repeat for KNN with K=1:","cd7669cb":"## Evaluation procedure #1: Train and test on the entire dataset","2bed7938":"### KNN (K=1)","1ff4f396":"Can we locate an even better value for K?","41d3f1b6":"### Problems with training and testing on the same data\n\n- Goal is to estimate likely performance of a model on **out-of-sample data**\n- But, maximizing training accuracy rewards **overly complex models** that won't necessarily generalize\n- Unnecessarily complex models **overfit** the training data","9127f53d":"## Evaluation procedure #2: Train\/test split","3988c0f3":"1. Split the dataset into two pieces: a **training set** and a **testing set**.\n2. Train the model on the **training set**.\n3. Test the model on the **testing set**, and evaluate how well we did.","1a7fbcd1":"Repeat for KNN with K=5:","39eb5d5c":"- Known as **training accuracy** when you train and test the model on the same data","d6c1871e":"What did this accomplish?\n\n- Model can be trained and tested on **different data**\n- Response values are known for the testing set, and thus **predictions can be evaluated**\n- **Testing accuracy** is a better estimate than training accuracy of out-of-sample performance","25c84aba":"### Logistic regression","7fe9ced6":"1. Train the model on the **entire dataset**.\n2. Test the model on the **same dataset**, and evaluate how well we did by comparing the **predicted** response values with the **true** response values.","d2cef130":"- Provides a **high-variance estimate** of out-of-sample accuracy\n- **K-fold cross-validation** overcomes this limitation\n- But, train\/test split is still useful because of its **flexibility and speed**","3941e922":"## Resources\n\n- Quora: [What is an intuitive explanation of overfitting?](http:\/\/www.quora.com\/What-is-an-intuitive-explanation-of-overfitting\/answer\/Jessica-Su)\n- Video: [Estimating prediction error](https:\/\/www.youtube.com\/watch?v=_2ij6eaaSl0&t=2m34s) (12 minutes, starting at 2:34) by Hastie and Tibshirani\n- [Understanding the Bias-Variance Tradeoff](http:\/\/scott.fortmann-roe.com\/docs\/BiasVariance.html)\n    - [Guiding questions](https:\/\/github.com\/justmarkham\/DAT8\/blob\/master\/homework\/09_bias_variance.md) when reading this article\n- Video: [Visualizing bias and variance](http:\/\/work.caltech.edu\/library\/081.html) (15 minutes) by Abu-Mostafa","85083bb9":"[Overfitting](http:\/\/commons.wikimedia.org\/wiki\/File:Overfitting.svg#\/media\/File:Overfitting.svg) \n"}}