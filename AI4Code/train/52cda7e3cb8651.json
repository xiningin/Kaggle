{"cell_type":{"ee8c9fed":"code","31c63632":"code","52f5bc87":"code","2ceb59b2":"code","fcbf91a5":"code","610d8adf":"code","518b5b2e":"code","f5f79a6e":"code","c1c07dd9":"code","3ce529cc":"code","3e154921":"code","c9e5740e":"code","6151f4fa":"code","205ee341":"code","bfb4c872":"code","69eaa671":"code","e4e823e5":"code","e0aac9f7":"code","7338d688":"code","fc33c648":"code","39f3fdbc":"code","0ea1ee2e":"code","cc485705":"code","24618d5b":"code","b683b4cb":"code","be414f00":"code","2b75758f":"code","a28254c3":"code","4351e958":"code","c8d297d9":"markdown","78e7ffe0":"markdown","69605f8c":"markdown","c6b673fa":"markdown","00aef8f4":"markdown","726294d6":"markdown","9903d2ac":"markdown","fc613166":"markdown","eb271d2c":"markdown","8aee9339":"markdown","d8e7cda2":"markdown","12de6a99":"markdown","798b2d6b":"markdown","17882d8c":"markdown","26ebe070":"markdown","68619534":"markdown","b1a5373c":"markdown","66dd2ad1":"markdown","de8074a6":"markdown","086aaf6d":"markdown","9ed690c7":"markdown","b16427e0":"markdown","1ead773d":"markdown","9b082736":"markdown","487960d0":"markdown","a262ece6":"markdown"},"source":{"ee8c9fed":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","31c63632":"data = pd.read_csv('\/kaggle\/input\/sentiment140\/training.1600000.processed.noemoticon.csv')","52f5bc87":"data = pd.read_csv('\/kaggle\/input\/sentiment140\/training.1600000.processed.noemoticon.csv',encoding='ISO-8859-1')","2ceb59b2":"data.head()","fcbf91a5":"data.columns","610d8adf":"data = data.drop(['1467810369', 'Mon Apr 06 22:19:45 PDT 2009', 'NO_QUERY', '_TheSpecialOne_'],axis=1)","518b5b2e":"data.head()","f5f79a6e":"data = data.rename(columns={'0':'Sentiment', \n                            \"@switchfoot http:\/\/twitpic.com\/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\":'Text'})","c1c07dd9":"data.head()","3ce529cc":"def add1(value):\n    return value+1\ndata['Sentiment'].apply(add1)","3e154921":"data['Sentiment'] = data['Sentiment'].apply(add1)","c9e5740e":"data.head()","6151f4fa":"def clean_text(text):\n    #First, let's make the text lowercase.\n    text = text.lower()\n    \n    #Next, let's remove exclamation marks.\n    text_list = text.split('!') #This returns a list of items, split by !.\n    text = ''.join(text_list) #This puts the list back into a string. The exclamation marks are gone.\n    \n    #Same drill, but with other types of punctuation.\n    #For each punctuation mark in this list...\n    for punctuation in ['.','?',':','@','#','$','%','^','&','*','(',')','\"',\"'\",\",\",\";\",'[',']']:\n        #Split the text by that punctuation mark\n        text_list = text.split(punctuation)\n        #And rejoin it!\n        text = ''.join(text_list)\n    \n    return text","205ee341":"clean_text('! jijd.? SOIDJOIS.SJOIJ(^*(SJIJj.s!!io))')","bfb4c872":"data = data[ data['Text'].apply(len) < 25 ]\ndata","69eaa671":"data['Cleaned'] = data['Text'].apply(clean_text)","e4e823e5":"data.head()","e0aac9f7":"data = data.reset_index()\ndata","7338d688":"data = data.drop('index',axis=1)\ndata","fc33c648":"from sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer() #Create an instance, as usual\n\nvectorizer.fit(data['Cleaned']) #Like any sklearn model, it needs to be fit.","39f3fdbc":"transformed = vectorizer.transform(data['Cleaned']) #Then, we can use the vectorizer to 'transform', or vectorize the cleaned data.\n#We can store the data into variable called 'transformed'.","0ea1ee2e":"transformed.shape","cc485705":"X = transformed #These are the 'features', the X, that we will be plugging into the model...\ny = data['Sentiment'] #...and we want to get a sentiment rating from 1 to 5 out.","24618d5b":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.6,stratify=y) \n#Just to speed up training time. We normally wouldn't want to make the test larger than the train.\n\n#Stratifying is simply making sure that there is an equal amount of a class in a training set (e.g. 1k of label 1, 1k of label 2, etc.)\n#Prevents overfitting to one specific class.","b683b4cb":"from sklearn.linear_model import LogisticRegression #import model\nlog = LogisticRegression() #Create instance\nlog.fit(X_train,y_train) #Fit","be414f00":"from sklearn.tree import DecisionTreeClassifier\ndec = DecisionTreeClassifier()\ndec.fit(X_train,y_train)","2b75758f":"from sklearn.metrics import accuracy_score\naccuracy_score(log.predict(X_test),y_test)","a28254c3":"accuracy_score(dec.predict(X_test),y_test)","4351e958":"print(\"Enter your text.\")\ntext = input(':: ') #Let this be the text input.\n\n#Step 1 of the pipeline: clean the text\ntext1 = clean_text(text)\n\n#Step 2 of the pipeline: vectorize the cleaned text\ntext2 = vectorizer.transform([text1])\n\n#Step 3 of the pipeline: get a prediction for the sentiment.\nprediction = log.predict(text2)\n\nprint(\"Sentiment:\",prediction)\n\n#Try out: 'this is horrible' 'omg i love you'","c8d297d9":"Time to get the accuracy scores of each!","78e7ffe0":"Uh-oh: it seems that the standard encoding, utf-8, doesn't work. In this case, we'll use a different encoding.\nAn encoding is simply a format data is stored. Not all data will be stored in utf-8, which is the standard.","69605f8c":"Copy-paste the file directory into pd.read_csv.","c6b673fa":"Nice! Our data is now nicely formatted.","00aef8f4":"Great! Time to apply it to our text.\n\nHowever, just to shorten up our training process, I'mn reducing the data to those whose texts are only under 25 characters.","726294d6":"# Step 3: Vectorizing the Text","9903d2ac":"Great! Now we have training sets.\n\n\nWe'll be training the data on two of the algorithms we learned last week. (Random Forest takes too long)","fc613166":"Great! No errors.","eb271d2c":"Now, it's time to train our model. However, in order to fairly evaluate the model, what do we have to do?\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\nTrain test split!","8aee9339":"First, let's drop the columns that we don't need: 1467810369, Mon Apr 06 22:19:45 PDT 2009, NO_QUERY, and TheSpecialOne columns. All we want is the text and the class label.","d8e7cda2":"Let's try our function out.","12de6a99":"# Step 4: Training a model","798b2d6b":"It seems that this data needs a lot of cleaning! In this data, the column names seem to be the first row. This is another data cleaning task. For now, we'll have to assume what the columns mean based on their meaning.","17882d8c":"Our job now is to remove special characters from the text. We're going to be using .apply().\n\n.apply(function) takes in a function and applies it to a column.\n\nFor example:","26ebe070":"It's 111,025 by 47,678. Obviously, 111,025 represents each one of the texts.\n\nWhat does 47,678 represent?\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\nIt represents the number of total distinct words in the corpus. Each of the words is its own 'column'.","68619534":"# Step 1: Load the dataset.\nRun the cell below to see the files in the directory.","b1a5373c":"# Step 6: Build an interface","66dd2ad1":"You can see that what was once 0 became a 1, and what was once a 4 became a 5.\n\nIn fact, this would be useful, since we usually put 5-rating scales on 1 to 5, not 0 to 4.\n\nLet's set the Sentiment column to itself, plus 1.","de8074a6":"Much better! It's ready to be vectorized.\n\nAll we need is to get rid of the cluttered index. Does anyone remember how we do that?\n\n.\n\n.\n\n.\n\n.\n\n.","086aaf6d":"Now, let's make a function that processes text.","9ed690c7":"Let's get a list of the columns in the data.","b16427e0":"Let's look at the shape of our data.","1ead773d":"Perfect! Now we just have to rename the columns.\n\nLet's use .rename(). This function renames columns and takes in a dictionay of form {original_column_name:new_column_name}","9b082736":"# Step 2: Data Cleaning","487960d0":"We need to build a pipeline.\n\nA pipeline is a fancy way to say building a set of processes to transform the input text.","a262ece6":"Which algorithm performed the best?\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\nWe see that the logistic regression model performs the best. Now, it's time to build an interface."}}