{"cell_type":{"b5107476":"code","9bdb3a24":"code","4e65b660":"code","6b4c5f2b":"code","03280e13":"code","a899168a":"code","2559b08b":"code","5fae8dee":"code","cc7a3a26":"code","59991ed6":"code","5d9ea827":"code","fcc6cb43":"code","a6b476fc":"code","c0582d01":"code","822c00e0":"code","034df126":"code","87c3b5c5":"code","a4329167":"markdown","8561f214":"markdown","1cf9e387":"markdown","72d7259f":"markdown","7e934ea9":"markdown","4978aa48":"markdown","4082c649":"markdown","ea667823":"markdown","a4d9466b":"markdown"},"source":{"b5107476":"import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# import data into dataframe\ndf = pd.read_csv('..\/input\/heart.csv')","9bdb3a24":"# quick peek\ndf.head()","4e65b660":"# check for null values\ndf.isnull().sum()","6b4c5f2b":"df.describe()","03280e13":"# split into inputs and targets\nX = df.drop('target',axis=1)\ny = df['target']","a899168a":"# Feature normalization\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler().fit(X)\nX_scaled = scaler.transform(X)","2559b08b":"# split into train and test\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X_scaled, y)","5fae8dee":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\n\nclf = LogisticRegression(solver=\"lbfgs\")\nfrom sklearn.model_selection import validation_curve\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nparam_range = [0.01,0.025,0.05,0.1,0.2]\ntrain_scores, valid_scores = validation_curve(clf, X_train, y_train,\n                                              \"C\",param_range,\n                                              cv=5,scoring=\"f1\")\n\ntrain_mean = np.mean(train_scores,axis=1)\nvalid_mean = np.mean(valid_scores,axis=1)\n\nplt.plot(param_range, train_mean, label=\"Training\")\nplt.plot(param_range, valid_mean, label=\"Validation\")\nplt.tight_layout()\nplt.legend(loc=\"best\")\nplt.show()","cc7a3a26":"from sklearn.metrics import roc_curve\n\nclf = LogisticRegression(solver=\"lbfgs\", C=0.1).fit(X_train, y_train)\ny_pred_quant = clf.predict_proba(X_test)[:, 1]\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_quant)\n\nfig, ax = plt.subplots()\nax.plot(fpr, tpr)\nax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\".3\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.rcParams['font.size'] = 12\nplt.title('ROC curve for Logistic Classifier')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.grid(True)","59991ed6":"from sklearn.metrics import auc\nprint(auc(fpr, tpr))","5d9ea827":"from sklearn.metrics import confusion_matrix\npredict = clf.predict(X_test)\ncnf_matrix = confusion_matrix(y_test,predict)\n\nclass_names = [0,1]\nfig,ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks,class_names)\nplt.yticks(tick_marks,class_names)\n\n#create a heat map\nsns.heatmap(pd.DataFrame(cnf_matrix), annot = True, cmap = 'YlGnBu',\n           fmt = 'g')\nax.xaxis.set_label_position('top')\nplt.tight_layout()\nplt.title('Confusion matrix for Logistic Regression', y = 1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()","fcc6cb43":"from sklearn.ensemble import GradientBoostingClassifier\n\nclf = GradientBoostingClassifier()\n\nparam_range = [2,3,5,7,10]\ntrain_scores, valid_scores = validation_curve(clf, X_train, y_train,\n                                              \"n_estimators\",param_range,\n                                              cv=5,scoring=\"f1\")\n\ntrain_mean = np.mean(train_scores,axis=1)\nvalid_mean = np.mean(valid_scores,axis=1)\n\nplt.plot(param_range, train_mean, label=\"Training\")\nplt.plot(param_range, valid_mean, label=\"Validation\")\nplt.tight_layout()\nplt.legend(loc=\"best\")\nplt.show()","a6b476fc":"clf = GradientBoostingClassifier(n_estimators=5)\n\nparam_range = [0.01,0.02,0.05,0.075,0.1,0.2]\ntrain_scores, valid_scores = validation_curve(clf, X_train, y_train,\n                                              \"learning_rate\",param_range,\n                                              cv=5,scoring=\"f1\")\n\ntrain_mean = np.mean(train_scores,axis=1)\nvalid_mean = np.mean(valid_scores,axis=1)\n\nplt.plot(param_range, train_mean, label=\"Training\")\nplt.plot(param_range, valid_mean, label=\"Validation\")\nplt.tight_layout()\nplt.legend(loc=\"best\")\nplt.show()","c0582d01":"clf = GradientBoostingClassifier(n_estimators=3, learning_rate=0.075)\n\nparam_range = [2,3,4,5,6]\ntrain_scores, valid_scores = validation_curve(clf, X_train, y_train,\n                                              \"max_depth\",param_range,\n                                              cv=5,scoring=\"f1\")\n\ntrain_mean = np.mean(train_scores,axis=1)\nvalid_mean = np.mean(valid_scores,axis=1)\n\nplt.plot(param_range, train_mean, label=\"Training\")\nplt.plot(param_range, valid_mean, label=\"Validation\")\nplt.tight_layout()\nplt.legend(loc=\"best\")\nplt.show()","822c00e0":"clf = GradientBoostingClassifier(n_estimators=3, learning_rate=0.05, max_depth=4).fit(X_train, y_train)\ny_pred_quant = clf.predict_proba(X_test)[:, 1]\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_quant)\n\nfig, ax = plt.subplots()\nax.plot(fpr, tpr)\nax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\".3\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.rcParams['font.size'] = 12\nplt.title('ROC curve for GBDT')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.grid(True)","034df126":"from sklearn.metrics import auc\nprint(auc(fpr, tpr))","87c3b5c5":"predict = clf.predict(X_test)\ncnf_matrix = confusion_matrix(y_test,predict)\n\nclass_names = [0,1]\nfig,ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks,class_names)\nplt.yticks(tick_marks,class_names)\n\n#create a heat map\nsns.heatmap(pd.DataFrame(cnf_matrix), annot = True, cmap = 'YlGnBu',\n           fmt = 'g')\nax.xaxis.set_label_position('top')\nplt.tight_layout()\nplt.title('Confusion matrix for GBDT', y = 1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()","a4329167":"Find the optimal parameters with validation curves. The model will be optimized via cross validation on the F1 score, which combines Precision and recall. Ideally I would set the focus on recall as this is a medical classification and I want to avoid false positives as much as possible (Who wants a false heart attack prediction?)","8561f214":"## Gradient Boosting Decision tree\nsame idea here. Try to find the best hyperparameters with validation curves with a F1 scorer","1cf9e387":"# Preprocessing","72d7259f":"# Train some models","7e934ea9":"GDBT performs a bit better in terms of false positives.","4978aa48":"## Logistic Regression","4082c649":"AUC is 0.89 - it's in the upper \"good\" range.","ea667823":"Attribute Information: \n> 1. age \n> 2. sex \n> 3. chest pain type (4 values) \n> 4. resting blood pressure \n> 5. serum cholestoral in mg\/dl \n> 6. fasting blood sugar > 120 mg\/dl\n> 7. resting electrocardiographic results (values 0,1,2)\n> 8. maximum heart rate achieved \n> 9. exercise induced angina \n> 10. oldpeak = ST depression induced by exercise relative to rest \n> 11. the slope of the peak exercise ST segment \n> 12. number of major vessels (0-3) colored by flourosopy \n> 13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect","a4d9466b":"More false positives than negatives."}}