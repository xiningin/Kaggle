{"cell_type":{"8cf205cf":"code","ea73cef6":"code","fab7e111":"markdown","775b2bd2":"markdown","668aa69c":"markdown","4aaf551b":"markdown","ab1a5d9b":"markdown","5e08d91e":"markdown"},"source":{"8cf205cf":"import pandas as pd; pd.set_option(\"display.max_colwidth\", 60)\n# Context\nctx = \"\"\"World War II, often abbreviated as WW2, was a global war that lasted from 1939 to 1945. It involved the vast majority of the world's countries forming two opposing military alliances: the Allies and the Axis powers.\"\"\"\n\nq1 = \"How is the World War II often abbreviated?\"\n\nq2 = \"What was the World War II?\"\n\nq3 = \"How long did the WW2 last?\"\n\nq4 = \"Which were the two bands in the WW2?\"\n\nvalid_inputs = [(q1, ctx), (q2, ctx), (q3, ctx), (q4, ctx)]\n\npd.DataFrame(valid_inputs, columns=['Question', 'Context'])","ea73cef6":"a1 = \"WW2\" # How is the World War II often abbreviated?\n\na2 = \"a global war\" # What was the World War II?\n\na3 = \"from 1939 to 1945\" # How long did the WW2 last?\n\na4 = \"the Allies and the Axis powers\" # Which were the two bands in the WW2?\n\nvalid_training_samples = [(q1, ctx, a1), (q2, ctx, a2), (q3, ctx, a3), (q4, ctx, a4)]\n\npd.DataFrame(valid_training_samples, columns=['Question', 'Context', 'Answer'])","fab7e111":"Finnaly, adding the expected answers will turn this data into _training samples_ for a potential ML model:\n\n","775b2bd2":"<!--\n\nhttps:\/\/www.kaggle.com\/c\/quora-question-pairs\/overview\/description\nhttps:\/\/www.kaggle.com\/c\/tensorflow2-question-answering\/overview\nhttps:\/\/www.kaggle.com\/c\/google-quest-challenge\/overview\n\n-->","668aa69c":"Note that since this is **Extractive** Question Answering, the answer **must be** contained in the context. So, for example, Answer 3 cannot be `6 years`, although it would be a more accurate answer to question 3.\n\nA last note is the following: real models typically get the answer by signalling the start token and the end token of the answer (or the initial token and the amount of them):\n\n<h2 style=\"text-align: center\">(<i>question<\/i>, <i>context<\/i>) $\\rightarrow$ <i>(start position, end position)<\/i><\/h2>\n\nThis is, simply put, the problem at hand. Let's check the code requirements of the competition now.","4aaf551b":"# Code requirements \n\nYou can check them in the [Code Requirements](https:\/\/www.kaggle.com\/c\/chaii-hindi-and-tamil-question-answering\/overview\/code-requirements)' tab of the competition, but I summarize them below:\n\n* Kernel Only\n* Internet disabled\n* 5-hour runtime \n* Freely & publicly available external data is allowed, including pre-trained models\n\nWhat does all this mean?\n\n# Kernel Only\n`Kernel Only` means that the only way to submit results is from Kaggle notebooks. We cannot generate a submission on another computer and upload it as a CSV. Instead, we should create a Kernel that writes a `submission.csv` file. In turn, the `sample_submission.csv` and `test.csv` files we have access to are a placeholder with just a few examples, and they don't matter as we will see in the [next notebook](https:\/\/www.kaggle.com\/julian3833\/2-the-dataset-qa-for-qa-noobs).\n\nThe actual test set is always hidden, and our notebooks are fed with it when we decide to submit a version of them to the competition. \n\n# Internet disabled: How to use pretrained-models?\n\nIn order to be allowed to submit it, you should make sure the Internet is off:\n\n<p style=\"text-align:center\" >\n<img src=\"https:\/\/i.imgur.com\/6TeYNK6.png\" width=\"40%\"  \/>\n<\/p>\n\n(Also, since you will probably be working with transformers, you might want to turn on the GPU!)\n\nThis requirement is a measure of security, but it's quite messy when you start, because the transformers' [modelhub](https:\/\/huggingface.co\/models) relies on the Internet and using transformers is the go-to nowadays.\n\nThe common workaround here is as follows: people upload the pre-trained models as Kaggle Datasets, and you can add them with the \"+ Add data\" button at the top-right corner of the Kernel edition window:\n\n\n<p style=\"text-align:center\" >\n<img src=\"https:\/\/i.imgur.com\/l2tKMLe.png\" width=\"40%\"  \/>\n<\/p>\n\nSee, for example, these Datasets:\n* [Huggingface BERT Variants](https:\/\/www.kaggle.com\/sauravmaheshkar\/huggingface-bert-variants)\n* [Huggingface Roberta Variants](https:\/\/www.kaggle.com\/sauravmaheshkar\/huggingface-roberta-variants)\n\nThen when using your `transformers` loader, instead of specifying the model name as, let's say, `xlm-roberta-large`, we have to specify the full path to the added inputs folder, which looks like this: `..\/inputs\/huggingface-roberta-variants\/xlm-roberta-large`.\n\n\n&nbsp;\n&nbsp;\n&nbsp;\n\nFor example, the models we will review in the fourth notebook [\n4 - Exploring Public Models [QA for QA noobs]](https:\/\/www.kaggle.com\/julian3833\/4-exploring-public-models-qa-for-qa-noobs), use the following dataset, added using \"+Add data\" button we just mentioned:\n\n\n\n<p style=\"text-align:center\" >\n<img src=\"https:\/\/i.imgur.com\/T9zUJcL.png\" width=\"60%\"  \/>\n<\/p>\n\n<!--\n\nThe dataset, in turn, is just a copy of [xlm-roberta-large-squad2](https:\/\/huggingface.co\/deepset\/xlm-roberta-large-squad2) from the transformers' modelhub, but now instead of using this this token when creating the models, we will need to use a path.\n\nDon't worry if all this doesn't make sense for you now. If this doesn't make sense for you, don't worry. You can continue without this until the notebook 4 and come back later. This is quite advance for now.\n\n-->\n\nThe `5-hour runtime` will probably matter later on, when ensembles start to appear. For now, we can ignore that.\n\nThat's it for now. We reviewed the main features of the competition.\n\n## What's next?\n\nWe just gained a basic idea of what is Question Answering and what is this competition about. Let's go and take a look at the Dataset in the [next notebook](https:\/\/www.kaggle.com\/julian3833\/2-the-dataset-qa-for-qa-noobs\/)!\n\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n\n## Remember to upvote the notebook if you found it useful! \ud83e\udd17\n","ab1a5d9b":"# The competition: [chaii - Hindi and Tamil Question Answering](https:\/\/www.kaggle.com\/c\/chaii-hindi-and-tamil-question-answering)\n\n<p style=\"text-align:center\">\n<img src=\"https:\/\/i.imgur.com\/74vfye4.png\" width=\"70%\"\/>\n<\/p>\n\n\n\nThe main features of this competition are outlined below:\n\n## The task\n1. **Task:** Question Answering\n1. **Metric:** Jaccard coefficient (see the [third notebook](https:\/\/www.kaggle.com\/julian3833\/3-the-metric-jaccard-qa-for-qa-noobs) to understand the metric)\n1. **Particularity:** Non-English language. Actually, two Indian languages: `Hindi` and `Tamil`. (see the [second notebook](https:\/\/www.kaggle.com\/julian3833\/2-the-dataset-qa-for-qa-noobs) to understand the dataset)\n1. **Prize:** 10K. Weirdly shared equally as 2K for each of the top 5.\n\n&nbsp;\n&nbsp;\n&nbsp;\n\n\n## Question Answering\n\n\nWhile QA might refer to [slightly different NLP tasks](https:\/\/en.wikipedia.org\/wiki\/Question_answering), we can stick to the current mainstream use, `Extractive Question Answering`, which is the following task:\n\n\n<h2 style=\"text-align: center; background-color:#C8FF33;padding:40px;border-radius: 30px;\">\n    Given a <i>question<\/i> and a <i>context<\/i>, <b>extract<\/b> the <i>answer<\/i> to the <i>question<\/i> from the <i>context<\/i>.\n<\/h2>\n\n&nbsp;\n&nbsp;\n\n\nIn terms of inputs and outputs, we can put it this way:\n\n<h2 style=\"text-align: center;padding:10px;border-radius: 30px;\">\n    (<i>question<\/i>, <i>context<\/i>) $\\rightarrow$ <i>answer<\/i>\n\n<\/h2>\n\n\nFor example:\n<h2 style=\"text-align: center;padding:10px;\">\n    (<i>\"How old is Kevin?\"<\/i>, <i>\"Kevin just turned 28 last week.\"<\/i>) $\\rightarrow$ <i>\"28\"<\/i>\n\n<\/h2>\n\n\nNote that since this is **Extractive** Question Answering, the answer **must be contained** in the context. It should be a **substring**, not a rephrasing or a generated piece of information.\n\n<p style=\"text-align:center\">\n<img src=\"https:\/\/i.imgur.com\/TNqhXx0.png\" width=\"50%\" \/>\n<\/p>\n\n\n\n\n\nLet's see some code:\n\n\n### Example of Extractive Question Answering\n\nConsider the following text (a reduced version of the first two sentences of [Wikipedia's article](https:\/\/en.wikipedia.org\/wiki\/World_War_II) about the `Second World War`):\n\n>World War II, often abbreviated as WW2, was a global war that lasted from 1939 to 1945. It involved the vast majority of the world's countries forming two opposing military alliances: the Allies and the Axis powers.\n\nThen, the following could be some valid _inputs_ for an extractive QA system:","5e08d91e":"<img src=\"https:\/\/i.imgur.com\/RFR6UZX.jpg\" width=\"100%\"\/>\n\n\n# 1. The competition\n### [chaii - Hindi and Tamil Question Answering](https:\/\/www.kaggle.com\/c\/chaii-hindi-and-tamil-question-answering) - A quick overview for QA noobs\n\nHi and welcome! This is the first kernel of the series `chaii - Hindi and Tamil Question Answering - A quick overview for QA noobs`.\n\n**In this short kernel we will go over the competition specifics, define Question Answering and respond to some common questions regarding the submission requirements, laying the foundation for digging into the data and the public models.**\n\nThis series aims to get a good understanding of the specific topic (Question Answering for a non-English language), including going over the dataset, learning common approaches, and understanding the best models proposed by the community from both a technical and theoretical point of view. \n\nThe ideal reader is a Data Scientist noob with some general knowledge about Deep Learning, but no technical expertise in Question Answering. \n\n---\n\nThe full series consists of the following notebooks:\n1. _[The competition](https:\/\/www.kaggle.com\/julian3833\/1-the-competition-qa-for-qa-noobs) (This notebook)_\n2. [The dataset](https:\/\/www.kaggle.com\/julian3833\/2-the-dataset-qa-for-qa-noobs) \n3. [The metric (Jaccard)](https:\/\/www.kaggle.com\/julian3833\/3-the-metric-jaccard-qa-for-qa-noobs) \n4. [Exploring Public Models](https:\/\/www.kaggle.com\/julian3833\/4-exploring-public-models-qa-for-qa-noobs\/)\n5. [\ud83e\udd47 XLM-Roberta + Torch's extra data [LB: 0.749]](https:\/\/www.kaggle.com\/julian3833\/5-xlm-roberta-torch-s-extra-data-lb-0-749)\n6. [\ud83e\udd17 Pre & post processing](https:\/\/www.kaggle.com\/julian3833\/6-pre-post-processing-qa-for-qa-noobs\/)\n\n\n\nThis is an ongoing project, so expect more notebooks to be added to the series soon. Actually, we are currently working on the following ones:\n* Exploring Public Models Revisited\n* Reviewing `squad2`, `mlqa` and others\n* About `xlm-roberta-large-squad2`\n* Own improvements\n\n\n---"}}