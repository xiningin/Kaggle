{"cell_type":{"b95f8456":"code","b3990736":"code","1cbf6bf6":"code","083274b8":"code","79ff6857":"code","df6fdee6":"code","3b49e002":"code","021e1201":"code","ec4b0f40":"code","0a8de7c9":"code","a4f5c0dd":"code","ae3675bc":"code","d0d40f7b":"code","1c0d0cda":"code","f3c9b830":"code","abfc6a4f":"code","e30b7934":"code","83826a10":"code","56d396a6":"code","2a21055f":"code","8e1a1392":"code","d226b4ef":"code","ff103976":"code","67a8995a":"code","91abfe40":"code","554709c5":"code","b4bd31f7":"code","ae2a1812":"code","248f7a0a":"code","dcbab732":"code","fb213e0e":"code","ae16e615":"code","8ca82d4a":"code","1235ab60":"markdown","74b8ab19":"markdown","dd0231a3":"markdown","97b1fe39":"markdown","3575280f":"markdown","cfe2433e":"markdown","d56ef946":"markdown","d6a2baac":"markdown","ff90dce6":"markdown","eba11892":"markdown","0c042cdb":"markdown","91498801":"markdown","b828bfdf":"markdown","4b7ac9aa":"markdown","1323d2ac":"markdown"},"source":{"b95f8456":"import numpy as np\nimport pandas as pd\n\n# import matplotlib\nimport matplotlib.pyplot as plt\n# import seaborn\nimport seaborn as sns\n# settings for seaborn plotting style\nsns.set(color_codes=True)\n# settings for seaborn plot sizes\nsns.set(rc={'figure.figsize':(9.5,5)})","b3990736":"# import uniform distribution\nfrom scipy.stats import uniform\n\n# generate random numbers from a uniform distribution\nsample_size = 10000\nparam_loc = 5 # left-hand endpoint of the domain interval\nparam_scale = 10 # width of the domain interval\ndata_uniform = uniform.rvs(size=sample_size, loc=param_loc, scale=param_scale)\n\n# print a few values from the distribution:\nprint('The first 5 values from this distribution:')\nprint(data_uniform[0:5])\n\n# plot a historgram of the output\nax = sns.distplot(data_uniform,\n                  bins=100,\n                  kde_kws={\"label\": \"KDE\"},\n                  hist_kws={\"label\": \"Histogram\"})\nax.set(xlabel='x ', ylabel='Frequency', title=f'Uniform Distribution: Sample Size = {sample_size}. loc={param_loc}, scale={param_scale}');","1cbf6bf6":"from scipy.stats import norm\n\n# generate random numbers from a normal distribution\n\nsample_size = 100\nparam_loc = 3 # mean\nparam_scale = 2 # standard deviation\ndata_normal = norm.rvs(size=sample_size,loc=param_loc,scale=param_scale)\n\n# print a few values from the distribution:\nprint('The first 5 values from this distribution:')\nprint(data_normal[0:5])\n\n# plot a histogram of the output\nax = sns.distplot(data_normal,\n                  bins=100,\n                  kde_kws={\"label\": \"KDE\"},\n                  hist_kws={\"label\": \"Histogram\"})\nax.set(xlabel='x ', ylabel='Frequency', title=f'Normal Distribution: Sample Size = {sample_size}, loc={param_loc}, scale={param_scale}');","083274b8":"from scipy.stats import norm\ndata_normal = norm.rvs(size=100,loc=3,scale=2)\nprint(data_normal)","79ff6857":"# import bernoulli\nfrom scipy.stats import bernoulli\n\n# generate bernoulli data\nsample_size = 100000\nparam_p = 0.3 # probability of sucess\ndata_bern = bernoulli.rvs(size=sample_size,p=param_p)\n\n# print a few values from the distribution:\nprint('The first 5 values from this distribution:')\nprint(data_bern[0:5])\n\n# Create the Plot\nax= sns.distplot(data_bern,                  \n                  kde=False,\n                  hist_kws={\"label\": \"Histogram\"})\nax.set(xlabel='x ', ylabel='Frequency', title=f'Bernoulli Distribution: Sample Size = {sample_size}, p={param_p}');\nax.legend();","df6fdee6":"from scipy.stats import binom\n\n# Generate Binomial Data\nsample_size = 10000\nparam_n = 10 # number of trials\nparam_p = 0.7 # probability of success in one trial\ndata_binom = binom.rvs(size=sample_size, n=param_n,p=param_p,)\n\n# print a few values from the distribution:\nprint('The first 5 values from this distribution:')\nprint(data_binom[0:5])\n\n# Create the Plot\nax = sns.distplot(data_binom,\n                  kde=False,\n                  hist_kws={\"label\": \"Histogram\"})\nax.set(xlabel='x ', ylabel='Frequency', title=f'Binomial Distribution: n={param_n} ,p={param_p}')\nax.legend();","3b49e002":"from scipy.stats import poisson\n\n# Generate Poisson Data\nsample_size = 10000\nparam_mu = 3 #(rate of events per time, often denoted lambda)\ndata_poisson = poisson.rvs(size=sample_size, mu=param_mu)\n\n# print a few values from the distribution:\nprint('The first 5 values from this distribution:')\nprint(data_poisson[0:5])\n\n# Create the Plot\nax = sns.distplot(data_poisson,\n                  kde=False,\n                  hist_kws={\"label\": \"Histogram\"})\nax.set(xlabel='x ', ylabel='Frequency', title=f'Poisson Distribution: Sample Size = {sample_size}, mu={param_mu}');\nax.legend();","021e1201":"from scipy.stats import beta\n\n# Generate beta Data\nsample_size = 100000\nparam_a = 1\nparam_b = 5\ndata_beta = beta.rvs(param_a, param_b, size=sample_size)\n\n# print a few values from the distribution:\nprint('The first 5 values from this distribution:')\nprint(data_beta[0:5])\n\n# Create the Plot\nax = sns.distplot(data_beta,\n                  kde_kws={\"label\": \"KDE\"},\n                  hist_kws={\"label\": \"Histogram\"})\nax.set(xlabel='x ', ylabel='Frequency', title=f'Beta({param_a},{param_b}) Distribution: Sample Size = {sample_size}');\nax.legend();","ec4b0f40":"from scipy.stats import beta\n\n# Generate beta Data\nsample_size = 100000\nparam_a = .5\nparam_b = .5\ndata_beta = beta.rvs(param_a, param_b, size=sample_size)\n\n# print a few values from the distribution:\nprint('The first 5 values from this distribution:')\nprint(data_beta[0:5])\n\n# Create the Plot\nax = sns.distplot(data_beta,\n                  kde_kws={\"label\": \"KDE\"},\n                  hist_kws={\"label\": \"Histogram\"})\nax.set(xlabel='x ', ylabel='Frequency', title=f'Beta({param_a},{param_b}) Distribution: Sample Size = {sample_size}');\nax.legend();","0a8de7c9":"from scipy.stats import beta\n\n# Generate beta Data\nsample_size = 100000\nparam_a = 5\nparam_b = 10\ndata_beta = beta.rvs(param_a, param_b, size=sample_size)\n\n# print a few values from the distribution:\nprint('The first 5 values from this distribution:')\nprint(data_beta[0:5])\n\n# Create the Plot\nax = sns.distplot(data_beta,\n                  kde_kws={\"label\": \"KDE\"},\n                  hist_kws={\"label\": \"Histogram\"})\nax.set(xlabel='x ', ylabel='Frequency', title=f'Beta({param_a},{param_b}) Distribution: Sample Size = {sample_size}');\nax.legend();","a4f5c0dd":"from scipy.stats import gamma\n\n# Generate Gamma Data\nsample_size = 100000\nparam_a = 3 # shape parameter, sometimes denoted k or alpha\nparam_scale = 2 # this is the scale parameter theta.  Sometime this is given as rate parameter called beta, where theta=1\/beta.\ndata_gamma = gamma.rvs(size=sample_size, a=param_a, scale=param_scale)\n\n# print a few values from the distribution:\nprint('The first 5 values from this distribution:')\nprint(data_gamma[0:5])\n\n# Create the Plot\nax = sns.distplot(data_gamma,\n                  kde_kws={\"label\": \"KDE\"},\n                  hist_kws={\"label\": \"Histogram\"})\nax.set(xlabel='x ', ylabel='Frequency', title=f'Gamma Distribution: Sample Size = {sample_size}, a=k={param_a}, scale='+r'$\\theta$'+f'={param_scale}');\nax.legend();","ae3675bc":"print('Comparing the data mean to the distribution mean:')\nprint(np.mean(data_gamma))\nprint(param_a*param_scale)","d0d40f7b":"# Generate Mulitvariate Gaussian Data\nsample_size=10000\nparam_mean = [0, 2]\nparam_cov = [(1, .5), (.5, 1)]\ndata = np.random.multivariate_normal(param_mean, param_cov, size=sample_size)\n# create a data frame from the data\ndf = pd.DataFrame(data, columns=[\"x\", \"y\"])\n\n# Create the Plot\nax = sns.jointplot(x=\"x\", y=\"y\", data=df, alpha=0.25);\nax.fig.subplots_adjust(top=0.9)\nax.fig.suptitle(\"Scatterplot of Guassian Distribution Data\");\n\nax = sns.jointplot(x=\"x\", y=\"y\", data=df, kind=\"kde\");\nax.fig.subplots_adjust(top=0.9)\nax.fig.suptitle(\"Kenel Density Estimation of Gausssian Distribution Data\");\n\nax = sns.jointplot(x=\"x\", y=\"y\", data=df, kind=\"hex\", color=\"k\");\nax.fig.subplots_adjust(top=0.9)\nax.fig.suptitle(\"Hexbin Plot of Guassian Distribution Data\");\n\nf, ax = plt.subplots(figsize=(6, 6))\ncmap = sns.cubehelix_palette(as_cmap=True, dark=0, light=1, reverse=True)\nax = sns.kdeplot(df.x, df.y, cmap=cmap, n_levels=60, shade=True);\nax.set(xlabel='x ', ylabel='y', title=f'Continuous Shading Plot of Gaussian Distribution Data');","1c0d0cda":"# Generate Mulitvariate Gaussian Data\nsample_size=1000\nparam_mean = [0, 2]\nparam_cov = [(1, .5), (.5, 1)]\ndata = np.random.multivariate_normal(param_mean, param_cov, size=sample_size)\n# create a data frame from the data\ndf = pd.DataFrame(data, columns=[\"x\", \"y\"])\n\n# Create the Plot\nax = sns.jointplot(x=\"x\", y=\"y\", data=df, alpha=0.3).plot_joint(sns.kdeplot, zorder=0, n_levels=6)","f3c9b830":"print('Compute the eigenvalues and eigenvectors of the covariance to compare to the plot.')\ne = np.linalg.eig(param_cov)\nprint(f'Eigenvalues{e[0]}')\nprint(f'Eigenvectors{e[1]}')","abfc6a4f":"from scipy.stats import wishart\n\n# Generate data\nparam_df = 2\nparam_scale = np.asarray([[2,1],[1,2]])\ndata_wishart = wishart.rvs(param_df, param_scale, size=3)\n\n# Print the data\nprint(data_wishart)","e30b7934":"from scipy.stats import binom\n# set the parameters\nparam_n = 10 # number of trials\nparam_p = 0.3 # probability of success in a single trial\nk_value = 5 #NOTE: k = the number of successes\n\n# compute the probability\/likelihood\np = binom.pmf(k=k_value, n=param_n, p=param_p) \n\nprint('For the discrete binomial distribution:')\nprint(f'L(n={param_n},p={param_p}|k={k_value}) = p(k={k_value}|n={param_n},p={param_p}) = {p}')","83826a10":"\n# set the parameters\nparam_n = 10 # number of trials\nparam_p = 0.3 # probability of success in a single trial\nk_value = 5 #NOTE: k = the number of successes\n\n# compute the probability\/likelihood\np1 = binom.pmf(k=k_value, n=param_n, p=param_p)\n\n\n# set the parameters\nparam_n = 10 # number of trials\nparam_p = 0.4 # probability of success in a single trial\nk_value = 5 #NOTE: k = the number of successes\n\n# compute the probability\/likelihood\np2 = binom.pmf(k=k_value, n=param_n, p=param_p) \n\nprint(p1)\nprint(p2)\nprint(p2\/p1)","56d396a6":"from scipy.stats import norm\n# set the parameters\nparam_loc = 3 # mean\nparam_scale = 2 # standard deviation\nx_value = 5\n\n# compute the probability\/likelihood\np = norm.pdf(x=x_value, loc=param_loc,scale=param_scale)\n\nprint('For the continuous gaussian distribution:')\nprint(f'L(\\u03BC={param_loc},\\u03C3={param_scale}|x={x_value}) = f(x={x_value}|\\u03BC={param_loc},\\u03C3={param_scale}) = {p}')\n# See https:\/\/gist.github.com\/beniwohli\/765262 for the unicodes for the greek alphabet ","2a21055f":"from scipy.stats import binom\n\n# compute the likelihoods\nk_value, param_n, param_p = 21, 50, 0.5,\nL1 = binom.pmf(k=k_value, n=param_n, p=param_p) \nprint(f'L(n={param_n},p={param_p}|k={k_value}) = p(k={k_value}|n={param_n},p={param_p}) = {L1}')\nk_value, param_n, param_p = 21, 50, 0.1 \nL2 = binom.pmf(k=k_value, n=param_n, p=param_p) \nprint(f'L(n={param_n},p={param_p}|k={k_value}) = p(k={k_value}|n={param_n},p={param_p}) = {L2}')","8e1a1392":"N = L1*0.5\nprint(f'Numerator: {N}')\nD = L1*0.5 + L2*0.5\nprint(f'Denominator: {D}')\nprint(f'P(n=50,p=0.5|k=21) = {N\/D}')","d226b4ef":"# create an array of possible values for p\nx = np.arange(0, 1, 0.01)\nprint(f'x = {x}')\n\n# compute the likelihoods for each of these\nL = binom.pmf(k=21, n=50, p=x)\nprint(f'L = {L}')\n\n# compute the denominator in Bayes Theorem (i.e. the normalizing factor)\nprior_prob = 1\/len(L)\nD = np.sum(L*prior_prob)\nprint(f'D = {D}')\n\n# now compute the probability for each x-vaue using Bayes Theorem\nP= L*prior_prob \/ D\nprint(f'P={P}')\n","ff103976":"import seaborn as sns\nax = sns.scatterplot(x, P)\nax.set(xlabel='x', ylabel='P(p=x)', title=f'Posterior Probability Mass Function for p (discrete distribution, every 0.01 points)');","67a8995a":"# compute the denominator in Bayes Theorem (i.e. the normalizing factor) approximating the integral\nprior_prob = 1\/len(L)\ndelta_theta = 0.01\nD = np.sum(L*prior_prob*delta_theta)\nprint(f'D = {D}')\n\n# now compute the probability for each x-value using Bayes Theorem\nP= L*prior_prob \/ D\nprint(f'P={P}')","91abfe40":"ax = sns.lineplot(x, P)\nax.set(xlabel='x', ylabel='f(x)', title=f'Probability Density Function for p (continuous distribution)');","554709c5":"# Create the Y,Y grid\nX, Y = np.meshgrid(np.arange(-10, 10, 0.1), np.arange(0, 10, 0.1))\n\n# THIS IS JUST A PLACEHOLDER FUNCTION FOR Z.  TO ANSWER THE CHALLENGE QUESTION, YOU MUST REPLACE THIS FORMULA WITH P(x,y).\nZ = np.exp(-X**2\/50-(Y-8)**2\/20)","b4bd31f7":"plt.contour(X, Y, Z, 20, cmap='twilight_shifted');","ae2a1812":"plt.contourf(X, Y, Z, 20, cmap='RdGy')\nplt.colorbar();","248f7a0a":"contours = plt.contour(X, Y, Z, 3, colors='black')\nplt.clabel(contours, inline=True, fontsize=8)\n\nplt.imshow(Z, extent=[np.min(X), np.max(X), np.min(Y), np.max(Y)], origin='lower',\n           cmap='RdGy', alpha=0.5)\nplt.colorbar();","dcbab732":"ax = plt.axes(projection='3d')\nax.plot_wireframe(X, Y, Z, color='r');","fb213e0e":"ax = plt.axes(projection='3d');\nax.plot_surface(X, Y, Z, cmap='jet');","ae16e615":"from matplotlib import cm# Normalize the colors based on Z value\nnorm = plt.Normalize(Z.min(), Z.max())\ncolors = cm.jet(norm(Z))\nax = plt.axes(projection='3d')\nsurf = ax.plot_surface(X, Y, Z, facecolors=colors, shade=False)\nsurf.set_facecolor((0,0,0,0))","8ca82d4a":"ax = plt.axes(projection='3d')\nax.contour3D(X, Y, Z, 55, cmap='twilight_shifted');","1235ab60":"Now we can input these into our formula above to get the probability that the coin is fair.","74b8ab19":"The Wishart distribution is a little differenet because the support is the set of pxp positive definite matrices.  So we just create and view one sample, instead of a plot.","dd0231a3":"**QUESTION 2:** Suppose that we have flipped a coin 50 times and got heads 21 times. Estimate the probability distribution for the probability $p$ for getting heads of a single coin toss.  Do this first by assuming that $p$ has one of the values $\ud835\udc5d=0,\ud835\udc5d=0.01,\ud835\udc5d=0.02,...,\ud835\udc5d=0.98,\ud835\udc5d=0.99,\ud835\udc5d=1$ and has a discrete distribution, then use this to approximate the PDF for a continuous distribution for $p$.","97b1fe39":"# Probabilities, Likelihoods, and Bayes Theorem\n\n**SUMMARY:**  In this notebook we will be reiveiwing some probability distributions, showing histograms and scatterplots to visualizae the distributions in Plython in Section 1.  In Section 2 we define likelihoods, and show how to compute them in Python.  In Section 3, we explain Bayes Theorem, and show how to use Bayes Theorem to compute the posterior probability for parameters based on observed data, using the likelihoods from Section 2.","3575280f":"This cruve represents a continuous probability distribution.  Its integral, equal to the area under the curve, is 1. (or approximately 1, since this plot is a numerical approximation...)","cfe2433e":"# 3. Using Bayes Theorem\n\nBayes theorem is:\n$$\nP(\\theta | X) = \\frac{P(X|\\theta)P(\\theta)}{P(X)},\n$$\nwhere: \n* $P(\\theta | X)$ is the probability for the parameter is $\\theta$ given the observed outcom $X$.\n* $P(X|\\theta)$ the likelihood for the probability distribution.  It is standard to use the discrite case notation, in which this is the probability of getting the outcome $X$ if $\\theta$ is the underlying parameter for the distribution.\n* $P(\\theta)$ is the prior probability for $\\theta$\n* $P(X)$ is the probability of getting $X$ as an outcome.\n\nNote that if we there are only a finite number of possible values $\\{\\theta_1,\\theta_2,...,\\theta_N\\}$ for the parameter $\\theta$, then we can compute $P(X)$ by\n$$\nP(X) = P(X|\\theta_1)P(\\theta_1) + P(X|\\theta_2)P(\\theta_2) +\\cdots + P(X|\\theta_N)P(\\theta_N).\n$$\nThis is called the 'normalization' or 'conditioning' formula.  If there is a continuous set of possible $\\theta$ values, for example an interval $[a,b]$, this sum is replaced by an integral.","d56ef946":"Note that we use filled circle markers in this plot because this represents a discte distribution.  The sum of the all of the values is 1.","d6a2baac":"**ANSWER:** We are going to use the same formulas as above, but for $p=0,p=0.01,p=0.02,...,p=0.98,p=0.99,p=1$.","ff90dce6":"The binomial distribution with parameters n and p is the discrete probability distribution of the number of successes in a sequence of n independent trials.\n\nA ***probability*** is a number assigned to a possible outcome, and the sum (or integral) of the probabilities of all possible outcomes in the support of the distrubution is equal to one.\n\nA ***likelihood*** is a number that is assigned to a hypothesis about the underlying paraemters for a distribution.  \n* For discrete random variables, the likelihood of the parameters $\\theta$ given the outcome $X$, written $L(\\theta|X)$, is equal to the probability mass function (PMF) for this outcome given these parameters, $L(\\theta|X)=P(X|\\theta)$. \n* For continuous random variables, the likelihood of the parameters $\\theta$ given the outcome $X$, written $L(\\theta|X)$, is equal to the probability desnity function (PDF) for this outcome given these parameters, $L(\\theta|X)=f(X|\\theta)$.\n\nIn summary, the likelihood is a function of the parameters (assuming the outcome is fixed) and the probability (mass\/density) function is a funciton of the outcome, assuming the parameters for the distribution are fixed.  But, both are equal to the probability mass\/density function for the given outcome and parameters.","eba11892":"**ANSWER**:  Notice that this question is really asking about the underlying parameter for the probability distribution for the coin flip: is p=0.5 or is p=0.1?  So we are going to use Bayes theorem to compute these two probabilities.\n\nTo start, we are going to be using the binomial probability distribution - for given parameter values for $n$ flips of a coin that has probability $p$ of getting heads, the probability probability of getting $k$ heads is:\n$$\nP(k|n,p)={n \\choose k}p^k(1-p)^{n-k}.\n$$\n\nFor our given problem:\n* In Bayes Theorem, $\\theta$ represents the parameter(s) for the distribution, which in this case is $n=50, p=0.5$ or $n=50, p=0.1$.\n* In Bayes Theorem, $X$ represented the observed outcome, which in this case is $k=21$ heads out of our coin flips.\n* We are not told any information about the prior probability, we are going to assume the possible parameters have the same prio probability, $P(n=50, p=0.5)=P(n=50, p=0.1)=0.5$.\n\nBayes Theorem is then:\n$$\nP(n=50, p=0.5|k=21)=\\frac{P(k=21|n=50, p=0.5)P(n=50, p=0.5)}{P(k=21)}.\n$$\n\nNow, using the fact that the prior probability is 0.5 and using the normlization formula:\n$$\nP(n=50, p=0.5|k=21)=\\frac{P(k=21|n=50, p=0.5)0.5}{P(k=21|n=50, p=0.1)0.5+P(k=21|n=50, p=0.5)0.5}.\n$$\n\nSo all we really nedd to compute are the likelihoods $P(k=21|n=50, p=0.1)$ and $P(k=21|n=50, p=0.5)$.","0c042cdb":"# 2. In this section, we will compute some *probabilities* and *likelihoods*.","91498801":"# 4. CHALLENGE QUESTION\n\nHere is a challenge question if you want to test your understanding, and see if you can extend the use of Bayes Theorem above to a 2D distribution.\n\nSuppose you have a single observation $x=4.2$ and you believe it was generated by a normal distribution.  Estimate the probability distribution for the parameters $\\mu$ and $\\sigma$ of the normal distribution which generated $x=4.2$. (Since we are estimating a distribution for $\\mu$ and $\\sigma$, this is called a joint distribution, but the principles are the same, just in 2-dimensions.)  \n\nDo this first by computing $P(\\mu=x, \\sigma=y)$ for all values of $(x,y)$ in a grid with $x$ in the interval $[-10,10]$ and y in the interval $[0,10]$ with a stepsize of $0.1$.   Assume that the distribution is continuous, so you have to normalize\/condition by an integral.  The Python Code for generating the X and Y values is provided below along with some plotting code.  You just have to compute the formula for $Z = P(\\mu=x, \\sigma=y)$.","b828bfdf":"To compute the PDF for the continuous case, we will change the normalizing\/conditioning constant to an integral:\n$$\nP(X)=\\int_0^1 P(\\theta|X)P(\\theta)d\\theta \\approx \\sum_i P(\\theta_i|X) P(\\theta_i)\\Delta \\theta\n$$\n","4b7ac9aa":"# 1. In this section we generate samples from a variety of distributions and use seaborn and matplotlib to plot the resulting data.","1323d2ac":"**QUESTION 1:** Suppose that we have a coin that is either fair (p=0.5 where p is the probaiblity of getting heads) or unfiar with p=0.1.  We have flipped a coin 50 times and got heads 21 times.  What is the probability that this coin is fair?"}}