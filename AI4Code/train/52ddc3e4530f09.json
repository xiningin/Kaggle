{"cell_type":{"348a7648":"code","89e78185":"code","dd6299e1":"code","d0531274":"code","59a1e4d9":"code","97ab6ae5":"code","a38953c0":"code","cca48e19":"code","3acf7657":"code","39138673":"code","433c3a58":"code","2a3f6c34":"code","da96af56":"code","17dcb43f":"code","bf867fcc":"code","ef9da731":"code","a2bedccb":"markdown"},"source":{"348a7648":"from tensorflow.python.keras.datasets import imdb","89e78185":"#We will create 4 arrays which will be populated by load_data function.\n(x_train,y_train),(x_test,y_test)=imdb.load_data(num_words=10000)   #This means that only the top 10000 from the Bag of words will be used and the rest will be ignored.","dd6299e1":"x_train[0] #We see that we have a review at 0'th index already converted to its numeric representation","d0531274":"y_train[0] #Its an output variable which is positive or negative review","59a1e4d9":"word_index=imdb.get_word_index()\nprint(word_index[\"love\"])","97ab6ae5":"reverse_word_index=dict((value,key)for key,value in word_index.items())\n\ndef decode(review):\n    text=\"\"\n    for i in review:\n        text += reverse_word_index[i]\n        text +=\" \"\n    return text\n        \n    ","a38953c0":"decode (x_train[0])","cca48e19":"def show_lengths():\n    print('Length of 1st training example: ', len(x_train[0]))\n    print('Length of 2nd training example: ',  len(x_train[1]))\n    print('Length of 1st test example: ', len(x_test[0]))\n    print('Length of 2nd test example: ',  len(x_test[1]))\n    \nshow_lengths()","3acf7657":"from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n\nx_train = pad_sequences(x_train, value = word_index['the'], padding = 'post', maxlen = 256)\nx_test = pad_sequences(x_test, value = word_index['the'], padding = 'post', maxlen = 256)","39138673":"show_lengths()","433c3a58":"decode(x_train[0])","2a3f6c34":"from tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Embedding, Dense, GlobalAveragePooling1D\n\nmodel = Sequential([\n    Embedding(10000, 16),             #First layer, vocab size=10,000 & 16 is no. of features\n    GlobalAveragePooling1D(),         #16 dimension vector for each batch\n    Dense(16, activation = 'relu'),\n    Dense(1, activation = 'sigmoid')  #output layer, sigmoid is for binary classification\n])\n\nmodel.compile(\n    optimizer = 'adam',\n    loss = 'binary_crossentropy',\n    metrics = ['acc']\n)\n\nmodel.summary()","da96af56":"from tensorflow.python.keras.callbacks import LambdaCallback\n\nsimple_logging = LambdaCallback(on_epoch_end = lambda e, l: print(e, end='.'))\n\nE = 20\n\nh = model.fit(\n    x_train, y_train,\n    validation_split = 0.2,\n    epochs = E,\n    callbacks = [simple_logging],\n    verbose = False\n)","17dcb43f":"import matplotlib.pyplot as plt\n%matplotlib inline","bf867fcc":"plt.plot(range(E),h.history['acc'],label='Training')\nplt.plot(range(E),h.history['val_acc'],label='Validation')\nplt.legend()\nplt.show()","ef9da731":"loss,acc=model.evaluate(x_test,y_test)\nprint(acc*100)","a2bedccb":"We will be using IMBD dataset which comes packaged with Keras & has already been preprocessed:\n\nThis is a binary classification problem where we want to classify the review as either Positive or Negative.\n\nWe have approx 50,000 movie reviews in IMDB dataset. We will put 25k in Training set & 25K in Test set.\nEach review is a text of different length. \nWe collect all the unique words present in the reviews and called that Bag of words(BOW).\n& then we assign each word with a numeric representation "}}