{"cell_type":{"986f529c":"code","d1f01f66":"code","f72fbed1":"code","112c6e80":"code","d69c8dc3":"code","d4bb2987":"code","23af133a":"code","28caefcd":"code","02a96cdf":"code","c35afad9":"code","ad5f8dc5":"code","a304b701":"code","cbc64697":"code","f29fafa5":"code","97f204d2":"code","587331ef":"code","88ec76b9":"code","6dfc6c57":"code","c2b46dd1":"code","6521a8b9":"code","c63a4b1b":"code","fe18253e":"code","d94fc437":"code","e72eed4b":"code","5ddfd42b":"code","8659de72":"markdown"},"source":{"986f529c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d1f01f66":"from tqdm import tqdm\nimport torch\nfrom torch.utils.data import DataLoader\nfrom datasets import load_metric\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm.auto import tqdm\nimport pickle\nfrom sklearn.metrics import accuracy_score, roc_curve, auc\nimport torch.nn.functional as F\n","f72fbed1":"file_location='..\/input\/multilingualabusivecomment\/ShareChat-IndoML-Datathon-NSFW-CommentChallenge_Train.csv'\nfile=open(file_location,'r').read()\ntotal_lines=len(file.split('\\n'))\nlabel=[]\ntext=[]\nid=[]\nwith open(file_location,'r') as file:\n    for line in tqdm(file,total=total_lines-1): \n        tokens=line.split(',')\n        label.append(tokens[-1])\n        id_text=tokens[:2]\n        id.append(id_text[0])\n        text.append(id_text[1].rstrip('\\n'))\ndf=pd.DataFrame()\nlabels=[i.strip('\\n')[-1] for i in label]\ndf['id']=id[1:]\ndf['text']=text[1:]\ndf['label']=labels[1:]\npd.set_option('display.max_colwidth',-1)\ndf.head()\n\n    ","112c6e80":"df=df.astype(dtype={\"id\":int, \"text\":str,'label':int})\ndf.info()\ndf['label'].unique()\n","d69c8dc3":"def fast_encode(texts, tokenizer, chunk_size=256, maxlen=128):\n    \n    input_ids = []\n    tt_ids = []\n    at_ids = []\n    \n    for i in tqdm(range(0, len(texts), chunk_size)):\n        text_chunk = texts[i:i+chunk_size]\n        encs = tokenizer(\n                    text_chunk,\n                    max_length = 128,\n                    padding='max_length',\n                    truncation=True\n                    )\n        \n        input_ids.extend(encs['input_ids'])\n        tt_ids.extend(encs['token_type_ids'])\n        at_ids.extend(encs['attention_mask'])\n    \n    return {'input_ids': input_ids, 'token_type_ids': tt_ids, 'attention_mask':at_ids}","d4bb2987":"X=df['text']\ny=df['label']\nX_train,X_val,y_train,y_val=train_test_split(X,y,test_size=0.2,random_state=5)\ntokenizer = AutoTokenizer.from_pretrained(\"google\/muril-base-cased\")\ntrain_encoding = fast_encode(list(X_train.values), tokenizer)\nval_encoding = fast_encode(list(X_val.values), tokenizer)\n","23af133a":"class TwitterDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        #item=defaultdict(dict)\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\ntrain_dataset = TwitterDataset(train_encoding, list(y_train))\nval_dataset = TwitterDataset(val_encoding, list(y_val))\ndel train_encoding\ndel val_encoding\ndel X,y, X_train","28caefcd":"import torch.nn as nn\nclass MurilClassifier(nn.Module):\n    \"\"\"Bert Model for Classification Tasks.\n    \"\"\"\n    def __init__(self, freeze_bert=False):\n        \"\"\"\n        @param    bert: a BertModel object\n        @param    classifier: a torch.nn.Module classifier\n        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n        \"\"\"\n        super(MurilClassifier, self).__init__()\n        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n        D_in, H, D_out = 197285, 128, 2\n        #D_in, H, D_out = 1536, 128, 2\n\n        # Instantiate BERT model\n        #self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.muril=AutoModelForMaskedLM.from_pretrained(\"google\/muril-base-cased\")\n\n        # Instantiate an one-layer feed-forward classifier\n        self.classifier = nn.Sequential(\n            nn.Linear(D_in, H),\n            nn.ReLU(),\n            #nn.Dropout(0.5),\n            nn.Linear(H, D_out)\n        )\n\n        # Freeze the BERT model\n        if freeze_bert:\n            for param in self.muril.parameters():\n                param.requires_grad = False\n        \n    def forward(self, input_ids, attention_mask):\n        \"\"\"\n        Feed input to BERT and the classifier to compute logits.\n        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n                      max_length)\n        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n                      information with shape (batch_size, max_length)\n        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n                      num_labels)\n        \"\"\"\n        # Feed input to BERT\n        outputs = self.muril(input_ids=input_ids,\n                            attention_mask=attention_mask)\n        \n        # Extract the last hidden state of the token `[CLS]` for classification task\n        last_hidden_state_cls = outputs[0][:, 0, :]\n\n        # Feed input to classifier to compute logits\n        logits = self.classifier(last_hidden_state_cls)\n\n        return logits","02a96cdf":"from transformers import get_scheduler,AdamW\n#from transformers BertForSequenceClassification\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n\n#model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n#model = AutoModelForMaskedLM.from_pretrained(\"google\/muril-base-cased\")\nmodel=MurilClassifier(freeze_bert=False)\nmodel.to(device)\nmodel.train()\n\nbatch_size=32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader=DataLoader(val_dataset,batch_size=batch_size)\nloss_fn = nn.CrossEntropyLoss()\n\noptim = AdamW(model.parameters(), lr=5e-5)\n\nnum_epochs = 3\nnum_training_steps = num_epochs * len(train_loader)\n#progress_bar = tqdm(range(num_training_steps))\n\nlr_scheduler = get_scheduler(\n    \"linear\",\n    optimizer=optim,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps\n)\n\n","c35afad9":"def train(epoch):\n    print(\"Epoch:\",epoch)\n    total_loss=0\n    with tqdm(train_loader, unit=\"batch\") as tepoch:\n        for batch in tepoch:\n            outputs=[]\n            tepoch.set_description(f\"Epoch {epoch}\")\n            input_ids = batch['input_ids'].to(device)\n            #print('input id size',len(input_ids))\n            attention_mask = batch['attention_mask'].to(device)\n            #print('Attention MAsk Size',len(attention_mask))\n            labels = batch['labels'].to(device)\n            #print('Label Size',len(labels))\n            #outputs = model(input_ids, attention_mask=attention_mask, labels=labels.unsqueeze(1))\n            logits=model(input_ids,attention_mask)\n            loss=loss_fn(logits,labels)  \n            total_loss+=loss.item()\n            #loss = outputs[0]\n            #print(\"Training Loss\",loss.item())\n            loss.backward()\n            optim.step()\n            lr_scheduler.step()\n            optim.zero_grad()\n            del input_ids\n            del attention_mask\n            del labels\n            tepoch.set_postfix(loss=loss.item())\n    print(\"Training Loss:\",total_loss\/batch_size)\n","ad5f8dc5":"best_acc=0\ndef test(epoch):\n    global best_acc\n    model.eval()\n    true_prediction=0\n    total_sample_size=0  \n    metric = load_metric(\"accuracy\")\n    with tqdm(test_loader, unit=\"batch\") as tepoch:\n        for batch in tepoch:\n            labels = batch['labels'].to(device)\n            input_ids = batch['input_ids'].to(device)            \n            attention_mask=batch['attention_mask'].to(device)\n            #batch = {k: v.to(device) for k, v in batch.items()}\n            with torch.no_grad():\n                #out=model(**batch)\n                logits=model(input_ids,attention_mask)\n            l=loss_fn(logits,labels)\n            #l=out.loss\n            #prediction=torch.argmax(out.logits,dim=1)\n            #metric.add_batch(predictions=prediction, references=batch[\"labels\"])  \n            prediction=torch.argmax(logits,dim=1)\n            metric.add_batch(predictions=prediction, references=labels)\n            tepoch.set_postfix(loss=l.item())\n    accuracy=metric.compute()['accuracy']\n    print(\"Test Accuracy:\",accuracy)\n     # Save checkpoint for the model which yields best accuracy\n    if accuracy>=best_acc:\n        print(\"Saving checkpoint with accuracy = \",accuracy)\n        best_acc=accuracy\n        torch.save({\n            'epoch':epoch,\n            'model_state_dict':model.state_dict(),\n            'optimizer_state_dict':optim.state_dict(),\n            'loss':l,\n            'accuracy':best_acc\n        },'ckpt.pth')","a304b701":"def train_test(epoch):\n    best_acc=0\n    for i in range(1,epoch+1):\n        print(\"Training\")\n        train(i)\n        print(\"Testing\")\n        test(i)\n    print(\"Training ENDS\")","cbc64697":"train_test(num_epochs)","f29fafa5":"#model.load_state_dict(checkpoint['model_state_dict'])","97f204d2":"import re\ndef read_csv(path:str):\n    file = open(path, \"r\").read()\n    ix = []\n    ctx = []\n\n    for row in file.split(\"\\n\"):\n        l = re.sub(',(?!(?=[^\"]*\"[^\"]*(?:\"[^\"]*\"[^\"]*)*$))', \"\\t\", row)\n        try:\n            lk = l.split(\"\\t\")\n            if len(lk)>2 and len(lk[0])<6:\n                p,q= lk[0], lk[1]\n                ix.append(p)\n                ctx.append(q)\n            elif len(lk)==1 and lk[0]!='':\n                #print(lk)\n                p=lk[0].split(',')[0].rstrip('\\n')\n                q=lk[0].split(',')[0].rstrip('\\n')\n                ix.append(p)\n                ctx.append(q)    \n            \n            else:\n                lk=row.replace('\"', \" \")\n                lk=lk.split(\",\")\n                p,q = lk[0], lk[1]\n                ix.append(p)\n                ctx.append(q)\n        except Exception as e:\n            print(\"Exception occurred!.\", e)\n            print(f\"Length of ids obtained: {len(ix)}, and text: {len(ctx)}\")\n\n    df = pd.DataFrame()\n    df[\"CommentId\"]=ix[1:]\n    df[\"commentText\"]=ctx[1:]\n    df = df.astype(dtype={\"CommentId\":int, \"commentText\":str})\n    return df\n\n%time df = read_csv(\"..\/input\/multilingualabusivecomment\/ShareChat-IndoML-Datathon-NSFW-CommentChallenge_Test_20_Percent_NoLabel.csv\")\ndf.head()","587331ef":"df.tail()","88ec76b9":"len(df['CommentId'].unique())","6dfc6c57":"df.info()","c2b46dd1":"class TestDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings,length):\n        self.encodings = encodings\n        self.length=length\n        \n    def __getitem__(self, idx):\n        #item=defaultdict(dict)\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        return item\n    def __len__(self):\n        return self.length","6521a8b9":"def predict(model,test_loader):\n    model.eval()\n    all_prediction=[]\n    with tqdm(test_loader, unit=\"batch\") as tepoch:\n        for batch in tepoch:\n            input_ids = batch['input_ids'].to(device)            \n            attention_mask=batch['attention_mask'].to(device)\n            with torch.no_grad():\n                logits=model(input_ids,attention_mask)\n            prediction=torch.argmax(logits,dim=1)\n            all_prediction.append(prediction)\n    return [item.item() for sublist in all_prediction for item in sublist]\n          ","c63a4b1b":"X_test=df['commentText']\ntest_encoding = fast_encode(list(X_test.values), tokenizer)\ntest_dataset = TestDataset(test_encoding,len(df['CommentId'].unique()))\nprint(len(test_dataset))\ntest_loader=DataLoader(test_dataset,batch_size=batch_size)\nprediction = predict(model, test_loader)\n","fe18253e":"submission=pd.DataFrame()\nsubmission['commentId']=df['CommentId']\nsubmission['Label']=prediction","d94fc437":"submission.head()\n","e72eed4b":"submission.to_csv('Submission.csv',index=False,header=True)","5ddfd42b":"len(submission)","8659de72":"# **Embeddings That Can be Used**\n(a). https:\/\/www.cfilt.iitb.ac.in\/~diptesh\/embeddings\/README.md\n1. as - Assamese\n2. bn - Bengali\n3. gu - Gujarati\n4. hi - Hindi\n5. kn - Kannada\n6. ko - Konkani\n8. ml - Malayalam\n9. mr - Marathi\n10. ne - Nepali\n11. pa - Punjabi\n12. sa - Sanskrit\n13. ta - Tamil\n14. te - Telugu\n\n(b). MuRIL\n<ol>\n    <li>Assamese<\/li>\n    <li>Bengali<\/li>\n    <li>English<\/li>\n    <li>Gujarati<\/li>\n    <li>Hindi<\/li>\n    <li>Kannada<\/li>\n    <li>Kashmiri<\/li>\n    <li>Malayalam<\/li>\n    <li>Marathi<\/li>\n    <li>Nepali<\/li>\n    <li>Oriya<\/li>\n    <li>Punjabi<\/li>\n    <li>Sanskrit<\/li>\n    <li>Sindhi<\/li>\n    <li>Tamil<\/li>\n    <li>Telugu<\/li>\n    <li>Urdu<\/li>\n    <\/ol>\n"}}