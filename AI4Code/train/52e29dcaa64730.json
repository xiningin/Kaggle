{"cell_type":{"0e596f52":"code","b59c960e":"code","0750c72a":"code","017cd9a0":"code","29d92489":"code","f73faa99":"code","825f0585":"code","a5f1f848":"code","e85776ac":"code","85943ad4":"code","b479e424":"code","ab35a1fb":"code","dc6eb25a":"code","ce8ecf86":"code","196a6a9c":"code","fe68ae97":"code","79787cbb":"code","c7318819":"code","a7df8629":"code","0031ffc9":"code","b79d99c9":"code","5baf513e":"code","44abd742":"code","168a28bb":"code","0387208a":"code","94da6b80":"code","ef6632d8":"code","606ff2f6":"code","d8181e62":"code","3941677f":"code","7289acf2":"code","d9862db9":"code","07c2268f":"code","377270a7":"code","1f82952a":"code","5ff16389":"code","5347844a":"code","25107f4b":"code","2a260efe":"code","85a823ed":"code","c8d2db97":"code","4bbc2e4f":"code","d7608743":"code","6dd0d688":"code","0dcebde8":"code","7fab014a":"code","81a9bc8c":"code","9568350d":"code","8dd407fc":"code","d4d7f244":"code","aa001115":"code","14ae9cdb":"code","c8471b56":"code","3f9f1cc0":"code","4611403b":"code","c8a47b81":"code","10efd3cb":"code","c1c1cd28":"code","4916e009":"code","2948af3d":"code","fdb575ed":"code","dadcaa23":"code","167204a8":"code","372076c7":"code","61b2ea21":"code","7a77f502":"code","c572a074":"code","cd070259":"code","8687b621":"code","77a0aae4":"code","015b562c":"code","3bb6e196":"code","b188f925":"code","a45f03ed":"code","4f5c2b68":"code","699c44e2":"code","a2bebf40":"code","02055d4e":"code","a92afb88":"code","fd270ed5":"code","7a8f93ba":"markdown","3f776839":"markdown","6ec379f7":"markdown","42216993":"markdown","042e736d":"markdown","3fc8f502":"markdown","c46696ea":"markdown","a9a7eff1":"markdown","ae2bf144":"markdown","0e01724b":"markdown","b08a59bb":"markdown","b78ecc96":"markdown","5fa332da":"markdown","724b0512":"markdown","d7e21a83":"markdown","478713a0":"markdown","52b83bfc":"markdown","6868e0a8":"markdown","80094263":"markdown","272d9366":"markdown","e0d7a44c":"markdown"},"source":{"0e596f52":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b59c960e":"import warnings\nwarnings.filterwarnings(\"ignore\")","0750c72a":"#importing visualization library\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","017cd9a0":"#Reading the data\ndf = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')\n","29d92489":"df.head(10)","f73faa99":"df.info()","825f0585":"#checking different descriptive statistics of data\ndf.describe()","a5f1f848":"df.dtypes\n# all variables are of float type","e85776ac":"df.isnull().sum().max()\n#no null values","85943ad4":"#checking distribution of class\nfraud = round(len(df[df['Class']==1])\/len(df) *100,2)\nnofraud = round(len(df[df['Class']==0])\/len(df) *100,2)\nprint(\"No fraud transaction is:\", str(nofraud)+'%' ,\"of the dataset\")\nprint(\"fraud transaction is:\", str(fraud)+'%' ,\"of the dataset\")","b479e424":"#CLASS VARIABLE\nsns.countplot(x='Class',data=df)\nplt.title('0:No Fraud  1:Fraud transaction')","ab35a1fb":"# AMOUNT\namount_val = df['Amount'].values\nplt.figure(figsize = (12,6))\nsns.distplot(df['Amount'])\nplt.xlim(min(amount_val),max(amount_val))\nplt.title('Distribution of amount')","dc6eb25a":"time_val = df['Time'].values\nplt.figure(figsize = (12,6))\nsns.distplot(df['Time'])\nplt.xlim(min(time_val),max(time_val))\nplt.title('Distribution of Transaction time')","ce8ecf86":"# DISTRIBUTION OF V'S\n'''fig, ax = plt.subplots(nrows = 7,ncols = 4,figsize=(16,24))\n\nfor i in range(1,29):\n    m = (i-1)\/\/4\n    n = (i-1)%4\n    col = 'V' + str(i)\n    ax[m,n].hist(df[col],bins=40)\n    ax[m,n].set_title(col)\n    ax[m,n].vlines(x = df[col].mean(),ymin =0,ymax = 10**3,linestyle = 'dashed',colors = 'g')\n    ax[m,n].set_yscale('log')'''","196a6a9c":"df.describe()['Amount']","fe68ae97":"#CLASS VS AMOUNT\nsns.boxplot(x='Class',y='Amount',data=df)\nplt.title('Amount vs Class')\n#There are very less points having Amount > 10,000.\n#Therefore these values should be excluded from dataset","79787cbb":"\nlen(df[df['Amount']>2500])","c7318819":"(abs(284358-284807)\/284807)*100","a7df8629":"df = df[df['Amount']<2500]\ndf.describe()","0031ffc9":"sns.boxplot(x='Class',y='Amount',data=df)\nplt.title('Amount vs Class')\n# now no transaction of higher amount is detected","b79d99c9":"len(df[df['Class']==1])\n#no fraud data is deleted or affected only non fraud transacton which were of high amount are deleted","5baf513e":"#Amount and Time Distribution\n# during which hour of day or which minute of hour transaction is happening\ndata_new = df\ntimedelta = pd.to_timedelta(data_new['Time'], unit='s')\n#new variable for further analysis\ndata_new['Time_min'] = (timedelta.dt.components.minutes).astype(int)\n#new variable for further analysis\ndata_new['Time_hour'] = (timedelta.dt.components.hours).astype(int)","44abd742":"data_new['Time_min'].unique()","168a28bb":"data_new['Time_hour'].unique()","0387208a":"df.groupby(by='Time_min').count()['Class']","94da6b80":"df.groupby(by='Time_hour').count()['Class']","ef6632d8":"''''ax = sns.lmplot(y=\"Amount\", x=\"Time_min\", fit_reg=False, aspect=2.5, data=data_new, hue='Class')\nplt.title(\"Amounts by Minutes of Frauds and Normal Transactions\",fontsize=12)'''","606ff2f6":"ax = sns.lmplot(y=\"Amount\", x=\"Time_hour\", fit_reg=False, aspect=2.5, data=data_new, hue='Class')\nplt.title(\"Amounts by hours of Frauds and Normal Transactions\",fontsize=12)\n#plt.savefig('Amount_VS_Time_Scatter.png')","d8181e62":"plt.figure(figsize=(15,8))\n# Non-Fraudulent Transactions over Time (in hr) - GREEN\nsns.distplot(data_new[data_new['Class'] == 0][\"Time_hour\"],\n             color='g')\n# Fraudulent Transactions over time (in hr) - RED\nsns.distplot(data_new[data_new['Class'] == 1][\"Time_hour\"],\n             color='r')\nplt.title('Fraud x Normal Transactions by Hours (Red: Fraud; Green:Normal)', fontsize=14)\nplt.xlim([-1,25])","3941677f":"''''plt.figure(figsize=(20,6))\n# Non-Fraudulent Transactions over Time (in min) - GREEN\nsns.distplot(data_new[data_new['Class'] == 0][\"Time_min\"],\n             color='g')\n# Fraudulent Transactions over time (in hr) - RED\nsns.distplot(data_new[data_new['Class'] == 1][\"Time_min\"],\n             color='r')\nplt.title('Fraud x Normal Transactions by min (Red: Fraud; Green:Normal)', fontsize=12)\nplt.xlim([-1,60])\n# in every minute approximate same % of fraud and non fraud transaction happens'''","7289acf2":"from sklearn.preprocessing import RobustScaler\nrob_scaler = RobustScaler()\n\ndf['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\n\ndf.drop(['Time','Amount'], axis=1, inplace=True)\n","d9862db9":"df.drop(['Time_min', 'Time_hour'],axis=1,inplace =True)","07c2268f":"#inserting these scaled columns at 0,1\nscaled_amount = df['scaled_amount']\n\ndf.drop(['scaled_amount'], axis=1, inplace=True)\ndf.insert(0, 'scaled_amount', scaled_amount)\n\ndf.head()","377270a7":"df.describe()['scaled_amount']","1f82952a":"from sklearn.model_selection import StratifiedKFold\n# getting test data for our model evaluation in future\n\n\nX = df.drop('Class', axis=1)\ny = df['Class']\n\nsss = StratifiedKFold(n_splits=5, random_state=1234, shuffle=False)\n\nfor train_index, test_index in sss.split(X, y):\n    print(\"Train:\", train_index, \"Test:\", test_index)\n    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]\n# We already have X_train and y_train for undersample data thats why I am using original to distinguish and to not overwrite these variables.\n\n# Turn into an array\n''''original_Xtrain = original_Xtrain.values\noriginal_Xtest = original_Xtest.values\noriginal_ytrain = original_ytrain.values\noriginal_ytest = original_ytest.values'''\n\n# See if both the train and test label distribution are similarly distributed\ntrain_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=True)\ntest_unique_label, test_counts_label = np.unique(original_ytest, return_counts=True)\nprint('-' * 100)\n\n\nprint('Label Distributions: \\n')\nprint(train_counts_label\/ len(original_ytrain))\nprint(test_counts_label\/ len(original_ytest))","5ff16389":"original_Xtrain.loc[df['Class']==1].shape","5347844a":"df2 = original_Xtrain\ny = original_ytrain","25107f4b":"df2['Class'] = y","2a260efe":"df2.loc[df['Class'] == 1]\n# getting fraud data","85a823ed":"# Since our classes are highly skewed we should make them equivalent in order to have a equal distribution of the classes.\n\n# Lets shuffle the data before creating the subsamples\n\ndf2 = df2.sample(frac=1)\n\n# amount of fraud classes 492 rows.\nfraud_df = df2.loc[df['Class'] == 1]\nnon_fraud_df = df2.loc[df['Class'] == 0][:394]\n\nnormal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n\n# Shuffle dataframe rows\nnew_df = normal_distributed_df.sample(frac=1, random_state=42)\n\nnew_df.head()","c8d2db97":"print('Distribution of the Classes in the subsample dataset')\nprint(new_df['Class'].value_counts()\/len(new_df))\n\n\n\nsns.countplot('Class', data=new_df)\nplt.title('Equally Distributed Classes', fontsize=14)\nplt.show()","4bbc2e4f":"f, ax = plt.subplots(1, 1, figsize=(24,10))\n\n# comparing correlation between dataset\n# Entire DataFrame\n''''corr = df.corr()\nsns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax1)\nax1.set_title(\"Imbalanced Correlation Matrix \\n\", fontsize=14)'''\n\n# new_df\nsub_sample_corr = new_df.corr()\nsns.heatmap(sub_sample_corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax)\nax.set_title('Balanced Data Correlation Matrix \\n (use for reference)', fontsize=14)","d7608743":"corr = new_df.corr()\ncorr[['Class']]\ncorr[corr.Class<-0.6]['Class']","6dd0d688":"corr = new_df.corr()\ncorr[['Class']]\ncorr[corr.Class>0.5]['Class']","0dcebde8":"f, axes = plt.subplots(ncols=3, figsize=(20,4))\n\n# Negative Correlations with our Class (The lower our feature value the more likely it will be a fraud transaction)\n\nsns.boxplot(x=\"Class\", y=\"V14\", data=new_df, ax=axes[0])\naxes[0].set_title('V14 vs Class Negative Correlation')\n\n\nsns.boxplot(x=\"Class\", y=\"V12\", data=new_df,  ax=axes[1])\naxes[1].set_title('V12 vs Class Negative Correlation')\n\n\nsns.boxplot(x=\"Class\", y=\"V10\", data=new_df, ax=axes[2])\naxes[2].set_title('V10 vs Class Negative Correlation')\n","7fab014a":"f, axes = plt.subplots(ncols=2, figsize=(20,4))\n\n# Positive correlations (The higher the feature the probability increases that it will be a fraud transaction)\nsns.boxplot(x=\"Class\", y=\"V11\", data=new_df,  ax=axes[0])\naxes[0].set_title('V11 vs Class Positive Correlation')\n\nsns.boxplot(x=\"Class\", y=\"V4\", data=new_df, ax=axes[1])\naxes[1].set_title('V4 vs Class Positive Correlation')","81a9bc8c":"# visualization of features from which we are gonna eliminate outliers\nfrom scipy.stats import norm\n\nf, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20, 6))\n\nv14_fraud_dist = new_df['V14'].loc[new_df['Class'] == 1].values\nsns.distplot(v14_fraud_dist,ax=ax1, fit=norm, color='#FB8861')\nax1.set_title('V14 Distribution \\n (Fraud Transactions)', fontsize=14)\n\nv12_fraud_dist = new_df['V12'].loc[new_df['Class'] == 1].values\nsns.distplot(v12_fraud_dist,ax=ax2, fit=norm, color='#56F9BB')\nax2.set_title('V12 Distribution \\n (Fraud Transactions)', fontsize=14)\n\n\nv10_fraud_dist = new_df['V10'].loc[new_df['Class'] == 1].values\nsns.distplot(v10_fraud_dist,ax=ax3, fit=norm, color='#C5B3F9')\nax3.set_title('V10 Distribution \\n (Fraud Transactions)', fontsize=14)","9568350d":"\n# # -----> V14 Removing Outliers (Highest Negative Correlated with Labels)\nv14_fraud = new_df['V14'].loc[new_df['Class'] == 1].values\nq25, q75 = np.percentile(v14_fraud, 25), np.percentile(v14_fraud, 75)\nprint('Quartile 25: {} | Quartile 75: {}'.format(q25, q75))\nv14_iqr = q75 - q25\nprint('iqr: {}'.format(v14_iqr))\n\nv14_cut_off = v14_iqr * 1.5\nv14_lower, v14_upper = q25 - v14_cut_off, q75 + v14_cut_off\nprint('Cut Off: {}'.format(v14_cut_off))\nprint('V14 Lower: {}'.format(v14_lower))\nprint('V14 Upper: {}'.format(v14_upper))\n\noutliers = [x for x in v14_fraud if x < v14_lower or x > v14_upper]\nprint('Feature V14 Outliers for Fraud Cases: {}'.format(len(outliers)))\nprint('V10 outliers:{}'.format(outliers))\n\nnew_df = new_df.drop(new_df[(new_df['V14'] > v14_upper) | (new_df['V14'] < v14_lower)].index)\nprint('----' * 44)\n\n# -----> V12 removing outliers from fraud transactions\nv12_fraud = new_df['V12'].loc[new_df['Class'] == 1].values\nq25, q75 = np.percentile(v12_fraud, 25), np.percentile(v12_fraud, 75)\nv12_iqr = q75 - q25\n\nv12_cut_off = v12_iqr * 1.5\nv12_lower, v12_upper = q25 - v12_cut_off, q75 + v12_cut_off\nprint('V12 Lower: {}'.format(v12_lower))\nprint('V12 Upper: {}'.format(v12_upper))\noutliers = [x for x in v12_fraud if x < v12_lower or x > v12_upper]\nprint('V12 outliers: {}'.format(outliers))\nprint('Feature V12 Outliers for Fraud Cases: {}'.format(len(outliers)))\nnew_df = new_df.drop(new_df[(new_df['V12'] > v12_upper) | (new_df['V12'] < v12_lower)].index)\nprint('Number of Instances after outliers removal: {}'.format(len(new_df)))\nprint('----' * 44)\n\n\n# Removing outliers V10 Feature\nv10_fraud = new_df['V10'].loc[new_df['Class'] == 1].values\nq25, q75 = np.percentile(v10_fraud, 25), np.percentile(v10_fraud, 75)\nv10_iqr = q75 - q25\n\nv10_cut_off = v10_iqr * 1.5\nv10_lower, v10_upper = q25 - v10_cut_off, q75 + v10_cut_off\nprint('V10 Lower: {}'.format(v10_lower))\nprint('V10 Upper: {}'.format(v10_upper))\noutliers = [x for x in v10_fraud if x < v10_lower or x > v10_upper]\nprint('V10 outliers: {}'.format(outliers))\nprint('Feature V10 Outliers for Fraud Cases: {}'.format(len(outliers)))\nnew_df = new_df.drop(new_df[(new_df['V10'] > v10_upper) | (new_df['V10'] < v10_lower)].index)\nprint('Number of Instances after outliers removal: {}'.format(len(new_df)))","8dd407fc":"f,(ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20,6))\n\ncolors = ['#B3F9C5', '#f9c5b3']\n# Boxplots with outliers removed\n# Feature V14\nsns.boxplot(x=\"Class\", y=\"V14\", data=new_df,ax=ax1, palette=colors)\nax1.set_title(\"V14 Feature \\n Reduction of outliers\", fontsize=14)\nax1.annotate('Fewer extreme \\n outliers', xy=(0.98, -17.5), xytext=(0, -12),\n            arrowprops=dict(facecolor='black'),\n            fontsize=14)\n\n# Feature 12\nsns.boxplot(x=\"Class\", y=\"V12\", data=new_df, ax=ax2, palette=colors)\nax2.set_title(\"V12 Feature \\n Reduction of outliers\", fontsize=14)\nax2.annotate('Fewer extreme \\n outliers', xy=(0.98, -17.3), xytext=(0, -12),\n            arrowprops=dict(facecolor='black'),\n            fontsize=14)\n\n# Feature V10\nsns.boxplot(x=\"Class\", y=\"V10\", data=new_df, ax=ax3, palette=colors)\nax3.set_title(\"V10 Feature \\n Reduction of outliers\", fontsize=14)\nax3.annotate('Fewer extreme \\n outliers', xy=(0.95, -16.5), xytext=(0, -12),\n            arrowprops=dict(facecolor='black'),\n            fontsize=14)","d4d7f244":"from sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nimport matplotlib.patches as mpatches\nimport time\n# New_df is from the random undersample data (fewer instances)\nX = new_df.drop('Class', axis=1)\ny = new_df['Class']\n# WE ARE MAPPING OUR DATA FROM HIGH DIMENSIONS TO LOW DIMENSION\n\n# T-SNE Implementation\nt0 = time.time()\nX_reduced_tsne = TSNE(n_components=2, random_state=42,perplexity =30).fit_transform(X.values)\nt1 = time.time()\nprint(\"T-SNE took {:.2} s\".format(t1 - t0))\n\n# PCA Implementation\nt0 = time.time()\nX_reduced_pca = PCA(n_components=2, random_state=42).fit_transform(X.values)\nt1 = time.time()\nprint(\"PCA took {:.2} s\".format(t1 - t0))","aa001115":"\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,8))\n# labels = ['No Fraud', 'Fraud']\nf.suptitle('Clusters using Dimensionality Reduction', fontsize=14)\n\n\nblue_patch = mpatches.Patch(color='#0A0AFF', label='No Fraud')\nred_patch = mpatches.Patch(color='#AF0000', label='Fraud')\n\n\n# t-SNE scatter plot\nax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\nax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\nax1.set_title('t-SNE', fontsize=14)\n\n\nax1.legend(handles=[blue_patch, red_patch])\n\n\n# PCA scatter plot\nax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\nax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\nax2.set_title('PCA', fontsize=14)\n\nax2.legend(handles=[blue_patch, red_patch])","14ae9cdb":"\nX_train = new_df.drop('Class', axis=1)\ny_train = new_df['Class']","c8471b56":"# CONVERTIG THESE INTO ARRAYS\nX_train = X_train.values\n#X_test = X_test.values\ny_train = y_train.values\n#y_test = y_test.values","3f9f1cc0":"print('X_shapes:\\n', 'X_train:', X_train.shape ,'\\n')\nprint('Y_shapes:\\n', 'Y_train:', y_train.shape)\nX.columns","4611403b":"#IMPORTING 6 MODELS\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb","c8a47b81":"# CREATING OBJECT\nModels = {\n    \"LogisiticRegression\": LogisticRegression(),\n    \"KNearest\": KNeighborsClassifier(),\n    \"Support Vector Classifier\": SVC(),\n    #\"DecisionTreeClassifier\": DecisionTreeClassifier(),\n    #\"RandomForestClassifier\" : RandomForestClassifier(),\n    #\"XgBoost\" : xgb.XGBClassifier(eval_metric='logloss')\n}","10efd3cb":"from sklearn.model_selection import cross_val_score\n\n\nfor key, classifier in Models.items():\n    classifier.fit(X_train, y_train)\n    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")","c1c1cd28":"# Use GridSearchCV to find the best parameters.\nfrom sklearn.model_selection import GridSearchCV\n'''Steps:\n    1. creating dict of parameters we are going to check\n    2. applying grid search cv on our model and get model with best parameters\n    3. Fit this new Model with X_train,y_train\n    \n'''\nt0=time.time()\n# Logistic Regression \nlog_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n\ngrid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params,scoring = 'f1')\ngrid_log_reg.fit(X_train, y_train)\n# We automatically get the logistic regression with the best parameters.\nlog_reg = grid_log_reg.best_estimator_\n\n\nknears_params = {\"n_neighbors\": list(range(2,10,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n\ngrid_knears = GridSearchCV(KNeighborsClassifier(), knears_params)\ngrid_knears.fit(X_train, y_train)\n# KNears best estimator\nknears_neighbors = grid_knears.best_estimator_\n\n# Support Vector Classifier\nsvc_params = {'C': [0.5, 0.7, 0.9, 1], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\ngrid_svc = GridSearchCV(SVC(), svc_params)\ngrid_svc.fit(X_train, y_train)\n\n# SVC best estimator\nsvc = grid_svc.best_estimator_\n\n'''# DecisionTree Classifier\ntree_params = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,4,1)), \n              \"min_samples_leaf\": list(range(5,7,1))}\ngrid_tree = GridSearchCV(DecisionTreeClassifier(), tree_params)\ngrid_tree.fit(X_train, y_train)\n\n# tree best estimator\ntree_clf = grid_tree.best_estimator_\n# Random Forest Classifier\nrfc_params = {\"n_estimators\" :[100,150,200,250,300],\n             \"min_samples_split\" : [2,5,10],\n             \"max_features\": ['auto', 'sqrt', 'log2', 'None']}\ngrid_rfc = GridSearchCV(RandomForestClassifier(),rfc_params)\ngrid_rfc.fit(X_train,y_train)\n\nrfc = grid_rfc.best_estimator_\n# the n_estimator parameter controls the number of trees inside the classifier.\n\n# XGBoost\nxg_param = {\n    \"max_depth\": [3, 4, 5, 7],\n    \"learning_rate\": [0.1, 0.01, 0.05],\n    \"gamma\": [0, 0.25, 1],\n    \"reg_lambda\": [0, 1, 10],\n    \"scale_pos_weight\": [1, 3, 5],\n    \"subsample\": [0.8],\n    \"colsample_bytree\": [0.5],\n}\ngrid_xg = GridSearchCV(xgb.XGBClassifier(eval_metric = 'logloss'),xg_param)\ngrid_xg.fit(X_train,y_train)\nxg = grid_xg.best_estimator_'''\nt1= time.time()\nprint(t1-t0)","4916e009":"print(log_reg,'\\n', knears_neighbors,'\\n' ,svc)\n","2948af3d":"\n\nlog_reg_score = cross_val_score(log_reg, X_train, y_train, cv=5)\nprint('Logistic Regression Cross Validation Score: ', round(log_reg_score.mean() * 100, 2).astype(str) + '%')\n\n\nknears_score = cross_val_score(knears_neighbors, X_train, y_train, cv=5)\nprint('Knears Neighbors Cross Validation Score', round(knears_score.mean() * 100, 2).astype(str) + '%')\n\nsvc_score = cross_val_score(svc, X_train, y_train, cv=5)\nprint('Support Vector Classifier Cross Validation Score', round(svc_score.mean() * 100, 2).astype(str) + '%')\n\n'''tree_score = cross_val_score(tree_clf, X_train, y_train, cv=5)\nprint('DecisionTree Classifier Cross Validation Score', round(tree_score.mean() * 100, 2).astype(str) + '%')\n\nrfc_score = cross_val_score(rfc, X_train, y_train, cv=5)\nprint('Random Forest Classifier Cross Validation Score', round(rfc_score.mean() * 100, 2).astype(str) + '%')\n\nxg_score = cross_val_score(xg, X_train, y_train, cv=5)\nprint('Xg Boost Cross Validation Score', round(xg_score.mean() * 100, 2).astype(str) + '%')'''\n","fdb575ed":"from sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import plot_precision_recall_curve","dadcaa23":"org_log_reg_pred = log_reg.predict(original_Xtest)\n\norg_knears_pred = knears_neighbors.predict(original_Xtest)\n\norg_svc_pred = svc.predict(original_Xtest)\n\n'''org_tree_pred = tree_clf.predict(original_Xtest)\n\norg_rfc_pred = rfc.predict(original_Xtest)\n\norg_xg_pred = xg.predict(original_Xtest)'''","167204a8":"from sklearn.metrics import confusion_matrix,classification_report","372076c7":"org_log_reg_cf = confusion_matrix(original_ytest, org_log_reg_pred)\norg_kneighbors_cf = confusion_matrix(original_ytest, org_knears_pred)\norg_svc_cf = confusion_matrix(original_ytest, org_svc_pred)\n'''org_tree_cf = confusion_matrix(original_ytest,org_tree_pred)\norg_rfc_cf = confusion_matrix(original_ytest, org_rfc_pred)\norg_xg_cf = confusion_matrix(original_ytest,org_xg_pred)'''\n\nprint(org_log_reg_cf, '\\n', org_kneighbors_cf, '\\n', org_svc_cf) #'\\n', org_tree_cf , '\\n', org_rfc_cf, '\\n', org_xg_cf)","61b2ea21":"print('Logistic Regression:')\nprint(classification_report(original_ytest, org_log_reg_pred))\n\nprint('KNears Neighbors:')\nprint(classification_report(original_ytest, org_knears_pred))\n\nprint('Support Vector Classifier:')\nprint(classification_report(original_ytest, org_svc_pred))\n\n'''print('Tree Classifier:')\nprint(classification_report(original_ytest, org_tree_pred))\n\nprint('Random Forest Classifier:')\nprint(classification_report(original_ytest, org_rfc_pred))\n\nprint('XG:')\nprint(classification_report(original_ytest, org_xg_pred))'''\n","7a77f502":"fig, ax = plt.subplots(1, 1,figsize=(12,6))\nplot_precision_recall_curve(log_reg, original_Xtest, original_ytest,ax=ax)\nplot_precision_recall_curve(knears_neighbors, original_Xtest, original_ytest,ax=ax)\nplot_precision_recall_curve(svc, original_Xtest, original_ytest,ax=ax)\n'''plot_precision_recall_curve(tree_clf, original_Xtest, original_ytest,ax=ax)'''\nax.set_title('Precision - Recall Curve')\nplt.legend()","c572a074":"import sklearn.metrics","cd070259":"org_lr_auprc = sklearn.metrics.average_precision_score(original_ytest,org_log_reg_pred)","8687b621":"org_lr_auprc","77a0aae4":"from imblearn.over_sampling import SMOTE\n","015b562c":"os = SMOTE(random_state = 1234)\nX = df.drop('Class', axis=1)\ny = df['Class']\nsss = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\nfor train_index, test_index in sss.split(X, y):\n    #print(\"Train:\", train_index, \"Test:\", test_index)\n    o_Xtrai, o_Xtest = X.iloc[train_index], X.iloc[test_index]\n    o_ytrai, o_ytest = y.iloc[train_index], y.iloc[test_index]\nprint('Length of training data : ',+ len(o_ytrai))\nprint('Length of test data : ',+ len(o_ytest))\ncolumns = o_Xtest.columns\ncolumns","3bb6e196":"os_data_X,os_data_y=os.fit_resample(o_Xtrai,o_ytrai)\nos_data_X = pd.DataFrame(data=os_data_X,columns=columns )\nos_data_y= pd.DataFrame(data=os_data_y,columns=[\"Class\"])\nprint('Length of original training data : ',+ len(o_ytrai))\n#print(227840*2)\nprint(\"length of oversampled data is \",len(os_data_X))\nprint(\"Number of normal transcation in oversampled data\",len(os_data_y[os_data_y[\"Class\"]==0]))\nprint(\"No.of fraud transcation\",len(os_data_y[os_data_y[\"Class\"]==1]))\nprint(\"Proportion of Normal data in oversampled data is \",len(os_data_y[os_data_y[\"Class\"]==0])\/len(os_data_X))\nprint(\"Proportion of fraud data in oversampled data is \",len(os_data_y[os_data_y[\"Class\"]==1])\/len(os_data_X))","b188f925":"o_Xtest.shape","a45f03ed":"columns","4f5c2b68":"lr = LogisticRegression()\nlr.fit(os_data_X,os_data_y)\npred = lr.predict(o_Xtest)\nprint(classification_report(o_ytest,pred))","699c44e2":"print(confusion_matrix(o_ytest,pred))","a2bebf40":"#print('Log Regression: ', roc_auc_score(o_ytest, pred))","02055d4e":"over_auprc = sklearn.metrics.average_precision_score(o_ytest,pred)\nover_auprc","a92afb88":"'''xgo = xgb.XGBClassifier(eval_metric = 'logloss')\nxgo.fit(os_data_X,os_data_y)\npred_xg = xgo.predict(o_Xtest)\nprint(classification_report(o_ytest,pred_xg))\nprint(confusion_matrix(o_ytest,pred_xg))'''","fd270ed5":"'''xg_param = {\n    \"max_depth\": [3, 4, 5, 7],\n    \"learning_rate\": [0.1, 0.01, 0.05],\n    \"gamma\": [0, 0.25, 1],\n    \"reg_lambda\": [0, 1, 10],\n    \"scale_pos_weight\": [1, 3, 5],\n    \"subsample\": [0.8],\n    \"colsample_bytree\": [0.5],\n}\ngrid_xg = GridSearchCV(xgb.XGBClassifier(eval_metric = 'logloss'),xg_param)\ngrid_xg.fit(os_data_X,os_data_y)\nxg = grid_xg.best_estimator_'''","7a8f93ba":"TIME HAS BIMODAL DISTRIBUTION i.e. AFTER ONE PEAK TRANSACTION COMES DOWN AND AGAIN RISES. (THIS MAY HAPPEN BCOZ OF NIGHT TIME.)","3f776839":"To know more about [univariate analysis](https:\/\/towardsdatascience.com\/exploring-univariate-data-e7e2dc8fde80)\nand further we can get knowledge about [EDA](https:\/\/towardsdatascience.com\/exploratory-data-analysis-eda-python-87178e35b14#:~:text=Bivariate%20Analysis%20If%20we%20analyze%20data%20by%20taking,a%20dataset%2C%20it%20is%20known%20as%20Bivariate%20Analysis.)","6ec379f7":"MAXIMUM TRANSACTION ARE OF LOW AMOUNT AND HIGH AMOUNT TRANSACTION ARE VERY LESS","42216993":"# BIVARIATE ANALYSIS","042e736d":"# CORRELATION","3fc8f502":"# OVER SAMPLING BY SMOTE","c46696ea":" # [AUPRC](https:\/\/glassboxmedicine.com\/2019\/03\/02\/measuring-performance-auprc\/#:~:text=The%20area%20under%20the%20precision-recall%20curve%20%28AUPRC%29%20is,care%20a%20lot%20about%20finding%20the%20positive%20examples.) and [one more article on auprc](https:\/\/www.listendata.com\/2019\/07\/precision-recall-curve-simplified.html|)","a9a7eff1":"**This gives us an indication that further predictive models will perform pretty well in separating fraud cases from non-fraud cases.**\n","ae2bf144":"AIM : to classify fraud and non fraudulent transaction ","0e01724b":"# Testing on Test data","b08a59bb":"# GRID SEARCH","b78ecc96":"# IMPLEMENTING MODEL","5fa332da":"# SPLITTING \nsplitting the original data for testing as we cannot test on undersample or oversample data that will not give us correct estimate of our model.","724b0512":"THIS TELLS US OUR DATA IS VERY IMBALANCE 99.83% NON FRAUD AND ONLY 0.17 % FRAUD","d7e21a83":"# UNDERSAMPLING\n\nperforming random undersampling","478713a0":"# SCALING","52b83bfc":"# EDA : **UNIVARIATE ANALYSIS** ","6868e0a8":"# TSNE and PCA :DIMENSIONAITY REDUCTION VISUALIZATION","80094263":"# DETECTING OUTLIER","272d9366":"[Standardiation](https:\/\/scikit-learn.org\/stable\/modules\/preprocessing.html#preprocessing-scaler) and [Robust scalar](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.RobustScaler.html#sklearn.preprocessing.RobustScaler)","e0d7a44c":"# Confussion Matrix"}}