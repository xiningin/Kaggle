{"cell_type":{"9121db24":"code","498aaaf5":"code","936f609b":"code","1e63f4ec":"code","04fc6ac0":"code","ce147345":"code","a5b19ab0":"code","bad2a138":"code","34565d10":"code","5a03e860":"code","b6da6049":"code","268df453":"code","62ea775a":"code","c3d26443":"code","57938a55":"code","e8837720":"code","453a5b49":"code","05a98df2":"code","8d3ed16f":"code","66413aa6":"code","e93e980d":"code","8d42ab3b":"code","b0f09879":"code","2ee8ab51":"code","80f8fb81":"code","31ef185c":"code","c636eff2":"code","8ca86bd2":"code","73e6b227":"code","65af8bec":"code","496a2fe7":"code","df2386f3":"code","75d8256a":"code","61422f41":"code","74974177":"code","792a78e0":"code","e55da386":"code","1a73606a":"code","bf0d82bb":"code","ef8c4417":"code","6c8d9b1e":"markdown","390bda44":"markdown","86151e96":"markdown","973ba3d4":"markdown","798418b9":"markdown","d6b4c461":"markdown","2e5c23aa":"markdown","5f2b8c4a":"markdown","9ac04252":"markdown","17e26808":"markdown","0023846c":"markdown","1c231284":"markdown","5259b485":"markdown","e7e65744":"markdown","f6bbfc9e":"markdown","25a57306":"markdown","211433a5":"markdown","d9cc9d51":"markdown","9751771a":"markdown","472c8e51":"markdown","ffc807dc":"markdown","2cc9c79d":"markdown","4e47a9a9":"markdown","b6e00a16":"markdown","573eb71b":"markdown","b0a88a58":"markdown","b5e45b7b":"markdown","138bfbe8":"markdown","d40891c5":"markdown","982f8f8a":"markdown","7b3fe5ad":"markdown"},"source":{"9121db24":"# Import packages\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nimport sklearn.metrics as metrics\nfrom sklearn.metrics import accuracy_score\nimport math\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectFromModel\nfrom time import time\nimport time \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.svm import  SVR\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.ensemble import BaggingRegressor, AdaBoostRegressor, VotingRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn import metrics\n","498aaaf5":"sample_submission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\n#Creating a copy of the train and test datasets\nc_test  = test.copy()\nc_train  = train.copy()\n","936f609b":"c_train.head()","1e63f4ec":"c_test.head()","04fc6ac0":"c_train['train']  = 1\nc_test['train']  = 0\ndf = pd.concat([c_train, c_test], axis=0,sort=False)\n","ce147345":"#Percentage of NAN Values \nNAN = [(c, df[c].isna().mean()*100) for c in df]\nNAN = pd.DataFrame(NAN, columns=[\"column_name\", \"percentage\"])","a5b19ab0":"NAN = NAN[NAN.percentage > 50]\nNAN.sort_values(\"percentage\", ascending=False)","bad2a138":"#Drop PoolQC, MiscFeature, Alley and Fence features\ndf = df.drop(['Alley','PoolQC','Fence','MiscFeature'],axis=1)\n","34565d10":"object_columns_df = df.select_dtypes(include=['object'])\nnumerical_columns_df =df.select_dtypes(exclude=['object'])","5a03e860":"object_columns_df.dtypes","b6da6049":"numerical_columns_df.dtypes","268df453":"#Number of null values in each feature\nnull_counts = object_columns_df.isnull().sum()\nprint(\"Number of null values in each column:\\n{}\".format(null_counts))\n","62ea775a":"columns_None = ['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','GarageType','GarageFinish','GarageQual','FireplaceQu','GarageCond']\nobject_columns_df[columns_None]= object_columns_df[columns_None].fillna('None')","c3d26443":"columns_with_lowNA = ['MSZoning','Utilities','Exterior1st','Exterior2nd','MasVnrType','Electrical','KitchenQual','Functional','SaleType']\n#fill missing values for each column (using its own most frequent value)\nobject_columns_df[columns_with_lowNA] = object_columns_df[columns_with_lowNA].fillna(object_columns_df.mode().iloc[0])\n","57938a55":"#Number of null values in each feature\nnull_counts = numerical_columns_df.isnull().sum()\nprint(\"Number of null values in each column:\\n{}\".format(null_counts))\n","e8837720":"print((numerical_columns_df['YrSold']-numerical_columns_df['YearBuilt']).median())\nprint(numerical_columns_df[\"LotFrontage\"].median())\n","453a5b49":"numerical_columns_df['GarageYrBlt'] = numerical_columns_df['GarageYrBlt'].fillna(numerical_columns_df['YrSold']-35)\nnumerical_columns_df['LotFrontage'] = numerical_columns_df['LotFrontage'].fillna(68)\n","05a98df2":"numerical_columns_df= numerical_columns_df.fillna(0)\n","8d3ed16f":"object_columns_df['Utilities'].value_counts().plot(kind='bar',figsize=[10,3])\nobject_columns_df['Utilities'].value_counts() \n","66413aa6":"object_columns_df['Street'].value_counts().plot(kind='bar',figsize=[10,3])\nobject_columns_df['Street'].value_counts() ","e93e980d":"object_columns_df['Condition2'].value_counts().plot(kind='bar',figsize=[10,3])\nobject_columns_df['Condition2'].value_counts() \n","8d42ab3b":"object_columns_df['RoofMatl'].value_counts().plot(kind='bar',figsize=[10,3])\nobject_columns_df['RoofMatl'].value_counts() ","b0f09879":"object_columns_df['Heating'].value_counts().plot(kind='bar',figsize=[10,3])\nobject_columns_df['Heating'].value_counts() #======> Drop feature one Type\n","2ee8ab51":"object_columns_df = object_columns_df.drop(['Heating','RoofMatl','Condition2','Street','Utilities'],axis=1)\n","80f8fb81":"numerical_columns_df['Age_House']= (numerical_columns_df['YrSold']-numerical_columns_df['YearBuilt'])\nnumerical_columns_df['Age_House'].describe()\n","31ef185c":"Negatif = numerical_columns_df[numerical_columns_df['Age_House'] < 0]\nNegatif\n","c636eff2":"numerical_columns_df.loc[numerical_columns_df['YrSold'] < numerical_columns_df['YearBuilt'],'YrSold' ] = 2009\nnumerical_columns_df['Age_House']= (numerical_columns_df['YrSold']-numerical_columns_df['YearBuilt'])\nnumerical_columns_df['Age_House'].describe()\n","8ca86bd2":"numerical_columns_df['TotalBsmtBath'] = numerical_columns_df['BsmtFullBath'] + numerical_columns_df['BsmtFullBath']*0.5\nnumerical_columns_df['TotalBath'] = numerical_columns_df['FullBath'] + numerical_columns_df['HalfBath']*0.5 \nnumerical_columns_df['TotalSA']=numerical_columns_df['TotalBsmtSF'] + numerical_columns_df['1stFlrSF'] + numerical_columns_df['2ndFlrSF']\n","73e6b227":"numerical_columns_df.head()","65af8bec":"bin_map  = {'TA':2,'Gd':3, 'Fa':1,'Ex':4,'Po':1,'None':0,'Y':1,'N':0,'Reg':3,'IR1':2,'IR2':1,'IR3':0,\"None\" : 0,\n            \"No\" : 2, \"Mn\" : 2, \"Av\": 3,\"Gd\" : 4,\"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3,\"BLQ\" : 4, \"ALQ\" : 5, \"GLQ\" : 6\n            }\nobject_columns_df['ExterQual'] = object_columns_df['ExterQual'].map(bin_map)\nobject_columns_df['ExterCond'] = object_columns_df['ExterCond'].map(bin_map)\nobject_columns_df['BsmtCond'] = object_columns_df['BsmtCond'].map(bin_map)\nobject_columns_df['BsmtQual'] = object_columns_df['BsmtQual'].map(bin_map)\nobject_columns_df['HeatingQC'] = object_columns_df['HeatingQC'].map(bin_map)\nobject_columns_df['KitchenQual'] = object_columns_df['KitchenQual'].map(bin_map)\nobject_columns_df['FireplaceQu'] = object_columns_df['FireplaceQu'].map(bin_map)\nobject_columns_df['GarageQual'] = object_columns_df['GarageQual'].map(bin_map)\nobject_columns_df['GarageCond'] = object_columns_df['GarageCond'].map(bin_map)\nobject_columns_df['CentralAir'] = object_columns_df['CentralAir'].map(bin_map)\nobject_columns_df['LotShape'] = object_columns_df['LotShape'].map(bin_map)\nobject_columns_df['BsmtExposure'] = object_columns_df['BsmtExposure'].map(bin_map)\nobject_columns_df['BsmtFinType1'] = object_columns_df['BsmtFinType1'].map(bin_map)\nobject_columns_df['BsmtFinType2'] = object_columns_df['BsmtFinType2'].map(bin_map)\n\nPavedDrive =   {\"N\" : 0, \"P\" : 1, \"Y\" : 2}\nobject_columns_df['PavedDrive'] = object_columns_df['PavedDrive'].map(PavedDrive)\n\n","496a2fe7":"#Select categorical features\nrest_object_columns = object_columns_df.select_dtypes(include=['object'])\n#Using One hot encoder\nobject_columns_df = pd.get_dummies(object_columns_df, columns=rest_object_columns.columns) \n","df2386f3":"object_columns_df.head()","75d8256a":"df_final = pd.concat([object_columns_df, numerical_columns_df], axis=1,sort=False)\ndf_final.head()","61422f41":"df_final = df_final.drop(['Id',],axis=1)\n\ndf_train = df_final[df_final['train'] == 1]\ndf_train = df_train.drop(['train',],axis=1)\n\n\ndf_test = df_final[df_final['train'] == 0]\ndf_test = df_test.drop(['SalePrice'],axis=1)\ndf_test = df_test.drop(['train',],axis=1)\n","74974177":"target= df_train['SalePrice']\ndf_train = df_train.drop(['SalePrice'],axis=1)","792a78e0":"X_train,X_val,y_train,y_val = train_test_split(df_train,target,test_size=0.33,random_state=0)","e55da386":"def acc_summary(pipeline, X_train, y_train, X_val, y_val):\n    sentiment_fit = pipeline.fit(X_train, y_train)\n    y_pred = sentiment_fit.predict(X_val)\n    rmse = math.sqrt(metrics.mean_squared_error(y_val, y_pred))\n    print(\"root mean squre error : {0:.2f}\".format(rmse))\n    #print(\"train and test time: {0:.2f}s\".format(train_test_time))\n    print(\"-\"*80)\n    return rmse","1a73606a":"names = [ \n        'Gradient Boosting Regressor',  \n        \"Bagging Regressor\",\n        \"AdaBoost Regressor\", \n        \"K Nearest Neighbour Regressor\",\n         \"Decison Tree Regressor\",\n         \"Random Forest Regressor\",\n        \"Gaussian Process Regressor\",\n        \"XGB Regressor\",\n        \"LGBM Regressor\"\n         ]\nregressors = [\n    \n    GradientBoostingRegressor(), \n    BaggingRegressor(),\n    AdaBoostRegressor(),\n    KNeighborsRegressor(),\n    DecisionTreeRegressor(),\n    RandomForestRegressor(),\n    GaussianProcessRegressor(),\n    XGBRegressor( booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.6, gamma=0,\n             importance_type='gain', learning_rate=0.02, max_delta_step=0,\n             max_depth=4, min_child_weight=1.5, n_estimators=2000,\n             n_jobs=1, nthread=None, objective='reg:linear',\n             reg_alpha=0.6, reg_lambda=0.6, scale_pos_weight=1, \n             silent=None, subsample=0.8, verbosity=1),\n     LGBMRegressor(objective='regression', \n                                       num_leaves=4, #was 3\n                                       learning_rate=0.01, \n                                       n_estimators=11000, #8000\n                                       max_bin=200, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.4, # 'was 0.2'\n                                       )\n\n        ]\n\nzipped_clf = zip(names,regressors)","bf0d82bb":"def regressor_comparator(X_train,y_train,X_val,y_val,regressor=zipped_clf): \n    result = []\n    for n,r in regressor:\n        checker_pipeline = Pipeline([\n            ('regressor', r)\n        ])\n        print(\"Validation result for {}\".format(n))\n        #print(r)\n        clf_acc= acc_summary(checker_pipeline,X_train, y_train, X_val, y_val)\n        result.append((n,clf_acc))\n    return result","ef8c4417":"regressor_comparator(X_train,y_train,X_val,y_val)","6c8d9b1e":"* Concat Categorical(after encoding) and numerical features","390bda44":"* **Ordinal categories features** - Mapping from 0 to N","86151e96":"* Like we see here tha the minimun is -1 ???\n* It is strange to find that the house was sold in 2007 before the YearRemodAdd 2009.\nSo we decide to change the year of sold to 2009\n\n","973ba3d4":"* <font color='blue'>  Getting information about train dataset <\/font>","798418b9":"* Deeling with **categorical** feature ","d6b4c461":"## Splitting The Data into Train and Test set.","2e5c23aa":"## <font color='blue'>  Loading The Datasets <\/font>","5f2b8c4a":"* We finally end up with a clean dataset\n\n","9ac04252":"* We can drop PoolQC, MiscFeature, Alley and Fence features because they have more than 80% of missing values.\n","17e26808":"* **Now we will create some new features**","0023846c":"* Now we have a clean categorical features\n* In the next step we will deal with the **numerical** features","1c231284":"# Implementing The regressors comparing accuracies.","5259b485":"* Will we use One hot encoder to encode the rest of categorical features","e7e65744":"\n* <font color='blue'>  Concat Train and Test datasets <\/font>\n","f6bbfc9e":"**Now the next step is to encode categorical features\n**","25a57306":"1. * So we will fill the year with 1979 and the Lot frontage with 68","211433a5":"1. We have 81 columns.\n2. Our target variable is SalePrice.\n3. Id is just an index that we can drop but we will need it in the final submission.\n1. We have many missing values \n\n * * * * we have 79 features in our dataset.\n\n","d9cc9d51":"#  <font color='blue'> House Prices : Data cleaning, viz and modeling  <\/font>","9751771a":"* **Numerical Features** :","472c8e51":"* After making some plots we found that we have some colums with low variance so we decide to delete them","ffc807dc":"\n* <font color='red'>  Calculating the percentage of missing values of each feature <\/font>\n","2cc9c79d":"* **Categorical Features** :\n","4e47a9a9":"\n* <font color='blue'>  Getting information about test dataset <\/font>\n","b6e00a16":"* Fill the rest of columns with 0","573eb71b":"Separate Train and Targets","b0a88a58":"\n\n* <font color='blue'>  Features with more than 50% of missing values. <\/font>\n","b5e45b7b":"1. Fill GarageYrBlt and LotFrontage\n1. Fill the rest of columns with 0","138bfbe8":"* We will fill -- **BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, GarageType, GarageFinish, GarageQual, FireplaceQu, GarageCond** -- with \"None\" (Take a look in the data description).\n*  We will fill the rest of features with th most frequent value (using its own most frequent value)\n","d40891c5":"# Thankyou for Reading...","982f8f8a":"* Now we will select numerical and categorical features \n","7b3fe5ad":"* TotalBsmtBath : Sum of :\nBsmtFullBath and  1\/2 BsmtHalfBath\n\n* TotalBath : Sum of :\nFullBath and 1\/2 HalfBath\n\n* TotalSA : Sum of : \n1stFlrSF and 2ndFlrSF and basement area\n\n\n\n\n"}}