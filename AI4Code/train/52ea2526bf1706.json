{"cell_type":{"dac4c7a3":"code","164714cc":"code","7a78ee87":"code","22bc14f3":"code","852c3949":"code","f95711cb":"code","e71d0e7b":"code","38db9a3c":"code","61a13208":"code","08d117ff":"code","0688480c":"code","6a98c3dd":"code","c125d457":"code","ab1a83d6":"code","56bf64a8":"code","397741b9":"code","0fe92018":"code","89bd211e":"code","cbe66475":"code","aef06d21":"code","14656a50":"code","4ee27d21":"code","cd4f2ed2":"code","0776f221":"code","1a14606a":"code","8830b083":"code","63a0c3fa":"code","c14988d6":"code","b2a8974f":"code","b5810122":"code","678f864b":"code","feb8db58":"code","a3354fcf":"code","67be21ca":"markdown","419457cc":"markdown","723f168c":"markdown","4d5e1c83":"markdown","2cdf97cd":"markdown","a873b9bd":"markdown","a3023e23":"markdown","d3321a96":"markdown","43287345":"markdown","25009e27":"markdown","eaa6b704":"markdown","1aa53819":"markdown","0c68a59d":"markdown","94f08f2d":"markdown","b32b79f2":"markdown","8e89e477":"markdown","e87e6d62":"markdown","e663727e":"markdown","c86e0b87":"markdown","68c83f3f":"markdown","0e3297e2":"markdown","43b3ad93":"markdown","d8eea4f8":"markdown","c7d3fca9":"markdown","5f7be0e4":"markdown","366128b9":"markdown","01ac9f16":"markdown","36c7f2db":"markdown","c438f2e9":"markdown","38bd1a72":"markdown","e4b933f0":"markdown","e7af674a":"markdown","f42f71be":"markdown","7644fb4b":"markdown","ca997eec":"markdown","165c37ce":"markdown","d6a2cc2b":"markdown","9b51008d":"markdown"},"source":{"dac4c7a3":"import numpy as np\nimport pandas as pd\nimport imageio\nimport random\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n\nimport tensorflow as tf \nfrom tensorflow import keras\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.layers import Activation, Input, Conv2D, MaxPooling2D, BatchNormalization, Conv2DTranspose, concatenate\nfrom tensorflow.keras.models import Model, load_model\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Model, load_model\n\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')","164714cc":"image_path = [\"..\/input\/lyft-udacity-challenge\/\"+\"data\"+i+\"\/\"+\"data\"+i+\"\/CameraRGB\/\" for i in ['A', 'B', 'C', 'D', 'E']]\nmask_path = [\"..\/input\/lyft-udacity-challenge\/\"+\"data\"+i+\"\/\"+\"data\"+i+\"\/CameraSeg\/\" for i in ['A', 'B', 'C', 'D', 'E']]","7a78ee87":"def list_image_paths(directory_paths):\n    image_paths = []\n    for directory in range(len(directory_paths)):\n        image_filenames = os.listdir(directory_paths[directory])\n        for image_filename in image_filenames:\n            image_paths.append(directory_paths[directory] + image_filename)\n    return image_paths","22bc14f3":"image_paths = list_image_paths(image_path) \nmask_paths = list_image_paths(mask_path)\nnumber_of_images, number_of_masks = len(image_paths), len(mask_paths)\nprint(f\"1. There are {number_of_images} images and {number_of_masks} masks in our dataset\")\nprint(f\"2. An example of an image path is: \\n {image_paths[0]}\")\nprint(f\"3. An example of a mask path is: \\n {mask_paths[0]}\")\n","852c3949":"import random\nnumber_of_samples = len(image_paths)\n\nfor i in range(3):\n    N = random.randint(0, number_of_samples - 1)\n\n    img = imageio.imread(image_paths[N])\n    mask = imageio.imread(mask_paths[N])\n    mask = np.array([max(mask[i, j]) for i in range(mask.shape[0]) for j in range(mask.shape[1])]).reshape(img.shape[0], img.shape[1])\n\n    fig, arr = plt.subplots(1, 3, figsize=(20, 8))\n    arr[0].imshow(img)\n    arr[0].set_title('Image')\n    arr[0].axis(\"off\")\n    arr[1].imshow(mask)\n    arr[1].set_title('Segmentation')\n    arr[1].axis(\"off\")    \n    arr[2].imshow(mask, cmap='Paired')\n    arr[2].set_title('Image Overlay')\n    arr[2].axis(\"off\")","f95711cb":"# First split the image paths into training and validation sets\ntrain_image_paths, val_image_paths, train_mask_paths, val_mask_paths = train_test_split(image_paths, mask_paths, train_size=0.8, random_state=0)\n\n# Keep part of the validation set as test set\nvalidation_image_paths, test_image_paths, validation_mask_paths, test_mask_paths = train_test_split(val_image_paths, val_mask_paths, train_size = 0.80, random_state=0)\n\nprint(f'There are {len(train_image_paths)} images in the Training Set')\nprint(f'There are {len(validation_image_paths)} images in the Validation Set')\nprint(f'There are {len(test_image_paths)} images in the Test Set')","e71d0e7b":"def read_image(image_path, mask_path):\n    \n    image = tf.io.read_file(image_path)\n    image = tf.image.decode_png(image, channels=3)\n    image = tf.image.convert_image_dtype(image, tf.float32)\n    image = tf.image.resize(image, (256, 256), method='nearest')\n\n    mask = tf.io.read_file(mask_path)\n    mask = tf.image.decode_png(mask, channels=3)\n    mask = tf.math.reduce_max(mask, axis=-1, keepdims=True)\n    mask = tf.image.resize(mask, (256, 256), method='nearest')\n    \n    return image, mask","38db9a3c":"def data_generator(image_paths, mask_paths, buffer_size, batch_size):\n    \n    image_list = tf.constant(image_paths) \n    mask_list = tf.constant(mask_paths)\n    dataset = tf.data.Dataset.from_tensor_slices((image_list, mask_list))\n    dataset = dataset.map(read_image, num_parallel_calls=tf.data.AUTOTUNE)\n    dataset = dataset.cache().shuffle(buffer_size).batch(batch_size)\n    \n    return dataset","61a13208":"batch_size = 32\nbuffer_size = 500\n\ntrain_dataset = data_generator(train_image_paths, train_mask_paths, buffer_size, batch_size)\nvalidation_dataset = data_generator(validation_image_paths, validation_mask_paths, buffer_size, batch_size)\ntest_dataset = data_generator(test_image_paths, test_mask_paths, buffer_size, batch_size)","08d117ff":"def encoding_block(inputs, filters, dropout_probability=0, max_pooling=True):\n\n    \"\"\"\n    Convolutional encoding\/downsampling block\n    \n    Arguments:\n        inputs -- Input tensor\n        filters -- Number of filters for the convolutional layers\n        max_pooling -- Use MaxPooling2D to reduce the spatial dimensions of the output volume\n    Returns: \n        next_layer, skip_connection --  Next layer and skip connection inputs \n    \"\"\"\n    \n    C = Conv2D(filters, 3, padding=\"same\", kernel_initializer=\"he_normal\")(inputs)\n    C = BatchNormalization()(C)\n    C = Activation(\"relu\")(C)\n\n    C = Conv2D(filters, 3, padding=\"same\", kernel_initializer=\"he_normal\")(C)\n    C = BatchNormalization()(C)\n    C = Activation(\"relu\")(C)\n\n    skip_connection = C  # Set aside residual\n    \n    # if max_pooling is True, add a MaxPooling2D with 2x2 pool_size\n    if max_pooling:\n        next_layer = MaxPooling2D(pool_size=(2, 2))(C)        \n    else:\n        next_layer = C\n            \n    return next_layer, skip_connection","0688480c":"def decoding_block(inputs, skip_connection_input, filters):\n    \"\"\"\n    Convolutional decoding\/upsampling block\n    \n    Arguments:\n        inputs -- Input tensor from previous layer\n        skip_connection_input -- Input tensor from previous skip layer\n        filters -- Number of filters for the convolutional layers\n    Returns: \n        C -- Tensor output\n    \"\"\"\n\n    CT = Conv2DTranspose(filters, 3, strides=(2,2), padding=\"same\", kernel_initializer=\"he_normal\")(inputs)\n    \n    residual_connection = concatenate([CT, skip_connection_input], axis=3)\n\n    C = Conv2D(filters, 3, padding=\"same\", kernel_initializer=\"he_normal\")(residual_connection)\n    C = BatchNormalization()(C)\n    C = Activation(\"relu\")(C)\n    \n    C = Conv2D(filters, 3, padding=\"same\", kernel_initializer=\"he_normal\")(C)\n    C = BatchNormalization()(C)\n    C = Activation(\"relu\")(C)\n    \n  \n    return C","6a98c3dd":"def unet_model(input_size, filters, n_classes):\n    \"\"\"\n    Unet model\n    \n    Arguments:\n        input_size -- Input shape \n        filters -- Number of filters for the convolutional layers\n        n_classes -- Number of output classes\n    Returns: \n        model -- tf.keras.Model\n    \"\"\"\n    inputs = Input(input_size)\n        \n    # Contracting Path (encoding)\n    C1, S1 = encoding_block(inputs, filters, max_pooling=True)\n    C2, S2 = encoding_block(C1, filters * 2, max_pooling=True)\n    C3, S3 = encoding_block(C2, filters * 4, max_pooling=True)\n    C4, S4 = encoding_block(C3, filters * 8, max_pooling=True)\n    \n    C5, S5 = encoding_block(C4, filters * 16, max_pooling=False)\n    \n    # Expanding Path (decoding)\n    U6 = decoding_block(C5, S4, filters * 8)\n    U7 = decoding_block(U6, S3,  filters * 4)\n    U8 = decoding_block(U7, S2,  filters = filters * 2)\n    U9 = decoding_block(U8, S1,  filters = filters)\n\n    C10 = Conv2D(filters,\n                 3,\n                 activation='relu',\n                 padding='same',\n                 kernel_initializer='he_normal')(U9)\n\n    # Add a Conv2D layer with n_classes filter, kernel size of 1 and a 'same' padding\n    C11 = Conv2D(filters = n_classes, kernel_size = (1,1), activation='sigmoid', padding='same')(C10)\n    \n    model = Model(inputs=inputs, outputs=C11)\n\n    return model","c125d457":"img_height = 256\nimg_width = 256\nnum_channels = 3\nfilters = 32\nn_classes = 13\n\nmodel = unet_model((img_height, img_width, num_channels), filters=32, n_classes=23)\nmodel.summary()","ab1a83d6":"model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\ncallback = EarlyStopping(monitor='val_accuracy', patience=20, restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_accuracy',factor=1e-1, patience=5, verbose=1, min_lr = 2e-6)\nbatch_size = 32\nepochs = 30","56bf64a8":"history = model.fit(train_dataset, \n                    validation_data = validation_dataset, \n                    epochs = epochs, \n                    verbose=1, \n                    callbacks = [callback, reduce_lr], \n                    batch_size = batch_size, \n                    shuffle = True)","397741b9":"acc = [0.] + history.history['accuracy']\nval_acc = [0.] + history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.ylabel('Accuracy')\nplt.ylim([min(plt.ylim()),1])\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.ylabel('Cross Entropy')\nplt.ylim([0,1.0])\nplt.title('Training and Validation Loss')\nplt.xlabel('epoch')\nplt.show()","0fe92018":"model.save('carla-image-segmentation-model.h5')","89bd211e":"train_loss, train_accuracy = model.evaluate(train_dataset, batch_size = 32)\nvalidation_loss, validation_accuracy = model.evaluate(validation_dataset, batch_size = 32)\ntest_loss, test_accuracy = model.evaluate(test_dataset, batch_size = 32)","cbe66475":"print(f'Model Accuracy on the Training Dataset: {round(train_accuracy * 100, 2)}%')\nprint(f'Model Accuracy on the Validation Dataset: {round(validation_accuracy * 100, 2)}%')\nprint(f'Model Accuracy on the Test Dataset: {round(test_accuracy * 100, 2)}%')","aef06d21":"def create_mask(dataset, model):\n    true_masks, predicted_masks = [], []\n    for images, masks in dataset:\n        pred_masks = model.predict(images)\n        pred_masks = tf.expand_dims(tf.argmax(pred_masks, axis=-1), axis=-1)\n        true_masks.extend(masks)\n        predicted_masks.extend(pred_masks)\n        \n    true_masks = np.array(true_masks)\n    predicted_masks = np.array(predicted_masks)\n        \n    return true_masks, predicted_masks    ","14656a50":"true_train_masks, predicted_train_masks = create_mask(train_dataset, model)\ntrue_validation_masks, predicted_validation_masks = create_mask(validation_dataset, model)\ntrue_test_masks, predicted_test_masks = create_mask(test_dataset, model)","4ee27d21":"def evaluate_model(true_masks, predicted_masks, n_classes, smooth = 1e-6):\n    \n    \"\"\"\n    Evaluates semantic segmentation model\n    \n    Argument:\n        true_masks: ground truth segmentations\n        predicted_masks: predicted segmentations\n        n_classes: number of segmentation classes\n        smooth: a minute float digit added to denominators to avoid error from a zero division\n    \n    Returns:\n        class_wise_evaluations: a dictionary containing evaluation metric \n                                outputs the for each segmentation class \n        overall_evaluations: a dictionary containing evaluation metric \n                             outputs the for all segmentation classes\n        \"\"\"\n    # Create empty lists to store evaluation metric outputs\n    class_wise_true_positives, class_wise_true_negatives = [],[]\n    class_wise_false_positives, class_wise_false_negatives = [],[]\n    class_wise_precisions, class_wise_recalls = [],[] \n    class_wise_specificities, class_wise_ious = [],[] \n    class_wise_tdrs, class_wise_f1_scores = [],[]\n    classes = []\n            \n    for clas in range(n_classes):\n        true_positives, true_negatives, false_positives, false_negatives = 0,0,0,0\n        precisions, recalls, specificities, ious, f1_scores, tdrs = 0,0,0,0,0,0        \n        \n        number_of_masks = true_masks.shape[0]\n        \n        for mask_id in range(number_of_masks):\n            true_positive = np.sum(np.logical_and(true_masks[mask_id]==clas, predicted_masks[mask_id]==clas))\n            true_negative = np.sum(np.logical_and(true_masks[mask_id]!=clas, predicted_masks[mask_id]!=clas))\n            false_positive = np.sum(np.logical_and(true_masks[mask_id]!=clas, predicted_masks[mask_id]==clas))\n            false_negative = np.sum(np.logical_and(true_masks[mask_id]==clas, predicted_masks[mask_id]!=clas))\n            \n            true_positives += true_positive\n            true_negatives += true_negative\n            false_positives += false_positive\n            false_negatives += false_negative\n\n        recall = round(true_positives\/(true_positives + false_negatives + smooth), 2)\n        precision = round(true_positives\/(true_positives + false_positives + smooth), 2)\n        specificity = round(true_negatives\/(true_negatives + false_positives + smooth), 2)\n        tdr = round((1 - (false_negatives\/(true_positives + false_negatives + smooth))), 2)\n        iou = round(true_positives\/(true_positives + false_negatives + false_positives + smooth), 2)\n        f1_score = round((2 * precision * recall)\/(precision + recall + smooth), 2)\n        \n        class_wise_true_positives.append(true_positives)\n        class_wise_true_negatives.append(true_negatives)\n        class_wise_false_positives.append(false_positives)\n        class_wise_false_negatives.append(false_negatives)\n        class_wise_recalls.append(recall)\n        class_wise_precisions.append(precision)\n        class_wise_specificities.append(specificity)\n        class_wise_ious.append(iou)\n        class_wise_tdrs.append(tdr)\n        class_wise_f1_scores.append(f1_score)\n        classes.append(\"Class \" + str(clas+1))\n        # class_wise_pixel_accuracies.append(pixel_accuracy)\n        \n    total_true_positives = np.sum(class_wise_true_positives)\n    total_true_negatives = np.sum(class_wise_true_negatives)\n    total_false_positives = np.sum(class_wise_false_positives)\n    total_false_negatives = np.sum(class_wise_false_negatives)\n    mean_recall = round(np.average(np.array(class_wise_recalls)), 2)\n    mean_precision = round(np.average(np.array(class_wise_precisions)), 2)\n    mean_specificity = round(np.average(np.array(class_wise_specificities)), 2)\n    mean_iou = round(np.average(np.array(class_wise_ious)), 2)\n    mean_tdr = round(np.average(np.array(class_wise_tdrs)), 2)\n    mean_f1_score = round(np.average(np.array(class_wise_f1_scores)), 2)    \n         \n    class_wise_evaluations = {\"Class\": classes,\n                              \"True Positive Pixels\": class_wise_true_positives,\n                              \"True Negative Pixels\": class_wise_true_negatives,\n                              \"False Positive Pixels\": class_wise_false_positives,\n                              \"False Negative Pixels\": class_wise_false_negatives,\n                              \"Recall\": class_wise_recalls,\n                              \"Precision\": class_wise_precisions,\n                              \"Specificity\": class_wise_specificities,\n                              \"IoU\": class_wise_ious,\n                              \"TDR\": class_wise_tdrs,\n                              \"F1-Score\": class_wise_f1_scores}\n\n    overall_evaluations = {\"Class\": \"All Classes\",\n                        \"True Positive Pixels\": total_true_positives,\n                        \"True Negative Pixels\": total_true_negatives,\n                        \"False Positive Pixels\": total_false_positives,\n                        \"False Negative Pixels\": total_false_negatives,\n                        \"Recall\": mean_recall,\n                        \"Precision\": mean_precision,\n                        \"Specificity\": mean_specificity,\n                        \"IoU\": mean_iou,\n                        \"TDR\": mean_tdr,\n                        \"F1-Score\": mean_f1_score}\n    \n    evaluations = {\"Overall Evaluations\": overall_evaluations, \n                   \"Class-wise Evaluations\": class_wise_evaluations}\n    \n    return evaluations","cd4f2ed2":"def show_evaluations(evaluations, \n                     metrics=[\"Recall\", \"Precision\", \"Specificity\", \"IoU\", \"TDR\", \"F1 Score\"], \n                     class_list=None,\n                     display_evaluations=\"All\"):\n    \"\"\"\n    Returns a pandas dataframe containing specified metrics\n        \n        Arguments:\n            evaluations: evaluation output from the evaluate_model function\n            metrics: a list containing one or more of the following metrics:\n                     'True Positive', 'True Negative', 'False Positive', 'False Negative',\n                     'Recall', 'Precision', 'Specificity', 'F1 Score', 'IoU', 'TDR'\n            display_evaluations: one of 'All' to display both overall and class-wise evaluations,\n                                 'Overall' to display only the overall evaluations,\n                                 'Class-wise' to display only the classwise evaluations.\n            class_list: list or tuple containing names of segmentation class.\n    \"\"\"\n    \n    # Split evaluations into overall and class-wise evaluations\n    overall_evaluations = evaluations[\"Overall Evaluations\"]\n    class_wise_evaluations = evaluations[\"Class-wise Evaluations\"]\n    \n    # Validate list of metrics \n    for metric_id in range(len(metrics)):\n        metric = metrics[metric_id]\n        if metric not in overall_evaluations:\n            raise ValueError(\"'metrics argument' not properly defined. \"\n                            \"Kindly create a list containing one or more of the following metrics: \"\n                             \"'True Positive', 'True Negative', 'False Positive', 'False Negative', \"\n                             \"'Recall', 'Precision', 'Specificity', 'F1 Score', 'IoU', 'TDR'\") \n    \n    # Check if class_list is none\n    if class_list != None and all(isinstance(class_, str) for class_ in class_list):\n        if len(class_list) == len(class_wise_evaluations[\"Class\"]):\n            class_list = [class_list]\n        else:\n            raise ValueError(\"class_list argument' not properly defined. \" \n                             \"List is either shorter or longer than segmentation classes\") \n    else:\n        class_list = [class_wise_evaluations[\"Class\"]]                             \n    \n    # Extract data from the evaluations\n    overall_data = [overall_evaluations[\"Class\"]] + [overall_evaluations[metrics[metric_id]] for metric_id in range(len(metrics))]\n    classwise_data = class_list + [class_wise_evaluations[metrics[metric_id]] for metric_id in range(len(metrics))]\n    overall_data = np.array(overall_data).reshape(1,-1)\n    classwise_data = np.array(classwise_data).transpose()\n    \n    # Determine the type of evaluation report to display\n    if display_evaluations.lower() == \"all\":\n        data = np.concatenate((overall_data, classwise_data), axis=0)\n    elif display_evaluations.lower() == \"overall\":\n        data = overall_data\n    elif display_evaluations.lower() == \"class-wise\" or \"classwise\":\n        data = classwise_data\n    else:\n        raise ValueError(\"Display argument are not properly defined.\"\n                        \"Kindly use 'All' to display both overall and class-wise evaluations.\"\n                        \"Use 'Overall' to display only the overall evaluations.\"\n                        \"Or use 'Class-wise' to display only the class-wise evaluations\")\n\n    \n    # Create evaluation report as a pandas dataframe\n    dataframe = pd.DataFrame(data)\n    dataframe_titles = [\"Class\"] + metrics\n    dataframe.columns = dataframe_titles\n    # dataframe = dataframe.set_index(dataframe_titles[0], col_level=1)\n    \n    return dataframe","0776f221":"model_evaluation_on_train_dataset = evaluate_model(true_train_masks, predicted_train_masks, n_classes=13)\n\nshow_evaluations(model_evaluation_on_train_dataset, \n                 metrics=[\"Recall\", \"Precision\", \"Specificity\", \"IoU\", \"TDR\", \"F1-Score\"], \n                 class_list=None, \n                 display_evaluations=\"All\")","1a14606a":"model_evaluation_on_validation_dataset = evaluate_model(true_validation_masks, predicted_validation_masks, n_classes=13)\n\nshow_evaluations(model_evaluation_on_validation_dataset, \n                 metrics=[\"Recall\", \"Precision\", \"Specificity\", \"IoU\", \"TDR\", \"F1-Score\"], \n                 class_list=None, \n                 display_evaluations=\"All\")","8830b083":"model_evaluation_on_test_dataset = evaluate_model(true_test_masks, predicted_test_masks, n_classes=13)\n\nshow_evaluations(model_evaluation_on_test_dataset, \n                 metrics=[\"Recall\", \"Precision\", \"Specificity\", \"IoU\", \"TDR\", \"F1-Score\"], \n                 class_list=None, \n                 display_evaluations=\"All\")","63a0c3fa":"model = load_model('carla-image-segmentation-model.h5')","c14988d6":"def create_mask(pred_mask):\n    pred_mask = tf.argmax(pred_mask, axis=-1)\n    pred_mask = tf.expand_dims(pred_mask, axis=-1)\n    \n    return pred_mask[0]","b2a8974f":"def display(display_list):\n    plt.figure(figsize=(18, 18))\n\n    title = ['Input Image', 'True Mask', 'Predicted Mask']\n\n    for i in range(len(display_list)):\n        plt.subplot(1, len(display_list), i+1)\n        plt.title(title[i])\n        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n        plt.axis('off')\n    plt.show()","b5810122":"def show_predictions(dataset, num):\n    \"\"\"\n    Displays the first image of each of the num batches\n    \"\"\"\n    if dataset:\n        for image, mask in dataset.take(num):\n            pred_mask = model.predict(image)\n            display([image[0], mask[0], create_mask(pred_mask)])\n    else:\n        display([sample_image, sample_mask,\n             create_mask(model.predict(sample_image[tf.newaxis, ...]))])","678f864b":"show_predictions(train_dataset, 6)","feb8db58":"show_predictions(validation_dataset, 6)","a3354fcf":"show_predictions(test_dataset, 6)","67be21ca":"<a class=\"anchor\" id=\"2\" name=\"2\"><\/a>\n## **2. Data Preparation**\n\nThe Lyft Udacity Semantic Segmentation for Self-driving Cars Challenge data (images and masks) is splitted across five directories (dataA, dataB, dataC, dataD, and dataE). As part of the data preparation step, we will load images and masks from all the five directories and carry out the a few preprocessing steps to ensure we provide our model with quality dataset.","419457cc":"<a class=\"anchor\" id=\"4-2\" name=\"4-2\"><\/a>\n### **4.2. Evaluate Predicted Segmentations**","723f168c":"##### **3.1.2. Define a function for a decoding block (This function will merge the skip-connection input with the previous layer, process it, and return an output)** ","4d5e1c83":"<a class=\"anchor\" id=\"5-4\" name=\"5-4\"><\/a>\n### **5.4. Predict and compare masks of images in the test set**","2cdf97cd":"<a class=\"anchor\" id=\"3-2\" name=\"3-2\"><\/a>\n### **3.2. Model Training**","a873b9bd":"#### **4.2.2. Evaluate predicted segmentations of the validation images**","a3023e23":"##### **3.1.1. Define a function for an encoding block (This function will return the next layer output and the skip connection output for the corresponding block in the model)** ","d3321a96":"<a class=\"anchor\" id=\"2-1\" name=\"2-1\"><\/a>\n### **2.1. Load the images and masks from their directories**\n\nIn this data preparation step, we will:\n1. Create 2 lists containing the paths of images and masks\n2. Split the lists into training, validation and test sets","43287345":"**Create lists of image and mask paths by initializing the function above**","25009e27":"## **Result Overview**: \n\n| <br\/><font size=\"3\"><b> Dataset <\/b><\/font><br\/><br\/>  | <br\/><font size=\"3\"><b> Model Accuracy <\/b><\/font><br\/><br\/> | <br\/><font size=\"3\"><b> Mean Recall <\/b><\/font><br\/><br\/> | <br\/><font size=\"3\"><b> Mean Precision <\/b><\/font><br\/><br\/> | <br\/><font size=\"3\"><b> Mean Specificity <\/b><\/font><br\/><br\/> | <br\/><font size=\"3\"><b> Mean Intersection over Union <\/b><\/font><br\/><br\/> | <br\/><font size=\"3\"><b> Mean True Detection Rate <\/b><\/font><br\/><br\/> | <br\/><font size=\"3\"><b> Mean F1 Score\/Dice Co-efficient <\/b><\/font><br\/><br\/> | \n| :-- | :-- |  :-- |  :-- |  :-- |  :-- |  :-- |  :-- |\n| <font size=\"3\"><b>Training Dataset<\/b><\/font><br\/><br\/> | <font size=\"3\"> 99% <\/font><br\/><br\/> |  <font size=\"3\"> 93% <\/font><br\/><br\/> |  <font size=\"3\"> 95% <\/font><br\/><br\/> |  <font size=\"3\"> 100% <\/font><br\/><br\/> |  <font size=\"3\"> 89% <\/font><br\/><br\/> |  <font size=\"3\"> 93% <\/font><br\/><br\/> |  <font size=\"3\"> 94% <\/font><br\/><br\/> |\n| <font size=\"3\"><b>Validation Dataset<\/b><\/font><br\/><br\/> | <font size=\"3\"> 98% <\/font><br\/><br\/> |  <font size=\"3\"> 89% <\/font><br\/><br\/> |  <font size=\"3\"> 94% <\/font><br\/><br\/> |  <font size=\"3\"> 100% <\/font><br\/><br\/> |  <font size=\"3\"> 85% <\/font><br\/><br\/> |  <font size=\"3\"> 89% <\/font><br\/><br\/> |  <font size=\"3\"> 91% <\/font><br\/><br\/> |\n| <font size=\"3\"><b>Test Dataset<\/b><\/font><br\/><br\/> | <font size=\"3\"> 98% <\/font><br\/><br\/> |  <font size=\"3\"> 92% <\/font><br\/><br\/> |  <font size=\"3\"> 93% <\/font><br\/><br\/> |  <font size=\"3\"> 100% <\/font><br\/><br\/> |  <font size=\"3\"> 87% <\/font><br\/><br\/> |  <font size=\"3\"> 92% <\/font><br\/><br\/> |  <font size=\"3\"> 92% <\/font><br\/><br\/> |\n","eaa6b704":"<a class=\"anchor\" id=\"5-3\" name=\"5-3\"><\/a>\n### **5.3. Predict and compare masks of images in the validation set**","1aa53819":"## **Table of Contents**: \n<font size=\"3\">\n\n- [**1 - Import Required Packages**](#1)\n\n    \n- [**2 - Data Preparation**](#2)\n    - [2.1. Load the images and masks from their directories](#2-1)\n    - [2.2.  Create a data pipeline to read and preprocess our data](#2-2)\n\n    \n- [**3 - Model Architecture and Training**](#3)\n    - [3.1. - U-Net Model design](#3-1)\n    - [3.2. - Model training](#3-2)    \n\n    \n- [**4 - Model Evaluation**](#4)\n    - [4.1. - Create Segmentations\/Masks of Images in our Dataset](#4-1)\n    - [4.2. - Evaluate Predicted Segmentations](#4-2)\n\n    \n- [**5 - Predict image segmentations using the trained Model**](#5)\n    - [5.1. - Create functions to preprocess selected images and display their true state, true mask and predicted mask](#5-1)\n    - [5.2. - Prediction on the train set](#5-2)\n    - [5.3. - Prediction on the validation set](#5-3)\n    - [5.4. - Prediction on the test set](#5-4)    \n\n    \n<font>","0c68a59d":"<a class=\"anchor\" id=\"3-1\" name=\"3-1\"><\/a>\n\n### **3.1. U-Net Model Design**\n\nTo design our model, we will carry out the following steps\n1. Define a function for an encoding block. The function will return the next layer output and the skip connection output for the corresponding block in the model\n2. Define a function for a decoding block. This function will merge the skip-connection input with the previous layer, process it, and return an output\n3. Develop a model using both the encoding and decoding blocks output","94f08f2d":"#### **4.2.1. Evaluate predicted segmentations of the training images**","b32b79f2":"#### **2.2.3. Create data pipelines for the training, validation and test sets using both functions**","8e89e477":"<a class=\"anchor\" id=\"2-2\" name=\"2-2\"><\/a>\n### **2.2. - Create a data pipeline to read and preprocess our data**\n\nWe will be using the tf.data.Dataset API to load our images and masks for our model to process. The Dataset API allows us to build an asynchronous, highly optimized data pipeline to prevent our GPU from data starvation. It loads data from the disk (images or text), applies optimized transformations, creates batches and sends it to the GPU. Unlike former data pipelines made the GPU, the Dataset API wait for the CPU to load the data, leading to performance issues.\n\nTo do this, we will \n1. Create a function to read image and mask paths and return equivalent arrays\n2. Create a data generator function to read and load images and masks in batches\n3. Create data pipelines for the training, validation and test sets using both functions\n4. Preview sample images and their segmentations from the three dataset categories","e87e6d62":"<a class=\"anchor\" id=\"5-2\" name=\"5-2\"><\/a>\n### **5.2. Predict and compare masks of images in the training set**","e663727e":"#### **2.1.2. Split the image and mask paths into training, validation, and test sets**","c86e0b87":"## **Project Overview**: \n\n| <br\/><font size=\"3\"><b> Focus <\/b><\/font><br\/><br\/>  | <br\/><font size=\"3\"><b> Description <\/b><\/font><br\/><br\/> |\n| :-- | :-- |\n| <br\/><font size=\"3\"> <b>Project Title<\/b> <\/font>  <br\/><br\/>| <br\/><font size=\"3\"> CARLA Image Semantic Segmentation with U-Net<\/font> <br\/><br\/>|\n| <br\/><font size=\"3\"> <b>Project Type<\/b> <\/font> <br\/><br\/>| <br\/><font size=\"3\">Image Segmentation (Semantic Segmenetation) <\/font> <br\/><br\/>|\n| <br\/><font size=\"3\"><b>Project Objectives<\/b> <\/font>  <br\/><br\/>| <br\/><font size=\"3\">1. Train a model to predict semantic segmentations of CARLA images<br\/>  2. Evaluate model performance using various metrics <br\/>3. Predict masks\/segmentations of CARLA images using the model and compare predictions with ground-truth masks <\/font> <br\/><br\/>|\n| <br\/><font size=\"3\"> <b>Dataset Overview <\/b> <\/font> <br\/><br\/>| <br\/><font size=\"3\"> This dataset provides data images and labeled semantic segmentations captured via CARLA self-driving car simulator. The data which was generated as part of the 2018 Lyft Udacity Perception Challenge consists of **5000** images and their semantic segmentations. <\/font> <br\/><br\/>|\n| <br\/><font size=\"3\"> <b> Model Evaluation Metrics<\/b> <\/font> <br\/><br\/>| <br\/><font size=\"3\"> Model Accuracy, Mean Precision, Mean Recall, Mean Specificity, Mean Intersection over Union (mIoU), Mean True Detection Rate (TDR), Mean F1 Score\/Dice Co-efficient (TDR) <\/font> <br\/><br\/>|\n| <font size=\"3\"> <b>Image Segmentation Model Type<\/b> <\/font> | <br\/><font size=\"3\"> <a href=\"https:\/\/arxiv.org\/pdf\/1505.04597.pdf\"> <b> U-Net Architecture<\/b><\/a> <\/font> <br\/><br\/>|\n| <br\/><font size=\"3\"> <b>Major Libraries Used<\/b> <\/font> <br\/><br\/>| <br\/><font size=\"3\"> <a href=\"https:\/\/keras.io\"><b>Keras<\/b><\/a>, <a href=\"https:\/\/imageio.readthedocs.io\/\"><b>ImageIO<\/b><\/a>, <a href=\"https:\/\/scikit-learn.org\"><b> Scikit-Learn <\/b><\/a>, <a href=\"https:\/\/numpy.org\"><b>Numpy<\/b><\/a>, <a href=\"https:\/\/matplpotlib.org\"><b>Matplotlib<\/b><\/a><\/font> <br\/><br\/>|\n\n<br\/>","68c83f3f":"<a class=\"anchor\" id=\"3-3\" name=\"3-3\"><\/a>\n### **3.3. Compute Model Accuracy**","0e3297e2":"#### **2.1.1. Create lists containing the paths of images and masks**\n\nIn this step, we will\n\n* Create a list that contains all the paths to all directories in the main directory (a list that contains the path to dataA, dataB, dataC, dataD, and dataE)\n* Create a function to iterate over all the direcory paths where our data are located (list in 1.) and return the list of the image paths in those directories. \n* Create lists of image and mask paths by initializing the function above\n* Preview some masked and unmasked images by reading them from their paths","43b3ad93":"<a class=\"anchor\" id=\"5-1\" name=\"5-1\"><\/a>\n### **5.1. Create functions to preprocess selected images and display their true state, true mask and predicted mask**\n\nIn this step, we will:\n1. Load our model\n2. Define a function to create new masks using our model\n3. Define a function to display outputs of this process: an input image, its true mask, and its predicted mask.\n4. Define a function to select images from a specified dataset and return the images, their true masks and their predicted masks.","d8eea4f8":"<a class=\"anchor\" id=\"1\" name='1'><\/a>\n## **1. Import Required Packages**","c7d3fca9":"<a class=\"anchor\" id=\"3\" name=\"3\"><\/a>\n## **3. Model Architecture and Training**\nWe will using a the **U-Net architecture** to train our semantic segmentation model. The U-Net model was named for its U-shape architecture, and it was originally created for biomedical image segmentation tasks in 2015. However, the model has become a very popular choice for other semantic segmentation tasks. \n\nU-Net builds on a previous architecture called the Fully Convolutional Network, or FCN, which replaces the dense layers found in a typical CNN with a transposed convolution layer that upsamples the feature map back to the size of the original input image, while preserving the spatial information. This is necessary because the dense layers destroy spatial information (the \"where\" of the image), which is an essential part of image segmentation tasks. An added bonus of using transpose convolutions is that the input size no longer needs to be fixed, as it does when dense layers are used. \n\nThe U-Net architecture consists of:\n\n**1. Contracting path (Encoder containing downsampling steps)**:\n\nThe contracting path follows a regular CNN architecture, with convolutional layers, their activations, and pooling layers to downsample the image and extract its features. Images are first fed through several convolutional layers which reduce height and width, while growing the number of channels. \n\nIn detail, it consists of the repeated application of two 3 x 3 unpadded convolutions, each followed by a rectified linear unit (ReLU) and a 2 x 2 max pooling operation with stride 2 for downsampling. At each downsampling step, the number of feature channels is doubled.\n\nDuring the contracting process, convolution outputs are stored in a separate variable before size reduction (pooling of features). This is passed to the expanding blocks during the decoding process as feature map.\n\n\n**2. Expanding path (Decoder containing upsampling steps)**:\n\nThe expanding path performs the opposite operation of the contracting path, growing the image back to its original size, while shrinking the channels gradually. \n\nIn detail, each step in the expanding path upsamples the feature map, followed by a 2 x 2 convolution (the transposed convolution or upsampling). This transposed convolution halves the number of feature channels, while growing the height and width of the image.\n\nNext is a concatenation with the correspondingly cropped feature map from the contracting path, and two 3 x 3 convolutions, each followed by a ReLU.\n\n\n**3. Final Feature Mapping Block**: In the final layer, a 1x1 convolution is used to map each 64-component feature vector to the desired number of classes. The channel dimensions from the previous layer correspond to the number of filters used, so when you use 1x1 convolutions, you can transform that dimension by choosing an appropriate number of 1x1 filters. When this idea is applied to the last layer, you can reduce the channel dimensions to have one layer per class. \n\nThe U-Net network has 23 convolutional layers in total. \n<center><img src=\"https:\/\/i.ibb.co\/0287bZ1\/U-Net.webp\" alt=\"U-Net\" border=\"0\"><\/center>\n","5f7be0e4":"##### **5.1.4. Define a function to select images from a specified dataset and return the images, their true masks and their predicted masks** ","366128b9":"**Preview random masked and unmasked images by reading them from their paths**","01ac9f16":"<a class=\"anchor\" id=\"4\" name=\"4\"><\/a>\n## **4. Model Evaluation**\n\nModel Evaluation is an integral part of the model development process. It helps to find the best model that represents our data and how well the chosen model will work in the future. For classification tasks, precision and recall are the popular choice metrics used in addition with model accuracy to evaluate model performance since model accuracy is not always sufficient to judge if a model is optimal or not (especially if our dataset is skewed). The same rule applies to most dense prediction tasks like image segmentation where the goal is to simplify and\/or change the representation of an image into classes that is more meaningful and easier to analyze.\n \n\nSince the goal of our model is to partition an input image into various classes, it is often difficult to know if our model struggles to optimally partition one or more classes since it doesn't always reflect in the model accuracy, neither can it easily detected by the eyes. Hence, there is a need for supplementary metrics to evaluate model performance.\n\nIn this project, we will be using recall,precision, specificity, true detection rate (TDR), Intersection over Union (IoU), and F1-score as supplementary metrics to evaluate our model performance. These metrics were computed by identifying the variables true positive (TP), true negative (TN), false positive (FP), and false-negative (FN) by calculating the confusion matrix between the predicted segmentations and the ground truth segmentations. The expressions for these metrics are defined as:\n\n1. Precision = TP\/(TP + FP)\n2. Recall\/Sensitivity = TP\/(TP + FN)\n3. Specificity = TN\/(TN + FP)\n4. True Detection Rate (TDR) = 1 - (FN\/(TP + FN))\n5. Intersection over Union (IoU)\/Jaccard Similarity  = TP\/(TP + FP + FN)\n6. F1-score(JS)\/Dice coefficient = 2 * ((Precision * Recall)\/(Precision + Recall))\n\n<br\/>\n\nTo carry out these evaluations, we will:\n1. Create segmentations\/masks of images in our dataset\n2. Evaluate predicted segmentations\n","36c7f2db":"##### **5.1.2. Define a function to create new masks using our model** ","c438f2e9":"**Create a function to iterate over all the direcory paths where our data are located (list in 2.1.1.) and return the list of the image paths in those directories**","38bd1a72":"#### **2.2.2. Create a data generator function to read and load images and masks in batches**","e4b933f0":"##### **5.1.3. Define a function to display outputs of this process: an input image, its true mask, and its predicted mask** ","e7af674a":"#### **4.2.3. Evaluate predicted segmentations of the test images**","f42f71be":"##### **3.1.3. Develop a model using both the encoding and decoding blocks output** ","7644fb4b":"#### **2.2.1. Create a function to read image and mask paths and return equivalent arrays**\n\nThe **read_image** function will\n1. Read an image and its mask from their paths\n2. Convert the digital image and its mask to image arrays \n3. Normalize the datasets\n4. Resize the image and its masks to a desired dimension","ca997eec":"##### **5.1.1. Load our model** ","165c37ce":"<a class=\"anchor\" id=\"5\" name=\"5\"><\/a>\n## **5. Predict image segmentations using the trained Model**\n\nThough, our model have pretty decent accuracies and IoUs on our training, validation, and test datasets, visualizing how it performs on these datasets could give us additional gains.\n\nHence, we will\n\n1. Create a function to preprocess selected images and display their true state, true mask and predicted mask\n2. Predict and compare masks of images in the training set\n3. Predict and compare masks of images in the validation set\n4. Predict and compare masks of images in the test set","d6a2cc2b":"<a class=\"anchor\" id=\"4-1\" name=\"4-1\"><\/a>\n### **4.1. Create Segmentations\/Masks of Images in our Dataset**","9b51008d":"**A. Create a list that contains all the paths to all directories in the main directory (a list that contains the path to dataA, dataB, dataC, dataD, and dataE)**"}}