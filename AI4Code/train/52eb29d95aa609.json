{"cell_type":{"e8f3f7c1":"code","0db30a06":"code","6e33fcbb":"code","95d449d0":"code","e801285e":"code","12690b33":"code","bbc7cb67":"code","67fe9a99":"code","db9e3f7a":"code","b4f4f334":"code","4eff46c5":"code","a637a14e":"code","ab06f55d":"code","62a1b178":"code","166e85a4":"markdown","75167a48":"markdown","d1aab602":"markdown","ba76da17":"markdown","b8630380":"markdown","d0146585":"markdown","9edb8a73":"markdown"},"source":{"e8f3f7c1":"# imports\nimport os\nimport string\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nfrom sklearn import model_selection\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset\nimport transformers\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nimport tokenizers","0db30a06":"class config:\n    MAX_LEN = 192\n    TRAIN_BATCH_SIZE = 32\n    VALID_BATCH_SIZE = 8\n    EPOCHS = 5\n    TRAINING_FILE = '..\/input\/tweet-sentiment-extraction\/train.csv'\n    TEST_FILE = '..\/input\/tweet-sentiment-extraction\/test.csv'\n    MODEL_PATH = 'model.bin'\n    ROBERTA_PATH = \"..\/input\/roberta-base\"\n    TOKENIZER = tokenizers.ByteLevelBPETokenizer(\n        vocab_file=f\"{ROBERTA_PATH}\/vocab.json\", \n        merges_file=f\"{ROBERTA_PATH}\/merges.txt\", \n        lowercase=True,\n        add_prefix_space=True\n    )","6e33fcbb":"class TweetModel(transformers.BertPreTrainedModel):\n    def __init__(self, conf):\n        super(TweetModel, self).__init__(conf)\n        self.roberta = transformers.RobertaModel.from_pretrained(config.ROBERTA_PATH, config=conf)\n        self.drop_out = nn.Dropout(0.1)\n        self.l0 = nn.Linear(768 * 2, 2)\n        torch.nn.init.normal_(self.l0.weight, std=0.02)\n        # this is to initialize the weights of the matrix that would convert \n        # (batch_size, max_len, 2*768) to (batch_size, max_len, 1) with std=0.02 \n    \n    def forward(self, ids, mask, token_type_ids):\n        _, _, out = self.roberta(\n            ids,\n            attention_mask=mask,\n            token_type_ids=token_type_ids\n        )\n        # out dim -> (12, batch_size, max_len, 768)\n        # 12 denotes the 12 hidden layers of roberta\n\n        out = torch.cat((out[-1], out[-2]), dim=-1)\n        # out dim -> (batch_size, max_len, 2*768)\n        out = self.drop_out(out)\n        logits = self.l0(out)\n        # logits dim -> (batch_size, max_len, 2)\n\n        start_logits, end_logits = logits.split(1, dim=-1)\n        # start_logits and end_logits dim -> (batch_size, max_len, 1)\n\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n        # start_logits and end_logits dim -> (batch_size, max_len)\n\n        return start_logits, end_logits","95d449d0":"def process_data(tweet, selected_text, sentiment, tokenizer=config.TOKENIZER, max_len=config.MAX_LEN):\n    # roberta requires the text to have a prefix space at the beginning\n    tweet = \" \" + \" \".join(str(tweet).split(\" \"))\n    selected_text = \" \" + \" \".join(str(selected_text).split(\" \"))\n\n    # getting initial and final index of selected_text within the tweet\n    len_selected = len(selected_text) - 1\n    idx1 = idx2 = None\n    for idx, letter in enumerate(selected_text):\n        if (tweet[idx] == selected_text[1]) and (\" \" + tweet[idx: idx+len_selected] == selected_text):\n            idx1 = idx\n            idx2 = idx1 + len_selected - 1\n            break\n    \n    # making character targets\n    if idx1!=None and idx2!=None:\n        char_targets = [0] * len(tweet)\n        for i in range(idx1, idx2+1):\n            char_targets[i] = 1\n    else:\n        char_targets = [1] * len(tweet)\n\n    # encoding using pretrained tokenizer\n    tok_tweet = tokenizer.encode(tweet)\n    ids = tok_tweet.ids\n    mask = tok_tweet.attention_mask\n    type_ids = tok_tweet.type_ids\n\n    # getting indexes of tokens containing character in selected_text\n    target_idx = []\n    for i, (offset1, offset2) in enumerate(tok_tweet.offsets):\n        if sum(char_targets[offset1: offset2])>0:\n            target_idx.append(i)\n\n    # we just need the indexes of the start and end tokens as we are using \n    # nn. CrossEntropy as loss\n    start_target = target_idx[0]\n    end_target = target_idx[-1]\n\n    # token ids of sentiment as present in our vocab hard coded here\n    sentiment_ids = {\n        'positive': 1313,\n        'negative': 2430,\n        'neutral': 7974\n    }\n\n    # adding special tokens\n    ids = [0] + [sentiment_ids[sentiment]] + [2] + [2] + ids + [2]\n    mask = [1] * len(ids)\n    type_ids = [0] * len(ids)\n    offsets = [(0, 0)] * 4 + tok_tweet.offsets\n    start_target += 4\n    end_target += 4\n\n    # padding\n    padding_len = max_len - len(ids)\n    if padding_len>0:\n        ids = ids + [1] * padding_len\n        mask = mask + [0] * padding_len\n        type_ids = type_ids + [0] * padding_len\n        offsets = offsets + [(0, 0)] * padding_len\n\n    return {\n        'ids': ids,\n        'mask': mask,\n        'token_type_ids': type_ids,\n        'targets_start': start_target,\n        'targets_end': end_target,\n        'orig_tweet': tweet,\n        'orig_selected': selected_text,\n        'sentiment': sentiment,\n        'offsets': offsets,\n        'padding_len': padding_len\n    }","e801285e":"class TweetDataset(Dataset):\n    def __init__(self, tweet, sentiment, selected_text):\n        self.tweet = tweet\n        self.sentiment = sentiment\n        self.selected_text = selected_text\n\n    def __len__(self):\n        return len(self.tweet)\n\n    def __getitem__(self, item):\n        # processing data\n        data = process_data(\n            self.tweet[item], \n            self.selected_text[item], \n            self.sentiment[item]\n        )\n\n        # returning tensors\n        return {\n            'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n            'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n            'targets_start': torch.tensor(data[\"targets_start\"], dtype=torch.long),\n            'targets_end': torch.tensor(data[\"targets_end\"], dtype=torch.long),\n            'orig_tweet': data[\"orig_tweet\"],\n            'orig_selected': data[\"orig_selected\"],\n            'sentiment': data[\"sentiment\"],\n            'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long),\n            'padding_len': data[\"padding_len\"]\n        }","12690b33":"def train_fn(data_loader, model, optimizer, device, scheduler):\n    model.train()\n    for d in tqdm(data_loader):\n        # getting data\n        ids = d['ids']\n        token_type_ids = d['token_type_ids']\n        mask = d['mask']\n        targets_start = d['targets_start']\n        targets_end = d['targets_end']\n\n        # putting them into gpu\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets_start = targets_start.to(device, dtype=torch.float)\n        targets_end = targets_end.to(device, dtype=torch.float)\n\n        # zeroing gradients\n        optimizer.zero_grad()\n        # getting outputs\n        o1, o2 = model(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n        # calulating loss\n        loss = loss_fn(o1, o2, targets_start, targets_end)\n        # calculating gradients\n        loss.backward()\n        # updating model parameters\n        optimizer.step()\n        # stepping learning rate scheduler\n        scheduler.step()","bbc7cb67":"def eval_fn(data_loader, model, device, tokenizer=config.TOKENIZER):\n    model.eval()\n    # below array will store the respective data\n    all_ids = []\n    start_idx = []\n    end_idx = []\n    orig_selected = []\n    padding_len = []\n\n    for d in data_loader:\n        # getting data\n        ids = d['ids']\n        token_type_ids = d['token_type_ids']\n        mask = d['mask']\n        selected_text = d['orig_selected']\n        pad_len = d['padding_len']\n\n        # putting them in gpu\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n\n        # getting output\n        o1, o2 = model(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n\n        # adding to array to use latter\n        # also removing stuff from gpu\n        all_ids.append(ids.cpu().detach().numpy())\n        start_idx.append(torch.sigmoid(o1).cpu().detach().numpy())\n        end_idx.append(torch.sigmoid(o2).cpu().detach().numpy())\n        orig_selected.extend(selected_text)\n        padding_len.extend(pad_len)\n\n    # fixing dimensions\n    start_idx = np.vstack(start_idx)\n    end_idx = np.vstack(end_idx)\n    all_ids = np.vstack(all_ids)\n\n    # to store jaccard score to print mean of it latter\n    jaccards = []\n\n    # getting predicted text and calculating jaccard\n    for i in range(0, len(start_idx)):\n        start_logits = start_idx[i][4: -padding_len[i]-1]\n        end_logits = end_idx[i][4: -padding_len[i]-1]\n        this_id = all_ids[i][4: -padding_len[i]-1]\n\n        idx1 = idx2 = None\n        max_sum = 0\n        for ii, s in enumerate(start_logits):\n            for jj, e in enumerate(end_logits):\n                if  s+e > max_sum:\n                    max_sum = s+e\n                    idx1 = ii\n                    idx2 = jj\n\n        this_id = this_id[idx1: idx2+1]\n        predicted_text = tokenizer.decode(this_id, skip_special_tokens=True)\n        predicted_text = predicted_text.strip()\n        sel_text = orig_selected[i].strip()\n\n        jaccards.append(jaccard(predicted_text, sel_text))\n\n    # returning mean jaccard\n    return np.mean(jaccards)","67fe9a99":"# jaccard function as mentioned in evaluation section of the contest\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","db9e3f7a":"# loss function. Play around with it and see what works best\ndef loss_fn(o1, o2, t1, t2):\n    l1 = nn.CrossEntropyLoss()(o1, t1.long())\n    l2 = nn.CrossEntropyLoss()(o2, t2.long())\n    return l1 + l2","b4f4f334":"def run():\n    # reading train.csv\n    dfx = pd.read_csv(config.TRAINING_FILE).dropna().reset_index(drop=True)\n\n    # spliting into training and validation set\n    df_train, df_valid = model_selection.train_test_split(\n        dfx,\n        test_size=0.1,\n        random_state=42,\n        stratify=dfx.sentiment.values\n    )\n\n    df_train = df_train.reset_index(drop=True)\n    df_valid = df_valid.reset_index(drop=True)\n\n    # using TweetDataset function as coded above\n    train_dataset = TweetDataset(\n        tweet=df_train.text.values,\n        sentiment=df_train.sentiment.values,\n        selected_text=df_train.selected_text.values\n    )\n\n    valid_dataset = TweetDataset(\n        tweet=df_valid.text.values,\n        sentiment=df_valid.sentiment.values,\n        selected_text=df_valid.selected_text.values\n    )\n\n    # making pytorch dataloaders\n    train_data_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=config.TRAIN_BATCH_SIZE,\n        num_workers=4\n    )\n\n    valid_data_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=config.VALID_BATCH_SIZE,\n        num_workers=1\n    )\n\n    # making a instance of the model and putting it into gpu\n    device = torch.device(\"cuda\")\n    conf = transformers.RobertaConfig.from_pretrained(config.ROBERTA_PATH)\n    conf.output_hidden_states = True\n    model = TweetModel(conf)\n    model.to(device)\n    \n    # explicitly going through model parameters and removing weight decay\n    # from a few layers \n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    optimizer_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n    ]\n\n    # Coding out the optimizer and scheduler\n    num_train_steps = int(len(df_train) \/ config.TRAIN_BATCH_SIZE * config.EPOCHS)\n    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=0,\n        num_training_steps=num_train_steps\n    )\n\n    model = nn.DataParallel(model)\n\n    # saving model when we have best jaccard\n    best_jaccard = 0\n    for epoch in range(config.EPOCHS):\n        train_fn(train_data_loader, model, optimizer, device, scheduler)\n        jaccard = eval_fn(valid_data_loader, model, device)\n        print(f\"Jaccard Score = {jaccard}\")\n        if jaccard > best_jaccard:\n            torch.save(model.state_dict(), config.MODEL_PATH)\n            best_jaccard = jaccard","4eff46c5":"run()","a637a14e":"# prediction function having same logic as eval_fn\ndef predict(tweet, sentiment):\n    data = process_data(tweet, None, sentiment)\n\n    ids = data['ids']\n    token_type_ids = data['token_type_ids']\n    mask = data['mask']\n    padding_len = data['padding_len']\n\n    ids = torch.tensor([ids], dtype=torch.long)\n    token_type_ids = torch.tensor([token_type_ids], dtype=torch.long)\n    mask = torch.tensor([mask], dtype=torch.long)\n\n    ids = ids.to('cuda', dtype=torch.long)\n    token_type_ids = token_type_ids.to('cuda', dtype=torch.long)\n    mask = mask.to('cuda', dtype=torch.long)\n\n    start_logits, end_logits = model(\n        ids=ids,\n        mask=mask,\n        token_type_ids=token_type_ids\n    )\n\n    start_logits = start_logits.cpu().detach().numpy()\n    end_logits = end_logits.cpu().detach().numpy()\n    ids = ids.cpu().detach().numpy()\n    mask = mask.cpu().detach().numpy()\n    token_type_ids = token_type_ids.cpu().detach().numpy()\n\n    start_logits = start_logits[0][4: -padding_len-1]\n    end_logits = end_logits[0][4: -padding_len-1]\n    ids = ids[0][4: -padding_len-1]\n\n    idx1 = idx2 = None\n    max_sum = 0\n    for i, s in enumerate(start_logits):\n        for j, e in enumerate(end_logits):\n            if  s+e > max_sum:\n                max_sum = s+e\n                idx1 = i\n                idx2 = j\n    \n    if idx1==None or idx2==None:\n        return tweet\n\n    ids = ids[idx1: idx2+1]\n    predicted_text = config.TOKENIZER.decode(ids, skip_special_tokens=True)\n    predicted_text = predicted_text.strip()\n    \n    return predicted_text","ab06f55d":"# loading the model and putting it into gpu\nconf = transformers.RobertaConfig.from_pretrained(config.ROBERTA_PATH)\nconf.output_hidden_states = True\nmodel = TweetModel(conf)\nmodel = nn.DataParallel(model)\nmodel.load_state_dict(torch.load(config.MODEL_PATH))\nmodel.eval()","62a1b178":"# making submission\ntest = pd.read_csv(config.TEST_FILE)\ntest['selected_text'] = [predict(test.text.values[i], test.sentiment.values[i]) for i in tqdm(range(len(test)))]\nsubmission = test.drop(columns=['text', 'sentiment'])\nsubmission.to_csv('submission.csv', index=False)","166e85a4":"Now this function prepares our data so that we can put it into the model. Now we need to know how we need to tokenize. For this I have made another kernel so do check it out. But long story short the text need to be tokenized as follows:\n```\n[start token] seniment [seperation or end token] [seperation or end token] tweet [seperation or end token]\n```\nAnd also we need to know the tokens for a few things:\n```\n    [start token] : [0]\n    [seperation or end token] : [2]\n    positive : [1313]\n    negative : [2430]\n    neutral : [7974]\n    padding token: [1]\n```\n\nSo basically our tokens would look like this:\n```\n[0, sentiment_token, 2, 2, text_tokens, 2, 1, 1, .....]\n```\n\nOur token_type_ids need to be just a tensor of zeros because roberta doesn't depend on them.\n\nOur attention_mask need to have 1 in all places expect for padding tokens where it is 0.\n\nThe above logic is one that I have learnt from Abhishek Thakur's kernel on training bert for this contest. Here we first make a array having zeros and having length equal to the text. Then we change those indexes to 1 which are present in the selected text. Then using the offsets of our tokenizer to return start and end indexes. Go through the code and it will make a lot of sense.\n\nThen finally we have padded it.\n\nGoing character wise ensures that we don't miss out the answer even if it starts in the middle of the word.\n\nFinally all of the arrays were padded and returned as a dict.\n","75167a48":"The above function is my validation loop that just takes the output of the model and finds the $idx1$\nand $idx2$ such that $idx1+idx2$ is maximum and $idx1<idx2$. This is the same logic as the prediction function later down the line.","d1aab602":"Here notice the line torch.nn.init.normal_(self.l0.weight, std=0.02). This is a very important line. It initializes the weights of the linear layer with bunch of numbers which have standard deviation equal to 0.02. The way we initialize our weights is very important and can be the difference between the weights converging to a perfect fit after training or to explode and become completely untrainable. If you checkout the config.json file of the pretrained model then you will find that we need to have std equal to 0.02 for training this model.\n\nSo let us understand this model line by line. The model will take in ids which would be of the dimension (1, batch_size, max_len). The max_len here refers to same one as in the config class. It would also take mask and token_type_ids of the same dimension. RoBERTa doesn't need token_type_ids so we can just pass in a tensor containing zeros.\n\nThen in the config.json file of the pretrained roberta we have set output_hidden_layer to true. So this would give an additional output containing the hidden layers of RoBERTa. We are using RoBERTa base which has 12 hidden layers and each of the hidden layer has same dimension. Each hidden layer has 768 number output for each input id. So given the input as mentioned above each hidden layer would have dimension (batch_size, max_len, 768). 768 is characteristic to roberta base.\n\nNow what we do is that we take the last two hidden layers and concatenate them along the -1 dimension (which is the row dimension). So we end up with (batch_size, max_len, 2*768). Now when we pass this tensor through the linear layer we get tensor of dimension (batch_size, max_len, 2). We split it along -1 axis (which is the row axis) and end up with two (batch_size, max_len, 1) tensors.\n\nNow we squeeze them along -1 axis which would lead to a two (batch_size, max_len) tensors which is exactly what we wanted. Now we have two tensors start_logits and end_logits which give signify the chance of the particular index to be the start index and end index respectively.","ba76da17":"The above config class is made so that we can do adjustments in just this config class and train the model according to this class. It makes it very convenient to handle relative paths and also to handle batch size, epochs, tokenizer and the very model itself. One change here and the we can train the any data with any tokenizer and any model for any number of epochs and batch size.","b8630380":"This kernel is from a beginner who worked hard to learn to fine tune RoBERTa and don't want you guys to go thorugh the same trouble. I learned a lot from Abhishek Thakur's kernels and youtube page and the logic of this kernel may seem to be similar to his. But I made changes in eval function and prediction and other places. These changes have made the kernel more simple and easy to understand for beginners (which I am myself :P). The purpose of this kernel is to teach beginners how to train RoBERTa so they can change the logic and make it better to get results. I am not hoping for any great result on this kernel. Just want to pass on my knowledge on this topic in simple words. I am just a beginner so feel free to correct me in the comments :D","d0146585":"The above is a normal pytorch Dataset where we have just overloaded the __len and __getitem function and returned tensors and other necessary data that we would need later.","9edb8a73":"The above function is our training loop. Just basic pytorch stuff. Zeroing out gradients, getting outputs, calculating loss, calculating gradients and then updating parameters and learning rate scheduler."}}