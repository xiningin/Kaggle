{"cell_type":{"b88a45ad":"code","3b9f717b":"code","d498592c":"code","84cf9c52":"code","83ed823a":"code","7c668802":"code","bf085e36":"code","1f47ab08":"code","804b901e":"code","86f538d8":"code","3c2a8039":"code","7188dcbd":"code","fa657d56":"code","73bea9ce":"code","82da02fe":"code","25b6b9d5":"code","94c8c551":"code","b2b0496c":"code","4beea36d":"code","788aa075":"code","50d6f5e4":"code","80ae3ddb":"code","f9497854":"code","060e0413":"code","43b0caf3":"code","151c83ab":"code","cfbdd73c":"code","1dc8baa5":"code","ef75f3f1":"code","e7a15f5e":"code","6ca36c1e":"code","52e32452":"code","1d92eed0":"code","6f9b13f4":"code","054f7a34":"code","1244dad7":"code","64ff5133":"code","548ad498":"code","82103cde":"code","3333030b":"code","fb680275":"code","2276fe8f":"code","3e790e67":"code","305d8ca3":"code","ac5d37b9":"code","bed3a5df":"code","2bdb4e55":"code","91dbc759":"code","9a175782":"code","52dcb84d":"code","3501ba4f":"code","2561b8ef":"code","95fe1364":"code","47c99763":"code","10c41c4b":"code","fdf308c0":"code","6c9201fd":"code","1ba08c5a":"code","8050dde2":"code","8b10880d":"code","099f20c2":"code","40b13394":"code","209dede7":"code","c6b7ea67":"markdown","2db50cca":"markdown","49e3e9ea":"markdown","65232a74":"markdown","376a3482":"markdown","57c9c4f1":"markdown"},"source":{"b88a45ad":"!pip show tensorflow","3b9f717b":"import numpy as np\nimport pandas as pd\nimport tensorflow\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate\nfrom tensorflow.keras.layers import Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom tensorflow.keras.preprocessing import text, sequence\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nimport time\nimport pickle\nimport re\nfrom tqdm._tqdm_notebook import tqdm_notebook as tqdm\ntqdm.pandas()","d498592c":"CRAWL_EMBEDDING_PATH = '..\/input\/pickled-crawl300d2m-for-kernel-competitions\/crawl-300d-2M.pkl'\nGLOVE_EMBEDDING_PATH = '..\/input\/pickled-glove840b300d-for-10sec-loading\/glove.840B.300d.pkl'","84cf9c52":"train = pd.read_csv('..\/input\/hackathonservicenowautomation\/all_tickets.csv')\ntrain.head()","83ed823a":"train['impact'].value_counts()","7c668802":"train_urgency = train[['title','body','urgency']]","bf085e36":"train_urgency.head()","1f47ab08":"#combine 2 columns , title and body\n\ntrain_urgency['text'] = train_urgency['title'] + \" \" + train_urgency['body']","804b901e":"train_urgency = train_urgency.drop(['title','body'],axis=1)","86f538d8":"train_urgency = train_urgency.fillna('No Data')","3c2a8039":"#Handle URL\n\ntrain_urgency['text'] = train_urgency['text'].apply(lambda x: re.sub(r'http\\S+', '', x))","7188dcbd":"# Adjusting the load_embeddings function, to now handle the pickled dict.\n\ndef get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\n\ndef load_embeddings(path):\n    with open(path,'rb') as f:\n        emb_arr = pickle.load(f)\n    return emb_arr\n\ndef build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    unknown_words = []\n    \n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            unknown_words.append(word)\n    return embedding_matrix, unknown_words","fa657d56":"import operator \n\ndef check_coverage(vocab,embeddings_index):\n    a = {}\n    oov = {}\n    k = 0\n    i = 0\n    for word in tqdm(vocab):\n        try:\n            a[word] = embeddings_index[word]\n            k += vocab[word]\n        except:\n\n            oov[word] = vocab[word]\n            i += vocab[word]\n            pass\n\n    print('Found embeddings for {:.2%} of vocab'.format(len(a) \/ len(vocab)))\n    print('Found embeddings for  {:.2%} of all text'.format(k \/ (k + i)))\n    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n\n    return sorted_x\n\ndef build_vocab(sentences, verbose =  True):\n    \"\"\"\n    :param sentences: list of list of words\n    :return: dictionary of words and their count\n    \"\"\"\n    vocab = {}\n    for sentence in tqdm(sentences, disable = (not verbose)):\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab","73bea9ce":"# Lets load the embeddings \n\ntic = time.time()\nglove_embeddings = load_embeddings(GLOVE_EMBEDDING_PATH)\nprint(f'loaded {len(glove_embeddings)} word vectors in {time.time()-tic}s')","82da02fe":"# Lets check how many words we got covered \n\nvocab = build_vocab(list(train_urgency['text'].apply(lambda x:x.split())))\noov = check_coverage(vocab,glove_embeddings)\noov[:20]","25b6b9d5":"import gc\ngc.collect()","94c8c551":"import string\nlatin_similar = \"\u2019'\u2018\u00c6\u00d0\u018e\u018f\u0190\u0194\u0132\u014a\u0152\u1e9e\u00de\u01f7\u021c\u00e6\u00f0\u01dd\u0259\u025b\u0263\u0133\u014b\u0153\u0138\u017f\u00df\u00fe\u01bf\u021d\u0104\u0181\u00c7\u0110\u018a\u0118\u0126\u012e\u0198\u0141\u00d8\u01a0\u015e\u0218\u0162\u021a\u0166\u0172\u01afY\u0328\u01b3\u0105\u0253\u00e7\u0111\u0257\u0119\u0127\u012f\u0199\u0142\u00f8\u01a1\u015f\u0219\u0163\u021b\u0167\u0173\u01b0y\u0328\u01b4\u00c1\u00c0\u00c2\u00c4\u01cd\u0102\u0100\u00c3\u00c5\u01fa\u0104\u00c6\u01fc\u01e2\u0181\u0106\u010a\u0108\u010c\u00c7\u010e\u1e0c\u0110\u018a\u00d0\u00c9\u00c8\u0116\u00ca\u00cb\u011a\u0114\u0112\u0118\u1eb8\u018e\u018f\u0190\u0120\u011c\u01e6\u011e\u0122\u0194\u00e1\u00e0\u00e2\u00e4\u01ce\u0103\u0101\u00e3\u00e5\u01fb\u0105\u00e6\u01fd\u01e3\u0253\u0107\u010b\u0109\u010d\u00e7\u010f\u1e0d\u0111\u0257\u00f0\u00e9\u00e8\u0117\u00ea\u00eb\u011b\u0115\u0113\u0119\u1eb9\u01dd\u0259\u025b\u0121\u011d\u01e7\u011f\u0123\u0263\u0124\u1e24\u0126I\u00cd\u00cc\u0130\u00ce\u00cf\u01cf\u012c\u012a\u0128\u012e\u1eca\u0132\u0134\u0136\u0198\u0139\u013b\u0141\u013d\u013f\u02bcN\u0143N\u0308\u0147\u00d1\u0145\u014a\u00d3\u00d2\u00d4\u00d6\u01d1\u014e\u014c\u00d5\u0150\u1ecc\u00d8\u01fe\u01a0\u0152\u0125\u1e25\u0127\u0131\u00ed\u00eci\u00ee\u00ef\u01d0\u012d\u012b\u0129\u012f\u1ecb\u0133\u0135\u0137\u0199\u0138\u013a\u013c\u0142\u013e\u0140\u0149\u0144n\u0308\u0148\u00f1\u0146\u014b\u00f3\u00f2\u00f4\u00f6\u01d2\u014f\u014d\u00f5\u0151\u1ecd\u00f8\u01ff\u01a1\u0153\u0154\u0158\u0156\u015a\u015c\u0160\u015e\u0218\u1e62\u1e9e\u0164\u0162\u1e6c\u0166\u00de\u00da\u00d9\u00db\u00dc\u01d3\u016c\u016a\u0168\u0170\u016e\u0172\u1ee4\u01af\u1e82\u1e80\u0174\u1e84\u01f7\u00dd\u1ef2\u0176\u0178\u0232\u1ef8\u01b3\u0179\u017b\u017d\u1e92\u0155\u0159\u0157\u017f\u015b\u015d\u0161\u015f\u0219\u1e63\u00df\u0165\u0163\u1e6d\u0167\u00fe\u00fa\u00f9\u00fb\u00fc\u01d4\u016d\u016b\u0169\u0171\u016f\u0173\u1ee5\u01b0\u1e83\u1e81\u0175\u1e85\u01bf\u00fd\u1ef3\u0177\u00ff\u0233\u1ef9\u01b4\u017a\u017c\u017e\u1e93\"\nwhite_list = string.ascii_letters + string.digits + latin_similar + ' '\nwhite_list += \"'\"","b2b0496c":"glove_chars = ''.join([c for c in tqdm(glove_embeddings) if len(c) == 1])\nglove_symbols = ''.join([c for c in glove_chars if not c in white_list])\nglove_symbols","4beea36d":"jigsaw_chars = build_vocab(list(train_urgency['text']))\njigsaw_symbols = ''.join([c for c in jigsaw_chars if not c in white_list])\njigsaw_symbols","788aa075":"# Basically we can delete all symbols we have no embeddings for:\n\nsymbols_to_delete = ''.join([c for c in jigsaw_symbols if not c in glove_symbols])\nsymbols_to_delete","50d6f5e4":"# The symbols we want to keep we need to isolate from our words. So lets setup a list of those to isolate.\n\nsymbols_to_isolate = ''.join([c for c in jigsaw_symbols if c in glove_symbols])\nsymbols_to_isolate","80ae3ddb":"# Note : Next comes the next trick. Instead of using an inefficient loop of replace we use translate. \n# I find the syntax a bit weird, but the improvement in speed is worth the worse readablity. \n\nisolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\nremove_dict = {ord(c):f'' for c in symbols_to_delete}\n\n\ndef handle_punctuation(x):\n    x = x.translate(remove_dict)\n    x = x.translate(isolate_dict)\n    return x","f9497854":"#So lets apply that function to our text and reasses the coverage\n\ntrain_urgency['text'] = train_urgency['text'].apply(lambda x:handle_punctuation(x))","060e0413":"# remove whitespaces\ntrain_urgency['text'] = train_urgency['text'].apply(lambda x:' '.join(x.split()))","43b0caf3":"vocab = build_vocab(list(train_urgency['text'].apply(lambda x:x.split())))\noov = check_coverage(vocab,glove_embeddings)\noov[:10]","151c83ab":"from nltk.tokenize.treebank import TreebankWordTokenizer\ntokenizer = TreebankWordTokenizer()","cfbdd73c":"def handle_contractions(x):\n    x = tokenizer.tokenize(x)\n    x = ' '.join(x)\n    return x","1dc8baa5":"train_urgency['text'] = train_urgency['text'].apply(lambda x:handle_contractions(x))","ef75f3f1":"vocab = build_vocab(list(train_urgency['text'].apply(lambda x:x.split())),verbose=False)\noov = check_coverage(vocab,glove_embeddings)\noov[:20]","e7a15f5e":"def fix_quote(x):\n    x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n    x = ' '.join(x)\n    return x","6ca36c1e":"train_urgency['text'] = train_urgency['text'].apply(lambda x:fix_quote(x.split()))","52e32452":"tic = time.time()\ncrawl_embeddings = load_embeddings(CRAWL_EMBEDDING_PATH)\nprint(f'loaded {len(glove_embeddings)} word vectors in {time.time()-tic}s')","1d92eed0":"vocab = build_vocab(list(train_urgency['text'].apply(lambda x:x.split())))\noov = check_coverage(vocab,crawl_embeddings)\noov[:20]","6f9b13f4":"X = train_urgency['text']\ny = train_urgency['urgency']","054f7a34":"NUM_MODELS = 2\nLSTM_UNITS = 200\nDENSE_HIDDEN_UNITS = 2 * LSTM_UNITS\nMAX_LEN = 600 #220\nmax_features = 200000\n\nBATCH_SIZE = 50  #changed from 150\nEPOCHS = 4","1244dad7":"# Its really important that you intitialize the keras tokenizer correctly. Per default it does lower case and removes a lot of symbols. We want neither of that!\n\ntokenizer = text.Tokenizer(num_words = max_features, filters='',lower=False)","64ff5133":"tokenizer.fit_on_texts(list(X))","548ad498":"crawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, CRAWL_EMBEDDING_PATH)\nprint('n unknown words (crawl): ', len(unknown_words_crawl))\n\nglove_matrix, unknown_words_glove = build_matrix(tokenizer.word_index, GLOVE_EMBEDDING_PATH)\nprint('n unknown words (glove): ', len(unknown_words_glove))\n\nmax_features = max_features or len(tokenizer.word_index) + 1\nmax_features\n\nembedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis=-1)\nembedding_matrix.shape\n\nimport gc\ndel crawl_matrix\ndel glove_matrix\ngc.collect()","82103cde":"X = tokenizer.texts_to_sequences(X)","3333030b":"X = sequence.pad_sequences(X, maxlen=MAX_LEN)","fb680275":"checkpoint_predictions = []\nweights = []","2276fe8f":"# Check F1 score\n\nfrom keras import backend as K\n\ndef recall_m(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives \/ (possible_positives + K.epsilon())\n        return recall\n\ndef precision_m(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","3e790e67":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate,Flatten,Lambda\nfrom tensorflow.keras.layers import Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D,PReLU,LSTM\nfrom tensorflow.keras.preprocessing import text, sequence\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing import text, sequence\nfrom tensorflow.keras import regularizers\nfrom sklearn.model_selection import train_test_split\nimport tensorflow_hub as hub","305d8ca3":"X_train , X_val, y_train  , y_val = train_test_split(X , \n                                                     y , \n                                                     stratify = y.values , \n                                                     train_size = 0.8,\n                                                     random_state = 100)","ac5d37b9":"unique = train_urgency['urgency'].nunique()","bed3a5df":"unique","2bdb4e55":"from tensorflow.keras.callbacks import EarlyStopping \nes = EarlyStopping(monitor='val_loss', mode ='min' ,verbose =1)","91dbc759":"train_urgency['urgency'].value_counts()","9a175782":"def build_model(embedding_matrix, num_aux_targets):\n    words = Input(shape=(MAX_LEN,))\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words) #Finds word embeddings for each word\n    x = SpatialDropout1D(0.3)(x) #This version performs the same function as Dropout, however it drops entire 1D feature maps instead of individual elements\n    x = LSTM(LSTM_UNITS, return_sequences=True)(x)\n    x = LSTM(LSTM_UNITS, return_sequences=True)(x)\n    x = LSTM(LSTM_UNITS, return_sequences=True)(x)\n    hidden = concatenate([\n        GlobalMaxPooling1D()(x), \n        GlobalAveragePooling1D()(x),#layer returns a fixed-length output vector for each example by averaging over the sequence dimension. This allows the model to handle input \n        #of variable length in the simplest way possible.\n    ])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)]) #This fixed-length output vector is piped through a fully-connected (Dense) layer with x hidden units.\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    result = Dense(unique, activation='softmax')(hidden)\n    model = Model(inputs=words, outputs= result)\n    model.compile(loss='sparse_categorical_crossentropy',metrics = ['accuracy'], optimizer='adam')\n    model.summary()\n    return model","52dcb84d":"type(y_val)","3501ba4f":"model = build_model(embedding_matrix,1)\n\nmodel.fit(\n    X_train,\n    np.asarray(y_train),\n    validation_data = (X_val, np.asarray(y_val)),\n    batch_size=BATCH_SIZE,\n    epochs=100,\n    verbose=2,\n    callbacks=[\n        LearningRateScheduler(lambda epoch: 1e-3 * (1 ** 2)),\n        es\n    ]\n)","2561b8ef":"y_pred = model.predict(X_val)","95fe1364":"y_pred =  np.argmax(y_pred,axis=1)","47c99763":"from sklearn.metrics import classification_report\nprint(classification_report(y_val,y_pred))","10c41c4b":"from PIL import Image\nfrom IPython.display import display, HTML, clear_output\nfrom ipywidgets import widgets, Layout","fdf308c0":"def init_widgets():\n    text_subject = widgets.Text(\n    description=\"Subject\", layout=Layout(minwidth=\"70%\")\n  )\n\n    text_body = widgets.Text(\n    description=\"Body\", layout=Layout(minwidth=\"70%\")\n  )\n    submit_button = widgets.Button(description=\"Submit\")\n\n    display(text_subject)\n    display(text_body)\n    display(submit_button)\n\n    prediction = submit_button.on_click(lambda b: on_button_click(\n      b,text_subject,text_body\n  ))\n  #display(prediction)\n    return prediction,text_subject,text_body","6c9201fd":"def on_button_click(b,text_subject,text_body):\n    clear_output()\n    subject = text_subject.value\n    body = text_body.value\n\n    text = subject + \" \" + body\n\n    tokenizer.fit_on_texts(list(text))\n  \n    X_text = pd.DataFrame()\n    X_text['text'] = text\n    X_text = tokenizer.texts_to_sequences(X_text)\n    X_text = sequence.pad_sequences(X_text, maxlen=MAX_LEN)\n\n    #display(X_text)\n    pred = model.predict(X_text)\n    pred =  np.argmax(pred,axis=1)\n    display(pred[0])\n    return pred[0]","1ba08c5a":"prediction,text_subject,text_body = init_widgets()","8050dde2":"test = pd.read_csv('..\/input\/hackathonservicenowautomation\/test_tickets.csv')","8b10880d":"test['text'] = test['title'] + \" \" + test['body']\ntest = test.drop(['title','body'],axis=1)\ntest = test.fillna('No Data')\ntest['text'] = test['text'].apply(lambda x: re.sub(r'http\\S+', '', x))","099f20c2":"test = tokenizer.texts_to_sequences(test['text'])\ntest = sequence.pad_sequences(test, maxlen=MAX_LEN)","40b13394":"prediction  = loaded_model.predict(test)","209dede7":"prediction =  np.argmax(prediction,axis=1)","c6b7ea67":"## Pre-processing for embeddings","2db50cca":"## Predict in CSV","49e3e9ea":"## Demo","65232a74":"## Model","376a3482":"## Pre-Proccessing","57c9c4f1":"## CRAWL embeddings"}}