{"cell_type":{"cce74518":"code","118db653":"code","182b1b12":"code","1b864d8e":"code","177617e0":"code","a109bce9":"code","3a27b64c":"code","89de6767":"code","4ce3518a":"code","4316fa96":"code","95565c72":"code","bfbc1741":"code","b9e676e2":"code","9d58a1ec":"code","c9a600df":"code","91c73bc2":"code","1aa1bae9":"code","9f2f6e8f":"code","11d0eced":"code","2b925e53":"code","5fbe2fe9":"code","4bd72aff":"code","f9a1069a":"code","7a660d9c":"code","b3d5fcdd":"code","14912e41":"code","159c7081":"code","aee66f42":"code","19a56f37":"code","baeaef81":"code","3de54935":"code","5458eac2":"code","b6235458":"code","49e1ccb4":"code","001166da":"code","a468e981":"code","c7c6666e":"code","8b2beba9":"code","db6a00c0":"code","be16340f":"code","5ecc3e91":"code","69608b52":"markdown","3413f10f":"markdown","902b1f7c":"markdown","0177eb4a":"markdown","b4a962ee":"markdown","836dcc0f":"markdown","2396e11b":"markdown","46045454":"markdown"},"source":{"cce74518":"#IMPORT LIBRARIES \n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pylab as plt\nfrom sklearn import preprocessing\nimport seaborn as sns\nfrom sklearn.model_selection import KFold\nimport joblib\nimport pickle\nimport lightgbm as lgb\nimport xgboost as xg\nimport gc\nimport os\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom tqdm import tqdm\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader,Dataset\nfrom torch.autograd import Variable\nimport torch.optim as optim\nfrom datetime import datetime, timedelta\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n\nseed_everything(seed=42)","118db653":"#\u041d\u044d\u043c\u044d\u043b\u0442 \u043e\u0433\u043d\u043e\u043e\u0442\u043e\u0439 \u0445\u043e\u043b\u0431\u043e\u043e\u0442\u043e\u0439 \u0431\u043e\u043b\u043e\u043d \u0434\u0430\u043c\u043c\u0438 \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0438\u0434 \u04af\u04af\u0441\u0433\u044d\u0445\n\ndef additional_features(df):\n\n    df['year'] = df['date'].dt.year\n    df['month'] = df['date'].dt.month\n    df['dayofweek'] = df['date'].dt.dayofweek\n    df['hour'] = df['date'].dt.hour\n    df['day'] = df['date'].dt.day\n    df['dayofyear'] = df['date'].dt.dayofyear\n    df['week'] = df['date'].dt.week\n\n    df['month_dummy'] = 0\n    df.loc[df['month'] > 10, 'month_dummy'] = 1\n    df.loc[df['month'] < 3, 'month_dummy'] = 1\n\n    df['hour_dummy'] = 0\n    df.loc[(df['month_dummy'] == 1) & (df['hour'] > 19) & (df['hour'] < 23), 'hour_dummy'] = 1\n    df.loc[(df['month_dummy'] == 1) & (df['hour'] > 5) & (df['hour'] < 9), 'hour_dummy'] = 1\n\n    df['day_dummy'] = 0\n    df.loc[(df['hour'] > 9) & (df['hour'] < 19), 'day_dummy'] = 1\n\n    return df","182b1b12":"#\u04e8\u0433\u04e9\u0433\u0434\u043b\u0438\u0439\u0433 \u0434\u0443\u0443\u0434\u0430\u0445\n\ndef get_data(ROOT):\n\n    CAL_DTYPES = {\"latitude\": \"float64\", \"longitude\": \"float64\", \"type\": \"category\",\n                  \"source\": \"category\", \"station\": \"category\", 'aqi': 'float64'}\n\n    train = pd.read_csv(ROOT + \"\/pm_train.csv\", dtype=CAL_DTYPES)\n    test = pd.read_csv(ROOT + \"\/pm_test.csv\", dtype=CAL_DTYPES)\n    weather = pd.read_csv(ROOT + \"\/weather.csv\")\n    sub = pd.read_csv(ROOT + \"\/sample_submission.csv\")\n\n    return train, test, weather, sub","1b864d8e":"#\u04e8\u0433\u04e9\u0433\u0434\u043b\u0438\u0439\u0433 \u0431\u044d\u043b\u0442\u0433\u044d\u0445 \u0437\u0430\u0441\u0432\u0430\u0440\u043b\u0430\u0445, \u043d\u044d\u0433\u0442\u0433\u044d\u0445\n\ndef preprocess(ROOT='..\/input\/ulaanbaatar-city-air-pollution-prediction', cat_feat = [], rolling_means=[], \n               shifts=[], diffs = [], aqi_mean=False):\n    \n    #\u0442\u0430\u0441\u0440\u0430\u043b\u0442\u0433\u04af\u0439 \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0438\u0434\n    cont_vars = ['temperature','apparentTemperature',\n                 'dewPoint', 'humidity', 'windSpeed',\n                'windBearing', 'cloudCover', 'uvIndex', 'visibility']\n    \n    #\u0442\u0430\u0441\u0440\u0430\u043b\u0442\u0442\u0430\u0439 \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0438\u0434    \n    cat_feat = [\"type\", \"source\", \"station\", \"hour\", \"year\", \"dayofyear\",\n           \"dayofweek\"]\n    \n    # \u04e9\u0433\u04e9\u0433\u0434\u043b\u0438\u0439\u0433 \u0434\u0443\u0443\u0434\u0430\u0445\n    train, test, weather, sub = get_data(ROOT=ROOT)\n    \n    # \u0441\u0443\u0440\u0433\u0430\u043b\u0442\u044b\u043d \u0431\u043e\u043b\u043e\u043d \u0442\u0435\u0441\u0442 \u04e9\u0433\u04e9\u0433\u0434\u043b\u0438\u0439\u0433 \u043d\u044d\u0433\u0442\u0433\u044d\u0445\n    df = pd.concat([train, test], axis=0)\n    \n    #\u043e\u0433\u043d\u043e\u043e\u0434 \u0445\u04e9\u0440\u0432\u04af\u04af\u043b\u044d\u0445\n    df['date'] = pd.to_datetime(df['date'])\n    weather['date'] = pd.to_datetime(weather['date'])\n    \n    #\u0446\u0430\u0433 \u0430\u0433\u0430\u0430\u0440\u044b\u043d \u04e9\u0433\u04e9\u0433\u0434\u043b\u0438\u0439\u0433 \u043e\u0433\u043d\u043e\u043e\u0433\u043e\u043e\u0440 \u0441\u043e\u0440\u0442\u043b\u043e\u0445\n    weather = weather.sort_values(by='date')\n    \n    #\u0437\u0430\u0440\u0438\u043c \u043e\u0433\u043d\u043e\u043e \u0446\u0430\u0433 \u0430\u0433\u0430\u0430\u0440\u044b\u043d \u04e9\u0433\u04e9\u0433\u0434\u04e9\u043b\u0434 \u0431\u0430\u0439\u0433\u0430\u0430\u0433\u04af\u0439 \u0442\u0443\u043b \u0442\u044d\u0434\u0433\u044d\u044d\u0440\u0438\u0439\u0433 \u043e\u0440\u0443\u0443\u043b\u0441\u0430\u043d \u0448\u0438\u043d\u044d \u0445\u04af\u0441\u043d\u044d\u0433\u0442 \u04af\u04af\u0441\u0433\u044d\u0445\n    weather_df = pd.DataFrame()\n    weather_df['date'] = pd.date_range(start=weather.iloc[0]['date'], end=weather.iloc[-1]['date'], freq='H')\n    \n    #\u04e9\u0433\u04e9\u0433\u0434\u0441\u04e9\u043d \u0446\u0430\u0433 \u0430\u0433\u0430\u0430\u0440\u044b\u043d \u043e\u0433\u043d\u043e\u043e\u0433 \u0448\u0438\u043d\u044d \u0445\u04af\u0441\u043d\u044d\u0433\u0442\u044d\u0439 \u043d\u044d\u0433\u0442\u0433\u044d\u0445\n    weather_df = pd.merge(weather_df, weather.iloc[:, 1:], on=['date'], how='left')\n    \n    #\u04e9\u0433\u04e9\u0433\u0434\u0441\u04e9\u043d \u0446\u0430\u0433 \u0430\u0433\u0430\u0430\u0440\u044b\u043d \u04e9\u0433\u04e9\u0433\u0434\u043b\u0438\u0439\u0433 \u0443\u0441\u0442\u0433\u0430\u0445\n    del weather\n    \n    \n    #\u043d\u044d\u043c\u044d\u043b\u0442 \u0436\u0438\u043b, \u0436\u0438\u043b\u0438\u0439\u043d \u04e9\u0434\u04e9\u0440, \u0446\u0430\u0433 \u0433\u044d\u0441\u044d\u043d \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0438\u0434 \u04af\u04af\u0441\u0433\u044d\u0445\n    weather_df['year'] =  weather_df['date'].dt.year\n    weather_df['dayofyear'] = weather_df['date'].dt.dayofyear\n    weather_df['hour'] = weather_df['date'].dt.hour\n    \n    # \u043d\u044d\u0433\u0442\u0433\u044d\u0441\u044d\u043d \u0441\u0443\u0440\u0433\u0430\u043b\u0442\u044b\u043d \u04e9\u0433\u04e9\u0433\u0434\u04e9\u043b\u0434 \u043d\u044d\u043c\u044d\u043b\u0442 \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0434\u044b\u0433 \u04af\u04af\u0441\u0433\u044d\u0445\n    df = additional_features(df)\n    \n    #\u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u043a\u0430\u043b \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0434\u044b\u0433 \u043d\u0430\u0442\u0443\u0440\u0430\u043b \u0442\u043e\u043e\u0440\u0443\u0443 \u0445\u04e9\u0440\u0432\u04af\u04af\u043b\u044d\u0445\n    for col in cat_feat:\n        le = preprocessing.LabelEncoder()\n        df[col] = le.fit_transform(df[col])\n    \n    #\u0446\u0430\u0433 \u0430\u0433\u0430\u0430\u0440\u044b\u043d \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u043a\u0430\u043b \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0434\u044b\u043d \u0445\u043e\u043e\u0441\u043e\u043d \u0443\u0442\u0433\u044b\u0433 \u0434\u04af\u04af\u0440\u0433\u044d\u0436, \u043d\u0430\u0442\u0443\u0440\u0430\u043b \u0442\u043e\u043e\u0440\u0443\u0443 \u0445\u0430\u0440\u0432\u04af\u04af\u043b\u044d\u0445\n    for col in ['summary', 'icon', \"year\", \"dayofyear\", \"hour\"]:\n        weather_df[col] = weather_df[col].fillna(method=\"ffill\")\n        weather_df[col] = weather_df[col].fillna(method=\"bfill\")\n        le = preprocessing.LabelEncoder()\n        weather_df[col] = le.fit_transform(weather_df[col])\n   \n    #\u0446\u0430\u0433 \u0430\u0433\u0430\u0430\u0440\u044b\u043d \u0442\u0430\u0441\u0440\u0430\u043b\u0442\u0433\u04af\u0439 \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0434\u044b\u043d \u0445\u043e\u043e\u0441\u043e\u043d \u0443\u0442\u0433\u044b\u0433 \u0434\u04af\u04af\u0440\u0433\u044d\u0445\n    for col in weather_df.columns[3:]:\n        weather_df[col] = weather_df[col].fillna(weather_df[col].rolling(4, min_periods=1).mean())\n        weather_df[col] = weather_df[col].fillna(weather_df.groupby([\"dayofyear\", \"hour\"])[col].transform('mean'))\n    \n    #\u0446\u0430\u0433 \u0430\u0433\u0430\u0430\u0440\u044b\u043d \u0442\u0430\u0441\u0440\u0430\u043b\u0442\u0433\u04af\u0439 \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0434\u0430\u0430\u0441 \u0433\u04af\u0439\u0441\u044d\u043d \u0434\u0443\u043d\u0434\u0430\u0436 \u0443\u0442\u0433\u044b\u0433 \u043e\u043b\u0436, \u0448\u0438\u043d\u044d \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447 \u04af\u04af\u0441\u0433\u044d\u0445\n    for col in weather_df.columns[3:-2]:\n        for lag in rolling_means:\n            weather_df['%s_rolling_%s_mean' % (col, lag)] = weather_df[col].rolling(lag).mean()\n    \n    #\u0442\u0430\u0441\u0440\u0430\u043b\u0442\u0433\u04af\u0439 \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0434\u044b\u043d \u043b\u0430\u0433 \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0434\u044b\u0433 \u04af\u04af\u0441\u0433\u044d\u0445\n    for col in weather_df.drop(['date', 'summary', 'icon', 'dayofyear',\n                         'year'], axis=1).columns:\n        for lag in [2, 4]:\n            weather_df['%s_shift_%s' % (col, lag)] = weather_df[col].shift(lag)\n\n    #\u0442\u0430\u0441\u0440\u0430\u043b\u0442\u0433\u04af\u0439 \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0434\u0430\u0430\u0441 \u04e9\u04e9\u0440\u0447\u043b\u04e9\u043b\u0442\u0438\u0439\u043d \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447 \u04af\u04af\u0441\u0433\u044d\u0445\n    for col in cont_vars:\n        for lag in diffs:\n            weather_df['%s_diff_%s' % (col, lag)] = weather_df[col].diff(lag)\n    \n    #\u0446\u0430\u0433 \u0430\u0433\u0430\u0430\u0440\u044b\u043d \u04e9\u0433\u04e9\u0433\u0434\u043b\u0438\u0439\u043d \u0445\u043e\u043e\u0441\u043e\u043d \u0443\u0442\u0433\u044b\u0433 \u0443\u0441\u0442\u0433\u0430\u0445\n    weather_df = weather_df.dropna()\n    \n    #\u0441\u0443\u0440\u0433\u0430\u043b\u0442\u044b\u043d, \u0442\u0435\u0441\u0442\u0438\u0439\u043d \u0431\u043e\u043b\u043e\u043d \u0446\u0430\u0433 \u0430\u0433\u0430\u0430\u0440\u044b\u043d \u04e9\u0433\u04e9\u0433\u0434\u043b\u0438\u0439\u0433 \u043d\u044d\u0433\u0442\u0433\u044d\u0445\n    df = pd.merge(df, weather_df.iloc[:, 1:], on=[\"year\", \"dayofyear\", \"hour\"], how='left')\n    \n    #\u043d\u044d\u0433\u0442\u0433\u044d\u0441\u044d\u043d \u04e9\u0433\u04e9\u0433\u0434\u043b\u0438\u0439\u043d \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u043a\u0430\u043b \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0434\u044b\u0433 \u0434\u0430\u0445\u0438\u043d \u043d\u0430\u0442\u0443\u0440\u0430\u043b \u0442\u043e\u043e\u0440\u0443\u0443 \u0445\u04e9\u0440\u0432\u04af\u04af\u043b\u044d\u0445\n    for col in cat_feat[2:]:\n        le = preprocessing.LabelEncoder()\n        df[col] = le.fit_transform(df[col])\n    \n    if aqi_mean==True:\n        # \u0445\u0430\u043c\u0430\u0430\u0440\u0430\u0433\u0447 \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0430\u0430\u0441 \u0437\u0430\u0440\u0438\u043c \u0448\u0438\u043d\u044d \u0445\u0443\u044c\u0441\u0430\u0433\u0447 \u04af\u04af\u0441\u0433\u044d\u0445 \n        icols = [['type', 'station', 'month', 'hour'],\n                ['type', 'station', 'week', 'hour'],\n                 ['type', 'station', 'icon', 'month', 'hour'],\n                 ['type', 'station', 'summary', 'month', 'hour']]\n\n\n        temp_df = df.copy()\n        col_fill = '_' + '_'.join(icols[0]) + '_'\n        for col in icols:\n            col_name = '_' + '_'.join(col) + '_'\n            temp = temp_df[col + ['aqi']].groupby(col, as_index=False)['aqi'].mean()\n            temp = temp.rename(columns={'aqi': 'enc%smean' % col_name})\n            df = df.merge(temp, on=col, copy=False, how='left')\n            df['enc%smean' % col_name] = df['enc%smean' % col_name]\n            df['enc%smean' % col_name] = df['enc%smean' % col_name].fillna(df['enc%smean' % col_fill])\n\n    return df, weather_df, sub","177617e0":"# LightGBM \u0437\u0430\u0433\u0432\u0430\u0440\u044b\u0433 \u0441\u0443\u0440\u0433\u0430\u0445\ndef train_lightGBM(train, test, sub_light, target, feat, cat_feat,\n                   top_n_features, Nfolds, params_k, MODEL_ROOT):\n    \n    # submission file \u0431\u044d\u043b\u0442\u0433\u044d\u0445\n    sub_light[target] = 0\n    \n    # \u0441\u0443\u0440\u0433\u0430\u043b\u0442\u044b\u043d \u04e9\u0433\u04e9\u0433\u0434\u043b\u0438\u0439\u0433 \u0434\u0430\u0445\u0438\u043d \u0438\u043d\u0434\u0435\u043a\u0441\u0436\u04af\u04af\u043b\u044d\u0445\n    train = train.reset_index(drop=True)\n    # \u0431\u0430\u0442\u0430\u043b\u0433\u0430\u0430\u0436\u0443\u0443\u043b\u0430\u043b\u0442\u044b\u043d \u04e9\u0433\u04e9\u0433\u0434\u043b\u0438\u0439\u0433 \u0441\u043e\u043d\u0433\u043e\u0445\n    val_set = test.loc[test[target].dropna().index]\n\n    # LightGBM \u0437\u0430\u0433\u0432\u0430\u0440\u044b\u043d \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0438\u0439\u043d \u0447\u0443\u0445\u043b\u044b\u043d \u0437\u044d\u0440\u044d\u0433\u0442 \u04af\u043d\u0434\u044d\u0441\u043b\u044d\u043d \u0445\u0430\u043c\u0433\u0438\u0439\u043d \u0447\u0443\u0445\u0430\u043b n \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0438\u0439\u0433 \u0441\u043e\u043d\u0433\u043e\u0445\n    if top_n_features!=-1:\n        \n        train_data = lgb.Dataset(data=train[feat],\n                                 label=train[target],\n                                 categorical_feature=cat_feat,\n                                 free_raw_data=False)\n\n        valid_data = lgb.Dataset(data=val_set[feat],\n                                 label=val_set[target],\n                                 categorical_feature=cat_feat,\n                                 free_raw_data=False)\n\n        model_gbm = lgb.train(params_k, train_data, valid_sets=[train_data, valid_data],\n                              num_boost_round=1500, early_stopping_rounds=50,\n                              verbose_eval=100)\n\n        pred_val = model_gbm.predict(val_set[feat], num_iteration=model_gbm.best_iteration)\n        print(\"CV score:\", np.sqrt(mean_squared_error(val_set[target].values, pred_val)))\n\n        feature_importance_df = pd.DataFrame()\n        feature_importance_df[\"feature\"] = feat\n        feature_importance_df[\"importance\"] = model_gbm.feature_importance()\n\n        all_features = feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n            by=\"importance\", ascending=False)\n        all_features.reset_index(inplace=True)\n\n        feat = list(all_features[0:top_n_features]['feature'])\n\n        temp = []\n        for col in cat_feat:\n            if col in feat:\n                temp.append(col)\n\n        cat_feat = temp\n    else:\n        print('No feature selection')\n        \n        \n    #\u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447 \u0441\u043e\u043d\u0433\u043e\u043b\u0442\u044b\u043d \u0434\u0430\u0440\u0430\u0430 \u044d\u0446\u0441\u0438\u0439\u043d \u0437\u0430\u0433\u0432\u0430\u0440\u044b\u0433 \u0441\u0443\u0440\u0433\u0430\u0445\n    kf = KFold(n_splits=Nfolds, shuffle=True)\n\n    scores = []\n    scores_fold = []\n    fold_ = 0\n    for train_index, val_index in kf.split(train.index):\n        train_set = train.loc[train_index]\n        val_set2 = train.loc[val_index]\n        val_set_full = val_set\n\n        train_data = lgb.Dataset(data=train_set[feat],\n                                 label=train_set[target],\n                                 categorical_feature=cat_feat,\n                                 free_raw_data=False)\n\n        valid_data = lgb.Dataset(data=val_set_full[feat],\n                                 label=val_set_full[target],\n                                 categorical_feature=cat_feat,\n                                 free_raw_data=False)\n\n        model_gbm = lgb.train(params_k, train_data, valid_sets=[train_data, valid_data],\n                              num_boost_round=1500, early_stopping_rounds=50,\n                              verbose_eval=100)\n\n\n        pred_val = model_gbm.predict(val_set[feat])\n        rmse = np.sqrt(mean_squared_error(val_set[target].values, pred_val))\n        scores.append(rmse)\n        print(\"CV score:\", rmse)\n\n\n        val_set2[\"pred\"] = model_gbm.predict(val_set2[feat])\n        rmse_fold = np.sqrt(mean_squared_error(val_set2[target].values, val_set2[\"pred\"].values))\n        scores_fold.append(rmse_fold)\n        print(\"CV score fold:\", rmse_fold)\n\n        pred = model_gbm.predict(test[feat].fillna(0))\n        sub_light[target] = sub_light[target] + pred \/ Nfolds\n\n        fold_ = fold_ + 1\n\n    return sub_light, scores, scores_fold\n","a109bce9":"# CatBoost \u0437\u0430\u0433\u0432\u0430\u0440\u044b\u0433 \u0441\u0443\u0440\u0433\u0430\u0445 \ndef training_CatBoost(train, test, sub_cb, target, feat, cat_feat,\n                   top_n_features, Nfolds, params_k, MODEL_ROOT):\n    \n    # submission file \u0431\u044d\u043b\u0442\u0433\u044d\u0445\n    sub_cb[target] = 0\n    \n    # \u0441\u0443\u0440\u0433\u0430\u043b\u0442\u044b\u043d \u04e9\u0433\u04e9\u0433\u0434\u043b\u0438\u0439\u0433 \u0434\u0430\u0445\u0438\u043d \u0438\u043d\u0434\u0435\u043a\u0441\u0436\u04af\u04af\u043b\u044d\u0445\n    train = train.reset_index(drop=True)\n    # \u0431\u0430\u0442\u0430\u043b\u0433\u0430\u0430\u0436\u0443\u0443\u043b\u0430\u043b\u0442\u044b\u043d \u04e9\u0433\u04e9\u0433\u0434\u043b\u0438\u0439\u0433 \u0441\u043e\u043d\u0433\u043e\u0445\n    val_set = test.loc[test[target].dropna().index]\n    \n    # \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u043a\u0430\u043b \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0434\u044b\u043d \u0445\u043e\u043e\u0441\u043e\u043d \u0443\u0442\u0433\u0430 \u0431\u0430\u0439\u0432\u0430\u043b 0-\u044d\u044d\u0440 \u0434\u04af\u04af\u0440\u0433\u044d\u0436, \u0431\u04af\u0445\u044d\u043b \u0431\u043e\u043b\u0433\u043e\u0445\n    for col in cat_feat:\n        train[col] = train[col].fillna(0).astype(np.int)\n        val_set[col] = val_set[col].fillna(0).astype(np.int)\n        test[col] = test[col].fillna(0).astype(np.int)\n    \n    # CatBoost \u0437\u0430\u0433\u0432\u0430\u0440\u044b\u043d \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0438\u0439\u043d \u0447\u0443\u0445\u043b\u044b\u043d \u0437\u044d\u0440\u044d\u0433\u0442 \u04af\u043d\u0434\u044d\u0441\u043b\u044d\u043d \u0445\u0430\u043c\u0433\u0438\u0439\u043d \u0447\u0443\u0445\u0430\u043b n \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0438\u0439\u0433 \u0441\u043e\u043d\u0433\u043e\u0445\n    if top_n_features != -1:\n   \n        if params_k==True:\n            model_cb = CatBoostRegressor(\n                n_estimators=2000,\n                learning_rate=0.05,\n                loss_function='MAE',\n                eval_metric='RMSE',\n                cat_features=cat_feat,\n                max_bin=50,\n                subsample=0.9,\n                colsample_bylevel=0.5,\n                verbose=100)\n        else:\n            model_cb = CatBoostRegressor(\n                n_estimators=2000,\n                loss_function='MAE',\n                eval_metric='RMSE',\n                cat_features=cat_feat,\n                learning_rate=0.05,\n                verbose=100)\n\n        model_cb.fit(train[feat], train[target], use_best_model=True, eval_set=(val_set[feat], val_set[target]),\n                     early_stopping_rounds=25)\n\n        feature_importance_df = pd.DataFrame()\n        feature_importance_df[\"feature\"] = feat\n        feature_importance_df[\"importance\"] = model_cb.get_feature_importance()\n\n        all_features = feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n            by=\"importance\", ascending=False)\n        all_features.reset_index(inplace=True)\n\n        feat = list(all_features[0:top_n_features]['feature'])\n\n        temp = []\n        for col in cat_feat:\n            if col in feat:\n                temp.append(col)\n\n        cat_feat = temp\n    else:\n        print('No feature selection')\n    \n    \n    #\u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447 \u0441\u043e\u043d\u0433\u043e\u043b\u0442\u044b\u043d \u0434\u0430\u0440\u0430\u0430 \u044d\u0446\u0441\u0438\u0439\u043d \u0437\u0430\u0433\u0432\u0430\u0440\u044b\u0433 \u0441\u0443\u0440\u0433\u0430\u0445\n    kf = KFold(n_splits=Nfolds, shuffle=True)\n\n    fold_ = 0\n    scores = []\n    scores_fold = []\n    for train_index, val_index in kf.split(train.index):\n        train_set = train.loc[train_index]\n        val_set2 = train.loc[val_index]\n        val_set_full = val_set\n\n        if params_k == True:\n            model_cb = CatBoostRegressor(\n                n_estimators=2000,\n                learning_rate=0.05,\n                loss_function='MAE',\n                eval_metric='RMSE',\n                cat_features=cat_feat,\n                max_bin=50,\n                subsample=0.9,\n                colsample_bylevel=0.5,\n                verbose=100)\n        else:\n            model_cb = CatBoostRegressor(\n                n_estimators=2000,\n                loss_function='MAE',\n                eval_metric='RMSE',\n                cat_features=cat_feat,\n                learning_rate=0.05,\n                verbose=100)\n\n        model_cb.fit(train_set[feat], train_set[target], use_best_model=True,\n                     eval_set=(val_set_full[feat], val_set_full[target]),\n                     early_stopping_rounds=25)\n\n        pred_val = model_cb.predict(val_set[feat])\n        rmse = np.sqrt(mean_squared_error(val_set[target].values, pred_val))\n        scores.append(rmse)\n        print(\"CV score:\", rmse)\n\n        val_set2[\"pred\"] = model_cb.predict(val_set2[feat])\n        rmse_fold = np.sqrt(mean_squared_error(val_set2[target].values, val_set2[\"pred\"].values))\n        scores_fold.append(rmse_fold)\n        print(\"CV score fold:\", rmse_fold)\n\n        pred = model_cb.predict(test[feat].fillna(0))\n        sub_cb[target] = sub_cb[target] + pred \/ Nfolds\n\n        fold_ = fold_ + 1\n\n    return sub_cb, scores, scores_fold","3a27b64c":"# XGBoost \u0437\u0430\u0433\u0432\u0430\u0440\u044b\u0433 \u0441\u0443\u0440\u0433\u0430\u0445 \ndef training_XGboost(train, test, sub_xg, target, feat, cat_feat,\n                   top_n_features, Nfolds, params_k, MODEL_ROOT):\n    \n    # submission file \u0431\u044d\u043b\u0442\u0433\u044d\u0445\n    sub_xg[target] = 0\n    \n    # \u0441\u0443\u0440\u0433\u0430\u043b\u0442\u044b\u043d \u04e9\u0433\u04e9\u0433\u0434\u043b\u0438\u0439\u0433 \u0434\u0430\u0445\u0438\u043d \u0438\u043d\u0434\u0435\u043a\u0441\u0436\u04af\u04af\u043b\u044d\u0445\n    train = train.reset_index(drop=True)\n    val_set = test.loc[test[target].dropna().index]\n    \n    # \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u043a\u0430\u043b \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0434\u044b\u043d \u0445\u043e\u043e\u0441\u043e\u043d \u0443\u0442\u0433\u0430 \u0431\u0430\u0439\u0432\u0430\u043b 0-\u044d\u044d\u0440 \u0434\u04af\u04af\u0440\u0433\u044d\u0436, \u0431\u04af\u0445\u044d\u043b \u0431\u043e\u043b\u0433\u043e\u0445\n    for col in cat_feat:\n        train[col] = train[col].fillna(0).astype(np.int)\n        val_set[col] = val_set[col].fillna(0).astype(np.int)\n        test[col] = test[col].fillna(0).astype(np.int)\n    \n    # XGBoost \u0437\u0430\u0433\u0432\u0430\u0440\u044b\u043d \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0438\u0439\u043d \u0447\u0443\u0445\u043b\u044b\u043d \u0437\u044d\u0440\u044d\u0433\u0442 \u04af\u043d\u0434\u044d\u0441\u043b\u044d\u043d \u0445\u0430\u043c\u0433\u0438\u0439\u043d \u0447\u0443\u0445\u0430\u043b n \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0438\u0439\u0433 \u0441\u043e\u043d\u0433\u043e\u0445\n    if top_n_features != -1:\n    \n        if params_k==True:\n            model_xg = xg.XGBRegressor(\n                n_estimators=2000,\n                learning_rate=0.05,\n                eval_metric='RMSE',\n                cat_features=cat_feat,\n                max_bin=50,\n                subsample=0.75,\n                colsample_bylevel=0.75,\n                verbose=50)\n        else:\n            model_xg = xg.XGBRegressor(\n                n_estimators=2000,\n                eval_metric='RMSE',\n                cat_features=cat_feat,\n                learning_rate=0.05,\n                verbose=50)\n\n        model_xg.fit(train[feat], train[target], eval_set=[(val_set[feat], val_set[target])],\n                     eval_metric='rmse', early_stopping_rounds=25)\n\n\n        feature_importance_df = pd.DataFrame()\n        feature_importance_df[\"feature\"] = feat\n        feature_importance_df[\"importance\"] = model_xg.feature_importances_\n\n        all_features = feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n            by=\"importance\", ascending=False)\n        all_features.reset_index(inplace=True)\n\n        feat = list(all_features[0:top_n_features]['feature'])\n\n        temp = []\n        for col in cat_feat:\n            if col in feat:\n                temp.append(col)\n\n        cat_feat = temp\n    else:\n        print('No feature selection')\n    \n    #\u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447 \u0441\u043e\u043d\u0433\u043e\u043b\u0442\u044b\u043d \u0434\u0430\u0440\u0430\u0430 \u044d\u0446\u0441\u0438\u0439\u043d \u0437\u0430\u0433\u0432\u0430\u0440\u044b\u0433 \u0441\u0443\u0440\u0433\u0430\u0445\n    kf = KFold(n_splits=Nfolds, shuffle=True)\n\n    fold_ = 0\n    scores = []\n    scores_fold = []\n    for train_index, val_index in kf.split(train.index):\n        train_set = train.loc[train_index]\n        val_set2 = train.loc[val_index]\n        val_set_full = val_set\n\n        if params_k == True:\n            model_xg = xg.XGBRegressor(\n                n_estimators=2000,\n                learning_rate=0.05,\n                eval_metric='RMSE',\n                cat_features=cat_feat,\n                max_bin=50,\n                subsample=0.75,\n                colsample_bylevel=0.75,\n                verbose=50)\n        else:\n            model_xg = xg.XGBRegressor(\n                n_estimators=2000,\n                eval_metric='RMSE',\n                cat_features=cat_feat,\n                learning_rate=0.05,\n                verbose=50)\n\n        model_xg.fit(train[feat], train[target], eval_set=[(val_set[feat], val_set[target])],\n                         eval_metric='rmse', early_stopping_rounds=25)\n\n            \n        pred_val = model_xg.predict(val_set[feat])\n        rmse = np.sqrt(mean_squared_error(val_set[target].values, pred_val))\n        scores.append(rmse)\n        print(\"CV score:\", rmse)\n\n        val_set2[\"pred\"] = model_xg.predict(val_set2[feat])\n        rmse_fold = np.sqrt(mean_squared_error(val_set2[target].values, val_set2[\"pred\"].values))\n        scores_fold.append(rmse_fold)\n        print(\"CV score fold:\", rmse_fold)\n\n        pred = model_xg.predict(test[feat])\n        sub_xg[target] = sub_xg[target] + pred \/ Nfolds\n\n        fold_ = fold_ + 1\n\n    return sub_xg, scores, scores_fold","89de6767":"#### Hyperparameters ############\ncat_feats = ['type', 'source', 'station', 'dayofyear',\n            'hour',  'month', 'year', 'dayofweek', 'month_dummy',\n            'hour_dummy', 'day_dummy', 'summary', 'icon', 'week']\n\n# \u0425\u044d\u0440\u044d\u0433\u0433\u0433\u04af\u0439 \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0438\u0434\nuseless_columns = ['ID', 'date']\n# \u0445\u0430\u043c\u0430\u0430\u0440\u0430\u0433\u0447 \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\ntarget = 'aqi'\n\n# lightGBM \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0435\u0440\u0441\nparams_lgb = {\n            'boosting_type': 'gbdt',\n            'objective': 'regression',\n            'metric': 'rmse',\n            'subsample': 0.9,\n            'subsample_freq': 1,\n            'learning_rate': 0.03,\n            'num_leaves': 2**8-1,\n            'min_data_in_leaf': 2**9-1,\n            'feature_fraction': 0.5,\n            'max_bin': 50,\n            'n_estimators': 2000,\n            'boost_from_average': False,\n            \"random_seed\":42,\n            }\n\n\n# \u0411\u044d\u043b\u0442\u0433\u044d\u0441\u044d\u043d \u04e9\u0433\u04e9\u0433\u0434\u043b\u0438\u0439\u0433 \u0433\u0430\u0440\u0433\u0430\u0436 \u0430\u0432\u0430\u0445\ndf, weather, sub = preprocess(ROOT='..\/input\/ulaanbaatar-city-air-pollution-prediction', \n                              cat_feat = cat_feats, rolling_means=[4, 8, 12], shifts=[2, 4], diffs=[2, 4], aqi_mean=True)\n\n# submission file \u0443\u0442\u0433\u0443\u0443\u0434\u044b\u0433 \u0442\u044d\u0433\u043b\u044d\u0445\nsub[target] = 0\n\n# \u0441\u0443\u0440\u0433\u0430\u043b\u0442\u044b\u043d \u0431\u043e\u043b\u043e\u043d \u0442\u0435\u0441\u0442\u0438\u0439\u043d \u04e9\u0433\u04e9\u0433\u0434\u043b\u0438\u0439\u0433 \u0445\u0443\u0432\u0430\u0430\u0445\ntrain = df.loc[df['date']<'2019-01-01']\ntest = df.loc[df['date']>='2019-01-01']\n\n# \u0437\u0430\u0433\u0432\u0430\u0440 \u0441\u0443\u0440\u0433\u0430\u0445\u0430\u0434 \u0430\u0448\u0438\u0433\u043b\u0430\u0433\u0434\u0430\u0445 \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0434\u044b\u0433 \u0441\u043e\u043d\u0433\u043e\u0445\nfeat = list(train.drop(useless_columns + [target], axis=1))\n\n# \u0445\u0430\u043c\u0433\u0438\u0439\u043d \u0447\u0443\u0445\u0430\u043b 100 \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0438\u0439\u0433 \u0441\u043e\u043d\u0433\u043e\u0436 \u0437\u0430\u0433\u0432\u0430\u0440\u0443\u0443\u0434\u044b\u0433 \u0441\u0443\u0440\u0433\u0430\u0445 \u0433\u044d\u0445\u0434\u044d\u044d \u0433\u044d\u0445\u0434\u044d\u044d \u0445\u0430\u043c\u0430\u0430\u0440\u0430\u0433\u0447 \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0434\u0430\u0430\u0441 \u04af\u04af\u0441\u0433\u044d\u0441\u044d\u043d \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0434\u044b\u0433 \u0430\u0448\u0438\u0433\u043b\u0430\u0445\nfor top_n_features in [100]:\n    print(\"*\"*25, \"top_n_features\", top_n_features, \"*\"*25,)\n    sub_light, scores, scores_fold = train_lightGBM(train=train.copy(), test=test.copy(), sub_light=sub.copy(), Nfolds=8,\n                               target=target, feat=feat, cat_feat=cat_feats,\n                               top_n_features=top_n_features, params_k=params_lgb, MODEL_ROOT='models\/lgb')\n\n    print('LighGBM CV:', np.mean(scores), '+\/-', np.std(scores))\n    print('LighGBM CV fold:', np.mean(scores_fold), '+\/-', np.std(scores_fold))\n    sub_light.to_csv(\"sub_lgb_%s.csv\" %top_n_features, index=False)\n        \n    print(\"*\" * 25, \"top_n_features\", top_n_features, \"*\" * 25, )\n\n    sub_cb, scores, scores_fold = training_CatBoost(train=train.copy(), test=test.copy(), sub_cb=sub.copy(), Nfolds=8,\n                               target=target, feat=feat, cat_feat=cat_feats,\n                               top_n_features=top_n_features, params_k=True, MODEL_ROOT='models\/cb')\n\n    print('CatBoost CV:', np.mean(scores), '+\/-', np.std(scores))\n    print('CatBoost CV fold:', np.mean(scores_fold), '+\/-', np.std(scores_fold))\n    sub_cb.to_csv(\"sub_cb_%s.csv\" %top_n_features, index=False)\n    \n    sub_xg, scores, scores_fold = training_XGboost(train=train.copy(), test=test.copy(), sub_xg=sub.copy(), Nfolds=8,\n                               target=target, feat=feat, cat_feat=cat_feats,\n                               top_n_features=top_n_features, params_k=True, MODEL_ROOT='models\/xg')\n\n    print('XGBoost CV:', np.mean(scores), '+\/-', np.std(scores))\n    print('XGBoost CV fold:', np.mean(scores_fold), '+\/-', np.std(scores_fold))\n    sub_xg.to_csv(\"sub_xg_%s.csv\" %top_n_features, index=False)\n    \n# \u0445\u0430\u043c\u0433\u0438\u0439\u043d \u0447\u0443\u0445\u0430\u043b 100 \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0438\u0439\u0433 \u0441\u043e\u043d\u0433\u043e\u0436 \u0437\u0430\u0433\u0432\u0430\u0440\u0443\u0443\u0434\u044b\u0433 \u0441\u0443\u0440\u0433\u0430\u0445   \n# \u0433\u044d\u0445\u0434\u044d\u044d \u0445\u0430\u043c\u0430\u0430\u0440\u0430\u0433\u0447 \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0434\u0430\u0430\u0441 \u04af\u04af\u0441\u0433\u044d\u0441\u044d\u043d \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0434\u044b\u0433 \u0430\u0448\u0438\u0433\u043b\u0430\u0445\u0433\u04af\u0439 \n# \u043c\u0438\u043d\u0438\u0439 \u0445\u0443\u0432\u044c\u0434 \u0442\u0435\u0441\u0442 \u04e9\u0433\u04e9\u0433\u0434\u04e9\u043b \u0434\u044d\u044d\u0440 \u043d\u044d\u044d\u043b\u0442\u0442\u044d\u0439 \u04e9\u0433\u04e9\u0433\u0434\u0441\u04e9\u043d \u0445\u0430\u043c\u0430\u0430\u0440\u0430\u0433\u0447 \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0438\u0439\u043d \u0443\u0442\u0433\u044b\u0433 \n# \u0445\u0430\u043c\u0430\u0430\u0440\u0430\u0433\u0447 \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0430\u0430\u0441 \u04af\u04af\u0441\u0433\u044d\u0441\u044d\u043d \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0434\u044b\u043d \u0442\u043e\u043e\u0446\u043e\u043e\u043b\u043e\u043b\u0434 \u043e\u0440\u0443\u0443\u043b\u0441\u0430\u043d \u043d\u044c \u0445\u0443\u0443\u0440\u0430\u043c\u0447 CV \u0443\u0442\u0433\u0430\u043d\u0434 \u0438\u0442\u0433\u044d\u0445 \u043d\u04e9\u0445\u0446\u04e9\u043b \u0431\u043e\u043b\u0441\u043e\u043d \u0431\u0438\u043b\u044d\u044d. \n# \u0418\u0439\u043c\u0434 \u044d\u0434\u0433\u044d\u044d\u0440 \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0438\u0439\u0433 \u0430\u0448\u0438\u0433\u043b\u0430\u0445\u0433\u04af\u0439\u0433\u044d\u044d\u0440 \u0434\u0430\u0445\u0438\u043d \u0437\u0430\u0433\u0432\u0430\u0440 \u0441\u0443\u0440\u0433\u0430\u0436, \u0442\u0430\u0430\u043c\u0430\u0433\u043b\u0430\u043b\u044b\u0433 \u043d\u044d\u0433\u0442\u0433\u044d\u0445\u044d\u0434 \u0430\u0448\u0438\u0433\u043b\u0430\u0441\u0430\u043d.\n\n# \u0411\u044d\u043b\u0442\u0433\u044d\u0441\u044d\u043d \u04e9\u0433\u04e9\u0433\u0434\u043b\u0438\u0439\u0433 \u0433\u0430\u0440\u0433\u0430\u0436 \u0430\u0432\u0430\u0445\ndf, weather, sub = preprocess(ROOT='..\/input\/ulaanbaatar-city-air-pollution-prediction', \n                              cat_feat = cat_feats, rolling_means=[4, 8, 12], shifts=[2, 4], diffs=[2, 4], aqi_mean=False)\n\n# submission file \u0443\u0442\u0433\u0443\u0443\u0434\u044b\u0433 \u0442\u044d\u0433\u043b\u044d\u0445\nsub[target] = 0\n\n# \u0441\u0443\u0440\u0433\u0430\u043b\u0442\u044b\u043d \u0431\u043e\u043b\u043e\u043d \u0442\u0435\u0441\u0442\u0438\u0439\u043d \u04e9\u0433\u04e9\u0433\u0434\u043b\u0438\u0439\u0433 \u0445\u0443\u0432\u0430\u0430\u0445\ntrain = df.loc[df['date']<'2019-01-01']\ntest = df.loc[df['date']>='2019-01-01']\n\n# \u0437\u0430\u0433\u0432\u0430\u0440 \u0441\u0443\u0440\u0433\u0430\u0445\u0430\u0434 \u0430\u0448\u0438\u0433\u043b\u0430\u0433\u0434\u0430\u0445 \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0434\u044b\u0433 \u0441\u043e\u043d\u0433\u043e\u0445\nfeat = list(train.drop(useless_columns + [target], axis=1))\n\nfor top_n_features in [100]:\n    print(\"*\"*25, \"top_n_features\", top_n_features, \"*\"*25,)\n    sub_light, scores, scores_fold = train_lightGBM(train=train.copy(), test=test.copy(), sub_light=sub.copy(), Nfolds=8,\n                               target=target, feat=feat, cat_feat=cat_feats,\n                               top_n_features=top_n_features, params_k=params_lgb, MODEL_ROOT='models\/lgb')\n\n    print('LighGBM CV:', np.mean(scores), '+\/-', np.std(scores))\n    print('LighGBM CV fold:', np.mean(scores_fold), '+\/-', np.std(scores_fold))\n    sub_light.to_csv(\"sub_lgb_V1_%s.csv\" %top_n_features, index=False)\n        \n    print(\"*\" * 25, \"top_n_features\", top_n_features, \"*\" * 25, )\n\n    sub_cb, scores, scores_fold = training_CatBoost(train=train.copy(), test=test.copy(), sub_cb=sub.copy(), Nfolds=8,\n                               target=target, feat=feat, cat_feat=cat_feats,\n                               top_n_features=top_n_features, params_k=True, MODEL_ROOT='models\/cb')\n\n    print('CatBoost CV:', np.mean(scores), '+\/-', np.std(scores))\n    print('CatBoost CV fold:', np.mean(scores_fold), '+\/-', np.std(scores_fold))\n    sub_cb.to_csv(\"sub_cb_V1_%s.csv\" %top_n_features, index=False)\n    \n    sub_xg, scores, scores_fold = training_XGboost(train=train.copy(), test=test.copy(), sub_xg=sub.copy(), Nfolds=8,\n                               target=target, feat=feat, cat_feat=cat_feats,\n                               top_n_features=top_n_features, params_k=True, MODEL_ROOT='models\/xg')\n\n    print('XGBoost CV:', np.mean(scores), '+\/-', np.std(scores))\n    print('XGBoost CV fold:', np.mean(scores_fold), '+\/-', np.std(scores_fold))\n    sub_xg.to_csv(\"sub_xg_V1_%s.csv\" %top_n_features, index=False)","4ce3518a":"# \u0446\u0430\u0433 \u0430\u0433\u0430\u0430\u0440\u044b\u043d \u04e9\u0433\u04e9\u0433\u0434\u043b\u0438\u0439\u0433 \u0431\u044d\u043b\u0442\u0433\u044d\u0445\ndef prepare_weather_data(ROOT='data', cat_feat=[], rolling_means=[], shifts=[],\n               diffs=[]):\n    \n    #\u0442\u0430\u0441\u0440\u0430\u043b\u0442\u0433\u04af\u0439 \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0438\u0434\n    cont_vars = ['precipIntensity', 'precipProbability', 'temperature', 'apparentTemperature',\n                 'dewPoint', 'humidity', 'windSpeed',\n                 'windBearing', 'cloudCover', 'uvIndex', 'visibility']\n    \n    #\u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u043a\u0430\u043b \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0438\u0434\n    cat_feat = ['summary', 'icon', 'month', 'dayofweek',\n                'hour', 'day', 'dayofyear', 'week', 'year',\n                'month_dummy', 'hour_dummy']\n    \n    #\u0446\u0430\u0433 \u0430\u0433\u0430\u0430\u0440\u044b\u043d \u04e9\u0433\u04e9\u0433\u0434\u043b\u0438\u0439\u0433 \u0434\u0443\u0443\u0434\u0430\u0445\n    weather = pd.read_csv(ROOT + \"\/weather.csv\")\n    \n    #\u043e\u0433\u043d\u043e\u043e\u0440\u04af\u04af \u0445\u04e9\u0440\u0432\u04af\u04af\u043b\u044d\u0445\n    weather['date'] = pd.to_datetime(weather['date'])\n    weather = weather.sort_values(by='date')\n    \n    #\u0437\u0430\u0440\u0438\u043c \u043e\u0433\u043d\u043e\u043e \u0446\u0430\u0433 \u0430\u0433\u0430\u0430\u0440\u044b\u043d \u04e9\u0433\u04e9\u0433\u0434\u04e9\u043b\u0434 \u0431\u0430\u0439\u0433\u0430\u0430\u0433\u04af\u0439 \u0442\u0443\u043b \u0442\u044d\u0434\u0433\u044d\u044d\u0440\u0438\u0439\u0433 \u043e\u0440\u0443\u0443\u043b\u0441\u0430\u043d \u0448\u0438\u043d\u044d \u0445\u04af\u0441\u043d\u044d\u0433\u0442 \u04af\u04af\u0441\u0433\u044d\u0445\n    weather_df = pd.DataFrame()\n    weather_df['date'] = pd.date_range(start=weather.iloc[0]['date'], end=weather.iloc[-1]['date'], freq='H')\n    weather_df = pd.merge(weather_df, weather.iloc[:, 1:], on=['date'], how='left')\n    \n    #'precipIntensity', 'precipProbability' \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0434\u044b\u043d 0 \u0431\u043e\u043b\u043e\u043d 3-\u0430\u0430\u0441 \u04e9\u043d\u0434\u04e9\u0440 \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0434\u044b\u0433 \u0445\u043e\u043e\u0441\u043e\u043d \u0431\u043e\u043b\u0433\u043e\u0445\n    weather_df.loc[weather_df['precipIntensity'] > 3, 'precipIntensity'] = np.nan\n    weather_df.loc[weather_df['precipIntensity']==0, 'precipIntensity']=np.nan\n    weather_df.loc[weather_df['precipProbability'] == 0, 'precipProbability'] = np.nan\n    \n    # \u04e9\u0433\u04e9\u0433\u0434\u0441\u04e9\u043d \u0446\u0430\u0433 \u0430\u0433\u0430\u0430\u0440\u044b\u043d \u04e9\u0433\u04e9\u0433\u0434\u043b\u0438\u0439\u0433 \u0443\u0441\u0442\u0433\u0430\u0445\n    del weather\n\n    # \u0446\u0430\u0433 \u0430\u0433\u0430\u0430\u0440\u044b\u043d \u04e9\u0433\u04e9\u0433\u0434\u04e9\u043b\u0434 \u043d\u044d\u043c\u044d\u043b\u0442 \u04e9\u0433\u04e9\u0433\u0434\u04e9\u043b \u04af\u04af\u0441\u0433\u044d\u0445\n    weather_df = additional_features(weather_df)\n\n    #\u043a\u0430\u0442 \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0434\u044b\u0433 \u043d\u0430\u0442\u0443\u0440\u0430\u043b \u0442\u043e\u043e\u0440\u0443\u0443 \u0445\u04e9\u0440\u0432\u04af\u04af\u043b\u044d\u0445\n    for col in cat_feat:\n        le = preprocessing.LabelEncoder()\n        weather_df.loc[weather_df[col].dropna().index, col] = \\\n            le.fit_transform(weather_df.loc[weather_df[col].dropna().index, col])\n\n    #\u0442\u0430\u0441\u0440\u0430\u043b\u0442\u0433\u04af\u0439 \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0434\u044b\u043d \u0445\u043e\u043e\u0441\u043e\u043d \u0443\u0442\u0433\u044b\u0433 \u0434\u04af\u04af\u0440\u0433\u044d\u0445\n    for col in cont_vars:\n        if weather_df[col].isnull().sum(axis=0) > 0:\n            weather_df[col] = weather_df.groupby(['dayofyear', 'hour'])[col].transform(lambda x: x.fillna(x.mean()))\n            weather_df[col] = weather_df.groupby(['week', 'hour'])[col].transform(lambda x: x.fillna(x.mean()))\n            weather_df[col] = weather_df[col].fillna(weather_df[col].rolling(4, min_periods=1).mean())\n\n    #\u043a\u0430\u0442 \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0434\u044b\u0433 \u0445\u043e\u043e\u0441\u043e\u043d \u0443\u0442\u0433\u044b\u0433 \u0434\u04af\u04af\u0440\u0433\u044d\u0445\n    for col in cat_feat:\n        if weather_df[col].isnull().sum(axis=0)>0:\n            weather_df[col] = weather_df.groupby(['week', 'hour'])[col].transform(lambda x: x.fillna(x.mode()[0]))\n\n    #\u0442\u0430\u0441\u0440\u0430\u043b\u0442\u0433\u04af\u0439 \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0434\u044b\u0433 \u0433\u04af\u0439\u0441\u044d\u043d \u0434\u0443\u043d\u0434\u0430\u0436 \u0443\u0442\u0433\u044b\u0433 \u0434\u04af\u04af\u0440\u0433\u044d\u0445\n    for col in cont_vars:\n        for lag in rolling_means:\n            weather_df['%s_rolling_%s_mean' % (col, lag)] = weather_df[col].rolling(lag).mean()\n    \n    #\u0445\u043e\u0446\u0440\u043e\u0433\u0434\u043e\u043b \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0434\u044b\u0433 \u04af\u04af\u0441\u0433\u044d\u0445\n    for col in (weather_df.drop(['date']+cat_feat, axis=1).columns):\n        for lag in shifts:\n            weather_df['%s_shift_%s' % (col, lag)] = weather_df[col].shift(lag)\n\n    return weather_df\n","4316fa96":"# \u04e8\u0433\u04e9\u0433\u0434\u043b\u0438\u0439\u0433 \u043d\u044d\u0433\u0442\u0433\u044d\u0436, \u0431\u044d\u043b\u0442\u0433\u044d\u0445\ndef preprocess(weather_df, ROOT='data', center = [106.91787, 47.91667], aqi_mean=False):\n\n    # \u0442\u0430\u0441\u0440\u0430\u043b\u0442\u0433\u04af\u0439 \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0438\u0434\n    cont_vars = ['temperature', 'apparentTemperature',\n                 'dewPoint', 'humidity', 'windSpeed',\n                 'windBearing', 'cloudCover', 'uvIndex', 'visibility']\n    \n    # \u043a\u0430\u0442 \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0438\u0434\n    cat_feat = [\"type\", \"source\", \"station\"]\n    \n    # \u04e9\u0433\u04e9\u0433\u0434\u043b\u0438\u0439\u0433 \u0434\u0443\u0443\u0434\u0430\u0445\n    train, test, weather_dontneed, sub = get_data(ROOT=ROOT)\n\n    #\u0441\u0443\u0440\u0433\u0430\u043b\u0442\u044b\u043d \u0431\u043e\u043b\u043e\u043d \u0442\u0435\u0441\u0442\u0438\u0439\u043d \u04e9\u0433\u04e9\u0433\u0434\u043b\u0438\u0439\u0433 \u043d\u044d\u0433\u0442\u0433\u044d\u0445\n    df = pd.concat([train, test], axis=0)\n    \n    #\u0441\u0442\u0430\u043d\u0446 \u0431\u04af\u0440\u0438\u0439\u043d \u0442\u04e9\u0432\u04e9\u04e9\u0441 \u0445\u043e\u043b \u0431\u0430\u0439\u0445 \u0437\u0430\u0439\u0433 \u0442\u043e\u043e\u0446\u043e\u0445\n    df['distance'] = np.sqrt((np.asarray(df['latitude']) - center[0]) ** 2 +\n                             (np.asarray(df['longitude']) - center[1]) ** 2)\n    \n    #\u0441\u0442\u0430\u043d\u0446 \u0431\u04af\u0440\u0438\u0439\u043d \u0442\u04e9\u0432\u04e9\u04e9\u0441 \u0445\u0430\u0437\u0430\u0439\u0445 \u04e9\u043d\u0446\u0433\u0438\u0439\u0433 \u0442\u043e\u043e\u0446\u043e\u0445\n    cos = (np.asarray(df['latitude']) * center[0] + np.asarray(df['longitude']) * center[1]) \/ \\\n          (np.sqrt(np.asarray(df['latitude']) ** 2 + np.asarray(df['longitude']) ** 2) * np.sqrt(\n              center[0] ** 2 + center[1] ** 2))\n\n    df['angle'] = 1000 * np.arccos(cos)\n\n    #\u043e\u0433\u043d\u043e\u043e\u043d\u0434 \u0445\u04e9\u0440\u0432\u04af\u04af\u043b\u044d\u0445\n    df['date'] = pd.to_datetime(df['date'])\n\n    #\u043a\u0430\u0442 \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0434\u044b\u0433 \u0442\u043e\u043e\u043d \u0445\u04e9\u0440\u0432\u04af\u04af\u043b\u044d\u0445\n    for col in cat_feat:\n        le = preprocessing.LabelEncoder()\n        df[col] = le.fit_transform(df[col])\n    \n    #\u0446\u0430\u0433 \u0430\u0433\u0430\u0430\u0440\u044b\u043d \u04e9\u0433\u04e9\u0433\u0434\u04e9\u043b \u0431\u043e\u043b\u043e\u043d \u0441\u0443\u0440\u0433\u0430\u043b\u0442, \u0442\u0435\u0441\u0442\u0438\u0439\u043d \u04e9\u0433\u04e9\u0433\u0434\u043b\u0438\u0439\u0433 \u043d\u044d\u0433\u0442\u0433\u044d\u0445\n    df = pd.merge(df, weather_df, on=[\"date\"], how='left')\n    \n    if aqi_mean==True:\n        #\u0445\u0430\u043c\u0430\u0430\u0440\u0430\u0433\u0447 \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0430\u0430\u0441 \u0445\u0430\u043c\u0430\u0430\u0440\u0441\u0430\u043d \u043d\u044d\u043c\u044d\u043b\u0442 \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447 \u04af\u04af\u0441\u0433\u044d\u0445\n        icols = [['type', 'station', 'month', 'hour'],\n                ['type', 'station', 'week', 'hour'],\n                 ['type', 'station', 'icon', 'month', 'hour'],\n                 ['type', 'station', 'summary', 'month', 'hour']]\n\n        temp_df = df.copy()\n        col_fill = '_' + '_'.join(icols[0]) + '_'\n        for col in icols:\n            col_name = '_' + '_'.join(col) + '_'\n            temp = temp_df[col + ['aqi']].groupby(col, as_index=False)['aqi'].mean()\n            temp = temp.rename(columns={'aqi': 'enc%smean' % col_name})\n            df = df.merge(temp, on=col, copy=False, how='left')\n            df['enc%smean' % col_name] = df['enc%smean' % col_name]\n            df['enc%smean' % col_name] = df['enc%smean' % col_name].fillna(df['enc%smean' % col_fill])\n\n    return df, sub","95565c72":"# \u041c\u0418\u041d\u041c\u0410\u041a\u0421 \u041d\u041e\u0420\u041c\u0410\u041b\u0418\u0417\u0410\u0419\u0428\u041d\ndef normalization(df):\n    if len(df.shape)==2:\n        scaler = MinMaxScaler().fit(df.values)\n        X = scaler.transform(df.values)\n    else:\n        scaler = MinMaxScaler().fit(np.expand_dims(df.values, axis=1))\n        X = scaler.transform(np.expand_dims(df.values, axis=1))\n\n    return X, scaler","bfbc1741":"# \u04e8\u0413\u04e8\u0413\u0414\u04e8\u041b \u041c\u0410\u0410\u041d\u042c \u041a\u0410\u0422 \u0425\u0423\u0412\u042c\u0421\u0410\u0413\u0427\u0418\u0414 \u0410\u0413\u0423\u0423\u041b\u0421\u0410\u041d \u0422\u0423\u041b \u042d\u041c\u0411\u0415\u0414\u0414\u0418\u0419\u041d \u0414\u0410\u0412\u0425\u0410\u0420\u0413\u0410\u0414 \u0417\u041e\u0420\u0418\u0423\u041b\u0416 \u04e8\u0413\u04e8\u0413\u0414\u041b\u0418\u0419\u0413 \u0411\u042d\u041b\u0422\u0413\u042d\u0425\ndef make_2Dinput(dt, cont_cols, cat_cols):\n    input = {\"rnn\": dt[cont_cols].to_numpy()}\n    for i, v in enumerate(cat_cols):\n        input[v] = dt[[v]].to_numpy()\n    return input","b9e676e2":"# \u04e8\u0413\u04e8\u0413\u0414\u04e8\u041b \u041c\u0410\u0410\u041d\u042c \u041a\u0410\u0422 \u0425\u0423\u0412\u042c\u0421\u0410\u0413\u0427\u0418\u0414 \u0410\u0413\u0423\u0423\u041b\u0421\u0410\u041d \u0422\u0423\u041b \u042d\u041c\u0411\u0415\u0414\u0414\u0418\u0419\u041d \u0414\u0410\u0412\u0425\u0410\u0420\u0413\u0410\u0414 \u0417\u041e\u0420\u0418\u0423\u041b\u0416 PYTORCH DATALOADER \u04ae\u04ae\u0421\u0413\u042d\u0425\nclass MAPLoader:\n\n    def __init__(self, X, y, shuffle=True, batch_size=1000, cat_cols=[]):\n        self.X_cont = X[\"rnn\"]\n        try:\n            self.X_cat = np.concatenate([X[k] for k in cat_cols], axis=1)\n        except:\n            self.X_cat = np.concatenate([np.expand_dims(X[k], axis=1) for k in cat_cols], axis=1)\n        self.y = y\n\n        self.shuffle = shuffle\n        self.batch_size = batch_size\n        self.n_conts = self.X_cont.shape[1]\n        self.len = self.X_cont.shape[0]\n        n_batches, remainder = divmod(self.len, self.batch_size)\n\n        if remainder > 0:\n            n_batches += 1\n        self.n_batches = n_batches\n        self.remainder = remainder  # for debugging\n\n        self.idxes = np.array([i for i in range(self.len)])\n\n    def __iter__(self):\n        self.i = 0\n        if self.shuffle:\n            ridxes = self.idxes\n            np.random.shuffle(ridxes)\n            self.X_cat = self.X_cat[[ridxes]]\n            self.X_cont = self.X_cont[[ridxes]]\n            if self.y is not None:\n                self.y = self.y[[ridxes]]\n\n        return self\n\n    def __next__(self):\n        if self.i >= self.len:\n            raise StopIteration\n\n        if self.y is not None:\n            y = torch.FloatTensor(self.y[self.i:self.i + self.batch_size].astype(np.float32))\n\n        else:\n            y = None\n\n        xcont = torch.FloatTensor(self.X_cont[self.i:self.i + self.batch_size])\n        xcat = torch.LongTensor(self.X_cat[self.i:self.i + self.batch_size])\n\n        batch = (xcont, xcat, y)\n        self.i += self.batch_size\n        return batch\n\n    def __len__(self):\n        return self.n_batches\n","9d58a1ec":"#RMSE LOSS FUNCTION\nclass RMSE(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mse = nn.MSELoss()\n\n    def forward(self, y_pred, y_true):\n        y_pred = y_pred.squeeze()\n        return torch.sqrt(self.mse(y_pred, y_true))","c9a600df":"\ndevice = ('cuda' if torch.cuda.is_available() else 'cpu')\n\n#XAVIER INITILIZATION\ndef init_weights(m):\n    if type(m) == nn.Linear:\n        torch.nn.init.xavier_uniform(m.weight)\n        m.bias.data.fill_(0.01)\n\n####### Simple MLP model for 2D input ############################\n\nclass MLP_MAPP(nn.Module):\n    def __init__(self, emb_dims, n_cont, hidden_dim, device=device):\n        super().__init__()\n        self.device = device\n\n        # Embedding layers\n        self.emb_layers = nn.ModuleList([nn.Embedding(x, y) for x, y in emb_dims])\n        n_embs = sum([y for x, y in emb_dims])\n\n        n_embs = n_embs\n        n_cont = n_cont\n        inp_dim = n_cont + n_embs\n        \n        #HIDDEN LAYERS\n        self.fc0 = nn.Linear(inp_dim, hidden_dim)\n        self.drop0 = nn.Dropout(0.4)\n        self.fc1 = nn.Linear(hidden_dim, int(hidden_dim))\n        self.drop1 = nn.Dropout(0.2)\n        self.fc2 = nn.Linear(int(hidden_dim), int(hidden_dim\/2))\n        self.fc3 = nn.Linear(int(hidden_dim\/2), 1)\n        \n        #apply initilizations\n        self.fc0.apply(init_weights)\n        self.fc1.apply(init_weights)\n        self.fc2.apply(init_weights)\n    \n    #train embedding layers and concat cat and cont variables\n    def encode_and_combine_data(self, cont_data, cat_data):\n        xcat = [el(cat_data[:, k]) for k, el in enumerate(self.emb_layers)]\n        xcat = torch.cat(xcat, 1)\n        x = torch.cat([xcat, cont_data], 1)\n        return x\n\n    def forward(self, cont_data, cat_data):\n\n        cont_data = cont_data.to(self.device)\n        cat_data = cat_data.to(self.device)\n        x = self.encode_and_combine_data(cont_data, cat_data)\n\n        hz = F.relu(self.fc0(x))\n        hz = self.drop0(hz)\n        hz = F.relu(self.fc1(hz))\n        hz = self.drop1(hz)\n        hz = F.relu(self.fc2(hz))\n        out = F.relu(self.fc3(hz))\n\n        return out","91c73bc2":"#training neural network model\n\ndef training_nn(df, sub_nn, target, cont_cols, cat_cols, MLP_model,\n                Nfolds, dt_start, dt_end, epoch=50, patience=5, MODEL_ROOT='models\/nn',\n                hidden_dim=1024):\n\n    #Drop Nan values because pytorch can't manage nan values\n    df = df.loc[df[cont_cols].dropna().index]\n\n    # For embedding layers, categorical feautures must be integer and start from zero\n    # create number of uniques of categorical variables for embedding layer\n    uniques = {}\n    for i, v in enumerate((cat_cols)):\n        le = LabelEncoder()\n        le.fit(df[v])\n        df[v] = le.transform(df[v])\n        uniques[v] = (len(df[v].unique()))\n\n    if not os.path.exists(MODEL_ROOT):\n        os.makedirs(MODEL_ROOT)\n\n    ##### MinMax normalization for faster convergence\n    df[cont_cols], scaler_x = normalization(df[cont_cols])\n    y_norm, scaler_y = normalization(df[target].dropna())\n    df.loc[df[target].dropna().index, target] = y_norm\n\n    df = df.reset_index(drop=True)\n\n    test = df.loc[df['date'] > '2018-11-01']\n\n    ## Checking dimension of test set. It must be equal to submission dimension\n    print(test.shape)\n\n    fold_ = 0\n    pred_val = []\n    true_y = []\n\n    for dt0, dt1 in zip(dt_start, dt_end):\n        val_index = df.loc[(df['date'] > dt0) & (df['date'] < dt1), target].dropna().index\n        train_index = df.loc[(df['date'] < dt0) | (df['date'] > dt1), target].dropna().index\n\n        model_path = MODEL_ROOT + '\/model_nn_%s_%s.pt' % (hidden_dim, fold_)\n\n        train_set = df.loc[train_index].sample(frac=0.8)\n        val_set = df.loc[val_index]\n\n        #Make input for pytorch loader because we have categorical and continues features\n        X_train, y_train = make_2Dinput(train_set[cont_cols+cat_cols], cont_cols=cont_cols, cat_cols=cat_cols), train_set[target]\n        validx, validy = make_2Dinput(val_set[cont_cols+cat_cols], cont_cols=cont_cols, cat_cols=cat_cols), val_set[target]\n\n        #Make loader for pytorch\n        train_loader = MAPLoader(X_train, y_train.values, cat_cols=cat_cols, batch_size=1024, shuffle=True)\n        val_loader = MAPLoader(validx, validy.values, cat_cols=cat_cols, batch_size=256, shuffle=False)\n\n        if os.path.isfile(model_path):\n            model_final = torch.load(model_path)\n        else:\n\n            ## make embedding dimensions\n            # cat_feats = ['summary', 'icon', 'month', 'dayofweek', 'hour', 'day',\n            #              'dayofyear', 'week', 'year', 'type', 'source', 'station']\n            dims = [3, 3, 2, 2, 3, 4, 15, 5, 1, 1, 1, 3]\n            emb_dims = [(uniques[col], y) for col, y in zip(cat_cols, dims)]\n\n            # number of continues variables\n            n_cont = train_loader.n_conts\n\n            #neural network model\n            model = MLP_model(emb_dims=emb_dims, n_cont=n_cont, hidden_dim=hidden_dim).to(device)\n\n            #loss function\n            criterion = RMSE()\n\n            #adam optimizer has been used for training\n            optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n\n            #learning rate scheduler\n            scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3,\n                                                      max_lr=1e-2, epochs=epoch, steps_per_epoch=len(train_loader))\n\n\n            best_rmse=np.inf\n            counter=0\n            for ep in range(epoch):\n                train_loss, val_loss = 0, 0\n\n                #training phase for single epoch\n                model.train()\n                for i, (X_cont, X_cat, y) in enumerate(train_loader):\n\n                    optimizer.zero_grad()\n                    out= model(X_cont, X_cat)\n\n                    loss = criterion(out, y.to(device))\n                    loss.backward()\n\n                    optimizer.step()\n                    scheduler.step()\n\n                    with torch.no_grad():\n                        train_loss += loss.item() \/ len(train_loader)\n\n                # Validation phase for single epoch\n                phase='Valid'\n                with torch.no_grad():\n                    model.eval()\n                    y_true = []\n                    y_pred = []\n                    rloss = 0\n                    for i, (X_cont, X_cat, y) in enumerate(val_loader):\n                        out = model(X_cont, X_cat)\n                        loss = criterion(out, y.to(device))\n                        rloss += loss.item() \/ len(val_loader)\n                        y_pred += list(out.detach().cpu().numpy().flatten())\n                        y_true += list(y.cpu().numpy())\n\n                    rmse = np.sqrt(mean_squared_error(scaler_y.inverse_transform(np.expand_dims(y_true, axis=1)),\n                                                      scaler_y.inverse_transform(np.expand_dims(y_pred, axis=1))))\n\n\n                    print(f\"[{phase}] Epoch: {ep} | Tain loss: {train_loss:.4f} | Val Loss: {rloss:.4f} | RMSE: {rmse:.4f} \")\n\n                    if best_rmse > rmse:\n                        best_rmse = rmse\n                        best_model = model\n                        torch.save(best_model, model_path)\n                        counter = 0\n                    else:\n                        counter = counter + 1\n\n                #early stopping \n                if counter>=patience:\n                    print(\"Early stopping\")\n                    break\n\n        fold_ = fold_ + 1\n\n        #call the best model for each fold\n        model_final=torch.load(model_path)\n\n        with torch.no_grad():\n            model_final.eval()\n            y_true = []\n            y_pred = []\n            for i, (X_cont, X_cat, y) in enumerate(val_loader):\n                out = model_final(X_cont, X_cat)\n                y_pred += list(out.detach().cpu().numpy().flatten())\n                y_true += list(y.cpu().numpy())\n\n            y_true = scaler_y.inverse_transform(np.expand_dims(y_true, axis=1))\n            y_pred = scaler_y.inverse_transform(np.expand_dims(y_pred, axis=1))\n\n            true_y +=list(y_true[:, 0])\n            pred_val += list(y_pred[:, 0])\n\n            rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n\n            print(\"*\" * 50, 'Fold %s' % fold_, \"*\" * 50)\n            print(\"CV score:\", rmse)\n            print(\"*\" * 50, 'Fold %s' % fold_, \"*\" * 50)\n        \n        #make prediction for test set\n        testx = make_2Dinput(test[cont_cols+cat_cols], cont_cols=cont_cols, cat_cols=cat_cols)\n        test_loader = MAPLoader(testx, None, cat_cols=cat_cols, batch_size=1024, shuffle=False)\n        y_pred = []\n    \n        with torch.no_grad():\n            model_final.eval()\n            for i, (X_cont, X_cat, y) in enumerate(test_loader):\n                out = model_final(X_cont, X_cat)\n                y_pred += list(out.detach().cpu().numpy().flatten())\n\n        sub_nn.loc[test['ID'], target] =sub_nn.loc[test['ID'], target].values+ scaler_y.inverse_transform(np.expand_dims(y_pred, axis=1))[:, 0]\/Nfolds\n\n    return sub_nn, pred_val, true_y","1aa1bae9":"#### Hyperparameters ############\ncat_feats = ['summary', 'icon', 'month', 'dayofweek', 'hour', 'day',\n             'dayofyear', 'week', 'year', 'type', 'source', 'station']\n\n#useless variables\nuseless_columns = ['ID', 'date', 'latitude', 'longitude']\n\n#target variable\ntarget = 'aqi'\n\n#prepare weather dataset\nweather = prepare_weather_data(ROOT='..\/input\/ulaanbaatar-city-air-pollution-prediction', \n                               cat_feat = cat_feats, rolling_means=[4, 8],\n                                            shifts=[1, 3], diffs=[1, 2])\n\n##merge and prepare train, test and weather datasets \ndf, sub = preprocess(weather, ROOT='..\/input\/ulaanbaatar-city-air-pollution-prediction', aqi_mean=True)\n\n#splitting dataset based on date\n\ndt_start = ['2018-11-01', '2018-11-01', '2018-11-01', '2018-11-01', '2018-11-01']\ndt_end = ['2020-11-01', '2020-11-01', '2020-11-01', '2020-11-01', '2020-11-01']\n\n#prepare submision file\nsub_nn = sub.set_index('ID', drop=True)\nsub_nn['aqi'] = 0\n\n#hidden layers \nfor hidden_layer in [256]:\n    \n    #categorical variables\n    cat_feats = ['summary', 'icon', 'month', 'dayofweek', 'hour', 'day',\n                 'dayofyear', 'week', 'year', 'type', 'source', 'station']\n    \n    #make onehot encoding for these variables\n    one_hot_encode = ['summary', 'icon', 'year', 'type', 'station']\n\n    for col in one_hot_encode:\n        for e in df[col].unique():\n            df['%s_%s' %(col, e)] = np.where(df[col].values==e, 1, 0)\n\n    #continues variables\n    cont_cols = list(df.drop(useless_columns+cat_feats+['aqi'], axis=1).columns)\n    print(np.asarray(cont_cols))\n    \n    ##train neural network model\n    print(\"*\" * 25, \"hidden_layer\", hidden_layer, \"*\" * 25, )\n    sub_nn= sub.set_index('ID', drop=True)\n    sub_nn, val_pred, true_y = training_nn(df=df, sub_nn=sub_nn, target=target, MLP_model=MLP_MAPP,\n                                                cont_cols=cont_cols, cat_cols=cat_feats, Nfolds=5,\n                                                dt_start=dt_start, dt_end=dt_end,\n                                                epoch=50, patience=8, MODEL_ROOT='models\/nn',\n                                                hidden_dim=hidden_layer)\n\n    print('Neural Network CV:', np.sqrt(mean_squared_error(np.asarray(true_y), np.asarray(val_pred))))\n    sub_nn.to_csv(\"sub_nn_%s.csv\" %hidden_layer)\n    \n    \n#merge and prepare train, test and weather datasets \ndf, sub = preprocess(weather, ROOT='..\/input\/ulaanbaatar-city-air-pollution-prediction', aqi_mean=False)\n\n#prepare submision file\nsub_nn = sub.set_index('ID', drop=True)\nsub_nn['aqi'] = 0\n\n# \u0445\u0430\u043c\u0430\u0430\u0440\u0430\u0433\u0447 \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0434\u0430\u0430\u0441 \u04af\u04af\u0441\u0433\u044d\u0441\u044d\u043d \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0434\u044b\u0433 \u0430\u0448\u0438\u0433\u043b\u0430\u0445\u0433\u04af\u0439 neural network \u0437\u0430\u0433\u0432\u0430\u0440\u044b\u0433 \u0441\u0443\u0440\u0433\u0430\u0445\n# \u043c\u0438\u043d\u0438\u0439 \u0445\u0443\u0432\u044c\u0434 \u0442\u0435\u0441\u0442 \u04e9\u0433\u04e9\u0433\u0434\u04e9\u043b \u0434\u044d\u044d\u0440 \u043d\u044d\u044d\u043b\u0442\u0442\u044d\u0439 \u04e9\u0433\u04e9\u0433\u0434\u0441\u04e9\u043d \u0445\u0430\u043c\u0430\u0430\u0440\u0430\u0433\u0447 \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0438\u0439\u043d \u0443\u0442\u0433\u044b\u0433 \n# \u0445\u0430\u043c\u0430\u0430\u0440\u0430\u0433\u0447 \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0430\u0430\u0441 \u04af\u04af\u0441\u0433\u044d\u0441\u044d\u043d \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0434\u044b\u043d \u0442\u043e\u043e\u0446\u043e\u043e\u043b\u043e\u043b\u0434 \u043e\u0440\u0443\u0443\u043b\u0441\u0430\u043d \u043d\u044c \u0445\u0443\u0443\u0440\u0430\u043c\u0447 CV \u0443\u0442\u0433\u0430\u043d\u0434 \u0438\u0442\u0433\u044d\u0445 \u043d\u04e9\u0445\u0446\u04e9\u043b \u0431\u043e\u043b\u0441\u043e\u043d \u0431\u0438\u043b\u044d\u044d. \n# \u0418\u0439\u043c\u0434 \u044d\u0434\u0433\u044d\u044d\u0440 \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0438\u0439\u0433 \u0430\u0448\u0438\u0433\u043b\u0430\u0445\u0433\u04af\u0439\u0433\u044d\u044d\u0440 \u0434\u0430\u0445\u0438\u043d \u0437\u0430\u0433\u0432\u0430\u0440 \u0441\u0443\u0440\u0433\u0430\u0436, \u0442\u0430\u0430\u043c\u0430\u0433\u043b\u0430\u043b\u044b\u0433 \u043d\u044d\u0433\u0442\u0433\u044d\u0445\u044d\u0434 \u0430\u0448\u0438\u0433\u043b\u0430\u0441\u0430\u043d.\nfor hidden_layer in [256]:\n    \n    #categorical variables\n    cat_feats = ['summary', 'icon', 'month', 'dayofweek', 'hour', 'day',\n                 'dayofyear', 'week', 'year', 'type', 'source', 'station']\n    \n    #make onehot encoding for these variables\n    one_hot_encode = ['summary', 'icon', 'year', 'type', 'station']\n\n    for col in one_hot_encode:\n        for e in df[col].unique():\n            df['%s_%s' %(col, e)] = np.where(df[col].values==e, 1, 0)\n\n    #continues variables\n    cont_cols = list(df.drop(useless_columns+cat_feats+['aqi'], axis=1).columns)\n    print(np.asarray(cont_cols))\n    \n    \n    ##train neural network model\n    print(\"*\" * 25, \"hidden_layer\", hidden_layer, \"*\" * 25, )\n    sub_nn= sub.set_index('ID', drop=True)\n    sub_nn, val_pred, true_y = training_nn(df=df, sub_nn=sub_nn, target=target, MLP_model=MLP_MAPP,\n                                                cont_cols=cont_cols, cat_cols=cat_feats, Nfolds=5,\n                                                dt_start=dt_start, dt_end=dt_end,\n                                                epoch=50, patience=8, MODEL_ROOT='models\/nn_v1',\n                                                hidden_dim=hidden_layer)\n\n    print('Neural Network CV:', np.sqrt(mean_squared_error(np.asarray(true_y), np.asarray(val_pred))))\n    sub_nn.to_csv(\"sub_nn_V1_%s.csv\" %hidden_layer)","9f2f6e8f":"#preprocess dataset for LSTM\ndef preprocess_rnn(ROOT='data', cat_feat=[], rolling_means=[], shifts=[], diffs=[], aqi_mean=False):\n    \n    #Continues variables\n    rnn_cols = ['temperature', 'apparentTemperature', 'dewPoint', 'humidity',\n                'windSpeed', 'windBearing', 'cloudCover', 'uvIndex', 'visibility']\n\n    #categorical variables\n    cat_feat = [\"type\", \"source\", \"station\"]\n\n    #call datasets\n    train, test, weather, sub = get_data(ROOT=ROOT)\n    \n    #concat train and test sets\n    df = pd.concat([train, test], axis=0)\n    \n    #parse date\n    df['date'] = pd.to_datetime(df['date'])\n    weather['date'] = pd.to_datetime(weather['date'])\n    \n    #fix visibility variable\n    lower_max =  weather.loc[weather['Unnamed: 0'] < 31058, 'visibility'].max()\n    \n    #checking visibility by plotting it\n    weather['visibility'] = np.where(weather['visibility'].values>lower_max, lower_max, weather['visibility'].values)\n    weather['visibility'].hist()\n    plt.show()\n    \n    #weather data by sort\n    weather = weather.sort_values(by='date')\n    \n    #create new weather dataframe with missing dates\n    weather_df = pd.DataFrame()\n    weather_df['date'] = pd.date_range(start=weather.iloc[0]['date'], end=weather.iloc[-1]['date'], freq='H')\n    weather_df = pd.merge(weather_df, weather.iloc[:, 1:], on=['date'], how='left')\n\n    #delete given weather set after creating new one\n    del weather\n    \n    #generate additional features\n    weather_df = additional_features(weather_df)\n\n    #categorical variables should be converted to integer\n    for col in cat_feat:\n        le = preprocessing.LabelEncoder()\n        df[col] = le.fit_transform(df[col])\n    \n    #fill missing values for categorical features\n    for col in ['summary', 'icon', \"year\", \"dayofyear\", \"hour\"]:\n        weather_df[col] = weather_df[col].fillna(method=\"ffill\")\n        weather_df[col] = weather_df[col].fillna(method=\"bfill\")\n        le = preprocessing.LabelEncoder()\n        weather_df[col] = le.fit_transform(weather_df[col])\n\n    #fill misssing values for continues variables\n    for col in rnn_cols:\n        weather_df[col] = weather_df[col].fillna(weather_df[col].rolling(4, min_periods=1).mean())\n        weather_df[col] = weather_df[col].fillna(weather_df.groupby([\"dayofyear\", \"hour\"])[col].transform('mean'))\n    \n    #generate rolling mean variables for continues variables\n    for col in rnn_cols:\n        for lag in rolling_means:\n            weather_df['%s_rolling_%s_mean' % (col, lag)] = weather_df[col].rolling(lag).mean()\n\n    \n    #weather_df = weather_df.dropna()\n    df_full = pd.DataFrame()\n    for stat in df['station'].unique():\n        for type in df['type'].unique():\n            temp = df.loc[(df['station']==stat) & (df['type']==type)]\n            if temp.shape[0]>0:\n                temp = pd.merge(weather_df, temp, on=[\"date\"], how='left')\n                print(temp.shape)\n                temp[['latitude', 'longitude', 'station', 'source', 'type']]=temp[\n                    ['latitude', 'longitude', 'station', 'source', 'type']].fillna(method='ffill')\n                temp[['latitude', 'longitude', 'station', 'source', 'type']] = temp[\n                    ['latitude', 'longitude', 'station', 'source', 'type']].fillna(method='bfill')\n                df_full = pd.concat([df_full, temp], axis=0)\n    \n    if aqi_mean==True:\n        #generate new variables using target variable\n        icols = [['type', 'station', 'month', 'hour'],\n                ['type', 'station', 'week', 'hour'],\n                 ['type', 'station', 'icon', 'month', 'hour'],\n                 ['type', 'station', 'summary', 'month', 'hour']]\n\n        temp_df = df_full.copy()\n        temp_df = temp_df.dropna()\n        col_fill = '_' + '_'.join(icols[0]) + '_'\n        for col in icols:\n            col_name = '_' + '_'.join(col) + '_'\n            temp = temp_df[col + ['aqi']].groupby(col, as_index=False)['aqi'].mean()\n            temp = temp.rename(columns={'aqi': 'enc%smean' % col_name})\n            df_full = df_full.merge(temp, on=col, copy=False, how='left')\n            df_full['enc%smean' % col_name] = df_full['enc%smean' % col_name]\n            df_full['enc%smean' % col_name] = df_full['enc%smean' % col_name].fillna(method='ffill')\n            df_full['enc%smean' % col_name] = df_full['enc%smean' % col_name].fillna(df_full['enc%smean' % col_fill])\n\n    return df_full, weather_df, sub","11d0eced":"#make sequence data\ndef sliding_windows(data, seq_length=23):\n    x = []\n    for i in range(len(data)-seq_length-1):\n        _x = data[i:(i+seq_length+1)]\n        x.append(_x)\n    return np.array(x)","2b925e53":"#make 3D [batch_size, sequence_length, number_of_variables] input for LSTM model\n\ndef make3Dinput(df_full, rnn_cols, cat_feats, seq_len = 8, test=False):\n    \n    #if test set, we don't need target variable\n    #instead, we need index of row (ID variable)\n    if test==True:\n        rnns = []\n        cats = {}\n        output=[]\n        k=0\n        for typ in (df_full['type'].unique()):\n            for stat in (df_full['station'].unique()):\n                #select subset based type and station\n                df_temp = df_full.loc[(df_full['type'] == typ) & (df_full['station'] == stat)]\n                if df_temp.shape[0]>0:\n                    #the fisrt seq_len rows should be removed\n                    #This is test set. So we select ID as our target\n                    temp_target = df_full.loc[(df_full['type'] == typ) & (df_full['station'] == stat), 'ID'].to_numpy()[seq_len:]\n                    \n                    #we don't need nan target variables\n                    output.append(temp_target[~np.isnan(temp_target)])\n                    \n                    #if target variable is nan, we don't need those rows as well\n                    rnns.append(sliding_windows(df_temp[rnn_cols].values, seq_length=seq_len - 1)[~np.isnan(temp_target)])\n                    \n                    #for embedding layers, categorical variables should be appended one by one\n                    for v in cat_feats:\n                        if k == 0:\n                            cats[v] = list(df_temp[[v]].to_numpy()[seq_len:][~np.isnan(temp_target)])\n                        else:\n                            cats[v] += list(df_temp[[v]].to_numpy()[seq_len:][~np.isnan(temp_target)])\n                    k = k + 1\n\n        output = np.concatenate((output), axis=0)\n        rnns = np.concatenate((rnns), axis=0)\n        \n        #make dictionary for pytorch dataloader\n        inputX = {}\n        inputX['rnn'] = rnns\n        for i, v in enumerate(cat_feats):\n            inputX[v] = np.concatenate((cats[v]), axis=0)\n\n    else:\n        #training set, we need target variable \n        output = []\n        rnns = []\n        cats = {}\n        k = 0\n        for typ in (df_full['type'].unique()):\n            for stat in (df_full['station'].unique()):\n                df_temp = df_full.loc[(df_full['type'] == typ) & (df_full['station'] == stat)]\n                if df_temp.shape[0] > 0:\n                    \n                    #we select our target variable here\n                    temp_target = df_full.loc[(df_full['type'] == typ) & (df_full['station'] == stat), 'aqi'].to_numpy()[seq_len:]\n                    output.append(temp_target[~np.isnan(temp_target)])\n                    rnns.append(sliding_windows(df_temp[rnn_cols].values, seq_length=seq_len - 1)[~np.isnan(temp_target)])\n                    for v in cat_feats:\n                        if k == 0:\n                            cats[v] = list(df_temp[[v]].to_numpy()[seq_len:][~np.isnan(temp_target)])\n                        else:\n                            cats[v] += list(df_temp[[v]].to_numpy()[seq_len:][~np.isnan(temp_target)])\n\n                    k = k + 1\n\n        output = np.concatenate((output), axis=0)\n        rnns = np.concatenate((rnns), axis=0)\n        \n        #make dictionary for pytorch dataloader\n        inputX = {}\n        inputX['rnn'] = rnns\n        for i, v in enumerate(cat_feats):\n            inputX[v] = np.concatenate((cats[v]), axis=0)\n\n    return inputX, output\n\n","5fbe2fe9":"####### LSTM model for 3D input \n\nclass LSTM_MAPP(nn.Module):\n\n    def __init__(self, emb_dims, input_rnn_size, hidden_size, num_layers, seq_length,\n                 device=device):\n        \n        super(LSTM_MAPP, self).__init__()\n\n        self.num_layers = num_layers\n        self.input_rnn_size = input_rnn_size\n        self.hidden_size = hidden_size\n        self.seq_length = seq_length\n        self.device = device\n\n        # Embedding layers\n        self.emb_layers = nn.ModuleList([nn.Embedding(x, y) for x, y in emb_dims])\n        n_embs = sum([y for x, y in emb_dims])\n\n        #LSTM layer\n        self.lstm = nn.LSTM(input_size=input_rnn_size, hidden_size=hidden_size,\n                            num_layers=num_layers, batch_first=True)\n\n        \n        inp_dim = n_embs + hidden_size\n        self.models = torch.nn.ModuleList()\n        \n        #Feed forward layers\n        self.fc0 = nn.Linear(inp_dim, 512)\n        self.drop = nn.Dropout(0.25)\n        self.fc1 = nn.Linear(512, 1)\n        \n        #apply initilization\n        self.lstm.apply(init_weights)\n        self.fc0.apply(init_weights)\n        self.fc1.apply(init_weights)\n\n    #train embedding layers\n    def encode_data(self, cat_data):\n        xcat = [el(cat_data[:, k]) for k, el in enumerate(self.emb_layers)]\n        xcat = torch.cat(xcat, 1)\n        return xcat\n    \n    \n    def forward(self, cont_x, cat_x):\n        cont_x = cont_x.to(self.device)\n        cat_x = cat_x.to(self.device)\n\n        h_0 = Variable(torch.zeros(\n            self.num_layers, cont_x.size(0), self.hidden_size)).to(device)\n\n        c_0 = Variable(torch.zeros(\n            self.num_layers, cont_x.size(0), self.hidden_size)).to(device)\n\n        # Propagate input through LSTM\n        lstm_out, _ = self.lstm(cont_x, (h_0, c_0))\n        \n        #embed categorical variables\n        cat_out = self.encode_data(cat_x)\n        \n        #concat output of lstm and embedded vectors\n        inp = torch.cat((lstm_out[:, -1, :], cat_out), dim=1)\n        out = F.relu(self.fc0(inp))\n        out = self.drop(out)\n        out = self.fc1(out)\n\n        return out","4bd72aff":"#Training LSTM model\n\ndef training_lstm(df, sub_nn, target, dt_start, cont_cols, cat_cols,\n                Nfolds, seq_len=8, epoch=50, patience=5,\n                num_layers=1, MODEL_ROOT='models\/lstm',\n                hidden_dim=512):\n\n    uniques = {}\n    \n    #create number of uniques of categorical variables for embedding layer\n    for i, v in enumerate((cat_cols)):\n        le = LabelEncoder()\n        le.fit(df[v].dropna())\n        df[v] = le.transform(df[v].fillna(0))\n        uniques[v] = (len(df[v].dropna().unique()))\n\n    if not os.path.exists(MODEL_ROOT):\n        os.makedirs(MODEL_ROOT)\n    \n\n    fold_ = 0\n    scores = []\n    scores_fold = []\n\n    for dt in dt_start:\n        #test train splitting\n        train = df.loc[df['date'] <= '2018-10-31']\n        val = df.loc[(df['date'] > dt)]\n        test = df.loc[df['date'] > '2018-10-31']\n        \n        #fill missing values by zero\n        test[cont_cols+cat_cols] = test[cont_cols+cat_cols].fillna(0)\n        train[cont_cols + cat_cols] = train[cont_cols + cat_cols].fillna(0)\n        val[cont_cols + cat_cols] = val[cont_cols + cat_cols].fillna(0)\n        train = train.reset_index(drop=True)\n        val = val.reset_index(drop=True)\n        test = test.reset_index(drop=True)\n        \n        #MinMax normalization for input variables\n        train[cont_cols], scaler_x = normalization(train[cont_cols])\n        test[cont_cols] = scaler_x.transform(test[cont_cols].values)\n        val[cont_cols] = scaler_x.transform(val[cont_cols].values)\n        \n        #Minmax normalizatin for target variables\n        train_y, scaler_y = normalization(train.loc[~train[target].isnull(), target])\n        train.loc[~train[target].isnull(), target] = train_y\n        val_y = scaler_y.transform(val.loc[~val[target].isnull(), [target]])\n        val.loc[~val[target].isnull(), target] = val_y\n        test_y = scaler_y.transform(test.loc[~test[target].isnull(), [target]])\n        test.loc[~test[target].isnull(), target] = test_y\n\n        #Create 3D input for LSTM model\n        trainX, trainY = make3Dinput(df_full=train,\n                                       rnn_cols=cont_cols, cat_feats=cat_cols,\n                                       seq_len=seq_len, test=False)\n\n        validx, validy = make3Dinput(df_full=val,\n                                     rnn_cols=cont_cols, cat_feats=cat_cols,\n                                     seq_len=seq_len, test=False)\n\n        validx1, validy1 = make3Dinput(df_full=test,\n                                   rnn_cols=cont_cols, cat_feats=cat_cols,\n                                   seq_len=seq_len, test=False)\n\n        testx, testy = make3Dinput(df_full=test,\n                               rnn_cols=cont_cols, cat_feats=cat_cols,\n                               seq_len=seq_len, test=True)\n        \n        #Make test dataloader for pytorch\n        test_loader = MAPLoader(testx, testy, cat_cols=cat_cols, batch_size=1024, shuffle=False)\n\n\n        model_path = MODEL_ROOT + '\/model_lstm_%s_%s.pt' % (hidden_dim, fold_)\n\n        X_train, y_train = trainX, trainY\n        \n        #Make train and validation dataloaders for pytorch\n        train_loader = MAPLoader(X_train, y_train, cat_cols=cat_cols, batch_size=1024, shuffle=True)\n        val_loader = MAPLoader(validx, validy, cat_cols=cat_cols, batch_size=512, shuffle=False)\n        val_loader1 = MAPLoader(validx1, validy1, cat_cols=cat_cols, batch_size=256, shuffle=False)\n        \n        #create embedding dimensions \n        # cat_feats = ['summary', 'icon', 'dayofyear', 'hour', 'year', 'month', 'day',\n        #              'dayofweek', 'station', 'type', 'source', 'week']\n        dims = [1, 1, 15, 4, 1, 3, 5, 2, 3, 1, 1, 5]\n        emb_dims = [(uniques[col], y) for col, y in zip(cat_cols, dims)]\n        \n        #number continues variables\n        n_cont = len(cont_cols)\n\n        #LSTM model\n        model = LSTM_MAPP(emb_dims=emb_dims, input_rnn_size=n_cont,\n                          hidden_size=hidden_dim, num_layers=num_layers, seq_length=seq_len).to(device)\n        \n        #Loss function\n        criterion = RMSE()\n\n   \n         #Adam optimizer\n        optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n        #learning rate scheduler\n        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3,\n                                                  max_lr=1e-2, epochs=epoch, steps_per_epoch=len(train_loader))\n       \n\n\n        best_rmse=np.inf\n        best_model=None\n        counter=0\n        for ep in range(epoch):\n            train_loss, val_loss = 0, 0\n            \n            #model training phase for single epoch\n            model.train()\n            for i, (X_cont, X_cat, y) in enumerate(train_loader):\n\n                optimizer.zero_grad()\n\n                out = model(X_cont, X_cat)\n\n                loss = criterion(out, y.to(device))\n\n                loss.backward()\n                optimizer.step()\n                scheduler.step()\n\n                with torch.no_grad():\n                    train_loss += loss.item() \/ len(train_loader)\n\n            # Validation phase for single epoch\n            phase='Val'\n            with torch.no_grad():\n                model.eval()\n\n                y_true = []\n                y_pred = []\n                rloss = 0\n\n                for i, (X_cont, X_cat, y) in enumerate(val_loader1):\n                    out = model(X_cont, X_cat)\n\n                    loss = criterion(out, y.to(device))\n\n                    rloss += loss.item() \/ len(val_loader1)\n                    y_pred += list(out.detach().cpu().numpy().flatten())\n                    y_true += list(y.cpu().numpy())\n\n                rmse = np.sqrt(mean_squared_error(scaler_y.inverse_transform(np.expand_dims(y_true, axis=1)),\n                                                  scaler_y.inverse_transform(np.expand_dims(y_pred, axis=1))))\n                if best_rmse>rmse:\n                    best_rmse=rmse\n                    best_model=model\n                    torch.save(best_model, model_path)\n                    counter = 0\n                else:\n                    counter = counter+1\n\n                y_true = []\n                y_pred = []\n                for i, (X_cont, X_cat, y) in enumerate(val_loader):\n                    out = model(X_cont, X_cat)\n\n                    y_pred += list(out.detach().cpu().numpy().flatten())\n                    y_true += list(y.cpu().numpy())\n\n                rmse_fold = np.sqrt(mean_squared_error(scaler_y.inverse_transform(np.expand_dims(y_true, axis=1)),\n                                                       scaler_y.inverse_transform(np.expand_dims(y_pred, axis=1))))\n\n                print(f\"[{phase}] Epoch: {ep} | Tain loss: {train_loss:.4f} | Val Loss: {rloss:.4f} | RMSE: {rmse:.4f} | RMSE fold: {rmse_fold:.4f}\")\n                \n\n            if counter>=patience:\n                print(\"Early stopping\")\n                break\n\n        fold_ = fold_ + 1\n        \n        #call the best model\n        final_model = torch.load(model_path)\n\n        with torch.no_grad():\n            final_model.eval()\n\n            y_true = []\n            y_pred = []\n\n            for i, (X_cont, X_cat, y) in enumerate(val_loader1):\n                out = final_model(X_cont, X_cat)\n                y_pred += list(out.detach().cpu().numpy().flatten())\n                y_true += list(y.cpu().numpy())\n\n            rmse = np.sqrt(mean_squared_error(scaler_y.inverse_transform(np.expand_dims(y_true, axis=1)),\n                                              scaler_y.inverse_transform(np.expand_dims(y_pred, axis=1))))\n    \n\n            scores.append(rmse)\n            y_true = []\n            y_pred = []\n            for i, (X_cont, X_cat, y) in enumerate(val_loader):\n                out = final_model(X_cont, X_cat)\n\n                y_pred += list(out.detach().cpu().numpy().flatten())\n                y_true += list(y.cpu().numpy())\n\n            rmse_fold = np.sqrt(mean_squared_error(scaler_y.inverse_transform(np.expand_dims(y_true, axis=1)),\n                                              scaler_y.inverse_transform(np.expand_dims(y_pred, axis=1))))\n\n        \n            scores_fold.append(rmse_fold)\n\n        ## predict test set\n        y_pred = []\n        ids = []\n        with torch.no_grad():\n            final_model.eval()\n            for i, (X_cont, X_cat, y) in enumerate(test_loader):\n                out = final_model(X_cont, X_cat)\n                y_pred += list(out.detach().cpu().numpy().flatten())\n                ids += list(y.cpu().numpy())\n\n        y_pred = scaler_y.inverse_transform(np.expand_dims(y_pred, axis=1))\n        sub_nn.loc[ids, target] = sub_nn.loc[ids, target] + y_pred[:, 0]\/Nfolds\n\n    return sub_nn, scores, scores_fold","f9a1069a":"# training mistaked LSTM model\n# \u0411\u0438 \u044d\u043d\u0434 for loop \u0434\u044d\u044d\u0440 epoch \u0433\u044d\u0441\u044d\u043d \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0438\u0439\u0433 \u0445\u0430\u043c\u0442 \u0437\u0430\u0440\u043b\u0430\u0441\u0430\u043d \u0431\u0430\u0439\u0441\u0430\u043d. \n# \u0422\u0438\u0439\u043c \u0431\u043e\u043b\u043e\u0445\u043e\u043e\u0440 \u0445\u0430\u043c\u0433\u0438\u0439\u043d \u0441\u0430\u0439\u043d \u0437\u0430\u0433\u0432\u0430\u0440\u0430\u0430 \u043e\u043b\u0436 \u0430\u0432\u0447 \u0447\u0430\u0434\u0430\u0430\u0433\u04af\u0439 \u0431\u0430\u0439\u0445 \u0431\u043e\u043b\u043e\u043c\u0436\u0442\u043e\u0439. \n# \u0413\u044d\u0441\u044d\u043d \u0447 \u044d\u043d\u044d\u043d\u044d\u044d\u0441 \u0433\u0430\u0440\u0441\u0430\u043d \u04af\u0440 \u0434\u04af\u043d\u0433 \u0431\u043b\u0435\u043d\u0434\u0438\u043d\u0433 \u0445\u0438\u0439\u0445\u044d\u0434 \u0430\u0448\u0438\u0433\u043b\u0430\u0441\u0430\u043d \u0442\u0443\u043b \u044d\u043d\u044d \u0444\u0443\u043d\u043a\u0446\u0438\u0439\u0433 \u0437\u0430\u0439\u043b\u0448\u0433\u04af\u0439 \u0434\u0430\u0445\u0438\u043d \n# \u0431\u0438\u0447\u0438\u0445 \u0445\u044d\u0440\u044d\u0433\u0442\u044d\u0439 \u044e\u043c.\n\ndef training_lstm_mistaked(df, sub_nn, target, dt_start, cont_cols, cat_cols,\n                Nfolds, seq_len=8, epoch=50, patience=5,\n                num_layers=1, MODEL_ROOT='models\/lstm',\n                hidden_dim=512):\n\n    uniques = {}\n    \n    #create number of uniques of categorical variables for embedding layer\n    for i, v in enumerate((cat_cols)):\n        le = LabelEncoder()\n        le.fit(df[v].dropna())\n        df[v] = le.transform(df[v].fillna(0))\n        uniques[v] = (len(df[v].dropna().unique()))\n\n    if not os.path.exists(MODEL_ROOT):\n        os.makedirs(MODEL_ROOT)\n    \n\n    fold_ = 0\n    scores = []\n    scores_fold = []\n\n    for dt in dt_start:\n        #test train splitting\n        train = df.loc[df['date'] <= '2018-10-31']\n        val = df.loc[(df['date'] > dt)]\n        test = df.loc[df['date'] > '2018-10-31']\n        \n        #fill missing values by zero\n        test[cont_cols+cat_cols] = test[cont_cols+cat_cols].fillna(0)\n        train[cont_cols + cat_cols] = train[cont_cols + cat_cols].fillna(0)\n        val[cont_cols + cat_cols] = val[cont_cols + cat_cols].fillna(0)\n        train = train.reset_index(drop=True)\n        val = val.reset_index(drop=True)\n        test = test.reset_index(drop=True)\n        \n        #MinMax normalization for input variables\n        train[cont_cols], scaler_x = normalization(train[cont_cols])\n        test[cont_cols] = scaler_x.transform(test[cont_cols].values)\n        val[cont_cols] = scaler_x.transform(val[cont_cols].values)\n        \n        #Minmax normalizatin for target variables\n        train_y, scaler_y = normalization(train.loc[~train[target].isnull(), target])\n        train.loc[~train[target].isnull(), target] = train_y\n        val_y = scaler_y.transform(val.loc[~val[target].isnull(), [target]])\n        val.loc[~val[target].isnull(), target] = val_y\n        test_y = scaler_y.transform(test.loc[~test[target].isnull(), [target]])\n        test.loc[~test[target].isnull(), target] = test_y\n\n        #Create 3D input for LSTM model\n        trainX, trainY = make3Dinput(df_full=train,\n                                       rnn_cols=cont_cols, cat_feats=cat_cols,\n                                       seq_len=seq_len, test=False)\n\n        validx, validy = make3Dinput(df_full=val,\n                                     rnn_cols=cont_cols, cat_feats=cat_cols,\n                                     seq_len=seq_len, test=False)\n\n        validx1, validy1 = make3Dinput(df_full=test,\n                                   rnn_cols=cont_cols, cat_feats=cat_cols,\n                                   seq_len=seq_len, test=False)\n\n        testx, testy = make3Dinput(df_full=test,\n                               rnn_cols=cont_cols, cat_feats=cat_cols,\n                               seq_len=seq_len, test=True)\n        \n        #Make test dataloader for pytorch\n        test_loader = MAPLoader(testx, testy, cat_cols=cat_cols, batch_size=1024, shuffle=False)\n\n\n        model_path = MODEL_ROOT + '\/model_lstm_%s_%s.pt' % (hidden_dim, fold_)\n\n        X_train, y_train = trainX, trainY\n        \n        #Make train and validation dataloaders for pytorch\n        train_loader = MAPLoader(X_train, y_train, cat_cols=cat_cols, batch_size=1024, shuffle=True)\n        val_loader = MAPLoader(validx, validy, cat_cols=cat_cols, batch_size=512, shuffle=False)\n        val_loader1 = MAPLoader(validx1, validy1, cat_cols=cat_cols, batch_size=256, shuffle=False)\n        \n        #create embedding dimensions \n        # cat_feats = ['summary', 'icon', 'dayofyear', 'hour', 'year', 'month', 'day',\n        #              'dayofweek', 'station', 'type', 'source', 'week']\n        dims = [1, 1, 15, 4, 1, 3, 5, 2, 3, 1, 1, 5]\n        emb_dims = [(uniques[col], y) for col, y in zip(cat_cols, dims)]\n        \n        #number continues variables\n        n_cont = len(cont_cols)\n\n        #LSTM model\n        model = LSTM_MAPP(emb_dims=emb_dims, input_rnn_size=n_cont,\n                          hidden_size=hidden_dim, num_layers=num_layers, seq_length=seq_len).to(device)\n        \n        #Loss function\n        criterion = RMSE()\n\n   \n         #Adam optimizer\n        optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n        #learning rate scheduler\n        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3,\n                                                  max_lr=1e-2, epochs=epoch, steps_per_epoch=len(train_loader))\n       \n\n\n        best_rmse=np.inf\n        best_model=None\n        counter=0\n        \n        #I made mistake here.######################################################################################################################################\n        for epoch in range(epoch):\n            train_loss, val_loss = 0, 0\n            \n            #model training phase for single epoch\n            model.train()\n            for i, (X_cont, X_cat, y) in enumerate(train_loader):\n\n                optimizer.zero_grad()\n\n                out = model(X_cont, X_cat)\n\n                loss = criterion(out, y.to(device))\n\n                loss.backward()\n                optimizer.step()\n                scheduler.step()\n\n                with torch.no_grad():\n                    train_loss += loss.item() \/ len(train_loader)\n\n            # Validation phase for single epoch\n            phase='Val'\n            with torch.no_grad():\n                model.eval()\n\n                y_true = []\n                y_pred = []\n                rloss = 0\n\n                for i, (X_cont, X_cat, y) in enumerate(val_loader1):\n                    out = model(X_cont, X_cat)\n\n                    loss = criterion(out, y.to(device))\n\n                    rloss += loss.item() \/ len(val_loader1)\n                    y_pred += list(out.detach().cpu().numpy().flatten())\n                    y_true += list(y.cpu().numpy())\n\n                rmse = np.sqrt(mean_squared_error(scaler_y.inverse_transform(np.expand_dims(y_true, axis=1)),\n                                                  scaler_y.inverse_transform(np.expand_dims(y_pred, axis=1))))\n                if best_rmse>rmse:\n                    best_rmse=rmse\n                    best_model=model\n                    torch.save(best_model, model_path)\n                    counter = 0\n                else:\n                    counter = counter+1\n\n                y_true = []\n                y_pred = []\n                for i, (X_cont, X_cat, y) in enumerate(val_loader):\n                    out = model(X_cont, X_cat)\n\n                    y_pred += list(out.detach().cpu().numpy().flatten())\n                    y_true += list(y.cpu().numpy())\n\n                rmse_fold = np.sqrt(mean_squared_error(scaler_y.inverse_transform(np.expand_dims(y_true, axis=1)),\n                                                       scaler_y.inverse_transform(np.expand_dims(y_pred, axis=1))))\n\n                print(f\"[{phase}] Epoch: {epoch} | Tain loss: {train_loss:.4f} | Val Loss: {rloss:.4f} | RMSE: {rmse:.4f} | RMSE fold: {rmse_fold:.4f}\")\n                \n\n            if counter>=patience:\n                print(\"Early stopping\")\n                break\n\n        fold_ = fold_ + 1\n        \n        #call the best model\n        final_model = torch.load(model_path)\n\n        with torch.no_grad():\n            final_model.eval()\n\n            y_true = []\n            y_pred = []\n\n            for i, (X_cont, X_cat, y) in enumerate(val_loader1):\n                out = final_model(X_cont, X_cat)\n                y_pred += list(out.detach().cpu().numpy().flatten())\n                y_true += list(y.cpu().numpy())\n\n            rmse = np.sqrt(mean_squared_error(scaler_y.inverse_transform(np.expand_dims(y_true, axis=1)),\n                                              scaler_y.inverse_transform(np.expand_dims(y_pred, axis=1))))\n    \n\n            scores.append(rmse)\n            y_true = []\n            y_pred = []\n            for i, (X_cont, X_cat, y) in enumerate(val_loader):\n                out = final_model(X_cont, X_cat)\n\n                y_pred += list(out.detach().cpu().numpy().flatten())\n                y_true += list(y.cpu().numpy())\n\n            rmse_fold = np.sqrt(mean_squared_error(scaler_y.inverse_transform(np.expand_dims(y_true, axis=1)),\n                                              scaler_y.inverse_transform(np.expand_dims(y_pred, axis=1))))\n\n        \n            scores_fold.append(rmse_fold)\n\n        ## predict test set\n        y_pred = []\n        ids = []\n        with torch.no_grad():\n            final_model.eval()\n            for i, (X_cont, X_cat, y) in enumerate(test_loader):\n                out = final_model(X_cont, X_cat)\n                y_pred += list(out.detach().cpu().numpy().flatten())\n                ids += list(y.cpu().numpy())\n\n        y_pred = scaler_y.inverse_transform(np.expand_dims(y_pred, axis=1))\n        sub_nn.loc[ids, target] = sub_nn.loc[ids, target] + y_pred[:, 0]\/Nfolds\n\n    return sub_nn, scores, scores_fold","7a660d9c":"#### Hyperparameters ############\n#categorical features\ncat_feats = ['summary', 'icon', 'dayofyear', 'hour', 'year', 'month', 'day',\n             'dayofweek',  'station', 'type', 'source', 'week']\n\n#useless variables\nuseless_columns = ['ID', 'date', 'precipIntensity', 'precipProbability',\n                   'latitude', 'longitude']\n\n#target variable\ntarget = 'aqi'\n\n#call dataset\ndf, weather, sub = preprocess_rnn(ROOT='..\/input\/ulaanbaatar-city-air-pollution-prediction', cat_feat = cat_feats, rolling_means=[4, 8, 12],\n                                            shifts=[2, 4], diffs=[2, 4], aqi_mean=True)\n\n\nsub_nn = sub.set_index('ID', drop=True)\n\n#continues variables\nrnn_cols = list(df.drop(useless_columns+cat_feats+['aqi'], axis=1).columns)\ndt_start = ['2018-10-01', '2018-10-06', '2018-10-11', '2018-10-16', '2018-10-20']\n\n#train mistaked LSTM model\n# \u0411\u0438 \u044d\u043d\u0434 for loop \u0434\u044d\u044d\u0440 epoch \u0433\u044d\u0441\u044d\u043d \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0438\u0439\u0433 \u0445\u0430\u043c\u0442 \u0437\u0430\u0440\u043b\u0430\u0441\u0430\u043d \u0431\u0430\u0439\u0441\u0430\u043d. \n# \u0422\u0438\u0439\u043c \u0431\u043e\u043b\u043e\u0445\u043e\u043e\u0440 \u0445\u0430\u043c\u0433\u0438\u0439\u043d \u0441\u0430\u0439\u043d \u0437\u0430\u0433\u0432\u0430\u0440\u0430\u0430 \u043e\u043b\u0436 \u0430\u0432\u0447 \u0447\u0430\u0434\u0430\u0430\u0433\u04af\u0439 \u0431\u0430\u0439\u0445 \u0431\u043e\u043b\u043e\u043c\u0436\u0442\u043e\u0439.\n# \u041c\u04e9\u043d \u0442\u04af\u04af\u043d\u0447\u043b\u044d\u043d \u0445\u0430\u043c\u0430\u0430\u0440\u0430\u0433\u0447 \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0434\u0430\u0430\u0441 \u04af\u04af\u0441\u0433\u044d\u0441\u044d\u043d \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0434\u044b\u0433 \u0430\u0448\u0438\u0433\u043b\u0430\u0445\u0433\u04af\u0439 neural network \u0437\u0430\u0433\u0432\u0430\u0440\u044b\u0433 \u0441\u0443\u0440\u0433\u0430\u0445\n# \u043c\u0438\u043d\u0438\u0439 \u0445\u0443\u0432\u044c\u0434 \u0442\u0435\u0441\u0442 \u04e9\u0433\u04e9\u0433\u0434\u04e9\u043b \u0434\u044d\u044d\u0440 \u043d\u044d\u044d\u043b\u0442\u0442\u044d\u0439 \u04e9\u0433\u04e9\u0433\u0434\u0441\u04e9\u043d \u0445\u0430\u043c\u0430\u0430\u0440\u0430\u0433\u0447 \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0438\u0439\u043d \u0443\u0442\u0433\u044b\u0433 \n# \u0445\u0430\u043c\u0430\u0430\u0440\u0430\u0433\u0447 \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0430\u0430\u0441 \u04af\u04af\u0441\u0433\u044d\u0441\u044d\u043d \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0434\u044b\u043d \u0442\u043e\u043e\u0446\u043e\u043e\u043b\u043e\u043b\u0434 \u043e\u0440\u0443\u0443\u043b\u0441\u0430\u043d \u043d\u044c \u0445\u0443\u0443\u0440\u0430\u043c\u0447 CV \u0443\u0442\u0433\u0430\u043d\u0434 \u0438\u0442\u0433\u044d\u0445 \u043d\u04e9\u0445\u0446\u04e9\u043b \u0431\u043e\u043b\u0441\u043e\u043d \u0431\u0438\u043b\u044d\u044d. \n# \u0418\u0439\u043c\u0434 \u044d\u0434\u0433\u044d\u044d\u0440 \u0445\u0443\u0432\u044c\u0441\u0430\u0433\u0447\u0438\u0439\u0433 \u0430\u0448\u0438\u0433\u043b\u0430\u0445\u0433\u04af\u0439\u0433\u044d\u044d\u0440 \u0434\u0430\u0445\u0438\u043d \u0437\u0430\u0433\u0432\u0430\u0440 \u0441\u0443\u0440\u0433\u0430\u0436, \u0442\u0430\u0430\u043c\u0430\u0433\u043b\u0430\u043b\u044b\u0433 \u043d\u044d\u0433\u0442\u0433\u044d\u0445\u044d\u0434 \u0430\u0448\u0438\u0433\u043b\u0430\u0441\u0430\u043d.\nhidden_layer = 512\nsub_nn['aqi']=0\n\nprint(\"*\" * 25, \"hidden_layer\", hidden_layer, \"*\" * 25, )\nsub_nn, scores, scores_fold = training_lstm_mistaked(df=df.copy(), sub_nn=sub_nn, target=target, dt_start=dt_start,\n                                          cont_cols=rnn_cols, cat_cols=cat_feats, Nfolds=5, seq_len=6, num_layers=2,\n                                          epoch=50, patience=5, MODEL_ROOT='models\/lstm_v1',\n                                        hidden_dim=hidden_layer)\n\nprint('Neural Network CV:', np.mean(scores), '+\/-', np.std(scores))\nprint('Neural Network CV fold:', np.mean(scores_fold), '+\/-', np.std(scores_fold))\n\nsub_nn.to_csv(\"sub_lstm_%s.csv\" %hidden_layer)\n\n\n#call dataset\ndf, weather, sub = preprocess_rnn(ROOT='..\/input\/ulaanbaatar-city-air-pollution-prediction', cat_feat = cat_feats, rolling_means=[4, 8, 12],\n                                            shifts=[2, 4], diffs=[2, 4], aqi_mean=False)\n\n\nsub_nn = sub.set_index('ID', drop=True)\n\n#continues variables\nrnn_cols = list(df.drop(useless_columns+cat_feats+['aqi'], axis=1).columns)\n\nsub_nn['aqi']=0\n\nprint(\"*\" * 25, \"hidden_layer\", hidden_layer, \"*\" * 25, )\nsub_nn, scores, scores_fold = training_lstm(df=df.copy(), sub_nn=sub_nn, target=target, dt_start=dt_start,\n                                          cont_cols=rnn_cols, cat_cols=cat_feats, Nfolds=5, seq_len=6, num_layers=2,\n                                          epoch=50, patience=5, MODEL_ROOT='models\/lstm_v1',\n                                        hidden_dim=hidden_layer)\n\nprint('Neural Network CV:', np.mean(scores), '+\/-', np.std(scores))\nprint('Neural Network CV fold:', np.mean(scores_fold), '+\/-', np.std(scores_fold))\n\nsub_nn.to_csv(\"sub_lstm_v1_%s.csv\" %hidden_layer)","b3d5fcdd":"#\u0411\u0443\u0440\u0443\u0443 CV-\u0442\u044d\u0439 \u0437\u0430\u0433\u0432\u0430\u0440\u044b\u043d \u04af\u0440 \u0434\u04af\u043d\u0433\u04af\u04af\u0434\u0438\u0439\u0433 \u0434\u0443\u0443\u0434\u0430\u0445\nsubl=pd.read_csv('.\/sub_lgb_100.csv')\nsubcb = pd.read_csv('.\/sub_cb_100.csv')\nsubxg = pd.read_csv('.\/sub_xg_100.csv')\nsubnn = pd.read_csv('.\/sub_nn_256.csv')\nsub_lstm = pd.read_csv('.\/sub_lstm_512.csv')\nsub_gru = pd.read_csv('.\/sub_lstm_512.csv')\n\n#\u0442\u0435\u0441\u0442\u0438\u0439\u043d \u04e9\u0433\u04e9\u0433\u0434\u043b\u0438\u0439\u0433 \u0434\u0430\u0445\u0438\u043d \u0434\u0443\u0443\u0434\u0430\u0445\ntest = pd.read_csv('..\/input\/ulaanbaatar-city-air-pollution-prediction\/pm_test.csv')[['ID', 'date', 'type', 'station', 'aqi']]\ntest['date'] = pd.to_datetime(test['date'])\ntest['hour'] = test['date'].dt.hour\ntest['month'] = test['date'].dt.month","14912e41":"# ID-\u0430\u0430\u0440 \u0438\u043d\u0434\u0435\u043a\u0441\u0436\u04af\u04af\u043b\u0436, \u043b\u0438\u0441\u0442 \u04af\u04af\u0441\u0433\u044d\u0445\nsubl = subl.set_index('ID', drop=True)\nsubcb = subcb.set_index('ID', drop=True)\nsubxg = subxg.set_index('ID', drop=True)\nsubnn = subnn.set_index('ID', drop=True)\nsubnn = subnn.loc[test['ID']]\nsub_lstm = sub_lstm.set_index('ID', drop=True)\nsub_lstm = sub_lstm.loc[test['ID']]\nsub_gru = sub_gru.set_index('ID', drop=True)\nsub_gru = sub_gru.loc[test['ID']]\n\nnames = ['LightGBM', 'CatBoost', 'XGBoost', \n        'NeuralNet', 'LSTM', 'GRU']\n\nsubs = [subl, subcb, subxg, subnn, sub_lstm, sub_gru]","159c7081":"# \u041d\u0438\u0439\u0442 \u0434\u04af\u043d \u0431\u043e\u043b\u043e\u043d \u0442\u043e\u043e\u0441\u043e\u043d\u0446\u043e\u0440\u044b\u043d \u0442\u04e9\u0440\u04e9\u043b \u043e\u043d\u0443\u0443\u0434 \u0434\u044d\u044d\u0440 \u0430\u043b\u0434\u0430\u0430\u0433 \u0442\u043e\u043e\u0446\u043e\u0436, \u0431\u043b\u0435\u043d\u0434\u0438\u043d\u0433 \u0445\u0438\u0439\u0445 \u0445\u0443\u0432\u0438\u0439\u0433 \u043e\u0439\u0440\u043e\u043b\u0446\u043e\u043e\u043b\u043e\u0445 \u0445\u04e9\u0448\u04af\u04af\u0440\u044d\u0433 \u0431\u043e\u043b\u0433\u043e\u0445\nsub_test = test.set_index('ID', drop=True).dropna()\n\nfor n, s in zip(names, subs):\n    print(n, \":\", np.sqrt(mean_squared_error(sub_test['aqi'].values, s.loc[sub_test.index, 'aqi'].values)))\n\n\nfor typ in test['type'].unique():\n    print('*'*50, 'type-%s 2019, 2020' %typ, '*'*50,)\n    sub_test = test.loc[(test['type']==typ) & (test['date']>'2019-06-20')].set_index('ID', drop=True).dropna()\n    for n, s in zip(names, subs):\n        print(n, \":\", np.sqrt(mean_squared_error(sub_test['aqi'].values, s.loc[sub_test.index, 'aqi'].values)))\n    print('*'*50, 'type-%s 2018' %typ, '*'*50,)\n    sub_test = test.loc[(test['type']==typ) & (test['date']<'2019-06-20')].set_index('ID', drop=True).dropna()\n    for n, s in zip(names, subs):\n        print(n, \":\", np.sqrt(mean_squared_error(sub_test['aqi'].values, s.loc[sub_test.index, 'aqi'].values)))","aee66f42":"# \u0422\u043e\u043e\u0441\u043e\u043d\u0446\u043e\u0440\u044b\u043d \u0442\u04e9\u0440\u04e9\u043b \u0431\u043e\u043b\u043e\u043d \u043e\u043d\u0443\u0443\u0434\u044b\u043d \u0445\u0443\u0432\u044c\u0434 \u0437\u0430\u0433\u0432\u0430\u0440\u0443\u0443\u0434\u044b\u043d \u04af\u0440 \u0434\u04af\u043d\u0433 \u043d\u044d\u0433\u0442\u0433\u044d\u0445 \u0436\u0438\u043d\u0433\u04af\u04af\u0434 \u043e\u043d\u043e\u043e\u0445\nweights_PM25_2019  = [0.40, 0.20, 0.10, 0.15, 0.10, 0.05]\nweights_PM25_2018  = [0.40, 0.20, 0.10, 0.15, 0.10, 0.05]\nweights_PM10_2019  = [0.40, 0.20, 0.10, 0.15, 0.10, 0.05]\nweights_PM10_2018  = [0.40, 0.20, 0.10, 0.15, 0.10, 0.05]\n\nsub = subl.copy()\nsub['aqi']=0\n\n# \u041e\u043d\u043e\u043e\u0441\u043e\u043d \u0436\u0438\u043d\u0433\u04af\u04af\u0434\u044d\u0434 \u0442\u0443\u043b\u0433\u0443\u0443\u0440\u043b\u0430\u043d \u0442\u0430\u0430\u043c\u0430\u0433\u043b\u0430\u043b\u044b\u043d \u0443\u0442\u0433\u044b\u0433 \u043d\u044d\u0433\u0442\u0433\u044d\u0445\nfor typ in test['type'].unique():\n    if type=='PM2.5':\n        weight0=weights_PM25_2019\n        weight1=weights_PM25_2018\n    else:\n        weight0 = weights_PM10_2019\n        weight1 = weights_PM10_2018\n\n    idx = test.loc[(test['type']==typ) & (test['date']>'2019-06-20'), 'ID']\n    for i, s in enumerate(subs):\n        sub.loc[idx, 'aqi'] = sub.loc[idx, 'aqi'] + weight0[i]* s.loc[idx, 'aqi'].values\n\n    idx = test.loc[(test['type'] == typ) & (test['date'] < '2019-06-20'), 'ID']\n    for i, s in enumerate(subs):\n        sub.loc[idx, 'aqi'] = sub.loc[idx, 'aqi'] + weight1[i]* s.loc[idx, 'aqi'].values\n\n# \u041d\u044d\u0433\u0442\u0433\u044d\u0441\u044d\u043d \u0443\u0442\u0433\u0430 \u0434\u044d\u044d\u0440 \u0442\u0435\u0441\u0442\u0438\u0439\u043d \u043d\u044d\u044d\u043b\u0442\u0442\u044d\u0439 \u0431\u0430\u0439\u0433\u0430\u0430 \u04e9\u0433\u04e9\u0433\u0434\u043b\u0438\u0439\u043d \u0445\u0443\u0432\u044c\u0434 \u0430\u043b\u0434\u0430\u0430\u0433 \u0442\u043e\u043e\u0446\u043e\u0445\nsub_test = test.set_index('ID', drop=True).dropna()\nprint(\"Ensemble:\", np.sqrt(mean_squared_error(sub_test['aqi'].values, sub.loc[sub_test.index, 'aqi'].values)))","19a56f37":"# \u041f\u043e\u0441\u0442 \u043f\u0440\u043e\u0446\u0435\u0441\u0441 \nsub['date'] = test['date'].values\nsub['type'] = test['type'].values\nsub['station']=test['station'].values\n\n# \u0421\u0430\u0439\u0436\u0440\u0443\u0443\u043b\u0441\u0430\u043d \u0442\u04af\u043b\u0448\u0442\u044d\u0439 \u0445\u043e\u043b\u0431\u043e\u043e\u0442\u043e\u0439 2019, 2020 \u043e\u043d\u0443\u0443\u0434\u0430\u0434 \u0430\u0433\u0430\u0430\u0440\u044b\u043d \u0431\u043e\u0445\u0438\u0440\u0434\u043e\u043b \u0431\u0443\u0443\u0440\u0441\u0430\u043d \u0442\u0443\u043b \u0442\u0430\u0430\u043c\u0430\u0433\u043b\u0430\u0433\u0434\u0441\u0430\u043d \u0443\u0442\u0433\u044b\u0433 \u0442\u043e\u0433\u0442\u043c\u043e\u043b \u0445\u0443\u0432\u0438\u0430\u0440 \u0431\u0443\u0443\u0440\u0443\u0443\u043b\u0430\u0445\n# LB \u0445\u044d\u0440 \u0431\u0443\u0443\u0440\u0447 \u0431\u0430\u0439\u0433\u0430\u0430\u0433\u0430\u0430\u0441 \u0448\u0430\u043b\u0442\u0433\u0430\u0430\u043b\u0430\u043d \u0442\u0443\u0445\u0430\u0439\u043d \u0445\u0443\u0432\u0438\u0443\u0434\u044b\u0433 \u04e9\u04e9\u0440\u0447\u0438\u043b\u0441\u04e9\u043d\nsub.loc[(sub['date']<'2019-06-01') & (sub['type']=='PM2.5'), 'aqi'] = 1 * sub.loc[(sub['date']<'2019-06-01') & (sub['type']=='PM2.5'), 'aqi'].values\nsub.loc[(sub['date']>'2019-06-01') & (sub['date']<'2020-09-01') & (sub['type']=='PM2.5'), 'aqi'] = 0.85 * sub.loc[(sub['date']>'2019-06-01') & (sub['date']<'2020-09-01') & (sub['type']=='PM2.5'), 'aqi'].values\nsub.loc[(sub['date']>'2020-09-01') & (sub['type']=='PM2.5'), 'aqi'] = 0.94 * sub.loc[(sub['date']>'2020-09-01') & (sub['type']=='PM2.5'), 'aqi'].values\n\nsub.loc[(sub['date']<'2019-06-01') & (sub['type']=='PM10'), 'aqi'] = 0.98 * sub.loc[(sub['date']<'2019-06-01') & (sub['type']=='PM10'), 'aqi'].values\nsub.loc[(sub['date']>'2019-06-01') & (sub['date']<'2020-09-01') & (sub['type']=='PM10'), 'aqi'] = 0.85 * sub.loc[(sub['date']>'2019-06-01') & (sub['date']<'2020-09-01') & (sub['type']=='PM10'), 'aqi'].values\nsub.loc[(sub['date']>'2020-09-01') & (sub['type']=='PM10'), 'aqi'] = 0.94 * sub.loc[(sub['date']>'2020-09-01') & (sub['type']=='PM10'), 'aqi'].values","baeaef81":"# \u0421\u0442\u0430\u043d\u0446\u0443\u0443\u0434\u044b\u043d \u0445\u0443\u0432\u044c\u0434 \u043f\u043e\u0441\u0442 \u043f\u0440\u043e\u0446\u0435\u0441\u0441 \u0445\u0438\u0439\u0445\n# \u0410\u041d\u0423-\u044b\u043d \u044d\u043b\u0447\u0438\u043d \u0431\u043e\u043b\u043e\u043d \u041c\u043e\u043d\u0433\u043e\u043b \u0433\u0430\u0437\u0430\u0440-\u044b\u043d \u0445\u0443\u0432\u044c\u0434 \u0442\u0430\u0430\u043c\u0430\u0433\u043b\u0430\u043b\u044b\u043d \u0430\u043b\u0434\u0430\u0430 \u043d\u044d\u044d\u043b\u0442\u0442\u044d\u0439 \u0442\u0435\u0441\u0442 \u04e9\u0433\u04e9\u0433\u0434\u04e9\u043b \u0434\u044d\u044d\u0440 \u04e9\u043d\u0434\u04e9\u0440 \u0431\u0430\u0439\u0441\u0430\u043d \u0442\u0443\u043b \u043d\u044d\u043c\u044d\u0433\u0434\u04af\u04af\u043b\u0436 \u04af\u0437\u0441\u044d\u043d.\nsub.loc[sub['station']=='\u0410\u041d\u0423-\u044b\u043d \u042d\u043b\u0447\u0438\u043d \u0441\u0430\u0439\u0434\u044b\u043d \u044f\u0430\u043c', 'aqi'] = 1.15*sub.loc[sub['station']=='\u0410\u041d\u0423-\u044b\u043d \u042d\u043b\u0447\u0438\u043d \u0441\u0430\u0439\u0434\u044b\u043d \u044f\u0430\u043c', 'aqi'].values\nsub.loc[sub['station']=='\u041c\u043e\u043d\u0433\u043e\u043b \u0433\u0430\u0437\u0430\u0440', 'aqi'] = 1.02*sub.loc[sub['station']=='\u041c\u043e\u043d\u0433\u043e\u043b \u0433\u0430\u0437\u0430\u0440', 'aqi'].values\n\n#\u0413\u044d\u0440 \u0445\u043e\u0440\u043e\u043e\u043b\u043e\u043b \u0438\u0445\u0442\u044d\u0439 \u0431\u04af\u0441 \u0442\u0443\u043b \u043d\u044d\u043c\u044d\u0433\u0434\u04af\u04af\u043b\u0436 \u04af\u0437\u0441\u044d\u043d. \u041d\u04e9\u0433\u04e9\u04e9 \u0442\u0430\u043b\u0430\u0430 \u0441\u0443\u0440\u0433\u0430\u043b\u0442\u044b\u043d \u04e9\u0433\u04e9\u0433\u0434\u043b\u0438\u0439\u043d \u0434\u0443\u043d\u0434\u0430\u0436 \u0443\u0442\u0433\u0430 \u044d\u043d\u044d \u0445\u043e\u0451\u0440 \u0434\u044d\u044d\u0440 \u0431\u0443\u0441\u0430\u0434 \u0441\u0442\u0430\u043d\u0446\u0430\u0430\u0441 \u0431\u0430\u0433\u0430 \u0437\u044d\u0440\u044d\u0433 \n# \u0431\u0430\u0439\u0440\u0448\u043b\u044b\u043d \u0445\u0443\u0432\u044c\u0434 \u043e\u0439\u0440\u043e\u043b\u0446\u043e\u043e \u0442\u0443\u043b \u0431\u0430\u0433\u0430 \u0437\u044d\u0440\u044d\u0433 \u043d\u044d\u043c\u044d\u0433\u0434\u04af\u04af\u043b\u0441\u044d\u043d. \u0422\u0430\u0430\u043c\u0430\u0433\u043b\u0430\u0433\u0434\u0441\u0430\u043d \u0434\u0443\u043d\u0434\u0430\u0436 \u0443\u0442\u0433\u0430 \u0422\u043e\u043b\u0433\u043e\u0439\u0442\u044b\u043d \u0411\u0430\u044f\u043d\u0445\u043e\u0448\u0443\u0443\u043d\u0430\u0430\u0441 \u0431\u0430\u0433\u0430 \u0431\u0430\u0439\u0441\u0430\u043d \u0442\u0443\u043b \n# \u0422\u043e\u043b\u0433\u043e\u0439\u0442\u044b\u0433 \u0431\u0430\u0433\u0430\u0430\u0440 \u043d\u044d\u043c\u044d\u0433\u0434\u04af\u04af\u043b\u0441\u044d\u043d \u0431\u0430\u0439\u0441\u0430\u043d\nsub.loc[(sub['station']=='\u0411\u0430\u044f\u043d\u0445\u043e\u0448\u0443\u0443'), 'aqi'] = 1.12*sub.loc[(sub['station']=='\u0411\u0430\u044f\u043d\u0445\u043e\u0448\u0443\u0443'), 'aqi'].values\nsub.loc[(sub['station']=='\u0422\u043e\u043b\u0433\u043e\u0439\u0442'), 'aqi'] = 1.05*sub.loc[(sub['station']=='\u0422\u043e\u043b\u0433\u043e\u0439\u0442'), 'aqi'].values\n\n\nsub.loc[sub_test.index, 'aqi'] = sub_test['aqi'].values\n\nsub['aqi'].to_csv('sub_ensemble_blending_v1.csv')","3de54935":"#\u0411\u0443\u0440\u0443\u0443 CV-\u0442\u044d\u0439 \u0437\u0430\u0433\u0432\u0430\u0440\u044b\u043d \u04af\u0440 \u0434\u04af\u043d\u0433\u04af\u04af\u0434\u0438\u0439\u0433 \u0434\u0443\u0443\u0434\u0430\u0445\nsubl=pd.read_csv('.\/sub_lgb_100.csv')\nsubcb = pd.read_csv('.\/sub_cb_100.csv')\nsubxg = pd.read_csv('.\/sub_xg_100.csv')\nsubnn = pd.read_csv('.\/sub_nn_256.csv')\nsub_lstm = pd.read_csv('.\/sub_lstm_512.csv')\nsub_gru = pd.read_csv('.\/sub_lstm_512.csv')\n\n#\u0442\u0435\u0441\u0442\u0438\u0439\u043d \u04e9\u0433\u04e9\u0433\u0434\u043b\u0438\u0439\u0433 \u0434\u0430\u0445\u0438\u043d \u0434\u0443\u0443\u0434\u0430\u0445\ntest = pd.read_csv('..\/input\/ulaanbaatar-city-air-pollution-prediction\/pm_test.csv')[['ID', 'date', 'type', 'station', 'aqi']]\ntest['date'] = pd.to_datetime(test['date'])\ntest['hour'] = test['date'].dt.hour\ntest['month'] = test['date'].dt.month","5458eac2":"# ID-\u0430\u0430\u0440 \u0438\u043d\u0434\u0435\u043a\u0441\u0436\u04af\u04af\u043b\u0436, \u043b\u0438\u0441\u0442 \u04af\u04af\u0441\u0433\u044d\u0445\nsubl = subl.set_index('ID', drop=True)\nsubcb = subcb.set_index('ID', drop=True)\nsubxg = subxg.set_index('ID', drop=True)\nsubnn = subnn.set_index('ID', drop=True)\nsubnn = subnn.loc[test['ID']]\nsub_lstm = sub_lstm.set_index('ID', drop=True)\nsub_lstm = sub_lstm.loc[test['ID']]\nsub_gru = sub_gru.set_index('ID', drop=True)\nsub_gru = sub_gru.loc[test['ID']]\n\nnames = ['LightGBM', 'CatBoost', 'XGBoost', \n        'NeuralNet', 'LSTM', 'GRU']\n\nsubs = [subl, subcb, subxg, subnn, sub_lstm, sub_gru]","b6235458":"weights_PM25_2019  = [0.35, 0.20, 0.10, 0.30, 0.025, 0.025]\nweights_PM25_2018  = [0.35, 0.20, 0.10, 0.30, 0.025, 0.025]\nweights_PM10_2019  = [0.35, 0.20, 0.10, 0.30, 0.025, 0.025]\nweights_PM10_2018  = [0.35, 0.20, 0.10, 0.30, 0.025, 0.025]\n\nsub = subl.copy()\nsub['aqi']=0\nfor typ in test['type'].unique():\n    if type=='PM2.5':\n        weight0=weights_PM25_2019\n        weight1=weights_PM25_2018\n    else:\n        weight0 = weights_PM10_2019\n        weight1 = weights_PM10_2018\n\n    idx = test.loc[(test['type']==typ) & (test['date']>'2019-06-20'), 'ID']\n    for i, s in enumerate(subs):\n        sub.loc[idx, 'aqi'] = sub.loc[idx, 'aqi'] + weight0[i]* s.loc[idx, 'aqi'].values\n\n    idx = test.loc[(test['type'] == typ) & (test['date'] < '2019-06-20'), 'ID']\n    for i, s in enumerate(subs):\n        sub.loc[idx, 'aqi'] = sub.loc[idx, 'aqi'] + weight1[i]* s.loc[idx, 'aqi'].values\n\nsub_test = test.set_index('ID', drop=True).dropna()\nprint(\"Ensemble:\", np.sqrt(mean_squared_error(sub_test['aqi'].values, sub.loc[sub_test.index, 'aqi'].values)))","49e1ccb4":"sub['date'] = test['date'].values\nsub['type'] = test['type'].values\nsub['station']=test['station'].values\n\nsub.loc[(sub['date']<'2019-06-01') & (sub['type']=='PM2.5'), 'aqi'] = 1 * sub.loc[(sub['date']<'2019-06-01') & (sub['type']=='PM2.5'), 'aqi'].values\nsub.loc[(sub['date']>'2019-06-01') & (sub['date']<'2020-09-01') & (sub['type']=='PM2.5'), 'aqi'] = 0.85 * sub.loc[(sub['date']>'2019-06-01') & (sub['date']<'2020-09-01') & (sub['type']=='PM2.5'), 'aqi'].values\nsub.loc[(sub['date']>'2020-09-01') & (sub['type']=='PM2.5'), 'aqi'] = 0.94 * sub.loc[(sub['date']>'2020-09-01') & (sub['type']=='PM2.5'), 'aqi'].values\n\nsub.loc[(sub['date']<'2019-06-01') & (sub['type']=='PM10'), 'aqi'] = 0.98 * sub.loc[(sub['date']<'2019-06-01') & (sub['type']=='PM10'), 'aqi'].values\nsub.loc[(sub['date']>'2019-06-01') & (sub['date']<'2020-09-01') & (sub['type']=='PM10'), 'aqi'] = 0.85 * sub.loc[(sub['date']>'2019-06-01') & (sub['date']<'2020-09-01') & (sub['type']=='PM10'), 'aqi'].values\nsub.loc[(sub['date']>'2020-09-01') & (sub['type']=='PM10'), 'aqi'] = 0.94 * sub.loc[(sub['date']>'2020-09-01') & (sub['type']=='PM10'), 'aqi'].values\n\nprint('Postprocess:', np.sqrt(mean_squared_error(sub_test['aqi'].values, sub.loc[sub_test.index, 'aqi'].values)))","001166da":"#\u0411\u043b\u0435\u043d\u0434\u0438\u043d\u0433-1-\u0438\u0439\u043d \u04af\u0440 \u0434\u04af\u043d\u0433 \u0434\u0443\u0443\u0434\u0430\u0445 \n\nbest_sub = pd.read_csv('.\/sub_ensemble_blending_v1.csv')\n\n#40:60-\u0430\u0430\u0440 \u0431\u043b\u0435\u043d\u0434 \u0445\u0438\u0439\u0445\nsub['aqi'] =0.4*best_sub['aqi'].values+0.6*sub['aqi'].values","a468e981":"sub.loc[sub['station']=='\u0410\u041d\u0423-\u044b\u043d \u042d\u043b\u0447\u0438\u043d \u0441\u0430\u0439\u0434\u044b\u043d \u044f\u0430\u043c', 'aqi'] = 1.15*sub.loc[sub['station']=='\u0410\u041d\u0423-\u044b\u043d \u042d\u043b\u0447\u0438\u043d \u0441\u0430\u0439\u0434\u044b\u043d \u044f\u0430\u043c', 'aqi'].values\nsub.loc[sub['station']=='\u041c\u043e\u043d\u0433\u043e\u043b \u0433\u0430\u0437\u0430\u0440', 'aqi'] = 1.02*sub.loc[sub['station']=='\u041c\u043e\u043d\u0433\u043e\u043b \u0433\u0430\u0437\u0430\u0440', 'aqi'].values\nsub.loc[(sub['station']=='\u0411\u0430\u044f\u043d\u0445\u043e\u0448\u0443\u0443'), 'aqi'] = 1.12*sub.loc[(sub['station']=='\u0411\u0430\u044f\u043d\u0445\u043e\u0448\u0443\u0443'), 'aqi'].values\nsub.loc[(sub['station']=='\u041c\u04ae\u041e\u041d\u0420\u0422'), 'aqi'] = 1.1*sub.loc[(sub['station']=='\u041c\u04ae\u041e\u041d\u0420\u0422'), 'aqi'].values\n\nsub.loc[sub_test.index, 'aqi'] = sub_test['aqi'].values\n\nsub['aqi'].to_csv('sub_ensemble_blending_v2.csv')","c7c6666e":"#\u0417\u04e9\u0432 CV-\u0442\u044d\u0439 \u0437\u0430\u0433\u0432\u0430\u0440\u044b\u043d \u04af\u0440 \u0434\u04af\u043d\u0433\u04af\u04af\u0434\u0438\u0439\u0433 \u0434\u0443\u0443\u0434\u0430\u0445\nsubl=pd.read_csv('.\/sub_lgb_V1_100.csv')\nsubcb = pd.read_csv('.\/sub_cb_V1_100.csv')\nsubxg = pd.read_csv('.\/sub_xg_V1_100.csv')\nsubnn = pd.read_csv('.\/sub_nn_V1_256.csv')\nsub_lstm = pd.read_csv('.\/sub_lstm_v1_512.csv')\nsub_gru = pd.read_csv('.\/sub_lstm_v1_512.csv')\n\n#\u0442\u0435\u0441\u0442\u0438\u0439\u043d \u04e9\u0433\u04e9\u0433\u0434\u043b\u0438\u0439\u0433 \u0434\u0430\u0445\u0438\u043d \u0434\u0443\u0443\u0434\u0430\u0445\ntest = pd.read_csv('..\/input\/ulaanbaatar-city-air-pollution-prediction\/pm_test.csv')[['ID', 'date', 'type', 'station', 'aqi']]\ntest['date'] = pd.to_datetime(test['date'])\ntest['hour'] = test['date'].dt.hour\ntest['month'] = test['date'].dt.month","8b2beba9":"subl = subl.set_index('ID', drop=True)\nsubcb = subcb.set_index('ID', drop=True)\nsubxg = subxg.set_index('ID', drop=True)\nsubnn = subnn.set_index('ID', drop=True)\nsubnn = subnn.loc[test['ID']]\nsub_lstm = sub_lstm.set_index('ID', drop=True)\nsub_lstm = sub_lstm.loc[test['ID']]\nsub_gru = sub_gru.set_index('ID', drop=True)\nsub_gru = sub_gru.loc[test['ID']]\n\nnames = ['LightGBM', 'CatBoost', 'XGBoost', \n        'NeuralNet', 'LSTM', 'GRU']\n\nsubs = [subl, subcb, subxg, subnn, sub_lstm, sub_gru]","db6a00c0":"weights_PM25_2019  = [0.30, 0.30, 0.20, 0.15, 0.05, 0.00]\nweights_PM25_2018  = [0.30, 0.30, 0.20, 0.15, 0.05, 0.00]\nweights_PM10_2019  = [0.30, 0.30, 0.20, 0.15, 0.05, 0.00]\nweights_PM10_2018  = [0.30, 0.30, 0.20, 0.15, 0.05, 0.00]\n\nsub = subl.copy()\nsub['aqi']=0\nfor typ in test['type'].unique():\n    if type=='PM2.5':\n        weight0=weights_PM25_2019\n        weight1=weights_PM25_2018\n    else:\n        weight0 = weights_PM10_2019\n        weight1 = weights_PM10_2018\n\n    idx = test.loc[(test['type']==typ) & (test['date']>'2019-06-20'), 'ID']\n    for i, s in enumerate(subs):\n        sub.loc[idx, 'aqi'] = sub.loc[idx, 'aqi'] + weight0[i]* s.loc[idx, 'aqi'].values\n\n    idx = test.loc[(test['type'] == typ) & (test['date'] < '2019-06-20'), 'ID']\n    for i, s in enumerate(subs):\n        sub.loc[idx, 'aqi'] = sub.loc[idx, 'aqi'] + weight1[i]* s.loc[idx, 'aqi'].values\n\n#sub_test = test.loc[test['date']>'2019-06-20'].set_index('ID', drop=True).dropna()\nsub_test = test.set_index('ID', drop=True).dropna()\nprint(\"Ensemble:\", np.sqrt(mean_squared_error(sub_test['aqi'].values, sub.loc[sub_test.index, 'aqi'].values)))\n","be16340f":"############ postprocessing ###########################\nsub['date'] = test['date'].values\nsub['type'] = test['type'].values\nsub['station']=test['station'].values\n\nsub.loc[(sub['date']<'2019-06-01') & (sub['type']=='PM2.5'), 'aqi'] = 0.98 * sub.loc[(sub['date']<'2019-06-01') & (sub['type']=='PM2.5'), 'aqi'].values\nsub.loc[(sub['date']>'2019-06-01') & (sub['date']<'2020-09-01') & (sub['type']=='PM2.5'), 'aqi'] =0.95* sub.loc[(sub['date']>'2019-06-01') & (sub['date']<'2020-09-01') & (sub['type']=='PM2.5'), 'aqi'].values\nsub.loc[(sub['date']>'2020-09-01') & (sub['type']=='PM2.5'), 'aqi'] = 0.96 * sub.loc[(sub['date']>'2020-09-01') & (sub['type']=='PM2.5'), 'aqi'].values\n\nsub.loc[(sub['date']<'2019-06-01') & (sub['type']=='PM10'), 'aqi'] = 0.95 * sub.loc[(sub['date']<'2019-06-01') & (sub['type']=='PM10'), 'aqi'].values\nsub.loc[(sub['date']>'2019-06-01') & (sub['date']<'2020-09-01') & (sub['type']=='PM10'), 'aqi'] = 0.85 * sub.loc[(sub['date']>'2019-06-01') & (sub['date']<'2020-09-01') & (sub['type']=='PM10'), 'aqi'].values\nsub.loc[(sub['date']>'2020-09-01') & (sub['type']=='PM10'), 'aqi'] = 0.85 * sub.loc[(sub['date']>'2020-09-01') & (sub['type']=='PM10'), 'aqi'].values\n\nprint('Postprocess:', np.sqrt(mean_squared_error(sub_test['aqi'].values, sub.loc[sub_test.index, 'aqi'].values)))","5ecc3e91":"best_sub = pd.read_csv('.\/sub_ensemble_blending_v2.csv')\n\nsub['aqi'] =0.70*best_sub['aqi'].values+0.30*sub['aqi'].values\n\nsub.loc[sub['station']=='\u0410\u041d\u0423-\u044b\u043d \u042d\u043b\u0447\u0438\u043d \u0441\u0430\u0439\u0434\u044b\u043d \u044f\u0430\u043c', 'aqi'] = 1.15*sub.loc[sub['station']=='\u0410\u041d\u0423-\u044b\u043d \u042d\u043b\u0447\u0438\u043d \u0441\u0430\u0439\u0434\u044b\u043d \u044f\u0430\u043c', 'aqi'].values\nsub.loc[(sub['station']=='\u0411\u0430\u044f\u043d\u0445\u043e\u0448\u0443\u0443') & (sub['type']=='PM2.5'), 'aqi'] = 1.1*sub.loc[(sub['station']=='\u0411\u0430\u044f\u043d\u0445\u043e\u0448\u0443\u0443') & (sub['type']=='PM2.5'), 'aqi'].values\nsub.loc[(sub['station']=='\u0411\u0430\u0440\u0443\u0443\u043d 4 \u0437\u0430\u043c')& (sub['type']=='PM2.5'), 'aqi'] = 0.95*sub.loc[(sub['station']=='\u0411\u0430\u0440\u0443\u0443\u043d 4 \u0437\u0430\u043c') & (sub['type']=='PM2.5'), 'aqi'].values\nsub.loc[(sub['station']=='100 \u0430\u0439\u043b')& (sub['type']=='PM10'), 'aqi'] = 1.15*sub.loc[(sub['station']=='100 \u0430\u0439\u043b')& (sub['type']=='PM10'), 'aqi'].values\nsub.loc[(sub['station']=='\u0411\u04e9\u0445\u0438\u0439\u043d \u04e9\u0440\u0433\u04e9\u04e9') & (sub['type']=='PM10'), 'aqi'] = 1.15*sub.loc[(sub['station']=='\u0411\u04e9\u0445\u0438\u0439\u043d \u04e9\u0440\u0433\u04e9\u04e9') & (sub['type']=='PM10'), 'aqi'].values\nsub.loc[(sub['station']=='\u041c\u043e\u043d\u0433\u043e\u043b \u0433\u0430\u0437\u0430\u0440') & (sub['type']=='PM10'), 'aqi'] = 1.2*sub.loc[(sub['station']=='\u041c\u043e\u043d\u0433\u043e\u043b \u0433\u0430\u0437\u0430\u0440') & (sub['type']=='PM10'), 'aqi'].values\nsub.loc[(sub['station']=='\u041c\u0438\u0448\u044d\u044d\u043b \u044d\u043a\u0441\u043f\u043e') & (sub['type']=='PM10'), 'aqi'] = 1.1*sub.loc[(sub['station']=='\u041c\u0438\u0448\u044d\u044d\u043b \u044d\u043a\u0441\u043f\u043e') & (sub['type']=='PM10'), 'aqi'].values\n\nsub.loc[sub_test.index, 'aqi'] = sub_test['aqi'].values\n\nsub['aqi'].to_csv('sub_ensemble_final.csv')","69608b52":"# \u0411\u043b\u0435\u043d\u0434\u0438\u043d\u0433-2","3413f10f":"# LSTM MODEL","902b1f7c":"\u0428\u0430\u0442\u0430\u043b\u0441\u0430\u043d \u0431\u043b\u0435\u043d\u0434\u0438\u043d\u0433 \u0431\u0443\u044e\u0443 \u04e9\u043c\u043d\u04e9\u0445 \u0431\u043b\u0435\u043d\u0434\u0438\u043d\u0433\u0438\u0439\u043d \u04af\u0440 \u0434\u04af\u043d\u0433 \u0434\u0430\u0445\u0438\u043d \u0430\u0448\u0438\u0433\u043b\u0430\u0445 \u0437\u0430\u043c\u0430\u0430\u0440 LB-\u0433 \u0431\u0430\u0433\u0430 \u0431\u0430\u0433\u0430\u0430\u0440 \u0431\u0443\u0443\u0440\u0443\u0443\u043b\u0430\u0445 \u0431\u043e\u043b\u043e\u043c\u0436\u0442\u043e\u0439 \u0431\u0430\u0439\u0434\u0430\u0433. \u041f\u043e\u0441\u0442 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u0438\u043d\u0433 \u043c\u0430\u0430\u043d\u044c \u0445\u044d\u0442\u044d\u0440\u0445\u0438\u0439 \u043e\u043b\u043e\u043d \u0437\u04af\u0439\u043b\u044d\u044d\u0441 \u0445\u0430\u043c\u0430\u0430\u0440\u0430\u043b\u0442\u0430\u0439 \u0431\u0430\u0439\u0433\u0430\u0430 \u0431\u043e\u043b\u043e\u0445\u043e\u043e\u0440 \u04e9\u043c\u043d\u04e9\u0445 \u04af\u0440 \u0434\u04af\u043d\u0433 \u0434\u0430\u0445\u0438\u043d \u0430\u0448\u0438\u0433\u043b\u0430\u043d \u0442\u043e\u0433\u0442\u043c\u043e\u043b\u0443\u0443\u0434\u044b\u0433 \u04e9\u04e9\u0440\u0447\u043b\u04e9\u0445 \u043d\u044c \u0441\u0430\u0439\u043d \u04af\u0440 \u0434\u04af\u043d\u0433\u044d\u044d\u0441\u044d\u044d \u0445\u044d\u0442 \u043e\u043b\u0434\u043e\u0445\u043e\u043e\u0441 \u0441\u044d\u0440\u0433\u0438\u0439\u043b\u0434\u044d\u0433. ","0177eb4a":"\u041a\u0430\u0430\u0433\u043b\u044b\u043d \u0442\u044d\u043c\u0446\u044d\u044d\u043d\u04af\u04af\u0434\u0438\u0439\u043d \u0445\u0443\u0432\u044c\u0434 \u0445\u0430\u043c\u0433\u0438\u0439\u043d \u0442\u04af\u0433\u044d\u044d\u043c\u044d\u043b \u0430\u0448\u0438\u0433\u043b\u0430\u0433\u0434\u0434\u0430\u0433 \u0437\u04af\u0439\u043b \u0431\u043e\u043b \u043f\u043e\u0441\u0442 \u043f\u0440\u043e\u0446\u0435\u0441\u0441. LB-\u043e\u043e\u0441 \u043e\u043b\u0436 \u0430\u0432\u0430\u0445 \u0433\u0430\u043d\u0446 \u043c\u044d\u0434\u044d\u044d\u043b\u044d\u043b \u0431\u043e\u043b \u0442\u0435\u0441\u0442 \u04e9\u0433\u04e9\u0433\u0434\u043b\u0438\u0439\u043d \u0442\u0430\u0440\u0445\u0430\u043b\u0442\u044b\u043d \u0442\u0430\u043b\u0430\u0430\u0440\u0445 \u043c\u044d\u0434\u044d\u044d\u043b\u044d\u043b \u044e\u043c. \u0422\u0430\u0430\u043c\u0430\u0433\u043b\u0430\u0433\u0434\u0441\u0430\u043d \u0443\u0442\u0433\u0443\u0443\u0434\u044b\u0433 1-\u044d\u044d\u0441 \u0438\u0445 \u044d\u0441\u0432\u044d\u043b \u0431\u0430\u0433\u0430 \u0442\u043e\u043e\u0433\u043e\u043e\u0440 \u0433\u044d\u0445\u0434\u044d\u044d 1-\u0442\u044d\u0439 \u043e\u0439\u0440\u0445\u043e\u043d \u0442\u043e\u043e\u0433\u043e\u043e\u0440 \u04af\u0440\u0436\u04af\u04af\u043b\u044d\u0445 \u0437\u0430\u043c\u0430\u0430\u0440 LB \u043e\u043d\u043e\u043e \u0431\u0443\u0443\u0440\u0447 \u0431\u0430\u0439\u0433\u0430\u0430 \u0441\u0430\u0439\u0436\u0438\u0440\u0447 \u0431\u0430\u0439\u0433\u0430\u0430 \u044d\u0441\u044d\u0445\u0438\u0439\u0433 \u0442\u043e\u043e\u0446\u043e\u043e\u0434, \u0443\u043b\u043c\u0430\u0430\u0440 \u0442\u04af\u04af\u043d\u0438\u0439\u0433 \u043e\u0439\u0440\u0442\u0443\u0443\u043b\u0430\u0445 \u0437\u0430\u043c\u0430\u0430\u0440 \u04af\u0440 \u0434\u04af\u043d\u0433\u044d\u044d \u0441\u0430\u0439\u0436\u0440\u0443\u0443\u043b\u0436 \u0431\u043e\u043b\u0434\u043e\u0433. \n\n\u04ae\u04af\u043d\u0438\u0439\u0433 \u043a\u0430\u0430\u0433\u043b-\u0434 dark magic \u0433\u044d\u0436 \u043d\u044d\u0440\u043b\u044d\u0434\u044d\u0433 \u0431\u04e9\u0433\u04e9\u04e9\u0434 \u0431\u0438\u0434 CV-\u0433 \u04af\u043b \u0442\u043e\u043e\u0436, LB-\u0434 \u0438\u0442\u0433\u044d\u0436 \u0431\u0430\u0439\u0433\u0430\u0430 \u043d\u044d\u0433 \u0445\u044d\u043b\u0431\u044d\u0440 \u044e\u043c. \u0417\u0430\u0440\u0438\u043c \u0442\u043e\u0445\u0438\u043e\u043b\u0434\u043e\u043b\u0434 \u043c\u044d\u0434\u044d\u044d\u0436 \u0430\u043c\u0436\u0438\u043b\u0442\u0442\u0430\u0439, \u0437\u0430\u0440\u0438\u043c \u0442\u043e\u0445\u0438\u043e\u043b\u0434\u043e\u043b\u0434 \u043e\u0432\u0435\u0440\u0444\u0438\u0442 \u0431\u043e\u043b\u0434\u043e\u0433. \u041c\u0438\u043d\u0438\u0439 \u0445\u0443\u0432\u044c\u0434 \u0442\u043e\u0434\u043e\u0440\u0445\u043e\u0439 \u04af\u043d\u0434\u044d\u0441\u043b\u044d\u043b\u044d\u044d\u0440 1-\u044d\u044d\u0441 \u0431\u0430\u0433\u0430 \u0442\u043e\u043e\u0433\u043e\u043e\u0440 \u04af\u0440\u0436\u0441\u044d\u043d \u0431\u04e9\u0433\u04e9\u04e9\u0434 \u04af\u04af\u043d\u0438\u0439\u0433 \u04e9\u04e9\u0440\u0438\u0439\u043d EDA notebook \u0434\u044d\u044d\u0440 \u0441\u0430\u0439\u0436\u0440\u0443\u0443\u043b\u0441\u0430\u043d \u0442\u04af\u043b\u0448\u0442\u044d\u0439 \u0445\u043e\u043b\u0431\u043e\u043e\u0442\u043e\u0439 distribution shift problem \u0431\u0430\u0439\u0433\u0430\u0430 \u0433\u044d\u0436 \u0434\u0443\u0440\u044c\u0434\u0441\u0430\u043d \u0431\u0438\u043b\u044d\u044d. ","b4a962ee":"# \u0411\u041b\u0415\u041d\u0414\u0418\u041d\u0413-3","836dcc0f":"# ENSEMBLE MODELS","2396e11b":"# \u0411\u043b\u0435\u043d\u0434\u0438\u043d\u0433-1","46045454":"# NEURAL NETWORK MODELS"}}