{"cell_type":{"24236947":"code","ae8a7594":"code","ed7197fc":"code","4c1c1cb5":"code","06c97f2c":"code","be52de5e":"code","c2010649":"code","904f872b":"code","cdaae4fa":"code","2c60bc31":"code","b78b0e6d":"code","9c63a1cf":"code","d463f4f1":"code","05158632":"markdown","e7d614e3":"markdown","3afd3418":"markdown","2f920878":"markdown","4e9e1cc8":"markdown","9b362998":"markdown","9b62fbd1":"markdown","3a1f47b1":"markdown","6a769f52":"markdown","6ff16805":"markdown","2e2f7958":"markdown","d7710ff6":"markdown","39a5b9f3":"markdown"},"source":{"24236947":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport os\nfrom tqdm import tqdm","ae8a7594":"root_path = \"\/kaggle\/input\"\ndataset_dir = \"graduate-admissions\"\nfile_name = \"Admission_Predict_Ver1.1.csv\"\n\ndataset = pd.read_csv(os.path.join(root_path, dataset_dir, file_name))\ndataset.head()","ed7197fc":"featureset = dataset.values[:, 1:8]\ntarget = dataset.values[:, 8]\n\nprint(\"Size of the featureset: \", featureset.shape)\nprint(\"Size of the target: \", target.shape)","4c1c1cb5":"print(\"Number of training examples: \", featureset.shape[0])\nprint(\"Number of feature: \", featureset.shape[1])","06c97f2c":"def feature_scaling(X):\n    \"\"\" Normalize the features. \"\"\"\n    mean = np.mean(X, axis=0)\n    std = np.std(X, axis=0)\n    return np.divide(X - mean, std), mean, std","be52de5e":"norm_featureset, mean, std = feature_scaling(featureset)","c2010649":"def compute_cost_grad(m, X, y, theta):\n    \"\"\" Calculate the cost and gradient \"\"\"\n    \n    hx = tf.matmul(X, theta) # Hypothesis\n    \n    cost = 1\/(2*m) * tf.reduce_sum( tf.square(hx - y), axis=0 ) # Cost\n    \n    grad = tf.matmul(tf.transpose(X), hx-y) # Gradient\n    \n    return cost[0], grad\n\n\ndef predictions(X, theta):\n    \"\"\" Predict with hypothesis \"\"\"\n    return tf.matmul(X, theta)\n\n\ndef gradient_descent(m, X, y, theta, lr, itr):\n    \"\"\" Apply gradient descent. \"\"\"\n    \n    costs = [] # To store cost per iteration\n    \n    for i in tqdm(range(itr)):\n        cost, grad = compute_cost_grad(m, X, y, theta) # calculate cost and gradient\n        \n        theta.assign_sub( lr\/m * grad ) # Update the values of theta\n        \n        costs.append(cost.numpy())\n        \n    # Predict with learned parameters\n    predicted = predictions(X, theta)\n    \n    return {\n        'costs':costs,\n        'theta': theta.numpy(),\n        'predicted': predicted\n    }","904f872b":"iterations=500 # Number of iterations\nlearning_rate = 0.03\nX = norm_featureset\ny = target.reshape(-1,1)\n\nm = y.shape[0] # Number of training examples\nn=X.shape[1] # Number of features\n\n# Add intercept term\nX = np.hstack((np.ones((m, 1)), X))\nX = X.astype(np.float32) # type conversion\n\n# Initialize parameters\ntheta = tf.Variable(tf.zeros(dtype=tf.float32, shape=(n+1,1)))","cdaae4fa":"result = gradient_descent(m, X, y, theta, learning_rate, iterations)","2c60bc31":"plt.plot(result['costs'])\nplt.xlabel(\"# iteration\")\nplt.ylabel(\"cost\")\nplt.title('Cost convergence graph')\nplt.show()","b78b0e6d":"learning_rates = [0.001, 0.005, 0.01, 0.05, 0.1]\nresults = []\nfor lr in learning_rates:\n    # Initialize parameters\n    theta = tf.Variable(tf.zeros(dtype=tf.float32, shape=(n+1,1)))\n    result = gradient_descent(m, X, y, theta, lr, iterations)\n    results.append(result)\n    plt.plot(result['costs'])\n\nplt.legend([0.001, 0.005, 0.01, 0.05, 0.1])\nplt.xlabel(\"# iteration\")\nplt.ylabel(\"cost\")\nplt.title('Cost convergence graph')\nplt.show()","9c63a1cf":"for i, lr in enumerate(learning_rates):\n    mse = np.square(target-results[i]['predicted']).mean()\n    print(f\"Mean squeared error for learning rate {lr}: {mse*100}\")","d463f4f1":"index = np.random.randint(0,47)\nexample = X[index]\nreal = y[index]\n_theta = results[2]['theta']\n\nprint('Predicted Chance of Admit: ', np.dot(example, _theta))\nprint('Real Chance of Admit: ', real)","05158632":"#### Mean Squared Error\nCalculate mean squared error on training data","e7d614e3":"* **Hypothesis** for linear regression is\n$$h_\\theta (x) = \\theta^T x = \\theta_0 + \\theta_1 x_1$$\n* **Cost** function is\n$$J(\\theta) = \\frac{1}{2m} \\sum\\limits_{i=1}^{m} (h_\\theta (x^i)-y^i)^2$$\n\n\nObjective of the Linear Regression is minimize the cost function $J(\\theta)$<br>\nTo minimize the cost function we have to adjust the value of the $\\theta_j$. Using batch **Gradient Descent** we can do this.\n\nFor each iteration the batch **gradient descent** performs the update which is:\n$$\\theta_j := \\theta_j-\\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m} (h_\\theta (x^i)-y^i)x^i_j$$","3afd3418":"#### Import required packages","2f920878":"#### Training and visualizing cost convergence graph with different learning rate","4e9e1cc8":"*Prediction*\n\nPredict an training example for learning rate `0.01`","9b362998":"#### Feature scaling\/ normalization","9b62fbd1":"Visualize cost convergence graph for learning rate `0.03`","3a1f47b1":"#### Load Dataset","6a769f52":"**Purpose**: practice **TensorFlow 2** and **Linear Regression**.\n\nI am going to implement *Linear Regression* from scratch with *TensorFlow 2*.","6ff16805":"#### Training","2e2f7958":"Split dataset into featureset(X) and target(y)","d7710ff6":"Required information about data","39a5b9f3":"Setup for Linear Regression"}}