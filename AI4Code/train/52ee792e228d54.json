{"cell_type":{"aca0f797":"code","0ea646df":"code","73fa0165":"code","0a4d6e76":"code","783a4ac1":"code","988a914f":"code","e3929750":"code","035b95a4":"code","3f3fb637":"code","25ce8c47":"code","56ddb682":"code","20fe469c":"code","78aa3bd6":"code","584f3f35":"code","611dce0b":"code","3422dcc6":"code","da421c66":"code","b4ab2f1d":"code","57c472e6":"code","a60040a3":"code","5d4d8716":"code","4ad852e5":"code","1bd21ea3":"code","cd55e121":"code","fac075eb":"code","1c375f35":"code","fdaef17c":"code","4a445d3d":"code","04fc2fa1":"code","eb646988":"code","37c83aa1":"code","98ca606c":"code","966d0d0b":"code","ff4f32cc":"code","8047ab1e":"code","69ef0764":"code","aea31229":"code","bb19f05e":"code","827b1443":"code","69e2023b":"code","a3bf3122":"code","7eaf5c39":"code","c5d4b519":"code","50e8f95d":"code","90ebb1da":"code","e9c9548c":"markdown","3a2bf0a2":"markdown","cdd54b92":"markdown","dd43f41c":"markdown","259014d1":"markdown","1fbcd5df":"markdown","b83e3602":"markdown","c7c4108c":"markdown","428c1dea":"markdown","bfcdbf03":"markdown","cf94d12f":"markdown","76e167f8":"markdown","dc12c7e3":"markdown","c253b114":"markdown","322669d3":"markdown","0c9fec12":"markdown","0bcb39c8":"markdown","2b8bedc1":"markdown","f6de3c48":"markdown","8a04f39b":"markdown","b2357f61":"markdown","99e7c85a":"markdown","74cd964e":"markdown","f6c28b2d":"markdown","602dd194":"markdown","c3a3fce5":"markdown","6493c1f5":"markdown","4b0e610d":"markdown","e0abc5c3":"markdown","7d48b886":"markdown","a21a9b35":"markdown"},"source":{"aca0f797":"# import the necessary packages\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(color_codes=True)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom scipy.stats import zscore\nfrom sklearn import svm\nfrom sklearn.utils import resample\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","0ea646df":"# Read the dataset from csv file and print the first 10 rows\ndata = pd.read_excel('..\/input\/bank-loan-modelling\/Bank_Personal_Loan_Modelling.xlsx','Data')\ndata.head(10)","73fa0165":"print('Number of features in the data : ', data.shape[1])\nprint('Number of samples in the data  : ', data.shape[0])","0a4d6e76":"# List the information about each column such as non null count, datatype etc.\ndata.info()","783a4ac1":"# gives an idea about distribution of each attribute\ndata.describe().T","988a914f":"data[data['Experience'] < 0].shape","e3929750":"sns.pairplot(data.iloc[:,1:])","035b95a4":"# Lets get the categorical features. We consider the features which are having less than 25 values as categorical.\ncat_features = [feature for feature in data.columns if data[feature].nunique()<25]\nprint('Categorical Features : ', cat_features)\ndata[cat_features].head()","3f3fb637":"# Continues features\ncont_features = [feature for feature in data.columns if feature not in cat_features + ['ID']]\ncont_features","25ce8c47":"for feature in cat_features:\n    sns.countplot(data[feature])\n    plt.show()","56ddb682":"for feature in cont_features:\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,5))\n    sns.distplot(data[feature], kde=False, ax=ax1)\n    sns.boxplot(data[feature], ax=ax2)\n    plt.show()\n","20fe469c":"sns.scatterplot(x='Income',y='CCAvg',hue='Personal Loan', data=data)","78aa3bd6":"plt.figure(figsize=(15,8))\nsns.pointplot(x='Income',y='CCAvg', data=data, estimator=np.median, ci=None)","584f3f35":"sns.boxplot(x='Education',y='Income',hue='Personal Loan',data=data)","611dce0b":"plt.figure(figsize=(15,8))\nsns.boxplot(x='Experience',y='Income',hue='Personal Loan',data=data)","3422dcc6":"print(data['ZIP Code'].min())\nprint(data['ZIP Code'].max())\ndata['ZIP Code'].unique()","da421c66":"data['ZIP Code'] = data['ZIP Code'].apply(lambda zip: zip*10 if zip\/10000 < 1 else zip)","b4ab2f1d":"sns.boxplot(data['ZIP Code'])","57c472e6":"zipBins = np.arange(90000, 99999, 500)\nzipCat = pd.cut(data['ZIP Code'], bins = zipBins)\ndata['ZIPCat'] = zipCat\ndata.head()","a60040a3":"woe_df = data.groupby('ZIPCat')['Personal Loan'].mean()\nwoe_df = pd.DataFrame(woe_df)\nwoe_df = woe_df.rename(columns = {'Personal Loan': 'Good'})\nwoe_df['Bad'] = 1 - woe_df['Good']\nwoe_df['Good'] = np.where(woe_df['Good'] == 0, 0.000001, woe_df['Good'])\nwoe_df['Bad'] = np.where(woe_df['Bad'] == 0, 0.000001, woe_df['Bad'])\nwoe_df['WoE'] = np.log(woe_df['Good']\/woe_df['Bad'])\ndata.loc[:, 'ZIP_WoE'] = data['ZIPCat'].map(woe_df['WoE'])\ndata","5d4d8716":"print('Number of customers with No House mortgage: ',data[data['Mortgage'] == 0].shape[0], ' out of ', data.shape[0])\nprint('Number of customers with No spending on credit card: ',data[data['CCAvg'] == 0].shape[0], ' out of ', data.shape[0])","4ad852e5":"# If CCAvg > 0, set the CreditCard as 1, else 0\ndata['CreditCard'] = np.where(data['CCAvg'] > 0 , 1, 0)","1bd21ea3":"print('Number of rows with Negative Experience : ', sum(data['Experience'] < 0))\nprint('Ages of rows rows with Negative Experince : ', data[data['Experience'] < 0].Age.unique())","cd55e121":"sns.lmplot('Age', 'Experience', data=data[data['Experience'] > 0])","fac075eb":"lr_model = LinearRegression()\nlr_model.fit(data[['Age']], data['Experience'])\ndata.loc[data['Experience'] < 0, 'Experience'] = lr_model.predict( data[data['Experience'] < 0].loc[:,'Age':'Age'] )","1c375f35":"print('Number of rows with Negative Experience : ', sum(data['Experience'] < 0))\nprint('Ages of rows rows with Negative Experince : ', data[data['Experience'] < 0].Age.unique())","fdaef17c":"data.loc[data['Experience'] < 0, 'Experience'] = 0\ndata[data['Experience'] < 0]","4a445d3d":"sns.scatterplot(data['Age'], data['Experience'])","04fc2fa1":"plt.figure(figsize=(16,8))\nsns.heatmap(data.corr(), annot=True)","eb646988":"# Lets remove the unwanted variables split the data to X and y\nX = data.drop(['ID', 'ZIP Code', 'ZIPCat','Personal Loan'], axis = 1)\ny = data['Personal Loan']\nX","37c83aa1":"vif=pd.DataFrame()\nvif['Features']=X.columns\nvif['VIF']=[variance_inflation_factor(X.values,i) for i in range(X.shape[1])]\nvif['VIF']=round(vif['VIF'],2)\nvif=vif.sort_values(by='VIF', ascending=False)\nvif","98ca606c":"X.drop('Age', axis = 1, inplace=True)\nvif=pd.DataFrame()\nvif['Features']=X.columns\nvif['VIF']=[variance_inflation_factor(X.values,i) for i in range(X.shape[1])]\nvif['VIF']=round(vif['VIF'],2)\nvif=vif.sort_values(by='VIF', ascending=False)\nvif","966d0d0b":"X.drop('CreditCard', axis = 1, inplace=True)\nvif=pd.DataFrame()\nvif['Features']=X.columns\nvif['VIF']=[variance_inflation_factor(X.values,i) for i in range(X.shape[1])]\nvif['VIF']=round(vif['VIF'],2)\nvif=vif.sort_values(by='VIF', ascending=False)\nvif","ff4f32cc":"X.drop('ZIP_WoE', axis = 1, inplace=True)\nvif=pd.DataFrame()\nvif['Features']=X.columns\nvif['VIF']=[variance_inflation_factor(X.values,i) for i in range(X.shape[1])]\nvif['VIF']=round(vif['VIF'],2)\nvif=vif.sort_values(by='VIF', ascending=False)\nvif","8047ab1e":"# Split the data to test and train set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1234)\n\nprint('Number of 0s in Train dataset : ', y_train.value_counts()[0])\nprint('Number of 1s in Train dataset : ', y_train.value_counts()[1])\nprint('----------------')\nprint('Number of 0s in Test dataset : ', y_test.value_counts()[0])\nprint('Number of 1s in Test dataset : ', y_test.value_counts()[1])\n\nX_train = X_train.apply(zscore)\nX_test = X_test.apply(zscore)","69ef0764":"# Function to print the different metrics such as confusion matrix, roc, accuracy, precision, recall etc\n\ndef printModelMetrics(model, X_train, X_test, y_train, y_test):\n    y_train_pred = model.predict(X_train)\n    y_test_pred = model.predict(X_test)\n    print('Train set accuracy = ', metrics.accuracy_score(y_train, y_train_pred))\n    print('Test set accuracy = ', metrics.accuracy_score(y_test, y_test_pred))\n    print(metrics.classification_report(y_test, y_test_pred))\n\n    cm = metrics.confusion_matrix(y_test, y_test_pred)\n    cm = pd.DataFrame(cm, columns=['Predicted No Loan', 'Predicted Loan'], index=['Truth No Loan', 'Truth Loan'])\n    sns.heatmap(cm, annot=True, fmt='g', cbar=False)\n    plt.show()\n\n    y_test_proba = model.predict_proba(X_test)\n    y_test_proba = y_test_proba[:,1]\n    # generate a no skill prediction (majority class)\n    ns_probs = [0 for _ in range(len(y_test))]\n    # calculate scores\n    ns_auc = metrics.roc_auc_score(y_test, ns_probs)\n    lr_auc = metrics.roc_auc_score(y_test, y_test_proba)\n    # summarize scores\n    print('ROC AUC=%.3f' % (lr_auc))\n    # calculate roc curves\n    ns_fpr, ns_tpr, _ = metrics.roc_curve(y_test, ns_probs)\n    lr_fpr, lr_tpr, _ = metrics.roc_curve(y_test, y_test_proba)\n    # plot the roc curve for the model\n    plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n    plt.plot(lr_fpr, lr_tpr, marker='.', label='Model Skill')\n    \n    # axis labels\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    # show the legend\n    plt.legend()\n    # show the plot\n    plt.show()","aea31229":"lr_model = LogisticRegression(solver='liblinear')\nlr_model.fit(X_train, y_train)\nprintModelMetrics(lr_model, X_train, X_test, y_train, y_test)","bb19f05e":"# Naive Bayes \nnb_model = BernoulliNB()\n\nnb_model.fit(X_train, y_train)\nprintModelMetrics(lr_model, X_train, X_test, y_train, y_test)","827b1443":"# KNN Classifier\nX_trainScaled = X_train.apply(zscore)\nX_testScaled = X_test.apply(zscore)\n\nmodel = KNeighborsClassifier(n_neighbors=20, weights='distance')\nmodel.fit(X_trainScaled, y_train)\nprintModelMetrics(model, X_trainScaled, X_testScaled, y_train, y_test)","69e2023b":"\nmodel = svm.SVC(gamma = 0.025, C=3, probability=True)\nmodel.fit(X_train, y_train)\nprintModelMetrics(model, X_train, X_test, y_train, y_test)","a3bf3122":"#Lets oversample the positive classes in the data\n\n# Separate majority and minority classes\ndf_majority = data[data['Personal Loan']==0]\ndf_minority = data[data['Personal Loan']==1]\n\n# Upsample minority class\ndf_minority_upsampled = resample(df_minority, \n                                 replace=True,     # sample with replacement\n                                 n_samples=df_majority.shape[0],    # to match majority class\n                                 random_state=123) # reproducible results\n\n#Combine majority class with upsampled minority class\ndf_upsampled = pd.concat([df_majority, df_minority_upsampled])\n \n# Display new class counts\ndf_upsampled['Personal Loan'].value_counts()\n\nX = df_upsampled.drop(['ID', 'ZIP Code', 'ZIPCat', 'Personal Loan', 'Experience'], axis = 1)\ny = df_upsampled['Personal Loan']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1)\n\nprint('Number of 0s in Train dataset : ', y_train.value_counts()[0])\nprint('Number of 1s in Train dataset : ', y_train.value_counts()[1])\nprint('----------------')\nprint('Number of 0s in Test dataset : ', y_test.value_counts()[0])\nprint('Number of 1s in Test dataset : ', y_test.value_counts()[1])","7eaf5c39":"lr_model = LogisticRegression(solver='liblinear')\nlr_model.fit(X_train, y_train)\nprintModelMetrics(lr_model, X_train, X_test, y_train, y_test)","c5d4b519":"nb_model = BernoulliNB()\n\nnb_model.fit(X_train, y_train)\nprintModelMetrics(lr_model, X_train, X_test, y_train, y_test)","50e8f95d":"X_trainScaled = X_train.apply(zscore)\nX_testScaled = X_test.apply(zscore)\n\nmodel = KNeighborsClassifier(n_neighbors=20, weights='distance')\nmodel.fit(X_trainScaled, y_train)\nprintModelMetrics(model, X_trainScaled, X_testScaled, y_train, y_test)","90ebb1da":"model = svm.SVC(gamma = 0.2, C=5, probability=True)\nmodel.fit(X_train, y_train)\nprintModelMetrics(model, X_train, X_test, y_train, y_test)","e9c9548c":"### Observations from the above heatmap:\n#### &emsp;&emsp;-> Age and Experience are highly correlated(0.99)\n#### &emsp;&emsp;-> CCAVg and Income are correlated (we saw this earlier too).\n#### &emsp;&emsp;-> Personal loan is correlated to Income.","3a2bf0a2":"### The observations from above are:\n#### &emsp;&emsp;-> People having more income tend to spend more on their credit card.\n#### &emsp;&emsp;-> From the 1st figure it is clear that people with higher income opted for the Personal Loan.\n#### &emsp;&emsp;-> Also, people don't tend to spend more than their income on the credit card. (Scatterplot triangular)\n\n### Let us now see how the income varies with the Education and Experience","cdd54b92":"### We have test accuracy of 94% , Not bad!!\n### Or is it? If we take a closer look, we can see that the recall of customers who opted the loan(our target) is very less (54%). In simpler terms, we predicted 46% of the people as Non Personal Loan takers, when they had actually taken the loan. This is something which affects the marketing campaign and should be reduced.\n### Let's look at other models now.","dd43f41c":"#### Let us categorize the ZIP code into bins and use weight of evidence numerical transformation on them.\n(https:\/\/www.kdnuggets.com\/2016\/08\/include-high-cardinality-attributes-predictive-model.html)","259014d1":"### Now our recall is 91% with accuracy of 89%. Even if we had to lower the accuracy, we were able to target more customers who would opt for the Personal Loan.","1fbcd5df":"### The observations from above are:\n\n#### Income and CCAvg are positively skewed distribution with some outliers towards the higher end. Chances for it being incorrectly entered or measured is less, as there are very few people with outstanding salaries. \n#### Mortgage is also having some outliers towards the higher end. It can be seen from describe function that 75% of customers had Mortgage value less than 101 indicating the high peak on the distribution plot, even though maximum was 635. Similar to above, chances for it being incorrectly entered or measured is less.\n\n#### Majority of the people are aged between 35 to 55 with experience between 10 to 30. (Relationship between the two is evident)<br>\n\n### Let us check how the CCAvg varies with Income","b83e3602":"### K Nearest Neighbours","c7c4108c":"### It seems like the Age and Experience form a great couple \ud83d\udc91. They have a very strong Linear Relationship between them. (can we use this relationship to fill the incorrect negative values of Experience?)\n#### Other interesting pairs to note are (Income, CCAvg) and (Income, Mortgage) . It is clear from the scatter plots that CCAvg and Mortgage values are lower than the income.","428c1dea":"### Hmm.. Somethings not right here. Earlier, we saw that number of customers who did not use the credit card was higher (>3500). But the CCAvg says only 106 customers did not have spending on credit card. \n#### As the saying goes, \"The numbers speak for themselves\", let's assume CCAvg is right and reset the values of CreditCard.","bfcdbf03":"#### 1 zipcode is having only 4 digit which probably is a typo error. We will change it to 5 digits by appending a 0 to maintain the geographic area of atleast first 4 digits. ","cf94d12f":"### Lets take care of the negative values in Experience now. Since we have a strong linear relationship between Experience and Age, let us build a linear model between them and then use it to predict the values of negative experience using Age.","76e167f8":"### We'll use the VIF to remove the attributes which are having multicollinearity. Let's remove the attributes for which VIF > 10","dc12c7e3":"### These negative values are predicted by the model. We will not be able to impute the Experience for people with age below 25 using the model as it will predict negative experience (As they say, \"There's no perfect relationship\").\n### Let us instead replace leftover negative Experience values by 0","c253b114":"### Let us use the model to predict Experience and then replace negative Experience values by 0","322669d3":"### Let us visualize the distribution of each of the categorical features","0c9fec12":"### Naive Bayes","0bcb39c8":"### Logistic Regression","2b8bedc1":"### SVM","f6de3c48":"### That's Interesting!! People with lower education are earning more income. And Income does'nt vary based on one's Experience. Also, Personal loan neither strongly depend on Education nor Experience.  \n### Personal loan is dependent on the income though. It is evident from the higher band boxplots for Personal Loan, people with higher income tend to opt for Personal Loan (supports our earlier stament).","8a04f39b":"### We can see that the number of customers who took the loan last year(our target variable) were very less (<500) compared to the set of customers who haven't taken the loan(>4500). As such, the dataset is highly imbalanced. (Will this affect our algorithms? \ud83d\ude46\u200d\u2642\ufe0f)\n#### Very few customers have Securities account or CD account. (< 500)\n#### Majority of the customers do not own Credit Cards as well. (>3500)<br>\n\n#### Let us check the distribution of the continuous features now.","b2357f61":"### With recall of 100% and accuracy of 99.5%, SVM is definitely the best model to choose for the campaign.","99e7c85a":"### K Nearest Neighbours","74cd964e":"### SVM","f6c28b2d":"### We have 52 rows having negative expeience (which is 1% of the total sample). We'll deal with them later.","602dd194":"### The highest recall we could get was 68% with an accuracy of 96% from SVM Model.\n\n### Let us try to improve our model.\n### Initially, we observed that the data was highly imbalanced with majority people as Non loan customers. Let's balance this data by oversampling the loan classes and then building the models again.","c3a3fce5":"### Context\nThis case is about a bank (Thera Bank) whose management wants to explore ways of converting its liability customers to personal loan customers (while retaining them as depositors). A campaign that the bank ran last year for liability customers showed a healthy conversion rate of over 9% success. This has encouraged the retail marketing department to devise campaigns with better target marketing to increase the success ratio with minimal budget.<br>\n### Attribute Information:\n ID  : Customer ID<br>\n Age : Customer's age in completed years<br>\n Experience : #years of professional experience<br>\n Income : Annual income of the customer (\\\\$000)<br>\n ZIP Code : Home Address ZIP code.<br>\n Family : Family size of the customer<br>\n CCAvg : Avg. spending on credit cards per month (\\\\$000)<br>\n Education : Education Level.<br>\n    &emsp;1. Undergrad<br>\n    &emsp;2. Graduate<br>\n    &emsp;3. Advanced\/Professional<br>\n Mortgage : Value of house mortgage if any. (\\\\$000)<br>\n Personal Loan : Did this customer accept the personal loan offered in\nthe last campaign?<br>\n Securities Account : Does the customer have a securities account with\nthe bank?<br>\n CD Account : Does the customer have a certificate of deposit (CD)\naccount with the bank?<br>\n Online : Does the customer use internet banking facilities?<br>\n Credit card : Does the customer use a credit card issued by\nUniversalBank?<br>","6493c1f5":"### Hmm.. Distribution looks okay except the Experience attribute, for which minimum it is -3. Can anybody be negatively experienced \ud83e\udd14","4b0e610d":"### Naive Bayes","e0abc5c3":"### Let's try building different classification models to fit our data and test the performance.\n### Logistic Regression","7d48b886":"### From the ZIP Code box plot, there is an outlier. Seems like one of the customers is in Asgard. Let's try convincing Heimdall, the gatekeeper of the Bifr\u00f6st bridge to bring him back to Earth.","a21a9b35":"### We can see from above that there are no null values in any of the features \ud83d\ude31. Another peculiarity of the dataset is all the columns are numerical(data is already encoded)."}}