{"cell_type":{"f93b45bb":"code","4e31113d":"code","a6f14092":"code","4f579637":"code","aad1219d":"code","d48ed1e9":"code","80ecf7db":"code","c471b0b2":"code","0b4c7691":"code","ca7112ac":"code","1798256b":"code","9a7c08f4":"code","994f4969":"code","5278c262":"code","2cae1b92":"code","e1e8e9a1":"code","f6550257":"code","468185ab":"code","ad692529":"code","98a1d905":"code","149beaeb":"code","9e7a3f56":"code","002be352":"code","563d5479":"code","68a7bb65":"code","9968d9f4":"code","761a3856":"code","2712cfc8":"code","c4924a30":"code","1db74409":"code","9a7d607a":"code","b49f36f7":"markdown","a0e9b8ad":"markdown","a3af3349":"markdown","37cc09e2":"markdown","4e9999f7":"markdown","0df2d85c":"markdown","eee4c9ea":"markdown","09f927a7":"markdown","0b1ea6c2":"markdown","22148255":"markdown","1e6dd4b9":"markdown","f5b5fd2c":"markdown","5da78f94":"markdown","b9548aaa":"markdown","7be78e5b":"markdown","5a981678":"markdown","2cbc7b0d":"markdown","c10d8833":"markdown","86abe93d":"markdown","88adc519":"markdown","e0da1bc0":"markdown","351d6153":"markdown","5c75f3a7":"markdown","2e8d95cb":"markdown","9ffa4994":"markdown","4321ab0c":"markdown","9e6515e7":"markdown","1f247e3f":"markdown","6d0d4d4e":"markdown","21e0f7ec":"markdown","8cdf98a9":"markdown","d5b994ed":"markdown","bed5761f":"markdown","ac1f26ad":"markdown"},"source":{"f93b45bb":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud, ImageColorGenerator\nfrom PIL import Image\nfrom sklearn.manifold import TSNE\n\nfrom sklearn.utils import shuffle\n\nfrom sklearn.naive_bayes import MultinomialNB, BernoulliNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\n\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom gensim.models import Word2Vec\n\nfrom imblearn.under_sampling import RandomUnderSampler\n\nimport nltk\nfrom nltk.tokenize import TweetTokenizer\nnltk.download('stopwords')","4e31113d":"tweets = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntweets = tweets.drop([\"id\", \"keyword\", \"location\"], axis=1)\ntweets = shuffle(tweets, random_state=42)\ntweets.head()","a6f14092":"mapping = {0: 'Disaster', 1: 'No disaster'}\ncategories = tweets.target.map(mapping)\n\nplt.figure(figsize=(10, 8))\nplot = sns.histplot(categories, color='orange', shrink=0.9)\nplt.xticks(fontsize=20)\nplt.yticks(fontsize=20)\nplt.xlabel('xlabel', fontsize=20)\nplt.ylabel('ylabel', fontsize=20)\nplot.set(xlabel='Target', ylabel='Frequence')","4f579637":"stop_words = set(nltk.corpus.stopwords.words(\"english\"))\ntknzr = TweetTokenizer()\n\nvect = CountVectorizer(stop_words=stop_words, tokenizer=tknzr.tokenize)\ntmp_texts = vect.fit_transform(tweets[tweets.target==0].text)\nfreq = tmp_texts.toarray().sum(axis=0)\nwords = vect.get_feature_names()\n\nfreq_words = dict(zip(words, freq))\n\nmask = np.array(Image.open(\"\/kaggle\/input\/twitter\/twitter.png\"))\n\nwc = WordCloud(background_color=\"white\",\n               width=1000, height=1000, max_words=150,\n               mask=mask,\n               max_font_size=200).generate_from_frequencies(freq_words)\n\nimage_colors = ImageColorGenerator(mask)\n\nplt.figure(figsize=(15, 10))\nplt.title(\"No disaster\")\nplt.imshow(wc.recolor(color_func=image_colors), interpolation=\"bilinear\")\n","aad1219d":"stop_words = set(nltk.corpus.stopwords.words(\"english\"))\ntknzr = TweetTokenizer()\n\nvect = CountVectorizer(stop_words=stop_words, tokenizer=tknzr.tokenize)\ntmp_texts = vect.fit_transform(tweets[tweets.target==1].text)\nfreq = tmp_texts.toarray().sum(axis=0)\nwords = vect.get_feature_names()\n\nfreq_words = dict(zip(words, freq))\n\nmask = np.array(Image.open(\"\/kaggle\/input\/twitter\/twitter.png\"))\n\nwc = WordCloud(background_color=\"white\",\n               width=1000, height=1000, max_words=150,\n               mask=mask,\n               max_font_size=200).generate_from_frequencies(freq_words)\n\nimage_colors = ImageColorGenerator(mask)\n\nplt.figure(figsize=(15, 10.))\nplt.title(\"Disaster\")\nplt.imshow(wc.recolor(color_func=image_colors), interpolation=\"bilinear\")\n","d48ed1e9":"vect = CountVectorizer(tokenizer=tknzr.tokenize)\ntmp_texts = vect.fit_transform(tweets[tweets.target==0].text)\nfreq = tmp_texts.toarray().sum(axis=0)\nwords = vect.get_feature_names()\nfreq_words_for = dict(zip(words, freq))\n\nvect = CountVectorizer(tokenizer=tknzr.tokenize)\ntmp_texts = vect.fit_transform(tweets[tweets.target==1].text)\nfreq = tmp_texts.toarray().sum(axis=0)\nwords = vect.get_feature_names()\nfreq_words_against = dict(zip(words, freq))","80ecf7db":"texts = tweets.text.str.lower()\ntexts = texts.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n\nX = texts.apply(tknzr.tokenize)\n\nemb = Word2Vec(X, vector_size=100, min_count=80, workers=8)","c471b0b2":"tokens = []\nlabels = []\n\nfor word in emb.wv.key_to_index:\n  tokens.append(emb.wv[word])\n  labels.append(word)\n\ntsne = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500)\nnew_values = tsne.fit_transform(tokens)","0b4c7691":"x = []\ny = []\nfor value in new_values:\n    x.append(value[0])\n    y.append(value[1])\n    \nplt.figure(figsize=(20, 15)) \nfor i in range(len(x)):\n    if freq_words_for[labels[i]] > freq_words_against[labels[i]]:\n      plt.scatter(x[i],y[i], color = 'red')\n    else:\n      plt.scatter(x[i],y[i], color = 'blue')\n    plt.annotate(labels[i],\n                  xy=(x[i], y[i]),\n                  xytext=(5, 2),\n                  textcoords='offset points',\n                  ha='right',\n                  va='bottom')\nplt.show()","ca7112ac":"X, y = tweets.text, tweets.target # Separate input (text) and target\n\nunder_sampling = RandomUnderSampler()\n\nX, y = under_sampling.fit_resample(np.array(X).reshape(-1,1), y)\nX = X.flatten()","1798256b":"# Putting the strings of X in var texts, to use X before\ntexts = X","9a7c08f4":"# tuned_parameters = [\n#     {'alpha': [0.01, 0.1, 0.3, 0.5, 0.6, 1]}\n# ]\n## Multinomial Naive Bayes\n# ngram_list = [(1,1), (2,2), (5,5), (8,8), (1,5), (3,8)]\n# analyzer_list = ['word', 'char']\n\n# for analyzer in analyzer_list:\n#     for ngram in ngram_list:\n#         print(\"ngram_range: {} | analyzer: {}\".format(ngram, analyzer))\n        \n#         bow = CountVectorizer(ngram_range=ngram, analyzer=analyzer)\n#         X = bow.fit_transform(texts)\n#         grid = GridSearchCV(MultinomialNB(), tuned_parameters, n_jobs=-1, cv=3)\n#         grid.fit(X, y)\n#         print(\"Melhores par\u00e2metros:\", grid.best_params_)\n        \n#         clf = MultinomialNB(**grid.best_params_)\n#         acc_cv = cross_val_score(clf, X, y, cv=5, n_jobs=-1)\n#         print(\"Scores:\", acc_cv)\n#         print(\"Acur\u00e1cia m\u00e9dia:\", acc_cv.mean())\n#         print(\"----\")","994f4969":"# tuned_parameters = [\n#     {'alpha': [0.01, 0.1, 0.3, 0.5, 0.6, 1]}\n# ]\n\n# ngram_list = [(1,1), (2,2), (5,5), (8,8), (1,5), (3,8)]\n# analyzer_list = ['word', 'char']\n\n# for analyzer in analyzer_list:\n#     for ngram in ngram_list:\n#         print(\"ngram_range: {} | analyzer: {}\".format(ngram, analyzer))\n        \n#         bow = CountVectorizer(ngram_range=ngram, analyzer=analyzer)\n#         X = bow.fit_transform(texts)\n#         grid = GridSearchCV(BernoulliNB(), tuned_parameters, n_jobs=-1, cv=3)\n#         grid.fit(X, y)\n#         print(\"Melhores par\u00e2metros:\", grid.best_params_)\n        \n#         clf = BernoulliNB(**grid.best_params_)\n#         acc_cv = cross_val_score(clf, X, y, cv=5, n_jobs=-1)\n#         print(\"Scores:\", acc_cv)\n#         print(\"Acur\u00e1cia m\u00e9dia:\", acc_cv.mean())\n#         print(\"----\")","5278c262":"# tuned_parameters = [\n#     {\"kernel\": [\"rbf\"], \"gamma\": [1e-3, 1e-4], \"C\": [0.01, 0.1, 1, 10, 100]},\n#     {\"kernel\": [\"linear\"], \"C\": [0.01, 0.1, 1, 10, 100]},\n# ]\n\n# ngram_list = [(1,1), (2,2), (5,5), (8,8), (1,5), (3,8)]\n# analyzer_list = ['word', 'char']\n\n# for analyzer in analyzer_list:\n#     for ngram in ngram_list:\n#         print(\"ngram_range: {} | analyzer: {}\".format(ngram, analyzer))\n        \n#         bow = CountVectorizer(ngram_range=ngram, analyzer=analyzer)\n#         X = bow.fit_transform(texts)\n#         grid = GridSearchCV(SVC(), tuned_parameters, n_jobs=-1, cv=3)\n#         grid.fit(X, y)\n#         print(\"Melhores par\u00e2metros:\", grid.best_params_)\n        \n#         clf = SVC(**grid.best_params_)\n#         acc_cv = cross_val_score(clf, X, y, cv=5, n_jobs=-1)\n#         print(\"Scores:\", acc_cv)\n#         print(\"Acur\u00e1cia m\u00e9dia:\", acc_cv.mean())\n#         print(\"----\")","2cae1b92":"# tuned_parameters = [\n#     {\"n_estimators\": [10, 100, 200], \"max_depth\": [10, 100, 200, 400]}\n# ]\n\n# ngram_list = [(1,1), (2,2), (5,5), (8,8), (1,5), (3,8)]\n# analyzer_list = ['word', 'char']\n\n# for analyzer in analyzer_list:\n#     for ngram in ngram_list:\n#         print(\"ngram_range: {} | analyzer: {}\".format(ngram, analyzer))\n        \n#         bow = CountVectorizer(ngram_range=ngram, analyzer=analyzer)\n#         X = bow.fit_transform(texts)\n#         grid = GridSearchCV(RandomForestClassifier(), tuned_parameters, n_jobs=-1, cv=3)\n#         grid.fit(X, y)\n#         print(\"Melhores par\u00e2metros:\", grid.best_params_)\n        \n#         clf = RandomForestClassifier(**grid.best_params_)\n#         acc_cv = cross_val_score(RandomForestClassifier(), X, y, cv=5, n_jobs=-1)\n#         print(\"Scores:\", acc_cv)\n#         print(\"Acur\u00e1cia m\u00e9dia:\", acc_cv.mean())\n#         print(\"----\") ","e1e8e9a1":"# ngram_list = [(1,5), (3,8)]\n# analyzer_list = ['char']\n\n# for analyzer in analyzer_list:\n#     for ngram in ngram_list:\n#         print(\"ngram_range: {} | analyzer: {}\".format(ngram, analyzer))\n        \n#         bow = CountVectorizer(ngram_range=ngram, analyzer=analyzer)\n#         X = bow.fit_transform(texts)\n        \n#         clf = AdaBoostClassifier(DecisionTreeClassifier(), n_estimators=200)\n#         acc_cv = cross_val_score(clf, X, y, cv=5, n_jobs=7)\n#         print(\"Scores:\", acc_cv)\n#         print(\"Acur\u00e1cia m\u00e9dia:\", acc_cv.mean())\n#         print(\"----\")","f6550257":"# tuned_parameters = [\n#     {\"n_estimators\": [10, 100], \"learning_rate\": [0.1, 1], \"max_depth\": [10, 200, 400]}\n# ]\n\n# ngram_list = [(1,1), (2,2), (5,5), (8,8), (1,5), (3,8)]\n# analyzer_list = ['word', 'char']\n\n# for analyzer in analyzer_list:\n#     for ngram in ngram_list:\n#         print(\"ngram_range: {} | analyzer: {}\".format(ngram, analyzer))\n        \n#         bow = CountVectorizer(ngram_range=ngram, analyzer=analyzer)\n#         X = bow.fit_transform(texts)\n#         grid = GridSearchCV(GradientBoostingClassifier(), tuned_parameters, n_jobs=7, cv=3)\n#         grid.fit(X, y)\n#         print(\"Melhores par\u00e2metros:\", grid.best_params_)\n        \n#         clf = GradientBoostingClassifier(**grid.best_params_)\n#         acc_cv = cross_val_score(GradientBoostingClassifier(), X, y, cv=5, n_jobs=7)\n#         print(\"Scores:\", acc_cv)\n#         print(\"Acur\u00e1cia m\u00e9dia:\", acc_cv.mean())\n#         print(\"----\")","468185ab":"bow = CountVectorizer()\nX = bow.fit_transform(texts)\n\nclf = LinearSVC(C=0.1)\nacc_cv = cross_val_score(clf, X, y, cv=10, n_jobs=-1, scoring=\"f1\")\nprint(\"Scores:\", acc_cv)\nprint(\"Average F1:\", acc_cv.mean())","ad692529":"# Training the model\nclf = LinearSVC(C=0.1)\nclf.fit(X, y)","98a1d905":"# Reading test data\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\ntest = test[[\"id\", \"text\"]]\nX_test = bow.transform(test.text)","149beaeb":"# Making the predictions\ny_predicted = clf.predict(X_test)\nsns.histplot(y_predicted)","9e7a3f56":"# submission = pd.DataFrame({\"id\": test.id, \"target\":y_predicted})\n# submission.to_csv(\"tt_submission.csv\",index=False)","002be352":"# tuned_parameters = [\n#     {'alpha': [0.01, 0.1, 0.3, 0.5, 0.6, 1]}\n# ]\n\n# ngram_list = [(1,1), (2,2), (5,5), (8,8), (1,5), (3,8)]\n# analyzer_list = ['word', 'char']\n\n# for analyzer in analyzer_list:\n#     for ngram in ngram_list:\n#         print(\"ngram_range: {} | analyzer: {}\".format(ngram, analyzer))\n        \n#         tfidf = TfidfVectorizer(ngram_range=ngram, analyzer=analyzer)\n#         X = tfidf.fit_transform(texts)\n#         grid = GridSearchCV(MultinomialNB(), tuned_parameters, n_jobs=-1, cv=3)\n#         grid.fit(X, y)\n#         print(\"Melhores par\u00e2metros:\", grid.best_params_)\n        \n#         clf = MultinomialNB(**grid.best_params_)\n#         acc_cv = cross_val_score(clf, X, y, cv=5, n_jobs=-1)\n#         print(\"Scores:\", acc_cv)\n#         print(\"Acur\u00e1cia m\u00e9dia:\", acc_cv.mean())\n#         print(\"----\")","563d5479":"# tuned_parameters = [\n#     {'alpha': [0.01, 0.1, 0.3, 0.5, 0.6, 1]}\n# ]\n\n# ngram_list = [(1,1), (2,2), (5,5), (8,8), (1,5), (3,8)]\n# analyzer_list = ['word', 'char']\n\n# for analyzer in analyzer_list:\n#     for ngram in ngram_list:\n#         print(\"ngram_range: {} | analyzer: {}\".format(ngram, analyzer))\n        \n#         tfidf = TfidfVectorizer(ngram_range=ngram, analyzer=analyzer)\n#         X = tfidf.fit_transform(texts)\n#         grid = GridSearchCV(BernoulliNB(), tuned_parameters, n_jobs=-1, cv=3)\n#         grid.fit(X, y)\n#         print(\"Melhores par\u00e2metros:\", grid.best_params_)\n        \n#         clf = BernoulliNB(**grid.best_params_)\n#         acc_cv = cross_val_score(clf, X, y, cv=5, n_jobs=-1)\n#         print(\"Scores:\", acc_cv)\n#         print(\"Acur\u00e1cia m\u00e9dia:\", acc_cv.mean())\n#         print(\"----\")","68a7bb65":"# tuned_parameters = [\n#     {\"n_estimators\": [10, 100, 200], \"max_depth\": [10, 100, 200, 400]}\n# ]\n\n# ngram_list = [(1,1), (2,2), (5,5), (8,8), (1,5), (3,8)]\n# analyzer_list = ['word', 'char']\n\n# for analyzer in analyzer_list:\n#     for ngram in ngram_list:\n#         print(\"ngram_range: {} | analyzer: {}\".format(ngram, analyzer))\n        \n#         tfidf = TfidfVectorizer(ngram_range=ngram, analyzer=analyzer)\n#         X = tfidf.fit_transform(texts)\n#         grid = GridSearchCV(RandomForestClassifier(), tuned_parameters, n_jobs=7, cv=3)\n#         grid.fit(X, y)\n#         print(\"Melhores par\u00e2metros:\", grid.best_params_)\n        \n#         clf = RandomForestClassifier(**grid.best_params_)\n#         acc_cv = cross_val_score(RandomForestClassifier(), X, y, cv=5, n_jobs=7)\n#         print(\"Scores:\", acc_cv)\n#         print(\"Acur\u00e1cia m\u00e9dia:\", acc_cv.mean())\n#         print(\"----\")\n ","9968d9f4":"# tuned_parameters = [\n#     {\"kernel\": [\"rbf\"], \"gamma\": [1e-3, 1e-4], \"C\": [0.01, 0.1, 1, 10, 100]},\n#     {\"kernel\": [\"linear\"], \"C\": [0.01, 0.1, 1, 10, 100]},\n# ]\n\n# ngram_list = [(1,1), (2,2), (5,5), (8,8), (1,5), (3,8)]\n# analyzer_list = ['word', 'char']\n\n# for analyzer in analyzer_list:\n#     for ngram in ngram_list:\n#         print(\"ngram_range: {} | analyzer: {}\".format(ngram, analyzer))\n        \n#         tfidf = TfidfVectorizer(ngram_range=ngram, analyzer=analyzer)\n#         X = tfidf.fit_transform(texts)\n#         grid = GridSearchCV(SVC(), tuned_parameters, n_jobs=-1, cv=3)\n#         grid.fit(X, y)\n#         print(\"Melhores par\u00e2metros:\", grid.best_params_)\n        \n#         clf = SVC(**grid.best_params_)\n#         acc_cv = cross_val_score(clf, X, y, cv=5, n_jobs=-1)\n#         print(\"Scores:\", acc_cv)\n#         print(\"Acur\u00e1cia m\u00e9dia:\", acc_cv.mean())\n#         print(\"----\")","761a3856":"tfidf = TfidfVectorizer(ngram_range = (1,5), analyzer = \"char\")\nX = tfidf.fit_transform(texts)\n\nclf = LinearSVC(C=1)\nacc_cv = cross_val_score(clf, X, y, cv=10, n_jobs=-1, scoring=\"f1\")\nprint(\"Scores:\", acc_cv)\nprint(\"Average F1:\", acc_cv.mean())","2712cfc8":"# Training the model\nclf = LinearSVC(C=1)\nclf.fit(X, y)","c4924a30":"# Reading test data\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\ntest = test[[\"id\", \"text\"]]\nX_test = tfidf.transform(test.text)","1db74409":"# Making the predictions\ny_predicted = clf.predict(X_test)\nsns.histplot(y_predicted)","9a7d607a":"submission = pd.DataFrame({\"id\": test.id, \"target\":y_predicted})\nsubmission.to_csv(\"tt_submission.csv\",index=False)","b49f36f7":"# Undersampling\n\nBefore go to the classification, let's randomly remove data of the majority class to have a balanced problem.","a0e9b8ad":"Some word stand out from other when comparing the two clouds. So we can formulate the hypothesis that the frequency has (or has not) influence to a tweet be about disasters or not. But, like any text problem, some words appear on both cloud with no difference visible (for example, the \"people\" token).\n\nTo construct the word cloud I used a NLP aproach to transform text in frequencies called Bag of Words, that will be talked again when we arrive the classification section.\n\nBut, another aproach that takes into account the structure of the text to generate a representation of words on vectorial space. I am talking about embeddings, and I use here Word2Vec","a3af3349":"## Multinomial Naive Bayes","37cc09e2":"# Tf-idf\n\n","4e9999f7":"Best configuration:\n\nngram_range: (3, 8) | analyzer: char\n\nScores: [0.73567609 0.69595111 0.74464832 0.68730887 0.70718654]\n\nAcur\u00e1cia m\u00e9dia: 0.7141541854439858","0df2d85c":"## SVM","eee4c9ea":"Best configuration:\n\nngram_range: (1, 1) | analyzer: word\n\nMelhores par\u00e2metros: {'alpha': 0.6}\n\nScores: [0.7868602  0.78380443 0.80351682 0.78287462 0.78211009]\n\nAcur\u00e1cia m\u00e9dia: 0.7878332317080293","09f927a7":"Target is 1 when the tweet talks about some disaster, and 0 otherwise. The problem is almost balanced.","0b1ea6c2":"# Bag of Words\n\nThis aproach can be seem as naive, I don't think so. Bag of Words consists of taking the frequence of the tokens in a sentence, and that will be the feautures. You can realize that it results on a large number of features, and for that some people use KBest or PCA to reduce the dimensionality (what is strongly recommended to try).\n\n**Important**: I commented the code that runs the models and the grid search because some of them are really slowly, and there is no need to run them again to bring the information we have.","22148255":"Best configuration:\n\nngram_range: (5, 5) | analyzer: char\n\nMelhores par\u00e2metros: {'max_depth': 200, 'n_estimators': 200}\n\nScores: [0.76012223 0.7578304  0.79740061 0.74923547 0.75917431]\n\nAcur\u00e1cia m\u00e9dia: 0.7647526066306423","1e6dd4b9":"Best configuration:\n\nngram_range: (1, 1) | analyzer: word\n\nMelhores par\u00e2metros: {'C': 0.1, 'kernel': 'linear'}\n\nScores: [0.80061115 0.78838808 0.81422018 0.77293578 0.77293578]\n\nAcur\u00e1cia m\u00e9dia: 0.7898181958354652","f5b5fd2c":"Best configuration:\n\nngram_range: (1, 1) | analyzer: word\n\nMelhores par\u00e2metros: {'alpha': 0.6}\n\nScores: [0.78456837 0.78151261 0.80504587 0.78287462 0.77828746]\n\nAcur\u00e1cia m\u00e9dia: 0.786457785783204","5da78f94":"Let's see the text and the target we have in the training dataset","b9548aaa":"## AdaBoost","7be78e5b":"Best configuration:\n\nngram_range: (5, 5) | analyzer: char\n\nMelhores par\u00e2metros: {'max_depth': 400, 'n_estimators': 200}\n\nScores: [0.78533231 0.7578304  0.7912844  0.76452599 0.76376147]\n\nAcur\u00e1cia m\u00e9dia: 0.7725469170153466","5a981678":"# Libraries\n\nI am using python to develop this notebook, so I imported some of the classic libraries to manuipulate and visualize data, further libraries to apply machine learning models on text, and process them.","2cbc7b0d":"## SVM + Tf-idf predictions (best model so far)","c10d8833":"Best configuration:\n\nngram_range: (1, 1) | analyzer: word\n\nMelhores par\u00e2metros: {'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 100}\n\nScores: [0.74178762 0.71963331 0.74770642 0.70718654 0.69801223]\n\nAcur\u00e1cia m\u00e9dia: 0.7228652261571851","86abe93d":"## Bernoulli Naive Bayes","88adc519":"# SVM","e0da1bc0":"Best configuration:\n\nngram_range: (1, 5) | analyzer: char\n\nMelhores par\u00e2metros: {'C': 1, 'kernel': 'linear'}\n\nScores: [0.79220779 0.7776929  0.82568807 0.77370031 0.7851682 ]\n\nAcur\u00e1cia m\u00e9dia: 0.7908914524942586","351d6153":"## Random Forest","5c75f3a7":"## Gradient Boosting","2e8d95cb":"## Multinomial Naive Bayes","9ffa4994":"## SVM + BoW predictions (best model so far)","4321ab0c":"## Random Forest","9e6515e7":"Best configuration:\n\nngram_range: (1, 1) | analyzer: word\n\nMelhores par\u00e2metros: {'alpha': 0.5}\n\nScores: [0.78609626 0.77616501 0.80351682 0.78211009 0.77599388]\n\nAcur\u00e1cia m\u00e9dia: 0.784776412650131","1f247e3f":"There are no clear cluestes of words, but we can realize that are some similiarity thanks the word2vec aproach. May be helpfull creating a model, but probabily not\n","6d0d4d4e":"## Bernoulli Naive Bayes","21e0f7ec":"# Tweets and disasters - A NLP aproach to understand and indentify disasters by text\n\nSocial Networks are part of the atual society, by them we share informations instantly and can see what's going on outside (sometimes, of the bubble). Disregarding some problems of this universe (like fake news), the lot of text about something really bad happening can help people to avoid some region or call  for help. Some tweets bring information like that, and the objective of this notebook is understand and to classify what text is about disasters and what is not.\n","8cdf98a9":"# Classification\n\nA important part of Natural Language Processing is the step of process natural language, although it is typically ignored. First I will apply an exhaustive grid search on classical models of Machine Learning using Bag of Words as vectorization aproach. When the best model is found, I'll use some aproaches of proccess the text and see if has any significant difference using the best model (like remove stop words, replace number by a unique token etc). Having a good and robust baseline with bag of words, I'll apply grid search again with Tf-idf, and test Word2Vec, LSTM, CNN and BERT (not today). I personally don't like to remove any data from text, except for rebalance the data.","d5b994ed":"A wordcloud can help to see the sobreposition of text between the two categories","bed5761f":"# Analysis","ac1f26ad":"Best configuration:\n\nngram_range: (1, 1) | analyzer: word\n\nMelhores par\u00e2metros: {'alpha': 0.6}\n\nScores: [0.79526356 0.77922078 0.79969419 0.78363914 0.76834862]\n\nAcur\u00e1cia m\u00e9dia: 0.7852332592753531"}}