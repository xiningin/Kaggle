{"cell_type":{"225b337c":"code","f5acbe6b":"code","49090d2c":"code","59bdc2f2":"code","1e53e1ac":"code","02269cfa":"code","a2caba38":"code","f46e95e9":"code","e04c4ecd":"code","56522ad4":"code","7289ecdd":"code","48ec6cdb":"code","37fdc5fb":"code","a4fa72af":"code","5d60c66d":"code","4f162231":"code","0e6849e1":"code","11d6fffa":"code","7329bbc3":"markdown","3cbc047b":"markdown","c288ec6d":"markdown","cdec6cff":"markdown","85e936dd":"markdown"},"source":{"225b337c":"import numpy as np\nimport pandas as pd\nimport pickle\nfrom sklearn import preprocessing\nfrom math import ceil\nfrom datetime import datetime\nfrom  datetime import datetime, timedelta\n\n\nINPUT_DIR = '\/kaggle\/input\/m5-forecast\/'\nINPUT_DIR2 = '\/kaggle\/input\/m5-forecasting-accuracy\/'","f5acbe6b":"features_columns = ['item_id',\n 'dept_id',\n 'cat_id',\n 'sell_price',\n 'price_max',\n 'price_min',\n 'price_std',\n 'price_mean',\n 'price_norm',\n 'price_nunique',\n 'price_momentum',\n 'price_momentum_m',\n 'price_momentum_y',\n 'event_type_1',\n 'event_type_2',\n 'snap',\n 'tm_d',\n 'tm_dw',\n 'tm_w',\n 'tm_m',\n 'tm_q',\n 'tm_y',\n 'tm_wm',\n 'tm_w_end',\n 'sales_lag_7',\n 'sales_lag_28',\n 'rolling_mean_tmp_0_7',\n 'rolling_mean_tmp_0_14',\n 'rolling_mean_tmp_0_28',\n 'rolling_mean_tmp_7_7',\n 'rolling_mean_tmp_7_14',\n 'rolling_mean_tmp_7_28',\n 'rolling_mean_tmp_14_7',\n 'rolling_mean_tmp_14_14',\n 'rolling_mean_tmp_14_28']\n\n \n\n## Memory Reducer\n# :df pandas dataframe to reduce size             # type: pd.DataFrame()\n# :verbose                                        # type: bool\ndef reduce_mem_usage(df, verbose=True):\n    start_mem = df.memory_usage().sum() \/ 1024**2  \n    \n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                       df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n","49090d2c":"eval_end_day = 1941 # the last day in evaluation data set\ndef create_test_data(test_start=1800,is_train=True, add_test=False):\n    # data types of each columns \n    # start_day = train_start if is_train else test_start\n    numcols = [f\"d_{day}\" for day in range(test_start,eval_end_day+1)]\n    catcols = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id']\n    SALE_DTYPES = {numcol:\"float32\" for numcol in numcols} \n    SALE_DTYPES.update({col: \"category\" for col in catcols if col != \"id\"}) \n\n    # loading data\n    sale_data = pd.read_csv(INPUT_DIR2 + 'sales_train_evaluation.csv',dtype=SALE_DTYPES,usecols=catcols+numcols)\n\n\n    # # add test days with nan \n    for day in range(eval_end_day+1, eval_end_day+ 28 +1): # 1942 to 1942+28\n        sale_data[f\"d_{day}\"] = np.nan\n\n    # In the sales dataset, each row represents one item in one specific store. since our target is sales,\n    # We can tranform horizontal representation \n    # into vertical \"view\" so that each row represents for sales for one day.\n    # Our \"index\" will be 'id','item_id','dept_id','cat_id','store_id','state_id'\n    # and labels are 'd_' coulmns\n    grid_df = pd.melt(sale_data,\n            id_vars = catcols,\n            value_vars = [col for col in sale_data.columns if col.startswith(\"d_\")],\n            var_name = \"d\",\n            value_name = \"sales\")\n    \n#     # we can add test days with nan after melt, but more tedious\n#     if add_test == True:\n#         END_TRAIN = 1913         # Last day in train set\n#         # To be able to make predictions\n#         # we need to add \"test set\" to our grid\n#         add_grid = pd.DataFrame()\n#         for i in range(1,29):\n#             # construct the index columns\n#             temp_df = sale_data[catcols]\n#             temp_df = temp_df.drop_duplicates() # Actually, no need this since each row in original sale data indexed by catcols representing one item in one store\n#             # add label column for sales\n#             temp_df['d'] = 'd_'+ str(END_TRAIN+i)\n#             # add sales column\n#             temp_df['sales'] = np.nan\n#             add_grid = pd.concat([add_grid,temp_df])\n#         grid_df = pd.concat([grid_df,add_grid])\n#      #Remove some temoprary DFs\n#         del temp_df, add_grid\n        \n    # the index of concated df keep the original ones, needs to be reset\n    grid_df = grid_df.reset_index(drop=True)\n        \n    # We will not need original sale-data\n    # anymore and can remove it\n    del sale_data\n    \n    \n    # It seems that leadings zero values\n    # in each train_df item row\n    # are not real 0 sales but mean\n    # absence for the item in the store\n    # by doing inner join we can remove\n    # such zeros\n    PRICE_DTYPES = {\"store_id\": \"category\", \"item_id\": \"category\", \"wm_yr_wk\": \"int16\",\"sell_price\":\"float32\" }\n    CAL_DTYPES={\"event_name_1\": \"category\", \"event_name_2\": \"category\", \"event_type_1\": \"category\", \n            \"event_type_2\": \"category\", \"weekday\": \"category\", 'wm_yr_wk': 'int16', \"wday\": \"int16\",\n            \"month\": \"int16\", \"year\": \"int16\", \"snap_CA\": \"float32\", 'snap_TX': 'float32', 'snap_WI': 'float32' }\n    cal_data = pd.read_csv(INPUT_DIR2 + '.\/calendar.csv',dtype=CAL_DTYPES)\n    price_data = pd.read_csv(INPUT_DIR2 + '\/sell_prices.csv',dtype=PRICE_DTYPES)\n    ## get wm_yr_wk as key for join the price table\n    grid_df = grid_df.merge(cal_data[['d', 'wm_yr_wk']], on= \"d\", copy = False)\n    grid_df = grid_df.merge(price_data[[\"store_id\", \"item_id\", \"wm_yr_wk\"]], on = [\"store_id\", \"item_id\", \"wm_yr_wk\"])\n    \n    \n    return grid_df\n","59bdc2f2":"def create_prices_features(prices_df):\n    ########################### Prices\n    #################################################################################\n    print('Create Prices Features')\n\n    # We can do some basic aggregations\n    prices_df['price_max'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('max')\n    prices_df['price_min'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('min')\n    prices_df['price_std'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('std')\n    prices_df['price_mean'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('mean')\n\n    # and do price normalization (min\/max scaling)\n    prices_df['price_norm'] = prices_df['sell_price']\/prices_df['price_max']\n\n    # Some items are can be inflation dependent\n    # and some items are very \"stable\"\n    prices_df['price_nunique'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('nunique') # how many prices for each items(in a store), reflect inflation and stable\n    \n    # since group by object will only leave string columns, get categorical for item_id\n    # also, interestingly, groupby will do combination if any element for groupby is cagtegory type\n    # https:\/\/github.com\/pandas-dev\/pandas\/issues\/17594\n#     prices_df['item_id_a'] =prices_df['item_id'].cat.codes\n#     prices_df['item_nunique'] = prices_df.groupby(['store_id','sell_price'])['item_id_a'].transform('nunique') # how many items(in a store) have same price\n#     prices_df = prices_df.drop('item_id_a', axis=1)\n\n    # I would like some \"rolling\" aggregations, i,e. price \"momentum\" (some sort of)\n    # but would like months and years as \"window\", so the next three commands add months and years as columns\n    CAL_DTYPES={\"event_name_1\": \"category\", \"event_name_2\": \"category\", \"event_type_1\": \"category\", \n        \"event_type_2\": \"category\", \"weekday\": \"category\", 'wm_yr_wk': 'int16', \"wday\": \"int16\",\n        \"month\": \"int16\", \"year\": \"int16\", \"snap_CA\": \"float32\", 'snap_TX': 'float32', 'snap_WI': 'float32' }\n    cal_data = pd.read_csv(INPUT_DIR2+'calendar.csv',dtype=CAL_DTYPES)\n    ## get month, year to join into prices_df\n    calendar_prices = cal_data[['wm_yr_wk','month','year']]\n    # approcimately have (the length of the original\/ 7), since the calendar_df is recorded by day, now week\n    calendar_prices = calendar_prices.drop_duplicates(subset=['wm_yr_wk'])\n    prices_df = prices_df.merge(calendar_prices[['wm_yr_wk','month','year']], on=['wm_yr_wk'], how='left')\n    del calendar_prices\n\n    # Now we can add price \"momentum\" (some sort of)\n    # Shifted by week \n    # by month mean\n    # by year mean\n    prices_df['price_momentum'] = prices_df['sell_price']\/prices_df.groupby(['store_id','item_id'])['sell_price'].transform(lambda x: x.shift(1)) # the rate with sell price last day\n#     ## cannot use built-in mean which would output nan if the group has nan values\n#     prices_df['price_momentum_m'] = prices_df['sell_price']\/prices_df.groupby(['store_id','item_id','month'])['sell_price'].transform('mean') # the rate with sell price last month\n#     prices_df['price_momentum_y'] = prices_df['sell_price']\/prices_df.groupby(['store_id','item_id','year'])['sell_price'].transform('mean') # the rate with sell price last year\n    prices_df['price_momentum_m'] = prices_df['sell_price']\/prices_df.groupby(['store_id','item_id','month'])['sell_price'].transform(lambda x: np.mean([i for i in x if not np.isnan(i)]))\n    prices_df['price_momentum_y'] = prices_df['sell_price']\/prices_df.groupby(['store_id','item_id','year'])['sell_price'].transform(lambda x: np.mean([i for i in x if not np.isnan(i)])) # the rate with sell price last year\n#     # for testing the problem of transform('mean') which gives different values by using costom mean function(not because of null value)\n#     idx = (price_data['store_id'] == 'WI_3') & (price_data['item_id'] == 'FOODS_3_827') & (price_data['month'] == 6)\n#     price_data[['sell_price']][idx]\n\n    del prices_df['month'], prices_df['year']\n    \n    prices_df = reduce_mem_usage(prices_df)\n    \n    return prices_df","1e53e1ac":"grid_df = create_test_data()\n\n# add price features\nPRICE_DTYPES = {\"store_id\": \"category\", \"item_id\": \"category\", \"wm_yr_wk\": \"int16\",\"sell_price\":\"float32\" }\nprices_df = pd.read_csv(INPUT_DIR2 + '\/sell_prices.csv',dtype=PRICE_DTYPES)\nprices_df = create_prices_features(prices_df)\ngrid_df = grid_df.merge(prices_df, on=['store_id','item_id','wm_yr_wk'], how='left')\ndel prices_df","02269cfa":"# These 16 items in the specific stores are lack of sell_prices in one specific wm_yr_wk. So totally 16*7=112\n# ['HOUSEHOLD_1_183_CA_4_evaluation', 'FOODS_3_296_CA_1_evaluation',\n#        'FOODS_3_296_CA_2_evaluation', 'FOODS_3_296_TX_2_evaluation',\n#        'FOODS_3_296_WI_2_evaluation', 'HOUSEHOLD_1_512_CA_3_evaluation',\n#        'FOODS_3_296_CA_4_evaluation', 'HOUSEHOLD_1_400_WI_2_evaluation',\n#        'FOODS_3_595_CA_1_evaluation', 'HOUSEHOLD_1_311_CA_2_evaluation',\n#        'HOUSEHOLD_1_405_CA_2_evaluation', 'HOUSEHOLD_1_278_CA_3_evaluation', 'FOODS_3_595_CA_3_evaluation',\n#        'HOUSEHOLD_1_400_CA_4_evaluation', 'HOUSEHOLD_1_386_WI_1_evaluation','HOUSEHOLD_1_020_WI_2_evaluation']\n\n#  853720 = 30490 items in 10 stores * 28 days which would be predicted\n \nfor col in list(grid_df):\n    num_na = grid_df[col].isnull().sum()\n    if num_na != 0:\n        print(col, str(num_na))","a2caba38":"# check some anomolous records\n# idx = (grid_df.d=='d_1812') & (grid_df.id == 'HOUSEHOLD_1_183_CA_4_evaluation')\n# grid_df[idx]","f46e95e9":"def make_time_features(grid_df):\n    # Convert to DateTime\n    grid_df['date'] = pd.to_datetime(grid_df['date'])\n\n    # Make some features from date:  \n    # \u6709\u7684\u65f6\u95f4\u7279\u5f81\u6ca1\u6709\uff0c\u901a\u8fc7datetime\u7684\u65b9\u6cd5\u81ea\u52a8\u751f\u6210\n    grid_df['tm_d'] = grid_df['date'].dt.day.astype(np.int8)\n    grid_df['tm_dw'] = grid_df['date'].dt.dayofweek.astype(np.int8)\n    grid_df['tm_w'] = grid_df['date'].dt.week.astype(np.int8)\n    grid_df['tm_m'] = grid_df['date'].dt.month.astype(np.int8)\n    grid_df['tm_q'] = grid_df['date'].dt.quarter.astype(np.int8)\n    grid_df['tm_y'] = grid_df['date'].dt.year\n    grid_df['tm_y'] = (grid_df['tm_y'] - grid_df['tm_y'].min()).astype(np.int8)\n    grid_df['tm_wm'] = grid_df['tm_d'].apply(lambda x: ceil(x\/7)).astype(np.int8)\n    \n    # whether it is weekend\n    grid_df['tm_w_end'] = (grid_df['tm_dw']>=5).astype(np.int8)\n\n    # Remove date\n    #del grid_df['date']\n    \n    \n    return grid_df","e04c4ecd":"# add time features: from calendar files and pandas functions\n## calendar files\nCAL_DTYPES={\"event_name_1\": \"category\", \"event_name_2\": \"category\", \"event_type_1\": \"category\", \n        \"event_type_2\": \"category\", \"weekday\": \"category\", 'wm_yr_wk': 'int16', \"wday\": \"int16\",\n        \"month\": \"int16\", \"year\": \"int16\", \"snap_CA\": \"float32\", 'snap_TX': 'float32', 'snap_WI': 'float32' }\ncal_data = pd.read_csv(INPUT_DIR2+'calendar.csv',dtype=CAL_DTYPES)\ncal_data.info()\n\n\n## Merge calendar partly, other generate using pandas datetime functions\nicols = ['date',\n         'd',\n         'event_name_1',\n         'event_type_1',\n         'event_name_2',\n         'event_type_2',\n         'snap_CA',\n         'snap_TX',\n         'snap_WI']\ngrid_df = grid_df.merge(cal_data[icols], on=['d'], how='left')\n\n\n## only consider SNAP for the correct state\nenc = preprocessing.OneHotEncoder()\nstate = enc.fit_transform(grid_df[['state_id']]).toarray()\ngrid_df['snap'] = np.multiply(grid_df[['snap_CA', 'snap_TX', 'snap_WI']].values,state).sum(axis=1)\ngrid_df = grid_df.drop(['snap_CA', 'snap_TX', 'snap_WI'], axis=1)\ngrid_df = make_time_features(grid_df)","56522ad4":"grid_df.columns","7289ecdd":"# Now, wwe only need to consider lags features which would be predicted each and then used to predict the next one\n[col for col in features_columns if col not in grid_df.columns]","48ec6cdb":"for col in ['event_type_1', 'event_type_2']:\n    grid_df[col] = grid_df[col].cat.codes.astype(\"int16\")\n    grid_df[col] -= grid_df[col].min()\n    \n# change day type to int so that it is easily manipulated\ngrid_df['d'] = [int(day[2:]) for day in grid_df['d']]\ngrid_df['d'] = grid_df['d'].astype(np.int16)","37fdc5fb":"for col in [col for col in features_columns if col not in grid_df.columns]:\n    grid_df[col] = -1","a4fa72af":"def predict_for_store(test_data, train_cols,m_lgb, alpha):\n    '''\n    Input: test_data: the data in one store with 3049 items\n    Outout: test-data with predicted sales\n    '''\n    \n\n    date = datetime(2016,5, 23) \n    for i in range(0, 28):\n        day = date + timedelta(days=i)\n        print(i, day)\n\n        # LAGS of sell price for the item in each stores(30490)\n        LAGS_SPLIT = [7, 28]\n        lag_cols = [f\"sales_lag_{lag}\" for lag in LAGS_SPLIT ]\n        #lag_df = create_lags(grid_df[['id','d','sales']], LAGS_SPLIT, groupby=['id'])\n        for lag, lag_col in zip(LAGS_SPLIT, lag_cols):\n            test_data.loc[test_data.date == day, lag_col] = test_data.loc[test_data.date ==day-timedelta(days=lag), 'sales'].values   # 3049 items\n\n            # cannot use test_data.shift(lag) or  test_data.groupby('item_id').transform(lambda x: x.shift(7)) \n            # since there are 3049* n days 'sales' for 3049 items\n            # test_data = grid_df[grid_df.store_id == 'CA_1']\n            # day = datetime(2016,5, 23) \n            # test_data[['sales','item_id']].groupby('item_id').apply(lambda x: x.shift(7))# all the days\n\n        \n        # LAG_ROLLING_WIN_STATISTICS:just average\/mean here\n        for win in [7,14,28]:\n            for lag in [0,7,14]:\n                df_window = test_data[(test_data.date <= day-timedelta(days=lag)) & (test_data.date > day-timedelta(days=lag+win))]\n                df_window_grouped = df_window.groupby(\"item_id\").agg({'sales':'mean'})\n                #df_window_grouped = df_window_grouped.reindex(test_data.loc[test_data.date==day,'item_id']) # I am not sure why it appears, but someone adds this\n                test_data.loc[test_data.date == day,f\"rolling_mean_tmp_{lag}_{win}\"] = df_window_grouped.sales.values  \n\n        \n        test = test_data.loc[test_data.date == day , train_cols]\n        test_data.loc[test_data.date == day, \"sales\"] = alpha*m_lgb.predict(test) # predict 3049 items in that day in that store\n        \n#     return test_data.loc[(test_data.date >= date) & (test_data.date < date+timedelta(days=28)), 'sales']\n    return test_data.loc[(test_data.date >= date), ['id', 'd', 'sales']]\n\n\n","5d60c66d":"alphas = [1.035, 1.03, 1.028,1.025,  1.023, 1.02,  1.025]\nalpha = 1.018\n# for alpha in alphas:\nVER = 2\nstores = ['CA_1','CA_2', 'CA_3','CA_4',  'TX_1','TX_2','TX_3','WI_1','WI_2','WI_3']\nsubmission_each_store = []\nfor store in stores:\n\n\n    test_data = grid_df[grid_df.store_id == store]\n    # load the model\n    model_path = 'lgb_model_'+store+'_v'+str(VER)+'.bin' \n    model_path = INPUT_DIR + model_path\n    model = pickle.load(open(model_path, 'rb'))\n    test_data = predict_for_store(test_data, features_columns,model, alpha) # 30490 items for 1 store in 28days\n\n\n\n    # test_data[\"F\"] = [f\"F{rank}\" for rank in test_data.groupby(\"id\")[\"id\"].cumcount()+1]\n    test_data['F'] = [f\"F{rank}\" for rank in test_data['d']-1941]\n    # after checking there is no null value, otherwise, fillna: test_sub.fillna(0., inplace = True)\n    # test_data[test_data.sales.isnull()] \n    test_data = test_data.set_index([\"id\", \"F\" ]).unstack()['sales'][ [f\"F{i}\" for i in range(1,29)]]\n\n    submission_each_store.append(test_data)\n\nsubmission_df_eval = pd.concat(submission_each_store)\n# join with right order in samle_submission\nsub = pd.read_csv(INPUT_DIR2+'sample_submission.csv')\nsubmission_df_eval = submission_df.loc[list(sub[30490:]['id']),:].reset_index()\n    ","4f162231":"# get ground true of validation\nsales_evaluation = pd.read_csv(INPUT_DIR2+'sales_train_evaluation.csv')\nd_cols = [f'd_{str(day)}' for day in range(1914, 1942)]\nsales_evaluation = sales_evaluation[['id'] + d_cols]\n# change column names to F_#\nnew_d_cols = [f'F{day}'  for day in range(1,29)]\nsales_evaluation.columns = ['id'] + new_d_cols\n# change id name\nsales_evaluation['id'] = sales_evaluation[\"id\"].str.replace(\"evaluation$\", \"validation\")\n\nsales_evaluation.to_csv('validation_ground_truth.csv', index=False)","0e6849e1":"# join result of validation and evaluation\npd.concat([sales_evaluation, submission_df_eval]).to_csv('submission_'+str(alpha)+'.csv', index=False)\n\n\n\n\n# Calculate by weight","11d6fffa":"# store = 'CA_1'\n# test_data = grid_df[grid_df.store_id == store ]\n# # load the model\n# model_path = 'lgb_model_'+store+'_v'+str(2)+'.bin' \n# model_path = INPUT_DIR + model_path\n# model = pickle.load(open(model_path, 'rb'))\n# test_data = predict_for_store(test_data, features_columns,model, 1.018) # 30490 items for 1 store in 28days\n\n\n\n# # test_data[\"F\"] = [f\"F{rank}\" for rank in test_data.groupby(\"id\")[\"id\"].cumcount()+1]\n# test_data['F'] = [f\"F{rank}\" for rank in test_data['d']-1941]\n# # after checking there is no null value, otherwise, fillna: test_sub.fillna(0., inplace = True)\n# # test_data[test_data.sales.isnull()] \n# # test_data = test_data.set_index([\"id\", \"F\" ]).unstack()['sales'][ [f\"F{i}\" for i in range(1,29)]].reset_index()","7329bbc3":"## Add price features","3cbc047b":"## Prediction(including construction of lags and rolling mean features\n\nThere are many strategies to fill the lag, mean which could be applied here. \n1. Fill the lag and mean for the data in each store and then predict sales using one particular model in that day (from day 1942)\n2. Fill the lag and mean for the data for all store and predict sales using all the models in that day(from day 1942)\n\nI use the first strategy.","c288ec6d":"This notebook is based on outputs of my previous notebooks for feature engineering and model training.\n* [M5 - Feature Engineering](https:\/\/www.kaggle.com\/sergioli212\/m5-all-feature-engineering-ready-to-use)\n* [M5 - Modelling using LightGBM](https:\/\/www.kaggle.com\/sergioli212\/m5-modelling-using-lightgbm)\n","cdec6cff":"## Construct null values for these features","85e936dd":"## Add time features"}}