{"cell_type":{"3929654a":"code","c7e368cd":"code","551af948":"code","0e03450d":"code","4750545d":"code","d50bfc91":"code","718c2e51":"code","011bf9ab":"code","870bd8ee":"code","888343aa":"code","60e5dba6":"code","08c6d91e":"code","e79f3e64":"code","f9a66102":"code","ea017a52":"code","edcd2d6e":"code","05e2a04b":"code","3d7e43c3":"code","b77220e0":"code","715f57d5":"code","1ce35f9c":"code","cb017a2c":"code","facdd7e4":"code","bad590a0":"code","91e89271":"code","ffa48fd3":"code","4f2d1b02":"code","1b3f05b4":"code","ca597867":"code","01a61157":"code","4abcdc42":"code","902d09e3":"code","db611f02":"code","d37e2cb0":"code","c6bf4379":"code","d2dc1fa1":"code","2ae66ca2":"code","64c8e5e4":"code","032b216d":"code","ef733524":"code","fc993159":"code","578aca66":"code","6b237b0b":"code","9935435a":"code","657a79d9":"code","9b390fbd":"code","abbbdc98":"code","be28fc2e":"code","31a1033e":"code","de4cd741":"code","d298bf6c":"code","7a0efb42":"code","5a40bacf":"code","e8f2e866":"code","3cd0b383":"code","c57c469f":"code","fe124e29":"code","14c6308f":"code","1d177956":"code","367ec4f2":"code","bc95da91":"code","5c42fbd6":"code","1267d67b":"code","367c3f2e":"code","0ed83f42":"code","150070d3":"code","74872a3b":"code","c5e3583c":"code","86568c86":"code","dd8ed480":"code","59502f00":"code","40aa5612":"code","0cb04cbb":"markdown","cbb02ef5":"markdown","c64046ff":"markdown","8abf25f8":"markdown","c960c15c":"markdown","f3c203d6":"markdown","5aa8a282":"markdown","87df58a0":"markdown","875c98ff":"markdown","f8e29e77":"markdown","ebbda49c":"markdown","a8f66766":"markdown","e950ae21":"markdown","e44db96a":"markdown","59b96be7":"markdown","594b3bcd":"markdown","a44f19cb":"markdown","6a4aaf2f":"markdown","a089a6f6":"markdown","7f756b44":"markdown","25942ecb":"markdown","763e776f":"markdown","c8512768":"markdown","a14a8b65":"markdown","63c41e46":"markdown","cd2a197c":"markdown","3f473f78":"markdown","5670a15b":"markdown","7c64391d":"markdown","23a21044":"markdown","58a07fdb":"markdown","4c5e5de5":"markdown","1c30c5a3":"markdown"},"source":{"3929654a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c7e368cd":"import seaborn as sns","551af948":"df=pd.read_csv(\"..\/input\/regression-data\/fake_reg.csv\")\ndf.head()","0e03450d":"sns.pairplot(df)","4750545d":"X=df[[\"feature1\",\"feature2\"]]\nX","d50bfc91":"y=df[\"price\"] #This is our target value to predict\ny","718c2e51":"from sklearn.model_selection import train_test_split","011bf9ab":"X_train, X_test, y_train, y_test=train_test_split(X,y, test_size=0.3)\n#here we divide our data as %70 train set and %30 test set","870bd8ee":"from sklearn.linear_model import LinearRegression\n#First we will use linear regression and examine its performance","888343aa":"model1=LinearRegression()","60e5dba6":"model1.fit(X_train,y_train)\n# Here we make our model fit the train data","08c6d91e":"predictions1=model1.predict(X_test)","e79f3e64":"predictions1 #This is predictions of the linear regression algorithm","f9a66102":"from sklearn import metrics","ea017a52":"metrics.mean_absolute_error(y_test,predictions1)","edcd2d6e":"metrics.mean_squared_error(y_test,predictions1)","05e2a04b":"np.sqrt(metrics.mean_squared_error(y_test,predictions1))","3d7e43c3":"sns.distplot((y_test-predictions1))","b77220e0":"sns.set_style(\"whitegrid\")","715f57d5":"sns.scatterplot(y_test,predictions1)","1ce35f9c":"import matplotlib.pyplot as plt","cb017a2c":"plt.scatter(y_test,predictions1)","facdd7e4":"from sklearn.tree import DecisionTreeRegressor","bad590a0":"model2=DecisionTreeRegressor()","91e89271":"model2.fit(X_train, y_train)","ffa48fd3":"predictions2=model2.predict(X_test)\npredictions2","4f2d1b02":"print(\"Predictions of Decision Tree Regression:\",metrics.mean_absolute_error(y_test,predictions2))\n\nprint(\"Predictions of Linear Regression:\",metrics.mean_absolute_error(y_test,predictions1))","1b3f05b4":"print(\"Predictions of Decision Tree Regression:\",metrics.mean_squared_error(y_test,predictions2))\n\nprint(\"Predictions of Linear Regression:\",metrics.mean_squared_error(y_test,predictions1))","ca597867":"sns.scatterplot(y_test,predictions2)\nsns.scatterplot(y_test,predictions1)","01a61157":"#Before using deep learning algorithms we have to transform our data into numpy arrays\nX_deep=X.values\nX_deep","4abcdc42":"y_deep=y.values\ny_deep","902d09e3":"X_deep_train, X_deep_test, y_deep_train, y_deep_test=train_test_split(X_deep, y_deep,test_size=0.3)\nX_deep_train","db611f02":"X_deep_test.shape","d37e2cb0":"X_deep_train.shape","c6bf4379":"from sklearn.preprocessing import MinMaxScaler","d2dc1fa1":"scaler=MinMaxScaler()","2ae66ca2":"scaler.fit(X_deep_train)","64c8e5e4":"X_deep_train= scaler.transform(X_deep_train)\nX_deep_test=scaler.transform(X_deep_test)","032b216d":"from tensorflow.keras.models import Sequential\n# this will create sequential layers\n","ef733524":"from tensorflow.keras.layers import Dense\n#this will add dense layers\n#In dense layers, every neuron is connected with the other neurons","fc993159":"model= Sequential([Dense(4,activation=\"relu\"), Dense(4,activation=\"relu\"),Dense(4,activation=\"relu\"), Dense(1)])\n#The last sould be assigned with respect to how many ourput we want to have\n#In our example, because we want to predict hosue prices, we will use just 1 layer as the last layer","578aca66":"#There is another way to create the same model as follows:\nmodel2=Sequential()\nmodel2.add(Dense(4, activation=\"sigmoid\"))\nmodel2.add(Dense(4, activation=\"softplus\"))\nmodel2.add(Dense(1)) ","6b237b0b":"model.compile(optimizer=\"rmsprop\", loss=\"mse\")\n#here we choose rmsprop as our optimizer and Mean Suared Error as our loss function","9935435a":"model2.compile(optimizer=\"rmsprop\", loss=\"mse\")","657a79d9":"model.fit(x=X_deep_train, y=y_deep_train,epochs=300)","9b390fbd":"model.history.history \n#This function will show separately the history of the loss values","abbbdc98":"loss_df=pd.DataFrame(model.history.history)\nloss_df.head() #we can create a pandas data frame in order to visualize the history of the loss function","be28fc2e":"loss_df.plot()","31a1033e":"model2.fit(x=X_deep_train, y=y_deep_train, epochs=250)","de4cd741":"model2.history.history","d298bf6c":"loss_df2=pd.DataFrame(model2.history.history)\nloss_df2.head()","7a0efb42":"loss_df2.plot()","5a40bacf":"model.evaluate(X_deep_train, y_deep_train)","e8f2e866":"model.evaluate(X_deep_test, y_deep_test)","3cd0b383":"model2.evaluate(X_deep_train, y_deep_train)","c57c469f":"model2.evaluate(X_deep_test, y_deep_test)","fe124e29":"predictions_deep1=model.predict(X_deep_test)\npredictions_deep1","14c6308f":"predictions_deep1_df=pd.DataFrame(predictions_deep1, columns=[\"Model Predictiions\"])\npredictions_deep1_df.head()","1d177956":"y_test_df=pd.DataFrame(y_deep_test, columns=[\"Real Prices\"])\ny_test_df.head()","367ec4f2":"comparison_df=pd.concat([predictions_deep1_df, y_test_df], axis=1)\ncomparison_df.head()\n","bc95da91":"print(\"Predictions of Decision Tree Regression:\",metrics.mean_absolute_error(y_test,predictions2))\n\nprint(\"Predictions of Linear Regression:\",metrics.mean_absolute_error(y_test,predictions1))\nprint(\"Predictions of Deep Learning Model:\",metrics.mean_absolute_error(y_deep_test,predictions_deep1))\n","5c42fbd6":"df.describe()","1267d67b":"print(\"Predictions of Decision Tree Regression:\",metrics.mean_squared_error(y_test,predictions2))\n\nprint(\"Predictions of Linear Regression:\",metrics.mean_squared_error(y_test,predictions1))\nprint(\"Predictions of Deep Learning Model:\",metrics.mean_squared_error(y_deep_test,predictions_deep1))\nprint(\"The Evaluation Mean Squared Error of Our Deep Learning Model :\",model.evaluate(X_deep_test, y_deep_test))","367c3f2e":"predictions_deep_1_dimesional=predictions_deep1.reshape(300,)\npredictions_deep_1_dimesional.shape","0ed83f42":"sns.scatterplot(y_test,predictions1, legend=\"full\")\nsns.scatterplot(y_deep_test,predictions_deep_1_dimesional, legend=\"full\")\n#Here is the comparison of the predictions of Linear regression and Deep Learning Model, \n#it is very similar but deep learning performs better with larger datasets","150070d3":"new_features=[[998,1000]]\nnew_features=scaler.transform(new_features)#we have to tranform it because our model used scaled data\nnew_features\n","74872a3b":"model.predict(new_features)\n#Accoding to these feature, the prices should be 420.54 according to our model","c5e3583c":"new_features2=[[1000,998]]\nnew_features2=scaler.transform(new_features2)\nmodel.predict(new_features2)","86568c86":"from tensorflow.keras.models import load_model","dd8ed480":"model.save(\"my_price_pridecition_model.h5\")","59502f00":"my_model_back=load_model(\"my_price_pridecition_model.h5\")","40aa5612":"my_model_back.predict(new_features)","0cb04cbb":"Now we will evaluate the performance of the linear regression","cbb02ef5":"Above we can see the compared results of our model's predictions and real prices ","c64046ff":"Linear regression algorithm performs better than decision tree regression algorithm","8abf25f8":"2.2. Available optimizers:\n\nThe loss function is the guide to the terrain, telling the optimizer when it\u2019s moving in the right or wrong direction.\n\n2.2.1.SGD:Stochastic gradient descent (often abbreviated SGD) is an iterative method for optimizing an objective function with suitable smoothness properties (e.g. differentiable or subdifferentiable). It can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data). \n\n\n2.2.2.RMSprop:Optimizer that implements the RMSprop algorithm.\n\nThe gist of RMSprop is to:\n\nMaintain a moving (discounted) average of the square of gradients\nDivide the gradient by the root of this average\n\n2.2.3.Adam:Optimizer that implements the Adam algorithm.\n\nAdam optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments.\n\n\n2.2.4.Adadelta: Optimizer that implements the Adadelta algorithm.\n\nAdadelta optimization is a stochastic gradient descent method that is based on adaptive learning rate per dimension to address two drawbacks:\n\nThe continual decay of learning rates throughout training\nThe need for a manually selected global learning rate\n\n2.2.5.Adagrad:Optimizer that implements the Adagrad algorithm.\n\nAdagrad is an optimizer with parameter-specific learning rates, which are adapted relative to how frequently a parameter gets updated during training. The more updates a parameter receives, the smaller the updates.\n\n\n2.2.6.Adamax:Optimizer that implements the Adamax algorithm.\n\nIt is a variant of Adam based on the infinity norm. Default parameters follow those provided in the paper. Adamax is sometimes superior to adam, specially in models with embeddings.\n\n\nNadam:Optimizer that implements the NAdam algorithm.\n\nMuch like Adam is essentially RMSprop with momentum, Nadam is Adam with Nesterov momentum.\n\nFtrl:Optimizer that implements the FTRL algorithm.\n","c960c15c":"When we have new data and try to find which we should determine for these data, we can use model.predict() method for new prices as follows:","f3c203d6":"From the plots above, we understand that all the data is normally distributed and there is strong positive correlation between feature 2 and price","5aa8a282":"evaluate(x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, return_dict=False) method of tensorflow.python.keras.engine.sequential.Sequential instance\n    Returns the loss value & metrics values for the model in test mode.","87df58a0":"# 1. Creating Layers:","875c98ff":"# 3. Fitting Model and Training the Data:","f8e29e77":"#Accoding to these feature, the prices should be 323.56 according to our model\n\nWe understand that feature 2 affect much more than feature 1 because I got lower price when exchanged the same values in the first new data.","ebbda49c":"Here I reuse my model and get actual the same prediction for new features","a8f66766":"Now we will use deep learning algortihm and will compare results with the previous one","e950ae21":"This means that our model have mean squared error as 26.748348236083984","e44db96a":"save(filepath, overwrite=True, include_optimizer=True, save_format=None, signatures=None, options=None) method of tensorflow.python.keras.engine.sequential.Sequential instance\n    Saves the model to Tensorflow SavedModel or a single HDF5 file.\n    \n    The savefile includes:\n    \n    - The model architecture, allowing to re-instantiate the model.\n    - The model weights.\n    - The state of the optimizer, allowing to resume training\n        exactly where you left off.\n    \n    This allows you to save the entirety of the state of a model\n    in a single file.","59b96be7":"2.3. Choosing Right Loss Function\n\nLoss function is a mathematical way of measuring how wrong your predictions are.\n\nKeep in mind what kind of problem you are trying to solve:\n\n# For a multi-class classification problem\nmodel.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# For a binary classification problem\nmodel.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# For a mean squared error regression problem\nmodel.compile(optimizer='rmsprop',\n              loss='mse')","594b3bcd":"As we can see from the plots above, the predictions of linear regression is very good","a44f19cb":"Here we load our saved model and reuse it","6a4aaf2f":"# 4. Evaluation of Our Model:","a089a6f6":"Because the weights are very important in neural networks, we need to scale our train data in order to standardize it, but we can not standardize our test set because it will be a cheating","7f756b44":"we can save our model and download it later if we aim to use it in the future","25942ecb":"This plot helps to understand better how the values in the loss function go down","763e776f":"# 2. Choosing Right Optimizer and Cost Function:","c8512768":"In this data set we will try to predict price according to two feature that are not specified\n\nSo it is a supervised learning problem","a14a8b65":"# 1. Exploratory Data Analysis:","63c41e46":"compile(optimizer='rmsprop', loss=None, metrics=None, loss_weights=None, weighted_metrics=None, run_eagerly=None, **kwargs) method of tensorflow.python.keras.engine.sequential.Sequential instance\n    Configures the model for training.\n    \n    Arguments:\n        optimizer:this determines how we want to perform the gradient descent like adam optimizer\n        loss= represents cost function we want to use\n        ","cd2a197c":"The error above show that deep learning model has a error value around 4 dolars from the real prices like Linear regression, but when it comes to Decision tree model it is about 9.5 dolar off from the real prices. The mean of the prices is 498 and our model's prediction is less then %1 and it predicts very good.","3f473f78":"# 5. Saving Our Model for Future Use","5670a15b":"2.1. Choosing an optimizer:\nKeep in mind what kind of problem we are trying to solve:\n\nFor a multi-class classification problem:\n\nmodel.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n              \n              \nFor a binary classification problem:\n\nmodel.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n              \n              \n\nFor a mean squared error regression problem:\n\nmodel.compile(optimizer='rmsprop',\n              loss='mse')\n","7c64391d":"MinMaxScaler(feature_range=(0, 1), *, copy=True)\n |  \n |  Transform features by scaling each feature to a given range.\n |  \n |  This estimator scales and translates each feature individually such\n |  that it is in the given range on the training set, e.g. between\n |  zero and one.","23a21044":"Now we will visualize to understand our model's performance","58a07fdb":"First we will use normal machine learning algortihms before deep learning in order to compare results","4c5e5de5":"Now we will predict the y_test and compare them with the real values","1c30c5a3":"tf.keras.layers.Dense(\n    units,\n    activation=None,\n    use_bias=True,\n    kernel_initializer=\"glorot_uniform\",\n    bias_initializer=\"zeros\",\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n\n\nunits represents how many neurons we want to have\n\nactivation represents which activation function we want to use\n"}}