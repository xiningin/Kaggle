{"cell_type":{"d92e5888":"code","b0e48e0a":"code","07b15a23":"code","19b7f425":"code","8353d4b6":"code","740c7b60":"code","887c2c76":"code","e78e4a67":"code","7ebe805b":"code","e16fca9e":"code","f3631983":"code","6c26bb58":"code","58d62258":"code","69346608":"code","538eb90b":"code","0cc61a69":"code","8e0cc90a":"code","a7d5b7de":"code","e0d204c5":"code","8b4490e9":"code","187d6450":"code","360bc908":"code","097d93d1":"code","f5fe585f":"code","22aa69d1":"code","b84eb356":"code","72d2b928":"code","bfefcd90":"code","794d10b8":"code","3f7fe753":"code","bbe6200e":"code","c5e7aa7a":"code","73c980f6":"code","9e025dc2":"code","8dd22a12":"code","7f7814e2":"code","0899aca1":"code","45964080":"code","cb5ed581":"code","f57eece3":"code","ad428482":"code","74795fbc":"code","82dd7334":"code","8cc42d87":"code","13de3379":"code","9fd5bf9c":"code","1c45f0ee":"code","45e07045":"code","b6943320":"code","9d8781d3":"code","4dac2b6c":"code","f1adc3c5":"code","08146829":"code","94e69ceb":"code","415caf32":"code","ca9c8820":"code","d537d32f":"code","15a06dfd":"code","d6679a2d":"code","27dc6d68":"code","45b624f1":"code","0d657c5e":"code","7d76050a":"code","af5fbce8":"code","86486aa2":"code","d127d2ff":"code","9471f619":"code","7f0ad0f9":"code","fc9e7322":"code","72dbfee5":"code","e5f2351b":"code","4e4cc2c4":"code","d6c9efc6":"code","95a6328f":"code","4ec58c4c":"code","b167fee5":"code","2c34867e":"code","eae46605":"code","39816aec":"code","ed5773c7":"markdown","05d62c70":"markdown","bc58f0b5":"markdown","a519bcd9":"markdown","e1eefb46":"markdown","e7690336":"markdown","a6b17e9e":"markdown","85636a57":"markdown","4fd99d7b":"markdown","27c82299":"markdown","74facc81":"markdown","128ae7b2":"markdown","a9a93e33":"markdown","368585fd":"markdown","7659bcba":"markdown","3542e9cc":"markdown","d607079b":"markdown","5700102b":"markdown","9bf59ac7":"markdown"},"source":{"d92e5888":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b0e48e0a":"import numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import roc_auc_score, roc_curve, accuracy_score\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn import preprocessing\nfrom scipy import stats\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","07b15a23":"test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')","19b7f425":"#Showing first few rows\ntrain.head()","8353d4b6":"test.head()","740c7b60":"test.describe()","887c2c76":"\ntrain.describe()","e78e4a67":"test.dtypes","7ebe805b":"train.dtypes","e16fca9e":"train.info()","f3631983":"#summary of SalesPrice\ntrain['SalePrice'].describe()","6c26bb58":"sns.distplot(train['SalePrice']);","58d62258":"corr = train.corr()\ncorr.sort_values([\"SalePrice\"], ascending = False, inplace = True)\nprint(corr.SalePrice)","69346608":"# We will see if there's a relationship between SalesPrice and OverallQual: overall material and finish of the house.\n# 1-10 where 1=Very Poor and 10=Very Excellent\nsns.barplot(train.OverallQual,train.SalePrice)","538eb90b":"# Scatter plot to see the relationship between numerical values\n\n#lotarea = px.scatter(house, x='LotArea', y='SalePrice')\n#lotarea.update_layout(title='Sales Price Vs Area',xaxis_title=\"Area\",yaxis_title=\"Price\")\n#lotarea.show()\n\n\nplt.scatter(x =train.LotArea,y = train.SalePrice,c = 'black')\nplt.title('Sales Price Vs Area')\nplt.xlabel('LotArea')\nplt.ylabel('SalePrice')\nplt.show()","0cc61a69":"#scatter plot \ndata = pd.concat([train['SalePrice'], train['GrLivArea']], axis=1)\ndata.plot.scatter(x='GrLivArea', y='SalePrice');","8e0cc90a":"data = pd.concat([train['SalePrice'], train['TotalBsmtSF']], axis=1)\ndata.plot.scatter(x='TotalBsmtSF', y='SalePrice');","a7d5b7de":"#scatter plot \ndata = pd.concat([train['SalePrice'], train['YearBuilt']], axis=1)\ndata.plot.scatter(x='YearBuilt', y='SalePrice');","e0d204c5":"# Distribution of SalesPrice \nprint(train['SalePrice'].describe())\nsns.distplot(train['SalePrice'], color='g', bins=100, hist_kws={'alpha': 0.5});","8b4490e9":"#Some analysis on target variable\n\nplt.subplots(figsize=(12,9))\nsns.distplot(train['SalePrice'], fit=stats.norm)\n\n# Get the fitted parameters used by the function\n\n(mu, sigma) = stats.norm.fit(train['SalePrice'])\n\n# plot with the distribution\n\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')\nplt.ylabel('Frequency')\n\n#Probablity plot\n\nfig = plt.figure()\nstats.probplot(train['SalePrice'], plot=plt)\nplt.show()\n","187d6450":"sns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(train[cols], size = 2.5)\nplt.show();","360bc908":"# We will use log for our target variable to make more normal distribution\n#we use log function which is in numpy\ntrain['SalePrice'] = np.log1p(train['SalePrice'])\n\n#Check again for more normal distribution\n\nplt.subplots(figsize=(12,9))\nsns.distplot(train['SalePrice'], fit=stats.norm)\n\n# Get the fitted parameters used by the function\n\n(mu, sigma) = stats.norm.fit(train['SalePrice'])\n\n# plot with the distribution\n\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')\nplt.ylabel('Frequency')\n\n#Probablity plot\n\nfig = plt.figure()\nstats.probplot(train['SalePrice'], plot=plt)\nplt.show()","097d93d1":"# For train\ntrain_null = train.isnull().sum()\ntrain_null = train_null[train_null>0]\ntrain_null.sort_values(ascending=False)","f5fe585f":"# For test\ntest_null = test.isnull().sum()\ntest_null = test_null[test_null>0]\ntest_null.sort_values(ascending=False)","22aa69d1":"categorical_features = train.select_dtypes(include=['object']).columns\ncategorical_features","b84eb356":"numerical_features = train.select_dtypes(exclude = [\"object\"]).columns\nnumerical_features","72d2b928":"train.columns[train.isnull().any()]\n","bfefcd90":"#plot of missing value attributes\nplt.figure(figsize=(12, 6))\nsns.heatmap(train.isnull())\nplt.show()","794d10b8":"#missing value counts in each of these columns\nIsnull = train.isnull().sum()\/len(train)*100\nIsnull = Isnull[Isnull>0]\nIsnull.sort_values(inplace=True, ascending=False)\nIsnull","3f7fe753":"Isnull = Isnull.to_frame()\n\n","bbe6200e":"Isnull.columns = ['count']\n","c5e7aa7a":"Isnull.index.names = ['Name']\n","73c980f6":"Isnull['Name'] = Isnull.index","9e025dc2":"#we will not plot Missing values\nplt.figure(figsize=(13, 5))\nsns.set(style='whitegrid')\nsns.barplot(x='Name', y='count', data=Isnull)\nplt.xticks(rotation = 90)\nplt.show()","8dd22a12":"train_corr = train.select_dtypes(include=[np.number])\n","7f7814e2":"train_corr.shape\n","0899aca1":"del train_corr['Id']","45964080":"#Coralation heatmap plot\ncorr = train_corr.corr()\nplt.subplots(figsize=(20,9))\nsns.heatmap(corr, annot=True)","cb5ed581":"top_feature = corr.index[abs(corr['SalePrice']>0.5)]\nplt.subplots(figsize=(12, 8))\ntop_corr = train[top_feature].corr()\nsns.heatmap(top_corr, annot=True)\nplt.show()","f57eece3":"#unique value of OverallQual\ntrain.OverallQual.unique()","ad428482":"sns.barplot(train.OverallQual, train.SalePrice)\n","74795fbc":"#boxplot\nplt.figure(figsize=(18, 8))\nsns.boxplot(x=train.OverallQual, y=train.SalePrice)","82dd7334":"col = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'TotRmsAbvGrd', 'YearBuilt']\nsns.set(style='ticks')\nsns.pairplot(train[col], size=3, kind='reg')","8cc42d87":"print(\"Most important features relative to target\")\ncorr = train.corr()\ncorr.sort_values(['SalePrice'], ascending=False, inplace=True)\ncorr.SalePrice","13de3379":"# PoolQC has missing value ratio is 99%+. So, there is fill by None\ntrain['PoolQC'] = train['PoolQC'].fillna('None')","9fd5bf9c":"#We saw that 50% missing values attributes have been fill by None\ntrain['MiscFeature'] = train['MiscFeature'].fillna('None')\ntrain['Alley'] = train['Alley'].fillna('None')\ntrain['Fence'] = train['Fence'].fillna('None')\ntrain['FireplaceQu'] = train['FireplaceQu'].fillna('None')","1c45f0ee":"\ntrain['LotFrontage'] = train.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))","45e07045":"#GarageType, GarageFinish, GarageQual and GarageCond these are replacing with None\nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    train[col] = train[col].fillna('None')","b6943320":"#GarageYrBlt, GarageArea and GarageCars these are replacing with zero\nfor col in ['GarageYrBlt', 'GarageArea', 'GarageCars']:\n    train[col] = train[col].fillna(int(0))","9d8781d3":"#BsmtFinType2, BsmtExposure, BsmtFinType1, BsmtCond, BsmtQual these are replacing with None\nfor col in ('BsmtFinType2', 'BsmtExposure', 'BsmtFinType1', 'BsmtCond', 'BsmtQual'):\n    train[col] = train[col].fillna('None')","4dac2b6c":"#MasVnrArea : replace with zero\ntrain['MasVnrArea'] = train['MasVnrArea'].fillna(int(0))","f1adc3c5":"#MasVnrType : replace with None\ntrain['MasVnrType'] = train['MasVnrType'].fillna('None')","08146829":"#There is put mode value \ntrain['Electrical'] = train['Electrical'].fillna(train['Electrical']).mode()[0]","94e69ceb":"#There is no need of Utilities\ntrain = train.drop(['Utilities'], axis=1)","415caf32":"#Checking there is any null value or not\nplt.figure(figsize=(10, 5))\nsns.heatmap(train.isnull())","ca9c8820":"cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold', 'MSZoning', 'LandContour', 'LotConfig', 'Neighborhood',\n        'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n        'Exterior2nd', 'MasVnrType', 'MasVnrArea', 'Foundation', 'GarageType', 'MiscFeature', \n        'SaleType', 'SaleCondition', 'Electrical', 'Heating')","d537d32f":"from sklearn.preprocessing import LabelEncoder\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(train[c].values)) \n    train[c] = lbl.transform(list(train[c].values))","15a06dfd":"train_y = train.SalePrice\npred_cols = ['LotArea', 'OverallQual', 'YearBuilt', 'GrLivArea']","d6679a2d":"#prediction data\nfrom sklearn import linear_model\n\ntrain_x = train[pred_cols]\nmodel = linear_model.LinearRegression()\n#model =  LinearRegression()\n\nmodel.fit(train_x, train_y)","27dc6d68":"test_x = test[pred_cols]\npred_prices = model.predict(test_x)\nprint(pred_prices)","45b624f1":"#save file\nayesha_submission_final = pd.DataFrame({'Id': test.Id, 'SalePrice' : pred_prices})\nayesha_submission_final.to_csv('submission.csv', index=False)","0d657c5e":"#Linear Regression part 2","7d76050a":"y = train['SalePrice']","af5fbce8":"#Delete the saleprice\ndel train['SalePrice']","86486aa2":"#Take their values in X and y\nX = train.values\ny = y.values","d127d2ff":"# Split data into train and test formate\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)","9471f619":"#Train the model\nfrom sklearn import linear_model\nmodel = linear_model.LinearRegression()","7f0ad0f9":"#Fit the model\nmodel.fit(X_train, y_train)","fc9e7322":"#Prediction\nprint(\"Predict value \" + str(model.predict([X_test[142]])))\nprint(\"Real value \" + str(y_test[142]))","72dbfee5":"pred = model.predict(X)","e5f2351b":"#save file\nayesha_submission_final2 = pd.DataFrame({'Id': train.Id, 'SalePrice' : pred})\nayesha_submission_final2.to_csv('submission1.csv', index=False)","4e4cc2c4":"#Score\/Accuracy\nprint(\"Accuracy --> \", model.score(X_test, y_test)*100)","d6c9efc6":"# RandomForestRegression\n#Train the model\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor(n_estimators=1000)\n","95a6328f":"#Fit\nmodel.fit(X_train, y_train)","4ec58c4c":"#Score\/Accuracy\nprint(\"Accuracy --> \", model.score(X_test, y_test)*100)","b167fee5":"# GradientBoostingRegressor","2c34867e":"#Train the model\nfrom sklearn.ensemble import GradientBoostingRegressor\nGBR = GradientBoostingRegressor(n_estimators=100, max_depth=4)","eae46605":"#Fit\nGBR.fit(X_train, y_train)","39816aec":"print(\"Accuracy --> \", GBR.score(X_test, y_test)*100)\n","ed5773c7":"OverallQual is highly correlated with target feature SalePrice 0.79 can you see. we'll see how it effected the saleprice in below graph.\n\n","05d62c70":"Now we only have 38 numerical attribute. We will also delete the Id because that is not needed for corralation plot","bc58f0b5":"As we have seen before, our target varibale is right skewed. We need to tranform this variable to make it normal distribution.\n","a519bcd9":"These graphs above are useful to see what variables will play an important role in our predictions.\n\n","e1eefb46":"From the descriptive statistics summary we can see that the minimum price is more than Zero and shows a decent amount which means we do not have to clean the data.\n\n","e7690336":"After looking at the graph we can see that the variable SalesPrice is skewed right and there are some outliers which are above ~500,000. We will get rid of them to get normal ditribution of the independent variable (SalePrice).\n\n","a6b17e9e":"Here you can see how each variable is correlated with SalePrice.","85636a57":"It is skewed to the right and show peakedness. There are outliers in the data and they lie above ~500,000. We will get ride of these to get a normal ditriibution for this independent variable ('SalePrice').\n\nWe will se the correlatio and see which features relative to the target","4fd99d7b":"We will group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\n","27c82299":"We will now check the null values as they are causing issue in the overall result\n\n","74facc81":"We see a linear relationship between SalePrice and GrLivArea. We can also see outlier in the image\n\n","128ae7b2":"Data Prediction","a9a93e33":"We will now differetiate numerical variables and categorical variables. Excluding the target.\n\n","368585fd":"Here we can see that from this heatmap that OverallQual is highly correlated with target feature of saleprice by 82%","7659bcba":"This shows more of a strong linear (ecponential) relationship between SalePrice and TotalBsmtSF\n\n","3542e9cc":"We can see from the heatmap that Top 50% Corralation train attributes with sale-price\n","d607079b":"We will now convert it into dataframe","5700102b":"We can see that there are 38 numerical attribute from 81 attributes. We will separate variable into new dataframe from original dataframe which has only numerical values\n","9bf59ac7":"Linear Regression"}}