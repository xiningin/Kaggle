{"cell_type":{"190e5886":"code","f5c4f4af":"code","9142ffe9":"code","cf528855":"code","8d2d2976":"code","80aadba4":"code","a745596a":"code","c309de65":"code","e4d5e00b":"code","868a3f87":"code","57d58177":"code","b0301543":"code","e2f61f60":"code","539a7781":"code","e9af56a1":"code","32ffe6c2":"code","8408adb4":"code","ae939e9a":"code","de78fe59":"code","64f04c62":"code","460273c9":"code","43f49f2b":"code","4abf3c84":"code","12e7830e":"code","8f3b11f2":"code","0902f340":"code","31a55610":"code","d0d67789":"code","6e8d8ed8":"code","029f4f9a":"code","6deddf73":"code","11484a31":"code","a02b5545":"code","622fff75":"code","82b1f1f1":"markdown","b5cd0174":"markdown","78fade46":"markdown","803fcf0a":"markdown","e667f7fb":"markdown"},"source":{"190e5886":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f5c4f4af":"import pandas as pd\nimport numpy as np\nimport multiprocessing\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nimport catboost \nimport gc\nfrom time import time\nimport datetime\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import log_loss\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nwarnings.simplefilter('ignore')\nsns.set()\n%matplotlib inline\n","9142ffe9":"test_features = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')\ntrain_features = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\ntrain_targets_scored = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_nonscored.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')\n","cf528855":"train_features.head()","8d2d2976":"test_features.head()","80aadba4":"test_features.shape","a745596a":"train_features.shape","c309de65":"train_features.dtypes","e4d5e00b":"train_features.info()","868a3f87":"print(\"# of common columns : \",len(set(sample_submission.columns.values.tolist()) & set(train_targets_scored.columns.values.tolist())))\nprint(\"# of columns in sample submission : \", len(sample_submission.columns.values.tolist()))","57d58177":"print(train_features.shape)\nprint(train_targets_scored.shape)","b0301543":"train_features.describe()","e2f61f60":"test_features.describe()","539a7781":"train_targets_scored.describe()","e9af56a1":"train = pd.merge(train_features, train_targets_scored, on='sig_id')","32ffe6c2":"train.shape,train_features.shape,train_targets_scored.shape","8408adb4":"target_columns = train_targets_scored.columns","ae939e9a":"train_targets_scored[\\\n                     [col for col in train_targets_scored.columns if col not in ['sig_id']]\\\n                    ].sum(axis=1)\\\n                    .unique()","de78fe59":"X_train = []\nX_train_columns = train.columns\n\nfor v in train.values:\n    info = v[:876]\n    binary = v[876:]\n    index = [k for k, i in enumerate(binary) if i==1]\n    \n    for i in index:\n        for k in range(len(binary)):\n            if k==i:\n#                 binary_transformed = list(copy.copy(binary))\n#                 binary_transformed[i] = 0\n                X_train.append(list(info) + [X_train_columns[876+k]])\n\nX_train = pd.DataFrame(X_train, columns=train_features.columns.tolist() + ['pred'])\n","64f04c62":"X_train.shape","460273c9":"X_train.head()","43f49f2b":"X_test = test_features\ny_train = X_train[['pred']]","4abf3c84":"y_train.value_counts()","12e7830e":"y_train.head()","8f3b11f2":"len(y_train['pred'].unique().tolist())","0902f340":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\n\nX_test['pred'] = -1\ndata = X_train.append(X_test)\n\nfor col in ['cp_type','cp_time','cp_dose']:\n    data.loc[:, col] = le.fit_transform(data[col])\n    \nX_train = data[:X_train.shape[0]]\nX_test = data[-X_test.shape[0]:]","31a55610":"mapping = {v:k for k,v in enumerate(sample_submission.iloc[:,1:].columns.tolist())}\n# inverse_mapping = {k:v for k,v in enumerate(sample_submission.iloc[:,1:].columns.tolist())}","d0d67789":"y_train['pred'] = y_train['pred'].map(mapping)\ny_train","6e8d8ed8":"from catboost import CatBoostClassifier","029f4f9a":"cat_features = ['cp_type','cp_time','cp_dose']\n","6deddf73":"model = CatBoostClassifier(task_type='GPU',iterations = 50000,learning_rate = 0.1)\n\nmodel.fit(X_train.drop(columns=['sig_id','pred']),\n          y_train,\n          verbose=5000,\n          cat_features=cat_features)","11484a31":"prediction = model.predict_proba(X_test.drop(columns=['sig_id'], axis=1))","a02b5545":"sample_submission.iloc[:, 1:] = prediction","622fff75":"sample_submission.to_csv('submission.csv',index=False)","82b1f1f1":"### Loading Library**","b5cd0174":"### Unique value","78fade46":"### 1. Introduction to CatBoost \n\nTable of Contents\n\nCatBoost documentation says that-\n\n\"CatBoost is a high-performance open source library for gradient boosting on decision trees.\"\"\n\nSo, CatBoost is an algorithm for gradient boosting on decision trees.\nIt is a readymade classifier in scikit-learn\u2019s conventions terms that would deal with categorical features automatically.\nIt can easily integrate with deep learning frameworks like Google\u2019s TensorFlow and Apple\u2019s Core ML.\nIt can work with diverse data types to help solve a wide range of problems (described later) that businesses face today.\nIt is developed by Yandex researchers and engineers, and is used for search, recommendation systems, personal assistant, self-driving cars, weather prediction and many other tasks.\nAlso, it provides best-in-class accuracy.\nIt is especially powerful in two ways:\n1.It yields state-of-the-art results without extensive data training typically required by other machine learning methods, and\n\n2.Provides powerful out-of-the-box support for the more descriptive data formats that accompany many business problems.\n\n\u201cCatBoost\u201d name comes from two words - \u201cCategory\u201d and \u201cBoosting\u201d.\nIt works well with multiple categories of data, such as audio, text, image including historical data.\n\u201cBoost\u201d comes from gradient boosting machine learning algorithm as this library is based on gradient boosting library. Gradient boosting is a powerful machine learning algorithm that is widely applied to multiple types of business challenges like fraud detection, recommendation items, forecasting and it performs well also. It can also return very good results with relatively less data, unlike DL models that need to learn from a massive amount of data.\nIt is in open-source and can be used by anyone.\n\n### 2. Advantages of CatBoost library \nTable of Contents\n\nAdvantages of CatBoost library are as follows:-\n\nPerformance: CatBoost provides state of the art results and it is competitive with any leading machine learning algorithm on the performance front.\nHandling Categorical features automatically: We can use CatBoost without any explicit pre-processing to convert categories into numbers. CatBoost converts categorical values into numbers using various statistics on combinations of categorical features and combinations of categorical and numerical features.\nRobust: It reduces the need for extensive hyper-parameter tuning and lower the chances of overfitting also which leads to more generalized models. Although, CatBoost has multiple parameters to tune and it contains parameters like the number of trees, learning rate, regularization, tree depth, fold size, bagging temperature and others.\nEasy-to-use: We can use CatBoost from the command line, using an user-friendly API for both Python and R.\n\n### 3. Comparision of CatBoost and other Boosting algorithms \nTable of Contents\n\nWe have multiple boosting libraries like XGBoost, H2O and LightGBM and all of these perform well on variety of problems.\n\nCatBoost developer have compared the performance with competitors on standard ML datasets.\n\nThis comparision is depicted in the following diagram:\n\nComparision of CatBoost and other Boosting algorithms\n\nThe comparison above shows the log-loss value for test data and it is lowest in the case of CatBoost in most cases. It clearly signifies that CatBoost mostly performs better for both tuned and default models.\n\nIn addition to this, CatBoost does not require conversion of data set to any specific format like XGBoost and LightGBM.\n","803fcf0a":"### Head of Data","e667f7fb":"### Importing Dataset"}}