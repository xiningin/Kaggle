{"cell_type":{"f446dec5":"code","84a3e220":"code","dc745be0":"code","f4dca498":"code","218d6972":"code","0a9e8bb7":"code","853ec493":"code","bd83325e":"code","8ee05ac8":"code","234fa89a":"code","6dc00522":"code","421914cb":"code","0d0b3514":"code","2d7dc246":"code","e2ed373d":"code","a982c53e":"code","7365f05f":"code","e105af0c":"code","72cae8e6":"code","6c656cde":"code","6341684a":"code","92fb378d":"code","43af5474":"code","22af6f63":"code","02632178":"code","2dfab5bb":"code","1e3d1212":"code","24c2ed9f":"code","f376945c":"code","3f6e22bb":"code","d14eb719":"code","ff36e2d6":"code","84bf45dc":"code","6bf1e56a":"code","78de88a2":"code","93254d2d":"code","5e27242e":"code","993650b8":"code","26807e94":"code","42b20abc":"code","b90862c7":"code","834612b7":"code","44cac1bb":"code","a1a7b047":"code","9c61efbd":"code","bac1b0eb":"code","047165bf":"markdown","32334b2d":"markdown","9bf2493b":"markdown","a975d12d":"markdown","597e4c7b":"markdown","cacf713e":"markdown","95cad3fd":"markdown","b12aef1d":"markdown","f535d4b0":"markdown","46af3a8d":"markdown","4e3ed915":"markdown","83c6c02b":"markdown","43efcf0f":"markdown","30c15b3f":"markdown","34452e5c":"markdown","83e15bec":"markdown","8304fb4f":"markdown","d009d462":"markdown","9b55ab30":"markdown","4d0f1021":"markdown","a5ea38e4":"markdown","7e972970":"markdown","71c4d83e":"markdown","3cbcd125":"markdown","56ac5943":"markdown","d2619018":"markdown","e4089d0b":"markdown","f4106e60":"markdown"},"source":{"f446dec5":"# Loading all the required libraries\n%matplotlib inline\nimport os\nimport pandas as pd\nfrom pandas import ExcelFile\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns","84a3e220":"# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Settings\nplt.rcParams['axes.labelsize'] = 14\nplt.rcParams['xtick.labelsize'] = 12\nplt.rcParams['ytick.labelsize'] = 12\nplt.rcParams['figure.figsize'] = (16, 4)\n\npd.options.display.max_columns = 500","dc745be0":"# Loading IBM HR Employee Attrition dataset\nhr_file_name = '..\/input\/WA_Fn-UseC_-HR-Employee-Attrition.csv'\n\nemp_data_org = pd.read_csv(hr_file_name)\n\nprint('Dataset dimension: {} rows, {} columns'.format(emp_data_org.shape[0], emp_data_org.shape[1]))","f4dca498":"# Metadata of IBM HR Employee Attrition dataset\nemp_data_org.info()","218d6972":"# Basic statistics of numerical features\nemp_data_org.describe()","0a9e8bb7":"# Basic statistics of categorical features\nemp_data_org.describe(include=[np.object])","853ec493":"attrition_freq = emp_data_org[['Attrition']].apply(lambda x: x.value_counts())\nattrition_freq['frequency_percent'] = round((100 * attrition_freq \/ attrition_freq.sum()),2)\n\nprint(attrition_freq)\n\n# Attrition distribution bar plot\nplot = attrition_freq[['frequency_percent']].plot(kind=\"bar\");\nplot.set_title(\"Attrition Distribution\", fontsize=20);\nplot.grid(color='lightgray', alpha=0.5);","bd83325e":"null_feat_df = pd.DataFrame()\nnull_feat_df['Null Count'] = emp_data_org.isnull().sum().sort_values(ascending=False)\nnull_feat_df['Null Pct'] = null_feat_df['Null Count'] \/ float(len(emp_data_org))\n\nnull_feat_df = null_feat_df[null_feat_df['Null Pct'] > 0]\n\ntotal_null_feats = null_feat_df.shape[0]\nnull_feat_names = null_feat_df.index\nprint('Total number of features having null values: ', total_null_feats)\ndel null_feat_df","8ee05ac8":"emp_viz_df = emp_data_org.copy() # Copy cleaned dataset for EDA & feature changes\n\n# Let's add 2 features for EDA: Employee left and not left\nemp_viz_df['Attrition_Yes'] = emp_viz_df['Attrition'].map({'Yes':1, 'No':0}) # 1 means Employee Left\nemp_viz_df['Attrition_No'] = emp_viz_df['Attrition'].map({'Yes':0, 'No':1}) # 1 means Employee didnt leave\n\n# Let's look into the new dataset and identify features for which plots needs to be build for categorical features\nemp_viz_df.head()","234fa89a":"cat_col_names = emp_viz_df.select_dtypes(include=[np.object]).columns.tolist() # Get categorical feature names\ncat_col_names","6dc00522":"def generate_frequency_graph(col_name):\n    \n    # Plotting of Employee Attrition against feature\n    temp_grp = emp_viz_df.groupby(col_name).agg('sum')[['Attrition_Yes', 'Attrition_No']]\n    temp_grp['Percentage Attrition'] =  temp_grp['Attrition_Yes'] \/ (temp_grp['Attrition_Yes'] + temp_grp['Attrition_No']) * 100\n    print(temp_grp)\n    emp_viz_df.groupby(col_name).agg('sum')[['Attrition_Yes', 'Attrition_No']].plot(kind='bar', stacked=False, color=['red', 'green'])\n    plt.xlabel(col_name)\n    plt.ylabel('Attrition');","421914cb":"# Plotting of Employee Attrition against Business Travel feature\ngenerate_frequency_graph('BusinessTravel')\n# We can see Employee Attrition rate of Travel Frequently is high, then Travel Rarely & then Non-Travel","0d0b3514":"# Plotting of Employee Attrition against Department feature\ngenerate_frequency_graph('Department')\n# We can see Employee Attrition rate for Sales department is high then HR and finally Research & Development","2d7dc246":"# Plotting of Employee Attrition against Gender feature\ngenerate_frequency_graph('Gender')\n# We can see Employee Attrition rate for Male is higher than Female","e2ed373d":"# Plotting of Employee Attrition against MaritalStatus feature\ngenerate_frequency_graph('MaritalStatus')\n# We can see Employee Attrition rate for Single is higher than any other.","a982c53e":"# Plotting of Employee Attrition against MaritalStatus feature\ngenerate_frequency_graph('OverTime')\n# It is obivious that employee who are doing overtime has higher attrition rate","7365f05f":"# Plotting of Employee Attrition against Education feature\n# 1-Below College, 2-College, 3-Bachelor, 4-Master, 5-Doctor\ngenerate_frequency_graph('Education')\n# We can see Employee Attrition rate is high for Below College & Bachelor","e105af0c":"# Plotting of Employee Attrition against Education feature\n# 1-Low, 2-Medium, 3-High, 4-Very High\ngenerate_frequency_graph('JobSatisfaction')\n# Attrition is directly proportional to JobSatisfaction","72cae8e6":"# Plotting of Employee Attrition against Education feature\n# 1-Bad, 2-Good, 3-Better, 4-Best\ngenerate_frequency_graph('WorkLifeBalance')\n# Attrition rate of Bad work life balance is high, however attrition rate of Best is higher than Good","6c656cde":"fig = plt.figure(figsize=(16,4))\n\n# Histogram Plot for Employee Age\nplt.subplot(1,2,1)\nplt.hist(emp_viz_df['Age'], bins=15, color='green')\nplt.title('Distribution of Employee Age')\nplt.xlabel(\"Employee Age\")\n\n# Histogram Plot for employee Monthly Income\nplt.subplot(1,2,2)\nplt.hist(emp_viz_df['MonthlyIncome'], bins=30, color='red')\nplt.title('Distribution of Employee Monthly Income')\nplt.xlabel(\"Employee Monthly Income\")\n\nfig.tight_layout();","6341684a":"fig = plt.figure(figsize=(16,4))\n\n# Histogram Plot for Employee Age\nplt.subplot(1,2,1)\nplt.hist(np.log1p(emp_viz_df['Age']), bins=20, color='green')\nplt.title('Distribution of Employee Age')\nplt.xlabel(\"Log of Employee Age\")\n\n# Histogram Plot for employee Monthly Income\nplt.subplot(1,2,2)\nplt.hist(np.log1p(emp_viz_df['MonthlyIncome']), bins=15, color='red')\nplt.title('Distribution of Employee Monthly Income')\nplt.xlabel(\"Log of Employee Monthly Income\")\n\nfig.tight_layout();","92fb378d":"print('Skewness in employee Age feature: ', emp_viz_df['Age'].skew())\nprint('Skewness in employee Monthly Income feature: ', emp_viz_df['MonthlyIncome'].skew())","43af5474":"emp_proc_df = emp_data_org.copy() # Copy cleaned dataset for feature engineering\n\nemp_proc_df['TenurePerJob'] = 0\n\nfor i in range(0, len(emp_proc_df)):\n    if emp_proc_df.loc[i,'NumCompaniesWorked'] > 0:\n        emp_proc_df.loc[i,'TenurePerJob'] = emp_proc_df.loc[i,'TotalWorkingYears'] \/ emp_proc_df.loc[i,'NumCompaniesWorked']\n\nemp_proc_df['YearWithoutChange1'] = emp_proc_df['YearsInCurrentRole'] - emp_proc_df['YearsSinceLastPromotion']\nemp_proc_df['YearWithoutChange2'] = emp_proc_df['TotalWorkingYears'] - emp_proc_df['YearsSinceLastPromotion']\n\nmonthly_income_median = np.median(emp_proc_df['MonthlyIncome'])\nemp_proc_df['CompRatioOverall'] = emp_proc_df['MonthlyIncome'] \/ monthly_income_median\n\nprint('Dataset dimension: {} rows, {} columns'.format(emp_proc_df.shape[0], emp_proc_df.shape[1]))","22af6f63":"# Features to remove\nfeat_to_remove = ['EmployeeNumber', 'EmployeeCount', 'Over18', 'StandardHours']\n\nemp_proc_df.drop( feat_to_remove , axis = 1, inplace = True )\nprint('Dataset dimension: {} rows, {} columns'.format(emp_proc_df.shape[0], emp_proc_df.shape[1]))","02632178":"full_col_names = emp_proc_df.columns.tolist()\nnum_col_names = emp_proc_df.select_dtypes(include=[np.int64,np.float64]).columns.tolist() # Get numerical feature names\n\n# Preparing list of ordered categorical features\nnum_cat_col_names = ['Education', 'EnvironmentSatisfaction', 'JobInvolvement', 'JobLevel', 'JobSatisfaction',\n                     'PerformanceRating', 'RelationshipSatisfaction', 'WorkLifeBalance', 'StockOptionLevel']\n\ntarget = ['Attrition']\n\nnum_col_names = list(set(num_col_names) - set(num_cat_col_names)) # Numerical features w\/o Ordered Categorical features\ncat_col_names = list(set(full_col_names) - set(num_col_names) - set(target)) # Categorical & Ordered Categorical features\n\nprint('Total number of numerical features: ', len(num_col_names))\nprint('Total number of categorical & ordered categorical features: ', len(cat_col_names))\n\ncat_emp_df = emp_proc_df[cat_col_names]\nnum_emp_df = emp_proc_df[num_col_names]","2dfab5bb":"# Let's create dummy variables for each categorical attribute for training our calssification model\nfor col in num_col_names:\n    if num_emp_df[col].skew() > 0.80:\n        num_emp_df[col] = np.log1p(num_emp_df[col])\n\nnum_emp_df.head()","1e3d1212":"# Let's create dummy variables for each categorical attribute for training our calssification model\nfor col in cat_col_names:\n    col_dummies = pd.get_dummies(cat_emp_df[col], prefix=col)\n    cat_emp_df = pd.concat([cat_emp_df, col_dummies], axis=1)\n\n# Use the pandas apply method to numerically encode our attrition target variable\nattrition_target = emp_proc_df['Attrition'].map({'Yes':1, 'No':0})\n\n# Drop categorical feature for which dummy variables have been created\ncat_emp_df.drop(cat_col_names, axis=1, inplace=True)\n\ncat_emp_df.head()","24c2ed9f":"num_corr_df = num_emp_df[['MonthlyIncome', 'CompRatioOverall', 'YearWithoutChange1', 'DistanceFromHome']]\ncorr_df = pd.concat([num_corr_df, attrition_target], axis=1)\ncorr = corr_df.corr()\n\nplt.figure(figsize = (10, 8))\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nsns.axes_style(\"white\")\n#sns.heatmap(data=corr, annot=True, mask=mask, square=True, linewidths=.5, vmin=-1, vmax=1, cmap=\"YlGnBu\")\nsns.heatmap(data=corr, annot=True, square=True, linewidths=.5, vmin=-1, vmax=1, cmap=\"YlGnBu\")\nplt.show()","f376945c":"# Another way to check for correlation between attributes is to use Pandas\u2019 scatter_matrix function,\n\nfrom pandas.plotting import scatter_matrix\n\nscatter_matrix(corr_df, figsize=(16, 10));","3f6e22bb":"# Concat the two dataframes together columnwise\nfinal_emp_df = pd.concat([num_emp_df, cat_emp_df], axis=1)\n\nprint('Dataset dimension after treating categorical features with dummy variables: {} rows, {} columns'.format(final_emp_df.shape[0], final_emp_df.shape[1]))\nfinal_emp_df.head()","d14eb719":"# Import the train_test_split method\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\n# Split data into train and test sets as well as for validation and testing\nX_train, X_val, y_train, y_val = train_test_split(final_emp_df, attrition_target,\n                                                  test_size= 0.30, random_state=42);\n\nprint(\"Stratified Sampling: \", len(X_train), \"train set +\", len(X_val), \"validation set\")\n\n# Stratified Splitting\n#split = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n#for train_index, test_index in split.split(emp_data_proc, emp_data_proc['Gender']):\n#    strat_train_set = emp_data_proc.loc[train_index]\n#    strat_test_set = emp_data_proc.loc[test_index]","ff36e2d6":"from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.feature_selection import SelectFromModel\n\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, f1_score","84bf45dc":"def gen_model_performance(actual_target, pred_target):\n    model_conf_matrix = confusion_matrix(actual_target, pred_target)\n    model_roc_score = roc_auc_score(actual_target, pred_target)\n    model_accuracy = accuracy_score(actual_target, pred_target) * 100.0\n    \n    TP = model_conf_matrix[1][1]; TN = model_conf_matrix[0][0]; \n    FP = model_conf_matrix[0][1]; FN = model_conf_matrix[1][0];\n    sensitivity = TP \/ float(TP + FN) * 100.0; specificity = TN \/ float(TN + FP) * 100.0;\n    precision = TP \/ float(TP + FP) * 100.0;\n    \n    return sensitivity, specificity, model_accuracy, precision, model_roc_score","6bf1e56a":"def evaluate_model_score(X, y, scoring='accuracy'):\n    \n    logreg_model = LogisticRegression(random_state=0)\n    logreg_cv_model = LogisticRegressionCV()\n    rfc_model = RandomForestClassifier()\n    extrees_model = ExtraTreesClassifier()\n    gboost_model = GradientBoostingClassifier()\n    dt_model = DecisionTreeClassifier()\n    aboost_model = AdaBoostClassifier()\n    gnb_model = GaussianNB()\n\n    models = [logreg_model, logreg_cv_model, dt_model, rfc_model, \n              extrees_model, gboost_model, aboost_model, gnb_model]\n    \n    model_results = pd.DataFrame(columns = [\"Model\", \"Accuracy\", \"Precision\", \"CV Score\",\n                                            \"Sensitivity\",\"Specificity\",\"ROC Score\"])\n    \n    for model in models:\n        model.fit(X, y,)\n        y_pred = model.predict(X)\n        score = cross_val_score(model, X, y, cv=5, scoring=scoring)\n        \n        sensitivity, specificity, accuracy, precision, roc_score = gen_model_performance(y, y_pred)\n    \n        scores = cross_val_score(model, X, y, cv=5)\n    \n        model_results = model_results.append({\"Model\": model.__class__.__name__,\n                              \"Accuracy\": accuracy, \"Precision\": precision,\n                              \"CV Score\": scores.mean()*100.0,\n                              \"Sensitivity\": sensitivity, \"Specificity\": specificity,\n                              \"ROC Score\": roc_score}, ignore_index=True)\n    return model_results","78de88a2":"model_results = evaluate_model_score(X_train, y_train)\n\nmodel_results","93254d2d":"rfc_model = RandomForestClassifier();\n\nrefclasscol = X_train.columns\n\n# fit random forest classifier on the training set\nrfc_model.fit(X_train, y_train);\n\n# extract important features\nscore = np.round(rfc_model.feature_importances_, 3)\nimportances = pd.DataFrame({'feature':refclasscol, 'importance':score})\nimportances = importances.sort_values('importance', ascending=False).set_index('feature')\n\n# random forest classifier parameters used for feature importances\nprint(rfc_model)\n\n# plot importances\n#importances.plot.bar();","5e27242e":"high_imp_df = importances[importances.importance>=0.015]\nhigh_imp_df.plot.bar();\ndel high_imp_df","993650b8":"mid_imp_df = importances[importances.importance<=0.015]\nmid_imp_df = mid_imp_df[mid_imp_df.importance>=0.0050]\nmid_imp_df.plot.bar();\ndel mid_imp_df","26807e94":"selection = SelectFromModel(rfc_model, threshold = 0.002, prefit=True)\n\nX_train_select = selection.transform(X_train)\nX_val_select = selection.transform(X_val)\n\nprint('Train dataset dimension before Feature Selection: {} rows, {} columns'.format(X_train.shape[0], X_train.shape[1]))\nprint('Train dataset dimension after Feature Selection: {} rows, {} columns'.format(X_train_select.shape[0], X_train_select.shape[1]))","42b20abc":"model_results = evaluate_model_score(X_train_select, y_train)\n\nmodel_results","b90862c7":"final_rfc_model = RandomForestClassifier()\nfinal_rf_scores = cross_val_score(final_rfc_model, X_train_select, y_train, cv=5)\n\nfinal_rfc_model.fit(X_train_select, y_train)\ny_trn_pred = final_rfc_model.predict(X_train_select)\nsensitivity, specificity, accuracy, precision, roc_score = gen_model_performance(y_train, y_trn_pred)\n\nprint(\"Train Accuracy: %.2f%%, Precision: %.2f%%, CV Mean Score=%.2f%%, Sensitivity=%.2f%%, Specificity=%.2f%%\" % \n      (accuracy, precision, final_rf_scores.mean()*100.0, sensitivity, specificity))\nprint('***************************************************************************************\\n')\n\ny_val_pred = final_rfc_model.predict(X_val_select)\nsensitivity, specificity, accuracy, precision, roc_score = gen_model_performance(y_val, y_val_pred)\n\nprint(\"Validation Accuracy: %.2f%%, Precision: %.2f%%, Sensitivity=%.2f%%, Specificity=%.2f%%\" % \n      (accuracy, precision, sensitivity, specificity))\nprint('***************************************************************************************\\n')","834612b7":"from imblearn.over_sampling import SMOTE\n\noversampler=SMOTE(random_state=0)\nX_train_smote, y_train_smote = oversampler.fit_sample(X_train_select, y_train)","44cac1bb":"model_results = evaluate_model_score(X_train_smote, y_train_smote)\n\nmodel_results","a1a7b047":"final_rfc_model = RandomForestClassifier()\nfinal_rf_scores = cross_val_score(final_rfc_model, X_train_smote, y_train_smote, cv=5)\n\nfinal_rfc_model.fit(X_train_smote, y_train_smote)\ny_trn_pred = final_rfc_model.predict(X_train_smote)\nsensitivity, specificity, accuracy, precision, roc_score = gen_model_performance(y_train_smote, y_trn_pred)\n\nprint(\"Train Accuracy: %.2f%%, Precision: %.2f%%, CV Mean Score=%.2f%%, Sensitivity=%.2f%%, Specificity=%.2f%%\" % \n      (accuracy, precision, final_rf_scores.mean()*100.0, sensitivity, specificity))\nprint('***************************************************************************************\\n')\n\ny_val_pred = final_rfc_model.predict(X_val_select)\nsensitivity, specificity, accuracy, precision, roc_score = gen_model_performance(y_val, y_val_pred)\n\nprint(\"Validation Accuracy: %.2f%%, Precision: %.2f%%, Sensitivity=%.2f%%, Specificity=%.2f%%\" % \n      (accuracy, precision, sensitivity, specificity))\nprint('***************************************************************************************\\n')","9c61efbd":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\n\n# Hyperparameters Tuning for Random Forest\nrfc_param_grid = {\n                 'max_depth' : [None, 4, 6, 8],\n                 'n_estimators': [10, 30, 50],\n                 'max_features': ['sqrt', 'auto', 'log2'],\n                 'min_samples_split': [2, 3, 10],\n                 'min_samples_leaf': [1, 3, 10],\n                 }\nrfc_best_model = RandomForestClassifier()\nrfc_cross_val = StratifiedKFold(n_splits=5)\n\nrfc_grid_search = GridSearchCV(rfc_best_model,\n                               scoring='accuracy',\n                               param_grid=rfc_param_grid,\n                               cv=rfc_cross_val,\n                               verbose=1)\n\nrfc_grid_search.fit(X_train_smote, y_train_smote)\nrfc_model = rfc_grid_search\nrfc_parameters = rfc_grid_search.best_params_\n\nrfc_best_model = RandomForestClassifier(**rfc_parameters)\n\nscores = cross_val_score(rfc_best_model, X_train_smote, y_train_smote, cv=5, scoring='accuracy')\n    \nprint('Cross-validation of : {0}'.format(rfc_best_model.__class__))\nprint('After Hyperparameters tuning CV score = {0}'.format(np.mean(scores) * 100.0))\nprint('Best score: {}'.format(rfc_grid_search.best_score_))\nprint('Best parameters: {}'.format(rfc_grid_search.best_params_))\n\nrfc_best_model.fit(X_train_smote, y_train_smote)\ny_trn_pred = rfc_best_model.predict(X_train_smote)\nsensitivity, specificity, accuracy, precision, roc_score = gen_model_performance(y_train_smote, y_trn_pred)\n\nprint(\"\\nTrain Accuracy: %.2f%%, Precision: %.2f%%, CV Mean Score=%.2f%%, Sensitivity=%.2f%%, Specificity=%.2f%%\" % \n      (accuracy, precision, scores.mean()*100.0, sensitivity, specificity))\nprint('***************************************************************************************\\n')","bac1b0eb":"y_val_pred = rfc_best_model.predict(X_val_select)\nsensitivity, specificity, accuracy, precision, roc_score = gen_model_performance(y_val, y_val_pred)\n\nprint(\"Validation Accuracy: %.2f%%, Precision: %.2f%%, Sensitivity=%.2f%%, Specificity=%.2f%%\" % \n      (accuracy, precision, sensitivity, specificity))\nprint('***************************************************************************************\\n')","047165bf":"- We can observe 3 models which performs very well i.e. RandomForestClassifier, GradientBoostingClassifier & AdaBoostClassifier.\n- Out of 3 models, we can see cross validation score of AdaBoostClassifier is best and Sensitivity i.e. Recall rate of  RandomForestClassifier is the best.\n- Our goal is to identify whether employee will leave or not i.e. Attrition=1 which means our Sensitivity metric is the main evaluation metric along with Precision and Accuracy.\n- Even though DecisionTreeClassifier & ExtraTreesClassifier has 100% Accuracy which means model is overfitting, hence not selected.\n\n### Step 5.4 - Feature Selection Through Ensemble Model\nFeature selection can also be acheived by the insights provided by some Machine Learning models. Tree based models calculates feature importance for they need to keep the best performing features as close to the root of the tree. The feature importance in tree based models are calculated based on Gini Index, Entropy or Chi-Square value. We will be using basic random forest classifier and extract the features based on their importances.","32334b2d":"### Step 3.2 - Exploratory Data Analysis - Categorical Features\nLet's visualize & understand how different categorical features are behaving against attrition indicator","9bf2493b":"We can look into other ordered categorical features like **EnvironmentSatisfaction, JobInvolvement, PerformanceRating, RelationshipSatisfaction** the same way and draw inferences which can help us to create new features for our model or which features we need to select for our model.","a975d12d":"### Step 4.2 - Remove Unnecessary Observations\n- Above we have identified during EDA that there are some features which has only 1 value resulting into very minimal variations. Hence, those observations can be removed. \n- We need to remove **EmployeeNumber, EmployeeCount, StandardHours & Over18** which doesn't have any significance on attrition. We made this inference at the top while describing dataset.","597e4c7b":"### Step 6.2 - Train Models & Evaluate through Cross-Validation for SMOTE Dataset\nWe will train number of  classification models i.e. Logistic Regression, Decision Tree Classifier, Random Forest Classifier, ExtraTrees Classifier, Boosting Classifier and Naive Bayes. Once we build all the models, we will identify the best performer through accuracy, CV score, sensitivity & specificity parameters.","cacf713e":"## Step 6 - Handling Target Class Imbalance\n- We can observe that accuracy, precision, etc. metrics are performing very good on training dataset, however, on validation dataset recall & precision of attrition drops down significantly.\n- This is happening due to imbalance class. Model doesn't have enough observations to train on employees who have left i.e. attrition = 'Yes'\n- Since we have already noted the severe imbalance in the values within the target variable, let us implement the SMOTE method in the dealing with this skewed value via the imblearn Python package.\n\n### 6.1 SMOTE to Oversample Target Variable","95cad3fd":"### Step 3.4 - Exploratory Data Analysis - Numerical Features\nLet's visualize data distribution of numerical features. Usually, prediction model works well if the data distribution is normal distribution. So, if there are skewed data distirubtion then we can make them normal distribution through log transformations.","b12aef1d":"### Step 4.5 - Correlation of Numerical Features against Attrition","f535d4b0":"### Step 3.3 - Exploratory Data Analysis - Ordered Categorical Features\nLet's visualize & understand how different ordered categorical features are behaving against attrition indicator i.e. **Education, EnvironmentSatisfaction, JobInvolvement, JobLevel, JobSatisfaction, PerformanceRating, RelationshipSatisfaction, WorkLifeBalance, StockOptionLevel**","46af3a8d":"- Just by a quick inspection of the counts of the number of 'Yes' and 'No' in the target variable tells us that there is quite a large skew in target variable.\n- Therefore we have to keep in mind that there is quite a big imbalance in our target variable. Many statistical techniques have been put forth to treat imbalances in data (oversampling or undersampling).\n- In this notebook, I will use an oversampling technique known as SMOTE to treat this imbalance.\n- We will see the prediction model with and without SMOTE treatment for imbalance class issue.","4e3ed915":"- **Over18** has only one unique value which represents all employees are above 18 years of age. So, ideally this feature also has no variation adn won't help in our prediction model.","83c6c02b":"After SMOTE mechanism to improve target class imbalance and identifying best hyper-parameters, we were able to improve the metrics of training model, however, validation metrics are improved slightly specially Sensitivity has very little impact.\n\nSo, we can do more permutations & combinations w.r.t feature engineering, feature selection, hyper-parameters tuning, class imbalance, etc. to identify the best model that generates best metrics on validation dataset.","43efcf0f":"- We can observe above that without log transformation, \"Age\" feature looks normally distributed more than \"MonthlyIncome\" feature.\n- Hence, after log transformation, there is hardly any change in \"Age\" distributions but there is significant transformation in \"MonthlyIncome\" feature.\n- This can also be confirmed by checking the skewness of each feature by applying numpy skew metric and higher the skewness, higher the value.\n- So, we will use some threshold value of skewness to carry out log transformation on features with high skewness.","30c15b3f":"- We can see that after feature selection based on random forest classifier technique, metrics of random forest model has improved.\n- There are similar feature selection mechanism for GradientBoostingClassifier & AdaBoostClassifier models which cna be used to improve their model respectively. However, we can leave that for the next part.\n### Step 5.6 - Model Performance on Validation Dataset\nFor checking model performance sake, we can choose RandomForest Classifier as final model and build the model on training dataset. Once the model is built, let's see its performance on validation dataset which was never seen by this model. This will give us the true picture of the model, how it performs on unseen new data and what is the accuracy of prediction.","34452e5c":"We can look into other categorical features like **EducationField, JobRole** the same way and draw inferences which can help us to create new features for our model or which features we need to select for our model.","83e15bec":"# IBM HR Employee Attrition Analysis & Prediction\n\n## Step 1 - Understand the Problem and Objective \n\n**Objective:** To predict if an employee is going to resign or not. Uncover the factors that lead to employee attrition and explore how each feature is co-related with attrition.\n\n**Dataset:** We are using a dataset put up by IBM for our analysis. The dataset contain 35 features along with Attrition target variable. This is a fictional data set created by IBM data scientists. It can be downloaded from the following link\nLink- https:\/\/www.ibm.com\/communities\/analytics\/watson-analytics-blog\/hr-employee-attrition\/ or https:\/\/www.kaggle.com\/pavansubhasht\/ibm-hr-analytics-attrition-dataset\n\n**Methodology:**\n1. Through our analysis we intend to build a model which can predict if an employee is about to quit. \n2. We shall be looking at all variables through some plots and infer about it in our exploratory analysis. \n3. After our exploration we shall build some features based on the variables at hand and take a call on inclusion\/exclusion of few variables.","8304fb4f":"### Step 5.5 - Train Feature Selected Models & Evaluate through Cross-Validation\nWe will train number of  classification models i.e. Logistic Regression, Decision Tree Classifier, Random Forest Classifier, ExtraTrees Classifier, Boosting Classifier and Naive Bayes. Once we build all the models, we will identify the best performer through accuracy, CV score, sensitivity & specificity parameters.","d009d462":"- We can observe that after applying SMOTE technique the accuracy of all the models have increased including Sensitivity & Precision metrics.\n### Step 6.3 - Model Performance on Validation Dataset\nFor checking model performance sake, we can choose RandomForest Classifier as final model and build the model on training dataset which was transformed by SMOTE mechanism. Once the model is built, let's see its performance on validation dataset which was never seen by this model. This will give us the true picture of the model, how it performs on unseen new data and what is the accuracy of prediction.","9b55ab30":"Thre are few variables which are represented as numerical but they are actually ordered categorical variables i.e. **Education, EnvironmentSatisfaction, JobInvolvement, JobLevel, JobSatisfaction, PerformanceRating, RelationshipSatisfaction, WorkLifeBalance, StockOptionLevel**\n- **EmployeeNumber** can be removed as it is an unique representation of Employee and doesn't have any significance with attrition.\n- **EmployeCount** has only 1 value which again won't help in our prediction model. \n- **StandardHours** has only one value which is 80, again won't help in our prediction model.","4d0f1021":"## Step 3 - Exploratory Data Analysis\n\n### Step 3.1 - Identify Features with NULL values\nTo understand how each features are impacting the attrition indicator, we need to first handle the null \/ missing values, otherwise our observations might not be accurate and will lead to wrong conclusions.","a5ea38e4":"### Attrition Target Variable Distribution","7e972970":"## Step 7 - Identify Best Hyper-Parameters of Model\nOnce the model is build, we can tune the hyper-parameters of the model to get the best score and parameters. This will help us in getting better accuracy model. This can be achieved by **GridSearchCV** library by passing a range of values for different parameters.","71c4d83e":"### Step 4.4 - Transform Categorical Features\n- Machine Learning model works only on numerical datasets, hence, we need to transform categorical features into numerical features.\n- One of the best strategy is to convert each category value into a new column and assigns 1 or 0 (True\/False) value to the column. This has the benefit of not weighting a value improperly but does have the downside of adding more columns to the data set. \n- This approach is also called as **\"One Hot Encoding\"**. We can use Pandas feature get_dummies to achieve this transformation.\n- There is another way to handled **ordered categorical feature** is to give ordered value based on their definitions **i.e Low-Meidum-High would be 1-2-3.** We can try this approach some other time. But, this can be evaluated to check the performance of the model.","3cbcd125":"### Step 5.3 - Train Models & Evaluate through Cross-Validation\nWe will train number of  classification models i.e. Logistic Regression, Decision Tree Classifier, Random Forest Classifier, ExtraTrees Classifier, Boosting Classifier and Naive Bayes. Once we build all the models, we will identify the best performer through accuracy, CV score, sensitivity & specificity parameters.","56ac5943":"## Step 4 - Feature Engineering\nFeature engineering is one aspect which provided a huge impact on the outcome rather than the model. Here, we try at creating new features with the existing variables we have based on my assumptions.\n\n### Step 4.1 - Addition of New Features\n- Tenure per job: Usually, people who have worked with many companies but for small periods at every organization tend to leave early as they always need a change of Organization to keep them going.\n- Years without Change: For any person, a change either in role or job level or responsibility is needed to keep the work exciting to continue. We create a variable to see how many years it has been for an employee without any sort of change using Promotion, Role and Job Change as a metric to cover different variants of change.\n- Compensation Ratio: Compa Ratio is the ratio of the actual pay of an Employee to the midpoint of a salary range. The salary range can be that of his\/her department or organization or role. The benchmark numbers can be a organization\u2019s pay or Industry average.","d2619018":"### Step 4.3 - Transform Numerical Features\nIn order to fix the skewness, let\u2019s take the log for all numerical features with an absolute skew greater than 80% (Note: log+1, to avoid division by zero issues).","e4089d0b":"## Step 5 - Model Building and Validation\nSince, we have to predict a binary class, we will be using classification models for training & predicting Employee Attrition. We need to keep in mind that our focus should be to have a better accuracy of predicting attrition i.e. Attrition = 1 which in confusion matrix will be \"True Positive\". However, we should not forget the prediction accuracy of not qualifying for attrition i.e. Attrition = 0 which will be \"True Negative\" in confusion matrix.\n\nSo, we need to focus on four parameters:\n- **Accuracy:** Overall, how often is the classifier correct? i.e {(TP+TN)\/Total}\n- **True Positive Rate:** When it's actually yes, how often does it predict yes? default_ind = 1, {TP\/Actual YES}, this is also known as \"Sensitivity\" or \"Recall\"\n- **Precision:** When it predicts yes, how often is it correct? i.e. {TP\/(TP+FP)}\n- **Specificity:** When it's actually no, how often does it predict no? default_ind = 0, {TN\/actual NO}\n- **Cross Validation Score:** Cross Validation is a technique which involves reserving a particular sample of a dataset on which you do not train the model. Later, you test your model on this sample before finalizing it. Do this for k folds and take mean of accuracy scores of the k fold models.\n- **F1 Score:** This is a weighted average of the true positive rate (recall) and precision.\n- **ROC Curve:** This is a commonly used graph that summarizes the performance of a classifier over all possible thresholds. It is generated by plotting the True Positive Rate (y-axis) against the False Positive Rate (x-axis) as you vary the threshold for assigning observations to a given class. \n\nAbove information was taken and more details can be found at https:\/\/www.dataschool.io\/simple-guide-to-confusion-matrix-terminology\/\n\n### Step 5.1 - Prepare Train & Test Dataset\nThe data should be divided into train and test data. We will use train_test_split feature to divide the data and we will be using 70-30 ratio","f4106e60":"## Step 2 - Understand the Data\nLet's look how many observations and features are present in complete dataset. Also, what is the data type of each feature. First few rows of dataset to get an idea how the data value looks."}}