{"cell_type":{"f00dc46a":"code","d8d5371e":"code","1add16ce":"code","b069be01":"code","daa9e15a":"code","b73d2bfe":"code","b96a6c31":"code","92cd517b":"code","75e76b5e":"code","40485f96":"code","4a12c6d6":"markdown","730cd605":"markdown","544f609d":"markdown","40443005":"markdown","1e76965f":"markdown","a6e1633b":"markdown","7316e283":"markdown","6d931725":"markdown"},"source":{"f00dc46a":"import numpy as np \nimport cv2                                         # working with, mainly resizing, images\nimport os,random                                          # dealing with directories\nfrom keras.models import Sequential,Model                # creating sequential model of CNN\nfrom keras.layers import Conv2D,BatchNormalization             # creating convolution layer\nfrom keras.layers import MaxPooling2D              # creating maxpool layer\nfrom keras.layers import Flatten,Activation                   # creating input vector for dense layer\nfrom keras.layers import Dense,GlobalAveragePooling2D                     # create dense layer or fully connected layer\nfrom keras.layers import Dropout                   # use to avoid overfitting by droping some parameters\nimport matplotlib.pyplot as plt   \nfrom sklearn.model_selection import train_test_split   \nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras.optimizers import Adam,Adadelta","d8d5371e":"IMG_WIDTH,IMG_HEIGTH=150,150\nBatch_Size=8\nEpoch=6\n\ndef onehotlabel(resim):\n    if resim==\"BOLT_LEFT\":\n        return 0\n    elif resim==\"BOLT_RIGHT\":\n        return 1 \n    elif resim==\"FASTENING_MODEL_LEFT\":\n        return 2\n    elif resim==\"FASTENING_MODEL_RIGHT\":\n        return 3 \n    else: return -1\n","1add16ce":"def generate_data(DATADIR):\n    path = os.path.join(DATADIR)\n    dataset = []\n    for imge in os.listdir(DATADIR):        \n        for img in os.listdir(os.path.join(DATADIR,imge)): \n            if img.endswith(\"jpg\"):\n                lbl=onehotlabel(imge)\n                if (lbl!=-1):\n                    im = cv2.imread(os.path.join(path,imge,img),cv2.IMREAD_GRAYSCALE)\n                    im = cv2.resize(im, (IMG_WIDTH, IMG_HEIGTH))\n                    dataset.append([im,onehotlabel(imge)]) \n    \n    random.shuffle(dataset)\n    data = []\n    labels = []\n    for features, label in dataset:        \n        data.append(features.astype('float32') \/ 255)\n        labels.append(label)\n    data = np.array(data)\n#    data.reshape(data.shape[0], IMG_WIDTH ,IMG_HEIGTH,  1)\n\n    train_data,test_data,train_labels,test_labels = train_test_split(data,labels,stratify=labels,test_size=0.2, random_state=42)\n    return train_data,test_data,train_labels,test_labels\n","b069be01":"path=\"..\/input\/rail-travers-dataset\"\n\ntrain_data,test_data,train_labels,test_labels= generate_data(path)","daa9e15a":"print(train_data[2].shape)\nprint(np.max(train_data[2]))\nplt.imshow(train_data[2], cmap='gray')","b73d2bfe":"train_data=train_data.reshape(-1,IMG_WIDTH ,IMG_HEIGTH,1)\n#train_data=train_data\/255 #Don't need to divide by 255 again.\ntest_data=test_data.reshape(-1,IMG_WIDTH ,IMG_HEIGTH,1)\n#test_data=test_data\/255\n\n\ndatagen_train=ImageDataGenerator(\n    #rescale=1.\/255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True)\n\ndatagen_train.fit(train_data)","b96a6c31":"from keras.utils import to_categorical\nprint(\"Before categorical encoding\")\nprint(train_labels[:5])\ntrain_labels = to_categorical(train_labels, 5)\ntest_labels = to_categorical(test_labels,5)\nprint(\"After categorical encoding\")\nprint(train_labels[:5])","92cd517b":"model = Sequential()\nmodel.add(Conv2D(32, (3, 3), padding=\"same\",input_shape=(IMG_WIDTH ,IMG_HEIGTH, 1)))\nmodel.add(Activation(\"relu\"))\nmodel.add(BatchNormalization()) #Y\u0131\u011f\u0131n normalle\u015ftirme derin sinir a\u011flar\u0131ndaki herhangi bir katmana 0\u2019a ortalanm\u0131\u015f ve 1 ile 0 aras\u0131nda de\u011ferlere sahip veriler vermemizi sa\u011flar.\nmodel.add(MaxPooling2D(pool_size=(3, 3)))\nmodel.add(Dropout(0.20))\nmodel.add(Conv2D(64, (3, 3), padding=\"same\"))\nmodel.add(Activation(\"relu\"))\n\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64, (3, 3), padding=\"same\"))\nmodel.add(Activation(\"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.3))\n\nmodel.add(Conv2D(64, (3, 3), padding=\"same\"))\nmodel.add(Activation(\"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(32, (3, 3), padding=\"same\"))\nmodel.add(Activation(\"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.4))\nmodel.add(Flatten())\nmodel.add(Dense(1024))\nmodel.add(Activation(\"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\n#model.add(Dense(1)) #Changed to 5 (below)\nmodel.add(Dense(5))\nmodel.add(Activation(\"softmax\"))\n# model.compile(loss='binary_crossentropy',optimizer=Adam(),metrics=['accuracy']) #Changed to categorical_crossentropy(below)\nmodel.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])\nmodel.summary()","75e76b5e":"history = model.fit_generator(\n    datagen_train.flow(train_data, train_labels, batch_size=Batch_Size),\n    steps_per_epoch=len(train_data),\n    validation_steps = len(test_data),\n    epochs=5,\n    verbose = 1,\n    validation_data=(test_data,test_labels)\n    )","40485f96":"y_pred = model.predict_classes(test_data)\nacc = np.sum(y_pred == test_labels) \/ np.size(y_pred)\nprint(\"Test accuracy = {}\".format(acc))\n\n\nfinal_loss, final_acc = model.evaluate(test_data, test_labels, verbose=1)\nprint(\"validation loss: {0:.6f}, validation accuracy: {1:.6f}\".format(final_loss, final_acc))\n\naccuracy = history.history['accuracy']\nloss = history.history['loss']\nval_accuracy = history.history['val_accuracy']\nval_loss = history.history['val_loss']\n\nprint(f'Training Accuracy: {np.max(accuracy)}')\nprint(f'Training Loss: {np.min(loss)}')\nprint(f'Validation Accuracy: {np.max(val_accuracy)}')\nprint(f'Validation Loss: {np.min(val_loss)}')\n\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')","4a12c6d6":"I've printed the first 5 train labels to make it clearer as to what happens in categorical encoding.\nSo 2 became [0, 0, 1, 0, 0] because the matrix corresponds to our classes which are 0,1,2,3,4.\nWe can see that 0 became [1,0,0,0,0] and 3 became [0,0,0,1,0] as well.\n\nHence, now our network should output a vector of size 5, of which the maximum will be selected (softmax). Hence, the last neuron should be 5. I've also changed the loss to `categorical_crossentropy`.\n\nAlso added a learning rate to Adam.","730cd605":"Seems like achieve a high accuracy (80%+) on the validation data. You can try to optimize the accuracy by changing the epochs and learning rate of Adam, but the data is quite limited in this dataset and the best way to increase accuracy would be to get more images or augment those images yourself.","544f609d":"I've added a stratify to the train_test_data to make sure that our training data and test data is balanced. Also added random_state=42 for reproduciblity.","40443005":"The loss function in the original kernel is wrong. Since we are classifying the images into one of five classes, we can't use `binary_crossentropy` (which is for 2 classes classification). We have to use `categorical_crossentropy` that allows us to do classification of more than two classes. But before using that, we need to convert the data to categorical data.","1e76965f":"I hope this kernel helped you realize the mistakes in your original kernel, cuneyt.","a6e1633b":"The [original uploader (cuneytdemir)](https:\/\/www.kaggle.com\/cuneytdemir) of this dataset has made most of this kernel and the model. I'm just correcting the mistakes I found and the potential reasons behind their low accuracy of about 13%. I have removed the Kaggle bot comments but retained the in-code comments of the original author.\n\nTheir original notebook (which this kernel is forked from) can be found [here](https:\/\/www.kaggle.com\/cuneytdemir\/rail-classification-swish-activation)","7316e283":"So far, everything is correct. However, onwards, the data is divided by 255 two times, despite it already being normalized using the code `        data.append(features.astype('float32') \/ 255)\n` in the earlier cells in  As you can see below, the maximum value of an image is already 1 and the images are already normalized (all values between 0 and 1). You don't need to normalize it thrice (in the original kernel, the images were normalized three times by 255).","6d931725":"So we don't need the three comments that I commented down."}}