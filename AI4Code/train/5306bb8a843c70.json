{"cell_type":{"556c69b7":"code","ad3039ff":"code","80ed36af":"code","464cf4f4":"code","2959c74f":"code","a0fa3a18":"code","f0d8a877":"code","cdd58172":"code","0b015a6a":"code","45fb4289":"code","f2e2a0b3":"code","60c880aa":"code","8d4b29fd":"code","31972290":"code","82a436c3":"code","16fa7c1b":"code","944215af":"code","ce588f5c":"code","753eac8a":"code","56a45ba2":"code","e1208c92":"code","70098ac2":"code","a09fedf5":"code","a0a8f98e":"code","b797d02a":"code","57eefc3d":"code","c33473fa":"code","d15cd6ec":"code","54ee100b":"code","3a448271":"code","9e592f7b":"code","cdc1e20d":"code","ffd55f37":"code","1fe6633d":"code","0c181487":"code","ad26b1ee":"code","ff8a4518":"code","de22c6e4":"code","a619f035":"code","588af5cb":"code","eefd297f":"code","a582143e":"code","d0c5d88f":"code","f241d2bf":"code","e9fc55ba":"code","98792ac7":"code","b53be62d":"code","63b29a24":"code","e90d0f7b":"code","01a5793d":"code","5a7a9796":"code","d0bae106":"code","8712c4cd":"code","8034f0cc":"code","560e46a5":"code","2d4cf52a":"code","eea6f255":"code","1e0a0369":"code","41592653":"code","c3c887b0":"code","748d0046":"markdown","9627e038":"markdown","4559265e":"markdown","263e1d82":"markdown","fab96121":"markdown","acda337c":"markdown","941a7dd9":"markdown","cdc0308e":"markdown","99ce84bb":"markdown","cd179c57":"markdown","10dbb24d":"markdown","425edd5e":"markdown","1bd924cd":"markdown","31d880da":"markdown","b557ea5b":"markdown","0e877a2f":"markdown","eea6afe1":"markdown","2db61a0e":"markdown","5d625671":"markdown","d3f5cbda":"markdown","62d7f924":"markdown","45e64d49":"markdown","22c67b05":"markdown","03112461":"markdown","9e4c3373":"markdown","443e7913":"markdown","f5439e96":"markdown"},"source":{"556c69b7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom scipy.stats import ttest_ind\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline\npd.options.display.float_format = '{:.3f}'.format\n\nfrom collections import Counter\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ad3039ff":"train_df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_PassengerId = test_df[\"PassengerId\"]","80ed36af":"train_df.head()","464cf4f4":"train_df.tail()","2959c74f":"train_df.columns","a0fa3a18":"train_df.info()","f0d8a877":"train_df.describe()","cdd58172":"category = [\"Survived\",\"Sex\",\"Pclass\",\"Embarked\",\"SibSp\", \"Parch\"]\n\nfor i in category:\n    c = train_df[i]\n    counts = c.value_counts()\n    \n    #Visualization\n    plt.bar(counts.index, counts.values)\n    plt.title(i)\n    plt.ylabel(\"Frequency\")\n    plt.xticks(counts.index)\n    plt.show()\n    print(\"{}: \\n {}\".format(i,counts))","0b015a6a":"def histogram(feature):\n    plt.figure(figsize = (9,3))\n    plt.hist(train_df[feature], bins = 50)\n    plt.xlabel(feature)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"{} distribution with hist\".format(feature))\n    plt.show()","45fb4289":"numerical_var = [\"Fare\", \"Age\"]\nfor n in numerical_var:\n    histogram(n)","f2e2a0b3":"train_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","60c880aa":"train_df[[\"Sex\",\"Survived\"]].groupby([\"Sex\"], as_index=False).mean().sort_values(by=\"Survived\",ascending=False)","8d4b29fd":"train_df[[\"SibSp\",\"Survived\"]].groupby([\"SibSp\"], as_index=False).mean().sort_values(by=\"Survived\",ascending=False)","31972290":"train_df[[\"Parch\",\"Survived\"]].groupby([\"Parch\"], as_index=False).mean().sort_values(by=\"Survived\",ascending=False)","82a436c3":"g = sns.FacetGrid(train_df, col='Survived')\ng.map(plt.hist, 'Age', bins=20)\nplt.show()","16fa7c1b":"g2 = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=2.2, aspect=1.5)\ng2.map(plt.hist, 'Age', alpha=.5, bins=20)\ng2.add_legend();","944215af":"def detect_outliers(df,features):\n    outlier_indices = []\n    \n    for c in features:\n        # 1st quartile\n        Q1 = np.percentile(df[c],25)\n        # 3rd quartile\n        Q3 = np.percentile(df[c],75)\n        # IQR\n        IQR = Q3 - Q1\n        # Outlier step\n        outlier_step = IQR * 1.5\n        # Detect outlier and their indeces\n        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index\n        # Store indeces\n        outlier_indices.extend(outlier_list_col)\n    \n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)\n    \n    return multiple_outliers","ce588f5c":"train_df.loc[detect_outliers(train_df,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])]","753eac8a":"# drop outliers\ntrain_df = train_df.drop(detect_outliers(train_df,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"]),axis = 0).reset_index(drop = True)","56a45ba2":"train_df.columns[train_df.isnull().any()]","e1208c92":"train_df.isnull().sum()","70098ac2":"train_df[train_df[\"Age\"].isnull()]","a09fedf5":"train_df[train_df[\"Embarked\"].isnull()]","a0a8f98e":"train_df[(train_df.Pclass == 1) & (train_df.Fare == 80)][[\"Embarked\",\"Fare\"]]","b797d02a":"train_df.boxplot(column=\"Fare\",by = \"Embarked\")\nplt.show()","57eefc3d":"train_df[\"Embarked\"] = train_df[\"Embarked\"].fillna(\"C\")\ntrain_df[train_df[\"Embarked\"].isnull()]","c33473fa":"sns.factorplot(x=\"Sex\",y=\"Age\",data=train_df,kind = \"box\")\nplt.show()","d15cd6ec":"sns.factorplot(x = \"Sex\", y = \"Age\", hue = \"Pclass\",data = train_df, kind = \"box\")\nplt.show()","54ee100b":"sns.factorplot(x = \"Parch\", y = \"Age\", data = train_df, kind = \"box\")\nsns.factorplot(x = \"SibSp\", y = \"Age\", data = train_df, kind = \"box\")\nplt.show()","3a448271":"# # We can use that code if we want to see sex feature in the heatmap because heatmap just shows numerical features correlation.\n# train_df[\"Sex\"] = [1 if i == \"male\" else 0 for i in train_df[\"Sex\"]]","9e592f7b":"sns.heatmap(train_df[[\"Age\",\"Sex\",\"SibSp\",\"Parch\",\"Pclass\"]].corr(), annot = True)\nplt.show()","cdc1e20d":"index_miss_age = list(train_df[\"Age\"][train_df[\"Age\"].isnull()].index)\nfor i in index_miss_age:\n    age_pred = train_df[\"Age\"][((train_df[\"SibSp\"] == train_df.iloc[i][\"SibSp\"]) &(train_df[\"Parch\"] == train_df.iloc[i][\"Parch\"])& (train_df[\"Pclass\"] == train_df.iloc[i][\"Pclass\"]))].median()\n    age_med = train_df[\"Age\"].median()\n    if not np.isnan(age_pred):\n        train_df[\"Age\"].iloc[i] = age_pred\n    else:\n        train_df[\"Age\"].iloc[i] = age_med","ffd55f37":"train_df[train_df[\"Age\"].isnull()]","1fe6633d":"train_df[\"Name\"].head(20)","0c181487":"name = train_df[\"Name\"]\ntrain_df[\"Title\"] = [i.split(\".\")[0].split(\",\")[-1].strip() for i in name]","ad26b1ee":"train_df[\"Title\"].head(10)","ff8a4518":"sns.countplot(x=\"Title\", data = train_df)\nplt.xticks(rotation = 45)\nplt.show()","de22c6e4":"# convert to categorical\ntrain_df[\"Title\"] = train_df[\"Title\"].replace([\"Lady\",\"the Countess\",\"Capt\",\"Col\",\"Don\",\"Dr\",\"Major\",\"Rev\",\"Sir\",\"Jonkheer\",\"Dona\"],\"other\")\ntrain_df[\"Title\"] = [0 if i == \"Master\" else 1 if i == \"Miss\" or i == \"Ms\" or i == \"Mlle\" or i == \"Mrs\" else 2 if i == \"Mr\" else 3 for i in train_df[\"Title\"]]\ntrain_df[\"Title\"].head(20)","a619f035":"sns.countplot(x=\"Title\", data = train_df)\nplt.xticks(rotation = 60)\nplt.show()","588af5cb":"g = sns.factorplot(x = \"Title\", y = \"Survived\", data = train_df, kind = \"bar\")\ng.set_xticklabels([\"Master\",\"Mrs\",\"Mr\",\"Other\"])\ng.set_ylabels(\"Survival Probability\")\nplt.show()","eefd297f":"train_df = pd.get_dummies(train_df,columns=[\"Title\"])","a582143e":"train_df.drop(labels = [\"Name\"], axis = 1, inplace = True)\ntrain_df.head()","d0c5d88f":"train_df[\"FamSize\"] = train_df[\"SibSp\"] + train_df[\"Parch\"] + 1\ntrain_df.head()","f241d2bf":"g = sns.factorplot(x = \"FamSize\", y = \"Survived\", data = train_df, kind = \"bar\")\ng.set_ylabels(\"Survival\")\nplt.show()","e9fc55ba":"train_df[\"family_size\"] = [1 if i < 5 else 0 for i in train_df[\"FamSize\"]]\ntrain_df.head(10)","98792ac7":"sns.countplot(x = \"family_size\", data = train_df)\nplt.show()","b53be62d":"g = sns.factorplot(x = \"family_size\", y = \"Survived\", data = train_df, kind = \"bar\")\ng.set_ylabels(\"Survival\")\nplt.show()","63b29a24":"train_df = pd.get_dummies(train_df, columns= [\"family_size\"])\ntrain_df.head()","e90d0f7b":"train_df = pd.get_dummies(train_df, columns=[\"Embarked\"])\ntrain_df.head()","01a5793d":"train_df[\"Pclass\"] = train_df[\"Pclass\"].astype(\"category\")\ntrain_df = pd.get_dummies(train_df, columns= [\"Pclass\"])\ntrain_df.head()","5a7a9796":"train_df[\"Sex\"] = train_df[\"Sex\"].astype(\"category\")\ntrain_df = pd.get_dummies(train_df, columns=[\"Sex\"])\ntrain_df.head()","d0bae106":"train_df.drop(labels = [\"PassengerId\", \"Cabin\", \"Ticket\"], axis = 1, inplace = True)\ntrain_df.columns","8712c4cd":"X = train_df.drop([\"Survived\"],axis=1)\ny = train_df[\"Survived\"]","8034f0cc":"X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size=0.33, random_state=111)","560e46a5":"log_reg = LogisticRegression(solver='lbfgs', multi_class=\"ovr\")\nlog_reg.fit(X_train, y_train)\ntrain_accuracy = log_reg.score(X_train, y_train)\ntest_accuracy = log_reg.score(X_test, y_test)\n\nprint('One-vs.-Rest', '-'*30, \n      'Accuracy on Train Data : {:.2f}'.format(train_accuracy), \n      'Accuracy on Test Data  : {:.2f}'.format(test_accuracy), sep='\\n')","2d4cf52a":"log_reg_mnm = LogisticRegression(multi_class='multinomial', solver='lbfgs')\nlog_reg_mnm.fit(X_train, y_train)\n\ntrain_accuracy = log_reg_mnm.score(X_train, y_train)\ntest_accuracy = log_reg_mnm.score(X_test, y_test)\n\nprint('Multinomial (Softmax)', '-'*20, \n      'Accuracy on Train Data : {:.2f}'.format(train_accuracy), \n      'Accuracy on Test Data  : {:.2f}'.format(test_accuracy), sep='\\n')","eea6f255":"from sklearn.metrics import confusion_matrix , accuracy_score\n\ny_pred = log_reg.predict(X_test)\n\n# Print the prediction accuracy\nprint(accuracy_score(y_test, y_pred))\nconfusion_matrix(y_test, y_pred)","1e0a0369":"y_pred = log_reg_mnm.predict(X_test)\n\n# Print the prediction accuracy\nprint(accuracy_score(y_test, y_pred))\nconfusion_matrix(y_test, y_pred)","41592653":"train_df_len = len(train_df)\ntest = train_df[:418]\ntest.drop(labels = [\"Survived\"],axis = 1, inplace = True)\ntest_PassengerId = test_df[\"PassengerId\"]","c3c887b0":"test_survived = pd.Series(log_reg.predict(test), name = \"Survived\").astype(int)\nresults = pd.concat([test_PassengerId, test_survived],axis = 1)\nresults.to_csv(\"titanic.csv\", index = False)\nresults","748d0046":"# Tukey Method for Outlier Detection","9627e038":"## Family","4559265e":"As you can see top that markdown, we have 177 null value for \"Age\", 687 null value for \"Cabin\" and 2 null value for \"Embarked\".","263e1d82":"## Sex","fab96121":"# Filling Missing Values","acda337c":"As you can see in the boxplot chart, first class passenger's median value > second class passenger's median value > third class passenger's median value. We can use that feature for prediction.","941a7dd9":"# Outliers","cdc0308e":"We can see that who had better class in Titanic, they had more chance to stay alive.","99ce84bb":"I filled missing age values with median values of who have the same SibSp, Parch, Pclass. Some combinations for this features didn't have any match, so I filled them with ages median value.","cd179c57":"We can also use SipSp and Parch for prediction of age because that box plot shows that these features seem to have correlation with age. Let's look at to heatmap to see if they are correlated.","10dbb24d":"## Embarked","425edd5e":"What is completely obvious is female has more chance to stay alive in Titanic.","1bd924cd":"Passengers with id numbers 60 and 821 paid 80 for tickets. We can see in the boxplot, 80 is in the whiskers for \"C\". Thus, we will fill that missing values with \"C\".","31d880da":"## Age","b557ea5b":"We can see that in the data frame who has first class and fare 80 is just in C. To be sure that we can fill the missing values with \"C\", we will check boxplots.","0e877a2f":"## Variable Description\n1. PassengerId: Unique ID number to each passenger\n2. Survived: Passenger survive(1) or died(0)\n3. Pclass: Passenger class\n4. Name: Name\n5. Sex: Gender of passenger\n6. Age: Age of passenger\n7. SibSp: Number of siblings\/spouses\n8. Parch: Number of parents\/children\n9. Ticket: Ticket number\n10. Fare: Amount of money spent on ticket\n11. Cabin: Cabin category\n12. Embarked: Port where passenger embarked (C = Cherbourg, Q = Queenstown, S = Southampton)","eea6afe1":"## Drop PassengerID, Cabin and Ticket","2db61a0e":"## Embarked","5d625671":"# Pclass","d3f5cbda":"We have missing values in 'Age', 'Cabin', 'Embarked' columns. Let's dig deep into that columns and see which column has how many null value.","62d7f924":"We can also see that in heatmap these features we chose have correlation with age. Everything seems to ready for age prediction, so let's continue with it.","45e64d49":"# Missing Values","22c67b05":"# Feature Engineering","03112461":"## Title","9e4c3373":"Our two models work with accuracy score of 83%. That looks pretty satisfying and when we check our precision score that looks enough. Long story short, Our model is totally enough","443e7913":"It's not looking like there is a meaningful difference between median values for male and female. Thus, I won't use it for age prediction to fill missing values.","f5439e96":"## Prediction"}}