{"cell_type":{"0ab6e41b":"code","ccf1256a":"code","50c67b5f":"code","8e20d45c":"code","18583817":"code","5e60e023":"code","00b33bba":"code","fad3f7e2":"code","0c249058":"code","916845c7":"code","944107b3":"code","8687be7f":"code","aacb7330":"code","dbce2bb1":"code","1a4ec6ab":"code","e17cfda8":"code","08431b2c":"code","d681bf67":"code","4cf4a848":"code","62c570e1":"code","f2a5293e":"code","2f04b916":"code","b9970c82":"code","6ac52388":"code","441a91a9":"code","50e15dd2":"code","b745dcc4":"code","8b9402f2":"code","2d8c2bf4":"markdown","a9fbbb71":"markdown"},"source":{"0ab6e41b":"import os\nimport numpy as np\nimport pandas as pd\nimport time\nimport copy\nimport cv2\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\nimport torchvision\nimport torch.utils.data as data\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport PIL\nfrom PIL import Image\nfrom torch.utils.data import DataLoader\n\nfrom sklearn.model_selection import train_test_split\n\nimport warnings\nwarnings.filterwarnings('ignore')","ccf1256a":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","50c67b5f":"data_path = \"\/kaggle\/input\/road-signs-recognition\/\"\ntrain_ann_path = os.path.join(data_path,'train.csv')\ntrain_path = os.path.join(data_path, 'train\/train\/')","8e20d45c":"train_df = pd.read_csv(train_ann_path)","18583817":"train, val = train_test_split(train_df, test_size=0.15, random_state=13, stratify=train_df.class_number)","5e60e023":"train.reset_index(inplace=True, drop=True)\nval.reset_index(inplace=True, drop=True)","00b33bba":"num_classes = 67\n\nbatch_size = 32\n\nnum_epochs = 12\n\ninput_size = 224","fad3f7e2":"def train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs=5):\n    since = time.time()\n\n    val_acc_history = []\n    \n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}'.format(epoch, num_epochs - 1))\n        print(f\"lr: {scheduler.optimizer.param_groups[0]['lr']}\")\n        print('-' * 10)\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train() \n            else:\n                model.eval() \n\n            running_loss = 0.0\n            running_corrects = 0\n\n            for inputs, labels, _ in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                optimizer.zero_grad()\n\n                with torch.set_grad_enabled(phase == 'train'): \n                    outputs = model(inputs)\n                    loss = criterion(outputs, labels)\n                    _, preds = torch.max(outputs, 1)\n\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                        scheduler.step()\n\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n            epoch_loss = running_loss \/ len(dataloaders[phase].dataset)\n            epoch_acc = running_corrects.double() \/ len(dataloaders[phase].dataset)\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n            if phase == 'val':\n                val_acc_history.append(epoch_acc)\n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    model.load_state_dict(best_model_wts)\n    return model, val_acc_history","0c249058":"def set_parameter_requires_grad(model, feature_extracting):\n    if feature_extracting:\n        for param in model.parameters():\n            param.requires_grad = False","916845c7":"def initialize_model(num_classes, feature_extract=True, use_pretrained=True):\n    model_ft = models.resnet18(pretrained=use_pretrained)\n    set_parameter_requires_grad(model_ft, feature_extract)\n    num_ftrs = model_ft.fc.in_features\n    model_ft.fc = nn.Linear(num_ftrs, num_classes)\n    \n    for params in model_ft.layer4.parameters():\n        params.requires_grad = True\n        \n    for params in model_ft.layer3.parameters():\n        params.requires_grad = True\n        \n    return model_ft","944107b3":"model = initialize_model(num_classes)","8687be7f":"class SignsDataset(data.Dataset):\n    \n    def __init__(self, root, flist, transform=None):\n        \"\"\"\n        root: path to images\n        imlist: pandas DataFrame with columns file_name, class\n        transform: torchvision transform applied to every image\n        \"\"\"\n        self.root  = root\n        self.flist = flist \n        self.transform = transform\n        \n    def __getitem__(self, index):\n        impath, target = self.flist.loc[index] \n\n        full_imname = os.path.join(self.root, impath)\n        assert os.path.exists(full_imname), f'No file {full_imname}' \n\n        \n        image = cv2.imread(full_imname)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        image = self.transform(image=image)[\"image\"]\n        \n        return image, target, impath\n    \n    def __len__(self):\n        return len(self.flist)","aacb7330":"train_transformation = A.Compose([A.SmallestMaxSize(max_size=224),\n                                  A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),\n                                  A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n                                  A.RandomBrightnessContrast(p=0.5),\n                                  A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n                                  ToTensorV2()\n                                 ])\n        \nval_transformation =  A.Compose([A.SmallestMaxSize(max_size=224),\n                                 A.CenterCrop(height=224, width=224),\n                                 A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n                                 ToTensorV2()\n                                ])","dbce2bb1":"train_dataset = SignsDataset(root=train_path, flist=train, transform=train_transformation)\nval_dataset = SignsDataset(root=train_path, flist=val, transform=val_transformation)\n\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=8, shuffle=True, pin_memory=True)                          \nval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, num_workers=8, pin_memory=True)","1a4ec6ab":"dataloaders = {'train': train_dataloader, 'val': val_dataloader}","e17cfda8":"model = model.to(device)","08431b2c":"# \u0423 \u0432\u0441\u0435\u0445 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u043a\u0440\u043e\u043c\u0435 \u043f\u0435\u0440\u0435\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u043d\u043d\u044b\u0445 \u043f\u043e\u043b\u043d\u043e\u0441\u0432\u044f\u0437\u043d\u044b\u0445 \u0441\u043b\u043e\u0435\u0432 \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 initialize_model\n# param.requires_grad \u0432\u044b\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u043e \u0432 False \u043c\u0435\u0442\u043e\u0434\u043e\u043c set_parameter_requires_grad\nparams_to_update = [param for name, param in model.named_parameters() if param.requires_grad]","d681bf67":"for name, param in model.named_parameters():\n    if param.requires_grad: print(name)","4cf4a848":"optimizer = optim.Adam(params_to_update, lr=0.03)\ncriterion = nn.CrossEntropyLoss()\nscheduler = CosineAnnealingLR(optimizer, T_max=int(len(train_dataset)\/batch_size + 1)*num_epochs)","62c570e1":"model, hist = train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs=num_epochs)","f2a5293e":"state = {\n        'state_dict': model.state_dict(),\n        'optimizer' : optimizer.state_dict()\n        }\n\ntorch.save(state, '\/kaggle\/working\/resnet18.pth')","2f04b916":"def prediction(model, dataloader):\n    filename = []\n    class_number = []\n    for inputs, _, path in dataloader:\n        inputs = inputs.to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n        filename.extend(path)\n        class_number.extend(preds.tolist())\n            \n    return pd.DataFrame(list(zip(filename, class_number)), columns=['filename', 'class_number'])","b9970c82":"test_path =  os.path.join(data_path, 'test\/test\/')\n\ntest = pd.DataFrame(os.listdir(test_path), columns=['filename']) \ntest['class_number'] = np.nan\n\ntest_dataset = SignsDataset(root=test_path, flist=test, transform=val_transformation)\n\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, num_workers=8, pin_memory=True)","6ac52388":"checkpoint = torch.load('\/kaggle\/working\/resnet18.pth')\nmodel_state_dict = checkpoint['state_dict']","441a91a9":"model.load_state_dict(model_state_dict)\nmodel = model.to(device)","50e15dd2":"result = prediction(model, test_dataloader)","b745dcc4":"result.head()","8b9402f2":"result.to_csv('\/kaggle\/working\/resnet18sv6.csv', index=False)","2d8c2bf4":"**\u0427\u0442\u0435\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445**","a9fbbb71":"\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435"}}