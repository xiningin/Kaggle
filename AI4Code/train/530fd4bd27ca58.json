{"cell_type":{"ec0f2aac":"code","18a812ff":"code","b9b63964":"code","2b941eb5":"code","639acd2e":"code","c1ffea6f":"code","807b0df4":"code","539f735b":"code","bf8b2568":"code","d4339214":"code","b4f409c7":"code","ba55e1d3":"code","8df22123":"code","9c7bbf48":"code","0c2c76f5":"code","4f4462cb":"code","214b3167":"code","ae3761f7":"code","bc28d3f5":"markdown","89359ab5":"markdown","63fefe0b":"markdown","e3da774e":"markdown","e0223197":"markdown","d0cacc47":"markdown","04b09644":"markdown","4db89806":"markdown","1543c73b":"markdown","6b7eaf8d":"markdown","47a49a0f":"markdown","047f81c7":"markdown","1da894a8":"markdown","d574f409":"markdown","63f0053c":"markdown"},"source":{"ec0f2aac":"import pandas as pd\nimport pandas_profiling as pp\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nimport os\n\n# Metrics\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n# Validation\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold\nfrom sklearn.pipeline import Pipeline\n\n# Tuning\nfrom sklearn.model_selection import GridSearchCV\n\n# Feature Extraction\nfrom sklearn.feature_selection import RFE\n\n# Preprocessing\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, Normalizer, Binarizer\n\n# Models\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Ensembles\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\nsns.set(style='whitegrid')\nplt.style.use('seaborn-darkgrid')\n\ndf_train = pd.read_csv('..\/input\/learn-together\/train.csv')\ndf_test = pd.read_csv('..\/input\/learn-together\/test.csv')\n\n\n\n\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","18a812ff":"df_train.head()","b9b63964":"print('Train size: ',df_train.shape)\nprint('Test size: ', df_test.shape)","2b941eb5":"df_train.columns","639acd2e":"df_train.info() #you can check the dtypes using this method df_train.dtypes or df_train.info()","c1ffea6f":"df_train.isna().sum()","807b0df4":"df_train.duplicated().sum()","539f735b":"df_train.describe()","bf8b2568":"colormap = plt.cm.RdBu\nplt.figure(figsize=(50,35))\nplt.title('Pearson Correlation of Features', y=1.05, size=50)\nsns.heatmap(df_train.corr(),linewidths=0.1, vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)","d4339214":"df_train.hist(figsize=(20,30));","b4f409c7":"target = df_train.Cover_Type.value_counts()\nsns.countplot(x='Cover_Type', data=df_train)\nplt.title('Class Distribution');\nprint(target)","ba55e1d3":"pp.ProfileReport(df_train)","8df22123":"lst = ['Id', 'Cover_Type']\n\nX = df_train.drop(lst, axis=1)\ny = df_train.Cover_Type\ntest_X = df_test.drop('Id', axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                   y,\n                                                   test_size=0.3,\n                                                   random_state=0)\n\nprint('X_train: ',X_train.shape)\nprint('X_test: ',X_test.shape)\nprint('y_train: ',y_train.shape)\nprint('y_test: ',y_test.shape)","9c7bbf48":"from sklearn.model_selection import KFold\nmodels = []\nmodels.append(( ' LR ' , LogisticRegression()))\nmodels.append(( ' LDA ' , LinearDiscriminantAnalysis()))\nmodels.append(( ' KNN ' , KNeighborsClassifier()))\nmodels.append(( ' NB ' , GaussianNB()))\nmodels.append(( ' SVM ' , SVC()))\n\nresults = []\nnames = []\n\nfor name, model in models:\n    Kfold = KFold(n_splits=10, random_state=0)\n    cv_results = cross_val_score(model, X_train, y_train, cv=Kfold, scoring= 'accuracy')\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std());\n    print(msg)","0c2c76f5":"from sklearn.model_selection import KFold\nmodels = []\nmodels.append(( 'Adab' , AdaBoostClassifier()))\nmodels.append(( 'Bagging' , BaggingClassifier()))\nmodels.append(( 'GBC' , GradientBoostingClassifier()))\nmodels.append(( 'RF' , RandomForestClassifier()))\n\n\nresults = []\nnames = []\n\nfor name, model in models:\n    Kfold = KFold(n_splits=10, random_state=0)\n    cv_results = cross_val_score(model, X_train, y_train, cv=Kfold, scoring= 'accuracy')\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std());\n    print(msg)","4f4462cb":"scaler = StandardScaler().fit(X_train)\nX_train = scaler.transform(X_train)\n\n\nmodel = RandomForestClassifier()\nparam_grid = { \n    'n_estimators': [10,20,50,100],\n    'max_features': ['auto', 'sqrt', 'log2']\n}\n\nkfold = KFold(n_splits=10, random_state=0)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy', cv=kfold)\ngrid_result = grid.fit(X_train, y_train)\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n\n\n","214b3167":"\nscaler = StandardScaler().fit(X_train)\nX_train = scaler.transform(X_train)\nX_test =scaler.transform(X_test)\nRF = RandomForestClassifier(n_estimators=100, max_features='auto').fit(X_train,y_train)\ny_pred = RF.predict(test_X)\n\n\n","ae3761f7":"y_pred_1 = RF.predict(test_X)\nsub = pd.DataFrame({'ID': df_test.Id,\n                       'TARGET': y_pred_1})\nsub.to_csv('submission.csv')","bc28d3f5":"# Exploratory Data Analysis\n\n* This is another method of analysing our data you can choose what method you want to use either this or the hard coding techniques you can check the documentation here: https:\/\/pandas-profiling.github.io\/pandas-profiling\/docs\/","89359ab5":"# Conclusion\n> In our training set we got a 85% accuracy while in our testing set we got just only 74% you can improve the accuracy by decreasing the features you can use (RFE, Principle component analysis, and features importance techniques to help you to choose what features you need to use) also using other preprocessing techniques like ( MinMaxScaler, StandardScaler, Normalizer, Binarizer ). If you guys have other techniques to help to improve the accuracy you can share your kernel in the comment section i want to learn more.\n\n\n# Summary\n\n* **Problem** (Classify forest types based on information about the area)\n* **Evaluation Algorithms** ( Knn got the highest accuracy )\n* **Esemble Methods** ( Randomforest much better than other algorithms )\n* **Algorithm Tuning** ( using n_estimators =100 and max_Features = auto, the accuracy of random forest improve by 3 %)\n\n","63fefe0b":"# Statistical Summary","e3da774e":"# Let's build our model","e0223197":"# Splitting our data to training and validation set\n* We will split our data to training set(70%), validation set(30%). We need to split our data for model evaluation to know how good the model is.\n\n# Target Variable\n\n* Our target variable is Cover_Type\n\n**1** - Spruce\/Fir\n\n**2** - Lodgepole Pine\n\n**3** - Ponderosa Pine\n\n**4** - Cottonwood\/Willow\n\n**5** - Aspen\n\n**6** - Douglas-fir\n\n**7** - Krummholz","d0cacc47":"![](https:\/\/wi-images.condecdn.net\/image\/p9bvjVrG32E\/crop\/2040\/f\/upvote-default.png)","04b09644":"# Checking if we have a duplicate data","4db89806":"# Parameter tuning","1543c73b":"# Improve Performance with Ensembles and Algorithm Tuning\n","6b7eaf8d":"# Checking if we have a balance dataset","47a49a0f":"# Checking the data type of each attribute\n\n* All of our attributes are integer ( int64 )\n","047f81c7":"# Data Fields\n\n* **Elevation** - Elevation in meters\n* **Aspect** - Aspect in degrees azimuth\n* **Slope** - Slope in degrees\n* **Horizontal_Distance_To_Hydrology** - Horz Dist to nearest surface water features\n* **Vertical_Distance_To_Hydrology** - Vert Dist to nearest surface water features\n* **Horizontal_Distance_To_Roadways** - Horz Dist to nearest roadway\n* **Hillshade_9am (0 to 255 index)** - Hillshade index at 9am, summer solstice\n* **Hillshade_Noon (0 to 255 index)** - Hillshade index at noon, summer solstice\n* **Hillshade_3pm (0 to 255 index)** - Hillshade index at 3pm, summer solstice\n* **Horizontal_Distance_To_Fire_Points** - Horz Dist to nearest wildfire ignition points\n* **Wilderness_Area (4 binary columns, 0 = absence or 1 = presence)** - Wilderness area designation\n* **Soil_Type (40 binary columns, 0 = absence or 1 = presence)** - Soil Type designation\n* **Cover_Type (7 types, integers 1 to 7)** - Forest Cover Type designation\n\nThe wilderness areas are:\n\n**1** - Rawah Wilderness Area\n\n**2** - Neota Wilderness Area\n\n**3** - Comanche Peak Wilderness Area\n\n**4** - Cache la Poudre Wilderness Area\n\n","1da894a8":"\n# Predicting types of trees in an area based on various geographic features \n\n   Arapaho National Forest is a National Forest located in north-central Colorado, United States. The facility is managed jointly with the Roosevelt National Forest and the Pawnee National Grassland from the United States Forest Service office in Fort Collins, Colorado. It has a wildlife refuge which manages a protection for all birds and mammals. The combined facility of 1,730,603 acres (2,704.07 sq mi, or 7,420.35 km\u00b2) is denoted as ARP (Arapaho, Roosevelt, Pawnee) by the Forest Service. Separately, Arapaho National Forest consists of 723,744 acres (1,130.85 sq mi, or 2,928.89 km\u00b2).\n\n\n![](https:\/\/www.forestryengland.uk\/sites\/default\/files\/styles\/forest_slide_wide_desk\/public\/media\/woodland3.jpg?h=78566fdd&itok=HXeDhGX0)\n\n    \n   The forest is located in the Rocky Mountains, straddling the continental divide in the Front Range west of Denver. It was established on July 1, 1908 by President Theodore Roosevelt and named for the Arapaho tribe of Native Americans which previously inhabited the Colorado Eastern Plains. The forest includes part of the high Rockies and river valleys in the upper watershed of the Colorado River and South Platte River. The forest is largely in Grand and Clear Creek counties, but spills over into neighboring (in descending order of land area) Gilpin, Park, Routt, Jackson, and Jefferson counties. There are local ranger district offices located in Granby and Idaho Springs.","d574f409":"# Dimensions of Data\n\n* We have 15,120 rows and 56 columns in our training set while in our testing set we have 565,892 rows and 55 columns so the problem here is we have too many rows and our algorithms may take too long also in columns we have 56 this means that some algorithms can be distracted or suffer poor performance due to the curse of dimensionality.\n\n","63f0053c":"# Checking for NaN Values\n* We don't have any Nan values data."}}