{"cell_type":{"31f0cc4e":"code","faac9749":"code","44eafbd4":"code","a271ccc5":"code","68d6d5f2":"code","91f2fca6":"code","76b6dd76":"code","3b825645":"code","1f295f21":"code","7d6ea006":"code","30260c76":"code","0d64df67":"code","41f2c659":"code","798efbcd":"code","b1132642":"markdown","32edaaed":"markdown","d5125638":"markdown","f854e7ea":"markdown","3254981c":"markdown","f84d4ee0":"markdown","0d96adc4":"markdown","79fe9909":"markdown"},"source":{"31f0cc4e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport time\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","faac9749":"import pandas as pd\n\n# Data Visualizations\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport seaborn as sns\n\n# Metrics\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report","44eafbd4":"df = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndf.head()","a271ccc5":"df.describe()","68d6d5f2":"# The classes are heavily Imbalanced\ndf['Class'].value_counts()","91f2fca6":"# Plotting Distribution of Amount and Time Feature\nfig, ax = plt.subplots(1, 2, figsize=(18,4))\n\namount_val = df['Amount'].values\ntime_val = df['Time'].values\n\nsns.distplot(amount_val, ax=ax[0], color='r')\nax[0].set_title('Distribution of Transaction Amount', fontsize=14)\nax[0].set_xlim([min(amount_val), max(amount_val)])\n\nsns.distplot(time_val, ax=ax[1], color='b')\nax[1].set_title('Distribution of Transaction Time', fontsize=14)\nax[1].set_xlim([min(time_val), max(time_val)])\n\nplt.show()","76b6dd76":"# Scaling the dataset with RobustScaler as it is less prone to outliers, One of the method for outlier handling\n# Or one can remove outliers too, but not only performing the influence of outliers on output 'Class'\n\n# 'Amount' feature might have some outliers independent of the class features judging from the distribution plots\nfrom sklearn.preprocessing import RobustScaler\n\nrob_scaler = RobustScaler()\n\ndf['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\ndf['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n\ndf.drop(['Time','Amount'], axis=1, inplace=True)","3b825645":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\nprint('No Frauds', round(df['Class'].value_counts()[0]\/len(df) * 100,2), '% of the dataset')\nprint('Frauds', round(df['Class'].value_counts()[1]\/len(df) * 100,2), '% of the dataset')\n\nX = df.drop('Class', axis=1)\ny = df['Class']\n\n# Used to get train_index and test_index\nsss = StratifiedKFold(n_splits=2, random_state=None, shuffle=False)\n\nfor train_index, test_index in sss.split(X, y):\n    print(\"Train:\", train_index, \"Test:\", test_index)\n    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]\n\n# Check the Distribution of the labels\n# Turn into an array\noriginal_Xtrain = original_Xtrain.values\noriginal_Xtest = original_Xtest.values\noriginal_ytrain = original_ytrain.values\noriginal_ytest = original_ytest.values\n\n# See if both the train and test label distribution are similarly distributed\ntrain_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=True)\ntest_unique_label, test_counts_label = np.unique(original_ytest, return_counts=True)\nprint('-' * 100)\n\nprint('Label Distributions: \\n')\nprint(train_counts_label\/ len(original_ytrain))\nprint(test_counts_label\/ len(original_ytest))","1f295f21":"# Under-Sampling and plotting the Correlation matrix\n\n# Shuffle\ndf = df.sample(frac=1)\n\n# amount of fraud classes 492 rows.\nfraud_df = df.loc[df['Class'] == 1]\nnon_fraud_df = df.loc[df['Class'] == 0][:492] # UNDERSAMPLING\n\nnormal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n\n# Shuffle dataframe rows\nnew_df = normal_distributed_df.sample(frac=1, random_state=42)\n\n\n# Correlation Matrix of Sub-sampled data\nsub_sample_corr = new_df.corr()\nsns.heatmap(sub_sample_corr, cmap='coolwarm_r', annot_kws={'size':20})","7d6ea006":"# Outlier removal\n# Selecting only features between IQR 25 to 75\n\n# V14 Removing Outliers (Highest Negative Correlated with Labels)\nv14_fraud = new_df['V14'].loc[new_df['Class'] == 1].values\nq25, q75 = np.percentile(v14_fraud, 25), np.percentile(v14_fraud, 75)\nprint('Quartile 25: {} | Quartile 75: {}'.format(q25, q75))\nv14_iqr = q75 - q25\nprint('iqr: {}'.format(v14_iqr))\n\nv14_cut_off = v14_iqr * 1.5\nv14_lower, v14_upper = q25 - v14_cut_off, q75 + v14_cut_off\nprint('Cut Off: {}'.format(v14_cut_off))\nprint('V14 Lower: {}'.format(v14_lower))\nprint('V14 Upper: {}'.format(v14_upper))\n\noutliers = [x for x in v14_fraud if x < v14_lower or x > v14_upper]\nprint('Feature V14 Outliers for Fraud Cases: {}'.format(len(outliers)))\nprint('V10 outliers:{}'.format(outliers))\n\nnew_df = new_df.drop(new_df[(new_df['V14'] > v14_upper) | (new_df['V14'] < v14_lower)].index)\nprint('--' * 44)\n\n# V12 removing outliers from fraud transactions\nv12_fraud = new_df['V12'].loc[new_df['Class'] == 1].values\nq25, q75 = np.percentile(v12_fraud, 25), np.percentile(v12_fraud, 75)\nv12_iqr = q75 - q25\n\nv12_cut_off = v12_iqr * 1.5\nv12_lower, v12_upper = q25 - v12_cut_off, q75 + v12_cut_off\nprint('V12 Lower: {}'.format(v12_lower))\nprint('V12 Upper: {}'.format(v12_upper))\noutliers = [x for x in v12_fraud if x < v12_lower or x > v12_upper]\nprint('V12 outliers: {}'.format(outliers))\nprint('Feature V12 Outliers for Fraud Cases: {}'.format(len(outliers)))\nnew_df = new_df.drop(new_df[(new_df['V12'] > v12_upper) | (new_df['V12'] < v12_lower)].index)\nprint('Number of Instances after outliers removal: {}'.format(len(new_df)))\nprint('--' * 44)\n\n\n# outliers V10 Feature\nv10_fraud = new_df['V10'].loc[new_df['Class'] == 1].values\nq25, q75 = np.percentile(v10_fraud, 25), np.percentile(v10_fraud, 75)\nv10_iqr = q75 - q25\n\nv10_cut_off = v10_iqr * 1.5\nv10_lower, v10_upper = q25 - v10_cut_off, q75 + v10_cut_off\nprint('V10 Lower: {}'.format(v10_lower))\nprint('V10 Upper: {}'.format(v10_upper))\noutliers = [x for x in v10_fraud if x < v10_lower or x > v10_upper]\nprint('V10 outliers: {}'.format(outliers))\nprint('Feature V10 Outliers for Fraud Cases: {}'.format(len(outliers)))\nnew_df = new_df.drop(new_df[(new_df['V10'] > v10_upper) | (new_df['V10'] < v10_lower)].index)\nprint('Number of Instances after outliers removal: {}'.format(len(new_df)))","30260c76":"# New_df is from the random undersample data (fewer instances)\nX = new_df.drop('Class', axis=1)\ny = new_df['Class']\n\n\n# Dimentionality Reduction ------\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD\n\n\n# T-SNE Implementation\nt0 = time.time()\nX_reduced_tsne = TSNE(n_components=2, random_state=42).fit_transform(X.values)\nt1 = time.time()\nprint(\"T-SNE took {:.2} s\".format(t1 - t0))\n\n# PCA Implementation\nt0 = time.time()\nX_reduced_pca = PCA(n_components=2, random_state=42).fit_transform(X.values)\nt1 = time.time()\nprint(\"PCA took {:.2} s\".format(t1 - t0))\n\n# TruncatedSVD\nt0 = time.time()\nX_reduced_svd = TruncatedSVD(n_components=2, algorithm='randomized', random_state=42).fit_transform(X.values)\nt1 = time.time()\nprint(\"Truncated SVD took {:.2} s\".format(t1 - t0))\n\n\n\n\n# Visualize Clusters ------\n\nf, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24,6))\n# labels = ['No Fraud', 'Fraud']\nf.suptitle('Clusters using Dimensionality Reduction', fontsize=14)\n\n\nblue_patch = mpatches.Patch(color='#0A0AFF', label='No Fraud')\nred_patch = mpatches.Patch(color='#AF0000', label='Fraud')\n\n\n# t-SNE scatter plot\nax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\nax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\nax1.set_title('t-SNE', fontsize=14)\n\nax1.grid(True)\n\nax1.legend(handles=[blue_patch, red_patch])\n\n\n# PCA scatter plot\nax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\nax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\nax2.set_title('PCA', fontsize=14)\n\nax2.grid(True)\n\nax2.legend(handles=[blue_patch, red_patch])\n\n# TruncatedSVD scatter plot\nax3.scatter(X_reduced_svd[:,0], X_reduced_svd[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\nax3.scatter(X_reduced_svd[:,0], X_reduced_svd[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\nax3.set_title('Truncated SVD', fontsize=14)\n\nax3.grid(True)\n\nax3.legend(handles=[blue_patch, red_patch])\n\nplt.show()","0d64df67":"# Undersampling before cross validating (prone to overfit)\nX = new_df.drop('Class', axis=1)\ny = new_df['Class']\n\n# Our data is already scaled we should split our training and test sets\nfrom sklearn.model_selection import train_test_split\n\n# This is explicitly used for undersampling.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Turn the values into an array for feeding the classification algorithms.\nX_train = X_train.values\nX_test = X_test.values\ny_train = y_train.values\ny_test = y_test.values\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nclassifiers = {\n    \"LogisiticRegression\": LogisticRegression(),\n    \"KNearest\": KNeighborsClassifier(),\n    \"Support Vector Classifier\": SVC(),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier()\n}\n\nfrom sklearn.model_selection import cross_val_score\n\n\nfor key, classifier in classifiers.items():\n    classifier.fit(X_train, y_train)\n    training_score = cross_val_score(classifier, X_train, y_train, cv=5, scoring='f1')\n    print(\"Classifiers: \", classifier.__class__.__name__, \"has a\", round(training_score.mean(), 2) * 100, \"f1 score\")","41f2c659":"from imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\n\n\nprint('Length of X (train): {} | Length of y (train): {}'.format(len(original_Xtrain), len(original_ytrain)))\nprint('Length of X (test): {} | Length of y (test): {}'.format(len(original_Xtest), len(original_ytest)))\n\n# List to append the score and then find the average\naccuracy_lst = []\nprecision_lst = []\nrecall_lst = []\nf1_lst = []\nauc_lst = []\n\n# Classifier with optimal parameters\n# log_reg_sm = grid_log_reg.best_estimator_\nlog_reg_sm = LogisticRegression()\n\n\nlog_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n\nrand_log_reg = RandomizedSearchCV(LogisticRegression(), log_reg_params, n_iter=4)\n\n\n# Implementing SMOTE Technique \n# Cross Validating the right way\n# Parameters\n\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\n\nfor train, test in sss.split(original_Xtrain, original_ytrain):\n    pipeline = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), rand_log_reg) # SMOTE happens during Cross Validation not before.\n    model = pipeline.fit(original_Xtrain[train], original_ytrain[train])\n    best_est = rand_log_reg.best_estimator_\n    prediction = best_est.predict(original_Xtrain[test])\n    \n    accuracy_lst.append(pipeline.score(original_Xtrain[test], original_ytrain[test]))\n    precision_lst.append(precision_score(original_ytrain[test], prediction))\n    recall_lst.append(recall_score(original_ytrain[test], prediction))\n    f1_lst.append(f1_score(original_ytrain[test], prediction))\n    auc_lst.append(roc_auc_score(original_ytrain[test], prediction))\n    \nprint('-' * 45)\nprint('')\nprint(\"accuracy: {}\".format(np.mean(accuracy_lst)))\nprint(\"precision: {}\".format(np.mean(precision_lst)))\nprint(\"recall: {}\".format(np.mean(recall_lst)))\nprint(\"f1: {}\".format(np.mean(f1_lst)))\nprint('-' * 45)","798efbcd":"labels = ['No Fraud', 'Fraud']\nsmote_prediction = best_est.predict(original_Xtest)\nprint(classification_report(original_ytest, smote_prediction, target_names=labels))","b1132642":"# Outlier Removal\nKeeping values between IQR 25 - 75","32edaaed":"# Different Ways to Handle the Class Imbalance problem in Tabular Dataset:\n\nThe following ways are mentioned in a order of precedence.\n\n**>> For Under-sampling and Over-sampling remember that only sample Train dataset. Don't undersample or oversample cross-validation data and test data**\n\n## Under-Sampling:\n\nThe Main issue in under-sampling is loss in information and may lead to not accurate predictions for majority class.\n\n**Steps:**\n\n- The first thing we have to do is determine how imbalanced is our class (use \"value_counts()\" on the class column to determine the amount for each label)\n- Once we determine how many instances are considered fraud transactions (Fraud = \"1\") , we should bring the non-fraud transactions to the same amount as fraud transactions (assuming we want a 50\/50 ratio), this will be equivalent to 492 cases of fraud and 492 cases of non-fraud transactions.\n- After implementing this technique, we have a sub-sample of our dataframe with a 50\/50 ratio with regards to our classes. Then the next step we will implement is to shuffle the data to see if our models can maintain a certain accuracy everytime we run this script.\n\nWe then perform Dimensionality reduction(using t-SNE, PCA and TruncatedSVD) and see if classes visibly correctly clustered. **If classes are correctly clustered then we can be confident that our predictive model can perform good on this sub-sampled dataset**\n\n\n## Over-Sampling:\nTime for training model increases\nSynthetic Minority Over-sampling Technique(SMOTE) can only be used in data with non-encoded features\n\n## Some other Methods:\n- Class weights: We can pass class-weights based on the distribution as dictionary (key:value::label:weight) to `class_weight` argument to classifiers which are similar to sklearn API\n- **Preferred Algorithms:** Tree based Algorithms are more immune to Data Imbalance problem\n- During each step of training while traning a Wide Dense Classifier in Tensorflow, one can probalistically drop samples from majority class and maintain a mini-batch distribution uniform (i.e. in 50-50 ratio), ***class weights*** must not be simultaneously used in this method\n\n*(Reach out to me in comments sections, if more info. is needed on these other methods)*","d5125638":"# Basic EDA","f854e7ea":"# Undersampling and  Checking for Correlated features\n\n### Conclusion\n\n- Negative Correlations: V17, V14, V12 and V10 are negatively correlated\n- Positive Correlations: V2, V4, V11, and V19 are positively correlated ","3254981c":"### Dimentionality Reduction and then visualize Clusters\n\nWe first perform Dimensionality reduction(using t-SNE, PCA and TruncatedSVD) and see if classes visibly correctly clustered. **If classes are correctly clustered then we can be confident that our predictive model can perform good on this sub-sampled dataset**\n\n**Conclusion:**\nIn this below visualizations the clusters are distinct hence we can proceed with predictive models","f84d4ee0":"# Oversampling using SMOTE\n\nTime to train model is more in this case\n\nSteps:\n- We create synthetic samples of minority class","0d96adc4":"Creating a sub-sample of the dataset such that Class 0 and 1 have same counts using **Random Oversampling**, referring to methodology in [this](https:\/\/www.kaggle.com\/janiobachmann\/credit-fraud-dealing-with-imbalanced-datasets) notebook","79fe9909":"## This is a practice Notebook for Credit Card Fraud Detection referring to [this](https:\/\/www.kaggle.com\/janiobachmann\/credit-fraud-dealing-with-imbalanced-datasets) notebook"}}