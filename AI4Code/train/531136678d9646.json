{"cell_type":{"732aa693":"code","bf837e43":"code","6adfd081":"code","d3237aea":"code","38aa7106":"code","69585ea0":"code","11a677b2":"code","593d41b2":"code","a8b7256a":"code","4d940d5e":"code","c4f9c049":"code","fe46442e":"code","e877339d":"code","3e36bbaa":"code","54aa5762":"code","592b9b35":"code","8773d2c5":"code","fe0de89c":"code","4e0d7eed":"code","f7ce11c2":"code","466bbbf7":"code","ea401bce":"code","68543d20":"code","997ce4a9":"code","af8cc60b":"code","81f4d41b":"code","0fb49936":"code","f9ed2172":"code","c7c826ad":"code","b099f54e":"code","a5423a8c":"code","0ff577bf":"code","1ccaf345":"code","7a0ed2d4":"code","fc201e72":"code","67ca0fa0":"code","90dac844":"code","582200f0":"code","e7da3c9a":"code","0913a224":"code","d47839cd":"code","2ae2b5f4":"code","887605b7":"code","442dacad":"code","6bf8fba3":"markdown","eb4406e1":"markdown","461a59c3":"markdown","4cbc3ddd":"markdown","6cd87764":"markdown","6fbfba73":"markdown","31c6f952":"markdown","24ff9b22":"markdown","b62ca41e":"markdown","a971ac1c":"markdown","050252d8":"markdown","821c770f":"markdown","7d28567d":"markdown","d1f9fe62":"markdown","b42b0a22":"markdown","aa579ad4":"markdown","6b252971":"markdown","5f9bc781":"markdown","c3e04b0f":"markdown","2f739b4b":"markdown","b7dd9b86":"markdown","8a1af5a5":"markdown","820bd528":"markdown","989fefc7":"markdown","f5847404":"markdown","55cd75c0":"markdown","106602d5":"markdown","46802698":"markdown","6a3accdc":"markdown","83c35be9":"markdown","2a850f0c":"markdown","14deff98":"markdown","042531b6":"markdown","d9f8845a":"markdown","8bef48e8":"markdown","8ea61f8e":"markdown","cc9e17a5":"markdown"},"source":{"732aa693":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import Imputer \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n#importing all the liabriaries we will need to boost our model \nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","bf837e43":"hp_train = pd.read_csv('..\/input\/train.csv')\nhp_test = pd.read_csv('..\/input\/test.csv')\ntrain_length=1460\nsaleprice=pd.DataFrame(hp_train.iloc[:,-1])","6adfd081":"# #hp_train = pd.read_csv('https:\/\/raw.githubusercontent.com\/K1TS\/mypackage\/master\/train.csv')\n# #hp_test = pd.read_csv('https:\/\/raw.githubusercontent.com\/K1TS\/House-Prices-Advanced-Regression-Techniques\/master\/test.csv')\n#sumple_supmition =pd.read_csv('https:\/\/raw.githubusercontent.com\/K1TS\/House-Prices-Advanced-Regression-Techniques\/master\/sample_submission.csv')","d3237aea":"# We are Concatinating the two datasets so it can be easy to clean data at once for future engineering   \ndf_all=pd.concat([hp_train.drop(columns=['SalePrice']), hp_test])","38aa7106":"# The Shape Of The Two DataSets Combined\ndf_all.shape","69585ea0":"df_all.head()# viewing the dataset to avoid large output space","11a677b2":"#We Decided To Cut The Correlation Graph In Half Since The Upper Part Is Really Repetition Of the Lower Triangle  \ncorr_matrix = hp_train.corr()\nmask = np.zeros_like(corr_matrix, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nf, ax = plt.subplots(figsize=(25, 15))\nsns.heatmap(corr_matrix, mask=mask, vmax=0.8, vmin=0.05, annot=True);\n\n","593d41b2":"#COLUMNS THAT ARE CORRELATED TO THE SALES PRICE AND INVESLY CORRELATED TO SALES PRICE \ndf=pd.DataFrame(hp_train.corr()['SalePrice'].sort_values(ascending=False))\ndf.plot(kind='bar',figsize=(12,5),color='red')\nplt.title('Brief Correlation Graph of Features and Sales Price')\n\n\n","a8b7256a":"fig, axes = plt.subplots(nrows= 3,ncols = 3, figsize=(20,12))\n\naxes[0,0].scatter(hp_train['YearBuilt'], hp_train['SalePrice'], color='orange')\naxes[0,1].scatter(hp_train['GarageYrBlt'], hp_train['SalePrice'],  color='green')\naxes[0,2].scatter(hp_train['GrLivArea'], hp_train['SalePrice'], color='blue')\n\n\naxes[1,0].scatter(hp_train['TotRmsAbvGrd'], hp_train['SalePrice'],  color='orange')\naxes[1,1].scatter(hp_train['GarageCars'], hp_train['SalePrice'], color='green')\naxes[1,2].scatter(hp_train['GarageArea'], hp_train['SalePrice'],  color='blue')\n\naxes[2,0].scatter(hp_train['TotalBsmtSF'], hp_train['SalePrice'],  color='orange')\naxes[2,1].scatter(hp_train['1stFlrSF'], hp_train['SalePrice'], color='green')\naxes[2,2].scatter(hp_train['OverallQual'], hp_train['SalePrice'],  color='blue')\n\n#Naming Titles Of The Columns  \naxes[0,0].set_title('YearBuilt')\naxes[0,1].set_title('GarageYrBlt')\naxes[0,2].set_title('GrLivArea')\n\naxes[1,0].set_title('TotRmsAbvGrd')\naxes[1,1].set_title('GarageCars')\naxes[1,2].set_title('GarageArea')\n\n\naxes[2,0].set_title('TotalBsmtSF')\naxes[2,1].set_title('1stFlrSF')\naxes[2,2].set_title('OverallQual')","4d940d5e":"#hp_train.isnull().sum().sort_values(ascending = False).head(20)\n# columns that have NaN on the train dataset.\nis_null=df_all.isnull().sum().sort_values(ascending=False)\nNaN_train=(is_null[is_null>0])\ndict(NaN_train)\nNaN_train","c4f9c049":"#THIS ARE THE VISUALS  TO SHOW THE MISSING DATA IN OUR DATASET ..THE COLUMNS WE SORTED IN THE Visualization\nplt.figure(figsize=(15, 8))\nsns.barplot(NaN_train,NaN_train.index)\nplt.title('Missing  Data In The Dataset')\n","fe46442e":"\ndf_all[\"PoolQC\"] = df_all[\"PoolQC\"].fillna(\"None\")\n\ndf_all[\"MiscFeature\"] = df_all[\"MiscFeature\"].fillna(\"None\")\n\ndf_all[\"Alley\"] = df_all[\"Alley\"].fillna(\"None\")\n\ndf_all[\"Fence\"] = df_all[\"Fence\"].fillna(\"None\")\n\ndf_all[\"FireplaceQu\"] = df_all[\"FireplaceQu\"].fillna(\"None\")\n\n#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\ndf_all[\"LotFrontage\"] = df_all.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\n\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    df_all[col] =df_all[col].fillna('None')\n\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    df_all[col] = df_all[col].fillna(0)\n    \nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    df_all[col] = df_all[col].fillna(0)\n\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    df_all[col] = df_all[col].fillna('None')\n    \ndf_all[\"MasVnrType\"] = df_all[\"MasVnrType\"].fillna(\"None\")\ndf_all[\"MasVnrArea\"] = df_all[\"MasVnrArea\"].fillna(0)\n\ndf_all['MSZoning'] = df_all['MSZoning'].fillna(df_all['MSZoning'].mode()[0])\n\ndf_all[\"Functional\"] = df_all[\"Functional\"].fillna(\"Typ\")\n\ndf_all['Electrical'] = df_all['Electrical'].fillna(df_all['Electrical'].mode()[0])\n\ndf_all['KitchenQual'] = df_all['KitchenQual'].fillna(df_all['KitchenQual'].mode()[0])\n\ndf_all['Exterior1st'] = df_all['Exterior1st'].fillna(df_all['Exterior1st'].mode()[0])\ndf_all['Exterior2nd'] = df_all['Exterior2nd'].fillna(df_all['Exterior2nd'].mode()[0])\n\ndf_all['SaleType'] = df_all['SaleType'].fillna(df_all['SaleType'].mode()[0])\n\ndf_all['MSSubClass'] = df_all['MSSubClass'].fillna(\"None\")\n","e877339d":"from scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n(mu, sigma) = norm.fit(saleprice['SalePrice'])","3e36bbaa":"#We Used This Code To Check The skiweness Of The  Salesprice Column \n(mu, sigma) = norm.fit(saleprice['SalePrice'])\nsns.distplot(saleprice['SalePrice'],fit=norm)\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],loc='best')","54aa5762":"quantile_plot=stats.probplot(saleprice['SalePrice'], plot=plt)","592b9b35":"saleprice[\"SalePrice\"] = np.log1p(saleprice[\"SalePrice\"])\ny=saleprice\ny.head()","8773d2c5":"(mu, sigma) = norm.fit(saleprice['SalePrice'])\nplt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.distplot(saleprice['SalePrice'],fit=norm)\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\nplt.subplot(1, 2, 2)\nquantile_plot=stats.probplot(saleprice['SalePrice'], plot=plt)","fe0de89c":"fat = 'OverallQual'\ndata = pd.concat([hp_train['SalePrice'], hp_train[fat]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=fat, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nplt.savefig('lethabo.png')","4e0d7eed":"plt.figure(figsize=(10,10))\nsns.boxenplot(df_all[\"LotFrontage\"],df_all[\"Neighborhood\"])","f7ce11c2":"df_all=df_all.drop(['GarageYrBlt','TotRmsAbvGrd','GarageArea','PoolQC', 'MiscFeature', 'Fence','MiscVal','PoolArea','Utilities'], axis=1)","466bbbf7":"df_all['TotalBath'] = df_all['FullBath'] + df_all['HalfBath']*0.5 + df_all['BsmtFullBath'] + df_all['BsmtHalfBath']*0.5\ndf_all['TotalFlrSF'] = df_all['1stFlrSF'] + df_all['2ndFlrSF']\ndf_all['BsmtFinSF'] = df_all['BsmtFinSF1'] + df_all['BsmtFinSF2']\n# Deleting singulars since we have combined above:\n\nsingles_to_drop = ['FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath', '1stFlrSF', '2ndFlrSF', 'BsmtFinSF1', 'BsmtFinSF2']\nfor col in singles_to_drop:\n    df_all.drop([col], axis =1, inplace = True)","ea401bce":"\ndf_all.head()","68543d20":"df_all = pd.get_dummies(df_all)\nX_test=df_all.iloc[train_length:,:]\nX_train=df_all.iloc[:train_length,:]\n","997ce4a9":"X=X_train","af8cc60b":"df_all.head()","81f4d41b":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X_train.values)\n    rmse= np.sqrt(-cross_val_score(model, X_train.values, y, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","0fb49936":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))","f9ed2172":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","c7c826ad":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)","b099f54e":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\n","a5423a8c":"import random\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.04, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =random.randint(0,int(2**16)), nthread = -1)\n","0ff577bf":"lgb_model = lgb.LGBMRegressor(lcolsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.04, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =random.randint(0,int(2**16)), nthread = -1)","1ccaf345":"import random\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.04, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =random.randint(0,int(2**16)), nthread = -1)\n","7a0ed2d4":"score = rmsle_cv(lasso)\nprint(\"\\nLASSO: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","fc201e72":"score = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","67ca0fa0":"score = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","90dac844":"score = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","582200f0":"score = rmsle_cv(model_xgb)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","e7da3c9a":"score = rmsle_cv(lgb_model)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","0913a224":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ","d47839cd":"averaged_models = AveragingModels(models = (GBoost, lasso,KRR,model_xgb))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","2ae2b5f4":"averaged_models.fit(X,y)","887605b7":"y_average=np.expm1(averaged_models.predict(X_test))","442dacad":"sample2 =pd.read_csv('..\/input\/sample_submission.csv')\nsales2=pd.DataFrame(y_average,columns=['SalePrice'])\nsample2['SalePrice']=sales2['SalePrice']\nsample2.head()\nsample2.to_csv('avg6.csv',index=False)","6bf8fba3":"#### Kernel Ridge Regression","eb4406e1":"* The idea of stacking models just means we take the models we have tested and simply average them together in hopes of attaining a better score\n* Averaged base models class**\n* Averaged base models score**\n* We just average four models here **ENet, GBoost,XGB,KNN,LGB,and lasso**.  Of course we could easily add more models in the mix. ","461a59c3":"Brief Correlation Graph Showing Positive And Negative Correlation ","4cbc3ddd":"#### Lasso  Regression ","6cd87764":"#### The Following Two Graphs Shows The Skiewness Of Our Data And how It Looks Like When It Is Normalised  \n* For The Data To Be Normalised The Blue Dots  Have To  Be Close To The Red Line\n* On The Next Plot We Can See The Datapints Are Much Better Then Before","6fbfba73":"### We will Only Avarage Those Whicha Have Good Mean Scores","31c6f952":"### This are all  Columns with Null Values In The Train Csv \n","24ff9b22":"### Model Stacking","b62ca41e":"### Model Boosting\n* Importing All The Liabriaries We Will Need To Boost Our Model  And The Nesessary Python Liabriries That We Might Need Later ","a971ac1c":"###                                            Missing Values","050252d8":"### Visulization Of The Columns With High Correlation ","821c770f":"* We Going To Stack All The Models Together So That We Can Improve Our Predictive Score","7d28567d":"Check Correlation Of Columns To The Target Columns To See Which Columns We Should Drop","d1f9fe62":"### The Columns That Are More Correlated With The Target Are Bellow\n* This Columns Are The Columns That Have Same Relationship To Each Other And Are Correlated To The Sales Price column \n* we will Need To Drop 5 Columns In The Below List That Are Related To Each Other Since They Have The Same Information \n\n  * YearBuilt or GarageYrBlt\n  * GrLivArea or TotRmsAbvGrd\n  * GarageCars or GarageArea\n  * TotalBsmtSF or 1stFlrSF\n  * SalePrice or overalqal","b42b0a22":"# Team 14 The Catalyst\n  On This Notebook We Will Solve The House Prices Using Advance Regression Techniques\n","aa579ad4":"#### Light Gradient Boosting","6b252971":"* We Can See That Sales Price Is Not Normally Distributed\n* We Need To See The Realtionship Using A Normal Probabilty Plot\/Normal Quantile Plot","5f9bc781":"### Analysing The Sales Price \n \n * We Do This Since SalesPrice Is A Target Variable And It Is  What We Want To Predict","c3e04b0f":"#### XGB","2f739b4b":"### Import The Important Packages For Data Viewing Data Analysis,Data Manipulation,Mechine-Learning ","b7dd9b86":"####  GBoost Regression","8a1af5a5":"####  Elastic Net Regression","820bd528":"### Importing Our Datasets","989fefc7":"* Colums We Droped Successfully On The Dataset\n* If You Take A Look At The Dataset There Is Catergorial Values And Numerical Values And Machine Learning Only Understand Numbers\n* We Will Have To Convert The Catergorial Values To Numerical By Getting Dummie Variables ","f5847404":"### Base Models Scores","55cd75c0":"### Now The Data Is Clean We Convert Catergorial Values To Numerical ","106602d5":"### Visualization Of the Null Values In our Dataset","46802698":"### To Use The Dataset On Your Pc And Not On Kaggle Use Get The Dataset From Github With The Following Links , 'Dont Forget To Remove #' ","6a3accdc":"### Predictions For Submittions","83c35be9":"### Droping Columns That Have Less Correlation To Sales Price ","2a850f0c":"### The Correlation Graphs","14deff98":"### The Base Models ","042531b6":"#### Extreme Gradient Boosting XGB","d9f8845a":"### Cross Validation\nCross Validation\nWe use the cross_val_score function of Sklearn in order for the model to be stacked together","8bef48e8":"* We Need To Look At How The Data\/Obsevations Might Fall Closely To The Line\n* We See That Most Of The Deviations Apear Mostly On The Left And The Right \n* This Is A Right Skewed Data Sets Tend To Be Close To Zero And Less Data Points In The Upper Bound \/ As We Get To Higher Values\n* We Then Need To  Apply The Log Transformation  On The Sales Price Column To Make Sure That Atlist Most Of The Datapoints Fall On The Line Of the Q-Q plot\n* we will use log(1+x) to do this","8ea61f8e":"### Box Plot Overallqual\/Saleprice","cc9e17a5":"### Filling In The Missing Values "}}