{"cell_type":{"8f793156":"code","1031e7bd":"code","310011f9":"code","1c6f7cde":"code","2094ffcb":"code","f8aa5cd1":"code","d5cc85c9":"code","9502cc44":"code","e0f50231":"code","a88a8b7a":"code","e6905926":"code","5ddaa550":"code","8f4f6863":"code","1e7f4780":"code","2a9f1be1":"code","74fd7d38":"code","b0581c21":"code","f1fd7bdf":"code","f0cfa1c6":"markdown"},"source":{"8f793156":"import numpy as np\nimport pandas as pd \nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\nimport itertools \nimport tensorflow as tf\nimport matplotlib.gridspec as gridspec \nfrom random import randint\nfrom keras.utils.np_utils import to_categorical\nfrom keras.layers import Conv2D,MaxPool2D,Dropout,Dense,Flatten,Activation\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom keras.losses import categorical_crossentropy\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.optimizers import RMSprop\nfrom keras.callbacks import ReduceLROnPlateau\n#Loading Librraries","1031e7bd":"test = pd.read_csv(\"..\/input\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/train.csv\")\nlabels = train.label\nsns.countplot(labels)\nplt.title('Categories');","310011f9":"train.drop(\"label\",axis=1, inplace=True)","1c6f7cde":"train = train \/ 255.0\ntest = test \/ 255.0\ntrain = train.values.reshape(-1,28,28,1)\ntest = test.values.reshape(-1,28,28,1)\nlabels = to_categorical(labels)","2094ffcb":"g = plt.imshow(train[1][:,:,0]) # Must be a 0","f8aa5cd1":"X_train, X_test, Y_train, Y_test = train_test_split(train, labels, test_size=0.1, )","d5cc85c9":"# model = tf.keras.models.Sequential()\n# model.add(tf.keras.layers.Conv2D(32, (5,5), input_shape = (28,28,1) , activation= tf.nn.relu, padding= \"valid\"))\n# model.add(tf.keras.layers.MaxPool2D(pool_size = (3,3), padding = \"same\",strides =(2,2)))\n# model.add(tf.keras.layers.Conv2D(64, (5,5) , activation= tf.nn.relu, padding= \"same\"))\n# model.add(tf.keras.layers.MaxPool2D(pool_size = (3,3), strides=(2,2)))\n# model.add(tf.keras.layers.Flatten())\n# model.add(tf.keras.layers.Dense(512, activation=tf.nn.relu))\n# model.add(tf.keras.layers.Dropout(0.5))\n# model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))","9502cc44":"model = Sequential()\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', input_shape = (28,28,1)))\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation = \"softmax\"))","e0f50231":"optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)","a88a8b7a":"model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","e6905926":"# Set a learning rate annealer\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)","5ddaa550":"epochs = 1 # Turn epochs to 30 to get 0.9967 accuracy\nbatch_size = 86","8f4f6863":"datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n\ndatagen.fit(X_train)","1e7f4780":"# Fit the model\nhistory = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size),\n                              epochs = epochs, validation_data = (X_test,Y_test),\n                              verbose = 2, steps_per_epoch=X_train.shape[0] \/\/ batch_size\n                              , callbacks=[learning_rate_reduction])\n","2a9f1be1":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=0)\n    plt.yticks(tick_marks, classes)\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\ny_pred = model.predict(X_test)\ny_pred_classes = np.argmax(y_pred,axis = 1) \ny_true = np.argmax(Y_test,axis = 1) \nconfusion_mtx = confusion_matrix(y_true, y_pred_classes) \nplot_confusion_matrix(confusion_mtx, classes = range(10)) ","74fd7d38":"predictions = model.predict(test)\nlabel = np.argmax(predictions,axis = 1) ","b0581c21":"test_pred = pd.DataFrame(model.predict(test))\ntest_pred = pd.DataFrame(test_pred.idxmax(axis = 1))\ntest_pred.index.name = 'ImageId'\ntest_pred = test_pred.rename(columns = {0: 'Label'}).reset_index()\ntest_pred['ImageId'] = test_pred['ImageId'] + 1\n\ntest_pred.head()","f1fd7bdf":"test_pred.to_csv('msubmission.csv', index = False)","f0cfa1c6":"**Some Basics of CNN**\n\nImage is nothing but an array of numbers. Also with three channels(RGB) but here we only have a \nConvolution layers and Maxpooling layers are the real feature extractor. In convolution layer we're just convoluting filters with the image to create a corresponding feature map. Then this feature map is modified by Maxpooling which is nothing but a maximum function applied in a window. In my case this window is (3,3). Also,In between convolution and Max pooling layer I'm using a Relu fuction.\n\nOur data is non linear,infact most of the times we'll be tackling nonlinear data and our convolution is a linear operation. So we need to make it non linear using any non linear fuction. I've used Relu for that, you are free to choose any other non linear fuctions like Sigmoid, Tanh, leaky Relu.\n\n\nNow after all these operations we've so many pooled feature maps. Now we'll add flatten layer which simply converts the 2d pooled feature maps into 1d array and will feed it to our Dense layer with 512 nodes having relu activation. Finally after mutiple iterations made by the optimizer of loss function using gradeint descent and back propogation to minimize the loss, our model will be trained!! \nUsing softmax we'll collect the accuracy. "}}