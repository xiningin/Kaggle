{"cell_type":{"c6b1b1f1":"code","60cac045":"code","87eeadde":"code","007a5ad7":"code","b592cd27":"code","3928c156":"code","bd3c07eb":"code","0168cad5":"code","d1819433":"code","6ba25377":"code","87d9b53c":"code","0d02a32c":"code","48a21d1a":"code","e56f782c":"code","a5f8ad19":"code","7d8a682c":"code","a853f319":"code","84edcb42":"code","cacfda92":"markdown","34544825":"markdown","d27c2a43":"markdown","5343a8e3":"markdown","ce33751a":"markdown","b7708cd2":"markdown","da125434":"markdown","0b33ddb4":"markdown","948df1d4":"markdown","20bd15ea":"markdown","b361ce24":"markdown","09794496":"markdown","c2b089e8":"markdown"},"source":{"c6b1b1f1":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom PIL import Image\nimport time\nimport glob\nimport imageio\nfrom IPython import display\nimport cv2\nimport pathlib\nimport zipfile\nimport torch\nimport sys\nimport pandas as pd \nimport math\n\nimport torchvision\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader, ConcatDataset, TensorDataset\nfrom torchvision.utils import make_grid\nimport torch.optim as optim\nfrom torchvision.datasets import MNIST\n\nfrom skimage import io, transform\n\n!pip install torchsummary\nfrom torchsummary import summary\n\n!pip install torchviz\nfrom torchviz import make_dot\n\nfrom torch.utils.tensorboard import SummaryWriter","60cac045":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\ndevice","87eeadde":"class Generator(nn.Module):\n\n    def __init__(self, z_dim=10, im_chan=1, hidden_dim=64):\n        super(Generator, self).__init__()\n        \n        self.z_dim = z_dim\n        \n        self.gen = nn.Sequential(\n            \n            self.get_generator_block(z_dim, \n                                     hidden_dim * 4,\n                                     kernel_size=3, \n                                     stride=2),\n            \n            self.get_generator_block(hidden_dim * 4, \n                                     hidden_dim * 2,\n                                     kernel_size=4,\n                                     stride = 1),\n            \n            self.get_generator_block(hidden_dim * 2,\n                                     hidden_dim ,\n                                     kernel_size=3,\n                                     stride = 2,\n                                    ),\n\n            self.get_generator_final_block(hidden_dim,\n                                           im_chan,\n                                           kernel_size=4,\n                                           stride=2)\n            \n\n        )\n        \n        \n    def get_generator_block(self, input_channel, output_channel, kernel_size, stride = 1, padding = 0):\n        return nn.Sequential(\n                nn.ConvTranspose2d(input_channel, output_channel, kernel_size, stride, padding),\n                nn.BatchNorm2d(output_channel),\n                nn.ReLU(inplace=True),\n        )\n    \n    \n    def get_generator_final_block(self, input_channel, output_channel, kernel_size, stride = 1, padding = 0):\n        return  nn.Sequential(\n                nn.ConvTranspose2d(input_channel, output_channel, kernel_size, stride, padding),\n                nn.Tanh()\n            )\n    \n    \n    def forward(self, noise):\n        x = noise.view(len(noise), self.z_dim, 1, 1)\n        return self.gen(x)\n    \n    \n    \nsummary(Generator(100).to(device), (100,))\nprint(Generator(100))","007a5ad7":"class Discriminator(nn.Module):\n\n    def __init__(self, im_chan=1, hidden_dim=16):\n        super(Discriminator, self).__init__()\n        self.disc = nn.Sequential(\n            self.get_critic_block(im_chan,\n                                         hidden_dim * 4,\n                                         kernel_size=4,\n                                         stride=2),\n            \n            self.get_critic_block(hidden_dim * 4,\n                                         hidden_dim * 8,\n                                         kernel_size=4,\n                                         stride=2,),\n            \n            self.get_critic_final_block(hidden_dim * 8,\n                                               1,\n                                               kernel_size=4,\n                                               stride=2,),\n\n        )\n\n        \n    def get_critic_block(self, input_channel, output_channel, kernel_size, stride = 1, padding = 0):\n        return nn.Sequential(\n                nn.Conv2d(input_channel, output_channel, kernel_size, stride, padding),\n                nn.BatchNorm2d(output_channel),\n                nn.LeakyReLU(0.2, inplace=True)\n        )\n    \n    \n    def get_critic_final_block(self, input_channel, output_channel, kernel_size, stride = 1, padding = 0):\n        return  nn.Sequential(\n                nn.Conv2d(input_channel, output_channel, kernel_size, stride, padding),\n            )\n    \n    def forward(self, image):\n        return self.disc(image)\n    \n    \nsummary(Discriminator().to(device) , (1,28,28))\nprint(Discriminator())","b592cd27":"def get_noise(n_samples, z_dim, device='cpu'):\n    return torch.randn(n_samples,z_dim,device=device)","3928c156":"z_dim = 64\nbatch_size = 128\n\nfixed_noise = get_noise(batch_size, z_dim, device=device)\n\ntrain_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,)),\n])\n\ndataloader = DataLoader(\n    MNIST('.', download=True, transform=train_transform),\n    batch_size=batch_size,\n    shuffle=True)","bd3c07eb":"start = time.time()\ndataiter = iter(dataloader)\nimages,labels = dataiter.next()\nprint ('Time is {} sec'.format(time.time()-start))\n\nplt.figure(figsize=(8,8))\nplt.axis(\"off\")\nplt.title(\"Training Images\")\nplt.imshow(np.transpose(make_grid(images.to(device), padding=2, normalize=True).cpu(),(1,2,0)))\n\nprint('Shape of loading one batch:', images.shape)\nprint('Total no. of batches present in trainloader:', len(dataloader))","0168cad5":"mnist_shape = (1, 28, 28)\nn_classes = 10","d1819433":"def get_input_dimensions(z_dim, mnist_shape, n_classes):\n    generator_input_dim = z_dim + n_classes\n    discriminator_im_chan =  mnist_shape[0] + n_classes\n    return generator_input_dim, discriminator_im_chan","6ba25377":"lr = 0.0002\nbeta_1 = 0.5 \nbeta_2 = 0.999\n\ngenerator_input_dim, discriminator_im_chan = get_input_dimensions(z_dim, mnist_shape, n_classes)\n\ncriterion = nn.BCEWithLogitsLoss()\n\ndef weights_init(m):\n    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n    if isinstance(m, nn.BatchNorm2d):\n        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n        torch.nn.init.constant_(m.bias, 0)\n\n        \ngen = Generator(z_dim=generator_input_dim).to(device)\ngen_opt = torch.optim.Adam(gen.parameters(), lr=lr, betas=(beta_1, beta_2))\n\ndisc  = Discriminator(im_chan = discriminator_im_chan).to(device) \ndisc_opt = torch.optim.Adam(disc.parameters(), lr=lr, betas=(beta_1, beta_2))\n\ngen = gen.apply(weights_init)\ndisc = disc.apply(weights_init) ","87d9b53c":"import torch.nn.functional as F\n\ndef get_one_hot_labels(labels, n_classes):\n    return F.one_hot(labels,n_classes)\n","0d02a32c":"def show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28), nrow=5, show_fig=False, epoch=0):\n    image_unflat = image_tensor.detach().cpu().view(-1, *size)\n    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n    plt.axis('off')\n    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n    if show_fig:\n        plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n        \n    plt.show()","48a21d1a":"def combine_vectors(x, y):\n    combined = torch.cat((x.float(),y.float()) , 1)\n    return combined","e56f782c":"# x = get_noise(1, 64, device=device)\n\n# model = Generator(z_dim=64).to(device)\n\n# mk_dot = make_dot(model(x), params=dict(model.named_parameters()))\n# mk_dot.format = 'png'\n# mk_dot.render(\"attached\")\n\n# plt.figure(figsize=(100,30))\n# plt.axis('off')\n# img = mpimg.imread('attached.png')\n# imgplot = plt.imshow(img)\n# plt.show()\n\n\n# def double_backprop(inputs, net):\n#     y = net(x)\n#     y = criterion(y, torch.zeros_like(y))\n#     grad,  = torch.autograd.grad(y, x, create_graph=True, retain_graph=True)\n#     return grad\n\n# x = torch.randn(1,64,1,1).requires_grad_(True)\n# mk_dot = make_dot(double_backprop(x, model), params=dict(list(model.named_parameters()) + [('x', x)]))\n# mk_dot.format = 'png'\n# mk_dot.render(\"attached_back\")\n\n# plt.figure(figsize=(100,30))\n# img = mpimg.imread('attached_back.png')\n# plt.axis('off')\n# imgplot = plt.imshow(img)\n# plt.show()","a5f8ad19":"n_epochs = 100\ncur_step = 0\ntotal_steps = 0\nstart_time = time.time()\ncur_step = 0\n\nD_mean_losses = []\nG_mean_losses = []\n\ngenerator_losses = []\ndiscriminator_losses = []\n\nnoise_and_labels = False\nfake = False\ndisplay_step = 5\nfake_image_and_labels = False\nreal_image_and_labels = False\ndisc_fake_pred = False\ndisc_real_pred = False\n \n\nfixed_one_hot_labels = get_one_hot_labels(torch.tensor([0,1,2,3,4,5,6,7,8,9]).to(device), n_classes)\nfixed_noise = get_noise(10, z_dim, device=device)\nfixed_noise_and_labels = combine_vectors(fixed_noise, fixed_one_hot_labels)\n\nfor epoch in range(n_epochs):\n    \n    cur_step = 0\n    start = time.time()\n    \n    for real, labels in dataloader:\n        \n        cur_batch_size = len(real)\n        \n        real = real.to(device)\n\n        one_hot_labels = get_one_hot_labels(labels.to(device), n_classes)\n       \n        image_one_hot_labels = one_hot_labels[:, :, None, None]\n        image_one_hot_labels = image_one_hot_labels.repeat(1, 1, mnist_shape[1], mnist_shape[2])\n        \n        ## discriminator train step\n        disc_opt.zero_grad()\n        fake_noise = get_noise(cur_batch_size, z_dim, device=device)\n        \n        noise_and_labels =  combine_vectors(fake_noise, one_hot_labels)\n        fake = gen(noise_and_labels)\n        \n        fake_image_and_labels = combine_vectors(fake, image_one_hot_labels)\n        real_image_and_labels = combine_vectors(real, image_one_hot_labels)\n        disc_fake_pred = disc(fake_image_and_labels.detach())\n        disc_real_pred = disc(real_image_and_labels)\n        disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n        disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n\n        disc_loss = (disc_fake_loss + disc_real_loss) \/ 2\n        disc_loss.backward(retain_graph=True)\n        disc_opt.step() \n\n        \n        discriminator_losses += [disc_loss.item()]\n\n        ## generator train step\n        gen_opt.zero_grad()\n\n        fake_image_and_labels = combine_vectors(fake, image_one_hot_labels)\n        disc_fake_pred = disc(fake_image_and_labels)\n        gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))\n        gen_loss.backward()\n        \n        gen_opt.step()\n\n        generator_losses += [gen_loss.item()]\n        \n        cur_step += 1\n        total_steps += 1\n        \n        print_val = f\"Epoch: {epoch}\/{n_epochs} Steps:{cur_step}\/{len(dataloader)}\\t\"\n        print_val += f\"Epoch_Run_Time: {(time.time()-start):.6f}\\t\"\n        print_val += f\"Loss_D : {disc_loss.item():.6f}\\t\"\n        print_val += f\"Loss_G : {gen_loss.item():.6f}\\t\"  \n        print(print_val, end='\\r',flush = True)\n\n\n#         if cur_step % display_step == 0 and cur_step > 0:\n#             gen_mean = sum(generator_losses[-display_step:]) \/ display_step\n#             disc_mean = sum(discriminator_losses[-display_step:]) \/ display_step\n#             print(f\"Step {cur_step}: Generator loss: {gen_mean}, discriminator loss: {disc_mean}\")\n#             show_tensor_images(fake)\n#             show_tensor_images(real)\n#             step_bins = 20\n#             x_axis = sorted([i * step_bins for i in range(len(generator_losses) \/\/ step_bins)] * step_bins)\n#             num_examples = (len(generator_losses) \/\/ step_bins) * step_bins\n#             plt.plot(\n#                 range(num_examples \/\/ step_bins), \n#                 torch.Tensor(generator_losses[:num_examples]).view(-1, step_bins).mean(1),\n#                 label=\"Generator Loss\"\n#             )\n#             plt.plot(\n#                 range(num_examples \/\/ step_bins), \n#                 torch.Tensor(discriminator_losses[:num_examples]).view(-1, step_bins).mean(1),\n#                 label=\"Discriminator Loss\"\n#             )\n#             plt.legend()\n#             plt.show()\n\n    print()\n    gen_mean = sum(generator_losses[-cur_step:]) \/ cur_step\n    disc_mean = sum(discriminator_losses[-cur_step:]) \/ cur_step\n    \n    D_mean_losses.append(disc_mean)\n    G_mean_losses.append(gen_mean)\n    \n    print_val = f\"Epoch: {epoch}\/{n_epochs} Total Steps:{total_steps}\\t\"\n    print_val += f\"Total_Time : {(time.time() - start_time):.6f}\\t\"\n    print_val += f\"Loss_D : {disc_loss.item():.6f}\\t\"\n    print_val += f\"Loss_G : {gen_loss.item():.6f}\\t\"\n    print_val += f\"Loss_D_Mean : {disc_mean:.6f}\\t\"\n    print_val += f\"Loss_G_Mean : {gen_mean:.6f}\\t\"\n    print(print_val)\n    \n    fake = gen(fixed_noise_and_labels)\n    show_tensor_images(fake, show_fig=True, epoch=epoch)\n#     show_tensor_images(real)\n    \n    cur_step = 0\n    ","7d8a682c":"plt.figure(figsize=(10,5))\nplt.title(\"Generator and Discriminator Loss During Training\")\nplt.plot(generator_losses,label=\"G-Loss\")\nplt.plot(discriminator_losses,label=\"C-Loss\")\nplt.xlabel(\"iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","a853f319":"plt.figure(figsize=(10,5))\nplt.title(\"Generator and Discriminator Loss During Training\")\nplt.plot(G_mean_losses,label=\"G-Loss\")\nplt.plot(D_mean_losses,label=\"C-Loss\")\nplt.xlabel(\"iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","84edcb42":"anim_file = 'Conditional-GAN.gif'\n\nwith imageio.get_writer(anim_file, mode='I') as writer:\n  filenames = glob.glob('image*.png')\n  filenames = sorted(filenames)\n  for filename in filenames:\n    image = imageio.imread(filename)\n    writer.append_data(image)\n  image = imageio.imread(filename)\n  writer.append_data(image)\n\n\n!pip install -q git+https:\/\/github.com\/tensorflow\/docs\nimport tensorflow_docs.vis.embed as embed\nembed.embed_file(anim_file)","cacfda92":"# Model Training Process","34544825":"# Device Mode\n","d27c2a43":"# Animated GIF Create & Show","5343a8e3":"# MNIST Dataset Load","ce33751a":"# Loss Function & Optimizer","b7708cd2":"# Generator","da125434":"# One Hot Labels","0b33ddb4":"# Discriminator","948df1d4":"# Noise Creator Function","20bd15ea":"# Import Packages","b361ce24":"# After Tranning Loss Visualization","09794496":"# Resources\n\n\n[Conditional Generative Adversarial Nets (Mirza and Osindero, 2014):](https:\/\/arxiv.org\/abs\/1411.1784)\n","c2b089e8":"# Conditional GAN"}}