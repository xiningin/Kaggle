{"cell_type":{"dd8acb88":"code","e9738ef0":"code","9dbecc27":"code","63168dfa":"code","07e573d9":"code","5e31bf2a":"code","929efa36":"code","5840b294":"code","04eda14a":"code","b2666228":"code","0caaa0e1":"code","612e23e1":"code","423401da":"code","4f382e12":"code","4d037718":"code","dd5999ad":"code","5c1a5411":"code","f8b18112":"code","cd5d70ab":"code","8ec1e315":"code","9005139b":"code","5f09202a":"code","80606c62":"code","df7ee1b6":"code","0dddeafe":"code","448508d2":"code","fad9bdf4":"code","30e9afe2":"code","477e7771":"code","5ed0b4ff":"code","4fd52fb6":"code","84f48563":"code","c6980079":"code","731b9e2c":"markdown","3976c956":"markdown","76b78a7e":"markdown","5c97a56d":"markdown","4dd3a06f":"markdown","50729a9d":"markdown","09434c0f":"markdown","592fd570":"markdown","ed366073":"markdown","30a07999":"markdown","6a913b52":"markdown","c045c41c":"markdown","b62aac22":"markdown","def2a2dd":"markdown","366ee98b":"markdown","b0c74a65":"markdown","c824a6b1":"markdown","15d74db7":"markdown","261a2688":"markdown","1aa67eb6":"markdown","044e96ab":"markdown","457aa862":"markdown","b14fb3bb":"markdown","01631111":"markdown","9f398ae3":"markdown","d05bb420":"markdown"},"source":{"dd8acb88":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom calendar import monthrange\nfrom itertools import product\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport lightgbm as lgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pickle\n\n%matplotlib inline","e9738ef0":"shops = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/shops.csv')\nitems = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv')\ncatgs = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\nsales = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\n\ntestd = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/test.csv')\nsampl = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv')","9dbecc27":"print(sales.isna().sum(), '\\n')\nprint(testd.isna().sum())","63168dfa":"plt.figure(figsize=(10,4))\nplt.xlim(-100, 3000)\nsns.boxplot(x=sales.item_cnt_day)\n\nprint('Item count day - Min: {}, Max: {}'.format(sales.item_cnt_day.min(), sales.item_cnt_day.max()))\n\nplt.figure(figsize=(10,4))\nplt.xlim(sales.item_price.min(), sales.item_price.max()*1.1)\nsns.boxplot(x=sales.item_price)\n\nprint('Item price - Min: {}, Max: {}'.format(sales.item_price.min(), sales.item_price.max()))","07e573d9":"# Remove outliers\nsales = sales[sales.item_price <= 100000]\nsales = sales[sales.item_cnt_day <= 1000]\n\n# Adjusting negatice prices (change it for median values)\nmedian = sales[(sales.shop_id == 32) & (sales.item_id == 2973) & (sales.date_block_num == 4) & (sales.item_price > 0)].item_price.median()\nsales.loc[sales.item_price < 0, 'item_price'] = median","5e31bf2a":"# \u042f\u043a\u0443\u0442\u0441\u043a \u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435, 56\nsales.loc[sales.shop_id == 0, 'shop_id'] = 57\ntestd.loc[testd.shop_id == 0, 'shop_id'] = 57\n# \u042f\u043a\u0443\u0442\u0441\u043a \u0422\u0426 \"\u0426\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u0439\"\nsales.loc[sales.shop_id == 1, 'shop_id'] = 58\ntestd.loc[testd.shop_id == 1, 'shop_id'] = 58\n# \u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439 \u0443\u043b. \u0427\u043a\u0430\u043b\u043e\u0432\u0430 39\u043c\u00b2\nsales.loc[sales.shop_id == 10, 'shop_id'] = 11\ntestd.loc[testd.shop_id == 10, 'shop_id'] = 11\n# \u0420\u043e\u0441\u0442\u043e\u0432\u041d\u0430\u0414\u043e\u043d\u0443 \u0422\u0420\u041a \"\u041c\u0435\u0433\u0430\u0446\u0435\u043d\u0442\u0440 \u0413\u043e\u0440\u0438\u0437\u043e\u043d\u0442\"\nsales.loc[sales.shop_id == 39, 'shop_id'] = 40\ntestd.loc[testd.shop_id == 39, 'shop_id'] = 40","929efa36":"shops.shop_name.unique()","5840b294":"shops.loc[shops.shop_name == '\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"', 'shop_name'] = '\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"'\nshops['shop_category'] = shops['shop_name'].str.split(' ').map(lambda x:x[1]).astype(str)\ncategories = ['\u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435,', '\u0422\u0426', '\u0422\u0420\u041a', '\u0422\u0420\u0426','\u0443\u043b.', '\u041c\u0430\u0433\u0430\u0437\u0438\u043d', '\u0422\u041a', '\u0441\u043a\u043b\u0430\u0434']\nshops.shop_category = shops.shop_category.apply(lambda x: x if (x in categories) else 'etc')\nshops.shop_category.unique()","04eda14a":"shops.groupby(['shop_category']).sum()","b2666228":"category = ['\u0422\u0426', '\u0422\u0420\u041a', '\u0422\u0420\u0426', '\u0422\u041a']\nshops.shop_category = shops.shop_category.apply(lambda x: x if (x in category) else 'etc')\nprint('Category Distribution', shops.groupby(['shop_category']).sum())\n\nshops['shop_category_code'] = LabelEncoder().fit_transform(shops['shop_category'])","0caaa0e1":"shops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\nshops.loc[shops.city == '!\u042f\u043a\u0443\u0442\u0441\u043a', 'city'] = '\u042f\u043a\u0443\u0442\u0441\u043a'\nshops['city_code'] = LabelEncoder().fit_transform(shops['city'])\nshops = shops[['shop_id','city_code', 'shop_category_code']]\n\nshops.head()","612e23e1":"print(len(catgs.item_category_name.unique()))\ncatgs.item_category_name.unique()","423401da":"catgs['type'] = catgs.item_category_name.apply(lambda x: x.split(' ')[0]).astype(str)\ncatgs.loc[(catgs.type == '\u0418\u0433\u0440\u043e\u0432\u044b\u0435') | (catgs.type == '\u0410\u043a\u0441\u0435\u0441\u0441\u0443\u0430\u0440\u044b'), 'category'] = '\u0418\u0433\u0440\u044b'\ncatgs.loc[catgs.type == 'PC', 'category'] = '\u041c\u0443\u0437\u044b\u043a\u0430'\ncategory = ['\u0418\u0433\u0440\u044b', '\u041a\u0430\u0440\u0442\u044b', '\u041a\u0438\u043d\u043e', '\u041a\u043d\u0438\u0433\u0438','\u041c\u0443\u0437\u044b\u043a\u0430', '\u041f\u043e\u0434\u0430\u0440\u043a\u0438', '\u041f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u044b', '\u0421\u043b\u0443\u0436\u0435\u0431\u043d\u044b\u0435', '\u0427\u0438\u0441\u0442\u044b\u0435', '\u0410\u043a\u0441\u0435\u0441\u0441\u0443\u0430\u0440\u044b']\ncatgs['type'] = catgs.type.apply(lambda x: x if (x in category) else 'etc')\nprint(catgs.groupby(['type']).sum())\ncatgs['type_code'] = LabelEncoder().fit_transform(catgs['type'])\n\n# if subtype is nan then type\ncatgs['split'] = catgs.item_category_name.apply(lambda x: x.split('-'))\ncatgs['subtype'] = catgs['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\ncatgs['subtype_code'] = LabelEncoder().fit_transform(catgs['subtype'])\ncatgs = catgs[['item_category_id','type_code', 'subtype_code']]\n\ncatgs.head()","4f382e12":"sales['date'] = pd.to_datetime(sales['date'], format='%d.%m.%Y')\nsales['month'] = sales['date'].dt.month\nsales['year'] = sales['date'].dt.year\nsales = sales.drop(columns=['date'])\n\n# sales.head()\nto_append = testd[['shop_id', 'item_id']].copy()\n\nto_append['date_block_num'] = sales['date_block_num'].max() + 1\nto_append['year'] = 2015\nto_append['month'] = 11\nto_append['item_cnt_day'] = 0\nto_append['item_price'] = 0\n\nsales = pd.concat([sales, to_append], ignore_index=True, sort=False)\nsales.head()","4d037718":"period = sales[['date_block_num', 'year', 'month']].drop_duplicates().reset_index(drop=True)\nperiod['days'] = period.apply(lambda r: monthrange(r.year, r.month)[1], axis=1)\n\nsales = sales.drop(columns=['month', 'year'])\n\nperiod.head()","dd5999ad":"agg_sales = sales.groupby(['date_block_num','shop_id','item_id'], as_index=False).agg({'item_price' : np.mean, 'item_cnt_day' : np.sum})\\\n    .rename(columns={'item_cnt_day' : 'item_cnt_month'})\n\nshop_item_sales = pd.pivot_table(agg_sales, values='item_price', index=['date_block_num'],\n                    columns=['shop_id', 'item_id'], fill_value=np.nan)\nshop_item_sales","5c1a5411":"shop_item_sales = shop_item_sales.fillna(method='ffill')\nshop_item_sales","f8b18112":"agg_sales_future = shop_item_sales.stack().stack().reset_index().rename(columns={0 : 'item_price'})\n\nprint('agg_sales shape: ', agg_sales.shape, '\\n')\nprint('agg_sales_future shape: ',agg_sales_future.shape)\n\nagg_sales_future.head(10)","cd5d70ab":"month_summary = pd.merge(agg_sales_future, agg_sales.drop(columns='item_price'), how='left', on=['date_block_num', 'shop_id', 'item_id'])\\\n            .fillna(0.0)\\\n            .rename(columns= {'item_price' : 'item_price_month'})\\\n            .sort_values(by=['shop_id', 'item_id', 'date_block_num'])","8ec1e315":"# Join dimensional data\nmonth_summary = pd.merge(month_summary, shops, on='shop_id')\nmonth_summary = pd.merge(month_summary, items, on='item_id')\nmonth_summary = pd.merge(month_summary, catgs, on='item_category_id')\nmonth_summary = pd.merge(month_summary, period, on='date_block_num')\n\n# Adjusting columns order\nmonth_summary = month_summary[['date_block_num', 'year', 'month', 'days', 'city_code', 'shop_category_code', 'shop_id', 'item_category_id', \n                               'type_code', 'subtype_code', 'item_id', 'item_price_month', 'item_cnt_month']]\n\n# Downcasting values\nfor c in ['date_block_num', 'month', 'days', 'city_code', 'shop_category_code', 'shop_id', 'item_category_id', 'type_code', 'subtype_code']:\n    month_summary[c] = month_summary[c].astype(np.int8)\nmonth_summary['item_id'] = month_summary['item_id'].astype(np.int16)\nmonth_summary['year'] = month_summary['year'].astype(np.int16)\nmonth_summary['item_cnt_month'] = month_summary['item_cnt_month'].astype(np.float16)\nmonth_summary['item_price_month'] = month_summary['item_price_month'].astype(np.float16)\n\n# Remove unused and temporary datasets\ndel shops, items, catgs, to_append, shop_item_sales, agg_sales, agg_sales_future\n\nmonth_summary.head()","9005139b":"print('Min: {} and Max: {} item_cnt_month values'.format(month_summary['item_cnt_month'].min(), month_summary['item_cnt_month'].max()))","5f09202a":"month_summary['item_cnt_month'] = month_summary['item_cnt_month'].clip(0,20)","80606c62":"def agg_by(month_summary, group_cols, new_col, target_col = 'item_cnt_month', agg_func = 'mean'):\n    aux = month_summary\\\n        .groupby(group_cols, as_index=False)\\\n        .agg({target_col : agg_func})\\\n        .rename(columns= {target_col : new_col})\n    aux[new_col] = aux[new_col].astype(np.float16)\n\n    return pd.merge(month_summary, aux, how='left', on=group_cols)\n\ndef lag_feature(df, col, lags=[1,2,3,6,12]):\n    tmp = df[['date_block_num','shop_id','item_id', col]]\n    for i in lags:\n        shifted = tmp.copy()\n        cols = ['date_block_num','shop_id','item_id', '{}_lag_{}'.format(col, i)]\n        shifted.columns = cols\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left').fillna(value={(cols[-1]) : 0.0})\n    return df\n\ndef agg_by_and_lag(month_summary, group_cols, new_col, lags=[1,2,3,6,12], target_col = 'item_cnt_month', agg_func = 'mean'):\n    tmp = agg_by(month_summary, group_cols, new_col, target_col, agg_func)\n    tmp = lag_feature(tmp, new_col, lags)\n    return tmp.drop(columns=[new_col])","df7ee1b6":"# date_avg_item_cnt\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num'], 'date_avg_item_cnt', [1])\n\n# date_avg_item_cnt\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'item_id'], 'date_item_avg_item_cnt', [1,2,3,6,12])\n\n# date_city_avg_item_cnt\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'city_code'], 'date_city_avg_item_cnt', [1])\n\n# date_shop_avg_item_cnt\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_id'], 'date_shop_avg_item_cnt', [1,2,3,6,12])\n\n# date_cat_avg_item_cnt\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'item_category_id'], 'date_cat_avg_item_cnt', [1])\n\n# date_type_avg_item_cnt\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'type_code'], 'date_type_avg_item_cnt', [1])\n\n# date_subtype_avg_item_cnt\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'subtype_code'], 'date_subtype_avg_item_cnt', [1])\n\n# date_shop_category_avg_item_cnt\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_category_code'], 'date_shop_category_avg_item_cnt', [1])\n\n# date_shop_cat_avg_item_cnt\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_id', 'item_category_id'], 'date_shop_cat_avg_item_cnt', [1])\n\n# date_shop_type_avg_item_cnt\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_id', 'type_code'], 'date_shop_type_avg_item_cnt', [1])\n\n# date_shop_subtype_avg_item_cnt\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_id', 'subtype_code'], 'date_shop_subtype_avg_item_cnt', [1])\n\n# date_shop_category_subtype_avg_item_cnt\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_category_code', 'subtype_code'], 'date_shop_category_subtype_avg_item_cnt', [1])\n\n# date_item_city_avg_item_cnt\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'city_code', 'item_id'], 'date_item_city_avg_item_cnt', [1])","0dddeafe":"# date_avg_item_price\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num'], 'date_avg_item_price', [1], 'item_price_month')\n\n# date_item_avg_item_price\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'item_id'], 'date_item_avg_item_price', [1,2,3,6,12], 'item_price_month')\n\n# date_city_avg_item_price\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'city_code'], 'date_city_avg_item_price', [1], 'item_price_month')\n\n# date_shop_avg_item_price\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_id'], 'date_shop_avg_item_price', [1,2,3,6,12], 'item_price_month')\n\n# date_cat_avg_item_price\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'item_category_id'], 'date_cat_avg_item_price', [1], 'item_price_month')\n\n# date_type_avg_item_price\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'type_code'], 'date_type_avg_item_price', [1], 'item_price_month')\n\n# date_subtype_avg_item_price\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'subtype_code'], 'date_subtype_avg_item_price', [1], 'item_price_month')\n\n# date_shop_category_avg_item_price\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_category_code'], 'date_shop_category_avg_item_price', [1], 'item_price_month')\n\n# date_shop_cat_avg_item_price\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_id', 'item_category_id'], 'date_shop_cat_avg_item_price', [1], 'item_price_month')\n\n# date_shop_type_avg_item_price\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_id', 'type_code'], 'date_shop_type_avg_item_price', [1], 'item_price_month')\n\n# date_shop_subtype_avg_item_price\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_id', 'subtype_code'], 'date_shop_subtype_avg_item_price', [1], 'item_price_month')\n\n# date_shop_category_subtype_avg_item_price\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'shop_category_code', 'subtype_code'], 'date_shop_category_subtype_avg_item_price', [1], 'item_price_month')\n\n# date_item_city_avg_item_price\nmonth_summary = agg_by_and_lag(month_summary, ['date_block_num', 'city_code', 'item_id'], 'date_item_city_avg_item_price', [1], 'item_price_month')","448508d2":"month_summary['item_shop_first_sale'] = month_summary['date_block_num'] - month_summary.groupby(['item_id','shop_id'])['date_block_num'].transform('min')\nmonth_summary['item_first_sale'] = month_summary['date_block_num'] - month_summary.groupby('item_id')['date_block_num'].transform('min')","fad9bdf4":"month_summary.to_pickle('month_summary.pkl')\nmonth_summary.info()","30e9afe2":"month_summary = pd.read_pickle('month_summary.pkl')","477e7771":"def generate_subsample(month_summary, target='item_cnt_month'):\n    X_test = month_summary[month_summary['date_block_num'] == 34]\n    X_test = X_test.drop(columns=[target])\n\n    X_val = month_summary[month_summary['date_block_num'] == 33]\n    y_val = X_val[target]\n    X_val = X_val.drop(columns=[target])\n\n    X_train = month_summary[(month_summary['date_block_num'] >= 12) & (month_summary['date_block_num'] < 33)]\n    y_train = X_train[target]\n    X_train = X_train.drop(columns=[target])\n\n    return X_train, y_train, X_val, y_val, X_test","5ed0b4ff":"X_train, y_train, X_val, y_val, X_test = generate_subsample(month_summary.drop(columns=['item_price_month']), 'item_cnt_month')\n\ndel month_summary","4fd52fb6":"def train_gbmodel(X_train, y_train, X_val, y_val):\n\n    RAND_SEED = 42\n\n    lgb_params = {'num_leaves': 2**8, 'max_depth': 19, 'max_bin': 107, #'n_estimators': 3747,\n              'bagging_freq': 1, 'bagging_fraction': 0.7135681370918421, \n              'feature_fraction': 0.49446461478601994, 'min_data_in_leaf': 2**8, # 88\n              'learning_rate': 0.015980721586917768, 'num_threads': 2, \n              'min_sum_hessian_in_leaf': 6,\n              'random_state' : RAND_SEED,\n              'bagging_seed' : RAND_SEED,\n              'boost_from_average' : 'true',\n              'boost' : 'gbdt',\n              'metric' : 'rmse',\n              'verbose' : 1}\n\n    lgb_train = lgb.Dataset(X_train, label=y_train)\n    lgb_val = lgb.Dataset(X_val, label=y_val)\n\n    return lgb.train(lgb_params, lgb_train, \n                      num_boost_round=300,\n                      valid_sets=[lgb_train, lgb_val],\n                      early_stopping_rounds=20)","84f48563":"# model_old_item = train_gbmodel(pd.concat([X_train, X_val]), pd.concat([y_train, y_val]).clip(0, 20), X_val, y_val.clip(0, 20))\ngbm_model = train_gbmodel(X_train, y_train, X_val, y_val)\n\ny_hat = gbm_model.predict(X_val).clip(0, 20)\nprint(np.sqrt(mean_squared_error(y_val.clip(0, 20), y_hat)))\n\nwith open('.\/gbm_model.pickle', 'wb') as handle:\n    pickle.dump(gbm_model, handle)","c6980079":"y_pred = gbm_model.predict(X_test).clip(0, 20)\n\nresult = pd.merge(testd, X_test.assign(item_cnt_month=y_pred), how='left', on=['shop_id', 'item_id'])[['ID', 'item_cnt_month']]\nresult.to_csv('submission.csv', index=False)","731b9e2c":"# EDA: Search for Outliers\n\nSearch for NaN values","3976c956":"We think that category '\u0418\u0433\u0440\u043e\u0432\u044b\u0435 \u043a\u043e\u043d\u0441\u043e\u043b\u0438' and '\u0410\u043a\u0441\u0435\u0441\u0441\u0443\u0430\u0440\u044b' are same as '\u0418\u0433\u0440\u044b'.\nSo, we transform the two features to '\u0418\u0433\u0440\u044b'\nAlso, PC - \u0413\u0430\u0440\u043d\u0438\u0442\u0443\u0440\u044b\/\u041d\u0430\u0443\u0448\u043d\u0438\u043a\u0438 and change to \u041c\u0443\u0437\u044b\u043a\u0430 - \u0413\u0430\u0440\u043d\u0438\u0442\u0443\u0440\u044b\/\u041d\u0430\u0443\u0448\u043d\u0438\u043a\u0438","76b78a7e":"As can be observed, on the same situation of shop_id=2 and item_id=30, almost all prices with NaN values are now filled with the most recent defined price, the exception are the first rows, since there are no previous information and no item sold we can't infer this information.\n\nNow if the missing information defined, it's time to change the dataset to it's original format (\"unpivot\" the table).","5c97a56d":"As can be seen on this data set, there's a lot of NaN information. For instance, shop_id=2 and item_id=30, this column show that the price of this shop\/item has changed during, but also show some missing data during the course of time.\n\nThe intuition of the next steps is that once a price is defined it will be the same until it is explicitly changed, and the missing data (between this period) means that no item was sold (then item count will be zero).\n\nThat been said, let's use the fillna function to copy the actual value of a price to future periods until a new value is defined.","4dd3a06f":"# Train Model\n\nLets use the train and validation data to train a simples lightgbm model.","50729a9d":"Our dataset is now ready, let's summarize it.","09434c0f":"Since a lot of missing data was infered, the agg_sales_future have much more rows than the original agg_sales data set, however the agg_sales data set still have one extra column, the item_cnt_month.\n\nThe next step will join this two tables and fulfill all missing data (on item_cnt_month column) with zero (due the assumtion that no items were sold in this cases).","592fd570":"Let's categorize shops in ['\u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435,' '\u0422\u0426' '\u0422\u0420\u041a' '\u0422\u0420\u0426', '\u0443\u043b.' '\u041c\u0430\u0433\u0430\u0437\u0438\u043d' '\u0422\u041a' '\u0441\u043a\u043b\u0430\u0434' ]\nThen transform other values to 'etc'","ed366073":"Extract City name information from the Shop name","30a07999":"As stated on the problem [evaluation section](https:\/\/www.kaggle.com\/c\/competitive-data-science-predict-future-sales\/overview\/evaluation):\n\n    Submissions are evaluated by root mean squared error (RMSE). True target values are clipped into [0,20] range.\n    \nthen, let's **clip(0,20)** target (`item_cnt_month`) values. This way train target will be similar to the test predictions.","6a913b52":"# Date dataset preprocessing\n\nLet's remove all date data (except `date_block_num`) from sales and store it on `period`.","c045c41c":"# Mean Encoded Features\n\nThis section is focused on the generation of new features (measures) based on the existing ones. For instance, we can create a generalization feature that calculate the mean of `item_cnt_month` for every shop on a specific month and add this as a new feature `date_shop_avg_item_cnt`. This technique can be used with other dimensions (`item_id`, `item_category_id`, `city_code`) or a combination of features (`shop_id` + `item_category_id`).\n\nThis is a powerful technique to help generalize the prediction capabilities of a model. However, our test data does not contain any `item_price` or `item_cnt_month` (in fact, we're trying to predict this one) data. That been said, we can't count on any existing or generated actual feature, **but we can** still count on existing or generated features of **past data**. This means that, for instance, we can use the last 12 months prices of an `item_id` or last 3 months of any feature combination (`shop_id` + `item_category_id`).\n\nTo achieve this goal let's define the `agg_by_and_lag` function, it will generate mean encoded features based on an informed `group_cols` list of columns and \"lagging\" the data N months informed on the `","b62aac22":"# Split Data\n\nLet's split the generated data into train, validation and test data.\n\nFor test data we will take the last month (34), this is the month we must predict the `item_cnt_month`.\n\nFor the validation data we will use the last month in the original training set (33).\n\nAnd for the training data we will use all data between month 12 (since we have lagged some features in 12 months) and 32.","def2a2dd":"Now let's group and summarize the sales dataset. The new dataset (agg_sales) will contain the mean price and total number of items shops and items for every single month.\n\nAfter grouping the sales dataset, it's time to convert shop_id and item_id into columns using a pivot function (shop_item_sales).","366ee98b":"# Categories dataset preprocessing","b0c74a65":"No NaN values found, look for data distribution","c824a6b1":"Mean encode and lag `item_cnt_month` data.","15d74db7":"Now, let's add more dimensional data (`date_block_num`, `year`, `month`, `days`, `city_code`, `shop_category_code`, `shop_id`, `item_category_id`, `type_code`, `subtype_code`, `item_id`) to the above dataset. To achieve this goal let's join it with the other pre precessed datasets.\n\nAlso, let's downcast this dataset.","261a2688":"However, some categories have small values. So we reduce categories 9 to 5.\n['\u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435,', '\u0422\u0426', '\u0422\u0420\u041a', '\u0422\u0420\u0426','\u0443\u043b.', '\u041c\u0430\u0433\u0430\u0437\u0438\u043d', '\u0422\u041a', '\u0441\u043a\u043b\u0430\u0434', 'etc'] => ['\u0422\u0426', '\u0422\u0420\u041a', '\u0422\u0420\u0426', '\u0422\u041a', 'etc']**","1aa67eb6":"Mean encode and lag `item_price_month` data.","044e96ab":"# Extra features\n\nLet's extract some extra features, like the difference in months between and actual sell and the first time it happens.","457aa862":"# Shops dataset preprocessing\n\nSince I speak no Russian, I took advantage of other people work to help extract these features. Great part of this code was extracted from [this notebook](https:\/\/www.kaggle.com\/karell\/xgb-baseline-advanced-feature-engineering).\n\nSeveral shops are duplicates of each other (according to its name). Fix sales and testd set.","b14fb3bb":"As can be seen on the graphs, there are some high outliers on prices and item count. \n\nAlso there's some negative values on prices and count. Nagative values are expected on count values (devolution cases), but not expected on prices.\n\nLet's remove the highest outliers and change the strange price values for a common value.","01631111":"# Append train and test data\n\nConcatenate train (sales) and test (testd) data. Also add manually some missing information on the test data like: date_block_num, year, month, item_cnt_day, item_price.\n\n`item_price` is a missing information and `item_cnt_day` is part of the information we're trying to predict (in fact we're looking for `item_cnt_month`. `item_cnt_month` is the sum of `item_cnt_day` of a given shop and a given item on a month). For now we're gonna fill these values with 0.\n","9f398ae3":"# Loading Initial Data","d05bb420":"With the trained model, let's finally use it to predict the `item_cnt_month` of the test dataset."}}