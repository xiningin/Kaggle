{"cell_type":{"32b97a82":"code","545936f9":"code","ef948166":"code","18571206":"code","d64afa79":"code","4e1c3b79":"code","f3e258b3":"code","b466aad1":"code","003d6299":"code","731560bc":"code","836f6be3":"code","fe83eb1a":"code","f2baaf0a":"markdown","f38e7250":"markdown","3555b34a":"markdown","fa5c8d87":"markdown","ef2e16b8":"markdown","755da849":"markdown"},"source":{"32b97a82":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport gc\nimport cv2\nimport time\nimport requests\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nsns.set()\nseed = 1234\nimport datatable as dt\nnp.random.seed(seed)\nfrom datatable import dt, fread\n\nfrom datatable import dt, f, by, g, join, sort, update, ifelse\n\n%config IPCompleter.use_jedi = False\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","545936f9":"# Path to the directory where data is stored\ndata_path = Path(\"..\/input\/wikipedia-image-caption\/\")\n\n# Selecting only a subset of columns as there\n# many columns that we don't need.\ncolumns_to_select = [\"language\",\n                     \"page_url\",\n                     \"image_url\",\n                     \"caption_title_and_reference_description\"\n                    ]\n\n# Get the list of all tsv files we need to read for training\ntsvs = sorted(list(data_path.glob(\"*.tsv\")))\n\n# Remove test tsv file as we don't need it for now\ntsvs.remove(data_path \/ \"test.tsv\")\n\nprint(\"Number of TSV files found: \", len(tsvs))","ef948166":"# File path\nfilepath = \"\/kaggle\/input\/wikipedia-image-caption\/train-00000-of-00005.tsv\"\n\n# Read using pandas now\nstart_time = time.time()\ndf = dt.fread(filepath)\nprint(f\"Time taken to read {filepath} in datatable format: {time.time()-start_time:.2f} seconds\")\nprint(\"\")\n\ndf.head()","18571206":"print(\"Train size:\", df.shape)","d64afa79":"df.head(2)","4e1c3b79":"print(\"displaying few rows\")\ndf[[2, 3, 4], :]","f3e258b3":"print(\"Subset of a datafame based on a condition on one column\")\ndf[f.language==\"en\", :].head(1)","b466aad1":"print(\"Creating a new column\")\ndf['new_col'] = 2\ndf[:, update(new_col=2)]","003d6299":"print(\"Updating column names\")\ndf.names = {\"new_col\": \"updated_new_col\"}","731560bc":"del df['updated_new_col']","836f6be3":"df.sort('page_changed_recently').head(1)","fe83eb1a":"%%html\n<marquee style='width: 90% ;height:70%; color: #45B39D ;'>\n    <b>Do UPVOTE if you like my work, I will be adding some more content to this kernel post understanding the files :) <\/b><\/marquee>","f2baaf0a":"# IMPORTING DATATABLE AND READING THE DATA","f38e7250":"# Other Links and Notebooks! \n\n## **Links for datatable:**\n- https:\/\/www.kaggle.com\/sudalairajkumar\/getting-started-with-python-datatable\n- https:\/\/www.kaggle.com\/santhraul\/methods-to-read-large-datasets\n- https:\/\/docs.h2o.ai\/driverless-ai\/latest-stable\/docs\/userguide\/datasets-import.html\n- https:\/\/github.com\/h2oai\/datatable\n\n\n## **For Dask:**\n- **Please use the notebook provided by [NAIN](https:\/\/www.kaggle.com\/aakashnain\/can-we-read-faster)**\n","3555b34a":"You can read the data this way! \nEnjoy exploring the data and also the learning :)\n\n\n for further help please visit : https:\/\/datatable.readthedocs.io\/en\/latest\/manual\/comparison_with_pandas.html","fa5c8d87":"# Endnote\n\nIf there are any suggesion for the notebook please comment, that would be helpful. Also please upvote if you liked it! Thank you!!\n\nMy work for this competition \n\n**[Notebook for EDA on the WIKIPEDIA IMAGE AND CAPTION MATCHING](https:\/\/www.kaggle.com\/udbhavpangotra\/eda-wikipedia-image-cap-matching)**\n\n### Some of my other works:\n\n* [TPS- APR](https:\/\/www.kaggle.com\/udbhavpangotra\/tps-apr21-eda-model) \n* [HEART ATTACKS](https:\/\/www.kaggle.com\/udbhavpangotra\/heart-attacks-extensive-eda-and-visualizations) \n* [YOUTUBE DATA EXPLORATION](https:\/\/www.kaggle.com\/udbhavpangotra\/what-do-people-use-youtube-for-in-great-britain)\n* [TPS MAY](https:\/\/www.kaggle.com\/udbhavpangotra\/tps-may-21-extensive-eda-catboost-shap)\n* [COVID-19 DIGITAL LEARNING](https:\/\/www.kaggle.com\/udbhavpangotra\/how-did-covid-19-impact-digital-learning-eda)\n* [TPS - SEPT](https:\/\/www.kaggle.com\/udbhavpangotra\/extensive-eda-baseline-shap)\n\n* [also try this dataset ReliefWeb Crisis Figures Data](https:\/\/www.kaggle.com\/udbhavpangotra\/reliefweb-crisis-figures-data)","ef2e16b8":"HI, \n\nIn the challenge presented to us we have a lot of data, and that's the issue. \n\nNO, too much data isn't the issue, reading and using it is. We have limited disk space allotted to us in the kernels we use, And we need to make sure we can fit the data we get into this pace. But for this challenge we have a bit too much of data! What do we do? \nWell, there are a lot of ways we can handle this issue. \n\n* The first one is we take a small sample of data, that sounds good but is losing data optimum? NO.\n* Datatable\n* Dask\n\nI will be showing how we can use Datatable for the task at hand and also providing the links to various notebooks and documentation that would help you as well! \n","755da849":"# About Datatable\n\n\nIt is a Python package for manipulating 2-dimensional tabular data structures\n\nAs per the documentation :\n> This is a Python package for manipulating 2-dimensional tabular data structures (aka data frames). It is close in spirit to pandas or SFrame; however we put specific emphasis on speed and big data support. As the name suggests, the package is closely related to R's data.table and attempts to mimic its core algorithms and API.\n> \n> Requirements: Python 3.6+ (64 bit) and pip 20.3+."}}