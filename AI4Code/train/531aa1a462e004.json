{"cell_type":{"0b820fb0":"code","385ded66":"code","798b6d70":"code","8935ef9d":"code","d996031e":"code","51c5dc21":"code","a5f46797":"code","2e1c0c46":"code","8945cd28":"code","a01247b1":"code","3cebd4f4":"code","9f7b30e6":"code","abbc89cf":"code","b0b8d7d6":"code","b973d64e":"markdown","016daeac":"markdown","80bb21de":"markdown","85973d02":"markdown","1e3a77bb":"markdown","fccef646":"markdown","8b7704f2":"markdown","1e5b2e27":"markdown","5a79596d":"markdown"},"source":{"0b820fb0":"import tensorflow as tf\nfrom tensorflow.python.framework import ops\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom skimage import data, color\nfrom skimage.transform import rescale, resize, downscale_local_mean\nimport cv2\n%matplotlib inline\nimport os\nprint(os.listdir(\"..\/input\"))\n","385ded66":"#Load in our image dataset into a Numpy matrix\nImage_path='..\/input\/Sign-language-digits-dataset\/X.npy' \nX = np.load(Image_path)\n#Load in our classification into a Numpy matrix\nlabel_path='..\/input\/Sign-language-digits-dataset\/Y.npy'\nY = np.load(label_path)\n#Let's see the dimensions of our pixel matrix and classification matrix\nprint(\"Our feature vector is of size: \" + str(np.shape(X)))\nprint(\"Our classification vector is of size: \" + str(np.shape(Y)))","798b6d70":"X[0] #Let's see how each image is stored","8935ef9d":"#Let's plot a few sample images, so we have a good sense of the type of images we are feeding into our training algorithm.\nprint('Sample images from dataset (this is 9 in sign language):')\nn = 5\nplt.figure(figsize=(15,15))\nfor i in range(1, n+1):\n    ax = plt.subplot(1, n, i)\n    plt.imshow(X[i])\n    plt.gray()\n    plt.axis('off')","d996031e":"#Let's split our data into test\/training sets\n#We'll use ~2\/3 for training and the remaining 1\/3 for testing\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.33, random_state=42)\n\nprint(\"Training set feature matrix shape: \" + str(X_train.shape))\nprint(\"Training set classification matrix shape: \" + str(Y_train.shape))\nprint(\"Testing set feature matrix shape: \" + str(X_test.shape))\nprint(\"Testing set classification matrix shape: \" + str(Y_test.shape))","51c5dc21":"#Flatten our data\nX_train_flat = X_train.reshape(X_train.shape[0],X_train.shape[1]*X_train.shape[2]).T #Flatten our data and transpose ~ (64*64,1381)\nX_test_flat = X_test.reshape(X_test.shape[0],X_train.shape[1]*X_train.shape[2]).T #Flatten our data and transpose ~ (64*64,681)\nY_train = Y_train.T\nY_test = Y_test\nprint(str(X_train_flat.shape))\nprint(str(Y_train.shape))\nprint(str(X_test_flat.shape))\nprint(str(Y_test.shape))\n","a5f46797":"#Let's create some tensorflow place holder values to be used in our model later.\ndef create_placeholders(n_x, n_y):\n    \n    X = tf.placeholder(tf.float32, [n_x, None], name=\"X\")\n    Y = tf.placeholder(tf.float32, [n_y, None], name=\"Y\")\n    \n    return X, Y","2e1c0c46":"#Let's initialize the parameters to be used in our neural network\ndef initialize_params():\n    \n    W1 = tf.get_variable(\"W1\",[25,4096],initializer = tf.contrib.layers.xavier_initializer()) #We will be using Xavier initialization for our weight parameters\n    b1 = tf.get_variable(\"b1\",[25,1],initializer=tf.zeros_initializer()) #We will be using a zero vector for our intercept parameter initialization\n    W2 = tf.get_variable(\"W2\",[15,25],initializer = tf.contrib.layers.xavier_initializer())\n    b2 = tf.get_variable(\"b2\",[15,1],initializer=tf.zeros_initializer())\n    W3 = tf.get_variable(\"W3\",[10,15],initializer = tf.contrib.layers.xavier_initializer())\n    b3 = tf.get_variable(\"b3\",[10,1],initializer=tf.zeros_initializer())\n    \n    #Create a dictionary of our parameters to be used in forward propogation\n    parameters = {\"W1\": W1,\n                 \"W2\": W2,\n                 \"W3\": W3,\n                 \"b1\": b1,\n                 \"b2\": b2,\n                 \"b3\": b3}\n    \n    return parameters","8945cd28":"#Define our forward propogation algorithm\ndef forward_prop(X,parameters):\n    \n    W1 = parameters['W1']\n    b1 = parameters['b1']\n    W2 = parameters['W2']\n    b2 = parameters['b2']\n    W3 = parameters['W3']\n    b3 = parameters['b3']\n    \n    Z1 = tf.add(tf.matmul(W1,X),b1)\n    A1 = tf.nn.tanh(Z1)\n    Z2 = tf.add(tf.matmul(W2,A1),b2)\n    A2 = tf.nn.tanh(Z2)\n    Z3 = tf.add(tf.matmul(W3,A2),b3)\n    \n    return Z3","a01247b1":"#Define our cost function\ndef cost_calc(Z3,Y,parameters):\n    \n    W1 = parameters['W1']\n    W2 = parameters['W2']\n    W3 = parameters['W3']\n    \n    logits = tf.transpose(Z3)\n    labels = tf.transpose(Y)\n    \n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits,labels = labels) + 0.001*(tf.nn.l2_loss(W1) + tf.nn.l2_loss(W2) + tf.nn.l2_loss(W3)))\n    \n    return cost","3cebd4f4":"#Define our neural network model\ndef neural_net(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001):\n\n    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n    (n_x, m) = X_train_flat.shape                     # (n_x: input size, m : number of examples in the train set)\n    n_y = Y_train.shape[0]                            # n_y : output size\n    costs = []                                        # To keep track of the cost\n    \n    # Create our placeholder variables of shape (n_x, n_y)\n    X, Y = create_placeholders(n_x, n_y)\n\n    # Initialize our parameters\n    parameters = initialize_params()\n    \n    # Forward propagation: Build the forward propagation in the tensorflow graph\n    Z3 = forward_prop(X, parameters)\n    \n    # Cost function: Add cost function to tensorflow graph\n    cost = cost_calc(Z3, Y,parameters)\n    \n    # Backpropagation: We will be using an Adam optimizer for our backward propogation algorithm\n    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n    \n    # Initialize all the variables\n    init = tf.global_variables_initializer()\n\n    # Start the session to compute the tensorflow graph\n    with tf.Session() as sess:\n        \n        # Run the initialization\n        sess.run(init)\n        \n        #Loop through 2000 iterations of our Adam optimizer to determine optimal parameters\n        for i in range (1,2000):   \n            a,b = sess.run([optimizer,cost],feed_dict={X: X_train, Y: Y_train})\n            costs.append(b)\n            \n        parameters = sess.run(parameters)\n        print (\"Parameters have been optimized.\")\n\n        # Calculate the correct predictions\n        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n\n        # Calculate accuracy on the test set\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n\n        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n        \n        plt.plot(np.squeeze(costs))\n        plt.ylabel('cost')\n        plt.xlabel('iterations')\n        plt.title(\"Learning rate =\" + str(learning_rate))\n        plt.show()\n\n    return parameters\n    \n    ","9f7b30e6":"parameters = neural_net(X_train_flat, Y_train, X_test_flat, Y_test.T, learning_rate = 0.0001,\n         )","abbc89cf":"X_single_test = X_test[7] #Let's use the 7th image in our test set for our individual example.\nY_single_test = Y_test[7]\nprint(\"Here's the 7th example in our test set.  This is sign-language for 7.\")\nplt.imshow(X_single_test)\nprint(\"The classification vector associated with this image is: \" + str(Y_single_test))\nprint(\"This is what our model should predict when we input this image.\")","b0b8d7d6":"X_single_test_flat = X_test_flat[:,7:8]\nZ3 = forward_prop(X_single_test_flat, parameters)\ninit = tf.global_variables_initializer()\nwith tf.Session() as sess:\n    sess.run(init)\n    a = sess.run(tf.sigmoid(Z3)) #We'll take the max of the sigmoid of the output vector to determine the prediction\nz = 1*(a == np.max(a))\nz.T\nif int(z[2]) == 1:\n    print(\"The image represents a 7 in sign-language\")\n    ","b973d64e":"# Step 2)  Splitting and massaging our data\n\nLet's split and massage our data to prepare it for our deep learning model.\n\n1. let's split our data into two sets - a training set and a test set.  Our training set will be used to train our deep learning model.  Our test set will be used to test it's accuracy.  We'll use sklearn to split our data quickly and efficiently, in a random shuffle.  2\/3 of our dataset will be used to train our model, and the remaining 1\/3 will be used for testing.\n\n1. Now, we must flatten our data.  Notice how our data has dimensions ( X , Y , Z ).  We would like to reduce the dimensionality to ( X , Y ). ","016daeac":"Our output vector is the same, and our neural network has accurately predicted what the image represents!  \n\nThis concludes the kernel.  \n\n# Thank you for reading!","80bb21de":"# Step 5)  Run our model, test the results\n\nOur model is complete!  Let's run our model and analyze it in the following steps:\n\n1. Check accuracy on the training set\n1. Check accuracy on the cross-validation set\n1. Plot out cost function as a function of iterations to be sure our algorithm is performing as intended.  This should be consistently decreasing.\n*Note: The model takes a few minutes to complete.  This could be shortened by implementing mini-batch gradient descent  (an alteration for another time!)","85973d02":"# Step 3)  Setting up the deep neural net using Tensorflow\n\nOur data is ready to be implemented into our model.  We will be building a three layer neural network with 25 nodes in the first layer, 15 in the second, and 10 in the output layer (as a result of this being a multiclass classification problem - the images can represent any of the 0-9 digits).\n\nLet's carry out the following steps to implement our deep learning model:\n\n1. Set up Tensorflow placeholder variables to be used later in our neural network as the train\/test input matrices and classification matrices\n1. Initialize our weight parameters using Xavier initialiation, and our intercept parameters as a vector of 0's\n1. Define our forward propogation algorithm\n1. Define our cost function and add regularization factors to it\n1. Bring all of the above together to create our model","1e3a77bb":"# Step 1)  Observing our data\n\nLet's start off by taking a look at the structure of our data.  This will involve the following steps:\n\n1. Loading the database of images into a Numpy array.  The data consists of 2062, 64x64 pixel images of sign-language representations of digits 0-9, which we will represent in a vector of pixel densities.\n\n1. Look at the structure of our pixel matrices.  This will allow us to gain a sense of the type of data we will be training our deep neural network with.\n\n1. Look at a few samples of the images we will be working with.","fccef646":"Now we know what our output from the neural network should be - a 1x10 zeros with a 1 in the third column.  This represents a 7 in sign language (the classification vectors from the dataset are out of order.  It would have been cleaner to have made the 8th entry in the classification vector represent a 7, for example)\n\nLet's take the sigmoid of our Z3 vector, set the maximum entry equal to 1, and the other entries equal to 0.  Recall that the outputs of our sigmoid function here are essentially \"probabilities\" of what our image represents.  E.g. is Z3[0] = 50%, there is a 50% chance that the image is a zero in sign language. ","8b7704f2":"# Step 4)  Create our deep learning model\n\nWe now have all of the components of our model set up.  Let's bring it all together and construct our deep learning model in the following steps\n\n1. Call our placeholder function\n1. Call our parameter initialization function\n1. Call our forward propagation algorithm and store it in variable Z3\n1. Call our cost function\n1. Define our optimization algorithm.  We will be using an Adam optimizer to train our parameters\n1. Run 2000 iterations of our algorithm to effectively train our parameters","1e5b2e27":"# Deep Learning with Tensorflow - Numerical Sign Language Recognition\n\nWelcome to my second machine learning project, and my first experience with computer vision and Tensorflow!  The purpose of this kernel is to use Tensorflow to construct a deep neural network that is capable of recognizing digits from 0-9 in sign language with reasonable accuracy.  \n\nLet's begin by importing the packages we will be using throughout this kernel.","5a79596d":"The accuracy on the training set is excellent, and on the test-set it's not too bad either!  The variance does look a little high, but for the purposes of this kernel, the accuracy is acceptable.\n\nSo overall, our model is working pretty well!  For completeness, let's take a look at how this algorithm works on a single example.\n\nLet's go ahead and pick an example and try it out."}}