{"cell_type":{"476c3132":"code","613946b4":"code","9cc86b9f":"code","a3daabb3":"code","9fb06fae":"code","c1a4e93e":"code","b798f9e3":"code","3130b6a2":"markdown","4402e6d7":"markdown","515da82c":"markdown","1aa9b9df":"markdown","242da007":"markdown","31e2aca3":"markdown","a6dd3ae3":"markdown","e64b272b":"markdown","75d915d8":"markdown"},"source":{"476c3132":"import numpy as np, pandas as pd\nimport warnings; warnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport tensorflow as tf","613946b4":"train = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntargs = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntest = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ny = targs[targs.columns[55]].values","9cc86b9f":"pca_ = PCA(n_components=2)\npca = pca_.fit_transform(train.drop([\"sig_id\", 'cp_type', 'cp_time', 'cp_dose'], axis=1))\npca_t = pca_.fit_transform(test.drop([\"sig_id\"], axis=1))\ntsne_ = TSNE(n_components=2)\ntsne = tsne_.fit_transform(train.drop([\"sig_id\", 'cp_type', 'cp_time', 'cp_dose'], axis=1))\nprint('Explained variance for PCA', pca_.explained_variance_ratio_.sum())","a3daabb3":"fig = plt.figure(figsize=(10, 10));colors=['green', 'red']\nplt.axis('off')\nfor color, i, ax, option in zip(colors, [0, 1], [121, 122], [pca, tsne]):\n    plt.scatter(tsne[y == i, 0], tsne[y == i, 1], color=color, s=1,\n                alpha=.8, marker='.')","9fb06fae":"fig, axs = plt.subplots(1, 2, figsize=(20, 10));colors=['green', 'red']\nfor color, i, ax, option in zip(colors, [0, 1], [121, 122], [pca, pca_t]):\n    axs[0].scatter(pca[y == i, 0], pca[y == i, 1], color=color, s=1,\n                alpha=.8, marker='.')\n    axs[1].scatter(pca_t[y == i, 0], pca_t[y == i, 1], color=color, s=1,\n                alpha=.8, marker='.')","c1a4e93e":"def create():\n    model = tf.keras.Sequential([\n    tf.keras.layers.Input(2),\n    tf.keras.layers.Dense(128),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Activation(\"relu\"),\n    tf.keras.layers.Dense(512),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.Dense(400),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(206, activation=\"sigmoid\")\n    ])\n    model.compile(optimizer=tf.optimizers.Adam(),\n                  loss='binary_crossentropy', \n                  )\n    return model\n","b798f9e3":"model = create()\nmodel.fit(tsne, targs.drop([\"sig_id\"], axis=1).values.astype(float), epochs=8, verbose=False)\npreds = model.predict(pca_t)\nfig = plt.figure(figsize=(9, 9));colors=['green', 'red']\nfor color, i, ax, option in zip(colors, [0, 1], [121, 122], [pca, tsne]):\n    plt.scatter(preds[y == i, 0], preds[y == i, 1], color=color, s=1,\n                alpha=.8, marker='.')","3130b6a2":"Now however, you might be wondering about how I selected the `y` variable in the code that follows. It is simply **the most balanced label in the whole data (unless I missed something glaringly obvious)**. The competition data in itself works particularly well with neural networks (and we have a nice neural network surprise at the end.)","4402e6d7":"This model has been taken from the popular kernel [keras Multilabel Neural Network](https:\/\/www.kaggle.com\/simakov\/keras-multilabel-neural-network-v1-2) and will be utilized to plot and cluster the output of this neural network..","515da82c":"Now let's check the output clusters - there are very, very few red points which means our model still has a long, long way to go in training. The model itself is pitifully small, so I have full confidence that the *ideal* way to plot the output activations would be to use a much larger network (perhaps LSTMs\/GRUs would do the trick?). \n\nAnyways, thank you for reading this kernel, and if you like it an upvote would be much appreciated. This is a demonstration of clusters in the data and where we can go from here - so please take away something from this as well and potentially improve on my work.","1aa9b9df":"So now we define the clustering: we fit a PCA on the training and test data and a tSNE to the training data. tSNE takes a much longer time than PCA, so expect to wait a fair bit (however contrarily in most instances tSNE is more trustworthy than a PCA when dealing with dangerous data).","242da007":"In this notebook I'm going to provide a demonstration of **how to appropriately cluster** using dimensionality reduction techniques such as PCA and tSNE. For more helpful resources check out **[this wonderful kernel by Tilii](https:\/\/www.kaggle.com\/tilii7\/dimensionality-reduction-pca-tsne?rvi=1)** and also be sure to check the documentation for some more in-depth explanations of the dimensionality reduction techniques.","31e2aca3":"Now however the PCA transformed data is completely, and by far much more differently clustered - almost all the reds are located in one place with several greens, which means we've not fully sequestered the reds from the greens in the data.","a6dd3ae3":"<h1><center>Clustering with PCA, NNs and tSNE<\/center><\/h1>\n    \n<hr>","e64b272b":"Now we take the plunge and plot the output of our tSNE plot, it looks like there's only one principal cluster and everything else's grouped into a lot of other, smaller clusters. You can see very few reds in the plot, exacerbating the imbalanced classes (side effect of dimensionality reduction?).","75d915d8":"We are now done with traditional clustering and are currently moving on **to checking the intermediate activations of a neural network.** (*NOTE: There are are only a few predictions so the output might not be as expected*). This might not necessarily produce a better result by any means over the PCA and tSNE."}}