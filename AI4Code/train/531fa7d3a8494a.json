{"cell_type":{"96ae65ca":"code","2a8ac72f":"code","099b3154":"code","e952b4ef":"code","88937eab":"code","e8279819":"code","27ebf03c":"code","f500c071":"code","3019aaa7":"code","19934fb1":"code","109c5dd1":"code","4f5ec192":"code","5340b2ac":"code","ec3c86a4":"code","3a4a5a1a":"code","df1cb636":"code","8b8b8149":"code","dbe62dc3":"code","8dd5b3c6":"code","d31fddfc":"markdown","b6283e90":"markdown","e2ff5b1f":"markdown","27374137":"markdown","9a4dcd62":"markdown","89f68944":"markdown","99634544":"markdown","64083ce0":"markdown","1a1f0541":"markdown","9e169ef7":"markdown","65b14d39":"markdown"},"source":{"96ae65ca":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\nimport unicodedata\nimport re\nimport os\nimport io\n\nnp.random.seed(42)\ntf.random.set_seed(42)\n\npath_to_file = '..\/input\/bilingual-sentence-pairs\/fra.txt'\n\n# If the notebook is running in colab the above file doesn't exist\nif not os.path.exists(path_to_file):\n    # if you are running this notebook in colab please download the file from \n    # the link above and put it in the current directory\n    path_to_file = os.path.join(os.curdir, 'fra.txt')","2a8ac72f":"def unicode_to_ascii(s):\n    return ''.join(c for c in unicodedata.normalize('NFD', s)\n            if unicodedata.category(c) != 'Mn')\n\ndef preprocess(s):\n    s = unicode_to_ascii(s.lower().strip())\n    s = re.sub(r\"([?.!,\u00bf])\", r\" \\1 \", s)\n    s = re.sub(r'[\" \"]+', \" \", s)\n    s = re.sub(r\"[^a-zA-Z?.!,\u00bf]+\", \" \", s)\n    s = s.strip()\n    # adding start-of-sequence (sos) token and end-of-sequence (eos) token\n    s = '<sos> ' + s + ' <eos>'\n    return s","099b3154":"def tokenize(language):\n    # Use <unk> token for unkown words\n    tokenizer = Tokenizer(filters='', oov_token='<unk>')\n    tokenizer.fit_on_texts(language)\n\n    tensor = tokenizer.texts_to_sequences(language)\n    tensor = pad_sequences(tensor, padding='post')\n    return tensor, tokenizer\n\ndef load_dataset(path, num_examples=None, prints=False) :\n    lines = open(path, encoding='UTF-8').read().strip().split('\\n')\n    \n    # list containing word pairs in the format: [[ENGLISH], [FRENCH]]\n    word_pairs = [[preprocess(w) for w in l.split('\\t')[:-1]] for l in lines[:num_examples]]\n    targ_lang, input_lang = zip(*word_pairs)\n\n    if prints:\n        print(targ_lang[-1])\n        print(input_lang[-1])\n        return\n    \n    input_tensor, input_tokenizer = tokenize(input_lang)\n    targ_tensor, targ_tokenizer = tokenize(targ_lang)\n\n    return input_tensor, targ_tensor, input_tokenizer, targ_tokenizer\n\nload_dataset(path_to_file, num_examples=1000, prints=True)","e952b4ef":"inp_tensor, targ_tensor, inp_lang, targ_lang = load_dataset(path_to_file)\ninp_tensor_train, inp_tensor_val, \\\ntarg_tensor_train, targ_tensor_val = \\\ntrain_test_split(inp_tensor, targ_tensor, test_size=0.2)\nprint(\"Input tensors: \", inp_tensor_train.shape, inp_tensor_val.shape)\nprint(\"Target tensors: \", targ_tensor_train.shape, targ_tensor_val.shape)","88937eab":"buffer_size = len(inp_tensor_train)\nbatch_size = 64\nsteps_per_epoch = len(inp_tensor_train) \/\/ batch_size\nembedding_dim = 256\nunits = 1024 \nvocab_inp_size = len(inp_lang.index_word) + 1\nvocab_targ_size = len(targ_lang.index_word) + 1\n\ndef create_dataset(shuffle=True, buffer_size=buffer_size, batch_size=batch_size):\n    ds = tf.data.Dataset.from_tensor_slices((inp_tensor_train, targ_tensor_train))\n    if shuffle:\n        ds = ds.shuffle(buffer_size)\n    ds = ds.batch(batch_size, drop_remainder=True)\n    return ds.prefetch(1)\n\ntrain_dataset = create_dataset()\nvalid_dataset = create_dataset(shuffle=False)\ninp_batch, targ_batch = next(iter(train_dataset))\ninp_batch.shape, targ_batch.shape","e8279819":"from tensorflow.keras.layers import Embedding, LSTM, dot, Dense\n\nclass Encoder(tf.keras.Model):\n    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size):\n        super().__init__()\n        self.batch_size = batch_size\n        self.enc_units = enc_units\n        self.embedding = Embedding(vocab_size, embedding_dim)\n        self.lstm = LSTM(self.enc_units, return_state=True, return_sequences=True)\n\n    def call(self, inp_batch, state_h, state_c):\n        inp_batch = self.embedding(inp_batch)\n        output, state_h, statec = self.lstm(inp_batch, initial_state=[state_h, state_c])\n        return output, state_h, state_c\n\n    def initialize_state(self):\n        return [tf.zeros((self.batch_size, self.enc_units)), tf.zeros((self.batch_size, self.enc_units))]\n\nencoder = Encoder(vocab_inp_size, embedding_dim, units, batch_size)\n[enc_state_h, enc_state_c] = encoder.initialize_state()\nenc_output, enc_state_h, enc_state_c = encoder(inp_batch, enc_state_h, enc_state_c)\nprint (f'encoder output:       (batch size, seq length, enc_units)  {enc_output.shape}')\nprint (f'encoder hidden state h: (batch size, enc_units)            {enc_state_h.shape}')\nprint (f'encoder hidden state c: (batch size, enc_units)            {enc_state_c.shape}')","27ebf03c":"def print_shapes(enc_output, dec_state, score, attention_weights, context_vector):\n    print(f\"btach_size: {batch_size}\")\n    print(f\"seq_length: {inp_tensor_train.shape[1]}\")\n    print(f\"enc_units: {units}\")\n    print()\n    print(f\"enc_output:        {enc_output.shape}\")\n    print(f\"dec_state:         {dec_state.shape}\")\n    print(f\"score:             {score.shape}\")\n    print(f\"attention_weights: {attention_weights.shape}\")\n    print(f\"context_vector:    {context_vector.shape}\")","f500c071":"class LuongAttention(tf.keras.layers.Layer):\n    def __init(self):\n        super().__init__()\n\n    def call(self, dec_state_h, dec_state_c, enc_output, prints=False):\n        dec_state = tf.add(dec_state_h, dec_state_c)\n        dec_state = dec_state[:, :, tf.newaxis]\n    \n        score = dot([enc_output, dec_state], axes=[2, 1])\n        attention_weights = tf.nn.softmax(score, axis=1)\n\n        context_vector = attention_weights * enc_output\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n\n        if prints:\n            print_shapes(enc_output, dec_state, score, attention_weights, context_vector)\n            return\n        return context_vector, attention_weights\n\nattention_layer = LuongAttention()\n# at the beginning we set the decoder state to the encoder state\ndec_state_h, dec_state_c = enc_state_h, enc_state_c\nattention_layer(dec_state_h, dec_state_c, enc_output, prints=True)","3019aaa7":"class Decoder(tf.keras.Model):\n    def __init__(self, vocab_size, embedding_dim, dec_units, batch_size):\n        super().__init__()\n        self.batch_size = batch_size\n        self.embedding = Embedding(vocab_size, embedding_dim)\n        self.lstm = LSTM(dec_units, return_state=True, return_sequences=True)\n        self.fc = Dense(vocab_size, activation=\"softmax\")\n        self.attention = LuongAttention()\n\n    def call(self, dec_input, dec_state_h, dec_state_c, enc_output):\n        context_vector, attention_weights = self.attention(dec_state_h, dec_state_c, enc_output)\n        # context_vactor: (batch_size, 1, embedding_dim)\n        context_vector = context_vector[:, tf.newaxis, :]\n\n        # x: (batch_size, 1, embedding_dim)\n        x = self.embedding(dec_input)\n        # x: (batch_size, 1, embedding_dim + enc_units)\n        x = tf.concat([context_vector, x], axis=-1)\n        \n        # output: (batch_size, 1, dec_units), state: (batch_size, dec_units)\n        output, state_h, state_c = self.lstm(x)\n        # output: (batch_size, dec_units)\n        output = tf.reshape(output, (-1, output.shape[2]))\n        x = self.fc(output)\n        return x, state_h, state_c, attention_weights\n\ndecoder = Decoder(vocab_targ_size, embedding_dim, units, batch_size)\ndec_input = tf.random.uniform((batch_size, 1))\ndec_output, dec_state_h, dec_state_c, _ = decoder(dec_input, dec_state_h, dec_state_c, enc_output)\n\nprint (f'decoder output:       (batch size, vocab_size)  {dec_output.shape}')\nprint (f'decoder hidden state h: (batch size, dec_units)   {dec_state_h.shape}')\nprint (f'decoder hidden state c: (batch size, dec_units)   {dec_state_c.shape}')","19934fb1":"def print_status_bar(iteration, total, loss):    \n    metrics = \"loss: {:.4f}\".format(loss) \n    end = \"\" if iteration < total else \"\\n\"\n    print(\"\\r{}\/{} - \".format(iteration, total) + metrics,end=end)","109c5dd1":"optimizer = tf.keras.optimizers.Adam()\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(reduction='none')\n\ndef loss_function(real, pred):\n  mask = tf.math.logical_not(tf.math.equal(real, 0))\n  loss_ = loss_object(real, pred)\n\n  mask = tf.cast(mask, dtype=loss_.dtype)\n  loss_ *= mask\n  return tf.reduce_mean(loss_)\n\n@tf.function\ndef train_step(inp_batch, targ_batch, enc_state_h, enc_state_c):\n    loss = 0\n\n    with tf.GradientTape() as tape:\n        enc_output, enc_state_h, enc_state_c = encoder(inp_batch, enc_state_h, enc_state_c)\n        # at the beginning we set the decoder state to the encoder state\n        dec_state_h, dec_state_c = enc_state_h, enc_state_c\n\n        # at the begining we feed the <sos> token as input for the decoder, \n        # then we will feed the target as input\n        dec_input = tf.expand_dims([targ_lang.word_index['<sos>']] * batch_size, 1)\n        for t in range(1, targ_batch.shape[1]): # targ_batch.shape[1] == seq length\n            predictions, dec_state_h, dec_state_c, _ = decoder(dec_input, dec_state_h, dec_state_c, enc_output)\n            loss += loss_function(targ_batch[:, t], predictions)\n\n            dec_input = tf.expand_dims(targ_batch[:, t], 1)\n        \n    batch_loss = loss \/ int(targ_batch.shape[1])\n    variables = encoder.trainable_variables + decoder.trainable_variables\n    gradients = tape.gradient(loss, variables)\n    optimizer.apply_gradients(zip(gradients, variables))\n    return batch_loss","4f5ec192":"epochs = 15\n\nfor epoch in range(epochs):\n    print(\"Epoch {}\/{}\".format(epoch + 1, epochs))\n    [enc_state_h, enc_state_c] = encoder.initialize_state()\n    total_loss = 0\n    \n    for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n        batch_loss = train_step(inp, targ, enc_state_h, enc_state_c)\n        total_loss += batch_loss\n\n        print_status_bar(batch, steps_per_epoch, batch_loss.numpy())\n    print_status_bar(steps_per_epoch, steps_per_epoch, total_loss \/ steps_per_epoch)","5340b2ac":"def evaluate(sentence, targ_tensor, inp_tensor):\n    # targ_tensor.shape[1] == max seq length for the target language (EN)\n    # inp_tensor.shape[1] == max seq length for the input language (FR)\n    attention_plot = np.zeros((targ_tensor.shape[1], inp_tensor.shape[1]))\n    \n    sentence = preprocess(sentence)\n\n    inputs = inp_lang.texts_to_sequences([sentence])\n    inputs = pad_sequences(inputs, maxlen=inp_tensor.shape[1], padding='post')\n    inputs = tf.convert_to_tensor(inputs)\n    result = ''\n\n    [enc_state_h, enc_state_c] = [tf.zeros((1, units)), tf.zeros((1, units))]\n    enc_output, enc_state_h, enc_state_c = encoder(inputs, enc_state_h, enc_state_c)\n    dec_state_h, dec_state_c = enc_state_h, enc_state_c\n    dec_input = tf.expand_dims([targ_lang.word_index['<sos>']], 0)\n\n    for t in range(targ_tensor.shape[1]):\n        predictions, dec_state_h, dec_state_c, attention_weights = decoder(dec_input, \n                                                                           dec_state_h, \n                                                                           dec_state_c, \n                                                                           enc_output)\n        \n        attention_weights = tf.reshape(attention_weights, (-1, ))\n        attention_plot[t] = attention_weights.numpy()\n\n        predicted_id = tf.argmax(predictions[0]).numpy()\n        result += targ_lang.index_word[predicted_id] + ' '\n\n        # stop prediction\n        if targ_lang.index_word[predicted_id] == '<eos>':\n            return result, sentence, attention_plot\n\n        dec_input = tf.expand_dims([predicted_id], 0)\n\n    return result, sentence, attention_plot","ec3c86a4":"# function for plotting the attention weights to visualize how the model works internally\ndef plot_attention(attention, sentence, predicted_sentence):\n  fig = plt.figure(figsize=(10,10))\n  ax = fig.add_subplot(1, 1, 1)\n  ax.matshow(attention, cmap='viridis')\n    \n  ax.set_xticklabels([''] + sentence, rotation=90)\n  ax.set_yticklabels([''] + predicted_sentence)\n\n  plt.show()\n\n\ndef translate(sentence, ground_truth=None, plot_weights=True):\n    result, sentence, attention_plot = evaluate(sentence, targ_tensor, inp_tensor)\n\n    print(f'{\"Input:\":15s} {sentence}')\n    print(f'{\"Prediction:\":15s} {result}')\n    if ground_truth: print(f'{\"Ground truth:\":15s} {ground_truth}') \n    \n    if plot_weights:\n        attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n        plot_attention(attention_plot, sentence.split(' '), result.split(' '))","3a4a5a1a":"def preprocess_sequence(seq, language):\n    sentence = language.sequences_to_texts([seq.numpy()])[0]\n    sentence = sentence.split(' ')\n    sentence = [s for s in sentence if s != '<sos>' and s != '<eos>' and s != '<unk>']\n    return ' '.join(sentence)","df1cb636":"for inp_batch, targ_batch in train_dataset.take(20):\n    for inp, targ in zip(inp_batch, targ_batch):\n        sentence = preprocess_sequence(inp, inp_lang)\n        ground_truth = preprocess_sequence(targ, targ_lang)\n        translate(sentence, ground_truth, plot_weights=False)\n        print()\n        break","8b8b8149":"translate('sais-tu traduire cette phrase?')","dbe62dc3":"translate('Vous pouvez expliquer encore une fois?')","8dd5b3c6":"translate('Traduction automatique neuronale avec attention luong .')","d31fddfc":"# Create the Dataset","b6283e90":"# Load and Split de Data","e2ff5b1f":"# Translation with Attention Plots","27374137":"# Build a Custom Training Loop","9a4dcd62":"# TODO\n\nTheir are few things to do to improve this model:\n\n* implement an accuracy function so that we can have an idea how well the model is doing.\n\n* the decoder output a probabilty for each word in the vocabilary (14410), this slow down training terribly. Instead, we can output a probability for a random sample of words (containing the correct word) using tf.nn.sampled_softmax_loss() function.\n\n* Make a nice interface so that users can play with the model.\n\n# References\n\n* [Neural machine translation with attention](https:\/\/www.tensorflow.org\/tutorials\/text\/nmt_with_attention)","89f68944":"# Import","99634544":"In this project we will build and encoder-decoder model with Luong attention that will translate French sentences to English.\n\nThe dataset used in this notebook is provided by: [http:\/\/www.manythings.org\/anki\/](http:\/\/www.manythings.org\/anki\/).\n\nThis dataset contains approximatly 180 000 tab-delimited English-French sentence pairs.","64083ce0":"# Evaluation on the Validation Set\n\nLet's try to translate some sentences from the validation set and see how well the model is:","1a1f0541":"# Create the Model\n\nThe model will be similar to the TensorFlow tutorial except that I will use Luong attention instead of Bahdanau and LSTM layer instead of GRU.","9e169ef7":"# Translate","65b14d39":"# Download and preprocess the data"}}