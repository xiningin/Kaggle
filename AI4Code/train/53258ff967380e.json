{"cell_type":{"940c2bb1":"code","c6440181":"code","2cb70e4a":"code","b3794359":"code","14d3bf05":"code","04319440":"code","bd61d2f0":"code","c45850cb":"code","3e339365":"code","1c125b4c":"code","128c4ba6":"code","26376942":"code","2d29529d":"code","94e3218b":"code","a8bfbfc9":"code","41f611f1":"code","50a60632":"code","d85c4a82":"code","e05bf603":"code","2406fb27":"code","8501abc0":"code","99a7700a":"code","994d0454":"code","102ac2e3":"code","e665f5dd":"code","330ec32d":"code","30e4da78":"code","e76073b3":"code","8cc36bb4":"code","6311f718":"code","1b7b47ce":"code","6ba5332a":"code","464d6023":"code","dbe22311":"code","65007c3f":"code","038777bd":"code","e063a0ad":"code","670c4d1f":"code","a849e447":"code","69f844a3":"code","1cbf5c4c":"code","c5e3e012":"code","156f66a6":"code","f196c2e1":"code","f8d7cae8":"code","861f95f8":"code","904d9919":"code","d0f683a4":"code","18283027":"code","7cd03f03":"code","bc885862":"code","a9e24267":"code","65ee6873":"code","c325f73a":"code","c67ec701":"code","2f4845e5":"code","68ee051c":"code","e5f35fc5":"code","bdfc5e72":"code","f73bb5a1":"code","ed18141d":"code","8f2d7511":"code","766df453":"code","5b9b9f60":"code","8359fc9d":"code","3c335ae8":"code","4400936a":"markdown","fb41777d":"markdown","1acfbc20":"markdown","08ec31b8":"markdown","ca593bc0":"markdown","36b6e7b8":"markdown","b80d522c":"markdown","4c750ea9":"markdown","08d4f290":"markdown","4792e53d":"markdown","f287d600":"markdown","44ff3a27":"markdown","c0f14d0a":"markdown","5e50990c":"markdown","1292b3f1":"markdown","d92e3d90":"markdown","8b9e3466":"markdown","b0212b7d":"markdown","8e279f55":"markdown","0f6ded4c":"markdown","a31877b6":"markdown","3d12bdad":"markdown","b0a8b783":"markdown","751a9fea":"markdown","834da9af":"markdown","19535c69":"markdown","e26ba935":"markdown","c87522ea":"markdown","f780ecf5":"markdown","e80ab64e":"markdown","238f627a":"markdown","2b7d3bd7":"markdown","41ab56d1":"markdown","8e3ae2b1":"markdown","600ddb8c":"markdown","64e701ed":"markdown","91824d46":"markdown","238f9399":"markdown","10bcefa5":"markdown","dc939379":"markdown","17797246":"markdown","ce73dd5d":"markdown","b7adc81f":"markdown","0c4ab617":"markdown","0387056c":"markdown","4fd2c4cf":"markdown","fa891f8f":"markdown","a7c11305":"markdown","bf3980fb":"markdown","82e2f8a9":"markdown","0ed3b94d":"markdown","beecef70":"markdown","d07ff605":"markdown","deb9e982":"markdown","7db6a298":"markdown","e2768e1f":"markdown","c1dc336d":"markdown","daa35baf":"markdown"},"source":{"940c2bb1":"%%capture\n!pip install advertools","c6440181":"import advertools as adv\nimport pandas as pd\npd.options.display.max_columns = None","2cb70e4a":"# robots_df = adv.robotstxt_to_df('https:\/\/www.whitehouse.gov\/robots.txt')\n\nrobots_df = pd.read_csv('\/kaggle\/input\/the-white-house-website\/robotstxt_df.csv')\nrobots_df","b3794359":"# sitemap_df = adv.sitemap_to_df('https:\/\/www.whitehouse.gov\/sitemap_index.xml')\n\nsitemap_df = pd.read_csv('\/kaggle\/input\/the-white-house-website\/sitemap_df.csv', parse_dates=['lastmod'])\nsitemap_df.sample(5)","14d3bf05":"sitemap_df.set_index('lastmod').resample('A')['loc'].count()","04319440":"sitemap_df['sitemap'].value_counts()","bd61d2f0":"selectors = {'briefing_title': '.page-header__title::text',\n             'briefing_date': 'time::text',\n             'briefing_body_text': '.editor p::text',\n             'briefing_category': '.issue-flag a::text'}","c45850cb":"sitemaps_unique = sitemap_df['sitemap'].drop_duplicates().tolist()\nsitemaps_unique","3e339365":"# for i, sitemap in enumerate(sitemaps_unique):\n#     df = sitemap_df[sitemap_df['sitemap']==sitemaps_unique[i]]\n#     adv.crawl(df['loc'], f'crawls\/wh_crawl_{i+1}.csv', css_selectors=selectors)","1c125b4c":"df = pd.DataFrame({'colors': ['blue@@green@@yellow', 'red@@blue', 'green@@blue@@pink@@orange'],\n                   'months': ['Jan', 'Feb', 'Mar']})\ndf.style.set_caption('How do we count the colors?')","128c4ba6":"df['colors'].str.split('@@')","26376942":"df['colors'].str.split('@@').explode() ","2d29529d":"df['colors'].str.split('@@').explode().value_counts()","94e3218b":"df.assign(colors_split=df['colors'].str.split('@@'))","a8bfbfc9":"df.assign(colors_split=df['colors'].str.split('@@')).explode('colors_split')","41f611f1":"month_color =  {month: color.split('@@') for month, color in zip(df['months'], df['colors'])}\nmonth_color","50a60632":"list()","d85c4a82":"from collections import defaultdict\n\ndd = defaultdict(list)\n\nfor month, color_list in month_color.items():\n    for color in color_list:\n        dd[color].append(month)","e05bf603":"dict(dd)","2406fb27":"import os\ncrawl_df = pd.concat([pd.read_csv('\/kaggle\/input\/the-white-house-website\/' + file)\n                      for file in os.listdir('\/kaggle\/input\/the-white-house-website\/') if 'wh_crawl' in file],\n                     ignore_index=True)\ncrawl_df.head(2)","8501abc0":"crawl_df.filter(regex='briefing')","99a7700a":"crawl_df.info()","994d0454":"set(crawl_df['url']).difference(crawl_df['url_redirected_to'])","102ac2e3":"set(crawl_df['url_redirected_to']).difference(crawl_df['url'])","e665f5dd":"crawl_df['status'].value_counts()","330ec32d":"pd.cut(crawl_df['size'], bins=10).value_counts()","30e4da78":"pd.cut(crawl_df['size'], bins=10).value_counts().sort_index()","e76073b3":"crawl_df['title'].duplicated().mean()","8cc36bb4":"crawl_df['title'].str.split('@@').str.len().value_counts()[:10]","6311f718":"adv.emoji_search('think')","1b7b47ce":"crawl_df['title'].str.split('@@', expand=True)","6ba5332a":"adv.emoji_search('scream')","464d6023":"(crawl_df\n ['title'].str.split('@@')\n .explode()\n .value_counts()\n .reset_index()\n .rename(columns={'index': 'title_tags', 'title': 'count'})\n [:10])","dbe22311":"(crawl_df\n ['title'].str.replace('@@', ' ')\n .str.split()\n .explode()\n .value_counts()\n .reset_index()\n .rename(columns={'index': 'title_tags', 'title': 'count'})\n [:15])","65007c3f":"(pd.cut(\n    crawl_df['title']\n    .str.split('@@')\n    .str[0]\n    .str.len(), [0, 40, 50, 60, 70, 80, 90, 999])\n .value_counts()\n .sort_index()\n .reset_index()\n .rename(columns={'index': 'Title text length in characters (range)', 'title': 'count'})\n .style.background_gradient('cividis')\n .format({'count': '{:,}'})\n .set_caption('Distribution of title tag lengths - thewhitehouse.com'))","038777bd":"crawl_df['h1'].str.split('@@').str.len().value_counts()","e063a0ad":"(crawl_df\n ['h1'].str.split('@@')\n .explode()\n .value_counts()\n .reset_index()\n .rename(columns={'index': 'h1_tags',\n                  'h1': 'count'})[:15])","670c4d1f":"crawl_df['h2'].str.split('@@').str.len().value_counts()","a849e447":"crawl_df.filter(regex='links').apply(lambda series: series.str[:25])","69f844a3":"from urllib.parse import urlparse\n\nurl = 'https:\/\/www.example.com\/category\/sub-cat\/article-title;some_param?one=1&two=2&three=3#fragment'\n\nparsed = urlparse(url)\nparsed","1cbf5c4c":"parsed.netloc","c5e3e012":"parsed.query","156f66a6":"urlparse('anystring\/http:\/\/hello')","f196c2e1":"(crawl_df['links_url']\n .str.split('@@')\n .str.len()\n .value_counts()\n .reset_index()\n .rename(columns={'index':'links_on_page',\n                  'links_url': 'count'})\n .head(15))","f8d7cae8":"internal_links = (crawl_df\n                  ['links_url'].str.split('@@')\n                  .explode()\n                  .apply(lambda s: s if 'https:\/\/www.whitehouse.gov' in s else None)\n                  .dropna())\ninternal_links.head()","861f95f8":"(pd.Series(urlparse(url).path\n           for url in internal_links)\n .value_counts()\n .reset_index()\n .rename(columns={'index': 'path', 0: 'count'})\n .head(50))","904d9919":"external_links = (crawl_df\n                  ['links_url'].str.split('@@')\n                  .explode()\n                  .apply(lambda s: s if 'https:\/\/www.whitehouse.gov' not in s else None)\n                  .dropna())\nexternal_links","d0f683a4":"external_domains = pd.Series(urlparse(url).netloc for url in external_links).value_counts()\nprint('external domains linked to: ', external_domains.index.nunique(), '\\n')\nexternal_domains[:10]","18283027":"external_links_social = pd.Series(urlparse(url).netloc for url in external_links).value_counts()[:5]\nexternal_links_social","7cd03f03":"external_links_nonsocial = (pd.Series(link for link in external_links\n                                      if urlparse(link).netloc not in external_links_social))\nprint('Unique external non-social links:', external_links_nonsocial.nunique(), '\\n')\nexternal_links_nonsocial","bc885862":"# adv.crawl(external_links_nonsocial.drop_duplicates(), 'external_links.csv', follow_links=False)\n# external_links_df = pd.read_csv('external_links.csv')\n# external_links_df[external_links_df['status']>400]['url']","a9e24267":"link_text_counts = (crawl_df\n                    ['links_text'].str.split('@@')\n                    .explode()\n                    .str.strip()\n                    .value_counts())\nlink_text_counts[:60].reset_index()","65ee6873":"adv.word_frequency(crawl_df\n                   ['links_text'].str.split('@@')\n                   .explode()\n                   .str.strip()).iloc[:20, :2]","c325f73a":"adv.word_frequency(crawl_df                   \n                   ['links_text'].str.split('@@')\n                   .explode()\n                   .str.strip(),\n                   phrase_len=2).iloc[:20, :2]","c67ec701":"urls_links = crawl_df[['url', 'links_url']].copy()\nurls_links.head(3)","2f4845e5":"urls_links['internal_links'] = [[link for link in links.split('@@') if 'www.whitehouse.gov' in link] for links in urls_links['links_url']]","68ee051c":"outgoing_internal_links = {link: internal_links for link, internal_links in zip(urls_links['url'], urls_links['internal_links'])}\noutgoing_internal_links","e5f35fc5":"incoming_internal_links = defaultdict(list)\n\nfor page, outgoing in outgoing_internal_links.items():\n    for link in outgoing:\n        incoming_internal_links[link].append(page)\n# incoming_internal_links = dict(incoming_internal_links)\nincoming_internal_links","bdfc5e72":"len(incoming_internal_links), len(outgoing_internal_links)","f73bb5a1":"incoming_internal_links_counts ={key: len(val) for key, val in incoming_internal_links.items()}\npd.DataFrame({'url': list(incoming_internal_links_counts.keys()),\n              'incoming_links_count': list(incoming_internal_links_counts.values())})['incoming_links_count'].value_counts()","ed18141d":"incoming_internal_links['https:\/\/www.whitehouse.gov\/briefings-statements\/first-lady-melania-trump-announces-chief-staff\/?utm_source=link']","8f2d7511":"[x for x in incoming_internal_links_counts.items() if 'https:\/\/www.whitehouse.gov\/briefings-statements\/first-lady-melania-trump-announces-chief-staff' in x[0]] ","766df453":"(crawl_df['img_src']\n .str.split('@@').str.len()\n .value_counts()\n .reset_index()\n .rename(columns={'index':'images_on_page',\n                  'img_src': 'count'})\n .head(15))","5b9b9f60":"(crawl_df['img_src']\n .str.split('@@').str.len()\n .value_counts()\n .reset_index()\n .rename(columns={'index':'images_on_page',\n                  'img_src': 'count'})\n .sort_values('images_on_page')\n .head(15)\n)","8359fc9d":"crawl_df.filter(regex='resp_headers').sample(5)","3c335ae8":"crawl_df.filter(regex='request_headers').sample(5)","4400936a":"## \ud83d\ude31 \ud83d\ude31 \ud83d\ude31 \ud83d\ude31 \ud83d\ude31","fb41777d":"We get the values split correctly, but we end up with lists instead of strings. The lists also have different lengths. So how do we deal with this? How do we count the colors?  \n#### `explode`!","1acfbc20":"The parsed URL is split into its elements, and those elements can be retreived using dot notation: ","08ec31b8":"<a id='xml_sitemaps'><\/a>\n# Sitemaps\n\nNow we get the sitemaps using the `sitemap_to_df` function:","ca593bc0":"<a id=\"response_headers\"><\/a>\n# Response Headers\nThese columns all start with `resp_headers_` and vary from site to site. Sometimes you get five or six columns, and sometimes many more.","36b6e7b8":"* `url` is the original url sent in the request. In some cases you have redirects, and the URL that you request might be redirected to another one. These URLs will be savd in the `url_redirected_to` column. So `url_redirected_to` is the one we are actually dealing with.  \nLet's see if we have any differences between them. We compare the difference using set operations:","b80d522c":"`body_text` is already a standard element that is provided by the `crawl` function. The problem is that by default it cannot know what exactly constitutes the \"body text\" of a page. So it extracts all the `p`, `span`, and `li` elements within `body` tags from pages, which might contain some additional noise. So in this case I selected a specific CSS selector for that. \n\n<a id=\"crawling\"><\/a>\n# Crawling\n\nNow that we have the full list of the sitemap URLs we have two options: \n1. **Crawl the sitemap URLs:** This is a straightforward approach, and goes through the given URLs only, which I have done in this example. This approach is good if you know exactly what you are looking for (and not checking if there are hidden pages, or issues with the sitemaps). You might use this for monitoring purposes as well.\n2. **Crawl the website starting from the home page and following links (`follow_links=True`):** This might be a better approach for an SEO audit because you crawl independently from the sitemap and can then compare the URLs you were able to discover through links with what is available in the sitemap. You might discover pages that are not in the sitemaps and vice versa, and this can really help in understanding some issues you might come across. I recommend the second approach for this situation, but I did the first one as this is just to demonstrate a way for exploring the output file.  \n\nNow, we'll take the unique sitemaps, crawl each set of URLs separately, and save each output to a different file. ","4c750ea9":"Keep in mind that this function dot not _validate_ URLs. It is only useful when you know that you are dealing with correct URLs which we do in this case because they are extracted from pages. Otherwise you will get meaningless results: ","08d4f290":"## `<title>` Tag Lengths\nThis is another important thing that you might want to check.  \nThe first tag is probably what is going to be picked up by search engines like in this case (where we have multiple title tags in pages). You can easily see that the first ones are the relevant descriptive ones, that are actually related to the page's content.  \nSo we can extract the first title tags, and check their lengths. \n\nSince 50-60 characters is a generally accepted guideline for the good title length, I split the title lengths into groups that fall between [0, 40, 50, 60, 70, 80, 90, 999]. Feel free to experiment with other splits.","4792e53d":"# `explode`  \ud83d\udca3\ud83d\udd25\ud83d\udca3\ud83d\udd25\ud83d\udca3\n\nNow we have the files ready, but before analyzing them you need to make sure that you master the usage of the pandas `explode` method. (skip if you know it already).  \nLet's say you have a DataFrame like this, where multiple values are delimited with `@@` although they each form a single string of characters. So we need to split them and handle each of the elements. ","f287d600":"20.9% of the page titles are duplicated. Let's check if they have multiple titles per page. \n\n> ### It is usually a good practice to always split by `@@` for text columns in case they have multiple values","44ff3a27":"So, we have 7,863 pages with seven title tags each. ","c0f14d0a":"<a id=\"title_tags\"><\/a>\n# `<title>` Tags\n\nLet's see first if we have duplicated title tags. Now *we* get to be GSC!","5e50990c":"Notice that the original information is preserved. All rows and indexes have been duplicated, and each color still belongs to the same correct row.  \n\nAnother way you might want to handle this situation is by converting to a dictionary, mapping months to colors. But watch out, this only works if you have unique keys. If you have duplicates, then you will overwrite the original values, and mess up the data.","1292b3f1":"1,307 unique links out of a total of 1,430.  \nOne thing you can do with them is `crawl` them and setting `follow_links=False`. Then open the file into a DataFrame, and get the URLs of the rows where the status code is greater than 400. For example: ","d92e3d90":"# SEO Crawl Analysis Template\n\n# Please check the updated [SEO crawl analysis template](https:\/\/www.kaggle.com\/eliasdabbas\/seo-crawl-analysis-template)\n\n[advertools](https:\/\/advertools.readthedocs.io\/) is a Python package for online marketing and advertising with many functions for SEO, SEM, social media, and text analysis. `crawl` is a funtion that.. crawls websites!\n\nAfter running [crawl](https:\/\/advertools.readthedocs.io\/en\/master\/advertools.spider.html#advertools.spider.crawl) and saving your output file, you need to take a look and start your analysis. This is meant to be template for an initial exploration of the data that is typically available in the output.  \nWe will go through the default columns, conventions in storing and naming data, and exploratory techniques that you will probably also want to run during your first exploration of the dataset.\n\n* [robots.txt](#robotstxt)\n* [XML Sitemaps](#xml_sitemaps)\n* [Crawling](#crawling)\n* [Status Codes](#status_codes)\n* [Title Tags](#title_tags)\n* [Header Tags](#header_tags)\n* [Hyper Links](#links)\n* [Anchor Text](#anchor_text)\n* [Images](#images)\n* [Response & Request Headers](#response_headers)","8b9e3466":"Looking more closely at titles, here we split, and then explode, making it easy to count the values of individual title tags. ","b0212b7d":"url | links_url\n-----|---------\nhttp:\/\/example.com | http:\/\/one.com@@http:\/\/two.com@@http:\/\/three.com\nhttp:\/\/example.com\/1 | http:\/\/one.com\/hello@@http:\/\/two.com\n\nLet's start!","8e279f55":"`explode`:","0f6ded4c":"An empty list will be given, to which we will append the desired item.  \nI'm aware that this is not a sufficient explanation if it's your first time with `defaultdict` so please check the documentation and some examples online for more info. Let's see it in action.  \nFor our `month_color` dictionary, we will loop over the items:","a31877b6":"How many links does each page have on average?","3d12bdad":"It seems all URLs are fine. \n\nThe `size` column shows the size of the pages in bytes. This is not very helpful, and can be misleading. It only measures the text of the response of the page, but does not measure images, or videos, scripts that might be calling certain services, the possibilities are endless. Keep this in mind. \nThe `cut` function can help in putting the page sizes into bins. There are different ways of `cut`ting series objects, feel free to explore other ways.","b0a8b783":"Note that the index values have been duplicated, and this preserves the position of the colors. So even though they each have their own row, we can tell which original row they belong to by looking at their index.  \nAnd now we can count the colors: ","751a9fea":"58,797 links.  \nHere it is easier to check the domains that they point to in order to get an overview. ","834da9af":"`set()` means the empty set. So the two columns are identical (no redirects), and it doesn't matter which one we use. In case there are redirects you should mainly use the `url_redirected_to`.\n\n<a id='status_codes'><\/a>\n# Status codes (200, 301, 302, 403, 404, etc.)\nCounting the values of the status codes that we have can immediately show if we have any issues and of what type:","19535c69":"Similar to what we saw with internal links, we have the big ones at the top, suddenly going down to 82. Let's remove those, and check the rest. ","e26ba935":"\"Words\" don't necessarily have to be single words. They can be phrases consisting of two or more words.  \nThe [word_frequency](https:\/\/advertools.readthedocs.io\/en\/master\/advertools.word_frequency.html) function has the option of specifying this through the `phrase_len` parameter. Now we can take a look at phrases consisting of two words (bigrams is the technical term).","c87522ea":"The site chosen for this template was because it had a few thousand pages (not too few, and not too many), and it has a simple HTML structure, so it was striaghtforward to extract the required elements. Many times you will face different kinds of issues when crawling and scraping pages. I'm sure you've heard of [The White House](https:\/\/www.whitehouse.gov\/).\n\n<a id='robotstxt'><\/a>\n# Robots.txt\n\nWe are pretending to be Google, right?  \nSo we start by getting the robotstxt file. ","f780ecf5":"Another way of looking at titles, is by replacing the `@@` with spaces, and counting individual words in the titles. ","e80ab64e":"So the majority of pages have 52-55 links each. We can now separate them in to internal and external links based on whether or not they contain \"https:\/\/www.whitehouse.gov\": ","238f627a":"In almost all pages (8,167) we have one h1 tag.  \nLet's see what these tags are, and count them (also checking if there are duplicates): ","2b7d3bd7":"# Internal Links Analysis\nA good starting point for this would be to have to datasets for internal liks: \n1. Outgoing links: a mapping from each URL to all the internal outgoing links it contains\n2. Incoming links: a mapping from each URL to all the internal incoming links it contains\n\nA simple example to illustrate: \n\nLet's say in our site we have four URLs: \n\nURL | Links to\n--|------\nexample.com\/page_A | B, C\nexample.com\/page_B | A\nexample.com\/page_C | A, B, D\nexample.com\/page_D | A, B\n\nThis is already available, we have the `url` column and we have the `links_url` column. We just have to make sure it only contains internal links, and this is easy to accomplish. \n\nThe next step is to invert this mapping with `defaultdict` so we can have a dictionary where each key has a URL, and its values are the other internal URLs **pointing to it**.\n\nSo, for the above example, we can visually infer the following: \n\nURL | Pages linking to it\n--|------\nexample.com\/page_A | B, C, D\nexample.com\/page_B | A, C, D\nexample.com\/page_C | A\nexample.com\/page_D | C\n\n","41ab56d1":"<a id=\"header_tags\"><\/a>\n# `<h1>` Tags\n\nLet's do the same with h1 tags:","8e3ae2b1":"# `<h2>` Tags","600ddb8c":"Now we have the opposite view of the data, from the point of view of the colors instead of the months.  \nThere are many cases where this will be encountered in this dataset, for example the `links_url` column will contain many links delimited by `@@` which are mapped to the URL on which they occur:","64e701ed":"Not much information here. But we will save it for later crawls where things might change.","91824d46":"Since these are meaningful phrases, and not URLs, we can count the words that are used in the anchor text using `word_frequency`. ","238f9399":"The first forty links have pretty much the same number, suddenly going down to 1,721, and further down when it comes to `\/issues\/` pages. So it is clear that these are the links that are available in all pages (navigation, sidebar, footer, etc.). The manually added special links seem to start at index 47. You can explore these separately if you want.  \nLet's do the same with the external links.","10bcefa5":"# Request Headers\nThis depends on how we are running the crawl and what parameters were specified. All request headers can be customized by using the `custom_settings` parameter in the `crawl` function. \n\n`adv.crawl('https:\/\/example.com', 'output_file.csv', cutom_settings={'DEFAULT_REQUEST_HEADERS': {'User-Agent': 'your_user_agent'}} `","dc939379":"And again, in the opposite direction: ","17797246":"Since they all point to the same domain, it makes sense to check the `path` attribute and check where those internal links point to.","ce73dd5d":"A more natural way to look at page sizes is to sort them by the size, and then see how the frequency changes as the size increases. ","b7adc81f":"So we can see that most pages have between eight and twelve images.","0c4ab617":"We might also get a clue regarding how the content is distributed across the sitemaps:","0387056c":"<a id=\"links\"><\/a>\n# Links (`href`, `text`, `fragment`, & `nofollow`)  \nLinks come with four attributes by default as their names imply.","4fd2c4cf":"Sorting the index to see the frequency by the number of images on the pages (as opposed to sorting by the count).","fa891f8f":"## \ud83e\udd14 \ud83e\udd14 \ud83e\udd14 \ud83e\udd14 \ud83e\udd14","a7c11305":"A quick look at the frequency of content publishing shows that they only started in 2017 with the following annual numer of pages published:","bf3980fb":"Let's see what happens when we split that column by `@@`:","82e2f8a9":"These were some options that you might want to start with. What you do next depends on what you are tracking, the cutom data you extracted, and you strategy. \n\nYour turn. ","0ed3b94d":"Congratulations! We have just implemented a broken-links checker!\n\n<a id=\"anchor_text\"><\/a>\n# Anchor Text (`links_text`)\n\nDoing the same as we did with the URLs.","beecef70":"The `explode` method also works on DataFrames, and in this case you have to specify the column. We first `assign` a new column to `df`, which is basically the colors after splitting: ","d07ff605":"One final trick to explore, as many of the columns in the dataset will be in this format.  \nThe dicionary above maps months to colors. It is \"month-centric\". What if we wanted it to be color-centric? What if we wanted to know in which months blue occurs, and the same for the other colors? In other words, how do we invert the mapping, even though the lengths are different, and there might be duplicates?  \n\nThis is one of the interesting things that can be done with `defaultdict`.  \nWhen instantiating it, you have to specify a function. Then, when you try to assign to a key, if that key exists, the assignment happens, otherwise it will call the function. \n\nFor this case I'll use the `list` function. If the key does not exist then we get the output of calling `list()`:","deb9e982":"Let's take a quick look at the `urlparse` function before we move further:","7db6a298":"It seems the majority of pages are for briefings and statements, and the second biggest topic is presidential actions. We also have a few generic pages.  \nIf you check any of those pages, you'll see how simple they are, and how straightforward it is to extract the required data from them. The `crawl` function returns a standard set of page elements, but it also allows you to add any columns you like, name them whatever you want and extract any data by using CSS and\/or XPath selectors.  You can do this by passing dictionary to either (or both) of those parameters (`css_selectors` and `xpath_selectors`).  \nLooking at both types of pages (briefings and actions) I realized that they each have four main elements in their pages that might be interesting; The title, the date, the category, and the body text of the content. So I created this dictionary to extract them.  \nThe keys of this dictionary will become column names, and their values will be the selectors used to extrat those data from the pages (if they are available and `NaN` otherwise). I named the keys `briefing_*` but they will be valid for both briefings and presidential actions.","e2768e1f":"Remember that we requested additional columns to the default ones, and they all start with `briefing_`, so we can easily filter them and take a look. ","c1dc336d":"<a id=\"images\"><\/a>\n# Images","daa35baf":"Now let's take a look at the available columns, and conventions used for naming and storing them:"}}