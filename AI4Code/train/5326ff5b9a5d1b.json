{"cell_type":{"df46a90c":"code","9c5ca4cd":"code","8c002f83":"code","b145f126":"code","3a13e1a7":"code","e3d62681":"code","04b11720":"code","9e4fa16a":"code","be4374ae":"code","f9d41103":"code","807a5f8d":"code","72f1522a":"code","1eaba9e0":"code","381f3791":"code","af8337fe":"code","196422b2":"markdown","93a72576":"markdown","35552e3d":"markdown","5fd6b761":"markdown","a7ad4e09":"markdown","ba353053":"markdown","d4b4a178":"markdown","cf79cfc9":"markdown"},"source":{"df46a90c":"import numpy as np\nimport pandas as pd","9c5ca4cd":"train = pd.read_csv('..\/input\/emnist-balanced-train.csv', header=None)\ntest = pd.read_csv('..\/input\/emnist-balanced-test.csv', header=None)\ntrain.head()","8c002f83":"train_data = train.iloc[:, 1:]\ntrain_labels = train.iloc[:, 0]\ntest_data = test.iloc[:, 1:]\ntest_labels = test.iloc[:, 0]","b145f126":"train_labels = pd.get_dummies(train_labels)\ntest_labels = pd.get_dummies(test_labels)\ntrain_labels.head()","3a13e1a7":"train_data = train_data.values\ntrain_labels = train_labels.values\ntest_data = test_data.values\ntest_labels = test_labels.values\ndel train, test","e3d62681":"import matplotlib.pyplot as plt\n%matplotlib inline\nplt.imshow(train_data[45].reshape([28, 28]), cmap='Greys_r')\nplt.show()","04b11720":"def rotate(image):\n    image = image.reshape([28, 28])\n    image = np.fliplr(image)\n    image = np.rot90(image)\n    return image.reshape([28 * 28])\ntrain_data = np.apply_along_axis(rotate, 1, train_data)\/255\ntest_data = np.apply_along_axis(rotate, 1, test_data)\/255","9e4fa16a":"plt.imshow(train_data[45].reshape([28, 28]), cmap='Greys_r')\nplt.show()","be4374ae":"import tensorflow as tf","f9d41103":"tf.reset_default_graph()\nxs = tf.placeholder(tf.float32, [None, 784], name='input')\nys = tf.placeholder(tf.float32, [None, 47], name='exp_output')\ndropout = tf.placeholder(tf.float32, name='dropout')","807a5f8d":" # define the computation graph\nx_image = tf.reshape(xs, [-1, 28, 28, 1])\nlayer = tf.layers.conv2d(x_image, 64, [5,5], padding='same', activation=tf.nn.relu, kernel_initializer=tf.glorot_uniform_initializer())\nlayer = tf.layers.max_pooling2d(layer, pool_size=(2,2), strides=2) # [-1, 14, 14, 64]\nlayer = tf.layers.batch_normalization(layer)\nlayer = tf.layers.conv2d(layer, 128, [2,2], padding='same', activation=tf.nn.relu, kernel_initializer=tf.glorot_uniform_initializer())\nlayer = tf.layers.max_pooling2d(layer, pool_size=(2,2), strides=2) # [-1, 7, 7, 128]\nx_flat = tf.reshape(layer, [-1, 7*7*128])\nflatten = tf.layers.dense(x_flat, 1024, activation=tf.nn.relu, kernel_initializer=tf.glorot_uniform_initializer())\nflatten = tf.nn.dropout(flatten, keep_prob=1-dropout)\nflatten = tf.layers.dense(flatten, 512, activation=tf.nn.relu, kernel_initializer=tf.glorot_uniform_initializer())\nflatten = tf.layers.batch_normalization(flatten)\nflatten = tf.layers.dense(flatten, 128, activation=tf.nn.relu, kernel_initializer=tf.glorot_uniform_initializer())\nflatten = tf.layers.dense(flatten, 47)\npred = tf.nn.softmax(flatten, name='output')\n    ","72f1522a":"cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n    labels=ys,\n    logits=flatten))","1eaba9e0":"train = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)\ncorrect = tf.equal(tf.argmax(flatten, 1), tf.argmax(ys, 1))\naccuracy = tf.reduce_mean(tf.cast(correct, tf.float32))","381f3791":"init = tf.global_variables_initializer()","af8337fe":"NUM = 112800\nwith tf.Session() as sess:\n    sess.run(init)\n    for epoch in range(20):\n        for i in range(int(NUM \/ 100)):\n            x_batches, y_batches = train_data[i * 100: (i + 1) * 100], train_labels[i * 100: (i + 1) * 100]\n            sess.run(train, feed_dict={xs: x_batches, ys: y_batches, dropout: 0.5})\n            \n            if i % 1000 == 0:\n                acc, entropy = sess.run([accuracy, cross_entropy], feed_dict={xs: test_data,\n                                                    ys: test_labels,\n                                                    dropout: 0})\n                print('Train Entropy : ', sess.run(cross_entropy, feed_dict={xs: x_batches, ys: y_batches, dropout: 0.5}))\n                print('Test Accr & Entropy : ', acc, entropy)\n                # save_and_generate_proto(sess)\n    acc = sess.run(accuracy, feed_dict={xs: test_data,\n                                                ys: test_labels,\n                                                dropout: 0})\n    print(acc)","196422b2":"Firstly, we need to load our data, notice that there is no column names in csv files and thus header shold be set to `None`.","93a72576":"Turn our Dataframes into numpy array and delete `train` and `test` to save up memory.","35552e3d":"Now define our loss function, in this case, it is `softmax_cross_entropy_with_logits` but the official document shows it is deprecated as labels will be affected in backprop. But we use placeholders to feed data, and this is will not be a problem for us. In order to get rid of the warn info and keep our notebook neat, I use the `_v2` version. For more information on these two functions, you can refer to [StackExchange](https:\/\/stats.stackexchange.com\/questions\/327348\/how-is-softmax-cross-entropy-with-logits-different-from-softmax-cross-entropy-wi)","5fd6b761":"The final accuracy of our network is about 88.27%.  We are using the same balanced dataset and achieved these results. We have normalized the input pixels by dividing them with 255, hence a boost in accuracy was achieved from 85 to 88%. We have also used batch norms to tackle with the problem of covarient shifts. From our training results we conclude that the model has high varience, it is evident from the fact that training loss got down to 0.15 and testing loss was 3 times more than training loss. With this we conclude either our model was overfitting or that train data is not enough. Since we have used dropouts, overfitting is not an issue, The issue is the dataset, We can gather more training data and that should possibly solve the problem of high varience.","a7ad4e09":"For some reason, sadly, the EMNIST dataset was rotated and flipped and we need fix that.","ba353053":"One hot encoding with `get_dummies()` and you can compare it with the original labels.","d4b4a178":"Now split labels and images from original dataframe.","cf79cfc9":"Now let's import tensorflow to start building our network, with slim we can keep our code neat."}}