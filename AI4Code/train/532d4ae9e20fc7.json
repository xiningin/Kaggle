{"cell_type":{"d71a7bab":"code","c3e7d317":"code","6e110a5f":"code","565cd76d":"code","9039d758":"code","b5a2bd79":"code","2f320aa0":"code","1301ebdf":"code","62f3af6d":"code","26966754":"code","33c64c0e":"code","d5ff6ec5":"code","a94ab24f":"code","fc3e4546":"code","03a8cc45":"code","98f2216b":"code","9fdc0634":"code","9e0676cc":"code","86c35562":"code","80fac2f1":"code","886ab7ce":"code","83df1033":"code","8090559a":"code","f7416664":"code","381adfa3":"code","aea75b47":"code","25dc8387":"code","e7a5ecef":"code","8348bff1":"code","3d7ebdd9":"code","b9960483":"code","46a56a15":"code","0281cac2":"code","f93d5fe7":"code","78a9ff41":"code","a2fc4cad":"code","36706a99":"code","7870b0ce":"code","825ecd20":"code","bfa52a18":"code","525ef1e0":"code","ea6faf77":"code","c82e526a":"code","acdfbdfa":"code","cb1c8d50":"code","0e102158":"code","57b61325":"code","ec5b9eab":"code","3a3254dc":"code","d7880a9b":"code","9d568b25":"code","4a2fceac":"code","a5003014":"code","edc67abb":"code","91484f59":"code","701162de":"code","8b492f54":"code","1954d73d":"code","73c2125a":"code","555e7a1e":"code","cffbf657":"code","1e5d7ef2":"code","c8bb9a0f":"code","416fe5ba":"code","1bab0b91":"code","08413bfa":"code","bd0fd3dd":"code","e70b9e8e":"code","065dc5fb":"code","efd2c01f":"code","7c46bd30":"code","8b54f7c4":"code","fc94b5b7":"code","42c33148":"code","07234c37":"code","0f7a432a":"code","1099b82f":"code","421812d1":"code","58c63c22":"code","c0282bc5":"code","3c82e1c3":"code","96b2d007":"code","9ddf81d9":"code","7c9fff3a":"code","e4687d9c":"code","48f99ed7":"code","d1895a29":"code","45711026":"code","276e52b3":"code","b3da8603":"markdown","c1b0d439":"markdown","a4e7c12b":"markdown","204526ab":"markdown","f56d5318":"markdown","a6b72280":"markdown","a304192d":"markdown","f9bbc62a":"markdown","ce20b75f":"markdown","b8ce4b39":"markdown"},"source":{"d71a7bab":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","c3e7d317":"data = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/train.csv')","6e110a5f":"data.shape","565cd76d":"#categorical vars with na value\nvars_with_na = [var for var in data.columns if data[var].isnull().sum()>1 and data[var].dtypes=='O']\nNull_List =[]\nfor var in vars_with_na:\n    Null_List.append([var,np.round(data[var].isnull().mean(),3)])","9039d758":"Null_List","b5a2bd79":"def  RemoveHighNA(Null_List):\n    \n    NullvalueResult =  []\n    for var in Null_List:\n        if var[1] > 0.25:\n            NullvalueResult.append(var[0])\n            \n            #print(var)\n    return NullvalueResult     ","2f320aa0":"result = RemoveHighNA(Null_List)\nprint(result)","1301ebdf":"data = data.drop(result,axis = 1)\n","62f3af6d":"# to handle datasets\nimport pandas as pd\nimport numpy as np\n\n# for plotting\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# to divide train and test set\nfrom sklearn.model_selection import train_test_split\n\n# feature scaling\nfrom sklearn.preprocessing import MinMaxScaler\n\n# to visualise al the columns in the dataframe\npd.pandas.set_option('display.max_columns', None)","26966754":"X_train , X_test ,y_train ,y_test = train_test_split(data,data.SalePrice,test_size = 0.1,random_state = 0)\nX_train.shape,X_test.shape","33c64c0e":"y_actual_val = y_test","d5ff6ec5":"y_train.head()","a94ab24f":"X_train.head()","fc3e4546":"vars_with_na = [var for var in X_train.columns if X_train[var].isnull().sum()>1 and X_train[var].dtypes=='O']","03a8cc45":"vars_with_na","98f2216b":"#Fill categorical variables with na values \ndef fill_categorical_na(df,var_list):\n    X = df.copy()\n    X[var_list] = df[var_list].fillna('Missing')\n    return X","9fdc0634":"X_train = fill_categorical_na(X_train,vars_with_na)\nX_test = fill_categorical_na(X_test , vars_with_na)\n\n#check if missing values are still tehre \nX_train[vars_with_na].isnull().sum()","9e0676cc":"# check that test set does not contain null values in the engineered variables\n[vr for var in vars_with_na if X_train[var].isnull().sum()>0]","86c35562":"#Make a list of numerical variables that have missing values \nvars_with_na = [var for var in data.columns if X_train[var].isnull().sum()>1 and X_train[var].dtypes!='O']\n\n#print varibles and missing values \nfor var in vars_with_na:\n    print(var, np.round(X_train[var].isnull().mean(), 3))","80fac2f1":"#replace missing values \nfor var in vars_with_na:\n    #caliculate mode \n    mode_val = X_train[var].mode()[0]\n    \n    #train \n    #X_train[var+'_na'] = np.where(X_train[var].isnull(),0,1)\n    X_train[var].fillna(mode_val,inplace = True)\n    # test\n    #X_test[var+'_na'] = np.where(X_test[var].isnull(), 0, 1)\n    X_test[var].fillna(mode_val, inplace=True)\n\n# check that we have no more missing values in the engineered variables\nX_train[vars_with_na].isnull().sum()","886ab7ce":"# check that we have the added binary variables that capture missing information\n#X_train[['LotFrontage_na', 'MasVnrArea_na', 'GarageYrBlt_na']].head()","83df1033":"# check that test set does not contain null values in the engineered variables\n[vr for var in vars_with_na if X_test[var].isnull().sum()>0]","8090559a":"#Lets engineer relation between year var and house price \ndef elapsed_years(df, var):\n    # capture difference between year variable and year the house was sold\n    df[var] = df['YrSold'] - df[var]\n    return df","f7416664":"ListvarsYears = ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt']","381adfa3":"for var in ListvarsYears:\n    X_train = elapsed_years(X_train, var)\n    X_test = elapsed_years(X_test, var)","aea75b47":"#verify the same data \n# check that test set does not contain null values in the engineered variables\n[vr for var in ListvarsYears if X_test[var].isnull().sum()>0]","25dc8387":"[vr for var in ListvarsYears if X_train[var].isnull().sum()>0]","e7a5ecef":"vars_with_numbers_log = [var for var in data.columns if  X_train[var].dtypes!='O' and var != \"SalePrice\" and var != 'Id' and var != 'YearBuilt' and var != 'YearRemodAdd' and var != 'GarageYrBlt' and var != 'YrSold' ]","8348bff1":"len(vars_with_numbers_log)","3d7ebdd9":"vars_with_numbers_log","b9960483":"vars_with_numbers_log","46a56a15":"#listnumericalvars =['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'MiscVal', 'SalePrice']\n#listnumericalvars = ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt']","0281cac2":"def diaognizeplots(df,variable):\n  \n    plt.figure(figsize=(15,6))\n    plt.subplot(1,2,1)\n    df[variable].hist(bins = 30)\n    plt.subplot(1,2,2)\n    stats.probplot(df[variable],dist = \"norm\",plot=plt)\n    plt.show()\nfor var in vars_with_numbers_log:\n    if var != \"Id\":\n        diaognizeplots(X_train,var)","f93d5fe7":"from sklearn.preprocessing import KBinsDiscretizer\n","78a9ff41":"X_train[\"LotFrontage\"]","a2fc4cad":"from sklearn.preprocessing import KBinsDiscretizer\nLotFrontage_Tran  = KBinsDiscretizer(n_bins=5,encode='ordinal',strategy='kmeans')\nLotFrontage_Tran.fit(X_train[vars_with_numbers_log])","36706a99":"Exppdescritizer = LotFrontage_Tran.transform(X_train[vars_with_numbers_log])\nExppdescritizer = pd.DataFrame(Exppdescritizer,columns=vars_with_numbers_log)","7870b0ce":"Exppdescritizer.head()","825ecd20":"\nfor var in Exppdescritizer:\n    if var != \"Id\":\n        print(\"Current plt =\",var)\n        diaognizeplots(X_train,var)\n        diaognizeplots(disckbins,var)","bfa52a18":"from sklearn.preprocessing import KBinsDiscretizer\ndisckbins  = KBinsDiscretizer(n_bins=10,encode='ordinal',strategy='quantile')\ndisckbins.fit(X_train[vars_with_numbers_log])","525ef1e0":"print(type(vars_with_numbers_log))","ea6faf77":"disckbins = disckbins.transform(X_train[vars_with_numbers_log])\ndisckbins = pd.DataFrame(disckbins,columns=vars_with_numbers_log)","c82e526a":"for var in disckbins:\n    if var != \"Id\":\n        print(\"Current plt =\",var)\n        diaognizeplots(X_train,var)\n        diaognizeplots(disckbins,var)\n","acdfbdfa":"#Engineer the numerical varibles \nfor var in vars_with_numbers_log:\n    if 0 in data[var].unique():\n        pass\n    else:\n        #df[var] = np.log(df[var])\n        X_train[var] = np.log(X_train[var])\n        X_test[var]= np.log(X_test[var])","cb1c8d50":"def diaognizeplots(df,variable):\n    print(\"Current plt =\",variable)\n    plt.figure(figsize=(15,6))\n    plt.subplot(1,2,1)\n    df[variable].hist(bins = 30)\n    plt.subplot(1,2,2)\n    stats.probplot(df[variable],dist = \"norm\",plot=plt)\n    plt.show()\nfor var in vars_with_numbers_log:\n    if var != \"Id\":\n        diaognizeplots(X_train,var)","0e102158":"#checking null values \n[var for var in vars_with_numbers_log if X_test[var].isnull().sum()>0]","57b61325":"#same for train \n[var for var in vars_with_numbers_log if X_train[var].isnull().sum()>0]","ec5b9eab":"X_train.columns","3a3254dc":"#catogerical varibles \n#lets remove values that have less than 1 % values \ncat_vars = [var for var in X_train.columns if X_train[var].dtype == 'O']\ncat_vars","d7880a9b":"def find_frequent_labels(df,var,rare_prec):\n    #find labels tht are shared by more than one percent of data \n    df = df.copy()\n    tmp = df.groupby(var)['SalePrice'].count()\/len(df)\n    return tmp[tmp>rare_prec].index\nfor var in cat_vars:\n    freuent_ls = find_frequent_labels(X_train,var,0.01)\n    X_train[var] = np.where(X_train[var].isin(freuent_ls),X_train[var],'Rare')\n    X_test[var] = np.where(X_test[var].isin(freuent_ls),X_test[var],'Rare')\n    ","9d568b25":"#this functin will assign descrete varible\n#Smaller value corrensponds to smaller mean of the target \ndef replace_categorical_vars(train,test,var,target):\n    extraindex = train.groupby([var])[target].mean().sort_values()\n    print(\"Extra index ++++++++++++++++++\",extraindex)\n    ordered_labels = train.groupby([var])[target].mean().sort_values().index\n    print(ordered_labels)\n    ordinal_label  = {k:i for i,k in enumerate(ordered_labels,0)}\n    print(\"Second print==================\",ordinal_label)\n    train[var]=train[var].map(ordinal_label)\n    test[var] = test[var].map(ordinal_label)\n","4a2fceac":"for var in cat_vars:\n    replace_categorical_vars(X_train,X_test,var,'SalePrice')\n    ","a5003014":"#check for absence of na \n[var for var in X_train.columns if X_train[var].isnull().sum()>0]","edc67abb":"[var for var in X_test.columns if X_test[var].isnull().sum()>0]","91484f59":"#analyse catogorical varibles \ndef analyse_vars(df,var):\n    df = df.copy()\n    df.groupby(var)['SalePrice'].median().plot.bar(color = ['r','b','g','y'])\n    plt.title(var)\n    plt.ylabel('SalePrice')\n    plt.show()\nfor var in cat_vars:\n    analyse_vars(X_train,var)","701162de":"#Lets do some feature scaling \nlen(X_train.columns)","8b492f54":"train_vars = [var for var in X_train.columns if var not in ['Id']]\nlen(train_vars)","1954d73d":"X_train['Id']","73c2125a":"\n#X_train.drop('SalePrice',axis=1,inplace=True)\n#X_test.drop('SalePrice',axis=1,inplace=True)\n","555e7a1e":"#X_train.drop('Id',axis=1,inplace=True)\n#X_test.drop('Id',axis=1,inplace=True)","cffbf657":"X_train.head()","1e5d7ef2":"#for val in X_train.columns:\n#    print(val)\n #   print(X_train[var].dtypes())\n#np.max(X_test.SalePrice)\n    ","c8bb9a0f":"#for val in train_vars:\n    #print(X_train[val].dtypes)\n#print(len(train_vars))\nfor val in train_vars:\n    print(val)\n    for temp in val:\n        if temp == '-inf' or temp == 'nan':\n            print(temp)","416fe5ba":"X_train[train_vars].iloc[10]","1bab0b91":"X_train['MiscVal']","08413bfa":"print(X_train.head())","bd0fd3dd":"train_vars","e70b9e8e":"def diaognizeplots(df,variable):\n    print(\"Current plt =\",variable)\n    plt.figure(figsize=(15,6))\n    plt.subplot(1,2,1)\n    df[variable].hist(bins = 30)\n    plt.subplot(1,2,2)\n    stats.probplot(df[variable],dist = \"norm\",plot=plt)\n    plt.show()\nfor var in X_train.columns:\n    if var != \"Id\":\n        diaognizeplots(X_train,var)","065dc5fb":"print(max(X_test['SalePrice']))\nprint(min(X_test['SalePrice']))\nMax_sale = max(X_test['SalePrice'])\nMin_sale = min(X_test['SalePrice'])","efd2c01f":"#fit Scalar \nscaler  = MinMaxScaler() #ceate instance \nscaler.fit(X_train[train_vars]) #  fit  the scaler to the train set for later use\n\nscalertwo  = MinMaxScaler() #ceate instance \nscalertwo.fit(X_test[train_vars]) \n\n# transform the train and test set, and add on the Id and SalePrice variables\n\nX_train = pd.concat([X_train[['Id']].reset_index(drop=True),\n                    pd.DataFrame(scaler.transform(X_train[train_vars]), columns=train_vars)],\n                    axis=1)\n#train_vars.append\n\n\n\n\nX_test = pd.concat([X_test[['Id']].reset_index(drop=True),\n                  pd.DataFrame(scalertwo.transform(X_test[train_vars]), columns=train_vars)],\n                 axis=1)\ny_train = X_train['SalePrice']\ny_test = X_test['SalePrice']\n#resactual = scalertwo.inverse_transform(X_test[train_vars])\n\nX_train.drop('SalePrice',axis=1,inplace=True)\nX_test.drop('SalePrice',axis=1,inplace=True)\ntrain_vars.remove('SalePrice')","7c46bd30":"X_train.head()","8b54f7c4":"#checking for  outliar varibles \ndef find_outliers(df,var):\n    df = df.copy()\n    if 0 in data[var].unique():\n        pass\n    else:\n        #df[var] = np.log(df[var])\n        df.boxplot(column= var)\n        plt.title(var)\n        plt.ylabel(var)\n        plt.show()\nfor var in X_train.columns:\n    if var != 'LotFrontage_na':\n        find_outliers(X_train,var)","fc94b5b7":"import scipy.stats as stats","42c33148":"#X_train.SalePrice.hist(bins=100)vgjk","07234c37":"m,n.","0f7a432a":"print(y_train.head())","1099b82f":"X_train.head()    \n    ","421812d1":"X_train.isnull().sum()","58c63c22":"#Saving the new data that has been transformed \nX_train.to_csv('xtrain_tran.csv', index=False)\nX_test.to_csv('xtest_tran.csv', index=False)","c0282bc5":"rangeone = Max_sale- Min_sale ","3c82e1c3":"X_train.head()","96b2d007":"y_train.head()","9ddf81d9":"from xgboost import XGBRegressor\n\nmy_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\nmy_model.fit(X_train, y_train,early_stopping_rounds=5, \n             eval_set=[(X_train, y_train)], \n             verbose=True)","7c9fff3a":"from sklearn.metrics import mean_absolute_error\n\npredictions = my_model.predict(X_test)\nprint(predictions)\nprint(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions, y_test)))","e4687d9c":"print(type(predictions))","48f99ed7":"predact = []\nfor temp in predictions:\n    resulttemp = temp*rangeone+Min_sale\n    predact.append(resulttemp)","d1895a29":"actval = []\nfor temp in y_test:\n    resulttemp = temp*rangeone+Min_sale\n    actval.append(resulttemp)","45711026":"print(len(y_test))","276e52b3":"leny = len(y_test)\nresavg = 0 \nfor i,j,k,l in zip(predact,actval,predictions,y_test):\n    print(int(i),'values',j,\"difference = \",((int(i)-j)*100)\/i)\n    tempavgval = (((int(i)-j)*100)\/i)\n    resavg = resavg+tempavgval\n    \n    #print(k,'second print',l,\"difference = \",((k-l)*100)\/k)\nprint(\"the difference val ============\",resavg\/leny)","b3da8603":"Log did not work fine .. I will use a different function .. aim is to clear outlier first","c1b0d439":"fix the outliers ","a4e7c12b":"Lets do some detialed plots the above ones are not that great \n","204526ab":"I am using equal frequency discretisation from sikitlearn  ","f56d5318":"Look back again ","a6b72280":"K bins did not work out .. \nI am trying n bins approach","a304192d":"> X_train , X_test ,y_train ,y_test****","f9bbc62a":"check this step again ","ce20b75f":"log transform all the numerical varibles ","b8ce4b39":"In the following code i am using kbins tech to see if  iam able to remove outliers "}}