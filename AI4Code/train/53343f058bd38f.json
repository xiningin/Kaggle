{"cell_type":{"58cce269":"code","e27c5063":"code","ca1bafd5":"code","abfa9bf4":"code","010d8cc5":"code","60a1580b":"code","a0d71e22":"code","9694dbb0":"code","bdbc1a00":"code","c2372352":"code","cbf93dbd":"code","08bcdcac":"code","78162e06":"code","a9c5a756":"code","86aae96d":"code","e111f203":"code","5004cc28":"code","b7c762ab":"code","7838e7ab":"code","979b9fa9":"code","e4f52234":"code","6cb306e0":"code","93fed72d":"code","34c4383f":"code","7fab1d5c":"code","5f61ddb0":"code","e5a8da5c":"markdown","cbda13af":"markdown","198fb755":"markdown","7848e5c7":"markdown","e6cd42f1":"markdown","755db474":"markdown","e688857a":"markdown","17f422cb":"markdown"},"source":{"58cce269":"import numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e27c5063":"df = pd.DataFrame(pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv'))\ndf.head()","ca1bafd5":"plt.figure(figsize=(10,7))\nsns.distplot(df['Time'])","abfa9bf4":"plt.figure(figsize=(10,7))\nsns.distplot(df['Amount'])","010d8cc5":"sns.countplot(x='Class',data=df)","60a1580b":"print(\"Percentage of non-fraud transactions:\", round(100*df[df['Class']==0]['Class'].value_counts()\/len(df),2))\nprint(\"Percentage of fraud transactions:\", round(100*df[df['Class']==1]['Class'].value_counts()\/len(df),2))","a0d71e22":"sns.stripplot(x='Class', y='Amount',data=df)","9694dbb0":"df.describe()","bdbc1a00":"plt.figure(figsize=(30, 25))\nsns.heatmap(df.corr(), annot=True)","c2372352":"sns.scatterplot(x='Amount',y='V2',data=df)","cbf93dbd":"sns.scatterplot(x='Amount', y='V5',data=df)","08bcdcac":"sns.scatterplot(x='Amount', y='V7',data=df)","78162e06":"sns.scatterplot(x='Amount', y='V20',data=df)","a9c5a756":"plt.figure(figsize=(10,7))\nsns.scatterplot(x='Time', y='V3',data=df)","86aae96d":"from sklearn.preprocessing import RobustScaler\nrobsc = RobustScaler()","e111f203":"df['scaled_amount'] = robsc.fit_transform(df['Amount'].values.reshape(-1,1))\ndf['scaled_time'] = robsc.fit_transform(df['Time'].values.reshape(-1,1))","5004cc28":"df.drop('Amount', inplace=True, axis=1)\ndf.drop('Time', inplace=True, axis=1)\ndisplay(df.head())","b7c762ab":"scaled_amount = df['scaled_amount']\nscaled_time = df['scaled_time']\n\ndf.drop(['scaled_amount','scaled_time'], axis=1, inplace=True)\n\ndf.insert(0,'scaled_amount', scaled_amount)\ndf.insert(1,'scaled_time', scaled_time)","7838e7ab":"from sklearn.model_selection import train_test_split\n\nX=df.drop('Class', axis=1)\ny=df['Class']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42, stratify=df['Class'])","979b9fa9":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.svm import SVC\n\nclassifiers = {\n    \"LogisiticRegression\": LogisticRegression(),\n    \"KNearest\": KNeighborsClassifier(),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n    \"GradientBoostingClassifier\": GradientBoostingClassifier(),\n    \"RandomForestClassifier\": RandomForestClassifier(),\n    \"Support Vector Classifier\": SVC()\n}","e4f52234":"from sklearn.metrics import classification_report, confusion_matrix","6cb306e0":"from imblearn.under_sampling import TomekLinks\n\ntl = TomekLinks()\nX_tomek, y_tomek= tl.fit_sample(X_train, y_train)","93fed72d":"from collections import Counter\nprint('Resampled dataset shape %s' % Counter(y_tomek))","34c4383f":"for key, classifier in classifiers.items():\n    classifier.fit(X_tomek, y_tomek)\n    print(\"Classifiers: \", classifier.__class__.__name__)\n    y_pred = classifier.predict(X_test)\n    print(classification_report(y_test, y_pred))\n    print(confusion_matrix(y_test, y_pred))","7fab1d5c":"from imblearn.over_sampling import SMOTE\n\nsm = SMOTE()\nX_sm, y_sm= sm.fit_sample(X_train, y_train)","5f61ddb0":"for key, classifier in classifiers.items():\n    classifier.fit(X_sm, y_sm)\n    print(\"Classifiers: \", classifier.__class__.__name__)\n    y_pred = classifier.predict(X_test)\n    print(classification_report(y_test, y_pred))\n    print(confusion_matrix(y_test, y_pred))","e5a8da5c":"Conclusion: Random Forest Classifier gave best result out of all the models for  both undersampled and oversampled data. ","cbda13af":"To solve the problem of imbalanced dataset we will scale the data using both the techniques of undersampling and oversampling. But before we do that we will split the data into training and testing set and apply resampling only on the training set. ","198fb755":"Observation:\n1) Dataset is extremely imbalanced.\n2) Dataset contains large number of outliers in 'Amount' column. ","7848e5c7":"Oversampling our training data using SMOTE: ","e6cd42f1":"Undersampling our training data using Tomek Links:","755db474":"# EDA","e688857a":"We have to scale both 'Time' and 'Amount' columns as the rest of the columns are already scaled. We will use Robust Scaler as 'Amount' contains large number of outliers. ","17f422cb":"We will use 6 models to make our predictions. The 6 models are: Logistic Regression, KNeighbors Classifier, Decision Tree Classifier, Gradient Boosting Classifier, Random Forest Classifier and Support Vector Classifier.  "}}