{"cell_type":{"4424319f":"code","f45c7ff6":"code","c1f36bdb":"code","a1760f6e":"code","2720b6fa":"code","de715db5":"code","e310c269":"markdown","26216ebc":"markdown","336f9e32":"markdown","5e2e3eee":"markdown"},"source":{"4424319f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv('..\/input\/httpswwwkagglecomcryptocurrency-dataset\/CRYPTOCURRENCY_COINDESK_BTCUSD_NEW.csv', parse_dates=['Date'], thousands=',')\nA = np.array([np.ones(data.shape[0]), list(range(1,data.shape[0]+1))]).transpose()\nb = np.array(data['Close'].values.tolist())\nx = np.linalg.solve(np.matmul(A.transpose(),A), np.matmul(A.transpose(),b))\n\nprint(x)\n\nplt.scatter(A.transpose()[1],b, color='red')\nplt.plot(A.transpose()[1],np.matmul(A, x))\nplt.show()","f45c7ff6":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\ndata = pd.read_csv('..\/input\/httpswwwkagglecomcryptocurrency-dataset\/CRYPTOCURRENCY_COINDESK_BTCUSD_NEW.csv', parse_dates=['Date'], thousands=',')\nb = np.array(data['Close'].values.tolist())\n\nsamples = random.sample(range(0,data.shape[0]), data.shape[0])\nindeces = list(range(1,data.shape[0]+1))\n\ntrain_ind = np.array([indeces[i] for i in samples[0:150]]).astype(np.int64)\ntrain_ind = np.sort(train_ind)\ntrain_set = np.array([b[i-1] for i in train_ind[0:150]]).astype(np.int64)\n\nvalid_ind = np.array([indeces[i] for i in samples[151:300]]).astype(np.int64)\nvalid_ind = np.sort(valid_ind)\nvalid_set = np.array([b[i-1] for i in valid_ind[0:150]])\n\ntest_set  = np.array([b[i] for i in samples[301:365]])\ntest__ind = np.array([indeces[i] for i in samples[301:365]])","c1f36bdb":"# gradient descent approach\n\nA_n = np.array([np.ones(len(train_ind)), np.copy(train_ind), np.copy(train_ind)**2, np.copy(train_ind)**3, np.copy(train_ind)**4]).transpose()\ny = np.array(train_set)\n\n# 2 : 800000    , 0.0000001\n# 3 : 6400000   , 0.0000000000027\nfor i in range(2,3):\n    \ud835\udefe = 0.0000001\n    x = np.zeros(i)\n    A = np.copy(A_n[:,0:i])\n    for j in range(800000):\n        x_prev = np.copy(x)\n        x = x + \ud835\udefe*np.matmul(A.transpose(), (y - np.matmul(A, x)))\n    plt.scatter(A[:,1], train_set, color='red')\n    plt.plot(train_ind,np.matmul(A, x))\n    plt.show()\n\nprint(x)","a1760f6e":"# direct approach\n# finding curves and errors with training data\nmax_degree = 21\n\u03b5_train = np.zeros(max_degree-1)\n\u03b5_valid = np.zeros(max_degree-1)\nA_n = np.array([np.ones(len(train_ind)), np.copy(train_ind), np.copy(train_ind)**2, np.copy(train_ind)**3, np.copy(train_ind)**4, np.copy(train_ind)**5, np.copy(train_ind)**6, np.copy(train_ind)**7, np.copy(train_ind)**8, np.copy(train_ind)**9, np.copy(train_ind)**10, np.copy(train_ind)**11, np.copy(train_ind)**12, np.copy(train_ind)**13, np.copy(train_ind)**14, np.copy(train_ind)**15, np.copy(train_ind)**16, np.copy(train_ind)**17, np.copy(train_ind)**18, np.copy(train_ind)**19, np.copy(train_ind)**20,np.copy(train_ind)**21]).transpose()\nV_n = np.array([np.ones(len(valid_ind)), np.copy(valid_ind), np.copy(valid_ind)**2, np.copy(valid_ind)**3, np.copy(valid_ind)**4, np.copy(valid_ind)**5, np.copy(valid_ind)**6, np.copy(valid_ind)**7, np.copy(valid_ind)**8, np.copy(valid_ind)**9, np.copy(valid_ind)**10, np.copy(valid_ind)**11, np.copy(valid_ind)**12, np.copy(valid_ind)**13, np.copy(valid_ind)**14, np.copy(valid_ind)**15, np.copy(valid_ind)**16, np.copy(valid_ind)**17, np.copy(valid_ind)**18, np.copy(valid_ind)**19, np.copy(valid_ind)**20,np.copy(valid_ind)**21]).transpose()\nX_n = []\nfig, axs = plt.subplots(6,4)\nfor i in range(2,max_degree+1):\n    A = np.copy(A_n[:,0:i])\n    y = np.array(train_set)\n    x = np.linalg.solve(np.matmul(A.transpose(),A), np.matmul(A.transpose(),y))\n\n    # print(x)\n    X_n.append(x)\n\n    \u0394y = y - np.matmul(A,x)\n    \u03b5_train[i-2] = np.linalg.norm(\u0394y, ord=2)\n\n    v = np.array(valid_set)\n    B = np.copy(V_n[:,0:i])\n    \u0394v = v - np.matmul(B,x)\n    \u03b5_valid[i-2] = np.linalg.norm(\u0394v, ord=2)\n\n    row, col= int((i-2)\/4), (i-2)%4\n    axs[row,col].scatter(A.transpose()[1],y, color='red')\n    axs[row,col].scatter(B.transpose()[1],v, color='green')\n    axs[row,col].plot(A.transpose()[1],np.matmul(A, x))\n\naxs[5,2].scatter(range(1,len(b)+1),b, color='red')","2720b6fa":"plt.plot(range(2,len(\u03b5_train)+2),\u03b5_train, color ='green')\nplt.plot(range(2,len(\u03b5_valid)+2),\u03b5_valid, color ='blue')\nplt.show()\n\n# find best degree\nbest = 15\nfor i in range(best,0,-1):\n    if \u03b5_train[i] >  \u03b5_valid[i]:\n        best = i\n        break\n\nprint(\"best point is\",best)","de715db5":"\n# predict prices for next 10 days\nr = np.array(range(367,377))\nM_n = np.array([np.ones(10), r, r**2, r**3, r**4, r**5, r**6, r**7, r**8, r**9, r**10, r**11, r**12, r**13, r**14, r**15]).transpose()\nplt.plot(range(367,377),np.matmul(M_n[:,0:best], X_n[best-2]), color ='blue')\nplt.show()","e310c269":"Somehow with larger values for \u03b1, the function will go west! with such a tiny value for learning step, the procedure will be cumbersome. indeed there are ways to improve the procedure, but they are out of our subject, so I'm going to neglect them for now.using direct approach with such a tiny dataset, we can find other curves as well using direct approach.","26216ebc":"We can also approximate above line using gradient descend method.\nin order to perform gradient method, we must calculate function gradient in each point using equation (1-1) we've got above.\n\\\\[\\nabla (y-A\\theta)^2 = -A^T(y-A\\theta)\\\\]\nwe can approach minimum by simply following the opposite direction of gradient in each step. if W_k denotes step k of the prpcedure, then we can simply write : \n\\\\[ W_k = W_{k-1} + \\alpha A^T(y-A\\theta)\\\\]\nwhere \u03b1 is a constant (in the simplest form) learning rate and a hyperparameter.\nthere is a demonstration for driving above coefficients in the following,<br>\nbut before that, we are going to arrange our data and mark them as three groups, training set, validation set and test set.\nafter choosing random index for each group, we can proceed to gradient descend method.","336f9e32":"after finding error for training set (drawn below by green color) as well as validation set (drawn below by blue color), we can find best degree to use in our estimation. the best candidate is the point where training error is minimum and at the same time, validation error is lower than training error.","5e2e3eee":"Linear Regression Procudure :<br> \nOur objective is to minimize cost function :\n\\\\[ Min_{\\theta} (y-A\\theta)^2 \\\\]\nwhere : \n\\\\[y = \\begin{pmatrix} y_1 \\\\ \u22ee \\\\ y_n \\end{pmatrix}, A = \\begin{pmatrix} 1 , x_1 , \u22ef , x^m_1 \\\\ \u22ee \\ \\ \u22ee  \u22ef  \u22ee  \\\\ 1, x_n , \u22ef ,x^m_n \\end{pmatrix}, \\theta=\\begin{pmatrix}\\theta_1 \\\\ \u22ee \\\\ \\theta_m \\end{pmatrix}\\\\]\nWe can solve above equation either by using direct approach or gradient descend approach.<br>\nIn direct approach we will first get the gradient of the function : \n\\\\[\\nabla (y-A\\theta)^2 = -A^T(y-A\\theta)\\ \\ \\ \\ \\ \\ \\ \\ \\  (1-1) \\\\]\nMinimum value of the function occures where gradient is zero, we must solve the : \n\\\\[ -A^T(y-A\\theta) = 0\\\\]\nto optain best parameters for our problem.\n\\\\[ y-A\\theta = 0 \u2192 \\theta = (A^TA)^{-1}A^Ty\\\\]"}}