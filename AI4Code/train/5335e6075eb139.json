{"cell_type":{"04af31a8":"code","f3126577":"code","0667eba6":"code","39a00064":"code","53ea2901":"code","95e5a671":"code","e30d4dc3":"code","38bb3dff":"code","212ad121":"code","58919a5e":"code","f286bc9f":"code","bd91924a":"code","8390a453":"code","bb81b4f7":"code","3f4d7ad3":"code","bb5e2433":"code","a1d8c398":"code","7c4e63ea":"code","9ee8c5a6":"code","9fb499ca":"code","8fa4f240":"code","f888c824":"code","051d710d":"code","13d29454":"code","a19d60f3":"code","f0b88a8e":"code","78743aab":"markdown","e4e08e87":"markdown","28f04482":"markdown","722f023f":"markdown","db62a6f4":"markdown","d47cbdb8":"markdown","7777fabf":"markdown","403e3eed":"markdown","7520cd26":"markdown","b92735b4":"markdown","143c626f":"markdown","082d408f":"markdown","d94da581":"markdown"},"source":{"04af31a8":"import pandas as pd\nimport numpy as np\nimport os\n\n# Data Visualisation libraries \n%matplotlib inline\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport cv2\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n## Image Augmentation \n\n# skimage\nfrom skimage.io import imshow, imread, imsave\nfrom skimage.transform import rotate, AffineTransform, warp,rescale, resize, downscale_local_mean\nfrom skimage import color,data\nfrom skimage.exposure import adjust_gamma\nfrom skimage.util import random_noise","f3126577":"from tqdm import tqdm","0667eba6":"PATH = '..\/input\/seti-breakthrough-listen\/'","39a00064":"df_train = pd.read_csv(PATH+'train_labels.csv')","53ea2901":"df_train.head()","95e5a671":"df_train.target.value_counts()","e30d4dc3":"source = df_train['target'].value_counts()\nfig = go.Figure(data=[go.Pie(labels=source.index,values=source.values)])\nfig.update_layout(title='Target distribution')\nfig.show()\n","38bb3dff":"def get_train_filename_by_id(_id: str) -> str:\n    return f\"{PATH}\/train\/{_id[0]}\/{_id}.npy\"","212ad121":"get_train_filename_by_id(df_train.iloc[0][\"id\"])","58919a5e":"tmp_filename = get_train_filename_by_id(df_train.iloc[0][\"id\"])\ntr_data = np.load(tmp_filename)\ntr_data.shape","f286bc9f":"def show_cadence(filename: str, label: int) -> None:\n    plt.figure(figsize=(16, 10))\n    arr = np.load(filename)\n    for i in range(6):\n        plt.subplot(6, 1, i + 1)\n        if i == 0:\n            plt.title(f\"ID: {os.path.basename(filename)} TARGET: {label}\", fontsize=18)\n        plt.imshow(arr[i].astype(float), interpolation='nearest', aspect='auto')\n        plt.text(5, 100, [\"ON\", \"OFF\"][i % 2], bbox={'facecolor': 'white'})\n        plt.xticks([])\n    plt.show()","bd91924a":"def display_cadance(df,num_sample:int,target:int) -> None:\n    for i in range(1,num_sample):\n        show_cadence(get_train_filename_by_id(df_train[df_train[\"target\"] == target]['id'][i]),df_train[df_train[\"target\"] == target]['target'][i])","8390a453":"def display_cadance(df,num_sample:int,target:int) -> None:\n    df = df[df['target']==target].sample(num_sample)\n    for idx,row in df.iterrows():\n        show_cadence(get_train_filename_by_id(row[\"id\"]),row[\"target\"])","bb81b4f7":"display_cadance(df_train,num_sample=5,target=0)","3f4d7ad3":"display_cadance(df_train,num_sample=5,target=1)","bb5e2433":"!pip install -q pretrainedmodels","a1d8c398":"import glob\nimport cv2\nfrom sklearn.model_selection import train_test_split\nimport sklearn.metrics\nimport albumentations\n\nimport torch","7c4e63ea":"df_train","9ee8c5a6":"import torch\n\nclass ClassificationDataset:\n    \n    def __init__(self, image_paths, targets, resize=None, augmentations=None): \n        self.image_paths = image_paths\n        self.targets = targets\n        self.resize = resize\n        self.augmentations = augmentations\n\n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, item):      \n        image = np.load(self.image_paths[item]).astype(float)\n\n        targets = self.targets[item]\n        \n        if self.resize is not None:\n            image = np.transpose(image, (1,2,0))\n            image = cv2.resize(image, dsize=self.resize, interpolation=cv2.INTER_CUBIC)        \n        \n        if self.augmentations is not None:\n            augmented = self.augmentations(image=image)\n            image = augmented[\"image\"]\n            \n        # pytorch expects CHW instead of HWC\n        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n        \n        return {\n            \"image\": torch.tensor(image, dtype=torch.float),\n            \"targets\": torch.tensor(targets, dtype=torch.long),\n        }","9fb499ca":"df_train['img_pth'] = df_train['id'].apply(lambda x:f'{PATH}\/train\/{x[0]}\/{x}.npy')","8fa4f240":"def data_prep(img_paths,tgts,dataclass):\n    \n    images = img_paths.values\n    targets = tgts.values\n    train_images, valid_images, train_targets, valid_targets = train_test_split(images, targets, stratify=targets, random_state=42)\n\n    train_dataset = dataclass(train_images,train_targets,resize=(224,224))\n    train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=16,shuffle=True,num_workers=4)\n\n    valid_dataset = dataclass(valid_images,valid_targets,resize=(224,224))\n    valid_loader = torch.utils.data.DataLoader(valid_dataset,batch_size=16,shuffle=False,num_workers=4)\n    \n    return train_loader,valid_loader","f888c824":"train_loader,valid_loader = data_prep(df_train['img_pth'],df_train[\"target\"],ClassificationDataset)","051d710d":"def train(data_loader, model, optimizer, device):\n    \n    model.train()\n    \n    for data in tqdm(data_loader, position=0, leave=True, desc='Training'):\n        inputs = data[\"image\"]\n        targets = data['targets']\n        \n        inputs = inputs.to(device, dtype=torch.float)\n        targets = targets.to(device, dtype=torch.float)\n        \n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))\n        loss.backward()\n        optimizer.step()\n        \ndef evaluate(data_loader, model, device):\n    model.eval()\n    \n    final_targets = []\n    final_outputs = []\n    \n    with torch.no_grad():\n        \n        for data in tqdm(data_loader, position=0, leave=True, desc='Evaluating'):\n            inputs = data[\"image\"]\n            targets = data[\"targets\"]\n            inputs = inputs.to(device, dtype=torch.float)\n            targets = targets.to(device, dtype=torch.float)\n            \n            output = model(inputs)\n            \n            targets = targets.detach().cpu().numpy().tolist()\n            output = output.detach().cpu().numpy().tolist()\n            \n            final_targets.extend(targets)\n            final_outputs.extend(output)\n            \n    return final_outputs, final_targets","13d29454":"import torch.nn as nn\nimport pretrainedmodels\n\ndef get_model(pretrained):\n    if pretrained:\n        model = pretrainedmodels.__dict__[\"resnet18\"](pretrained='imagenet')\n    else:\n        model = pretrainedmodels.__dict__[\"resnet18\"](pretrained=None)\n        \n    model.last_linear = nn.Sequential(\n        nn.BatchNorm1d(512),\n        nn.Dropout(p=0.25),\n        nn.Linear(in_features=512, out_features=1024),\n        nn.ReLU(),\n        nn.BatchNorm1d(1024, eps=1e-05, momentum=0.1),\n        nn.Dropout(p=0.5),\n        nn.Linear(in_features=1024, out_features=1)\n    )\n    \n    return model","a19d60f3":"device = \"cuda\"\nepochs = 5\n\nmodel = get_model(pretrained=False)\nmodel.conv1 = nn.Conv2d(6, 64, kernel_size=7, stride=2, padding=3,bias=False)\nmodel.to(device)\naug = None\noptimizer = torch.optim.Adam(model.parameters(), lr=5e-4)","f0b88a8e":"for epoch in range(epochs):\n    train(train_loader, model, optimizer, device=device)\n    predictions, valid_targets = evaluate(valid_loader, model, device=device)\n    roc_auc = sklearn.metrics.roc_auc_score(valid_targets, predictions)\n    print(f\"Epoch={epoch}, Valid ROC AUC={roc_auc}\")","78743aab":"#####  Refence for below model , Kindly upvote below kernal\n\n[link](https:\/\/www.kaggle.com\/jiny333\/pytorch-simple-baseline-resnet-18-trn-infer)","e4e08e87":"<br>\n<h1 style = \"font-size:60px; font-family:Garamond ; font-weight : normal; background-color: #f6f5f5 ; color : #fe346e; text-align: center; border-radius: 100px 100px;\">SETI - Signal Search <br>  EDA & Baseline Model <\/h1>\n<br>","28f04482":"# Lets find out Shape of training data","722f023f":"## Observation \n\n- Dataset is pretty unbalanced as per above pie chart \n- Need to chose the appropirate sampling strategy while modeling ","db62a6f4":"### Lets kick start modeling","d47cbdb8":"# Preparing data for Training & Validation","7777fabf":"# Contents\n\n* [<font size=4>EDA<\/font>](#1)\n    * [Preparing the ground](#1.1)\n\n\n* [<font size=4>Image processing <\/font>](#2)\n    * [Visualise the data](#2.1)\n","403e3eed":"<br>\n<h1 style = \"font-size:60px; font-family:Garamond ; font-weight : normal; background-color: #f6f5f5 ; color : #fe346e; text-align: center; border-radius: 100px 100px;\">SETI - Signal Search <br>  Getting Sense of Cadence data <\/h1>\n<br>","7520cd26":"## Referance :- \n\n Data plot Code taken from below kernal  \n\n[link](https:\/\/www.kaggle.com\/ihelon\/signal-search-exploratory-data-analysis)","b92735b4":"# Kindly Upvote if you like this kernal","143c626f":"## ...working to add another model in this kernal for ROC-AUC comparison ","082d408f":"# Visualise the training data","d94da581":"## <a name=\"Wheat Detection\">SETI - Signal Search <\/a>\n\n#### <a name=\"About_Competition\"> Introduction <\/a>\n\nIn this competition, use your data science skills to help identify anomalous signals in scans of Breakthrough Listen targets. Because there are no confirmed examples of alien signals to use to train machine learning algorithms, the team included some simulated signals (that they call \u201cneedles\u201d) in the haystack of data from the telescope. They have identified some of the hidden needles so that you can train your model to find more. The data consist of two-dimensional arrays, so there may be approaches from computer vision that are promising, as well as digital signal processing, anomaly detection, and more. The algorithm that\u2019s successful at identifying the most needles will win a cash prize, but also has the potential to help answer one of the biggest questions in science.\n\n\n\n#### <a name=\"Specific Objectives\">Specific Objectives<\/a>           \n\nThe main objective of the competition is to develop machine learning-based models to accurately classify anomalous signals in scans of Breakthrough Listen targets.\n\n\n#### <a name=\"dataset_description\">Dataset Description<\/a>: \n\nIn this competition you are tasked with looking for technosignature signals in cadence snippets taken from the Green Bank Telescope (GBT). Please read the extended description on the Data Information tab for detailed information about the data (that's too lengthy to include here).\n\n#### Files - \n##### train\/ -\n  Training set of cadence snippet files stored in numpy float16 format (v1.20.1), one file per cadence snippet id, with corresponding labels found in the train_labels.csv file. \n  \n Each file has dimension (6, 273, 256)\n \n*  1st dimension representing the 6 positions of the cadence\n*  2nd and 3rd dimensions representing the 2D spectrogram.\n \n \n##### test\/ - \nthe test set cadence snippet files; you must predict whether or not the cadence contains a \"needle\", which is the target for this competition\n\nsample_submission.csv - a sample submission file in the correct format\ntrain_labels - targets corresponding (by id) to the cadence snippet files found in the train\/ folder\n\n\n  1. cadence snippet id of file \n  2. target - 1 or 0\n    \n\n#### <a name=\"target_variable\">Target Variable<\/a>                                        \n* __Submission data__  \n    1. cadence snippet id of file \n    2. target - 1 or 0\n\nCatagory of Labels :- \n\n"}}