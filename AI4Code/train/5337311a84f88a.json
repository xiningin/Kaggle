{"cell_type":{"780ec6ea":"code","04ddc57f":"code","71f53e8e":"code","8a7319ca":"code","bcfa8c13":"code","1674b029":"code","e8dc4372":"code","77ab3253":"code","8abb2217":"code","6f6b2493":"code","96707c43":"code","996defcf":"code","0cbb4a3f":"code","5ebcebc0":"code","6a031c85":"code","f2022403":"code","94c02050":"code","d1943947":"code","fd48e57e":"code","9896e992":"code","a9a54714":"code","87e3b405":"code","f6e5b3b5":"code","8395a2fa":"code","f5fc9abf":"code","6d57a633":"code","b6cb4a20":"code","dbb22eca":"code","2946c778":"code","9e4f9876":"code","1563e2da":"code","5279b22c":"code","7233f046":"code","e57664d5":"code","a412da97":"code","b18f8063":"code","1de26d41":"code","6c363cd6":"code","56c3d23b":"code","c6adef15":"code","da897f67":"code","c925e3cd":"code","e37a3432":"code","5c8e1954":"code","a92e56e3":"markdown","23ca2aaf":"markdown","2358db77":"markdown","74021428":"markdown","fb429c2b":"markdown","21eb2911":"markdown","9e01594a":"markdown","dfc8db92":"markdown","ab214f69":"markdown","46dc5573":"markdown","2c6d1f50":"markdown","c67e198b":"markdown","ff62ca27":"markdown","6b38caac":"markdown","841536b1":"markdown","df1a5c15":"markdown","98b8bc1c":"markdown","0bbf6839":"markdown","69cd9ab0":"markdown","b6e341e3":"markdown","8c1feebd":"markdown","a67419ef":"markdown","f9520bb2":"markdown","fdb18cf1":"markdown","fd48b279":"markdown","5c0770d8":"markdown"},"source":{"780ec6ea":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","04ddc57f":"# Import related libraries\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport gc\nimport psutil\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA, KernelPCA\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import confusion_matrix,accuracy_score,precision_score,recall_score,f1_score,make_scorer\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,VotingClassifier\n\nfrom imblearn.pipeline import make_pipeline\nfrom imblearn.under_sampling import OneSidedSelection\nfrom imblearn.under_sampling import NearMiss\nfrom imblearn.under_sampling import EditedNearestNeighbours\nfrom imblearn.under_sampling import TomekLinks\nfrom imblearn.under_sampling import RandomUnderSampler\n\npd.set_option('display.max_columns', None)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nRANDOM_SEED = 101\n\nimport collections\nfrom mpl_toolkits import mplot3d","71f53e8e":"# Input file\nmain_df = pd.read_csv(\"\/kaggle\/input\/predicting-a-pulsar-star\/pulsar_stars.csv\")","8a7319ca":"main_df.shape","bcfa8c13":"main_df.head()","1674b029":"main_df[\"target_class\"].value_counts().plot.bar()","e8dc4372":"main_df.describe()","77ab3253":"main_df[main_df[\"target_class\"] == 0].describe()","8abb2217":"main_df[main_df[\"target_class\"] == 1].describe()","6f6b2493":"# Check whether there is any missing values\nmain_df.isnull().sum()","96707c43":"main_df_corr = main_df.iloc[:,:-1].corr()\nmain_df_corr[np.abs(main_df_corr)<.5] = 0\n\n# Correlation plot\nplt.figure(figsize=(15,10))\nsns.heatmap(main_df_corr,\n            vmin=-1,\n            cmap='bone',\n            annot=True);","996defcf":"# Release memory and garbage collection\ndel main_df_corr\ngc.collect()","0cbb4a3f":"for col in main_df.columns[:-1]:\n    fig = plt.figure(figsize = (8,6))\n    ax = fig.add_subplot(111)\n    ax = sns.kdeplot(main_df[col][(main_df[\"target_class\"] == 0)], color=\"Red\", shade = True)\n    ax = sns.kdeplot(main_df[col][(main_df[\"target_class\"] == 1)], color=\"Blue\", shade= True)\n    ax.set_xlabel(col)\n    ax.set_ylabel(\"Frequency\")\n    ax.legend([\"Not Pulsar Star\",\"Pulsar Star\"],loc=\"best\")\n    ax.set_title('Frequency Distribution of {}'.format(col), fontsize = 15)\n    print('\\n')\n    print('\\n')","5ebcebc0":"sns.pairplot(data=main_df,\n             palette=\"husl\",\n             hue=\"target_class\",\n             vars=main_df.columns[:-1])\n\nplt.suptitle(\"Paiplot for variables\",fontsize=18)","6a031c85":"# Create a Pipeline\nestimators_pca = []\nestimators_pca.append(('Scaler',StandardScaler()))\nestimators_pca.append(('PCA',PCA(n_components=2)))\n\npca = Pipeline(estimators_pca)","f2022403":"# Transform data with PCA\ndata_pca = pca.fit_transform(main_df.iloc[:,:-1].values)\n\npca_df = pd.DataFrame(data = data_pca, columns = ['comp 1', 'comp 2'])\npca_df = pd.concat([pca_df, main_df[['target_class']]], axis = 1)","94c02050":"# Release memory and Garbage Collection\ndel data_pca\ngc.collect()","d1943947":"# Plot Linear PCA with 2 components\nfig = plt.figure(figsize = (20,15))\nax = fig.add_subplot(1,1,1) \nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 2', fontsize = 15)\nax.set_title('2 component PCA', fontsize = 20)\ntargets = [0,1]\ncolors = ['r', 'g']\nfor target, color in zip(targets,colors):\n    indicesToKeep = pca_df['target_class'] == target\n    ax.scatter(pca_df.loc[indicesToKeep, 'comp 1'],pca_df.loc[indicesToKeep, 'comp 2'],c = color,s = 50)\nax.legend(targets)\nax.grid()","fd48e57e":"estimators_kpca = []\nestimators_kpca.append(('Scaler',StandardScaler()))\nestimators_kpca.append(('KPCA',KernelPCA(kernel=\"rbf\", n_components=2)))\n\nkpca = Pipeline(estimators_kpca)\n\ndata_kpca = kpca.fit_transform(main_df.iloc[:,:-1].values)\n\nkpca_df = pd.DataFrame(data = data_kpca, columns = ['comp 1', 'comp 2'])\nkpca_df = pd.concat([kpca_df, main_df[['target_class']]], axis = 1)\n\ndel data_kpca\ngc.collect()","9896e992":"fig = plt.figure(figsize = (20,15))\nax = fig.add_subplot(1,1,1) \nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 2', fontsize = 15)\nax.set_title('2 component PCA', fontsize = 20)\ntargets = [0,1]\ncolors = ['r', 'g']\nfor target, color in zip(targets,colors):\n    indicesToKeep = kpca_df['target_class'] == target\n    ax.scatter(kpca_df.loc[indicesToKeep, 'comp 1'],kpca_df.loc[indicesToKeep, 'comp 2'],c = color,s = 50)\nax.legend(targets)\nax.grid()","a9a54714":"estimators_kpca = []\nestimators_kpca.append(('Scaler',StandardScaler()))\nestimators_kpca.append(('KPCA',KernelPCA(kernel=\"sigmoid\", n_components=2)))\n\nkpca = Pipeline(estimators_kpca)\n\ndata_kpca = kpca.fit_transform(main_df.iloc[:,:-1].values)\n\nkpca_df = pd.DataFrame(data = data_kpca, columns = ['comp 1', 'comp 2'])\nkpca_df = pd.concat([kpca_df, main_df[['target_class']]], axis = 1)\n\ndel data_kpca\ngc.collect()","87e3b405":"fig = plt.figure(figsize = (20,15))\nax = fig.add_subplot(1,1,1) \nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 2', fontsize = 15)\nax.set_title('2 component PCA', fontsize = 20)\ntargets = [0,1]\ncolors = ['r', 'g']\nfor target, color in zip(targets,colors):\n    indicesToKeep = kpca_df['target_class'] == target\n    ax.scatter(kpca_df.loc[indicesToKeep, 'comp 1'],kpca_df.loc[indicesToKeep, 'comp 2'],c = color,s = 50)\nax.legend(targets)\nax.grid()","f6e5b3b5":"estimators_kpca = []\nestimators_kpca.append(('Scaler',StandardScaler()))\nestimators_kpca.append(('KPCA',KernelPCA(kernel=\"sigmoid\", n_components=3)))\nkpca3 = Pipeline(estimators_kpca)\n\ndata_kpca3 = kpca3.fit_transform(main_df.iloc[:,:-1].values)\n\nkpca3_df = pd.DataFrame(data = data_kpca3, columns = ['comp 1', 'comp 2', 'comp 3'])\nkpca3_df = pd.concat([kpca3_df, main_df[['target_class']]], axis = 1)","8395a2fa":"del data_kpca3\ngc.collect()","f5fc9abf":"fig = plt.figure(figsize = (40,25))\nax = fig.add_subplot(111, projection='3d')\nax.set_xlabel('Principal Component 1', fontsize = 30)\nax.set_ylabel('Principal Component 2', fontsize = 30)\nax.set_zlabel('Principal Component 3', fontsize = 30)\nax.set_title('3 component Kernel - PCA', fontsize = 35)\nax.tick_params(labelsize=10)\ntargets = [0,1]\ncolors = ['r', 'g']\nfor target, color in zip(targets,colors):\n    indicesToKeep = kpca3_df['target_class'] == target\n    ax.scatter(kpca3_df.loc[indicesToKeep, 'comp 1'],\n               kpca3_df.loc[indicesToKeep, 'comp 2'],\n               kpca3_df.loc[indicesToKeep, 'comp 3'],\n               c = color,\n               s = 50)\nax.legend(targets)\nax.grid()","6d57a633":"# Delete all prior data created for transformations\ndel pca_df\ndel kpca_df\ngc.collect()","b6cb4a20":"# Split the data in train and test for baseline models\nX_train,X_test,y_train,y_test = train_test_split(kpca3_df.iloc[:,:-1].values,\n                                                 kpca3_df.iloc[:,-1].values,\n                                                 test_size=0.25,\n                                                 random_state=RANDOM_SEED)","dbb22eca":"# List of models to be used\nclassification_models = ['LogisticRegression',\n                         'SVC',\n                         'DecisionTreeClassifier',\n                         'RandomForestClassifier',\n                         'AdaBoostClassifier']","2946c778":"# Metrics to be captured for each model\ncm = []\nacc = []\nprec = []\nrec = []\nf1 = []\nmodels = []\nestimators = []\nestimators_us = []","9e4f9876":"# Instantiate every model, fit on training data, make predictions on testing data and capture prediction metrices\nfor classfication_model in classification_models:\n    \n    model = eval(classfication_model)()\n    \n    model.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    \n    models.append(classfication_model)\n    estimators.append((classfication_model,model))\n    cm.append(confusion_matrix(y_test,y_pred))\n    acc.append(accuracy_score(y_test,y_pred))\n    prec.append(precision_score(y_test,y_pred))\n    rec.append(recall_score(y_test,y_pred))\n    f1.append(f1_score(y_test,y_pred))","1563e2da":"model_dict = {\"Models\":models,\n             \"CM\":cm,\n             \"Accuracy\":acc,\n             \"Precision\":prec,\n             \"Recall\":rec,\n             \"f1_score\":f1}\nmodel_df = pd.DataFrame(model_dict)\nmodel_df.sort_values(by=['f1_score','Recall','Precision','Accuracy'],ascending=False,inplace=True)\nmodel_df","5279b22c":"# Different Under Sampling methods\nundersamplers = ['TomekLinks','RandomUnderSampler','EditedNearestNeighbours',\n                 'NearMiss','OneSidedSelection']","7233f046":"print(\"Shape before under-sampling: \",main_df.shape[0])\nprint(\"Target spread before under-sampling: \",main_df['target_class'].value_counts())\nprint('\\n')\nfor undersampler in undersamplers:\n    \n    us_obj = eval(undersampler)()\n    X_res, y_res = us_obj.fit_resample(main_df.iloc[:,:-1].values,\n                                       main_df.iloc[:,-1].values)\n    print(undersampler,\" : \")\n    print(\"Shape after under-sampling: \",y_res.shape)\n    print(\"Target spread after under-sampling: \",collections.Counter(y_res))\n    print('\\n')","e57664d5":"# Visualizing the undersampled transformed data\nfor undersampler in undersamplers:\n    us_obj = eval(undersampler)()\n    if undersampler in ['RandomUnderSampler','NearMiss']:\n        X_res, y_res = us_obj.fit_resample(main_df.iloc[:,:-1].values,\n                                           main_df.iloc[:,-1].values)\n    else:\n        X_res, y_res = us_obj.fit_sample(main_df.iloc[:,:-1].values,\n                                         main_df.iloc[:,-1].values)\n    est_US_kpca = []\n    est_US_kpca.append(('Scaler',StandardScaler()))\n    est_US_kpca.append(('KPCA',KernelPCA(kernel=\"sigmoid\", n_components=3)))\n    kpca3_us = Pipeline(est_US_kpca)\n\n    data_US_kpca3 = kpca3_us.fit_transform(X_res)\n\n    kpca3_US_df = pd.DataFrame(data = data_US_kpca3, columns = ['comp 1', 'comp 2', 'comp 3'])\n    kpca3_US_df = pd.concat([kpca3_US_df, \n                             pd.DataFrame(y_res, columns=['target_class'])], \n                            axis = 1)\n    \n    # Plot the Graphs\n    fig = plt.figure(figsize = (40,25))\n    ax = fig.add_subplot(111, projection='3d')\n    ax.set_xlabel('Principal Component 1', fontsize = 30)\n    ax.set_ylabel('Principal Component 2', fontsize = 30)\n    ax.set_zlabel('Principal Component 3', fontsize = 30)\n    ax.set_title('3 component Kernel-PCA with Undersampler : {}'.format(undersampler), \n                 fontsize = 35)\n    targets = [0,1]\n    colors = ['r', 'g']\n    for target, color in zip(targets,colors):\n        indicesToKeep = kpca3_US_df['target_class'] == target\n        ax.scatter(kpca3_US_df.loc[indicesToKeep, 'comp 1'],\n                   kpca3_US_df.loc[indicesToKeep, 'comp 2'],\n                   kpca3_US_df.loc[indicesToKeep, 'comp 3'],\n                   c = color,\n                   s = 50)\n    ax.legend(targets)\n    ax.grid()\n    \n    del X_res\n    del y_res\n    del kpca3_US_df\n    del data_US_kpca3\n    gc.collect()\n    \n    print('\\n')\n    print('\\n')","a412da97":"model_sampler = [(model,sampler) for model in classification_models for sampler in undersamplers]","b18f8063":"for classification_model,undersampler in model_sampler:\n    \n    us_obj = eval(undersampler)()\n    X_res, y_res = us_obj.fit_resample(main_df.iloc[:,:-1].values,\n                                       main_df.iloc[:,-1].values)\n    \n    kpca3_us = Pipeline([('Scaler',StandardScaler()),\n                         ('KPCA',KernelPCA(kernel=\"sigmoid\", n_components=3))])\n\n    data_US_kpca3 = kpca3_us.fit_transform(X_res)\n    X_train,X_test,y_train,y_test = train_test_split(data_US_kpca3,\n                                                     y_res,\n                                                     test_size=0.25,\n                                                     random_state=RANDOM_SEED)\n    model = eval(classification_model)()\n    \n    model.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    \n    model_name = classification_model + \"_\" + undersampler\n    models.append(model_name)\n    estimators_us.append((model_name,model))\n    cm.append(confusion_matrix(y_test,y_pred))\n    acc.append(accuracy_score(y_test,y_pred))\n    prec.append(precision_score(y_test,y_pred))\n    rec.append(recall_score(y_test,y_pred))\n    f1.append(f1_score(y_test,y_pred))\n    \n    del X_res\n    del y_res\n    del data_US_kpca3\n    gc.collect()","1de26d41":"model_dict = {\"Models\":models,\n             \"CM\":cm,\n             \"Accuracy\":acc,\n             \"Precision\":prec,\n             \"Recall\":rec,\n             \"f1_score\":f1}\nmodel_df = pd.DataFrame(model_dict)\nmodel_df.sort_values(by=['f1_score','Recall','Precision','Accuracy'],ascending=False,inplace=True)\nmodel_df.head(10)","6c363cd6":"best_models = model_df.head(5)[\"Models\"].tolist()\n\nbest_estimators = []\nfor model,est in estimators_us:\n    if model in best_models:\n        best_estimators.append((model,est))","56c3d23b":"vc = VotingClassifier(best_estimators)\nvc.fit(X_train,y_train)\ny_pred = vc.predict(X_test)\n    \nmodels.append(type(vc).__name__ + \"Best_Under_Sampled_Baseline\")\n\ncm.append(confusion_matrix(y_test,y_pred))\nacc.append(accuracy_score(y_test,y_pred))\nprec.append(precision_score(y_test,y_pred))\nrec.append(recall_score(y_test,y_pred))\nf1.append(f1_score(y_test,y_pred))","c6adef15":"model_dict = {\"Models\":models,\n             \"CM\":cm,\n             \"Accuracy\":acc,\n             \"Precision\":prec,\n             \"Recall\":rec,\n             \"f1_score\":f1}\nmodel_df = pd.DataFrame(model_dict)\nmodel_df.sort_values(by=['f1_score','Recall','Precision','Accuracy'],ascending=False,inplace=True)\nmodel_df.head(10)","da897f67":"# Define f1-metric function\ndef f1_metric(y_test,y_pred):\n    return f1_score(y_test,y_pred)","c925e3cd":"# Define f1 scorer\nf1_scorer = make_scorer(f1_metric,greater_is_better=True)","e37a3432":"# Get cross-validated f1-scores\ncv_scores = cross_val_score(vc,\n                            X_train,\n                            y_train,\n                            scoring=f1_scorer,\n                            cv=20,\n                            n_jobs=-1)","5c8e1954":"print(\"F1-score statistics: \")\nprint(\"Mean: \",cv_scores.mean())\nprint(\"Standard Deviation: \",cv_scores.std())\nprint(\"Max value: \",cv_scores.max())\nprint(\"Min value: \",cv_scores.min())","a92e56e3":"#### Create Baseline models with Under-Sampled data","23ca2aaf":"** From the above observations from baseline models we can conclude: **\n- The learning of the models are getting skewed towards identifying negatives. As a result, we see high accuracy score but low recall and f1-scores.\n- Therefore, this is an Imbalanced Classification. We need to balance the data with respect to the target classes.\n- We will under-sample the majority target class using different methods.","2358db77":"#### Basic Exploration on the Data","74021428":"** Lets pick the top five performing models and create a stacked ensemble **","fb429c2b":"## Identifying the problem","21eb2911":"#### Data ingestion","9e01594a":"** There are some clear as well as some overlapping differences between variables in order to identify the target. Lets try to visualize after PCA transformations and see we can have better distinguishable parameters to separate and identify the targets. **","dfc8db92":"** Now this is a much better representation of our data, therefore we will move ahead with 3 component PCA transformations on the data **","ab214f69":"** We still see there is overlapping space. Lets see if this could be resolved with Kernel-PCA with 2 components **","46dc5573":"#### Explore the skewness","2c6d1f50":"#### Projecting the Class Imbalance","c67e198b":"#### Create a Baseline Models","ff62ca27":"## Conclusion","6b38caac":"** KernelPCA with \"rbf\" kernel doesn't yield better separable class boundaries. Lets try with \"sigmoid\" kernel ** ","841536b1":"## Exloratory Data Analysis","df1a5c15":"- We get much better results with re-sampling in case of Imbalanced Classification\n- We can try over-sampling techniques as well","98b8bc1c":"** Here we have just used Baseline models on the PCA transformed data and we have the following observations ** \n- The Accuracy score is high for all of the five models \n- The Recall and F1-score is suffering for all of the five models","0bbf6839":"#### Visualizing high correlations","69cd9ab0":"** Create a Stacked Ensemble **\n##### Note - We will train the stacked ensemble without undersampling the data","b6e341e3":"## Cross-Validate the stacked ensemble baseline","8c1feebd":"## Baselining","a67419ef":"## Data Tranformations","f9520bb2":"#### Linear PCA","fdb18cf1":"** KernelPCA with \"sigmoid\" kernel using 2 components reveal that there is a separable boundary, as wee see the data points representing \"target_class = 1\" overlayed on the data points representing \"target_class = 0\" in the overlapping space. Lets try KernelPCA with \"sigmoid\" kernel using 3 components and visualize on a 3-dimensional space. **","fd48b279":"#### Explore the inter-dependance of each ","5c0770d8":"## Introduction"}}