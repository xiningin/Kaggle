{"cell_type":{"288c7f2e":"code","e0b19918":"code","6772274d":"code","adc9d747":"code","4f0ad1db":"code","9f119310":"code","563bcdef":"code","ed2dd770":"code","77ee4f51":"code","79156258":"code","c85c072b":"code","e478af6c":"code","219994b4":"code","32caf67e":"code","fa28b75b":"code","fb9c78a4":"code","9452a50e":"code","50858890":"code","a1a864ee":"code","97fa297e":"code","527e16dc":"code","fbaeb28d":"code","f1f74c51":"code","5c3e7255":"code","cb99eb21":"code","8b9e7ee8":"code","a7274e83":"code","4eaa0b73":"code","310d93a4":"code","565f9850":"code","d064cbfa":"code","2451fee6":"code","6d6a26bf":"code","989ab64b":"code","7440676a":"code","7989763c":"code","ef7b4ab4":"code","33d825c2":"code","d0949db2":"code","8a5e4e50":"code","5f2ec202":"code","e01b4b03":"code","fab77c4b":"code","c5e2c00b":"code","4c9fc1c2":"code","fe0e0c48":"code","113c73de":"code","26869816":"code","ad1123a8":"code","b0b9f71d":"code","310b7f17":"markdown","0f0143ff":"markdown","3760dcc0":"markdown","a41883a1":"markdown","3a5fbe00":"markdown","43e0f213":"markdown","eebb1fc5":"markdown","2285420e":"markdown","bebe9d07":"markdown","93baf5f2":"markdown","3bb52321":"markdown","ce6366dc":"markdown","74cfa5b7":"markdown","026c8fa4":"markdown","3aa7e379":"markdown","75155a23":"markdown","f3943e0b":"markdown","ee8ef590":"markdown","73ecfedd":"markdown","1df4a627":"markdown","052aac9b":"markdown","7a386327":"markdown","9bdd26bb":"markdown","cdb8f017":"markdown","7c6933c7":"markdown","dcf801ce":"markdown","b1237fb5":"markdown","d905a3ff":"markdown","81e6e49e":"markdown","0736223e":"markdown","e02d250f":"markdown","d9192b7d":"markdown","aff5ca13":"markdown","0104b7f3":"markdown","a2ddabfd":"markdown","e2a4ab7d":"markdown","6de1b16a":"markdown","d408ffaa":"markdown","90984e25":"markdown","253c3e66":"markdown","38dda4f9":"markdown","5e9fcbb0":"markdown","0bea5b09":"markdown","3d88b6b2":"markdown","684240f1":"markdown","0e377f44":"markdown","3edb272b":"markdown","826f2bac":"markdown","dfcd2e8f":"markdown","151b8256":"markdown","2e204ad0":"markdown","520acec4":"markdown","4a5ea791":"markdown","d04a8918":"markdown","11d45b88":"markdown","cf45b517":"markdown","1869d4db":"markdown","6689f38e":"markdown","67f5ebe9":"markdown","65f44d7a":"markdown","6c00a5fb":"markdown","30d130e5":"markdown","9240e60a":"markdown"},"source":{"288c7f2e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e0b19918":"!pip install textstat","6772274d":"import string\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\n\n%matplotlib inline\n\nfrom plotly import tools\nimport plotly.offline as py\nimport plotly.figure_factory as ff\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nfrom sklearn import model_selection, preprocessing, metrics, ensemble, naive_bayes, linear_model\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\nfrom statistics import *\nimport concurrent.futures\nimport time\nimport pyLDAvis.sklearn\nfrom pylab import bone, pcolor, colorbar, plot, show, rcParams, savefig\nimport textstat\n\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\n\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999\n\nfrom geopy.geocoders import Nominatim\nfrom geopy.extra.rate_limiter import RateLimiter\nimport folium \nfrom folium import plugins \n\n\n#utility functions:\ndef plot_readability(a,b,title,bins=0.1,colors=['#3A4750', '#F64E8B']):\n    trace1 = ff.create_distplot([a,b], [\" Real disaster tweets\",\"Not real disaster tweets\"], bin_size=bins, colors=colors, show_rug=False)\n    trace1['layout'].update(title=title)\n    py.iplot(trace1, filename='Distplot')\n    table_data= [[\"Statistical Measures\",\" Not real disaster tweets\",\"real disaster tweets\"],\n                [\"Mean\",mean(a),mean(b)],\n                [\"Standard Deviation\",pstdev(a),pstdev(b)],\n                [\"Variance\",pvariance(a),pvariance(b)],\n                [\"Median\",median(a),median(b)],\n                [\"Maximum value\",max(a),max(b)],\n                [\"Minimum value\",min(a),min(b)]]\n    trace2 = ff.create_table(table_data)\n    py.iplot(trace2, filename='Table')\n\npunctuations = string.punctuation\nstopwords = list(STOP_WORDS)\n\nparser = English()\ndef spacy_tokenizer(sentence):\n    mytokens = parser(sentence)\n    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n    mytokens = [ word for word in mytokens if word not in stopwords and word not in punctuations ]\n    mytokens = \" \".join([i for i in mytokens])\n    return mytokens\n\nimport re\ndef cleanhtml(raw_html):\n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, '', raw_html)\n    return cleantext\n\ndef removeurl(raw_text):\n    clean_text = re.sub(r'^https?:\\\/\\\/.*[\\r\\n]*', '', raw_text, flags=re.MULTILINE)\n    return clean_text","adc9d747":"train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\nsub = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')","4f0ad1db":"#glimpse at train dataset\ntrain.head()","9f119310":"#glimpse at test dataset\ntest.head()","563bcdef":"#some basic cleaning\ntrain['text'] = train['text'].apply(lambda x:cleanhtml(x))\ntest['text'] = test['text'].apply(lambda x:cleanhtml(x))\n\n#removing url tags\ntrain['text'] = train['text'].apply(lambda x:removeurl(x))\ntest['text'] = test['text'].apply(lambda x:removeurl(x))\n","ed2dd770":"cnt_srs = train['target'].value_counts()\ntrace = go.Bar(\n    x=cnt_srs.index,\n    y=cnt_srs.values,\n    marker=dict(\n        color=cnt_srs.values,\n        colorscale = 'Jet',\n        reversescale = True\n    ),\n)\n\nlayout = go.Layout(\n    title='Target Count',\n    font=dict(size=18)\n)\n\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"TargetCount\")\n\n## target distribution ##\nlabels = (np.array(cnt_srs.index))\nsizes = (np.array((cnt_srs \/ cnt_srs.sum())*100))\n\ntrace = go.Pie(labels=labels, values=sizes)\nlayout = go.Layout(\n    title='Target distribution',\n    font=dict(size=18),\n    width=600,\n    height=600,\n)\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"usertype\")","77ee4f51":"cnt_ = train['location'].value_counts()\ncnt_.reset_index()\ncnt_ = cnt_[:20,]\ntrace1 = go.Bar(\n                x = cnt_.index,\n                y = cnt_.values,\n                name = \"Number of tweets in dataset according to location\",\n                marker = dict(color = 'rgba(200, 74, 55, 0.5)',\n                             line=dict(color='rgb(0,0,0)',width=1.5)),\n                )\n\ndata = [trace1]\nlayout = go.Layout(barmode = \"group\",title = 'Number of tweets in dataset according to location')\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig)","79156258":"train1_df = train[train[\"target\"]==1]\ntrain0_df = train[train[\"target\"]==0]\ncnt_1 = train1_df['location'].value_counts()\ncnt_1.reset_index()\ncnt_1 = cnt_1[:20,]\n\ncnt_0 = train0_df['location'].value_counts()\ncnt_0.reset_index()\ncnt_0 = cnt_0[:20,]\n\ntrace1 = go.Bar(\n                x = cnt_1.index,\n                y = cnt_1.values,\n                name = \"Number of tweets about real disaster location wise\",\n                marker = dict(color = 'rgba(255, 74, 55, 0.5)',\n                             line=dict(color='rgb(0,0,0)',width=1.5)),\n                )\ntrace0 = go.Bar(\n                x = cnt_0.index,\n                y = cnt_0.values,\n                name = \"Number of tweets other than real disaster location wise\",\n                marker = dict(color = 'rgba(79, 82, 97, 0.5)',\n                             line=dict(color='rgb(0,0,0)',width=1.5)),\n                )\n\n\ndata = [trace0,trace1]\nlayout = go.Layout(barmode = 'stack',title = 'Number of tweets in dataset according to location')\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig)","c85c072b":"df = train['location'].value_counts()[:20,]\ndf = pd.DataFrame(df)\ndf = df.reset_index()\ndf.columns = ['location', 'counts'] \ngeolocator = Nominatim(user_agent=\"specify_your_app_name_here\")\ngeocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)\ndictt_latitude = {}\ndictt_longitude = {}\nfor i in df['location'].values:\n    print(i)\n    location = geocode(i)\n    dictt_latitude[i] = location.latitude\n    dictt_longitude[i] = location.longitude\ndf['latitude']= df['location'].map(dictt_latitude)\ndf['longitude'] = df['location'].map(dictt_longitude)","e478af6c":"map1 = folium.Map(location=[10.0, 10.0], tiles='CartoDB dark_matter', zoom_start=2.3)\nmarkers = []\nfor i, row in df.iterrows():\n    loss = row['counts']\n    if row['counts'] > 0:\n        count = row['counts']*0.4\n    folium.CircleMarker([float(row['latitude']), float(row['longitude'])], radius=float(count), color='#ef4f61', fill=True).add_to(map1)\nmap1","219994b4":"from wordcloud import WordCloud, STOPWORDS\n\n# Thanks : https:\/\/www.kaggle.com\/aashita\/word-clouds-of-various-shapes ##\ndef plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), \n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color='black',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    \nplot_wordcloud(train[train[\"target\"]==1], title=\"Word Cloud of tweets if real disaster\")","32caf67e":"plot_wordcloud(train[train[\"target\"]==0], title=\"Word Cloud of tweets if not a real disaster\")","fa28b75b":"from collections import defaultdict\ntrain1_df = train[train[\"target\"]==1]\ntrain0_df = train[train[\"target\"]==0]\n\n## custom function for ngram generation ##\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n## custom function for horizontal bar chart ##\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"word\"].values[::-1],\n        x=df[\"wordcount\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\n## Get the bar chart from sincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in train0_df[\"text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), 'red')\n\n## Get the bar chart from insincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), 'red')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent words if tweet is not real disaster\", \n                                          \"Frequent words if tweet is real disaster\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\npy.iplot(fig, filename='word-plots')\n\n","fb9c78a4":"freq_dict = defaultdict(int)\nfor sent in train0_df[\"text\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), 'green')\n\n\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"text\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), 'green')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,horizontal_spacing=0.15,\n                          subplot_titles=[\"Frequent words if tweet is not real disaster\", \n                                          \"Frequent words if tweet is real disaster\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Bigram Count Plots\")\npy.iplot(fig, filename='word-plots')","9452a50e":"freq_dict = defaultdict(int)\nfor sent in train0_df[\"text\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), 'orange')\n\n\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"text\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), 'orange')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04, horizontal_spacing=0.2,\n                          subplot_titles=[\"Frequent words if tweet is not real disaster\", \n                                          \"Frequent words if tweet is real disaster\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=1200, width=1200, paper_bgcolor='rgb(233,233,233)', title=\"Trigram Count Plots\")\npy.iplot(fig, filename='word-plots')","50858890":"train[\"num_words\"] = train[\"text\"].apply(lambda x: len(str(x).split()))\ntest[\"num_words\"] = test[\"text\"].apply(lambda x: len(str(x).split()))\n\n## Number of unique words in the text ##\ntrain[\"num_unique_words\"] = train[\"text\"].apply(lambda x: len(set(str(x).split())))\ntest[\"num_unique_words\"] = test[\"text\"].apply(lambda x: len(set(str(x).split())))\n\n## Number of characters in the text ##\ntrain[\"num_chars\"] = train[\"text\"].apply(lambda x: len(str(x)))\ntest[\"num_chars\"] = test[\"text\"].apply(lambda x: len(str(x)))\n\n## Number of stopwords in the text ##\ntrain[\"num_stopwords\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\ntest[\"num_stopwords\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n\n## Number of punctuations in the text ##\ntrain[\"num_punctuations\"] =train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\ntest[\"num_punctuations\"] =test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n\n## Number of title case words in the text ##\ntrain[\"num_words_upper\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ntest[\"num_words_upper\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n\n## Number of title case words in the text ##\ntrain[\"num_words_title\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\ntest[\"num_words_title\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n\n## Average length of the words in the text ##\ntrain[\"mean_word_len\"] = train[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ntest[\"mean_word_len\"] = test[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","a1a864ee":"train['num_words'].loc[train['num_words']>60] = 100 #truncation for better visuals\ntrain['num_punctuations'].loc[train['num_punctuations']>25] = 25 #truncation for better visuals\ntrain['num_chars'].loc[train['num_chars']>350] = 350 #truncation for better visuals\n\nf, axes = plt.subplots(3, 1, figsize=(10,20))\nsns.boxplot(x='target', y='num_words', data=train, ax=axes[0])\naxes[0].set_xlabel('Target', fontsize=12)\naxes[0].set_title(\"Number of words in each class\", fontsize=15)\n\nsns.boxplot(x='target', y='num_chars', data=train, ax=axes[1])\naxes[1].set_xlabel('Target', fontsize=12)\naxes[1].set_title(\"Number of characters in each class\", fontsize=15)\n\nsns.boxplot(x='target', y='num_punctuations', data=train, ax=axes[2])\naxes[2].set_xlabel('Target', fontsize=12)\naxes[2].set_title(\"Number of punctuations in each class\", fontsize=15)\nplt.show()","97fa297e":"train1_df = train[train[\"target\"]==1]\ntrain0_df = train[train[\"target\"]==0]\n\nfig = go.Figure()\nfig.add_trace(go.Histogram(x=train1_df['num_words'],name = 'Number of words in tweets about real disaster'))\nfig.add_trace(go.Histogram(x=train0_df['num_words'],name = 'Number of words in tweets other than real disaster'))\n\n# Overlay both histograms\nfig.update_layout(barmode='stack')\n# Reduce opacity to see both histograms\nfig.update_traces(opacity=0.75)\nfig.show()","527e16dc":"fig = go.Figure()\nfig.add_trace(go.Histogram(x=train1_df['num_chars'],name = 'Number of chars in tweets about real disaster',marker = dict(color = 'rgba(200, 100, 0, 0.8)')))\nfig.add_trace(go.Histogram(x=train0_df['num_chars'],name = 'Number of chars in tweets about real disaster',marker = dict(color = 'rgba(25, 133, 120, 0.8)')))\n\n# Overlay both histograms\nfig.update_layout(barmode='stack')\n# Reduce opacity to see both histograms\nfig.update_traces(opacity=0.75)\nfig.show()","fbaeb28d":"fig = go.Figure()\nfig.add_trace(go.Histogram(x=train1_df['num_punctuations'],name = 'Number of punctuations in tweets about real disaster',marker = dict(color = 'rgba(97, 175, 222, 0.8)')))\nfig.add_trace(go.Histogram(x=train0_df['num_punctuations'],name = 'Number of punctuations in tweets other than real disaster',marker = dict(color = 'rgba(200, 10, 150, 0.8)')))\n\n# Overlay both histograms\nfig.update_layout(barmode='stack')\n# Reduce opacity to see both histograms\nfig.update_traces(opacity=1)\nfig.show()","f1f74c51":"fig = go.Figure()\nfig.add_trace(go.Histogram(x=train['num_words'],name = 'Number of words in training tweets',marker = dict(color = 'rgba(255, 0, 0, 0.8)')))\nfig.add_trace(go.Histogram(x=test['num_words'],name = 'Number of words in testing tweets ',marker = dict(color = 'rgba(0, 187, 187, 0.8)')))\n\n# Overlay both histograms\nfig.update_layout(barmode='stack')\n# Reduce opacity to see both histograms\nfig.update_traces(opacity=0.75)\nfig.show()","5c3e7255":"fig = go.Figure()\nfig.add_trace(go.Histogram(x=train['num_chars'],name = 'Number of chars in training tweets',marker = dict(color = 'rgba(25, 13, 8, 0.8)')))\nfig.add_trace(go.Histogram(x=test['num_chars'],name = 'Number of chars in testing tweets ',marker = dict(color = 'rgba(8, 25, 187, 0.8)')))\n\n# Overlay both histograms\nfig.update_layout(barmode='stack')\n# Reduce opacity to see both histograms\nfig.update_traces(opacity=0.75)\nfig.show()","cb99eb21":"fig = go.Figure()\nfig.add_trace(go.Histogram(x=train['num_punctuations'],name = 'Number of punctuations in training tweets',marker = dict(color = 'rgba(222, 111, 33, 0.8)')))\nfig.add_trace(go.Histogram(x=test['num_punctuations'],name = 'Number of punctuations in testing tweets ',marker = dict(color = 'rgba(33, 111, 222, 0.8)')))\n\n# Overlay both histograms\nfig.update_layout(barmode='stack')\n# Reduce opacity to see both histograms\nfig.update_traces(opacity=0.75)\nfig.show()","8b9e7ee8":"tqdm.pandas()\nfre_notreal = np.array(train[\"text\"][train[\"target\"] == 0].progress_apply(textstat.flesch_reading_ease))\nfre_real = np.array(train[\"text\"][train[\"target\"] == 1].progress_apply(textstat.flesch_reading_ease))\nplot_readability(fre_notreal,fre_real,\"Flesch Reading Ease\",20)","a7274e83":"fkg_notreal = np.array(train[\"text\"][train[\"target\"] == 0].progress_apply(textstat.flesch_kincaid_grade))\nfkg_real = np.array(train[\"text\"][train[\"target\"] == 1].progress_apply(textstat.flesch_kincaid_grade))\nplot_readability(fkg_notreal,fkg_real,\"Flesch Kincaid Grade\",4,['#C1D37F','#491F21'])","4eaa0b73":"fog_notreal = np.array(train[\"text\"][train[\"target\"] == 0].progress_apply(textstat.gunning_fog))\nfog_real = np.array(train[\"text\"][train[\"target\"] == 1].progress_apply(textstat.gunning_fog))\nplot_readability(fog_notreal,fog_real,\"The Fog Scale (Gunning FOG Formula)\",4,['#E2D58B','#CDE77F'])","310d93a4":"ari_notreal = np.array(train[\"text\"][train[\"target\"] == 0].progress_apply(textstat.automated_readability_index))\nari_real = np.array(train[\"text\"][train[\"target\"] == 1].progress_apply(textstat.automated_readability_index))\nplot_readability(ari_notreal,ari_real,\"Automated Readability Index\",10,['#488286','#FF934F'])","565f9850":"cli_notreal = np.array(train[\"text\"][train[\"target\"] == 0].progress_apply(textstat.coleman_liau_index))\ncli_real = np.array(train[\"text\"][train[\"target\"] == 1].progress_apply(textstat.coleman_liau_index))\nplot_readability(cli_notreal,cli_real,\"The Coleman-Liau Index\",10,['#8491A3','#2B2D42'])","d064cbfa":"lwf_notreal = np.array(train[\"text\"][train[\"target\"] == 0].progress_apply(textstat.linsear_write_formula))\nlwf_real = np.array(train[\"text\"][train[\"target\"] == 1].progress_apply(textstat.linsear_write_formula))\nplot_readability(lwf_notreal,lwf_real,\"Linsear Write Formula\",2,['#8D99AE','#EF233C'])","2451fee6":"dcr_notreal = np.array(train[\"text\"][train[\"target\"] == 0].progress_apply(textstat.dale_chall_readability_score))\ndcr_real = np.array(train[\"text\"][train[\"target\"] == 1].progress_apply(textstat.dale_chall_readability_score))\nplot_readability(dcr_notreal,dcr_real,\"Dale-Chall Readability Score\",1,['#C65D17','#DDB967'])","6d6a26bf":"def consensus_all(text):\n    return textstat.text_standard(text,float_output=True)\n\ncon_notreal = np.array(train[\"text\"][train[\"target\"] == 0].progress_apply(consensus_all))\ncon_real = np.array(train[\"text\"][train[\"target\"] == 1].progress_apply(consensus_all))\nplot_readability(con_notreal,con_real,\"Readability Consensus based upon all the above tests\",2)","989ab64b":"notreal_text = train[\"text\"][train[\"target\"] == 0].progress_apply(spacy_tokenizer)\nreal_text = train[\"text\"][train[\"target\"] == 1].progress_apply(spacy_tokenizer)\n#count vectorization\nvectorizer_notreal = CountVectorizer(min_df=5, max_df=0.9, stop_words='english', lowercase=True, token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\nnotreal_vectorized = vectorizer_notreal.fit_transform(notreal_text)\nvectorizer_real = CountVectorizer(min_df=5, max_df=0.9, stop_words='english', lowercase=True, token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\nreal_vectorized = vectorizer_real.fit_transform(real_text)","7440676a":"# Latent Dirichlet Allocation Model\nlda_notreal = LatentDirichletAllocation(n_components=10, max_iter=5, learning_method='online',verbose=True)\nnotreal_lda = lda_notreal.fit_transform(notreal_vectorized)\nlda_real = LatentDirichletAllocation(n_components=10, max_iter=5, learning_method='online',verbose=True)\nreal_lda = lda_real.fit_transform(real_vectorized)","7989763c":"def selected_topics(model, vectorizer, top_n=10):\n    for idx, topic in enumerate(model.components_):\n        print(\"Topic %d:\" % (idx))\n        print([(vectorizer.get_feature_names()[i], topic[i])\n                        for i in topic.argsort()[:-top_n - 1:-1]]) ","ef7b4ab4":"print(\"Not real disaster tweets LDA Model:\")\nselected_topics(lda_notreal, vectorizer_notreal)","33d825c2":"print(\"Real disaster tweets LDA Model:\")\nselected_topics(lda_real, vectorizer_real)","d0949db2":"pyLDAvis.enable_notebook()\ndash = pyLDAvis.sklearn.prepare(lda_notreal, notreal_vectorized, vectorizer_notreal, mds='tsne')\ndash","8a5e4e50":"pyLDAvis.enable_notebook()\ndash = pyLDAvis.sklearn.prepare(lda_real, real_vectorized, vectorizer_real, mds='tsne')\ndash","5f2ec202":"tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\ntfidf_vec.fit_transform(train['text'].values.tolist() + test['text'].values.tolist())\ntrain_tfidf = tfidf_vec.transform(train['text'].values.tolist())\ntest_tfidf = tfidf_vec.transform(test['text'].values.tolist())","e01b4b03":"train_y = train[\"target\"].values\n\ndef runModel(train_X, train_y, test_X, test_y, test_X2):\n    model = linear_model.LogisticRegression(C=5., solver='sag')\n    model.fit(train_X, train_y)\n    pred_test_y = model.predict_proba(test_X)[:,1]\n    pred_test_y2 = model.predict_proba(test_X2)[:,1]\n    return pred_test_y, pred_test_y2, model\n\nprint(\"Building model.\")\ncv_scores = []\npred_full_test = 0\npred_train = np.zeros([train.shape[0]])\nkf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\nfor dev_index, val_index in kf.split(train):\n    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    pred_val_y, pred_test_y, model = runModel(dev_X, dev_y, val_X, val_y, test_tfidf)\n    pred_full_test = pred_full_test + pred_test_y\n    pred_train[val_index] = pred_val_y\n    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n    break","fab77c4b":"from tqdm import tqdm\ndef threshold_search(y_true, y_proba):\n#reference: https:\/\/www.kaggle.com\/hung96ad\/pytorch-starter\n    best_threshold = 0\n    best_score = 0\n    for threshold in tqdm([i * 0.001 for i in range(1000)]):\n        score = metrics.f1_score(y_true=y_true, y_pred=y_proba > threshold)\n        if score > best_score:\n            best_threshold = threshold\n            best_score = score\n    search_result = {'threshold': best_threshold, 'f1': best_score}\n    return search_result\nsearch_result = threshold_search(val_y, pred_val_y)\nsearch_result","c5e2c00b":"print(\"F1 score at threshold {0} is {1}\".format(0.381, metrics.f1_score(val_y, (pred_val_y>0.381).astype(int))))\nprint(\"Precision at threshold {0} is {1}\".format(0.381, metrics.precision_score(val_y, (pred_val_y>0.381).astype(int))))\nprint(\"recall score at threshold {0} is {1}\".format(0.381, metrics.recall_score(val_y, (pred_val_y>0.381).astype(int))))","4c9fc1c2":"import eli5\neli5.show_weights(model, vec=tfidf_vec, top=100, feature_filter=lambda x: x != '<BIAS>')","fe0e0c48":"#loading markovify module\nimport markovify","113c73de":"#preparing dataset\ndata_notreal = train[\"text\"][train[\"target\"] == 0]\ndata_real = train[\"text\"][train[\"target\"] == 1]","26869816":"text_model_notreal = markovify.NewlineText(data_notreal, state_size = 2)\ntext_model_real = markovify.NewlineText(data_real, state_size = 2)","ad1123a8":"for i in range(10):\n    print(text_model_notreal.make_sentence())","b0b9f71d":"for i in range(10):\n    print(text_model_real.make_sentence())","310b7f17":"## 9.8 Readability Consensus based upon all the above tests <br>\n* Based upon all the above tests, returns the estimated school grade level required to understand the text. <br>","0f0143ff":"## 9.3 The Fog Scale (Gunning FOG Formula) <br>\n* In linguistics, the Gunning fog index is a readability test for English writing. The index estimates the years of formal education a person needs to understand the text on the first reading. For instance, a fog index of 12 requires the reading level of a United States high school senior (around 18 years old). <br>\n* The formula to calculate Fog scale: <br>\n![](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/84cd504cf61d43230ef59fbd0ecf201796e5e577)\n* Read more : [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Gunning_fog_index)","3760dcc0":"# 5. Word clouds of each class","a41883a1":"## 6.1 Word Frequency ","3a5fbe00":"## 8.5 Histogram plots of number of chars in train and test sets","43e0f213":"## About the competition\nTwitter has become an important communication channel in times of emergency.\nThe ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n\nBut, it\u2019s not always clear whether a person\u2019s words are actually announcing a disaster","eebb1fc5":"*  Most of the tweets are from USA,London,Canada.","2285420e":"## 6.3 Trigram plots","bebe9d07":"![](http:\/\/ushuaianoticias.com\/wp-content\/uploads\/2019\/10\/twitter.jpg)","93baf5f2":"### Getting latitude and longitudes of locations for plotting.","3bb52321":"## 9.7 Dale-Chall Readability Score <br>\n* Different from other tests, since it uses a lookup table of the most commonly used 3000 English words. Thus it returns the grade level using the New Dale-Chall Formula. <br>\n* The formula for calculating the raw score of the Dale\u2013Chall readability score is given below: <br>\n![](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/0541f1e629f0c06796c5a5babb3fac8d100a858c)\n\n* Score - Understood by\n\n** 4.9 or lower - average 4th-grade student or lower ** <br>\n** 5.0\u20135.9 - average 5th or 6th-grade student ** <br>\n** 6.0\u20136.9 - average 7th or 8th-grade student ** <br>\n** 7.0\u20137.9 - average 9th or 10th-grade student ** <br>\n** 8.0\u20138.9 - average 11th or 12th-grade student ** <br>\n** 9.0\u20139.9 - average 13th to 15th-grade (college) student** <br>\n\n* Read More : [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Dale%E2%80%93Chall_readability_formula)","ce6366dc":"## 4.3 Visualising number of tweets per location on maps","74cfa5b7":"## 10.4 Visualizing LDA results of a real disaster tweets with pyLDAvis ","026c8fa4":"# 12. Building basic Logistic regression model","3aa7e379":"# 6. Word Frequency plots per each class(0 or 1)","75155a23":"## 8.4 Histogram plots of number of words in train and test sets","f3943e0b":"## 10.2 Printing keywords","ee8ef590":"# 8. Histogram plots","73ecfedd":"# 4. Exploring location column","1df4a627":"## 8.6 Histogram plots of number of punctuations in train and test sets","052aac9b":"# 11. Tfidf Vectorization","7a386327":"# 7. Creating Meta Features:\n\nNow we will create some meta features and then look at how they are distributed between the classes. The ones that we will create are\n\n1. Number of words in the text <br>\n2. Number of unique words in the text <br>\n3. Number of characters in the text<br>\n4. Number of stopwords<br>\n5. Number of punctuations<br>\n6. Number of upper case words<br>\n7. Number of title case words<br>\n8. Average length of the words<br>","9bdd26bb":"### <font color='red'>If you find this kernel useful please consider upvoting it \ud83d\ude0a which keeps me motivated for doing hard work and to produce more quality content.<\/font>","cdb8f017":"*  By observing the above word cloud we can see some words like earthquake,fire,wildfires etc.., which refer to real disasters.","7c6933c7":"## 10.3 Visualizing LDA results of not a real disaster tweets with pyLDAvis","dcf801ce":"## 6.2 Bigram plots","b1237fb5":"*  By examining we can observe words like disney,Wrecked etc.., we need to explore more.","d905a3ff":"# 14. Generating random tweets using Markov Chains <br>\n* This is just a fun part in this kernal. Until now in this kernel we saw about text analytics,Topic modelling and basic model fitting. Now we will also learn about Generating text using Markov Chains. <br>\n","81e6e49e":"# 10. Topic Modelling <br>\n* Topic modeling is a machine learning technique that automatically analyzes text data to determine cluster words for a set of documents. This is known as \u2018unsupervised\u2019 machine learning because it doesn\u2019t require a predefined list of tags or training data that\u2019s been previously classified by humans. <br>\n* Topic modeling involves counting words and grouping similar word patterns to infer topics within unstructured data. <br>\n* Read More : [Here](https:\/\/monkeylearn.com\/blog\/introduction-to-topic-modeling\/) <br>","0736223e":"* NOTE: Adjust zoom for better visualisation. ","e02d250f":"## What is Markov Chains? <br>\n#### Markov chains, named after Andrey Markov, are mathematical systems that hop from one \"state\" (a situation or set of values) to another. For example, if you made a Markov chain model of a baby's behavior, you might include \"playing,\" \"eating\", \"sleeping,\" and \"crying\" as states, which together with other behaviors could form a 'state space': a list of all possible states. In addition, on top of the state space, a Markov chain tells you the probabilitiy of hopping, or \"transitioning,\" from one state to any other state---e.g., the chance that a baby currently playing will fall asleep in the next five minutes without crying first. <br>\n#### Below is the example of markov chains: <br>\n![](https:\/\/cdn-images-1.medium.com\/max\/800\/1*MbHRwYNA8F29hzes8EPHiQ.gif) <br>\n* Credits and references:\n* [Reference 1](http:\/\/setosa.io\/ev\/markov-chains\/)<br>\n* [Reference 2](https:\/\/www.kaggle.com\/nulldata\/meaningful-random-headlines-by-markov-chain\/notebook)\n* [Reference 3](https:\/\/hackernoon.com\/from-what-is-a-markov-model-to-here-is-how-markov-models-work-1ac5f4629b71) \n","d9192b7d":"## 7.1 Plotting of meta features vs each target class (0 or 1)","aff5ca13":"Now let us look at the important words used for classifying when target = 1.","0104b7f3":"## 9.5 The Coleman-Liau Index <br>\n* Returns the grade level of the text using the Coleman-Liau Formula. This is a grade formula in that a score of 9.3 means that a ninth grader would be able to read the document. <br>\n* The Coleman\u2013Liau index is calculated with the following formula: <br>\n![](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/cae44bbb96eaaca26e6aaf3b65c342f69f3d49ce) <br>\nL is the average number of letters per 100 words and S is the average number of sentences per 100 words. <br>\n* Read More : [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Coleman%E2%80%93Liau_index)","a2ddabfd":"## 4.1 Number of tweets according to location(top 20)","e2a4ab7d":"## Columns\n* id - a unique identifier for each tweet\n* text - the text of the tweet\n* location - the location the tweet was sent from (may be blank)\n* keyword - a particular keyword from the tweet (may be blank)\n* target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)","6de1b16a":"## 14.2 Generating tweets about not a real disaster","d408ffaa":"*  We can observe that about 43% of tweets in the dataframe is about real disaster.","90984e25":"* We can observe most of the tweets are from America region few are from Australia,India,Africa Etc..,","253c3e66":"## 9.4 Automated Readability Index <br>\n* Returns the ARI (Automated Readability Index) which outputs a number that approximates the grade level needed to comprehend the text.For example if the ARI is 6.5, then the grade level to comprehend the text is 6th to 7th grade.\n* Formula to calculate ARI: <br>\n![](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/878d1640d23781351133cad73bdf27bdf8bfe2fd) <br>\n* Read More: [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Automated_readability_index)","38dda4f9":"## 9.1 The Flesch Reading Ease formula <br>\n* In the Flesch reading-ease test, higher scores indicate material that is easier to read; lower numbers mark passages that are more difficult to read. The formula for the Flesch reading-ease score (FRES) test is\n![](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/bd4916e193d2f96fa3b74ee258aaa6fe242e110e)","5e9fcbb0":"# 9. Readability features ","0bea5b09":"## 4.2 Number of tweets according to location per class (0 or1)","3d88b6b2":"## <font color='blue'>In this notebook we will explore the given data in depth and build basic model.<\/font>\n","684240f1":"## 10.1 Latent Dirichlet Allocation (LDA) <br>\n* Latent Dirichlet Allocation (LDA) and LSA are based on the same underlying assumptions: the distributional hypothesis, (i.e. similar topics make use of similar words) and the statistical mixture hypothesis (i.e. documents talk about several topics) for which a statistical distribution can be determined. The purpose of LDA is mapping each document in our corpus to a set of topics which covers a good deal of the words in the document. <br>\n\n* What LDA does in order to map the documents to a list of topics is assign topics to arrangements of words, e.g. n-grams such as best player for a topic related to sports. This stems from the assumption that documents are written with arrangements of words and that those arrangements determine topics.LDA ignores syntactic information and treats documents as bags of words. It also assumes that all words in the document can be assigned a probability of belonging to a topic. That said, the goal of LDA is to determine the mixture of topics that a document contains. <br>\n\n* In other words, LDA assumes that topics and documents look like this: <br>\n![](https:\/\/pcdn.piiojs.com\/i\/kqctmw\/vw,1536,vh,0,r,1,pr,1.3,wp,1\/https%3A%2F%2Flh4.googleusercontent.com%2FyW-fEumGuN1mRPbcUG3K0br-7pGgNcFTMhv1flKg1foshZ7fzbUD7hDxifs9seBLJcnEBBAo-sCeO3zjjhCSduETRtdSwIkk5gDosxV4Ijo98NtibeknHBrPD2bH_AHampsy2lGM) <br>\n* And, when LDA models a new document, it works this way: <br>\n![](https:\/\/pcdn.piiojs.com\/i\/kqctmw\/vw,1536,vh,0,r,1,pr,1.3,wp,1\/https%3A%2F%2Flh3.googleusercontent.com%2FiCqGC2Yuyjb0HqlQcaVmAioCg3k56PQJXs7-pinKZwCiy00tL6ObLnVdJy6JRrGWRtnkunP6v94RRoxRlJsdJk00ydPKWnmSxr-zns4WJz4k7IHIMfbnKFSXb0tfvs3_09Txebjo) <br>\n\n\n","0e377f44":"# Thats all for now.<font color='red'>If you like this kernel please consider Upvoting it<\/font>.I welcome suggestions to improve this kernel further.<br>\n## Thank you \ud83d\ude0a,Happy learning. <br>","3edb272b":"## 8.1 Histogram Plots of number of words per each class (0 or 1)","826f2bac":"# Table of contents: <br>\n1. Importing necessary modules <br>\n2. Importing Dataframes <br>\n3. Target value distribution <br>\n4. Exploring Location column <br>\n   4.1   Number of tweets according to location(top 20) <br>\n   4.2   Number of tweets according to location per class (0 or1) <br>\n   4.3   Visualising number of tweets per location on maps <br>\n5. Word clouds of each class <br>\n6. Word Frequency plots per each class(0 or 1) <br>\n   6.1   Word Frequency <br>\n   6.2   Bigram Plots <br>\n   6.3   Trigram Plots <br>\n7. Creating Meta Features <br>\n   7.1   Plotting of meta features vs each target class (0 or 1) <br>\n8. Histogram Plots <br>\n   8.1   Histogram Plots of number of words per each class (0 or 1) <br>\n   8.2   Histogram Plots of number of characters per each class (0 or 1) <br>\n   8.3   Histogram Plots of number of punctuations per each class (0 or 1) <br>\n   8.4   Histogram plots of number of words in train and test sets <br>\n   8.5   Histogram plots of number of characters in train and test sets <br>\n   8.6   Histogram plots of number of punctuations in train and test sets <br>\n9. Readability features <br>\n   9.1   The Flesch Reading Ease formula <br>\n   9.2   The Flesch-Kincaid Grade Level <br>\n   9.3   The Fog Scale (Gunning FOG Formula) <br>\n   9.4   Automated Readability Index <br>\n   9.5   The Coleman-Liau Index <br>\n   9.6   Linsear Write Formula <br>\n   9.7   Dale-Chall Readability Score <br>\n   9.8  Readability Consensus based upon all the above tests <br>\n10. Topic Modelling <br>\n    10.1  Latent Dirichlet Allocation(LDA) <br>\n    10.2  Printing keywords <br>\n    10.3  Visualizing LDA results of not a real disaster tweets with pyLDAvis <br>\n    10.4  Visualizing LDA results of a real disaster tweets with pyLDAvis <br>\n11.  Tfidf Vectorization <br>\n12.  Building basic Logistic regression model<br>\n13.  Threshold value search for better score <br>\n14.  Generating random tweets using Markov Chains <br>\n     14.1  Building text model <br>\n     14.2  Generating tweets about not a real disaster <br>\n     14.3  Generating tweets about real disaster<br>","dfcd2e8f":"## Score - Difficulty <br>\n* 90-100 - Very Easy\n* 80-89 - Easy\n* 70-79 - Fairly Easy\n* 60-69 - Standard\n* 50-59 - Fairly Difficult\n* 30-49 - Difficult\n* 0-29 - Very Confusing\n\nRead More : [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Flesch%E2%80%93Kincaid_readability_tests)","151b8256":"# 2. Importing Dataframes","2e204ad0":"## 14.1 Building text model","520acec4":"##### References : This notebook was refered from [this](https:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-notebook-qiqc) notebook. And also readability features and topic modelling was taken from this [notebook](https:\/\/www.kaggle.com\/thebrownviking20\/analyzing-quora-for-the-insinceres)<br>\n##### Thanks to the author of the kernels [SRK](https:\/\/www.kaggle.com\/sudalairajkumar) and [Siddharth Yadav](https:\/\/www.kaggle.com\/thebrownviking20) <br>\n##### Other references and credits : [https:\/\/monkeylearn.com\/blog\/introduction-to-topic-modeling\/](https:\/\/monkeylearn.com\/blog\/introduction-to-topic-modeling\/) <br>","4a5ea791":"# 3. Target Value Distribution","d04a8918":"# 1. Importing necessary modules","11d45b88":"## 8.2 Histogram Plots of number of characters per each class (0 or 1)","cf45b517":"# 13. Threshold value search for better score","1869d4db":"## 9.6 Linsear Write Formula <br>\n* Returns the grade level of the text using the Coleman-Liau Formula. This is a grade formula in that a score of 9.3 means that a ninth grader would be able to read the document. <br>\n* Read More : [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Linsear_Write)","6689f38e":"#### Installing textstat package.","67f5ebe9":"* The graph says it all!!","65f44d7a":"## 8.3 Histogram Plots of number of punctuations per each class (0 or 1)","6c00a5fb":"## 9.2 The Flesch-Kincaid Grade Level <br>\n* These readability tests are used extensively in the field of education. The \"Flesch\u2013Kincaid Grade Level Formula\" instead presents a score as a U.S. grade level, making it easier for teachers, parents, librarians, and others to judge the readability level of various books and texts. It can also mean the number of years of education generally required to understand this text, relevant when the formula results in a number greater than 10. The grade level is calculated with the following formula: <br>\n![](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/8e68f5fc959d052d1123b85758065afecc4150c3) <br>\n\n* A score of 9.3 means that a ninth grader would be able to read the document. <br>\n* Read more: [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Flesch%E2%80%93Kincaid_readability_tests)","30d130e5":"### Readability is the ease with which a reader can understand a written text. In natural language processing, the readability of text depends on its content. It focuses on the words we choose, and how we put them into sentences and paragraphs for the readers to comprehend.","9240e60a":"## 14.3 Generating tweets about real disaster"}}