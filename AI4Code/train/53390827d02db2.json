{"cell_type":{"c28f41a1":"code","f6ca0671":"code","fb51b7ac":"code","ae9c4ad7":"code","6e1abe74":"code","6383400e":"code","4303d617":"code","8ae7eb74":"code","3e549adb":"code","6db36b67":"code","e5378bab":"code","72fa5e24":"markdown","674a1f66":"markdown","436acab0":"markdown","b3ccac78":"markdown","d26d4b2c":"markdown"},"source":{"c28f41a1":"# standard libs\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\nimport json\n\n# plotting libs\nimport seaborn as sns\n\n# geospatial libs\nfrom mpl_toolkits.basemap import Basemap\nfrom shapely.geometry import Polygon\nimport geopandas as gpd\nimport folium\nimport plotly.graph_objects as go\nimport plotly_express as px\n\n# set in line plotly \nfrom plotly.offline import init_notebook_mode;\ninit_notebook_mode(connected=True)\n\nprint(os.getcwd())","f6ca0671":"class cdp_kpi:\n    \"\"\"\n    import corporate climate change response data\n    \"\"\"\n    cc_df = pd.read_csv('..\/input\/cdp-unlocking-climate-solutions\/Corporations\/Corporations Responses\/Climate Change\/2019_Full_Climate_Change_Dataset.csv')\n    \"\"\"\n    import corporate water security response data\n    \"\"\"\n    ws_df = pd.read_csv('..\/input\/cdp-unlocking-climate-solutions\/Corporations\/Corporations Responses\/Water Security\/2019_Full_Water_Security_Dataset.csv')\n    # import cities response df\n    cities_df = pd.read_csv(\"..\/input\/cdp-unlocking-climate-solutions\/Cities\/Cities Responses\/2020_Full_Cities_Dataset.csv\")\n    # external data - import CDC social vulnerability index data - census tract level\n    svi_df = pd.read_csv(\"..\/input\/cdp-unlocking-climate-solutions\/Supplementary Data\/CDC Social Vulnerability Index 2018\/SVI2018_US.csv\")\n    \"\"\"\n    cities metadata - lat,lon locations for US cities\n    \"\"\"\n    cities_meta_df = pd.read_csv(\"..\/input\/cdp-unlocking-climate-solutions\/Supplementary Data\/Simple Maps US Cities Data\/uscities.csv\")\n    \"\"\"\n    cities metadata - CDP metadata on organisation HQ cities\n    \"\"\"\n    cities_cdpmeta_df = pd.read_csv(\"..\/input\/cdp-unlocking-climate-solutions\/Supplementary Data\/Locations of Corporations\/NA_HQ_public_data.csv\")\n    \n    def list_dedupe(self, x):\n        \"\"\"\n        Convert list to dict and back to list to dedupe\n\n        Parameters\n        ----------\n        x: list\n            Python list object\n\n        Returns\n        -------\n        dictionary:\n            dictionary object with duplicates removed\n\n        \"\"\"\n        return list(dict.fromkeys(x))\n    \n    def __init__(self):\n        \"\"\"\n        Extract city response data for question 6.2 Does your city collaborate in partnership with businesses in your city on sustainability projects?\n        Map cities to organisations who are headquartered within that city, using the NA_HQ_public_data.csv meta data file\n        \"\"\"\n        self.cities_6_2 = self.cities_df[self.cities_df['Question Number'] == '6.2']\\\n            .rename(columns={'Organization': 'City'})\n        self.cities_6_2['Response Answer'] = self.cities_6_2['Response Answer'].fillna('No Response')\n        # map dict to clean full state names to abbreviations\n        self.cities_cdpmeta_df['state'] = self.cities_cdpmeta_df['address_state'].map(self.us_state_abbrev)\n\n        # infill non-matched from dict\n        self.cities_cdpmeta_df['state'] = self.cities_cdpmeta_df['state'].fillna(self.cities_cdpmeta_df['address_state'])\n        self.cities_cdpmeta_df['state'] = self.cities_cdpmeta_df['state'].replace({'ALBERTA':'AB'})\n        self.cities_cdpmeta_df['address_city'] = self.cities_cdpmeta_df['address_city'].replace({'CALGARY':'Calgary'})\n        self.cities_cdpmeta_df= self.cities_cdpmeta_df.drop(columns=['address_state'])\n\n        # create joint city state variable\n        self.cities_cdpmeta_df['city_state'] = self.cities_cdpmeta_df['address_city'].str.cat(self.cities_cdpmeta_df['state'],sep=\", \")\n        #Summarise the cities metadata to count the number organisations (HQ) per city\n        self.cities_count = self.cities_cdpmeta_df[['organization', 'address_city', 'state', 'city_state']]\\\n        .groupby(['address_city', 'state', 'city_state']).count().\\\n            sort_values(by = ['organization'],ascending = False)\\\n                .reset_index()\\\n                    .rename(columns={'organization' : 'num_orgs'})\n        # convert indexes to columns'\n        self.cities_count.reset_index(inplace=True)\n        self.cities_count = self.cities_count.rename(columns = {'index':'city_id'})\n        self.cities_df.reset_index(inplace=True)\n        self.cities_df = self.cities_df.rename(columns = {'index':'city_org_id'})\n\n        # convert id and city label columns into lists\n        self.city_id_no = self.list_dedupe(self.cities_count['city_id'].tolist())\n        self.city_name = self.list_dedupe(self.cities_count['address_city'].tolist())\n\n        self.city_org_id_no = self.list_dedupe(self.cities_df['city_org_id'].tolist())\n        self.city_org_name = self.list_dedupe(self.cities_df['Organization'].tolist())\n\n        # remove added index column in cities df\n        self.cities_df.drop('city_org_id', inplace=True, axis=1)\n        self.cities_count.drop('city_id', inplace=True, axis=1)\n\n        # zip to join the lists and dict function to convert into dicts\n        self.city_dict = dict(zip(self.city_id_no, self.city_name))\n        self.city_org_dict = dict(zip(self.city_org_id_no, self.city_org_name))\n        \n        # compare dicts - matching when city name appears as a substring in the full city org name\n        self.city_names_df = pd.DataFrame(columns=['City ID No.','address_city', 'City Org ID No.','City Org', 'Match']) # initiate empty df\n\n        for ID, seq1 in self.city_dict.items():\n            for ID2, seq2 in self.city_org_dict.items():\n                m = re.search(seq1, seq2) # match string with regex search \n                if m:\n                    match = m.group()\n                    # Append rows in Empty Dataframe by adding dictionaries \n                    self.city_names_df = self.city_names_df.append({'City ID No.': ID, 'address_city': seq1, 'City Org ID No.': ID2, 'City Org': seq2, 'Match' : match}, ignore_index=True)\n\n        # subset for city to city org name matches\n        self.city_names_df = self.city_names_df.loc[:,['address_city','City Org']]\n        self.cities_count  = pd.merge(self.cities_count, self.city_names_df, on='address_city', how='left')\n        self.cities_6_2 = self.cities_6_2[['City', 'Response Answer']].rename(columns={'City' : 'City Org'})\n        self.cities_count = pd.merge(left=self.cities_count, right=self.cities_6_2, how='left', \n                                on ='City Org').rename(columns={'Response Answer' : 'Sustainability Project Collab.'})\n\n        self.cities_count['Sustainability Project Collab.'] = self.cities_count['Sustainability Project Collab.'].fillna('No Response')\n        self.cities_meta_df = self.cities_meta_df[['city', 'state_id', 'lat','lng']].rename(columns={'city' : 'address_city', 'state_id' : 'state'})\n        \n        # join coordinates to cities count\n        self.cities_count = pd.merge(left=self.cities_count, right=self.cities_meta_df, how='left', on=['address_city', 'state'])\n\n        # convert text response to question 6.2 to an integar encoding \n        resp_int_df = self.cities_count[[\"Sustainability Project Collab.\"]]\n        resp_int_df= resp_int_df.rename(columns={'Sustainability Project Collab.' : 'resp_int'})\n\n        labels = resp_int_df['resp_int'].unique().tolist()\n        mapping = dict( zip(labels,range(len(labels))) )\n        resp_int_df.replace({'resp_int': mapping},inplace=True)\n\n        resp_list = resp_int_df['resp_int'].tolist()\n        self.cities_count['resp_int'] = resp_list \n        \n        self.cc_2_4a = self.cc_df[self.cc_df['question_number'] == 'C2.4a']\n        cities_cdpmeta_join = self.cities_cdpmeta_df[[\"account_number\", 'survey_year', 'address_city']]\n        self.cc_2_4a = pd.merge(left=self.cc_2_4a, right=cities_cdpmeta_join,  left_on=['account_number','survey_year'], right_on = ['account_number','survey_year'])\n        \n    def City_SVI_Geo(self, city_svi_df, city, shapefile):\n        cc_nyc = self.cc_2_4a[(self.cc_2_4a['address_city'] == city)]\n        self.cities_6_2['City Org'] = self.cities_6_2['City Org'].replace({city +' City':city})\n        cc_nyc = pd.merge(left=cc_nyc, right= self.cities_6_2,  left_on=['address_city'], right_on = ['City Org']).rename(columns={'Response Answer' : 'sustain_collab'})\n\n        #e.g.'..\/input\/cdp-unlocking-climate-solutions\/Supplementary Data\/NYC CDP Census Tract Shapefiles\/nyu_2451_34505.shp'\n        # import shapefile of NYC census tracts\n        self.geodf = gpd.read_file(shapefile)\n\n        # join geospatial data to SVI unemployment rates ('E_UNEMP')\n        gdf_join = self.geodf[['tractid', 'geometry']].to_crs('+proj=robin')\n        nyc_join =  nyc_svi_df[['E_UNEMP', 'FIPS']]\n        gdf_join[\"tractid\"] = pd.to_numeric(self.geodf[\"tractid\"])\n        gdf_nyc = pd.merge(left=gdf_join, right=nyc_join, how='left', left_on='tractid', right_on = 'FIPS')\n        return gdf_nyc\n        \n    def City_CC_Resp(self, cc_city, city_svi_df, county, bb_df):\n        # subset for Bronx\n        #bb_df = city_svi_df[(city_svi_df.COUNTY == county)]\n\n        # join to city and climate change response data\n        print(cc_city.shape)\n        cc_city_temp = cc_city.rename(columns={'City Org' : 'City'})\n        city_df = pd.merge(cc_city_temp,bb_df,on='City',how='outer')\n        return city_df\n        \n    \n    # state abbreviation dictionary\n    us_state_abbrev = {\n        'Alabama': 'AL',\n        'Alaska': 'AK',\n        'American Samoa': 'AS',\n        'Arizona': 'AZ',\n        'Arkansas': 'AR',\n        'California': 'CA',\n        'Colorado': 'CO',\n        'Connecticut': 'CT',\n        'Delaware': 'DE',\n        'District of Columbia': 'DC',\n        'Florida': 'FL',\n        'Georgia': 'GA',\n        'Guam': 'GU',\n        'Hawaii': 'HI',\n        'Idaho': 'ID',\n        'Illinois': 'IL',\n        'Indiana': 'IN',\n        'Iowa': 'IA',\n        'Kansas': 'KS',\n        'Kentucky': 'KY',\n        'Louisiana': 'LA',\n        'Maine': 'ME',\n        'Maryland': 'MD',\n        'Massachusetts': 'MA',\n        'Michigan': 'MI',\n        'Minnesota': 'MN',\n        'Mississippi': 'MS',\n        'Missouri': 'MO',\n        'Montana': 'MT',\n        'Nebraska': 'NE',\n        'Nevada': 'NV',\n        'New Hampshire': 'NH',\n        'New Jersey': 'NJ',\n        'New Mexico': 'NM',\n        'New York': 'NY',\n        'North Carolina': 'NC',\n        'North Dakota': 'ND',\n        'Northern Mariana Islands':'MP',\n        'Ohio': 'OH',\n        'Oklahoma': 'OK',\n        'Oregon': 'OR',\n        'Pennsylvania': 'PA',\n        'Puerto Rico': 'PR',\n        'Rhode Island': 'RI',\n        'South Carolina': 'SC',\n        'South Dakota': 'SD',\n        'Tennessee': 'TN',\n        'Texas': 'TX',\n        'Utah': 'UT',\n        'Vermont': 'VT',\n        'Virgin Islands': 'VI',\n        'Virginia': 'VA',\n        'Washington': 'WA',\n        'West Virginia': 'WV',\n        'Wisconsin': 'WI',\n        'Wyoming': 'WY'\n    }\n","fb51b7ac":"c = cdp_kpi()","ae9c4ad7":"#verify data captured\nprint(\"c.cc_df.head()\")\nprint(c.cc_df.head())\nprint(\"c.cities_df.head()\")\nprint(c.cities_df.head())\nprint(\"c.svi_df.head()\")\nprint(c.svi_df.head())\nprint(\"c.cities_meta_df.head()\") \nprint(c.cities_meta_df.head()) \nprint(\"c.cities_cdpmeta_df.head()\")\nprint(c.cities_cdpmeta_df.head())\nprint(\"c.cities_6_2.head()\")\nprint(c.cities_6_2.head())\nprint(\"c.cities_count.head()\")\nprint(c.cities_count.head())\nprint(\"c.cc_2_4a.head()\")\nprint(c.cc_2_4a.head())","6e1abe74":"nyc_svi_df = c.svi_df[c.svi_df['STCNTY'].isin([36005, 36047, 36061, 36081, 36085])]\nnyc_svi_df['City'] = 'New York'\ncc_city = c.City_SVI_Geo(nyc_svi_df, \"Bronx\", '..\/input\/cdp-unlocking-climate-solutions\/Supplementary Data\/NYC CDP Census Tract Shapefiles\/nyu_2451_34505.shp')\nbb_df = nyc_svi_df[(nyc_svi_df.COUNTY == \"Bronx\")]\ncc_city = cc_city.rename(columns={'City Org' : 'City'})\n#print(cc_city)\nnyc_df = c.City_CC_Resp(cc_city,nyc_svi_df, \"Bronx\", bb_df)","6383400e":"\nprint(nyc_df.shape)","4303d617":"nyc_df.head()","8ae7eb74":"ws_df_4_1c = ws_df[ws_df['question_number'] == 'W4.1c']\nws_df_4_1c = ws_df_4_1c[ws_df_4_1c['response_value'].notnull()]\nws_df_4_1c.head()         ","3e549adb":"# pivot data\nws_df_4_1c_wide = ws_df_4_1c.pivot_table(index=['account_number', 'organization', 'row_number'],\n                                     columns='column_name', \n                                     values='response_value',\n                                     aggfunc=lambda x: ' '.join(x)).reset_index()\n# identify orgs with facilities within the Hudson river basin\nws_df_4_1c_wide = ws_df_4_1c_wide[ws_df_4_1c_wide['W4.1c_C2River basin'].str.contains('Hudson', na=False)]\nws_df_4_1c_wide.head()","6db36b67":"ws_df.head()","e5378bab":"sub.to_csv('submission.csv')","72fa5e24":"#### Water Responses\n\nIdentify organisations with facilities oeprating in the Hudson river basin, flagging companies who's operations may impact NYC's major fresh water resource\n","674a1f66":"Reshape data\n\n- Climate change and water response datasets are often presented in long format in the CDP datasets.\n- These data sets will become more useful when widened on the 'column_name' variable, enabling you to derive measurable metrics and KPIs from questionnaire response data","436acab0":"## Imports","b3ccac78":"### Modelling\n\n#### What next?\n\nSuggested analysis and modelling techniques that you can be apply as you tackle the [competitions problem statement](https:\/\/www.kaggle.com\/c\/cdp-unlocking-climate-solutions\/overview\/description).\n\nSuggestions below are **only** a guide. You are not limited to these approaches -  use your imagination and publically available data to tackle this challenge from any angle you can dream of! \n\n\n**NLP principles to investigate the social-environmental overlap between Corporations and Cities Climate Change 'Readiness'**\n\n- Utilise pythons NLP capabilities and tokenization approaches such as [Term Frequency\u2013inverse Document Frequency (TF-IDF)](https:\/\/medium.com\/analytics-vidhya\/getting-started-with-nlp-tokenization-document-term-matrix-tf-idf-2ea7d01f1942) (1) to construct a Document Term Matrix (DTM) from questionnaire responses, highlighting key terms in free text answers to aid in topic identification\n\n        - e.g. summarise city 'readiness' for climate change and the hazards they anticipate (Cities Question 2.1)\n        - e.g outline the future adaptations cities must implement to prepare for environmental challenges (City Question 3.0)\n        - e.g. find common topics in examples of colloboration between cities and business on sustainability projects (City Question 6.2a)\n\n- Apply [sentiment analysis](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC6267440\/) to detect whether a city sees opportunity (positive sentiment\/polarity) (Cities Question 6.0) or concern (negative sentiment\/polarity) (City Question 2.2) over future climate scenarios\n\n\n- Combine DTM and Sentiment analysis to build a combined KPI that incorporates measures of sentiment and susceptibility into one metric, identifying cities with high levels of percieved risk who may be open to colloboration with business as they foster climate resilience.\n\n        - e.g. Sentiment x Susceptibility  = Climate Risk Sensitivity Score\n\n\n\n**Social Accounting with Water Shadow Price Modeling**\n\nUsing external datasets and water-related risks identified by Corporations (Water Security Question W4.2), build a 'Shadow Price' of water for Corporations operating in a selection of North American cities. \n\n- A [shadow price](https:\/\/www.fir-pri-awards.org\/wp-content\/uploads\/MasterThesis_Chisem.pdf) (3) can attempt to account for the total cost of a Corporations water use, estimating all internal and external costs ,as well as exposure to water stress. \n\n- The shadow price coefficient can be combined within volumetric withdrawal data (Water Security Question W5.1a) to assign a Water Risk Cost per company, weighting corporate activties with a measure of the inersection between environmental risks and social impact.\n\n    - e.g. Water risk cost for Company  = Shadow price for Company  * Water withdrawal volume for Company \n\n\n\n**References**\n\n1. Mu\u00f1oz (2020). Getting started with NLP: Tokenization, Document-Term Matrix, TF-IDF. Medium. https:\/\/medium.com\/analytics-vidhya\/getting-started-with-nlp-tokenization-document-term-matrix-tf-idf-2ea7d01f1942\n\n2. Reyes-Menendez A, Saura JR, Alvarez-Alonso C. Understanding #WorldEnvironmentDay User Opinions in Twitter: A Topic-Based Sentiment Analysis Approach. Int J Environ Res Public Health. 2018;15(11):2537. Published 2018 Nov 13. doi:10.3390\/ijerph15112537. https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC6267440\/\n\n3. FIR-PRI. Portfolio Analysis Using Water Shadow Pricing: How Valuing Water Risk Can Reduce Carbon Emissions. https:\/\/www.fir-pri-awards.org\/wp-content\/uploads\/MasterThesis_Chisem.pdf","d26d4b2c":"# CDP Competition Starter Notebook\nExample data mapping, EDA and data wrangling pipeline to relate CDP Corporate response data to CDP Cities data and external data sets containing social equity data.\n\n#### Parameters\n\n#### Input\n\n**CDP Corporate Questionnaire response data sets**\n- **2019_Full_Climate_Change_Dataset.csv** = 2019 Climate Change publically disclosed questionnaire responses for North America\n- **2019_Full_Water_Security_Dataset.csv** = 2019 Water Security publically disclosed questionnaire responses for North America\n\n**CDP Cities Questionnaire response data sets**\n- **2020_-_Full_Cities_Dataset.csv** = Full 2020 Cities Questionnaire response data set\n\n**CDP Cities Meta data sets**\n- **NA_HQ_public_data.csv** = CDP curated Organisations metadata, mapping publically disclosed North American organisations to HQ city and state\n\n**External Non-CDP data sets**\n- **SVI2018_US.csv** = US Centers for Disease Control and Prevention (CDC) Social Vulnerability Index (SVI) Data for 2018 (*Census tract level*) - available publicly  bat https:\/\/www.atsdr.cdc.gov\/placeandhealth\/svi\/data_documentation_download.html\n- **SVI2018_US_COUNTY.csv** = US Centers for Disease Control and Prevention (CDC) Social Vulnerability Index (SVI) Data for 2018 (*County level*) - available publicly at https:\/\/www.atsdr.cdc.gov\/placeandhealth\/svi\/data_documentation_download.html\n- **uscities.csv** = metadata for United States cities and towns, with information such as populations size, median age and lat,lng location coordinates - available publicly at https:\/\/simplemaps.com\/data\/us-cities.\n\nSVI 2018 Documentation and Data Dictionary https:\/\/www.atsdr.cdc.gov\/placeandhealth\/svi\/documentation\/SVI_documentation_2018.html\n\n#### Output\n\nEDA and Visualisations to begin investigating the CDP competition data sets, environmental performance indicators and social-equity KPIs.\n"}}