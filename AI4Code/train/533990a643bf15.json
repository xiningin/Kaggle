{"cell_type":{"c6b593d3":"code","2fcbb6fa":"code","333ae569":"code","e5ea35e2":"code","226ecf73":"code","79b53870":"code","76cfbb21":"code","abda698a":"code","bd98ec0c":"code","8727ea04":"code","9c426c49":"code","08fc76a6":"code","e6f1fed1":"code","dd0848fd":"code","74fd60fb":"code","aa42a597":"code","e727e2ff":"code","54e6d67b":"code","29917403":"code","3ca1b8f9":"code","084a6dee":"code","b2c0585a":"markdown","07ac3443":"markdown","03ca5ac2":"markdown","d16ab334":"markdown","bfe359e7":"markdown","45f2879f":"markdown","616a2b79":"markdown","c7eea80e":"markdown"},"source":{"c6b593d3":"import os\nimport random\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nplt.rc('font', size=16)\nimport warnings\n\nwarnings.filterwarnings('ignore')\ntf.get_logger().setLevel('ERROR')\n\ntfk = tf.keras\ntfkl = tf.keras.layers\nprint(tf.__version__)","2fcbb6fa":"# Random seed for reproducibility\nseed = 42\n\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\ntf.random.set_seed(seed)\ntf.compat.v1.set_random_seed(seed)","333ae569":"# Utility function to create folders and callbacks for training\nfrom datetime import datetime\n\n\ndef create_folders_and_callbacks(model_name):\n    exps_dir = os.path.join('experiments')\n    if not os.path.exists(exps_dir):\n        os.makedirs(exps_dir)\n\n    now = datetime.now().strftime('%b%d_%H-%M-%S')\n\n    exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\n    if not os.path.exists(exp_dir):\n        os.makedirs(exp_dir)\n\n    callbacks = [\n        tfk.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=10, restore_best_weights=True),\n        tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', patience=5, factor=0.5, min_lr=1e-5)\n    ]\n\n    # Model checkpoint\n    # ----------------\n    ckpt_dir = os.path.join(exp_dir, 'ckpts')\n    if not os.path.exists(ckpt_dir):\n        os.makedirs(ckpt_dir)\n\n    ckpt_callback = tf.keras.callbacks.ModelCheckpoint(\n        filepath=os.path.join(ckpt_dir, 'cp'),\n        save_weights_only=False,\n        save_best_only=False)\n    callbacks.append(ckpt_callback)\n\n    # Visualize Learning on Tensorboard\n    # ---------------------------------\n    tb_dir = os.path.join(exp_dir, 'tb_logs')\n    if not os.path.exists(tb_dir):\n        os.makedirs(tb_dir)\n\n    tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir,\n                                                 profile_batch=0,\n                                                 histogram_freq=1)\n    callbacks.append(tb_callback)\n\n    return callbacks","e5ea35e2":"dataset = pd.read_csv('\/kaggle\/input\/multivariatetimeseries\/Training.csv')\nprint(dataset.shape)\ndataset.head()","226ecf73":"dataset.info()","79b53870":"def inspect_dataframe(df, columns):\n    figs, axs = plt.subplots(len(columns), 1, sharex=True, figsize=(17, 17))\n    for i, col in enumerate(columns):\n        axs[i].plot(df[col])\n        axs[i].set_title(col)\n    plt.show()\n\n\ninspect_dataframe(dataset, dataset.columns)","76cfbb21":"# Data normalization\n\nX_train_raw = dataset\nX_min = X_train_raw.min()\nX_max = X_train_raw.max()\nX_train_raw = (X_train_raw - X_min) \/ (X_max - X_min)\n\nprint(X_train_raw.shape)\nprint(dataset)\n\n# Plot dataset after normalization\ninspect_dataframe(X_train_raw, X_train_raw.columns)","abda698a":"window = 100\nstride = 5","bd98ec0c":"future = dataset[-window:]\nfuture = (future - X_min) \/ (X_max - X_min)\nfuture = np.expand_dims(future, axis=0)\nfuture.shape","8727ea04":"def build_sequences(df, p_target_labels=dataset.columns, p_window=100, p_stride=5, p_telescope=64):\n    # Sanity check to avoid runtime errors\n    assert p_window % p_stride == 0\n    ds = []\n    labels = []\n    temp_df = df.copy().values\n    temp_label = df[p_target_labels].copy().values\n    padding_len = len(df) % p_window\n\n    if padding_len != 0:\n        # Compute padding length\n        padding_len = p_window - len(df) % p_window\n        padding = np.zeros((padding_len, temp_df.shape[1]), dtype='float32')  # Change to float32\n        temp_df = np.concatenate((padding, df))\n        padding = np.zeros((padding_len, temp_label.shape[1]), dtype='float32')\n        temp_label = np.concatenate((padding, temp_label))\n        assert len(temp_df) % p_window == 0\n\n    for idx in np.arange(0, len(temp_df) - p_window - p_telescope, p_stride):\n        ds.append(temp_df[idx:idx + p_window])\n        labels.append(temp_label[idx + p_window:idx + p_window + p_telescope])\n\n    ds = np.array(ds)\n    labels = np.array(labels)\n    return ds, labels","9c426c49":"target_labels = dataset.columns\ntelescope = 64","08fc76a6":"X_train, y_train = build_sequences(X_train_raw, target_labels, window, stride, telescope)\n# X_test, y_test = build_sequences(X_test_raw, target_labels, window, stride, telescope)\nX_train.shape, y_train.shape","e6f1fed1":"def inspect_multivariate(X, y, columns, p_telescope, idx=None):\n    if idx is None:\n        idx = np.random.randint(0, len(X))\n\n    figs, axs = plt.subplots(len(columns), 1, sharex=True, figsize=(17, 17))\n    for i, col in enumerate(columns):\n        axs[i].plot(np.arange(len(X[0, :, i])), X[idx, :, i])\n        axs[i].scatter(np.arange(len(X[0, :, i]), len(X_train[0, :, i]) + p_telescope), y[idx, :, i], color='orange')\n        axs[i].set_title(col)\n        axs[i].set_ylim(0, 1)\n    plt.show()","dd0848fd":"inspect_multivariate(X_train, y_train, target_labels, telescope)","74fd60fb":"input_shape = X_train.shape[1:]\noutput_shape = y_train.shape[1:]\nbatch_size = 64\nepochs = 200","aa42a597":"import tensorflow.keras.backend as K\n\nclass SeqSelfAttention(tfkl.Layer):\n\n    ATTENTION_TYPE_ADD = 'additive'\n    ATTENTION_TYPE_MUL = 'multiplicative'\n\n    def __init__(self,\n                 units=32,\n                 attention_width=None,\n                 attention_type=ATTENTION_TYPE_ADD,\n                 return_attention=False,\n                 history_only=False,\n                 kernel_initializer='glorot_normal',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 bias_regularizer=None,\n                 kernel_constraint=None,\n                 bias_constraint=None,\n                 use_additive_bias=True,\n                 use_attention_bias=True,\n                 attention_activation=None,\n                 attention_regularizer_weight=0.0,\n                 **kwargs):\n        \"\"\"Layer initialization.\n        For additive attention, see: https:\/\/arxiv.org\/pdf\/1806.01264.pdf\n        :param units: The dimension of the vectors that used to calculate the attention weights.\n        :param attention_width: The width of local attention.\n        :param attention_type: 'additive' or 'multiplicative'.\n        :param return_attention: Whether to return the attention weights for visualization.\n        :param history_only: Only use historical pieces of data.\n        :param kernel_initializer: The initializer for weight matrices.\n        :param bias_initializer: The initializer for biases.\n        :param kernel_regularizer: The regularization for weight matrices.\n        :param bias_regularizer: The regularization for biases.\n        :param kernel_constraint: The constraint for weight matrices.\n        :param bias_constraint: The constraint for biases.\n        :param use_additive_bias: Whether to use bias while calculating the relevance of inputs features\n                                  in additive mode.\n        :param use_attention_bias: Whether to use bias while calculating the weights of attention.\n        :param attention_activation: The activation used for calculating the weights of attention.\n        :param attention_regularizer_weight: The weights of attention regularizer.\n        :param kwargs: Parameters for parent class.\n        \"\"\"\n        super(SeqSelfAttention, self).__init__(**kwargs)\n        self.supports_masking = True\n        self.units = units\n        self.attention_width = attention_width\n        self.attention_type = attention_type\n        self.return_attention = return_attention\n        self.history_only = history_only\n        if history_only and attention_width is None:\n            self.attention_width = int(1e9)\n\n        self.use_additive_bias = use_additive_bias\n        self.use_attention_bias = use_attention_bias\n        self.kernel_initializer = tfk.initializers.get(kernel_initializer)\n        self.bias_initializer = tfk.initializers.get(bias_initializer)\n        self.kernel_regularizer = tfk.regularizers.get(kernel_regularizer)\n        self.bias_regularizer = tfk.regularizers.get(bias_regularizer)\n        self.kernel_constraint = tfk.constraints.get(kernel_constraint)\n        self.bias_constraint = tfk.constraints.get(bias_constraint)\n        self.attention_activation = tfk.activations.get(attention_activation)\n        self.attention_regularizer_weight = attention_regularizer_weight\n        self._backend = K.backend()\n\n        if attention_type == SeqSelfAttention.ATTENTION_TYPE_ADD:\n            self.Wx, self.Wt, self.bh = None, None, None\n            self.Wa, self.ba = None, None\n        elif attention_type == SeqSelfAttention.ATTENTION_TYPE_MUL:\n            self.Wa, self.ba = None, None\n        else:\n            raise NotImplementedError('No implementation for attention type : ' + attention_type)\n\n    def get_config(self):\n        config = {\n            'units': self.units,\n            'attention_width': self.attention_width,\n            'attention_type': self.attention_type,\n            'return_attention': self.return_attention,\n            'history_only': self.history_only,\n            'use_additive_bias': self.use_additive_bias,\n            'use_attention_bias': self.use_attention_bias,\n            'kernel_initializer': tfk.initializers.serialize(self.kernel_initializer),\n            'bias_initializer': tfk.initializers.serialize(self.bias_initializer),\n            'kernel_regularizer': tfk.regularizers.serialize(self.kernel_regularizer),\n            'bias_regularizer': tfk.regularizers.serialize(self.bias_regularizer),\n            'kernel_constraint': tfk.constraints.serialize(self.kernel_constraint),\n            'bias_constraint': tfk.constraints.serialize(self.bias_constraint),\n            'attention_activation': tfk.activations.serialize(self.attention_activation),\n            'attention_regularizer_weight': self.attention_regularizer_weight,\n        }\n        base_config = super(SeqSelfAttention, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    def build(self, input_shape):\n        if self.attention_type == SeqSelfAttention.ATTENTION_TYPE_ADD:\n            self._build_additive_attention(input_shape)\n        elif self.attention_type == SeqSelfAttention.ATTENTION_TYPE_MUL:\n            self._build_multiplicative_attention(input_shape)\n        super(SeqSelfAttention, self).build(input_shape)\n\n    def _build_additive_attention(self, input_shape):\n        feature_dim = int(input_shape[2])\n\n        self.Wt = self.add_weight(shape=(feature_dim, self.units),\n                                  name='{}_Add_Wt'.format(self.name),\n                                  initializer=self.kernel_initializer,\n                                  regularizer=self.kernel_regularizer,\n                                  constraint=self.kernel_constraint)\n        self.Wx = self.add_weight(shape=(feature_dim, self.units),\n                                  name='{}_Add_Wx'.format(self.name),\n                                  initializer=self.kernel_initializer,\n                                  regularizer=self.kernel_regularizer,\n                                  constraint=self.kernel_constraint)\n        if self.use_additive_bias:\n            self.bh = self.add_weight(shape=(self.units,),\n                                      name='{}_Add_bh'.format(self.name),\n                                      initializer=self.bias_initializer,\n                                      regularizer=self.bias_regularizer,\n                                      constraint=self.bias_constraint)\n\n        self.Wa = self.add_weight(shape=(self.units, 1),\n                                  name='{}_Add_Wa'.format(self.name),\n                                  initializer=self.kernel_initializer,\n                                  regularizer=self.kernel_regularizer,\n                                  constraint=self.kernel_constraint)\n        if self.use_attention_bias:\n            self.ba = self.add_weight(shape=(1,),\n                                      name='{}_Add_ba'.format(self.name),\n                                      initializer=self.bias_initializer,\n                                      regularizer=self.bias_regularizer,\n                                      constraint=self.bias_constraint)\n\n    def _build_multiplicative_attention(self, input_shape):\n        feature_dim = int(input_shape[2])\n\n        self.Wa = self.add_weight(shape=(feature_dim, feature_dim),\n                                  name='{}_Mul_Wa'.format(self.name),\n                                  initializer=self.kernel_initializer,\n                                  regularizer=self.kernel_regularizer,\n                                  constraint=self.kernel_constraint)\n        if self.use_attention_bias:\n            self.ba = self.add_weight(shape=(1,),\n                                      name='{}_Mul_ba'.format(self.name),\n                                      initializer=self.bias_initializer,\n                                      regularizer=self.bias_regularizer,\n                                      constraint=self.bias_constraint)\n\n    def call(self, inputs, mask=None, **kwargs):\n        input_len = K.shape(inputs)[1]\n\n        if self.attention_type == SeqSelfAttention.ATTENTION_TYPE_ADD:\n            e = self._call_additive_emission(inputs)\n        elif self.attention_type == SeqSelfAttention.ATTENTION_TYPE_MUL:\n            e = self._call_multiplicative_emission(inputs)\n\n        if self.attention_activation is not None:\n            e = self.attention_activation(e)\n        if self.attention_width is not None:\n            if self.history_only:\n                lower = K.arange(0, input_len) - (self.attention_width - 1)\n            else:\n                lower = K.arange(0, input_len) - self.attention_width \/\/ 2\n            lower = K.expand_dims(lower, axis=-1)\n            upper = lower + self.attention_width\n            indices = K.expand_dims(K.arange(0, input_len), axis=0)\n            e -= 10000.0 * (1.0 - K.cast(lower <= indices, K.floatx()) * K.cast(indices < upper, K.floatx()))\n        if mask is not None:\n            mask = K.expand_dims(K.cast(mask, K.floatx()), axis=-1)\n            e -= 10000.0 * ((1.0 - mask) * (1.0 - K.permute_dimensions(mask, (0, 2, 1))))\n\n        # a_{t} = \\text{softmax}(e_t)\n        e = K.exp(e - K.max(e, axis=-1, keepdims=True))\n        a = e \/ K.sum(e, axis=-1, keepdims=True)\n\n        # l_t = \\sum_{t'} a_{t, t'} x_{t'}\n        v = K.batch_dot(a, inputs)\n        if self.attention_regularizer_weight > 0.0:\n            self.add_loss(self._attention_regularizer(a))\n\n        if self.return_attention:\n            return [v, a]\n        return v\n\n    def _call_additive_emission(self, inputs):\n        input_shape = K.shape(inputs)\n        batch_size, input_len = input_shape[0], input_shape[1]\n\n        # h_{t, t'} = \\tanh(x_t^T W_t + x_{t'}^T W_x + b_h)\n        q = K.expand_dims(K.dot(inputs, self.Wt), 2)\n        k = K.expand_dims(K.dot(inputs, self.Wx), 1)\n        if self.use_additive_bias:\n            h = K.tanh(q + k + self.bh)\n        else:\n            h = K.tanh(q + k)\n\n        # e_{t, t'} = W_a h_{t, t'} + b_a\n        if self.use_attention_bias:\n            e = K.reshape(K.dot(h, self.Wa) + self.ba, (batch_size, input_len, input_len))\n        else:\n            e = K.reshape(K.dot(h, self.Wa), (batch_size, input_len, input_len))\n        return e\n\n    def _call_multiplicative_emission(self, inputs):\n        # e_{t, t'} = x_t^T W_a x_{t'} + b_a\n        e = K.batch_dot(K.dot(inputs, self.Wa), K.permute_dimensions(inputs, (0, 2, 1)))\n        if self.use_attention_bias:\n            e += self.ba[0]\n        return e\n\n    def compute_output_shape(self, input_shape):\n        output_shape = input_shape\n        if self.return_attention:\n            attention_shape = (input_shape[0], output_shape[1], input_shape[1])\n            return [output_shape, attention_shape]\n        return output_shape\n\n    def compute_mask(self, inputs, mask=None):\n        if self.return_attention:\n            return [mask, None]\n        return mask\n\n    def _attention_regularizer(self, attention):\n        batch_size = K.cast(K.shape(attention)[0], K.floatx())\n        input_len = K.shape(attention)[-1]\n        indices = K.expand_dims(K.arange(0, input_len), axis=0)\n        diagonal = K.expand_dims(K.arange(0, input_len), axis=-1)\n        eye = K.cast(K.equal(indices, diagonal), K.floatx())\n        return self.attention_regularizer_weight * K.sum(K.square(K.batch_dot(\n            attention,\n            K.permute_dimensions(attention, (0, 2, 1))) - eye)) \/ batch_size\n\n    @staticmethod\n    def get_custom_objects():\n        return {'SeqSelfAttention': SeqSelfAttention}","e727e2ff":"def build_ConvBiLSTM_AR_with_SelfAttention_model(p_input_shape, p_output_shape):\n    # Build the neural network layer by layer\n    input_layer = tfkl.Input(shape=p_input_shape, name='Input')\n\n    convlstm = tfkl.Bidirectional(tfkl.LSTM(64, return_sequences=True))(input_layer)\n    att = SeqSelfAttention(attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n                       kernel_regularizer=tfk.regularizers.l2(1e-4),\n                       bias_regularizer=tfk.regularizers.l1(1e-4),\n                       attention_regularizer_weight=1e-4,\n                       name='Attention1')(convlstm)\n    convlstm = tfkl.Conv1D(128, 3, padding='same', activation='relu')(att)\n    convlstm = tfkl.MaxPool1D()(convlstm)\n    convlstm = tfkl.Bidirectional(tfkl.LSTM(32, return_sequences=True))(convlstm)\n    att = SeqSelfAttention(attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n                       kernel_regularizer=tfk.regularizers.l2(1e-4),\n                       bias_regularizer=tfk.regularizers.l1(1e-4),\n                       attention_regularizer_weight=1e-4,\n                       name='Attention2')(convlstm)\n    convlstm = tfkl.Conv1D(256, 3, padding='same', activation='relu')(att)\n    convlstm = tfkl.GlobalAveragePooling1D()(convlstm)\n    convlstm = tfkl.Dropout(.5)(convlstm)\n\n    # In order to predict the next values for more than one channel,\n    # we can use a Dense layer with a number given by telescope*num_channels,\n    # followed by a Reshape layer to obtain a tensor of dimension \n    # [None, telescope, num_channels]\n    dense = tfkl.Dense(p_output_shape[-1] * p_output_shape[-2], activation='relu')(convlstm)\n    output_layer = tfkl.Reshape((p_output_shape[-2], p_output_shape[-1]))(dense)\n    output_layer = tfkl.Conv1D(p_output_shape[-1], 1, padding='same')(output_layer)\n\n    # Connect input and output through the Model class\n    built_model = tfk.Model(inputs=input_layer, outputs=output_layer, name='ConvBiLSTM_AR_with_SelfAttention_model')\n\n    # Compile the model\n    built_model.compile(loss=tfk.losses.MeanSquaredError(), optimizer=tfk.optimizers.Adam(),\n                        metrics=[tf.keras.metrics.RootMeanSquaredError()])\n\n    # Return the model\n    return built_model","54e6d67b":"model = build_ConvBiLSTM_AR_with_SelfAttention_model(input_shape, output_shape)\nmodel.summary()\ntfk.utils.plot_model(model, expand_nested=True)","29917403":"# Train the model\ncallbacks = create_folders_and_callbacks(model_name='ConvBiLSTM_AR_with_SelfAttention_w100_s5_tel64')\n\nhistory = model.fit(\n    x=X_train,\n    y=y_train,\n    batch_size=batch_size,\n    epochs=epochs,\n    validation_split=.1,\n    callbacks=callbacks\n).history","3ca1b8f9":"model.save('ConvBiLSTM_AR_with_SelfAttention_w100_s5_tel64')\n# model = tfk.models.load_model('ConvBiLSTM_AR_with_SelfAttention_w100_s5_tel64')","084a6dee":"best_epoch = np.argmin(history['val_loss'])\nplt.figure(figsize=(17, 4))\nplt.plot(history['loss'], label='Training loss', alpha=.8, color='#ff7f0e')\nplt.plot(history['val_loss'], label='Validation loss', alpha=.9, color='#5a9aa5')\nplt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\nplt.title('Mean Squared Error (Loss)')\nplt.legend()\nplt.grid(alpha=.3)\nplt.show()\n\nplt.figure(figsize=(17, 4))\nplt.plot(history['root_mean_squared_error'], label='Training RMSE', alpha=.8, color='#ff7f0e')\nplt.plot(history['val_root_mean_squared_error'], label='Validation RMSE', alpha=.9, color='#5a9aa5')\nplt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\nplt.title('RMSE')\nplt.legend()\nplt.grid(alpha=.3)\nplt.show()\n\nplt.figure(figsize=(18, 3))\nplt.plot(history['lr'], label='Learning Rate', alpha=.8, color='#ff7f0e')\nplt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\nplt.legend()\nplt.grid(alpha=.3)\nplt.show()","b2c0585a":"### Autoregressive Forecasting","07ac3443":"# Assignment 2 | Image Classification, December 2021\n\n- [Artificial Neural Networks and Deep Learning 2021 - Homework 2](https:\/\/codalab.lisn.upsaclay.fr\/competitions\/621)\n\n## Three convolutioneers\n\n- *Aleksandra Krajnovic*\n- *Iva Milojkovic*\n- *Mariusz Wi\u015bniewski*","03ca5ac2":"### Utility function to create folders and callbacks for training","d16ab334":"Load the dataset","bfe359e7":"### Exploration Data Analysis (EDA)\n","45f2879f":"### Import libraries","616a2b79":"### Set seed for reproducibility","c7eea80e":"Normalization"}}