{"cell_type":{"ff0b4cf7":"code","57ab58ad":"code","fe2263b0":"code","00862319":"code","651747de":"code","585c96b5":"code","3e619c60":"code","ebfdc753":"code","c82951ed":"code","09ef446d":"code","7ba7cb6b":"code","3a226cb4":"code","40fa40ee":"code","581ed0b3":"code","4e5ed2ef":"code","f69cdf1f":"code","c2e3ac4f":"code","4803d045":"code","715bd060":"code","e2d9178b":"code","6fabbc91":"code","87a374e9":"code","8478fd78":"markdown","1fb622a2":"markdown","49aa3f5b":"markdown","449a6275":"markdown","60bc7d68":"markdown","0441134b":"markdown","f0821790":"markdown","5618c235":"markdown","9e2d5420":"markdown","e9e68a0c":"markdown","7c13e3cb":"markdown","ace31b58":"markdown","5ab8e843":"markdown"},"source":{"ff0b4cf7":"import pandas as pd\nimport seaborn as sb\nimport numpy as np\nimport xgboost\nimport matplotlib.pyplot as plt\nfrom sklearn import *","57ab58ad":"df = pd.read_csv(\"..\/input\/sloan-digital-sky-survey-dr16\/Skyserver_12_30_2019 4_49_58 PM.csv\")","fe2263b0":"df.head()","00862319":"df = df.rename(columns={'ra': 'r_ascension', 'dec': 'declination', 'u': 'u_band',\n                        'g': 'g_band', 'r': 'r_band', 'i': 'i_band', 'z': 'z_band',\n                        'camcol': 'camera_col', 'class': 'label'})\n\ndf = df.replace(to_replace=\"QSO\", value=\"QUASAR\")\n\nprint(df.label.value_counts())","651747de":"print(\"Distinct objid: %d\" %len(df.objid.unique()))\nprint(\"Distinct specobjid: %d\" %len(df.specobjid.unique()))\n\ndf = df.drop(columns=[\"objid\", \"specobjid\"])","585c96b5":"df['label'] = pd.Categorical(df['label'],\n                             categories = ['STAR', 'GALAXY', 'QUASAR'])","3e619c60":"summary = pd.DataFrame()\nsummary['Name'] = df.columns\nsummary['Type'] = df.dtypes.values\nsummary['NA'] = df.isna().sum().values\nprint(summary.to_string(index=False))","ebfdc753":"melted = df.melt(id_vars=['label'],\n                 value_vars=['r_ascension','declination','u_band','g_band','r_band','i_band','z_band','redshift'])\ng = sb.FacetGrid(melted, col='variable', col_wrap=4, hue='label',\n                 margin_titles=True, sharex=False, sharey=False)\ng = g.map(sb.kdeplot, 'value', shade=True)\ng = g.add_legend()","c82951ed":"g = sb.FacetGrid(df, col='label', hue='label', margin_titles=True, sharex=False, sharey=False)\ng = g.map(sb.kdeplot, 'redshift', shade=True)","09ef446d":"melted = df.melt(id_vars=['label'],\n                 value_vars=['run','rerun','camera_col','field','plate','mjd','fiberid'])\ng = sb.FacetGrid(melted, col='variable', col_wrap=4, hue='label',\n                 margin_titles=True, sharex=False, sharey=False)\ng = g.map(sb.kdeplot, 'value', shade=True)\ng = g.add_legend()","7ba7cb6b":"df = df.drop(columns=[\"run\", \"rerun\", \"camera_col\", \"field\", \"plate\", \"mjd\", \"fiberid\"])","3a226cb4":"columns = ['r_ascension','declination','u_band','g_band','r_band','i_band','z_band','redshift']","40fa40ee":"corr = df.corr()\ncormap = sb.heatmap(corr, mask=np.triu(np.ones_like(corr, dtype=np.bool)), cmap=sb.diverging_palette(220, 10, as_cmap=True))","581ed0b3":"x = df.loc[:, df.columns != 'label']\ny = df.loc[:,'label'].to_numpy()","4e5ed2ef":"x_train, x_test, y_train, y_test = model_selection.train_test_split(x, y, test_size=0.1, random_state=1, stratify=y)","f69cdf1f":"univariate_selection = feature_selection.SelectKBest(k='all').fit(x_train, y_train)\nunivariate_scores = dict(zip(columns, univariate_selection.scores_))\nsorted(univariate_scores.items(), key=lambda t: t[1])","c2e3ac4f":"columns = ['u_band', 'g_band', 'r_band', 'i_band', 'z_band', 'redshift']\nx_train = x_train.loc[:, columns]\nx_test = x_test.loc[:, columns]","4803d045":"scaler = preprocessing.StandardScaler().fit(x_train)\nx_train = scaler.transform(x_train)\nx_test = scaler.transform(x_test)","715bd060":"model = xgboost.XGBClassifier(booster='gbtree', max_depth=10,\n                                    learning_rate=0.6, reg_lambda=2,\n                                    n_estimators=400).fit(x_train, y_train)\npredictions = model.predict(x_test)","e2d9178b":"def plot_confusion_matrix(cm, labels):\n    display_labels = labels\n    display = metrics.ConfusionMatrixDisplay(confusion_matrix=cm,\n                                             display_labels=display_labels)\n    return display.plot(include_values=True,\n                        cmap='viridis', ax=None, xticks_rotation='horizontal',\n                        values_format=None)","6fabbc91":"print(\"f1-score:  %.3f\" %metrics.f1_score(y_test, predictions, average='weighted'))\nprint(\"Balanced accuracy:  %.3f\" %metrics.balanced_accuracy_score(y_test, predictions))\n\ncm = plot_confusion_matrix(metrics.confusion_matrix(y_test, predictions), labels=[\"GALAXY\",\"QUASAR\",\"STAR\"])","87a374e9":"ax = xgboost.plot_importance(model)\norder = [int(i.get_text()[1:]) for i in ax.get_yticklabels()]\nax.set_yticklabels(np.array(columns)[order])\n\nplt.show()","8478fd78":"## Data loading\n\nLets' first set up libraries and load data.","1fb622a2":"## Exploratory data analysis\n\nNow it is time to visual inspect the data. I am going to plot the column distribution by grouping by each class label.","49aa3f5b":"As far as physical features are concerned, we can note a pretty much similar distribution across the classes for <i>r_ascension<\/i> and <i>declination<\/i>, on the other hand we can note distinct distributions in each other other feature with particular regard to the <i>quasar<\/i> class showing very uniques patterns. <i>redshift<\/i> is a special case that needed the three classes to be displayed separately because of the spiked distribution of <i>star<\/i> class. All in all, I would guess that <i>r_ascension<\/i> and <i>declination<\/i> will not be very significant in classification, <i>band<\/i> features will be more important especially to discriminate <i>quasar<\/i> and <i>not quasar<\/i>, and lastly <i>redshift<\/i> will be the most significative one. This is also logical since those two features are not related to physical properties but rather to their location in the sky vault.\n\nTalking about acquisition features, they all pretty much shows the same pattern accross all of the classes except <i>mjd<\/i> which is the acquisition date that doesn't have any logical meaning in classification. In addition, <i>rerun<\/i> raised some warnings saying that data must have a variance, meaning that it shows the same value throughout the samples. All in all, even if some of these features may have some correlation I'm not going to keep them because these are not physical properties and any classification ability is probably just due to statistical fluctuations. If I kept them I might even get better result but they might be biased for this dataset and may not be good for unseen data.","449a6275":"## Model selection\n\nI ran many cross-validation steps on my machine with different algorithms that I won't be showing here for perfomance reasons. I found out that all algorithms perfomed really good (accuracy greater than 97%), but the one that outclass them all was XGBoost. I also tried voting ensembles but the overall model performed worse than the single XGBoost. Reason for this is probably that most of the entries could be perfectly classified by each of the models but the remaining were misclassified by the majority of the others estimators making the voting ensemble perform worse than XGBoost.\nI finally ran a hard tuning of hyperparameters in order to get the most out of XGBoost.","60bc7d68":"## Final model evalutation","0441134b":"It can be noticed that there are two different id columns due to the <i>join<\/i> operation to create this table.","f0821790":"As expected, the two results are coherent with my previous predictions, namely, <i>r_ascension<\/i> and <i>declination<\/i> not very useful for classification purposes due to the same inter-class distribution.","5618c235":"We can see that there are many physical features that will probably be used for classification, but also some features concerning the process of acquisition which might be discarded.\n\nLet's first do some renaming, as a matter of personal preference.","9e2d5420":"As final step columns will be normalised in standard scale since many classification algorithms prefer to have columns with the same scale.","e9e68a0c":"Some IDs in the first table are not distinct whereas IDs in the second are all unique. Reason for this is not given, but I am assuming that every row is a different and unique sample and so I can discard those columns without any problem.\n\nConvert then label column to categorical and heck for any missing values.","7c13e3cb":"## Introduction\n\nThe Sloan Digital Sky Survey or SDSS is a multi-spectral imaging map using at Apache Point Observatory in New Mexico. It is the most detailed three-dimensional map of the universe ever made, with multi-color images of one third of the sky, and spectra for more than three million astronomical objects. Data are available at its website and can be accessed via <i>SQL<\/i> query [1][sdss] [2][sdss_wiki].\n\nData used in this project are results from a query which joins two tables: <i>PhotoObj<\/i>, which contains photometric data, and <i>SpecObj<\/i>, which contains spectral data. Data can be retrieved using SkyServer SQL Search with the command provided in the project description. This query does a table JOIN between the imaging (PhotoObj) and spectra (SpecObj) tables and includes the necessary columns in the SELECT to upload the results to the SAS (Science Archive Server) for FITS file retrieval [3][kaggle]. \n\n[sdss]: www.sdss.org\n[sdss_wiki]: en.wikipedia.org\/wiki\/Sloan_Digital_Sky_Survey\n[kaggle]: www.kaggle.com\/muhakabartay\/sloan-digital-sky-survey-dr16","ace31b58":"# Sloan digital sky survey\n\n## Space bodies classification\n\n***Francesco Pudda, 21\/10\/2020***","5ab8e843":"## Feature engineering\n\nNow it is necessary to select the best features for classification. I might arbitrarly choose the ones I consider the most likely best, but I prefer to use statistical tools to help me decide. I will start by plotting the correlation matrix to get a general idea of features correlation and then move on to univariate filter selection methods and a recursive feature elimination algorithm."}}