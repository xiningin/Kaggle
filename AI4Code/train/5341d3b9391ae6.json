{"cell_type":{"4456f67b":"code","eca15637":"code","7696e19a":"code","86c6192d":"code","5b267133":"code","4654899f":"code","337d34e2":"code","6d3a6796":"code","a08b969d":"code","76cae77d":"code","fe30d5de":"code","c58d4a08":"code","37a74645":"code","1226376b":"code","c0415b90":"code","c7da0d05":"code","b25c74a3":"code","5e251e6e":"code","fa9338dc":"code","07521b35":"code","fa885bd6":"code","6b14b2ed":"code","8f56e12d":"code","4ae94487":"code","feaa47be":"code","55b53795":"code","8c314fcd":"code","f91eb2a0":"code","edd2e8f1":"code","eaf203fa":"code","4196fda1":"code","f73f769f":"code","f1de629b":"code","fcfa3637":"markdown","ce5886f7":"markdown","456f0f5b":"markdown","5df131c0":"markdown","f9c5a7ec":"markdown","51bb9206":"markdown","b9199b1d":"markdown","f72bc0e2":"markdown","2c190d3b":"markdown","6ca44150":"markdown","719fe660":"markdown","c741290a":"markdown","52be7f58":"markdown"},"source":{"4456f67b":"import pandas as pd\nimport numpy as np\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nimport plotly\nimport plotly.plotly as py\nimport plotly.figure_factory as ff\n\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n\nimport math\n%matplotlib inline\nimport seaborn as sns\n\ninit_notebook_mode(connected=True)\n\nimport warnings\nfrom collections import Counter\nwarnings.filterwarnings('ignore')\n\nfrom sklearn import svm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, roc_auc_score ,roc_curve,auc\nfrom sklearn.model_selection import StratifiedKFold,GridSearchCV\nimport missingno as mssno\nseed =45\n% matplotlib inline","eca15637":"#importando dados\n#fonte: https:\/\/www.kaggle.com\/c\/porto-seguro-safe-driver-prediction\/data\ntest = pd.read_csv('..\/input\/test.csv', header=0)\ntrain = pd.read_csv('..\/input\/train.csv', header=0)","7696e19a":"train.head()","86c6192d":"#Removendo registros duplicados do test\nprint('Antes:', test.shape)\ntest.drop_duplicates()\nprint('Depois:', test.shape)","5b267133":"#Removendo registros duplicados do train\nprint('Antes:', train.shape)\ntrain.drop_duplicates()\nprint('Depois:', train.shape)","4654899f":"#An\u00e1lise dos conjuntos de dados para observar a distribui\u00e7\u00e3o dos atributos\ndisplay(train.describe())\ndisplay(test.describe())","337d34e2":"#Cria\u00e7\u00e3o dos Metadados do conjunto de treino\ndata = []\nfor f in train.columns:\n    # definindo o uso (entre r\u00f3tulo, id e atributos)\n    if f == 'target':\n        role = 'target' # r\u00f3tulo\n    elif f == 'id':\n        role = 'id'\n    else:\n        role = 'input' # atributos\n         \n    # definindo o tipo do dado\n    if 'bin' in f or f == 'target':\n        level = 'binary'\n    elif 'cat' in f or f == 'id':\n        level = 'nominal'\n    elif train[f].dtype == float:\n        level = 'interval'\n    elif train[f].dtype == int:\n        level = 'ordinal'\n        \n    # mantem keep como verdadeiro pra tudo, exceto id\n    keep = True\n    if f == 'id':\n        keep = False\n    \n    # cria o tipo de dado\n    dtype = train[f].dtype\n    \n    # cria dicion\u00e1rio de metadados\n    f_dict = {\n        'varname': f,\n        'role': role,\n        'level': level,\n        'keep': keep,\n        'dtype': dtype\n    }\n    data.append(f_dict)\n    \nmeta = pd.DataFrame(data, columns=['varname', 'role', 'level', 'keep', 'dtype'])\nmeta.set_index('varname', inplace=True)","6d3a6796":"#Visualiza\u00e7\u00e3o dos Metadados do conjunto de treino\nmeta","a08b969d":"#Contagem dos atributos por tipo de uso e dado\npd.DataFrame({'count' : meta.groupby(['role', 'level'])['role'].size()}).reset_index()","76cae77d":"df_barras = train['target'].value_counts().reset_index(name='count')\ndf_barras","fe30d5de":"#Como os dados do dataset est\u00e3o balanceados\nx = df_barras['index']\ny = df_barras['count']\n\ntrace1 = go.Bar(\n    x= x,\n    y=y,\n    text=y,\n    name='Distibui\u00e7\u00e3o do Target',\n    textposition = 'auto',\n    marker=dict(\n        color='rgb(158,202,225)',\n        line=dict(\n            color='rgb(8,48,107)',\n            width=1.5),\n        ),\n    opacity=0.6\n)\n\ndata = [trace1]\nplotly.offline.iplot(data, filename='grouped-bar-direct-labels')","c58d4a08":"arr = df_barras['index'].values\nsoma = df_barras['count'].values","37a74645":"#Como percebido o dataset tem um grande desbalancemente do target\ncolors = ['rgb(158,202,225)', 'rgb(8,48,107)']\ntrace = go.Pie(labels=arr, values=soma, marker=dict(colors=colors))\n\ndata = [trace]\nplotly.offline.iplot(data, filename='basic_pie_chart')","1226376b":"#Verificar valores faltantes\natributos_missing = []\n\nfor f in train.columns:\n    missings = train[train[f] == -1][f].count()\n    if missings > 0:\n        atributos_missing.append(f)\n        missings_perc = missings\/train.shape[0]\n        \n        print('Atributo {} tem {} amostras ({:.2%}) com valores faltantes'.format(f, missings, missings_perc))\n        \nprint('No total, h\u00e1 {} atributos com valores faltantes'.format(len(atributos_missing)))","c0415b90":"# removendo ps_car_03_cat e ps_car_05_cat que tem muitos valores faltantes\nvars_to_drop = ['ps_car_03_cat', 'ps_car_05_cat']\ntrain = train.drop(vars_to_drop, axis=1)\ntest = test.drop(vars_to_drop, axis=1)\nmeta.loc[(vars_to_drop),'keep'] = False  # atualiza os metadados para ter como refer\u00eancia (processar o test depois)","c7da0d05":"# Preenchendo dados faltantes dos atributos com valores faltantes\nfrom sklearn.preprocessing import Imputer\n\nmedia_imp = Imputer(missing_values=-1, strategy='mean', axis=0)\nmoda_imp = Imputer(missing_values=-1, strategy='most_frequent', axis=0)\ntrain['ps_reg_03'] = media_imp.fit_transform(train[['ps_reg_03']]).ravel()\ntrain['ps_car_12'] = media_imp.fit_transform(train[['ps_car_12']]).ravel()\ntrain['ps_car_14'] = media_imp.fit_transform(train[['ps_car_14']]).ravel()\ntrain['ps_car_11'] = moda_imp.fit_transform(train[['ps_car_11']]).ravel()\n\ntest['ps_reg_03'] = media_imp.fit_transform(test[['ps_reg_03']]).ravel()\ntest['ps_car_12'] = media_imp.fit_transform(test[['ps_car_12']]).ravel()\ntest['ps_car_14'] = media_imp.fit_transform(test[['ps_car_14']]).ravel()\ntest['ps_car_11'] = moda_imp.fit_transform(test[['ps_car_11']]).ravel()","b25c74a3":"# Separando o arquivo do treino entre float e int\ntrain_float = train.select_dtypes(include=['float64'])\ntrain_int = train.select_dtypes(include=['int64'])\nCounter(train.dtypes.values)","5e251e6e":"# Verificando a correla\u00e7\u00e3o entre as vari\u00e1veis float\ncolormap = plt.cm.jet\nplt.figure(figsize=(16,12))\nplt.title('Correla\u00e7\u00e3o de Pearson dados float', y=1.05, size=15)\nsns.heatmap(train_float.corr(),linewidths=0.1,vmax=1.0, square=True, cmap='Blues', linecolor='white', annot=True)","fa9338dc":"# Verificando a correla\u00e7\u00e3o entre as vari\u00e1veis int\ncolormap = plt.cm.jet\nplt.figure(figsize=(21,16))\nplt.title('Correla\u00e7\u00e3o de Pearson dados inteiros', y=1.05, size=15)\nsns.heatmap(train_int.corr(),linewidths=0.1,vmax=1.0, square=True, cmap='Blues', linecolor='white', annot=False)","07521b35":"# Como podemos observar N\u00e3o existe nenhuma correla\u00e7\u00e3o entre as vari\u00e1veis ps_calc e por isso vamos removelas\ncolormap = plt.cm.jet\ncotrain = train_int.drop(['id','target', 'ps_calc_04', 'ps_calc_05', 'ps_calc_06', 'ps_calc_07', 'ps_calc_08', 'ps_calc_09', 'ps_calc_10', 'ps_calc_11', 'ps_calc_12', 'ps_calc_13', 'ps_calc_14', 'ps_calc_15_bin', 'ps_calc_16_bin', 'ps_calc_17_bin', 'ps_calc_18_bin', 'ps_calc_19_bin', 'ps_calc_20_bin'], axis=1)\nplt.figure(figsize=(21,16))\nplt.title('Correla\u00e7\u00e3o de Pearson dados inteiros Excluindo ps_calc', y=1.05, size=12)\nsns.heatmap(cotrain.corr(),linewidths=0.1,vmax=1.0, square=True, cmap='Blues', linecolor='white', annot=False)","fa885bd6":"# Verificando a correla\u00e7\u00e3o entre todos os dados\ncolormap = plt.cm.jet\n# train = train.drop(['id', 'target'], axis=1)\nplt.figure(figsize=(25,25))\nplt.title('Pearson correlation of All the features', y=1.05, size=15)\nsns.heatmap(train.corr(),linewidths=0.1,vmax=1.0, square=True, cmap='Blues', linecolor='white', annot=False)","6b14b2ed":"# Verificando atributos nomanais para saber valores distintos em cada um\nv = meta[(meta.level == 'nominal') & (meta.keep)].index\n\nfor f in v:\n    dist_values = train[f].value_counts().shape[0]\n    print('Atributo {} tem {} valores distintos'.format(f, dist_values))","8f56e12d":"# Gerando One-hot encoding\nv = meta[(meta.level == 'nominal') & (meta.keep)].index\nprint('Antes do one-hot encoding tinha-se {} atributos'.format(train.shape[1]))\ntrain = pd.get_dummies(train, columns=v, drop_first=True)\nprint('Depois do one-hot encoding tem-se {} atributos'.format(train.shape[1]))\n\ntest = pd.get_dummies(test, columns=v, drop_first=True)\nmissing_cols = set( train.columns ) - set( test.columns )\nfor c in missing_cols:\n    test[c] = 0\n    \ntrain, test = train.align(test, axis=1)","4ae94487":"# Verificando Train e Test se tem o mesmo tamanho\/formato\nprint(train.shape)\nprint(test.shape)","feaa47be":"# Aplicando a Regress\u00e3o utilizando o balanceamento do target\nX_train = train.drop(['id', 'target'], axis=1)\ny_train = train['target']\n\nX_test  = test.drop(['id', 'target'], axis=1)\ny_test  = test['target']\n\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(class_weight='balanced')\n#model = LogisticRegression()\n\nmodel.fit(X_train, y_train)\nmodel.score(X_test, y_test)","55b53795":"# Criando o predict\ny_pred = model.predict_proba(X_test)[:,1]\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(model.score(X_test, y_test)))","8c314fcd":"# Matriz de Confus\u00e3o\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix = confusion_matrix(y_test, model.predict(X_test))\nprint(confusion_matrix)","f91eb2a0":"# Matriz de Confus\u00e3o com fun\u00e7\u00e3o do pandas e maior facilidade de entender os resultados\nprint(pd.crosstab(y_test, model.predict(X_test), rownames= ['Real'], colnames = ['Predito'], margins=True ))","edd2e8f1":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\n\n# Test options and evaluation metric\nnum_folds = 10\nseed = 8\nscoring = 'accuracy'\n\nX = train.drop(['id','target'], axis=1)\nY = train.target\n\nvalidation_size = 0.3\nX_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size, random_state=seed)","eaf203fa":"models = [('LR', LogisticRegression()), \n          ('LDA', LinearDiscriminantAnalysis()),\n          ('CART', DecisionTreeClassifier()),\n          ('NB', GaussianNB())]\nresults = []\nnames = []\nfor name, model in models:\n    print(\"Training model %s\" %(name))\n    model.fit(X_train, Y_train)\n    result = model.score(X_test, y_test)\n    msg = \"Classifier score %s: %f\" % (name, result)\n    print(msg)\nprint(\"----- Training Done -----\")","4196fda1":"y_pred","f73f769f":"#Enviando a previs\u00e3o para o Kaggle\nprevisao = pd.DataFrame()\nprevisao['id'] = test['id']\nprevisao['target'] = y_pred","f1de629b":"previsao.to_csv('previsao.csv',index = False)","fcfa3637":"# **Criando Metadados**","ce5886f7":"# **Aplicando Modelo de Regress\u00e3o Linear - com parametro class_weight='balanced'**","456f0f5b":"### Nome dos integrantes\n### Aluno 1: Aline Cristini - 183132\n### Aluno 2: Camila Rodrigues - 183143\n### Aluno 3: Rafael Gimenes Leite - 101634\n### Aluno 4: Vitor Dam\u00e1zio - 090773","5df131c0":"# **An\u00e1lise do TARGET**","f9c5a7ec":"# **Leitura do arquivo**","51bb9206":"# **Removendo Registros Duplicados**","b9199b1d":"# **Matriz de Confus\u00e3o**","f72bc0e2":"# **Bibliotecas**","2c190d3b":"# **Dados Faltantes**","6ca44150":"# **Avaliando outros modelos**","719fe660":"# **Predict**","c741290a":"# **One-hot encoding**","52be7f58":"# **An\u00e1lise da Correla\u00e7\u00e3o**"}}