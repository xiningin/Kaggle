{"cell_type":{"5ef59223":"code","26aa7944":"code","a4e922f1":"code","efbf45bb":"code","6dfaa71b":"code","bb8af0cf":"code","82bb61b6":"code","e7c26498":"code","5a016fc2":"code","e8c2898c":"code","29b68329":"code","e1849546":"code","5e7d64ec":"code","d188d47f":"code","1cfef6f0":"code","87f6e1a7":"code","42d92b19":"code","916b0e2c":"code","f88ab581":"code","d66712f6":"code","cf768d1f":"code","41dbf104":"markdown","930e6957":"markdown","83144e8d":"markdown","fa048612":"markdown","267f8d52":"markdown","ec4d22d0":"markdown","561830b3":"markdown","7f9aa768":"markdown","1d9e6db5":"markdown","399333cf":"markdown","d800f016":"markdown","c316e307":"markdown"},"source":{"5ef59223":"!pip install mido","26aa7944":"import mido # easy to use python MIDI library\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\n\nfrom mido import MidiFile, MidiTrack, Message","a4e922f1":"os.listdir('..\/input')","efbf45bb":"paths = []\nsongs = []\n#append every filepath in the blues folder to paths[]\nfor r, d, f in os.walk(r'..\/input\/blues'):\n    for file in f:\n        if '.mid' in file:\n            paths.append(os.path.join(r, file))\n\n#for each path in the array, create a Mido object and append it to song[]\nfor path in paths:\n    mid = MidiFile(path, type = 1)\n    songs.append(mid)\ndel paths","6dfaa71b":"#first Mido Object\nprint(songs[0])","bb8af0cf":"notes = []\ndataset = []\nchunk = []\n\n#for each in midi object in list of songs\nfor i in range(len(songs)):\n    #for each note in midi object\n    for msg in songs[i]:\n        #filtering out meta messages\n        if not msg.is_meta:\n            #filtering out control changes\n            if (msg.type == 'note_on'):\n                #normalizing note and velocity values\n                notes.append(msg.note)\n    for i in range(1, len(notes)):\n        chunk.append(notes[i])\n    dataset.append(chunk)\n    chunk = []\n    notes = []\ndel chunk\ndel notes","82bb61b6":"print(dataset[0])","e7c26498":"dataset = np.array(dataset)\ndataset.shape","5a016fc2":"notes = []\ndataset = []\nchunk = []\n\n#for each in midi object in list of songs\nfor i in range(len(songs)):\n    #for each note in midi object\n    for msg in songs[i]:\n        #filtering out meta messages\n        if not msg.is_meta:\n            #filtering out control changes\n            if (msg.type == 'note_on'):\n                #normalizing note and velocity values\n                notes.append(msg.note)\n    for i in range(1, len(notes)):\n        chunk.append(notes[i])\n        #save each 16 note chunk\n        if (i % 16 == 0):\n            dataset.append(chunk)\n            chunk = []\n    chunk = []\n    notes = []\ndel chunk\ndel notes","e8c2898c":"print(dataset[0])","29b68329":"dataset = np.array(dataset)\ndataset.shape","e1849546":"import tensorflow as tf\nfrom tensorflow import keras\nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout, Bidirectional\nfrom keras.layers import LSTM, Reshape, RepeatVector, TimeDistributed\nfrom sklearn.model_selection import train_test_split\nfrom keras.layers.advanced_activations import LeakyReLU","5e7d64ec":"dataset = dataset.reshape(len(dataset),16,1)\ndataset.shape","d188d47f":"noise = np.random.normal(0,1,(len(dataset),4,4))\nnoise.shape","1cfef6f0":"#initialize model\nmodel = Sequential()\n\n#encoder model\nmodel.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=(4, 4)))\nmodel.add(LeakyReLU(alpha=0.2))\n\nmodel.add(Bidirectional(LSTM(128, return_sequences=True)))\nmodel.add(LeakyReLU(alpha=0.2))\nmodel.add(Dropout(0.3))   \n\nmodel.add(Bidirectional(LSTM(128)))\nmodel.add(LeakyReLU(alpha=0.2))\nmodel.add(Dropout(0.3))\n\n#specifying output to have 16 timesteps\nmodel.add(RepeatVector(16))\n\n#decoder model\nmodel.add(Bidirectional(LSTM(128, return_sequences=True)))\nmodel.add(LeakyReLU(alpha=0.2))\nmodel.add(Dropout(0.3))\n\nmodel.add(Bidirectional(LSTM(128, return_sequences=True)))\nmodel.add(LeakyReLU(alpha=0.2))\nmodel.add(Dropout(0.3))   \n\nmodel.add(Bidirectional(LSTM(128, return_sequences=True)))\nmodel.add(LeakyReLU(alpha=0.2))\nmodel.add(Dropout(0.3))   \n\nmodel.add(TimeDistributed(Dense(128)))\nmodel.add(LeakyReLU(alpha=0.2))\nmodel.add(Dropout(0.3))\nmodel.add(TimeDistributed(Dense(128)))\nmodel.add(LeakyReLU(alpha=0.2))\nmodel.add(Dropout(0.3))\n#specifying 1 feature as the output\nmodel.add(TimeDistributed(Dense(1)))\nmodel.add(LeakyReLU(alpha=0.2))\n\nmodel.compile(loss='mean_squared_error', optimizer='adam')\n\nmodel.summary()","87f6e1a7":"#normalize note values to be between 0 and 1\nscale = np.max(dataset)\ndataset = dataset\/scale\n#splitting data into train and test sets. 3\/4 train, 1\/4 test.\nx_train,x_test,y_train,y_test = train_test_split(noise, dataset, test_size=0.25, shuffle=True, random_state=42)","42d92b19":"history = model.fit(x_train, y_train, epochs=25, batch_size=108, verbose=1,validation_data=(x_test, y_test))","916b0e2c":"# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Test', 'Validation'], loc='upper right')\nplt.show()","f88ab581":"random = np.random.normal(0,1,(1,4,4))\n\npredict = model.predict(random)\n\n#adjusting from normalization\npredict = predict * scale","d66712f6":"print(predict)","cf768d1f":"midler = MidiFile()\ntrack = MidiTrack()\nmidler.tracks.append(track)\ntrack.append(Message('program_change', program=2, time=0))\nfor x in range(16):\n    track.append(Message('note_on', note=int(predict[0][x][0]), velocity=64, time=20))\n    track.append(Message('note_off', note=int(predict[0][x][0]), velocity=64, time=20))\n    midler.save('new_song.mid')","41dbf104":"Bidirectional LSTM Sequence to Sequence Model","930e6957":"# Back to MIDI\nSave generated melody back to a .mid file","83144e8d":"# Melody Generation\nGenerating random input and letting model predict output","fa048612":"# Keras Bidirectional LSTM Example","267f8d52":"![notes](https:\/\/www.noterepeat.com\/images\/other\/other_midi_terms_explained_2.png)","ec4d22d0":"Reshaping data to be 3 dimensions needed for LSTM input","561830b3":"Defining arbritary input space to be (4,4)\nGenerating noise as input to generative model","7f9aa768":"## Introduction\nI am working on a Midi Music Generation project and gathered many midi songs of different genres. This is an ongoing project and I will add more songs and more genres over time. This is a starter notebook meant to help you get started quickly on this dataset.","1d9e6db5":"# Data Preprocessing","399333cf":"# Mido\nI'm going to be using Mido to handle parsing information from the .mid files.\nMido is a really easy library to work with.\n* [Documentation](https:\/\/mido.readthedocs.io\/en\/latest\/)\n* [Github](https:\/\/github.com\/mido\/mido)\n* [Midi Basics](https:\/\/www.noterepeat.com\/articles\/how-to\/213-midi-basics-common-terms-explained)","d800f016":"# All Notes From Each Song\nDataset will be 40 arrays containing all the notes of each song. Each array has a different length.","c316e307":"# Chunks of Notes From Each Song\nDataset will be arrays of 16 note chunks from each song. Each song will contribute multiple chunks."}}