{"cell_type":{"86e9ff27":"code","4e0c9769":"code","183e9d8b":"code","a50d039f":"code","5c95f027":"code","cac4f452":"code","067e7e76":"code","8ea91cc4":"code","9a97fe24":"code","d29eb9d4":"code","650e619a":"code","d2d98a63":"code","a2afe040":"code","7b18ff2e":"code","97d79e82":"code","58a8345d":"code","fc7442a7":"code","5d69c07e":"code","b69f1a70":"code","b37973a3":"code","f11cba50":"code","fcde1c9c":"code","6ea74c40":"code","42bc9c6c":"code","ad98a7a7":"code","e636c8f7":"code","2570da5e":"code","26051881":"code","74863b77":"code","aa67d166":"code","00653a5e":"code","b15cac7c":"code","536f67bb":"code","ef3ec9e4":"code","392a4229":"code","70f486ec":"code","145f12aa":"code","d053e45c":"code","3f227b97":"code","f7d577a4":"code","3fe0630c":"code","ff050caa":"code","517aa49a":"code","4419a0a4":"code","43a500c9":"code","7b1e4560":"code","1e06969a":"code","7b805897":"code","7b4390e6":"code","98cb7b72":"code","faba9fb4":"code","dd78199e":"code","2aa1230b":"code","75fdc034":"code","d0571e5c":"code","ad058c41":"code","1c864542":"code","1f7d0b2b":"code","e1347bdf":"code","5397149a":"code","873ded80":"code","3e4ddb57":"code","87277d44":"code","54be4c7d":"code","247fc293":"code","9196cb88":"code","cd549ecf":"code","a3ff4488":"code","2cc7e88b":"code","f170230b":"code","407065c6":"code","0bff77fb":"code","5d132898":"code","da366b12":"code","3d526480":"code","77dfd58d":"code","78821dfc":"code","246d18e1":"code","5e684eb4":"code","8251b5d3":"code","5a1d8e4f":"code","cc368242":"code","ecf8126e":"code","ad12a01f":"markdown","c9323ece":"markdown","c2891736":"markdown","96ddca70":"markdown","af6f3bdf":"markdown","da779c22":"markdown","31610707":"markdown","75896d9a":"markdown","bb3a47dc":"markdown","cfedb551":"markdown","c67bfd0a":"markdown","f2934218":"markdown","a3a4cbe6":"markdown","d859d86d":"markdown","869f3356":"markdown","bc6b8df8":"markdown","6dd9cb37":"markdown","cf22537d":"markdown","d92b5ab2":"markdown","f66805fe":"markdown","a58b7d32":"markdown","2136c0d4":"markdown","bed3938b":"markdown","0ce5175b":"markdown","ab2dc9fc":"markdown","baca64fe":"markdown","0610a6e3":"markdown","6bea9531":"markdown","1be267b7":"markdown","c3bec770":"markdown","8d7217ea":"markdown","2bba328d":"markdown","21024266":"markdown","3bada2ad":"markdown","a06c83d1":"markdown","e3ea98ac":"markdown","f1ecc5d8":"markdown","78abbbfd":"markdown","c232abcd":"markdown","46eb03f8":"markdown","8e6e0d7b":"markdown","265df7c0":"markdown","4e8c00d0":"markdown","8d22a144":"markdown","59b63cbb":"markdown","3b171007":"markdown","3d8a6c8e":"markdown","c53c954b":"markdown"},"source":{"86e9ff27":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport numpy as np","4e0c9769":"import warnings\nwarnings.filterwarnings(\"ignore\")","183e9d8b":"df_orig = pd.read_csv('..\/input\/ckdisease\/kidney_disease.csv')","a50d039f":"df_orig.head()","5c95f027":"df_orig.info()","cac4f452":"df_orig.describe()","067e7e76":"df_orig.shape","8ea91cc4":"(df_orig.isnull().sum() \/ df_orig.shape[0] * 100.00).round(2)","9a97fe24":"df_orig.shape","d29eb9d4":"df_orig.drop('id', axis = 1, inplace = True)","650e619a":"df_orig.columns = ['age', 'blood_pressure', 'specific_gravity', 'albumin', 'sugar', 'red_blood_cells', 'pus_cell',\n              'pus_cell_clumps', 'bacteria', 'blood_glucose_random', 'blood_urea', 'serum_creatinine', 'sodium',\n              'potassium', 'haemoglobin', 'packed_cell_volume', 'white_blood_cell_count', 'red_blood_cell_count',\n              'hypertension', 'diabetes_mellitus', 'coronary_artery_disease', 'appetite', 'peda_edema',\n              'aanemia', 'class']","d2d98a63":"df_orig.head()","a2afe040":"df_orig['packed_cell_volume'] = pd.to_numeric(df_orig['packed_cell_volume'], errors='coerce')\ndf_orig['white_blood_cell_count'] = pd.to_numeric(df_orig['white_blood_cell_count'], errors='coerce')\ndf_orig['red_blood_cell_count'] = pd.to_numeric(df_orig['red_blood_cell_count'], errors='coerce')","7b18ff2e":"cat_col=[col for col in df_orig.columns if df_orig[col].dtype=='object']\nfor col in cat_col:\n    print('{} has {} values '.format(col,df_orig[col].unique()))\n    print('\\n')","97d79e82":"df_orig['diabetes_mellitus'].replace(to_replace = {'\\tno':'no','\\tyes':'yes',' yes':'yes'},inplace=True)\n\ndf_orig['coronary_artery_disease'] = df_orig['coronary_artery_disease'].replace(to_replace = '\\tno', value='no')\n\ndf_orig['class'] = df_orig['class'].replace(to_replace = 'ckd\\t', value = 'ckd')\n\ndf_orig['class'] = df_orig['class'].replace(to_replace = 'notckd', value = 'not ckd')\n\n\nfor col in cat_col:\n    print('{} has {} values  '.format(col, df_orig[col].unique()))\n    print('\\n')","58a8345d":"df_orig['class'] = df_orig['class'].map({'ckd': 0, 'not ckd': 1})\ndf_orig['class'] = pd.to_numeric(df_orig['class'], errors='coerce')\ncat_cols = [col for col in df_orig.columns if df_orig[col].dtype == 'object']\nfor col in cat_cols:\n    print(f\"{col} has {df_orig[col].unique()} values\\n\")","fc7442a7":"cat_cols = [col for col in df_orig.columns if df_orig[col].dtype == 'object']\nnum_cols = [col for col in df_orig.columns if df_orig[col].dtype != 'object']\nnum_cols = num_cols[:-1]\nprint(cat_cols)\nprint(num_cols)","5d69c07e":"df_orig_1 = df_orig\n\ndef mean_value_imputation(feature):\n    mean = df_orig[feature].mean()\n    df_orig[feature] = df_orig[feature].fillna(mean)\n    \n    \nfor col in num_cols:\n    mean_value_imputation(col)","b69f1a70":"def impute_mode(feature):\n    mode = df_orig[feature].mode()[0]\n    df_orig[feature] = df_orig[feature].fillna(mode)\n    \nfor col in cat_cols:\n    impute_mode(col)","b37973a3":"(df_orig.isnull().sum() \/ df_orig.shape[0] * 100.00).round(2)","f11cba50":"from sklearn.impute import KNNImputer\n\nimputer = KNNImputer(n_neighbors=11)\nimputer.fit_transform(df_orig_1[num_cols])\n\ndef impute_mode(feature):\n    mode = df_orig_1[feature].mode()[0]\n    df_orig_1[feature] = df_orig_1[feature].fillna(mode)\n    \nfor col in cat_cols:\n    impute_mode(col)","fcde1c9c":"(df_orig_1.isnull().sum() \/ df_orig_1.shape[0] * 100.00).round(2)","6ea74c40":"import pandas_profiling as pp\n\nprofile = pp.ProfileReport(df_orig, title=\"Chronic Kidney Disease Dataset Profile\", html={\"style\": {\"full_width\": True}}, sort=None)\nprofile","42bc9c6c":"num_cols1 = num_cols[:-2]\nfig = px.box(df_orig[num_cols1], y=num_cols1)\nfig.show()","ad98a7a7":"fig = px.box(df_orig['white_blood_cell_count'], y='white_blood_cell_count')\nfig.show()","e636c8f7":"fig = px.box(df_orig['red_blood_cell_count'], y='red_blood_cell_count')\nfig.show()","2570da5e":"import plotly.graph_objects as go\ndf_px = df_orig[num_cols]\nfig = px.imshow(df_px.corr())\nfig.show()","26051881":"df_px = df_orig[['class', 'red_blood_cell_count', 'white_blood_cell_count', 'specific_gravity', 'packed_cell_volume']]\nfig = px.scatter_matrix(df_px, \n    dimensions = ['red_blood_cell_count', 'white_blood_cell_count', 'specific_gravity', 'packed_cell_volume'],\n    color=\"class\")\nfig.show()","74863b77":"fig = go.Figure([go.Histogram(x = df_orig['age'])])\nfig.show()","aa67d166":"fig = go.Figure([go.Histogram(x = df_orig['white_blood_cell_count'])])\nfig.show()","00653a5e":"fig = go.Figure([go.Histogram(x = df_orig['red_blood_cell_count'])])\nfig.show() ","b15cac7c":"fig = go.Figure([go.Histogram(x = df_orig['blood_glucose_random'])])\nfig.show() ","536f67bb":"fig = px.scatter(df_orig, \n    x = df_orig['age'], y = df_orig['diabetes_mellitus'],\n    color=\"class\")\nfig.show()","ef3ec9e4":"import numpy as np\n# IQR\ndef IQR_outliers(col):\n\n    Q1 = np.percentile(df_orig[col], 25,\n                       interpolation = 'midpoint')\n\n    Q3 = np.percentile(df_orig[col], 75,\n                       interpolation = 'midpoint')\n    \n    per_95 = np.percentile(df_orig[col], 95,\n                       interpolation = 'midpoint')\n    \n    IQR = Q3 - Q1\n    \n    upper = Q3+1.5*IQR\n    lower = Q1-1.5*IQR\n    \n    df_orig[col] = np.where(df_orig[col] > upper, per_95, df_orig[col])\n    df_orig[col] = np.where(df_orig[col] < lower, lower, df_orig[col])\n\n    return df_orig\n\n\n\nfor col in num_cols:\n    df_orig = IQR_outliers(col)","392a4229":"num_cols1 = num_cols[:-2]\nfig = px.box(df_orig[num_cols1], y=num_cols1)\nfig.show()","70f486ec":"fig = px.box(df_orig['white_blood_cell_count'], y='white_blood_cell_count')\nfig.show()","145f12aa":"fig = px.box(df_orig['red_blood_cell_count'], y='red_blood_cell_count')\nfig.show()","d053e45c":"for col in cat_cols:\n    print(f\"{col} has {df_orig[col].nunique()} categories\\n\")","3f227b97":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\nfor col in cat_cols:\n    df_orig[col] = le.fit_transform(df_orig[col])","f7d577a4":"df_orig.head()","3fe0630c":"df_orig.info()","ff050caa":"from sklearn.model_selection import train_test_split\ndf, df_test = train_test_split(df_orig, test_size = 0.15, random_state = 42)","517aa49a":"import statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\ndf_vif = df_orig\ndf_vif['const'] = 1\n\nX = df_vif[num_cols+['const']]\n\nvif_info = pd.DataFrame()\nvif_info['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif_info['Column'] = X.columns\nvif_info.sort_values('VIF', ascending=False)","4419a0a4":"ind_col = [col for col in df.columns if col != 'class']\ndep_col = 'class'\n\nX = df[ind_col]\ny = df[dep_col]\n\nX_test = df_test[ind_col]\ny_test = df_test[dep_col]","43a500c9":"print(X.shape)\nprint(X_test.shape)","7b1e4560":"from sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.20, random_state = 0)","1e06969a":"from sklearn.decomposition import PCA\npca = PCA(n_components=24)\nprincipalComponents = pca.fit_transform(X_train)\nprint (pca.explained_variance_ratio_.cumsum())","7b805897":"pca.fit(X_train)\nX_valid_new = pca.transform(X_valid)","7b4390e6":"#pca.fit(X_train)\nX_train_new = pca.transform(X_train)","98cb7b72":"X_train_new.shape","faba9fb4":"X_valid_new.shape","dd78199e":"print(X_valid_new)","2aa1230b":"from yellowbrick.cluster import KElbowVisualizer\nfrom sklearn.cluster import KMeans\nmodel = KElbowVisualizer(KMeans(), k=15)\nmodel.fit(X_train)\nmodel.show()","75fdc034":"from sklearn.cluster import KMeans\nkmeans = KMeans(\n    init=\"random\",\n    n_clusters=5,\n    n_init=10,\n    max_iter=300,\n    random_state=42\n)\nX_clustered = kmeans.fit_transform(X_train)\n                        \nfrom sklearn.metrics import silhouette_score\nscore = silhouette_score(X_clustered, kmeans.labels_)\nprint(score)","d0571e5c":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\ndtc = DecisionTreeClassifier()\ndtc.fit(X_train_new, y_train)\n\n# accuracy score, confusion matrix and classification report of decision tree\n\ndtc_acc = accuracy_score(y_valid, dtc.predict(X_valid_new))\ndtc_acc_test = accuracy_score(y_test, dtc.predict(X_test))\n\nprint(f\"Training Accuracy of Decision Tree Classifier is {accuracy_score(y_train, dtc.predict(X_train_new))}\")\nprint(f\"Test Accuracy of Decision Tree Classifier is {dtc_acc_test} \\n\")\nprint(f\"Validation Accuracy of Decision Tree Classifier is {accuracy_score(y_valid, dtc.predict(X_valid_new))}\")\n\nprint(f\"Confusion Matrix :- \\n{confusion_matrix(y_valid, dtc.predict(X_valid_new))}\\n\")\nprint(f\"Classification Report :- \\n {classification_report(y_valid, dtc.predict(X_valid_new))}\")","ad058c41":"from sklearn.model_selection import GridSearchCV\ngrid_param = {\n    'criterion' : ['gini', 'entropy'],\n    'max_depth' : [3, 5, 7, 10],\n    'splitter' : ['best', 'random'],\n    'min_samples_leaf' : [1, 2, 3, 5, 7],\n    'min_samples_split' : [1, 2, 3, 5, 7],\n    'max_features' : ['auto', 'sqrt', 'log2']\n}\n\ngrid_search_dtc = GridSearchCV(dtc, grid_param, cv = 11, n_jobs = -1, verbose = 1)\ngrid_search_dtc.fit(X_train, y_train)\nprint(grid_search_dtc.best_params_)\nprint(grid_search_dtc.best_score_)","1c864542":"dtc = grid_search_dtc.best_estimator_\n\nprint(dtc)\n\n# accuracy score, confusion matrix and classification report of grid search\n\ndtc_gs_acc = accuracy_score(y_valid, dtc.predict(X_valid))\ndtc_gs_acc_test = accuracy_score(y_test, dtc.predict(X_test))\n\nprint(f\"Training Accuracy of Decision Tree Classifier is {accuracy_score(y_train, dtc.predict(X_train))}\")\nprint(f\"Test Accuracy of Decision Tree Classifier is {dtc_gs_acc_test}\")\nprint(f\"Validation Accuracy of Decision Tree Classifier is {accuracy_score(y_valid, dtc.predict(X_valid))} \\n\")\n\nprint(f\"Confusion Matrix :- \\n{confusion_matrix(y_valid, dtc.predict(X_valid_new))}\\n\")\nprint(f\"Classification Report :- \\n {classification_report(y_valid, dtc.predict(X_valid_new))}\")","1f7d0b2b":"plt.figure(figsize=(12,3))\nfeatures = X_test.columns.values.tolist()\nimportance = dtc.feature_importances_.tolist()\nft_imp = pd.DataFrame()\nft_imp['feature'] = features\nft_imp['importance'] = importance\nft_imp.sort_values(by=['importance'], ascending = False, inplace=True)\nprint(ft_imp)\nfeature_series = pd.Series(data=importance,index=features)\nfeature_series.plot.bar()\nplt.title('Feature Importance')","e1347bdf":"import lime\nimport lime.lime_tabular\n\nexplainer = lime.lime_tabular.LimeTabularExplainer(X_train.values, feature_names=X_test.columns.values.tolist(),\n                                                  class_names=['skd', 'not skd'], verbose=True, mode='classification')","5397149a":"j = len(X_train) - 1\nexp = explainer.explain_instance(X_train.values[j], grid_search_dtc.predict_proba, num_features=10)","873ded80":"exp.show_in_notebook(show_table=True)","3e4ddb57":"!pip install borutashap","87277d44":"from BorutaShap import BorutaShap\n\nFeature_Selector = BorutaShap(importance_measure='shap', classification=False)\n\nFeature_Selector.fit(X=X_train, y=y_train, n_trials=50, random_state=0)","54be4c7d":"Feature_Selector.plot(which_features='all', figsize=(16,12))\n\nselected_columns = list()\nselected_columns.append(sorted(Feature_Selector.Subset().columns))\n    \nprint(f\"Selected features are: {selected_columns[-1]}\")","247fc293":"from sklearn.ensemble import RandomForestClassifier\n\nrd_clf = RandomForestClassifier(criterion = 'entropy', max_depth = 10, max_features = 'auto', min_samples_leaf = 2, min_samples_split = 7, n_estimators = 12)\nrd_clf.fit(X_train_new, y_train)\n\n# accuracy score, confusion matrix and classification report of random forest\n\nrd_clf_acc = accuracy_score(y_valid, rd_clf.predict(X_valid_new))\nrd_clf_acc_test = accuracy_score(y_test, rd_clf.predict(X_test))\n\nprint(f\"Training Accuracy of Random Forest Classifier is {accuracy_score(y_train, rd_clf.predict(X_train_new))}\")\nprint(f\"Test Accuracy of Random Forest Classifier is {rd_clf_acc_test} \\n\")\nprint(f\"Validation Accuracy of Random Forest Classifier is {accuracy_score(y_valid, rd_clf.predict(X_valid_new))}\")\n\nprint(f\"Confusion Matrix :- \\n{confusion_matrix(y_valid, rd_clf.predict(X_valid_new))}\\n\")\nprint(f\"Classification Report :- \\n {classification_report(y_valid, rd_clf.predict(X_valid_new))}\")","9196cb88":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier(objective = 'binary:logistic', learning_rate = 0.5, max_depth = 5, n_estimators = 150)\nxgb.fit(X_train_new, y_train)\n\n# accuracy score, confusion matrix and classification report of xgboost\n\nxgb_acc = accuracy_score(y_valid, xgb.predict(X_valid_new))\nxgb_acc_test = accuracy_score(y_test, xgb.predict(X_test))\n\nprint(f\"Training Accuracy of XgBoost is {accuracy_score(y_train, xgb.predict(X_train_new))}\")\nprint(f\"Test Accuracy of XgBoost is {xgb_acc_test}\")\nprint(f\"Validation Accuracy of XgBoost is {accuracy_score(y_valid, xgb.predict(X_valid_new))} \\n\")\n\nprint(f\"Confusion Matrix :- \\n{confusion_matrix(y_valid, xgb.predict(X_valid_new))}\\n\")\nprint(f\"Classification Report :- \\n {classification_report(y_valid, xgb.predict(X_valid_new))}\")","cd549ecf":"!pip install pytorch-tabnet","a3ff4488":"from pytorch_tabnet.tab_model import TabNetClassifier\nfrom sklearn.model_selection import KFold\nimport torch as torch","2cc7e88b":"clf = TabNetClassifier(verbose=1,seed=42)","f170230b":"clf.fit(X_train=X_train_new, y_train=y_train,\n               eval_metric=['auc'])","407065c6":"tbnt_acc = accuracy_score(y_valid, clf.predict(X_valid_new))\n#tbnt_acc_test = accuracy_score(y_test, clf.predict(X_test))\n\nprint(f\"Training Accuracy of TabNet is {accuracy_score(y_train, clf.predict(X_train_new))}\")\n#print(f\"Test Accuracy of TabNet is {tbnt_acc_test} \\n\")\nprint(f\"Validation Accuracy of TabNet is {accuracy_score(y_valid, clf.predict(X_valid_new))}\")\n\nprint(f\"Confusion Matrix :- \\n{confusion_matrix(y_valid, clf.predict(X_valid_new))}\\n\")\nprint(f\"Classification Report :- \\n {classification_report(y_valid, clf.predict(X_valid_new))}\")","0bff77fb":"import optuna\nfrom optuna import Trial","5d132898":"EPOCHS = 30\nBATCH_SIZE = 32\n\ndef objective(trial):\n    # parameter set by optuna\n    N_D = trial.suggest_int('N_D', 8, 32)\n    N_A = N_D\n    GAMMA = trial.suggest_float('GAMMA', 1.0, 2.0)\n    N_STEPS = trial.suggest_int('N_STEPS', 1, 3, 1)\n    LAMBDA_SPARSE = trial.suggest_loguniform(\"LAMBDA_SPARSE\", 1e-5, 1e-1)\n    \n    # changes\n    # introduced lambda-sparse\n    clf = TabNetClassifier(\n                       optimizer_fn=torch.optim.Adam,\n                       optimizer_params=dict(lr=2e-2),\n                       scheduler_params={\"step_size\":4,\n                                         \"gamma\":0.9},\n                       scheduler_fn=torch.optim.lr_scheduler.StepLR,\n                       mask_type='sparsemax',\n                          n_d = N_D,\n                          n_a = N_A,\n                          gamma = GAMMA,\n                          n_steps = N_STEPS,\n                          lambda_sparse = LAMBDA_SPARSE)\n    \n    clf.fit(X_train_new, y_train,\n        eval_set=[(X_train_new, y_train),(X_valid_new, y_valid)],\n        max_epochs = EPOCHS,\n        batch_size = BATCH_SIZE,\n        patience = 5,\n        eval_name=['train', 'valid'],\n        eval_metric=['auc']\n           )\n    \n    # changed, now score is max val_uac\n    score = np.max(clf.history['valid_auc'])\n    \n    return score","da366b12":"study = optuna.create_study(direction='maximize', study_name = 'tabnet-study')\n\nstudy.optimize(objective, n_trials=150, timeout = 3600*8)\n\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","3d526480":"print('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","77dfd58d":"from optuna.visualization import plot_optimization_history\n\nplot_optimization_history(study)","78821dfc":"from optuna.visualization import plot_param_importances\n\nplot_param_importances(study)","246d18e1":"params_tb = study.best_trial.params\nprint(params_tb)","5e684eb4":"clf_opt = TabNetClassifier(verbose=1, seed=42,\n                        optimizer_fn=torch.optim.Adam,\n                       optimizer_params=dict(lr=2e-2),\n                       scheduler_params={\"step_size\":4,\n                                         \"gamma\":0.9},\n                       scheduler_fn=torch.optim.lr_scheduler.StepLR,\n                       mask_type='sparsemax',\n                          n_d = params_tb['N_D'],\n                          n_a = params_tb['N_D'],\n                          gamma = params_tb['GAMMA'],\n                          n_steps = params_tb['N_STEPS'],\n                          lambda_sparse = params_tb['LAMBDA_SPARSE'])","8251b5d3":"clf_opt.fit(X_train=X_train_new, y_train=y_train,\n        max_epochs = 100,\n        batch_size = BATCH_SIZE,\n        patience = 5,\n        eval_metric=['auc']\n       )","5a1d8e4f":"tbnt_opt_acc = accuracy_score(y_valid, clf_opt.predict(X_valid_new))\n#tbnt_opt_acc_test = accuracy_score(y_test, clf_opt.predict(X_test))\n\nprint(f\"Training Accuracy of TabNet is {accuracy_score(y_train, clf_opt.predict(X_train_new))}\")\n#print(f\"Test Accuracy of TabNet is {tbnt_opt_acc_test} \\n\")\nprint(f\"Validation Accuracy of TabNet is {accuracy_score(y_valid, clf_opt.predict(X_valid_new))}\")\n\nprint(f\"Confusion Matrix :- \\n{confusion_matrix(y_valid, clf_opt.predict(X_valid_new))}\\n\")\nprint(f\"Classification Report :- \\n {classification_report(y_valid, clf_opt.predict(X_valid_new))}\")","cc368242":"models = pd.DataFrame({\n    'Model' : [ 'Decision Tree Classifier', 'TabNet', 'Tabnet + Optuna', 'Decision Tree + GridSearch', 'Random Forest Classifier',\n             'XgBoost'],\n    'Score' : [dtc_acc, tbnt_acc, tbnt_opt_acc, dtc_gs_acc, rd_clf_acc, xgb_acc]\n})\n\n\nmodels = models.sort_values(by = 'Score', ascending = True)","ecf8126e":"px.bar(data_frame = models, x = 'Score', y = 'Model', color = 'Score', template = 'seaborn', \n       title = 'Models Validation Score Comparison')","ad12a01f":"According to National Health Service UK:\n> Chronic kidney disease is usually caused by other conditions that put a strain on the kidneys. Often it's the result of a combination of different problems.\n\nNHS gives further descriptions of possible CKD causes, such as:\n\n>*CKD can be caused by:*\n>* high blood pressure \u2013 over time, this can put strain on the small blood vessels in the kidneys and stop the kidneys working properly\n>* diabetes \u2013 too much glucose in your blood can damage the tiny filters in the kidneys\n>* high cholesterol \u2013 this can cause a build-up of fatty deposits in the blood vessels supplying your kidneys, which can make it harder for them to work properly\n>* kidney infections\n>* glomerulonephritis \u2013 kidney inflammation\n>* polycystic kidney disease \u2013 an inherited condition where growths called cysts develop in the kidneys\n>* blockages in the flow of urine \u2013 for example, from kidney stones that keep coming back, or an enlarged prostate long-term, regular use of certain medicines \u2013 such as lithium and non-steroidal anti-inflammatory drugs (NSAIDs)\n","c9323ece":"<a id=\"11\"><\/a>\n# XGBoost","c2891736":"---> using LIME","96ddca70":"Let's check best k for kmeans clustering:","af6f3bdf":"# Table of contents\n1. [Import packages](#1)\n1. [Import data](#2)\n1. [Data Cleaning](#3)\n1. [EDA](#4)\n1. [Variance Inflation Factor](#5)\n1. [Principal Component Analysis](#6)  \n1. [Decision Tree model](#7)   \n1. [GridSearch](#8)    \n1. [Feature Importance](#9)    \n1. [Random Forest model](#10)    \n1. [XgBoost model](#11)    \n1. [TabNet model](#12)    \n1. [TabNet + Optuna](#13)    \n1. [Results](#14)    \n1. [Conclusion](#15)    ","da779c22":"<a id=\"15\"><\/a>\n# Conclusions","31610707":"## Analyze categorical columns","75896d9a":"3 methods were used to analyze feature importance for GridSearch model:\n\n---> using built-in function","bb3a47dc":"2) use knn to replace nan values:","cfedb551":"*The PCA did not improve model perfomance so I did not use it, n_components = 24 remains the best if we use it.*","c67bfd0a":"<a id=\"4\"><\/a>\n# EDA","f2934218":"# Modeling","a3a4cbe6":"## Deal with Outliers","d859d86d":"<a id=\"2\"><\/a>\n# Import data","869f3356":"Both methods work fine, but knn would be better for model accuracy.","bc6b8df8":"Predict on valid\/test using best parameters:","6dd9cb37":"# Chronic Kidney Disease Analysis and Prediction","cf22537d":"---> using boruta-shap","d92b5ab2":"Judging by the plots I would check the outliers and also use scaling for the columns mentioned above.","f66805fe":"<a id=\"8\"><\/a>\n# Use GridSearch","a58b7d32":"![](https:\/\/th.bing.com\/th\/id\/R.7508e9f5ce7c05d2f2feb20cf9ea4436?rik=ap1KNSWzuYqBqQ&riu=http%3a%2f%2fwww.assignmentpoint.com%2fwp-content%2fuploads%2f2015%2f10%2fBioinformatics.jpg&ehk=jfYuZa%2b8bJCbDO2yzjFlWxGkCkkz6U%2b%2fN05aWJ95koQ%3d&risl=&pid=ImgRaw&r=0&sres=1&sresct=1)","2136c0d4":"<a id=\"6\"><\/a>\n# Perform PCA on data","bed3938b":"## Split the train\/test data","0ce5175b":"<a id=\"12\"><\/a>\n# TabNet","ab2dc9fc":"<a id=\"5\"><\/a>\n# Check VIF (Variance Inflation Factor)","baca64fe":"## Transform column names","0610a6e3":"#### Thank you for making it till the end of my notebook! I hope you enjoyed the content above, please feel free to comment and upvote :)","6bea9531":"<a id=\"13\"><\/a>\n# TabNet Optimization (Optuna)","1be267b7":"### KMeans Clustering","c3bec770":"## Replace NaN values","8d7217ea":"##### Best parameters after using GridSearch are: \n*{'criterion': 'gini', 'max_depth': 7, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 7, 'splitter': 'best'}*","2bba328d":"##### All methods may slightly differ in their results, but all of them correspond to scientific definition of possible CKD causes. In conlusion, anomalies in specific gravity (kidney's ability to concentrate urine), blood pressure and presence of diabetes\/high glucose level can be an alarming indicator of CKD.","21024266":"Let's check if diabetes and kidney disease are connected:","3bada2ad":"### Check outliers","a06c83d1":"<a id=\"7\"><\/a>\n# Build the Decision Tree model","e3ea98ac":"Seeing the there is no instances where there are both diabetes and ckd we can not assume that there is any correlation between them.","f1ecc5d8":"All features have VIF below 5, so they do not have multicolinearity.\n\n*Not adding a column with constant leads to anomal values of VIF, the issue is described here:\nhttps:\/\/github.com\/statsmodels\/statsmodels\/issues\/2376*","78abbbfd":"I split train\/validation\/test as 65\/20\/15%, but because the dataset is small I would consider using 70\/20\/10% split.","c232abcd":"<a id=\"10\"><\/a>\n# Random Forest","46eb03f8":"From my perspective scaling age is not necessary, because the distribution is close to normal.","8e6e0d7b":"## Use LabelEncoder for categorical values","265df7c0":"<a id=\"14\"><\/a>\n# Plot results","4e8c00d0":"<a id=\"1\"><\/a>\n# Import packages","8d22a144":"<a id=\"3\"><\/a>\n# Data Cleaning","59b63cbb":"1) replace nan with mode for categorical values and mean for numerical:","3b171007":"**In this notebook:**\n* analyzed Chronic Kidney Disease dataset\n* performed EDA\n* tried out Principal Component Analysis (PCA)\n* used different models for prediction: Decision Tree, Grid Search, Random Forest, XgBoost, TabNet (with Optuna optimization)\n* used different packages for feature importance analysis: LIME, SHAP, Boruta\n\nIn conclusion, Decision Trees and TabNet perform well with optimization (in our case GridSearch\/Optuna), whereas XgBoost is capable of outperforming them singlehandedly. TabNet is proven itself to work good with tabular data, which is common among classic neural network models, but still, tree models and boosting are more effiecient in this case - even though they are easier to implement due to their simplicity, they can maintain high performance with data like used here. When analyzing feature importance we can see, that important features are the same which are considered to be main causes of CKD.","3d8a6c8e":"<a id=\"9\"><\/a>\n# Feature Importance","c53c954b":"Check for outliers (there is apparently a lot of them):"}}