{"cell_type":{"fb986e30":"code","832a87ed":"code","63e008ab":"code","f735f215":"code","9352755f":"code","53e07ecc":"code","a02e1933":"code","742a15d4":"code","986064c2":"code","123372c6":"code","f083dcda":"code","ebde17dd":"code","3a91f957":"code","15fd6170":"code","e388f3eb":"code","b0bdc5cc":"code","ec1bcec7":"code","7630ac5f":"code","434a1a1d":"code","bb4828e7":"code","8145fcb8":"code","1b60d2a1":"code","600695ce":"code","54ac5a80":"code","3355e31b":"code","5f842921":"markdown","7a9343f0":"markdown","96d78d93":"markdown","5dff2fa0":"markdown","65ebc9f0":"markdown","775e4eb9":"markdown","691a20ab":"markdown","a3080435":"markdown","a20d1fc0":"markdown","ad52ab54":"markdown","ee360754":"markdown","ddcccbff":"markdown","59836eb7":"markdown","a8dcd242":"markdown","bcc274bd":"markdown","d20b1f31":"markdown","5e10a05c":"markdown","5743567d":"markdown","22aee2ea":"markdown","e4a17043":"markdown","6961fb40":"markdown","67fad329":"markdown","1c06c373":"markdown","4a0226e9":"markdown","449b8b5e":"markdown"},"source":{"fb986e30":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","832a87ed":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\n\npd.set_option('display.max_columns', 500)","63e008ab":"# setting the paths to variables to access when required\n#Taking  a subset of the data to analyze\ntrain = pd.read_csv(\"\/kaggle\/input\/jane-street-market-prediction\/train.csv\")\nsample_test = pd.read_csv(\"\/kaggle\/input\/jane-street-market-prediction\/features.csv\")\n\n#Shape of teh Datasets\ntrain.shape, sample_test.shape","f735f215":"train.info()","9352755f":"missing_val = pd.DataFrame(train.isna().sum().sort_values(ascending=False)*100\/train.shape[0],columns=['missing %'])[:50]\nmissing_val.style.background_gradient(cmap='Oranges_r')","53e07ecc":"train.fillna(-999,inplace=True)","a02e1933":"#Defining a mappper function to replace missing values with withs mean.\nfor i in list(train.columns):\n    x = train[i].mean()\n    print(i,\"    : \",x)\n    #mapp[i] = x\n    #train[i]=train[i].fillna(x)","742a15d4":"#Lets Define the action in the data\n\n#We are using pi value 1 to take only the Weight!=0. Weight = 0 is only for data completeness. \ntrain=train[train['weight']!=0]\ntrain['action']=(train['resp']>0)*1\n\ntrain.action.value_counts()","986064c2":"sns.countplot(train.action)","123372c6":"fig, axs = plt.subplots(1, 3, figsize=(20, 6))\nsns.distplot(train['action'], ax=axs[0])\nsns.distplot(train['resp'], ax=axs[1])\nsns.distplot(train['weight'], ax=axs[2])","f083dcda":"fig, axs = plt.subplots(1, 4, figsize=(20, 6))\nsns.distplot(train['resp_1'], ax=axs[0])\nsns.distplot(train['resp_2'], ax=axs[1])\nsns.distplot(train['resp_3'], ax=axs[2])\nsns.distplot(train['resp_4'], ax=axs[3])","ebde17dd":"fig, ax = plt.subplots(figsize=(15, 5))\n\nreturns = pd.Series(train['resp']).cumsum()\n\nresp_1= pd.Series(train['resp_1']).cumsum()\nresp_2= pd.Series(train['resp_2']).cumsum()\nresp_3= pd.Series(train['resp_3']).cumsum()\nresp_4= pd.Series(train['resp_4']).cumsum()\n\nax.set_xlabel (\"Trade\", fontsize=18)\nax.set_title (\"Cummulative Return of 4 diffrerent Time Regions\", fontsize=20)\nreturns.plot()\nresp_1.plot()\nresp_2.plot()\nresp_3.plot()\nresp_4.plot()\nplt.legend()","3a91f957":"corr = train[['resp_1','resp_2','resp_3','resp_4','resp','action']].corr()\n\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\n\nfig = plt.figure(figsize = (15, 8))\n\n# plot the data using seaborn\nax = sns.heatmap(corr, \n                 mask = mask, \n                 annot=True,\n                 vmax = 0.3, \n                 square = True,  \n                 cmap = \"viridis\")\n# set the title for the figure\nax.set_title(\"Correlation Actions Vs resp, resp_{1,2,3,4}\");","15fd6170":"import warnings\nwarnings.filterwarnings('ignore')","e388f3eb":"feature_set = train.loc[:,train.columns.str.contains('feature')].columns\n\n#For analyzing part I am using only 20 features distribution. \nfor i in feature_set[:20]:\n    fig, ax = plt.subplots(figsize=(10, 4))\n    feature_ = pd.Series(train[str(i)]).cumsum()\n    ax.set_xlabel (\"Trade\", fontsize=18)\n    ax.set_ylabel (f\"feature_{i} (cumulative)\", fontsize=15);\n    feature_.plot(lw=2)","b0bdc5cc":"bad_feats = train[[\"feature_3\",\"feature_4\",\"feature_5\",\"feature_6\",\"feature_73\",\"feature_75\",\"feature_76\",\"feature_77\",\"feature_79\",\"feature_82\",\"resp\",\"action\"]]\nbad_feats.corr().style.background_gradient(cmap='viridis', low=1, high=0, axis=None).set_precision(2)","ec1bcec7":"def find_skewed_boundaries(df, variable, distance):\n\n    IQR = df[variable].quantile(0.75) - df[variable].quantile(0.25)\n\n    lower_boundary = df[variable].quantile(0.25) - (IQR * distance)\n    upper_boundary = df[variable].quantile(0.75) + (IQR * distance)\n\n    return upper_boundary, lower_boundary\n\nupper_resp,lower_resp = find_skewed_boundaries(train,'resp',1.5)\n\n\nprint('Capping are',lower_resp,upper_resp)","7630ac5f":"#Only Extracting the int64 type features\nnum_features = []\nfor i,j in zip(train.columns,train.dtypes.tolist()):\n    if(j==\"object\" or j == \"datetime64[ns]\"):\n        pass\n    else:\n        num_features.append(i)","434a1a1d":"num_features[:20]","bb4828e7":"from sklearn.feature_selection import RFE\nfrom xgboost import XGBClassifier\n\n\n\nmodel = XGBClassifier(\n        n_estimators = 500,\n        max_depth=11,\n        learning_rate=0.06,\n        subsample=0.85,\n        colsample_bytree=0.6,\n        random_state=0,\n        tree_method='gpu_hist'\n)\n","8145fcb8":"#Selecting Top 50 features\n#selector = RFE(model,n_features_to_select=50,step=2)\n#selector.fit(X_train[num_features],y_train)","1b60d2a1":"feats = ['feature_2',\n 'feature_9',\n 'feature_12',\n 'feature_16',\n 'feature_19',\n 'feature_24',\n 'feature_30',\n 'feature_32',\n 'feature_35',\n 'feature_37',\n 'feature_39',\n 'feature_42',\n 'feature_45',\n 'feature_57',\n 'feature_61',\n 'feature_62',\n 'feature_63',\n 'feature_67',\n 'feature_69',\n 'feature_76',\n 'feature_81',\n 'feature_82',\n 'feature_83',\n 'feature_88',\n 'feature_105',\n 'feature_120',\n 'feature_121',\n 'feature_122',\n 'feature_124',\n 'feature_129']","600695ce":"for feats in feats:\n    fig, axs = plt.subplots(1, 3, figsize=(15, 4))\n    sns.distplot(train[feats], ax=axs[0])\n    sns.distplot(train[train.weight !=0][feats], ax=axs[1])\n    sns.boxplot(train[train.weight !=0][feats], ax=axs[2])\n    fig.suptitle(feats, fontsize=15, y=1.1)\n    \n    axs[0].set_title('Distribution of feats')\n    axs[1].set_title('weight ! 0')\n    axs[2].set_title('Box Plot Distribution')\n    \n    \n    plt.tight_layout()\n    plt.show()","54ac5a80":"from sklearn.inspection import permutation_importance\n\ndef permutation_feature_selection(model,xtrain,ytrain,n_repeats=2):\n\n    '''\n    model : Empty model constructor of any ML algorithm.(e.g RandomForestRegressor(random_state=2020))\n    xtrain : Training dataset\n    ytrain : Output label of training dataset\n    n_repeats : The number of times the feature will be shuffled\n\n    '''\n\n\n    model.fit(xtrain,ytrain)\n    train_cols = []\n    val_cols = []\n\n    # Feature Selection on just Training data\n\n    r_train = permutation_importance(model,xtrain,ytrain,n_repeats=n_repeats,random_state=0)\n    for i in r_train.importances_mean.argsort()[::-1]:\n        train_cols.append(xtrain.columns[i])\n\n\n\n# Calling Code\nmodel = XGBClassifier(\n        n_estimators = 500,\n        max_depth=11,\n        learning_rate=0.06,\n        subsample=0.85,\n        colsample_bytree=0.6,\n        random_state=0,\n        tree_method='gpu_hist'\n)\n\n#Takes a lot of time\n#t_cols= permutation_feature_selection(model,train[num_features],train['action'])","3355e31b":"#Top 50 features based on Permutation feature Importance.\n#t_cols[:50]","5f842921":"As it is a Trading Dataset These extreme values may be a important factor to consider. So we will keep it.","7a9343f0":"![Jane Strret Market Prediction](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/c\/c9\/Jane_Street_Capital_Logo.svg\/1200px-Jane_Street_Capital_Logo.svg.png)","96d78d93":"## Handling Missing Values\n","5dff2fa0":"**So, In this competition we need to build a trading Model to find out the Maximum return from the stock Exchange. In the train data, each row is corrosponding to a trade opportuninty or action value. If the action value is 1 the it will be taken and if 0 it will pass on.**\n\n* If this notebbok really helps to get some little bit more insights of teh features and data please UPVOTE my work. I will update more on feature Selection.","65ebc9f0":"There are a total 14 features having Missing Values >10%\/ let's found out what are the best stratigies to fill up the missing Values.","775e4eb9":"* Insights\n\n1. One thing that is very Noticable that with our action(Traget feature) there is really very correlation with these feature swhich has a negative slope with trade.","691a20ab":"# Feature Selection","a3080435":"## Correlation of Different Time Horizons and Action","a20d1fc0":"Thanks @Carl for poiniting out this insight.\n[https:\/\/www.kaggle.com\/carlmcbrideellis\/jane-street-eda-of-day-0-and-feature-importance](http:\/\/)\n\nFrom the above plots we can observe the trends of the features whether it is linear in nature or noisy pattern.\n*  Considering teh whole data,OneImportant thing to be noticed here, that feature 3,4,5,6,73,75,76,77,79,82 the trade is decreasing where as for others there is a increasing trends(Curvilinear and Linear Patterns).\n\n* We can sort out the Noisy features as a different subset to the model.","ad52ab54":"Let's Analyze the Top 50 features distribution over the span of Data","ee360754":"weight is completely left skewed whreas resp has a normal distribution. Let's find out the different time horizons distributions and their differences","ddcccbff":"#### Let's found out the cummulative returns of 4 different Time Horizons","59836eb7":"Importing Necessary Libraries","a8dcd242":"## Outlier Analysis","bcc274bd":"---------------------","d20b1f31":"* One thing here to be noticed that Most of the features, (weight !=0) the distribution is very skewed.","5e10a05c":"Importing the Necesssary Datas","5743567d":"**Approach 1: Fill in Missing Values with -999 or -9999**\n\nIn this particular scenario the missing values are in random as the data is a real-time trading data. So this approch is appropiate for this dataset.","22aee2ea":"## Analyzing the Noisy Features","e4a17043":"Target Label is Balanced or Not?","6961fb40":"From the above plot it is visible that the target feature is balanced so we need not to worry  about the unbalances in the data. ","67fad329":"**Approach 2: Fill in Missing Values with mean replacement of each column**","1c06c373":"Understanding teh Column Types and Missing Values Imputation Techniques","4a0226e9":"## Distribution of Different Time Horizons and Action ","449b8b5e":"If you uncomment the below code It will take a lot of time. "}}