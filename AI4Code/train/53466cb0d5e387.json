{"cell_type":{"1072f9cc":"code","4fa264ab":"code","dcd35ee9":"code","e6067a3b":"code","8e2fb085":"code","0bc16bc8":"code","3ec22b2a":"code","ec108859":"code","5e48d237":"code","09a54028":"code","c65acd32":"code","ca4e6850":"code","c24a5ab7":"code","dbfd089c":"code","bffcf414":"code","eb3ec2e4":"code","deb636a9":"code","da653088":"code","96ead53f":"code","2abb291a":"code","fef4fa92":"code","168e9de1":"code","fe355e4b":"code","b633f15e":"code","2ce03b45":"code","7de99b2a":"code","e0805b43":"code","1ad0cdc4":"code","6528f0f8":"code","71baa319":"code","0251c830":"code","9020c76e":"code","4e6ff89d":"code","05392d08":"code","b6be62ab":"code","b1f8e92d":"code","041338d1":"code","28c71477":"code","26cd9794":"code","67c2a937":"code","36107484":"code","26bf21c5":"code","25d60ab1":"code","c5e30c72":"code","cdd1e911":"code","be75b5a3":"code","d9cc6dca":"code","e015c26e":"code","7f0621d7":"code","3f147ba3":"code","e51bba38":"code","9243bc71":"code","66af4ceb":"code","15563990":"code","bc140b7f":"code","e434efb2":"code","24b106e1":"code","8103c09f":"code","ab46e924":"code","0aa92b8d":"code","0d992b96":"code","5e1b75a2":"code","ececfaa5":"code","c6484ff4":"code","54d01e8b":"code","efe100e3":"code","005f0c5c":"code","a6d650e6":"code","35ffa390":"code","a95b4b98":"code","14138885":"code","b11cff2d":"code","4e1595d9":"code","c3538356":"code","f9e952b3":"code","71f23c14":"code","c37951f8":"code","84206ad9":"code","3495de23":"code","06c2a04f":"code","86577945":"code","4f0eaabe":"code","1b0b1cd3":"code","9d5ae9ca":"code","91c28960":"code","37b54171":"markdown","846cde98":"markdown","4baa8acb":"markdown","0f78a24a":"markdown","554a1d3a":"markdown","6e1266a9":"markdown","528e7f71":"markdown","f6bb2dd0":"markdown","d6bd04b0":"markdown","c8484e4d":"markdown","10bc6007":"markdown","da6c0ff5":"markdown","4a2eec90":"markdown","e044ba53":"markdown","a1e0b0e7":"markdown","05f9c4f8":"markdown","4266f5ed":"markdown","d4550eb2":"markdown","bda6f719":"markdown","da7e8e08":"markdown","8eb17eb6":"markdown","23c79744":"markdown","d968bc35":"markdown","17993d09":"markdown","e20d066f":"markdown","fe9a732f":"markdown","37c22148":"markdown","e03fec78":"markdown","1c21fe95":"markdown","1d13a926":"markdown","1f6b74ed":"markdown","729bbb3f":"markdown","3301d198":"markdown","119a8f57":"markdown","f50b52ee":"markdown","25d7d233":"markdown","89d1d5f6":"markdown","2013f90f":"markdown","27993819":"markdown","c9885af6":"markdown","83bf7210":"markdown","21939c81":"markdown","7745f3fd":"markdown","5a3e2905":"markdown","b4fd2a36":"markdown","eca03fde":"markdown"},"source":{"1072f9cc":"\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","4fa264ab":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nimport seaborn as sns","dcd35ee9":"Data = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")\nData.head()","e6067a3b":"from sklearn.model_selection import train_test_split\ntrain, test =train_test_split(Data,test_size=0.2,random_state=0)\nprint(\"train : {} , test : {}\".format(train.shape, test.shape))","8e2fb085":"y_train = train[\"target\"]\nX_train = train.drop(\"target\",axis=1)\ny_test = test[\"target\"]\nX_test = test.drop('target',axis=1)","0bc16bc8":"X_train.isnull().sum()","3ec22b2a":"X_test.isnull().sum()","ec108859":"plt.figure(figsize=(20,20))\ni=1\nfor elt in X_train.columns:\n    plt.subplot(4,4,i)\n    X_train[elt].hist(bins=20)\n    plt.xlabel(elt)\n    i+=1\nplt.show()","5e48d237":"features=[\"sex\",\"cp\",\"fbs\",\"restecg\", \"exang\",\"slope\", 'ca', 'thal']\ni=1\nplt.figure(figsize=(10,20))\nfor feature in features:\n    plt.subplot(4,2,i)\n    vals = np.sort(X_train[feature].unique()).tolist()\n    for v in vals:\n        s_b = X_train[X_train[feature]==v]\n        s_b2 = y_train[s_b.index]\n        a=s_b2[s_b2==1].shape[0]\n        b=s_b2[s_b2==0].shape[0]\n        plt.bar(v - 0.125, a, color = 'r', width = 0.25)\n        plt.bar(v + 0.125, b, color = 'g', width = 0.25)\n    plt.xlabel(feature)\n    plt.legend([\"Has Heart Disease\",\"Healthy\"])\n    i+=1\nplt.show()","09a54028":"c_features=[\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\"]\nplt.figure(figsize=(10,25))\ni=1\nfor idx,elt in enumerate(c_features):\n    f= idx + 1\n    while f < len(c_features):\n        plt.subplot(5,2,i)\n        sns.scatterplot(X_train[c_features[f]],X_train[elt],hue = y_train,\n                 palette=['green','red'],legend='full')\n        plt.xlabel(c_features[f])\n        plt.ylabel(elt)\n        plt.legend()\n        f+=1\n        i+=1\nplt.show()","c65acd32":"from scipy.stats import skew","ca4e6850":"for elt in c_features: \n    print(elt, skew(X_train[elt]))","c24a5ab7":"features_to_str=[\"cp\",\"restecg\", \"slope\", 'ca', 'thal']\nfor elt in features_to_str:\n    X_train[elt] = X_train[elt].apply(str)\n    X_test[elt] = X_test[elt].apply(str)","dbfd089c":"X_train = pd.get_dummies(data=X_train, prefix=features_to_str, \n                        prefix_sep='=', columns=features_to_str)\nX_train","bffcf414":"X_test = pd.get_dummies(data=X_test, prefix=features_to_str, \n                        prefix_sep='=', columns=features_to_str)\nX_test","eb3ec2e4":"for elt in X_train.columns : \n    if elt not in X_test.columns: \n        X_test[elt]= np.zeros(len(X_test))\n        print(elt)\nprint(\"X_train: {}, X_test: {}\".format(X_train.shape, X_test.shape))","deb636a9":"X_test = X_test[X_train.columns]\nX_test","da653088":"features_to_log = [\"chol\", \"oldpeak\"]\nfor elt in features_to_log:\n    X_train[elt] = np.log(1+ X_train[elt])\n    X_test[elt] = np.log(1+ X_test[elt])","96ead53f":"from sklearn.preprocessing import StandardScaler\ndata_scaler = StandardScaler()\ndata_scaler.fit(X_train[c_features])\nX_train[c_features] = data_scaler.transform(X_train[c_features])\nX_test[c_features] = data_scaler.transform(X_test[c_features])\nX_train.head()","2abb291a":"X_test.head()","fef4fa92":"from sklearn.decomposition import PCA","168e9de1":"pca = PCA(n_components=None)\npca.fit(X_train)\npca_components = pd.DataFrame(pca.explained_variance_ratio_,columns=['Data Variance per Component'])\npca_components['Total Captured Variance'] = pca_components['Data Variance per Component'].cumsum()","fe355e4b":"pca_components","b633f15e":"pca= PCA(n_components=10)\nX_train_pca = pca.fit_transform(X_train)\nX_test_pca = pca.transform(X_test)","2ce03b45":"from sklearn.decomposition import KernelPCA\nkpca = KernelPCA(kernel='rbf', n_components = 10)\nX_train_kpca = kpca.fit_transform(X_train)\nX_test_kpca = kpca.transform(X_test)","7de99b2a":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nlda = LinearDiscriminantAnalysis(n_components=1)\nlda.fit(X_train,y_train)\nX_train_lda= lda.transform(X_train)\nX_test_lda = lda.transform(X_test)","e0805b43":"from sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import accuracy_score, recall_score, make_scorer","1ad0cdc4":"def CAP_performance(y_true,y_pred,p=0.5,plot=False):\n    df =pd.DataFrame()\n    df[\"GT\"] = y_true\n    df[\"Predictions\"]=y_pred\n    positive_percentage = df[\"GT\"].sum() \/ len(df)\n    df.sort_values(by=[\"Predictions\"],axis=0,ascending=False, inplace=True)\n    df.reset_index(inplace=True,drop=True)\n    df['CumPredictions'] = df[\"GT\"].cumsum()\n    df['CumAcc']=df['CumPredictions'] \/ len(df)\n    idx = int(np.trunc(p*len(df)))\n    CAP=(df['CumAcc'].values[idx]+df['CumAcc'].values[idx+1])\/2\n    if plot: \n        plt.figure()\n        #random line : \n        plt.plot([0,len(df)],[0,100],color='black',label=\"Random Model\")\n        #perfect model : \n        plt.plot([0,df['GT'].sum(),len(df)],[0,100,100],color='green',label=\"Cristal Ball Model\")\n        #our model:\n        x=list(range(len(df)+1))\n        y= [0]+ (df['CumAcc']*100\/positive_percentage).values.tolist()\n        plt.plot(x,y,color='red',label=\"Model Performance\")\n        plt.plot([p*len(df),p*len(df),0],[0,CAP*100\/positive_percentage,CAP*100\/positive_percentage],\n                 color='blue',label='{} %'.format(CAP*100\/positive_percentage))\n        plt.xlim(0,len(df)+1)\n        plt.ylim(0,101)\n        plt.legend()\n        plt.show()\n    return CAP\/positive_percentage","6528f0f8":"performances = {}\nperformances[\"Method\"]=[]\nperformances[\"Healthy_Recall\"]=[]\nperformances[\"Disease_Recall\"]=[]\nperformances[\"CAP\"]=[]","71baa319":"from sklearn.linear_model import LogisticRegression\nLR = LogisticRegression(random_state=0)\nLR_pca = LogisticRegression(random_state=0)\nLR_kpca = LogisticRegression(random_state=0)\nLR_lda = LogisticRegression(random_state=0)","0251c830":"LR.fit(X_train,y_train)\ny_pred = LR.predict(X_test)\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\ncap = CAP_performance(y_test,y_pred,plot=True)","9020c76e":"performances[\"Method\"].append(\"Logistic Regression\")\nperformances[\"Healthy_Recall\"].append(recall_score(y_test,y_pred,average=None)[0])\nperformances[\"Disease_Recall\"].append(recall_score(y_test,y_pred,average=None)[1])\nperformances[\"CAP\"].append(cap)","4e6ff89d":"LR_pca.fit(X_train_pca,y_train)\ny_pred = LR_pca.predict(X_test_pca)\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\ncap = CAP_performance(y_test,y_pred,plot=True)","05392d08":"performances[\"Method\"].append(\"Logistic Regression PCA\")\nperformances[\"Healthy_Recall\"].append(recall_score(y_test,y_pred,average=None)[0])\nperformances[\"Disease_Recall\"].append(recall_score(y_test,y_pred,average=None)[1])\nperformances[\"CAP\"].append(cap)","b6be62ab":"LR_kpca.fit(X_train_kpca,y_train)\ny_pred = LR_kpca.predict(X_test_kpca)\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\ncap = CAP_performance(y_test,y_pred,plot=True)","b1f8e92d":"performances[\"Method\"].append(\"Logistic Regression K_PCA\")\nperformances[\"Healthy_Recall\"].append(recall_score(y_test,y_pred,average=None)[0])\nperformances[\"Disease_Recall\"].append(recall_score(y_test,y_pred,average=None)[1])\nperformances[\"CAP\"].append(cap)","041338d1":"LR_lda.fit(X_train_lda,y_train)\ny_pred = LR_lda.predict(X_test_lda)\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\ncap = CAP_performance(y_test,y_pred,plot=True)","28c71477":"performances[\"Method\"].append(\"Logistic Regression LDA\")\nperformances[\"Healthy_Recall\"].append(recall_score(y_test,y_pred,average=None)[0])\nperformances[\"Disease_Recall\"].append(recall_score(y_test,y_pred,average=None)[1])\nperformances[\"CAP\"].append(cap)","26cd9794":"from sklearn.svm import SVC\nSVM = SVC(C=0.5,gamma=0.1,random_state=0)\nSVM_pca = SVC(C=0.5,gamma=0.1,random_state=0)\nSVM_kpca = SVC(C=0.5,gamma=0.1,random_state=0)\nSVM_lda = SVC(C=0.5,gamma=0.1,random_state=0)","67c2a937":"SVM.fit(X_train,y_train)\ny_pred = SVM.predict(X_test)\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\ncap = CAP_performance(y_test,y_pred,plot=True)","36107484":"performances[\"Method\"].append(\"SVM\")\nperformances[\"Healthy_Recall\"].append(recall_score(y_test,y_pred,average=None)[0])\nperformances[\"Disease_Recall\"].append(recall_score(y_test,y_pred,average=None)[1])\nperformances[\"CAP\"].append(cap)","26bf21c5":"SVM_pca.fit(X_train_pca,y_train)\ny_pred = SVM_pca.predict(X_test_pca)\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\ncap = CAP_performance(y_test,y_pred,plot=True)","25d60ab1":"performances[\"Method\"].append(\"SVM PCA\")\nperformances[\"Healthy_Recall\"].append(recall_score(y_test,y_pred,average=None)[0])\nperformances[\"Disease_Recall\"].append(recall_score(y_test,y_pred,average=None)[1])\nperformances[\"CAP\"].append(cap)","c5e30c72":"SVM_kpca.fit(X_train_kpca,y_train)\ny_pred = SVM_kpca.predict(X_test_kpca)\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\ncap = CAP_performance(y_test,y_pred,plot=True)","cdd1e911":"performances[\"Method\"].append(\"SVM K_PCA\")\nperformances[\"Healthy_Recall\"].append(recall_score(y_test,y_pred,average=None)[0])\nperformances[\"Disease_Recall\"].append(recall_score(y_test,y_pred,average=None)[1])\nperformances[\"CAP\"].append(cap)","be75b5a3":"SVM_lda.fit(X_train_lda,y_train)\ny_pred = SVM_lda.predict(X_test_lda)\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\ncap = CAP_performance(y_test,y_pred,plot=True)","d9cc6dca":"performances[\"Method\"].append(\"SVM LDA\")\nperformances[\"Healthy_Recall\"].append(recall_score(y_test,y_pred,average=None)[0])\nperformances[\"Disease_Recall\"].append(recall_score(y_test,y_pred,average=None)[1])\nperformances[\"CAP\"].append(cap)","e015c26e":"from sklearn.neighbors import KNeighborsClassifier\nKNN= KNeighborsClassifier(n_neighbors=5)\nKNN_pca= KNeighborsClassifier(n_neighbors=5)\nKNN_kpca= KNeighborsClassifier(n_neighbors=5)\nKNN_lda= KNeighborsClassifier(n_neighbors=5)","7f0621d7":"KNN.fit(X_train,y_train)\ny_pred = KNN.predict(X_test)\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\ncap = CAP_performance(y_test,y_pred,plot=True)","3f147ba3":"performances[\"Method\"].append(\"KNN 5\")\nperformances[\"Healthy_Recall\"].append(recall_score(y_test,y_pred,average=None)[0])\nperformances[\"Disease_Recall\"].append(recall_score(y_test,y_pred,average=None)[1])\nperformances[\"CAP\"].append(cap)","e51bba38":"KNN_pca.fit(X_train_pca,y_train)\ny_pred = KNN_pca.predict(X_test_pca)\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\ncap = CAP_performance(y_test,y_pred,plot=True)","9243bc71":"performances[\"Method\"].append(\"KNN 5 PCA\")\nperformances[\"Healthy_Recall\"].append(recall_score(y_test,y_pred,average=None)[0])\nperformances[\"Disease_Recall\"].append(recall_score(y_test,y_pred,average=None)[1])\nperformances[\"CAP\"].append(cap)","66af4ceb":"KNN_kpca.fit(X_train_kpca,y_train)\ny_pred = KNN_kpca.predict(X_test_kpca)\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\ncap = CAP_performance(y_test,y_pred,plot=True)","15563990":"performances[\"Method\"].append(\"KNN 5 K_PCA\")\nperformances[\"Healthy_Recall\"].append(recall_score(y_test,y_pred,average=None)[0])\nperformances[\"Disease_Recall\"].append(recall_score(y_test,y_pred,average=None)[1])\nperformances[\"CAP\"].append(cap)","bc140b7f":"KNN_lda.fit(X_train_lda,y_train)\ny_pred = KNN_lda.predict(X_test_lda)\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\ncap = CAP_performance(y_test,y_pred,plot=True)","e434efb2":"performances[\"Method\"].append(\"KNN 5 LDA\")\nperformances[\"Healthy_Recall\"].append(recall_score(y_test,y_pred,average=None)[0])\nperformances[\"Disease_Recall\"].append(recall_score(y_test,y_pred,average=None)[1])\nperformances[\"CAP\"].append(cap)","24b106e1":"from sklearn.tree import DecisionTreeClassifier\nDTC = DecisionTreeClassifier(criterion='entropy',random_state=0)\nDTC_pca = DecisionTreeClassifier(criterion='entropy',random_state=0)\nDTC_kpca = DecisionTreeClassifier(criterion='entropy',random_state=0)\nDTC_lda = DecisionTreeClassifier(criterion='entropy',random_state=0)","8103c09f":"DTC.fit(X_train,y_train)\ny_pred = DTC.predict(X_test)\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\ncap = CAP_performance(y_test,y_pred,plot=True)","ab46e924":"performances[\"Method\"].append(\"DTC\")\nperformances[\"Healthy_Recall\"].append(recall_score(y_test,y_pred,average=None)[0])\nperformances[\"Disease_Recall\"].append(recall_score(y_test,y_pred,average=None)[1])\nperformances[\"CAP\"].append(cap)","0aa92b8d":"DTC_pca.fit(X_train_pca,y_train)\ny_pred = DTC_pca.predict(X_test_pca)\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\ncap = CAP_performance(y_test,y_pred,plot=True)","0d992b96":"performances[\"Method\"].append(\"DTC PCA\")\nperformances[\"Healthy_Recall\"].append(recall_score(y_test,y_pred,average=None)[0])\nperformances[\"Disease_Recall\"].append(recall_score(y_test,y_pred,average=None)[1])\nperformances[\"CAP\"].append(cap)","5e1b75a2":"DTC_kpca.fit(X_train_kpca,y_train)\ny_pred = DTC_kpca.predict(X_test_kpca)\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\ncap = CAP_performance(y_test,y_pred,plot=True)","ececfaa5":"performances[\"Method\"].append(\"DTC K_PCA\")\nperformances[\"Healthy_Recall\"].append(recall_score(y_test,y_pred,average=None)[0])\nperformances[\"Disease_Recall\"].append(recall_score(y_test,y_pred,average=None)[1])\nperformances[\"CAP\"].append(cap)","c6484ff4":"DTC_lda.fit(X_train_lda,y_train)\ny_pred = DTC_lda.predict(X_test_lda)\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\ncap = CAP_performance(y_test,y_pred,plot=True)","54d01e8b":"performances[\"Method\"].append(\"DTC LDA\")\nperformances[\"Healthy_Recall\"].append(recall_score(y_test,y_pred,average=None)[0])\nperformances[\"Disease_Recall\"].append(recall_score(y_test,y_pred,average=None)[1])\nperformances[\"CAP\"].append(cap)","efe100e3":"from sklearn.ensemble import RandomForestClassifier\nRFC = RandomForestClassifier(n_estimators = 150 , criterion='entropy', random_state=0)\nRFC_pca = RandomForestClassifier(n_estimators = 150 , criterion='entropy', random_state=0)\nRFC_kpca = RandomForestClassifier(n_estimators = 150 , criterion='entropy', random_state=0)\nRFC_lda = RandomForestClassifier(n_estimators = 150 , criterion='entropy', random_state=0)","005f0c5c":"RFC.fit(X_train,y_train)\ny_pred = RFC.predict(X_test)\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\ncap = CAP_performance(y_test,y_pred,plot=True)","a6d650e6":"performances[\"Method\"].append(\"RF\")\nperformances[\"Healthy_Recall\"].append(recall_score(y_test,y_pred,average=None)[0])\nperformances[\"Disease_Recall\"].append(recall_score(y_test,y_pred,average=None)[1])\nperformances[\"CAP\"].append(cap)","35ffa390":"RFC_pca.fit(X_train_pca,y_train)\ny_pred = RFC_pca.predict(X_test_pca)\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\ncap = CAP_performance(y_test,y_pred,plot=True)","a95b4b98":"performances[\"Method\"].append(\"RF PCA\")\nperformances[\"Healthy_Recall\"].append(recall_score(y_test,y_pred,average=None)[0])\nperformances[\"Disease_Recall\"].append(recall_score(y_test,y_pred,average=None)[1])\nperformances[\"CAP\"].append(cap)","14138885":"RFC_kpca.fit(X_train_kpca,y_train)\ny_pred = RFC_kpca.predict(X_test_kpca)\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\ncap = CAP_performance(y_test,y_pred,plot=True)","b11cff2d":"performances[\"Method\"].append(\"RF K_PCA\")\nperformances[\"Healthy_Recall\"].append(recall_score(y_test,y_pred,average=None)[0])\nperformances[\"Disease_Recall\"].append(recall_score(y_test,y_pred,average=None)[1])\nperformances[\"CAP\"].append(cap)","4e1595d9":"RFC_lda.fit(X_train_lda,y_train)\ny_pred = RFC_lda.predict(X_test_lda)\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\ncap = CAP_performance(y_test,y_pred,plot=True)","c3538356":"performances[\"Method\"].append(\"RF LDA\")\nperformances[\"Healthy_Recall\"].append(recall_score(y_test,y_pred,average=None)[0])\nperformances[\"Disease_Recall\"].append(recall_score(y_test,y_pred,average=None)[1])\nperformances[\"CAP\"].append(cap)","f9e952b3":"from xgboost import XGBClassifier\nXGC = XGBClassifier(n_estimators = 150 , random_state=0)\nXGC_pca = XGBClassifier(n_estimators = 150 , random_state=0)\nXGC_kpca = XGBClassifier(n_estimators = 150 , random_state=0)\nXGC_lda = XGBClassifier(n_estimators = 150 , random_state=0)","71f23c14":"XGC.fit(X_train,y_train)\ny_pred = XGC.predict(X_test)\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\ncap = CAP_performance(y_test,y_pred,plot=True)","c37951f8":"performances[\"Method\"].append(\"XGB\")\nperformances[\"Healthy_Recall\"].append(recall_score(y_test,y_pred,average=None)[0])\nperformances[\"Disease_Recall\"].append(recall_score(y_test,y_pred,average=None)[1])\nperformances[\"CAP\"].append(cap)","84206ad9":"XGC_pca.fit(X_train_pca,y_train)\ny_pred = XGC_pca.predict(X_test_pca)\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\ncap = CAP_performance(y_test,y_pred,plot=True)","3495de23":"performances[\"Method\"].append(\"XGB PCA\")\nperformances[\"Healthy_Recall\"].append(recall_score(y_test,y_pred,average=None)[0])\nperformances[\"Disease_Recall\"].append(recall_score(y_test,y_pred,average=None)[1])\nperformances[\"CAP\"].append(cap)","06c2a04f":"XGC_kpca.fit(X_train_kpca,y_train)\ny_pred = XGC_kpca.predict(X_test_kpca)\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\ncap = CAP_performance(y_test,y_pred,plot=True)","86577945":"performances[\"Method\"].append(\"XGB K_PCA\")\nperformances[\"Healthy_Recall\"].append(recall_score(y_test,y_pred,average=None)[0])\nperformances[\"Disease_Recall\"].append(recall_score(y_test,y_pred,average=None)[1])\nperformances[\"CAP\"].append(cap)","4f0eaabe":"XGC_lda.fit(X_train_lda,y_train)\ny_pred = XGC_lda.predict(X_test_lda)\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\ncap = CAP_performance(y_test,y_pred,plot=True)","1b0b1cd3":"performances[\"Method\"].append(\"XGB LDA\")\nperformances[\"Healthy_Recall\"].append(recall_score(y_test,y_pred,average=None)[0])\nperformances[\"Disease_Recall\"].append(recall_score(y_test,y_pred,average=None)[1])\nperformances[\"CAP\"].append(cap)","9d5ae9ca":"performances_df = pd.DataFrame(performances)\nperformances_df","91c28960":"plt.figure(figsize=(5,15))\nplt.subplot(3,1,1)\nplt.plot(performances_df[\"Healthy_Recall\"])\nplt.xticks(performances_df.index,performances_df[\"Method\"],rotation='vertical')\nplt.xlabel(\"Models\")\nplt.ylabel(\"Recall of Healthy Data Points\")\nplt.grid()\nplt.subplot(3,1,2)\nplt.plot(performances_df[\"Disease_Recall\"])\nplt.xticks(performances_df.index,performances_df[\"Method\"],rotation='vertical')\nplt.xlabel(\"Models\")\nplt.ylabel(\"Recall of Diseased Data Points\")\nplt.grid()\nplt.subplot(3,1,3)\nplt.plot(performances_df[\"CAP\"])\nplt.xticks(performances_df.index,performances_df[\"Method\"],rotation='vertical')\nplt.xlabel(\"Models\")\nplt.ylabel(\"Cumulative Accuracy Profile\")\nplt.grid()\nplt.tight_layout(.5)\nplt.show()","37b54171":"* Transforming the data type to str.","846cde98":"## Random Forest","4baa8acb":"So we note that two features are skewed to the right (i.e., chol and oldpeak). These two variables will be transformed with a log function so that we can reduce their skewness.","0f78a24a":"Now for the features that look like a categorical features let's draw the Bar plots. ","554a1d3a":"* Let's scale data\n\nSome classification methods specially similarity-based like K-Nearest Neighbors are sensitive to the scale of features. for this, we will transform the numeric variables to the same scale using a standard transformation. \n\nIt is important to make sure that we will only scale the continuous data meaning that categorical data and dummy variables will not be scaled as they are already in an [0,1] interval. It is also important to make sure that the scaler will be fitted to only the training data and then applied to both training and testing data to avoid data leakage.","6e1266a9":"* 2D Scatter Plots:","528e7f71":"### Feature Engineering","f6bb2dd0":"Now for continuous variables let's see how they are related to each other. ","d6bd04b0":"#### Linear Discriminant Analysis (LDA)","c8484e4d":"The principal component analysis is an unsupervised method for feature engineering. It is unsupervised because it won't consider the effects of the transformation on the target value. PCA creates new features that are a linear combination of the old once. It identifies the patterns in data, detects correlations and tries to reduce dimensionality while catching most of the variance. The number of returned new features that we technically call components is the most important parameter of the approach. At this point, we will retrieve all components (the same number of features). We will analyze this output then we will decide on the number of components.","10bc6007":"Let's start by visualizing the data:\n* Histograms:","da6c0ff5":"To see the predictions we will use the confusion matrix, and we will use the full report on classification. ","4a2eec90":"Now let's apply the decision we made. ","e044ba53":"# Model Building & Predictions:","a1e0b0e7":"Now let's first start by defining the performance metrics. We have here a two-class classification problem. We can use several metrics like precision, accuracy, F1 score, etc...  \n\n1.     Accuracy Classification accuracy is our starting point. It is the number of correct predictions made divided by the total number of predictions made, multiplied by 100 to turn it into a percentage.\n1.     Precision is the number of True Positives divided by the number of True Positives and False Positives. Put another way, it is the number of positive predictions divided by the total number of positive class values predicted. It is also called the Positive Predictive Value (PPV).\n1.     Sensitivity is the number of True Positives divided by the number of True Positives and the number of False Negatives. Put another way it is the number of positive predictions divided by the number of positive class values in the test data. It is also called Recall or the True Positive Rate.\n1.     Specificity\n\nUnlike the regression problems, the classification problem is a multi-objective problem. In a two-class classification problem, we find two types of errors: \n* False Positives: when the ground truth for the observation is class 0 or passive class (for example a healthy person is our case) and the model predicts this data point as class 1 or active class (in our case diagnosed with heart disease). Well, this type of error is important but not dangerous. In our case, for example, those who are diagnosed with heart disease will only require some small changes in their life, like a special food diet, should quit smoking, or prescribed some medications. I would like to think this kind of error is a warning. \n* False Negatives: when the ground truth for the observation is class 1 (diagnosed with heart disease) and the model predicts the patient as healthy (class 0). This type of error is extremely dangerous as it causes that a person who is ill or presents a risk of having health complications will not get the medical attention he needs and may even cause some health complications. \n\nTherefore, in this application, we will focus mainly on reducing the False-negative errors and thus maximizing the recall_score for class 1.","05f9c4f8":"## K-nearest neighbors","4266f5ed":"## Decision Tree","d4550eb2":"# Analyzing the Results","bda6f719":"Linear discriminant analysis is a supervised feature engineering method. Unlike the PCA, LDA will create new features that maximize the separation between the different classes. Since we have only two classes (has heart disease and healthy) then we can create only one component using the LDA.","da7e8e08":"* Now let's reduce the skewness of the numeric variables","8eb17eb6":"#### Principal Component Analysis (Kernel PCA)","23c79744":"## Support Vector Machines","d968bc35":"## Data Analysis","17993d09":"#### Principal Component Analysis (PCA)","e20d066f":"Now let's divide the data into train and test sets","fe9a732f":"## Extreme Gradient Boost","37c22148":"Kernel PCA works almost like the PCA but on nonlinear problems. At first, the algorithm will increase the dimension of data by applying a kernel transformation (e.g., RBF) to \"linearize\" the problem. Then it will apply the PCA. Kernel PCA has two important parameters, the kernel function (in this example we will use the Gaussian kernel RBF) and the number of components (similar to PCA we will use 10 components).","e03fec78":"* Getting the dummy variables.","1c21fe95":"### Categorical Features","1d13a926":"* Bar Plots: ","1f6b74ed":"We can notice that some of these categorical features have no ordering relationship. Meaning that value in the middle can have more patients diagnosed with heart disease than other values for example thal. \nIt would be wise to transform these features into categorical ones and then replace them with dummy variables.\n\nFrom these visualizations, we conclude that numeric categorical features with multiple values (i.e., cp, restecg, slope, ca, and thal) will be transformed into categorical features and will be replaced with dummy variables.","729bbb3f":"## Data Processing","3301d198":"This shows us the amount of variance we will catch with that number of component e.g., for one component we will have 0.257 of the data variance. Since our goal is to reduce the dimension of data we will choose only 10 components. so that we will be catching 89% of the data total variance.","119a8f57":"We can notice that the X_train has now 27 features but the X_test has only 25 features. This is due to the transformation of categorical features to dummy variables. Some categorical features of the test data do not have all the values as the train data. So we will add the missing columns with zero values.","f50b52ee":"### Numerical Features","25d7d233":"Here we will code the function to produce the cumulative accuracy profile. The cumulative accuracy profile (CAP) is a great metric to measure the performance of the model. Well, let's think of it this way if we have a percentage of people who are diagnosed ill from a set a population we want our model to detect them immediately and avoid us from recheck if the people he selected to be ill are not ill. Well, this is the perfect performance that one can hope for. It sounds like having a crystal ball and we will directly predict the subjects that will be diagnosed with heart disease. The closer the performance of our model to this behavior the better it is. However, if we choose a completely random model we will always have the same percentage of detected illness and we will have to check the whole population to detect all the ill patients. Well, this is the baseline for a prediction model if any model is even lower than the random it is completely rubbish. Now the CAP is computed as the ratio of the area between the cumulative accuracy of our model and the random line over the area between the perfect model and the random line. Since this is quite an expensive computation we can approximate this result by estimating the CAP for 50% of the population. Once we have this value there is a rule of thumb to determine the performance of the model as follow: \n* CAP(50%) < 60% very poor model \n* 60% < CAP(50%) < 70% poor model \n* 70% < CAP(50%) < 80% good model\n* 80% < CAP(50%) < 90% very good model \n* 90% < CAP(50%) a too good to be true model (one should check for forward seeing predictors i.e., predictors that describe directly the outcome and that we can only obtain by knowing the outcome, or this may be caused by overfitting the model, or simply it's a great model).","89d1d5f6":"Great now we have the same number of columns. One more thing before moving to the next step. Let's make sure that both X_train and X_test have the same order in the features.","2013f90f":"There is no missing values in the dataset.","27993819":"Let's create a data frame in which we will save all the performances of the models. For this we will create a dictionnary than we will convert it into a dataframe. ","c9885af6":"The recall_score function can provides us with the recall of each of the classes when using the option \"average=None\". Otherwise, since we are dealing with a binary (two-classes) classification problem then using the recall score directly will return only the recall score for the active class (1). ","83bf7210":"# Introduction \n\nIn this notebook, I took the UCI Heart Disease database to start a beginner guide to classification problems. I dealt with the data as if it was in a kaggle competition. Hence, I divided it into two sets: training and testing sets. The analysis of data and the adaptation of the processing phases were done considering only the training set. These modifications are then applied to the test set. This allows us to avoid having data leakage which is a common data science problem and a lot of notebooks in kaggle suffer from this problem. For those who do not know what the data leakage is. It's a problem when the test set data contribute to the processing and analysis phases. Thus, it will influence the model used and we will obtain great results. So what's the problem? We are trying to increase the performance of our models so what's the fuss? Well in real-life problems we won't have access to the testing data. We will obtain this data when the model is set in place to do its job. Therefore, including information that is available only in the test set will cause our model to be unrealistic.  \n\nAs I was saying I treated the data as if it was a kaggle competition. Therefore, I won't be using the cross-validation method for my built models. Of course, if you are interested to do a more precise work you can use the whole data and use cross-validation with some pipelines to process the data each time the cross-validation is done.\n\nI hope my explanations are precise and clear. Please leave your comments and your remark so I can improve the notebook. \n\n","21939c81":"## Logistic Regression","7745f3fd":"Let's check the skewness of the continuous variables!","5a3e2905":"# Data Analysis and Processing\n## Data Acquisition and Preparation","b4fd2a36":"Now our data is ready. Although we can still do some processing like feature engineering using Principal Component Analysis: PCA (for linear problems) and Kernel PCA (for nonlinear problems) and\/or Linear Discriminant Analysis. To compare the efficiency of these methods, we will apply them to our problem. ","eca03fde":"We can notice that the age, trestbps, chol, thalach, and oldpeak are continuous values."}}