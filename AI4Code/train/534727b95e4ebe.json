{"cell_type":{"bb93e600":"code","fc2054db":"code","579ca6de":"code","a29d688a":"code","badad130":"code","ccad0c1f":"code","a86e8a17":"code","cb9a5283":"code","810ec75d":"code","9de2dbf6":"markdown","109d8457":"markdown","91f3b883":"markdown","2de46947":"markdown","38d8ec25":"markdown"},"source":{"bb93e600":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","fc2054db":"import math\nimport array\nfrom IPython import display\nfrom matplotlib import cm\nfrom matplotlib import gridspec\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn import metrics\nimport tensorflow as tf\nfrom tensorflow.python.data import Dataset\nimport datetime\nimport random\nimport seaborn as sns\nfrom functools import reduce\nrng = np.random\ntf.logging.set_verbosity(tf.logging.ERROR)\npd.options.display.max_rows = 10\npd.options.display.float_format = '{:.2f}'.format","579ca6de":"train = pd.read_csv(\"..\/input\/train.csv\", sep=\",\")\ntest = pd.read_csv(\"..\/input\/test.csv\", sep=\",\")\nfull = train.append(test)\n\ntrain = train.reindex(np.random.permutation(train.index))\n#full.info()","a29d688a":"train.shape","badad130":"training = train.head(40000)\nvalidation = train.tail(2000)\n\ntraining_Y = training[\"label\"]\ntraining_X = training.drop(labels=[\"label\"], axis=1)\n\n\nvalidation_Y = validation[\"label\"]\nvalidation_X = validation.drop(labels=[\"label\"], axis=1)\n","ccad0c1f":"#\u8a2d\u5b9a\u5b78\u7fd2\u7387,\u5b78\u6b21\u6578\u7b49\u53c3\u6578\nlearning_rate = 0.001\ntraining_epochs = 100\ndisplay_step = 10\nbatch_size = 100\n\n#\u8a2d\u5b9aX\u7684\u8f38\u5165\u6578,\u795e\u7d93\u7db2\u7d61\u5c64\u7b49\u53c3\u6578\nn_inputs = 28*28 # MNIST\n#units = [900, 400, 100]\n#units = [900, 900, 900, 900]\nunits = [100,100,100,100,100,100, 100, 100, 100]\nn_outputs = 10\ndropout_rate = 0.5\n\n\nX = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\ny = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n\ntr = tf.placeholder_with_default(False, shape=(), name='training')\n\n#\u5faa\u74b0\u69cb\u5efa\u795e\u7d93\u7db2\u7d61\u5c64,\u6fc0\u6d3b\u51fd\u6578\u8a2d\u70barelu,\u4e26\u4f7f\u7528dropout\u51fd\u6578\u63d0\u9ad8\u8a13\u7df4\u6548\u679c\ndense = X\nhe_init = tf.keras.initializers.he_normal()\nfor unit in units:\n    dropout = tf.layers.dropout(dense, dropout_rate, training=tr)\n    dense = tf.layers.dense(inputs=dropout, units=unit, activation=tf.nn.relu)\nlogits= tf.layers.dense(inputs=dense, units=n_outputs, activation=None)\n\n#sparse_softmax_cross_entropy_with_logits\u51fd\u6578\u5c07\u6c42\u51falogits\u7684\u8f38\u51fa\u548c\u5be6\u969b\u7d50\u679c\u7684y\u4e4b\u9593\u7684\u4ea4\u53c9\u71b5,\u4ee5\u6b64\u4f5c\u70ba\u640d\u5931\u51fd\u6578\nxentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y,logits = logits)\nloss = tf.reduce_mean(xentropy, name='loss')\n\n#\u8a08\u7b97\u51falogits\u7684\u7d50\u679c\u548cy\u662f\u5426\u76f8\u540c,\u8a08\u7b97\u51fa\u6a21\u578b\u6e96\u78ba\u7387\ncorrect = tf.nn.in_top_k(logits ,y ,1)\naccuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n\n#\u7d93\u904e\u6e2c\u8a66\u5f8cAdamOptimizer\u7684\u7d50\u679c\u6700\u4f73,\u4ea6\u53ef\u5728\u6b64\u8655\u6539\u7528\u5176\u4ed6\u512a\u5316\u5668\u4f5c\u6e2c\u8a66\noptimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n#optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9).minimize(loss)\n#optimizer = tf.train.FtrlOptimizer(learning_rate).minimize(loss)\n\n\n\nthreshold = 1.0\n\ndef ClipIfNotNone(grad, threshold):\n    if grad is None:\n        return grad\n    return tf.clip_by_value(grad, -threshold, threshold)\noptimizer = tf.train.AdamOptimizer(learning_rate)\ngrads_and_vars = optimizer.compute_gradients(loss)\ncapped_gvs = [(ClipIfNotNone(grad, threshold), var)\n              for grad, var in grads_and_vars]\ntraining_op = optimizer.apply_gradients(capped_gvs)\noptimizer = training_op\n\ninit = tf.global_variables_initializer()\n","a86e8a17":"#\u958b\u59cb\u8a13\u7df4\nwith tf.Session() as sess:\n    sess.run(init)\n    for epoch in range(training_epochs):\n        for iteration in range(training.shape[0] \/\/ batch_size):\n            sample = training.sample(batch_size)\n            y_batch = sample[\"label\"]\n            X_batch = sample.drop(labels=[\"label\"], axis=1)\n            sess.run(optimizer, feed_dict={X: X_batch, y: y_batch})\n        if (epoch + 1) % display_step == 0:\n            a = sess.run(accuracy, feed_dict={X: training_X, y: training_Y})\n            va = sess.run(accuracy, feed_dict={X: validation_X, y: validation_Y})\n            print(\"Epoch:\", '%04d' % (epoch + 1), \"accuracy=\", a, \"v_accuracy=\", va)\n    print(\"Optimization Finished!\")\n    \n    l = sess.run(logits, feed_dict={X: test})\n    p = sess.run(tf.argmax(l, axis=1))","cb9a5283":"result = pd.DataFrame()\nresult['ImageId'] = test.index + 1\nresult['label'] = p\nresult.to_csv('result.csv', index=False)","810ec75d":"from IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\ndef create_download_link(df, title = \"Download CSV file\", filename = \"data.csv\"):  \n    csv = df.to_csv(index=False)\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n\n# create a link to download the dataframe\ncreate_download_link(result)","9de2dbf6":"\u65b0\u589e\u51fd\u6578\u5c07\u7d50\u679c\u76f4\u63a5\u4e0b\u8f09,\u653e\u4fbf\u8a13\u7df4\u5b8c\u6a21\u578b\u5f8c\u76f4\u63a5submit\u4f86\u6aa2\u8996\u8a13\u7df4\u6210\u679c","109d8457":"3.\u8f38\u51fa\u7d50\u679c","91f3b883":"2.\u4f7f\u7528\u6df1\u5ea6\u795e\u7d93\u7db2\u7d61\u5b78\u7fd2Mnist\u6578\u64da","2de46947":"\u5c07train\u6578\u64da\u5206\u70batraining\u548cvalidation\u5169\u7d44,validation\u7528\u65bc\u9a57\u8b49","38d8ec25":"1.Import Libary and Dataset \u5c0e\u5165\u5eab\u8207\u6578\u64da"}}