{"cell_type":{"511776b8":"code","b620b8c3":"code","45865a7f":"code","32d3134b":"code","de115289":"code","7b3bff8f":"code","97530d9c":"code","7bc31a5e":"code","dc74462d":"code","f1d96337":"code","92c6a307":"code","ddc1f798":"code","08f1148c":"code","f7034214":"code","cbf89e85":"code","c21d79a0":"code","2842c183":"code","d42a86e2":"code","5bbd6200":"code","92fbedfa":"code","67e8ba41":"code","c24e08f1":"code","a6871b65":"code","604b6d56":"code","e529c5c7":"code","30f1336c":"code","c729b93e":"code","2715e6a8":"code","7b7542ac":"code","4372ead2":"code","d89a8795":"code","412dc9af":"code","f8f761b2":"code","22ede612":"code","22ad81d9":"code","e597095b":"code","67ca5d0e":"code","fe6d2c1f":"code","b30b7ae9":"code","c970c62f":"code","8361b212":"code","eb8be4c2":"code","253adddb":"code","2baf8fcd":"code","10c2e166":"code","6c44aa00":"markdown","60e0c201":"markdown","120d6048":"markdown","736d70b3":"markdown","9c0292f1":"markdown","d58f7a39":"markdown","9480d8f5":"markdown","0569ce9a":"markdown","4a092c4d":"markdown","da5dfa02":"markdown","bf515571":"markdown","20a5c4db":"markdown","83fcadd3":"markdown","039e5ce7":"markdown"},"source":{"511776b8":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b620b8c3":"import pandas as pd\nimport numpy as np\nimport re\nimport plotly.express as px\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nstopwords = set(STOPWORDS)\nimport collections\nimport nltk\nfrom nltk.tokenize import sent_tokenize,word_tokenize\n\nnltk.download('punkt')","45865a7f":"train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","32d3134b":"train.head()","de115289":"train.describe()","7b3bff8f":"def cleanhtml(raw_html):\n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, '', raw_html)\n    return cleantext\n\ndef removeurl(raw_text):\n    clean_text = re.sub(r'^https?:\\\/\\\/.*[\\r\\n]*', '', raw_text, flags=re.MULTILINE)\n    return clean_text\n\n","97530d9c":"train['text'] = train['text'].apply(lambda x: cleanhtml(x))\ntest['text'] = test['text'].apply(lambda x: cleanhtml(x))\n\ntrain['text'] = train['text'].apply(lambda x:removeurl(x))\ntest['text'] = test['text'].apply(lambda x: removeurl(x))","7bc31a5e":"train['text'] = train['text'].map(lambda x: re.sub(r'\\W+', ' ', x))\ntest['text'] = test['text'].map(lambda x: re.sub(r'\\W+', ' ', x))\n","dc74462d":"train","f1d96337":"fig = px.bar(train.groupby('target')['target'].count().reset_index(name='count'), x='target', y='count',color='target')\nfig.show()","92c6a307":"data = train.groupby('location')['location'].count().reset_index(name='count').sort_values(by='count',ascending=False)","ddc1f798":"data = data.head(20)","08f1148c":"fig = px.bar(data, x='location', y='count',color='location')\nfig.show()","f7034214":"data = train.groupby(['location','target'])['location'].count().reset_index(name='count').sort_values(by='count',ascending=False)","cbf89e85":"data = data.head(20)","c21d79a0":"fig = px.bar(data, x=\"location\", y='count',color='target')\nfig.show()","2842c183":"\ndef show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=stopwords,\n        max_words=200,\n        max_font_size=40, \n        scale=3,\n        random_state=1 # chosen at random by flipping a coin; it was heads\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize=(12, 12))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=20)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()\n","d42a86e2":"show_wordcloud(train[train['target']==1]['text'])","5bbd6200":"show_wordcloud(train[train['target']==0]['text'])","92fbedfa":"data = train[train['target']==1].text.str.split(expand=True).stack().value_counts().reset_index(name='count')","67e8ba41":"data","c24e08f1":"fig = px.bar(data.head(50), x=\"count\", y=\"index\", orientation='h',color_discrete_sequence=px.colors.qualitative.Light24)\nfig.show()","a6871b65":"data = train[train['target']==0].text.str.split(expand=True).stack().value_counts().reset_index(name='count')","604b6d56":"fig = px.bar(data.head(50), x=\"count\", y=\"index\", orientation='h',color_discrete_sequence=px.colors.qualitative.Light24)\nfig.show()","e529c5c7":"# counts = collections.Counter()\n# for sent in train[train['target']==1]['text']: \n#         words = nltk.word_tokenize(sent)\n#         counts.update(nltk.bigrams(words))","30f1336c":"# data = pd.DataFrame.from_dict(counts, orient='index').reset_index()","c729b93e":"# data = data.sort_values(by=0,ascending=False)","2715e6a8":"# data.columns = ['Bi-gram','Count']","7b7542ac":"# data['Bi-gram'] = data['Bi-gram'].astype(str)\n# data['Count'] = data['Count'].astype(int)","4372ead2":"# fig = px.bar(data.head(50), x=\"Count\", y=\"Bi-gram\", orientation='h',color_discrete_sequence=px.colors.qualitative.Light24)\n# fig.show()","d89a8795":"# counts = collections.Counter()\n# for sent in train[train['target']==0]['text']:\n#     words = nltk.word_tokenize(sent)\n#     counts.update(nltk.bigrams(words))\n# data = pd.DataFrame.from_dict(counts, orient='index').reset_index()\n# data = data.sort_values(by=0,ascending=False)\n# data.columns = ['Bi-gram','Count']\n# data['Bi-gram'] = data['Bi-gram'].astype(str)\n# data['Count'] = data['Count'].astype(int)\n# fig = px.bar(data.head(70), x=\"Count\", y=\"Bi-gram\", orientation='h',color_discrete_sequence=px.colors.qualitative.Light24)\n# fig.show()","412dc9af":"#Creating Meta Features","f8f761b2":"train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","22ede612":"train['text'] = train['text'].apply(lambda x: cleanhtml(x))\ntest['text'] = test['text'].apply(lambda x: cleanhtml(x))\n\ntrain['text'] = train['text'].apply(lambda x:removeurl(x))\ntest['text'] = test['text'].apply(lambda x: removeurl(x))","22ad81d9":"train['text'] = train['text'].astype(str)\ntest['text'] = test['text'].astype(str)","e597095b":"def number_of_words(tweet):\n  return len(word_tokenize(tweet))\n#number_of_words('Our Deeds are')","67ca5d0e":"train['Number_of_words'] = train['text'].apply(lambda x: number_of_words(x))\ntest['Number_of_words'] = test['text'].apply(lambda x: number_of_words(x))\n","fe6d2c1f":"def number_of_sentences(tweet):\n\n   return len(sent_tokenize(tweet))\n","b30b7ae9":"train['Number_of_Sentences'] = train['text'].apply(lambda x: number_of_sentences(x))\ntest['Number_of_Sentences'] = test['text'].apply(lambda x: number_of_sentences(x))\n","c970c62f":"def Number_of_Unique_Words(tweet):\n  return len(set(tweet.split()))","8361b212":"train['Number_of_Unique_Words'] = train['text'].apply(lambda x: Number_of_Unique_Words(x))\ntest['Number_of_Unique_Words'] = test['text'].apply(lambda x: Number_of_Unique_Words(x))\n","eb8be4c2":"def Number_of_Stop_Words(tweet):\n  word_tokens = word_tokenize(tweet) #splitta i pezzi\n\n  stopwords_x = [w for w in word_tokens if w in STOPWORDS]\n\n  return len(stopwords_x)","253adddb":"train['Number_of_Stop_Words'] = train['text'].apply(lambda x: Number_of_Stop_Words(x))\ntest['Number_of_Stop_Words'] = test['text'].apply(lambda x: Number_of_Stop_Words(x))\n","2baf8fcd":"train","10c2e166":"test","6c44aa00":"test.head()","60e0c201":"Word Frequency ","120d6048":"Setup","736d70b3":"Tweet count grouped by location","9c0292f1":"Word Cloud","d58f7a39":"Tweets distribution per location","9480d8f5":"#Distributio of Target Column","0569ce9a":"Citation","4a092c4d":"- Evident from word cloud that words belonging to different targets differ greatly\n\n- Words Occuring in Target class 1\n  - Fire \n  - Earthquake\n  - people\n  - Shelter\n  - evacuation\n\n- Words Occuring in Target class 0\n  - Summer\n  - fruits\n  - lovely\n  - Disney","da5dfa02":"Reading the data","bf515571":"End of citation","20a5c4db":"- Number of words\n- Number of Sentences\n- Number of Unique Words\n- Number of Stop words\n","83fcadd3":"Author - Mihir Ahuja\n\nData Citation - https:\/\/www.kaggle.com\/c\/nlp-getting-started\n\nOther Citation - Followed Steps by ratan123 ( Ratan's notebook is a great starting guide for this dataset)\n Citaion For  https:\/\/www.kaggle.com\/ratan123\/start-from-here-disaster-tweets-eda-basic-model (Snippets used)\n\nOther Citation - Citaion For  https:\/\/www.kaggle.com\/ratan123\/start-from-here-disaster-tweets-eda-basic-model (Snippets used)","039e5ce7":"Bi-Grams Count"}}