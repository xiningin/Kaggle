{"cell_type":{"f6d4641c":"code","873963d8":"code","723e20d1":"code","47de82a4":"code","20d93662":"code","9cc5c7c2":"code","ebd9eced":"code","f5cf90d4":"code","dc67c5d6":"code","3a94da41":"code","15c5dde1":"code","c48e9425":"code","336c636e":"code","67e832d6":"code","1658bbb6":"code","7c665624":"code","beb8e366":"code","346ad370":"code","fe3ab287":"code","e1c096d9":"code","37f4cbe8":"code","60e25cd8":"code","da81de43":"code","af6f3871":"code","29b8416a":"markdown","85f7e982":"markdown","0fe98dde":"markdown","2ed671fb":"markdown","9d9d6837":"markdown","5ed71ae7":"markdown","3287a5ee":"markdown","cca0baf3":"markdown","cb35959c":"markdown","1e975155":"markdown","1357efb5":"markdown","8f8ac5dc":"markdown"},"source":{"f6d4641c":"%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom ipywidgets import interact, interactive, fixed, interact_manual,FloatSlider","873963d8":"uniform = lambda x: (np.abs(x) <= 1) and 1\/2 or 0\ntriangle = lambda x: (np.abs(x) <= 1) and  (1 - np.abs(x)) or 0\ngaussian = lambda x: (1.0\/np.sqrt(2*np.pi))* np.exp(-.5*x**2) ","723e20d1":"plt.rcParams['figure.figsize'] = [20, 5]\nplt.subplot(1, 3, 1)\nplt.title('Uniform')\nplt.plot([uniform(i) for i in np.arange(-2, 2, 0.1)])\n\nplt.subplot(1, 3, 2)\nplt.title('Triangular')\nplt.plot([triangle(i) for i in np.arange(-2, 2, 0.1)])\n\nplt.subplot(1, 3, 3)\nplt.title('Gaussian')\nplt.plot([gaussian(i) for i in np.arange(-2, 2, 0.1)])\nplt.show()","47de82a4":"plt.rcParams['figure.figsize'] = [20, 5]\nplt.hist(np.array([np.random.normal(0, 1, 200) + np.random.rand(200) * 4,np.random.normal(5, 2, 200)+ np.random.rand(200),np.random.normal(10, 1, 200)+ np.random.rand(200)]).flatten(), bins=100);","20d93662":"@interact(h=FloatSlider(min=.1,max=10,step=.01,value=30))\ndef distributions(h):\n    plt.rcParams['figure.figsize'] = [20, 10]\n    plt.plot([triangle(ln\/h)  for ln in np.arange(-10, 10, 0.1)])\n    plt.plot([gaussian(ln\/h)  for ln in np.arange(-10, 10, 0.1)])\n    plt.plot([uniform(ln\/h)  for ln in np.arange(-10, 10, 0.1)])  \n","9cc5c7c2":"inp = np.array([np.random.normal(0, 1, 200) + np.random.rand(200) * 4,np.random.normal(5, 2, 200)+ np.random.rand(200),np.random.normal(10, 1, 200)+ np.random.rand(200)]).flatten()\n@interact_manual(h=FloatSlider(min=.1,max=3,step=.01,value=30))\ndef kdf(h):\n    plt.rcParams['figure.figsize'] = [20, 10]\n    plt.plot([(1.0\/(len(inp)*h))*np.sum([triangle((ln - d)\/h) for d in inp]) for ln in np.arange(0, 20, 0.1)],label=\"triangle\")\n    plt.plot([(1.0\/(len(inp)*h))*np.sum([gaussian((ln - d)\/h) for d in inp]) for ln in np.arange(0, 20, 0.1)],label=\"gaussian\")\n    plt.plot([(1.0\/(len(inp)*h))*np.sum([uniform((ln - d)\/h) for d in inp]) for ln in np.arange(0, 20, 0.1)],label=\"uniform\")\n    plt.legend()","ebd9eced":"inp1 = np.array([np.random.normal(0, 1, 200)]).flatten()\n@interact_manual(h=FloatSlider(min=.1,max=3,step=.01,value=30))\ndef kdf(h):\n    plt.rcParams['figure.figsize'] = [20, 10]\n    plt.plot([(1.0\/(len(inp)*h))*np.sum([triangle((ln - d)\/h) for d in inp1]) for ln in np.arange(-10, 10, 0.1)],label=\"triangle\")\n    plt.plot([(1.0\/(len(inp)*h))*np.sum([gaussian((ln - d)\/h) for d in inp1]) for ln in np.arange(-10, 10, 0.1)],label=\"gaussian\")\n    plt.plot([(1.0\/(len(inp)*h))*np.sum([uniform((ln - d)\/h) for d in inp1]) for ln in np.arange(-10, 10, 0.1)],label=\"uniform\")\n    plt.legend()","f5cf90d4":"import tensorflow as tf\nfrom sklearn.model_selection import train_test_split\n","dc67c5d6":"\ntrain = pd.read_csv('..\/input\/train.csv')\nlabels = train.loc[:, train.columns == 'label'].values.flatten()\nimages = train.loc[:, train.columns != 'label'].values\n    \nx_train, x_test, y_train, y_test = train_test_split(images,labels, test_size=0.33, random_state=42)","3a94da41":"# create a boolean matrix of the correct answers\ny_train_labels = [[(value == i) * 1 for i in range(0,10)] for value in y_train]\ny_test_labels = [[(value == i) * 1 for i in range(0,10)] for value in y_test]","15c5dde1":"# uniform_tf = lambda x: (tf.math.abs(x) <= 1) and 1\/2 or 0\n# triangle_tf = lambda x: (np.abs(x) <= 1) and  (1 - np.abs(x)) or 0\ngaussian_tf = lambda x: (1.0\/tf.sqrt(2*np.pi))* tf.exp(-.5*x**2) ","c48e9425":"def _pattern(input,name,weight_shape,h,droput):\n    with tf.variable_scope(name) as scope:\n        w1 = tf.get_variable('weight',weight_shape,initializer=tf.initializers.truncated_normal(0,1))\n        b1 = tf.get_variable('bias',[weight_shape[0], 1],initializer=tf.constant_initializer(0))\n        layer = tf.nn.dropout(tf.add(tf.matmul(w1, tf.transpose(inputs)),b1),droput)\n        bandwidth = tf.constant(1.0\/(h * weight_shape[0]))\n        return tf.multiply(tf.reduce_sum(tf.map_fn(lambda x: (gaussian_tf(x)\/h),layer),axis=0),bandwidth)","336c636e":"# shape of image test samples \n# 60000 sample with a 28X28 size image flatten 784\nx_train.shape","67e832d6":"\ntf.reset_default_graph() \n# N number of traning example with a 28*28 size image\ninputs = tf.placeholder(tf.float32, shape=(None,x_train.shape[1]), name='inputs')\n# 0-9\nlabels = tf.placeholder(tf.float32, shape=(None, 10), name='labels')\n# droupout probability\nkeep_prob = tf.placeholder(tf.float32,shape=(), name='keep_prob')\n\nzero = _pattern(inputs,'zero',[20,x_train.shape[1]],.4,keep_prob)\none = _pattern(inputs,'one',[20,x_train.shape[1]],.4,keep_prob)\ntwo = _pattern(inputs,'two',[20,x_train.shape[1]],.4,keep_prob)\nthree = _pattern(inputs,'three',[20,x_train.shape[1]],.4,keep_prob)\nfour = _pattern(inputs,'four',[20,x_train.shape[1]],.4,keep_prob)\nfive = _pattern(inputs,'five',[20,x_train.shape[1]],.4,keep_prob)\nsix = _pattern(inputs,'six',[20,x_train.shape[1]],.4,keep_prob)\nseven = _pattern(inputs,'seven',[20,x_train.shape[1]],.4,keep_prob)\neight = _pattern(inputs,'eight',[20,x_train.shape[1]],.4,keep_prob)\nnine = _pattern(inputs,'nine',[20,x_train.shape[1]],.4,keep_prob)\nresult = tf.stack([zero, one,two,three,four,five,six,seven,eight,nine],axis=1)\n","1658bbb6":"# Loss function and optimizer\nlr = tf.placeholder(tf.float32, shape=(), name='learning_rate')\nloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=result, labels=labels))\noptimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss)\n\n# Prediction\npred_label = tf.argmax(result,1)\ncorrect_prediction = tf.equal(pred_label, tf.argmax(labels, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))","7c665624":"# Configure GPU not to use all memory\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\n","beb8e366":"# Start a new tensorflow session and initialize variables\nsess = tf.InteractiveSession(config=config)\nsess.run(tf.global_variables_initializer())\n\n# This is the main training loop: we train for 50 epochs with a learning rate of 0.05 and another \n# 50 epochs with a smaller learning rate of 0.01\nperformance_drouput = []\nfor learning_rate in [0.05, 0.01]:\n    for epoch in range(200):\n        avg_cost = 0.0\n\n        # For each epoch, we go through all the samples we have.\n        for i in range(0,x_train.shape[0]):\n            # Finally, this is where the magic happens: run our optimizer, feed the current example into X and the current target into Y\n            _, c = sess.run([optimizer, loss], feed_dict={lr:learning_rate, \n                                                          inputs: [x_train[i]],\n                                                          labels: [y_train_labels[i]],\n                                                         keep_prob:.5})\n            \n            avg_cost += c\n        avg_cost \/= x_train.shape[0]    \n        performance_drouput += [accuracy.eval(feed_dict={inputs: x_test, labels: y_test,keep_prob:1})]\n        \n        # Print the cost in this epcho to the console.\n        if epoch % 10 == 0:\n            print(\"Epoch: {:3d}    Train Cost: {:.4f}\".format(epoch, avg_cost))","346ad370":"acc_train = accuracy.eval(feed_dict={inputs: x_train, labels: y_train_labels,keep_prob:1})\nprint(\"Train accuracy: {:3.2f}%\".format(acc_train*100.0))\n\nacc_test = accuracy.eval(feed_dict={inputs: x_test, labels: y_test_labels,keep_prob:1})\nprint(\"Test accuracy:  {:3.2f}%\".format(acc_test*100.0))","fe3ab287":"sess.close()","e1c096d9":"# Start a new tensorflow session and initialize variables\nsess = tf.InteractiveSession(config=config)\nsess.run(tf.global_variables_initializer())","37f4cbe8":"# This is the main training loop: we train for 50 epochs with a learning rate of 0.05 and another \n# 50 epochs with a smaller learning rate of 0.01\nperformance_no_droupout = []\nfor learning_rate in [0.05, 0.01]:\n    for epoch in range(200):\n        avg_cost = 0.0\n\n        # For each epoch, we go through all the samples we have.\n        for i in range(0,x_train.shape[0]):\n            # Finally, this is where the magic happens: run our optimizer, feed the current example into X and the current target into Y\n            _, c = sess.run([optimizer, loss], feed_dict={lr:learning_rate, \n                                                          inputs: [x_train[i]],\n                                                          labels: [y_train[i]],\n                                                         keep_prob:1})\n            avg_cost += c\n        avg_cost \/= x_train.shape[0]    \n        performance_no_droupout += [accuracy.eval(feed_dict={inputs: x_test, labels: y_test,keep_prob:1})]\n        \n        # Print the cost in this epcho to the console.\n        if epoch % 10 == 0:\n            print(\"Epoch: {:3d}    Train Cost: {:.4f}\".format(epoch, avg_cost))","60e25cd8":"acc_train = accuracy.eval(feed_dict={inputs: x_train, labels: y_train,keep_prob:1})\nprint(\"Train accuracy: {:3.2f}%\".format(acc_train*100.0))\n\nacc_test = accuracy.eval(feed_dict={inputs: x_test, labels: y_test,keep_prob:1})\nprint(\"Test accuracy:  {:3.2f}%\".format(acc_test*100.0))","da81de43":"sess.close()","af6f3871":"plt.plot(performance_drouput,label='with drouput')\nplt.plot(performance_no_droupout,label='without dropout')\nplt.legend()","29b8416a":"# Training","85f7e982":"### No Droput at Pattern Layer","0fe98dde":"# MNIST","2ed671fb":"![](https:\/\/i.imgur.com\/vmEfX8G.png)","9d9d6837":"### Droput at Pattern Layer","5ed71ae7":"## Kernels\n","3287a5ee":"each class kernel is estimated using a Parzen window or a kernel desnisty function.\n\n$f_h(x) = \\frac{1}{n}\\sum_{i=1}^{n}{K_h(x-x_i)} = \\frac{1}{nh}\\sum_{i=1}^{n}{K(\\frac{x-x_i}{h})}$\n\nK is a kernel that is centered around zero. diffrence function of K can be used to change the estimation. h is used to estimate the distribution given the set of data points. h is a smooting parameter over a given value of x. A large h produces a smoother result while a smaller h is more sensitive to noise. ","cca0baf3":"# Performance","cb35959c":"1. ### Input Layer\ninput values feed into the PNN network. each entry is a predictor for the network\n2. ### Pattern Layer\nThe euclidian distance of the input value centered around zero fed into a parzen window density estimator. non parametric method used to estimate over a distribution.\n3. ### Summation Layer\nA summation of the results from the pattern layer\n4. ### Output Layer\nthe max is used to denote the target class","1e975155":"## Cleaning","1357efb5":"## Building Model","8f8ac5dc":"# Kernel Density Estimation"}}