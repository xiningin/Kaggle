{"cell_type":{"407def93":"code","f2ecd38b":"code","690614b7":"code","4e9c32dd":"code","5dc66ffd":"code","93642d76":"code","ff70e106":"code","874ed2bb":"code","be0cb5cb":"code","3285e42c":"code","35740f57":"code","fc3c823a":"code","eecb88db":"code","74cc9bcc":"code","cc449315":"code","f50e0413":"code","a6b82c3c":"code","6087d37b":"code","4cd6fd96":"code","2ca45e0c":"code","c25bda02":"code","84dca6d4":"code","7127043f":"code","b27d232f":"code","271616df":"code","739b032b":"code","326370d5":"code","978c200f":"code","586c368c":"code","34d6f639":"code","a2a84d7e":"code","c913122b":"code","247c8f85":"code","6a2b1280":"code","18346acb":"code","79d70bba":"code","77383506":"code","3e279072":"code","a80115d2":"code","746e057a":"code","cb581d10":"code","93d3add3":"code","5d41025b":"code","3e740470":"code","5a87b239":"code","e4aec59d":"code","ff43987d":"code","21300cb1":"code","270a0623":"code","dbb8689a":"code","86f2db3d":"code","f7b70183":"code","812a5d83":"code","6a9ca8bc":"code","701b4b64":"code","0964fa6d":"code","82592f0f":"code","dbdb1ba4":"code","2b57a1cd":"code","79fb5d34":"code","82292f5e":"code","4383969b":"code","4504e7f3":"code","bb7c6221":"code","3a0156d9":"code","03966c25":"code","eb7db94d":"code","6f818bfd":"code","fbf5f390":"code","162ba71b":"code","ab01a371":"code","a52055e7":"markdown","1659e253":"markdown","2ac3f7ba":"markdown","7da6a1d9":"markdown","fac57037":"markdown","33582e0a":"markdown","14bb38bb":"markdown","5da66997":"markdown","432cac1f":"markdown","11899b3c":"markdown","827bbc2b":"markdown","8bfb76c8":"markdown","40006171":"markdown","df91481a":"markdown","038c318a":"markdown","5e626cd0":"markdown","14fe74f5":"markdown"},"source":{"407def93":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport os\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay,classification_report,plot_roc_curve,accuracy_score\npd.set_option('display.max_columns',25)\nwarnings.filterwarnings('ignore')","f2ecd38b":"# Importing Dataset\ndata = pd.read_csv(r'..\/input\/default-of-credit-card-clients-dataset\/UCI_Credit_Card.csv')\ndata.head(10)","690614b7":"data.info()\n#info shows that there is no null values and all the features are numeric","4e9c32dd":"data.describe(include='all') # Descriptive analysis","5dc66ffd":"data.rename(columns={'PAY_0':'PAY_1','default.payment.next.month':'def_pay'},inplace=True) \n#rename few columns","93642d76":"plt.figure(figsize=(10,6))\ndata.groupby('def_pay')['AGE'].hist(legend=True)\nplt.show()\n#here we can see that, between age 20 to 45 most of the people will fall into..","ff70e106":"sns.distplot(data['AGE'])\nplt.title('Age Distribution')","874ed2bb":"sns.boxplot('def_pay','LIMIT_BAL',data=data)","be0cb5cb":"data[data['LIMIT_BAL']>700000].sort_values(ascending=False,by='LIMIT_BAL')","3285e42c":"plt.figure(figsize=(16,5))\nplt.subplot(121)\nsns.boxplot(x='SEX', y= 'AGE',data = data)\nsns.stripplot(x='SEX', y= 'AGE',data = data,linewidth = 0.9)\nplt.title ('Sex vs AGE')\n\nplt.subplot(122)\nax = sns.countplot(x='EDUCATION',data = data,  order= data['EDUCATION'].value_counts().index)\nplt.title ('EDUCATION')\nlabels = data['EDUCATION'].value_counts()\nfor i, v in enumerate(labels):\n    ax.text(i,v+100,v, horizontalalignment='center')\nplt.show()","35740f57":"plt.figure(figsize=(20,5))\nplt.subplot(121)\nsns.boxplot(x='def_pay', y= 'AGE',data = data)\nsns.stripplot(x='def_pay', y= 'AGE',data = data,linewidth = 0.9)\nplt.title ('Age vs def_pay')\n\nax2=plt.subplot(1,2,2)\npay_edu = data.groupby('EDUCATION')['def_pay'].value_counts(normalize=True).unstack()\npay_edu = pay_edu.sort_values(ascending=False,by=1)\npay_edu.plot(kind='bar',stacked= True,color=[\"#3f3e6fd1\", \"#85c6a9\"], ax = ax2)\nplt.legend(loc=(1.04,0))\nplt.title('Education vs def_pay')\nplt.show()","fc3c823a":"# function for Multivariate analysis\n# This method is used to show point estimates and confidence intervals using scatter plot graphs\ndef plotfig(df1,col11,col22,deft1):\n    plt.figure(figsize=(16,6))\n\n    plt.subplot(121)\n    sns.pointplot(df1[col11], df1[deft1],hue = df1[col22])\n\n    plt.subplot(122)\n    sns.countplot(df1[col11], hue = df1[col22])\n    plt.show() \n\n\ndef varplot(df2, col1, col2, deft, bin=3, unique=10):\n    df=df2.copy()\n    if len(df[col1].unique())>unique:\n        df[col1+'cut']= pd.qcut(df[col1],bin)\n        if len(df[col2].unique())>unique:\n            df[col2+'cut']= pd.qcut(df[col2],bin)\n            return plotfig(df,col1+'cut',col2+'cut',deft)\n        else:\n            df[col2+'cut']= df[col2]\n            return plotfig(df,col1+'cut',col2+'cut',deft)\n    else:\n        return plotfig(df,col1,col2,deft)","eecb88db":"varplot(data,'AGE','SEX','def_pay',3)","74cc9bcc":"varplot(data,'LIMIT_BAL','AGE','def_pay',3)","cc449315":"# Univariate Analysis\ndf = data.drop('ID',1)\nnuniq = df.nunique()\ndf = data[[col for col in df if nuniq[col]>1 and nuniq[col]<50]]\nrow, cols = df.shape\ncolnames = list(df)\ngraph_perrow = 5\ngraph_row = (cols+graph_perrow-1)\/ graph_perrow\nmax_graph = 20\nplt.figure(figsize=(graph_perrow*12,graph_row*8))\nfor i in range(min(cols,max_graph)):\n    plt.subplot(graph_row,graph_perrow,i+1)\n    coldf = df.iloc[:,i]\n    if (not np.issubdtype(type(coldf),np.number)):\n        sns.countplot(colnames[i],data= df, order= df[colnames[i]].value_counts().index)\n    else:\n        coldf.hist()\n    plt.title(colnames[i])\nplt.show()","f50e0413":"cont_var = df.select_dtypes(exclude='object').columns\nnrow = (len(cont_var)+5-1)\/5\nplt.figure(figsize=(12*5,6*2))\nfor i,j in enumerate(cont_var):\n    plt.subplot(nrow,5,i+1)\n    sns.distplot(data[j])\nplt.show()","a6b82c3c":"# from the above,we can see that we have maximum clients from 20-30 age group followed by 31-40. \n# Hence with increasing age group the number of clients that will default the payment next month is decreasing. \n# Hence we can see that Age is important feature to predict the default payment for next month.","6087d37b":"plt.subplots(figsize=(26,20))\ncorr = data.corr()\nsns.heatmap(corr,annot=True)\nplt.show()","4cd6fd96":"from statsmodels.stats.outliers_influence import variance_inflation_factor\ndf= data.drop(['def_pay','ID'],1)\nvif = pd.DataFrame()\nvif['Features']= df.columns\nvif['vif']= [variance_inflation_factor(df.values,i) for i in range(df.shape[1])]\nvif","2ca45e0c":"# From this heatmap and VIF we can see that there are some multicolinearity(values >10) in the data which we can handle\n# simply doing feature engineering of some columns \n\nbill_tot = pd.DataFrame(data['BILL_AMT1']+data['BILL_AMT2']+data['BILL_AMT3']+data['BILL_AMT4']+data['BILL_AMT5']+data['BILL_AMT6'],columns=['bill_tot'])\npay_tot =pd.DataFrame(data['PAY_1']+data['PAY_2']+data['PAY_3']+data['PAY_4']+data['PAY_5']+data['PAY_6'],columns=['pay_tot'])\npay_amt_tot = pd.DataFrame(data['PAY_AMT1']+data['PAY_AMT2']+data['PAY_AMT3']+data['PAY_AMT4']+data['PAY_AMT5']+data['PAY_AMT6'],columns=['pay_amt_tot'])\nframes=[bill_tot,pay_tot,pay_amt_tot,data['def_pay']]\ntot = pd.concat(frames,axis=1)\n","c25bda02":"plt.figure(figsize=(20,4))\nplt.subplot(131)\nsns.boxplot(x='def_pay',y='pay_tot',data = tot)\nsns.stripplot(x='def_pay',y='pay_tot',data = tot,linewidth=1)\n\nplt.subplot(132)\nsns.boxplot(x='def_pay', y='bill_tot',data=tot)\nsns.stripplot(x='def_pay', y='bill_tot',data=tot,linewidth=1)\n\nplt.subplot(133)\nsns.boxplot(x='def_pay', y='pay_amt_tot',data=tot)\nsns.stripplot(x='def_pay', y='pay_amt_tot',data=tot,linewidth=1)\nplt.show()","84dca6d4":"sns.pairplot(tot[['bill_tot','pay_amt_tot','pay_tot','def_pay']],hue='def_pay')\nplt.show()","7127043f":"sns.violinplot(x=tot['def_pay'], y= tot['bill_tot'])","b27d232f":"tot.drop('def_pay',1,inplace=True)","271616df":"data1 = pd.concat([data,tot],1)","739b032b":"data1.groupby('def_pay')['EDUCATION'].hist(legend=True)\nplt.show()","326370d5":"data1.groupby('def_pay')['AGE'].hist()\nplt.figure(figsize=(12,6))","978c200f":"# we know that the Bill_AMT is the most correlated column so using that we create a data\ndf= pd.concat([bill_tot,df],1)\ndf1 = df.drop(['BILL_AMT1','BILL_AMT2','BILL_AMT3','BILL_AMT4','BILL_AMT5','BILL_AMT6'],1)","586c368c":"vif = pd.DataFrame()\nvif['Features']= df1.columns\nvif['vif']= [variance_inflation_factor(df1.values,i) for i in range(df1.shape[1])]\nvif","34d6f639":"# above we can see that now our data doesnt have multicollinearty(no values >10)","a2a84d7e":"data2 = df1.copy()","c913122b":"# using the above plot we can create age bins\nage = [20,27,32,37,42,48,58,64,80]\nlab = [8,7,6,5,4,3,2,1]\ndata2['AGE'] = pd.cut(data2['AGE'],bins= age,labels=lab)","247c8f85":"data2 =  pd.concat([data2,data['def_pay']],1)\ndata2","6a2b1280":"data2.groupby('def_pay')['AGE'].hist()\nplt.figure(figsize=(12,6))","18346acb":"sns.countplot(data2['AGE'])","79d70bba":"data2.groupby('def_pay')['LIMIT_BAL'].hist(legend=True)\nplt.show()","77383506":"data2.columns","3e279072":"x= data2.drop(['def_pay'],1)\ny = data2['def_pay']\nx_train,x_test, y_train, y_test = train_test_split(x,y,test_size=0.20, random_state=1)\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)","a80115d2":"# Accuracy is not the best metric to use when evaluating imbalanced datasets as it can be misleading.\n# hence we are using Classification Report and Confusion Matrix\n# function for accuracy and confusion matrix\ndef res(y_test_valid,y_train_valid):\n    cm_log = confusion_matrix(y_test,y_test_valid)\n    ConfusionMatrixDisplay(cm_log).plot()\n    print(classification_report(y_test,y_test_valid))\n    print('train_accuracy:',accuracy_score(y_train,y_train_valid))\n    print('test_accuracy:',accuracy_score(y_test,y_test_valid))","746e057a":"log_model= LogisticRegression()\nlog_model.fit(x_train,y_train)\ny_pred_log = log_model.predict(x_test)\ny_pred_train = log_model.predict(x_train)\nres(y_pred_log,y_pred_train)","cb581d10":"plot_roc_curve(log_model,x_test,y_test)\nplt.show()","93d3add3":"# log model using Threshold\nthreshold = 0.36\ny_log_prob =  log_model.predict_proba(x_test)\ny_train_log_prob = log_model.predict_proba(x_train)\ny_log_prob=y_log_prob[:,1]\ny_train_log_prob= y_train_log_prob[:,1]\ny_pred_log_prob = np.where(y_log_prob>threshold,1,0)\ny_pred_log_prob_train = np.where(y_train_log_prob>threshold,1,0)\nres(y_pred_log_prob,y_pred_log_prob_train)","5d41025b":"dec_model = DecisionTreeClassifier()\ndec_model.fit(x_train,y_train)\ny_pred_dec = dec_model.predict(x_test)\ny_pred_dec_train = dec_model.predict(x_train)\nres(y_pred_dec,y_pred_dec_train)","3e740470":"parameters = {'max_depth':[1,2,3,4,5,6],'min_samples_split':[3,4,5,6,7],'min_samples_leaf':[1,2,3,4,5,6]}\ntree = GridSearchCV(dec_model, parameters,cv=10)\ntree.fit(x_train,y_train)\ntree.best_params_","5a87b239":"# We know that Decision tree will have high variance due to which the model overfit hence we can reduce this by \"Pruning\"\n# By using the best parameter from GridSearchCV best parameters\ndec_model1 = DecisionTreeClassifier(max_depth=4,min_samples_split=10,min_samples_leaf=1)\ndec_model1.fit(x_train,y_train)\ny_pred_dec1 = dec_model1.predict(x_test)\ny_pred_dec_train1 = dec_model1.predict(x_train)\nres(y_pred_dec1,y_pred_dec_train1)","e4aec59d":"rf_model = RandomForestClassifier(random_state=1)\nrf_model.fit(x_train,y_train)\ny_pred_rf = rf_model.predict(x_test)\ny_pred_rf_train = rf_model.predict(x_train)\nres(y_pred_rf,y_pred_rf_train)","ff43987d":"# parameters = {'n_estimators':[60,70,80],'max_depth':[1,2,3,4,5,6],'min_samples_split':[3,4,5,6,7],\n#               'min_samples_leaf':[1,2,3,4,5,6]}\n# clf = GridSearchCV(rf_model, parameters,cv=10)\n# clf.fit(x_train,y_train)\n# clf.best_params_\n    # {'max_depth': 5,\n    #  'min_samples_leaf': 4,\n    #  'min_samples_split': 3,\n    #  'n_estimators': 70}","21300cb1":"# Decision trees frequently perform well on imbalanced data. so using RandomForest uses bagging of n_trees will be a better idea.\nrf_model = RandomForestClassifier(n_estimators=70, max_depth=5, min_samples_leaf=4, min_samples_split=3)\nrf_model.fit(x_train,y_train)\ny_pred_rf = rf_model.predict(x_test)\ny_pred_rf_train = rf_model.predict(x_train)\nres(y_pred_rf,y_pred_rf_train)","270a0623":"# finding the K value\nerror = []\n\nfor i in range(1,21,2):\n    knn =  KNeighborsClassifier(n_neighbors=i)\n    knn.fit(x_train,y_train)\n    preds = knn.predict(x_test)\n    error.append(np.mean(preds!=y_test))\n\nplt.plot(range(1,21,2), error, linestyle = 'dashed', marker ='o', mfc= 'red')","dbb8689a":"# By using the elbow graph we can see that the k=5 will perform better in the first place so impute k = 5\nknn_model = KNeighborsClassifier(n_neighbors=5)\nknn_model.fit(x_train,y_train)\ny_pred_knn = knn_model.predict(x_test)\ny_pred_knn_train = knn_model.predict(x_train)\n\nres(y_pred_knn,y_pred_knn_train)","86f2db3d":"# use penalized learning algorithms that increase the cost of classification mistakes on the minority class.\nsvm_model = SVC(class_weight='balanced', probability=True)\nsvm_model.fit(x_train,y_train)\ny_pred_svm = svm_model.predict(x_test)\ny_pred_svm_train = svm_model.predict(x_train)\nres(y_pred_svm,y_pred_svm_train)","f7b70183":"# we can see in SVM that our recall of target variable is 0.56 which is the best we ever predicted.","812a5d83":"nb_model = GaussianNB()\nnb_model.fit(x_train,y_train)\ny_pred_nb = nb_model.predict(x_test)\ny_pred_nb_train = nb_model.predict(x_train)\nres(y_pred_nb,y_pred_nb_train)","6a9ca8bc":"# But here Naive bayes out performs every other model though over accuracy is acceptable, checkout the recall ","701b4b64":"from xgboost import XGBClassifier\n\nxgb_model = XGBClassifier()\nxgb_model.fit(x_train, y_train)\nxgb_y_predict = xgb_model.predict(x_test)\nxgb_y_predict_train = xgb_model.predict(x_train)\nres(xgb_y_predict,xgb_y_predict_train)","0964fa6d":"# Even Boosting technique gives low recall for our target variable","82592f0f":"# So from the above model we can conclude that the data imbalance is playing a major part \n# Hence we try to fix that by doing ReSample techniques","dbdb1ba4":"from collections import Counter\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import TomekLinks","2b57a1cd":"x= data2.drop(['def_pay'],1)\ny = data2['def_pay']\n\nrus = RandomUnderSampler(random_state=1)\nx_rus, y_rus = rus.fit_resample(x,y)\n\nprint('original dataset shape:', Counter(y))\nprint('Resample dataset shape', Counter(y_rus))","79fb5d34":"x_train,x_test, y_train, y_test = train_test_split(x_rus,y_rus,test_size=0.20, random_state=1)\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)","82292f5e":"# again we try to predict using Random Forest\nrf_model_rus = RandomForestClassifier(n_estimators=70, max_depth=5, min_samples_leaf=4, min_samples_split=3,random_state=1)\nrf_model_rus.fit(x_train,y_train)\ny_pred_rf_rus = rf_model_rus.predict(x_test)\ny_pred_rf_rus_train = rf_model_rus.predict(x_train)\nres(y_pred_rf_rus,y_pred_rf_rus_train)","4383969b":"x= data2.drop(['def_pay'],1)\ny = data2['def_pay']\n\nros = RandomOverSampler(random_state=42)\nx_ros, y_ros = ros.fit_resample(x, y)\n\nprint('Original dataset shape', Counter(y))\nprint('Resample dataset shape', Counter(y_ros))","4504e7f3":"x_train,x_test, y_train, y_test = train_test_split(x_ros,y_ros,test_size=0.20, random_state=1)\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)","bb7c6221":"rf_model_ros = RandomForestClassifier(n_estimators=70, max_depth=5, min_samples_leaf=4, min_samples_split=3,random_state=1)\nrf_model_ros.fit(x_train,y_train)\ny_pred_rf_ros = rf_model_ros.predict(x_test)\ny_pred_rf_ros_train = rf_model_ros.predict(x_train)\nres(y_pred_rf_ros,y_pred_rf_ros_train)","3a0156d9":"x= data2.drop(['def_pay'],1)\ny = data2['def_pay']\n\ntl = TomekLinks(sampling_strategy='majority')\nx_tl, y_tl = tl.fit_resample(x,y)\n\nprint('Original dataset shape', Counter(y))\nprint('Resample dataset shape', Counter(y_tl))","03966c25":"x_train,x_test, y_train, y_test = train_test_split(x_tl,y_tl,test_size=0.20, random_state=1)\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)","eb7db94d":"rf_model_tl = RandomForestClassifier(n_estimators=70, max_depth=5, min_samples_leaf=4, min_samples_split=3,random_state=1)\nrf_model_tl.fit(x_train,y_train)\ny_pred_rf_tl = rf_model_tl.predict(x_test)\ny_pred_rf_tl_train = rf_model_tl.predict(x_train)\nres(y_pred_rf_tl,y_pred_rf_tl_train)","6f818bfd":"from imblearn.over_sampling import SMOTE\nsmote = SMOTE()\n\nx_smote, y_smote = smote.fit_resample(x, y)\n\nprint('Original dataset shape', Counter(y))\nprint('Resample dataset shape', Counter(y_smote))","fbf5f390":"x_train,x_test, y_train, y_test = train_test_split(x_smote,y_smote,test_size=0.20, random_state=1)\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)","162ba71b":"x_train = pd.DataFrame(x_train).fillna(0)\nx_test = pd.DataFrame(x_test).fillna(0)","ab01a371":"rf_model_smote = RandomForestClassifier(n_estimators=70, max_depth=5, min_samples_leaf=4, min_samples_split=3,random_state=1)\nrf_model_smote.fit(x_train,y_train)\ny_pred_rf_smote = rf_model_smote.predict(x_test)\ny_pred_rf_smote_train = rf_model_smote.predict(x_train)\nres(y_pred_rf_smote,y_pred_rf_smote_train)","a52055e7":"# Model Creation\n#### We know that we have a dataset where we have imbalance in the target variable\n#### you get a pretty high accuracy just by predicting the majority class, but you fail to capture the minority class\n#### which is most often the point of creating the model in the first place.\n#### Hence we try to create more model to get the best results","1659e253":"# Logistic model","2ac3f7ba":"# Random over-sampling","7da6a1d9":"### Again hyper parameter tuning for Random Forest","fac57037":"# using Decision Tree model","33582e0a":"# Importing Packages","14bb38bb":"# Random Forest Model","5da66997":"# Synthetic Minority Oversampling Technique (SMOTE)","432cac1f":"### Hyper parameter tuning for DecisionTree","11899b3c":"# Random under-sampling\n### Let\u2019s apply some of these resampling techniques, using the Python library imbalanced-learn.","827bbc2b":"# Exploratory Data Analysis","8bfb76c8":"# Naive Bayes","40006171":"# KNN model","df91481a":"# Boosting model XGB Classifier","038c318a":"### Finally using SMOTE we can see our accuracy as well as recall and precision ratio are give equal ratio\n### Though all the above models performs well, based on the accuracy  but in a imbalance dataset like this,\n####              we actually prefer to change the performance metrics \n### We can get better result when we do SVM and Naive bayes with our original data\n### Even we dont have any variance in the model nor to much of bias\n### But when we do over or Under sample the date the other metrics like sensity and specificity was better\n### Hence we can conclue that if we use resample technique we will get better result","5e626cd0":"# Under-sampling: Tomek links","14fe74f5":"# SVM Model"}}