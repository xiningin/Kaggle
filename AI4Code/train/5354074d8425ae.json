{"cell_type":{"18f8f6b5":"code","7638becb":"code","fb599c87":"code","423cf92b":"code","4aeda336":"code","17002e74":"code","061b6d65":"code","490e138c":"code","084f88d7":"code","e003fc6c":"code","0a865e82":"code","842424f9":"code","7d7555e9":"code","baf8d5e1":"code","6e23e7c0":"code","a14901b3":"code","97de5d3a":"code","2eb55a5c":"code","6a287702":"code","67d2beb5":"code","b3beef99":"code","04a98ff5":"code","bd90506c":"code","a592d379":"code","cf69c863":"code","f72838da":"code","afad7579":"code","d485c6ae":"code","c2cadcdb":"code","a178335a":"code","0de4ff84":"code","496bc616":"code","6cd13ef5":"code","d281d2f2":"code","e3e4f5d6":"code","2134165e":"code","ee806df7":"code","cc4203ab":"code","0bf163c1":"code","e4de8542":"code","c47136a1":"code","60bee4e8":"code","10e389ac":"code","024c5beb":"code","f0cf0c14":"code","372175eb":"code","66209305":"code","a5af8abb":"code","6cdea97d":"code","7c872d24":"code","56d8d1d3":"code","ac19ff85":"code","036b3706":"code","30d45940":"code","36a71608":"code","1be712cb":"code","207fa81d":"code","0ea6345b":"code","ebc634d6":"markdown","d01b51bf":"markdown","eb85d94d":"markdown","9fb7909d":"markdown","b0432d03":"markdown","5abb2bab":"markdown"},"source":{"18f8f6b5":"import pandas as pd\nimport numpy as np\n\nimport re\nimport string\nimport os\nimport spacy\nimport en_core_web_lg\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import TweetTokenizer\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.svm import  LinearSVC\n\nplt.style.use('ggplot')\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM,Dense, SpatialDropout1D, Dropout\nfrom tensorflow.keras.optimizers import Adam\n# from utils import _get_basic_features , _get_wordcounts , _get_ngram","7638becb":"tweet = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")","fb599c87":"tweet.head(5)\n","423cf92b":"tweet.shape","4aeda336":"tweet.info()","17002e74":"plt.rcParams['figure.figsize'] = [8,4]\nplt.rcParams['figure.dpi'] = 80","061b6d65":"sns.countplot('target', data=tweet)\nplt.title('Real or Not Real Disaster Tweet')","490e138c":"tweet['target'].value_counts()","084f88d7":"# !pip install spacy==2.2.3\n# !python -m spacy download en_core_web_sm\n# !python -m spacy download en_core_web_lg\n# !pip install beautifulsoup4==4.9.1\n# !pip install textblob==0.15.3\n# !pip install ktrain","e003fc6c":"from nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import TweetTokenizer\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# nlp = en_core_web_sm.load()\n\npath = os.path.dirname(os.path.abspath(__file__))\nabbreviations_path = os.path.join(path, 'data','abbreviations_wordlist.json')\n\n\ndef _get_wordcounts(x):\n    length = len(str(x).split())\n    return length\n\ndef _get_charcounts(x):\n    s = x.split()\n    x = ''.join(s)\n    return len(x)\n\ndef _get_avg_wordlength(x):\n    count = _get_charcounts(x)\/_get_wordcounts(x)\n    return count\n\ndef _get_stopwords_counts(x):\n    l = len([t for t in x.split() if t in stopwords])\n    return l\n\ndef _get_hashtag_counts(x):\n    l = len([t for t in x.split() if t.startswith('#')])\n    return l\n\ndef _get_mentions_counts(x):\n    l = len([t for t in x.split() if t.startswith('@')])\n    return l\n\ndef _get_digit_counts(x):\n    digits = re.findall(r'[0-9,.]+', x)\n    return len(digits)\n\ndef _get_uppercase_counts(x):\n    return len([t for t in x.split() if t.isupper()])\n\ndef _cont_exp(x):\n    abbreviations = json.load(open(abbreviations_path))\n\n    if type(x) is str:\n        for key in abbreviations:\n            value = abbreviations[key]\n            x = x.replace(key, value)\n        return x\n    else:\n        return x\n\n\ndef _get_emails(x):\n    emails = re.findall(r'([a-z0-9+._-]+@[a-z0-9+._-]+\\.[a-z0-9+_-]+\\b)', x)\n    counts = len(emails)\n\n    return counts, emails\n\n\ndef _get_urls(x):\n    urls = re.findall(r'(http|https|ftp|ssh):\/\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\/~+#-]*[\\w@?^=%&\/~+#-])?', x)\n    counts = len(urls)\n\n    return counts, urls\n\ndef _remove_urls(x):\n    return re.sub(r'(http|https|ftp|ssh):\/\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\/~+#-]*[\\w@?^=%&\/~+#-])?', '' , x)\n\ndef _remove_rt(x):\n    return re.sub(r'\\brt\\b', '', x).strip()\n\ndef _remove_special_chars(x):\n    x = re.sub(r'[^\\w ]+', \"\", x)\n    x = ' '.join(x.split())\n    return x\n\ndef _remove_html_tags(x):\n    return BeautifulSoup(x, 'lxml').get_text().strip()\n\ndef _remove_accented_chars(x):\n    x = unicodedata.normalize('NFKD', x).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    return x\n\ndef _remove_stopwords(x):\n    return ' '.join([t for t in x.split() if t not in stopwords])\t\n\ndef _make_base(x):\n    x = str(x)\n    x_list = []\n    doc = nlp(x)\n\n    for token in doc:\n        lemma = token.lemma_\n        if lemma == '-PRON-' or lemma == 'be':\n            lemma = token.text\n\n        x_list.append(lemma)\n    return ' '.join(x_list)\n\ndef _get_value_counts(df, col):\n    text = ' '.join(df[col])\n    text = text.split()\n    freq = pd.Series(text).value_counts()\n    return freq\n\ndef _remove_common_words(x, freq, n=20):\n    fn = freq[:n]\n    x = ' '.join([t for t in x.split() if t not in fn])\n    return x\n\ndef _remove_rarewords(x, freq, n=20):\n    fn = freq.tail(n)\n    x = ' '.join([t for t in x.split() if t not in fn])\n    return x\n\n\n\ndef _spelling_correction(x):\n    x = TextBlob(x).correct()\n    return x\n\ndef _get_basic_features(df):\n    if type(df) == pd.core.frame.DataFrame:\n        df['char_counts'] = df['text'].apply(lambda x: _get_charcounts(x))\n        df['word_counts'] = df['text'].apply(lambda x: _get_wordcounts(x))\n        df['avg_wordlength'] = df['text'].apply(lambda x: _get_avg_wordlength(x))\n        df['stopwords_counts'] = df['text'].apply(lambda x: _get_stopwords_counts(x))\n        df['hashtag_counts'] = df['text'].apply(lambda x: _get_hashtag_counts(x))\n        df['mentions_counts'] = df['text'].apply(lambda x: _get_mentions_counts(x))\n        df['digits_counts'] = df['text'].apply(lambda x: _get_digit_counts(x))\n        df['uppercase_counts'] = df['text'].apply(lambda x: _get_uppercase_counts(x))\n    else:\n        print('ERROR: This function takes only Pandas DataFrame')\n\n    return df\n\ndef _get_ngram(df, col, ngram_range):\n    vectorizer = CountVectorizer(ngram_range=(ngram_range, ngram_range))\n    vectorizer.fit_transform(df[col])\n    ngram = vectorizer.vocabulary_\n    ngram = sorted(ngram.items(), key = lambda x: x[1], reverse=True)\n\n    return ngram","0a865e82":"tweet = _get_basic_features(tweet)","842424f9":"tweet = pd.DataFrame(tweet)\ntweet.head(5)","7d7555e9":"sns.distplot(tweet['char_counts'])","baf8d5e1":"sns.kdeplot(tweet[tweet['target'] == 1]['char_counts'],shade=True)\nsns.kdeplot(tweet[tweet['target'] == 0]['char_counts'],shade=True)","6e23e7c0":"sns.kdeplot(tweet[tweet['target'] == 1]['word_counts'],shade=True , color='red')\nsns.kdeplot(tweet[tweet['target'] == 0]['word_counts'],shade=True , color='magenta')","a14901b3":"sns.kdeplot(tweet[tweet['target'] == 1]['avg_wordlength'],shade=True , color='red')\nsns.kdeplot(tweet[tweet['target'] == 0]['avg_wordlength'],shade=True , color='magenta')","97de5d3a":"bigram = _get_ngram(tweet, 'text', ngram_range=2)\nbigram[-20:]","2eb55a5c":"def process_tweet(tweet):\n    \"\"\"Process tweet function.\n    Input:\n        tweet: a string containing a tweet\n    Output:\n        tweets_clean: a list of words containing the processed tweet\n\n    \"\"\"\n    stemmer = PorterStemmer()\n    stopwords_english = stopwords.words('english')\n    # remove duplicates\n    tweet = re.sub(\"(.)\\\\1{2,}\", \"\\\\1\", tweet)\n    # remove emails\n    tweet = re.sub(r'([a-z0-9+._-]+@[a-z0-9+._-]+\\.[a-z0-9+_-]+)',\"\", tweet)\n    # remove old style retweet text \"RT\"\n    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n    # remove hyperlinks\n    tweet = re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', tweet)\n    # remove hashtags\n    # only removing the hash # sign from the word\n    tweet = re.sub(r'#', '', tweet)\n\n\n    return tweet","6a287702":"tweet['text'] = tweet['text'].apply(lambda x: process_tweet(x))\n","67d2beb5":"tweet.head(5)","b3beef99":"from PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator","04a98ff5":"# Start with one review:\nt1 = tweet.text[0]\n\n# Create and generate a word cloud image:\nwordcloud = WordCloud().generate(t1)\n\n# Display the generated image:\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","bd90506c":"text = tweet['text']\ny = tweet['target']","a592d379":"tfidf = TfidfVectorizer()\nX = tfidf.fit_transform(text)","cf69c863":"X.shape","f72838da":"X_trian,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0,stratify=y)","afad7579":"def run_svm(clf,X_trian,y_train,X_test,y_test):\n  clf.fit(X_trian,y_train)\n  y_pred = clf.predict(X_test)\n  print('Classification Report: \\n',classification_report(y_test,y_pred))\n\n\n","d485c6ae":"clf = LinearSVC()\nrun_svm(clf,X_trian,y_train,X_test,y_test)","c2cadcdb":"nlp = en_core_web_lg.load()","a178335a":"def get_vec(x):\n    doc = nlp(x)\n    vec = doc.vector\n    return vec","0de4ff84":"tweet['vec'] = tweet['text'].apply(lambda x: get_vec(x))\n","496bc616":"tweet.head(5)","6cd13ef5":"X = tweet['vec'].to_numpy()\nX = X.reshape(-1,1)","d281d2f2":"X.shape","e3e4f5d6":"X = np.concatenate(np.concatenate(X,axis=0),axis=0).reshape(-1,300)\nX.shape","2134165e":"y = tweet['target']\n","ee806df7":"X_trian,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0,stratify=y)\n\nclf = LinearSVC()\nrun_svm(clf,X_trian,y_train,X_test,y_test)\n","cc4203ab":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Flatten,Embedding,Dropout\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D ","0bf163c1":"token = Tokenizer()\ntoken.fit_on_texts(text)","e4de8542":"vocab_size = len(token.word_index) + 1\nvocab_size","c47136a1":"print(token.word_index)","60bee4e8":"encoded_text = token.texts_to_sequences(text)\nprint(encoded_text)","10e389ac":"max_len = 40\nX = pad_sequences(encoded_text,maxlen=max_len,padding='post')\n","024c5beb":"X.shape","f0cf0c14":"X_trian,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0,stratify=y)","372175eb":"vec_size =100\nmodel = Sequential()\nmodel.add(Embedding(vocab_size,vec_size,input_length=max_len))\n\nmodel.add(Conv1D(32,2 ,activation='relu'))\nmodel.add(MaxPooling1D(2))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(16, activation='relu'))\n\nmodel.add(GlobalMaxPooling1D())\n\nmodel.add(Dense(1,activation='sigmoid'))\n\n","66209305":"model.summary()","a5af8abb":"%%time\nmodel.compile(optimizer='adam' , loss='binary_crossentropy' , metrics=['accuracy'])\nmodel.fit(X_trian,y_train,epochs=5,validation_data=(X_test,y_test))\n","6cdea97d":"def get_encoded(x):\n    x = process_tweet(x)\n    x = token.texts_to_sequences([x])\n    x = pad_sequences(x,maxlen=max_len,padding='post')\n    return x\n","7c872d24":"x = 'i am thrilled to see this'\nvec = get_encoded(x)","56d8d1d3":"np.argmax(model.predict(vec), axis = -1)","ac19ff85":"from ktrain import text\nimport ktrain\n","036b3706":"(X_train,y_train) , (X_test,y_test) , preproc = text.texts_from_df(train_df=tweet,text_column='text' , label_columns='target',maxlen=40,preprocess_mode='bert')","30d45940":"model = text.text_classifier(name='bert',train_data=(X_train,y_train),preproc=preproc)\n","36a71608":"learner = ktrain.get_learner(model=model,train_data=(X_train,y_train),val_data=(X_test,y_test),batch_size=64)\n","1be712cb":"learner.fit_onecycle(lr=2e-5,epochs=1)","207fa81d":"learner.fit_onecycle(lr=2e-4,epochs=3)\n","0ea6345b":"test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\npredictor = ktrain.get_predictor(learner.model,preproc)\ndata = ['i met you today by accident', 'i got today car accident, i am injured']\npredictor_class = predictor.predict(data, return_proba=True)\n\n","ebc634d6":"Word embeddings and classification","d01b51bf":"Most and least common words","eb85d94d":"Classification with TF-IDF and SVM","9fb7909d":"> # Exploratory Data Analysis","b0432d03":"Number of Charecters Distibution in Tweets\n","5abb2bab":"Number of words ,average word length,stopwords distribution in tweets"}}