{"cell_type":{"b6a490cc":"code","b531784e":"code","82be5d3b":"code","80b0721b":"code","d489ebf4":"code","20e12ac9":"code","db462eaf":"code","c7c1f368":"code","5aa921c2":"code","95859854":"code","fbb4410a":"code","d141f662":"code","97ae6b9e":"code","47232dab":"code","995dd8f9":"code","7fabdb9b":"code","7c157a44":"code","c57decd3":"code","671740cb":"code","2c28b53b":"code","d244c6ca":"code","80fb86ea":"code","eaffa4ec":"code","c95072e3":"code","a025485b":"code","8f19f363":"code","09d1c6af":"code","fb63ea9c":"code","378599a9":"markdown","8bedf34e":"markdown","2096d454":"markdown","7d940221":"markdown","b3bd145c":"markdown","7b0bddca":"markdown","39f0da1a":"markdown","3fa5f429":"markdown","95f2b176":"markdown"},"source":{"b6a490cc":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib as plt","b531784e":"df = pd.read_csv(\"..\/input\/market-basket-optimisationcsv\/Market_Basket_Optimisation.csv\", header = None)\ndf","82be5d3b":"records = []\nfor i in range(0, len(df)):\n    records.append([str(df.values[i,j]) for j in range(0, 20)])\n    records[i] = [x for x in records[i] if x != 'nan']","80b0721b":"dict_data = {'items': records}\ndf = pd.DataFrame.from_dict(dict_data)\n\ndf.head()","d489ebf4":"items = []\nfor i in range(len(df)):\n    for j in range(len(df['items'][i])):\n        items.append(df['items'][i][j])\n\nitems[0:10]","20e12ac9":"#Get unique element from list\/array\nunique_item = set(items)\nlist_unique_item = list(unique_item)\nlist_unique_item[0:5]","db462eaf":"count_unique = []\nfor value in (list_unique_item):\n    count_unique.append((value, items.count(value)))\ncount_unique[0:5]","c7c1f368":"candidate1 = pd.DataFrame(count_unique, columns=[\"itemset\", \"sup\"])\ncandidate1.head()\nlen(candidate1)","5aa921c2":"def filter_sup(candidate):\n    minimum_sup = 50\n    filtering = candidate['sup'] > minimum_sup\n    freq = candidate[filtering]\n    return freq","95859854":"freq_itemset1 = filter_sup(candidate1)","fbb4410a":"freq_itemset1","d141f662":"freq_itemset1['itemset'].iloc[0]","97ae6b9e":"def self_join(prev_freq_itemset):\n    self_join_candidate = []\n    for i in range(len(prev_freq_itemset['itemset'])):\n        for j in range((i+1), len(prev_freq_itemset['itemset'])):\n            itemset_i = prev_freq_itemset['itemset'].iloc[i]\n            itemset_j = prev_freq_itemset['itemset'].iloc[j]\n            if(type(itemset_i) == np.int64 and type(itemset_j) == np.int64):\n                itemset_i = {itemset_i}\n                itemset_j = {itemset_j}\n            union_candidate = itemset_i + \", \" + itemset_j\n\n            if union_candidate not in self_join_candidate:\n                self_join_candidate.append(union_candidate)\n    return self_join_candidate","47232dab":"candidate2_list = self_join(freq_itemset1)\n# candidate2_list","995dd8f9":"count_candidate2 = []\n\n#Set the Initial value of Second Count Candidate (C2)\nfor i in range(len(candidate2_list)):\n    count_candidate2.append((candidate2_list[i], 0))\n\ninitial_df_candidate = pd.DataFrame(count_candidate2, columns=['itemset', 'sup'])\nlen(initial_df_candidate)","7fabdb9b":"def count_support(database_dataframe, prev_candidate_list):\n    initial_df_candidate['sup'] = 0 #set All value into 0 only for initial value for consistency value when running this cell everytime.\n    count_prev_candidate = []\n\n    #Set the Initial value of Previous Candidate\n    for i in range(1000): #len(prev_candidate_list)\n        count_prev_candidate.append((prev_candidate_list[i], 0))\n    \n    df_candidate = pd.DataFrame(count_prev_candidate, columns=['itemset', 'sup'])\n    print('Database D dataframe\\n', database_dataframe)\n    print('(Initial) Dataframe from Candidate with All zeros sup\\n', df_candidate)\n    \n    for i in range(len(database_dataframe)):\n        for j in range(1000): #len(count_prev_candidate)\n            #using issubset() function to check whether every itemset is a subset of Database or not\n            if set(list(df_candidate['itemset'][j].split(\", \"))).issubset(set(list(database_dataframe['items'][i]))): \n                df_candidate.loc[j, 'sup'] += 1\n            \n    return df_candidate","7c157a44":"count_candidate2_df = count_support(df, candidate2_list)","c57decd3":"count_candidate2_df","671740cb":"freq_itemset2 = filter_sup(count_candidate2_df)\n\nfreq_itemset2 = freq_itemset2.reset_index(drop=True)\nfreq_itemset2","2c28b53b":"print(freq_itemset2[0:10])\nself_join_result = self_join(freq_itemset2)","d244c6ca":"print(len(self_join_result))\nself_join_result[0:10]","80fb86ea":"candidate3_list = self_join_result","eaffa4ec":"count_candidate3_df = count_support(df, candidate3_list)","c95072e3":"max(count_candidate3_df['sup'])","a025485b":"def filter_sup2(candidate):\n    minimum_sup = 20\n    filtering = candidate['sup'] > minimum_sup\n    freq = candidate[filtering]\n    return freq","8f19f363":"freq_itemset3 = filter_sup2(count_candidate3_df)\nfor i in range(len(freq_itemset3)):\n    print(\"Items: \", set(list(freq_itemset3['itemset'].iloc[i].split(\", \"))), \"\\t Support: \", freq_itemset3['sup'].iloc[i])","09d1c6af":"frequent_itemset = pd.concat([freq_itemset1, freq_itemset2, freq_itemset3], axis=0)\nfrequent_itemset_final = frequent_itemset.reset_index(drop=True)","fb63ea9c":"for i in range(len(frequent_itemset_final)):\n    print(frequent_itemset_final['itemset'].iloc[i], \"\\t\", frequent_itemset_final['sup'].iloc[i])","378599a9":"### Creating the Third Candidate (C3) - Using the Candidate Forming Technique","8bedf34e":"- **Support(s)** \u2013\nThe number of transactions that include items in the {X} and {Y} parts of the rule as a percentage of the total number of transaction.It is a measure of how frequently the collection of items occur together as a percentage of all transactions.\n\n    ```Support = \\sigma(X+Y) \\div total \u2013```\n\nIt is interpreted as fraction of transactions that contain both X and Y.\n\n- **Confidence(c)** \u2013\nIt is the ratio of the no of transactions that includes all items in {B} as well as the no of transactions that includes all items in {A} to the no of transactions that includes all items in {A}.\n\n    ```Conf(X=>Y) = Supp(X\\cupY) \\div Supp(X) \u2013```\n\nIt measures how often each item in Y appears in transactions that contains items in X also.\n\n- **Lift(l)** \u2013\nThe lift of the rule X=>Y is the confidence of the rule divided by the expected confidence, assuming that the itemsets X and Y are independent of each other.The expected confidence is the confidence divided by the frequency of {Y}.\n\n    ```Lift(X=>Y) = Conf(X=>Y) \\div Supp(Y) \u2013```\n\nLift value near 1 indicates X and Y almost often appear together as expected, greater than 1 means they appear together more than expected and less than 1 means they appear less than expected.Greater lift values indicate stronger association.","2096d454":"### Combining all the Frequent Itemsets","7d940221":"### Creating First Candidate (C1)","b3bd145c":"## Association Rule Mining\n\nAssociation rule mining finds interesting associations and relationships among large sets of data items. This rule shows how frequently a itemset occurs in a transaction. A typical example is Market Based Analysis.\n\nMarket Based Analysis is one of the key techniques used by large relations to show associations between items.It allows retailers to identify relationships between the items that people buy together frequently.\n\nGiven a set of transactions, we can find rules that will predict the occurrence of an item based on the occurrences of other items in the transaction.","7b0bddca":"### Creating Second Frequent Itemset (L2)","39f0da1a":"### Creating first Frequent Itemset (L1)","3fa5f429":"### Creating Third frequent Itemset","95f2b176":"### Choose second candidate"}}