{"cell_type":{"48b20eca":"code","c1560a2c":"code","9078ba29":"code","66207421":"code","959f4c35":"code","b505de10":"code","a64e4858":"code","7a7cc2f4":"code","5a61b69f":"code","21a98331":"code","0e01ed87":"markdown","7606c87a":"markdown","562ccc46":"markdown","c1ec9cf4":"markdown","7e571082":"markdown","13aa8007":"markdown","51318b27":"markdown","3d49e51b":"markdown","19972e4b":"markdown","3deb9ff2":"markdown","fb853f3f":"markdown","0abe6be4":"markdown"},"source":{"48b20eca":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # Matplot incase we need to chart some values\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c1560a2c":"train_data = pd.read_csv('..\/input\/30-days-of-ml\/train.csv') # Read the training file into a pandas dataframe called train_data\n\npd.set_option('display.max_columns', None) # When I do not set this option, the next line of code was only displaying some columns and not all.\n\ntrain_data.head() # This will display the first 5 rows of the training dataset. Run these two lines first and then go to the next line.\n","9078ba29":"#from pandas_profiling import ProfileReport\n#profile = ProfileReport(train_data, title=\"Pandas Profiling Report\")\n# profile.to_widgets()","66207421":"# Import the train_test_split function and uncomment\nfrom sklearn.model_selection import train_test_split\n\ny = train_data.target\n\nX = train_data.drop(['target','cat0','cat2','cat3','cat4','cat6','cat7','cat9'], axis=1) # This will remove the column named target from the training data\n\n# Remember X = features and y= target\n# Our models job is to guess 'y' when we provide it X. So the X_train data should have all columns except target.\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2)\n\n# train_test_split is a pandas function which helps us automatically split the data.\n# In this case I have asked pandas to split the data in 80:20 ration (0.8 and 0.2)\n# If you recollect we were using random_state = 1 for our excercises, this was just to help kaggle automatically evaluate our results.\n# Here we do not use random_state variable and hence the dataset will be split completely randomly.\n","959f4c35":"object_cols = ['cat1', 'cat5', 'cat8']\n","b505de10":"\"\"\"\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom hyperopt import Trials, STATUS_OK, tpe, hp, fmin, space_eval\nfrom catboost import CatBoostRegressor\nfrom math import sqrt\n\nspace ={\n    'learning_rate': hp.uniform('learning_rate', 0.01, 0.8),\n    'iterations': hp.quniform('iterations', 100, 150, 200),\n    'max_depth': hp.choice('max_depth', np.arange(2, 10, 1, dtype=int)),\n    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.5, 1.0),\n    'bagging_temperature': hp.uniform('bagging_temperature', 0.0, 100),\n    'random_strength': hp.uniform('random_strength', 0.0, 100)\n    }\n\n\ndef score(params):\n    model = CatBoostRegressor(**params)\n    \n    model.fit(X_train, y_train,\n               eval_set=(X_valid, y_valid),\n               cat_features=object_cols,\n               plot=True,\n               verbose=False)\n    y_pred = model.predict(X_valid).clip(0, 20)\n    score = sqrt(mean_squared_error(y_valid, y_pred))\n    print(score)\n    return {'loss': score, 'status': STATUS_OK}    \n    \ndef optimize(trials, space):\n    \n    best = fmin(score, space, algo=tpe.suggest, max_evals=10)\n    return best\n\ntrials = Trials()\nbest_params = optimize(trials, space)\n\n# Return the best parameters\n\nspace_eval(space, best_params)\n\"\"\"","a64e4858":"# TESTGROUND - ENDS","7a7cc2f4":"from catboost import CatBoostRegressor\nfrom math import sqrt\n\nmodel = CatBoostRegressor(\n    loss_function='RMSE',\n    learning_rate=0.5,\n    max_depth = 5,\n    random_strength = 1.0,\n    n_estimators = 2000,\n    bagging_temperature = 27.0,\n    colsample_bylevel = 1.0,\n    boosting_type = 'Ordered')\n\n# Fit model \nmodel.fit( X_train, y_train,\n               eval_set=(X_valid, y_valid),\n               cat_features=object_cols,\n               plot=True,\n               early_stopping_rounds=120,\n               verbose=False\n              )","5a61b69f":"from sklearn.metrics import mean_squared_error, r2_score\n\npreds_valid = model.predict(X_valid)\n\nscore = mean_squared_error(preds_valid,y_valid,squared=False)\n\nprint('RMSE:', score)","21a98331":"X_test = pd.read_csv('..\/input\/30-days-of-ml\/test.csv')\n\n# Fill in the line below: preprocess test data\nX_test_df = pd.DataFrame(X_test)\n\nfinal_X_test = X_test_df.drop(['cat0','cat2','cat3','cat4','cat6','cat7','cat9'], axis=1)\n\n\n\n# Fill in the line below: get test predictions\npreds_test = model.predict(final_X_test)\n\n\n# Save test predictions to file with only two columns, id and the prediction values under the target column\noutput = pd.DataFrame({'id': final_X_test.id,\n                       'target': preds_test})\noutput.to_csv('submission.csv', index=False)\n\nprint(output.tail())","0e01ed87":"# 3. Prepare the training data for EDA","7606c87a":" # 2. <div id=\"Dataset\"> Dataset <\/div>\n\n<span style=\"font-family:verdana;\"> So we find 3 files in the dataset. \n\n\n<span style=\"font-family:verdana;\"> **sample_submission.csv**: This helps us understand how our prediction submission should look like for Kaggle to generate out score. Dont worry about it now.\n\n<span style=\"font-family:verdana;\"> **train.csv** : This is the data with which we will train our model. The EDA will be done on this file. EDA just means we will explore this data and see what is inside it and based on the EDA, we can decide which model is more apt for this data. The training data needs to be split into training data and validation data, generally 25% is kept aside for validation. Read more about this in **[30 Days of ML] Day 9 Lesson 4**.\n    \n<span style=\"font-family:verdana;\"> **test.csv** : The test file has all the columns similar to the train file except the target column. \n    \n <\/span>","562ccc46":"# TESTGROUND - BEGINS","c1ec9cf4":"<span style=\"font-family:verdana;\"> \nHere I'm using the validation data set (20%) to get an approximate idea of my model accuracy. I have observed that this is very close to the score Kaggle gives usually to my submission. Hence I can improve my modelling without submitting the results to Kaggle every time.","7e571082":"<span style=\"font-family:verdana;\"> \nHere I create a list of column names that have categorical data. We will use this later to tell CATBOOST which features\/columns are categorical and which are continous\/numerical.","13aa8007":"# 8. Preparing for submission\n<span style=\"font-family:verdana;\"> \nI now prepare the test data set that was provided and use the model from earlier to predict the \"target\" column. \nNote that Kaggle wants you to submit the data exactly in the format visible in submission_sample.csv.","51318b27":"# 6. HYPEROPT - Automatic hyperparameter tuning\n\n<span style=\"font-family:verdana;\"> \nI'm  going to comment out the section below which I used to find the best hyper parameters for the CATBOOST Regressor algorithm. If you wish you can use the code to automatically test which parameter provides the best results. You can also use this with other algorithms like XGBoost or LGBM.\nThe reason I comment this is because I do not want it to run during my submission.","3d49e51b":"# 7. Algorithm selection, training and validation\n\n<span style=\"font-family:verdana;\"> \nNow that I have found some reasonable hyper parameters for my model, I will use it to train\/fit my model.\n    \n**Why Catboost Regressor?**\n    I tried using XGBoost, but after several iterations I could not get any improvements. It also had the added difficulty of preprocessing the categorical data as XGBoost does not handle categorical data without some modification or pre processing. Catboost on the other hand is designed to deal with categorical data automtically.\n    I also found it easy to tune Catboost and it does some tuning on its own.\n    \n** Why early_stopping_rounds during model fitting? **\n    \n    When you train a model, you dont want it to overfit the data that we have. This means if we fit our model to the exact data, then predictions on totally new data will be very inaccurate. We want it to stop when the fitting does not yield considerable improvements. This allows me to have n_estimator value at 2000. So if the model observes that its gaining accuracy, it will go on for 2000 iterations, but if its sees that 120 rounds of fitting gave the same RMSE, then it stops.\n\nAgain if you have questions, please post it in the discussion and I will explain further.","19972e4b":"# 4. Exploratory Data Analysis (EDA)\n<span style=\"font-family:verdana;\"> \nHere I have kept it simple and used the Pandas Profile Report library to automatically profile the train.csv data.\n\nSome key observations\n* There are 16 numeric columns or features. These are also known as continous variables.\n* There are 10 categorical columns or features. These have A - Z values in them.\n* Some cat columns have low cardinality ie. they have only few variations. For example cat0 has only A and B in it. I tried dropping such low cardinality columns but it had no impact on the model accuracy, hence I decided to keep them.\n* Columns have no missing data making our life quite easy. If there were missing values we would have to use imputation techniques. See ***[30 Days of ML] Day 13 - Lesson 4***\n* The target column is a continous variable column. Hence this is a **regression** problem.\n    \nNOTE: Uncomment\/Unhash the code below to see the profile report. I comment it out to make saving the notebook faster.\n<\/span>","3deb9ff2":"# Experimentation with CATBOOST Regression","fb853f3f":"# 5. Splitting the data (Training\/Validation)\n<span style=\"font-family:verdana;\"> \nThe training dataset needs to be split 80:20 for us to have a validation dataset with 20% of the known data.\nWhat this means is that we will train the algorithms on 80% of the training data and use the 20% data to validate the accuracy. If you have questions regarding this step, please post on the discussion board and I'll explain further. \nThis step was a bit difficult for me to understand as well in the beginning.\n<\/span>","0abe6be4":"# 1. Setting the stage\n<span style=\"font-family:verdana;\"> \n* Import the libraries.\n* Make sure the input files are accessible.\n<\/span>"}}