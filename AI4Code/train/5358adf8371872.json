{"cell_type":{"5eb85211":"code","5a70b30f":"code","8fffb29e":"code","d48e8ae0":"code","7cb2b6da":"code","9e9596f8":"code","844cc65f":"code","4da5a5cb":"code","d93ffa33":"code","324b3c54":"code","a3a33f6b":"code","8dfacab1":"code","6726425c":"code","8eee69b2":"code","fb863a2d":"code","13193942":"code","13c8088d":"markdown","39d5f558":"markdown","237ffeef":"markdown","2d272245":"markdown","f05d3045":"markdown","eca32224":"markdown","7280fee8":"markdown","caf73fd0":"markdown","2a347413":"markdown","ce33cdc1":"markdown","a1c8389f":"markdown","8c8851f2":"markdown","bf3f803c":"markdown","691a67cc":"markdown","635c1a5f":"markdown","57fe0f4a":"markdown","c1d6345c":"markdown","78f7b5e4":"markdown","31ea36ab":"markdown","7d4239e5":"markdown"},"source":{"5eb85211":"from numpy.lib.function_base import median\nimport pandas as pd\nimport numpy as np\nimport xarray as xr\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\ndf_train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv', index_col=0)\n\ndf_train.info()","5a70b30f":"\nimport re\nimport string\n\nurl_re = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\ntag_re = re.compile(r'<.*?>')\ntable_punct = str.maketrans('','',string.punctuation)\n\ndef text_preprocess(text):\n    text = url_re.sub(r'', text)\n    text = tag_re.sub(r'', text)\n    text = text.translate(table_punct)\n    return text\n\n\ntexts = df_train['text'].apply(text_preprocess)\ntarget = df_train['target']\n\n#TODO: does this need to happen, as gridsearch already is doing cross-validation\nX_train, X_test, y_train, y_test = train_test_split(texts, target,train_size=0.25)\n","8fffb29e":"\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import RidgeClassifier\n\n\npipe = Pipeline([\n    ('vect', CountVectorizer()),\n    ('tsvd', TruncatedSVD()),\n    ('clf', RidgeClassifier()),\n])\n\npipe.fit(X_train,y_train)\npipe.score(X_test, y_test)","d48e8ae0":"from sklearn.model_selection import GridSearchCV\n\nparams = {\n    'vect__min_df': np.linspace(0,0.01,4),\n    'tsvd__n_components': [int(n) for n in np.logspace(0.3,2,4)],\n    'tsvd__n_iter': [1,3,5,7],\n    'clf__alpha': np.linspace(0.5,1, 3),\n\n}\n\ngridsearch = GridSearchCV(pipe, params) \n\ngridsearch.fit(X_train, y_train)","7cb2b6da":"results = gridsearch.cv_results_\n","9e9596f8":"results['mean_test_score'].shape","844cc65f":"results['param_vect__min_df'].data.shape","4da5a5cb":"var_keys = ['mean_test_score','mean_fit_time']\nresults_downselect = {key: results[key] for key in var_keys}\n\nparam_grid = gridsearch.param_grid\nparam_coords = {param : results['param_' + param].data for param in param_grid.keys()}\n\nmi = pd.MultiIndex.from_arrays(list(param_coords.values()), names = param_grid.keys())\n\ndf = pd.DataFrame(results_downselect, index=mi)\n\ndf.head()","d93ffa33":"ds = xr.Dataset.from_dataframe(df)\nds","324b3c54":"ds['mean_test_score']","a3a33f6b":"\nds['mean_test_score'].plot(col='clf__alpha', row='tsvd__n_components')","8dfacab1":"ds_sel = ds.sel(tsvd__n_iter = 5, clf__alpha = 0.75)\n\nds_sel['mean_test_score'].plot()","6726425c":"ds_sel['mean_test_score'].plot(hue='vect__min_df', marker='o', xscale='log')","8eee69b2":"ds_sel['mean_fit_time'].plot(hue='vect__min_df', marker='o', xscale='log')","fb863a2d":"ds_sel['ratio'] = ds_sel['mean_test_score']\/ds_sel['mean_fit_time']","13193942":"ds_sel.to_array('var').plot(row = 'var', hue='vect__min_df', marker='o', xscale='log', sharey=False)","13c8088d":"At this stage we could analyze the results with the multindexed dataframe. However, working with multiindexes in pandas is somewhat cumbersome as we must keep track of the different levels of the index during downselection etc. \n\nThis is where xarray comes in, which is a package specifially meant for multidimensional datasets. xarray and pandas are highly interoperable, and we can simply create a dataset from using the Dataset.from_dataframe method","39d5f558":"One useful technique is to transform the variables into a dimension in a dataarray with the `Dataset.to_array` method, to be able to plot the variables along a column or row. \n\nBecause each variable will have a very different y axis, it's important to pass the `sharey = False` keyword argument.","237ffeef":"To plot the second dimension as different color lines we use the 'hue' parameter.\n\nMatplotlib keyword arguments can be passed to matplotlib, such as the 'markers' or 'xscale' parameter. ","2d272245":"And the corresponding coordinates for one of the grid search parameters:  ","f05d3045":"Let's calculate the ratio of the score to the fit time to try and quantify this tradeoff, and assign this to a new variable in the dataset ","eca32224":"# Conversion of grid search results to xarray DataSet and visualization","7280fee8":"# Analyzing hyperparameter gridsearch results with xarray\n\nThis notebook will show how to explore a multidimensional hyperparameter sweep using the xarray package. The xarray package builds upon pandas to work with and quickly analyze datasets with an arbitrary number of dimensions. We can use this to understand the effect of each hyperparameter into our model.\n\nI will keep the actual NLP methods used in this notebook to a basic gridsearchCV using a standard sklearn pipeline. The original contribution of the notebook is a conversion of of the gridsearchCV results to an xarray dataset, and an overview of how to use the xarray package to visualize the results. If you are familiar with doing a gridsearch with sklearn you should skip to that part. ","caf73fd0":"It doesn't seem like the TVSD number of interations or the classifier alpha don't seem to have a dramatic effect. I'm going to downselect the data visually to the point `tsvd_n_iter = 5` and `alpha = 0.75`","2a347413":"## Data visualiztion\n\nIt is possible to visualize up to 4 dimensions","ce33cdc1":"## Build the sklearn pipeline\n\nI build a standard sklearn pipeline for classifying text and perform a fit\/scoring just to ensure that things are setup properly. ","a1c8389f":"## Setup and perform a hyperparameter grid search","8c8851f2":"## Perform some basic text processing\n\nI am just using some functions to clean up the text from a 'getting started notebook'","bf3f803c":"Now we have a `xarray.Dataset` object, which is analagous to a `pandas.DataFrame`, where the 'data variables' are analagous to columns. The key difference is that now our data is represented as an N-dimensional array, instead of a 2D tabular form as the multindexed DataFrame. \n\nIndividual variables can be selected out as `xarray.DataArray` objects (analgous to `pandas.Series`)","691a67cc":"I won't go into the details of the xarray package but just demonstrate some basic visualization. ","635c1a5f":"## Load in the data","57fe0f4a":"At this stage, the most often practice is to just select the model parameters that gave the highest test score. However we may want to actually examine the dependence of the results on the model paramers in order to better understand our model, and see which parameters are particulalry important or not important. \n\nThe parameter-dependent results of the gridsearchCV are located in `gridsearch.cv_results_`. ","c1d6345c":"Even though our search was over a 'grid' of parameters, both the results and corresponding parameters are output in a 1D arrays. For example the 'mean_test_score':","78f7b5e4":"The score increases as we reduce the amount of dimensional reduction, as expected. Looking at the computational time we can see the tradeoff: ","31ea36ab":"# Perform NLP and grid search with sklearn","7d4239e5":"First we convert this data to a multindexed pandas dataframe. It would be possible to skip this step and just directly form the xarray Dataset, but this 1D input data is particularly amenable to making a multindexed dataframe, where each point as multiple indices. \n\nI will only look at some key results: `mean_test_score` and `mean_fit_time`"}}