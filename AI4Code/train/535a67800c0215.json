{"cell_type":{"055a7b3b":"code","cf65d7d9":"code","402dac06":"code","3d1d345e":"code","289484c6":"code","beb011c3":"markdown","b2052067":"markdown","8d3e9a33":"markdown","1bc22279":"markdown","dd7bb78f":"markdown","2d39873b":"markdown"},"source":{"055a7b3b":"#===========================================================================\n# import the libraries\n#===========================================================================\nimport pandas  as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#===========================================================================\n# read in the data\n#===========================================================================\ntrain_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data  = pd.read_csv('..\/input\/titanic\/test.csv')\n\n#===========================================================================\n# features: Sex, and a bit of class...\n#===========================================================================\nfeatures = [\"Sex\", \"Pclass\"]\n\n#===========================================================================\n# for the features that are categorical we use pd.get_dummies\n#===========================================================================\nX_train       = pd.get_dummies(train_data[features])\ny_train       = train_data[\"Survived\"]\nX_test        = pd.get_dummies(test_data[features])\n\n#===========================================================================\n# perform the classification using a decision tree \n#===========================================================================\nfrom sklearn import tree\nclassifier = tree.DecisionTreeClassifier(criterion='gini',\n                                         splitter='best',\n                                         max_depth=2,\n                                         random_state=42)\nclassifier.fit(X_train, y_train)","cf65d7d9":"from sklearn.tree import export_graphviz\nimport graphviz\n\ndot_data = export_graphviz(classifier, \n                           feature_names=X_train.columns, \n                           class_names=['Died', 'Survived'], \n                           filled=True, \n                           rounded=True,\n                           proportion=False)\ngraphviz.Source(dot_data)","402dac06":"classifier = tree.DecisionTreeClassifier(criterion='entropy',\n                                         splitter='best',\n                                         max_depth=2,\n                                         random_state=42)\nclassifier.fit(X_train, y_train)\n\n# now visualise the 'entropy' tree\ndot_data = export_graphviz(classifier, \n                           feature_names=X_train.columns, \n                           class_names=['Died', 'Survived'], \n                           filled=True, \n                           rounded=True,\n                           proportion=False)\ngraphviz.Source(dot_data)","3d1d345e":"plt.figure(figsize = (12,4))\nlimit = 1\nx = np.linspace(0.01, limit-0.01, 50)\nline_1 = 1 - (x**2) - (1-x)**2\nline_2 = ( -1*x*np.log2(x) ) - ( (1-x)*np.log2(1-x) )\n#------------------------------------------\nplt.plot(x,line_1, color='darkorange', linestyle='solid', lw=3)\nplt.plot(x,line_2, color='navy',       linestyle='dashed', lw=3)\n#------------------------------------------\nplt.title   (\"Plot of the gini (orange) and entropy (blue) impurity criterion\")\nplt.xlabel  (\"p\")\nplt.ylabel  (\"impurity\")\n#------------------------------------------\nplt.xlim    (0, limit)\nplt.ylim    (0, limit+0.1)\n#------------------------------------------\nplt.grid(True)\n#------------------------------------------\nplt.show()","289484c6":"# first fill an array with zeros (i.e. initially there are no survivors at all)\npredictions = np.zeros((418), dtype=int)\n\n# now use our model\nsurvived_df = X_test[((X_test[\"Pclass\"] ==1)|(X_test[\"Pclass\"] ==2)) & (X_test[\"Sex_female\"]==1 )]\n\nfor i in survived_df.index:\n    predictions[i] = 1 # the 1's are now the survivors\n    \n#===========================================================================\n# write out CSV submission file\n#===========================================================================\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission.csv', index=False)","beb011c3":"The final score is given at the very top of this notebook: we can see that this extremely simple and highly interpretable model is **77.5% accurate**.\n# Further reading\n* [Decision Trees](https:\/\/scikit-learn.org\/stable\/modules\/tree.html) on Scikit-learn\n* [Decision tree learning](https:\/\/en.wikipedia.org\/wiki\/Decision_tree_learning) on Wikipedia\n* [Lior Rokach and Oded Maimon \"Decision Trees\" in 'Data Mining and Knowledge Discovery Handbook' Chapter 9 pp 165-192 (2005)](https:\/\/doi.org\/10.1007\/0-387-25465-X_9)\n* [S. B. Kotsiantis \"Decision trees: A recent overview\",  Artif. Intell. Rev. **39** pp. 261-283 (2013). ](https:\/\/doi.org\/10.1007\/s10462-011-9272-4)\n\n# Related notebooks\nI am far from the first person on kaggle to apply a decision tree classifier to the Titanic dataset or otherwise, and here are links to some notebooks that are well worth taking a look at:\n* [Introduction to Decision Trees (Titanic dataset)](https:\/\/www.kaggle.com\/dmilla\/introduction-to-decision-trees-titanic-dataset) by [Diego Milla](https:\/\/www.kaggle.com\/dmilla)\n* [Playing with the knobs of sklearn decision tree](https:\/\/www.kaggle.com\/drgilermo\/playing-with-the-knobs-of-sklearn-decision-tree) by [Omri Goldstein](https:\/\/www.kaggle.com\/drgilermo)\n* [Decision-Tree Classifier Tutorial ](https:\/\/www.kaggle.com\/prashant111\/decision-tree-classifier-tutorial) by [Prashant Banerjee](https:\/\/www.kaggle.com\/prashant111)\n* ...and *many* more...","b2052067":"# Plot the decision tree\nNow let us take a look at the decisions that were made in producing our tree by [exporting the decision tree in DOT format](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.export_graphviz.html) and then using the visualization software [graphviz](http:\/\/https:\/\/graphviz.org\/) to draw the tree.","8d3e9a33":"## Which is best, gini or entropy?\nAs we have seen above, both splitting criteria result in the same tree, and indeed, there is no general consensus as to which is the best. In the paper [\"Theoretical Comparison between the Gini Index and Information Gain Criteria\"](https:\/\/doi.org\/10.1023\/B:AMAI.0000018580.96245.c6) by Raileanu and Stoffel they summarize by saying: \n> \"*...they disagree only in 2% of all cases, which\nexplains why most previously published empirical results concluded that it is not possible\nto decide which one of the two tests performs better.*\"\n\n# Score the model\nWe now proceed to apply our above model obtained from the training data `X_train` and `y_train`, which was \n\n> *Everyone dies except females who were in either 1st or 2nd class.*\n\nto the test data, `X_test`, and submit it to the competition:","1bc22279":"we can see that in this instance the results are exactly the same as those obtained with the *gini impurity*.\nWhat do these two impurity functions look like? We shall make a plot","dd7bb78f":"# Interpretation\nAt the top of the graph, the *root node*, a condition is framed whose answer is either `True` or `False`. The condition in the root node is the most influential, or ***best split*** in the tree, which is calculated using an impurity measure (here `gini`). Here the root node divides by `Sex`\nIn this case we now have two test, or *internal nodes*, which divides by `Pcalss`.\nFinally in this example, where the stopping criteria is `max_depth=2`, we end up with four terminal decision or *leaf nodes*, or simply *leaves*, this is where the classification takes place.\n\nIn this example the root node splits the 891 samples (i.e. passengers) contained in the training data according to `Sex`; the 577 males to the left (True) and the 314 females to the right (False). \nThe next two internal nodes split according to `Pclass`. \nFinally, the leftmost leaf contains males that were in 1st class (122), the next leaf contains males that were either in 2nd or 3rd class (455). The third leaf, the only leaf classifies as *Survived*, consists of females who were either in 1st or 2nd class (170). The final rightmost leaf contains the 144 females who were in the 3rd class. \n\n**In summary, in this simple decision tree everyone dies except females who were in either 1st or 2nd class. This is now our model.**\n\n# What is gini?\nWe can see that each node and leaf has a `gini` value. This is the *gini impurity* ($I_G$) splitting criteria, which is given by\n\n$$ I_G(p) = 1 - \\sum_{i=1}^J p_i^2 $$\n\nwhere $p_i$ is the fraction of samples in class $i$. In this case we have\n\n$$ I_G = 1 - p( \\mathrm{True} )^2 - p( \\mathrm{False} )^2 $$\n\nFor example, in the root node we see `gini = 0.473`:\n\n$$ I_G = 1 - \\left( \\frac{549}{891} \\right)^2 - \\left( \\frac{342}{891} \\right)^2 = 0.473 $$\n\nThe root node is the class that has the lowest gini impurity, i.e. leads to the best separation of classes, in this case Sex.\n\n# Shannon information entropy\nAnother impurity criteria is the [Shannon information entropy](http:\/\/people.math.harvard.edu\/~ctm\/home\/text\/others\/shannon\/entropy\/entropy.pdf) ($I_E$), which is implemented with the flag `entropy` and is given by\n\n$$ I_E(p) =  - \\sum_{i=1}^J p_i \\log_2 (p_i) $$\n\nwhich for the root node would be\n\n$$ I_E(p) = - \\left( \\frac{549}{891} \\right) \\log_2  \\left( \\frac{549}{891} \\right)  - \\left( \\frac{342}{891} \\right) \\log_2 \\left( \\frac{342}{891} \\right)  = 0.961 $$\n\nFor example:","2d39873b":"![image.png](attachment:image.png)\n# The Decision Tree\nDecision trees are perhaps one of the most venerable techniques used in machine learning, notably the [ID3](https:\/\/link.springer.com\/content\/pdf\/10.1007\/BF00116251.pdf) (published in 1986) and **C4.5** (published in 1993) algorithms  by [Ross Quinlan](https:\/\/en.wikipedia.org\/wiki\/Ross_Quinlan). The algorithm operates in a [greedy](https:\/\/en.wikipedia.org\/wiki\/Greedy_algorithm) top-down  recursive fashion.\nOne of the positive aspects of decision trees is their interpretability. Another attractive aspect is that they require very little pre-processing of the input data. A disadvantage of decision trees is that, since they are greedy, they can be overly sensitive to very small variations in the input training data.\n\n# A simple example:\nFor this example we shall use data from the kaggle [Titanic competition](https:\/\/www.kaggle.com\/c\/titanic). We shall use only two of the features: `Sex` (either male or female, which we treat as [dummy variables](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.get_dummies.html)) and `Pclass`, the passenger class, which adopts the values: 1 = 1st class, 2 = 2nd class, and 3 = 3rd class (hence the title of this notebook).\nWe shall be making use of the scikit-learn [DecisionTreeClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html)."}}