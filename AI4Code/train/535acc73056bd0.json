{"cell_type":{"16ca362d":"code","00ffc780":"code","6ad0bc43":"code","4ea3d1bb":"code","bb1376d7":"code","9df4a0cc":"code","cbd649ec":"code","f2e866b9":"code","4ee19fcf":"code","e102e816":"code","a0fc618d":"code","0a059fad":"code","e8ee32f2":"code","e690cc18":"code","69719c10":"code","27467622":"code","1af4a991":"code","a0de6843":"code","f772c48f":"code","7ebf8a62":"code","9d283426":"code","2c75685c":"markdown","f3054d9c":"markdown","52a30c77":"markdown","ba021d82":"markdown","032e43f5":"markdown","712bfbac":"markdown","ae05fac9":"markdown","d54d268a":"markdown","9855bc8f":"markdown","8cd7949a":"markdown","b552231c":"markdown","ccb737ab":"markdown","00c8a12b":"markdown","7dcd3a63":"markdown"},"source":{"16ca362d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport seaborn as sns\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix","00ffc780":"BASE_PATH = \"..\/input\/medical-mnist\/\"","6ad0bc43":"def count_of_images(dir_path):\n    path, dirs, files = next(os.walk(dir_path))\n    return len(files)","4ea3d1bb":"abdomenCT_count = count_of_images(BASE_PATH+\"AbdomenCT\/\")\nbreastMRI_count = count_of_images(BASE_PATH+\"BreastMRI\/\")\ncxr_count = count_of_images(BASE_PATH+\"CXR\/\")\nchestCT_count = count_of_images(BASE_PATH+\"ChestCT\/\")\nhand_count = count_of_images(BASE_PATH+\"Hand\/\")\nheadCT_count = count_of_images(BASE_PATH+\"HeadCT\/\")","bb1376d7":"classes = [\"AbdomenCT\", \"BreastMRI\", \"CXR\", \"ChestCT\", \"Hand\", \"HeadCT\"]\nnumber_of_classes = [abdomenCT_count, breastMRI_count, cxr_count, chestCT_count, hand_count, headCT_count]","9df4a0cc":"plt.figure(figsize=(10, 10))\nsns.barplot(classes, number_of_classes)","cbd649ec":"abdomen_img = np.asarray(Image.open(\"..\/input\/medical-mnist\/AbdomenCT\/000000.jpeg\"))\nplt.imshow(abdomen_img, cmap=\"gray\")","f2e866b9":"breast_img = np.asarray(Image.open(\"..\/input\/medical-mnist\/BreastMRI\/000000.jpeg\"))\nplt.imshow(breast_img, cmap=\"gray\")","4ee19fcf":"cxr_img = np.asarray(Image.open(\"..\/input\/medical-mnist\/CXR\/000000.jpeg\"))\nplt.imshow(cxr_img, cmap=\"gray\")","e102e816":"chest_img = np.asarray(Image.open(\"..\/input\/medical-mnist\/ChestCT\/000000.jpeg\"))\nplt.imshow(chest_img, cmap=\"gray\")","a0fc618d":"hand_img = np.asarray(Image.open(\"..\/input\/medical-mnist\/Hand\/000000.jpeg\"))\nplt.imshow(hand_img, cmap=\"gray\")","0a059fad":"head_img = np.asarray(Image.open(\"..\/input\/medical-mnist\/HeadCT\/000000.jpeg\"))\nplt.imshow(head_img, cmap=\"gray\")","e8ee32f2":"transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.3), (0.3))\n])","e690cc18":"dataset = datasets.ImageFolder(\"..\/input\/medical-mnist\/\", transform=transform)\ntrain_set, val_set = torch.utils.data.random_split(dataset, [50000, 8954])\n\ntrain_dataloader = torch.utils.data.DataLoader(train_set, batch_size=4)\nval_dataloader = torch.utils.data.DataLoader(val_set, batch_size=1)","69719c10":"for i, data in enumerate(train_dataloader):\n    img, target = data\n    img = img.detach().numpy()[0]\n    img = np.transpose(img, (2, 1, 0))\n    plt.imshow(img, cmap=\"gray\")\n    break","27467622":"class MedMNISTModel(nn.Module):\n    \n    def __init__(self):\n        super(MedMNISTModel, self).__init__()\n        self.conv_model = nn.Sequential(\n            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        \n        self.classification_model = nn.Sequential(\n            nn.Linear(in_features=2304, out_features=128),\n            nn.ReLU(),\n            nn.Linear(in_features=128, out_features=128),\n            nn.ReLU(),\n            nn.Linear(in_features=128, out_features=6),\n        )\n        \n        self.optimizer = optim.Adam(self.parameters(), lr=0.001)\n        self.criterion = nn.CrossEntropyLoss()\n        \n    def forward(self, x):\n        x = self.conv_model(x)\n        \n        # Flattening\n        x = torch.flatten(x, start_dim=1)\n        \n        x = self.classification_model(x)\n        return x\n    \nmodel = MedMNISTModel()\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)","1af4a991":"EPOCHS = 10\n\nfor epoch in range(EPOCHS):\n    running_loss = 0.0\n    for i, data in enumerate(train_dataloader):\n        imgs, targets = data[0].to(device), data[1].to(device)\n        outputs = model(imgs)\n        loss = model.criterion(outputs, targets)\n    \n        model.optimizer.zero_grad()\n        loss.backward()\n        model.optimizer.step()\n        running_loss += loss.item()\n        if i % 2000 == 1999:   \n            print('[%d, %5d] loss: %.3f' %\n                  (epoch + 1, i + 1, running_loss \/ 2000))\n            running_loss = 0.0","a0de6843":"predicted = []\ntargets = []\n\nfor i, data in enumerate(val_dataloader):\n    img, target = data[0].to(device), data[1].item()\n    output = np.argmax(F.softmax(model(img).cpu(), dim=1).detach().numpy())\n    predicted.append(output)\n    targets.append(target)","f772c48f":"torch.save(model.state_dict(), \"model.pth\")","7ebf8a62":"accuracy = np.asarray(accuracy_score(targets, predicted)*100)\naccuracy = accuracy.round(2)\nprint(\"Accuracy of the model is {}%\".format(accuracy))","9d283426":"cf_mt = confusion_matrix(targets, predicted)\nplt.figure(figsize=(10, 10))\nsns.heatmap(cf_mt, annot=True)","2c75685c":"### How we can see on the confusion matrix, the model has a problem with recognizing chest CT and headCT. It has 68 mistakes.","f3054d9c":"### The model has accuracy on level 99%. Medical MNIST is the dataset that is easy to learn. The images are relatively small (64x64 px). They don't have a lot of details, but this model we can use to further learning.","52a30c77":"## Classes in the dataset:\n* AbdomenCT - computed tomography of the abdominal cavity\n* BreastMRI - MRI of the breast\n* CXR - chest X-RAY\n* ChestCT - computed tomography of the chest\n* Hand - hand (X-RAY)\n* HeadCT - computed tomography of the head","ba021d82":"# Confusion matrix","032e43f5":"## In this dataset, there are 10000 images in every class, not including Breast MRI where are 8954 images.","712bfbac":"# Sample images","ae05fac9":"# Computing the accuracy","d54d268a":"# Saving the model","9855bc8f":"# Data overview","8cd7949a":"# Preparing datasets","b552231c":"# Creating model","ccb737ab":"# Validating the model","00c8a12b":"# Training model","7dcd3a63":"# Summary"}}