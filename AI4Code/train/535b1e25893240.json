{"cell_type":{"8fbe26d7":"code","0049fd08":"code","8e8c7f22":"code","b9b191b3":"code","d7475cda":"code","2a422c9e":"code","011d1694":"code","c1ad955b":"code","31da85bc":"code","baa53530":"code","32cb157b":"code","1a81039b":"code","d8e1226b":"markdown","904caeb3":"markdown","5d3184d1":"markdown","ed665f54":"markdown","db9c8df3":"markdown","da2c19a5":"markdown","f89d03a9":"markdown","00798a65":"markdown"},"source":{"8fbe26d7":"#Import necessay libraries\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport matplotlib\nmatplotlib.rcParams[\"figure.figsize\"] = (20,10)\nimport seaborn as sns\n\n#Preprocessing\nfrom sklearn import model_selection,metrics\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split,KFold,StratifiedKFold\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler,OrdinalEncoder,LabelEncoder,RobustScaler,MinMaxScaler\nfrom sklearn.model_selection import RepeatedKFold\n\n#Model\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error,roc_auc_score\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_log_error,mean_squared_error\nfrom catboost import CatBoostRegressor\nimport time","0049fd08":"#import the data and shape\ntrain = pd.read_csv(\"..\/input\/song-popularity-prediction\/train.csv\")\ntest = pd.read_csv(\"..\/input\/song-popularity-prediction\/test.csv\")\nsample=pd.read_csv(\"..\/input\/song-popularity-prediction\/sample_submission.csv\")\nprint(train.shape,test.shape,sample.shape)\ntrain.describe().transpose()\n","8e8c7f22":"train.info()","b9b191b3":"plt.rc('figure',figsize= (10,12))\nsns.set_context('paper',font_scale=1)\n\nplt.title('Missing value status',fontweight = 'bold')\nax = sns.heatmap(train.isnull().sum().to_frame(),annot=True,fmt = 'd',cmap = 'rainbow')\nax.set_xlabel('Amount Missing')\nplt.show()","d7475cda":"# !rm -r kuma_utils\n!git clone https:\/\/github.com\/analokmaus\/kuma_utils.git\n    \nimport sys\nsys.path.append(\"kuma_utils\/\")\nfrom kuma_utils.preprocessing.imputer import LGBMImputer\n\nimputer = LGBMImputer(n_iter=100,verbose=True)\ntrain_im = pd.DataFrame(imputer.fit_transform(train))\ntest_im = pd.DataFrame(imputer.fit_transform(test))\n#remove column\ntrain_im.columns = train.columns\ntest_im.columns = test.columns\n\ntrain = train_im\ntest = test_im","2a422c9e":"#insert the kfold columns\ntrain['kfold'] = -1\n#distributing the data\nkfold=KFold(n_splits=10,random_state=42)\nfor fold, (tr_i,va_i) in enumerate(kfold.split(X=train)):\n    train.loc[va_i,'kfold'] = fold\n    \nprint(train.kfold.value_counts())\ntrain.to_csv(\"folds_10.csv\",index=False)\nprint(\"successfully folds\")\n","011d1694":"train.isnull().sum()","c1ad955b":"train.head()","31da85bc":"df = pd.read_csv(\".\/folds_10.csv\")\n\n#features taken to train\nfeatures = [f for f in df.columns if f not in(\"id\",\"kfold\",\"song_popularity\")]\ntest= test[features]\n","baa53530":"df[features].head()","32cb157b":"prediction = []\nscore = []\n\nfor fold in range (10):\n    xtrain = df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = test.copy()\n    \n    ytrain = xtrain.song_popularity\n    yvalid = xvalid.song_popularity\n    \n    xtrain = xtrain[features]\n    xvalid = xvalid[features]\n    \n    lE = RobustScaler()\n    xtrain[features] = lE.fit_transform(xtrain[features])\n    xvalid[features] = lE.transform(xvalid[features])\n    xtest[features] = lE.transform(xtest[features])\n    \n    \n    #Model hyperparameter of XGboostRegressor\n    #lgb parameters\n    params_lgb = {\n    \"task\": \"train\",\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"regression\",\n    'subsample': 0.95312,\n    'learning_rate': 0.001635,\n    \"max_depth\": 6,\n    \"feature_fraction\": 0.2256038826485174,\n    \"bagging_fraction\": 0.7705303688019942,\n    \"min_child_samples\": 290,\n    \"reg_alpha\": 14.68267919457715,\n    \"reg_lambda\": 66.156,\n    \"max_bin\": 772,\n    \"min_data_per_group\": 177,\n    \"bagging_freq\": 1,\n    \"cat_smooth\": 96,\n    \"cat_l2\": 17,\n    \"verbosity\": -1,\n    'random_state':42,\n    'n_estimators':15000,\n    'colsample_bytree':0.1107\n    }\n    \n    lgb_train = lgb.Dataset(xtrain, ytrain)\n    lgb_val = lgb.Dataset(xvalid, yvalid)\n    \n    model = lgb.train(params=params_lgb,\n                      train_set=lgb_train,\n                      valid_sets=lgb_val,\n                      early_stopping_rounds=300,\n                      verbose_eval=False)\n    \n   \n    preds_valid = model.predict(xvalid,num_iteration=model.best_iteration)\n    test_predict = model.predict(xtest,num_iteration=model.best_iteration)\n    \n    prediction.append(test_predict)\n    roc1= roc_auc_score(yvalid,preds_valid)\n    #Score \n    score.append(roc1)\n    print(f\"fold|split:{fold},roc:{roc1}\")\n    \nprint(np.mean(score),np.std(score))","1a81039b":"\nfinal_predict = np.mean(np.column_stack(prediction),axis=1)\nprint(final_predict)\nsample.song_popularity = final_predict\nsample.to_csv(\"submission_lgb.csv\",index=False)\nprint(\"Final achieve to send lgbboost output data\")\n\n","d8e1226b":"## **Song Popularity Prediction**\n\nDay1: [https:\/\/www.kaggle.com\/venkatkumar001\/spp0-baseline-pls-rf](http:\/\/),\n\nDay2: [https:\/\/www.kaggle.com\/venkatkumar001\/spp1-xgb](http:\/\/) Version6 (XGB),\nDay2: https:\/\/www.kaggle.com\/venkatkumar001\/spp1-xgb --> Version 9 (CAT)\n\nDay3:[https:\/\/www.kaggle.com\/venkatkumar001\/spp2-lgbm](http:\/\/)\n\n**Today I'm trying LGBM**\n\n1. Import necessary Library\n2. Read Data, identify shape, describe what's data have in statistical formats\n3. Then identify Null data and graphical formats\n4. Then NaN place replace the Mean,Median,Mode values in statistical\n5. Cross validation using KFOLD (5split)\n6. feature separation and train the model in different hyparameters\n7. Roc_auc_score\n8. Save the CSV file\n\n### **Explore ---> Different Hyperparameters tuning and train the model**","904caeb3":"## **Predict output**","5d3184d1":"## **Feature Separation**","ed665f54":"## **Build_Model**","db9c8df3":"## **Apply Simple_Imputers**","da2c19a5":"**I think Boosting techniques are awesome**\n\n**If you are any queries refer the link,**\n\n**LGBM: [https:\/\/lightgbm.readthedocs.io\/en\/latest\/pythonapi\/lightgbm.LGBMRegressor.html](http:\/\/)**","f89d03a9":"### **I think step by step process any queries(30dayml playlist [https:\/\/youtu.be\/_55G24aghPY](http:\/\/))**\n\n**Any suggestion to improve the problem comment**\n\n### ***ThankYou for visiting (Explore different hyperparameter using LGBM) using OPTUNA***","00798a65":"## **Read Data**"}}