{"cell_type":{"3f9b10d3":"code","c0090aab":"code","5379c013":"code","64f6694c":"code","a65fdb20":"code","36f13efa":"code","70776cc8":"code","207aa57b":"code","d7cae5a2":"code","564b6688":"code","ac18b7b5":"code","2bdfab54":"code","a23a96bf":"code","abd02bc1":"code","606ae96c":"code","30392aee":"code","5867864f":"code","a8edfa0a":"code","505f1989":"code","4bba03b5":"code","e8b67396":"code","dc31c173":"code","51ec1a77":"code","a121c07e":"code","84e7c3a1":"code","cb59123f":"code","edd21cf9":"code","92708af0":"code","c54c3942":"code","16203537":"code","b5f60f97":"code","866b47b6":"code","1d39335f":"code","b95b1dcd":"code","995ad0bc":"code","11a04317":"code","083439c5":"code","c8d8a9a1":"code","9b5dbf34":"code","f8795f3f":"code","c4abd4d4":"code","3a778a05":"code","f9ad5e56":"code","584605ee":"code","5618750b":"code","c197a233":"code","63f4f4ca":"code","6ac8884a":"code","484340a4":"code","c730393d":"code","a65b1d80":"code","77236b9d":"code","6f99c321":"markdown","5c4045ba":"markdown","9863bf72":"markdown","eab1b732":"markdown","25ac2863":"markdown","6111d83c":"markdown","16b55863":"markdown","b1865151":"markdown","1ab1191c":"markdown","933dd5af":"markdown","98c6336d":"markdown","40608f53":"markdown","e1f91dd6":"markdown","27f8e362":"markdown","6e961a69":"markdown","71cee259":"markdown","b3d4fffd":"markdown","e6ccc9ff":"markdown","a04bf9d3":"markdown","da4b3699":"markdown","af8dfc9e":"markdown","e98690e3":"markdown","87c0dffc":"markdown","ab7a4c70":"markdown","3d6905ee":"markdown","7bf08266":"markdown","a78dbced":"markdown","643c48c8":"markdown","90121e25":"markdown","4c832bf2":"markdown","3b1f28cb":"markdown","a6464f86":"markdown","86bfb8fc":"markdown","ea4c3d4b":"markdown","6cc2aeea":"markdown","065a98a7":"markdown","19d2cb24":"markdown","3b2e93c9":"markdown","b4397a59":"markdown","176427b2":"markdown","b7b56d99":"markdown","6cba8037":"markdown","f2689aa5":"markdown","2c54b355":"markdown","92d1315e":"markdown","ced5052e":"markdown","26d9981c":"markdown","f447e855":"markdown","8f5f3d68":"markdown","a8cbf7d6":"markdown","0cf253f1":"markdown","94d8030a":"markdown","6169629f":"markdown","39fd4d6c":"markdown","41fc608d":"markdown","20e38ac6":"markdown","55e84906":"markdown","a473ab73":"markdown","57317411":"markdown","55f2b06b":"markdown","6f59fbd0":"markdown","0bdba828":"markdown","9b1bca58":"markdown","ab0f1e11":"markdown","e642a7c6":"markdown","99b3ab15":"markdown","ce7ccf77":"markdown","36247346":"markdown","8f71d3f5":"markdown","1d73052c":"markdown","ed4b1d9d":"markdown","0321caf5":"markdown","53af365d":"markdown","3f2c7c6f":"markdown","5736db48":"markdown","dd662724":"markdown","0376f79c":"markdown"},"source":{"3f9b10d3":"import pandas as pd\nimport numpy as np # linear algebra\nimport re # regular expressions\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix # model evaluation\nimport sklearn.metrics as metrics # model evaluation\nimport seaborn as sns # visualization\nimport matplotlib.pyplot as plt # visualization\nimport scikitplot as skplt # ROC curve","c0090aab":"train = pd.read_csv('..\/input\/titanic\/train.csv',sep=',') # importing train dataset\ntest = pd.read_csv('..\/input\/titanic\/test.csv',sep=',') # importing test dataset","5379c013":"train.head() # seeing the first 6 rows from train dataset","64f6694c":"test.head() # seeing the first 6 rows from test dataset","a65fdb20":"# Seeing shape of each dataset\nprint('train shape:',train.shape)\nprint('test shape:',test.shape)","36f13efa":"survived = train['Survived'] # saving 'Survived' column from train dataset to be used latter\ntrain = train.drop('Survived',axis=1) # dropping 'Survived' column from train dataset to join datasets\n\ntitanic = pd.concat([train,test],axis=0,sort=False) # join train and test into titanic\ntitanic.shape # seeing shape of join result","70776cc8":"titanic.describe() # Seeing a summary of numeric columns","207aa57b":"titanic['Ticket'].describe() # Taking a looking into Ticket column","d7cae5a2":"titanic = titanic.drop('Ticket',axis=1) # dropping Ticket column from titanic dataset","564b6688":"plt.subplots(0,0, figsize = (18,5)) # difyning figure size\nax = (titanic.isnull().sum()\/len(titanic)).sort_values(ascending = False).plot.bar(color = 'blue')\nplt.axhline(y=0.1, color='r', linestyle='-')\nplt.title('Missing values percent per columns', fontsize = 20) # plotting a title\n\n# create a list to collect the plt.patches data\ntotals = []\n\n# find the values and append to list\nfor i in ax.patches:\n    totals.append(i.get_height())\n\n# set individual bar lables using above list\ntotal = sum(totals)\n\n# set individual bar lables using above list\nfor i in ax.patches:\n    # get_x pulls left or right; get_height pushes up or down\n    ax.text(i.get_x()-.03, i.get_height()+.05, str(round((i.get_height()\/total)*100, 2))+'%', fontsize=10,\n                color='black')","ac18b7b5":"# replacing na's of Age using linear regression method\ntitanic['Age'] = titanic['Age'].interpolate(method=\"linear\",\n                                         limit_direction=\"forward\")\n\n# replacing na's of Age using linear regression method\ntitanic['Fare'] = titanic['Fare'].interpolate(method=\"linear\",\n                                         limit_direction=\"forward\")","2bdfab54":"# replacing na's of Embarked from U\ntitanic['Embarked'] = titanic['Embarked'].fillna('U')\ntitanic['Embarked'] = titanic['Embarked'].astype('category') # Converting into categorys","a23a96bf":"titanic.isna().sum() # verifying na's","abd02bc1":"num_vars = titanic[['Pclass','Age','SibSp','Parch','Fare']][1:891] # taking numerical variables to correlation plot\ncor_df = pd.concat([num_vars,survived],axis=1) # join numerical variables and our target\n\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(14,12)) # difyning plot size\nplt.title('Pearson Correlation of Features', y=1.05, size=15) # plotting a title to our plot\nsns.heatmap(cor_df.astype(float).corr(),linewidths=0.5,vmax=1, \n            square=True, cmap=colormap, linecolor='white', annot=True);","606ae96c":"plt.figure(figsize=(10,6)) # difyning plt size\nsns.distplot(titanic['Fare']) # plotting distribution\nplt.title('Fare Distribution'); # difyning a tittle","30392aee":"plt.figure(figsize=(8,8)) # difyning plot size\nsns.boxplot(y='Fare',x='Survived',data=cor_df, # boxplotting\n                 palette=\"colorblind\")\nplt.title('Distribution of Fare into Survived'); # difyning a title","5867864f":"plt.figure(figsize=(10,6)) # difyning plt size\nsns.distplot(titanic['Age']) # plotting distribution\nplt.title('Age Distribution'); # difyning a tittle","a8edfa0a":"plt.figure(figsize=(8,8)) # difyning plot size\nsns.boxplot(y='Age',x='Survived',data=cor_df, # boxplotting\n                 palette=\"colorblind\")\nplt.title('Distribution of Age into Survived'); # difyning a title","505f1989":"plt.figure(figsize=(8,8)) # difyning plot size\nsns.boxplot(y='Parch',x='Survived',data=cor_df, # boxplotting\n                 palette=\"colorblind\")\nplt.title('Distribution of Parch into Survived'); # difyning a title","4bba03b5":"plt.figure(figsize=(8,8)) # difyning plot size\nsns.boxplot(y='Pclass',x='Survived',data=cor_df, # boxplotting\n                 palette=\"colorblind\")\nplt.title('Distribution of Pclass into Survived'); # difyning a title","e8b67396":"plt.figure(figsize=(12,8)) # difyning plot size\nsurvived.value_counts().plot(kind='bar',color=['darkred','blue']) #  counting category values and plotting\nplt.title('Our class categorys frequency'); # difyning a title","dc31c173":"data = [titanic] # passing our dataset as a list\n\nfor dataset in data: # creating a loop to go throught titanic dataset\n    dataset['Age'] = dataset['Age'].astype(int) # converting our Age column to integer numbers\n    \n    # creating the groups\n    dataset.loc[dataset['Age'] <=11,'Age'] =8\n    dataset.loc[(dataset['Age'] >11) & (dataset['Age']<=18),'Age'] = 7\n    dataset.loc[(dataset['Age'] >18) & (dataset['Age']<=22),'Age'] = 6\n    dataset.loc[(dataset['Age'] >22) & (dataset['Age']<=27),'Age'] = 5\n    dataset.loc[(dataset['Age'] >27) & (dataset['Age']<=33),'Age'] = 4\n    dataset.loc[(dataset['Age'] >33) & (dataset['Age']<=40),'Age'] = 3\n    dataset.loc[(dataset['Age'] >40) & (dataset['Age']<=66),'Age'] = 2\n    dataset.loc[dataset['Age']>66,'Age'] = 1","51ec1a77":"plots_df = pd.concat([titanic[0:891],survived],axis=1) # creating a dataset to do boxplots\n\n# Plotting Distribution of Age into Survived\nplt.figure(figsize=(8,8))\nsns.boxplot(y='Age',x='Survived',data=plots_df, \n                 palette=\"colorblind\")\nplt.title('Distribution of Age into Survived');","a121c07e":"data = [titanic] # passing our dataset as a list\n\nfor dataset in data: # creating a loop to go throught titanic dataset\n    dataset['Fare'] = dataset['Fare'].astype(int) # converting our Fare column to integer numbers\n    \n    # Creating the groups\n    dataset.loc[dataset['Fare'] <=50,'Fare'] =10\n    dataset.loc[(dataset['Fare'] >50) & (dataset['Fare']<=100),'Fare'] = 5\n    dataset.loc[(dataset['Fare'] >100) & (dataset['Fare']<=200),'Fare'] = 4\n    dataset.loc[(dataset['Fare'] >200) & (dataset['Fare']<=300),'Fare'] = 3\n    dataset.loc[dataset['Fare']>300,'Fare'] = 0.5","84e7c3a1":"plots_df = pd.concat([titanic[0:891],survived],axis=1) # creating a dataset to do boxplots\n\n# Plotting Distribution of Fare into Survived\nplt.figure(figsize=(8,8))\nsns.boxplot(y='Fare',x='Survived',data=plots_df, \n                 palette=\"colorblind\")\nplt.title('Distribution of Fare into Survived');","cb59123f":"gender = {'male':0,'female':1} # creating a dictionary to storage numeric representation of male and female\ndata = [titanic] # passing as a list\n\nfor dataset in data: # loop \n    dataset['Sex'] =dataset['Sex'].map(gender) # mapping by genger","edd21cf9":"data = [titanic] # passing titanic dataset as a list\n\nfor dataset in data: # loop to go throught the list\n    dataset['Title'] = dataset.Name.str.extract('([A-Za-z]+)\\.',expand = False) # extracting titles from Name column\n    \n    #Replace title with more common one\n    dataset['Title'] = dataset['Title'].replace(['Lady','Countess','Capt','Col','Don','Dr', \n                                                'Major','Rev','Sir','Jonkheer','Dona'],'Rare')\n    dataset['Title'] = dataset['Title'].replace('Ms','Miss')\n    dataset['Title'] = dataset['Title'].replace('Mlle','Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme','Mrs')\n    dataset['Title'] = dataset['Title'].fillna('unknow') # fill na's\n    dataset['Title']  = dataset['Title'].astype('category') # converting into categorys\n\ntitanic = titanic.drop('Name',axis=1) # Dropping 'Name' column","92708af0":"titanic.head() # Verifying result","c54c3942":"data = [titanic] # passing titanic dataset as a list\n\nfor dataset in data: # loop to create the new feature\n    dataset['Cabin'] = dataset['Cabin'].fillna('H0') # replacing missing values by 'H0'\n    dataset['Deck']  = dataset['Cabin'].map(lambda x: re.compile('([a-z,A-Z]+)').search(x).group()) # extracting deck initial letter from Cabin\n    dataset['Deck']  = dataset['Deck'].astype('category') # converting Deck feature into categorys\n\ntitanic = titanic.drop(['Cabin'],axis = 1) # dropping Cabin column\ntitanic.head() # seeing the first 6 rows","16203537":"titanic.isna().sum() # looking for missing values","b5f60f97":"# creating a variables for passengers that have a family aboard\ntitanic['family_aboard'] = np.where((titanic['Parch']>=1) & (titanic['SibSp']>=1),1,0)\n\n# creating a variable to represent couples with no childrens\ntitanic['no_child_couple'] = np.where((titanic['Parch']==0) & (titanic['SibSp']==1),1,0)\n\n# creating a variables to discribe the family size\ntitanic['family_small'] = np.where((titanic['Parch']<=1) & (titanic['SibSp']<=1),1,0)\ntitanic['family_median'] = np.where((titanic['Parch']>1) & (titanic['SibSp']>1) & (titanic['Parch']<=2) & (titanic['SibSp']<=2),1,0)\ntitanic['family_large'] = np.where((titanic['Parch']>2) & (titanic['SibSp']>2),1,0)","866b47b6":"titanic = titanic.drop(['Parch','SibSp'],axis=1) # drooping the 'Parch' and 'SibSp' columns","1d39335f":"# converting our categorical variables into new binary variables\ndf_emb = pd.get_dummies(titanic['Embarked'])\ndf_Tit = pd.get_dummies(titanic['Title'])\ndf_Dec = pd.get_dummies(titanic['Deck'])\n\n# Concat new columns with binary values into titanic dataset\ntitanic = pd.concat([titanic, df_emb, df_Tit, df_Dec], axis=1)\n\n# drooping the old columns with categorys\ntitanic = titanic.drop(['Embarked','Title','Deck'],axis=1)","b95b1dcd":"titanic.head() # seeing the first 6 rows","995ad0bc":"titanic.columns # seeing columns names","11a04317":"cols = [] # creating a empty list\ncount = 1\n\n# replacing duplicate names\nfor column in titanic.columns:\n    if column == 'C':\n        cols.append(f'C_{count}')\n        count+=1\n        continue\n    cols.append(column)\ntitanic.columns = cols","083439c5":"titanic.columns # seeing columns names","c8d8a9a1":"train = titanic[0:891] # taking the first 891 rows to train\ntest = titanic[891:1310]","9b5dbf34":"print('train shape:',train.shape)\nprint('test shape:',test.shape)","f8795f3f":"# Saving PassengerId of test dataset to create submission dataset latter\npassengerId = test['PassengerId']\n\n# Drooping PassengerId column from train and test\ntrain = train.drop('PassengerId',axis=1)\ntest = test.drop('PassengerId',axis=1)","c4abd4d4":"x_train = train # defyning train dataset\ny_train = survived # defyning target to train\nx_test = test # defyning test dataset","3a778a05":"tree = DecisionTreeClassifier() # creating the model}\n\ntree.fit(x_train,y_train) # training the model on train dataset","f9ad5e56":"tree.score(x_train,y_train) # seeing accuracy","584605ee":"pip install pydotplus # installing pydotplus","5618750b":"# decision tree visualization\nfrom IPython.display import Image\nimport pydotplus\nfrom sklearn.externals.six import StringIO   \nfrom sklearn.tree import export_graphviz","c197a233":"dot_data = StringIO()\n\nexport_graphviz(tree, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True)\n\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \nImage(graph.create_png())","63f4f4ca":"y_pred = pd.DataFrame(tree.predict(x_test)) # predicting on test dataset\n\n# creating the submission file\ny_pred['Survived'] = y_pred[0]\ny_pred.drop(0,axis=1,inplace=True)\ny_pred['PassengerId'] = passengerId\ny_pred_Dtrees = y_pred\n\ny_pred_Dtrees.to_csv('Decision_tree_model.csv',index=False) # exporting submission file","6ac8884a":"tree2 = DecisionTreeClassifier(criterion = \"gini\", max_depth = 3) # creating the model}\n\ntree2.fit(x_train,y_train) # training the model on train dataset","484340a4":"tree2.score(x_train,y_train) # seeing accuracy","c730393d":"dot_data = StringIO()\n\nexport_graphviz(tree2, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True)\n\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \nImage(graph.create_png())","a65b1d80":"y_probs = tree2.predict_proba(x_train) # predicting probabilities to plot ROC curve\n\nskplt.metrics.plot_roc(y_train, y_probs, figsize=(15,10)) # creating the plot\nplt.show() # showing the plot","77236b9d":"y_pred = pd.DataFrame(tree2.predict(x_test)) # predicting on test dataset\n\n# creating the submission file\ny_pred['Survived'] = y_pred[0]\ny_pred.drop(0,axis=1,inplace=True)\ny_pred['PassengerId'] = passengerId\ny_pred_Dtrees2 = y_pred\n\ny_pred_Dtrees2.to_csv('Decision_tree_pruned.csv',index=False) # exporting submission file","6f99c321":"Now that we already train our tree, let's see the accuracy in train dataset","5c4045ba":" We have 929 unique values in Ticket, let's just droop out this column","9863bf72":"As we can see only train dataset has the column 'Survived', we have some numerical, categorical and text features and we also have missing values represent by 'NaN',\nlet's take a look into our datasets shape","eab1b732":"Ticket variables seems to have many unique values with numbers and text, let's deal with this latter","25ac2863":"A beathful curve, our almost 90% of AUC in class 0 and class 1, or model are very stable.\nNow we have a tree much more simple and problably not overfited, let's predict on test dataset and submit our prediction to kaggle","6111d83c":"Now let's separate our data for train and test","16b55863":"### Loading Required Librarys and Datasets","b1865151":"#### Creating new features\n\nHere let's do some text mining into text features and during this process let's create another features","1ab1191c":"First let's take a look into 'Age' distribution","933dd5af":"Clearly not a normal distribution, some values are far to the right, seems to be outlayers, let's deal with then latter,\nnow let's see Fare distribution in relation with our target","98c6336d":"First let's create a basic decistion tree model using all information that we have","40608f53":"Perfect! Let's take a look into our dataset, just the first 6 rows","e1f91dd6":"Clearly passengers from all classes can survive but almost passengers that died was from second and third classes, \nlet's take a look into our target categorys distribution","27f8e362":"Now let's take a look into our first 6 rows and see if works","6e961a69":"<img src='https:\/\/i.imgur.com\/LYBcwHa.png' style='width:1000px;height:200px'\/>","71cee259":"80% of accuracy, a great result!, 82,5% in train and 80,38% in test dataset, the model now are capable of generalize","b3d4fffd":"#### Introducing family_aboard, no_child_couple and features related to family size\n\nHere i dicide to create new variables with family information, see for i did each variable below:\n\n**family_aboard:** I considerate passengers with family aboard if they have at least 1 parents\/children and spouses\n\n**no_child_couple:** How it's impossible now if the number in 'Parch' variable it's a children or not, i dicide to create a variable to represent couples with no children\n\n**family size variables:** Here i'm considerating familys with 'Parch' and 'SibSp' less or equal 1 as small_familys, with 'Parch' and 'SibSp' great then 1 and less or equal 2 as median_familys and with 'Parch' and 'SibSp' great then 3 as large familys","e6ccc9ff":"First let's start visualizing missing values percentage proportion in each variable","a04bf9d3":"Now let's take a look again into boxplot relation between Fare and our target Survived","da4b3699":"Now let's loading the required librarys to the decision tree plot","af8dfc9e":"Age distribution seems to be more likely a normal distribution but still not perfect, let's see the relation of Age with our target Survived with boxplots","e98690e3":"Perfect, now let's visualize new relation between Age distributions and our target with boxplots","87c0dffc":"# Titanic: Survivers Prediction, Top 6% with Decision Tree Classifier","ab7a4c70":"When we use get.dummies function maybe some of the new columns are created with the same name, let's verify and solve this","3d6905ee":"### Feature Engineering\n\nHere let's do transform some features for better readability, create new features and do one-hot-encoding on categorical features","7bf08266":"<img src='https:\/\/i.imgur.com\/F03cyKQ.png' style='width:700px;height:400px'\/>","a78dbced":"#### Evaluating our model with ROC curve and AUC","643c48c8":"Now let's drop the ogirinal 'Parch' and 'SibSp' variables","90121e25":"Now let's take a look into a summary of our numerical variables","4c832bf2":"As we can see we have 2 numerical and 2 categorical features with missing values, let's start replacing missing values from\n'Age' and 'Fare' using interpolate function with linear method, let's replace na's of Age and Fare using a linear regression to predict missing age and fare values","3b1f28cb":"### Dealing with missing values\n\nWe have diferent types of variables so here i'll apply diferent types of missing values replacement","a6464f86":"#### One-Hot-Encoding\n\nFor categorical variables where no such ordinal relationship exists, the integer encoding is not enough. In fact, using this encoding and allowing the model to assume a natural ordering between categories may result in poor performance or unexpected results (predictions halfway between categories). In this case, a one-hot encoding can be applied to the integer representation. This is where the integer encoded variable is removed and a new binary variable is added for each unique integer value.\n\nWe have some categorical features(Embarked, Title and Deck), we need to convert this features into binary numerical features, to solve this problem let's use one-hot-encoding using\npd.get_dummies() function from pandas.","86bfb8fc":"### Exploratory Analysis\n\nHere let's take a look into the principal variables with correlation matrix plots and boxplots","ea4c3d4b":"Clearly Passengers with less then 1 parch are mo likely to survive, latter let's create groups with this information,\nnow let's take a look into Pclass variable","6cc2aeea":"Perfect! Now let's save and remove PassengerId column","065a98a7":"Let's see if works well","19d2cb24":"### Preparing data for modeling\n\nHere we need to prepare our dataset to modeling fase, let's split our data and remove some unusefull columns","3b2e93c9":"First of all we nned to install pydotplus","b4397a59":"How was expected the majority of the passengers die, this make more dificult to  predict survivers then passengers that died,\nlet's finally go to feature engineering and deal with some problems in our variables","176427b2":"### Modeling\nWith Exploratory analysis we can discover what type of passenger are more likely to survive, but if we want literaly to predict if a passenger will survive or not based on his features? My goal here will get at least 80% of accuracy, let's try this with decision tree classifier!","b7b56d99":"<img src='https:\/\/i.imgur.com\/1jBYN9w.jpg' style='width:1000px;height:400px'\/>","6cba8037":"First let's split titanic dataset into train and test again","f2689aa5":"Clearly we have many outliers and the Fare below 100 concentrate the most passengers, let's deal with this in feature engineering,\nnow let's take a look into 'Age' variable","2c54b355":"Now let's save the Survived column from train data set and join our train and test dataset into titanic dataset to deal with missing values and do EDA","92d1315e":"Let's prune our tree durating the train fase using \"gini\" creterion and max_depth = 3","ced5052e":"### Summary\n\n#### Loading Required Librarys and Datasets\n\n#### Dealing with missing values\n\n#### Exploratory Analysis\n\n#### Feature Engineering\n   - Feature Transformation\n   - Creating New Features\n   - One-hot-encoding\n\n#### Preparing data for modeling\n\n#### Modeling\n   - Decision Tree Classifier\n   - Decision Tree Prunning","26d9981c":"### The Challenge\n\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question that be capable of predicting with pessanger will survive or not using passenger data (ie name, age, gender, socio-economic class, etc).","f447e855":"WOW! 90% of accuracy, this seems not realistic for me, let's visualize our decision tree to understand better our results","8f5f3d68":"Perfect! Now we have tree much more simple and less susceptible to overfiting, now let's take a look into AUC to understand better our accuracy","a8cbf7d6":"Now we need take a first look into our datasets structure and features, let's see the first 6 rows","0cf253f1":"<img src='https:\/\/i.imgur.com\/V6Pt0cE.png' style='width:700px;height:300px'\/>","94d8030a":"#### Feature transformation","6169629f":"As we can see we have 2 columns with the 'C' name, let's deal with this using a loop for","39fd4d6c":"Now we already have missing values on Cabin variables, let's deal with this variable latter, let's see this missing values","41fc608d":"#### Decision Tree Classifier\n\nDecision tree builds classification or regression models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes. A decision node (e.g., Outlook) has two or more branches (e.g., Sunny, Overcast and Rainy). Leaf node (e.g., Play) represents a classification or decision. The topmost decision node in a tree which corresponds to the best predictor called root node. Decision trees can handle both categorical and numerical data. ","20e38ac6":"First let's deal with name variable, here i extract the name title of each passenger, passengers that have not a title i replace ad 'unknow', latter i'll do one-hot-encoding in this new variables and another categorical variables","55e84906":"82,5% of accuracy seems to be more realistic, let's see our tree to analyse the results of the pruning","a473ab73":"74,6% of accuracy, compared to 90% from train dataset prediction clearly our model was not capable of generalize and are overfited, how can we solve this using the same  model?","57317411":"Our tree are very extensive and probably our model are overfited, let's create a submission with test predictions and upload on kaggle to confirm","55f2b06b":"#### Decision Tree Pruning\nPruning is an approach that reduces the size of decision tree by removing parts of the tree that provide little power to classify\/predict the data. This technique improves the generalization power of the decision tree and reduces the over-fitting.","6f59fbd0":"Perfect, now let's verify missing values presence before continuos our features creation","0bdba828":"First of all let's take a look into our numerical variables correlation","9b1bca58":"With this boxplot we can see that the majority of passengers are concentrated between 20 and 40 years old, we have also have outlayers above 60 years old\nlet's deal with then latter, now let's take a look into Parch relation with our target","ab0f1e11":"As we can see we have not high correlated variables, let's take a look into Age, Fare, Parch and Pclass with boxplots","e642a7c6":"Now finaly we can do the plot without problems","99b3ab15":"Perfect! Now we just need to deal with 'Cabin' variable, here i dicide to extract the deck from cabin information\nusing text mining technics","ce7ccf77":"Now let's load the datasets from competition inputs","36247346":"Let's start loading the required librarys that we will use in this kernel","8f71d3f5":"Perfect! Now we have not outliers anymore and the majority of our passengers and concentrated between 3 and 6 group, now let's deal with Fare variable, here i decide replace some range of Fare values per decreasing values based on boxplot distribution in relation to target, Fare below 50 concentrate the majority of passenger so this range of Fare values\nwas replaced by 10 and another ranges was replaced by equivalent values less then 5, my logic here was represent each range of Fare based on amount of passengers concentrated on it. My objective here was  eliminaty the majority of outliers and create a feature more capable to predict passengers that survived.","1d73052c":"Let's start with our variable **'Age'**, to solve outlayers problems i dicide to divide passengers ages into 8 groups, \nfor this i assumed that much more young was the passenger, more likely to survive they are","ed4b1d9d":"First let's take a look into Fare distribution","0321caf5":"Perfect, now we have a create concentration of Fare values on passengers that survived to titanic desaster, now let's do convert\nour Sex variable into binary numeric","53af365d":"Now let's replace na's from Embarked variables by 'U' to indicate an unknown Port of Embarkation","3f2c7c6f":"**Thanks for time, i really appreciate that, if this kernel was useful please upvote!**","5736db48":"Perfect! Now the duplicated columns names are named as C_1 and C_2 and we can finally go to modeling fase","dd662724":"<img src='https:\/\/i.imgur.com\/ZHmgPhZ.png' style='width:1000px;height:200px'\/>","0376f79c":"Now let's take a look into our 'Ticket' variable"}}