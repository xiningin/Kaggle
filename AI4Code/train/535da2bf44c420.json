{"cell_type":{"6c0bb967":"code","22a45489":"code","13776dca":"code","95390497":"code","068a8e3e":"code","065682b9":"code","7bca1490":"code","e64d54ed":"code","b308f158":"code","c027dbaa":"code","44141995":"code","e9c43d4a":"code","22c90800":"code","c525af46":"code","b88bedf8":"code","0aa34f5a":"code","535683b4":"code","ff0282e2":"code","eed3b3d7":"code","d178e5b4":"code","710d0730":"code","9955ede8":"code","eae121b0":"code","092254e2":"code","7e80ac2e":"code","a46befa4":"code","a1246665":"code","fa5c762d":"code","dbda5c8a":"code","ae67c92f":"code","57e69fe2":"code","1ed940e9":"code","7a0477c3":"code","f4d3dcad":"code","11ab21cb":"code","ff68af4b":"code","6441aa26":"code","3d06a7c3":"code","088b24b3":"code","5131cb09":"code","ea7cdb88":"code","a5447b52":"code","c5bb38a2":"code","a0b08cc0":"code","117f5e4c":"code","6339cd1e":"code","79061c66":"code","26190070":"code","23b8e513":"code","4e8a5783":"code","7d5d293f":"code","74774833":"code","aa79d6ae":"code","9371a684":"code","c9506c86":"code","f6a211f3":"code","35f63009":"code","9c05ce6b":"code","5590fb38":"code","f6c56e06":"code","3d21d0b9":"code","c54e7ab7":"code","40dd5ad1":"code","b0914a78":"code","ebd17b8d":"code","ccb0b8cf":"code","fc9c98a0":"code","a9af3b13":"code","f8877e3f":"code","f1423a63":"code","3068b476":"code","0659e7b7":"code","6fea2049":"code","5e575094":"code","30360c70":"code","60f8c414":"code","e98cc154":"code","fd12c8dc":"code","7e00ecdc":"code","36f2a228":"code","1f658c7f":"code","1b845457":"code","e2a36bef":"code","d60eb28f":"code","2ac676d9":"code","977fb208":"code","1fc60d30":"code","13e5df6f":"code","f96f7d24":"code","b9278514":"code","b556c847":"code","ff048368":"code","a1550e1e":"code","7287d5d4":"code","c80972fd":"code","0788583e":"code","526c0309":"code","b6f5bb42":"code","392d3cbf":"code","bab21bed":"code","442fe744":"code","ff5fa8ba":"code","f4a2e666":"code","d8934e95":"code","74a9742f":"code","30439820":"code","4590af65":"code","16ae60e2":"code","a7117234":"code","8f6d0fe2":"code","f8d09988":"code","a4a6a560":"code","fc9f0079":"code","58948225":"code","3fd1d718":"markdown","a6376c91":"markdown","37f6ddff":"markdown","d5c9488c":"markdown","eeafd698":"markdown","a1599cf2":"markdown","8e6e801b":"markdown","819b7ab8":"markdown","ce3ac965":"markdown","de82dbfb":"markdown","ec48cf97":"markdown","08e1ea3b":"markdown","bacbf08e":"markdown","b593669f":"markdown","92865a64":"markdown","5c1fc38c":"markdown","0603ce48":"markdown","beecd525":"markdown","a2435eb8":"markdown","6d50a06f":"markdown","95df78d7":"markdown","cecae09c":"markdown","8645c056":"markdown","f7ae0d6b":"markdown","e7bb0411":"markdown","2c5292c0":"markdown","9b37654c":"markdown","bda7e386":"markdown","4692bf57":"markdown","e974e718":"markdown","da3b6591":"markdown","52255b27":"markdown","9ddeddf3":"markdown","1c9446cf":"markdown","483e2264":"markdown","cd7aa9ed":"markdown","bb9fcfda":"markdown","f29a7eab":"markdown","96999686":"markdown","bd797391":"markdown","c276aaed":"markdown","f27b92ba":"markdown","6267107d":"markdown","f17ff066":"markdown","3cd1caad":"markdown","7970caa5":"markdown","48b0728c":"markdown","7a2695eb":"markdown","2f42094c":"markdown","c8b2c15b":"markdown","f7bad023":"markdown","132719ba":"markdown","b29de30a":"markdown","791a0343":"markdown","c5a3a497":"markdown","6283bb66":"markdown","07c5bdba":"markdown","727c2310":"markdown","8fa1f6f6":"markdown","521bd030":"markdown","846e54a5":"markdown","f32d3762":"markdown","3b68d948":"markdown","f1cba7b9":"markdown","93daa81b":"markdown","c509da05":"markdown","ccf1e66f":"markdown","7192c613":"markdown","b891a791":"markdown","9b1f9203":"markdown","c016eb50":"markdown","9f47f568":"markdown","147929a4":"markdown","2659f7cb":"markdown","6e79cf02":"markdown","9e9f9e05":"markdown","e78f66db":"markdown","97e50a8b":"markdown","29dae147":"markdown","da14b47c":"markdown","877081ae":"markdown"},"source":{"6c0bb967":"!pip install numpy pandas matplotlib seaborn --quiet\n\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('seaborn-whitegrid')\nfrom sklearn import metrics\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom datetime import datetime\nimport math\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport warnings\nwarnings.filterwarnings('ignore')\n\n!pip install pingouin\nimport pingouin as pg\n\n \n","22a45489":"features = pd.read_csv('\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/features.csv.zip')\nstores = pd.read_csv('\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/stores.csv')\ntrain = pd.read_csv('\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/train.csv.zip')\ntest = pd.read_csv('\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/test.csv.zip')\nsampleSubmission = pd.read_csv('\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/sampleSubmission.csv.zip')","13776dca":"pd.DataFrame(features.dtypes, columns=['Type']).T","95390497":"pd.DataFrame(stores.dtypes, columns=['Type']).T","068a8e3e":"pd.DataFrame(train.dtypes, columns=['Type']).T","065682b9":"pd.DataFrame(test.dtypes, columns=['Type']).T","7bca1490":"print(features.shape)\nprint(stores.shape)\nprint(train.shape)\nprint(test.shape)\nprint(\"El ratio de train data : test data es \", \n      (round(train.shape[0]*100\/(train.shape[0]+test.shape[0])),100-round(train.shape[0]*100\/(train.shape[0]+test.shape[0]))))","e64d54ed":"feature_store = features.merge(stores, how='inner', on = \"Store\")","b308f158":"print(\"El numero de filas y columnas es de: \",(feature_store.shape))\nprint(\"Existen\",(len(feature_store.Store.unique())), 'tiendas unicas')","c027dbaa":"\ntrain_df = train.merge(feature_store, how='inner', \n                       on = ['Store','Date','IsHoliday']).sort_values(by=['Store','Dept','Date']).reset_index(drop=True)","44141995":"print(\"El numero de filas y columnas es de: \",(train_df.shape))","e9c43d4a":"test_df = test.merge(feature_store, how='inner', \n                     on = ['Store','Date','IsHoliday']).sort_values(by = ['Store','Dept','Date']).reset_index(drop=True)","22c90800":"print(\"El numero de filas y columnas es de: \",(test_df.shape))","c525af46":"train_df['Date'] = pd.to_datetime(train_df['Date'])\ntest_df['Date'] = pd.to_datetime(test_df['Date'])\n\nprint(train_df[\"Date\"].dtypes)\nprint(test_df[\"Date\"].dtypes)","b88bedf8":"train_df['Day'] = train_df['Date'].dt.day\ntrain_df['Week'] = train_df['Date'].dt.week\ntrain_df['Month'] = train_df['Date'].dt.month\ntrain_df['Year'] = train_df['Date'].dt.year\n\n\ntest_df['Day'] = test_df['Date'].dt.day\ntest_df['Week'] = test_df['Date'].dt.week\ntest_df['Month'] = test_df['Date'].dt.month\ntest_df['Year'] = test_df['Date'].dt.year\n\nprint(\"El numero de filas y columnas es de: \",(train_df.shape))\nprint(\"El numero de filas y columnas es de: \",(test_df.shape))\n","0aa34f5a":"train_df.head(1)","535683b4":"test_df.head(1)","ff0282e2":"ax = sns.countplot(stores.Type ,facecolor=(0,0,0,0),linewidth=10,\n                   edgecolor=sns.color_palette(\"spring\", 3))\nfor p in ax.patches:\n    ax.annotate(f'Number of\\n stores:\\n {p.get_height()}', (p.get_x() + p.get_width() \/ 2., p.get_height()-4),\n               ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points',fontsize=12);","eed3b3d7":"store_type = pd.concat([stores['Type'], stores['Size']], axis=1)\nplt.figure(figsize=(15, 6))\nsns.boxplot(x='Type', y='Size', data=store_type);","d178e5b4":"plt.figure(figsize=(15,5))\nsns.barplot(x='Store',y='Size',data=stores,order=stores.sort_values('Size')['Store'].tolist())\nplt.title('Superficie de todas las tiendas.',fontsize=15)\nplt.tight_layout();\n","710d0730":"print('Type vs Weekly_Sales')\nplt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.stripplot(y=train_df['Weekly_Sales'],x=train_df['Type'])\nplt.subplot(1,2,2)\nsns.boxenplot(y=train_df['Weekly_Sales'],x=train_df['Type']);","9955ede8":"plt.figure(figsize=(15,3))\ntrain_df[train_df['Type']=='A'].groupby('Month').mean()['Weekly_Sales'].plot()\ntrain_df[train_df['Type']=='B'].groupby('Month').mean()['Weekly_Sales'].plot()\ntrain_df[train_df['Type']=='C'].groupby('Month').mean()['Weekly_Sales'].plot()\nplt.title('Promedio de ventas mensuales de por formato de tienda', fontsize=18)\nplt.legend(['Type A', 'Type B', 'Type C'], loc='best', fontsize=16)\nplt.ylabel('Ventas', fontsize=16)\nplt.xlabel('Meses', fontsize=16);","eae121b0":"print(\"Correlacion del promedio de ventas mensuales de los formatos A y C\")\ndisplay(pg.corr(train_df['Type']=='A', train_df['Type']=='C', method='pearson'))\nprint(\"--------------------------------------------------------\")\nprint(\"Correlacion del promedio de ventas mensuales de los formatos B y C\")\ndisplay(pg.corr(train_df['Type']=='B', train_df['Type']=='C', method='pearson'))","092254e2":"for df in [train_df,test_df]:\n    df['Week'] = df['Date'].dt.week\n    df['Month'] = df['Date'].dt.month\n    df['Year'] = df['Date'].dt.year\n\nplt.figure(figsize=(15,3))\ntrain_df[train_df['Year']==2010].groupby('Month').mean()['Weekly_Sales'].plot()\ntrain_df[train_df['Year']==2011].groupby('Month').mean()['Weekly_Sales'].plot()\ntrain_df[train_df['Year']==2012].groupby('Month').mean()['Weekly_Sales'].plot()\nplt.title('Promedio de ventas mensuales de Walmart para cada a\u00f1o', fontsize=18)\nplt.legend(['2010', '2011', '2012'], loc='best', fontsize=16)\nplt.ylabel('Ventas', fontsize=16)\nplt.xlabel('Meses', fontsize=16);\n","7e80ac2e":"for df in [train_df,test_df]:\n    df['Week'] = df['Date'].dt.week\n    df['Month'] = df['Date'].dt.month\n    df['Year'] = df['Date'].dt.year\n\nplt.figure(figsize=(15,3))\ntrain_df[train_df['Year']==2010].groupby('Day').mean()['Weekly_Sales'].plot()\ntrain_df[train_df['Year']==2011].groupby('Day').mean()['Weekly_Sales'].plot()\ntrain_df[train_df['Year']==2012].groupby('Day').mean()['Weekly_Sales'].plot()\nplt.title('Promedio de ventas para cada dia del mes', fontsize=18)\nplt.legend(['2010', '2011', '2012'], loc='best', fontsize=16)\nplt.ylabel('Ventas', fontsize=16)\nplt.xlabel('Dias', fontsize=16);","a46befa4":"for df in [train_df,test_df]:\n    df['Week'] = df['Date'].dt.week\n    df['Month'] = df['Date'].dt.month\n    df['Year'] = df['Date'].dt.year\n\nplt.figure(figsize=(15,8))\ntrain_df[train_df['Month']==1].groupby('Day').mean()['Weekly_Sales'].plot()\ntrain_df[train_df['Month']==2].groupby('Day').mean()['Weekly_Sales'].plot()\ntrain_df[train_df['Month']==3].groupby('Day').mean()['Weekly_Sales'].plot()\ntrain_df[train_df['Month']==4].groupby('Day').mean()['Weekly_Sales'].plot()\ntrain_df[train_df['Month']==5].groupby('Day').mean()['Weekly_Sales'].plot()\ntrain_df[train_df['Month']==6].groupby('Day').mean()['Weekly_Sales'].plot()\ntrain_df[train_df['Month']==7].groupby('Day').mean()['Weekly_Sales'].plot()\ntrain_df[train_df['Month']==8].groupby('Day').mean()['Weekly_Sales'].plot()\ntrain_df[train_df['Month']==9].groupby('Day').mean()['Weekly_Sales'].plot()\ntrain_df[train_df['Month']==10].groupby('Day').mean()['Weekly_Sales'].plot()\ntrain_df[train_df['Month']==11].groupby('Day').mean()['Weekly_Sales'].plot()\ntrain_df[train_df['Month']==12].groupby('Day').mean()['Weekly_Sales'].plot()\nplt.title('Promedio de ventas de walmart para cada dia del mes', fontsize=18)\nplt.legend(['Ene', 'Feb', 'Mar','Abr','May','Jun','Jul','Ago','Sep','Oct','Nov','Dic'], loc='best', fontsize=16)\nplt.ylabel('Ventas', fontsize=16)\nplt.xlabel('Dias', fontsize=16);","a1246665":"plt.figure(figsize=(15,3))\ntrain_df.groupby('Date')['Weekly_Sales'].mean().plot()\nplt.title('Promedio de las ventas semanales de Walmart a traves de una linea de tiempo', fontsize=18)\nplt.ylabel('Sales', fontsize=16)\nplt.xlabel('Date', fontsize=16);","fa5c762d":"data = pd.concat([train_df['Week'], train_df['Weekly_Sales']], axis=1)\nf, ax = plt.subplots(figsize=(20, 6))\nfig = sns.boxplot(x='Week', y=\"Weekly_Sales\", data=data, showfliers=False)","dbda5c8a":"plt.figure (figsize = (20,7))\nsns.set_style('darkgrid')\nsns.barplot ( data = train_df, x = 'Dept', y =  'Weekly_Sales');","ae67c92f":"df_weeks = train_df.groupby('Week').sum()","57e69fe2":"weekly_sales = train_df.groupby(['Year','Week'], as_index = False).agg({'Weekly_Sales': ['mean', 'median']})\nweekly_sales2010 = train_df.loc[train_df['Year']==2010].groupby(['Week']).agg({'Weekly_Sales': ['mean', 'median']})\nweekly_sales2011 = train_df.loc[train_df['Year']==2011].groupby(['Week']).agg({'Weekly_Sales': ['mean', 'median']})\nweekly_sales2012 = train_df.loc[train_df['Year']==2012].groupby(['Week']).agg({'Weekly_Sales': ['mean', 'median']})","1ed940e9":"fig7 = go.Figure()\nfig7.add_trace(go.Scatter( x = weekly_sales2010['Weekly_Sales']['mean'].index, y = weekly_sales2010['Weekly_Sales']['mean'], name = 'Media Ventas 2010', mode = 'lines') )\nfig7.add_trace(go.Scatter( x = weekly_sales2011['Weekly_Sales']['mean'].index, y = weekly_sales2011['Weekly_Sales']['mean'], name = 'Media Ventas 2011', mode = 'lines') )\nfig7.add_trace(go.Scatter( x = weekly_sales2012['Weekly_Sales']['mean'].index, y = weekly_sales2012['Weekly_Sales']['mean'], name = 'Media Ventas 2012', mode = 'lines') )\nfig7.update_layout(title = 'Ventas 2010, 2011, 2012', xaxis_title = 'Semanas')\n","7a0477c3":"fig1 = go.Figure()\nfig1.add_trace(go.Scatter( x = df_weeks.index, y = df_weeks['MarkDown1'], name = 'MarkDown1', mode = 'lines') )\nfig1.add_trace(go.Scatter( x = df_weeks.index, y = df_weeks['Weekly_Sales'], name = 'Ventas semanales', mode = 'lines') )\nfig1.update_layout(title = 'Ventas vs Markdown1', xaxis_title = 'Semanas')","f4d3dcad":"df_weeks_Markdown1 = df_weeks.iloc[[0,1,2,3,4]]\nprint(\"Correlacion entre Markdow1 y las cinco primeras semanas del anno\")\ndisplay(pg.corr(df_weeks_Markdown1['Weekly_Sales'], df_weeks_Markdown1['MarkDown1'], method='pearson'))","11ab21cb":"fig2 = go.Figure()\nfig2.add_trace(go.Scatter( x = df_weeks.index, y = df_weeks['MarkDown2'], name = 'MarkDownw2', mode = 'lines') )\nfig2.add_trace(go.Scatter( x = df_weeks.index, y = df_weeks['Weekly_Sales'], name = 'Ventas semanales', mode = 'lines') )\nfig2.update_layout(title = 'Ventas vs Markdown2', xaxis_title = 'Semanas')","ff68af4b":"df_weeks_Markdown2 = df_weeks.iloc[[0,1,2,3,4,5,6,7,8,13,44,51]]\nprint(\"Correlacion entre las semanas de promocion Markdown2 y las ventas semanales\")\ndisplay(pg.corr(df_weeks_Markdown2['Weekly_Sales'], df_weeks_Markdown2['MarkDown2'], method='pearson'))","6441aa26":"fig3 = go.Figure()\nfig3.add_trace(go.Scatter( x = df_weeks.index, y = df_weeks['MarkDown3'], name = 'MarkDown3', mode = 'lines') )\nfig3.add_trace(go.Scatter( x = df_weeks.index, y = df_weeks['Weekly_Sales'], name = 'Ventas semanales', mode = 'lines') )\nfig3.update_layout(title = 'Ventas vs Markdown3', xaxis_title = 'Semanas')","3d06a7c3":"df_weeks_Markdown3 = df_weeks.iloc[[45,46,47]]\nprint(\"Correlacion entre las semanas de promocion Markdown3 y las ventas semanales \")\ndisplay(pg.corr(df_weeks_Markdown3['Weekly_Sales'], df_weeks_Markdown3['MarkDown3'], method='pearson'))","088b24b3":"fig4 = go.Figure()\nfig4.add_trace(go.Scatter( x = df_weeks.index, y = df_weeks['MarkDown4'], name = 'MarkDown4', mode = 'lines') )\nfig4.add_trace(go.Scatter( x = df_weeks.index, y = df_weeks['Weekly_Sales'], name = 'Ventas semanales', mode = 'lines') )\nfig4.update_layout(title = 'Ventas vs Markdown4', xaxis_title = 'Semanas')","5131cb09":"df_weeks_Markdown4 = df_weeks.iloc[[0,1,2,3,4,8,30]]\nprint(\"Correlacion entre las semanas de promocion Markdown4 y las ventas semanales \")\ndisplay(pg.corr(df_weeks_Markdown4['Weekly_Sales'], df_weeks_Markdown4['MarkDown4'], method='pearson'))","ea7cdb88":"fig5 = go.Figure()\nfig5.add_trace(go.Scatter( x = df_weeks.index, y = df_weeks['MarkDown5'], name = 'MarkDown5', mode = 'lines') )\nfig5.add_trace(go.Scatter( x = df_weeks.index, y = df_weeks['Weekly_Sales'], name = 'Ventas semanales', mode = 'lines') )\nfig5.update_layout(title = 'Ventas vs Markdown5', xaxis_title = 'Semanas')","a5447b52":"df_weeks_Markdown5 = df_weeks.iloc[[45,46,47]]\nprint(\"Correlacion entre las semanas de promocion 46,47 y 48 Markdown5 y las ventas semanales \")\ndisplay(pg.corr(df_weeks_Markdown5['Weekly_Sales'], df_weeks_Markdown5['MarkDown5'], method='pearson'))","c5bb38a2":"fig6 = go.Figure()\nfig6.add_trace(go.Scatter( x = df_weeks.index, y = df_weeks['MarkDown1'], name = 'MarkDown1', mode = 'lines') )\nfig6.add_trace(go.Scatter( x = df_weeks.index, y = df_weeks['MarkDown2'], name = 'MarkDown2', mode = 'lines') )\nfig6.add_trace(go.Scatter( x = df_weeks.index, y = df_weeks['MarkDown3'], name = 'MarkDown3', mode = 'lines') )\nfig6.add_trace(go.Scatter( x = df_weeks.index, y = df_weeks['MarkDown4'], name = 'MarkDown4', mode = 'lines') )\nfig6.add_trace(go.Scatter( x = df_weeks.index, y = df_weeks['MarkDown5'], name = 'MarkDown5', mode = 'lines') )\nfig6.add_trace(go.Scatter( x = df_weeks.index, y = df_weeks['Weekly_Sales'], name = 'Weekly Sales', mode = 'lines+markers') )\nfig6.update_layout(title = 'Ventas vs Total Markdown', xaxis_title = 'Semanas')\n","a0b08cc0":"print('IsHoliday vs Weekly_Sales')\nplt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.stripplot(y=train_df['Weekly_Sales'],x=train_df['IsHoliday'])\nplt.subplot(1,2,2)\nsns.violinplot(y=train_df['Weekly_Sales'],x=train_df['IsHoliday']);","117f5e4c":"# Convertir la temperatura a grados centigrados para una mejor interpretacion.\n#train_df['Temperature'] = train_df['Temperature'].apply(lambda x :  (x - 32) \/ 1.8)\n#train_df['Temperature'] = train_df['Temperature'].apply(lambda x :  (x - 32) \/ 1.8 )\ntrain_df['Temperature'] = (train_df['Temperature']-32.)\/1.8\ntest_df['Temperature'] = (test_df['Temperature']-32.)\/1.8","6339cd1e":"plt.figure (figsize = (15,7))\nsns.set_style('darkgrid')\nsns.scatterplot ( data = train_df, x = 'Temperature', y =  'Weekly_Sales', hue = 'IsHoliday');\n","79061c66":"plt.figure(figsize=(15,7))\nsns.scatterplot(x=train_df.Temperature, y=train_df.Weekly_Sales, hue=train_df.Type, s=80);\n\n#plt.xticks( fontsize=16)\n#plt.yticks( fontsize=16)\nsns.set_style('darkgrid')\nplt.xlabel('Temperature', fontsize=18)\nplt.ylabel('Sales', fontsize=18);","26190070":"plt.figure (figsize = (15,7))\nsns.set_style('darkgrid')\nsns.set_palette('Paired')\nsns.scatterplot ( data = train_df, x = 'Fuel_Price', y =  'Weekly_Sales', hue = 'IsHoliday');","23b8e513":"plt.figure (figsize = (15,7))\nsns.set_style('darkgrid')\nsns.set_palette('Paired')\nsns.scatterplot ( data = train_df, x = 'Fuel_Price', y =  'Weekly_Sales', hue = 'IsHoliday');","4e8a5783":"plt.figure (figsize = (15,7))\nsns.set_style('darkgrid')\nsns.scatterplot ( data = train_df, x = 'CPI', y =  'Weekly_Sales', hue = 'IsHoliday');","7d5d293f":"plt.figure (figsize = (15,7))\nsns.set_style('darkgrid')\nsns.scatterplot ( data = train_df, x = 'CPI', y =  'Weekly_Sales', hue = 'Type' );","74774833":"plt.figure (figsize = (15,7))\nsns.set_style('darkgrid')\nsns.scatterplot ( data = train_df, x = 'Unemployment', y =  'Weekly_Sales', hue = 'IsHoliday' );\n","aa79d6ae":"plt.figure (figsize = (15,7))\nsns.set_style('darkgrid')\nsns.scatterplot ( data = train_df, x = 'Unemployment', y =  'Weekly_Sales', hue = 'Type' );","9371a684":"sns.set(style=\"white\")\n\ncorr = train_df.corr()\n\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\nf, ax = plt.subplots(figsize=(20, 15))\n\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\nplt.title('Correlation Matrix', fontsize=18)\n\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, vmin=-1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True, fmt='.2f')\n\nplt.show()","c9506c86":"weekly_sales_corr = train_df.corr().iloc[2,:]","f6a211f3":"corr_df = pd.DataFrame(data = weekly_sales_corr, index = weekly_sales_corr.index ).sort_values (by = 'Weekly_Sales', ascending = False)","35f63009":"plt.figure(figsize = (20,5))\nsns.set_palette('mako')\nplt.xticks(rotation = 45)\nplt.title('Feature correlation')\nsns.barplot (data = corr_df, x = corr_df.index, y = 'Weekly_Sales');","9c05ce6b":"data_train = train_df.copy()\ndata_test = test_df.copy()","5590fb38":"data_train['SuperBowlWeek'] = train_df['Week'].apply(lambda x: 1 if x == 6 else 0)\ndata_train['LaborDay'] = train_df['Week'].apply(lambda x: 1 if x == 36 else 0)\ndata_train['Thanksgiving'] = train_df['Week'].apply(lambda x: 1 if x == 47 else 0)\ndata_train['Christmas'] = train_df['Week'].apply(lambda x: 1 if x == 52 else 0)\n","f6c56e06":"data_test['SuperBowlWeek'] = test_df['Week'].apply(lambda x: 1 if x == 6 else 0)\ndata_test['LaborDay'] = test_df['Week'].apply(lambda x: 1 if x == 36 else 0)\ndata_test['Thanksgiving'] = test_df['Week'].apply(lambda x: 1 if x == 47 else 0)\ndata_test['Christmas'] = test_df['Week'].apply(lambda x: 1 if x == 52 else 0)","3d21d0b9":"data_train.insert(24,'Quarter',data_train['Week'])\ndata_test.insert(23,'Quarter',data_test['Week'])","c54e7ab7":"data_train['Quarter'] = data_train['Quarter'].replace([1,2,3,4,5,6,7,8,9,10,11,12,13], 1)\ndata_train['Quarter'] = data_train['Quarter'].replace([14,15,16,17,18,19,20,21,22,23,24,25,26], 2)\ndata_train['Quarter'] = data_train['Quarter'].replace([27,28,29,30,31,32,33,34,35,36,37,38,39], 3)\ndata_train['Quarter'] = data_train['Quarter'].replace([40,41,42,43,44,45,46,47,48,49,50,51,52], 4)\n\ndata_test['Quarter'] = data_train['Quarter'].replace([1,2,3,4,5,6,7,8,9,10,11,12,13], 1)\ndata_test['Quarter'] = data_train['Quarter'].replace([14,15,16,17,18,19,20,21,22,23,24,25,26], 2)\ndata_test['Quarter'] = data_train['Quarter'].replace([27,28,29,30,31,32,33,34,35,36,37,38,39], 3)\ndata_test['Quarter'] = data_train['Quarter'].replace([40,41,42,43,44,45,46,47,48,49,50,51,52], 4)\n","40dd5ad1":"data_train['MarkdownsSum'] = train_df['MarkDown1'] + train_df['MarkDown2'] + train_df['MarkDown3'] + train_df['MarkDown4'] + train_df['MarkDown5']\ndata_test['MarkdownsSum'] = test_df['MarkDown1'] + test_df['MarkDown2'] + test_df['MarkDown3'] + test_df['MarkDown4'] + test_df['MarkDown5']","b0914a78":"#Package called missingno (https:\/\/github.com\/ResidentMario\/missingno) !pip install quilt\nimport missingno as msno\nmsno.matrix(data_train)","ebd17b8d":"msno.matrix(data_test)\n","ccb0b8cf":"# Sumatorio de valores missing para el dataset 'train'.\nprint('Datos missing en data_train')\nprint('---------------------')\nprint(data_train.isna().sum())\nprint('')\nprint('Datos missing en data_test')\nprint('---------------------')\nprint(data_test.isna().sum())","fc9c98a0":"data_train.fillna(0, inplace = True)","a9af3b13":"data_test['CPI'].fillna(data_test['CPI'].mean(), inplace = True)\ndata_test['Unemployment'].fillna(data_test['Unemployment'].mean(), inplace = True)","f8877e3f":"# Los valores missing en 'test' que corresponden solamente a las rebajas los rellenamos con \"0\"\ndata_test.fillna(0, inplace = True)","f1423a63":"data_train['IsHoliday'] = data_train['IsHoliday'].apply(lambda x: 1 if x == True else 0)\ndata_test['IsHoliday'] = data_test['IsHoliday'].apply(lambda x: 1 if x == True else 0)","3068b476":"data_train['Type'] = data_train['Type'].apply(lambda x: 1 if x == 'A' else (2 if x == 'B' else 3))\ndata_test['Type'] = data_test['Type'].apply(lambda x: 1 if x == 'A' else (2 if x == 'B' else 3))","0659e7b7":"data_train.corr()['Weekly_Sales'][:5].sort_values(ascending = False)","6fea2049":"X_f = data_train.drop(['Date','Weekly_Sales'], axis = 'columns' )\ny_f = data_train['Weekly_Sales']","5e575094":"from sklearn.ensemble import RandomForestRegressor\nrf_features = RandomForestRegressor() ","30360c70":"%%time\nrf_features.fit(X_f, y_f)","60f8c414":"importance_df = pd.DataFrame({\n    'feature': X_f.columns,\n    'importance': rf_features.feature_importances_\n}).sort_values('importance', ascending=False)","e98cc154":"plt.figure(figsize=(10,6))\nplt.title('Feature Importance')\nsns.barplot(data=importance_df.head(26), x='importance', y='feature');","fd12c8dc":"from sklearn.linear_model import LinearRegression\n\ndt_vif = data_train.copy(deep = True)\nfeatures = list(dt_vif.columns)\nfeatures.remove('Weekly_Sales') # Borrado de la variable objetivo\nfeatures.remove('Date') # Una variable en formato Date no puede ser procesada por la funcion.\ndt_vif = dt_vif[features]\n\nfor i in range(len(features)):\n    var = features[i]\n    fet = features[:]\n    fet.remove(var)\n    \n    x = dt_vif[fet]\n    y = data_train[var]\n    \n    model = LinearRegression()\n    model.fit(x, y)\n    \n    vif = 1 \/ (1 - model.score(x, y))\n    \n    print ('El valor del VIF para la variable', var, 'es:', vif)","7e00ecdc":"dt_vif = data_train.copy(deep = True)\nfeatures = list(dt_vif.columns)\nfeatures.remove('Weekly_Sales')\nfeatures.remove('Date')\nfeatures.remove('MarkdownsSum') # Eliminamos esta variable por ser la que posee mas VIF del conjunto Markdown\nfeatures.remove('Week') # Eliminamos la variable Week por tener un VIF de 40,090\ndt_vif = dt_vif[features]\n\nfor i in range(len(features)):\n    var = features[i]\n    fet = features[:]\n    fet.remove(var)\n    \n    x = dt_vif[fet]\n    y = data_train[var]\n    \n    model = LinearRegression()\n    model.fit(x, y)\n    \n    vif = 1 \/ (1 - model.score(x, y))\n    \n    print ('El valor del VIF para la variable', var, 'es:', vif)","36f2a228":"dt_vif = data_train.copy(deep = True)\nfeatures = list(dt_vif.columns)\nfeatures.remove('Weekly_Sales')\nfeatures.remove('Date')\nfeatures.remove('MarkdownsSum') \nfeatures.remove('Week') \nfeatures.remove('Quarter') # Esta variable es la siguientye candidata por tener un VIF de 15\n\n\ndt_vif = dt_vif[features]\n\nfor i in range(len(features)):\n    var = features[i]\n    fet = features[:]\n    fet.remove(var)\n    \n    x = dt_vif[fet]\n    y = data_train[var]\n    \n    model = LinearRegression()\n    model.fit(x, y)\n    \n    vif = 1 \/ (1 - model.score(x, y))\n    \n    print ('El valor del VIF para la variable', var, 'es:', vif)\n","1f658c7f":"dt_vif = data_train.copy(deep = True)\nfeatures = list(dt_vif.columns)\nfeatures.remove('Weekly_Sales')\nfeatures.remove('Date')\nfeatures.remove('MarkdownsSum') \nfeatures.remove('Week') \nfeatures.remove('Quarter') \nfeatures.remove('IsHoliday') # Eliminamos IsHoliday por tener valor Inf.\n\n\ndt_vif = dt_vif[features]\n\nfor i in range(len(features)):\n    var = features[i]\n    fet = features[:]\n    fet.remove(var)\n    \n    x = dt_vif[fet]\n    y = data_train[var]\n    \n    model = LinearRegression()\n    model.fit(x, y)\n    \n    vif = 1 \/ (1 - model.score(x, y))\n    \n    print ('El valor del VIF para la variable', var, 'es:', vif)\n\n    ","1b845457":"data_train.drop(['MarkdownsSum', 'Week','Quarter','IsHoliday','Date'], axis = 'columns', inplace=True)\ndata_test.drop(['MarkdownsSum', 'Week','Quarter','IsHoliday','Date'], axis = 'columns', inplace=True)","e2a36bef":"print(data_train.columns)\nprint('---------------------------------------------------------------------------')\nprint(data_test.columns)\n","d60eb28f":"X = data_train[['Store', 'Dept','Temperature', 'Fuel_Price',\n       'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI',\n       'Unemployment', 'Type', 'Size', 'Day', 'Month', 'Year', 'SuperBowlWeek',\n       'LaborDay', 'Thanksgiving', 'Christmas']]\n\ny = data_train['Weekly_Sales']","2ac676d9":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","977fb208":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\n\n# Creamos modelo de Regresion Lineal\nlrModel = lr.fit(X_train, y_train) \n\n# Prediccion para Train\npreds_train_LR = lrModel.predict(X_train) \n# Prediccion para Test\npreds_test_LR = lrModel.predict(X_test) \n\nprint('R2 medio para Train es : ',metrics.mean_squared_error(y_train, preds_train_LR, squared = False)) # Obtenemos el R2 medio para Train\nprint('R2 medio para Test es : ',metrics.mean_squared_error(y_test, preds_test_LR , squared = False)) #Obtenemos el R2 medio para Test\n\n","1fc60d30":"test = data_test[['Store', 'Dept', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2',\n       'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'Type',\n       'Size', 'Day', 'Month', 'Year', 'SuperBowlWeek', 'LaborDay',\n       'Thanksgiving', 'Christmas']]\n\npredict_LR = lrModel.predict(test)","13e5df6f":"sampleSubmission['Weekly_Sales'] = predict_LR\nsampleSubmission.to_csv('submission_LR.csv',index=False)","f96f7d24":"sampleSubmission.head()\n","b9278514":"from sklearn.ensemble import GradientBoostingRegressor\ngb = GradientBoostingRegressor(random_state=42)\n\n# Creamos modelo Gradient Boosting Regressor\ngbModel = gb.fit(X_train, y_train)\n\n# Prediccion para Train\npreds_train_GBR = gbModel.predict(X_train)\n# Prediccion para Test\npreds_test_GBR = gbModel.predict(X_test)\n\n# Obtenemos el R2 medio para Train\nprint('R2 medio para Train es : ',metrics.mean_squared_error(y_train, preds_train_GBR, squared = False))\n# Obtenemos el R2 medio para Test\nprint('R2 medio para Test es : ',metrics.mean_squared_error(y_test, preds_test_GBR, squared = False))\n","b556c847":"test = data_test[['Store', 'Dept', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2',\n       'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'Type',\n       'Size', 'Day', 'Month', 'Year', 'SuperBowlWeek', 'LaborDay',\n       'Thanksgiving', 'Christmas']]\npredict_GBR = gb.predict(test)","ff048368":"sampleSubmission['Weekly_Sales'] = predict_GBR\nsampleSubmission.to_csv('submission_GBR.csv',index=False)","a1550e1e":"sampleSubmission.head()","7287d5d4":"from xgboost import XGBRegressor\n\ngbm = XGBRegressor(random_state=42, n_jobs=-1)\n\n# Creamos modelo Gradient Boosting Regressor\ngbmModel = gbm.fit(X_train, y_train)\n\n# Prediccion para Train\npreds_train_XGB = gbmModel.predict(X_train)\n# Prediccion para Test\npreds_test_XGB = gbmModel.predict(X_test)\n\n# Obtenemos el R2 medio para Train\nprint('R2 medio para Train es : ',metrics.mean_squared_error(y_train, preds_train_XGB, squared = False))\n# Obtenemos el R2 medio para Test\nprint('R2 medio para Test es : ',metrics.mean_squared_error(y_test, preds_test_XGB, squared = False))\n\n","c80972fd":"test = data_test[['Store', 'Dept', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2',\n       'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'Type',\n       'Size', 'Day', 'Month', 'Year', 'SuperBowlWeek', 'LaborDay',\n       'Thanksgiving', 'Christmas']]\npredict_XGB = gbm.predict(test)","0788583e":"sampleSubmission['Weekly_Sales'] = predict_XGB\nsampleSubmission.to_csv('submission_XGB.csv',index=False)","526c0309":"sampleSubmission.head()","b6f5bb42":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(random_state=42)\n\n#Creamos modelo  Random Forest Regressor\nrfModel = rf.fit(X_train, y_train)\n\n# Prediccion para Train\npreds_train_RFR = rfModel.predict(X_train)\n# Prediccion para Test\npreds_test_RFR = rfModel.predict(X_test)\n\n# Obtenemos el R2 medio para Train\nprint('R2 medio para Train es : ', metrics.mean_squared_error(y_train, preds_train_RFR, squared = False))\n# Obtenemos el R2 medio para Test\nprint('R2 medio para Test es : ', metrics.mean_squared_error(y_test, preds_test_RFR, squared = False))\n\n\n","392d3cbf":"test = data_test[['Store', 'Dept', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2',\n       'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'Type',\n       'Size', 'Day', 'Month', 'Year', 'SuperBowlWeek', 'LaborDay',\n       'Thanksgiving', 'Christmas']]\npredict_RFR = rf.predict(test)","bab21bed":"sampleSubmission['Weekly_Sales'] = predict_RFR\nsampleSubmission.to_csv('submission_RFR.csv',index=False)","442fe744":"sampleSubmission.head()","ff5fa8ba":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(n_estimators=100, max_depth=None,random_state=42,bootstrap=False)\n\n#Creamos modelo  Random Forest Regressor\nrfModel = rf.fit(X_train, y_train)\n\n# Prediccion para Train\npreds_train_RFR = rfModel.predict(X_train)\n# Prediccion para Test\npreds_test_RFR = rfModel.predict(X_test)\n\n# Obtenemos el R2 medio para Train\nprint('R2 medio para Train es : ', metrics.mean_squared_error(y_train, preds_train_RFR, squared = False))\n# Obtenemos el R2 medio para Test\nprint('R2 medio para Test es : ', metrics.mean_squared_error(y_test, preds_test_RFR, squared = False))","f4a2e666":"%%time\n\nfrom sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(max_depth=30, random_state=42)\n\n#Creamos modelo  Random Forest Regressor\nrfModel = rf.fit(X_train, y_train)\n\n# Prediccion para Train\npreds_train_RFR = rfModel.predict(X_train)\n# Prediccion para Test\npreds_test_RFR = rfModel.predict(X_test)\n\n# Obtenemos el R2 medio para Train\nprint('R2 medio para Train es : ', metrics.mean_squared_error(y_train, preds_train_RFR, squared = False))\n# Obtenemos el R2 medio para Test\nprint('R2 medio para Test es : ', metrics.mean_squared_error(y_test, preds_test_RFR, squared = False))","d8934e95":"%%time\n\nfrom sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(max_depth=30, random_state=42,max_features=15)\n\n#Creamos modelo  Random Forest Regressor\nrfModel = rf.fit(X_train, y_train)\n\n# Prediccion para Train\npreds_train_RFR = rfModel.predict(X_train)\n# Prediccion para Test\npreds_test_RFR = rfModel.predict(X_test)\n\n# Obtenemos el R2 medio para Train\nprint('R2 medio para Train es : ', metrics.mean_squared_error(y_train, preds_train_RFR, squared = False))\n# Obtenemos el R2 medio para Test\nprint('R2 medio para Test es : ', metrics.mean_squared_error(y_test, preds_test_RFR, squared = False))","74a9742f":"%%time\n\nfrom sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(max_depth=30, random_state=42,max_features=15,n_jobs=-1 )\n\n#Creamos modelo  Random Forest Regressor\nrfModel = rf.fit(X_train, y_train)\n\n# Prediccion para Train\npreds_train_RFR = rfModel.predict(X_train)\n# Prediccion para Test\npreds_test_RFR = rfModel.predict(X_test)\n\n# Obtenemos el R2 medio para Train\nprint('R2 medio para Train es : ', metrics.mean_squared_error(y_train, preds_train_RFR, squared = False))\n# Obtenemos el R2 medio para Test\nprint('R2 medio para Test es : ', metrics.mean_squared_error(y_test, preds_test_RFR, squared = False))","30439820":"%%time\n\nfrom sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(n_estimators=80 ,max_depth=30, random_state=42,max_features=15,n_jobs=-1)\n\n#Creamos modelo  Random Forest Regressor\nrfModel = rf.fit(X_train, y_train)\n\n# Prediccion para Train\npreds_train_RFR = rfModel.predict(X_train)\n# Prediccion para Test\npreds_test_RFR = rfModel.predict(X_test)\n\n# Obtenemos el R2 medio para Train\nprint('R2 medio para Train es : ', metrics.mean_squared_error(y_train, preds_train_RFR, squared = False))\n# Obtenemos el R2 medio para Test\nprint('R2 medio para Test es : ', metrics.mean_squared_error(y_test, preds_test_RFR, squared = False))","4590af65":"test = data_test[['Store', 'Dept', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2',\n       'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'Type',\n       'Size', 'Day', 'Month', 'Year', 'SuperBowlWeek', 'LaborDay',\n       'Thanksgiving', 'Christmas']]\npredict_RFR_Tunned = rf.predict(test)","16ae60e2":"sampleSubmission['Weekly_Sales'] = predict_RFR_Tunned\nsampleSubmission.to_csv('submission_RFR_Tunned.csv',index=False)","a7117234":"%%time\n\nfrom sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(n_estimators=80 ,min_samples_split = 4, max_depth=30, random_state=42,max_features=15,n_jobs=-1)\n\n#Creamos modelo  Random Forest Regressor\nrfModel = rf.fit(X_train, y_train)\n\n# Prediccion para Train\npreds_train_RFR = rfModel.predict(X_train)\n# Prediccion para Test\npreds_test_RFR = rfModel.predict(X_test)\n\n# Obtenemos el R2 medio para Train\nprint('R2 medio para Train es : ', metrics.mean_squared_error(y_train, preds_train_RFR, squared = False))\n# Obtenemos el R2 medio para Test\nprint('R2 medio para Test es : ', metrics.mean_squared_error(y_test, preds_test_RFR, squared = False))","8f6d0fe2":"test = data_test[['Store', 'Dept', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2',\n       'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'Type',\n       'Size', 'Day', 'Month', 'Year', 'SuperBowlWeek', 'LaborDay',\n       'Thanksgiving', 'Christmas']]\npredict_RFR_Tunned2 = rf.predict(test)","f8d09988":"sampleSubmission['Weekly_Sales'] = predict_RFR_Tunned2\nsampleSubmission.to_csv('submission_RFR_Tunned2.csv',index=False)","a4a6a560":"%%time\n\nfrom sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(n_estimators=80,min_samples_leaf= 2, max_depth=30, random_state=42,max_features=15,n_jobs=-1)\n\n#Creamos modelo  Random Forest Regressor\nrfModel = rf.fit(X_train, y_train)\n\n# Prediccion para Train\npreds_train_RFR = rfModel.predict(X_train)\n# Prediccion para Test\npreds_test_RFR = rfModel.predict(X_test)\n\n# Obtenemos el R2 medio para Train\nprint('R2 medio para Train es : ', metrics.mean_squared_error(y_train, preds_train_RFR, squared = False))\n# Obtenemos el R2 medio para Test\nprint('R2 medio para Test es : ', metrics.mean_squared_error(y_test, preds_test_RFR, squared = False))","fc9f0079":"test = data_test[['Store', 'Dept', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2',\n       'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'Type',\n       'Size', 'Day', 'Month', 'Year', 'SuperBowlWeek', 'LaborDay',\n       'Thanksgiving', 'Christmas']]\npredict_RFR_Tunned3 = rf.predict(test)","58948225":"sampleSubmission['Weekly_Sales'] = predict_RFR_Tunned3\nsampleSubmission.to_csv('submission_RFR_Tunned3.csv',index=False)","3fd1d718":"\u00bfQue hemos conseguido?\n\n* Hemos obtenido dos dataframes _'train_df' y _'test_df\u2019 donde hemos mergeado en 'train y 'test' las columnas de los dataframes de 'features' y 'stores'. \n* La columna 'Date' estaba en formato objeto y se ha cambiado al formato date.\n* Hemos descompuesto la variable 'Date' creando nuevas variables 'Day','Week', 'Month' y 'Year'.\n","a6376c91":"#### Selecci\u00f3n de variables\n\nUna vez rellenos los valores missing observamos que las variables que poseen m\u00e1s correlaci\u00f3n con las ventas semanales son el tama\u00f1o de la tienda, el departamento, as\u00ed como las diferentes campa\u00f1as de rebajas.\n","37f6ddff":"Empezaremos por eliminar de los dataset data_train y data_test las siguientes variables :\n\n'MarkdownsSum', 'Week', 'Quarter', 'IsHoliday' y 'Date'","d5c9488c":"#### 3.3.1 Observamos los valores faltantes","eeafd698":"#### 2.3.2 Contraste entre Ventas Semanales y temperatura ambiente\n\nSe observa que la las ventas se mantienen estables independientemente de la temperatura exterior como norma general.\n\n\n","a1599cf2":"Utilizaremos train_test_split y dividiremos el conjunto en 70% training y el 30% en test.","8e6e801b":"# <img src=https:\/\/image.shutterstock.com\/image-vector\/big-data-visualization-abstract-graphic-600w-1106564813.jpg>","819b7ab8":"#### 2.2.6 An\u00e1lisis de ventas de las 52 semanas vs Rebajas Totales\n\nEl \u00e9xito en las diferentes campa\u00f1as permite obtener una linealidad en las ventas a lo largo del a\u00f1o. No obstante, existen periodos concretos donde se produce un significativo descenso en las ventas, estos ser\u00edan las cuatro primeras semanas del a\u00f1o y la semana anterior y posterior al Dia de Acci\u00f3n de Gracias.\n","ce3ac965":"### 1.2 Carga de datos","de82dbfb":"## 1. Importar Librer\u00edas & Cargar datos","ec48cf97":"#### 4.4.7 RandomForestRegressor con max_depth = 30 + max features = 15, n_jobs = -1, n_estimators = 80, min_samples_split = 4\n\nlimitamos aun mas las capacidades de cada arbol, en este caso el numero minimo de ejemplos requeridos para que se expanda un nodo y cree ramas pasara de 2 a 4. No se ha encontrado un empeoramiento significativo por lo que lo mantendremos de esta manera.\n\nFinalmente comprobamos la prediccion en Kaggle y observamos que empeora por lo que este cambio no ha sido beneficioso.\n\n#### Kaggle Score: 3,613","08e1ea3b":"###  3.2 Feature Engineering nueva variable: Quarter\n\n\nAgrupamos las semanas por trimestre Q1, Q2,Q3 y Q4. El motivo es para conocer si las ventas y las promociones estan sujetas a los resultados trimestrales que pueda ofrecer la empresa.\n\n* Q1:Semana de la 1 a la 13.\n* Q2:Semana de la 14 a la 26.\n* Q3:Semana de la 27 a la 39.\n* Q4:Semana de la 40 a la 52.\n","bacbf08e":"#### 2.1.10 Ventas Semanales y Departamento\n\nEn la siguiente grafica de barras se comprueba que no todos los departamentos contribuyen de la misma manera en las ventas semanales","b593669f":"### 3.3 Feature Engineering: Rellenar valores missing\n\nPodemos comprobar en los gr\u00e1ficos adjuntos la ausencia de bastantes valores en las variables de las rebajas, CPI y Unemployment.\n\nLas rebajas cuando sean NA el valor ser\u00e1 sustituido por 0, CPI y Unemployment ser\u00e1n rellenos por la media.\n","92865a64":"#### \u00bfQu\u00e9 haremos en esta secci\u00f3n?\n\nCrearemos las siguientes variables:\n\n- Semanas clave:\n    - SuperBowlWeek\n    - LaborDay\n    - Thanksgiving\n    - Christmas\n    \n- Variable Quarter\n- Variable MarkdownSum\n- Rellenar valores missing:\n- Encoding categorical data en IsHoliday.\n- Encoding categorical data en Type.\n- Feature importance\n- An\u00e1lisis de VIF y eliminaci\u00f3n de algunas variables.\n\n \n","5c1fc38c":"#### 2.1.8 Promedio de ventas diarias por meses\n\nLas ventas sufren un ligero descenso seg\u00fan avanzan los d\u00edas del mes a excepci\u00f3n de los meses de noviembre y diciembre.\n","0603ce48":"<img src=https:\/\/www.nticmaster.es\/wp-content\/uploads\/2021\/03\/logotipos-ucm-ntic.png>","beecd525":"## 4. Machine Learning","a2435eb8":"###  1.4 Merge de datasets 'features' y 'stores'\n\nVamos a crear un nuevo dataset en el que a 'Features' le vamos a incluir el \u00e1rea de la tienda, as\u00ed como su tipo que \naparecen en 'Stores' utilizando como elemento com\u00fan entre ambos la columna 'Store' siendo la uni\u00f3n a trav\u00e9s de inner join.\nComprobamos que el nuevo dataset \"feature_store\" posee las 8190 filas, pero con 14 variables en vez de 12 para las 45 tiendas.\n\n\n","6d50a06f":"### 4.2 GradientBoostingRegressor\n\nGB construye un modelo aditivo de manera progresiva por etapas; permite la optimizaci\u00f3n de funciones de p\u00e9rdida diferenciables arbitrarias. En cada etapa se ajusta un \u00e1rbol de regresi\u00f3n al gradiente negativo de la funci\u00f3n de p\u00e9rdida dada.\n\n##### Kaggle Score : 10,038","95df78d7":"Existen algunas variables que presentan valor infinito por tanto eliminamos la siguiente variable\n   * Eliminar la variable IsHoliday\n   ","cecae09c":"#### 4.4.5 RandomForestRegressor con max_depth = 30 + max features = 15, n_jobs = -1\n\nConseguimos una mejora en tiempo de respuesta, al obtener resultados en 23 segundos frente a los 4 minutos de la version inicial.\nNo obstante los resultados del modelo en Train y Test no varian frente a la version anterior","8645c056":"#### 3.3.2 Los valores missing en 'data_train' los rellenamos con el valor '0'.\n\nPara el caso de 'data_train' solo se observan valores missing en las variables Markdown.","f7ae0d6b":"#### 2.1.9 L\u00ednea temporal con el promedio de las ventas semanales.\n\nSe observa un patr\u00f3n que se repite de forma similar para los a\u00f1os de estudio.\n","e7bb0411":"### 2.1 An\u00e1lisis de formatos de tiendas y ventas","2c5292c0":"<img src=https:\/\/image.shutterstock.com\/image-vector\/big-data-artificial-intelligence-concept-600w-1092234560.jpg>","9b37654c":"<img src=https:\/\/www.luismaram.com\/wp-content\/uploads\/2008\/07\/walmart_logo.gif>","bda7e386":"# 1. Introducci\u00f3n\n\nLas cadenas de Retail siempre han trabajado en un entorno competitivo con m\u00e1rgenes cada vez m\u00e1s ajustados. El correcto uso de los datos puede marcar la diferencia, por tanto predecir las ventas as\u00ed como conocer la eficacia en los diferentes  departamentos, las campa\u00f1as comerciales y el comportamiento del consumidor puede marcar la diferencia y nos ayudara a ser comercialmente m\u00e1s eficientes, tener un correcto stock de los productos, ajustar la plantilla de trabajo entre otros. \nEl presente trabajo permite facilitar y automatizar esta tarea con el fin de facilitar la toma de decisiones.\nPara realizar este trabajo se han utilizado datos hist\u00f3ricos de ventas para 45 tiendas Walmart ubicadas en diferentes regiones. Cada tienda contiene varios departamentos y se tiene la tarea de predecir las ventas de todo el departamento para cada tienda.\nAdem\u00e1s, Walmart organiza varios eventos promocionales de rebajas durante todo el a\u00f1o. Estas rebajas preceden a los feriados importantes, los cuatro m\u00e1s grandes de los cuales son el Super Bowl, el Labor Day, Thanksgiving y Christmas. Las semanas que incluyen estos d\u00edas festivos se ponderan cinco veces m\u00e1s en la evaluaci\u00f3n que las semanas que no son festivos. Parte del desaf\u00edo que presenta esta competencia es modelar los efectos de las rebajas en estas semanas de vacaciones en ausencia de datos hist\u00f3ricos completos \/ ideales.\nDe forma adicional se estudia el comportamiento de las ventas en las tiendas seg\u00fan su formato y conocer cuales son los departamentos que mas contribuyen a la cifra semanal. Se estudiara la estacionalidad de las ventas y los resultados de los cinco tipos de rebajas a lo largo del a\u00f1o para conocer como de efectivas ha sido tanto de forma individual y conjunta.\nAnalizaremos el comportamiento del consumidor en funci\u00f3n de si la semana es festiva o no, la temperatura ambiente, el precio del carburante, el IPC as\u00ed como la tasa de desempleo.\n\n#### El presente estudio forma parte del proceso de evaluacion del master de Big Data & Data Science impartido por la Universidad Complutense de Madrid y NTIC\n\n\n\n# 1. Abstract\n\n\nRetail chains have always worked in a competitive environment with increasingly tight margins. The correct use of data can make a difference, therefore predicting sales as well as knowing the effectiveness in different departments, commercial campaigns and consumer behavior can make a difference and help us to be more commercially efficient, have a correct Stock of products, adjust the workforce among others.\nThe present work allows to facilitate and automate this task in order to facilitate decision-making.\nTo carry out this work, historical sales data have been used for 45 Walmart stores located in different regions. Each store contains several departments and you are tasked with predicting the sales of the entire department for each store.\nIn addition, Walmart organizes several promotional sales events throughout the year. These sales precede major holidays, the four biggest of which are the Super Bowl, Labor Day, Thanksgiving and Christmas. The weeks that include these holidays are weighted five times more in the evaluation than the weeks that are not holidays. Part of the challenge this competition presents is modeling the effects of sales on these vacation weeks in the absence of complete \/ ideal historical data.\nAdditionally, the behavior of sales in stores is studied according to their format and to know which are the departments that contribute the most to the weekly figure. The seasonality of sales and the results of the five types of sales throughout the year will be studied to know how effective it has been both individually and jointly.\nWe will analyze consumer behavior depending on whether the week is a holiday or not, the ambient temperature, the price of fuel, the CPI as well as the unemployment rate.\n\n#### This study is part of the evaluation process of the Master of Big Data & Data Science taught by the Complutense University of Madrid and NTIC\n","4692bf57":"#### 2.1.2 Boxplot Superficie y formato de tienda\nPara los tres formatos de tienda, el modelo A es el mayor tama\u00f1o. El formato B es el modelo que presenta un rango de superficie mayor. finalmente, el modelo de tienda C posee un formato que con una superficie similar a todas las tiendas siendo las tiendas de tipo C a un formato de tienda de conveniencia.","e974e718":"### 2.4 Matriz de correlaci\u00f3n","da3b6591":"### 4.1 LinearRegression\n\nRegresi\u00f3n lineal por m\u00ednimos cuadrados ordinarios.\n\nLinearRegression se ajusta a un modelo lineal con coeficientes w = (w1,\u2026, wp) para minimizar la suma residual de cuadrados entre los objetivos observados en el conjunto de datos y los objetivos predichos por la aproximaci\u00f3n lineal.\n\n###### Kaggle Score : 19,651\n","52255b27":"## 3. Feature Engineering","9ddeddf3":"### 3.6 Feature importance.","1c9446cf":"#### \u00bfQu\u00e9 haremos en esta secci\u00f3n?\n\n* En esta secci\u00f3n importaremos las librer\u00edas necesarias y los datos desde kaggle.com\n* Haremos una inspeccion inicial de los archivos\n* Procederemos a crear los dataframes mergeandolos unos con otros\n* Convertiremos la columna \"Date\" a formato datetime\n* Descompondremos la fecha creando nuevas variables 'Day','Week','Month','Year","483e2264":"#### 4.4.6 RandomForestRegressor con max_depth = 30 + max features = 15, n_jobs = -1, n_estimators = 80\n\nLimitando el numero de arboles a 80 obtenemos una ligera mejora. Por debajo de 80 y por encima de este valor hemos obtenido resultados peores.\n\nHemos conseguido una leve mejora en la prediccion frente a la version sin modificar parametros.\n\n#### Kaggle Score :3,569","cd7aa9ed":"#### 4.4.4 RandomForestRegressor con max_depth = 30 + max features = 4,6,10,15\n\nVamos a limitar el numero de variables en cada split para comprobar si de esa manera conseguimos una mejora en el modelo anterior.\n\nCon un numero de 4,6, y 10 hemos empeorado el modelo, por tanto ponemos el limite en 15.","bb9fcfda":"#### 4.4.8 RandomForestRegressor con max_depth = 30 + max features = 15, n_jobs = -1, n_estimators = 80, , min_samples_leaf= 2\n\nPor ultimo, ajustaremos el algoritmo para que el numero minimo de muestras necesarias para estar en un nodo de hoja pase de 1 a 2.\n\nSe comprueba que perjudica a Train pero no tanto a Test y nos podria dar una pista que nos indica que evita el sobreajuste.\n\nFinalmente comprobamos que esta opcion tampoco mejora el modelo.\n\n#### Kaggle Score: 3,594","f29a7eab":"### 1.5 Merge de datasets 'train' & 'feature_store'\n\nRealizamos la operaci\u00f3n para los dataset 'train' & 'feature_store'.","96999686":"## 2.  Exploratory Data Analysis","bd797391":"### 1.7 Convertir columna \"Date\" a formato datetime","c276aaed":"#### 2.1.1 An\u00e1lisis de los formatos de tienda\n\nObservamos que el formato dominante son las tiendas de tipo A seguidos del formato B y C respectivamente.","f27b92ba":"#### 2.3.3 Contraste entre Ventas Semanales y precio de carburante\n\nSe observa que las ventas se mantienen estables siempre que el gal\u00f3n de gasolina no supere los 4.25 USD","6267107d":"#### 2.1.3 Grafico de barras Superficie y Tienda\n\nComplementando al boxplot del apartado 2.2, se observa un primer bloque de 10 tiendas con superficie similar, esto es debido a los outliers del formato B que se solapan con la superficie del formato A.\n\nPosteriormente se observa un grupo intermedio de tiendas con una superficie bastante variable que corresponden al formato B y A.\n\nEn el \u00faltimo tramo existen tiendas con una superficie superior a 200,000 que pertenecen solamente al grupo A.\n","f17ff066":"#### 2.1.11 An\u00e1lisis de ventas de las 52 semanas\n\nSe observa durante las cuatro primeras semanas de enero el periodo de ventas m\u00e1s bajo del a\u00f1o, con una fuerte subida en febrero donde se mantendr\u00edan las cifras en unos valores relativamente constantes hasta la semana 43. Entre la semana 44 hasta las 49 se\nproducen unas ventas muy bajas a excepci\u00f3n de la semana 47 donde se produce una fuerte subida en las ventas.\n\nLa semana 51 comprende las fechas entre el 20 y el 26 de diciembre estando por tanto en Navidades, es comprensible que se produzcan en esa semana las mayores ventas del a\u00f1o.\n\nSe comprueba que las ventas tienen un componente estacionario a lo largo de los a\u00f1os al observarse que repite el mismo patr\u00f3n en 2010, 2011 y 2012.","3cd1caad":"#### 2.1.6 Promedio de ventas mensuales por a\u00f1o.\n\nSe observa un comportamiento similar en casi todos los meses para los a\u00f1os 2010,2011 y 2012. Las mayores ventas se producen en los meses 11 y 12 y tras las Navidades las ventas llegan a sus valores m\u00ednimos","7970caa5":"### 1.8 Descomponemos la fecha creando nuevas variables 'Day','Week','Month','Year'","48b0728c":"### 1.6 Merge de datasets 'test' & 'feature_store'\n\nRealizamos la misma operaci\u00f3n para los dataset 'test' & 'feature_store'. El nuevo dataset mantiene las mismas filas\nque 'test' y tiene las mismas variables que 'train\u2019 a excepci\u00f3n de 'Weekly_Sales'","7a2695eb":"#### Que haremos en esta secci\u00f3n?\n\n* Analizaremos los diferentes formatos de tienda.\n* Obtendremos un boxplot para cada superficie y formato de tienda.\n* Grafico de barras de barras de superficie para cada determinante de tienda.\n* Grafico de ventas semanales por formato de tienda.\n* Promedio de ventas mensuales por formato de tienda.\n* Promedio de ventas mensuales para cada a\u00f1o.\n* Promedio de ventas para cada d\u00eda del mes.\n* Promedio de ventas semanales a trav\u00e9s de una l\u00ednea temporal.\n* Ventas semanales por departamento.\n* An\u00e1lisis de ventas de las 52 semanas del a\u00f1o.\n* An\u00e1lisis de los resultados de las Rebajas 1.\n* An\u00e1lisis de los resultados de las Rebajas 2.\n* An\u00e1lisis de los resultados de las Rebajas 3.\n* An\u00e1lisis de los resultados de las Rebajas 4.\n* An\u00e1lisis de los resultados de las Rebajas 5.\n* An\u00e1lisis de los resultados de las Rebajas Totales.\n* An\u00e1lisis de ventas si la semana es festiva.\n* Contraste entre ventas semanales y temperatura ambiente\n* Contraste entre ventas semanales y precio de carburante\n* Contraste entre ventas semanales e IPC.\n* Contraste entre ventas semanales y desempleo.\n* Matriz de correlaci\u00f3n.\n* Tabla de correlaci\u00f3n de las diferentes variables vs Ventas Semanales\n","2f42094c":"### 4.4 RandomForestRegressor\n\nUn RandomForestRegressor es un metaestimador que se ajusta a una serie de \u00e1rboles de decisi\u00f3n de clasificaci\u00f3n en varias submuestras del conjunto de datos y utiliza promedios para mejorar la precisi\u00f3n predictiva y controlar el sobreajuste. El tama\u00f1o de la submuestra se controla con el par\u00e1metro max_samples si bootstrap = True (predeterminado); de lo contrario, se usa todo el conjunto de datos para construir cada \u00e1rbol.\n\n - n_estimatorsint, default=100\n - criterion{\u201cmse\u201d, \u201cmae\u201d}, default=\u201dmse\u201d\n - max_depth: int, default=None\n - min_samples_split: int or float, default=2\n - min_samples_leaf: int or float, default=1\n - min_weight_fraction_leaf: float, default=0.0\n - max_features: {\u201cauto\u201d, \u201csqrt\u201d, \u201clog2\u201d}, int or float, default=\u201dauto\u201d\n - max_leaf_nodes: int, default=None\n - min_impurity_decrease: float, default=0.0\n - min_impurity_split: float, default=None\n - bootstrap: bool, default=True\n - oob_score: bool, default=False\n - n_jobs: int, default=None\n\n\n","c8b2c15b":"### 2.2 An\u00e1lisis de los resultados de las Rebajas","f7bad023":"#### 2.3.4 Contraste entre Ventas Semanales y desempleo","132719ba":"#### 3.3.3 Los valores missing en 'data_test' correspondientes a 'CPI' y 'Unemployment' los rellenamos con la media.","b29de30a":"###  1.1 Librer\u00edas a importar","791a0343":"#### 2.2.2 An\u00e1lisis de ventas de las 52 semanas vs Rebajas2\n\nAnalizando el comportamiento de las Rebajas2 (Markdown2), se aprecia una estrategia para fechas especificas (semanas desde la 01 hasta la 08, semana 14, semana 45 y semana 52). La correlaci\u00f3n es moderadamente negativa del -0.44. Por tanto, al aumentar las rebajas disminuyen las ventas, esto podr\u00eda ser debido a que se trata de productos b\u00e1sicos donde las unidades vendidas son las mismas independientemente del descuento y el aplicar las rebajas existe menos facturaci\u00f3n.\n","c5a3a497":"#### 2.2.5 An\u00e1lisis de ventas de las 52 semanas vs Rebajas5\n\nSe produce un incremento del modelo de rebajas Markdown5 en la semana 48 siendo esta la semana posterior al Dia de Acci\u00f3n de Gracias y su finalidad podr\u00eda ser eliminar los posibles excedentes de aquellos productos enfocados al 25 de noviembre o bien intentar evitar una ca\u00edda en las ventas en la semana posterior a la festividad. La correlaci\u00f3n es moderadamente negativa del -54% sugiriendo que se est\u00e1 eliminando stock en posibles productos perecederos.\n\n","6283bb66":"#### 2.2.1 An\u00e1lisis de ventas de las 52 semanas vs Rebajas1\n\nAnalizando el grafico resultante se observa una fuerte subida en las ventas en la semana 5 y en Rebajas1 (MarkDown1).\nExiste una fuerte correlaci\u00f3n del 99% entre las cinco primeras semanas del a\u00f1o y la estrategia de ventas con las rebajas Markdown1, por tanto, se supone que pudiera ser una estrategia comercial enfocada en superar la cuesta de enero.\n","07c5bdba":"Se observan algunas variables que tienen un VIF superior a 5 y algunos valores son muy elevados, procederemos a irlas eliminando y ejecutaremos el proceso nuevamente","727c2310":"#### 4.4.2 RandomForestRegressor con bootstrap = False\n\nVamos a cambiar el parametro de bootstrap a False, con esto conseguiremos utilizar todas las variables en cada arbol.\nSe comprueba que da mal resultado pues existe overfitting en Train con un R2 medio muy bajo pero en Test es mas impreciso que en la version original.","8fa1f6f6":"Resumen:\n\n* Hemos creado nuevas variables en aquellas semanas que son claves en las ventas como SuperBowlWeek, LaborDay, Thanksgiving y Christmas\n\n* Igualmente se crea la variable Quarter en la que agrupamos segun los difereentes Q1,Q2,Q3 y Q4.\n* La nueva variable MarkdownSum muestra la suma de todas las promociones para cada fecha dada.\n* Se han rellenado los valores missing en data_train y data_test.\n* Por ultimo las variables IsHoliday y Type se les aplica Encoding Categorical Data para volverlas numericas.\n","521bd030":"#### 2.1.7 Promedio de ventas diarias para cada a\u00f1o.\n\nLas ventas diarias se mantienen de forma relativamente constante en el mes. Los picos que se observan los d\u00edas 23, 24 y 25 son motivados por el aumento de ventas en los meses de noviembre y diciembre.\n","846e54a5":"#### 4.4.1 RandomForestRegressor parametros de serie\n##### Kaggle Score : 3,693","f32d3762":"### 3.3 Feature Engineering nueva variable: Rebajas\n\n\nCreamos una nueva variable 'MarkdownSum' que ser\u00e1 el sumatorio de las cuatro promociones Markdown. En caso de que 2 promociones se solapen en el tiempo se ver\u00e1 reflejado en el sumatorio.\n\n\n","3b68d948":"### 2.3 An\u00e1lisis de diferentes variables\n","f1cba7b9":"#### 2.4.1 Matriz de correlaci\u00f3n de las diferentes variables\n\n\nLas rebajas Markdown1 y Markdown4 est\u00e1n sincronizadas para incrementar las ventas en el mes de enero y por eso se observa una correlaci\u00f3n del 82%\n\nLas rebajas Markdown3 tienen una l\u00f3gica correlaci\u00f3n positiva con IsHoliday por ser una promoci\u00f3n de Dia de Acci\u00f3n de Gracias.\n\nEl precio del combustible tiene una fuerte correlaci\u00f3n positiva con el a\u00f1o, confirmando una subida gradual del precio de carburante a lo largo del tiempo.\n","93daa81b":"Evaluamos la cantidad de filas y columnas en cada uno de los datasets. Observamos que train y test comprenden el 78% y 22% respectivamente del total de los datos de las 45 tiendas.","c509da05":"#### 2.1.5 Promedio de ventas mensuales por formato de tienda\n\nEl mayor promedio de ventas mensuales se produce en el formato A debido a que existen m\u00e1s unidades y su formato es mayor.\nEl formato B posee un comportamiento similar al A con una curva en el crecimiento de las ventas a partir del mes 10.\nPara el formato C se aprecia un comportamiento lineal con una leve ca\u00edda en ventas el mes 12, esto podr\u00eda ser debido a que las tiendas de menor tama\u00f1o ofrecen un servicio de tiendas de proximidad donde abastece de los productos de uso diario y el mes de diciembre al aumentarse las ventas en los formatos A y B, cubre las necesidades de las familias no siendo necesaria la compra en el formato C. Se observa por tanto una correlaci\u00f3n negativa entre los formatos A y C, as\u00ed como B y C.\n","ccf1e66f":"#### 2.4.2 Tabla de correlaci\u00f3n de las diferentes variables vs Ventas Semanales\n\n\nLas variables que m\u00e1s importancia tienen en las ventas semanales son la superficie de venta de la tienda, el departamento y las diferentes rebajas.\n\nEl formato de tienda, el desempleo y el IPC parecen afectar negativamente de forma leve en las ventas cuando estas variables incrementan su valor.\n","7192c613":"### 4.3 XGBRegressor\n\nXGBoost significa \"Extreme Gradient Boosting\" y es una implementaci\u00f3n del algoritmo de \u00e1rboles de aumento de gradiente. XGBoost es un popular modelo de aprendizaje autom\u00e1tico supervisado con caracter\u00edsticas como velocidad de c\u00e1lculo, paralelizaci\u00f3n y rendimiento.\n\n##### Kaggle Score : 5,545","b891a791":"#### 4.4.3 RandomForestRegressor con max_depth = 30\n Probamos limitando la profundidad del arbol por si de esta manera evitamos que se produzca un sobreajuste.\n \nSe ha probado con diferentes ajustes pero con valores de 15 o 20 se ha comprobado que el R2 en Train y Test empeora notablemente.\nSe observa una muy leve mejora pero es un punto de partida para ir cambiando otros parametros","9b1f9203":"<img src=https:\/\/image.shutterstock.com\/image-vector\/vector-background-featuring-abstract-industry-600w-1171013590.jpg>","c016eb50":"###  3.5 Feature Engineering : Type  Encoding Categorical Data.\n\nla variable 'Type' tendra valor 1 si la tienda era de la categoria A, 2 si es de la categoria B y 3 si es de la categoria C.","9f47f568":"#### 2.3.1 An\u00e1lisis de ventas si la semana es festiva\n\nCuando la semana es festiva se observa un aumento de las ventas semanales con bastante dispersi\u00f3n de valores frente a una semana regular.\n","147929a4":"### 3.1 Feature Engineering Nueva variable:  Semanas Clave\n\n\nSe van a crear cuatro nuevas variables que corresponden a aquellas semanas que se consideran claves en las ventas, estas ser\u00edan:\n\n* SuperBowlWeek: Semana 6 del a\u00f1o.\n* LaborDay: Semana 36 del a\u00f1o.\n* Thanksgiving: Semana 47 del a\u00f1o.\n* Christmas: Semana 52 del a\u00f1o.\n\n\n","2659f7cb":"#### 2.2.3 An\u00e1lisis de ventas de las 52 semanas vs Rebajas3\nSe tratar\u00eda de una promoci\u00f3n espec\u00edfica para la semana 47 del d\u00eda de Acci\u00f3n de Gracias (25 de noviembre) con una correlaci\u00f3n positiva del 99%\n","6e79cf02":"### 1.3 Inspecci\u00f3n inicial\nObservamos el tipo de dato de cada una de las variables para los diferentes archivos:","9e9f9e05":"#### 2.1.4 Grafico ventas semanales por formato de tienda","e78f66db":"\u00bfQu\u00e9 informaci\u00f3n hemos obtenido hasta ahora?\n\n* El dataset _'features', 'train' y 'test'_ poseen la fecha en formato objet y debe ser modificada.\n* El dataset 'stores' posee 45 filas correspondientes al n\u00famero de tiendas que vamos a analizar\n* Los conjuntos _'train'_ y _'test_ comprenden el 78% y 22% respectivamente del total de los datos de estudio.","97e50a8b":"### 3.4 Feature Engineering: IsHoliday Encoding Categorical Data\n\nLa variable 'IsHoliday' tendr\u00e1 valor 1 si su valor original es 'True' y 'False' en caso contrario.\n\n","29dae147":"#### 2.3.4 Contraste entre Ventas Semanales e IPC\n\nEl IPC no parece ser un factor determinante que afecte a las ventas semanales. ","da14b47c":"<img src=https:\/\/image.shutterstock.com\/image-photo\/download-data-storage-business-technology-600w-1420922999.jpg>","877081ae":"#### 2.2.4 An\u00e1lisis de ventas de las 52 semanas vs Rebajas4\n\nEs una campa\u00f1a enfocada en las cuatro primeras semanas del a\u00f1o, la semana 9 y la semana 31 con una fuerte correlaci\u00f3n positiva del 82%.\n"}}