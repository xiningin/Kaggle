{"cell_type":{"c2e6cd02":"code","a6a75364":"code","a12040f3":"code","d9197698":"code","a177eea4":"code","a0a959a3":"code","57d58d88":"code","eb6b2f70":"code","137b30b5":"code","104cdd5c":"code","19fde1ce":"code","33099c5c":"code","d029e92d":"code","eaed87c4":"code","83f81ef4":"code","5e7b5a6a":"code","55517344":"code","ecc8eba2":"code","ce729561":"code","ea8d6df0":"code","d226810a":"code","7db5de90":"code","5569cca0":"code","8ea76e0f":"code","c77f0969":"code","ffb1affb":"markdown","7cb053cc":"markdown","a730f548":"markdown","acf9afda":"markdown","0c91bc50":"markdown","958ec6e9":"markdown"},"source":{"c2e6cd02":"#Import all relevant libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier","a6a75364":"#Read the data and show a sample\nurl = '..\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv'\ndf = pd.read_csv(url, index_col = 'sl_no')\ndict = {'Placed': 1, 'Not Placed': 0}\n#Here I map the status feature to 0s and 1s\ndf['status'] = df['status'].map(dict)\ndf.head(10)","a12040f3":"#Check missing values in df\ndf.isnull().any()\n#We see that missing values are not present except in salary which I drop later, so there is no need to replace NaNs","d9197698":"sns.countplot(x='status' , hue = 'gender' , data =df)","a177eea4":"sns.countplot(x='status' , hue = 'degree_t' , data =df)","a0a959a3":"sns.countplot(x='status' , hue = 'specialisation' , data =df)","57d58d88":"sns.countplot(x='status' , hue = 'workex' , data =df)","eb6b2f70":"sns.countplot(x='status' , hue = 'hsc_b' , data =df)","137b30b5":"sns.countplot(x='status' , hue = 'ssc_b' , data =df)","104cdd5c":"sns.countplot(x='status' , hue = 'hsc_s' , data =df)","19fde1ce":"sns.scatterplot(x=df.mba_p, y=df.status)","33099c5c":"sns.scatterplot(x=df.etest_p, y=df.status)","d029e92d":"sns.scatterplot(x=df.degree_p, y=df.status)","eaed87c4":"sns.scatterplot(x=df.hsc_p, y=df.status)","83f81ef4":"sns.scatterplot(x=df.ssc_p, y=df.status)","5e7b5a6a":"df.status.value_counts()\n#We can see that the data is not balanced, we can attempt to balance the data but I didn't see any significant improvement in the predictions when doing that","55517344":"#Split the data in X and y, I'm dropping the salary feature since we are trying to predict the status so it doesn't make sense to consider the salary as a predictor.\ny = df.status\nX = df.drop(['status', 'salary'], axis=1)","ecc8eba2":"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)","ce729561":"#Selecting categorical colums\ncat_cols = [cname for cname in X_train.columns if X_train[cname].dtype == \"object\"]\n\n# Select numerical columns\nnum_cols = [cname for cname in X_train.columns if X_train[cname].dtype in ['int64', 'float64']]","ea8d6df0":"# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[cat_cols]), index = X_train.index)\nOH_cols_test = pd.DataFrame(OH_encoder.transform(X_test[cat_cols]), index = X_test.index)\n\n\n# Add one-hot encoded columns to numerical features\nOH_X_train = pd.concat([X_train[num_cols], OH_cols_train], axis=1)\nOH_X_test = pd.concat([X_test[num_cols], OH_cols_test], axis=1)","d226810a":"#scaling\nfrom sklearn.preprocessing import scale\nscaled_X_train = pd.DataFrame(scale(OH_X_train[num_cols]), index=OH_X_train.index)\nscaled_X_test = pd.DataFrame(scale(OH_X_test[num_cols]), index=OH_X_test.index)\n\nscaled_X_train.columns = OH_X_train[num_cols].columns\nscaled_X_test.columns = OH_X_test[num_cols].columns\n\nOH_X_train[num_cols] = scaled_X_train[num_cols]\nOH_X_test[num_cols] = scaled_X_test[num_cols]","7db5de90":"OH_X_train.head(10)","5569cca0":"#KNN neighbors classifier model\nknn_model = KNeighborsClassifier(n_neighbors=10)\nknn_model.fit(OH_X_train, y_train)\npreds = knn_model.predict(OH_X_test)\nprint(accuracy_score(y_test, preds))","8ea76e0f":"#Random forestclassifier model\nrf_model = RandomForestClassifier(n_estimators = 100, random_state = 0)\nrf_model.fit(OH_X_train, y_train)\npreds = rf_model.predict(OH_X_test)\nprint(accuracy_score(y_test, preds))","c77f0969":"#logistic regression model\nlog_model = LogisticRegression(max_iter=100)\nlog_model.fit(OH_X_train, y_train)\npreds = log_model.predict(OH_X_test)\nprint(accuracy_score(y_test, preds))","ffb1affb":"# Preprocessing","7cb053cc":"# 3 different models for predicting placement status","a730f548":"The model that showed the best performance was logistic regression.","acf9afda":"# Basic EDA\nHere we see that gender, ssc_p, hsc_p, degree_p, workex, specialization and mba_p are the more relevant features","0c91bc50":"# Thanks for checking my notebook out, I'm still a beginner so any comment or suggestion is welcome, thanks!","958ec6e9":"# Beginner approach to campus palcement prediction"}}