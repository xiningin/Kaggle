{"cell_type":{"9d0e2cec":"code","15f32234":"code","20360b48":"code","33ae9306":"code","b471d64e":"code","2e62eb29":"code","7373990a":"code","49444823":"code","390e175d":"code","e204c018":"code","fc12fe55":"code","1889708b":"code","151186b7":"code","eee2e7de":"code","99257ea5":"code","1c321171":"code","624217ef":"code","e2c01e38":"code","b7db8df4":"code","6d6d4c12":"code","93d2c03d":"code","f63b4f69":"code","8e86a10b":"code","a0cdedab":"code","379846c3":"code","1f8aa0af":"code","09d661d9":"code","57cfbd8a":"markdown","78475e84":"markdown","b861467f":"markdown","d48a3333":"markdown","b66519b6":"markdown","e8b144b5":"markdown","9fb581ac":"markdown","9a0285d1":"markdown","1c7f53f0":"markdown","ea8f4904":"markdown","597fa90a":"markdown","188f2cd1":"markdown","23798220":"markdown","457a81bf":"markdown"},"source":{"9d0e2cec":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport tensorflow as tf\nfrom tensorflow.keras import models, layers\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.optimizers import Adam\n\n# ignoring warnings\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\nimport os, cv2, json\nfrom PIL import Image\nfrom random import randint","15f32234":"# File Parameters\nWORK_DIR = \"..\/input\/humpback-whale-identification\"\nlabel_col = \"Id\"\nimg_col = \"Image\"\ntrain_folder = \"train\"\ntest_folder = \"test\"\n\nos.listdir(WORK_DIR)","20360b48":"print('Train images: %d' %len(os.listdir(\n    os.path.join(WORK_DIR, train_folder))))","33ae9306":"train_labels = pd.read_csv(os.path.join(WORK_DIR, \"train.csv\"))\nlabel_names = train_labels[label_col].value_counts().index\nlabel_map = {name:i for (i,name) in enumerate(label_names)}\ninv_label_map = {v: k for k, v in label_map.items()}\n\ntrain_labels['label_name'] = train_labels[label_col].copy()\ntrain_labels[label_col] = train_labels[label_col].map(label_map)\ndisplay(train_labels.head())","b471d64e":"# Main parameters\nBATCH_SIZE = 8\nSTEPS_PER_EPOCH = len(train_labels)*0.8 \/ BATCH_SIZE\nVALIDATION_STEPS = len(train_labels)*0.2 \/ BATCH_SIZE\nEPOCHS = 4\nTARGET_SIZE = 512","2e62eb29":"sns.set_style(\"whitegrid\")\nfig, ax = plt.subplots(figsize = (6, 4))\n\nfor i in ['top', 'right', 'left']:\n    ax.spines[i].set_visible(False)\nax.spines['bottom'].set_color('black')\n\nsns.countplot(y = train_labels[\"label_name\"], order=train_labels[\"label_name\"].value_counts().index[:5], edgecolor = 'black',\n              palette = reversed(sns.color_palette(\"viridis\", 5)))\nplt.xlabel('Classes', fontfamily = 'serif', size = 15)\nplt.ylabel('Count', fontfamily = 'serif', size = 15)\nplt.xticks(fontfamily = 'serif', size = 12)\nplt.yticks(fontfamily = 'serif', size = 12)\nax.grid(axis = 'y', linestyle = '--', alpha = 0.9)\nplt.show()","7373990a":"def img_plot(df, label=None):\n    if label:\n        sample = df[df[\"label_name\"] == label].sample(3)\n        folder = train_folder\n    else:\n        sample = df.sample(3)\n        sample[\"label_name\"] = \"test\"\n        folder = test_folder\n    plt.figure(figsize=(15, 5))\n    for ind, (image_id, label) in enumerate(zip(sample[img_col], sample[\"label_name\"])):\n        plt.subplot(1, 3, ind + 1)\n        img = cv2.imread(os.path.join(WORK_DIR, folder, image_id))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        plt.title(\"Label: {}\\nShape: {}\".format(label, img.shape))\n        plt.imshow(img)\n        plt.axis(\"off\")\n\n    plt.show()","49444823":"for lbl in label_names[:3]:\n    img_plot(train_labels, lbl)","390e175d":"train_datagen = ImageDataGenerator(validation_split = 0.2,\n                                     preprocessing_function = None,\n                                     rotation_range = 45,\n                                     zoom_range = 0.2,\n                                     horizontal_flip = True,\n                                     vertical_flip = True,\n                                     fill_mode = 'nearest',\n                                     shear_range = 0.1,\n                                     height_shift_range = 0.1,\n                                     width_shift_range = 0.1)\n\ntrain_generator = train_datagen.flow_from_dataframe(train_labels,\n                         directory = os.path.join(WORK_DIR, train_folder),\n                         subset = \"training\",\n                         x_col = img_col,\n                         y_col = label_col,\n                         color_mode='grayscale',\n                         target_size = (TARGET_SIZE, TARGET_SIZE),\n                         batch_size = BATCH_SIZE,\n                         class_mode = \"raw\")\n\n\nvalidation_datagen = ImageDataGenerator(validation_split = 0.2)\n\nvalidation_generator = validation_datagen.flow_from_dataframe(train_labels,\n                         directory = os.path.join(WORK_DIR, train_folder),\n                         subset = \"validation\",\n                         color_mode='grayscale',\n                         x_col = img_col,\n                         y_col = label_col,\n                         target_size = (TARGET_SIZE, TARGET_SIZE),\n                         batch_size = BATCH_SIZE,\n                         class_mode = \"raw\")","e204c018":"def plot_from_generator(train_generator, label_names, n=5):\n    for _ in range(n):\n        batch_num = randint(0, len(train_generator))\n        bach_element = randint(0, BATCH_SIZE-1)\n\n        batch = train_generator[batch_num]\n        aug_images = [i\/ 255 for i in batch[0]]\n        aug_labels = list(batch[1])\n\n        fig, axes = plt.subplots(1, 5, figsize = (20, 10))\n        axes = axes.flatten()\n        for img, lbl, ax in zip(aug_images, aug_labels, axes):\n            img = np.squeeze(img)\n            ax.imshow(img)\n            ax.set_title(lbl)\n            ax.axis('off')\n        plt.tight_layout()\n        plt.show()","fc12fe55":"plot_from_generator(train_generator, label_names, n=5)","1889708b":"def create_model():\n    conv_base = EfficientNetB0(include_top = False, weights = None,\n                               input_shape = (TARGET_SIZE, TARGET_SIZE, 1))\n    model = conv_base.output\n    model = layers.GlobalAveragePooling2D()(model)\n    model = layers.Dense(len(label_names)+1, activation = \"softmax\")(model)\n    model = models.Model(conv_base.input, model)\n\n    model.compile(optimizer = Adam(lr = 0.001),\n                  loss = \"sparse_categorical_crossentropy\",\n                  metrics = [\"acc\"])\n    return model","151186b7":"model = create_model()\nmodel.summary()","eee2e7de":"print('Our EfficientNet CNN has %d layers' %len(model.layers))","99257ea5":"# model.load_weights('')","1c321171":"model_name = '.\/EffNetB0_512_8_best_weights.h5'\nmodel_save = ModelCheckpoint(model_name, \n                             save_best_only = True, \n                             save_weights_only = True,\n                             monitor = 'val_loss', \n                             mode = 'min', verbose = 1)\nearly_stop = EarlyStopping(monitor = 'val_loss', min_delta = 0.001, \n                           patience = 5, mode = 'min', verbose = 1,\n                           restore_best_weights = True)\nreduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.3, \n                              patience = 2, min_delta = 0.001, \n                              mode = 'min', verbose = 1)\n\n\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch = STEPS_PER_EPOCH,\n    epochs = EPOCHS,\n    validation_data = validation_generator,\n    validation_steps = VALIDATION_STEPS,\n    callbacks = [model_save, early_stop, reduce_lr]\n)","624217ef":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\nsns.set_style(\"white\")\nplt.suptitle('Train history', size = 15)\n\nax1.plot(epochs, acc, \"bo\", label = \"Training acc\")\nax1.plot(epochs, val_acc, \"b\", label = \"Validation acc\")\nax1.set_title(\"Training and validation acc\")\nax1.legend()\n\nax2.plot(epochs, loss, \"bo\", label = \"Training loss\", color = 'red')\nax2.plot(epochs, val_loss, \"b\", label = \"Validation loss\", color = 'red')\nax2.set_title(\"Training and validation loss\")\nax2.legend()\n\nplt.show()","e2c01e38":"model.load_weights(model_name)","b7db8df4":"def activation_layer_vis(img, activation_layer = 0, layers = 10):\n    layer_outputs = [layer.output for layer in model.layers[:layers]]\n    activation_model = models.Model(inputs = model.input, outputs = layer_outputs)\n    activations = activation_model.predict(img)\n    fig, axes = plt.subplots(1, 1, figsize = (5, 5))\n    axes.matshow(np.squeeze(activations[activation_layer][0, :, :, :]), cmap = 'viridis')\n    axes.axis('off')\n    plt.tight_layout()\n    plt.show()","6d6d4c12":"img_tensor = validation_generator[0][0]","93d2c03d":"activation_layer_vis(img_tensor)","f63b4f69":"def all_activations_vis(img, layers = 10):\n    layer_outputs = [layer.output for layer in model.layers[:layers]]\n    activation_model = models.Model(inputs = model.input, outputs = layer_outputs)\n    activations = activation_model.predict(img)\n    \n    layer_names = []\n    for layer in model.layers[:layers]: \n        layer_names.append(layer.name) \n\n    \n    images_per_row = 5\n    rows = np.ceil(13 \/ 5)\n    \n    plt.figure(figsize=(images_per_row*5, rows*5))\n    i = 0\n    for (layer_name, layer_activation) in zip(layer_names, activations): \n        n_features = layer_activation.shape[-1]\n\n        size = layer_activation.shape[1]\n        channel_image = np.squeeze(layer_activation[0, :, :, 0]) \n        channel_image -= channel_image.mean() \n        channel_image \/= channel_image.std() \n        channel_image *= 64 \n        channel_image += 128 \n        channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n        \n        plt.subplot(rows, images_per_row, i + 1)\n        plt.imshow(channel_image)\n        \n        plt.title(layer_name) \n        plt.grid(False)\n        plt.axis('off')\n        \n        i += 1\n    plt.tight_layout(pad=.5)\n    plt.show()","8e86a10b":"all_activations_vis(img_tensor, 10)","a0cdedab":"ss = pd.read_csv(os.path.join(WORK_DIR, \"sample_submission.csv\"))\nss","379846c3":"for lbl in label_names[:3]:\n    img_plot(ss)","1f8aa0af":"preds = []\ntop_n = 5\nfor image_id in ss[img_col]:\n    image = Image.open(os.path.join(WORK_DIR, test_folder, image_id)).convert('L')\n    image = image.resize((TARGET_SIZE, TARGET_SIZE))\n    image = np.expand_dims(image, axis = 0)\n    arr = model.predict(image)[0].argsort()[-top_n:][::-1]\n    p = \" \".join(np.vectorize(inv_label_map.get)(arr))\n    preds.append(p)\nss[label_col] = preds\nss","09d661d9":"ss.to_csv('submission.csv', index = False)\nprint(ss.shape)","57cfbd8a":"### Loading weights","78475e84":"## Loading packages","b861467f":"# Visualization of CNN intermediate activations","d48a3333":"### Training","b66519b6":"# Modeling","e8b144b5":"### Visualization of the first layer","9fb581ac":"## Cassava Leaf Disease Classification:","9a0285d1":"### Visualization of the first 5 layers","1c7f53f0":"## Work directory","ea8f4904":"# Preparation for modeling","597fa90a":"# Prediction","188f2cd1":"# First look at the data","23798220":"### ImageDataGenerator","457a81bf":"Visualization of intermediate activations gives a rough step-by-step understanding of how CNN works."}}