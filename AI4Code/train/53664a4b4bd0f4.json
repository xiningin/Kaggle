{"cell_type":{"40ee3fd6":"code","0adc1d8b":"code","ae90b623":"code","f1fbe9a1":"code","ad0d277c":"code","6f64d818":"code","27563262":"code","92538821":"code","441a0655":"code","5bad5d20":"code","7b958565":"code","8db78a32":"code","43692a5d":"code","45434773":"code","2017b9c5":"code","83045ca6":"code","5b8d343c":"code","ba2e981f":"code","4cf302ed":"code","3eadf78a":"code","67226a14":"code","7ffd3527":"code","b58580d9":"code","44a6337c":"code","e2ce90d6":"code","a3d9072f":"code","63c5b3c8":"code","ab6ae5cf":"markdown","323015a6":"markdown","09b2f6a3":"markdown","6b0eebf2":"markdown","970a5ba6":"markdown"},"source":{"40ee3fd6":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom scipy.stats import beta\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader,TensorDataset\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import TensorBoardLogger,CSVLogger\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping","0adc1d8b":"# needed for deterministic output\npl.seed_everything(2)\n\n# device in which the model will be trained\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","ae90b623":"dataset = pd.read_csv(\"..\/input\/santander-customer-transaction-prediction\/train.csv\")\ndataset","f1fbe9a1":"dataset.info()","ad0d277c":"dataset.groupby(\"target\")[\"ID_code\"].count() \/ len(dataset)","6f64d818":"# dataset stratified split: train 10% - valid 10% - test 80%\n\nskf = StratifiedKFold(n_splits=10, shuffle=True, random_state=2)\nsplit = skf.split(dataset, dataset.target)\n_,train_index = next(split)\n_,valid_index = next(split)\n\ntrain_dset = dataset.loc[train_index].reset_index(drop=True)\nvalid_dset = dataset.loc[valid_index].reset_index(drop=True)\ntest_dset = dataset.drop(train_index).drop(valid_index).reset_index(drop=True)","27563262":"display(train_dset.groupby(\"target\")[\"ID_code\"].count() \/ len(train_dset))\ndisplay(valid_dset.groupby(\"target\")[\"ID_code\"].count() \/ len(valid_dset))\ndisplay(test_dset.groupby(\"target\")[\"ID_code\"].count() \/ len(test_dset))","92538821":"input_features = dataset.columns[2:].tolist()\ntarget = \"target\"","441a0655":"# parsing inputs as pytorch tensor dataset\n\ntrain_tensor_dset = TensorDataset(\n    torch.tensor(train_dset[input_features].values, dtype=torch.float),\n    torch.tensor(train_dset[target].values.reshape(-1,1), dtype=torch.float)\n)\n\nvalid_tensor_dset = TensorDataset(\n    torch.tensor(valid_dset[input_features].values, dtype=torch.float),\n    torch.tensor(valid_dset[target].values.reshape(-1,1), dtype=torch.float)\n)\n\ntest_tensor_dset = TensorDataset(\n    torch.tensor(test_dset[input_features].values, dtype=torch.float),\n    torch.tensor(test_dset[target].values.reshape(-1,1), dtype=torch.float) \n)","5bad5d20":"len(train_dset)","7b958565":"len(valid_dset)","8db78a32":"len(test_dset)","43692a5d":"class DNN(pl.LightningModule):\n\n    def __init__(self, input_dim, output_dim, nn_depth, nn_width, dropout, momentum):\n        super().__init__()\n\n        self.bn_in = nn.BatchNorm1d(input_dim, momentum=momentum)\n        self.dp_in = nn.Dropout(dropout)\n        self.ln_in = nn.Linear(input_dim, nn_width, bias=False)\n\n        self.bnorms = nn.ModuleList([nn.BatchNorm1d(nn_width, momentum=momentum) for i in range(nn_depth-1)])\n        self.dropouts = nn.ModuleList([nn.Dropout(dropout) for i in range(nn_depth-1)])\n        self.linears = nn.ModuleList([nn.Linear(nn_width, nn_width, bias=False) for i in range(nn_depth-1)])\n        \n        self.bn_out = nn.BatchNorm1d(nn_width, momentum=momentum)\n        self.dp_out = nn.Dropout(dropout\/2)\n        self.ln_out = nn.Linear(nn_width, output_dim, bias=False)\n\n        self.loss = nn.BCEWithLogitsLoss()\n\n    def forward(self, x):\n        x = self.bn_in(x)\n        x = self.dp_in(x)\n        x = nn.functional.relu(self.ln_in(x))\n\n        for bn_layer,dp_layer,ln_layer in zip(self.bnorms,self.dropouts,self.linears):\n            x = bn_layer(x)\n            x = dp_layer(x)\n            x = ln_layer(x)\n            x = nn.functional.relu(x)\n            \n        x = self.bn_out(x)\n        x = self.dp_out(x)\n        x = self.ln_out(x)\n        return x\n\n    def training_step(self, batch, batch_idx):\n        X, y = batch\n        y_hat = self.forward(X)\n        loss = self.loss(y_hat, y)\n        self.log('train_loss', loss)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        X, y = batch\n        y_hat = self.forward(X)\n        loss = self.loss(y_hat, y)\n        self.log('valid_loss', loss)\n        \n    def test_step(self, batch, batch_idx):\n        X, y = batch\n        y_logit = self.forward(X)\n        y_probs = torch.sigmoid(y_logit).detach().cpu().numpy()\n        loss = self.loss(y_logit, y)\n        metric = roc_auc_score(y.cpu().numpy(), y_probs)\n        self.log('test_loss', loss)\n        self.log('test_metric', metric)\n        \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=5e-3, weight_decay=1e-4)\n        scheduler = {\n            'scheduler': ReduceLROnPlateau(\n                optimizer, \n                mode=\"min\", \n                factor=0.5, \n                patience=5, \n                min_lr=1e-5),\n            'interval': 'epoch',\n            'frequency': 1,\n            'reduce_on_plateau': True,\n            'monitor': 'valid_loss',\n        }\n        return [optimizer], [scheduler]","45434773":"model = DNN(\n    input_dim=len(input_features), \n    output_dim=1, \n    nn_depth=3, \n    nn_width=128, \n    dropout=0.2, \n    momentum=0.1\n)\n\nlogger = logger = CSVLogger(\"logs\", name=\"mlp_wo_mixup\")\n\nearly_stop_callback = EarlyStopping(\n   monitor='valid_loss',\n   min_delta=.0,\n   patience=20,\n   verbose=True,\n   mode='min'\n)\n\ntrainer = pl.Trainer(\n    callbacks=[early_stop_callback], \n    min_epochs=10, \n    max_epochs=200, \n    gpus=0, \n    logger=logger, \n    deterministic=True\n)","2017b9c5":"model.summarize()","83045ca6":"trainer.fit(\n    model, \n    DataLoader(train_tensor_dset, batch_size=1024, shuffle=True, num_workers=4),\n    DataLoader(valid_tensor_dset, batch_size=1024, shuffle=False, num_workers=4)\n)","5b8d343c":"# AUC on validation dataset\ntrainer.test(model, DataLoader(valid_tensor_dset, batch_size=1024, shuffle=False, num_workers=4))","ba2e981f":"# AUC on test dataset\ntrainer.test(model, DataLoader(test_tensor_dset, batch_size=1024, shuffle=False, num_workers=4))","4cf302ed":"metrics = pd.read_csv(\"logs\/mlp_wo_mixup\/version_0\/metrics.csv\")\n\ndf1 = metrics.loc[:,[\"step\",\"train_loss\"]].dropna()\ndf2 = metrics.loc[:,[\"step\",\"valid_loss\"]].dropna()\n\nplt.figure(figsize=(12,5))\nplt.plot(df1.step, df1.train_loss, \"o-\", label=\"train_loss\")\nplt.plot(df2.step, df2.valid_loss, \"o-\", label=\"valid_loss\")\nplt.grid()\nplt.legend(loc=\"best\")\nplt.show()","3eadf78a":"alpha = 0.25\n\nx = np.linspace(beta.ppf(0.01, alpha, alpha), beta.ppf(0.99, alpha, alpha), 100)\nplt.plot(x, beta.pdf(x, alpha, alpha), 'r-', lw=5, alpha=0.6, label='beta pdf')\nplt.grid()\nplt.show()","67226a14":"class DNN(pl.LightningModule):\n\n    def __init__(self, input_dim, output_dim, nn_depth, nn_width, dropout, momentum, alpha=0.8):\n        super().__init__()\n        \n        self.alpha = alpha\n        \n        self.bn_in = nn.BatchNorm1d(input_dim, momentum=momentum)\n        self.dp_in = nn.Dropout(dropout)\n        self.ln_in = nn.Linear(input_dim, nn_width, bias=False)\n\n        self.bnorms = nn.ModuleList([nn.BatchNorm1d(nn_width, momentum=momentum) for i in range(nn_depth-1)])\n        self.dropouts = nn.ModuleList([nn.Dropout(dropout) for i in range(nn_depth-1)])\n        self.linears = nn.ModuleList([nn.Linear(nn_width, nn_width, bias=False) for i in range(nn_depth-1)])\n        \n        self.bn_out = nn.BatchNorm1d(nn_width, momentum=momentum)\n        self.dp_out = nn.Dropout(dropout\/2)\n        self.ln_out = nn.Linear(nn_width, output_dim, bias=False)\n\n        self.loss = nn.BCEWithLogitsLoss()\n\n    def forward(self, x):\n        x = self.bn_in(x)\n        x = self.dp_in(x)\n        x = nn.functional.relu(self.ln_in(x))\n\n        for bn_layer,dp_layer,ln_layer in zip(self.bnorms,self.dropouts,self.linears):\n            x = bn_layer(x)\n            x = dp_layer(x)\n            x = ln_layer(x)\n            x = nn.functional.relu(x)\n            \n        x = self.bn_out(x)\n        x = self.dp_out(x)\n        x = self.ln_out(x)\n        return x\n\n    def training_step(self, batch, batch_idx):\n        X, y = batch\n        \n        lam = np.random.beta(alpha,alpha)\n        lam = torch.FloatTensor([lam]).to(self.device)\n        n = len(batch)\/\/2\n        X = lam * X[:n,:] + (1-lam) * X[n:,:]\n        y = lam * y[:n,:] + (1-lam) * y[n:,:]\n        \n        y_hat = self.forward(X)\n        loss = self.loss(y_hat, y)\n        self.log('train_loss', loss)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        X, y = batch\n        y_hat = self.forward(X)\n        loss = self.loss(y_hat, y)\n        self.log('valid_loss', loss)\n        \n    def test_step(self, batch, batch_idx):\n        X, y = batch\n        y_logit = self.forward(X)\n        y_probs = torch.sigmoid(y_logit).detach().cpu().numpy()\n        loss = self.loss(y_logit, y)\n        metric = roc_auc_score(y.cpu().numpy(), y_probs)\n        self.log('test_loss', loss)\n        self.log('test_metric', metric)\n        \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=5e-3, weight_decay=1e-4)\n        scheduler = {\n            'scheduler': ReduceLROnPlateau(\n                optimizer, \n                mode=\"min\", \n                factor=0.5, \n                patience=5, \n                min_lr=1e-5),\n            'interval': 'epoch',\n            'frequency': 1,\n            'reduce_on_plateau': True,\n            'monitor': 'valid_loss',\n        }\n        return [optimizer], [scheduler]","7ffd3527":"model = DNN(\n    input_dim=len(input_features), \n    output_dim=1, \n    nn_depth=3, \n    nn_width=128, \n    dropout=0.2, \n    momentum=0.1,\n    alpha=0.25,\n)\n\nlogger = logger = CSVLogger(\"logs\", name=\"mlp_w_mixup\")\n\nearly_stop_callback = EarlyStopping(\n   monitor='valid_loss',\n   min_delta=.0,\n   patience=20,\n   verbose=True,\n   mode='min'\n)\n\ntrainer = pl.Trainer(\n    callbacks=[early_stop_callback], \n    min_epochs=10, \n    max_epochs=200, \n    gpus=0, \n    logger=logger,\n    deterministic=True\n)","b58580d9":"model.summarize()","44a6337c":"trainer.fit(\n    model, \n    DataLoader(train_tensor_dset, batch_size=1024, shuffle=True, num_workers=4, drop_last=True),\n    DataLoader(valid_tensor_dset, batch_size=1024, shuffle=False, num_workers=4)\n)","e2ce90d6":"# AUC on validation dataset\ntrainer.test(model, DataLoader(valid_tensor_dset, batch_size=1024, shuffle=False, num_workers=4))","a3d9072f":"# AUC on test dataset\ntrainer.test(model, DataLoader(test_tensor_dset, batch_size=1024, shuffle=False, num_workers=4))","63c5b3c8":"metrics = pd.read_csv(\"logs\/mlp_w_mixup\/version_0\/metrics.csv\")\n\ndf1 = metrics.loc[:,[\"step\",\"train_loss\"]].dropna()\ndf2 = metrics.loc[:,[\"step\",\"valid_loss\"]].dropna()\n\nplt.figure(figsize=(12,5))\nplt.plot(df1.step, df1.train_loss, \"o-\", label=\"train_loss\")\nplt.plot(df2.step, df2.valid_loss, \"o-\", label=\"valid_loss\")\nplt.grid()\nplt.legend(loc=\"best\")\nplt.show()","ab6ae5cf":"***\n## 3-layers MLP with mixup","323015a6":"***\n## data preparation","09b2f6a3":"# Assessment of a plain MLP+mixup on SCTP\n","6b0eebf2":"***\n## 3-layers MLP without mixup","970a5ba6":"***"}}