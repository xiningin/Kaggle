{"cell_type":{"da5787a7":"code","66a6fb1d":"code","182022ad":"code","e06fd7ab":"code","43895f41":"code","25c6ee0a":"code","73af855b":"code","0c2410ef":"code","77c262fd":"code","b45f80b8":"code","2a3f1f60":"code","47f6ee2e":"markdown","2723ded8":"markdown","bd76bff6":"markdown","50bb5b5f":"markdown","c43d18ac":"markdown","8cc0116b":"markdown","f7877089":"markdown","b396e844":"markdown","86c6aa33":"markdown","b08c67d6":"markdown"},"source":{"da5787a7":"X_train = np.load('..\/input\/train-test-split-of-open-images-dataset\/X_train.npy')\ny_train = np.load('..\/input\/train-test-split-of-open-images-dataset\/y_train.npy')\nX_val = np.load('..\/input\/train-test-split-of-open-images-dataset\/X_val.npy')\ny_val = np.load('..\/input\/train-test-split-of-open-images-dataset\/y_val.npy')","66a6fb1d":"L2 = 0.001\ndropout_rate = 0.5\nlearning_rate = 0.00001\noptimizer = Adam(learning_rate)\nhidden_dense = 2\nhidden_dropout = 2","182022ad":"import wandb\nfrom wandb.keras import WandbCallback\nwandb.init(config={\"L2\": L2, \"Learning Rate\":learning_rate, \"Optimizer\":optimizer, \"Hidden Dense-Layers\":hidden_dense\n                   , \"Dropout Rate\":dropout_rate, \"Hidden Dropout Layers\":hidden_dropout}, project='ResNet')","e06fd7ab":"base_model = ResNet50(input_shape = (240,240,3), include_top = False)\nbase_model.trainable = True","43895f41":"def ResNet():\n    inputs = Input(X_train.shape[1:])\n    \n    X = BatchNormalization(axis=3)(inputs, training=False)\n    X = base_model(X)\n    X = Flatten()(X)\n    X = Dense(512, activation='relu', trainable=True, kernel_regularizer=regularizers.l2(L2))(X)\n    X = Dropout(dropout_rate)(X)\n    X = Dense(512, activation='relu', trainable=True, kernel_regularizer=regularizers.l2(L2))(X)\n    X = Dropout(dropout_rate)(X)\n    X = Dense(29, activation='sigmoid', trainable=True)(X)\n    \n    model = Model(inputs, X, name='ResNet')\n    \n    return model","25c6ee0a":"ResNet = ResNet()\nResNet.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\nResNet.summary()","73af855b":"ResNet.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val), callbacks=[WandbCallback()])","0c2410ef":"X_test = np.load('..\/input\/train-test-split-of-open-images-dataset\/X_test.npy')\ny_test = np.load('..\/input\/train-test-split-of-open-images-dataset\/y_test.npy')","77c262fd":"y_pred = (ResNet.predict(X_test) > 0.5)\nmatrix = multilabel_confusion_matrix(y_test, y_pred)\nreport = classification_report(y_test, y_pred)\nprint(report)\n","b45f80b8":"ResNet.save('Final_Model')","2a3f1f60":"#[1'Toilet',2'Swimming pool',3'Bed',4'Billiard table',5'Sink',6'Fountain',7'Oven',8'Ceiling fan',9'Television',\n #10'Microwave oven',11'Gas stove',12'Refrigerator',13'Washing machine',14'Bathtub',15'Stairs',16'Fireplace',17'Pillow',18'Mirror',\n #19'Shower',20'Couch',21'Countertop',22'Coffeemaker',23'Dishwasher',24'Sofa bed',25'Tree house',26'Towel',27'Porch',28'Wine rack',29'Jacuzzi']","47f6ee2e":"#Import all the Tools and libraries you need. I use Tensorflow and Keras, but feel free to adjust the Notebook\n#to the Framework you prefer.\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n#import os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n #   for filename in filenames:\n  #      print(os.path.join(dirname, filename))\n\nfrom keras import layers\nfrom keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\nfrom keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\nfrom keras.models import Model\nfrom keras.preprocessing import image\n#from keras.utils import layer_utils\n#from keras.utils.data_utils import get_file\n#from keras.applications.imagenet_utils import preprocess_input\n#import pydot\n#from IPython.display import SVG\n#from keras.utils.vis_utils import model_to_dot\n#from keras.utils import plot_model\n#from kt_utils import *\n\nimport keras.backend as K\nK.set_image_data_format('channels_last')\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow\nfrom keras.applications.resnet import ResNet50\nfrom keras.applications import Xception\nfrom keras.optimizers import Adam\nfrom keras import regularizers\nfrom sklearn.metrics import multilabel_confusion_matrix, classification_report","2723ded8":"The last step is to evaluate the model, with your held out test set. You can calculate a confusion matrix as well as a classification report or some other metrics you find intereseting and helpful. I like the classification, because I feed like this gives me the most information in a very compact form. ","bd76bff6":"# What is Multi-Label Classification?\n\nThe goal of Multi-Label Classification is to predict the labels of images, but instead of just one single label an image can have multiple labels. One task could be to accurately predict if an image shows a table, chairs and maybe if there is some delicious food on the table. With normal Classification you would need 3 seperate models, instead with Multi-Label Classification you only need one. \n\nMulit-Label Classification is often confused with Multi-Class Classification. So just to clear things up: \"Normal\" Classification labels images in a binary way, like Cat vs No Cat. Multi-Class Classifcation can choose between many classes but can only assign one label, for example either Cat, Dog, Bird, or Fish. Multi-Label Classification on the other hand can also choose between many different classes, but can assign multiple labels and not just one like Multi-Class Classification. \n\nThere are actually not so many Notebooks on Kaggle covering the Topic of Multi-Label Classification, so I thought I could make an easy to understand Notebook with a very big Dataset as a basis for others (you) to play around. Have fun with it ;)","50bb5b5f":"# Build your own Neural Network around the Base Model\n\nHere you can just play around with the architecture. \n\nAs you can see my model begins with a BatchNorm Layer. This is because the standard way of normalizing the Input (deviding the arrays by 255) didn't work, because the Notebook ran out of RAM-space. In contrast, the BatchNorm layer needs very little RAM-space as the input is normalized in Mini-Batches of 32 examples and not everything at once.\n\nFollowing the Base model I added two Fully Connected layers followed by dropout-layers for normalization. The final output layer needs one node per class resulting in 29 nodes. It's very important that you use the Sigmoid Activation-function, not the Softmax-Function. If you would use Softmax, you would build a Multi-Class Classifier instead of a Multi-Label Classifier.","c43d18ac":"# Set the Hyperparameters\n\nIt is good practice to keep your Hyperparameters clean in one spot, so that you don't need to search them once you start tuning the Hyperparameters. ","8cc0116b":"When compiling the model, make sure to use the \"Binary_Crossentropy\" loss function. If you would use the Categorical Loss-function you again would build a Multi-Class Classifier.","f7877089":"# Optional Result-Tracking with Weights & Biases\n\nWeights & Biases is a tool to track the results of all your training runs, together with the used Hyperparamters and Model architecture organized in one spot. I won't go into too much detail. If you're interested, but don't know what Weights & Biases is I would recommend checking out their Website wandb.com or to watch some of their Videos on YouTube.\n\nIn this codeblock I fed a Dictionary to the config function, so that Weights & Biases tracks the Hyperparameters of each run. ","b396e844":"# Import a Base-Model with or without pretrained parameters\n\nKeras provides a quite large collection of ready to use models, for example pretrained on the ImageNet-Dataset. These ImageNet Models are built for Multi-Class Classification with 1,000 possible classes, so technically they are not built for Multi-Label Classification. This is no problem as the models are very easy to modify. \n\nI would recommend to use a model which doesn't take too much of memory capacity, because RAM is a very rare and thus valuable resource in Kaggle. I like to use ResNet50, but please try out some of the other Models included in Keras. Just follow this link: (insert link here)","86c6aa33":"# Import the Data\n\nThe Dataset consists of 16.500 images downloaded from Open Images V4 (instert link) and is stored in Numpy Arrays, split in a Train-, Val- and Test-split. Be aware that the Data is not normalized, meaning the pixel-values still range from 0 to 255, not from 0 to 1. \n\nThe arrays are set up as follows: (sample size, height, width, # of channels) so Dataset, if it weren't split, would have the follwing shape: (insert shape here, 240, 240, 3).\n\nBe aware that there are two conventions how to set up the Dataset-arrays. The other convention would be (sample size, # of channels, height, width). Both ways are totally fine. I personally use the first one and will also do in this Notebook, just because it is the default option in Keras and Tensorflow and I'm more used to it.","b08c67d6":"Now you need to fit your Model to the training data. You can feed in the validation data so Keras will calculate the accuracy on the val-set after each Epoch. Here you can also feed in the Callback to Weights & Biases if you want."}}