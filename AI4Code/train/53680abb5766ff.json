{"cell_type":{"6b836515":"code","bf98bb70":"code","0704be22":"code","75f866ba":"code","cce35912":"code","9b2268ec":"markdown","8d0fab3e":"markdown","538d35f7":"markdown","46818f95":"markdown"},"source":{"6b836515":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bf98bb70":"X_full = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv', index_col='Id')\nX_test_full = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv', index_col='Id')\n\nX_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X_full.SalePrice\nX_full.drop(['SalePrice'], axis=1, inplace=True)\n\ncategorical_cols = [cname for cname in X_full.columns if \n                    X_full[cname].dtype == \"object\"]\n\nnumerical_cols = [cname for cname in X_full.columns if \n                X_full[cname].dtype in ['int64', 'float64']]\n\nmy_cols = categorical_cols + numerical_cols\nX_train = X_full[my_cols].copy()\ny_train = y\nX_test = X_test_full[my_cols].copy()\n\nX_train.head()","0704be22":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n\nnumerical_transformer = SimpleImputer(strategy='constant')\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)\n])","75f866ba":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import GridSearchCV\n\nparameters = {\n    'model__n_estimators': (400, 450, 500),\n    'model__learning_rate': (0.1, 0.01, 0.001),\n    'model__max_depth': (3, 5, 7),\n}\n\nprint(\"Gradient Boosting Regressor Model\")\nprint(\"Potential Parameters:\")\nprint(parameters)\n\nmodel = GradientBoostingRegressor()\ntraining_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('model', model)]\n)\n\ngrid_search = GridSearchCV(training_pipeline, parameters, scoring='neg_mean_absolute_error')\ngrid_search.fit(X_train, y_train)\nprint(\"Best Parameters (MAE=%0.3f):\" % -grid_search.best_score_)\nprint(grid_search.best_params_)","cce35912":"preds_test = grid_search.predict(X_test)\noutput = pd.DataFrame({'Id': X_test.index,\n                       'SalePrice': preds_test})\noutput.to_csv('submission.csv', index=False)","9b2268ec":"Originally, I searched through `n_estimators` from 20 to 200 in increments of 20. The optimal number of estimators ended up being 200, which pointed in the direction that I should be searching above the range I have been looking into. I decided to concentrate on the number of estimators and find optimal value for the default parameters of `GradientBoostingRegressor`. This helped me zoom in on potential parameters that are close to optimal.","8d0fab3e":"# Housing Price Prediction with Gradient Boosting and Grid Search\nFirst of all, I loaded the data using Pandas. No columns were dropped and every column was classified as either categorical or numerical.","538d35f7":"The Gradient Boosting algorithm was chosen because it proves to be successful on tabular data. I performed Grid Search with three hyper-parameters: `n_estimators`, `learning_rate`, and `max_depth`.","46818f95":"Every categorical column have been encoded using one-hot encoding and the \"most frequent\" strategy was used for imputation of missing data. Numerical features were imputed with their mean averages."}}