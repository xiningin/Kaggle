{"cell_type":{"44897aaf":"code","3081a12c":"code","2349c518":"code","88ccb3b6":"code","ca2281a4":"code","a957ae9f":"code","468b7c65":"code","a2905320":"code","52924ea2":"code","021fd25e":"code","a07d7dcc":"code","e955d575":"code","2c09fb9b":"code","7cd27282":"code","7b4aaaba":"code","c3f19ac3":"code","4d763cb4":"markdown","cd37de15":"markdown","bfb7c214":"markdown","99882841":"markdown","44fce63b":"markdown","b1baa563":"markdown","7f1f3059":"markdown","a5402c27":"markdown","f437d2b1":"markdown"},"source":{"44897aaf":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3081a12c":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.metrics import classification_report, accuracy_score\n# tensorflow libraries\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten,Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import TensorBoard,EarlyStopping","2349c518":"# importing train and test dataset\ntrain = pd.read_csv(\"\/kaggle\/input\/sign-language-mnist\/sign_mnist_train\/sign_mnist_train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/sign-language-mnist\/sign_mnist_test\/sign_mnist_test.csv\")","88ccb3b6":"train","ca2281a4":"train.label.nunique()","a957ae9f":"train = train.values\ntest = test.values","468b7c65":"train_label = train[:,0]\ntrain_images = train[:,1:]\n\ntest_label = test[:,0]\ntest_images = test[:,1:]\n\n#changing shape of data\ntrain_images = train_images.reshape(train_images.shape[0], *(28, 28, 1))\ntest_images = test_images.reshape(test_images.shape[0],*(28,28,1))","a2905320":"# new shape of data for acceptance to CNN\nprint(train_images.shape)\nprint(test_images.shape)","52924ea2":"import random\n#Randomly check some image from the file\n#Run this block multiple times to see different images\ni = random.randint(1, 27456)\nplt.imshow(train[i, 1:].reshape(28,28))\nlabel = train[i, 0]\nprint(label)","021fd25e":"i = random.randint(1, 27456)\nplt.imshow(train[i, 1:].reshape(28,28))\nlabel = train[i, 0]\nprint(label)","a07d7dcc":"# defining training and testing ImageDataGenerator\ntrain_datagen = ImageDataGenerator(#preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,\n                                  rescale = 1.\/255,\n                                  rotation_range=20,\n                                  width_shift_range=0.2,\n                                  height_shift_range=0.2,\n                                  shear_range=0.2,\n                                  zoom_range=0.2,\n                                  horizontal_flip=True)\n\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\n\ntrain_gen = train_datagen.flow(train_images,train_label, \n                                batch_size=32\n                                                 )\ntest_gen = test_datagen.flow( test_images,test_label,\n                                                batch_size=32,\n                                               )","e955d575":"# defining Sequential model\nmodel = Sequential()\n\n#adding conv-pool layers\nmodel.add(Conv2D(filters=32,kernel_size=(3,3),strides=(1,1),activation=\"relu\",input_shape=(28, 28, 1)))\nmodel.add(Conv2D(filters=32, kernel_size=(3,3), strides=(1,1),activation=\"relu\"))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n\n#addin flatten layer\nmodel.add(Flatten())\n\n#adding dense layer\nmodel.add(Dense(512,activation=\"relu\"))\nmodel.add(Dropout(rate=0.3))\nmodel.add(Dense(64, activation=\"relu\"))\n\n#adding output layer\nmodel.add(Dense(25, activation=\"softmax\")) #softmax used for multiclass instead of sigmoid\n\n#compiling model\nmodel.compile(optimizer='adam', loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n\n#summary of model\nmodel.summary()","2c09fb9b":"history = model.fit(train_gen,validation_data=test_gen, epochs=30,verbose=1)","7cd27282":"acc = history.history[\"accuracy\"]\nloss = history.history[\"loss\"]\n\nval_acc = history.history[\"val_accuracy\"]\nval_loss = history.history[\"val_loss\"]\n\nplt.figure(figsize=(8,8))\nplt.subplot(2,1,1)\nplt.plot(acc,label=\"Training accuracy\")\nplt.plot(val_acc, label=\"Validation Accuracy\")\nplt.legend()\nplt.ylabel(\"Accuracy\", fontsize=12)\nplt.title(\"Training and Validation Accuracy\", fontsize=12)\nplt.show()","7b4aaaba":"plt.figure(figsize=(8,8))\nplt.subplot(2,1,1)\nplt.plot(loss, label=\"Training Loss\")\nplt.plot(val_loss, label = \"Validation Loss\")\nplt.legend()\nplt.ylabel(\"Loss\", fontsize=12)\nplt.title(\"Training and Validation Loss\",fontsize=12)\nplt.show()","c3f19ac3":"model.evaluate(test_gen, verbose=1)","4d763cb4":"![gesture.jpeg](attachment:5d5b2cfa-27d8-4600-828b-112efabee310.jpeg)","cd37de15":"# Converting dataframe into Array","bfb7c214":"# Accuracy and Loss","99882841":"# Data Augmentation\n* The Keras ImageDataGenerator class actually works by:\n\n1. Accepting a batch of images used for training.\n2. Taking this batch and applying a series of random transformations to each image in the batch (including random rotation, resizing, shearing, etc.).\n3. Replacing the original batch with the new, randomly transformed batch.\n4. Training the CNN on this randomly transformed batch (i.e., the original data itself is not used for training).","44fce63b":"![dataaug.png](attachment:a29b73a9-22ac-4ae2-a239-cc16693c1cde.png)","b1baa563":"# Importing Modules","7f1f3059":"# **Sign Language: CNN**","a5402c27":"Please UPVOTE if you find this notebook insightful!\n\nThanks in advance.","f437d2b1":"# CNN Model (Convolutional Neural Network):\n* A Convolutional Neural Network (ConvNet\/CNN) is a Deep Learning algorithm which can take in an input image, assign importance (learnable weights and biases) to various aspects\/objects in the image and be able to differentiate one from the other. \n\n![conv2d.png](attachment:2291cc97-81c1-43f1-b17b-0cd0e7648a97.png)"}}