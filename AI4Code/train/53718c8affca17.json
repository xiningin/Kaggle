{"cell_type":{"6d551de7":"code","6730f916":"code","92167460":"code","a3318562":"code","1b1e64fe":"markdown","9fc4d7e5":"markdown","dbfda6e9":"markdown","2b208550":"markdown"},"source":{"6d551de7":"from tensorflow.keras.layers import Activation, Dense, Input\nfrom tensorflow.keras.layers import Conv2D, Flatten\nfrom tensorflow.keras.layers import Reshape, Conv2DTranspose\nfrom tensorflow.keras.layers import LeakyReLU\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.models import load_model\nimport tensorflow as tf \nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nimport os\n# import argparse","6730f916":"def plot_images(generator,\n                noise_input,\n                show=False,\n                step=0,\n                model_name=\"gan\"):\n    \"\"\"Generate fake images and plot them\n    For visualization purposes, generate fake images\n    then plot them in a square grid\n    Arguments:\n        generator (Model): The Generator Model for \n            fake images generation\n        noise_input (ndarray): Array of z-vectors\n        show (bool): Whether to show plot or not\n        step (int): Appended to filename of the save images\n        model_name (string): Model name\n    \"\"\"\n    os.makedirs(model_name, exist_ok=True)\n    filename = os.path.join(model_name, \"%05d.png\" % step)\n    images = generator.predict(noise_input)\n    plt.figure(figsize=(2.2, 2.2))\n    num_images = images.shape[0]\n    image_size = images.shape[1]\n    rows = int(math.sqrt(noise_input.shape[0]))\n    for i in range(num_images):\n        plt.subplot(rows, rows, i + 1)\n        image = np.reshape(images[i], [image_size, image_size])\n        plt.imshow(image, cmap='gray')\n        plt.axis('off')\n    plt.savefig(filename)\n    if show:\n        plt.show()\n    else:\n        plt.close('all')","92167460":"def get_generator(inputs, image_size):\n    \"\"\"\n    stack of BN - ReLU - Conv2DTranspose to generate fake image\n    using Conv2DTranspose: to upsampling the image \n    \n    Argumments:\n        inputs: is the z-vector,\n        image_size: the target image\n        \n    Returns: \n        generator (Model)\n    \"\"\"\n    image_resize = image_size \/\/ 4\n    kernel_size =5\n    layer_filters = [128, 64, 32, 1]\n    #\n    x = Dense(image_resize * image_resize * layer_filters[0])(inputs)\n    x = Reshape((image_resize, image_resize, layer_filters[0]))(x)\n    #\n    for filters in layer_filters:\n        if filters > layer_filters[-2]:\n            strides = 2\n        else:\n            strides = 1\n        x = BatchNormalization()(x)\n        x = Activation('relu')(x)\n        x = Conv2DTranspose(filters = filters, kernel_size = kernel_size, strides = strides, padding = 'same')(x)\n    outputs = Activation('sigmoid')(x)\n    #\n    generator = Model(inputs, outputs, name='generator')\n    return generator\ndef get_discriminator(inputs):\n    \"\"\"\n    the discriminator is similar binary CNN classifiers. \n    which the input ia a image 28 x28 x 1. that is classified either real(1.0)\n    and fake(0.0)\n    \"\"\"\n    kernel_size = 5\n    layer_filters = [32, 64,128, 256]\n    #\n    x = inputs\n    for filters in layer_filters:\n        if filters == layer_filters[-1]:\n            strides = 1\n        else: \n            strides = 2\n        x = LeakyReLU(alpha = 0.2)(x)\n        x = Conv2D(filters = filters,\n            kernel_size = kernel_size,\n            strides = strides,\n            padding = 'same')(x)\n    x = Flatten()(x)\n    x = Dense(1)(x)\n    x = Activation('sigmoid')(x)\n    discriminator = Model(inputs, x, name = 'discriminator')\n    return discriminator\n        \ndef build_and_train_models():\n    # load data\n    (x_train, _),(_,_) = mnist.load_data()\n    image_size = x_train.shape[1]\n    x_train = x_train[..., tf.newaxis].astype(\"float32\")\/255.\n    model_name = \"dcgan_mnist\"\n    latent_size = 100\n    batch_size = 64\n    train_steps = 40000\n    lr = 2e-4\n    decay = 6e-8\n    input_shape = (image_size, image_size, 1)\n    #\n    inputs = Input(shape = input_shape, name = 'discriminator_input')\n    discriminator = get_discriminator(inputs)\n    #\n    # https:\/\/blog.paperspace.com\/intro-to-optimization-momentum-rmsprop-adam\/#:~:text=prone%20to%20overshooting)-,Adam,of%20both%20Momentum%20and%20RMSProp.\n    optimizer =RMSprop(lr = lr, decay = decay)\n    discriminator.compile(loss = 'binary_crossentropy',\n        optimizer = optimizer,\n        metrics = ['accuracy'])\n    discriminator.summary()\n    #\n    input_shape = (latent_size, )\n    inputs = Input(shape = input_shape, name ='z_input')\n    generator = get_generator(inputs, image_size)\n    generator.summary()\n    #\n    # build adversarial model \n    optimizer = RMSprop(lr = lr *0.5, decay = decay *0.5)\n    discriminator.trainable = False\n    #\n    adversarial = Model(inputs, discriminator(generator(inputs)), name = model_name)\n    adversarial.compile(loss = 'binary_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n    adversarial.summary()\n    #\n    models = (generator, discriminator, adversarial)\n    #\n    params = (batch_size, latent_size, train_steps, model_name)\n    train(models, x_train, params)\n    \ndef train(models, x_train, params):\n    generator, discriminator, adversarial = models\n    #\n    batch_size, latent_size,train_steps, model_name = params\n    save_interval = 500\n    #\n    noise_input = np.random.uniform(-1.0,1, size = [16, latent_size])\n    #\n    train_size = x_train.shape[0]\n    for i in range(train_steps):\n        rand_indexes = np.random.randint(0, train_size, size = batch_size)\n        real_images = x_train[rand_indexes]\n        #\n        noise = np.random.uniform(-1.0,1.0, size=[batch_size, latent_size])\n        #\n        fake_images = generator.predict(noise)\n        #\n        x = np.concatenate((real_images, fake_images))\n        #\n        y = np.ones([2*batch_size, 1])\n        y[batch_size:,:] = 0\n        #\n        loss, acc = discriminator.train_on_batch(x,y)\n        #\n        log = \"%d: [discriminator loss: %f, acc: %f]\" % (i, loss, acc)\n        print(log)\n        #\n        noise = np.random.uniform(-1.0,1.0, size = [batch_size, latent_size])\n        y = np.ones([batch_size, 1])\n        #\n        loss, acc = adversarial.train_on_batch(noise,y)\n        log = \"%s [adversarial loss: %f, acc: %f]\" % (log, loss, acc)\n        print(log)\n        #\n        if (i+1) %save_interval ==0:\n            plot_images(generator,noise_input = noise_input, show = True, step = i+1, model_name=model_name)\n        generator.save(model_name +\".h5\")\n        \n","a3318562":"build_and_train_models()","1b1e64fe":"#### [T2.01] DCGAN that is used to generate fake MNIST images\n* DCGAN that is used to generate fake MNIST images\n<img src=\"attachment:image.png\" alt=\"Girl in a jacket\" width=50% height=50%>\n* the discriminator can be trained by minimizing the loss function: $$L^{(D)}(\\theta^{(G)},\\theta^{(D)}) =-E_{x~P_{data}}logD(x)-E_zlog(1-D(G(z)))$$\n  * The loss is the negative sum of the expectation of correctly identifying real data, D(x). And the expectation of 1.0 minus correcly identifying synthetic data 1 -D(G(z))\n  * x - real data from the sampled data (x~ $p_{data}$) with a label 1.0\n  * x' = G(z) fake data from generator with label 0\n* The generator loss function is simply the negative of the discriminator loss function: $$L^{G}(\\theta^G,\\theta^D) = -L^D(\\theta^G,theta^D)$$\n  * why generator is negative of discriminator: https:\/\/stackoverflow.com\/questions\/51302915\/loss-function-of-generator-in-gans\n* the generator training critenrio can be written as a minimax problem: \n....","9fc4d7e5":"* Implementing DCGAN in Keras \n<img src=\"attachment:image.png\" alt=\"Girl in a jacket\" width=75% height=75%>\n* Generator following design principles: \n  * Use stride > 1, and convolution instead of MaxPooling2D or UpSampling2D with strides > 1, the CNN learns how to resize the feature maps\n  * Avoid using Dense, use CNN in all layers. Dense also utilized only the first layer of Generator and the output layer.\n  * use Batch Normalization \n  * the generator learns to generate fake image from 100-dim uniform distribution with a range of -1.0 to 1.0","dbfda6e9":"### [T1.0]Summary of mathematics of deep learning\n* MLP: multiplayer Perception \n* CNN: convolution neural net \n* RNN: Recurrent Neural net \n* non- linear activation functions: \n  * relu: $relu(x)= max(0,x)$\n    * use during back-propagation\n  * softplus: $shoftplus(x) = log(1+e^x)$\n  * elu: $elu(x,a) = \\left\\{\n  \\begin{array}{@{}ll@{}}\n    x, & \\text{if}\\ x>=0 \\\\\n    a(e^x-1), & \\text{otherwise}\n  \\end{array}\\right.$\n    * where: $a>=0$ and is a tuable hyperparameter\n  * sulu: $selu(x) = k x elu(x,a)$\n    * where: k = 1.0507.... and a = 1.6732\n  * sigmoid: $sigmoid(x)= \\frac{1}{1+e^{-x}}$\n  * tanh: $tanh(x) = \\frac{e^x-e^{-x}}{e^x+e^{-x}}$\n* To represent the output of NN: \n  * softmax: $sofmax(x_i) = \\frac{e^{x_i}}{\\sum_{i=0}^{N-1}e^{x_j}}$\n* Loss function:\n  * mean_squared_error: $\\frac{1}{categories}\\sum_{i=1}^{categories}(y_i - \\hat{y_i})^2$\n  * mean_absolute_error: $\\frac{1}{categories}\\sum_{i=1}^{categories}|(y_i - \\hat{y_i})|$  \n  * categorical_crossentropy: $-\\sum_{i=1}^{categories} y_ilog\\hat{y_i}$\n  * binary_crossentropy: $-\\frac{1}{N}\\sum_i^N[y_ilog(\\hat{y_i}) + (1-y_i)log(1-\\hat{y_i})]$  \n* Normalization:\n  * Batch normalization: \n    * Calculate the mean and variances of the layers input: \n      $$ Batch-mean-| => \\upsilon_B = \\frac{1}{m}\\sum_{i=1}^{m} x_i$$\n      $$ Batch-variance-|=>\\sigma_B^2 = \\frac{1}{m}\\sum_{i=1}^{m} (x_i - \\upsilon_B)^2$$ \n    * normalize the layer inputs\n      $$\\bar{x_i} = \\frac{x_i - \\upsilon_B}{\\sqrt{\\sigma^2+\\epsilon}}$$\n* Optimization:\n  * the idea is that if the loss is reduced to an acceptable level, the model has indirectly learned the fucntion that maps inputs to outputs\n    * SGD(stochastic gradient descent) (read more [backpropagation](http:\/\/neuralnetworksanddeeplearning.com\/chap2.html)  and force )\n    * Adam(Adaptive Moments)\n    * Root Mean Squared Propagation\n* LEARNING RATE METHODS: \n  * RMSprop:","2b208550":"### [T2.0] Working with Gain\n * step 1: [understand concept of 2 inputs - 1 output](https:\/\/www.kaggle.com\/vuonglam\/mnist-with-2-input-layers) \n * step 2: try to build audo - ending and decoding \n * step 3: build basic Gan - auto generator MNIST"}}