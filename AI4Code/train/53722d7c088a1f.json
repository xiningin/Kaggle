{"cell_type":{"0ab6220a":"code","904e8b69":"code","2e004ac6":"code","901ac8b8":"code","c07ef025":"code","179ad80c":"code","6a63aaf8":"code","7e121bd3":"code","427ca485":"code","8e7408db":"code","3ff3ba79":"code","433e7387":"code","57415796":"code","134bb981":"code","fa3649c6":"code","29dc9ac6":"code","ad61c899":"code","e1dd5013":"code","09854f9f":"code","7a17413b":"code","dfc053e4":"code","82e71c13":"code","357939c4":"code","cafd7447":"code","8cb99d32":"code","5231f7fd":"code","415a6dc5":"code","1322a708":"code","ad4c830f":"code","f9b081c5":"code","8d3abe3e":"code","e46994b2":"code","9cef852d":"code","b2fb6051":"code","ac9d4792":"code","7a52f76d":"code","f6b27ac7":"code","b0db2ab2":"code","05d750b0":"code","23a13b2d":"markdown","a74e29ec":"markdown","5f1e7bb3":"markdown","17ff37ec":"markdown","ec67113b":"markdown","d2138aea":"markdown","7fc09f03":"markdown","a4340595":"markdown","aa0a979c":"markdown","379d0da7":"markdown"},"source":{"0ab6220a":"import os\nimport datetime\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.set(style='darkgrid')\npd.set_option('display.float_format',lambda  x: '%.2f' %x)\n","904e8b69":"sales = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")\ntest = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv')\nitems = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/items.csv')\nitem_cats = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\nshops = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/shops.csv')","2e004ac6":"print('sales:',sales.shape,'test:',test.shape,'items:',items.shape,'item_cats:',item_cats.shape,'shop:',shops.shape)","901ac8b8":"sales['date'] = pd.to_datetime(sales['date'], format='%d.%m.%Y')","c07ef025":"sales.head(3)\n","179ad80c":"test.head(3)","6a63aaf8":"test.shape\n","7e121bd3":"items.head(3)","427ca485":"item_cats.head(3)","8e7408db":"shops.head(3)","3ff3ba79":"sales[sales['item_price']<=0]","433e7387":"sales[(sales.shop_id==32)&(sales.item_id==2973)&(sales.date_block_num==4)]","57415796":"median = sales[(sales.shop_id==32)&(sales.item_id==2973)&(sales.date_block_num==4)&\n               (sales.item_price>0)].item_price.median()","134bb981":"sales.loc[sales.item_price<0,'item_price'] =median","fa3649c6":"\ndataset = sales.pivot_table(index = ['shop_id','item_id'],values = ['item_cnt_day'],columns = ['date_block_num'],fill_value = 0,aggfunc='sum')","29dc9ac6":"dataset","ad61c899":"\ndataset.reset_index(inplace = True)","e1dd5013":"dataset.head()","09854f9f":"dataset.shape","7a17413b":"dataset = pd.merge(test,dataset,on = ['item_id','shop_id'],how = 'left')","dfc053e4":"dataset","82e71c13":"dataset.fillna(0,inplace = True)\n\ndataset.head()","357939c4":"dataset.drop(['shop_id','item_id','ID'],inplace = True, axis = 1)\ndataset.head()","cafd7447":"# X we will keep all columns execpt the last one \nX_train = np.expand_dims(dataset.values[:,:-1],axis = 2)\n# the last column is our label\ny_train = dataset.values[:,-1:]\n\n# for test we keep all the columns execpt the first one\nX_test = np.expand_dims(dataset.values[:,1:],axis = 2)\n\n# lets have a look on the shape \nprint(X_train.shape,y_train.shape,X_test.shape)","8cb99d32":"# importing libraries \nfrom keras.models import Sequential\nfrom keras.layers import LSTM,Dense,Dropout","5231f7fd":"my_model = Sequential()\nmy_model.add(LSTM(units = 64,input_shape = (33,1)))\nmy_model.add(Dropout(0.3)) #The dropout rate is set to 30%, meaning one in 3.33 inputs will be randomly excluded from each update cycle.\nmy_model.add(Dense(1))\n\nmy_model.compile(loss = 'mse',optimizer = 'adam', metrics = ['mean_squared_error'])\nmy_model.summary()","415a6dc5":"lst_pred = my_model.fit(X_train,y_train,batch_size = 4000,epochs = 8)","1322a708":"y_pred = my_model.predict(X_test)","ad4c830f":"from keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import GridSearchCV","f9b081c5":"\ndef build_classifier(optimizer):\n    grid_model = Sequential()\n    grid_model.add(LSTM(units = 64,input_shape = (33,1)))\n    grid_model.add(Dropout(0.4))\n    grid_model.add(Dense(1))\n\n    grid_model.compile(loss = 'mse',optimizer = optimizer, metrics = ['mean_squared_error'])\n    return my_model\n\ngrid_model = KerasClassifier(build_fn=build_classifier)\nparameters = {'batch_size' : [4000,4080],\n              'epochs' : [8,10],\n              'optimizer' : ['adam','Adadelta'] }\n\ngrid_search  = GridSearchCV(estimator = grid_model,\n                            param_grid = parameters,\n                            scoring = 'accuracy',\n                            cv = 2)\n","8d3abe3e":"grid_search = grid_search.fit(X_train,y_train)","e46994b2":"\nbest_parameters = grid_search.best_params_\nbest_accuracy = grid_search.best_score_","9cef852d":"# Let us check our best parameters\nbest_parameters","b2fb6051":"# Let us check our best accuracy got through grid search\nbest_accuracy","ac9d4792":"plt.plot(lst_pred.history['loss'], label= 'loss(mse)')\nplt.plot(np.sqrt(lst_pred.history['mean_squared_error']), label= 'rmse')\nplt.legend(loc=1)","7a52f76d":"from sklearn.metrics import mean_squared_error\nimport math","f6b27ac7":"# calculate root mean squared error\ntrainScore = math.sqrt(mean_squared_error(y_train, y_pred[:,0]))\nprint('Train Score: %.2f RMSE' % (trainScore))","b0db2ab2":"# creating submission file \nsubmission_pfs = my_model.predict(X_test)\n# we will keep every value between 0 and 20\nsubmission_pfs = submission_pfs.clip(0,20)\n# creating dataframe with required columns \nsubmission = pd.DataFrame({'ID':test['ID'],'item_cnt_month':submission_pfs.ravel()})\n# creating csv file from dataframe\n","05d750b0":"submission.to_csv('sub_pfs.csv',index = False)","23a13b2d":"Reset our indices, so that data should be in way we can easily manipulate","a74e29ec":"Change date form to DDMMYYYY","5f1e7bb3":"### Grid search\n\nFor simplicity we are keeping less parameters.","17ff37ec":"**Pivot Table **","ec67113b":"we want get total count value of an item over the whole month for a shop. \nThat why we made shop_id and item_id our indices and date_block_num our column \nthe value we want is item_cnt_day and used sum as aggregating function ","d2138aea":"Merge our pivot table with the test_data because we want to keep the data for prediction","7fc09f03":"Drop shop_id and item_id because we do not need them","a4340595":"**Load Data**","aa0a979c":"Lets fill all NaN values with 0","379d0da7":"**Import Libraries**"}}