{"cell_type":{"61805af8":"code","b24c476a":"code","b93e5005":"code","a40c88b3":"code","a7af0bb0":"code","1f905711":"code","612f78ec":"code","8f0b8958":"code","fd976309":"code","025c3d16":"code","169477c7":"code","7025fcc4":"code","044fd30d":"code","b031ff81":"code","3c8a5a69":"code","167ae33f":"code","b789c449":"code","050bcd24":"code","c9013a66":"code","f34839f0":"code","3a76c080":"code","24908447":"code","50486dd0":"code","1e5d60a2":"code","88103bcd":"code","a9fce3ed":"code","041beee2":"code","a575dea8":"code","e472cace":"code","5609b6ed":"code","80eeec6a":"code","c676848d":"code","7f781964":"code","861a8e24":"code","ce4dbbf0":"code","af71b7e7":"code","b1e1c982":"code","42718435":"code","27f69317":"code","a23af13c":"code","c1e66ce3":"code","7bff16fe":"code","48edff1e":"code","527fd45c":"code","29dd0d58":"code","77f93508":"code","9e8e0ef2":"code","84f6c9fb":"code","ab0c9373":"code","84b8c8e4":"code","eafd7540":"code","31b9832b":"code","1f285be7":"code","2cefcd7c":"code","d11292bb":"code","b1eaf986":"code","0421803a":"code","7380ee15":"markdown","a789897a":"markdown","347b20f1":"markdown","2cba47cc":"markdown","0da46e4d":"markdown","89067c83":"markdown","0d921bbb":"markdown","840f4f61":"markdown","c1dc3c08":"markdown","db9850ac":"markdown","3cafdab0":"markdown","857d492d":"markdown","b174b4a6":"markdown","3d4e6301":"markdown","2bed1cb7":"markdown","507c0449":"markdown"},"source":{"61805af8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b24c476a":"#loading the required libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score\n%matplotlib inline","b93e5005":"#loading the dataset\ntraindf = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntestdf = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","a40c88b3":"#first 5 rows of train dataset\ntraindf.head()","a7af0bb0":"#first 5 rows of test dataset\ntestdf.head()","1f905711":"#row and column size of train dataset\ntraindf.shape","612f78ec":"#row and column size of test dataset\ntestdf.shape","8f0b8958":"traindf.info","fd976309":"traindf.dtypes","025c3d16":"#number of missing values in train dataset\n#pd.options.display.max_rows= 81\ntraindf.isnull().sum().sum()","169477c7":"#number of missing values in test dataset\ntestdf.isnull().sum().sum()","7025fcc4":"#percentage of missing values per column in train dataset\npd.options.display.max_rows= 81\nmissing_percentage = traindf.isnull().sum()\/len(traindf)*100\nmissing_percentage","044fd30d":"#percentage of missing values per column in train dataset\npd.options.display.max_rows= 81\nmissing_percentage = testdf.isnull().sum()\/len(testdf)*100\nmissing_percentage","b031ff81":"#columns with missing values in train dataset\nmissing_columns_train = traindf.loc[:,traindf.isnull().any()].columns\nmissing_columns_train","3c8a5a69":"#dropping the columns with more than 60% missing values in train dataset\ntraindf.dropna(thresh = len(traindf)*0.4 ,axis = 1,inplace=True)","167ae33f":"#dropping the columns with more than 60% missing values in test dataset\ntestdf.dropna(thresh = len(testdf)*0.4 ,axis = 1,inplace=True)","b789c449":"traindf.shape","050bcd24":"#re-checking the columns with missing values in train dataset\nmissing_value_df_train = traindf.loc[:,traindf.isnull().any()]\nmissing_columns_train = missing_value_df_train.columns\nmissing_columns_train","c9013a66":"#re-checking the columns with missing values in test dataset\nmissing_value_df_test = testdf.loc[:,testdf.isnull().any()]\nmissing_columns_test = missing_value_df_test.columns\nmissing_columns_test","f34839f0":"#datatypes of columns with missing values in train dataset\nmissing_value_df_train.dtypes","3a76c080":"#percentage of missing values in columns with missing values in train dataset\nmissing_value_df_train.isnull().sum()\/len(missing_value_df_train)*100","24908447":"#percentage of missing values in columns with missing values in test dataset\nmissing_value_df_test.isnull().sum()\/len(missing_value_df_test)*100","50486dd0":"#FireplaceQu column elements\nmissing_value_df_train.FireplaceQu","1e5d60a2":"sns.violinplot(traindf[\"FireplaceQu\"],traindf[\"SalePrice\"])","88103bcd":"print(\"Number of missing values in LotFrontage column in train dataset is :\",traindf[\"LotFrontage\"].isnull().sum())","a9fce3ed":"print(\"Number of missing values in LotFrontage column in test dataset is :\",testdf[\"LotFrontage\"].isnull().sum())","041beee2":"sns.distplot(traindf[\"LotFrontage\"])","a575dea8":"import random as rand\n#treating missing values in LotFrontage column in train dataset\nlot_mean = traindf[\"LotFrontage\"].mean()\nlot_sd = traindf[\"LotFrontage\"].std()\nrandom_numbers = list()\nfor i in range(259):\n    random_numbers.append(rand.uniform(lot_mean-3*lot_sd,lot_mean+3*lot_sd))\n#imputing missing values of LotFrontage column with random_numbers with values between 3 sample sd's of LotFrontage column\ntraindf.loc[traindf[\"LotFrontage\"].isnull(),\"LotFrontage\"] = random_numbers","e472cace":"sns.distplot(testdf[\"LotFrontage\"])","5609b6ed":"import random as rand\n#treating missing values in LotFrontage column in test data\nlot_mean = testdf[\"LotFrontage\"].mean()\nlot_sd = testdf[\"LotFrontage\"].std()\nrandom_numbers1 = list()\nfor i in range(227):\n    random_numbers1.append(rand.uniform(lot_mean-3*lot_sd,lot_mean+3*lot_sd))\n#imputing missing values of LotFrontage column with random_numbers with values between 3 sample sd's of LotFrontage column in test dataset\ntestdf.loc[testdf[\"LotFrontage\"].isnull(),\"LotFrontage\"] = random_numbers1","80eeec6a":"testdf.shape","c676848d":"#filling missing values of GarageYrBlt column with the column's median value in train dataset.\ntraindf[\"GarageYrBlt\"].fillna(traindf[\"GarageYrBlt\"].median(),inplace=True)\n#filling missing values of MasVnrArea column with the column's mean value in train dataset.\ntraindf[\"MasVnrArea\"].fillna(traindf[\"MasVnrArea\"].mean(),inplace=True)","7f781964":"#datatypes of columns with missing values in train dataset\nmissing_value_df_test.dtypes","861a8e24":"impute_mean = [\"MasVnrArea\",\"BsmtFinSF1\",\"BsmtFinSF2\",\"BsmtUnfSF\",\"TotalBsmtSF\",\"GarageArea\"]\nimpute_mode = [\"BsmtFullBath\",\"BsmtHalfBath\",\"GarageCars\"]\n#filling missing values of GarageYrBlt column with the column's median value in test dataset.\ntestdf[\"GarageYrBlt\"].fillna(traindf[\"GarageYrBlt\"].median(),inplace=True)\n#filling missing values of columns in impute_mean with each of the column's mean value in dataset.\nfor columns in impute_mean:\n    testdf[columns].fillna(traindf[columns].mean(),inplace=True)\nfor columns in impute_mode:\n    testdf[columns].fillna(traindf[columns].mode(),inplace=True)","ce4dbbf0":"missing_value_df_train = traindf.loc[:,traindf.isnull().any()]\nmissing_value_columns_train = list(missing_value_df_train.columns)\n#prints reamining missing value columns except FireplaceQu\nmissing_value_columns1_train = [x for x in missing_value_columns_train if x!= \"FireplaceQu\"]\nmissing_value_columns1_train","af71b7e7":"missing_value_df_test = testdf.loc[:,testdf.isnull().any()]\nmissing_value_columns_test = list(missing_value_df_test.columns)\n#prints reamining missing value columns except FireplaceQu\nmissing_value_columns1_test = [x for x in missing_value_columns_test if x!= \"FireplaceQu\"]\nmissing_value_columns1_test","b1e1c982":"#replacing missing value columns with their respective modes except the FireplaceQu column in train dataset\nfor columns in missing_value_columns1_train:\n    traindf[columns].fillna(traindf[columns].mode()[0],inplace=True)\ntraindf.isnull().sum()","42718435":"#replacing missing value columns with their respective modes except the FireplaceQu column in test dataset\nfor columns in missing_value_columns1_test:\n    testdf[columns].fillna(testdf[columns].mode()[0],inplace=True)\ntestdf.isnull().sum()","27f69317":"from sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.compose import ColumnTransformer\n#selecting categorical columns to encode from train dataset\ncat_columns_train = list(traindf.select_dtypes(\"object\").columns)\n#as the column FireplaceQu has NaN values so it cannot be imputed directly, so we are removing it from the list\ncat_columns_train = [x for x in cat_columns_train if x!= \"FireplaceQu\"]\n#setting up the imputer\ntransformer = ColumnTransformer(transformers=[(\"L\",OrdinalEncoder(),cat_columns_train)],remainder='passthrough')\ncat_columns_imputed_train = transformer.fit_transform(traindf[cat_columns_train])\n#pasting the imputed values in the each of the categorical columns of the original train dataset\ntraindf[cat_columns_train] = cat_columns_imputed_train","a23af13c":"#selecting categorical columns to encode from test dataset\ncat_columns_test = list(testdf.select_dtypes(\"object\").columns)\n#as the column FireplaceQu has NaN values so it cannot be imputed directly, so we are removing it from the list\ncat_columns_test = [x for x in cat_columns_test if x!= \"FireplaceQu\"]\n#setting up the imputer\ntransformer = ColumnTransformer(transformers=[(\"L\",OrdinalEncoder(),cat_columns_test)],remainder='passthrough')\ncat_columns_imputed_test = transformer.fit_transform(testdf[cat_columns_test])\n#pasting the imputed values in the each of the categorical columns of the original test dataset\ntestdf[cat_columns_test] = cat_columns_imputed_test","c1e66ce3":"#encoding the non-null values of FireplaceQu column in train dataset\nencoder = OrdinalEncoder()\nnon_null_train = traindf[\"FireplaceQu\"].dropna()\nencoded_train = encoder.fit_transform(np.array(non_null_train).reshape(-1,1))\ntraindf.loc[traindf[\"FireplaceQu\"].notnull(),\"FireplaceQu\"] = np.squeeze(encoded_train)","7bff16fe":"#encoding the non-null values of FireplaceQu column in test dataset\nencoder = OrdinalEncoder()\nnon_null_test = testdf[\"FireplaceQu\"].dropna()\nencoded_test = encoder.fit_transform(np.array(non_null_test).reshape(-1,1))\ntestdf.loc[testdf[\"FireplaceQu\"].notnull(),\"FireplaceQu\"] = np.squeeze(encoded_test)","48edff1e":"#KNN imputing the FireplaceQu column of train dataset\n!pip install fancyimpute\nfrom fancyimpute import KNN\n##KNN imputing the FireplaceQu column of train dataset\nimputer = KNN(k=5)\nFireplaceQu_imputed = imputer.fit_transform(np.array(traindf[\"FireplaceQu\"]).reshape(-1,1))\ntraindf[\"FireplaceQu\"] = FireplaceQu_imputed","527fd45c":"#KNN imputing the FireplaceQu column of test dataset\nimputer = KNN(k=5)\nFireplaceQu_imputed = imputer.fit_transform(np.array(testdf[\"FireplaceQu\"]).reshape(-1,1))\ntestdf[\"FireplaceQu\"] = FireplaceQu_imputed","29dd0d58":"#checking if Id column is relevant\ntraindf[\"Id\"].corr(traindf[\"SalePrice\"])","77f93508":"traindf.drop(\"Id\",axis=1,inplace=True)\ntestdf.drop(\"Id\",axis=1,inplace=True)","9e8e0ef2":"#Setting predictior and target Variables\nX = traindf.iloc[:,:75]\ny = traindf[\"SalePrice\"]","84f6c9fb":"#splitting the dataset in train and test sets\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)","ab0c9373":"#fitting decision tree model and checking accuracy\nmodel0 = DecisionTreeRegressor()\nmodel0.fit(X_train,y_train)\ny_pred = model0.predict(X_test)\nmae_0 = mean_absolute_error(y_test,y_pred)\nrmse_0 = np.sqrt(mean_squared_error(y_test,y_pred))\nr_squared_0 = r2_score(y_test,y_pred)\nprint(\"Root mean square error : \",rmse_0)\nprint(\"Mean absolute error : \",mae_0)\nprint(\"r squared : \",r_squared_0)","84b8c8e4":"#fitting Random Forest model and checking accuracy\nmodel1 = RandomForestRegressor(n_estimators=100,random_state=2)\nmodel1.fit(X_train,y_train)\ny_pred = model1.predict(X_test)\nmae_1 = mean_absolute_error(y_test,y_pred)\nrmse_1 = np.sqrt(mean_squared_error(y_test,y_pred))\nr_squared_1 = r2_score(y_test,y_pred)\nprint(\"Root mean square error : \",rmse_1)\nprint(\"Mean absolute error : \",mae_1)\nprint(\"r squared : \",r_squared_1)","eafd7540":"#fitting Linear Regression model and checking accuracy\nmodel2 = LinearRegression()\nmodel2.fit(X_train,y_train)\ny_pred = model2.predict(X_test)\nmae_2 = mean_absolute_error(y_test,y_pred)\nrmse_2 = np.sqrt(mean_squared_error(y_test,y_pred))\nr_squared_2 = r2_score(y_test,y_pred)\nprint(\"Root mean square error : \",rmse_2)\nprint(\"Mean absolute error : \",mae_2)\nprint(\"r squared : \",r_squared_2)","31b9832b":"#fitting the XGBoost Regressor model\nfrom xgboost import XGBRegressor\nmodel3 = XGBRegressor(eta=0.1)\nmodel3.fit(X_train,y_train)\ny_pred = model3.predict(X_test)\nmae_3 = mean_absolute_error(y_test,y_pred)\nrmse_3 = np.sqrt(mean_squared_error(y_test,y_pred))\nr_squared_3 = r2_score(y_test,y_pred)\nprint(\"Root mean square error : \",rmse_3)\nprint(\"Mean absolute error : \",mae_3)\nprint(\"r squared : \",r_squared_3)","1f285be7":"#storing the scores in a dataframe to plot a graph\nplotdf = pd.DataFrame({\"Models\":[\"Decision Tree\",\"Random Forest\",\"Linear Regression\",\"XGBoost Regressor\"],\n                      \"RMSE\":[rmse_0,rmse_1,rmse_2,rmse_3],\"MAE\":[mae_0,mae_1,mae_2,mae_3],\n                      \"R-Squared\":[r_squared_0,r_squared_1,r_squared_2,r_squared_3]})\nplotdf","2cefcd7c":"#representing the results with graph\nplt.figure(figsize=(20,10))\nplt.subplot(131)\nsns.barplot(x=\"Models\",y=\"RMSE\",data=plotdf)\nplt.title(\"RMSE Scores of Different Models\",size=20)\nplt.xticks(rotation=45,size=10)\nplt.subplot(132)\nsns.barplot(x=\"Models\",y=\"MAE\",data=plotdf)\nplt.title(\"MAE Scores of Different Models\",size=20)\nplt.xticks(rotation=45,size=10)\nplt.subplot(133)\nsns.barplot(x=\"Models\",y=\"R-Squared\",data=plotdf)\nplt.title(\"R-Squared Scores of Different Models\",size=20)\nplt.xticks(rotation=45,size=10)","d11292bb":"#predicting the result using test dataset\ny_pred_test = model3.predict(testdf)\ny_pred_test","b1eaf986":"#storing the result\nsubmission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nsubmission[\"SalePrice\"] = y_pred_test","0421803a":"#saving the file\nsubmission.to_csv(\"submission.csv\",index=False)","7380ee15":"The count of missing values of the column in LotFrontage column is high for both the train and test dataset, we will impute the values with values within 3 standard deviation's from the mean of the available values of the column.","a789897a":"As, the distribution of the attribute LotFrontage is approximiately normal so we can impute the missing values using the empirical rule of normal distribution which states 99.7% of population values lies within three standard deviations away from mean.","347b20f1":"# Missing Value Imputaion and Categorical Features Encoding","2cba47cc":"Checking the test dataset we see that the columns \"BsmtFullBath\",\"BsmtHalfBath\",\"GarageCars\" represents categorical features which are better taken care of by replacing with mode and the columns \"MasVnrArea\",\"BsmtFinSF1\",\"BsmtFinSF2\",\"BsmtUnfSF\",\"TotalBsmtSF\",\"GarageArea\" represnts area.So it is better to impute them by their mean.","0da46e4d":"# Model Fitting","89067c83":"All the missing values are imputed and all the categorical columns are encoded. Now, we can fit the models.","0d921bbb":"The attribute LotFrontage also follows normal distribution in testdf, so we can do the same thing once again for missing values of LotFrontage attribute of testdf.","840f4f61":"Dropping the Id column in both train and test dataset as they are irrelevant.","c1dc3c08":"We can see the XGBoost Regressor model performed best having the lowest MAE and RMSE values along with highest R-Squared value of 0.92.\nSo now we are going to predict the House Pricings on the test data using the XGBoost model and save the result.","db9850ac":"# Exploration of The Datasets","3cafdab0":"The column \"GarageYrBlt\" refers to the year when Garage of a particular house was built. It is better to replace the missing values of that column with median value of the available values of the column as it is referring to a Year.\n\nThe column \"MasVnrArea\" is referring to Masonry veneer area in square feet. As, the missing values are refering to area so it will be better to replace them with mean of the available values from that column.\n","857d492d":"# Visualization of Accuracy Scores of Different Models","b174b4a6":"Now, all the missing value columns with object datatypes are left to be treated. We are going to replace the missing values with mode for all these columnn except the FireplaceQu column as we are going to run a classification model to predict this column.","3d4e6301":"# Final Prediction Using Test Dataset","2bed1cb7":"The violin plot shows different categories differetly distributed when compared with Saleprice so this feature must be useful and cannot be dropped and as, 47.26% rows of the column FireplaceQu are missing, so we will predict the column using suitable classification algorithm.","507c0449":"Features with too low or too high correlation is not relevant."}}