{"cell_type":{"61f27a50":"code","7d3144f2":"code","cd1ac810":"code","c75a3f5b":"code","ac5ea87a":"code","b011a098":"code","55e58d07":"code","a1031ec5":"code","6bd2101c":"code","faa99118":"code","018b4016":"code","73092ba1":"code","c0e13aad":"code","10bfccd2":"code","2fedf4a5":"code","c16544c9":"code","2bd28a81":"code","debc7656":"code","b00c4cbb":"code","abc4f137":"code","18da56a4":"code","321da542":"code","cc98c643":"code","76c846f4":"code","2409bea4":"code","433863a9":"code","482d7000":"code","b0bea61a":"code","cfdb1593":"code","0dc799f4":"code","e4bf2dfa":"code","4862666b":"code","6c6f4290":"code","fda5bae2":"code","9925703c":"code","dad4dacc":"code","986299a7":"code","b14abfd3":"code","f044b9b9":"markdown","35dc7cb9":"markdown","06f42aeb":"markdown","ceb5bf87":"markdown","c3992f36":"markdown","d4a2a7de":"markdown","ebaca069":"markdown","73f67171":"markdown","c4f91ec1":"markdown","fad6993b":"markdown","0c94a0ab":"markdown","dbd72a00":"markdown","16e8929f":"markdown","b6176345":"markdown","446e73ff":"markdown","d6e2755a":"markdown","9ec811d1":"markdown","4c47c4f4":"markdown","ec185fd1":"markdown","d64a356e":"markdown","74db1cfc":"markdown","bab0763b":"markdown","d3f202d8":"markdown","f4444051":"markdown","9b49a832":"markdown","b78a059f":"markdown","73e212b6":"markdown","88d94d6a":"markdown","910a3e71":"markdown","ed98e448":"markdown","4a655370":"markdown","73196da5":"markdown","6e6332a7":"markdown","3497d986":"markdown","f28c5c74":"markdown","09c86edf":"markdown","742422ca":"markdown","f326f71e":"markdown","742eec90":"markdown","99db9b67":"markdown","bfc1f860":"markdown","d42d008d":"markdown","6a72f79f":"markdown","adc9bde2":"markdown","39d8e37c":"markdown","2200e1a7":"markdown","e204f041":"markdown","5610da25":"markdown","01df84fb":"markdown","db366a2e":"markdown","f69d6244":"markdown","21884d2c":"markdown","e92c29b2":"markdown","33f4d54a":"markdown","1dac24bd":"markdown","08bf6a2d":"markdown","c2a84345":"markdown","77c5c0f8":"markdown"},"source":{"61f27a50":"import pandas as pd\nimport numpy as np\nfrom lightgbm import LGBMClassifier\nimport scipy\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.impute import SimpleImputer\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import make_pipeline\nfrom imblearn.combine import SMOTEENN\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.feature_selection import SelectFromModel\nimport time\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly.offline import init_notebook_mode, iplot\nfrom plotly.figure_factory import create_table\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly\nfrom plotly import tools\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\ninit_notebook_mode(connected=True)\npd.set_option('display.max_columns', 100)\nfrom plotly.figure_factory import create_table\ndf_eda = pd.read_csv(\"..\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv\").drop(columns = ['enrollee_id'])\n","7d3144f2":"df_eda.describe().T.round(3)","cd1ac810":"df_eda.describe(include=['O']).T","c75a3f5b":"df_eda[\"target\"] = df_eda[\"target\"].apply(lambda x: \"Yes\" if x == 1 else  \"No\")","ac5ea87a":"trace0 = go.Histogram(\n    x=df_eda.loc[df_eda['target'] == 'No']['city_development_index'], name='Does not want to change',\n    opacity=0.55\n)\ntrace1 = go.Histogram(\n    x=df_eda.loc[df_eda['target'] == 'Yes']['city_development_index'], name='Wants to change',\n    opacity=0.55\n)\n\ndata = [trace0, trace1]\nlayout = go.Layout(barmode='overlay', title='City development index distribution', template = \"plotly_white\")\nfig = go.Figure(data=data, layout=layout)\n\nfig.update_layout(\n    title=\"Distribution of data scientists in cities with different development index\",\n    xaxis_title=\"City development index\",\n    yaxis_title=\"Count\",\n    font=dict(\n        family=\"Segoe UI\",\n        size=13\n    )\n)\n\niplot(fig)","b011a098":"def plot_bars(x):\n    df1 = df_eda[[x,\"target\"]]\n    df1[\"count\"] = 1\n    df1 = df1.groupby([x,\"target\"]).sum().reset_index()\n\n    fig = px.bar(\n        df1, \n        y=x, \n        x=\"count\", \n        color=\"target\", \n        title=\"Data scientists by \" + x, \n        template = \"plotly_white\",  \n        width=700, \n        height = 500, \n        color_discrete_sequence=px.colors.qualitative.Vivid_r,\n    )\n\n    fig.update_layout(\n        barmode='stack', \n        yaxis={'categoryorder':'total ascending'},\n        xaxis={'categoryorder':'total descending'},\n        xaxis_title=\"\",\n        yaxis_title=\"\",\n        font=dict(\n        family=\"Segoe UI\",\n        size=13\n    )\n    )\n    if x == \"experience\":\n        fig.update_layout(yaxis={'categoryorder':'array', 'categoryarray':['<1','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20','>20']},\n        font=dict(\n        family=\"Segoe UI\",\n        size=10\n    ))\n    elif x == \"company_size\":\n        fig.update_layout(yaxis={'categoryorder':'array', 'categoryarray':['<10','10\/49','50-99','100-500','500-999','1000-4999','5000-9999','10000+']})\n    elif x == \"last_new_job\":\n        fig.update_layout(yaxis={'categoryorder':'array', 'categoryarray':['never','1','2','3','4','>4']})\n\n    fig.show()","55e58d07":"for x in df_eda.describe(include=['O']).T.index[1:-1]:\n    plot_bars(x) ","a1031ec5":"df1 = df_eda[[\"city\",\"target\"]]\ndf1[\"count\"] = 1\ndf1 = df1.groupby([\"city\",\"target\"]).sum().reset_index()\nfig = px.bar(df1, x=\"city\", y=\"count\", color=\"target\", title=\"Data scientists by \" + \"city\", template = \"plotly_white\", color_discrete_sequence=px.colors.qualitative.Vivid_r)\nfig.update_layout(barmode='stack', xaxis={'categoryorder':'total descending'},\n    xaxis_title=\"\",\n    yaxis_title=\"\",\n    font=dict(\n        family=\"Segoe UI\",\n        size=13\n    )\n)\nfig.show()\n","6bd2101c":"df = pd.read_csv(\"..\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv\")\ntest = pd.read_csv(\"..\/input\/hr-analytics-job-change-of-data-scientists\/aug_test.csv\")\ndf = df.append(test)","faa99118":"df[\"city\"] = df[\"city\"].apply(lambda x: \"Other\" if len(df[df.city.eq(x)]) < 300 else x)\ncity = pd.DataFrame(df[\"city\"].value_counts())","018b4016":"city = pd.DataFrame(df[\"city\"].value_counts())\nprint(city)","73092ba1":"df.replace(to_replace = 'Has relevent experience',value = '1',inplace = True)\ndf.replace(to_replace = 'No relevent experience',value='0',inplace = True )\n\ndf.replace(to_replace = '<1',value = '0',inplace = True)\ndf.replace(to_replace = '>20',value = '21',inplace=True)\ndf.replace(to_replace = 'never',value = '0',inplace=True)\ndf.replace(to_replace = '>4',value = '5',inplace=True)\n\ndf['company_size'].replace(to_replace = '<10',value = '0',inplace = True)\ndf['company_size'].replace(to_replace = '10\/49',value = '1',inplace = True)\ndf['company_size'].replace(to_replace = '50-99',value = '2',inplace = True)\ndf['company_size'].replace(to_replace = '100-500',value = '3',inplace = True)\ndf['company_size'].replace(to_replace = '500-999',value = '4',inplace = True)\ndf['company_size'].replace(to_replace = '1000-4999',value = '5',inplace = True)\ndf['company_size'].replace(to_replace = '5000-9999',value = '6',inplace = True)\ndf['company_size'].replace(to_replace = '10000+',value = '7',inplace = True)\n\n\ndf.replace(to_replace = 'Primary School',value = '0',inplace=True)\ndf.replace(to_replace = 'High School',value = '1',inplace=True)\ndf.replace(to_replace = 'Graduate',value = '2',inplace=True)\ndf.replace(to_replace = 'Masters',value = '3',inplace=True)\ndf.replace(to_replace = 'Phd',value = '4',inplace=True)\n","c0e13aad":"df['relevent_experience'] = df['relevent_experience'].astype(float).astype(\"Int64\")\ndf['experience'] = df['experience'].astype(float).astype(\"Int64\")\ndf['last_new_job'] = df['last_new_job'].astype(float).astype(\"Int64\")\ndf['company_size'] = df['company_size'].astype(float).astype(\"Int64\")\ndf['education_level'] = df['education_level'].astype(float).astype(\"Int64\")\ndf['training_hours'] = df['training_hours'].astype(float).astype(\"Int64\")\n","10bfccd2":"missing = pd.DataFrame(df.isna().sum().reset_index()).rename(columns={0: \"Missing\"})\nmissing = missing.loc[missing['Missing'] > 1]\nmissing['Missing'] = missing['Missing'].apply(lambda x: (x\/len(df))*100)\n\nfig = px.bar(\n    missing, \n    y=\"index\", \n    x=\"Missing\", \n    template = \"plotly_white\",\n    width=700, \n    height=400, \n    color_discrete_sequence=px.colors.qualitative.Vivid\n    )\n\nfig.update_layout(\n    title=\"Percentage of missing values\",\n    xaxis_title=\"\",\n    yaxis_title=\"\",\n    yaxis={'categoryorder':'total ascending'},\n    font=dict(\n        family=\"Segoe UI\",\n        size=13\n    )\n)\n\nfig.show()","2fedf4a5":"num_imputer = SimpleImputer()\ncat_imputer = SimpleImputer(strategy = \"most_frequent\")\ndf_num = ['city_development_index', 'relevent_experience', 'education_level', 'experience', 'company_size', 'last_new_job', 'target','training_hours']\ndf_cat = ['city', 'gender', 'enrolled_university', 'major_discipline', 'company_type',]\ndf[df_num] = num_imputer.fit_transform(df[df_num])\ndf[df_cat]= cat_imputer.fit_transform(df[df_cat])","c16544c9":"df = pd.get_dummies(df, columns = df_cat)\ndf.shape","2bd28a81":"X = df.drop(columns = ['target', 'enrollee_id'])\nfeature_names = X.columns\ny = df.target\nX = X.head(19158)\nX_sub = X.tail(2129)\ny = y.head(19158)","debc7656":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42)","b00c4cbb":"cv = StratifiedKFold(n_splits = 8)","abc4f137":"lgbm = LGBMClassifier().fit(X_train,y_train)\ny_pred = lgbm.predict(X_test)\nprint(classification_report(y_test,y_pred))\nplot_confusion_matrix(lgbm, X_test,y_test)\nscores = cross_val_score(lgbm,X_train, y_train, cv = cv, scoring = 'roc_auc')\nprint(\"AUC score on test data \", roc_auc_score(y_test, y_pred).round(3))\nprint(\"AUC score in each fold of a 8 fold cross validation: \")\nprint(scores)\nprint(\"Mean: \", scores.mean().round(3), \"\\nStandard deviation: \", np.sqrt(scores.var()).round(5))\n","18da56a4":"balance = pd.DataFrame(df_eda['target'].value_counts()).reset_index()\n\nbalance\nfig = px.pie(\n    balance, \n    names=\"index\", \n    values=\"target\", \n    template = \"plotly_white\",\n    width=700, \n    height=400, \n    color_discrete_sequence=px.colors.qualitative.Vivid\n    )\n\nfig.update_layout(\n    title=\"Proportion of yes and no\",\n    xaxis_title=\"\",\n    yaxis_title=\"\",\n    yaxis={'categoryorder':'total ascending'},\n    font=dict(\n        family=\"Segoe UI\",\n        size=13\n    )\n)\n\nfig.show()","321da542":"from sklearn.model_selection import StratifiedKFold\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.combine import SMOTEENN\nfrom sklearn.metrics import f1_score\n\nkf = StratifiedKFold(n_splits=8)\n\nfor fold, (train_index, test_index) in enumerate(kf.split(X_train, y_train)):\n    X_train_fold = X_train.iloc[train_index]\n    y_train_fold = y_train.iloc[train_index]  # Based on your code, you might need a ravel call here, but I would look into how you're generating your y\n    X_test_fold = X_train.iloc[test_index]\n    y_test_fold = y_train.iloc[test_index]  # See comment on ravel and  y_train\n    sm = SMOTE()\n    X_train_oversampled, y_train_oversampled = sm.fit_resample(X_train_fold, y_train_fold)\n    model = LGBMClassifier()\n    model.fit(X_train_oversampled, y_train_oversampled )  \n    y_pred = model.predict(X_test)\n    print(f'For fold {fold}:')\n    print(f'Accuracy: {model.score(X_test, y_test).round(3)}')\n    print(f'f-score: {f1_score(y_test, y_pred).round(3)}')\n    print(f'AUC: {roc_auc_score(y_test, y_pred).round(3)}')","cc98c643":"lgbm = LGBMClassifier(is_unbalance = True).fit(X_train,y_train.ravel())\ny_pred = lgbm.predict(X_test)\nprint(classification_report(y_test,y_pred))\nplot_confusion_matrix(lgbm, X_test,y_test)\nscores = cross_val_score(lgbm,X_train, y_train, cv = cv, scoring = 'roc_auc')\nprint(\"AUC score on test data \", roc_auc_score(y_test, y_pred).round(3))\nprint(\"AUC score in each fold of a 8 fold cross validation: \")\nprint(scores)\nprint(\"Mean: \", scores.mean().round(3), \"\\nStandard deviation: \", np.sqrt(scores.var()).round(5))\n","76c846f4":"feature_imp = pd.DataFrame(sorted(zip(lgbm.feature_importances_,X.columns)), columns=['Value','Feature'])\n\nfig = px.bar(feature_imp, y=\"Value\", x=\"Feature\", template = \"plotly_white\",  width=700, height=500, color_discrete_sequence=px.colors.qualitative.Vivid)\n\nfig.update_layout(\n    title=\"Feature importance\",\n    xaxis_title=\"\",\n    yaxis_title=\"\",\n            xaxis={'categoryorder':'total descending'},\n    font=dict(\n        family=\"Segoe UI\",\n        size=11\n    )\n)\n\nfig.show()\n","2409bea4":"th = []\nsdlist = []\nauc = []\nfor x in range(0,100):\n    selection = SelectFromModel(lgbm, threshold=x, prefit=True)\n    select_X_train = selection.transform(X_train)\n    # train model\n    selection_model = LGBMClassifier(is_unbalance = True)\n    selection_model.fit(select_X_train, y_train)\n    # test model\n    select_X_test = selection.transform(X_test)\n    y_pred = selection_model.predict(select_X_test)\n    score = cross_val_score(lgbm,select_X_train, y_train, cv = cv, scoring = 'roc_auc')\n    aucValue = score.mean()\n    sdValue = np.sqrt(score.var())\n    th.append(x)\n    auc.append(aucValue)\n    sdlist.append(sdValue)\nd = {'Threshold': th, 'AUC': auc, 'SD': sdlist}\nscores = pd.DataFrame(data=d)","433863a9":"import plotly.express as px\nfig = px.line(scores, x=\"Threshold\", y=\"AUC\", width = 800, height = 400, template= \"plotly_white\", color_discrete_sequence=px.colors.qualitative.Vivid)\n\nfig.update_layout(\n    title=\"AUC change with different feature selection thresholds\",\n    xaxis_title=\"\",\n    yaxis_title=\"\",\n    font=dict(\n        family=\"Segoe UI\",\n        size=10,\n        \n    )\n)\n\nfig.show()","482d7000":"import plotly.express as px\nfig = px.line(scores, x=\"Threshold\", y=\"SD\", width = 800, height = 400, template= \"plotly_white\", color_discrete_sequence=px.colors.qualitative.Vivid)\n\nfig.update_layout(\n    title=\"8 fold standard deviation change with different feature selection thresholds\",\n    xaxis_title=\"\",\n    yaxis_title=\"\",\n    font=dict(\n        family=\"Segoe UI\",\n        size=10,\n        \n    )\n)\n\nfig.show()","b0bea61a":"selection = SelectFromModel(lgbm, threshold=18, prefit=True)\nselect_X_train = selection.transform(X_train)\nselect_X_test = selection.transform(X_test)","cfdb1593":"selection_model = LGBMClassifier(is_unbalance = True)\nselection_model.fit(select_X_train, y_train)\n\ny_pred = selection_model.predict(select_X_test)\nplot_confusion_matrix(selection_model, select_X_test,y_test)\nprint(classification_report(y_test,y_pred))\nscores = cross_val_score(selection_model,select_X_train, y_train, cv = cv, scoring = 'roc_auc')\nprint(\"AUC score on test data \", roc_auc_score(y_test, y_pred).round(3))\nprint(\"AUC score in each fold of a 8 fold cross validation: \")\nprint(scores)\nprint(\"Mean: \", scores.mean().round(3), \"\\nStandard deviation: \", np.sqrt(scores.var()).round(5))","0dc799f4":"from sklearn.model_selection import GridSearchCV\n\nclf = LGBMClassifier(is_unbalance = True)\n\nparam_grid = {\n        'max_depth' : range(1,20),\n }\n\nrs_clf = GridSearchCV(clf, param_grid,\n                            n_jobs=-1, verbose=2, cv= cv,\n                            scoring='roc_auc', refit=False)\n\nrs_clf.fit(select_X_train, y_train)\nbest_score = rs_clf.best_score_\nbest_params = rs_clf.best_params_ \nprint(\"Best score: {}\".format(best_score))\nprint(\"Best params: \")\nfor param_name in sorted(best_params.keys()):\n    print('%s: %r' % (param_name, best_params[param_name]))","e4bf2dfa":"from sklearn.model_selection import GridSearchCV\n\nclf = LGBMClassifier(max_depth = 4, is_unbalance = True)\n\nparam_grid = {\n        'num_leaves' : range(2,15),\n }\n\nrs_clf = GridSearchCV(clf, param_grid,\n                            n_jobs=-1, verbose=2, cv= cv,\n                            scoring='roc_auc', refit=False)\n\nrs_clf.fit(select_X_train, y_train)\nbest_score = rs_clf.best_score_\nbest_params = rs_clf.best_params_ \nprint(\"Best score: {}\".format(best_score))\nprint(\"Best params: \")\nfor param_name in sorted(best_params.keys()):\n    print('%s: %r' % (param_name, best_params[param_name]))","4862666b":"clf = LGBMClassifier(is_unbalance = True, max_depth = 4, num_leaves = 9, feature_fraction_seed = 42)\n\nparam_grid = {\n        'min_gain_to_split' : scipy.stats.uniform(0,3),\n        'lambda_l2' : scipy.stats.uniform(0,3),\n        'min_data_in_leaf': scipy.stats.randint(30,150)\n }\n\nrs_clf = RandomizedSearchCV(clf, param_grid,\n                            n_jobs=-1, verbose=2, cv= cv,\n                            random_state=42, n_iter = 1000)\nprint(\"Randomized search..\")\nsearch_time_start = time.time()\nrs_clf.fit(select_X_train, y_train)\nprint(\"Randomized search time:\", time.time() - search_time_start)\n\nbest_score = rs_clf.best_score_\nbest_params = rs_clf.best_params_\nprint(\"Best score: {}\".format(best_score))\nprint(\"Best params: \")\nfor param_name in sorted(best_params.keys()):\n    print('%s: %r' % (param_name, best_params[param_name]))","6c6f4290":"clf = LGBMClassifier(is_unbalance = True, \n                                 max_depth = 4, \n                                 num_leaves = 9, \n                                 feature_fraction_seed = 42,\n                                **best_params\n                    )\n\nparam_grid = {\n        'learning_rate' : scipy.stats.uniform(0,0.3)\n }\n\nrs_clf = RandomizedSearchCV(clf, param_grid,\n                            n_jobs=-1, verbose=2, cv=cv,\n                            scoring='roc_auc', refit=False, random_state=42, n_iter = 100)\nprint(\"Randomized search..\")\nsearch_time_start = time.time()\nrs_clf.fit(select_X_train, y_train)\nprint(\"Randomized search time:\", time.time() - search_time_start)\n\nbest_score = rs_clf.best_score_\nbest_learning_rate = rs_clf.best_params_\nprint(\"Best score: {}\".format(best_score))\nprint(\"Best params: \")\nfor param_name in sorted(best_learning_rate.keys()):\n    print('%s: %r' % (param_name, best_learning_rate[param_name]))","fda5bae2":"final_model = LGBMClassifier(is_unbalance = True, \n                                 max_depth = 4, \n                                 num_leaves = 9, \n                                 feature_fraction_seed = 42,\n                                 **best_params,\n                                 **best_learning_rate\n                                 \n                                )\nfinal_model.fit(select_X_train, y_train)\n\ny_pred = final_model.predict(select_X_test)\n\nscores = cross_val_score(final_model,select_X_train, y_train, cv = cv, scoring = 'roc_auc')\n","9925703c":"plot_confusion_matrix(selection_model, select_X_test,y_test)\nprint(classification_report(y_test,y_pred))\nprint(\"AUC score on test data \", roc_auc_score(y_test, y_pred).round(3))\nprint(\"AUC score in each fold of a 8 fold cross validation: \")\nprint(scores)\nprint(\"Mean: \", scores.mean().round(3), \"\\nStandard deviation: \", np.sqrt(scores.var()).round(5))","dad4dacc":"from sklearn.dummy import DummyClassifier\ndummy = DummyClassifier(strategy = \"uniform\").fit(X_train,y_train)\n\ny_pred_dummy = dummy.predict(X_test)\n\nprint('AUC: ', roc_auc_score(y_test,y_pred_dummy).round(3))\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred_dummy))\n\n\nplot_confusion_matrix(dummy, X_test,y_test)","986299a7":"# roc curve and roc auc on an imbalanced dataset\nfrom sklearn.datasets import make_classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom matplotlib import pyplot\n \n# plot no skill and model roc curves\ndef plot_roc_curve(test_y, naive_probs, model_probs):\n\t# plot naive skill roc curve\n\tfpr, tpr, _ = roc_curve(test_y, naive_probs)\n\tpyplot.plot(fpr, tpr, linestyle='--', label='No Skill')\n\t# plot model roc curve\n\tfpr, tpr, _ = roc_curve(test_y, model_probs)\n\tpyplot.plot(fpr, tpr, marker='.', label='LightGBM Model')\n\t# axis labels\n\tpyplot.xlabel('False Positive Rate')\n\tpyplot.ylabel('True Positive Rate')\n\t# show the legend\n\tpyplot.legend()\n\t# show the plot\n\tpyplot.show()\n \nyhat = dummy.predict_proba(X_test)\nnaive_probs = yhat[:, 1]\n# calculate roc auc\nroc_auc = roc_auc_score(y_test, y_pred_dummy)\nprint('No Skill ROC AUC %.2f' % roc_auc)\n# # skilled model\n# lgbm.fit(X_train, y_train)\nyhat = selection_model.predict_proba(select_X_test)\nmodel_probs = yhat[:, 1]\n# calculate roc auc\nroc_auc = roc_auc_score(y_test, y_pred)\nprint('LightGBM ROC AUC on test data %.3f' % roc_auc)\n# plot roc curves\nplot_roc_curve(y_test, naive_probs, model_probs)","b14abfd3":"X_sub = selection.transform(X_sub)\n\npredict = final_model.predict_proba(X_sub)\npredict = predict[:, 1]\n\nenrollee = pd.read_csv(\"..\/input\/hr-analytics-job-change-of-data-scientists\/aug_test.csv\")\n\nsubmission = pd.DataFrame({'enrollee_id':enrollee['enrollee_id'],'target':predict})\n\nsubmission.to_csv('submission.csv',index=False)\n","f044b9b9":"# Features importance  \n\n## Which variables influence data scientists the most?  \n\nIn the following plot we can see which are the most important features for the model predictions. ","35dc7cb9":"**Filtering the features and retraining the model with 19 as feature threshold**","06f42aeb":"## Baseline model","ceb5bf87":"# Feature selection","c3992f36":"This is the baseline model. I will compare it to the tuned model to see if parameters tuning improve it. We can immediately see that the recall score is not good, so the predictions for the Yes class are not very reliable. It's expected for imbalanced datasets, but it will be improved later.\n\nEvery score will be computed with an 8 fold cross validation with stratified splits.","d4a2a7de":"The observations are not homogeneous: most of the data scientists live in few cities. This will be important to know later.","ebaca069":"# The final model  \n\nRandom search optimization further improved the mean AUC score but slightly increased the standard deviation. Let's retrain it with the best parameters found.","73f67171":"** City variable**  \nThere are 123 unique cities, but most of the data scientists live in the first 10. I choose to set to \"Other\" every city that has less than 300 observations. This will result in having less features after doing the one hot encoding.","c4f91ec1":"**Defining CV splits**","fad6993b":"# Missing values  \nThere are some missing value in the data that need to be imputed or dropped.","0c94a0ab":"Three variables are numeric, every other one is categorical. ","dbd72a00":"**IMPORTANT: in this case the  SMOTE needs to be applied on each fold of the Kfold CV. Not doing this will result in training data leaking as test data, making the model invalid.**","16e8929f":"Removing less important features didn't decrease the AUC, but decreased the variance and made the model faster. ","b6176345":"One of the main changes from all the other GBMs, like XGBoost, is the way tree is constructed. In LightGBM, a leaf-wise tree growth strategy is adopted. In LightGBM, the leaf-wise tree growth finds the leaves which will reduce the loss the maximum, and split only that leaf and not bother with the rest of the leaves in the same level. This results in an asymmetrical tree where subsequent splitting can very well happen only on one side of the tree.\n\nLeaf-wise tree growth strategy tend to achieve lower loss as compared to the level-wise growth strategy, but it also tends to overfit, especially small datasets.\n\n[For more informations](https:\/\/deep-and-shallow.com\/2020\/02\/21\/the-gradient-boosters-iii-lightgbm\/)\n\nWe will tune the parameters to assure that the model is not overfitting.\n","446e73ff":"Converting the target variable in a Yes\/No variable, for a better visualization","d6e2755a":"# Exploratory data analysis","9ec811d1":"## Tuning `max_depth` and `num_leaves`\n\nThose are the most sensible parameters in gradient boost and need to be tuned first.\n`max_depth` is the parameter that dictates the maximum depth that each tree in a boosting round can grow to. \n`num_leaves` is the maximum number of leaves a tree can have.  \n\nBoth are useful to control overfitting.","4c47c4f4":"**Converting variables to the right type**  \n\nThe columns are still of type \"object\", so they need to be converted in an integer type. Int64 allows for NaN values.","ec185fd1":"## SMOTE + K Fold cross validation\n\nSince the dataset is unbalanced, it might be beneficial to run SMOTE to make it balanced by oversampling the minority and downsampling the majority class.","d64a356e":"![](https:\/\/miro.medium.com\/max\/1618\/0*4nrDSJJcTHNjMjmb.png)","74db1cfc":"# Encoding ordinal variables  \nOrdinal variables are categorical variables that have an order. I'm encoding them to use them as numerical variables.","bab0763b":"When the dataset in unbalanced, looking to a single metric can be misleading. For example, the dummy model has a precision score for the 0 class for 0.75. This means that it's getting 75% of the predictions right guessing completely at random. In this case a precision score for my model of 0.7 would be bad.\n\nHowever, since the 1 class is the minority, guessing at random can only get 25% of the minority class results correctly.","d3f202d8":"## Tuning other hyperparameters with RandomizedSearchCV\n\n\nRandomized search is faster than grid search because it doesn't try every possible value, it take random values from the distributions I specified. It also performs a built in 8 fold cross validation.\n\nI'm manually setting the parameters we've found so far, and I'll search for the others. For the search I will use the default learning rate of 0.1 so it will be faster.\n\nThe parameters I choose to tune are:\n- `min_data_in_leaf`: This is is a way to reduce overfitting. It requires each leaf to have the specified number of observations so that the model does not become too specific.\n- `min_gain_to_split`: When adding a new tree node, LightGBM chooses the split point that has the largest gain. Gain is basically the reduction in training loss that results from adding a split point. \n- Adding some regularization through `lambda_l2`.","f4444051":"I'm merging train and test datasets to process the data. They will be divided later in the same way that kaggle provided them. ","9b49a832":"# Making submission","b78a059f":"It looks like the variables that influence a data scientist the most in their decision to change his job are training hours, experience, and city development index.","73e212b6":"The learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function. Higher learning rate means faster model, at the risk of being less precise. Setting learning rate too low might lead to overfitting.","88d94d6a":"It looks like there are features with very low importance. With the following loop I'm testing each threshold for removing the least important features. It will make the model faster and potentially more precise.","910a3e71":"# Dealing with the unbalanced dataset","ed98e448":"### Imputing missing values  \nI'm using two imputers for the numerical and categorical variables. After different attempts, I find out that the most effective strategy is to impute every numerical variable with the mean of the columns, and every categorical with the most frequent observation.","4a655370":"**LightGBM Classifier**  \n![](https:\/\/repository-images.githubusercontent.com\/64991887\/dc855780-e34b-11ea-9ab8-e08ca33288b0)\n\nMy algorithm of choice is the Light Gradient Boosting for its speed and performance. LightGBM is a gradient boosting framework made by Microsoft that uses tree based learning algorithms.","73196da5":"There are apparently no improvement. We'll go on with the unbalanced dataset using the **is_unbalance = True** parameter in lightgbm.","6e6332a7":"**One hot encoding categorical variables**  \nAfter the one hot encoding there are 37 features in total.","3497d986":"The 8 fold cross validation shows some slight overfitting.","f28c5c74":"## Tuning `learning_rate`","09c86edf":"There are also some ordinal variables that we will encode into integers later.","742422ca":"The report shows that:\n- 91% of the predicted No are actually No. 53% of the predicted Yes are actually Yes. \n- 78% of the total No are predicted as No. 75% of the total Yes are predicted Yes.  \n\nWhile the precision for the Yes class might seem low, it's expected since our test train is unbalanced. Since we have more negatives than positives, the higher number of True Negatives will influence the number of False Negatives too, resulting in a lower recall.  \n\nWe will see that our model with 0.8 AUC is substantially better than the dummy classifier.","f326f71e":"# Model building","742eec90":"The hyperparameter tuning resulted in some improvement in the recall score for the minority class: this is important for an imbalanced dataset.\n\nThe mean AUC on the cross validation folds, and the AUC on test data also increased.","99db9b67":"# Hyperparameters optimization","bfc1f860":"Adding this parameters yielded very good results. The AUC on test data went from 0.71 to 0.76. But most importantly, the recall score improved a lot: it went from 0.55 to 0.73. The precision fot the majority class also increased.  \nThe AUC on the KFold splits is to see if the model overfits, while the AUC on test data is useful to understand how the model behaves on out of sample data. ","d42d008d":"Most of the data scientists in the sample: \n- Are male\n- Have relevant experience\n- Are not currently enrolled in a university course\n- Are graduate in a STEM discipline\n- Have more than 20 years of work experience\n- Work in a private company\n- Work in a medium sized company\n- Got their new job in the last year","6a72f79f":"# Comments and suggestions are very welcome!","adc9bde2":"The graph shows that removing the best threshold for removing features. Doing this will results in a faster and more performant model.","39d8e37c":"### A parameter for unbalanced datasets: `is_unbalance`","2200e1a7":"## Model evaluation on the test set","e204f041":"**Training the model**","5610da25":"# Model evaluation","01df84fb":"LightGBM documentation suggests value of `num_leaves` of a maximum of 2^`max_depth`-1","db366a2e":"The dataset is unbalaced. Only 24.9% of the data scientists in the sample are looking to change their job.","f69d6244":"### Comparison with the dummy model  \nSince the test set is unbalanced, it makes sense to compare our model to the dummy model which classifies every instance at random.","21884d2c":"## **HR Analytics Data Scientists Job Change Prediction Model**\n\n![](https:\/\/www.datocms-assets.com\/14946\/1596797558-da-vs-ds-diagram.png)\n\nIn this notebook I will build and optimize step by step a LightGBM model that tries to predict whether a data scientist is looking to change his job or not.  \n\n**Available features:**\n\nenrollee_id : Unique ID for candidate  \ncity: City code\ncity_development_index : Developement index of the city (scaled)  \ngender: Gender of candidate  \nrelevent_experience: Relevant experience of candidate  \nenrolled_university: Type of University course enrolled if any  \neducation_level: Education level of candidate  \nmajor_discipline: Education major discipline of candidate  \nexperience: Candidate total experience in years  \ncompany_size: No of employees in current employer's company  \ncompany_type: Type of current employer  \nlastnewjob: Difference in years between previous job and current job  \ntraining_hours: training hours completed  \ntarget: 0 \u2013 Not looking for job change, 1 \u2013 Looking for a job change\n","e92c29b2":"The blue line represents the \"no skill\" model, while the orange line is the LightGBM model. The area under the ROC curve is 0.77, which is very good compared to the no skill.","33f4d54a":"# Feature engineering and data preparation","1dac24bd":"**Dividing train and submission sets again**  \nAfter the feature engineering the sets are divided again the same splits that they were provided. I also dropped the enrollee_id variable because it's not a feature.  \nThe X_sub dataset will be used later for the submission.","08bf6a2d":"# Splitting test and train datasets  \n \nI am splitting a test\/train dataset with a ratio of 80\/20. The test dataset will be kept to evaluate performance on the model after.","c2a84345":"In the most developed cities, less data scientists want to change their job.","77c5c0f8":"Testing on the test set shows an AUC score of 0.76"}}