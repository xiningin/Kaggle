{"cell_type":{"a95df62a":"code","78cd9fbe":"code","0b3dab41":"code","04bb20f7":"code","8926fd1e":"code","724c7aab":"code","576bdef3":"code","b7f5dbe9":"code","a643d10a":"code","b2a0b65b":"code","4638f2f6":"code","b9be60c2":"code","3aae9ad8":"code","f8904513":"code","99b55e5e":"code","65c82e6a":"code","92e0b39b":"code","585954c9":"code","b5f03076":"code","8738860c":"code","33972b43":"code","02138fb1":"code","4873d38b":"code","dd979238":"code","e5bb60df":"code","d3a1d52e":"code","32165e8d":"code","121e1b20":"code","f2a913b9":"code","22659b98":"code","4c303736":"code","f5e156e5":"code","720ec9ed":"code","38583897":"code","60a1d85f":"code","712e3b3c":"code","02b78d7f":"code","3bbb3aca":"code","044009e5":"code","6b8aa7bc":"code","df462d2a":"code","0bd2ab70":"code","f4f731af":"markdown","9eb29944":"markdown","9c60ab78":"markdown","e204f380":"markdown","f6bdc758":"markdown","fc6e734e":"markdown","b579cc97":"markdown","507b8bee":"markdown","d329a60f":"markdown","719e59fa":"markdown","4a9fe76f":"markdown","4008a10c":"markdown","2ec59891":"markdown","fbf013b7":"markdown","8777cff4":"markdown","f1e1144e":"markdown","494043a1":"markdown","21f2eaca":"markdown","3049f713":"markdown","0cb44e1a":"markdown","4926bfc8":"markdown","0f98df9d":"markdown","4f40a4aa":"markdown"},"source":{"a95df62a":"# Checking System Version\nimport sys\nprint(sys.version)","78cd9fbe":"from nltk.book import *","0b3dab41":"# Recall text by typing their name only\ntext7","04bb20f7":"# It gives you list as per the nltk book import\ntexts()","8926fd1e":"# Searching text\ntext1.concordance(\"monstrous\") ","724c7aab":"text2.concordance(\"affection\")","576bdef3":"text1.similar(\"monstrous\") ","b7f5dbe9":"text2.similar(\"monstrous\")","a643d10a":"text2.common_contexts([\"monstrous\",\"very\"])","b2a0b65b":"text4.dispersion_plot([\"citizens\",\"democracy\",\"freedom\",\"duties\",\"America\"])","4638f2f6":"print(len(\"nishant;., is here\"))\n\nList_N1 = ['Call', 'me', 'Nishant', '.']\nprint(len(List_N1))\n\nprint(len(text3))","b9be60c2":"set(text3)","3aae9ad8":"len(set(text3))","f8904513":"sorted(set(text3))","99b55e5e":"len(text3)\/len(set(text3))  #tokens (individual words and punctuation) occur in a given text, divided by how many types (unique words and punctuation)","65c82e6a":"print(text5.count('lol')) # count of a specific word.\n100*(text5.count('lol'))\/len(text5) # percentage against total word count.","92e0b39b":"#tokens (individual words and punctuation) occur in a given text, divided by how many types (unique words and punctuation)\ndef lex_dive(text):\n    return len(text)\/len(set(text))","585954c9":"def percentage(count, total):\n    return 100*count\/total","b5f03076":"lex_dive(text3)","8738860c":"percentage(text5.count('lol'),len(text5))","33972b43":"# We can inspect the total number of words (\u201coutcomes\u201d) that have been counted up using FreqDist\nfdist1 = FreqDist(text1)\nfdist1","02138fb1":"#The expression keys() gives us a list of all the distinct types in the text\nvocab1 = fdist1.keys()\nvocab1","4873d38b":"# Count of word 'whale' in the frequency distribution list\nfdist1['whale']","dd979238":"# To set figure size of the plot\nfrom matplotlib.pyplot import figure\nfigure(num=None, figsize=(10, 5), dpi=80, facecolor='w', edgecolor='k')\n\n# Cumulative Frequency Plot for 50 words\nfdist1.plot(50,cumulative = True)","e5bb60df":"fdist1.hapaxes()","d3a1d52e":"V = set(text1)\nlong_words = [w for w in V if len(w) > 15]\nsorted(long_words)","32165e8d":"fdist5 = FreqDist(text5)\nsorted(w for w in set(text5) if len(w) > 7 and fdist5[w] > 7)","121e1b20":"from nltk import bigrams\nlist(bigrams(['more', 'is', 'said', 'than', 'done']))","f2a913b9":"bigrams(['more', 'is', 'said', 'than', 'done'])","22659b98":"text4.collocations()","4c303736":"[len(w) for w in text1]  #  A list of the lengths of words in text1","f5e156e5":"fdist = FreqDist(len(w) for w in text1) # Frequency distribution of lengths of words in text1\nprint(fdist)\nfdist","720ec9ed":"print(fdist.most_common()) # A list of frequency disctribution of lengths of words\n\nprint(fdist.max()) # Most frequent word length\n\nprint(fdist[3]) # Frequncy of word length 3\n\nfdist.freq(3) # Percent Frequency of word length 3","38583897":"sorted(w for w in set(text1) if w.endswith('ableness')) # Operator endswith","60a1d85f":"sorted(term for term in set(text4) if 'gnt' in term) # words with substring 'gnt'","712e3b3c":"sorted(item for item in set(text6) if item.istitle()) # titlecased words","02b78d7f":"sorted(item for item in set(sent7) if item.isdigit()) #test if s is non-empty and all characters in s are digits","3bbb3aca":"sorted(w for w in set(text7) if '-' in w and 'index' in w)  # Words with two substring(s)\/character(s)","044009e5":"sorted(wd for wd in set(text3) if wd.istitle() and len(wd) > 10)  # Titlecased words with more than 10 characters","6b8aa7bc":"sorted(w for w in set(sent7) if not w.islower()) # test if s contains cased characters and all are lowercase. Here with not","df462d2a":"[len(w) for w in text1]\n[w.upper() for w in text1]  # test if s contains cased characters and all are uppercase","0bd2ab70":"sent1 = ['Call', 'me', 'Nishant', '.']\n\n# Looping with Condition using 'for' & 'if'\nfor xyzzy in sent1:\n    if xyzzy.endswith('t'):\n        print(xyzzy)","f4f731af":"**INFREQUENT\/RARE WORDS (OCCURS ONLY ONCE):**\n\nThe **words that occur once** only, the socalled **hapaxes**. View them by typing **fdist1.hapaxes()**. This list contains lexicographer, cetological, contraband, expostulations, and about 9,000 others. It seems that there are too many rare words, and without seeing the context we probably can\u2019t guess what half of the hapaxes mean in any case! Since neither frequent nor infrequent words help, we need to try something else.","9eb29944":"**Frequency Distribution**  tells us the frequency of each vocabulary item in the text. (In general, it could count any kind of\nobservable event.) It is a \u201cdistribution\u201d since it tells us how the total number of word tokens in the text are distributed across the vocabulary items.","9c60ab78":"**Lexical Richness** : In computational linguistics, lexical richness is a measure of how many tokens (individual words and punctuation) occur in a given text, divided by how many types (**unique words and punctuation**) occur in that same text.\n\nlet\u2019s calculate a measure of the lexical richness of the text. The next example shows us that each word is used 16 times on average.","e204f380":"Here are all words from the chat corpus that are longer than seven characters, that occur more than seven times:","f6bdc758":"**By wrapping sorted()** around the Python expression set(text3) , we obtain** a sorted list of vocabulary items**, beginning with various punctuation symbols and continuing with words starting with A. All capitalized words precede lowercase words. We discover the size of the vocabulary indirectly, by asking for the number of items in the set, and again we can use len to obtain this number . Although it has 44,764 tokens, this book has only **2,789 distinct words, or \u201cword types.\u201d** A word type is the form or spelling of the word independently of its specific occurrences in a text\u2014that is, the word considered as a unique item of vocabulary. **Our count of 2,789 items will include punctuation symbols, so we will generally call these unique items types instead of word types.**","fc6e734e":"**SEARCHING TEXT**    \n\nA **concordance** permits us to see **words in context** and **search for a specific word**. For example, we saw that monstrous occurred in contexts such as the **___ pictures** and the **___ size**.","b579cc97":"This kernel is based on the practice done over the book \"Natural Language Processing with Python\" by Steven Bird, Ewan Klein, Edward Loper.\n\nIn this part, we will focus on \"Computing with Language : Texts & Words\". It gives us interest and better understanding of basic statistics around language processing. ","507b8bee":"It is one thing to automatically detect that a particular word occurs in a text, and to\ndisplay some words that appear in the same context. However, we can also determine\nthe location of a word in the text: how many words from the beginning it appears. This\npositional information can be displayed using a **dispersion plot**. Each stripe represents\nan instance of a word, and each row represents the entire text.","d329a60f":"Downloading the **NLTK Book Collection** from 'nltk' package. It consists of about **30 compressed files** requiring about 100Mb disk space.","719e59fa":"'**similar**'is used to get **similar words in the given range of context provided by 'concordance'**. Observe that **we get different results for different texts**. Austen uses this word quite differently from Melville; for her, monstrous has positive connotations, and sometimes functions as an intensifier like the word very.","4a9fe76f":"**The vocabulary of a text** is just the **set of (unique)tokens** that it uses, since in a set, **all duplicates are collapsed together**. In Python we can obtain the vocabulary items of text3 with the command: **set(text3)**. When you do this, many screens of words will fly past. ","4008a10c":"**50 MOST FREQUENTLY USED WORDS:**\n\nOnly one word, **whale, is slightly informative!** It occurs over 900 times. The rest of the words tell us nothing about the text; they\u2019re just English \u201cplumbing.\u201d \nWhat proportion of the text is taken up with such words? We can generate a **cumulative frequency plot** for these words, using **fdist1.plot(50, cumulative=True)**, to produce the graph (as given below). **These 50 words account for nearly half the book!**","2ec59891":"**Word Comparison Operator**\n\ns.startswith(t)\t -->  test if s starts with t <br>\ns.endswith(t)\t-->  test if s ends with t <br>\nt in s\t              -->  test if t is a substring of s <br>\ns.islower()\t      -->  test if s contains cased characters and all are lowercase <br>\ns.isupper()\t     -->  test if s contains cased characters and all are uppercase <br>\ns.isalpha()  \t -->  test if s is non-empty and all characters in s are alphabetic <br>\ns.isalnum() \t-->  test if s is non-empty and all characters in s are alphanumeric <br>\ns.isdigit()\t       -->  test if s is non-empty and all characters in s are digits <br>\ns.istitle()     \t-->  test if s contains cased characters and is titlecased (i.e. all words in s have initial capitals) <br>","fbf013b7":"**Make Functions** for lexical diversity\/richness and percentage as follows: ","8777cff4":"**Collocations** are essentially **just frequent bigrams**, except that we want to pay more attention to the cases that involve rare words. In particular, we want to find **bigrams that occur more often** than we would expect based on the frequency of the individual words. The collocations() function does this for us.","f1e1144e":"**Collocations and Bigrams**\n\nA collocation is a sequence of words that occur together unusually often. Thus **red wine is a collocation**, whereas **the wine is not**. To get a handle on collocations, we start off by extracting from a text a list of word pairs, also known as bigrams.","494043a1":"**Fine-grained Selection of Words**\n\nLet's find the words from the vocabulary of the text that are more than 15 characters long.","21f2eaca":"We can look at the distribution of word lengths in a text, by creating a FreqDist out of a long list of numbers, where each number is the length of the corresponding word in the text:","3049f713":"**Functions Defined for NLTK's Frequency Distributions:**\n\nfdist = FreqDist(samples)\t-->   create a frequency distribution containing the given samples <br>\nfdist[sample] += 1\t-->  increment the count for this sample <br>\nfdist['monstrous']\t-->  count of the number of times a given sample occurred <br>\nfdist.freq('monstrous')\t-->  frequency of a given sample <br>\nfdist.N()\t-->  total number of samples <br>\nfdist.most_common(n)\t-->  the n most common samples and their frequencies <br>\nfor sample in fdist:\t-->  iterate over the samples <br>\nfdist.max()\t-->  sample with the greatest count <br>\nfdist.tabulate()\t-->  tabulate the frequency distribution <br>\nfdist.plot()\t-->  graphical plot of the frequency distribution <br>\nfdist.plot(cumulative=True)\t-->  cumulative plot of the frequency distribution <br>\nfdist1 |= fdist2\t-->  update fdist1 with counts from fdist2 <br>\nfdist1 < fdist2\t-->  test if samples in fdist1 occur less frequently than in fdist2 <br>","0cb44e1a":"**'len'** can be used to count vocabulary in a text line. It will count** words, punctuations, symbols, white spaces** and return you total count for that. So Genesis(text3) has 44,764 words and punctuation symbols, or \u201ctokens.\u201d A** token** is the technical name for **a sequence of characters**\u2014such as hairy, his, or :)\u2014that we want to treat as a group. When we count the number of tokens in a text, say, the phrase to be or not to be, we are **counting occurrences of these sequences**.","4926bfc8":"We can count how often a word occurs in a text using '**count**' and so on it's percentage...","0f98df9d":"The term **common_contexts** allows us to examine just the* **contexts that are shared by\ntwo or more words***, such as monstrous and very. We have to enclose these words by\nsquare brackets as well as parentheses, and separate them with a comma:","4f40a4aa":"**Note**\n*If you omitted list() above, and just typed bigrams(['more', ...]), you would have seen output of the form <generator object bigrams at 0x10fb8b3a8>. This is Python's way of saying that it is ready to compute a sequence of items, in this case, bigrams. For now, you just need to know to tell Python to convert it into a list, using list().*"}}