{"cell_type":{"6baf2664":"code","fd5c32dd":"code","5e31615f":"code","1968be46":"code","02bc0f69":"code","5ba3584c":"code","89f2b2bf":"code","48fcc062":"code","b316c993":"code","ffb6e3aa":"code","ce950fd7":"markdown","ea33ed3f":"markdown","40d707bf":"markdown"},"source":{"6baf2664":"import numpy as np\nimport pandas as pd \nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns","fd5c32dd":"train = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/train.csv\")\ntrain.head()","5e31615f":"train = train.drop('id', axis=1)","1968be46":"train.info()","02bc0f69":"fig, ax = plt.subplots()\nsns.countplot(x='target', data=train, order=sorted(train['target'].unique()), ax=ax)\nax.set_ylim(0, 63000)\nax.set_title('Target Distribution', weight='bold')\nplt.show()","5ba3584c":"train.describe().T.style.bar(subset=['mean'], color='#205ff2')\\\n                            .background_gradient(subset=['std'], cmap='Reds')\\\n                            .background_gradient(subset=['50%'], cmap='coolwarm')","89f2b2bf":"X = train.copy()\n\n# Label encoding for categoricals\n# Now, only \"target\" is categorical\nX[\"target\"], _ = X[\"target\"].factorize()\nX.head()","48fcc062":"y = X.pop(\"target\")","b316c993":"# from sklearn.feature_selection import mutual_info_regression\nfrom sklearn.feature_selection import mutual_info_classif\n\ndef make_mi_scores(X, y):\n#     mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n    mi_scores = mutual_info_classif(X, y)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\nmi_scores = make_mi_scores(X, y)\nmi_scores[::3]  # show a few features with their MI scores","ffb6e3aa":"def plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")\n\n\n# plt.figure(dpi=100, figsize=(8, 5))\nplt.figure(dpi=100, figsize=(20, 15))\nplot_mi_scores(mi_scores)","ce950fd7":"# Introduction\n\nWhen you create a model in table data, you often want to know correlations between the explanatory variables and the objective variables.\n\n**Mutual Information(MI)** is one of the methods to know correlations even it is not linear.\n\nMI of two discrete random variables X and Y is defined like below.\n\n$$\nI(X;Y)=\\sum _{{y\\in Y}}\\sum _{{x\\in X}}p(x,y)\\log {\\frac  {p(x,y)}{p(x)\\,p(y)}}\n$$\n\n\n- Easy to understand MI  \n[kaggle course about MI](https:\/\/www.kaggle.com\/ryanholbrook\/mutual-information)\n\n- For further information  \n[wikipedia](https:\/\/en.wikipedia.org\/wiki\/Mutual_information)\n\n\nThis competition itself, I referred to  \n[[TPS-May] Categorical EDA](https:\/\/www.kaggle.com\/subinium\/tps-may-categorical-eda)  \nThis notebook is very instructive for EDA(Explanatory Data Analysis) even if you are kaggle beginner(so am I!)","ea33ed3f":"The high-scoring **feature_14** exhibits a strong relationship with **target**.","40d707bf":"I hope this mutual information result would be somewhat helpful for your prediction.\n\nThanks\ud83d\ude0a"}}