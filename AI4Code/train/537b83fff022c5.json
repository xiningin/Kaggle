{"cell_type":{"22091a53":"code","65da1339":"code","b8fb8653":"code","b9446ae0":"code","fb997d1c":"code","a6f37759":"code","e75e35c6":"code","0e13a0a9":"code","a5680f33":"code","344b02e4":"code","d2460594":"code","f3b109a3":"code","9615c75c":"code","a2e06e24":"code","baff7e16":"code","d0598376":"code","6c06a03c":"code","c2fa95a1":"code","44debbd1":"code","432e8dad":"code","c51872b7":"code","70a58505":"code","463a0a22":"code","4f3270fa":"code","32a6bc44":"code","4fd104cc":"code","af22377b":"code","9ef8d2ec":"markdown","a4606cbd":"markdown"},"source":{"22091a53":"import os\nimport numpy as np\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly as py\nimport plotly.graph_objs as go\n\ninit_notebook_mode(connected=True) #do not miss this line\n\nfrom gensim import corpora, models, similarities\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport nltk\nimport re\nimport pandas as pd","65da1339":"datafile = '..\/input\/detik18\/detik18.csv'","b8fb8653":"tweets = pd.read_csv(datafile, encoding='latin1')\ntweets = tweets.assign(Time=pd.to_datetime(tweets.Time))\n\ntweets.head(10)","b9446ae0":"range(len(tweets['Tweet']))","fb997d1c":"'''import plotly.plotly as py\nimport plotly.graph_objs as go\n'''\ntweets['Time'] = pd.to_datetime(tweets['Time'], format='%y-%m-%d %H:%M:%S')\ntweetsT = tweets['Time']\n\ntrace = go.Histogram(\n    x=tweetsT,\n    marker=dict(\n        color='blue'\n    ),\n    opacity=0.75\n)\n\nlayout = go.Layout(\n    title='Tweet Activity Over Years',\n    height=450,\n    width=1200,\n    xaxis=dict(\n        title='Month and year'\n    ),\n    yaxis=dict(\n        title='Tweet Quantity'\n    ),\n    bargap=0.2,\n)\n\ndata = [trace]\n\nfig = go.Figure(data=data, layout=layout)\npy.offline.iplot(fig)","a6f37759":"#initialize stopWords\nstopWords = []\n\n#start replaceTwoOrMore\ndef replaceTwoOrMore(s):\n    #look for 2 or more repetitions of character and replace with the character itself\n    pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL)\n    return pattern.sub(r\"\\1\\1\", s)\n#end","e75e35c6":"#start getStopWordList\ndef getStopWordList(stopWordListFileName):\n    #read the stopwords file and build a list\n    stopWords = []\n    stopWords.append('AT_USER')\n    stopWords.append('URL')\n\n    fp = open(stopWordListFileName, 'r')\n    line = fp.readline()\n    while line:\n        word = line.strip()\n        stopWords.append(word)\n        line = fp.readline()\n    fp.close()\n    return stopWords\n#end","0e13a0a9":"#start getStopWordList\ndef getStopWordList(stopWordListFileName):\n    #read the stopwords file and build a list\n    stopWords = []\n    stopWords.append('AT_USER')\n    stopWords.append('URL')\n\n    fp = open(stopWordListFileName, 'r')\n    line = fp.readline()\n    while line:\n        word = line.strip()\n        stopWords.append(word)\n        line = fp.readline()\n    fp.close()\n    return stopWords\n#end","a5680f33":"#import regex\nimport re\n#start process_tweet\ndef processTweet(tweet):\n    # process the tweets\n    #Convert to lower case\n    tweet = tweet.lower()\n    #Convert www.* or https?:\/\/* to URL\n    tweet = re.sub('((www\\.[^\\s]+)|(https?:\/\/[^\\s]+))','URL',tweet)\n    #Convert @username to AT_USER\n    tweet = re.sub('@[^\\s]+','AT_USER',tweet)\n    #Remove additional white spaces\n    tweet = re.sub('[\\s]+', ' ', tweet)\n    #Replace #word with word\n    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n    #trim\n    tweet = tweet.strip('\\'\"')\n    return tweet\n#end\n\n#start getfeatureVector\ndef getFeatureVector(tweet):\n    featureVector = []\n    #split tweet into words\n    words = tweet.split()\n    for w in words:\n        #replace two or more with two occurrences\n        w = replaceTwoOrMore(w)\n        #strip punctuation\n        w = w.strip('\\'\"?,.')\n        #check if the word stats with an alphabet\n        val = re.search(r\"^[a-zA-Z][a-zA-Z0-9]*$\", w)\n        #ignore if it is a stop word\n        if(w in stopWords or val is None):\n            continue\n        else:\n            featureVector.append(w.lower())\n    return featureVector\n#end","344b02e4":"#Read the tweets one by one and process it\nfp = open('..\/input\/detik18\/detik18.csv', 'r')\nline = fp.readline()\n\nstopWords = getStopWordList('..\/input\/stopword\/stopwordsID.txt')\nkalimat = []\nwhile line:\n    processedTweet = processTweet(line)\n    featureVector = getFeatureVector(processedTweet)\n    #print (featureVector)\n    kalimat.append(featureVector)\n    line = fp.readline()\n#end loop\nfp.close()\n","d2460594":"a = kalimat[1:]\n","f3b109a3":"corpus=[]\na=[]\nfor i in range(len(kalimat)):\n        a=kalimat[i]\n        for t in range (len(a)):\n            b=a[t]\n            corpus.append(b)\n#print(corpus)","9615c75c":"corpus[0:5]","a2e06e24":"import gensim\nimport logging\nimport tempfile\n\nTEMP_FOLDER = tempfile.gettempdir()\nprint('Folder \"{}\" will be used to save temporary dictionary and corpus.'.format(TEMP_FOLDER))\n\nfrom gensim import corpora\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)","baff7e16":"from nltk.corpus import stopwords\nfrom string import punctuation\n\n# remove common words and tokenize\nlist1 = ['RT','rt']\nstoplist = stopwords.words('indonesian') + list(punctuation) + list1\n\ntexts = [[word for word in str(document).lower().split() if word not in stoplist] for document in corpus]","d0598376":"dictionary = corpora.Dictionary(texts)\ndictionary.save(os.path.join(TEMP_FOLDER, 'elon.dict'))  # store the dictionary, for future reference\n#print(dictionary)","6c06a03c":"corpus = [dictionary.doc2bow(text) for text in texts]\ncorpora.MmCorpus.serialize(os.path.join(TEMP_FOLDER, 'elon.mm'), corpus)  # store to disk, for later use","c2fa95a1":"from gensim import corpora, models, similarities","44debbd1":"tfidf = models.TfidfModel(corpus) # step 1 -- initialize a model","432e8dad":"corpus_tfidf = tfidf[corpus]  # step 2 -- use the model to transform vectors","c51872b7":"total_topics = 5","70a58505":"lda = models.LdaModel(corpus, id2word=dictionary, num_topics=total_topics)\ncorpus_lda = lda[corpus_tfidf] # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi","463a0a22":"#Show first n important word in the topics:\nlda.show_topics(total_topics,5)","4f3270fa":"from collections import OrderedDict\n\ndata_lda = {i: OrderedDict(lda.show_topic(i,25)) for i in range(total_topics)}\n#data_lda","32a6bc44":"import pandas as pd\n\ndf_lda = pd.DataFrame(data_lda)\nprint(df_lda.shape)\ndf_lda = df_lda.fillna(0).T\nprint(df_lda.shape)","4fd104cc":"df_lda","af22377b":"import pyLDAvis.gensim\n\npyLDAvis.enable_notebook()\npanel = pyLDAvis.gensim.prepare(lda, corpus_lda, dictionary, mds='tsne')\npanel","9ef8d2ec":"***Tweet Activity Over Years***","a4606cbd":"In the previous cells, we created a corpus of documents represented as a stream of vectors. To continue, let\u2019s fire up gensim and use that corpus:"}}