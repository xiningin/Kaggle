{"cell_type":{"4b061bf9":"code","b3bce061":"code","2abd0285":"code","78f1d4ec":"code","54ef53dc":"code","2eb6c1b1":"code","f0d472cd":"code","faba2697":"code","11f9f473":"code","de90214a":"code","f7e05a39":"code","24592895":"code","6e8b1f05":"code","88d692cf":"markdown","1b496e62":"markdown","67a3652c":"markdown","753c681a":"markdown","98946665":"markdown","7c7c58c1":"markdown","46b29812":"markdown","40a6f13e":"markdown"},"source":{"4b061bf9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\n%matplotlib inline\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b3bce061":"#1-D normal Python\nimport math\n\n'''Class for Gaussian Kernel Regression'''\nclass GKR:\n    \n    def __init__(self, x, y, b):\n        self.x = x\n        self.y = y\n        self.b = b\n    \n    '''Implement the Gaussian Kernel'''\n    def gaussian_kernel(self, z):\n        return (1\/math.sqrt(2*math.pi))*math.exp(-0.5*z**2)\n    \n    '''Calculate weights and return prediction'''\n    def predict(self, X):\n        kernels = [self.gaussian_kernel((xi-X)\/self.b) for xi in self.x]\n        weights = [len(self.x) * (kernel\/np.sum(kernels)) for kernel in kernels]\n        return np.dot(weights, self.y)\/len(self.x)\n    \n    def visualize_kernels(self, precision):\n        plt.figure(figsize = (10,5))\n        for xi in self.x:\n            x_normal = np.linspace(xi - 3*self.b, xi + 3*self.b, precision)\n            y_normal = stats.norm.pdf(x_normal, xi, self.b)\n            plt.plot(x_normal, y_normal, label='Kernel at xi=' + str(xi))\n            \n        plt.ylabel('Kernel Weights wi')\n        plt.xlabel('x')\n        plt.legend()\n    \n    def visualize_predictions(self, precision, X):\n        plt.figure(figsize = (10,5))\n        max_y = 0\n        for xi in self.x:\n            x_normal = np.linspace(xi - 3*self.b, xi + 3*self.b, precision)\n            y_normal = stats.norm.pdf(x_normal, xi, self.b)\n            max_y = max(max(y_normal), max_y)\n            plt.plot(x_normal, y_normal, label='Kernel at xi=' + str(xi))\n            \n        plt.plot([X,X], [0, max_y], 'k-', lw=1,dashes=[2, 2])\n        plt.ylabel('Kernel Weights wi')\n        plt.xlabel('x')\n        plt.legend()","2abd0285":"gkr = GKR([10,20,30,40,50,60,70,80,90,100,110,120], [2337,2750,2301,2500,1700,2100,1100,1750,1000,1642, 2000,1932], 10)","78f1d4ec":"gkr.visualize_kernels(100)","54ef53dc":"gkr.visualize_predictions(100, 50)","2eb6c1b1":"%%time \ngkr.predict(50)","f0d472cd":"%%time\ngkr.predict(11)","faba2697":"%%time\ngkr.predict(100)","11f9f473":"# N-dimensional using numpy\n\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom scipy.stats import multivariate_normal\n\n'''Class for Gaussian Kernel Regression'''\nclass GKR:\n    \n    def __init__(self, x, y, b):\n        self.x = np.array(x)\n        self.y = np.array(y)\n        self.b = b\n    \n    '''Implement the Gaussian Kernel'''\n    def gaussian_kernel(self, z):\n        return (1\/np.sqrt(2*np.pi))*np.exp(-0.5*z**2)\n    \n    '''Calculate weights and return prediction'''\n    def predict(self, X):\n        kernels = np.array([self.gaussian_kernel((np.linalg.norm(xi-X))\/self.b) for xi in self.x])\n        weights = np.array([len(self.x) * (kernel\/np.sum(kernels)) for kernel in kernels])\n        return np.dot(weights.T, self.y)\/len(self.x)\n    \n    def visualize_kernels(self):\n        zsum = np.zeros((120,120))\n        plt.figure(figsize = (10,5))\n        ax = plt.axes(projection = '3d')\n        for xi in self.x:\n            x, y = np.mgrid[0:120:120j, 0:120:120j]\n            xy = np.column_stack([x.flat, y.flat])\n            z = multivariate_normal.pdf(xy, mean=xi, cov=self.b)\n            z = z.reshape(x.shape)\n            zsum += z\n            \n        ax.plot_surface(x,y,zsum)\n            \n        ax.set_ylabel('y')\n        ax.set_xlabel('x')\n        ax.set_zlabel('Kernel Weights wi')\n        plt.legend()","de90214a":"gkr = GKR([[11,15],[22,30],[33,45],[44,60],[50,52],[67,92],[78,107],[89,123],[100,137]], [2337,2750,2301,2500,1700,1100,1000,1642, 1932], 10)","f7e05a39":"gkr.visualize_kernels()","24592895":"%%time\ngkr.predict([50,52])","6e8b1f05":"%%time\ngkr.predict([20,40])","88d692cf":"As you can see below, for each input point $x_i$, we build a Gaussian Kernel with $x_i$ as mean and $b$ (a hyperparameter) as the standard deviation.","1b496e62":"Let's see the code by plugging in the equations and modularizing","67a3652c":"## N-D Feature Vector - using numpy and Euclidean distance","753c681a":"Next, to predict the output for new input $X=50$, we find the weights that the Gaussain of each data point $x_i$ has in the prediction. This can be visualized as the y-axis values corresponding to where the vertical line of $x=50$ intersects the various Gaussians. This is given below. Finally, we multiply the weight vector $w$ with the labels vector $y$ and take its average","98946665":"## Why do we need Kernel Regression?\nFirst, a recap on linear regression. Given a feature vector $x = [x_1, x_2, ... x_n]$, consisting of $n$ features and the corresponding labels $y$, linear regression tries to find the optimal coefficients $c_i, i\\in \\{1, ..., {n+1}\\}$ of the line equation $y = c_1x_1 + c_2x_2 + ... + c_nx_n + c_{n+1}$ usually by gradient descent and measured on the RMSE metric. The equation is then used to predict the target for new unseen inputs.\n\nLinear regression is a simple algorithm that cannot model very complex relationships between the input features. Mathematically, this is because well, it is linear with the degree of the equation being 1, which means that linear regression will always model a straight line.\n\nBut what if our data doesn't have the form of a straight line? In this case, we can use polynomial regression in which the degree of the aforementioned equation is $n, n\\gneq 1$ However, with polynomial regression another problem arises: as a data analyst, you cannot know what the degree of the equation should be (which is trial and error) and moreover, the model built using polyreg is difficult to visualize above degree 3.\n\nSo, what do we do? We use Kernel Regression\n\n## What is Kernel Regression?\nUnlike linear and polynomial regression in which the parameter vector $c$ needs to be learnt, kernel regression is non-parametric, meaning that it calculates the ouput directly from the input given. \n\nHow?\n\nGiven data points $(x_i, y_i)$ Kernel Regression goes about predicting by first constructing a kernel for each data point $x_i$. Then for a given new input $X$, it computes a similarity score with each $x_i$ using the kernel, which acts as a weight $w_i$ that represents the importance of that kernel (and corresponding label $y_i$) in predicting the target corresponding to $X$. The prediction is then obtained by multiplying the weight vector $w$ with $y$ and averaging it.\n\nIn Gaussian Kernel Regression, the kernel $k$ is a Gaussain with mean $x_i$ and deviation $b$ (which is a hyperparameter). Next, the similarity score is obtained using the equation\n\n$$w_i =  {Nk({x_i - X \\over b})\\over {\\sum\\limits_{l=1}^{N} {k({x_l - X \\over b})}}}$$ \n\nwhich implies a higher score if $X$ is closer to $x_i$.\n\nThe prediction is obtained by:\n$$ {1\\over N} \\sum\\limits_{i=1}^{N} w_iy_i$$","7c7c58c1":"## 1-D Feature Vector - using normal Python","46b29812":"For N-dimenisonal inputs, the only modification is in the weight calculating equation where we replace $x_i - X$ with the Euclidean distance $||x_i-X||$","40a6f13e":"Let's predict some values now"}}