{"cell_type":{"8163ea0b":"code","d11dc3da":"code","6682f23f":"code","5e259d0d":"code","3b0b8cfb":"code","72e70e87":"code","a8e0b3a4":"code","6362e453":"code","4d6acf36":"code","b9d3b0c1":"code","496743d0":"code","25d3c481":"code","99bb587b":"code","0f0fa0e7":"code","e1618a4e":"code","e8f45e02":"code","e5232899":"markdown","e1bb4311":"markdown","3d88f756":"markdown","4b133b1d":"markdown","934bc03d":"markdown","66f25ace":"markdown","0cec0c7e":"markdown","54d69a01":"markdown"},"source":{"8163ea0b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d11dc3da":"import pandas as pd\nimport matplotlib.pyplot as plt \nimport numpy as np\n\ndf=pd.read_csv(\"..\/input\/sonardata\/sonar.csv\")\ndf.head(5)\ndf.shape","6682f23f":"from sklearn.model_selection import train_test_split\nX=df.iloc[:,:60]\ny=df.iloc[:,60]\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)","5e259d0d":"from sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\n#Initalize the classifier\n\n#Individual Classifiers\nlog_clf = LogisticRegression(random_state=42)\nknn_clf = KNeighborsClassifier(n_neighbors=10)\nsvm_clf = SVC(gamma=\"auto\", random_state=42, probability=True)\n\n#hardvoting\nvoting_clf = VotingClassifier(\n    estimators=[('lr', log_clf), ('knn', knn_clf), ('svc', svm_clf)],\n    voting='hard')\n\n#softvoting\nvoting_clf_soft = VotingClassifier(\n    estimators=[('lr', log_clf), ('knn', knn_clf), ('svc', svm_clf)],\n    voting='soft')\n\n# Voting can be chnaged to 'Soft', however the classifer must support predict probability","3b0b8cfb":"voting_clf.fit(X_train, y_train)\nvoting_clf_soft.fit(X_train, y_train)\nfrom sklearn.metrics import accuracy_score\n\nfor clf in (log_clf, knn_clf, svm_clf, voting_clf,voting_clf_soft):\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n    a_row = pd.Series([clf.__class__.__name__, accuracy_score(y_test, y_pred)])\n    row_df = pd.DataFrame([a_row])\n    df = pd.concat([row_df, df], ignore_index=False)","72e70e87":"df.iat[0,0]='Soft'","a8e0b3a4":"from sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\ntree_clf = DecisionTreeClassifier(random_state=42)\ntree_clf.fit(X_train, y_train)\ny_pred_tree = tree_clf.predict(X_test)\nprint(accuracy_score(y_test, y_pred_tree))","6362e453":"a_row = pd.Series([tree_clf.__class__.__name__, accuracy_score(y_test, y_pred_tree)])\nrow_df = pd.DataFrame([a_row])\ndf = pd.concat([row_df, df], ignore_index=True)","4d6acf36":"from sklearn.ensemble import BaggingClassifier\nbag_clf = BaggingClassifier(\n    DecisionTreeClassifier(random_state=42), n_estimators=500,\n    max_samples=120, bootstrap=True, n_jobs=-1, random_state=42)\nbag_clf.fit(X_train, y_train)\ny_pred = bag_clf.predict(X_test)\nprint(accuracy_score(y_test, y_pred))","b9d3b0c1":"from sklearn.ensemble import BaggingClassifier\nfrom sklearn.linear_model import LogisticRegression\nbag_clf = BaggingClassifier(\n  LogisticRegression(random_state=40), n_estimators=500,\n    max_samples=120, bootstrap=True, n_jobs=-1, random_state=42)\nbag_clf.fit(X_train, y_train)\ny_pred = bag_clf.predict(X_test)\nprint(accuracy_score(y_test, y_pred))","496743d0":"a_row = pd.Series([bag_clf.__class__.__name__, accuracy_score(y_test, y_pred)])\nrow_df = pd.DataFrame([a_row])\ndf = pd.concat([row_df, df], ignore_index=True)","25d3c481":"from sklearn.ensemble import RandomForestClassifier\nrnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1, random_state=42)\nrnd_clf.fit(X_train, y_train)\ny_pred_rf = rnd_clf.predict(X_test)\nprint(accuracy_score(y_test, y_pred_rf))","99bb587b":"a_row = pd.Series([rnd_clf.__class__.__name__, accuracy_score(y_test, y_pred_rf)])\nrow_df = pd.DataFrame([a_row])\ndf = pd.concat([row_df, df], ignore_index=True)","0f0fa0e7":"nc=np.arange(5,60,5)\nacc=np.empty(11)\ni=0\nfor k in np.nditer(nc):\n    rnd_clf=RandomForestClassifier(n_estimators=500, max_leaf_nodes=int(k), n_jobs=-1, random_state=42)\n    rnd_clf.fit(X_train, y_train)\n    acc[i]=rnd_clf.score(X_test, y_test)\n    i = i + 1\nacc","e1618a4e":"x=pd.Series(acc,index=nc)\nx.plot()\n# Add title and axis names\nplt.title('Random Forest No of leaves vs Accuracy')\nplt.xlabel('Numer of Trees')\nplt.ylabel('Accuracy')\nplt.show()","e8f45e02":"df.columns=['Method','Accuracy']\nfrom itertools import cycle, islice\nmy_colors = list(islice(cycle(['b', 'g', 'r', 'c', 'm', 'y', 'k']), None, len(df)))\ndf.plot.barh(x='Method', y='Accuracy', rot=0,color=my_colors,figsize=(15,8))","e5232899":"# Reading Data","e1bb4311":"# Bagging","3d88f756":"class sklearn.ensemble.BaggingClassifier(base_estimator=None, n_estimators=10, *, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0)","4b133b1d":"# Prediction by single tree","934bc03d":"# Creating Ensembles","66f25ace":"# Splitting in training and Testing","0cec0c7e":"# Testing Ensembles","54d69a01":"# Random Forest"}}