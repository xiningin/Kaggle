{"cell_type":{"6cd070b8":"code","a7b9b49d":"code","de067397":"code","d14a7e57":"code","4bb3381f":"code","6c0e96d9":"code","c81d3c5f":"code","142acb0d":"code","9d6ca375":"code","48a06bee":"code","6c188370":"code","7c1fdc78":"code","0537cece":"code","fb3ddb65":"code","8aeb5510":"code","ae483c3d":"code","9ec583b6":"code","4700fcde":"code","16617ee9":"code","71a915d8":"code","6131b009":"code","9e7b8068":"code","cfe42190":"markdown","1f598082":"markdown","31d2e1ba":"markdown","28943908":"markdown","0930c1ad":"markdown","baf729c9":"markdown","4bf15f6e":"markdown","d35fe7d9":"markdown","3eb1c59e":"markdown","580a7886":"markdown","2cdd26fd":"markdown","b299d1c9":"markdown","1da9f625":"markdown","a12ee441":"markdown"},"source":{"6cd070b8":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom tqdm import tqdm\nfrom sklearn.svm import SVC\n# from keras.models import Sequential\n# from keras.layers.recurrent import LSTM, GRU\n# from keras.layers.core import Dense, Activation, Dropout\n# from keras.layers.embeddings import Embedding\n# from keras.layers.normalization import BatchNormalization\n# from keras.utils import np_utils\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\n# from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n# from keras.preprocessing import sequence, text\n# from keras.callbacks import EarlyStopping\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')","a7b9b49d":"train = pd.read_csv('..\/input\/simplified-fake-news-dataset\/test.csv')\ntest = pd.read_csv('..\/input\/simplified-fake-news-dataset\/train.csv')","de067397":"train.head()","d14a7e57":"test = test.iloc[:, :-1]","4bb3381f":"test.head()","6c0e96d9":"def multiclass_logloss(actual, predicted, eps=1e-15):\n    \"\"\"Multi class version of Logarithmic Loss metric.\n    :param actual: Array containing the actual target classes\n    :param predicted: Matrix with class predictions, one probability per class\n    \"\"\"\n    # Convert 'actual' to a binary array if it's not already:\n    if len(actual.shape) == 1:\n        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n        for i, val in enumerate(actual):\n            actual2[i, val] = 1\n        actual = actual2\n\n    clip = np.clip(predicted, eps, 1 - eps)\n    rows = actual.shape[0]\n    vsota = np.sum(actual * np.log(clip))\n    return -1.0 \/ rows * vsota","c81d3c5f":"lbl_enc = preprocessing.LabelEncoder()\ny = lbl_enc.fit_transform(train.fake.values)\n\ny","142acb0d":"xtrain, xvalid, ytrain, yvalid = train_test_split(train.text.values, y, \n                                                  stratify=y, \n                                                  random_state=42, \n                                                  test_size=0.1, shuffle=True)","9d6ca375":"print (xtrain.shape)\nprint (xvalid.shape)","48a06bee":"%time\n# Always start with these features. They work (almost) everytime!\ntfv = TfidfVectorizer(min_df=3,  max_features=None, \n            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = 'english')\n\n# Fitting TF-IDF to both training and test sets (semi-supervised learning)\ntfv.fit(list(xtrain) + list(xvalid))\nxtrain_tfv =  tfv.transform(xtrain) \nxvalid_tfv = tfv.transform(xvalid)","6c188370":"xtrain_tfv","7c1fdc78":"%time\n\n# Fitting a simple Logistic Regression on TFIDF\nclf = LogisticRegression(C=1.0)\nclf.fit(xtrain_tfv, ytrain)\npredictions = clf.predict_proba(xvalid_tfv)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","0537cece":"%time\nctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), stop_words = 'english')\n\n# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\nctv.fit(list(xtrain) + list(xvalid))\nxtrain_ctv =  ctv.transform(xtrain) \nxvalid_ctv = ctv.transform(xvalid)","fb3ddb65":"%time\n# Fitting a simple Logistic Regression on Counts\nclf = LogisticRegression(C=1.0)\nclf.fit(xtrain_ctv, ytrain)\npredictions = clf.predict_proba(xvalid_ctv)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","8aeb5510":"%time\n# Fitting a simple Naive Bayes on TFIDF\nclf = MultinomialNB()\nclf.fit(xtrain_tfv, ytrain)\npredictions = clf.predict_proba(xvalid_tfv)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","ae483c3d":"%time\n# Fitting a simple Naive Bayes on Counts\nclf = MultinomialNB()\nclf.fit(xtrain_ctv, ytrain)\npredictions = clf.predict_proba(xvalid_ctv)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","9ec583b6":"%time\n# Apply SVD, I chose 120 components. 120-200 components are good enough for SVM model.\nsvd = decomposition.TruncatedSVD(n_components=120)\nsvd.fit(xtrain_tfv)\nxtrain_svd = svd.transform(xtrain_tfv)\nxvalid_svd = svd.transform(xvalid_tfv)\n\n# Scale the data obtained from SVD. Renaming variable to reuse without scaling.\nscl = preprocessing.StandardScaler()\nscl.fit(xtrain_svd)\nxtrain_svd_scl = scl.transform(xtrain_svd)\nxvalid_svd_scl = scl.transform(xvalid_svd)","4700fcde":"%time\n# Fitting a simple SVM\nclf = SVC(C=1.0, probability=True) # since we need probabilities\nclf.fit(xtrain_svd_scl, ytrain)\npredictions = clf.predict_proba(xvalid_svd_scl)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","16617ee9":"%time\n# Fitting a simple xgboost on tf-idf\nclf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nclf.fit(xtrain_tfv.tocsc(), ytrain)\npredictions = clf.predict_proba(xvalid_tfv.tocsc())\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","71a915d8":"%time\n# Fitting a simple xgboost on tf-idf\nclf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nclf.fit(xtrain_ctv.tocsc(), ytrain)\npredictions = clf.predict_proba(xvalid_ctv.tocsc())\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","6131b009":"%time\n# Fitting a simple xgboost on tf-idf svd features\nclf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nclf.fit(xtrain_svd, ytrain)\npredictions = clf.predict_proba(xvalid_svd)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","9e7b8068":"%time\n# Fitting a simple xgboost on tf-idf svd features\nclf = xgb.XGBClassifier(nthread=10)\nclf.fit(xtrain_svd, ytrain)\npredictions = clf.predict_proba(xvalid_svd)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","cfe42190":"using \n\n### count vectoriser next.\n\nCountVectorizer is a great tool provided by the scikit-learn library in Python. It is used to transform a given text into a vector on the basis of the frequency (count) of each word that occurs in the entire text.","1f598082":"\nOne more ancient algorithms in the list is \n\n### SVMs.\n\nSVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.\n\nSome people \"love\" SVMs. So, we must try SVM on this dataset.\n\nSince SVMs take a lot of time, we will reduce the number of features from the TF-IDF using Singular Value Decomposition before applying SVM.\n\nAlso, note that before applying SVMs, we must standardize the data.","31d2e1ba":"Now it's time to apply SVM. After running the following cell, feel free to go for a walk or talk to your girlfriend\/boyfriend. :P","28943908":"These are the simple ways in which you can tackle from any NLP dataset.","0930c1ad":"## Encoding labels\n\nWe use the LabelEncoder from scikit-learn to convert text labels to integers, 0, 1 2. In this case its only 0 or 1. But this is an important step.","baf729c9":"More improvement!\n\nin statistics,\n\n### naive Bayes classifiers \n\nare a family of simple \"probabilistic classifiers\" based on applying Bayes' theorem with strong (na\u00efve) independence assumptions between the features (see Bayes classifier). They are among the simplest Bayesian network models,[1] but coupled with kernel density estimation, they can achieve higher accuracy levels.\n\nLet's see what happens when we use naive bayes on these two datasets:","4bf15f6e":"### Parameter for judgement\nLots of time in competettions kaggle implements a metric for judging the results. Think of multiclass logloss as a metric.","d35fe7d9":"## Building Basic Models\nLet's start building our very first model.\n\nOur very first model is a simple TF-IDF (Term Frequency - Inverse Document Frequency) followed by a simple Logistic Regression.","3eb1c59e":"# Approaching an NLP program - in practice.\n\n**WARNING: some of the cells take along time to run so look at their time before starting the execution!\n**\n\n![](https:\/\/i.ytimg.com\/vi\/-BnqU5K9t5M\/maxresdefault.jpg)\n\nIn this I am using guidance from this detailed notebook : https:\/\/www.kaggle.com\/abhishek\/approaching-almost-any-nlp-problem-on-kaggle\/data. This notebook is hugely popular but I want to apply it on a custom dataset.\n\n## Dataset source\n\nWe will be trying from https:\/\/www.kaggle.com\/clmentbisaillon\/fake-and-real-news-dataset. I converted the dataset given into what we need using this notebook: https:\/\/www.kaggle.com\/fanbyprinciple\/fake-and-real-dataset.\n","580a7886":"### train_test_split\nsplitting into train, test and validation sets.","2cdd26fd":"## Things to try next\n\n1. grid search for finding right parameters\n2. neural networks","b299d1c9":"### Removing the label\n\nIn test data we already have the label. So we need to remove it.","1da9f625":"Xgboost also performs reasonably well.","a12ee441":"0.043 is good.\nBefore moving further, lets apply the most popular algorithm on Kaggle: \n\n### xgboost!\n\nXGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way."}}