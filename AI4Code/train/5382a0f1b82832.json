{"cell_type":{"781767ba":"code","9290c0dc":"code","7d4935d1":"code","9dacfc79":"code","184fe36b":"code","94ef0665":"code","ed0ef5b7":"code","56768d8b":"code","1f43a508":"code","bf5ad185":"code","c1761199":"code","5b0c0c23":"code","add4d2a4":"code","75a24457":"code","7c17eaf0":"code","48acd994":"code","8d168f3e":"code","57e40f9c":"code","392f0000":"code","3e35fd97":"code","cf431652":"code","619b5d41":"code","6da08411":"code","82e1b91a":"code","a762cdef":"code","1366bca3":"code","23351271":"code","95b5ba4c":"code","239b6617":"code","28b89d84":"code","4ced199c":"code","91e1177e":"code","b0fce939":"code","9efc26a0":"code","ef28d193":"code","cb62c102":"code","62b2ee2b":"code","58ce2bc3":"code","a63df186":"code","7d8f8dc8":"code","3fc03bc9":"code","bc599b7e":"code","35b57f64":"code","720dbceb":"code","85cf2205":"code","79d88875":"code","f655b4f0":"code","3491b256":"code","4a385c20":"code","01f55c7c":"code","5bc76f52":"code","65631c7d":"code","9b1e90d6":"code","243666a9":"code","070d898a":"code","5c24f367":"code","423987e8":"code","11c20f94":"code","e44eb65c":"code","3d9c4571":"code","38cdfe8b":"markdown","fad5df7c":"markdown","00d17dc3":"markdown","a2472787":"markdown","93b11a79":"markdown","c5cd64a8":"markdown","6bc297af":"markdown","06f0c463":"markdown","eb7dad50":"markdown","a4cf9eee":"markdown","640301eb":"markdown","b1af15c9":"markdown","bb925c37":"markdown"},"source":{"781767ba":"import pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom collections import defaultdict\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\n\n%matplotlib inline","9290c0dc":"# Check the mpl version (3.1.1 causes issues with seaborn)\nmatplotlib.__version__","7d4935d1":"# command for readable pandas formatting\npd.options.display.float_format = \"{:.2f}\".format","9dacfc79":"# Load the data\ndf = pd.read_csv('..\/input\/big-five-personality-test\/IPIP-FFM-data-8Nov2018\/data-final.csv', sep='\\t')","184fe36b":"# Inspect the data\ndf.head(1)","94ef0665":"# Inspect the metadata.\ndf.info(verbose=False)","ed0ef5b7":"# Inspect the data shape\nprint('Number of rows:',df.shape[0])","56768d8b":"# Classify the columns to categorical and numerical\nnum_cols = df._get_numeric_data().columns\ncat_cols = [col for col in df.columns if col not in num_cols]\nprint('Number of columns:',len(df.columns),\n      f' (numerical:{len(num_cols)},',\n      f' categorical:{len(cat_cols)})')","1f43a508":"# First step of cleaning- IPC.\n# Limit the analysis to IPC =1 to get rid of duplicated submissions.\n\"\"\"\nAs per Kaggle dataset description:\nThe number of records from the user's IP address in the dataset. \nFor max cleanliness, only use records where this value is 1. \nHigh values can be because of shared networks (e.g. entire universities) or multiple submissions\n\"\"\"\ndf = df.loc[df['IPC']==1]","bf5ad185":"# Get rid of invalid results \n# As the answers are in scale 1 to 5, we want to delete invalid inputs \ndf = df.loc[(df[df.columns.tolist()[:49]] >= 1).all(axis=1)]","c1761199":"# positive questions adding to the trait.\npos_questions = [ \n    'EXT1','EXT3','EXT5','EXT7','EXT9',                       # 5\n    'EST1','EST3','EST5','EST6','EST7','EST8','EST9','EST10', # 8\n    'AGR2','AGR4','AGR6','AGR8','AGR9','AGR10',               # 6\n    'CSN1','CSN3','CSN5','CSN7','CSN9','CSN10',               # 6\n    'OPN1','OPN3','OPN5','OPN7','OPN8','OPN9','OPN10',        # 7\n]\n\n# negative (negating) questions subtracting from the trait.\nneg_questions = [ \n    'EXT2','EXT4','EXT6','EXT8','EXT10', # 5\n    'EST2','EST4',                       # 2\n    'AGR1','AGR3','AGR5','AGR7',         # 4\n    'CSN2','CSN4','CSN6','CSN8',         # 4\n    'OPN2','OPN4','OPN6',                # 3\n]\n\n# Replace the question answer with -2 to 2 scale depending if the question is positive or negative.\ndf[pos_questions] = df[pos_questions].replace({1:-2, 2:-1, 3:0, 4:1, 5:2})\ndf[neg_questions] = df[neg_questions].replace({1:2, 2:1, 3:0, 4:-1, 5:-2})","5b0c0c23":"# Check for missing data.\ndf.isna().mean().sum()","add4d2a4":"df = df.dropna()\ndf.isna().mean().sum()","75a24457":"# columns with time spent answering questions\nqtime_cols = list(df.columns)[50:100]","7c17eaf0":"# Check if selected correct columns\nqtime_cols[0], qtime_cols[-1]","48acd994":"# Calculate the total time for each survey\ndf['total_time']=df[qtime_cols].sum(axis=1)","8d168f3e":"df['total_time'].describe()","57e40f9c":"# Can't see anything due to large outliers\nax = sns.distplot(df['total_time'])","392f0000":"# See how much data will be lost if we get rid of the outliers\ntotal_respondents = len(df)\nfast_respondents = len(df[df['total_time']<10000])\nslow_respondents = len(df[df['total_time']>1000000])\n\nprint(\"Total respondents:\",total_respondents)\nprint(\"Slowest respondents:\",slow_respondents\/total_respondents)\nprint(\"Fastest respondents:\",fast_respondents\/total_respondents)","3e35fd97":"df = df[df['total_time'].between(10000,1000000)]","cf431652":"from matplotlib import style\nstyle.use(\"seaborn-darkgrid\")\ndf[['total_time']].plot(kind='hist',bins=20)\nplt.title('Test completion times')\nplt.show()","619b5d41":"# List the redundant cols such as longitude and latitudee\ndrop_cols=list(df.columns[50:107])+['lat_appx_lots_of_err','long_appx_lots_of_err']","6da08411":"# Drop the redundant cols\ndf=df.drop((drop_cols), axis=1)","82e1b91a":"df","a762cdef":"# List the number of unique countries, count them\ncountries = df['country'].unique()\nlen(countries)","1366bca3":"# A list of all EU countries, count them\nEU = [\"AT\", \"BE\", \"BG\", \"CY\", \"CZ\", \"DE\", \"DK\", \"EE\", \"ES\", \"FI\", \"FR\", \"GB\", \"GR\", \"HR\", \"HU\", \"IE\", \"IT\", \"LT\", \"LU\", \"LV\", \"MT\", \"NL\", \"PL\", \"PT\", \"RO\", \"SE\", \"SI\", \"SK\"]\nlen(EU)","23351271":"# Check if all EU countries are in the data\nintersection = set(EU).intersection(set(countries))\nlen(intersection)","95b5ba4c":"# Limit the analysis to EU countries\ndf = df.loc[df['country'].isin(EU)]","239b6617":"# Count responses by country\ndf['country'].value_counts()[:5]","28b89d84":"# This gives us percentage of responses from each country\ndf['country'].value_counts(normalize=True) * 100","4ced199c":"df.head()","91e1177e":"# Create an aggregated feature for each of the five personality dimensions.\n# They will average the 10 answers across the dimension.\n\n# Extraversion \nEXT = list(df.columns[:10])\n# Emotional Stability\nEST = list(df.columns[10:20])\n# Agreeableness\nAGR = list(df.columns[20:30])\n# Conscientiousness\nCSN = list(df.columns[30:40])\n# Openness\nOPN = list(df.columns[40:50])\n\ndimensions = [EXT,EST,AGR,CSN,OPN]\ndimension_averages=[\"extraversion\",\"emotional_stability\",\n       \"agreeableness\",\"conscientiousness\",\"openness\"]","b0fce939":"for d in range(len(dimensions)):\n    df[dimension_averages[d]] = df[dimensions[d]].mean(axis=1)","9efc26a0":"df.head(1)","ef28d193":"# Analyse the aggregated features\ndf[dimension_averages].describe()","cb62c102":"# Use a boxlot to visualise the 5 variables\n# This method will give us a good overview of the distribution across the variables\nsns.set_style(\"darkgrid\")\n\n#reset default parameters\nsns.set()\nplt.figure(figsize=(12, 6))\nsns.set(font_scale=1.5)\nsns.boxplot(data=df[dimension_averages]);\nplt.title(\"Average characteristics of European citizens\",fontsize=22)\nplt.savefig('avg_char.png')\nplt.show()","62b2ee2b":"#reset default parameters\nsns.set()\nplt.figure(figsize=(12, 6))\n\n# Visualise the correlation\ncorr=df[dimension_averages].corr()\nmask = np.triu(corr)\nsns.set(font_scale=1.2)\nsns.heatmap(df[dimension_averages].corr(),\n            vmin=0,\n            vmax=1,\n            annot = True,\n            square=True, \n            mask=mask,\n            cbar=True,\n            cmap='Blues')\nplt.title('Correlation between personality traits',fontsize=22)\nplt.savefig('correlations.png')\nplt.show()","58ce2bc3":"# Subset df to only those with country GB, PL\ngb = df.loc[df['country']==\"GB\"]\npl = df.loc[df['country']==\"PL\"]","a63df186":"# Limit the analysis to two countries and averages across 5 dimensions\ngb = gb[gb.columns[-6:]]\npl = pl[pl.columns[-6:]]","7d8f8dc8":"def transpose_table(df, col_list):\n    \"\"\"\n    INPUT \n        df - a dataframe holding the col_list columns\n        col_list- columns that we want to transpose into rows\n        \n    OUTPUT\n        new_df- a transposed dataframe.\n    \"\"\"\n    new_df = defaultdict(int)\n    for i in col_list:\n        new_df[i]=df[i].mean()\n    new_df = pd.DataFrame(pd.Series(new_df)).reset_index()\n    new_df.rename(columns={'index': 'personality', 0: 'average'}, inplace=True)\n    new_df.set_index('personality', inplace=True)\n    return new_df ","3fc03bc9":"dimension_averages","bc599b7e":"gb_avg = transpose_table(gb,dimension_averages)\npl_avg = transpose_table(pl,dimension_averages)\ncomp_df = pd.merge(gb_avg, pl_avg, left_index=True, right_index=True)\ncomp_df.columns = ['gb_avg', 'pl_avg']\ncomp_df['value_difference'] = comp_df['gb_avg'] - comp_df['pl_avg']\ncomp_df.style.bar(subset=['value_difference'], align='mid', color=['#d65f5f', '#5fba7d'])","35b57f64":"df.head()","720dbceb":"# Add binary column to indicate if Great Britain \ndf['is_gb'] = df['country'].apply(lambda x: 1 if x =='GB' else 0)","85cf2205":"# Copy the dataframe\ndf_ml = df.copy()\n\nto_drop =[\"country\",\"total_time\"]\n          #+[\"extraversion\",\"emotional_stability\",\"agreeableness\",\"conscientiousness\",\"openness\"]\n    \n# Delete old column indicating country\ndf_ml = df_ml.drop(columns=to_drop)\n\n# Shuffle the data to ensure that split is fair\ndf_ml = df_ml.sample(n=len(df_ml),random_state=42)","79d88875":"corr_data = pd.DataFrame(df_ml.corr()['is_gb'][:])","f655b4f0":"corr_data = corr_data.reset_index()","3491b256":"corr_data = corr_data.sort_values(by=['is_gb'])","4a385c20":"corr_data[:3]","01f55c7c":"corr_data[-4:-1]","5bc76f52":"top_correlation = corr_data.sort_values('is_gb', ascending=False).head(10)['index'].to_list()\nleast_correlation = corr_data.sort_values('is_gb', ascending=False).tail(5)['index'].to_list()","65631c7d":"# Count the outcome variables to identify the baseline\npositives = len(df.loc[df_ml['is_gb']==1])\nnegatives = len(df.loc[df_ml['is_gb']==0])\n1-(positives\/(positives+negatives))","9b1e90d6":"# Select the dependent variable\nY = df_ml['is_gb']\nX = df_ml.drop('is_gb',axis=1)","243666a9":"# Split the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)","070d898a":"import xgboost as xgb\n\n# Instantiate the model\nxgb_model = xgb.XGBClassifier(learning_rate=0.05, \n              max_depth=3,\n              gamma=0.08435594187707007,\n              colsample_bytree=0.5336629698328548,\n              n_estimators=1000, \n              objective='binary:logistic', \n              random_state=42)\n\n# fit model to training data\nxgb_model.fit(X_train, y_train)","5c24f367":"# make predictions for test data\ny_pred = xgb_model.predict(X_test)\npredictions = [round(value) for value in y_pred]\n\n# evaluate predictions\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","423987e8":"# Find optimal threshold\nthresholds=(np.linspace(0.45,0.50,20))\nfor t in thresholds:\n    predictions=xgb_model.predict_proba(X_test)[:,1]>t\n    print(\"AUC for threshold\",t,\":\",\n         roc_auc_score(y_test, predictions))\n    accuracy = accuracy_score(y_test, predictions)\n    print(\"XGB Classifier accuracy: %.2f%%\" % (accuracy * 100.0))","11c20f94":"69\/61","e44eb65c":"# Check the most important features\nimportance = xgb_model.get_booster().get_score(importance_type= 'gain')\nsorted(importance.items(), key=lambda x:x[1],reverse=True)[:3]","3d9c4571":"gb_df = df.loc[df['is_gb']==1]\neu_df = df.loc[df['is_gb']==0]\ncomp_metrics = ['AGR3','CSN8','EST9']\n\ngb_df = transpose_table(gb_df,comp_metrics)\neu_df = transpose_table(eu_df,comp_metrics)\ncomp_df = pd.merge(gb_df, eu_df, left_index=True, right_index=True)\ncomp_df.columns = ['gb_avg','eu_avg']\ncomp_df['value_difference'] = comp_df['gb_avg'] - comp_df['eu_avg']\ncomp_df.style.bar(subset=['value_difference'], align='mid', color=['#d65f5f', '#5fba7d'])","38cdfe8b":"# Feature normalization","fad5df7c":"Credits: Tyler B https:\/\/www.kaggle.com\/bluewizard\/scoring-the-big-five-personality-test-items\n","00d17dc3":"# VISUALISE THE DATA","a2472787":"# CORRELATIONS","93b11a79":"# ANALYSE THE DATA","c5cd64a8":"# IMPORTS","6bc297af":"# INSPECT THE DATA","06f0c463":"# FEATURE AGGREGATION","eb7dad50":"# XGBOOST","a4cf9eee":"# CLEAN THE DATA","640301eb":"<b> Question code mapping\n    \nAGR3: I insult people\n    \nCSN8: I shirk my duties\n    \nEST9: I get irritated easily","b1af15c9":"# MODELLING","bb925c37":"**See GitHub for summary findings:**\n[https:\/\/github.com\/gajdulj\/personalityanalysis](http:\/\/)\n\n**Associated Medium article: **\n[https:\/\/medium.com\/@jakubgajdul\/4-things-that-data-tells-us-about-our-personalities-210cdd8f71f](http:\/\/)"}}