{"cell_type":{"96c64c5a":"code","6a6c7430":"code","c7eed2f6":"code","e1c8bade":"code","58bb8fcd":"code","81692871":"code","d3ba49b8":"code","7393cd34":"code","7dcc82e9":"code","32350b59":"code","45d81655":"code","186df4c1":"code","66a6d641":"code","be9366c4":"code","f6d52e18":"code","17692550":"code","b67a1690":"code","a73f7492":"code","59dea0f6":"code","aedbfa5c":"code","77877150":"code","e92cf9f1":"code","5324cd22":"code","e8686110":"code","ec07d07c":"code","a9cbffdd":"code","3a041bca":"code","7bb6dfe3":"code","05be9803":"code","a9ba73f4":"code","d8169fc1":"code","91bd8c7b":"code","fda80571":"code","786aa73c":"markdown","949d338d":"markdown","c308612f":"markdown","14c2bfed":"markdown","d08b76b2":"markdown","92f10f23":"markdown","f270735e":"markdown","81c9b5bf":"markdown","e96a1b7e":"markdown","19c9dd22":"markdown","276f74c1":"markdown","56ff8c58":"markdown","07137366":"markdown","87a7307a":"markdown"},"source":{"96c64c5a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport os\nimport gc\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_predict, cross_val_score\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom lightgbm import LGBMClassifier, plot_importance\nfrom xgboost import XGBClassifier\n\nfrom sklearn.decomposition import PCA\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nalready_preprocessed=False\ntrain_file='train.csv'\ntest_file='test.csv'\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        if filename==train_file:\n            train_dir=dirname\n        if filename==test_file:\n            test_dir=dirname\n            \n# Any results you write to the current directory are saved as output.","6a6c7430":"import pip._internal as pip\npip.main(['install', '--upgrade', 'numpy==1.17.2'])\n\nimport numpy as np\nprint('> NumPy version: {}'.format(np.__version__))","c7eed2f6":"# Reading the csv file with the whole dataset\ndata = pd.read_csv(train_dir+'\/'+train_file, sep=',', header=0, index_col=0)\n# Train data, erasing label column\nX_train = data.drop('Cover_Type', axis=1)\ny_train = data['Cover_Type']\n# Read the test data\ntest = pd.read_csv(test_dir+'\/'+test_file, sep=',', header=0, index_col=0)\n\nn_train=len(X_train)","e1c8bade":"from sklearn.preprocessing import StandardScaler\nall_data=X_train.append(test)\n\nscaler = StandardScaler().fit(all_data)\ntest_scaled = scaler.transform(test)\nX_train_scaled = scaler.transform(X_train)","58bb8fcd":"from sklearn.mixture import GaussianMixture\n# Num clusters o components?  \n# Standarscaler for test data?\n\ngmix = GaussianMixture(n_components=10)\ngmix.fit(test_scaled)\n\nx_train_gmix = gmix.predict(X_train_scaled)\ntest_gmix = gmix.predict(test_scaled)","81692871":"X_train['Cluster_Test']=x_train_gmix\ntest['Cluster_Test']=test_gmix\n\n#Convert to categorical\nX_train = pd.get_dummies(X_train,columns = ['Cluster_Test'])\ntest = pd.get_dummies(test,columns = ['Cluster_Test'])","d3ba49b8":"#Apply PCA to obtain new features based in all datasets (train and test)\ndef add_PCA_features(X):\n    pca = PCA(n_components=0.99, random_state=0).fit(X)\n    X_pca = pca.transform(X)\n    \n    return X_pca\n\ncomponents = add_PCA_features(all_data)\n\nprint('PCA components dimension: ',components.shape)\n\nfor i in range(components.shape[1]):\n    col_name= 'pca'+str(i+1)\n    X_train[col_name] = components[:n_train, i]\n    test[col_name] = components[n_train:, i]\n\nprint(test.shape,X_train.shape)","7393cd34":"# Plot the feature importance determined by the classifier clf\ndef feature_importances(clf, X, y):\n    clf = clf.fit(X, y)\n    \n    importances = pd.DataFrame({'Features': X.columns, \n                                'Importances': clf.feature_importances_})\n    \n    importances.sort_values(by=['Importances'], axis='index', ascending=False, inplace=True)\n\n    fig = plt.figure(figsize=(14, 4))\n    sns.barplot(x='Features', y='Importances', data=importances)\n    plt.xticks(rotation='vertical')\n    plt.show()\n    \n# Calculate the accuracy using a cross validated approach \ndef cv_accuracy(clf, X, y):\n    scores = cross_val_score(clf, X, y, \n                             cv=5, \n                             scoring='accuracy',\n                             verbose=0, \n                             n_jobs=-1)\n    \n    print('Cross-validation accuracy: {:.3f} {}'.format(np.mean(scores),  \n                                                        np.around(scores, 3)))\n\n# Shows the confusion matrix using criss validated predictions\ndef cv_confusion(clf, X, y):\n    prediction = cross_val_predict(clf, X, y, \n                                   cv=5, \n                                   verbose=0, \n                                   n_jobs=-1)\n    \n    classes = sorted(y.unique().tolist())\n\n    conf_mat = confusion_matrix(y, prediction)\n    norm_conf_mat = np.around(conf_mat \/ conf_mat.sum(axis=1), 2)\n\n    fig = plt.figure(figsize=(14, 8))\n\n    fig.add_subplot(1, 2, 1)\n    ax = sns.heatmap(norm_conf_mat, annot=True, cmap='OrRd', \n                     xticklabels=classes, yticklabels=classes)\n    ax.set(xlabel='Predicted Class', ylabel='True Class', title='Normalized')\n\n\n    fig.add_subplot(1, 2, 2)\n    ax = sns.heatmap(conf_mat, annot=True, fmt='d', cmap='OrRd', \n                     xticklabels=classes, yticklabels=classes)\n    ax.set(xlabel='Predicted Class', ylabel='True Class', title ='Counts')\n\n    #plt.tight_layout()\n    plt.show()\n    \n# Plot correlations between numerical features\ndef plot_correlations(X, annot=False):\n    numericals = [col for col in X.columns if \n                  'Soil_Type' not in col and \n                  'Wilderness_Area' not in col]\n\n    numericals = sorted(numericals)\n\n    # Place correlations in four bins\n    corr = np.around(X[numericals].corr().abs(), 1)\n    \n    corr_bin = corr.copy()\n    corr_bin = corr_bin.where(corr_bin > 0.30, 0.30)\n\n    corr_bin = corr_bin.where((corr_bin <= 0.30) | \n                              (corr_bin > 0.50), 0.50)\n\n    corr_bin = corr_bin.where((corr_bin <= 0.50) | \n                              (corr_bin > 0.70), 0.70)\n\n    corr_bin = corr_bin.where(corr_bin <= 0.70, 1.0)\n    \n    if annot:\n        annot = corr\n        \n    # Show binned correlation plot\n    fig = plt.figure(figsize=(12, 12))\n    sns.heatmap(corr_bin, annot=annot, linewidths=1, square=True, \n                cmap='BuPu', cbar_kws={'shrink':0.5})\n    plt.title('Feature Correlations')\n    plt.show()\n\n# Drop features with mode frequency > 99% of data\n# Those columns are irrelevant, they have almost just one value \ndef drop_unimportant(X_):\n    X = X_.copy()\n    \n    n_rows = X.shape[0]\n    hi_freq_cols = []\n    \n    for col in X.columns:\n        mode_frequency = 100.0 * X[col].value_counts().iat[0] \/ n_rows \n        \n        if mode_frequency > 99.0:\n            hi_freq_cols.append(col)\n            \n    X = X.drop(hi_freq_cols, axis='columns')\n    \n    return hi_freq_cols,X\n\ndef drop_correlated(X_):\n    X = X_.copy()\n   \n    drop_cols=['Hillshade_9am','Hillshade_Noon','Hillshade_3pm','Vertical_Distance_To_Hydrology','Horizontal_Distance_To_Hydrology',\n              'Horizontal_Distance_To_Fire_Points', 'Horizontal_Distance_To_Roadways']\n    \n    drop_cols = [col for col in drop_cols if col in X.columns]\n    \n    X = X.drop(drop_cols, axis='columns')\n    \n    return drop_cols,X","7dcc82e9":"#This classifier is used to evaluate the performance of the features in diferent scenarios: importance, correlation and so.\nclf = RandomForestClassifier(n_estimators=125,\n                             min_samples_leaf=1,\n                             max_depth=None,\n                             verbose=0,\n#                             class_weight ={1:0.4,2:0.4,3:0.05,4:0.05,5:0.05,6:0.05,7:0.01},\n                             random_state=0)","32350b59":"#feature_importances(clf, X_train, y_train)\n#cv_accuracy(clf, X_train, y_train)\n#cv_confusion(clf, X_train, y_train)","45d81655":"# This new features has been extracted from the kernel previously citated:\n# https:\/\/www.kaggle.com\/kwabenantim\/forest-cover-stacking-multiple-classifiers\n\ndef add_features(X_):\n    X = X_.copy()\n    \n    X['Hydro_Elevation_sum'] = X[['Elevation',\n                                  'Vertical_Distance_To_Hydrology']\n                                 ].sum(axis='columns')\n    \n    X['Hydro_Elevation_diff'] = X[['Elevation',\n                                   'Vertical_Distance_To_Hydrology']\n                                  ].diff(axis='columns').iloc[:, [1]]\n\n    X['Hydro_Euclidean'] = np.sqrt(X['Horizontal_Distance_To_Hydrology']**2 +\n                                   X['Vertical_Distance_To_Hydrology']**2)\n\n    X['Hydro_Manhattan'] = (X['Horizontal_Distance_To_Hydrology'] +\n                            X['Vertical_Distance_To_Hydrology'].abs())\n    \n    \n    X['Hydro_Distance_sum'] = X[['Horizontal_Distance_To_Hydrology',\n                                 'Vertical_Distance_To_Hydrology']\n                                ].sum(axis='columns')\n\n    X['Hydro_Distance_diff'] = X[['Horizontal_Distance_To_Hydrology',\n                                  'Vertical_Distance_To_Hydrology']\n                                 ].diff(axis='columns').iloc[:, [1]]\n    \n    X['Hydro_Fire_sum'] = X[['Horizontal_Distance_To_Hydrology',\n                             'Horizontal_Distance_To_Fire_Points']\n                            ].sum(axis='columns')\n\n    X['Hydro_Fire_diff'] = X[['Horizontal_Distance_To_Hydrology',\n                              'Horizontal_Distance_To_Fire_Points']\n                             ].diff(axis='columns').iloc[:, [1]].abs()\n\n    X['Hydro_Fire_mean'] = X[['Horizontal_Distance_To_Hydrology',\n                              'Horizontal_Distance_To_Fire_Points']\n                             ].mean(axis='columns')\n\n    X['Hydro_Fire_median'] = X[['Horizontal_Distance_To_Hydrology',\n                                'Horizontal_Distance_To_Fire_Points']\n                               ].median(axis='columns')\n                               \n    X['Hydro_Road_sum'] = X[['Horizontal_Distance_To_Hydrology',\n                             'Horizontal_Distance_To_Roadways']\n                            ].sum(axis='columns')\n\n    X['Hydro_Road_diff'] = X[['Horizontal_Distance_To_Hydrology',\n                              'Horizontal_Distance_To_Roadways']\n                             ].diff(axis='columns').iloc[:, [1]].abs()\n\n    X['Hydro_Road_mean'] = X[['Horizontal_Distance_To_Hydrology',\n                              'Horizontal_Distance_To_Roadways']\n                             ].mean(axis='columns')\n\n    X['Hydro_Road_median'] = X[['Horizontal_Distance_To_Hydrology',\n                                'Horizontal_Distance_To_Roadways']\n                               ].median(axis='columns')\n    \n    X['Road_Fire_sum'] = X[['Horizontal_Distance_To_Roadways',\n                            'Horizontal_Distance_To_Fire_Points']\n                           ].sum(axis='columns')\n\n    X['Road_Fire_diff'] = X[['Horizontal_Distance_To_Roadways',\n                             'Horizontal_Distance_To_Fire_Points']\n                            ].diff(axis='columns').iloc[:, [1]].abs()\n\n    X['Road_Fire_mean'] = X[['Horizontal_Distance_To_Roadways',\n                             'Horizontal_Distance_To_Fire_Points']\n                            ].mean(axis='columns')\n\n    X['Road_Fire_median'] = X[['Horizontal_Distance_To_Roadways',\n                               'Horizontal_Distance_To_Fire_Points']\n                              ].median(axis='columns')\n    \n    X['Hydro_Road_Fire_mean'] = X[['Horizontal_Distance_To_Hydrology',\n                                   'Horizontal_Distance_To_Roadways',\n                                   'Horizontal_Distance_To_Fire_Points']\n                                  ].mean(axis='columns')\n\n    X['Hydro_Road_Fire_median'] = X[['Horizontal_Distance_To_Hydrology',\n                                     'Horizontal_Distance_To_Roadways',\n                                     'Horizontal_Distance_To_Fire_Points']\n                                    ].median(axis='columns')\n\n    X['Hillshade_sum'] = X[['Hillshade_9am',\n                            'Hillshade_Noon',\n                            'Hillshade_3pm']\n                           ].sum(axis='columns')\n\n    X['Hillshade_mean'] = X[['Hillshade_9am',\n                             'Hillshade_Noon',\n                             'Hillshade_3pm']\n                            ].mean(axis='columns')\n\n    X['Hillshade_median'] = X[['Hillshade_9am',\n                               'Hillshade_Noon',\n                               'Hillshade_3pm']\n                              ].median(axis='columns')\n\n    X['Hillshade_min'] = X[['Hillshade_9am',\n                            'Hillshade_Noon',\n                            'Hillshade_3pm']\n                           ].min(axis='columns')\n\n    X['Hillshade_max'] = X[['Hillshade_9am',\n                            'Hillshade_Noon',\n                            'Hillshade_3pm']\n                           ].max(axis='columns')\n    \n    X['Hillshade_std'] = X[['Hillshade_9am',\n                            'Hillshade_Noon',\n                            'Hillshade_3pm']\n                           ].std(axis='columns')\n    \n    # Compute Soil_Type number from Soil_Type binary columns\n    X['Stoneyness'] = sum(i * X['Soil_Type{}'.format(i)] for i in range(1, 41))\n    \n    # For all 40 Soil_Types, 1=rubbly, 2=stony, 3=very stony, 4=extremely stony, 0=?\n    stoneyness = [4, 3, 1, 1, 1, 2, 0, 0, 3, 1, \n                  1, 2, 1, 0, 0, 0, 0, 3, 0, 0, \n                  0, 4, 0, 4, 4, 3, 4, 4, 4, 4, \n                  4, 4, 4, 4, 1, 4, 4, 4, 4, 4]\n    \n    # Replace Soil_Type number with \"stoneyness\" value\n    X['Stoneyness'] = X['Stoneyness'].replace(range(1, 41), stoneyness)\n\n    return X","186df4c1":"#Aspect\ndef transform_Aspect(X_):\n    X = X_.copy()\n    \n    X['Aspect'] = X['Aspect'].astype(int) % 360\n    \n    from bisect import bisect\n    \n    cardinals = [i for i in range(45, 361, 90)]\n    points = ['N', 'E', 'S', 'W']\n    \n    X['Cardinal'] = X.Aspect.apply(lambda x: points[bisect(cardinals, x) % 4])\n    X.loc[:,'North']= X['Cardinal']=='N'\n    X.loc[:,'East']= X['Cardinal']=='E'\n    X.loc[:,'West']= X['Cardinal']=='W'\n    X.loc[:,'South']= X['Cardinal']=='S'\n    \n    #X['Sin_Aspect'] = np.sin(np.radians(X['Aspect'])) # not important feature at all\n    X['Cos_Aspect'] = np.cos(np.radians(X['Aspect']))\n    \n    return X\n\ndef transform_Slope(X_):\n    X = X_.copy()\n    \n    X['Slope_hyd'] = np.arctan(X['Vertical_Distance_To_Hydrology']\/(X['Horizontal_Distance_To_Hydrology']+0.001))\n    X.Slope_hyd=X.Slope_hyd.map(lambda x: 0 if np.isinf(x) else x) # remove infinite value if any\n    \n    X['Sin_Slope_hyd'] = np.sin(np.radians(X['Slope_hyd']))\n    X['Cos_Slope_hyd'] = np.cos(np.radians(X['Slope_hyd']))\n\n    #X['Sin_Slope'] = np.sin(np.radians(X['Slope'])) # not important feature at all\n    X['Cos_Slope'] = np.cos(np.radians(X['Slope']))\n    \n    return X","66a6d641":"# Add the new features to the dataset\nX_train = add_features(X_train)\ntest = add_features(test)\n\nprint(test.shape,X_train.shape)\n\nfeature_importances(clf, X_train, y_train)\ncv_accuracy(clf, X_train, y_train)","be9366c4":"#Convert to categorical\nX_train = pd.get_dummies(X_train,columns = ['Stoneyness'])\ntest = pd.get_dummies(test,columns = ['Stoneyness'])\nprint(test.shape,X_train.shape)","f6d52e18":"X_train.drop(['Soil_Type{}'.format(i) for i in range(1, 41)], axis='columns', errors='ignore', inplace=True)\ntest.drop(['Soil_Type{}'.format(i) for i in range(1, 41)], axis='columns', errors='ignore', inplace=True)\nprint(test.shape,X_train.shape)","17692550":"# Add the new features to the dataset\n#X_train = transform_Aspect(X_train)\n#test = transform_Aspect(test)\n\n# Dropping Cardinal columns\n#X_train.drop('Cardinal',axis=1, inplace=True)\n#test.drop('Cardinal',axis=1, inplace=True)\n\n#print(test.shape,X_train.shape)\n#cv_accuracy(clf, X_train, y_train)","b67a1690":"# Add the new features to the dataset\n#X_train = transform_Slope(X_train)\n#test = transform_Slope(test)\n\n#print(test.shape,X_train.shape)\n#cv_accuracy(clf, X_train, y_train)","a73f7492":"#Only execute in training or evaluating new features\n\n#feature_importances(clf, X_train, y_train)\n#cv_accuracy(clf, X_train, y_train)\n#cv_confusion(clf, X_train, y_train)","59dea0f6":"# Drop umportant columns from train data\nunimportant_cols,X_train = drop_unimportant(X_train)\n# Drop umportant columns from test data\ntest = test.drop(unimportant_cols, axis='columns')\nprint(test.shape,X_train.shape)","aedbfa5c":"#Only execute in training or evaluating new features\n#cv_accuracy(clf, X_train, y_train)","77877150":"plot_correlations(X_train, annot=False)\n#Drop correlated columns from test data\ndropped_cols, X_train = drop_correlated(X_train)\n#Drop correlated columns from test data\ntest = test.drop(dropped_cols, axis='columns')\nprint(test.shape,X_train.shape)","e92cf9f1":"#Lets evaluate our final dataset\n#Only execute in training or evaluating new features\n\n#cv_accuracy(clf, X_train, y_train)\n#feature_importances(clf, X_train, y_train)\n#cv_confusion(clf, X_train, y_train)","5324cd22":"#Only executed when new combination of features is created\n\n#df1 = X_train.assign(Cover_Type=y_train)\n#df1.to_csv('train_fe.csv', sep=',', header=True, index=True, index_label='Id')\n#test.to_csv('test_fe.csv', sep=',', header=True, index=True, index_label='Id')","e8686110":"# Tranform to numpy array of float type\nX = X_train.values.astype('float64')\ny = y_train.values.ravel()\ntest_ds= test.values.astype('float64')\n\n#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = 0)\nprint('X: ', X.shape)\nprint('y: ', y.shape)\nprint('test: ', test_ds.shape)","ec07d07c":"#Define a ratio for every class weighted\ncount = { 1: 0.37062,\n 2: 0.49657,\n 3: 0.05947,\n 4: 0.00106,\n 5: 0.01287, \n 6: 0.02698, \n 7: 0.03238} \n\ncount_rf = { 1: 0.37062,\n 2: 0.49657,\n 3: 0.05947,\n 4: 0.00106,\n 5: 0.01287, \n 6: 0.02698} \n\nweight = [count[x]\/(sum(count.values())) for x in range(1,7+1)]\nclass_weight_lgbm = {i: v for i, v in enumerate(weight)}","a9cbffdd":"#Estimators: 400, 400, 300, 400, 250 (reduce to 100 100 100 100 for some tests)\nmodels = {\n    'Random Forest': RandomForestClassifier(criterion = 'gini',n_estimators=750, max_depth=None, min_samples_split=2, min_samples_leaf=1, \n                                    max_leaf_nodes=None,random_state = 0, class_weight = count_rf),\n    'AdaBoost': AdaBoostClassifier(base_estimator=DecisionTreeClassifier(criterion = 'gini', max_depth=None, min_samples_split=2,\n                                                                         min_samples_leaf=1,max_leaf_nodes=None,max_features='auto',\n                                                                         random_state = 0, class_weight = count_rf),\n                                   n_estimators=750,learning_rate=0.2,random_state=0),\n    \n    'Bagging': BaggingClassifier(base_estimator=DecisionTreeClassifier(criterion = 'gini', max_depth=None, min_samples_split=2,\n                                                                         min_samples_leaf=1,max_leaf_nodes=None,max_features='auto',\n                                                                         random_state = 0, class_weight = count_rf),\n                                   n_estimators=500,max_features=0.75, max_samples=1.0, random_state=0,n_jobs=-1,verbose=0),\n    #{'max_features': 0.75, 'max_samples': 1.0, 'n_estimators': 300}\n    #'GBM': GradientBoostingClassifier(n_estimators=500, learning_rate= 0.2, max_depth=10, min_samples_leaf=1, \n    #                                  min_samples_split=2,random_state=0,verbose=1),\n    'LGBM': LGBMClassifier(n_estimators=500, learning_rate= 0.1, objective= 'multiclass', num_class=7, random_state= 0, \n                           n_jobs=-1, class_weight = class_weight_lgbm),\n    #'LGBM': LGBMClassifier(n_estimators=300, num_leaves=100, verbosity=0, random_state=0,n_jobs=-1),\n    #'KNN': KNeighborsClassifier(n_neighbors=7, n_jobs=-1),\n    # 'XGB': XGBClassifier(n_estimator= 200, learning_rate= 0.1, max_depth= 50, objective= 'binary:logistic',random_state= 0,n_jobs=-1),\n    'Extra Tree': ExtraTreesClassifier(criterion = 'gini', n_estimators=400, max_depth=None, min_samples_split=2,min_samples_leaf=1, \n                                      max_leaf_nodes=None,oob_score=False, warm_start=True, random_state = 0, \n                                      class_weight = count_rf)\n}","3a041bca":"from sklearn.model_selection import KFold, cross_val_score\n\n# Create the splits for cross validation\ncv = KFold(n_splits=5, shuffle=True, random_state=0)","7bb6dfe3":"def cross_validate_L1_Clfs(models,X,y,cv): \n    results= dict()\n    for name, model in models.items():\n        print('Evaluating Model: ',name)\n        cv_results = cross_val_score(model, X, y,\n                                    cv=cv, \n                                    scoring='accuracy')\n        results[name] = cv_results\n        print(name, 'Accuracy Mean {0:.4f}, Std {1:.4f}'.format(\n                  cv_results.mean(), cv_results.std()))\n\n    accuracies= dict()\n    for name, accs in results.items():\n        accuracies[name]=accs.mean()\n    \n    best_model=max(accuracies, key=accuracies.get)\n    print('Best Model: ',best_model,' Accuracy: ',accuracies[best_model])\n    \n    return accuracies, best_model","05be9803":"#Cross Validate the L1 classifiers to extract the best model (Execute only on evaluation time)\n# accs, best_model = cross_validate_L1_Clfs(models,X,y,cv)\n#meta_model=models[best_model]\n# Estimators: 150\nmeta_model=RandomForestClassifier(criterion = 'entropy',n_estimators=250, max_depth=None, min_samples_split=2, min_samples_leaf=1, \n                                    max_leaf_nodes=None,random_state = 0, class_weight = count)\n#meta_model=LogisticRegression(max_iter=1000, n_jobs=-1, solver= 'lbfgs',multi_class = 'multinomial')","a9ba73f4":"from mlxtend.classifier import StackingCVClassifier\n\nclfs = [m for m in models.values()]\n\nstacked_model = StackingCVClassifier(classifiers=clfs,\n                             meta_classifier=meta_model,\n                             cv=cv,\n                             use_probas=True,\n                             drop_last_proba=False,\n                             use_features_in_secondary=True,\n                             verbose=1,\n                             #store_train_meta_features=True,\n                             random_state=0,\n                             n_jobs=-1)\n","d8169fc1":"#It is time for evaluating the stacked model (Executed only in training or evluation time)\n#scores = cross_val_score(stacked_model, X, y, cv=cv, scoring='accuracy', verbose=0)\n\n#print('Accuracy: %0.4f (+\/- %0.4f)' % (scores.mean(), scores.std()))","91bd8c7b":"# Fit and predict the stacked model on both train and test data\nstacked_model.fit(X, y)\npredictions = stacked_model.predict(X)\npredictions_test = stacked_model.predict(test_ds)\nprint('Stacked Model Accuracy: ',round(accuracy_score(y, predictions),4))","fda80571":"submission = pd.DataFrame({ 'Id': test.index.values,\n                            'Cover_Type': predictions_test })\nsubmission.to_csv(\"submission_data.csv\", index=False)","786aa73c":"We will use a simple RandomForest classifier to analyze the features, it will be the performance baseline ","949d338d":"## Classifiers for Level 1 in the stacked model\n\nCreate a dictionary of the diferents models for Level 1 in the stacked model. There are many posibilities, I have tested some of them using GridSearchCV for some parameter tunning (but they can be improved with some other experiments). You can check some great kernels publish by others members:\n\nhttps:\/\/www.kaggle.com\/stevegreenau\/stacking-multiple-classifiers-clustering\nhttps:\/\/www.kaggle.com\/phsheth\/forestml-part-6-stacking-selfets-gmix-smote\n","c308612f":"## Classifier for Level 2 of the Stacked model\nNow we have determined the best model and its accuracy, so the next step is to create a stacked model whose L1 classifiers are the previuos models and the meta-classifier (L2 classifier) will be the best model. Finally we will test the model using prediction probabilities in L1 and adding or not the features","14c2bfed":"Finally we will predict on the training and test data to get the final results ","d08b76b2":"Lets create new featured bsed on Aspect and Slope transformations, they are extracted for a very interesting and successful kerner by arateris https:\/\/www.kaggle.com\/arateris\/2-layer-k-fold-learning-forest-cover\n","92f10f23":"We will repeat the previous analyze with our new dataset, containing the new features","f270735e":"## Loading the train and test datasets\n","81c9b5bf":"It is neccesary to upgrade numpy version to 1.17.2 to run LGBMClassifier. The performance of that algo implementation with actual numpy version in kaggle is very poor, it takes hours.","e96a1b7e":"Some kernels extract some new information, or new features, from the test data and then include that new information to the training dataset. A simple and effective aproach is to create a Gaussian Mixture model as is described for example in:\nhttps:\/\/www.kaggle.com\/stevegreenau\/stacking-multiple-classifiers-clustering or in\n1. https:\/\/www.kaggle.com\/arateris\/2-layer-k-fold-learning-forest-cover\n\nLets include this new features:","19c9dd22":"Create the submission file with the predictions for the test dataset","276f74c1":"Finally we will drop unimportant features, those containing just a few values. Mode frequency > 99% of data. And the last step will be to drop those correlated features.","56ff8c58":"In this kernell we are goint to generate a group of features that many others competitors have worked on and looks loke they can produce a grat result. Thanks to kwabenantim, most of the feature engineering and selection in this kernells have been extracted from his excellent kernell https:\/\/www.kaggle.com\/kwabenantim\/forest-cover-stacking-multiple-classifiers. I stringllly recomend to read carefully and study it, not all the steps in that kernell are included in this one. This one is simpler and very easy to understand.\n","07137366":"Lets evaluate our level-1 classifiers using cross validation, splits = 5","87a7307a":"## Feature engineering and selection"}}