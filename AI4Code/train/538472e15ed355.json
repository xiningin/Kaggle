{"cell_type":{"62b99794":"code","75d76024":"code","49170326":"code","d6614e64":"code","4e36c707":"code","2506531c":"code","07fb1cee":"code","30206cf4":"code","8142897b":"code","121ca1ab":"code","a16757c0":"code","ac7302fe":"code","4a305954":"code","95dcac6b":"code","92db92cd":"code","7e8c4ce9":"code","304ebc9e":"code","070f345a":"code","41cd0b66":"code","0c7c2fec":"code","d8feb90f":"code","027d749f":"code","d74091e4":"code","30e63f12":"code","8172a1a2":"code","7f5580d0":"code","6840c996":"code","1737d90b":"code","08c8aab7":"code","4a090ba2":"code","635e49a8":"code","d8158956":"code","535c7c62":"code","766a88e9":"code","3f71e792":"code","cc88408d":"code","cbb94440":"code","e42f92fa":"code","ec21e992":"code","e038c92a":"code","f554618c":"code","7efb51e4":"code","cfd12010":"code","9e8047ec":"code","7391f6dc":"code","c4984a70":"code","778f138d":"code","e80d0d96":"code","763b2b2e":"code","0a9dad3a":"code","e54dd290":"code","2cb1078a":"code","71815e9e":"code","11a852a9":"markdown","4912d63b":"markdown","c0eaf57c":"markdown","220febb9":"markdown","bae6d34e":"markdown","89e8907b":"markdown","0fe97de6":"markdown","a8e5585a":"markdown","2d773fda":"markdown","93584cc0":"markdown","3fc3b434":"markdown","022ff364":"markdown","099b72fd":"markdown","1080bdd9":"markdown","e3cce45e":"markdown"},"source":{"62b99794":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nimport os\nimport time\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold\nimport lightgbm as lgb\nsns.set()\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nimport warnings\nwarnings.filterwarnings(\"ignore\")","75d76024":"train=pd.read_csv(\"..\/input\/X_train.csv\")\ntrain.head()","49170326":"test=pd.read_csv(\"..\/input\/X_test.csv\")\ntest.head()","d6614e64":"sample_submission=pd.read_csv(\"..\/input\/sample_submission.csv\")\nsample_submission.head()","4e36c707":"test_id=sample_submission['series_id']\ny_train=pd.read_csv(\"..\/input\/y_train.csv\")\ny_train.head()","2506531c":"y_train['surface'].value_counts().plot.bar();","07fb1cee":"train.shape,test.shape,y_train.shape","30206cf4":"plt.figure(figsize=(26, 20))\nfor i, col in enumerate(train.columns[3:]):\n    plt.subplot(3, 4, i + 1)\n    plt.plot(train.loc[train['series_id'] == 1, col])\n    plt.title(col)","8142897b":"plt.figure(figsize=(26, 20))\nfor i, col in enumerate(test.columns[3:]):\n    plt.subplot(3, 4, i + 1)\n    plt.plot(test.loc[test['series_id'] == 1, col])\n    plt.title(col)","121ca1ab":"# refrence from https:\/\/www.kaggle.com\/jsaguiar\/surface-recognition-baseline\ndef feature_extraction(raw_frame):\n    frame = pd.DataFrame()\n    raw_frame['angular_velocity'] = raw_frame['angular_velocity_X'] + raw_frame['angular_velocity_Y'] + raw_frame['angular_velocity_Z']\n    raw_frame['linear_acceleration'] = raw_frame['linear_acceleration_X'] + raw_frame['linear_acceleration_Y'] + raw_frame['linear_acceleration_Y']\n    raw_frame['velocity_to_acceleration'] = raw_frame['angular_velocity'] \/ raw_frame['linear_acceleration']\n    \n    for col in raw_frame.columns[3:]:\n        frame[col + '_mean'] = raw_frame.groupby(['series_id'])[col].mean()\n        frame[col + '_std'] = raw_frame.groupby(['series_id'])[col].std()\n        frame[col + '_max'] = raw_frame.groupby(['series_id'])[col].max()\n        frame[col + '_min'] = raw_frame.groupby(['series_id'])[col].min()\n        frame[col + '_max_to_min'] = frame[col + '_max'] \/ frame[col + '_min']\n        \n        frame[col + '_mean_abs_change'] = raw_frame.groupby('series_id')[col].apply(lambda x: np.mean(np.abs(np.diff(x))))\n        frame[col + '_abs_max'] = raw_frame.groupby('series_id')[col].apply(lambda x: np.max(np.abs(x)))\n    return frame","a16757c0":"train_df = feature_extraction(train)\ntest_df = feature_extraction(test)\ntrain_df.head()","ac7302fe":"train_df.shape,test_df.shape","4a305954":"train_df[\"orientation_X_mean\"].hist()","95dcac6b":"train_df[\"orientation_X_std\"].hist()","92db92cd":"test_df[\"orientation_X_mean\"].hist()","7e8c4ce9":"test_df[\"orientation_X_std\"].hist()","304ebc9e":"le = LabelEncoder()\ntarget_train = le.fit_transform(y_train['surface'])","070f345a":"train_df['surface']=target_train\nsns.violinplot(data=train_df,x=\"surface\", y=\"orientation_X_mean\")","41cd0b66":"sns.violinplot(data=train_df,x=\"surface\", y=\"orientation_Y_mean\")","0c7c2fec":"sns.violinplot(data=train_df,x=\"surface\", y=\"orientation_Z_mean\")","d8feb90f":"sns.violinplot(data=train_df,x=\"surface\", y=\"angular_velocity_X_mean\")","027d749f":"sns.violinplot(data=train_df,x=\"surface\", y=\"angular_velocity_Z_mean\")","d74091e4":"train_df=train_df.drop(['surface'],axis=1)","30e63f12":"X_train, X_test, Y_train, Y_test = train_test_split(train_df, target_train, random_state = 0)\nX_train.shape,X_test.shape","8172a1a2":"sc=StandardScaler()\nX_train=sc.fit_transform(X_train)\nX_test=sc.transform(X_test)","7f5580d0":"preds = []\nK = 12\nkf = KFold(n_splits = K, random_state = 3228, shuffle = True)","6840c996":"alg =  RandomForestClassifier()","1737d90b":"for train_index, test_index in kf.split(X_train):\n    train_X, valid_X = X_train[train_index], X_train[test_index]\n    train_y, valid_y = Y_train[train_index], Y_train[test_index]\n    alg.fit( train_X,  train_y)                   \n    pred = alg.predict(X_test)\n    preds.append(list(pred))","08c8aab7":"predx=[]\nfor i in range(len(preds[0])):\n    sum=[]\n    for j in range(K):\n        sum.append(preds[j][i])\n            \n    predx.append(max(set(sum), key =sum.count))","4a090ba2":"accuracy_score(Y_test, predx)","635e49a8":"cm = confusion_matrix(Y_test, predx)\ncm ","d8158956":"params = {\n    'num_leaves': 20,\n    'min_data_in_leaf': 15,\n    'objective': 'multiclass',\n    'max_depth': 8,\n    'learning_rate': 0.01,\n    \"boosting\": \"gbdt\",\n    \"bagging_freq\": 5,\n    \"bagging_fraction\": 0.8126672064208567,\n    \"bagging_seed\": 11,\n    \"verbosity\": -1,\n    'reg_alpha': 0.1302650970728192,\n    'reg_lambda': 0.3603427518866501,\n    \"num_class\": 9,\n    'nthread': -1\n}\n\ndef multiclass_accuracy(preds, train_data):\n    labels = train_data.get_label()\n    pred_class = np.argmax(preds.reshape(9, -1).T, axis=1)\n    return 'multi_accuracy', np.mean(labels == pred_class), True\n\nt0 = time.time()\ntrain_set = lgb.Dataset(X_train, label=Y_train)\neval_hist = lgb.cv(params, train_set, nfold=8, num_boost_round=1200,\n                   early_stopping_rounds=80, seed=19, feval=multiclass_accuracy)\nnum_rounds = len(eval_hist['multi_logloss-mean'])\n# retrain the model and make predictions for test set\nclf = lgb.train(params, train_set, num_boost_round=num_rounds)\npredictions = clf.predict(X_test, num_iteration=None)\nprint(\"Timer: {:.1f}s\".format(time.time() - t0))","535c7c62":"idx = predictions.argmax(axis=1)\ny_pred1 = (idx[:,None] == np.arange(predictions.shape[1])).astype(int)\ny_pred1","766a88e9":"y_pred1 = [np.where(r == 1)[0][0] for r in y_pred1]","3f71e792":"accuracy_score(Y_test, y_pred1)","cc88408d":"cm = confusion_matrix(Y_test, y_pred1)\ncm ","cbb94440":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.optimizers import SGD","e42f92fa":"Y_traind = np.zeros((2857, 9))\nY_traind[np.arange(2857), Y_train] = 1\nY_traind","ec21e992":"predsd = []\nK = 3\nkf = KFold(n_splits = K, random_state = 3228, shuffle = True)","e038c92a":"for train_index, test_index in kf.split(X_train):\n    train_X, valid_X = X_train[train_index], X_train[test_index]\n    train_y, valid_y = Y_train[train_index], Y_train[test_index]\n    classifier = Sequential()\n    classifier.add(Dense(output_dim = 100, init = 'uniform', activation = 'relu', input_dim = 91))\n    classifier.add(Dropout(0.1))\n    classifier.add(Dense(output_dim = 60, init = 'uniform', activation = 'relu'))\n    classifier.add(Dropout(0.1))\n    classifier.add(Dense(output_dim = 40, init = 'uniform', activation = 'relu'))\n    classifier.add(Dense(output_dim = 9, init = 'uniform', activation = 'softmax'))\n    classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    classifier.fit(X_train,Y_traind, nb_epoch = 130)\n    pred = classifier.predict(X_test)\n    predsd.append(list(pred))","f554618c":"f=0\nfor i in range(K):\n    a=predsd[i]\n    f+=np.array(a)\nf=f\/K; \nf=np.array(f)","7efb51e4":"idx = f.argmax(axis=1)\ny_predx = (idx[:,None] == np.arange(f.shape[1])).astype(int)\ny_predx","cfd12010":"y_predx = [np.where(r == 1)[0][0] for r in y_predx]","9e8047ec":"accuracy_score(Y_test, y_predx)","7391f6dc":"cm = confusion_matrix(Y_test, y_predx)\ncm","c4984a70":"lt=[y_predx,y_pred1,predx]","778f138d":"predx=[]\nfor i in range(len(lt[0])):\n    sum=[]\n    for j in range(3):\n        \n        sum.append(lt[j][i])\n            \n    predx.append(max(set(sum), key =sum.count))","e80d0d96":"accuracy_score(Y_test, predx)","763b2b2e":"params = {\n    'num_leaves': 20,\n    'min_data_in_leaf': 15,\n    'objective': 'multiclass',\n    'max_depth': 10,\n    'learning_rate': 0.01,\n    \"boosting\": \"gbdt\",\n    \"bagging_freq\": 5,\n    \"bagging_fraction\": 0.8126672064208567,\n    \"bagging_seed\": 11,\n    \"verbosity\": -1,\n    'reg_alpha': 0.1302650970728192,\n    'reg_lambda': 0.3603427518866501,\n    \"num_class\": 9,\n    'nthread': -1\n}\n\ndef multiclass_accuracy(preds, train_data):\n    labels = train_data.get_label()\n    pred_class = np.argmax(preds.reshape(9, -1).T, axis=1)\n    return 'multi_accuracy', np.mean(labels == pred_class), True\n\nt0 = time.time()\ntrain_set = lgb.Dataset(train_df, label=target_train)\neval_hist = lgb.cv(params, train_set, nfold=20, num_boost_round=1400,\n                   early_stopping_rounds=80, seed=19, feval=multiclass_accuracy)\nnum_rounds = len(eval_hist['multi_logloss-mean'])\n# retrain the model and make predictions for test set\nclf = lgb.train(params, train_set, num_boost_round=num_rounds)\npredictions = clf.predict(test_df, num_iteration=None)\nprint(\"Timer: {:.1f}s\".format(time.time() - t0))","0a9dad3a":"predictions","e54dd290":"v1, v2 = eval_hist['multi_logloss-mean'][-1], eval_hist['multi_accuracy-mean'][-1]\nprint(\"Validation logloss: {:.4f}, accuracy: {:.4f}\".format(v1, v2))\nplt.figure(figsize=(10, 4))\nplt.title(\"CV multiclass logloss\")\nnum_rounds = len(eval_hist['multi_logloss-mean'])\nax = sns.lineplot(x=range(num_rounds), y=eval_hist['multi_logloss-mean'])\nax2 = ax.twinx()\np = sns.lineplot(x=range(num_rounds), y=eval_hist['multi_logloss-stdv'], ax=ax2, color='r')\n\nplt.figure(figsize=(10, 4))\nplt.title(\"CV multiclass accuracy\")\nnum_rounds = len(eval_hist['multi_accuracy-mean'])\nax = sns.lineplot(x=range(num_rounds), y=eval_hist['multi_accuracy-mean'])\nax2 = ax.twinx()\np = sns.lineplot(x=range(num_rounds), y=eval_hist['multi_accuracy-stdv'], ax=ax2, color='r')","2cb1078a":"importance = pd.DataFrame({'gain': clf.feature_importance(importance_type='gain'),\n                           'feature': clf.feature_name()})\nimportance.sort_values(by='gain', ascending=False, inplace=True)\nplt.figure(figsize=(10, 20))\nax = sns.barplot(x='gain', y='feature', data=importance)","71815e9e":"sample_submission['surface'] = le.inverse_transform(predictions.argmax(axis=1))\nsample_submission.to_csv('Lgb.csv', index=False)","11a852a9":"# LightGBM","4912d63b":"## accuracy_score from deep learning","c0eaf57c":"## accuracy_score from combined model","220febb9":"## accuracy_score","bae6d34e":"## Feature engineering","89e8907b":"## first We are going to split the data and then we will see how the different models works..","0fe97de6":"## Transform Y_train into onehot encoded form","a8e5585a":"## Combined all 3 model","2d773fda":"## Deep Learning with k-fold","93584cc0":"## y_predx from deeplearning\n##y_pred1 from lightgbm\n##predx from randomforest","3fc3b434":"## confusion_matrix","022ff364":"## lightgbm provides here some better result than randomforest and deeplearning","099b72fd":"## RandomForestClassifier with k-fold","1080bdd9":"## confusion_matrix","e3cce45e":"## confusion_matrix"}}