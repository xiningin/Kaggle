{"cell_type":{"12f7256a":"code","316830f7":"code","fdae8250":"code","b5459349":"code","5e628513":"code","f20c98c2":"code","cf63e7ff":"code","7f641b5c":"code","15124dee":"code","d6d4c6c6":"code","8ead313e":"code","d8d3a9d9":"code","8ceb8bac":"code","6a83da6a":"code","494759dd":"code","2227b900":"code","c0dfc3da":"code","a42aff0b":"code","0827ef8e":"code","9cceb2ca":"code","0f465e85":"code","2945f365":"code","ac1177a1":"code","069bf213":"code","98ddd016":"code","efe5096e":"code","d30541e5":"code","2b648656":"code","add529d5":"code","6065a4df":"code","7be61c10":"code","78ead5f3":"code","6af1c551":"code","8ab8c686":"code","4ddc92dc":"code","d529bb45":"code","fca61e7c":"code","a9565514":"code","2778e1c2":"code","675a74f1":"code","7afe1102":"code","13e9504c":"code","3e8cf50d":"code","d8f6b3c3":"code","6ec14624":"code","b0b61dc6":"code","02ad4b12":"code","055dc6b1":"code","f27e0d08":"code","1c276911":"code","9da45a5f":"code","81c4468e":"code","8edfaad9":"code","48e5b13a":"code","812c9019":"code","6b3b6125":"code","54e78a06":"code","81f64364":"code","69050679":"code","da8b35f4":"code","280f6aac":"code","a7a17c44":"code","8f63a995":"code","d2abc7a2":"code","5b5ed3db":"code","e4562174":"markdown","5bc110e2":"markdown","596e2f54":"markdown","4aa0202a":"markdown","6dbeb64a":"markdown","0a56be55":"markdown","b2de33ce":"markdown","e5cfd551":"markdown","73d6e4fe":"markdown","cd79a27b":"markdown","767e8a81":"markdown","717c6b66":"markdown","a333fc6e":"markdown","4991a154":"markdown","f7706e4b":"markdown","a0ef8c16":"markdown","b7d203d9":"markdown","d724084a":"markdown","9b3f8c13":"markdown","ac782a7e":"markdown","43406325":"markdown","b32d8339":"markdown","15a00834":"markdown","ab5b75b7":"markdown","2fcc2a4a":"markdown","4388a5c3":"markdown","b3c71bda":"markdown","46a512f7":"markdown","2274a838":"markdown","52792f63":"markdown","69cd6de7":"markdown","235ee980":"markdown","56225293":"markdown","aab25f1e":"markdown","6083e5b9":"markdown","ed49699b":"markdown","221a7cc5":"markdown","38b753b6":"markdown","429f4079":"markdown","ea256715":"markdown","7a57ad42":"markdown","1fe090e4":"markdown","ae2eb3dc":"markdown","01b501e1":"markdown","c05dbd2d":"markdown","0cdc1b45":"markdown","9a0302e6":"markdown","69f9b2bc":"markdown","ce14a613":"markdown","d35577c8":"markdown","8991540d":"markdown","2978abd7":"markdown","05caab7a":"markdown","b7f29c45":"markdown","e57d0b96":"markdown","0ddb8472":"markdown","e2409349":"markdown","f25c5c46":"markdown","657d37ce":"markdown","739fbc9f":"markdown","34566420":"markdown","7c690dc9":"markdown","bda0782c":"markdown","62451b09":"markdown"},"source":{"12f7256a":"\"\"\"General Data Science Packages\"\"\"\nimport numpy as np\nimport pandas as pd\nimport os\nimport sys\n\n\"\"\"Graphing Packages\"\"\"\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport networkx as nx\nimport seaborn as sns\n\n\"\"\"Other Useful Libraries\"\"\"\nimport random\nimport math\nimport itertools\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import euclidean_distances\nfrom collections import Counter\nfrom sklearn.model_selection import GroupKFold, StratifiedKFold, RepeatedStratifiedKFold\nimport multiprocessing\nimport scipy\n\nLOOK_AT = 10\nSEED = 42\nnp.random.seed(SEED)\nrandom.seed(SEED)","316830f7":"data_by_artist = pd.read_csv(\"..\/input\/icm-problem-d\/2021_ICM_Problem_D_Data\/data_by_artist.csv\")\ndata_by_year = pd.read_csv(\"..\/input\/icm-problem-d\/2021_ICM_Problem_D_Data\/data_by_year.csv\")\nfull_music_data = pd.read_csv(\"..\/input\/icm-problem-d\/2021_ICM_Problem_D_Data\/full_music_data.csv\")\ninfluence_data = pd.read_csv(\"..\/input\/icm-problem-d\/2021_ICM_Problem_D_Data\/influence_data.csv\")","fdae8250":"features_list = data_by_artist.columns[2:]\nfeatures_list","b5459349":"id_genre_dict = {}\n\nfor i in range(len(influence_data)):\n    row = influence_data.loc[i]\n    in_id = row['influencer_name']\n    fol_id = row['follower_name']\n    in_genre = row['influencer_main_genre']\n    fol_genre = row['follower_main_genre']\n    if in_id not in id_genre_dict:\n        id_genre_dict[in_id] = set()\n    id_genre_dict[in_id].add(in_genre)\n    \n    if fol_id not in id_genre_dict:\n        id_genre_dict[fol_id] = set()\n    id_genre_dict[fol_id].add(fol_genre)","5e628513":"c = 0\nfor k, v in id_genre_dict.items():\n    c += 1\n    if len(v) != 1:\n        print(k, v)","f20c98c2":"%%time\n\nartist_df = pd.DataFrame()\nartists_not_included = []\nfor i in range(len(data_by_artist)):\n    try:\n        for genre in id_genre_dict[data_by_artist.loc[i]['artist_name']]:\n            if genre == \"Unknown\" or genre == \"Children's\":\n                continue\n            tmp_df = pd.DataFrame(data_by_artist.loc[i]).T\n            tmp_df['genre'] = genre\n            artist_df = pd.concat((artist_df, tmp_df))\n    except:\n        artists_not_included.append(data_by_artist.loc[i]['artist_name'])\n        \n#print(artists_not_included)\nartist_df = artist_df.reset_index().drop(\"index\", axis=1)\nartist_df","cf63e7ff":"genre_list = artist_df['genre'].unique()\ngenre_list.sort()\nN_CLUSTERS = len(genre_list)\n\ncluster_pipeline = Pipeline([('scaler', StandardScaler()), ('kmeans', KMeans(n_clusters=N_CLUSTERS, n_init=50, max_iter=1000, random_state=SEED))])\nX = artist_df.loc[:, features_list]\ncluster_pipeline.fit(X)\nartist_df['cluster'] = cluster_pipeline.predict(X)\nartist_df","7f641b5c":"tsne_pipeline = Pipeline([('scaler', StandardScaler()), ('tsne', TSNE(n_components=2, random_state=SEED))])\ngenre_embedding = tsne_pipeline.fit_transform(X)\nprojection = pd.DataFrame(columns=['x', 'y'], data=genre_embedding)\nprojection['genre'] = artist_df['genre']\nprojection['cluster'] = artist_df['cluster']\nprojection","15124dee":"fig = px.scatter(projection, x='x', y='y', color='cluster', hover_data=['x', 'y', 'genre'])\nfig.update_layout(title={'text': \"K-Means Generated Clusters After TSNE Dimensionality Reduction\", 'x': 0.5,\n                             'xanchor': 'center', 'font': {'size': 20}})\nfig.show()","d6d4c6c6":"clusters = artist_df.groupby(['genre', 'cluster']).size()\nclusters_df = pd.DataFrame()\nr = 0\nfor f in genre_list:\n    tmp_cluster = clusters[f]\n    tmp_cluster = tmp_cluster.reindex(np.arange(N_CLUSTERS))\n    tmp_cluster = tmp_cluster.fillna(0)\n    clusters_df[f] = tmp_cluster\n    \nclusters_df = clusters_df.T\nmax_perc = []\nfor genre in clusters_df.index:\n    max_perc.append(max(clusters_df.loc[genre])\/clusters_df.loc[genre].sum(axis=0))\n    \nclusters_df[\"Max Clustering\"] = max_perc\nclusters_df","8ead313e":"%%time\n\ndef mcc_AB(artist_df, N_CLUSTERS):\n    random_df = artist_df.copy()\n    np.random.shuffle(random_df['genre'].to_numpy())\n    \n    cluster_pipeline = Pipeline([('scaler', StandardScaler()), ('kmeans', KMeans(n_clusters=N_CLUSTERS, n_init=50, max_iter=1000, random_state=SEED))])\n    X = random_df.loc[:, features_list]\n    cluster_pipeline.fit(X)\n    random_df['cluster'] = cluster_pipeline.predict(X)\n    \n    r_clusters = random_df.groupby(['genre', 'cluster']).size()\n    r_clusters_df = pd.DataFrame()\n    r = 0\n    for f in genre_list:\n        tmp_cluster = r_clusters[f]\n        tmp_cluster = tmp_cluster.reindex(np.arange(N_CLUSTERS))\n        tmp_cluster = tmp_cluster.fillna(0)\n        r_clusters_df[f] = tmp_cluster\n\n    r_clusters_df = r_clusters_df.T\n    max_perc = []\n    for genre in r_clusters_df.index:\n        max_perc.append(max(r_clusters_df.loc[genre])\/r_clusters_df.loc[genre].sum(axis=0))\n    \n    r_clusters_df[\"Max Clustering\"] = max_perc\n    return r_clusters_df[\"Max Clustering\"]\n\nmcc_random_df = pd.DataFrame()\nAB_ITERATIONS = 50\nfor i in range(AB_ITERATIONS):\n    mcc_series = mcc_AB(artist_df, N_CLUSTERS)\n    mcc_random_df[f\"Iteration {i}\"] = mcc_series\n    \nmcc_random_df = mcc_random_df.T\nmcc_describe = mcc_random_df.describe()\nmcc_describe","d8d3a9d9":"fig = go.Figure()\nfig.add_trace(go.Bar(name=\"Max Clustering Coefficient\", x=genre_list, y=clusters_df[\"Max Clustering\"]))\nfig.add_trace(go.Bar(name='Randomized Clustering', x=genre_list, y=mcc_describe.loc['mean'], \n                     error_y=dict(type='data', array=[2*mcc_describe.loc['std'][genre] for genre in genre_list])))\nfig.update_layout(barmode='group', title={'text': \"AB Tested Max Clustering Coefficient\", 'x': 0.5,\n                             'xanchor': 'center', 'font': {'size': 20}}, yaxis_title=\"Max Clustering Coefficient\")\nfig.show()","8ceb8bac":"sorted_clusters_df = clusters_df.sort_values(\"Max Clustering\", ascending=False)\nfig = px.bar(sorted_clusters_df, y=\"Max Clustering\")\nfig.update_layout(title={'text': \"Max Clustering Coefficient (sorted)\", 'x': 0.5,\n                             'xanchor': 'center', 'font': {'size': 20}})\nfig.show()","6a83da6a":"COLOR_MAP = {'Pop\/Rock': 'red', 'R&B;': 'green', 'Country': 'orange', 'Jazz': 'brown', 'Vocal': 'pink', 'Latin': 'yellow', 'Classical': 'grey', \n             'International': 'lightblue', 'Reggae': 'lightgreen', 'Electronic': 'purple', 'Folk': 'tan', 'Blues': 'blue', 'Stage & Screen': 'maroon',\n             'Easy Listening': 'yellowgreen', 'Religious': 'orchid', 'Comedy\/Spoken': 'darkred', 'New Age': 'cyan', 'Avant-Garde': 'lightgrey'}\n\nprojection = projection.sort_values(\"genre\")\nfig = px.scatter(projection, x='x', y='y', color='genre', color_discrete_map=COLOR_MAP, hover_data=['x', 'y', 'genre', 'cluster'])\nfig.update_layout(title={'text': \"Genre Clustering\", 'x': 0.5,\n                             'xanchor': 'center', 'font': {'size': 20}})\nfig.show()","494759dd":"projection_data = projection.drop(\"cluster\", axis=1).groupby('genre').describe()\nprojection_data[\"Total STD\"] = np.sqrt(np.power(projection_data[('x', 'std')], 2) + np.power(projection_data[('y', 'std')], 2))\nprojection_data = projection_data.sort_values(\"Total STD\")[\"Total STD\"]\nprojection_data","2227b900":"%%time\n\nrandomized_projection = projection.copy()\ncomb_random_data = pd.DataFrame()\n#all_std_list = []\n\nfor i in range(AB_ITERATIONS):\n    np.random.shuffle(randomized_projection['genre'].to_numpy())\n    random_data = randomized_projection.drop(\"cluster\", axis=1).groupby('genre').describe()\n    random_data[\"Total STD\"] = np.sqrt(np.power(random_data[('x', 'std')], 2) + np.power(random_data[('y', 'std')], 2))\n    comb_random_data[f\"Iteration {i}\"] = random_data[\"Total STD\"]\n    #all_std_list.append(random_data[\"Total STD\"])\n\ncomb_random_data = comb_random_data.T\ncomb_describe = comb_random_data.describe()\ncomb_describe","c0dfc3da":"fig = go.Figure()\nfig.add_trace(go.Bar(name=\"Actual STD\", x=genre_list, y=projection_data))\nfig.add_trace(go.Bar(name='Randomized STD', x=genre_list, y=comb_describe.loc['mean'], \n                     error_y=dict(type='data', array=[2*comb_describe.loc['std'][genre] for genre in genre_list])))\nfig.update_layout(barmode='group', title={'text': \"AB Tested Genre Clustering Standard Deviation\", 'x': 0.5,\n                             'xanchor': 'center', 'font': {'size': 20}}, yaxis_title=\"Standard Deviation\")\nfig.show()","a42aff0b":"drop_list = [\"mode\", \"key\"]\nnormalized_data = artist_df.copy().drop(drop_list, axis=1)\n\n# Same implementation as StandardScaler\nfor i in features_list:\n    if i in drop_list:\n        continue\n    column = normalized_data[i].to_numpy()\n    normalized_column = (column - np.average(column)) \/ np.std(column)\n    normalized_data[i] = normalized_column\n    \nnormalized_data","0827ef8e":"genre_count = normalized_data.groupby('genre').size()\nscaler = MinMaxScaler()\ngenre_norm = pd.DataFrame(scaler.fit_transform(genre_count.to_numpy().reshape(-1, 1)))\ngenre_norm.index = genre_count.index\ngenre_norm.columns = [\"Scaled\"]\ngenre_norm","9cceb2ca":"def distance(vec1, vec2):\n    return np.power((np.power(vec1-vec2, 2)).sum(axis=1), 0.5)\n\nupdated_list = features_list.drop(drop_list)\ndef KNN(artist_name, vec1, df, K):\n    distance_list = distance(vec1, df[updated_list])\n    \n    df[\"Distance\"] = distance_list\n    df = df.sort_values(\"Distance\")\n    n = len(id_genre_dict[artist_name]) # correction\n    neighbors_list = df['genre'][n:K+n].to_numpy()\n    n_most = Counter(neighbors_list).most_common()\n    \n    possible_ans = []\n    for t in n_most:\n        possible_ans.append([t[0], t[1]\/genre_norm.loc[t[0]][0] if genre_norm.loc[t[0]][0] != 0 else 0])\n         \n    possible_ans.sort(key=lambda x: x[1], reverse=True)\n    return possible_ans[0][0], possible_ans[0][1], distance_list","0f465e85":"%%time\n\nEXPORT_PREDICTION = False\nK_VALUE = 7\n\nif EXPORT_PREDICTION:\n    prediction_df = normalized_data.copy()\n    predictions = []\n    knn_value = []\n    for i in range(len(normalized_data)):\n        if i%1000 == 0:\n            print(i)\n        row = normalized_data.loc[i]\n        label, num, distance_list = KNN(row['artist_name'], row[updated_list], normalized_data.copy(), K_VALUE)\n        predictions.append(label)\n        knn_value.append(num)\n        prediction_df[row['artist_name']] = distance_list\n\n    prediction_df[\"Prediction\"] = predictions\n    prediction_df[\"KNN Value\"] = knn_value\n    prediction_df.to_csv(\"prediction.csv\")\nelse:\n    prediction_df = pd.read_csv(\"..\/input\/icm-problem-d\/prediction.csv\")\n    \nprediction_df","2945f365":"p_df = prediction_df[[\"artist_name\", \"genre\", \"Prediction\"]]\nprint(\"%d Correct out of %d\" % (np.count_nonzero(p_df[\"Prediction\"] == p_df[\"genre\"]), len(p_df)))\nprint(\"%.3f%c Correct\" % (np.count_nonzero(p_df[\"Prediction\"] == p_df[\"genre\"])\/len(p_df)*100, '%'))\np_df","ac1177a1":"plt.figure(figsize=(16, 8))\nsns.set(style=\"whitegrid\")\ncorr = clusters_df.T.drop(\"Max Clustering\", axis=0).corr()\nsns.heatmap(corr, annot=True)\nplt.title(\"Pearson Correlation Matrix for Genres\", fontsize=20)\nplt.show()","069bf213":"significant_corr = []\nfor i in range(len(genre_list)):\n    genre_x = genre_list[i]\n    for j in range(i+1, len(genre_list)):\n        genre_y = genre_list[j]\n        if genre_x == genre_y:\n            continue\n        if abs(corr[genre_x][genre_y]) >= 0.7:\n            significant_corr.append([f\"{genre_x} + {genre_y}\", round(corr[genre_x][genre_y], 3)])\n            \nsignificant_corr","98ddd016":"corr_df = pd.DataFrame(significant_corr, columns=[\"Genres\", \"Correlation\"])\ncorr_df = corr_df.sort_values(\"Correlation\", ascending=False)\nfig = px.bar(corr_df, x=\"Genres\", y=\"Correlation\")\nfig.update_layout(title={'text': \"Significantly Correlated Genres\", 'x': 0.5,\n                             'xanchor': 'center', 'font': {'size': 20}})\nfig.show()","efe5096e":"mean_projection = projection.groupby('genre').mean().reset_index()\nfig = px.scatter(mean_projection, x='x', y='y', color='genre', color_discrete_map=COLOR_MAP, hover_data=['x', 'y', 'genre'])\nfig.update_layout(title={'text': \"Genre Mean after TSNE Dimensionality Reduction\", 'x': 0.5,\n                             'xanchor': 'center', 'font': {'size': 20}})\nfig.show()","d30541e5":"i_dict = {k: set() for k in influence_data['influencer_name'].unique()}\nfor i in range(len(influence_data)):\n    influencer = influence_data['influencer_name'][i]\n    follower = influence_data['follower_name'][i]\n    i_dict[influencer].add(follower)","2b648656":"\"\"\"\nfor influencer in all_influencers:\n    queue = []\n    visited = set()\n    while queue:\n        influencer = top element in queue\n        visit influencer followers\n        if depth > MAX_DEPTH:\n            break\n        add those visits to the queue & visited\n        delete influencer from queue\n    influencer influence = length of visited\n\"\"\"","add529d5":"def step(i_dict, influencer, vis, d, q, MAX_DEPTH):\n    \"\"\"\n    Parameters:\n    i_dict - Input graph\n    influencer - Current influencer\n    d - Depth\n    q - Queue\n    vis - Visited array (whether the influencer has been accounted for in the recursion already)\n    \n    Notes:\n    Sufficient bfs for our small dataset. Use dynamic programming for O(n^2) complexity.\n    \"\"\"\n    \n    vis.add(influencer)\n    \n    if d > MAX_DEPTH+1 or influencer not in i_dict:\n        return vis, q\n    \n    for follower in i_dict[influencer]:\n        if follower not in vis:\n            q.append([follower, d])\n            \n    return vis, q\n\ndef g_influence(MAX_DEPTH, SORT_BY=\"Influencer\"):\n    \"\"\"\n    Generate influence. Function which generates influence.\n    Pass MAX_DEPTH (detailed above)\n    SORT_BY - Influencer, Count, or None\n    \"\"\"\n    \n    i_val_dict = {k: 0 for k in i_dict.keys()}\n    for influencer in i_dict:        \n        vis = set()\n        q = [[influencer, 1]]\n        while q:\n            next_i = q[0][0]\n            next_d = q[0][1]\n            del q[0]\n            if next_i not in vis:\n                vis, q = step(i_dict, next_i, vis, next_d+1, q, MAX_DEPTH=MAX_DEPTH)\n        \n        i_val_dict[influencer] = len(vis)\n\n    depth_df = pd.DataFrame.from_dict(i_val_dict, orient='index').reset_index()\n    depth_df.columns = [\"Influencer\", f\"Depth {MAX_DEPTH}\"]\n    \n    if SORT_BY.lower() == \"influencer\":\n        depth_df = depth_df.sort_values(\"Influencer\", ascending=False)\n    elif SORT_BY.lower() == \"count\":\n        depth_df = depth_df.sort_values(f\"Depth {MAX_DEPTH}\", ascending=False)\n\n    return depth_df","6065a4df":"%%time\n\nEXPORT = False\nif EXPORT:\n    FACTOR = 2\n    MAX_DEPTH = 10\n\n    influence_df = g_influence(0) # Storage DataFrame\n    total_influence = influence_df[\"Depth 0\"] - 1\n    for depth in range(1, MAX_DEPTH+1):\n        tmp_df = g_influence(depth)\n\n        # If there are no more connections, break\n        if influence_df[f\"Depth {depth-1}\"].equals(tmp_df[f\"Depth {depth}\"]):\n            break\n\n        influence_df[f\"Depth {depth}\"] = tmp_df[f\"Depth {depth}\"]\n        total_influence += (influence_df[f\"Depth {depth}\"] - influence_df[f\"Depth {depth-1}\"]) \/ pow(FACTOR, depth-1)\n\n    influence_df[\"Total\"] = total_influence\n    influence_df = influence_df.sort_values(\"Total\", ascending=False)\n    influence_df.to_csv(\"influence_df.csv\")\nelse:\n    MAX_DEPTH = 10\n    influence_df = pd.read_csv(\"..\/input\/icm-problem-d\/influence_df.csv\").drop(\"Unnamed: 0\", axis=1)\n    \ninfluence_df","7be61c10":"plt.figure(figsize=(16, 8))\nsns.set(style=\"whitegrid\")\ncorr = influence_df.drop(\"Depth 0\", axis=1).corr()\nsns.heatmap(corr, annot=True)\nplt.title(\"Correlation Between Depths\", fontsize=20)\nplt.show()","78ead5f3":"def vis_figure(df, LOOK_AT):\n    fig = go.Figure()\n    fig.add_trace(go.Bar(x=df['Influencer'][:LOOK_AT], y=df['Total'][:LOOK_AT]))\n    fig.update_layout(title={'text': f\"Top {LOOK_AT} Influencing Artists by Score\", 'x': 0.5,\n                             'xanchor': 'center', 'font': {'size': 20}}, yaxis_title=\"Influence Score\")\n    fig.show()\n    \nvis_figure(influence_df, LOOK_AT)","6af1c551":"fig = go.Figure()\nfor i in range(LOOK_AT):\n    fig.add_trace(go.Scatter(x=np.arange(0, MAX_DEPTH+1), y=influence_df.loc[:, influence_df.columns.str.contains(\"Depth\")].iloc[i].to_numpy(), \n                             name=influence_df[\"Influencer\"].iloc[i]))\n    \nfig.update_layout(xaxis=dict(range=[0, MAX_DEPTH]), title={'text': f\"Artists Followers by Depth\", 'x': 0.5,\n                             'xanchor': 'center', 'font': {'size': 20}}, xaxis_title=\"Depth\", yaxis_title=\"Followers\")\nfig.show()","8ab8c686":"print(\"Mean Influence Score: \", influence_df[\"Total\"].mean())\nfig = px.histogram(influence_df[\"Total\"]) \nfig.update_layout(title={'text': \"Distribution of Influence Scores\", 'x': 0.5,\n                             'xanchor': 'center', 'font': {'size': 20}}, xaxis_title=\"Score\", yaxis_title=\"Count\", showlegend=False)\nfig.show()","4ddc92dc":"plt.figure(figsize=(16, 9))\ndiff = influence_data['follower_active_start'] - influence_data['influencer_active_start']\nsns.histplot(diff, binwidth=10)\nmean = diff.mean()\nprint(f\"Mean Years After: {mean}\")\nplt.axvline(mean)\nplt.xlabel(\"Years\", fontsize=14)\nplt.title(\"Influenced After X Years Distribution\", fontsize=20)\nplt.show()","d529bb45":"fm = influence_data.groupby('follower_name').mean()\nfx = fm['follower_active_start'] - fm['influencer_active_start']\n\nim = influence_data.groupby('influencer_name').mean()\nix = im['follower_active_start'] - im['influencer_active_start']\n\nprint(f\"Influencer after average {round(ix.mean(), 3)} years\\nFollower after average {round(fx.mean(), 3)} years\")\n\npd.DataFrame(data=[ix, fx]).T.plot.hist(bins=np.arange(-20, 50, 1), stacked=True, figsize=(16, 9))\nplt.legend([\"Influencers After X Years\", \"Followers After X Years\"])\nplt.title(\"Influence After X Years Aggregated by Artist\", fontsize=20)\nplt.xlabel(\"Years\", fontsize=14)\nplt.show()","fca61e7c":"num_influence = {influence_df.loc[i][\"Influencer\"]: influence_df.loc[i][\"Depth 1\"] for i in range(len(influence_df))}\ndef step_nx(i_dict, influencer, vis, d, q, MAX_DEPTH):\n    \"\"\"\n    Parameters:\n    i_dict - Input graph\n    influencer - Current influencer\n    d - Depth\n    q - Queue\n    vis - Visited array (whether the influencer has been accounted for in the recursion already)\n    \n    Notes:\n    Sufficient bfs for our small dataset. Use dynamic programming for O(n^2) complexity.\n    \"\"\"\n    \n    vis.add(influencer)\n    \n    if d > MAX_DEPTH+1 or influencer not in i_dict:\n        return vis, q\n    \n    for follower in i_dict[influencer]:\n        if follower not in vis:\n            q.append([follower, d, influencer])\n            \n    return vis, q\n\ndef nx_influence(influencer, color_def, MAX_DEPTH, sep=0.5, SORT_BY=\"Influencer\"):\n    color_map = []\n    for rgb_color in color_def:\n        c_now = rgb_color.replace('rgb', '').replace('(', '').replace(')', '').split(',')\n        color_map.append('#%02x%02x%02x' % (int(c_now[0]), int(c_now[1]), int(c_now[2])))\n    \n    G = nx.Graph(day=\"Music\")\n    vis = set()\n    q = [[influencer, 1, influencer]]\n    G.add_node(influencer, nodesize=num_influence[influencer]*2, color='#90EE90')\n    while q:\n        next_i = q[0][0]\n        next_d = q[0][1]\n        curr_i = q[0][2]\n        del q[0]\n\n        if next_i not in vis:\n            if curr_i not in G.nodes:\n                G.add_node(curr_i, nodesize=num_influence[curr_i] if curr_i in num_influence else 5, color=color_map[next_d-1])\n            \n            if next_i not in G.nodes:\n                G.add_node(next_i, nodesize=num_influence[next_i] if next_i in num_influence else 5, color=color_map[next_d-1])\n                \n            G.add_weighted_edges_from([(curr_i, next_i, 1)])\n            vis, q = step_nx(i_dict, next_i, vis, next_d+1, q, MAX_DEPTH=MAX_DEPTH)\n            \n    plt.figure(figsize=(20, 20))\n\n    sizes = [G.nodes[node]['nodesize']*150 for node in G]\n    colors = [G.nodes[node]['color'] for node in G]\n    pos = nx.spring_layout(G, k=sep, iterations=5, seed=SEED)\n    nx.draw(G, with_labels=False, node_size=sizes, node_color=colors, pos=pos, width=1, edge_color=\"#FFDEA2\", font_weight='regular')\n    for node, (x, y) in pos.items():\n        plt.text(x, y, node, fontsize=num_influence[node]\/max(num_influence.values())*30+8 if node in num_influence else 8, ha='center', va='center')\n\n    plt.savefig(f'.\/network_{influencer}_{MAX_DEPTH}.jpg')","a9565514":"influence_df.loc[(influence_df['Depth 1'] > 10) & (influence_df['Depth 10'] > 100)]","2778e1c2":"%time nx_influence(\"Juan Gabriel\", px.colors.sequential.RdBu, 10, sep=0.3)","675a74f1":"influence_df.loc[(influence_df['Depth 1'] > 20) & (influence_df['Depth 10'] > 300)]","7afe1102":"%time nx_influence(\"Coldplay\", px.colors.sequential.RdBu, 10, sep=0.3)","13e9504c":"%%time\n\nG = nx.Graph(day=\"Music\")\nNAME = \"The Beatles\"\nnum_influence = {influence_df.loc[i][\"Influencer\"]: influence_df.loc[i][\"Depth 1\"] for i in range(len(influence_df))}\nfor i in range(len(influence_data)):\n    influencer = influence_data['influencer_name'][i]\n    follower = influence_data['follower_name'][i]\n    if influencer == NAME:\n        G.add_node(influencer, nodesize=num_influence[influencer])\n        G.add_node(follower, nodesize=num_influence[follower] if follower in num_influence else 5)\n        G.add_weighted_edges_from([(influencer, follower, 1)])\n    \nplt.figure(figsize=(30, 30))\n\nsizes = [G.nodes[node]['nodesize']*100 for node in G]\npos = nx.spring_layout(G, k=0.15, iterations=20, seed=SEED)\nnx.draw(G, node_color='orange', with_labels=False, node_size=sizes, pos=pos, width=1, edge_color=\"#FFDEA2\", font_weight='regular')\nfor node, (x, y) in pos.items():\n    plt.text(x, y, node, fontsize=num_influence[node]\/max(num_influence.values())*30+8 if node in num_influence else 8, ha='center', va='center')\n\nplt.savefig(f'.\/network_{NAME}.jpg')","3e8cf50d":"ii_dict = {}\nK_INF = 20\nfor influencer in i_dict:\n    n = len(i_dict[influencer])\n    if n < K_INF:\n        continue\n    \n    try:\n        neighbors_list = set(prediction_df[[\"artist_name\", influencer]].sort_values(influencer).drop_duplicates()[1:K_INF+1]['artist_name'])\n        ii_dict[influencer] = len(i_dict[influencer].intersection(neighbors_list))\/K_INF\n    except:\n        pass","d8f6b3c3":"def plot_dist(xx_dict, K, title_list):\n    plt.figure(figsize=(16, 9))\n    arr_now = np.array(list(xx_dict.values()))\n    sns.histplot(arr_now*K, binwidth=1)\n    mean = arr_now.mean()*K\n    print(f\"Mean KNN Out of {K}: {round(mean, 3)} ({round(mean\/K*100, 3)}%)\")\n    plt.xlabel(\"Number of Neighbors\", fontsize=14)\n    plt.title(f\"Is a {title_list[0]}'s Music Similar to Their {title_list[1]}?\", fontsize=20)\n    plt.axvline(mean)\n    plt.show()\n    \nplot_dist(ii_dict, K_INF, [\"Influencer\", \"Followers\"])","6ec14624":"f_dict = {k: set() for k in influence_data['follower_name'].unique()}\nfor i in range(len(influence_data)):\n    influencer = influence_data['influencer_name'][i]\n    follower = influence_data['follower_name'][i]\n    f_dict[follower].add(influencer)","b0b61dc6":"ff_dict = {}\nK_FOL = 10\nfor follower in f_dict:\n    n = len(f_dict[follower])\n    if n < K_FOL:\n        continue\n    \n    try:\n        neighbors_list = set(prediction_df[[\"artist_name\", follower]].sort_values(follower).drop_duplicates()[1:K_FOL+1]['artist_name'])\n        ff_dict[follower] = len(f_dict[follower].intersection(neighbors_list))\/K_FOL\n    except:\n        pass\n    \nplot_dist(ff_dict, K_FOL, [\"Follower\", \"Influencers\"])","02ad4b12":"full_music_data['artist_names'] = full_music_data['artist_names'].apply(eval)","055dc6b1":"EXPORT_MUSIC = False\n\nif EXPORT_MUSIC:\n    genre_list = []\n    for i in range(len(full_music_data)):\n        if i%10000 == 0:\n            print(i)\n        for artist_id in full_music_data.loc[i]['artist_names']:\n            if artist_id in id_genre_dict:\n                for genre in id_genre_dict[artist_id]:\n                    x = full_music_data.loc[i].append(pd.Series(genre))\n                    genre_list.append(x)\n\n    music_df = pd.DataFrame(data=genre_list)\n    music_df.columns = list(full_music_data.columns)+[\"Genre\"]\n    music_df.to_csv(\"music_genre.csv\")\nelse:\n    music_df = pd.read_csv(\"..\/input\/icm-problem-d\/music_genre.csv\").drop(\"Unnamed: 0\", axis=1)\n    \nmusic_df","f27e0d08":"normalized_music = music_df.copy().drop(drop_list, axis=1)\ncolumn_labels = normalized_music.columns.tolist()[2:14]\n\nfor i in column_labels:\n    column = normalized_music[i].to_numpy()\n    normalized_column = (column - np.average(column)) \/ np.std(column)\n    normalized_music[i] = normalized_column\n    \nnormalized_music = normalized_music.sort_values(\"Genre\")\nnormalized_music","1c276911":"normalized_genre_data = {}\nnew_genre_list = normalized_music['Genre'].unique()\n\nfor i in new_genre_list:\n    genre_df = normalized_music.loc[normalized_music['Genre'] == i]\n    genre_col_data = {}\n    \n    for j in column_labels:\n        genre_col_vals = genre_df[j].tolist()\n        genre_col_data[j] = sum(genre_col_vals) \/ len(genre_col_vals)\n    \n    normalized_genre_data[i] = genre_col_data","9da45a5f":"fig, ax = plt.subplots(5, 4, figsize = (30, 30))\n\nxcounter = 0\nycounter = 0\n\nfor i in normalized_genre_data.keys():\n    genre_vals = normalized_genre_data[i]\n    ax[xcounter][ycounter].bar(genre_vals.keys(), genre_vals.values())\n    ax[xcounter][ycounter].set_title(i, fontsize=20)\n    ax[xcounter][ycounter].set_ylabel('Z-Score')\n    ax[xcounter][ycounter].grid(False)\n    for tick in ax[xcounter][ycounter].get_xticklabels():\n        tick.set_rotation(90)\n    \n    xcounter = (xcounter + 1) % 5\n    ycounter = (ycounter + 1) % 4\n\nfig.tight_layout()\nfig.show()","81c4468e":"feature_distinctiveness = {}\n\nfig, ax = plt.subplots(4, 3, figsize = (30, 30))\n\nxcounter = 0\nycounter = 0\n\nfor i in column_labels:\n    genre_column_vals = {}\n    \n    for j in new_genre_list:\n        genre_column_vals[j] = normalized_genre_data[j][i]\n    \n    ax[xcounter][ycounter].bar(genre_column_vals.keys(), genre_column_vals.values())\n    ax[xcounter][ycounter].set_title(i, fontsize=20)\n    ax[xcounter][ycounter].set_ylabel('Z-Score')\n    ax[xcounter][ycounter].grid(False)\n    for tick in ax[xcounter][ycounter].get_xticklabels():\n        tick.set_rotation(90)\n\n    xcounter = (xcounter + 1) % 4\n    ycounter = (ycounter + 1) % 3\n    \n    feature_std_across_genres = np.std(list(genre_column_vals.values()))\n    feature_distinctiveness[i] = feature_std_across_genres\n\nfig.tight_layout()\nfig.show()","8edfaad9":"sorted_feature_distinctiveness = {k: v for k, v in sorted(feature_distinctiveness.items(), key=lambda item: item[1], reverse=True)}\n\nplt.figure(figsize=(16, 9))\nplt.bar(sorted_feature_distinctiveness.keys(), sorted_feature_distinctiveness.values())\nplt.xticks(rotation=90)\nplt.ylabel('Standard Deviation', fontsize=14)\nplt.title('Feature Distinctiveness', fontsize=20)\nplt.show()","48e5b13a":"fig = px.line(music_df.groupby('year').size())\nfig.update_layout(title={'text': f\"Number of Songs Per Year\", 'x': 0.5,\n                         'xanchor': 'center', 'font': {'size': 20}}, yaxis_title=\"Count\", showlegend=False)\nfig.show()","812c9019":"fig = px.line(influence_data.groupby('follower_active_start').size())\nfig.add_vrect(x0=1957, x1=1957, name=\"The Beatles begin\")\nfig.add_vrect(x0=1961, x1=1961, name=\"Bob Dylan begins\")\nfig.add_vrect(x0=1991, x1=1991, name=\"Advent of internet\")\nfig.update_layout(title={'text': \"Followers Over Time\", 'x': 0.5,\n                             'xanchor': 'center', 'font': {'size': 20}}, xaxis_title=\"Year\", yaxis_title=\"Follower Start Count\", showlegend=False)","6b3b6125":"fig = go.Figure()\ngroup_df = music_df.groupby(['Genre', 'year']).size()\n\nfor genre in genre_list:\n    fig.add_trace(go.Scatter(x=group_df[genre].index, y=group_df[genre], name=genre, line_color=COLOR_MAP[genre]))\n\nfig.update_layout(title={'text': \"Number of Songs Per Year By Genre\", 'x': 0.5,\n                         'xanchor': 'center', 'font': {'size': 20}}, yaxis_title=\"Count\")\nfig.show()","54e78a06":"fig = px.bar(music_df.groupby('year').size().diff())\nfig.update_layout(title={'text': f\"Increase in Songs Over Time\", 'x': 0.5,\n                         'xanchor': 'center', 'font': {'size': 20}}, yaxis_title=\"Increase\", showlegend=False)\nfig.show()","81f64364":"song_features = normalized_music.columns[2:14]\nsong_features","69050679":"normalized_music_by_year = normalized_music.groupby('year').mean()\nfig = px.line(normalized_music_by_year[song_features])\nfig.update_layout(title={'text': \"Features for Songs Over Time\", 'x': 0.5,\n                             'xanchor': 'center', 'font': {'size': 20}}, yaxis_title=\"Z Score\")\nfig.show()","da8b35f4":"music_by_year = music_df.groupby('year').mean()\ntsne_pipeline = Pipeline([('scaler', StandardScaler()), ('tsne', TSNE(n_components=2, random_state=SEED))])\nX = music_by_year.to_numpy()\ngenre_embedding = tsne_pipeline.fit_transform(X)\nprojection_year = pd.DataFrame(columns=['x', 'y'], data=genre_embedding)\nprojection_year['year'] = music_by_year.index\nprojection_year","280f6aac":"fig = px.scatter(projection_year, x='x', y='y', color='year', hover_data=['x', 'y', 'year'])\nfig.update_layout(title={'text': \"Yearly Progressing in Music After Dimensionality Reduction\", 'x': 0.5,\n                             'xanchor': 'center', 'font': {'size': 20}})\nfig.show()","a7a17c44":"fig = go.Figure()\nfig.add_trace(go.Scatter(name=\"years\", x=projection_year['x'], y=projection_year['y'], text=projection_year['year'], marker_color=projection_year['year'], \n                               marker=dict(colorbar=dict(title=\"year\")), mode='markers'))\nfig.add_trace(go.Scatter(name='genres', x=mean_projection['x'], y=mean_projection['y'], text=mean_projection['genre'], mode='markers'))\nfig.update_layout(title={'text': \"Yearly Progressing in Music After Dimensionality Reduction\", 'x': 0.5,\n                             'xanchor': 'center', 'font': {'size': 20}})\nfig.show()","8f63a995":"normalized_music_genre = normalized_music.groupby(['Genre', 'year']).mean()\nfig = px.line(normalized_music_genre.loc['R&B;'][song_features])\nfig.update_layout(title={'text': \"Features for R&B; Over Time\", 'x': 0.5,\n                             'xanchor': 'center', 'font': {'size': 20}}, yaxis_title=\"Z Score\")\nfig.show()","d2abc7a2":"genres = []\nfor i in range(len(influence_df)):\n    my_set = id_genre_dict[influence_df.loc[i]['Influencer']]\n    for genre in my_set:\n        genres.append(genre)\n        break","5b5ed3db":"influence_genre_df = influence_df.copy()\ninfluence_genre_df['genre'] = genres\nvis_figure(influence_genre_df.loc[influence_genre_df['genre'] == 'R&B;'], LOOK_AT)","e4562174":"<h3> Genre Clustering <\/h3>\n\nWe don't have to use K-Means to analyze our clusters; instead, we can analyze the clusters of genres themselves and find the average standard deviation which will determine how clustered our genres are.","5bc110e2":"<h3> KNN Algorithm <\/h3>\n\nFor the sake of brevity, this analysis will most likely not be included in our final paper; however, it is interesting to note and analyze here. We hypothesize that a KNN classifier will accurately classify an artist's genre iff there are inherent similarities between the music within a genre.","596e2f54":"Print out which artists domain over more than one genre. (Are there multiple people named the same thing as the people below?)","4aa0202a":"We then use a similar approach to analyze if a follower\u2019s music is similar to that of their influencers. Since each follower has generally less influencers than influencers have followers, we set our K-value to 10 here. The results of our analysis are shown below.","6dbeb64a":"<h2> Feature Importance with Statistics <\/h2>","0a56be55":"The distribution is clearly right-skewed and the mean influence score for all artists is ~209.","b2de33ce":"# Task #4: Feature Importance & Predictive Modeling\n\n* Are some music characteristics more \u2018contagious\u2019 than others, or do they all have similar roles in influencing a particular artist\u2019s music?\n\nIt is quite difficult to figure out which characteristics necessarily translate over through the network; however, what we can look at is the importance of each characteristics towards defining an artist\/their genre. We analyzed which characteristics are most important for distinguishing between different genres. We did so using two methods: feature importance with a predictive model and feature importance using statistical analyses.\n\nCode courtesy to @DJ1852.","e5cfd551":"We devised a metric for measuring influence called Influence Score, which weights closer, more direct connections heavier than further connections. Concretely, Influence Score is defined as follows:\n![image.png](attachment:image.png)\nwhere *f_d* is the number of followers at depth *d* and *FACTOR* is some exponential smoothing constant (we set *FACTOR* = 2 for this paper).","73d6e4fe":"The number of songs in production drastically increased in the 1950s and 1960s.\n\nWe can extract some useful data regarding the historical events that might have affected the music industry and the influence of music. 1939 marks the end of the Great Depression, which could explain this rapid growth of music production after 1940, as new economic growth could have fueled the music industry. The invention of the computer in 1943 could also explain this large growth in song production, as computers could be used to produce, advertise, and sell music more easily. The years with the highest number of songs were in the 1960s, which coincides with the invention of cassette tapes (Kendall, 2017). These would have provided more access to songs to more people, as well as facilitate the selling and production of music. The number of songs per year started to decline rapidly around 2008, which could be due to the Great Recession in \u201808.\n\nUsing the \u201cinfluence_data\u201d data set, we also created a plot of the number of followers present per year, which can be seen below.","cd79a27b":"<h3> Extra TSNE Reduced Graph Over Time <\/h3>","767e8a81":"Each datapoint is a t-SNE reduced representation of an artist based on their primary characteristics. The colors represent different clusters generated by the K-Means algorithm.","717c6b66":"# Task #1: Similarities Within and Between Genres\n\nNotes: We use artist names instead of ID; when exploring the data, we noticed that some artists (grouped by names) were counted in more than 1 genre, but when using artists (grouped by ID), artists were surjective. Our team was unsure as to whether different IDs implied different artists or if it implied a change in genre of artists. The difference should be negligible for these visualizations, but should be considered for future work.\n\n<h3> Questions of Focus <\/h3>\n\n* Are artists within genre more similar than artists between genres? \n* Are some genres related to others?","a333fc6e":"**Note: In the KNN Algorithm provided below, the Genres ```unknown``` and ```Children's``` were included. The following DataFrame drops them because we removed those genres later in the analysis. If you are trying to replicate this work, please keep note of that.**","4991a154":"<h2> Follower --> Influencer <\/h2>","f7706e4b":"<h2> Are artists within genres more similar than artists between genres? <\/h2>\n\n**Observation:** Per Assumption 3, if artists within a genre are more similar to each other than to artists outside of their genre, the characteristics of their music should be similar enough such that we can accurately cluster and distinguish music from one genre to another.","a0ef8c16":"<h3> Generate New DataFrame <\/h3>\n\nDataFrame is generated from original ```data_by_artist``` file, but now incorporates ```id_genre_dict``` and shows the genre for each artist. Artists with multiple genres were duplicated, and we believe this is a fine practice, since there are ```< 20``` duplicate artists anyways (and there was no simple way to distinguish an artist being in one of the genres compared to the other). We also drop the genre ```Unknown``` (3) and ```Children's``` (4) in our analysis.","b7d203d9":"Correlation matrix for each depth and total score. The total number of people each artist influences seems to level out around Depth 4 (correlations are very similar to every subsequent depth).","d724084a":"**Because we have to use GPUs for Prediction Models\/Feature Importance, the code for those is on another notebook linked here: https:\/\/www.kaggle.com\/ironicninja\/problem-d-prediction-models**","9b3f8c13":"All of the artists on this list are big names (The Beatles, Elvis Presley!)","ac782a7e":"<h3> The Beatles <\/h3>","43406325":"# Team 2121741 Problem D Code (The Influence of Music)\n\nSolution for 2021 ICM (Interdisciplinary Contest in Modeling).\n\n<h2> Introduction <\/h2>\n\nThis notebook focuses on analyzing the influence of music through networks using illustrative visualizations, statistical analyses, and predictive modeling. In the first section, we analyze the inherent similarities of music within and between genres using a K-Means clustering algorithm and visualize the data using T-distributed Stochastic Neighbor Embedding dimensionality reduction. We continue by analyzing which genres have music most similar to each other (aside from themselves). To do so, we utilize a Pearson correlation matrix and plot the genres with significant correlation on a bar chart.\n\nIn the second section, we devise a novel method of calculating influence scores by combining a breadth-first search algorithm with an exponential decay function, and then plot the top influence scores on a bar chart. We also extract the timeframe of an influencer influencing a follower and create meaningful subnetwork visualizations using NetworkX.\n\nIn the third section, we analyze whether influencers have a significant impact on their followers by deploying a K-Nearest Neighbors algorithm. We observe that influencers should have similar music to their followers if there is true influence, and therefore they should be neighbors with one another when plotted in multi-dimensional space. \n\nIn the fourth section, we built a predictive model using XGBoost and extracted the most important & distinguishing features for genre prediction. We supplemented our feature importance analysis with alternative statistical testing.\n\nIn the fifth and final section, we explored trends in how songs characteristics have changed over time and connected them to real world events and explanations. We made observations about trends for all songs, and also specifically analyzed Rhythm and Blues (R&B) and its most influential artists.\n\nThrough these analyses, we hope to enlighten the reader on the influence of music through networks and how music has changed over time using a combination of math, computer science, and of course, music.\n\n<h2> Problem Statement <\/h2>\n\nProblem Statement: https:\/\/www.comap-math.com\/mcm\/2021_ICM_Problem_D.pdf\n\nMusic has the power to culturally, morally, and emotionally influence our society. The sound and messages artists release through their art form directly impact their followers and listeners in powerful ways (Huang, 2014). In an effort to better understand the impact of music on musical artists and society, we have been asked by the Integrative Collective Music (ICM) Society to analyze musical influence across time, genres, and artists. The primary focus of this paper is to capture revolutionary changes, extraneous circumstances, and influential artists using illustrative visualizations and analyses.\n\n<h2> Important Definitions <\/h2>\n\n* **Cluster:** A group of data tightly packed together in some dimension.\n* **Feature:** Analogous to \u201ccharacteristic\u201d. This notation is used interchangeably throughout the paper.\n* **Primary artist features:** ['danceability', 'energy', 'valence', 'tempo', 'loudness', 'mode', 'key', 'acousticness', 'instrumentalness', 'liveness', 'speechiness', 'duration_ms', 'popularity', 'count']\n* **Z-score:** A standard unit of measurement. Each sample x in our dataset is scaled down to z-scores using the formula z = x-\u03bc\/\ud835\udf0e, where \u03bc is the mean and \ud835\udf0e is the standard deviation.\n\n<h2> Problem Assumptions <\/h2>\n\n* **Assumption 1:** The data given in the datasets \u201cinfluence_data\u201d, \u201cfull_musics_data\u201d, \u201cdata_by_artist\u201d, and \u201cdata_by_year\u201d is accurate.\n    * **Reasoning:** The efficacy of our analysis is largely dependent on the accuracy of the data. Because we cannot use any other data in this paper, we must assume that the data provided to us is correct.\n* **Assumption 2:** Each feature in the dataset has equal weighting; normalizing the data using z-scores is sufficient for analysis.\n    * **Reasoning:** There\u2019s no reasonable way to weigh certain features heavier than others.\n* **Assumption 3:** Two genres or artists being similar is defined by and only by the inherent characteristics in their music, and not by extraneous factors such as genre and song title.\n    * **Reasoning:** The whole purpose of using these numerical features is to analyze whether there are inherent similarities between genres\/artists.\n* **Assumption 4:** Drop song and artists in the genre \u201cUnknown\u201d and \u201cChildren\u2019s\u201d.\n    * **Reasoning:** There are only 3 artists labeled under \u201cUnknown\u201d and 4 artists labeled under \u201cChildren\u2019s\u201d, compared to the 5653 total artists in the data.\n\n<h2> Appendices\/Other Resources <\/h2>\n\n* Prediction models notebook (to separate GPU usage): https:\/\/www.kaggle.com\/ironicninja\/the-influence-of-music-prediction-models\n* Officially submitted paper: https:\/\/docs.google.com\/document\/d\/1GxArjRCE95swOcifBco8uLnS9-ABKzQetMoKBTBmtWA\/","b32d8339":"Features of songs over time. Given the graphs of these features over time, we looked at important changes that indicated musical revolutions.\n\nThe most apparent change of music was in the 1960s, where the acousticness of music dropped heavily. This identified one of the most important revolutions in music history, when electric guitars and amplification became a mainstay in popular music. Following this, acousticness continued to decline with the introduction of synthesizers and sampling.\n\nExplicitness also rose over time, most notably in the past decade, and this actually indicated a revolution in how music reaches its listeners. In the age of radio, stations censored lyrics and generally discouraged explicitness. However, with the explosion of streaming services in the past few years, musicians that publish to these services are freer to use explicit language in their songs (Ross, 2017).\n\nLoudness also followed a steady upwards trend, which indicated a musical phenomenon referred to as the \u201cloudness war\u201d. Throughout time, producers continued to drive up the loudness of their songs, as louder songs generally stood out more compared to quieter ones (NPR). In the 1990s, the arrival of digital signal processing allowed producers to drive up loudness even more, which correlated with increasing loudness on the graph.\n\nFinally, we noticed that valence followed a downwards trend. Similarly we calculated the average mode each year, and that also followed a downwards trend. Since a mode of 0 indicated a minor key, and minor keys are generally less bright than major keys, we took this as another indicator of decreasing positivity. Though this pattern was interesting, it was difficult to connect it to any specific events, perhaps it indicated that people are generally more negative recently, or that sad music has become more trendy.\n","15a00834":"Finally, we noticed that valence followed a downwards trend. Similarly we calculated the average mode each year, and that also followed a downwards trend. Since a mode of 0 indicated a minor key, and minor keys are generally less bright than major keys, we took this as another indicator of decreasing positivity. Though this pattern was interesting, it was difficult to connect it to any specific events, perhaps it indicated that people are generally more negative recently, or that sad music has become more trendy.\n\n(Additionally, we noticed that popularity followed an upwards trend, but since the popularity metric was based on recent listeners, it seemed obvious that the more recent songs would be more popular.)\n\nFinally, we analyzed genre and artist-specific trends (Fig, 25-26). We focused on R&B, and found a strong downwards trend in the average acousticness. Then, we filtered songs in R&B to only ones authored by James Brown, the most influential in the genre according to our metrics, and we found the same downwards trend. It was hard to establish whether he caused this trend though, especially considering the small amount of data we had for the artist. (The graphs are not necessarily depicted here).","ab5b75b7":"# Task #2: Measuring Influence\n\n<h3> Questions of Focus <\/h3>\n\n* How do we develop a model to measure influence?\n* After how many years were most artists influenced?\n* Visualizing and Exploring a Subnetwork","2fcc2a4a":"<h2> Are some genres related to others? <\/h2>","4388a5c3":"Jazz peaked in the 1950s, whereas Pop\/Rock grew drastically in the 1960s.\n\nThe Pop\/Rock genre was responsible for most of the growth in music from the 1960s and onwards. Other genres of music declined while Pop\/Rock grew. The Beatles, which we identified as the most influential artist, formed in 1957, which marks the rise of the Pop\/Rock genre. The popularity and influentialness of the Beatles could have contributed to the rapid growth of Pop\/Rock in the 1960s. Another observation from the data is that Jazz music rose and declined rapidly in popularity from about 1950 to 1970, and was actually the most published genre around 1955. This roughly marks the period when Bebop and cool Jazz were popular. The decline of Jazz from 1955 onwards also mirrors the growth of Pop\/Rock. ","b3c71bda":"<h3> Small Network - Juan Gabriel <\/h3>","46a512f7":"<h2> After how many years were most artists influenced? <\/h2>","2274a838":"Total standard deviation is calculated by $\\sqrt{std_x^2 + std_y ^2}$.","52792f63":"<h3> K-Means Clustering <\/h3>\n\nParameters: https:\/\/realpython.com\/k-means-clustering-python\/.\n\nWe utilize a K-Means Clustering Algorithm to group our data. This algorithm uses Expectation-Maximization to mathematically assign groupings and find the centroid of each cluster (Dabbura, 2018). We initialize the number of clusters equal to the number of genres in our dataset (18). Remember, data is ALREADY normalized with the StandardScaler function here! We set the parameters of n_init and max_iter high to generate the most accurate clustering possible. ","69cd6de7":"Line 1 - The Beatles\u2019 first release; Line 2 - Bob Dylan\u2019s first release; Line 3 - World Wide Web. We would like to note that the above graph overcounts the number of followers as it does not remove duplicates of followers (followers who say they are influenced by more than one influencer). The shape of the graph should be similar to if we only counted unique followers though.\n\nThe trend in the number of followers over time is roughly comparable to the number of songs produced every year. The number of artists who start skyrocketed in the generation after The Beatles, Bob Dylan, and some of the other artists with large influence scores, which confirms the notion that our influence scores are somewhat accurate. Interestingly, the release of the World Wide Web is correlated with a sudden drop in follower count, which may be due to the advent of more original, contemporary music. ","235ee980":"Before we are able to measure influence, we must be able to store a graph of the network in a data structure. We do that with a simple dictionary, where each key represents an influencer, and each value represents that influencer\u2019s followers. ","56225293":"<h3> Medium-Sized Network - Coldplay <\/h3>","aab25f1e":"For each genre, we plot the average value of each feature using the songs of that genre in the \u201cfull_music_data\u201d data set. We normalized the data for each feature in the data set using z-scores to put all the features on the same scale. The results of the analysis are seen in the figure below. Note that since we use z-scores to normalize the data, values may be negative to indicate that a certain genre\u2019s feature\u2019s average is below the average of that feature for all the songs in the data set.","6083e5b9":"<h3> Potential AB Testing with KNN Algorithm <\/h3>\n\nWe don't do any formal analysis here since the KNN algorithm takes quite a long time to run (even with Multiprocessing implemented). However, just note that the average genres classified correctly at random is ~1050 - 1300, and so our KNN algorithm of 2083 correct is quite good.","ed49699b":"<h3> A\/B Testing <\/h3>\n\nWe conduct A\/B Testing with 50 iterations once again.","221a7cc5":"7 nearest neighbors are analyzed. This number was arbitrarily chosen.","38b753b6":"# Conclusion\n\nFrom our K-Means clustering analyses, we find that artists are much more similar within genres than between genres. From our Pearson\u2019s Correlation matrix, we also find that certain genres are more closely related feature wise to each other than to other genres (Fig 5). Using a data network, we were able to model the influence of artists, and we found that the most influential artists were The Beatles.\n\nUsing a KNN algorithm, we were able to measure the similarity between influencers and their followers. We found that influencers seem to influence their followers more than followers are influenced by their influencers. However, in all there are not that many significant similarities between the music of influencers and their followers. We also determined the features that distinguish genres from each other. By using the standard deviation of the features across genres, we found that instrumentalness was the best feature to distinguish between genres.\n\nBy analyzing the changes in song number, genre popularity, and song features over time, we were able to attribute some of these changes to certain historical events. For example, the rise of new technologies like the cassette tape and CD in the second half of the 20th century sparked rapid growth in music production, and the formation of the Beatles marked the rise of the pop\/rock genre.\n\nAdditionally, we found that the popularization of the electric guitar led to a drop in the acousticness of songs, and the popularization of music streaming platforms led to higher explicitness in songs. Finally, we connected trends in genres to major influencers in those genres, for example, we observed that a decrease in the overall acousticness of R&B coincided with a decrease in the acousticness of James Brown\u2019s songs.\n\n","429f4079":"We then measure the total standard deviation for the values in that data, and repeat this procedure for every feature. The feature with the highest standard deviation should be the most distinctive feature, and thus is most useful\/important when distinguishing between genres.","ea256715":"From the data above, we can isolate the distinctive features of each genre. For example, Avant-Garde is characterized with higher-than-average acousticness and lower-than-average loudness and energy and Country is characterized with higher-than-average danceability and lower-than-average instrumentalism and duration. Next, we analyze how each feature varies across each genre. For each feature, we plot the average value of the feature for each genre. Again, we used data normalized using z-scores to do this. The results are shown below.\n","7a57ad42":"# References\n\nLinks for the references are at the bottom of the official paper.\n\n* Dabbura, I. (2018).  K-means Clustering: Algorithm, Applications, Evaluation Methods, and Drawbacks.\n* Huang, B. (2014). What Kind of Impact Does Our Music Really Make on Society? \n* Kapri, A. (2020). PCA vs LDA vs T-SNE \u2014 Let\u2019s Understand the difference between them!\n* Kendall, J. (2017). From Discs to Digital: The Odd History of Music Formats.\n* NPR (2009). The Loudness Wars: Why Music Sounds Worse.\n* Ross, E. (2017). Parental Advisory: How Songs With Explicit Lyrics Came to Dominate the Charts.","1fe090e4":"A Pearson Coefficient Value > 0.7 or < -0.7 implies significant positive or negative correlation, respectively.\n\nIt is hard to see any meaningful correlation between genres using a matrix, so we extracted any significant correlation values and plotted them on a bar chart.\n","ae2eb3dc":"<h3> TSNE Dimensionality Reduction <\/h3>\n\nDocumentation: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.manifold.TSNE.html\n\nWe also utilize T-distributed Stochastic Neighbor Embedding (t-SNE) to reduce the dimensionality of our data from 14 (length of primary artist features) to 2. Since our primary objective with dimensionality reduction is visualization (so not for predictive models), we choose t-SNE instead of the popular alternative Principal Components Analysis (Kapri, 2020).","01b501e1":"<h3> Analysis - Maximum Clustering Coefficient <\/h3>\n\nTo analyze whether the clustering algorithm was successfully able to separate different genres based on their features, we devise a test statistic called the **Maximum Clustering Coefficient (MCC)**. The Maximum Clustering Coefficient is determined by the cluster that has the most artists divided by the total number of artists in that genre.","c05dbd2d":"# Task #3: Impact of Influencers\n\n* Do the \u2018influencers\u2019 actually affect the music created by the followers? \n\n**Observation:** The more similar influencers and their followers\u2019 music are to each other, the greater the impact that influencer had on the follower.\nTo analyze this potential similarity, we employ a K-Nearest Neighbors (KNN) algorithm, which finds the K artists with music most similar to another artist. First, we converted our features into Z-scores for a stable, standard unit. Then, we used Euclidean distance for our measure of similarity.","0cdc1b45":"Actual MCC (blue) plotted against A\/B tested MCC (red). Black whisker lines represent 95% confidence interval (2 STD away from mean). \n\nIt is quite obvious that our clustering results are statistically significant, as aside from Avant-Garde, the p-value for all genres are < 0.05. Thus, we can confidently conclude that artists within one genre are more similar than artists within another genre, since the K-Means clustering algorithm was able to, for the most part, accurately cluster the data.","9a0302e6":"Folk and Local are the most correlated genres based on their features.\n\nIn essence, yes, some genres are related to each other, but most have low correlation (if at all). Note that our correlation is calculated only from the inherent numerical features of each genre.","69f9b2bc":"From the data above, we also get a sense for which features are most distinctive for each genre. For example, from the graph for comedy\/speechiness, we can see that Comedy\/Spoken has a much, much higher speachiness, liveliness, and explicit values than that of any other genre. We also see, for example, that New Age has a higher instrumentalness value than other genres and that Classical has lower loudness and energy values than other genres.","ce14a613":"For this scenario, we set our K-value to 20, which means we store each influencer\u2019s 20 closest neighbors (excluding itself) in a set. We find the intersection between that KNN-generated set and each influencer\u2019s followers and take the length of that intersection to extract the total amount of overlap.","d35577c8":"<h2> Visualizing and Exploring a Subnetwork <\/h2>\n\nAlthough we can visualize a small portion of the network, it is hard to visualize the entire network in a meaningful way due to the sheer number of connections present. We present three visualizations below: one of a small subnetwork to Depth 10, one of a medium-sized subnetwork to Depth 10, and the final of the Beatles with Depth 1. We use ```influence_df``` to find which subnetworks to analyze.","8991540d":"Using that graph, we can calculate influence by implementing a breadth-first search algorithm (BFS) which works as follows: (pseudocode)","2978abd7":"<h3> Generate Dictionary <\/h3>","05caab7a":"<h2> Influencer --> Follower <\/h2>","b7f29c45":"<h3> A\/B Testing on Max Clustering Coefficient <\/h3>\n\nWe also deploy an A\/B Test with 50 iterations to determine whether our results are significant.","e57d0b96":"<h2> Feature Importance & Predictive Modeling with XGBoost <\/h2>","0ddb8472":"<h3> Analysis - Correlation Matrix <\/h3>\n\nTo answer this question, we look for whether certain genres are correlated with each other. Our measure of correlation is the Pearson Coefficient, which is defined as:\n<img src=\"https:\/\/media.geeksforgeeks.org\/wp-content\/uploads\/20200311233526\/formula6.png\"> <\/img>\n> <a href=\"https:\/\/media.geeksforgeeks.org\/wp-content\/uploads\/20200311233526\/formula6.png\"> Source <\/a>\n\nShown below is a Pearson correlation matrix for each genre (excluding the dropped genres).\n","e2409349":"# Task #5: Changes Over Time - Characteristics + Revolutionary Influencers\n\nTo first get an idea of how the music industry changes over time, we used the \u201cfull_music_data\u201d data set to plot the number of songs released per year. This plot can be seen below.\n\nAnalysis courtesy to @treywiedmann and @DJ1852.","f25c5c46":"In conclusion, instrumentalness (rank 1 & 2) is definitely the most distinctive feature, followed by speechiness (rank 6 & 1) and acousticness (rank 2 & 4). Using both analyses, explicit is not a very distinctive feature.","657d37ce":"Implement breadth-first search algorithm to extract the number of followers for each influencer.","739fbc9f":"Bar graph of the intersection between a follower\u2019s influencers and the followers\u2019s 10 most similar artists. The vertical line represents the mean.\nFor a vast majority of followers, very few of a follower\u2019s influencers fall within the follower\u2019s most similar artists, similar to the analysis of influencer \u2192 follower. \n\nOn average, only 0.253\/10 artists (2.5%) overlap between a follower\u2019s influencers and the follower\u2019s most similar artists. \n\nFrom these two analyses, on average, influencers seem to influence their followers more than followers are influenced by their influencers. However, neither influencers nor followers have that similar music compared to their counterparts\u2014the influencer-follower connection between an artist and another artist does not confidently result in similar music between the two.\n\nWe leave out an application of the clustering algorithm here since it is not necessary and is most likely computationally inefficient.","34566420":"<h3> Specific Genre Analysis - R&B; <\/h3>","7c690dc9":" On average, influencers influence their followers after 12.5 years, whereas followers are influenced by their influencers after 14.7 years. \n","bda0782c":"Potential code for adding genre labels.","62451b09":"Bar graph of the intersection between an influencer\u2019s followers and the influencer\u2019s 20 most similar artists. The vertical line represents the mean.\n\nWe can see that for a majority of influencers, only a few of their followers are the people who have music most similar to them. On average, only 1.3\/20 artists (6.5%) overlap between an influencer\u2019s followers and an influencer\u2019s most similar artists. \n"}}