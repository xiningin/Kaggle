{"cell_type":{"114085de":"code","48267c69":"code","6db1f24f":"code","6bc530d7":"code","49f7d29e":"code","a49944ea":"code","15193d5c":"code","e6465230":"code","52e09012":"code","88e1b2c2":"code","19d20592":"code","41c05301":"code","9ef8d1bf":"code","30a294df":"code","c887885c":"code","161c1f3e":"code","09b5258f":"code","d97439a4":"code","04a43e61":"code","20cc7c01":"code","ca09c2f7":"code","726c6637":"markdown","d5e9bbb3":"markdown","57e85f9d":"markdown","7f16bff7":"markdown","5514ab25":"markdown","05b1a284":"markdown","6cef4a3e":"markdown","32bee982":"markdown","6f6644d8":"markdown","5458da65":"markdown","92d76948":"markdown","a54e116f":"markdown","b030fde5":"markdown","40e4cfce":"markdown","836dad52":"markdown","2771dbd7":"markdown","ff37a377":"markdown"},"source":{"114085de":"# Importing the important libraries\n\nimport numpy as np\nimport random\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler , MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report , confusion_matrix\nfrom sklearn.preprocessing import PowerTransformer\n\n\nimport pylab\nimport scipy.stats as stats\nimport warnings\nwarnings.filterwarnings('ignore')","48267c69":"# Loading the dataset\n\ndf = pd.read_csv('..\/input\/heart-failure-prediction\/heart.csv')\ndf.head()","6db1f24f":"print(df.shape)\nprint(df[\"HeartDisease\"].value_counts())","6bc530d7":"df.describe()","49f7d29e":"df.info()","a49944ea":"# Seperating the dependent and the independent columns\n\nX = df.drop(\"HeartDisease\" , axis = 1)\ny = df['HeartDisease']","15193d5c":"# Seperating the numerical and the categorical columns\n\nnum_cols = [col for col in X.columns if X[col].dtype != \"object\"]\ncat_cols = [col for col in X.columns if col not in num_cols]","e6465230":"for col in num_cols:\n    sns.histplot(x = col , data = df , kde = True)\n    plt.show()","52e09012":"# Checking which of the columns follows the normal distribution\n\n# for col in num_cols:\n#     stats.probplot(df[col], dist=\"norm\", plot=pylab)\n#     plt.title(col)\n#     pylab.show()","88e1b2c2":"for col in cat_cols:\n    sns.barplot(x = df[col] , y = y)\n    plt.show()","19d20592":"# Checking for the Outlier in the dataset\n\nfor col in num_cols:\n    sns.boxplot(y = col , data = df)\n    plt.show()","41c05301":"normal_cols = [\"Age\",\"MaxHR\",\"RestingBP\"]\nfor col in normal_cols:\n    mean = np.mean(df[col])\n    std = np.std(df[col])\n    lower_range = mean - (3*std)\n    upper_range = mean + (3*std)\n    df[col] = np.where(((df[col] < lower_range) | (df[col] > upper_range))\n                            ,random.randint(int(lower_range),int(upper_range)),df[col])\n    \n  \n","9ef8d1bf":"IQR = np.percentile(df[\"Cholesterol\"],75) - np.percentile(df[\"Cholesterol\"],25)\nlower_bound = np.percentile(df[\"Cholesterol\"],25) - 1.5 * IQR\nupper_bound = np.percentile(df[\"Cholesterol\"],75) + 1.5 * IQR\nmedian_cholesterol = np.median(df[\"Cholesterol\"])\n\ndf[\"Cholesterol\"] = np.where(((df[\"Cholesterol\"] > upper_bound) | (df[\"Cholesterol\"] < lower_bound)) \n                                 ,random.randint(int(np.percentile(df[\"Cholesterol\"],25)),\n                                                 int(np.percentile(df[\"Cholesterol\"],75))),df[\"Cholesterol\"])\n","30a294df":"# Seperating the train and test dataset \n\nx_train,x_test,y_train,y_test = train_test_split(X,y,stratify = y , random_state=42,test_size=0.2)","c887885c":"scaler = StandardScaler()\nscaler.fit(x_train[num_cols])\nx_train[num_cols] = scaler.transform(x_train[num_cols])\nx_test[num_cols] = scaler.transform(x_test[num_cols])","161c1f3e":"x_train = pd.get_dummies(data = x_train , drop_first=True)\nx_test = pd.get_dummies(data = x_test , drop_first = True)","09b5258f":"log_reg = LogisticRegression()\nlog_reg.fit(x_train,y_train)\ny_pred = log_reg.predict(x_test)\n\nprint(log_reg.score(x_test,y_test))\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\n","d97439a4":"from sklearn.model_selection import GridSearchCV\nknn = KNeighborsClassifier()\nparam_grid = {\"n_neighbors\": np.arange(1,50,2)}\nknn_gscv = GridSearchCV(knn, param_grid, cv=5)\nknn_gscv.fit(x_train, y_train)","04a43e61":"knn_gscv.best_params_","20cc7c01":"# knn = KNeighborsClassifier(n_neighbors=7)\n# from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n# for i in range(1,len(x_train.columns)+1):\n#     sfs1 = sfs(knn, k_features=i, forward=True, scoring='accuracy')\n#     sfs1 = sfs1.fit(x_train, y_train)\n#     feat_names = list(sfs1.k_feature_names_)\n#     x_train_feat = x_train[feat_names]\n#     x_test_feat = x_test[feat_names]\n#     knn = KNeighborsClassifier(n_neighbors=7)\n#     knn.fit(x_train_feat,y_train)\n#     y_pred_feat = knn.predict(x_test_feat)\n    \n#     print(\"Considering top {} features\".format(i))\n#     print(feat_names)\n#     print(knn.score(x_test_feat,y_test))\n#     print(classification_report(y_test,y_pred_feat))\n#     print(confusion_matrix(y_test,y_pred_feat))\n#     print(\"-\"*150)","ca09c2f7":"knn = KNeighborsClassifier(n_neighbors=7)\nknn.fit(x_train,y_train)\ny_pred = knn.predict(x_test)\n\nprint(knn.score(x_test,y_test))\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\n","726c6637":"#### Observation:\n- Age and MaxHR almost follow the noraml distribution\n- RestingBP is slightly Right Skewed","d5e9bbb3":"### Observation:\n##### Columns with outlier present in them are\n- RestingBP\n- Cholesterol\n- MaxHR\n- OldPeak","57e85f9d":"- Applying the Logisictic regression","7f16bff7":"- Best K value is 7 as per the 5-fold CV","5514ab25":"- Standardadising the features of training and testing dataset","05b1a284":"### Performing EDA on Categorical Columns based on the target feature","6cef4a3e":"#### More Algorithm will be used and updated. If You like the notbook please upvote the notebook this will really motive me.","32bee982":"#### Main Motive to have the minimum False Negative\n1. False negative here means thought the patient is having a heart disease, still our model predicts that the person is safe and does not have a heart disease.\n2. This could be a very big problem with the life of the patient.\n3. Thus in this notebook we build such model which has the minimum False negative.","6f6644d8":"- Handling Ouliers from Cholestrol feature by using the IQR meathod and replacing it by the random number between the lower_bound and the upper_bound\n- lower_bound is the lower_whisker of boxplot and upper_bound in the upper_whisker of boxplot","5458da65":"- Here we have 918 rows and 11 independent feature and 1 dependent feature as HeartDisease\n- Also the dataset is not imbalance, it is fairly balance","92d76948":"- This shows there is no null values present in the dataset","a54e116f":"- This is the code for the forward feature selection\n- Now finding the feature importance,  finding which feature helps us to reduce the False Negative and increases the Accuracy\n- As there is already less feature considering all features also maximum accuracy and lowest False Negative is achieved with all feature here\n","b030fde5":"#### Applying KNN, finding the best K value and the top features that reduces the False Negative as our model should be such that not patient with the heart disease should be left ","40e4cfce":"### Handling Outliers of dataset\n1. Handling Outlier of the features that are having normal distribution\n- Considering the value that are more than 3 standard deviation away from the mean as outlier\n- Then replacing the outliers with the random number between the lower_range and the upper_range of the respective feature\n- Below:- lower_range is the value which is exactly 3 standard deviation from the mean at the left side and Upper_Range is the value which is exactly 3 standard deviation from the mean at the right side","836dad52":"#### Performing EDA on the dataset","2771dbd7":"#### Observation\n- Sex:- Male has double chance of getting heart disease as compared to females\n- ChestPain:- People having ASY type chest pain has high chance of heart Disease while People with the ATA type chest pain has minimum chance of test pain\n- ExerciseAngina :- People who has Exercise Angina has high chance of Heart Disease\n- ST_Slope:- People with ST_Slop UP has minimum chance of heart Disease\n","ff37a377":"- Applying One hot Encoding on the Categorical columns"}}