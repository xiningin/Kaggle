{"cell_type":{"e6c9f45f":"code","be3573e9":"code","8591f1d4":"code","14b38715":"code","1271cec7":"code","3659fab1":"code","1f232c3e":"code","a0939cc1":"code","57df6aea":"code","745af7ab":"code","ed71df6f":"code","95127083":"code","d7e229d2":"code","9fb2be41":"code","0dd53b9a":"code","e0a26743":"code","3070edd6":"code","1e6f7809":"code","0de5167e":"code","c9a2e655":"code","cb9697d4":"code","c0a87f1e":"code","940cde2a":"code","35ac031c":"code","ed7a8ea9":"code","74cc6f6e":"code","bd48925f":"code","7d217fb9":"markdown","f528b850":"markdown","dba391e7":"markdown","e144ddd6":"markdown","c39e1d74":"markdown","99a3227f":"markdown","0f61ea80":"markdown","a55ac62b":"markdown","54eeddf1":"markdown","17e31d91":"markdown","00097ca6":"markdown","8d2d42dd":"markdown","7c29c509":"markdown","6a15040f":"markdown","297c2223":"markdown","cced5d88":"markdown","fc1bd3f7":"markdown","0a5d9f17":"markdown","84bb67cf":"markdown","f62b639c":"markdown","c661ef3f":"markdown","d6ab1d3d":"markdown","378a0500":"markdown","1a2ef708":"markdown","1ed2b72f":"markdown","c4bac71a":"markdown","818f4bc3":"markdown","3f0aef17":"markdown","e52be35c":"markdown","0f17f9cb":"markdown","9343c5f7":"markdown","499a0d69":"markdown","d245ecee":"markdown","0802f6ad":"markdown","aa696307":"markdown","5e1a8228":"markdown","690f3eb9":"markdown","99ce2817":"markdown","2b521eb7":"markdown","753cd51d":"markdown","ed0469e0":"markdown","b8203880":"markdown","d8e0fc5a":"markdown","7f528d82":"markdown","76179d7f":"markdown"},"source":{"e6c9f45f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","be3573e9":"df = pd.read_csv(\"\/kaggle\/input\/mhblues\/mhb.csv\")\ndf.head()","8591f1d4":"from sklearn.datasets import make_blobs # N\u00e3o se preocupe com essa fun\u00e7\u00e3o, ela apenas gera um conjunto de dados com labels\n\nX, y = make_blobs(1000) # Criando 1000 pontos num conjunto de 2 dimens\u00f5es, cada um com sua label. \ndf_sample = pd.DataFrame(data=np.hstack([X,y.reshape(y.shape[0],1)])) # Criando um dataFrame com base nos dados X, y\n\nsns.scatterplot(x=X[:,0],y=X[:,1],hue=y) # Mostrando o dataset de uma forma visual\nplt.plot()\n\ndf_sample.head() ","14b38715":"X, y = df.iloc[:,:-1], df.iloc[:,-1]","1271cec7":"print(y)","3659fab1":"y_example = np.array([\"banana\"]*10 + [\"maca\"]* 10 + [\"laranja\"] * 10) # Criando vetor com as strings banana, laranja e maca\nnp.random.shuffle(y_example) # Deixando esse vetor de forma aleat\u00f3ria\nprint(f\"Antes da fatoriza\u00e7\u00e3o: {y_example}\")\ny_example, labels =  pd.factorize(y_example) # Fatorizando vetor\nprint(f\"Depois da fatoriza\u00e7\u00e3o: {y_example}\")\nfor i in range(len(labels)):\n    print(f\"O n\u00famero {i} representa a label {labels[i]}\") # Descobrindo qual label est\u00e1 ligada a cada n\u00famero","1f232c3e":"y, labels = pd.factorize(y)","a0939cc1":"from sklearn.model_selection import train_test_split","57df6aea":"X_, y_ = make_blobs(1000,random_state=42) # Criando 100 pontos separados em 3 labels\nX_train_, X_test_, y_train_, y_test_ = train_test_split(X_, y_, test_size=0.1, random_state=42)\n_, c = np.unique(y_,return_counts=True) # Np.unique -> retorna uma tupla (valores \u00fanicos da lista, contagem de cada valor)\n_, c_Train = np.unique(y_train_,return_counts=True)\n_, c_Test  = np.unique(y_test_,return_counts=True)\nprint(100*c\/c.sum()) # Porcentagem da frequ\u00eancia de cada label no dataset \nprint(100*c_Train\/c_Train.sum()) # Porcentagem da frequ\u00eancia de cada label no dataset de treinamento\nprint(100*c_Test\/c_Test.sum()) # Porcentagem da frequ\u00eancia de cada label no dataset de teste\n","745af7ab":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","ed71df6f":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","95127083":"from sklearn.dummy import DummyClassifier\ndummy = DummyClassifier()\ndummy","d7e229d2":"dummy.fit(X_train, y_train)","9fb2be41":"y_pred = dummy.predict(X_test) # conseguindo predi\u00e7\u00e3o com base no X_test\nprint(f\"valor real: {y_test}\")\nprint(f\"valor previsto: {y_pred}\")\n\n\nplt.imshow(np.vstack([y_test.reshape(1,y_test.shape[0]),y_pred.reshape(1,y_pred.shape[0])])) # representando test x pred em forma de imagem\nplt.title(\"labels reais (em cima) e labels previstas (em baixo)\")\nplt.show()\n","0dd53b9a":"dummy.score(X_test, y_test)","e0a26743":"from sklearn.tree import DecisionTreeClassifier\ndecision_tree = DecisionTreeClassifier(random_state=42)","3070edd6":"decision_tree.fit(X_train, y_train)\ndecision_tree.score(X_test, y_test)","1e6f7809":"y_pred = decision_tree.predict(X_test) \nplt.imshow(np.vstack([y_test.reshape(1,y_test.shape[0]),y_pred.reshape(1,y_pred.shape[0])])) \nplt.title(\"labels reais (em cima) e labels previstas (em baixo)\")\nplt.show()","0de5167e":"decision_tree_md = DecisionTreeClassifier(max_depth=5,random_state=42)\ndecision_tree_md.fit(X_train, y_train)\ndecision_tree_md.score(X_test, y_test)","c9a2e655":"print(decision_tree.get_depth())\nprint(decision_tree_md.get_depth())","cb9697d4":"decision_trees = []\n# Cria \u00e1rvores com profundidades de 1 a 10\nfor i in range(1,10):\n    # Ao escolhermos o parametro max_depth, estamos dizendo que a \u00e1rvore n\u00e3o pode passar dessa profundidade, \n    # ou seja, sua profundidade \u00e9 sempre menor ou igual a max_depth \n    decision_tree = DecisionTreeClassifier(max_depth=i,random_state=42)\n    decision_trees.append(decision_tree)","c0a87f1e":"for i, decision_tree in enumerate(decision_trees):\n    # O m\u00e9todo fit cria os n\u00f3s das \u00e1rvores a partir dos dados de treinamento\n    decision_trees[i].fit(X_train, y_train)","940cde2a":"\ntrain_scores = np.zeros(0)\ntest_scores = np.zeros(0)\n\nfor i, decision_tree in enumerate(decision_trees):\n    # O m\u00e9todo score retorna a porcentagem de acertos do modelo com os dados de treino\n    train_scores = np.hstack([train_scores,decision_tree.score(X_train, y_train)])\n    test_scores = np.hstack([test_scores,decision_tree.score(X_test, y_test)])\n\n","35ac031c":"depths = [decision_tree.get_depth() for decision_tree in decision_trees]\nplt.plot(depths,train_scores,label=\"train score\")\nplt.plot(depths,test_scores,label=\"test score\")\nplt.xlabel('Profundidade')\nplt.ylabel('Taxa de Acertos')\nplt.legend()\nplt.show()\n","ed7a8ea9":"from sklearn.ensemble import RandomForestClassifier\ntrain_scores = np.zeros(0)\ntest_scores = np.zeros(0)\nrandom_forests = []\nfor i in range(1,10):\n    # Ao escolhermos o parametro n_estimators, estamos dizendo quantas arvores queremos que nossa random forest tenha\n    random_forest = RandomForestClassifier(n_estimators=i,max_depth=10,random_state=42)\n    random_forest.fit(X_train,y_train)\n    train_scores = np.hstack([train_scores, random_forest.score(X_train,y_train)])\n    test_scores = np.hstack([test_scores, random_forest.score(X_test,y_test)])\n    random_forests.append(random_forest)\nax = sns.lineplot(x=range(1,10),y=train_scores,label=\"train score\")\nsns.lineplot(x=range(1,10),y=test_scores,label=\"test score\")\nplt.xlabel('N\u00famero de \u00c1rvores')\nplt.ylabel('Taxa de Acertos')\nplt.legend()\nplt.show()","74cc6f6e":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\nknn.score(X_test, y_test)","bd48925f":"train_scores = np.zeros(0)\ntest_scores = np.zeros(0)\nknns = []\nfor i in range(1,10):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    train_scores = np.hstack([train_scores, knn.score(X_train,y_train)])\n    test_scores = np.hstack([test_scores, knn.score(X_test,y_test)])\n    knns.append(knn)\nplt.plot(range(1,10),train_scores,label=\"train score\")\nplt.plot(range(1,10),test_scores,label=\"test score\")\nplt.xlabel('N\u00famero de Vizinhos')\nplt.ylabel('Taxa de Acertos')\nplt.legend()\nplt.show()","7d217fb9":"* A sintaxe do `train_test_split` \u00e9 simples, o primeiro argumento \u00e9 o X do dataset, o segundo o y, declaramos qual a quantidade do dataset original queremos usar para teste com o argumento `test_size` (essa quantidade vai de 0 a 1) e o `random_state`, que ser\u00e1 explicado posteriormente. A fun\u00e7\u00e3o no retorna uma tupla `(X_train, X_test, y_train, y_test)`, que representam, respectivamente, o X de treinamento, X de teste, y do treinamento e y do teste (usado **apenas** para valida\u00e7\u00e3o do modelo).\n* A separa\u00e7\u00e3o \u00e9 simples, de forma aleat\u00f3ria,(1 - test_size) dos samples do dataset (X,y) vai para o treinamento e test_size dos samples vai para o teste. Embora seja separado de forma aleat\u00f3ria, a `train_test_split` funciona para n\u00e3o deixar desproporcional o n\u00famero de samples, principalmente no treinamento. Se o seu dataset for balanceado, a fun\u00e7\u00e3o conseguir\u00e1 divir o dataset em n\u00famero de samples por label quase igual, por isso temos uma divis\u00e3o justa. Ela tenta fazer isso para o teste tamb\u00e9m, mas geralmente n\u00e3o \u00e9 t\u00e3o precisa assim para a valida\u00e7\u00e3o. \n* Em rela\u00e7\u00e3o ao `random_state` tempos que primeiro entender como funciona a escolha aleat\u00f3ria para o computador. Para uma explica\u00e7\u00e3o simplificada: O seu computador n\u00e3o joga dados. Todos os n\u00fameros \"aleat\u00f3rios\" que s\u00e3o gerados s\u00e3o na verdade pseudoaleat\u00f3rios, gerados a partir de uma fun\u00e7\u00e3o num\u00e9rica complexa que recebe um n\u00famero `a` e retorna um n\u00famero `b` ou um conjunto de n\u00fameros `B`. O n\u00famero `a`, a entrada dessa fun\u00e7\u00e3o \u00e9 chamado de seed, \u00e9 ela que alimenta a fun\u00e7\u00e3o para gerar um n\u00famero pseudoaleat\u00f3rio, se o mesmo n\u00famero for colocado na fun\u00e7\u00e3o, os mesmos n\u00fameros ser\u00e3o gerados. Se voc\u00ea j\u00e1 se aventurou usando n\u00fameros aleat\u00f3rios, seja em python ou outra linguagens, geralmente a parte do `seed` \u00e9 \"escondida\" de voc\u00ea, o `seed` utilizado \u00e9 um n\u00famero que muda com o tempo, uma pr\u00e1tica muito comum em C \u00e9 usar o [tempo atual da m\u00e1quina em segundos como seed](https:\/\/stackoverflow.com\/questions\/52801380\/srandtimenull-function), assim, sempre que o programa rodar teremos resultados diferentes. O problema \u00e9 que nem sempre queremos isso, \u00e0s vezes queremos replicar exatamente o mesmo resultado para quando formos rodar de novo nosso c\u00f3digo. Assim, poder controlar a seed de fun\u00e7\u00f5es que utilizam artif\u00edcios randomicos (alguns modelos do sklearn, `train_test_split`, etc) \u00e9 algo que ajuda muito. No caso, colocamos a seed da fun\u00e7\u00e3o usando o argumento `random_state`, que recebe um n\u00famero inteiro (a seed).\n* N\u00e3o existe seed certa e errada, o importante \u00e9 manter a seed que voc\u00ea usou para replicar seus resultados. \n* Finalmente, veja um exemplo do uso do train_test_split e as propor\u00e7\u00f5es de cada label no dataset, no treinamento e no teste:","f528b850":"Como voc\u00ea pode ver, todas as suas labels no momento s\u00e3o strings, que representam qual g\u00eanero musical cada sample pertence. O problema disso \u00e9 que os modelos podem n\u00e3o trabalhar muito bem com strings em algumas situa\u00e7\u00f5es, isso porqu\u00ea alguma vez o c\u00e1lculo para o treinamento\/previs\u00e3o depende da computa\u00e7\u00e3o do pr\u00f3prio valor da label. Embora boa parte dos implementados pelo sklearn funcione com strings (incluindo os que veremos agora), uma boa pr\u00e1tica \u00e9 fatorizar as suas labels, ou seja, transformar elas de strings para n\u00fameros inteiros. \nFelizmente, o pandas (`pd`) j\u00e1 possui uma fun\u00e7\u00e3o para isso. A fun\u00e7\u00e3o `factorize` recebe um y (em forma de `pd.Series` ou `np.array`) e retorna uma tupla, na qual o primeiro valor \u00e9 o y \"fatorizado\" e o segundo uma lista na sequ\u00eancia de fatoriza\u00e7\u00e3o. Essa lista \u00e9 \u00fatil para quando queremos saber para qual label o n\u00famero est\u00e1 representando.\n###### Exemplo:","dba391e7":"#### Tarefa 06: Usando o `StandardScaler` do m\u00f3dulo `sklearn.preprocessing`, d\u00ea fit no `X_train` e normalize os dados de `X_train` e `X_test`, armazenando essas normaliza\u00e7\u00f5es nas mesmas vari\u00e1veis (`X_train` e `X_test`).","e144ddd6":"#### Tarefa 13: Crie duas listas: `train_scores` e `test_scores`. Usaremos o `test_scores` para armazenar na posi\u00e7\u00e3o `i` o valor de `decisions_trees[i].score` com os dados de teste e a `train_score` para os dados de treinamento (sim, estamos calculando a precis\u00e3o com os dados que usamos para testar, mas calma, \u00e9 um *overfitting* para raz\u00f5es educativas)","c39e1d74":"Como podemos ver, aumentar a profundidade da \u00e1rvore melhora as previs\u00f5es para os casos de treinamento mas nem sempre melhora para os casos testes, podendo inclusive piorar. Da mesma forma, se a \u00e1rvore for muito rasa, ela pode n\u00e3o conseguir prever bem os casos tamb\u00e9m, por isso \u00e9 importante saber balancear a profundidade da \u00e1rvore. Nesse exemplo espec\u00edfico, vemos que a \u00e1rvore apresenta um pico na sua taxa de acertos quando temos uma profundidade 4, o que parece indicar que essa \u00e9 a melhor escolha. ","99a3227f":"# Aula 04 - Classifica\u00e7\u00e3o Supervisionada","0f61ea80":"Uma coisa a ser observada \u00e9 que com K=1, a taxa de acerto para o `train_score` \u00e9 100%, isso por causa do que falamos na aula sobre overfitting com KNN, usando os dados de treinamento para teste com K=1 o vizinho vai ser o pr\u00f3prio ponto que quer ser estimado, da\u00ed, todos os pontos tem uma taxa de 100% de acerto. Com `n_neighbors=3` temos uma taxa de acerto relativamente alta (+80%), por\u00e9m ainda menor que a das Random Forests\/\u00c1rvores de Decis\u00e3o, isso devido \u00e0 simplicidade do modelo KNN e a como os dados est\u00e3o organizados, talvez aplicando algumas transforma\u00e7\u00f5es nos dados antes do treinamento conseguir\u00edamos um resultado mais satisfat\u00f3rio. Mas mesmo assim, o modelo mostrou que temos dados relativamente bem organizados.","a55ac62b":"Como se pode ver, com uma profundidade menor conseguimos uma precis\u00e3o melhor! Isso signifca que diminuindo o max depth \u00e0s vezes conseguimos generalizar o problema de uma forma melhor. ","54eeddf1":"#### Tarefa 07: A classe relativa \u00e0s \u00c1rvores de Classifica\u00e7\u00e3o \u00e9 a `DecisionTreeClassifier` do m\u00f3dulo `sklearn.tree`. Importe ela e a instancie com a vari\u00e1vel `decision_tree`. Por enquanto, como argumento adicional vamos usar o `random_state=42` para obtermos os mesmos resultados (Lembre-se que as \u00e1rvores de decis\u00e3o tem componentes aleat\u00f3rios em seu come\u00e7o, nas suas retas de divis\u00e3o que ser\u00e3o corrigidas de acordo com o treinamento)","17e31d91":"Como pode ter observado, a precis\u00e3o foi menor do que a pior das \u00e1rvores que n\u00f3s criamos, o KNN \u00e9 um modelo que funciona de forma mais simples que as \u00c1rvores de Decis\u00e3o e, consequentemente, das Random Forests, com isso na maioria dos casos ele apresenta resultados _inferiores_. Mas isso n\u00e3o significa que n\u00e3o deve ser usado, \u00e9 um bom modelo para ver qu\u00e3o bem voc\u00ea est\u00e1 organizando seus dados e pode ter acur\u00e1cias maiores ajustando seus par\u00e2metros (no caso alteraremos apenas o `n_neighbors`).","00097ca6":"### Ajustando par\u00e2metros da \u00c1rvore de Decis\u00e3o\nAgora, em vez de apenas ter o `random_state` como par\u00e2metro usaremos o `max_depth` que define o qu\u00e3o profunda uma \u00e1rvore pode ser, com uma profundidade muito alta a \u00e1rvore acaba cometendo overfitting e com uma profundidade muito baixa acabamos com um underfitting. Definimos a profundidade m\u00e1xima da \u00e1rvore na sua cria\u00e7\u00e3o:\n```python3\narvore = DecisionTreeClassifier(max_depth=40) # \u00c1rvore com profundidade m\u00e1xima de 40\n```\nImportante ressaltar que profundidade m\u00e1xima \u00e9 diferente de profundidade, se no treinamento o modelo ver que com uma profundidade x ele j\u00e1 possui o m\u00e1ximo de precis\u00e3o que alcan\u00e7aria, ele n\u00e3o aumenta sua profundidade desnecessariamente para alcan\u00e7ar a profundidade m\u00e1xima.","8d2d42dd":"### Treinando e Testando seu modelo\n#### Tarefa 08: Treine seu modelo com os dados de treinamento (`X_train` e `y_train`) e veja sua precis\u00e3o com os dados de teste (`X_test` e `y_test`).","7c29c509":"#### Tarefa 04: Fatorize os valores de `y`, deixando o vetor de valores fatorizados em `y` e as labels em `labels`","6a15040f":"### Previs\u00e3o\n\nA previs\u00e3o do nosso modelo \u00e9 feita usando a fun\u00e7\u00e3o `MODELO.predict` e vai receber um argumento `X`, com v\u00e1rios samples sem label e retornar um `y_pred`, com cada elemento sendo a label prevista correspondente ao sample de mesma posi\u00e7\u00e3o em `X`.","297c2223":"Como vimos na aula, usar os dados de teste para o treinamento do modelo levar\u00e1 ao _overfitting_, para fazermos essa divis\u00e3o de uma forma mais r\u00e1pida e justa (voc\u00ea vai entender essa parte depois) n\u00f3s usamos a fun\u00e7\u00e3o `train_test_split` do m\u00f3dulo `sklearn.model_selection`","cced5d88":"#### Tarefa 18 (Opcional): Alterne os valores dos par\u00e2metros que voc\u00ea usou e tente achar precis\u00f5es maiores das achadas aqui.","fc1bd3f7":"#### Tarefa 15: Reproduza as tarefas de 11 a 14, s\u00f3 que usando o `RandomForestClassifier` em vez do `DecisionTreeClassifier`, s\u00f3 que em vez de alternar o `max_depth` alternaremos o `n_estimators` de 1 a 10 e colocaremos os modelos numa lista `random_forests` (N\u00e3o se esque\u00e7a do `random_state=42`). Em vez de plotar em rela\u00e7\u00e3o \u00e0 profundidade, plote em rela\u00e7\u00e3o ao n\u00famero de \u00e1rvores (`n_estimators`).","0a5d9f17":"Por ser um modelo mais simples e n\u00e3o precisar de nenhum componente aleat\u00f3rio, o KNN n\u00e3o precisa de um `random_state` quando instanciado, o \u00fanico argumento adicional que usaremos \u00e9 o `n_neighbors`, que escolhe o n\u00famero de K vizinhos que ele usar\u00e1 para fazer a vota\u00e7\u00e3o na hora de prever um novo ponto. A classe usada \u00e9 a `KNeighborsClassifier` do m\u00f3dulo `sklearn.neighbors`","84bb67cf":"## Importando o Dataset\n\nEstamos estudando modelos que funcionam bem sobre dados tabulados, como estamos estudando classifica\u00e7\u00e3o musical, a tabula\u00e7\u00e3o dos dados ser\u00e1 obtida a partir dos features do som. No dataset que utilizaremos agora, das colunas 0-23 temos informa\u00e7\u00f5es que remetem \u00e0 m\u00e9dia ou ao desvio padr\u00e3o de features n\u00e3o normalizadas do som como Centr\u00f3ide, Flatness, RMS e coeficientes do MFCC. Como a ideia agora \u00e9 apenas a aplica\u00e7\u00e3o dos modelos nesses dados, saber o que cada coluna signfica n\u00e3o \u00e9 o nosso foco. \nCada linha do dataset (chamaremos sample) representa uma m\u00fasica e suas respectivas features. Na coluna 24 temos a label (classe) de cada sample, o desafio em quest\u00e3o \u00e9 construir um modelo que separe bem os tr\u00eas g\u00eaneros musicais baseado nos dados que temos com esse dataset.\n\n\n#### Tarefa 01: Importe o dataset `mhb.csv` para a vari\u00e1vel `df` e pegue a sua cabe\u00e7a","f62b639c":"### Fatoriza\u00e7\u00e3o das labels\n#### Tarefa 03: Printe a sua vari\u00e1vel `y`","c661ef3f":"Como podemos ver, um n\u00famero maior de \u00e1rvores tende a resultar em melhores previs\u00f5es, embora nem sempre isso ocorra. Como realizamos os testes com um n\u00famero pequeno de \u00e1rvores (10 no m\u00e1ximo), a contribui\u00e7\u00e3o de cada \u00e1rvore individual ainda possuia um peso relativamente grande, o que \u00e9 mostrado pela forma como as taxas de acerto variam no gr\u00e1fico acima, mas conforme o n\u00famero de \u00e1rvores aumenta a tendencia \u00e9 de que cada \u00e1rvore individual tenha menos influ\u00eancia no resultado previsto. Vale ressaltar que o resultado obtido pela random forest foi superior ao da \u00e1vore de classifica\u00e7\u00e3o, mesmo com um n\u00famero pequeno de \u00e1rvores. Abaixo, podemos ver o score de cada \u00e1rvore individual da forest, e vemos que, mesmo os resultados individuais n\u00e3o sendo altos sozinhos, o resultado combinado de todas as \u00e1rvores acaba sendo maior pois umas compensam os erros das outras.","d6ab1d3d":"## Normaliza\u00e7\u00e3o dos dados","378a0500":"## Separa\u00e7\u00e3o de dados de Treinamento e de Teste","1a2ef708":"# Random Forest\nA implementa\u00e7\u00e3o da Random Forest \u00e9 muito parecida com a da \u00c1rvore de Decis\u00e3o, podemos definir um `max_depth` (que ser\u00e1 o mesmo para as \u00e1rvores de uma floresta) e podemos definir o `n_estimators`, que \u00e9 o n\u00famero de \u00e1rvores que vamos ter. Em vez de alternar o `max_depth`, trabalharemos apenas com o `n_estimators`. A classe das Random Forests \u00e9 a `RandomForestClassifier` do m\u00f3dulo `sklearn.ensemble`. Importante ver que nesse caso se setarmos o `n_estimators` com um valor a floresta ter\u00e1 esse n\u00famero de \u00e1rvores, n\u00e3o ir\u00e1 \"otimizar\" o n\u00famero no seu treinamento diferente da profundidade.","1ed2b72f":"## Tratamento dos Dados\nPara treinarmos o modelo, ele precisar\u00e1 de um X (tabela de samples sem as labels) e um y (coluna das labels respectivas a cada sample de X). Se isso ficou confuso, talvez um exemplo seja melhor:","c4bac71a":"#### Tarefa 14: plote os valores das duas listas em rela\u00e7\u00e3o ao valor da lista de profundidade `depths` que ir\u00e1 armazenar na posi\u00e7\u00e3o `i` a profundidade da \u00e1rvore `decisions_trees[i]` ap\u00f3s seu treinamento. \nDica: o plot pode ser feito com `plt.plot(depths,lista)`, se quiser legendar a linha use `plt.plot(depths,lista,label=\"legenda\")` e coloque `plt.legend` antes do `plt.show()`","818f4bc3":"Sim, j\u00e1 vimos esse assunto na \u00faltima aula. Mas precisamos ressaltar uma coisa, n\u00e3o aplique a normaliza\u00e7\u00e3o baseada no seu dataset inteiro. Isso porqu\u00ea seus dados de teste n\u00e3o podem ser usados como nenhum tipo de m\u00e9trica para a gera\u00e7\u00e3o do modelo. N\u00f3s normalizaremos o dataset de teste, mas com um normalizador baseado apenas nos dados de treinamento.","3f0aef17":"#### Tarefa 10: Printe a profunidade dos dois modelos","e52be35c":"#### Tarefa 02: Separe os valores correspondentes \u00e0s informa\u00e7\u00f5es dos samples na vari\u00e1vel `X` e os correspondentes \u00e0s labels na vari\u00e1vel `y`. *\u00c9 importante manter a ordem entre os dois*, ou seja, `X[0]` tem que ser da classe `y[0]`, `X[1]` de `y[1]`, etc.\n###### Dica: Voc\u00ea pode usar o `df.iloc` ou  `df.loc` para fazer essa separa\u00e7\u00e3o.","0f17f9cb":"Em rela\u00e7\u00e3o \u00e0 segunda pergunta, uma forma de checar os melhores valores para max_depth \u00e9 checar alguns poss\u00edveis valores e ver qual se d\u00e1 melhor com a precis\u00e3o do teste. \n#### Tarefa 11: Crie uma lista com 10 inst\u00e2ncias de `DecisionTreeClassifier` chamada `decisions_trees`, cada uma com um valor de max depth: a primeira com `max_depth=1`, a segunda com `max_depth=2`, ... (n\u00e3o se esque\u00e7a do `random_state=42`)\n","9343c5f7":"### Valida\u00e7\u00e3o\nComo vimos na \u00faltima aula, existem v\u00e1rias m\u00e9tricas para validar o nosso modelo. A mais simples \u00e9 a precis\u00e3o. Para conseguirmos a precis\u00e3o do nosso modelo usando a fun\u00e7\u00e3o `MODELO.score`, que recebe um `X` e um `y`, no caso relativos aos dados de teste.","499a0d69":"Como pode ser visto, os valores de X (que s\u00e3o as informa\u00e7\u00f5es que o modelo tem para decidir qual classe aquele sample vai pertencer) s\u00e3o separados dos de y (a classe a qual cada sample pertence). No caso acima, n\u00f3s juntamos essas duas informa\u00e7\u00f5es em um dataframe, o que temos que fazer nesse caso \u00e9 separar as informa\u00e7\u00f5es do nosso DataFrame em X e y.","d245ecee":"### Treinamento\n\nO treinamento \u00e9 similar ao fit do `StandardScaler`, que voc\u00ea j\u00e1 est\u00e1 acostumado. usamos a fun\u00e7\u00e3o `MODELO.fit`, que recebe, no caso de modelos de aprendizado supervisionado, um `X` e um `y`, que no caso s\u00e3o os relativos aos **dados de treinamento**.","0802f6ad":"Como se pode ver, a precis\u00e3o n\u00e3o \u00e9 a das melhores (ela \u00e9 dada numa escala de 0 a 1), nos nossos modelos iremos ter melhores resultados.","aa696307":"#### Tarefa 09: Repita a tarefa 07 e 08 para a vari\u00e1vel `decision_tree_md` s\u00f3 que agora instanciando com o `max_depth=5`.","5e1a8228":"## Sintaxe b\u00e1sica de um modelo do Sklearn\nOs modelos ques veremos em seguida seguem em base a mesma sintaxe, para exemplificar, vamos usar o `DummyClassifier` do m\u00f3dulo `sklearn.dummy`, um modelo de treinamento usado apenas para demonstra\u00e7\u00f5es, que n\u00e3o faz nenhum c\u00e1lculo complexo e usa estrat\u00e9gias como escolher sempre a label mais presente no treinamento, n\u00e3o se preocupe com ele.","690f3eb9":"#### Tarefa 12: Treine todos esses modelos com os dados de treinamento. ","99ce2817":"#### Tarefa 16: Instancie o `KNeighborsClassifier` na vari\u00e1vel `knn` sem argumentos adicionais (o `n_neighbors` padr\u00e3o \u00e9 5), treine com os dados de treinamento e obtenha a acur\u00e1cia com os dados de teste.","2b521eb7":"## K-Nearest Neighbors (KNN)","753cd51d":"#### Tarefa 05: Use a `train_test_split` no nosso dataset, com um `test_size` de `0.2` e um `random_state` igual a `42`. (Vamos escolher o `random_state` desse treinamento para termos resultados iguais). Retorne a fun\u00e7\u00e3o nas vari\u00e1veis `X_train, X_test, y_train, y_test`","ed0469e0":"#### Tarefa 8.1 (Opcional): Assim como no exemplo do DummyClassifier, plote a imagem de compara\u00e7\u00e3o das labels reais de `y_test` com a previs\u00e3o do modelo `decision_tree`.","b8203880":"Dessa vez tivemos uma precis\u00e3o maior! 80% de precis\u00e3o \u00e9 um valor interessante, mas qual foi a profundidade escolhida pela \u00e1rvore nas duas ocasi\u00f5es e qual \u00e9 o melhor valor de profundidade que podemos ter nessas condi\u00e7\u00f5es? \nPara a primeira pergunta, podemos ter a profundiade da \u00e1rvore com a fun\u00e7\u00e3o `ARVORE.get_depth()`","d8e0fc5a":"#### Tarefa 17: Reproduza as tarefas de 11 a 14 (ou a tarefa 15), usando o `KNeighborsClassifier`, alterando os `n_neighbors` de 1 a 10, colocando todos os modelos numa lista `knns` e plotando em rela\u00e7\u00e3o ao n\u00famero de vizinhos (`n_neighbors`)","7f528d82":"# \u00c1rvores de Classifica\u00e7\u00e3o\nInicialmente iremos criar uma \u00c1rvore de Classifica\u00e7\u00e3o e verificar o efeito da profundidade da \u00e1rvore em suas previs\u00f5es","76179d7f":"### Instancia\u00e7\u00e3o\nPara criar um modelo, instanciamos a sua classe da forma `modelo = Classe(argumentos adicionais)`. No caso do DummyClassifier n\u00e3o usaremos nenhum argumento adicional"}}