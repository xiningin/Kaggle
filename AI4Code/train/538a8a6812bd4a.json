{"cell_type":{"ac2b474b":"code","383d5ff1":"code","17579f1d":"code","ac0770aa":"code","7e922082":"code","180e15b4":"code","065e0f8d":"code","baa29a3b":"code","5ac9abad":"code","36150439":"code","2a455a62":"code","7be5460a":"code","2b2dfa8c":"code","ed8e5b6c":"code","5602a5fe":"code","1069b914":"code","2a297c50":"code","ef191e08":"code","1c670192":"code","9570a755":"code","cfbf7d19":"code","b6d87933":"code","34b5476a":"code","f7355bc6":"code","bfc22e78":"code","d72d6e18":"code","48879bae":"code","78852eb5":"code","927a3b56":"code","9936f781":"code","8c7cd6cd":"code","a320dffd":"code","1ca81590":"code","9c1e5575":"code","6efe0516":"code","8c6d3fb5":"markdown","c3274533":"markdown","cc9161b7":"markdown","1fda08c4":"markdown","3e3f6ff8":"markdown","b5f61599":"markdown","e57d6eb2":"markdown","6a26e84c":"markdown","505a93cc":"markdown","a0c09a5b":"markdown","7157efe2":"markdown","0df3350e":"markdown","53b5f2c6":"markdown","4f7ca3dc":"markdown"},"source":{"ac2b474b":"import numpy as np\nimport pandas as pd\nfrom glob import glob\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm_notebook as tqdm\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import *","383d5ff1":"train_df = pd.read_csv('\/kaggle\/input\/land-cover-class\/train.csv')","17579f1d":"train_dir = '\/kaggle\/input\/land-cover-class\/train\/'","ac0770aa":"test_images = sorted([fn.split('\/')[-1] for fn in glob('\/kaggle\/input\/land-cover-class\/test\/*.jpg')])","7e922082":"classes = sorted(train_df['class'].unique())\nclasses","180e15b4":"train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(validation_split=0.2)\ntest_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.\/255)","065e0f8d":"size=224\ntrain_generator = train_datagen.flow_from_dataframe(\n    train_df, \n    directory=train_dir,\n    x_col='fn',\n    y_col='class',\n    target_size=(size, size), \n    subset='training'\n)\n\nvalid_generator = train_datagen.flow_from_dataframe(\n    train_df, \n    directory=train_dir,\n    x_col='fn',\n    y_col='class',\n    target_size=(size, size), \n    subset='validation'\n)\n\ntest_generator = test_datagen.flow_from_directory(\n    directory='\/kaggle\/input\/land-cover-class\/',\n    classes=['test'],\n    batch_size=32,\n    target_size=(size, size), \n    shuffle=False\n)","baa29a3b":"from tensorflow.keras.applications import EfficientNetB0","5ac9abad":"X, y = train_generator[0]\nX.shape, y.shape","36150439":"inp = Input(shape=(224, 224, 3))\nmodel = EfficientNetB0(include_top=False, input_tensor=inp, weights='imagenet', classes=len(classes))\nmodel.trainable = False\n\n# Rebuild top\nx = GlobalAveragePooling2D()(model.output)\nx = BatchNormalization()(x)\n\ntop_dropout_rate = 0.2\nx = Dropout(top_dropout_rate)(x)\nout = Dense(len(classes), activation=\"softmax\")(x)\n\nmodel = tf.keras.Model(inp, out)","2a455a62":"model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n    )","7be5460a":"model.summary()","2b2dfa8c":"#tf.keras.utils.plot_model(model, show_shapes=True, to_file='model.png')","ed8e5b6c":"model.fit(train_generator, epochs=1, validation_data=valid_generator)","5602a5fe":"model.fit(train_generator, epochs=5, validation_data=valid_generator)","1069b914":"def _create_generator(**kwargs):\n    train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n        **kwargs\n    )\n    size=224\n    train_generator = train_datagen.flow_from_dataframe(\n        train_df, \n        directory=train_dir,\n        x_col='fn',\n        y_col='class',\n        target_size=(size, size), \n        subset='training',\n        shuffle=False\n    )\n\n    return train_generator","2a297c50":"np.random.seed(123)","ef191e08":"def plot_augmentation(batch, sample, **kwargs):\n    train_generator = _create_generator(**kwargs)\n\n    imgs = []\n    for i in range(8):\n        imgs.append(train_generator[batch][0][sample].astype(int))\n        \n    fig, axs = plt.subplots(2, 4, figsize=(15, 10))\n    for img, ax in zip(imgs, axs.flat):\n        ax.imshow(img)\n    plt.tight_layout()","1c670192":"plot_augmentation(0, 3)","9570a755":"plot_augmentation(0, 3, horizontal_flip=True, vertical_flip=True)","cfbf7d19":"plot_augmentation(0, 3, rotation_range=45)","b6d87933":"plot_augmentation(0, 3, brightness_range=(0.5, 1.5))","34b5476a":"train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n    validation_split=0.2,\n    brightness_range=(0.8, 1.2),\n#     rotation_range=10,\n    horizontal_flip=True, vertical_flip=True,\n)","f7355bc6":"size=224\ntrain_generator = train_datagen.flow_from_dataframe(\n    train_df, \n    directory=train_dir,\n    x_col='fn',\n    y_col='class',\n    target_size=(size, size), \n    subset='training'\n)\n\nvalid_generator = train_datagen.flow_from_dataframe(\n    train_df, \n    directory=train_dir,\n    x_col='fn',\n    y_col='class',\n    target_size=(size, size), \n    subset='validation'\n)","bfc22e78":"from tensorflow.keras.layers.experimental import preprocessing","d72d6e18":"X, y = train_generator[0]","48879bae":"X.shape","78852eb5":"inp = Input(shape=(224, 224, 3))\nx = inp\n# x = preprocessing.RandomRotation(factor=0.05)(x)\nmodel = EfficientNetB0(include_top=False, input_tensor=x, weights='imagenet', classes=len(classes))\nmodel.trainable = False\n\n# Rebuild top\nx = GlobalAveragePooling2D()(model.output)\nx = BatchNormalization()(x)\n\ntop_dropout_rate = 0.2\nx = Dropout(top_dropout_rate)(x)\nout = Dense(len(classes), activation=\"softmax\")(x)\n\nmodel = tf.keras.Model(inp, out)","927a3b56":"model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n    )","9936f781":"model.fit(train_generator, epochs=5, validation_data=valid_generator, workers=4, use_multiprocessing=True)","8c7cd6cd":"def unfreeze_model(model):\n    # We unfreeze the top 20 layers while leaving BatchNorm layers frozen\n    for layer in model.layers[-20:]:\n        if not isinstance(layer, BatchNormalization):\n            layer.trainable = True\n\n    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n    model.compile(\n        optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n    )","a320dffd":"unfreeze_model(model)","1ca81590":"model.fit(train_generator, epochs=5, validation_data=valid_generator, workers=4, use_multiprocessing=True)","9c1e5575":"def create_submission(model, test_generator, classes, test_images):\n    probs = model.predict(test_generator)\n    preds = np.argmax(probs, 1)\n    pred_classes = [classes[i] for i in preds]\n    sub =  pd.DataFrame({'fn': test_images, 'class': pred_classes})\n    return sub","6efe0516":"sub = create_submission(model, test_generator, classes, test_images)\nsub.to_csv('submission1.csv', index=False)","8c6d3fb5":"# Transfer learning and data augmentation\n\nIn the last lesson we learned how CNNs work and got some pretty decent results on our classification task. In this notebook we will see how we can get even better scores in less than a minute and then see how we can do even better by augmenting our training data.\n\nFirst, we will do exactly the same steps from the previous notebook.","c3274533":"# Your turn\n\nHere are the basi steps to recreate:\n\n1. Load a pretrained model and train a good classifier.\n2. Use Data augmentation to get an even better model, submit to Kaggle.\n\nHere are some things to try to get a better score.\n\n1. Try out different data augmentation settings and see how that affects the score.\n2. Try a different pretrained model. How about Xception: https:\/\/keras.io\/api\/applications\/xception\/\n3. Create an ensemble of different neural networks.","cc9161b7":"## Train model with augmentation\n\nSo let's now see what effect this has on our accuracy. For now we will chose flipping, and moderate rotation and brightness changes.\n\nHowever, one technical thing to be aware of is that using rotation from the ImageDataGenerator class is very slow because it's doing matrix multiplications on the GPU. Rather we can add it into our model later.","1fda08c4":"Holy cow! After only one epoche we are already doing better than with our previous model. How does that work?","3e3f6ff8":"Without augmentation, the network will see exactly the same sample every epoch.","b5f61599":"## Transfer learning\n\nNow let's do sometihng kind of weird. Let's take an already defined and trained model from Keras called EfficientNet.","e57d6eb2":"## Data augmentation\n\nOk, so let's see how we can do even betterwith one last and very powerful technique to prevent overfitting: data augmentation. This doesn't change anything about the network itself but rather modifies the training features.\n\nIn the case of images, for example, the network should still come up with the same output regardless of whether the image is flipped or rotated a little. Let see how we can do this.\n\nHere, we will pass the augmentation arguments to the ImageDataGenerator class.","6a26e84c":"We are going to load the model that has already been trained and just add a few more layers to it. Then we train as usual.","505a93cc":"This model we are using requires images to be of size 224, so we just upscale them.","a0c09a5b":"Finally we can try unfreezing the backbone of the network. That means making these weights trainable as well. Becasuse we don't want to change them too much, let's lower the learning rate as well.","7157efe2":"So which ones make sense? The image should still clearly be identifyable by a human. So use your intuition or just try it out. For example, vertical flipping makes sense for satellite images but not so much for pictures of cars.\n\nCheck out all the possible augmentations in the ImageDataGenerator class: https:\/\/keras.io\/api\/preprocessing\/image\/","0df3350e":"We can also rotate the image within a range. Here we have to be careful though because this will introduce artifacts at the edges.","53b5f2c6":"Finally, we could also change the brightness.","4f7ca3dc":"Now we can turn on horizontal and vertical flipping. This will be randomly chosen every epoch."}}