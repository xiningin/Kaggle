{"cell_type":{"a1f49159":"code","7e695e7f":"code","67dfcc9d":"code","c39ef162":"code","bca9f7b2":"code","ede6c5a7":"code","7d1e1859":"code","d1e7aec5":"code","2d41777c":"code","f17857ae":"code","5213c942":"code","9983c37c":"code","4eca29b3":"code","fe4cc037":"code","e6738b34":"code","9272f747":"markdown","96a90bf0":"markdown","164d8293":"markdown","5f2f08aa":"markdown","3b7c8b83":"markdown","0457390b":"markdown"},"source":{"a1f49159":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.simplefilter('ignore')\nimport plotly.express as px\nfrom sklearn.preprocessing import StandardScaler,LabelEncoder,MinMaxScaler\nimport plotly.graph_objects as go\nfrom sklearn.feature_selection import SelectFromModel\nfrom lightgbm import LGBMClassifier\nfrom sklearn.utils import class_weight\nfrom sklearn.decomposition import NMF\nfrom sklearn.model_selection import train_test_split,KFold, GroupKFold, StratifiedKFold\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error","7e695e7f":"# Initialize local variables\n\nSEED = 1992\nTARGET = ['target']\nSAMPLE = 2000 #For Visualization purposees","67dfcc9d":"# Import the data and prepare for the analysis. I have used sampling for speed up purposes. You can remove it :)\n\ntrain = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/test.csv')\nsample_submission = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/sample_submission.csv')\n\n#Prepare train and test dataset\ntrain = train.drop('id', axis=1)\ntest = test.drop('id', axis=1)\n\n#Checking the null Data\nnull_data = (train.isna().sum().sort_values(ascending=False) \/ len(train) * 100) \nfig, ax = plt.subplots(1,1,figsize=(35, 7)) \nax.bar(null_data.index, 100, color='#dadada', width=0.6) \nbar = ax.bar(null_data.index,null_data, width=0.6) \nax.bar_label(bar, fmt='%.01f %%') \nax.spines.left.set_visible(False) \nax.set_yticks([]) \nax.set_title('Null Data Ratio', fontweight='bold') \nplt.show()\n\ntrain.describe().drop('count').T\\\n        .style.bar(subset=['mean'])\\\n        .background_gradient(subset=['std'])\\\n        .background_gradient(subset=['50%'])\\\n        .background_gradient(subset=['max'])","c39ef162":"def label_encoder(c):\n    le = LabelEncoder()\n    return le.fit_transform(c)\n\nall_df = pd.concat([train, test]).reset_index(drop=True)\nall_df[TARGET]=label_encoder(all_df[TARGET])\ntrain_last_id = train.shape[0]\n\nX = all_df.drop(TARGET,axis=1)\ny = all_df[TARGET]\nprint('Feature dataset format:\\t{}\\nTarget dataset format:\\t{}\\nTrain dataset size:\\t{}'.format(X.shape,y.shape,train_last_id))","bca9f7b2":"# Calculate the classes weights\nclass_ratio = class_weight.compute_class_weight('balanced', y.target.unique(), y.target)\nprint(class_ratio)","ede6c5a7":"#Variable standarization and preparation\nX_scaled = MinMaxScaler().fit_transform(X)\ntrain_X_to_select = X_scaled[:train_last_id]\ntest_X_to_select = X_scaled[train_last_id:]\n\ntrain_y_to_select = y[:train_last_id]\ntest_y_to_select = y[train_last_id:]","7d1e1859":"# How scaled matrix looks like\nX_scaled","d1e7aec5":"num_classes = len(class_ratio)\n\n#Initialize NMF instance\nnmf = NMF(n_components=num_classes, init='random', random_state=SEED)\n\n#Weights calculation\nW_train = nmf.fit_transform(train_X_to_select)\nW_test = nmf.fit_transform(test_X_to_select)","2d41777c":"# Create column labels accoring to nmf components number\n\ncolumn_labels = []\nfor i in range(num_classes):\n        column_labels.append('NMFClass_{}'.format(i+1))\n\nNMF_DF = pd.DataFrame(W_train , columns = column_labels)\nNMF_DF = pd.concat([NMF_DF , y] , axis = 1)","f17857ae":"lgbm_params = {\n    'n_estimators': 45000,\n    'objective' : 'multiclass',\n    'metric' : 'multi_logloss',\n    'random_state': SEED,\n    'learning_rate': 0.02,\n    'min_child_samples': 150,\n    'reg_alpha': 750,\n    'reg_lambda': 9e-2,\n    'num_leaves': 20,\n    'max_depth': 3,#16\n    'colsample_bytree': 0.15,\n    'subsample': 0.7,\n    'subsample_freq': 2,\n    'max_bin': 240,\n    'device':'gpu'\n}","5213c942":"cols = list(NMF_DF.columns)\ncols.remove(\"target\")\n\nNMF_train = NMF_DF[:train_last_id]\nNMF_test = NMF_DF[train_last_id:]","9983c37c":"def KFoldTraining(classifier):\n    test_preds = None\n    train_rmse = 0\n    val_rmse = 0\n    n_splits = 5\n\n    classifier = classifier\n    \n\n    kf = KFold(n_splits = n_splits , shuffle = True , random_state = 42)\n    for fold, (tr_index , val_index) in enumerate(kf.split(NMF_train[cols].values , NMF_train['target'].values)):\n\n        print(\"-\" * 50)\n        print(f\"Fold {fold + 1}\")\n\n        x_train,x_val = NMF_DF[cols].values[tr_index] , NMF_train[cols].values[val_index]\n        y_train,y_val = NMF_DF['target'].values[tr_index] , NMF_train['target'].values[val_index]\n\n        eval_set = [(x_val, y_val)]\n\n        model = classifier\n        model.fit(x_train, y_train, eval_set = eval_set, verbose = 1000)\n\n        train_preds = model.predict(x_train)\n        train_rmse += mean_squared_error(y_train ,train_preds , squared = False)\n        print(\"Training RMSE : \" , mean_squared_error(y_train ,train_preds , squared = False))\n\n        val_preds = model.predict(x_val)\n        val_rmse += mean_squared_error(y_val , val_preds , squared = False)\n        print(\"Validation RMSE : \" , mean_squared_error(y_val , val_preds , squared = False))\n\n        if test_preds is None:\n            test_preds = model.predict_proba(NMF_test[cols].values)\n        else:\n            test_preds += model.predict_proba(NMF_test[cols].values)\n\n    print(\"-\" * 200)\n    print(\"Average Training RMSE : \" , train_rmse \/ n_splits)\n    print(\"Average Validation RMSE : \" , val_rmse \/ n_splits)\n\n    test_preds \/= n_splits\n    return test_preds","4eca29b3":"LGBM_classifier = LGBMClassifier(**lgbm_params)\nLGBM_pred = KFoldTraining(LGBM_classifier)\n\n\ntest_preds = LGBM_pred.copy() # Next I will make model ensembing","fe4cc037":"sample_submission['Class_1']=test_preds[:,0]\nsample_submission['Class_2']=test_preds[:,1]\nsample_submission['Class_3']=test_preds[:,2]\nsample_submission['Class_4']=test_preds[:,3]\nsample_submission.head()","e6738b34":"sample_submission.to_csv(\"NMF_1.csv\",index=False)","9272f747":"<h1 style=\"background-color:#A50034; font-family:segoeui; font-size:200%; text-align:center; border-radius: 15px 50px;\"> Dataset Preparation <\/h1>\n","96a90bf0":"<h1 style=\"background-color:#A50034; font-family:segoeui; font-size:200%; text-align:center; border-radius: 15px 50px;\"> Prediction <\/h1>\nSample prediction \n","164d8293":"<h1 style=\"background-color:#A50034; font-family:segoeui; font-size:200%; text-align:center; border-radius: 15px 50px;\"> NMF  Non-Negative Matrix Factorization <\/h1>\n\n# This notebook is kind of spinnof of my [PCA from scratch](https:\/\/www.kaggle.com\/marcinstasko\/pca-analysis-tutorial-from-scratch). \n\nAs we have tested many suprvised method in this competition without spectacular sucess I will test NMF with is an example of unsupervised feature selection. We will does not analyse the y labels. <br>\n# If you like this notboook let me know. I will concider to make it as tutorial\n","5f2f08aa":"<h1 style=\"background-color:#A50034; font-family:segoeui; font-size:200%; text-align:center; border-radius: 15px 50px;\">NMF Applicaton<\/h1>\n","3b7c8b83":"<h1 style=\"background-color:#A50034; font-family:segoeui; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 2. Initial Setup-Data Preparaton <\/h1>\n","0457390b":"<h1 style=\"background-color:#A50034; font-family:segoeui; font-size:200%; text-align:center; border-radius: 15px 50px;\"> Notebook development plan<\/h1>\n\n# If you like the notebook I will be happy. This motivate me to develop it :) <br>\n# Plans\n- Making notebook as NMF tutorial - if you will be interested in\n- The ideas in comments<br>\n\nLets connect on [LinkedIn](https:\/\/www.linkedin.com\/in\/marcinstasko\/?locale=en_US) :)"}}