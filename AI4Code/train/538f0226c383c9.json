{"cell_type":{"19861af5":"code","a92830b1":"code","6fe33421":"code","80716c12":"code","a696553b":"code","7a43e054":"code","8deb7a19":"code","c961ae51":"code","19acf851":"code","599e0238":"code","0d5e07d4":"code","a4f3cd60":"code","d9daa5ff":"code","3e2c72e3":"code","17a03fb4":"code","bfff90c6":"code","a90a04cc":"code","8bcb0c6a":"code","0b498804":"code","3441e6d3":"code","201ce7b8":"code","8e0bdcfa":"code","da07fff2":"code","4de6ed6e":"code","a050f395":"code","46d8b1c4":"code","2a6745fb":"code","80be1b24":"code","ecfbd207":"code","db44c955":"code","4c635b31":"code","8e977abf":"code","d3f596a4":"code","d5e97414":"code","6f59dd1a":"code","fb18fa7a":"code","03885494":"code","96a88d0d":"code","686ddc17":"code","e25d0930":"code","5d144706":"code","bdca0fd0":"code","21e67efb":"code","bcdcfa96":"code","f6175eca":"code","cf03042c":"code","d4c70b3c":"code","a995390d":"code","02818704":"code","e6dec851":"code","8b9d987c":"code","f8c491de":"code","602ae722":"code","ee8481bb":"code","1e784e8a":"code","a0ab4e58":"code","c6abcbf1":"markdown","544c59f8":"markdown","e9107eeb":"markdown","7f55c317":"markdown"},"source":{"19861af5":"!pip install pydot graphviz","a92830b1":"# for loading\/processing the images  \nfrom keras.preprocessing.image import load_img \nfrom keras.preprocessing.image import img_to_array \nfrom keras.applications.vgg16 import preprocess_input \n\n# models \nfrom keras.applications.vgg16 import VGG16 \nfrom keras.models import Model\n\n# clustering and dimension reduction\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n\n# for everything else\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\nimport pandas as pd\nimport pickle\nfrom kaggle_datasets import KaggleDatasets\n\n!pip install pydot graphviz","6fe33421":"path = \"..\/input\/gan-getting-started\/monet_jpg\/\"\n\n# this list holds all the image filename\nmonet_images = []\n\n# creates a ScandirIterator aliased as files\nwith os.scandir(path) as files:\n  # loops through each file in the directory\n    for file in files:\n        if file.name.endswith('.jpg'):\n          # adds only the image files to the monet_images list\n            monet_images.append(path + file.name)","80716c12":"monet_images[0]","a696553b":"# load the image as a 224x224 array\nimg = load_img(monet_images[0], target_size=(224,224))\n# convert from 'PIL.Image.Image' to numpy array\nimg = np.array(img)\n\nprint(img.shape)","7a43e054":"reshaped_img = img.reshape(1,224, 224, 3)\nprint(reshaped_img.shape)","8deb7a19":"x = preprocess_input(reshaped_img)","c961ae51":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport numpy as np\nimport time\nfrom PIL import Image\n%matplotlib inline","19acf851":"# load model\nmodel = VGG16()\n# remove the output layer\nmodel = Model(inputs=model.inputs, outputs=model.layers[-2].output)","599e0238":"features = model.predict(x)\nprint(features.shape)","0d5e07d4":"# load the model first and pass as an argument\nmodel = VGG16()\nmodel = Model(inputs = model.inputs, outputs = model.layers[-2].output)\n\ndef extract_features(file, model):\n    # load the image as a 224x224 array\n    img = load_img(file, target_size=(224,224))\n    # convert from 'PIL.Image.Image' to numpy array\n    img = np.array(img) \n    # reshape the data for the model reshape(num_of_samples, dim 1, dim 2, channels)\n    reshaped_img = img.reshape(1,224,224,3) \n    # prepare image for model\n    imgx = preprocess_input(reshaped_img)\n    # get the feature vector\n    features = model.predict(imgx, use_multiprocessing=True)\n    return features","a4f3cd60":"monet_images[0]","d9daa5ff":"data = {}\np = \"monet_images.pkl\"\n\n# lop through each image in the dataset\nfor monet_image in monet_images:\n    # try to extract the features and update the dictionary\n    feat = extract_features(monet_image,model)\n    data[monet_image] = feat\n          \n \n# get a list of the filenames\nfilenames = np.array(list(data.keys()))\n\n# get a list of just the features\nfeat = np.array(list(data.values()))\nfeat.shape\n(210, 1, 4096)\n\n# reshape so that there are 210 samples of 4096 vectors\nfeat = feat.reshape(-1,4096)\nfeat.shape\n(210, 4096)\n\nunique_labels = list(set(list(range(30))))","3e2c72e3":"pca = PCA(n_components=100, random_state=22)\npca.fit(feat)\nx = pca.transform(feat)","17a03fb4":"kmeans = KMeans(n_clusters=len(unique_labels),n_jobs=-1, random_state=22)\nkmeans.fit(x)","bfff90c6":"# holds the cluster id and the images { id: [images] }\ngroups = {}\nfor file, cluster in zip(filenames,kmeans.labels_):\n    if cluster not in groups.keys():\n        groups[cluster] = []\n        groups[cluster].append(file)\n    else:\n        groups[cluster].append(file)","a90a04cc":"groups[0]","8bcb0c6a":"import cv2\n\nimgs = groups[0]\n\nplt.figure()\n\n#subplot(r,c) provide the no. of rows and columns\nf, axarr = plt.subplots(len(imgs),1, figsize = (len(imgs)*30,30))\n\nfor i, img_path in enumerate(imgs):\n    axarr[i].imshow(cv2.imread(imgs[i]))","0b498804":"# we choose image 0 in every cluster as a representitive\nmonet_dataset_paths = []\nfor i in range(len(groups)):\n    monet_dataset_paths.append(groups[i][0])\nprint(monet_dataset_paths)\nlen(monet_dataset_paths)","3441e6d3":"import os\nimport time\nfrom tqdm import tqdm\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.layers import Conv2D, Flatten, ReLU, BatchNormalization, Conv2DTranspose, Dense\nimport albumentations as A\nimport cv2\nimport glob\nfrom functools import partial\n\nfrom IPython.display import clear_output\nfrom kaggle_datasets import KaggleDatasets\n# import matplotlib.animation as animation\n\nAUTOTUNE = tf.data.AUTOTUNE","201ce7b8":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    INPUT_PATH = KaggleDatasets().get_gcs_path()\nelse:\n    strategy = tf.distribute.get_strategy()\n    INPUT_PATH = \"..\/input\/gan-getting-started\"\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\nprint(\"Input Path:\", INPUT_PATH)","8e0bdcfa":"photo_tfrec_files = tf.io.gfile.glob(INPUT_PATH+\"\/photo_tfrec\/*.tfrec\")","da07fff2":"feature_description = {\n    'image': tf.io.FixedLenFeature([], tf.string),\n}","4de6ed6e":"def parse_tfrecord(record):\n    features = tf.io.parse_single_example(record, feature_description)\n    \n    image = features['image']\n    image = tf.io.decode_image(image)\n    image = tf.reshape(image, (256, 256, 3))\n    \n    return image","a050f395":"def parse_img(image_name):\n    image = cv2.imread(image_name)\n    return image","46d8b1c4":"monet_dataset_paths","2a6745fb":"test_img = parse_img(monet_dataset_paths[0])\nprint(type(test_img))","80be1b24":"p_transformation = 0.1\ntransforms = A.Compose(\n    [\n#         A.Blur(p=p_transformation, blur_limit=(5, 5)),\n#         A.CLAHE(p=p_transformation, clip_limit=(10, 10), tile_grid_size=(3, 3)),\n#         #A.CenterCrop(p=p_transformation, height=100, width=150),\n#         A.ChannelDropout(p=p_transformation, channel_drop_range=(1, 2), fill_value=0),\n#         A.ChannelShuffle(p=p_transformation),\n        A.RandomCrop(p=p_transformation, height=150, width=150),\n        A.Cutout(p=p_transformation, num_holes=8, max_h_size=15, max_w_size=15),\n#         A.Downscale(p=p_transformation, scale_min=0.01, scale_max=0.20, interpolation=0),\n#         A.Equalize(p=p_transformation, mode='cv', by_channels=True),\n        A.HorizontalFlip(p=p_transformation),\n        A.VerticalFlip(p=p_transformation),\n        A.Flip(p=p_transformation),\n#         A.GaussNoise(p=p_transformation, var_limit=(500.0, 500.0)),\n#         A.GridDistortion( p=p_transformation, num_steps=15, distort_limit=(-2., 2.), interpolation=0, border_mode=0, value=(0, 0, 0), mask_value=None),\n#         A.HueSaturationValue(p=p_transformation, \n#             hue_shift_limit=(-100, 100), \n#             sat_shift_limit=(-100, 100), \n#             val_shift_limit=(-100, 100)),\n#         A.ISONoise(p=p_transformation, intensity=(0.0, 2.0), color_shift=(0.0, 1.0)),\n#         A.ImageCompression(p=p_transformation, quality_lower=0, quality_upper=10, compression_type=0),\n#         A.InvertImg(p=p_transformation),\n#         A.JpegCompression(p=p_transformation, quality_lower=0, quality_upper=10),\n#         A.MotionBlur(p=p_transformation, blur_limit=(3, 50)),\n#         A.MultiplicativeNoise(p=p_transformation, multiplier=(0.1, 5.0), per_channel=True, elementwise=False),\n    ]\n)","ecfbd207":"def aug_fn(image, img_size):\n    data = {\"image\":image}\n    aug_data = transforms(**data)\n    aug_img = aug_data[\"image\"]\n    aug_img = tf.cast(aug_img, tf.float32)\n    aug_img = tf.image.resize(aug_img, size=[img_size, img_size], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n    return aug_img\n\ndef process_data(image, img_size):\n    aug_img = tf.numpy_function(func=aug_fn, inp=[image, img_size], Tout=tf.float32)\n    return aug_img\n\ndef random_jitter(image):\n    image = tf.image.resize(image, [286, 286], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n    image = tf.image.random_crop(image, size=[256,256, 3])\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_saturation(image, 0.7, 1.2)\n    return image\n\nBATCH_SIZE = 30\nBUFFER_SIZE = 1000\n\nprint(\"Batch Size:\", BATCH_SIZE)\nprint(\"Buffer Size:\", BUFFER_SIZE)\n\ndef normalize(image):\n    image = tf.image.resize(image, [256,256], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n    image = tf.cast(image, tf.float32)\n    image = (image \/ 127.5) - 1.0\n    \n    return image\n\ndef normalize_monet(image):\n    image = (image \/ 127.5) - 1.0\n    return image\n\nmonet_dataset = list(map(parse_img, monet_dataset_paths))\nmonet_dataset = tf.data.Dataset.from_tensor_slices(monet_dataset)\nmonet_dataset = monet_dataset.map(partial(process_data, img_size=256), num_parallel_calls=AUTOTUNE)\nmonet_dataset = monet_dataset.map(normalize_monet, num_parallel_calls=AUTOTUNE)\nmonet_dataset = monet_dataset.shuffle(BUFFER_SIZE)\nmonet_dataset = monet_dataset.batch(BATCH_SIZE)\nmonet_dataset = monet_dataset.prefetch(AUTOTUNE)\n\n# monet_dataset = map(parse_img, monet_jpg_files)\n# monet_dataset_list = list(map(random_jitter_monet, monet_dataset))\n\n# monet_dataset = tf.data.Dataset.from_tensor_slices(monet_dataset_list)\n# monet_dataset = monet_dataset.map(normalize, num_parallel_calls=AUTOTUNE)\n# monet_dataset = monet_dataset.repeat()\n# monet_dataset = monet_dataset.shuffle(BUFFER_SIZE)\n# monet_dataset = monet_dataset.batch(BATCH_SIZE)\n# monet_dataset = monet_dataset.prefetch(AUTOTUNE)","db44c955":"photo_dataset = tf.data.TFRecordDataset(photo_tfrec_files)\nphoto_dataset = photo_dataset.map(parse_tfrecord, num_parallel_calls=AUTOTUNE)\n# photo_dataset = photo_dataset.cache(\"\/kaggle\/tmp\/photo\")\nphoto_dataset = photo_dataset.map(random_jitter, num_parallel_calls=AUTOTUNE)\nphoto_dataset = photo_dataset.map(normalize, num_parallel_calls=AUTOTUNE)\nphoto_dataset = photo_dataset.shuffle(BUFFER_SIZE)\nphoto_dataset = photo_dataset.batch(BATCH_SIZE)\nphoto_dataset = photo_dataset.prefetch(AUTOTUNE)","4c635b31":"def downsample(filters, size, apply_batchnorm=True):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    \n    result = tf.keras.Sequential()\n    result.add(tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',kernel_initializer=initializer, use_bias=False))\n    \n    if apply_batchnorm:\n        result.add(tf.keras.layers.BatchNormalization())\n\n    result.add(tf.keras.layers.LeakyReLU())\n    return result","8e977abf":"def upsample(filters, size, apply_dropout=False):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    \n    result = tf.keras.Sequential()\n    result.add(\n        tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n                                    padding='same',\n                                    kernel_initializer=initializer,\n                                    use_bias=False))\n    \n    result.add(tf.keras.layers.BatchNormalization())\n    \n    if apply_dropout:\n        result.add(tf.keras.layers.Dropout(0.5))\n        \n    result.add(tf.keras.layers.ReLU())\n    \n    return result","d3f596a4":"def Discriminator():\n    initializer = tf.random_normal_initializer(0., 0.02)\n    \n    inp = tf.keras.layers.Input(shape=[256, 256, 3], name='input_image')\n    \n    down1 = downsample(64, 4, False)(inp) # (bs, 128, 128, 64)\n    down2 = downsample(128, 4)(down1) # (bs, 64, 64, 128)\n    down3 = downsample(256, 4)(down2) # (bs, 32, 32, 256)\n    \n    zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n    conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n                                  kernel_initializer=initializer,\n                                  use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n    \n    batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n    \n    leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n    \n    zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n    \n    last = tf.keras.layers.Conv2D(1, 4, strides=1,\n                                  kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n    \n    return tf.keras.Model(inputs=[inp], outputs=last)","d5e97414":"def Generator():\n    inputs = tf.keras.layers.Input(shape=[256,256,3])\n    \n    down_stack = [\n        downsample(64, 4, apply_batchnorm=False), # (bs, 128, 128, 64)\n        downsample(128, 4), # (bs, 64, 64, 128)\n        downsample(256, 4), # (bs, 32, 32, 256)\n        downsample(512, 4), # (bs, 16, 16, 512)\n        downsample(512, 4), # (bs, 8, 8, 512)\n        downsample(512, 4), # (bs, 4, 4, 512)\n        downsample(512, 4), # (bs, 2, 2, 512)\n        downsample(512, 4), # (bs, 1, 1, 512)\n    ]\n    \n    up_stack = [\n        upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)\n        upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)\n        upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)\n        upsample(512, 4), # (bs, 16, 16, 1024)\n        upsample(256, 4), # (bs, 32, 32, 512)\n        upsample(128, 4), # (bs, 64, 64, 256)\n        upsample(64, 4), # (bs, 128, 128, 128)\n    ]\n    \n    initializer = tf.random_normal_initializer(0., 0.02)\n    last = tf.keras.layers.Conv2DTranspose(3, 4,\n                                         strides=2,\n                                         padding='same',\n                                         kernel_initializer=initializer,\n                                         activation='tanh') # (bs, 256, 256, 3)\n    \n    x = inputs\n    \n    # Downsampling through the model\n    skips = []\n    for down in down_stack:\n        x = down(x)\n        skips.append(x)\n        \n    skips = reversed(skips[:-1])\n    \n    # Upsampling and establishing the skip connections\n    for up, skip in zip(up_stack, skips):\n        x = up(x)\n        x = tf.keras.layers.Concatenate()([x, skip])\n        \n    x = last(x)\n    \n    return tf.keras.Model(inputs=inputs, outputs=x)","6f59dd1a":"with strategy.scope():\n    # Instantiate generators\n    G_PtoM = Generator()\n    G_MtoP = Generator()\n    # Instantiate discriminators\n    D_P = Discriminator()\n    D_M = Discriminator()","fb18fa7a":"tf.keras.utils.plot_model(G_PtoM.layers[1], dpi=64, to_file=\"downsample.png\", show_layer_names=False)","03885494":"tf.keras.utils.plot_model(G_PtoM.layers[-3], dpi=64, to_file=\"upsample.png\", show_layer_names=False)","96a88d0d":"tf.keras.utils.plot_model(G_PtoM, show_shapes=True, dpi=64, to_file=\"generator.png\")","686ddc17":"tf.keras.utils.plot_model(D_P, show_shapes=True, dpi=64, to_file=\"discriminator.png\")","e25d0930":"LAMBDA = 10","5d144706":"with strategy.scope():\n    loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.SUM)","bdca0fd0":"def discriminator_loss(real, generated):\n    real_loss = loss_object(tf.ones_like(real), real)\n    generated_loss = loss_object(tf.zeros_like(generated), generated)\n    total_disc_loss = real_loss + generated_loss\n    total_disc_loss \/= len(real)\n    \n    return total_disc_loss * 0.5","21e67efb":"def generator_loss(generated):\n    return loss_object(tf.ones_like(generated), generated)\/len(generated)","bcdcfa96":"def calc_cycle_loss(real_image, cycled_image): \n        return LAMBDA * tf.reduce_mean(tf.abs(real_image - cycled_image))","f6175eca":"def identity_loss(real_image, same_image):\n    return LAMBDA * 0.5 * tf.reduce_mean(tf.abs(real_image - same_image))","cf03042c":"with strategy.scope():\n    G_MtoP_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    G_PtoM_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n\n    D_M_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    D_P_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)","d4c70b3c":"def generate_images(model, model_r, test_input, figsize=(12,12)):\n    prediction = model(test_input)\n    reconstruction = model_r(prediction)\n    identity = model_r(test_input)\n    \n    display_list = [test_input[0], prediction[0], reconstruction[0], identity[0]]\n    plt.figure(figsize=figsize)\n    title = ['Input', 'Predicted', 'Reconstructed', 'Identity']\n\n    for i in range(4):\n        plt.subplot(1, 4, i+1)\n        plt.title(title[i])\n        # getting the pixel values between [0, 1] to plot it.\n        plt.imshow(display_list[i] * 0.5 + 0.5)\n        plt.axis('off')\n\n    plt.show()\n    \n    return display_list","a995390d":"for images in monet_dataset.take(400):\n    generate_images(G_MtoP, G_PtoM, images)","02818704":"with strategy.scope():\n    G_MtoP_loss = tf.keras.metrics.Mean(name='G_MtoP_loss')\n    G_PtoM_loss = tf.keras.metrics.Mean(name='G_PtoM_loss')\n    D_M_loss = tf.keras.metrics.Mean(name='D_M_loss')\n    D_P_loss = tf.keras.metrics.Mean(name='D_P_loss')","e6dec851":"def train_step(real_m, real_p):\n    \n    with tf.GradientTape(persistent=True) as tape:\n        \n        # G_PtoM translates P -> M\n        # G_MtoP translates M -> M\n        \n        fake_m = G_PtoM(real_p, training=True)\n        cycled_p = G_MtoP(fake_m, training=True)\n        \n        fake_p = G_MtoP(real_m, training=True)\n        cycled_m = G_PtoM(fake_p, training=True)\n        \n        # same_m and same_p for identity loss\n        same_m = G_PtoM(real_m, training=True)\n        same_p = G_MtoP(real_p, training=True)\n        \n        # disctiminator outputs\n        disc_real_m = D_M(real_m, training=True)\n        disc_real_p = D_P(real_p, training=True)\n        \n        disc_fake_m = D_M(fake_m, training=True)\n        disc_fake_p = D_P(fake_p, training=True)\n        \n        # Calculate Loss\n        gen_MtoP_loss = generator_loss(disc_fake_p)\n        gen_PtoM_loss = generator_loss(disc_fake_m)\n        \n        total_cycle_loss = calc_cycle_loss(real_m, cycled_m) + calc_cycle_loss(real_p, cycled_p)\n        \n        identity_loss_p = identity_loss(real_p, same_p)\n        identity_loss_m = identity_loss(real_m, same_m)\n        \n        # Total Loss\n        total_gen_MtoP_loss = gen_MtoP_loss + total_cycle_loss + identity_loss_p\n        total_gen_PtoM_loss = gen_PtoM_loss + total_cycle_loss + identity_loss_m\n        \n        disc_p_loss = discriminator_loss(disc_real_p, disc_fake_p)\n        disc_m_loss = discriminator_loss(disc_real_m, disc_fake_m)\n        \n    \n    # Calculate Gradients\n    gen_mtop_gradients = tape.gradient(total_gen_MtoP_loss, G_MtoP.trainable_variables)\n    gen_ptom_gradients = tape.gradient(total_gen_PtoM_loss, G_PtoM.trainable_variables)\n    disc_m_gradients = tape.gradient(disc_m_loss, D_M.trainable_variables)\n    disc_p_gradients= tape.gradient(disc_p_loss, D_P.trainable_variables)\n    \n    # Apply Gradients to optimizers\n    G_MtoP_optimizer.apply_gradients(zip(gen_mtop_gradients, G_MtoP.trainable_variables))\n    G_PtoM_optimizer.apply_gradients(zip(gen_ptom_gradients, G_PtoM.trainable_variables))\n    D_M_optimizer.apply_gradients(zip(disc_m_gradients, D_M.trainable_variables))\n    D_P_optimizer.apply_gradients(zip(disc_p_gradients, D_P.trainable_variables))\n    \n    # Update Running Loss\n    D_M_loss.update_state(disc_m_loss)\n    D_P_loss.update_state(disc_p_loss)\n    G_MtoP_loss.update_state(total_gen_MtoP_loss)\n    G_PtoM_loss.update_state(total_gen_PtoM_loss)","8b9d987c":"@tf.function\ndef distributed_train_step(real_m, real_p):\n    strategy.run(train_step, args=(real_m, real_p))","f8c491de":"# sample_m = next(iter(monet_dataset))\n# sample_p = next(iter(photo_dataset))\n\n# ptom_preds_images = []\n# mtop_reconstructions_images = []\n# ptop_identity_images = []\n\n# mtop_preds_images = []\n# ptom_reconstructions_images = []\n# mtom_identity_images = []\n\nEPOCHS = 500\n\nfor epoch in range(EPOCHS):\n    \n#     clear_output()\n#     pmp_display_list = generate_images(G_PtoM, G_MtoP, sample_p)\n#     ptom_preds_images.append(pmp_display_list[1])\n#     mtop_reconstructions_images.append(pmp_display_list[2])\n#     ptop_identity_images.append(pmp_display_list[3])\n#     \n#     mpm_display_list = generate_images(G_MtoP, G_PtoM, sample_m)\n#     mtop_preds_images.append(mpm_display_list[1])\n#     ptom_reconstructions_images.append(mpm_display_list[2])\n#     mtom_identity_images.append(mpm_display_list[3])\n    \n    G_MtoP_loss.reset_states()\n    G_PtoM_loss.reset_states()\n    D_M_loss.reset_states()\n    D_P_loss.reset_states()\n    \n    ds = tf.data.Dataset.zip((photo_dataset, monet_dataset))\n    with tqdm(ds, desc=\"Epoch {}\/{}\".format(epoch+1, EPOCHS)) as t:\n        for image_p, image_m in t:\n            distributed_train_step(image_m, image_p)\n            t.set_description(\n                f\"Epoch: {epoch+1}\/{EPOCHS}, \"\n                f\"G_MtoP: {G_MtoP_loss.result():.4f}, \"\n                f\"G_PtoM: {G_PtoM_loss.result():.4f}, \"\n                f\"D_M: {D_M_loss.result():.4f}, \"\n                f\"D_P: {D_P_loss.result():.4f}\"\n            )","602ae722":"test_photo_dataset = tf.data.TFRecordDataset(photo_tfrec_files)\ntest_photo_dataset = test_photo_dataset.map(parse_tfrecord, num_parallel_calls=AUTOTUNE)\ntest_photo_dataset = test_photo_dataset.map(normalize, num_parallel_calls=AUTOTUNE)\ntest_photo_dataset = test_photo_dataset.batch(BATCH_SIZE)\n# test_photo_dataset = photo_dataset","ee8481bb":"plt.figure(figsize=(20,20))\nfor images in test_photo_dataset.take(1):\n    predictions = G_PtoM(images, training=False)\n    for i in range(len(images)):\n        plt.subplot(8,8, 2*i+1)\n        plt.imshow((images[i]+1)\/2)\n        plt.title(\"Photo\")\n        plt.axis('off')\n\n        plt.subplot(8,8, 2*i+2)\n        plt.imshow((predictions[i]+1)\/2)\n        plt.title(\"To Monet\")\n        plt.axis('off')","1e784e8a":"def get_image(arr):\n    arr = (arr + 1) * 127.5\n    arr = tf.cast(arr, tf.int8)\n    return tf.keras.preprocessing.image.array_to_img(arr)","a0ab4e58":"from zipfile import ZipFile\n\nfile_index = 1\nwith ZipFile('images.zip', 'w') as submission_zip:\n    for images in tqdm(test_photo_dataset):\n        predictions = G_PtoM.predict(images)\n        for prediction in predictions:\n            filename = f'photo_to_monet_{file_index}.jpg'\n            img = get_image(prediction)\n            img.save(filename)\n            submission_zip.write(filename)\n            os.remove(filename)\n            file_index+=1\n        if file_index>=9950:\n            break\n    submission_zip.close()\n    \nprint(f\"Saved {file_index} files in images.zip\")","c6abcbf1":"# Using PCA to lower dimentions","544c59f8":"# Creating 30 Monet images dataset","e9107eeb":"# Extracting features using trained model","7f55c317":"# Loading Monet images"}}