{"cell_type":{"d4be262f":"code","04cc8788":"code","48d17fa8":"code","f53dd09a":"code","dadd6791":"code","89a10656":"code","09159e9f":"code","8554352d":"code","227a4bc8":"code","afa01149":"code","ccb91522":"code","c895cfe0":"code","24b899dd":"code","e7fa3396":"code","afa0312f":"code","63fbccf1":"code","dda62415":"code","b5f68314":"code","7699fba1":"code","68ff3fd6":"code","f2a4875a":"code","bd9441d4":"code","f793a311":"code","3b2a74ef":"code","0ebbb8ec":"code","fe5cb9f6":"code","e18a525e":"code","15441900":"code","21ca5fd4":"code","b96697db":"code","ac5ffa67":"code","1309ad49":"code","178b2cc4":"code","21dbfefa":"code","33cebcfb":"code","c846231a":"code","04d72ca3":"code","e22724cd":"code","60b4e96d":"code","f48611b1":"code","14e86e73":"code","477610c8":"code","36025217":"markdown","e9c38ebb":"markdown","617cdd1e":"markdown","db866b5e":"markdown","0cff4556":"markdown","0432e9f4":"markdown","2160bde3":"markdown","129c9427":"markdown","1288ff72":"markdown","21e74b56":"markdown","b108ac38":"markdown","2523a746":"markdown","c071448d":"markdown","04a45ea7":"markdown","b373ef50":"markdown","b863a7c9":"markdown","7549b532":"markdown","e932a07f":"markdown","d4b31eae":"markdown","c8f74367":"markdown","3d3cc24c":"markdown","c8d20b74":"markdown","44507e2e":"markdown","fbeb9631":"markdown","a2a50844":"markdown","89eabbec":"markdown","656184c9":"markdown","8aedf6e2":"markdown","d62d33f2":"markdown","64bc9bf6":"markdown","6fb04f8e":"markdown","668c2d6c":"markdown","1f8c5721":"markdown","b24d59e7":"markdown","b9ffeeaa":"markdown","7d136ded":"markdown","037d1e1f":"markdown","fa136d82":"markdown","a4ca8ffb":"markdown","f41e2c38":"markdown","b1a00bce":"markdown","dad1d092":"markdown","49472125":"markdown","affd64e9":"markdown","eb9b50f4":"markdown","fd87c34f":"markdown","1edad609":"markdown","f4db1fb4":"markdown","792c6f76":"markdown","48bfb34f":"markdown","85bb2ab8":"markdown","50da9127":"markdown","eb72391e":"markdown","40e23bdc":"markdown","c0056d0a":"markdown","1d57e317":"markdown","1731701b":"markdown","aafb2c26":"markdown","d15e834d":"markdown"},"source":{"d4be262f":"import numpy as np\nimport pandas as pd\n\nimport os\nimport math\nimport time\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.figure_factory as ff\nimport plotly.graph_objects as go\nimport plotly.express as px\n\n# Below libraries are for text processing using NLTK\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\n\n# Below libraries are for feature representation using sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Below libraries are for similarity matrices using sklearn\nfrom sklearn.metrics.pairwise import cosine_similarity  \nfrom sklearn.metrics import pairwise_distances","04cc8788":"news_articles = pd.read_json(\"\/kaggle\/input\/news-category-dataset\/News_Category_Dataset_v2.json\", lines = True)","48d17fa8":"news_articles.info()","f53dd09a":"news_articles.head()","dadd6791":"news_articles = news_articles[news_articles['date'] >= pd.Timestamp(2018,1,1)]","89a10656":"news_articles.shape","09159e9f":"news_articles = news_articles[news_articles['headline'].apply(lambda x: len(x.split())>5)]\nprint(\"Total number of articles after removal of headlines with short title:\", news_articles.shape[0])","8554352d":"news_articles.sort_values('headline',inplace=True, ascending=False)\nduplicated_articles_series = news_articles.duplicated('headline', keep = False)\nnews_articles = news_articles[~duplicated_articles_series]\nprint(\"Total number of articles after removing duplicates:\", news_articles.shape[0])","227a4bc8":"news_articles.isna().sum()","afa01149":"print(\"Total number of articles : \", news_articles.shape[0])\nprint(\"Total number of authors : \", news_articles[\"authors\"].nunique())\nprint(\"Total number of unqiue categories : \", news_articles[\"category\"].nunique())","ccb91522":"fig = go.Figure([go.Bar(x=news_articles[\"category\"].value_counts().index, y=news_articles[\"category\"].value_counts().values)])\nfig['layout'].update(title={\"text\" : 'Distribution of articles category-wise','y':0.9,'x':0.5,'xanchor': 'center','yanchor': 'top'}, xaxis_title=\"Category name\",yaxis_title=\"Number of articles\")\nfig.update_layout(width=800,height=700)\nfig","c895cfe0":"news_articles_per_month = news_articles.resample('m',on = 'date')['headline'].count()\nnews_articles_per_month","24b899dd":"fig = go.Figure([go.Bar(x=news_articles_per_month.index.strftime(\"%b\"), y=news_articles_per_month)])\nfig['layout'].update(title={\"text\" : 'Distribution of articles month-wise','y':0.9,'x':0.5,'xanchor': 'center','yanchor': 'top'}, xaxis_title=\"Month\",yaxis_title=\"Number of articles\")\nfig.update_layout(width=500,height=500)\nfig","e7fa3396":"fig = ff.create_distplot([news_articles['headline'].str.len()], [\"ht\"],show_hist=False,show_rug=False)\nfig['layout'].update(title={'text':'PDF','y':0.9,'x':0.5,'xanchor': 'center','yanchor': 'top'}, xaxis_title=\"Length of a headline\",yaxis_title=\"probability\")\nfig.update_layout(showlegend = False,width=500,height=500)\nfig","afa0312f":"news_articles.index = range(news_articles.shape[0])","63fbccf1":"# Adding a new column containing both day of the week and month, it will be required later while recommending based on day of the week and month\nnews_articles[\"day and month\"] = news_articles[\"date\"].dt.strftime(\"%a\") + \"_\" + news_articles[\"date\"].dt.strftime(\"%b\")","dda62415":"news_articles_temp = news_articles.copy()","b5f68314":"stop_words = set(stopwords.words('english'))","7699fba1":"for i in range(len(news_articles_temp[\"headline\"])):\n    string = \"\"\n    for word in news_articles_temp[\"headline\"][i].split():\n        word = (\"\".join(e for e in word if e.isalnum()))\n        word = word.lower()\n        if not word in stop_words:\n          string += word + \" \"  \n    if(i%1000==0):\n      print(i)           # To track number of records processed\n    news_articles_temp.at[i,\"headline\"] = string.strip()","68ff3fd6":"lemmatizer = WordNetLemmatizer()","f2a4875a":"for i in range(len(news_articles_temp[\"headline\"])):\n    string = \"\"\n    for w in word_tokenize(news_articles_temp[\"headline\"][i]):\n        string += lemmatizer.lemmatize(w,pos = \"v\") + \" \"\n    news_articles_temp.at[i, \"headline\"] = string.strip()\n    if(i%1000==0):\n        print(i)           # To track number of records processed","bd9441d4":"headline_vectorizer = CountVectorizer()\nheadline_features   = headline_vectorizer.fit_transform(news_articles_temp['headline'])","f793a311":"headline_features.get_shape()","3b2a74ef":"pd.set_option('display.max_colwidth', -1)  # To display a very long headline completely","0ebbb8ec":"def bag_of_words_based_model(row_index, num_similar_items):\n    couple_dist = pairwise_distances(headline_features,headline_features[row_index])\n    indices = np.argsort(couple_dist.ravel())[0:num_similar_items]\n    df = pd.DataFrame({'publish_date': news_articles['date'][indices].values,\n               'headline':news_articles['headline'][indices].values,\n                'Euclidean similarity with the queried article': couple_dist[indices].ravel()})\n    print(\"=\"*30,\"Queried article details\",\"=\"*30)\n    print('headline : ',news_articles['headline'][indices[0]])\n    print(\"\\n\",\"=\"*25,\"Recommended articles : \",\"=\"*23)\n    #return df.iloc[1:,1]\n    return df.iloc[1:,]\n\nbag_of_words_based_model(133, 11) # Change the row index for any other queried article","fe5cb9f6":"tfidf_headline_vectorizer = TfidfVectorizer(min_df = 0)\ntfidf_headline_features = tfidf_headline_vectorizer.fit_transform(news_articles_temp['headline'])","e18a525e":"def tfidf_based_model(row_index, num_similar_items):\n    couple_dist = pairwise_distances(tfidf_headline_features,tfidf_headline_features[row_index])\n    indices = np.argsort(couple_dist.ravel())[0:num_similar_items]\n    df = pd.DataFrame({'publish_date': news_articles['date'][indices].values,\n               'headline':news_articles['headline'][indices].values,\n                'Euclidean similarity with the queried article': couple_dist[indices].ravel()})\n    print(\"=\"*30,\"Queried article details\",\"=\"*30)\n    print('headline : ',news_articles['headline'][indices[0]])\n    print(\"\\n\",\"=\"*25,\"Recommended articles : \",\"=\"*23)\n    \n    #return df.iloc[1:,1]\n    return df.iloc[1:,]\ntfidf_based_model(133, 11)","15441900":"from gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nimport pickle","21ca5fd4":"os.chdir(r'\/kaggle\/input\/')","b96697db":"!ls","ac5ffa67":"pickle.format_version","1309ad49":"with open('googlew2v\/word2vec_model', 'rb') as file:\n    loaded_model = pickle.load(file)","178b2cc4":"loaded_model['porter']","21dbfefa":"vocabulary = loaded_model.keys()\nw2v_headline = []\nfor i in news_articles_temp['headline']:\n    w2Vec_word = np.zeros(300, dtype=\"float32\")\n    for word in i.split():\n        if word in vocabulary:\n            w2Vec_word = np.add(w2Vec_word, loaded_model[word])\n    w2Vec_word = np.divide(w2Vec_word, len(i.split()))\n    w2v_headline.append(w2Vec_word)\nw2v_headline = np.array(w2v_headline)","33cebcfb":"def avg_w2v_based_model(row_index, num_similar_items):\n    couple_dist = pairwise_distances(w2v_headline, w2v_headline[row_index].reshape(1,-1))\n    indices = np.argsort(couple_dist.ravel())[0:num_similar_items]\n    df = pd.DataFrame({'publish_date': news_articles['date'][indices].values,\n               'headline':news_articles['headline'][indices].values,\n                'Euclidean similarity with the queried article': couple_dist[indices].ravel()})\n    print(\"=\"*30,\"Queried article details\",\"=\"*30)\n    print('headline : ',news_articles['headline'][indices[0]])\n    print(\"\\n\",\"=\"*25,\"Recommended articles : \",\"=\"*23)\n    #return df.iloc[1:,1]\n    return df.iloc[1:,]\n\navg_w2v_based_model(133, 11)","c846231a":"from sklearn.preprocessing import OneHotEncoder ","04d72ca3":"category_onehot_encoded = OneHotEncoder().fit_transform(np.array(news_articles_temp[\"category\"]).reshape(-1,1))","e22724cd":"def avg_w2v_with_category(row_index, num_similar_items, w1,w2): #headline_preference = True, category_preference = False):\n    w2v_dist  = pairwise_distances(w2v_headline, w2v_headline[row_index].reshape(1,-1))\n    category_dist = pairwise_distances(category_onehot_encoded, category_onehot_encoded[row_index]) + 1\n    weighted_couple_dist   = (w1 * w2v_dist +  w2 * category_dist)\/float(w1 + w2)\n    indices = np.argsort(weighted_couple_dist.flatten())[0:num_similar_items].tolist()\n    df = pd.DataFrame({'publish_date': news_articles['date'][indices].values,\n               'headline':news_articles['headline'][indices].values,\n                'Weighted Euclidean similarity with the queried article': weighted_couple_dist[indices].ravel(),\n                'Word2Vec based Euclidean similarity': w2v_dist[indices].ravel(),\n                 'Category based Euclidean similarity': category_dist[indices].ravel(),\n                'Categoty': news_articles['category'][indices].values})\n    \n    print(\"=\"*30,\"Queried article details\",\"=\"*30)\n    print('headline : ',news_articles['headline'][indices[0]])\n    print('Categoty : ', news_articles['category'][indices[0]])\n    print(\"\\n\",\"=\"*25,\"Recommended articles : \",\"=\"*23)\n    #return df.iloc[1:,[1,5]]\n    return df.iloc[1:, ]\n\navg_w2v_with_category(528,10,0.1,0.8)","60b4e96d":"authors_onehot_encoded = OneHotEncoder().fit_transform(np.array(news_articles_temp[\"authors\"]).reshape(-1,1))","f48611b1":"def avg_w2v_with_category_and_authors(row_index, num_similar_items, w1,w2,w3): #headline_preference = True, category_preference = False):\n    w2v_dist  = pairwise_distances(w2v_headline, w2v_headline[row_index].reshape(1,-1))\n    category_dist = pairwise_distances(category_onehot_encoded, category_onehot_encoded[row_index]) + 1\n    authors_dist = pairwise_distances(authors_onehot_encoded, authors_onehot_encoded[row_index]) + 1\n    weighted_couple_dist   = (w1 * w2v_dist +  w2 * category_dist + w3 * authors_dist)\/float(w1 + w2 + w3)\n    indices = np.argsort(weighted_couple_dist.flatten())[0:num_similar_items].tolist()\n    df = pd.DataFrame({'publish_date': news_articles['date'][indices].values,\n                'headline':news_articles['headline'][indices].values,\n                'Weighted Euclidean similarity with the queried article': weighted_couple_dist[indices].ravel(),\n                'Word2Vec based Euclidean similarity': w2v_dist[indices].ravel(),\n                'Category based Euclidean similarity': category_dist[indices].ravel(),\n                'Authors based Euclidean similarity': authors_dist[indices].ravel(),       \n                'Categoty': news_articles['category'][indices].values,\n                'Authors': news_articles['authors'][indices].values})\n    print(\"=\"*30,\"Queried article details\",\"=\"*30)\n    print('headline : ',news_articles['headline'][indices[0]])\n    print('Categoty : ', news_articles['category'][indices[0]])\n    print('Authors : ', news_articles['authors'][indices[0]])\n    print(\"\\n\",\"=\"*25,\"Recommended articles : \",\"=\"*23)\n    #return df.iloc[1:,[1,6,7]]\n    return df.iloc[1:, ]\n\n\navg_w2v_with_category_and_authors(528,10,0.1,0.1,1)","14e86e73":"publishingday_onehot_encoded = OneHotEncoder().fit_transform(np.array(news_articles_temp[\"day and month\"]).reshape(-1,1))","477610c8":"def avg_w2v_with_category_authors_and_publshing_day(row_index, num_similar_items, w1,w2,w3,w4): #headline_preference = True, category_preference = False):\n    w2v_dist  = pairwise_distances(w2v_headline, w2v_headline[row_index].reshape(1,-1))\n    category_dist = pairwise_distances(category_onehot_encoded, category_onehot_encoded[row_index]) + 1\n    authors_dist = pairwise_distances(authors_onehot_encoded, authors_onehot_encoded[row_index]) + 1\n    publishingday_dist = pairwise_distances(publishingday_onehot_encoded, publishingday_onehot_encoded[row_index]) + 1\n    weighted_couple_dist   = (w1 * w2v_dist +  w2 * category_dist + w3 * authors_dist + w4 * publishingday_dist)\/float(w1 + w2 + w3 + w4)\n    indices = np.argsort(weighted_couple_dist.flatten())[0:num_similar_items].tolist()\n    df = pd.DataFrame({'publish_date': news_articles['date'][indices].values,\n                'headline_text':news_articles['headline'][indices].values,\n                'Weighted Euclidean similarity with the queried article': weighted_couple_dist[indices].ravel(),\n                'Word2Vec based Euclidean similarity': w2v_dist[indices].ravel(),\n                'Category based Euclidean similarity': category_dist[indices].ravel(),\n                'Authors based Euclidean similarity': authors_dist[indices].ravel(),   \n                'Publishing day based Euclidean similarity': publishingday_dist[indices].ravel(), \n                'Categoty': news_articles['category'][indices].values,\n                'Authors': news_articles['authors'][indices].values,\n                'Day and month': news_articles['day and month'][indices].values})\n    print(\"=\"*30,\"Queried article details\",\"=\"*30)\n    print('headline : ',news_articles['headline'][indices[0]])\n    print('Categoty : ', news_articles['category'][indices[0]])\n    print('Authors : ', news_articles['authors'][indices[0]])\n    print('Day and month : ', news_articles['day and month'][indices[0]])\n    print(\"\\n\",\"=\"*25,\"Recommended articles : \",\"=\"*23)\n    #return df.iloc[1:,[1,7,8,9]]\n    return df.iloc[1:, ]\n\n\navg_w2v_with_category_authors_and_publshing_day(528,10,0.1,0.1,0.1,1)","36025217":"### 3.b Removing all the short headline articles ","e9c38ebb":"Above function takes one extra weight argument **w3** for **author**.\n\nIn the ouput, we can observe that the recommended articles are from the same **author** as the queried article **author** due to high weightage to **w3**.","617cdd1e":"Let's first group the data on monthly basis using **resample()** function. ","db866b5e":"From the bar chart, we can observe that **politics** category has **highest** number of articles then **entertainment** and so on.  ","0cff4556":"### 6.a Using Bag of Words method","0432e9f4":"### 6.b Using TF-IDF method","2160bde3":"### 6.f Weighted similarity based on headline, category, author and publishing day ","129c9427":"### 6.e Weighted similarity based on headline, category and author","1288ff72":"Compared to **BoW** method, here **TF-IDF** method recommends the articles with headline containing words like \"employer\", \"fire\", \"flip\" in top 5 recommendations and these words occur less frequently in the corpus.   ","21e74b56":"Above function takes two extra arguments **w1** and **w2** for weights corresponding to **headline** and **category**. It is always a good practice to pass the **weights** in a range scaled from **0 to 1**, where a value close to 1 indicates high weight whereas close to 0 indicates less weight.  \n\nHere, we can observe that the recommended articles are from the same **category** as the queried article **category**. This is due to passing of high value to **w2**.","b108ac38":"### 6.d Weighted similarity based on headline and category","2523a746":"Now, the number of news articles comes down to 8583.","c071448d":"## 4. Basic Data Exploration ","04a45ea7":"### 4.d PDF for the length of headlines ","b373ef50":"Above function takes one extra weight argument **w4** for **day of the week and month**.\n\nIn the ouput, we can observe that the recommended articles are from the same **day of the week and month** as the queried article due to high weightage to **w4**.","b863a7c9":"**Word2Vec** is one of the techniques for **semantic** similarity which was invented by **Google** in 2013. For a given corpus, during training it observes the patterns and respresents each word by a **d-dimensional** vector. To get better results we need fairly large corpus.\n\nSince our corpus size is small so let's use Google's pretrained model on **google news** articles. This standard model contains vector representation for billions of words obtained by training on millions of new articles. Here, each word is represented by a **300** dimensional dense vector. \n\n\n","7549b532":"From the bar chart, we can observe that **January** month has **highest** number of articles then **March** and so on.  ","e932a07f":"### 4.c Number of articles per month","d4b31eae":"Generally, we assess **similarity** based on **distance**. If the **distance** is minimum then high **similarity** and if it is maximum then low **similarity**.\nTo calculate the **distance**, we need to represent the headline as a **d-dimensional** vector. Then we can find out the **similarity** based on the **distance** between vectors.\n\nThere are multiple methods to represent a **text** as **d-dimensional** vector like **Bag of words**, **TF-IDF method**, **Word2Vec embedding** etc. Each method has its own advantages and disadvantages. \n\nLet's see the feature representation of headline through all the methods one by one.","c8f74367":"Since after text preprocessing the original headlines will be modified and it doesn't make sense to recommend articles by displaying modified headlines so let's copy the dataset into some other dataset and perform text preprocessing on the later.","3d3cc24c":"## 2. Loading Data","c8d20b74":"Since some articles are exactly same in headlines, so let's remove all such articles having duplicate headline appearance.","44507e2e":"Here, **Word2Vec** based representation recommends the headlines containing the word **white house** which is associated with the word **trump** in the queried article. Similarly, it recommends the headlines with words like \"offical\", \"insist\" which have semantic similarity to the words \"employer\", \"sue\" in the queried headline.","fbeb9631":"## Preface","a2a50844":"**TF-IDF** method is a weighted measure which gives more importance to less frequent words in a corpus. It assigns a weight to each term(word) in a document based on **Term frequency(TF)** and **inverse document frequency(IDF)**.\n\n**TF(i,j)** = (# times word i appears in document j) \/ (# words in document j)\n\n**IDF(i,D)** = log_e(#documents in the corpus D) \/ (#documents containing word i)\n\nweight(i,j) = **TF(i,j)** x **IDF(i,D)**\n\nSo if a word occurs more number of times in a document but less number of times in all other documents then its **TF-IDF** value will be high.\n","89eabbec":"## 3. Data Preprocessing","656184c9":"## 5. Text Preprocessing","8aedf6e2":"## Notebook - Table of Content","d62d33f2":"Since the dataset size is quite large so processing through entire dataset may consume too much time. To refrain from this, we are only considering the latest articles from the year 2018. ","64bc9bf6":"1. [**Importing necessary Libraries**](# 1.-Importing-necessary-Libraries)   \n2. [**Loading Data**](#2.-Loading-Data)  \n3. [**Data Preprocessing**](#3.-Data-Preprocessing)  \n    3.a [**Fetching only the articles from 2018**](#3.a-Fetching-only-the-articles-from-2018)  \n    3.b [**Removing all the short headline articles**](#3.b-Removing-all-the-short-headline-articles)  \n    3.c [**Checking and removing all the duplicates**](#3.c-Checking-and-removing-all-the-duplicates)  \n    3.d [**Checking for missing values**](#3.d-Checking-for-missing-values)  \n4. [**Basic Data Exploration**](#4.-Basic-Data-Exploration)  \n    4.a [**Basic statistics - Number of articles,authors,categories**](#4.a-Basic-statistics---Number-of-articles,authors,categories)  \n    4.b [**Distribution of articles category-wise**](#4.b-Distribution-of-articles-category-wise)  \n    4.c [**Number of articles per month**](#4.c-Number-of-articles-per-month)   \n    4.d [**PDF for length of headlines**](#4.d-PDF-for-length-of-headlines)\n5. [**Text Preprocessing**](#5.-Text-Preprocessing)  \n    5.a [**Stopwords removal**](#5.a-Stopwords-removal)  \n    5.b [**Lemmatization**](#5.b-Lemmatization)  \n6. [**Headline based similarity on new articles**](#6.-Headline-based-similarity-on-new-articles)  \n    6.a [**Using Bag of Words method**](#6.a-Using-Bag-of-Words method)  \n    6.b [**Using TF-IDF method**](#6.b-Using-TF-IDF-method)  \n    6.c [**Using Word2Vec embedding**](#6.c-Using-Word2Vec-embedding)   \n    6.d [**Weighted similarity based on headline and category**](#6.d-Weighted-similarity-based-on-headline-and-category)  \n    6.e [**Weighted similarity based on headline, category and author**](#6.e-Weighted-similarity-based-on-headline,-category-and-author)  \n    6.f [**Weighted similarity based on headline, category, author and publishing day**](#6.f-Weighted-similarity-based-on-headline,-category,-author-and-publishing-day)  \n ","6fb04f8e":"By Data processing in Step 2, we get a subset of original dataset which has different index labels so let's make the indices uniform ranging from 0 to total number of articles. ","668c2d6c":"## 1. Importing necessary Libraries","1f8c5721":"**Disadvantages :- **\n\n**Bow** and **TF-IDF** method do not capture **semantic** and **syntactic** similarity of a given word with other words but this can be captured using **Word embeddings**.\n\nFor example: there is a good association between words like \"trump\" and \"white house\", \"office and employee\", \"tiger\" and \"leopard\", \"USA\" and \"Washington D.C\" etc. Such kind of **semantic** similarity can be captured using **word embedding** techniques.\n**Word embedding** techniques like **Word2Vec**, **GloVe** and **fastText** leverage semantic similarity between words. ","b24d59e7":"Above function recommends **10 similar** articles to the **queried**(read) article based on the headline. It accepts two arguments - index of already read artile and the total number of articles to be recommended.\n\nBased on the **Euclidean distance** it finds out 10 nearest neighbors and recommends. \n\n**Disadvantages**\n1. It gives very low **importance** to less frequently observed words in the corpus. Few words from the queried article like \"employer\", \"flip\", \"fire\" appear less frequently in the entire corpus so **BoW** method does not recommend any article whose headline contains these words. Since **trump** is commonly observed word in the corpus so it is recommending the articles with headline containing \"trump\".   \n2. **BoW** method doesn't preserve the order of words.\n\nTo overcome the first disadvantage we use **TF-IDF** method for feature representation. \n","b9ffeeaa":"**Additional NOTE**\n\nIf you are interested in learning or exploring more about importance of feature selection in machine learning, then refer to my below blog offering.\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2020\/10\/a-comprehensive-guide-to-feature-selection-using-wrapper-methods-in-python\/","7d136ded":"A **Bag of Words(BoW)** method represents the occurence of words within a **document**. Here, each headline can be considered as a **document** and set of all headlines form a **corpus**.\n\nUsing **BoW** approach, each **document** is represented by a **d-dimensional** vector, where **d** is total number of **unique words** in the corpus. The set of such unique words forms the **Vocabulary**.","037d1e1f":"### 4.b Distribution of articles category-wise","fa136d82":"### 3.c Checking and removing all the duplicates","a4ca8ffb":"Let's find the base form(lemma) of words to consider different inflections of a word same as lemma.","f41e2c38":"Due to easy availibilty of enormous items(services) on your favourite online platforms like *e-commerce*, *job portal*, *food delivery*, *music or video streaming*, it is very hard and time consuming to find out the desired item of your choice quickly. These platforms could help you by **recommending** items as per your interest and preference by just analyzing your past interaction or behaviour with the system.  \n\nFrom **Amazon** to **Linkedin**, **Uber eats** to **Spotify**, **Netflix** to **Facebook**, **Recommender systems** are most extensively used to suggest \"Similar items\", \"Relevant jobs\", \"preferred foods\", \"Movies of interest\" etc to their users. \n\n**Recommender system** with appropiate item suggestions helps in boosting sales, increasing revenue, retaining customers and also adds competitive advantage. \nThere are basically two kind of **recommendation** methods.\n1. **Content based recommendation**\n2. **Collaborative filtering**","b1a00bce":"### 3.a Fetching only the articles from 2018  ","dad1d092":"Since the model gives vector representation for each **word** but we calculate the distance between **headlines** so we need to obtain vector representation for each **headline**. One way to obtain this is by first adding vector representation of all the words available in the **headline** and then calculating the average. It is also known as **average Word2Vec** model.   \n\nBelow code cell performs the same. ","49472125":"After stop words removal from headline, the articles with very short headline may become blank headline articles. So let's remove all the articles with less words(<5) in the headline.   ","affd64e9":"The output **BoW matrix**(headline_features) is a sparse matrix.","eb9b50f4":"The probability distribution function of headline length is almost similar to a **Guassian distribution**, where most of the headlines are 58 to 80 words long in length. ","fd87c34f":"Let's first see articles similarity based on **headline** and **category**. We are using **onehot encoding** to get feature representation for **category**.\n\nSometimes as per the business requirements, we may need to give more preference to the articles from the same **category**. In such cases we can assign more weight to **category** while recommending. Higher the weight, more the significance. Similarly less weight leads to less signficance to a particular feature.\n","1edad609":"### 6.c Using Word2Vec embedding","f4db1fb4":"### 4.a Basic statistics - Number of articles,authors,categories","792c6f76":"**Content based recommendation ** is based on similarity among users\/items obtained through their **attributes**. It uses the additional information(meta data) about the **users** or **items** i.e. it relies on what kind of **content** is already available. This meta data could be **user's demograpic information** like *age*, *gender*, *job*, *location*, *skillsets* etc. Similarly for **items** it can be *item name*, *specifications*, *category*, *registration date* etc.\n\nSo the core idea is to recommend items by finding similar items\/users to the concerned **item\/user** based on their **attributes**. \n\nIn this kernel, I am going to discuss about **Content based recommendation** using **News category** dataset. The goal is to recommend **news articles** which are similar to the already read article by using attributes like article *headline*, *category*, *author* and *publishing date*.\n\nSo let's get started without any further delay.","48bfb34f":"### 5.b Lemmatization","85bb2ab8":"Since this **pre-trained Word2Vec** model is **1.5 GB** in compressed form. So it needs a high end RAM to load it in the memory after unzipping.\n\nHere, we are loading this pre-build model from a **pickle** file which contains this model in advance.","50da9127":"### 5.a Stopwords removal","eb72391e":"Stop words are not much helpful in analyis and also their inclusion consumes much time during processing so let's remove these. ","40e23bdc":"Now let's see calcualte articles similarity based on **author** along with **headline** and **category**. Again, we are encoding **author** through **onehot encoding**.","c0056d0a":"### 3.d Checking for missing values","1d57e317":"So far we were recommending using only one feature i.e. **headline** but in order to make a **robust** recommender system we need to consider **multiple** features at a time. Based on the business interest and rules, we can decide weight for each feature.\n\nLet's see different models with combinations of different features for article similarity.","1731701b":"Now let's see calcualte articles similarity based on the publishing **week day** **author** along with **headline**, **category** and **author**. Again, we are encoding this new feature through **onehot encoding**.","aafb2c26":"The dataset contains about two million records of six different features. ","d15e834d":"## 6. Headline based similarity on new articles"}}