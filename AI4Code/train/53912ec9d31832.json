{"cell_type":{"744a2811":"code","0a13788b":"code","768b2fad":"code","e9729e06":"code","ffb932fe":"code","1d2f22f8":"code","140559a8":"code","fb20dc39":"code","16c95ba9":"code","236161be":"code","f35551dc":"code","f2f24a3f":"code","76e18a9b":"code","7dd6bee9":"code","c37bbf20":"code","09498572":"code","8977b597":"markdown","b9de3fbf":"markdown","66d7046f":"markdown","428d0168":"markdown","d327bc0c":"markdown","727ec17e":"markdown"},"source":{"744a2811":"import numpy as np\nimport pandas as pd\nimport torch\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv('..\/data\/sat_gpa.csv')\n\nsat = df['SAT']\ngpa = df['GPA']\n\nplt.scatter(gpa, sat,c=\"g\", alpha=0.5, label=\"SAT vs GPA\")\nplt.xlabel(\"GPA\")\nplt.ylabel(\"SAT\")\nplt.legend(loc='upper left')\nplt.show()\n","0a13788b":"def plot_losses(losses, min_y = None, max_y = None):\n    plt.plot(losses, label=\"Loss (MSE)\")\n    plt.xlabel(\"iteration\")\n    plt.ylabel(\"MSE\")\n    if min_y is not None and max_y is not None:\n        plt.ylim((min_y, max_y))\n    plt.show()","768b2fad":"import torch.nn\n\n# convert data to tensors\nx_data = torch.tensor(df['GPA'].values.reshape(-1,1), dtype=torch.float)\ny_data = torch.tensor(df['SAT'].values.reshape(-1,1), dtype=torch.float)\n\n# build model\nmodel = torch.nn.Linear(1, 1)\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01) ","e9729e06":"losses = []\nnum_epochs = 10_000\n\nfor i, epoch in enumerate(range(num_epochs)):\n    model.train()\n    optimizer.zero_grad()\n    # Forward pass\n    y_pred = model(x_data.float())\n    # Compute Loss\n    loss = criterion(y_pred.float(), y_data.float())\n    losses.append(loss)\n    # Backward pass\n    loss.backward()\n    optimizer.step()\n    \n    \nprint(f\"Final train loss: {loss.item():.3f}\")\nplot_losses(losses, min_y=6_000, max_y=20_000)","ffb932fe":"def normalize(data):\n    return (data - data.mean()) \/ data.std()\n\ndef unnormalize(data, mean, std):\n    return data * std + mean\n\ndef print_arr_stats(data):\n    print(f\"Tensor: {data.tolist()} Shape: {data.shape}\\nMean: {data.mean().item()} Std: {data.std().item()}\")\n\n # Let's test it out.\ntest_arr = torch.tensor([1, 2, 1, 2], dtype=torch.float)\ntest_arr_norm = normalize(test_arr)\ntest_arr_unnorm = unnormalize(test_arr_norm, test_arr.mean(), test_arr.std())\n\nfor arr in [test_arr, test_arr_norm, test_arr_unnorm]:\n    print_arr_stats(arr)\n    print(\"\")","1d2f22f8":"x_data_norm = normalize(x_data)\ny_data_norm = normalize(y_data)\n\nx_data_unnorm = unnormalize(x_data_norm, x_data.mean(), x_data.std())\n\n# Print the first 3 to make sure it worked\nlist(zip(x_data, x_data_unnorm))[:3]","140559a8":"# Now let's train!\n\nlosses = []\nnum_epochs = 200\ncheck_10_x = num_epochs\/10\n\n# Reset model and optimizer by recreating or we'll be fine tuning\nmodel = torch.nn.Linear(1, 1)\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01) \n\nmodel.train()\n\nfor i, epoch in enumerate(range(num_epochs)):\n    \n    optimizer.zero_grad()\n    \n    y_pred = model(x_data_norm)\n    \n    loss = criterion(y_pred, y_data_norm)\n    \n    losses.append(loss)\n    loss.backward()\n    \n    optimizer.step()\n    \n    if i > 0 and i % check_10_x == 0:\n        print(f\"Train loss @ iter {i}: {loss.item():.3f}\")\n        \nplot_losses(losses)","fb20dc39":"def train(lr, num_epochs=200):\n    \n    losses = []\n    model = torch.nn.Linear(1, 1)\n    criterion = torch.nn.MSELoss()\n    optimizer = torch.optim.SGD(model.parameters(), lr=lr) \n    \n    model.train()\n\n    for i, epoch in enumerate(range(num_epochs)):\n        optimizer.zero_grad()   \n        y_pred = model(x_data_norm)  \n        loss = criterion(y_pred, y_data_norm)  \n        losses.append(loss.item())\n        loss.backward()\n        optimizer.step()\n\n    return losses","16c95ba9":"lrs = [1.0, 1e-1, 1e-2, 1e-3, 1e-4]\nloss_lrs = [train(lr, num_epochs=1000) for lr in lrs] \n\n# Now let's plot.\nfor lr, lr_losses in zip(lrs, loss_lrs):\n    plt.plot(lr_losses, label=f\"{lr:.0e}\")\nplt.legend()\nplt.show()","236161be":"lr_loss = list((lr, min(ll)) for (lr, ll) in zip(lrs, loss_lrs)) \nmin_loss = min(lr_loss, key=lambda x: x[1])\n\nprint(\"Losses:\")\nprint(lr_loss)\nprint(f\"\\nLowest loss: {min_loss[0]:.0e} = {min_loss[1]:.2f}\")","f35551dc":"def train(model, x_train, y_train, lr, num_epochs=200):\n    \n    losses = []\n    criterion = torch.nn.MSELoss()\n    optimizer = torch.optim.SGD(model.parameters(), lr=lr) \n    \n    model.train()\n\n    for i, epoch in enumerate(range(num_epochs)):\n        optimizer.zero_grad()\n        y_pred = model(x_train)\n        loss = criterion(y_pred, y_train)\n        losses.append(loss.item())\n        loss.backward()\n        optimizer.step()\n\n    return losses\n\ndef validate(model, x_valid, y_valid):\n    \n    losses = []\n    model.eval()\n\n    with torch.no_grad():\n        y_pred = model(x_valid)\n        loss = criterion(y_pred, y_valid)\n        losses.append(loss.item())\n\n    return losses","f2f24a3f":"train_size = int(x_data_norm.shape[0] * 0.95) # 95% train, 5% valid\n\nx_train = x_data_norm[:train_size, :]\ny_train = y_data_norm[:train_size, :]\n\nx_valid = x_data_norm[train_size:, :]\ny_valid = y_data_norm[train_size:, :]\n\nx_train.shape, y_train.shape, x_valid.shape, y_valid.shape","76e18a9b":"model = torch.nn.Linear(1, 1)\ntrain_losses = train(model, x_train, y_train, 1e-2)\nvalid_losses = validate(model, x_valid, y_valid)\n\nprint(f\"Train loss: {train_losses[-1]:.2f}\")\nprint(f\"Valid loss: {valid_losses[-1]:.2f}\")","7dd6bee9":"new_x = torch.Tensor([[1]])\ny_pred = model(new_x)\n\nplt.scatter(x_data_norm, y_data_norm, c=\"g\", alpha=0.5, label=\"SAT vs GPA\")\nplt.scatter(new_x.detach().numpy(),y_pred.detach().numpy(),  c=\"r\")\nplt.xlabel(\"GPA\")\nplt.ylabel(\"SAT\")\nplt.legend(loc='upper left')\nplt.show()\n","c37bbf20":"# different ways to print weights (results) \n\nfor name, param in model.named_parameters():\n    if param.requires_grad:\n        print (name, param.data)\n\nfor param in model.parameters():\n    print(param.data)\n\n\n# weight \nw= list(model.parameters())[0].data.numpy()[0]\n\n# bias\nb = list(model.parameters())[1].data.numpy()[0]\n\n# or \n[w,b] = model.parameters()\n","09498572":"predicted_norm = model(x_data_norm).data.numpy()\n\n# prediction of y given x\nnew_x = torch.Tensor([[1]])\nnew_y = model(new_x)\n\n# graph data points\nplt.scatter(x_data_norm, y_data_norm, c=\"g\", alpha=0.5, label=\"SAT vs GPA\")\n\n# graph regression line\nplt.plot(x_data_norm, predicted_norm, c=\"orange\", label=\"Regression\")\n\n# graph predicted new point\nplt.scatter(new_y.detach().numpy(), new_x.detach().numpy(), c=\"r\", label=\"Predicted \")\n\n\nplt.xlabel(\"SAT\")\nplt.ylabel(\"GPA\")\nplt.legend(loc='upper left')\n\nplt.show()\n","8977b597":"## Messing around with learning rates\nSo why use 0.01 (1e-2)? Is it a magic number? Let's play with other learning rates to see how they do. To help us out, we'll abstract out the train loop. ","b9de3fbf":"## Predicting","66d7046f":"Generally, they say that if your validation loss is less than your training loss, then you're underfitting and can keep training. ","428d0168":"But which learning rate actually achieved the lowest train loss? **1e-2** \n\nWhoomp there it is!","d327bc0c":"## Splitting into train and validation sets\nWe want to split our data into training and validation sets so that we can monitor if the neural network is memorizing, or overfitting, on the data. So we'll separate a set of the data out to be a validation set. We'll also add a validation method and refactor our train method to take a model and train data.","727ec17e":"## Normalizing tensors\nThe loss looks particularly large to us because the tensors are unnormalized, meaning they don't have small numerical values close to 0. So let's normalize them. Why do this and does it need to be done to the target? Discussion here: https:\/\/stats.stackexchange.com\/questions\/111467\/is-it-necessary-to-scale-the-target-value-in-addition-to-scaling-features-for-re\n\nWe'll test out normalizing and reverting the normalizations. "}}