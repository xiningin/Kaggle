{"cell_type":{"bd3d34c4":"code","c6828fd2":"code","710fa5af":"code","f41456f1":"code","e797b2b7":"code","7fce5f79":"code","8195b9cd":"code","c4ef083a":"code","9686016f":"code","e9e662fb":"code","d9a072af":"code","4274224b":"code","c1b5407b":"code","7b308691":"code","8f07d829":"code","09728ab6":"markdown","56219165":"markdown","85783289":"markdown"},"source":{"bd3d34c4":"#Now we are switching from regression problems to classification problems. \n#Don't be confused by the name \"Logistic Regression\"; it is named that way for \n#historical reasons and is actually an approach to classification problems, not regression problems.\n\n#for more info on naming convention \n# Goto: https:\/\/www.graphpad.com\/guides\/prism\/7\/curve-fitting\/index.htm?reg_the_term_logistic.htm","c6828fd2":"import os\nprint(os.listdir(\"..\/input\"))\n","710fa5af":"import pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\nimport zipfile\nimport matplotlib.pyplot as plt\n\nimport plotly\nplotly.tools.set_credentials_file(username='enter_your_details_here', api_key='8tUzNDbRW7G2wlH4js1P')\nimport plotly.plotly as py\nimport plotly.graph_objs as go","f41456f1":"df = pd.read_csv('..\/input\/week_3-ex_2.txt',header=None,names=('Exam 1 score','Exam 2 score','Status'))","e797b2b7":"print(df.head())\nprint(df.keys())\nprint(df.shape)\nm, n = df.shape","7fce5f79":"X = df.iloc[:,0:2]\ny = df.iloc[:,2]\nprint(X.head())\nprint(y.head())\nprint(type(X))\nprint(type(y))","8195b9cd":"admitted = np.where(y[:] == 1)\nnot_admitted = np.where(y[:] == 0)\n\nprint(type(admitted))\nprint(admitted)","c4ef083a":"# Create a trace\ntrace1 = go.Scatter(\n    x = X['Exam 1 score'].iloc[admitted],\n    y = X['Exam 2 score'].iloc[admitted],\n    name = 'Admitted',  \n    marker={'color': 'blue', 'symbol': 100},\n    mode = 'markers',\n)\n\ntrace2 = go.Scatter(\n    x = X['Exam 1 score'].iloc[not_admitted],\n    y = X['Exam 2 score'].iloc[not_admitted],\n    name = 'Not Admitted',   \n    marker={'color': 'red', 'symbol': 104},\n    mode = 'markers'\n   \n)\n#data = [trace1,trace2]\n\n# Plot and embed in ipython notebook!\n#py.iplot(data, filename='basic-scatter')\n\n\ndata=go.Data([trace1,trace2])\nlayout=go.Layout(title=\"Admission statistics\", xaxis={'title':'Exam 1 score'}, yaxis={'title':'Exam 2 score'})\nfigure=go.Figure(data=data,layout=layout)\npy.iplot(figure, filename='pyguide_1')","9686016f":"def sigmoid(z):\n    return 1\/(1+np.exp(-z))\n\n# np.exp is already vectorized function\n# for more info \n# Goto: https:\/\/stackoverflow.com\/questions\/42594695\/how-to-apply-a-function-map-values-of-each-element-in-a-2d-numpy-array-matrix?rq=1","e9e662fb":"X = np.column_stack((X['Exam 1 score'],X['Exam 2 score']))\nX = np.hstack((np.full([m,1],1), X))\ny = y[:,np.newaxis]\n# Goto: https:\/\/stackoverflow.com\/questions\/29241056\/how-does-numpy-newaxis-work-and-when-to-use-it\nprint(type(X))\nprint(X.shape)\n\nprint(type(y))\nprint(y.shape)\n\ntheta = np.full([(n-1)+1, 1],0)\nprint(theta.shape)\ntheta = np.array([[-24], [0.2], [0.2]])","d9a072af":"def computeCost(X, y, theta):    \n    h = sigmoid(np.dot(X, theta))    \n    return (-1)*(np.dot(y.T,np.log(h)) + np.dot(1-y.T,np.log(1-h)))\/m","4274224b":"# initial cost\ncomputeCost(X,y,theta)","c1b5407b":"def gradientDescent(theta, X, y, alpha, iterations):\n    \n    # theta_vals, J_vals stores the intermediate values of theta and J during optimiztion usng gradient descent\n    theta_vals=np.full([1,n],0)\n    J_vals = np.full([1],computeCost(X,y,theta))\n    \n    for _ in range(iterations):        \n        theta = theta - (alpha\/m)*(np.dot(X.T, (sigmoid(np.dot(X, theta)) - y)))\n        theta_vals = np.vstack((theta_vals,theta.T))\n        J_vals = np.vstack((J_vals,computeCost(X, y, theta)))\n    return [theta_vals,J_vals]\n\n(theta_vals,J_vals) = gradientDescent(theta, X, y, 0.1,400000)\ntheta = np.reshape(theta_vals[-1],[n,1])\nprint(\"Theta: \",theta)\nprint(computeCost(X, y, theta))","7b308691":"# print the decision boundary\npositives = np.where(y[:,0] == 1);\nnegatives = np.where(y[:,0] == 0);\nplt.scatter(X[positives,1], X[positives,2], marker='+', label='Admitted')\nplt.scatter(X[negatives,1], X[negatives,2], marker='o', label='Not admitted')\npx = np.array([np.min(X[:,1])-2, np.max(X[:,2])+2])\npy = (-1 \/ theta[2]) * (theta[1]*px + theta[0])\nplt.plot(px, py)\nplt.xlabel('Exam 1 score')\nplt.ylabel('Exam 2 score')\nplt.legend()\nplt.show()","8f07d829":"a=np.array([1,45,85])\nb=sigmoid(np.dot(a,theta))\nb","09728ab6":"## ML:Logistic Regression","56219165":"### Cost function","85783289":"### Gradient Descent"}}