{"cell_type":{"740c256e":"code","808e9b3a":"code","ea38c7be":"code","cc210f39":"code","994786cc":"code","dc27eb5e":"code","47bbe747":"code","1f7b98f7":"code","10668ca3":"code","9582181d":"code","b98f055c":"code","129357c7":"code","fde22e6a":"code","ee5f3377":"code","bbaee643":"code","694518e5":"code","634ca96f":"code","06a9c0c5":"code","c2482138":"code","38faa7e3":"code","1becd68e":"code","79d0796b":"code","6dbef850":"code","e8d090a4":"code","5eb3ac6a":"markdown","0ee9f9cc":"markdown","b1b7f673":"markdown","62703bbe":"markdown","686bd5e6":"markdown","5aec73ec":"markdown","7d0ac561":"markdown","3b00cb47":"markdown","f1d6f83f":"markdown","0c0b6090":"markdown","7044eae7":"markdown","e1ba1b5a":"markdown","7c8f2680":"markdown","dd8af156":"markdown","525198d3":"markdown","65b1503d":"markdown","46dd8d9a":"markdown","f81412e8":"markdown","c098d491":"markdown","6d460f7c":"markdown","bdf45565":"markdown","a1399ba8":"markdown","a1d83782":"markdown","8cc62b51":"markdown","f79be592":"markdown","6655889d":"markdown","881e02e8":"markdown","2a584873":"markdown","b0798b73":"markdown","16baae62":"markdown","d46675fe":"markdown","890f0b57":"markdown","3ebb00fb":"markdown","af1eec6a":"markdown","6fededa1":"markdown"},"source":{"740c256e":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn import model_selection\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n","808e9b3a":"# path to train dataset\ntrain_path = '..\/input\/titanic\/train.csv'\n# path to test dataset\ntest_path = '..\/input\/titanic\/test.csv'\n\n# Read a comma-separated values (csv) file into pandas DataFrame\ntrain_data = pd.read_csv(train_path)\ntest_data = pd.read_csv(test_path)\n\n# shape of tha data\nprint('Train shape: ', train_data.shape)\nprint('Test shape: ', test_data.shape)","ea38c7be":"# Passengers with wrong number of siblings and parch\ntrain_data.loc[train_data['PassengerId'] == 69, ['SibSp', 'Parch']] = [0,0]\ntest_data.loc[test_data['PassengerId'] == 1106, ['SibSp', 'Parch']] = [0,0]\n\n# Age outlier \ntrain_data.loc[train_data['PassengerId'] == 631, 'Age'] = 48","cc210f39":"# check data for NA values\ntrain_NA = train_data.isna().sum()\ntest_NA = test_data.isna().sum()\npd.concat([train_NA, test_NA], axis=1, sort = False, keys = ['Train NA', 'Test NA'])","994786cc":"plt.figure(figsize = (16, 7))\n\nplt.subplot(1,2,1)\nsns.heatmap(train_data.isnull(), cbar=False)\nplt.xticks(rotation = 35,     horizontalalignment='right',\n    fontweight='light'  )\nplt.title('Training dataset missing values')\n\nplt.subplot(1,2,2)\nsns.heatmap(test_data.isnull(), cbar=False)\nplt.xticks(rotation=35,     horizontalalignment='right',\n    fontweight='light'  )\nplt.title('Test dataset missing values')\n\nplt.tight_layout()","dc27eb5e":"# Add new variable Age_NA indicates that there is no age in the original data.\ntrain_data.loc[train_data['Age'].isna(), 'Age_NA'] = 1     # 1 for missing Age value\ntrain_data.loc[train_data['Age_NA'].isna(), 'Age_NA'] = 0  # 0 if Age value is not null\ntest_data.loc[test_data['Age'].isna(), 'Age_NA'] = 1       \ntest_data.loc[test_data['Age_NA'].isna(), 'Age_NA'] = 0\n\n# titles categories dict\ntitle_dict = {  'Mr':     'Mr',\n                'Mrs':    'Mrs',\n                'Miss':   'Miss',\n                'Master': 'Master',              \n                'Ms':     'Miss',\n                'Mme':    'Mrs',\n                'Mlle':   'Miss',\n                'Capt':   'military',\n                'Col':    'military',\n                'Major':  'military',\n                'Dr':     'Dr',\n                'Rev':    'Rev',                  \n                'Sir':    'honor',\n                'the Countess': 'honor',\n                'Lady':   'honor',\n                'Jonkheer': 'honor',\n                'Don':    'honor',\n                'Dona':   'honor' }\n\n# add title variable\ntrain_data['Title'] = train_data['Name'].str.split(',', expand = True)[1].str.split('.', expand = True)[0].str.strip(' ')\ntest_data['Title'] = test_data['Name'].str.split(',', expand = True)[1].str.split('.', expand = True)[0].str.strip(' ')\n\n# map titles to category\ntrain_data['Title_category'] = train_data['Title'].map(title_dict)\ntest_data['Title_category'] = test_data['Title'].map(title_dict)\n\n# delete Title variable\ndel train_data['Title']\ndel test_data['Title']\n\n# Filling the missing values in Age with the medians of Sex and Pclass, Title groups\ntrain_data['Age'] = train_data.groupby(['Pclass', 'Sex', 'Title_category'])['Age'].apply(lambda x: x.fillna(x.median()))\ntest_data['Age'] = test_data.groupby(['Pclass', 'Sex', 'Title_category'])['Age'].apply(lambda x: x.fillna(x.median()))","47bbe747":"train_data[train_data['Embarked'].isna()]","1f7b98f7":"mode_emb = train_data[(train_data['Fare'] > 77) & (train_data['Fare'] < 82)& (train_data['Pclass']==1)]['Embarked'].mode()\ntrain_data.loc[train_data['Embarked'].isna(), 'Embarked'] = mode_emb[0]","10668ca3":"# Filling the missing values in Age with the medians of Sex and Pclass, Title groups\ntest_data['Fare'] = test_data.groupby(['Pclass', 'Sex', 'Title_category', 'Parch'])['Fare'].apply(lambda x: x.fillna(x.median()))","9582181d":"def feature_generator (data, train = False):\n    \n    features_data = data\n    \n    # Deck\n    # Extract deck letter from cabin number\n    features_data['deck'] = features_data['Cabin'].str.split('', expand = True)[1]\n    # If cabin is NA - deck = U\n    features_data.loc[features_data['deck'].isna(), 'deck'] = 'U'\n    # If cabin is T - change to A (see EDA)\n    features_data.loc[features_data['deck'] == 'T', 'deck'] = 'A'\n    # Create dummy variables with prefix 'deck'\n    features_data = pd.concat([features_data,\n                               pd.get_dummies(features_data['deck'], prefix = 'deck')], \n                               axis=1)\n    \n    \n    # titles dummy\n    features_data = pd.concat([features_data, \n                               pd.get_dummies(features_data['Title_category'],\n                                              prefix = 'title')], axis=1)\n\n    # family size\n    features_data['Family_size'] = features_data['SibSp'] + features_data['Parch'] + 1\n    features_data['Family_size_group'] = features_data['Family_size'].map(\n                                            lambda x: 'f_single' if x == 1 \n                                                    else ('f_usual' if 5 > x >= 2 \n                                                          else ('f_big' if 8 > x >= 5 \n                                                               else 'f_large' )))\n    features_data = pd.concat([features_data, \n                               pd.get_dummies(features_data['Family_size_group'], \n                                              prefix = 'family')], axis=1)     \n    \n    \n    # Sex to number\n    features_data['Sex'] = features_data['Sex'].map({'female': 1, 'male': 0}).astype(int)\n    \n    # embarked dummy\n    features_data = pd.concat([features_data, \n                               pd.get_dummies(features_data['Embarked'], \n                                              prefix = 'embarked')], axis=1)\n    \n    # zero fare feature\n    features_data['zero_fare'] = features_data['Fare'].map(lambda x: 1 if x == 0 else (0))\n    \n    # from numeric to categorical\n    features_data['SibSp'] = features_data['SibSp'].map(lambda x: 1 if x > 0 else (0))\n    features_data['Parch'] = features_data['Parch'].map(lambda x: 1 if x > 0 else (0))\n    \n    # delete variables we are not going to use anymore\n    del features_data['PassengerId']\n    del features_data['Ticket']\n    del features_data['Cabin']\n    del features_data['deck']    \n    del features_data['Title_category']\n    del features_data['Name']\n    del features_data['Family_size']\n    del features_data['Family_size_group'] \n    del features_data['Embarked']    \n    \n    return features_data     ","b98f055c":"# Extract target variable (label) from training dataset\nall_train_label = train_data['Survived']\ndel train_data['Survived']\n\n# Generate features from training dataset\nall_train_features = feature_generator(train_data)\n# Generate features from test dataset\nall_test_features = feature_generator(test_data)","129357c7":"plt.figure(figsize=(12,10))\ncor = all_train_features.corr()\nsns.heatmap(cor)","fde22e6a":"# set model. max_iter - Maximum number of iterations taken for the solvers to converge.\nlg_model = LogisticRegression(random_state = 64, max_iter = 1000)\n\n# set parameters values we are going to check\noptimization_dict = {'class_weight':['balanced', None],\n                     'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n                     'C': [0.01, 0.05, 0.07, 0.1, 0.5, 1, 2, 4, 5, 10, 15, 20]\n                     }\n# set GridSearchCV parameters\nmodel = GridSearchCV(lg_model, optimization_dict, \n                     scoring='accuracy', n_jobs = -1, cv = 10)\n\n# use training features\nmodel.fit(all_train_features, all_train_label)\n\n# print result\nprint(model.best_score_)\nprint(model.best_params_)","ee5f3377":"# set best parameters to the model\nlg_tuned_model =  LogisticRegression(solver = 'newton-cg',\n                                     C = 0.5,\n                                     random_state = 64,\n                                     n_jobs = -1)","bbaee643":"# train our model with training data\nlg_tuned_model.fit(all_train_features, all_train_label)\n\n# calculate importances based on coefficients.\nimportances = abs(lg_tuned_model.coef_[0])\nimportances = 100.0 * (importances \/ importances.max())\n# sort \nindices = np.argsort(importances)[::-1]\n\n# Rearrange feature names so they match the sorted feature importances\nnames = [all_train_features.columns[i] for i in indices]\n\n# visualize\nplt.figure(figsize = (12, 5))\nsns.set_style(\"whitegrid\")\nchart = sns.barplot(x = names, y = importances[indices])\nplt.xticks(\n    rotation=45, \n    horizontalalignment='right',\n    fontweight='light'  \n)\nplt.title('Logistic regression. Feature importance')\nplt.tight_layout()","694518e5":"# set model\nrf_model = RandomForestClassifier(oob_score = True, n_jobs = -1, random_state = 64)\n# create a dictionary of parameters values we want to try\noptimization_dict = {'criterion':['gini', 'entropy'],\n                     'n_estimators': [100, 500, 1000, 1700],\n                     'max_depth': [7, 10, 11, 12],\n                     'min_samples_split': [6, 7, 8, 10],\n                     'min_samples_leaf': [3, 4, 5]\n                     }\n\n# set GridSearchCV parameters\nmodel = GridSearchCV(rf_model, optimization_dict, \n                     scoring='accuracy', verbose = 1, n_jobs = -1, cv = 5)\n\n# use training data\nmodel.fit(all_train_features, all_train_label)\n\n# print best score and best parameters combination\nprint(model.best_score_)\nprint(model.best_params_)","634ca96f":"# set best parameters to the model\nrf_tuned_model =  RandomForestClassifier(criterion = 'gini',\n                                       n_estimators = 100,\n                                       max_depth = 12,\n                                       min_samples_split = 6,\n                                       min_samples_leaf = 4,\n                                       max_features = 'auto',\n                                       oob_score = True,\n                                       random_state = 64,\n                                       n_jobs = -1)","06a9c0c5":"# train model using training dataset\nrf_tuned_model.fit(all_train_features, all_train_label)\n\n# Calculate feature importances\nimportances = rf_tuned_model.feature_importances_\n\n# Visualize Feature Importance\n# Sort feature importances in descending order\nindices = np.argsort(importances)[::-1]\n\n# Rearrange feature names so they match the sorted feature importances\nnames = [all_train_features.columns[i] for i in indices]\n\nplt.figure(figsize = (12, 5))\nsns.set_style(\"whitegrid\")\nchart = sns.barplot(x = names, y=importances[indices])\nplt.xticks(\n    rotation=45, \n    horizontalalignment='right',\n    fontweight='light'  \n)\nplt.title('Random forest. Feature importance')\nplt.tight_layout()","c2482138":"# set model\nxgb_model = XGBClassifier(random_state = 64)\n# create a dictionary of parameters values we want to try\noptimization_dict = {'n_estimators': [200, 1000, 1700, 2000],\n                     'max_depth': [4, 6, 8, 10],\n                     'learning_rate': [0.001, 0.01, 0.1, 0.5],\n                     'gamma': [0, 1, 5],\n                     'min_child_weight':[3, 6, 10],\n                     'subsample': [0.5, 0.8, 0.9]\n                     }\n# set GridSearchCV parameters\nmodel = GridSearchCV(xgb_model, optimization_dict, \n                     scoring='accuracy', verbose = 1, n_jobs = -1, cv = 5)\n\n# use training data\nmodel.fit(all_train_features, all_train_label)\nprint(model.best_score_)\nprint(model.best_params_)","38faa7e3":"# set model with best parameters\nxgb_tuned_model =  XGBClassifier(n_estimators = 200,\n                               max_depth = 8,\n                               learning_rate = 0.5,\n                               gamma = 1,\n                               min_child_weight = 6,\n                               subsample = 0.9,\n                               random_state = 64)","1becd68e":"# train model with training dataset\nxgb_tuned_model.fit(all_train_features, all_train_label)\n\n# Calculate feature importances\nimportances = xgb_tuned_model.feature_importances_\n\n# Visualize Feature Importance\n# Sort feature importances in descending order\nindices = np.argsort(importances)[::-1]\n\n# Rearrange feature names so they match the sorted feature importances\nnames = [all_train_features.columns[i] for i in indices]\n\nplt.figure(figsize = (12, 5))\nsns.set_style(\"whitegrid\")\nchart = sns.barplot(x = names, y=importances[indices])\nplt.xticks(rotation=45, horizontalalignment='right', fontweight='light')\nplt.title('XGBoost. Feature importance')\nplt.tight_layout()","79d0796b":"models = []\n# add our tuned models into list\nmodels.append(('Logistic Regression', lg_tuned_model))\nmodels.append(('Random Forest', rf_tuned_model))\nmodels.append(('XGBoost', xgb_tuned_model))\n\nresults = []\nnames = []\n\n# evaluate each model in turn\nfor name, model in models:\n    kfold = model_selection.KFold(n_splits=10, shuffle = True, random_state = 64)\n    cv_results = model_selection.cross_val_score(model, all_train_features, \n                                                 all_train_label, \n                                                 cv = 10, scoring = 'accuracy')\n    results.append(cv_results)\n    names.append(name)\n    # print mean accuracy and standard deviation\n    print(\"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std()))\n","6dbef850":"fig = plt.figure(figsize=(6,4))\nplt.boxplot(results)\nplt.title('Algorithm Comparison')\nplt.xticks([1,2,3], names)\nplt.show()","e8d090a4":"# train chosen model on training dataset\nrf_tuned_model.fit(all_train_features, all_train_label)\n\n# get predictions on test dataset\npredictions = rf_tuned_model.predict(all_test_features)\n\n# Save results in the required format\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId,\n                       'Survived': predictions})\noutput.to_csv('submission_xgb.csv', index=False)\noutput.head()","5eb3ac6a":"* title Mr. is the most important feature to this model, similar to previous one\n* Sex  - takes only fouth position\n* Unknow deck (unknown cabin number) in the top-features","0ee9f9cc":"Now, we can go back to [EDA](https:\/\/www.kaggle.com\/demidova\/titanic-eda-tutorial-with-seaborn) part and compare our assumptions with this graph! As we expected, Sex is very important parameter to make prediction of survival. As well as titles Master an Mr, family size and class of passenger. \nThe least important feature for Logistic regression is Fare, probably because it is correlated with features above.","b1b7f673":"#### 6.1.3.3 Feature importance","62703bbe":"# 5. Feature generation\nFrom 11 variables I will generate **31 features** for each passenger.  \nWe need to transform categorical variables into numbers, because most models only accept numbers as input. I will use following ways to do it:\n1. **Dummy variables** -a numerical variable used to represent subgroups of the sample in your study. Takes only the value 0 or 1 to indicate the absence or presence of some feature. This method are very suitable for nominal categorical variables, which has no intrinsic ordering to its categories.\n![dummy.png](attachment:dummy.png)\n2. **Replace string values by numbers** - suitable for ordinal variables (such as passenger ticket class) or if we have only two possible values (In this dataset we can implement it to Sex, for example).\n\nFeatures:\n* **Deck** (8 features) - From the number of the cabin we can extract first letter, which will tell us about placement of the cabin on the ship (Deck). If there is no Cain number, the value will be 'U' - unknown. \nUsing get_dummies function, I convert it to 8 dummy features with prefix 'deck'.\n* **Title_category** (8 features) - We already created 'Title_category' variable in missing imputation part.\nUsing get_dummies function, I convert it to 8 dummy features with prefix 'title'.\n* **Family_size_group** (4 features) - I calculate Family_size by summarizing SibSp and Parch variables and adding 1. Then I create Family_size_group (4 possible values) based on family size. \nUsing get_dummies function, I convert it to 8 dummy features with prefix 'family'.\n* **Age_NA** (1 feature) -  We created this variable during missing imputation process.\n* **Zero_fare** (1 feature) - Mark people with zero fare, since they can be related to the White Star Line.\n* **Embarked** (3 features) - Using get_dummies function, I convert 'Embarked' variable to 3 dummy features with prefix 'embarked'.\n* **Sex**  (1 feature) - since we have only 2 possible values, I conver it to binary feature - 1 if female, 0 if male.\n* **Age** (1 feature) - already numeric (continuous).\n* **Fare** (1 feature) - already numeric (continuous).\n* **Pclass** (1 feature) - already numeric (ordinal categorical variable).\n* **SibSp** (1 feature) -  since we already use size of family, I conver SibSp to binary feature - 1 if there are siblings \/ spouses aboard the Titanic, 0 if not.\n* **Parch** (1 feature) -  since we already use size of family, I conver Parch to binary feature - 1 if there are parents \/ children aboard the Titanic, 0 if not.\n\n\n","686bd5e6":"# 1. Introduction\n\nThis notebook is the **second part** of my work with Titanic dataset and contains:\n* Missing data imputation\n* Feature generation\n* Models implementation and tuning: Logistic Regression, Random Forest, XGBoost\n* Comparing models and submission\n\n!!! The [First part of my work - **Titanic EDA tutorial with seaborn**](https:\/\/www.kaggle.com\/demidova\/titanic-eda-tutorial-with-seaborn) contains EDA (exploratory data analysis) and missing data research. Check it first ;)\n\n*If you have any suggestions, questions about this notebook - welcome to comments! *","5aec73ec":"# 2. Load libraries\nList of libraries I am using in this notebook:\n\n* **pandas** - offers data structures and operations for manipulating numerical tables and time series. (imported as pd)\n* **seaborn** - data visualization library based on matplotlib.\n* **matplotlib.pyplot** - to create some visualizations. (imported as plt)\n* **numpy** - The fundamental package for scientific computing with Python. (imported as np)\n* **XGBClassifier** from xgboost.\n\nFunctions and modules from **Scikit-learn** -  a free software machine learning library:\n* **model_selection** - contains functions to implement Cross Validation, Tuning models, Metrics. [Documentation]( https:\/\/scikit-learn.org\/stable\/model_selection.html)\n* **GridSearchCV** - function from model_selection, import separatrely, because I will use it many times. \n* **LogisticRegression** from linear_model module. \n* **RandomForestClassifier** from ensamle module. \n\nI'll tell you more about most of the features later in the notebook.","7d0ac561":"![titanic-A-1200x615.jpg](attachment:titanic-A-1200x615.jpg)","3b00cb47":"## 5.2 Generation\nGenerte features for training and test datasets:","f1d6f83f":"### 4.2.2 Embarked\nThere are two missing Embarked values. Both of the passengers are with ticket number = 113572 (and same cabin number), so they traveled together. I will imput values with mode value for same class passengers in similar fare group (77-82), because Fare should depend on class and duration of the trip (which depends on the place of boarding).","0c0b6090":"I would say it's hard to choose :)","7044eae7":"# 4. Handling missing values\/ outliers\/ mistakes\nTo work with missing values, outliers, and errors, you must conduct research on the data. I've been researching the dataset [**here**](https:\/\/www.kaggle.com\/demidova\/titanic-eda-tutorial-with-seaborn) and based on the insights I've received, I'm just applying the appropriate techniques in current notebook.\n\n## 4.1. Mistakes, outliers\nFrom my EDA we discovered some mistakes in data:\n* Two passengers with ids 69 and 1106 have wrong number of SibSp and Parch.\n* Passenger 631 has wrong age.","e1ba1b5a":"## 6.1 Explore and tune models","7c8f2680":"### 6.1.3 Random forest","dd8af156":"### 6.1.1 How to tune\nFor tuning models I will use [GridSearchCV](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html ) from sklearn. Grid search is the process of performing hyper parameter tuning in order to determine the optimal values for a given model.   \nGridSearchCV checks all combinations of the proposed parameters and returns the best configuration based on scoring parameter.  \nGridSearch**CV** uses cross validation technique. Depending on cv parameter value, it splits data on n folders and in each iteration uses one folder as a validation data (unseen data), switching folders. This allowes to use all the data to test the model, and evaluate the model on different data combinations to find the average value.\n![grid_search_cross_validation.png](attachment:grid_search_cross_validation.png)\n\nGridSearchCV parameters:\n* **estimator**: estimator object you created\n* **params_grid**: the dictionary object that holds the hyperparameters you want to try\n* **scoring**: evaluation metric that you want to use, you can simply pass a valid string\/ object of evaluation metric\n* **cv**: number of cross-validation you have to try for each selected set of hyperparameters\n* **verbose**: you can set it to 1 to get the detailed print out while you fit the data to GridSearchCV\n* **n_jobs**: number of processes you wish to run in parallel for this task if it -1 it will use all available processors.","525198d3":"## 4.2. Missing values\nMissing values can be caused by different reasons, there are three main [mechanisms](https:\/\/www.theanalysisfactor.com\/missing-data-mechanism\/):  \n* **Missing Completely at Random** (MCAR) - there is no relationship between the missingness of the data and any values, observed or missing.\n* **Missing at Random** (MAR) - there is a systematic relationship between the propensity of missing values and the observed data, but not the missing data.\n* **Missing Not at Random**(MNAR) - there is a relationship between the propensity of a value to be missing and its values. \n\nDepending on reasons, why values are missing, we should act differently with data imputation. It's not easy to say for sure, what type of missing values we have, but we can do some assumptions.\n\nFor example, based on the fact that among the passengers without the specified cabin, the percentage of drowned is higher, you might think that the presence of this information tells us something (MAR). Cabin information is missing from a very large number of passengers (687 NA in training dataset and 327 in test dataset). I read about the [assumptions](https:\/\/www.encyclopedia-titanica.org\/cabins.html) that this information was collected from the words of survived passengers. If this is the case, then the presence or absence of data is itself important information.  \nTo preserve a possible insight, we can add a separate flag that the passenger cabin is unknown. I use the cabin to determine the passenger's deck, for those who do not have the information, I will add the 'U' flag. I don't plan to use the cabin itself.  \n\nAge can be a similar situation (there are bigger percent of passengers who survived among passengers with not NA Age), so I will add a flag that age is not specified ('Age_NA') before inserting the assumed values into the age variable.  \n\nThere are table of missing values:  ","65b1503d":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1.-Introduction\" data-toc-modified-id=\"1.-Introduction-1\">1. Introduction<\/a><\/span><\/li><li><span><a href=\"#2.-Load-libraries\" data-toc-modified-id=\"2.-Load-libraries-2\">2. Load libraries<\/a><\/span><\/li><li><span><a href=\"#3.-Load-data\" data-toc-modified-id=\"3.-Load-data-3\">3. Load data<\/a><\/span><\/li><li><span><a href=\"#4.-Handling-missing-values\/-outliers\/-mistakes\" data-toc-modified-id=\"4.-Handling-missing-values\/-outliers\/-mistakes-4\">4. Handling missing values\/ outliers\/ mistakes<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#4.1.-Mistakes,-outliers\" data-toc-modified-id=\"4.1.-Mistakes,-outliers-4.1\">4.1. Mistakes, outliers<\/a><\/span><\/li><li><span><a href=\"#4.2.-Missing-values\" data-toc-modified-id=\"4.2.-Missing-values-4.2\">4.2. Missing values<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#4.2.1-Age\" data-toc-modified-id=\"4.2.1-Age-4.2.1\">4.2.1 Age<\/a><\/span><\/li><li><span><a href=\"#4.2.2-Embarked\" data-toc-modified-id=\"4.2.2-Embarked-4.2.2\">4.2.2 Embarked<\/a><\/span><\/li><li><span><a href=\"#4.2.3-Fare\" data-toc-modified-id=\"4.2.3-Fare-4.2.3\">4.2.3 Fare<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#5.-Feature-generation\" data-toc-modified-id=\"5.-Feature-generation-5\">5. Feature generation<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#5.1-Function\" data-toc-modified-id=\"5.1-Function-5.1\">5.1 Function<\/a><\/span><\/li><li><span><a href=\"#5.2-Generation\" data-toc-modified-id=\"5.2-Generation-5.2\">5.2 Generation<\/a><\/span><\/li><li><span><a href=\"#5.3-Correlation\" data-toc-modified-id=\"5.3-Correlation-5.3\">5.3 Correlation<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#6.-Models\" data-toc-modified-id=\"6.-Models-6\">6. Models<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#6.1-Explore-and-tune-models\" data-toc-modified-id=\"6.1-Explore-and-tune-models-6.1\">6.1 Explore and tune models<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#6.1.1-How-to-tune\" data-toc-modified-id=\"6.1.1-How-to-tune-6.1.1\">6.1.1 How to tune<\/a><\/span><\/li><li><span><a href=\"#6.1.2-Logistic-regression\" data-toc-modified-id=\"6.1.2-Logistic-regression-6.1.2\">6.1.2 Logistic regression<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#6.1.2.1-About-Logistic-regression\" data-toc-modified-id=\"6.1.2.1-About-Logistic-regression-6.1.2.1\">6.1.2.1 About Logistic regression<\/a><\/span><\/li><li><span><a href=\"#6.1.2.2-Tune\" data-toc-modified-id=\"6.1.2.2-Tune-6.1.2.2\">6.1.2.2 Tune<\/a><\/span><\/li><li><span><a href=\"#6.1.2.3-Feature-importance\" data-toc-modified-id=\"6.1.2.3-Feature-importance-6.1.2.3\">6.1.2.3 Feature importance<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#6.1.3-Random-forest\" data-toc-modified-id=\"6.1.3-Random-forest-6.1.3\">6.1.3 Random forest<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#6.1.3.1-What-is-random-forest\" data-toc-modified-id=\"6.1.3.1-What-is-random-forest-6.1.3.1\">6.1.3.1 What is random forest<\/a><\/span><\/li><li><span><a href=\"#6.1.3.2-Tune\" data-toc-modified-id=\"6.1.3.2-Tune-6.1.3.2\">6.1.3.2 Tune<\/a><\/span><\/li><li><span><a href=\"#6.1.3.3-Feature-importance\" data-toc-modified-id=\"6.1.3.3-Feature-importance-6.1.3.3\">6.1.3.3 Feature importance<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#6.1.4-XGBoost\" data-toc-modified-id=\"6.1.4-XGBoost-6.1.4\">6.1.4 XGBoost<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#6.1.4.1-XGBoost---eXtreme-Gradient-Boosting.\" data-toc-modified-id=\"6.1.4.1-XGBoost---eXtreme-Gradient-Boosting.-6.1.4.1\">6.1.4.1 XGBoost - e<strong>X<\/strong>treme <strong>G<\/strong>radient <strong>B<\/strong>oosting.<\/a><\/span><\/li><li><span><a href=\"#6.1.4.2-Tune\" data-toc-modified-id=\"6.1.4.2-Tune-6.1.4.2\">6.1.4.2 Tune<\/a><\/span><\/li><li><span><a href=\"#6.4.1.3-Feature-impotance\" data-toc-modified-id=\"6.4.1.3-Feature-impotance-6.1.4.3\">6.4.1.3 Feature impotance<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#6.2-Compare-models\" data-toc-modified-id=\"6.2-Compare-models-6.2\">6.2 Compare models<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#7.-Submission\" data-toc-modified-id=\"7.-Submission-7\">7. Submission<\/a><\/span><\/li><\/ul><\/div>","46dd8d9a":"### 4.2.3 Fare\nOnly one fare value is missed. I will fill NA with median Fare calculated inside of same  Pclass + Sex + Title_category + Parch group.","f81412e8":"#### 6.1.4.2 Tune  \nHyperparameters:\n* **booster** - Which booster to use. Can be gbtree, gblinear or dart; gbtree and dart use tree based models while gblinear uses linear functions. I will use default - gbtree.\n* **random_state** - Random number seed.\n* **n_estimators** - Number of gradient boosted trees. Equivalent to number of boosting rounds.\n* **max_depth** \u2013 Maximum tree depth for base learners. Increasing this value will make the model more complex and more likely to overfit.\n* **learning_rate** - Step size shrinkage used in update to prevents overfitting. After each boosting step, we can directly get the weights of new features, and eta shrinks the feature weights to make the boosting process more conservative.\n* **gamma (min_split_loss)** -  Minimum loss reduction required to make a further partition on a leaf node of the tree.\n* **min_child_weight** \u2013 Minimum sum of instance weight(hessian) needed in a child.\n* **subsample** - Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting.","c098d491":"#### 6.1.3.1 What is random forest\nTo understand the random forest, first, we need to understand other Machine Learning algorithm - Decision Trees.  \n\n**Decision trees**, in simple words, it is a tree-like model of decisions. A decision tree starts with a single node (whole population), then branches by condition (feature) into possible outcomes. These outcomes, in turn, lead to nodes, which branch off into other possibilities or to leaf node, which represents a class label.  \nWe choose the next feature for splitting the data based on information gain value in order to decrease in entropy after a data-set is split on an attribute or gini index. \n\n**Random forest** consists of a number of individual decision trees that operate as an ensemble. It uses bagging and feature randomness when building each individual tree to try to create an uncorrelated forest of trees whose prediction by committee is more accurate than that of any individual tree. Random forest makes decision by taking the majority vote.\n\nFor better understanding, I drew a very simple version of decision tree and random forest:\n\n![trees.png](attachment:trees.png)\nI will use [**RandomForestClassifier**](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html) from sklearn.ensemble.","6d460f7c":"# 3. Load data\nAs input information I have two CSV files:\n\n* **train.csv** - training part of the dataset, contains labels and information (11 variables) about 891 passengers.\n* **test.csv** - testing part of the dataset, 11 variables, 418 passengers, doesn't contain labels.\n\nI am using pandas read_csv to files into pandas DataFrames.","bdf45565":"#### 6.4.1.3 Feature impotance","a1399ba8":"## 5.3 Correlation\nCorrelation refers to how close two variables are to having a linear relationship with each other. Features with high correlation are more linearly dependent and hence have almost the same effect on the dependent variable. So, when two features have high correlation, we can drop one of the two features.  ","a1d83782":"### 6.1.4 XGBoost\n#### 6.1.4.1 XGBoost - e**X**treme **G**radient **B**oosting. \n\n[XGBoost](https:\/\/towardsdatascience.com\/a-beginners-guide-to-xgboost-87f5d4c30ed7) algorithm can be based on decision trees as well, but uses a different teqnicue to \"combine them\" -  gradient boosting. Boosting is a sequential technique which works on the principle of an ensemble. It combines a set of weak learners and delivers improved prediction accuracy. Rather than training all of the models in isolation of one another, boosting trains models in succession, with each new model being trained to correct the errors made by the previous ones. Models are added sequentially until no further improvements can be made.  \nThe advantage of this iterative approach is that the new models being added are focused on correcting the mistakes which were caused by other models.  \n[XGBoost documentation.](https:\/\/xgboost.readthedocs.io\/)\n\nIf you are interested, I highly recommend [this \"Intermediate Machine Learning\"](https:\/\/www.kaggle.com\/learn\/intermediate-machine-learning) course from @alexisbcook, where you can find information about [xgboost](https:\/\/www.kaggle.com\/alexisbcook\/xgboost), and more!","8cc62b51":"# 6. Models\nWho would have survived the Titanic disaster - a classification problem. Base on input (features) we need to determine to which 'class' the passenger belongs - survived or not. In this notebook I will explain, tune and compare three different models, to solve this problem:\n* Logistic regression\n* Random forest\n* XGBoost","f79be592":"#### 6.1.2.3 Feature importance\nFor better understanding our data and how our model works, now we can calculate and visualize feature importance!","6655889d":"## 5.1 Function","881e02e8":"Using sns.heatmap we can visualise missing values:","2a584873":"### 6.1.2 Logistic regression\n#### 6.1.2.1 About Logistic regression\n[Logistic regression](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html) is used to predict the probability of an event occurring by comparing it with the logistic curve. This regression returns the response as the probability of a binary event (Survived or not).\n\n#### 6.1.2.2 Tune\nHyperparameters I will tune:\n* **C** - regularization parameter = 1\/\u03bb, where \u03bb controls the trade-off between allowing the model to increase it's complexity. Small values of C increases the regularization strength which will create simple models which underfit the data. \n* **solver** - Algorithm to use in the optimization problem.\n* **class_weight** - Weights associated with classes in the form {class_label: weight}. If not given, all classes are supposed to have weight one. The \u201cbalanced\u201d mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples \/ (n_classes * np.bincount(y)).","b0798b73":"# 7. Submission","16baae62":"\u0410rom the graph, we can see that some features have correlation (for example, here is a title Mr and the Sex of the passenger, the size of the family - single and the presence of siblings, fare and class). However, I decided to leave all the attributes to see how the models will sort them by importance.","d46675fe":"## 6.2 Compare models\nTo compare models, I will use cross-validation technique as well. To do it, I will use model_selection.KFold to split data on 'folders'\nand model_selection.cross_val_score to get accuracy scores from each iterations.","890f0b57":"Thank you for your time! If you have any suggestions or comments, welcome!","3ebb00fb":"### 4.2.1 Age \nFirst, add Age_NA variable indicates that there is no age in the original data.  \nSecond, I will imput median Age calculated inside of each Pclass + Sex + Title_category group. To do so, I extracting Title from Name variable and map it on title category dictionary.\n","af1eec6a":"* There a we can see different peacture - Fare feature is 3rd by importancy for the model.  \n* The most important - presence of title Mr. As we remember from EDA , Mr is the most popular title, and the percentage of drownings in this category is almost the largest, so this feature can split the dataand decrease in entropy after a data-set is split.\n* Sex is second by its importancy.","6fededa1":"#### 6.1.3.2 Tune\nHyperparameters I will tune:\n* **criterion** - the function to measure the quality of a split. It can either be \u201cgini\u201d or \u201centropy\u201d. \u201cgini\u201d uses the Gini impurity while \u201centropy\u201d makes the split based on the information gain. \n* **n_estimators** - the number of trees in the forest.\n* **max_depth** - the maximum depth of the tree (the length of the longest path from the tree root to a leaf).  \n    If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.  \n    The deeper trees, the more complex your model will become. But if this parameter is too high, then the decision tree might simply overfit the training data without capturing useful patterns as we would like and will cause bigger test error.\n* **min_samples_split** - the minimum number of samples required to split an internal node.  \n* **min_samples_leaf** - the minimum number of samples required to be at a leaf node.\n\nAdditional hyperparameters I will set:\n* **random_state** - controls both the randomness of the bootstrapping of the samples used when building trees and the sampling of the features to consider when looking for the best split at each node. \n* **n_jobs** - The number of jobs to run in parallel. fit, predict, decision_path and apply are all parallelized over the trees. -1 means using all processors. \n* **oob_score** - to use or not out-of-bag samples to estimate the generalization accuracy.\n    Each of decision tree is trained separately on bootstrap samples (each tree has different training sample). Out of Bag sample - examples not included in the training sample. So we can use it to calculate accuracy.\n* **verbose** - Controls the verbosity when fitting and predicting."}}