{"cell_type":{"8763a5af":"code","abc3b438":"code","9415d879":"code","9035a89e":"code","8077ef15":"code","9058cc97":"code","b4d6a413":"code","4cb85998":"code","b7315883":"code","d63479cc":"code","888fac84":"code","50d0e401":"code","1021203c":"code","7ca8159b":"code","f2a23f1a":"code","f00dafd9":"code","2f067216":"code","cc6c6600":"code","324c3a05":"code","9d44b5a5":"code","a27109a0":"code","c6be3469":"markdown","095b8a5e":"markdown","71a827fe":"markdown","3debc14e":"markdown","0e1670fe":"markdown","f2b399d3":"markdown","a308e447":"markdown","27e04d73":"markdown","6cf49e64":"markdown","e256dd8e":"markdown","f5ad76fb":"markdown","5ee4630a":"markdown","caa6ba6a":"markdown","ee14465b":"markdown","b18a5f74":"markdown","ec2b9045":"markdown","bbdf4689":"markdown","659f387f":"markdown"},"source":{"8763a5af":"#   importing libraries\n\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\ndata = pd.read_csv('..\/input\/insurance\/insurance.csv')","abc3b438":"# Getting essence of our data!\ndata.info()","9415d879":"data.head()","9035a89e":"fig,axes=plt.subplots(2,1,figsize=(5,7))\nsns.countplot(data.region,palette='spring',ax=axes[0])\naxes[0].set_title(\"Region-wise distribution of dataset\",fontsize=20)\nsns.countplot(data.sex,palette='rainbow',ax=axes[1])\naxes[1].set_title(\"Gender-wise distribution of dataset\",fontsize=20)\nplt.tight_layout();","8077ef15":"sns.countplot(data.smoker,palette='prism')\nplt.title(\"Smokers in our data\");","9058cc97":"sns.jointplot(data.bmi,data.charges,color='orange');","b4d6a413":"fig,axes = plt.subplots(1,2,figsize=(14,5))\nsns.kdeplot(data.charges,color='purple',ax=axes[0])\nsns.boxenplot(data.charges,color='green',ax=axes[1]);","4cb85998":"backup_data = data.copy()\ndata.charges = np.log(data.charges)","b7315883":"fig,axes = plt.subplots(1,2,figsize=(14,5))\nsns.distplot(data.charges,color='orange',ax=axes[0])\nsns.boxenplot(data.charges,color='orange',ax=axes[1]);","d63479cc":"data.children.value_counts()","888fac84":"data.children.replace([3,4,5],'More than 3',inplace=True)\ndata.children.replace(0,'Zero',inplace=True)\ndata.children.replace(1,'One',inplace=True)\ndata.children.replace(2,'Two',inplace=True)","50d0e401":"dummies = pd.get_dummies(data[['sex','smoker','region','children']],drop_first=True)\ndf_dummies = pd.concat([data,dummies],axis=1)\ndf_dummies.drop(['sex','smoker', 'region','charges','children'],axis=1,inplace=True)\ndf_dummies.head(3)","1021203c":"from sklearn.model_selection import train_test_split\nX=df_dummies\ny=data.charges\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=101)","7ca8159b":"# importing \nfrom sklearn.linear_model import LinearRegression\n\nlm=LinearRegression()\nlm.fit(X_train,y_train)\npred_lm = lm.predict(X_test)\n\n# Our predictions\nplt.scatter(y_test,pred_lm)\n\n# Perfect predictions\nplt.plot(y_test,y_test,'r');","f2a23f1a":"from sklearn.metrics import mean_absolute_error,mean_squared_error,explained_variance_score,r2_score\nprint(f\"Mean absolute error (MAE) is: {mean_absolute_error(y_test,pred_lm).round(3)}\\n\\\nMean squared error (MSE) is: {mean_squared_error(y_test,pred_lm).round(3)}\\n\\\nRoot Mean Squared error (RMSE) is: {np.sqrt(mean_squared_error(y_test,pred_lm)).round(3)}\\n\\\nExplained Variance Score is: {explained_variance_score(y_test,pred_lm).round(3)}\\n\\\nR-squared for transformed target variable is: {r2_score(y_test,pred_lm).round(3)}\")","f00dafd9":"print(f\"R-squared for actual target variable is: {r2_score(np.exp(y_test),np.exp(pred_lm)).round(3)}\")","2f067216":"# import the library\nfrom sklearn.model_selection import cross_val_score\n\n# Compute 4-fold cross-validation scores: cv_scores\ncv_scores = cross_val_score(lm, X, y, cv=4)\n\nprint(\"Average 4-Fold CV Score: {}\".format(np.mean(cv_scores).round(4)))","cc6c6600":"import statsmodels.api as sm\nX1 = sm.add_constant(X)\nresults = sm.OLS(y,X1).fit()\nresults.summary()","324c3a05":"# Fitting Polynomial Regression to the dataset \nfrom sklearn.preprocessing import PolynomialFeatures \n  \npoly = PolynomialFeatures(degree = 2) \nX_poly = poly.fit_transform(X_train)\npoly.fit(X_poly, y_train) \nlin2 = LinearRegression() \nlin2.fit(X_poly, y_train)\npoly_pred=lin2.predict(poly.fit_transform(X_test))\n# Visualising the Polynomial Regression results \n# Our predictions\nplt.scatter(y_test,poly_pred)\n# Perfect predictions\nplt.plot(y_test,y_test,'r');","9d44b5a5":"print(f\"Mean absolute error (MAE) is: {mean_absolute_error(y_test,poly_pred).round(3)}\\n\\\nMean squared error (MSE) is: {mean_squared_error(y_test,poly_pred).round(3)}\\n\\\nRoot Mean Squared error (RMSE) is: {np.sqrt(mean_squared_error(y_test,poly_pred)).round(3)}\\n\\\nExplained Variance Score is: {explained_variance_score(y_test,poly_pred).round(3)}\\n\\\nR-squared for transformed target variable is: {r2_score(y_test,poly_pred).round(3)}\")","a27109a0":"print(f\"R-squared for actual target variable is: {r2_score(np.exp(y_test),np.exp(poly_pred)).round(3)}\")","c6be3469":"Our data is quite balanced with respect to sex and region features.","095b8a5e":"### Evaluating the model","71a827fe":"> *Please provide feedback and suggestions and help me to improve.*\nIf any other point could be concluded then share it in comments section.\nLeave an upvote to encourage me.","3debc14e":"Seems better.\n\n# Linear Regression\n\n> In case you are new to regression, read [this](https:\/\/ml-cheatsheet.readthedocs.io\/en\/latest\/linear_regression.html) article.\n\nHere's a small recap of assumptions of linear regression:\n* **Linearity**- This means that the mean of the response variable is a linear combination of the parameters (regression coefficients) and the predictor variables. A linear relationship suggests that a change in response Y due to one unit change in X1 is constant, regardless of the value of X1. An additive relationship suggests that the effect of X1 on Y is independent of other variables. Polynomial terms (X, X\u00b2, X\u00b3) can be included in model to capture the non-linear effect.\n* Error terms must be **normally distributed** with mean 0. If the errors are not normally distributed, non \u2013 linear transformation of the variables (response or predictors) can bring improvement in the model.\n* **Constant variance** (a.k.a. **homoscedasticity**)- This means that different values of the response variable have the same variance in their errors, regardless of the values of the predictor variables. Look at residual vs fitted values plot to check this assumption. If heteroskedasticity exists, the plot would exhibit a funnel shape pattern.\n* **No Autocorrelation**- There should be no correlation between the residual (error) terms. It is most likely to occur in time series model. If the error terms are correlated, the estimated standard errors tend to underestimate the true standard error. To check this use [Durbin-Watson test](https:\/\/en.wikipedia.org\/wiki\/Durbin%E2%80%93Watson_statistic). The Durbin Watson test reports a test statistic, with a value from 0 to 4, where 2 is no autocorrelation,0 to <2 is positive autocorrelation and >2 to 4 is negative autocorrelation. A rule of thumb is that test statistic values in the range of 1.5 to 2.5 are relatively normal. Values outside of this range could be cause for concern. \n* **No Multicollinearity**- The independent variables should not be correlated. A variance inflation factor(VIF) detects multicollinearity in regression analysis. VIFs are calculated by taking a predictor, and regressing it against every other predictor in the model. VIF ranges from 1 to infinity. For example, a VIF of 1.9 tells you that the variance of a particular coefficient is 90% bigger than what you would expect if there was no multicollinearity \u2014 if there was no correlation with other predictors. A rule of thumb for interpreting the variance inflation factor:1 means not correlated, Between 1 and 5 means moderately correlated and >5 means highly correlated.","0e1670fe":"# Applying regression using SciKit Learn library","f2b399d3":"* There is not clear linear relationship between target variable 'charges' and feature 'bmi'.\n* We can see that charges distribution is not normally distributed which is important assumption for linear regression.\n* In BMI histogram we can see that most of the observations have BMI centered around 25-35.\n\nLet's explore our target variable which is _charges_.","a308e447":"# EDA","27e04d73":"There are very less smokers as compared to non-smokers.","6cf49e64":"There is no missing value in this dataset which is really rare in practical world.","e256dd8e":"## Cross validation\n\nCross-validation is a vital step in evaluating a model. It maximizes the amount of data that is used to train the model.\nIn cross-validation, we split the training data into several subgroups. Then we use each of them in turn to evaluate the model fitted on the remaining portion of the data. It helps us to obtain reliable estimates of the model's generalization performance. So, it helps us to understand how well the model performs on unseen data. We can perform cross validation as follows:-","f5ad76fb":"So, RMSE decreased from 0.417 to 0.345 and R-squared increased to 85%.","5ee4630a":"# Applying regression using Statsmodel library","caa6ba6a":"> * RMSE is the standard deviation of the residuals. RMSE gives us the standard deviation of the unexplained variance by the model. It can be calculated by taking square root of Mean Squared Error. The more concentrated the data is around the regression line, the lower the residuals and hence lower the standard deviation of residuals. It results in lower values of RMSE. So, lower values of RMSE indicate better fit of data.\n\n> * R2 Score is another metric to evaluate performance of a regression model. It is also called Coefficient of Determination. It gives us an idea of goodness of fit for the linear regression models. It indicates the percentage of variance that is explained by the model. In general, the higher the R2 Score value, the better the model fits the data. Usually, its value ranges from 0 to 1. Its value can become negative if our model is wrong.\n\nSo, this model explains 79% variance of the target variable.","ee14465b":"### Converting categorical columns into numerical ones using dummy variables","b18a5f74":"**CONCLUSION :**\n\n* R-squared (0.769) implies that our regression line explains 76% variation of y.\n* A predictor that has a low p-value is likely to be a meaningful addition to our model because changes in the predictor's value are related to changes in the response variable. Seeing p-values in table we can conclude that all our variables are significant except 'children_Two' which is infact surprising.\n* Durbin-Watson test suggests that there is negligible autocorrelation as it is close to 2. Assumption of autocorrelation is also satisfied.\n* > Prob(Omnibus): One of the assumptions of OLS is that the errors are normally distributed. Omnibus test is performed in order to check this. Here, the null hypothesis is that the errors are normally distributed. Prob(Omnibus) is supposed to be close to the 1 in order for it to satisfy the OLS assumption.\n\n* In this case Prob(Omnibus) is 0., which implies that the OLS assumption is not satisfied. Due to this, the coefficients estimated out of it are not Best Linear Unbiased Estimators(BLUE).\n* It seems like a case where we would need to model this data using methods that can model non-linear relationships. Also variables need to be transformed to satisfy the normality assumption.\n\n# Polynomial Regression\n\nThe implementation of polynomial regression is a two-step process. First, we transform our data into a polynomial using the PolynomialFeatures function from sklearn and then use linear regression to fit the parameters. With the increasing degree of the polynomial, the complexity of the model also increases. Therefore, the value of n must be chosen precisely. If this value is low, then the model won\u2019t be able to fit the data properly and if high, the model will overfit the data easily.","ec2b9045":"Target variable is not normally distributed.Let's apply **logarithmic transformation** to solve this problem.","bbdf4689":"# Predicting Medical costs using Linear Regression\n\n## Inspiration\nCan we accurately predict insurance costs based on given features?\n## **About the dataset:** Columns:\n\n* age: age of primary beneficiary\n\n* sex: insurance contractor gender, female, male\n\n* bmi: Body mass index, providing an understanding of body, weights that are relatively high or low relative to height,\nobjective index of body weight (kg \/ m ^ 2) using the ratio of height to weight, ideally 18.5 to 24.9\n\n* children: Number of children covered by health insurance \/ Number of dependents\n\n* smoker: insurance contractor is a smoker or not\n\n* region: the beneficiary's residential area in the US: northeast, southeast, southwest, northwest.\n\n* charges: Individual medical costs billed by health insurance (*Target variable*)","659f387f":"Now, our dataset is ready for regression.\n\n### Splitting data into train and test sets"}}