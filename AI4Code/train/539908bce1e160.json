{"cell_type":{"5e8cd635":"code","80d154e5":"code","49dde443":"code","d2bb141a":"code","1eaf1790":"code","f96ab83f":"code","493b9829":"code","35886d8a":"code","7b6ec14e":"code","258859a5":"code","8377f057":"code","af7e464e":"code","02a892ae":"code","e8dc804f":"code","06aad2f7":"code","716a5936":"code","51b4e62b":"code","66719805":"code","970b25f1":"markdown","0101636f":"markdown","1d444ed7":"markdown","9b15cce2":"markdown","ca4f4e99":"markdown","220a14d2":"markdown","a0baf28e":"markdown","e3cfac81":"markdown","69e9d38d":"markdown","9584e7cd":"markdown","1178af05":"markdown","62f23d98":"markdown"},"source":{"5e8cd635":"import numpy as np \nimport pandas as pd\nimport xgboost as xgb\nfrom tqdm import tqdm\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport pandas as pd\nfrom collections import Counter\nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import TfidfTransformer\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold\n\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import AdaBoostClassifier\n\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer","80d154e5":"train = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\nsample = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")","49dde443":"count_vec = CountVectorizer()\ntfidf = TfidfTransformer()","d2bb141a":"def remove_punctuation(text):\n    '''a function for removing punctuation'''\n    import string\n    # replacing the punctuations with no space, \n    # which in effect deletes the punctuation marks \n    translator = str.maketrans('', '', string.punctuation)\n    # return the text stripped of punctuation marks\n    return text.translate(translator)\n\nsw = stopwords.words('english')\n\ndef stopwords(text):\n    '''a function for removing the stopword'''\n    # removing the stop words and lowercasing the selected words\n    text = [word.lower() for word in text.split() if word.lower() not in sw]\n    # joining the list of words with space separator\n    return \" \".join(text)\n\n\n# create an object of stemming function\nstemmer = SnowballStemmer(\"english\")\n\ndef stemming(text):    \n    '''a function which stems each word in the given text'''\n    text = [stemmer.stem(word) for word in text.split()]\n    return \" \".join(text) \n","1eaf1790":"train_keywords = train['keyword'].fillna('None')\ntest_keywords = test['keyword'].fillna('None')\n\ntrain['text'] = train_keywords + \" \" + train['text']\ntest['text'] = test_keywords + \" \" + test['text']\n\ntrain['text'] = train['text'].apply(remove_punctuation).apply(stopwords).apply(stemming)\ntest['text'] = test['text'].apply(remove_punctuation).apply(stopwords).apply(stemming)\n\ntrain_vec = train['text'].tolist()\ntest_vec = test['text'].tolist()","f96ab83f":"# example of what data now looks like\ntrain_vec[100:105]","493b9829":"# preparing train text\ntrain_counts = count_vec.fit_transform(train_vec)\ntrain_tfidf = tfidf.fit_transform(train_counts)\n    \n# preparing test text\ntest_counts = count_vec.transform(test_vec)\ntest_tfidf = tfidf.transform(test_counts)","35886d8a":"ntrain = train.shape[0]\nntest = test.shape[0]\nSEED = 0 \nNFOLDS = 5\nkf = KFold(n_splits=NFOLDS, random_state=SEED)\nkf.get_n_splits(train_tfidf)","7b6ec14e":"class SklearnHelper(object):\n    def __init__(self, clf, seed=0, params=None):\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self,x,y):\n        return self.clf.fit(x,y)","258859a5":"def get_oof(clf, x_train, y_train, x_test):\n    \n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n    for i, (train_index, test_index) in enumerate(kf.split(x_train)):\n        x_tr = x_train[train_index]\n        y_tr = y_train[train_index]\n        x_te = x_train[test_index]\n\n        try:\n            clf.train(x_tr, y_tr)\n        except: \n            clf = clf.fit(x_tr, y_tr)\n\n        oof_train[test_index] = clf.predict(x_te)\n        oof_test_skf[i, :] = clf.predict(x_test)\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)","8377f057":"# AdaBoost parameters\nab_params = {\n    'n_estimators': 500,\n    'learning_rate' : 0.5\n}\n\n# Naive Bayes parameters \nnb_params = {\n    # use 'alpha' : 1.0\n    'alpha' : 1.0,\n    'fit_prior' : True\n}\n\n# XGBoost parameters\nxg_params = {\n            'learning_rate' : 0.1,\n            'n_estimators': 500,\n            'max_depth': 6,\n            'min_child_weight': 2,\n            'gamma':1,                        \n            'subsample':0.8,\n            'colsample_bytree':0.8,\n            'objective': 'binary:logistic',\n            'nthread': -1,\n            'scale_pos_weight':1\n}\n\n# Logistic Regression parameters \nlg_params = {\n    'C' : 1.0,\n    'verbose' : 0\n}","af7e464e":"nb = SklearnHelper(clf=MultinomialNB, seed=SEED, params=nb_params) # Naive Bayes\nxg = SklearnHelper(clf=xgb.XGBClassifier, seed=SEED, params=xg_params) # XGBoost\nab = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ab_params) # AdaBoost\nlg = SklearnHelper(clf=LogisticRegression, seed=SEED, params= lg_params) # Logistic Regression","02a892ae":"nb_oof_train, nb_oof_test = get_oof(nb,train_tfidf, train['target'], test_tfidf) # AdaBoost\nprint('Done')\nxg_oof_train, xg_oof_test = get_oof(xg, train_tfidf, train['target'], test_tfidf) # XGBoost \nprint('Done')\nab_oof_train, ab_oof_test = get_oof(ab, train_tfidf, train['target'], test_tfidf) # AdaBoost \nprint('Done')\nlg_oof_train, lg_oof_test = get_oof(lg, train_tfidf, train['target'], test_tfidf) # Logistic Regression\nprint('Done')","e8dc804f":"# Getting hard labels\nab_oof_test=ab_oof_test.ravel()\nab_oof_test = [int(x) for x in np.rint(ab_oof_test)]\n\nab_oof_train=ab_oof_train.ravel()\nab_oof_train = [int(x) for x in np.rint(ab_oof_train)]\n\nnb_oof_test=nb_oof_test.ravel()\nnb_oof_test = [int(x) for x in np.rint(nb_oof_test)]\n\nnb_oof_train=nb_oof_train.ravel()\nnb_oof_train = [int(x) for x in np.rint(nb_oof_train)]\n\nxg_oof_test=xg_oof_test.ravel()\nxg_oof_test = [int(x) for x in np.rint(xg_oof_test)]\n\nxg_oof_train=xg_oof_train.ravel()\nxg_oof_train = [int(x) for x in np.rint(xg_oof_train)]\n\nlg_oof_test=lg_oof_test.ravel()\nlg_oof_test = [int(x) for x in np.rint(lg_oof_test)]\n\nlg_oof_train=lg_oof_train.ravel()\nlg_oof_train = [int(x) for x in np.rint(lg_oof_train)]","06aad2f7":"base_predictions_train = pd.DataFrame({\n    'AdaBoost': ab_oof_train,\n    'NaiveBayes' : nb_oof_train,\n    'XGBoost' : xg_oof_train, \n    'Logistic' : lg_oof_train\n})\n\nbase_predictions_test = pd.DataFrame({\n    'AdaBoost': ab_oof_test,\n    'NaiveBayes' : nb_oof_test,\n    'XGBoost' : xg_oof_test,\n    'Logistic': lg_oof_test\n})\n\nx_train = np.array(base_predictions_train)\nx_test = np.array(base_predictions_test)\n\nbase_predictions_train.head()","716a5936":"import plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\ndata = [\n    go.Heatmap(\n        z= base_predictions_train.astype(float).corr().values ,\n        x=base_predictions_train.columns.values,\n        y= base_predictions_train.columns.values,\n          colorscale='Viridis',\n            showscale=True,\n            reversescale = True\n    )\n]\npy.iplot(data, filename='labelled-heatmap')","51b4e62b":"gbm = xgb.XGBClassifier(\n    #learning_rate = 0.02,\n n_estimators= 2000,\n max_depth = 5,\n min_child_weight= 2,\n #gamma=1,\n gamma=1,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread= -1,\n scale_pos_weight=1).fit(x_train, train['target'])\nsample['target'] = gbm.predict(x_test)\nsample.to_csv('submission.csv', index = False)","66719805":"Counter(sample['target'])","970b25f1":"We would like to obtain Out of Fold predictions so that we don't introduce unnecessary sources of overfitting. ","0101636f":"## Ensembling and Stacking models\n\nEnsembling enables us to combine predictions from various models, which should hopefully result in a model that generalizes our data better and is less prone to overfitting. \nWe select a number of models we will be predicting our data on as our **first-level base models** and then use a **second-level** model to predict the final output by feeding it the output of the first-level predictions. \n\nAs a rule of thumb, at first level one hopes to use models with lowest correlation levels between them, with the thought that combining very different models produces a model which take the best from everyone.\n\n### Helper functions\nThe class below extends the functionality of sklearn models we will be using. \n\nThis is not necessary to perform ensembling. Models vary in functionality and methods one can call on them, but this approach is very neat and saves a lot of time debugging code for each model. Using this approach makes sure that one can keep adding new models seemlessly with copy-pasting existing lines of code. ","1d444ed7":"Next steps might involve including Grid Search to find best parameters for each model, and optimize the number of folds. But this kind of hyperparameter tuning can be very expensive on datasets much larger than this one.\n\nThanks for your time if you made it to the end of the notebook. This is my first kernel, I hope this brought something interesting to the learning experience in this competition - how ensembling and stacking could be successfully used for nlp classification problems. However all basic models in NLP find it very difficult to achieve very high accuracy.\n\nFeel free to leave any feedback on my code or any questions you might have. ","9b15cce2":"Using a separate instance of XGBoost for our second level prediction and producing the final predictions. ","ca4f4e99":"Here we can see the correlation between our models - after a lot of experimenting I decided that these would be best to include as SVM, Gradient Boosting and Random Forests were too similar to the chosen models and final predictions were not affected. ","220a14d2":"## Text Processing\n\nRemoving punctuation, stopwords and transforming words into word stems should be credited entirely to Kassem's notebook https:\/\/www.kaggle.com\/elcaiseri\/nlp-the-simplest-way. This is not required for the ensemble, but should help us to simplify how our models handle data.","a0baf28e":"Here I will be joining keywords and text of the tweet together and preparing data to be fed into various models. As many before me I use CountVectorizer and TF-IDF.\n","e3cfac81":"For each model used, all parameters go here. ","69e9d38d":"# Creating a Basic Ensemble for Disaster Tweets\n\nIn this kernel I will be using methods described by Anisotropic (kaggle.com\/arthurtok) in https:\/\/www.kaggle.com\/arthurtok\/introduction-to-ensembling-stacking-in-python in order to perform binary classification of disaster tweets. \n\nIf you are not familiar with ensembles his kernel is the perfect way to start learning about them, which is what I did. I will be definitely be looking for opportunities to use this methods on other datasets. ","9584e7cd":"Now we define objects for each of the model we decided on using our Helper class.\n\nFor this model I stuck with Naive Bayes, which produces a high benchmark, XGBoost, AdaBoost and Logistic Regression with the expectation that they will be all different as possible.","1178af05":"Because we would like to classify tweets and not use probabilities, one must tranform output into hard labels.","62f23d98":"Output from training our first-level models."}}