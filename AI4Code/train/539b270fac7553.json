{"cell_type":{"3f88800b":"code","a757baf2":"code","9fabb77d":"code","e35ea02d":"code","ed41610f":"code","517eaf23":"code","9d533423":"code","c5cfb7da":"code","d3cca94b":"code","536111c0":"code","f2a1edda":"code","1458eb97":"code","74d93d7b":"code","b703ff73":"code","1e306bcb":"code","2bc0dd9f":"code","d47e8221":"code","aeb3f3d1":"code","8da485f1":"code","474fe2b3":"code","4b287355":"code","76220511":"code","170198e6":"code","2682f588":"code","e5493fdb":"code","1fe8882d":"code","71584c47":"code","f29a8438":"code","e9b51e50":"code","d83ee9c9":"code","95b8a917":"code","f1cd0127":"code","209540e8":"code","2a798fc6":"code","185b02a6":"code","7ff5cece":"code","c6892e59":"code","08e08c4a":"code","9a77b2d6":"code","c30d1dac":"code","3d0c12a2":"code","ea6de242":"code","bd01c334":"code","23ce2c8c":"code","b2158eb9":"code","6be4bd4d":"code","fe8f3fbc":"code","9e20948f":"code","95625870":"code","4d745f12":"code","ada44181":"code","b0568883":"code","85a72b7d":"code","c273e631":"code","b19d7de8":"code","97197800":"code","11b9b9db":"code","effdb43d":"code","3c72a213":"code","d4086a05":"markdown","5eb83c0d":"markdown","885b5a63":"markdown","b938af6c":"markdown","cb9b930e":"markdown","9eff7960":"markdown","097bc075":"markdown","cae2f70e":"markdown","12e651c7":"markdown","e3b609ea":"markdown","7520323a":"markdown","5c0b7f22":"markdown","9232b36b":"markdown","abd649cc":"markdown","75642e51":"markdown","40f0affc":"markdown","5000ad4c":"markdown","d07b99fb":"markdown","ccc1ebcb":"markdown","9a709f19":"markdown","476f708b":"markdown","2379f887":"markdown","1b49450c":"markdown","5c01de27":"markdown","5556841f":"markdown","11389298":"markdown","202495d1":"markdown","81f04122":"markdown","c22f3839":"markdown","48994fbf":"markdown","462ce47d":"markdown","9e28306c":"markdown","c51aeb92":"markdown","ffa4415e":"markdown","5f723288":"markdown","bba9d881":"markdown","8c2da064":"markdown","0f5511cc":"markdown","3d5a0abe":"markdown","af101c3e":"markdown","67d12a1e":"markdown","7b61c601":"markdown","7b49f4d5":"markdown","37f30d46":"markdown","c188870a":"markdown","c72a1298":"markdown","59ad7c3b":"markdown","d9b3d91c":"markdown","1b56379b":"markdown","a04531dd":"markdown","cc4c3f5b":"markdown","44721a50":"markdown","0d8bbc57":"markdown","87669aed":"markdown","527d18f4":"markdown","7d0f0e36":"markdown","23bb4b82":"markdown","bff69eeb":"markdown","fabf228f":"markdown","57bdc990":"markdown","5f949501":"markdown","8fbaaef6":"markdown","e660a04e":"markdown"},"source":{"3f88800b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\npd.options.display.max_columns = 100","a757baf2":"# \n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","9fabb77d":"data = pd.read_csv(\"\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv\")","e35ea02d":"# Cell 1\nlen(data.index), len(data.columns)","ed41610f":"# Cell 2\ndata.shape","517eaf23":"data.head()","9d533423":"data.tail()","c5cfb7da":"data.info()","d3cca94b":"data.isna()","536111c0":"data.isna().any()","f2a1edda":"data.isna().sum() ","1458eb97":"data = data.dropna(axis='columns')","74d93d7b":"data.describe(include=\"O\")","b703ff73":"data.diagnosis.value_counts()","1e306bcb":"data.head(2)","2bc0dd9f":"diagnosis_unique = data.diagnosis.unique()","d47e8221":"diagnosis_unique","aeb3f3d1":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n%matplotlib inline\nsns.set_style('darkgrid')","8da485f1":"plt.figure(figsize=(15, 5))\n\nplt.subplot(1, 2, 1)\nplt.hist( data.diagnosis)\n# plt.legend()\nplt.title(\"Counts of Diagnosis\")\nplt.xlabel(\"Diagnosis\")\n\n\nplt.subplot(1, 2, 2)\n\nsns.countplot('diagnosis', data=data); # \";\" to remove output like this > <matplotlib.axes._subplots.AxesSubplot at 0x7f3a1dddba50>\n\n# plt.show() ","474fe2b3":"# plt.figure(figsize=(7,12))\npx.histogram(data, x='diagnosis')\n# plt.show()","4b287355":"cols = [\"diagnosis\", \"radius_mean\", \"texture_mean\", \"perimeter_mean\", \"area_mean\"]\n\nsns.pairplot(data[cols], hue=\"diagnosis\")\nplt.show()","76220511":"size = len(data['texture_mean'])\n\narea = np.pi * (15 * np.random.rand( size ))**2\ncolors = np.random.rand( size )\n\nplt.xlabel(\"texture mean\")\nplt.ylabel(\"radius mean\") \nplt.scatter(data['texture_mean'], data['radius_mean'], s=area, c=colors, alpha=0.5);","170198e6":"from sklearn.preprocessing import LabelEncoder","2682f588":"data.head(2)","e5493fdb":"labelencoder_Y = LabelEncoder()\ndata.diagnosis = labelencoder_Y.fit_transform(data.diagnosis)","1fe8882d":"data.head(2)","71584c47":"print(data.diagnosis.value_counts())\nprint(\"\\n\", data.diagnosis.value_counts().sum())","f29a8438":"cols = ['diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']\nprint(len(cols))\ndata[cols].corr()\n","e9b51e50":"plt.figure(figsize=(12, 9))\n\nplt.title(\"Correlation Graph\")\n\ncmap = sns.diverging_palette( 1000, 120, as_cmap=True)\nsns.heatmap(data[cols].corr(), annot=True, fmt='.1%',  linewidths=.05, cmap=cmap);","d83ee9c9":"plt.figure(figsize=(15, 10))\n\n\nfig = px.imshow(data[cols].corr());\nfig.show()","95b8a917":"from sklearn.model_selection import train_test_split\n\nfrom sklearn.preprocessing import StandardScaler\n","f1cd0127":"from sklearn.linear_model import LogisticRegression\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.neighbors import KNeighborsClassifier\n","209540e8":"from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n\nfrom sklearn.metrics import classification_report\n\nfrom sklearn.model_selection import KFold\n\nfrom sklearn.model_selection import cross_validate, cross_val_score\n\nfrom sklearn.svm import SVC\n\nfrom sklearn import metrics","2a798fc6":"data.columns","185b02a6":"prediction_feature = [ \"radius_mean\",  'perimeter_mean', 'area_mean', 'symmetry_mean', 'compactness_mean', 'concave points_mean']\n\ntargeted_feature = 'diagnosis'\n\nlen(prediction_feature)","7ff5cece":"X = data[prediction_feature]\nX\n\n# print(X.shape)\n# print(X.values)","c6892e59":"y = data.diagnosis\ny\n\n# print(y.values)","08e08c4a":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=15)\n\nprint(X_train)\n# print(X_test)","9a77b2d6":"# Scale the data to keep all the values in the same magnitude of 0 -1 \n\nsc = StandardScaler()\n\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)\n","c30d1dac":"def model_building(model, X_train, X_test, y_train, y_test):\n    \"\"\"\n    \n    Model Fitting, Prediction And Other stuff\n    return ('score', 'accuracy_score', 'predictions' )\n    \"\"\"\n    \n    model.fit(X_train, y_train)\n    score = model.score(X_train, y_train)\n    predictions = model.predict(X_test)\n    accuracy = accuracy_score(predictions, y_test)\n    \n    return (score, accuracy, predictions)    ","3d0c12a2":"models_list = {\n    \"LogisticRegression\" :  LogisticRegression(),\n    \"RandomForestClassifier\" :  RandomForestClassifier(n_estimators=10, criterion='entropy', random_state=5),\n    \"DecisionTreeClassifier\" :  DecisionTreeClassifier(criterion='entropy', random_state=0),\n    \"SVC\" :  SVC(),\n}\n\n# print(models_list)","ea6de242":"print(list(models_list.keys()))\nprint(list(models_list.values()))\n\n# print(zip(list(models_list.keys()), list(models_list.values())))","bd01c334":"# Let's Define the function for confision metric Graphs\n\ndef cm_metrix_graph(cm):\n    \n    sns.heatmap(cm,annot=True,fmt=\"d\")\n    plt.show()\n        \n","23ce2c8c":"df_prediction = []\nconfusion_matrixs = []\ndf_prediction_cols = [ 'model_name', 'score', 'accuracy_score' , \"accuracy_percentage\"]\n\nfor name, model in zip(list(models_list.keys()), list(models_list.values())):\n    \n    (score, accuracy, predictions) = model_building(model, X_train, X_test, y_train, y_test )\n    \n    print(\"\\n\\nClassification Report of '\"+ str(name), \"'\\n\")\n    \n    print(classification_report(y_test, predictions))\n\n    df_prediction.append([name, score, accuracy, \"{0:.2%}\".format(accuracy)])\n    \n    # For Showing Metrics\n    confusion_matrixs.append(confusion_matrix(y_test, predictions))\n    \n        \ndf_pred = pd.DataFrame(df_prediction, columns=df_prediction_cols)\n","b2158eb9":"print(len(confusion_matrixs))","6be4bd4d":"plt.figure(figsize=(10, 2))\n# plt.title(\"Confusion Metric Graph\")\n\n\nfor index, cm in enumerate(confusion_matrixs):\n\n    up\n#     plt.xlabel(\"Negative Positive\")\n#     plt.ylabel(\"True Positive\")\n\n    \n    \n    # Show The Metrics Graph    \n    cm_metrix_graph(cm) # Call the Confusion Metrics Graph\n    plt.tight_layout(pad=True)","fe8f3fbc":"df_pred","9e20948f":"df_pred.sort_values('score', ascending=False)\n# df_pred.sort_values('accuracy_score', ascending=False)","95625870":"len(data)\n# print(len(X))\n","4d745f12":"# Sample For testing only\n\ncv_score = cross_validate(LogisticRegression(), X, y, cv=3,\n                        scoring=('r2', 'neg_mean_squared_error'),\n                        return_train_score=True)\n\npd.DataFrame(cv_score).describe().T\n","ada44181":"def cross_val_scorring(model):\n    \n#     (score, accuracy, predictions) = model_building(model, X_train, X_test, y_train, y_test )\n    \n    model.fit(data[prediction_feature], data[targeted_feature])\n    \n    # score = model.score(X_train, y_train)    \n    \n    predictions = model.predict(data[prediction_feature])    \n    accuracy = accuracy_score(predictions, data[targeted_feature])\n    print(\"\\nFull-Data Accuracy:\", round(accuracy, 2))\n    print(\"Cross Validation Score of'\"+ str(name), \"'\\n\")\n    \n    \n    # Initialize K folds.\n    kFold = KFold(n_splits=5) # define 5 diffrent data folds\n    \n    err = []\n    \n    for train_index, test_index in kFold.split(data):\n        # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n\n        # Data Spliting via fold indexes\n        X_train = data[prediction_feature].iloc[train_index, :] # train_index = rows and all columns for Prediction_features\n        y_train = data[targeted_feature].iloc[train_index] # all targeted features trains\n        \n        X_test = data[prediction_feature].iloc[test_index, :] # testing all rows and cols\n        y_test = data[targeted_feature].iloc[test_index] # all targeted tests\n        \n        # Again Model Fitting\n        model.fit(X_train, y_train)\n\n        err.append(model.score(X_train, y_train))\n        \n        print(\"Score:\", round(np.mean(err),  2) )","b0568883":"for name, model in zip(list(models_list.keys()), list(models_list.values())):\n    cross_val_scorring(model)","85a72b7d":"from  sklearn.model_selection import GridSearchCV","c273e631":"# Let's Implement Grid Search Algorithm\n\n# Pick the model\nmodel = DecisionTreeClassifier()\n\n# Tunning Params\nparam_grid = {'max_features': ['auto', 'sqrt', 'log2'],\n              'min_samples_split': [2,3,4,5,6,7,8,9,10], \n              'min_samples_leaf':[2,3,4,5,6,7,8,9,10] }\n\n\n# Implement GridSearchCV\ngsc = GridSearchCV(model, param_grid, cv=10) # For 10 Cross-Validation\n\ngsc.fit(X_train, y_train) # Model Fitting\n\nprint(\"\\n Best Score is \")\nprint(gsc.best_score_)\n\nprint(\"\\n Best Estinator is \")\nprint(gsc.best_estimator_)\n\nprint(\"\\n Best Parametes are\")\nprint(gsc.best_params_)","b19d7de8":"# Pick the model\nmodel = KNeighborsClassifier()\n\n\n# Tunning Params\nparam_grid = {\n    'n_neighbors': list(range(1, 30)),\n    'leaf_size': list(range(1,30)),\n    'weights': [ 'distance', 'uniform' ]\n}\n\n\n# Implement GridSearchCV\ngsc = GridSearchCV(model, param_grid, cv=10)\n\n# Model Fitting\ngsc.fit(X_train, y_train)\n\nprint(\"\\n Best Score is \")\nprint(gsc.best_score_)\n\nprint(\"\\n Best Estinator is \")\nprint(gsc.best_estimator_)\n\nprint(\"\\n Best Parametes are\")\nprint(gsc.best_params_)","97197800":"# Pick the model\nmodel = SVC()\n\n\n# Tunning Params\nparam_grid = [\n              {'C': [1, 10, 100, 1000], \n               'kernel': ['linear']\n              },\n              {'C': [1, 10, 100, 1000], \n               'gamma': [0.001, 0.0001], \n               'kernel': ['rbf']\n              }\n]\n\n\n# Implement GridSearchCV\ngsc = GridSearchCV(model, param_grid, cv=10) # 10 Cross Validation\n\n# Model Fitting\ngsc.fit(X_train, y_train)\n\nprint(\"\\n Best Score is \")\nprint(gsc.best_score_)\n\nprint(\"\\n Best Estinator is \")\nprint(gsc.best_estimator_)\n\nprint(\"\\n Best Parametes are\")\nprint(gsc.best_params_)","11b9b9db":"# Pick the model\nmodel = RandomForestClassifier()\n\n\n# Tunning Params\nrandom_grid = {'bootstrap': [True, False],\n 'max_depth': [40, 50, None], # 10, 20, 30, 60, 70, 100,\n 'max_features': ['auto', 'sqrt'],\n 'min_samples_leaf': [1, 2], # , 4\n 'min_samples_split': [2, 5], # , 10\n 'n_estimators': [200, 400]} # , 600, 800, 1000, 1200, 1400, 1600, 1800, 2000\n\n# Implement GridSearchCV\ngsc = GridSearchCV(model, random_grid, cv=10) # 10 Cross Validation\n\n# Model Fitting\ngsc.fit(X_train, y_train)\n\nprint(\"\\n Best Score is \")\nprint(gsc.best_score_)\n\nprint(\"\\n Best Estinator is \")\nprint(gsc.best_estimator_)\n\nprint(\"\\n Best Parametes are\")\nprint(gsc.best_params_)","effdb43d":"import pickle as pkl","3c72a213":"# Trainned Model # You can also use your own trainned model\nlogistic_model = LogisticRegression()\nlogistic_model.fit(X_train, y_train)\n\nfilename = 'logistic_model.pkl'\npkl.dump(logistic_model, open(filename, 'wb')) # wb means write as binary\n","d4086a05":"Before, sending it to the prediction check the key and values to store it's values in DataFrame below.","5eb83c0d":"After installing numpy and pandas package, we are ready to fetch data using pandas package, Befor we use it, We need to know where's our dataset located. Means what is the path of our dataset","885b5a63":"After converting into numerical values, we can check it's values using this way, ","b938af6c":" using  `value_counts` method we can see number of unique values in categorical type of feature.","cb9b930e":"#### Now, You can check your current directory. You can see the file with named \"logistic_model.pkl\"","9eff7960":"### Observation \n\nUsing this Algorithm, we can see that\n- The best score is increases\n- know the best estimator parametes for final model\n- get the best parametes for it.","097bc075":"<a id=\"1\"><\/a><br>\n\n# 1. Data Collection.","cae2f70e":"After collecting data, we need to know what are the shape of this dataset, Here we have attribute(`property`) called `data.shape`\n\nFor that we have 2 type of methods to show the shape of the datasets.\n\n1. `len(data.index), len(data.columns)`\n- `data.shape`\n\nBoth methods are giving us the same output, As you can see in the below cells`","12e651c7":"# \ud83e\udd80 Breast Cancer Prediction Using Machine Learning.","e3b609ea":"<a id=\"6\"><\/a><br>\n# HyperTunning the ML Model\n\n\n---\n---\n\n\n\n### Tuning Parameters applying...\n\n<!-- https:\/\/www.kaggle.com\/gargmanish\/basic-machine-learning-with-cancer  -->","7520323a":"For HyperTunning we can use `GridSearchCV` to know the best performing parameters","5c0b7f22":"<center><img src=\"https:\/\/healthitanalytics.com\/images\/site\/article_headers\/_normal\/ThinkstockPhotos-495951912.jpg\" alt=\"Breast Cancer Prediction Using Machine Learning\" height=\"70%\" width=\"100%\" \/><\/center>","9232b36b":"### Model Implementing\n","abd649cc":"Now, Train the model one by one and show the classification report of perticular models wise.","75642e51":"- *As we can see abouve result there are only one single feature is categorical and it's values are `B` and `M`*","40f0affc":"####  Arguments \n1. model => ML Model Object\n2. Feature Training Set data \n3. Feature Testing Set data\n4. Targetd Training Set data \n5. Targetd Testing Set data","5000ad4c":"- Now, we have one categorical feature, so we need to convert it into numeric values using `LabelEncoder` from `sklearn.preprocessing` packages","d07b99fb":"### To know how many unique values","ccc1ebcb":"Finnaly, We can see in this output categorical values converted into 0 and 1.","9a709f19":"- GridSearchCV implements a \u201cfit\u201d and a \u201cscore\u201d method. It also implements \u201cpredict\u201d, \u201cpredict_proba\u201d, \u201cdecision_function\u201d, \u201ctransform\u201d and \u201cinverse_transform\u201d if they are implemented in the estimator used.\n\n- The parameters of the estimator used to apply these methods are optimized by cross-validated grid-search over a parameter grid.\n\n","476f708b":"### Import Machine Learning Models\n","2379f887":"### Perform Feature Standerd Scalling","1b49450c":"Standardize features by removing the mean and scaling to unit variance\n\nThe standard score of a sample x is calculated as:\n\n- z = (x - u) \/ s","5c01de27":"### K-Fold Applying ...","5556841f":"- Some of the model are giving prefect scorring. it means sometimes overfitting occurs","11389298":"\n--- \n\n\n---\n\n\n---\n\n","202495d1":"<div class=\"text-success \"><h3> Table Of Contains<\/h3><\/div>\n\n--- \n\n> ### Steps are:\n\n\n1. [Gathering Data](#1)\n- [Exploratory Data Analysis](#2)\n- [Data Visualizations](#3)\n- [Model Implementation.](#4)\n- [ML Model Selecting and Model PredPrediction](#5)\n- [HyperTunning the ML Model](#6)\n- [Deploy Model](#7)\n\n\n\n \n**Hope** you guys ****Love It**** and get a better **learning experience**.  \ud83d\ude4f","81f04122":"<a id=\"4\"><\/a><br>\n\n# Model Implementation\n\n---\n---\n\n\n#### Train Test Splitting\n","c22f3839":"<a id=\"3\"><\/a><br>\n\n# 3. Data Visualization.","48994fbf":"### I hope you enjoy in this kernel and give Upvote it. \ud83d\udc4d","462ce47d":"### Conclusion\n\n- In this kernal, We had seen the data clearning and EDA using pandas methods and show some visual graphs to know the behaviour of this dataset and finnaly we train some model for it and calculate the prediction and it's acciracy scores and hyper tunning. I have wroted some basic codes in this notebook. So, After socessfully completed we can deploye our models to the live production mode using **exporting models and some python web applications.** For that we can use `Flask`, `Django` or `FastAPI` frameworks.","9e28306c":"While Predicting we can store model's score and prediction values to new generated dataframe","c51aeb92":"Let's  Implementing RandomForestClassifier for hyper Tunning\n\n> Remember while you run the below cell, it will take time for prediction and give the best params and estimators","ffa4415e":"Call the function to know the cross validation function by mean for our select model predictions.","5f723288":"### Attribute Information:\n\n1. ID number\n- Diagnosis (M = malignant, B = benign)\n\nTen real-valued features are computed for each cell nucleus:\n\n1. radius (mean of distances from center to points on the perimeter)\n- texture (standard deviation of gray-scale values)\n- perimeter\n- area\n- smoothness (local variation in radius lengths)\n- compactness (perimeter^2 \/ area - 1.0)\n- concavity (severity of concave portions of the contour)\n- concave points (number of concave portions of the contour)\n- symmetry\n- fractal dimension (\"coastline approximation\" - 1)","bba9d881":"- Take the dependent and independent feature for prediction","8c2da064":"Let's make a dictionary for multiple models for bulk predictions","0f5511cc":"\n- Finally, we are done so far. The last step is to deploy our model in production map. So we need to export our model and bind with web application API. \n\nUsing pickle we can export our model and store in to `model.pkl` file, so we can ealy access this file and calculate customize prediction using Web App API.\n\n\n### A little bit information about pickle:\n\n`Pickle` is the standard way of serializing objects in Python. You can use the pickle operation to serialize your machine learning algorithms and save the serialized format to a file. Later you can load this file to deserialize your model and use it to make new predictions\n\n\n>>  Here is example of the Pickle export model\n\n\n\n```\nmodel.fit(X_train, Y_train)\n# save the model to disk\nfilename = 'finalized_model.sav'\npickle.dump(model, open(filename, 'wb'))\n\n# some time later...\n\n# load the model from disk\nloaded_model = pickle.load(open(filename, 'rb'))\nresult = loaded_model.score(X_test, Y_test)\nprint(result)\n```","3d5a0abe":"- *Let's apply same criteria for* **K Neighbors Classification**\n\n[**To know the right params chckout its doc params**](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html)","af101c3e":"### Data Filtering\n","67d12a1e":"- Finally, Implement same strategy for **SVM**","7b61c601":"Let's define a functino for cross validation scorring for multiple ML models\n","7b49f4d5":"### Check the Model Accuracy, Errors and it's Validations","37f30d46":"### Observation \n\nUsing this Algorithm, we can see that\n- It's  gives slight better score \n- Showing the Best Estimator Parametes for final model\n","c188870a":"* LabelEncoder can be used to normalize labels.\n","c72a1298":"### Identify dependent and independent","59ad7c3b":"- Using this method, we can see how many `object(categorical)` type of feature exists in dataset","d9b3d91c":"<a id=\"2\"><\/a><br>\n# 2. Exploring Data Analysis","1b56379b":"- print the hightest accuracy score using sort values","a04531dd":"<a id=\"5\"><\/a><br>\n# ML Model Selecting and Model PredPrediction\n\n\n\n---\n---\n\n#### Model Building","cc4c3f5b":"Select feature for predictions","44721a50":"- To read model from file\n\n```\n# load the model from disk\nloaded_model = pkl.load(open(filename, 'rb')) # rb means read as binary\nresult = loaded_model.score(X_test, Y_test)\n\n```","0d8bbc57":"### Observation \n\nUsing this Algorithm, we can see that\n- A little score improved compared to previous model\n- Showing the Best Estimator Parametes for final model\n- We can see the Best Parametes for KNN Model.","87669aed":"### Observation \n\n\nUsing this Algorithm, we can see that\n- It's  gives slight better score \n- Showing the Best Estimator Parametes for final model\n\n\n---","527d18f4":"#### Find the correlation between other features, mean features only","7d0f0e36":"##### Preprocessing and model selection\n","23bb4b82":"--- \n---\n\n<div class=\"text-center\">\n    <h1>That's it Guys,<\/h1>\n    <h1>\ud83d\ude4f<\/h1>\n    \n        \n        I Hope you guys you like and enjoy it, and learn something interesting things from this notebook, \n        \n        Even I learn a lots of things while I'm creating this notebook\n    \n        Keep Learning,\n        Regards,\n        Vikas Ukani.\n    \n<\/div>\n\n---\n---\n\n<img src=\"https:\/\/static.wixstatic.com\/media\/3592ed_5453a1ea302b4c4588413007ac4fcb93~mv2.gif\" align=\"center\" alt=\"Thank You\" style=\"min-height:20%; max-height:20%\" width=\"90%\" \/>\n\n","bff69eeb":"### Get object features","fabf228f":"Using, Plotly Pacage we can show it in interactive graphs like this,","57bdc990":"Now, we are ready to build our model for prediction, for the I made function for model building and preforming prediction and measure it's prediction and accuracy score.","5f949501":"- Splite the dataset into TrainingSet and TestingSet by 33% and set the 15 fixed records","8fbaaef6":"<a id=\"7\"><\/a><br>\n# 7. Deploy Model\n","e660a04e":"### Feature Selection"}}