{"cell_type":{"b63235c6":"code","918d271d":"code","72ae059e":"code","1aa0b650":"code","2bf96218":"code","a7cd42bf":"code","c51ee178":"code","4a7cb3d4":"code","5fa0e97e":"code","5036f338":"code","e420cd2f":"code","470161dd":"code","2383913a":"code","e312965e":"code","c140c8ee":"code","b3e08f12":"code","c8b53fbb":"code","af0eb893":"code","3dea63e9":"code","de193f39":"code","fbc0d0ce":"code","028c2a33":"code","329974ab":"markdown","0f1b65bf":"markdown","da417f15":"markdown","4229e601":"markdown","cc8e5175":"markdown","ec7099ce":"markdown","50a3abd5":"markdown","28eed4aa":"markdown","b4b1eb81":"markdown","f84ec023":"markdown","ab0ef1f0":"markdown","60934759":"markdown","7f5012fb":"markdown","60430c0d":"markdown","84fd26cd":"markdown","8acb2635":"markdown","91a9763e":"markdown","62149865":"markdown","c790ee72":"markdown","9ad288a8":"markdown","b6617f73":"markdown","d6399b66":"markdown"},"source":{"b63235c6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","918d271d":"train=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntrain.head()","72ae059e":"train.shape","1aa0b650":"train.info()","2bf96218":"train['target'].value_counts()","a7cd42bf":"import seaborn as sns\nsns.countplot(train['target'])","c51ee178":"(train['target'].value_counts()\/train.shape[0])*100","4a7cb3d4":"train.isnull().sum()","5fa0e97e":"train['keyword'].fillna('no_keyword',inplace=True)\ntrain['location'].fillna('no_location',inplace=True)","5036f338":"train.isnull().sum().sum()","e420cd2f":"train['location'].value_counts()[:10]","470161dd":"train['location'].value_counts()[:20].plot(kind='bar')","2383913a":"train['keyword'].value_counts()[:10]","e312965e":"train['keyword'].value_counts()[:20].plot(kind='bar')","c140c8ee":"#List of stopwords in english\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nprint(stopwords.words('english'))","b3e08f12":"import re\nfrom nltk.stem.porter import PorterStemmer","c8b53fbb":"corpus = []\nfor i in range(0,train.shape[0]):\n  review = re.sub('[^a-zA-Z]', ' ',train['text'][i])\n  review = review.lower()\n  review = review.split()\n  ps = PorterStemmer()\n  all_stopwords = stopwords.words('english')\n  review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\n  review = ' '.join(review)\n  corpus.append(review)","af0eb893":"corpus[:5]","3dea63e9":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features = 1000)\nX = cv.fit_transform(corpus).toarray()\nX[:10]","de193f39":"from sklearn.model_selection import train_test_split\ny=train.iloc[:,-1].values\nX_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)","fbc0d0ce":"from sklearn.naive_bayes import MultinomialNB\nclassifier = MultinomialNB()\nclassifier.fit(X_train, Y_train)","028c2a33":"from sklearn.metrics import confusion_matrix, accuracy_score\nprediction=classifier.predict(X_test)\ncm = confusion_matrix(Y_test,prediction)\nprint(cm)\naccuracy_score(Y_test,prediction)","329974ab":"As shown above,we saw the count of target feature value-0,1 and visualised using seaborn countplot,Finally checked the percentage of count.","0f1b65bf":"# Missing Values ","da417f15":"# Please upvote if you like,any suggestions and mistakes put it in comments,Thank you.","4229e601":"# Text Preprocessing","cc8e5175":"# Spliting data into training and testing","ec7099ce":"# Reading the dataset ","50a3abd5":"# Creating bag of words model","28eed4aa":"# Visualising keyword and location features.","b4b1eb81":"As you see,there are missing values in keyword and location features.we now fill those missing values with 'no_keyword' and 'no_location'","f84ec023":"No missing values left.","ab0ef1f0":"# Stop words:\n                A stop word is a commonly used word (such as \u201cthe\u201d, \u201ca\u201d, \u201can\u201d, \u201cin\u201d) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query.We can check list of stopwords as shown below.","60934759":"# Bag of words:\n                            The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity.\n  \n Let\u2019s take an example to understand this concept in depth.\n\n\u201cIt was the best of times\u201d\n\u201cIt was the worst of times\u201d\n\u201cIt was the age of wisdom\u201d\n\u201cIt was the age of foolishness\u201d\n\nWe treat each sentence as a separate document and we make a list of all words from all the four documents excluding the punctuation. We get,\n\n\u2018It\u2019, \u2018was\u2019, \u2018the\u2019, \u2018best\u2019, \u2018of\u2019, \u2018times\u2019, \u2018worst\u2019, \u2018age\u2019, \u2018wisdom\u2019, \u2018foolishness\u2019\n\nThe next step is the create vectors. Vectors convert text that can be used by the machine learning algorithm.\n\nWe take the first document \u2014 \u201cIt was the best of times\u201d and we check the frequency of words from the 10 unique words.\n\u201cit\u201d = 1\n\u201cwas\u201d = 1\n\u201cthe\u201d = 1\n\u201cbest\u201d = 1\n\u201cof\u201d = 1\n\u201ctimes\u201d = 1\n\u201cworst\u201d = 0\n\u201cage\u201d = 0\n\u201cwisdom\u201d = 0\n\u201cfoolishness\u201d = 0\n\nRest of the documents will be:\n\u201cIt was the best of times\u201d = [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n\u201cIt was the worst of times\u201d = [1, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n\u201cIt was the age of wisdom\u201d = [1, 1, 1, 0, 1, 0, 0, 1, 1, 0]\n\u201cIt was the age of foolishness\u201d = [1, 1, 1, 0, 1, 0, 0, 1, 0, 1]","7f5012fb":"# Stemming:\n                    Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma.A stemming algorithm reduces the words \u201cchocolates\u201d, \u201cchocolatey\u201d, \u201cchoco\u201d to the root word, \u201cchocolate\u201d and \u201cretrieval\u201d, \u201cretrieved\u201d, \u201cretrieves\u201d reduce to the stem \u201cretrieve\u201d.","60430c0d":"# Target feature ","84fd26cd":"# Important terms and definitions","8acb2635":"# Tf-IDF(Term Frequency and Inverse Document Frequency):\n                 TF-IDF weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus.\n1.     Term Frequency (TF): is a scoring of the frequency of the word in the current document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. The term frequency is often divided by the document length to normalize.\n                \n                TF=Number of times term t appers in documnet\/total no of terms in document.\n\n2.Inverse Document Frequency (IDF): is a scoring of how rare the word is across documents. IDF is a measure of how rare a term is. Rarer the term, more is the IDF score.                \n                    \n                IDF=log(total no of documents\/no of documents with term t in it)\n                \n                \n                Final score=TF*IDF.\n                  ","91a9763e":"# lemmatization:\n                        Lemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meaning to one word. \n                        Difference between stemming and lemmatization is that lemmatization gives proper meaningful dictionary words.","62149865":"info() gives us the details of datatypes of each feature and null value count.","c790ee72":"# Implementation:\n                        Here i use stemming and bag of words model.Importing nessesary libraries for text preprocessing.\n      1.re-used for regular expressions.\n      2.nltk-natural language tool kit-one of the best library for nlp.\n      3.topwords.\n      4.PorterStemmer(for stemming).","9ad288a8":"#  Naive Bayes model ","b6617f73":"# Descriptive statistics using info() and describe()","d6399b66":"# Code Explanation:\n                1.First created a list called corpus to store all the sentences.\n                2.looping through all the sentences in text feature and perform following steps.\n                3.Replace non alphabets with space.\n                4.Convert everything into lowercase.\n                5.Select all the words apart from stop words and apply stemming.\n                6.Finally join all the reviews and append to corpus.\n     The corpus is as follows           "}}