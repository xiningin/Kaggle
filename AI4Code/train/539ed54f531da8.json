{"cell_type":{"e31ffac6":"code","90f1c030":"code","2268efec":"code","20963d24":"code","55f704b3":"code","e372c4ca":"code","f225462b":"code","72753be1":"code","2d190d12":"code","4b375dab":"code","a4cfd898":"code","d96625ff":"code","f5398a31":"code","50fb7559":"code","2791bd98":"code","d05a2662":"code","fddfdd3d":"code","8cfb48e3":"code","d4e7440c":"code","6dfad457":"markdown","9a944668":"markdown","a966800f":"markdown","3f61a066":"markdown","5bc7f9a5":"markdown","ca4746de":"markdown","052aa8a5":"markdown","b6cb7317":"markdown","79a03d3a":"markdown","85d661e5":"markdown","a41006ff":"markdown","bb058708":"markdown","82c200b7":"markdown","d387f374":"markdown","cb4b5417":"markdown","d3e22ad0":"markdown","74996c4e":"markdown","450b5c4d":"markdown","6bb1a307":"markdown","f313ec18":"markdown"},"source":{"e31ffac6":"# Setup code. Make sure you run this first!\n\nimport os\nimport random\nimport math\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\n\nfrom learntools.core import binder; binder.bind(globals())\nfrom learntools.embeddings.ex1_embedding_layers import *\n\ninput_dir = '..\/input'\n\n# Load a 10% subset of the full MovieLens data.\ndf = pd.read_csv(os.path.join(input_dir, 'mini_rating.csv'))\n\n# Some hyperparameters. (You might want to play with these later)\nLR = .005 # Learning rate\nEPOCHS = 8 # Default number of training epochs (i.e. cycles through the training data)\nhidden_units = (32,4) # Size of our hidden layers\n\ndef build_and_train_model(movie_embedding_size=8, user_embedding_size=8, verbose=2, epochs=EPOCHS):\n    tf.set_random_seed(1); np.random.seed(1); random.seed(1) # Set seeds for reproducibility\n\n    user_id_input = keras.Input(shape=(1,), name='user_id')\n    movie_id_input = keras.Input(shape=(1,), name='movie_id')\n    user_embedded = keras.layers.Embedding(df.userId.max()+1, user_embedding_size, \n                                           input_length=1, name='user_embedding')(user_id_input)\n    movie_embedded = keras.layers.Embedding(df.movieId.max()+1, movie_embedding_size, \n                                            input_length=1, name='movie_embedding')(movie_id_input)\n    concatenated = keras.layers.Concatenate()([user_embedded, movie_embedded])\n    out = keras.layers.Flatten()(concatenated)\n\n    # Add one or more hidden layers\n    for n_hidden in hidden_units:\n        out = keras.layers.Dense(n_hidden, activation='relu')(out)\n\n    # A single output: our predicted rating\n    out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n\n    model = keras.Model(\n        inputs = [user_id_input, movie_id_input],\n        outputs = out,\n    )\n    model.compile(\n        tf.train.AdamOptimizer(LR),\n        loss='MSE',\n        metrics=['MAE'],\n    )\n    history = model.fit(\n        [df.userId, df.movieId],\n        df.y,\n        batch_size=5 * 10**3,\n        epochs=epochs,\n        verbose=verbose,\n        validation_split=.05,\n    )\n    return history\n\n# Train two models with different embedding sizes and save the training statistics.\n# We'll be using this later in the exercise.\nhistory_8 = build_and_train_model(verbose=0)\nhistory_32 = build_and_train_model(32, 32, verbose=0)\n\nprint(\"Setup complete!\")","90f1c030":"# embedding_variables should contain all the variables you would use an embedding layer for\n# For your convenience, we've initialized it with all variables in the dataset, so you can \n# just delete or comment out the variables you want to exclude.\nembedding_variables = {\n    'user_id',\n    'song_id',\n    'artist_id'\n}\npart1.check()","2268efec":"# part1.solution()","20963d24":"history_FS = (15, 5)\ndef plot_history(histories, keys=('mean_absolute_error',), train=True, figsize=history_FS):\n    if isinstance(histories, tf.keras.callbacks.History):\n        histories = [ ('', histories) ]\n    for key in keys:\n        plt.figure(figsize=history_FS)\n        for name, history in histories:\n            val = plt.plot(history.epoch, history.history['val_'+key],\n                           '--', label=str(name).title()+' Val')\n            if train:\n                plt.plot(history.epoch, history.history[key], color=val[0].get_color(), alpha=.5,\n                         label=str(name).title()+' Train')\n\n        plt.xlabel('Epochs')\n        plt.ylabel(key.replace('_',' ').title())\n        plt.legend()\n        plt.title(key)\n\n        plt.xlim([0,max(max(history.epoch) for (_, history) in histories)])\n\nplot_history([ \n    ('base model', history_8),\n])","55f704b3":"plot_history([ \n    ('8-d embeddings', history_8),\n    ('32-d embeddings', history_32),\n])","e372c4ca":"# Example: shrinking movie embeddings and growing user embeddings\nhistory_biguser_smallmovie = build_and_train_model(movie_embedding_size=4, user_embedding_size=16)","f225462b":"part2.solution()","72753be1":"part3.a.solution()","2d190d12":"part3.b.solution()","4b375dab":"user_embedding_size = movie_embedding_size = 8\nuser_id_input = keras.Input(shape=(1,), name='user_id')\nmovie_id_input = keras.Input(shape=(1,), name='movie_id')\nuser_embedded = keras.layers.Embedding(df.userId.max()+1, user_embedding_size, \n                                       input_length=1, name='user_embedding')(user_id_input)\nmovie_embedded = keras.layers.Embedding(df.movieId.max()+1, movie_embedding_size, \n                                        input_length=1, name='movie_embedding')(movie_id_input)\nconcatenated = keras.layers.Concatenate()([user_embedded, movie_embedded])\nout = keras.layers.Flatten()(concatenated)\n\n# Add one or more hidden layers\nfor n_hidden in hidden_units:\n    out = keras.layers.Dense(n_hidden, activation='relu')(out)\n\n# A single output: our predicted rating (before adding bias)\nout = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n\n################################################################################\n############################# YOUR CODE GOES HERE! #############################\n# TODO: you need to create the variable movie_bias. Its value should be the output of calling a layer.\n# I recommend giving the layer that holds your biases a distinctive name (this will help in an upcoming question)\n#movie_bias = \nbias_embedded = keras.layers.Embedding(df.movieId.max()+1, 1, input_length=1, name='bias',\n                                      )(movie_id_input)\nmovie_bias = keras.layers.Flatten()(bias_embedded)\n################################################################################\nout = keras.layers.Add()([out, movie_bias])\n\nmodel_bias = keras.Model(\n    inputs = [user_id_input, movie_id_input],\n    outputs = out,\n)\nmodel_bias.compile(\n    tf.train.AdamOptimizer(LR),\n    loss='MSE',\n    metrics=['MAE'],\n)\nmodel_bias.summary()","a4cfd898":"# part3.c.hint()","d96625ff":"# part3.c.solution()","f5398a31":"history_bias = model_bias.fit(\n    [df.userId, df.movieId],\n    df.y,\n    batch_size=5 * 10**3,\n    epochs=EPOCHS,\n    verbose=2,\n    validation_split=.05,\n);","50fb7559":"plot_history([ \n    ('no_bias', history_8),\n    ('bias', history_bias),\n]);","2791bd98":"bias_layer = model_bias.get_layer('bias')\n\npart3.d.check()\n\n(b,) = bias_layer.get_weights()\nprint(\"Loaded biases with shape {}\".format(b.shape))","d05a2662":"# part3.d.solution()","fddfdd3d":"movies = pd.read_csv(os.path.join(input_dir, 'movie.csv'), index_col=0, \n                     usecols=['movieId', 'title', 'genres', 'year'])\nntrain = math.floor(len(df) * .95)\ndf_train = df.head(ntrain)\n\n# Mapping from original movie ids to canonical ones\nmids = movieId_to_canon = df.groupby('movieId')['movieId_orig'].first()\n# Add bias column\nmovies.loc[mids.values, 'bias'] = b\n# Add columns for number of ratings and average rating\ng = df_train.groupby('movieId_orig')\nmovies.loc[mids.values, 'n_ratings'] = g.size()\nmovies.loc[mids.values, 'mean_rating'] = g['rating'].mean()\n\nmovies.dropna(inplace=True)\n\nmovies.head()","8cfb48e3":"from IPython.display import display\nn = 10\ndisplay(\n    \"Largest biases...\",\n    movies.sort_values(by='bias', ascending=False).head(n),\n    \"Smallest biases...\",\n    movies.sort_values(by='bias').head(n),\n)","d4e7440c":"n = 1000\nmini = movies.sample(n, random_state=1)\n\nfig, ax = plt.subplots(figsize=(13, 7))\nax.scatter(mini['mean_rating'], mini['bias'], alpha=.4)\nax.set_xlabel('Mean rating')\nax.set_ylabel('Bias');","6dfad457":"### Coding up biases","9a944668":"How does it compare to the results we got from the model without biases? Run the code cell below to compare their loss over the course of training.","a966800f":"How did adding biases affect our results?\n\nAveraged over many runs, biases seem to help a bit, but there's enough variance between runs (as a result of different random initializations), that you might be seeing better or worse results. If you're seeing a *big* difference (more than, say, +-.02 in the final loss) in either direction, something may have gone wrong.\n\nSo biases weren't the huge win we might have hoped for, but it still seems worth testing our hypothesis about how bias values will be distributed among movies.","3f61a066":"Once you've successfully set the value of `bias_layer`, run the cell below which loads a dataframe containing movie metadata and adds the biases found in the previous step as a column.","5bc7f9a5":"Run the cell below to generate a scatter plot of movies' average ratings against the biases learned for those movies.","ca4746de":"Do you have an idea about what the bias values will look like? Are there certain movies you expect will have high or low biases?","052aa8a5":"## Part 2: Choosing embedding sizes\n\nIn the [tutorial](https:\/\/www.kaggle.com\/colinmorris\/embedding-layers), we (somewhat arbitrarily) set `output_dim=8` when creating our movie and user embedding layers. Run the code cell below to see a visualization of our training and validation error over 8 epochs of training, again using 8-dimensional embeddings.","b6cb7317":"If you're feeling experimental, feel free to try some other configurations. So far we've varied movie and user embedding size in lock step, but there's no reason they have to be the same. Do you have an intuition about whether one or the other should be bigger?","79a03d3a":"### Training\n\nRun the code below to train the model you built in the previous step. (If you get an error when trying to fit your model, you may have made a mistake in your bias implementation. See `part3.c.solution()` for a working implementation.)","85d661e5":"No idea where to start? Don't panic! Uncomment and run the line below for some hints.","a41006ff":"Below is the code we used to train our embedding model in the tutorial. Modify it (where indicated by the comments) to add per-movie biases.","bb058708":"# Exercise (embeddings)\n\nIn this exercise you'll answer some questions about embeddings and experiment with a couple variations on the movie rating prediction model we trained in the [tutorial](https:\/\/www.kaggle.com\/colinmorris\/embedding-layers).\n\nRun the cell below to do some setup. (It will take a minute or two to run, since we're training a couple models for use later in the exercise. While it's running, you can jump ahead and start on part 1.)\n\n> *Aside*: Training the model in the tutorial took around a minute per epoch. To make training time more manageable, we'll be working with a sample of 10% of the total dataset (or around 2 million ratings). We've also limited our sample to relatively popular movies (movies with more than 1k ratings in the full dataset). For these reasons, our results in this exercise won't be directly comparable to the error rates reported in the tutorial.","82c200b7":"Which movies have the lowest and highest learned biases? Run the cell below to find out.","d387f374":"Considering this plot and the list of our highest and lowest bias movies, do our model's learned biases agree with what you expected?","cb4b5417":"## Part 1: When to use embeddings?\n\nYou're a data scientist for the hot new music streaming service, Tidify All Access\u2122 with the task of building a machine learning model to generate suggested songs to show users. You have lots of historical data about songs that users have listened to in the past. The dataset of 500 million streams includes the following variables:\n\n| Variable name | Description                                       | Number of unique values | Example values\n|---------------|---------------------------------------------------|------|----------------------------------\n| stream_id     | Unique id for this streaming event (i.e. per row) | 500m | 480744481269, 228441807745, 182969356277, ...\n| user_id       | Unique id for this user                           | 5m   | 3592022173, 2596402742, 3506743568, ...\n| song_id       | Unique id for this song                           | 10m  | 3150630225,  590655137, 3617674674, ...\n| timestamp     | When did the song start playing?                  | 80m  | 10\/08\/2017 01:20:44 PM, 04\/22\/2017 01:58:59 PM, ...\n| artist_id     | Unique id for the recording artist of this song   | 1m   | 122168143,  52958907, 803608525, ...\n| song_duration | How long is the song (in seconds)?                | 450  | 257, 155, 212...\n| explicit      | Is this song flagged as having adult language?    | 2    | True, False\n| user_country  | Where is this user located? (3-letter ISO country code) | 150  | CAN, KOR, CHE...\n\nIf you wanted to train a neural net on streaming data, which of the variables above would you include and use use an embedding layer for? Use the variable `embedding_variables` below to give your answer.\n\n(You can assume that, if necessary, string variables can be converted to unique numerical identifiers as a preprocessing step.)","d3e22ad0":"At the start of the notebook we also trained a model with 64-dimensional movie and user embeddings. How do you expect the results to compare? Make a prediction, then run the cell below to see.","74996c4e":"## Part 3: Adding biases\n\nIn this final section, you'll implement a modification to our model's architecture: per-movie biases.\n\nIn ML-speak, a **bias** is just a number that gets added to a node's output value. For each movie, we'll learn a single number that we'll add to the output of what was previously our final node. Here's what that looks like:\n\n<img src=\"https:\/\/docs.google.com\/a\/google.com\/drawings\/d\/e\/2PACX-1vSmf5H7Rcr771flhidl7Wf31OXiZTUgNH2qzoVc-2dtH6Cf9XmSF3xQcY7m1RwCRBu2_lE-dH5Vb6ny\/pub?w=1050&h=594\" \/>\n\n### Why?\n\nDo you think this will improve the accuracy of our predictions? Why or why not?\n\nThink about it, then uncomment the line below to see an explanation.","450b5c4d":"When you're ready, uncomment the cell below for some explanation of what's going on.","6bb1a307":"\n---\nThat's the end of this exercise. How'd it go? If you have any questions, be sure to post them on the [forums](https:\/\/www.kaggle.com\/learn-forum).\n\n**P.S.** This course is still in beta, so I'd love to get your feedback. If you have a moment to [fill out a super-short survey about this exercise](https:\/\/form.jotform.com\/82826270084256), I'd greatly appreciate it.\n\n# Keep going\n\nWhen you're ready to continue, [click here](https:\/\/www.kaggle.com\/colinmorris\/matrix-factorization) to continue on to the next tutorial on matrix factorization.\n","f313ec18":"### Inspecting learned bias values\n\nLet's take a look at the bias values our model has learned. Fill in the missing code in the cell below to load an array of bias values - `b` should be an array with one number for each movie in our training set.\n\n**Hint:** you may want to check out the docs for [`keras.Model.get_layer`](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/Model#get_layer). This will be easier if you gave your `Embedding` bias layer a distinctive name in part 2. If you didn't, it may help to look at the output of `model_bias.summary()`."}}