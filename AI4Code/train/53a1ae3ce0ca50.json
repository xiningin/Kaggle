{"cell_type":{"7b1dac11":"code","4727f517":"code","9ee671ea":"code","e6e72cb6":"code","1f2312b2":"code","40856bf4":"code","e120d1d5":"code","377ccc31":"code","a9e819cf":"code","1bbc769e":"code","6080fd9a":"code","020aef62":"code","08a60b8a":"code","71041c58":"code","36617d33":"code","3dcef11b":"markdown","06a8219f":"markdown","7f06cedc":"markdown","56d86c6e":"markdown","fb938709":"markdown","5f5de5b2":"markdown","8930a49d":"markdown","b779333b":"markdown","50b95201":"markdown","3fbbd5fa":"markdown","d7f2dd39":"markdown","b33066a8":"markdown","ae3d8c61":"markdown","2cb23360":"markdown","38d0c120":"markdown"},"source":{"7b1dac11":"import numpy as np\nimport os\nimport time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\nfrom datetime import datetime\n\n#For faster compute\nfrom numba import jit, cuda\nfrom numba import float64, int64, prange\n\n#For Data Processing\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.autograd import Variable\nfrom torch.utils.data.dataset import Dataset\nfrom torch.utils.data.sampler import SubsetRandomSampler","4727f517":"files = []\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        files.append(os.path.join(dirname, filename))\nfiles.sort()\ntrain_files = files[3:4]\ntest_files = files[1:6]\nval_files = files[-1]","9ee671ea":"def read_files(folder):\n    df = [pd.read_csv(f) for f in folder]\n    big_frame = pd.concat(df, ignore_index=True)\n    big_frame = big_frame[big_frame.symbol =='XBTUSD']\n    big_frame['timestamp'] = big_frame.timestamp.map(lambda t: datetime.strptime(t[:-3], '%Y-%m-%dD%H:%M:%S.%f'))\n    return big_frame","e6e72cb6":"train_df = read_files(train_files)\ntrain_df.head()","1f2312b2":"'''\nVolume weighted average price: tracked for specific interval\n'''\ndef compute_vwap(df):\n    weights = df['foreignNotional']\n    prices = df['price']\n    vwap = np.sum(weights*prices) \/\/ np.sum(weights)\n    df['vwap'] = vwap\n    return df\n\n","40856bf4":"#Check on train data\ndata = train_df\n\ndata_timeidx = data.set_index('timestamp')\ndata_time_grp = data_timeidx.groupby(pd.Grouper(freq='15Min'))\nnum_time_bars = len(data_time_grp) \ndata_time_vwap = data_time_grp.apply(compute_vwap)\ndata_time_vwap['vwap'].plot(label='Time', figsize=(15,5))","e120d1d5":"total_ticks = len(data)\n#round to the nearest thousand\nnum_ticks_per_bar = round(total_ticks \/ num_time_bars, -3)\ndata_tick_grp = data.reset_index().assign(grpId=lambda row: row.index \/\/ num_ticks_per_bar)\ndata_tick_vwap =  data_tick_grp.groupby('grpId').apply(compute_vwap)\ndata_tick_vwap.set_index('timestamp', inplace=True)\ndata_time_vwap['vwap'].plot(label='Time', figsize=(15,5))\ndata_tick_vwap['vwap'].plot(label='Tick', figsize=(15,5))\nplt.legend();","377ccc31":"data_cm_vol = data.assign(cmVol=data['homeNotional'].cumsum()) \ntotal_vol = data_cm_vol.cmVol.values[-1]\nvol_per_bar = total_vol \/ num_time_bars\nvol_per_bar = round(vol_per_bar, -3) # round to the nearest thousand\ndata_vol_grp = data_cm_vol.assign(grpId=lambda row: row.cmVol \/\/ vol_per_bar)\ndata_vol_vwap =  data_vol_grp.groupby('grpId').apply(compute_vwap)\ndata_vol_vwap.set_index('timestamp', inplace=True)\ndata_time_vwap['vwap'].plot(label='Time', figsize=(15,5))\ndata_vol_vwap['vwap'].plot(label='Tick', figsize=(15,5))","a9e819cf":"data_cm_dol = data.assign(cmVol=data['foreignNotional'].cumsum()) \ntotal_dol = data_cm_vol.cmVol.values[-1]\ndol_per_bar = total_vol \/ num_time_bars\ndol_per_bar = round(vol_per_bar, -3) # round to the nearest thousand\ndata_dol_grp = data_cm_vol.assign(grpId=lambda row: row.cmVol \/\/ vol_per_bar)\ndata_dol_vwap =  data_vol_grp.groupby('grpId').apply(compute_vwap)\ndata_dol_vwap.set_index('timestamp', inplace=True)\ndata_time_vwap['vwap'].plot(label='Time', figsize=(15,5))\ndata_dol_vwap['vwap'].plot(label='Tick', figsize=(15,5))","1bbc769e":"def convert_tick_direction(tick_direction):\n    if tick_direction in ('PlusTick', 'ZeroPlusTick'):\n        return 1\n    elif tick_direction in ('MinusTick', 'ZeroMinusTick'):\n        return -1\n    else:\n        raise ValueError('converting invalid input: '+ str(tick_direction))\ndata_timeidx['tickDirection'] = data_timeidx.tickDirection.map(convert_tick_direction)","6080fd9a":"data_signed_flow = data_timeidx.assign(bv = data_timeidx.tickDirection * data_timeidx.foreignNotional)","020aef62":"@jit((float64[:], int64), nopython=True, nogil=True,) #compiler options for function input and its runtime feature\ndef _ewma(arr_in, window):\n    n = arr_in.shape[0]\n    ewma = np.empty(n, dtype=float64)\n    alpha = 2 \/ float(window + 1)\n    w = 1\n    ewma_old = arr_in[0]\n    ewma[0] = ewma_old\n    for i in prange(1, n):\n        w += (1-alpha)**i\n        ewma_old = ewma_old*(1-alpha) + arr_in[i]\n        ewma[i] = ewma_old \/ w\n    return ewma","08a60b8a":"abs_Ebv_init = np.abs(data_signed_flow['bv'].mean()) #Use the mean value as its close to expected bv\nE_T_init = 500000 # 500000 ticks to warm up\n\ndef compute_Ts(bvs, E_T_init, abs_Ebv_init):\n    #Tickets and their positions\n    Ts, i_s = [], []\n    i_prev, E_T, abs_Ebv  = 0, E_T_init, abs_Ebv_init\n    \n    n = bvs.shape[0]\n    bvs_val = bvs.values.astype(np.float64)\n    abs_thetas, thresholds = np.zeros(n), np.zeros(n)\n    abs_thetas[0], cur_theta = np.abs(bvs_val[0]), bvs_val[0]\n    for i in prange(1, n):\n        cur_theta += bvs_val[i]\n        abs_theta = np.abs(cur_theta)\n        abs_thetas[i] = abs_theta\n        \n        threshold = E_T * abs_Ebv\n        thresholds[i] = threshold\n        if abs_theta >= threshold:\n            cur_theta = 0\n            #Tick length as distance b\/w bar indexes\n            Ts.append(np.float64(i - i_prev)) \n            i_s.append(i)\n            i_prev = i\n\n            E_T = _ewma(np.array(Ts), window=np.int64(len(Ts)))[-1]\n            abs_Ebv = np.abs( _ewma(bvs_val[:i], window=np.int64(E_T_init * 3))[-1] ) # window of 3 bars\n    return Ts, abs_thetas, thresholds, i_s\nTs, abs_thetas, thresholds, i_s = compute_Ts(data_signed_flow.bv, E_T_init, abs_Ebv_init)","71041c58":"n = data_signed_flow.shape[0]\ni_iter = iter(i_s + [n])\ni_cur = i_iter.__next__()\ngrpId = np.zeros(n)\nfor i in range(1, n):\n    if i <= i_cur:\n        grpId[i] = grpId[i-1]\n    else:\n        grpId[i] = grpId[i-1] + 1\n        i_cur = i_iter.__next__()","36617d33":"data_dollar_imb_grp = data_signed_flow.assign(grpId = grpId)\ndata_dollar_imb_vwap = data_dollar_imb_grp.groupby('grpId').apply(compute_vwap)\ndata_time_vwap['vwap'].plot(label='Time', figsize=(15,5))\ndata_dollar_imb_vwap['vwap'].plot(label='Tick', figsize=(15,5))\nplt.legend();","3dcef11b":"### Dollar Bars","06a8219f":"#### Aggregate the ticks into groups","7f06cedc":"Now the expected values are calculated using ewma but general ewma implementation of pandas is quite slow([reason](https:\/\/stackoverflow.com\/questions\/42869495\/numpy-version-of-exponential-weighted-moving-average-equivalent-to-pandas-ewm\/51392341#51392341)) for which we are using the above numba implementation","56d86c6e":"#### Accumulate the imbalance bars","fb938709":"### Volume Bars","5f5de5b2":"## Import libraries","8930a49d":"#### Calculate the imbalance bars","b779333b":"#### Notice a few spikes around 03-02 00 and 03-03 12. These indicate that there is large exchange in value at these points.","50b95201":"### Imbalance bars\nThe idea here is to sample when ever there are is large imbalance in the tick directions either in volume or in dollar i.e price. We wish to determine the tick index, T, such that the accumulation of signed ticks (signed according to the tick rule) exceeds a given threshold.\n<br>\nAlogrithm:\n<br>Sample bar when:\n|Imbalance| \u2265 Expected imbalance\n<br>Expected imbalance = (Expected # of ticks per bar) * |Expected imbalance per tick|","3fbbd5fa":"## Data Loading","d7f2dd39":"### Volume averaged price","b33066a8":"#### Compute the signed flow","ae3d8c61":"### Time Bars","2cb23360":"### Tick Bars","38d0c120":"### Train-test-val split"}}