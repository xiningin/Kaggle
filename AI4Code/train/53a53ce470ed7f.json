{"cell_type":{"a20dddba":"code","d38cf6ec":"code","3191ba34":"code","e1edba61":"code","de05e55a":"code","5f36ae97":"code","a4f714b7":"code","7b84e96a":"code","1cdcef35":"code","506239ca":"code","f4ad41e8":"code","acc2e98f":"code","5cf8d0d5":"code","c7e1c4c0":"code","bcad42c3":"code","07777ce3":"code","4d20885e":"code","1e0abc5e":"code","06c47d80":"code","810ad348":"markdown","34f481c7":"markdown","1d8a82d8":"markdown","5ac6fd11":"markdown","5d7983e4":"markdown","43240d4e":"markdown","14699be0":"markdown","5f653052":"markdown","6b86dc4d":"markdown","a64e1929":"markdown","aeaf9ebe":"markdown","20b8859b":"markdown"},"source":{"a20dddba":"import numpy as np\nimport pandas as pd\nimport os, time, gc, pickle, random\nfrom tqdm._tqdm_notebook import tqdm_notebook as tqdm\nfrom keras.preprocessing import text, sequence\nimport torch\nfrom torch import nn\nfrom torch.utils import data\nfrom torch.nn import functional as F","d38cf6ec":"# disable progress bars when submitting\ndef is_interactive():\n    return 'SHLVL' not in os.environ\n\nif not is_interactive():\n    def nop(it, *a, **k):\n        return it\n\n    tqdm = nop","3191ba34":"def seed_everything(seed=1234):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything()","e1edba61":"CRAWL_EMBEDDING_PATH = '..\/input\/pickled-crawl300d2m-for-kernel-competitions\/crawl-300d-2M.pkl'\nGLOVE_EMBEDDING_PATH = '..\/input\/pickled-glove840b300d-for-10sec-loading\/glove.840B.300d.pkl'\n\nNUM_MODELS = 2\nLSTM_UNITS = 128\nDENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\nMAX_LEN = 220","de05e55a":"def build_matrix(word_index, emb_path, unknown_token='unknown'):\n    with open(emb_path, 'rb') as fp:\n        embedding_index = pickle.load(fp)\n    \n    # TODO: Build random token instead of using unknown\n    unknown_token = embedding_index[unknown_token].copy()\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    unknown_words = []\n    \n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word].copy()\n        except KeyError:\n            embedding_matrix[i] = unknown_token\n            unknown_words.append(word)\n            \n    del embedding_index; gc.collect()\n    return embedding_matrix, unknown_words","5f36ae97":"class SpatialDropout(nn.Dropout2d):\n    def forward(self, x):\n        x = x.unsqueeze(2)    # (N, T, 1, K)\n        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n        x = x.squeeze(2)  # (N, T, K)\n        return x\n    \nclass NeuralNet(nn.Module):\n    def __init__(self, embedding_matrix, num_aux_targets):\n        super(NeuralNet, self).__init__()\n        embed_size = embedding_matrix.shape[1]\n        \n        self.embedding = nn.Embedding(max_features, embed_size)\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n        self.embedding_dropout = SpatialDropout(0.3)\n        \n        self.lstm1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n        self.lstm2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n    \n        self.linear1 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n        self.linear2 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n        \n        self.linear_out = nn.Linear(DENSE_HIDDEN_UNITS, 1)\n        self.linear_aux_out = nn.Linear(DENSE_HIDDEN_UNITS, num_aux_targets)\n        \n    def forward(self, x):\n        h_embedding = self.embedding(x)\n        h_embedding = self.embedding_dropout(h_embedding)\n        \n        h_lstm1, _ = self.lstm1(h_embedding)\n        h_lstm2, _ = self.lstm2(h_lstm1)\n        \n        # global average pooling\n        avg_pool = torch.mean(h_lstm2, 1)\n        # global max pooling\n        max_pool, _ = torch.max(h_lstm2, 1)\n        \n        h_conc = torch.cat((max_pool, avg_pool), 1)\n        h_conc_linear1  = F.relu(self.linear1(h_conc))\n        h_conc_linear2  = F.relu(self.linear2(h_conc))\n        \n        hidden = h_conc + h_conc_linear1 + h_conc_linear2\n        \n        result = self.linear_out(hidden)\n        aux_result = self.linear_aux_out(hidden)\n        out = torch.cat([result, aux_result], 1)\n        \n        return out","a4f714b7":"def preprocess(data):\n    '''\n    Credit goes to https:\/\/www.kaggle.com\/gpreda\/jigsaw-fast-compact-solution\n    '''\n    punct = \"\/-'?!.,#$%\\'()*+-\/:;<=>@[\\\\]^_`{|}~`\" + '\"\"\u201c\u201d\u2019' + '\u221e\u03b8\u00f7\u03b1\u2022\u00e0\u2212\u03b2\u2205\u00b3\u03c0\u2018\u20b9\u00b4\u00b0\u00a3\u20ac\\\u00d7\u2122\u221a\u00b2\u2014\u2013&'\n    def clean_special_chars(text, punct):\n        for p in punct:\n            text = text.replace(p, ' ')\n        return text\n\n    data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))\n    return data","7b84e96a":"train = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv')\ntest = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv')\n\nx_test = preprocess(test['comment_text'])\nx_train = preprocess(train['comment_text'])\n\ny_train = np.where(train['target'] >= 0.5, 1, 0)\ny_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']].values","1cdcef35":"tokenizer = text.Tokenizer()\ntokenizer.fit_on_texts(list(x_train) + list(x_test))\n\nx_train = tokenizer.texts_to_sequences(x_train)\nx_test  = tokenizer.texts_to_sequences(x_test)\nx_train_lens = [len(i) for i in x_train]\nx_test_lens  = [len(i) for i in x_test]","506239ca":"max_features = None\nmax_features = max_features or len(tokenizer.word_index) + 1\nmax_features","f4ad41e8":"crawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, emb_path=CRAWL_EMBEDDING_PATH, unknown_token='unknown')\nprint('n unknown words (crawl): ', len(unknown_words_crawl))","acc2e98f":"glove_matrix, unknown_words_glove = build_matrix(tokenizer.word_index, emb_path=GLOVE_EMBEDDING_PATH, unknown_token='unknown')\nprint('n unknown words (glove): ', len(unknown_words_glove))","5cf8d0d5":"embedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis=-1)\nembedding_matrix.shape\n\ndel crawl_matrix\ndel glove_matrix\ngc.collect()","c7e1c4c0":"class TextDataset(data.Dataset):\n    def __init__(self, text, lens, y=None):\n        self.text = text\n        self.lens = lens\n        self.y = y\n\n    def __len__(self):\n        return len(self.lens)\n\n    def __getitem__(self, idx):\n        if self.y is None:\n            return self.text[idx], self.lens[idx]\n        return self.text[idx], self.lens[idx], self.y[idx]","bcad42c3":"class Collator(object):\n    def __init__(self,test=False,percentile=100):\n        self.test = test\n        self.percentile = percentile\n        \n    def __call__(self, batch):\n        global MAX_LEN\n        \n        if self.test:\n            texts, lens = zip(*batch)\n        else:\n            texts, lens, target = zip(*batch)\n\n        lens = np.array(lens)\n        max_len = min(int(np.percentile(lens, self.percentile)), MAX_LEN)\n        texts = torch.tensor(sequence.pad_sequences(texts, maxlen=max_len), dtype=torch.long).cuda()\n        \n        if self.test:\n            return texts\n        \n        return texts, torch.tensor(target, dtype=torch.float32).cuda()","07777ce3":"final_y_train = np.hstack([y_train[:, np.newaxis], y_aux_train])\n\ntrain_collate = Collator(percentile=96)\ntrain_dataset = TextDataset(x_train, x_train_lens, final_y_train)\ntrain_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=512, shuffle=True, collate_fn=train_collate)\n\ntest_collate = Collator(test=True)\ntest_dataset = TextDataset(x_test, x_test_lens)\ntest_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=512, shuffle=False , collate_fn=test_collate)\n\n# del y_train, y_aux_train; gc.collect()","4d20885e":"def sigmoid(x):\n    return 1 \/ (1 + np.exp(-x))\n\ndef train_model(model, train_loader, test_loader, loss_fn, output_dim, lr=0.001,\n                batch_size=512, n_epochs=4,\n                enable_checkpoint_ensemble=True):\n    param_lrs = [{'params': param, 'lr': lr} for param in model.parameters()]\n    optimizer = torch.optim.Adam(param_lrs, lr=lr)\n\n    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.6 ** epoch)\n    \n    all_test_preds = []\n    checkpoint_weights = [2 ** epoch for epoch in range(n_epochs)]\n    \n    for epoch in range(n_epochs):\n        start_time = time.time()\n        \n        scheduler.step()\n        \n        model.train()\n        avg_loss = 0.\n        \n        for step, (seq_batch, y_batch) in enumerate(tqdm(train_loader, disable=False)):\n            y_pred = model(seq_batch)            \n            loss = loss_fn(y_pred, y_batch)\n\n            optimizer.zero_grad()\n            loss.backward()\n\n            optimizer.step()\n            avg_loss += loss.item() #\/ len(train_loader)\n            \n            if step > 0 and step % 100 == 0:\n                print(step, avg_loss \/ step)\n            \n        model.eval()\n        test_preds = np.zeros((len(test), output_dim))\n    \n        for step, seq_batch in enumerate(test_loader):\n            y_pred = sigmoid(model(seq_batch).detach().cpu().numpy())\n            test_preds[step * batch_size:step * batch_size + y_pred.shape[0], :] = y_pred[:,:1]\n\n        all_test_preds.append(test_preds)\n        elapsed_time = time.time() - start_time\n        print('Epoch {}\/{} \\t loss={:.4f} \\t time={:.2f}s'.format(\n              epoch + 1, n_epochs, avg_loss \/ len(train_loader), elapsed_time))\n\n    if enable_checkpoint_ensemble:\n        test_preds = np.average(all_test_preds, weights=checkpoint_weights, axis=0)    \n    else:\n        test_preds = all_test_preds[-1]\n        \n    return test_preds","1e0abc5e":"aux_size = final_y_train.shape[-1] - 1  # targets\n\nall_test_preds = []\nfor model_idx in range(NUM_MODELS):\n    print('Model ', model_idx)\n    seed_everything(1234 + model_idx)\n    \n    model = NeuralNet(embedding_matrix, aux_size)\n    model.cuda()\n    \n    test_preds = train_model(model, train_loader, test_loader, output_dim=1, loss_fn=nn.BCEWithLogitsLoss(reduction='mean'))\n    all_test_preds.append(test_preds)\n    print()","06c47d80":"submission = pd.DataFrame.from_dict({\n    'id': test['id'],\n    'prediction': np.mean(all_test_preds, axis=0)[:, 0]\n})\n\nsubmission.to_csv('submission.csv', index=False)","810ad348":"# Ways to improve this kernel","34f481c7":"# Training","1d8a82d8":"This kernel is just a simple baseline kernel, so there are many ways to improve it. Some ideas to get you started:\n- ~~Use bucketing to train faster and fit more networks into the two hours. The winning team of the quora competition successfully used bucketing to drastically reduce the time it took to train RNNs. An excerpt from their [solution summary](https:\/\/www.kaggle.com\/c\/quora-insincere-questions-classification\/discussion\/80568#latest-487092):~~\n- Add a contraction mapping. E. g. mapping \"is'nt\" to \"is not\" can help the network because \"not\" is explicitly mentioned. They were very popular in the recent quora competition, see for example [this kernel](https:\/\/www.kaggle.com\/theoviel\/improve-your-score-with-some-text-preprocessing).\n- Try to reduce the number of words that are not found in the embeddings. At the moment, around 170k words are not found. We can take some steps to decrease this amount, for example trying to find a vector for a processed (capitalized, stemmed, ...) version of the word when the vector for the regular word can not be found. See the [3rd place solution](https:\/\/www.kaggle.com\/wowfattie\/3rd-place) of the quora competition for an excellent implementation of this.\n- Try cyclic learning rate (CLR). I have found CLR to almost always improve my network recently compared to the default parameters for Adam. In this case, we are already using a learning rate scheduler, so this might not be the case. But it is still worth to try it out. See for example my [my other PyTorch kernel](https:\/\/www.kaggle.com\/bminixhofer\/deterministic-neural-networks-using-pytorch) for an implementation of CLR in PyTorch.\n\n> We aimed at combining as many models as possible. To do this, we needed to improve runtime and the most important thing to achieve this was the following. We do not pad sequences to the same length based on the whole data, but just on a batch level. That means we conduct padding and truncation on the data generator level for each batch separately, so that length of the sentences in a batch can vary in size. Additionally, we further improved this by not truncating based on the length of the longest sequence in the batch, but based on the 95% percentile of lengths within the sequence. This improved runtime heavily and kept accuracy quite robust on single model level, and improved it by being able to average more models.\n\n- Try a (weighted) average of embeddings instead of concatenating them. A 600d vector for each word is a lot, it might work better to average them instead. See [this paper](https:\/\/www.aclweb.org\/anthology\/N18-2031) for why this even works.\n- Limit the maximum number of words used to train the NN. At the moment, there is no limit set to the maximum number of words in the tokenizer, so we use every word that occurs in the training data, even if it is only mentioned once. This could lead to overfitting so it might be better to limit the maximum number of words to e. g. 100k.\n\nThanks for reading. Good luck and have fun in this competition!","5ac6fd11":"This kernel is a PyTorch version of the [Simple LSTM kernel](https:\/\/www.kaggle.com\/thousandvoices\/simple-lstm). All credit for architecture and preprocessing goes to @thousandvoices.\nThere is a lot of discussion whether Keras, PyTorch, Tensorflow or the CUDA C API is best. But specifically between the PyTorch and Keras version of the simple LSTM architecture, there are 2 clear advantages of PyTorch:\n- Speed. The PyTorch version runs about 20 minutes faster.\n- Determinism. The PyTorch version is fully deterministic. Especially when it gets harder to improve your score later in the competition, determinism is very important.\n\nI was surprised to see that PyTorch is that much faster, so I'm not completely sure the steps taken are exactly the same. If you see any difference, we can discuss it in the comments :)\n\nThe most likely reason the score of this kernel is higher than the @thousandvoices version is that the optimizer is not reinitialized after every epoch and thus the parameter-specific learning rates of Adam are not discarded after every epoch. That is the only difference between the kernels that is intended.","5d7983e4":"Happy Kaggling!","43240d4e":"# Imports & Utility functions","14699be0":"# Preprocessing","5f653052":"Hey all. This is a for of @bminixhofer Kernel. My only addition is to demonstrate the use variable batch size for accelerated training times, and of course I use my picked embeddings which load faster and help with memory management.","6b86dc4d":"# Forward","a64e1929":"Note that the solution is not validated in this kernel. So for tuning anything, you should build a validation framework using e. g. KFold CV. If you just check what works best by submitting, you are very likely to overfit to the public LB.","aeaf9ebe":"# Preface","20b8859b":"# Batching"}}