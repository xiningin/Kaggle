{"cell_type":{"a43da95c":"code","7d3a2ce8":"code","9b6b4f02":"code","44866cb4":"code","dc23d364":"code","0d4f4937":"code","5c550e37":"code","1ed16fd6":"code","36e6b0c4":"code","702d77fe":"code","ab5880ed":"code","f746fc0b":"code","919320f7":"code","fec658e2":"code","03d4667c":"code","1bbab9c7":"code","a892dbdd":"code","d3ea5f8a":"code","782cf1e2":"code","4ca59398":"code","05e42d2a":"code","3548f373":"code","2f16e720":"code","ace68d75":"code","5b5003e9":"code","60091cd0":"code","dd7c4b34":"code","c60ec097":"code","ea048a32":"code","731c0001":"code","068c783b":"code","d1bb0e20":"code","d83fb185":"code","db0ef487":"code","8cc492f2":"code","3936e738":"code","717cc045":"markdown","cf7e653d":"markdown","1f72791f":"markdown","9b67ed3b":"markdown","c69317e4":"markdown","73b66fad":"markdown","306ee6b2":"markdown","f6532052":"markdown","69cf6cad":"markdown","724f10e8":"markdown","c080a192":"markdown","06304c3b":"markdown","e4f88f68":"markdown","e67895f3":"markdown","3a46a972":"markdown","6c810f72":"markdown","d5b156a5":"markdown","2b732fe0":"markdown","f5364b53":"markdown","2426c617":"markdown","f4b37db8":"markdown","ce4f798e":"markdown","980504e2":"markdown","fda591fa":"markdown","ca1783c5":"markdown","2bdd0513":"markdown","c43f7112":"markdown","04617493":"markdown"},"source":{"a43da95c":"import math\nimport os\nimport shutil\nimport sys\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport glob\nimport pydicom\nimport cv2\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm","7d3a2ce8":"random_stat = 123\nnp.random.seed(random_stat)","9b6b4f02":"!git clone https:\/\/github.com\/pjreddie\/darknet.git\n\n# Build gpu version darknet\n!cd darknet && sed '1 s\/^.*$\/GPU=1\/; 2 s\/^.*$\/CUDNN=1\/' -i Makefile\n\n# -j <The # of cpu cores to use>. Chang 999 to fit your environment. Actually i used '-j 50'.\n!cd darknet && make -j 50 -s\n!cp darknet\/darknet darknet_gpu","44866cb4":"DATA_DIR = \"..\/input\"\n\ntrain_dcm_dir = os.path.join(DATA_DIR, \"stage_2_train_images\")\ntest_dcm_dir = os.path.join(DATA_DIR, \"stage_2_test_images\")\n\nimg_dir = os.path.join(os.getcwd(), \"images\")  # .jpg\nlabel_dir = os.path.join(os.getcwd(), \"labels\")  # .txt\nmetadata_dir = os.path.join(os.getcwd(), \"metadata\") # .txt\n\n# YOLOv3 config file directory\ncfg_dir = os.path.join(os.getcwd(), \"cfg\")\n# YOLOv3 training checkpoints will be saved here\nbackup_dir = os.path.join(os.getcwd(), \"backup\")\n\nfor directory in [img_dir, label_dir, metadata_dir, cfg_dir, backup_dir]:\n    if os.path.isdir(directory):\n        continue\n    os.mkdir(directory)","dc23d364":"!ls -shtl","0d4f4937":"annots = pd.read_csv(os.path.join(DATA_DIR, \"stage_2_train_labels.csv\"))\nannots.head()","5c550e37":"def save_img_from_dcm(dcm_dir, img_dir, patient_id):\n    img_fp = os.path.join(img_dir, \"{}.jpg\".format(patient_id))\n    if os.path.exists(img_fp):\n        return\n    dcm_fp = os.path.join(dcm_dir, \"{}.dcm\".format(patient_id))\n    img_1ch = pydicom.read_file(dcm_fp).pixel_array\n    img_3ch = np.stack([img_1ch]*3, -1)\n\n    img_fp = os.path.join(img_dir, \"{}.jpg\".format(patient_id))\n    cv2.imwrite(img_fp, img_3ch)\n    \ndef save_label_from_dcm(label_dir, patient_id, row=None):\n    # rsna defualt image size\n    img_size = 1024\n    label_fp = os.path.join(label_dir, \"{}.txt\".format(patient_id))\n    \n    f = open(label_fp, \"a\")\n    if row is None:\n        f.close()\n        return\n\n    top_left_x = row[1]\n    top_left_y = row[2]\n    w = row[3]\n    h = row[4]\n    \n    # 'r' means relative. 'c' means center.\n    rx = top_left_x\/img_size\n    ry = top_left_y\/img_size\n    rw = w\/img_size\n    rh = h\/img_size\n    rcx = rx+rw\/2\n    rcy = ry+rh\/2\n    \n    line = \"{} {} {} {} {}\\n\".format(0, rcx, rcy, rw, rh)\n    \n    f.write(line)\n    f.close()\n        \ndef save_yolov3_data_from_rsna(dcm_dir, img_dir, label_dir, annots):\n    for row in tqdm(annots.values):\n        patient_id = row[0]\n\n        img_fp = os.path.join(img_dir, \"{}.jpg\".format(patient_id))\n        if os.path.exists(img_fp):\n            save_label_from_dcm(label_dir, patient_id, row)\n            continue\n\n        target = row[5]\n        # Since kaggle kernel have samll volume (5GB ?), I didn't contain files with no bbox here.\n        if target == 0:\n            continue\n        save_label_from_dcm(label_dir, patient_id, row)\n        save_img_from_dcm(dcm_dir, img_dir, patient_id)","1ed16fd6":"save_yolov3_data_from_rsna(train_dcm_dir, img_dir, label_dir, annots)","36e6b0c4":"!du -sh images labels","702d77fe":"ex_patient_id = annots[annots.Target == 1].patientId.values[10]\nex_img_path = os.path.join(img_dir, \"{}.jpg\".format(ex_patient_id))\nex_label_path = os.path.join(label_dir, \"{}.txt\".format(ex_patient_id))\nprint(ex_patient_id)\n\nplt.imshow(cv2.imread(ex_img_path))\n\nimg_size = 1014\nwith open(ex_label_path, \"r\") as f:\n    for line in f:\n        print(line)\n        class_id, rcx, rcy, rw, rh = list(map(float, line.strip().split()))\n        x = (rcx-rw\/2)*img_size\n        y = (rcy-rh\/2)*img_size\n        w = rw*img_size\n        h = rh*img_size\n        plt.plot([x, x, x+w, x+w, x], [y, y+h, y+h, y, y])","ab5880ed":"def write_train_list(metadata_dir, img_dir, name, series):\n    list_fp = os.path.join(metadata_dir, name)\n    with open(list_fp, \"w\") as f:\n        for patient_id in series:\n            line = \"{}\\n\".format(os.path.join(img_dir, \"{}.jpg\".format(patient_id)))\n            f.write(line)","f746fc0b":"# Following lines do not contain data with no bbox\npatient_id_series = annots[annots.Target == 1].patientId.drop_duplicates()\n\ntr_series, val_series = train_test_split(patient_id_series, test_size=0.1, random_state=random_stat)\nprint(\"The # of train set: {}, The # of validation set: {}\".format(tr_series.shape[0], val_series.shape[0]))\n\n# train image path list\nwrite_train_list(metadata_dir, img_dir, \"tr_list.txt\", tr_series)\n# validation image path list\nwrite_train_list(metadata_dir, img_dir, \"val_list.txt\", val_series)","919320f7":"def save_yolov3_test_data(test_dcm_dir, img_dir, metadata_dir, name, series):\n    list_fp = os.path.join(metadata_dir, name)\n    with open(list_fp, \"w\") as f:\n        for patient_id in series:\n            save_img_from_dcm(test_dcm_dir, img_dir, patient_id)\n            line = \"{}\\n\".format(os.path.join(img_dir, \"{}.jpg\".format(patient_id)))\n            f.write(line)","fec658e2":"test_dcm_fps = list(set(glob.glob(os.path.join(test_dcm_dir, '*.dcm'))))\ntest_dcm_fps = pd.Series(test_dcm_fps).apply(lambda dcm_fp: dcm_fp.strip().split(\"\/\")[-1].replace(\".dcm\",\"\"))\n\nsave_yolov3_test_data(test_dcm_dir, img_dir, metadata_dir, \"te_list.txt\", test_dcm_fps)","03d4667c":"#ex_patient_id = test_dcm_fps[0]\nex_patient_id = \"0100515c-5204-4f31-98e0-f35e4b00004a\"\nex_img_path = os.path.join(img_dir, \"{}.jpg\".format(ex_patient_id))\n\nplt.imshow(cv2.imread(ex_img_path))","1bbab9c7":"data_extention_file_path = os.path.join(cfg_dir, 'rsna.data')\nwith open(data_extention_file_path, 'w') as f:\n    contents = \"\"\"classes= 1\ntrain  = {}\nvalid  = {}\nnames  = {}\nbackup = {}\n    \"\"\".format(os.path.join(metadata_dir, \"tr_list.txt\"),\n               os.path.join(metadata_dir, \"val_list.txt\"),\n               os.path.join(cfg_dir, 'rsna.names'),\n               backup_dir)\n    f.write(contents)","a892dbdd":"!cat cfg\/rsna.data","d3ea5f8a":"# Label list of bounding box.\n!echo \"pneumonia\" > cfg\/rsna.names","782cf1e2":"!wget -q https:\/\/pjreddie.com\/media\/files\/darknet53.conv.74","4ca59398":"!wget --no-check-certificate -q \"https:\/\/docs.google.com\/uc?export=download&id=18ptTK4Vbeokqpux8Onr0OmwUP9ipmcYO\" -O cfg\/rsna_yolov3.cfg_train","05e42d2a":"# !.\/darknet_gpu detector train cfg\/rsna.data cfg\/rsna_yolov3.cfg_train darknet53.conv.74 -i 0 | tee train_log.txt","3548f373":"# !.\/darknet_gpu detector train cfg\/rsna.data cfg\/rsna_yolov3.cfg_train backup\/rsna_yolov3_1000.weights -gpus 0,1,2,3 | tee train_log.txt","2f16e720":"ex_patient_id = annots[annots.Target == 1].patientId.values[10]\nshutil.copy(ex_img_path, \"test.jpg\")\nprint(ex_patient_id)","ace68d75":"!wget --load-cookies \/tmp\/cookies.txt -q \"https:\/\/docs.google.com\/uc?export=download&confirm=$(wget --quiet --save-cookies \/tmp\/cookies.txt --keep-session-cookies --no-check-certificate 'https:\/\/docs.google.com\/uc?export=download&id=1FDzMN-kGVYCvBeDKwemAazldSVkAEFyd' -O- | sed -rn 's\/.*confirm=([0-9A-Za-z_]+).*\/\\1\\n\/p')&id=1FDzMN-kGVYCvBeDKwemAazldSVkAEFyd\" -O backup\/rsna_yolov3_15300.weights && rm -rf \/tmp\/cookies.txt","5b5003e9":"!ls -alsth backup","60091cd0":"!wget --no-check-certificate -q \"https:\/\/docs.google.com\/uc?export=download&id=10Yk6ZMAKGz5LeBbikciALy82aK3lX-57\" -O cfg\/rsna_yolov3.cfg_train","dd7c4b34":"!cd darknet && .\/darknet detector test ..\/cfg\/rsna.data ..\/cfg\/rsna_yolov3.cfg_train ..\/backup\/rsna_yolov3_15300.weights ..\/test.jpg -thresh 0.005","c60ec097":"# ![](predictions.jpg)\nplt.imshow(cv2.imread(\".\/darknet\/predictions.jpg\"))","ea048a32":"!wget --no-check-certificate -q \"https:\/\/docs.google.com\/uc?export=download&id=1-KTV7K9G1bl3SmnLnzmpkDyNt6tDmH7j\" -O darknet.py","731c0001":"from darknet import *","068c783b":"threshold = 0.2","d1bb0e20":"submit_file_path = \"submission_stage_train_2.csv\"\ncfg_path = os.path.join(cfg_dir, \"rsna_yolov3.cfg_train\")\nweight_path = os.path.join(backup_dir, \"rsna_yolov3_15300.weights\")\n\ntrain_img_list_path = os.path.join(metadata_dir, \"te_list.txt\")","d83fb185":"gpu_index = 0\nnet = load_net(cfg_path.encode(),\n               weight_path.encode(), \n               gpu_index)\nmeta = load_meta(data_extention_file_path.encode())","db0ef487":"submit_dict = {\"patientId\": [], \"PredictionString\": []}\n\nwith open(train_img_list_path, \"r\") as train_img_list_f:\n    # tqdm run up to 1000(The # of test set)\n    for line in tqdm(train_img_list_f):\n        patient_id = line.strip().split('\/')[-1].strip().split('.')[0]\n\n        infer_result = detect(net, meta, line.strip().encode(), thresh=threshold)\n\n        submit_line = \"\"\n        for e in infer_result:\n            confi = e[1]\n            w = e[2][2]\n            h = e[2][3]\n            x = e[2][0]-w\/2\n            y = e[2][1]-h\/2\n            submit_line += \"{} {} {} {} {} \".format(confi, x, y, w, h)\n\n        submit_dict[\"patientId\"].append(patient_id)\n        submit_dict[\"PredictionString\"].append(submit_line)\n\npd.DataFrame(submit_dict).to_csv(submit_file_path, index=False)","8cc492f2":"# !ls -lsht\n!rm -rf darknet images labels metadata backup cfg\n!rm -rf train_log.txt darknet53.conv.74 darknet.py darknet_gpu\n!rm -rf test.jpg\n!rm -rf __pycache__ .ipynb_checkpoints","3936e738":"!ls -alsht","717cc045":"### 2.4. Generate train\/val file path list (.txt)\n* We should give the list of image paths to YOLO. two seperate list textfiles for training images and validation images.","cf7e653d":"## 2. Data Migration for YOLOv3\nIt might take a while.","1f72791f":"## 3. Prepare Configuration Files for Using YOLOv3\nWe should prepare and modify config files, and bring pre-trained weights necessary for training. This proceeds with following four steps.\n```\n cfg\/rsna.data\n cfg\/rsna.names\n darknet53.conv.74\n cfg\/rsna_yolov3.cfg_train\n```","9b67ed3b":"### - cfg\/rsna_yolov3.cfg_train\n* Basically, you can use darknet\/cfg\/yolov3.cfg files. However it won't work for RSNA. you need to edit for RSNA.\n* You can just download a cfg file I edited for RSNA with following wget command.\n\n\n* I refer to the following articles for editing cfg files.\n  * [YOLOv3 blog](https:\/\/pjreddie.com\/darknet\/yolo\/)\n  * [YOLOv3 paper](https:\/\/pjreddie.com\/media\/files\/papers\/YOLOv3.pdf)\n  * [how to train yolov2 blog](https:\/\/medium.com\/@manivannan_data\/how-to-train-yolov2-to-detect-custom-objects-9010df784f36)\n  * [darknet github issues\/236](https:\/\/github.com\/pjreddie\/darknet\/issues\/236)","c69317e4":"## 6. Generate Submission Files with YOLOv3 Python Wrapper","73b66fad":"## 7. Future works & Etc\n\n### Future works (Things to try)\n* Image augmentation\n* More training\n* Utilizing the not labeled images because we got rid of not labeled images above\n\n### ETC\n* For a private matter, i can not proceed RSNA task after 09\/27. If you have any ideas, questions and problems with this kernel after 09\/27, Please leave those things anyway~! Collaborator '@John Byun' will reply to your comments.","306ee6b2":"### 2.2. Generate images and labels for training YOLOv3\n* YOLOv3 needs .txt file for each image, which contains ground truth object in the image that looks like:\n```\n<object-class_1> <x_1> <y_1> <width_1> <height_1>\n<object-class_2> <x_2> <y_2> <width_2> <height_2>\n```\n* <object-class\\>: Since RSNA task is binary classification basically, <object-class\\> is 0.\n* <x\\>, <y\\>: Those are float values of bbox center coordinate, divided by image width and height respectively.\n* <w\\>, <h\\>: Those are width and height of bbox, divided by image width and height respectively.\n\n* So it is different from the format of label data provided by kaggle. We should change it.","f6532052":"## Contents\n### YOLO v3 for image detection\n0. <a href='#0.-Introduction'>Introduction<\/a>\n1. <a href='#1.-Clone-and-Build-YOLOv3'>Clone and Build YOLOv3<\/a>\n2. <a href='#2.-Data-Migration-for-YOLOv3'>Data Migration<\/a>\n3. <a href='#3.-Prepare-Configuration-Files-for-Using-YOLOv3'>Prepare Configuration Files for training<\/a>\n4. <a href='#4.-Training-YOLOv3'>Training model<\/a>\n5. <a href='#5.-How-to-use-trainined-YOLOv3-for-test-images-(command-line)'>How to use trained model for test images (command line)<\/a>\n6. <a href='#6.-Generate-Submission-Files-with-YOLOv3-Python-Wrapper'>Generate Submission Files Using YOLOv3 Python Wrapper<\/a>\n7. <a href='#7.-Future-works-&-Etc'>Future works & Etc<\/a>","69cf6cad":"### 2.1. Load stage_1_train_labels.csv","724f10e8":"### 4.1. Command for training with Multi-gpu after 1000 iteration\n\nIf you are trying to train with multi-gpu, there are three things to watch out.\n* (The # of gpus)x('learning rate' in 'cfg\/rsna_yolov3.cfg_train') is the real learning rate for training\n* I don't recommend you to use multi-gpu for first 1000 iterations. with multi-gpu, training would not be stable. Use single gpu before 1000 and after 1000, continue with more gpus.\n* By the way, If the # of gpus is over 5, training is not stable.\n\n```\nAbove things will depend on your environment. The best way to find the most appropriate method is to just give it a try :)\n```","c080a192":"### 5.0. Copy sample test image","06304c3b":"### 6.1. Load darknet python wrapper module","e4f88f68":"### 6.0. Download darknet python wrapper (darknet.py)\n* Basically, you can use darknet\/python\/darknet.py files. However it'll show error.\n* So, I edited the darknet.py. There are two main modifications.\n  * Change print statement to print function for python3\n  * Edit dynamic library('libdarknet.so') file path\n* I leaved '# ===' marks where i edited in darknet.py. For example,\n```\n# ==============================================================================\n#lib = CDLL(\"\/home\/pjreddie\/documents\/darknet\/libdarknet.so\", RTLD_GLOBAL)\ndarknet_lib_path = os.path.join(os.getcwd(), \"darknet\", \"libdarknet.so\")\nlib = CDLL(darknet_lib_path, RTLD_GLOBAL)\n# ==============================================================================\n```","e67895f3":"### - darknet53.conv.74  (Download Pre-trained Model)\nFor training, we would download the pre-trained model weights(darknet53.conv.74) using following wget command. I recommend you to use this pre-trained weight too. Author of darknet also uses this pre-trained weights in different fields of image recognition.","3a46a972":"### - cfg\/rsna.data\nThis file point to RSNA data path\n  * train: Path to training image list textfile\n  * val: Path to validation image list textfile\n  * names: RSNA class name list (see <a href='#3.1.-cfg\/rsna.names'>3.1<\/a>)\n  * backup: A directory where trained weights(checkpoints) will be stored as training progresses.","6c810f72":"### 5.2. cfg file for test (not for training)","d5b156a5":"## 4. Training YOLOv3","2b732fe0":"### 4.0. Command for training with Pre-trained CNN Weights (darknet53.conv.74)\n* I didn't run following command on kaggle kernel becuase of the long output.\n* If you crash with  'CUDA Error: out of memory', Solve it by Editing 'batch' and 'subdivisions' in 'cfg\/rsna_yolov3.cfg_train'\n* If 'batch' and 'subdivisions' are 64 and 64 respectively, for every iteration only one image will be loaded on GPU memory. So it will use less GPU memory.","f5364b53":"### - cfg\/rsna.names","2426c617":"## 0. Introduction\n* I'll introduce super easy and quick way to train [YOLOv3](https:\/\/pjreddie.com\/darknet\/yolo\/) on RSNA and to generate submission file (to be honest, not super easy ...!).\n\n\n* The purpose of this competition is 'object detection'. Generally, object detection algorithms with deep learning take a long time to train model and require a lot of gpu resources. Most individual participants use one or two gpu (... or zero). Therefore, there is a need for **algorithms that works quickly with less gpu resources.**\n\n\n* I tried to use Mask R-CNN, UNet, Fast R-CNN and FCN algorithms, But eventually switched to YOLOv3.\n\n\n* In comparison to YOLOv3, Other algorithms(Mask R-CNN, UNet, FCN, ...) which contain instance\/sementic segmentation tasks are very slow, and require more gpu resources, redundant parameter tunning and post-processes. Therefore, if you try to use these algorithms, you may experience difficulties in terms of training time and gpu resources. (Please see [YOLOv3 paper](https:\/\/pjreddie.com\/media\/files\/papers\/YOLOv3.pdf) for details)\n\n\n* In addition, **YOLOv3 was able to obtain high score (LB: 0.141) without additional processes(data augmentation, parameter tunning, etc...)** compared to other algorithms. So I think YOLOv3 has sufficient potential for this competition.\n\n\n* In this notebook, I'll introduce how to simply apply YOLOv3 on RSNA data. I hope this notebook would be helpful for everyone.","f4b37db8":"### 5.1. Load trained model (at 15300 iteration)\nSince i uploaded the weights file (large big file) on my google drive, the command is very very long ...\n* It's a weight file at 15300 iteration, which I made submission file with. If you use this weight, you'll get a score of 0.141LB.\n  * Up to 15300 iteration, It takes about 8 hours.\n    * In .cfg file, I set 'batch' and 'subdivisions' as 64 and 8 respectively.\n    * Up to 1000 iteration from 0, it takes about 1h with **one** Tesla P100 GPU.      **(1000 iter\/h)**\n    * Up to 15300 iteration from 1000, it takes about 7h with **four** Tesla P100 GPU. **(2043 iter\/h)**","ce4f798e":"### 2.0. Make subdirectories","980504e2":"### 2.6. Plot a sample test Image","fda591fa":"### 6.2. Generate submission files\n* When making submission files, be aware of label format which is different in yolo.","ca1783c5":"### 2.3. Plot a sample train image and label","2bdd0513":"## 1. Clone and Build YOLOv3","c43f7112":"### 2.5. Create test image and labels for YOLOv3","04617493":"## 5. How to use trainined YOLOv3 for test images (command line)"}}