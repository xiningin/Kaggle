{"cell_type":{"9da0e849":"code","e7b0c7fa":"code","0b38f4a3":"code","545dc464":"code","73ff8b75":"code","f1305172":"code","a77485e4":"code","270030a5":"code","5fffcd54":"code","aac8e8dd":"code","09a5b230":"code","1849c5c8":"code","3c977c80":"code","70c36de2":"code","d22978c0":"code","270d459f":"code","58a185fb":"code","3dd7ab3b":"code","5d6fdb61":"code","8112f3e0":"code","64e9077a":"code","37cdcf95":"code","6c3d2cc8":"code","835a43b6":"code","dae5b204":"code","ec9c0fd6":"code","5188dbe8":"code","228ff92c":"code","566c5744":"code","7b753ac2":"code","b0e12f01":"code","e46502e1":"code","df93a42e":"code","93f81145":"code","c9d532c3":"code","9586c524":"code","e99d248b":"code","fc1a01d0":"code","f84f7901":"code","61cf1368":"code","712acbe2":"code","3d999e47":"code","c0f463bc":"code","9ee4b094":"code","bca1df0e":"code","3d892618":"code","f38be153":"code","82d85c64":"code","7ecf659a":"code","8da6b849":"code","063073c8":"code","e8d1d790":"code","179e189c":"code","257ad519":"code","91061195":"code","6d1f3db5":"code","7f1da0ea":"code","49d79a51":"code","aec834fb":"code","d1d378da":"code","39637380":"code","e58f18d3":"code","3bd51eb1":"code","322f16f9":"code","1f44c51c":"code","170de511":"code","dc86acd0":"code","7443134c":"code","3ed7b985":"code","6c3fa29f":"code","9afc3b4c":"code","87921561":"code","e127ec7e":"code","91af436e":"code","8c342054":"code","78572f0f":"code","35bd3e93":"code","d1aaad6f":"code","b7e78e47":"code","9df482eb":"code","f7f3809b":"code","d6859576":"code","0db02b28":"code","eee583fc":"code","c1c19e58":"code","8e1315c0":"code","e4caf2b2":"code","98c25021":"code","b38489a7":"code","864249cf":"code","9ea396bf":"code","1a08080b":"code","0914f7a5":"code","12fdc1da":"code","419cfc06":"code","189e903f":"code","712e988c":"code","e5ba377f":"code","91cb2488":"code","d5a5c806":"code","9d17a01e":"code","7a7cf8b4":"code","7bf708c2":"code","704a0192":"code","99954a3e":"code","3ff9a394":"code","9363f749":"code","c7c0bfe1":"code","deb5b2e5":"code","3fd1250d":"code","03c10742":"code","745add52":"code","13684292":"code","46ea8ca6":"code","8a2cc10b":"code","9564f35f":"code","1c983705":"code","9166bbaa":"code","3461b4b0":"code","ff64f8f1":"code","f85d719c":"code","e388b2f1":"markdown","222633e9":"markdown","e30e8414":"markdown","f720afb3":"markdown","6a060bfa":"markdown","1344974b":"markdown","405cfd93":"markdown","43d8e05b":"markdown","c86fe85b":"markdown","5a11c15c":"markdown","9ccb34ed":"markdown","eb0d6065":"markdown","2dccfae8":"markdown","51d5ebdb":"markdown","5141f761":"markdown","96e656a7":"markdown","f259d098":"markdown","041fddc8":"markdown","765faf6d":"markdown","121e4fb9":"markdown","c56a4109":"markdown","296c9a23":"markdown","0b746ee4":"markdown","dfcce393":"markdown","e6d92c54":"markdown","fc22b9a7":"markdown","a8c86850":"markdown","5f77535f":"markdown","7d475a3d":"markdown","86a732bd":"markdown","b4c8896c":"markdown","3f894ac4":"markdown","9b9d0ff0":"markdown","766aa1c7":"markdown","e8aadbe8":"markdown","c96fda6e":"markdown","96a03c39":"markdown","9b2cbe43":"markdown","cda256b0":"markdown","05c506fe":"markdown","2d5c1c64":"markdown","8d0a69c9":"markdown","ab83df37":"markdown","5e1f2861":"markdown","9c9693a5":"markdown","a7090c78":"markdown"},"source":{"9da0e849":"from IPython.display import Image\nImage('.\/ML_canvas_2.png') ","e7b0c7fa":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nimport os\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# time calculation to track some processes\nimport time\n\n# python core library for machine learning and data science\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import KNNImputer, SimpleImputer\nfrom sklearn.cluster import KMeans","0b38f4a3":"PATH_DATA = \"..\/input\/easymoneygrupo5\/\"\n#PATH_DATA = '\/content\/drive\/MyDrive\/Nuclio Data Science - Proyecto Final\/data\/'\n#PATH_DATA = '\/Users\/carlosperezricardo\/Documents\/data\/'","545dc464":"commercial_activity = pd.read_csv(PATH_DATA+'commercial_activity_df.csv', encoding='utf-8')\ncommercial_activity.drop(columns=['Unnamed: 0'], inplace=True)\n\nproducts = pd.read_csv(PATH_DATA+'products_df.csv', encoding='utf-8')\nproducts.drop(columns=['Unnamed: 0'], inplace=True)\n\nsociodemographic = pd.read_csv(PATH_DATA+'sociodemographic_df.csv', encoding='utf-8')\nsociodemographic.drop(columns=['Unnamed: 0'], inplace=True)","73ff8b75":"df_whole = pd.merge( commercial_activity, products, on = ['pk_cid','pk_partition'] )\ndf_whole = pd.merge( df_whole, sociodemographic, on=['pk_cid','pk_partition'] )","f1305172":"df_whole.shape","a77485e4":"df_whole.info(verbose=False)","270030a5":"# Codificar variables logicas (1\/0 = bool)\nfrom sys import getsizeof\n\nboolean_cols = [\"short_term_deposit\", \"loans\", \"mortgage\", \"funds\", \"securities\",\"long_term_deposit\", \"em_account_pp\", \"credit_card\", \"payroll_account\", \"emc_account\", \"debit_card\", \"em_account_p\", \"em_acount\", \"payroll\", \"pension_plan\"] \n# payroll y pension_plan tienen nulos en el dataset completo\n\nprint('El dataset ocupa {:1f} MB'.format(df_whole.memory_usage(index=True).sum()\/1048576))\n\nfor x in boolean_cols:\n    df_whole[x] = df_whole[x].astype(bool)\n\nprint('El dataset ocupa {:1f} MB'.format(df_whole.memory_usage(index=True).sum()\/1048576))","5fffcd54":"df_whole.head(3).T","aac8e8dd":"df_whole.columns.to_list()","09a5b230":"df = pd.DataFrame()","1849c5c8":"salary_df = df_whole.groupby('pk_cid')[['segment','salary']].last()","3c977c80":"salary_df['segment'].value_counts()","70c36de2":"top_mean_salary = salary_df[ salary_df['segment'] == '01 - TOP' ]['salary'].median()\nparticulares_mean_salary = salary_df[ salary_df['segment'] == '02 - PARTICULARES' ]['salary'].median()\nuniversitarios_mean_salary = salary_df[ salary_df['segment'] == '03 - UNIVERSITARIO' ]['salary'].median()\nmean_salary = salary_df['salary'].median()","d22978c0":"print('Salario mediana TOP '+str(top_mean_salary))\nprint('Salario mediana PARTICULARES '+str(particulares_mean_salary))\nprint('Salario mediana UNIVERSITARIOS '+str(universitarios_mean_salary))","270d459f":"salary_df['salary'] = np.where( (salary_df['salary'].isna()) & (salary_df['segment']=='01 - TOP'), top_mean_salary, salary_df['salary'] )\nsalary_df['salary'] = np.where( (salary_df['salary'].isna()) & (salary_df['segment']=='02 - PARTICULARES'), particulares_mean_salary, salary_df['salary'] )\nsalary_df['salary'] = np.where( (salary_df['salary'].isna()) & (salary_df['segment']=='03 - UNIVERSITARIO'), universitarios_mean_salary, salary_df['salary'] )\nsalary_df['salary'] = np.where( salary_df['salary'].isna(), mean_salary, salary_df['salary'] )","58a185fb":"df['salary'] = salary_df['salary']\ndf.isnull().sum()","3dd7ab3b":"df_whole.groupby('pk_cid')['deceased'].max().value_counts()","5d6fdb61":"df_whole['deceased'] = df_whole['deceased'].replace({'N':0,'S':1})\n#df['deceased'] = df_whole.groupby('pk_cid')['deceased'].max()","8112f3e0":"df['active_customer'] = df_whole.groupby('pk_cid')['active_customer'].mean()","64e9077a":"df['age'] = df_whole.groupby('pk_cid')['age'].max()","37cdcf95":"# hay errores en dias 29-02 deberian ser corregidos\ndf['entry_year'] = df_whole['entry_date'].apply( lambda x: str(x)[:4])","6c3d2cc8":"top_regions = df_whole.groupby('pk_cid')['region_code'].last().to_frame().value_counts().head(25)\nlist_top_regions = top_regions.reset_index()['region_code'].to_list()\ntop_regions\n#list_top_regions","835a43b6":"spanish_regions_code = {1:['Elciego', 'Pa\u00eds Vasco'], 2:['Albacete', 'Castilla-La Mancha'], 3:['Alicante\/Alacant', 'Com. Valenciana'],4:['Almer\u00eda','Andaluc\u00eda'],5:['\u00c1vila','Castilla y Le\u00f3n'], 6:['Badajoz','Extremadura'],\t7:['Palma','Islas Baleares'], 8:['Barcelona','Catalu\u00f1a'],9:['Burgos','Castilla y Le\u00f3n'], 10: ['C\u00e1ceres','Extremadura'], 11: ['C\u00e1diz','Andaluc\u00eda'], 12: ['Castell\u00f3n de la Plana\/Castell\u00f3 de la Plana','Com. Valenciana'], 13: ['Ciudad Real','Castilla-La Mancha'], 14: ['C\u00f3rdoba','Andaluc\u00eda'], 15: ['Coru\u00f1a (A)','Galicia'], 16: ['Cuenca','Castilla-La Mancha'], 17: ['Girona','Catalu\u00f1a'], 18: ['Granada','Andaluc\u00eda'], 19: ['Guadalajara','Castilla-La Mancha'], 20: ['Donostia-San Sebasti\u00e1n','Pa\u00eds Vasco'], 21: ['Huelva','Andaluc\u00eda'], 22: ['Huesca','Arag\u00f3n'], 23: ['Ja\u00e9n','Andaluc\u00eda'], 24: ['Le\u00f3n','Castilla y Le\u00f3n'], 25: ['Lleida','Catalu\u00f1a'], 26: ['Logro\u00f1o','La Rioja'], 27: ['Lugo','Galicia'], 28: ['Madrid','Madrid'], 29: ['M\u00e1laga','Andaluc\u00eda'], 30: ['Murcia','Murcia'], 31: ['Pamplona\/Iru\u00f1a','Com. de Navarra'], 32: ['Ourense','Galicia'], 33: ['Oviedo','Princ. de Asturias'], 34: ['Palencia','Castilla y Le\u00f3n'], 35: ['Palmas de Gran Canaria (Las)','Islas Canarias'], 36: ['Pontevedra','Galicia'], 37: ['Salamanca','Castilla y Le\u00f3n'], 38: ['Santa Cruz de Tenerife','Islas Canarias'], 39: ['Santander','Cantabria'], 40: ['Segovia','Castilla y Le\u00f3n'], 41: ['Sevilla','Andaluc\u00eda'], 42: ['Soria','Castilla y Le\u00f3n'], 43: ['Tarragona','Catalu\u00f1a'], 44: ['Teruel','Arag\u00f3n'], 45: ['Toledo','Castilla-La Mancha'], 46: ['Valencia','Com. Valenciana'], 47: ['Valladolid','Castilla y Le\u00f3n'], 48: ['Bilbao','Pa\u00eds Vasco'], 49: ['Zamora','Castilla y Le\u00f3n'], 50: ['Zaragoza','Arag\u00f3n'], 51: ['Ceuta','Ceuta y Melilla'], 52: ['Melilla','Ceuta y Melilla']}","dae5b204":"spain = pd.DataFrame(spanish_regions_code).T\nspain.columns = ['Ciudad','Com.Autonoma']\nspain.reset_index(inplace=True)","ec9c0fd6":"top_regions = pd.DataFrame(list_top_regions, columns=['region_id'])\npd.merge(top_regions, spain, how='left', left_on='region_id', right_on='index')","5188dbe8":"# Codificar como 0 y 1\n#df_whole['region_code'] = np.where( df_whole['region_code'].isin( list_top_regions ), True, False )\n#df_whole.groupby('pk_cid')['region_code'].last().value_counts()","228ff92c":"# Frequency encoding\ndf_whole['region_code'].fillna(-1, inplace=True)\nfreq_encoder = df_whole.groupby('pk_cid')['region_code'].last().to_frame().value_counts()\nfreq_encoder","566c5744":"df_whole['region'] = df_whole['region_code'].replace(freq_encoder)","7b753ac2":"df['region'] = df_whole.groupby('pk_cid')['region'].last()","b0e12f01":"df_whole['country_id'] = np.where( df_whole['country_id'] == 'ES', True, False )\ndf_whole.groupby('pk_cid')['country_id'].last().value_counts()\n#df['country'] = df_whole.groupby('pk_cid')['country_id'].last()","e46502e1":"df_whole['payroll'] = np.where( df_whole['country_id'] == 'ES', True, False )\ndf_whole.groupby('pk_cid')['payroll'].max().value_counts()\n#df['payroll'] = df_whole.groupby('pk_cid')['payroll'].last()","df93a42e":"customers = df.index\nlen(customers)","93f81145":"partitions = ['2018-01-28','2018-02-28','2018-03-28','2018-04-28','2018-05-28','2018-06-28', \\\n    '2018-07-28','2018-08-28','2018-09-28','2018-10-28','2018-11-28','2018-12-28','2019-01-28', \\\n        '2019-02-28','2019-03-28','2019-04-28','2019-05-28']\n\nlist_products = ['short_term_deposit','loans','mortgage','funds','securities',\n    'long_term_deposit','em_account_pp','credit_card','pension_plan',\n    'payroll_account','emc_account','debit_card','em_account_p','em_acount']\n\nproducts_dict = {\"short_term_deposit\":\"ahorro e inversi\u00f3n\", \"loans\":\"financiaci\u00f3n\", \"mortgage\":\"financiaci\u00f3n\", \n    \"funds\":\"ahorro e inversi\u00f3n\", \"securities\":\"ahorro e inversi\u00f3n\", \"long_term_deposit\":\"ahorro e inversi\u00f3n\", \n    \"em_account_pp\":\"cuenta\", \"credit_card\":\"financiaci\u00f3n\", \"payroll_account\":\"cuenta\", \"pension_plan\":\"ahorro e inversi\u00f3n\", \n    \"emc_account\":\"cuenta\", \"debit_card\":\"financiaci\u00f3n\", \"em_account_p\":\"cuenta\", \"em_acount\":\"cuenta\"}\n\ncost_product = {'cuenta':10, 'ahorro e inversi\u00f3n':40, 'financiaci\u00f3n':60}","c9d532c3":"def determinar_altas(data):\n    data = pd.DataFrame(data)\n    data.columns=['product']\n    data['prev'] = data['product'].shift(1)\n    data['diff'] = data['product'] - data['prev']\n\n    return len(data[data['diff'] == 1])","9586c524":"test_data = pd.DataFrame( [0,0,1,1,1,0,0,0,1,1,1,1,0,0,0,1] )\ndeterminar_altas(test_data)\n# expected result: 3","e99d248b":"def determinar_bajas(data):\n    data = pd.DataFrame(data)\n    data.columns=['product']\n    data['prev'] = data['product'].shift(1)\n    data['diff'] = data['product'] - data['prev']\n\n    return len(data[data['diff'] == -1])","fc1a01d0":"test_data = pd.DataFrame( [0,0,1,1,1,0,0,0,1,1,1,1,0,0,0,0,0] )\ndeterminar_bajas(test_data)\n# expected result: 2","f84f7901":"def determinar_altas_cobradas(data):\n    data = pd.DataFrame(data)\n    data.reset_index(inplace=True)\n\n    data.columns=['pk_partition','product']\n    \n    data['prev_month1'] = data['product'].shift(1)\n    data['prev_month2'] = data['product'].shift(2)\n    data['prev_month3'] = data['product'].shift(3)\n\n    data.fillna(-2, inplace=True)\n\n    # nuevas contrataciones (ya cobrados - llevan 3 meses y no se han dado de baja)\n    #cond3 = (data['product']==1) & (data['prev_month1']==1) & (data['prev_month2']==1) &\\\n    #    ((data['prev_month3']==0) | ((data['prev_month3'] == -2) & (data['pk_partition']!='2018-03-28')))\n    \n    cond1 = (data['product']==1) & (data['prev_month1']==1) & (data['prev_month2']==1) & (data['prev_month3']==0)\n    cond2 = (data['product']==1) & (data['prev_month1']==1) & (data['prev_month2']==1) & (data['prev_month3']==-2) & (data['pk_partition']!='2018-03-28')\n\n    cond3 = cond1 | cond2\n    print(data)\n    contrataciones_cobradas = data[ cond3 ] \n\n    return len(contrataciones_cobradas)","61cf1368":"test_data = pd.DataFrame( [0,0,1,1,1,1,0,0,1,1,0,0,0,0,0,1,1], index=partitions )\ntest_data.reset_index(inplace=True)\ntest_data.columns = ['pk_partition','product']\ntest_data['pk_partition'] = pd.to_datetime( test_data['pk_partition'] )\ntest_data.set_index('pk_partition',inplace=True)\n\ndeterminar_altas_cobradas(test_data)\n# expected result: 1","712acbe2":"test_data = pd.DataFrame( [1,1,1,1,0,1,0,0,1,1,1,1,0,0,1,1,1], index=partitions )\ntest_data.reset_index(inplace=True)\ntest_data.columns = ['pk_partition','product']\ntest_data['pk_partition'] = pd.to_datetime( test_data['pk_partition'] )\ntest_data.set_index('pk_partition',inplace=True)\n\ndeterminar_altas_cobradas(test_data)\n# expected result: 2","3d999e47":"test_data = pd.DataFrame( [1,1,1,1,0,1,0,0,1,1,1,1,0], index=partitions[4:] )\ntest_data.reset_index(inplace=True)\ntest_data.columns = ['pk_partition','product']\ntest_data['pk_partition'] = pd.to_datetime( test_data['pk_partition'] )\ntest_data.set_index('pk_partition',inplace=True)\n\ndeterminar_altas_cobradas(test_data)\n# expected result: 2","c0f463bc":"def determinar_altas_all(data):\n    data.columns=['pk_partition','pk_cid','product']\n    data['prev'] = data.groupby('pk_cid')['product'].shift(1)\n    data['diff'] = data['product'] - data['prev']\n\n    # Solo queremos 1 \n    data['diff'] = np.where( (data['product']==1) & (data['diff'].isna()) & (data['pk_partition']!='2018-01-28'), 1, data['diff'] )\n    data['diff'].fillna(0,inplace=True)\n    data['diff'] = np.where( data['diff'] == -1, 0, data['diff'] )\n\n    return data.groupby('pk_cid')['diff'].sum()","9ee4b094":"def determinar_bajas_all(data):\n    data.columns=['pk_cid','product']\n    data['prev'] = data.groupby('pk_cid')['product'].shift(1)\n    data['diff'] = data['product'] - data['prev']\n\n    # Solo queremos -1 \n    data['diff'].fillna(0,inplace=True)\n    data['diff'] = np.where( data['diff'] == 1, 0, data['diff'] )\n    data['diff'] = np.where( data['diff'] == -1, 1, data['diff'] )\n\n    return data.groupby('pk_cid')['diff'].sum()","bca1df0e":"def determinar_altas_cobradas_all(data):\n    data.columns=['pk_partition','pk_cid','product']\n\n    data['prev_month1'] = data.groupby('pk_cid')['product'].shift(1)\n    data['prev_month2'] = data.groupby('pk_cid')['product'].shift(2)\n    data['prev_month3'] = data.groupby('pk_cid')['product'].shift(3)\n\n    data.fillna(-2, inplace=True)\n\n    # nuevas contrataciones (ya cobrados - llevan 3 meses y no se han dado de baja)\n    #cond3 = (data['product']==1) & (data['prev_month1']==1) & (data['prev_month2']==1) &\\\n    #    ((data['prev_month3']==0) | ((data['prev_month3'] == -2) & (data['pk_partition']!='2018-03-28')))\n\n    cond31 = (data['product']==1) & (data['prev_month1']==1) & (data['prev_month2']==1) & (data['prev_month3']==0)\n    cond32 = (data['product']==1) & (data['prev_month1']==1) & (data['prev_month2']==1) & (data['prev_month3']==-2) & (data['pk_partition']!='2018-03-28')\n    cond33 = (data['product']==1) & (data['prev_month1']==1) & (data['prev_month2']==1) & (data['prev_month3']==1) & (data['pk_partition']=='2018-03-28')\n    \n    cond3 = cond31 | cond32 | cond33\n\n    data['comprado'] = cond3\n\n    return data.groupby('pk_cid')['comprado'].sum()","3d892618":"df_whole = df_whole.sort_values(['pk_partition','pk_cid'], ascending=True)","f38be153":"if not os.path.isfile(os.path.join(PATH_DATA, \"ALTAS_BAJAS_COBROS.pkl\")):\n    new_cols = []\n    for prod in list_products:\n        df[prod+'_altas'] = determinar_altas_all( df_whole[['pk_partition','pk_cid',prod]] )\n        df[prod+'_bajas'] = determinar_bajas_all( df_whole[['pk_cid',prod]] )\n        df[prod+'_cobros'] = determinar_altas_cobradas_all( df_whole[['pk_partition','pk_cid',prod]] )\n\n        new_cols.append(prod+'_altas')\n        new_cols.append(prod+'_bajas')\n        new_cols.append(prod+'_cobros')\n\n        print(prod)\n        new_cols \n        df[new_cols].reset_index().to_pickle(\"ALTAS_BAJAS_COBROS.pkl\")\nelse:\n    altas_bajas_cobros = pd.read_pickle(os.path.join(PATH_DATA,\"ALTAS_BAJAS_COBROS.pkl\"))\n    df = pd.merge(df.reset_index(), altas_bajas_cobros, on='pk_cid', how='left')\n    df.set_index('pk_cid',inplace=True)","82d85c64":"df.head(3).T","7ecf659a":"init_cols = ['ahorros_altas','ahorros_bajas','ahorros_cobros',\n    'financiacion_altas','financiacion_bajas','financiacion_cobros',\n    'cuenta_altas','cuenta_bajas','cuenta_cobros',\n    'unique_altas','recurrencia']\n    \nfor col in init_cols:\n    df[col] = 0\n\n\nfor key, value in products_dict.items():\n    # corregimos titulos largos y con tildes\n    if value == 'ahorro e inversi\u00f3n':\n        value = 'ahorros'\n    elif value == 'financiaci\u00f3n':\n        value = 'financiacion'\n    elif value == 'cuenta':\n        value = 'cuenta'\n\n    df[value+'_altas'] += df[key+'_altas'] \n    df[value+'_bajas'] += df[key+'_bajas'] \n    df[value+'_cobros'] += df[key+'_cobros'] \n    \n    df['unique_altas'] += np.where( df[key+'_altas'] != 0, 1, 0)\n    df['recurrencia'] += np.where(df[key+'_altas'] >= 2, df[key+'_altas'], 0)\n    \n    #df.drop(key+'_altas', axis=1, inplace=True)\n    #df.drop(key+'_bajas', axis=1, inplace=True)\n    #df.drop(key+'_cobros', axis=1, inplace=True)\n\ndf['total_altas'] = df['ahorros_altas'] + df['financiacion_altas'] + df['cuenta_altas']","8da6b849":"df.head(5).T","063073c8":"assert len(df[ df['total_altas'] < df['unique_altas']]) == 0","e8d1d790":"for key, value in cost_product.items():\n    if key == 'ahorro e inversi\u00f3n':\n        key = 'ahorros'\n    elif key == 'financiaci\u00f3n':\n        key = 'financiacion'\n    elif key == 'cuenta':\n        key = 'cuenta'\n\n    if 'gastado' not in df.columns:\n        df['gastado'] = df[key+'_cobros']*value\n    else:\n        df['gastado'] += df[key+'_cobros']*value\n\n    #df['bajas'] += df[key+'_bajas']\n    #df.drop(key+'_bajas', axis=1, inplace=True)\n    #df.drop(key+'_cobros', axis=1, inplace=True)","179e189c":"len(df.columns)","257ad519":"products_dict = {\"short_term_deposit\":\"ahorro e inversi\u00f3n\", \"loans\":\"financiaci\u00f3n\", \"mortgage\":\"financiaci\u00f3n\", \n    \"funds\":\"ahorro e inversi\u00f3n\", \"securities\":\"ahorro e inversi\u00f3n\", \"long_term_deposit\":\"ahorro e inversi\u00f3n\", \n    \"em_account_pp\":\"cuenta\", \"credit_card\":\"financiaci\u00f3n\", \"payroll_account\":\"cuenta\", \"pension_plan\":\"ahorro e inversi\u00f3n\", \n    \"emc_account\":\"cuenta\", \"debit_card\":\"financiaci\u00f3n\", \"em_account_p\":\"cuenta\", \"em_acount\":\"cuenta\"}\n\nahorros = [\"pk_partition\",\"short_term_deposit\",\"funds\",\"securities\",\"long_term_deposit\",\"pension_plan\"]\ncuentas = [\"pk_partition\",\"em_acount\",\"em_account_pp\",\"emc_account\",\"em_account_p\",\"em_account_pp\",\"payroll_account\"]\nfinanciacion = [\"pk_partition\",\"short_term_deposit\",\"funds\",\"securities\",\"long_term_deposit\",\"pension_plan\"]","91061195":"df_whole[ df_whole['pk_cid'] == 16203 ][cuentas]\n# cuentas: altas = 1, cobradas = 1, bajas = 0","6d1f3db5":"df_whole[ df_whole['pk_cid'] == 15891 ][cuentas]\n# cuentas: altas = 1, cobradas = 0, bajas = 1","7f1da0ea":"def determinar_permanencia(data):\n    data = pd.DataFrame(data)\n    data.reset_index(inplace=True)\n\n    data.columns=['pk_partition','product']\n\n    data['diff'] = data['product'].diff()\n\n    data['prev'] = np.where( (data['diff'].isna()) & (data['product'] == 1), 1, 0 )\n    data = data[ data['diff'] != 0 ]\n\n    data['date_diff'] = round(data['pk_partition'].diff()\/np.timedelta64(1, 'M'))\n\n    return data[data['diff'] == -1]","49d79a51":"test_data = pd.DataFrame( [0,0,1,1,1,0,0,0,1,1,1,1,0,0,0,1,1], index=partitions )\ntest_data.reset_index(inplace=True)\ntest_data.columns = ['pk_partition','product']\ntest_data['pk_partition'] = pd.to_datetime( test_data['pk_partition'] )\ntest_data.set_index('pk_partition',inplace=True)\n\nperm = determinar_permanencia(test_data)\nprint(perm)\n\nperm['date_diff'].sum()\/len(perm['date_diff'])\n# expected result: 3, 4","aec834fb":"def determinar_permanencia_all(data):\n    data.columns=['pk_partition','pk_cid','product']\n    data['pk_partition'] = pd.to_datetime(data['pk_partition'])\n\n    data['prev'] = data.groupby('pk_cid')['product'].shift(1)\n    data['diff'] = data['product'] - data['prev']\n    #data['diff'] = data['product'].diff()\n\n    data['prev'] = np.where( (data['diff'].isna()) & (data['product'] == 1), 1, 0 )\n    data = data[ data['diff'] != 0 ]\n    \n    data = data.sort_values(['pk_cid','pk_partition'])\n    \n    data['date_diff'] = round(data['pk_partition'].diff()\/np.timedelta64(1, 'M'))\n\n    data = data[data['diff'] == -1]\n    \n    #print(data)\n    summary = data.groupby('pk_cid')['date_diff'].agg(['sum','count'])\n    summary['result'] = summary['sum']\/summary['count']\n    summary.reset_index(inplace=True)\n    #print(summary)\n\n    return summary[['pk_cid','result']]","d1d378da":"list_perms = []\nfor prod in list_products:\n    perms = determinar_permanencia_all( df_whole[['pk_partition','pk_cid',prod]] )\n    #print(perms)\n    perms.columns = ['pk_cid',prod+'_perm']\n    df = pd.merge(df, perms, how='left', on='pk_cid')\n    list_perms.append(prod+'_perm')\n    print(prod)","39637380":"df['permanencia'] = df[list_perms].sum(axis=1)\/df[list_perms].count(axis=1)\ndf['permanencia'].value_counts(dropna=False).head(10)","e58f18d3":"df['permanencia'].fillna(0, inplace=True)","3bd51eb1":"df.drop(list_perms, axis=1, inplace=True)","322f16f9":"df['total_products'] = df_whole.groupby('pk_cid')[list_products].max().sum(axis=1).reset_index(drop=True)","1f44c51c":"df['total_products'].head(10)","170de511":"fig, ax = plt.subplots(figsize=(16,8))\nsns.countplot(df['total_products'])\nax.set_ylabel('Count',fontsize = 20)\nax.set_xlabel('N\u00ba de productos contratados',fontsize = 20)\n\nfor p in ax.patches:\n    percentage = '{}'.format(p.get_height())\n    x = p.get_x() + p.get_width()\n    y = p.get_height()\n    ax.annotate(percentage, (x-0.4, y+1200),ha='center')\n    \nplt.show()\n\n#fig = px.histogram(df, x='total_products')\n#fig.show()","dc86acd0":"obtain = True\nif obtain == True:\n    fig.savefig('num_productos_contratados.png')","7443134c":"test_data = pd.DataFrame( [0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,1,1], index=partitions )\ntest_data.reset_index(inplace=True)\ntest_data.columns = ['pk_partition','product1']\ntest_data['product2'] = [0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0]\ntest_data\n# expected result: 2, 2, 11","3ed7b985":"test_data['pk_partition'] = pd.to_datetime(test_data['pk_partition'])\ntest_data['diff1'] = test_data['product1'].diff()\ntest_data['diff2'] = test_data['product2'].diff()\n\nlist_diffs = ['diff1','diff2']\n\ntest_data['diff'] = test_data[list_diffs].max(axis=1)\nonly_altas = test_data[ test_data['diff'] == 1 ]\n\nonly_altas['prev_date'] = only_altas['pk_partition'].shift(1)\nonly_altas['prev_date'].fillna( test_data['pk_partition'][0],inplace=True)\nonly_altas['diff_date'] = round((only_altas['pk_partition'] - only_altas['prev_date'])\/np.timedelta64(1, 'M'))\nonly_altas","6c3fa29f":"# Ahora por cliente y todos los productos \ndf_ = df_whole.copy(deep=True)","9afc3b4c":"df_['pk_partition'] = pd.to_datetime(df_['pk_partition'])\nlist_diffs = []\n\nfor prod in list_products:\n    df_['diff_'+prod] = df_[prod] - df_.groupby('pk_cid')[prod].shift(1) \n    list_diffs.append('diff_'+prod)\n    \ndf_['diff'] = df_[list_diffs].max(axis=1)\ndf_.drop(list_diffs, axis=1, inplace=True)","87921561":"only_altas = df_[ df_['diff'] == 1 ]\nonly_altas['prev_date'] = only_altas.groupby('pk_cid')['pk_partition'].shift(1)","e127ec7e":"df_ = df_.sort_values(['pk_cid','pk_partition'])\nfirst_partitions = df_.groupby('pk_cid').first()['pk_partition']\nfirst_partitions = first_partitions.to_frame().reset_index()\nfirst_partitions.columns = ['pk_cid','first_partition']\nfirst_partitions.head(5)","91af436e":"only_altas = pd.merge(only_altas, first_partitions, how='left', on='pk_cid')\n#only_altas\n#only_altas['diff_date'] = round((only_altas['pk_partition'] - only_altas['prev_date'])\/np.timedelta64(1, 'M'))\n\n#only_altas['prev_date'] = np.where( only_altas['prev_date'].isna(), only_altas['first_partition'], 0 )\n#only_altas['prev_date'].isna()\nonly_altas['prev_date'] = only_altas[['prev_date','first_partition']].max(axis=1)","8c342054":"only_altas['diff_date'] = round((only_altas['pk_partition'] - only_altas['prev_date'])\/np.timedelta64(1, 'M'))\nonly_altas['diff_date'] = np.where( only_altas['pk_partition'] == only_altas['prev_date'], 0, only_altas['diff_date'] ) \n# no nos quedamos con las primeras compras, no tiene sentido calcular el tiempo entre compras\nonly_altas = only_altas[only_altas['diff_date'] != 0]","78572f0f":"only_altas.head(3).T","35bd3e93":"result = only_altas.groupby('pk_cid')['diff_date'].agg(['sum','count','max','min','mean'])","d1aaad6f":"tiempo_entre_compras = pd.DataFrame(result['mean']).reset_index()\ntiempo_entre_compras.columns = ['pk_cid','tiempo_entre_compras']\n\ndf = pd.merge(df, tiempo_entre_compras, how='left', on='pk_cid')","b7e78e47":"fig, ax = plt.subplots(figsize=(16,8))\nsns.histplot(tiempo_entre_compras['tiempo_entre_compras'], bins=16)\nax.set_ylabel('Count',fontsize = 20)\nax.set_xlabel('Tiempo entre compras',fontsize = 20)\nplt.show()","9df482eb":"obtain = True\nif obtain == True:\n    fig.savefig('tiempo_compras.png')","f7f3809b":"#Que hacemos con en el tiempo de compras de clientes que no han comprado?\ndf['tiempo_entre_compras'].fillna(17, inplace=True)\ndf['tiempo_entre_compras'].isnull().sum()","d6859576":"# drop de columnas de altas, bajas y cobros por cada producto\nfor key, value in products_dict.items():\n    df.drop(key+'_altas', axis=1, inplace=True)\n    df.drop(key+'_bajas', axis=1, inplace=True)\n    df.drop(key+'_cobros', axis=1, inplace=True)","0db02b28":"df['bajas'] = df['ahorros_bajas'] + df['financiacion_bajas'] + df['cuenta_bajas'] \n\ndf['ing_potencial'] = df['ahorros_altas']*40 + df['financiacion_altas']*60 + df['cuenta_altas']*10   \ndf['conversion'] = df['gastado'] \/ df['ing_potencial'] \n\ndf.replace([np.inf, -np.inf], np.nan, inplace=True)\n\ndf['altas'] = df['ahorros_altas'] + df['financiacion_altas'] + df['cuenta_altas']\n#df['recurrencia'] = df['altas'] \/ df['unique_altas'] - 1\n\ndf['conversion'].fillna(0,inplace=True)\n#df['recurrencia'].fillna(0,inplace=True)","eee583fc":"df.head(5).T","c1c19e58":"redundant_cols = ['altas','ahorros_altas','ahorros_bajas','ahorros_cobros','financiacion_altas','financiacion_bajas','financiacion_cobros',\n                  'cuenta_altas','cuenta_bajas','cuenta_cobros','unique_altas','total_products','bajas','gastado','recurrencia']\ncorr = df[redundant_cols].corr()\ncorr.style.background_gradient(cmap='coolwarm').set_precision(3)","8e1315c0":"show_columns = df.columns.to_list()\n\ncols_to_remove = ['pk_cid','payroll','ahorros_bajas','financiacion_bajas','ahorro_bajas','cuenta_bajas','cuenta_cobros',\n                  'ahorros_cobros','financiacion_cobros','deceased','salary','region','age','entry_year',\n                  'bajas','ing_potencial','total_products','total_altas','tiempo_entre_compras','permanencia',\n                  'active_customer','gastado','altas']\n                  # 'permanencia', 'ahorros_altas','financiacion_altas','cuenta_altas'\nfor col in cols_to_remove:\n    try:\n        show_columns.remove(col)\n    except:\n        pass\n\nprint(len(show_columns))\n\ncorr = df[show_columns].corr()\ncorr.style.background_gradient(cmap='coolwarm').set_precision(3)","e4caf2b2":"df[show_columns].columns","98c25021":"df_final = df[show_columns]","b38489a7":"df_final.isnull().sum()","864249cf":"df_final.describe()","9ea396bf":"# Convertimos bools y str a int \nfor col in df_final.columns:\n    if df_final[col].dtype == 'bool':\n        df_final[col] = df_final[col].astype(int)\n\nfor col in df_final.columns:\n    if df_final[col].dtype == 'object':\n        df_final[col] = df_final[col].astype(int)","1a08080b":"df_final.info()","0914f7a5":"fig, ax = plt.subplots( len(df_final.columns), 1, figsize=(10,50) )\nfor col, i in zip(df_final.columns, range(len(df_final.columns))):\n    #ax[i].boxplot(df_final[col], vert=False)\n    sns.boxplot(ax = ax[i], x=df_final[col])\n    ax[i].set_title(col)\n    \nplt.show()","12fdc1da":"class ArrayToDataFrame(BaseEstimator, TransformerMixin):\n    '''\n    Clase que transforma un array en un DataFrame.\n    Necesita como par\u00e1metros el nombre de las columnas y el \u00edndice.\n    '''\n    \n    def __init__(self, columns, index = None):\n        self.columns = columns\n        self.index = index\n        \n    def fit(self, X, y = None):\n        return self\n    \n    def transform(self, X, y = None):\n        \n        if self.index != None:\n            df = pd.DataFrame(X, columns = self.columns, index = self.index)\n            \n        else:\n            df = pd.DataFrame(X, columns = self.columns)\n            \n        return df","419cfc06":"# back up\nX = df_final.copy(deep=True)\ncolumns = list(X.columns)\nindex = list(X.index)\nlen(X.columns.to_list())","189e903f":"pipe = Pipeline(steps = [\n    (\"ArrayToDataFrame\", ArrayToDataFrame(columns, index = index)),\n    (\"MinMaxScaler\", MinMaxScaler())\n    #(\"StandardScaler\", StandardScaler())\n])","712e988c":"st = time.time()\ndf_scaled_transformed_no_outliers = pipe.fit_transform(X)\net = time.time()\nprint(\"Fit and transform took {} minutes.\".format(round((et - st)\/60), 2))","e5ba377f":"CALCULATE_ELBOW = True","91cb2488":"if CALCULATE_ELBOW:\n    st = time.time()\n\n    sse = {}\n\n    for k in range(2, 15):\n\n        print(f\"Fitting pipe with {k} clusters\")\n\n        clustering_model = KMeans(n_clusters = k, random_state=42)\n\n        clustering_model.fit(df_scaled_transformed_no_outliers)\n\n        sse[k] = clustering_model.inertia_\n\n    et = time.time()\n    print(\"Elbow curve took {} minutes.\".format(round((et - st)\/60), 2))","d5a5c806":"if CALCULATE_ELBOW:\n    fig = plt.figure(figsize = (16, 8))\n    ax = fig.add_subplot()\n\n    x_values = list(sse.keys())\n    y_values = list(sse.values())\n\n    ax.plot(x_values, y_values, label = \"Inertia\/dispersi\u00f3n de los cl\u00fasters\")\n    plt.xlabel('N\u00ba clusters',fontsize = 20)\n    plt.ylabel('Inertia',fontsize = 20)\n    fig.suptitle(\"Variaci\u00f3n de la dispersi\u00f3n de los cl\u00fasters en funci\u00f3n de la k\", fontsize = 20);","9d17a01e":"X = df_final.copy(deep=True)\ncolumns = list(X.columns)\nindex = list(X.index)\n\nnormalized = pipe.fit_transform(X)\n\nclustering_model = KMeans(n_clusters = 6, random_state=1234)\nclustering_model.fit(normalized)\n\n#print(clustering_model.cluster_centers_)\nprint(clustering_model.inertia_)","7a7cf8b4":"labels = clustering_model.predict(normalized)\nX['cluster'] = labels\nX['cluster'] += 1\nX['cluster'].value_counts()","7bf708c2":"copy_cols = ['gastado','unique_altas','age','active_customer','tiempo_entre_compras','ahorros_altas','financiacion_altas','cuenta_altas','permanencia']\nfor col in copy_cols:\n    if col not in X.columns.to_list():\n        X[col] = df[col] ","704a0192":"X['ing_potencial'] = X['ahorros_altas']*40 + X['financiacion_altas']*60 + X['cuenta_altas']*10   \n\nif 'conversion' not in X.columns.to_list():\n    X['conversion'] = X['gastado'] \/ X['ing_potencial'] \n    X.replace([np.inf, -np.inf], np.nan, inplace=True)\n    X['conversion'].fillna(0,inplace=True)\n\nX['altas'] = X['ahorros_altas'] + X['financiacion_altas'] + X['cuenta_altas']\nX['ahorros_pct'] =  X['ahorros_altas'] \/ X['altas'] \nX['financiacion_pct'] =  X['financiacion_altas'] \/ X['altas'] \nX['cuentas_pct'] =  X['cuenta_altas'] \/ X['altas'] \n\nif 'recurrencia' not in X.columns.to_list():\n    X['recurrencia'] = X['altas'] \/ X['unique_altas'] - 1\n\nX.fillna(0,inplace=True)","99954a3e":"cols = list(X.columns)\ncols.remove('cluster')\n#cols.remove('salary')\n\npt = pd.pivot_table( X, index='cluster', values = cols, aggfunc='mean')\n\nadd = pd.pivot_table( X, index='cluster', values = 'gastado', aggfunc='count')\npt['count'] = add\ncols.append('count')\n\nadd = pd.pivot_table( X, index='cluster', values = 'gastado', aggfunc='sum')\npt['total_gastado'] = add\npt['total_gastado_pct'] = pt['total_gastado']\/pt['total_gastado'].sum()\ncols.append('total_gastado')\n\ncols = ['altas','ahorros_pct','financiacion_pct','cuentas_pct','unique_altas','recurrencia','gastado','total_gastado_pct','conversion','count']\n\npt.fillna(0, inplace=True)\n\npt[cols].style.background_gradient(cmap='coolwarm').set_precision(3)","3ff9a394":"run = True\nif run == True:\n    X_ = X.copy(deep=True)\n    X_['pk_cid'] = df['pk_cid'] \n    X_[['pk_cid','cluster']].to_csv('client_segmentation.csv')","9363f749":"#X.fillna(0,inplace=True)\nrun = True\nif run == True:\n    X.to_pickle('cluster_segmentation.pkl')","c7c0bfe1":"cluster = 3\nX[X['cluster']==cluster][['conversion']].describe()","deb5b2e5":"cols = show_columns\nX_normalized = pd.DataFrame(normalized, columns = cols)","3fd1250d":"from sklearn.metrics import silhouette_samples, silhouette_score\n\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport numpy as np\n\n\nrange_n_clusters = [5, 6, 7, 8, 9]\nrange_n_clusters = [6]\n\nrun = False\nif run == True:\n    for n_clusters in range_n_clusters:\n        # Create a subplot with 1 row and 1 column\n        fig, ax1 = plt.subplots(1, 1)\n        fig.set_size_inches(15, 7)\n\n        # The 1st subplot is the silhouette plot\n        # The silhouette coefficient can range from -1, 1 but in this example all\n        # lie within [-0.1, 1]\n        ax1.set_xlim([-0.1, 1])\n        # The (n_clusters+1)*10 is for inserting blank space between silhouette\n        # plots of individual clusters, to demarcate them clearly.\n        ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n        \n        # Initialize the clusterer with n_clusters value and a random generator\n        # seed of 10 for reproducibility.\n        clusterer = KMeans(n_clusters=n_clusters, random_state=1234)\n        cluster_labels = clusterer.fit_predict(X_normalized)\n\n        # The silhouette_score gives the average value for all the samples.\n        # This gives a perspective into the density and separation of the formed\n        # clusters\n        silhouette_avg = silhouette_score(X_normalized, cluster_labels)\n        print(\"For n_clusters =\", n_clusters,\n              \"The average silhouette_score is :\", silhouette_avg)\n\n        # Compute the silhouette scores for each sample\n        sample_silhouette_values = silhouette_samples(X_normalized, cluster_labels)\n\n        y_lower = 10\n        for i in range(n_clusters):\n            # Aggregate the silhouette scores for samples belonging to\n            # cluster i, and sort them\n            ith_cluster_silhouette_values = \\\n                sample_silhouette_values[cluster_labels == i]\n\n            ith_cluster_silhouette_values.sort()\n\n            size_cluster_i = ith_cluster_silhouette_values.shape[0]\n            y_upper = y_lower + size_cluster_i\n\n            color = cm.nipy_spectral(float(i) \/ n_clusters)\n            ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                              0, ith_cluster_silhouette_values,\n                              facecolor=color, edgecolor=color, alpha=0.7)\n\n            # Label the silhouette plots with their cluster numbers at the middle\n            ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n            # Compute the new y_lower for next plot\n            y_lower = y_upper + 10  # 10 for the 0 samples\n\n        ax1.set_title(\"The silhouette plot for the various clusters.\")\n        ax1.set_xlabel(\"The silhouette coefficient values\")\n        ax1.set_ylabel(\"Cluster label\")\n\n        # The vertical line for average silhouette score of all the values\n        ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n\n        ax1.set_yticks([])  # Clear the yaxis labels \/ ticks\n        ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n\n        # Labeling the clusters\n        centers = clusterer.cluster_centers_\n\n        plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n                      \"with n_clusters = %d\" % n_clusters),\n                     fontsize=14, fontweight='bold')\n\n    plt.show()","03c10742":"from IPython.display import Image\nImage(filename=PATH_DATA+'cuchillos.png') ","745add52":"obtain = True\nif obtain == True:\n    fig.savefig('cuchillos.png')","13684292":"cluster_names = {0:'Los Fosa de las Marianas',1:'Los Once-in-a-lifetime',2:'Los Curiosos',3:'Los Pretenciosos',4:'Los M\u00e1s-vale-mal-conocido',5:'Los Comprometidos'}\ncluster_colors = {0:'lightblue',1:'indianred',2:'orange',3:'lightseagreen',4:'mediumpurple',5:'darkgray'}\n\ncluster_info = {}\ncluster_info['names'] = cluster_names\ncluster_info['colors'] = cluster_colors","46ea8ca6":"def plot_clusters(X, selected_clusters, variable, type_plot, cluster_info):\n    fig = go.Figure()\n    names = cluster_info['names']\n    colors = cluster_info['colors']\n\n    for cluster in selected_clusters:\n        if type_plot == 'Violin':\n            fig.add_trace(go.Violin(x = X[X['cluster']==cluster][variable], name='Cluster '+names[cluster],\n                                   line_color=colors[cluster] ))\n        elif type_plot == 'Box':\n            fig.add_trace(go.Box(x = X[X['cluster']==cluster][variable], name='Cluster '+names[cluster],\n                                 line_color=colors[cluster]))\n\n    fig.update_layout(legend=dict(\n          orientation=\"h\", yanchor=\"bottom\", y=1.02,\n          xanchor=\"center\", x=0.5,\n          font=dict(family=\"Courier\",size=14,color=\"black\")\n      ),margin=go.layout.Margin(\n            l=20, #left margin\n            r=20, #right margin\n            b=20, #bottom margin\n            t=20  #top margin\n      ))\n    \n    return fig","8a2cc10b":"fig = plot_clusters(X, range(6), \"recurrencia\", \"Violin\", cluster_info)\nfig.update_xaxes(title=\"Recurrencia\")\nfig.show()","9564f35f":"fig = plot_clusters(X, range(6), \"active_customer\", \"Box\", cluster_info)\nfig.update_xaxes(title=\"Active Customer\")\nfig.show()","1c983705":"fig = plot_clusters(X, range(6), \"unique_altas\", \"Box\", cluster_info)\nfig.update_xaxes(title=\"Altas distintas\")\nfig.show()","9166bbaa":"fig = plot_clusters(X, range(6), \"tiempo_entre_compras\", \"Violin\", cluster_info)\nfig.update_xaxes(title=\"Tiempo entre compras\")\nfig.show()","3461b4b0":"cluster=5\nX_ = X[X['cluster']==cluster]\nX_melted = pd.melt(X_, value_vars=['ahorros_altas','financiacion_altas','cuenta_altas'])\nX_melted","ff64f8f1":"fig = go.Figure()\n#ahorros_altas financiacion_altas cuenta_altas\n\nfig.add_trace(go.Violin(x=X_melted[X_melted['variable'] == 'ahorros_altas']['variable'],\n                            y=X_melted[X_melted['variable'] == 'ahorros_altas']['value'],\n                            legendgroup='Ahorros', scalegroup='Ahorros', name='Ahorros',\n                            line_color='orange')\n             )\nfig.add_trace(go.Violin(x=X_melted[X_melted['variable'] == 'financiacion_altas']['variable'],\n                            y=X_melted[X_melted['variable'] == 'financiacion_altas']['value'],\n                            legendgroup='Financiacion', scalegroup='Financiacion', name='Financiacion',\n                            line_color='blue')\n             )\nfig.add_trace(go.Violin(x=X_melted[X_melted['variable'] == 'cuenta_altas']['variable'],\n                            y=X_melted[X_melted['variable'] == 'cuenta_altas']['value'],\n                            legendgroup='Cuentas', scalegroup='Cuentas', name='Cuentas',\n                            line_color='red')\n             )\n\nfig.update_traces(box_visible=True, meanline_visible=True)\nfig.update_layout(violinmode='group')\nfig.show()","f85d719c":"fig = go.Figure()\n\nfor cluster in range(7):\n    X_ = X[X['cluster']==cluster]\n    X_melted = pd.melt(X_, value_vars=['ahorros_altas','financiacion_altas','cuenta_altas'])\n    \n    fig.add_trace(go.Violin(x=X_melted[X_melted['variable'] == 'ahorros_altas']['variable'],\n                            y=X_melted[X_melted['variable'] == 'ahorros_altas']['value'],\n                            legendgroup='Ahorros', scalegroup='Ahorros', name='Ahorros'+str(cluster),\n                            line_color='orange')\n             )\n    fig.add_trace(go.Violin(x=X_melted[X_melted['variable'] == 'financiacion_altas']['variable'],\n                                y=X_melted[X_melted['variable'] == 'financiacion_altas']['value'],\n                                legendgroup='Financiacion', scalegroup='Financiacion', name='Financiacion',\n                                line_color='blue')\n                 )\n    fig.add_trace(go.Violin(x=X_melted[X_melted['variable'] == 'cuenta_altas']['variable'],\n                                y=X_melted[X_melted['variable'] == 'cuenta_altas']['value'],\n                                legendgroup='Cuentas', scalegroup='Cuentas', name='Cuentas',\n                                line_color='red')\n                 )\n\n#fig.update_traces(box_visible=True, meanline_visible=True)\nfig.update_layout(violinmode='group')\nfig.show()","e388b2f1":"## Pa\u00eds (country_id) <a class=\"anchor\" id=\"18\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nPa\u00eds de residencia del cliente. Dado que tan s\u00f3lo hay 146 clientes que no son de Espa\u00f1a, y un cluster conteniendo tan pocos registros no es suficientemente grande, se opta por no entrar esta variable al modelo.","222633e9":"Realizamos un backup del dataframe y nos quedamos con el nombre de las columnas y los \u00edndices.","e30e8414":"La interpretaci\u00f3n que se puede realizar del Diagrama de Silhouette es que cuando los valores del coeficiente est\u00e1n por debajo del score medio representado con la l\u00ednea vertical roja entrecortada, indica que el cluster no es id\u00f3neo dado que muchas de sus instancias est\u00e1n bastante cerca de las de otros clusters. Idealmente una clusterizaci\u00f3n ideal, todos los cuchillos deber\u00edan sobrepasar por la derecha la l\u00ednea vertical.\n\nEn nuetro caso el diagrama de Silhouette indica que los clusters no son los id\u00f3neos, esto muy probablemente se debe a que hay un cluster enrome con todos los cleintes que no realizaron ninguna compra. ","f720afb3":"El siguiente paso es agrupar la informaci\u00f3n de los productos por tipo de producto, es decir agrupamos las cuentas en cuenta_altas, cuenta_bajas y cuenta_cobros; reduciendo as\u00ed el n\u00famero de columnas. ","6a060bfa":"## Provincia (region_code) <a class=\"anchor\" id=\"17\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nEsta variable indica la provincia de residencia de los clientes de Espa\u00f1a. Dado que el algoritmo que se desea implementar KMeans es geom\u00e9trico, introducir tantas columnas como provincias tenemos (One Hot Encoding) no es para nada id\u00f3neo, dado que al augmentar las dimensiones aumentamos la distancia y los puntos cada vez est\u00e1n m\u00e1s alejados (<a href=\"https:\/\/www.iartificial.net\/la-maldicion-de-la-dimension-en-machine-learning\/\">Maldici\u00f3n de la dimensionalidad<\/a>). La distancia media aumenta con el n\u00famero de dimensiones.\n\nEs por ello que si se desea entrar esta variable en el modelo deber\u00e1 trabajarse con una sola columna y que \u00e9sta ofrezca informaci\u00f3n de manera ordinal o con sentido ordinal, es decir que para valores m\u00e1s grandes o m\u00e1s peque\u00f1os se espere un comportamiento o se tenga un significado para el negocio. Es por ello que hacer un LabelEncoder tampoco es interesante, porque qu\u00e9 diferencia de significado hay entre la regi\u00f3n 28: Madrid y la 8: Barcelona; con la 13: Ciudad Real que est\u00e1 en medio pero dista mucho de la cantidad de clientes en las otras dos.\n\nSe plantean 2 posibilidades para entrar esta variable al modelo:\n- Codificar como 1 las regiones que m\u00e1s clientes tienen y el resto como 0. De esta manera el modelo diferenciar\u00e1 los clientes seg\u00fan si provienen de localidades como Madrid, Barcelona, Valencia, Sevilla... \n- Realizar un Frequency Encoding. Tiene el mismo fin, pero permite aumentar a\u00fan m\u00e1s la diferencia entre los clientes de cada provincia.","1344974b":"### Tiempo entre compras  <a class=\"anchor\" id=\"115\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nEn esta secci\u00f3n final se calcula el tiempo entre compras para cada cliente. Al igual que en permanencia se escoge la media. En este caso hay que tener en cuenta que las compras pueden producirse en varios productos, por lo que el tiempo entre compras puede ser entre compras dentro de un mismo producto o entre compras de distintos productos.\n\nPrimeramente, se genera una dataset de test y se comprueba que los resultados obtenidos son satisfactorios. ","405cfd93":"# Data Cleaning <a class=\"anchor\" id=\"2\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nFinalmente se limpia el dataframe con los datos que se desean entrar al modelo. Primeramente se eliminan todas las columnas de altas, bajas y cobros por productos. Como se realizaba anteriormente, estas columnas son agrupadas por tipo de producto. \n\nY seguidamente se computan las bajas totales siendo esto la suma de todas las bajas en todos los productos o la suma de todas las bajas en todos los tipo de producto. \n","43d8e05b":"Si se observa la gr\u00e1fica del codo, el n\u00famero \u00f3ptimo de clusters ser\u00eda de 5. Pese a que en el enunciado del ejercicio se suger\u00edan 7 o 8 clusters se opta por 6 clusters por varias razones. Con 7 clusters, aparecen clusters de tan s\u00f3lo 3500 clientes lo que indica que es un cluster muy particular y que probablemente este pueda ser combinado con otro por las semejanzas que tienen. Y la principal raz\u00f3n de escoger 6 es que los clusters que aparecen tienen un buen significado y resultan interesantes para el negocio.\n\nTras m\u00e1s de 10 clusterizaciones con diferentes variables, este resultado se considera como satisfactorio y creemos que estos grupos permiten a los empleados de la compa\u00f1\u00eda entender mejor a sus clientes. ","c86fe85b":"Se debe prestar atenci\u00f3n para el caso de los cobros o altas cobradas. Dado que se debe cumplir que la permanencia sea de almenos 3 meses para as\u00ed poder cobrar el producto al cliente y que en la primera partici\u00f3n se desconoce el estado anterior de los productos, se definen 3 posibles casos que deben ser comprobados. ","5a11c15c":"A continuaci\u00f3n se recuperan algunas columnas del dataframe origen con el fin de poder visualizar diferencias extras entre los diferentes clusters. ","9ccb34ed":"### N\u00famero de productos distintos contratados <a class=\"anchor\" id=\"114\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nTambi\u00e9n se pueden obtener el n\u00famero de productos totales contratados por cada cliente.","eb0d6065":"Guardamos el assigment de cada cliente a cada cluster en un csv, para as\u00ed poder leer los resultados en otra tarea o notebook. ","2dccfae8":"En el c\u00f3digo anterior se testea un caso y se observa que el cliente estuvo 3 y 4 meses con el producto d\u00e1ndose 2 veces de alta, la media es 3.5. ","51d5ebdb":"Finalmente, generamos el df_final con las columnas seleccionadas.","5141f761":"## G\u00e9nero (gender) <a class=\"anchor\" id=\"16\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nNo la utilizamos, dado que queremos realizar una clusterizaci\u00f3n lo m\u00e1s \u00e9tica posible y no hacer distinciones ni por sexo ni raza. ","96e656a7":"Guardamos los resultados de salario en el dataframe. C\u00f3mo se puede observar ya no hay nulos. ","f259d098":"- **Cluster 1: (Los Fosa de las Marianas)** (296891) \nEste cluster representa un gran reto para todo el equipo de easyMoney, al contener el 65% de nuestros clientes captados de manera inicial pero sin conversi\u00f3n ya que no completan los tiempos de permanencia para poder facturar el producto contratado. \nTenemos la ventaja de haberlos captado inicialmente y debemos identificar por que no compran, cual fue su motivaci\u00f3n inicial en afiliarse y por que no se cumplieron sus expectativas? Debemos lograr incrementar la tasa de conversi\u00f3n en este cluster, por lo que puede implicar un caso de estudio espec\u00edfico. \n\n    La Fosa de las Marianas es un sitio en el \u00f3ceano pac\u00edfico que a\u00fan no ha podido ser explorado. Uno de los pocos sitios a los que el ser humano no ha conseguido poder explorar a\u00fan.\n\n- **Cluster 2: (Los Once-in-a-lifetime)** (109899)\nClientes que se dieron de alta con una cuenta (em_acount) y no volvieron a contratar nada m\u00e1s. Su aporte a los ingresos ha sido muy alto pero el inter\u00e9s por los productos de la compa\u00f1\u00eda es pr\u00e1cticamente nulo.\n\n\n- **Cluster 3: (Los Curiosos)** (15698)\nClientes que les gusta darse de alta en muchos productos y en productos de tipos distintos. Su conversi\u00f3n es bastante alta del 63%. Es un grupo que gasta alrededor de 70 euros de media.\n\n- **Cluster 4: (Los Pretenciosos)** (14308)\nClientes que se dan de alta en los productos m\u00e1s caros, los de financiaci\u00f3n y que no terminan pag\u00e1ndose. Es el grupo con peor conversi\u00f3n. Pese a no comprarlos siempre vuelven a solicitarlos m\u00e1s tarde, tienen una alta recurrencia. \n\n- **Cluster 5: (Los M\u00e1s-vale-mal-conocido)** (13984)\nClientes que siempre contratan los mismos productos varias veces, especialmente los de ahorros. Es el grupo con la recurrencia m\u00e1s alta, han contratado 4 veces el mismo producto de media. Es el grupo que m\u00e1s gasta de media, su conversi\u00f3n es del 47% y es el grupo m\u00e1s peque\u00f1o.\n\n- **Cluster 6: (Los Comprometidos)** (5593)\nClientes que tan s\u00f3lo se han dado de alta en 2 productos de media, pero los han terminado pagando. Tienen una conversi\u00f3n muy alta del 73%. \n\n\n**Conclusiones**\n\nClusters 3 y 6, ser\u00e1n los elegidos para nuestra campa\u00f1a inicial de email marketing, debido a su engagement m\u00e1s elevado en una variedad de productos m\u00e1s amplia, lo cual nos permite elaborar un modelo de recomendaci\u00f3n mucho m\u00e1s preciso lo que nos puede permitir un aumento de facturaci\u00f3n a corto plazo.\n\nLos cluster 1 y 2 representan el 89% de nuestra BD de clientes con 406790. Para estos cluster pudi\u00e9ramos hacer, en una segunda etapa, un an\u00e1lisis espec\u00edfico para recuperarlos \/ aumentar su engagement. \n\nEl an\u00e1lisis deber\u00e1 incluir informaci\u00f3n demogr\u00e1fica externa ya que su hist\u00f3rico con nosotros no aporta mucha informaci\u00f3n \u00fatil, sin embargo pudi\u00e9ramos plantear de manera inicial una recomendaci\u00f3n basada en popularidad de productos, para lo cual tendr\u00edamos los siguientes 2 escenarios:\n- Cluster 1: Fidelizarlos con nuestro producto estrella \u201cem_account\u201d el cual suele ser el primer producto contratado por el resto de nuestros clientes. Con un 20% de engagement en este producto para este cluster puede significar un aumento de ingresos de aproximadamente 500k \u20ac\n- Cluster 2: Ya que estos clientes se han dado de alta en su mayor\u00eda con productos de categor\u00eda \u201cCuentas\u201d podr\u00edamos enfocar nuestro esfuerzo en aumentar el engagement en productos de \u201cfinanciaci\u00f3n\u201d como nuestro segundo producto m\u00e1s vendido \u201cdebit_card\u201d. Siguiendo el mismo objetivo de crecimiento del 20% esto representar\u00eda ingresos appx por 1.3M \u20ac\n\nPor lo tanto con un potencial de casi 2M \u20ac y 400k registros de clientes, es interesante como un siguiente proyecto a enfocarse.\n\nPara los Cluster 4 y 5 tenemos una poblaci\u00f3n menor, solo 28292 clientes en total. Sin embargo se enfocan en los productos de mayor valor (Ahorros y Financiaci\u00f3n) por lo que si aumentamos su engagement en un 20% representar\u00eda 280k \u20ac m\u00e1s de ingresos.  ","041fddc8":"# Preparaci\u00f3n del dashboard <a class=\"anchor\" id=\"4\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nFinalmente en este apartado se obtienen una serie de funciones y gr\u00e1ficas que permitir\u00edan visualizar los resultados en el dashboard. Primero, se asigna un nombre y un color a cada cluster y se guarda en un diccionario.\n\nSeguidamente se realiza una funci\u00f3n para visualizar la informaci\u00f3n de los diferentes clusters. ","765faf6d":"Observamos la distribuci\u00f3n de cada una de las variables. Se puede observar que en muchas columnas existen outliers. ","121e4fb9":"## Fallecido (deceased) <a class=\"anchor\" id=\"12\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nAl tener tan pocos valores distintos (es decir pocos fallecidos), se decide no entrar esta variable en el modelo.","c56a4109":"## Selecci\u00f3n del n\u00famero \u00f3ptimo de clusters <a class=\"anchor\" id=\"31\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nEl objetivo de esta secci\u00f3n es encontrar el n\u00famero de clusters \u00f3ptimo consultando la Curva del Codo (o Elbow Curve). Para ello primero se hace un fit del dataset utilizando el pipe. ","296c9a23":"# Tarea 2: Segmentaci\u00f3n\n\n<a href=\"https:\/\/github.com\/carlosperez1997\/proyecto_easyMoney\/blob\/main\/README.md\" target=\"_blank\">Volver a P\u00e1gina Principal<\/a>\n\nEl objetivo de esta tarea es segmentar los clientes en diferentes grupos con el fin de poder conocer mejor a nuestra base de clientes para orientar nuestra actividad comercial. \n\nEstos grupos conocidos como clusters deber\u00e1n ser homog\u00e9neos y distintos entre ellos. En otras palabras los miembros dentro de cada grupo deber\u00e1n ser similar entre ellos pero a la vez muy distintos de los miembros de otro grupo (referencia: <a href=\"https:\/\/www.amazon.es\/Hands-Unsupervised-Learning-Using-Python\/dp\/1492035645\">Ankur A. Patel, Hands-On Unsupervised Learning Using Python<\/a>).\n\nLa informaci\u00f3n y descripci\u00f3n de cada cluster deber\u00e1 ser observada en un <a href=\"http:\/\/carlosperez1697.pythonanywhere.com\/segmentacion\">dashboard<\/a> donde se puedan apreciar las diferencias entre cada uno de los clusters, con el fin de poder extraer conclusiones.","0b746ee4":"El siguiente paso es observar que las variables de un mismo tipo de producto est\u00e1n muy correlacionadas. Es normal que si un cliente se da mucho de alta en ahorros, se d\u00e9 mucho de baja en ahorros, y que los cobros tambi\u00e9n tengan una correlaci\u00f3n fuerte. ","dfcce393":"Se eval\u00faa el correcto funcionamiento de la funci\u00f3n con un dataset de test. ","e6d92c54":"Esta gr\u00e1fica es guardada como png y ser\u00e1 incluida en el dashboard de la Tarea 1. ","fc22b9a7":"Hacemos un join o merge de los 3 datasets uniendo por pk_cid y por pk_partition. Lo que resulta en un dataset de . Seguidamente se reduce el tama\u00f1o del dataset codificando como boolean las variables de productos que toman valores de 0 y 1. ","a8c86850":"## Fecha de entrada (entry_date) <a class=\"anchor\" id=\"15\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nFecha en la que realiz\u00f3 la primera contrataci\u00f3n a trav\u00e9s de easyMoney. De esta variable nos quedamos con el a\u00f1o de entrada. ","5f77535f":"## Domiciliaciones (payroll) <a class=\"anchor\" id=\"19\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nEsta variable indica si el cliente tiene domiciliada alguna de las cuentas de la compa\u00f1\u00eda, es decir que su salario se ingresa en alguna de nuestras cuentas. Dado que no hay ning\u00fan cliente con la cuenta domiciliada esta variable no se entra en el modelo. ","7d475a3d":"# Model selection and construction <a class=\"anchor\" id=\"3\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nEn este apartado se selecciona y construye el modelo para permite realizar una segmentaci\u00f3n de la cartera de clientes. En nuestro caso se opta por el algoritmo KMeans (KMedias en espa\u00f1ol). <a href=\"https:\/\/es.wikipedia.org\/wiki\/K-medias#:~:text=K%2Dmedias%20es%20un%20m%C3%A9todo,utilizado%20en%20miner%C3%ADa%20de%20datos.\">KMedias<\/a> es un m\u00e9todo de agrupamiento, que tiene como objetivo la partici\u00f3n de un conjunto de n observaciones en k grupos en el que cada observaci\u00f3n pertenece al grupo cuyo valor medio es m\u00e1s cercano. \n\nPara la construcci\u00f3n del modelo se construye un pipeline. Es por ello que crear una clase con fit y transform para pasar un Array a DataFrame. ","86a732bd":"Seguidemente se presentan las versiones _all y se calculan en el dataset de clientes. Este proceso suele tardar bastante por lo que se guarda en un pickle y as\u00ed no se debe ejecutar esta parte del c\u00f3digo cada vez que queramos obtener un modelo.","b4c8896c":"## Tabla de Contenidos <a class=\"anchor\" id=\"0\"><\/a>\n\n1. [Data Preparation](#origin) <br> \n    1.1. [Salario (salary)](#11) <br> \n    1.2. [Fallecido (deceased)](#12) <br> \n    1.3. [Cliente activo (active_customer)](#13) <br> \n    1.4. [Edad (age)](#14) <br> \n    1.5. [Fecha de entrada (entry_date)](#15) <br> \n    1.6. [G\u00e9nero (gender)](#16) <br> \n    1.7. [Provincia (region_code)](#17) <br> \n    1.8. [Pa\u00eds (country_id)](#18) <br> \n    1.9. [Domiciliaciones (payroll)](#19) <br> \n    1.10. [Productos](#110) <br> \n    - [Altas, bajas y cobros](#111) <br> \n    - [Customer Lifetime Value](#112) <br> \n    - [Permanencia](#113) <br> \n    - [N\u00famero de productos contratados](#114) <br> \n    - [Tiempo entre compras](#115) <br> \n2. [Data Cleaning](#2) <br> \n    2.1. [Feature Selection](#21) <br> \n3. [Model selection and construction](#3) <br> \n    3.1. [Selecci\u00f3n del n\u00famero \u00f3ptimo de clusters](#31) <br> \n    3.2. [Clusterizaci\u00f3n y definici\u00f3n de los clusters](#32) <br> \n    3.3. [Evaluaci\u00f3n de la clusterizaci\u00f3n](#33) <br> \n4. [Preparaci\u00f3n del dashboard](#4) <br> \n5. [Conclusiones](#5) <br>\n","3f894ac4":"## Productos <a class=\"anchor\" id=\"110\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nExisten 14 productos de los que se pueden obtener diferentes m\u00e9tricas como altas, bajas, n\u00famero de cobros, permanencia en cada uno de ellos, etc. C\u00f3mo se comentaba anteriormente con el fin de evitar la maldici\u00f3n de la dimensionalidad cu\u00e1ntas menos columnas mejor por lo que en esta secci\u00f3n se decidir\u00e1 que variables resumen mejor el comportamiento de los clientes con los productos de la empresa. \n\n- cantidad de altas (17)\n- cantidad de bajas sin cobro \/ altas que no han sido cobradas (17)\n- cantidad de bajas con cobro (17)\n\n# Por tipo de producto\n\n- Cantidad de altas (3)\n- Cantidad de bajas (3)\n\n# Customer Lifetime Value\n\n- Gastos totales de cada cliente (1)\n- Gastos totales en cada tipo de producto (3)\n- Numero de productos contratados en su historia (1)\n\n","9b9d0ff0":"**Importamos librer\u00edas y dataset**","766aa1c7":"## Edad (age) <a class=\"anchor\" id=\"14\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nEsta variable nos ofrece la edad del cliente, variable que podr\u00eda ser interesante para la clusterizaci\u00f3n. Nos quedamos con la \u00faltima o con la edad m\u00e1s grande, en el caso de que esta var\u00ede. ","e8aadbe8":"## Cliente activo (active_customer) <a class=\"anchor\" id=\"13\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nEsta variable *active_customer* indica si el cliente ha estado activo en los \u00faltimos 3 meses en las apps y la plataforma. Dado que tenemos la informaci\u00f3n de como m\u00e1ximo 17 hist\u00f3ricos, hacer la media indica el grado de actividad de un cliente en toda su estancia en la empresa. \n\nEsta variable es guardada en el modelo de clustering. ","c96fda6e":"## Clusterizaci\u00f3n y definici\u00f3n de los clusters <a class=\"anchor\" id=\"32\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nTras seleccionar el n\u00famero de clusters ideal, se realiza la clusterizaci\u00f3n y se visualizan los clusters. En este caso aparece un cluster con la mayor\u00eda de clientes que son los que no han comprado nada en las particiones que se tienen. ","96a03c39":"La siguiente pregunta es saber qu\u00e9 hacemos con los clientes que no han comprado nada, que tiempo de compras establecemos. En este caso se opta por el valor m\u00e1ximo de 17 meses. ","9b2cbe43":"## Feature selection <a class=\"anchor\" id=\"21\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nLa segmentaci\u00f3n de clientes consiste en agrupar los clientes en diferentes segmentos, cada uno de ellos caracterizados por unas ciertas condiciones o propiedades que describen el comportamiento de los Clientes que pertenecen a ellos.\n\nHay infinitas caracter\u00edsticas del Cliente que se pueden usar para hacer una Segmentaci\u00f3n. Sin embargo, las caracter\u00edsticas \u00f3ptimas y la metodolog\u00eda a seguir depender\u00e1n mucho del objetivo de negocio que se persiga con dicha Segmentaci\u00f3n. Por lo tanto, no hay un \u00fanico m\u00e9todo correcto ni una \u00fanica manera para hacer una <a href=\"https:\/\/www.predictland.com\/big_data_segmentacion_clientes\/\">Segmentaci\u00f3n de Clientes<\/a>.\n\nLas variables que se entrar\u00e1n al modelo determinar\u00e1n las diferencias entre los grupos. Es por ello que para realizar una segmentaci\u00f3n efectiva (que permitan conocer mejor a nuestra base de clientes para orientar nuestra actividad comercial), variables como edad o provincia\/regi\u00f3n no aportan conocimiento sobre c\u00f3mo se comporta el cliente con nuestra empresa. Los segmentos ser\u00e1n m\u00e1s efectivos si se pueden vincular directamente con algo concreto, como por ejemplo con el recorrido del Cliente, con la afinidad de producto, etc.\n\nComo se propone en <a href=\"https:\/\/www.predictland.com\/big_data_segmentacion_clientes\/\">Segmentaci\u00f3n de Clientes<\/a>, podemos adoptar el an\u00e1lisis RFM que trata de segmentar los Clientes en funci\u00f3n de tres variables: Recency, Frequency y Monetary. Es decir, dado un Cliente en particular, las variables a analizar son, por un lado, los d\u00edas que han pasado desde la \u00faltima compra, por otro la frecuencia con la que el Cliente ha comprado productos en un periodo determinado, y por otro el valor monetario agregado que el Cliente se ha gastado.\n\n- Frequency: la cantidad de veces que un Cliente ha visitado la tienda durante el periodo analizado.\n- Monetary: el valor (\u20ac) agregado que se ha gastado el Cliente en este periodo.\n- Recency: el n\u00famero de d\u00edas que han pasado desde la \u00faltima visita.\n\nTras haber probado con varios clusters con diferentes variables, se vio que dependiendo de las variables que se entraban pod\u00edan aparecer 2 clusters iguales pero que se diferencian \u00fanicamente por la edad, por la regi\u00f3n o por su actividad en las apps. El enfoque comercial para ellos ser\u00e1 pr\u00e1cticamente id\u00e9ntico, \u00fanicamente se diferencian por d\u00f3nde residen o la edad que tienen; lo que no es interesante para el negocio. \n\nFinalmente, las 6 variables seleccionadas son:\n\n- ahorros_altas\n- financiacion_altas\n- cuenta_altas\n- unique_altas\n- conversion\n- recurrencia\n\nLas variables de *ahorros_altas*, *financiacion_altas* y *cuenta_altas* indican las preferencia e intereses de los usuarios, *conversion*, *recurrencia* y *unique_altas* har\u00e1n referencia al compromiso, inter\u00e9s general e interacci\u00f3n con los productos de easyMoney. \n\nSe espera obtener diferentes clusters cada uno con ciertas caracter\u00edsticas que lo diferencian del resto. ","cda256b0":"# Data Preparation <a class=\"anchor\" id=\"origin\"><\/a>\n\nEn esta secci\u00f3n se preparan y limpian los datos para ser entrados en el modelo de segmentaci\u00f3n.\n\n## Salario (salary) <a class=\"anchor\" id=\"11\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nEsta variable tiene muchos NaNs y su veracidad es cuestionable. El salario fue preguntando en una encuesta y seg\u00fan se tiene entendido no se comprob\u00f3, por lo que no se puede asegurar que estos valores sean ver\u00eddicos. C\u00f3mo se determina a continuaci\u00f3n la mediana de salario (de la unidad familiar) de los clientes con cuentas Universitarias es de alrededor de 80.000 euros, siendo Espa\u00f1a este resultado dista mucho de la realidad. \n\nPese a esto, se probar\u00e1 realizar un cluster con esta variable. Para ello primero hay que darle un valor a los NaNs, una opci\u00f3n ser\u00eda realizar un KNNImputer pero dada la larga cantidad de valores nulos y las dimensiones de la matriz, este proceso es muy largo por lo que se opta por poner la mediana de los salarios de cada segmento y para los que no tienen segmento la mediana de salarios global. ","05c506fe":"### Altas, bajas y cobros <a class=\"anchor\" id=\"111\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nEn este apartado se crean una serie de funciones que permiten obtener las altas, bajas y cobros de cada cliente. Con el fin de ilustrar el funcionamiento y comprobar que el resultado que devuelve es correcto, se han desarollado las funciones *determinar_altas*, *determinar_bajas* y *determinar_cobros*. Las versiones *_all* incluyen estos c\u00e1lculos pero lo calculan en todo el dataset. ","2d5c1c64":"# Conclusiones <a class=\"anchor\" id=\"5\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nTras haber realizado la Tarea 2, donde el objetivo era realizar una segmentaci\u00f3n de nuestra cartera de clientes los resultados son satisfactorios. Se ha observado que la parte m\u00e1s importante en la segmentaci\u00f3n es la preparaci\u00f3n de los atributos y qu\u00e9 significado tienen estos para el negocio. En nuestro caso, el objetivo de la segmentaci\u00f3n era obtener diferentes grupos seg\u00fan su actividad comercial. \n\nEs por ello que se seleccionan cuidadosamente que variables se introducen al modelo de clusterizaci\u00f3n. Se probaron m\u00e1s de 10 clusters y este fue el resultado que mejor sentido y significado para el negocio ten\u00eda. En las primeras iteraciones, se introdujeron variables como edad o provincia, y aparec\u00edan 2 clusters id\u00e9nticos: mismos gustos e intereses, misma cantidad gastada pero que \u00fanicamente se diferenciaban que uno viv\u00eda en Madrid y otro no; lo que resultaba ser un poco estridente. \n\nFinalmente se escogieron 6 clusters porque los clusters que aparecen son bastante diferentes en cuanto a su significado, aunque el diagrama de Silhouette indique lo contrario. Esta segmentaci\u00f3n podr\u00e1 ser utilizada en otras tareas.","8d0a69c9":"Una vez comprobado el correcto funcionamiento, se calcula el tiempo entre compras de cada cliente considerando todos los productos. Para ello se hace una copia del dataframe original y por otro lado tambi\u00e9n se calcula cu\u00e1l es la primera partici\u00f3n de cada cliente.","ab83df37":"## Evaluaci\u00f3n de la clusterizaci\u00f3n <a class=\"anchor\" id=\"33\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nUno podr\u00eda decir que el mejor modelo es el que menor inercia tiene, sin embargo a medida que aumentan los clusters obviamente la suma de las distancias al cluster m\u00e1s cercano disminuyen, es por ello que se visualiza la Elbow Curve. Esta t\u00e9cnica es un poco vulgar, existe otra manera pero que resulta ser mucho m\u00e1s demandante computacionalmente. <a href=\"https:\/\/www.amazon.es\/Hands-Machine-Learning-Scikit-Learn-TensorFlow\/dp\/1491962291\">Aur\u00e9lien G\u00e9ron. \u201cHands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\u201d<\/a>. \n\nEsta t\u00e9cnica es el Silhouette score, que es la media del coeficiente silhouette sobre todos los registros. El coeficiente silhouette de un registro es igual a (b \u2013 a) \/ max(a, b), donde a es la distancia media a los registros en ese cluster (the mean intra-cluster distance) y b es la distancia media al cluster m\u00e1s pr\u00f3ximo (es decir la distancia media los registros del siguiente cluster, siendo el registro que minimza b exluyendo las instancias en el mismo clsuter). \n\nEl coeficiente de silhouette puede tomar valores entre -1 y +1. Un coeficiente cercano a +1 significa que el registro est\u00e1 bien dentro de su cluster y est\u00e1 lejano a otros clusters, mientras que un coeficiente cercano a 0 indica que el registro est\u00e1 cercano a la frontera del cluster, mientras que un valor cercano a -1 indica que el registro ha sido asignado al cluster equivocado. \n\nA continuaci\u00f3n se obtiene el Diagrama de Silhouette; cada cuchillo indica cada cluster, la altura de cada uno indica la cantidad de registros que el cluster contiene, y su anchura representa los coeficientes de silhouette ordenador (cu\u00e1nto m\u00e1s ancho mejor). La l\u00ednea entre cortada indica la media del coeficiente de silhouette. ","5e1f2861":"### Customer Lifetime Value <a class=\"anchor\" id=\"112\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nC\u00f3mo se comenta en la <a href=\"https:\/\/www.inboundcycle.com\/blog-de-inbound-marketing\/customer-lifetime-value-cltv-como-calcularlo\">Tarea 1 (OJO! PONER LINK BIEN!)<\/a>, el <a href=\"https:\/\/www.inboundcycle.com\/blog-de-inbound-marketing\/customer-lifetime-value-cltv-como-calcularlo\">Customer Lifetime Value<\/a> (CLV) o valor del tiempo de vida del cliente es un pron\u00f3stico sobre la cantidad de dinero que espera recibir la empresa por parte de un usuario, durante todo el tiempo en que este siga siendo su cliente.\n\nEn esta secci\u00f3n se calcula el dinero gastado por cada cliente. N\u00f3tese que esta variable lleva oculta parte de la informaci\u00f3n de los cobros de los clientes. Por lo que m\u00e1s adelante se determinar\u00e1 que variable entrar si la cantidad de cobros totales, la cantidad de cobros en cada tipo de producto o el dinero gastado.","9c9693a5":"### Permanencia <a class=\"anchor\" id=\"113\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nSimilarmente a las altas, bajas y cobros es posible determinar la permanencia de cada cliente en cada producto. Sin embargo, esta m\u00e9trica presenta una gran desventaja respecto a las otras. C\u00f3mo guardo una permanencia de un cliente que se ha dado de alta 2 veces con un mismo producto y c\u00f3mo contabilizo las permanencias de varios productos. \n\nEs por ello, que se define la permanencia como la suma de permanencias medias en un mismo producto y en todos los productos. N\u00f3tese que tan s\u00f3lo se contabilizan los productos que se han contratado. \n\n$$permanencia = \\frac{ \\frac{\\sum perm_i} {m}  }\u00a0{ N } $$\n\nSiendo $perm_i$ el n\u00famero de meses con el producto, m el n\u00famero de altas en ese producto y N el n\u00famero de productos dados de alta alguna vez. ","a7090c78":"Inicializamos un nuevo dataframe en el que se ir\u00e1 almacenando las columnas que se desean entrar en el modelo de clustering. "}}