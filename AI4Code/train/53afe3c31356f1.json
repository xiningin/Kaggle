{"cell_type":{"36c581f6":"code","6239c4c4":"code","5b4b970a":"code","44a1f0c0":"code","73e9cb31":"code","450b5821":"code","15590785":"code","248e929d":"code","af6bca63":"code","dc708293":"code","15b3dde7":"code","7e013969":"code","7025bb43":"code","c1af788e":"code","cd0124a7":"code","06964569":"code","625b8584":"code","55d3b953":"code","caf081a2":"code","542fe95e":"code","dc1f99f9":"code","02203b62":"code","7f95a520":"code","7530b3e8":"code","1eacc656":"code","0dfb9003":"code","692b6a15":"code","8c71d38b":"code","7feda867":"code","387542d3":"code","b941123f":"code","119f57e5":"code","47f3b97b":"code","648fea27":"code","4d9da99c":"code","d4b5328a":"code","87a388fd":"markdown","e4c716b1":"markdown","fded5ad2":"markdown","90e18e93":"markdown","47592b3a":"markdown","0930231b":"markdown","89642082":"markdown","1e54e406":"markdown","58cb5588":"markdown","ba19fd79":"markdown","3a4a4914":"markdown","9f075d17":"markdown","8bb913a8":"markdown","47ecef03":"markdown","f889cc1a":"markdown","39013036":"markdown","76e1d8b2":"markdown","a082abe1":"markdown","32e2cdfb":"markdown","582f1b9e":"markdown","ee3f8829":"markdown","b0cbeafd":"markdown","357b554d":"markdown","faaa298c":"markdown","b4325c34":"markdown","0a567ee0":"markdown","7e11dad9":"markdown","5c3e815b":"markdown","b0b1af2b":"markdown","de01cfb8":"markdown","28ef6127":"markdown","7a586d51":"markdown"},"source":{"36c581f6":"import os\nimport glob\nimport copy\nimport time\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm.notebook import tqdm\nfrom collections import namedtuple\nimport SimpleITK as sitk\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torch.optim as optim","6239c4c4":"df_annotations = pd.read_csv('\/kaggle\/input\/luna16\/annotations.csv')\ndf_annotations.head()","5b4b970a":"df_annotations.shape","44a1f0c0":"df_candidates = pd.read_csv('\/kaggle\/input\/luna16\/candidates_V2\/candidates_V2.csv')\ndf_candidates.head()","73e9cb31":"df_candidates.shape","450b5821":"print(f'Total annotations: {df_annotations.shape[0]}, Unique CT scans: {len(df_annotations.seriesuid.unique())}')\nprint(f'Total candidates: {df_candidates.shape[0]}, Unique CT scans: {len(df_candidates.seriesuid.unique())}')","15590785":"# The `diameters` dict will have each `seriesuid` as a key.\n# The value will be a list of all center coorinates in the CT scan with that `seriesuid`\ndiameters = {}\n\n# Loop through every annotation\nfor _, row in df_annotations.iterrows():\n    \n    # Create a tuple to represent the center\n    center_xyz = (row.coordX, row.coordY, row.coordZ)\n    \n    # Append the center to the corresponding `seriesuid`\n    diameters.setdefault(row.seriesuid, []).append(\n        (center_xyz, row.diameter_mm)\n    )","248e929d":"%%time\n\n# Using a namedtuple makes it easy to access values in a tuple\n# using indexes or field names\nCandidateInfoTuple = namedtuple(\n    'CandidateInfoTuple',\n    ['is_nodule', 'diameter_mm', 'series_uid', 'center_xyz']\n)\n\n# A list to store all candidates in the dataset\ncandidates = []\n\nfor _, row in df_candidates.iterrows():\n    \n    # Create a tuple to represent the candidate center\n    # We suffix the name with `_xyz` to make it clear that we're using the\n    # patient coordinate system: http:\/\/dicomiseasy.blogspot.com\/2013\/06\/getting-oriented-using-image-plane.html\n    candidate_center_xyz = (row.coordX, row.coordY, row.coordZ)\n\n    # We begin by assuming the candidate doesn't have a corresponding annotation.\n    # If this is the case, then the candidate will have a diameter of 0.\n    candidate_diameter = 0.0\n    \n    # We then fetch the diameters of the CT scan we're looking at currently,\n    # and loop over them to find a match for the candidate\n    for annotation in diameters.get(row.seriesuid, []):\n        \n        # Extract the center and diameter of the annotation from the tuple\n        annotation_center_xyz, annotation_diameter = annotation\n        \n        # For each of the coordinates - X, Y and Z, we check if\n        # the candidate and the annotation are \"close by\"\n        # (remember the really long and complicated sentence above?)\n        \n        # Since we've stored coordinates as tuples, we can index into them\n        for i in range(3):\n            \n            # Find the absolute difference between the two coordinates\n            delta = abs(candidate_center_xyz[i] - annotation_center_xyz[i])\n            \n            # If the coorindate of the candidate is more than half the radius away,\n            # we don't consider it the same nodule as the annotation we're currently looking at\n            if delta > annotation_diameter \/ 4:\n                    break\n            \n        # The `else` block of a for loop in Python executes if the loop ends \"naturally\"\n        # i.e. if it terminates because all iterations are complete, and not by a break statement\n        # So if we go into this else block, then all 3 coordinates are within half the radius,\n        # and we can consider the candidate and the annotation as the same nodule\n        else:\n            candidate_diameter = annotation_diameter\n            \n            # We don't need to look at any other remaining annotations,\n            # because we've already found a match!\n            break\n            \n            \n    \n    candidates.append(CandidateInfoTuple(\n        bool(row['class']),\n        candidate_diameter,\n        row.seriesuid,\n        candidate_center_xyz\n    ))","af6bca63":"candidates.sort(reverse=True)","dc708293":"# %%time\n\n# from concurrent.futures import ThreadPoolExecutor\n\n# def find_missing_and_multiple(startidx, endidx):\n    \n#     missing_cts = []\n#     multiple_cts = []\n    \n#     for c in tqdm(candidates[startidx: endidx]):\n#         filepaths = glob.glob(f'\/kaggle\/input\/luna16\/subset*\/*\/{c.series_uid}.mhd')\n#         if len(filepaths) == 0:\n#             missing_cts.append(c.series_uid)\n#         elif len(filepaths) > 1:\n#             multiple_cts.append(c.series_uid)\n    \n#     return missing_cts, multiple_cts\n\n\n# all_missing = []\n# all_multiple = []\n\n# with ThreadPoolExecutor() as executor:\n\n#     total = len(candidates)\n#     middle = total \/\/ 2\n#     quarter = middle \/\/ 2\n    \n#     startidx = [0, quarter, middle, middle + quarter]\n#     endidx = [quarter, middle, middle + quarter, total]\n    \n#     for res in executor.map(find_missing_and_multiple, startidx, endidx):\n#         all_missing += res[0]\n#         all_multiple += res[1]\n\n# missing_uids = {uid for uid in all_missing}","15b3dde7":"with open('\/kaggle\/input\/luna16missingcandidates\/missing.txt', 'r') as f:\n    missing_uids = {uid.split('\\n')[0] for uid in f}\n    \nlen(missing_uids)","7e013969":"candidates_clean = list(filter(lambda x: x.series_uid not in missing_uids, candidates))\n\nprint(f'All candidates in dataset: {len(candidates)}')\nprint(f'Candidates with CT scan  : {len(candidates_clean)}')","7025bb43":"candidate = candidates_clean[0]\n\ncandidate","c1af788e":"# Look for the file `<series_uid>.mhd` inside the `subset` folders\nfilepaths = glob.glob(f'\/kaggle\/input\/luna16\/subset*\/*\/{candidate.series_uid}.mhd')\n\n# We removed all candidates that don't have corresponding CT scan files\n# This line is another fail-safe to know when a CT scan doesn't exist\nassert len(filepaths) != 0, f'CT scan with seriesuid {candidate.series_uid} not found!'\n\nfilepaths","cd0124a7":"mhd_file_path = filepaths[0]\n\nmhd_file_path","06964569":"mhd_file = sitk.ReadImage(mhd_file_path)","625b8584":"ct_scan = np.array(sitk.GetArrayFromImage(mhd_file), dtype=np.float32)","55d3b953":"ct_scan.clip(-1000, 1000, ct_scan)","caf081a2":"origin_xyz = mhd_file.GetOrigin()\nvoxel_size_xyz = mhd_file.GetSpacing()\ndirection_matrix = np.array(mhd_file.GetDirection()).reshape(3, 3)","542fe95e":"origin_xyz_np = np.array(origin_xyz)\nvoxel_size_xyz_np = np.array(voxel_size_xyz)","dc1f99f9":"# Convert the coordinates of the center of the candidate\n# from the patient coordinate system to column, row, index\ncri = ((center_xyz - origin_xyz_np) @ np.linalg.inv(direction_matrix)) \/ voxel_size_xyz_np\n\n# Since we'll be using column, row and index values to index into arrays,\n# we round them to the nearest integer.\ncri = np.round(cri)\n\n# Going forward, we'll need the scan to be in the order index, row, column\nirc = (int(cri[2]), int(cri[1]), int(cri[0]))","02203b62":"ct_scan.shape","7f95a520":"dims_irc = (10, 18, 18)","7530b3e8":"# We will create three slices - one for each direction - to use to extract\n# a region of interest from the CT scan\nslice_list = []\n\nfor axis, center_val in enumerate(irc):\n    \n    # Get start and end index for the dimension so that the\n    # nodule center is at the center of the 3d array we extract\n    start_index = int(round(center_val - dims_irc[axis]\/2))\n    end_index = int(start_index + dims_irc[axis])\n\n    # Adjust the indexes if the start_index is out of the CT scan array\n    if start_index < 0:\n        start_index = 0\n        end_index = int(dims_irc[axis])\n    \n    # Do the same check for the end_index\n    if end_index > ct_scan.shape[axis]:\n        end_index = ct_scan.shape[axis]\n        start_index = int(ct_scan.shape[axis] - dims_irc[axis])\n        \n    slice_list.append(slice(start_index, end_index))\n    \ntuple(slice_list)","1eacc656":"ct_scan_chunk = ct_scan[tuple(slice_list)]\nct_scan_chunk.shape","0dfb9003":"# Create a tensor from the NumPy array of the CT scan chunk\nct_scan_chunk_tensor = torch.from_numpy(ct_scan_chunk)\n\n# convert it to a tensor of float32\nct_scan_chunk_tensor = ct_scan_chunk_tensor.to(torch.float32)\n    \n# Add an extra dimension to represent a single channel in the 3d image\nct_scan_chunk_tensor = ct_scan_chunk_tensor.unsqueeze(0)\n\nct_scan_chunk_tensor.shape","692b6a15":"candidate.is_nodule","8c71d38b":"torch.tensor([\n    not candidate.is_nodule,\n    candidate.is_nodule,\n], dtype=torch.long)","7feda867":"# We need to change the imports and use some other libraries for caching to work\n# See: https:\/\/github.com\/deep-learning-with-pytorch\/dlwpt-code\/issues\/27\n\n!pip install diskcache cassandra-driver","387542d3":"# The code in this cell is from the Deep Learning with PyTorch book's GitHub repository\n# https:\/\/github.com\/deep-learning-with-pytorch\/dlwpt-code\/blob\/master\/util\/disk.py\n\n# The imports have slightly been modified to make the code work\n\n\nimport gzip\n\nfrom cassandra.cqltypes import BytesType\nfrom diskcache import FanoutCache, Disk, core\nfrom diskcache.core import io, MODE_BINARY\nfrom io import BytesIO\n\nclass GzipDisk(Disk):\n    def store(self, value, read, key=None):\n        \"\"\"\n        Override from base class diskcache.Disk.\n\n        Chunking is due to needing to work on pythons < 2.7.13:\n        - Issue #27130: In the \"zlib\" module, fix handling of large buffers\n          (typically 2 or 4 GiB).  Previously, inputs were limited to 2 GiB, and\n          compression and decompression operations did not properly handle results of\n          2 or 4 GiB.\n\n        :param value: value to convert\n        :param bool read: True when value is file-like object\n        :return: (size, mode, filename, value) tuple for Cache table\n        \"\"\"\n        # pylint: disable=unidiomatic-typecheck\n        if type(value) is BytesType:\n            if read:\n                value = value.read()\n                read = False\n\n            str_io = BytesIO()\n            gz_file = gzip.GzipFile(mode='wb', compresslevel=1, fileobj=str_io)\n\n            for offset in range(0, len(value), 2**30):\n                gz_file.write(value[offset:offset+2**30])\n            gz_file.close()\n\n            value = str_io.getvalue()\n\n        return super(GzipDisk, self).store(value, read)\n\n\n    def fetch(self, mode, filename, value, read):\n        \"\"\"\n        Override from base class diskcache.Disk.\n\n        Chunking is due to needing to work on pythons < 2.7.13:\n        - Issue #27130: In the \"zlib\" module, fix handling of large buffers\n          (typically 2 or 4 GiB).  Previously, inputs were limited to 2 GiB, and\n          compression and decompression operations did not properly handle results of\n          2 or 4 GiB.\n\n        :param int mode: value mode raw, binary, text, or pickle\n        :param str filename: filename of corresponding value\n        :param value: database value\n        :param bool read: when True, return an open file handle\n        :return: corresponding Python value\n        \"\"\"\n        value = super(GzipDisk, self).fetch(mode, filename, value, read)\n\n        if mode == MODE_BINARY:\n            str_io = BytesIO(value)\n            gz_file = gzip.GzipFile(mode='rb', fileobj=str_io)\n            read_csio = BytesIO()\n\n            while True:\n                uncompressed_data = gz_file.read(2**30)\n                if uncompressed_data:\n                    read_csio.write(uncompressed_data)\n                else:\n                    break\n\n            value = read_csio.getvalue()\n\n        return value\n\ndef getCache(scope_str):\n    return FanoutCache('data-unversioned\/cache\/' + scope_str,\n                       disk=GzipDisk,\n                       shards=64,\n                       timeout=1,\n                       size_limit=3e11,\n                       )\n\nraw_cache = getCache('ct_scan_raw')\n\n@raw_cache.memoize(typed=True)\ndef getCtScanChunk(series_uid, center_xyz, dims_irc):\n\n        filepaths = glob.glob(f'\/kaggle\/input\/luna16\/subset*\/*\/{series_uid}.mhd')\n        assert len(filepaths) != 0, f'CT scan with seriesuid {series_uid} not found!'\n        mhd_file_path = filepaths[0]\n        \n        mhd_file = sitk.ReadImage(mhd_file_path)\n        ct_scan = np.array(sitk.GetArrayFromImage(mhd_file), dtype=np.float32)\n        ct_scan.clip(-1000, 1000, ct_scan)\n        \n        origin_xyz = mhd_file.GetOrigin()\n        voxel_size_xyz = mhd_file.GetSpacing()\n        direction_matrix = np.array(mhd_file.GetDirection()).reshape(3, 3)\n        \n        origin_xyz_np = np.array(origin_xyz)\n        voxel_size_xyz_np = np.array(voxel_size_xyz)\n        \n        cri = ((center_xyz - origin_xyz_np) @ np.linalg.inv(direction_matrix)) \/ voxel_size_xyz_np\n        cri = np.round(cri)\n        irc = (int(cri[2]), int(cri[1]), int(cri[0]))\n        \n        slice_list = []\n        for axis, center_val in enumerate(irc):\n            \n            start_index = int(round(center_val - dims_irc[axis]\/2))\n            end_index = int(start_index + dims_irc[axis])\n            \n            if start_index < 0:\n                start_index = 0\n                end_index = int(dims_irc[axis])\n                \n            if end_index > ct_scan.shape[axis]:\n                end_index = ct_scan.shape[axis]\n                start_index = int(ct_scan.shape[axis] - dims_irc[axis])\n\n            slice_list.append(slice(start_index, end_index))\n            \n        ct_scan_chunk = ct_scan[tuple(slice_list)]\n        \n        return ct_scan_chunk","b941123f":"class LunaDataset(Dataset):\n    \n    def __init__(self, is_validation_set=False, validation_stride=0):\n        '''Create a PyTorch dataset for the CT scans\n        \n        If `is_validation_set` is `True` then every `validation_stride` item is kept.\n        Otherwise, every `validation_stride` item is deleted\n        '''\n        \n        # Make a copy of all the candidates.\n        # Pick every 350th candidate so that we have about 1k candidates in the dataset\n        # It takes agonizingly long to load more data!\n        self.candidates = copy.copy(candidates_clean[::350])\n        \n        # If this is the validation set, keep every `validation_stride` item\n        if is_validation_set:\n            self.candidates = self.candidates[::validation_stride]\n        \n        # If this is the training set, delete every `validation_stride` item\n        else:\n            del self.candidates[::validation_stride]\n            \n    def __len__(self):\n        '''Returns the number of items in the dataset'''\n        return len(self.candidates)\n    \n    def __getitem__(self, i):\n        '''Get the `i`the item in the dataset'''\n        \n        # Get the `i`th candidate\n        candidate = self.candidates[i]\n        \n        # We want to resize each CT scan to the following dimensions\n        dims_irc = (10, 18, 18)\n        \n        # Use the utility function to fetch the CT scan\n        ct_scan_np = getCtScanChunk(candidate.series_uid, candidate.center_xyz, dims_irc)\n        \n        # Convert the CT scan to a tensor\n        ct_scan_tensor = torch.from_numpy(ct_scan_np).to(torch.float32).unsqueeze(0)\n        \n        # Convert the target to a tensor\n        label_tensor = torch.tensor([\n            not candidate.is_nodule,\n            candidate.is_nodule\n        ], dtype=torch.long)\n        \n        return ct_scan_tensor, label_tensor","119f57e5":"VALIDATION_STRIDE=10\nBS=16\n\ntrain_ds = LunaDataset(is_validation_set=False, validation_stride=VALIDATION_STRIDE)\nval_ds = LunaDataset(is_validation_set=True, validation_stride=VALIDATION_STRIDE)\n\ntrain_dl = DataLoader(train_ds, batch_size=BS, num_workers=0)\nval_dl = DataLoader(val_ds, batch_size=BS, num_workers=0)","47f3b97b":"def train_loop(model, dataloader, criterion, optimizer, ds_size):\n    '''Train the model for one epoch'''\n    \n    # Put the model in training mode to activate dropout\n    model.train()\n    \n    # Keep a track of the loss and correct predictions for the epoch\n    running_loss = 0.0\n    running_corrects = 0\n    \n    # Track the total number of positives and true positives\n    running_pos = 0\n    running_pos_correct = 0\n    \n    # Track the total number of negatives and true negatives\n    running_neg = 0\n    running_neg_correct = 0\n    \n    for inputs, labels in tqdm(dataloader):\n        \n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        \n        # --- Standard PyTorch training process ---\n        optimizer.zero_grad()\n        \n        outputs = model(inputs)\n        _, preds = torch.max(outputs, 1)\n        \n        loss = criterion(outputs, labels[:,1])\n        loss.backward()\n        \n        optimizer.step()\n        # -----------------------------------------\n        \n        # Calculate loss and correct predictions in batch\n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += torch.sum(preds == labels.data[:,1])\n        \n        # Calculate positives and true positives in batch\n        running_pos += labels.data[:,1].sum()\n        running_pos_correct += ((preds == labels.data[:,1]) & (labels.data[:,1] == 1)).sum()\n        \n        # Calculate negatives and true negatives in batch\n        running_neg += labels.data[:,0].sum()\n        running_neg_correct += ((preds == labels.data[:,1]) & (labels.data[:,1] == 0)).sum()\n\n    epoch_loss = running_loss \/ ds_size\n    epoch_acc = running_corrects.double() \/ ds_size\n    \n    return epoch_loss, epoch_acc, (running_pos_correct, running_pos), (running_neg_correct, running_neg)\n    \n    \n\ndef eval_loop(model, dataloader, criterion, ds_size):\n    '''Evaluate the model performance for one epoch'''\n\n    # Put the model in evaluation mode to deactivate dropout\n    model.eval()\n\n    # Keep track of loss, predictions, and other numbers we are interested in\n    # just like in the training loop\n    running_loss = 0.0\n    running_corrects = 0\n    \n    running_pos = 0\n    running_pos_correct = 0\n    running_neg = 0\n    running_neg_correct = 0\n    \n    # Don't calculate gradients\n    with torch.no_grad():\n    \n        for inputs, labels in tqdm(dataloader):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n        \n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            loss = criterion(outputs, labels[:,1])\n        \n            running_loss += loss.item() * inputs.size(0)\n            running_corrects += torch.sum(preds == labels.data[:,1])\n            \n            running_pos += labels.data[:,1].sum()\n            running_pos_correct += ((preds == labels.data[:,1]) & (labels.data[:,1] == 1)).sum()\n\n            running_neg += labels.data[:,0].sum()\n            running_neg_correct += ((preds == labels.data[:,1]) & (labels.data[:,1] == 0)).sum()\n        \n    epoch_loss = running_loss \/ ds_size\n    epoch_acc = running_corrects.double() \/ ds_size\n    \n    return epoch_loss, epoch_acc, (running_pos_correct, running_pos), (running_neg_correct, running_neg)","648fea27":"class LunaModel(nn.Module):\n    \n    def __init__(self):\n        \n        super().__init__()\n        \n        self.conv1 = nn.Conv3d(1, 32, kernel_size=3, padding=1, bias=True)\n        self.relu1 = nn.ReLU()\n        self.maxpool1 = nn.MaxPool3d(2)\n        \n        self.conv2 = nn.Conv3d(32, 64, kernel_size=3, padding=1, bias=True)\n        self.relu2 = nn.ReLU()\n        self.maxpool2 = nn.MaxPool3d(2)\n        \n        self.flatten = nn.Flatten()\n        \n        self.fc1 = nn.Linear(2048, 1024)\n        self.relu3 = nn.ReLU()\n        \n        self.dropout = nn.Dropout(0.2)\n        \n        self.fc2 = nn.Linear(1024, 2)\n    \n    def forward(self, X):\n        \n        # Dimensions of X => [BS, 1, 10, 18, 18]\n        \n        X = self.maxpool1(self.relu1(self.conv1(X)))\n        X = self.maxpool2(self.relu2(self.conv2(X)))\n        \n        X = self.flatten(X)\n\n        X = self.relu3(self.fc1(X))\n        X = self.dropout(X)\n        \n        return self.fc2(X)\n        ","4d9da99c":"# Create an instance of the model\nmodel = LunaModel()\n\n# Use the GPU if it is available\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# Use the cross-entropy loss function\ncriterion = nn.CrossEntropyLoss()\n\n# Use the AdamW optimizer\noptimizer = optim.AdamW(model.parameters(), weight_decay=0.1)","d4b5328a":"EPOCHS = 5\n\nfor epoch in range(EPOCHS):\n\n    epoch_start = time.time()\n\n    train_loss, train_acc, train_pos, train_neg = train_loop(\n        model, train_dl, criterion,\n        optimizer, len(train_ds)\n    )\n\n    val_loss, val_acc, val_pos, val_neg = eval_loop(\n        model, val_dl, criterion, len(val_ds)\n    )\n\n    time_elapsed = time.time() - epoch_start\n    print(f'Epoch: {epoch+1:02} | Epoch Time: {time_elapsed \/\/ 60:.0f}m {time_elapsed % 60:.0f}s')\n    print()\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\tTrain - correct pos: {train_pos[0]}\/{train_pos[1]} | correct neg: {train_neg[0]}\/{train_neg[1]}')\n    print()\n    print(f'\\tVal. Loss: {val_loss:.3f} |  Val. Acc: {val_acc*100:.2f}%')\n    print(f'\\tVal. - correct pos: {val_pos[0]}\/{val_pos[1]} | correct neg: {val_neg[0]}\/{val_neg[1]}')\n    print()","87a388fd":"The `SimpleITK` package has a very simple API that we'll use to get details about the CT scan. They have [great documentation](https:\/\/simpleitk.org\/SimpleITK-Notebooks\/01_Image_Basics.html) available if you want to read more.\n\nWe read the CT scan and store it as a NumPy array.","e4c716b1":"We convert the origin and voxel size to NumPy arrays so that they are easier to use in calculations.","fded5ad2":"We then sort the list of candidates in reverse order. Since the list contains tuples, the order of fields will determine the way the list is sorted.\n\nIn particular, after sorting, we'll have all candidates with the value of `is_nodule` as `True` at the beginning of the list. Among these candidates, those with the largest diameter will come before those with smaller diameters. All candidates with a diameter of zero (recall this happens when the candidate coordinates are not \"close enough\" to any annotation coordinates, or there is no corresponding annotation for the particular candidate) will come after these.\n\nCandidates that have a value of `False` for `is_nodule` will come last in the list with the same relative order of diameters as above.\n\n![image.png](attachment:b9b5d21a-10c2-437e-bc94-383e0182f2c5.png)","90e18e93":"There are 443 `seriesuid`s in the annotations and candidates CSV files that don't have corresponding .mhd files.\n\nWe now remove from our list of candidates those that don't have a CT scan.","47592b3a":"Next, we create `DataLoader`s out of our datasets.\n\nWe use a `VALIDATION_STRIDE` of 10 which means every 10th CT scan will be in the validation set.","0930231b":"With the utility function and caching set up, we can now create a PyTorch datset.","89642082":"We can now load this file using the `SimpleITK` package.","1e54e406":"## Introduction\n\nThis notebook was created as a part of the [Weights & Biases PyTorch Book Reading Group](https:\/\/community.wandb.ai\/c\/community-events\/pytorch-book\/32) hosted by [Sanyam Butani](https:\/\/www.kaggle.com\/init27).\n\nThe idea was to find a notebook on Kaggle with a TensorFlow model trained on the Luna16 dataset and try to convert it to PyTorch.\n\nThis notebook is my attempt to train the model that [Sentdex](https:\/\/www.kaggle.com\/sentdex) built in his notebook [First pass through Data w\/ 3D ConvNet](https:\/\/www.kaggle.com\/sentdex\/first-pass-through-data-w-3d-convnet) in PyTorch.\n\nMost of the code used to read the data comes from the amazing book we're reading in the group - [Deep Learning with PyTorch](https:\/\/www.manning.com\/books\/deep-learning-with-pytorch).\n\n## Load required libraries","58cb5588":"We use the `glob` module to find the `.mhd` and `.raw` files associated with the candidate.\n\nThe files could be in any one of the `subset` folders in the dataset. The `glob` module allows us to find the file by using patterns instead of manually looking inside each of the folders.","ba19fd79":"The actual values in the CT scan are in [Hounsfield units (HU)](https:\/\/radiopaedia.org\/articles\/hounsfield-unit) which goes from -1000 to 3000.\n\nWe use a range of -1000 or 1000 to remove extremely dense materials from the CT scan.\n\n[This notebook](https:\/\/www.kaggle.com\/gzuidhof\/full-preprocessing-tutorial) has a great walkthrough and visualization of this data format.","3a4a4914":"The `glob` package can return multiple files that match the pattern specified.\n\nHowever, for simplicity we'll assume that the dataset contains only one `.mhd` file for a given `seriesuid`.\n\nThat path will be available at index 0 of the result.","9f075d17":"We need to also convert the output we want from the model (`is_nodule`) into a PyTorch tensor.","8bb913a8":"## Missing data\n\nThere are some `seriesuid`s in the dataset that don't have corresponding CT scans in the dataset.\n\nThe cell below finds all such `seriesuid`s. Since it takes about 10 minutes to run, I have made those `seriesuid`s available as a separate dataset: https:\/\/www.kaggle.com\/mashruravi\/luna16missingcandidates. We'll load data from this dataset to save precious GPU time on Kaggle.","47ecef03":"We now have three slices we can use in each direction to extract the chunk we need.","f889cc1a":"We will extract a chunk by getting a list of three slices - one for each direction - and then using that to extract the actual values from the CT scan.","39013036":"We lost almost 50% of the data! *PANIC!!!*\n\nWell, we'd panic if this wasn't something we're just playing with.\n\nFor now, we'll try not to worry about this and move on with the data that we have left.\n\n## Load the data\n\nWe'll now walk through how we want to convert the data we have into a format that we can consume with PyTorch.\n\nWe will walk through and understand all the steps using a single candidate before putting the code together into utility functions and a PyTorch `Dataset`.","76e1d8b2":"There are over 750k candidates.\n\nThe huge difference between candidates and annotations tells us that we will have many candidates for which we won't have a diameter in the annotations file. However, if fine for now as we won't be using the diameter information when building this simple model.\n\nAnother thing to note is that there can be multiple annotations and candidates in a single CT scan.","a082abe1":"Similarly, we group the candidates that are part of the same CT scan and then use the diameters dictionary we created above to fetch each candidate's diameter.\n\nThe X, Y and Z coordinates of the center can be slightly different in the annotations and candidate files.\n\nTherefore, when looking for a candidate's diameter from the `diameters` dict, we'll assume that if the center coordinates of the candidate are less than half the radius of the annotated nodule away from the center coordinates of the annotated nodule, then they are the same nodule.\n\nThat sentence was a mouthful! Maybe this diagram will be easier to understand:\n\n![image.png](attachment:93cf076d-063b-4b3f-90d4-1a1ab3f6fe04.png)\n\nWe will create a `namedtuple` to store the information that we combine from the candidates and annotations.","32e2cdfb":"That's equivalent to 41 RGB color images of resolution 512x512 for a single CT scan!\n\nSince most of the CT scan doesn't contain any interesting to us, we will extract 3-dimensional chunks of the CT scan that contain nodules as input for our model.\n\nLet's say we want to extract a chunk of size 10 along the index column, and 18 rows and columns.","582f1b9e":"Now, let's put it all together in a few utility functions and a `Dataset` class.\n\nFirst, let us set up some utility functions for caching the dataset. As we'll see later, this will significantly speed up training after the first epoch.\n\nThe code used to set up caching is from the Deep Learning with PyTorch book, and can be found in the book's [GitHub repository](https:\/\/github.com\/deep-learning-with-pytorch\/dlwpt-code\/blob\/master\/util\/disk.py).","ee3f8829":"We now have a CT scan chunk `ct_scan_chunk`. The center of the nodule is at index `irc` in the complete scan `ct_scan`.\n\nThe next step would be to convert this chunk of CT scan to a PyTorch tensor.","b0cbeafd":"## Understand the Dataset\n\nThe Luna16 (Lung Nodule Analysis 2016) dataset contains chest CT scans and annotations indicating where there are nodules in each CT scan and their diameters.\n\nMore information: https:\/\/luna16.grand-challenge.org\/\n\nThe model we will build will try to predict whether a particular region of a CT scan has a nodule or not.\n\nThere are two CSV files that we'll be working with: **annotations.csv** and **candidates_V2.csv**.\n\n### Annotations\n\nThe annotations file contains the center and diameter of each mass in CT scans.","357b554d":"We can now train the model.","faaa298c":"We also get the following information:\n\n - Center point of reference of the CT scan (also known as the origin)\n - Size of each voxel (short for volume pixel) since each CT scan can have a different size of voxels\n - Direction matrix that has a direction vector for of each axis in the CT scan","b4325c34":"There are 1,186 total annotations available.\n\n### Candidates\n\nThe candidates file contains a `class` flag for each mass in the CT scans.\n\n`seriesuid` is the unique identifer of the CT scan.\n\n`coordX`, `coordY` and `coordZ` are coorindates of the center of the mass.\n\n`class` is 0 if the mass isn't a nodule, and 1 if it is a nodule (both malignant and benign).","0a567ee0":"The entire CT scan is currently very large for us to work with.","7e11dad9":"## Train the Model\n\nThis model is a PyTorch version of [Sentdex](https:\/\/www.kaggle.com\/sentdex)'s TensorFlow model in [this notebook](https:\/\/www.kaggle.com\/sentdex\/first-pass-through-data-w-3d-convnet).\n","5c3e815b":"We will be using the [cross-entropy loss](https:\/\/ravimashru.dev\/blog\/2021-07-18-understanding-cross-entropy-loss\/) function to train our model so we need two columns for the output - one-hot encoded values of the boolean `is_nodule` value we're interested in.","b0b1af2b":"We will first group all annotations that are part of the same CT scan (i.e. have the same `seriesuid`).\n\nThis will allow us to easily access the **centers** and respective **diameters** of all nodules in a particular CT scan.","de01cfb8":"We now create an instance of the model, the loss function, and an optimizer to train the model.","28ef6127":"We now convert the origin and voxel size from the patient coorindate system previously mentioned to coordinates that we can use to index into the NumPy array representing the CT scan.","7a586d51":"The first epoch takes a long time, but once the data is cached, the other epochs are super fast!\n\n## Conclusion\n\nWe've got an accuracy of over 99% on the validation set! Time to pop the champagne!\n\nBut hang on... That's not the entire story. The numbers below the accuracy give us a clearer picture of what is going on.\n\nThe model predicted all negatives correctly, but didn't make any correct predictions for the positive data points. So this model is not very useful right now.\n\nThis probably happened because we don't have enough positive samples in the training and validation set. It would be unrealistic to expect the model to learn how to predict positive samples with just a couple of data points.\n\nAlso, because our dataset is highly imbalanced, we need a better strategy to train our model and also a better indication of model performance instead of accuracy.\n\nI guess we'll have to keep that champagne on the ice for a little longer..."}}