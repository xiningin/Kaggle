{"cell_type":{"fa2146dc":"code","cc994b35":"code","39a253db":"code","1e1c3e0c":"code","fa5fc3dd":"code","5ee80fda":"code","eaf2fb89":"code","1409df8d":"code","87570983":"code","223f9b7e":"code","b5ecfe7e":"code","f198866c":"code","d8b0be59":"code","758a93ed":"code","4d63db63":"code","6cdb2571":"code","e0776546":"code","b3b94c7e":"code","c65fc778":"code","d2cf67b7":"code","5640d207":"code","b5494d4a":"code","0fb79745":"code","dc762650":"code","32cfbe16":"code","f2f17632":"code","d1b32b5e":"code","d1b38f0f":"code","ed70c9ca":"code","5621613b":"code","d7a8e7b6":"code","ae250498":"markdown","657e146b":"markdown","a3a28a4d":"markdown","870f51b3":"markdown","b19897ba":"markdown","67fc4415":"markdown"},"source":{"fa2146dc":"# Importing core libraries\nimport numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import recall_score, precision_score, confusion_matrix\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.cluster import KMeans\nfrom sklearn.covariance import EllipticEnvelope","cc994b35":"# Custom anomaly detectors\nfrom sklearn.base import BaseEstimator\nimport numpy as np\nfrom math import gamma, pi\nfrom scipy.stats import multivariate_normal\n\nclass MultivariateGaussian(BaseEstimator):\n    \"\"\"\n    Anomaly detector using the Multivariate Gaussian class\n    Based on Andrew Ng's lecture\n    \"\"\"\n    def __init__(self, epsilon=None):\n        self.epsilon = epsilon\n    \n    def fit(self, train_data):\n        X = np.array(train_data)\n        # Calculating n-dimensional mean vector (mu)\n        self.mu = np.mean(X, axis=0)\n        # Calculating n-by-n covariance matrix (Sigma)\n        self.Sigma = np.cov(X, rowvar=False)\n        assert self.Sigma.shape == (X.shape[1], X.shape[1]), f\"The covariance matrix must be {X.shape[1]} by {X.shape[1]}\"\n        return self\n\n    def predict(self, test_data):\n        X_test = np.array(test_data)\n        p_X = multivariate_normal.pdf(X_test, mean=self.mu, cov=self.Sigma, allow_singular=True)\n        assert p_X.shape == (X_test.shape[0],), f\"The predicted probability densities must be a {X_test.shape[0]}-dimensional vector\"\n        # Calculating m-dimensional prediction vector (probability density function)\n        predictions = np.where(p_X < self.epsilon, 1, 0)\n        return predictions\n\nclass MultivariateTDistribution(MultivariateGaussian):\n    \"\"\"\n    Anomaly detector using the 'fat-tail' Student's t-Distribution.\n    \"\"\"\n    def __init__(self, epsilon=None, df=None):\n        self.epsilon = epsilon\n        # Degrees of freedom parameter\n        self.df = df\n\n    def predict(self, test_data):\n        X_test = np.array(test_data)\n        df = self.df\n        n = test_data.shape[1]\n        mu = self.mu\n        Sigma = self.Sigma\n        pdf_list = []\n        # The following codes should be vectorised, any suggestions are welcomed\n        for x_test in X_test:\n            term_1_num = gamma((n+df)\/2)\n            term_1_denom = gamma((df\/2)) * df**(n\/2) * pi**(n\/2) * np.linalg.det(Sigma)**0.5\n            term_1 = term_1_num \/ term_1_denom\n            term_2 = (1 + 1\/df * (x_test - mu).T @ np.linalg.inv(Sigma) @ (x_test - mu)) ** (-1 * (n+df)\/2)\n            pdf_x = term_1 * term_2\n            pdf_list.append(pdf_x)\n        pdf_X = np.array(pdf_list)\n        assert pdf_X.shape == (X_test.shape[0],), f\"The predicted probability densities must be a {X_test.shape[0]}-dimensional vector\"\n        predictions = np.where(pdf_X < self.epsilon, 1, 0)\n        return predictions\n\nclass SimpleAnomalyDetector(BaseEstimator):\n    \"\"\"\n    Simple anomaly detector, a predicted probability density is the product of all features' probabilities.\n    Based on Andrew Ng's lecture.\n    \"\"\"\n    def __init__(self, epsilon=None):\n        self.epsilon = epsilon\n\n    def fit(self, train_data):\n        X = np.array(train_data)\n        # Calculating n-dimensional mean vector (mu)\n        self.mu = np.mean(X, axis=0).reshape(-1, 1)\n        # Calculating n-dimensional vector of feature variances (sigma)\n        self.sigma = np.var(X, axis=0).reshape(-1, 1)\n        assert self.mu.shape == (X.shape[1], 1)\n        assert self.sigma.shape == (X.shape[1], 1)\n        return self\n\n    def predict(self, test_data):\n        X_test = np.array(test_data)\n        mu = self.mu\n        sigma = self.sigma\n        term_1 = -1 * np.square(X_test - np.tile(mu, len(X_test)).T)\n        term_2 = 2 * np.tile(sigma, len(X_test)).T\n        term_3 = np.exp(term_1 \/ term_2)\n        term_4 = (1 \/ ((2*np.pi)**0.5 * np.power(sigma, np.array(0.5))))\n        term_4 = np.tile(term_4, len(X_test)).T\n        term_5 = term_3 * term_4\n        pdf_X = np.prod(term_5, axis=1)\n        assert pdf_X.shape == (X_test.shape[0],), f\"The predicted probability densities must be a {X_test.shape[0]}-dimensional vector\"\n        predictions = np.where(pdf_X < self.epsilon, 1, 0)\n        return predictions","39a253db":"# Importing the Credit Card Fraud dataset\ndata = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')\ny_data = data.copy()['Class'].values\noriginal_data = data.copy()\n\n# Clean data set\nnormal_only_data = data[data['Class']==0]\nprint('Normal only data shape: ', normal_only_data.shape)\n# Fraud data set\nfraud_only_data = data[data['Class']==1]\nprint('Fraud only data shape: ', fraud_only_data.shape)\n\n# Shuffling the data\nnormal_only_data = normal_only_data.sample(frac=1, random_state=42)\nfraud_only_data = fraud_only_data.sample(frac=1, random_state=42)\n\n# 80\/10\/10 data split for normal data\ntrain_set, dev_set, test_set = np.split(normal_only_data, [int(0.8*len(normal_only_data)), int(0.9*len(normal_only_data))])\ntrain_set = train_set.drop('Class', axis=1)\n\n# 50\/50 data split for fraud data\nfraud_set_1, fraud_set_2 = np.split(fraud_only_data, [int(0.5*len(fraud_only_data))])\n\n# Appending fraud data to dev and test set\ndev_set = dev_set.append(fraud_set_1)\ny_dev_set = dev_set['Class']\ndev_set = dev_set.drop('Class', axis=1)\ntest_set = test_set.append(fraud_set_2)\ny_test_set = test_set['Class']\ntest_set = test_set.drop('Class', axis=1)\n\n# Showing shapes\nfor name, data in zip(['Train data shape: ', 'Dev data shape: ', 'Test data shape: '],[train_set, dev_set, test_set]):\n    print(name, data.shape)","1e1c3e0c":"# Showing the first few rows of the data\noriginal_data.head()","fa5fc3dd":"# Helper function to evaluate models\nlabels = ['Normal', 'Fraud']\ndef evaluate_model(y_true, y_preds, labels=labels):\n    cm = confusion_matrix(y_true, y_preds)\n    print('Recall score:\\n', recall_score(y_true, y_preds))\n    print('Precision score:\\n', precision_score(y_true, y_preds))\n    print('Confusion matrix:\\n')\n    cm_df = pd.DataFrame({'Normal (predicted)': (cm[0, 0], cm[1, 0]),\n                         'Fraud (predicted)': (cm[0, 1], cm[1, 1])},\n                        index=['Normal (true)', 'Fraud (true)'])\n    print(cm_df)","5ee80fda":"# Training Multivariate Gaussian Anomaly Detector\nmvg = MultivariateGaussian(epsilon=0.05**30)\nmvg.fit(train_set)","eaf2fb89":"# Evaluating on the Dev set\nmvg_y_dev_preds = mvg.predict(dev_set)\nevaluate_model(y_dev_set, mvg_y_dev_preds)","1409df8d":"# Training Simple Anomaly Detector (SAD)\n# Side note: I'm still thinking about a new name for this Estimator ... suggestions are welcomed\nsimple = SimpleAnomalyDetector(epsilon=0.05**30)\nsimple.fit(train_set)","87570983":"# Evaluating on the Dev Set\ny_simple_dev_preds = simple.predict(dev_set)\nevaluate_model(y_dev_set, y_simple_dev_preds)","223f9b7e":"# Training Multivariate T Anomaly Detector\nmvt = MultivariateTDistribution(epsilon=0.05**30, df=3)\nmvt.fit(train_set)","b5ecfe7e":"# Evaluating on the Dev Set\nmvt_y_dev_preds = mvt.predict(dev_set)\nevaluate_model(y_dev_set, mvt_y_dev_preds)","f198866c":"# Training Local Outlier Factor Anomaly Detector\nlof = LocalOutlierFactor(novelty=True, metric='euclidean')\nlof.fit(train_set)","d8b0be59":"# Evaluating on the Dev set\nlof_y_dev_preds = lof.predict(dev_set)\nlof_y_dev_preds[lof_y_dev_preds==1] = 0\nlof_y_dev_preds[lof_y_dev_preds==-1] = 1\nevaluate_model(y_dev_set, lof_y_dev_preds)","758a93ed":"# Training Isolation Forest Anomaly Detector\nifr = IsolationForest(random_state=42, behaviour=\"new\")\nifr.fit(train_set)","4d63db63":"# Evaluating on the Dev Set\nifr_y_dev_preds = ifr.predict(dev_set)\nifr_y_dev_preds[ifr_y_dev_preds==1] = 0\nifr_y_dev_preds[ifr_y_dev_preds==-1] = 1\nevaluate_model(y_dev_set, ifr_y_dev_preds)","6cdb2571":"# Training Kmeans Clustering\nkmeans = KMeans(n_clusters=2, algorithm='auto')\nkmeans.fit(train_set)","e0776546":"# Evaluating KMeans on the Dev Set\nkmeans_y_dev_preds = kmeans.predict(dev_set)\n# Since KMeans only does clustering, we can decide which cluster would be Normal and which cluster would be Fraud\nkmeans_y_dev_preds = np.where(kmeans_y_dev_preds==1, 0, 1)\nevaluate_model(y_dev_set, kmeans_y_dev_preds)","b3b94c7e":"# Trying out Isolation Forest on contaminated dataset\nifr_2 = IsolationForest(random_state=42, behaviour=\"new\")\nifr_2_y_preds = ifr_2.fit_predict(original_data.drop('Class', axis=1))\nifr_2_y_preds[ifr_2_y_preds==1] = 0\nifr_2_y_preds[ifr_2_y_preds==-1] = 1\nevaluate_model(y_data, ifr_2_y_preds)","c65fc778":"from keras.layers import Input, Dense\nfrom keras.models import Model, Sequential\nfrom keras import regularizers\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n# Considered using XGBoost, my it would take a while on my EY laptop","d2cf67b7":"# I use smaller sample to save computational time\ntrain_data = normal_only_data[:100000].append(fraud_only_data[:390])\ntrain_data = train_data.sample(frac=1, random_state=42)\nprint('NN Train data shape:\\n', train_data.shape)\ndev_data = normal_only_data[100000:120000].append(fraud_only_data[390:])\ndev_data = dev_data.sample(frac=1, random_state=42)\nprint('NN Dev data shape:\\n', dev_data.shape)","5640d207":"# Separating X and y\nX_train_nn, y_train_nn = train_data.drop('Class', axis=1, inplace=False), train_data['Class'].values\nscaler = MinMaxScaler()\nX_train_nn = scaler.fit_transform(X_train_nn)\nprint(X_train_nn.shape, y_train_nn.shape)\nX_dev_nn, y_dev_nn = dev_data.drop('Class', axis=1, inplace=False), dev_data['Class'].values\nX_dev_nn = scaler.transform(X_dev_nn)\nprint(X_dev_nn.shape, y_dev_nn.shape)","b5494d4a":"# Building computational graph for NN Autocencoder\n# Input layer\ninput_layer = Input(shape=(X_train_nn.shape[1],))\n\n# Encoding part\nencoded_1 = Dense(200, activation='tanh', activity_regularizer=regularizers.l1(10e-5))(input_layer)\nencoded_2 = Dense(100, activation='relu')(encoded_1)\n\n# Decoding part\ndecoded_1 = Dense(100, activation='tanh')(encoded_2)\ndecoded_2 = Dense(200, activation='tanh')(decoded_1)\n\n# Output layer\noutput_layer = Dense(X_train_nn.shape[1], activation='relu')(decoded_2)\n\n# Compiling model\nautoencoder = Model(input_layer, output_layer)\nautoencoder.compile(optimizer='adam', loss='mse')","0fb79745":"# Training the model\nautoencoder.fit(X_train_nn, X_train_nn,\n                batch_size=256, epochs=20,\n                shuffle=True, validation_split=0.2\n               )","dc762650":"autoencoder.summary()","32cfbe16":"# Building a computation graph to get the hidden representations of X\nhidden_representation = Sequential()\nhidden_representation.add(autoencoder.layers[0])\nhidden_representation.add(autoencoder.layers[1])\nhidden_representation.add(autoencoder.layers[2])","f2f17632":"# Obtaining the hidden representations of X_train\nrep_X_train = hidden_representation.predict(X_train_nn)","d1b32b5e":"# Logistic Regression - mapping NN Training output to y\nlogreg = LogisticRegression(solver='lbfgs')\nlogreg.fit(rep_X_train, y_train_nn)","d1b38f0f":"# Now moving on to the Dev Set\nrep_X_dev = hidden_representation.predict(X_dev_nn)\nnn_y_dev_preds = logreg.predict(rep_X_dev)\nnp.unique(nn_y_dev_preds)","ed70c9ca":"# Evaluating the Dev Set\nevaluate_model(y_dev_nn, nn_y_dev_preds)","5621613b":"# Random Forest Classifier - training on hidden representations of X\nrf = RandomForestClassifier(random_state=42)\nrf.fit(rep_X_train, y_train_nn)","d7a8e7b6":"# Evaluating on the Dev Set\nnn_y_dev_preds_rf = rf.predict(rep_X_dev)\nevaluate_model(y_dev_nn, nn_y_dev_preds_rf)","ae250498":"## Trying out Neural Network Autoencoders\n\n#### Intuition\nThe basic idea behind NN Autoencoders is to learn the very low-level representations of the data. After this process hopefully the 'noise' have been minimised from the data, and the result representations (outputs of NN Autoencoders) can be used as inputs to simplier classifiers such as Logistic Regression.","657e146b":"#### Contaminated dataset","a3a28a4d":"## Trying out Custom Estimators - Novelty detection\nFor simplicity, I will train and test the custom and a few scikit-learn models on a novelty detection bias.\n\nNovelty detection uses clean data (normal) only to train, and predict on contaminated data. Whereas Outlier detection uses contaminated (normal + fraud) data to train.\n\nFor more information, refer to scikit-learn's [Guidance](https:\/\/scikit-learn.org\/stable\/modules\/outlier_detection.html#outlier-detection) here","870f51b3":"## Loading data - Credit Card Fraud dataset\n#### Context on the dataset\n\nV1 to V28 are PCA components of the data. Only features 'Time' and 'Amount' are untransformed features.\n\nFor more details about the dataset, visit the [Kaggle site here](https:\/\/www.kaggle.com\/mlg-ulb\/creditcardfraud).\n\n>The datasets contains transactions made by credit cards in September 2013 by european cardholders.\nThis dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. \nThe dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\nIt contains only numerical input variables which are the result of a PCA transformation.","b19897ba":"In the following code block, I write the implementation of anomaly detectors from Andrew Ng's Machine Learning lectures (Section 15)","67fc4415":"## Importing core libraries and custom estimators"}}