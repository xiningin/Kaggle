{"cell_type":{"e285b14b":"code","26b513cc":"code","7882f55c":"code","96088659":"code","420c1f23":"code","50ead380":"code","b3a63bca":"code","c3814f5d":"code","2c5badce":"code","4627da79":"code","70a12c52":"code","9f23b65b":"code","2d7ed6d4":"code","7d0f97d8":"code","c60881b2":"code","a1d3e30e":"code","12060b12":"code","a1d4be45":"code","2b06e7d6":"code","5453c8fb":"code","ab14d637":"code","df21f39e":"code","cfb595bb":"code","18bdda90":"code","6a3089bf":"code","b7600ce1":"code","98c2cd74":"code","ad566fb7":"code","2ea22f7f":"code","b8fb35a8":"code","a6a19d39":"code","55fd6f0b":"code","02861294":"code","ef0ccbdb":"code","964c4076":"code","1673d06d":"code","e3e58cc0":"code","cda5b327":"code","b28a974e":"code","ff0a027b":"code","2b927d59":"code","cfee86d4":"code","51aa1bcd":"code","f1b9bb2f":"code","1e653ca6":"code","26accd40":"markdown","a667847b":"markdown","e79fd248":"markdown","1e18aee6":"markdown","8d032ba6":"markdown","cc694900":"markdown","890be568":"markdown","d2117ff2":"markdown","962e08b4":"markdown","6adae637":"markdown","049fe5db":"markdown","94edaa07":"markdown","16c7d590":"markdown","c16b2253":"markdown","c9bcb8de":"markdown","a65e0a41":"markdown","334511ab":"markdown","1c955381":"markdown","974c5776":"markdown","6b18f777":"markdown"},"source":{"e285b14b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","26b513cc":"! pip install pandas-profiling[notebook] >> \/dev\/null","7882f55c":"import numpy as np\nimport pandas as pd\nfrom pandas_profiling import ProfileReport\nimport seaborn as sns\nfrom sklearn.model_selection import StratifiedKFold\nimport lightgbm as lgb\nfrom sklearn.metrics import accuracy_score\nfrom matplotlib import pyplot as plt\n\n\ndf_train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ndf_gender_submission = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")","96088659":"df_train.head()","420c1f23":"df_test.head()","50ead380":"profile = ProfileReport(df_train, title='Pandas Profiling Report', explorative=True)\nprofile","b3a63bca":"df_train['Survived'].value_counts(normalize=True)","c3814f5d":"sns.countplot(x='Pclass', data=df_train, hue='Survived')","2c5badce":"for pclass in sorted(df_train['Pclass'].unique()):\n    ratio = df_train[df_train['Pclass'] == pclass]['Survived'].mean()\n    print(f'{pclass}, {ratio:.3f}')","4627da79":"df_train['Family_Size'] = df_train['Parch'] + df_train['SibSp'] + 1\ndf_test['Family_Size'] = df_test['Parch'] + df_test['SibSp'] + 1\nsns.countplot(x='Family_Size', data=df_train, hue='Survived')","70a12c52":"for Family_Size in sorted(df_train['Family_Size'].unique()):\n    ratio = df_train[df_train['Family_Size'] == Family_Size]['Survived'].mean()\n    print(f'{Family_Size}, {ratio:.3f}')","9f23b65b":"df_train['Sex'].replace(['male','female'], [0, 1], inplace=True)\ndf_train['Embarked'].fillna(('S'), inplace=True)\ndf_train['Embarked'] = df_train['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\ndf_train['Fare'].fillna(np.mean(df_train['Fare']), inplace=True)\ndf_train['Age'].fillna(df_train['Age'].median(), inplace=True)\ndf_train['FamilySize'] = df_train['Parch'] + df_train['SibSp'] + 1\ndf_train['IsAlone'] = 0\ndf_train.loc[df_train['FamilySize'] == 1, 'IsAlone'] = 1","2d7ed6d4":"df_test['Sex'].replace(['male','female'], [0, 1], inplace=True)\ndf_test['Embarked'].fillna(('S'), inplace=True)\ndf_test['Embarked'] = df_test['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\ndf_test['Fare'].fillna(np.mean(df_test['Fare']), inplace=True)\ndf_test['Age'].fillna(df_test['Age'].median(), inplace=True)\ndf_test['FamilySize'] = df_test['Parch'] + df_test['SibSp'] + 1\ndf_test['IsAlone'] = 0\ndf_test.loc[df_test['FamilySize'] == 1, 'IsAlone'] = 1","7d0f97d8":"df_train['Title'] = df_train.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\ndf_test['Title'] = df_test.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)","c60881b2":"df_train['Title'] = df_train['Title'].replace(['Lady', 'Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Jonkheer', 'Dona'], 'Rare')\ndf_train['Title'] = df_train['Title'].replace(['Countess', 'Lady', 'Sir'], 'Royal')\ndf_train['Title'] = df_train['Title'].replace('Mlle', 'Miss')\ndf_train['Title'] = df_train['Title'].replace('Ms', 'Miss')\ndf_train['Title'] = df_train['Title'].replace('Mme', 'Mrs')\n\ndf_train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","a1d3e30e":"df_test['Title'] = df_test['Title'].replace(['Lady', 'Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Jonkheer', 'Dona'], 'Rare')\ndf_test['Title'] = df_test['Title'].replace(['Countess', 'Lady', 'Sir'], 'Royal')\ndf_test['Title'] = df_test['Title'].replace('Mlle', 'Miss')\ndf_test['Title'] = df_test['Title'].replace('Ms', 'Miss')\ndf_test['Title'] = df_test['Title'].replace('Mme', 'Mrs')","12060b12":"#map each of the title groups to a numerical value\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Royal\": 5, \"Rare\": 6}\n\ndf_train['Title'] = df_train['Title'].map(title_mapping)\ndf_train['Title'] = df_train['Title'].fillna(0)\ndf_test['Title'] = df_test['Title'].map(title_mapping)\ndf_test['Title'] = df_test['Title'].fillna(0)","a1d4be45":"df_train","2b06e7d6":"df_test","5453c8fb":"df_train.shape, df_test.shape","ab14d637":"df_train.head()","df21f39e":"df_test","cfb595bb":"df_all = pd.concat([df_train[df_test.columns], df_test], axis=0)\ndf_all","18bdda90":"Cabin2num = dict(zip(list(df_all['Cabin'].unique()), np.arange(len(list(df_all['Cabin'].unique())))))\ndf_train['Cabin'] = df_train['Cabin'].map(Cabin2num)\ndf_test['Cabin'] = df_test['Cabin'].map(Cabin2num)","6a3089bf":"Cabin2num","b7600ce1":"Embarked2num = dict(zip(list(df_all['Embarked'].unique()), np.arange(len(list(df_all['Embarked'].unique())))))\ndf_train['Embarked'] = df_train['Embarked'].map(Embarked2num)\ndf_test['Embarked'] = df_test['Embarked'].map(Embarked2num)","98c2cd74":"Sex2num = dict(zip(list(df_all['Sex'].unique()), np.arange(len(list(df_all['Sex'].unique())))))\ndf_train['Sex'] = df_train['Sex'].map(Sex2num)\ndf_test['Sex'] = df_test['Sex'].map(Sex2num)","ad566fb7":"df_train.head()","2ea22f7f":"df_test.head()","b8fb35a8":"params = {\n    'objective': 'binary',\n    'learning_rate': 0.05,\n    'max_depth': 4,\n    'min_data_in_leaf': 5\n}","a6a19d39":"target_col = 'Survived'\ny = df_train[target_col]\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n\n\nfor fold_id, (train_index, valid_index) in enumerate(cv.split(df_train, y)):\n    print('*'*20, fold_id, '*'*20)\n    X_train, y_train = df_train.iloc[train_index], y.iloc[train_index]\n    X_val, y_val = df_train.iloc[valid_index], y.iloc[valid_index]\n    print(X_train.shape, X_val.shape)\n    print('train\\n', y_train.value_counts() \/ len(y_train))\n    print('val\\n',y_val.value_counts() \/ len(y_val))","55fd6f0b":"target_col = 'Survived'\nmodels = []\n# Out of fold\u306e\u7565. \u624b\u5143\u306e\u30c7\u30fc\u30bf\u304cvalidation\u3060\u3063\u305f\u6642\u306e\u4e88\u6e2c\u5024\noof = np.zeros(len(df_train))\ny = df_train[target_col]\nimportances = pd.DataFrame()\nscores = []\ndrop_cols = ['PassengerId', 'Name', 'Ticket', 'Survived']\ncate_cols = ['Cabin', 'Embarked', 'Pclass', 'Sex']\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n\n\nfor fold_id, (train_index, valid_index) in enumerate(cv.split(df_train, y)):\n    X_train, y_train = df_train.iloc[train_index], y.iloc[train_index]\n    X_val, y_val = df_train.iloc[valid_index], y.iloc[valid_index]\n    \n    # \u4e0d\u8981\u5217\u306e\u524a\u9664\n    X_train = X_train.drop(drop_cols, axis=1)  # \u7279\u5fb4\u91cf\u3068\u3057\u3066\u4f7f\u308f\u306a\u3044\u5217\u306e\u524a\u9664\n    X_val = X_val.drop(drop_cols, axis=1)\n    \n    print(X_train.shape, y_train.shape)\n    features = sorted(X_train.columns)\n    print(y_train.mean(), y_val.mean())\n    X_train = X_train[features]\n    X_val = X_val[features]\n\n    # dataset\n    train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=cate_cols)\n    val_data = lgb.Dataset(X_val, label=y_val, categorical_feature=cate_cols)\n    print(X_train.shape, X_val.shape)\n\n    # \u5b66\u7fd2\n    print('train...')\n    lgb_model = lgb.train(params,\n                          train_data,\n                          num_boost_round=1000,\n                          early_stopping_rounds=20,\n                          valid_sets=[train_data, val_data],\n                          verbose_eval=200)\n    print('write..')\n    models.append(lgb_model)\n    lgb_model.save_model(f'{fold_id+1}_model.lgb')\n    \n    # oof\n    print('oof...')\n    y_pred = lgb_model.predict(X_val, num_iteration=lgb_model.best_iteration)\n    oof[valid_index] = y_pred\n    \n    print('importance...')\n    imp_df = pd.DataFrame()\n    imp_df['feature'] = features\n    imp_df['gain'] = lgb_model.feature_importance()\n    imp_df['fold'] = 1\n    importances = pd.concat([importances, imp_df], axis=0, sort=False)\n    \n    # validaation\u30c7\u30fc\u30bf\u3078\u306e\u7cbe\u5ea6. \u4e88\u6e2c\u5024\u304c0.5\u4ee5\u4e0a\u306e\u3082\u306e\u3092\u751f\u5b58\u4e88\u6e2c\u3068\u3059\u308b\n    score = accuracy_score(y_val, y_pred>0.5)\n    scores.append(score)\n    print('*'*10, f'cv_score_{score}', '*'*10)","02861294":"# 5\u3064\u306ecv\u306e\u4e88\u6e2c\u7cbe\u5ea6\nscores","ef0ccbdb":"models","964c4076":"def save_importances(importances_: pd.DataFrame):\n    mean_gain = importances_[['gain', 'feature']].groupby('feature').mean()\n    importances_['mean_gain'] = importances_['feature'].map(mean_gain['gain'])\n    plt.figure(figsize=(8, 12))\n    sns.barplot(\n        x='gain',\n        y='feature',\n        data=importances_.sort_values('mean_gain', ascending=False)[:300])\n    plt.tight_layout()\n    plt.savefig('importances.png')\nsave_importances(importances)","1673d06d":"# \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3078\u306e\u4e88\u6e2c. 5cv\u3067\u4f5c\u3089\u308c\u305f5\u3064\u306e\u30e2\u30c7\u30eb\u306e\u4e88\u6e2c\u5024\u306e\u5e73\u5747\u5024\ny_test_pred  = np.zeros(len(df_test))\nfor model in models:\n    y_test_pred += model.predict(df_test[features])\ny_test_pred = y_test_pred \/ len(models)","e3e58cc0":"y_test_pred","cda5b327":"df_sub = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\ndf_sub","b28a974e":"plt.hist(y_test_pred)","ff0a027b":"y_test_pred = y_test_pred > 0.5","2b927d59":"df_sub['Survived'] = y_test_pred","cfee86d4":"df_sub['Survived'] = df_sub['Survived'].astype(int)\ndf_sub","51aa1bcd":"df_sub['Survived'].value_counts() \/ len(df_sub)","f1b9bb2f":"df_train['Survived'].value_counts() \/ len(df_train)","1e653ca6":"df_sub.to_csv(\"submission.csv\", index=False)","26accd40":"# \u4e00\u56de\u30c7\u30fc\u30bf\u306e\u78ba\u8a8d","a667847b":"\u30b3\u30e1\u30f3\u30c8: \n- \u3053\u3093\u306a\u611f\u3058\u3067\u4eee\u8aac\u30d9\u30fc\u30b9\u3067\u7279\u5fb4\u91cf\u3092\u4f5c\u308b\u3053\u3068\u304c\u591a\u3044\n- \u3082\u3061\u308d\u3093\u30b3\u30f3\u30da\u7279\u6709\u306e\u30a2\u30a4\u30c7\u30a3\u30a2\u3082\u5fc5\u8981\u3060\u304c\u3001\u8133\u5185DB\u304b\u3089\u904e\u53bb\u30b3\u30f3\u30da\u3067\u3053\u3093\u306a\u7279\u5fb4\u91cf\u304c\u52b9\u3044\u3066\u3044\u305f\u3068\u3044\u3046\u60c5\u5831\u3092\u5f15\u3063\u5f35\u308a\u51fa\u3059\u3053\u3068\u3082\u5927\u5207\u3002\n- \u5168\u7279\u5fb4\u91cf\u53ef\u8996\u5316\u3092\u3059\u308b\u304b\u3068\u8a00\u308f\u308c\u308c\u3070\u3001\u3059\u308b\u3079\u304d\u3060\u304c\u9762\u5012\u306a\u306e\u3067\u3057\u306a\u3044\u3002\u7279\u5fb4\u91cf\u8ffd\u52a0\u3057\u3066CV\u5024\u304c\u4e0a\u304c\u308b\u304b\u3092\u78ba\u8a8d","e79fd248":"kaggle\u306e\u4ed5\u69d8\u3068\u3057\u3066\u3001'submission.csv'\u3092\u6240\u5b9a\u306e\u4f4d\u7f6e\u306b\u66f8\u304d\u51fa\u3059\u3068submit\u3067\u304d\u308b","1e18aee6":"\u2191train\u3068val\u3067\u76ee\u7684\u5909\u6570\u306e\u5206\u5e03\u304c\u4e00\u81f4","8d032ba6":"\u30b3\u30e1\u30f3\u30c8\n- \u6642\u9593\u306e\u95a2\u4fc2\u7aef\u6298\u308a\u307e\u3059\u304c\u3001\u672c\u5f53\u306f\u3082\u3063\u3068\u3061\u3083\u3093\u3068\u53ef\u8996\u5316\u3059\u3079\u304d\n- \u3084\u308a\u8fbc\u3082\u3046\u3068\u601d\u3048\u3070\u7121\u9650\u306bEDA\u51fa\u6765\u308b\u306e\u3067\u7279\u306b\u4ed5\u4e8b\u3067\u306f\u5fc5\u8981\u306a\u90e8\u5206\u3092\u6700\u77ed\u3067\u884c\u3046\u80fd\u529b\u304c\u6c42\u3081\u3089\u308c\u308b\n- \u53c2\u8003: https:\/\/www.kaggle.com\/ash316\/eda-to-prediction-dietanic","cc694900":"# cv\u3092\u4f5c\u6210\u3057\u3001\u5b66\u7fd2","890be568":"## \u4eee\u8aac\u306b\u57fa\u3065\u304d\u7279\u5fb4\u91cf\u3092\u4f5c\u308b\n\n\u4f8b\u3048\u3070\u5bb6\u65cf\u306e\u4eba\u6570(Parch + sibsp)\u306f\u751f\u5b58\u7387\u306b\u52b9\u304d\u305d\u3046\u3068\u3044\u3046\u4eee\u8aac","d2117ff2":"\u500b\u4eba\u7684\u306b\u300c\u3078\u30fc\u300d\u3068\u601d\u3063\u305f\u30e9\u30f3\u30ad\u30f3\u30b01\u4f4d\u306e\u540d\u524d\u306e\u80a9\u66f8\u304d\u7279\u5fb4\u91cf","962e08b4":"# \u30c7\u30fc\u30bf\u306e\u78ba\u8a8d","6adae637":"# \u305d\u306e\u4ed6\u7279\u5fb4\u91cf\u8ffd\u52a0","049fe5db":"Warning\u306f\u300c\u3053\u308c\u4ee5\u4e0a\u6728\u3092\u6210\u9577\u3067\u304d\u307e\u305b\u3093\u300d\u3068\u3044\u3046\u610f\u5473  \nhttps:\/\/github.com\/microsoft\/LightGBM\/issues\/640  \n(\u30bf\u30a4\u30bf\u30cb\u30c3\u30af\u306bLightGBM\u306f\u3060\u3044\u3076\u5927\u9248\u306a\u306e\u3067\u3001\u6728\u3092\u5207\u308c\u306a\u304f\u306a\u308b\u3053\u3068\u306b\u7d0d\u5f97\u306f\u3057\u3066\u3044\u308b)  \n(\u3053\u306e\u7a0b\u5ea6\u306e\u30c7\u30fc\u30bf\u306a\u3089\u3001\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\u306e\u65b9\u304c\u3044\u3044\u304b\u3082\u3057\u308c\u306a\u3044)","94edaa07":"# sub\u3092\u4f5c\u308b","16c7d590":"CV\u3092\u5207\u308b  \n\u624b\u5143\u306b\u3042\u308b\u30c7\u30fc\u30bf\u304b\u3089\u306a\u3093\u3068\u304b\u672a\u77e5\u306e\u65b0\u898f\u30c7\u30fc\u30bf\u3063\u307d\u3044\u72b6\u6cc1\u3092\u4f5c\u308a\u51fa\u3059\u3092\u8907\u6570\u56de\u7e70\u308a\u8fd4\u3059  \n\u4eca\u56de\u306fStratifiedKFold(\u6b63\u76f4\u305d\u3053\u307e\u3067\u4e0d\u5747\u8861\u3067\u306a\u3044\u306e\u3067\u3001random split\u3067\u3044\u3044\u304b\u3082)  \nvalidation\u30c7\u30fc\u30bf\u3078\u306e\u4e88\u6e2c\u7cbe\u5ea6\u3067\u771f\u306e\u4e88\u6e2c\u7cbe\u5ea6\u3092\u985e\u63a8\u3059\u308b","c16b2253":"## pandas_profiling","c9bcb8de":"# \u4eee\u8aac\u30d9\u30fc\u30b9\u3067\u306e\u7279\u5fb4\u91cf\u4f5c\u6210","a65e0a41":"# EDA","334511ab":"\u9762\u5012\u306a\u306e\u3067\u3001\u8272\u3005\u306anotebook\u304b\u3089\u30b3\u30d4\u30da","1c955381":"## \u7c21\u5358\u306b\u53ef\u8996\u5316","974c5776":"Pclass(Ticket class)\u306b\u3088\u308a\u751f\u5b58\u7387\u304c\u5927\u304d\u304f\u5909\u308f\u308a\u305d\u3046","6b18f777":"# \u30ab\u30c6\u30b4\u30ea\u5909\u6570\u3092map"}}