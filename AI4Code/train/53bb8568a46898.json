{"cell_type":{"f0890578":"code","9e08d89f":"code","066541b2":"code","84b9953f":"code","9b35faee":"code","6add32ae":"code","16666421":"code","5c6010a7":"code","9bfc1d2c":"code","57e85f3c":"code","fdd6f682":"code","b860168b":"code","ae0c704c":"code","82a63b45":"code","fa021a2d":"code","1974ed0b":"code","69584474":"code","0e104323":"code","4de4b1aa":"code","5b1d4df8":"code","3844161d":"code","d77ec10a":"code","1fe665c0":"code","64a5d1c5":"code","6e08c16c":"code","17db63a8":"code","e496bb87":"code","e541c79d":"code","d006d955":"code","a2f01630":"code","d69698dc":"code","36c46840":"code","6ecacdd2":"code","c32e0d86":"code","cf177431":"code","d1c4796c":"code","7850bde3":"code","c31a15df":"code","54870da3":"code","ef900893":"code","1c595239":"code","937477f6":"code","16f94d54":"code","a4d61000":"code","563e2b5b":"code","92960953":"code","b292c5b4":"code","ad06e467":"code","d7f9c656":"code","ae7c3f0d":"code","6bd25b93":"code","b99b48ca":"code","fc1957fa":"code","865a62f2":"code","0f97cbac":"code","cfe6d8ad":"code","98ff99e8":"code","83c0e7bc":"code","8b962dce":"code","e3e47f43":"code","f31562f3":"code","81531b3d":"code","3c02801e":"code","4d6e067b":"code","4b40c92e":"code","97290351":"code","981593fa":"code","e2870c54":"code","c326a392":"code","87b5d99d":"code","d6f28845":"code","0517627d":"code","ef381f12":"code","d8d235ad":"code","bbc8d737":"code","6b2c926d":"code","98f92c09":"code","432e781a":"code","a108e469":"markdown","5fe5250c":"markdown","693b13bd":"markdown","855dcfac":"markdown","11f539aa":"markdown","a0742ae8":"markdown","791827f0":"markdown","e89d83db":"markdown","92e99e14":"markdown","dc903669":"markdown","f1df6e17":"markdown","0fcb59f2":"markdown","66fcb207":"markdown","4d421d73":"markdown","bf7a6b41":"markdown","daf00ba7":"markdown","9160f27a":"markdown","ce9862fa":"markdown","c4d72090":"markdown","0e73a4b1":"markdown","7bce6dc3":"markdown","d7a56314":"markdown","17791b07":"markdown","7ef14126":"markdown","9d1e8f0e":"markdown","d3209777":"markdown","9f150f45":"markdown","a64a01db":"markdown","a0ab5d04":"markdown","2c479c06":"markdown","6a66c214":"markdown","c6abdf2a":"markdown","af1fb279":"markdown","0ad71a7c":"markdown","4962cd36":"markdown","d2aa6ac8":"markdown","00f0aef4":"markdown","5d91db11":"markdown","712723df":"markdown","1c1a12b4":"markdown","25c0b2a7":"markdown","a6ffe6c1":"markdown","73307bc4":"markdown","3f29c3d7":"markdown","8828c79a":"markdown","1ec01b72":"markdown","5a7c77f5":"markdown","a4b2bd6c":"markdown","05e57fc8":"markdown","0e51193c":"markdown","d7018527":"markdown","43e39835":"markdown","75ea19da":"markdown","dddd4176":"markdown","752098cd":"markdown","dc576bc9":"markdown","14e024cd":"markdown","87d6d9cc":"markdown","1158a24a":"markdown","77db0fc4":"markdown","d94292ec":"markdown","f504c057":"markdown","c1078559":"markdown","2ae91a5d":"markdown","7181f2d3":"markdown","ce45c635":"markdown"},"source":{"f0890578":"!pip install tweepy","9e08d89f":"#Libraries Used\nimport pandas as pd\nimport requests\nimport tweepy\nimport os\nimport json\nimport numpy as np\nimport re\n%matplotlib inline","066541b2":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","84b9953f":"url = 'https:\/\/d17h27t6h515a5.cloudfront.net\/topher\/2017\/August\/599fd2ad_image-predictions\/image-predictions.tsv'\nresponse = requests.get(url)\nwith open(os.path.join(os.getcwd(), url.split('\/')[-1]), mode='wb') as file:\n    file.write(response.content)","9b35faee":"for dirname, _, filenames in os.walk('\/kaggle\/working'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","6add32ae":"consumer_key = 'U4hUt6MkwsBunrLeP7gBrfs9q'\nconsumer_secret = '9RBX4KeUOxVhBRab0TBHjDcJp9hcSyvHieyA50as2auN5PxzWJ'\naccess_token = '1929558530-UuS1sgoWlZtz5xHhJVbWpq0pWoCdR9X7H8Cq89P'\naccess_secret = '36sqFNfP4b8QIY374adQUgUBrk0Ui5UEB4i3Z5e2qS5Qm'","16666421":"auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\nauth.set_access_token(access_token, access_secret)\n\napi = tweepy.API(auth, parser=tweepy.parsers.JSONParser(), wait_on_rate_limit=True)","5c6010a7":"df_twitter_archive = pd.read_csv('..\/input\/twitter-archive-enhanced.csv')","9bfc1d2c":"for tweet_id in df_twitter_archive.tweet_id:\n    try:\n        tweet_json = api.get_status(tweet_id, tweet_mode = 'extented')\n        with open('\/kaggle\/working\/tweet_json.txt', mode='a') as file:\n            json.dump(tweet_json, file)\n            file.write('\\n')\n    except Exception as e:\n        print(str(tweet_id) + ': ' + str(e))","57e85f3c":"lists = [] #an empyty list to store a dictionaries\nwith open('\/kaggle\/working\/tweet_json.txt') as file:\n    lines = file.read().splitlines()\n    for line in lines:\n        data = json.loads(line)\n        row = {\n            'tweet_id'      : data['id'],\n            'retweet_count' : data['retweet_count'],\n            'favorite_count': data['favorite_count']\n        }\n        lists.append(row)\ndf_tweet_data = pd.DataFrame(lists,columns=['tweet_id','retweet_count','favorite_count'])","fdd6f682":"df_img_predictions = pd.read_csv('\/kaggle\/working\/image-predictions.tsv', sep='\\t')","b860168b":"df_twitter_archive","ae0c704c":"df_img_predictions","82a63b45":"df_tweet_data","fa021a2d":"df_twitter_archive.info()","1974ed0b":"df_img_predictions.info()","69584474":"df_tweet_data.info()","0e104323":"all_columns = pd.Series(list(df_twitter_archive) + list(df_img_predictions) + list(df_tweet_data))\nall_columns[all_columns.duplicated()]","4de4b1aa":"df_twitter_archive.tweet_id.nunique()","5b1d4df8":"df_img_predictions.tweet_id.nunique()","3844161d":"df_tweet_data.tweet_id.nunique()","d77ec10a":"df_twitter_archive[df_twitter_archive.text.duplicated()]","1fe665c0":"df_twitter_archive.source.value_counts()","64a5d1c5":"df_twitter_archive.sample(25)","6e08c16c":"df_twitter_archive.name.isnull().sum()","17db63a8":"df_twitter_archive.loc[np.random.randint(0,df_twitter_archive.shape[0],40), ['text','name']]","e496bb87":"df_twitter_archive.name.value_counts()","e541c79d":"df_twitter_archive.describe()","d006d955":"df_twitter_archive.rating_numerator.value_counts()","a2f01630":"df_twitter_archive.rating_denominator.value_counts()","d69698dc":"df_img_predictions.describe()","36c46840":"df_tweet_data.describe()","6ecacdd2":"#Create a copy of all the gathered dataframes\ndf_twitter_archive_copy = df_twitter_archive.copy()\ndf_img_predictions_copy = df_img_predictions.copy()\ndf_tweet_data_copy = df_tweet_data.copy()","c32e0d86":"#In the text the name always starts with a capital letter.\ndef extract_name_from_text(row):\n    try:\n        if 'This is' in row['text']:\n            name = re.search('This is ([A-Z]\\w+)',row['text']).group(1)\n        elif 'Meet' in row.text:\n            name = re.search('Meet ([A-Z]\\w+)', row['text']).group(1)\n        elif 'Say hello to' in row.text:\n            name = re.search('Say hello to ([A-Z]\\w+)', row['text']).group(1)\n        elif 'named' in row.text:\n            name = re.search('named ([A-Z]\\w+)', row['text']).group(1)\n        else:\n            name = ''\n    except AttributeError:\n        name = ''\n    return name","cf177431":"df_twitter_archive_copy['name'] = df_twitter_archive_copy.apply(extract_name_from_text, axis=1)","d1c4796c":"df_twitter_archive_copy.name.value_counts()","7850bde3":"def extract_source(row):\n    try:\n        source = re.search('>(.+)<\/a>', row['source']).group(1)\n    except AttributeError:\n        source = ''\n    return source","c31a15df":"df_twitter_archive_copy['source'] = df_twitter_archive_copy.apply(extract_source, axis=1)\ndf_twitter_archive_copy['source'] = df_twitter_archive_copy.source.astype('category')","54870da3":"df_twitter_archive_copy.source.dtype","ef900893":"df_twitter_archive_copy.source.value_counts()","1c595239":"def extract_gender(row):\n    if 'He' in row['text']:\n        gender = 'M'\n    elif 'She' in row['text']:\n        gender = 'F'\n    else:\n        gender = ''\n    return gender","937477f6":"df_twitter_archive_copy['gender'] = df_twitter_archive_copy.apply(extract_gender, axis=1)\ndf_twitter_archive_copy['gender'] = df_twitter_archive_copy.gender.astype('category')","16f94d54":"df_twitter_archive_copy.gender.dtype","a4d61000":"df_twitter_archive_copy.gender.value_counts()","563e2b5b":"def extract_hashtag(row):\n    try:\n        if '#' in row['text']:\n            hashtag = re.search('#(\\w+)[\\s\\.]', row['text']).group(1)\n        else:\n            hashtag = float('NaN')\n    except AttributeError:\n        hashtag = ''\n    return hashtag\n\ndf_twitter_archive_copy['hashtag'] = df_twitter_archive_copy.apply(extract_hashtag, axis=1)","92960953":"df_twitter_archive_copy.hashtag.value_counts()","b292c5b4":"def get_dog_stage(row):\n    if 'doggo' in row['text'].lower():\n        stage = 'doggo'\n    elif 'floof' in row['text'].lower():\n        stage = 'floofer'\n    elif 'pupper' in row['text'].lower():\n        stage = 'pupper'\n    elif 'puppo' in row['text'].lower():\n        stage = 'puppo'\n    else:\n        stage = ''\n    return stage","ad06e467":"df_twitter_archive_copy['stage'] = df_twitter_archive_copy.apply(get_dog_stage, axis=1)\ndf_twitter_archive_copy['stage'] = df_twitter_archive_copy.stage.astype('category')","d7f9c656":"df_twitter_archive_copy.drop(['doggo','pupper','floofer','puppo'], axis=1, inplace=True)","ae7c3f0d":"df_twitter_archive_copy.stage.value_counts()","6bd25b93":"df_twitter_archive_copy.stage.dtype","b99b48ca":"list(df_twitter_archive_copy)","fc1957fa":"breed = []\nconfidence = []\n\ndef get_breed_and_confidence(row):\n    if row['p1_dog'] == True:\n        breed.append(row['p1'])\n        confidence.append(row['p1_conf'])\n    elif row['p2_dog'] == True:\n        breed.append(row['p2'])\n        confidence.append(row['p2_conf'])\n    elif row['p3_dog'] == True:\n        breed.append(row['p3'])\n        confidence.append(row['p3_conf'])\n    else:\n        breed.append('Not identified')\n        confidence.append(np.nan)\n        \ndf_img_predictions_copy.apply(get_breed_and_confidence, axis=1)\ndf_img_predictions_copy['breed'] = pd.Series(breed)\ndf_img_predictions_copy['confidence'] = pd.Series(confidence)\ndf_img_predictions_copy.drop(['p1','p1_conf','p1_dog','p2','p2_conf','p2_dog','p3','p3_conf','p3_dog'], axis=1, inplace=True)","865a62f2":"df_img_predictions_copy.head()","0f97cbac":"df_img_predictions_copy.info()","cfe6d8ad":"df = pd.merge(df_twitter_archive_copy, df_img_predictions_copy, on='tweet_id')\ndf = df.merge(df_tweet_data_copy, on='tweet_id')","98ff99e8":"list(df)","83c0e7bc":"df.head()","8b962dce":"df.info()","e3e47f43":"df = df.query('breed != \"Not identified\"')","f31562f3":"df.query('breed == \"Not identified\"').shape[0]","81531b3d":"df.info()","3c02801e":"df.drop(['in_reply_to_status_id', 'in_reply_to_user_id', 'retweeted_status_user_id', 'retweeted_status_id', 'retweeted_status_timestamp'], axis=1, inplace=True)","4d6e067b":"df.columns","4b40c92e":"df.tweet_id = df.tweet_id.to_string()\ndf.timestamp = pd.to_datetime(df.timestamp, yearfirst=True)","97290351":"df.info()","981593fa":"df_twitter_archive_copy.rating_numerator.describe()","e2870c54":"df['rating'] = df.rating_numerator\/df.rating_denominator\n\n#Use ratings to divide into categories\ndf['rating_category'] = pd.cut(df.rating, bins = [0.0, np.percentile(df.rating,25), np.percentile(df.rating,50), np.percentile(df.rating,75), np.max(df.rating)],labels=['Low','Below_average','Above_average','High'])\n\n#Drop the unwanted columns\ndf.drop(['rating_numerator','rating_denominator'], axis=1, inplace=True)","c326a392":"df.rating_category.value_counts()","87b5d99d":"df.columns","d6f28845":"df.loc[df['name'] == '', 'name'] = None\ndf.loc[df['gender'] == '', 'gender'] = None\ndf.loc[df['stage'] == '', 'stage'] = None\ndf.loc[df['breed'] == '', 'breed'] = None\ndf.loc[df['rating'] == 0.0, 'rating'] = np.nan\ndf.loc[df['rating'] == 0.0, 'rating_category'] = None","0517627d":"df.info()","ef381f12":"#Store the final cleaned dataframe\ndf.to_csv('twitter_archive_master.csv', index=False)","d8d235ad":"df.gender.value_counts().plot(kind='bar');","bbc8d737":"df.source.value_counts().plot(kind='bar');","6b2c926d":"df.name.value_counts()[0:19].plot(kind='bar');","98f92c09":"df.breed.value_counts()[0:19].plot(kind='bar');","432e781a":"#group by breed and store the means of retweet_count and favorite_count.\ndf_group = df.groupby(['breed'])['retweet_count', 'favorite_count'].mean()\n#order by retweet_count and favorite_count.\ndf_group = df_group.sort_values(['retweet_count', 'favorite_count'], ascending=False)\n#plot the top 15 average counts.\ndf_group.iloc[0:14,].plot(kind='bar');","a108e469":"##### Code","5fe5250c":"##### Code","693b13bd":"#### Missing and incorrect dog names extracted from text","855dcfac":"A lot of columns still have value as '' or 0.0. These should be coverted to NaN in case of a quantitative variable(rating) and None in case of a qualitative variable(name,  gender, stage, breed, rating_category). Also rating as 0 should be NaN.","11f539aa":"##### Define\nRemove these unwanted using drop function in pandas.","a0742ae8":"The ratings seem to be absurd as the numerator is greater than the denominator and also the numerator varies over a long range.Create a new variable called rating which stores the ratio of the numerator and denominator and accordingly divide the dogs into different categories using the ratio value. Also, remove the rating_numerator and rating_denominator colums.","791827f0":"#### A lot of null values are not null.","e89d83db":"##### Code","92e99e14":"### Quality","dc903669":"##### Test","f1df6e17":"##### Define\ntweet_id which is in int64 format should be in string format as we don't need to perform any mathematic operations on tweet_id. The timestamp should be an datetime object instead of string. Use pandas to_string() function to convert tweet_id to string and use to_datetime() function to convert timestamp to datetime object.","0fcb59f2":"##### Code","66fcb207":"##### Define\nInstead of showing 3 predictions, show the top dog breed prediction. Only consider that prediction for which the dog prediction is true. Also, remove the unwanted columns.","4d421d73":"## Gather\nFor this I had to gather three pieces of data, all three from different sources. First, I gathered the WeRateDogs Twitter archive from a csv file which was manually downloaded from a link provided. Second, I gathered the tweet image predictions data from a link programatically using the python library requests. Finally, I used Tweepy python access library for Twitter to fetch the tweet data for each tweet_id in the WeRateDogs. Using the tweepy library, I got JSON data which I wrote to a text file, gathering all the tweets JSON data in text file. Later on fetched tweet ID, retweet count, and favorite count from the text file line by line and then created a data base.","bf7a6b41":"##### Test","daf00ba7":"#### Hashtags could also be extracted from text","9160f27a":"##### Test","ce9862fa":"### Analyse","c4d72090":"##### Test","0e73a4b1":"#### The `image predictions` could be condensed to show just the most confident dog breed prediction","7bce6dc3":"#### Test","d7a56314":"##### Test","17791b07":"### Quality\n- Missing and incorrect dog names extracted from text. 'a' is the most popular name which itself is not a name.\n- Timestamp is in string format.\n- Source not extracted properly from hyperlink tag.\n- A lot of null values are not null.\n- Columns: in_reply_to_status_id, in_reply_to_user_id, retweeted_status_user_id, retweeted_status_id and retweeted_status_timestamp, have a lot of null values.\n- Gender of dog could be extracted from text.\n- Hashtags could also be extracted.\n- Absurd rating values.\n- Records without dog breed prediction\n\n### Tidiness\n- The dog stage columns in twitter_archive can be arranged into a single column.\n- The image predictions could be condensed to show just the most confident dog breed prediction.\n- All three dataframes can be combined into one single dataframe.","7ef14126":"#### Source not extracted properly from hyperlink tag","9d1e8f0e":"##### Define","d3209777":"##### Code","9f150f45":"##### Define\n","a64a01db":"Since, the input directory is a read-only directory. So,we can write the file into the working directory.","a0ab5d04":"# Data Wrangling\nI worked on this project where i had to wrangle tweet data from different sources to create a clean dataset on which i could perform analysis and provide useful insights. First, let me explain what is Data Wrangling?\n> *Data Wrangling* is a process where first data is gathered from different sources, then the quality of the data is assessed and finally the data is cleaned to create a dataset on which exploratory data analysis could be performed.\n\nBasically the following three processes are performed in Data Wrangling:-\n\n- **Gather** : The data is gathered from different sources. The data could be downloaded from a link, scraped from a website, uploaded from txt, csv and more kind of files.\n- **Assess** : After gathering data, it is assesed visually as well as programatically. The data is assesed on the basis of its quality and tidiness. Dirty(poor quality) data and untidy(messy) data are the two unwanted traits of gathered data that should be assessed properly and then cleaned.\n- **Clean** : After assessing the gathered data and noting down all the unwanted traits, the data is cleaned programatically. Cleaning consists of three steps: Define, Code & Test.","2c479c06":"##### Test","6a66c214":"##### Code","c6abdf2a":"##### Define\nExtract the hashtag from the tweet using regular expressions. Since, # is always exceeded by alphanumeric characters and after the hashtag there is a whitespace or fullstop, a regular expression can be created.","af1fb279":"#### Incorrect Data types","0ad71a7c":"Get your credentials from [here](https:\/\/developer.twitter.com\/en.html)    ","4962cd36":"#### Test","d2aa6ac8":"#### Top Sources\nOut of the 4 sources, `Twitter for iPhone` is clearly the most widely used source to share tweets peratining to dogs.  ","00f0aef4":"##### Code","5d91db11":"#### Records without dog breed prediction","712723df":"#### Gender Analysis\n`Male` dogs are more famous as compared to female dogs. ","1c1a12b4":"##### Define\nRemove the records from dataframe where breed is Not identified. Use pandas query() function to select the records accordingly.","25c0b2a7":"##### Code","a6ffe6c1":"##### Test","73307bc4":"### Missing Data","3f29c3d7":"##### Define\nExtract the correct names from the text column using regular expression and also get rid of the incorrect names like 'a', 'an', 'the',etc. <b>search()<\/b> function of the <b>re<\/b> library can be used to extract the names from the text. Notice that the dog names always start with an uppercase character and then is followed by all lowercase characters.","8828c79a":"##### Define\nOn assessing it can be seen that almost all of the texts are indicative of the gender of the dog as 'He'\/'She' is used. Extract the gender of the dog tweet using string operations by searching for He\/She in the text.","1ec01b72":"### Tidiness","5a7c77f5":"##### Test","a4b2bd6c":"### Store","05e57fc8":"#### Top Breeds\n`Golden Retriever` is top breed. ","0e51193c":"##### Define\nExtract the proper source of the dog tweet using regular expression and since there are only 4 unique sources convert source to a categorical variable. Create a generic function to extract the sources and then use <b>apply()<\/b> function of the <b>pandas<\/b> library to apply the function to the entire column.","d7018527":"#### Gender of the dog could be extracted from text","43e39835":"#### Columns: in_reply_to_status_id, in_reply_to_user_id, retweeted_status_user_id, retweeted_status_id and retweeted_status_timestamp, have a lot of null values","75ea19da":"## Clean\nCleaning process consists of three steps: Define, code & Test. First we define how to tackle the issue. Then, we code to resolve the issue and finally we test our code to see if the issues with the data have been resolved. So, in order to clean these 3 dataframes, I carried out the 3 steps for each of the issues and was finally able to achive a clean dataframe. For cleaning purposes, I used pandas's functions: cut, merge, apply, etc. The cleaned dataset was then stored into a csv file.","dddd4176":"#### Code","752098cd":"##### Test","dc576bc9":"#### Average Retweet and Favorite counts for dog breeds\n`Standard_poodle` had the highest average retweet count while `Saluki` had the highest favorite count.","14e024cd":"##### Code","87d6d9cc":"##### Test","1158a24a":"#### Top Names\n`Cooper`, `Lucy`, `Tucker` and `Charlie` are the most common dog names.","77db0fc4":"#### Define\nAll the columns in the 3 dataframes describe the dog data and can be fit into a single table for further analysis and visualisations. Use the pandas merge function to merge all the three dataframes on tweet_id.","d94292ec":"#### The dog stage columns in `twitter_archive` can be arranged into a single column","f504c057":"#### Define\nCondense the 4 dog stages(doggo, floofer, puppo, gender) into a single stage column and convert it into a categorical variable. Also, remove the unwanted columns.","c1078559":"##### Code","2ae91a5d":"#### Absurd rating values","7181f2d3":"#### All three dataframes can be combined into one single dataframe","ce45c635":"## Assess\nAfter gathering all the 3 files, I stored their data into a dataframe for easier assessment and cleaning. In order to assess the data, I examined it visually and programatically using python's pandaslibrary. First, i printed out all the dataframes entirely, used the info() function to assess the datatypes, used describe() function to summarise the quantitative variables in the datasets, etc. Then i examined the dataframes more specifically by examining each variable separately and found out the following issues:-"}}