{"cell_type":{"900c86db":"code","ff022ed1":"code","648035c6":"code","52a72c6d":"code","4c0dcc55":"code","9a86cef9":"code","c510f1d3":"code","d55ea1ae":"code","2e6f97fb":"code","333268d7":"code","5c79473b":"code","68eae30e":"code","23b15aa0":"code","0e341f0e":"code","3e2bc3a8":"code","ff7040cc":"code","3ba75a3b":"code","cdcc23e2":"code","b7b0572a":"code","e13bae74":"code","dbff3011":"code","90a4ab7b":"code","346fd927":"code","194512d4":"code","8fa7f0ab":"code","f5312880":"code","2a8cb476":"code","e3141ce3":"code","d52d63a1":"code","1e09a1a7":"code","a542642b":"code","1b9e81c3":"code","9b372107":"code","6a088146":"code","211ba69d":"code","53523e01":"code","a1a75af3":"code","4fdfc56e":"code","8a45cd8e":"code","a2392522":"code","5eed0889":"code","3aafaddd":"code","9b7b16aa":"code","40ca7a20":"code","bc444843":"code","b8325984":"code","e71340dd":"code","d2df9332":"code","619d4a77":"code","ffd98495":"code","f814fad0":"code","44fdf5da":"code","6f0ce72b":"code","7df62f1c":"code","76c10793":"code","c06e7384":"code","ce0b3686":"code","ccf442df":"code","45e06560":"code","33639c83":"code","3228db47":"code","ac663420":"markdown","9c825df0":"markdown","7406546a":"markdown","42a2f9bd":"markdown","1409c9dc":"markdown","daf4f1e0":"markdown","5006f1de":"markdown","c299a2e2":"markdown","c6e42d2f":"markdown","85f7d43c":"markdown","5d597bab":"markdown","956cd6e3":"markdown"},"source":{"900c86db":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nplt.style.use(\"seaborn-whitegrid\")       \nimport pandas_profiling as pp \n\nimport seaborn as sns\n\nfrom collections import Counter\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ff022ed1":"df = pd.read_csv(\"\/kaggle\/input\/diabetes\/diabetes.csv\")","648035c6":"df.head()","52a72c6d":"df.info()","4c0dcc55":"df.isnull().sum()","9a86cef9":"df.describe().T","c510f1d3":"import pandas_profiling as pp \nprofile_df = pp.ProfileReport(df)","d55ea1ae":"profile_df","2e6f97fb":"df.hist(figsize=(10, 10), bins=50, xlabelsize=5, ylabelsize=5);","333268d7":"sns.catplot(x=\"Outcome\",data=df, kind=\"count\");","5c79473b":"df.plot(kind=\"density\", layout=(6,5),subplots=True,sharex=False, sharey=False, figsize=(15,15));\nplt.tight_layout() ","68eae30e":"sns.pairplot(df, kind = \"reg\")","23b15aa0":"df_corr = df.corr()","0e341f0e":"sns.heatmap(df_corr, linewidths = 1);","3e2bc3a8":"sns.pairplot(df_corr, kind = \"reg\");","ff7040cc":"from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.metrics import roc_auc_score,roc_curve","3ba75a3b":"X = df.drop([\"Outcome\"], axis = 1)\ny = df[\"Outcome\"]\n\n#or \n#X = df[:,0:8]\n#y = df[:, 8]","cdcc23e2":"X_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size = 0.30, \n                                                    random_state = 42)","b7b0572a":"from sklearn.linear_model import LogisticRegression\nlog = LogisticRegression(solver = \"liblinear\")\nlog_model = log.fit(X_train,y_train)\nlog_model","e13bae74":"y_pred = log_model.predict(X_test)","dbff3011":"\nconfusion_matrix(y_test, y_pred)\nprint(classification_report(y_test, y_pred))\n","90a4ab7b":"accuracy_score(y_test, log_model.predict(X_test))\ncross_val_score(log_model, X_test, y_test, cv = 10).mean()","346fd927":"logit_roc_auc = roc_auc_score(y_test, log_model.predict(X_test))\n\nfpr, tpr, thresholds = roc_curve(y_test, log_model.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='AUC (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive ')\nplt.ylabel('True Positive ')\nplt.title('ROC')\nplt.show()","194512d4":"from sklearn.naive_bayes import GaussianNB\n\n\nnb = GaussianNB()\nnb_model = nb.fit(X_train, y_train)\nnb_model\n","8fa7f0ab":"y_pred = nb_model.predict(X_test)\naccuracy_score(y_test, y_pred)\n","f5312880":"cross_val_score(nb_model, X_test, y_test, cv = 10).mean()","2a8cb476":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn_model = knn.fit(X_train, y_train)\nknn_model\n\n","e3141ce3":"y_pred = knn_model.predict(X_test)\naccuracy_score(y_test, y_pred)\n\n","d52d63a1":"knn_params = {\"n_neighbors\": np.arange(1,20)}\nknn = KNeighborsClassifier()\nknn_cv = GridSearchCV(knn, knn_params, cv=10)\nknn_cv.fit(X_train, y_train)","1e09a1a7":"print(\"Best KNN score:\" + str(knn_cv.best_score_))\nprint(\"Best KNN parameter: \" + str(knn_cv.best_params_))","a542642b":"knn = KNeighborsClassifier(1)\nknn_tuned = knn.fit(X_train, y_train)","1b9e81c3":"y_pred = knn_tuned.predict(X_test)\naccuracy_score(y_test, y_pred)","9b372107":"d = {'Accuracy in KNN before GridSearchCV ': [0.77], 'Accuracy in KNN After GridSearchCV': [0.95]}\nknn_data = pd.DataFrame(data=d)\nknn_data","6a088146":"from sklearn.svm import SVC\n\n\nsvm_model = SVC(kernel = \"linear\").fit(X_train, y_train)\n\ny_pred = svm_model.predict(X_test)\naccuracy_score(y_test, y_pred)\n\n","211ba69d":"svc_params = {\"C\": np.arange(1,10)}\n\nsvc = SVC(kernel = \"linear\")\n\nsvc_cv_model = GridSearchCV(svc,svc_params, \n                            cv = 10, \n                            n_jobs = -1, \n                            verbose = 2 )\nsvc_cv_model.fit(X_train, y_train)\n","53523e01":"print(\"Best Params: \" + str(svc_cv_model.best_params_))","a1a75af3":"svc_tuned = SVC(kernel = \"linear\", C = 2).fit(X_train, y_train)\n\ny_pred = svc_tuned.predict(X_test)\naccuracy_score(y_test, y_pred)","4fdfc56e":"confusion_matrix(y_test, y_pred)\nprint(classification_report(y_test, y_pred))","8a45cd8e":"d = {'Accuracy in SVM before GridSearchCV ': [0.7983], 'Accuracy in SVM After GridSearchCV': [0.7933]}\nsvm_data = pd.DataFrame(data=d)\nsvm_data","a2392522":"from sklearn.ensemble import RandomForestClassifier\nrf_model = RandomForestClassifier().fit(X_train, y_train)\ny_pred = rf_model.predict(X_test)\naccuracy_score(y_test, y_pred)","5eed0889":"rf_params = {\"max_depth\": [2,5,8],\n            \"max_features\": [2,5,8],\n            \"n_estimators\": [10,500,1000],\n            \"min_samples_split\": [2,5,10]}\n\nrf_model = RandomForestClassifier()\n\nrf_cv_model = GridSearchCV(rf_model, \n                           rf_params, \n                           cv = 10, \n                           n_jobs = -1, \n                           verbose = 2) \n\nrf_cv_model.fit(X_train, y_train)","3aafaddd":"print(\"Best Params: \" + str(rf_cv_model.best_params_))","9b7b16aa":"rf_tuned = RandomForestClassifier(max_depth = 8, \n                                  max_features = 8, \n                                  min_samples_split = 2,\n                                  n_estimators = 1000)","40ca7a20":"rf_tuned.fit(X_train, y_train)\ny_pred = rf_tuned.predict(X_test)\naccuracy_score(y_test, y_pred)","bc444843":"confusion_matrix(y_test, y_pred)\nprint(classification_report(y_test, y_pred))","b8325984":"Importance = pd.DataFrame({\"Importance\": rf_tuned.feature_importances_*100},\n                         index = X_train.columns)\n\nImportance.sort_values(by = \"Importance\", \n                       axis = 0, \n                       ascending = True).plot(kind =\"barh\", color = \"r\");\n\n","e71340dd":"d = {'Accuracy in RF before GridSearchCV ': [0.97], 'Accuracy in RF After GridSearchCV': [0.92]}\nrf_data = pd.DataFrame(data=d)\nrf_data","d2df9332":"from sklearn.ensemble import GradientBoostingClassifier\n\ngbm_model = GradientBoostingClassifier().fit(X_train, y_train)\n\ny_pred = gbm_model.predict(X_test)\naccuracy_score(y_test, y_pred)","619d4a77":"gbm_params = {\"learning_rate\" : [0.001, 0.01, 0.1, 0.05],\n             \"n_estimators\": [100,500,100],\n             \"max_depth\": [3,5,10],\n             \"min_samples_split\": [2,5,10]}\n\ngbm = GradientBoostingClassifier()\n\ngbm_cv = GridSearchCV(gbm, gbm_params, cv = 10, n_jobs = -1, verbose = 2)\ngbm_cv.fit(X_train, y_train)","ffd98495":"print(\"Best Params: \" + str(gbm_cv.best_params_))","f814fad0":"gbm = GradientBoostingClassifier(learning_rate = 0.1, \n                                 max_depth = 10,\n                                min_samples_split = 2,\n                                n_estimators = 100)\n\ngbm_tuned =  gbm.fit(X_train,y_train)","44fdf5da":"y_pred = gbm_tuned.predict(X_test)\naccuracy_score(y_test, y_pred)","6f0ce72b":"confusion_matrix(y_test, y_pred)\nprint(classification_report(y_test, y_pred))","7df62f1c":"Importance = pd.DataFrame({\"Importance\": gbm_tuned.feature_importances_*100},\n                         index = X_train.columns)\n\nImportance.sort_values(by = \"Importance\", \n                       axis = 0, \n                       ascending = True).plot(kind =\"barh\", color = \"r\");","76c10793":"d = {'Accuracy in GBM before GridSearchCV ': [0.87], 'Accuracy in GBM After GridSearchCV': [0.95]}\ngbm_data = pd.DataFrame(data=d)\ngbm_data","c06e7384":"models = [\n    knn_tuned,\n    log_model,\n    svc_tuned,\n    nb_model,\n    rf_tuned,\n    gbm_tuned,\n    \n]\n\n\nfor model in models:\n    name = model.__class__.__name__\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    print(\"-\"*28)\n    print(name + \":\" )\n    print(\"Accuracy: {:.4%}\".format(accuracy))","ce0b3686":"result = []\n\nresults = pd.DataFrame(columns= [\"Models\",\"Accuracy\"])\n\nfor model in models:\n    name = model.__class__.__name__\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)    \n    result = pd.DataFrame([[name, accuracy*100]], columns= [\"Models\",\"Accuracy\"])\n    results = results.append(result)\n    \n    \nsns.barplot(x= 'Accuracy', y = 'Models', data=results, color=\"r\")\nplt.xlabel('Accuracy %')\nplt.title('accuracy rate of models'); ","ccf442df":"sns.catplot(x=\"Outcome\",data=df, kind=\"count\");","45e06560":"df[\"Outcome\"].value_counts()","33639c83":"df[\"Insulin\"].value_counts() ","3228db47":"df[\"BMI\"].value_counts()","ac663420":"* You can use PandasProfile in order to analysis data.\n","9c825df0":"* As can be seen in heatmap and pairplot, there is no specific correlation between both data and result.","7406546a":"# Naive Bayes","42a2f9bd":"# Logistic Regression","1409c9dc":"# Data Visualization","daf4f1e0":"# GBM","5006f1de":"* You can use visualization tools(such as seaborn, matmatplotlib) in order to analysis data. ","c299a2e2":"# SVM - Support Vector Model","c6e42d2f":"# Model Selection","85f7d43c":"# Suggestions\n\n* There is a high difference between the outcome data.( 0: 1316 , 1 : 684). This can affect model results. You use *from sklearn.utils import class_weight* in order to avoid unbalanced distribution.\n  \n* There are too many zeros in the database.Especially in values such as insulin, glucose, BMI.If your values are zero, you are probably dead.:) So, you find zeros in this values and drop its. After doing this, you can model it again.\n\n* You can apply a standardscaler to the data before modeling.\n\n* You can examine how all these changes affect the results.","5d597bab":"# KNN","956cd6e3":"# Random Forests"}}