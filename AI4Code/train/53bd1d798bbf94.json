{"cell_type":{"b6001b27":"code","9feeacf2":"code","a4b22e71":"code","144f1e13":"code","110733e7":"code","c39c441b":"code","4efcd1c9":"code","1131811c":"code","d0d9aff3":"code","da44e38f":"code","a8e60f12":"code","54ef90da":"code","eb22cf6b":"code","0d838720":"code","e0cd1d02":"code","17b1c165":"code","5ddc3cc7":"code","0a767545":"code","65a622c6":"code","bc6be333":"code","8b2b0d18":"code","cb7df8a5":"code","5f956a24":"code","f93bbfcb":"code","e19fef45":"code","f755b8c3":"code","a73ae737":"code","2b7b93d0":"code","061668c1":"code","1477a136":"code","0a096fba":"code","acfd9a2d":"code","82f6c7ac":"code","199685cb":"code","0a60667d":"code","cfc9b592":"code","8d7b6e4e":"code","c76b4220":"code","770235da":"code","ae07f8b4":"code","1f82a510":"code","95f579e8":"code","30b4df21":"code","4a1b3084":"code","fc4c68b1":"code","9a867417":"code","e7870df5":"code","34abafe7":"code","deb82906":"code","c5bb4efe":"code","f1c86f99":"code","59e4b0f9":"code","3e82a01a":"code","01fa5475":"code","96973cc8":"code","3d1167c0":"markdown","1a2c1f65":"markdown","ebc06a17":"markdown","bdf3da33":"markdown","c937114c":"markdown","3ff33f1f":"markdown","77afdc73":"markdown","3bd5781a":"markdown","8b9e81e0":"markdown","2b54f122":"markdown","3dae36ef":"markdown","8e2d0f49":"markdown","9e4af956":"markdown","7cdd9a99":"markdown","56b314cd":"markdown","748491ea":"markdown","87d4fc18":"markdown","4868b010":"markdown","6bb736a1":"markdown","6727cb8b":"markdown","eb7e71a4":"markdown","9f7ee8f9":"markdown","36773cc9":"markdown"},"source":{"b6001b27":"# %reload_ext nb_black\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport plotly\nimport seaborn as sns\nimport plotly.express as px\n\n%matplotlib inline\n\n# plt.style.use([\"dark_background\"])\n# %matplotlib ipympl\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom category_encoders import LeaveOneOutEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\nimport warnings\n\n# from functions_pkg import print_vif, predictions_df\nfrom sklearn.metrics import (\n    plot_confusion_matrix,\n    plot_roc_curve,\n    plot_precision_recall_curve,\n    classification_report,\n    confusion_matrix,\n    precision_score,\n    recall_score,\n)\nfrom sklearn.calibration import calibration_curve","9feeacf2":"# functions from package that won't import into notebook\ndef print_vif(feature_df):\n    \"\"\"\n    Utility for checking multicollinearity assumption\n    :param feature_df: input features to check using VIF. This is assumed to be a pandas.DataFrame\n    :return: nothing is returned the VIFs are printed as a pandas series\n    \"\"\"\n    # Silence numpy FutureWarning about .ptp\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        feature_df = sm.add_constant(feature_df)\n\n    vifs = []\n    for i in range(feature_df.shape[1]):\n        vif = variance_inflation_factor(feature_df.values, i)\n        vifs.append(vif)\n\n    print(\"VIF results\\n-------------------------------\")\n    print(pd.Series(vifs, index=feature_df.columns))\n    print(\"-------------------------------\\n\")\n\n\ndef predictions_df(X_test, y_test, y_preds):\n    \"\"\"\n    Function to create a predictions dataframe from X_test, y_test, y_predictions input\n\n    :param X_test:\n    :param y_test:\n    :param y_preds: X_test predictions; model.predict(X_test)\n    :return pred_df, fig: returns predictions data frame and plotly express fig object\n    \"\"\"\n\n    pred_df = X_test.copy()\n    pred_df[\"y_true\"] = y_test\n    pred_df[\"y_preds\"] = y_preds\n    pred_df[\"residuals\"] = pred_df.y_preds - pred_df.y_true\n    pred_df[\"abs_residuals\"] = pred_df.residuals.abs()\n    pred_df = pred_df.sort_values(\"abs_residuals\", ascending=False)\n\n    fig = px.scatter(data_frame=pred_df, x=\"y_true\", y=\"y_preds\")\n    fig.add_shape(\n        type=\"line\", x0=y_test.min(), y0=y_test.min(), x1=y_test.max(), y1=y_test.max()\n    )\n\n    return pred_df, fig","a4b22e71":"path = \"..\/input\/cardiovascular-disease-dataset\/cardio_train.csv\"\ndf = pd.read_csv(path, sep=\";\", index_col=\"id\")","144f1e13":"# column renaming\nmapping = {\n    \"ap_hi\": \"bp_hi\",\n    \"ap_lo\": \"bp_lo\",\n    \"gluc\": \"glucose\",\n    \"alco\": \"alcohol\",\n    \"cardio\": \"disease\",\n}\n\ndf = df.rename(columns=mapping)","110733e7":"# dataset is well balanced\ndisplay(df.disease.value_counts())\n\n# gender is a bit unbalanced in dataset\ndisplay(df.gender.value_counts())","c39c441b":"# no null values in the data\ndf.isna().mean().sort_values(ascending=False)","4efcd1c9":"df.head()","1131811c":"# change gender to 0-1 binary\ndf.loc[:, \"gender\"] = df.gender - 1","d0d9aff3":"# reduce interval in cholesterol & glucose from 1-3 to 0-2\ndf.loc[:, \"cholesterol\"] = df.cholesterol - 1\ndf.loc[:, \"glucose\"] = df.glucose - 1","da44e38f":"num_cols = [\"age\", \"bp_hi\", \"bp_lo\"]\n","a8e60f12":"for col in num_cols:\n    sns.violinplot(x=\"disease\", y=col, data=df)\n    plt.show()","54ef90da":"# extreme values in bp_hi need to be corrected\nbp_cols = [\"bp_hi\", \"bp_lo\"]\nfor col in bp_cols:\n    sns.violinplot(x=\"disease\", y=col, data=df)\n    plt.show()","eb22cf6b":"# 993 samples with extreme values for bp_hi or bp_lo\nidx = df[(abs(df.bp_hi) > 300) | (abs(df.bp_lo) > 200)].index\ndf = df.drop(index=idx)","0d838720":"# drop samples with negative bp_values\nidx = df[(df.bp_hi < 0) | (df.bp_lo < 0)].index\ndf = df.drop(index=idx)","e0cd1d02":"# drop samples with bp_hi or bp_lo values less than 50; data entry error\nidx = df[(df.bp_lo < 50) | (df.bp_hi < 50)].index\ndf = df.drop(index=idx)","17b1c165":"# create column for height in ft\ndf[\"height_ft\"] = df.height \/ 30.48\n\n# drop samples with heights below 5 feet and above 7 feet\nidx = df[(df.height_ft < 4.5) | (df.height_ft > 7)].index\ndf = df.drop(index=idx)","5ddc3cc7":"# blood pressure difference column\ndf[\"bp_diff\"] = df.bp_hi - df.bp_lo\n\n# BMI column to replace height and weight\n# bmi = weight (kgs) \/ (height (m))^2\ndf[\"bmi\"] = df.weight \/ (df.height \/ 100) ** 2\n\n# added some more common measurement unit columns for better understanding\ndf[\"yrs\"] = df.age \/ 365\ndf[\"height_ft\"] = df.height \/ 30.48\ndf[\"weight_lbs\"] = df.weight * 2.205","0a767545":"# extreme values in bp_hi need to be corrected\nbp_cols = [\"bp_diff\", \"bmi\", \"height_ft\", \"weight_lbs\"]\nfor col in bp_cols:\n    sns.violinplot(x=\"disease\", y=col, data=df)\n    plt.show()","65a622c6":"# 68,621 samples after dropping errors\ndf.shape[0]","bc6be333":"feat = df[\"weight\"]\nfeat1 = df[df.disease == 1][\"weight\"]\nfeat0 = df[df.disease == 0][\"weight\"]\nfig, ax = plt.subplots()\nsns.distplot(feat1, color=\"#b51616\", label=\"Disease\")\nsns.distplot(feat0, color=\"#0bbd1a\", label=\"No Disease\")\nax.set_xlabel(\"weight\")\nax.set_title(f\"{'weight'} Distribution\")\nax.legend()\nplt.show()","8b2b0d18":"import plotly.figure_factory as ff\n\n# Group data together\nhist_data = [feat1, feat0]\n\ngroup_labels = [\"Disease\", \"No Disease\"]\n\n# Create distplot with custom bin_size\nfig = ff.create_distplot(hist_data, group_labels)\nfig.update_layout(title_text=f\"{'weight'} Distribution\", xaxis_title=\"weight\")\nfig.show()","cb7df8a5":"# feat = df[stat]\n# feat1 = df[df.disease == 1][stat].value_counts()\n# feat0 = df[df.disease == 0][stat].value_counts()\nfeat = df[\"cholesterol\"]\nfeat1 = df[df.disease == 1][\"cholesterol\"].value_counts()\nfeat0 = df[df.disease == 0][\"cholesterol\"].value_counts()\n#     d = {'no_disease':feat0, 'disease':feat1}\n#     f = pd.DataFrame(data=d)\nfig, ax = plt.subplots()\n#     st.write(f.style.background_gradient())\n# fig, ax = plt.subplots()\nwidth = 0.25\ncd = ax.bar(\n    x=feat1.index - width \/ 2,\n    height=feat1,\n    width=width,\n    color=\"#e60909\",\n    label=\"Disease\",\n)\nno_cd = ax.bar(\n    x=feat0.index + width \/ 2,\n    height=feat0,\n    width=width,\n    color=\"#09e648\",\n    label=\"No Disease\",\n)\n\n# Attach a text label above each bar in *rects*, displaying its height\nfor rect in cd:\n    height = rect.get_height()\n    ax.annotate(\n        \"{}\".format(height),\n        xy=(rect.get_x() + rect.get_width() \/ 2, height),\n        xytext=(0, 3),  # 3 points vertical offset\n        textcoords=\"offset points\",\n        ha=\"center\",\n        va=\"bottom\",\n    )\nfor rect in no_cd:\n    height = rect.get_height()\n    ax.annotate(\n        \"{}\".format(height),\n        xy=(rect.get_x() + rect.get_width() \/ 2, height),\n        xytext=(0, 3),  # 3 points vertical offset\n        textcoords=\"offset points\",\n        ha=\"center\",\n        va=\"bottom\",\n    )\n\nax.set_xlabel(\"cholesterol\")\nax.set_xticks(feat.unique())\nax.set_title(\"cholesterol\")\nax.legend()\nfig.tight_layout()\nplt.show()\n# st.pyplot(fig)","5f956a24":"import plotly.graph_objects as go\n\nstat = \"Cholesterol\"\nintervals = list(feat.unique())\n\nfig = go.Figure(\n    data=[\n        go.Bar(name=\"Disease\", x=intervals, y=list(feat1)),\n        go.Bar(name=\"No Disease\", x=intervals, y=list(feat0)),\n    ]\n)\n# Change the bar mode\nfig.update_layout(\n    barmode=\"group\", title_text=f\"{stat} Distribution\", xaxis_title=f\"{stat} Values\"\n)\nfig.show()","f93bbfcb":"sns.catplot(x=\"gender\", y=\"bp_hi\", hue=\"disease\", kind=\"violin\", split=True, data=df)\nplt.show()\nsns.catplot(x=\"gender\", y=\"bp_lo\", hue=\"disease\", kind=\"violin\", split=True, data=df)\nplt.show()\nsns.catplot(x=\"gender\", y=\"bp_diff\", hue=\"disease\", kind=\"violin\", split=True, data=df)\nplt.show()","e19fef45":"import plotly.graph_objects as go\n\n\nfig = go.Figure()\n\nfig.add_trace(\n    go.Violin(\n        x=df[\"gender\"][df[\"disease\"] == 0],  # no disease\n        y=df[\"bp_hi\"][df[\"disease\"] == 0],\n        legendgroup=\"No Disease\",\n        scalegroup=\"Yes\",\n        name=\"No Disease\",\n        side=\"negative\",\n        line_color=\"#09e648\",\n    )\n)\nfig.add_trace(\n    go.Violin(\n        x=df[\"gender\"][df[\"disease\"] == 1],\n        y=df[\"bp_hi\"][df[\"disease\"] == 1],\n        legendgroup=\"Disease\",\n        scalegroup=\"Disease\",\n        name=\"Disease\",\n        side=\"positive\",\n        line_color=\"#e60909\",\n    )\n)\nfig.update_traces(meanline_visible=True)\nfig.update_layout(violingap=0, violinmode=\"overlay\")\nfig.show()","f755b8c3":"df.head()","a73ae737":"drop_cols = [\n    \"disease\",\n    \"yrs\",\n    \"height_ft\",\n    \"bp_diff\",\n    \"weight_lbs\",\n    #     \"bmi\",\n    #     \"height\",\n    \"weight\",\n]\n\nX = df.drop(columns=drop_cols)\ny = df.disease\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.5, random_state=28, stratify=df.disease\n)","2b7b93d0":"# checking the variance inflation factor to identify any redundancy in the feature vars\n# weight and bmi understandably show high vif which is why weight was dropped from the feature vars\nprint_vif(X_train)","061668c1":"# categorical columns to be encoded\ncat_cols = [\"cholesterol\", \"glucose\"]\ndrop_cat = [0, 0]\n# data preprocessing\npreprocessing = ColumnTransformer(\n    [\n        #         (\"encode_cats\", OneHotEncoder(drop=drop_cat), cat_cols),\n        #         (\"encode_cats\", LeaveOneOutEncoder(), cat_cols),\n    ],\n    remainder=\"passthrough\",\n)","1477a136":"pipeline = Pipeline(\n    [\n        (\"processing\", preprocessing),\n        (\"model\", XGBClassifier(use_label_encoder=False)),\n    ]\n)\n\npipeline.fit(X_train, y_train)\n\ntrain_score = pipeline.score(X_train, y_train)\ntest_score = pipeline.score(X_test, y_test)\n\nprint(f\"\\ntrain score: {train_score}\")\nprint(f\"test score: {test_score}\")","0a096fba":"# grid search values other than optimal hyperparameters removed to lower notebook run time\n# fmt: off\ngrid = {\n    \"model__n_estimators\": np.arange(1, 3),\n    \"model__learning_rate\": np.arange(0, 50, 10),\n    #     \"model__subsample\": [],\n    \"model__colsample_bytree\": np.arange(0.7,1,0.1),\n    \"model__max_depth\": np.arange(4,7),\n}\n# fmt: on\npipeline_cv = GridSearchCV(pipeline, grid, cv=2, verbose=2, n_jobs=-1)\npipeline_cv.fit(X_train, y_train)\n\nbest_params = pipeline_cv.best_params_\nbest_params","acfd9a2d":"train_score = pipeline_cv.score(X_train, y_train)\ntest_score = pipeline_cv.score(X_test, y_test)\n\nprint(f\"train_score {train_score}\")\nprint(f\"test_score {test_score}\")","82f6c7ac":"feature_importances = pipeline_cv.best_estimator_[\"model\"].feature_importances_\nfeature_importances = pd.DataFrame(\n    {\"feature\": X_train.columns, \"importance\": feature_importances}\n).sort_values(\"importance\", ascending=False)\nfeature_importances","199685cb":"y_preds = pipeline_cv.predict(X_test)\npreds_df, fig = predictions_df(X_test, y_test, y_preds)\n\n# confusion matrix\ncm = confusion_matrix(y_test, y_preds)\ndisplay(cm)\n\n# classification report\nclass_report = classification_report(y_test, y_preds)\nprint(class_report)\n\n# prediction probabilities\npred_prob = pipeline_cv.predict_proba(X_test)\n# add prediction probs to preds_df\npreds_df[\"pred_prob\"] = pred_prob[:, 1]\n\npreds_df = preds_df.drop(columns=[\"residuals\", \"abs_residuals\"])\n# preds_df.head()","0a60667d":"prob_true, prob_pred = calibration_curve(y_test, pred_prob[:, 1], n_bins=10)\nplt.plot(prob_pred, prob_true, \"-o\")\nplt.show()","cfc9b592":"# # changing the prediction percentage threshold to 45%\n# adj_preds.loc[adj_preds.pred_prob > 0.49, \"y_preds\"] = 1\n\n# # classification report with new threshold\n# print(classification_report(adj_preds.y_true, adj_preds.y_preds))","8d7b6e4e":"# dataframe for false negatives sorted by prediction probability descending\nf_negs = preds_df[(preds_df.y_true == 1) & (preds_df.y_preds == 0)].sort_values(\n    \"pred_prob\", ascending=False\n)\nf_negs","c76b4220":"f_negs.mean()","770235da":"preds_df.pred_prob.hist()","ae07f8b4":"# drop columns for testing sets\ndrop_cols = [\n    \"disease\",\n    \"yrs\",\n    \"height_ft\",\n    \"bp_diff\",\n    \"weight_lbs\",\n    #     \"smoke\",\n    #     \"active\",\n    #     \"alcohol\",\n    #     \"bmi\",\n    #     \"height\",\n    \"weight\",\n]\n\n# train test split of data\nX = df.drop(columns=drop_cols)\ny = df.disease\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=28, stratify=df[\"gender\"]\n)","1f82a510":"print_vif(X_train)","95f579e8":"# categorical columns to be encoded\ncat_cols = [\"cholesterol\", \"glucose\"]\n\nnum_cols = [\n    \"age\",\n    \"height\",\n    #     \"weight\",\n    \"bp_hi\",\n    \"bp_lo\",\n]\ndrop_cat = [0, 0]\npreprocessing = ColumnTransformer(\n    [\n        #         (\"encode_cats\", OneHotEncoder(drop=drop_cat), cat_cols),\n        (\"encode_cats\", LeaveOneOutEncoder(), cat_cols),\n        (\"scaler\", StandardScaler(), num_cols),\n        #         (\"scaler\", MinMaxScaler(), num_cols),\n    ],\n    remainder=\"passthrough\",\n)","30b4df21":"# fmt: off\nlr_pipeline = Pipeline(\n    [\n        (\"processing\", preprocessing),\n        (\"model\", LogisticRegression(solver=\"lbfgs\", penalty=\"none\", max_iter=1000, random_state=28))\n    ]\n)\n\nlr_pipeline.fit(X_train, y_train)\n\n# best_params = pipeline.best_params_\nlr_train_score = lr_pipeline.score(X_train, y_train)\nlr_test_score = lr_pipeline.score(X_test, y_test)\n\nprint(f\"train score: {lr_train_score}\")\nprint(f\"test score: {lr_test_score}\")","4a1b3084":"# fmt: off\nlr_grid = {\n    \"model__solver\": ['lbfgs'],\n    \"model__penalty\": [\"l2\",\"none\"],\n    \"model__C\": [0.75],\n}\n# fmt: on\nlr_pipeline_cv = GridSearchCV(lr_pipeline, lr_grid, cv=5, verbose=1, n_jobs=2)\nlr_pipeline_cv.fit(X_train, y_train)\n\nlr_best_params = lr_pipeline_cv.best_params_\nlr_best_params","fc4c68b1":"lr_train_score = lr_pipeline_cv.score(X_train, y_train)\nlr_test_score = lr_pipeline_cv.score(X_test, y_test)\n\nprint(f\"train_score {lr_train_score}\")\nprint(f\"test_score {lr_test_score}\")","9a867417":"lr_pred_prob = lr_pipeline_cv.predict_proba(X_test)\n# pred_prob\n\nlr_prob_true, lr_prob_pred = calibration_curve(y_test, lr_pred_prob[:, 1], n_bins=10)\nplt.plot(lr_prob_pred, lr_prob_true, \"-o\")\nplt.show()","e7870df5":"import plotly.graph_objects as go","34abafe7":"# Create traces\nfig = go.Figure()\nfig.add_trace(\n    go.Scatter(x=lr_prob_pred, y=lr_prob_true, mode=\"lines\", name=\"Calibration Curve\")\n)\n\nfig.show()","deb82906":"# prediction percentages\nlr_preds = lr_pipeline_cv.predict(X_test)\n\n# df created from predictions\nlr_preds_df, _ = predictions_df(X_test, y_test, lr_preds)\n\n# add prediction probs to preds_df\nlr_preds_df[\"pred_prob\"] = lr_pred_prob[:, 1]\n\n# classification target, residuals not needed\nlr_preds_df = lr_preds_df.drop(columns=[\"residuals\", \"abs_residuals\"])\n\n# confusion matrix\nlr_cm = confusion_matrix(y_test, lr_preds)\ndisplay(lr_cm)\n\n# classification report\nprint(classification_report(y_test, lr_preds))","c5bb4efe":"# dataframe for false negatives sorted by prediction probability descending\nlr_f_negs = lr_preds_df[\n    (lr_preds_df.y_true == 1) & (lr_preds_df.y_preds == 0)\n].sort_values(\"pred_prob\", ascending=False)","f1c86f99":"sns.distplot(lr_preds_df.pred_prob)\nplt.show()","59e4b0f9":"X_train.mean()","3e82a01a":"lr_preds_df[lr_preds_df.y_true == 1].mean()","01fa5475":"preds_df[preds_df.y_true == 1].mean()","96973cc8":"lr_preds_df.head()","3d1167c0":"# Data Cleaning","1a2c1f65":"* Categorical or continuous\n* 10,000+ samples\n* 20+ features","ebc06a17":"## BP value errors","bdf3da33":"---\n---\n# Logistic Regression Model","c937114c":"---\n## LR error analysis","3ff33f1f":"XGBoost","77afdc73":"---\n## XGB error analysis","3bd5781a":"---\n## XGB predictions","8b9e81e0":"> predicitve value of XGBoost model is limited","2b54f122":"> The overall classification ability of the logistic regression model isn't quite as good as the XGBoost model, however the predictive range of the logistic regression model is more versatile and how better predictive validity","3dae36ef":"# Imports","8e2d0f49":"---\n## LR predictions","9e4af956":"---\n## LR hyperparameter tuning","7cdd9a99":"> Categorical columns encoded; decision tree class models don't require numerical scaling","56b314cd":"---\n## XGB feature importance","748491ea":"> False negatives","87d4fc18":"> prediction probability distribution","4868b010":"# Exploration","6bb736a1":"---\n# Features","6727cb8b":"age = days (int)","eb7e71a4":"## Height value errors","9f7ee8f9":"# Gradient Boosting Model","36773cc9":"---\n## XGB hyperparameter tuning"}}