{"cell_type":{"72e845ef":"code","62e67cff":"code","ac114c76":"code","a6de667a":"code","3297f2b6":"code","aafcd951":"code","f4de49ac":"code","df09300a":"code","7b98d2d5":"code","446b734a":"code","0716e7dc":"code","0334f083":"code","5310844b":"code","40f51538":"code","281db8d8":"code","ae0a0116":"code","9fe1af2b":"code","c166794b":"code","59c0a6b9":"code","3369f744":"code","5a692761":"code","eccb72f5":"code","956e02f3":"code","a83bf696":"code","0a44f5ed":"code","27d357e8":"code","8035f508":"code","e54a3e61":"code","b45ef60a":"code","e1d8be49":"code","0783dd45":"code","8d4e99b8":"code","93147aff":"code","aff1f4ba":"markdown","427f1ff6":"markdown","15abee0b":"markdown","f3a2d0fe":"markdown","85223816":"markdown","ea6e00da":"markdown"},"source":{"72e845ef":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport glob\nimport json\n\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')","62e67cff":"!ls \/kaggle\/input\/CORD-19-research-challenge\/","ac114c76":"root_path = '\/kaggle\/input\/CORD-19-research-challenge\/'\nmetadata_path = f'{root_path}\/metadata.csv'\nmeta_df = pd.read_csv(metadata_path, dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str\n})\nmeta_df.head()","a6de667a":"meta_df.info()","3297f2b6":"# load the nltk\n\nfrom nltk.tokenize.punkt import PunktSentenceTokenizer\n\ndata = \"All work and no play makes jack dull boy. All work and no play makes jack a dull boy.\"\nsentTokenizer = PunktSentenceTokenizer()\nprint(sentTokenizer.span_tokenize(data))","aafcd951":"# helper functions\n\ndef mergeConsecutiveSpans(spans):\n    newSpans = []\n    for ((start, _), (_, end)) in zip(spans, spans[1:]):\n        newSpans.append((start, end))\n    return newSpans\n\nmergeConsecutiveSpans([(1,2), (3, 5), (4, 7)])","f4de49ac":"def findSentence(text, sentence_spans, ref):\n    if 'text' in ref:\n        ref_info = ref['text']\n    elif 'mention' in ref:\n        ref_info = ref['mention']\n    else:\n        ref_info = f\"Unknown info from {ref}\"\n    start = ref['start']\n    end = ref['end']\n    sentence = \"\"\n    for (sent_start, sent_end) in sentence_spans:\n        if sent_start <= start and sent_end >= end:\n            sentence = text[sent_start:sent_end]\n            break\n    if len(sentence) == 0:\n        for (sent_start, sent_end) in mergeConsecutiveSpans(sentence_spans):\n            if sent_start <= start and sent_end >= end:\n                sentence = text[sent_start:sent_end]\n                break\n        \n    if len(sentence) == 0:\n        sentence = f\"Couldn't find ({start}, {end}) in sentences:\\n\"\n        for (sent_start, sent_end) in sentence_spans:\n            sentence += f\"\\t({sent_start}, {sent_end}): {text[sent_start:sent_end]}\\n\"\n    return sentence\n    \n\nwith open('\/kaggle\/input\/CORD-19-research-challenge\/custom_license\/custom_license\/pdf_json\/42a44518a00b962207226bf61b4d71a0f596e2a1.json') as file:\n    sentTokenizer = PunktSentenceTokenizer()\n    finfo = json.load(file)\n    body_text = finfo['body_text']\n    #print(body_text)\n    for idx, item in enumerate(body_text):\n        text = item['text']\n        sentence_spans = sentTokenizer.span_tokenize(text)\n        #print(f\"sentence spans = {sentence_spans}\")\n        for ref in item['ref_spans']:\n            sentence = findSentence(text, sentence_spans, ref)\n            print(f\"In {idx}th reference text: {sentence} ref_id: {ref['ref_id']}\")\n        for cite in item['cite_spans']:\n            sentence = findSentence(text, sentence_spans, cite)\n            print(f\"In {idx}th citation: {sentence} cite_id: {cite['ref_id']}\")","df09300a":"def getCitationPoints(sentTokenizer, finfo):\n    body_text = finfo['body_text']\n    #print(body_text)\n    citations = dict()\n    for idx, item in enumerate(body_text):\n        text = item['text']\n        sentence_spans = sentTokenizer.span_tokenize(text)\n        #print(f\"sentence spans = {sentence_spans}\")\n        for cite in item['cite_spans']:\n            ref_id = cite['ref_id']\n            if ref_id is None:\n                continue\n            sentence = findSentence(text, sentence_spans, cite)\n            #print(f\"In {idx}th citation: {sentence} cite_id: {cite['ref_id']}\")\n            sentences = citations.get(cite['ref_id'], list())\n            sentences.append(sentence)\n            citations[cite['ref_id']] = sentences\n    return citations\n                \nwith open('\/kaggle\/input\/CORD-19-research-challenge\/custom_license\/custom_license\/pdf_json\/42a44518a00b962207226bf61b4d71a0f596e2a1.json') as file:\n    sentTokenizer = PunktSentenceTokenizer()\n    finfo = json.load(file)\n    print(len(finfo['bib_entries']))\n    print(getCitationPoints(sentTokenizer, finfo))","7b98d2d5":"from tqdm.notebook import tqdm","446b734a":"meta_title = meta_df.set_index('title')\n\n# add new column\nmeta_title['json_present'] = False\n\n# drop na\nmeta_title = meta_title.loc[meta_title.index.dropna()]\n\n# remove duplicates\nmeta_title = meta_title[~meta_title.index.duplicated(keep='first')]\nmeta_title.index.is_unique\n\n# sort index\nmeta_title = meta_title.sort_index()","0716e7dc":"def checkBibs(fname, meta_title, titles_from_json, bibs, paper_sentences, duplicate_jsons):\n    with open(fname) as file:\n        sentTokenizer = PunktSentenceTokenizer()\n        finfo = json.load(file)\n        title = finfo['metadata']['title']\n        if title not in meta_title.index:\n            #print(f\"{title} not found in {finfo['metadata']}, paper_id = {finfo['paper_id']}\")\n            return\n        meta_title.loc[title, \"json_present\"] = True\n        bib_entries = finfo.get('bib_entries', None)\n        cit_points = getCitationPoints(sentTokenizer, finfo)\n        # some papers dont have citation points, so for now we are going to not use those\n        #if (bib_entries is not None and len(bib_entries) > 0 ) and len(cit_points) == 0:\n        #    print(f\"no citation_points found for {fname}\")\n        if bib_entries is None and len(cit_points) > 0:\n            print(f\"no bib_entries found for {fname}\")\n            return\n        if title in titles_from_json:\n            #print(f\"{title} already exists!\")\n            duplicate_jsons.add(title)\n        titles_from_json.add(title)\n        bibs[title] = bib_entries\n        paper_sentences[title] = cit_points\n        \nbibs = dict()\ntitles_from_json = set()\npaper_sentences = dict()\nduplicate_jsons = set()\ncheckBibs('\/kaggle\/input\/CORD-19-research-challenge\/custom_license\/custom_license\/pdf_json\/42a44518a00b962207226bf61b4d71a0f596e2a1.json', \n          meta_title, titles_from_json, bibs, paper_sentences, duplicate_jsons)\nprint(titles_from_json)\nprint(bibs)\nprint(duplicate_jsons)","0334f083":"all_json = glob.glob(f'{root_path}\/**\/*.json', recursive=True)\nlen(all_json)","5310844b":"# test with a small number first\nfor fname in tqdm(all_json[:4]):\n    checkBibs(fname, meta_title, titles_from_json, bibs, paper_sentences, duplicate_jsons)\nprint(titles_from_json)\nprint(bibs)\nprint(duplicate_jsons)","40f51538":"# now for all the files\nbibs = dict()\ntitles_from_json = set()\npaper_sentences = dict()\nduplicate_jsons = set()\nfor fname in tqdm(all_json):\n    checkBibs(fname, meta_title, titles_from_json, bibs, paper_sentences, duplicate_jsons)\nprint(f\"# of unique titles: {len(titles_from_json)}\")\nprint(f\"# of unique bib entries {len(bibs)}\")\nprint(f\"# of duplicate entries {len(duplicate_jsons)}\")","281db8d8":"# filter out the metadata for files not present\nmeta_title = meta_title[meta_title['json_present'] == True]","ae0a0116":"import itertools","9fe1af2b":"ref_graph = list()\nfor title in tqdm(paper_sentences):\n    refs_to_sentences = paper_sentences[title]\n    bibs_to_titles = bibs[title]\n    for ref in refs_to_sentences:\n        sentences = refs_to_sentences[ref]\n        if ref in bibs_to_titles:\n            totitle = bibs_to_titles[ref]['title']\n            #import pdb; pdb.set_trace()\n            if totitle in meta_title.index:\n                for s in sentences:\n                    ref_graph.append([title, totitle, s])\n    #break\nref_graph = pd.DataFrame(ref_graph, columns=['fromtitle', 'totitle', 'ref_sentence'])\nref_graph","c166794b":"len(ref_graph['totitle'].unique())","59c0a6b9":"len(ref_graph['fromtitle'].unique())","3369f744":"grouped_ref_sentences = ref_graph.pivot_table(values='ref_sentence', columns=['totitle'], aggfunc='. '.join).transpose()\ngrouped_ref_sentences","5a692761":"ref_graph.memory_usage()","eccb72f5":"paper_sentences[list(paper_sentences.keys())[1]]","956e02f3":"!pip install scispacy\n!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_core_sci_lg-0.2.4.tar.gz","a83bf696":"import spacy\nimport scispacy\nfrom tqdm import tqdm\n\nnlp = spacy.load(\"en_core_sci_lg\")","0a44f5ed":"# sub questions from the 2 tasks. \n\nquestions = [\n    # What has been published about ethical and social science considerations?\n    \"Efforts to articulate and translate existing ethical principles and standards to salient issues in COVID-2019\",\n    \"Efforts to embed ethics across all thematic areas, engage with novel ethical issues that arise and coordinate to minimize duplication of oversight\",\n    \"Efforts to support sustained education, access, and capacity building in the area of ethics\",\n    \"Efforts to establish a team at WHO that will be integrated within multidisciplinary research and operational platforms and that will connect with existing and expanded global networks of social sciences.\",\n    \"Efforts to develop qualitative assessment frameworks to systematically collect information related to local barriers and enablers for the uptake and adherence to public health measures for prevention and control. This includes the rapid identification of the secondary impacts of these measures. (e.g. use of surgical masks, modification of health seeking behaviors for SRH, school closures)\",\n    \"Efforts to identify how the burden of responding to the outbreak and implementing public health measures affects the physical and psychological health of those providing care for Covid-19 patients and identify the immediate needs that must be addressed.\",\n    \"Efforts to identify the underlying drivers of fear, anxiety and stigma that fuel misinformation and rumor, particularly through social media.\",\n    # What has been published about information sharing and inter-sectoral collaboration?\n    \"Methods for coordinating data-gathering with standardized nomenclature.\"\n    \"Sharing response information among planners, providers, and others.\",\n    \"Understanding and mitigating barriers to information-sharing.\",\n    \"How to recruit, support, and coordinate local (non-Federal) expertise and capacity relevant to public health emergency response (public, private, commercial and non-profit, including academic).\",\n    \"Integration of federal\/state\/local public health surveillance systems.\",\n    \"Value of investments in baseline public health response infrastructure preparedness\",\n    \"Modes of communicating with target high-risk populations (elderly, health care workers).\",\n    \"Risk communication and guidelines that are easy to understand and follow (include targeting at risk populations\u2019 families too).\",\n    \"Communication that indicates potential risk of disease to all population groups.\",\n    \"Misunderstanding around containment and mitigation.\",\n    \"Action plan to mitigate gaps and problems of inequity in the Nation\u2019s public health capability, capacity, and funding to ensure all citizens in need are supported and can access information, surveillance, and treatment.\",\n    \"Measures to reach marginalized and disadvantaged populations.\",\n    \"Data systems and research priorities and agendas incorporate attention to the needs and circumstances of disadvantaged populations and underrepresented minorities.\",\n    \"Mitigating threats to incarcerated people from COVID-19, assuring access to information, prevention, diagnosis, and treatment.\",\n    \"Understanding coverage policies (barriers and opportunities) related to testing, treatment, and care\",\n]","27d357e8":"qvectors = list()\nfor q in questions:\n    qvectors.append(nlp(q).vector)","8035f508":"qvectors = np.array(qvectors)\nqvectors.shape","e54a3e61":"def cosine_similarity(u, v):\n    \"\"\"\n    Arguments:\n        u -- a word vector of shape (m,n)          \n        v -- a word vector of shape (n,)\n\n    Returns:\n        cosine_similarity -- the cosine similarity between u and v defined by the formula above.\n    \"\"\"\n    \n    distance = 0.0\n\n    #import pdb; pdb.set_trace()\n    # Compute the dot product between u and v (\u22481 line)\n    dot = np.dot(u, v.T)\n    # Compute the L2 norm of u (\u22481 line)\n    norm_u = np.sqrt(np.sum(u * u, axis=1))\n    \n    # Compute the L2 norm of v (\u22481 line)\n    norm_v = np.sqrt(np.sum(v * v))\n    # Compute the cosine similarity defined by formula (1) (\u22481 line)\n    cosine_similarity = dot \/ (norm_u * norm_v)\n    \n    return cosine_similarity","b45ef60a":"from tqdm.notebook import tqdm","e1d8be49":"question_answers = None\nfor idx in tqdm(grouped_ref_sentences.index):\n    text = grouped_ref_sentences.loc[idx, 'ref_sentence']\n    text = text[:1000000]\n    textvector = nlp(text).vector\n    similarities = cosine_similarity(qvectors, textvector).reshape(1, -1)\n    #import pdb; pdb.set_trace()\n    if question_answers is None:\n        question_answers = similarities\n    else:\n        question_answers = np.append(question_answers, similarities, axis=0)\nquestion_answers.shape","0783dd45":"question_matches = pd.DataFrame(question_answers, columns=[\"q\" + str(i) for i, _ in enumerate(questions)])","8d4e99b8":"# find top 5 matches for each question\nquestion_matches.idxmax()","93147aff":"top5 = pd.DataFrame(dict([(q, grouped_ref_sentences.iloc[question_matches.sort_values('q' + str(i), \n                                                                             ascending=False).head(5).index].index.values) for i, q in enumerate(questions)])).T\ntop5","aff1f4ba":"# Questions","427f1ff6":"Lets load the metadata","15abee0b":"# Goal\n\nIn this notebook I try to answer the last 2 task questions:\n\n> What has been published about ethical and social science considerations?\n> What has been published about information sharing and inter-sectoral collaboration?\n\nTo this end my idea is using the sentences which refer to other papers (topaper) to charecterize the topaper. Then use cosine similarity of the scispacy vectors to find the closest answer","f3a2d0fe":"Find all the json files","85223816":"# Create knowledge graph\n* 1. ~~Find to papers~~ Done\n> 1. ~~Create title shas and set as new column in meta_df~~ Using sorted title as index\n> 2. ~~loop through each paper json\/bib_entries and create dictionary of `fromtitlesha -> [totitlesha]`~~ Done\n* 2. ~~Find sentences in from paper where to papers are referenced.~~ Done\n> 1. Loop through each paper json\/body_text and tokenize into sentences\n> 2. ~~For each sentence if match reference regex like `[number+]` then create map `titlesha -> [sentence -> [totitlesha]]`~~\n>> 1. ~~Instead using text[cite_spans]~~\n>> 1. ~~Check every json: if they have a bib_entries citation section then it should have found the rsentences with citations~~\n>>    Some don't have citations in text even though there are citation sections. Plan on not using papers with no citation sentences.\n* 3. ~~Create graph as list of tuples of (from paperid, to paperid, sentence)~~\n> 1. ~~Use 1.2 and 2.2 to get above tuples~~\n> 2. ~~Filter for only totitleid nodes in meta_df from 1.1~~\n* 4. Use sentence vector to xform to (fromid, toid, sentence_vector)\n> 1. Use 3.2 and scispacy or [transformer](https:\/\/github.com\/UKPLab\/sentence-transformers) or other things to get sentence_vector\n* 5. Use cosine similarity to find the closest paper","ea6e00da":"# Top 5 matches for each question"}}