{"cell_type":{"94b4354a":"code","da39615f":"code","afb8e8ed":"code","2a002aa1":"code","e4ceacfc":"code","31e754be":"code","d66a3c84":"code","006faaff":"code","2f422ea5":"code","77dfcb7f":"code","0998733b":"code","3e07359c":"code","65f811a3":"code","2ce78bf6":"code","bc973207":"code","ed543f93":"code","8eb2f7df":"code","1e5e0985":"code","41a6827d":"code","ef6139f5":"code","88f6821c":"code","1c452878":"code","402cec1a":"code","bd6a5866":"code","ef263e09":"code","f0ee726b":"code","83efef7a":"code","438d54b3":"code","da9ae8bc":"code","b190530a":"code","9978397d":"code","1e1ee6ee":"code","389273bf":"code","7ba84775":"code","c882a830":"code","133a46e8":"code","a20bccfd":"code","bae9f95c":"code","4d29978a":"code","0cc66430":"code","b60e73b7":"code","b7d1c25a":"code","b912686a":"code","ba6f12a7":"code","fb8f47e5":"code","42be54d2":"code","2e42317a":"code","97f09417":"code","68113101":"code","729082c8":"code","241a5d97":"code","03d9d180":"code","9960573f":"code","2eb4ad58":"code","7b5fe72e":"code","2d9c1691":"code","33e9693d":"code","747df076":"code","5351bef3":"code","4cec3fba":"code","47fe1956":"code","aab44dfe":"code","8e125970":"code","0a7211cf":"code","11aa408c":"code","66fa5624":"code","2c852fb5":"code","74104d01":"code","2a14f96a":"code","387a0948":"code","1b5b2d50":"code","03ba7d03":"code","46a7bf6c":"code","a034d7a3":"code","e888498e":"code","d90b01e9":"code","2c0d39dd":"code","54a05201":"code","126dcd8c":"code","1e7a87e9":"code","ce1ad7ef":"code","e8c69c5b":"code","4ebd934f":"code","d7c79492":"markdown","5a08bdb5":"markdown","5b440bac":"markdown","67974985":"markdown","448cad01":"markdown","221184b0":"markdown","13139230":"markdown","79740acb":"markdown","87a82416":"markdown","b8149a43":"markdown","bf23cada":"markdown","a8a7ca00":"markdown","a64f5d1c":"markdown","f483d9e9":"markdown","1ba19813":"markdown","9d673dc9":"markdown","542d1f44":"markdown","a264a9db":"markdown","5429695c":"markdown","467b3b45":"markdown","23efb1f7":"markdown","58f85dc6":"markdown","64556034":"markdown","5bc3ae50":"markdown","b1489002":"markdown","8478bb11":"markdown","f6f99fd7":"markdown","2c20f3bf":"markdown","0bb81422":"markdown","32cdff77":"markdown","e915039d":"markdown","01e19186":"markdown","29c51507":"markdown","410446fe":"markdown","853dfed5":"markdown","f9ea7e4e":"markdown","70fd6de3":"markdown","c2e4f5c3":"markdown","2c5c462e":"markdown","468597df":"markdown","38ff3a67":"markdown","160c2931":"markdown","072a7440":"markdown","c3b0b645":"markdown","1f7b3391":"markdown","e0c13a45":"markdown","a367280e":"markdown","6b94c71c":"markdown","2d0d0c7d":"markdown","307db64d":"markdown","9fa06b80":"markdown","48d8c5ca":"markdown","966f1c29":"markdown"},"source":{"94b4354a":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","da39615f":"#Data_train\ndf_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\n#Data_test\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\n#Data (Train + Test)\ndata = pd.concat((df_train, df_test)).reset_index(drop=True)\ndata.drop(['SalePrice'], axis=1, inplace=True)\n\ndf_train.head()","afb8e8ed":"df_train.shape, df_test.shape","2a002aa1":"df_train.describe()","e4ceacfc":"df_train.keys()","31e754be":"#Different types of the features\ndf_train.dtypes","d66a3c84":"#histogram\ndf_train['SalePrice'].hist(bins = 40)","006faaff":"#skewness & kurtosis\nprint(\"Skewness: %f\" % df_train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % df_train['SalePrice'].kurt())","2f422ea5":"#correlation matrix\ncorrmat = df_train.corr()\n\n#Plot a heatmap to visualize the correlations\nf, ax = plt.subplots(figsize=(30, 19))\nsns.set(font_scale=1.45)\nsns.heatmap(corrmat, square=True,cmap='coolwarm');","77dfcb7f":"correlations = corrmat[\"SalePrice\"].sort_values(ascending=False)\nfeatures = correlations.index[0:10]\nfeatures","0998733b":"sns.pairplot(df_train[features], size = 2.5)\nplt.show();","3e07359c":"df_train.drop(['Id'], axis=1, inplace=True)\ndf_test.drop(['Id'], axis=1, inplace=True)\ndata.drop(['Id'], axis=1, inplace=True)","65f811a3":"training_null = pd.isnull(df_train).sum()\ntesting_null = pd.isnull(df_test).sum()\n\nnull = pd.concat([training_null, testing_null], axis=1, keys=[\"Training\", \"Testing\"])","2ce78bf6":"null","bc973207":"#Based on the description data file provided, all the variables who have meaningfull Nan\n\nnull_with_meaning = [\"Alley\", \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\", \"FireplaceQu\", \"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\", \"PoolQC\", \"Fence\", \"MiscFeature\"]","ed543f93":"#Replacing every Nan value with \"None\"\n\nfor i in null_with_meaning:\n    df_train[i].fillna(\"None\", inplace=True)\n    df_test[i].fillna(\"None\", inplace=True)\n    data[i].fillna(\"None\", inplace=True)","8eb2f7df":"null_many = null[null.sum(axis=1) > 200]  #a lot of missing values\nnull_few = null[(null.sum(axis=1) > 0) & (null.sum(axis=1) < 200)]  #few missing values\nnull_many","1e5e0985":"df_train.drop(\"LotFrontage\", axis=1, inplace=True)\ndf_test.drop(\"LotFrontage\", axis=1, inplace=True)\ndata.drop(\"LotFrontage\", axis=1, inplace=True)","41a6827d":"null_few","ef6139f5":"#I chose to use the mean function for replacement\ndf_train[\"GarageYrBlt\"].fillna(df_train[\"GarageYrBlt\"].mean(), inplace=True)\ndf_test[\"GarageYrBlt\"].fillna(df_test[\"GarageYrBlt\"].mean(), inplace=True)\ndata[\"GarageYrBlt\"].fillna(data[\"GarageYrBlt\"].mean(), inplace=True)\ndf_train[\"MasVnrArea\"].fillna(df_train[\"MasVnrArea\"].mean(), inplace=True)\ndf_test[\"MasVnrArea\"].fillna(df_test[\"MasVnrArea\"].mean(), inplace=True)\ndata[\"MasVnrArea\"].fillna(data[\"MasVnrArea\"].mean(), inplace=True)\n\ndf_train[\"MasVnrType\"].fillna(\"None\", inplace=True)\ndf_test[\"MasVnrType\"].fillna(\"None\", inplace=True)\ndata[\"MasVnrType\"].fillna(\"None\", inplace=True)","88f6821c":"types_train = df_train.dtypes #type of each feature in data: int, float, object\nnum_train = types_train[(types_train == int) | (types_train == float)] #numerical values are either type int or float\ncat_train = types_train[types_train == object] #categorical values are type object\n\n#we do the same for the test set\ntypes_test = df_test.dtypes\nnum_test = types_test[(types_test == int) | (types_test == float)]\ncat_test = types_test[types_test == object]","1c452878":"#we should convert num_train and num_test to a list to make it easier to work with\nnumerical_values_train = list(num_train.index)\nnumerical_values_test = list(num_test.index)\nfill_num = numerical_values_train+numerical_values_test\n\nprint(fill_num)","402cec1a":"for i in fill_num:\n    df_train[i].fillna(df_train[i].mean(), inplace=True)","bd6a5866":"fill_num.remove('SalePrice')","ef263e09":"print(fill_num)","f0ee726b":"for i in fill_num:\n    df_test[i].fillna(df_test[i].mean(), inplace=True)\n    data[i].fillna(data[i].mean(), inplace=True)","83efef7a":"df_train.shape, df_test.shape","438d54b3":"categorical_values_train = list(cat_train.index)\ncategorical_values_test = list(cat_test.index)","da9ae8bc":"fill_cat = []\n\nfor i in categorical_values_train:\n    if i in list(null_few.index):\n        fill_cat.append(i)\nprint(fill_cat)","b190530a":"def most_common_term(lst):\n    lst = list(lst)\n    return max(set(lst), key=lst.count)\n#most_common_term finds the most common term in a series\n\nmost_common = []\n\nfor i in fill_cat:\n    most_common.append(most_common_term(data[i]))\n    \nmost_common","9978397d":"most_common_dictionary = {fill_cat[0]: [most_common[0]], fill_cat[1]: [most_common[1]], fill_cat[2]: [most_common[2]], fill_cat[3]: [most_common[3]],\n                          fill_cat[4]: [most_common[4]], fill_cat[5]: [most_common[5]], fill_cat[6]: [most_common[6]], fill_cat[7]: [most_common[7]],\n                          fill_cat[8]: [most_common[8]]}\nmost_common_dictionary","1e1ee6ee":"k = 0\nfor i in fill_cat:  \n    df_train[i].fillna(most_common[k], inplace=True)\n    df_test[i].fillna(most_common[k], inplace=True)\n    data[i].fillna(most_common[k], inplace=True)\n    k += 1","389273bf":"training_null = pd.isnull(df_train).sum()\ntesting_null = pd.isnull(df_test).sum()\n\nnull = pd.concat([training_null, testing_null], axis=1, keys=[\"Training\", \"Testing\"])\nnull[null.sum(axis=1) > 0]","7ba84775":"(np.log(df_train[\"SalePrice\"])).hist(bins = 40)","c882a830":"df_train[\"LogPrice\"] = np.log(df_train[\"SalePrice\"])\ndf_train.head()","133a46e8":"df_train_add = df_train.copy()\n\ndf_train_add['TotalSF']=df_train_add['TotalBsmtSF'] + df_train_add['1stFlrSF'] + df_train_add['2ndFlrSF']\n\ndf_train_add['Total_Bathrooms'] = (df_train_add['FullBath'] + (0.5 * df_train_add['HalfBath']) +\n                               df_train_add['BsmtFullBath'] + (0.5 * df_train_add['BsmtHalfBath']))\n\ndf_train_add['Total_porch_sf'] = (df_train_add['OpenPorchSF'] + df_train_add['3SsnPorch'] +\n                              df_train_add['EnclosedPorch'] + df_train_add['ScreenPorch'] +\n                              df_train_add['WoodDeckSF'])\n\ndf_test_add = df_test.copy()\n\ndf_test_add['TotalSF']=df_test_add['TotalBsmtSF'] + df_test_add['1stFlrSF'] + df_test_add['2ndFlrSF']\n\ndf_test_add['Total_Bathrooms'] = (df_test_add['FullBath'] + (0.5 * df_test_add['HalfBath']) +\n                               df_test_add['BsmtFullBath'] + (0.5 * df_test_add['BsmtHalfBath']))\n\ndf_test_add['Total_porch_sf'] = (df_test_add['OpenPorchSF'] + df_test_add['3SsnPorch'] +\n                              df_test_add['EnclosedPorch'] + df_test_add['ScreenPorch'] +\n                              df_test_add['WoodDeckSF'])","a20bccfd":"## For ex, if PoolArea = 0 , Then HasPool = 0 too\n\ndf_train_add['haspool'] = df_train_add['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ndf_train_add['has2ndfloor'] = df_train_add['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ndf_train_add['hasgarage'] = df_train_add['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\ndf_train_add['hasbsmt'] = df_train_add['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\ndf_train_add['hasfireplace'] = df_train_add['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\ndf_test_add['haspool'] = df_test_add['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ndf_test_add['has2ndfloor'] = df_test_add['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ndf_test_add['hasgarage'] = df_test_add['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\ndf_test_add['hasbsmt'] = df_test_add['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\ndf_test_add['hasfireplace'] = df_test_add['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","bae9f95c":"df_train[df_train[\"SalePrice\"] > 600000 ] #Discovering the outliers","4d29978a":"categorical_values_train = list(cat_train.index)\ncategorical_values_test = list(cat_test.index)\nprint(categorical_values_train)","0cc66430":"df_train_add = df_train.copy()\ndf_test_add = df_test.copy()\nfrom sklearn.preprocessing import LabelEncoder\n\nlb_make = LabelEncoder()\nfor i in categorical_values_train:\n    df_train_add[i] = lb_make.fit_transform(df_train[i])\n    \nfor i in categorical_values_test:\n    df_test_add[i] = lb_make.fit_transform(df_test[i])","b60e73b7":"for i in categorical_values_train:\n    feature_set = set(df_train[i])\n    for j in feature_set:\n        feature_list = list(feature_set)\n        df_train.loc[df_train[i] == j, i] = feature_list.index(j)\n        df_train_add.loc[df_train[i] == j, i] = feature_list.index(j)\n\nfor i in categorical_values_test:\n    feature_set2 = set(df_test[i])\n    for j in feature_set2:\n        feature_list2 = list(feature_set2)\n        df_test.loc[df_test[i] == j, i] = feature_list2.index(j)\n        df_test_add.loc[df_test[i] == j, i] = feature_list2.index(j)","b7d1c25a":"df_train_add.head()","b912686a":"df_test_add.head()","ba6f12a7":"#Importing all the librairies we'll need\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score, KFold","fb8f47e5":"X_train = df_train_add.drop([\"SalePrice\",\"LogPrice\"], axis=1)\ny_train = df_train_add[\"LogPrice\"]","42be54d2":"from sklearn.model_selection import train_test_split #to create validation data set\n\nX_training, X_valid, y_training, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=0) #X_valid and y_valid are the validation sets","2e42317a":"from sklearn.linear_model import LinearRegression\nlm = LinearRegression()\nlm.fit(X_training,y_training)\nprint(lm)","97f09417":"# print the intercept\nprint(lm.intercept_)","68113101":"print(lm.coef_)","729082c8":"predictions = lm.predict(X_valid)\npredictions= predictions.reshape(-1,1)","241a5d97":"submission_predictions = np.exp(predictions)","03d9d180":"from sklearn import metrics\nprint('MAE:', metrics.mean_absolute_error(y_valid, submission_predictions))\nprint('MSE:', metrics.mean_squared_error(y_valid, submission_predictions))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_valid, submission_predictions)))","9960573f":"linreg = LinearRegression()\nparameters_lin = {\"fit_intercept\" : [True, False], \"normalize\" : [True, False], \"copy_X\" : [True, False]}\ngrid_linreg = GridSearchCV(linreg, parameters_lin, verbose=1 , scoring = \"r2\")\ngrid_linreg.fit(X_training, y_training)\n\nprint(\"Best LinReg Model: \" + str(grid_linreg.best_estimator_))\nprint(\"Best Score: \" + str(grid_linreg.best_score_))","2eb4ad58":"linreg = grid_linreg.best_estimator_\nlinreg.fit(X_training, y_training)\nlin_pred = linreg.predict(X_valid)\nr2_lin = r2_score(y_valid, lin_pred)\nrmse_lin = np.sqrt(mean_squared_error(y_valid, lin_pred))\nprint(\"R^2 Score: \" + str(r2_lin))\nprint(\"RMSE Score: \" + str(rmse_lin))","7b5fe72e":"scores_lin = cross_val_score(linreg, X_training, y_training, cv=10, scoring=\"r2\")\nprint(\"Cross Validation Score: \" + str(np.mean(scores_lin)))","2d9c1691":"ridge = Ridge()\nparameters_ridge = {\"fit_intercept\" : [True, False], \"normalize\" : [True, False], \"copy_X\" : [True, False], \"solver\" : [\"auto\"]}\ngrid_ridge = GridSearchCV(ridge, parameters_ridge, verbose=1, scoring=\"r2\")\ngrid_ridge.fit(X_training, y_training)\n\nprint(\"Best Ridge Model: \" + str(grid_ridge.best_estimator_))\nprint(\"Best Score: \" + str(grid_ridge.best_score_))","33e9693d":"ridge = grid_ridge.best_estimator_\nridge.fit(X_training, y_training)\nridge_pred = ridge.predict(X_valid)\nr2_ridge = r2_score(y_valid, ridge_pred)\nrmse_ridge = np.sqrt(mean_squared_error(y_valid, ridge_pred))\nprint(\"R^2 Score: \" + str(r2_ridge))\nprint(\"RMSE Score: \" + str(rmse_ridge))","747df076":"scores_ridge = cross_val_score(ridge, X_training, y_training, cv=10, scoring=\"r2\")\nprint(\"Cross Validation Score: \" + str(np.mean(scores_ridge)))","5351bef3":"from sklearn import ensemble","4cec3fba":"params = {'n_estimators': 20000, 'max_depth': 5, 'min_samples_split': 2,\n          'learning_rate': 0.05, 'loss': 'ls' , 'max_features' : 20}\nclf = ensemble.GradientBoostingRegressor(**params)\n\nclf.fit(X_training, y_training)","47fe1956":"clf_pred=clf.predict(X_valid)\nclf_pred= clf_pred.reshape(-1,1)\nr2_clf = r2_score(y_valid, clf_pred)\nrmse_clf = np.sqrt(mean_squared_error(y_valid, clf_pred))\nprint(\"R^2 Score: \" + str(r2_clf))\nprint(\"RMSE Score: \" + str(rmse_clf))","aab44dfe":"scores_clf = cross_val_score(clf, X_training, y_training, cv=10, scoring=\"r2\")\nprint(\"Cross Validation Score: \" + str(np.mean(scores_clf)))","8e125970":"from sklearn.tree import DecisionTreeRegressor\ndtreg = DecisionTreeRegressor(random_state = 100)\nparameters_dtr = {\"criterion\" : [\"mse\", \"friedman_mse\", \"mae\"], \"splitter\" : [\"best\", \"random\"], \"min_samples_split\" : [2, 3, 5, 10], \n                  \"max_features\" : [\"auto\", \"log2\"]}\ngrid_dtr = GridSearchCV(dtreg, parameters_dtr, verbose=1, scoring=\"r2\")\ngrid_dtr.fit(X_training, y_training)\n\nprint(\"Best DecisionTreeRegressor Model: \" + str(grid_dtr.best_estimator_))\nprint(\"Best Score: \" + str(grid_dtr.best_score_))","0a7211cf":"dtr = grid_dtr.best_estimator_\ndtreg.fit(X_training, y_training)\ndtr_pred = dtreg.predict(X_valid)\nr2_dtr = r2_score(y_valid, dtr_pred)\nrmse_dtr = np.sqrt(mean_squared_error(y_valid, dtr_pred))\nprint(\"R^2 Score: \" + str(r2_dtr))\nprint(\"RMSE Score: \" + str(rmse_dtr))","11aa408c":"scores_dtr = cross_val_score(dtreg, X_training, y_training, cv=10, scoring=\"r2\")\nprint(\"Cross Validation Score: \" + str(np.mean(scores_dtr)))","66fa5624":"rfr = RandomForestRegressor()\nparemeters_rf = {\"n_estimators\" : [5, 10, 15, 20], \"criterion\" : [\"mse\" , \"mae\"], \"min_samples_split\" : [2, 3, 5, 10], \n                 \"max_features\" : [\"auto\", \"log2\"]}\ngrid_rf = GridSearchCV(rfr, paremeters_rf, verbose=1, scoring=\"r2\")\ngrid_rf.fit(X_training, y_training)\n\nprint(\"Best RandomForestRegressor Model: \" + str(grid_rf.best_estimator_))\nprint(\"Best Score: \" + str(grid_rf.best_score_))","2c852fb5":"rf = grid_rf.best_estimator_\nrfr.fit(X_training, y_training)\nrf_pred = rfr.predict(X_valid)\nr2_rf = r2_score(y_valid, rf_pred)\nrmse_rf = np.sqrt(mean_squared_error(y_valid, rf_pred))\nprint(\"R^2 Score: \" + str(r2_rf))\nprint(\"RMSE Score: \" + str(rmse_rf))","74104d01":"scores_rf = cross_val_score(rfr, X_training, y_training, cv=10, scoring=\"r2\")\nprint(\"Cross Validation Score: \" + str(np.mean(scores_rf)))","2a14f96a":"from xgboost import XGBRegressor\n\nxgboost = XGBRegressor(learning_rate=0.01,n_estimators=20000,\n                                     max_depth=3, min_child_weight=0,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     objective='reg:linear', nthread=-1,\n                                     scale_pos_weight=1, seed=27,\n                                     reg_alpha=0.006)\nxgb = xgboost.fit(X_training, y_training)","387a0948":"xgb_pred = xgb.predict(X_valid)\nr2_xgb = r2_score(y_valid, xgb_pred)\nrmse_xgb = np.sqrt(mean_squared_error(y_valid, xgb_pred))\nprint(\"R^2 Score: \" + str(r2_xgb))\nprint(\"RMSE Score: \" + str(rmse_xgb))","1b5b2d50":"from lightgbm import LGBMRegressor\n\nlightgbm = LGBMRegressor(objective='regression', \n                                       num_leaves=4,\n                                       learning_rate=0.01, \n                                       n_estimators=20000,\n                                       max_bin=2000, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.2,\n                                       feature_fraction_seed=7,\n                                       verbose=-1,\n                                       )\ngbm = lightgbm.fit(X_training, y_training)","03ba7d03":"gbm_pred = gbm.predict(X_valid)\nr2_gbm = r2_score(y_valid, gbm_pred)\nrmse_gbm = np.sqrt(mean_squared_error(y_valid, gbm_pred))\nprint(\"R^2 Score: \" + str(r2_gbm))\nprint(\"RMSE Score: \" + str(rmse_gbm))","46a7bf6c":"model_performances = pd.DataFrame({\n    \"Model\" : [\"Linear Regression\", \"Ridge\", \"Decision Tree Regressor\", \"Random Forest Regressor\",\"Gradient Boosting Regression\",\"XGBoost\",\"LGBM Regressor\"],\n    \"R Squared\" : [str(r2_lin)[0:5], str(r2_ridge)[0:5],  str(r2_dtr)[0:5], str(r2_rf)[0:5] , str(r2_clf)[0:5], str(r2_xgb)[0:5], str(r2_gbm)[0:5]],\n    \"RMSE\" : [str(rmse_lin)[0:8], str(rmse_ridge)[0:8],  str(rmse_dtr)[0:8], str(rmse_rf)[0:8], str(rmse_clf)[0:8], str(rmse_xgb)[0:8], str(rmse_gbm)[0:8]]\n})\nmodel_performances.round(4)\n\nprint(\"Sorted by R Squared:\")\nmodel_performances.sort_values(by=\"R Squared\", ascending=False)","a034d7a3":"print(\"Sorted by RMSE:\")\nmodel_performances.sort_values(by=\"RMSE\", ascending=True)","e888498e":"learning_rates = [0.75 ,0.5, 0.25, 0.1, 0.05, 0.01]\n\nr2_results = []\nrmse_results = []\n\nfor eta in learning_rates:\n    model = ensemble.GradientBoostingRegressor(learning_rate=eta)\n    model.fit(X_training, y_training)\n    y_pred = model.predict(X_valid)\n    r2_clf = r2_score(y_valid, y_pred)\n    rmse_clf = np.sqrt(mean_squared_error(y_valid, y_pred))\n    r2_results.append(r2_clf)\n    rmse_results.append(rmse_clf)\n    \nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(learning_rates, r2_results, 'b', label='R^2')\nline2, = plt.plot(learning_rates, rmse_results, 'r', label='RMSE')\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('Score')\nplt.xlabel('learning_rates')\nplt.show()","d90b01e9":"n_estimators = [1, 2, 16, 32, 64, 100, 200, 500]\nr2_results = []\nrmse_results = []\n\nfor estimator in n_estimators:\n    model = ensemble.GradientBoostingRegressor(n_estimators=estimator)\n    model.fit(X_training, y_training)\n    y_pred = model.predict(X_valid)\n    r2_clf = r2_score(y_valid, y_pred)\n    rmse_clf = np.sqrt(mean_squared_error(y_valid, y_pred))\n    r2_results.append(r2_clf)\n    rmse_results.append(rmse_clf)\n    \nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(n_estimators, r2_results, 'b', label='R^2')\nline2, = plt.plot(n_estimators, rmse_results, 'r', label='RMSE')\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('Score')\nplt.xlabel('n_estimators')\nplt.show()","2c0d39dd":"max_depths = np.linspace(1, 10, 10, endpoint=True)\nr2_results = []\nrmse_results = []\n\nfor max_depth in max_depths:\n    model = ensemble.GradientBoostingRegressor(max_depth=max_depth)\n    model.fit(X_training, y_training)\n    y_pred = model.predict(X_valid)\n    r2_clf = r2_score(y_valid, y_pred)\n    rmse_clf = np.sqrt(mean_squared_error(y_valid, y_pred))\n    r2_results.append(r2_clf)\n    rmse_results.append(rmse_clf)\n    \nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(max_depths, r2_results, 'b', label='R^2')\nline2, = plt.plot(max_depths, rmse_results, 'r', label='RMSE')\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('Score')\nplt.xlabel('max_depths')\nplt.show()","54a05201":"min_samples_splits = np.linspace(0.1, 1.0, 10, endpoint=True)\nr2_results = []\nrmse_results = []\n\nfor min_samples_split in min_samples_splits:\n    model = ensemble.GradientBoostingRegressor(min_samples_split=min_samples_split)\n    model.fit(X_training, y_training)\n    y_pred = model.predict(X_valid)\n    r2_clf = r2_score(y_valid, y_pred)\n    rmse_clf = np.sqrt(mean_squared_error(y_valid, y_pred))\n    r2_results.append(r2_clf)\n    rmse_results.append(rmse_clf)\n    \nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(min_samples_splits, r2_results, 'b', label='R^2')\nline2, = plt.plot(min_samples_splits, rmse_results, 'r', label='RMSE')\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('Score')\nplt.xlabel('min_samples_splits')\nplt.show()","126dcd8c":"min_samples_leafs = np.linspace(0.1, 0.5, 5, endpoint=True)\nr2_results = []\nrmse_results = []\n\nfor min_samples_leaf in min_samples_leafs:\n    model = ensemble.GradientBoostingRegressor(min_samples_leaf=min_samples_leaf)\n    model.fit(X_training, y_training)\n    y_pred = model.predict(X_valid)\n    r2_clf = r2_score(y_valid, y_pred)\n    rmse_clf = np.sqrt(mean_squared_error(y_valid, y_pred))\n    r2_results.append(r2_clf)\n    rmse_results.append(rmse_clf)\n    \nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(min_samples_leafs, r2_results, 'b', label='R^2')\nline2, = plt.plot(min_samples_leafs, rmse_results, 'r', label='RMSE')\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('Score')\nplt.xlabel('min_samples_leafs')\nplt.show()","1e7a87e9":"max_features = list(range(1,30))\nr2_results = []\nrmse_results = []\n\nfor max_feature in max_features:\n    model = ensemble.GradientBoostingRegressor(max_features=max_feature)\n    model.fit(X_training, y_training)\n    y_pred = model.predict(X_valid)\n    r2_clf = r2_score(y_valid, y_pred)\n    rmse_clf = np.sqrt(mean_squared_error(y_valid, y_pred))\n    r2_results.append(r2_clf)\n    rmse_results.append(rmse_clf)\n    \nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(max_features, r2_results, 'b', label='R^2')\nline2, = plt.plot(max_features, rmse_results, 'r', label='RMSE')\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('Score')\nplt.xlabel('max_features')\nplt.show()","ce1ad7ef":"def blend_models_predict(X):\n    return ((0.05 * lm.predict(X)) + \\\n            (0.05 * linreg.predict(X)) + \\\n            (0.05 * ridge.predict(X)) + \\\n            (0.1 * clf.predict(X)) + \\\n            (0.2 * gbm.predict(X)) + \\\n            (0.15 * rfr.predict(X)) + \\\n            (0.4 * xgb.predict(X)))\n","e8c69c5b":"submission_predictions = np.exp(blend_models_predict(df_test_add))\nprint(submission_predictions)","4ebd934f":"res=pd.DataFrame(columns = ['Id', 'SalePrice'])\nres['Id'] = df_test.index + 1461\nres['SalePrice'] = submission_predictions\nres.to_csv('submission1.csv',index=False)","d7c79492":"First of all, we will start with dropping the Id column , as it doesn't add any information for our model.","5a08bdb5":"As we've seen before, there are so many columns to work with, so let's try to figure out the correlations to get a better idea of which columns are strongly related to the Sale Price of the house. This will help us eliminating the features that won't do a good job predicting the Sale Price.","5b440bac":"<a id=\"p8\"><\/a>\n# 8. Parameter tuning for Gradient Boosting","67974985":"**Model Evaluation**\nLet's evaluate the model by checking out it's coefficients and how we can interpret them.","448cad01":"## Random Forest Regression ","221184b0":"the features GarageYrBlt, MasVnrArea, and MasVnrType all have a fairly decent amount of missing values. MasVnrType is categorical so we can replace the missing values with \"None\", as we did before.\nFor the others we have many options : choosing between filling with the mean or the median, and these measures can be applied on the train, test or both at the same time.","13139230":"<a id=\"p2\"><\/a>\n# 2. Loading and Inspecting Data\nWe will try to load our Training and Test data set with some Pandas functions as well as inspect it in order to get an idea of the data we're working with. It is necessary to understand the data features before to start running any model.","79740acb":"**Numerical Imputing**\n\nWe'll impute with mean of the values, or we can use the median since the distributions are a little bit skewed as we saw before.","87a82416":"### Defining Training\/Test Sets\n\nWe have already dropped the Id column for the training set since those are not involved in predicting the Sale Price of a house. We will also drop The SalePrice column from our training dataset and make LogPrice our target instead. This will improve model performance and yield a much smaller RMSE because of the scale.","b8149a43":"In this large data, we have a lot of missing values in the cells. In order to effectively train our model, we must first deal with the missing values. There are missing values for both numerical and categorical data.\n\nFor numerical imputing, we will try to fill the missing values with the mean. For categorical imputing, I chose to fill the missing values with the most common term that appeared from the entire column.","bf23cada":"<a id=\"p5\"><\/a>\n# 5. Feature Engineering\n\nAs we saw before, the target variable \"SalePrice\" is not uniformly distributed and it's skewed towards the left . Therefore, we will try to use log to remove the skewness.","a8a7ca00":"<a id=\"p1\"><\/a>\n# 1.  Importing Packages\nWe will need as usual the numpy and pandas libraries to work with numbers and data, seaborn and matplotlib to visualize data. We would also like to filter out unnecessary warnings.","a64f5d1c":"So we will choose min_samples_splits = 0.28","f483d9e9":"## Max_depth\nThis indicates how deep the built tree can be. The deeper the tree, the more splits it has and it captures more information about how the data. ","1ba19813":"**Predictions from our Model** ","9d673dc9":"Based on the plots above, we will choose a max_depth = 5.","542d1f44":"<a id=\"p6\"><\/a>\n# 6. Process : ML Models\nNow that we've explored the data, we can begin to build and test different models for regression to predict the SalePrice of each house.\nIn classification, we used accuracy as a evaluation metric. In regression, we will use the R^2 score as well as the RMSE to evaluate our model performance. We will also use cross validation to optimize our model hyperparameters.","a264a9db":"It appears that adding the logarithm has made the target SalePrice more normally distributed. Machine Learning models tend to work much better with normally distributed targets, rather than greatly skewed targets. By transforming the prices, we can improve the performance later.","5429695c":"## Max_features\nmax_features represents the number of features to consider when looking for the best split.","467b3b45":"<a id=\"p4\"><\/a>\n# 4. Imputing Null Values\n","23efb1f7":"Now, the features with a lot of missing values have been taken care of! Let's move on to the features with fewer missing values.","58f85dc6":"### Adding new features: ","64556034":"**Categorical Imputing**\n\nWe'll impute with the most common term that appears in the entire list.","5bc3ae50":"## Learning_rate","b1489002":" Increasing this value can cause underfitting.","8478bb11":"<a id=\"p9\"><\/a>\n# 9. Blending + Submission","f6f99fd7":"<a id=\"p3\"><\/a>\n# 3. Inspecting Data: Target Variable and its Correlation\n","2c20f3bf":"## Ridge Model","0bb81422":"### LGBM Regressor","32cdff77":"## Xgboost","e915039d":"We can clearly see that the target variable has a normal ditribution that is skewed towards the left. Now let's calculate the Skewness and Kurtosis :","01e19186":"<a id=\"p7\"><\/a>\n# 7. Model Comparison\nAfter applying different models and evaluating them, now we will use test data to predict the LogPrice with the most adequat one.","29c51507":"We see that using a high learning rate results in overfitting. For this data, a learning rate of 0.05 is optimal.","410446fe":"The Training data contains 1460 rows and 81 columns (features), testing data has 80 columns.","853dfed5":"To show the most common term for each of the categorical features that we're working with. We'll replace the null values with these.","f9ea7e4e":"In our case, using more than 100 trees is good for our model.","70fd6de3":"### Splitting into Validation\n\nTry to split our training data again into validation sets. This will help us evaluate our model performance and maybe avoid overfitting.","c2e4f5c3":"## Imputing \"Real\" NaN Values\nNow we are going to deal with the real NaN values that were not recorded.","2c5c462e":"## NaN values are important\nIn fact, the NaN values actually mean something in some columns. This means that if a value is NaN, the house might not have that certain attribute, which will affect the price of the house.\nSo, we will try to fill in the null cell with a new category called \"None\".","468597df":"We can see that the feature LotFrontage has too many Null values, so it's better to just drop it.","38ff3a67":"Finally, I decided to use the XGBoost on the test set because I believe it will perform the best based on the comparison above. It has a high R^2 value and a low RMSE.\nBut before doing that, let's try to improve our Gradient Boosting Regression model by tuning its parameters.","160c2931":"## Min_samples_leaf\nThe minimum number of samples required to be at a leaf node. ","072a7440":"Let's check how many null values are remaining.","c3b0b645":"The problem is to predict housing prices (houses) based on their characteristics. This is a regression problem because we try to predict a continuous value instead of a binary value.\n\n\n\n# Contents\n1. [Importing Packages](#p1)\n2. [Loading Data](#p2)\n3. [Inspecting Data: Target Variable and its correlation](#p3)\n4. [Imputing Null Values](#p4)\n5. [Feature Engineering](#p5)\n6. [ML Models](#p6)\n7. [Model Comparison](#p7)\n8. [Parameter tuning for Gradient Boosting](#p8)\n9. [Blending + Submission](#p9)","1f7b3391":"## Min_samples_split\nmin_samples_split represents the minimum number of samples required to split an internal node. This can vary between considering at least one sample at each node to considering all of the samples at each node. When we increase this parameter, the tree becomes more constrained as it has to consider more samples at each node.","e0c13a45":"If it works successful, the code above should print an empty table.","a367280e":"## Linear Regression Model","6b94c71c":"**Adding the GridSearchCV function**","2d0d0c7d":"## <div align=\"center\"><span style=\"color:red\">Don't forget to upvote and all sugesstions are welcomed!<\/span><\/div>","307db64d":"## <div align=\"center\"><span style=\"color:red\">Please upvote if you liked the notebook!<\/span><\/div>","9fa06b80":"## N_estimators\nN_estimators represents the number of trees in the forest. Usually the higher the number of trees the better to learn the data.","48d8c5ca":"## Decision Tree Regression","966f1c29":"## Gradient Boosting Regression"}}