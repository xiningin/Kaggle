{"cell_type":{"680375e6":"code","696bf332":"code","8ba91949":"code","87a5e53b":"code","10f198a7":"code","4e5d0109":"code","d941174d":"code","ab36819e":"code","07c70a7d":"code","4f5aa524":"code","e268f602":"code","16aebbb7":"code","f6b517fd":"code","7156fb0d":"markdown","425a562c":"markdown","fdc9bea2":"markdown","65b49160":"markdown","7dcba862":"markdown"},"source":{"680375e6":"import pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport time","696bf332":"def add_features(df):\n    #df['area'] = df['time_step'] * df['u_in']\n    #df['area'] = df.groupby('breath_id')['area'].cumsum()\n\n    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n\n\n    # fast area calculation\n    df['time_delta'] = df['time_step'].diff()\n    df['time_delta'].fillna(0, inplace=True)\n    df['time_delta'].mask(df['time_delta'] < 0, 0, inplace=True)\n    df['tmp'] = df['time_delta'] * df['u_in']\n    df['area_true'] = df.groupby('breath_id')['tmp'].cumsum()\n    df['tmp'] = df['u_out']*(-1)+1 # inversion of u_out\n\n    df['u_in_lag1'] = df.groupby('breath_id')['u_in'].shift(1)\n    #df['u_out_lag1'] = df.groupby('breath_id')['u_out'].shift(1)\n    df['u_in_lag_back1'] = df.groupby('breath_id')['u_in'].shift(-1)\n    #df['u_out_lag_back1'] = df.groupby('breath_id')['u_out'].shift(-1)\n    df['u_in_lag2'] = df.groupby('breath_id')['u_in'].shift(2)\n    #df['u_out_lag2'] = df.groupby('breath_id')['u_out'].shift(2)\n    df['u_in_lag_back2'] = df.groupby('breath_id')['u_in'].shift(-2)\n    #df['u_out_lag_back2'] = df.groupby('breath_id')['u_out'].shift(-2)\n    df['u_in_lag3'] = df.groupby('breath_id')['u_in'].shift(3)\n    #df['u_out_lag3'] = df.groupby('breath_id')['u_out'].shift(3)\n    df['u_in_lag_back3'] = df.groupby('breath_id')['u_in'].shift(-3)\n    #df['u_out_lag_back3'] = df.groupby('breath_id')['u_out'].shift(-3)\n    df['u_in_lag4'] = df.groupby('breath_id')['u_in'].shift(4)\n    #df['u_out_lag4'] = df.groupby('breath_id')['u_out'].shift(4)\n    df['u_in_lag_back4'] = df.groupby('breath_id')['u_in'].shift(-4)\n    #df['u_out_lag_back4'] = df.groupby('breath_id')['u_out'].shift(-4)\n    df = df.fillna(0)\n\n    df['breath_id__u_in__max'] = df.groupby(['breath_id'])['u_in'].transform('max')\n    #df['breath_id__u_out__max'] = df.groupby(['breath_id'])['u_out'].transform('max')\n\n    df['u_in_diff1'] = df['u_in'] - df['u_in_lag1']\n    #df['u_out_diff1'] = df['u_out'] - df['u_out_lag1']\n    df['u_in_diff2'] = df['u_in'] - df['u_in_lag2']\n    #df['u_out_diff2'] = df['u_out'] - df['u_out_lag2']\n\n    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n\n    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n\n    df['u_in_diff3'] = df['u_in'] - df['u_in_lag3']\n    #df['u_out_diff3'] = df['u_out'] - df['u_out_lag3']\n    df['u_in_diff4'] = df['u_in'] - df['u_in_lag4']\n    #df['u_out_diff4'] = df['u_out'] - df['u_out_lag4']\n    #df['cross']= df['u_in']*df['u_out']\n    #df['cross2']= df['time_step']*df['u_out']\n\n    df['R'] = df['R'].astype(str)\n    df['C'] = df['C'].astype(str)\n    df['R__C'] = df[\"R\"].astype(str) + '__' + df[\"C\"].astype(str)\n    df = pd.get_dummies(df)\n    return df","8ba91949":"print(\"Loading data\")\ntrain = pd.read_csv('..\/input\/ventilator-pressure-prediction\/train.csv')\ntest = pd.read_csv('..\/input\/ventilator-pressure-prediction\/test.csv')\nmasks=np.array(train['u_out']==0).reshape(-1, 80)\ntargets = train[['pressure']].to_numpy().reshape(-1, 80)\n\nprint(\"Adding features\")\ntrain = add_features(train)\ntest = add_features(test)\n\nfrom sklearn.preprocessing import RobustScaler, normalize\n\nprint(\"Dropping some features\")\ntrain.drop(['pressure', 'id', 'breath_id'], axis=1, inplace=True)\ntest = test.drop(['id', 'breath_id'], axis=1)\ncolumns=train.columns\nnp.save('columns',np.array(train.columns))\n\nprint(\"Normalizing\")\nRS = RobustScaler()\ntrain = RS.fit_transform(train)\ntest = RS.transform(test)\n\nprint(\"Reshaping\")\ntrain = train.reshape(-1, 80, train.shape[-1])\ntest = test.reshape(-1, 80, train.shape[-1])","87a5e53b":"from sklearn.model_selection import KFold\n\nfold=0\n\nkf = KFold(n_splits=10,random_state=2020,shuffle=True)\n\ntrain_features=[train[i] for i in list(kf.split(train))[fold][0]]\nval_features=[train[i] for i in list(kf.split(train))[fold][1]]\ntrain_targets=[targets[i] for i in list(kf.split(targets))[fold][0]]\nval_targets=[targets[i] for i in list(kf.split(targets))[fold][1]]\ntrain_masks=[masks[i] for i in list(kf.split(targets))[fold][0]]\nval_masks=[masks[i] for i in list(kf.split(targets))[fold][1]]\n\n#exit()\n\nprint(f\"### in total there are {len(train_features)} in train###\")\nprint(f\"### in total there are {len(val_features)} in val###\")","10f198a7":"import numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nimport random\n\n\nbatch_size=128\n\nclass SAKTDataset(Dataset):\n    def __init__(self, features, targets, masks, train=True): #HDKIM 100\n        super(SAKTDataset, self).__init__()\n        self.features = features\n        self.targets = targets\n        self.masks = masks\n\n    def __len__(self):\n        return len(self.features)\n\n    def __getitem__(self, index):\n        return self.features[index].astype('float32'),self.targets[index].astype('float32'),self.masks[index].astype('bool')\n\nclass TestDataset(Dataset):\n    def __init__(self, features): #HDKIM 100\n        super(TestDataset, self).__init__()\n        self.features = features\n\n    def __len__(self):\n        return len(self.features)\n\n    def __getitem__(self, index):\n\n        return self.features[index].astype('float32')\n\ntrain_dataset = SAKTDataset(train_features,train_targets,train_masks)\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\ndel train_features\n\nval_dataset = SAKTDataset(val_features,val_targets,val_masks)\nval_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\ndel val_features","4e5d0109":"import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nclass ResidualLSTM(nn.Module):\n\n    def __init__(self, d_model):\n        super(ResidualLSTM, self).__init__()\n        self.LSTM=nn.LSTM(d_model, d_model, num_layers=1, bidirectional=True)\n        self.linear1=nn.Linear(d_model*2, d_model*4)\n        self.linear2=nn.Linear(d_model*4, d_model)\n\n\n    def forward(self, x):\n        res=x\n        x, _ = self.LSTM(x)\n        x=F.relu(self.linear1(x))\n        x=self.linear2(x)\n        x=res+x\n        return x\n    \nclass SAKTModel(nn.Module):\n    def __init__(self, n_skill, n_cat, nout, max_seq=100, embed_dim=128, pos_encode='LSTM', nlayers=2, rnnlayers=3,\n    dropout=0.1, nheads=8):\n        super(SAKTModel, self).__init__()\n        self.n_skill = n_skill\n        self.embed_dim = embed_dim\n        #self.pos_encoder = PositionalEncoding(embed_dim, dropout)\n        if pos_encode=='LSTM':\n            self.pos_encoder = nn.ModuleList([ResidualLSTM(embed_dim) for i in range(rnnlayers)])\n        elif pos_encode=='GRU':\n            self.pos_encoder = nn.ModuleList([ResidualGRU(embed_dim) for i in range(rnnlayers)])\n        elif pos_encode=='GRU2':\n            self.pos_encoder = nn.GRU(embed_dim,embed_dim, num_layers=2,dropout=dropout)\n        elif pos_encode=='RNN':\n            self.pos_encoder = nn.RNN(embed_dim,embed_dim,num_layers=2,dropout=dropout)\n        self.pos_encoder_dropout = nn.Dropout(dropout)\n        self.embedding = nn.Linear(n_skill, embed_dim)\n        self.cat_embedding = nn.Embedding(n_cat, embed_dim, padding_idx=0)\n        self.layer_normal = nn.LayerNorm(embed_dim)\n        encoder_layers = [nn.TransformerEncoderLayer(embed_dim, nheads, embed_dim*4, dropout) for i in range(nlayers)]\n        conv_layers = [nn.Conv1d(embed_dim,embed_dim,(nlayers-i)*2-1,stride=1,padding=0) for i in range(nlayers)]\n        deconv_layers = [nn.ConvTranspose1d(embed_dim,embed_dim,(nlayers-i)*2-1,stride=1,padding=0) for i in range(nlayers)]\n        layer_norm_layers = [nn.LayerNorm(embed_dim) for i in range(nlayers)]\n        layer_norm_layers2 = [nn.LayerNorm(embed_dim) for i in range(nlayers)]\n        self.transformer_encoder = nn.ModuleList(encoder_layers)\n        self.conv_layers = nn.ModuleList(conv_layers)\n        self.layer_norm_layers = nn.ModuleList(layer_norm_layers)\n        self.layer_norm_layers2 = nn.ModuleList(layer_norm_layers2)\n        self.deconv_layers = nn.ModuleList(deconv_layers)\n        self.nheads = nheads\n        self.pred = nn.Linear(embed_dim, nout)\n        self.downsample = nn.Linear(embed_dim*2,embed_dim)\n\n    def forward(self, numerical_features, categorical_features=None):\n        device = numerical_features.device\n        numerical_features=self.embedding(numerical_features)\n        x = numerical_features#+categorical_features\n        x = x.permute(1, 0, 2)\n        for lstm in self.pos_encoder:\n            lstm.LSTM.flatten_parameters()\n            x=lstm(x)\n\n        x = self.pos_encoder_dropout(x)\n        x = self.layer_normal(x)\n\n\n\n        for conv, transformer_layer, layer_norm1, layer_norm2, deconv in zip(self.conv_layers,\n                                                               self.transformer_encoder,\n                                                               self.layer_norm_layers,\n                                                               self.layer_norm_layers2,\n                                                               self.deconv_layers):\n            #LXBXC to BXCXL\n            res=x\n            x=F.relu(conv(x.permute(1,2,0)).permute(2,0,1))\n            x=layer_norm1(x)\n            x=transformer_layer(x)\n            x=F.relu(deconv(x.permute(1,2,0)).permute(2,0,1))\n            x=layer_norm2(x)\n            x=res+x\n\n        x = x.permute(1, 0, 2)\n\n        output = self.pred(x)\n\n        return output.squeeze(-1)\n    ","d941174d":"model = SAKTModel(train.shape[-1], 10, 1, embed_dim=256, pos_encode='LSTM',\n                  max_seq=None, nlayers=3, rnnlayers=3,\n                  dropout=0,nheads=16).cuda()","ab36819e":"#install ranger optimizer\n#! git clone https:\/\/github.com\/lessw2020\/Ranger-Deep-Learning-Optimizer\n#! pip install -e Ranger-Deep-Learning-Optimizer\n! pip install pytorch_ranger\n","07c70a7d":"#optimizer and criterion\nfrom pytorch_ranger import Ranger\noptimizer = Ranger(model.parameters(), lr=8e-4)\ncriterion = nn.L1Loss(reduction='none')","4f5aa524":"epochs=150\nval_metric = 100\nbest_metric = 100\ncos_epoch=int(epochs*0.75)\nscheduler=torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,(epochs-cos_epoch)*len(train_dataloader))\nsteps_per_epoch=len(train_dataloader)\nval_steps=len(val_dataloader)","e268f602":"for epoch in range(epochs):\n    model.train()\n    train_loss=0\n    t=time.time()\n    for step,batch in enumerate(train_dataloader):\n        #series=batch.to(device)#.float()\n        features,targets,mask=batch\n        features=features.cuda()\n        targets=targets.cuda()\n        mask=mask.cuda()\n        #exit()\n\n        optimizer.zero_grad()\n        output=model(features,None)\n        #exit()\n        #exit()\n\n        loss=criterion(output,targets)#*loss_weight_vector\n        loss=torch.masked_select(loss,mask)\n        loss=loss.mean()\n        loss.backward()\n        # with amp.scale_loss(loss, optimizer) as scaled_loss:\n        #     scaled_loss.backward()\n        optimizer.step()\n\n        train_loss+=loss.item()\n        #scheduler.step()\n        print (\"Step [{}\/{}] Loss: {:.3f} Time: {:.1f}\"\n                           .format(step+1, steps_per_epoch, train_loss\/(step+1), time.time()-t),end='\\r',flush=True)\n        if epoch > cos_epoch:\n            scheduler.step()\n        #break\n    print('')\n    train_loss\/=(step+1)\n\n    #exit()\n    model.eval()\n    val_metric=[]\n    val_loss=0\n    t=time.time()\n    preds=[]\n    truths=[]\n    masks=[]\n    for step,batch in enumerate(val_dataloader):\n        features,targets,mask=batch\n        features=features.cuda()\n        targets=targets.cuda()\n        mask=mask.cuda()\n        with torch.no_grad():\n            output=model(features,None)\n\n            loss=criterion(output,targets)\n            loss=torch.masked_select(loss,mask)\n            loss=loss.mean()\n            val_loss+=loss.item()\n            #val_metric.append(MCMAE(output.reshape(-1,4),labels.reshape(-1,4),stds[-4:]))\n            preds.append(output.cpu())\n            truths.append(targets.cpu())\n            masks.append(mask.cpu())\n        print (\"Validation Step [{}\/{}] Loss: {:.3f} Time: {:.1f}\"\n                           .format(step+1, val_steps, val_loss\/(step+1), time.time()-t),end='\\r',flush=True)\n\n    preds=torch.cat(preds).numpy()\n    truths=torch.cat(truths).numpy()\n    masks=torch.cat(masks).numpy()\n    val_metric=(np.abs(truths-preds)*masks).sum()\/masks.sum()#*stds['pressure']\n    #exit()\n    print('')\n    #val_metric=torch.stack(val_metric).mean().cpu().numpy()\n    val_loss\/=(step+1)\n\n\n    if val_metric < best_metric:\n        best_metric=val_metric\n        torch.save(model.state_dict(),f'model{fold}.pth')","16aebbb7":"test_dataset = TestDataset(test)\ntest_dataloader = DataLoader(test_dataset, batch_size=512, shuffle=False, num_workers=4)","f6b517fd":"PRESSURE_MIN=-1.895744294564641\nPRESSURE_STEP=0.07030214545121005\nPRESSURE_MAX = 64.82099173863328\n\nsubmission=pd.read_csv('..\/input\/ventilator-pressure-prediction\/sample_submission.csv')\n\npreds=[]\nfor batch in tqdm(test_dataloader):\n    features=batch.cuda()\n    #features=features\n    with torch.no_grad():\n        temp=[]\n        #for model in MODELS:\n        output=model(features,None)\n        preds.append(output.cpu())\n\npreds=torch.cat(preds)#.reshape(-1).numpy()\nprint(preds.shape)\npost_processed=preds.reshape(-1)\npost_processed=torch.round( (post_processed - PRESSURE_MIN)\/PRESSURE_STEP ) * PRESSURE_STEP + PRESSURE_MIN\nsubmission['pressure']=post_processed.numpy().clip(PRESSURE_MIN,PRESSURE_MAX)\nsubmission.to_csv('submission.csv',index=False)","7156fb0d":"# My feature engineering function\nMost of my features are taken from public notebooks and are relatively simple","425a562c":"# After training, create test dataset and make predictions on test dataset","fdc9bea2":"# Split dataset into train\/val","65b49160":"# Training loop","7dcba862":"# Create dataloader"}}