{"cell_type":{"032ff132":"code","22df8f04":"code","7b3a4171":"code","811461be":"code","e6836059":"code","8f01e794":"code","eae46abe":"code","8136fd1a":"code","a7c3cbb1":"code","b424dda0":"code","265f4b54":"code","a095863e":"code","ff0e83df":"code","a2d81bfc":"code","a1833ecc":"code","e70e6af7":"code","a84d639a":"code","16ae5ea2":"code","c9709473":"code","6d66bd97":"code","8307a4d7":"code","68b0a6d5":"code","20e125d3":"code","9d7d4249":"code","273a511f":"markdown","f4e7dac8":"markdown","3856871a":"markdown","b981412d":"markdown","ec57fc5d":"markdown","6f742d20":"markdown","f4dfdf82":"markdown"},"source":{"032ff132":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","22df8f04":"# Import modules for model analysis\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\n\n# Import xgb modules\nimport xgboost as xgb","7b3a4171":"# Read in the data\ntrain = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv',index_col=0)\ntest  = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv', index_col=0)\n\ntrain.head()","811461be":"# Check the memory consumed by the DataFrame\ntrain.info(memory_usage='deep')","e6836059":"# Memory usage by variable in MB\ntrain.memory_usage(deep=True) * 1e-6","8f01e794":"# Lets reduce the memory usage of the features\n# First - check the integer values and downcast\ndef int_downcast(df):\n    int_cols = df.select_dtypes(include=['int64'])\n\n    for col in int_cols.columns:\n        print(col, 'min:',df[col].min(),'; max:',df[col].max())\n        df[col] = pd.to_numeric(df[col], downcast ='integer')\n    return df\n\nint_downcast(train)\ntrain.memory_usage(deep=True) * 1e-6","eae46abe":"# Second - check the float values and downcast. Method will have to be applied to the train and test DataFrames\ndef float_downcast(df):\n    float_cols = df.select_dtypes(include=['float64'])\n\n    for col in float_cols.columns:\n#         print(col, 'min:',df[col].min(),'; max:',df[col].max())\n        df[col] = pd.to_numeric(df[col], downcast ='float')\n    return df\n\nfloat_downcast(train)\nfloat_downcast(test)","8136fd1a":"# Check the memory usage by feature\ntrain.memory_usage(deep=True) * 1e-6\ntest.memory_usage(deep=True) * 1e-6","a7c3cbb1":"# Review the memory usage by DataFrame\ntrain.info(memory_usage='deep')\ntest.info(memory_usage='deep')","b424dda0":"# Check for missing values\ntrain.isnull().sum()\ntest.isnull().sum()\n\n# Add a dummy missing value for a row with missing data\nfeatures = [x for x in train.columns.values if x[0]==\"f\"]\ntrain['n_missing'] = train[features].isna().sum(axis=1)\ntest['n_missing'] = test[features].isna().sum(axis=1)","265f4b54":"X = train.drop('claim', axis=1)\ny = train['claim']","a095863e":"# Impute the missing value as the median value\n\n# Create function for the missing value review\ndef impute_miss_values(df, strategy='median'):\n    \n    # List of column names for review\n    column_names = [col for col in df.columns]\n    \n    # create the imputer, the strategy can be mean and median.\n    imputer = SimpleImputer(missing_values=np.nan, strategy=strategy)\n\n    # fit the imputer to the train data\n    imputer.fit(df)\n\n    # apply the transformation to the train and test\n    df_out = pd.DataFrame(imputer.transform(df), columns=column_names)\n    return df_out","ff0e83df":"# Impute missing value for the X and test DataFrames\nX = impute_miss_values(X)\ntest = impute_miss_values(test)","a2d81bfc":"# Prepare the data to be used within the model. Make use of the lgb.Dataset() method to optimise the memory usage\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=6, stratify=y)","a1833ecc":"# Instantiate the XGBClassifier: xg_cl\nxg_cl = xgb.XGBClassifier(objective='binary:logistic', \n                          n_estimators=10, \n                          seed=123, \n                          use_label_encoder=False, \n                          eval_metric='auc', \n                          tree_method='gpu_hist')\n\n# Fit the classifier to the training set\nxg_cl.fit(X_train, y_train)\n\n# Predict the labels of the test set: preds\npreds = xg_cl.predict(X_test)\n\n# Compute the accuracy: accuracy\naccuracy = float(np.sum(preds==y_test))\/y_test.shape[0]\nprint(\"accuracy: %f\" % (accuracy))","e70e6af7":"# Evaluate models\ndef eval_model(model):\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    return roc_auc_score(y_test, y_pred)","a84d639a":"eval_model(xg_cl)","16ae5ea2":"# Lets use the boosting and inbuild CV methods\n\n# Create the DMatrix from X and y: churn_dmatrix\nd_train = xgb.DMatrix(data=X_train, label=y_train)\nd_test = xgb.DMatrix(data=X_test, label=y_test)\nxgd_test = xgb.DMatrix(data=test)\n\n# Create the parameter dictionary: params. NOTE: have to explicitly provide the objective param\nparams = {\"objective\":\"binary:logistic\", \n          \"max_depth\":3,\n#           \"use_label_encoder\":False, \n          \"eval_metric\":'auc', \n          \"tree_method\":'gpu_hist'\n         }\n\n# Reviewing the AUC metric\n# Perform cross_validation: cv_results\ncv_results = xgb.cv(dtrain=d_train, params=params,\n                  nfold=3, num_boost_round=10, \n                  metrics=\"auc\", as_pandas=True, seed=123)\n\n# Print cv_results\nprint(cv_results)\n\n# Print the AUC\nprint((cv_results[\"test-auc-mean\"]).iloc[-1])","c9709473":"# Review the train method\nparams = {\n    \"objective\": \"binary:logistic\", \n    \"max_depth\": 3,\n    \"eval_metric\": 'auc', \n    \"tree_method\": 'gpu_hist'\n}\n\n# train - verbose_eval option switches off the log outputs\nxgb_clf = xgb.train(\n    params,\n    d_train,\n    num_boost_round=5000,\n    evals=[(d_train, 'train'), (d_test, 'test')],\n    early_stopping_rounds=100,\n    verbose_eval=0\n)\n\n# predict\ny_pred = xgb_clf.predict(d_test)\n# Compute and print metrics\nprint(f\"AUC : {roc_auc_score(y_test, y_pred)}\")","6d66bd97":"def submission_sample(model, df_test, model_name):\n    sample = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')\n    sample['claim'] = model.predict(df_test)\n    return sample.to_csv(f'submission_{model_name}.csv',index=False)","8307a4d7":"# Baseline submission - original code versions\n# submission_sample(xgb_clf, xgd_test, 'xgb_base')","68b0a6d5":"# Max_depth - maximum number of nodes from root to leaves. Larger the more complex the model will be.\n# Min_child_weight - minimum weight required to create a new node\n\n# params_grid = {\n#     (max_depth, min_child_weight)\n#     for max_depth in np.arange(3, 11, 1)\n#     for min_child_weight in np.arange(5, 9, 1)\n# }\n\n# # Create the parameter dictionary: params.\n# params = {\"objective\":\"binary:logistic\", \n#           \"eval_metric\":'auc', \n#           \"tree_method\":'gpu_hist'\n#          }\n\n# # Define initial best params and MAE\n# auc_mean = float(\"Inf\")\n# best_params = None\n\n# for max_depth, min_child_weight in params_grid:\n#     print(f'max_depth: {max_depth} & min_child_weight {min_child_weight}')\n#     params['max_depth'] = max_depth\n#     params['min_child_weight'] = min_child_weight\n#     # Reviewing the AUC metric\n#     # Perform cross_validation: cv_results\n#     cv_results = xgb.cv(dtrain=d_train, params=params,\n#                       nfold=3, num_boost_round=10, \n#                       metrics=\"auc\", as_pandas=True, seed=123)\n\n#     # Print the AUC\n#     print((cv_results[\"test-auc-mean\"]).iloc[-1])\n#     # Update best AUC\n#     mean_auc = cv_results[\"test-auc-mean\"].iloc[-1]\n#     if mean_auc > auc_mean:\n#         auc_mean = mean_auc\n#         best_params = (max_depth, min_child_weight)\n# print(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], auc_mean))","20e125d3":"# Review the train method\nparams = {\n    \"objective\": \"binary:logistic\", \n    \"eval_metric\": 'auc', \n    \"tree_method\": 'gpu_hist',\n    \"max_depth\": 3,\n    \"min_child_weight\": 4,\n#     \"subsample\": .8\n    \"eta\": 0.05\n}\n\n# train - verbose_eval option switches off the log outputs\nxgb_clf = xgb.train(\n    params,\n    d_train,\n    num_boost_round=5000,\n    evals=[(d_train, 'train'), (d_test, 'test')],\n    early_stopping_rounds=100,\n    verbose_eval=0\n)\n\n# predict\ny_pred = xgb_clf.predict(d_test)\n# Compute and print metrics\nprint(f\"AUC : {roc_auc_score(y_test, y_pred)}\")","9d7d4249":"# Adjust ETA submission\nsubmission_sample(xgb_clf, xgd_test, 'xgb_eta')","273a511f":"# Make submission","f4e7dac8":"# Missing value treatment","3856871a":"# Model Analysis","b981412d":"### Perform Hyperparameter tuning","ec57fc5d":"# Baseline model","6f742d20":"# Extreme Gradient Boosting Model testing\nAim of this notebook is to review the extreme gradient boosting model which can be used during a binary classification challenge.","f4dfdf82":"# Extreme Gradient Boosting"}}