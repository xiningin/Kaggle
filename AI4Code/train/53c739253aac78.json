{"cell_type":{"e5a259ba":"code","8e2dbd88":"code","59687f42":"code","eaec21ff":"code","c3e5d21a":"code","884381f3":"code","afd463b3":"code","7c2c1c46":"code","833d1e04":"code","dcecb410":"code","e259a1e7":"code","b0cb6e15":"code","b0e78c5c":"code","9ff16f43":"code","424824b1":"code","aff3e5c1":"code","86b0aa7e":"code","070acf86":"code","9d97a022":"code","512f1052":"code","bc1b6326":"code","80ee6f72":"code","e9e430b1":"code","2031d45a":"code","b302cda0":"code","f320a115":"code","7c87d6e1":"code","53e40ec3":"code","025f1437":"code","ef219dbd":"code","18b00644":"code","b1933912":"code","3b54dd08":"code","c195cf08":"code","2f031f46":"code","e21fe4a1":"code","bae0ee4f":"markdown","cda48086":"markdown","8ed1cee7":"markdown","60706be0":"markdown","189135cc":"markdown","eb9970cf":"markdown","d1a57503":"markdown","94fc7a2c":"markdown","1ecc4cc3":"markdown","34be96a5":"markdown","454e2973":"markdown","3c4cd5a2":"markdown","82576baf":"markdown","37c50008":"markdown","be99ec35":"markdown","41bb0608":"markdown"},"source":{"e5a259ba":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom pandas.plotting import scatter_matrix\n#!pip install catboost\n#from catboost import CatBoostClassifier\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.ensemble import StackingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, PowerTransformer, QuantileTransformer, LabelEncoder\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.impute import KNNImputer\n!pip install sklearn-hierarchical-classification\nfrom sklearn_hierarchical_classification import classifier\nfrom sklearn_hierarchical_classification.constants import ROOT\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8e2dbd88":"dataset_path = '\/kaggle\/input\/dry-beans-classification-iti-ai-pro-intake01\/'\ndf = pd.read_csv(os.path.join(dataset_path, 'train.csv'))\nedf = pd.read_csv(os.path.join(dataset_path, 'test.csv'))\nprint('Number of instances, features', df.shape)","59687f42":"print(df.head())","eaec21ff":"print(df.info())","c3e5d21a":"print(df.describe())","884381f3":"dups = df.duplicated()\nprint(dups.any())","afd463b3":"print(df.groupby('y').size())","7c2c1c46":"df.hist(figsize = (30,20))\nplt.show()","833d1e04":"scatter_matrix(df, figsize=(30,30))\nplt.show()","dcecb410":"correlations = df.corr()\nfig = plt.figure(figsize=(10,10))\nax = fig.add_subplot(111)\ncax = ax.matshow(correlations, vmin=-1, vmax=1)\nfig.colorbar(cax)\nplt.show()","e259a1e7":"df.plot(kind='box', subplots=True, figsize=(15,15), layout=(5,5), sharex=False, sharey=False)\nplt.show()","b0cb6e15":"\"\"\"\ndf['Perimeter_area_ratio'] = df['Perimeter'] \/ df['Area']\ndf['Latus_length'] = df[['MajorAxisLength','MinorAxisLength']].apply(lambda x : (2*((x[1]\/2)**2))\/(x[0]\/2) if (x[0]\/2 >= x[1]\/2) else((2*((x[0]\/2)**2))\/(x[1]\/2)),axis=1)\n#df['Semi_latus_length'] = df[['MajorAxisLength','MinorAxisLength']].apply(lambda x : (((x[1]\/2)**2))\/(x[0]\/2) if (x[0]\/2 >= x[1]\/2) else((((x[0]\/2)**2))\/(x[1]\/2)),axis=1)\n#df['Mean_latus'] = df[['Latus_length','Semi_latus_length']].mean(axis=1)\n#df['Mean_area'] = df[['Area','ConvexArea']].mean(axis=1)\n#df['Equivalent_area'] = 3.14*(df['EquivDiameter']\/2)**2\ndf['Thickness'] = (df['MajorAxisLength']\/2) * df['Eccentricity']\ndf['Volume'] = 0.75 * 3.14 * (df['MajorAxisLength']\/2) * (df['MinorAxisLength']\/2) * df['Thickness']\ndf['Elongation'] = df['Area'] \/ df['Thickness']**2\ndf['Bounding_box_area'] = df['MajorAxisLength']*df['MinorAxisLength']\ndf['Fiber_length'] = (df['Perimeter'] - np.sqrt(abs(df['Perimeter']**2 - 16*df['Area']))) \/ 4\ndf['Fiber_width'] = df['Area'] \/ df['Fiber_length']\ndf['Curl'] = df['MajorAxisLength'] \/ df['Fiber_length']\n#df['E_s'] = df['Extent'] * df['Solidity']\n#df['Area_volume_ratio'] = df['Area'] \/ df['Volume']\ndf['Sphericity'] = np.cbrt(36*3.14*df['Volume']**2) \/ df['Area']\ndf['Mean_roundness'] = df[['roundness','Compactness']].mean(axis=1)\n#df['Mean_axis_length'] = df[['MajorAxisLength','MinorAxisLength']].mean(axis=1)\ndf['Surface_area'] = df[['MajorAxisLength','MinorAxisLength','Thickness']].apply(lambda x: 4*3.14*(((x[0]*x[1])**1.6+(x[0]*x[2])**1.6+(x[1]*x[2])**1.6)\/3)**(1\/1.6) ,axis=1)\ndf['Directrix'] = df[['MajorAxisLength','MinorAxisLength','Eccentricity']].apply(lambda x : (x[0]\/2) \/ (x[2]\/2) if ((x[0]\/2) >= (x[1]\/2)) else((x[1]\/2) \/ (x[2]\/2)), axis=1)\ndf['Shape'] = df[['ShapeFactor1','ShapeFactor2','ShapeFactor3','ShapeFactor4']].mean(axis=1)\ndf['Linear_eccentricity'] = df[['MajorAxisLength','MinorAxisLength']].apply(lambda x : np.sqrt((x[0]\/2)**2 - (x[1]\/2)**2),axis=1)\ndf['Focal_parameter'] = df[['MajorAxisLength','MinorAxisLength']].apply(lambda x : ((x[1]\/2)**2) \/ np.sqrt((x[0]\/2)**2 - (x[1]\/2)**2) if (x[0]\/2 >= x[1]\/2) else(((x[0]\/2)**2) \/ np.sqrt((x[0]\/2)**2 - (x[1]\/2)**2)),axis=1)\n#df['Shape'] = df['ShapeFactor1'] + df['ShapeFactor2'] + df['ShapeFactor3'] + df['ShapeFactor4'] \ndf['Form_factor'] = (4 * 3.14 * df['Area']) \/ np.sqrt(df['Perimeter'])\n#df['Convex_hull_area'] = df['Area'] \/ df['Solidity']\ndf['Mass'] = df['Solidity'] * df['Volume']\ndf['Rectangularity'] = df['Area'] \/ df['Bounding_box_area']\n\"\"\"","b0e78c5c":"for predictor in df.columns:\n    df.plot.scatter(x=predictor, y='y', figsize=(10,5), title=predictor+\" VS \"+ 'Bean Type')","9ff16f43":"label_encoder = LabelEncoder()\ntarget = label_encoder.fit_transform(df['y'])\npd.DataFrame(target).hist()","424824b1":"#sns.relplot( x=\"MajorAxisLength\", y=\"MinorAxisLength\", hue=\"AspectRation\", data=df, height=6);","aff3e5c1":"#sns.pairplot(data=df, hue='y');\n#plt.show()","86b0aa7e":"dataset = df.drop(columns=['y'])","070acf86":"def add_features(df):\n    df['Perimeter_area_ratio'] = df['Perimeter'] \/ df['Area']\n    #df['Latus_length'] = df[['MajorAxisLength','MinorAxisLength']].apply(lambda x : (2*((x[1]\/2)**2))\/(x[0]\/2) if (x[0]\/2 >= x[1]\/2) else((2*((x[0]\/2)**2))\/(x[1]\/2)),axis=1)\n    #df['Semi_latus_length'] = df[['MajorAxisLength','MinorAxisLength']].apply(lambda x : (((x[1]\/2)**2))\/(x[0]\/2) if (x[0]\/2 >= x[1]\/2) else((((x[0]\/2)**2))\/(x[1]\/2)),axis=1)\n    #df['Mean_latus'] = df[['Latus_length','Semi_latus_length']].mean(axis=1)\n    #df['Mean_area'] = df[['Area','ConvexArea']].mean(axis=1)\n    #df['Equivalent_area'] = 3.14*(df['EquivDiameter']\/2)**2\n    df['Thickness'] = (df['MajorAxisLength']\/2) * df['Eccentricity']\n    #df['Volume'] = 0.75 * 3.14 * (df['MajorAxisLength']\/2) * (df['MinorAxisLength']\/2) * df['Thickness']\n    #df['Elongation'] = df['Area'] \/ df['Thickness']**2\n    #df['Bounding_box_area'] = df['MajorAxisLength']*df['MinorAxisLength']\n    df['Fiber_length'] = (df['Perimeter'] - np.sqrt(abs(df['Perimeter']**2 - 16*df['Area']))) \/ 4\n    #df['Fiber_width'] = df['Area'] \/ df['Fiber_length']\n    #df['Curl'] = df['MajorAxisLength'] \/ df['Fiber_length']\n    #df['E_s'] = df['Extent'] * df['Solidity']\n    #df['Area_volume_ratio'] = df['Area'] \/ df['Volume']\n    #df['Sphericity'] = np.cbrt(36*3.14*df['Volume']**2) \/ df['Area']\n    df['Mean_roundness'] = df[['roundness','Compactness']].mean(axis=1)\n    #df['Mean_axis_length'] = df[['MajorAxisLength','MinorAxisLength']].mean(axis=1)\n    #df['Surface_area'] = df[['MajorAxisLength','MinorAxisLength','Thickness']].apply(lambda x: 4*3.14*(((x[0]*x[1])**1.6+(x[0]*x[2])**1.6+(x[1]*x[2])**1.6)\/3)**(1\/1.6) ,axis=1)\n    #df['Directrix'] = df[['MajorAxisLength','MinorAxisLength','Eccentricity']].apply(lambda x : (x[0]\/2) \/ (x[2]\/2) if ((x[0]\/2) >= (x[1]\/2)) else((x[1]\/2) \/ (x[2]\/2)), axis=1)\n    #df['Shape'] = df[['ShapeFactor1','ShapeFactor2','ShapeFactor3','ShapeFactor4']].mean(axis=1)\n    #df['Linear_eccentricity'] = df[['MajorAxisLength','MinorAxisLength']].apply(lambda x : np.sqrt((x[0]\/2)**2 - (x[1]\/2)**2),axis=1)\n    #df['Focal_parameter'] = df[['MajorAxisLength','MinorAxisLength']].apply(lambda x : ((x[1]\/2)**2) \/ np.sqrt((x[0]\/2)**2 - (x[1]\/2)**2) if (x[0]\/2 >= x[1]\/2) else(((x[0]\/2)**2) \/ np.sqrt((x[0]\/2)**2 - (x[1]\/2)**2)),axis=1)\n    #df['Shape'] = df['ShapeFactor1'] + df['ShapeFactor2'] + df['ShapeFactor3'] + df['ShapeFactor4'] \n    #df['Form_factor'] = (4 * 3.14 * df['Area']) \/ np.sqrt(df['Perimeter'])\n    #df['Convex_hull_area'] = df['Area'] \/ df['Solidity']\n    #df['Mass'] = df['Solidity'] * df['Volume']\n    #df['Rectangularity'] = df['Area'] \/ df['Bounding_box_area']\n    #X = df[['MajorAxisLength','MinorAxisLength','Area']]\n    #kmeans = KMeans(n_clusters=6)\n    #df[\"Cluster\"] = kmeans.fit_predict(X)\n    #df[\"Cluster\"] = df[\"Cluster\"].astype(\"category\")\n\n    return df","9d97a022":"def select_features(df):\n    rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=3)\n    # fit the model\n    rfe.fit(X, y)\n    # transform the data\n    X, y = rfe.transform(X, y)","512f1052":"def drop_columns(df):\n    #df = df.drop(columns=['ID','roundness','Compactness','ConvexArea'])\n    df = df.drop(columns=['ID'])\n    return df","bc1b6326":"def remove_outliers(df):\n    \n    numeric_var = df.columns\n    for i in numeric_var:\n        q75, q25 = np.percentile(df.loc[:,i], [75, 25])\n        iqr = q75 - q25\n        Innerfence = q25 - (iqr*1.5)\n        Upperfence = q75 + (iqr*1.5)\n    \n        df.loc[df[i]<Innerfence, i] = np.nan\n        df.loc[df[i]>Upperfence, i] = np.nan\n        \n    imputer = KNNImputer(n_neighbors=5)\n    df = imputer.fit_transform(df)\n    return df\n    ","80ee6f72":"def rescale_data(df):\n    scaler = StandardScaler()\n    return scaler.fit_transform(df)","e9e430b1":"def transform_data(df):\n    #return np.log1p(df)\n    #power = PowerTransformer(method='yeo-johnson', standardize=True)\n    power = QuantileTransformer(n_quantiles=100, output_distribution='normal')\n    return power.fit_transform(df)","2031d45a":"def perform_pca(df):\n    pca = PCA(n_components=0.99)\n    res = pca.fit_transform(df)\n    return res","b302cda0":"def preprocess_data(df):\n    #df = add_features(df)\n    df = drop_columns(df)\n    print(df.info())\n    print(df.head())\n    #df = remove_outliers(df)\n    #df = transform_data(df)\n    df = rescale_data(df)\n    #df = perform_pca(df)\n    return df","f320a115":"train_df = preprocess_data(dataset)","7c87d6e1":"from sklearn.feature_selection import f_classif\nfrom sklearn.feature_selection import SelectKBest\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline\nX = train_df\nY = df['y']\n\nover = SMOTE()\nunder = RandomUnderSampler(sampling_strategy='majority')\nsteps = [('o',over),('u', under)]\npipeline = Pipeline(steps=steps)\n#X, Y = pipeline.fit_resample(X, Y)\n\n# define the method\n#rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=10)\n# fit the model\n#rfe.fit(X, Y)\n# transform the data\n#X, Y = rfe.transform(X, Y)\n\n\noversample = SMOTE()\n# Balance dataset\n#X, Y = oversample.fit_resample(X, Y)\n\nX_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=42)\n","53e40ec3":"\"\"\"\nmodels = []\n#models.append(('LR', LogisticRegression()))\nestimators=[('lda',LinearDiscriminantAnalysis()),('svm',SVC(gamma = 0.14, C = 13))]\nmodels.append(('Stacked',StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(solver='sag', max_iter=1000))))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('Random Forest', RandomForestClassifier()))\nmodels.append(('Hist Gradient', HistGradientBoostingClassifier()))\n#models.append(('Ada', AdaBoostClassifier()))\n#models.append(('Gradient Booster', GradientBoostingClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC()))\n\nresults = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=5)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n\"\"\"","025f1437":"from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nestimators=[('lda',LinearDiscriminantAnalysis()),('KNN', KNeighborsClassifier()),('svm',SVC(gamma = 0.14, C = 13)),('hg',HistGradientBoostingClassifier(max_depth=3, max_iter=60))]\nimodel = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(solver='sag', max_iter=2000))\nclass_hierarchy = { ROOT: [\"BOMBAY\",\"D\",\"C\",\"HOROZ\", \"SEKER\"], \"C\": [\"DERMASON\", \"SIRA\"],\"D\":[\"CALI\", \"BARBUNYA\"]}\n#fmodel = classifier.HierarchicalClassifier(base_estimator=SVC(gamma = 0.1, C = 5, probability=True),class_hierarchy=class_hierarchy)\nfmodel = SVC(gamma = 0.1, C = 5)\n#fmodel = RandomForestClassifier(max_depth=20)\n#fmodel = AdaBoostClassifier(base_estimator = SVC(gamma = 0.1, C = 5, probability=True))\n#fmodel = CatBoostClassifier(verbose=0, depth=3)\n#fmodel = HistGradientBoostingClassifier(max_depth=3, max_iter=60, loss='categorical_crossentropy')\n#fmodel =  AdaBoostClassifier(base_estimator = RandomForestClassifier(max_depth = 10, n_estimators = 20, random_state = 42), n_estimators = 20, learning_rate = 0.6, random_state = 42)\nfmodel.fit(X_train,Y_train)\ntrain_result = fmodel.predict(X_train)\nvalid_result = fmodel.predict(X_validation)\n#fmodel.fit(X,Y).score(X_validation, valid_result)\nprint('Training score:', f1_score(Y_train, train_result,average='micro'))\nprint('Validation score:', f1_score(Y_validation, valid_result, average='micro'))","ef219dbd":"print(classification_report(Y_validation, valid_result))","18b00644":"from sklearn.metrics import confusion_matrix \ncm = confusion_matrix(Y_validation, valid_result)\nprint(cm)","b1933912":"# normalize\nrow_sums = cm.sum(axis=1)\nnm = cm \/ row_sums[:, np.newaxis]\n# overlapping\nov = np.zeros(nm.shape)\nfor i in range(nm.shape[0]):\n    for j in range(nm.shape[1]):\n        if i != j:\n            ov[i,j] = (nm[i,j]+nm[j,i])\/2\n        else:\n            ov[i, j] = 1\n# similarity\nsm = np.zeros(ov.shape)\nfor i in range(ov.shape[0]):\n    for j in range(ov.shape[1]):\n        sm[i,j] = 1 - ov[i,j]","3b54dd08":"from plotly.figure_factory import create_dendrogram\ndendro = create_dendrogram(sm, labels=['BARBUNYA','BOMBAY','CALI','DERMASON','HOROZ','SEKER','SIRA'])\ndendro.show()","c195cf08":"#XGBClassifier(max_depth=3, learning_rate=0.2, n_estimators=100)\n#class_hierarchy = {ROOT: [\"A\",\"BOMBAY\",\"B\"], \"A\": [\"SEKER\", \"C\"],\"C\":[\"DERMASON\",\"SIRA\"], \"B\":[\"HOROZ\",\"D\"], \"D\":[\"BARBUNYA\",\"CALI\"]}\nclass_hierarchy = {ROOT: [\"A\",\"B\"], \"A\": [\"SEKER\", \"C\"],\"C\":[\"DERMASON\",\"SIRA\"], \"B\":[\"BOMBAY\",\"D\"], \"D\":[\"HOROZ\",\"E\"] ,\"E\":[\"BARBUNYA\",\"CALI\"]}\nfmodel_h = classifier.HierarchicalClassifier(base_estimator=SVC(gamma=0.1,C=5,probability=True),class_hierarchy=class_hierarchy)\nfmodel_h.fit(X_train,Y_train)\ntrain_result = fmodel_h.predict(X_train)\nvalid_result = fmodel_h.predict(X_validation)\nprint('Training score:', f1_score(Y_train, train_result,average='micro'))\nprint('Validation score:', f1_score(Y_validation, valid_result, average='micro'))\nprint(classification_report(Y_validation, valid_result))","2f031f46":"from sklearn.metrics import confusion_matrix \ncm_new = confusion_matrix(Y_validation, valid_result)\nprint(cm_new)","e21fe4a1":"X_test = preprocess_data(edf)\ny_test_predicted = fmodel_h.predict(X_test)\nedf['y'] = y_test_predicted\nedf[['ID', 'y']].to_csv('\/kaggle\/working\/submission.csv', index=False)","bae0ee4f":"### Target vs Features","cda48086":"## Data Visualization","8ed1cee7":"## Data Description","60706be0":"## Identify Duplicates","189135cc":"### Correlation Matrix","eb9970cf":"### Features' Distribution","d1a57503":"### Scatter Plot Matrix","94fc7a2c":"## Data shape","1ecc4cc3":"### Clustering Analysis","34be96a5":"### Box Plot","454e2973":"## Models Evaluation","3c4cd5a2":"## Data Information","82576baf":"## Class Distribution","37c50008":"## Data Peek","be99ec35":"## Data Preprocessing","41bb0608":"### Target Distribution"}}