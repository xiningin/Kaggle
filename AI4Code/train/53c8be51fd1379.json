{"cell_type":{"c12dcfd3":"code","25db4a39":"code","c03b5e89":"code","69471ea0":"code","67b5ffa3":"code","8d8eaa01":"code","4c7e9975":"code","04fc5d8d":"code","5e7322b3":"code","70d48291":"code","ed01cf8a":"code","6e35b652":"code","2a1085e5":"code","e6914c4a":"code","4c5a5a2d":"code","25f48d75":"code","d8a2d1a6":"code","bd691e5b":"code","d42c58d3":"code","27f59f95":"code","7172261c":"code","b8dce20d":"code","55e96b21":"code","44a45094":"code","756f8a91":"code","d60934eb":"code","da0fe596":"code","8a138747":"code","81997ede":"code","c999a9bf":"code","d9af4f66":"code","f1385ad9":"code","a0e324b0":"code","77424c38":"code","c2fd7c11":"code","7680f735":"code","7005716d":"code","7ef57edb":"code","d6429b82":"markdown","6fe3c118":"markdown","7c97ebdd":"markdown","70f67626":"markdown"},"source":{"c12dcfd3":"#link kaggle: https:\/\/www.kaggle.com\/hnganhlnguyn\/ex5-rnn-lstm-2\n#Author by Hung Anh","25db4a39":"!nvidia-smi","c03b5e89":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport datetime","69471ea0":"# LSTM for air pollution problem with regression framing\nimport numpy\nimport matplotlib.pyplot as plt\nimport math\nimport pandas as pd\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense,Dropout,BatchNormalization\nfrom tensorflow.keras.layers import LSTM,Bidirectional\nfrom sklearn.preprocessing import MinMaxScaler,LabelEncoder\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error","67b5ffa3":"dataset = pd.read_csv('..\/input\/aapldataset\/AAPL.csv',index_col=0)","8d8eaa01":"dataset.head()","4c7e9975":"dataset.info()","04fc5d8d":"dataset.describe()","5e7322b3":"values = dataset.values\ngroups = [0,1,2,3,4,5] # V\u00ec c\u1ed9t 4 l\u00e0 ki\u1ec3u chu\u1ed7i\ni=1\nplt.figure(figsize=(20,20))\nfor group in groups:\n    plt.subplot(len(groups),1,i)\n    plt.plot(values[:,group])\n    plt.title(dataset.columns[group],y=0.5,loc='right')\n    i+=1\nplt.show()","70d48291":"# convert series to supervised learning\ndef series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n    n_vars = 1 if type(data) is list else data.shape[1]\n    df = pd.DataFrame(data)\n    cols, names = list(), list()\n    # input sequence (t-n, ... t-1)\n    for i in range(n_in, 0, -1):\n        cols.append(df.shift(i))        \n        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n    # forecast sequence (t, t+1, ... t+n)\n    for i in range(0, n_out):\n        cols.append(df.shift(-i))\n        if i == 0:\n            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n        else:\n            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n    # put it all together\n    agg = pd.concat(cols, axis=1)\n    agg.columns = names\n    # drop rows with NaN values\n    if dropnan:\n        agg.dropna(inplace=True) # k\u1ebft qu\u1ea3 sau khi drop g\u00e1n lu\u00f4n v\u00e0o agg\n    return agg","ed01cf8a":"#ensure all data is float\nvalues = values.astype('float32')\n#normalize features\nscaler = MinMaxScaler(feature_range=(0,1))\nscaler.fit(values)\nscaled = scaler.transform(values)\nprint('Frame as Series:')\nprint(scaled[:5])\n#frame as supervised learning\nreframed = series_to_supervised(scaled,1,1)\nprint('Frame as supervised learning: ')\nprint(reframed.head())","6e35b652":"#drop columns we don't want to predict\nreframed.drop(reframed.columns[[7,8,9,10,11]],axis = 1,inplace = True)\nprint(\"Frame will use:\")\nprint(reframed.head())","2a1085e5":"# c\u1ed9t var1(t) l\u00e0 c\u1ed9t c\u1ea7n d\u1ef1 \u0111o\u00e1n,","e6914c4a":"values = reframed.values\nn_train_hours = int(0.7*len(dataset))","4c5a5a2d":"train = values[:n_train_hours,:]\n# c\u00f2n l\u1ea1i th\u00ec l\u00e0 test\ntest = values[n_train_hours:,:]\n#split into input and output (c\u00e1c c\u1ed9t \u0111\u1ea7u, c\u1ed9t cu\u1ed1i)\ntrain_X,train_y = train[:,:-1],train[:,-1]\ntest_X,test_y = test[:,:-1],test[:,-1]\nprint('Before reshape:')\nprint(train_X.shape,train_y.shape,test_X.shape,test_y.shape)\n# reshape input to be 3D [samples,timesteps,features]\ntrain_X = train_X.reshape((train_X.shape[0],1,train_X.shape[1]))\ntest_X = test_X.reshape((test_X.shape[0],1,test_X.shape[1]))\nprint('After reshape:')\nprint(train_X.shape,train_y.shape,test_X.shape,test_y.shape)","25f48d75":"from tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau\n\nearly_stopping = EarlyStopping(min_delta=0.000001,\n                              patience = 50,\n                              restore_best_weights=True)\nlearning_rate = ReduceLROnPlateau(patience=10,verbose=1,factor=0.5,min_delta=0.000001)","d8a2d1a6":"import datetime\nmodel = Sequential()\nmodel.add(LSTM(200, activation='relu', return_sequences = True,input_shape = (train_X.shape[1],train_X.shape[2])))\nmodel.add(LSTM(200, activation='relu', return_sequences=True, input_shape=(1, 2)))\nmodel.add(LSTM(100, activation='relu', return_sequences=True))\nmodel.add(LSTM(50, activation='relu', return_sequences=True))\nmodel.add(LSTM(25, activation='relu'))\nmodel.add(Dense(20, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation='relu'))\nmodel.add(Dense(1))\n\n# Compiling the RNN\nmodel.compile(optimizer = 'adam', loss = 'mean_squared_error')","bd691e5b":"model.summary()","d42c58d3":"# Fitting the RNN to the Training set\nt0 = datetime.datetime.now()\nhistory = model.fit(train_X, train_y, validation_data=(test_X,test_y),epochs = 1000, batch_size = 128,callbacks=[early_stopping,learning_rate])\nt1 = datetime.datetime.now()","27f59f95":"print('Time training: ',t1-t0)","7172261c":"#plot \nplt.plot(history.history['loss'],label='train')\nplt.plot(history.history['val_loss'],label='test')\nplt.legend()\nplt.show()","b8dce20d":"model2 = Sequential()\nmodel2.add(Bidirectional(LSTM(200, activation='relu', return_sequences = True,input_shape = (train_X.shape[1],train_X.shape[2]))))\nmodel2.add(Bidirectional(LSTM(200, activation='relu', return_sequences=True, input_shape=(1, 2))))\nmodel2.add(Bidirectional(LSTM(100, activation='relu', return_sequences=True)))\nmodel2.add(Bidirectional(LSTM(50, activation='relu', return_sequences=True)))\nmodel2.add(Bidirectional(LSTM(25, activation='relu')))\nmodel2.add(Dense(20, activation='relu'))\nmodel2.add(Dropout(0.5))\nmodel2.add(Dense(10, activation='relu'))\nmodel2.add(Dense(1))\n\n# Compiling the RNN\nmodel2.compile(optimizer = 'adam', loss = 'mean_squared_error')","55e96b21":"# Fitting the RNN to the Training set\nt0 = datetime.datetime.now()\nhistory2 = model2.fit(train_X, train_y, validation_data=(test_X,test_y),epochs = 1000, batch_size = 128,callbacks=[early_stopping,learning_rate])\nt1 = datetime.datetime.now()","44a45094":"model2.summary()","756f8a91":"print('Time training: ',t1-t0)","d60934eb":"#plot \nplt.plot(history2.history['loss'],label='train')\nplt.plot(history2.history['val_loss'],label='test')\nplt.legend()\nplt.show()","da0fe596":"def invert_scaling(y, X, s):\n    # invert scaling for forecast\n    inv_y = np.concatenate((y, X[:,1:]), axis=1)\n    print(s, \"shape:\", inv_y.shape)\n    inv_y = scaler.inverse_transform(inv_y)\n    print(s, inv_y.shape)\n    # tr\u1ea3 l\u1ea1i h\u00ecnh d\u1ea1ng ban \u0111\u1ea7u\n    inv_y = inv_y[:,0]\n    return inv_y","8a138747":"yhat = model2.predict(test_X)\nprint(\"Test_x_shape:\",test_X.shape)\ntest_X_now= test_X.reshape((test_X.shape[0],test_X.shape[2]))\ntest_X_now.shape","81997ede":"inv_yhat = invert_scaling(yhat,test_X_now,\"inv_yhat\")","c999a9bf":"test_y = test_y.reshape((len(test_y),1))\ninv_y = invert_scaling(test_y,test_X_now,\"inv_y\")","d9af4f66":"rmse = math.sqrt(mean_squared_error(inv_y,inv_yhat))\nprint('Test RMSE: %.3f'%rmse)\nmae = mean_absolute_error(inv_y,inv_yhat)\nprint(\"Test MAE: %.3f\"%mae)","f1385ad9":"plt.figure(figsize=(15,10))\nplt.plot(inv_y-inv_yhat,label=\"Diff between y_test and y_test_hat\")\nplt.legend(title='Notes')\nplt.show()","a0e324b0":"y_train_hat = model2.predict(train_X)\ntrain_X_now = train_X.reshape((train_X.shape[0],train_X.shape[2]))\n\ninv_y_train_hat = invert_scaling(y_train_hat,train_X_now,\"inv_y_train_hat\")","77424c38":"plt.figure(figsize=(15,10))\nplt.plot(test_y,label='Test Real Data',color='red')\nplt.plot(yhat,label='Test prediction',color ='green')\nplt.legend(title='Notes')\nplt.show()","c2fd7c11":"plt.figure(figsize=(15,10))\nplt.plot(yhat,label='Test prediction',color ='green')\nplt.legend(title='Notes')\nplt.show()","7680f735":"print('val_loss train')\nmodel2.evaluate(train_X,train_y)","7005716d":"print('val_loss test')\nmodel2.evaluate(test_X,test_y)","7ef57edb":"model2.save('Ex5Model2BiLinear.h5')","d6429b82":"<h1>Build and Train Model<\/h1>","6fe3c118":"<h2>Nh\u1eadn x\u00e9t<\/h2>\n<li>Th\u00f4ng qua vi\u1ec7c s\u1eed d\u1ee5ng Bidirectional LSTM th\u00ec ta th\u1ea5y model2 c\u1ea3i thi\u1ec7n h\u01a1n model1 r\u1ea5t nhi\u1ec1u<\/li>","7c97ebdd":"<h1>Split data<\/h1>","70f67626":"<h1>Prepare<\/h1>"}}