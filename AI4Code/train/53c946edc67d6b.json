{"cell_type":{"250b14a6":"code","1871cbdf":"code","3c20bf25":"code","d5fb464d":"code","ef77ebe5":"code","49edc2ff":"code","32e495d6":"code","f45e75f8":"code","7479b2d7":"code","37fbc6be":"code","286f71a4":"code","6e3a3014":"code","2cbf09f0":"code","01355dbf":"code","f8b33318":"code","bd6cb7a0":"code","21daa462":"code","ac26a136":"code","a0bd8f4a":"code","2efefaf8":"code","8e69e7f6":"code","8b987d98":"code","9317c8be":"code","2ab29090":"code","ba75a8e2":"code","a9383306":"code","1ccaa9cf":"code","6d8b6350":"code","b566d3e1":"code","4b5d39c1":"markdown","a91bc57d":"markdown","5716424c":"markdown","61a60514":"markdown","3c9fe8e1":"markdown","aefe1af2":"markdown","60edbce2":"markdown","5b0a9f5a":"markdown","483189ff":"markdown"},"source":{"250b14a6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1871cbdf":"df_train  = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')   ","3c20bf25":"df_train[df_train.columns[1:]].corr()['SalePrice'][:]        ","d5fb464d":"df_train.describe()","ef77ebe5":"\ndf_train.isnull().sum().sort_values(ascending = False)[0:20]","49edc2ff":"\ndf_test.isnull().sum().sort_values(ascending = False)[0:40]","32e495d6":"for col in df_train:\n    if df_train[col].dtype == 'object':\n      df_train[col] = df_train[col].fillna(df_train[col].mode()[0])\n    else:\n       df_train[col].fillna(round(df_train[col].mean()),inplace = True)\n\n\nfor col in df_test:\n      if df_test[col].dtype == 'object':\n          df_test[col] = df_test[col].fillna(df_test[col].mode()[0])\n      else:\n        df_test[col].fillna(round(df_test[col].mean()),inplace = True)\n","f45e75f8":"df_train.isnull().any().any()\n","7479b2d7":"df_test.isnull().any().any()","37fbc6be":"df_train.drop(columns = ['Id', ],axis = 1, inplace =True)\ndf_test.drop(columns = ['Id'],axis = 1, inplace =True)","286f71a4":"import seaborn as sns\nsns.pairplot(df_train, x_vars=[\"SalePrice\"],\n                 y_vars=[ \"TotalBsmtSF\", 'YearBuilt','1stFlrSF','GrLivArea','2ndFlrSF','GarageArea'], \n             corner = True)","6e3a3014":"\ndef cat_onehotencoder(df_concat):\n    df_temp = df_concat\n    for col in df_temp:\n        if df_temp[col].dtype =='object':\n            df1 = pd.get_dummies(df_concat[col], drop_first = True)\n            df_concat.drop([col], axis = 1, inplace = True)\n            \n            df_concat = pd.concat([df_concat,df1], axis = 1)\n        \n    \n        \n    \n    return df_concat","2cbf09f0":"y = df_train.iloc[:,-1].values\ndf_t = df_train\ny","01355dbf":"df_train.drop(columns = ['SalePrice'], axis = 0, inplace = True)","f8b33318":"df_concat = pd.concat([df_train,df_test], axis = 0)\ndf_final =  cat_onehotencoder(df_concat)","bd6cb7a0":"df_final =df_final.loc[:,~df_final.columns.duplicated()]\ndf_final.shape","21daa462":"df_final","ac26a136":"import seaborn as sns\ncorrelations = df_train[df_train.columns].corr(method='pearson')\nsns.heatmap(correlations, cmap=\"YlGnBu\", annot = True)\n\nimport heapq\n\nprint('Absolute overall correlations')\nprint('-' * 30)\ncorrelations_abs_sum = correlations[correlations.columns].abs().sum()\nprint(correlations_abs_sum, '\\n')\n\nprint('Weakest correlations')\nprint('-' * 30)\nprint(correlations_abs_sum.nsmallest(5))\n","a0bd8f4a":"train = df_final.iloc[:1460,:]\ntest = df_final.iloc[1460:,:]","2efefaf8":"X= train.iloc[:,:].values\n","8e69e7f6":"\nfrom sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X,y, test_size = 0.2)\n\n\n","8b987d98":"#from sklearn.preprocessing import StandardScaler#\n#sc = StandardScaler()\n#X_train = sc.fit_transform(X_train)\n#X_val = sc.transform(X_val)","9317c8be":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import GridSearchCV\n#from sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\n\ngbreg=GradientBoostingRegressor()\nparameters= {'n_estimators':[100,200,300, 600],\n             'max_depth':[3,4,6,7]\n    }\n\ngbreg=GridSearchCV(gbreg, param_grid=parameters)\ngbreg.fit(X_train,y_train)\nprint(\"The best value of leanring rate is: \",gbreg.best_params_, )","2ab29090":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import r2_score, mean_squared_log_error\nGB_reg = GradientBoostingRegressor(max_depth = 4, n_estimators = 300)\nGB_reg.fit(X_train, y_train)\n\ny_predgb = GB_reg.predict(X_val)\nscore_gb = r2_score(y_predgb, y_val)\nMSL_gb = mean_squared_log_error(y_predgb,y_val)\nprint(score_gb, MSL_gb)\n","ba75a8e2":"from xgboost import XGBRegressor\nreg_xgb = XGBRegressor()\nreg_xgb.fit(X_train,y_train)\nypred_xgb = reg_xgb.predict(X_val)\n\nscore_xgb = r2_score(ypred_xgb, y_val)\nMSL_xgb = mean_squared_log_error(ypred_xgb,y_val)\nprint(score_xgb, MSL_xgb)","a9383306":"from sklearn.ensemble import RandomForestRegressor\nregressor_rf = RandomForestRegressor(n_estimators =75, random_state = 42)\nregressor_rf.fit(X_train,y_train)\nypred_rf = regressor_rf.predict(X_val)\nscore_rf = r2_score(ypred_rf, y_val)\nMSL_rf = mean_squared_log_error(ypred_rf,y_val)\nprint(score_rf, MSL_rf)","1ccaa9cf":"y_pred_final = GB_reg.predict(test)\ny_pred_final","6d8b6350":"sub = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')","b566d3e1":"SalePrice = pd.DataFrame(y_pred_final, columns = ['SalePrice'])\nlen(SalePrice)\nSalePrice.insert(0, 'Id', sub['Id'], True)\nSalePrice.to_csv('sample_submissions1.csv', index = False)","4b5d39c1":"* My first attempt to work on Regression model \n* My Data Analytics skills are still in beginner level. Hence little Data Analytics\n* Most of the Notebooks are overwhelming with lots of concepts, but this one will be simple\nHappy Hunting !\n![all-my-lifei-have-lived-by-a-code-and-the-46408606.png](attachment:all-my-lifei-have-lived-by-a-code-and-the-46408606.png)\n\nPlease upvote if you like this Notebook ","a91bc57d":"**Dealing with missing Values**\n* I made it simple by the filling the float and int type data with Mean of respective column\n* For Data type with Object fill with Mode\n\n\n**First check the Train Dataset and Test Dataset for Missing values**","5716424c":"**Now we have to deal with Categorical Data types**\n* One hot Encoded is applied to all the columns with Categorical Data\n\n* Below function is used for one hot encoding","61a60514":"* Check the Train and Test data now for any missing values","3c9fe8e1":"The corelation data shown above doesn't provide the accurate information as it contains lots of missing values ","aefe1af2":"I have a better score but not the best \nHyper parameters tuning is to be done on this to improve the results \nAlso i haven't dealt with data skewedness \n\n**If you find this useful Please UPVOTE this notebook, Thanks**","60edbce2":"**Split the Train and Test set from df_final**","5b0a9f5a":"Concatenate Test and Train data to develop the categorical data","483189ff":"* Now categorical columns has been taken care "}}