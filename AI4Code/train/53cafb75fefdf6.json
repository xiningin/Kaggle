{"cell_type":{"3b0fc01e":"code","759f61f2":"code","17d686a2":"code","be1a3157":"code","22e21961":"code","767a9a3a":"code","62424b8d":"code","1daca3c4":"code","7815c64b":"code","e69f9279":"code","f1a605e5":"code","b4fd21f3":"code","a9e74753":"code","fa704db6":"code","3a2c3028":"code","cb5bc0ab":"code","a6b1e5a0":"code","c491d315":"code","c3d2297a":"code","e23db364":"code","8d0182b6":"code","ded33603":"code","c2ff910d":"code","678e0e4c":"code","a068dcf0":"code","ba7b4e1d":"code","11f8e70b":"code","8cbdab24":"code","59fe435b":"code","8e3f0882":"code","618b8e72":"code","483b43ee":"code","e4544f6b":"code","f9143c13":"code","572254de":"code","fe56af1d":"code","4fd8f06c":"code","e41ef222":"code","1a840d9f":"code","01f4b6f1":"code","2ae893e8":"code","b2ca62aa":"code","831b7135":"code","ceb2bc40":"code","8dd3d368":"code","0524186a":"code","4568a899":"code","5f3d4c6f":"code","ddd9a170":"code","a4d41c6f":"code","2b51971d":"code","48b1aa1b":"code","a713c4ab":"code","a851875c":"code","5a51a3b6":"code","425043e4":"code","93281b1d":"code","862f9863":"code","6e2f539a":"code","840dedaa":"code","3deb509d":"code","968eab4a":"code","cf8b6ca1":"code","680aad7a":"code","7b00ef1e":"code","becd4a5f":"code","dab15820":"code","7a8de49d":"code","33fef263":"code","eff0415d":"code","70ed5b71":"code","eafd514b":"markdown","444f9a9f":"markdown","ba591396":"markdown","757907ce":"markdown","8ca36e4e":"markdown","c252721b":"markdown","ed528ea5":"markdown","37f80ff5":"markdown","155348cb":"markdown","00d223dc":"markdown","8d28de7c":"markdown","d26dec47":"markdown","4c9aef20":"markdown","9ed2a4c9":"markdown","775e0ebd":"markdown","4311caaa":"markdown","6dca5c30":"markdown","aa4a2b75":"markdown","d6be8f3a":"markdown","761dfeb9":"markdown","9375cf89":"markdown","66855c69":"markdown","7a623e82":"markdown","343c3007":"markdown","38eb7d58":"markdown","d7fa69b0":"markdown","c5e1efe9":"markdown","14d276c4":"markdown","b1c06b25":"markdown","d312f7d4":"markdown","76f2bb88":"markdown","b5c868df":"markdown","1a2aeed6":"markdown","010aea29":"markdown","7e0d1303":"markdown","fa0c658a":"markdown","ed84d428":"markdown","d83a0af3":"markdown","832888b0":"markdown","64aace4b":"markdown","8ed07c03":"markdown","64f15c3f":"markdown","56c5e7ac":"markdown","0284ade3":"markdown","f14db5dc":"markdown","63d6e135":"markdown","fde7213c":"markdown","bb47681d":"markdown","3c311ead":"markdown","ecf2b818":"markdown"},"source":{"3b0fc01e":"import numpy as np\nimport pandas as pd\n\nimport scipy.stats as st\nfrom scipy.special import boxcox1p\n\nimport missingno as msno\nimport seaborn as sns\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport warnings\n\n\nwarnings.filterwarnings('ignore')\n%matplotlib inline","759f61f2":"df_raw_train = pd.read_csv('..\/input\/train.csv')\ndf_raw_test = pd.read_csv('..\/input\/test.csv')","17d686a2":"df_raw_train.shape, df_raw_test.shape","be1a3157":"df_raw_train.info()","22e21961":"df_raw_train.get_dtype_counts()","767a9a3a":"# segregate numeric and categotical features\nnumeric_features = df_raw_train.select_dtypes(include=[np.number])\ncategorical_features = df_raw_train.select_dtypes(include=[np.object])","62424b8d":"pd.set_option('display.max_columns', len(df_raw_train.columns))\ndisplay(df_raw_train.head())","1daca3c4":"df_raw_train.describe()","7815c64b":"idsUnique = len(set(df_raw_train.Id))\nidsTotal = df_raw_train.shape[0]\nidsDupli = idsTotal - idsUnique\nprint(\"There are \" + str(idsDupli) + \" duplicate IDs for \" + str(idsTotal) + \" total entries\")","e69f9279":"# Visualizing the patterns of missing value occurrence in training set\n\nsns.heatmap(df_raw_train.isnull(), cbar=True)","f1a605e5":"missing_df = df_raw_train.isnull().sum(axis=0).reset_index()\nmissing_df.columns = ['column_name', 'missing_count']\nmissing_df['missing_ratio'] = missing_df['missing_count'] \/ df_raw_train.shape[0]\nmissing_df = missing_df.sort_values('missing_count', ascending = False)\nmissing_df.loc[missing_df['missing_count'] > 0]\n#missing_df","b4fd21f3":"# selecting rows whose column value is null \/ None \/ nan\ndf_raw_train[df_raw_train['MasVnrType'].isna()]","a9e74753":"# missingno correlation heatmap measures nullity correlation: how strongly the presence\/absence of one variable affects another variable\nmsno.heatmap(df_raw_train)","fa704db6":"# Dendrogram\n\nmsno.dendrogram(df_raw_train)","3a2c3028":"sns.distplot(df_raw_train.skew(),color='blue',axlabel ='Skewness')","cb5bc0ab":"skewness = df_raw_train.skew().sort_values(ascending = False)\nskewness","a6b1e5a0":"skew_features = df_raw_train[skewness[abs(skewness)>0.5].index]\nskew_features.columns","c491d315":"# We can treat skewness of a feature with the help of log transformation\n\n# skew_features = np.log1p(skew_features)\n\n# OR\n\n# We can use the scipy function boxcox1p which computes the Box-Cox transformation.\n# The goal is to find a simple transformation that lets us normalize data.\n\n# for i in skew_features:\n#    all_features[i] = boxcox1p(all_features[i], boxcox_normmax(all_features[i] + 1)) # all_features = train + test","c3d2297a":"kurtosis = df_raw_train.kurt().sort_values(ascending = False)\nkurtosis","e23db364":"sns.distplot(df_raw_train.kurt(),color='blue',axlabel ='Kurtosis')","8d0182b6":"df_raw_train['SalePrice'].describe()","ded33603":"# Histogram -  To get an idea of the distribution.\nplt.figure(figsize=(14,10))\nplt.subplot(2,2,1)\nplt.hist(df_raw_train['SalePrice'])\n\nplt.subplot(2,2,2)\nsns.distplot(df_raw_train['SalePrice'], color=\"r\", kde=True)\nplt.title(\"Distribution of Sale Price\")\nplt.ylabel(\"Number of Occurences\")\nplt.xlabel(\"Sale Price\")\n\nplt.subplot(2,2,3)\nplt.scatter(range(df_raw_train.shape[0]), df_raw_train[\"SalePrice\"].values,color='orange')\nplt.title(\"Distribution of Sale Price\")\nplt.xlabel(\"Number of Occurences\")\nplt.ylabel(\"Sale Price\")\n\nplt.show()","c2ff910d":"# Removing Outliers\n# upperlimit = np.percentile(df_raw_train.SalePrice.values, 99.5)\n# df_raw_train['SalePrice'].ix[houses['SalePrice']>upperlimit] = upperlimit","678e0e4c":"#skewness and kurtosis\nprint(\"Skewness: %f\" % df_raw_train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % df_raw_train['SalePrice'].kurt())","a068dcf0":"(mu, sigma) = st.norm.fit(df_raw_train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))","ba7b4e1d":"plt.figure(figsize=(8,8))\n\nplt.subplot(2,2,1)\nplt.title('Normal')\nsns.distplot(df_raw_train['SalePrice'], kde=False, fit=st.norm)\n\nplt.subplot(2,2,2)\nst.probplot(df_raw_train['SalePrice'], plot=plt)\nplt.show","11f8e70b":"plt.figure(figsize=(8,8))\n\nplt.subplot(2,2,1)\nplt.title('Normal')\nsns.distplot(np.log1p(df_raw_train['SalePrice']), kde=False, fit=st.lognorm)\n\nplt.subplot(2,2,2)\nst.probplot(np.log1p(df_raw_train['SalePrice']), plot=plt)\nplt.show","8cbdab24":"# df_raw_train.SalePrice = np.log1p(df_raw_train.SalePrice)","59fe435b":"correlations = df_raw_train.corr()\nattrs = correlations.iloc[:-1, :-1]  # all except target\n\nthreshold = 0.5\nimportant_corrs = (attrs[abs(attrs) > threshold][attrs != 1.0]) \\\n    .unstack().dropna().to_dict()\n\nunique_important_corrs = pd.DataFrame(\n    list(set([(tuple(sorted(key)), important_corrs[key])\n              for key in important_corrs])),\n    columns=['Attribute Pair', 'Correlation'])\n\n# sorted by absolute value\nunique_important_corrs = unique_important_corrs.ix[\n    abs(unique_important_corrs['Correlation']).argsort()[::-1]]\n\nunique_important_corrs","8e3f0882":"correlation = numeric_features.corr()\nprint(correlation['SalePrice'].sort_values(ascending = False),'\\n')","618b8e72":"f , ax = plt.subplots(figsize = (14,12))\nplt.title('Correlation of Numeric Features with Sale Price',y=1,size=16)\nsns.heatmap(correlation,square = True,  vmax=1)","483b43ee":"k= 11\ntop_corr_features = correlation.nlargest(k,'SalePrice')['SalePrice'].index\nprint(top_corr_features)\ncm = np.corrcoef(df_raw_train[top_corr_features].values.T)\nf , ax = plt.subplots(figsize = (14,12))\nsns.heatmap(cm, vmax=.8, linewidths=0.01,square=True,annot=True,cmap='viridis',\n            linecolor=\"white\",xticklabels = top_corr_features.values ,annot_kws = {'size':10},yticklabels = top_corr_features.values)","e4544f6b":"corrMatrix=df_raw_train[['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'GarageArea',\n       'TotalBsmtSF', '1stFlrSF', 'FullBath', 'TotRmsAbvGrd', 'YearBuilt',\n       'YearRemodAdd', 'GarageYrBlt', 'MasVnrArea', 'Fireplaces']].corr()\n\nsns.set(font_scale=1.10)\nplt.figure(figsize=(14, 12))\n\nsns.heatmap(corrMatrix, vmax=.8, linewidths=0.01,\n            square=True,annot=True,cmap='viridis',linecolor=\"white\")\nplt.title('Correlation between features');","f9143c13":"sns.set()\ndf_raw_train_copy = df_raw_train.copy()\ndf_raw_train_copy['SalePrice'] = np.log1p(df_raw_train_copy['SalePrice'])\nsns.pairplot(df_raw_train[top_corr_features],size = 2 ,kind ='scatter',diag_kind='kde')\nplt.show()","572254de":"columns = top_corr_features.drop('SalePrice')\nfor c in columns:\n    df = pd.concat([df_raw_train_copy['SalePrice'], df_raw_train_copy[c]], axis=1)\n    sns.lmplot(x=c, y=\"SalePrice\", data=df)\n    ","fe56af1d":"df_raw_train[['OverallQual', 'SalePrice']].groupby(['OverallQual'],\n                                                   as_index=False).mean().sort_values(by='OverallQual', ascending=False)","4fd8f06c":"sns.barplot(df_raw_train.OverallQual, df_raw_train.SalePrice)","e41ef222":"# boxplot is a method for graphically depicting groups of numerical data through their quartiles.\n\nvar = 'OverallQual'\ndata = pd.concat([df_raw_train['SalePrice'], df_raw_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(12, 8))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","1a840d9f":"df_raw_train['OverallQual'].value_counts().plot(kind=\"bar\");","01f4b6f1":"# Filling 'NaN' with string 'MISSING' in all categrical variables\n# Creating a copy, will impute relevant missing values to the main dataframe in the later section\n\n'''\nCopying advice\n\n# IS STILL POINTING TO ORIGINAL, EDITS TO NEW WILL ALSO BE APPLIED TO ORIGINAL\nnew_df = master_df \n\nnew_df[ 1,2 ] = b # will also update `master_df` as well. Careful!\n\n# makes a new copy to avoid this issue\nnew_df = df.copy()\n'''\n\ndf_raw_train_copy = df_raw_train.copy()\nfor c in categorical_features:\n    df_raw_train_copy[c] = df_raw_train_copy[c].astype('category')\n    if df_raw_train_copy[c].isnull().any():\n        df_raw_train_copy[c] = df_raw_train_copy[c].cat.add_categories(['MISSING'])\n        df_raw_train_copy[c] = df_raw_train_copy[c].fillna('MISSING')\n\ndef boxplot(x, y, **kwargs):\n    sns.boxplot(x=x, y=y)\n    x=plt.xticks(rotation=90)\nf = pd.melt(df_raw_train_copy, id_vars=['SalePrice'], value_vars=categorical_features)\ng = sns.FacetGrid(f, col=\"variable\",  col_wrap=2, sharex=False, sharey=False, size=5)\ng = g.map(boxplot, \"value\", \"SalePrice\")","2ae893e8":"var = 'Neighborhood'\ndata = pd.concat([df_raw_train['SalePrice'], df_raw_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 10))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nxt = plt.xticks(rotation=45)","b2ca62aa":"plt.figure(figsize = (12, 6))\nsns.countplot(x = 'Neighborhood', data = data)\nxt = plt.xticks(rotation=45)","831b7135":"data = pd.concat([df_raw_train['SalePrice'], df_raw_train['YearBuilt']], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=df_raw_train['YearBuilt'], y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nplt.xticks(rotation=45);","ceb2bc40":"sns.distplot(df_raw_train[\"YearBuilt\"], kde=False);","8dd3d368":"ConstructionAge =  df_raw_train['YrSold'] - df_raw_train['YearBuilt']\nplt.scatter(ConstructionAge, df_raw_train['SalePrice'])\nplt.ylabel('SalePrice')\nplt.xlabel(\"Construction Age of house\")","0524186a":"# Remove the Ids from train and test, as they are unique for each row and hence not useful for the model\ntrain_ID = df_raw_train['Id']\ntest_ID = df_raw_test['Id']\ndf_raw_train.drop(['Id'], axis=1, inplace=True)\ndf_raw_test.drop(['Id'], axis=1, inplace=True)\ndf_raw_train.shape, df_raw_test.shape","4568a899":"plt.figure(figsize=(16,4))\n\nplt.subplot(1,3,1)\nplt.title('GrLivArea')\nplt.scatter(y =df_raw_train.SalePrice,x = df_raw_train.GrLivArea)\n\nplt.subplot(1,3,2)\nplt.title('TotalBsmtSF')\nplt.scatter(y =df_raw_train.SalePrice,x = df_raw_train.TotalBsmtSF)\n\nplt.subplot(1,3,3)\nplt.title('OverallQual')\nplt.scatter(y =df_raw_train.SalePrice,x = df_raw_train.OverallQual)\n\n\nplt.show()","5f3d4c6f":"df_raw_train.drop(df_raw_train[(df_raw_train['GrLivArea']>4000) & (df_raw_train['SalePrice']<300000)].index, inplace=True)\ndf_raw_train.drop(df_raw_train[(df_raw_train['OverallQual']<5) & (df_raw_train['SalePrice']>200000)].index, inplace=True)","ddd9a170":"plt.figure(figsize=(16,4))\n\nplt.subplot(1,3,1)\nplt.title('GrLivArea')\nplt.scatter(y =df_raw_train.SalePrice,x = df_raw_train.GrLivArea)\n\nplt.subplot(1,3,2)\nplt.title('TotalBsmtSF')\nplt.scatter(y =df_raw_train.SalePrice,x = df_raw_train.TotalBsmtSF)\n\nplt.subplot(1,3,3)\nplt.title('OverallQual')\nplt.scatter(y =df_raw_train.SalePrice,x = df_raw_train.OverallQual)\n\n\nplt.show()","a4d41c6f":"df_raw_train.reset_index(drop=True, inplace=True)","2b51971d":"y_train = df_raw_train.SalePrice\ndf_raw_train.drop('SalePrice', axis=1, inplace=True)\ndf_raw_train.shape, df_raw_test.shape","48b1aa1b":"all_features = pd.concat([df_raw_train, df_raw_test]).reset_index(drop=True)\nall_features.shape","a713c4ab":"numeric_df =  all_features.select_dtypes(include=[np.number])\n\nmissing_numeric_df = numeric_df.isnull().sum(axis=0).reset_index()\nmissing_numeric_df.columns = ['column_name', 'missing_count']\nmissing_numeric_df['missing_ratio'] = missing_numeric_df['missing_count'] \/ numeric_df.shape[0]\nmissing_numeric_df = missing_numeric_df.sort_values('missing_count', ascending = False)\nmissing_numeric_df.loc[missing_numeric_df['missing_count'] > 0]","a851875c":"# Group the by neighborhoods, and fill in missing value by the median LotFrontage of the neighborhood\nall_features['LotFrontage'] = all_features.groupby(\n    'Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\n# Replacing the missing values with 0, since no garage = no cars in garage\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n        all_features[col] = all_features[col].fillna(0)\n        \n# Replacing the missing values with 0, since no basement = no bathrooms, no surface area\nfor col in ('BsmtHalfBath', 'BsmtFullBath', 'TotalBsmtSF', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF'):\n        all_features[col] = all_features[col].fillna(0)\n        \nall_features['MasVnrArea'] = all_features['MasVnrArea'].fillna(0)\n\n","5a51a3b6":"object_df =  all_features.select_dtypes(include='object')\n\nmissing_object_df = object_df.isnull().sum(axis=0).reset_index()\nmissing_object_df.columns = ['column_name', 'missing_count']\nmissing_object_df['missing_ratio'] = missing_object_df['missing_count'] \/ object_df.shape[0]\nmissing_object_df = missing_object_df.sort_values('missing_count', ascending = False)\nmissing_object_df.loc[missing_object_df['missing_count'] > 0]","425043e4":"# For a few columns there is lots of NaN entries.\n# However, reading the data description we find this is not missing data:\n# For PoolQC, NaN is not missing data but means no pool, likewise for Fence, FireplaceQu etc.\n\ncols_fillna = ['PoolQC', 'Alley', 'Fence', 'FireplaceQu', 'GarageCond', 'GarageQual', 'GarageFinish',\n               'GarageType', 'BsmtExposure', 'BsmtCond', 'BsmtQual', 'BsmtFinType2', 'BsmtFinType1', 'MasVnrType']\n\n# replace 'NaN' with 'None' in these columns\nfor col in cols_fillna:\n    all_features[col].fillna('None', inplace=True)\n    all_features[col].fillna('None', inplace=True)\nall_features['MSZoning'] = all_features.groupby(\n    'MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\nall_features['Exterior1st'] = all_features['Exterior1st'].fillna(\n    all_features['Exterior1st'].mode()[0])\nall_features['Exterior2nd'] = all_features['Exterior2nd'].fillna(\n    all_features['Exterior2nd'].mode()[0])\nall_features['SaleType'] = all_features['SaleType'].fillna(\n    all_features['SaleType'].mode()[0])\n\n# the data description states that NA refers to typical ('Typ') values\nall_features['Functional'] = all_features['Functional'].fillna('Typ')\n\nall_features['KitchenQual'] = all_features['KitchenQual'].fillna(\"TA\")\n# Replacing missing values with most frequent ones.\nall_features['Electrical'] = all_features['Electrical'].fillna(\"SBrkr\")\n\nall_features[\"Utilities\"] = all_features[\"Utilities\"].fillna(\"None\")\nall_features['MiscFeature'] = all_features['MiscFeature'].fillna(\"None\")","93281b1d":"all_features['HasWoodDeck'] = all_features['WoodDeckSF'].apply(lambda x: 1 if x > 0 else 0)\nall_features['HasOpenPorch'] = all_features['OpenPorchSF'].apply(lambda x: 1 if x > 0 else 0)\nall_features['Has3SsnPorch'] = all_features['3SsnPorch'].apply(lambda x: 1 if x > 0 else 0)\nall_features['HasScreenPorch'] = all_features['ScreenPorch'].apply(lambda x: 1 if x > 0 else 0)\nall_features['haspool'] = all_features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nall_features['has2ndfloor'] = all_features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nall_features['hasgarage'] = all_features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nall_features['hasbsmt'] = all_features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nall_features['hasfireplace'] = all_features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\nall_features['Total_sqr_footage'] = (all_features['BsmtFinSF1'] + all_features['BsmtFinSF2'] +\n                                 all_features['1stFlrSF'] + all_features['2ndFlrSF'])\nall_features['Total_Bathrooms'] = (all_features['FullBath'] + (0.5 * all_features['HalfBath']) +\n                               all_features['BsmtFullBath'] + (0.5 * all_features['BsmtHalfBath']))\nall_features['Total_porch_sf'] = (all_features['OpenPorchSF'] + all_features['3SsnPorch'] +\n                              all_features['EnclosedPorch'] + all_features['ScreenPorch'] +\n                              all_features['WoodDeckSF'])\n\nall_features['Total_Home_Quality'] = all_features['OverallQual'] + all_features['OverallCond']\n\nall_features['TotalSF'] = all_features['TotalBsmtSF'] + all_features['1stFlrSF'] + all_features['2ndFlrSF']\n\nall_features['YearsSinceRemodel'] = all_features['YrSold'].astype(int) - all_features['YearRemodAdd'].astype(int)\n\n#all_features['ConstructionAge'] = all_features['YrSold'] - all_features['YearBuilt']","862f9863":"#test = all_features.ix[all_features['ConstructionAge'] < 0]\n#test","6e2f539a":"def addSquared(dataframe, column_list):\n    m = dataframe.shape[1]\n    for col in column_list:\n        dataframe = dataframe.assign(newcol=pd.Series(dataframe[col]*dataframe[col]).values)   \n        dataframe.columns.values[m] = col + '_sq'\n        m += 1\n    return dataframe \n\ndef addCubed(dataframe, column_list):\n    m = dataframe.shape[1]\n    for col in column_list:\n        dataframe = dataframe.assign(newcol=pd.Series(dataframe[col]*dataframe[col]*dataframe[col]).values)   \n        dataframe.columns.values[m] = col + '_cube'\n        m += 1\n    return dataframe \n\ndef addsqrt(dataframe, column_list):\n    m = dataframe.shape[1]\n    for col in column_list:\n        dataframe = dataframe.assign(newcol=pd.Series(np.sqrt(dataframe[col])).values)   \n        dataframe.columns.values[m] = col + '_sqrt'\n        m += 1\n    return dataframe \n\ncolumns = top_corr_features.drop('SalePrice')\naddSquared(all_features, columns)\naddCubed(all_features, columns)\naddsqrt(all_features, columns)","840dedaa":"all_features['ExterQual'] = pd.Categorical(all_features['ExterQual'], categories=[\n    'Ex', 'Gd', 'TA', 'Fa', 'Po'], ordered=True).codes\nall_features['ExterCond'] = pd.Categorical(all_features['ExterCond'], categories=[\n    'Ex', 'Gd', 'TA', 'Fa', 'Po'], ordered=True).codes\nall_features['KitchenQual'] = pd.Categorical(all_features['KitchenQual'], categories=[\n    'Ex', 'Gd', 'TA', 'Fa', 'Po'], ordered=True).codes\nall_features['HeatingQC'] = pd.Categorical(all_features['HeatingQC'], categories=[\n    'Ex', 'Gd', 'TA', 'Fa', 'Po'], ordered=True).codes\nall_features['BsmtQual'] = pd.Categorical(all_features['BsmtQual'], categories=[\n    'Ex', 'Gd', 'TA', 'Fa', 'Po', 'None'], ordered=True).codes\nall_features['BsmtCond'] = pd.Categorical(all_features['BsmtCond'], categories=[\n    'Ex', 'Gd', 'TA', 'Fa', 'Po', 'None'], ordered=True).codes\nall_features['BsmtFinType1'] = pd.Categorical(all_features['BsmtFinType1'], categories=[\n    'GLQ', 'ALQ', 'BLQ', 'Rec', 'LwQ', 'Unf', 'None'], ordered=True).codes\nall_features['BsmtFinType2'] = pd.Categorical(all_features['BsmtFinType2'], categories=[\n    'GLQ', 'ALQ', 'BLQ', 'Rec', 'LwQ', 'Unf', 'None'], ordered=True).codes\nall_features['FireplaceQu'] = pd.Categorical(all_features['FireplaceQu'], categories=[\n    'Ex', 'Gd', 'TA', 'Fa', 'Po', 'None'], ordered=True).codes\nall_features['GarageQual'] = pd.Categorical(all_features['GarageQual'], categories=[\n    'Ex', 'Gd', 'TA', 'Fa', 'Po', 'None'], ordered=True).codes\nall_features['GarageCond'] = pd.Categorical(all_features['GarageCond'], categories=[\n    'Ex', 'Gd', 'TA', 'Fa', 'Po', 'None'], ordered=True).codes\nall_features['PoolQC'] = pd.Categorical(all_features['PoolQC'], categories=[\n    'Ex', 'Gd', 'TA', 'Fa', 'None'], ordered=True).codes\nall_features['LandSlope'] = pd.Categorical(all_features['LandSlope'], categories=[\n    'Gtl', 'Mod', 'Sev'], ordered=True).codes\nall_features['PavedDrive'] = pd.Categorical(all_features['PavedDrive'], categories=[\n    'Y', 'P', 'N'], ordered=True).codes\nall_features['GarageFinish'] = pd.Categorical(all_features['GarageFinish'], categories=[\n    'Fin', 'RFn', 'Unf', 'None'], ordered=True).codes\nall_features['BsmtExposure'] = pd.Categorical(all_features['BsmtExposure'], categories=[\n    'Gd', 'Av', 'Mn', 'No', 'None'], ordered=True).codes\nall_features['Functional'] = pd.Categorical(all_features['Functional'], categories=[\n    'Typ', 'Min1', 'Min2', 'Mod', 'Maj1', 'Maj2', 'Sev', 'Sal', 'None'], ordered=True).codes\nall_features['Fence'] = pd.Categorical(all_features['Fence'], categories=[\n    'GdPrv', 'MnPrv', 'GdWo', 'MnWw', 'None'], ordered=True).codes\nall_features['LotShape'] = pd.Categorical(all_features['LotShape'], categories=[\n    'Reg', 'IR1', 'IR2', 'IR3'], ordered=True).codes\n\n\nall_features['CentralAir'] = all_features['CentralAir'].apply(\n    lambda x: 0 if x == 'N' else 1)\nall_features['Street'] = all_features['Street'].apply(\n    lambda x: 0 if x == 'Pave' else 1)","3deb509d":"#from sklearn.preprocessing import LabelEncoder\n#cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond',\n#        'ExterQual', 'ExterCond', 'HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1',\n#        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n#        'LotShape', 'PavedDrive', 'CentralAir')\n# process columns, apply LabelEncoder to categorical features\n#for c in cols:\n#    lbl = LabelEncoder()\n#    lbl.fit(list(all_features[c].values))\n#    all_features[c] = lbl.transform(list(all_features[c].values))\n\n# shape\n#print('Shape all_data: {}'.format(all_features.shape))","968eab4a":"skewness = all_features.skew().sort_values(ascending = False)\nskew_features = all_features[skewness[abs(skewness)>0.5].index]\nfor i in skew_features:\n    all_features[i] = boxcox1p(all_features[i], st.boxcox_normmax(all_features[i] + 1))","cf8b6ca1":"#def add_log_columns(dataframe, column_list):\n#    m = dataframe.shape[1]\n#    for column_name in column_list:\n#        dataframe = dataframe.assign(newcol=pd.Series(np.log1p(dataframe[column_name])).values)\n#        dataframe.columns.values[m] = column_name + '_log'\n#        m += 1\n#    return dataframe\n\n#all_features = add_log_columns(all_features, skew_features)","680aad7a":"#def add_boxcox_columns(dataframe, column_list):\n#    m = dataframe.shape[1]\n#    for column_name in column_list:\n#        dataframe = dataframe.assign(newcol=pd.Series(boxcox1p(\n#       all_features[i], st.boxcox_normmax(all_features[i] + 1))).values)\n#        dataframe.columns.values[m] = column_name + '_log'\n#        m += 1\n#    return dataframe\n\n#all_features = add_boxcox_columns(all_features, skew_features)","7b00ef1e":"# MSSubClass = building class\nall_features['MSSubClass'] = all_features['MSSubClass'].astype(str)\n\n\n#Changing OverallCond into a categorical variable\nall_features['OverallCond'] = all_features['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\nall_features['YrSold'] = all_features['YrSold'].astype(str)\nall_features['MoSold'] = all_features['MoSold'].astype(str)","becd4a5f":"y_train = np.log1p(y_train)\n(mu, sigma) = st.norm.fit(y_train)\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))","dab15820":"# Remove any duplicated column names\nall_features = all_features.loc[:,~all_features.columns.duplicated()]\nall_features.shape","7a8de49d":"all_features = pd.get_dummies(all_features).reset_index(drop=True)\nall_features.shape","33fef263":"X_train = all_features[:df_raw_train.shape[0]]\nX_test = all_features[df_raw_train.shape[0]:]\nX_train.shape, X_test.shape","eff0415d":"X_train, X_valid, y_train, y_valid = train_test_split(X_train,y_train, test_size=0.2)\nprint(X_train.shape, X_valid.shape, y_train.shape, y_valid.shape)","70ed5b71":"lm = LinearRegression()\nlm.fit(X_train,y_train)\ny_pred = lm.predict(X_valid)\nrmse = np.sqrt(metrics.mean_squared_error(y_pred, y_valid))\n\nlm.score(X_train,y_train), lm.score(X_valid,y_valid), rmse","eafd514b":"<b> Pair Plot <\/b>\n - Check Outliers\n - Check relations b\/w variables","444f9a9f":"In a distplot, y-axis is probability density and not probability, the y-axis can take values greater than one. The only requirement of the density plot is that the total area under the curve integrates to one.","ba591396":"\"BsmtX\", \"GarageX\" have missing values in same rows. We may predict that the house doesn't have these features.","757907ce":"Or, we may let original skewed columns in the dataframe and add additional log\/boxcox(column) as new columns.","8ca36e4e":"### Estimate Skewness and Kurtosis\n\nhttps:\/\/towardsdatascience.com\/intro-to-descriptive-statistics-252e9c464ac9","c252721b":"### Correlation between  'SalePrice' and numeric features\n - Correlation Heat Map\n - Zoomed Heat Map\n - Pair Plot\n - Scatter Plot","ed528ea5":"### Create interesting features:\n\n- Simplifications of existing features\n- Combinations of existing features\n- Polynomials on the top 10 existing features","37f80ff5":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#House-Prices:-Advanced-Regression-Techniques\" data-toc-modified-id=\"House-Prices:-Advanced-Regression-Techniques-1\">House Prices: Advanced Regression Techniques<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Exploratory-Data-Analysis\" data-toc-modified-id=\"Exploratory-Data-Analysis-1.1\">Exploratory Data Analysis<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Import-Libraries\" data-toc-modified-id=\"Import-Libraries-1.1.1\">Import Libraries<\/a><\/span><\/li><li><span><a href=\"#Extract-data-and-get-info-about-the-dataframe\" data-toc-modified-id=\"Extract-data-and-get-info-about-the-dataframe-1.1.2\">Extract data and get info about the dataframe<\/a><\/span><\/li><li><span><a href=\"#Check-for-duplicates\" data-toc-modified-id=\"Check-for-duplicates-1.1.3\">Check for duplicates<\/a><\/span><\/li><li><span><a href=\"#Visualising-missing-values\" data-toc-modified-id=\"Visualising-missing-values-1.1.4\">Visualising missing values<\/a><\/span><\/li><li><span><a href=\"#Estimate-Skewness-and-Kurtosis\" data-toc-modified-id=\"Estimate-Skewness-and-Kurtosis-1.1.5\">Estimate Skewness and Kurtosis<\/a><\/span><\/li><li><span><a href=\"#Analysing-'SalePrice'\" data-toc-modified-id=\"Analysing-'SalePrice'-1.1.6\">Analysing 'SalePrice'<\/a><\/span><\/li><li><span><a href=\"#Multicollinearity-Check\" data-toc-modified-id=\"Multicollinearity-Check-1.1.7\">Multicollinearity Check<\/a><\/span><\/li><li><span><a href=\"#Correlation-between--'SalePrice'-and-numeric-features\" data-toc-modified-id=\"Correlation-between--'SalePrice'-and-numeric-features-1.1.8\">Correlation between  'SalePrice' and numeric features<\/a><\/span><\/li><li><span><a href=\"#Correlation-between-'SalePrice'-and-'OverallQual'\" data-toc-modified-id=\"Correlation-between-'SalePrice'-and-'OverallQual'-1.1.9\">Correlation between 'SalePrice' and 'OverallQual'<\/a><\/span><\/li><li><span><a href=\"#Correlation-between-'SalePrice'-and-categorical-features\" data-toc-modified-id=\"Correlation-between-'SalePrice'-and-categorical-features-1.1.10\">Correlation between 'SalePrice' and categorical features<\/a><\/span><\/li><li><span><a href=\"#Normalizing-independent-variables\" data-toc-modified-id=\"Normalizing-independent-variables-1.1.11\">Normalizing independent variables<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Feature-Engineering\" data-toc-modified-id=\"Feature-Engineering-1.2\">Feature Engineering<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Upfront-remove-unnecessary-columns(id's-etc.)\" data-toc-modified-id=\"Upfront-remove-unnecessary-columns(id's-etc.)-1.2.1\">Upfront remove unnecessary columns(id's etc.)<\/a><\/span><\/li><li><span><a href=\"#Remove-outliers\" data-toc-modified-id=\"Remove-outliers-1.2.2\">Remove outliers<\/a><\/span><\/li><li><span><a href=\"#Combine-train,-test-dataset\" data-toc-modified-id=\"Combine-train,-test-dataset-1.2.3\">Combine train, test dataset<\/a><\/span><\/li><li><span><a href=\"#Fill-missing-values\" data-toc-modified-id=\"Fill-missing-values-1.2.4\">Fill missing values<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Missing-numeric-values\" data-toc-modified-id=\"Missing-numeric-values-1.2.4.1\">Missing numeric values<\/a><\/span><\/li><li><span><a href=\"#Missing-object-values\" data-toc-modified-id=\"Missing-object-values-1.2.4.2\">Missing object values<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Create-interesting-features:\" data-toc-modified-id=\"Create-interesting-features:-1.2.5\">Create interesting features:<\/a><\/span><\/li><li><span><a href=\"#Feature-transformation---Categorical-to-ordinal\" data-toc-modified-id=\"Feature-transformation---Categorical-to-ordinal-1.2.6\">Feature transformation - Categorical to ordinal<\/a><\/span><\/li><li><span><a href=\"#Fix-skewed-features\" data-toc-modified-id=\"Fix-skewed-features-1.2.7\">Fix skewed features<\/a><\/span><\/li><li><span><a href=\"#Feature-transformation---Numeric-to-categorical\" data-toc-modified-id=\"Feature-transformation---Numeric-to-categorical-1.2.8\">Feature transformation - Numeric to categorical<\/a><\/span><\/li><li><span><a href=\"#Normalize-'SalePrice'\" data-toc-modified-id=\"Normalize-'SalePrice'-1.2.9\">Normalize 'SalePrice'<\/a><\/span><\/li><li><span><a href=\"#Getting-dummy-categorical-features\" data-toc-modified-id=\"Getting-dummy-categorical-features-1.2.10\">Getting dummy categorical features<\/a><\/span><\/li><li><span><a href=\"#Train-test-split\" data-toc-modified-id=\"Train-test-split-1.2.11\">Train test split<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Modelling\" data-toc-modified-id=\"Modelling-1.3\">Modelling<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Linear-Regression\" data-toc-modified-id=\"Linear-Regression-1.3.1\">Linear Regression<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><\/ul><\/div>","155348cb":"At initial glance it is observed that there are two light-red colored squares.\n\n - The first one refers to the 'TotalBsmtSF' and '1stFlrSF' variables.\n - Second one refers to the 'GarageX' variables.We can conclude that they give almost the same information.(Multicollinearity)","00d223dc":"### Fill missing values\n\n<b>Note the difference between NaN, '', None.<\/b>\n\n- NaN = not a number, still a float type, so think of it as an empty space that can still be passed through numerical operations\n\n- '' = is a empty string type\n\n- None = is also a empty space, but in DataFrames it is considered an object which cannot be processed through optimized numerical operations","8d28de7c":"### Linear Regression","d26dec47":"### Normalize 'SalePrice'\n- The SalePrice is skewed to the right. This is a problem because most ML models don't do well with non-normally distributed data. We can apply a log(1+x) tranform to fix the skew.\n- Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.","4c9aef20":"Lasso\n\nhttps:\/\/chrisalbon.com\/machine_learning\/linear_regression\/effect_of_alpha_on_lasso_regression\/","9ed2a4c9":"### Combine train, test dataset\nCombine train and test features in order to apply the feature transformation pipeline to the entire dataset","775e0ebd":"# House Prices: Advanced Regression Techniques\n<b> Kaggle : https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/overview <\/b>\n\n<b> Metric : Root-Mean-Squared-Error (RMSE) <\/b> between the logarithm of the predicted value and the logarithm of the observed sales price. (Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.)","4311caaa":"### Feature transformation - Numeric to categorical\n\n<b>Transforming some numerical variables that are really categorical<\/b>","6dca5c30":"<b> Zoomed Heat Map <\/b>","aa4a2b75":"Price of house goes down with its age.","d6be8f3a":"### Import Libraries","761dfeb9":"## Feature Engineering","9375cf89":"<b>Quick Tip:<\/b>\nFeature Scaling:\n - StandardScaler - subtract the mean and divide by std\n - MaxAbsScaler - transform down to [-1, 1] bounds\n - QuantileTransformer - transform down to [0 1] bounds\n\n","66855c69":"### Feature transformation - Categorical to ordinal\n<b>Label Encoding some categorical variables that may contain information in their ordering set<\/b>","7a623e82":"### Upfront remove unnecessary columns(id's etc.)","343c3007":"Ridge Regression\n\nhttps:\/\/www.quora.com\/What-is-Ridge-Regression-in-laymans-terms\n    \nhttps:\/\/www.youtube.com\/watch?v=Q81RR3yKn30","38eb7d58":"### Remove outliers\n\n<b> look into 'TotalBsmtSF', 'GrLivArea' <\/b>\n\n - Outliers removal is note always safe. We decided to delete these two as they are very huge and really bad ( extremely large areas for very low prices).\n\n - There are probably others outliers in the training data. However, removing all them may affect badly our models if ever there were also outliers in the test data. That's why , instead of removing them all, we will just manage to make some of our models robust on them. You can refer to the modelling part of this notebook for that.","d7fa69b0":"Some outliers after Sale Price > 6000000","c5e1efe9":"- One interesting observation is between 'TotalBsmtSF' and 'GrLiveArea'. In this figure we can see the dots drawing a linear line, which almost acts like a border. It totally makes sense that the majority of the dots stay below that line. Basement areas can be equal to the above ground living area, but it is not expected a basement area bigger than the above ground living area.\n\n- One more interesting observation is between 'SalePrice' and 'YearBuilt'. In the bottom of the 'dots cloud', we see what almost appears to be a exponential function.We can also see this same tendency in the upper limit of the 'dots cloud'\n\n-  Last observation is that prices are increasing faster now with respect to previous years.\n\n-  Visible outliers in  'TotalBsmtSF', 'GrLivArea'","14d276c4":"### Correlation between 'SalePrice' and categorical features","b1c06b25":"## Modelling","d312f7d4":"### Check for duplicates","76f2bb88":"### Extract data and get info about the dataframe","b5c868df":"It is apparent that SalePrice doesn't follow normal distribution, so before performing regression it has to be transformed to log.","1a2aeed6":"### Normalizing independent variables\n\n- <b> Normality <\/b> - When we talk about normality what we mean is that the data should look like a normal distribution. This is important because several statistic tests rely on this (e.g. t-statistics). In this exercise we'll just check univariate normality for 'SalePrice' (which is a limited approach). Remember that univariate normality doesn't ensure multivariate normality (which is what we would like to have), but it helps. Another detail to take into account is that in big samples (>200 observations) normality is not such an issue. However, if we solve normality, we avoid a lot of other problems (e.g. heteroscedacity) so that's the main reason why we are doing this analysis.\n\n- <b> Homoscedasticity <\/b> - Homoscedasticity refers to the 'assumption that dependent variable(s) exhibit equal levels of variance across the range of predictor variable(s)' (Hair et al., 2013). Homoscedasticity is desirable because we want the error term to be the same across all values of the independent variables.\n\n- <b> Linearity <\/b> - The most common way to assess linearity is to examine scatter plots and search for linear patterns. If patterns are not linear, it would be worthwhile to explore data transformations. However, we'll not get into this because most of the scatter plots we've seen appear to have linear relationships.\n\n- <b> Absence of correlated errors <\/b> - Correlated errors, like the definition suggests, happen when one error is correlated to another. For instance, if one positive error makes a negative error systematically, it means that there's a relationship between these variables. This occurs often in time series, where some patterns are time related. We'll also not get into this. However, if you detect something, try to add a variable that can explain the effect you're getting. That's the most common solution for correlated errors.\n\n","010aea29":"### Getting dummy categorical features","7e0d1303":"### Multicollinearity Check\n\nMulticollinearity refers to features that are correlated with other features. Multicollinearity occurs when your model includes multiple factors that are correlated not just to your target variable, but also to each other.\n\nProblem:\n- Multicollinearity increases the standard errors of the coefficients. That means, multicollinearity makes some variables statistically insignificant when they should be significant.\n\nTo avoid this we can do 3 things:\n- Completely remove those variables\n- Make new feature by adding them or by some other operation.\n- Use PCA, which will reduce feature set to small number of non-collinear features.","fa0c658a":"### Train test split","ed84d428":"### Visualising missing values","d83a0af3":"<b>Quick Tip <\/b>\nfit vs. fit_transform vs. transform\n - fit_transform\n   - Xtrain\u2212norm=Xtrain\u2212\u03bctrain\u03c3train\n - transform - note that we divide by the previously fit values\n   - Xtest\u2212norm=Xtest\u2212\u03bctrain\u03c3train\n - fit\n   - when you fit a scaler to dataset A, it calculates mean of A, and the standard deviation of A \n - transform\n   - this will actually look at ANY dataset and subtract previously fitted (calculated) variables mean A and divide by standard deviation of A fit_transform does both of these things in two steps.\n   \n   \nFor consistency purposes, it is best to fit_transform on your training dataset, but only transform your validation set. This ensures your validation and training set has been consistently transformed","832888b0":"<b> Correlation Heat Map <\/b>","64aace4b":"- GarageCars & GarageArea are closely correlated.\n- TotalBsmtSF and 1stFlrSF are also closely correlated.","8ed07c03":"### Correlation between 'SalePrice' and 'OverallQual'","64f15c3f":"Or, Let sklearn select the best ordering for them.","56c5e7ac":"## Exploratory Data Analysis\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/preprocessing.html\n\n - Univariate visualization, Bivariate visualization, Multivariate visualization\n \n   Identify: \n   \n - Trends\n - Distribution\n - Mean\n - Median\n - Outlier\n - Spread measurement (SD)\n - Correlations\n - Hypothesis testing\n - Visual Exploration","0284ade3":"<b>Quick Tip:<\/b>\n\ndf [['a', 'b']] vs df ['a']\n\ndf [['a', 'b']]:\n - dtype = dataframe\n - can select multiple columns from a dataframe\n \ndf ['a']:\n - dtype = series \n - can select single column from a dataframe","f14db5dc":"#### Missing numeric values\n<b>Try different strategies of filling in missing values (modes\/means\/medians\/etc.)<\/b>","63d6e135":"### Analysing 'SalePrice'","fde7213c":"### Fix skewed features\nFixing skewness is motivated by the fact that:\n- linear methods might fit such predictors with very small weights and most of the information contained in the values might be lost\n- predictions when such predictors take very high values might be also very high or misleading.\n\n<b>http:\/\/onlinestatbook.com\/2\/transformations\/box-cox.html<\/b>\n\n<b>https:\/\/docs.scipy.org\/doc\/scipy-0.19.0\/reference\/generated\/scipy.special.boxcox1p.html<\/b>","bb47681d":"Adding squares, sqrt is motivated by non-linearities in scatterplots \"predictor vs. log(SalePrice)\/SalePrice\"","3c311ead":"#### Missing object values","ecf2b818":"<b> Scatter Plot <\/b>"}}