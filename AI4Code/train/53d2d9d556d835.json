{"cell_type":{"70e907c1":"code","677ef821":"code","3a401a41":"code","1769eb6e":"code","3d5b84e2":"code","49ef5f12":"code","f6ed1b8a":"code","a3669bcf":"code","908b792c":"code","ca25fa4a":"code","31c722ec":"code","11474472":"code","d4ee221c":"code","63d03b4a":"code","410a3b27":"code","b442cab2":"code","11c77d52":"code","d7172467":"code","c5ac2e23":"code","f71e746a":"code","be447891":"code","65ab4bcd":"code","9b315ba1":"code","2d105239":"code","60c0a1cb":"code","a124b0d4":"code","2a3c9a9c":"code","6c3e351d":"code","b4d6203c":"code","a23417e5":"code","1e4ac255":"code","2e6caf2f":"code","1e7c31ae":"code","11dde934":"code","d07073a7":"code","68a3c300":"code","a28264d6":"code","ad212dae":"code","f6f5ab47":"code","ca25cb71":"code","9585d010":"code","0c96329e":"code","fbc42219":"code","2ab3b7f7":"code","45b0e18d":"code","86f38bc2":"code","a4c1e514":"code","7547dd77":"markdown","e95cd7f3":"markdown","7f08c9c2":"markdown","b68e342b":"markdown","4bd46522":"markdown","0bcf90bc":"markdown","2c8df145":"markdown","af8f7d45":"markdown","39aecabe":"markdown","7f7e6f6c":"markdown","3c66eb36":"markdown","f97e123c":"markdown","52dbd570":"markdown","762acdfe":"markdown","ba668225":"markdown","a03c87e4":"markdown"},"source":{"70e907c1":"from io import open \nimport os, string, random, time, math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np","677ef821":"from sklearn.model_selection import train_test_split","3a401a41":"import torch \nimport torch.nn as nn\nimport torch.optim as optim","1769eb6e":"from IPython.display import clear_output","3d5b84e2":"languages = []\ndata = []\nX = []\ny = []\n\nurl = '..\/input\/peta-data\/data.txt'\n\nwith open(url, 'r') as f:\n    for line in f:\n        line = line.split(',')\n        name = line[0].strip()\n        lang = line[1].strip()\n        if not lang in languages:\n            languages.append(lang)\n        X.append(name)\n        y.append(lang)\n        data.append((name, lang))\n        \nn_languages = len(languages)","49ef5f12":"print(languages)","f6ed1b8a":"print(data[0:10])","a3669bcf":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)","908b792c":"print(X_train)","ca25fa4a":"print(len(X_train), len(X_test))","31c722ec":"all_letters = string.ascii_letters + \" .,;'\"\nn_letters = len(all_letters)","11474472":"all_letters","d4ee221c":"def name_rep(name):\n    rep = torch.zeros(len(name), 1, n_letters)\n    for index, letter in enumerate(name):\n        pos = all_letters.find(letter)\n        rep[index][0][pos] = 1\n    return rep","63d03b4a":"def lang_rep(lang):\n    return torch.tensor([languages.index(lang)], dtype=torch.long)","410a3b27":"name_rep('Abreu')","b442cab2":"lang_rep('Portuguese')","11c77d52":"count = {}\nfor l in languages:\n    count[l] = 0\nfor d in data:\n    count[d[1]] += 1","d7172467":"print(count)","c5ac2e23":"plt_ = sns.barplot(list(count.keys()), list(count.values()))\nplt_.set_xticklabels(plt_.get_xticklabels(),rotation=90)\nplt.show()","f71e746a":"class RNN_net(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(RNN_net, self).__init__()\n        self.hidden_size = hidden_size\n        self.rnn_cell = nn.RNN(input_size, hidden_size)\n        self.h2o = nn.Linear(hidden_size, output_size)\n        self.softmax = nn.LogSoftmax(dim=1)\n    \n    def forward(self, input_, hidden = None, batch_size = 1):\n        out, hidden = self.rnn_cell(input_, hidden)\n        output = self.h2o(hidden.view(-1, self.hidden_size))\n        output = self.softmax(output)\n        return output, hidden\n    \n    def init_hidden(self, batch_size = 1):\n        return torch.zeros(1, batch_size, self.hidden_size)","be447891":"n_hidden = 128\nnet = RNN_net(n_letters, n_hidden, n_languages)","65ab4bcd":"def infer(net, name, device = 'cpu'):\n    name_ohe = name_rep(name).to(device)\n    \n    output, hidden = net(name_ohe)\n        \n    if type(hidden) is tuple: # For LSTM\n        hidden = hidden[0]\n    index = torch.argmax(hidden)\n    \n    return output","9b315ba1":"# output = \ninfer(net, 'Adam')\n# index = torch.argmax(output)\n# print(output, index)","2d105239":"def dataloader(npoints, X_, y_):\n    to_ret = []\n    for i in range(npoints):\n        index_ = np.random.randint(len(X_))\n        name, lang = X_[index_], y_[index_]\n        to_ret.append((name, lang, name_rep(name), lang_rep(lang)))\n        \n    return to_ret","60c0a1cb":"dataloader(2, X_train, y_train)","a124b0d4":"def eval(net, n_points, topk, X_, y_, device = 'cpu'):\n    net = net.eval().to(device)\n    data_ = dataloader(n_points, X_, y_)\n    correct = 0\n    \n    for name, language, name_ohe, lang_rep in data_:\n        \n        output = infer(net, name, device)\n        val, indices = output.topk(topk)\n        indices = indices.to('cpu')\n        \n        if lang_rep in indices:\n            correct += 1\n            \n    accuracy = correct\/n_points\n    return accuracy","2a3c9a9c":"eval(net, 1000, 1, X_test, y_test)","6c3e351d":"def batched_name_rep(names, max_word_size):\n    rep = torch.zeros(max_word_size, len(names), n_letters)\n    for name_index, name in enumerate(names):\n        for letter_index, letter in enumerate(name):\n            pos = all_letters.find(letter)\n            rep[letter_index][name_index][pos] = 1\n    return rep","b4d6203c":"def print_char(name_reps):\n    name_reps = name_reps.view((-1, name_reps.size()[-1]))\n    for t in name_reps:\n        if torch.sum(t) == 0:\n            print('<pad>')\n        else:\n            index = t.argmax()\n            print(all_letters[index])","a23417e5":"out_ = batched_name_rep(['Shyam','Ram'],5)\nprint(out_)\nprint(out_.shape)\nprint_char(out_)","1e4ac255":"def batched_lang_rep(langs):\n    rep = torch.zeros([len(langs)], dtype=torch.long)\n    for index, lang in enumerate(langs):\n        rep[index] = languages.index(lang)\n    return rep","2e6caf2f":"def batched_dataloader(npoints, X_, y_, verbose=False, device = 'cpu'):\n    names = []\n    langs = []\n    X_lengths = []\n    \n    for i in range(npoints):\n        index_ = np.random.randint(len(X_))\n        name, lang = X_[index_], y_[index_]\n        X_lengths.append(len(name))\n        names.append(name)\n        langs.append(lang)\n    max_length = max(X_lengths)\n    \n    names_rep = batched_name_rep(names, max_length).to(device)\n    langs_rep = batched_lang_rep(langs).to(device)\n#     print(names_rep, langs_rep)\n    padded_names_rep = torch.nn.utils.rnn.pack_padded_sequence(names_rep, X_lengths, enforce_sorted= False)\n    \n    if verbose:\n        print(names_rep.shape, padded_names_rep.data.shape)\n        print('--')\n    \n    if verbose:\n        print(names)\n        print_char(names_rep)\n        print('--')\n        \n    if verbose:\n        print_char(padded_names_rep.data)\n        print('Lang Reg', langs_rep.data)\n        print('Batch sized', padded_names_rep.batch_sizes)\n        \n    return padded_names_rep.to(device), langs_rep","1e7c31ae":"p, l = batched_dataloader(3, X_train, y_train, True)","11dde934":"def train(net, opt, criterion, n_points):\n    \n    opt.zero_grad()\n    total_loss = 0\n    \n    data_ = dataloader(n_points, X_train, y_train)\n    \n    total_loss = 0\n    \n    for name, language, name_ohe, lang_rep in data_:\n\n        hidden = net.init_hidden()\n\n        for i in range(name_ohe.size()[0]):\n            output, hidden = net(name_ohe[i:i+1], hidden)\n            \n        loss = criterion(output, lang_rep)\n        loss.backward(retain_graph=True)\n        \n        total_loss += loss\n        \n    opt.step()       \n    return total_loss\/n_points","d07073a7":"def train_batch(net, opt, criterion, n_points, device = 'cpu'):\n    \n    net.train().to(device)\n    opt.zero_grad()\n    \n    batch_input, batch_groundtruth = batched_dataloader(n_points, X_train, y_train, False, device)\n    \n    output, hidden = net(batch_input)\n    \n    loss = criterion(output, batch_groundtruth)\n    \n    loss.backward()\n    opt.step()\n    return loss","68a3c300":"net = RNN_net(n_letters, n_hidden, n_languages)\ncriterion = nn.NLLLoss()\nopt = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)","a28264d6":"%%time \ntrain(net, opt, criterion, 256)","ad212dae":"net = RNN_net(n_letters, n_hidden, n_languages)\ncriterion = nn.NLLLoss()\nopt = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)","f6f5ab47":"%%time\ntrain_batch(net, opt, criterion, 256)","ca25cb71":"def train_setup(net, lr = 0.01, n_batches = 100, batch_size = 10, momentum = 0.9, display_freq=5, device = 'cpu'):\n    net = net.to(device)\n    criterion = nn.NLLLoss()\n    opt = optim.SGD(net.parameters(), lr=lr, momentum=momentum)\n    \n    loss_arr = np.zeros(n_batches + 1)\n    \n    for i in range(n_batches):\n        loss_arr[i+1] = (loss_arr[i]*i + train_batch(net, opt, criterion, batch_size, device))\/(i + 1)\n        \n        if i%display_freq == display_freq-1:\n            clear_output(wait=True)\n            \n            print('Iteration', i, 'Loss', loss_arr[i])\n            # print('Top-1:', eval(net, len(X_test), 1, X_test, y_test), 'Top-2:', eval(net, len(X_test), 2, X_test, y_test))\n            plt.figure()\n            plt.plot(loss_arr[1:i], '-*')\n            plt.xlabel('Iteration')\n            plt.ylabel('Loss')\n            plt.show()\n            print('\\n\\n')\n            \n    print('Top-1:', eval(net, len(X_test), 1, X_test, y_test, device), 'Top-2:', eval(net, len(X_test), 2, X_test, y_test, device))","9585d010":"%%time\nn_hidden = 128\nnet = RNN_net(n_letters, n_hidden, n_languages)\ntrain_setup(net, lr=0.15, n_batches=5000, batch_size= 512, display_freq=500) # CPU Training example","0c96329e":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","fbc42219":"%%time\nnet = RNN_net(n_letters, 128, n_languages)\ntrain_setup(net, lr= 0.15, n_batches=5000, batch_size = 512, display_freq=100, device=device) # GPU Training Example","2ab3b7f7":"class LSTM_net(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(LSTM_net, self).__init__()\n        self.hidden_size = hidden_size\n        self.lstm_cell = nn.LSTM(input_size, hidden_size)\n        self.h2o = nn.Linear(hidden_size, output_size)\n        self.softmax = nn.LogSoftmax(dim=1)\n    \n    def forward(self, input, hidden = None):\n        out, hidden = self.lstm_cell(input, hidden)\n        output = self.h2o(hidden[0].view(-1, self.hidden_size))\n        output = self.softmax(output)\n        return output, hidden\n    \n    def init_hidden(self, batch_size = 1):\n        return (torch.zeros(1, batch_size, self.hidden_size), torch.zeros(1, batch_size, self.hidden_size))","45b0e18d":"n_hidden = 128\nnet = LSTM_net(n_letters, n_hidden, n_languages)\ntrain_setup(net, lr=0.15, n_batches=8000, batch_size = 512, display_freq=1000, device = device)","86f38bc2":"class GRU_net(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(GRU_net, self).__init__()\n        self.hidden_size = hidden_size\n        self.gru_cell = nn.GRU(input_size, hidden_size)\n        self.h2o = nn.Linear(hidden_size, output_size)\n        self.softmax = nn.LogSoftmax(dim=1)\n    \n    def forward(self, input, hidden = None):\n        out, hidden = self.gru_cell(input, hidden)\n        output = self.h2o(hidden.view(-1, self.hidden_size))\n        output = self.softmax(output)\n        return output, hidden","a4c1e514":"%%time\nn_hidden = 128\nnet = GRU_net(n_letters, n_hidden, n_languages)\ntrain_setup(net, lr=0.15, n_batches=8000, batch_size = 512, display_freq=1000, device = device)","7547dd77":"The Recurrent neural networks are a class of artificial neural networks where the connection between nodes form a directed graph along a temporal sequence. Unlike the feed-forward neural networks, the recurrent neural networks use their internal state memory for processing sequences. This dynamic behavior of the Recurrent neural networks allows them to be very useful and applicable to audio analysis, handwritten recognition, and several such applications.\n#### Simple RNN implementation.\nMathematically the simple RNN can be formulated as follows:\n\nWhere x(t) and y(t) are the input and output vectors, W\u1d62, W, and W are the weight matrices and f and f are the hidden and output unit activation functions.","e95cd7f3":"### RNN Cell","7f08c9c2":"More reading stuff\n1. [here](https:\/\/stanford.edu\/~shervine\/teaching\/cs-230\/cheatsheet-recurrent-neural-networks).\n2. [here](http:\/\/medium.com\/analytics-vidhya\/rnn-vs-gru-vs-lstm-863b0b7b1573).","b68e342b":"![RNN](https:\/\/github.com\/taruntiwarihp\/raw_images\/blob\/master\/1_goJVQs-p9kgLODFNyhl9zA.gif?raw=true)\n### Outline \n1. Introduce dataset\n2. Data Processing - Test-Train split, encoding, visualisation\n3. Basic RNN - Testing inference \n4. Evaluation and training \n5. LSTM\n6. GRU\n7. Batching for sequence models\n8. Padding and Packing in PyTorch\n9. Training with batched dataset\n10. Comparison of performance with batching and on GPU","4bd46522":"<img src='https:\/\/github.com\/taruntiwarihp\/raw_images\/blob\/master\/Kuhh0.jpeg?raw=true'>","0bcf90bc":"### LSTM Cell","2c8df145":"### Basic Visualisation","af8f7d45":"### Training \n### Basic setup","39aecabe":"### Evaluate Model","7f7e6f6c":"### Encoding names and language","3c66eb36":"### Batching","f97e123c":"### Basic Network and testing inference","52dbd570":"### Train - Test split","762acdfe":"### GRU Cell","ba668225":"### Dataset","a03c87e4":"## Full Training Setup"}}