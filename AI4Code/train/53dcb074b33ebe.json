{"cell_type":{"78f6c611":"code","cc8d45e2":"code","ba380e7e":"code","6f2dc612":"code","c444c3fd":"code","59f127d4":"code","ee4f3280":"code","23f0d461":"code","29c82300":"code","de5744aa":"code","508cbffc":"code","538ac5e3":"code","8c4cb7c1":"code","e5895707":"code","20f12996":"code","eae7407d":"code","216c3f04":"code","16aaaf3c":"code","a2f3b0ce":"code","f0c27c14":"code","ea33dafc":"code","fd795d3a":"code","f45c1862":"code","0ed2b4f6":"code","64d4425d":"code","a7445b89":"code","c912de52":"code","af0bf0b8":"code","2c2bcbd3":"code","4e65a5e3":"code","fecf7e95":"code","c3eef8e1":"code","8bc4de35":"code","05d16caf":"code","369c30f0":"code","e72275b9":"code","9628382a":"code","052313a9":"code","2975740e":"code","28944661":"code","b558e12c":"code","69c392b3":"code","dd16f320":"code","7148768b":"code","140ae4b3":"code","903bf97d":"code","8cd815b9":"code","33fc9bf1":"code","7d59203d":"code","4770ae7e":"code","078e3852":"code","064eefbf":"code","42bb309f":"code","49bf43b0":"code","b5fb55ff":"code","d13cbbff":"markdown","5f5e3f25":"markdown","7833e3f6":"markdown","4cc711b4":"markdown","d928d0f7":"markdown","fd04ca19":"markdown","26e112c5":"markdown","a260b864":"markdown","45b5313f":"markdown","35ce4a16":"markdown","128f01a1":"markdown","9a9e780b":"markdown","90a7f963":"markdown","60583a9a":"markdown","7b0b9392":"markdown","b87492db":"markdown","7b323e4d":"markdown","0337c405":"markdown","a810eb6c":"markdown","3e84394e":"markdown","2b60c750":"markdown","6fd629ee":"markdown","46e7df5b":"markdown","8e095d39":"markdown","d94d8132":"markdown"},"source":{"78f6c611":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","cc8d45e2":"from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold, train_test_split, StratifiedKFold\nfrom xgboost import XGBClassifier","ba380e7e":"test = pd.read_csv('..\/input\/poker-hand-testing.data', header=None)","6f2dc612":"train = pd.read_csv('..\/input\/poker-hand-training-true.data', header=None)","c444c3fd":"train.columns = ['S1', 'C1','S2', 'C2','S3', 'C3','S4', 'C4','S5', 'C5','Label']","59f127d4":"test.columns = ['S1', 'C1','S2', 'C2','S3', 'C3','S4', 'C4','S5', 'C5','Label']","ee4f3280":"train.head()","23f0d461":"train.shape","29c82300":"test.shape","de5744aa":"X_train = train.loc[:,train.columns != 'Label']","508cbffc":"X_test = test.loc[:,test.columns != 'Label']","538ac5e3":"Y_train = train['Label']","8c4cb7c1":"Y_test = test['Label']","e5895707":"Y_train.groupby(Y_train).size()","20f12996":"Y_test.groupby(Y_test).size()","eae7407d":"def preprocess_data(data:pd.DataFrame):\n    df = data.copy()\n    dfc = df[['C1', 'C2', 'C3', 'C4', 'C5']]\n    dfc.values.sort()\n    df[['C1', 'C2', 'C3', 'C4', 'C5']] = dfc\n    df = df[['C1', 'C2', 'C3', 'C4', 'C5', 'S1', 'S2', 'S3', 'S4', 'S5', 'Label']]\n    return df","216c3f04":"def add_counts(df:pd.DataFrame):\n    tmp = df[['C1', 'C2', 'C3', 'C4', 'C5']]\n    df['Cnt_C1'] = tmp.apply(lambda x: sum(x==x[0]) ,axis=1)\n    df['Cnt_C2'] = tmp.apply(lambda x: sum(x==x[1]) ,axis=1)\n    df['Cnt_C3'] = tmp.apply(lambda x: sum(x==x[2]) ,axis=1)\n    df['Cnt_C4'] = tmp.apply(lambda x: sum(x==x[3]) ,axis=1)\n    df['Cnt_C5'] = tmp.apply(lambda x: sum(x==x[4]) ,axis=1)\n    \n    tmp = df[['S1', 'S2', 'S3', 'S4', 'S5']]\n    df['Cnt_S1'] = tmp.apply(lambda x: sum(x==x[0]) ,axis=1)\n    df['Cnt_S2'] = tmp.apply(lambda x: sum(x==x[1]) ,axis=1)\n    df['Cnt_S3'] = tmp.apply(lambda x: sum(x==x[2]) ,axis=1)\n    df['Cnt_S4'] = tmp.apply(lambda x: sum(x==x[3]) ,axis=1)    \n    df['Cnt_S5'] = tmp.apply(lambda x: sum(x==x[4]) ,axis=1)","16aaaf3c":"def add_diffs(df:pd.DataFrame):\n    df['Diff1'] = df['C5'] - df['C4']\n    df['Diff2'] = df['C4'] - df['C3']\n    df['Diff3'] = df['C3'] - df['C2']\n    df['Diff4'] = df['C2'] - df['C1']","a2f3b0ce":"def add_unique_count(df:pd.DataFrame):\n    tmp = df[['S1', 'S2', 'S3', 'S4', 'S5']]\n    df['UniqueS'] = tmp.apply(lambda x: len(np.unique(x)) , axis=1)","f0c27c14":"def cross_validation(alg, X_train, Y_train, folds=10):\n    kf = KFold(n_splits = folds, shuffle=True)\n\n    acc = []\n    matrix = None\n    first = True\n\n    i = 1\n    for train_index, test_index in kf.split(X_train, Y_train):\n        print('{}-Fold'.format(i))\n        fX_train, fX_test = X_train.iloc[train_index,:], X_train.iloc[test_index,:]\n        fy_train, fy_test = Y_train[train_index], Y_train[test_index]\n        alg.fit(fX_train, fy_train)\n        fy_pred = alg.predict(fX_test)\n        curr = accuracy_score(fy_test, fy_pred, normalize=True)\n        acc.append(curr)\n        i = i+1\n\n    acc = pd.Series(acc)\n    return acc.mean()","ea33dafc":"alg = DecisionTreeClassifier(random_state=1)","fd795d3a":"alg.fit(X_train, Y_train)","f45c1862":"y_pred = alg.predict(X_test)","0ed2b4f6":"accuracy_score(Y_test, y_pred, normalize=True)","64d4425d":"X_train_pre = preprocess_data(train)","a7445b89":"X_test_pre = preprocess_data(test)","c912de52":"X_train = X_train_pre.loc[:,X_train_pre.columns != 'Label']","af0bf0b8":"X_test = X_test_pre.loc[:,X_test_pre.columns != 'Label']","2c2bcbd3":"alg = DecisionTreeClassifier(random_state=1, criterion='gini')\ncross_validation(alg, X_train, Y_train)","4e65a5e3":"alg = DecisionTreeClassifier(random_state=1, criterion='gini')\nalg.fit(X_train, Y_train)\ny_pred = alg.predict(X_test)\naccuracy_score(Y_test, y_pred, normalize=True)","fecf7e95":"pd.crosstab(y_pred, Y_test, rownames=['Predicted'], colnames=['True'], margins=True)","c3eef8e1":"pred_series = pd.Series(y_pred).groupby(y_pred).size()\ntrue_series = pd.Series(Y_test.values).groupby(Y_test).size()\npred_res = pd.DataFrame()\npred_res['TrueLabel'] = true_series\npred_res['PredictedLabel'] = pred_series","8bc4de35":"pred_res","05d16caf":"f, ax = plt.subplots()\nax.set(yscale=\"log\")\nsns.barplot(data=pred_res.stack().reset_index().rename(columns={0: 'Count', 'level_1': 'Variable'}), x='Label', y='Count', hue='Variable')","369c30f0":"add_unique_count(X_test)","e72275b9":"add_unique_count(X_train)","9628382a":"alg = DecisionTreeClassifier(random_state=1, criterion='gini')\ncross_validation(alg, X_train, Y_train)","052313a9":"alg = DecisionTreeClassifier(random_state=1, criterion='gini')\nalg.fit(X_train, Y_train)\ny_pred = alg.predict(X_test)\naccuracy_score(Y_test, y_pred, normalize=True)","2975740e":"pred_series = pd.Series(y_pred).groupby(y_pred).size()\ntrue_series = pd.Series(Y_test.values).groupby(Y_test).size()\npred_res = pd.DataFrame()\npred_res['TrueLabel'] = true_series\npred_res['PredictedLabel'] = pred_series\nf, ax = plt.subplots()\nax.set(yscale=\"log\")\nsns.barplot(data=pred_res.stack().reset_index().rename(columns={0: 'Count', 'level_1': 'Variable'}), x='Label', y='Count', hue='Variable')","28944661":"add_diffs(X_train)","b558e12c":"add_diffs(X_test)","69c392b3":"alg = DecisionTreeClassifier(random_state=1, criterion='gini')\ncross_validation(alg, X_train, Y_train)","dd16f320":"alg = DecisionTreeClassifier(random_state=1, criterion='gini')\nalg.fit(X_train, Y_train)\ny_pred = alg.predict(X_test)\naccuracy_score(Y_test, y_pred, normalize=True)","7148768b":"pred_series = pd.Series(y_pred).groupby(y_pred).size()\ntrue_series = pd.Series(Y_test.values).groupby(Y_test).size()\npred_res = pd.DataFrame()\npred_res['TrueLabel'] = true_series\npred_res['PredictedLabel'] = pred_series\nf, ax = plt.subplots()\nax.set(yscale=\"log\")\nsns.barplot(data=pred_res.stack().reset_index().rename(columns={0: 'Count', 'level_1': 'Variable'}), x='Label', y='Count', hue='Variable')","140ae4b3":"pd.crosstab(y_pred, Y_test, rownames=['Predicted'], colnames=['True'], margins=True)","903bf97d":"alg = RandomForestClassifier(criterion='gini', n_estimators=10, random_state=111, n_jobs=4)\ncross_validation(alg, X_train, Y_train)","8cd815b9":"alg = RandomForestClassifier(criterion='entropy', n_estimators=51, random_state=111, n_jobs=4)\ncross_validation(alg, X_train, Y_train)","33fc9bf1":"alg = GradientBoostingClassifier(n_estimators=10, random_state=111)\ncross_validation(alg, X_train, Y_train)","7d59203d":"alg = XGBClassifier(n_estimators=10, random_state=111)\ncross_validation(alg, X_train, Y_train)","4770ae7e":"alg = DecisionTreeClassifier(criterion='gini', random_state=111)","078e3852":"alg.fit(X_train, Y_train)","064eefbf":"y_pred = alg.predict(X_test)","42bb309f":"accuracy_score(y_pred=y_pred, y_true=Y_test, normalize=True)","49bf43b0":"feature_imp = pd.DataFrame(sorted(zip(X_train.columns, alg.feature_importances_), key=lambda k: k[1], reverse=True))\nfeature_imp.columns = ['Feature', 'Importance']","b5fb55ff":"f, ax = plt.subplots(figsize=(10, 7))\n# ax.set(yscale=\"log\")\nplt.xticks(rotation=45)\nsns.barplot(data=feature_imp, x='Feature', y='Importance')","d13cbbff":"# Load data\nLet's load data to Pandas dataframes and take a firts look at it.","5f5e3f25":"Random forest performs better with more estimators and entropy as split criterion, but decision tree is still better.","7833e3f6":"That's all folks!","4cc711b4":"# First try with Decision tree classifier","d928d0f7":"96% accuracy is pretty neat, we are almost two times better after sorting cards in hands.\n\nNow we will take a look at cases where our classifier performs poorly.","fd04ca19":"# Test data evaluation\n\nLet's do final test data evaluation with Decision tree classifier which had best accuracy among all tested classifiers.","26e112c5":"# Label distribution in train set","a260b864":"It's clear from the feature importance plot that classifier almost exclusively uses newly engineered features and original features like suites","45b5313f":"## Random forest 10-fold CV","35ce4a16":"# Feature importances","128f01a1":"CV accuracy is slightly better, although straight flushes get under-predicted, but problem with flushes is solved.\n\nNow we will add new features with values of differences between consecutive cards in hand.","9a9e780b":"# Label distribution in test set","90a7f963":"# 10-fold CV","60583a9a":"Your feedback is highly appreciated :)","7b0b9392":"There is clearly problem with prediction of flushes and straight\/royal flushes. We add new feature called *UniqueS* which contains number of unique suites of cards in hand.","b87492db":"Let's give it another try.","7b323e4d":"## Extreme Gradient Boosting Classifier 10-fold CV","0337c405":"That's pretty low accuracy for a purely deterministic task as this one.","a810eb6c":"# Extract features and labels","3e84394e":"Now we have really great accuracy of 99.99%, it's clear that out model somehow badly predicts straight flushes, but the rest is ok.\n\nDecision tree performed better than expected, but we can try other classifiers as well.","2b60c750":"# Preprocess data","6fd629ee":"# UCI Poker dataset classification with Pandas, Matplotlib and Scikit-learn\n\n***\n\nWe will be using UCI Poker dataset in this kernel.\n\nStart by importing needed packages.","46e7df5b":"## Gradient Boosting Classifier 10-fold CV","8e095d39":"## Sorting values\nIf we take a look at poker hands in this dataset, we will see that cards in these hands are out of order.\n\nFirst step at preprocessing is sorting cards in each hand, we will use function *preprocess_data* defined above.","d94d8132":"# Define helper functions"}}