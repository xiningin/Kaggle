{"cell_type":{"a5f98806":"code","6919c4fb":"code","caedfec9":"code","e490cd8c":"code","70314c8c":"code","b356de1e":"code","e218efd8":"code","89ba6bac":"code","bb40db99":"code","9f00aec8":"code","ac533764":"code","d0e1aae1":"code","eb94f014":"code","e1f0f39c":"code","b6471e17":"code","d2c30540":"code","49cf62f3":"code","be1a61f5":"code","97348105":"code","466c2aae":"code","d491a2a0":"code","a3a32275":"code","a124eca2":"code","1e867374":"code","d3e0590d":"code","a4d57ada":"code","169a9d91":"code","2291437c":"code","e47c15e5":"code","651269e6":"code","682a6eb2":"code","21d86d9c":"code","e4be799d":"code","9e5fdf68":"code","68c121c7":"code","d95953f3":"code","6e762187":"code","388b29b1":"code","9a8cf5af":"code","7e2bc658":"code","7cd42bea":"code","b82986ed":"code","611c92d7":"code","cb9300bf":"code","c585c4d9":"code","05e3f7cb":"code","f621ba79":"code","47f92c89":"code","c9b7b5ff":"code","886877eb":"code","91a95419":"code","1f87a54d":"code","bf988f10":"code","0f35c8bb":"code","121a6804":"code","c62e7210":"code","0bf0cb32":"code","ddc5866a":"code","f481cbe8":"code","502f7309":"code","8e682556":"code","601d9061":"code","e15e5d53":"code","4ddd450a":"code","c438700f":"code","a1b100c1":"code","908764dd":"code","4d64f71c":"code","b4e9973c":"code","f5c12b70":"code","17d0af6b":"code","ae486b9f":"code","b4014e74":"code","21e23021":"code","cf98c512":"code","db276c26":"code","fb679703":"code","b68d5169":"code","93594195":"code","a846c6d0":"code","6a5eb916":"code","62a38310":"code","7e1bb6f7":"code","b3397f97":"code","040bc1a1":"code","b71e7972":"code","e91b4ac8":"code","126f4a67":"code","2e4e0e53":"code","7f6bd672":"code","e18c3f95":"code","01de8737":"code","2363722d":"code","9e9b1250":"code","b8109695":"code","393398c4":"code","bf4bd4bc":"code","7e80fac8":"code","14922870":"code","833390cd":"code","d38d374e":"code","5f72135b":"code","367ed516":"code","6a81fb92":"code","9c500fdd":"code","ef8ca2d7":"code","4619ba00":"code","874b42df":"code","7989c9ba":"code","b53b998b":"code","a7996d90":"code","2a7ae8c0":"code","72c4f1f0":"code","91e7a282":"code","17628385":"code","1bc07f72":"code","9acc185c":"code","226242ef":"code","21ad697b":"code","3cb9809d":"code","3e47c9ac":"code","7406dab6":"code","29af2c75":"code","bd377ce4":"code","31d0e027":"code","d5147afc":"code","0336bce1":"code","0c20e9f6":"code","6c32f2f8":"code","eea10ec8":"code","35489a16":"code","21150ebc":"code","b9aa1cb8":"code","8cb5476a":"markdown","89818420":"markdown","45697582":"markdown","39b5eabc":"markdown","512e1d94":"markdown","8e383cad":"markdown","896aadcd":"markdown","36f9fc10":"markdown","c5b49d86":"markdown","4b2c32c7":"markdown","cfeb0690":"markdown","7239d6c3":"markdown","7bf543b8":"markdown","29b28489":"markdown","d14944cc":"markdown","8e2b3d1a":"markdown","ba3a334d":"markdown","044024ef":"markdown","f82acf0b":"markdown","95637e23":"markdown","e6ba22a9":"markdown","6a73835c":"markdown","bdf86681":"markdown","a2518013":"markdown","e0034734":"markdown","65d2e252":"markdown","d3ddd39b":"markdown","9a09c215":"markdown","4e050b6e":"markdown","082b29d1":"markdown","e3b42989":"markdown","794e2e96":"markdown","d9bf19fb":"markdown","8916d212":"markdown","add51ac9":"markdown","e26e8a80":"markdown","f4f14f6a":"markdown","f4050984":"markdown","0c06ed4c":"markdown","c36d9c29":"markdown"},"source":{"a5f98806":"%matplotlib inline\n#pip install folium\nimport seaborn as sns\nimport statsmodels.formula.api as smf\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\nfrom sklearn.model_selection  import train_test_split\nimport numpy as np\nfrom scipy.stats import norm # for scientific Computing\nfrom scipy import stats, integrate\nimport matplotlib.pyplot as plt\n\n#from sklearn.datasets import fetch_mldata\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics\n\nimport pandas as pd","6919c4fb":"# Any results you write to the current directory are saved as output.\n%time KSI_CLEAN=pd.read_csv('\/kaggle\/input\/killed-or-seriously-injured-ksi-toronto-clean\/KSI_CLEAN.csv')","caedfec9":"#missing values\nKSI_CLEAN = KSI_CLEAN.replace(' ', np.nan, regex=False)\n#printing percentage of missing values for each feature\nprint(KSI_CLEAN.isna().sum()\/len(KSI_CLEAN)*100)","e490cd8c":"fig, ax = plt.subplots(figsize=(15,7))\n#heatmap to visualize features with most missing values\nsns.heatmap(KSI_CLEAN.isnull(), yticklabels=False,cmap='Greens')","70314c8c":"KSI_CLEAN.shape","b356de1e":"## Dropping columns where missing values were greater than 80%\nKSI_CLEAN = KSI_CLEAN.drop([\"PEDTYPE\", \"PEDACT\", \"PEDCOND\", \"CYCLISTYPE\", \"CYCACT\", \"CYCCOND\", \"OFFSET\"], axis=1)\nKSI_CLEAN.shape","e218efd8":"\nKSI_CLEAN['ACCLASS'] = np.where(KSI_CLEAN['ACCLASS'] == 'Property Damage Only', 'Non-Fatal', KSI_CLEAN['ACCLASS'])\nKSI_CLEAN['ACCLASS'] = np.where(KSI_CLEAN['ACCLASS'] == 'Non-Fatal Injury', 'Non-Fatal', KSI_CLEAN['ACCLASS'])\nKSI_CLEAN.ACCLASS.unique()","89ba6bac":"# Verifying columns with object data type\nprint(KSI_CLEAN.select_dtypes([\"object\"]).columns)","bb40db99":"##changing all object data types to category \nobjdtype_cols = KSI_CLEAN.select_dtypes([\"object\"]).columns\nKSI_CLEAN[objdtype_cols] = KSI_CLEAN[objdtype_cols].astype('category')","9f00aec8":"KSI_CLEAN.info()","ac533764":"#Number of Unique accidents by Year\nNum_accident = KSI_CLEAN.groupby('YEAR')['ACCNUM'].nunique()\nplt.figure(figsize=(12,6))\nplt.title(\"Accidents caused in different years\")\nplt.ylabel('Number of Accidents (ACCNUM)')\n\nax = plt.gca()\nax.tick_params(axis='x', colors='blue')\nax.tick_params(axis='y', colors='red')\nmy_colors = list('rgbkymc')   #red, green, blue, black, etc.\nNum_accident.plot(\n    kind='bar', \n    color='blue',\n    edgecolor='black'\n)\n#Num_accident.plot(kind='bar',color= my_colors)\nplt.show()\n  ","d0e1aae1":"#Number of Unique accidents by Year\nNum_accident = KSI_CLEAN.groupby('MONTH')['ACCNUM'].nunique()\nplt.figure(figsize=(12,6))\nplt.title(\"Accidents caused in different months\")\nplt.ylabel('Number of Accidents (ACCNUM)')\n\n\n\nax = plt.gca()\nax.tick_params(axis='x', colors='blue')\nax.tick_params(axis='y', colors='red')\nmy_colors = list('rgbkymc')   #red, green, blue, black, etc.\nNum_accident.plot(\n    kind='bar', \n    color='blue',\n    edgecolor='black'\n)\n#Num_accident.plot(kind='bar',color= my_colors)\nplt.show()","eb94f014":"import statsmodels.formula.api as smf\nfrom sklearn import metrics\nfrom sklearn import neighbors\nfrom sklearn.metrics import mean_squared_error \nfrom sklearn import tree, metrics\nfrom scipy.stats import norm \nfrom scipy import stats, integrate\nimport matplotlib.pyplot as plt\nfrom IPython.display import HTML\nimport folium\nfrom folium.plugins import HeatMap\nfrom math import sqrt","e1f0f39c":"\nKSI_Fatal = KSI_CLEAN[KSI_CLEAN['INJURY'] == 'Fatal']\nKSI_Fatal = KSI_Fatal[['LATITUDE', 'LONGITUDE', 'FATAL']]\nlat_Toronto = KSI_CLEAN.describe().at['mean','LATITUDE']\nlng_Toronto = KSI_CLEAN.describe().at['mean','LONGITUDE']\n#Fatal_map = folium.Map(location = [lat_Toronto, lng_Toronto], zoom_start=5)\nToronto_location = [lat_Toronto, lng_Toronto]\n#Toronto_location = [43.6532, -79.3832]\n\nFatal_map = folium.Map(Toronto_location, zoom_start=10.255)\nHeatMap(KSI_Fatal.values, min_opacity =0.3).add_to(Fatal_map)\nFatal_map","b6471e17":"#Categorizing Fatal vs. non-Fatal Incident (non-unique i.e: one accident is counted depending upon involved parties)\n\nsns.catplot(x='YEAR', kind='count', data=KSI_CLEAN,  hue='ACCLASS')","d2c30540":"#Categorizing Fatal vs. non-Fatal Incident (non-unique i.e: one accident is counted depending upon involved parties)\n\nsns.catplot(x='YEAR', kind='count', data=KSI_CLEAN,  hue='FATAL')","49cf62f3":"#Lets look at Fatality over years (# of people died)\nFatality = KSI_CLEAN[KSI_CLEAN['INJURY'] =='Fatal']\nFatality = Fatality.groupby(KSI_CLEAN['YEAR']).count()\nplt.figure(figsize=(12,6))\n\n\nplt.ylabel('Number of Injury=Fatal')\nFatality['INJURY'].plot(kind='bar',color=\"blue\" , edgecolor='black')\n\nplt.show()\n","be1a61f5":"\n#Lets look at Fatality over years (# of people died)\nplt.figure(figsize=(12,6))\nFatal = KSI_CLEAN[KSI_CLEAN['FATAL'] ==1]\nFatal = Fatal.groupby(KSI_CLEAN['YEAR']).count()\nACCFatal=KSI_CLEAN[KSI_CLEAN['ACCLASS'] =='Fatal']\nACCFatal = ACCFatal.groupby(KSI_CLEAN['YEAR']).count()\n\n# multiple line plot\n# multiple line plot\nplt.plot( 'YEAR', 'FATAL', data=Fatal, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=7)\nplt.plot( 'YEAR', 'ACCLASS', data=ACCFatal, marker='*', color='black', linewidth=2)\n\nplt.legend()\n#ACCCLASS=FATAL","97348105":"#Looking at area where accident happens\n\nRegion_KSI_CLEAN = KSI_CLEAN['District'].value_counts()\nplt.figure(figsize=(12,6))\nplt.ylabel('Number of Accidents')\nRegion_KSI_CLEAN.plot(kind='bar',color=list('rgbkmc') )\nplt.show()","466c2aae":"Hood_KSI_CLEAN = KSI_CLEAN['Hood_Name'].value_counts()\nplt.figure(figsize=(12,6))\nplt.ylabel('Number of Accidents')\nHood_KSI_CLEAN.nlargest(10).plot(kind='bar',color=list('rgbkmc') )\nplt.show()","d491a2a0":"Weekday_KSI_CLEAN = KSI_CLEAN['WEEKDAY'].value_counts()\nplt.figure(figsize=(12,6))\nplt.ylabel('Number of Accidents')\nWeekday_KSI_CLEAN.plot(kind='bar',color=list('rgbkmc') )\nplt.show()","a3a32275":"## Driving condition VS accident #\n## creating a pivot table for accidents causing by 'SPEEDING', 'AG_DRIV', 'REDLIGHT', 'ALCOHOL'  for EDA.\nKSI_pivot_cause = KSI_CLEAN.pivot_table(index='YEAR', \n                           values = ['SPEEDING', 'AG_DRIV', 'REDLIGHT', 'ALCOHOL'],\n                           aggfunc=np.sum,\n                           margins = True,\n                           margins_name = 'Total Under Category')\nfig, ax1 = plt.subplots(figsize=(8,8))\nKSI_pivot_cause.iloc[11].plot(kind='pie', ax=ax1, autopct='%3.1f%%',fontsize=10)\nax1.set_ylabel('')\nax1.set_xlabel('Driving condition VS Accidents in Ontario in last 10 years(%age)',fontsize=20)","a124eca2":"## creating a pivot table for accidents causing by 'SPEEDING', 'AG_DRIV', 'REDLIGHT', 'ALCOHOL'  in 10 years\nKSI_pivot_cause.drop('Total Under Category', axis=0, inplace=True)\nfig, ax1 = plt.subplots(figsize=(12,5))\nKSI_pivot_cause.plot(kind='bar', ax=ax1, xticks=KSI_pivot_cause.index)\nplt.legend(bbox_to_anchor=(1.01, 1), loc='upper left')","1e867374":"#Causes for Fatal in line graph\nCause_Fatal = KSI_CLEAN.pivot_table(index='YEAR', margins=False ,values=['ALCOHOL', 'AG_DRIV', 'SPEEDING','REDLIGHT','DISABILITY'],aggfunc=np.sum)\nCause_Fatal.plot(figsize=(10,8), title=\"Causes for accidents\", grid=True)\nplt.ylabel('Accidents')","d3e0590d":"## vechile type VS accident #\n## creating a pivot table for accidents causing by 'AUTOMOBILE', 'MOTORCYCLE', 'TRUCK', 'TRSN_CITY_VEH', 'EMERG_VEH'   in 10 years\nKSI_pivot_Types = KSI_CLEAN.pivot_table(index='YEAR', \n                           values = [ 'AUTOMOBILE', 'MOTORCYCLE', 'TRUCK', 'TRSN_CITY_VEH', 'EMERG_VEH' ],\n                           aggfunc=np.sum,\n                           margins = True,\n                           margins_name = 'Total Under Category')\n\nfig, ax1 = plt.subplots(figsize=(8,8))\nKSI_pivot_Types.iloc[11].plot(kind='pie', ax=ax1, autopct='%3.1f%%',fontsize=10)\nax1.set_ylabel('')\nax1.set_xlabel('Vechile type VS Accidents in Ontario in last 10 years(%age)',fontsize=20)","a4d57ada":"KSI_pivot_Types.drop('Total Under Category', axis=0, inplace=True)\nfig, ax1 = plt.subplots(figsize=(12,5))\nKSI_pivot_Types.plot(kind='bar', ax=ax1, xticks=KSI_pivot_cause.index)\nplt.legend(bbox_to_anchor=(1.01, 1), loc='upper left')","169a9d91":"#Type of vehicles involved\nVeh_involved = KSI_CLEAN.pivot_table(index='YEAR',values=['AUTOMOBILE', 'CYCLIST', 'EMERG_VEH', 'MOTORCYCLE', 'TRUCK'],aggfunc=np.sum)\nVeh_involved.plot(figsize=(10,8), title=\"Type of Vehicle Involved\", grid=True)\nplt.ylabel('Vehicles')","2291437c":"## Victims VS accident #\n## creating a pivot table for Victims by 'CYCLIST','PEDESTRIAN','PASSENGER' \nKSI_pivot_CPP = KSI_CLEAN.pivot_table(index='YEAR', \n                           values = [ 'CYCLIST','PEDESTRIAN','PASSENGER' ],\n                           aggfunc=np.sum,\n                           margins = True,\n                           margins_name = 'Total Under Category')\nfig, ax1 = plt.subplots(figsize=(8,8))\nKSI_pivot_CPP.iloc[11].plot(kind='pie', ax=ax1, autopct='%3.1f%%',fontsize=10)\nax1.set_ylabel('')\nax1.set_xlabel('Victims VS Accidents in Ontario in last 10 years(%age)',fontsize=20)","e47c15e5":"## Fatal and Disability VS accident #\n## creating a pivot table for 'FATAL','DISABILITY' against accidents #\nKSI_pivot_DF = KSI_CLEAN.pivot_table(index='YEAR', \n                           values = [ 'FATAL','DISABILITY' ],\n                           aggfunc=np.sum,\n                           margins = True,\n                           margins_name = 'Total Under Category')\nfig, ax1 = plt.subplots(figsize=(8,8))\nKSI_pivot_DF.iloc[11].plot(kind='pie', ax=ax1, autopct='%3.1f%%',fontsize=10)\nax1.set_ylabel('')\nax1.set_xlabel('Total Accidents in Ontario in last 10 years(%age)',fontsize=20)","651269e6":"## creating a pivot table for 'FATAL','DISABILITY' against accidents # in 10 years\nKSI_pivot_DF.drop('Total Under Category', axis=0, inplace=True)\nfig, ax1 = plt.subplots(figsize=(12,5))\nKSI_pivot_DF.plot(kind='bar', ax=ax1, xticks=KSI_pivot_cause.index)\nplt.legend(bbox_to_anchor=(1.01, 1), loc='upper left')","682a6eb2":"data = KSI_CLEAN.groupby(by=['YEAR', 'MONTH'],as_index=False).sum()\ndata = data.pivot('MONTH','YEAR','FATAL')\ndata","21d86d9c":"plt.figure(figsize=(12,6))\nsns.heatmap(data, center=data.loc[1, 2007], annot=True, fmt=\"d\", cmap=\"YlGnBu\")\nplt.show()","e4be799d":"KSI_CLEAN.shape","9e5fdf68":"KSI_CLEAN.columns","68c121c7":"KSI_CLEAN.dtypes","d95953f3":"KSI_CLEAN_data=KSI_CLEAN[['ACCNUM', 'YEAR', 'MONTH', 'DAY', 'HOUR', 'MINUTES', 'WEEKDAY',\n       'LATITUDE', 'LONGITUDE',  'Hood_ID',\n        'District',  \n         'VISIBILITY', 'LIGHT', 'RDSFCOND', \n        'PEDESTRIAN', 'CYCLIST',\n       'AUTOMOBILE', 'MOTORCYCLE', 'TRUCK', 'TRSN_CITY_VEH', 'EMERG_VEH',\n       'PASSENGER', 'SPEEDING', 'AG_DRIV', 'REDLIGHT', 'ALCOHOL', 'DISABILITY']]","6e762187":"KSI_CLEAN_data.dtypes","388b29b1":"KSI_CLEAN_data['LATITUDE']=KSI_CLEAN_data['LATITUDE'].astype('int')\nKSI_CLEAN_data['LONGITUDE']=KSI_CLEAN_data['LATITUDE'].astype('int')","9a8cf5af":"print(\"Percentage of missing values in the KSI_CLEAN_data dataset\")\nKSI_CLEAN_data.isna().sum()\/len(KSI_CLEAN_data)*100","7e2bc658":"KSI_CLEAN_data['District'].unique()","7cd42bea":"KSI_CLEAN_data['VISIBILITY'].unique()","b82986ed":"KSI_CLEAN_data['LIGHT'].unique()","611c92d7":"KSI_CLEAN_data['RDSFCOND'].unique()","cb9300bf":"\nKSI_CLEAN_data = pd.get_dummies(KSI_CLEAN_data, columns=['VISIBILITY','RDSFCOND','LIGHT','District'])","c585c4d9":"KSI_CLEAN_data.shape","05e3f7cb":"\nKSI_CLEAN_target=KSI_CLEAN[[ 'FATAL']]\nKSI_CLEAN_data.dtypes","f621ba79":"data = KSI_CLEAN\n\nX = data.iloc[:,0:48]  #independent columns\ncols=[\"ACCNUM\",\"DAY\",\"LATITUDE\",\"LONGITUDE\",\"Hood_ID\",\"STREET1\",\"STREET2\",\"ROAD_CLASS\",\"LOCCOORD\",\"LIGHT\",\"RDSFCOND\",\"ACCLASS\",\"IMPACTYPE\",\"INVTYPE\",\"INVAGE\",\"INJURY\",\"FATAL_NO\",\"INITDIR\",\"MANOEUVER\",\"DRIVCOND\",\"PEDESTRIAN\",\"CYCLIST\",\"PASSENGER\",\"HOUR\",\"MINUTES\",\"YEAR\",\"ACCLOC\",\"WEEKDAY\",\"Hood_Name\",\"Ward_Name\",\"Ward_ID\",\"Division\",\"District\",\"MONTH\",\"VEHTYPE\"]\nX=X.drop(columns=cols)\nX1=X\nX1[\"Hood_Name\"]=KSI_CLEAN['Hood_Name']\nX1[\"District\"]=KSI_CLEAN['District']\nprint(X1.shape)\nprint(X.info())\nX = pd.get_dummies(X)\ny = data.iloc[:,-1]    #target column i.e price range\nfrom sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt\nmodel = ExtraTreesClassifier()\nmodel.fit(X,y)\n #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.show()\n","47f92c89":"drop_colmns = ['ACCNUM', 'YEAR', 'MONTH', 'DAY', 'HOUR', 'MINUTES', 'WEEKDAY',\n       'LATITUDE', 'LONGITUDE', 'Ward_Name', 'Ward_ID', 'Hood_ID',\n       'Division',  'STREET1', 'STREET2', 'ROAD_CLASS',\n       'LOCCOORD', 'ACCLOC', 'TRAFFCTL', 'VISIBILITY', 'LIGHT', 'RDSFCOND',\n       'ACCLASS', 'IMPACTYPE', 'INVTYPE', 'INVAGE', 'INJURY', 'FATAL_NO',\n       'INITDIR', 'VEHTYPE', 'MANOEUVER', 'DRIVACT', 'DRIVCOND',  'PEDESTRIAN',\n       'CYCLIST', 'MOTORCYCLE', 'TRUCK', 'TRSN_CITY_VEH',\n       'EMERG_VEH', 'PASSENGER', 'AUTOMOBILE']\ndf1 = KSI_CLEAN.drop(columns=drop_colmns)","c9b7b5ff":"df1_g2=df1.groupby(['Hood_Name','SPEEDING']).size().to_frame('count').reset_index()\ndf1speed = df1_g2.pivot(index='Hood_Name',columns='SPEEDING',values='count')\nprint(df1speed)\ndf1_g2=df1.groupby(['Hood_Name','AG_DRIV']).size().to_frame('count').reset_index()\ndf1agdriv = df1_g2.pivot(index='Hood_Name',columns='AG_DRIV',values='count')\ndf1_g2=df1.groupby(['Hood_Name','REDLIGHT']).size().to_frame('count').reset_index()\ndf1red = df1_g2.pivot(index='Hood_Name',columns='REDLIGHT',values='count')\ndf1_g2=df1.groupby(['Hood_Name','ALCOHOL']).size().to_frame('count').reset_index()\ndf1alco = df1_g2.pivot(index='Hood_Name',columns='ALCOHOL',values='count')\ndf1_g2=df1.groupby(['Hood_Name','DISABILITY']).size().to_frame('count').reset_index()\ndf1disb = df1_g2.pivot(index='Hood_Name',columns='DISABILITY',values='count')\ndf1speed = df1speed.drop(df1speed.columns[0], axis=1)\ndf1speed[2] = df1agdriv.drop(df1agdriv.columns[0], axis=1)\ndf1speed[3] = df1red.drop(df1red.columns[0], axis=1)\ndf1speed[4] = df1alco.drop(df1alco.columns[0], axis=1)\ndf1speed[5] = df1disb.drop(df1alco.columns[0], axis=1)\ndf1speed.columns.names = ['Cause'] \ndf1 = df1speed\ndf1 = df1.dropna()\ndf1.columns = ['SPEEDING', 'AG_DRIV', 'REDLIGHT', 'ALCOHOL','DISABILITY']\n","886877eb":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport matplotlib","91a95419":"scaler = StandardScaler()\nSum_of_squared_distances = []\nstd_scale = scaler.fit(df1)\nprint(df1.shape)\ndf_transformed = std_scale.transform(df1)\npca = PCA(n_components=3)\npca = pca.fit(df_transformed)\nX = pca.transform(df_transformed)\nK = range(1,15)\nfor k in K:\n    km = KMeans(n_clusters=k)\n    km = km.fit(df_transformed)\n    Sum_of_squared_distances.append(km.inertia_)","1f87a54d":"plt.plot(K, Sum_of_squared_distances, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Sum_of_squared_distances')\nplt.title('Elbow Method For Deciding Optimal k')\nplt.show()","bf988f10":"# import KMeans\nfrom sklearn.cluster import KMeans","0f35c8bb":"KSI_CLEAN_data_cluster=KSI_CLEAN[['ACCNUM', 'YEAR', 'MONTH', 'DAY', 'HOUR', 'MINUTES', 'WEEKDAY',\n        'Hood_ID',\n        'District',  \n         'VISIBILITY', 'LIGHT', 'RDSFCOND', \n        'PEDESTRIAN', 'CYCLIST',\n       'AUTOMOBILE', 'MOTORCYCLE', 'TRUCK', 'TRSN_CITY_VEH', 'EMERG_VEH',\n       'PASSENGER', 'SPEEDING', 'AG_DRIV', 'REDLIGHT', 'ALCOHOL', 'DISABILITY','FATAL']]","121a6804":"\n\nKSI_CLEAN_data_cluster= pd.get_dummies(KSI_CLEAN_data_cluster, columns=['VISIBILITY','RDSFCOND','LIGHT','District'])","c62e7210":"KSI_CLEAN_data_cluster.shape","0bf0cb32":"# create kmeans object\nkmeans = KMeans(n_clusters=4)\n# fit kmeans object to data\nkmeans.fit(X)\n# print location of clusters learned by kmeans object\n#cluster_center=pd.DataFrame(kmeans.cluster_centers_,columns=X.columns)\n\n# save new clusters for chart\n#y_km = kmeans.fit_predict(X)","ddc5866a":"#cluster_center.describe()","f481cbe8":"drop_colmns = ['ACCNUM', 'YEAR', 'MONTH', 'DAY', 'HOUR', 'MINUTES', 'WEEKDAY',\n       'LATITUDE', 'LONGITUDE', 'Ward_Name', 'Ward_ID', 'Hood_ID',\n       'Division', 'Hood_Name', 'STREET1', 'STREET2', 'ROAD_CLASS',\n       'LOCCOORD', 'ACCLOC', 'TRAFFCTL', 'VISIBILITY', 'LIGHT', 'RDSFCOND',\n       'ACCLASS', 'IMPACTYPE', 'INVTYPE', 'INVAGE', 'INJURY', 'FATAL_NO',\n       'INITDIR', 'VEHTYPE', 'MANOEUVER', 'DRIVACT', 'DRIVCOND',  'PEDESTRIAN',\n       'CYCLIST', 'MOTORCYCLE', 'TRUCK', 'TRSN_CITY_VEH',\n       'EMERG_VEH', 'PASSENGER', 'AUTOMOBILE']\ndk_dropped = KSI_CLEAN.drop(columns=drop_colmns)\ndk = dk_dropped[dk_dropped['FATAL']==1]\ndk.columns","502f7309":"dk_g21=X1.groupby(['District','SPEEDING']).size().to_frame('count').reset_index()\ndkspeed1 = dk_g21.pivot(index='District',columns='SPEEDING',values='count')\nprint(dkspeed1)\ndk_g21=X1.groupby(['District','AG_DRIV']).size().to_frame('count').reset_index()\ndkagdriv1 = dk_g21.pivot(index='District',columns='AG_DRIV',values='count')\n\ndk_g21=X1.groupby(['District','REDLIGHT']).size().to_frame('count').reset_index()\ndfred1 = dk_g21.pivot(index='District',columns='REDLIGHT',values='count')\n\ndk_g21=X1.groupby(['District','ALCOHOL']).size().to_frame('count').reset_index()\ndkalco1 = dk_g21.pivot(index='District',columns='ALCOHOL',values='count')\n\ndk_g21=X1.groupby(['District','DISABILITY']).size().to_frame('count').reset_index()\ndkdisb1 = dk_g21.pivot(index='District',columns='DISABILITY',values='count')\n\ndk_g21=X1.groupby(['District','TRUCK']).size().to_frame('count').reset_index()\ndktruck1 = dk_g21.pivot(index='District',columns='TRUCK',values='count')\n\n\ndk_g21=X1.groupby(['District','TRSN_CITY_VEH']).size().to_frame('count').reset_index()\ndktrsn1 = dk_g21.pivot(index='District',columns='TRSN_CITY_VEH',values='count')\n\n\ndk_g21=X1.groupby(['District','AUTOMOBILE']).size().to_frame('count').reset_index()\ndkauto1 = dk_g21.pivot(index='District',columns='AUTOMOBILE',values='count')\n\n\n\ndknew = dkspeed1\nprint(dknew)\n\n\ndknew = dknew.dropna()\n\n#dknew.columns = ['SPEEDING', 'AG_DRIV','TRUCK','AUTOMOBILE','TRSN_CITY_VEH','ALCOHOL','REDLIGHT']\n\ndknew[0]= dkagdriv1[1]\ndknew['TRUCK']=dktruck1[1]\ndknew['AUTOMOBILE']=dkauto1[1]\ndknew['TRSN_CITY_VEH']=dktrsn1[1]\ndknew['ALCOHOL']=dkalco1[1]\ndknew['REDLIGHT']=dfred1[1]\ndknew\nprint(dknew)\ndknew.columns.names = ['Cause'] \ndknew.columns = ['SPEEDING', 'AG_DRIV','TRUCK','AUTOMOBILE','TRSN_CITY_VEH','ALCOHOL','REDLIGHT']\nprint(dknew)","8e682556":"'''\ndk_g2=dk.groupby(['District','SPEEDING']).size().to_frame('count').reset_index()\n\ndkspeed = dk_g2.pivot(index='District',columns='SPEEDING',values='count')\n\ndk_g2=dk.groupby(['District','AG_DRIV']).size().to_frame('count').reset_index()\ndkagdriv = dk_g2.pivot(index='District',columns='AG_DRIV',values='count')\n\ndk_g2=dk.groupby(['District','REDLIGHT']).size().to_frame('count').reset_index()\ndfred = dk_g2.pivot(index='District',columns='REDLIGHT',values='count')\ndk_g2=dk.groupby(['District','ALCOHOL']).size().to_frame('count').reset_index()\ndkalco = dk_g2.pivot(index='District',columns='ALCOHOL',values='count')\ndk_g2=dk.groupby(['District','DISABILITY']).size().to_frame('count').reset_index()\ndkdisb = dk_g2.pivot(index='District',columns='DISABILITY',values='count')\ndknew = dkspeed\ndknew.columns.names = ['Cause'] \n\ndknew = dknew.dropna()\n\ndknew.columns = ['SPEEDING', 'AG_DRIV']\ndknew['AG_DRIV']= dkagdriv[1]\nprint(dknew)\n\n'''","601d9061":"dk_g2=dk.groupby(['District','SPEEDING']).size().to_frame('count').reset_index()\ndk_g2","e15e5d53":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport matplotlib\n\nscaler = StandardScaler()\nSum_of_squared_distances = []\nstd_scale1 = scaler.fit(dknew)\ndk_transformed1 = std_scale1.transform(dknew)\nfrom sklearn.cluster import KMeans\nkmeansk = KMeans(n_clusters=2,random_state=3425)\ncolors = ['green','blue']\n\n\nplt.figure(figsize=(15, 5)) \n\nax = plt.subplot(121)\nkc =kmeansk.fit(dk_transformed1)\nprint(kc)\nlabel = pd.DataFrame(kc.labels_)\nprint(label)\ndk_result =pd.DataFrame(dk_transformed1)\n# label = label.sort_values(by=0)\ndk_result['label']=label\n\nscatterd = plt.scatter(dk_result[0],dk_result[1],\n                     c=list(label.iloc[:,0]), cmap=matplotlib.colors.ListedColormap(colors),s=50)\nplt.title('K-Means Clustering VS District')\n\nplt.colorbar(scatterd)","4ddd450a":"print('We Conclude that the Highest number of accidents causing fatalitiesby speeding and aggresive driving in Toronto District from 2007-2017, based on Kmeans, occured in')\nneighborhoodsk = dknew.index\n\nneighborhoodsk = np.array(neighborhoodsk)\nprint(neighborhoodsk[np.where(label[0]==0)])\n\n#kdsafe = neighborhoodsk[np.where(label[0]==1)]\n#kdaccident = neighborhoodsk[np.where(label[0]==0)]","c438700f":"ClusterLabelk=pd.DataFrame(kmeansk.labels_) \nClusterLabelk['label']=dknew.index\nClusterLabelk","a1b100c1":"drop_colmns = ['ACCNUM', 'YEAR', 'MONTH', 'DAY', 'HOUR', 'MINUTES', 'WEEKDAY',\n       'LATITUDE', 'LONGITUDE', 'Ward_Name', 'Ward_ID', 'Hood_ID',\n       'Division',  'STREET1', 'STREET2', 'ROAD_CLASS',\n       'LOCCOORD', 'ACCLOC', 'TRAFFCTL', 'VISIBILITY', 'LIGHT', 'RDSFCOND',\n       'ACCLASS', 'IMPACTYPE', 'INVTYPE', 'INVAGE', 'INJURY', 'FATAL_NO',\n       'INITDIR', 'VEHTYPE', 'MANOEUVER', 'DRIVACT', 'DRIVCOND',  'PEDESTRIAN',\n       'CYCLIST', 'MOTORCYCLE', 'TRUCK', 'TRSN_CITY_VEH',\n       'EMERG_VEH', 'PASSENGER', 'AUTOMOBILE']\ndf_dropped = KSI_CLEAN.drop(columns=drop_colmns)\ndf = df_dropped[df_dropped['FATAL']==1]\ndf.columns","908764dd":"dk_g21=X1.groupby(['Hood_Name','SPEEDING']).size().to_frame('count').reset_index()\ndkspeed1 = dk_g21.pivot(index='Hood_Name',columns='SPEEDING',values='count')\n\ndk_g21=X1.groupby(['Hood_Name','AG_DRIV']).size().to_frame('count').reset_index()\ndkagdriv1 = dk_g21.pivot(index='Hood_Name',columns='AG_DRIV',values='count')\n\ndk_g21=X1.groupby(['Hood_Name','REDLIGHT']).size().to_frame('count').reset_index()\ndfred1 = dk_g21.pivot(index='Hood_Name',columns='REDLIGHT',values='count')\n\ndk_g21=X1.groupby(['Hood_Name','ALCOHOL']).size().to_frame('count').reset_index()\ndkalco1 = dk_g21.pivot(index='Hood_Name',columns='ALCOHOL',values='count')\n\ndk_g21=X1.groupby(['Hood_Name','DISABILITY']).size().to_frame('count').reset_index()\ndkdisb1 = dk_g21.pivot(index='Hood_Name',columns='DISABILITY',values='count')\n\ndk_g21=X1.groupby(['Hood_Name','TRUCK']).size().to_frame('count').reset_index()\ndktruck1 = dk_g21.pivot(index='Hood_Name',columns='TRUCK',values='count')\n\n\ndk_g21=X1.groupby(['Hood_Name','TRSN_CITY_VEH']).size().to_frame('count').reset_index()\ndktrsn1 = dk_g21.pivot(index='Hood_Name',columns='TRSN_CITY_VEH',values='count')\n\n\ndk_g21=X1.groupby(['Hood_Name','AUTOMOBILE']).size().to_frame('count').reset_index()\ndkauto1 = dk_g21.pivot(index='Hood_Name',columns='AUTOMOBILE',values='count')\n\n\ndfnew = dkspeed1\nprint(dknew)\n\n\ndknew = dknew.dropna()\n\n#dknew.columns = ['SPEEDING', 'AG_DRIV','TRUCK','AUTOMOBILE','TRSN_CITY_VEH','ALCOHOL','REDLIGHT']\n\ndfnew[0]= dkagdriv1[1]\ndfnew['TRUCK']=dktruck1[1]\ndfnew['AUTOMOBILE']=dkauto1[1]\ndfnew['TRSN_CITY_VEH']=dktrsn1[1]\ndfnew['ALCOHOL']=dkalco1[1]\ndfnew['REDLIGHT']=dfred1[1]\nprint(dfnew)\ndfnew.columns.names = ['Cause'] \ndfnew.columns = ['SPEEDING', 'AG_DRIV','TRUCK','AUTOMOBILE','TRSN_CITY_VEH','ALCOHOL','REDLIGHT']\nprint(dfnew)","4d64f71c":"'''\n\ndf_g2=df.groupby(['Hood_Name','SPEEDING']).size().to_frame('count').reset_index()\nprint(df_g2)\ndfspeed = df_g2.pivot(index='Hood_Name',columns='SPEEDING',values='count')\nprint(dfspeed)\ndf_g2=df.groupby(['Hood_Name','AG_DRIV']).size().to_frame('count').reset_index()\ndfagdriv = df_g2.pivot(index='Hood_Name',columns='AG_DRIV',values='count')\ndf_g2=df.groupby(['Hood_Name','REDLIGHT']).size().to_frame('count').reset_index()\nprint(dfagdriv)\ndfred = df_g2.pivot(index='Hood_Name',columns='REDLIGHT',values='count')\ndf_g2=df.groupby(['Hood_Name','ALCOHOL']).size().to_frame('count').reset_index()\ndfalco = df_g2.pivot(index='Hood_Name',columns='ALCOHOL',values='count')\ndf_g2=df.groupby(['Hood_Name','DISABILITY']).size().to_frame('count').reset_index()\ndfdisb = df_g2.pivot(index='Hood_Name',columns='DISABILITY',values='count')\ndfnew = dfspeed\n\ndfnew[0] = dfspeed[1]\ndfnew[1] = dfagdriv[1]\ndfdum= dfnew\n'''\n## edit\n'''\n\ndfdum[2]= dfred[1]\ndfdum[3]=dfalco[1]\ndfdum[4]=dfdisb[1]\n#dfnew[1] = dfagdriv\nprint(dfdum)\n\n\n'''\n","b4e9973c":"dfnew.columns.names = ['Cause'] \ndfnew = dfnew.fillna(0)\nprint(dfnew.shape)\n\n\"\"\"\ndfdum.columns.names = ['Cause'] \ndfdum = dfdum.fillna(0)\nprint(dfdum)\ndfdum.columns = ['SPEEDING', 'AG_DRIV','REDLIGHT','ALCHOHOL','DISABILITY']\ndfdum.nlargest(10,'SPEEDING')\n\n\"\"\"\n\n\n#dfnew.columns = ['SPEEDING', 'AG_DRIV']\ndfnew.nlargest(10,'SPEEDING')\n","f5c12b70":"print(dfnew.shape)","17d0af6b":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport matplotlib\n\nscaler = StandardScaler()\nSum_of_squared_distances = []\nstd_scale2 = scaler.fit(dfnew)\ndf_transformed2 = std_scale2.transform(dfnew)\nprint(df_transformed2.shape)","ae486b9f":"###\n'''\nscaler = StandardScaler()\nSum_of_squared_distances = []\nstd_scaledum2 = scaler.fit(dfdum)\ndf_transformeddum2 = std_scaledum2.transform(dfdum)\nprint(df_transformeddum2.shape)\n\n'''\n","b4014e74":"from sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=4,random_state=3425)\ncolors =  ['green','blue','red','black']\n\n\nplt.figure(figsize=(15, 5)) \n\nax = plt.subplot(121)\nkc =kmeans.fit(df_transformed2)\nlabel = pd.DataFrame(kc.labels_)\ndf_result2 =pd.DataFrame(df_transformed2)\nprint(df_transformed2.shape)\nprint(df_result2.shape)\n# label = label.sort_values(by=0)\ndf_result2['label']=label\n\nprint(df_result2)\nscatter = plt.scatter(df_result2[0],df_result2[1],\n                     c=list(label.iloc[:,0]), cmap=matplotlib.colors.ListedColormap(colors),s=50)\nplt.title('K-Means Clustering Hood_Name')\nplt.xlabel('Speeding')\nplt.ylabel('Aggresive Driving')\n\nplt.colorbar(scatter)\n","21e23021":"ClusterLabelh=pd.DataFrame(kmeans.labels_) \nClusterLabelh['label']=dfnew.index\n\nClusterLabelh.head(111)","cf98c512":"print('We Conclude that the Highest number of accidents causing fatalities in Toronto Neighborhoods from 2007-2017, based on Kmeans, occured in')\nneighborhoods = dfnew.index\nneighborhoods = np.array(neighborhoods)\nprint(neighborhoods[np.where(label[0]==2)])\nprint(neighborhoods[np.where(label[0]==0)])\n#ksafe = neighborhoods[np.where(label[0]==1)]\n#kaccident = neighborhoods[np.where(label[0]==0)]","db276c26":"dfnew.nlargest(3,'SPEEDING')","fb679703":"dfnew.nsmallest(10,'SPEEDING')","b68d5169":"dfnew.nsmallest(10,'SPEEDING').index","93594195":"'''\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn.cluster as cluster\nimport time\n%matplotlib inline\nsns.set_context('poster')\nsns.set_color_codes()\nplot_kwds = {'alpha' : 0.25, 's' : 80, 'linewidths':0}\ndef plot_clusters(data, algorithm, args, kwds):\n    start_time = time.time()\n    labels = algorithm(*args, **kwds).fit_predict(data)\n    end_time = time.time()\n    palette = sns.color_palette('deep', np.unique(labels).max() + 1)\n    colors = [palette[x] if x >= 0 else (0.0, 0.0, 0.0) for x in labels]\n    plt.scatter(data.T[0], data.T[1], c=colors, **plot_kwds)\n    frame = plt.gca()\n    frame.axes.get_xaxis().set_visible(False)\n    frame.axes.get_yaxis().set_visible(False)\n    plt.title('Clusters found by {}'.format(str(algorithm.__name__)), fontsize=24)\n    #plt.text(-0.5, 0.7, 'Clustering took {:.2f} s'.format(end_time - start_time), fontsize=14)\n    '''","a846c6d0":"'''\nimport time\nplot_clusters(df_transformed2, cluster.KMeans, (), {'n_clusters':4})\n'''","6a5eb916":"KSI_CLEAN.columns","62a38310":"KSI_CLEAN_data_clusterHieCluster=KSI_CLEAN[['FATAL','YEAR', 'MONTH', 'DAY','Hood_Name',\n                                            'SPEEDING', 'AG_DRIV', 'REDLIGHT', 'ALCOHOL','VISIBILITY', 'LIGHT', 'RDSFCOND']]\n\nKSI_CLEAN_data_clusterHieCluster = pd.get_dummies(KSI_CLEAN_data_clusterHieCluster, columns=['VISIBILITY','RDSFCOND','LIGHT','Hood_Name'])","7e1bb6f7":"# import hierarchical clustering libraries\nimport scipy.cluster.hierarchy as sch\nfrom sklearn.cluster import AgglomerativeClustering","b3397f97":"dfnew_a=dfnew\nprint(dfnew_a.shape)","040bc1a1":"# create dendrogram\ndendrogram = sch.dendrogram(sch.linkage(dfnew_a, method='ward'))\n# create clusters\ncluster = AgglomerativeClustering(n_clusters=4, affinity = 'euclidean', linkage = 'ward')\n# save clusters for chart\ncluster.fit_predict(dfnew_a )","b71e7972":"dfnew1=dfnew_a.values","e91b4ac8":"print(cluster.labels_)","126f4a67":"plt.figure(figsize=(10, 7))\nscatter1=plt.scatter(dfnew1[:,0], dfnew1[:,1], c=cluster.labels_, cmap='rainbow')\nplt.title('Agglomerative Clustering VS Hood_Name')\nplt.xlabel('Speeding')\nplt.ylabel('Aggresive Driving')\n\nplt.colorbar(scatter1)","2e4e0e53":"\nClusterLabelh1=pd.DataFrame(cluster.labels_) \nClusterLabelh1['labels']=dfnew_a.index\n\nClusterLabelh1.head(111)\n","7f6bd672":"label1 = pd.DataFrame(cluster.labels_)\n","e18c3f95":"print('We Conclude that the Highest number of accidents causing fatalities in Toronto Neighborhoods from 2007-2017, based on Agglomerative occured in')\nneighborhoods1 = dfnew_a.index\nneighborhoods1 = np.array(neighborhoods1)\nprint(neighborhoods1[np.where(label1[0]==2)])\nprint(neighborhoods1[np.where(label1[0]==1)])","01de8737":"import statsmodels.api as sm\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\n#Adding constant column of ones, mandatory for sm.OLS model\nX_1 = sm.add_constant(KSI_CLEAN_data)\n#Fitting sm.OLS model\nmodel = sm.OLS(KSI_CLEAN_target,X_1).fit()\n\"\"\"The p-value for each term tests the null hypothesis that the coefficient is equal to zero (no effect). \nA low p-value (< 0.05) indicates that you can reject the null hypothesis. \nIn other words, a predictor that has a low p-value is likely to be a meaningful addition to your model \nbecause changes in the predictor's value are related to changes in the response variable.\"\"\"\nmodel.pvalues\nmodel.pvalues>0.05","2363722d":"KSI_CLEAN_data.columns","9e9b1250":"##KSI_CLEAN = KSI_CLEAN.drop([\"PEDTYPE\", \"PEDACT\", \"PEDCOND\", \"CYCLISTYPE\", \"CYCACT\", \"CYCCOND\", \"OFFSET\"], axis=1)\nX_new0= KSI_CLEAN_data.drop([\"ACCNUM\",\"LATITUDE\", \"MINUTES\",\"LONGITUDE\",\"MONTH\",\"DAY\",\"VISIBILITY_Clear\",\"VISIBILITY_Drifting Snow\",\"VISIBILITY_Fog, Mist, Smoke, Dust\",\"VISIBILITY_Freezing Rain\",\"VISIBILITY_Other\",\"VISIBILITY_Rain\",\"VISIBILITY_Snow\",\"VISIBILITY_Strong wind\", \"EMERG_VEH\",\"Hood_ID\",\"AUTOMOBILE\",\"CYCLIST\"],axis=1)\nX_new0.columns","b8109695":"X_new0.shape","393398c4":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nX_new = SelectKBest(chi2, k=2).fit_transform(X_new0, KSI_CLEAN_target)\nKSI_CLEAN_data.shape","bf4bd4bc":"X_new.shape","7e80fac8":"from sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nclf = ExtraTreesClassifier(n_estimators=50)\nclf = clf.fit(X_new0, KSI_CLEAN_target)\n \nmodelETC = SelectFromModel(clf, prefit=True)\nX_new1 = modelETC.transform(X_new0)\nX_new1.shape   \n","14922870":"maskETC = modelETC.get_support(indices=False)    # this will return boolean mask for the columns\nX_new1 = X_new0.loc[:, maskETC]                      # the sliced dataframe, keeping selected columns\nfeatured_col_namesETC =X_new1.columns  # columns name index\nfeatured_col_namesETC","833390cd":"from sklearn.svm import LinearSVC\n\nfrom sklearn.feature_selection import SelectFromModel\n\nlsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X_new0, KSI_CLEAN_target)\nmodelSVC = SelectFromModel(lsvc, prefit=True)\nX_new2 = modelSVC.transform(X_new0)\nX_new2.shape","d38d374e":"maskSVC = modelSVC.get_support(indices=False)    # this will return boolean mask for the columns\nX_new2 = X_new0.loc[:, maskSVC]                      # the sliced dataframe, keeping selected columns\nfeatured_col_namesSVC =X_new2.columns  # columns name index\nfeatured_col_namesSVC","5f72135b":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeRegressor\nimport sklearn.ensemble as ske\nimport matplotlib.pyplot as plt\nregr_depth2 = DecisionTreeRegressor(max_depth=2)\nregr_depth5 = DecisionTreeRegressor(max_depth=5)","367ed516":"# test_size: what proportion of original data is used for test set\nXa_train, Xa_test, y_train,y_test = train_test_split(\n    X_new0, KSI_CLEAN_target,test_size=1\/7.0, random_state=1)\nregr_depth2.fit(Xa_train,y_train)\n\nscorea = regr_depth2.score(Xa_train,y_train)\nprint(scorea)","6a81fb92":"logisticRegrb = LogisticRegression(solver = 'lbfgs')\nlogisticRegrb.fit(Xa_train,y_train)\n\nscoreb = logisticRegrb.score(Xa_train,y_train)\nprint(scoreb)","9c500fdd":"RFRc = ske.RandomForestRegressor()\nRFRc.fit(Xa_train,y_train)\n\nscorec = RFRc.score(Xa_train,y_train)\nprint(scorec)","ef8ca2d7":"#'YEAR', 'MONTH', 'DAY', 'HOUR', 'MINUTES', 'WEEKDAY','Hood_ID', 'PASSENGER','SPEEDING'\nfeat_importancesc = pd.Series(RFRc.feature_importances_, index=Xa_train.columns)\nfeat_importancesc.nlargest(10).plot(kind='barh')","4619ba00":"Xa_train.shape","874b42df":"Xa_train.columns","7989c9ba":"KSI_CLEAN_hoodname=KSI_CLEAN[['Hood_Name', 'YEAR', 'MONTH',\n        'VISIBILITY','RDSFCOND','LIGHT',\n        'SPEEDING', 'AG_DRIV', 'REDLIGHT', 'ALCOHOL', 'DISABILITY']]\n\nKSI_CLEAN_hoodname = pd.get_dummies(KSI_CLEAN_hoodname, columns=['VISIBILITY','RDSFCOND','LIGHT','Hood_Name'])\nKSI_CLEAN_target=KSI_CLEAN[['FATAL']]\n\nXh_train, Xh_test, y_train,y_test = train_test_split(\n    KSI_CLEAN_hoodname, KSI_CLEAN_target,test_size=1\/7.0, random_state=1)\n\n\nRFRh = ske.RandomForestRegressor()\nRFRh.fit(Xh_train,y_train)\n\nscorehn = RFRh.score(Xh_train,y_train)\nprint(scorehn)","b53b998b":"KSI_CLEAN_hoodname.shape","a7996d90":"matplotlib.rcParams.update({'font.size': 10})\nfeat_importanceshn = pd.Series(RFRh.feature_importances_, index=Xh_train.columns)\nfeat_importanceshn.nlargest(20).plot(kind='barh')\n","2a7ae8c0":"KSI_CLEAN_hoodname.columns","72c4f1f0":"KSI_CLEAN_hoodnameSouthdale=KSI_CLEAN[['Hood_Name','YEAR', 'MONTH','VISIBILITY','RDSFCOND','LIGHT','SPEEDING', 'AG_DRIV', 'REDLIGHT', 'ALCOHOL','DISABILITY']]\nKSI_CLEAN_hoodnameSouthdale = pd.get_dummies(KSI_CLEAN_hoodnameSouthdale, columns=['Hood_Name'])\nKSI_CLEAN_hoodnameSouthdale_columnsname = KSI_CLEAN_hoodnameSouthdale.columns.tolist()\n","91e7a282":"KSI_CLEAN_hoodnameSouthdale=KSI_CLEAN_hoodnameSouthdale[['Hood_Name_South Parkdale (85)','YEAR', 'MONTH','VISIBILITY','RDSFCOND','LIGHT','SPEEDING', 'AG_DRIV', 'REDLIGHT', 'ALCOHOL','DISABILITY']]\nKSI_CLEAN_hoodnameSouthdale.columns","17628385":"\n\nKSI_CLEAN_hoodnameSouthdale = pd.get_dummies(KSI_CLEAN_hoodnameSouthdale, columns=['VISIBILITY','RDSFCOND','LIGHT'])\nKSI_CLEAN_target=KSI_CLEAN[['FATAL']]\nXhsd_train, Xhsd_test, y_train,y_test = train_test_split(\n    KSI_CLEAN_hoodnameSouthdale, KSI_CLEAN_target,test_size=1\/7.0, random_state=1)\n\n\nRFRhsd = ske.RandomForestRegressor()\nRFRhsd.fit(Xhsd_train,y_train)\n\nscorehnsd = RFRhsd.score(Xhsd_train,y_train)\nprint(scorehnsd)","1bc07f72":"KSI_CLEAN_hoodnameSouthdale.shape","9acc185c":"feat_importanceshn = pd.Series(RFRhsd.feature_importances_, index=Xhsd_train.columns)\nfeat_importanceshn.nlargest(20).plot(kind='barh')","226242ef":"feat_importanceshn.nlargest(10).index","21ad697b":"df = pd.read_csv('..\/input\/killed-or-seriously-injured-ksi-toronto-clean\/KSI_CLEAN.csv')\ndf.info()","3cb9809d":"feature_lst=['WEEKDAY','Ward_Name',  'Hood_Name', 'ACCLOC', 'TRAFFCTL', 'VISIBILITY', 'LIGHT', 'RDSFCOND', 'IMPACTYPE', 'INVTYPE', 'INVAGE', 'INJURY', \n       'INITDIR', 'VEHTYPE', 'MANOEUVER', 'DRIVACT', 'DRIVCOND', 'PEDTYPE',\n       'PEDACT', 'PEDCOND', 'CYCLISTYPE', 'CYCACT', 'CYCCOND', 'PEDESTRIAN',\n       'CYCLIST', 'AUTOMOBILE', 'MOTORCYCLE', 'TRUCK', 'TRSN_CITY_VEH',\n       'EMERG_VEH', 'PASSENGER', 'SPEEDING', 'AG_DRIV', 'REDLIGHT', 'ALCOHOL',\n       'DISABILITY', 'FATAL']","3e47c9ac":"df_sel=df[feature_lst].copy()\ndf_sel.info()","7406dab6":"df_sel.isnull().mean()","29af2c75":"df_sel.dropna(subset=df_sel.columns[df_sel.isnull().mean()!=0], how='any', axis=0, inplace=True)\ndf_sel.shape","bd377ce4":"target='FATAL'\n# Create arrays for the features and the response variable\nprint(df_sel.shape)\n# set X and y\ny = df_sel[target]\nX1 = df_sel.drop(target, axis=1)\nX = pd.get_dummies(X1, drop_first=True)\nprint(X.shape)","31d0e027":"# Split the data set into training and testing data sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21, stratify=y)","d5147afc":"# List of classification algorithms\nalgo_lst=['Logistic Regression',' K-Nearest Neighbors','Decision Trees','Random Forest', 'Neural Network']\n\n# Initialize an empty list for the accuracy for each algorithm\naccuracy_lst=[]","0336bce1":"# Logistic regression\nfrom sklearn.metrics import accuracy_score\nlr = LogisticRegression(random_state=0)\nlr.fit(X_train,y_train)\ny_pred=lr.predict(X_test)\n\n\n# Get the accuracy score\nacc=accuracy_score(y_test, y_pred)\n\n# Append to the accuracy list\naccuracy_lst.append(acc)\n\nprint(\"[Logistic regression algorithm] accuracy_score: {:.3f}.\".format(acc))","0c20e9f6":"# Import KNeighborsClassifier from sklearn.neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Import DecisionTreeClassifier from sklearn.tree\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Import LogisticRegression\nfrom sklearn.linear_model import LogisticRegression","6c32f2f8":"# Create a k-NN classifier with 6 neighbors\nknn = KNeighborsClassifier(n_neighbors=4)\n\n# Fit the classifier to the data\nknn.fit(X_train,y_train)\n\n# Predict the labels for the training data X\ny_pred = knn.predict(X_test)\n\n# Get the accuracy score\nacc=accuracy_score(y_test, y_pred)\n\n# Append to the accuracy list\naccuracy_lst.append(acc)\n\nprint('[K-Nearest Neighbors (KNN)] knn.score: {:.3f}.'.format(knn.score(X_test, y_test)))\nprint('[K-Nearest Neighbors (KNN)] accuracy_score: {:.3f}.'.format(acc))","eea10ec8":"# Decision tree algorithm\n\n# Instantiate dt_entropy, set 'entropy' as the information criterion\ndt_entropy = DecisionTreeClassifier(max_depth=8, criterion='entropy', random_state=1)\n\n\n# Fit dt_entropy to the training set\ndt_entropy.fit(X_train, y_train)\n\n# Use dt_entropy to predict test set labels\ny_pred= dt_entropy.predict(X_test)\n\n# Evaluate accuracy_entropy\naccuracy_entropy = accuracy_score(y_test, y_pred)\n\n\n# Print accuracy_entropy\nprint('[Decision Tree -- entropy] accuracy_score: {:.3f}.'.format(accuracy_entropy))\n\n\n\n# Instantiate dt_gini, set 'gini' as the information criterion\ndt_gini = DecisionTreeClassifier(max_depth=8, criterion='gini', random_state=1)\n\n\n# Fit dt_entropy to the training set\ndt_gini.fit(X_train, y_train)\n\n# Use dt_entropy to predict test set labels\ny_pred= dt_gini.predict(X_test)\n\n# Evaluate accuracy_entropy\naccuracy_gini = accuracy_score(y_test, y_pred)\n\n# Append to the accuracy list\nacc=accuracy_gini\naccuracy_lst.append(acc)\n\n# Print accuracy_gini\nprint('[Decision Tree -- gini] accuracy_score: {:.3f}.'.format(accuracy_gini))","35489a16":"# Random Forest algorithm\n\n#Create a Gaussian Classifier\nclf=RandomForestClassifier(n_estimators=100)\n\n#Train the model using the training sets y_pred=clf.predict(X_test)\nclf.fit(X_train,y_train)\n\ny_pred=clf.predict(X_test)\n\n\n# Get the accuracy score\nacc=accuracy_score(y_test, y_pred)\n\n# Append to the accuracy list\naccuracy_lst.append(acc)\n\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"[Random forest algorithm] accuracy_score: {:.3f}.\".format(acc))\n","21150ebc":"#Dependencies\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.callbacks import EarlyStopping\n# Neural network\n#create model\nmodel = Sequential()\n\n#get number of columns in training data\nn_cols = X_train.shape[1]\n\n#add model layers\nmodel.add(Dense(120, activation='relu', input_shape=(n_cols,)))\nmodel.add(Dense(30, activation='relu'))\nmodel.add(Dense(1))\n\nmodel.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n\n#set early stopping monitor so the model stops training when it won't improve anymore\nearly_stopping_monitor = EarlyStopping(patience=3)\n#train model\nmodel.fit(X_train, y_train, validation_split=0.2, epochs=100)\n\ny_pred = model.predict(X_test)\n\n\nfrom sklearn.metrics import accuracy_score\na = accuracy_score(y_test, y_pred.round())\nprint('Accuracy is:', a)\n\naccuracy_lst.append(a)\n","b9aa1cb8":"# Generate a list of ticks for y-axis\ny_ticks=np.arange(len(algo_lst))\n\n# Combine the list of algorithms and list of accuracy scores into a dataframe, sort the value based on accuracy score\ndf_acc=pd.DataFrame(list(zip(algo_lst, accuracy_lst)), columns=['Algorithm','Accuracy_Score']).sort_values(by=['Accuracy_Score'],ascending = True)\n\n# Export to a file\ndf_acc.to_csv('.\/Accuracy_scores_algorithms_{}.csv'.format('abc'),index=False)\n\n# Make a plot\nax=df_acc.plot.barh('Algorithm', 'Accuracy_Score', align='center',legend=False,color='blue')\n\n# Add the data label on to the plot\nfor i in ax.patches:\n    # get_width pulls left or right; get_y pushes up or down\n    ax.text(i.get_width()+0.02, i.get_y()+0.2, str(round(i.get_width(),3)), fontsize=10)\n\n# Set the limit, lables, ticks and title\nplt.xlim(0,1.1)\nplt.xlabel('Accuracy Score')\nplt.yticks(y_ticks, df_acc['Algorithm'], rotation=0)\nplt.title(' Which algorithm is better?')\n\nplt.show()","8cb5476a":"## Data Modeling\n\n## Data preparation for modeling by checking null value, get dummies\n","89818420":"#### 3.3 Feature selection","45697582":"Here is the list of dropping columns: 'ACCLOC', 'Ward_Name', 'Ward_ID', 'Hood_Name', 'Hood_ID',\n       'Division', 'STREET1', 'STREET2', 'ROAD_CLASS', 'LOCCOOR','TRAFFCTL',\n       'ACCLOC',  'RDSFCOND', 'ACCLASS','IMPACTYPE', 'INVTYPE', 'INVAGE', 'INJURY', 'FATAL_NO', 'INITDIR',\n       'VEHTYPE', 'MANOEUVER', 'DRIVACT', 'DRIVCOND'\n       'ACCLASS',\n\nReason:overlapping or missing value or not related\n       ","39b5eabc":"##### 3.2.3 Cluster analysis using Kmeans checking neighborhood using raw data with selected columns\n\nHere 'SPEEDING' and 'AG_DRIV' were selected as for investigation of Neighborhood\n\n1. It is Concluded that the Highest number of accidents causing fatalities by speeding and aggressive driving in Toronto District from 2007-2017, based on Kmeans, was occurred more in['Toronto East York']\n\n\n2. when hoodname was chozen as input,  'South Parkdale (85)' 'West Humber-Clairville (1)' and 'Wexford\/Maryvale (119)' rank top 3 out of all hoods as highest numbers of accidents that related to speeding and aggressive driving based on cluster that labeled as 2 and its relative pivot table for speeding and aggressive fatal numbers.\n\nFrom cluster that labeled as 0 and its relative pivot table for speeding and aggressive fatal numbers,  'Elms-Old Rexdale (5)', 'Kingsway South (15)','Broadview North (57)', 'Church-Yonge Corridor (75)','Englemount-Lawrence (32)', 'Highland Creek (134)','Hillcrest Village (48)', 'Lansing-Westgate (38)','O'Connor-Parkview (54)', 'Oakwood Village (107)' are the least accidents reported among all regions.\n","512e1d94":"# The distribution on basis of day of the week shows that Friday had highest accidents","8e383cad":"# The following kernel contains algorithms for KSI data exploration and analysis. \n# Machine Learning algorithms have been applied to predict the fatality of accidents","896aadcd":"Lets try to understand that what causes of accidents resulted in Fatal incidents (involving 1 or more deaths).\nTo do the first lets get the unique values of fatal incidents in a seperate df and then do the analysis. ","36f9fc10":"District         category\n\nTRAFFCTL         category\nVISIBILITY       category\nLIGHT            category\nRDSFCOND         category\nACCLASS          category","c5b49d86":"##### 3.3.3 Feature selection using ExtraTreesClassifier 5 columns  \n\nIn ExtraTreeClassifier, 12 columns such as 'YEAR', 'HOUR', 'WEEKDAY', 'PEDESTRIAN', 'PASSENGER', 'AG_DRIV'were selected for modeling","4b2c32c7":"##### 3.3.1 Feature selection using P value\n\nFrom P value, MONTH,DAY,Visbility, EMERG_VEH,HoodID,AUTOMOBILE,CYCLIST are over 0.05 that can be dropped for modelling. 40 columns were selected('ACCNUM', 'YEAR', 'HOUR', 'MINUTES', 'WEEKDAY', 'LATITUDE', 'LONGITUDE',\n       'PEDESTRIAN', 'MOTORCYCLE', 'TRUCK', 'TRSN_CITY_VEH', 'PASSENGER',\n       'SPEEDING', 'AG_DRIV', 'REDLIGHT', 'ALCOHOL', 'DISABILITY',\n       'RDSFCOND_Dry', 'RDSFCOND_Ice', 'RDSFCOND_Loose Sand or Gravel',\n       'RDSFCOND_Loose Snow', 'RDSFCOND_Other', 'RDSFCOND_Packed Snow',\n       'RDSFCOND_Slush', 'RDSFCOND_Spilled liquid', 'RDSFCOND_Wet',\n       'LIGHT_Dark', 'LIGHT_Dark, artificial', 'LIGHT_Dawn',\n       'LIGHT_Dawn, artificial', 'LIGHT_Daylight',\n       'LIGHT_Daylight, artificial', 'LIGHT_Dusk', 'LIGHT_Dusk, artificial',\n       'LIGHT_Other', 'District_Etobicoke York', 'District_No District',\n       'District_North York', 'District_Scarborough',\n       'District_Toronto East York').","cfeb0690":"##### 3.3.4 Feature selection using LinearSVC X_new2\n\nIn LinearSVC model, columns including '['YEAR', 'HOUR', 'MINUTES', 'WEEKDAY', 'PEDESTRIAN', 'TRUCK',\n       'TRSN_CITY_VEH', 'PASSENGER', 'SPEEDING', 'AG_DRIV', 'RDSFCOND_Wet',\n       'LIGHT_Dark', 'LIGHT_Daylight', 'District_Etobicoke York',\n       'District_North York', 'District_Scarborough',\n       'District_Toronto East York'] were selected for modeling","7239d6c3":"## District--Toronto East York has the most accident numbers.","7bf543b8":"Raw data modeling selecting one Hoodname instead of district for checking only hoodname southdale has problems. 32 columns \n\n'LIGHT_Daylight', 'AG_DRIV', 'LIGHT_Dark, artificial', 'LIGHT_Dark', 'ALCOHOL', 'RDSFCOND_Dry', 'REDLIGHT', 'RDSFCOND_Wet' are condition that need to be improved in South dale area when feature importance was used. Accidnets tend to be more on the dawn time and aggressive driving was the next contribution to fatality in South Parkdale region","29b28489":"## Cluster analysis\n\n### Elbow Method to Determine Number of Clusters\n\nTo begin with Cluster analysis, Elbow method was explored to identify optimized cluster number using 'SPEEDING', 'AG_DRIV', 'REDLIGHT', 'ALCOHOL','DISABILITY' as input and aggregate those numbers as output to further investigate and identify which neibourhood has highest numbers of accident related to input condition.\n\nElbow method revealed the optimal cluster to be 4 clusters.\n\n","d14944cc":"# August 2013 had maximum fatal accidents","8e2b3d1a":"# Data relationship exploration\n\n## Accident numbers against month and year","ba3a334d":"## Total number of incidnets have reduced slightly over the years.","044024ef":"## Creating a Heatmap of where Fatality happened (Injury == Fatal)","f82acf0b":"# Data Preparation\n\n##  Data cleaning by replacing blank value to NA","95637e23":"## ACCCLASS=FATAL","e6ba22a9":"## From the data above, accidents happened more from June to October","6a73835c":"# Finding Important features","bdf86681":"### The following heatmap shows the features having maximum missing values","a2518013":"##### 3.2.4 Cluster analysis using hierarchical clustering libraries","e0034734":"##### 3.3.2 Feature selection using chi2 and SelectKBest\n\nIn this model, only 2 columns was selected, namely, FATAL as output and DAY as input. ","65d2e252":"#### 2.5 Pivot table and pie chart for Summarison\n\nAutomobiles have been pretty consistent reason of accident over the years involving aggressive driving and pedestrians \n\n##### The columns in ksi_pivot can be classified into following categories:\n    - Driving condition for accidents (AG_DRIV, ALCOHOL, DISABILITY, REDLIGHT, SPEEDING)\n    - Type of vehicles involved (AUTOMOBILE, CYCLIST, EMERG_VEH, MOTORCYCLE, TRNS_CITY_VEH, TRUCK)\n\nFrom KSI_pivot_cause, AG_DRIV (Aggressive and Distracted Driving) are the major cause of accidents (62.9%), speeding accounts for 21.4, redlight is 10.4% and alcohol is 5.3 %.\n\nFrom KSI_pivot_Types, automobile is reponsible for 45.6 % accident while truck, motocycle,Transit or City Vehicle are all second to it.\n\nFrom KSI_pivot_CPP, passengers and PEDESTRIAN are major victims (each 40~ %) and cyclist (12 %) are second to them.\n\nFrom KSI_pivot_DF, Accidents caused 80~ % fatality.\n","d3ddd39b":"# Applying Machine Learning Models for prediction of Fatal Accidents","9a09c215":"##  Data cleaning by dropping columns with large amount of missing value from heat map","4e050b6e":"## Neighbourhood-- Waterfront Communities has most accidents","082b29d1":"#### 3.3.5 Data modeling (no pca and feature selection)  54 columns\n\n10 columns 'ACCNUM', 'YEAR', 'MONTH', 'DAY', 'HOUR', 'MINUTES', 'WEEKDAY','Hood_ID', 'PASSENGER','SPEEDING' were selected as top 10 feature importance.\n\nPerformance\n\nregr_depth2.score 0.028\n\nlogisticRegrb.score 0.864\n\nRFRc.score 0.971\n\n","e3b42989":"## Changing the property damage and non-fatal columns to Non-Fatal","794e2e96":"#### 2.2 Heat map of accidents","d9bf19fb":"##### 3.2.2 Cluster analysis using Kmeans in general\n\nIn this section, Kmean was applied to all other column besides those mentioned above. But using district instead of hoodname.","8916d212":"# ACCLASS VS Fatal VS injury --- looking for target column (FATAL)\n\nACCLASS is columns that classified into 3 catergory while fatal only show whether is fatal. \n\n1.In general column ACCLASS and FATAL both show same pattern. \n\n2.Injury miss a lot of information thus not very meaningful to be the target column\n\nAs Fatal column was already in int type and quite similar to ACCLASS, it will be used as output in data modeling later.","add51ac9":"# District vs Region ","e26e8a80":"Looking at cluster that labeled as 2 and its relative pivot table for speeding and aggressive fatal numbers,  'Elms-Old Rexdale (5)', 'Kingsway South (15)','Broadview North (57)', 'Church-Yonge Corridor (75)','Englemount-Lawrence (32)', 'Highland Creek (134)','Hillcrest Village (48)', 'Lansing-Westgate (38)','O'Connor-Parkview (54)', 'Oakwood Village (107)' are the least accidents reported among all regions and are in cluster 0.","f4f14f6a":"## Fatal injury was highest in 2016","f4050984":"## Data cleaning by changing data type","0c06ed4c":"## From 2007 to 2017, the numbers of non-fatal accident declined while those occurance of fatal accident kept unchanged.","c36d9c29":"##### 3.3.6 Raw data modeling selecting Hoodname instead of district for checking whcih hoodname has problems. 173 columns "}}