{"cell_type":{"20028721":"code","a27f7168":"code","ae7954a4":"code","509c6271":"code","f95b1a5b":"code","e2058dd3":"code","977d17d2":"code","6fc9d31b":"code","aaaff65d":"code","44d98024":"code","21b12e33":"code","72c4a51b":"code","58e801a0":"code","c1f3d7f2":"code","769e6675":"code","1610b4e1":"code","ee8aa23f":"code","d0c81bc8":"code","3311a3fa":"code","b77baf30":"code","42b6713e":"code","23bfda5f":"code","b7a36de9":"code","54add1ff":"code","1fa2865d":"code","a79c6ec5":"code","55ff1e8a":"code","bc4e96ed":"code","f3f925d8":"code","b8e9429b":"code","71186195":"code","c3e95ac0":"code","2ff88431":"code","18a5a9ed":"code","74cecf18":"code","5e2dbb27":"code","5d7e9356":"code","491890e7":"code","db268d75":"code","7fa7ce70":"code","d4e3f340":"code","0b58e21d":"code","a75c4934":"code","c495c113":"code","a0bf4f4c":"code","0f492e4f":"code","1be658df":"code","7b6480ef":"code","26ef16af":"code","841b8046":"code","ecc14869":"code","00acf6e0":"markdown","8a24c9c6":"markdown","fe32b425":"markdown","2bdbde0b":"markdown"},"source":{"20028721":"import pandas as pd\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.svm import SVC\nimport numpy as np\nfrom sklearn.metrics import classification_report\nimport matplotlib.pyplot as plt","a27f7168":"# reading data\ndata = pd.read_csv(\"\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\")","ae7954a4":"# printing data\ndata","509c6271":"# printing null values and types of data since data has not got any object or string we don't need to evaluate types\ndata.info()","f95b1a5b":"# printing statistical information about data\ndata.describe()","e2058dd3":"# This pie chart shows distribution of wine quality by quality classes\nplt.figure(1, figsize=(10,10))\ndata['quality'].value_counts().plot.pie(autopct=\"%1.1f%%\")","977d17d2":"import seaborn as sns; sns.set()\n\nplt.figure(figsize=(15, 15))\n\n# this graph shows that the higher the alcohol, the higher the quality.\nplt.subplot(4,4,1)\nsns.barplot(x = 'quality', y = 'alcohol', data = data)\n\n# this graph shows that quality decreases as volatile acidity decreases.\nplt.subplot(4,4,2)\nsns.barplot(x = 'quality', y = 'volatile acidity', data = data)\n\n# this graph shows that the higher the citric acid, the higher the quality.\nplt.subplot(4,4,3)\nsns.barplot(x = 'quality', y = 'citric acid', data = data)\n\n# this graph shows that the higher the sulphates, the higher the quality.\nplt.subplot(4,4,4)\nsns.barplot(x = 'quality', y = 'sulphates', data = data)","6fc9d31b":"# this line shows us there is no NaN value in dataset.\ndata.isna().sum().sum()","aaaff65d":"# this line shows us quality column has 6 different quality type and how many value they have for each quality.\ndata.quality.value_counts()","44d98024":"# We will assing new values to quality column. If quality is lower than 6 it will be low quality above that it will\n# be high quality\ndata[\"quality\"] = data.quality.apply(lambda q: 'low' if q < 6 else 'high')","21b12e33":"# Since ML algorithms don't work on string values we have to encode them. In order to do that we will use label encoder.\nlabel_quality = LabelEncoder()","72c4a51b":"# Applying encoder to our column it will transform high values to 0 low values to 1.\ndata['quality'] = label_quality.fit_transform(data['quality'])","58e801a0":"# checking that transformation is done correctly or not. Our 0 column should have 638+199+18 values and it is okay.\ndata.quality.value_counts()","c1f3d7f2":"# we will indicate feature and target columns. In this case quality column(y) will be our target \n# other columns are features(x)\nx = data.drop(\"quality\", axis=1)\ny = data[\"quality\"]","769e6675":"# splitting our database in order to avoid overfitting and testing it more accurately.\n# we have used scikit-learns train_test_split method.\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 43, shuffle=1)","1610b4e1":"# Defining standard scaler.\nsc = StandardScaler()","ee8aa23f":"# Apllying scaler to our x_train and x_test to obtain optimized results.\nx_train = sc.fit_transform(x_train)\nx_test = sc.fit_transform(x_test)","d0c81bc8":"\"\"\"\nInstead of writing same comments for each implementation. We will explain them in here.\n1) We will create our model. We used scikit-learn's libraries for models.\n2) We will fit our models by using x_train and y_train. Fitting basically means we will make our algorithm learn the\nrelationship between train and test data.\n3) Our model will make predictions by taking our test samples(x_test)\n4) Printing confusion matrix to check accuracy of our model. It will also show us wrong predictions as false true and \nfalse false\n5) Printing accuracy score of our model.\n6) Arranging parameters grid and implementing random search for Gradient Boosting and Random forest. We implemented\ngrid search for Support Vector Machines. We wanted to try both of them. More iteration could give us better results\nsince we don't have too much computation power we didn't obtain best parameters.\n7) Taking best parameters of random search or grid search and retraining our model with best parameters. \n8) Taking predictions from retrained models.\n9) Calculating accuray scores of new models. Except support vector machines all searches increased our accuracy \naround %1.\n10) Applying cross validation and taking its mean. \nWe used 3 classification algorithms which are support vector machines, gradient boosting and random forest \nin order to classify the wine quality.\nIn final random forest seems best algorithm with accuracy score %80. \nGradient boosting seems second best algorithm with accuracy score %79\nAs last one support vector machine seems algorithm with less accuracy. It is around %76.\n\"\"\"","3311a3fa":"GBModel = GradientBoostingClassifier()\nGBModel.fit(x_train, y_train)","b77baf30":"GBPredictions = GBModel.predict(x_test)","42b6713e":"metrics.confusion_matrix(y_test, GBPredictions)","23bfda5f":"metrics.accuracy_score(y_test, GBPredictions)","b7a36de9":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nnum_estimators = [250, 500]\nlearn_rates = [0.02, 0.08]\nmax_depths = [2, 5]\nmin_samples_leaf = [5, 10]\nmin_samples_split = [5, 10]\n\nparam_grid = {'n_estimators': num_estimators,\n              'learning_rate': learn_rates,\n              'max_depth': max_depths,\n              'min_samples_leaf': min_samples_leaf,\n              'min_samples_split': min_samples_split}\n\nrandom_search = RandomizedSearchCV(GradientBoostingRegressor(loss='huber'), param_grid, random_state=1, n_iter=20, cv=5, verbose=0, n_jobs=-1)\n\nrandom_search.fit(x_train, y_train)","54add1ff":"random_search.best_params_","1fa2865d":"randomGBModel = GradientBoostingClassifier(n_estimators=250, learning_rate=0.02, max_depth=5, min_samples_split=10, min_samples_leaf=5)\nrandomGBModel.fit(x_train, y_train)","a79c6ec5":"newGBpredictions = randomGBModel.predict(x_test)","55ff1e8a":"metrics.accuracy_score(y_test, newGBpredictions)","bc4e96ed":"GBcross = cross_val_score(estimator = randomGBModel, X = x_train, y = y_train, cv = 3)","f3f925d8":"GBcross.mean()","b8e9429b":"SVModel = SVC()\nSVModel.fit(x_train, y_train)","71186195":"SVPredictions = SVModel.predict(x_test)","c3e95ac0":"metrics.confusion_matrix(y_test, SVPredictions)","2ff88431":"metrics.accuracy_score(y_test, SVPredictions)","18a5a9ed":"from sklearn.model_selection import GridSearchCV\n\nparam = {\n    'C': [0.1,0.8,0.9,1,1.1,1.2,1.3,1.4],\n    'kernel':['linear', 'rbf'],\n    'gamma' :[0.1,0.8,0.9,1,1.1,1.2,1.3,1.4]\n}\ngrid_svc = GridSearchCV(SVModel, param_grid=param, scoring='accuracy', cv=10)\ngrid_svc.fit(x_train, y_train)","74cecf18":"grid_svc.best_params_","5e2dbb27":"gridSVModel = SVC(C=1.1, gamma=0.8, kernel=\"rbf\")\ngridSVModel.fit(x_train, y_train)","5d7e9356":"gridSVpredictions = gridSVModel.predict(x_test)","491890e7":"metrics.accuracy_score(y_test, gridSVpredictions)","db268d75":"SVMcross = cross_val_score(estimator = gridSVModel, X = x_train, y = y_train, cv = 3)","7fa7ce70":"SVMcross.mean()","d4e3f340":"RFModel = RandomForestClassifier()\nRFModel.fit(x_train, y_train)","0b58e21d":"RFPredictions = RFModel.predict(x_test)","a75c4934":"metrics.confusion_matrix(y_test,RFPredictions)","c495c113":"metrics.accuracy_score(y_test, RFPredictions)","a0bf4f4c":"from sklearn.ensemble import RandomForestRegressor\n\nnum_estimators = [500, 5000]\nmax_depths = [10, 50]\nmin_samples_leaf = [1, 15]\nmin_samples_split = [2, 20]\nmax_features = [\"auto\", \"sqrt\", \"log2\"]\n\nparam_grid = {'n_estimators': num_estimators,\n              'max_depth': max_depths,\n              'min_samples_leaf': min_samples_leaf,\n              'min_samples_split': min_samples_split,\n             \"max_features\": max_features}\n\nrandom_search = RandomizedSearchCV(RFModel, param_grid, random_state=1, n_iter=5, cv=3, verbose=0, n_jobs=-1)\n\nrandom_search.fit(x_train, y_train)","0f492e4f":"random_search.best_params_","1be658df":"randomRFModel = RandomForestClassifier(n_estimators=5000, min_samples_split=2, min_samples_leaf=1, max_features=\"sqrt\", max_depth=50)\nrandomRFModel.fit(x_train, y_train)","7b6480ef":"randomRFPredictions = randomRFModel.predict(x_test)","26ef16af":"metrics.accuracy_score(y_test, randomRFPredictions)","841b8046":"RFcross = cross_val_score(estimator = randomRFModel, X = x_train, y = y_train, cv = 10)","ecc14869":"RFcross.mean()","00acf6e0":"## Data Visualization","8a24c9c6":"## Random Forest","fe32b425":"## Support Vector Machines","2bdbde0b":"## Gradient Boosting"}}