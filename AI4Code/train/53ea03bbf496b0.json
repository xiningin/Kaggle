{"cell_type":{"a1ef4211":"code","ba9d7aaf":"code","34c216a3":"code","accd0116":"code","c5f6ca96":"code","bd330496":"code","03e075ea":"code","6cb1426e":"code","9fa9b6cd":"code","5aec08b6":"code","a6006ff5":"code","daedba12":"code","36915d93":"code","111c775d":"code","4767e527":"code","b5680b56":"code","eaf0fec4":"code","61984c5c":"markdown","9b15f98e":"markdown","0a487d69":"markdown","aa7986ba":"markdown","70dec757":"markdown"},"source":{"a1ef4211":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np \nimport pandas as pd\nfrom sklearn.cluster import DBSCAN \nfrom sklearn.datasets.samples_generator import make_blobs \nfrom sklearn.preprocessing import StandardScaler \nimport matplotlib.pyplot as plt \n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","ba9d7aaf":"# Create random data and store in feature matrix X and response vector y.\nX, y = make_blobs(n_samples=1500, centers=[[4,3], [2,-1], [-1,4]], cluster_std=0.5)\n\n# Standardize features by removing the mean and scaling to unit variance\nX = StandardScaler().fit_transform(X)","34c216a3":"epsilon = 0.3\nminimumSamples = 7\ndb = DBSCAN(eps=epsilon, min_samples=minimumSamples).fit(X)\nlabels = db.labels_\nlabels","accd0116":"# Firts, create an array of booleans using the labels from db.\ncore_samples_mask = np.zeros_like(db.labels_, dtype=bool)\ncore_samples_mask[db.core_sample_indices_] = True\ncore_samples_mask","c5f6ca96":"# Number of clusters in labels, ignoring noise if present.\nn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\nn_clusters_","bd330496":"# Remove repetition in labels by turning it into a set.\nunique_labels = set(labels)\nunique_labels","03e075ea":"# Create colors for the clusters.\ncolors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n\n# Plot the points with colors\nfor k, col in zip(unique_labels, colors):\n    if k == -1:\n        # Black used for noise.\n        col = 'k'\n\n    class_member_mask = (labels == k)\n\n    # Plot the datapoints that are clustered\n    xy = X[class_member_mask & core_samples_mask]\n    plt.scatter(xy[:, 0], xy[:, 1],s=50, c=[col], marker=u'o', alpha=0.5)\n\n    # Plot the outliers\n    xy = X[class_member_mask & ~core_samples_mask]\n    plt.scatter(xy[:, 0], xy[:, 1],s=50, c=[col], marker=u'o', alpha=0.5)","6cb1426e":"from sklearn.cluster import KMeans \nk = 3\nk_means3 = KMeans(init = \"k-means++\", n_clusters = k, n_init = 12)\nk_means3.fit(X)\nfig = plt.figure(figsize=(6, 4))\nax = fig.add_subplot(1, 1, 1)\nfor k, col in zip(range(k), colors):\n    my_members = (k_means3.labels_ == k)\n    plt.scatter(X[my_members, 0], X[my_members, 1],  c=col, marker=u'o', alpha=0.5)\nplt.show()","9fa9b6cd":"filename='weather-stations20140101-20141231.csv'\n\n#Read csv\npdf = pd.read_csv('\/kaggle\/input\/'+filename)\npdf.head(5)","5aec08b6":"# Simple cleaning\npdf = pdf[pd.notnull(pdf[\"Tm\"])]\npdf = pdf.reset_index(drop=True)\npdf.head(5)","a6006ff5":"#Visualization of stations on map using basemap package. The matplotlib basemap toolkit is a library for plotting 2D data on maps in Python. Basemap does not do any plotting on it\u2019s own, but provides the facilities to transform coordinates to a map projections. \n\n#Please notice that the size of each data points represents the average of maximum temperature for each station in a year.\nfrom mpl_toolkits.basemap import Basemap\nimport matplotlib.pyplot as plt\nfrom pylab import rcParams\n%matplotlib inline\nrcParams['figure.figsize'] = (14,10)\n\nllon=-140\nulon=-50\nllat=40\nulat=65\n\npdf = pdf[(pdf['Long'] > llon) & (pdf['Long'] < ulon) & (pdf['Lat'] > llat) &(pdf['Lat'] < ulat)]\n\nmy_map = Basemap(projection='merc',\n            resolution = 'l', area_thresh = 1000.0,\n            llcrnrlon=llon, llcrnrlat=llat, #min longitude (llcrnrlon) and latitude (llcrnrlat)\n            urcrnrlon=ulon, urcrnrlat=ulat) #max longitude (urcrnrlon) and latitude (urcrnrlat)\n\nmy_map.drawcoastlines()\nmy_map.drawcountries()\n# my_map.drawmapboundary()\nmy_map.fillcontinents(color = 'white', alpha = 0.3)\nmy_map.shadedrelief()\n\n# To collect data based on stations        \n\nxs,ys = my_map(np.asarray(pdf.Long), np.asarray(pdf.Lat))\npdf['xm']= xs.tolist()\npdf['ym'] =ys.tolist()\n\n#Visualization1\nfor index,row in pdf.iterrows():\n#   x,y = my_map(row.Long, row.Lat)\n   my_map.plot(row.xm, row.ym,markerfacecolor =([1,0,0]),  marker='o', markersize= 5, alpha = 0.75)\n#plt.text(x,y,stn)\nplt.show()","daedba12":"from sklearn.cluster import DBSCAN\nimport sklearn.utils\nfrom sklearn.preprocessing import StandardScaler\nsklearn.utils.check_random_state(1000)\nClus_dataSet = pdf[['xm','ym']]\nClus_dataSet = np.nan_to_num(Clus_dataSet)\nClus_dataSet = StandardScaler().fit_transform(Clus_dataSet)\n\n# Compute DBSCAN\ndb = DBSCAN(eps=0.15, min_samples=10).fit(Clus_dataSet)\ncore_samples_mask = np.zeros_like(db.labels_, dtype=bool)\ncore_samples_mask[db.core_sample_indices_] = True\nlabels = db.labels_\npdf[\"Clus_Db\"]=labels\n\nrealClusterNum=len(set(labels)) - (1 if -1 in labels else 0)\nclusterNum = len(set(labels)) \n\n\n# A sample of clusters\npdf[[\"Stn_Name\",\"Tx\",\"Tm\",\"Clus_Db\"]].head(5)","36915d93":"set(labels)","111c775d":"from mpl_toolkits.basemap import Basemap\nimport matplotlib.pyplot as plt\nfrom pylab import rcParams\n%matplotlib inline\nrcParams['figure.figsize'] = (14,10)\n\nmy_map = Basemap(projection='merc',\n            resolution = 'l', area_thresh = 1000.0,\n            llcrnrlon=llon, llcrnrlat=llat, #min longitude (llcrnrlon) and latitude (llcrnrlat)\n            urcrnrlon=ulon, urcrnrlat=ulat) #max longitude (urcrnrlon) and latitude (urcrnrlat)\n\nmy_map.drawcoastlines()\nmy_map.drawcountries()\n#my_map.drawmapboundary()\nmy_map.fillcontinents(color = 'white', alpha = 0.3)\nmy_map.shadedrelief()\n\n# To create a color map\ncolors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))\n\n\n\n#Visualization1\nfor clust_number in set(labels):\n    c=(([0.4,0.4,0.4]) if clust_number == -1 else colors[np.int(clust_number)])\n    clust_set = pdf[pdf.Clus_Db == clust_number]                    \n    my_map.scatter(clust_set.xm, clust_set.ym, color =c,  marker='o', s= 20, alpha = 0.85)\n    if clust_number != -1:\n        cenx=np.mean(clust_set.xm) \n        ceny=np.mean(clust_set.ym) \n        plt.text(cenx,ceny,str(clust_number), fontsize=25, color='red',)\n        print (\"Cluster \"+str(clust_number)+', Avg Temp: '+ str(np.mean(clust_set.Tm)))","4767e527":"from sklearn.cluster import DBSCAN\nimport sklearn.utils\nfrom sklearn.preprocessing import StandardScaler\nsklearn.utils.check_random_state(1000)\nClus_dataSet = pdf[['xm','ym','Tx','Tm','Tn']]\nClus_dataSet = np.nan_to_num(Clus_dataSet)\nClus_dataSet = StandardScaler().fit_transform(Clus_dataSet)\n\n# Compute DBSCAN\ndb = DBSCAN(eps=0.3, min_samples=10).fit(Clus_dataSet)\ncore_samples_mask = np.zeros_like(db.labels_, dtype=bool)\ncore_samples_mask[db.core_sample_indices_] = True\nlabels = db.labels_\npdf[\"Clus_Db\"]=labels\n\nrealClusterNum=len(set(labels)) - (1 if -1 in labels else 0)\nclusterNum = len(set(labels)) \n\n\n# A sample of clusters\npdf[[\"Stn_Name\",\"xm\",\"ym\",\"Tx\",\"Tm\",\"Tn\",\"Clus_Db\"]].head(5)","b5680b56":"set(labels)","eaf0fec4":"from mpl_toolkits.basemap import Basemap\nimport matplotlib.pyplot as plt\nfrom pylab import rcParams\n%matplotlib inline\nrcParams['figure.figsize'] = (14,10)\n\nmy_map = Basemap(projection='merc',\n            resolution = 'l', area_thresh = 1000.0,\n            llcrnrlon=llon, llcrnrlat=llat, #min longitude (llcrnrlon) and latitude (llcrnrlat)\n            urcrnrlon=ulon, urcrnrlat=ulat) #max longitude (urcrnrlon) and latitude (urcrnrlat)\n\nmy_map.drawcoastlines()\nmy_map.drawcountries()\n#my_map.drawmapboundary()\nmy_map.fillcontinents(color = 'white', alpha = 0.3)\nmy_map.shadedrelief()\n\n# To create a color map\ncolors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))\n\n\n\n#Visualization1\nfor clust_number in set(labels):\n    c=(([0.4,0.4,0.4]) if clust_number == -1 else colors[np.int(clust_number)])\n    clust_set = pdf[pdf.Clus_Db == clust_number]                    \n    my_map.scatter(clust_set.xm, clust_set.ym, color =c,  marker='o', s= 20, alpha = 0.85)\n    if clust_number != -1:\n        cenx=np.mean(clust_set.xm) \n        ceny=np.mean(clust_set.ym) \n        plt.text(cenx,ceny,str(clust_number), fontsize=25, color='red',)\n        print (\"Cluster \"+str(clust_number)+', Avg Temp: '+ str(np.mean(clust_set.Tm)))","61984c5c":"DBSCAN stands for Density-Based Spatial Clustering of Applications with Noise. This technique is one of the most common clustering algorithms  which works based on density of object.\nThe whole idea is that if a particular point belongs to a cluster, it should be near to lots of other points in that cluster.\n\nIt works based on two parameters: Epsilon and Minimum Points  \n__Epsilon__ determine a specified radius that if includes enough number of points within, we call it dense area  \n__minimumSamples__ determine the minimum number of data points we want in a neighborhood to define a cluster.","9b15f98e":"### Clustering of stations based on their location i.e. Lat & Lon\n\n__DBSCAN__ form sklearn library can runs DBSCAN clustering from vector array or distance matrix. In our case, we pass it the Numpy array Clus_dataSet to find core samples of high density and expands clusters from them. ","0a487d69":"### Clustering of stations based on their location, mean, max, and min Temperature\nIn this section we re-run DBSCAN, but this time on a 5-dimensional dataset:","aa7986ba":"To better underestand differences between partitional and density-based clusteitng, try to cluster the above dataset into 3 clusters using k-Means.  ","70dec757":"DBSCAN is specially very good for tasks like class identification on a spatial context. The wonderful attribute of DBSCAN algorithm is that it can find out any arbitrary shape cluster without getting affected by noise.\n\n## Canada Environment Data\n\n<h4 align = \"center\">\nEnvironment Canada    \nMonthly Values for July - 2015\t\n<\/h4>\n\n<style>\ntable {\n    font-family: arial, sans-serif;\n    border-collapse: collapse;\n    width: 100%;\n}\n\ntd, th {\n    border: 1px solid #dddddd;\n    text-align: left;\n    padding: 8px;\n}\n\ntr:nth-child(even) {\n    background-color: #dddddd;\n}\n<\/style>\n\n<table>\n  <tr>\n    <th>Name in the table<\/th>\n    <th>Meaning<\/th>\n  <\/tr>\n  <tr>\n    <td><font color = \"green\"><strong>Stn_Name<\/font><\/td>\n    <td><font color = \"green\"><strong>Station Name<\/font<\/td>\n  <\/tr>\n  <tr>\n    <td><font color = \"green\"><strong>Lat<\/font><\/td>\n    <td><font color = \"green\"><strong>Latitude (North+, degrees)<\/font><\/td>\n  <\/tr>\n  <tr>\n    <td><font color = \"green\"><strong>Long<\/font><\/td>\n    <td><font color = \"green\"><strong>Longitude (West - , degrees)<\/font><\/td>\n  <\/tr>\n  <tr>\n    <td>Prov<\/td>\n    <td>Province<\/td>\n  <\/tr>\n  <tr>\n    <td>Tm<\/td>\n    <td>Mean Temperature (\u00b0C)<\/td>\n  <\/tr>\n  <tr>\n    <td>DwTm<\/td>\n    <td>Days without Valid Mean Temperature<\/td>\n  <\/tr>\n  <tr>\n    <td>D<\/td>\n    <td>Mean Temperature difference from Normal (1981-2010) (\u00b0C)<\/td>\n  <\/tr>\n  <tr>\n    <td><font color = \"black\">Tx<\/font><\/td>\n    <td><font color = \"black\">Highest Monthly Maximum Temperature (\u00b0C)<\/font><\/td>\n  <\/tr>\n  <tr>\n    <td>DwTx<\/td>\n    <td>Days without Valid Maximum Temperature<\/td>\n  <\/tr>\n  <tr>\n    <td><font color = \"black\">Tn<\/font><\/td>\n    <td><font color = \"black\">Lowest Monthly Minimum Temperature (\u00b0C)<\/font><\/td>\n  <\/tr>\n  <tr>\n    <td>DwTn<\/td>\n    <td>Days without Valid Minimum Temperature<\/td>\n  <\/tr>\n  <tr>\n    <td>S<\/td>\n    <td>Snowfall (cm)<\/td>\n  <\/tr>\n  <tr>\n    <td>DwS<\/td>\n    <td>Days without Valid Snowfall<\/td>\n  <\/tr>\n  <tr>\n    <td>S%N<\/td>\n    <td>Percent of Normal (1981-2010) Snowfall<\/td>\n  <\/tr>\n  <tr>\n    <td><font color = \"green\"><strong>P<\/font><\/td>\n    <td><font color = \"green\"><strong>Total Precipitation (mm)<\/font><\/td>\n  <\/tr>\n  <tr>\n    <td>DwP<\/td>\n    <td>Days without Valid Precipitation<\/td>\n  <\/tr>\n  <tr>\n    <td>P%N<\/td>\n    <td>Percent of Normal (1981-2010) Precipitation<\/td>\n  <\/tr>\n  <tr>\n    <td>S_G<\/td>\n    <td>Snow on the ground at the end of the month (cm)<\/td>\n  <\/tr>\n  <tr>\n    <td>Pd<\/td>\n    <td>Number of days with Precipitation 1.0 mm or more<\/td>\n  <\/tr>\n  <tr>\n    <td>BS<\/td>\n   <td>Percent of Normal (1981-2010) Bright Sunshine<\/td>\n  <\/tr>\n  <tr>\n    <td>HDD<\/td>\n    <td>Degree Days below 18 \u00b0C<\/td>\n  <\/tr>\n  <tr>\n    <td>CDD<\/td>\n    <td>Degree Days above 18 \u00b0C<\/td>\n  <\/tr>\n  <tr>\n    <td>Stn_No<\/td>\n    <td>Climate station identifier (first 3 digits indicate   drainage basin, last 4 characters are for sorting alphabetically).<\/td>\n  <\/tr>\n  <tr>\n    <td>NA<\/td>\n      <td>Bright Sunshine (hours)<\/td>\n  <\/tr>\n  <tr>\n    <td>DwBS<\/td>\n    <td>Days without Valid Bright Sunshine<\/td>\n  <\/tr>\n  <tr>\n    <td>BS%<\/td>\n   <td>Not Available<\/td>\n  <\/tr>\n\n\n<\/table>"}}