{"cell_type":{"e1248d6f":"code","20bbd31c":"code","5f20d713":"code","d88bc269":"code","0704fe3b":"code","6c65d3c2":"code","60f8ab72":"code","d2219a69":"code","739115a9":"code","f84b9d9c":"code","635044a6":"code","153138ef":"code","49249c4a":"code","41a0ea1c":"code","d449a569":"code","8c51656e":"code","a72570f9":"code","8b25c4f6":"code","35cf6c10":"code","7459a2fb":"code","c26a233d":"code","9e2dda9b":"code","c9e2309f":"code","b2d2ea52":"code","23dd8d46":"code","1d3bbef8":"code","8e43b446":"code","931896fe":"code","0644e656":"markdown","291d85af":"markdown","ee4fb21f":"markdown","250fd1b3":"markdown"},"source":{"e1248d6f":"from IPython.core.interactiveshell import InteractiveShell","20bbd31c":"import os\nimport tensorflow as tf\nimport numpy as np\nimport json\nimport pandas as pd\nimport pathlib\nfrom tensorflow import keras as kr\nimport operator\nimport json\nimport os\nimport pathlib\nimport functools\n\nfrom datetime import datetime\nfrom enum import Enum\nfrom glob import glob\nfrom math import ceil\nfrom dataclasses import dataclass, field\n\nimport sklearn as sk\nimport sklearn.model_selection\n\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer, tokenizer_from_json\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nfrom typing import Tuple, List, Dict, Any, Callable, Union, Iterator, Iterable","5f20d713":"# Set the seed for random operations. \n# This let our experiments to be reproducible. \nSEED = 1234\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)","d88bc269":"EXPERIMENT_NAME = \"xception-e12-256ulstm-64em-d512\"\n\nSPLIT_VALIDATION_RATIO = 0.2\nDATA_PER_BATCH = 50\nIMAGE_SIZE = (256,256)\nEPOCHS = 12\n\nEMBEDDING_SIZE = 64\nLSTM_UNITS = 256\nLR = 1e-3\n\nMAX_WORLDS = 0","0704fe3b":"EXPERIMENT =\"{}_{}\".format(datetime.now().strftime(\"%m-%d_%H-%M\"), EXPERIMENT_NAME)","6c65d3c2":"PATH_DATA = pathlib.Path(\"\/kaggle\/input\/\") \/ \"anndl-2020-vqa\" \/ \"VQA_Dataset\"\nPATH_WORKING = pathlib.Path(\"\/kaggle\/working\")\n\nPATH_TRAINING = PATH_DATA \/ \"train_questions_annotations.json\"\nPATH_TEST = PATH_DATA \/ \"test_questions.json\"\n\nPATH_IMAGE = PATH_DATA \/ \"Images\"\n\nPATH_EXPERIMENT = PATH_WORKING \/ EXPERIMENT\n\nPATH_TOKENIZER = PATH_WORKING \/ \"tokenizer.json\"\n\nPATH_RESULT = PATH_EXPERIMENT \/ \"result.csv\"","60f8ab72":"os.makedirs(PATH_EXPERIMENT, exist_ok=True)","d2219a69":"LABELS_ANSWERS = {\n        '0': 0,\n        '1': 1,\n        '2': 2,\n        '3': 3,\n        '4': 4,\n        '5': 5,\n        'apple': 6,\n        'baseball': 7,\n        'bench': 8,\n        'bike': 9,\n        'bird': 10,\n        'black': 11,\n        'blanket': 12,\n        'blue': 13,\n        'bone': 14,\n        'book': 15,\n        'boy': 16,\n        'brown': 17,\n        'cat': 18,\n        'chair': 19,\n        'couch': 20,\n        'dog': 21,\n        'floor': 22,\n        'food': 23,\n        'football': 24,\n        'girl': 25,\n        'grass': 26,\n        'gray': 27,\n        'green': 28,\n        'left': 29,\n        'log': 30,\n        'man': 31,\n        'monkey bars': 32,\n        'no': 33,\n        'nothing': 34,\n        'orange': 35,\n        'pie': 36,\n        'plant': 37,\n        'playing': 38,\n        'red': 39,\n        'right': 40,\n        'rug': 41,\n        'sandbox': 42,\n        'sitting': 43,\n        'sleeping': 44,\n        'soccer': 45,\n        'squirrel': 46,\n        'standing': 47,\n        'stool': 48,\n        'sunny': 49,\n        'table': 50,\n        'tree': 51,\n        'watermelon': 52,\n        'white': 53,\n        'wine': 54,\n        'woman': 55,\n        'yellow': 56,\n        'yes': 57\n}","739115a9":"question_tokenizer = Tokenizer(filters='?!,.\"',num_words=MAX_WORLDS)","f84b9d9c":"with open(PATH_TRAINING) as file:\n    training_data = json.load(file)\nwith open(PATH_TEST) as file:\n    test_data = json.load(file)\n\nfor i in training_data:\n    training_data[i][\"id\"] = i\nfor i in test_data:\n    test_data[i][\"id\"] = i\n    \ntraining_data = list(training_data.values())\ntest_data = list(test_data.values())\n\ndef split_question(v):\n    v[\"question\"] = v[\"question\"].split(\" \")\n    return v\ndef add_eos(v):\n    v[\"question\"].append(\"<eos>\")\n    return v\ndef parse_answer(v):\n    v[\"answer\"] = LABELS_ANSWERS[v[\"answer\"]]\n    return v\ndef set_image_path(v):\n    v[\"image\"] = str(PATH_IMAGE \/ \"{}.png\".format(v[\"image_id\"]))\n    return v\n    \n\ntraining_data = map(split_question, training_data)\ntest_data = map(split_question, test_data)\n\ntraining_data = map(add_eos, training_data)\ntest_data = map(add_eos, test_data)\n\ntraining_data = map(parse_answer, training_data)\n\ntraining_data = map(set_image_path, training_data)\ntest_data = map(set_image_path, test_data)\n\ntraining_data = list(training_data)\ntest_data = list(test_data)","635044a6":"if PATH_TOKENIZER.exists():\n    with open(PATH_TOKENIZER) as f:\n        question_tokenizer = tokenizer_from_json(json.load(f))\n    \nelse:\n    question_tokenizer.fit_on_texts([\n                *list(map(operator.itemgetter('question'), training_data)),\n                *list(map(operator.itemgetter('question'), test_data))\n            ])\n    \n    with open(PATH_TOKENIZER, 'w', encoding='utf-8') as f:\n        f.write(json.dumps(question_tokenizer.to_json(), ensure_ascii=False))","153138ef":"def question_to_sequence(v):\n    v[\"question\"] = question_tokenizer.texts_to_sequences(v[\"question\"])\n    v[\"question\"] = functools.reduce(operator.iconcat, v[\"question\"], [])\n    return v\n\ntraining_data = map(question_to_sequence, training_data)\ntest_data = map(question_to_sequence, test_data)\n\ntraining_data = list(training_data)\ntest_data = list(test_data)","49249c4a":"max_len_question = max(len(v[\"question\"]) for v in [*training_data, *test_data])","41a0ea1c":"def pad_question(v):\n    v[\"question\"] = pad_sequences([v[\"question\"]], maxlen=max_len_question, padding='post').flatten()\n    return v\n\ntraining_data = map(pad_question, training_data)\ntest_data = map(pad_question, test_data)\n\ntraining_data = list(training_data)\ntest_data = list(test_data)","d449a569":"words_num = len(question_tokenizer.index_word) + 1","8c51656e":"class CustomDataset(tf.keras.utils.Sequence):\n    _seed = SEED\n\n    def __init__(self,\n                 data: List,\n                 data_generator_options: Dict = None,\n                 preprocessing_function: Callable[[np.array], np.array] = lambda x: x,\n                 image_size: Tuple[int, int] = (256, 256),\n                 data_per_batch: int = 1,\n                 shuffle: bool = False,\n                 is_test: bool = False\n                ):\n        '''\n        :param data: A list of data\n        :param data_generator_options: The configuration for augmentation generator\n        :param preprocessing_function: The preprocessing function is applies to the images before augumentation and resizing phases\n        :param image_size: The size of the output patches\n        :param data_per_batch: The number of data per batch, with 0 the batch will contain the whole dataset\n        :param shuffle: If it is True at each epoch the data are shuffled\n        :param is_test: Defines if it is a test dataset\n        '''\n        self._data = data\n        self._preprocessing_function = preprocessing_function\n        self._data_per_batch = data_per_batch\n        self._image_size = image_size\n        self._shuffle = shuffle\n        self._is_test = is_test\n        \n        self._transform_generator = self._get_transform_generator()\n        \n        # shuffles them if required\n        if self._shuffle:\n            np.random.shuffle(self._data)\n        \n        self._data_generator = None\n        if data_generator_options is not None:\n            # creates a new generator for image augmentation\n            self._data_generator = kr.preprocessing.image.ImageDataGenerator(**data_generator_options)\n        \n    def __len__(self):\n        if self._data_per_batch == 0:\n            return 1\n        \n        return ceil(len(self._data) \/ self._data_per_batch)\n\n    def __getitem__(self, batch_index: int) -> Union[Tuple[np.array, np.array], Tuple[Tuple[np.array, np.array], np.array]]:\n        batch_images=[]\n        batch_questions=[]\n        batch_answers=[]\n        \n        # retrieves the subset of the images in the batch\n        batch_indexes = range(len(self._data)) if self._data_per_batch == 0 else range(batch_index * self._data_per_batch,(1 + batch_index) * self._data_per_batch)\n        \n        for i in batch_indexes:\n            if len(self._data) <= i:\n                break\n            \n            # gets the i-th image and mask patches \n            image, question, answer = self._get_data(i)\n            \n            batch_images.append(image)\n            batch_questions.append(question)\n            \n            if answer is not None:\n                batch_answers.append(answer)\n        \n        # returns only the image batch if this is a prediction dataset\n        if self._is_test:\n            return [np.array(batch_images), np.array(batch_questions)]\n        \n        return [np.array(batch_images), np.array(batch_questions)], np.array(batch_answers)\n    \n    def _get_data(self, index: int) -> Tuple[np.array, np.array, Union[np.array, None]]:\n        data = self._data[index]\n        \n        # loads the image from file\n        image = kr.preprocessing.image.load_img(data[\"image\"], grayscale=False, color_mode=\"rgb\", target_size=IMAGE_SIZE, interpolation=\"nearest\")\n        \n        # converts image to array\n        image = kr.preprocessing.image.img_to_array(image)\n        \n        if self._data_generator is not None:\n            # gets a transformation\n            transf_image = next(self._transform_generator)\n\n            # applies the transformation to mask and image\n            image = self._data_generator.apply_transform(image, transf_image)\n            \n        # applies the preprocessing function to the image\n        image = self._preprocessing_function(image)\n                \n        # if this is a prediction dataset skips the processing of the mask\n        if self._is_test:\n            return image, data[\"question\"], None\n        \n        return image, data[\"question\"], data[\"answer\"]\n    \n    def on_epoch_end(self) -> None:\n        # at each epoch end shuffles the images if required \n        if self._shuffle:\n            np.random.shuffle(self._data)\n\n    def _get_transform_generator(self):\n        # retrives a pseudo-random transformation from a generator for images and masks\n        while True:\n            yield self._data_generator.get_random_transform(self._image_size, seed=CustomDataset._seed)\n            CustomDataset._seed += 1","a72570f9":"# splits training and validation set\ntraining_data, validation_data = sk.model_selection.train_test_split(training_data,\n                                              test_size=SPLIT_VALIDATION_RATIO,\n                                              random_state=SEED)\n\n# retirieves the prprocessing function for the net\npreprocessing_function = kr.applications.xception.preprocess_input\n\n# defines a set of images augmention rules\ngenerator_options = dict(\n                rotation_range=10,\n                zoom_range=0.5,\n                width_shift_range=0.1,\n                height_shift_range=0.1,\n                shear_range=0.15,\n                horizontal_flip=False,\n                fill_mode=\"constant\"\n            )\n\n# creates the training and validation dataset\nds_training = CustomDataset(\n    training_data,\n    data_generator_options=generator_options,\n    preprocessing_function=preprocessing_function,\n    image_size=IMAGE_SIZE,\n    data_per_batch=DATA_PER_BATCH,\n    shuffle=True\n)\nds_validation = CustomDataset(\n    validation_data,\n    data_generator_options=generator_options,\n    preprocessing_function=preprocessing_function,\n    image_size=IMAGE_SIZE,\n    data_per_batch=DATA_PER_BATCH,\n    shuffle=True\n)\n\n# creates the test dataset\nds_test = CustomDataset(\n    test_data,\n    preprocessing_function=preprocessing_function,\n    image_size=IMAGE_SIZE,\n    data_per_batch=DATA_PER_BATCH,\n    is_test=True\n)\n\nprint(\"{} data for training\".format(len(training_data)))\nprint(\"{} data for validation\".format(len(validation_data)))\nprint(\"{} data to test\".format(len(test_data)))","8b25c4f6":"def get_callbacks(\n                  save_model: bool = True,\n                  checkpoints: bool = False,\n                  tensorboard: bool = True,\n                  early_stopping: bool = True\n                  ) -> List[kr.callbacks.Callback]:\n    \"\"\"\n    Returns a list of desidered callbacks for the fit process\n    \"\"\"\n    callbacks = []\n    \n    if save_model:\n        os.makedirs(PATH_EXPERIMENT \/ \"model\", exist_ok=True)\n        callbacks.append(kr.callbacks.ModelCheckpoint(str(PATH_EXPERIMENT \/ \"model\"), save_best_only=True, mode='min'))\n\n    if checkpoints:\n        # if they are requesed, checkpoints will be saved in a specific experiment subdirecotry\n        os.makedirs(PATH_EXPERIMENT \/ \"checkpoints\", exist_ok=True)\n\n        callbacks.append(kr.callbacks.ModelCheckpoint(\n            str(PATH_EXPERIMENT \/ \"checkpoints\" \/ \"cp-{epoch:04d}.ckpt\")\n        ))\n\n    if tensorboard:\n        # if they are required, tensorboard files will be generated\n        os.makedirs(PATH_EXPERIMENT \/ \"tb_log\", exist_ok=True)\n\n        callbacks.append(kr.callbacks.TensorBoard(\n            PATH_EXPERIMENT \/ \"tb_log\",\n            histogram_freq=1,\n            profile_batch=0\n        ))\n    if early_stopping:\n        callbacks.append(kr.callbacks.EarlyStopping(\n            patience=5,\n            restore_best_weights=True\n        ))\n\n    return callbacks","35cf6c10":"image_input = kr.layers.Input(shape=(*IMAGE_SIZE, 3))\n\nimage_model = kr.applications.Xception(\n    weights=\"imagenet\",\n    include_top=False,\n    input_tensor=image_input\n)\nimage_model.trainable = False\n\nimage_model = kr.layers.Dense(LSTM_UNITS)(image_model.output)","7459a2fb":"question_input = kr.layers.Input(shape=(max_len_question))\n\nquestion_model = kr.layers.Embedding(words_num, EMBEDDING_SIZE, input_length=max_len_question, mask_zero=True)(question_input)\nquestion_model = tf.keras.layers.LSTM(units=LSTM_UNITS)(question_model)","c26a233d":"classificator_model = tf.keras.layers.Multiply()([image_model, question_model])\n\nclassificator_model = kr.layers.Flatten()(classificator_model)\nclassificator_model = kr.layers.Dense(128, activation=kr.activations.relu)(classificator_model)\nclassificator_model = kr.layers.Dropout(0.5)(classificator_model)\nclassificator_model = kr.layers.Dense(len(LABELS_ANSWERS), activation=kr.activations.softmax)(classificator_model)\n\nmodel = kr.Model(inputs=[image_input, question_input], outputs=classificator_model)\n\nmodel.inputs\n\nmodel.summary()","9e2dda9b":"loss = tf.keras.losses.SparseCategoricalCrossentropy()\noptimizer = tf.keras.optimizers.Adam(learning_rate=LR)\nmetrics = [\n    kr.metrics.SparseCategoricalAccuracy(name='accuracy')\n]\n\nmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics)","c9e2309f":"words_num, max_len_question, len(LABELS_ANSWERS)","b2d2ea52":"training_history = model.fit(\n    ds_training,\n    validation_data=ds_validation,\n    epochs=EPOCHS,\n    callbacks=get_callbacks()\n)","23dd8d46":"# Plot training & validation iou_score values\nplt.figure(figsize=(30, 5))\nplt.subplot(121)\nplt.plot(training_history.history['accuracy'])\nplt.plot(training_history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\n\n# Plot training & validation loss values\nplt.subplot(122)\nplt.plot(training_history.history['loss'])\nplt.plot(training_history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","1d3bbef8":"# computes the prediction exploiting the trained model\ntest_output = model.predict(ds_test)","8e43b446":"results = np.argmax(test_output, 1)\nresults = list(results)\n\nresults = map(lambda x: (x[0][\"id\"], x[1]), zip(test_data, results))\nresults = dict(results)","931896fe":"with open(PATH_RESULT, 'w') as f:\n    f.write('Id,Category\\n')\n\n    for key, value in results.items():\n        f.write(key + ',' + str(value) + '\\n')","0644e656":"### Model","291d85af":"### Dataset","ee4fb21f":"### Data preparation","250fd1b3":"# AN2DL - Third challenge\n\nThe work introduced is by the group Natural Intelligence, it concerns Visual Question Answering and it's going to be shown:\n* how the data are managed: solution for memory limitations, rearranging for training (and at inference about the testing) and preprocessing\n* mixing of architectures led to image features extraction and text understanding: Xception+LSTM\n\n## Dataset\n\nFirst thing, we chose to apply some operations on the given JSON in order to get two list of dict, one for training data and one for the test data.\nAfter separating each questions' word and postposed the special string `<eos>` , a `Tokenizer` class instance converts each question into a sequences of integer classes.\nWe found the maximum sentences' length as 22 words (`<eos>` included), so we chOose to use all the words to compose the vocabulary.\n\nNote the huge size of the given dataset, which requires an enormous amount of memory making impossible to store it entirely in the memory, to work around this limitation it has been used a custom dataset generator by`CustomDataset` class.\n\n### `CustomDataset` class\n\n`CustomDataset` extends the Keras' `Sequence` class: it allows to give an instance directly to the method `fit` of a Keras' model.\nThe class takes as parameters a list of dict with the keys `question` `answer` and `image`, an optional pre-processing function for the images and an `ImageDataGenerator` configuration.\nThis deals with split the given data in batches; when a batch is required, the`CustomDataset` instance loads in memory just the images that compose it, applying the data augmentation and\/or preprocessing (Xception's one in this application) if required.\nThis behavior solves the impossibility to load the whole dataset in the memory, because the data are load in memory only when required.\n\nDue to the nature of the model (it requires two kinds of input) the instance returns a batch in the form `(images, questions), answers`, and if `is_test` is set to `True` the answers are omitted (for the test dataset).\n\n### Dataset arranging\n\nAt this point the training data are split in training and validation datasets, respectively with a ratio of the 0.8 and 0.2 over the available data.\nThen, for training, validation and testing, instances of `CustomDataset` are properly set.\n\n## Model\n\nSince during the labs we have only seen the implementation of the CNN and LSTM singularly, a paper (https:\/\/arxiv.org\/pdf\/1505.00468v6.pdf) highlighted to let the two kind of neural networks work together: them collaboration is carried out by element-wise multiplication of the LSTM's and CNN's results.\nFor the CNN we have used a pre-trained Xception net by imagenet to handle the visual part of the overall task while for the textual part a LSTM with 512 neurons.\nAfter the concatenation between the two nets a fully-connected layer is added and finally a soft-max layer to get the answer.\n\n## Results\n\nBefore reaching this model design, we tried other configurations for the net: about the CNN we have tried to use even VGG16 or VGG19 (as proposed by the paper) net, while to analyse the questions a GRU and a Bidirectional LSTM with the same number of units.\nHowever the best performances has been driven by the Xception and the LSTM: by training over 12 epochs, we reached as max validation accuracy 0.6240.\n\n![plot](https:\/\/i.ibb.co\/ydG5x4j\/results-29-0.png)\n\nWith this model we reach a score of `0.63841` for the test dataset."}}