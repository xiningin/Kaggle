{"cell_type":{"5559e480":"code","5116be29":"code","ca0b2e4a":"code","0827fb6b":"code","47c0b4c4":"code","df8ee562":"code","4ffe5224":"code","2b547315":"code","18f0aa51":"code","d43afe7d":"code","307b94d1":"code","5553e5cb":"code","e7a75a1a":"code","41d91bfa":"code","1fec2e78":"code","74143db4":"markdown","9880f0d3":"markdown","ec9d1c52":"markdown","8c778923":"markdown","6c4a5bea":"markdown","cb373c7d":"markdown"},"source":{"5559e480":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","5116be29":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import preprocessing\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.metrics import classification_report, confusion_matrix  \nfrom sklearn.cluster import KMeans\nfrom scipy.cluster.vq import whiten","ca0b2e4a":"data = pd.read_csv(\"..\/input\/Absenteeism_at_work.csv\")\n#1st column to 13th column\nX =  data.iloc[:,range(0,13)]\n#14st column --> Labels\nY =  data.iloc[:,14]\n\n#normalising the data\nnormal_X = preprocessing.normalize(X)","0827fb6b":"#Splitting of data set\nTrain_X, Test_X, Train_y, Test_y = train_test_split(X, Y, test_size = 0.2, random_state = 239)\nTrain_y = Train_y.ravel()\nTest_y = Test_y.ravel()","47c0b4c4":"k = []\naccuracy = []\nfor i in range(int(np.sqrt(data.shape[0]))):\n    K_value = i+1\n    neigh = KNeighborsClassifier(n_neighbors = K_value, weights='uniform', algorithm='auto')\n    neigh.fit(Train_X, Train_y) \n    Pred_y = neigh.predict(Test_X)\n    #print(Pred_y)\n    k.append(K_value)\n    accuracy.append(accuracy_score(Test_y,Pred_y)*100)\n    ","df8ee562":"print(\"Accuracy of the model: \",max(accuracy))\nprint(\"K value: \",accuracy.index(max(accuracy))+1)","4ffe5224":"#graph for visualization\nplt.plot(k, accuracy)\nplt.xlabel('K Values')\nplt.ylabel('Accuracy')\nplt.title('K values vs Accuracy graph')\nplt.show()","2b547315":"clf_gini = DecisionTreeClassifier(criterion = \"gini\", random_state = 100, max_depth=3, min_samples_leaf=5)\nclf_gini.fit(Train_X, Train_y)\nPred_y_gini = clf_gini.predict(Test_X)","18f0aa51":"print(accuracy_score(Test_y,Pred_y_gini)*100)","d43afe7d":"print(confusion_matrix(Test_y,Pred_y_gini))","307b94d1":"print(classification_report(Test_y,Pred_y_gini))","5553e5cb":"clf_entropy = DecisionTreeClassifier(criterion = \"entropy\", random_state = 100, max_depth=3, min_samples_leaf=5)\nclf_entropy.fit(Train_X, Train_y)\nPred_y_entropy = clf_entropy.predict(Test_X)","e7a75a1a":"print(accuracy_score(Test_y,Pred_y_entropy)*100)","41d91bfa":"print(confusion_matrix(Test_y,Pred_y_entropy))","1fec2e78":"print(classification_report(Test_y,Pred_y_entropy))","74143db4":"**Decision Tree**\n\nCreteria -->GINI","9880f0d3":"**Required APIs**","ec9d1c52":"**Decision Tree**\n\nCreteria --> Entropy","8c778923":"**Why the value of k is chosen to be that? **\n\nWe know that the k-value should be an odd number because if it is even there are chances of ties and ambiguity as to which class to be placed in. Also k value shouldn\u2019t be a multiple of the number of classes. As a rule of thumb we know that the value of k is usually taken as square root of n.  So here we checked the accuracy value for every k in the range of [1, square root of n]. \nWe found that for k = 7, we have the highest accuracy of 49.32% for the given split of train and test data.\n\n**Comparison of models**\n\nThe accuracies we got for both the techniques, that is kNN and decision trees is pretty much the same.\nThe accuracy we got for KNN is 49.32%.\nFor decision trees , we got an accuracy of 51.35%. ( this is when we built the decision tree based on the gini index).\nKNN is generally good for continuous value inputs as it is in this case.\nIt is also  good for instances where there are less target variables and more number of data points. But here the number of target variables is high (i.e; 15) . So maybe , that\u2019s the reason we have a slightly higher accuracy in case of decision trees than in case of KNN.\n","6c4a5bea":"**Team Name  : Significantly Different**\n\nMaanvi Nunna --> 01FB16ECS187\n\nNiharika G --> 01FB16ECS229\n\nOmkar Metri --> 01FB16ECS239\n","cb373c7d":"**KNN Classificaation**"}}