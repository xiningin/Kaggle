{"cell_type":{"83fcc0b7":"code","1b7f5fb3":"code","90fb0904":"code","4f855bd4":"code","2750c302":"code","5c159bf3":"code","596f70f0":"code","9dddca8c":"code","936782ff":"code","aabd0010":"code","f7c90ee4":"code","1d46e31e":"code","c5128324":"code","2664f69e":"code","9293c27a":"code","f0f70c42":"code","ff1807c7":"code","af620a61":"code","68150737":"code","fd7f7e77":"code","ccb6515a":"code","1cbe4ff5":"code","eea86dc8":"code","4d758cb6":"code","7d611d0a":"code","2c4343a4":"code","b4d613a8":"code","06a0a3e6":"code","8299d621":"code","f7dfce52":"code","7979047b":"code","6e912c29":"code","9c929432":"code","52c39767":"code","40843127":"markdown","913785bd":"markdown","a7b12cd9":"markdown","81d1f722":"markdown","1f58bef9":"markdown","8862c30c":"markdown","65c86f63":"markdown","ebf7970e":"markdown","0f9340ee":"markdown","3acc7516":"markdown","9e748e04":"markdown","01277319":"markdown","f275cf69":"markdown","4b69713d":"markdown","7d14ce22":"markdown","651c7415":"markdown","1aca0e4f":"markdown"},"source":{"83fcc0b7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1b7f5fb3":"import plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\nimport matplotlib.pyplot as plt\nfrom colorama import Fore\n\nfrom pandas_profiling import ProfileReport\nimport seaborn as sns\nfrom sklearn import metrics\nfrom scipy import stats\nimport math\n\nfrom tqdm.notebook import tqdm\nfrom copy import deepcopy\n\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom umap import UMAP\nfrom sklearn.manifold import TSNE","90fb0904":"# Defining all our palette colours.\nprimary_blue = \"#496595\"\nprimary_blue2 = \"#85a1c1\"\nprimary_blue3 = \"#3f4d63\"\nprimary_grey = \"#c6ccd8\"\nprimary_black = \"#202022\"\nprimary_bgcolor = \"#f4f0ea\"\n\nprimary_green = px.colors.qualitative.Plotly[2]\n\nplt.rcParams['axes.facecolor'] = primary_bgcolor","4f855bd4":"colors = [primary_blue, primary_blue2, primary_blue3, primary_grey, primary_black, primary_bgcolor, primary_green]\nsns.palplot(sns.color_palette(colors))","2750c302":"plt.rcParams['figure.dpi'] = 120\nplt.rcParams['axes.spines.top'] = False\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['font.family'] = 'serif'","5c159bf3":"train_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-may-2021\/train.csv')\ntrain_df.columns = [column.lower() for column in train_df.columns]\n# train_df = train_df.drop(columns=['passengerid'])\n\ntest_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-may-2021\/test.csv')\ntest_df.columns = [column.lower() for column in test_df.columns]\n\nsubmission = pd.read_csv('\/kaggle\/input\/tabular-playground-series-may-2021\/sample_submission.csv')\nsubmission.head()\n\ntrain_df.head()","596f70f0":"feature_columns = train_df.iloc[:, 1:-1].columns.values\ntarget_column = 'target'\nfeature_columns","9dddca8c":"print(train_df.shape)\nprint(test_df.shape)","936782ff":"train_df.info()","aabd0010":"fig = px.histogram(\n    train_df, \n    x=target_column, \n    color=target_column,\n    color_discrete_sequence=px.colors.qualitative.G10,\n)\nfig.update_layout(\n    title_text='Target distribution', # title of plot\n    xaxis_title_text='Value', # xaxis label\n    yaxis_title_text='Count', # yaxis label\n    bargap=0.2, # gap between bars of adjacent location coordinates\n    paper_bgcolor=primary_bgcolor,\n    plot_bgcolor=primary_bgcolor,\n)\nfig.update_xaxes(\n    title='Target class',\n    categoryorder='category ascending',\n)\nfig.show()","f7c90ee4":"train_df.drop(columns=['id']).describe().T\\\n        .style.bar(subset=['mean'], color=px.colors.qualitative.G10[0])\\\n        .background_gradient(subset=['std'], cmap='Greens')\\\n        .background_gradient(subset=['50%'], cmap='BuGn')","1d46e31e":"columns_to_plot = ['feature_9', 'feature_14', 'feature_34']\n\nnum_rows, num_cols = 3,1\nf, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(16, 16), facecolor=primary_bgcolor)\nf.suptitle('Distribution of Features', fontsize=20, fontweight='bold', fontfamily='serif', x=0.13)\n\n\nfor index, column in enumerate(train_df[columns_to_plot].columns):\n    i,j = (index \/\/ num_cols, index % num_cols)\n    g = sns.kdeplot(train_df.loc[train_df[target_column] == 'Class_1', column], color=px.colors.qualitative.G10[1], shade=True, ax=axes[i])\n    g = sns.kdeplot(train_df.loc[train_df[target_column] == 'Class_2', column], color=px.colors.qualitative.G10[0], label=\"Skew: %.2f\"%(train_df[column].skew()), shade=True, ax=axes[i])\n    g = g.legend(loc=\"best\")\n    sns.kdeplot(train_df.loc[train_df[target_column] == 'Class_3', column], color=px.colors.qualitative.G10[3], shade=True, ax=axes[i])\n    sns.kdeplot(train_df.loc[train_df[target_column] == 'Class_4', column], color=px.colors.qualitative.G10[2], shade=True, ax=axes[i])\n\n# f.delaxes(axes[-1, -1])\nplt.tight_layout()\nplt.show()","c5128324":"num_rows, num_cols = 10,5\nf, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(20, 30))\nf.suptitle('Distribution of Features', fontweight='bold', fontfamily='serif')\n\nfor index, column in enumerate(feature_columns):\n    i,j = (index \/\/ num_cols, index % num_cols)\n\n    sns.kdeplot(train_df.loc[train_df[target_column] == 'Class_1', column], color=px.colors.qualitative.G10[1], shade=True, ax=axes[i,j])\n    sns.kdeplot(train_df.loc[train_df[target_column] == 'Class_2', column], color=px.colors.qualitative.G10[0], shade=True, ax=axes[i,j])\n    sns.kdeplot(train_df.loc[train_df[target_column] == 'Class_3', column], color=px.colors.qualitative.G10[3], shade=True, ax=axes[i,j])\n    sns.kdeplot(train_df.loc[train_df[target_column] == 'Class_4', column], color=px.colors.qualitative.G10[2], shade=True, ax=axes[i,j])\n\n#f.delaxes(axes[3, 2])\nplt.tight_layout()\nplt.show()\n","2664f69e":"corr = train_df[feature_columns].corr().abs()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\nfig, ax = plt.subplots(figsize=(12, 12), facecolor=primary_bgcolor)\nax.text(-1.1, -0.7, 'Correlation between the Features', fontsize=20, fontweight='bold', fontfamily='serif')\nax.text(-1.1, 0.2, 'There is no features that pass 0.02 correlation within each other', fontsize=13, fontweight='light', fontfamily='serif')\n\n\n# plot heatmap\nsns.heatmap(corr, mask=mask, annot=False, fmt=\".2f\", cmap='coolwarm',\n            cbar_kws={\"shrink\": .8}, vmin=0, vmax=0.05)\n# yticks\nplt.yticks(rotation=0)\nplt.show()\n","9293c27a":"# Take a subsample to reduce computational cost\ntrain_sub = train_df.sample(1000, random_state=2021)","f0f70c42":"umap_2d = UMAP(n_components=2, random_state=2021)\nproj_2d = umap_2d.fit_transform(train_sub[feature_columns])","ff1807c7":"fig_2d = px.scatter(\n    proj_2d, x=0, y=1, \n    labels={'color': 'target'},\n    color=train_sub.target,\n    color_discrete_sequence=px.colors.qualitative.G10,\n)\nfig_2d.update_layout(\n    title='<span style=\"font-size:24px; font-family:Serif\">UMAP<\/span>',\n)\n\nfig_2d.show()","af620a61":"tsne = TSNE(n_components=2, random_state=2021)\nprojections = tsne.fit_transform(train_sub[feature_columns])","68150737":"fig = px.scatter(\n    projections, x=0, y=1,\n    labels={'color': 'target'},\n    color=train_sub.target,\n    color_discrete_sequence=px.colors.qualitative.G10,\n)\nfig.update_layout(\n    title='<span style=\"font-size:24px; font-family:Serif\">t-SNE<\/span>',\n)\n\nfig.show()","fd7f7e77":"tsne = TSNE(n_components=3, random_state=2021)\nprojections = tsne.fit_transform(train_sub[feature_columns], )","ccb6515a":"fig = px.scatter_3d(\n    projections, x=0, y=1, z=2,\n    color=train_sub.target, labels={'color': 'target'}\n)\nfig.update_traces(marker_size=8)\nfig.show()","1cbe4ff5":"import h2o\nfrom h2o.automl import H2OAutoML\n\nh2o.init()","eea86dc8":"%%time\n\ntrain_hf = h2o.H2OFrame(train_df.copy())\ntest_hf = h2o.H2OFrame(test_df.copy())","4d758cb6":"train_hf[target_column] = train_hf[target_column].asfactor()","7d611d0a":"%%time\n\naml = H2OAutoML(\n    seed=2021, \n    max_runtime_secs=10 * 60,\n    nfolds = 3,\n    exclude_algos = [\"DeepLearning\"]\n)\n\naml.train(\n    x=list(feature_columns), \n    y=target_column, \n    training_frame=train_hf\n)","2c4343a4":"lb = aml.leaderboard \nlb.head(rows = lb.nrows)","b4d613a8":"%%time\n\npreds = aml.predict(h2o.H2OFrame(test_df[feature_columns].copy()))\npreds_df = h2o.as_list(preds)\npreds_df\n\nsubmission[['Class_1', 'Class_2', 'Class_3', 'Class_4']] = preds_df[['Class_1', 'Class_2', 'Class_3', 'Class_4']]\nsubmission.to_csv('h2o_automl_300s.csv', index=False)\nsubmission.head()","06a0a3e6":"pip install -U lightautoml","8299d621":"# Imports from our package\nfrom lightautoml.automl.presets.tabular_presets import TabularAutoML, TabularUtilizedAutoML\nfrom lightautoml.tasks import Task\nfrom sklearn.metrics import log_loss","f7dfce52":"N_THREADS = 4 # threads cnt for lgbm and linear models\nN_FOLDS = 5 # folds cnt for AutoML\nRANDOM_STATE = 2021 # fixed random state for various reasons\nTEST_SIZE = 0.2 # Test size for metric check\nTIMEOUT = 60 * 60 # Time in seconds for automl run","7979047b":"le = LabelEncoder()\ntrain_df[target_column] = le.fit_transform(train_df[target_column])","6e912c29":"%%time \n\ntask = Task('multiclass',)\n\nroles = {\n    'target': target_column,\n    'drop': ['id'],\n}\n\n\nautoml = TabularUtilizedAutoML(task = task, \n                               timeout = TIMEOUT,\n                               cpu_limit = N_THREADS,\n                               reader_params = {'n_jobs': N_THREADS},\n)\n\noof_pred = automl.fit_predict(train_df, roles = roles)\nprint('oof_pred:\\n{}\\nShape = {}'.format(oof_pred[:10], oof_pred.shape))","9c929432":"%%time\n\ntest_pred = automl.predict(test_df)\nprint('Prediction for test set:\\n{}\\nShape = {}'.format(test_pred[:5], test_pred.shape))\n\nprint('Check scores...')\nprint('OOF score: {}'.format(log_loss(train_df[target_column].values, oof_pred.data)))","52c39767":"submission.iloc[:, 1:] = test_pred.data\nsubmission.to_csv('lightautoml_v1_1hour.csv', index = False)","40843127":"# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:250%; text-align:center; border-radius: 15px 50px;\">Tabular Playground Series \ud83d\udcda - May 2021 \ud83d\udcc8<\/p>\n\nThe dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the category on an eCommerce product given various attributes about the listing. Although the features are anonymized, they have properties relating to real-world features.\n\nThis competition is a classification problem that classifies **4 classes and 50 integer features**.","913785bd":"The mean and the standard deviation have a large variability that also seem to be related. Aparently, when the mean increase, the standard deviation also does.\n\nIn the case of the median, its seems to be $0$ for most of the cases, but in 2 cases, the med\u00edan is $1$.","a7b12cd9":"As you can see, I have mapped the values (all are absolute values) between $0$ and $0.05$ and any value overpass the $0.02$ mark.","81d1f722":"<a id='1.1'><\/a>\n[back to top](#table-of-contents)\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">1.1 Target Variable<\/p>\n\nNow we are going to take a look at the target column to see how balanced the dataset is. This is a important metric to understand if we have to resample or not.","1f58bef9":"<a id='2.1'><\/a>\n[back to top](#table-of-contents)\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">2.1 UMAP<\/p>\n\nUniform Manifold Approximation and Projection (UMAP) is a dimension reduction technique that can be used for visualisation similarly to t-SNE, but also for general non-linear dimension reduction. The algorithm is founded on three assumptions about the data\n\n1.     The data is uniformly distributed on Riemannian manifold;\n2.     The Riemannian metric is locally constant (or can be approximated as such);\n3.     The manifold is locally connected.\n\nFrom these assumptions it is possible to model the manifold with a fuzzy topological structure. The embedding is found by searching for a low dimensional projection of the data that has the closest possible equivalent fuzzy topological structure.\n\nRef: https:\/\/umap-learn.readthedocs.io\/en\/latest\/","8862c30c":"<a id='2'><\/a>\n[back to top](#table-of-contents)\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">2. Dimension Reduction<\/p>","65c86f63":"<a id='5'><\/a>\n[back to top](#table-of-contents)\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">5. LightAutoML<\/p>","ebf7970e":"<a id='1.3'><\/a>\n[back to top](#table-of-contents)\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">1.3 Feature distribution by Class<\/p>\n\nAs there are too many variables and plotting all of them will carry us to an unleigble charts, i will plot just 3 of them to see how they behave.","0f9340ee":"<a id='1.4'><\/a>\n[back to top](#table-of-contents)\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">1.4 Correlation Analysis<\/p>\n\nNow we are going to see how correlated the features are and how correlated are they with the target variable.","3acc7516":"<a id='4'><\/a>\n[back to top](#table-of-contents)\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">4. H2O AutoML<\/p>","9e748e04":"<a id='1'><\/a>\n[back to top](#table-of-contents)\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">1. Data visualization: First Overview \ud83d\udcca<\/p>","01277319":"As we can see, there are no missing values in the dataset and all the features are integer, so we can forget about missings study!","f275cf69":"<a id='2.2'><\/a>\n[back to top](#table-of-contents)\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">2.2 t-SNE<\/p>\n\n**t-distributed stochastic neighbor embedding (t-SNE)** is a statistical method for visualizing high-dimensional data by giving each datapoint a location in a two or three-dimensional map. It is based on Stochastic Neighbor Embedding originally developed by Sam Roweis and Geoffrey Hinton, where Laurens van der Maaten proposed the t-distributed variant. It is a nonlinear dimensionality reduction technique well-suited for embedding high-dimensional data for visualization in a low-dimensional space of two or three dimensions. Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points with high probability.\n\nThe t-SNE algorithm comprises two main stages. First, t-SNE constructs a probability distribution over pairs of high-dimensional objects in such a way that similar objects are assigned a higher probability while dissimilar points are assigned a lower probability. Second, t-SNE defines a similar probability distribution over the points in the low-dimensional map, and it minimizes the Kullback\u2013Leibler divergence (KL divergence) between the two distributions with respect to the locations of the points in the map. While the original algorithm uses the Euclidean distance between objects as the base of its similarity metric, this can be changed as appropriate.\n\nRef: https:\/\/en.wikipedia.org\/wiki\/T-distributed_stochastic_neighbor_embedding","4b69713d":"As we can see, the distribution of the features based on the class value seems to be the same. So we can go ahead with no problems.\n\nIt's important to mark that all the **features seems to be left skewed**!","7d14ce22":"As we can see, `Class_2` is the majority class but there is not much difference between classes.","651c7415":"<a id='table-of-contents'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">Table of Content<\/p>\n\n* [1. Data visualization: Survival Analysis \ud83d\udcca](#1)\n    * [1.1 Target](#1.1)\n    * [1.2 General Feature Analysis](#1.2)\n    * [1.3 Feature distribution by Class](#1.3)\n    * [1.4 Correlation Analysis](#1.4)\n* [2. Dimension Reduction](#2)\n    * [2.1 UMAP](#2.1)\n    * [2.2 t-SNE](#2.2)\n* [3. Feature Engineering](#3)\n* [4. H2O Automl](#4)\n* [5. LightAutoML](#5)","1aca0e4f":"<a id='1.2'><\/a>\n[back to top](#table-of-contents)\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">1.2 General feature analysis<\/p>\n\nFirst of all we will take a look at the train dataset info in terms of features values distribution."}}