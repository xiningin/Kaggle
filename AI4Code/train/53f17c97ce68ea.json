{"cell_type":{"4396db49":"code","a89e9ae4":"code","eb2c04f1":"code","2d3d095f":"code","7b400965":"code","4e44fa18":"code","1f487b0d":"code","7d4b1fab":"code","0d6dc78d":"code","7fc0da4c":"code","4d128988":"code","6267f7f2":"code","53a9e940":"code","b4666a5b":"code","68d84012":"code","52033947":"code","80428f6b":"code","5352a00b":"code","3bfcca2f":"code","a7f580fe":"code","9e764fe5":"code","31d48c27":"code","5f674477":"code","a9abd9e7":"code","c1cf295b":"code","2d5c162e":"code","f803d76f":"markdown","29cef74f":"markdown","988fd8fb":"markdown","df97e845":"markdown","ed0485f4":"markdown","d5d572d0":"markdown","fcbead8b":"markdown","3b4a7a64":"markdown","8003b47e":"markdown","89201995":"markdown","664d4a6f":"markdown","6d65dad1":"markdown","bc82a7c1":"markdown","3198d6ce":"markdown","83ff04e7":"markdown","3b2a7384":"markdown","c01ea44c":"markdown","a7bba3f9":"markdown","1a2c8ccd":"markdown","692f72ee":"markdown","059220d0":"markdown"},"source":{"4396db49":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n%matplotlib inline","a89e9ae4":"df= pd.read_csv(\"..\/input\/avocado.csv\")","eb2c04f1":"df.head()","2d3d095f":"df.drop('Unnamed: 0',axis=1,inplace=True)","7b400965":"df.head()","4e44fa18":"df.info()","1f487b0d":"df['Date']=pd.to_datetime(df['Date'])\ndf['Month']=df['Date'].apply(lambda x:x.month)\ndf['Day']=df['Date'].apply(lambda x:x.day)","7d4b1fab":"df.head()","0d6dc78d":"byDate=df.groupby('Date').mean()\nplt.figure(figsize=(12,8))\nbyDate['AveragePrice'].plot()\nplt.title('Average Price')","7fc0da4c":"plt.figure(figsize=(12,6))\nsns.heatmap(df.corr(),cmap='coolwarm',annot=True)","4d128988":"df['region'].nunique()\n\n","6267f7f2":"df['type'].nunique()","53a9e940":"df_final=pd.get_dummies(df.drop(['region','Date'],axis=1),drop_first=True)","b4666a5b":"df_final.head()","68d84012":"df_final.tail()","52033947":"X=df_final.iloc[:,1:14]\ny=df_final['AveragePrice']\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)","80428f6b":"from sklearn.linear_model import LinearRegression\nlr=LinearRegression()\nlr.fit(X_train,y_train)\npred=lr.predict(X_test)","5352a00b":"from sklearn import metrics\nprint('MAE:', metrics.mean_absolute_error(y_test, pred))\nprint('MSE:', metrics.mean_squared_error(y_test, pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, pred)))","3bfcca2f":"plt.scatter(x=y_test,y=pred)","a7f580fe":"from sklearn.tree import DecisionTreeRegressor\ndtr=DecisionTreeRegressor()\ndtr.fit(X_train,y_train)\npred=dtr.predict(X_test)","9e764fe5":"plt.scatter(x=y_test,y=pred)\nplt.xlabel('Y Test')\nplt.ylabel('Predicted Y')","31d48c27":"print('MAE:', metrics.mean_absolute_error(y_test, pred))\nprint('MSE:', metrics.mean_squared_error(y_test, pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, pred)))","5f674477":"from sklearn.ensemble import RandomForestRegressor\nrdr = RandomForestRegressor()\nrdr.fit(X_train,y_train)\npred=rdr.predict(X_test)","a9abd9e7":"print('MAE:', metrics.mean_absolute_error(y_test, pred))\nprint('MSE:', metrics.mean_squared_error(y_test, pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, pred)))","c1cf295b":"sns.distplot((y_test-pred),bins=50)","2d5c162e":"data = pd.DataFrame({'Y Test':y_test , 'Pred':pred},columns=['Y Test','Pred'])\nsns.lmplot(x='Y Test',y='Pred',data=data,palette='rainbow')\ndata.head()","f803d76f":"well as a first observation we can see that we are lucky, we dont have any missing values (18249 complete data) and 13 columns.\nNow let's do some Feature Engineering on the Date Feature so we can be able to use the day and the month columns in building our machine learning model later. ( I didn't mention the year because its already there in data frame)","29cef74f":"The RMSE is low so we can say that we do have a good model, but lets check to be more sure.\nLets plot the y_test vs the predictions","988fd8fb":"Read in the Avocado Prices csv file as a DataFrame called df","df97e845":"Well as we can see the RMSE is lower than the two previous models, so the RandomForest Regressor is the best model in this case.","ed0485f4":"Cool right? now lets have an idea about the relationship between our Features(Correlation)","d5d572d0":"Now our data are ready! lets apply our model which is going to be the Linear Regression because our Target variable 'AveragePrice'is continuous.\nLet's now begin to train out regression model! We will need to first split up our data into an X array that contains the features to train on, and a y array with the target variable","fcbead8b":"Now lets do some plots!! \nI'll start by plotting the Avocado's Average Price  through the Date column","3b4a7a64":"In this study, we will try to see if we can predict the Avocado\u2019s Average Price based on different features.  . The features are different (Total Bags,Date,Type,Year,Region\u2026).\n\nThe variables of the dataset are the following:\n\n* Categorical: \u2018region\u2019,\u2019type\u2019\n* Date: \u2018Date\u2019\n* Numerical:\u2018Unamed: 0\u2019,\u2019Total Volume\u2019, \u20184046\u2019, \u20184225\u2019, \u20184770\u2019, \u2018Total Bags\u2019, \u2018Small Bags\u2019,\u2019Large Bags\u2019,\u2019XLarge Bags\u2019,\u2019Year\u2019\n* Target:\u2018AveragePrice\u2019\n\nThe unclear numerical variables terminology is explained in the next section:\n\n* \u2018Unamed: 0\u2019 : Its just a useless index feature that will be removed later\n* ,\u2019Total Volume\u2019 : Total sales volume of avocados\n* \u20184046\u2019 : Total sales volume of  Small\/Medium Hass Avocado\n* \u20184225\u2019 : Total sales volume of Large Hass Avocado\n* \u20184770\u2019 : Total sales volume of Extra Large Hass Avocado\n* \u2018Total Bags\u2019: Total number of Bags sold\n* \u2018Small Bags\u2019: Total number of Small Bags sold\n* Large Bags\u2019: Total number of Large Bags sold\n* \u2018XLarge Bags\u2019: Total number of XLarge Bags sold\n\n**So lets start by importing our usual suspects !!**\n","8003b47e":"Lets try working with the  DecisionTree Regressor model\n","89201995":"Nice, here we can see that we nearly have a straigt line, in other words its better than the Linear regression model, and to be more sure lets check the RMSE","664d4a6f":"as we can see we have 54 regions and 2 unique types, so it's going to be easy to to transform the type feature to dummies, but for the region its going to be a bit complexe so I decided to drop the entire column.\nI will drop the Date Feature as well because I already have 3 other columns for the Year, Month and Day.","6d65dad1":"Lets check our data head again to make sure that the Feature Unnamed:0 is removed","bc82a7c1":"The Feature \"Unnamed:0\" is just a representation of the indexes, so it's useless to keep it, lets remove it !","3198d6ce":"Very Nice, our RMSE is lower than the previous one we got with Linear Regression. ok now I am going to try one last model to see if I can improve my predictions for this data which is the RandomForestRegressor","83ff04e7":"As we can from the heatmap above, all the Features are not corroleted with the Average Price column, instead most of them are correlated with each other.\nSo now I am bit worried because that will not help us get a good model. Lets try and see.\nFirst we have to do some Feature Engineering on the categorical Features : region and type","3b2a7384":"Creating and Training the Model","c01ea44c":"Lets check our data head:","a7bba3f9":"Lets check the head to see what we have done:","1a2c8ccd":"Notice here that our residuals looked to be normally distributed and that's really a good sign which means that our model was a correct choice for the data. ","692f72ee":"As we can see that we dont have a straigt line so I am not sure that this is the best model we can apply on our data","059220d0":"Great! now lets use the info() methode to get an a general idea about our data:"}}