{"cell_type":{"a2da6fc9":"code","f356d50f":"code","664978dd":"code","f5e3351e":"code","76b4d8fe":"code","15745f8b":"code","7a0986fc":"code","936f1697":"code","71d8d61d":"code","8877c34d":"code","b721b2a7":"code","ab48f251":"code","d35bdd7c":"code","f1fde2e5":"code","bb545749":"code","c0e4e741":"code","593020b8":"code","4b690e4a":"code","7ab8ba6b":"code","f0bf3966":"code","7852f5d3":"code","98e6a99a":"code","7416aed0":"code","494c7ced":"code","4cf7a44f":"code","5ec28cfb":"code","dd7631b2":"code","fd982683":"code","e1b17a50":"code","d818a7f8":"code","37fb677f":"code","af262322":"code","4f347ecb":"code","8e18de5b":"code","13aad47a":"code","d100309c":"code","9ced078a":"code","5d58c545":"markdown","245122d3":"markdown","87c994fa":"markdown","6cc63361":"markdown","37c55c9e":"markdown","1e2e8035":"markdown","2096373b":"markdown","f88679f4":"markdown","e1d1759f":"markdown","cd584a4b":"markdown","80f84388":"markdown","55bf8af4":"markdown","97e26b20":"markdown","327f127e":"markdown","7bf103e3":"markdown","82a761c6":"markdown","ec1d8fef":"markdown","e50907b2":"markdown","f0201599":"markdown","358841db":"markdown","70c15cbf":"markdown","7979080c":"markdown","ebfe290f":"markdown"},"source":{"a2da6fc9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f356d50f":"data = pd.read_csv(\"\/kaggle\/input\/graduate-admissions\/Admission_Predict_Ver1.1.csv\")\ndata.head()","664978dd":"data.tail()","f5e3351e":"data.info()","76b4d8fe":"data=data.drop(\"Serial No.\",axis=1)","15745f8b":"data.head()","7a0986fc":"data.columns","936f1697":"data.columns = [\"gre_score\",\"toefl_score\",\"uni_rating\",\"sop\",\"lor\",\"cgpa\",\"research\",\"admit_chance\"]","71d8d61d":"data.columns","8877c34d":"data.describe()","b721b2a7":"corr = data.corr()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\nf, ax = plt.subplots(figsize=(12, 8))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, annot=True, mask=mask, cmap=cmap, ax=ax,)\nplt.show()","ab48f251":"def modify(feature):\n    if feature[\"admit_chance\"] >= 0.7:\n        return 1\n    return 0 \n\ndata[\"admit_chance\"] = data.apply(modify,axis=1)\ndata.head()","d35bdd7c":"sns.countplot(x = \"admit_chance\",data=data,palette=\"RdBu\")\nplt.show()","f1fde2e5":"c = sns.FacetGrid(data,col=\"admit_chance\",height=6)\nc.map(sns.distplot,\"gre_score\",bins=25)\nplt.show()\ndata.groupby(\"admit_chance\")[\"gre_score\"].mean()","bb545749":"c = sns.FacetGrid(data,col=\"admit_chance\",height=6)\nc.map(sns.distplot,\"toefl_score\",bins=25)\nplt.show()\ndata.groupby(\"admit_chance\")[\"toefl_score\"].mean()","c0e4e741":"c = sns.FacetGrid(data,col=\"admit_chance\",height=6)\nc.map(sns.distplot,\"cgpa\",bins=25)\nplt.show()\ndata.groupby(\"admit_chance\")[\"cgpa\"].mean()","593020b8":"sns.distplot(data[\"cgpa\"])\nplt.show()","4b690e4a":"sns.catplot(x=\"research\",y=\"admit_chance\",data=data,kind=\"bar\")\nplt.show()","7ab8ba6b":"sns.catplot(x=\"uni_rating\",y=\"admit_chance\",data=data,kind=\"bar\")\nplt.show()","f0bf3966":"def modify(feature):\n    if feature[\"uni_rating\"] >= 3:\n        return 1\n    return 0 \n\ndata[\"uni_rating\"] = data.apply(modify,axis=1)\ndata.head()","7852f5d3":"sns.catplot(x=\"uni_rating\",y=\"admit_chance\",data=data,kind=\"bar\")\nplt.show()","98e6a99a":"sns.catplot(x=\"sop\",y=\"admit_chance\",data=data,kind=\"bar\")\nplt.show()","7416aed0":"def modify(feature):\n    if feature[\"sop\"] >= 3.0:\n        return 1\n    return 0 \n\ndata[\"sop\"] = data.apply(modify,axis=1)\ndata.head()","494c7ced":"sns.catplot(x=\"sop\",y=\"admit_chance\",data=data,kind=\"bar\")\nplt.show()","4cf7a44f":"sns.catplot(x=\"lor\",y=\"admit_chance\",data=data,kind=\"bar\")\nplt.show()","5ec28cfb":"def modify(feature):\n    if feature[\"lor\"] >= 3.0:\n        return 1\n    return 0 \n\ndata[\"lor\"] = data.apply(modify,axis=1)\ndata.head()","dd7631b2":"sns.catplot(x=\"lor\",y=\"admit_chance\",data=data,kind=\"bar\")\nplt.show()","fd982683":"sns.scatterplot(data = data, x =\"gre_score\", y=\"toefl_score\",hue=\"admit_chance\")\nplt.show()","e1b17a50":"sns.scatterplot(data = data, x =\"gre_score\", y=\"cgpa\",hue=\"admit_chance\")\nplt.show()","d818a7f8":"sns.scatterplot(data = data, x =\"toefl_score\", y=\"cgpa\",hue=\"admit_chance\")\nplt.show()","37fb677f":"from sklearn.model_selection import train_test_split,GridSearchCV,StratifiedKFold\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier,VotingClassifier\nimport xgboost as xgb\nfrom xgboost import XGBClassifier","af262322":"x = data.drop(\"admit_chance\",axis = 1)\ny = data[\"admit_chance\"]\n\nscaler = StandardScaler()\nx = scaler.fit_transform(x)","4f347ecb":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 42)\n\nprint(\"x_train:\", len(x_train))\nprint(\"x_test:\", len(x_test))\nprint(\"y_train:\", len(y_train))\nprint(\"y_test:\", len(y_test))","8e18de5b":"logreg = LogisticRegression(max_iter=500)\n\nlogreg.fit(x_train,y_train)\n\nprint(\"Train Accuracy:\", logreg.score(x_train,y_train))\nprint(\"Test Accuracy:\", logreg.score(x_test,y_test))","13aad47a":"classifier = [KNeighborsClassifier(),\n              DecisionTreeClassifier(random_state=42),\n              LogisticRegression(random_state=42),\n              SVC(random_state=42),\n              RandomForestClassifier(random_state=42),\n              XGBClassifier(random_state=42, objective=\"binary:logistic\")]\n\nknn_params = {\"n_neighbors\": np.linspace(1,19,10,dtype=int),\n                 \"weights\": [\"uniform\",\"distance\"],\n                 \"metric\": [\"euclidean\",\"manhattan\",\"minkowski\"]}\n\ndt_params = {\"min_samples_split\" : range(10,500,20),\n                \"max_depth\": range(1,20,2)} \n\nlr_params = {\"C\":np.logspace(-3,3,7),\n             \"penalty\": [\"l1\",\"l2\"]}\n\nsvm_params = {\"kernel\" : [\"rbf\"],\n              \"gamma\": [0.001, 0.01, 0.1, 1],\n              \"C\": [1,10,50,100,200,300,1000]}\n\nrf_params = {\"max_features\": [1,3,10],\n                \"min_samples_split\":[2,3,10],\n                \"min_samples_leaf\":[1,3,10],\n                \"bootstrap\":[False],\n                \"n_estimators\":[100,300],\n                \"criterion\":[\"gini\"]}\n\nxgb_params = {\"learning_rate\":[0.01,0.1,1],\n              \"n_estimators\":[50,100,150],\n              \"max_depth\":[3,5,7],\n              \"gamma\":[1,2,3,4]}\n\nclassifier_params = [knn_params,\n                     dt_params,\n                     lr_params,\n                     svm_params,\n                     rf_params,\n                     xgb_params]","d100309c":"cv_result = []\nbest_estimators = []\nfor i in range(len(classifier)):\n    clf = GridSearchCV(estimator = classifier[i], param_grid = classifier_params[i],\n                       cv = StratifiedKFold(n_splits=10),scoring=\"accuracy\",n_jobs= -1, verbose= 1)\n    clf.fit(x_train,y_train)\n    cv_result.append(clf.best_score_)\n    best_estimators.append(clf.best_estimator_)\n    print(cv_result[i])","9ced078a":"cv_results = pd.DataFrame({\"ML Models\":[\"KNeighborsClassifier\",\n                                        \"Decision Tree\",\n                                        \"Logistic Regression\",\n                                        \"SVM\",\n                                        \"Random Forest\",\n                                        \"XGBClassifier\"],\n                           \"Cross Validation Means\":cv_result})\nprint(cv_results)","5d58c545":"### Hyperparameter tuning and GridSearchCV","245122d3":"* We have some exceptions like the little blue dot in the middle, but rest is expected. Students that have higher gre score usually have high toefl scores.","87c994fa":"* So we have 82% test accuracy, let's see if we can get it up 90%.","6cc63361":"* Other then research, all of our features impact chance of admit more then 0.6 rate. And most effective is cgpa with 0.88.\n* And we have some nice correlations between our features. These are:\n* toefl and gre score\n* cgpa and gre score\n* and cgpa and toefl score\n* These all have correlation higher than 0.8.\n* We'll take a look at all these correlations.","37c55c9e":"### Modeling","1e2e8035":"* Same trend goes for here too.","2096373b":"* We can see how gre scores distributed around 300 - 310 for admit chance = 0 \n* And of course it is higher for admit chance = 1, we have peak around 325.","f88679f4":"* But before we dive into the features, i'll seperate the chance of admit to two classes:\n    * if it is 0.7 or above it's 1\n    * if lower it's 0","e1d1759f":"* So here we have a winner: Logistic Regression with 87.7% accuracy.\n* We have used many classifiers but LR outperformed all.\n* If you like my notebook please upvote and if i have mistakes please tell me.","cd584a4b":"* First i want to fit a logistic regression model and then tune the parameters and do grid search.","80f84388":"* Same goes for here too, except we have not much outliers in here. Students with higher cgpa (higher than 9) usually admitted to university. ","55bf8af4":"### Visualization","97e26b20":"* Same goes for here too. If statement of purpose rating is lower then 3 it's highly unlikely that you'll accepted. So im going to group 1 to 2.5 as 0 and the rest as 1.","327f127e":"* Most of those who admitted's scores are around 110 and that is what visualization's tells us.","7bf103e3":"* I would like to add some informations about our features and what do they mean.\n* GRE Scores (out of 340)\n* TOEFL Scores (out of 120)\n* University Rating (out of 5)\n* Statement of Purpose and Letter of Recommendation Strength (out of 5)\n* Undergraduate GPA (out of 10)\n* Research Experience (either 0 or 1)\n* Chance of Admit (ranging from 0 to 1)","82a761c6":"* First lets look at correlation matrix.","ec1d8fef":"* We have no missing values, this is a good thing.\n* We already have index so serial no is unnecessary at this point, so i am just going to drop it.","e50907b2":"* We have some exceptions here but same goes for letter of recommendation too. Lower then 0 to 2.5 as 0 and rest is 1.","f0201599":"* Now we can look at distributions of our data.","358841db":"* We have to fix the column names, if you look at \"LOR\" and \"Chance of Admit\" you'll see there is a space in the end. \n* And we dont want space between words, so i am going to add underscore between words, and switch all uppercase letters to lowercase.","70c15cbf":"* Uni ratings 1 and 2 have very low chance of admit. I am going to group 1-2 as 0 and the rest as 1. ","7979080c":"* Now we checked and grouped our features. We can take a look at correlations between cgpa, toefl and gre scores.\n","ebfe290f":"* Most of who didnt have research experience have low chance of admit as expected."}}