{"cell_type":{"cca442d1":"code","62d39a8c":"code","83988ba3":"code","d08e4c42":"code","9374d115":"code","8208f111":"code","4bcf9149":"code","f78f0500":"code","56e4a779":"code","7c08f582":"code","4a4e04be":"code","5d568e1d":"code","3caf2110":"code","2243603b":"markdown","0d974b8f":"markdown","92384a5a":"markdown","31669844":"markdown","74b9cb2a":"markdown","33208ab8":"markdown","5ddb5fa3":"markdown","a8c195c4":"markdown","a0d62ed4":"markdown"},"source":{"cca442d1":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport lightgbm as lgb\nimport numpy as np\nimport matplotlib.pyplot as plt\n","62d39a8c":"df = pd.read_csv('..\/input\/multipleChoiceResponses.csv')\n\nquestions = df.iloc[0,:]\nanswers = df.iloc[1:,:]\n\n","83988ba3":"def tofloat(row, colname, replacement):\n    try:\n        row[colname] = float(row[colname])\n    except:\n        row[colname] = replacement\n    return row\n\ndef tryfloat(row, columns):\n    # try to make everything a float that does not resist with force\n    for col in columns:\n        try:\n            row[col] = float(row[col])\n        except:\n            pass\n    return row\n\ndef replace_by_middle(row, colname):\n    # try to replace range by middle value. It gets kind of messy to deal with the different notations in these columns\n    try:\n        row[colname] = float(row[colname])\n        return row\n    except:\n        pass\n    if row[colname] == 'NaN':\n        row[colname] = -999999\n        return row\n    try:\n        row[colname] = float(row[colname].split('+')[0])\n        return row\n    except:\n        pass \n    row[colname] = row[colname].replace(',000','')\n    if row[colname] == 'I do not wish to disclose my approximate yearly compensation':\n        row[colname] = -999999\n        return row\n    try:\n        split = row[colname].split('-')\n        row[colname] = (float(split[0])+float(split[1]))\/2\n    except:\n        #print('Mapping ' + str(row[colname]) + ' to low value')\n        row[colname] = -999999\n    return row\n\n# time tends to show up prominently, in feature importance. this does not look like a useful result\nanswers = answers.drop('Time from Start to Finish (seconds)', axis = 1)\ntflambda = lambda x: tryfloat(x, answers.columns)\nanswers = answers.apply(tflambda, axis = 1)\n\nfor column in answers.columns:\n    if '_OTHER_TEXT' in column:\n        conversion = lambda x: tofloat(x,column,-999999)\n        answers = answers.apply(conversion, axis=1)\n\nfor colname in ['Q2','Q8','Q9']:\n    conversion = lambda x: replace_by_middle(x,colname)\n    answers = answers.apply(conversion, axis=1)\n\nanswers = answers[answers['Q9'] > 0] # dismiss where no answer was provided\n","d08e4c42":"# remove were now answer is given\nanswers = answers[answers['Q9'] > 0]\n# The income distribution is pretty skewed, we will cut it off at the price which is still reasonably far away from the median.\n# This way the results are more meaningful for the biggest pack of people living around 50K\/year\nanswers = answers[answers['Q9'] < 250]\n# hist of incomes\nplt.figure()\nanswers['Q9'].plot.hist()","9374d115":"df_train = pd.get_dummies(answers)","8208f111":"y = df_train['Q9']\nX = df_train.drop('Q9', axis = 1)\nn_train = int(X.shape[0] * 0.7)\nX_train, y_train = X.iloc[:n_train], y.iloc[:n_train]\nX_valid, y_valid = X.iloc[n_train:], y.iloc[n_train:]","4bcf9149":"\nm = lgb.LGBMRegressor(objective = 'regression_l1',\n                      num_boost_round=1000,\n                        learning_rate = 0.1,\n                        num_leaves = 127,\n                        max_depth = -1,\n                        lambda_l1 = 0.0,\n                        lambda_l2 = 1.0,\n                        metric = 'l2',\n                        seed = 42)  \nm.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], \n            early_stopping_rounds=50)","f78f0500":"def getQuestion(row):\n    row['question'] = row['Feature'].split('_')[0]\n    return row\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# sorted(zip(clf.feature_importances_, X.columns), reverse=True)\nfeature_imp = pd.DataFrame(sorted(zip(m.feature_importances_,X.columns)), columns=['Value','Feature'])\nfeature_imp['Value'] = feature_imp['Value']\/feature_imp['Value'].sum() # normalize to 1\nfeature_imp = feature_imp.apply(getQuestion, axis = 1)\n#feature_imp = feature_imp.drop('Feature', axis = 1)\n#feature_imp = feature_imp.groupby(by='Feature', as_index=False).sum()\n# group the values by Question\nsorted_values = feature_imp.sort_values(by=\"Value\", ascending=False)\n# keep only the top 80% important\nsorted_values['cum'] = sorted_values['Value'].cumsum()\/sorted_values['Value'].sum()\nsorted_values = sorted_values[sorted_values['Value'] > 0.005]\n\n# save for later use\nimportant_features = sorted_values['Feature'].copy()\n#sorted_values = sorted_values.head(10)\n\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=sorted_values)\nplt.title('LightGBM Question Importanct')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-01.png')","56e4a779":"print('Correlation of Age and Experience: ' + str(answers['Q2'].corr(answers['Q8'])))\n# Age \nsns.jointplot(np.log(answers['Q2']), np.log(answers['Q9']), kind='kde').set_axis_labels('log Age [years]', 'log Income [$1000]', fontsize=16)\n# Working Experience im ML\nsns.jointplot(np.log(answers['Q8']), np.log(answers['Q9']), kind='kde').set_axis_labels('log Experience in ML\/DS [years]', 'log Income [$1000]', fontsize=16)\n","7c08f582":"answers['Q12_Part_4_TEXT'].describe()","4a4e04be":"def setranking(row):\n    if row['Q12_Part_4_TEXT'] < 0:\n        row['Q12_ranking'] = 'No answer'\n        return row\n    row['Q12_ranking'] = 'Some answer'\n    return row\n\nanswersq12 = answers.copy()\n#split by quantiles\nanswersq12['Q12_ranking'] = 'NaN'\nanswersq12 = answersq12.apply(setranking, axis=1)\nanswersq12 = pd.DataFrame(answersq12.filter(['Q12_ranking','Q9'],axis=1).groupby(by='Q12_ranking', as_index=False).median())\n\n#sns.jointplot(answersq12['Q12_Part_4_TEXT'], answersq12['Q9'], kind='kde') # make the col strictly positive\n\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Q9\", y=\"Q12_ranking\", data=answersq12.sort_values(by='Q9', ascending=False))\nplt.title('Median Income in Q12 groups')\nplt.tight_layout()\nplt.show()\nplt.savefig('q12_on_income.png')","5d568e1d":"def setranking(row):\n    q35 = [row['Q35_Part_1'], row['Q35_Part_2'], row['Q35_Part_3'], row['Q35_Part_4'], row['Q35_Part_5'], row['Q35_Part_6']]\n    if q35[0] == max(q35):\n        row['q35_group'] = 'Self taught'\n    if q35[1] == max(q35):\n        row['q35_group'] = 'Online Courses'\n    if q35[2] == max(q35):\n        row['q35_group'] = 'Work'\n    if q35[3] == max(q35):\n        row['q35_group'] = 'University'\n    if q35[4] == max(q35):\n        row['q35_group'] = 'Kaggle'\n    if q35[5] == max(q35):\n        row['q35_group'] = 'Other'\n    return row\n\nanswersq35 = answers.copy()\n#split by quantiles\nanswersq35['q35_group'] = 'Not specified'\nanswersq35 = answersq35.apply(setranking, axis=1)\nanswersq35 = pd.DataFrame(answersq35.filter(['q35_group','Q9'],axis=1).groupby(by='q35_group', as_index=False).median())\n\n#sns.jointplot(answersq12['Q12_Part_4_TEXT'], answersq12['Q9'], kind='kde') # make the col strictly positive\n\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Q9\", y=\"q35_group\", data=answersq35.sort_values(by=\"Q9\", ascending=False))\nplt.title('Median Income in Q35 groups')\nplt.tight_layout()\nplt.show()\nplt.savefig('q35_on_income.png')","3caf2110":"def setranking(row):\n    q34 = [row['Q34_Part_1'], row['Q34_Part_2'], row['Q34_Part_3'], row['Q34_Part_4'], row['Q34_Part_5'], row['Q34_Part_6']]\n    if q34[0] == max(q34):\n        row['q34_group'] = 'Gathering data'\n    if q34[1] == max(q34):\n        row['q34_group'] = 'Cleaning data'\n    if q34[2] == max(q34):\n        row['q34_group'] = 'Visualizing data'\n    if q34[3] == max(q34):\n        row['q34_group'] = 'Model building'\n    if q34[4] == max(q34):\n        row['q34_group'] = 'Putting Model to Production'\n    if q34[5] == max(q34):\n        row['q34_group'] = 'Stakeholder communication'\n    return row\n\nanswersq34 = answers.copy()\n#split by quantiles\nanswersq34['q34_group'] = 'Not specified'\nanswersq34 = answersq34.apply(setranking, axis=1)\nanswersq34 = pd.DataFrame(answersq34.filter(['q34_group','Q9'],axis=1).groupby(by='q34_group', as_index=False).median())\n\n#sns.jointplot(answersq12['Q12_Part_4_TEXT'], answersq12['Q9'], kind='kde') # make the col strictly positive\n\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Q9\", y=\"q34_group\", data=answersq34.sort_values(by=\"Q9\", ascending=False))\nplt.title('Median Income in Q34 groups')\nplt.tight_layout()\nplt.show()\nplt.savefig('q34_on_income.png')","2243603b":"This is the income distribution with values above 250K excluded and those who did not respond. It pretty skewed to the right.","0d974b8f":"Feature importances and KDE Plots indicate higher importance of Age, than working experience in ML\nOne explanation might be that ML is not the core qualification as workers for most kagglers.\n\nWhen diving into the Question 12, 35 and 34 we will be looking at the median income in different groups rather than the mean. This is because income is crowded in the lower segments with some high rollers. Thus it is far from normal and the median will give a value nearer to  the peak of the distribution.\n\nNext thing up is the Question 12 Part 4 which has scored as most important for income. It is a free text field asking which tools we use. Bad thing the free text answers are seperated and i do not know if there is any way to link this back to the original text. Also i am not sure what we see in the column, maybe the number of characters used in the answer?","92384a5a":"In the Kaggle Survey 2018 one of the questions considered the annual income. Some Kagglers submitted the information allowing some insight in why some make more than others. With this kernel i highlight the most important features of a kaggler coinciding with high income.\n\nThe following Hypothesis can be formed from the data:\n\n**Stakeholder communication is the most important skill among professionals**\nThe participants spending most of their time with Stakeholder communication (Q34) have the best yearly income. Thus this might be the most important skill for an actual ML\/DS Job\n\n**Age is more relevant than ML\/DS Experience**\n Age and professional Experience in ML\/DS are among the most important factors determining income. However Age is a stronger predicotr as you will see later. This might be a hint that most kagglers do not actually work in a ML\/DS related Job, or not since a long time\n\n**Education earned online is not as valued as formal education or working experience**\nWhen it comes to education (Q35) Working experience beats University education beats online education (including kaggle) as indicator for high income. \n\nThe Text field for Q12 Part 4, providing details about Local or hosted development environments as entered by the participant is the single most important feature in the model. However the actual text entered is lost to anonymization.\n\nPlease let me know whether you agree\/disagree with my interpretation of these observations.","31669844":"Questions Q2 and Q8 are about age and professional experience. Hence their importance for salary seems intuitive. All Features which contribute less than 0,5% are not shown. Note how country, result of Q3, does not show up.\n\nMost participants are from the US and India. This might indicate that the pay gap within each country is more significant than the pay gap between countries.\n\nFirst lets have a look at the relationship of age and working experience with salary. The axis are logarithmic to deal with the skewed nature of the distribution.","74b9cb2a":"We know for sure that -1 means no response. So we compare the median income of participants who have provided an answer and those who did not.","33208ab8":"The median income is highest for the group which spends most time with stakeholder communication, surpassing the next group by about 10K.\n\n\nThis is it for now, please leave a comment on what i can improve. Hope you enjoyed","5ddb5fa3":"To judge the importance of difference features for the income of the participant we will train a gradient boosting random forest to the data with the result of Q9, which is the yearly income, as target variable. To train a regression models the bins have been replaced by their midpoints in the data.\n\nThe resulting feature importances are shown below, but only for features with importance > 0.5%","a8c195c4":"It is no surprise to see work on the top. University education beats kaggle and online courses by about by around 10K. Being able to get the actual text entered for option OTHER would be a huge plus. But as of my understanding we can not get it.\n\nWe will look at Q34 in a very similar manner. Kagglers were asked to distribute a total of 100% on the following activites, in relation to time spent in projects:\n\n1: Gathering data\n\n2: Cleaning data\n\n3: Visualizing data\n\n4: Model building\n\n5: Putting the Model to production\n\n6: Stakeholder communication\n","a0d62ed4":"The Median income is 10K higher for people who have provided any reply for the kinds of tools they use. However i could not yet identify a strong relationship with the number given in the field. I suspect a lot of meaning was lost when scrambling the answers to provide anonymity.\n\nThe questions 35 deals with how much different sources of learning material contributet to your ML education. The question asked to distribute 100% toward the following sources of learning:\n\n1: self taught\n\n2: Online Courses\n\n3: Work\n\n4: University\n\n5: Kaggle\n\n6: Other\n\nSo we are going to split the participants based on where most points were given"}}