{"cell_type":{"8a4ab4c0":"code","1418e3bb":"code","0b3c1da7":"code","7fa8e510":"code","69a7702c":"code","011ddc15":"code","195f2a5a":"code","1a22c042":"code","d4beaa26":"code","2f3a7dba":"code","0961679f":"code","1ff19c43":"code","4aed7f7c":"markdown","e32ebcb0":"markdown","eed1a7e9":"markdown","fecb3a61":"markdown","92353205":"markdown","f64ff31c":"markdown","b6caa897":"markdown","75af2291":"markdown","ba3b80d6":"markdown"},"source":{"8a4ab4c0":"import jax\nfrom jax import numpy as jnp\nfrom jax import jit, grad\nfrom jax import random, vmap\n","1418e3bb":"key = random.PRNGKey(1)\nprint(type(key), key)","0b3c1da7":"def wandb(in_c, o_c, key):\n    keys = random.split(key, 2)\n    w = 0.01*random.normal(keys[0], (o_c, in_c))\n    b = 0.01*random.normal(keys[1], (o_c,))\n    return [w,b]\n\ndef mlp(sizes, key):\n    keys = random.split(key, len(sizes))\n    params = []\n    for i in range(len(keys)-1):\n        key_ = keys[i]\n        in_c = sizes[i]\n        o_c = sizes[i+1]\n        params.append(wandb(in_c, o_c, key_))\n    return params\n","7fa8e510":"@jit\ndef leaky_relu(x):\n    return jnp.maximum(0.1*x, x)\n\n@jit\ndef forward(params, x):\n    for w,b in params[:-1]:\n        x = leaky_relu(jnp.dot(w, x)+b)\n    y_pred = jnp.dot(params[-1][0], x) + params[-1][1]\n    return y_pred","69a7702c":"sizes = [1, 128, 256, 1]\n\nmlp_params = mlp(sizes, key)\nforward(mlp_params, jnp.zeros(1))","011ddc15":"batch_forward = jit(vmap(forward, in_axes=(None, 0)))","195f2a5a":"batch_forward(mlp_params, jnp.zeros((10, 1)), )","1a22c042":"@jit\ndef compute_loss(params, x, y):\n    y_pred = batch_forward(params, x)\n    loss = jnp.mean(jnp.square(y - y_pred))\n    #loss = jnp.mean(loss)\n    return loss\n@jit\ndef update(params, x, y, lr):\n    loss = compute_loss(params, x, y)\n    grads = grad(compute_loss)(params, x, y)\n    return jax.tree_multimap(lambda p,g:p-lr*g, params, grads), loss\ncompute_loss(mlp_params, jnp.zeros((10,1)), jnp.zeros((10, 1)))","d4beaa26":"#Generate datapoints for sinx regression\nX = jnp.linspace(-3.14, 3.14, 100000).reshape(-1,1)\nY = jnp.sin(X)+0.05*random.normal(key, X.shape)\nfrom matplotlib import pyplot as plt\nplt.scatter(X,Y)\nplt.show()","2f3a7dba":"#hyperparameters\nbatch_size = 4096\nnum_batches = X.shape[0]\/\/batch_size\nepochs = 800\nlr = 1e-2\nprint(\"no of datapoints {} \\nbatch size {} \\nno of batches : {}\".format(\n    X.shape[0],\n    batch_size,\n    num_batches\n))","0961679f":"import time\n#below is the basic training loop \n\nfor epoch in range(epochs):\n    loss = 0\n    start = time.time()\n    for batch in range(num_batches):\n        if(batch==num_batches-1):\n            batch_x = X[batch_size*batch:]\n            batch_y = Y[batch_size*batch:]\n        else:\n            batch_x = X[batch_size*batch:batch_size*(batch+1)]\n            batch_y = Y[batch_size*batch:batch_size*(batch+1)]\n\n        mlp_params, L = update(mlp_params, batch_x, batch_y, lr)\n        loss= loss + L*batch_size\n    loss\/=X.shape[0]\n    end = time.time()\n    if(epoch%50==0):   \n        print(\"epoch : {} | loss : {} | time : {}\".format(epoch, loss, end - start))","1ff19c43":"y_pred = batch_forward(mlp_params, X)\n\nplt.scatter(X,Y)\nplt.scatter(X, y_pred)\nplt.show()","4aed7f7c":"Alright!! <br>\nWe are done with the neural network forward computations. But, there is a missing piece, **the above written forward function, will it work for batch of data points?** <br>\n\n![download (5).jpeg](attachment:1da38acb-bff3-4b20-a75f-79c142464e7f.jpeg)\n\nUnfortunately, ***the answer is a big Nope.***<br>\nBut, no worries, the **jax.vmap comes to rescue**. Just wrapping up our function with jax.vmap will make it compatible for a batch of datapoints. So, how cool is that?<br>\n!!!!Also, did you notice the in_axes argument in the below snippet of code?!!!<br>\nBasically, the forward function has two argments: \n1. Parameters(params)\n2. Input(x)<br>\nAmong these, only x has to be considered for batching. **So that's why the in_axes is provided with a tuple (None, 0)**. (None at first position indicates that \"No batching is required for the first argument i.e parameters)","e32ebcb0":"## Functions for NN:\n1. The firs function \"wandb\" will generate random numbers when provided with input and output channels. Please note that, we have used something called \"jax.random.split\" which splits our mainkey into desired number of subkeys. Here, we are generated two subkeys, one each for weights and biases.\n\n2. Function \"mlp\" is self explanatory. We are generating n-1 number of subkeys where n is no of layers. <br>\n![600px-Neural_network.svg.png](attachment:d989764d-3b60-4129-b969-119fe869f6c2.png)<br>(as shown in the above image, if neural net has n number of layers, there will be n-1 number of weight matrices and bias matrices)<br>\n\n**Note that, we are saving the params in a list of lists format. This is one kind of [pytrees](https:\/\/jax.readthedocs.io\/en\/latest\/pytrees.html), we do this, because it makes our life easier while doing the updation of paramaters.**","eed1a7e9":"3. Leaky relu is a common activation function, that we use in deep learning. The function \"leaky_relu\" implements that in jax. The mathematical details are as follows: <br>\n![Screen_Shot_2020-05-25_at_3.09.45_PM.png](attachment:1f9658bc-115d-4f52-a2c9-e9028324617d.png)\n4. Finally, the function \"forward\", as the name suggests, it's written for forward computations of the neural network.","fecb3a61":"The update function here, uses something called jax.tree_multimap. This is why we have used pytrees in the first place. So that you can simply use tree_multimap to update all parameters.","92353205":"## Conclusion\n***Thanks for your time. Hope you have liked it. Do upvote my kernel, so that it motivates me to write more and more jax notebooks in future as well.***\n<br>I ran this notebook on TPU accelerator, feel free to run it on GPU and CPU as well.<br>\n### In case of any queries, shoot them up in the comment section below!","f64ff31c":"# INTRODUCTION TO JAX\nJAX is a Python library designed for high-performance numerical computing, especially machine learning research. Its API for numerical functions is based on NumPy, a collection of functions used in scientific computing. Both Python and NumPy are widely used and familiar, making JAX simple, flexible, and easy to adopt\n","b6caa897":"## Neural Network in JAX:\nSince, we gonna code a neural network, ofcourse, we need weights, and those are expected to be initialized randomly. The random number generation in JAX is a bit different from our tradition numpy.random function. In numpy, usually we don't care about seed\/state, this is tackled internally. But whereas in JAX, it's not the case, we generate a key\/state using jax.random.PRNGKey() function(PRNG stands for pseudo random number generator). <br>\n<br>\n    Once key is generated, we are gonna use the same key throughout the code to generate random numbers where ever necessary. A key is nothing but a tuple of numbers.","75af2291":"## Points worth noting\n1. JAX, unlike other popular ML frameworks like pytorch, uses functional programming paradigm. **So everthing you write should be in pure functions.**\n\n2. During the first time of execution, the framework caches the function, and it uses the cached version of function for suceeding function calls. So usage of any global variables might create issues. Don't really get panic, even if you don't understand this, **take my word, and avoid using global variables in JAX code.**\n\n3. There is something called **\"JIT\" which makes the functions even faster.**\n","ba3b80d6":"## Key features\n1. The framework is highly optimized, so ofcourse it is super fast!! <br>\n![download.jpeg](attachment:a9827f23-8a11-461d-96f6-bce15973db06.jpeg)\n\n2. Also, the framework is accelerator agnostic, i.e no matter what accelerator you are using, it can be TPU or GPU or neither of them, still the code works fine(no need to change any part). <br>\n![download (2).jpeg](attachment:df075f8c-c33d-4d7d-9f8f-91a9e11093f6.jpeg)\n\n3. Most of the functions are similar to what we have in numpy. So, one can easily adopt to JAX, atleast as far as basics are concerned, the claim is true. <br>\n![20211107_190844.jpg](attachment:107de67b-43b1-4f2c-8945-fdfc1d354558.jpg)\n\n4. One of the coolest features that JAX offer is, it can differentiate through a large subset of python features, including loops, ifs, recursions, etc.,. It can even take derivatives of derivatives of derivatives of derivatives.\n"}}