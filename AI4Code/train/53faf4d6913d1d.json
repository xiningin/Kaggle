{"cell_type":{"557e507e":"code","f109e963":"code","0a03f2c6":"code","d80ee5da":"code","7b956940":"code","82cb3a94":"code","fe3b8d04":"code","1a1b474c":"code","a5178cfa":"code","d343ff60":"code","dac3b488":"markdown","eca3a60a":"markdown","a73873d1":"markdown","cff15fad":"markdown","b6637ee7":"markdown","64a0daa0":"markdown","01736202":"markdown","17a00523":"markdown","97c63373":"markdown","b28c074b":"markdown","1e4da61e":"markdown","1dbb2280":"markdown","ad9f2125":"markdown","18f137fa":"markdown"},"source":{"557e507e":"!pip install -U transformers","f109e963":"from tokenizers import ByteLevelBPETokenizer\n\n# Create an empty ByteLevel tokenizer object\nbl_tokenizer = ByteLevelBPETokenizer()\n\n# Train tokenizer\nbl_tokenizer.train('..\/input\/tatoeba-en-bn\/tatoeba_en_bn.txt',\n                   vocab_size=10000, min_frequency=2,\n                   special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n\n# Savel tokenizer model\nbl_tokenizer.save_model(\".\")","0a03f2c6":"from tokenizers import BertWordPieceTokenizer\n\nwb_tokenizer = BertWordPieceTokenizer(clean_text=True, handle_chinese_chars=True,\n                                      strip_accents=True, lowercase=True)\n\nwb_tokenizer.train('..\/input\/tatoeba-en-bn\/tatoeba_en_bn.txt',\n                   vocab_size=10000, min_frequency=2,\n                   special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\nwb_tokenizer.save_model(\".\")","d80ee5da":"from transformers import BartTokenizer\n\ntokenizer = BartTokenizer(\".\/vocab.json\",\".\/merges.txt\",\n                               do_lower_case=True, do_basic_tokenize=True, \n                               padding=True,bos_token='[CLS]', \n                               eos_token='[SEP]', sep_token='[SEP]', \n                               cls_token='[CLS]', unk_token='[UNK]', \n                               pad_token='[PAD]', mask_token='[MASK]',)","7b956940":"from transformers import BertConfig, BertLMHeadModel\n\nconfig = BertConfig(tokenizer.vocab_size, hidden_size=300,\n                    num_hidden_layers=2, num_attention_heads=2, is_decoder=True,\n                    add_cross_attention=True)\nmodel = BertLMHeadModel(config)","82cb3a94":"import numpy as np\n\nlines = open('..\/input\/tatoeba-en-bn\/tatoeba_en_bn.txt').read().strip().split('\\n')\npairs = [pair.split('\\t') for pair in lines]\npairs = np.array(pairs)\nsrc = pairs[:,0]\ntar = pairs[:,1]\nprint(src[0:5])\nprint(tar[0:5])","fe3b8d04":"import torch\nfrom torch.utils.data import Dataset, DataLoader\n\n\nclass MyDataset(Dataset):\n    def __init__(self, src, tgt, tokenizer):\n        self.src = src\n        self.tgt = tgt\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.src)\n\n    def __getitem__(self, idx):\n        src = self.src[idx]\n        tgt = self.tgt[idx]\n        return [src, tgt]\n\ndataset = MyDataset(src, tar, tokenizer)","1a1b474c":"def data_collate_fn(dataset_samples_list):\n    arr = np.array(dataset_samples_list)\n    inputs = tokenizer.prepare_seq2seq_batch(src_texts=arr[:,0].tolist(),\n                                                  tgt_texts=arr[:,1].tolist(),\n                                                  padding='max_length',\n                                                  max_length=30,\n                                                  return_tensors='pt')\n    \n    return inputs","a5178cfa":"from transformers import Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\".\/bert\",\n    overwrite_output_dir=True,\n    num_train_epochs=10,\n    do_train=True,\n    per_gpu_train_batch_size=32,\n    save_steps=500,\n    save_total_limit=2\n\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset,\n    eval_dataset=dataset,\n    data_collator=data_collate_fn\n)","d343ff60":"trainer.train()","dac3b488":"**Conclusion**\n===\n<a class=\"anchor\" id=\"conclusion\"><\/a>\nIf you have any query regarding this notebook you can ask it in comment. I will try to help you \ud83d\ude00.","eca3a60a":"collate_fn() transform the data as you want after retrive data for a batch you may want to see [this](https:\/\/pytorch.org\/docs\/stable\/data.html#dataloader-collate-fn) documentation for better understanding. Here our data_collate_fn() will tokenize and return the inputs we need for pass into model.","a73873d1":"Create our Dataset class and its object. Also you may check out my [this](https:\/\/www.kaggle.com\/mojammel\/pytorch-dataset-dataloader-tutorial) notebook for understand how PyTorch Dataset and DataLoader class work.","cff15fad":"## WordPiece Tokenizer\n<a class=\"anchor\" id=\"tokenizer_2\"><\/a>\nSome useful link for understand WordPiece Tokenizer.\n\n\n*   [WordPiece](https:\/\/paperswithcode.com\/method\/wordpiece)\n*   [3 subword algorithms help to improve your NLP model performance](https:\/\/medium.com\/@makcedward\/how-subword-helps-on-your-nlp-model-83dd1b836f46)\n\nTake a quick tour in HuggingFace tokenizer summary and their tokenizer train example.\n\n*   [WordPiece Tokenizer](https:\/\/github.com\/huggingface\/tokenizers\/blob\/master\/bindings\/python\/examples\/train_bert_wordpiece.py)\n*   [Tokenizer Summary](https:\/\/huggingface.co\/transformers\/tokenizer_summary.html#)\n\nAlso it was first mention in this [paper](https:\/\/static.googleusercontent.com\/media\/research.google.com\/ja\/\/pubs\/archive\/37842.pdf) and later in [this](https:\/\/arxiv.org\/pdf\/1609.08144v2.pdf).\n\n","b6637ee7":"## ByteLevel Tokenizer\n<a class=\"anchor\" id=\"tokenizer_1\"><\/a>\nWell before train our tokenizer this are some blog which I think help you to understand what is a ByteLevel Tokenizer and why it introduced.\n\n*   [Byte Pair Encoding \u2014 The Dark Horse of Modern NLP.](https:\/\/towardsdatascience.com\/byte-pair-encoding-the-dark-horse-of-modern-nlp-eb36c7df4f10)\n*   [Byte-level BPE, an universal tokenizer but\u2026](https:\/\/medium.com\/@pierre_guillou\/byte-level-bpe-an-universal-tokenizer-but-aff932332ffe)\n\nYou can also take a quick tour HuggingFace tokenizer summary and their tokenizer train example.\n\n*   [ByteLevel Tokenizer](https:\/\/github.com\/huggingface\/tokenizers\/blob\/master\/bindings\/python\/examples\/train_bytelevel_bpe.py)\n*   [Tokenizer Summary](https:\/\/huggingface.co\/transformers\/tokenizer_summary.html#)\n\nCheck out ByteLevel Tokenizer paper [here](https:\/\/arxiv.org\/pdf\/1909.03341.pdf).","64a0daa0":"Finally let's train our model.","01736202":"**Language Model**\n===\n<a class=\"anchor\" id=\"langModel\"><\/a>","17a00523":"Configuring our Bert Language Model.","97c63373":"Create a Trainer class object for train our model.","b28c074b":"![image.png](attachment:image.png)\n**Note:** I will be updating this notebook gradually with other NLP task and models while I am learning HuggingFace library \ud83d\ude09 hopefully it will help someone in future.\n### Table of Contents\n\n* [Tokenizer](#tokenizer)\n    * [ByteLevel Tokenizer](#tokenizer_1)\n    * [WordPiece Tokenizer](#tokenizer_2)\n* [Language Model](#langModel)\n    * [Bert Model](#langModel_1)\n* [Conclusion](#conclusion)\n\nPip install the packages","1e4da61e":"We will be using ByteLevel tokenizer. As [BertTokenizer](https:\/\/huggingface.co\/transformers\/model_doc\/bert.html#berttokenizer) does not support ByteLevel tokenizer yet(it support WordPiece tokenizer check out documentation I added) so we will use [BartTokenizer](https:\/\/huggingface.co\/transformers\/model_doc\/bart.html#barttokenizer) by modify it a little.","1dbb2280":"**Tokenizer**\n===\n<a class=\"anchor\" id=\"tokenizer\"><\/a>","ad9f2125":"## Bert Model\n<a class=\"anchor\" id=\"langModel_1\"><\/a>","18f137fa":"Load and convert data into numpy array for further slicing purpose. Dataset can be download from [here](http:\/\/opus.nlpl.eu\/Tatoeba.php)."}}