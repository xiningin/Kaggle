{"cell_type":{"d5e50a56":"code","93869309":"code","dd7fe36f":"code","1801dc5e":"code","17088410":"code","1216fef7":"code","6d1a9223":"code","2060cb30":"code","65f9c560":"code","0c7d1209":"code","f57284d4":"code","af3e9e52":"markdown","542b91fc":"markdown","00dfde27":"markdown","690cb52c":"markdown","27537b1a":"markdown","dae37e48":"markdown","cbb1cc98":"markdown","1d40c3e7":"markdown","b2b81283":"markdown","e00b477d":"markdown","2227046a":"markdown"},"source":{"d5e50a56":"import numpy as np\nimport pandas as pd\nimport plotly.express as px\nfrom sklearn.linear_model import Lasso\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold, cross_val_score\n\ntrain = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\n\ntrain[\"Test\"]=0\ntest[\"Test\"]=1\nX_all = pd.concat((train, test),sort=True).reset_index(drop=True)\nX_all.loc[:,\"SalePrice\"] = np.log(X_all[\"SalePrice\"])","93869309":"summary = pd.DataFrame({\"Missing\":len(X_all)- X_all.count(),\"Typical\":np.nan})\n\nfor col in summary[(summary.Missing > 0)].index: \n    if col == \"SalePrice\": continue\n    colmode=X_all[col].mode()\n    summary.loc[col,\"Typical\"]=colmode[0]\n    X_all[col].fillna(colmode[0],inplace=True)\n\nsummary[summary.Missing > 0]","dd7fe36f":"outliers=[]\ntrain=X_all[X_all.Test==0].copy()\nhover=[\"Id\",\"GrLivArea\",\"OverallCond\",\"MSZoning\",\"YearBuilt\",\"YearRemodAdd\",\"Functional\",\"SaleCondition\",\"SaleType\",\"BldgType\",\"Neighborhood\",\"LotArea\"]\ntrain[\"LogSF\"]=np.log(train.GrLivArea)\nfig = px.scatter(train[train.SaleType==\"New\"],y='SalePrice', x='LogSF',color=\"OverallQual\",size=\"OverallCond\",\n           hover_data=hover)\nfig.show()\noutliers+=[524,1299,1325,49,689]","1801dc5e":"fig = px.scatter(train[train.SaleType!=\"New\"],y='SalePrice', x='LogSF',color=\"MSZoning\",size=\"OverallQual\",\n           hover_data=hover)\nfig.show()\n\nfig = px.scatter(train[(train.MSZoning==\"C (all)\") & (train.SaleType!=\"New\")],y='SalePrice', x='LogSF',color=\"SaleCondition\",size=\"OverallQual\",\n           hover_data=hover,trendline=\"ols\")\nfig.show()\noutliers+=[496,31,969]","17088410":"functional_map = {\"Sal\":1,\"Sev\":0.3,\"Maj1\":0.1,\"Maj2\":0.15,\"Mod\":0.1,\"Min1\":0.05,\"Min2\":0.05,\"Typ\":0} \nquality_map = {\"Ex\":5,\"Gd\":4,\"TA\":3,\"Fa\":2,\"Po\":1,\"NA\":0,0:0}\nexposure_map = {\"Gd\":3,\"Av\":1,\"Mn\":0,\"No\":0,\"NA\":0}\nlotqual_map = {\"CulDSac\":2,\"Inside\":0,\"Corner\":1,\"FR2\": -1,\"FR3\":-2}\n\n\ndef feature_eng (X,num_to_categ={},full_data=False):    \n     \n    # My preference has been to identify a small subset of features that will result in a good fit.\n    # However, including all features (set the parameter full_data = True) will slighlty improve \n    # CV & Kaggle scores and the final run for the submission will include all features.\n    \n    new_columns= [\"SF\",\"BaseGood\",\"BaseTA\",\"Age\",\"Pool_Ind\",\"New\",\"SaleCondition_Abn\",\"Res\",\"Pred\"]\n    for el in new_columns: X.insert(1,el,0)\n    \n    # Create summary indicators\n    \n    X.loc[(X.SaleType==\"New\") & (X.YearBuilt >= X.YrSold-1),\"New\"]= 1\n    X.loc[X.SaleCondition == \"Abnorml\",\"SaleCondition_Abn\"]=1\n    X.loc[(X.PoolArea > 0),\"Pool_Ind\"]= 1\n    \n    # Summarise categorical features as numerical\n    \n    qual_cols = [\"ExterQual\",\"ExterCond\",\"BsmtQual\",\"BsmtCond\",\"HeatingQC\",\"KitchenQual\",\"FireplaceQu\",\"GarageQual\"]\n    for col in qual_cols: X.loc[:,col]= X[col].map(quality_map)\n        \n    X[\"LotConfig\"]=X[\"LotConfig\"].map(lotqual_map)\n    X[\"Functional\"]=X[\"Functional\"].map(functional_map)\n    X[\"Age\"]= np.maximum((X[\"YrSold\"]-X[\"YearBuilt\"])\/10,0)\n    X[\"QualityScore\"]= (X.KitchenQual * 2.5 + X.FireplaceQu *1 + X.GarageQual* 1.5 + X.HeatingQC * 1.5 + 2* X.BsmtQual)\/8.5-3\n    X[\"OverallCond\"]= (X.OverallCond-5) \n    X[\"OverallQual\"]= (X.OverallQual-5) \n    \n    # Adjust total living area for above and below ground living areas based on quality:\n    # GrLivArea capures only the above ground surface \n    # whilst some properties include significant good quality below ground living areas\n    # First ground living space appears to be valued slightly higher than second ground\n    \n    X.loc[X.BsmtFinType1.isin({\"GLQ\",\"ALQ\"}),\"BaseGood\"]+= X.loc[X.BsmtFinType1.isin({\"GLQ\",\"ALQ\"}),\"BsmtFinSF1\"]\n    X.loc[X.BsmtFinType1.isin({\"BLQ\",\"Rec\",\"LwQ\"}),\"BaseTA\"]+= X.loc[X.BsmtFinType1.isin({\"BLQ\",\"Rec\",\"LwQ\"}),\"BsmtFinSF1\"]\n    X.loc[X.BsmtFinType2.isin({\"GLQ\",\"ALQ\"}),\"BaseGood\"]+= X.loc[X.BsmtFinType2.isin({\"GLQ\",\"ALQ\"}),\"BsmtFinSF2\"]\n    X.loc[X.BsmtFinType2.isin({\"BLQ\",\"Rec\",\"LwQ\"}),\"BaseTA\"]+= X.loc[X.BsmtFinType2.isin({\"BLQ\",\"Rec\",\"LwQ\"}),\"BsmtFinSF2\"]\n    X[\"SF\"]=  X.GrLivArea - 0.1*X[\"2ndFlrSF\"]+(X.BaseGood*.65+X.BaseTA*0.5)+0.25*X.BsmtUnfSF\n    \n    # Give additional credit for good exposure of the basement area\n    \n    X[\"BsmtExposure\"]= X[\"BsmtExposure\"].map(exposure_map)*((X.BaseGood+X.BaseTA)\/X.SF)\n    \n    # Apply log transform to surface areas\n    \n    X[\"SFLog\"]=np.log(X.SF)\n    X[\"LotArea\"]=np.log(X.LotArea) - np.log(X.SF)\n    \n    \n    # Select features to include in the model (for the final run we will include all)\n    \n    if not full_data:\n        X=X[[\"Res\",\"Pred\",\"SalePrice\",\"Test\",\"Id\"] + [\"SFLog\",\"Functional\",\"OverallQual\",\"OverallCond\",\n            \"LotArea\",\"GarageCars\",\"MSZoning\",\"Neighborhood\",\"New\",\"Age\",\"BldgType\",\"SaleCondition_Abn\",\n            \"Pool_Ind\",\"BsmtExposure\",\"QualityScore\",\"LotConfig\",\"Condition1\"]]\n          \n    X = pd.get_dummies(X,columns=list(num_to_categ))\n    X = pd.get_dummies(X)\n    \n    # Correct for overfit:\n    # I decided to exclude all features where fewer than 5 samples differ from the mode \n    \n    for el in set(X.columns).difference({\"Test\",\"Pred\",\"Res\"}):\n        col_mode=X.loc[X.Test==0,el].mode()[0]\n        if len(X.loc[(X.Test == 0) & (X[el] !=col_mode),\"SFLog\"]) < 5: \n                X.drop(columns=el, inplace = True)\n                print (\"Feature removed:\",el)\n            \n    # Add interaction terms for key features, e.g.\n    # overall quality tends to increase with surface area\n    # overall condition is related to property age (all new properties are rated 5)\n  \n    inter_features ={\"SFLog\",\"Age\",\"OverallCond\",\"OverallQual\",\"QualityScore\",\"Functional\"}.difference(num_to_categ) \n    poly = PolynomialFeatures(interaction_only=True,include_bias=False) \n    inter_cols = poly.fit_transform(X[inter_features])\n    X1= pd.DataFrame(inter_cols,columns= poly.get_feature_names(list(inter_features)),index=X.index)                            \n    X = pd.concat([X1,X.drop(columns=inter_features)],axis=1)\n    \n    return X","1216fef7":"def model_fit(model,X,target_col,folds=5):\n    \n    kf = KFold(folds, shuffle=True, random_state=4991)\n    \n    drop_cols = [\"Test\",\"Id\",\"Pred\",\"Res\",target_col]\n    X_train = X[X.Test==0].drop(columns=drop_cols)\n    y_train = X.loc[X.Test==0,target_col]\n    model.fit(X_train,y_train)\n    X[\"Pred\"] = model.predict(X.drop(columns=drop_cols))\n    X[\"Res\"]= X.Pred-X[target_col]\n    \n    score =  (-cross_val_score(model, X_train, y_train, scoring=\"neg_mean_squared_error\", cv = kf))**0.5\n    df = pd.DataFrame({\"Coeff\":model.coef_,\"Y mean\":(X_train*model.coef_).mean(),\"Y std\":(X_train*model.coef_).std()}).sort_values(\"Y std\",ascending=False)\n    return X, df, score\n\nX=X_all[~X_all.Id.isin(outliers)].copy()\nmodel = Lasso(alpha=0.0003,random_state=3591,max_iter=100000,fit_intercept=True)\n\nX = feature_eng(X,full_data=False)\nX, df, score = model_fit(model,X,\"SalePrice\")\nprint(\"\\nCV score: \",score,\"\\nMean: {:.4f} Std: {:.4f}\\n\".format(score.mean(), score.std()))\ndf.head(10)","6d1a9223":"hover=[\"Id\",\"OverallCond\",\"Functional\",\"SaleCondition_Abn\",\"LotArea\"]\nfig = px.scatter(X,y=\"Res\", x='SFLog',color=\"OverallQual\",size=\"Age\",hover_data=hover)\nfig.show()","2060cb30":"outliers+=[589,633,463,813,1433,411,804]\nX=X_all[~X_all.Id.isin(outliers)].copy()\nX = feature_eng(X)\nX, df, score = model_fit(model,X,\"SalePrice\")\nprint(\"\\nModel RMSLE CV score: \",score,\"\\nMean: {:.4f} Std: {:.4f}\\n\".format(score.mean(), score.std()))\ndf.head(10)","65f9c560":"model = Lasso(alpha=0.00033,random_state=3591,max_iter=100000,fit_intercept=True)\nX=X_all[~X_all.Id.isin(outliers)].copy()\nX = feature_eng(X,full_data=True)\nX, df, score = model_fit(model,X,\"SalePrice\")\nprint(\"\\nModel RMSLE CV score: \",score,\"\\nMean: {:.4f} Std: {:.4f}\\n\".format(score.mean(), score.std()))\ndf.head(10)","0c7d1209":"sub = pd.DataFrame({\"Id\":X.loc[X.Test==1,\"Id\"],\"SalePrice\":np.exp(X.loc[X.Test==1,\"Pred\"])})\nsub.to_csv('submissionV1.1.csv',index=False)","f57284d4":"X=X_all[~X_all.Id.isin(outliers)].copy()\nX = feature_eng(X,[\"Functional\"])\nX, df, score = model_fit(model,X,\"SalePrice\")\ndf.loc[[\"Functional_0.0\",\"Functional_0.05\",\"Functional_0.15\",\"Functional_0.1\"],\"Coeff\"]-df.loc[\"Functional_0.0\",\"Coeff\"]","af3e9e52":"Finally, we re-run the model with all features included. This approach does slightly improve the CV (and submission) score but will be  difficult explain as many of the features are correlated. I am not sure that I would prefer it over the initial model.  ","542b91fc":"## Modelling\n\nLet's go ahead and fit our regression model. ","00dfde27":"Now, let's have a look at outliers for the pro-owned homes. A small number of properties are classed as commercial. We will take a deeper look at these to help us confirm the most significant outliers. ","690cb52c":"## Preprocssing\n\n### Explore and understand the data\n\nThis is the key first step in any data science project. I know that this will take quite some time and effort but it is invaluable. There are excellent dicussions and visualisations in other notebooks - please have look! It will help you to become familiar with the data.  \n\n### Missing data\n\nI decided to simply fill in missing values with the most typical value for each feature. Have a look at some of the other kernels that contain more in-depth explanations for this step. ","27537b1a":"The most significant adjustments to property values relate to age, quality and condition of the property, as well as to lot and garage size. \n\nThis feels all very reasonable. \n\nA a final step, we will check the residuals to detect any additional outliers.   ","dae37e48":"## A simple Lasso model model to predict property prices \n\nI just wanted to share my first attempt at creating a predictive model in Python. Hopefully, you will find the notebook useful and can adapt it to your own regression projects.\n\n### Background on property valuations  \n\nBefore with dive into the data analysis, let's quickly recap how we would derive the value of a property \n\n**Property value = property size in sqft $\\times$ property cost per sqft  $\\times$ adjustments**, \n\nwhich is equivalent to\n\n**Log (property value) = Log (living area in sqft) + Log(cost per sqft) + $\\sum$  Log (adjustments)**\n\nThus, a linear model to predict the log of the property might be worth a try ...\n\nLet's get started!","cbb1cc98":"I decided to remove properties with residuals greater than +0.4 as the property values for these data points are not explained by the model. ","1d40c3e7":"### Conversion of categoricial features - Example\n\nAs promised, let's have a look at the functional deductions to check that the mapping weights are reasonable. ","b2b81283":"### Analyse and remove outliers\n\nAs identified in the original paper, the sale price for some of the new homes does not reflect the full market value. Two large and two smaller properties appear to be priced significantly below market value. In addition, there is one property with an abnormally high price. I decided to remove these properties from the train dataset.","e00b477d":"### Submission","2227046a":"## Feature engineering\n\nAs the number of samples is very small in relation to the number of features, I summarised the most relevant categorical features as numerical. This method should also limit the effect of outliers\/gaps in the train dataset on the prediction.\n\nThe calibration for the weights in the mapping tables was estimated by fitting a regression model that includes the categorical features - you can find an example of this appraoch at the end of the notebook."}}