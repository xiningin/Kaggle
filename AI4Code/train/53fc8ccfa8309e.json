{"cell_type":{"0240850c":"code","f5348497":"code","a19050c9":"code","783157f8":"code","22ab237c":"code","1cf848d1":"code","28ff4069":"code","e14b66ec":"code","f1266900":"code","14d84f3e":"code","0e3cb2c4":"code","2640937b":"code","8321e9bf":"code","4c695357":"code","49d2cb6e":"markdown","f37ff368":"markdown","c07deec1":"markdown","9a63bb15":"markdown","0a179c02":"markdown","b05be723":"markdown","bc43d23a":"markdown","ef08107d":"markdown","f38d972e":"markdown","31ff51b6":"markdown","da138422":"markdown","0d94643d":"markdown","483d0356":"markdown"},"source":{"0240850c":"# Importing useful libraries. \n\nimport numpy as np \nimport pandas as pd \nimport os\n%matplotlib inline\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n#set directory\nMAIN_DIR = '..\/input\/prostate-cancer-grade-assessment'\n# load data\ntrain = pd.read_csv(os.path.join(MAIN_DIR, 'train.csv')).set_index('image_id')\ntest = pd.read_csv(os.path.join(MAIN_DIR, 'test.csv')).set_index('image_id')\n","f5348497":"display(train.head())\nprint(\"Shape of training data :\", train.shape)\nprint(\"unique data provider :\", len(train.data_provider.unique()))\nprint(\"unique isup_grades (Target Variable) :\", len(train.isup_grade.unique()))\nprint(\"unique gleason_scores :\", len(train.gleason_score.unique()))","a19050c9":"display(test.head())\nprint('Shape of test data: ', test.shape)\nprint('unique data provider: ', len(test.data_provider.unique()))","783157f8":"# We now define a function that will be useful to us several times. Thanks to Rohit Singh's notebook.\ndef plot_count(df, feature, title='', size=2):\n    f, ax = plt.subplots(1,1, figsize=(4*size,3*size))\n    total = float(len(df))\n    sns.countplot(df[feature],order = df[feature].value_counts().index, palette='deep')\n    plt.title(title)\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height + 9,\n                '{:1.2f}%'.format(100*height\/total),\n                ha=\"center\") \n    plt.show()","22ab237c":"plot_count(df=train, feature='data_provider', title = 'Data provider - count and percentage share')","1cf848d1":"plot_count(df=train, feature='isup_grade', title = 'ISUP grade - count and percentage share')","28ff4069":"plot_count(df=train, feature='gleason_score', title = 'Gleason Score - count and percentage share', size=3)","e14b66ec":"print(len(train[train['gleason_score'] == '4+5'])\/(len(train[train['gleason_score'] == '4+5'])+len(train[train['gleason_score'] == '5+4'])+len(train[train['gleason_score'] == '5+5'])+len(train[train['gleason_score'] == '3+5'])+len(train[train['gleason_score'] == '5+3'])))","f1266900":"print(len(train[(train.data_provider == 'radboud') & (train.gleason_score == '0+0')]))\nprint(len(train[(train.data_provider == 'radboud') & (train.gleason_score == 'negative')]))\nprint(len(train[(train.data_provider == 'karolinska') & (train.gleason_score == '0+0')]))\nprint(len(train[(train.data_provider == 'karolinska') & (train.gleason_score == 'negative')]))","14d84f3e":"def plot_relative_distribution(df, feature, hue, title='', size=2):\n    f, ax = plt.subplots(1,1, figsize=(4*size,3*size))\n    total = float(len(df))\n    sns.countplot(x=feature, hue=hue, data=df, palette='deep')\n    plt.title(title)\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(100*height\/total),\n                ha=\"center\") \n    plt.show()","0e3cb2c4":"plot_relative_distribution(df=train, feature='gleason_score', hue='data_provider', title = 'relative distribution of Gleason score by Data provider', size=3.5)","2640937b":"plot_relative_distribution(df=train, feature='isup_grade', hue='data_provider', title = 'relative distribution of ISUP grade by data_provider', size=2)","8321e9bf":"plot_relative_distribution(df=train, feature='gleason_score', hue='isup_grade', title = 'relative distribution of Gleason score by ISUP grade', size=3)","4c695357":"print(train[(train.gleason_score == '0+0') & (train.isup_grade != 0)])\nprint(train[(train.gleason_score == '4+4') & (train.isup_grade != 4)])\nprint(train[(train.gleason_score == '3+3') & (train.isup_grade != 1)])\nprint(train[(train.gleason_score == '4+3') & (train.isup_grade != 3)])\nprint(train[(train.gleason_score == 'negative') & (train.isup_grade != 0)])\nprint(train[(train.gleason_score == '4+5') & (train.isup_grade != 5)])\nprint(train[(train.gleason_score == '3+4') & (train.isup_grade != 2)])\nprint(train[(train.gleason_score == '5+4') & (train.isup_grade != 5)])\nprint(train[(train.gleason_score == '5+5') & (train.isup_grade != 5)])\nprint(train[(train.gleason_score == '5+3') & (train.isup_grade != 4)])\nprint(train[(train.gleason_score == '3+5') & (train.isup_grade != 4)])","49d2cb6e":"This chart confirms what we checked earlier with regards the differing labels of benign examples across our two data providers. We also see more than twice as many 3+3 scores from karoliska than radboud. While the opposite is true for Gleason score 4+3. More of our severe cases (scores with 5 as minority or majority pattern) come from radboud rather than karolinska. Generally the cases from karolinska are skewed towards the less severe end of the scale compared to radboud. Let us now compare the relative distribution of ISUP grade by data provider.","f37ff368":"We see above the first 5 rows and header row of our data.. We also see our training data has shape 10616 x 3. 10616 image ids (images can be loaded from this unique id) and the corresponding data provider (karoliska or radboud), ISUP grade, and Gleason Score which directly leads to the ISUP grade as per Figure 1.","c07deec1":"There is just one suspicious case which we saw on the plot. A training example from karolinska with Gleason score 4+3 but ISUP grade of 2. We will probably remove this example unless there is a very obvious mis-labelling we can correct.\n\nOk so that is the end of our exploratory data analysis, we have isolated a mis-labelled example, discovered two differing labels for benign cases and gained an understanding about how our examples break down by data provider. This information may come in useful at future junctures in the design of this ML system.","9a63bb15":"We see here that within the test data we are provided with image id and data_provider. We have the same two unique data providers. As mentioned in the challenge overview this is a truncated version of the test file, full versions are available to notebooks upon submission. \n\nLet us now dig into the data further to check distribution of ISUP grades and Gleason Patterns, both generally and comparing across providers. We will also check that Gleason Patterns and ISUP Grades match for each training example.","0a179c02":"Let us now check how the distibrution of Gleason score's vary by provider. ","b05be723":"References: \n\nStr\u00f6m & Kartasalo, 2020 \n\nRohit Singh's Notebook: https:\/\/www.kaggle.com\/rohitsingh9990\/panda-eda-better-visualization-simple-baseline","bc43d23a":"We see in the above chart that there are comparatively few training examples for some of the most severe Gleason Patterns. The only category involving Gleason pattern 5 which is present in the traning data at a rate of more than 2.5% is 4+5. Let us compute the likelihood that the Gleason Score is 4+5 given that 5 is one of the scores. This may serve as a useful sanity check on model predictions.","ef08107d":"Prostate Cancer (PCa) Grade assessment.\n\nWe are provided with a labelled training set of 10,616 Whole Slide Images of prostate tissue biopsies. These tissue samples are examined by a pathologist and scored according to the Gleason grading system. The Gleason Score directly leads to the ISUP grade as per figure 1. The training set for this challenge is the most extensive multi-center dataset on Gleason grading available.\n\nBiopsies recieve their Gleason grade according to the architectural growth patterns of the tumor (Fig. 1). As shown in Fig. 1, the patterns present in a single biopsy are used to form a Gleason score, e.g. 4 + 3 = 7. \n\nTo simplify slightly, in the case of multiple different Gleason patterns being present, the score is composed of the most frequently occurring pattern and the second most frequent pattern in the biopsy, as judged by the pathologist. The minority pattern must account for at least 5% of the total area to be included, e.g. if pattern 3 is less than 5%, the Gleason score 4 + 3 will instead be 4 + 4. Further, the highest grade should always be part of the score. For example, a biopsy that contains 60% Gleason 4, 37% Gleason 3 and 3% Gleason 5 should get a score of 4 + 5 = 9. For details see (Epstein, 2016).\n\nThere is a risk of missing cancers, and a risk of overgrading resulting in unnecessary treatment. The system suffers from significant inter-observer variability, while in some countries there is a lack of qaulified pathologists. \n\nDeep Learning systems have shown promise in accurately grading PCa. These systems were not, until this challenge tested with multi-center datasets at scale.\n\nSubmissions will be scored based on their performance in predicting an ISUP grade for a hidden test set graded by expert pathologists. The quadratic weighted kappa is used to score submissions. A score of 1 represents a perfect submission with no errors on the test set, while 0 represents a performance no better than chance. The weighted kappa penalises incorrect predictions more the further they lie from the correct ISUP value.\n\nFigure 1.\n![Screen%20Shot%202020-04-08%20at%202.03.53%20PM.png](attachment:Screen%20Shot%202020-04-08%20at%202.03.53%20PM.png)","f38d972e":"We see that our training data is close enough to being evenly split between our two data providers. We see that over 25% of our data is benign, (ISUP grade 0), while another 25% is ISUP grade 1, the other 4 ISUP grades are present in our data at a rate in the range 11.5% to 12.7%. ","31ff51b6":"We see that ~1 training example with Gleason Pattern 4+3 is mapped to an unexpected ISUP grade. We shall find now find the image id for this suspicious case, and also ensure there are no other suspicious cases.","da138422":"We also see that benign slides have two different lables 0+0 or negative, presumably this is due to variation in labelling by data provider. The code below confirms this. Radboud labels benign examples as 'negative' label while karolinska uses '0+0'.","0d94643d":" **Exploratory Data Analysis** (Rohit Singh's Notebook listed in references was helpful for several aspects of this.)\n \n","483d0356":"We see karaolinska provides more of the training examples with ISUP grade 0 or 1, while radboud provides more of the training examples with ISUP grade 3,4 or 5.\n\nLet us now check to ensure that Gleason patterns are mapped to ISUP grades as per figure 1."}}