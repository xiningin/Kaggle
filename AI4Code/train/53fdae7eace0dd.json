{"cell_type":{"02899976":"code","dbb3c6d9":"code","665b2518":"code","a3820ffa":"code","f1e302e9":"code","6ceee2b3":"code","9533f057":"code","001fb215":"code","fa431cd9":"code","1dc1c5ad":"code","61731afe":"code","ee18f01f":"code","381b4156":"code","3323186d":"code","de796e83":"code","ee00673d":"code","f109cf42":"code","8843cea9":"code","d47f935e":"code","e58516a0":"code","6f5a145c":"code","59bf24f2":"code","0238452f":"code","7ce96529":"code","3c74c8e2":"code","70503b13":"code","ce93e706":"code","e60ded99":"code","0664359f":"code","17adc56e":"code","c0824c54":"markdown","8b547e0d":"markdown","8996f766":"markdown","4187f40c":"markdown","98d55d35":"markdown","a9f7caa6":"markdown","aa5a7013":"markdown","0241d4b6":"markdown","d513d4b7":"markdown","288a2f8a":"markdown","38cfabd2":"markdown","a0096dcb":"markdown"},"source":{"02899976":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dbb3c6d9":"TRAIN_FILE_PATH = '\/kaggle\/input\/ag-news-classification-dataset\/train.csv'\nTEST_FILE_PATH = '\/kaggle\/input\/ag-news-classification-dataset\/test.csv'","665b2518":"pd1 = pd.read_csv(TRAIN_FILE_PATH)\npd2 = pd.read_csv(TEST_FILE_PATH)","a3820ffa":"print(pd1.head())\nprint(pd2.head())","f1e302e9":"print(pd1.columns)\nprint(pd2.columns)","6ceee2b3":"pd1.shape","9533f057":"! pip install sweetviz","001fb215":"import sweetviz as sv","fa431cd9":"report = sv.analyze(pd1)","1dc1c5ad":"report.show_notebook()","61731afe":"pd1.columns","ee18f01f":"## we can combine title and description together before feeding it to bi directional lstm \n\nX_train  =  pd1['Title']+' '+pd1['Description'] # also removing the class from the training dataset\n\nX_test   =  pd2['Title']+'  '+pd2['Description'] # also removing the class from the training dataset\n\n\ny_train  =   pd1['Class Index'].apply(lambda x: x-1)  # assigning label of train\n\ny_test =    pd2['Class Index'].apply(lambda x: x-1) # assigning lale of test\n\n","381b4156":"## Finding the max no of words in a sentence in complete data set \n\nmax_len = X_train.map(lambda x : len(x.split())).max()","3323186d":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nvocabulary_size = 10000 # random value\nembed_size      = 32    # random value \n\ntok = Tokenizer(num_words=vocabulary_size)\ntok.fit_on_texts(X_train.values)\n\n\n# Token \nX_train = tok.texts_to_sequences(X_train)\nX_test  = tok.texts_to_sequences(X_test)\n\n# Now we need to pad all the sequences based on the max value \n\nX_train = pad_sequences(X_train,maxlen=max_len)\nX_test = pad_sequences(X_test,maxlen=max_len)\n","de796e83":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, GlobalMaxPooling1D, Bidirectional","ee00673d":"import pandas as pd\nimport numpy as np\n\n#Data Visualization\nimport matplotlib.pyplot as plt\n\n#Text Color\nfrom termcolor import colored\n\n#Train Test Split\nfrom sklearn.model_selection import train_test_split\n\n#Model Evaluation\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score\nfrom mlxtend.plotting import plot_confusion_matrix\n\n#Deep Learning\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, GlobalMaxPooling1D, Bidirectional\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.utils import plot_model","f109cf42":"from tensorflow.keras.layers import Embedding","8843cea9":"vocabulary_size = 10000 # random value\nembed_size      = 32  # random value","d47f935e":"# Implementing a sequential model\n\nmodel = Sequential()\nmodel.add(Embedding(vocabulary_size,embed_size,input_length = max_len)) #input layer is embedding layer\nmodel.add(Bidirectional(LSTM(128, return_sequences=True)))              # Bidirectinal LSTM\nmodel.add(Bidirectional(LSTM(64, return_sequences=True)))\nmodel.add(GlobalMaxPooling1D())                                         # Flattening layer to reduce everything in a vector form\nmodel.add(Dense(256, activation='relu'))                                                  # Dense layer\nmodel.add(Dropout(0.25))\nmodel.add(Dense(128, activation='relu')) \nmodel.add(Dropout(0.25))                                                # doing regularization in Neural Network\nmodel.add(Dense(64, activation='relu')) \nmodel.add(Dropout(0.25))\nmodel.add(Dense(4, activation='softmax'))                               #  we have 4 labels as output\n","e58516a0":"model.summary()","6f5a145c":"# callbacks = [\n#     EarlyStopping(     #EarlyStopping is used to stop at the epoch where val_accuracy does not improve significantly\n#         monitor='val_accuracy',\n#         min_delta=1e-4,\n#         patience=4,\n#         verbose=1\n#     ),\n#     ModelCheckpoint(\n#         filepath='weights.h5',\n#         monitor='val_accuracy', \n#         mode='max', \n#         save_best_only=True,\n#         save_weights_only=True,\n#         verbose=1\n#     )\n# ]","59bf24f2":"model.compile(loss = 'sparse_categorical_crossentropy',\n              optimizer = 'adam',\n              metrics = ['accuracy']             \n             )","0238452f":"# model.fit(X_train,y_train,batch_size=256,validation_data=(X_test,y_test),epochs=20, \n#           callbacks=callbacks)\n\nmodel.fit(X_train,y_train,batch_size=256,validation_data=(X_test,y_test),epochs=20)","7ce96529":"from tensorflow.keras.layers import GRU","3c74c8e2":"model = Sequential()\nmodel.add(Embedding(vocabulary_size,embed_size,input_length = max_len)) #input layer is embedding layer\nmodel.add(Bidirectional(GRU(128, return_sequences=True)))              # Bidirectinal LSTM\nmodel.add(Bidirectional(GRU(64, return_sequences=True)))\nmodel.add(GlobalMaxPooling1D())                                         # Flattening layer to reduce everything in a vector form\nmodel.add(Dense(256, activation='relu'))                                                  # Dense layer\nmodel.add(Dropout(0.25))\nmodel.add(Dense(128, activation='relu')) \nmodel.add(Dropout(0.25))                                                # doing regularization in Neural Network\nmodel.add(Dense(64, activation='relu')) \nmodel.add(Dropout(0.25))\nmodel.add(Dense(4, activation='softmax'))                               #  we have 4 labels as output","70503b13":"model.summary()","ce93e706":"model.compile(loss = 'sparse_categorical_crossentropy',\n              optimizer = 'rmsprop',\n              metrics = ['accuracy']             \n             )","e60ded99":"model.fit(X_train,y_train,batch_size=256,validation_data=(X_test,y_test),epochs=10)","0664359f":"# Trying Optimizer as adam\nmodel.compile(loss = 'sparse_categorical_crossentropy',\n              optimizer = 'adam',\n              metrics = ['accuracy']             \n             )","17adc56e":"model.fit(X_train,y_train,batch_size=256,validation_data=(X_test,y_test),epochs=10)","c0824c54":"#### Installing Sweetviz library","8b547e0d":"## Lets analyze data","8996f766":"## We can observe that we can achieve maximum validation accuracy of 90.49 and training accuracy of 98.16","4187f40c":"This is a classification problem where we need to classify a news article consisting of title and description into following category : 1-World, 2-Sports, 3-Business, 4-Sci\/Tech\nHere we have sequentional data \n\nThis is a sequential problem - \nSince we have the existing data available we can use bidirectional LSTM for this classification problem.","98d55d35":"1.Data Generator ( Shuffling the data  ) \n2.Pipeline ","a9f7caa6":"## MODEL","aa5a7013":"### FILE PATH","0241d4b6":"##  Data Pre-Processing ","d513d4b7":"## Lets tokenize the text data set.\n\nTokenization is one of the most important step of pre processing while modelling the text dataset ","288a2f8a":"## Lets analyze the accuracy using GRU model","38cfabd2":"## We can observe that we can achieve maximum validation accuracy of 91.96 and training accuracy of 92.88","a0096dcb":"## Reading the dataframe from the data set."}}