{"cell_type":{"abda1bbb":"code","4cae0135":"code","82d9de30":"code","1f67b798":"code","627b4e0d":"code","924637e7":"code","872f9b2c":"code","c196731f":"code","96dd92d4":"code","2b0ccd0a":"code","e9927ec5":"code","2b1aa32a":"code","6e3b832b":"code","5b8f978c":"code","757305fd":"code","0b5c7695":"code","94db8d8e":"code","40297be0":"code","ba3727eb":"code","e4222c97":"code","b1c481f8":"code","11857cc7":"code","25d75399":"code","c24665a0":"code","df243b32":"code","5e589925":"code","4b91f2f3":"code","38dcf8d9":"code","81e50b78":"code","37d05682":"code","5d324544":"code","5bc7f654":"code","7f4221bd":"code","84af297f":"code","2f4c25e4":"code","78a0e577":"code","cd11072c":"code","eea32ad6":"code","951940fe":"code","fe494696":"code","ba8611be":"code","2ab0afa6":"code","aefc6f7c":"code","80adfe95":"code","32270654":"code","937ddbfb":"code","4dd5c36b":"code","beea5e6d":"code","4b5c760c":"code","e4d624d6":"code","b34dc713":"code","6c1c6205":"markdown","09379775":"markdown","038e0fb2":"markdown","6962fc83":"markdown","b32f22ca":"markdown","6285a468":"markdown","eee3205b":"markdown","0a829797":"markdown","d0d2b73a":"markdown","d7f9a903":"markdown","ddee168b":"markdown","f717bcae":"markdown","0f15f5e6":"markdown","f4a96ed7":"markdown"},"source":{"abda1bbb":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","4cae0135":"train = pd.read_csv('..\/input\/home-credit-default-risk\/application_train.csv')\ntrain.head()","82d9de30":"#remove columns with mode and median building information \ndels = ['APARTMENTS_MODE', 'BASEMENTAREA_MODE', 'YEARS_BEGINEXPLUATATION_MODE', 'YEARS_BUILD_MODE', \n        'COMMONAREA_MODE', 'ELEVATORS_MODE', 'ENTRANCES_MODE', 'FLOORSMAX_MODE', 'FLOORSMIN_MODE', \n        'LANDAREA_MODE', 'LIVINGAPARTMENTS_MODE', 'LIVINGAREA_MODE', 'NONLIVINGAPARTMENTS_MODE', 'NONLIVINGAREA_MODE', \n        'APARTMENTS_MEDI', 'BASEMENTAREA_MEDI', 'YEARS_BEGINEXPLUATATION_MEDI', 'YEARS_BUILD_MEDI', \n        'COMMONAREA_MEDI', 'ELEVATORS_MEDI', 'ENTRANCES_MEDI', 'FLOORSMAX_MEDI', 'FLOORSMIN_MEDI', \n        'LANDAREA_MEDI', 'LIVINGAPARTMENTS_MEDI', 'LIVINGAREA_MEDI', 'NONLIVINGAPARTMENTS_MEDI', 'NONLIVINGAREA_MEDI', \n        'FONDKAPREMONT_MODE', 'HOUSETYPE_MODE', 'TOTALAREA_MODE', 'WALLSMATERIAL_MODE', 'EMERGENCYSTATE_MODE',\n        'FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_3', 'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_5',\n        'FLAG_DOCUMENT_6', 'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_8', 'FLAG_DOCUMENT_9', \n        'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11', 'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_13', \n        'FLAG_DOCUMENT_14', 'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17', \n        'FLAG_DOCUMENT_18', 'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20', 'FLAG_DOCUMENT_21']\n\n\ntrain1 = train.drop(train[dels], axis =1)\ntrain1.shape","1f67b798":"#Find proportion of target variable\n(train1['TARGET'].value_counts() \/ len(train1)).to_frame()","627b4e0d":"train1.info(max_cols = 100)","924637e7":"#find missing values\ncount = train1.isnull().sum().sort_values(ascending = False)\npercentage = ((train1.isnull().sum() \/ len(train1) * 100)).sort_values(ascending = False)\nmissing = pd.concat([count, percentage], axis = 1, keys = ['Count','Percentage'])\nmissing.head(35)","872f9b2c":"#distribution of credit amounts\nsns.displot(x ='AMT_CREDIT', x = train1, bins = 100)","c196731f":"#distribution of age\nsns.displot(x = 'DAYS_BIRTH', data = train1, bins = 100)","96dd92d4":"train1.groupby(['OCCUPATION_TYPE'])['OCCUPATION_TYPE'].count().sort_values(ascending=False).plot(kind='barh', figsize=(8,6))\nplt.show()","2b0ccd0a":"train1.groupby(['NAME_EDUCATION_TYPE'])['NAME_EDUCATION_TYPE'].count().sort_values(ascending=False).plot(kind='barh')\nplt.show()","e9927ec5":"train1.groupby(['NAME_FAMILY_STATUS'])['NAME_FAMILY_STATUS'].count().sort_values(ascending=False).plot(kind='barh')\nplt.show()\n","2b1aa32a":"#visualize income vs loan amount, identified by default\nfig, ax = plt.subplots(figsize=(10, 10))\na = sns.scatterplot(x = 'AMT_INCOME_TOTAL', y = 'AMT_CREDIT', data = train1, hue = 'TARGET')\na.set(xlim=(0, 1000000))\n","6e3b832b":"plt.subplots(figsize=(10, 10))\nb = sns.boxplot(x = 'WEEKDAY_APPR_PROCESS_START',\n            y = 'AMT_GOODS_PRICE',\n            hue = 'NAME_CONTRACT_TYPE',\n            data = train1,\n           palette = ['m', 'g'])\nb.set(ylim=(0, 1000000))","5b8f978c":"plt.subplots(figsize=(10, 10))\nc = sns.boxplot(x = train1['TARGET'],\n            y = train1['AMT_CREDIT'],\n            hue = train1['WEEKDAY_APPR_PROCESS_START'])\n\nc.set(ylim=(0, 1750000))","757305fd":"plt.subplots(figsize=(10, 10))\nd = sns.boxplot(x = 'NAME_EDUCATION_TYPE',\n            y = 'AMT_CREDIT',\n            hue = 'NAME_FAMILY_STATUS',\n            data = train1)\nd.set(ylim=(0, 2000000))","0b5c7695":"fig, ax = plt.subplots(figsize=(10, 10))\ne = sns.scatterplot(x = 'AMT_CREDIT', y = \"AMT_INCOME_TOTAL\", data = train1, hue = 'NAME_EDUCATION_TYPE', size = 'DAYS_BIRTH', alpha = 0.6)\ne.set(ylim=(25000, 600000))","94db8d8e":"sns.barplot(data = train1, x = 'AMT_CREDIT', y = 'WEEKDAY_APPR_PROCESS_START', hue = 'TARGET')","40297be0":"sns.barplot(x = 'TARGET', y = 'DAYS_BIRTH', data = train1)","ba3727eb":"corr1 = train1.corr()['TARGET'].sort_values()\n\n#strongest negative correlation\nprint('Features with Strongest Negative Correlation:')\ncorr1.head()","e4222c97":"neg = train1[['TARGET','EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_EMPLOYED']]\n\nneg_corr = neg.corr()\n\nsns.heatmap(neg_corr, annot = True)","b1c481f8":"#strongest positive correlation\nprint('Features with Strongest Positive Correlation:')\ncorr1.tail()","11857cc7":"pos = train1[['TARGET','DAYS_LAST_PHONE_CHANGE', 'REGION_RATING_CLIENT', 'REGION_RATING_CLIENT_W_CITY', 'DAYS_BIRTH']]\n\npos_corr = pos.corr()\n\nsns.heatmap(pos_corr, annot = True)","25d75399":"feat_corr = pd.DataFrame(train1).corr()","c24665a0":"plt.subplots(figsize=(16, 16))\nsns.heatmap(feat_corr, annot = False, label = 'small')","df243b32":"corr_df = feat_corr.where(np.triu(np.ones(feat_corr.shape), k = 1).astype(np.bool))\ncorr_df = corr_df.unstack().reset_index()\ncorr_df.columns = ['Feature A', 'Feature B', 'Correlation']\ncorr_df.dropna(subset = ['Correlation'], inplace = True)\ncorr_df['Correlation'] = round(corr_df['Correlation'], 2)\ncorr_df['Correlation'] = abs(corr_df['Correlation'])\nmatrix = corr_df.sort_values(by = 'Correlation', ascending = False)\nmax_corr = matrix[matrix['Correlation'] > 0.75]\nmax_corr","5e589925":"#replace 365243 in days employed with nan\ntrain1['DAYS_EMPLOYED'].replace(365243, np.nan, inplace = True)\n\n#convert age to years\ntrain1['AGE'] = train1['DAYS_BIRTH'] \/ - 365\n\n#set max income to 2.5 million\ntrain1 = train1[train1['AMT_INCOME_TOTAL'] < 2500000]","4b91f2f3":"#drop features\ndrop_list = ['DAYS_BIRTH', 'LIVINGAPARTMENTS_AVG', 'LIVINGAREA_AVG', \n             'CNT_FAM_MEMBERS',  'OBS_30_CNT_SOCIAL_CIRCLE',\n             'OBS_60_CNT_SOCIAL_CIRCLE', 'ELEVATORS_AVG', \n]\n\ntrain1 = train1.drop(drop_list, axis =1)\ntrain1.info(max_cols = 100)","38dcf8d9":"#create credit\/annuity ratio feature\ntrain1['CA_RATIO'] = train1['AMT_CREDIT'] \/ train1['AMT_ANNUITY']\n\n#create credit\/cost of goods ratio feature\ntrain1['CG_RATIO'] = train1['AMT_CREDIT'] \/ train1['AMT_GOODS_PRICE']\n\n#create avg of each row of EXIT_SOURCE values\ntrain1['AVG_EXT'] = train1.iloc[:, 39:42].sum(axis=1)\/(3- train1.iloc[:,39:42].isnull().sum(axis=1))\ntrain1.EXT_SOURCE_1.fillna(train1.AVG_EXT, inplace=True)\ntrain1.EXT_SOURCE_2.fillna(train1.AVG_EXT, inplace=True)\ntrain1.EXT_SOURCE_3.fillna(train1.AVG_EXT, inplace=True)\n\ntrain1.info(max_cols = 75)","81e50b78":"#convert catergorical festures to cat\n#cat_cols = ['FLAG_MOBIL', 'FLAG_EMP_PHONE', 'FLAG_WORK_PHONE', 'FLAG_CONT_MOBILE', \n           # 'FLAG_PHONE', 'FLAG_EMAIL', 'REGION_RATING_CLIENT', 'REGION_RATING_CLIENT_W_CITY',\n           # 'REG_REGION_NOT_LIVE_REGION', 'REG_REGION_NOT_WORK_REGION', 'LIVE_REGION_NOT_WORK_REGION',\n           # 'REG_CITY_NOT_LIVE_CITY', 'REG_CITY_NOT_WORK_CITY', 'LIVE_CITY_NOT_WORK_CITY']\n\n#train1[cat_cols] = train1[cat_cols].astype('category')","37d05682":"#one hot encoder function\n#def OHE(df, nan_as_category = True):\n    #columns = list(df.columns)\n    #cat_cols = df.select_dtypes(['category', 'object']).columns.tolist()","5d324544":"#####################################","5bc7f654":"#read credit card balance dataset into the notebook\ncc_balance = pd.read_csv('..\/input\/home-credit-default-risk\/credit_card_balance.csv')\ncc_balance.head()","7f4221bd":"#print shape of both datasets\nprint('Credit Card Balance Shape:', cc_balance.shape)\n","84af297f":"#create late payment feature\ncc_balance['LATE PYMT'] = cc_balance['SK_DPD'].apply(lambda x:1 if x>0 else 0)\n\n#create card use limit feature\ncc_balance['USE_LIMIT'] = cc_balance['AMT_BALANCE'] \/ cc_balance['AMT_CREDIT_LIMIT_ACTUAL']","2f4c25e4":"#group numerical features by SK_ID_CURR\ncc_num = cc_balance.groupby(by = ['SK_ID_CURR']).agg(['min', 'max', 'mean']).reset_index()\ncc_num","78a0e577":"#group categorical features by SK_ID_CURR\ncc_cat = pd.get_dummies(cc_balance.select_dtypes('object'))\ncc_cat['SK_ID_CURR'] = cc_balance['SK_ID_CURR']\ncc_cat = cc_cat = cc_cat.groupby(by = ['SK_ID_CURR']).mean().reset_index()\ncc_cat\n","cd11072c":"#merge cc_balance features into training dataset\ntrain1 = train1.merge(cc_num, on = 'SK_ID_CURR', how = 'left')\ntrain1 = train1.merge(cc_cat, on = 'SK_ID_CURR', how = 'left')","eea32ad6":"train1.info(max_cols = 300)","951940fe":"#read bureau balance dataset into the notebook\nb_balance = pd.read_csv('..\/input\/home-credit-default-risk\/bureau_balance.csv')\nb_balance.head()","fe494696":"b_balance.info()","ba8611be":"print('Bureau Balance Shape:', b_balance.shape)","2ab0afa6":"#read bureau balance dataset into the notebook\nbureau = pd.read_csv('..\/input\/home-credit-default-risk\/bureau.csv')\nbureau.head()","aefc6f7c":"print('Bureau Shape:', bureau.shape)","80adfe95":"bureau.info()","32270654":"#merge bureau and bureau balance\nbureau = bureau.merge(b_balance, on = 'SK_ID_BUREAU', how = 'left')","937ddbfb":"bureau.head()","4dd5c36b":"#create late payment feature\ncc_balance['LATE PYMT'] = cc_balance['SK_DPD'].apply(lambda x:1 if x>0 else 0)\n\n#create card use limit feature\ncc_balance['USE_LIMIT'] = cc_balance['AMT_BALANCE'] \/ cc_balance['AMT_CREDIT_LIMIT_ACTUAL']","beea5e6d":"cc_bal1 = cc_balance[['SK_ID_CURR', 'MONTHS_BALANCE', 'AMT_BALANCE', 'AMT_CREDIT_LIMIT_ACTUAL',\n                      'AMT_DRAWINGS_ATM_CURRENT', 'AMT_DRAWINGS_CURRENT', 'AMT_DRAWINGS_OTHER_CURRENT',\n                      'AMT_DRAWINGS_POS_CURRENT', 'AMT_INST_MIN_REGULARITY', 'AMT_PAYMENT_CURRENT', \n                      'AMT_PAYMENT_TOTAL_CURRENT', 'AMT_RECEIVABLE_PRINCIPAL', 'AMT_RECIVABLE', \n                      'AMT_TOTAL_RECEIVABLE']]","4b5c760c":"#Aggregate rows and group by current ID\ncc_agg = cc_bal1.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum'])","e4d624d6":"cc_agg","b34dc713":"# Combining numerical features\n#grp = cc_balance.drop('SK_ID_PREV', axis =1).groupby(by=['SK_ID_CURR']).mean().reset_index()\n#prev_columns = ['CC_'+column if column != 'SK_ID_CURR' else column for column in grp.columns ]\n#grp.columns = prev_columns\n#train_cc = train1.merge(grp, on =['SK_ID_CURR'], how = 'left')\n#train_cc.update(train[grp.columns].fillna(0))\n\n\n# Combining categorical features\n#cc_cat = pd.get_dummies(cc_balance.select_dtypes('object'))\n#cc_cat['SK_ID_CURR'] = cc_balance['SK_ID_CURR']\n#grp = cc_cat.groupby('SK_ID_CURR').mean().reset_index()\n#grp.columns = ['CC_'+column if column != 'SK_ID_CURR' else column for column in grp.columns]\n#train_cc = train1.merge(grp, on=['SK_ID_CURR'], how='left')\n#train1.update(train1[grp.columns].fillna(0))","6c1c6205":"## Credit Card Balance","09379775":"Of the 34 columns with missing data, 14 features have more than 50% missing values","038e0fb2":"## Feature Correlations","6962fc83":"Convert age to years from days","b32f22ca":"### Bureau and Bureau Balance","6285a468":"#eliminating outliers for numeric variables ## \nimport scipy.stats as stats\n\nQ1 = train1.quantile(q=.25)\nQ3 = train1.quantile(q=.75)\nIQR = train1.apply(stats.iqr)\n\n#only keep rows in dataframe that have values within 1.5*IQR of Q1 and Q3\ntrain_clean = train1[~((train1 < (Q1-1.5*IQR)) | (train1 > (Q3+1.5*IQR))).any(axis=1)]\n\n#find how many rows are left in the dataframe \ntrain_clean.shape","eee3205b":"#### Data Preparation and Feature Engineering","0a829797":"* This dataset consists of 307511 rows and 122 columns\n* Each row has unique id (SK_ID_CURR) and the output label (TARGET)\n* TARGET indicates by 0 (loan was repaid) or 1 (loan was not repaid)","d0d2b73a":"#### Additional data exploration","d7f9a903":"#### Exploratory Visualizations","ddee168b":"Consider dropping columns identified above. Potentially:\n* livingapartments_avg\n* livingarea_avg\n* cnt_fam_members\n* def_30_cent_social_circle\n* elevators_avg\n\n","f717bcae":"Credit levels are right skewed and outliers exist","0f15f5e6":"#### Target Distribution","f4a96ed7":"### Data Preparation & Feature Engineering"}}