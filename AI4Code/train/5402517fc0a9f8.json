{"cell_type":{"5bb6214f":"code","ad64c1a6":"code","d4c8afc7":"code","d7a9f4af":"code","6eb62185":"code","26f8f2ee":"markdown","fadc10a1":"markdown","baf1c4cc":"markdown","bbb93a20":"markdown","a743778b":"markdown","188ae45b":"markdown","7424edb9":"markdown","2a23bd2e":"markdown"},"source":{"5bb6214f":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image, display\nimport random\nimport math\n\nimport tensorflow as tf\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.utils import plot_model \nfrom tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Activation,Concatenate\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import backend, models","ad64c1a6":"\nbatch_size=128\nwidth=100\ntrain_dir='..\/input\/100-bird-species\/train\/';\ntest_dir='..\/input\/100-bird-species\/test\/'\nvalid_dir='..\/input\/100-bird-species\/valid\/';\n\ngenerator=ImageDataGenerator(rescale=1.\/255 )\ntrain_data=generator.flow_from_directory(train_dir, target_size=(width,width),batch_size=batch_size)\nvalid_data=generator.flow_from_directory(valid_dir, target_size=(width,width),batch_size=batch_size)\ntest_data=generator.flow_from_directory(test_dir, target_size=(width,width),batch_size=batch_size)\n\ntrain_steps_per_epoch=math.ceil(train_data.samples\/batch_size)\nvalid_steps_per_epoch=math.ceil(valid_data.samples\/batch_size)\ntest_steps_per_epoch=math.ceil(test_data.samples\/batch_size)","d4c8afc7":"model = Sequential()\nmodel.add(Conv2D(32, (3, 3), input_shape=(width, width, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Dropout(0.5))\n\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n\nmodel.add(Conv2D(256, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n\nmodel.add(Flatten())\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(250, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","d7a9f4af":"history=model.fit(train_data,\n                  steps_per_epoch =train_steps_per_epoch, \n                  validation_data=valid_data,\n                  epochs=23,\n                  validation_steps=valid_steps_per_epoch)","6eb62185":"#plot accuracy vs epoch\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot loss values vs epoch\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Evaluate against test data.\nscores = model.evaluate(test_data, verbose=1)\nprint('Test loss:', scores[0])\nprint('Test accuracy:', scores[1])","26f8f2ee":"# **Let's the results**\n","fadc10a1":"#  <h1 > Birds classification<\/h1>","baf1c4cc":"<h2>Althought in the epoch=23 the accuracy=81.9% we can see that accuracy on validation data don't really went up so much since epoch 15.<\/h2>\n<h2> As a result, we could stopped the training were val_accuracy wasn't changing<\/h2>\n<h2> Since ~epoch 19 our model is overfitting, training accuracy high, validation accuracy dropping <\/h2>\n","bbb93a20":"# Thoughts","a743778b":"<h2> What we could do to avoid overfitting ? <\/h2>\n\n* use data augumentation\n* change the model, model smaller\n","188ae45b":"# Imports","7424edb9":"# Building the model","2a23bd2e":"# reading data"}}