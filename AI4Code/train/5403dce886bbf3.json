{"cell_type":{"4e8cef80":"code","97340d3b":"code","ce60ca32":"code","3610db04":"code","50b87c63":"code","6f250d31":"code","911fa588":"code","d4c1bb6a":"code","749e3d74":"code","b2de1bb1":"code","747c302b":"code","b92e3a1d":"code","8b833ff9":"code","cf407906":"code","8c596c35":"code","2d822b46":"code","83132290":"code","4f2d919a":"code","e78311f3":"code","3ecd8398":"code","e14747e6":"code","3a446b38":"code","60b3af92":"code","44457baa":"code","ca4eb09e":"code","e77e8de4":"code","e9959523":"code","66c0cd8c":"code","faa85bf1":"code","bf0ba867":"code","14f0b03f":"code","5279ed85":"code","805fd8c6":"code","23c9bcb8":"code","c83c34da":"code","862d8e6a":"code","61d5fb34":"code","b90060d4":"code","5897cf7c":"code","8a5da078":"code","ec28b8ac":"code","aa197956":"code","db7961ae":"code","7813cc7a":"code","7a5a320b":"code","11d5b079":"code","c227089b":"code","faa57d61":"code","3ed4b608":"code","462fc373":"code","3184729b":"code","19eec524":"code","2a00f070":"code","f4baf8a6":"code","7b91607c":"code","5376d605":"markdown","4d541662":"markdown","bef2fa46":"markdown","796132d3":"markdown","20939db6":"markdown","4cd9310e":"markdown","bf1c8ab4":"markdown","7465e403":"markdown","e6ba1e89":"markdown","1f2f97be":"markdown","519bf0a9":"markdown","bd7f18b3":"markdown","d80d9503":"markdown","82854542":"markdown","832f58ef":"markdown","8419bc31":"markdown","8b9fab95":"markdown","b69231d3":"markdown","e22141f3":"markdown","c2c38213":"markdown","78f9ff34":"markdown","c24c8958":"markdown","edeba1a0":"markdown","7c3e879d":"markdown","ecb7e295":"markdown","73146b26":"markdown"},"source":{"4e8cef80":"#%config Completer.use_jedi = False","97340d3b":"from sklearn.model_selection import train_test_split\nimport numpy as np\n#import spacy\nimport pandas as pd\nimport os\nimport re\nimport torch\nfrom transformers import AutoTokenizer\nfrom datasets import Dataset\nfrom transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\nfrom transformers import AutoModelForSequenceClassification\nfrom datasets import Dataset, load_metric, DatasetDict\nimport nltk\nimport numpy as np\nfrom tqdm.auto import tqdm\nimport pyarrow.feather as feather\nfrom transformers import default_data_collator","ce60ca32":"ISLOCAL = False #if True, then it is for local development else Kaggle\n\nPREDICTOR = \"discourse_type\"\n\nsquad_v2 = True\n\n####################################################################################\n# DATA Preprocessing Switches\n####################################################################################\nUSE_LOCAL_DATASET = False # Will try to load raw_datasets from local directory if True. Else will create the dataset from scratch.\n\nUSE_SMALL_DATASET = False #If you want to train on whole dataset\nSMALL_DATASET_SIZE = 10\n\nDF_TRAIN_P1 = \"df_train_01\"\n\nTOP_COLUMNS_TO_KEEP = 13 #Dont try to have a dataset with ALL The types of predictions (like Claim 9 etc..) Just keep top colunns\n\n####################################################################################\n\n\n\n\n\n####################################################################################\n# TOKENIZER SWITCHES\n####################################################################################\nUSE_LOCAL_TOKENIZED_DATA = False\n\nTOKENIZER_NAME = \"roberta-base\" #\"distilbert-base-uncased\" #\"bert-base-uncased\" # \"allenai\/longformer-base-4096\" # \"distilbert-base-uncased\"\nTOKENIZED_PATH = \"{}_tokenized\".format(TOKENIZER_NAME)\n\n\nMAX_LEN = 480\nDOC_STRIDE = 200 #not multiple of 2...\n\n\n####################################################################################\n\n\n####################################################################################\n# Tokenized DataSet Constants\n####################################################################################\nTOKENIZER_PATH = \"{}_er\".format(TOKENIZER_NAME)\n\n\n####################################################################################\n\n\n####################################################################################\n# Model Constants\n####################################################################################\nUSE_LOCAL_MODEL = False\nMODEL_NAME = \"roberta-base\" #\"distilbert-base-uncased\" #if training a new model then the type\nMODEL_PATH = \"{}mdl\".format(MODEL_NAME) #if training a new model then its save path\nCHECKPOINT = \"checkpoint-38000\"\n\nif USE_LOCAL_MODEL:\n    MODEL_CHECKPOINT = os.path.join(MODEL_PATH, CHECKPOINT)\nelse:\n    MODEL_CHECKPOINT = MODEL_PATH\n\nBATCH_SIZE = 16\n\nEPOCHS = 1\n\n####################################################################################\n\n\n\n#CHECKPOINT = \"checkpoint-125\"\n\n\nDATASET_PATH = \"raw_dataset\"\n\n\n\nTRAINOUT_PATH = \"{}-finetuned\".format(TOKENIZER_NAME)\n\n#SHOULD_I_TRAIN_MODEL = False #if only inference is needed\n\nif ISLOCAL == True:\n    train_directory = \".\/train\"\n    test_directory = \".\/test\"\n    main_directory = \".\/\"\n    output_directory = \".\/QnA_Model_Output\/\"\n    save_directory = \".\/QnA_Model_Output\/\"\nelse:\n    train_directory = \"..\/input\/feedback-prize-2021\/train\"\n    test_directory = \"..\/input\/feedback-prize-2021\/test\"\n    main_directory = \"..\/input\/feedback-prize-2021\/\"\n    output_directory = \"..\/input\/robertatrainedmodel\/roberta-qa\/roberta-op\/\"\n    output_directory_model = \"..\/input\/robertatrainedmodel\/roberta_trained_results\/\"\n    save_directory = \".\/robertatrainedmodel\/\"\n    save_directory_datasets = \".\/robertatraindataset\/\"","3610db04":"def read_train_file(currid = \"423A1CA112E2\", curr_dir = train_directory):\n    with open(os.path.join(curr_dir, \"{}.txt\".format(currid)), \"r\") as f:\n        filetext = f.read()\n        \n    return filetext\n\n\ndef plot_displacy(df, currid = \"423A1CA112E2\", curr_dir = train_directory):\n    train = df.copy()\n    ents = []\n    for i, row in train[train['id'] == currid].iterrows():\n        ents.append({\n                        'start': int(row['discourse_start']), \n                         'end': int(row['discourse_end']), \n                         'label': row['discourse_type']\n                    })\n\n    data = read_train_file(currid, curr_dir)\n\n    docData = {\n        \"text\": data,\n        \"ents\": ents,\n    }\n\n    colors = {'Lead': '#EE11D0',\n              'Position': '#AB4DE1',\n              'Claim': '#1EDE71',\n              'Evidence': '#33FAFA',\n              'Counterclaim': '#4253C1',\n              'Concluding Statement': 'yellow',\n              'Rebuttal': 'red'}\n    options = {\"ents\": train.discourse_type.unique().tolist(), \"colors\": colors}\n    spacy.displacy.render(docData, style=\"ent\", options=options, manual=True, jupyter=True);","50b87c63":"# if ISLOCAL:\n#     train = pd.read_csv( os.path.join(main_directory, \"corrected_train.csv\") )\n# else:\n#     train = pd.read_csv(  \"..\/input\/feedback-prize-corrected-train-csv\/corrected_train.csv\")","6f250d31":"# #plot_displacy(train)\n\n# train[\"discourse_start\"] = train[\"new_start\"]\n# train[\"discourse_end\"] = train[\"new_end\"]\n\n# train[\"predictionstring\"] = train[\"new_predictionstring\"]\n# train[\"discourse_text\"] = train[\"text_by_new_index\"]","911fa588":"# #train[PREDICTOR].unique()\n\n# d_type_num_to_d_type = train[[\"discourse_type_num\", \"discourse_type\"]].drop_duplicates([\"discourse_type_num\"]).set_index([\"discourse_type_num\"]).to_dict()[\"discourse_type\"]","d4c1bb6a":"# d_type_num_to_d_type","749e3d74":"# column_to_keep = train[PREDICTOR].value_counts()[:50].index.values\n# print(\"We will only train the QnA data set for the columns : \", column_to_keep)","b2de1bb1":"#from tqdm.notebook import tqdm","747c302b":"# def return_training_dataset(dft, DIR = train_directory):\n#     '''\n#         This uses Prediction String to get start and end token numbers.\n#     '''\n#     DIR = train_directory\n#     ret = []\n\n#     for i in tqdm(train[\"id\"].unique()):\n#         temp = train[ train[\"id\"] == i]\n#         row = {\"id\" : i,\n#                \"context\" : read_train_file(i, DIR)\n#               }\n#         for j in temp[PREDICTOR]:\n#             p_str_beg = temp[temp[PREDICTOR] == j][\"predictionstring\"].values[0]\n#             p_str_beg, p_str_end = p_str_beg.split()[0], p_str_beg.split()[-1]\n            \n#             row.update( {\"start_{}\".format(j) : p_str_beg,\n#                          \"end_{}\".format(j) : p_str_end\n#                         }) #append the start and end tokens of the current discourse type as a column\n            \n#         ret.append(row.copy()) #append a single row per file id here\n#     df_train = pd.DataFrame(ret)\n#     df_train = df_train.rename( columns = { i: i.replace(\" \", \"_\") for i in df_train.columns} )\n#     df_train = df_train.fillna(-91) #-91 is arbitrary choice here.\n    \n#     return df_train\n\n# def return_training_dataset_v2(dft, DIR = train_directory):\n#     '''\n#         This uses Character positions to get start and end token numbers.\n#     '''\n#     print(\"CAUTION : We are NOT USING TOKEN POSITIONS, But character positions in the dataset now.\")\n#     DIR = train_directory\n#     train = dft.copy()\n#     ret = []\n\n#     for i in tqdm(train[\"id\"].unique()):\n#         temp = train[ train[\"id\"] == i]\n#         txt = read_train_file(i, DIR)\n        \n#         count = 0\n#         row = {\"id\" : i,\n#                     \"context\" : txt}\n        \n#         for j in temp[temp[PREDICTOR].isin(column_to_keep)].itertuples():\n#             p_str_beg = getattr(j , \"discourse_start\")\n#             p_str_end = getattr(j , \"discourse_end\")\n#             field_name = getattr(j, \"discourse_type_num\")\n            \n#             field_name = field_name.replace(\" \", \"|\") #later replace the - with space and map to the discourse type when creating QnA dataset.\n            \n#             row.update({\n#                     \"start_{}\".format(  field_name) : p_str_beg,\n#                     \"end_{}\".format( field_name  ) : p_str_end\n#               })\n#             count += 1 # Just a reference number to mark each column. Provides easier processing when creating question answering set...\n            \n#         ret.append(row.copy()) #append a single row per file id here\n#     df_train = pd.DataFrame(ret)\n    \n#     #df_train = df_train.rename( columns = { i: i.replace(\" \", \"_\") for i in df_train.columns} ) Not necessary after - character introduction\n    \n#     df_train = df_train.fillna(-91) #-91 is arbitrary choice here.\n    \n#     return df_train\n\n# if USE_LOCAL_DATASET == False:\n#     df_train = return_training_dataset_v2(train,\n#                                        train_directory)\n# else:\n#     print(\"Will use local dataset and not generate right now\")","b92e3a1d":"# df_train.head(1)","8b833ff9":"# df_train.shape","cf407906":"# #df_train.info()\n# if USE_LOCAL_DATASET == False:\n#     for i in df_train.columns:\n#         try:\n#             df_train[i] = pd.to_numeric(df_train[i], errors = 'raise')\n#         except ValueError as ve:\n#             #print(ve, i)\n#             continue\n# else:\n#     print(\"Will use local dataset and not generate right now\")","8c596c35":"# DEBUG BLOCK ONLY\n#USE_LOCAL_DATASET = True","2d822b46":"# if USE_LOCAL_DATASET == False:\n#     if os.path.exists(save_directory_datasets) == False:\n#         os.makedirs(save_directory_datasets)\n\n#     paths = os.path.join( save_directory_datasets, DF_TRAIN_P1)\n#     print(\"Trying to save the training dataset to {}\".format(paths))\n#     feather.write_feather( df_train,\n#                           paths)\n# else:\n#     #paths = os.path.join( output_directory, DF_TRAIN_P1)\n#     paths = '.\/robertatraindataset\/df_train_01'\n#     assert os.path.exists( paths ), \"Path does not exist for df_train part 01. {}\".format(paths)\n#     df_train = feather.read_feather(paths)","83132290":"# print(\"loaded dataset shape is \", df_train.shape)","4f2d919a":"# def prepare_question_answer_dataset(df_t):\n#     X_valid = df_t.copy()\n    \n#     unique_d = column_to_keep ##train[\"discourse_type_num\"].unique()\n#     unique_d = [x.replace(\" \", \"_\") for x in unique_d]\n#     #X_valid\n\n#     info_cols = [x for x in X_valid.columns if x.find(\"start_\") > -1]\n    \n#     ret = []\n#     from collections import defaultdict\n    \n    \n#     for i in tqdm(X_valid.iterrows(), total = len(X_valid)):\n#         #for j in unique_d:\n        \n#         #for j, d_type in d_type_num_to_d_type.items():\n#         type_counts = defaultdict(int)\n        \n#         for j in info_cols:\n#             current_column = j.split(\"_\")[1]\n#             current_question = current_column.replace(\"|\", \" \")\n#             current_question = d_type_num_to_d_type[current_question]\n\n#             start_e = int (i[1][\"start_\" + current_column])\n#             end_e = int (i[1][\"end_\" + current_column])\n            \n#             context = i[1][\"context\"]\n            \n            \n\n#             if (end_e == -91) or (start_e == -91):\n#                 answer_start = []\n#                 answer_text = []\n#             else:\n                \n#                 answer_start = [start_e]\n#                 answer_text = [context[start_e : end_e + 1]]\n                \n#             type_counts[current_question] += 1\n#             if answer_start == [] and type_counts[current_question] > 1:\n#                 continue\n#             else:\n#                 ret.append( { \"id\" : i[1][\"id\"],\n#                              \"context\" : context,\n#                               \"question\" : current_question,\n#                               \"answers\" : { \"text\" :  answer_text,\n#                                             \"answer_start\" : answer_start},\n#                               \"start_position\" : start_e,\n#                               \"end_position\" : end_e,\n\n#                             })\n#     return pd.DataFrame(ret)\n\n# if USE_LOCAL_DATASET == False:\n#     df_train = prepare_question_answer_dataset(df_train)\n# else:\n#     print(\"Will use local dataset and not generate right now\")","e78311f3":"# df_train.head(30)","3ecd8398":"# if USE_LOCAL_DATASET == False:\n#     train_size = 0.8\n    \n#     train_id = np.random.choice( df_train[\"id\"].unique(), \n#                                   int(train_size * len(df_train[\"id\"].unique())),\n#                                  replace = False)\n#     valid_id = df_train.loc[~df_train[\"id\"].isin(train_id), \"id\"]\n#     test_id = df_train.loc[~df_train[\"id\"].isin(train_id), \"id\"]\n    \n#     X_train = df_train[ df_train[\"id\"].isin(train_id) ]\n#     X_test = df_train[df_train[\"id\"].isin(valid_id) ]\n#     X_valid = df_train[df_train[\"id\"].isin(valid_id) ]\n    \n# #     X_train, X_test = train_test_split( df_train,\n# #                                        train_size = 0.7,\n# #                                       random_state = 91)\n\n# #     X_test, X_valid = train_test_split( X_test,\n# #                                        train_size = 0.5,\n# #                                       random_state = 91)\n#     del(df_train)\n# else:\n#     print(\"Will use local dataset and not generate right now\")","e14747e6":"# if USE_LOCAL_DATASET == False:\n#     train_set = Dataset.from_pandas(X_train)\n#     test_set = Dataset.from_pandas(X_test)\n#     valid_set = Dataset.from_pandas(X_valid)\n# else:\n#     print(\"Will use local dataset and not generate right now\")","3a446b38":"# if USE_LOCAL_DATASET == False:\n#     raw_datasets = DatasetDict( {\n#         'train' : train_set,\n#         'test': test_set,\n#         'validation': valid_set\n#     })\n# else:\n#     print(\"Will use local dataset and not generate right now\")","60b3af92":"# import datasets\n\n# if USE_LOCAL_DATASET == False:\n#     paths = os.path.join(save_directory_datasets, DATASET_PATH)\n#     print(\"Saving dataset to disk. \", paths)\n    \n#     raw_datasets.save_to_disk( paths )\n#     del(X_train)\n#     del(X_test)\n#     del(X_valid)\n#     del(train_set)\n#     del(test_set)\n#     del(valid_set)\n    \n# else:\n#     #paths = os.path.join(output_directory, DATASET_PATH)\n#     paths = \"..\/input\/rawdataset\/raw_dataset\"\n#     assert os.path.exists(paths), \"The dataset local path does not exist. Please retrain the notebook with appropriate switches.\"\n#     print(\"Loading the dataset from local directory. \", paths)\n#     raw_datasets = datasets.load_from_disk( paths )","44457baa":"# print(\"Shape of the raw datasets is : \", raw_datasets.shape)","ca4eb09e":"# from transformers import AutoTokenizer","e77e8de4":"# if USE_LOCAL_TOKENIZED_DATA == False:\n#     paths = os.path.join( save_directory_datasets, TOKENIZER_PATH)\n    \n#     tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)\n#     print(\"Saving Tokenizer to disk\", paths)\n#     tokenizer.save_pretrained( paths )\n    \n# else:\n#     #paths = os.path.join( output_directory, TOKENIZER_PATH)\n#     paths = \"..\/input\/robertabasetokernizer\/roberta-base_er\"\n#     assert os.path.exists(paths), \"Tokenizer path does not exist {}\".format(paths)\n    \n#     print(\"Loading the tokenizer now\")\n#     tokenizer = AutoTokenizer.from_pretrained( paths )","e9959523":"# import transformers\n# assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast), \"Tokenizer is not an instance of Fast tokenizers written in Rust. Please check https:\/\/huggingface.co\/transformers\/index.html#bigtable\"","66c0cd8c":"# pad_on_right = tokenizer.padding_side == \"right\"","faa85bf1":"# def prepare_train_features(examples):\n#     # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n#     # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n#     # left whitespace\n#     examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n\n#     # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n#     # in one example possible giving several features when a context is long, each of those features having a\n#     # context that overlaps a bit the context of the previous feature.\n#     tokenized_examples = tokenizer(\n#         examples[\"question\" if pad_on_right else \"context\"],\n#         examples[\"context\" if pad_on_right else \"question\"],\n#         truncation=\"only_second\" if pad_on_right else \"only_first\",\n#         max_length = MAX_LEN,\n#         stride = DOC_STRIDE,\n#         return_overflowing_tokens=True,\n#         return_offsets_mapping=True,\n#         padding=\"max_length\",\n#     )\n\n#     # Since one example might give us several features if it has a long context, we need a map from a feature to\n#     # its corresponding example. This key gives us just that.\n#     sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n#     # The offset mappings will give us a map from token to character position in the original context. This will\n#     # help us compute the start_positions and end_positions.\n#     offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n\n#     # Let's label those examples!\n#     tokenized_examples[\"start_positions\"] = []\n#     tokenized_examples[\"end_positions\"] = []\n\n#     for i, offsets in enumerate(offset_mapping):\n#         # We will label impossible answers with the index of the CLS token.\n#         input_ids = tokenized_examples[\"input_ids\"][i]\n#         cls_index = input_ids.index(tokenizer.cls_token_id)\n\n#         # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n#         sequence_ids = tokenized_examples.sequence_ids(i)\n\n#         # One example can give several spans, this is the index of the example containing this span of text.\n#         sample_index = sample_mapping[i]\n#         answers = examples[\"answers\"][sample_index]\n#         # If no answers are given, set the cls_index as answer.\n#         if len(answers[\"answer_start\"]) == 0:\n#             tokenized_examples[\"start_positions\"].append(cls_index)\n#             tokenized_examples[\"end_positions\"].append(cls_index)\n#         else:\n#             # Start\/end character index of the answer in the text.\n#             start_char = answers[\"answer_start\"][0]\n#             end_char = start_char + len(answers[\"text\"][0])\n\n#             # Start token index of the current span in the text.\n#             token_start_index = 0\n#             while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n#                 token_start_index += 1\n\n#             # End token index of the current span in the text.\n#             token_end_index = len(input_ids) - 1\n#             while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n#                 token_end_index -= 1\n\n#             # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n#             if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n#                 tokenized_examples[\"start_positions\"].append(cls_index)\n#                 tokenized_examples[\"end_positions\"].append(cls_index)\n#             else:\n#                 # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n#                 # Note: we could go after the last offset if the answer is the last word (edge case).\n#                 while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n#                     token_start_index += 1\n#                 tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n#                 while offsets[token_end_index][1] >= end_char:\n#                     token_end_index -= 1\n#                 tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n\n#     return tokenized_examples","bf0ba867":"# if USE_LOCAL_TOKENIZED_DATA == False:\n#     tokenized_datasets = raw_datasets.map(prepare_train_features, \n#                                           batched = True, \n#                                           remove_columns = raw_datasets[\"train\"].column_names)\n# else:\n#     print(\"Not tokenizing the dataset as load from local disk is set.\")","14f0b03f":"# if USE_LOCAL_TOKENIZED_DATA == False:\n#     paths = os.path.join( save_directory_datasets, TOKENIZED_PATH)\n#     print(\"Saving the tokenized dataset to local disk for re-usage\", paths)\n#     tokenized_datasets.save_to_disk(paths)\n# else:\n#     #paths = os.path.join( output_directory, TOKENIZED_PATH)\n#     paths = \"..\/input\/robertabasetokenized\/roberta-base_tokenized\"\n#     assert os.path.exists(paths), \"Switch set to local tokenized data loader but path does not exist. {}\".format(paths)\n#     tokenized_datasets = datasets.load_from_disk(paths)","5279ed85":"# print(\"Tokenized dataset shape is :\", tokenized_datasets.shape)","805fd8c6":"# from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer","23c9bcb8":"# # if USE_LOCAL_MODEL == False:\n# #     paths = os.path.join( save_directory , MODEL_CHECKPOINT)\n# #     print(\"Trying to load model\", MODEL_NAME)\n# #     model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME)\n# # else:\n# #     paths = os.path.join( output_directory_model , MODEL_CHECKPOINT)\n# #     assert os.path.exists(paths), \"Path to the checkpointed model does not exist {}\".format(paths)\n# #     model = AutoModelForQuestionAnswering.from_pretrained(paths)\n\n# # Below is for inference path\n# model = AutoModelForQuestionAnswering.from_pretrained(\"..\/input\/epoch1\/FINAL_MODEL_OUTPUT\/\")","c83c34da":"# DEBUG BLOCK\n#USE_SMALL_DATASET = True","862d8e6a":"# if USE_SMALL_DATASET:\n#     print(\"CAUTION: Using a small subset of data as per the switches. \")\n#     small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed = 91).select(range(SMALL_DATASET_SIZE))\n#     small_eval_dataset = tokenized_datasets[\"validation\"].shuffle(seed = 91).select(range(SMALL_DATASET_SIZE))\n# #    small_test_dataset = tokenized_datasets[\"test\"].shuffle(seed = 91).select(range(SMALL_DATASET_SIZE))\n# else:\n#     small_train_dataset = tokenized_datasets[\"train\"]\n#     small_eval_dataset = tokenized_datasets[\"validation\"]\n# #    small_test_dataset = tokenized_datasets[\"test\"]","61d5fb34":"# args = TrainingArguments(\n#     output_dir = MODEL_PATH,\n#     logging_steps = min(5000, len(small_train_dataset)),\n#     save_strategy = \"steps\",\n#     evaluation_strategy = \"steps\",\n#     save_steps = min(5000, len(small_train_dataset)),\n#     eval_steps = min(5000, len(small_train_dataset)),\n#     learning_rate=2e-5,\n#     per_device_train_batch_size = BATCH_SIZE,\n#     per_device_eval_batch_size = BATCH_SIZE,\n#     num_train_epochs = EPOCHS,\n#     weight_decay = 0.01,\n#     save_total_limit = 3,\n# #     fp16 = True,\n# #     fp16_full_eval = True,\n    \n#     load_best_model_at_end = True,\n#     report_to = \"none\",\n# )","b90060d4":"# data_collator = default_data_collator","5897cf7c":"# trainer = Trainer(\n#     model,\n#     args,\n#     train_dataset=small_train_dataset,   #tokenized_datasets[\"train\"],\n#     eval_dataset=small_eval_dataset,     #tokenized_datasets[\"validation\"],\n#     data_collator=data_collator,\n#     tokenizer=tokenizer,\n# )","8a5da078":"# if USE_LOCAL_MODEL == False:\n#     trainer.train()\n# else:\n#     print(\"No training today for you\")","ec28b8ac":"# if USE_LOCAL_MODEL == False:\n#     trainer.save_model( \".\/FINAL_MODEL_OUTPUT\" )\n# else:\n#     print(\"Cannot train without my training shorts.\")\n\n","aa197956":"import transformers, datasets\nfrom transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n\ntokenized_datasets = datasets.load_from_disk(\"..\/input\/robertaqnatokenized\/roberta-base_tokenized\")\n\n\n\nsmall_train_dataset = tokenized_datasets[\"train\"]\nsmall_eval_dataset = tokenized_datasets[\"validation\"].shuffle(seed = 91).select(range(10))\n\ntokenizer = AutoTokenizer.from_pretrained( \"..\/input\/robertabasetokernizer\/roberta-base_er\" )\n\nmodel = AutoModelForQuestionAnswering.from_pretrained(\"..\/input\/roberta-qna-train\/FINAL_MODEL_OUTPUT\/\")\n\nraw_datasets = datasets.load_from_disk(\"..\/input\/rawdataset\/raw_dataset\")\n\n\ntrainer = Trainer(model)\n\npad_on_right = tokenizer.padding_side == \"right\"","db7961ae":"def prepare_validation_features(examples):\n    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n    # left whitespace\n    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n\n    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n    # in one example possible giving several features when a context is long, each of those features having a\n    # context that overlaps a bit the context of the previous feature.\n    tokenized_examples = tokenizer(\n        examples[\"question\" if pad_on_right else \"context\"],\n        examples[\"context\" if pad_on_right else \"question\"],\n        truncation=\"only_second\" if pad_on_right else \"only_first\",\n        max_length=MAX_LEN,\n        stride=DOC_STRIDE,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    # Since one example might give us several features if it has a long context, we need a map from a feature to\n    # its corresponding example. This key gives us just that.\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n\n    # We keep the example_id that gave us this feature and we will store the offset mappings.\n    tokenized_examples[\"example_id\"] = []\n\n    for i in range(len(tokenized_examples[\"input_ids\"])):\n        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        context_index = 1 if pad_on_right else 0\n\n        # One example can give several spans, this is the index of the example containing this span of text.\n        sample_index = sample_mapping[i]\n        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n\n        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n        # position is part of the context or not.\n        tokenized_examples[\"offset_mapping\"][i] = [\n            (o if sequence_ids[k] == context_index else None)\n            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n        ]\n\n    return tokenized_examples","7813cc7a":"import collections\n\ndef calc_word_indices(full_text, discourse_start, discourse_end):\n    start_index = len(full_text[:discourse_start].split())\n    token_len = len(full_text[discourse_start:discourse_end].split())\n    output = list(range(start_index, start_index + token_len))\n    if len(output) == 0:\n        return [] #edge case. If the text predicted was \"\\r\" or \"\\n\" then the output would be empty throwing error at output[-1]\n    \n    if output[-1] >= len(full_text.split()):\n        output = list(range(start_index, start_index + token_len-1))\n    return output\n    \n\ndef get_raw_predictions( submission_dataset, \n                        model = trainer,\n                           column_to_be_removed = raw_datasets[\"validation\"].column_names ):\n    print(\"Processing a dataset of length \", len(submission_dataset) )\n\n    validation_features = submission_dataset.map(\n        prepare_validation_features,\n        batched = True,\n        remove_columns = column_to_be_removed\n    )\n    \n    validation_features.set_format(type=validation_features.format[\"type\"], columns=list(validation_features.features.keys()))\n    \n    print(\"Now appending features per example.\")\n    \n    import collections\n    examples = submission_dataset\n    features = validation_features\n\n    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n    features_per_example = collections.defaultdict(list)\n    for i, feature in enumerate(features):\n        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n    \n    \n    \n    return postprocess_qa_predictions(examples,\n                                      features,\n                                      model.predict(validation_features).predictions,\n                                      n_best_size = 20,\n                                      max_answer_length = 110\n                                     )\n\n\nfrom tqdm.auto import tqdm\n\ndef postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 110):\n    all_start_logits, all_end_logits = raw_predictions\n    # Build a map example to its corresponding features.\n    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n    features_per_example = collections.defaultdict(list)\n    for i, feature in enumerate(features):\n        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n\n    # The dictionaries we have to fill.\n    #predictions = collections.OrderedDict()\n    predictions = []\n\n    # Logging.\n    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n\n    # Let's loop over all the examples!\n    for example_index, example in enumerate(tqdm(examples)):\n        # Those are the indices of the features associated to the current example.\n        feature_indices = features_per_example[example_index]\n\n        min_null_score = None # Only used if squad_v2 is True.\n        valid_answers = []\n        \n        context = example[\"context\"]\n        question = example[\"question\"]\n        # Looping through all the features associated to the current example.\n        for feature_index in feature_indices:\n            # We grab the predictions of the model for this feature.\n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n            # This is what will allow us to map some the positions in our logits to span of texts in the original\n            # context.\n            offset_mapping = features[feature_index][\"offset_mapping\"]\n\n            # Update minimum null prediction.\n            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n            if min_null_score is None or min_null_score < feature_null_score:\n                min_null_score = feature_null_score\n\n            # Go through all possibilities for the `n_best_size` greater start and end logits.\n            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n                    # to part of the input_ids that are not in the context.\n                    if (\n                        start_index >= len(offset_mapping)\n                        or end_index >= len(offset_mapping)\n                        or offset_mapping[start_index] is None\n                        or offset_mapping[end_index] is None\n                    ):\n                        continue\n                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n\n                    start_char = offset_mapping[start_index][0]\n                    end_char = offset_mapping[end_index][1]\n                    valid_answers.append(\n                        {\n                            \"score\": start_logits[start_index] + end_logits[end_index],\n                            \"text\": context[start_char: end_char],\n                            \"discourse_start\" : start_char, \n                            \"discourse_end\" : end_char,\n                            \"question\" : question\n                        }\n                    )\n        \n        if len(valid_answers) > 0:\n            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n        else:\n            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n            # failure.\n            best_answer = {\"score\": 0.0,\n                           \"text\": \"\", \n                           \"discourse_start\" : -91,\n                           \"discourse_end\" : -91,\n                           \"question\" : question}\n        \n        # Let's pick our final answer: the best one or the null answer (only for squad_v2)\n        if not squad_v2:\n            predictions.append( {\"id\" : example[\"id\"],\n                                 \"text\" : best_answer[\"text\"],\n                                 \"discourse_start\" : best_answer[\"discourse_start\"],\n                                 \"discourse_end\" : best_answer[\"discourse_end\"],\n                                 \"question\" : best_answer[\"question\"]\n                                }\n                              )\n            # predictions[example[\"id\"]] = best_answer[\"text\"]\n        else:\n            if min_null_score is None:# WARNING THIS IS ACTUALLY WRONG AND INDICATES A BUG SOMEWHERE - AAM 2021-12-29\n                min_null_score = 0 # WARNING THIS IS ACTUALLY WRONG AND INDICATES A BUG SOMEWHERE - AAM 2021-12-29\n            \n            #answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n            \n            any_answer = 0 #default answer is nothing..\n            prev_start, prev_end = 1000, -1000\n            \n            valid_answers = sorted(valid_answers, key = lambda x: x[\"discourse_start\"],reverse= False)\n            # we sort valid answers by position so that we can filter below.\n            for all_ans in valid_answers:\n                    if all_ans[\"score\"] >= min_null_score:\n                        curr_start, curr_end = all_ans[\"discourse_start\"], all_ans[\"discourse_end\"]\n                        if (curr_start > prev_end) or (curr_end < prev_start):\n                            #This sequence has no overlap with the previous. So we may keep it. (TODO make it foolproof please)\n                            any_answer += 1\n                            predictions.append( {\"id\" : example[\"id\"],\n                                                 \"text\" : all_ans[\"text\"],\n                                                 \"discourse_start\" : curr_start,\n                                                 \"discourse_end\" : curr_end,\n                                                 \"question\" : question\n                                                }\n                                              )\n                            prev_start = curr_start\n                            prev_end = curr_end\n                        \n            if any_answer == 0: #there was no answer better than the scoring threshold.\n                predictions.append( {\"id\" : example[\"id\"],\n                                     \"text\" : \"\",\n                                     \"discourse_start\" : -91,\n                                     \"discourse_end\" : -91,\n                                     \"question\" : question\n                                    }\n                                  )\n#             if answer == \"\":\n#                 predictions.append( {\"id\" : example[\"id\"],\n#                                      \"text\" : \"\",\n#                                      \"discourse_start\" : -91,\n#                                      \"discourse_end\" : -91,\n#                                      \"question\" : question\n#                                     }\n#                                   )\n#             else:\n                \n#                 predictions.append( {\"id\" : example[\"id\"],\n#                                      \"text\" : answer,\n#                                      \"discourse_start\" : best_answer[\"discourse_start\"],\n#                                      \"discourse_end\" : best_answer[\"discourse_end\"],\n#                                      \"question\" : question\n#                                     }\n#                                   )\n                \n            #predictions[example[\"id\"]] = answer\n\n    return predictions","7a5a320b":"df_ss = pd.read_csv( os.path.join( main_directory, \"sample_submission.csv\" ) )","11d5b079":"# 'id', 'context', 'question', 'answers', 'start_position', 'end_position', '__index_level_0__'\ndef convert_subm_to_dataset(ss_df, experimental = False):\n    df_ss = ss_df.copy()\n    ret = []\n    counter = 0\n    for i in tqdm(df_ss[\"id\"].unique()):\n        for j in ['Lead', 'Position', 'Evidence', 'Claim', 'Concluding Statement',\n       'Counterclaim', 'Rebuttal']:\n            counter += 1\n            if experimental:\n                ret.append( {\"id\" : \"{}_{}\".format(i, counter),\n                         \"question\" : j.replace(\" \", \"_\"),\n                         \"context\" : read_train_file( i, test_directory ),\n                         \"answers\" : [],\n                         \"start_position\" : [],\n                         \"end_position\" : [],\n                         \"__index_level_0__\" : []\n                        } )\n            else:\n                ret.append( {\"id\" : i,\n                             \"question\" : j.replace(\" \", \"_\"),\n                             \"context\" : read_train_file( i, test_directory ),\n                             \"answers\" : [],\n                             \"start_position\" : [],\n                             \"end_position\" : [],\n                             \"__index_level_0__\" : []\n                            } )\n    \n    subm_pandas = pd.DataFrame( ret )\n    subm = Dataset.from_pandas( pd.DataFrame(ret) )\n    return subm\n\ndef preds_to_pandas(final_pred,\n                   experimental = False,\n                   direc = test_directory):\n    ret = []\n    for i in final_pred:\n        if i[\"text\"] != \"\":\n            if experimental == False:\n                fileid = i[\"id\"]\n            else:\n                fileid = i[\"id\"].split(\"_\")[0]\n                \n            txt = read_train_file( fileid , direc )\n            #print(\"DEBUG\", len(txt), i[\"discourse_start\"], i[\"discourse_end\"], fileid)\n            predictionstring = calc_word_indices(txt, i[\"discourse_start\"], i[\"discourse_end\"])\n            #print(predictionstring[:-1]) # THERE IS AN ERROR IN TRAINING MODEL. Last token is included by mistake.\n            ret.append({ \"id\" : fileid,\n                        \"class\" : i[\"question\"].replace(\"_\", \" \"),\n                        \"predictionstring\" : \" \".join([str(x) for x in predictionstring[:-1]])\n                       }\n                      )\n    return pd.DataFrame(ret)\n\n","c227089b":"def get_line_by_line_predictions(df_ss,\n                                experimental = False,\n                                experimental_batchsize = 1500):\n    aam2 = convert_subm_to_dataset(df_ss,\n                                  experimental = experimental)\n    if experimental == False:\n        preds = []\n        for i in tqdm(range(len(aam2))):\n            p = get_raw_predictions( aam2.select([i]),\n                                model = trainer,\n                                column_to_be_removed = raw_datasets[\"validation\"].column_names\n                               )\n            preds.extend(p)\n    else:\n        last_start = 0\n        preds = []\n        ranges = [x for x in range( experimental_batchsize, len(aam2)) if x % experimental_batchsize == 0 or x == (len(aam2)-1)]\n        if len(aam2) < experimental_batchsize + 1:\n            ranges = [len(aam2) - 1]\n        for i in ranges:\n            print(\"Processing entries in batches of batches. Current batch is {} to {}\".format(last_start, i))\n            aam2_batch = aam2.select( range(last_start, i) )\n            preds_row = get_raw_predictions( aam2_batch,\n                                        model = trainer,\n                                        column_to_be_removed = raw_datasets[\"validation\"].column_names\n                                       )\n            last_start = i\n            \n            preds.extend(preds_row)\n\n    final_merged = preds_to_pandas(preds,\n                                  experimental = experimental)\n    return final_merged","faa57d61":"final_merged = get_line_by_line_predictions(df_ss, \n                                           experimental = True)","3ed4b608":"final_merged = final_merged[final_merged[\"predictionstring\"].str.strip() != \"\"]\n\nif len(final_merged) > 0:\n    final_merged = final_merged.sort_values( by = [\"id\", \"predictionstring\"] )","462fc373":"# final_merged.to_csv(\"submission.csv\",\n#                    index = False)","3184729b":"def filter_by_rules(submission_df,\n                   verbose = False):\n    df_temp = submission_df.copy()\n    \n    d = []\n    for i in train[\"discourse_type\"].unique():\n        d.append( {\"class\" : i, \"predictionstring\" : \"0 1\", \"id\" : \"UNKNOWN\"})\n    d = pd.DataFrame(d)\n    \n    df_temp = pd.concat([ df_temp, d], axis = 0) #To ensure that the new dataframe contains all possible categories of predictions\n\n    df_temp = df_temp.rename(columns = {\"class\": \"discourse_type\"})\n    df_temp[\"startpred\"] = df_temp[\"predictionstring\"].apply(lambda x: x.split()[0])\n    df_temp[\"endpred\"] = df_temp[\"predictionstring\"].apply(lambda x: x.split()[-1])\n    df_temp[\"totaltokens\"] = df_temp[\"predictionstring\"].apply(lambda x: len(x.split()))\n    df_temp[\"startpred\"] = pd.to_numeric(df_temp[\"startpred\"])\n    df_temp[\"endpred\"] = pd.to_numeric(df_temp[\"endpred\"])\n    df_temp[\"totaltokens\"] = pd.to_numeric(df_temp[\"totaltokens\"])\n    df_temp = df_temp.sort_values(by = [\"id\", \"startpred\", \"totaltokens\"])\n    df_temp[\"position\"] = 1\n    df_temp[\"position\"] = df_temp.groupby('id')['position'].cumsum()\n    df_temp[\"verdict\"] = \"keep\"\n\n    d_details = df_temp[[\"discourse_type\", \"id\"]].pivot_table( columns = [\"discourse_type\"],\n                    index = [\"id\"],\n                    aggfunc = len).reset_index()\n\n    d_details = d_details.fillna(0)\n\n    for i in tqdm(df_temp[\"id\"].unique()):\n        testing_id = i\n        KEEP_TYPE = \"LARGER\" #either LARGER or SMALLER. Unused in this submission\n\n        ret = []\n        #check 1. Only a single Lead (for extreme cases there are max 2 Leads)\n        if d_details.loc[d_details[\"id\"] == testing_id, \"Lead\"].values[0] > 1:\n            # Check 1: If we have leads beyond position 2, then we delete all of them (considering we do have a lead at position 1.)\n\n            lead_at_pos1 = len(df_temp[ (df_temp[\"id\"] == testing_id) & (df_temp[\"discourse_type\"] == \"Lead\") & (df_temp[\"position\"] == 1)])\n            lead_at_pos2 = len(df_temp[ (df_temp[\"id\"] == testing_id) & (df_temp[\"discourse_type\"] == \"Lead\") & (df_temp[\"position\"] == 2)])\n            if lead_at_pos1 > 0: # we have a Lead at the beginning\n                if lead_at_pos2 > 0: #we also have a Lead at position 2\n                    if verbose:\n                        print(\"Lead at position 1 and 2 detected..\", testing_id)\n                    df_temp.loc[ (df_temp[\"id\"] == testing_id) & (df_temp[\"discourse_type\"] == \"Lead\") & (df_temp[\"position\"] == 2)\n                         , \"verdict\"] = \"delete\"\n                    # We are mergind position 1 & position 2 Leads here\n                    df_temp.loc[ (df_temp[\"id\"] == testing_id) & \\\n                                (df_temp[\"discourse_type\"] == \"Lead\") & \\\n                                (df_temp[\"position\"] == 1)\\\n                                , \"endpred\"] = df_temp.loc[ (df_temp[\"id\"] == testing_id) & \\\n                                                    (df_temp[\"discourse_type\"] == \"Lead\") & \\\n                                                    (df_temp[\"position\"] == 2),\n                                                    \"endpred\"].values[0]\n                else: #Position 2 is not a Lead so a Lead is detected at other positions. Lets Delete them.\n                    if verbose:\n                        print(\"Lead at position 1 and greater than 2 detected..\", testing_id)\n                    df_temp.loc[ (df_temp[\"id\"] == testing_id) & (df_temp[\"discourse_type\"] == \"Lead\") & (df_temp[\"position\"] > 2)\n                         , \"verdict\"] = \"delete\"\n            else: #We have multiple Leads and none at position 1 :)\n                if verbose:\n                    print(\"Multiple leads but none at position 1 detected.\", testing_id)\n                # This is a problem. We have multiple Leads but not in the beginning of the passage.\n                # WE can either delete all of them. Or keep them if they are within a percentage start of the passage.\n                # We are keeping the largest Lead in this case for now.\n                all_leads = df_temp.loc[ (df_temp[\"id\"] == testing_id) & (df_temp[\"discourse_type\"] == \"Lead\"), :].copy()\n                all_leads = all_leads.sort_values(by = [\"totaltokens\"], ascending = False)\n                df_temp.loc[df_temp.index.isin(all_leads.index[1:]), \"verdict\"] = \"delete\"\n        if d_details.loc[d_details[\"id\"] == testing_id, \"Concluding Statement\"].values[0] > 1:\n            # we have multiple concluding statements here.\n            if verbose:\n                print(\"Multiple concluding statements detected. \")\n            max_pos = df_temp.loc[ (df_temp[\"id\"] == testing_id) & (df_temp[\"discourse_type\"] == \"Concluding Statement\"), \"position\"].values[-1]\n            second_pos = df_temp.loc[ (df_temp[\"id\"] == testing_id) & (df_temp[\"discourse_type\"] == \"Concluding Statement\"), \"position\"].values[-2]\n            if abs(second_pos - max_pos) == 1:\n                # Both values are next to each other. Lets try to merge them together\n                if verbose:\n                    print(\"Two concluding statements next to each other detected. Merging and keeping one of them\", testing_id)\n                df_temp.loc[ (df_temp[\"id\"] == testing_id) & \\\n                            (df_temp[\"position\"] == second_pos) & \\\n                            (df_temp[\"discourse_type\"] == \"Concluding Statement\"), \"endpred\"] = df_temp.loc[ \\\n                                                                                          (df_temp[\"id\"] == testing_id) & \\\n                                                                                         (df_temp[\"position\"] == max_pos), \"endpred\"].values[0]\n                df_temp.loc[(df_temp[\"id\"] == testing_id) & \\\n                            (df_temp[\"position\"] == max_pos), \"verdict\"] = \"delete\"\n            else:\n                if verbose:\n                    print(\"Two concluding statements found NOT Next to each other. Keeping the last one only.\", testing_id)\n                max_pos = df_temp.loc[ (df_temp[\"id\"] == testing_id) & (df_temp[\"discourse_type\"] == \"Concluding Statement\"), \"position\"].values[-1]\n                df_temp.loc[ (df_temp[\"id\"] == testing_id) & (df_temp[\"discourse_type\"] == \"Concluding Statement\") & \\\n                           (df_temp[\"position\"] != max_pos), \"verdict\"] = \"delete\"\n\n\n        if d_details.loc[d_details[\"id\"] == testing_id, \"Rebuttal\"].values[0] > d_details.loc[d_details[\"id\"] == testing_id, \"Counterclaim\"].values[0]:\n            if verbose:\n                print(\"Number of Rebuttal is greater than Counterclaims.\", testing_id)\n                \n    df_temp = df_temp[df_temp[\"id\"] != \"UNKNOWN\"] #remove our artifact colletion\n    return df_temp\n\ndef get_filtered_data(r):\n    ret2 = filter_by_rules(r,\n                   verbose = False)\n    ret2 = ret2[ret2[\"verdict\"] == \"keep\"]\n    ret2 = ret2.rename( columns = { \"discourse_type\" : \"class\"})\n    ret2 = ret2[[\"id\", \"class\", \"predictionstring\"]]\n    return ret2","19eec524":"ret = get_filtered_data( final_merged )\n\nret.to_csv(\"submission.csv\", index = False)\n\nret.head(1)","2a00f070":"# ret.head(5)","f4baf8a6":"# df = pd.read_csv(\"..\/input\/feedback-prize-corrected-train-csv\/corrected_train.csv\")","7b91607c":"# df[\"discourse_type\"].unique()","5376d605":"# Load DataColator\n\n* After all no size is perfect","4d541662":"# Evaluation","bef2fa46":"# Initialize Trainer\n\n* Put those shorts on","796132d3":"# Apply tokenizer","20939db6":"# VERSION 8 Changes\n\n* Filter by rule version 1 implemented to check\n* Hand written rules to improve the predictions if needed. (I noticed small improvement in scores with such functions. I need to improve it further (like discard Evidence if it is only a single token etc.. etc..)","4cd9310e":"# Train Now","bf1c8ab4":"# Step 1\n\n* Convert the dataset into a pandas table with text from the file and the positions of each discourse types (start and end word position)","7465e403":"# Version **10 and 11, 12** are inference only notebooks. Please refer to V5 till V9 for your reference","e6ba1e89":"# Use Corrected_Train Fields","1f2f97be":"# Step 2\n\n* Remove the spaces in the resulting dataframe columns and fill na with -91","519bf0a9":"# Get Initial Predictions","bd7f18b3":"# Prepare the Model (Super-Models beware....)","d80d9503":"# Step 3\n\n* Train\/Test\/Valid split\n> * I did not like this way of split because ideally the model should not see any file in the test set. Change this if you would like. \nI changed the split based on 70 % files for training. test will never see the same file as train","82854542":"# Training Argument Setup","832f58ef":"# Predict Test Set\n\nTry to apply the prediction function on the submission set now.","8419bc31":"# Dont Worry\n\nCode is straight from transformers reference given on top of the notebook","8b9fab95":"* These constants govern whether you want to train the notebook Or you want to use it for inference only.\n* The loss of the model was decreasing until my GPU hours gave up. You can download the model and continue training if needed.\n* squad_v2 should be True so that the model provides answers as **empty** (No Lead or No Rebuttal etc..)","b69231d3":"# Introduction\n\nThis is based on my Distilbert Question Answering model. Consider this as a proof of concept model where the problem was converted into a Squad v2 like dataset and Question Answering mode was utilized for prediction. Training was only performed till 3 EPOCHs and loss was 0.89 and going down (until I ran out of the GPU hours.)\n\nThe model uses Roberta and uses code to seamlessly convert the long input sequences with strides. Distilbert score was 0.26 and Roberta came out @ 0.4 where further training and code improvement should help.\n\nhttps:\/\/www.kaggle.com\/aliasgherman\/question-answer-approach-distilbert-1-epoch\n\n* This code converts the problem to a Question\/Answer model and uses **strides** in order to be able to use smaller models (models which have input sequence limitations of 512 or lower).\n\n---\n\n# Reference\nThe only resource your will need to understand the hocus pocus below\n\nhttps:\/\/github.com\/huggingface\/notebooks\/blob\/master\/examples\/question_answering.ipynb\n\n# Change History\n\n* V7 [LB 0.402] Original Roberta-Base Version, Strides = 128, MAX_LEN = 384\n* V8 [LB 0.404] Original Roberta-Base Version, Strides = 128, MAX_LEN = 384 with Fitler by rules initial version\n* V9 [ ? ] Lets try to train Roberta-Base with Strides = 128, MAX_LEN = 384\n* V10 [ 0.453 ] Properly introduce the Corrected_train fields. Strids = 128+64 and MAX_LEN = 480 with Filter by rules","e22141f3":"# Trim Dataset (If required)\n","c2c38213":"# Helper Functions\n\nplot_displacy is from the excellent notebook https:\/\/www.kaggle.com\/thedrcat\/feedback-prize-eda-with-displacy\n","78f9ff34":"# Constants Below","c24c8958":"# Step 3\n\n* Prepare a question and answer data set where the question is the discourse type num parameter","edeba1a0":"# Final Submission\n\nGet the filtered submission and generate submission file now","7c3e879d":"# Save Tokenized Dataset","ecb7e295":"# Tokenizer Steps","73146b26":"# Magic Happens here\n\n* Now the issue with Question Answering trained on Discourse_Type is, that if we select **best answer** only, then we can have 1 Lead, or 1 Evidence etc...\n\n* I have modified the code to churn out all the answers which are better than the **null_score** for a specific question. Hence, we are providing multiple possible answers to a single question. This is how the model churns the outputs"}}