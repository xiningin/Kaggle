{"cell_type":{"c1851dbe":"code","1f523804":"code","c7a9202a":"code","ec90fa36":"code","24c6ccdd":"code","1f14c00f":"code","b8e7439b":"code","62a0dc82":"code","ce2069e5":"code","b61cb214":"code","b98c5732":"code","e14a314f":"code","ecf60b21":"code","c380bedf":"code","c1a64779":"code","d0236b3e":"code","4f2ca5f1":"code","16131bb6":"code","77d86d7b":"markdown","3ae6f7be":"markdown","01fecbba":"markdown","6b2d541a":"markdown","1bc5eda1":"markdown","dbf397eb":"markdown","d31afd3d":"markdown","76e1fcb8":"markdown"},"source":{"c1851dbe":"# import the necessary packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error,r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection\nfrom scipy.stats import boxcox\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import RidgeCV","1f523804":"# load data\ndata = \"..\/input\/insurance\/insurance.csv\"\ndf = pd.read_csv(data)\n\n# show data (6 row)\ndf.head(6)","c7a9202a":"df_encode = pd.get_dummies(data = df, columns = ['sex','smoker','region'])\ndf_encode.head()","ec90fa36":"# normalization\ny_bc,lam, ci= boxcox(df_encode['charges'],alpha=0.05)\ndf_encode['charges'] = np.log(df_encode['charges'])\n\ndf_encode.head()","24c6ccdd":"X = df_encode.drop(\"charges\",axis=1)\ny = df_encode[\"charges\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nridge_model = Ridge(alpha=0.1).fit(X_train, y_train)\nridge_model","1f14c00f":"ridge_model.coef_","b8e7439b":"ridge_model.intercept_","62a0dc82":"lambdas = 10**np.linspace(10,-2,100)*0.5 # Creates random numbers\nridge_model =  Ridge()\ncoefs = []\n\nfor i in lambdas:\n    ridge_model.set_params(alpha=i)\n    ridge_model.fit(X_train,y_train)\n    coefs.append(ridge_model.coef_)\n    \nax = plt.gca()\nax.plot(lambdas, coefs)\nax.set_xscale(\"log\")","ce2069e5":"ridge_model = Ridge().fit(X_train,y_train)\n\ny_pred = ridge_model.predict(X_train)\n\nprint(\"Predict: \", y_pred[0:10])\nprint(\"Real: \", y_train[0:10].values)","b61cb214":"RMSE = np.mean(mean_squared_error(y_train,y_pred)) # rmse = square root of the mean of error squares\nprint(\"train error: \", RMSE)","b98c5732":"Verified_RMSE = np.sqrt(np.mean(-cross_val_score(ridge_model, X_train, y_train, cv=20, scoring=\"neg_mean_squared_error\")))\nprint(\"Verified_RMSE: \", Verified_RMSE)","e14a314f":"ridge_model = Ridge(10).fit(X_train,y_train)\ny_pred = ridge_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test,y_pred))","ecf60b21":"ridge_model = Ridge(30).fit(X_train,y_train)\ny_pred = ridge_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test,y_pred))","c380bedf":"ridge_model = Ridge(90).fit(X_train,y_train)\ny_pred = ridge_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test,y_pred))","c1a64779":"lambdas1 = 10**np.linspace(10,-2,100)\nlambdas2 = np.random.randint(0,10000,100)\n\nridgeCV = RidgeCV(alphas = lambdas1,scoring = \"neg_mean_squared_error\", cv=10, normalize=True)\nridgeCV.fit(X_train,y_train)","d0236b3e":"ridgeCV.alpha_","4f2ca5f1":"# final model\nridge_tuned = Ridge(alpha = ridgeCV.alpha_).fit(X_train,y_train)\ny_pred = ridge_tuned.predict(X_test)\nnp.sqrt(mean_squared_error(y_test,y_pred))*100","16131bb6":"# for lambdas2\nridgeCV = RidgeCV(alphas = lambdas2,scoring = \"neg_mean_squared_error\", cv=10, normalize=True)\nridgeCV.fit(X_train,y_train)\nridge_tuned = Ridge(alpha = ridgeCV.alpha_).fit(X_train,y_train)\ny_pred = ridge_tuned.predict(X_test)\nnp.sqrt(mean_squared_error(y_test,y_pred))*100","77d86d7b":"We can find out which value will work better by trial and error. But with the method we will use below, we can find the most appropriate value more easily and quickly.","3ae6f7be":"In contrast to the different beta values, the changes in the coefficients of the variables in our data set appear in the graph above. As can be seen, as the coefficients increase, it approaches zero.","01fecbba":"# Ridge Regression\n\nThe aim is to find the coefficients that minimize the error sum of squares, by applying a penalty to these coefficients.\n\n<img src=\"https:\/\/datavedas.com\/wp-content\/uploads\/2018\/04\/image001-1.png\" \/>\n\nThe first y value in the formula is the real values and the second y value is the predicted value. After this equation is opened and betas are written in my place and solved, what remains are the coefficients.\n\n- It is resistant to over learning.\n- It is biased but its variance is low.\n- Better than OLS when there are too many parameters.\n- Offers a solution to the problem of multidimensionality.\n- Effective when there is a problem of multiple linear connections. Multiple linear connection problem; It means that there is a high correlation between independent variables. In other words, it carries the same information that a variable carries in another variable.\n- Builds a model with all variables. It does not remove unrelated variables from the model, it brings their coefficients closer to zero.\n- \u03bb is in the critical model. It allows to control the relative effects of two terms (in the formula).\n- It is important to find a good value for \u03bb. For this, the CV method is used.\n\n<img src=\"https:\/\/i.ibb.co\/2qMjXG8\/Untitled.png\" \/>\n\n- The value in the left part of the formula is the classical recession.\n- where \u03bb is zero is in OLS.\n- A set containing certain values \u200b\u200bfor \u03bb is selected and the cross validation test error is calculated for each.\n- The \u03bb which gives the smallest cross validation is chosen as the setting parameter.","6b2d541a":"There are two values above. One of them is unverified, the other is the values \u200b\u200bthat represent the square root of the sum of the verified error squares. As you can see, the unverified value is almost half of the verified value. This result shows us that it is more correct to use the second method, not the first method, while taking the square root of the mean of the error squares.","1bc5eda1":"## Ridge Regression - Prediction\n","dbf397eb":"## Model Tuning","d31afd3d":"We can use alpha_ feature to attract the most appropriate value.","76e1fcb8":"## Ridge Regression Model"}}