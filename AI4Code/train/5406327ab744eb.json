{"cell_type":{"5064d84b":"code","22291247":"code","f0f48c8f":"code","33141fa7":"code","8fd903e9":"code","537ef2f2":"code","256ab65e":"code","8d90fad2":"code","b1e2d1f5":"code","340e2f51":"code","edab3abe":"code","1b7608bc":"code","e9142cbc":"code","c224f646":"code","1b68e373":"code","b4f3cf54":"code","4f386a06":"code","384c06e2":"code","a71b25e5":"code","6bc3282c":"code","6a4b7afe":"code","79fe47da":"code","06c4128b":"code","5a286acf":"code","a8a6c59f":"code","bf7d6b98":"code","6d1d6577":"code","da5f8528":"code","d800ad6b":"code","2b84dddf":"code","c0b13b01":"code","20d4f6fa":"code","680b97ba":"markdown","2eae310e":"markdown","ece077d4":"markdown","773ba7b4":"markdown","fde885d1":"markdown","e3985913":"markdown","2ea0e0ff":"markdown","9bd16df1":"markdown","bbc4db8e":"markdown","df5b38b1":"markdown","85bf9c7d":"markdown","769a9582":"markdown","2fb37255":"markdown","68440943":"markdown","e6dcf71c":"markdown"},"source":{"5064d84b":"%matplotlib inline\n# data manipulation libraries\nimport pandas as pd\nimport numpy as np\n\nfrom time import time\n\n# Graphs libraries\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nplt.style.use('seaborn-white')\nimport seaborn as sns\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport plotly.figure_factory as ff\nfrom plotly import tools\n\n# ML Libraries\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, roc_curve, auc\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\nimport lime\nimport lime.lime_tabular\nimport shap\n\n# Design libraries\nfrom IPython.display import Markdown, display\nimport warnings\nwarnings.filterwarnings(\"ignore\")","22291247":"data = pd.read_csv('..\/input\/database.csv', na_values=['Unknown', ' '])","f0f48c8f":"# Save original data just in case\ndata_orig = data.copy()","33141fa7":"# Keep only solved crimes\ndata = data[data['Crime Solved'] == 'Yes']\n\n# Drop useless columns\ncols_to_drop = ['Record ID', 'Agency Code', 'Perpetrator Ethnicity', 'Perpetrator Race', 'Perpetrator Age', 'Incident','Crime Solved']\ndata.drop(columns=cols_to_drop, inplace=True)\n\n# Get numerical and categorical columns\nY_columns = ['Perpetrator Sex']\ncat_columns = []\nnum_columns = []\n\nfor col in data.columns.values:\n    if data[col].dtypes == 'int64':\n        num_columns += [col]\n    else:\n        cat_columns += [col]\n\n# Get median values \nmedian_val = pd.Series()\nfor col in num_columns:\n    median_val[col] = data[col].median()\n\n# Handle missing data\nfor col in data:\n    if col in median_val.index.values:\n        data[col] = data[col].fillna(median_val[col])\n    else:\n        val = data[col].value_counts().sort_values(ascending=False).index[0]\n        data[col] = data[col].fillna(val)\n\n# Correct the problem about the Victim Age\ndata['Victim Age'] = np.where(data['Victim Age'] == 998, np.median(data[data['Victim Age'] <= 100]['Victim Age']), data['Victim Age'])","8fd903e9":"data.head()","537ef2f2":"categorical_features = cat_columns\ncategorical_features_idx = [np.where(data.columns.values == col)[0][0] for col in categorical_features]\n\ndata_encoded = data.copy()\n\ncategorical_names = {}\nencoders = {}\n\n# Use Label Encoder for categorical columns (including target column)\nfor feature in categorical_features:\n    le = LabelEncoder()\n    le.fit(data_encoded[feature])\n    \n    data_encoded[feature] = le.transform(data_encoded[feature])\n    \n    categorical_names[feature] = le.classes_\n    encoders[feature] = le\n\n\nnumerical_features = [c for c in data.columns.values if c not in categorical_features]\n\nfor feature in numerical_features:\n    val = data_encoded[feature].values[:, np.newaxis]\n    mms = MinMaxScaler().fit(val)\n    data_encoded[feature] = mms.transform(val)\n    encoders[feature] = mms\n    \ndata_encoded = data_encoded.astype(float)","256ab65e":"data_encoded.head()","8d90fad2":"def decode_dataset(data, encoders, numerical_features, categorical_features):\n    df = data.copy()\n    for feat in df.columns.values:\n        if feat in numerical_features:\n            df[feat] = encoders[feat].inverse_transform(np.array(df[feat]).reshape(-1, 1))\n    for feat in categorical_features:\n        df[feat] = encoders[feat].inverse_transform(df[feat].astype(int))\n    return df","b1e2d1f5":"decode_dataset(data_encoded, encoders=encoders, numerical_features=numerical_features, categorical_features=categorical_features).head()","340e2f51":"X = data_encoded.drop(columns='Perpetrator Sex', axis=1)\ny = data_encoded['Perpetrator Sex']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","edab3abe":"display(Markdown('#### Train dataset shape :'))\nprint(X_train.shape)\ndisplay(Markdown('#### Test dataset shape :'))\nprint(X_test.shape)","1b7608bc":"y_test.value_counts()","e9142cbc":"clf = RandomForestClassifier(random_state=4242)","c224f646":"param_dist = {\"n_estimators\":[10,100],\n              \"max_depth\": [3, 10],\n              \"max_features\": [2,len(X.columns)],\n              \"min_samples_split\":  [2,20],\n              \"bootstrap\": [True, False],\n              \"criterion\": [\"gini\", \"entropy\"]}\n\nn_iter_search = 20\nrandom_search = RandomizedSearchCV(clf, param_distributions=param_dist, n_iter=n_iter_search, cv=5, verbose=8)\n\nstart = time()\n# random_search.fit(X_train, y_train)\nprint(\"RandomizedSearchCV took %.2f seconds for %d candidates parameter settings.\" % ((time() - start), n_iter_search))\n# print(random_search.cv_results_)","1b68e373":"clf.fit(X_train, y_train)","b4f3cf54":"def get_model_performance(X_test, y_true, y_pred, probs):\n    accuracy = accuracy_score(y_true, y_pred)\n    matrix = confusion_matrix(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred)\n    preds = probs[:, 1]\n    fpr, tpr, threshold = roc_curve(y_true, preds)\n    roc_auc = auc(fpr, tpr)\n\n    return accuracy, matrix, f1, fpr, tpr, roc_auc\n\ndef plot_model_performance(model, X_test, y_true):\n    y_pred = model.predict(X_test)\n    probs = model.predict_proba(X_test)\n    accuracy, matrix, f1, fpr, tpr, roc_auc = get_model_performance(X_test, y_true, y_pred, probs)\n\n    display(Markdown('#### Accuracy of the model :'))\n    print(accuracy)\n    display(Markdown('#### F1 score of the model :'))\n    print(f1)\n\n    fig = plt.figure(figsize=(15, 6))\n    ax = fig.add_subplot(1, 2, 1)\n    sns.heatmap(matrix, annot=True, cmap='Blues', fmt='g')\n    plt.title('Confusion Matrix')\n\n    ax = fig.add_subplot(1, 2, 2)\n    lw = 2\n    plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic curve')\n    plt.legend(loc=\"lower right\")","4f386a06":"plot_model_performance(clf, X_test, y_test)","384c06e2":"clf.feature_importances_\nfeature_imp = pd.Series(clf.feature_importances_, index=X_train.columns)\nfeature_imp.sort_values(ascending=False, inplace=True)","a71b25e5":"def get_trace(series, color='#2980b9'):\n    series.sort_values(ascending=True, inplace=True)\n    x, y = series.values, series.index\n    trace = go.Bar(x=x, y=y, marker=dict(color=color), opacity=0.9, orientation='h')\n    return trace\n\ntrace = get_trace(feature_imp)\nlayout = go.Layout(barmode='group', title='Feature Importance', yaxis=go.layout.YAxis(automargin=True))\nfig = go.Figure([trace], layout=layout)\npy.iplot(fig)","6bc3282c":"categorical_features = ['Agency Name', 'Agency Type', 'City', 'State', 'Month',\n                        'Crime Type', 'Victim Sex', 'Victim Race', 'Victim Ethnicity',\n                        'Relationship', 'Weapon', 'Record Source']\ncategorical_features_idx = [X.columns.values.tolist().index(col) for col in categorical_features]\ncategorical_names_LIME = {}\n\nfor feature, idx in zip(categorical_features, categorical_features_idx):\n    categorical_names_LIME[idx] = categorical_names[feature]\n    \n# Reverse the MinMaxScaler to get the original values back.\nfor feat in X.columns.values:\n    if feat in numerical_features:\n        X[feat] = encoders[feat].inverse_transform(np.array(X[feat]).reshape(-1, 1))","6a4b7afe":"# Initiate the LIME explainer\nexplainer = lime.lime_tabular.LimeTabularExplainer(X.values,\n                                                   feature_names=X.columns.values,\n                                                   class_names=['Female', 'Male'],\n                                                   categorical_features=categorical_features_idx, \n                                                   categorical_names=categorical_names_LIME)","79fe47da":"def get_trace(series, color='#2980b9'):\n    series.sort_values(ascending=True, inplace=True)\n    x, y = series.values, series.index\n    trace = go.Bar(x=x, y=y, marker=dict(color=color), opacity=0.9, orientation='h')\n    return trace\n\ntrace = get_trace(feature_imp)\nlayout = go.Layout(barmode='group', title='Feature Importance', yaxis=go.layout.YAxis(automargin=True))\nfig = go.Figure([trace], layout=layout)\npy.iplot(fig)","06c4128b":"def plot_lime_importance(exp):\n    importance = exp.as_list()\n    importance.reverse()\n\n    colors, x, y = list(), list(), list()\n\n    for feat, val in importance:\n        if val < 0:\n            colors.append('#3498db')\n        else:\n            colors.append('#e67e22')\n        x.append(np.abs(val))\n        y.append(feat)\n\n    trace = go.Bar(x=x, y=y, marker=dict(color=colors)\n                   , opacity=0.9, orientation='h')\n    layout = go.Layout(barmode='group',\n                       title='Importance for the current prediction',\n                       yaxis=go.layout.YAxis(automargin=True))\n    fig = go.Figure([trace], layout=layout)\n    py.iplot(fig)\n    \ndef plot_prediction_importance(data, idx, model):\n    row = pd.DataFrame(data=[data.iloc[idx,:]], columns=data.columns.values)\n \n    exp = explainer.explain_instance(row.values[0], model.predict_proba)\n\n    display(row)\n    # Plot function from LIME\n    exp.show_in_notebook(show_table=True, show_all=False)\n    # Custom plot function\n    plot_lime_importance(exp)","5a286acf":"plot_prediction_importance(X_test, 6, clf)","a8a6c59f":"def compute_mean(dataset):\n    return dataset.mean()\n\ndef compute_mean_abs(dataset):\n    return dataset.abs().mean()\n\nt0 = time()\nprint('start : %0.4fs'%((time() - t0)))\n    \nimportance = pd.DataFrame()\n\n# I will just take a sample of set because it's a long operation on all the dataset.\ntest_lime = X_test.iloc[0:1000,:]\n\nfor row in test_lime.values:\n    exp = explainer.explain_instance(row, clf.predict_proba)\n    feats = [x[0] for x in exp.as_list()]\n    val = [x[1] for x in exp.as_list()]\n    \n    for i in range(0,len(feats)):\n        for feat in test_lime.columns.values:\n            if(feat in feats[i]):\n                feats[i] = feat\n    \n    exp_df = pd.DataFrame(data=[val], columns=feats)\n    importance = importance.append(exp_df)\n\nprint('end : %0.4fs'%((time() - t0)))","bf7d6b98":"mean_imp = compute_mean(importance)\n# Stock importance to compare it with other importance after\nlime_importance = compute_mean_abs(importance)","6d1d6577":"trace = get_trace(mean_imp.abs())\nlayout = go.Layout(barmode='group', title='LIME : Mean Importance', yaxis=go.layout.YAxis(automargin=True))\nfig = go.Figure([trace], layout=layout)\npy.iplot(fig)\n\ntrace = get_trace(lime_importance)\nlayout = go.Layout(barmode='group', title='LIME : Mean absolute Importance', yaxis=go.layout.YAxis(automargin=True))\nfig = go.Figure([trace], layout=layout)\npy.iplot(fig)","da5f8528":"t0 = time()\nprint('start : %0.4fs'%((time() - t0)))\n\n# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(clf)\nprint('explainer end : %0.4fs'%((time() - t0)))\n\n# Calculate Shap values\ntest_shap = X_test.iloc[0:1000,:].copy()\n# shap_values = explainer.shap_values(test)\nshap_values = explainer.shap_values(test_shap)\nprint('shap values end : %0.4fs'%((time() - t0)))","d800ad6b":"def plot_shap_explain(data, idx, model, explainer, shap_values, categorical_features=None, encoders=None):\n    row = data.iloc[idx,:] \n    display(pd.DataFrame([row.values], columns=row.index))\n    proba = model.predict_proba([row])[0]\n    display(Markdown(\"Probability of having an Income <= 50K : **%0.2f**\"%proba[0]))\n    display(Markdown(\"Probability of having an Income > 50K : **%0.2f**\"%proba[1]))\n    \n    if categorical_features != None:\n        for feature in categorical_features:\n            row[feature] = encoders[feature].inverse_transform([int(row[feature])])[0]\n    \n    display(Markdown(\"#### Explaination based on the 0 label (Income <= 50K)\"))\n    display(shap.force_plot(explainer.expected_value[0], shap_values[0][idx,:], row))\n    display(Markdown(\"#### Explaination based on the 1 label (Income > 50K)\"))\n    display(shap.force_plot(explainer.expected_value[1], shap_values[1][idx,:], row))\n\n# Take a random index\nrandom_index = np.random.randint(0, len(test_shap))\n\nshap.initjs()\nplot_shap_explain(test_shap, random_index, clf, explainer, shap_values, categorical_features, encoders)","2b84dddf":"shap.summary_plot(shap_values[1], test_shap, test_shap.columns.values, plot_type='bar')\nshap.summary_plot(shap_values[1], test_shap, test_shap.columns.values, plot_type='dot')","c0b13b01":"shap_df = pd.DataFrame(data=shap_values[1], columns=test_shap.columns)\nshap_importance = compute_mean_abs(shap_df)","20d4f6fa":"def get_trace(series, color='#2980b9'):\n    series.sort_values(ascending=True, inplace=True)\n    x, y = series.values, series.index\n    trace = go.Bar(x=x, y=y, marker=dict(color=color), opacity=0.9, orientation='h')\n    return trace\n\ntrace1 = get_trace(feature_imp)\ntrace2 = get_trace(lime_importance)\ntrace3 = get_trace(shap_importance)\n\nfig = tools.make_subplots(rows=1, cols=3, subplot_titles=('Feature importance', 'LIME','SHAP'))\n\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 2)\nfig.append_trace(trace3, 1, 3)\nfig['layout'].update(title='Comparison of feature importance, LIME importance and SHAP importance',\n                     showlegend=False,\n                     yaxis=go.layout.YAxis(automargin=True),\n                     barmode='group')\npy.iplot(fig)","680b97ba":"Now like feature importance and the custom metrics from LIME we need to see what is the global measure on the model. So SHAP gives us a plot function `shap.summary_plot()` which gives us 2 possible plot : bar plot or dot plot.\n\nThe *bar* plot calculate the average absolute SHAP value and display a bar chart like the one for feature importance. The *dot* plot sorts features by the sum of SHAP value magnitudes over all samples, and uses SHAP values to show the distribution of the impacts each feature has on the model output. The color represents the feature value (red high, blue low).","2eae310e":"### 1.2 Load the data","ece077d4":"### 1.3 Prepare the data\n\nSince I already did this in [the previous notebook at the \"2. EDA and data preparation section](https:\/\/www.kaggle.com\/nathanlauga\/ethics-and-ai-how-to-prevent-bias-on-ml\/#2), I will to go into the details of the data preparation.","773ba7b4":"Now you have a quick overview of the SHAP library but I just used the `TreeExplainer` and there are other explainers like explainer for neural networks so if you are curious don't hesitate to look on the GitHub repository or the SHAP paper.\n\n### <a id='2.4'>2.4 Compare importance<\/a>\n\nSo to conclude this part let's see what each measure of importance say and which one we can use to explain our model.","fde885d1":"Now we have our shap values for each possible class (here : Income <= 50K or Income > 50K) and for each features \/ rows. SHAP provides us some plot function, especially one that plot shap values for a prediction : `shap.force_plot()`. It shows us the evolution between the *base value* $E[f(z)]$, which is the probability of a prediction if do not know any feature, and the current prediction $f(x)$\n\n![SHAP values attribute](http:\/\/image.noelshack.com\/fichiers\/2018\/50\/5\/1544793324-shap-values.png)\n\nNow let's see how it looks like on a random row of our test set.","e3985913":"Here the seconde plot reveals that, for example, More *Capital Gain* is high, higher is the chance that the model predicted an Income > 50K.\n\nBefore concluding this part I will just keep the importance to compare it after.","2ea0e0ff":"# Ethics and AI : how to understand a model ?  \u2696\n*Nathan Lauga*\n\nHi everyone, I'm currently working on a kernel series about AI and ethics, my first one was this one :\n* [Ethics and AI : how to prevent bias on ML](https:\/\/www.kaggle.com\/nathanlauga\/ethics-and-ai-how-to-prevent-bias-on-ml\/)\n\nToday, the topic that I will treat is the interpretability of a model. On a day to day basis, for a Data Scientist, it's more than likety to forget to ask the question : \"Why did I get this prediction ?\" or \"How my model is working ?\". \nOften, when this question is asked, the answer is just a simple visualization of feature importance but for peoples that are not Data Scientist, it's not sufficient !\n\nSo let's dive into this problem and to explore that, I will use the same Dataset than the previous Kernel.","9bd16df1":"We see that they are different results so how to decide which one to trust. The answer is : **SHAP** because it's not just the mathematics that are behind this method also they study the importance found with human intuition and here is their results :\n\n![Comparison between LIME, SHAP and human for importance](http:\/\/image.noelshack.com\/fichiers\/2018\/50\/5\/1544800972-shap-lime-human.png)\n\nThis image shows us the human intuition correspond to what SHAP determine.\n\nThanks for reading ! \ud83d\udc4d","bbc4db8e":"### 1.5 Performance of the model","df5b38b1":"We can see that the original model still has a great accuracy and F1 Score. I remind you that here I didn't optimize the models to be excellent I used default configurations.\n\nNow you have to ask yourself the question, **what is more important : performance or fair ?**\n\n## <a id='2'>2. Interpretable model<\/a>\n### <a id='2.1'>2.1 Feature importance<\/a>\n\nThe question of **model interpretation is becoming really important** and the most common way to explain a model is to use **feature importance**. It's usefull to get an insight of what the model is using in feature to predict something but it has limits. First of all let's see how it works.\n\nFeature importance changes according to the model that we're using. It's important to know that **it exists differents ways to measure feature importance** according to the model so I will introduce some measure for 3 types of models : Linear Regression, Decision Tree and Tree models.\n\n#### <a id='2.1.1'>2.1.1 Linear Regression<\/a>\n\nFor the Linear Regression if we have standardized the features the coefficient represents the influence of the feature on the model. So for a Linear model with a formula : \n\n$$h_\\theta(X) = \\theta_0x_0 + \\theta_1x_1 + ... + + \\theta_nx_n = \\theta^TX$$\n\nWe can just take all the theta with the vector $\\theta$ and it's our importance for each feature.\n\n#### <a id='2.1.2'>2.1.2 Decision Tree<\/a>\n\nFor the Decision Tree, feature importance is not really important because this model is human interpretable. For a prediction you just have to follow the leaves.\n\n#### <a id='2.1.3'>2.1.3 Tree models (e.g Random Forest)<\/a>\n\nI think this type of models (Random Forest, XGBoost, LGBM, ...) is well known in the industry today so the feature importance of those models are logically the most used for classification. So the most used measure of feature importance for Tree models is **Gini Importance or Mean Decrease in Impurity (MDI)**. \n\n![Random Forest](https:\/\/cdn-images-1.medium.com\/max\/1000\/1*xxahsU68wsbXyMYAFTf-Eg.png)\n\n**MDI** counts the times a feature is used to split a node, weighted by the number of samples it splits.\n\nWe can calculate it with this formula :\n\n$$Imp(X_j)=\\frac{1}{M}\\sum_{m=1}^{M}\\sum_{t\\in \\varphi_m} (j_t=j)[p(t)\\Delta i(t)]$$\n\nHere we calculate the importance of variable $X_j$ for an ensemble of $M$ trees. $j_t$ denotes the variable used at node $t$, $p(t) = \\frac{N_t}{N}$ is the proportion of rows used in that node (if it's the first node it will be 1) and $\\Delta i(t)$ is the impurity reduction at node $t$ : we can use Gini index or Entropy to measure impurity.\n\nI find out this formula from this course : [Understanding variable importances in forests of randomized trees](https:\/\/fr.slideshare.net\/glouppe\/understanding-variable-importances-in-forests-of-randomized-trees)\n\nAlso if you want more informations about others Tree models measure of feature importance you can take a look at this publication : [Feature Importance Measures for Tree Models\u200a\u2014\u200aPart I](https:\/\/medium.com\/the-artificial-impostor\/feature-importance-measures-for-tree-models-part-i-47f187c1a2c3)\n\nNow let's see what is the feature importance of our Random Forest model.","85bf9c7d":"We see that *Relationship*, *Victim Age* and *Year* are the features that are the most importance for this model if we use feature importance from sklearn RandomForestClassifier. But now we arrive at **the limits of feature importance**, if I want to know wich feature are important on **a specific prediction** I can't know that I just can suppose that it follows the feature importance measure but I can not be sure.\n\n### <a id='2.2'>2.2 LIME<\/a>\nSource : [LIME GitHub repository](https:\/\/github.com\/marcotcr\/lime)\n\nSo with the objective to find out wich features are important for a specific prediction appear **LIME** : *Local interpretable model-agnostic explanations* it means that LIME can explain any models in Machine Learning. We can think it like the model is a black box that just have the input and output to find out what feature is important.\n\n![LIME Black Box](https:\/\/cdn-images-1.medium.com\/max\/1000\/1*k-rxjnvUDTwk8Jfg6IYBkQ.png)\n\nTo understand how it works we have to define it is important to distinguish between features and interpretable data representations. As mentioned before, interpretable explanations need to use a representation that is understandable to humans, regardless of the actual features used by the model. For example, a possible interpretable representation for text classification is a binary vector indicating the presence or absence of a word.\n\nNow Let $f$ be the original prediction model to be explained and $g$ the explanation model such as a linear regression model. Here, we focus on local methods designed to explain a prediction $f(x)$ based on a single input $x$. Explanation models often use simplified inputs $x$ that map to the original inputs through a mapping function $x = h_x(x')$. Local methods try to ensure $g(z') \u2248 f(h_x(z'))$ whenever $z' \u2248 x'$ so it's a perturbed sample (Note that $hx(x') = x$ even though x may contain less information than $x$ because $h_x$ is specific to the current input $x'$).\n\nThe explanation model that is a linear function of binary variables that follows this formula :\n$$g(z') = \\phi_0 + \\sum_{i=1}^{M}\\phi_i z_i' $$\n\nwhere $z' \\in \\{0, 1\\}^M$, $M$ is the number of simplified input features, and $\\phi_i \\in R$\n\nTo find $\\phi$, LIME minimizes the following **objective function** : \n$$ \\xi =  \\underset{g \\in G}{arg min} \\space L(f,g,\\pi_{x'}) + \\Omega(g) $$\n\nFaithfulness of the explanation model $g(z')$ to the original model $f(h_x(z'))$ is enforced through the loss $L$ over a set of samples in the simplified input space weighted by the local kernel $\\pi_{x'}$ (which calculate the distance between the perturbed sample and the original point). $\\Omega$ penalizes the complexity of $g$. Since in LIME $g$ follows the linear equation above and $L$ is a squared loss, this equation can be solved using penalized linear regression.\n\nSo at the end we can obtain a explanation model like this : \n\n![Explanation model](https:\/\/raw.githubusercontent.com\/marcotcr\/lime\/master\/doc\/images\/lime.png)\n\nNow let's use LIME on our dataset and original model (Random Forest).","769a9582":"As we can see LIME is performant to understand the importance of features for a specific prediction and if we want to get an understanding of the model can construct some aggregate values like a mean of all importance. \n\nI decided to construct to global metrics where $f$ is a feature and $M$ the number of elements : \n* Mean of the importance values : $\\mu^f_{importance} = \\frac{1}{M} \\sum_{i=1}^{M} importance^f_i$\n* Mean of the absolute of importance values : $\\mu^f_{|importance|} = \\frac{1}{M} \\sum_{i=1}^{M} |importance^f_i|$\n\nBut to get this I have to regroup all the feature generate by LIME into the original feature. For example : it's possible to have a feature *Victim Age > 10* and for another row *5 < Victim Age < 24* so it needs to be just *Victim Age* at the end.","2fb37255":"Now we have the encoded dataset that will be use for train the model. Before training let's just create a function wich can decode the dataset.","68440943":"### 1.4 Train the model\nNow that the dataset is formated to train a model let's do it !\n\nI choose a Random Forest model so at the end, a comparison will be possible between the differents metrics","e6dcf71c":"We can see that taking the mean is not relevant because we can loose some information so I prefer to use the second one with the absolute value.\n\nNow you see how LIME works, but also it don't offer us the possibility to have a global value so it's not the best thing. But just one year after LIME another library appeared : **SHAP**, they proposed an alternative kernel from the LIME method and claims that only its approach can satisfy three key axioms of interpretability. \n\n### <a id='2.3'>2.3 SHAP<\/a>\nSource : [SHAP GitHub repository](https:\/\/github.com\/slundberg\/shap)\n\nAs I said before SHAP claims that it satisfy three axioms of interpretability. But, before going into the theory of SHAP let's dive into the **game theory**.\n\nLet's ask ourselves a question : *in a coalition that is composed of multiple players with differing skill sets, that results in some collective payoff, what is the fairest way to divide up that payoff among the players ?*. With this question we will find out the Shapley values. \n\n#### <a id='2.3.1'>2.3.1 Shapley Values<\/a>\nWhat is this value ? Before going into theory I need to use an example to have a better understanding about this value. \n\nLet's take 2 persons : Bob and Alice. They are going to a restaurant but at the end how will they split the bill ? To get the answer in a first place we need to know how much they spend if they were alone and together. Let's say that Bob alone would spend 50\u20ac , Alice would spend 30\u20ac and together 70\u20ac. So we can have the following information :\n$$  v(c) = \n\\begin{cases}\n50, \\ \\text{if} \\ c=\\{\\text{Bob}\\} \\\\ \n30, \\ \\text{if} \\ c=\\{\\text{Alice}\\} \\\\ \n70, \\ \\text{if} \\ c=\\{\\text{Bob & Alice}\\}\n\\end{cases} $$\n\nNow we have to know what they pay each depending on the order of arrive, for example if Alice arrives first she paid 30\u20ac and then Bob paid the difference between what they pay together and what Alice paid so 40\u20ac. We denote $\\pi$ the order of arrival and $\\delta_{\\pi}^G$ what they pay each depending on the order of arrival. So now we obtain this :\n\n$$\n\\begin{array}{c c}\n\\pi & \\delta_{\\pi}^G \\\\\n\\hline \n(\\text{Bob}, \\text{Alice}) & (\\text{Bob}:50,\\text{Alice}:20) \\\\\n(\\text{Alice}, \\text{Bob}) & (\\text{Bob}:40,\\text{Alice}:30) \\\\\n\\end{array}\n$$\n\nAnd now to obtain the Shapley values $\\phi$ we just have to get the mean on each person from $\\delta_{\\pi}^G$ :\n\n$$ \\phi = (\\text{Bob}:45,\\text{Alice}:25) $$\n\nSo the concept of this value is not that complicated and now here is the formula that get us the values : Here $|F|$ is the size of the full coalition. $S$ represents any subset of the coalition that doesn\u2019t include player $i$, and $|S|$ is the size of that subset. The bit at the end is just \"how much bigger is the payoff when we add player $i$ to this particular subset $S$\"\n\n$$ \\phi_i = \\sum_{S\\subseteq F\\setminus \\{i\\}} \\frac{|S|!(|F|-|S|-1)!}{|F|!} [f_{S\\cup \\{i\\}}(x_{S\\cup \\{i\\}}) - f_S(x_S)]$$\n\nSo this values satisfy the 3 following axioms of interpretability :\n* *Dummy Player* : If a player never adds any marginal value, their payoff portion should be 0.\n* *Substitutability* : If two players always add the same marginal value to any subset to which they\u2019re added, their payoff portion should be the same.\n* *Additivity* : If a game is composed of two subgames, you should be able to add the payoffs calculated on the subgames, and that should match the payoffs calculated for the full game.\n\n#### <a id='2.3.2'>2.3.2 How SHAP works<\/a>\n\nSHAP which means *SHapley Additive exPlanations*, uses both **game theory** (with Shapley value) and **local interpretation** (with LIME). But how it do that ? \n\nThey used the concept of simplifed feature with vector includes in {0,1} and they defined the 3 axioms but in mathematicals terms so *Dummy player* become *Missingness*, *Substitutability* become *Local accuracy* and *Additivity* become *Consistency* (more informations about this mathematicals formulas in [the SHAP paper](http:\/\/papers.nips.cc\/paper\/7062-a-unified-approach-to-interpreting-model-predictions.pdf)).\n\nAnd with that we obtain the following formula : Here $f$ is the original model for some specific inputs $x$, and $x'$ is the simplified input (which corresponds to the original input $x$), $M$ is the full number of features and $z'$ is a sample of $x'$.\n\n$$ \\phi_i(f,x) = \\sum_{z'\\subseteq x'} \\frac{|z'|!(M-|z'|-1)!}{M!} [f_x(z') - f_x(z'\\setminus i)]$$\n\nSo we see that is the **same logic than the Shapley value** but here instead of **players** we have **features**. \n\nThe following text is from the SHAP paper, it explains the conection between the formula of LIME and the one juste above : \n> At first glance, the regression formulation of LIME in Equation 2 seems\nvery different from the classical Shapley value formulation of Equation 8. However, since linear\nLIME is an additive feature attribution method, we know the Shapley values are the only possible\nsolution to Equation 2 that satisfies Properties 1-3 \u2013 local accuracy, missingness and consistency. A\nnatural question to pose is whether the solution to Equation 2 recovers these values. The answer\ndepends on the choice of loss function $L$, weighting kernel $\\pi_{x'}$ and regularization term $\\Omega$. \n\nSo to satisfy the 3 properties there is juste one combination of parameters that need to be follow :\n\n$\n\\Omega(g) = 0, \\\\\n\\pi_{x'}(z') = \\frac{(M-1)}{(M \\ choose \\ |z'|)|z'|(M-|z'|)}, \\\\\nL(f,x,\\pi_{x'}) = \\sum_{z'\\in Z}[f(h_x(z')) - g(z')]^2 \\pi_{x'}(z')\n$\n\nAnd this is how it works theoretically now let's practice with the library in Python.\n*****\n\n#### <a id='2.3.3'>2.3.3 SHAP in Python<\/a>\nFirst let's generate the explainer with the `RandomForestClassifier` from sklearn."}}