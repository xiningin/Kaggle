{"cell_type":{"484d7267":"code","5ece52ae":"code","9e09d87b":"code","a9f0ed7e":"code","de60735a":"code","6d76c85b":"code","4b3e0514":"code","602bbf64":"code","ee7a2a38":"code","644b4cb7":"code","9a8da326":"code","b8847d11":"code","6e2d54a5":"code","90640b0f":"code","9cdf3876":"code","6cd2db8d":"code","6d6a57b2":"code","5c9c468a":"code","02c81dca":"code","ec5aaf5f":"code","0995fa55":"code","16992a00":"code","11ca0e78":"code","88219050":"code","b3b573b3":"code","10c5127b":"code","b08dbc3e":"code","bb1e974b":"code","13b608d9":"code","ea1355bf":"code","803e16dd":"code","5c229b2c":"code","b555897d":"code","f05041a2":"code","4cf35c3f":"code","e3f701f9":"code","38d2cd02":"code","cefba6ec":"code","6ecc3752":"code","3e2f2298":"code","64b07db0":"code","7e337709":"code","f84542cf":"code","0cc89edf":"code","e156b952":"code","770ea888":"code","9449faf2":"code","5275da7b":"code","c86e225d":"code","2078a20f":"code","b369d5d4":"code","d9cb8bd8":"code","82ba845f":"code","09802ced":"code","210b9e87":"code","b519629d":"code","0fb2cd88":"code","43a45c53":"code","654ae24a":"code","26b15254":"markdown","2bac86e1":"markdown","8d420667":"markdown","c9399ed4":"markdown","91b20094":"markdown","644c320e":"markdown","b81a9505":"markdown","566e13c6":"markdown","a5232e2f":"markdown","2f135af0":"markdown","d30cecfa":"markdown","c6af6e35":"markdown","0b281a4a":"markdown"},"source":{"484d7267":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5ece52ae":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","9e09d87b":"train=pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest=pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","a9f0ed7e":"#train_id=train[\"PassengerId\"]\ntest_id=test[\"PassengerId\"]","de60735a":"train.describe()  #computes a summary of statistics pertaining to the DataFrame columns(numeric columns only)","6d76c85b":"train.head()","4b3e0514":"print(train.Pclass.value_counts(dropna=False))\nprint(\"--\"*50)\nprint(train.Embarked.value_counts(dropna=False))\nprint(\"--\"*50)\nprint(train.SibSp.value_counts(dropna=False))\nprint(\"--\"*50)\nprint(train.Parch.value_counts(dropna=False))\nprint(\"--\"*50)","602bbf64":"#Sort rows of a dataframe in descending order of NaN counts\n\ntrain.isnull().sum().sort_values(ascending = False)","ee7a2a38":"#Analysing the correlation of features \n\nmat=train.corr()\nfig,ax = plt.subplots(figsize = (10,10))\nsns.heatmap(mat,annot = True, annot_kws={'size': 12})","644b4cb7":"test.describe()","9a8da326":"test.head()","b8847d11":"print(train.Pclass.value_counts(dropna=False))  \nprint(\"--\"*50)\nprint(train.Embarked.value_counts())\nprint(\"--\"*50)\nprint(train.SibSp.value_counts())\nprint(\"--\"*50)\nprint(train.Parch.value_counts())\nprint(\"--\"*50)","6e2d54a5":"test.isnull().sum().sort_values(ascending=False)","90640b0f":"del train[\"Cabin\"]    #Deleting a column\n","9cdf3876":"train[\"Age\"].fillna(train.Age.mean(),inplace=True)      #Replacing null values with mean value ","6cd2db8d":"train.describe()","6d6a57b2":"train.isnull().sum().sort_values(ascending=False)\n\n#Embarked has 2 null values acc to the output of this cell","5c9c468a":"#dropping  2 missing values of embarked\ntrain.dropna(inplace=True)\n\n\n","02c81dca":"del test[\"Cabin\"]","ec5aaf5f":"test[\"Age\"].fillna(test.Age.mean(),inplace=True)","0995fa55":"test.describe()","16992a00":"test.isnull().sum().sort_values(ascending=False)","11ca0e78":"# As we cannot remove a value from test data we have to fill the missing value of fare\ntest.fillna(test.Fare.median(),inplace=True)\ntest.isnull().sum().sort_values(ascending=False)","88219050":"Y=train[\"Survived\"]","b3b573b3":"del train[\"PassengerId\"]\ndel test[\"PassengerId\"]","10c5127b":"del train[\"Survived\"]","b08dbc3e":"train.head()","bb1e974b":"train.shape","13b608d9":"test.shape","ea1355bf":"test.head()","803e16dd":"#Joining the two datasets \n\nfinal=pd.concat([train,test],axis =0)  ","5c229b2c":"final.shape","b555897d":"final.head()","f05041a2":"#improved 15% accuracy\n\ndef One_hot_encoding(columns):\n    final_df=final\n    i=0\n    for fields in columns:\n        col1=pd.get_dummies(final[fields],drop_first=True)   # drops the first col out of all created columns\n        \n        final.drop([fields],axis=1,inplace=True)   # that col itself is dropped \n        if i==0:\n            final_df=col1.copy()\n        else:           \n            final_df=pd.concat([final_df,col1],axis=1)\n        i=i+1\n       \n        \n    final_df=pd.concat([final,final_df],axis=1)\n        \n    return final_df","4cf35c3f":"columns=[\"Sex\",\"Embarked\",\"Pclass\",\"Parch\"]   #those cols are put who have a significant effect on survival as seen from heat map","e3f701f9":"df_final = One_hot_encoding(columns)","38d2cd02":"df_final.head()","cefba6ec":"df_final.drop(\"Name\",axis=1,inplace=True)\ndf_final.drop(\"Ticket\",axis=1,inplace=True)\n","6ecc3752":"df_final.head()","3e2f2298":"\nfrom sklearn import preprocessing\n\n# Get column names first\nnames = df_final.columns\n\n# Create the Scaler object\nscaler = preprocessing.StandardScaler()\n\n# Fit your data on the scaler object\nscaled_df = scaler.fit_transform(df_final)\ndf_final = pd.DataFrame(scaled_df, columns=names)","64b07db0":"#Making col name as 1.. 2 .. 3 so that there are no repeating column names \n\n\ncols = []\ncount = 1\nfor column in df_final.columns:\n    cols.append(count)\n    count+=1\n    continue\n    \n","7e337709":"cols","f84542cf":"df_final.columns = cols","0cc89edf":"df_final.head()","e156b952":"df_train=df_final.iloc[:889,:]   \ndf_test=df_final.iloc[889:,:]","770ea888":"X=df_train","9449faf2":"df_test.shape","5275da7b":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y)","c86e225d":"from sklearn.ensemble import RandomForestClassifier","2078a20f":"#RandomForestClassifier?","b369d5d4":"model_rforest = RandomForestClassifier()\nmodel_rforest.fit(X_train,Y_train)","d9cb8bd8":"print(\"R-Squared Value for Training Set: {:.3f}\".format(model_rforest.score(X_train,Y_train)))   #Returns the mean accuracy of the given test data\nprint(\"R-Squared Value for Test Set: {:.3f}\".format(model_rforest.score(X_test,Y_test)))","82ba845f":"predictions_01 = model_rforest.predict(df_test)  ","09802ced":"output_02 = pd.DataFrame({'PassengerId': test_id, 'Survived': predictions_01})\noutput_02.to_csv('Titanic_RF.csv', index=False)\nprint(\"Your submission was successfully saved!\")","210b9e87":"from sklearn.tree import DecisionTreeClassifier","b519629d":"model_dec = DecisionTreeClassifier()\nmodel_dec.fit(X_train, Y_train)","0fb2cd88":"print(\"R-Squared Value for Training Set: {:.3f}\".format(model_dec.score(X_train,Y_train)))\nprint(\"R-Squared Value for Test Set: {:.3f}\".format(model_dec.score(X_test,Y_test)))","43a45c53":"predictions_02 = model_dec.predict(df_test)","654ae24a":"output_02 = pd.DataFrame({'PassengerId': test_id, 'Survived': predictions_02})\noutput_02.to_csv('Titanic_DT.csv',  index=False)\nprint(\"Your submission was successfully saved!\")","26b15254":"-Dropping the \"cabin\" column as > 50% values in it are NaN\n\n-Filling the NaN values in other columns.\n\n-Dropping irrelevant columns\n","2bac86e1":"## Training dataset","8d420667":"# Feature Engineering","c9399ed4":"# Modeling","91b20094":"## Applying desicion Tree","644c320e":"## Training dataset","b81a9505":"# Data Analysis","566e13c6":"## Testing Dataset","a5232e2f":"# Handling missing values","2f135af0":"## Normalising datasets","d30cecfa":"## Applying Random Foresting","c6af6e35":"## Testing Dataset","0b281a4a":"## Spliting into train and test again"}}