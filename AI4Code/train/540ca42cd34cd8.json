{"cell_type":{"6d9e563f":"code","4beb2d30":"code","3a6ee650":"code","1d67bd9e":"code","036a7c22":"code","518eb8ad":"code","c8c0bf12":"code","ce85bdc0":"code","a5016117":"code","42377f6f":"code","27b631c3":"code","9592f580":"code","8e12ce4a":"code","2141cc58":"code","0a83d1f1":"code","205ad1de":"code","6df4c9a0":"code","39902a55":"code","85bb3587":"code","44803cde":"code","931c2a7e":"code","13a7009e":"code","cdeeb7dd":"code","bff28068":"code","2b9ba26e":"code","7dda94f4":"code","ca3bed75":"code","3cac886e":"code","c76e4687":"code","6b7fdb06":"code","9a5b969d":"code","84495dc1":"code","7c35fbbd":"code","a1e7169d":"markdown","e4684c28":"markdown","46dd4c4c":"markdown","ee9321ba":"markdown","3f560b90":"markdown","06d3e186":"markdown","9f4b2f32":"markdown","ca995f3f":"markdown","bbe1ac76":"markdown","b1974c3c":"markdown","0794051f":"markdown","e6db6a5c":"markdown","efce9814":"markdown","8ee93dc0":"markdown","3f680db2":"markdown","14acaa70":"markdown","42bb9740":"markdown","54709602":"markdown"},"source":{"6d9e563f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport random\nimport os\nimport glob\nfrom tqdm.notebook import tqdm\nimport plotly\nimport plotly.express as ex\nimport plotly.graph_objs as go\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Preprocessing the data\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.resnet50 import preprocess_input\nimport keras\nfrom tensorflow.keras.optimizers import Adam, SGD\nfrom keras.models import Model,load_model\nfrom tensorflow.keras.layers import Dense, Dropout,Conv2D,Flatten,MaxPooling2D,BatchNormalization\nfrom keras import backend as K\nfrom tensorflow.keras.layers.experimental import preprocessing\nfrom keras.callbacks import ReduceLROnPlateau,EarlyStopping","4beb2d30":"# Look at the training, testing and validation data numbers\ntest_data = glob.glob('..\/input\/plant-pathology-2021-fgvc8\/test_images\/*.jpg')\ntrain_data = glob.glob('..\/input\/plant-pathology-2021-fgvc8\/train_images\/*.jpg')\nsubmission=pd.read_csv('..\/input\/plant-pathology-2021-fgvc8\/sample_submission.csv')\ntrain_csv=pd.read_csv('..\/input\/plant-pathology-2021-fgvc8\/train.csv')\n\nprint(f\"Training Set has: {len(train_data)} images\")\nprint(f\"Testing Set has: {len(test_data)} images\")","3a6ee650":"train_csv.head()","1d67bd9e":"train_image_dir = \"\/kaggle\/input\/plant-pathology-2021-fgvc8\/train_images\/\"","036a7c22":"lab=[]\nfor v in train_csv['labels'].value_counts().index.tolist():\n    temp=v.split()\n    for i in temp:\n        lab.append(i)","518eb8ad":"labels=[]\nfor v in lab:\n    if v not in labels:\n        labels.append(v)","c8c0bf12":"### Total 6 unique classes\nlabels","ce85bdc0":"### get the list of labels for every record\ntrain_csv['labels']=train_csv['labels'].apply(lambda x:x.split(' '))","a5016117":"train_csv.head()","42377f6f":"from PIL import Image\n# create figure\nfig = plt.figure(figsize=(15, 15))\nimport glob\n\n# setting values to rows and column variables\nrows = 3\ncolumns = 3\n\n\nfor i in range(9):\n    image=glob.glob(\"\/kaggle\/input\/plant-pathology-2021-fgvc8\/train_images\/*\")[i]\n    img_arr=cv2.imread(image)\n    # Adds a subplot at the 1st position\n    fig.add_subplot(rows, columns, i+1)\n    # showing image\n    plt.imshow(cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB))\n    plt.axis('on')\n    plt.title(\"Image:\"+glob.glob(\"\/kaggle\/input\/plant-pathology-2021-fgvc8\/train_images\/*\")[i][len(train_image_dir):]+\"\\n\"+str(train_csv.iloc[i][1]))","27b631c3":"imageSize = 224\nbatchSize = 32\nNUM_CLASSES = 6 ## since there are 6 distinct classes","9592f580":"trainGen = ImageDataGenerator(preprocessing_function = tf.keras.applications.resnet50.preprocess_input, \n                              rescale = 1.\/255,\n                              shear_range = 0.2,\n                              zoom_range = 0.2,\n                              horizontal_flip = True,\n                              vertical_flip=True,\n                              brightness_range=(0.8, 1.2),\n                              featurewise_center=True,\n                              featurewise_std_normalization=True,\n                              rotation_range=20,\n                              validation_split = 0.2)","8e12ce4a":"train_generator = trainGen.flow_from_dataframe(dataframe = train_csv,\n                                               directory = train_image_dir,\n                                               x_col = \"image\",\n                                               y_col = \"labels\",\n                                               class_mode= \"categorical\", \n                                               subset = \"training\", \n                                               target_size = (imageSize, imageSize),\n                                               batch_size = batchSize,\n                                               seed = 123)\nval_generator = trainGen.flow_from_dataframe(dataframe = train_csv,\n                                               directory = train_image_dir,\n                                               x_col = \"image\",\n                                               y_col = \"labels\",\n                                               class_mode= \"categorical\", \n                                               subset = \"validation\", \n                                               target_size = (imageSize, imageSize),\n                                               batch_size = batchSize,\n                                               seed = 123)","2141cc58":"## lets check the type\ntype(val_generator) ## image generator object","0a83d1f1":"basemodel = tf.keras.applications.ResNet50(weights = \"imagenet\", include_top = False, pooling = \"avg\")\n## We dont want to train our weight , we will use pre trained weights of resnet50 and remove the output layer\nx = basemodel.output\nx = Dense(128, activation='relu')(x)\nx = BatchNormalization()(x)\ntop_dropout_rate = 0.2\nx = Dropout(top_dropout_rate, name=\"top_dropout\")(x)\nx = Dense(64, activation='relu')(x)\npredictions = Dense(NUM_CLASSES, activation='sigmoid')(x)\n# this is the model we will train\nmodel = Model(inputs=basemodel.input, outputs=predictions)","205ad1de":"basemodel.input### image of channel 3 and n*n dimension","6df4c9a0":"basemodel.output### flatten numpy array of size 2048","39902a55":"model.summary()","85bb3587":"earlystop = EarlyStopping(monitor='val_loss',\n                          min_delta=0.01,# threshold to consider as no change\n                          patience=1,# stop if  epochs with no change\n                          verbose=1,\n                          mode='auto',\n                          restore_best_weights= True)\n\n# lr = ReduceLROnPlateau(monitor='val_loss', \n#                              factor=0.1, \n#                              patience=20, \n#                              verbose=1, \n#                              mode='auto', \n#                              min_delta=0.0001, \n#                              cooldown=0, \n#                              min_lr=0.0001)","44803cde":"opt = keras.optimizers.Adam(lr=0.001, decay=1e-4)\nmodel.compile(optimizer = opt, loss = \"binary_crossentropy\", metrics = [\"accuracy\"])","931c2a7e":"## train for 3 epoch only\nhistory = model.fit(train_generator, \n                    epochs=3, \n                    validation_data=val_generator, \n                    verbose=True, \n                    callbacks = earlystop)","13a7009e":"model.save(\"resnet50_model.h5\")","cdeeb7dd":"print(history.history.keys())","bff28068":"test_df = pd.read_csv('..\/input\/plant-pathology-2021-fgvc8\/sample_submission.csv')","2b9ba26e":"test_image_dir = \"\/kaggle\/input\/plant-pathology-2021-fgvc8\/test_images\/\"\ntest_generator = trainGen.flow_from_dataframe(dataframe = test_df,\n                                               directory = test_image_dir,\n                                               x_col = \"image\",\n                                               y_col = \"labels\",\n                                               class_mode= \"categorical\", \n                                               target_size = (imageSize, imageSize),\n                                               batch_size = batchSize,\n                                               seed = 123)","7dda94f4":"predResult = []\nprediction = model.predict(test_generator).tolist()","ca3bed75":"prediction## list of list","3cac886e":"train_generator.class_indices","c76e4687":"THRESHOLD=0.5\npredResult = []\nfor img in prediction:\n    predictionLabels = []\n    index = 0\n    for categoryScore in img:\n        if categoryScore > THRESHOLD:\n            predictionLabels.append(index)\n        index += 1\n    if not predictionLabels:\n        predResult.append(np.argmax(prediction))\n    else:\n        predResult.append(predictionLabels)","6b7fdb06":"predResult","9a5b969d":"label_map = (train_generator.class_indices)\nfinal_label_map = dict((v,k) for k,v in label_map.items())\nprint(final_label_map)\n\nfor i in range(len(test_df)):\n    labels = \"\"\n    for idx in predResult[i]:\n        labels += final_label_map[idx] + \" \"\n    test_df.iloc[i, 1] = labels.strip()","84495dc1":"test_df.head()","7c35fbbd":"test_df.to_csv('submission.csv', index=False)","a1e7169d":"#### Input parameters of ImageDataGenerator for resnet50 pretrained model:\n1. preprocessing_function : resnet50\n2. rescale : for rescaling the input image \n3. shear range : Shear Intensity (Shear angle in counter-clockwise direction in degrees)\n4. zoom_range : Range for random zoom\n5. horizontal_flip : Randomly flip inputs horizontally\n6. validation_split : validation set ","e4684c28":"### Predictions on Test set","46dd4c4c":"### Prepare the image generator object","ee9321ba":"<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\">Visualize the image with labels&nbsp;&nbsp;&nbsp;&nbsp;<\/h1>","3f560b90":"<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\">Problem Statement&nbsp;&nbsp;&nbsp;&nbsp;<\/h1>\n\nApples are one of the most important temperate fruit crops in the world. Foliar (leaf) diseases pose a major threat to the overall productivity and quality of apple orchards. The current process for disease diagnosis in apple orchards is based on manual scouting by humans, which is time-consuming and expensive.\n\nAlthough computer vision-based models have shown promise for plant disease identification, there are some limitations that need to be addressed. Large variations in visual symptoms of a single disease across different apple cultivars, or new varieties that originated under cultivation, are major challenges for computer vision-based disease identification. These variations arise from differences in natural and image capturing environments, for example, leaf color and leaf morphology, the age of infected tissues, non-uniform image background, and different light illumination during imaging etc.\n\nPlant Pathology 2020-FGVC7 challenge competition had a pilot dataset of 3,651 RGB images of foliar disease of apples. For Plant Pathology 2021-FGVC8, we have significantly increased the number of foliar disease images and added additional disease categories. This year\u2019s dataset contains approximately 23,000 high-quality RGB images of apple foliar diseases, including a large expert-annotated disease dataset. This dataset reflects real field scenarios by representing non-homogeneous backgrounds of leaf images taken at different maturity stages and at different times of day under different focal camera settings.\n\n<b>Specific Objectives<\/b>\n\nThe main objective of the competition is to develop machine learning-based models to accurately classify a given leaf image from the test dataset to a particular disease category, and to identify an individual disease from multiple disease symptoms on a single leaf image.","06d3e186":"### Make prediction","9f4b2f32":"### Generate predicted labels","ca995f3f":"### Generated batches with augemented images from image generator class","bbe1ac76":"<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\">ResNet as a rescuer&nbsp;&nbsp;&nbsp;&nbsp;<\/h1> \n\nAfter the celebrated victory of AlexNet at the LSVRC2012 classification contest, deep Residual Network was arguably the most groundbreaking work in the computer vision\/deep learning community in the last few years. ResNet makes it possible to train up to hundreds or even thousands of layers and still achieves compelling performance.\n\nTaking advantage of its powerful representational ability, the performance of many computer vision applications other than image classification have been boosted, such as object detection and face recognition.\nSince ResNet blew people\u2019s mind in 2015, many in the research community have dived into the secrets of its success, many refinements have been made in the architecture.\n\nAccording to the universal approximation theorem, given enough capacity, we know that a feedforward network with a single layer is sufficient to represent any function. However, the layer might be massive and the network is prone to overfitting the data. Therefore, there is a common trend in the research community that our network architecture needs to go deeper.\n\nSince AlexNet, the state-of-the-art CNN architecture is going deeper and deeper. While AlexNet had only 5 convolutional layers, the VGG network and GoogleNet (also codenamed Inception_v1) had 19 and 22 layers respectively.\n\nHowever, increasing network depth does not work by simply stacking layers together. Deep networks are hard to train because of the notorious vanishing gradient problem \u2014 as the gradient is back-propagated to earlier layers, repeated multiplication may make the gradient infinitively small. As a result, as the network goes deeper, its performance gets saturated or even starts degrading rapidly.\n\nThe core idea of ResNet is introducing a so-called \u201cidentity shortcut connection\u201d that skips one or more layers, as shown in the following figure:\n<center><img src=\"https:\/\/miro.medium.com\/max\/816\/1*ByrVJspW-TefwlH7OLxNkg.png\"><\/center>\n<center><img src=\"https:\/\/miro.medium.com\/max\/1000\/1*2ns4ota94je5gSVjrpFq3A.png\"><\/center>\n<center><h5>the resnet archtecture<\/h5><\/center>\n<br>\n\nThe authors of argue that stacking layers shouldn\u2019t degrade the network performance, because we could simply stack identity mappings (layer that doesn\u2019t do anything) upon the current network, and the resulting architecture would perform the same. This indicates that the deeper model should not produce a training error higher than its shallower counterparts. They hypothesize that letting the stacked layers fit a residual mapping is easier than letting them directly fit the desired underlaying mapping. And the residual block above explicitly allows it to do precisely that.\n\nFollowing this intuition, the authors of refined the residual block and proposed a pre-activation variant of residual block, in which the gradients can flow through the shortcut connections to any other earlier layer unimpededly. In fact, using the original residual block in, training a 1202-layer ResNet resulted in worse performance than its 110-layer counterpart.\n\n<img src=\"https:\/\/miro.medium.com\/max\/1400\/1*M5NIelQC33eN6KjwZRccoQ.png\">\n<center><h5>variants of residual blocks<\/h5><\/center>","b1974c3c":"### Final results ","0794051f":"### Check the history","e6db6a5c":"<center><div class=\"alert alert-block alert-success\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 20px;\">\n    <img src=\"https:\/\/cdn2.iconfinder.com\/data\/icons\/digital-and-internet-marketing-3-1\/50\/144-512.png\" width=50 align='left'>\n    Contents\n<\/div><\/center>\n    \n<p id=\"toc\"><\/p>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#PS\">&nbsp;&nbsp;&nbsp;&nbsp;1.Problem Statement<\/a><\/h3>\n\n---\n    \n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#visual\">&nbsp;&nbsp;&nbsp;&nbsp;2.Visualization<\/a><\/h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#model\">&nbsp;&nbsp;&nbsp;&nbsp;3.Model Building<\/a><\/h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#results\">&nbsp;&nbsp;&nbsp;&nbsp;4.Conclusion and Results<\/a><\/h3>\n\n---\n","efce9814":"### Preprocessing the images","8ee93dc0":"### Save the trained Model","3f680db2":"<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\">Import Libraries&nbsp;&nbsp;&nbsp;&nbsp;<\/h1>","14acaa70":"### Generate unique labels from the data","42bb9740":"<h1 style=\"text-align: center; font-family: Verdana; font-size: 32px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; font-variant: small-caps; letter-spacing: 3px; color: red; background-color: #ffffff;\">Plant Pathology : ResNet50<\/h1>\n\n<center><img src=\"https:\/\/miro.medium.com\/max\/1416\/1*McwAbGJjA1lV_xBdg1w5XA.png\"><\/center>\n\n<center><div class=\"alert alert-danger\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\nIncreasing Network depths leads to worst performance<\/div>\n<\/center>","54709602":"### Create callback : Early stopping and ReduceLROnPlateau"}}