{"cell_type":{"d192426e":"code","5a66096d":"code","14149ab4":"code","498dc5d0":"code","64eefc3b":"code","583d9494":"code","c8d0d8a6":"code","7158dc25":"code","39606d16":"code","158134bc":"code","caa4f437":"code","baeadcf2":"code","b6f28094":"code","73fc3c77":"code","f4ddf8fd":"code","0034c81c":"code","3f524714":"code","9d4b16b1":"code","e7f69b70":"code","f8facdbc":"markdown","1d90ed47":"markdown"},"source":{"d192426e":"import pandas as pd\nimport numpy as np\nimport string\nimport json\nimport shutil\n\nfrom pathlib import Path\nimport os\nfrom os import listdir, mkdir\nfrom os.path import isfile, isdir, join, exists, abspath\nfrom keras.preprocessing import image\nfrom keras.applications.resnet import ResNet152, preprocess_input\nfrom sklearn.model_selection import train_test_split","5a66096d":"# ! pip install git+https:\/\/github.com\/crazyfrogspb\/RedditScore.git > \/dev\/null","14149ab4":"def _globalMaxPool1D(tensor):\n    _,_,_,size = tensor.shape\n    return [tensor[:,:,:,i].max() for i in range(size)]\n\ndef _getImageFeatures(model, img_path):\n    img = image.load_img(img_path, target_size=None)\n\n    img_data = image.img_to_array(img)\n    img_data = np.expand_dims(img_data, axis=0)\n    img_data = preprocess_input(img_data)\n\n    feature_tensor = model.predict(img_data)\n    get_img_id = lambda p: p.split('\/')[-1].split('.')[0]\n    return {\n        \"id\": get_img_id(img_path),\n        \"features\": _globalMaxPool1D(feature_tensor),\n    }\n\ndef _getJSON(path):\n    with open(path) as json_file:\n        return json.loads(json.load(json_file))\n    \ndef _clean_text(text):\n    text = text.replace(\"\\n\", \" \")\n    # onyshchak: only checking first 1000 characters, will need to extract summary propely\n    text = text[:1000].rsplit(' ', 1)[0]\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    return text\n\ndef _getTextFeatures(text_path):\n    data = _getJSON(text_path)\n    \n    return {\n        'id': data['id'],\n        'text': _clean_text(data['text']),\n        'title': data['title']\n    }\n\ndef _getImagesMeta(path):\n    data = _getJSON(path)['img_meta']\n    for x in data:\n        x['description'] = _clean_text(x['description'])\n        x['title'] = _clean_text(x['title'])\n    return data\n\ndef _getValidImagePaths(article_path):\n    img_path = join(article_path, 'img\/')\n    return [join(img_path, f) for f in listdir(img_path) if isfile(join(img_path, f)) and f[-4:].lower() == \".jpg\"]\n\ndef _dump(path, data):\n    with open(path, 'w', encoding='utf8') as outfile:\n        json.dump(data, outfile, indent=2, ensure_ascii=False)\n\ndef GetArticleData(article_path):\n    article_data = _getTextFeatures(join(article_path, 'text.json'))\n    article_data[\"img\"] = _getImagesMeta(join(article_path, 'img\/', 'meta.json'))\n    \n    return article_data\n\ndef ReadArticles(data_path, pred=None, offset=0, limit=None):\n    article_paths = [join(data_path, f) for f in listdir(data_path) if isdir(join(data_path, f))]\n    limit = limit if limit else len(article_paths) - offset\n    limit = min(limit, len(article_paths) - offset)\n    \n    articles = []\n    for i in range(offset, offset + limit):\n        path = article_paths[i]\n        if (i - offset) % 300 == 0: print(i - offset, \"articles have been read\")\n        article_data = GetArticleData(path)\n        if pred and not pred(i, article_data): continue\n        \n        articles.append(article_data)\n        if len(articles) >= limit: break  # useless?\n        \n    print(offset + limit, \"articles have been read\")\n    return articles\n\ndef GenerateVisualFeatures(data_path, offset=0, limit=None, model=None):\n    article_paths = [join(data_path, f) for f in listdir(data_path) if isdir(join(data_path, f))]\n    limit = limit if limit else len(article_paths) - offset\n    limit = min(limit, len(article_paths) - offset)\n    model = model if model else ResNet152(weights='imagenet', include_top=False) \n    \n    for i in range(offset, offset + limit):\n        path = article_paths[i]\n        print(i, path)\n    \n        meta_path = join(path, 'img\/', 'meta.json')\n        meta_arr = _getImagesMeta(meta_path)\n        for meta in meta_arr:\n            if 'features' in meta: continue\n            if meta['filename'][-4:].lower() != \".jpg\": continue\n                \n            img_path =  join(path, 'img\/', meta['filename'])\n            try:\n                features = _getImageFeatures(model, img_path)['features']\n                meta['features'] = [str(f) for f in features]\n            except Exception as e:\n                print(\"exception\", str(e))\n                print(img_path)\n                continue\n                \n        _dump(meta_path, json.dumps({\"img_meta\": meta_arr}))","498dc5d0":"%%time\ndataset_root = '\/kaggle\/input\/extended-wikipedia-multimodal-dataset\/'\narticles = ReadArticles(dataset_root + 'data_full\/data\/', offset=0, limit=None)\nprocessed_titles = _getJSON(dataset_root + \"parsed_titles.json\")","64eefc3b":"dataset_name = 'data_w2vv'\ndataset_path = join('.\/', dataset_name)\nif exists(dataset_path):\n    shutil.rmtree(dataset_path)\n    \nmkdir(dataset_path)\nsubsets = {\n    \"train\": {},\n    \"val\": {},\n    \"test\": {},\n}\n\nfor k, v in subsets.items():\n    v['name'] = dataset_name + k\n    v['path'] = join(dataset_path, v['name'])\n    mkdir(v['path'])\n    \n    v['feature_data_path'] = join(v['path'], 'FeatureData')\n    if k == 'train':\n        mkdir(v['feature_data_path'])\n    else:\n        dst = v['feature_data_path']\n        os.symlink(os.path.relpath(subsets['train']['feature_data_path'], Path(dst).parent), dst)\n\n    v[\"image_sets_path\"] = join(v['path'], 'ImageSets')\n    mkdir(v[\"image_sets_path\"])\n\n    v[\"text_data_path\"] = join(v['path'], 'TextData')\n    mkdir(v[\"text_data_path\"])","583d9494":"def to_file(arr, filepath):\n    with open(filepath, 'w') as f:\n        for x in arr:\n            f.write(\"%s\\n\" % x)","c8d0d8a6":"is_valid_img = lambda i: 'features' in i\nhas_valid_img = lambda a: len([i for i in a['img'] if is_valid_img(i)]) > 0\narticles = [a for a in articles if has_valid_img(a)]\nlen(articles)","7158dc25":"list2str = lambda l: \" \".join([str(x) for x in l])\nget_img_id = lambda i: os.path.splitext(i['filename'])[0]  # removing file extention\n\nimg_features = set(['{} {}'.format(get_img_id(i), list2str(i['features'])) for a in articles for i in a['img'] if is_valid_img(i)])\nprint(\"len(img_features) = \", len(img_features))\n\nraw_features_file_path = join(subsets['train'][\"feature_data_path\"], subsets['train']['name'] + \".features.txt\")\nto_file(img_features, raw_features_file_path)","39606d16":"def map_data(articles):\n    seen = set()\n    res = []\n    for a in articles:\n        for i in a['img']:\n            if 'features' not in i: continue\n                \n            img_id = os.path.splitext(i['filename'])[0]  # removing file extention\n            if img_id in seen:\n                # onyshchak: if image used in 2 articles, we only take the first one for simplicity\n                # TODO: use all the infomation without breaking the model\n                continue\n                \n            seen.add(img_id)\n            res.append({\n                \"filename\": img_id,\n                'article_id': a['id'],\n                'article_title': a['title'],\n                \"title\": os.path.splitext(i['title'])[0],\n                \"description\": i['description'],\n                \"text\": a['text'],\n                \"features\": i['features'],\n            })\n            \n    return res","158134bc":"ARTICLE_LEVEL_SPLIT = True\nseed = 1234\n\nif ARTICLE_LEVEL_SPLIT:\n    train, test = train_test_split(articles, test_size=0.04, random_state=seed)\n    train, val = train_test_split(train, test_size=0.043, random_state=seed)\n\n    subsets['train']['data'] = map_data(train)\n    subsets['val']['data'] = map_data(val)\n    subsets['test']['data'] = map_data(test)\nelse:\n    mapped_images = map_data(articles)\n    # 2325 test, 2057 val\n    train, test = train_test_split(mapped_images, test_size=1000, random_state=seed)\n    train, val = train_test_split(train, test_size=1000, random_state=seed)\n    \n    subsets['train']['data'] = train\n    subsets['val']['data'] = val\n    subsets['test']['data'] = test\n\nsubset_split_info = str([(k, len(v['data'])) for k, v in subsets.items()]).replace(\"(\", \"\").replace(\")\", \"\")\nsubset_split_info","caa4f437":"ids = [x['article_id'] for x in subsets['test']['data']]\nto_file(ids, join(subsets['test']['path'], \"test_articles_ids.txt\"))","baeadcf2":"for v in subsets.values():\n    ids = [x['filename'] for x in v['data']]\n    to_file(ids, join(v[\"image_sets_path\"], v['name'] + \".txt\"))","b6f28094":"# from redditscore.tokenizer import CrazyTokenizer\n\n# tokenizer = CrazyTokenizer(hashtags='split')\n\n# def split_hashtag_words(text):\n#     global tokenizer\n#     return \" \".join(tokenizer.tokenize(\"#\" + text))\n\n# def remove_auxiliary_words(text):\n#     aux = [\"jpeg\", \"jpg\", \"png\"] # only 1 valid word can contain png, other - image extention trash\n#     for a in aux:\n#         if a == text[-len(a):]:\n#             text = text[:-len(a)]\n#             break\n                    \n#     return text\n\n# process_title = lambda x: split_hashtag_words(remove_auxiliary_words(x))","73fc3c77":"%%time\n# onyshchak: originally ID also contained file extention e.g. *.jpg. but not in image_sets_path\ndef get_description(z):\n    if z['description']: return z['description']\n    elif z['filename'] in processed_titles: return processed_titles[z['filename']]\n    else: \n        print(\"Missing title\", z['filename'])\n        return z['title']\n\nfor v in subsets.values():\n    text_data = sorted(\n        ['{}#enc#0 {}'.format(x['filename'], x['text']) for x in v['data']] +\n        ['{}#enc#1 {}'.format(x['filename'], get_description(x)) for x in v['data']]\n    )\n\n    to_file(text_data, join(v[\"text_data_path\"], v['name'] + \".caption.txt\"))","f4ddf8fd":"del articles\ndel text_data\ndel train\ndel val\ndel ids\ndel test\ndel img_features\nfor k,v in subsets.items():\n    del v['data']","0034c81c":"! apt install --assume-yes python-pip > \/dev\/null\n! python2 -m pip install --user numpy scipy matplotlib ipython jupyter pandas nose > \/dev\/null","3f524714":"IS_FILE_LIST = 0\nFEATURE_DIMENTION = 2048\nfeature_data_path = subsets['train'][\"feature_data_path\"]\nbin_features_path = join(feature_data_path, \"pyresnet152-pool5os\/\")\n\n! python2 \/kaggle\/input\/w2vv-scripts\/simpleknn\/txt2bin.py $FEATURE_DIMENTION $raw_features_file_path $IS_FILE_LIST $bin_features_path --overwrite 1","9d4b16b1":"! mv $dataset_name\/* .\/\n! rmdir $dataset_name","e7f69b70":"version_notes = \"version_notes.txt\"\n! echo article level = $ARTICLE_LEVEL_SPLIT > $version_notes\n! echo seed = $seed >> $version_notes\n! echo \"caption 0 = article summary\" >> $version_notes\n! echo \"caption 1 = description(or parsed titles)\" >> $version_notes\n! echo split = $subset_split_info >> $version_notes\n\n! cat $version_notes","f8facdbc":"## Mapping Preprocessed Dataset into W2VV format","1d90ed47":"## Loading&Preprocessing Dataset"}}