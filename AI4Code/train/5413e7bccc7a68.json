{"cell_type":{"0f984760":"code","ccf40b3e":"code","506d06bd":"code","b20dceb9":"code","cb16bb40":"code","d13239cc":"code","e2526bbe":"code","1484ee56":"markdown","deb3c60d":"markdown","252bc11d":"markdown","49f8cfbc":"markdown"},"source":{"0f984760":"import numpy as np\nimport pandas as pd\nfrom lightgbm import LGBMClassifier\nfrom tqdm import tqdm","ccf40b3e":"import spacy\nspacy_model = spacy.load('en_core_web_lg') # 'lg' means large and 'en' means english so we are saying import the large spacy english model\n#there is small model too just change 'lg' to 'sm', note that the large model vectorize the text to 300d whereas the small model vectorize the text to 96d","506d06bd":"train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv') #load the data\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nsb = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","b20dceb9":"spacy_vectorizer = lambda text: spacy_model(text).vector # vectorize the data\nvectorized_train_documents = []\nfor i in tqdm(range(train.shape[0])):\n    vectorized_train_documents.append(spacy_vectorizer(train.iloc[i].text.lower()))\nprint('Vectorizing the Training documents is DONE!')\n\nvectorized_test_documents = []\nfor i in tqdm(range(test.shape[0])):\n    vectorized_test_documents.append(spacy_vectorizer(test.iloc[i].text.lower()))\nprint('Vectorizing the Testing documents is DONE!')","cb16bb40":"xtrain_spacy = np.array(vectorized_train_documents) #put the data in the right format\nxtest_spacy = np.array(vectorized_test_documents)\nytrain = train.target.values.reshape(-1, 1)","d13239cc":"lgbmc = LGBMClassifier(learning_rate=.05, n_estimators=150,) # train the model\nlgbmc.fit(xtrain_spacy, ytrain)","e2526bbe":"import os #save the predictions on the test set\nos.chdir('\/kaggle\/working\/')\nsb['target'] = lgbmc.predict(xtest_spacy)\nsb.to_csv('SubmitMe!.csv', index=False)","1484ee56":"<center>**Stay home and Machine Learn :))**<\/center>","deb3c60d":"In this notebook we are going to build a simple model for **Real or Not? NLP with Disaster Tweets Competition** that got me roughly .8 on the leaderboard but if you want to go higher I'll suggest using BERT, there are tons of great notebooks on kaggle that implements BERT and what I'd reccommend is reading about this model online then take a look at the implementation, anyways let's get started and what we are going to do here is just vectorizing sentences using spacy then give the vectorized sentences to a fine tuned LightGBM model. ","252bc11d":"![](http:\/\/)**What is spaCy ?**\nspaCy is a free open-source library for Natural Language Processing in Python. It features NER, POS tagging, dependency parsing, word vectors and more. but in this notebook we are interested in only text vectorization using spaCy","49f8cfbc":"**What is LightGBM ? ** It's an open-source Tree Based Gradient Boosting Framework.It was developed by Microsoft, it's pretty popular along with CatBoost and XGBoost that uses roughly same methodology"}}