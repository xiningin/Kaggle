{"cell_type":{"6866878f":"code","072fea9d":"code","ec03d558":"code","b06d6b16":"code","643bdd50":"code","88d681f8":"code","9018119c":"code","3a5a977f":"code","fdd1f8d0":"code","dd3c1045":"code","e2fb9d2b":"code","7763ef79":"code","56b3ef7e":"code","1ef5d49c":"code","8ed9c948":"code","b376d901":"code","1ad26245":"code","be0fe3c1":"code","cabd10d7":"code","e2ea1f0a":"code","d1560d2d":"code","cdbb732d":"code","41346b9e":"code","158fe655":"code","aa0eee20":"code","b1f713d3":"code","40213ee5":"code","077a40d1":"code","07f7fa82":"code","e3692c8c":"code","29a522c5":"code","899ca80f":"code","cc6e1930":"code","71ec6b6e":"code","54c1000f":"code","0515f4b9":"code","c3353c8a":"code","3ceb9117":"code","f5ae57d1":"code","dec3d2b1":"code","78b0ed2c":"code","d418459d":"code","d902c9c2":"code","0a3ef4da":"code","11f2d562":"code","d1bdbc22":"markdown","2399c147":"markdown","464f2c16":"markdown","3ea0d9da":"markdown","188494ca":"markdown","acb2bff2":"markdown","fc466d5b":"markdown","f1551018":"markdown","a652dcbd":"markdown","3b775237":"markdown","df1a4413":"markdown","f6ae0818":"markdown","3e303bd2":"markdown","57d6ac85":"markdown","38d625ae":"markdown","02a8ca5f":"markdown","7c0d0803":"markdown","0187765b":"markdown","60f47645":"markdown","d9f7edab":"markdown","3e2357dd":"markdown","6e411c06":"markdown","0341d1a6":"markdown","93225a8c":"markdown","14f0bb31":"markdown","47b7d1d9":"markdown","0ba15879":"markdown","6d638117":"markdown","a1bbfc93":"markdown","24f5f50c":"markdown","6645d1d1":"markdown","bb3e6406":"markdown","db6e0fdd":"markdown","7f4499fa":"markdown","79a16299":"markdown","2c6375f7":"markdown","810276cb":"markdown","982cec25":"markdown","ddf367b9":"markdown","2758aab4":"markdown","9d9a3174":"markdown","6dd9a589":"markdown","26ff18bc":"markdown","ac8cbc92":"markdown"},"source":{"6866878f":"import numpy as np\nimport pandas as pd\n\n# Disabling warnings\nimport sys\nimport warnings\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")","072fea9d":"def nan_predict(df,\n                skip_features_from_prediction_where_percent_missing_data_more_than = 100,\n                include_features_as_predictors_where_perc_missing_data_less_than = 50,\n                apply_fast_predictor_where_missing_data_less_than_percent = 100,\n                use_n_rows_for_train_not_more_than = 1000000000,    #  If your dataframe is large\n                randomizedSearchCV_iter_plus_perc_missing_data = 10,\n                n_estimators_parameter_for_LightGBM = 2000,\n                target_feature = None,   # For prediction at the end\n                ): \n    \n    import random\n    import pandas as pd\n    import numpy as np\n\n    # Disabling warnings\n    import sys\n    import warnings\n    if not sys.warnoptions:\n        warnings.simplefilter(\"ignore\")\n\n\n    from lightgbm import LGBMClassifier\n    from lightgbm import LGBMRegressor\n    from sklearn.model_selection import RandomizedSearchCV\n    from sklearn.model_selection import ShuffleSplit\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import mean_absolute_error\n    from sklearn.metrics import accuracy_score\n    from sklearn.metrics import f1_score\n    from sklearn.preprocessing import LabelEncoder\n    \n    \n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    %matplotlib inline\n    \n    \n\n    global counter_all_predicted_values\n    counter_all_predicted_values = 0\n    \n    global numeric_features\n    numeric_features = []\n    \n    global best_params\n    \n    \n    PARAMS  =  {'num_leaves': [12, 50, 120, 200, 300, 400, 500],   #np.arange(200, 600, step=100),\n                'max_depth': [4, 8, 12, 16],\n                'learning_rate': [0.001, 0.01, 0.1],\n                'n_estimators': [n_estimators_parameter_for_LightGBM],\n                'subsample': [0.1, 0.3, 0.5],\n                'feature_fraction': [0.1, 0.3, 0.5],\n                'bagging_fraction': [0.1, 0.3, 0.5],\n                'bagging_seed': np.arange(1, 3, step=1),\n                'lambda_l1': [0.2],\n                'lambda_l2': [0.1],\n                'min_child_samples': np.arange(2, 6, step=2),\n                'min_split_gain': [0.0001, 0.001]\n               }\n    \n    \n    CV = ShuffleSplit(n_splits=2, test_size=0.25, random_state=0)\n    \n    \n    \n\n    def NaN_info(df):\n        global null_view\n        try:\n            null_view = df[[col for col in df.columns if df[col].isna().sum() > 0]].isna().sum().sort_values(ascending = True)\n            null_view = pd.DataFrame(null_view, columns=['NANs'])\n            null_view[['PERCENT']] = null_view.NANs.apply(lambda x: round((x\/len(df))*100, 2))\n            null_view[['TYPE']] = df.dtypes\n        except:\n            return null_view\n        return null_view\n    \n    \n    def numeric_features(df):\n        num_features = [feature for feature in df.columns if df[feature].dtype in ['int64', 'float64']]\n        return num_features\n    \n    \n    def integer_features(df):\n        global int_features\n        int_features = [feature for feature in df.columns if df[feature].dtype in ['int64']]\n        return int_features\n\n\n    def encoding(work_predictors, df):\n        feature_power = 0.5          # Skew handling\n        for j in work_predictors:\n            el_type = df[j].dtype\n            if el_type == 'object':\n                df[j].replace(np.nan, 'NoNoNo', inplace=True)\n                labelencoder = LabelEncoder()\n                df.loc[:, j] = labelencoder.fit_transform(df.loc[:, j])\n            else:\n                df[j] = df[j]**feature_power\n        return df, work_predictors\n\n\n    def hyperparms_tuning(CV, X_train, X_test, y_train, y_test, n_iter_for_RandomizedSearchCV, PARAMS, alg, scoring):\n        global best_params\n        global pred_test_lgb\n\n        lgbm = alg(random_state = 0)\n        lgbm_randomized = RandomizedSearchCV(estimator=lgbm, \n                                            param_distributions=PARAMS, \n                                            n_iter=n_iter_for_RandomizedSearchCV, \n                                            scoring=scoring, \n                                            cv=CV, \n                                            verbose=0,\n                                            n_jobs = -1)\n\n        lgbm_randomized.fit(X_train, y_train)\n        \n        best_params = lgbm_randomized.best_params_\n        pred_test_lgb = lgbm_randomized.predict(X_test)\n        return best_params, pred_test_lgb\n\n    \n    def predict_regressor(best_params, X, y, miss_df):\n        print('Best parameters:')\n        print(best_params)\n        print('')\n        global pred_miss\n        lgbm = LGBMRegressor(**best_params, n_jobs=-1, random_state=0)\n        lgbm.fit(X, y)\n        pred_miss = list(lgbm.predict(miss_df))\n        print('-------------------------------')\n        print(f\"The first 100 predicted missing values: \\n{pred_miss[:100]}\")\n        return pred_miss\n\n\n    def predict_classifier(best_params, X, y, miss_df):\n        print('Best parameters:')\n        print(best_params)\n        print('')\n        global pred_miss\n        lgbm = LGBMClassifier(**best_params, n_jobs=-1, random_state=0)\n        lgbm.fit(X, y)\n        pred_miss = list(lgbm.predict(miss_df))\n        print('-------------------------------')\n        print(f\"The first 100 predicted missing values: \\n{pred_miss[:100]}\")\n        return pred_miss\n    \n    \n    def imput_missing_value_to_main_df(df, miss_indeces, pred_miss, el):\n        counter = 0\n        for idx in miss_indeces:\n            df.loc[idx, el] = pred_miss[counter]\n            counter += 1\n        return df\n    \n    \n    \n    # Go)\n\n    plt.figure(figsize=(20, 5))\n    sns.heatmap(df.isnull(), cbar=False)\n    \n    \n    print(NaN_info(df))\n    print('\\n\\n\\n')\n    \n    all_features = list(df.columns)\n    df_indeces = list(df.index)\n    df.reset_index(drop=True, inplace=True)\n    \n    integer_features(df)\n\n    delete_miss_features = list(\n        (null_view.loc[null_view['PERCENT'] > skip_features_from_prediction_where_percent_missing_data_more_than]).index)\n    print(f'Exclude from the prediction, because missing data more than \\\n    {skip_features_from_prediction_where_percent_missing_data_more_than}% :\\n{delete_miss_features}')\n    print('')\n    all_miss_features = list(null_view.index)\n\n    for delete_feature in delete_miss_features:\n        all_miss_features.remove(delete_feature)\n        \n    \n    if target_feature in all_miss_features:  # moving target_feature to end of the prediction\n        all_miss_features.append(all_miss_features.pop(all_miss_features.index(target_feature)))\n        \n    \n    for el in all_miss_features:\n        print('\\n\\n\\n\\n')\n        \n        # select features as predictors\n        NaN_info(df)\n        lot_of_miss_features = list(\n            (null_view.loc[null_view['PERCENT'] > include_features_as_predictors_where_perc_missing_data_less_than]).index)\n        now_predictors = list(set(all_features)-set(lot_of_miss_features))\n        work_predictors = list(set(now_predictors) - set([el]))\n\n        \n        # missing data (data for prediction)\n        miss_indeces = list((df[pd.isnull(df[el])]).index)\n        miss_df = df.iloc[miss_indeces][:]\n        miss_df = miss_df[work_predictors]\n        encoding(work_predictors, df=miss_df)\n\n        \n        # data without NaN rows (X data for train, evaluation of model)\n        work_indeces = list(set(df_indeces) - set(miss_indeces))\n        if len(work_indeces) > use_n_rows_for_train_not_more_than:\n            randomlist = random.sample(range(0, len(work_indeces)), use_n_rows_for_train_not_more_than)\n            work_indeces = [work_indeces[i] for i in randomlist]\n        \n        work_df = df.iloc[work_indeces][:] \n        encoding(work_predictors, df=work_df)\n        X = work_df[work_predictors]\n        y = work_df[el]\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n\n        \n        # Info\n        feature_type = df[el].dtypes\n        percent_missing_data = null_view['PERCENT'][el]\n        print(f'Feature: {el},   type: {feature_type},   missing values: {percent_missing_data}%\\n')    \n        print(f'Shape for train dataframe: {(X.shape)}')\n        print(f'Unused features as predictors, because missing data more than {include_features_as_predictors_where_perc_missing_data_less_than}% :')\n        print(lot_of_miss_features)\n        print('')\n        \n        \n        # PREDICTIONS\n        if percent_missing_data < apply_fast_predictor_where_missing_data_less_than_percent:\n            \n            # FAST Predictions without tuning hyperparameters\n            \n            print('FAST prediction without tuning hyperparameters\\n')\n            best_params = {}\n            if feature_type == 'object' or feature_type == 'bool':\n                print('FAST CLASSIFIER:')\n                labelencoder = LabelEncoder()\n                y_train = labelencoder.fit_transform(y_train)\n                y_test = labelencoder.fit_transform(y_test)\n                lgbm = LGBMClassifier(n_jobs=-1, random_state=0)\n                lgbm.fit(X_train, y_train)\n                pred_test_lgb_FAST = lgbm.predict(X_test)\n                accuracy = accuracy_score(y_test, pred_test_lgb_FAST)\n                print('Evaluations:')\n                print(f'first 10 y_test: {y_test[:10]}')\n                print(f'first 10 y_pred: {pred_test_lgb_FAST[:10]}\\n')\n                f1 = f1_score(y_test, pred_test_lgb_FAST, average='weighted')\n                print(f'accuracy_score:      {accuracy}')\n                print(f'f1_score (weighted): {f1}')\n                \n                predict_classifier(best_params, X, y, miss_df)\n                counter_all_predicted_values += len(miss_indeces)\n                imput_missing_value_to_main_df(df, miss_indeces, pred_miss, el)\n\n            elif feature_type == 'float64' or feature_type == 'int64':\n                print('FAST REGRESSOR:')\n                \n                lgbm = LGBMRegressor(n_jobs=-1, random_state=0)\n                lgbm.fit(X_train, np.log1p(y_train))\n                pred_test_lgb_FAST = lgbm.predict(X_test)\n                pred_test_lgb_FAST = np.expm1(pred_test_lgb_FAST)\n                MAE = mean_absolute_error(y_test,pred_test_lgb_FAST)\n                y_te = list(round(y_test[:10], 1))\n                y_pred = list(np.round(pred_test_lgb_FAST[:10], 1))\n                print('Evaluations:')\n                print(f'first 10 y_test: {y_te}')\n                print(f'first 10 y_pred: {y_pred}\\n')\n                print(f'mean_absolute_error: {MAE}')\n                print(f'mean for {el}: {df[el].mean()}')\n                \n                predict_regressor(best_params, X, y, miss_df)\n                counter_all_predicted_values += len(miss_indeces)\n                imput_missing_value_to_main_df(df, miss_indeces, pred_miss, el)\n\n            else:\n                print(f\"unprocessed feature: {el} - {feature_type} type\")\n                \n                  \n        else:\n            \n            # ADVANCED Predictions with tuning hyperparameters\n            \n            n_iter_for_RandomizedSearchCV = int(randomizedSearchCV_iter_plus_perc_missing_data + percent_missing_data * 1)\n            print(f'Iteration for RandomizedSearchCV: {n_iter_for_RandomizedSearchCV}\\n')\n            \n            if feature_type == 'object' or feature_type == 'bool':\n                print('ADVANCED CLASSIFIER:')\n                labelencoder = LabelEncoder()\n                y_train = labelencoder.fit_transform(y_train)\n                y_test = labelencoder.fit_transform(y_test)\n                hyperparms_tuning(CV, X_train, X_test, y_train, y_test, n_iter_for_RandomizedSearchCV, PARAMS, alg=LGBMClassifier, scoring='f1_weighted')\n                accuracy = accuracy_score(y_test, pred_test_lgb)\n                print('Evaluations:')\n                print(f'first 10 y_test: {y_test[:10]}')\n                print(f'first 10 y_pred: {pred_test_lgb[:10]}\\n')\n                f1 = f1_score(y_test, pred_test_lgb, average='weighted')\n                print(f'accuracy_score:      {accuracy}')\n                print(f'f1_score (weighted): {f1}')\n                \n                predict_classifier(best_params, X, y, miss_df)\n                counter_all_predicted_values += len(miss_indeces)\n                imput_missing_value_to_main_df(df, miss_indeces, pred_miss, el)\n\n            elif feature_type == 'float64' or feature_type == 'int64':\n                print('ADVANCED REGRESSOR:')\n                hyperparms_tuning(CV, X_train, X_test, y_train, y_test, n_iter_for_RandomizedSearchCV, PARAMS, alg=LGBMRegressor, scoring='neg_mean_squared_error')\n                MAE = mean_absolute_error(y_test,pred_test_lgb)\n                y_te = list(round(y_test[:10], 1))\n                y_pred = list(np.round(pred_test_lgb[:10], 1))\n                print('Evaluations:')\n                print(f'first 10 y_test: {y_te}')\n                print(f'first 10 y_pred: {y_pred}\\n')\n                print(f'mean_absolute_error: {MAE}')\n                print(f'mean for {el}: {df[el].mean()}')\n                \n                predict_regressor(best_params, X, y, miss_df)\n                counter_all_predicted_values += len(miss_indeces)\n                imput_missing_value_to_main_df(df, miss_indeces, pred_miss, el)\n\n            else:\n                print(f\"unprocessed feature: {el} - {feature_type} type\")\n        \n        plt.figure(figsize=(20, 5))\n        sns.heatmap(df.isnull(), cbar=False)\n\n        \n    for feature in int_features:\n        df[[feature]] = df[[feature]].astype('int64')\n        \n    df.index = df_indeces\n\n    print('\\n\\n\\n')\n    print(f'These features have not been processed, because missing data more than {skip_features_from_prediction_where_percent_missing_data_more_than}%')\n    print(NaN_info(df))\n    print('\\n\\n\\n')\n    print(f'{counter_all_predicted_values} values have been predicted and replaced')\n    print('\\n')\n    \n    return df","ec03d558":"df = pd.read_csv('..\/input\/17k-apple-app-store-strategy-games\/appstore_games.csv')\nprint(f'Shape: {df.shape}')\ndf.head(2)","b06d6b16":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nplt.figure(figsize=(5, 5))\nsns.heatmap(df.isnull(), cbar=False)","643bdd50":"def NaN_info(df):\n    global null_view\n    try:\n        null_view = df[[col for col in df.columns if df[col].isna().sum() > 0]].isna().sum().sort_values(ascending = True)\n        null_view = pd.DataFrame(null_view, columns=['NANs'])\n        null_view[['PERCENT']] = null_view.NANs.apply(lambda x: round((x\/len(df))*100, 2))\n        null_view[['TYPE']] = df.dtypes\n    except:\n        return null_view\n    return null_view\n\nNaN_info(df)","88d681f8":"for el in list(df.columns):\n    print(f'======================= {el} =======================')\n    print(df[el].value_counts(dropna=False))\n    print('')\n# for el in range(16990, 17005):\n#     print(df.loc[el, 'URL'])","9018119c":"features = ['Average User Rating', 'Languages', 'Price', 'Size']\n\nany_nan = df[df[features].isnull().any(axis=1)]\nindeces = list(any_nan.index)\nprint(f'number of rows with NaN (mostly in target feature): \\\n{len(indeces)} <= Drop non-borned apps')\ndf = df.drop(df.index[indeces])\nprint(df.shape)","3a5a977f":"df.drop_duplicates(subset=['ID'], inplace=True)\nprint(df.shape)","fdd1f8d0":"df = df.loc[df['Primary Genre'] == 'Games']\ndf.reset_index(drop=True, inplace=True)\ndf.shape","dd3c1045":"plt.figure(figsize=(5, 5))\nsns.heatmap(df.isnull(), cbar=False)","e2fb9d2b":"NaN_info(df)","7763ef79":"indexes = df.index.tolist()\nfor idx in indexes:\n    x = repr(df.loc[idx,'In-app Purchases'])\n    if x == 'nan':\n        df.loc[idx,'In-app Purchases'] = 0\n    else:\n        df.loc[idx,'In-app Purchases'] = 1","56b3ef7e":"df['Size'] = df['Size'].apply(lambda x: int(x\/(1024**2)))\ndf['Size'] = df['Size'].astype('int64')","1ef5d49c":"change={'4+': 4, '9+': 9, '12+': 12, '17+': 17}\ndf['Age Rating'] = df['Age Rating'].map(change)\ndf['Age Rating'] = df['Age Rating'].astype('int64')\nunique = df['Age Rating'].unique()\nprint(f'Unique: {unique}')","8ed9c948":"df.columns","b376d901":"genres = set()\nfor idx in indexes:\n    x = df.loc[idx,'Genres']\n    x = x.split(', ')\n    for el in x:\n        genres.add(el)\n        \n\ngenres = list(genres)\nprint(genres)\nprint('')\nprint(f'Number of genres: {len(genres)}')\n\n\n\nprint(f'shape before: {df.shape}')\n\nfor idx in indexes:\n    x = df.loc[idx,'Genres']\n    x = x.split(', ')\n    for el in x:\n        df.loc[idx,el] = 1\n\nfor el in genres:\n    df[el].replace(to_replace=np.NaN, value=0, inplace=True)\n    \n    change={1: 'Yes', 0: 'No'}\n    df[el] = df[el].map(change)\n    df[el] = df[el].astype('object')\n\n\nprint(f'shape after: {df.shape}')","1ad26245":"df = df.loc[df['Strategy'] == 'Yes']\ndf.reset_index(drop=True, inplace=True)","be0fe3c1":"df.drop(['Strategy', 'Games', 'Primary Genre'], axis=1, inplace=True)\ndf.shape","cabd10d7":"print(len(genres))\ngenres.remove('Strategy')\ngenres.remove('Games')\nprint(len(genres))","e2ea1f0a":"df[['Current Version Release Date']] = pd.to_datetime(df['Current Version Release Date'])\ndf[['num_years_unsupported']] = 2019 - df['Current Version Release Date'].dt.year\ndf.loc[5:11, ['Name', 'Current Version Release Date', 'num_years_unsupported', 'Average User Rating', 'User Rating Count']]","d1560d2d":"sns.set_palette('twilight_shifted')\n# sns.set_palette('winter_r')\nf, ax = plt.subplots(1, 2, figsize=(10, 5), gridspec_kw=dict(width_ratios=[5, 5]))\n\nsns.regplot(x=df['Average User Rating'], y=df['num_years_unsupported'], ax=ax[0])\nsns.regplot(x=(df['User Rating Count'])**0.5, y=df['num_years_unsupported'], ax=ax[1])\n\nf.tight_layout()","cdbb732d":"df[['Original Release Date']] = pd.to_datetime(df['Original Release Date'])\ndf[['age']] = 2019 - df['Original Release Date'].dt.year\ndf.loc[[0,100,1000,2000,3000,4000,5000,6000, 7000],['Original Release Date',\n                                                    'age', \n                                                    'num_years_unsupported', \n                                                    'Average User Rating', \n                                                    'User Rating Count']]","41346b9e":"f, ax = plt.subplots(1, 2, figsize=(10, 5), gridspec_kw=dict(width_ratios=[5, 5]))\n\nsns.regplot(x=df['Average User Rating'], y=df['age'], ax=ax[0])\nsns.regplot(x=(df['User Rating Count'])**0.5, y=df['age'], ax=ax[1])\n\nf.tight_layout()","158fe655":"a = df.loc[((df['age'] > 7) & (df['Average User Rating'] == 5) & (df['User Rating Count'] > 10000))]\na[['Name', 'Developer', 'age', 'num_years_unsupported', 'Average User Rating', 'User Rating Count']]","aa0eee20":"indexes = df.index.tolist()\nfor idx in indexes:\n    x = df.loc[idx,'Languages']\n    x = x.split(', ')\n    count = len(x)\n    if 'EN' in x:\n        count += 19\n    df.loc[idx,'Languages_score'] = count\n\ndf.loc[2:8, ['ID', 'Languages', 'Languages_score']]","b1f713d3":"f, ax = plt.subplots(1, 2, figsize=(10, 5), gridspec_kw=dict(width_ratios=[5, 5]))\n\nsns.regplot(x=df['Average User Rating'], y=df['Languages_score'], ax=ax[0])\nsns.regplot(x=(df['User Rating Count'])**0.5, y=df['Languages_score'], ax=ax[1])\n\nf.tight_layout()","40213ee5":"import plotly.graph_objs as go\nimport plotly.offline as py\n\n\n\ntemp_df = df['Average User Rating'].value_counts().reset_index()\n\ntrace1 = go.Bar(\n                x = temp_df['index'],\n                y = temp_df['Average User Rating'],\n                marker = dict(color = '#47E0FF',\n                              line=dict(color='#000000', \n                                        width=1)))\nlayout = go.Layout(template= \"plotly_dark\",\n                   title = 'Average User Rating', \n                   xaxis = dict(title = 'Average User Rating'), \n                   yaxis = dict(title = 'Count'))\n\nfig = go.Figure(data = [trace1], layout = layout)\nfig.show()\n\ndef pie_plot(count_series, title):\n    labels=count_series.index\n    values=count_series.values\n    trace = go.Pie(labels=labels, \n                   values=values, \n                   title=title, \n                   hoverinfo='percent+value', \n                   textinfo='percent',\n                   textposition='inside',\n                   hole=0.5,\n                   showlegend=True,\n                   marker=dict(colors=plt.cm.viridis_r(np.linspace(0, 1, 20)),\n                               line=dict(color='#ffffff',\n                                         width=2),\n                              )\n                  )\n    return trace\n\npy.iplot([pie_plot(df['Average User Rating'].value_counts(), 'Average User Rating')])","077a40d1":"temp_df1 = df.groupby(df['Original Release Date'].dt.year).mean().reset_index()\n\ntrace1 = go.Scatter(\n                    x=temp_df1['Original Release Date'], \n                    y=temp_df1['Average User Rating'], \n                    name=\"Original Release\", \n                    marker=dict(color = '#FFB3F7',\n                             line=dict(color='#000000',width=1)))\n\n\ntemp_df2 = df.groupby(df['Current Version Release Date'].dt.year).mean().reset_index()\n\ntrace2 = go.Scatter(\n                    x=temp_df2['Current Version Release Date'], \n                    y=temp_df2['Average User Rating'], \n                    name=\"Last Version\", \n                    marker=dict(color = '#47E0FF',\n                             line=dict(color='#000000',width=1)))\n\nlayout = go.Layout(hovermode='closest', \n                   title = 'Average User Rating over the years', \n                   xaxis = dict(title = 'Year'), \n                   yaxis = dict(title = 'Average User Rating'), \n                   template= \"plotly_dark\")\n\nfig = go.Figure(data = [trace1, trace2], layout=layout)\nfig.show()","07f7fa82":"temp_df1 = df.groupby(df['Original Release Date'].dt.year).mean().reset_index()\n\ntrace1 = go.Scatter(\n                    x=temp_df1['Original Release Date'], \n                    y=temp_df1['User Rating Count'], \n                    name=\"Original Release\", \n                    marker=dict(color = '#FFB3F7',\n                             line=dict(color='#000000',width=1)))\n\n\ntemp_df2 = df.groupby(df['Current Version Release Date'].dt.year).mean().reset_index()\n\ntrace2 = go.Scatter(\n                    x=temp_df2['Current Version Release Date'], \n                    y=temp_df2['User Rating Count'], \n                    name=\"Last Version\", \n                    marker=dict(color = '#47E0FF',\n                             line=dict(color='#000000',width=1)))\n\nlayout = go.Layout(hovermode='closest', \n                   title = 'User Rating Count over the years', \n                   xaxis = dict(title = 'Year'), \n                   yaxis = dict(title = 'User Rating Count'), \n                   template= \"plotly_dark\")\n\nfig = go.Figure(data = [trace1, trace2], layout=layout)\nfig.show()","e3692c8c":"from wordcloud import WordCloud\n\n\nfig, ax = plt.subplots(1, 6, figsize=(16,32))\n\nwordcloud_name = WordCloud(background_color='white',width=800, height=800).generate(' '.join(df['Name']))\nwordcloud_sub = WordCloud(background_color='white',width=800, height=800).generate(' '.join(df['Subtitle'].dropna().astype(str)))\nwordcloud_lang = WordCloud(background_color='white',width=800, height=800).generate(' '.join(df['Languages']))\nwordcloud_genre = WordCloud(background_color='white',width=800, height=800).generate(' '.join(df['Genres']))\nwordcloud_descr = WordCloud(background_color='white',width=800, height=800).generate(' '.join(df['Description']))\nwordcloud_dev = WordCloud(background_color='white',width=800, height=800).generate(' '.join(df['Developer']))\n\n\nax[0].imshow(wordcloud_name)\nax[0].axis('off')\nax[0].set_title('Names')\n\nax[1].imshow(wordcloud_sub)\nax[1].axis('off')\nax[1].set_title('Subtitles')\n\nax[2].imshow(wordcloud_lang)\nax[2].axis('off')\nax[2].set_title('Languges')\n\nax[3].imshow(wordcloud_sub)\nax[3].axis('off')\nax[3].set_title('Genres')\n\nax[4].imshow(wordcloud_descr)\nax[4].axis('off')\nax[4].set_title('Description')\n\nax[5].imshow(wordcloud_dev)\nax[5].axis('off')\nax[5].set_title('Developer')\n\nplt.show()","29a522c5":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n\ndf_permutation = df.copy()\n\ndrop = ['ID', 'Icon URL', 'URL', 'Subtitle', 'Languages', 'Description',\n       'Original Release Date', 'Current Version Release Date', 'Genres',]\n\ndf_permutation.drop(drop, axis=1, inplace=True)\n\ntarget = ['Average User Rating']\npredictors = list(set(list(df_permutation.columns)) - set(target))\n\n\ndef encoding(df, columns):\n    for j in columns:\n        el_type = df[j].dtype\n        if el_type == 'object':\n            labelencoder = LabelEncoder()\n            df.loc[:, j] = labelencoder.fit_transform(df.loc[:, j])\n        else:\n            df.loc[:, j] = df.loc[:, j]**0.5 \n    return df, columns\n\nencoding(df_permutation, df_permutation.columns)\n\n\nX = df_permutation[predictors]\ny = df_permutation[target]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","899ca80f":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\ndef permutation(X_train, X_test, y_train, y_test, alg):\n    model = alg(n_jobs=-1, random_state=0).fit(X_train, y_train)\n    perm = PermutationImportance(model, random_state=0).fit(X_test, y_test)\n    return eli5.show_weights(perm, feature_names = X_test.columns.tolist())","cc6e1930":"from lightgbm import LGBMRegressor\npermutation(X_train, X_test, y_train, y_test, LGBMRegressor)","71ec6b6e":"from sklearn.ensemble import RandomForestRegressor\npermutation(X_train, X_test, y_train, y_test, RandomForestRegressor)","54c1000f":"from matplotlib import pyplot as plt\nfrom pdpbox import pdp\nfrom lightgbm import LGBMRegressor\n\n\n\n\ndf_permutation_1 = df.copy()\n\ndrop = ['ID', 'Icon URL', 'URL', 'Subtitle', 'Languages', 'Description',\n       'Original Release Date', 'Current Version Release Date', 'Genres']\n\ndf_permutation_1.drop(drop, axis=1, inplace=True)\n\ntarget = ['Average User Rating']\npredictors = list(set(list(df_permutation_1.columns)) - set(target))\n\n\ndef encoding(df, columns):\n    for j in columns:\n        el_type = df[j].dtype\n        if el_type == 'object':\n            labelencoder = LabelEncoder()\n            df.loc[:, j] = labelencoder.fit_transform(df.loc[:, j])\n        else:\n            df.loc[:, j] = df.loc[:, j] \n    return df, columns\n\nencoding(df_permutation_1, df_permutation_1.columns)\n\n\nX_1 = df_permutation_1[predictors]\ny_1 = df_permutation_1[target]\nX_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_1, y_1, test_size=0.2, random_state=0)\n\n\nmodel = LGBMRegressor(random_state=0).fit(X_train_1, y_train_1)\n\n\n\n# research_for_features = list(X_1.columns)\n\nresearch_for_features = ['User Rating Count', \n                         'Price', \n                         'In-app Purchases', \n                         'num_years_unsupported', \n                         'age', \n                         'Languages_score', \n                         'Size', \n                         'Age Rating', \n                         'Developer', \n                         'Name', \n                         \n                         'Lifestyle', \n                         'Music', \n                         'Casual', \n                         'Family', \n                         'Puzzle', \n                         'Racing', \n                         'Social Networking', \n                         'Utilities', \n                         'Adventure', \n                         'Entertainment', \n                         'Board', \n                         'Word', \n                         'Card', \n                         'Travel', \n                         'Sports', \n                         'Simulation', \n                         'Role Playing', \n                         ]\n\nfor feature in research_for_features:\n    pdp_dist = pdp.pdp_isolate(model=model,\n                               dataset=X_test_1,\n                               model_features=X_test_1.columns, \n                               feature=feature)\n\n    pdp.pdp_plot(pdp_dist, feature)\n    plt.show()","0515f4b9":"import shap\nfrom xgboost import XGBRegressor\n\n\nmodel = XGBRegressor(random_state=0).fit(X_train, y_train)\n\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_test)\n\nshap.summary_plot(shap_values, X_test)","c3353c8a":"import random\nfrom sklearn.ensemble import RandomForestRegressor\n\n\n\nmy_model = RandomForestRegressor(n_estimators=30, random_state=1).fit(X_train, y_train)\nexplainer = shap.TreeExplainer(my_model)\n\n\n\n\ndata_1 = pd.concat([X, y], axis=1)\nrandomlist = random.sample(range(0, len(data_1)), 1000)\nwork_indeces = [data_1.index[i] for i in randomlist]\ndata_1 = data_1.iloc[work_indeces,:]\n\n\nshap_values = explainer.shap_values(data_1)\n\n# research_for_features = list(X.columns)\nresearch_for_features = ['User Rating Count', \n                         'Price', \n                         'In-app Purchases', \n                         'num_years_unsupported', \n                         'age', \n                         'Languages_score', \n                         'Size', \n                         'Age Rating', \n                         'Developer', \n                         'Name', \n                         \n                         'Lifestyle', \n                         'Casual', \n                         'Family', \n                         'Puzzle', \n                         'Music', \n                         'Social Networking', \n                         'Racing',  \n                         ]\n\n\nfor feature in research_for_features:\n    shap.dependence_plot(feature, shap_values, data_1, interaction_index='Average User Rating')\n","3ceb9117":"shap.dependence_plot('num_years_unsupported', shap_values, data_1, interaction_index='age')","f5ae57d1":"def predict(nan_predict, df, fraction_NaN, feature):\n    import random\n    \n    randomlist = random.sample(range(0, len(df_1)), int(len(df_1)*fraction_NaN))\n    indexx = [df_1.index[i] for i in randomlist]\n    for idx in indexx:\n        df_1.loc[idx,feature] = np.nan\n\n    return nan_predict(df_1)","dec3d2b1":"%%time\n\n\ndf_1 = df.copy()\ndrop = ['ID', 'Icon URL', 'URL', 'Subtitle', 'Languages', 'Description',\n        'Original Release Date', 'Current Version Release Date', 'Genres', \n        'User Rating Count']\ndf_1.drop(drop, axis=1, inplace=True)\n\npredict(nan_predict, df, 0.2, 'Average User Rating')","78b0ed2c":"df_1 = df.loc[((df['Average User Rating'] >= 5) & (df['User Rating Count'] > 100))]\ndf_1[['Name', 'Developer', 'Average User Rating', 'User Rating Count']]\n","d418459d":"for el in list(df_1.columns):\n    print(f'======================= {el} =======================')\n    print(df_1[el].value_counts(dropna=False))\n    print('')","d902c9c2":"from wordcloud import WordCloud\n\n\nfig, ax = plt.subplots(1, 6, figsize=(16,32))\n\nwordcloud_name = WordCloud(background_color='white',width=800, height=800).generate(' '.join(df_1['Name']))\nwordcloud_sub = WordCloud(background_color='white',width=800, height=800).generate(' '.join(df_1['Subtitle'].dropna().astype(str)))\nwordcloud_lang = WordCloud(background_color='white',width=800, height=800).generate(' '.join(df_1['Languages']))\nwordcloud_genre = WordCloud(background_color='white',width=800, height=800).generate(' '.join(df_1['Genres']))\nwordcloud_descr = WordCloud(background_color='white',width=800, height=800).generate(' '.join(df_1['Description']))\nwordcloud_dev = WordCloud(background_color='white',width=800, height=800).generate(' '.join(df_1['Developer']))\n\n\nax[0].imshow(wordcloud_name)\nax[0].axis('off')\nax[0].set_title('Names')\n\nax[1].imshow(wordcloud_sub)\nax[1].axis('off')\nax[1].set_title('Subtitles')\n\nax[2].imshow(wordcloud_lang)\nax[2].axis('off')\nax[2].set_title('Languges')\n\nax[3].imshow(wordcloud_sub)\nax[3].axis('off')\nax[3].set_title('Genres')\n\nax[4].imshow(wordcloud_descr)\nax[4].axis('off')\nax[4].set_title('Description')\n\nax[5].imshow(wordcloud_dev)\nax[5].axis('off')\nax[5].set_title('Developer')\n\nplt.show()","0a3ef4da":"temp_df1 = df_1.groupby(df_1['Original Release Date'].dt.year).mean().reset_index()\n\ntrace1 = go.Scatter(\n                    x=temp_df1['Original Release Date'], \n                    y=temp_df1['User Rating Count'], \n                    name=\"Original Release\", \n                    marker=dict(color = '#FFB3F7',\n                             line=dict(color='#000000',width=1)))\n\n\n\ntemp_df2 = df_1.groupby(df_1['Current Version Release Date'].dt.year).mean().reset_index()\n\ntrace2 = go.Scatter(\n                    x=temp_df2['Current Version Release Date'], \n                    y=temp_df2['User Rating Count'], \n                    name=\"Last Version\", \n                    marker=dict(color = '#47E0FF',\n                             line=dict(color='#000000',width=1)))\n\n\n\nlayout = go.Layout(hovermode='closest', \n                   title = 'User Rating Count over the years for TOP 5% DEVELOPERS', \n                   xaxis = dict(title = 'Year'), \n                   yaxis = dict(title = 'User Rating Count'), \n                   template= \"plotly_dark\")\n\nfig = go.Figure(data = [trace1, trace2], layout=layout)\nfig.show()","11f2d562":"temp_df1 = df_1.groupby('Developer').count().reset_index().sort_values('Name',ascending=False).head(10)\n\n\ntrace1 = go.Bar(\n                x = temp_df1['Developer'],\n                y = temp_df1['Name'],\n                marker = dict(color = '#61F2AE',\n                             line=dict(color='#000000',width=1)))\n\nlayout = go.Layout(template= \"plotly_dark\",\n                   title = '10 Best Developers & Number Their Best Products (FOR THIS DATA \ud83d\ude01)', \n                   xaxis = dict(title = 'DEVELOPER',tickangle=30), \n                   yaxis = dict(title = 'COUNT'))\nfig = go.Figure(data = [trace1], layout = layout)\nfig.show()","d1bdbc22":"### Create bool values for every genre (create new 36 columns)","2399c147":"### The most popular words in:","464f2c16":"### RandomForestRegressor","3ea0d9da":"### Best Genres and their comparative impact rating\n* Puzzle - 0.15\n* Lifestyle - 0.11\n* Music - 0.06\n* Casual - 0.05\n* Family - 0.03","188494ca":"# Influence Original Release Date & Current Version Release Date on Average User Rating","acb2bff2":"\n### Conclusion\nAge of the app and regular(every year) Version Release increase the User Rating Count","fc466d5b":"### Create new column 'Languages_score'\n\nEnglish => 20 points   \nOther => 1 point","f1551018":"### drop duplicate data","a652dcbd":"### 'age' & 'num_years_unsupported' (normalized)","3b775237":"### Average User Rating","df1a4413":"# Conclusion\n\ndefault LGBM (without tuning hyperparameters but with normalization target feature)\n\nfirst 10 y__test: [4.5, 4.0, 4.5, 4.5, 2.5, 4.0, 2.5, 5.0, 4.5, 4.5]   \nfirst 10 y_pred: [4.0, 3.8, 4.4, 4.2, 3.5, 4.2, 3.0, 4.4, 3.6, 4.2]   \n \nmean_absolute_error: 0.4972680371168059   \nmean for Average User Rating: 4.066278665740097     \n\nNice result!   \nWe can predict Average User Rating","f6ae0818":"# EDA","3e303bd2":"### Size => Mbytes(int)","57d6ac85":"### drop useless features","38d625ae":"# Bonus for investors \ud83d\ude01","02a8ca5f":"### High values of these features have positive impact on increasing Average User Rating\n* User Rating Count\n* Price\n* In-app Purchases (the ability to make purchases in the apps)","7c0d0803":"### Age Rating => numeric","0187765b":"# Can I improve the average rating of my apps?\n# Sure thing! Model isn't a black box!","60f47645":"# Influence Original Release Date & Current Version Release Date on User Rating Count","d9f7edab":"### LGBMRegressor","3e2357dd":"### Low values of these features have positive impact on increasing Average User Rating\n* Age Rating\n* Languages (but languges increas User Rating Count and It increase Average User Rating)\n* Size\n* age (new apps better)\n* num_years_unsupported (regular new versions increase Average User Rating)\n* Age Rating","6e411c06":"### Dependence Contribution Plot","0341d1a6":"# Total Conclusion","93225a8c":"### create new column 'age'","14f0bb31":"# Top 5% in !!! THIS DATA\n\n### WHERE\n### Average User Rating = 5   \n### User Rating Count > 100","47b7d1d9":"### Primary Genre == Games, drop other","0ba15879":"# Conclusion\n\nThe most influential features are at the top.   \nThe influence decreases from top to bottom.","6d638117":"### Conclusion:  \nUnsupporting lowers app rating","a1bbfc93":"### Missing data","24f5f50c":"# EDA for target = 'Average User Rating'","6645d1d1":"### Summary plot","bb3e6406":"### How does the number of languages affect the rating?","db6e0fdd":"# Permutation Importance\n### !!! ALL numeric features have been normalized  (x**0.5)","7f4499fa":"# Data cleaning","79a16299":"# Partial Plots\n### All numeric features have absolute values (without normalization).","2c6375f7":"### Dependence between:   \n1) 'num_years_unsupported' & 'Average User Rating'    \n2) 'num_years_unsupported' & 'User Rating Count'   ","810276cb":"# Preprocessing & Feature Engineering","982cec25":"## Conclusion:  \nGenerally:   \nage of apps lowers average rating   \nand increases count rating   \n\nSure if there is support for old product then it can have a high rating.  \n### And I expect to see interaction between these features in SHAP\n\nFor exaple: \n","ddf367b9":"### create new column 'num_years_unsupported'","2758aab4":"### 'In-app Purchases' => bool","9d9a3174":"### drop non-Strategy game","6dd9a589":"# Prediction Average User Rating\n\n### !!! we should drop 'User Rating Count' because it provide to target leakage","26ff18bc":"# SHAP\n### !!! ALL numeric features have been normalized (x**0.5)","ac8cbc92":"### Dependence between:   \n1) 'age' & 'Average User Rating'    \n2) 'age' & 'User Rating Count'   "}}