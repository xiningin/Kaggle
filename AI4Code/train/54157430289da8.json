{"cell_type":{"141eb842":"code","e5802663":"code","3d13937f":"code","35b764b3":"code","532dbb3d":"code","4e637d6c":"code","8daea676":"code","842c18d6":"code","409a7c99":"code","b9c6f595":"code","15902a5b":"code","cd0e0f79":"code","ec52f10b":"code","e15eaccc":"code","fbb439b4":"code","8eba5b9a":"code","53f2eac6":"code","e355f523":"code","b0623f48":"code","fa58e1f5":"code","8034af14":"code","1f5af73b":"code","9c8882ba":"code","e9955337":"code","99ce074d":"code","3e3b6f8f":"code","3096e1b7":"code","c11e62c1":"code","43a15007":"code","4ea13d91":"code","5c547403":"code","5178ec2c":"code","1b1137bd":"code","f0c35442":"code","62cf512b":"code","9e1fce83":"code","af82a5e5":"code","fe891e2d":"code","7bffa1c1":"code","d0cd3a16":"code","91a4649e":"code","ce5cc56b":"code","b5ce7ff4":"code","6fdbe576":"code","334b21d6":"code","7f7c388d":"code","e696fbe1":"code","900f7c82":"code","a759ffd2":"code","849ed25a":"code","221b13fc":"code","d914bc73":"code","18aa9c29":"code","051a8570":"code","b2e983ee":"code","25e19d5c":"code","3425af5a":"code","eba82ef5":"code","dcce2659":"code","1a8d1014":"code","537db6e4":"code","639295a7":"code","ad086ba1":"code","60505c7d":"code","0aeca75b":"code","c60fea11":"code","0564c31e":"code","3ef0d0b1":"code","d63552c7":"code","03b28f44":"code","2055189a":"code","ac6954cd":"code","05975050":"code","176186c4":"code","df0e5fe9":"code","9896dc67":"code","9a74a5b8":"code","949c0cfd":"code","fcaee48d":"code","5170e7a1":"code","5be9b1a9":"code","1e2c915f":"code","47d0c1c4":"code","6efddf9d":"code","a550ff5f":"code","c28e156b":"code","a8cae97b":"code","a2fd51ce":"code","0885e05f":"code","944e0af0":"code","2e5189d8":"code","7d81557d":"code","4ff1910c":"code","5b0b1815":"code","ff04e7ce":"code","d806ec84":"code","d8aada14":"code","834f4569":"code","e66c988a":"code","06d411fb":"code","044d1987":"code","68e50bbc":"code","b4bf5826":"code","e7a76c90":"code","f626a18b":"code","af5ed84b":"code","006312ec":"code","1c181e34":"code","23ca7921":"code","7ce9b214":"code","9c7ef52d":"code","26b03c82":"code","a3afcc6c":"code","7ef0e35a":"code","f65941bc":"code","922ece89":"code","2020eefd":"code","73b920b3":"code","b92947d2":"code","e642c64d":"code","220ee431":"code","9b235f53":"code","a0833b46":"code","2c2c2542":"code","835a8f1c":"code","9d0c5437":"code","f74e9f36":"code","20efb0c2":"code","43ae89c8":"code","54f91bfd":"code","468d827e":"code","77d16ff9":"code","0fa0dd00":"code","461f7de1":"code","a43a657d":"code","681873f2":"code","56944b8d":"markdown","a54a5e84":"markdown","f2283aaf":"markdown","7792d3ef":"markdown","a6aed269":"markdown","8cbe9eae":"markdown","18eea39d":"markdown","4de60dee":"markdown","7de273bf":"markdown","d11a058b":"markdown","d31af167":"markdown","16707ae0":"markdown","262b3663":"markdown","078992ce":"markdown","7e879dde":"markdown","ea122a05":"markdown","d9856a3c":"markdown","d40cd7aa":"markdown","bde33709":"markdown","fb58e6d9":"markdown","5fa484fe":"markdown","57c3332d":"markdown","0759d1b3":"markdown","c35e35f0":"markdown","e3c88cbe":"markdown","2d5040b3":"markdown","abdda0ad":"markdown","4bb53285":"markdown","cc7ec928":"markdown","df7d8991":"markdown","841b16c6":"markdown","bead6aa9":"markdown","c3a457e9":"markdown","b60c4af9":"markdown","47eb17af":"markdown","a0442ce9":"markdown","3bc1338e":"markdown","f8a78dea":"markdown","a0d92f9a":"markdown","0581257e":"markdown","ed1d6aa5":"markdown","d67aac07":"markdown","18a6ddb8":"markdown","1552f535":"markdown","f73d3413":"markdown","8898b3fe":"markdown","d00a623a":"markdown","61670b16":"markdown","30ad3864":"markdown","5f84d7e0":"markdown","91f8c9a1":"markdown","faab3451":"markdown"},"source":{"141eb842":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom pandas import DataFrame\nfrom pandas.plotting import scatter_matrix\nfrom pandas_profiling import ProfileReport\nimport statsmodels.api\nimport statsmodels.api as smt\nfrom sklearn.feature_selection import SelectKBest, chi2, f_regression\nimport warnings\nwarnings.filterwarnings(\"ignore\")","e5802663":"dbts= pd.read_csv(\"E:\\\\python\\\\Internship_Exposys\\diabetes_data.csv\")","3d13937f":"dbts.dtypes","35b764b3":"dbts.shape","532dbb3d":"dbts.head()","4e637d6c":"dbts.tail()","8daea676":"dbts.info()","842c18d6":"dbts.describe()","409a7c99":"dbts.isnull()","b9c6f595":"dbts.isnull().sum()","15902a5b":"dbts.isnull().nunique()","cd0e0f79":"dbts.value_counts()","ec52f10b":"dbts['class'].value_counts()","e15eaccc":"dbts.columns","fbb439b4":"dbts.nunique()","8eba5b9a":"dbts['Age'].mode()","53f2eac6":"dbts['Age'].mean()","e355f523":"dbts['Polyuria'].unique()","b0623f48":"dbts['Gender'].unique()","fa58e1f5":"dbts['Polydipsia'].unique()","8034af14":"dbts['Age'].unique()","1f5af73b":"dbts['muscle stiffness'].unique()","9c8882ba":"dbts['Alopecia'].unique()","e9955337":"dbts['partial paresis'].unique()","99ce074d":"age = dbts[dbts['Age'] >= 58]\nage","3e3b6f8f":"age1 = dbts[dbts['Age'] == 35]\nage1","3096e1b7":"age2 = dbts[dbts['Age'] >= 48]\nage2","c11e62c1":"age3 = dbts[dbts['Age'] >= 71]\nage3","43a15007":"age4 = dbts[dbts['Age'] >= 90]\nage4","4ea13d91":"# converting text in values for better clarity and find the corelation.","5c547403":"dbts['Gender']                 = dbts['Gender'].map({'Male':1,'Female':0})\ndbts['class']                  = dbts['class'].map({'Positive':1,'Negative':0})\ndbts['Polyuria']               = dbts['Polyuria'].map({'Yes':1,'No':0})\ndbts['Polydipsia']             = dbts['Polydipsia'].map({'Yes':1,'No':0})\ndbts['sudden weight loss']     = dbts['sudden weight loss'].map({'Yes':1,'No':0})\ndbts['weakness']               = dbts['weakness'].map({'Yes':1,'No':0})\ndbts['Polyphagia']             = dbts['Polyphagia'].map({'Yes':1,'No':0})\ndbts['Genital thrush']         = dbts['Genital thrush'].map({'Yes':1,'No':0})\ndbts['visual blurring']        = dbts['visual blurring'].map({'Yes':1,'No':0})\ndbts['Itching']                = dbts['Itching'].map({'Yes':1,'No':0})\ndbts['Irritability']           = dbts['Irritability'].map({'Yes':1,'No':0})\ndbts['delayed healing']        = dbts['delayed healing'].map({'Yes':1,'No':0})\ndbts['partial paresis']        = dbts['partial paresis'].map({'Yes':1,'No':0})\ndbts['muscle stiffness']       = dbts['muscle stiffness'].map({'Yes':1,'No':0})\ndbts['Alopecia']               = dbts['Alopecia'].map({'Yes':1,'No':0})\ndbts['Obesity']                = dbts['Obesity'].map({'Yes':1,'No':0})","5178ec2c":"corelation= dbts.corr()","1b1137bd":"fig, ax = plt.subplots(figsize=(20,20))\nsns.heatmap(corelation, xticklabels = corelation.columns, yticklabels = corelation.columns, annot = True)","f0c35442":"sns.boxplot(x='class', y='Age', data= dbts)","62cf512b":"sns.barplot(x='class',y='Age', data=dbts)","9e1fce83":"sns.histplot(dbts['Age'],bins=25)\nplt.figure(figsize=(5,8)) ","af82a5e5":"plt.hist(dbts['Age'], bins=25, density=True, alpha=0.5)\nplt.figure(figsize=(5,6)) \nplt.show()","fe891e2d":"sns.displot(age.weakness)\nplt.figure(figsize=(5,8)) ","7bffa1c1":"sns.countplot(x='class',data=dbts,hue='Gender')","d0cd3a16":"sns.boxplot(x='Polydipsia',y='class',data=dbts)\nplt.figure(figsize=(5,8)) ","91a4649e":"sns.barplot(x='Polydipsia',y='class',data=dbts)\nplt.figure(figsize=(5,8)) ","ce5cc56b":"sns.barplot(x='Polyuria',y='class',data=dbts)\nplt.figure(figsize=(5,8)) ","b5ce7ff4":"sns.barplot(x='sudden weight loss',y='class',data=dbts)\nplt.figure(figsize=(5,8)) ","6fdbe576":"sns.countplot(x='class',hue='sudden weight loss', data=dbts, color='Brown')\nplt.figure(figsize=(5,8)) ","334b21d6":"sns.barplot(x='weakness',y='class',data=dbts)\nplt.figure(figsize=(5,8)) ","7f7c388d":"sns.countplot(x='class',hue='Polyphagia', data=dbts)\nplt.figure(figsize=(5,8)) ","e696fbe1":"sns.countplot(x='class',hue='Genital thrush', data=dbts, color= 'Green')\nplt.figure(figsize=(5,8)) ","900f7c82":"sns.boxplot(x='class',y='Genital thrush', data=dbts)\nplt.figure(figsize=(5,8)) ","a759ffd2":"sns.catplot(x='class',y='Genital thrush', kind= 'point', data=dbts, color='Red')\nplt.figure(figsize=(8,10)) ","849ed25a":"sns.countplot(x='class',hue='visual blurring', data=dbts, color='Purple')\nplt.figure(figsize=(5,8)) ","221b13fc":"sns.barplot(y='visual blurring',x='class',data=dbts)\nplt.figure(figsize=(5,8)) ","d914bc73":"sns.barplot(y='Itching',x='class',data=dbts)\nplt.figure(figsize=(5,8)) ","18aa9c29":"sns.catplot(x='class',y='Itching', kind= 'point', data=dbts, color='Red')\nplt.figure(figsize=(8,10)) ","051a8570":"sns.barplot(x='Irritability',y='class',data=dbts)\nplt.figure(figsize=(5,8)) ","b2e983ee":"sns.barplot(x='delayed healing',y='class',data=dbts)\nplt.figure(figsize=(5,8)) ","25e19d5c":"sns.barplot(x='partial paresis',y='class',data=dbts)\nplt.figure(figsize=(5,8)) ","3425af5a":"sns.barplot(x='muscle stiffness',y='class',data=dbts)\nplt.figure(figsize=(5,8)) ","eba82ef5":"sns.barplot(x='Alopecia',y='class',data=dbts)\nplt.figure(figsize=(5,8)) ","dcce2659":"sns.barplot(x='Obesity',y='class',data=dbts)\nplt.figure(figsize=(5,8)) ","1a8d1014":"dbts.hist()\nplt.show()","537db6e4":"scatter_matrix(dbts)\nplt.figure(figsize=(80,100))\nplt.show()","639295a7":"def plotPie(value, title, label):\n    plt.figure(figsize=(4,4))\n    plt.pie(\n        value.value_counts(),\n        startangle=90,\n        labels = label,\n        autopct=(lambda p:f'{p:.2f}%\\n{p*sum(value.value_counts())\/100 :.0f} items')\n    )\n    plt.title(title)\n    plt.show()\n    ","ad086ba1":"dbts_symptoms = dbts[dbts.columns.difference([\"Age\", \"class\", \"Gender\"])]\nplt.figure(figsize=(5,8))\n\nfor column in dbts_symptoms.columns:\n    plotPie(dbts_symptoms[column], column.capitalize(),{'Yes':1,'No':0})","60505c7d":"profile = ProfileReport(dbts, title=\"Diabetes Profiling Report\", explorative=True)\nprofile","0aeca75b":"profile.to_widgets()","c60fea11":"X1 = dbts.iloc[:,0:-1]\ny1 = dbts.iloc[:,-1]","0564c31e":"best_feature = SelectKBest(score_func=chi2,k=10) #chi2= For classification\nfit = best_feature.fit(X1,y1)","3ef0d0b1":"dbts_scores = pd.DataFrame(fit.scores_)\ndbts_cols = pd.DataFrame(X1.columns)","d63552c7":"featurescores = pd.concat([dbts_cols,dbts_scores],axis=1)\nfeaturescores.columns=['column','scores']","03b28f44":"print(X1[:10])","2055189a":"featurescores","ac6954cd":"X1_new=SelectKBest(score_func=f_regression, k=10).fit_transform(X1,y1)   #f_regression- For regression","05975050":"X1_new=dbts.iloc[:1]\ny1_new=dbts.iloc[:,2]\nprint(X1_new[:10])","176186c4":"featurescores","df0e5fe9":"print(featurescores.nlargest(10,'scores'))","9896dc67":"featureview=pd.Series(fit.scores_, index=X1.columns)\nfeatureview.plot(kind='bar')","9a74a5b8":"X = dbts[['Polydipsia','sudden weight loss','partial paresis','Irritability','Polyphagia','Age','visual blurring']]\ny = dbts['class']","949c0cfd":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state=0)","fcaee48d":"from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nX_train = ss.fit_transform(X_train)\nX_test = ss.transform(X_test)","5170e7a1":"from sklearn.neighbors import KNeighborsClassifier\n\n\ntest_scores = []\ntrain_scores = []\n\nfor i in range(1,20):\n\n    knn = KNeighborsClassifier(i)\n    knn.fit(X_train,y_train)\n    \n    train_scores.append(knn.score(X_train,y_train))\n    test_scores.append(knn.score(X_test,y_test))","5be9b1a9":"max_train_score = max(train_scores)\ntrain_scores_ind = [i for i, v in enumerate(train_scores) if v == max_train_score]\nprint('Max train score {} % and k = {}'.format(max_train_score*100,list(map(lambda x: x+1, train_scores_ind))))","1e2c915f":"max_test_score = max(test_scores)\ntest_scores_ind = [i for i, v in enumerate(test_scores) if v == max_test_score]\nprint('Max test score {} % and k = {}'.format(max_test_score*100,list(map(lambda x: x+1, test_scores_ind))))","47d0c1c4":"knn = KNeighborsClassifier(13)\n\nknn.fit(X_train,y_train)\nknn.score(X_test,y_test)","6efddf9d":"from sklearn.metrics import confusion_matrix\n#let us get the predictions using the classifier we had fit above\ny_pred = knn.predict(X_test)\nconfusion_matrix(y_test,y_pred)\npd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)","a550ff5f":"y_pred = knn.predict(X_test)\nfrom sklearn import metrics\ncnf_matrix = metrics.confusion_matrix(y_test, y_pred)\np = sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","c28e156b":"from sklearn.metrics import classification_report  #import classification_report\nprint(classification_report(y_test,y_pred))","a8cae97b":"from sklearn.metrics import roc_curve                #roc-auc curve\ny_pred_proba = knn.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)","a2fd51ce":"plt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='Knn')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('Knn(n_neighbors=13) ROC curve')\nplt.show()","0885e05f":"from sklearn.metrics import roc_auc_score\nroc_auc_score(y_test,y_pred_proba)","944e0af0":"from sklearn.model_selection import GridSearchCV\n#In case of classifier like knn the parameter to be tuned is n_neighbors\nparam_grid = {'n_neighbors':np.arange(1,100)}\nknn = KNeighborsClassifier()\nknn_cv= GridSearchCV(knn,param_grid,cv=5)\nknn_cv.fit(X,y)\n\nprint(\"Best Score:\" + str(knn_cv.best_score_))\nprint(\"Best Parameters: \" + str(knn_cv.best_params_))","2e5189d8":"from sklearn.linear_model import LogisticRegression\n\nlg=LogisticRegression()\nlg.fit(X_train,y_train)","7d81557d":"pred1=lg.predict(X_test)","4ff1910c":"from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator=lg, X=X_train ,y=y_train,cv=10)\nprint(\"accuracy is {:.2f} %\".format(accuracies.mean()*100))\nprint(\"std is {:.2f} %\".format(accuracies.std()*100))","5b0b1815":"score = lg.score(X_test, y_test)                        #have a look into Score\nprint(score)","ff04e7ce":"print(confusion_matrix(pred1,y_test))                    #have a look into confusion matrix","d806ec84":"from sklearn.metrics import classification_report     #classification report\nprint(classification_report(pred1,y_test))","d8aada14":"grid={\"C\":np.logspace(1,100), \"penalty\":[\"l1\",\"l2\"]}     # l1 lasso l2 ridge\nlg=LogisticRegression()\nlg_cv=GridSearchCV(lg,grid,cv=5)\nlg_cv.fit(X_train,y_train)","834f4569":"print(\"Best Score:\" + str(lg_cv.best_score_))\nprint(\"Best Parameters: \" + str(lg_cv.best_params_))","e66c988a":"lg2=LogisticRegression(C=1,penalty=\"l2\")\nlg2.fit(X_train,y_train)\nprint(\"score\",lg2.score(X_test,y_test))","06d411fb":"roc_auc_score(pred1, y_test)","044d1987":"from sklearn.svm import SVC\nsv=SVC(kernel='linear',random_state=0)\nsv.fit(X_train,y_train)","68e50bbc":"from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator=sv, X=X_train ,y=y_train,cv=10)\nprint(\"accuracy is {:.2f} %\".format(accuracies.mean()*100))\nprint(\"std is {:.2f} %\".format(accuracies.std()*100))","b4bf5826":"pred2=sv.predict(X_test)","e7a76c90":"from sklearn.metrics import classification_report\nprint(classification_report(pred2,y_test))","f626a18b":"from sklearn.svm import SVC\nsvrf=SVC(kernel='rbf',random_state=0)\nsvrf.fit(X_train,y_train)","af5ed84b":"from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator=svrf, X=X_train ,y=y_train,cv=10)\nprint(\"accuracy is {:.2f} %\".format(accuracies.mean()*100))\nprint(\"std is {:.2f} %\".format(accuracies.std()*100))","006312ec":"pred2=svrf.predict(X_test)","1c181e34":"score = svrf.score(X_test, y_test)                        #have a look into Score\nprint(score)","23ca7921":"print(confusion_matrix(pred2,y_test))  ","7ce9b214":"from sklearn.metrics import classification_report\nprint(classification_report(pred2,y_test))","9c7ef52d":"roc_auc_score(pred2, y_test)","26b03c82":"from sklearn.tree import DecisionTreeClassifier\ndct=DecisionTreeClassifier(criterion='gini')\ndct.fit(X_train,y_train)","a3afcc6c":"from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator=dct, X=X_train ,y=y_train,cv=10)\nprint(\"accuracy is {:.2f} %\".format(accuracies.mean()*100))\nprint(\"std is {:.2f} %\".format(accuracies.std()*100))","7ef0e35a":"pred3=dct.predict(X_test)","f65941bc":"score = dct.score(X_test, y_test)                        #have a look into Score\nprint(score)","922ece89":"print(confusion_matrix(pred3,y_test)) ","2020eefd":"from sklearn.metrics import classification_report\nprint(classification_report(pred3,y_test))","73b920b3":"roc_auc_score(pred3, y_test)","b92947d2":"from sklearn.naive_bayes import GaussianNB\ngnb=GaussianNB()\ngnb.fit(X_train,y_train)","e642c64d":"from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator=gnb, X=X_train ,y=y_train,cv=10)\nprint(\"accuracy is {:.2f} %\".format(accuracies.mean()*100))\nprint(\"std is {:.2f} %\".format(accuracies.std()*100))","220ee431":"pred4=gnb.predict(X_test)","9b235f53":"score = gnb.score(X_test, y_test)                        #have a look into Score\nprint(score)","a0833b46":"print(confusion_matrix(pred4,y_test)) ","2c2c2542":"from sklearn.metrics import classification_report\nprint(classification_report(pred4,y_test))","835a8f1c":"roc_auc_score(pred4, y_test)","9d0c5437":"from sklearn.ensemble import RandomForestClassifier\nestime=[]\nfor i in range(1,100):\n    rf=RandomForestClassifier(n_estimators=i,criterion='entropy',random_state=0)\n    rf.fit(X_train,y_train)","f74e9f36":"from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator=rf, X=X_train ,y=y_train,cv=10)\nprint(\"accuracy is {:.2f} %\".format(accuracies.mean()*100))\nprint(\"std is {:.2f} %\".format(accuracies.std()*100))","20efb0c2":"pred5 = rf.predict(X_test)","43ae89c8":"score = rf.score(X_test, y_test)                        #have a look into Score\nprint(score)","54f91bfd":"print(confusion_matrix(pred5,y_test)) ","468d827e":"from sklearn.metrics import classification_report\nprint(classification_report(pred5,y_test))","77d16ff9":"roc_auc_score(pred5, y_test)","0fa0dd00":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n{'n_estimators': [25, 50], 'max_features': [5, 10], \n 'max_depth': [10, 50, None], 'bootstrap': [True, False]}\n]\n\ngrid_search_rf = GridSearchCV(rf, param_grid, cv=10, scoring='neg_mean_squared_error')\ngrid_search_rf.fit(X_train, y_train)","461f7de1":"cvres = grid_search_rf.cv_results_                     #now let's how the RMSE changes for each parameter configuration\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","a43a657d":"grid_search_rf.best_estimator_              #find the best model of grid search","681873f2":"RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, n_estimators=25, n_jobs=-1,\n                       oob_score=False, random_state=13, verbose=0,\n                       warm_start=False)\n","56944b8d":"Importing the libraries","a54a5e84":"# KnearestNeighbor","f2283aaf":"We will now read the data from a CSV file into a Pandas DataFrame","7792d3ef":"Feature selection using  classification and regression: ","a6aed269":"Hyper Parameter optimization\nGrid search is an approach to hyperparameter tuning that will methodically build and evaluate a model for each combination of algorithm parameters specified in a grid.","8cbe9eae":"Accuracies of all classification model overview:\n\n    \n1. KNN-                     98.07692307692307\n\n2. logistic Regression-     0.8942307692307693\n\n3. SVM-                     0.9807692307692307\n\n4. Decsion Tree-            0.9615384615384616\n\n5. Guassian NB-             0.8557692307692307\n\n6. Random Forest-           0.9807692307692307\n\n","18eea39d":"Corelation between independent and dependent variables considering independent variables that has high correlation with the dependent variables and less correlation with other variables","4de60dee":"# Profile report in brief","7de273bf":"Missing value imputation","d11a058b":"# Decision Tree","d31af167":"Max test score 98.07692307692307 % and k = [2, 3, 4, 5, 11, 13]","16707ae0":"Irritability:","262b3663":"Weakness:","078992ce":"Age vs class(dependent variable):","7e879dde":"Distribution of all variables:","ea122a05":"For numerical parameters, fields like mean, standard deviation, percentiles, and maximum have been populated. For categorical features, count, unique, and corresponding frequency have been populated. This gives us a broad idea of our dataset.","d9856a3c":"# EDA: Exploratory Data Analysis is a process of examining or understanding the data and extracting insights or main characteristics of the data. EDA is generally classified into two methods, i.e. graphical analysis and non-graphical analysis.","d40cd7aa":"Sudden Weight loss:","bde33709":"Descriptive Statistics","fb58e6d9":"X1 and X1_new both features are same from both methods, it means feature selection is carried out in right direction.","5fa484fe":"# Task 1:  Preparing the data-set","57c3332d":"Top 10 features:","0759d1b3":"Visual blurring:","c35e35f0":"These are the variables with their feature scores ,their importance\/contribution towards class","e3c88cbe":"Have a look on data and Data preprocessing ","2d5040b3":"muscle stiffness:","abdda0ad":"cross validation of score","4bb53285":"Polyuria:","cc7ec928":"Graphical Reprsentation:","df7d8991":"# Feature selection","841b16c6":"# Machine learning models","bead6aa9":"Itching:","c3a457e9":"# The best model is KNN, SVM and Random forest with 98.07% Accuracy","b60c4af9":"Diabetes is a type of chronic disease which is more common among the people of all age groups. \nPredicting this disease at an early stage can help a person to take the necessary actions to either prevent the occurrence of this disease or control the disease. \n\nMachine learning models helps in early detection and the diagnosis needed. Now predicting whether a person had diabetes or not by Data preparation, Data Pre processing, Exploratory Data Analysis, Feature selection techniques and Building models.\n\nTask:\n1. Prepare the data-set\n2. Build a model which can give high accuracy of predicting the disease. \n","47eb17af":"Polyphagia:","a0442ce9":"These are the variables with their feature scores ,their importance\/contribution towards class.","3bc1338e":"# Logistic Regression","f8a78dea":"Occurences of Symptoms in patients:","a0d92f9a":"Splitting diabetes(dbts) dataset into test and train:","0581257e":"# Early Detection of Diabetes ","ed1d6aa5":"# Random Forest","d67aac07":"partial paresis:","18a6ddb8":"Genital thrush:","1552f535":"Alopecia:","f73d3413":"# Guassian Naive Bayes","8898b3fe":"Age distribution:","d00a623a":"\nFeature selection is a technique where we choose those features in our data that contribute most to the target variable. In other words we choose the best predictors for the target variable. The classes in the sklearn.feature_selection module can be used for feature selection\/dimensionality reduction on sample sets, either to improve estimators accuracy scores or to boost their performance on very high-dimensional datasets.\n\nAdvantages:\nReduces Overfitting: Less redundant data means less possibility of making decisions based on redundant data\/noise.\nImproves Accuracy: Less misleading data means modeling accuracy improves.\nReduces Training Time: Less data means that algorithms train faster.\n","61670b16":"delayed healing:","30ad3864":"Gender:","5f84d7e0":"Polydipsia:","91f8c9a1":"Obesity:","faab3451":"# Support Vector Machine(SVM)"}}