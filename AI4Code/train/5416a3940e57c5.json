{"cell_type":{"8244c33a":"code","256e85ba":"code","e27fecfc":"code","3954eae5":"code","3adfb5c3":"code","66b5fdd6":"code","0fc51fdf":"code","e0e8f1bc":"code","229a6e41":"code","473751fa":"code","dcd7f8a4":"code","a1b2e1ea":"code","200f68d8":"code","f81e0bc0":"code","f6fbe1c6":"code","cce76122":"code","5efbcf4e":"code","0e374948":"code","95d433d0":"code","6b9ba093":"code","a6b68d4c":"code","6239c769":"code","1172ab2f":"code","254fc098":"code","fed88081":"code","abd6d05c":"code","d7933218":"code","10110588":"code","8ff844c2":"code","a5c89d25":"code","052ea5b6":"code","6e1c6d48":"code","a4112738":"code","280fb5f6":"code","13bfb27f":"code","c94c890d":"code","bf23238f":"code","a263b86b":"code","9a63d02d":"code","10ff56cd":"code","9075b76f":"code","ced86b64":"code","19e8121d":"code","caaf7ae3":"code","0038e5fe":"code","f285467d":"code","5da518b6":"code","7aefa810":"code","c1e2bbe2":"code","8bb6e13e":"code","b9cffcfd":"code","793f57f9":"code","d2f1143d":"code","df815766":"code","c9e1f476":"code","42b3dcfe":"code","813ceb67":"code","36bd4dc7":"code","5a9986a6":"code","978f7006":"code","c2b4c593":"code","707c8656":"code","73735b16":"code","7f6d21ec":"code","5ee9e26c":"code","04c46a46":"code","961e2eb2":"code","53ac85b0":"code","719adeb1":"code","6ecaadcd":"code","65d94a73":"code","43f04499":"code","7bd298d4":"code","767be20e":"code","4f2df5bd":"code","0cbceb5d":"code","5ce0fa46":"code","286d727b":"code","40d13c4f":"code","23db4452":"code","577fe4df":"code","66167820":"code","f8a10448":"code","6e443186":"code","661a7bc6":"code","7ff9a881":"code","f3de3d34":"code","10cc283c":"code","cb0340b1":"code","1edf2442":"code","44d8ad14":"code","0460a0db":"code","b52e2fb4":"code","575d0d63":"code","c81351ec":"code","861e9f02":"code","1da60112":"code","85de6c26":"code","50f38be5":"code","3ea1900d":"code","05426644":"code","9d6516a3":"code","9b011c42":"code","e6a7037e":"code","481b0f37":"code","3c7fb3f8":"code","20806ec4":"code","35ada25c":"code","bf58a22b":"code","e691eab4":"code","f0a4c56e":"code","b9b2ce51":"code","1fc87040":"markdown","c46d7b89":"markdown","50b28ccb":"markdown","fa886a70":"markdown","528662c0":"markdown","3348b10f":"markdown","72de3072":"markdown","1a850290":"markdown","a628d051":"markdown","3d7cf25d":"markdown","ce7e436c":"markdown","f68d0d3c":"markdown","52ed7d4b":"markdown","fec37157":"markdown","27bfb0b5":"markdown","07696d68":"markdown","bc643f73":"markdown","6ede7f04":"markdown","fdd81c0b":"markdown","b726a7d0":"markdown","c8207557":"markdown","13327ee8":"markdown","956850d9":"markdown","47da45ef":"markdown","00052797":"markdown","d16b9b03":"markdown","c7ff6386":"markdown","cf11bf97":"markdown","ab7a2f56":"markdown","445edbb0":"markdown","84842b05":"markdown","a88dc42b":"markdown","fa49351e":"markdown","9372a75a":"markdown","b0d8344d":"markdown","4bc0a73f":"markdown","f0b07c4d":"markdown","d9cef6f4":"markdown","66821a76":"markdown","d3d17d38":"markdown","4078ce66":"markdown","40ed5295":"markdown","fdec9ff0":"markdown","cbaf8b85":"markdown","6d7752b4":"markdown","b4aa9b6a":"markdown","44fd6eab":"markdown","3f389fb2":"markdown","004243e3":"markdown","61326bcc":"markdown","2307989f":"markdown","5f8ac193":"markdown","46139802":"markdown","d0ea0b3f":"markdown","fb0b14b8":"markdown","95aca90e":"markdown","19d0ea35":"markdown","5901b351":"markdown","df7559ce":"markdown","f78a3000":"markdown","8a981a3a":"markdown","a9e06a06":"markdown","2f42a166":"markdown","4e4a8a7d":"markdown","f999b890":"markdown","267b8a65":"markdown","cef165e9":"markdown"},"source":{"8244c33a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.impute import SimpleImputer\nfrom collections import Counter\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score,cross_val_predict\nfrom sklearn.metrics import precision_score,recall_score,f1_score\nfrom sklearn.metrics import precision_recall_curve\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","256e85ba":"train_data=pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data=pd.read_csv('..\/input\/titanic\/test.csv')","e27fecfc":"train_data.head(10)","3954eae5":"train_data.describe()","3adfb5c3":"train_data.isna().any()","66b5fdd6":"train_data.drop(['PassengerId','Ticket','Cabin'],axis=1,inplace=True)","0fc51fdf":"train_data['Embarked'].isna().value_counts()","e0e8f1bc":"train_data.isna().any()","229a6e41":"imp=SimpleImputer(strategy='most_frequent')\ntrain_data[['Embarked']]=imp.fit_transform(train_data[['Embarked']])","473751fa":"mean=train_data['Age'].mean()\ntrain_data['Age'].fillna(mean,inplace=True)","dcd7f8a4":"train_data['Age']=np.ceil(train_data['Age'])","a1b2e1ea":"def get_age(val):\n    age_classification={'Child':range(0,13),\n                        'Teen':range(13,21),\n                        'Young':range(21,31),\n                        '30_adult':range(31,41),\n                        '40_adult':range(41,51),\n                        'Elderly':range(51,90)}\n    for key,value in age_classification.items():\n        if val in value:\n            return key","200f68d8":"for i in range(len(train_data)):\n    train_data['Age'][i]=('{}'.format(get_age(train_data['Age'][i])))    ","f81e0bc0":"train_data['Name'].unique()","f6fbe1c6":"def get_title(val):\n    words=val.split()\n    title={'Officer':['Capt.','Col.','Major.','Dr.','Rev.'],\n          'Royalty':['Jonkheer.','Don.','Sir.','the Countess.','Lady.'],\n          'Mrs':['Mme.','Ms.','Mrs.'],\n          'Mr':['Mr.'],\n          'Miss':['Mlle.','Miss.'],\n          'Master':['Master.']}\n    for key,value in title.items():\n        for word in words:\n            if word in value:\n                return str(key)","cce76122":"train_data['Title']=np.NAN\nfor i in range(len(train_data)):\n    train_data['Title'][i]=get_title(train_data['Name'][i])","5efbcf4e":"train_data['Title'].isna().value_counts()","0e374948":"train_data['Fam_mem']=train_data['SibSp']+train_data['Parch']+1","95d433d0":"def fam_size(val):\n    fam={'Single':[1],\n        'Small_family':[2,3,4],\n        'large_family':[5,6,7,8,9,10,11]}\n    for key,value in fam.items():\n        if val in value:\n            return key","6b9ba093":"for i in range(len(train_data)):\n    train_data['Fam_mem'][i]=fam_size(train_data['Fam_mem'][i])","a6b68d4c":"train_data.head(10)","6239c769":"train_data.isna().sum()","1172ab2f":"def most_common(lst):\n    data=Counter(lst)\n    return data.most_common(1)[0][0]\nfrequent=most_common(train_data['Title'])","254fc098":"train_data['Title'].fillna(frequent,inplace=True)","fed88081":"train_data['Title'].unique()","abd6d05c":"train_data.head(10)","d7933218":"oe=OrdinalEncoder()\ntrain_data[['Embarked','Sex','Age','Title','Fam_mem']]=oe.fit_transform(train_data[['Embarked','Sex','Age','Title','Fam_mem']])","10110588":"train_data.head(10)","8ff844c2":"train_data.hist(bins=10,figsize=(20,15))","a5c89d25":"corr_mat=train_data.corr()\ncorr_mat['Survived'].sort_values(ascending=False)","052ea5b6":"attributes=['Survived','Fare','Embarked','Pclass','Sex']\nscatter_matrix(train_data[attributes],figsize=(15,10),alpha=0.1)","6e1c6d48":"plt.scatter(train_data.iloc[:,1],train_data.iloc[:,3],c=train_data.iloc[:,0],s=50,cmap='RdBu')","a4112738":"men_survived_truth=(((train_data['Sex']==1)&(train_data['Survived']==1)))\nmen_death_truth=(((train_data['Sex']==1)&(train_data['Survived']==0)))\nwomen_survived_truth=(((train_data['Sex']==0)&(train_data['Survived']==1)))\nwomen_death_truth=(((train_data['Sex']==0)&(train_data['Survived']==0)))\nmen_survived=men_survived_truth.value_counts()\nmen_death=men_death_truth.value_counts()\nwomen_survived=women_survived_truth.value_counts()\nwomen_death=women_death_truth.value_counts()\nmen=[men_survived[1],men_death[1]]\nwomen=[women_survived[1],women_death[1]]","280fb5f6":"men_ratio=[(men[0]\/(men[0]+men[1]))*100,(men[1]\/(men[0]+men[1]))*100]\nprint(men_ratio)\nwomen_ratio=[(women[0]\/(women[0]+women[1]))*100,(women[1]\/(women[0]+women[1]))*100]\nprint(women_ratio)","13bfb27f":"fig,ax=plt.subplots(1,2,figsize=(15,10))\nexplode=(0.1,0)\nax[0].pie(men_ratio,explode=explode,labels=['Survivors','Deaths'],autopct='%1.2f%%',shadow=True,startangle=90)\nax[0].set_title('Men Ratio')\nax[1].pie(women_ratio,explode=explode,labels=['Survivors','Deaths'],autopct='%1.2f%%',shadow=True,startangle=90)\nax[1].set_title('Women Ratio')","c94c890d":"men_survivors=[]\nmen_death=[]\nfor i in range(1,4):\n    Pclassmen_survived_truth=(((train_data['Sex']==1)&(train_data['Survived']==1)&(train_data['Pclass']==i)))\n    Pclassmen_death_truth=(((train_data['Sex']==1)&(train_data['Survived']==0)&(train_data['Pclass']==i)))\n    pclassmen_survivors=Pclassmen_survived_truth.value_counts()\n    pclassmen_deaths=Pclassmen_death_truth.value_counts()\n    men_survivors.append(pclassmen_survivors[1])\n    men_death.append(pclassmen_deaths[1])\nmen=[men_survivors,men_death]","bf23238f":"women_survivors=[]\nwomen_death=[]\nfor i in range(1,4):\n    Pclasswomen_survived_truth=(((train_data['Sex']==0)&(train_data['Survived']==1)&(train_data['Pclass']==i)))\n    Pclasswomen_death_truth=(((train_data['Sex']==0)&(train_data['Survived']==0)&(train_data['Pclass']==i)))\n    pclasswomen_survivors=Pclasswomen_survived_truth.value_counts()\n    pclasswomen_deaths=Pclasswomen_death_truth.value_counts()\n    women_survivors.append(pclasswomen_survivors[1])\n    women_death.append(pclasswomen_deaths[1])\nwomen=[women_survivors,women_death]","a263b86b":"print(men)\nprint(women)","9a63d02d":"fig,ax=plt.subplots(1,2,figsize=(15,10))\nsur_death=['Survived','Death']\nwidth=0.5\nfor i,axi in enumerate(ax.flat):\n    N=3\n    ind=[x for x in np.arange(1,N+1)]\n    axi.bar(ind,women[i],width,label='Women',bottom=men[i],color='Pink')\n    axi.bar(ind,men[i],width,label='Men',color='Blue')\n    axi.set_xticklabels(['0','Pclass 1','','Pclass 2','','Pclass 3'])\n    axi.set_title(sur_death[i])\n    axi.legend()","10ff56cd":"sns.pairplot(train_data)","9075b76f":"survivors=[]\ndeath=[]\nfor i in range(0,6):\n    title_survived_truth=(((train_data['Survived']==1)&(train_data['Title']==i)))\n    title_death_truth=(((train_data['Survived']==0)&(train_data['Title']==i)))\n    title_survived=title_survived_truth.value_counts()\n    title_death=title_death_truth.value_counts()\n    survivors.append(title_survived[1])\n    death.append(title_death[1])\ntitle_sur_death=[survivors,death]\ntitle_sur_death","ced86b64":"oe.categories_","19e8121d":"fig,ax=plt.subplots(1,2,figsize=(15,10))\nto_plot=[survivors,death]\nname=['Survivors','Deaths']\ncolor=['green','red']\nfor i,axi in enumerate(ax.flat):\n    N=6\n    ind=[x for x in np.arange(1,N+1)]\n    axi.bar(ind,to_plot[i],width,label=('{}'.format(name[i])),color=color[i])\n    axi.set_xticklabels(['0','Master','Miss','Mr','Mrs','Officer','Royalty'])\n    axi.set_title('{} on basis of Title'.format(name[i]))\n    axi.legend()","caaf7ae3":"survivors=[]\ndeath=[]\nfor i in range(0,6):\n    age_survived_truth=(((train_data['Survived']==1)&(train_data['Age']==i)))\n    age_death_truth=(((train_data['Survived']==0)&(train_data['Age']==i)))\n    age_survived=age_survived_truth.value_counts()\n    age_death=age_death_truth.value_counts()\n    survivors.append(age_survived[1])\n    death.append(age_death[1])\nage_sur_death=[survivors,death]\nage_sur_death","0038e5fe":"fig,ax=plt.subplots(1,2,figsize=(15,10))\nto_plot=[survivors,death]\nname=['Survivors','Deaths']\ncolor=['green','red']\nfor i,axi in enumerate(ax.flat):\n    N=6\n    ind=[x for x in np.arange(1,N+1)]\n    axi.bar(ind,to_plot[i],width,label=('{}'.format(name[i])),color=color[i])\n    axi.set_xticklabels(['0','30_adult','40_adult','Child','Elderly','Teen','Young'])\n    axi.set_title('{} on basis of Age'.format(name[i]))\n    axi.legend()","f285467d":"embarked_survivor=[]\nembarked_death=[]\nfor i in range(3):\n    embarked_survived_truth=(((train_data['Survived']==1)&(train_data['Embarked']==i)))\n    embarked_death_truth=(((train_data['Survived']==0)&(train_data['Embarked']==i)))\n    embarked_survivors=embarked_survived_truth.value_counts()\n    embarked_deaths=embarked_death_truth.value_counts()\n    embarked_survivor.append(embarked_survivors[1])\n    embarked_death.append(embarked_deaths[1])\nembarked=[embarked_survivor,embarked_death]\nembarked","5da518b6":"fig=plt.figure(figsize=(15,10))\nN=3\nind=[x for x in np.arange(1,N+1)]\nplt.bar(ind,embarked_survivor,width,label='Survived',bottom=embarked_death,color='Orange')\nplt.bar(ind,embarked_death,width,label='Death',color='cyan')\nplt.xticks(ind,['C','Q','S'])\nplt.title('Survive\/death on basis of Embarked')\nplt.legend()","7aefa810":"survivors=[]\ndeath=[]\nfor i in range(0,3):\n    fam_survived_truth=(((train_data['Survived']==1)&(train_data['Fam_mem']==i)))\n    fam_death_truth=(((train_data['Survived']==0)&(train_data['Fam_mem']==i)))\n    fam_survived=fam_survived_truth.value_counts()\n    fam_death=fam_death_truth.value_counts()\n    survivors.append(fam_survived[1])\n    death.append(fam_death[1])\nfam_sur_death=[survivors,death]\nfam_sur_death","c1e2bbe2":"sur_ratio=[(survivors[0]\/(survivors[0]+survivors[1]+survivors[2]))*100,(survivors[1]\/(survivors[0]+survivors[1]+survivors[2]))*100,\n          (survivors[2]\/(survivors[0]+survivors[1]+survivors[2]))*100]\ndeath_ratio=[(death[0]\/(death[0]+death[1]+death[2]))*100,(death[1]\/(death[0]+death[1]+death[2]))*100,\n            (death[2]\/(death[0]+death[1]+death[2]))*100]\nprint(sur_ratio)\nprint(death_ratio)","8bb6e13e":"fig,ax=plt.subplots(1,2,figsize=(15,10))\nexplode=(0.1,0.1,0.1)\nax[0].pie(sur_ratio,explode=explode,labels=['Single','Small_family','Large_family'],autopct='%1.2f%%',shadow=True,startangle=90)\nax[0].set_title('Survive Ratio')\nax[1].pie(death_ratio,explode=explode,labels=['Single','Small_family','Large_family'],autopct='%1.2f%%',shadow=True,startangle=90)\nax[1].set_title('Death Ratio')","b9cffcfd":"train_data.head(10)","793f57f9":"preprocess_data=train_data.copy()","d2f1143d":"preprocess_data=pd.get_dummies(preprocess_data,columns=['Pclass','Sex','Age','Embarked','Title','Fam_mem'])","df815766":"preprocess_data.head(10)","c9e1f476":"corr_mat=preprocess_data.corr()\ncorr_mat['Survived'].sort_values(ascending=False)","42b3dcfe":"preprocess_data.drop(['Age_0.0','Title_5.0','Embarked_1.0','Age_1.0','Age_4.0','Age_3.0','Title_4.0','SibSp','Age_5.0',\n                     'Parch','Title_0.0','Pclass_2','Name'],axis=1,inplace=True)","813ceb67":"preprocess_data.shape","36bd4dc7":"scale=StandardScaler()\npreprocess_data[['Fare']]=scale.fit_transform(preprocess_data[['Fare']])","5a9986a6":"preprocess_data.head()","978f7006":"preprocess_feature=preprocess_data.drop('Survived',axis=1)\npreprocess_label=preprocess_data['Survived']","c2b4c593":"preprocess_feature","707c8656":"preprocess_label","73735b16":"split=StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=42)\nfor train_index,test_index in split.split(preprocess_data,preprocess_data['Survived']):\n    strat_preprocess_data=preprocess_data.loc[train_index]\n    strat_preprocess_label=preprocess_data.loc[test_index]","7f6d21ec":"X_train=strat_preprocess_data.drop('Survived',axis=1)\ny_train=strat_preprocess_data['Survived']\nX_test=strat_preprocess_label.drop('Survived',axis=1)\ny_test=strat_preprocess_label['Survived']","5ee9e26c":"print(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","04c46a46":"tree=DecisionTreeClassifier(random_state=42,criterion='entropy',splitter='best')\ntree.fit(X_train,y_train)\ny_pred=tree.predict(X_test)\naccuracy_score(y_test,y_pred)","961e2eb2":"param_grid={'max_depth':[2,3,4,5,6]}\ngrid_tree=GridSearchCV(tree,param_grid,cv=5)\ngrid_tree.fit(X_train,y_train)\ny_pred=grid_tree.best_estimator_.predict(X_test)\nprint(accuracy_score(y_test,y_pred))\nprint(grid_tree.best_params_)","53ac85b0":"mat=confusion_matrix(y_test,y_pred)\nsns.heatmap(mat.T,square=True,annot=True,cbar=False)\nplt.xlabel('true label')\nplt.ylabel('predicted label')","719adeb1":"log_reg=LogisticRegression()\nlog_reg.fit(X_train,y_train)\ny_pred=log_reg.predict(X_test)\naccuracy_score(y_test,y_pred)","6ecaadcd":"param_grid={'C':[0.0001,0.001,0.01,0.1,0.5],'penalty':['l1','l2']}\ngrid_log=GridSearchCV(log_reg,param_grid,cv=5)\ngrid_log.fit(X_train,y_train)\ny_pred=grid_log.predict(X_test)\nprint(accuracy_score(y_test,y_pred))\nprint(grid_log.best_params_)","65d94a73":"mat=confusion_matrix(y_test,y_pred)\nsns.heatmap(mat.T,square=True,annot=True,cbar=False)\nplt.xlabel('true label')\nplt.ylabel('predicted label')","43f04499":"rfc=RandomForestClassifier(criterion='entropy',random_state=42)\nrfc.fit(X_train,y_train)\ny_pred=rfc.predict(X_test)\naccuracy_score(y_test,y_pred)","7bd298d4":"param_grid={'max_depth':[2,3,4,5,6],'n_estimators':[100,200,300,400,500]}\ngrid_forest=GridSearchCV(rfc,param_grid,cv=5)\ngrid_forest.fit(X_train,y_train)\ny_pred=grid_forest.predict(X_test)\nprint(accuracy_score(y_test,y_pred))\nprint(grid_forest.best_params_)","767be20e":"mat=confusion_matrix(y_test,y_pred)\nsns.heatmap(mat.T,square=True,annot=True,cbar=False,fmt='d')\nplt.xlabel('true label')\nplt.ylabel('predicted label')","4f2df5bd":"sgd_clf=SGDClassifier(random_state=42)\nsgd_clf.fit(X_train,y_train)\ny_pred=sgd_clf.predict(X_test)\naccuracy_score(y_test,y_pred)","0cbceb5d":"param_grid={'penalty':['l1','l2'],'alpha':[0.0001,0.001,0.01,0.1],'max_iter':[1000,1500,2000,2500]}\ngrid_sgd=GridSearchCV(sgd_clf,param_grid,cv=5)\ngrid_sgd.fit(X_train,y_train)\ny_pred=grid_sgd.predict(X_test)\nprint(accuracy_score(y_test,y_pred))\nprint(grid_sgd.best_params_)","5ce0fa46":"mat=confusion_matrix(y_test,y_pred)\nsns.heatmap(mat.T,square=True,annot=True,cbar=False)\nplt.xlabel('true label')\nplt.ylabel('predicted label')","286d727b":"svc_clf=SVC(kernel='rbf',random_state=42)\nsvc_clf.fit(X_train,y_train)\ny_pred=svc_clf.predict(X_test)\naccuracy_score(y_test,y_pred)","40d13c4f":"param_grid={'C':[1,2,3,4,5],'degree':[2,3,4,5,6],'gamma':['scale','auto']}\ngrid_svc=GridSearchCV(svc_clf,param_grid,cv=5)\ngrid_svc.fit(X_train,y_train)\ny_pred=grid_svc.predict(X_test)\nprint(accuracy_score(y_test,y_pred))\nprint(grid_svc.best_params_)","23db4452":"mat=confusion_matrix(y_test,y_pred)\nsns.heatmap(mat.T,square=True,annot=True,cbar=False)\nplt.xlabel('true label')\nplt.ylabel('predicted label')","577fe4df":"gnb=GaussianNB()\ngnb.fit(X_train,y_train)\ny_pred=gnb.predict(X_test)\naccuracy_score(y_test,y_pred)","66167820":"mat=confusion_matrix(y_test,y_pred)\nsns.heatmap(mat.T,square=True,annot=True,cbar=False)\nplt.xlabel('true label')\nplt.ylabel('predicted label')","f8a10448":"knn=KNeighborsClassifier()\nknn.fit(X_train,y_train)\ny_pred=knn.predict(X_test)\naccuracy_score(y_test,y_pred)","6e443186":"param_grid={'n_neighbors':[4,5,6,7,8],'algorithm':['ball_tree','kd_tree','brute'],'p':[1,2]}\ngrid_knn=GridSearchCV(knn,param_grid,cv=5)\ngrid_knn.fit(X_train,y_train)\ny_pred=grid_knn.predict(X_test)\nprint(accuracy_score(y_test,y_pred))\nprint(grid_knn.best_params_)","661a7bc6":"mat=confusion_matrix(y_test,y_pred)\nsns.heatmap(mat.T,square=True,annot=True,cbar=False)\nplt.xlabel('true label')\nplt.ylabel('predicted label')","7ff9a881":"def display_scores(scores):\n    print('Scores:',scores)\n    print('Mean:',scores.mean())    \n    print('Standard Deviation:',scores.std()) ","f3de3d34":"svc_clf=SVC(kernel='rbf',random_state=42,C=2,degree=2,gamma='auto')\nscores=cross_val_score(svc_clf,X_train,y_train,cv=3,scoring='accuracy')\ndisplay_scores(scores)","10cc283c":"y_scores=cross_val_predict(svc_clf,X_train,y_train,method='decision_function')","cb0340b1":"print(precision_score(y_test,y_pred))\nprint(recall_score(y_test,y_pred))\nprint(f1_score(y_test,y_pred))","1edf2442":"precisions,recalls,thresholds=precision_recall_curve(y_train,y_scores)","44d8ad14":"threshold_90_precision=thresholds[np.argmax(precisions>=0.90)]","0460a0db":"y_train_pred_90=(y_scores>=threshold_90_precision)","b52e2fb4":"precision_score(y_train,y_train_pred_90)","575d0d63":"recall_score(y_train,y_train_pred_90)","c81351ec":"threshold_90_precision","861e9f02":"svc_clf=SVC(kernel='rbf',random_state=42,C=2,degree=2,gamma='auto')\nscores=cross_val_score(svc_clf,preprocess_feature,preprocess_label,cv=3,scoring='accuracy')\ndisplay_scores(scores)","1da60112":"test_data.head()","85de6c26":"test_data.describe()","50f38be5":"test_data.isna().any()","3ea1900d":"def get_age(val):\n    age_classification={'Child':range(0,13),\n                        'Teen':range(13,21),\n                        'Young':range(21,31),\n                        '30_adult':range(31,41),\n                        '40_adult':range(41,51),\n                        'Elderly':range(51,90)}\n    for key,value in age_classification.items():\n        if val in value:\n            return key\ndef get_title(val):\n    words=val.split()\n    title={'Officer':['Capt.','Col.','Major.','Dr.','Rev.'],\n          'Royalty':['Jonkheer.','Don.','Sir.','the Countess.','Lady.'],\n          'Mrs':['Mme.','Ms.','Mrs.'],\n          'Mr':['Mr.'],\n          'Miss':['Mlle.','Miss.'],\n          'Master':['Master.']}\n    for key,value in title.items():\n        for word in words:\n            if word in value:\n                return str(key)\ndef fam_size(val):\n    fam={'Single':[1],\n        'Small_family':[2,3,4],\n        'large_family':[5,6,7,8,9,10,11]}\n    for key,value in fam.items():\n        if val in value:\n            return key\ndef most_common(lst):\n    data=Counter(lst)\n    return data.most_common(1)[0][0]","05426644":"from sklearn.base import BaseEstimator,TransformerMixin\nclass CombinedWorks(BaseEstimator,TransformerMixin):\n    def fit(self,X,y=None):\n        return self\n    def transform(self,X,y=None):\n        from sklearn.preprocessing import OrdinalEncoder\n        from sklearn.preprocessing import StandardScaler\n        oe=OrdinalEncoder()\n        scale=StandardScaler()\n        mean_age=X['Age'].mean()\n        mean_fare=X['Fare'].mean()\n        X['Age'].fillna(mean_age,inplace=True)\n        X['Fare'].fillna(mean_fare,inplace=True)\n        PassengerId=X['PassengerId']\n        X.drop(['PassengerId','Cabin','Ticket'],axis=1,inplace=True)\n        X['Title']=np.NAN\n        X['Age']=np.ceil(X['Age'])\n        X['Fam_mem']=X['SibSp']+X['Parch']+1\n        for i in range(len(X)):\n            X['Age'][i]=('{}'.format(get_age(X['Age'][i])))  \n            X['Title'][i]=get_title(X['Name'][i])\n            X['Fam_mem'][i]=fam_size(X['Fam_mem'][i])\n        frequent=most_common(X['Title'])\n        X['Title'].fillna(frequent,inplace=True)\n        X[['Embarked','Sex','Age','Title','Fam_mem']]=oe.fit_transform(X[['Embarked','Sex','Age','Title','Fam_mem']])\n        X=pd.get_dummies(X,columns=['Pclass','Sex','Age','Embarked','Title','Fam_mem'])\n        X[['Fare']]=scale.fit_transform(X[['Fare']])\n        name=X['Name']\n        X.drop(['SibSp','Parch','Name'],axis=1,inplace=True)\n        return X,name,oe.categories_,PassengerId","9d6516a3":"cw=CombinedWorks()\ntest_data,Name,catagories,passenger_id=cw.fit_transform(test_data)","9b011c42":"test_data.head()","e6a7037e":"catagories","481b0f37":"test_data.drop(['Age_0.0','Embarked_1.0','Age_1.0','Age_4.0','Age_3.0','Title_4.0','Age_5.0',\n                'Title_0.0','Pclass_2'],axis=1,inplace=True)","3c7fb3f8":"test_data.shape","20806ec4":"svc_clf=SVC(kernel='rbf',random_state=42,C=2,degree=2,gamma='auto')\nsvc_clf.fit(preprocess_feature,preprocess_label)","35ada25c":"y_pred=svc_clf.predict(test_data)","bf58a22b":"y_pred","e691eab4":"prediction=np.c_[Name,y_pred]","f0a4c56e":"for i in range(len(test_data)):\n    if prediction[i][1]==1:\n        prediction[i][1]='Yes'\n    else:\n        prediction[i][1]='No'","b9b2ce51":"for i in range(len(test_data)):\n    print('Name:{0}\\t\\tSurvived:{1}'.format(prediction[i][0],prediction[i][1]))","1fc87040":"Seems that people who embarked from Queenstown are unfortunate.","c46d7b89":"And still we have the missing value in title column as we mentioned earlier. We can replace the missing title with the most frequent title in the Title column.","50b28ccb":"Pair plot doesn't give a clear intution. Let's predict the survivability with respect to titles.","fa886a70":"Let us visualize the ratio of survivability based on the family size.","528662c0":"We could see that the titles stand common amidst the name. So we can use it for prediction.","3348b10f":"Let's visualise the ratio of survivability of men to women.","72de3072":"Lets try GAUSSIAN NAIVE BAYES......","1a850290":"Good prediction in cross validation.","a628d051":"This also does the same as logistic regression and random forest.","3d7cf25d":"Now we can see that the test data has been modified and lets see if there are all the feature value that were in train data.","ce7e436c":"After removing the unneccasary feature let's scale Fare feature as it has range of higher values.","f68d0d3c":"We have got a new column called Title that display the title of the passengers. We could see that there is a miising value which means that the title was not given to that particular member. So lets deal with it later.\n                                                                                                Now lets us get some information from SibSp and Parch. These are nothing but the number of siblings and parents that particular passenger have in the board. This is also important because those who are having small family managed to escape which gives us a good correlation for our prediction. Lets convert it into family member(Fam_mem) which tells the total number of family members on the board. Then we will convert the numbers into three columns stating the single passengers as singleton, 2 to 4 members as small family, more than 4 as large_family ","52ed7d4b":"Let us visualize the survivability based on Age.","fec37157":"And yes ofcourse there are missing values and we need a lot of data cleaning to be done to get a good visualization and prediction.","27bfb0b5":"It gives us 80% accuracy. That's ok. Let us visualize in the form of heat map.","07696d68":"Let's see RANDOM FOREST CLASSIFIER......","bc643f73":"RMS Titanic was a British passenger liner operated by the White Star Line that sank in the North Atlantic Ocean in the early morning hours of 15 April 1912, after striking an iceberg during her maiden voyage from Southampton to New York City. Of the estimated 2,224 passengers and crew aboard, more than 1,500 died, making the sinking one of modern history's deadliest peacetime commercial marine disasters. RMS Titanic was the largest ship afloat at the time she entered service and was the second of three Olympic-class ocean liners operated by the White Star Line. She was built by the Harland and Wolff shipyard in Belfast. Thomas Andrews, chief naval architect of the shipyard at the time, died in the disaster.\n                                                                                                 The wreck of Titanic was discovered in 1985 (more than 70 years after the disaster) during a Franco-American expedition and US military mission. The ship was split in two and is gradually disintegrating at a depth of 12,415 feet (2,069.2 fathoms; 3,784 m). Thousands of artefacts have been recovered and displayed at museums around the world. Titanic has become one of the most famous ships in history, depicted in numerous works of popular culture, including books, folk songs, films, exhibits, and memorials. Titanic is the second largest ocean liner wreck in the world, only being surpassed by her sister ship HMHS Britannic, however, she is the largest sunk while in service as a liner, as Britannic was in use as a hospital ship at the time of her sinking. The final survivor of the sinking, Millvina Dean, aged two months at the time, died in 2009 at the age of 97.\n                                                                                                  Let's do some visualisation and predict who have survived the tragedy. Here we are going to do this project in a clear linear way so that everyone could understand the process being undertaken.Here it goes.                                                                                         ","6ede7f04":"Now we done with cleaning work let's convert the categorical columns like Sex,Age,Embarked,Title,Fam_mem to Ordinal Values which will be very usefull for going ahead with visualization.","fdd81c0b":"Let us see KNEIGHBORS......","b726a7d0":"Let us check the correlation of each numerical data against the Survived column to see how far each feature contribute to our prediction.","c8207557":"Now LOGISTIC REGRESSION......","13327ee8":"Let us try SUPPORT VECTOR MACHINE........","956850d9":"The classifier did its best. Lets try the same procedure for all other classifiers.","47da45ef":"With the fine tuned hyperparameters random forest does equal to logistic regression","00052797":"Let us first get the histogram of the data.","d16b9b03":"Seems that the feature Sex contribute a lot than others in negative way. This shows that greater the value lesser the survival rate. In Sex feature male is given as 1 and female is given as 0. When the value of feature 'Sex' is '1' the possibility of survival is '0'(i.e If it is a 'male' passenger the probability that he survived is 'low'). Fare feature has given positive correlation. Let us visualize the important correlators in graphical form.","c7ff6386":"As I mentioned we can see a lot of good positive and negative correlations are obtained. With the help of this our classifier does good job in predictiong the value.\nEventhough there are good correlators some doesn't have good correlation. Let's remove those as it does not going to affect our predictions. ","cf11bf97":"This performs even more better than Decision Tree.","ab7a2f56":"Gaussian naive bayes also does same prediction as random forest.","445edbb0":"# 2.Data Preprocessing And Cleaning","84842b05":"# 7.Predicting the test data\nBefore doing prediction we need to preprocess the data as we did for training data. ","a88dc42b":"It gives us accuracy of 79%. Lets fine tune hyperparameters.","fa49351e":"Now let's seperate the data for training","9372a75a":"Let's visualize on survivability basis of embarked places","b0d8344d":"Let us try SGD Classifier......","4bc0a73f":"Let us check the dataset for any missing values and also analyse if we have to do any data cleaning process for taking it to prediction","f0b07c4d":"Above all this gives better results.","d9cef6f4":"Seems that many small member family made their way out,but large family members could not make their way out","66821a76":"# 4.Preprocessing Part-2.0","d3d17d38":"It does poor compared to all above done classifier.","4078ce66":"# 6.Precision,Recall,F1 score\nLets calculate the precision,recall,f1 score of our chosen classifier on the performed data","40ed5295":"Done with the all the preprocessing and cleaning part let's visualize it to get some insights.","fdec9ff0":"We can see that how our classifier have performed in cross validation .\nLet us see precision,recall,f1 score.","cbaf8b85":"We can see that the test data has no member related to the title 'Royalty'. Now lets remove the uncorrelated data as in the training data","6d7752b4":"First lets go ahead with DECISION TREE......","b4aa9b6a":"The preprocess can be done simply by defining a class with fit,transform and fit_transform method. Transform the data by passing it to the class.","44fd6eab":"With now the Age column changed we shall see the names so that we could get anything common in names.","3f389fb2":"# 3.Visualizing","004243e3":"Now that with the data getting ready for prediction we could make some more process so that our classifiers performs well on the dataset. The process includes getting dummies for features like Pclass,Sex,Age,Embarked,Title,Fam_mem. This is done because from our above visualization we could see some values in a feature contibute a lot to survivability. Eg: We can definitly say that a women from Pclass '2' has survivability of 95%. So this gives a good correlation than taking the overall feature of Pclass against survivability. ","61326bcc":"We got about 80% precision and only 59% recall. Precision here means among the classifier predicted the survivors it tend to correctly predict 80% of them as survived and also predicted 20% as suvived but they are dead. Recall means the actula ratio of how much the classifier must predict that the passenger survived.(i.e here the classifier correctly predicted only 59% of total survivors).\nLet us see what will be the recall score if the threshold is increased so that the precision is 90%.","2307989f":"Let us visualize how many men and women in each class managed to survive.","5f8ac193":"Overall we come to the conclusion that SUPPORT VECTOR MACHINE performs well on the training data with fine tuned hyperparameters.","46139802":"Now we can see how the data transformation made changes","d0ea0b3f":"Let's fit the training data to the fine tuned classifier and predict the result","fb0b14b8":"It is clear that lot of men lost their life in this tragedy and lot of women suvived this tragedy.","95aca90e":"#                          Titanic Tragedy Survivors\/Deaths Prediction","19d0ea35":"It's clearly evident that almost all women from Pclass1 and Pclass2 managed to survive.","5901b351":"Let's cross validate and see the accuracy for the overall data we made in the preprocessing part-2.0","df7559ce":"Still we can see missing data in columns like Age and places where people embarked. We can fill the age of the missing people with the help of mean of remaining people's age and fill the embarked people column with the most frequent embarked place.","f78a3000":"It seems Mr are very unfortunate.","8a981a3a":"And now we got the predicted values of those who were in the board. Atlast we did it. We made a lot way gaining a lot of intution about the data and finally making a good prediction. Thank you for travelling with me. :-).If you like it please upvote it.","a9e06a06":"We may have heard that most children were saved but the data shows that about 30 children died in this tragedy. So sad....","2f42a166":"# 1.Importing Library And Dataset\nLet's import the necessary library and the data set","4e4a8a7d":"Since it is a classification typed problem we couldn't get a good insight. But we could make a better visualization with bar and pie charts.","f999b890":"# 5.Selecting the Classifier\nDon't hurry!!!! For prediction we need a best classifier and good hyperparameters tuned for that particular classifier.\nWith the training dataset we are going to split it into train set and test set. The split will be of Stratified Sampling in which the test data will be taken in the correct ratio. With the help of this training and test set we are going to select the best classifier. We will also do grid search to get the fine hyperparameters to do the prediction perfectly.","267b8a65":"With all the missing data filled we shall now change the varying age of people to a particular category. This we do by categorise the age from 0 to 12 as Child, 13 to 20 as Teen, 21 to 30 as Young, 31 to 40 as 30_adult, 41 to 50 as 40_adult and more than 51 as Elderly. By doing this we could get a clear insight of how many children and adults were saved. First lets round off the age.","cef165e9":"From the data insight we could  see that PassengerId is same as the index(not exactly but it just shows the index number of passengers).So we can remove the PassengerId column from our dataset.\nNext comming to the ticket column we don't see anything unique that contributes for the survival of passengers. So we can delete that column from our dataset.\nCabin column have lot of missing data and also doesn't contribute to survival. So we can delete the Cabin column from our data"}}