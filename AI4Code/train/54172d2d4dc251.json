{"cell_type":{"5701b98a":"code","370dfa62":"code","b6411d3f":"code","19a65ae7":"code","5d2f2293":"code","c0814246":"code","6c890818":"code","77b75390":"code","52bb8736":"code","c8bd701a":"code","fc823a09":"code","4bcb6693":"code","b2fe6748":"code","5d4d3416":"code","be51562a":"code","e57d6acd":"code","26901ac3":"code","d778072a":"code","613e2a19":"code","b3e4d8c9":"code","1cadcf46":"code","bc66da7d":"code","53dc6761":"code","454a675b":"code","5f74045c":"code","20f860ae":"code","8a3f4b3f":"code","c183ba33":"code","9e9ee5c0":"code","c7f3f3fb":"code","34cfed78":"code","c6764b94":"code","d53f9467":"code","4546c384":"code","ddabea46":"code","303e55fd":"code","f9b0c898":"code","2c6f25bd":"code","4b31f204":"code","4885226c":"code","bbffaa1d":"code","0494b485":"code","2bfabe50":"code","e3c178f9":"code","4f54e806":"code","50fbc5bf":"code","e657fbb2":"code","a81975fa":"code","23912b58":"code","5d2872aa":"code","0988ad39":"code","919bbbf6":"code","a3e709ca":"code","3b2ea643":"code","989d34a1":"code","aeab49a6":"code","d12a2022":"code","80481aec":"code","de394e13":"code","26f844fd":"code","c2459869":"code","810efcfd":"code","9d1aacea":"code","e7a1a0d7":"code","fa44d1d9":"code","de8320bf":"code","2741e541":"code","31b768af":"code","ba676784":"code","47b7ee23":"code","21900f1f":"code","b5fc2ffe":"code","48c86561":"code","ede5b355":"code","6f23d6b5":"code","0bf6a481":"code","9c324a85":"code","7f0475a2":"code","36cb1d7b":"code","79698b55":"code","2498d3c1":"code","eca91b99":"code","c6888b0a":"code","af239b63":"code","6743bb29":"code","ed315cf1":"code","f8a873c8":"code","d17cd3e5":"code","725b9932":"code","35e2e7dc":"code","951401b2":"markdown","b36b2de5":"markdown","cc7f5c3d":"markdown","6af89600":"markdown","1f21f297":"markdown","94d542ee":"markdown","3352e8df":"markdown","f59a7146":"markdown","aefe0221":"markdown","cb66b70f":"markdown","042ef9c8":"markdown","403d68d6":"markdown","64cfd804":"markdown","729337b3":"markdown","27216c91":"markdown","bb44d224":"markdown","73f354b8":"markdown","b40c1fe2":"markdown","f956404c":"markdown","b6c95ba1":"markdown","12f39222":"markdown","c15b505e":"markdown","a7a331e2":"markdown","24e106ca":"markdown","fdffe3b2":"markdown","684906e2":"markdown","8345bbc5":"markdown","7d4ab47b":"markdown","c5e499f4":"markdown","5b512da5":"markdown","48ffef26":"markdown","cddcf633":"markdown","44a8ae27":"markdown","d4211601":"markdown","8bab12fa":"markdown","426b9795":"markdown","2f059615":"markdown","eb8d1c95":"markdown","a48a6268":"markdown","a1f30c06":"markdown","21faf7ee":"markdown","250d8d16":"markdown","28ee60cc":"markdown","c6f6d45f":"markdown","571b7414":"markdown","ee14f75d":"markdown","d0e8afd8":"markdown","cca48224":"markdown","99f6b6e6":"markdown","93143983":"markdown","fe25dcbb":"markdown","435bb8f9":"markdown","0f6d0f6d":"markdown","91ed9f3d":"markdown","9497dac3":"markdown","6ec9623a":"markdown","1bd6d62d":"markdown","e2b9c1eb":"markdown","f9b22377":"markdown","ade400d3":"markdown","da4e4da2":"markdown","7a26fe4c":"markdown","ed1e3f78":"markdown","571e2239":"markdown","710fa601":"markdown","71b0b652":"markdown","bf03acc5":"markdown","e69e1d1e":"markdown","6fa7a5e0":"markdown","65302e5a":"markdown","96f35c0e":"markdown","da3031f6":"markdown","848b8633":"markdown","34a41a53":"markdown","6ead74f1":"markdown","e723b7c3":"markdown","27229eb5":"markdown","1fa30905":"markdown","45b15760":"markdown","128533a7":"markdown","bc21b9b8":"markdown","f138bd7a":"markdown","2885370d":"markdown","6b8f60e5":"markdown","68e13929":"markdown","9b692782":"markdown","ed881089":"markdown","64eeeab2":"markdown","1528d2c8":"markdown","5b929e0b":"markdown","bb4b3dee":"markdown","4468c327":"markdown","a5d88b04":"markdown"},"source":{"5701b98a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n# To create temporary directory, type in following in the Console\n# os.chdir(\"\/kaggle\/\")\n# !mkdir temp\n# os.listdir()\n\n# Any results we write to the current directory are saved as output in '\/kaggle\/working\/' directory\nprint()\nprint(os.listdir('..'))\nprint(os.listdir('\/kaggle\/input'))\nprint(os.listdir('\/kaggle\/working\/'))\n# print(os.listdir('\/kaggle\/temp\/'))\n","370dfa62":"# Importing needed libraries\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# Importing library to see calculation progress inside loops in Real Time\nfrom tqdm import tqdm\n\n\n# Importing Tensorflow\nimport tensorflow as tf\n\n\n# Importing Keras\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D\nfrom keras.utils import plot_model\n\n\n# Importing timer\nfrom timeit import default_timer as timer\n\n\n# Check point\n# Hint: to print emoji via Unicode, replace '+' with '000' and add prefix '\\'\n# For instance, emoji with Unicode 'U+1F44C' can be printed as '\\U0001F44C'\nprint('Libraries are successfully loaded ' + '\\U0001F44C')\n","b6411d3f":"# Reading input image by OpenCV library\n# In this way image is opened as Numpy array\n\n# (!) OpenCV by default reads images in BGR format (order of channels)\nimage_BGR = cv2.imread('\/kaggle\/input\/images-for-testing\/cat.png')\n\n\n# Converting image to RGB by OpenCV function\nimage_RGB = cv2.cvtColor(image_BGR, cv2.COLOR_BGR2RGB)\n\n\n# Converting image to GRAY by OpenCV function\nimage_GRAY = cv2.cvtColor(image_BGR, cv2.COLOR_BGR2GRAY)\n\n\n# Check points\n# Showing type and shapes of loaded and converted image\nprint('Type of image_BGR is   :', type(image_BGR))\nprint('Shape of image_RGB is  :', image_RGB.shape)\nprint('Shape of image_GRAY is :', image_GRAY.shape)\n","19a65ae7":"# Magic function that renders the figure in a jupyter notebook\n# instead of displaying a figure object\n%matplotlib inline\n\n\n# Setting default size of the plot\nplt.rcParams['figure.figsize'] = (10.0, 8.0)\n\n\n# Defining a figure object with number of needed subplots\nfigure, ax = plt.subplots(nrows=1, ncols=2)\n# ax is a (2,) Numpy array and to access specific subplot we call it by ax[0]\n\n\n# Adjusting 1st column with RGB image\nax[0].imshow(image_RGB)\n\n\n# Adjusting 2nd column with GRAY image\nax[1].imshow(image_GRAY, cmap=plt.get_cmap('gray'))\n\n\n# Hiding axes to all subplots\nfor i in range(2):\n    ax[i].axes.xaxis.set_ticks([])\n    ax[i].axes.yaxis.set_ticks([])\n\n\n# Giving names to subplots along X axis\nax[0].set_xlabel('RGB', fontsize=18)\nax[1].set_xlabel('GRAY', fontsize=18)\n\n\n# Moving subplots names to the top positions\nax[0].xaxis.set_label_position('top')\nax[1].xaxis.set_label_position('top')\n\n\n# Adjusting distance between subplots\nplt.tight_layout()\n\n\n# Showing the plot\nplt.show()\n","5d2f2293":"# Check points\n# Showing shapes of GRAY and RGB images\nprint('Shape of image_GRAY is   :', image_GRAY.shape)\nprint('Shape of image_RGB is    :', image_RGB.shape)\nprint()\n\n\n# Reshaping GRAY image to get following: (batch size, rows, columns, channels)\nx_input_GRAY = image_GRAY.reshape(1, image_GRAY.shape[0], image_GRAY.shape[1], 1).astype(np.float32)\n\n\n# Reshaping RGB image to get following: (batch size, rows, columns, channels)\nx_input_RGB = image_RGB.reshape(1, image_RGB.shape[0], image_RGB.shape[1], 3).astype(np.float32)\n\n\n# Check points\n# Showing shapes of reshaped images\nprint('Shape of x_input_GRAY is :', x_input_GRAY.shape)\nprint('Shape of x_input_RGB is  :', x_input_RGB.shape)\n","c0814246":"# Sobel filter to detect vertical changes on image\nf1 = np.array([[1, 0, -1],\n               [2, 0, -2],\n               [1, 0, -1]])\n\n\n# Laplacian filter to detect regions with different brightness on image\nf2 = np.array([[0, 1, 0], \n               [1, -4, 1], \n               [0, 1, 0]])\n\n\n# Check points\n# Showing shapes of the filters\nprint('Shape of the Sobel filter     is :', f1.shape)\nprint('Shape of the Laplacian filter is :', f2.shape)\n","6c890818":"# Collecting 3 channels for the Sobel filter\nf1_3 = np.array((f1, f1, f1))\n\n\n# Moving channels dimension to the last position: (height, width, channels)\nf1_3 = f1_3.transpose(1, 2, 0)\n\n\n# Check points\n# Showing one channel and shape of the 3-channeled filter\nprint(f1_3[:, :, 0])\nprint()\nprint(f1_3.shape)\nprint()","77b75390":"# Collecting 3 channels for the Laplacian filter\nf2_3 = np.array((f2, f2, f2))\n\n\n# Moving channels dimension to the last position: (height, width, channels)\nf2_3 = f2_3.transpose(1, 2, 0)\n\n\n# Check points\n# Showing one channel and shape of the 3-channeled filter\nprint(f2_3[:, :, 0])\nprint()\nprint(f2_3.shape)\n","52bb8736":"# Assembling 1-channeled filters into one, united array\nf11 = np.array((f1, f2))\n\n\n# Check point\n# Showing shape of the united array\nprint(f11.shape)  # (2, 3, 3)\n\n\n# Reshaping united array to get following: (filters, height, width, channels)\nf11 = f11.reshape(f11.shape + tuple([1]))\n\n\n# Check point\n# Showing shape of the united array\nprint(f11.shape)  # (2, 3, 3, 1)\n\n\n# Moving filters dimension to the last position: (height, width, channels, filters)\nf11 = f11.transpose(1, 2, 3, 0)\n\n\n# Check point\n# Showing shape of the united array\nprint(f11.shape)  # (3, 3, 1, 2)\nprint()\n\n\n# Check points\n# Showing one channel of the 2 filters from united array\nprint(f11[:, :, 0, 0])\nprint()\nprint(f11[:, :, 0, 1])\n","c8bd701a":"# Assembling 3-channeled filters into one, united array\nf22 = np.array((f1_3, f2_3))\n\n\n# Check point\n# Showing shape of the united array\nprint(f22.shape)  # (2, 3, 3, 3)\n\n\n# Moving filters dimension to the last position: (height, width, channels, filters)\nf22 = f22.transpose(1, 2, 3, 0)\n\n\n# Check point\n# Showing shape of the united array\nprint(f22.shape)  # (3, 3, 3, 2)\nprint()\n\n\n# Check points\n# Showing one channel of the 2 filters from united array\nprint(f22[:, :, 0, 0])\nprint()\nprint(f22[:, :, 0, 1])\n","fc823a09":"# Applying to input GRAY image pad frame with zero values to process boundaries\n# Using Numpy method 'pad'\nimage_GRAY_pad = np.pad(image_GRAY, (1, 1), mode='constant', constant_values=0)\n\n\n# Check points\n# Showing shapes of input GRAY image and its version with pad frame\nprint('Shape of image_GRAY is : ', image_GRAY.shape)\nprint('With pad               : ', image_GRAY_pad.shape)\nprint()\n","4bcb6693":"# Preparing zero valued array for convolved output image\n# Dimension is the same with input image according to chosen hyperparameters\n# Passing as argument tuple with needed shape\noutput = np.zeros(image_GRAY.shape)\n\n\n# Check point\n# Showing shape of the output\nprint('Shape of the output is : ', output.shape)\n","b2fe6748":"# Implementing convolution operation to GRAY image\n# Sliding through entire input image (that is with pad frame) by Sobel filter\n# Wrapping the loop with 'tqdm' in order to see progress in Real Time\nfor i in tqdm(range(image_GRAY_pad.shape[0] - 2)):\n    for j in range(image_GRAY_pad.shape[1] - 2):\n        # Extracting (slicing) a 3x3 patch (the same size with filter)\n        # from input image with pad frame\n        patch = image_GRAY_pad[i:i+3, j:j+3]\n\n        # Applying elementwise multiplication and summation -\n        # this is convolution operation\n        # When we use '*' with matrices, then elementwise multiplication\n        # will be applied\n\n        # With Sobel filter\n        output[i, j] = np.sum(patch * f1)\n\n\n# Check point\nprint('Convolution is successfully applied ' + '\\U0001F44C')\n","5d4d3416":"# Check point\nprint(output)\n","be51562a":"# To exclude values that are less than 0 and more than 255,\n# Numpy function 'clip' is applied\n# It keeps values of Numpy array in the given range\n# And it replaces non-needed values with boundary numbers\noutput = np.clip(output, 0, 255)\n\n\n# Check point\nprint(output)\n","e57d6acd":"# Magic function that renders the figure in a jupyter notebook\n# instead of displaying a figure object\n%matplotlib inline\n\n\n# Setting default size of the plot\nplt.rcParams['figure.figsize'] = (10.0, 8.0)\n\n\n# Creating a plot with convolved image\nplt.imshow(output, cmap=plt.get_cmap('gray'))\n\n\n# Hiding axes\nplt.axis('off')\n\n\n# Showing the plot\nplt.show()\n","26901ac3":"# If you're using environment for GPU, there might be an issue like:\n'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n# In this case, try out following options:\n# Option-1: close all Jupyter Notebooks, and re-open just this one Jupyter Notebook\n# Option-2: switch to the environment for CPU usage only instead of GPU\n\n\n# Initializing Conv2D layer for GRAY input\nlayer = tf.keras.layers.Conv2D(filters=1,\n                               kernel_size=(3, 3),\n                               strides=1,\n                               padding='same',\n                               activation='relu',\n                               input_shape=x_input_GRAY.shape[1:],\n                               use_bias=False,\n                               kernel_initializer=tf.keras.initializers.constant(f1))\n\n\n# Check point\nprint('Tensorflow Conv2D layer is successfully initialized ' + '\\U0001F44C')\n","d778072a":"# Passing GRAY input to the initialized Conv2D layer\noutput = layer(x_input_GRAY)\n\n\n# Check points\nprint('Convolution is successfully applied ' + '\\U0001F44C')\nprint('\\n' + 'Shape of the output is :', output.shape)\nprint('\\n')\nprint('\\n' + 'Data type of the output is :', type(output))\n","613e2a19":"# Slicing from the output just feature map\n# Converting output Tensor into Numpy array\noutput = np.array(output[0, :, :, 0])\n\n\n# Check points\nprint('Shape of the output is :', output.shape)\nprint('\\n')\nprint('\\n' + 'Data type of the output is :', type(output))\nprint('\\n')\nprint(output)\n","b3e4d8c9":"# To exclude values that are less than 0 and more than 255,\n# Numpy function 'clip' is applied\n# It keeps values of Numpy array in the given range\n# And it replaces non-needed values with boundary numbers\noutput = np.clip(output, 0, 255)\n\n\n# Check point\nprint(output)\n","1cadcf46":"# Magic function that renders the figure in a jupyter notebook\n# instead of displaying a figure object\n%matplotlib inline\n\n\n# Setting default size of the plot\nplt.rcParams['figure.figsize'] = (10.0, 8.0)\n\n\n# Creating a plot with convolved image\nplt.imshow(output, cmap=plt.get_cmap('gray'))\n\n\n# Hiding axes\nplt.axis('off')\n\n\n# Showing the plot\nplt.show()\n","bc66da7d":"# Getting type of the container that holds weights\nprint('Datatype of the container that holds weights is :', type(layer.get_weights()))  # list\nprint('Datatype of the container that holds weights is :', type(layer.weights))        # list\nprint()\n\n\n# Getting lengths of the list that holds weights\nprint('Lengths of the list that holds weights is       :', len(layer.get_weights()))   # 1\nprint('Lengths of the list that holds weights is       :', len(layer.weights))         # 1\nprint()\n\n\n# Showing shape of the weights\nprint(layer.get_weights()[0].shape)  # (3, 3, 1, 1)\nprint(layer.weights[0].shape)        # (3, 3, 1, 1)\nprint('\\n')\n\n\n# Getting type of weights themselves\nprint(type(layer.get_weights()[0]))  # Numpy array\nprint(type(layer.weights[0]))        # tf.Tensor\nprint()\n\n\n# Showing one channel of the weights themselves\nprint(layer.get_weights()[0][:, :, 0, 0])\nprint()\nprint(layer.weights[0][:, :, 0, 0])\n","53dc6761":"# Showing shape of the pre-defined filter\nprint(f2.shape)\n\n\n# Reshaping filter to get following: (height, width, channels, filters)\nf_new = f2.reshape(3, 3, 1, 1)\n\n\n# Showing shape of the updated filter\nprint(f_new.shape)\n","454a675b":"# Showing one channel of the current weights\nprint(layer.get_weights()[0][:, :, 0, 0])\nprint()\n\n\n# Setting up new weights\nlayer.set_weights([f_new])\n\n\n# Showing one channel of the updated weights\nprint(layer.get_weights()[0][:, :, 0, 0])\n","5f74045c":"# Passing GRAY input to the Conv2D layer with new weights\noutput = layer(x_input_GRAY)\n\n\n# Slicing from the output just feature map\n# Converting output Tensor into Numpy array\noutput = np.array(output[0, :, :, 0])\n\n\n# Excluding non-needed values (less than 0 and more than 255)\noutput = np.clip(output, 0, 255)\n\n\n# Check points\nprint('Convolution is successfully applied ' + '\\U0001F44C')\nprint('\\n' + 'Shape of the output is :', output.shape)\nprint('\\n' + 'Feature map is successfully sliced from the output')\nprint('\\n' + 'Non-needed values are successfully excluded')\n","20f860ae":"# Magic function that renders the figure in a jupyter notebook\n# instead of displaying a figure object\n%matplotlib inline\n\n\n# Setting default size of the plot\nplt.rcParams['figure.figsize'] = (10.0, 8.0)\n\n\n# Creating a plot with convolved image\nplt.imshow(output, cmap=plt.get_cmap('gray'))\n\n\n# Hiding axes\nplt.axis('off')\n\n\n# Showing the plot\nplt.show()\n","8a3f4b3f":"# If you're using environment for GPU, there might be an issue like:\n'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n# In this case, try out following options:\n# Option-1: close all Jupyter Notebooks, and re-open just this one Jupyter Notebook\n# Option-2: switch to the environment for CPU usage only instead of GPU\n\n\n# Initializing Conv2D layer for GRAY input\nlayer = tf.keras.layers.Conv2D(filters=1,\n                               kernel_size=(3, 3),\n                               strides=1,\n                               padding='same',\n                               activation='relu',\n                               input_shape=x_input_GRAY.shape[1:],\n                               use_bias=True,\n                               kernel_initializer=tf.keras.initializers.constant(f1))\n\n\n# Check point\nprint('Tensorflow Conv2D layer is successfully initialized ' + '\\U0001F44C')\n","c183ba33":"# Passing GRAY input to the initialized Conv2D layer\noutput = layer(x_input_GRAY)\n\n\n# Check points\nprint('Convolution is successfully applied ' + '\\U0001F44C')\nprint('\\n' + 'Shape of the output is :', output.shape)\n","9e9ee5c0":"# Getting type of the container that holds weights and bias\nprint('Datatype of the container that holds weights and bias is :', type(layer.get_weights()))\n\n\n# Getting lengths of the list that holds weights and bias\nprint('Lengths of the list that holds weights and bias is       :', len(layer.get_weights()))\nprint()\n\n\n# Showing shape of the weights\nprint(layer.get_weights()[0].shape)  # (3, 3, 1, 1)\nprint()\n\n\n# Showing one channel of the weights themselves\nprint(layer.get_weights()[0][:, :, 0, 0])\nprint()\n\n\n# Showing shape of the bias\nprint(layer.get_weights()[1].shape)\nprint()\n\n\n# Showing bias itself\nprint(layer.get_weights()[1])\n","c7f3f3fb":"# Showing shape of the pre-defined filter\nprint(f2.shape)\n\n\n# Reshaping filter to get following: (height, width, channels, filters)\nf_new = f2.reshape(3, 3, 1, 1)\n\n\n# Showing shape of the updated filter\nprint(f_new.shape)\n","34cfed78":"# Defining array for bias\nb = np.array([0.])\n\n\n# Showing shape of the bias\nprint(b.shape)\n","c6764b94":"# Showing one channel of the current weights\nprint(layer.get_weights()[0][:, :, 0, 0])\nprint()\n\n\n# Setting up new weights and bias\nlayer.set_weights([f_new, b])\n\n\n# Showing one channel of the updated weights\nprint(layer.get_weights()[0][:, :, 0, 0])\n","d53f9467":"# Passing GRAY input to the Conv2D layer with new weights\noutput = layer(x_input_GRAY)\n\n\n# Slicing from the output just feature map\n# Converting output Tensor into Numpy array\noutput = np.array(output[0, :, :, 0])\n\n\n# Excluding non-needed values (less than 0 and more than 255)\noutput = np.clip(output, 0, 255)\n\n\n# Check points\nprint('Convolution is successfully applied ' + '\\U0001F44C')\nprint('\\n' + 'Shape of the output is :', output.shape)\nprint('\\n' + 'Feature map is successfully sliced from the output')\nprint('\\n' + 'Non-needed values are successfully excluded')\n","4546c384":"# Magic function that renders the figure in a jupyter notebook\n# instead of displaying a figure object\n%matplotlib inline\n\n\n# Setting default size of the plot\nplt.rcParams['figure.figsize'] = (10.0, 8.0)\n\n\n# Creating a plot with convolved image\nplt.imshow(output, cmap=plt.get_cmap('gray'))\n\n\n# Hiding axes\nplt.axis('off')\n\n\n# Showing the plot\nplt.show()\n","ddabea46":"# Showing shape of the pre-defined filter\nprint(f1.shape)\n\n\n# Reshaping filter to get following: (height, width, channels, filters)\nf_new = f1.reshape(3, 3, 1, 1)\n\n\n# Showing shape of the updated filter\nprint(f_new.shape)\n","303e55fd":"# If you're using environment for GPU, there might be an issue like:\n'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n# In this case, try out following options:\n# Option-1: close all Jupyter Notebooks, and re-open just this one Jupyter Notebook\n# Option-2: switch to the environment for CPU usage only instead of GPU\n\n\n# Initializing Conv2D layer for GRAY input\nlayer = tf.keras.layers.Conv2D(filters=1,\n                               kernel_size=(3, 3),\n                               strides=1,\n                               padding='same',\n                               activation='relu',\n                               input_shape=x_input_GRAY.shape[1:],\n                               use_bias=False,\n                               weights=[f_new])\n\n\n# Check point\nprint('Tensorflow Conv2D layer is successfully initialized ' + '\\U0001F44C')\n","f9b0c898":"# Passing GRAY input to the Conv2D layer\noutput = layer(x_input_GRAY)\n\n\n# Slicing from the output just feature map\n# Converting output Tensor into Numpy array\noutput = np.array(output[0, :, :, 0])\n\n\n# Excluding non-needed values (less than 0 and more than 255)\noutput = np.clip(output, 0, 255)\n\n\n# Check points\nprint('Convolution is successfully applied ' + '\\U0001F44C')\nprint('\\n' + 'Shape of the output is :', output.shape)\nprint('\\n' + 'Feature map is successfully sliced from the output')\nprint('\\n' + 'Non-needed values are successfully excluded')\n","2c6f25bd":"# Magic function that renders the figure in a jupyter notebook\n# instead of displaying a figure object\n%matplotlib inline\n\n\n# Setting default size of the plot\nplt.rcParams['figure.figsize'] = (10.0, 8.0)\n\n\n# Creating a plot with convolved image\nplt.imshow(output, cmap=plt.get_cmap('gray'))\n\n\n# Hiding axes\nplt.axis('off')\n\n\n# Showing the plot\nplt.show()\n","4b31f204":"# If you're using environment for GPU, there might be an issue like:\n'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n# In this case, try out following options:\n# Option-1: close all Jupyter Notebooks, and re-open just this one Jupyter Notebook\n# Option-2: switch to the environment for CPU usage only instead of GPU\n\n\n# Initializing Conv2D layer for RGB input\nlayer = tf.keras.layers.Conv2D(filters=1,\n                               kernel_size=(3, 3),\n                               strides=1,\n                               padding='same',\n                               activation='relu',\n                               input_shape=x_input_RGB.shape[1:],\n                               use_bias=False,\n                               kernel_initializer=tf.keras.initializers.constant(f1_3))\n\n\n# Check point\nprint('Tensorflow Conv2D layer is successfully initialized ' + '\\U0001F44C')\n","4885226c":"# Passing RGB input to the initialized Conv2D layer\noutput = layer(x_input_RGB)\n\n\n# Check points\nprint('Convolution is successfully applied ' + '\\U0001F44C')\nprint('\\n' + 'Shape of the output is :', output.shape)\n","bbffaa1d":"# Getting type of the container that holds weights\nprint('Datatype of the container that holds weights is :', type(layer.get_weights()))\n\n\n# Getting lengths of the list that holds weights\nprint('Lengths of the list that holds weights is       :', len(layer.get_weights()))\nprint()\n\n\n# Showing shape of the weights\nprint(layer.get_weights()[0].shape)\nprint('\\n')\n\n\n# Showing one channel of the weights themselves\nprint(layer.get_weights()[0][:, :, 0, 0])\n","0494b485":"# Showing shape of the pre-defined filter\nprint(f2_3.shape)\n\n\n# Reshaping filter to get following: (height, width, channels, filters)\nf_new = f2_3.reshape(3, 3, 3, 1)\n\n\n# Showing shape of the updated filter\nprint(f_new.shape)\n","2bfabe50":"# Showing one channel of the current weights\nprint(layer.get_weights()[0][:, :, 0, 0])\nprint()\n\n\n# Setting up new weights\nlayer.set_weights([f_new])\n\n\n# Showing one channel of the updated weights\nprint(layer.get_weights()[0][:, :, 0, 0])\n","e3c178f9":"# Passing RGB input to the Conv2D layer with new weights\noutput = layer(x_input_RGB)\n\n\n# Slicing from the output just feature map\n# Converting output Tensor into Numpy array\noutput = np.array(output[0, :, :, 0])\n\n\n# Excluding non-needed values (less than 0 and more than 255)\noutput = np.clip(output, 0, 255)\n\n\n# Check points\nprint('Convolution is successfully applied ' + '\\U0001F44C')\nprint('\\n' + 'Shape of the output is :', output.shape)\nprint('\\n' + 'Feature map is successfully sliced from the output')\nprint('\\n' + 'Non-needed values are successfully excluded')\n","4f54e806":"# Magic function that renders the figure in a jupyter notebook\n# instead of displaying a figure object\n%matplotlib inline\n\n\n# Setting default size of the plot\nplt.rcParams['figure.figsize'] = (10.0, 8.0)\n\n\n# Creating a plot with convolved image\nplt.imshow(output, cmap=plt.get_cmap('gray'))\n\n\n# Hiding axes\nplt.axis('off')\n\n\n# Showing the plot\nplt.show()\n","50fbc5bf":"# If you're using environment for GPU, there might be an issue like:\n'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n# In this case, try out following options:\n# Option-1: close all Jupyter Notebooks, and re-open just this one Jupyter Notebook\n# Option-2: switch to the environment for CPU usage only instead of GPU\n\n\n# Initializing Conv2D layer for GRAY input and set of filters\nlayer = tf.keras.layers.Conv2D(filters=2,\n                               kernel_size=(3, 3),\n                               strides=1,\n                               padding='same',\n                               activation='relu',\n                               input_shape=x_input_GRAY.shape[1:],\n                               use_bias=False,\n                               kernel_initializer=tf.keras.initializers.constant(f11))\n\n\n# Check point\nprint('Tensorflow Conv2D layer is successfully initialized ' + '\\U0001F44C')\n","e657fbb2":"# Passing GRAY input to the initialized Conv2D layer with set of filters\noutput = layer(x_input_GRAY)\n\n\n# Check points\nprint('Convolution is successfully applied ' + '\\U0001F44C')\nprint('\\n' + 'Shape of the output is :', output.shape)\nprint('\\n')\n","a81975fa":"# Getting type of the container that holds weights\nprint('Datatype of the container that holds weights is :', type(layer.get_weights()))\n\n\n# Getting lengths of the list that holds weights\nprint('Lengths of the list that holds weights is       :', len(layer.get_weights()))\nprint()\n\n\n# Showing shape of the weights\nprint(layer.get_weights()[0].shape)  # (3, 3, 1, 2)\nprint('\\n')\n\n\n# Showing one channel of the 2 weights themselves\nprint(layer.get_weights()[0][:, :, 0, 0])\nprint('\\n')\nprint(layer.get_weights()[0][:, :, 0, 1])\n","23912b58":"# Showing shape of the pre-defined GRAY set of filters\nprint(f11.shape)\nprint('\\n')\n","5d2872aa":"# Showing one channel of the current weights\nprint('Originally defined weights:\\n')\nprint(f11[:, :, 0, 0])\nprint()\nprint(f11[:, :, 0, 1])\nprint('\\n')\n\n\n# Setting up new weights\n# Swapping them\nf11[:, :, 0, 0],  f11[:, :, 0, 1] = np.copy(f11[:, :, 0, 1]),  np.copy(f11[:, :, 0, 0])\n\n\n\n\n# Showing one channel of the updated weights\nprint('Weights after swapping:\\n')\nprint(f11[:, :, 0, 0])\nprint()\nprint(f11[:, :, 0, 1])\n","0988ad39":"# Showing one channel of the current weights\nprint('Initialized weights:\\n')\nprint(layer.get_weights()[0][:, :, 0, 0])\nprint()\nprint(layer.get_weights()[0][:, :, 0, 1])\nprint('\\n')\n\n\n# Setting up new weights\nlayer.set_weights([f11])\n\n\n# Showing one channel of the updated weights\nprint('Updated weights:\\n')\nprint(layer.get_weights()[0][:, :, 0, 0])\nprint()\nprint(layer.get_weights()[0][:, :, 0, 1])\n","919bbbf6":"# Passing GRAY input to the Conv2D layer with new weights\noutput = layer(x_input_GRAY)\n\n\n# Slicing from the output just feature maps\n# Converting output Tensor into Numpy array\noutput = np.array(output[0, :, :, :])\n\n\n# Excluding non-needed values (less than 0 and more than 255)\noutput = np.clip(output, 0, 255)\n\n\n# Check points\nprint('Convolution is successfully applied ' + '\\U0001F44C')\nprint('\\n' + 'Shape of the output is :', output.shape)\nprint('\\n' + 'Feature maps are successfully sliced from the output')\nprint('\\n' + 'Non-needed values are successfully excluded')\n","a3e709ca":"# Magic function that renders the figure in a jupyter notebook\n# instead of displaying a figure object\n%matplotlib inline\n\n\n# Setting default size of the plot\nplt.rcParams['figure.figsize'] = (10.0, 8.0)\n\n\n# Defining a figure object with number of needed subplots\nfigure, ax = plt.subplots(nrows=1, ncols=2)\n# ax is a (2,) Numpy array and to access specific subplot we call it by ax[0]\n\n\n# Adjusting 1st column\nax[0].imshow(output[:, :, 0], cmap=plt.get_cmap('gray'))\n\n\n# Adjusting 2nd column\nax[1].imshow(output[:, :, 1], cmap=plt.get_cmap('gray'))\n\n\n# Hiding axes to all subplots\nfor i in range(2):\n    ax[i].axes.xaxis.set_ticks([])\n    ax[i].axes.yaxis.set_ticks([])\n\n\n# Giving names to subplots along X axis\nax[0].set_xlabel('Laplacian', fontsize=18)\nax[1].set_xlabel('Sobel', fontsize=18)\n\n\n# Moving subplots names to the top positions\nax[0].xaxis.set_label_position('top')\nax[1].xaxis.set_label_position('top')\n\n\n# Adjusting distance between subplots\nplt.tight_layout()\n\n\n# Showing the plot\nplt.show()\n","3b2ea643":"# If you're using environment for GPU, there might be an issue like:\n'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n# In this case, try out following options:\n# Option-1: close all Jupyter Notebooks, and re-open just this one Jupyter Notebook\n# Option-2: switch to the environment for CPU usage only instead of GPU\n\n\n# Initializing Conv2D layer for GRAY input and set of filters\nlayer = tf.keras.layers.Conv2D(filters=2,\n                               kernel_size=(3, 3),\n                               strides=1,\n                               padding='same',\n                               activation='relu',\n                               input_shape=x_input_RGB.shape[1:],\n                               use_bias=False,\n                               kernel_initializer=tf.keras.initializers.constant(f22))\n\n\n# Check point\nprint('Tensorflow Conv2D layer is successfully initialized ' + '\\U0001F44C')\n","989d34a1":"# Passing RGB input to the initialized Conv2D layer with set of filters\noutput = layer(x_input_RGB)\n\n\n# Check points\nprint('Convolution is successfully applied ' + '\\U0001F44C')\nprint('\\n' + 'Shape of the output is :', output.shape)\nprint('\\n')\n","aeab49a6":"# Getting type of the container that holds weights\nprint('Datatype of the container that holds weights is :', type(layer.get_weights()))\n\n\n# Getting lengths of the list that holds weights\nprint('Lengths of the list that holds weights is       :', len(layer.get_weights()))\nprint()\n\n\n# Showing shape of the weights\nprint(layer.get_weights()[0].shape)\nprint('\\n')\n\n\n# Showing one channel of the 2 weights themselves\nprint(layer.get_weights()[0][:, :, 0, 0])\nprint('\\n')\nprint(layer.get_weights()[0][:, :, 0, 1])\n","d12a2022":"# Showing shape of the pre-defined RGB set of filters\nprint(f22.shape)\nprint('\\n')\n","80481aec":"# Showing one channel of the current weights\nprint('Originally defined weights:\\n')\nprint(f22[:, :, 0, 0])\nprint()\nprint(f22[:, :, 0, 1])\nprint('\\n')\n\n\n# Setting up new weights\n# Swapping them\nf22[:, :, :, 0],  f22[:, :, :, 1] = np.copy(f22[:, :, :, 1]),  np.copy(f22[:, :, :, 0])\n\n\n\n\n\n# Showing one channel of the updated weights\nprint('Weights after swapping:\\n')\nprint(f22[:, :, 0, 0])\nprint()\nprint(f22[:, :, 0, 1])\n","de394e13":"# Showing one channel of the current weights\nprint('Initialized weights:\\n')\nprint(layer.get_weights()[0][:, :, 0, 0])\nprint()\nprint(layer.get_weights()[0][:, :, 0, 1])\nprint('\\n')\n\n\n# Setting up new weights\nlayer.set_weights([f22])\n\n\n# Showing one channel of the updated weights\nprint('Updated weights:\\n')\nprint(layer.get_weights()[0][:, :, 0, 0])\nprint()\nprint(layer.get_weights()[0][:, :, 0, 1])\n","26f844fd":"# Passing RGB input to the Conv2D layer with new weights\noutput = layer(x_input_RGB)\n\n\n# Slicing from the output just feature maps\n# Converting output Tensor into Numpy array\noutput = np.array(output[0, :, :, :])\n\n\n# Excluding non-needed values (less than 0 and more than 255)\noutput = np.clip(output, 0, 255)\n\n\n# Check points\nprint('Convolution is successfully applied ' + '\\U0001F44C')\nprint('\\n' + 'Shape of the output is :', output.shape)\nprint('\\n' + 'Feature maps are successfully sliced from the output')\nprint('\\n' + 'Non-needed values are successfully excluded')\n","c2459869":"# Magic function that renders the figure in a jupyter notebook\n# instead of displaying a figure object\n%matplotlib inline\n\n\n# Setting default size of the plot\nplt.rcParams['figure.figsize'] = (10.0, 8.0)\n\n\n# Defining a figure object with number of needed subplots\nfigure, ax = plt.subplots(nrows=1, ncols=2)\n# ax is a (2,) Numpy array and to access specific subplot we call it by ax[0]\n\n\n# Adjusting 1st column\nax[0].imshow(output[:, :, 0], cmap=plt.get_cmap('gray'))\n\n\n# Adjusting 2nd column\nax[1].imshow(output[:, :, 1], cmap=plt.get_cmap('gray'))\n\n\n# Hiding axes to all subplots\nfor i in range(2):\n    ax[i].axes.xaxis.set_ticks([])\n    ax[i].axes.yaxis.set_ticks([])\n\n\n# Giving names to subplots along X axis\nax[0].set_xlabel('Laplacian', fontsize=18)\nax[1].set_xlabel('Sobel', fontsize=18)\n\n\n# Moving subplots names to the top positions\nax[0].xaxis.set_label_position('top')\nax[1].xaxis.set_label_position('top')\n\n\n# Adjusting distance between subplots\nplt.tight_layout()\n\n\n# Showing the plot\nplt.show()\n","810efcfd":"# If you're using environment for GPU, there might be an issue like:\n'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n# In this case, try out following options:\n# Option-1: close all Jupyter Notebooks, and re-open just this one Jupyter Notebook\n# Option-2: switch to the environment for CPU usage only instead of GPU\n\n\n# Initializing Conv2D layer for GRAY input and set of filters\nlayer_for_GRAY_input = tf.keras.layers.Conv2D(filters=2,\n                               kernel_size=(3, 3),\n                               strides=1,\n                               padding='same',\n                               activation='relu',\n                               input_shape=x_input_GRAY.shape[1:],\n                               use_bias=False,\n                               kernel_initializer=tf.keras.initializers.constant(f11))\n\n\n# Check point\nprint('Tensorflow Conv2D layer is successfully initialized ' + '\\U0001F44C')\n","9d1aacea":"# If you're using environment for GPU, there might be an issue like:\n'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n# In this case, try out following options:\n# Option-1: close all Jupyter Notebooks, and re-open just this one Jupyter Notebook\n# Option-2: switch to the environment for CPU usage only instead of GPU\n\n\n# Initializing Conv2D layer for RGB input and set of filters\nlayer_for_RGB_input = tf.keras.layers.Conv2D(filters=2,\n                               kernel_size=(3, 3),\n                               strides=1,\n                               padding='same',\n                               activation='relu',\n                               input_shape=x_input_RGB.shape[1:],\n                               use_bias=False,\n                               kernel_initializer=tf.keras.initializers.constant(f22))\n\n\n# Check point\nprint('Tensorflow Conv2D layer is successfully initialized ' + '\\U0001F44C')\n","e7a1a0d7":"# Passing GRAY input to the Conv2D layer with new weights\noutput_from_GRAY_input = layer_for_GRAY_input(x_input_GRAY)\n\n\n# Slicing from the output just feature maps\n# Converting output Tensor into Numpy array\noutput_from_GRAY_input = np.array(output_from_GRAY_input[0, :, :, :])\n\n\n# Excluding non-needed values (less than 0 and more than 255)\noutput_from_GRAY_input = np.clip(output_from_GRAY_input, 0, 255)\n\n\n# Check points\nprint('Convolution is successfully applied ' + '\\U0001F44C')\nprint('\\n' + 'Shape of the output is :', output_from_GRAY_input.shape)\nprint('\\n' + 'Feature maps are successfully sliced from the output')\nprint('\\n' + 'Non-needed values are successfully excluded')\n","fa44d1d9":"# Passing RGB input to the Conv2D layer with new weights\noutput_from_RGB_input = layer_for_RGB_input(x_input_RGB)\n\n\n# Slicing from the output just feature maps\n# Converting output Tensor into Numpy array\noutput_from_RGB_input = np.array(output_from_RGB_input[0, :, :, :])\n\n\n# Excluding non-needed values (less than 0 and more than 255)\noutput_from_RGB_input = np.clip(output_from_RGB_input, 0, 255)\n\n\n# Check points\nprint('Convolution is successfully applied ' + '\\U0001F44C')\nprint('\\n' + 'Shape of the output is :', output_from_RGB_input.shape)\nprint('\\n' + 'Feature maps are successfully sliced from the output')\nprint('\\n' + 'Non-needed values are successfully excluded')\n","de8320bf":"# Magic function that renders the figure in a jupyter notebook\n# instead of displaying a figure object\n%matplotlib inline\n\n\n# Setting default size of the plot\nplt.rcParams['figure.figsize'] = (10.0, 6.5)\n\n\n# Defining a figure object with number of needed subplots\nfigure, ax = plt.subplots(nrows=2, ncols=2)\n# ax is a (2, 2) Numpy array and to access specific subplot we call it by ax[0, 0]\n\n\n# Adjusting 1st column\nax[0, 0].imshow(output_from_GRAY_input[:, :, 0], cmap=plt.get_cmap('gray'))\nax[1, 0].imshow(output_from_RGB_input[:, :, 0], cmap=plt.get_cmap('gray'))\n\n\n# Adjusting 2nd column\nax[0, 1].imshow(output_from_GRAY_input[:, :, 1], cmap=plt.get_cmap('gray'))\nax[1, 1].imshow(output_from_RGB_input[:, :, 1], cmap=plt.get_cmap('gray'))\n\n\n# Hiding axes to all subplots\nfor i in range(2):\n    for j in range(2):\n        ax[i, j].axes.xaxis.set_ticks([])\n        ax[i, j].axes.yaxis.set_ticks([])\n\n\n# Giving names to X & Y axes\nax[0, 0].set_xlabel('Laplacian', fontsize=18)\nax[0, 0].xaxis.set_label_position('top')\nax[0, 1].set_xlabel('Sobel', fontsize=18)\nax[0, 1].xaxis.set_label_position('top')\n\nax[0, 0].set_ylabel('GRAY         \\n input          ', fontsize=18, rotation='horizontal')\nax[1, 0].set_ylabel('RGB         \\n input          ', fontsize=18, rotation='horizontal')\n\n\n# Adjusting distance between subplots\nplt.tight_layout()\n\n\n# Saving the plot\nfigure.savefig('2d_convolution_Tensorflow.png', transparent=True, dpi=500)\n\n\n# Showing the plot\nplt.show()\n","2741e541":"# Showing shape of the pre-defined GRAY set of filters\nprint(f11.shape)\n\n\n# Showing shape of the pre-defined RGB set of filters\nprint(f22.shape)\nprint('\\n')\n","31b768af":"# If you're using environment for GPU, there might be an issue like:\n'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n# In this case, try out following options:\n# Option-1: close all Jupyter Notebooks, and re-open just this one Jupyter Notebook\n# Option-2: switch to the environment for CPU usage only instead of GPU\n\n\n# Initializing Keras model for GRAY input and set of filters by class 'Sequential()'\n# Other option is to initialize Keras model by class 'Model()'\nmodel_GRAY = Sequential()\n\n\n# Adding Conv2D layer to the model for GRAY input and set of filters\nmodel_GRAY.add(Conv2D(filters=2,\n                      kernel_size=(3, 3),\n                      strides=1,\n                      padding='same',\n                      activation='relu',\n                      input_shape=x_input_GRAY.shape[1:],\n                      use_bias=False,\n                      weights=[f11]))\n\n\n# Check point\nprint('Keras model with Conv2D layer is successfully initialized ' + '\\U0001F44C')\n","ba676784":"# If you're using environment for GPU, there might be an issue like:\n'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n# In this case, try out following options:\n# Option-1: close all Jupyter Notebooks, and re-open just this one Jupyter Notebook\n# Option-2: switch to the environment for CPU usage only instead of GPU\n\n\n# Initializing Keras model for RGB input and set of filters by class 'Sequential()'\n# Other option is to initialize Keras model by class 'Model()'\nmodel_RGB = Sequential()\n\n\n# Adding Conv2D layer to the model for RGB input and set of filters\nmodel_RGB.add(Conv2D(filters=2,\n                     kernel_size=(3, 3),\n                     strides=1,\n                     padding='same',\n                     activation='relu',\n                     input_shape=x_input_RGB.shape[1:],\n                     use_bias=False,\n                     weights=[f22]))\n\n\n# Check point\nprint('Keras model with Conv2D layer is successfully initialized ' + '\\U0001F44C')\n","47b7ee23":"# Showing GRAY model's summary in form of table\nmodel_GRAY.summary()\n","21900f1f":"# Showing RGB model's summary in form of table\nmodel_RGB.summary()\n","b5fc2ffe":"# Plotting GRAY model's layers in form of flowchart\nplot_model(model_GRAY,\n           show_shapes=True,\n           show_layer_names=False,\n           rankdir='LR',\n           dpi=500)\n","48c86561":"# Plotting RGB model's layers in form of flowchart\nplot_model(model_RGB,\n           show_shapes=True,\n           show_layer_names=False,\n           rankdir='LR',\n           dpi=500)\n","ede5b355":"# Passing GRAY input to the Conv2D layer\noutput_from_GRAY_input = model_GRAY.predict(x_input_GRAY)\n\n\n# Slicing from the output just feature maps\n# Converting output Tensor into Numpy array\noutput_from_GRAY_input = np.array(output_from_GRAY_input[0, :, :, :])\n\n\n# Excluding non-needed values (less than 0 and more than 255)\noutput_from_GRAY_input = np.clip(output_from_GRAY_input, 0, 255)\n\n\n# Check points\nprint('Convolution is successfully applied ' + '\\U0001F44C')\nprint('\\n' + 'Shape of the output is :', output_from_GRAY_input.shape)\nprint('\\n' + 'Feature maps are successfully sliced from the output')\nprint('\\n' + 'Non-needed values are successfully excluded')\n","6f23d6b5":"# Passing RGB input to the Conv2D layer\noutput_from_RGB_input = model_RGB.predict(x_input_RGB)\n\n\n# Slicing from the output just feature maps\n# Converting output Tensor into Numpy array\noutput_from_RGB_input = np.array(output_from_RGB_input[0, :, :, :])\n\n\n# Excluding non-needed values (less than 0 and more than 255)\noutput_from_RGB_input = np.clip(output_from_RGB_input, 0, 255)\n\n\n# Check points\nprint('Convolution is successfully applied ' + '\\U0001F44C')\nprint('\\n' + 'Shape of the output is :', output_from_RGB_input.shape)\nprint('\\n' + 'Feature maps are successfully sliced from the output')\nprint('\\n' + 'Non-needed values are successfully excluded')\n","0bf6a481":"# Getting type of the container that holds layers for the entire model\nprint('Datatype of the container that holds layers is  :', type(model_GRAY.layers))  # list\nprint('Datatype of the container that holds layers is  :', type(model_RGB.layers))   # list\nprint()\n\n\n# Getting length of the list that holds layers for the entire model\nprint('Lengths of the list that holds layers is        :', len(model_GRAY.layers))  # 1\nprint('Lengths of the list that holds layers is        :', len(model_RGB.layers))   # 1\nprint()\n\n\n# Getting type of the container that holds weights for 2D convolutional layer\nprint('Datatype of the container that holds weights is :', type(model_GRAY.layers[0].get_weights()))\nprint('Datatype of the container that holds weights is :', type(model_RGB.layers[0].get_weights()))\nprint()\n\n\n# Getting lengths of the list that holds weights for 2D convolutional layer\nprint('Lengths of the list that holds weights is       :', len(model_GRAY.layers[0].get_weights()))\nprint('Lengths of the list that holds weights is       :', len(model_RGB.layers[0].get_weights()))\nprint()\n\n\n# Showing shape of the weights\nprint(model_GRAY.layers[0].get_weights()[0].shape)  # (3, 3, 1, 2)\nprint(model_RGB.layers[0].get_weights()[0].shape)   # (3, 3, 3, 2)\nprint()\n\n\n# Showing one channel of the weights themselves\nprint(model_GRAY.layers[0].get_weights()[0][:, :, 0, 0])\nprint()\nprint(model_RGB.layers[0].get_weights()[0][:, :, 0, 0])\nprint()\n","9c324a85":"# Setting up new weights for GRAY model\nmodel_GRAY.layers[0].set_weights([f11])\n\n\n\n# Setting up new weights for RGB model\nmodel_RGB.layers[0].set_weights([f22])\n\n\n","7f0475a2":"# Magic function that renders the figure in a jupyter notebook\n# instead of displaying a figure object\n%matplotlib inline\n\n\n# Setting default size of the plot\nplt.rcParams['figure.figsize'] = (10.0, 6.5)\n\n\n# Defining a figure object with number of needed subplots\nfigure, ax = plt.subplots(nrows=2, ncols=2)\n# ax is a (2, 2) Numpy array and to access specific subplot we call it by ax[0, 0]\n\n\n# Adjusting 1st column\nax[0, 0].imshow(output_from_GRAY_input[:, :, 0], cmap=plt.get_cmap('gray'))\nax[1, 0].imshow(output_from_RGB_input[:, :, 0], cmap=plt.get_cmap('gray'))\n\n\n# Adjusting 2nd column\nax[0, 1].imshow(output_from_GRAY_input[:, :, 1], cmap=plt.get_cmap('gray'))\nax[1, 1].imshow(output_from_RGB_input[:, :, 1], cmap=plt.get_cmap('gray'))\n\n\n# Hiding axes to all subplots\nfor i in range(2):\n    for j in range(2):\n        ax[i, j].axes.xaxis.set_ticks([])\n        ax[i, j].axes.yaxis.set_ticks([])\n\n\n# Giving names to X & Y axes\nax[0, 0].set_xlabel('Laplacian', fontsize=18)\nax[0, 0].xaxis.set_label_position('top')\nax[0, 1].set_xlabel('Sobel', fontsize=18)\nax[0, 1].xaxis.set_label_position('top')\n\nax[0, 0].set_ylabel('GRAY         \\n input          ', fontsize=18, rotation='horizontal')\nax[1, 0].set_ylabel('RGB         \\n input          ', fontsize=18, rotation='horizontal')\n\n\n# Adjusting distance between subplots\nplt.tight_layout()\n\n\n# Saving the plot\n# (!) On Windows, the path might look like following:\n# r'images\\2d_convolution_Keras.png'\n# or:\n# 'images\\\\2d_convolution_Keras.png'\nfigure.savefig('2d_convolution_Keras.png', transparent=True, dpi=500)\n\n\n# Showing the plot\nplt.show()\n","36cb1d7b":"# Applying to input GRAY image pad frame\nimage_GRAY_pad = np.pad(image_GRAY, (1, 1), mode='constant', constant_values=0)\n\n\n# Preparing zero valued array for convolved output image\noutput = np.zeros(image_GRAY.shape)\n\n\n# Measuring spent time\n# Start point\nstart = timer()\n\n\n# Implementing convolution operation to GRAY image\n# Sliding through entire input image (that is with pad frame) by Sobel filter\n# Wrapping the loop with 'tqdm' in order to see progress in Real Time\nfor i in tqdm(range(image_GRAY_pad.shape[0] - 2)):\n    for j in range(image_GRAY_pad.shape[1] - 2):\n        # Extracting (slicing) a 3x3 patch (the same size with filter)\n        # from input image with pad frame\n        patch = image_GRAY_pad[i:i+3, j:j+3]\n\n        # Applying elementwise multiplication and summation -\n        # this is convolution operation\n        # When we use '*' with matrices, then elementwise multiplication\n        # will be applied\n\n        # With Sobel filter\n        output[i, j] = np.sum(patch * f1)\n\n        \n# Measuring spent time\n# End point\nend = timer()\n\n\n# Check points\n# Showing computed time\nprint('Convolution is successfully applied ' + '\\U0001F44C')\nprint('Time : {0:.5f} seconds'.format(end - start))\n","79698b55":"# If you're using environment for GPU, there might be an issue like:\n'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n# In this case, try out following options:\n# Option-1: close all Jupyter Notebooks, and re-open just this one Jupyter Notebook\n# Option-2: switch to the environment for CPU usage only instead of GPU\n\n\n# Initializing Conv2D layer for GRAY input\nlayer = tf.keras.layers.Conv2D(filters=1,\n                               kernel_size=(3, 3),\n                               strides=1,\n                               padding='same',\n                               activation='relu',\n                               input_shape=x_input_GRAY.shape[1:],\n                               use_bias=False,\n                               kernel_initializer=tf.keras.initializers.constant(f1))\n\n\n# Measuring spent time\n# Start point\nstart = timer()\n\n\n# Passing GRAY input to the Tensorflow Conv2D layer\noutput = layer(x_input_GRAY)\n\n\n# Measuring spent time\n# End point\nend = timer()\n\n\n# Check points\n# Showing computed time\nprint('Convolution is successfully applied ' + '\\U0001F44C')\nprint('Time : {0:.5f} seconds'.format(end - start))\n","2498d3c1":"# Reshaping filter to get following: (height, width, channels, filters)\nf_new = f1.reshape(3, 3, 1, 1)\n\n\n# Initializing Keras model for GRAY input and set of filters\nmodel_GRAY = Sequential()\n\n\n# Adding Conv2D layer to the model for GRAY input\nmodel_GRAY.add(Conv2D(filters=1,\n                      kernel_size=(3, 3),\n                      strides=1,\n                      padding='same',\n                      activation='relu',\n                      input_shape=x_input_GRAY.shape[1:],\n                      use_bias=False,\n                      weights=[f_new]))\n\n\n# Measuring spent time\n# Start point\nstart = timer()\n\n\n# Passing GRAY input to the Keras Conv2D layer\noutput = model_GRAY.predict(x_input_GRAY)\n\n\n# Measuring spent time\n# End point\nend = timer()\n\n\n# Check points\n# Showing computed time\nprint('Convolution is successfully applied ' + '\\U0001F44C')\nprint('Time : {0:.5f} seconds'.format(end - start))\n","eca91b99":"# If you're using environment for GPU, there might be an issue like:\n'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n# In this case, try out following options:\n# Option-1: close all Jupyter Notebooks, and re-open just this one Jupyter Notebook\n# Option-2: switch to the environment for CPU usage only instead of GPU\n\n\n# Defining 'VideoCapture' object and reading video from a file\nvideo = cv2.VideoCapture('\/kaggle\/input\/images-for-testing\/polar_bear.mp4')\n\n\n# Preparing variables for writers that will be used to write processed frames into video files\nwriter_BGR = None\nwriter_GRAY = None\n\n\n# Preparing variables for spatial dimensions of the captured frames\nh, w = None, None\n\n\n# Getting version of OpenCV that is currently used\n# Converting string into the list by dot as separator and getting first number\nv = cv2.__version__.split('.')[0]\n\n\n# Defining variable for counting frames\n# At the end we will show total amount of processed frames\nf = 0\n\n\n# Defining variable for calculating total spent time\n# At the end we will show time spent for processing all frames\nt = 0\n\n\n# Defining loop for capturing frames\nwhile True:\n    # Capturing frame-by-frame\n    ret, frame_BGR = video.read()\n\n    # If the frame was not retrieved\n    # e.g.: at the end of the video,\n    # then we break the loop\n    if not ret:\n        break\n\n    # Getting spatial dimension of the frame\n    # We do it only once from the very beginning\n    # All other frames have the same dimensions\n    if w is None or h is None:\n        # Getting shape of the caught frame\n        (h, w, c) = frame_BGR.shape\n        \n        \n        # Initializing Conv2D layer for RGB input\n        # We do it only once from the very beginning\n        layer = tf.keras.layers.Conv2D(filters=1,\n                                       kernel_size=(3, 3),\n                                       strides=1,\n                                       padding='same',\n                                       activation='relu',\n                                       input_shape=(h, w, c),\n                                       use_bias=False,\n                                       kernel_initializer=tf.keras.initializers.constant(f1_3))\n        \n        \n    \"\"\"\n    Start of:\n    Implementing 2D convolution to the captured frame\n    \"\"\"\n    \n    # Converting captured frame to RGB by OpenCV function\n    frame_RGB = cv2.cvtColor(frame_BGR, cv2.COLOR_BGR2RGB)\n    \n    \n    # Reshaping frame to get following: (batch size, rows, columns, channels)\n    x_input_RGB = frame_RGB.reshape(1, h, w, c).astype(np.float32)\n    \n    \n    # Passing RGB input to the initialized Conv2D layer\n    # Calculating time spent for 2D convolution\n    start = timer()\n    output = layer(x_input_RGB)\n    end = timer()\n    \n    \n    # Slicing from the output just feature map\n    # Converting output Tensor into Numpy array\n    output = np.array(output[0, :, :, 0])\n    \n    \n    # To exclude values that are less than 0 and more than 255,\n    # Numpy function 'clip' is applied\n    # It keeps values of Numpy array in the given range\n    # And it replaces non-needed values with boundary numbers\n    output = np.clip(output, 0, 255).astype(np.uint8)\n    \n    \n    # Increasing counters for frames and total spent time\n    f += 1\n    t += end - start\n    \n    \n    # Showing spent time for single current frame\n    print('Frame number {0} took {1:.5f} seconds'.format(f, end - start))\n    \n\n    \"\"\"\n    End of:\n    Implementing 2D convolution to the captured frame\n    \"\"\"\n    \n    \n    \"\"\"\n    Start of:\n    Finding contours\n    \"\"\"\n    \n    # Finding contours\n    # (!) Different versions of OpenCV returns different number of parameters\n    # when using function cv2.findContours()\n\n    # In OpenCV version 3 function cv2.findContours() returns three parameters:\n    # modified image, found contours and hierarchy\n    # All found contours from current frame are stored in the list\n    # Each individual contour is a Numpy array of(x, y) coordinates\n    # of the boundary points of the object\n    # We are interested only in contours\n\n    # Checking if OpenCV version 3 is used\n    if v == '3':\n        _, contours, _ = cv2.findContours(output, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n    \n    \n    \n    \n    # In OpenCV version 4 function cv2.findContours() returns two parameters:\n    # found contours and hierarchy\n    # All found contours from current frame are stored in the list\n    # Each individual contour is a Numpy array of(x, y) coordinates\n    # of the boundary points of the object\n    # We are interested only in contours\n\n    # Checking if OpenCV version 4 is used\n    else:\n        contours, _ = cv2.findContours(output, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n    \n    \n    \n    \n    # Finding the biggest contour by sorting from biggest to smallest\n    contours = sorted(contours, key=cv2.contourArea, reverse=True)\n    \n    \n    \"\"\"\n    End of:\n    Finding contours\n    \"\"\"\n    \n    \n    \"\"\"\n    Start of:\n    Drawing bounding box and label\n    \"\"\"\n    \n    # Extracting coordinates of the biggest contour if any was found\n    if contours:\n        # Getting rectangle coordinates and spatial dimension of the biggest contour\n        # Function cv2.boundingRect() is used to get an approximate rectangle\n        # around the region of interest in the binary image after contour was found\n        (x_min, y_min, box_width, box_height) = cv2.boundingRect(contours[0])\n        \n        \n        # Drawing bounding box on the current BGR frame\n        cv2.rectangle(frame_BGR,\n                      (x_min, y_min),\n                      (x_min + box_width, y_min + box_height),\n                      (0, 255, 0),\n                      3)\n        \n        \n        # Preparing text for the label\n        label = 'Polar Bear'\n        \n        \n        # Putting text with label on the current BGR frame\n        cv2.putText(frame_BGR,\n                    label,\n                    (x_min - 5, y_min - 25),\n                    cv2.FONT_HERSHEY_SIMPLEX,\n                    1.5,\n                    (0, 255, 0),\n                    2)        \n        \n    \"\"\"\n    End of:\n    Drawing bounding box and label\n    \"\"\"\n    \n    \n    \"\"\"\n    Start of:\n    Writing processed frames into the files\n    \"\"\"\n    \n    # Initializing writer for BGR frame\n    # we do it only once from the very beginning\n    # when we get spatial dimensions of the frames\n    if writer_BGR is None:\n        # Constructing code of the codec\n        # to be used in the function VideoWriter\n        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n\n        # Writing current processed BGR frame into the video file\n        writer_BGR = cv2.VideoWriter('result_RGB.mp4',\n                                     fourcc,\n                                     30,\n                                     (w, h),\n                                     True)\n        \n        \n    # Initializing writer for GRAY output\n    # we do it only once from the very beginning\n    # when we get spatial dimensions of the frames\n    if writer_GRAY is None:\n        # Constructing code of the codec\n        # to be used in the function VideoWriter\n        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n\n        # Writing current processed GRAY output into the video file\n        writer_GRAY = cv2.VideoWriter('result_GRAY.mp4',\n                                      fourcc,\n                                      30,\n                                      (w, h),\n                                      False)\n        \n        \n    # Write processed current BGR frame to the file\n    writer_BGR.write(frame_BGR)\n    \n    # Write processed current GRAY output to the file\n    writer_GRAY.write(output)\n    \n    \n    \"\"\"\n    End of:\n    Writing processed frames into the files\n    \"\"\"\n    \n    \n# Printing final results\nprint()\nprint('Total number of frames is              :', f)\nprint('Total spent time for 2D convolution is : {:.5f} seconds'.format(t))\nprint('FPS rate is                            :', round((f \/ t), 1))\nprint('\\n')\n\n\n# Releasing video reader and writers\nvideo.release()\nwriter_BGR.release()\nwriter_GRAY.release()\n","c6888b0a":"import numpy as np\n\n\nprint(help(np.copy))\nprint('\\n')\nprint(help(np.clip))\n","af239b63":"import tensorflow as tf\n\n\nprint(help(tf.keras.layers.Conv2D))\nprint('\\n')\n","6743bb29":"from keras.models import Sequential\nfrom keras.layers import Conv2D\nfrom keras.utils import plot_model\n\n\nprint(help(Sequential))\nprint('\\n')\nprint(help(Conv2D))\nprint('\\n')\nprint(help(plot_model))\nprint('\\n')\nprint(help(Sequential.get_weights))\nprint('\\n')\nprint(help(Sequential.set_weights))\nprint('\\n')\nprint(help(Sequential.load_weights))\nprint('\\n')\nprint(help(Sequential.predict))\n","ed315cf1":"import cv2\n\n\nprint(help(cv2.VideoCapture))\nprint('\\n')\nprint(help(cv2.findContours))\nprint('\\n')\nprint(help(cv2.rectangle))\nprint('\\n')\nprint(help(cv2.putText))\nprint('\\n')\nprint(help(cv2.VideoWriter_fourcc))\nprint('\\n')\nprint(help(cv2.VideoWriter))\nprint('\\n')\n","f8a873c8":"# Prewitt filter to detect vertical changes on image\nf3 = np.array([[1, 0, -1], \n               [1, 0, -1], \n               [1, 0, -1]])\n","d17cd3e5":"# Prewitt filter to detect horizontal changes on image\nf4 = np.array([[1, 1, 1],  \n               [0, 0, 0],  \n               [-1, -1, -1]])\n","725b9932":"# Sobel filter to detect horizontal changes on image\nf5 = np.array([[1, 2, 1],  \n               [0, 0, 0],  \n               [-1, -2, -1]])\n","35e2e7dc":"# Gaussian blur filter\nf6 = (1 \/ 16) * np.array([[1, 2, 1],\n                          [2, 4, 2],\n                          [1, 2, 1]])\n","951401b2":"### 4.5.6 Visualize obtained feature maps for GRAY input","b36b2de5":"### 1.2 Visualize input image","cc7f5c3d":"### 4.6.2 Pass GRAY and RGB input through Tensorflow Conv2D layer","6af89600":"### 4.1.3 Slice from the output just feature map","1f21f297":"### 4.4.3 Get current weights","94d542ee":"### 4.4.6 Visualize obtained feature map","3352e8df":"### 4.4.2 Pass RGB input through Tensorflow Conv2D layer","f59a7146":"## 4.5 How to use set of filters in one layer?","aefe0221":"#  \u27b0 Step 3: Apply 2D convolution by 'for' loops","cb66b70f":"### 3.3 Apply 2D convolution by 'for' loops to GRAY input","042ef9c8":"### 4.1.4 Exclude non-needed values (less than 0 and more than 255)","403d68d6":"### 4.2.3 Get current weights and bias","64cfd804":"### 4.1.2 Pass GRAY input through Tensorflow Conv2D layer","729337b3":"### 4.5.9 Get current weights for RGB input","27216c91":"## 4.2 What if bias is True?","bb44d224":"### 2.3 Assemble 1-channeled filters into united array","73f354b8":"\ud83d\udcdc **Sub-Content:**  \n  \n\ud83d\udca0 4.3.**1** **Set** up weights  \n\ud83d\udca0 4.3.**2** **Initialize** 2D Convolutional Layer by Tensorflow for GRAY input  \n\ud83d\udca0 4.3.**3** **Pass** GRAY input through Tensorflow Conv2D layer  \n\ud83d\udca0 4.3.**4** **Visualize** obtained feature map  \n","b40c1fe2":"### 4.5.1 Initialize 2D Convolutional Layer by Tensorflow for GRAY input and set of filters","f956404c":"\ud83d\udcdc **Content:**  \n  \n\ud83d\udca0 6.**1** Time spent for 2D convolution by **'for' loop**  \n\ud83d\udca0 6.**2** Time spent for 2D convolution by **Tensorflow**  \n\ud83d\udca0 6.**3** Time spent for 2D convolution by **Keras**  \n","b6c95ba1":"# \ud83d\uddbc\ufe0f Step 1: Load input image","12f39222":"### 3.4 Exclude non-needed values (less than 0 and more than 255)","c15b505e":"\ud83d\udcdc **Content:**  \n  \n\ud83d\udca0 5.**1** **Set** up weights for GRAY and RGB input  \n\ud83d\udca0 5.**2** **Initialize** 2D Convolutional Layer by Keras for GRAY and RGB input and set of filters  \n\ud83d\udca0 5.**3** **Display** built CNN models  \n\ud83d\udca0 5.**4** **Pass** GRAY and RGB input through Keras Conv2D layer  \n\ud83d\udca0 5.**5** **Visualize** obtained feature maps for GRAY and RGB input  \n","a7a331e2":"### 5.2 Initialize 2D Convolutional Layer by Keras for GRAY and RGB input and set of filters","24e106ca":"### 4.2.2 Pass GRAY input through Tensorflow Conv2D layer","fdffe3b2":"### 3.2 Apply pad frame to GRAY input","684906e2":"\u2714\ufe0f To get more details for usage of ***Numpy functions:***  \n> print(help(**np.copy**))  \n> print(help(**np.clip**))  \n  \n\ud83d\udd17 More details and examples are here:  \n> https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.copy.html  \n> https:\/\/numpy.org\/devdocs\/reference\/generated\/numpy.clip.html  \n  \n  <br\/>\n  \n\u2714\ufe0f To get more details for usage of ***Tensorflow:***  \n> print(help(**tf.keras.layers.Conv2D**))  \n  \n\ud83d\udd17 More details and examples are here:  \n> https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/Conv2D  \n  \n  <br\/>\n  \n\u2714\ufe0f To get more details for usage of ***Keras:***  \n> print(help(**Sequential**))  \n> print(help(**Conv2D**))  \n> print(help(**plot_model**))  \n> print(help(**Sequential.get_weights**))  \n> print(help(**Sequential.set_weights**))  \n> print(help(**Sequential.load_weights**))  \n> print(help(**Sequential.predict**))  \n  \n\ud83d\udd17 More details and examples are here:  \n> https:\/\/keras.io\/api\/models\/sequential\/  \n> https:\/\/keras.io\/api\/layers\/convolution_layers\/convolution2d\/  \n> https:\/\/keras.io\/api\/utils\/model_plotting_utils\/#plot_model-function  \n  \n  <br\/>\n  \n\u2714\ufe0f To get more details for usage of ***OpenCV functions:***  \n> print(help(**cv2.VideoCapture**))  \n> print(help(**cv2.findContours**))  \n> print(help(**cv2.rectangle**))  \n> print(help(**cv2.putText**))  \n> print(help(**cv2.VideoWriter_fourcc**))  \n> print(help(**cv2.VideoWriter**))  \n  \n\ud83d\udd17 More details and examples are here:  \n> https:\/\/docs.opencv.org\/3.4\/d8\/dfe\/classcv_1_1VideoCapture.html  \n> https:\/\/docs.opencv.org\/3.4\/d4\/d73\/tutorial_py_contours_begin.html  \n> https:\/\/docs.opencv.org\/4.0.0\/d4\/d73\/tutorial_py_contours_begin.html  \n> https:\/\/docs.opencv.org\/master\/dc\/da5\/tutorial_py_drawing_functions.html  \n> https:\/\/docs.opencv.org\/3.4\/dd\/d9e\/classcv_1_1VideoWriter.html  \n> http:\/\/www.fourcc.org  \n  \n  <br\/>\n  ","8345bbc5":"### 5.5 Visualize obtained feature maps for GRAY and RGB input","7d4ab47b":"### 4.4.5 Pass RGB input through Tensorflow Conv2D layer with updated weights","c5e499f4":"#  \ud83d\udd06 Step 4: Apply 2D convolution by Tensorflow","5b512da5":"# \ud83d\udcfd\ufe0f Step 7: Implement 2D convolution on video","48ffef26":"### 1.3 Prepare input to the 2D Convolutional Layer","cddcf633":"### 4.1.1 Initialize 2D Convolutional Layer by Tensorflow for GRAY input","44a8ae27":"## 4.6 Visualize final results","d4211601":"# \ud83d\uddd2\ufe0f Some comments","8bab12fa":"### 3.5 Visualize obtained feature map","426b9795":"### 4.5.10 Set up new weights for RGB input","2f059615":"### 4.3.3 Pass GRAY input through Tensorflow Conv2D layer","eb8d1c95":"# \ud83c\udf93 Course:  Convolutional Neural Networks for Image Classification\n\n## &nbsp; \u26e9\ufe0f Section-9\n### &nbsp; &nbsp; \ud83c\udf9b\ufe0f 2D Image Convolution: Numpy, Tensorflow, Keras\n\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;**Description:**  \n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;*Explained theory on how **2D Convolutional Layer** creates **feature maps**.*  \n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;*Instructions on how to use **Tensorflow** and **Keras** functions **\u2018get_weights()\u2019** and **\u2018set_weights()\u2019**.*  \n\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;**File:** *2d_image_convolution.ipynb*","a48a6268":"**Design**, **Train** & **Test** deep CNN for Image Classification.\n\n**Join** the course & enjoy new opportunities to get deep learning skills:\n\n\n[https:\/\/www.udemy.com\/course\/convolutional-neural-networks-for-image-classification\/](https:\/\/www.udemy.com\/course\/convolutional-neural-networks-for-image-classification\/?referralCode=12EE0D74A405BF4DDE9B)\n\n\n![](https:\/\/github.com\/sichkar-valentyn\/1-million-images-for-Traffic-Signs-Classification-tasks\/blob\/main\/images\/slideshow_classification.gif?raw=true)\n","a1f30c06":"**Preparing hyperparameters for convolution**  \nTo get convolved image with the same dimension as input image, it is needed to set following:  \n  \n> <font color='#00a1e6' size=3>$f_{size} = 3$<\/font>, filter size (kernel), width and height are equal;  \n> <font color='#00a1e6' size=3>$stride = 1$<\/font>, stride (step) for sliding;  \n> <font color='#00a1e6' size=3>$pad = 1$<\/font>, pad to process boundaries (zero valued frame around image).  \n\nOutput image dimension is calculated by following equations:  \n> <font color='#00a1e6' size=4>$height_{output} = \\frac {height_{input} - f_{size} + 2 * pad}{step} + 1$<\/font>  \n>  \n> <font color='#00a1e6' size=4>$width_{output} = \\frac {width_{input} - f_{size} + 2 * pad}{step} + 1$<\/font>\n  \nFor instance, dimension of the input GRAY image is 853x1280 (height and width), then dimension of the output image will be as following:  \n> <font color='#00a1e6' size=4>$height_{output} = \\frac {853 - 3 + 2 * 1}{1} + 1 = 853$<\/font>  \n>  \n> <font color='#00a1e6' size=4>$width_{output} = \\frac {1280 - 3 + 2 * 1}{1} + 1 = 1280$<\/font>  \n","21faf7ee":"### 2.4 Assemble 3-channeled filters into united array","250d8d16":"### 4.5.5 Pass GRAY input through Tensorflow Conv2D layer with updated weights","28ee60cc":"\ud83d\udcdc **Content:**  \n  \n\ud83d\udca0 3.**1** **Set up** hyperparameters  \n\ud83d\udca0 3.**2** **Apply** pad frame to GRAY input  \n\ud83d\udca0 3.**3** **Apply** 2D convolution by 'for' loops to GRAY input  \n\ud83d\udca0 3.**4** **Exclude** non-needed values (less than 0 and more than 255)  \n\ud83d\udca0 3.**5** **Visualize** obtained feature map  \n","c6f6d45f":"### 4.6.1 Initialize 2D Convolutional Layer by Tensorflow for GRAY and RGB input and set of filters","571b7414":"### \ud83d\udca1 Algorithm:\n\n**\u2714\ufe0f Step 1:** Load input image  \n**\u2714\ufe0f Step 2:** Set up filters for **edge detection**  \n**\u2714\ufe0f Step 3:** Apply 2D convolution by **'for' loops**  \n**\u2714\ufe0f Step 4:** Apply 2D convolution by **Tensorflow**  \n**\u2714\ufe0f Step 5:** Apply 2D convolution by **Keras**  \n**\u2714\ufe0f Step 6:** Compute **time** spent for 2D convolution  \n**\u2714\ufe0f Step 7:** Implement 2D convolution on **video**  \n  \n  \n### \ud83c\udfaf **Result:**  \n**\u2705 Plot** of GRAY images with detected edges  \n**\u2705 Time** spent for 2D convolution by different approaches  \n**\u2705 Video** of GRAY object with detected edge  \n**\u2705 Video** of RGB object with bounding box and label  \n  ","ee14f75d":"### 6.1 Time spent for 2D convolution by 'for' loop","d0e8afd8":"### 4.1.8 Pass GRAY input through Tensorflow Conv2D layer with updated weights","cca48224":"### 6.2 Time spent for 2D convolution by Tensorflow","99f6b6e6":"## 4.1 Implement kernel_initializer","93143983":"### 4.3.1 Set up weights","fe25dcbb":"### 4.1.9 Visualize newly obtained feature map","435bb8f9":"### 2.2 Extend 1-channeled filters to 3-channeled with identical channels","0f6d0f6d":"### 4.3.2 Initialize 2D Convolutional Layer by Tensorflow for GRAY input","91ed9f3d":"### 4.3.4 Visualize obtained feature map","9497dac3":"### 4.1.5 Visualize obtained feature map","6ec9623a":" # \u23f1\ufe0f Step 6: Compute time spent for 2D convolution","1bd6d62d":"### 5.1 Set up weights for GRAY and RGB input","e2b9c1eb":"### 2.1 Define 1-channeled common 3x3 filters (kernels) for edge detection","f9b22377":"\ud83d\udcdc **Sub-Content:**  \n  \n\ud83d\udca0 4.2.**1** **Initialize** 2D Convolutional Layer by Tensorflow for GRAY input  \n\ud83d\udca0 4.2.**2** **Pass** GRAY input through Tensorflow Conv2D layer  \n\ud83d\udca0 4.2.**3** **Get** current weights and bias  \n\ud83d\udca0 4.2.**4** **Set up** new weights and bias  \n\ud83d\udca0 4.2.**5** **Pass** GRAY input through Tensorflow Conv2D layer with updated weights and bias  \n\ud83d\udca0 4.2.**6** **Visualize** newly obtained feature map  \n","ade400d3":"### 4.5.2 Pass GRAY input through Tensorflow Conv2D layer with set of filters","da4e4da2":"### 4.2.5 Pass GRAY input through Tensorflow Conv2D layer with updated weights and bias","7a26fe4c":"## 4.3 Implement weights=[]","ed1e3f78":"### 4.5.8 Pass RGB input through Tensorflow Conv2D layer with set of filters","571e2239":"\ud83d\udcdc **Content:**  \n  \n\ud83d\udca0 2.**1** **Define** 1-channeled common 3x3 filters (kernels) for edge detection  \n\ud83d\udca0 2.**2** **Extend** 1-channeled filters to 3-channeled with identical channels  \n\ud83d\udca0 2.**3** **Assemble** 1-channeled filters into united array  \n\ud83d\udca0 2.**4** **Assemble** 3-channeled filters into united array  \n","710fa601":"### 4.4.4 Set up new weights","71b0b652":"### 4.1.7 Set up new weights","bf03acc5":"### 4.5.4 Set up new weights for GRAY input","e69e1d1e":"\ud83d\udcdc **Content:**  \n  \n\ud83d\udca0 1.**1** **Load** input image by OpenCV  \n\ud83d\udca0 1.**2** **Visualize** input image  \n\ud83d\udca0 1.**3** **Prepare** input to the 2D Convolutional Layer  \n","6fa7a5e0":"### 4.6.3 Visualize obtained feature maps for GRAY and RGB input","65302e5a":"# \ud83d\udce5 Import libraries","96f35c0e":"# \ud83c\udf93 Related course for classification tasks","da3031f6":"\ud83d\udcdc **Sub-Content:**  \n  \n\ud83d\udca0 4.5.**1** **Initialize** 2D Convolutional Layer by Tensorflow for **GRAY input** and set of filters  \n\ud83d\udca0 4.5.**2** **Pass** GRAY input through Tensorflow Conv2D layer with set of filters  \n\ud83d\udca0 4.5.**3** **Get** current weights for GRAY input  \n\ud83d\udca0 4.5.**4** **Set up** new weights for GRAY input  \n\ud83d\udca0 4.5.**5** **Pass GRAY** input through Tensorflow Conv2D layer with updated weights  \n\ud83d\udca0 4.5.**6** **Visualize** obtained feature maps for GRAY input  \n\ud83d\udca0 4.5.**7** **Initialize** 2D Convolutional Layer by Tensorflow for **RGB input** and set of filters  \n\ud83d\udca0 4.5.**8** **Pass** RGB input through Tensorflow Conv2D layer with set of filters  \n\ud83d\udca0 4.5.**9** **Get** current weights for RGB input  \n\ud83d\udca0 4.5.**10** **Set up** new weights for RGB input  \n\ud83d\udca0 4.5.**11** **Pass RGB** input through Tensorflow Conv2D layer with updated weights  \n\ud83d\udca0 4.5.**12** **Visualize** obtained feature maps for RGB input  \n","848b8633":"\ud83d\udcdc **Sub-Content:**  \n  \n\ud83d\udca0 4.4.**1** **Initialize** 2D Convolutional Layer by Tensorflow for RGB input  \n\ud83d\udca0 4.4.**2** **Pass** RGB input through Tensorflow Conv2D layer  \n\ud83d\udca0 4.4.**3** **Get** current weights  \n\ud83d\udca0 4.4.**4** **Set up** new weights  \n\ud83d\udca0 4.4.**5** **Pass** RGB input through Tensorflow Conv2D layer with updated weights  \n\ud83d\udca0 4.4.**6** **Visualize** obtained feature map  \n","34a41a53":"## 4.4 What if input image is RGB?","6ead74f1":"\ud83d\udcdc **Sub-Content:**  \n  \n\ud83d\udca0 4.6.**1** **Initialize** 2D Convolutional Layer by Tensorflow for GRAY and RGB input and set of filters  \n\ud83d\udca0 4.6.**2** **Pass** GRAY and RGB input through Tensorflow Conv2D layer  \n\ud83d\udca0 4.6.**3** **Visualize** obtained feature maps for GRAY and RGB input  \n","e723b7c3":"### 6.3 Time spent for 2D convolution by Keras","27229eb5":"\ud83d\udcdc **Sub-Content:**  \n  \n\ud83d\udca0 4.1.**1** **Initialize** 2D Convolutional Layer by Tensorflow for GRAY input  \n\ud83d\udca0 4.1.**2** **Pass** GRAY input through Tensorflow Conv2D layer  \n\ud83d\udca0 4.1.**3** **Slice** from the output just feature map  \n\ud83d\udca0 4.1.**4** **Exclude** non-needed values (less than 0 and more than 255)  \n\ud83d\udca0 4.1.**5** **Visualize** obtained feature map  \n\ud83d\udca0 4.1.**6** **Get** current weights  \n\ud83d\udca0 4.1.**7** **Set up** new weights  \n\ud83d\udca0 4.1.**8** **Pass** GRAY input through Tensorflow Conv2D layer with updated weights  \n\ud83d\udca0 4.1.**9** **Visualize** newly obtained feature map  \n","1fa30905":"### 4.5.7 Initialize 2D Convolutional Layer by Tensorflow for RGB input and set of filters","45b15760":"### 3.1 Set up hyperparameters","128533a7":"### 4.5.11 Pass RGB input through Tensorflow Conv2D layer with updated weights","bc21b9b8":"### 4.5.12 Visualize obtained feature maps for RGB input","f138bd7a":"### 4.5.3 Get current weights for GRAY input","2885370d":"### 1.1 Load input image by OpenCV","6b8f60e5":"### 5.3 Display built CNN models","68e13929":"### 5.4 Pass GRAY and RGB input through Keras Conv2D layer","9b692782":"\ud83d\udcdc **Content:**  \n  \n\ud83d\udca0 4.**1** **Implement** *kernel_initializer*  \n\ud83d\udca0 4.**2** What if **bias** is **True?**  \n\ud83d\udca0 4.**3** **Implement** *weights=[]*  \n\ud83d\udca0 4.**4** What if **input** image is **RGB?**  \n\ud83d\udca0 4.**5** How to use **set of filters** in one layer?  \n\ud83d\udca0 4.**6** Visualize **final results**  \n","ed881089":"### 4.2.6 Visualize newly obtained feature map","64eeeab2":"### 4.4.1 Initialize 2D Convolutional Layer by Tensorflow for RGB input","1528d2c8":"### 4.1.6 Get current weights","5b929e0b":"#  \ud83c\udf00 Step 2: Set up filters for edge detection","bb4b3dee":"#  \ud83d\udd18 Step 5: Apply 2D convolution by Keras","4468c327":"### 4.2.1 Initialize 2D Convolutional Layer by Tensorflow for GRAY input","a5d88b04":"### 4.2.4 Set up new weights and bias"}}