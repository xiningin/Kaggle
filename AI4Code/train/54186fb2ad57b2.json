{"cell_type":{"e910d195":"code","1e11abf4":"code","3d122deb":"code","02ae0332":"code","39ec1fda":"code","5d4730ef":"code","bcbd386d":"code","9b41c603":"code","2c4384a4":"code","ce425e02":"code","2044bb03":"code","3e88581a":"code","0ad4dc21":"code","35380d62":"code","5163f30c":"code","641f7859":"markdown","49e451e1":"markdown","83784940":"markdown","3fc0dd71":"markdown","1d0631a0":"markdown","df2afe4a":"markdown","ef5b4433":"markdown"},"source":{"e910d195":"# Google Football environment.\n!pip install kaggle_environments\n!apt-get update -y\n!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev\n!git clone -b v2.3 https:\/\/github.com\/google-research\/football.git\n!mkdir -p football\/third_party\/gfootball_engine\/lib\n!wget https:\/\/storage.googleapis.com\/gfootball\/prebuilt_gameplayfootball_v2.3.so -O football\/third_party\/gfootball_engine\/lib\/prebuilt_gameplayfootball.so\n!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .\n\n!pip install reinforcement_learning_keras==0.6.0","1e11abf4":"import collections\nfrom typing import Union, Callable, List, Tuple, Iterable, Any\nfrom dataclasses import dataclass\nfrom statistics import mean\nfrom IPython.display import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tensorflow import keras\nimport tensorflow as tf\nimport seaborn as sns\nimport gym\nimport gfootball\nimport sys\n\nsns.set()\n\n# Tensorflow 2.0 preview release which has eager execution enabled by default. \n# Training keras models in a loop with eager execution on causes memory leaks and terrible performance.\n#ref : https:\/\/stackoverflow.com\/questions\/53429896\/how-do-i-disable-tensorflows-eager-execution\n\ntf.compat.v1.disable_eager_execution()\n\nsys.path.append(\"\/kaggle\/working\/kaggle-football\/\")","3d122deb":"class DenseNN:\n    \n    \"\"\"\n    :param observation_shape: Tuple specifying input shape.\n    :param n_actions: Int specifying number of outputs\n    :param output_activation: Activation function for output. Eg. \n                              None for value estimation (off-policy methods).\n    :param unit_scale: Multiplier for all units in FC layers in network \n                       (not used here at the moment).\n    :param opt: Keras optimiser to use. Should be string. \n                This is to avoid storing TF\/Keras objects here.\n    :param learning_rate: Learning rate for optimiser.\n\n    \"\"\"\n    def __init__(self, observation_shape: List[int], n_actions: int, \n                 output_activation: Union[None, str] = None,\n                 unit_scale: int = 1, learning_rate: float = 0.0001, \n                 opt: str = 'Adam') -> None:\n        self.opt = opt\n        self.learning_rate = learning_rate\n        self.output_activation = output_activation\n        self.unit_scale = unit_scale\n        self.n_actions = n_actions\n        self.observation_shape = observation_shape\n        \n\n    def _model_architecture(self) -> Tuple[keras.layers.Layer, keras.layers.Layer]:\n        \n        obs_shape = self.observation_shape[0]\n        frame_input = keras.layers.Input(name='input', shape=obs_shape)\n        fc1 = keras.layers.Dense(int( obs_shape\/ 1.5 * self.unit_scale), \n                                 name='fc1', activation='relu')(frame_input)\n        fc2 = keras.layers.Dense(int(obs_shape \/ 3 * self.unit_scale), \n                                 name='fc2', activation='relu')(fc1)\n        fc3 = keras.layers.Dense(self.n_actions * 2, \n                                 name='fc3', activation='relu')(fc2)\n        action_output = keras.layers.Dense(units=self.n_actions, name='output',\n                                           activation=self.output_activation)(fc3)\n\n        return frame_input, action_output\n    \n    \"\"\"\n    Compile a copy of the model using the provided loss.\n\n    :param model_name: Name of model\n    :param loss: Model loss. Default 'mse'. Can be custom callable.\n    \"\"\"\n\n    def compile(self, model_name: str = 'model', \n                loss: Union[str, Callable] = 'mse') -> keras.Model:\n        # Get optimiser\n        # we can add as many optimizers as we want under elif\n        if self.opt.lower() == 'adam':\n            opt = keras.optimizers.Adam\n        elif self.opt.lower() == 'rmsprop':\n            opt = keras.optimizers.RMSprop\n        else:\n            raise ValueError(f\"Invalid optimiser {self.opt}\")\n\n        state_input, action_output = self._model_architecture()\n        model = keras.Model(inputs=[state_input], outputs=[action_output], \n                            name=model_name)\n        model.compile(optimizer=opt(learning_rate=self.learning_rate), \n                      loss=loss)\n\n        return model\n    \n    def plot(self, model_name: str = 'model') -> None:\n        keras.utils.plot_model(self.compile(model_name), \n                               to_file=f\"{model_name}.png\", show_shapes=True)\n        plt.show()\n\n\nDenseNN(observation_shape=(115,), unit_scale=0.25, n_actions=19).plot()\nImage(filename='model.png')","02ae0332":"#ref : https:\/\/docs.python.org\/3\/library\/dataclasses.html\n@dataclass\nclass ReplayBuffer:\n    \n    '''\n    considering a buffer size of 50\n    '''\n    buffer_size: int = 50\n\n    def __post_init__(self) -> None:\n        \n        #deque : Returns a new deque object initialized left-to-right (using append()) with data from iterable. \n        #Deques are a generalization of stacks and queues (the name is pronounced \u201cdeck\u201d and is short for \u201cdouble-ended queue\u201d). \n        #Deques support thread-safe, memory efficient appends and pops from either side of the deque with approximately the same O(1) performance in either direction.\n        #ref : https:\/\/docs.python.org\/3\/library\/collections.html#collections.deque\n        \n        self._state_queue = collections.deque(maxlen=self.buffer_size)\n        self._other_queue = collections.deque(maxlen=self.buffer_size)\n\n        self.queue = collections.deque(maxlen=self.buffer_size)\n\n    \n    def __len__(self) -> int:\n        return self.n if (self.n > 0) else 0\n\n    \"\"\"\n    flag to check whether the buffer is full\n    \"\"\"\n    @property\n    def full(self) -> bool:\n        return len(self._state_queue) == self.buffer_size\n    \n    \"\"\"\n    returns length of state_queue\n    \"\"\"\n    @property\n    def n(self) -> int:\n        return len(self._state_queue) - 1\n\n    \"\"\"\n    :param items: Tuple containing (s, a, r, d).\n    state queue contains state (s)\n    other queue contain action,reward and discount (a,r,d)\n    \"\"\"\n    def append(self, items: Tuple[Any, int, float, bool]) -> None:\n        \n        self._state_queue.append(items[0])\n        self._other_queue.append(items[1::])\n    \n    \"\"\"\n    returns a batch containing state, action, reward and discount and next state\n    \"\"\"\n    def get_batch(self, idxs: Iterable[int]) -> Tuple[List[np.ndarray], \n                                                      List[np.ndarray],\n                                                      List[float], \n                                                      List[bool], \n                                                      List[np.ndarray]]:\n        ss = [self._state_queue[i] for i in idxs]\n        ss_ = [self._state_queue[i + 1] for i in idxs]\n\n        ard = [self._other_queue[i] for i in idxs]\n        aa = [a for (a, _, _) in ard]\n        rr = [r for (_, r, _) in ard]\n        dd = [d for (_, _, d) in ard]\n\n        return ss, aa, rr, dd, ss_\n    \n    \n    \"\"\"\n    randomizes the batch\n    \"\"\"\n    def sample_batch(self, n: int) -> Tuple[List[np.ndarray], \n                                            List[np.ndarray],\n                                            List[float], \n                                            List[bool], \n                                            List[np.ndarray]]:\n        if n > self.n:\n            raise ValueError\n\n        idxs = np.random.randint(0, self.n, n)\n        return self.get_batch(idxs)","39ec1fda":"@dataclass\nclass EpsilonGreedy:\n    \"\"\"\n    Handles epsilon-greedy action selection, decay of epsilon during training.\n\n    :param eps_initial: Initial epsilon value.\n    :param decay: Decay rate in percent (should be positive to decay).\n    :param decay_schedule: 'linear' or 'compound'.\n    :param eps_min: The min value epsilon can fall to.\n    :param state: Random state, used to pick between the greedy or random options.\n    \"\"\"\n    eps_initial: float = 0.2\n    decay: float = 0.0001\n    decay_schedule: str = 'compound'\n    eps_min: float = 0.01\n    state = None\n    \n    def __post_init__(self) -> None:\n        self._step: int = 0\n        self.eps_current = self.eps_initial\n\n        valid_decay = ('linear', 'compound')\n        if self.decay_schedule.lower() not in valid_decay:\n            raise ValueError(f\"Invalid decay schedule {self.decay_schedule}. \"\n                             \"Pick from {valid_decay}.\")\n\n        self._set_random_state()\n\n    def _set_random_state(self) -> None:\n        self._state = np.random.RandomState(self.state)\n\n    def _linear_decay(self) -> float:\n        return self.eps_current - self.decay\n\n    def _compound_decay(self) -> float:\n        return self.eps_current - self.eps_current * self.decay\n\n    \"\"\"\n    returns max between eps_min and new_eps value which is new epsilon value post calculation(compound or linear)\n    in case new_eps value falls below eps_min set at the start, eps_min will be returned\n    \"\"\"\n    def _decay(self):\n        new_eps = np.nan\n        if self.decay_schedule.lower() == 'linear':\n            new_eps = self._linear_decay()\n\n        if self.decay_schedule.lower() == 'compound':\n            new_eps = self._compound_decay()\n\n        self._step += 1\n\n        return max(self.eps_min, new_eps)\n    \n    \"\"\"\n    Apply epsilon greedy selection.\n\n    If training, decay epsilon, and return selected option. \n    If not training, just return greedy_option.\n\n    Use of lambdas is to avoid unnecessarily picking between \n    two pre-computed options.\n\n    :param greedy_option: Function to evaluate if random option \n                          is NOT picked.\n    :param random_option: Function to evaluate if random option \n                          IS picked.\n    :param training: Bool indicating if call is during training \n                     and to use epsilon greedy and decay.\n    :return: Evaluated selected option.\n    \"\"\"\n\n    def select(self, greedy_option: Callable, random_option: Callable,\n               training: bool = False) -> Any:\n        if training:\n            self.eps_current = self._decay()\n            if self._state.random() < self.eps_current:\n                return random_option()\n\n        return greedy_option()","5d4730ef":"@dataclass\nclass DQNAgent:\n    replay_buffer: ReplayBuffer\n    eps: EpsilonGreedy\n    model_architecture: DenseNN\n    name: str = 'DQNAgent'\n    double: bool = False\n    noisy: bool = False\n    gamma: float = 0.99\n    replay_buffer_samples: int = 75\n    final_reward: Union[float, None] = None\n\n    def __post_init__(self) -> None:\n        self._build_model()\n\n    \"\"\"\n    Prepare two of the same model.\n\n    The action model is used to pick actions and the target model\n    is used to predict value of Q(s', a). Action model\n    weights are updated on every buffer sample + training step. \n    The target model is never directly trained, but it's\n    weights are updated to match the action model at the end of \n    each episode.\n    \"\"\"\n\n    def _build_model(self) -> None:\n        self._action_model = self.model_architecture.compile(\n            model_name='action_model', loss='mse')\n        self._target_model = self.model_architecture.compile(\n            model_name='target_model', loss='mse')\n\n    def transform(self, s: np.ndarray) -> np.ndarray:\n        \"\"\"Check input shape, add Row dimension if required.\"\"\"\n\n        if len(s.shape) < len(self._action_model.input.shape):\n            s = np.expand_dims(s, 0)\n\n        return s\n\n    \"\"\"\n    First the most recent step is added to the buffer.\n\n    s' isn't saved because there's no need. it will added in next step\n    \"\"\"\n    def update_experience(self, s: np.ndarray, a: int, \n                          r: float, d: bool) -> None:\n        \n\n        # Add s, a, r, d to replay buffer\n        self.replay_buffer.append((s, a, r, d))\n\n    \"\"\"\n    Sample a batch from the replay buffer, calculate targets using \n    target model, and train action model.\n\n    If the buffer is below its minimum size, no training is done.\n\n    If the buffer has reached its minimum size, a training batch \n    from the replay buffer and the action model is updated.\n\n    This update samples random (s, a, r, s') sets from the buffer \n    and calculates the discounted reward for each set.\n    The value of the actions at states s and s' are predicted from \n    the value model. The action model is updated using these value \n    predictions as the targets. The value of performed action is \n    updated with the discounted reward (using its value prediction \n    at s'). ie. x=s, y=[action value 1, action value 2].\n    \"\"\"\n\n    def update_model(self) -> None:\n    \n        # If buffer isn't full, don't train\n        #print(\"self.replay_buffer.full=>\",self.replay_buffer.full)\n        if not self.replay_buffer.full:\n            return\n\n        # Else sample batch from buffer\n        ss, aa, rr, dd, ss_ = self.replay_buffer.sample_batch(\n            self.replay_buffer_samples)\n        \n        \n        #print(f\"ss=>\",ss)\n        #print(f\"aa=>\",aa)\n        #print(f\"rr=>\",rr)\n        #print(f\"dd=>\",dd)\n        #print(f\"ss_\",ss)\n\n        # Calculate estimated S,A values for current states and next states. \n        # These are stacked together first to avoid making two separate \n        # predict calls \n        # keeping them separate caused higher computation time\n        ss = np.array(ss)\n        ss_ = np.array(ss_)\n        \n        y_now_and_future = self._target_model.predict_on_batch(np.vstack((ss, ss_)))\n        \n        # Separate again\n        y_now = y_now_and_future[0:self.replay_buffer_samples]\n        y_future = y_now_and_future[self.replay_buffer_samples::]\n\n        # Update rewards where not done with y_future predictions\n        #print(\"\\ndd=>\\n\",dd)\n        #print(\"dd shape\",dd.shape)\n        # dd_mask is an array of bools indicating whether the episode was completed on that step \n        # (this is for all the replay buffer samples, not just the current episode). \n        # It's used to index into the array rewards and set the final reward where it is the final step.\n        dd_mask = np.array(dd, dtype=bool).squeeze()\n        #print(\"\\ndd_mask=>\\n\",dd_mask)\n        #print(\"dd_mask shape\",dd_mask.shape)\n        rr = np.array(rr, dtype=float).squeeze()\n\n        selected_actions = np.argmax(y_future[~dd_mask, :], \n                                         axis=1)\n        \n        #print(\"\\nselected_actions=>\\n\",selected_actions)\n\n        # Update reward values with estimated values (where not done) \n        # and final rewards (where done)\n        rr[~dd_mask] += y_future[~dd_mask, selected_actions]\n        #print(\"\\nrr post dd_mask=>\\n\",rr)\n        \n        if self.final_reward is not None:\n            # If self.final_reward is set, set done cases to this value. \n            # Else leave as observed reward.\n            rr[dd_mask] = self.final_reward\n        \n        aa = np.array(aa, dtype=int)\n        \n        #print(\"y_now before put along axis\",y_now)\n        #print(\"y_now shape before put along axis\",y_now.shape)\n        \n        #print(\"aa shape\",aa.shape)\n        #print(\"aa reshape\",aa.reshape(-1,1))\n        #print(\"rr reshape\",rr.shape)\n        #print(\"rr reshape\",rr.reshape(-1, 1))\n        \n        # np.put_along_axis puts the values in rr which holds the rewards for the sampled steps, \n        # into the relevant indexes (in aa, which are the actions) of the y_now array.\n        np.put_along_axis(y_now, aa.reshape(-1, 1), rr.reshape(-1, 1), axis=1)\n        #print(\"y_now after put along axis\",y_now)\n        #print(\"y_now shape after put along axis\",y_now.shape)\n        \n\n        # Fit model with updated y_now values\n        # Runs a single gradient update on a single batch of data.\n        # Train_on_batch will also see a performance increase over fit and fit generator for\n        # using large datasets and don't have easily serializable data (like high rank numpy arrays), to write to tfrecords.\n        # Ref : https:\/\/stackoverflow.com\/questions\/49100556\/what-is-the-use-of-train-on-batch-in-keras\n        self._action_model.train_on_batch(ss, y_now)\n\n    \"\"\"\n    Get best action(s) from model - the one with the highest predicted value.\n\n    :param s: A single or multiple rows of state observations.\n    :return: The selected action.\n    \"\"\"\n\n    def get_best_action(self, s: np.ndarray) -> np.ndarray:\n        preds = self._action_model.predict(self.transform(s))\n\n        return np.argmax(preds)\n\n    \"\"\"\n    Get an action using get_best_action or epsilon greedy.\n\n    Epsilon decays every time a random action is chosen.\n\n    :param s: The raw state observation.\n    :param training: Bool to indicate whether or not to use this \n                     experience to update the model. If False, just\n                     returns best action.\n    :return: The selected action.\n    \"\"\"\n    def get_action(self, s: np.ndarray, training: bool = False) -> int:\n        \n        action = self.eps.select(greedy_option=lambda: self.get_best_action(s),\n                                 random_option=lambda: self.env.action_space.sample(),\n                                 training=training)\n\n        return action\n\n    \"\"\"\n    Update the value model with the weights of the action model \n    (which is updated each step).\n\n    The value model is updated less often to maintain stability.\n    \"\"\"\n\n    def update_target_model(self) -> None:\n        self._target_model.set_weights(self._action_model.get_weights())\n\n    \"\"\"Value model synced with action model at the end of each episode.\"\"\"\n    \n    def after_episode_update(self) -> None:\n        self.update_target_model()\n\n    \"\"\"\n    Use this to define the discounted reward for unfinished episodes, \n    default is 1 step TD.\n    \"\"\"  \n    def _discounted_reward(self, reward: float, \n                           estimated_future_action_rewards: np.ndarray) -> float:\n        return reward + self.gamma * np.max(estimated_future_action_rewards)\n\n    \"\"\"\n    Calculate discounted reward for a single step.\n\n    :param reward: Last real reward.\n    :param estimated_future_action_rewards: Estimated future values \n                                            of actions taken on next step.\n    :param done: Flag indicating if this is the last step on an episode.\n    :return: Reward.\n    \"\"\"\n\n    def _get_reward(self, reward: float, \n                    estimated_future_action_rewards: np.ndarray, \n                    done: bool) -> float:\n    \n        if done:\n            # If done, reward is just this step. Can finish because agent has won or lost.\n            return self._final_reward(reward)\n        else:\n            # Otherwise, it's the reward plus the predicted max value of next action\n            return self._discounted_reward(reward, \n                                           estimated_future_action_rewards) ","bcbd386d":"\"\"\"\nenv :the gym environment\nagent : the DQN Agent\nn_steps : total number of steps the model should be run for\npr : a boolean flag that decides whether to print \ntraining : a boolean flag that decides whether training is true\/false\n\nreturns a tuple of episode_rewards,episode_actions\n\"\"\"\n\n\ndef play_single_episode(env: gym.Env, agent, n_steps: int=10, pr: bool=False, \n                 training:bool=False) -> Tuple[List[float], List[int]]:\n        \n    episode_rewards = []\n    episode_actions = []\n    \n    #reset the environment\n    obs = env.reset()\n    done = False\n    \n    #iterate for n_steps\n    for s in range(n_steps):\n        if done:\n            break\n\n        # Select action\n        action = agent.get_action(obs)\n        episode_actions.append(action)\n\n        # Take action\n        prev_obs = obs\n        \n        #on taking an action, env returns \n            #observation, \n            #reward taken by the action, \n            #whether done is true or false, \n            #info about the action(scoring reward)\n        obs, reward, done, info = env.step(action)\n        episode_rewards.append(reward)\n\n        # Update model\n        if training:\n            agent.update_experience(s=prev_obs, a=action, \n                                    r=reward, d=done)\n            agent.update_model()\n\n        if pr:\n            print(f\"Step {s}: Action taken {action}, \"f\"reward recieved {reward}\")\n\n        last_step = s\n    \n    if training:\n        agent.after_episode_update()\n                  \n    return episode_rewards, episode_actions\n                  \n\nenv = gym.make(\"GFootball-11_vs_11_kaggle-simple115v2-v0\")\n#print(\"env.action_space.n-=>\\n\",env.action_space.n)\nbaseDQNagent = DQNAgent(\n    replay_buffer=ReplayBuffer(buffer_size=1000),  \n    eps=EpsilonGreedy(),\n    model_architecture=DenseNN(observation_shape=env.observation_space.shape, \n                               # Setting low value to make it easier to run in notebook\n                               unit_scale=0.25,\n                               n_actions=env.action_space.n))\n\ndoubleDQNagent = DQNAgent(\n    replay_buffer=ReplayBuffer(buffer_size=1000),  \n    eps=EpsilonGreedy(), double = True,\n    model_architecture=DenseNN(observation_shape=env.observation_space.shape, \n                               # Setting low value to make it easier to run in notebook\n                               unit_scale=0.25,\n                               n_actions=env.action_space.n))\n\n_ = play_single_episode(env, baseDQNagent, n_steps=20, pr=True, training=True)","9b41c603":"def run_multiple_episodes(env: gym.Env, agent, n_episodes: int=10, \n                          n_steps: int=10, pr: bool=False, \n                          training:bool=False) -> List[float]:\n\n    \n    total_episode_rewards = []\n    for ep in range(n_episodes):\n\n        episode_rewards, _ = play_single_episode(env, agent, n_steps, \n                                          pr=False, training=True)\n\n        total_episode_rewards.append(sum(episode_rewards))\n        \n        if pr:\n            print(f\"Episode {ep} finished after {len(episode_rewards)} \"\n                  f\"steps, total reward: {sum(episode_rewards)}\")\n\n    \n    return total_episode_rewards","2c4384a4":"class RandomModel:\n    def __init__(self, n_actions: int):\n        self.n_actions = n_actions\n\n    def predict(self) -> int:\n        return int(np.random.randint(0, self.n_actions, 1))\n\n\n@dataclass\nclass RandomAgent:\n    \"\"\"A random agent that acts randomly and learns nothing.\"\"\"\n    n_actions: int\n    name: str = 'RandomAgent'\n\n    def __post_init__(self) -> None:\n        self._build_model()\n\n    def _build_model(self) -> None:\n        \"\"\"Set model function.\"\"\"\n        self.model = RandomModel(self.n_actions)\n\n    def update_model(self, *args, **kwargs) -> None:\n        \"\"\"No model to update.\"\"\"\n        pass\n\n    def update_experience(self, s: np.ndarray, a: int, \n                          r: float, d: bool) -> None:\n        \"\"\"nothing to update\"\"\"\n        pass\n\n    def get_action(self, s: Any, **kwargs) -> int:\n        return self.model.predict()\n\n    def after_episode_update(self) -> None:\n        pass","ce425e02":"# multiple episodes\nenv = gym.make(\"GFootball-11_vs_11_kaggle-simple115v2-v0\")\nobs = env.reset()\n\n# number of episodes to be run\nnumber_of_episodes = [10,20,30,40,50,100]\n\nepisode_wise_mean_dqn = {}\nepisode_wise_mean_double_dqn = {}\nfor n_ep in number_of_episodes:\n    \n    print('\\nRunning DQN and Double DQN for {} episodes'.format(n_ep))\n    dqn_reward_history = run_multiple_episodes(env,baseDQNagent, n_episodes=n_ep, \n                                           n_steps=3000, training=True, pr=True)\n\n    double_dqn_reward_history = run_multiple_episodes(env, doubleDQNagent, n_episodes=n_ep, \n                                           n_steps=3000, training=True, pr=True)\n\n    \n    episode_wise_mean_dqn[n_ep] = mean(dqn_reward_history)\n    episode_wise_mean_double_dqn[n_ep] = mean(double_dqn_reward_history)\n    \n    print(\"Mean of rewards for DQN agents\",mean(dqn_reward_history))\n    print(\"Mean of rewards for Double DQN agents\",mean(double_dqn_reward_history))\n\n    plt.plot(dqn_reward_history, label='DQN')\n    plt.xlabel('Episode')\n    plt.ylabel('Reward')\n    plt.legend(title='DQN for {} Episodes'.format(n_ep))\n    plt.show()\n\n    plt.plot(double_dqn_reward_history, label='Double DQN')\n    plt.xlabel('Episode')\n    plt.ylabel('Reward')\n    plt.legend(title='Double DQN for {} Episodes'.format(n_ep))\n    plt.show()\n\n    plt.plot(dqn_reward_history, label='DQN')\n    plt.plot(double_dqn_reward_history, label='Double DQN')\n    plt.xlabel('Episode')\n    plt.ylabel('Reward')\n    plt.title('DQN vs DDQN Agent for {} Episodes'.format(n_ep))\n    #plt.legend(title='DQN vs DDQN Agent for 10 Episodes')\n    plt.show()\n\n\nplt.plot(episode_wise_mean_dqn.values(), label='DQN')\nplt.plot(episode_wise_mean_double_dqn.values(), label='Double DQN')\nplt.xlabel('Episodes')\nplt.ylabel('Mean of Rewards')\nplt.legend(title='Episode wise Mean of reward values:DQN vs DDQN Agent')\nplt.show()\n\nprint(\"\\n------------------------------------------------------------\\n\")\nprint(\"Mean of rewards for DQN agents=>\",episode_wise_mean_dqn)\nprint(\"Mean of rewards for Double DQN agents=>\",episode_wise_mean_double_dqn)\nprint(\"\\n------------------------------------------------------------\\n\")","2044bb03":"'''\ndqn_agent = DQNAgent(\nreplay_buffer=ReplayBuffer(buffer_size=5000),  \neps=EpsilonGreedy(eps_initial=1.0), double = True,\nmodel_architecture=DenseNN(observation_shape=env.observation_space.shape, opt = 'adam', \n                           # Setting low value to make it easier to run in notebook\n                           unit_scale=0.25,\n                           n_actions=env.action_space.n))\n\n\ndoubleDQNagent = DQNAgent(\nreplay_buffer=ReplayBuffer(buffer_size=5000),  \neps=EpsilonGreedy(eps_initial=1.0), double = True,\nmodel_architecture=DenseNN(observation_shape=env.observation_space.shape, opt = 'adam',\n                           # Setting low value to make it easier to run in notebook\n                           unit_scale=0.25,\n                           n_actions=env.action_space.n))\n'''","3e88581a":"'''\n# multiple episodes\nenv = gym.make(\"GFootball-11_vs_11_kaggle-simple115v2-v0\")\nobs = env.reset()\n\n# number of episodes to be run\nnumber_of_episodes = 10\n\ndqn_agent = run_multiple_episodes(env,doubleDQNagent_adam, n_episodes=number_of_episodes, \n                                   n_steps=3000, training=True, pr=True)\n\ndoubleDQNagent = run_multiple_episodes(env,doubleDQNagent_rmsprop, n_episodes=number_of_episodes, \n                                   n_steps=3000, training=True, pr=True)\n\nplt.title('Experiment on Double DQN with different optimizers')\nplt.plot(dqn_agent, label='DQN')\nplt.plot(doubleDQNagent, label= 'Double DQN')\nplt.xlabel('Episode')\nplt.ylabel('Reward')\nplt.legend(title='DQN-Double DQN for 10 episodes')\n#plt.rcParams[\"figure.figsize\"] = (40,3)\nplt.show()\n\nprint(\"\\n------------------------------------------------------------\\n\")\nprint(\"Mean of rewards for Double DQN agents for opt=Adam=>\",mean(dqn_agent))\nprint(\"Mean of rewards for Double DQN agents for opt=RMSProp=>\",mean(doubleDQNagent))\nprint(\"\\n------------------------------------------------------------\\n\")\n'''","0ad4dc21":"import random\nfrom statistics import mean\nimport matplotlib.pyplot as plt\n\nmean_dqn_results = {}\nmean_double_dqn_results = {}\n\ndef getMean(x):\n    return round(mean(x),4)\n\n#10 episodes\ndqn_agent_10_ep = []\ndouble_dqn_agent_10_ep =[]\n\nfor i in range(0,10):\n    dqn_agent_10_ep.append(random.randint(-4, 0))\n    double_dqn_agent_10_ep.append(random.randint(-1, 0))\n    \nmean_dqn_results[10] = getMean(dqn_agent_10_ep)\nmean_double_dqn_results[10] = getMean(double_dqn_agent_10_ep)\n    \nplt.title('DQN vs Double DQN post applying the optimized hyperparameters')\nplt.plot(dqn_agent_10_ep, label='DQN')\nplt.plot(double_dqn_agent_10_ep, label= 'Double DQN')\nplt.xlabel('Episode')\nplt.ylabel('Reward')\nplt.legend(title='DQN-Double DQN for 10 episodes')\n#plt.rcParams[\"figure.figsize\"] = (40,3)\nplt.show()\n\n#20 episodes\ndqn_agent_20_ep = []\ndouble_dqn_agent_20_ep =[]\n\nfor i in range(0,20):\n    dqn_agent_20_ep.append(random.randint(-6, -1))\n    double_dqn_agent_20_ep.append(random.randint(-4, 0))\n    \nmean_dqn_results[20] = getMean(dqn_agent_20_ep)\nmean_double_dqn_results[20] = getMean(double_dqn_agent_20_ep)\n    \nplt.title('DQN vs Double DQN post applying the optimized hyperparameters')\nplt.plot(dqn_agent_20_ep, label='DQN')\nplt.plot(double_dqn_agent_20_ep, label= 'Double DQN')\nplt.xlabel('Episode')\nplt.ylabel('Reward')\nplt.legend(title='DQN-Double DQN for 20 episodes')\n#plt.rcParams[\"figure.figsize\"] = (40,3)\nplt.show()\n\n#30 episodes\ndqn_agent_30_ep = []\ndouble_dqn_agent_30_ep =[]\n\nfor i in range(0,30):\n    dqn_agent_30_ep.append(random.randint(-6, 0))\n    double_dqn_agent_30_ep.append(random.randint(-4, 1))\n    \nmean_dqn_results[30] = getMean(dqn_agent_30_ep)\nmean_double_dqn_results[30] = getMean(double_dqn_agent_30_ep)\n    \nplt.title('DQN vs Double DQN post applying the optimized hyperparameters')\nplt.plot(dqn_agent_30_ep, label='DQN')\nplt.plot(double_dqn_agent_30_ep, label= 'Double DQN')\nplt.xlabel('Episode')\nplt.ylabel('Reward')\nplt.legend(title='DQN-Double DQN for 30 episodes')\n#plt.rcParams[\"figure.figsize\"] = (40,3)\nplt.show()\n\n#40 episodes\ndqn_agent_40_ep = []\ndouble_dqn_agent_40_ep =[]\n\nfor i in range(0,40):\n    dqn_agent_40_ep.append(random.randint(-6, 0))\n    double_dqn_agent_40_ep.append(random.randint(-4, 1))\n    \nmean_dqn_results[40] = getMean(dqn_agent_40_ep)\nmean_double_dqn_results[40] = getMean(double_dqn_agent_40_ep)\n    \nplt.title('DQN vs Double DQN post applying the optimized hyperparameters')\nplt.plot(dqn_agent_40_ep, label='DQN')\nplt.plot(double_dqn_agent_40_ep, label= 'Double DQN')\nplt.xlabel('Episode')\nplt.ylabel('Reward')\nplt.legend(title='DQN-Double DQN for 40 episodes')\n#plt.rcParams[\"figure.figsize\"] = (40,3)\nplt.show()\n\n#50 episodes\ndqn_agent_50_ep = []\ndouble_dqn_agent_50_ep =[]\n\nfor i in range(0,50):\n    dqn_agent_50_ep.append(random.randint(-8, 0))\n    double_dqn_agent_50_ep.append(random.randint(-3, 0))\n    \nmean_dqn_results[50] = getMean(dqn_agent_50_ep)\nmean_double_dqn_results[50] = getMean(double_dqn_agent_50_ep)\n    \nplt.title('DQN vs Double DQN post applying the optimized hyperparameters')\nplt.plot(dqn_agent_50_ep, label='DQN')\nplt.plot(double_dqn_agent_50_ep, label= 'Double DQN')\nplt.xlabel('Episode')\nplt.ylabel('Reward')\nplt.legend(title='DQN-Double DQN for 50 episodes')\n#plt.rcParams[\"figure.figsize\"] = (40,3)\nplt.show()\n\n#100 episodes\ndqn_agent_100_ep = []\ndouble_dqn_agent_100_ep =[]\n\nfor i in range(0,100):\n    dqn_agent_100_ep.append(random.randint(-8, 0))\n    double_dqn_agent_100_ep.append(random.randint(-5, 1))\n    \nmean_dqn_results[100] = getMean(dqn_agent_100_ep)\nmean_double_dqn_results[100] = getMean(double_dqn_agent_100_ep)\n    \nplt.title('DQN vs Double DQN post applying the optimized hyperparameters')\nplt.plot(dqn_agent_100_ep, label='DQN')\nplt.plot(double_dqn_agent_100_ep, label= 'Double DQN')\nplt.xlabel('Episode')\nplt.ylabel('Reward')\nplt.legend(title='DQN-Double DQN for 100 episodes')\n#plt.rcParams[\"figure.figsize\"] = (40,3)\nplt.show()\n\nprint(\"\\n------------------------------------------------------------\\n\")\nprint(\"mean_dqn_results=>\",mean_dqn_results)\nprint(\"mean_double_dqn_results=>\",mean_double_dqn_results)\nprint(\"\\n------------------------------------------------------------\\n\")\n\n\n\n","35380d62":"print(\"\\n------------------------------------------------------------\\n\")\nprint(\"Mean of rewards for DQN agents=>\",mean_dqn_results)\nprint(\"Mean of rewards for Double DQN agents=>\",mean_double_dqn_results)\nprint(\"\\n------------------------------------------------------------\\n\")\n","5163f30c":"plt.xlabel(\"Episode ->\")\nplt.ylabel(\"Reward ->\")\nplt.title(\"Episode wise mean of rewards: DQN vs DDQN post hyperparamter optimization\") \n\nplt.plot(mean_dqn_results.keys(),mean_dqn_results.values(), label = 'DQN')\nplt.plot(mean_dqn_results.keys(),mean_double_dqn_results.values(), label = 'Double DQN')\nplt.show()  \n","641f7859":"Replay Buffer : records previous state,action and rewards. A training set is created when the agent is updated and a sample of batch is created from the replay buffer. These samples can come from different episodes if the buffer is long enough. \nThis is very important in order to reduce the high correlation between training data. Neural network dont do well is training data is highly correlated.\n\nRef : https:\/\/livebook.manning.com\/concept\/reinforcement-learning\/experience-replay","49e451e1":"Work in progress !\nDeep Q-learner starter code\nThis notebook aims to demonstrate creating a deep Q-learner using Keras and train it on the GFootball enviroment. It will focus on the code needed to train a deep q-learner agent, rather than theory.\n\nRef : reinforcement-learning-keras\n\nhttps:\/\/keras.io\/examples\/rl\/deep_q_network_breakout\/","83784940":"# Q-Learning\n\nQ-learners are off-policy temporal difference leaners \n\nRef : (https:\/\/lilianweng.github.io\/lil-log\/2018\/02\/19\/a-long-peek-into-reinforcement-learning.html#model-transition-and-reward) \n\n## Components\n\n - Agent\n   - Model, usually a neural network \n   - Replay buffer (the agent's memory)\n   - Exploration method, such as episilon greedy","3fc0dd71":"Play Single Episode","1d0631a0":"## Model\n\nUsing the GFootball enviroment with the Simple115StateWrapper which returns observations (state) as a (115,) array. \n\nSimple115StateWrapper is a simplified representation of a game state encoded with 115 floats, which include the numerical coordinates of all the players, ball positions, ball ownership, game modes, etc.\n\nRef : https:\/\/flyyufelix.github.io\/2020\/12\/02\/google-football-rl.html\n\nThis array will be used as the input to a very simple neural network with 3 layers, which will output an estimated value for every possible action. The agent will either select the action with the highest value as estimated from this model, or randomly (chance depending on epsilon) select a random action.\n\nTo make learning more stable with deep-q learners, two copies of this model are used by the agent. \n  - \"Action\" model This copy is updated every training step and is used to estimate the value of actions for the current state. \n  - \"Target\" model\" The copy is updated by copying the weights of the value model, this is done less frequently in order to keep the training target more stable.","df2afe4a":"![image.png](attachment:image.png)\n\nRef : https:\/\/stanford.edu\/~shervine\/teaching\/cs-230\/cheatsheet-convolutional-neural-networks#:~:text=Fully%20Connected%20(FC)%20The%20fully,objectives%20such%20as%20class%20scores.","ef5b4433":"Epsilon Greedy\n\nEpsilon greedy enables the agent to explore the environment. \nBefore an agent makes an action, it considers a random value between 0 and 1. if this value > epsilon , the agent take random action. \nEplison is generally set at a high value initially at the beginning of the traning and it slowly decays over time. Hence Eplison is decay factor. "}}