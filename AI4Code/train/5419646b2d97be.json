{"cell_type":{"c2d1f66f":"code","922bc4f1":"code","3af1cdb4":"code","48a829c0":"code","10b5ab82":"code","f2e14eec":"code","82dcf076":"code","8fe7db84":"code","e5c508b8":"code","2979ccb1":"code","277c88a5":"code","2ebc4a1d":"code","11cbd48b":"code","8fdd20db":"code","1378fef8":"code","87d23ad6":"code","0ab6c8ad":"code","cebfd476":"code","8beb6795":"code","74a40928":"code","0c9eb260":"code","d056bf99":"code","3b71c490":"code","36351caa":"code","04d926fa":"markdown","f3fe4000":"markdown","402fd1df":"markdown","23bc1dc3":"markdown","e62c93b7":"markdown","4250a40d":"markdown","2506c62f":"markdown","24ef2f9c":"markdown","2a9d0886":"markdown","6fde8828":"markdown","7bb44fba":"markdown","df7503ed":"markdown","62028cba":"markdown","26a6d4bf":"markdown","bbcfc374":"markdown","e0ba5b0b":"markdown","c708f524":"markdown","5219db2d":"markdown","2afdd88e":"markdown","ba4505a4":"markdown","c1bb747d":"markdown"},"source":{"c2d1f66f":"import io\nimport openpyxl\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","922bc4f1":"insurance_ds = pd.read_csv(\"..\/input\/insurance\/insurance.csv\", sep=\",\")\n\ninsurance_ds","3af1cdb4":"#Checking a dataset sample\n\npd.set_option(\"display.max_rows\", 100)\npd.set_option(\"display.max_columns\", 100)\npd.options.display.float_format=\"{:,.2f}\".format\ninsurance_ds.sample(n=10, random_state=0)","48a829c0":"#Checking dataset info by feature\n\ninsurance_ds.info(verbose=True, null_counts=True)","10b5ab82":"#Checking the existence of zeros in rows\n\n(insurance_ds==0).sum(axis=0).to_excel(\"zeros_per_feature.xlsx\")\n(insurance_ds==0).sum(axis=0)","f2e14eec":"#Checking the existence of duplicated rows\n\ninsurance_ds.duplicated().sum()","82dcf076":"#Checking basic statistical data by feature\n\ninsurance_ds.describe(include=\"all\")","8fe7db84":"#1\n\ninsurance_ds.drop_duplicates(inplace=True)\n\n#2\n\ninsurance_ds = pd.concat([insurance_ds, pd.get_dummies(insurance_ds[\"sex\"], prefix=\"sex\")], axis=1)\ninsurance_ds = pd.concat([insurance_ds, pd.get_dummies(insurance_ds[\"smoker\"], prefix=\"smoker\")], axis=1)\ninsurance_ds = pd.concat([insurance_ds, pd.get_dummies(insurance_ds[\"region\"], prefix=\"region\")], axis=1)\n\n#3\n\ninsurance_ds[\"age_range\"] = np.where(insurance_ds.age>=60, \"60+\", np.where(insurance_ds.age>=50, \"50-60\", np.where(insurance_ds.age>=40, \"40-50\", np.where(insurance_ds.age>=30, \"30-40\", np.where(insurance_ds.age>=18, \"18-30\", \"18-\")))))\ninsurance_ds[\"bmi_range\"] = np.where(insurance_ds.bmi>=50, \"50+\", np.where(insurance_ds.bmi>=40, \"40-50\", np.where(insurance_ds.bmi>=30, \"30-40\", np.where(insurance_ds.bmi>=20, \"20-30\", np.where(insurance_ds.bmi>=15, \"15-20\", \"15-\")))))\ninsurance_ds[\"children_range\"] = np.where(insurance_ds.children>5, \"5+\", np.where(insurance_ds.children==5, \"5\", np.where(insurance_ds.children==4, \"4\", np.where(insurance_ds.children==3, \"3\", np.where(insurance_ds.children==2, \"2\", np.where(insurance_ds.children==1, \"1\", \"0\"))))))\ninsurance_ds[\"charges_range\"] = np.where(insurance_ds.charges>=50000, \"50000+\", np.where(insurance_ds.charges>=40000, \"40000-50000\", np.where(insurance_ds.charges>=30000, \"30000-40000\", np.where(insurance_ds.charges>=20, \"20000-30000\", np.where(insurance_ds.charges>=10000, \"10000-20000\", \"10000-\")))))\n\ninsurance_ds.to_excel(\"insurance_ds_clean.xlsx\")","e5c508b8":"#Plotting Categorical Variables\n\nfig, ax = plt.subplots(1, 2)\ninsurance_ds[\"sex\"].value_counts().plot.bar(color=\"purple\", ax=ax[0])\ninsurance_ds[\"sex\"].value_counts().plot.pie(autopct='%1.1f%%',shadow=True,textprops={\"fontsize\": 10},ax=ax[1])\nfig.suptitle(\"Gender Frequency\", fontsize=15)\nplt.xticks(rotation=90)\nplt.yticks(rotation=45)\n\nfig, ax = plt.subplots(1, 2)\ninsurance_ds[\"smoker\"].value_counts().plot.bar(color=\"purple\", ax=ax[0])\ninsurance_ds[\"smoker\"].value_counts().plot.pie(autopct='%1.1f%%',shadow=True,textprops={\"fontsize\": 10},ax=ax[1])\nfig.suptitle(\"Smoking Frequency\", fontsize=15)\nplt.xticks(rotation=90)\nplt.yticks(rotation=45)\n\nfig, ax = plt.subplots(1, 2)\ninsurance_ds[\"region\"].value_counts().plot.bar(color=\"purple\", ax=ax[0])\ninsurance_ds[\"region\"].value_counts().plot.pie(autopct='%1.1f%%',shadow=True,textprops={\"fontsize\": 10},ax=ax[1])\nfig.suptitle(\"Region Frequency\", fontsize=15)\nplt.xticks(rotation=90)\nplt.yticks(rotation=45)\n\n\n#Plotting Numerical Variables\n\nfig, ax = plt.subplots(1, 3)\nfig.suptitle(\"Charges Distribution\", fontsize=15)\nsns.distplot(insurance_ds[\"charges\"], ax=ax[0])\nsns.boxplot(insurance_ds[\"charges\"], ax=ax[1])\nsns.violinplot(insurance_ds[\"charges\"], ax=ax[2])\n\nfig, ax = plt.subplots(1, 3)\nfig.suptitle(\"Age Distribution\", fontsize=15)\nsns.distplot(insurance_ds[\"age\"], ax=ax[0])\nsns.boxplot(insurance_ds[\"age\"], ax=ax[1])\nsns.violinplot(insurance_ds[\"age\"], ax=ax[2])\n\nfig, ax = plt.subplots(1, 3)\nfig.suptitle(\"BMI Distribution\", fontsize=15)\nsns.distplot(insurance_ds[\"bmi\"], ax=ax[0])\nsns.boxplot(insurance_ds[\"bmi\"], ax=ax[1])\nsns.violinplot(insurance_ds[\"bmi\"], ax=ax[2])\n\nfig, ax = plt.subplots(1, 3)\nfig.suptitle(\"Children Distribution\", fontsize=15)\nsns.distplot(insurance_ds[\"children\"], ax=ax[0])\nsns.boxplot(insurance_ds[\"children\"], ax=ax[1])\nsns.violinplot(insurance_ds[\"children\"], ax=ax[2])","2979ccb1":"#Alternatively using Profile Report to see variables statistics and correlations\n\n# from pandas_profiling import ProfileReport\n# profile = ProfileReport(insurance_ds, title=\"Medical Cost\")\n# profile.to_file(output_file=\"Medical_Cost.html\")","277c88a5":"#Plotting Bar Charts, also considering all numerical to categorical variables created at the step before\n\nfig, axarr = plt.subplots(3, 2, figsize=(20, 12))\nsns.countplot(x=\"age_range\", hue = \"charges_range\", data = insurance_ds, ax=axarr[0][0])\nsns.countplot(x=\"bmi_range\", hue = \"charges_range\", data = insurance_ds, ax=axarr[0][1])\nsns.countplot(x=\"children_range\", hue = \"charges_range\", data = insurance_ds, ax=axarr[1][0])\nsns.countplot(x=\"sex\", hue = \"charges_range\", data = insurance_ds, ax=axarr[1][1])\nsns.countplot(x=\"smoker\", hue = \"charges_range\", data = insurance_ds, ax=axarr[2][0])\nsns.countplot(x=\"region\", hue = \"charges_range\", data = insurance_ds, ax=axarr[2][1])\n\n#Deleting original categorical columns\n\ninsurance_ds.drop([\"sex\", \"smoker\", \"region\", \"age_range\", \"bmi_range\", \"children_range\", \"charges_range\"], axis=1, inplace=True)\n\n#Plotting a Heatmap\n\nfig, ax = plt.subplots(1, figsize=(15,15))\nsns.heatmap(insurance_ds.corr(), annot=True, fmt=\",.2f\")\nplt.title(\"Heatmap Correlation\", fontsize=20)\nplt.tick_params(labelsize=12)\nplt.xticks(rotation=90)\nplt.yticks(rotation=45)\n\n#Plotting a Pairplot\n\nsns.pairplot(insurance_ds)","2ebc4a1d":"#Plotting a Feature Importance\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom matplotlib import pyplot\n#Defining Xs and y\nX = insurance_ds.drop([\"charges\"], axis=1)\ny = insurance_ds[\"charges\"]\n#Defining the model\nmodel = RandomForestRegressor().fit(X, y)\n#Getting importance\nimportance = model.feature_importances_\n#Summarizing feature importance\nfor i,v in enumerate(importance):\n    print(\"Feature:{0:} - Score:{1:,.4f}\".format(X.columns[i], v))\n#Plotting feature importance\npd.Series(model.feature_importances_[::-1], index=X.columns[::-1]).plot.barh(figsize=(25,25))\n","11cbd48b":"#Defining Xs and y\n\nX = insurance_ds[[\"smoker_no\", \"bmi\", \"age\", \"children\"]]\ny = insurance_ds[[\"charges\"]]\n\n#Scaling all features\n\nfrom sklearn.preprocessing import MinMaxScaler\nsc_X = MinMaxScaler()\nX_scaled = sc_X.fit_transform(X)\nX_scaled = pd.DataFrame(X_scaled)\n\n#Setting train\/test split\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, random_state=0)","8fdd20db":"#Creating a Polynomial Regression model and checking its Metrics\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n\n#Creating a Linear Regressor\nlin_regressor = LinearRegression()\n\n#Trying different polynomial degrees\ndegrees = [1, 2, 3, 4, 5]\nprint(\"Testing degrees:\")\nfor a in degrees:\n    poly = PolynomialFeatures(degree=a)\n    X_train_degree = poly.fit_transform(X_train)\n    X_test_degree = poly.fit_transform(X_test)\n    model_pr = lin_regressor.fit(X_train_degree, y_train)\n    y_preds_train = model_pr.predict(X_train_degree)\n    y_preds_test = model_pr.predict(X_test_degree)\n    score_train = r2_score(y_train, y_preds_train)\n    score_test = r2_score(y_test, y_preds_test)\n    mse_train = mean_squared_error(y_train, y_preds_train)\n    mse_test = mean_squared_error(y_test, y_preds_test)\n    print(\"Train: Degree:{0:,.0f}, R2:{1:,.3f}, MSE:{2:,.2f}, RMSE:{3:,.2f}\".format(a, score_train, mse_train, np.sqrt(mse_train)))\n    print(\"Test : Degree:{0:,.0f}, R2:{1:,.3f}, MSE:{2:,.2f}, RMSE:{3:,.2f}\".format(a, score_test, mse_test, np.sqrt(mse_test)))       \nprint(\"\")\n\n#Choosing the best polynomial degree\nchosen_degree = 2\npoly = PolynomialFeatures(degree=chosen_degree)\n\n#Working on X_train & X_test in the polynomial chosen degree\nX_train_degree = poly.fit_transform(X_train)\nX_test_degree = poly.fit_transform(X_test)\n\n#Fitting to the Linear Regressor\nmodel_pr = lin_regressor.fit(X_train_degree, y_train)\nprint(f\"Linear Regression Intercept: {model_pr.intercept_}\")\nprint(f\"Linear Regression Coefficients: {model_pr.coef_}, \\n\")\n\n#Getting the predictions & Metrics\ny_preds_train = model_pr.predict(X_train_degree)\ny_preds_test = model_pr.predict(X_test_degree)\nscore_train = r2_score(y_train, y_preds_train)\nscore_test = r2_score(y_test, y_preds_test)\nmse_train = mean_squared_error(y_train, y_preds_train)\nmse_test = mean_squared_error(y_test, y_preds_test)\nprint(\"Chosen degree:\")\nprint(\"Train: Degree:{0:,.0f}, R2:{1:,.3f}, MSE:{2:,.2f}, RMSE:{3:,.2f}\".format(chosen_degree, score_train, mse_train, np.sqrt(mse_train)))\nprint(\"Test : Degree:{0:,.0f}, R2:{1:,.3f}, MSE:{2:,.2f}, RMSE:{3:,.2f}\".format(chosen_degree, score_test, mse_test, np.sqrt(mse_test)))   \n\n#Visualizing y_pred in the dataset\nX_degree = poly.fit_transform(X_scaled)\ny_pred_all = model_pr.predict(X_degree)\ninsurance_ds[\"charges_predicted\"] = y_pred_all\ninsurance_ds.to_excel(\"model_pr.xlsx\")","1378fef8":"#Creating a Ridge Regression model and checking its Metrics\n\nfrom sklearn.linear_model import Ridge\n\n#Trying different alphas\nalphas = [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\nprint(\"Testing alphas:\")\nfor a in alphas:\n    model_ridge = Ridge(alpha=a, normalize=True).fit(X_train, y_train) \n    y_preds_train = model_ridge.predict(X_train)\n    y_preds_test = model_ridge.predict(X_test)\n    score_train = r2_score(y_train, y_preds_train)\n    score_test = r2_score(y_test, y_preds_test)\n    mse_train = mean_squared_error(y_train, y_preds_train)\n    mse_test = mean_squared_error(y_test, y_preds_test)\n    print(\"Train: Alpha:{0:,.6f}, R2:{1:,.3f}, MSE:{2:,.2f}, RMSE:{3:,.2f}\".format(a, score_train, mse_train, np.sqrt(mse_train)))\n    print(\"Test : Alpha:{0:,.6f}, R2:{1:,.3f}, MSE:{2:,.2f}, RMSE:{3:,.2f}\".format(a, score_test, mse_test, np.sqrt(mse_test)))\nprint(\"\")\n\n#Choosing the best alpha\na_final = 0.000001\nmodel_ridge = Ridge(alpha=a_final, normalize=True).fit(X_train, y_train) \ny_preds_train = model_ridge.predict(X_train)\ny_preds_test = model_ridge.predict(X_test)\nscore_train = r2_score(y_train, y_preds_train)\nscore_test = r2_score(y_test, y_preds_test)\nmse_train = mean_squared_error(y_train, y_preds_train)\nmse_test = mean_squared_error(y_test, y_preds_test)\nprint(f\"Linear Regression Intercept: {model_ridge.intercept_}\")\nprint(f\"Linear Regression Coefficients: {model_ridge.coef_}, \\n\")\nprint(\"Chosen alpha:\")\nprint(\"Train: Alpha:{0:,.6f}, R2:{1:,.3f}, MSE:{2:,.2f}, RMSE:{3:,.2f}\".format(a_final, score_train, mse_train, np.sqrt(mse_train)))\nprint(\"Test : Aplha:{0:,.6f}, R2:{1:,.3f}, MSE:{2:,.2f}, RMSE:{3:,.2f}\".format(a_final, score_test, mse_test, np.sqrt(mse_test)))\n\n#Plotting\nx_ax = range(len(X_test))\nplt.scatter(x_ax, y_test, s=5, color=\"blue\", label=\"original\")\nplt.plot(x_ax, y_preds_test, lw=0.8, color=\"red\", label=\"predicted\")\nplt.legend()\n\n#Visualizing y_pred in the dataset\ny_pred_all = model_ridge.predict(X_scaled)\ninsurance_ds[\"charges_predicted\"] = y_pred_all\ninsurance_ds.to_excel(\"model_ridge.xlsx\")","87d23ad6":"#Creating a RidgeCV Regression model and checking its Metrics\nfrom sklearn.linear_model import RidgeCV\n\n#Choosing the best alpha\nmodel_ridge_cv = RidgeCV(alphas=alphas, normalize=True).fit(X_train,y_train) \ny_preds_train = model_ridge_cv.predict(X_train)\ny_preds_test = model_ridge_cv.predict(X_test)\nscore_train = r2_score(y_train, y_preds_train)\nscore_test = r2_score(y_test, y_preds_test)\nmse_train = mean_squared_error(y_train, y_preds_train)\nmse_test = mean_squared_error(y_test, y_preds_test)\nprint(f\"Linear Regression Intercept: {model_ridge_cv.intercept_}\")\nprint(f\"Linear Regression Coefficients: {model_ridge_cv.coef_}, \\n\")\nprint(\"Train: R2:{0:,.3f}, MSE:{1:,.2f}, RMSE:{2:,.2f}\".format(score_train, mse_train, np.sqrt(mse_train)))\nprint(\"Test : R2:{0:,.3f}, MSE:{1:,.2f}, RMSE:{2:,.2f}\".format(score_test, mse_test, np.sqrt(mse_test)))\n\n#Plotting\nx_ax = range(len(X_test))\nplt.scatter(x_ax, y_test, s=5, color=\"blue\", label=\"original\")\nplt.plot(x_ax, y_preds_test, lw=0.8, color=\"red\", label=\"predicted\")\nplt.legend()\n\n#Visualizing y_pred in the dataset\ny_pred_all = model_ridge_cv.predict(X_scaled)\ninsurance_ds[\"charges_predicted\"] = y_pred_all\ninsurance_ds.to_excel(\"model_ridge_cv.xlsx\")","0ab6c8ad":"#Creating a Lasso Regression model and checking its Metrics\n\nfrom sklearn.linear_model import Lasso\n\n#Trying different alphas\nalphas = [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\nprint(\"Testing alphas:\")\nfor a in alphas:\n    model_lasso = Lasso(alpha=a, normalize=True).fit(X_train,y_train) \n    y_preds_train = model_lasso.predict(X_train)\n    y_preds_test = model_lasso.predict(X_test)\n    score_train = r2_score(y_train, y_preds_train)\n    score_test = r2_score(y_test, y_preds_test)\n    mse_train = mean_squared_error(y_train, y_preds_train)\n    mse_test = mean_squared_error(y_test, y_preds_test)\n    print(\"Train: Alpha:{0:,.6f}, R2:{1:,.3f}, MSE:{2:,.2f}, RMSE:{3:,.2f}\".format(a, score_train, mse_train, np.sqrt(mse_train)))\n    print(\"Test : Alpha:{0:,.6f}, R2:{1:,.3f}, MSE:{2:,.2f}, RMSE:{3:,.2f}\".format(a, score_test, mse_test, np.sqrt(mse_test)))\nprint(\"\")\n\n#Choosing the best alpha\na_final = 0.000001\nmodel_lasso = Lasso(alpha=a_final, normalize=True).fit(X_train,y_train) \ny_preds_train = model_lasso.predict(X_train)\ny_preds_test = model_lasso.predict(X_test)\nscore_train = r2_score(y_train, y_preds_train)\nscore_test = r2_score(y_test, y_preds_test)\nmse_train = mean_squared_error(y_train, y_preds_train)\nmse_test = mean_squared_error(y_test, y_preds_test)\nprint(f\"Linear Regression Intercept: {model_lasso.intercept_}\")\nprint(f\"Linear Regression Coefficients: {model_lasso.coef_}, \\n\")\nprint(\"Chosen alpha:\")\nprint(\"Train: Alpha:{0:,.6f}, R2:{1:,.3f}, MSE:{2:,.2f}, RMSE:{3:,.2f}\".format(a_final, score_train, mse_train, np.sqrt(mse_train)))\nprint(\"Test : Aplha:{0:,.6f}, R2:{1:,.3f}, MSE:{2:,.2f}, RMSE:{3:,.2f}\".format(a_final, score_test, mse_test, np.sqrt(mse_test)))\n\n#Plotting\nx_ax = range(len(X_test))\nplt.scatter(x_ax, y_test, s=5, color=\"blue\", label=\"original\")\nplt.plot(x_ax, y_preds_test, lw=0.8, color=\"red\", label=\"predicted\")\nplt.legend()\n\n#Visualizing y_pred in the dataset\ny_pred_all = model_lasso.predict(X_scaled)\ninsurance_ds[\"charges_predicted\"] = y_pred_all\ninsurance_ds.to_excel(\"model_lasso.xlsx\")","cebfd476":"#Creating a LassoCV Regression model and checking its Metrics\n\nfrom sklearn.linear_model import LassoCV\n\n#Choosing the best alpha\nmodel_lasso_cv = LassoCV(alphas=alphas, normalize=True).fit(X_train,y_train) \ny_preds_train = model_lasso_cv.predict(X_train)\ny_preds_test = model_lasso_cv.predict(X_test)\nscore_train = r2_score(y_train, y_preds_train)\nscore_test = r2_score(y_test, y_preds_test)\nmse_train = mean_squared_error(y_train, y_preds_train)\nmse_test = mean_squared_error(y_test, y_preds_test)\nprint(f\"Linear Regression Intercept: {model_lasso_cv.intercept_}\")\nprint(f\"Linear Regression Coefficients: {model_lasso_cv.coef_}, \\n\")\nprint(\"Train: R2:{0:,.3f}, MSE:{1:,.2f}, RMSE:{2:,.2f}\".format(score_train, mse_train, np.sqrt(mse_train)))\nprint(\"Test : R2:{0:,.3f}, MSE:{1:,.2f}, RMSE:{2:,.2f}\".format(score_test, mse_test, np.sqrt(mse_test)))\n\n#Plotting\nx_ax = range(len(X_test))\nplt.scatter(x_ax, y_test, s=5, color=\"blue\", label=\"original\")\nplt.plot(x_ax, y_preds_test, lw=0.8, color=\"red\", label=\"predicted\")\nplt.legend()\n\n#Visualizing y_pred in the dataset\ny_pred_all = model_lasso_cv.predict(X_scaled)\ninsurance_ds[\"charges_predicted\"] = y_pred_all\ninsurance_ds.to_excel(\"model_lasso_cv.xlsx\")","8beb6795":"#Creating a Random Forest Regression model and checking its Metrics\n\nfrom sklearn.ensemble import RandomForestRegressor\n\n#Trying different depths\ndepths = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nprint(\"Testing depths:\")\nfor a in depths:\n    model_rf = RandomForestRegressor(max_depth=a, random_state=0).fit(X_train,y_train.values.ravel()) \n    y_preds_train = model_rf.predict(X_train)\n    y_preds_test = model_rf.predict(X_test)\n    score_train = r2_score(y_train, y_preds_train)\n    score_test = r2_score(y_test, y_preds_test)\n    mse_train = mean_squared_error(y_train, y_preds_train)\n    mse_test = mean_squared_error(y_test, y_preds_test)\n    print(\"Train: Depth:{0:,.0f}, R2:{1:,.3f}, MSE:{2:,.2f}, RMSE:{3:,.2f}\".format(a, score_train, mse_train, np.sqrt(mse_train)))\n    print(\"Test : Depth:{0:,.0f}, R2:{1:,.3f}, MSE:{2:,.2f}, RMSE:{3:,.2f}\".format(a, score_test, mse_test, np.sqrt(mse_test)))\nprint(\"\")\n\n#Choosing the best depth\na_final = 4\nmodel_rf = RandomForestRegressor(max_depth=a_final, random_state=0).fit(X_train,y_train.values.ravel()) \ny_preds_train = model_rf.predict(X_train)\ny_preds_test = model_rf.predict(X_test)\nscore_train = r2_score(y_train, y_preds_train)\nscore_test = r2_score(y_test, y_preds_test)\nmse_train = mean_squared_error(y_train, y_preds_train)\nmse_test = mean_squared_error(y_test, y_preds_test)\nprint(\"Chosen depth:\")\nprint(\"Train: Depth:{0:,.0f}, R2:{1:,.3f}, MSE:{2:,.2f}, RMSE:{3:,.2f}\".format(a_final, score_train, mse_train, np.sqrt(mse_train)))\nprint(\"Test : Depth:{0:,.0f}, R2:{1:,.3f}, MSE:{2:,.2f}, RMSE:{3:,.2f}\".format(a_final, score_test, mse_test, np.sqrt(mse_test)))\n\n#Plotting\nx_ax = range(len(X_test))\nplt.scatter(x_ax, y_test, s=5, color=\"blue\", label=\"original\")\nplt.plot(x_ax, y_preds_test, lw=0.8, color=\"red\", label=\"predicted\")\nplt.legend()\n\n#Visualizing y_pred in the dataset\ny_pred_all = model_rf.predict(X_scaled)\ninsurance_ds[\"charges_predicted\"] = y_pred_all\ninsurance_ds.to_excel(\"model_rf.xlsx\")","74a40928":"#Creating a XGBoost Regression model and checking its Metrics\n\nfrom xgboost import XGBRegressor\n\n#Trying different depths\ndepths = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nprint(\"Testing depths:\")\nfor a in depths:\n    model_xgb = XGBRegressor(max_depth=a, random_state=0).fit(X_train,y_train) \n    y_preds_train = model_xgb.predict(X_train)\n    y_preds_test = model_xgb.predict(X_test)\n    score_train = r2_score(y_train, y_preds_train)\n    score_test = r2_score(y_test, y_preds_test)\n    mse_train = mean_squared_error(y_train, y_preds_train)\n    mse_test = mean_squared_error(y_test, y_preds_test)\n    print(\"Train: Depth:{0:,.0f}, R2:{1:,.3f}, MSE:{2:,.2f}, RMSE:{3:,.2f}\".format(a, score_train, mse_train, np.sqrt(mse_train)))\n    print(\"Test : Depth:{0:,.0f}, R2:{1:,.3f}, MSE:{2:,.2f}, RMSE:{3:,.2f}\".format(a, score_test, mse_test, np.sqrt(mse_test)))\nprint(\"\")\n\n#Choosing the best depth\na_final = 2\nmodel_xgb = XGBRegressor(max_depth=a_final, random_state=0).fit(X_train,y_train) \ny_preds_train = model_xgb.predict(X_train)\ny_preds_test = model_xgb.predict(X_test)\nscore_train = r2_score(y_train, y_preds_train)\nscore_test = r2_score(y_test, y_preds_test)\nmse_train = mean_squared_error(y_train, y_preds_train)\nmse_test = mean_squared_error(y_test, y_preds_test)\nprint(\"Chosen depth:\")\nprint(\"Train: Depth:{0:,.0f}, R2:{1:,.3f}, MSE:{2:,.2f}, RMSE:{3:,.2f}\".format(a_final, score_train, mse_train, np.sqrt(mse_train)))\nprint(\"Test : Depth:{0:,.0f}, R2:{1:,.3f}, MSE:{2:,.2f}, RMSE:{3:,.2f}\".format(a_final, score_test, mse_test, np.sqrt(mse_test)))\n\n#Plotting\nx_ax = range(len(X_test))\nplt.scatter(x_ax, y_test, s=5, color=\"blue\", label=\"original\")\nplt.plot(x_ax, y_preds_test, lw=0.8, color=\"red\", label=\"predicted\")\nplt.legend()\n\n#Visualizing y_pred in the dataset\ny_pred_all = model_xgb.predict(X_scaled)\ninsurance_ds[\"charges_predicted\"] = y_pred_all\ninsurance_ds.to_excel(\"model_xgb.xlsx\")","0c9eb260":"#Creating a Deep Learning Regression model and checking its Metrics\n\nfrom keras import Sequential\nfrom keras.layers import Dense\nfrom keras.callbacks import EarlyStopping,ReduceLROnPlateau\n\n#Creating a model\nmodel_dl = Sequential()\n\n#Input and First Hidden Layer\nmodel_dl.add(Dense(units=256, activation=\"relu\", input_dim=X_train.shape[1]))\n#Second Hidden Layer\nmodel_dl.add(Dense(units=256, activation=\"relu\"))\n#Third Hidden Layer\nmodel_dl.add(Dense(units=256, activation=\"relu\"))\n#Output Layer\nmodel_dl.add(Dense(units=1))\n\n#Compiling the neural network\nmodel_dl.compile(optimizer=\"adam\",loss=\"mean_squared_error\")\n\n#Fitting to the model\nmodel_dl.fit(X_train,y_train, callbacks=[EarlyStopping(patience=10),ReduceLROnPlateau(monitor=\"val_loss\",min_lr=0.01)], epochs=250)\n\n#Getting the predictions & Metrics\ny_preds_train = model_dl.predict(X_train)\ny_preds_test = model_dl.predict(X_test)\nscore_train = r2_score(y_train, y_preds_train)\nscore_test = r2_score(y_test, y_preds_test)\nmse_train = mean_squared_error(y_train, y_preds_train)\nmse_test = mean_squared_error(y_test, y_preds_test)\nprint(\"Train: R2:{0:,.3f}, MSE:{1:,.2f}, RMSE:{2:,.2f}\".format(score_train, mse_train, np.sqrt(mse_train)))\nprint(\"Test : R2:{0:,.3f}, MSE:{1:,.2f}, RMSE:{2:,.2f}\".format(score_test, mse_test, np.sqrt(mse_test)))\n\n#Plotting\nx_ax = range(len(X_test))\nplt.scatter(x_ax, y_test, s=5, color=\"blue\", label=\"original\")\nplt.plot(x_ax, y_preds_test, lw=0.8, color=\"red\", label=\"predicted\")\nplt.legend()\n\n#Visualizing y_pred in the dataset\ny_pred_all = model_dl.predict(X_scaled)\ninsurance_ds[\"charges_predicted\"] = y_pred_all\ninsurance_ds.to_excel(\"model_dl.xlsx\")","d056bf99":"#Entering Xs\n\n# smoker_no_input = str(input(\"Smoker (Yes\/No)? \"))\n# if smoker_no_input == \"No\":\n#     smoker_no_input = int(1)\n# else:\n#     smoker_no_input = int(0)\n# bmi_input = float(input(\"Enter the BMI: \"))\n# age_input = float(input(\"Enter the Age: \"))\n# children_input = int(input(\"Children (number of): \"))\n    \n#Defining Xs\n\n# X_mod_dep = pd.DataFrame({\"smoker_no\": [smoker_no_input], \"bmi\": [bmi_input], \"age\": [age_input], \"children\": [children_input]})\n#Example for random client\nX_mod_dep = pd.DataFrame({\"smoker_no\": [1], \"bmi\": [33], \"age\": [45], \"children\": [1]})\n    \n    \n#Appending X_mod_dep to original X dataframe, so we can scale it all together next\n\nX_with_X_mode_dep = X.append(X_mod_dep)\n\n#Scaling all features\n\nfrom sklearn.preprocessing import MinMaxScaler\nsc_X = MinMaxScaler()\nX_scaled = sc_X.fit_transform(X_with_X_mode_dep)\nX_scaled = pd.DataFrame(X_scaled)\n\n#Recovering X_mod_dep row in dataframe after scaling\n\nX_mod_dep = X_scaled.tail(1)\n\n#Predicting results\n\nprint(f\"Predicted Charge = {model_xgb.predict(X_mod_dep)}.\")","3b71c490":"#Copying html content to new editable directory\n\n!mkdir static\n!mkdir templates\n!cp -r \"..\/input\/Medical-Cost\/static\/input_page.html\" \"\/kaggle\/working\/static\"\n!cp -r \"..\/input\/Medical-Cost\/static\/input_page.css\" \"\/kaggle\/working\/static\"\n!cp -r \"..\/input\/Medical-Cost\/templates\/input_page.html\" \"\/kaggle\/working\/templates\"\n!cp -r \"..\/input\/Medical-Cost\/templates\/input_page.css\" \"\/kaggle\/working\/templates\"\n\n!ls","36351caa":"#Creating the picke file\nimport pickle\n\npickle.dump(RandomForestRegressor(max_depth=a_final, random_state=0), open('model.pkl','wb'))\n\nmodel = pickle.load(open('model.pkl','rb'))\n\n\n#Making an API which receives sales details through GUI and computes the predicted sales value based on our model\nfrom flask import Flask, request, jsonify, render_template\n\napp = Flask(__name__)\nmodel = pickle.load(open('model.pkl', 'rb'))\n\n@app.route('\/')\ndef home():\n    return render_template('input_page.html')\n\n@app.route('\/predict',methods=['POST'])\ndef predict():\n\n    int_features = [int(x) for x in request.form.values()]\n    final_features = [np.array(int_features)]\n    prediction = model_xgb.predict(final_features)\n\n    output = round(prediction[0], 2)\n\n    return render_template('input_page.html', prediction_text='Predicted Charge = {}'.format(output))\n\n@app.route('\/results',methods=['POST'])\ndef results():\n\n    data = request.get_json(force=True)\n    prediction = model_xgb.predict([np.array(list(data.values()))])\n\n    output = prediction[0]\n    return jsonify(output)\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n\n\n#Using requests module to call APIs\nimport requests\n\nurl = 'http:\/\/localhost:5000\/results'\nr = requests.post(url,json={'smoker_no':0, 'bmi':0, 'age':0, 'children':0})\n\nprint(r.json())\n\n# Open http:\/\/127.0.0.1:5000\/ in your web-browser, and the GUI as shown below should appear.","04d926fa":"# 5. Data Cleaning\n\n    We\u00b4ll perform the following:\n\n    1. Remove duplicated row\n    \n\n    2. Convert categorical variables (sex, smoker, region) to dummies\n    \n    \n    3. Convert all numerical variables to categorical ranges (to be used in next step when analyzing correlations)\n\n\n    * no missing, zero or invalid values to treat\n    * no columns to remove\n    * no outliers to treat\n    * the entire dataset will be taken","f3fe4000":"# 1. Introduction: Business Goal & Problem Definition\n\nThe goal of this project is to study and predict the best insurance price to be charged to the customers according to their unique characteristics, allowing the company to adopt a reasonable and fair price in the market and to maximize its profits. For that we\u00b4ll use the Medical Cost Personal Dataset available in Kaggle, containing 1338 observations, each with the following attributes:\n\nIF YOU LIKE IT OR IF IT HELPS YOU SOMEHOW, COULD YOU PLEASE UPVOTE? THANK YOU VERY MUCH!!!\n\n* Age: age of primary beneficiary\n* Sex: insurance contractor gender, female, male\n* BMI: Body mass index, providing an understanding of body, weights that are relatively high or low relative to height,\nobjective index of body weight (kg \/ m ^ 2) using the ratio of height to weight, ideally 18.5 to 24.9\n* Children: Number of children covered by health insurance \/ Number of dependents\n* Smoker: Smoking\n* Region: the beneficiary's residential area in the US, northeast, southeast, southwest, northwest.\n* Charges: Individual medical costs billed by health insurance","402fd1df":"# 9.8 Deep Learning Regression","23bc1dc3":"# 10. Model Deployment","e62c93b7":"# 6. Data Exploration","4250a40d":"# 9.1 Polynomial Regression","2506c62f":"# 9.5 LassoCV Regression","24ef2f9c":"# 9. Machine Learning Algorithms Implementation & Assessment","2a9d0886":"# 9.3 RidgeCV Regression","6fde8828":"# 7. Correlations Analysis & Features Selection","7bb44fba":"# 8. Data Modelling","df7503ed":"# 10.2 Entering Independent Variables in a Web Page\n\nObs: to be deployed in localhost or paid cloud service","62028cba":"# 4. Data Preliminary Exploration","26a6d4bf":"# 9.2 Ridge Regression","bbcfc374":"# 9.6 Random Forest Regression","e0ba5b0b":"# 3. Data Collection","c708f524":"# 9.7 XGBoost Regression","5219db2d":"# 10.1 Entering Independent Variables in Python","2afdd88e":"# 9.4 Lasso Regression","ba4505a4":"# 11. Conclusions\n\nIF YOU LIKE IT OR IF IT HELPS YOU SOMEHOW, COULD YOU PLEASE UPVOTE? THANK YOU VERY MUCH!!!\n\nIn this project we went through all the process from defining the business objective, collecting data, exploring features and distributions, treating data, understanding correlations, selecting relevant features, data modelling and presenting 8 different algorithms with metrics to select the best to predict the best price to charge the customers, what\u00b4s vital for an insurance company in order to minimize the profit losses and maximize the revenue. The chosen model was XGBoost, with around 85% accuracy.","c1bb747d":"# 2. Importing Basic Libraries "}}