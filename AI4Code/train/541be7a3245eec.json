{"cell_type":{"5d516955":"code","9a00bfad":"code","429ef1c9":"code","fea299ef":"code","58d11b74":"code","ba78c05c":"code","76540d7d":"code","a64db584":"code","0fdafe0d":"code","43d11972":"markdown","0ae1f63d":"markdown","84deb792":"markdown","5b473a0f":"markdown","50c75163":"markdown","8900252b":"markdown","6fc6e704":"markdown","2e1710ce":"markdown"},"source":{"5d516955":"!pip install transformers","9a00bfad":"from __future__ import print_function\nimport ipywidgets as widgets\nfrom transformers import pipeline","429ef1c9":"nlp_sentence_classif = pipeline('sentiment-analysis')\nnlp_sentence_classif('Such a nice weather outside !')","fea299ef":"nlp_token_class = pipeline('ner')\nnlp_token_class('Hugging Face is a French company based in New-York.')","58d11b74":"nlp_qa = pipeline('question-answering')\nnlp_qa(context='Hugging Face is a French company based in New-York.', question='Where is based Hugging Face ?')","ba78c05c":"nlp_fill = pipeline('fill-mask')\nnlp_fill('Hugging Face is a French company based in <mask>')","76540d7d":"import numpy as np\nnlp_features = pipeline('feature-extraction')\noutput = nlp_features('Hugging Face is a French company based in Paris')\nnp.array(output).shape   # (Samples, Tokens, Vector Size)\n","a64db584":"task = widgets.Dropdown(\n    options=['sentiment-analysis', 'ner', 'fill_mask'],\n    value='ner',\n    description='Task:',\n    disabled=False\n)\n\ninput = widgets.Text(\n    value='',\n    placeholder='Enter something',\n    description='Your input:',\n    disabled=False\n)\n\ndef forward(_):\n    if len(input.value) > 0: \n        if task.value == 'ner':\n            output = nlp_token_class(input.value)\n        elif task.value == 'sentiment-analysis':\n            output = nlp_sentence_classif(input.value)\n        else:\n            if input.value.find('<mask>') == -1:\n                output = nlp_fill(input.value + ' <mask>')\n            else:\n                output = nlp_fill(input.value)                \n        print(output)\n\ninput.on_submit(forward)\ndisplay(task, input)","0fdafe0d":"context = widgets.Textarea(\n    value='Einstein is famous for the general theory of relativity',\n    placeholder='Enter something',\n    description='Context:',\n    disabled=False\n)\n\nquery = widgets.Text(\n    value='Why is Einstein famous for ?',\n    placeholder='Enter something',\n    description='Question:',\n    disabled=False\n)\n\ndef forward(_):\n    if len(context.value) > 0 and len(query.value) > 0: \n        output = nlp_qa(question=query.value, context=context.value)            \n        print(output)\n\nquery.on_submit(forward)\ndisplay(context, query)","43d11972":"## 5. Projection - Features Extraction ","0ae1f63d":"## 1. Sentence Classification - Sentiment Analysis","84deb792":"## 4. Text Generation - Mask Filling","5b473a0f":"## 2. Token Classification - Named Entity Recognition","50c75163":"## How can I leverage State-of-the-Art Natural Language Models with only one line of code ?","8900252b":"## 3. Question Answering","6fc6e704":"Newly introduced in transformers v2.3.0, **pipelines** provides a high-level, easy to use,\nAPI for doing inference over a variety of downstream-tasks, including: \n\n- Sentence Classification (Sentiment Analysis): Indicate if the overall sentence is either positive or negative. _(Binary Classification task or Logitic Regression task)_\n- Token Classification (Named Entity Recognition, Part-of-Speech tagging): For each sub-entities _(**tokens**)_ in the input, assign them a label _(Classification task)_.\n- Question-Answering: Provided a tuple (question, context) the model should find the span of text in **content** answering the **question**.\n- Mask-Filling: Suggests possible word(s) to fill the masked input with respect to the provided **context**.\n- Feature Extraction: Maps the input to a higher, multi-dimensional space learned from the data.\n\nPipelines encapsulate the overall process of every NLP process:\n \n 1. Tokenization: Split the initial input into multiple sub-entities with ... properties (i.e. tokens).\n 2. Inference: Maps every tokens into a more meaningful representation. \n 3. Decoding: Use the above representation to generate and\/or extract the final output for the underlying task.\n\nThe overall API is exposed to the end-user through the `pipeline()` method with the following \nstructure:\n\n```python\nfrom transformers import pipeline\n\n# Using default model and tokenizer for the task\npipeline(\"<task-name>\")\n\n# Using a user-specified model\npipeline(\"<task-name>\", model=\"<model_name>\")\n\n# Using custom model\/tokenizer as str\npipeline('<task-name>', model='<model name>', tokenizer='<tokenizer_name>')\n```","2e1710ce":"Alright ! Now you have a nice picture of what is possible through transformers' pipelines, and there is more\nto come in future releases. \n\nIn the meantime, you can try the different pipelines with your own inputs"}}