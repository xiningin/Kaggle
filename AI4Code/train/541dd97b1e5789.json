{"cell_type":{"0b90785a":"code","a3c371f1":"code","af03aadb":"code","86c33a29":"code","80087c97":"code","04f4b626":"code","2e363cba":"code","7d176055":"code","900278f5":"code","32904035":"code","1458e0a1":"code","5fd394fc":"code","c5c48f1a":"code","40f01251":"code","01f9fbea":"code","2ca2637a":"code","0b409bd5":"code","363a8c54":"code","02adc69c":"code","aa248158":"code","891fde92":"code","23232e90":"code","0bf09c32":"code","27eef1d2":"code","f409d857":"code","243f3e5c":"code","94195ab5":"code","285e5331":"code","1dee7124":"code","a1a5094f":"code","0588812d":"code","bfbaae52":"code","66e955e7":"code","db4552ae":"code","19e0a398":"code","aa2f14fa":"code","fef82298":"code","4e27cb00":"code","403271bd":"code","7a4606e8":"code","199f23ee":"code","c84ef015":"code","bcad5ecd":"code","345a85a6":"code","42576814":"markdown","c07fe0d8":"markdown","f4692949":"markdown","9e8db28c":"markdown","87558d2d":"markdown","6f157324":"markdown","45ae8da5":"markdown","12a0f6f1":"markdown","3495a25a":"markdown","511f9759":"markdown","850743f7":"markdown","7f224737":"markdown","29dc79f6":"markdown","8a271efb":"markdown","915100ce":"markdown","ef9edd7d":"markdown","70b7d120":"markdown","a98144f2":"markdown","7097af1c":"markdown","7de6bb04":"markdown","af79e66a":"markdown","b0de2a4d":"markdown","dcdbb6de":"markdown","aa1b0e71":"markdown","75cccd34":"markdown","4d414da0":"markdown","9a0a5444":"markdown","d7fab3c4":"markdown","ec1f1a53":"markdown","af3b7b6e":"markdown","b60cdcd5":"markdown","23485811":"markdown","639c215e":"markdown","74b73394":"markdown","d0b5246e":"markdown","342d37c5":"markdown","cc24c0c9":"markdown","6b9b4b88":"markdown","14d6b977":"markdown","d916f244":"markdown","1f738a2b":"markdown","6d2c3879":"markdown","2223d41d":"markdown","a05be621":"markdown","01042e7e":"markdown","5c0279b1":"markdown","a7dd3e69":"markdown","d40c0be6":"markdown","d2fead4d":"markdown","4314fab3":"markdown","67740253":"markdown","f5866ec5":"markdown","cdee2cbc":"markdown","ebb74676":"markdown","9b7e20c4":"markdown","718e23eb":"markdown","deb0a838":"markdown","ecbb3c68":"markdown","917bcb5f":"markdown","ca2c52d7":"markdown","a4950b9d":"markdown"},"source":{"0b90785a":"import os\nimport time\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nsns.set(style='darkgrid')\n\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom matplotlib import pyplot as plt\n%matplotlib inline","a3c371f1":"train_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')\n\nprint('Length of Training Dataset: {}'.format(len(train_data)))\nprint('Length of Test Dataset: {}'.format(len(test_data)))","af03aadb":"def combine_data(train, test):\n    # here we combine the two datasets\n    return pd.concat([train, test], sort=True).reset_index(drop=True)\n\ndef divide_data(all_data):\n    # with this one we can divide them back into train and test\n    return all_data.loc[:890], all_data.loc[891:].drop(['Survived'], axis=1)\n\nall_data = combine_data(train_data, test_data)\nprint('Length of Complete Dataset: {}'.format(len(all_data)))","86c33a29":"# In this cell we have the correct solution for a 100% Kaggle Score.\n# This is only for faster evaluation of our solution.\n# Please don't upload this 100%-solutaion as a Submission to the Competition!\n# that would be not fair to the other Kagglers.\n\nsolution = pd.DataFrame({ 'PassengerId' : test_data['PassengerId'],\n                          'Survived': [0,1,0,0,1,1,0,1,1,0,0,0,1,0,1,1,0,0,0,1,0,\n                                       1,1,1,1,0,1,0,0,0,0,0,1,0,1,0,1,0,1,1,1,0,\n                                       0,0,1,0,1,0,1,1,0,0,1,1,0,0,0,1,0,1,0,0,0,\n                                       1,1,0,0,0,1,1,1,0,0,0,1,0,0,1,0,0,0,0,0,0,\n                                       0,0,0,1,0,1,1,0,1,0,0,1,1,0,0,0,1,0,0,1,1,\n                                       0,1,1,0,0,0,1,0,0,0,0,0,1,0,0,1,0,1,0,1,1,\n                                       0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,\n                                       0,0,0,1,0,0,0,0,1,1,1,0,0,1,1,1,0,0,1,0,0,\n                                       1,1,0,0,0,0,0,1,1,1,1,1,0,0,1,0,1,0,1,0,0,\n                                       0,0,1,0,0,1,0,1,1,0,0,1,0,0,1,0,1,0,1,1,0,\n                                       0,1,0,0,0,0,0,0,1,0,1,0,1,1,1,1,1,0,0,0,0,\n                                       1,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,1,0,1,1,\n                                       0,0,0,0,0,0,1,1,1,0,1,0,1,0,0,0,1,0,0,0,1,\n                                       0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,1,1,0,1,\n                                       0,0,1,0,0,0,1,0,0,0,1,1,0,1,0,1,0,1,0,0,1,\n                                       0,0,0,0,0,0,1,0,0,1,0,1,0,0,0,1,0,0,1,0,0,\n                                       0,0,0,0,0,1,0,1,0,1,0,1,0,1,1,0,0,0,1,0,1,\n                                       0,1,0,0,1,1,0,1,0,0,0,1,1,0,1,1,0,1,1,0,0,\n                                       0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,1,0,1,0,\n                                       0,1,0,1,0,1,1,0,0,1,0,0,1,0,0,1,0,0,1]})","80087c97":"all_data.info()","04f4b626":"train_data.sample(8)","2e363cba":"train_data.drop(['Cabin'], axis=1, inplace=True)\ntest_data.drop(['Cabin'], axis=1, inplace=True)","7d176055":"sns.countplot(train_data.Survived)\nplt.show()","900278f5":"fig, axs = plt.subplots(1,2,figsize=(12,5))\nsns.countplot(train_data.Pclass, ax=axs[0])\nsns.barplot(x='Pclass', y='Survived', data=train_data, ax=axs[1])\nplt.show()","32904035":"fig, axs = plt.subplots(1,2,figsize=(12,5))\nsns.countplot(train_data.Sex, ax=axs[0])\nsns.barplot(x='Sex', y='Survived', data=train_data, ax=axs[1])\nplt.show()","1458e0a1":"fig, axs = plt.subplots(1,2,figsize=(12,5))\naxs[0].hist(train_data.Age, edgecolor='black', bins=15)\naxs[0].set_xlabel('Age')\naxs[0].set_ylabel('count')\nsns.distplot(train_data[train_data['Survived']==1].Age.dropna(), bins=15, label='survived', ax=axs[1], kde=False)\nsns.distplot(train_data[train_data['Survived']==0].Age.dropna(), bins=15, label='not survived', ax=axs[1], kde=False)\naxs[1].legend()\nplt.show()","5fd394fc":"fig, axs = plt.subplots(1,2,figsize=(12,5))\nsns.countplot(train_data.SibSp, ax=axs[0])\nsns.barplot(x='SibSp', y='Survived', data=train_data, ax=axs[1])\nplt.show()","c5c48f1a":"fig, axs = plt.subplots(1,2,figsize=(12,5))\nsns.countplot(train_data.Parch, ax=axs[0])\nsns.barplot(x='Parch', y='Survived', data=train_data, ax=axs[1])\nplt.show()","40f01251":"fig, axs = plt.subplots(1,2,figsize=(12,5))\nsns.countplot(train_data.Embarked, ax=axs[0])\nsns.barplot(x='Embarked', y='Survived', data=train_data, ax=axs[1])\nplt.show()","01f9fbea":"all_data.info()","2ca2637a":"train_data.Name.head(10)","0b409bd5":"whole_data = train_data.append(test_data)\nwhole_data['Title'] = whole_data.Name.str.extract(r'([A-Za-z]+)\\.', expand=False)\nwhole_data.Title.value_counts()","363a8c54":"Common_Title = ['Mr', 'Mrs', 'Master']\nwhole_data['Title'].replace(['Lady', 'Miss', 'Ms', 'Mlle', 'Mme'], 'Mrs', inplace=True)\nwhole_data['Title'].replace(['Sir', 'Rev'], 'Mr', inplace=True)\nwhole_data['Title'][~whole_data.Title.isin(Common_Title)] = 'Others'","02adc69c":"train_data = whole_data[:len(train_data)]\ntest_data = whole_data[len(train_data):]\n\nAgeMedian_by_titles = train_data.groupby('Title')['Age'].median()\nAgeMedian_by_titles","aa248158":"for title in AgeMedian_by_titles.index:\n    train_data['Age'][(train_data.Age.isnull()) & (train_data.Title == title)] = AgeMedian_by_titles[title]\n    test_data['Age'][(test_data.Age.isnull()) & (test_data.Title == title)] = AgeMedian_by_titles[title]","891fde92":"train_data[train_data['Embarked'].isnull()]","23232e90":"train_data['Embarked'].fillna('S', inplace=True)","0bf09c32":"test_data[test_data['Fare'].isnull()]","27eef1d2":"median_fare = all_data.groupby(['Pclass', 'Parch', 'SibSp']).Fare.median()[3][0][0]\nprint('Median Fare: {:.3f}'.format(median_fare))\ntest_data['Fare'].fillna(median_fare, inplace=True)","f409d857":"print(train_data.Fare.sort_values(ascending=False).head(5))\ntrain_data.loc[train_data.Fare>512, 'Fare'] = 263","243f3e5c":"train_data['Sex'] = train_data['Sex'].map({'female':1, 'male':0}).astype('int')\ntest_data['Sex'] = test_data['Sex'].map({'female':1, 'male':0}).astype('int')\n\ntrain_data['Embarked'] = train_data['Embarked'].map({'S':0, 'C':1, 'Q':2}).astype('int')\ntest_data['Embarked'] = test_data['Embarked'].map({'S':0, 'C':1, 'Q':2}).astype('int')","94195ab5":"train_data['AgeBin_5'] = pd.qcut(train_data['Age'], 5)\ntest_data['AgeBin_5'] = pd.qcut(test_data['Age'], 5)\n\nfig = plt.figure(figsize=(12,5))\nsns.barplot(x='AgeBin_5', y='Survived', data=train_data)\nplt.show()","285e5331":"train_data['FareBin_5'] = pd.qcut(train_data['Fare'], 5)\ntest_data['FareBin_5'] = pd.qcut(test_data['Fare'], 5)\n\nfig = plt.figure(figsize=(12,5))\nsns.barplot(x='FareBin_5', y='Survived', data=train_data)\nplt.show()","1dee7124":"label = LabelEncoder()\ntrain_data['AgeBin_Code_5'] = label.fit_transform(train_data['AgeBin_5'])\ntest_data['AgeBin_Code_5'] = label.fit_transform(test_data['AgeBin_5'])\nlabel = LabelEncoder()\ntrain_data['FareBin_Code_5'] = label.fit_transform(train_data['FareBin_5'])\ntest_data['FareBin_Code_5'] = label.fit_transform(test_data['FareBin_5'])","a1a5094f":"train_data['FamilySize'] = train_data.SibSp + train_data.Parch + 1\ntest_data['FamilySize'] = test_data.SibSp + test_data.Parch + 1\n\ntrain_data['Alone'] = train_data.FamilySize.map(lambda x: 1 if x == 1 else 0)\ntest_data['Alone'] = test_data.FamilySize.map(lambda x: 1 if x == 1 else 0)","0588812d":"fig, axs = plt.subplots(1,2,figsize=(12,5))\nsns.countplot(train_data.FamilySize, ax=axs[0])\nsns.barplot(x='FamilySize', y='Survived', data=train_data, ax=axs[1])\nplt.show()\n\nfig, axs = plt.subplots(1,2,figsize=(12,5))\nsns.countplot(train_data.Alone, ax=axs[0])\nsns.barplot(x='Alone', y='Survived', data=train_data, ax=axs[1])\nplt.show()","bfbaae52":"fig, axs = plt.subplots(1,2,figsize=(12,5))\nsns.countplot(train_data.Title, ax=axs[0])\nsns.barplot(x='Title', y='Survived', data=train_data, ax=axs[1])\nplt.show()","66e955e7":"train_data['Title_Code'] = train_data.Title.map({'Mr':0, 'Mrs':1, 'Master':2, 'Others':3}).astype('int')\ntest_data['Title_Code'] = test_data.Title.map({'Mr':0, 'Mrs':1, 'Master':2, 'Others':3}).astype('int')","db4552ae":"train_data[['Name', 'Ticket']].sort_values('Name').head(20)","19e0a398":"whole_data = train_data.append(test_data)\nwhole_data['Surname'] = whole_data.Name.str.extract(r'([A-Za-z]+),', expand=False)\nwhole_data['TixPref'] = whole_data.Ticket.str.extract(r'(.*\\d)', expand=False)\nwhole_data['SurTix'] = whole_data['Surname'] + whole_data['TixPref']\nwhole_data['IsFamily'] = whole_data.SurTix.duplicated(keep=False)*1\nsns.countplot(whole_data.IsFamily)\nplt.show()","aa2f14fa":"whole_data['Child'] = whole_data.Age.map(lambda x: 1 if x <= 13 else 0)\nFamilyWithChild = whole_data[(whole_data.IsFamily==1)&(whole_data.Child==1)]['SurTix'].unique()\nprint('There are {} families with children.'.format(len(FamilyWithChild)))","fef82298":"whole_data['FamilyId'] = 0\nx = 1\nfor tix in FamilyWithChild:\n    whole_data.loc[whole_data.SurTix==tix, ['FamilyId']] = x\n    x += 1","4e27cb00":"whole_data['SurvivedDemo'] = whole_data['Survived'].fillna(9)\npd.crosstab(whole_data.FamilyId, whole_data.SurvivedDemo).drop([0]).plot(kind='bar', stacked=True, color=['black','g','grey'], figsize=(13,5))\nplt.show()","403271bd":"whole_data['ConnectedSurvival'] = 1 \nSurvived_by_FamilyId = whole_data.groupby('FamilyId').Survived.sum()\nfor i in range(1, len(FamilyWithChild)+1):\n    if Survived_by_FamilyId[i] >= 1:\n        whole_data.loc[whole_data.FamilyId==i, ['ConnectedSurvival']] = 2\n    elif Survived_by_FamilyId[i] == 0:\n        whole_data.loc[whole_data.FamilyId==i, ['ConnectedSurvival']] = 0\ntrain_data = whole_data[:len(train_data)]\ntest_data = whole_data[len(train_data):]\nsns.barplot(x='ConnectedSurvival', y='Survived', data=train_data)\nplt.show()","7a4606e8":"X_train = train_data.drop(['Name', 'Parch', 'PassengerId', 'SibSp', \n                           'Ticket', 'Title', 'AgeBin_5', 'FareBin_5',  \n                           'Surname', 'TixPref', 'SurTix', 'IsFamily', 'Child', \n                           'FamilyId', 'Survived', 'SurvivedDemo', 'Age', 'Fare'], axis=1)\n\nY_train = train_data['Survived']","199f23ee":"model = RandomForestClassifier(n_estimators=600, random_state=2)\n\nmodel.fit(X_train,Y_train)\nimportance = pd.DataFrame({'feature':X_train.columns, 'importance': np.round(model.feature_importances_,3)})\nimportance = importance.sort_values('importance', ascending=False).set_index('feature')\n\nimportance.plot(kind='bar', rot=90, figsize=(8,3))\nplt.show()","c84ef015":"final = ['Title_Code', 'ConnectedSurvival', 'FareBin_Code_5', 'Pclass',  'FamilySize']","bcad5ecd":"model = RandomForestClassifier(n_estimators=100, bootstrap=True, criterion='entropy',\n                               min_samples_leaf=5, min_samples_split=4, random_state=42)\n\nmodel.fit(X_train[final],Y_train)\n\n# this function compares two numeric data columns and returns the percentage difference\ndef compare(col1, col2):\n    new = abs(col1-col2)\n    return (1- (new.sum() \/ len(new)))\n\nprediction_train = model.predict(X_train[final])\noutput_train = pd.DataFrame({'PassengerId': train_data.PassengerId, 'Survived': prediction_train.astype(int)})\nprint('Training Accuracy: {:.5f}'.format(compare(output_train['Survived'], train_data['Survived'])))","345a85a6":"X_test = test_data[final]\nprediction = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': prediction.astype(int)})\noutput.to_csv(time.strftime('submission_%y%m%d_%H%M.csv', time.localtime()), index=False)\n\nprint('Kaggle Score: {:.4f}'.format(compare(output['Survived'], solution['Survived'])))","42576814":"### Cabin","c07fe0d8":"### Embarked","f4692949":"Now I can choose my final features. I don't use the suggestion by the importance plot above exactly. I ignore the ```Sex```, because this information is already in ```Titl```, just in another form. Also I ignore the ```Age``` feature, because I registered a lesser performance with it. Please notice that a less number of features avoids overfitting.","9e8db28c":"In this notebook I reached a Kaggle Score of 0.8134, which is in the best 3% of the competition leaderboard.","87558d2d":"Common titles in the list above are Mr, Mrs and Master. Some of the titles (Miss, Ms, Lady, ...) can be grouped to the common ones. Unclassified titles can be frouped to a new group \"Others\" to reduce the number of used titles.","6f157324":"In this chapter we are finally training the model. All the parameters are optimized for this training data. We are training with the selected final features. After fitting we compare our solution with the correct answers given in the dataset and reaching a training accuracy of 85%.","45ae8da5":"### Number of Siblings\/Spouse on Board (SipSp)","12a0f6f1":"In this section we will look at the hole data and analyse their importance for the next steps. To list all the available features of the dataset we can use ```info()``` to print a summary. Now we know the column names in the dataset with the corresponding datatype and a Non-Null count, which shows us how many things are missing.","3495a25a":"With a look at some example passengers in the dataset we see all the features with their entries. In the next steps we will look at each feature seperatly and analyse the data to give a first forecast of the relevance to survive this disaster.\n\nFrom the table, we can note a few things. First of all, that we need to convert a lot of features into numeric, so that the machine learning algorithms can process them. Furthermore, we can see that the features have widely different ranges, that we will need to convert into roughly the same scale. We can also spot some more features, that contain missing values (NaN = not a number), that wee need to deal with.","511f9759":"The passenger class ```Pclass``` shows the class for every passenger as a proxy for the socio-economic status. So 1st = Upper, 2nd = Middle and 3rd = Lower class. In the left plot we see, that most of the people are in 3rd class. The 1st and 2nd class are nearly even distributed.\n\nAfter that we want to see, if the passenger class is related to the chance to survive. And yes, passengers in 1st class are a lot more likely to survive than passengers in 3rd class. The ```Class``` feature is a good index for survival prediction. Also with the numeric values it is easily to use for an algorithmic prediction model. No further adjustment necessary.","850743f7":"Now we will dig out every family with children (with Age >13) on board. Than we will encode the families with chilrens with generating unique family ID's.","7f224737":"The ```Cabin``` data is showing the cabin number of the corresponding passanger. In the summary we see, that we have only 295 from 1309 entries, which means that we don't know the data for more than 70% of the passengers. So Cabin is not able to provide any meaning information to the learning process. So I decide to delete it from the dataset.","29dc79f6":"In this chapter we want to create new features and look where we can optimize the data in new ways or cluster them to completly new features.","8a271efb":"There is only one passenger in the test dataset with missing ```Fare``` value. When we assume, that the Fare is related to the Passenger Class ```Pclass``` and the family size (```Parch``` and ```SibSp```) we can extract the median value of the other corresponding passengers. The relevant passenger is male, 60 years old, 3rd class with no family on board.","915100ce":"Let's look at the relationship between the titles and the age and list the median age for every title.","ef9edd7d":"The data from the titanic dataset is provided in two csv-files. In this files we find the training and test data. The test data is structured the same like training except of the ```Survived``` information. After successfully loading the files we can look on the size of the dataset. We have 891 entries for training and 418 for testing. This is a good breakdown of the data.","70b7d120":"The ```SipSp``` feature handling the number of Siblings (brothers and sisters) and Spouses (husband or wife) on board. When we look at the given data we see, that most of the passengers are not traveling with siblings or spouses. All the passengers travelling with 1-2 siblings\/spouses are more likely to survive than the others.","a98144f2":"### Gender and Embarked","7097af1c":"### Survived","7de6bb04":"In this chapter we will analyze the importance of the features to select some of them for the final training. We delete some features which are not necessary and create the training inputs in ```X_train``` and the target survival value in ```Y_train```.","af79e66a":"For the ```Embarked``` feature we don't know the port for two of the passengers in the training dataset. When we look at the passenger data we see, that both are female, upper class, with the same Cabin and both survived the titanic disaster. On the website (https:\/\/www.encyclopedia-titanica.org\/titanic-survivor\/martha-evelyn-stone.html) you find the following information:\n\n\"*Mrs Stone boarded the Titanic in Southampton ... and was travelling in first class with her maid Amelie Icard ...*\"\n\nAccording this information we will fill the missing data with \"S\" for Southampton.","b0de2a4d":"For better data analysis we schould combine the train and test data to a big new dataset. This allows us to look at the dependencies of the whole dataset instead of every part seperatly.","dcdbb6de":"### Age","aa1b0e71":"To setup the Notebook we import the necessary libraries. For data processing we use ```pandas``` and to visualize we are using ```seaborn``` and ```matplotlib```. All the needed algorithmic parts for learning the data are provided by ```sklearn```.","75cccd34":"The ```Sex``` feature says which gender the passengers are in \"male\" and \"female\". The proportion between male and female is 2\/1, so we have twice as many men as women on board. Like the passenger class, the gender is a really good feature for survival prediction. Female passengers have a ~70% chance to survive, but men only have a chance of 20%. That's a lot of difference. For further usage of this feature we have to encode the strings intro numbers later.","4d414da0":"### Number of Parents\/Childrens (Parch)","9a0a5444":"## <center>FEATURE SELECTION<\/center>","d7fab3c4":"### Title","ec1f1a53":"## <center>SUBMISSION<\/center>","af3b7b6e":"We created ```Title``` for eliminating the missing entries from the Age. But it may be usefull to use the Title as a new feature. So let's firstly look at the survival rate for every Title. We see, that the most passengers are \"Mr.\" with the lessest chance to survive. All the others have a much better chance. After that we will encode the Title into ```Title_Code``` in numbers from 0-4.","b60cdcd5":"We see, that people with same surnames have the same or similar ticket names. So we will extract the surnames and ticket names to find duplicates in the dataset. It is also possible that there are passengers in train and test set from the same family. We save the Results in a new feature, called ```IsFamily```. In the plot we see, that 2\/3 peoples are travelling with their family.","23485811":"### Embarked","639c215e":"### Connected Survival","74b73394":"The ```SibSp``` and ```Parch``` features are both related to the number of family members. We will combine them to a single feature named ```FamilySize```. After That we can group them into travelling alone or not in the feature ```Alone```. There we have a binary feature. The chance to survive, when you are travelling alone is about 30%, when you are travelling with family it's about 50%.","d0b5246e":"We have 177 entries in the train dataset, where we don't know the age of the passenger. In the used approach I will use the titles from the ```Name``` feature to approximate the mean age for every title. This information then will be used for filling the missing age values.\n\nSo firstly we look at a sample of the Names feature. Then extract all titles and list them to find the most used ones. ","342d37c5":"### Age and Fare","cc24c0c9":"### Gender","6b9b4b88":"For each family above, if there is at least one member survived, we assume that the others have a chance to survive too. The propability to survive is much higher for passangers travelling with the family, having one or more children and having one or more survivors in the family.","14d6b977":"### Travelling Alone","d916f244":"## <center>SETUP THE NOTEBOOK<\/center>","1f738a2b":"In this Chapter we will finally predict the missing ```Survived``` solution for the test dataset. The solution will be written in a csv-file to submit it to the competition. We will also compare our solution with the correct answer to directly see our reached kaggle score.","6d2c3879":"The ```Survived``` information is the relevant data which has to be predicted for the test dataset. In the train dataset we have a overall survival rate of approx. 38%. The survival-state is a boolean which shows 0 = deceased and 1 = survived.","2223d41d":"### Age","a05be621":"### Passanger Class","01042e7e":"For importance prediction we use a Random Forest Classifier. In the plot we can look at the feature importance. We see, that the ```Title``` has the best value, followed by the ```Sex```.","5c0279b1":"## <center>FEATURE CREATION<\/center>","a7dd3e69":"The ```Parch``` feature is similar to ```SipSp``` and showing the number of Parents or Childrens on board. More than 70% of the passengers are not travelling with parents\/childrens. But when you are not travelling alone you have a better chance to survive.","d40c0be6":"To better use the ```Age``` and ```Fare``` feature we will group the data into classes. So the Age will be grouped into 9 classes depending on the age. (first group: 0-16 years; second group: 16-21; etc.). Also the Fare will be grouped to 9 classes. After we group the data, we will encode the Age\/Fare groups into bins for modelling. As example: Age Group 0-16 is Bin 0.","d2fead4d":"## <center>DATA ANALYSIS<\/center>","4314fab3":"Now we can impute the missing age values according to the median title ages.","67740253":"The ```Embarked``` feature is showing the port, where the passengers joined the titanic tour. There we have S = Southampton, Q = Queenstown and C = Cherbourg. Most of the people embarked in Southampton. When you embarked in Cherbourg you have the best possabilities to survive.","f5866ec5":"In the train dataset we have 3 main outliers (see value of 512 in the list). We have to eliminate this, because outlierts have a negative impact on the training (distort the distribution). We can replace the value with the maximum or median value. In this case I decide to set them to the second highest value.","cdee2cbc":"## <center>MISSING DATA \/ OUTLIERS<\/center>","ebb74676":"# Titanic Survival Prediction\n\nIn this Notebook we will analyse the Titanic Disaster Dataset corresponding to the Kaggle Titanic Competition (https:\/\/www.kaggle.com\/c\/titanic). We will look at the data seperatly and try to learn the dependencies in the data and how to learn the data with a Random Forest Model.\n\nWith this Notebook I reach a Kaggle-Score of 0.8134, which is part of the 3% best solutions.","9b7e20c4":"## <center>MODEL<\/center>","718e23eb":"For me it makes sense if the ```Age``` feature is correlated to the survival rate, because in this disaster situations everybody on board would prefer the kids. So let's look at the Age distribution of the passengers. Most of the people on board are between 20-40 years old. You have a good chance to survive if you are between 0-5 and 20-35 years old.","deb0a838":"The features ```Sex``` and ```Embarked``` are given in strings. Now we will encode them into numbers. When group the data into bins the model will be more robust und we avoid overfitting. The Gender is transformed with Male = 0 and Female = 1. The Embarked Code will be S = 0, C = 1 and Q = 2.","ecbb3c68":"## <center>DATA TRANSFORMATION<\/center>\n\nWith the last chapter we filled missing data and removed outliers. Now we are ready to transform the data with encoding the strings to numbers for better modelling the problem.","917bcb5f":"### Fare","ca2c52d7":"I found the idea of connected survival in the article from Tim Chan published on Medium. (source: https:\/\/medium.com\/analytics-vidhya\/kaggle-titanic-survival-prediction-top-3-ea6c8dcc9b6c)\n\nIt is obvious that family groups will sourvive better, because they help each other. In addition children have a first priority for the safe boats. To find family groups we look at the Surnames and the Tickets.","a4950b9d":"With the Family ID's we can plot the survival state of each family individually. The Black data says that the family members are dead. Green data is when they survived. And Gray are the unknown members from the test dataset. The shown results proves the concept of connected survival."}}