{"cell_type":{"1025513b":"code","d9bdc5b8":"code","16438954":"code","420bb458":"code","f5e9bdee":"code","be7360de":"code","1a35901a":"code","4dc0d8bc":"code","46fd69bd":"code","07695d00":"code","fd8f34bf":"code","667b1227":"code","91c5dbb3":"code","ae986b88":"code","44cdedea":"code","9558627f":"code","b825ef60":"code","abf7a667":"code","b53231ef":"code","abdb87da":"code","83a14f86":"code","b078a612":"code","1f8dbc3c":"code","93d477be":"code","17c28f91":"code","9f63c807":"code","cdbaa083":"code","9058735d":"code","500b3662":"code","80e47ca3":"code","5142292d":"code","88596354":"code","d76e55cd":"code","9ad9de24":"code","0de35b50":"code","7946252b":"code","d79565e0":"markdown","0e20ab6b":"markdown","5d2a3c84":"markdown","e169c02b":"markdown","3bbf8ac5":"markdown","7e9ebae4":"markdown","f071d2a4":"markdown","e3af2b66":"markdown","d0b159c5":"markdown","fea7f3ac":"markdown","d3c785a9":"markdown","4be18d42":"markdown","e2ee16e9":"markdown","8dec4177":"markdown","bf0fc03f":"markdown","72c15ba8":"markdown","2f2b5730":"markdown","0d2a32ba":"markdown","7ba9ea39":"markdown","a07b7fcc":"markdown","4a9ac22a":"markdown","1a7385bd":"markdown","96ddb978":"markdown","fb1e0b4e":"markdown","5f926cee":"markdown","e99872bd":"markdown","2502d9b7":"markdown","a6064af4":"markdown","94e72419":"markdown","7e7b8a97":"markdown","5faae3d9":"markdown"},"source":{"1025513b":"!pip install prophet\n!pip install -U scikit-learn","d9bdc5b8":"import pandas as pd\nimport numpy as np\nimport io\nimport requests\n\nfrom pandas.tseries.offsets import DateOffset\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.ensemble import RandomForestRegressor\nfrom lightgbm import LGBMRegressor\n\nimport statsmodels.api as sm\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nimport statsmodels.graphics.tsaplots as sgt\nimport statsmodels.tsa.stattools as sts\n\nimport matplotlib.pyplot as plt\nfrom pylab import rcParams\n\nfrom prophet import Prophet\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","16438954":"url=\"http:\/\/favt.gov.ru\/opendata\/7714549744-statpassga\/data-20210201-structure-20151111.csv\"\nheaders = {\"User-Agent\": \"Mozilla\/5.0 (Macintosh; Intel Mac OS X 10.14; rv:66.0) Gecko\/20100101 Firefox\/66.0\"}\nreq = requests.get(url, headers=headers)\ndata = io.StringIO(req.text)\ndf = pd.read_csv(data, sep = ';', engine=\"python\", encoding=\"cp1251\", parse_dates={'date':[0,1]})\ncols = ['mvl', 'dz', 'sng', 'inner', 'current', 'total']\npd.to_datetime(df['date'], format='%Y-%m')\ndf.set_index('date', inplace=True)\ndf.columns = cols\ndf","420bb458":"df.info()","f5e9bdee":"BEGIN_YEAR = 2015\nEND_YEAR = 2019\ndf = df[(df.index.year>=BEGIN_YEAR)&(df.index.year<=END_YEAR)]","be7360de":"for col in cols:\n  df[col]=df[col].str.replace(\" \", \"\").astype('int')","1a35901a":"data = df[['total']]","4dc0d8bc":"rcParams['figure.figsize'] = 20, 10\ndecomposition = sm.tsa.seasonal_decompose(data, model='additive') # additive seasonal index\ndecomposition.plot()\nplt.show()","46fd69bd":"train_len = len(data)-12\ntrain = data[0:train_len]\ntest = data[train_len:] ","07695d00":"def plotMovingAverage(series, n):\n    \n    rolling_mean = series.rolling(window=n).mean()\n    #we can also plot 95% confidence intervals\n    rolling_std =  series.rolling(window=n).std()\n    upper_bond = rolling_mean+1.96*rolling_std\n    lower_bond = rolling_mean-1.96*rolling_std\n\n    plt.figure(figsize=(15,5))\n    plt.title(\"Moving average\\n window size = {}\".format(n))\n    plt.plot(rolling_mean, \"g\", label=\"Moving average\")\n\n    plt.plot(upper_bond, \"r--\", label=\"CI upper\/lower bounds\")\n    plt.plot(lower_bond, \"r--\")\n    plt.plot(series[n:], label=\"Actual values\")\n    plt.legend(loc=\"upper left\")\n    plt.grid(True)","fd8f34bf":"plotMovingAverage(data, 3)","667b1227":"plotMovingAverage(data, 12)","91c5dbb3":"plotMovingAverage(data, 6)","ae986b88":"adf_test = sm.tsa.stattools.adfuller(data['total'])\nprint('ADF stat.: %f' % adf_test[0])\nprint('Crit value @ 0.01: %.2f' % adf_test[4]['1%'])\nprint('Crit value @ 0.05: %.2f' % adf_test[4]['5%'])\nprint('Crit value @ 0.1: %.2f' % adf_test[4]['10%'])\nprint('p-value: %f' % adf_test[1])","44cdedea":"from statsmodels.tsa.stattools import kpss\nkpss_test = kpss(data['total'], regression='ct')\nprint('KPSS stat: %f' % kpss_test[0])\nprint('Crit value @ 0.01: %.2f' % kpss_test[3]['1%'])\nprint('Crit value @ 0.05: %.2f' % kpss_test[3]['5%'])\nprint('Crit value @ 0.1: %.2f' % kpss_test[3]['10%'])\nprint('p-value: %f' % kpss_test[1])","9558627f":"def make_features(data, lags, rolling_mean_size=12):\n    \n    df = data.copy()\n    df.columns = [\"y\"]\n    \n    #Making sure that there is no information about 2019 in the test set\n    for i in range(12, lags+12, 1):\n     df[\"lag_{}\".format(i)] = df.y.shift(i)\n    \n    #Adding specified moving average\n    df['rolling_mean'] = df['y'].shift(12).rolling(rolling_mean_size).mean()\n    #Adding date-related features\n    df[\"month\"] = df.index.month\n    df[\"year\"] = df.index.year\n    \n    return df","b825ef60":"df_featured = make_features(data, 12, 12)\ndf_featured = df_featured.dropna()","abf7a667":"X = df_featured.drop(columns = 'y')\ny = df_featured.y\nX_train = X[X.index.year !=2019]\nX_test = X[X.index.year ==2019] \ny_train = y[y.index.year !=2019]\ny_test = y[y.index.year == 2019]","b53231ef":"num_cols = [x for x in df_featured.columns if x.startswith('lag') ]\nnum_cols.append('rolling_mean')","abdb87da":"scaler = StandardScaler()\nscaler.fit(X_train[num_cols])\nX_train[num_cols] = scaler.transform(X_train[num_cols])\nX_test[num_cols]=scaler.transform(X_test[num_cols])","83a14f86":"models = [RandomForestRegressor(random_state=42),\n          LGBMRegressor(),\n          Ridge(random_state=42),\n          Lasso(random_state=42)\n         ]\nparam_grid = [{'n_estimators': [50, 100, 200],\n             'max_depth': [3, 4, 8]},\n              \n              {\"max_depth\": [3, 4, 8],\n              'learning_rate': [0.05, 0.1, 0.5]},\n             \n             {'alpha': np.logspace(0,3,4)},\n             \n             {'alpha': np.logspace(0,3,4)}]\n\ncv = TimeSeriesSplit(n_splits=8)\nresults = []\nbest_score = 1000000000\nbest_model = None\nfor i, model in enumerate(models):\n    grid_search = GridSearchCV(estimator=model,\n                          param_grid=param_grid[i],\n                          cv = cv,\n                          scoring = 'neg_mean_squared_error')\n    result = grid_search.fit(X_train, y_train)\n    cv_results = pd.DataFrame(result.cv_results_)[['mean_test_score',\n                                                   'std_test_score',\n                                                   'mean_fit_time',\n                                                   'mean_score_time',\n                                                   'rank_test_score']]\n    cv_results['Model'] = str(model).split('(')[0]\n    cv_results['RMSE'] = np.sqrt(abs(cv_results['mean_test_score']))\n    if cv_results['RMSE'].min() < best_score:\n        best_score=cv_results['RMSE'].min()\n        best_model = result.best_estimator_\n    results.append(cv_results)\nbest_model","b078a612":"result.best_estimator_","1f8dbc3c":"pd.concat([result for result in results], axis=0).sort_values(by='RMSE', ascending = True).reset_index(drop=True).head(10)","93d477be":"y_hat_lasso = pd.Series(best_model.predict(X_test))\ny_hat_lasso.index = y_test.index","17c28f91":"plt.figure(figsize=(20,5))\nplt.grid()\nplt.plot(train, label='Train')\nplt.plot(test, label='Test')\nplt.plot(y_hat_lasso, label='Forecast')\n\nplt.legend(loc='best')\nplt.title('Lasso Forecast')\nplt.show()","9f63c807":"rmse = np.sqrt(mean_squared_error(y_test, y_hat_lasso)).round(2)\nmape = (mean_absolute_percentage_error(y_test, y_hat_lasso)*100).round(2)\nresults=pd.DataFrame()\ntempResults = pd.DataFrame({'Method':['Lasso'], 'RMSE': [rmse],'MAPE': [mape]})\nresults = pd.concat([results, tempResults])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","cdbaa083":"y_hat_hwa = test.copy()\nmodel = ExponentialSmoothing(np.asarray(train) ,seasonal_periods=12 ,trend='add', seasonal='add')\nmodel_fit = model.fit(optimized=True)\nprint(model_fit.params)\ny_hat_hwa['hw_forecast'] = model_fit.forecast(12)","9058735d":"plt.figure(figsize=(20,5))\nplt.grid()\nplt.plot(train, label='Train')\nplt.plot(test, label='Test')\nplt.plot(y_hat_hwa['hw_forecast'], label='Model forecast ')\nplt.legend(loc='best')\nplt.title('Holt-Winters model forecast')\nplt.show()","500b3662":"rmse = np.sqrt(mean_squared_error(test, y_hat_hwa['hw_forecast'])).round(2)\nmape = (mean_absolute_percentage_error(test, y_hat_hwa['hw_forecast'])*100).round(2)\n\ntempResults = pd.DataFrame({'Method':['Holt-Winters model'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","80e47ca3":"prophet_train=train.reset_index()\nprophet_train.columns = ['ds', 'y']","5142292d":"m = Prophet( )\nm.fit(prophet_train)","88596354":"pred = Prophet.make_future_dataframe(m, periods=12, freq='M')\npred[-12:]['ds'] = pred[-12:]['ds']+DateOffset(days = 1)","d76e55cd":"forecast = m.predict(pred)\nforecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()\nprediction = pd.Series(forecast[-12:].yhat)\nprediction.index = test.index","9ad9de24":"fig1 = m.plot(forecast)","0de35b50":"plt.figure(figsize=(20,5))\nplt.grid()\nplt.plot(train, label='Train')\nplt.plot(test, label='Test')\nplt.plot(prediction, label='Prophet forecast')\nplt.legend(loc='best')\nplt.title('Forecast with Prophet')\nplt.show()","7946252b":"rmse = np.sqrt(mean_squared_error(test, prediction)).round(2)\nmape = (mean_absolute_percentage_error(test, prediction)*100).round(2)\n\ntempResults = pd.DataFrame({'Method':['Prophet'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults.reset_index(drop=True)\n","d79565e0":"## Exploring and processing data","0e20ab6b":"data must be fed in a certain way, so some minor changes are required","5d2a3c84":"## Checking stationarity","e169c02b":"Exctracting *total* column for further use","3bbf8ac5":"For training we'll use data from 2010 and afterwards, and we will not use the data for 2020 for obvious reasons","7e9ebae4":"### Testing the best model","f071d2a4":"## Conclusion","e3af2b66":"Depending on the metric, the best result was achieved either by Prophet - minimal RMSE or by Regression model - minimal MAPE\n- Overall, all three models seem to be working good for non-stationary data,\n- Using Regression models is the most time-consuming relative to other methods. (generating features, comparing models, tuning the parameters)\n- I didn't spend much time with Prophet and did the bare minimum required for it to work, maybe after further exploration it can provide significantly better results\n","d0b159c5":"Splitting the data - we'll explore only prior to 2019","fea7f3ac":"## Fetching data\n* Loading the csv file from the agency's website using *requests* module\n* assigning columns manually due to bad encoding \n\n(labels taken from dataset description and really don't matter right now, we'll be working with the *total* column)","d3c785a9":"# Forecast with Prophet library","4be18d42":"KPSS test Null hypothesis suggests trend-stationarity","e2ee16e9":"To use linear models we'll generate some features from available infromation\n* lagging values\n* n-months moving average\n* month\n* year","8dec4177":"We'll compare several regression models with different hyperparameters with GridSearchCV ","bf0fc03f":"We can reject H0 at 5%, but cannot quite reject it at 1%","72c15ba8":"First cell shoud be executed (Prophet library and latest sklearn version were used)","2f2b5730":"in this model we can take in the account trend and seasonality in data","0d2a32ba":"### Plotting predictions","7ba9ea39":"### Forecast with Exponential Smoothing aka Holt-Winters","a07b7fcc":"Top 10 results by cross-validation","4a9ac22a":"### Importing libraries","1a7385bd":"Scaling numerical features","96ddb978":"* We can see a clear upward trend starting from 2016\n* Seasonality has a 1-year period","fb1e0b4e":"## Forecast\n### Linear Models","5f926cee":"Sesonal decomposition - looking at trend, seasonality and residuals","e99872bd":"Assigning proper data type","2502d9b7":"## Looking at Moving Averages","a6064af4":"We cannot reject the null hypotesis at any level","94e72419":"Making train and test sets","7e7b8a97":"Augmented Dickey Fuller test tests the null hypothesis that a unit root is present in a time series sample.","5faae3d9":"# Predicting volume of passengers flown by russian airlines in 2019\n\nIn this notebook, I am working with an open dataset available at the russian Federal Air Transport Agency website.\n<br>The dataset provides numbers of passengers flown by russian airlines from 2001 to 2020 (last updated in Feb 2021)\n\nThe goal is to predict the total volume of passengers flown in 2019 with different methods and compare the results. \n<br>The methods I've chosen to try out:\n* Regression models\n* Exponential Smoothing aka Holt-Winters\n* Prophet library\n\nMetrics for model evaluation:\n* Root of Mean Squared Error (RMSE)\n* Mean Absolute Percentage Error (MAPE)"}}