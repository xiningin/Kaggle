{"cell_type":{"03dfd133":"code","7e3a542c":"code","61ba6b46":"code","88479514":"code","79f8cb8c":"code","21763507":"code","2cdb793e":"code","e1606b98":"code","33ebb45b":"code","54bd968b":"code","4ccd3d62":"markdown","fb45b065":"markdown","e3501013":"markdown","9a99f687":"markdown","46cda0eb":"markdown","d378966a":"markdown","2fef76e5":"markdown","c530dcb7":"markdown","b075241b":"markdown","272860a2":"markdown","fd767a17":"markdown"},"source":{"03dfd133":"import numpy as np\nimport pandas as pd\n\nfrom hyperopt import hp, tpe\nfrom hyperopt.fmin import fmin\n\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import make_scorer\n\nimport xgboost as xgb\n\nimport lightgbm as lgbm","7e3a542c":"df = pd.read_csv('..\/input\/train.csv')\n\nX = df.drop(['id', 'target'], axis=1)\nY = df['target']","61ba6b46":"def gini(truth, predictions):\n    g = np.asarray(np.c_[truth, predictions, np.arange(len(truth)) ], dtype=np.float)\n    g = g[np.lexsort((g[:,2], -1*g[:,1]))]\n    gs = g[:,0].cumsum().sum() \/ g[:,0].sum()\n    gs -= (len(truth) + 1) \/ 2.\n    return gs \/ len(truth)\n\ndef gini_xgb(predictions, truth):\n    truth = truth.get_label()\n    return 'gini', -1.0 * gini(truth, predictions) \/ gini(truth, truth)\n\ndef gini_lgb(truth, predictions):\n    score = gini(truth, predictions) \/ gini(truth, truth)\n    return 'gini', score, True\n\ndef gini_sklearn(truth, predictions):\n    return gini(truth, predictions) \/ gini(truth, truth)\n\ngini_scorer = make_scorer(gini_sklearn, greater_is_better=True, needs_proba=True)","88479514":"def objective(params):\n    params = {'n_estimators': int(params['n_estimators']), 'max_depth': int(params['max_depth'])}\n    clf = RandomForestClassifier(n_jobs=4, class_weight='balanced', **params)\n    score = cross_val_score(clf, X, Y, scoring=gini_scorer, cv=StratifiedKFold()).mean()\n    print(\"Gini {:.3f} params {}\".format(score, params))\n    return score\n\nspace = {\n    'n_estimators': hp.quniform('n_estimators', 25, 500, 25),\n    'max_depth': hp.quniform('max_depth', 1, 10, 1)\n}\n\nbest = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=10)","79f8cb8c":"print(\"Hyperopt estimated optimum {}\".format(best))","21763507":"def objective(params):\n    params = {\n        'max_depth': int(params['max_depth']),\n        'gamma': \"{:.3f}\".format(params['gamma']),\n        'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),\n    }\n    \n    clf = xgb.XGBClassifier(\n        n_estimators=250,\n        learning_rate=0.05,\n        n_jobs=4,\n        **params\n    )\n    \n    score = cross_val_score(clf, X, Y, scoring=gini_scorer, cv=StratifiedKFold()).mean()\n    print(\"Gini {:.3f} params {}\".format(score, params))\n    return score\n\nspace = {\n    'max_depth': hp.quniform('max_depth', 2, 8, 1),\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, 1.0),\n    'gamma': hp.uniform('gamma', 0.0, 0.5),\n}\n\nbest = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=10)","2cdb793e":"print(\"Hyperopt estimated optimum {}\".format(best))","e1606b98":"def objective(params):\n    params = {\n        'num_leaves': int(params['num_leaves']),\n        'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),\n    }\n    \n    clf = lgbm.LGBMClassifier(\n        n_estimators=500,\n        learning_rate=0.01,\n        **params\n    )\n    \n    score = cross_val_score(clf, X, Y, scoring=gini_scorer, cv=StratifiedKFold()).mean()\n    print(\"Gini {:.3f} params {}\".format(score, params))\n    return score\n\nspace = {\n    'num_leaves': hp.quniform('num_leaves', 8, 128, 2),\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, 1.0),\n}\n\nbest = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=10)","33ebb45b":"print(\"Hyperopt estimated optimum {}\".format(best))","54bd968b":"rf_model = RandomForestClassifier(\n    n_jobs=4,\n    class_weight='balanced',\n    n_estimators=325,\n    max_depth=5\n)\nparams={'n_estimators': list(range(40,61, 1))}\n\nxgb_model = xgb.XGBClassifier(\n    n_estimators=250,\n    learning_rate=0.05,\n    n_jobs=4,\n    max_depth=2,\n    colsample_bytree=0.7,\n    gamma=0.15\n)\n\nlgbm_model = lgbm.LGBMClassifier(\n    n_estimators=500,\n    learning_rate=0.01,\n    num_leaves=16,\n    colsample_bytree=0.7\n)\n\nmodels = [\n    ('Random Forest', rf_model),\n    ('XGBoost', xgb_model),\n    ('LightGBM', lgbm_model),\n]\n\nfor label, model in models:\n    scores = cross_val_score(model, X, Y, cv=StratifiedKFold(), scoring=gini_scorer)\n    print(\"Gini coefficient: %0.4f (+\/- %0.4f) [%s]\" % (scores.mean(), scores.std(), label))","4ccd3d62":"So hyperopt hass explored the parameter space again - let's have a look at the estimated optimum!","fb45b065":"Next up we define the gini evaluation functions. This is not a standard metric so we need to code it  ourself (actually i borrowed the code for this).","e3501013":"## Tuning Random Forest\n\nIn this part we'll tune the random forest classsifier, using hyperopt library. The hyperopt library has a similar purpose as gridsearch, but instead of doing an exhaustive search of the parameter space it evaluates a few well-chosen data points and then extrapolates the optimal solution based on modeling. In practice that means it often needs much fewer iterations to find a good solution.\n\nRandom forests work by averaging predictions from many decision trees - the idea is that by averaging many trees the mistakes of each tree are ironed out. Each decision tree can be somewhat overfitted, by averaging them the final result should be good.\n\nThe important parameters to tune are:\n* Number of trees in the forest (n_estimators)\n* Tree complexity (max_depth)","9a99f687":"## Tuning LightGBM\n\nLightGBM is very similar to xgboost, it is also uses a gradient boosted tree approach. So the explanation above mostly holds also.\n\nThe important parameters to tune are:\n* Number of estimators\n* Tree complexity - in lightgbm that is controlled by number of leaves (num_leaves)\n* Learning rate\n* Feature fraction\n\nWe will fix number of estimators to 500 and learning rate to 0.01 (chosen experimentally) and tune the remaining parameters with hyperopt. Then later we could revisit for better results! ","46cda0eb":"## Tuning XGBoost\n\nSimilar to tuning above, now we will tune xgboost parameters using hyperopt!\n\nXGBoost is also an based on an ensemble of decision trees, but different from random forest. The trees are not averaged, but added. The decision trees are trained to correct residuals from the previous trees. The idea is that many small decision trees are trained, each adding a bit of info to improve overall predictions.\n\nI will follow this guide for tuning [Tuning XGBoost](https:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/complete-guide-parameter-tuning-xgboost-with-codes-python\/)\n\nI'm initially fixing the number of trees to 250 and learning rate to 0.05 (determined that with a quick experiment) - then we can find good values for the other parameters. Later we can re-iterate this.\n\nThe most important parameters are:\n* Number of trees (n_estimators)\n* Learning rate - later trees have less influence (learning_rate)\n* Tree complexity (max_depth)\n* Gamma - Make individual trees conservative, reduce overfitting \n* Column sample per tree - reduce overfitting","d378966a":"So we can see hyperopt exploring parameter space above - let's see what hyperopt estimates as the optimal parameters?","2fef76e5":"## Intepretation of tuning\n\nIt's interesting to compare the parameters that hyperopt found for random forest and XGBoost. Random forest ended up with 375 trees of depth 7, where XGBoost has 250 of depth 5. This fits the theory that random forest averages many complex (independantly trained) trees to get good results, where xgboost & lightgbm (boosting) add up many simple trees (trained on residuals).","c530dcb7":"## Preparation\n\nFirst we load some libraries...","b075241b":"## Comparing the models\n\nNow let's see how the models perform - if hyperopt has determined a sensible set of parameters for us...","272860a2":"Now let's see what hyperopt estimates as the optimum?","fd767a17":"Next we load the data ; note i'm only loading training data because i'm just tuning parameters and comparing models  in this notebook. To speed up tuning, you can also load part of the data (add  nrows=50000), note this will give different results though, so you should re-tune with full data."}}