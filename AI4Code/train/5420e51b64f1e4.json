{"cell_type":{"ecd2e453":"code","d8096ee5":"code","4335c826":"code","9945570f":"code","13f053ac":"code","35a01c9b":"code","c0d3d9ae":"code","39c8e4d7":"code","1dce0b64":"code","5f03a3f4":"code","28864913":"code","70c29836":"code","b3ab5da0":"code","c325361c":"code","17900f8f":"code","99f2ce83":"code","e8d39570":"code","eba1c7a4":"code","7c0c5d6d":"code","b95f44cf":"code","cb2b3b3b":"code","5aab11da":"code","fdd2b27b":"code","d6fa4240":"code","c0a446e4":"code","d07a537e":"code","94d12bca":"code","710305c4":"code","3921ca28":"code","139431c7":"code","455e9bc5":"code","fa0bde97":"code","0ae9f937":"code","183ef73e":"code","58fec138":"code","6f8e8684":"code","98c155c5":"code","6fe2481a":"code","b8cda34d":"code","89a9cad8":"code","6eedb3c9":"code","3ee462ef":"code","cbafaee9":"code","e7a40599":"code","7a3839a7":"code","af6202aa":"code","9a8333c7":"code","b0611973":"code","b3290c7b":"code","ec4c6502":"code","95d0bea2":"code","3087b1d5":"code","d75e6194":"code","03c0a7c3":"code","2e13c391":"code","dda32cc5":"code","0cdd85b0":"code","cf11a712":"code","71f7c3ab":"markdown","10cd2a4f":"markdown","42ba738e":"markdown","a1846337":"markdown","816572be":"markdown","507d77c2":"markdown","96948add":"markdown","b996b3f4":"markdown","ac1763b8":"markdown","8cbc53b8":"markdown","2ffbf3ce":"markdown","4bb1309b":"markdown","d033b118":"markdown","acc6440d":"markdown","2e03114d":"markdown","de19d462":"markdown","cbddfe0c":"markdown","ab5f2d4c":"markdown","659d6ce4":"markdown","1be171ca":"markdown","675de3df":"markdown","49a823c1":"markdown","adf0f1b0":"markdown","26dbebf4":"markdown","577c38b3":"markdown","504e8d39":"markdown","673d8d1b":"markdown","2dd4d9d1":"markdown","512c2654":"markdown","31b74140":"markdown","64693d0c":"markdown","d0e32a54":"markdown","ab637424":"markdown","5d908870":"markdown","cd2cfd22":"markdown"},"source":{"ecd2e453":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","d8096ee5":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pylab import rcParams\nimport matplotlib.cm as cm\n\nimport sklearn\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\nfrom sklearn.metrics import classification_report, precision_score, recall_score, f1_score, accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\nfrom sklearn.ensemble import VotingClassifier\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline","4335c826":"telcom=pd.read_csv(\"..\/input\/WA_Fn-UseC_-Telco-Customer-Churn.csv\")","9945570f":"telcom.head()","13f053ac":"#A brief overview of our data value\ntelcom.shape","35a01c9b":"#Search missing value\npd.isnull(telcom).sum()","c0d3d9ae":"telcom[\"Churn\"].value_counts()","39c8e4d7":"telcom.dtypes","1dce0b64":"telcom['TotalCharges']=telcom['TotalCharges'].convert_objects(convert_numeric=True)\ntelcom[\"TotalCharges\"].dtypes","5f03a3f4":"pd.isnull(telcom[\"TotalCharges\"]).sum()","28864913":"telcom.dropna(inplace=True)\ntelcom.shape","70c29836":"telcomdata=telcom","b3ab5da0":"telcom['Churn'].replace(to_replace='Yes', value=1, inplace=True)\ntelcom['Churn'].replace(to_replace='No',  value=0, inplace=True)\ntelcom['Churn'].head()","c325361c":"churnvalue=telcom[\"Churn\"].value_counts()\nlabels=telcom[\"Churn\"].value_counts().index\n\nrcParams[\"figure.figsize\"]=6,6\nplt.pie(churnvalue,labels=labels,colors=[\"whitesmoke\",\"yellow\"], explode=(0.1,0),autopct='%1.1f%%', shadow=True)\nplt.title(\"Proportions of Customer Churn\")\nplt.show()","17900f8f":"tel_dummies = pd.get_dummies(telcom.iloc[:,1:21])\ntel_dummies.head()","99f2ce83":"plt.figure(figsize=(15,8))\ntel_dummies.corr()['Churn'].sort_values(ascending = False).plot(kind='bar')\nplt.title(\"Correlations between Churn and variables\")","e8d39570":"f, axes = plt.subplots(nrows=1, ncols=3, figsize=(15,5))\n\nplt.subplot(1,3,1)\ngender=sns.countplot(x=\"gender\",hue=\"Churn\",data=telcom,palette=\"Pastel1\")\nplt.xlabel(\"gender\")\nplt.title(\"Churn by Gender\")\n\nplt.subplot(1,3,2)\ngender=sns.countplot(x=\"PhoneService\",hue=\"Churn\",data=telcom,palette=\"Pastel1\")\nplt.xlabel(\"PhoneService\")\nplt.title(\"Churn by PhoneService\")\n\nplt.subplot(1,3,3)\ngender=sns.countplot(x=\"MultipleLines\",hue=\"Churn\",data=telcom,palette=\"Pastel1\")\nplt.xlabel(\"MultipleLines\")\nplt.title(\"Churn by MultipleLines\")","eba1c7a4":"plt.figure(figsize=(20,16))\ncharges=telcom.iloc[:,1:20]\ncorr = charges.apply(lambda x: pd.factorize(x)[0]).corr()\nax = sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, \n                 linewidths=.2, cmap=\"YlGnBu\",annot=True)\nplt.title(\"Correlation between variables\")","7c0c5d6d":"covariables=[\"OnlineSecurity\", \"OnlineBackup\", \"DeviceProtection\", \"TechSupport\", \"StreamingTV\", \"StreamingMovies\"]\nfig,axes=plt.subplots(nrows=2,ncols=3,figsize=(16,10),sharex=False,sharey=True)\nfor i, item in enumerate(covariables):\n    plt.subplot(2,3,(i+1),sharey=ax)\n    ax=sns.countplot(x=item,hue=\"Churn\",data=telcom,palette=\"Pastel2\",order=[\"Yes\",\"No\",\"No internet service\"])\n    plt.xlabel(str(item))\n    plt.title(\"Churn by \"+str(item))\n    i=i+1\nplt.show()","b95f44cf":"telcomvar=telcom.iloc[:,1:20]","cb2b3b3b":"def uni(columnlabel):\n    print(columnlabel,\"--\" ,telcomvar[columnlabel].unique())\n    \ntelcomobject=telcomvar.select_dtypes(['object'])\nfor i in range(0,len(telcomobject.columns)):\n    uni(telcomobject.columns[i])","5aab11da":"telcomvar.replace(to_replace='No internet service', value='No', inplace=True)\ntelcomvar.replace(to_replace='No phone service', value='No', inplace=True)\nfor i in range(0,len(telcomobject.columns)):\n    uni(telcomobject.columns[i])","fdd2b27b":"copy=telcomvar.copy()\ncopyobj=copy.select_dtypes(['object'])\ndef labelencode(columnlabel):\n    copy[columnlabel] = LabelEncoder().fit_transform(copy[columnlabel])\n\nfor i in range(0,len(copyobj.columns)):\n    labelencode(copyobj.columns[i])","d6fa4240":"#Feature importance\nx=copy\ny=telcom['Churn']\n\nfrom sklearn.ensemble import ExtraTreesClassifier\nmodel=ExtraTreesClassifier()\nmodel.fit(x,y)\nprint(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=x.columns)\nfeat_importances.nlargest(20).plot(kind='barh')\nplt.show()","c0a446e4":"dropvar=[\"PhoneService\"]\ntelcomvar.drop(columns=dropvar,axis=1, inplace=True)\ntelcomvar.head()","d07a537e":"telcomvar = pd.get_dummies(telcomvar)\ntelcomvar.head()","94d12bca":"def kdeplot(feature):\n    plt.figure(figsize=(9, 4))\n    plt.title(\"KDE for {}\".format(feature))\n    ax0 = sns.kdeplot(telcom[telcom['Churn'] == 0 ][feature].dropna(), color= 'navy', label= 'Churn: No')\n    ax1 = sns.kdeplot(telcom[telcom['Churn'] == 1 ][feature].dropna(), color= 'orange', label= 'Churn: Yes')\nkdeplot('tenure')\nkdeplot('MonthlyCharges')\nkdeplot('TotalCharges')","710305c4":"fig,ax=plt.subplots(nrows=2,ncols=1,figsize=(10,10))\n\nplt.subplot(2,1,1)\nsns.barplot(x=\"Contract\",y=\"Churn\", data=telcom, palette=\"Pastel1\", order= ['Month-to-month', 'One year', 'Two year'])\nplt.title(\"Churn by Contract type\")\n\nplt.subplot(2,1,2)\nsns.barplot(x=\"PaymentMethod\",y=\"Churn\",data=telcom,palette=\"Pastel1\")\nplt.title(\"Churn by PaymentMethod\")","3921ca28":"scaler = StandardScaler(copy=False)\nscaler.fit_transform(telcomvar[['tenure','MonthlyCharges','TotalCharges']])","139431c7":"telcomvar[['tenure','MonthlyCharges','TotalCharges']]=scaler.transform(telcomvar[['tenure','MonthlyCharges','TotalCharges']])","455e9bc5":"# check outliers\nplt.figure(figsize = (8,4))\nnumbox = sns.boxplot(data=telcomvar[['tenure','MonthlyCharges','TotalCharges']], palette=\"Set2\")\nplt.title(\"Check outliers of standardized tenure, MonthlyCharges and TotalCharges\")","fa0bde97":"X=telcomvar\ny=telcom[\"Churn\"].values\n\nsss=StratifiedShuffleSplit(n_splits=5, test_size=0.2,random_state=0)\nsss.get_n_splits(X,y)","0ae9f937":"#So this is the cross-validator that we are using\nprint(sss)","183ef73e":"#Split train\/test sets of X and y\nfor train_index, test_index in sss.split(X, y):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train,X_test=X.iloc[train_index], X.iloc[test_index]\n    y_train,y_test=y[train_index], y[test_index]","58fec138":"#Let's see the number of sets in each class of training and testing datasets\nprint(pd.Series(y_train).value_counts())\nprint(pd.Series(y_test).value_counts())","6f8e8684":"from imblearn.over_sampling import ADASYN\nada= ADASYN()\nX_resample,y_resample=ada.fit_sample(X_train,y_train)","98c155c5":"#concat oversampled \"x\" and \"y\" into one DataFrame\nX_resample=pd.DataFrame(X_resample)\ny_resample=pd.DataFrame(y_resample)\n#replace column labels using the labels of original datasets\nX_resample.columns=telcomvar.columns\ny_resample.columns=[\"Churn\"]","6fe2481a":"y_resample[\"Churn\"].value_counts()","b8cda34d":"Classifiers=[[\"Support Vector Machine\",SVC()],\n             [\"LogisticRegression\",LogisticRegression()],\n             [\"Naive Bayes\",GaussianNB()],\n             [\"Decision Tree\",DecisionTreeClassifier()],\n             [\"Random Forest\",RandomForestClassifier()],\n             [\"AdaBoostClassifier\", AdaBoostClassifier()],\n]\n\nnames=[]\nprediction=[]\nClassify_result=[]\nfor name,classifier in Classifiers:\n    classifier=classifier\n    classifier.fit(X_resample,y_resample)\n    y_pred=classifier.predict(X_test)\n    recall=recall_score(y_test,y_pred)\n    precision=precision_score(y_test,y_pred)\n    f_score=f1_score(y_test,y_pred)\n    class_eva=pd.DataFrame([recall,precision,f_score])\n    Classify_result.append(class_eva)\n    name=pd.Series(name)\n    names.append(name)\n    y_pred=pd.Series(y_pred)\n    prediction.append(y_pred)\n","89a9cad8":"names=pd.DataFrame(names)\nnames=names[0].tolist()\nresult=pd.concat(Classify_result,axis=1)\nresult.columns=names\nresult.index=[\"recall\",\"precision\",\"f-socre\"]\nresult","6eedb3c9":"prediction=pd.DataFrame(prediction)\ny_pred_svc=np.array(prediction.iloc[0,:])\ny_pred_lr=np.array(prediction.iloc[1,:])\ny_pred_NB=np.array(prediction.iloc[2,:])\ny_pred_dt=np.array(prediction.iloc[3,:])\ny_pred_rf=np.array(prediction.iloc[4,:])\ny_pred_AdaB=np.array(prediction.iloc[5,:])","3ee462ef":"predictions=[y_pred_svc,y_pred_lr, y_pred_NB, y_pred_dt, y_pred_rf, y_pred_AdaB]\n\nimport itertools\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n        \n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\nfig,axes=plt.subplots(nrows=2,ncols=3,figsize=(25,10))\nfor i, item in enumerate(predictions):\n    plt.subplot(2,3,(i+1))\n    cnf_matrix = confusion_matrix(y_test,item)\n    class_names = [\"Remain\",\"Churn\"]\n    title_label=[\"SVC\",\"LogisticRegression\",\"NaiveBayes\",\"DecisionTree\", \"RandomForest\",\"AdaBoostClassifier\"]\n    plot_confusion_matrix(cnf_matrix\n                      , classes=class_names\n                      , title='CM of '+str(title_label[i]))\n    i=i+1\nplt.show()","cbafaee9":"from sklearn.ensemble import VotingClassifier\nclf1 = GaussianNB()\nclf2 = SVC(probability=True)\nclf3 = LogisticRegression()\neclf1 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft')\neclf1.fit(X_resample, y_resample)\ny_pred1= eclf1.predict(X_test)\nlabels=[\"Remain\",\"Churn\"]\n","e7a40599":"print(classification_report(y_test, y_pred1 ,target_names=labels,digits=5))","7a3839a7":"cnf_matrix = confusion_matrix(y_test,y_pred1)\nclass_names = [\"Remain\",\"Churn\"]\nplot_confusion_matrix(cnf_matrix\n                      , classes=class_names\n                      , title='CM of Voting Classifier')\n\nplt.show()","af6202aa":"# compute AUC and plot RPC curve\nplt.figure(figsize=(8,6))\ny_pred_proba1 = eclf1.predict_proba(X_test)[::,1]\nfpr1, tpr1, _1 = roc_curve(y_test,  y_pred_proba1)\nauc1 = roc_auc_score(y_test, y_pred_proba1)\n\nsvc = SVC(probability=True).fit(X_resample,y_resample)\ny_pred_probasvc= svc.predict_proba(X_test)[::,1]\nfpr, tpr, _ = roc_curve(y_test,  y_pred_probasvc)\naucsvc = roc_auc_score(y_test, y_pred_probasvc)\n\nNB = GaussianNB().fit(X_resample,y_resample)\ny_pred_probaNB= NB.predict_proba(X_test)[::,1]\nfpr2, tpr2, _2 = roc_curve(y_test,  y_pred_probaNB)\naucNB = roc_auc_score(y_test, y_pred_probaNB)\n\nlrg = LogisticRegression().fit(X_resample,y_resample)\ny_pred_probalrg= lrg.predict_proba(X_test)[::,1]\nfpr3, tpr3, _3 = roc_curve(y_test,  y_pred_probalrg)\nauclrg = roc_auc_score(y_test, y_pred_probalrg)\n\n\n\nlw=2\nplt.plot([0, 1], [0, 1], color='lightgray', lw=lw, linestyle='--')\n\nplt.plot(fpr1,tpr1,\n         label=\"Voting ROC, auc=\"+str(auc1), \n         color='navy', linewidth=4)\n\n\nplt.plot(fpr, tpr,\n         label='SVM ROC, auc='+str(aucsvc),\n         color='yellow', linestyle='-',  linewidth=2)\n\nplt.plot(fpr2, tpr2,\n         label='NB ROC, auc='+str(aucNB),\n         color='red', linestyle=':',  linewidth=2)\n\nplt.plot(fpr3, tpr3,\n         label='LogReg ROC, auc='+str(auclrg),\n         color='darkorange', linestyle='-.',  linewidth=2)\n\nplt.legend(loc=4)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title(\"ROC curve of classifiers\")\nplt.legend(loc=\"lower right\")\n\nplt.show()","9a8333c7":"y_pred= pd.DataFrame(np.array(prediction.iloc[1,:]))\nchurn_test=y_pred[y_pred[0]==1].index\nchurner_index= X_test.iloc[churn_test,:].index\nchurner=telcom.iloc[churner_index,1:20]\nchurner_var = pd.get_dummies(churner)\nn=len(churner_var.columns)","b0611973":"#Firstly, we need to check cumulative explained variance ratio to find out how many dimensions are necessary.\npca=PCA(n_components=n,random_state=0)\ntranspca=pca.fit_transform(churner_var)","b3290c7b":"print(\"Explained Variance Ratio => {}\\n\".format(pca.explained_variance_ratio_))\nprint(\"Explained Variance Ratio(csum) => {}\\n\".format(pca.explained_variance_ratio_.cumsum()))","ec4c6502":"pca=PCA(n_components=2,random_state=0)\ntranspca=pca.fit_transform(churner_var)\nreduced_data=pd.DataFrame(transpca,columns=['D_1','D_2'])\nreduced_data.head()","95d0bea2":"def biplot(churner_var, reduced_data, pca):\n    '''\n    Produce a biplot that shows a scatterplot of the reduced\n    data and the projections of the original features.\n    \n    good_data: original data, before transformation.\n               Needs to be a pandas dataframe with valid column names\n    reduced_data: the reduced data (the first two dimensions are plotted)\n    pca: pca object that contains the components_ attribute\n\n    return: a matplotlib AxesSubplot object (for any additional customization)\n    \n    This procedure is inspired by the script:\n    https:\/\/github.com\/teddyroland\/python-biplot\n    '''\n\n    fig, ax = plt.subplots(figsize = (14,8))\n    # scatterplot of the reduced data    \n    ax.scatter(x=reduced_data.loc[:, 'D_1'], y=reduced_data.loc[:, 'D_2'], \n        facecolors='b', edgecolors='b', s=70, alpha=0.5)\n    \n    feature_vectors = pca.components_.T\n\n    ax.set_xlabel(\"Dimension 1\", fontsize=14)\n    ax.set_ylabel(\"Dimension 2\", fontsize=14)\n    ax.set_title(\"2-Dimension Visualization of Predicted Churn Customer Data\", fontsize=16);\n    return ax","3087b1d5":"biplot(churner_var, reduced_data, pca)","d75e6194":"def sil_coeff(no_clusters):\n    # Apply your clustering algorithm of choice to the reduced data \n    clusterer_1 = KMeans(n_clusters=no_clusters, random_state=0 )\n    clusterer_1.fit(reduced_data)\n    \n    # Predict the cluster for each data point\n    preds_1 = clusterer_1.predict(reduced_data)\n    \n    # Find the cluster centers\n    centers_1 = clusterer_1.cluster_centers_\n\n    # Calculate the mean silhouette coefficient for the number of clusters chosen\n    score = silhouette_score(reduced_data, preds_1)\n    \n    print(\"silhouette coefficient for `{}` clusters => {:.4f}\".format(no_clusters, score))\n    \nclusters_range = range(2,16)\nfor i in clusters_range:\n    sil_coeff(i)","03c0a7c3":"samples = churner.sample(n=6)\nindices= samples.index\nprint(\"Indices of Samples => {}\".format(indices))\n\n# Create a DataFrame of the chosen samples\nsamples_info = pd.DataFrame(telcom.loc[indices],columns = telcom.keys()).reset_index(drop = True).iloc[:,:20]\nprint(\"\\nChosen samples of telecom customers dataset:\")\ndisplay(samples_info)\n\n# Apply PCA on samples\nsamples_var= pd.DataFrame(churner_var.loc[indices], columns = churner_var.keys()).reset_index(drop = True)\npca=PCA(n_components=2,random_state=0, svd_solver='full')\ntransam=pca.fit_transform(samples_var)\npca_samples=pd.DataFrame(transam,columns=['D_1','D_2'])","2e13c391":"def cluster_results(reduced_data, preds, centers, pca_samples):\n    \n#Visualizes the PCA-reduced cluster data in two dimensions\n#Adds cues for cluster centers and selected sample data\n    \n    predictions = pd.DataFrame(preds, columns = ['Cluster']) \n    plot_data = pd.concat([predictions, reduced_data], axis = 1)\n    \n    # Generate the cluster plot\n    fig, ax = plt.subplots(figsize = (14,8))\n\n    # Color map\n    cmap = cm.get_cmap('gist_rainbow')\n\n    # Color the points based on assigned cluster\n    for i, cluster in plot_data.groupby('Cluster'):   \n        cluster.plot(ax = ax, kind = 'scatter', x = 'D_1', y = 'D_2', \\\n                     color = cmap((i)*1.0\/(len(centers)-1)), label = 'Cluster %i'%(i), s=30);\n\n    # Plot centers with indicators\n    for i, c in enumerate(centers):\n        ax.scatter(x = c[0], y = c[1], color = 'white', edgecolors = 'black', \\\n                   alpha = 1, linewidth = 2, marker = 'o', s=200);\n        ax.scatter(x = c[0], y = c[1], marker='$%d$'%(i), alpha = 1, s=100);\n\n    # Plot transformed sample points \n    ax.scatter(x = pca_samples.iloc[:,0], y = pca_samples.iloc[:,1], \\\n               s = 150, linewidth = 4, color = 'black', marker = 'x');\n\n# Set plot title\n    ax.set_title(\"2 Segments of Predicted Churn Customers - Samples Marked by Black Cross\");\n","dda32cc5":"# Display the results of the clustering from implementation for 3 clusters\nclusterer = KMeans(n_clusters = 2)\nclusterer.fit(reduced_data)\npreds = clusterer.predict(reduced_data)\ncenters = clusterer.cluster_centers_\nsample_preds = clusterer.predict(pca_samples)\n\ncluster_results(reduced_data, preds, centers, pca_samples)","0cdd85b0":"#Data Recovery\nst_centers = pca.inverse_transform(centers)\n\n# Display the centers\nsegments = ['Segment {}'.format(i) for i in range(0,len(centers))]\ntrue_centers = pd.DataFrame(np.round(st_centers), columns = churner_var.keys())\ntrue_centers.index = segments\ndisplay(true_centers)","cf11a712":"# Display the predictions\nfor i, pred in enumerate(sample_preds):\n    print(\"Sample point\", i, \"predicted to be in Cluster\", pred)","71f7c3ab":"> Stratified ShuffleSplit cross-validator Provides train\/test indices to split data in train\/test sets. This cross-validation object is a merge of StratifiedKFold and ShuffleSplit, which returns stratified randomized folds. The folds are made by preserving the percentage of samples for each class. Note: like the ShuffleSplit strategy, stratified random splits do not guarantee that all folds will be different, although this is still very likely for sizeable datasets.\n\n-- Scikit-learn organization (https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.StratifiedShuffleSplit.html#sklearn.model_selection.StratifiedShuffleSplit)","10cd2a4f":"2 clusters got highest silhouette score at 0.7031 which is the highest score. Therefore we can assign churn customers into 2 segments.\n\nBefore visualizing the clusters, let's select six samples as examples.","42ba738e":"The proportion of Churn customers with Month-to-month contract is larger than One-year-contract and Two-year contract. It may represent that set longe-term contract could be a effective approach to retain existing customers.","a1846337":"OnlineSecurity, OnlineBackup, DeviceProtection, TechSupport, StreamingTV, StreamingMovies have strong correlations between each other, which means their data point are moving together towards the same direction.\nMultipleLines and PhoneService also have strong correlation.","816572be":"Since TotalCharges supposed to be numerical data rather than object, we need to convert it to float.","507d77c2":"Firstly, we can check our the proportions of \"Churn\".","96948add":"# Segments that samples belonging to ","b996b3f4":"Look at these six barcharts, the values are similar, but not exactly the same. That means although they have strong correlation between each other, they can not replace each other. So we will keep all of them for better model training.\n\nHowever, the churn values of no internet service are all the same within six variables above. It is probably because the six factors above could influence customers' decision making only if the customers are using internet service. \n\nTherefore we can simply merge \"No internet service\" value to \"No\" value.","ac1763b8":"There are 11 missing values in TotalCharges variable, which is not a big amount. We can drop these rows directly.","8cbc53b8":"Firstly, we need to drop CustomerID since it is random characters for each customer. Also we will split variables and target Churn.\n\nAs we mentioned previously, \"gender\" and \"PhoneService\" have extremely low correlation with \"Churn\". We can ignore them since they have little influence on customer churn.","2ffbf3ce":"As we mentioned previously, the \"No internet service\" value of six variables is not a determinant for customers who are not using internet service to make churn decision. These customers are not using any internet product. \n\nTherefore \"No internet service\" is the same as \"No\", so we can replace it by using \"No\".","4bb1309b":"# Clustering with K-means","d033b118":"# Import packages","acc6440d":"# Models training and comparison","2e03114d":"# Exploratory Data Analysis","de19d462":"# Data Recovery","cbddfe0c":"No obvious outliers existing within three variables.","ab5f2d4c":"Now we are steping in the second part, Clustering for customer segmentation. In this part, we are not going to split training or testing data set. So we need to apply PCA to data in \"telcomvar\". \n\nSince using PCA on a dataset calculates the dimensions which best maximize variance, we will find which compound combinations of features best describe customers.","659d6ce4":"# Oversampling training set","1be171ca":"# Stratified Cross-Validation - Split training\/testing datasets","675de3df":"There are 7043 rows of telecom customer dataset, 21 columns, including 20 variables and 1 target column \"Churn\"","49a823c1":"We also need to replace the value of Churn column using \"1\" for \"Yes\" and \"0\" for \"No\"","adf0f1b0":"There are 5174 \"No\" and 1869 \"Yes\" in Churn, the dataset is imbalanced.","26dbebf4":"### **Feature Revelance**","577c38b3":"The unique values of the variables above are all categorical. Some of them are binary, and all of them have four or less unique values. Therefore we can convert them into numerical values using Scikit-learn OneHotEncoder","504e8d39":"### **Feature Importance**","673d8d1b":"### **Data Visualization**","2dd4d9d1":"From the *cumulative explained variance ratio* we can see that 99.998% variance is covered by first two dimension. Therefore we can reduce dimentionality to **two dimensions** to reduce the complexity of the problem.\n\nDimensionality reduction comes at a cost: Fewer dimensions used implies less of the total variance in the data is being explained.","512c2654":"### **A Brief Overview of The Data**","31b74140":"Check if TotalCharges has missing value.","64693d0c":"# PCA implementation","d0e32a54":"# Data Preprocess","ab637424":"Here is no null value data. ","5d908870":"### **ONE HOT ENCODING**","cd2cfd22":"In the middle of the crrelation diagram, the dummie values of \"gender\" and \"PhoneService\" are approaching 0, which means these two variables have very low correlation that influencing \"Churn\" prediction.\n\nWe can drop them as they are not influential factors on determining churn or not churn."}}