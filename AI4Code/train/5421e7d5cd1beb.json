{"cell_type":{"23093318":"code","73402541":"code","5ffa73c2":"code","b4b8fde7":"code","b5a92a71":"code","5634e76d":"code","e0864096":"code","8532adc4":"code","2b0aabaf":"code","28c293dc":"code","7d6ad697":"code","1132c4d7":"code","93cc3d29":"code","dc374ae2":"markdown","f3948786":"markdown","50908b7b":"markdown","08c9f0b4":"markdown","6271fd1a":"markdown","8b252f9e":"markdown","ed18138d":"markdown","0fea0651":"markdown"},"source":{"23093318":"import sys\nsys.path.append(\"..\/input\/moa-scripts\")\nfrom moa import load_datasets, preprocess, split\nfrom metrics import logloss\nimport lgbm as lgb_tools\nfrom lightgbm import LGBMClassifier\n\nimport numpy as np\nimport pandas as pd\nimport warnings\nimport joblib\nimport gc\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nfrom tqdm.auto import tqdm\n\nsns.set_style(\"dark\", {\"axes.facecolor\": \".92\"})\n%matplotlib inline\nwarnings.filterwarnings(\"ignore\")","73402541":"X, y, _, test, _ = load_datasets(\"..\/input\/lish-moa\")\nX, _, test, test_control = preprocess(X, y, test, standard=False, onehot=True)\ntest = test[~test_control]\ntest = pd.DataFrame(test, columns=X.columns)\nn_train = len(X)\nX = pd.concat([X, test])\nX['target'] = 0\nX['target'].iloc[n_train:] = 1\nX.reset_index(drop=True, inplace=True)\ndel test; gc.collect()\n\nX.shape, X.target.sum(), X.shape[0] - X.target.sum()","5ffa73c2":"class AdversarialValidation:\n    \"\"\"\n    Simple class for adversarial analysis\n    Bucket and compare each feature from train and test samples using PSI or KL\n    PSI: https:\/\/www.lexjansen.com\/wuss\/2017\/47_Final_Paper_PDF.pdf\n    KL : https:\/\/en.wikipedia.org\/wiki\/Kullback%E2%80%93Leibler_divergence\n    \"\"\"\n    def __init__(self, eps=1-3, target='target'):\n        self.target = target\n        self.eps = eps\n    \n    def breakpoints(self, x, n_bins, btype):\n        if btype == 'bins':\n            min, max = x.min(), x.max()\n            return np.linspace(min, max, n_bins)\n        elif btype == 'quantiles':\n            qnt = np.linspace(0, 100, n_bins)\n            return np.stack([np.percentile(x, q) for q in qnt])\n        return btype\n\n    def kullback_leibler(self, p, q) -> float:\n        lg = np.log(p \/ q)\n        lg = np.where(np.isinf(lg), 0, lg)\n        lg = np.where(np.isnan(lg), 0, lg)\n        return np.sum(p * lg)\n\n    def psi(self, p, q) -> float:\n        lg = np.log(p \/ q)\n        lg = np.where(np.isinf(lg), 0, lg)\n        lg = np.where(np.isnan(lg), 0, lg)\n        return np.sum((p - q) * lg)\n    \n    def difference(self, X, method='kl', buckets=10, buckettype='bins'):\n        result = {}\n        target_mask = X[self.target] == 1\n        Na = target_mask.sum() #  `actual`: take test sample as real data\n        Ne = len(X) - Na       # `expected`: take train sample as standard\n        for c in tqdm(X.columns, desc=method):\n            if c == self.target: \n                continue\n            a, e = X.loc[target_mask, c].values, X.loc[~target_mask, c].values\n            breakpoints = self.breakpoints(X[c].copy(), buckets, buckettype)\n            e = np.histogram(e, breakpoints)[0] \/ Ne\n            a = np.histogram(a, breakpoints)[0] \/ Na\n            r = self.kullback_leibler(a, e) if method=='kl' else self.psi(a, e)\n            result[c] = r\n        return result\n","b4b8fde7":"av = AdversarialValidation()\ndiff = av.difference(X, 'psi', 10, 'quantiles')","b5a92a71":"features = list(diff)\nsusp = list(zip(*sorted(diff.items(), key=lambda x: x[1])[-16:][::-1]))[0]\n\nfrom pprint import pprint\npprint(sorted(diff.items(), key=lambda x: x[1])[-16:][::-1])","5634e76d":"test_mask = X.target == 1\n\nfig, axes = plt.subplots(4, 4, figsize=(20, 20))\naxes=axes.flatten()\nfor i in range(4*4):\n    sns.distplot(X.loc[~test_mask, susp[i]], label='train', hist=0, ax=axes[i])\n    sns.distplot(X.loc[test_mask, susp[i]], label='test', hist=0, ax=axes[i])\n    axes[i].legend()\nsns.despine()","e0864096":"np.random.seed(1)\n\nX = X.sample(frac=1.0, random_state=1)\ny = X['target'].values\nX.drop('target', axis=1, inplace=True)\nX_train, y_train = X.iloc[:15000], y[:15000]\nX_valid, y_valid = X.iloc[15000:], y[15000:]","8532adc4":"from sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(C=0.01, random_state=1)\nlr.fit(X_train, y_train)\nprint(f\"Adversarial AUC [LR]:{roc_auc_score(y_valid, lr.predict_proba(X_valid)[:, 1])}\")\n\ngbm = LGBMClassifier(num_leaves=7, seed=1)\ngbm.fit(X_train, y_train, eval_set=(X_valid, y_valid), early_stopping_rounds=5, verbose=0);\nfrom sklearn.metrics import roc_auc_score\nprint(f\"Adversarial AUC[GBM]:{roc_auc_score(y_valid, gbm.predict_proba(X_valid)[:, 1])}\")","2b0aabaf":"gbm_susp = [f for i, f in zip(gbm.feature_importances_, features) if i]\nset(susp) & set(gbm_susp) # psi and gbm feature sets not intersect (almost)","28c293dc":"def eval_selection(selection, label):\n    xt, xv = X_train.copy().drop(selection, axis=1), X_valid.copy().drop(selection, axis=1)\n    \n    lr = LogisticRegression(C=0.01, random_state=1)\n    lr.fit(xt, y_train)\n    print(f\"Adversarial AUC [LR]-[{label}]:{roc_auc_score(y_valid, lr.predict_proba(xv)[:, 1])}\")    \n    \n    gbm = LGBMClassifier(num_leaves=7, seed=1)\n    gbm.fit(xt, y_train, eval_set=(xv, y_valid), early_stopping_rounds=5, verbose=0);\n    print(f\"Adversarial AUC[GBM]-[{label}]:{roc_auc_score(y_valid, gbm.predict_proba(xv)[:, 1])}\")","7d6ad697":"eval_selection(list(susp), 'PSI')\neval_selection(gbm_susp, 'GBM')\neval_selection(list(set(susp)|set(gbm_susp)), 'PSI+GBM')","1132c4d7":"# PSI features\nsusp","93cc3d29":"# GBM features\ngbm_susp","dc374ae2":"\n## Final conclusions: \n* train and test samples are almost homogenous\n* it's possible to make them absolutely homogenous after removing (or fixing somehow) small amount of features\n* however, it requires building a large number of different models and time consuming","f3948786":"## Adversarial validation with model","50908b7b":"* we did't use logreg's most valuable features and feature selection didn't affect its AUC\n* but GBM AUC become much better","08c9f0b4":"## PSI","6271fd1a":"### Build train\/test dataset, test sample is target","8b252f9e":"### We can consider all features as homogeneous, since the worst index is << 0.1 (~0.01) ","ed18138d":"### *Train-test distributions looks almost the same","0fea0651":"This kernel is dedicated to a comparison of the test and training samples.\nSometimes it is critical for proper validation to ensure that the train and test samples are homogeneous.\n\nKernel plan:\n1. Compare features by bucketing method: Population Stability Index\n2. Extract most important features from GBM adversarial validation\n3. Explore results"}}