{"cell_type":{"4ff691b0":"code","67f09d39":"code","0f2b7ff3":"code","dfb5d44d":"code","e9703f49":"code","efe77f13":"code","367ce6cc":"code","7274da0b":"code","7e76218e":"code","4291d929":"code","15d13909":"code","bf4dd551":"code","de3b4861":"code","914a7cd1":"code","f06c7fea":"code","04f4213f":"code","5ca4a42c":"code","7f939131":"code","1fe74544":"code","e35b5f21":"code","eac5c06c":"code","e8e37c5c":"code","a3ac03a2":"code","09000d2a":"code","a4d2f237":"code","ddc5df2d":"code","3ef4c231":"code","7b182ff5":"code","7017d6da":"code","f43e0a75":"code","35e0bf6e":"code","05964d45":"code","fde5f5f2":"code","962724b1":"code","b02ff5d5":"code","e62c0928":"code","de545e53":"code","7855cb92":"code","cec76418":"markdown","976a42e1":"markdown","56d20a8a":"markdown","c79ebcb2":"markdown","ad483721":"markdown","d30abd57":"markdown","93aab2c2":"markdown","05f3c3ca":"markdown","ada44ade":"markdown","f5e3d6b1":"markdown","1f47c22e":"markdown","1d0302d5":"markdown","dd00f734":"markdown","f3a20da1":"markdown","e88d453b":"markdown","07ead074":"markdown","892b1be4":"markdown","050564e3":"markdown","b8a33886":"markdown","d8c6f99a":"markdown","bef31cfc":"markdown","ca010e29":"markdown","bd31c8ab":"markdown","2d04f99b":"markdown","f80b40bb":"markdown","a3e91e40":"markdown","7c854ded":"markdown","d5fb7bb1":"markdown","39c55a3f":"markdown","7394caf7":"markdown","ec7c7f56":"markdown","9bca70b6":"markdown","d40eeb18":"markdown","d44d7f67":"markdown","55ec4560":"markdown","8c45c1c2":"markdown","0478f834":"markdown"},"source":{"4ff691b0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom mpl_toolkits.basemap import Basemap #Plot onto map\nimport matplotlib.pyplot as plt #Plotting\n%matplotlib inline\n\nplt.rcParams['figure.figsize'] = (15,10) #Set the default figure size\nplt.style.use('ggplot') #Set the plotting method\n\nfrom sklearn.model_selection import train_test_split #Split the data into train and test\nfrom sklearn.ensemble import RandomForestRegressor #Forest for prediction and regression\nfrom sklearn.linear_model import LinearRegression #Regression for prediction\nfrom sklearn.preprocessing import StandardScaler #Scale the data\nfrom sklearn.metrics import mean_squared_error #Error testing\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","67f09d39":"bnb = pd.read_csv(\"..\/input\/us-airbnb-open-data\/AB_US_2020.csv\") #Read the airbnb csv\nbnb.head() #Take a peek at the dataset","0f2b7ff3":"bnb[\"price\"] = bnb[\"price\"].apply(lambda x: 1 if x < 1 else x) #Make 0's 1 so the log function works","dfb5d44d":"#Print some attributes about the prices\nprint(\"Max Price: \", np.max(bnb[\"price\"]))\nprint(\"Min Price: \", np.min(bnb[\"price\"]))\nprint(\"Num Prices Below 20: \", len(bnb.loc[bnb[\"price\"] < 20]))\nprint(\"Num Prices Above 1000: \", len(bnb.loc[bnb[\"price\"] > 1000]))\nprint(\"Num Locations\", len(bnb))","e9703f49":"print(bnb.isnull().any()) #Check for null values","efe77f13":"print(bnb.loc[bnb[\"reviews_per_month\"].isnull()]) #See where reviews_per_month is null","367ce6cc":"print(bnb.loc[bnb[\"host_name\"].isnull()]) #See where host_name is null","7274da0b":"print(bnb.loc[bnb[\"name\"].isnull()]) #See where name is null","7e76218e":"bnb[\"name\"] = bnb[\"name\"].fillna(\"AIRBNB HOUSING\") #Fill the null name values with \"AIRBNB HOUSING\"\nprint(bnb.loc[bnb[\"name\"] == \"AIRBNB HOUSING\"]) #See where name is fixed to make sure this works","4291d929":"bnb[\"host_name\"] = bnb[\"host_name\"].fillna(\"AIRBNB HOST\") #Fill the null host name values with \"AIRBNB HOST\"\nprint(bnb.loc[bnb[\"host_name\"] == \"AIRBNB HOST\"]) #See where host_name is fixed to make sure this works","15d13909":"bnb[\"neighbourhood_group\"] = bnb[\"neighbourhood_group\"].fillna(\"Other\") #Fill the null neighbourhood group values with \"Other\"","bf4dd551":"bnb[\"reviews_per_month\"] = bnb[\"reviews_per_month\"].fillna(0) #Fill the null reviews_per_month values with 0","de3b4861":"bnb[\"last_review\"] = bnb[\"last_review\"].fillna(\"01\/01\/01\") #Fill the null last_review values with 01\/01\/01\nbnb[\"last_review\"] = pd.to_datetime(bnb[\"last_review\"]) #Convert the last review to datetime\nprint(bnb[\"last_review\"]) #Print the last review","914a7cd1":"print(bnb.isnull().any()) #Check for null values","f06c7fea":"#Change the column names to ones I prefer\nbnb = bnb.rename(columns = {\"host_id\" : \"hostId\", \"host_name\" : \"hostName\", \"neighbourhood_group\" : \"neighGroup\",\n                            \"neighbourhood\" : \"neigh\", \"room_type\" : \"roomType\", \"minimum_nights\" : \"minNights\",\n                            \"number_of_reviews\" : \"numReviews\", \"last_review\" : \"lastReview\", \"reviews_per_month\" : \"monthlyReviews\",\n                            \"calculated_host_listings_count\" : \"numListings\", \"availability_365\" : \"available\"})\nbnb.head() #Take a peek at the dataset","04f4213f":"fig = plt.figure() #Plot figure to add subplots\nlat = list(bnb[\"latitude\"]) #Get the list of latitudes\nlon = list(bnb[\"longitude\"]) #Get the list of longitudes\n\nax = fig.add_subplot(211) #Add a subplot for the contiguous US\nax.set_title(\"Main US\") #Add a title for the main US\n\n#Set a basemap for the contiguous United States\nmUS = Basemap(width = 5000000, height = 4000000, projection = \"lcc\",\n            resolution = \"c\", lat_1 = 35.0, lat_2 = 45.0, lat_0 = 40.0, lon_0 = -97.0)\n\nmUS.drawcoastlines() #Draw the coastlines\nmUS.fillcontinents(color = \"lightgreen\") #Make the land light green\nxlon, ylat = mUS(lon, lat) #Fit the coordinates to fit with the map\nmUS.plot(xlon, ylat, \"b.\") #Plot the US with our AirBNB data\n\nax = fig.add_subplot(212) #Add a subplot for Hawaii\nax.set_title(\"Hawaii\") #Set title to hawaii\n\n#Set a basemap for Hawaii\nmH = Basemap(width=625000,height=500000,projection='lcc',\n            resolution='l',lat_1=16.,lat_2=26,lat_0=21,lon_0=-156.)\n\nmH.drawcoastlines() #Draw the coastline\nmH.fillcontinents(color = \"lightgreen\") #Make the land light green\nxlon, ylat = mH(lon, lat) #Fit the coordinates into the map\nmH.plot(xlon, ylat, \"b.\") #Plot the AirBNB data\n\nplt.show() #Show the maps","5ca4a42c":"print(bnb[\"city\"].unique()) #See all the unique \"cities\" in the data","7f939131":"hawaii = len(bnb.loc[bnb[\"city\"] == \"Hawaii\"]) #Get the number of Hawaii Locations\nprint(\"Number of Hawaii Locations: {}\".format(hawaii)) #Print the number of hawaii locations\nprint(\"Percent of Hawaii Locations in Dataset: {}\".format( hawaii \/ len(bnb))) #Print the proportion of hawaii locations","1fe74544":"#A list of areas in the dataset that are part of the San Francisco major area\nSF = [\"Oakland\", \"Pacific Grove\", \"San Clara Country\", \"Santa Cruz County\", \"San Mateo County\", \"San Francisco\"]\n\n#Fix the cities into their major areas\n#Input: the city\/state\/county named state (this column has so many different things)\n#Output: the fixed label\ndef fixState(state):\n    \n    #Fix the labels whose major areas are not as clear\n    if state == \"Broward County\":\n        return \"Miami\"\n    if state == \"Twin Cities MSA\":\n        return \"Minneapolis\"\n    if state == \"Clark County\":\n        return \"Las Vegas\"\n    \n    #Lump labels together if thier major area is the same\n    if state == \"Boston\" or state == \"Cambridge\":\n        return \"Boston\"\n    if state == \"Portland\" or state == \"Salem\":\n        return \"Portland\"\n    if state == \"Jersey City\":\n        return \"New York City\"\n    if state in SF:\n        return \"San Francisco\"\n    \n    return state #Return the label if it does not need to change\n\nbnb[\"city\"] = bnb[\"city\"].apply(fixState) #Fix the city column with its major areas","e35b5f21":"percentages = [] #A list variable to hold the percentages of overall listings in each major area\ncounts = [] #A list variable to hold the counts of each major area\ncities = bnb[\"city\"].unique() #Get the unique major areas to loop through\n\n#For each major area, get its number of listings and the proportion of the overall data that count takes up\nfor city in cities:\n    cityCount = len(bnb.loc[bnb[\"city\"] == city]) #Get the number of listings in that city\/major area\n    counts.append((city, cityCount)) #Append the count with its city name into the counts list\n    percentages.append((city, cityCount\/len(bnb))) #Append the proportion (Percent) for the city into the percentages list\n    \ncityDF = pd.DataFrame(counts, columns = [\"City\", \"Count\"]) #Put the count list into a dataframe\nperDF = pd.DataFrame(percentages, columns = [\"City\", \"Percentage\"]) #Put the percentage list into a dataframe\ncityDF = cityDF.join(perDF.set_index(\"City\"), on = \"City\") #Join the percentage and count sets\n\ncityCount = cityDF.sort_values(\"Count\", ascending = False) #Sort the joined datafrmame by its counts\ncityCount.plot.bar(x = \"City\", y = \"Count\") #Plot the counts for each city in a bar graph\n\ncityPer = cityDF.sort_values(\"Percentage\", ascending = False) #Sort the joined dataframe by its percentage proportions\ncityPer = cityPer.set_index(\"City\") #Set the index to city so the pie chart function works correctly\ncityPer.plot.pie(y = \"Percentage\", fontsize = 10, legend = False, figsize = (24, 16)) #Plot a pie chart of city proportions in the dataset","eac5c06c":"price = bnb[\"price\"].copy() #Take the price as its own variable. That is what we are looking for\nprice = np.log(price) #Take the log of the set for normalization","e8e37c5c":"print(bnb.loc[bnb[\"price\"] > 10000])","a3ac03a2":"characteristics = bnb.copy() #Take a copy of the dataframe for usage\ncharacteristics = characteristics.drop(columns = {\"price\"}) #Remove the price, since we cannot predict price if it is already there\ncharacteristics.head() #Take a peek at the data without the price","09000d2a":"charact = characteristics.drop(columns = {\"name\", \"hostName\",  \"neigh\", \"lastReview\"}) #Remove the variables discussed above\ncharact.head() #Take a peek at the data after removing the variables","a4d2f237":"charact = pd.get_dummies(charact) #Get the dummies for easier model training\nscale = StandardScaler() #Add a standard scaler to scale our data for easier use later\nscale.fit(charact) #Fit the scaler with our characteristics\nchara = scale.transform(charact) #Transform the data with our scaler","ddc5df2d":"print(len(chara[0])) #Print the scaled data","3ef4c231":"charaTrain, charaTest, priceTrain, priceTest = train_test_split(chara, price, test_size = 0.1) #Split the data into train and test\nprint(priceTest) #Print one of the splits","7b182ff5":"forest = RandomForestRegressor(n_estimators = 150) #Build a whole forest of trees\nforest.fit(charaTrain, priceTrain) #Fit the forest","7017d6da":"predict = forest.predict(charaTest) #Get the predictions for RMSE","f43e0a75":"overallAccuracy = (\"Overall\", forest.score(charaTest, priceTest)) #Get the overall accuracy\nprint(\"Forest Accuracy: \", forest.score(charaTest, priceTest)) #Print the accuracy\nprint(\"Root Mean Square Error: \", np.sqrt(mean_squared_error(priceTest, predict))) #Print the root mean square error","35e0bf6e":"attributes = charact.columns #Get the tested attributes\nattributes = list(zip(attributes, forest.feature_importances_)) #Zip the attributes together with their coefficient\nsortAtt = sorted(attributes, key = lambda x: x[1], reverse = True) #Sort the zipped attributes by their coefficients\n\nprint(\"According to the Random Forest (most accurate), the most important factors for pricing are: \") #Start printing the most important labels\ni=0 #Counter variable so only the top five are printed\n\n#For each attribute in the sorted attributes\nfor label, coef in sortAtt:\n    if i<5: #If there has not been five printed yet\n        print(label) #Print the label as an important factor\n    i += 1 #Increase i by 1","05964d45":"predictions = pd.DataFrame({\"truePrice\": priceTest.values, \"predPrice\": predict}) #Create a dataframe with the predictions\npredictions.head(10) #Take a peek at the predictions","fde5f5f2":"error = np.subtract(np.exp(predictions[\"truePrice\"]), np.exp(predictions[\"predPrice\"])) #Get the variance by subtracting the true and prediction\nb = plt.hlines(500, xmin = 0, xmax = 25000, lw = 3) #Print a line to show variance\nc = plt.hlines(-300, xmin = 0, xmax = 25000, lw = 3) #Print a lower line to show variance\na = plt.plot(error, \"b.\") #Plot the error\nplt.show() #Show the plot","962724b1":"#ExtractChara: extracts the desired characteristics like I did step by step for the full dataset\n#Input: the area dataset\n#Output: the extracted characteristics\ndef extractChara(data):\n    characteristics = data.copy() #Take a copy of the dataframe for usage\n    characteristics = characteristics.drop(columns = {\"price\"}) #Remove the price, since we cannot predict price if it is already there\n    \n    charact = characteristics.drop(columns = {\"name\", \"hostName\", \"neigh\", \"lastReview\"}) #Remove the variables discussed before\n\n    charact = pd.get_dummies(charact) #Get the dummies for easier model training\n    scale = StandardScaler() #Add a standard scaler to scale our data for easier use later\n    scale.fit(charact) #Fit the scaler with our characteristics\n    chara = scale.transform(charact) #Transform the data with our scaler\n    \n    return chara #Return the extracted characteristics\n\n#RandomForest: build a random forest for the given area\n#Input: the given characteristics, the prices, and the area this is representing\n#Output: the model accuracy with the area\ndef randomForest(chara, price, area):\n    charaATrain, charaATest, priceATrain, priceATest = train_test_split(chara, price, test_size = 0.1) #Split the data into train and test\n    \n    forest = RandomForestRegressor(n_estimators = 150) #Build a whole forest of trees\n    forest.fit(charaATrain, priceATrain) #Fit the forest\n    predictA = forest.predict(charaATest) #Get the predictions for RMSE\n    \n    accuracyA = forest.score(charaATest, priceATest)\n    \n    print(\"{} Accuracy: {}\".format(area, accuracyA)) #Print the accuracy\n    print(\"{} Root Mean Square Error: {}\".format(area, np.sqrt(mean_squared_error(priceATest, predictA)))) #Print the root mean square error\n    \n    return (area, accuracyA) #Return the accuracy with the area for visualization","b02ff5d5":"areas = bnb[\"city\"].unique() #Get all the unique major areas\naccuracies = [overallAccuracy] #Build a list to build the accuracies\n\nfor area in areas:\n    areaData = bnb.loc[bnb[\"city\"] == area] #Look only at the data for the area\n    \n    priceArea = areaData[\"price\"].copy() #Take the price as its own variable. That is what we are looking for\n    priceArea = np.log(priceArea) #Take the log of the set for normalization\n    \n    areaData = areaData.drop(columns = {\"city\"})\n    charaArea = extractChara(areaData) #Extract the wanted characteristics\n    \n    accuracies.append(randomForest(charaArea, priceArea, area)) #Call the random forest function for the specific area","e62c0928":"accDF = pd.DataFrame(accuracies, columns = [\"Area\", \"Accuracy\"]) #Put the accuracies list into a dataframe\n\naccSort = accDF.sort_values(\"Accuracy\", ascending = False) #Sort the accuracies datafrmame by its accuracies\naccSort.plot.bar(x = \"Area\", y = \"Accuracy\") #Plot the model accuracy for each major area in a bar graph","de545e53":"bnbTrim = bnb.loc[bnb[\"price\"] > 20] #Trim out values lower than 20\nbnbTrim = bnbTrim.loc[bnbTrim[\"price\"] < 2000] #Trim out values higher than 1500\nprint(\"Max: \",np.max(bnbTrim[\"price\"])) #Print the current max price\nprint(\"Min: \",np.min(bnbTrim[\"price\"])) #Print the current min price","7855cb92":"priceTrim = bnbTrim[\"price\"].copy() #Take the price as its own variable. That is what we are looking for\npriceTrim = np.log(priceTrim) #Take the log of the set for normalization\n    \ncharaTrim = extractChara(bnbTrim) #Extract the wanted characteristics\n    \narea, accuracyTrim = randomForest(charaTrim, priceTrim, \"Trimmed\") #Call the random forest function for the specific area","cec76418":"# Check for Null Values","976a42e1":"Hawaii accounts for a massive 10% of the dataset at 22434 locations. The map is justified.","56d20a8a":"# AirBNB Locations","c79ebcb2":"---","ad483721":"## Split the Data","d30abd57":"# Visualize Major Areas","93aab2c2":"(I will be using a Random Forest Regression, as that proved best when I compared regression accuracies in a previous project).","05f3c3ca":"---","ada44ade":"This dataset appears to ignore relatively smaller areas, but this should not cause problems.","f5e3d6b1":"---","1f47c22e":"# Regression on the Overall Dataset","1d0302d5":"# Fix City Names into Major Areas","dd00f734":"There are several columns that I should not take into account here. The name and hostName columns are all considered categorical data filled with the entirely different values, so there is not nearly enough memory to handle pandas bringing that to dummies. The same goes for lastReview if left as a string. As a datetime (which I set it to), the scaler does not recognize it. Then there is neigh, which I actually tried to use. It turned fitting the model into an hour long endeavor due to it creating a lot of dummy variables and only increased accuracy by about 3%. The trade off for that one is not worth it.","f3a20da1":"Both name and host_name do not inherently have any importance since their ID's are what is important. I can decide what to drop later after more exploration, so I will fill these with generic, obviously filled in names. I am thinking \"AIRBNB HOUSING\" and \"AIRBNB HOST\".\n\nAs for neighborhood_group, it appears this is just here to emphasize certain areas of cities like New York. I will change nulls to \"Other\" in this case, as not all cities have neighborhoods like New York. The neighborhood names (besides NY) also seem inconsistent when looking at it through the other null prints, so that is something to keep in mind.","e88d453b":"Coded by Luna McBride","07ead074":"---","892b1be4":"# Regression Project: AirBNB Price Prediction","050564e3":"---","b8a33886":"All of the null values have been fixed.","d8c6f99a":"---","bef31cfc":"Reviews_Per_Month and Last_Review appear to be null when there are no nulls. The best way to fill these would probably be to make Reviews_Per_Month 0 and a dummy date for last review (01-01-01). This data field will have to be fixed into yyyy-mm-dd anyway, so a non-date value will cause problems.","ca010e29":"The accuracy seems to have gone down upon removing values, which means the mean was not the problem. The root mean square error has remained below 1 every time though.","bd31c8ab":"# Conclusion","2d04f99b":"---","f80b40bb":"In this project, the model only came to a 60% accuracy with the random forest, but root mean square errors remained below 1 the whole time. This likely means it cannot quite be fully predicted due the the strangeness that comes with users inputting prices. The accuracy could be higher or lower by major area, so it seems the areas are also inconsistent.\n\nLooking at the characteristics the model determined was most important, these were roomType_Entire home\/apt, longitude, latitude, id, and monthlyReviews. Of course higher monthly reviews and getting an entire house would make the price go higher. It means more people are coming in and wanting to not share the AirBNB. This makes perfect sense. The ID being here means that it depends on the property itself, which could have been hidden in the title or similar means. The method I was trying to practice just did not suit NLP. As for Latitude and Longitude, I am quite surprised. I originally cut them out since they were encompassed in the city\/major area variable, but that variable never made it into the top spot, even when ignoring items like latitude and longitude. These appear to be more important in grouping major areas together rather than looking at one as a whole, which is pretty interesting.","a3e91e40":"# Fix the Null Values","7c854ded":"Source: https:\/\/basemaptutorial.readthedocs.io\/en\/latest\/subplots.html , Basemap Documentation","d5fb7bb1":"The max value is 24999 and the min values is 0, so all of these mean errors are going to have issues. So, I will see what happens if I remove unusually low and high values (keep 60 < x < 1000)","39c55a3f":"# Fix Column Names for My Comfort","7394caf7":"# Overall Removing Extraneous Values","ec7c7f56":"---","9bca70b6":"Hawaii is littered with AirBNB locations. I should check the data to make sure that makes sense.","d40eeb18":"---","d44d7f67":"---","55ec4560":"## Fit the Forest Regressor","8c45c1c2":"The best accuracy I could get was around 60%. This only happened when I added back longitude and latitude (which I removed since I thought the city\/major area would cover that). 60% is still not the best, but considering how users can determine their own prices, it is not surprising. There are listings for 24999 and 1 in the price field, so I definitely think the hosts do not use the same criteria when determining price. \n\nThis is for the overall data, however. I wonder if looking at different locations in isolation will provide better accuracies.","0478f834":"# Regression by City"}}