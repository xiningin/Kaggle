{"cell_type":{"01e1373d":"code","971dfada":"code","d0e72049":"code","9370ddf3":"code","4d9366e6":"code","3d374841":"code","9bcefd9c":"code","bfbb6e34":"code","a171ad45":"code","b4632d67":"code","ace0c5f6":"code","f1ac214e":"code","686305bf":"code","ca2c1d71":"code","18b56de5":"code","999a922b":"code","02a3a3b5":"code","945ecbf4":"code","2637b813":"code","eff42c6b":"code","0306ba17":"code","e1a7efa4":"code","e23dad68":"code","fe04471a":"code","91c8f31f":"code","cc2d10e2":"code","afbedb68":"code","67891a68":"code","8ad2e2ff":"code","98214ba4":"code","0035f4e5":"code","379aa27f":"code","3f526cad":"code","e39c2ec2":"code","1e5758eb":"code","650c4ab4":"code","d0a794db":"code","1a1f2aac":"code","650a0885":"code","a8731a0a":"code","8069529d":"code","fe3c6194":"code","41d77ad0":"code","cc5a2b59":"code","facc5a14":"code","4c16fefc":"code","3840fdb3":"code","78a7dd14":"code","b733f5ba":"code","833e6bae":"code","8dd8c9f3":"code","05193a0c":"code","afbf5015":"code","63c48e28":"code","d07b8e2b":"code","533cb68e":"code","ac2288e5":"code","9b815adf":"code","c398549c":"code","b7c51c30":"code","7a85bcb2":"code","72d1f664":"code","66f04baa":"code","d188f428":"code","8dc21180":"code","06ac2fcd":"code","4ecc4806":"code","3583cadf":"code","e0fab8b4":"code","46719520":"code","c7895e86":"code","18fb3608":"code","1719cb02":"code","aba6a0e0":"code","b94905f9":"code","08b32c84":"code","ccea50e1":"code","90b064ec":"code","53e7fa18":"code","4d5a375b":"code","7a2dd1d1":"code","b49cfa77":"code","df07dc2e":"code","c3fe3d6e":"code","01f353a0":"code","03de52bc":"code","e19b9365":"code","c4587e40":"code","197b1f71":"code","82bcf10d":"code","8a0a380e":"code","76c59046":"code","89da14e8":"code","ed190314":"code","5f4d0acf":"code","66e825c4":"code","1dda01f9":"code","f428c4aa":"code","b920d4d3":"code","6e8b002d":"code","72e1694e":"code","0a4c02e0":"code","afe8cb82":"code","a2a52c12":"code","c768c5bb":"code","6781c120":"code","2edefa1f":"code","ced4d05c":"code","e398919f":"code","47f9980a":"code","51965d57":"code","653eddff":"code","26e70d55":"code","9d7d98ce":"code","8066d7a7":"code","20b6563b":"code","160e803a":"code","87042132":"code","936c31d1":"code","caad28db":"code","b71ec601":"code","e66429e0":"code","4a443165":"code","b70f77ba":"code","7e671239":"code","2a31da9f":"code","16bd2fbe":"code","a74aa5f1":"code","7a39f364":"code","ccbf5a49":"code","4427caf8":"code","54c4d5a1":"code","fd8ca0cd":"code","c65befcf":"code","b6eedfc0":"code","88ce8cbe":"code","0009cd68":"code","edee02fa":"code","5cb11de1":"code","c1b159f1":"code","87201fd4":"code","8fc53cca":"code","bf9ef534":"code","910a4d9a":"code","2e7d3048":"code","d6d25584":"code","cadfdc5e":"code","98fc4bcf":"code","76402d10":"code","1fde0124":"code","a0a81c66":"code","ee364123":"code","986606ba":"code","a4440e8b":"markdown","115b1e2d":"markdown","82c00e16":"markdown","cd506c61":"markdown","9d63058b":"markdown","faf77343":"markdown","bede51e4":"markdown","c9928891":"markdown","322417a9":"markdown","767e306a":"markdown","301f35af":"markdown","712e7818":"markdown","42d01b0d":"markdown","928fe4e7":"markdown","b032d89c":"markdown","231da10c":"markdown","24577869":"markdown","67c69ac5":"markdown","1aea1b2d":"markdown","47b386e4":"markdown","7c5b757a":"markdown","72598585":"markdown","184cfc14":"markdown","09dbb329":"markdown"},"source":{"01e1373d":"import warnings\nwarnings.filterwarnings('ignore')\n\n# Import Relevant Packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.impute import SimpleImputer \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, RandomForestRegressor\nfrom sklearn.svm import SVC\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import accuracy_score, roc_auc_score, precision_score, log_loss, f1_score, mean_squared_error","971dfada":"sm, md, lg = 13, 15, 20 \nplt.rc('font', size=sm)\nplt.rc('axes', labelsize=sm)\nplt.rc('xtick', labelsize=md)\nplt.rc('ytick', labelsize=md)\nplt.rc('legend', fontsize=md)\nplt.rc('figure', titlesize=lg)","d0e72049":"root = '..\/input\/indian-school-education-statistics\/'","9370ddf3":"drop_out = pd.read_csv(root+'dropout-ratio-2012-2015.csv')\nenrol = pd.read_csv(root+'gross-enrollment-ratio-2013-2016.csv')\ncomp = pd.read_csv(root+'percentage-of-schools-with-comps-2013-2016.csv')\nelect = pd.read_csv(root+'percentage-of-schools-with-electricity-2013-2016.csv')\nwater = pd.read_csv(root+'percentage-of-schools-with-water-facility-2013-2016.csv')\nboys = pd.read_csv(root+'schools-with-boys-toilet-2013-2016.csv')\ngirls = pd.read_csv(root+'schools-with-girls-toilet-2013-2016.csv')","4d9366e6":"drop_out['State_UT'] = drop_out['State_UT'].apply(lambda x: \"Arunachal Pradesh\" if x == 'Arunachal  Pradesh' else x)\ndrop_out['State_UT'] = drop_out['State_UT'].apply(lambda x: \"Madhya Pradesh\" if x == 'Madhya  Pradesh' else x)\ndrop_out['State_UT'] = drop_out['State_UT'].apply(lambda x: \"Tamil Nadu\" if x == 'Tamil  Nadu' else x)\ndrop_out['State_UT'] = drop_out['State_UT'].apply(lambda x: \"Andaman & Nicobar Islands\" if x == 'A & N Islands' else x)","3d374841":"drop_out.sort_values(by=['State_UT','year'],inplace=True,ignore_index=True)\nenrol.sort_values(by=['State_UT','Year'], inplace=True,ignore_index=True)\ncomp.sort_values(by=['State_UT','year'], inplace=True,ignore_index=True)\nelect.sort_values(by=['State_UT','year'], inplace=True,ignore_index=True)\nwater.sort_values(by=['State\/UT','Year'], inplace=True,ignore_index=True)\nboys.sort_values(by=['State_UT','year'], inplace=True,ignore_index=True)\ngirls.sort_values(by=['State_UT','year'], inplace=True,ignore_index=True)","9bcefd9c":"elect.head(3)","bfbb6e34":"comp.head(3)","a171ad45":"def CreateDataSets(mean_dataframes, tier):\n    '''\n    tier: which student level to create, 1 for primary,2 for upper primary, 3 for secondary, 4 for Higher secondary.\n    result: DataFrame containing only specific student level\n    '''\n    columns = mean_dataframes.columns.to_list()[0:]\n    index = mean_dataframes[columns[0]]\n    print('Create Dataset with {} features'.format(len(columns))) \n\n    if tier == 1:\n        coresult_1 = mean_dataframes[columns[1]]\n        coresult_2 = mean_dataframes[columns[2]]\n        coresult_3 = mean_dataframes[columns[3]]\n    elif tier == 2:\n        coresult_1 = mean_dataframes[columns[4]]\n        coresult_2 = mean_dataframes[columns[5]]\n        coresult_3 = mean_dataframes[columns[6]]\n    elif tier == 3:\n        coresult_1 = mean_dataframes[columns[7]]\n        coresult_2 = mean_dataframes[columns[8]]\n        coresult_3 = mean_dataframes[columns[9]]\n\n    elif tier == 4:\n        coresult_1 = mean_dataframes[columns[10]]\n        coresult_2 = mean_dataframes[columns[11]]\n        if len(columns)>12: coresult_3 = mean_dataframes[columns[12]]\n        else: coresult_3 = pd.Series(np.ones(len(mean_dataframes)))\n        \n    result = pd.DataFrame(pd.concat([index, coresult_1,coresult_2,coresult_3], axis = 1))\n    result.set_index('State_UT', inplace=True)\n    return result","b4632d67":"def get_df_name(df):\n    name =[x for x in globals() if globals()[x] is df][0]\n    return name + '_'\n\ndef PreprocessFiles(dataframe, categories, year_filter=['2013-14','2014-15'], year='year', city = 'State_UT'):\n    '''\n    year_filter: years to be used while aggregating the dataset\n    '''\n    \n    tmp = []\n    if len(year_filter)>1:\n        for categ in categories:\n            dataframe[categ] = dataframe[categ].astype(float)                # If the datatype of our numerical features is object so we need to change to float.\n            tmp.append(pd.DataFrame({'mean_' + categ : dataframe.iloc[np.where( (dataframe[year]==year_filter[0]) | (dataframe[year]==year_filter[1]))].groupby([city])[categ].mean()}))  \n    \n    else:\n        for categ in categories:\n            dataframe[categ] = dataframe[categ].astype(float)                # If the datatype of our numerical features is object so we need to change to float.\n            tmp.append(pd.DataFrame({'mean_' + categ : dataframe.iloc[np.where( (dataframe[year]==year_filter[0]) )].groupby([city])[categ].mean()}))  \n    \n    mean_dataframe_per_state = pd.DataFrame(tmp[0])     # Initially add the first Student Category type in the DataFrame so it's easy to use pd.merge()\n    for Stu_type in range(1, len(tmp)):           # Starting at an Index of 1 since since I already initialized our dataframe with the first Student type \n        tmp[Stu_type].reset_index(inplace = True)\n        mean_dataframe_per_state = pd.merge(mean_dataframe_per_state, tmp[Stu_type], on = city) \n    columns = mean_dataframe_per_state.columns.to_list()\n\n    new_cols_name = [city]\n    new_cols_name.extend([get_df_name(dataframe) + col for col in columns if col != city])\n    mapper = {columns[i]: new_cols_name[i] for i in range(len(columns))} \n    mean_dataframe_per_state.rename(columns = mapper, inplace=True)\n    \n    return mean_dataframe_per_state","ace0c5f6":"drop_out.head(3)","f1ac214e":"imputer = SimpleImputer(missing_values = 'NR', strategy='constant', fill_value=0)\nimputer_1 = SimpleImputer(missing_values = 'Uppe_r_Primary', strategy='constant', fill_value=0)","686305bf":"drop_out_cols = drop_out.columns.to_list()\ndrop_out = imputer.fit_transform(drop_out)\ndrop_out = pd.DataFrame(imputer_1.fit_transform(drop_out), columns=drop_out_cols)","ca2c1d71":"mean_drop_out_per_state = PreprocessFiles(drop_out, drop_out.columns[2:], year_filter=['2012-13', '2013-14'])","18b56de5":"test_drop_out = PreprocessFiles(drop_out, drop_out.columns[2:], year_filter=['2014-15'])","999a922b":"mean_drop_out_per_state.head(3)","02a3a3b5":"primary_drop_out = CreateDataSets(mean_drop_out_per_state, 1)\nupp_drop_out = CreateDataSets(mean_drop_out_per_state, 2)\nsec_drop_out = CreateDataSets(mean_drop_out_per_state, 3)\nhigher_drop_out = CreateDataSets(mean_drop_out_per_state, 4)","945ecbf4":"eval_primary_drop_out = CreateDataSets(test_drop_out, 1)\neval_upp_drop_out = CreateDataSets(test_drop_out, 2)\neval_sec_drop_out = CreateDataSets(test_drop_out, 3)\neval_higher_drop_out = CreateDataSets(test_drop_out, 4)","2637b813":"boys.head(3)","eff42c6b":"mean_boys_per_state = PreprocessFiles(boys, boys.columns[2:])","0306ba17":"test_boys = PreprocessFiles(boys, boys.columns[2:], year_filter=['2015-16'])","e1a7efa4":"mean_boys_per_state.head(3)","e23dad68":"primary_boys = CreateDataSets(mean_boys_per_state, 1)\nupp_boys = CreateDataSets(mean_boys_per_state, 2)\nsec_boys = CreateDataSets(mean_boys_per_state, 3)\nhigher_boys = CreateDataSets(mean_boys_per_state, 4)","fe04471a":"higher_boys.drop(columns=[0], inplace=True)","91c8f31f":"higher_boys.head(2)","cc2d10e2":"eval_primary_boys = CreateDataSets(test_boys, 1)\neval_upp_boys = CreateDataSets(test_boys, 2)\neval_sec_boys = CreateDataSets(test_boys, 3)\neval_higher_boys = CreateDataSets(test_boys, 4)","afbedb68":"eval_higher_boys.drop(columns=[0], inplace=True)","67891a68":"mean_girls_per_state = PreprocessFiles(girls, girls.columns[2:])","8ad2e2ff":"test_girls = PreprocessFiles(girls, girls.columns[2:], year_filter=['2015-16'])","98214ba4":"mean_girls_per_state.head(3)","0035f4e5":"primary_girls = CreateDataSets(mean_girls_per_state, 1)\nupp_girls = CreateDataSets(mean_girls_per_state, 2)\nsec_girls = CreateDataSets(mean_girls_per_state, 3)\nhigher_girls = CreateDataSets(mean_girls_per_state, 4)","379aa27f":"higher_girls.drop(columns=[0], inplace=True)","3f526cad":"higher_girls.head(3)","e39c2ec2":"eval_primary_girls = CreateDataSets(test_girls, 1)\neval_upp_girls = CreateDataSets(test_girls, 2)\neval_sec_girls = CreateDataSets(test_girls, 3)\neval_higher_girls = CreateDataSets(test_girls, 4)","1e5758eb":"eval_higher_girls.drop(columns=[0], inplace=True)","650c4ab4":"water.head(3)","d0a794db":"mean_water_facilities = PreprocessFiles(water, water.columns[2:], city ='State\/UT', year = 'Year')","1a1f2aac":"test_water_fac = PreprocessFiles(water, water.columns[2:],city ='State\/UT', year = 'Year', year_filter=['2015-16'])\ntest_water_fac.rename(columns={'State\/UT':'State_UT'}, inplace=True)","650a0885":"mean_water_facilities.head(3)","a8731a0a":"mean_water_facilities.rename(columns={'State\/UT':'State_UT'}, inplace=True)","8069529d":"primary_water = CreateDataSets(mean_water_facilities, 1)\nupp_water = CreateDataSets(mean_water_facilities, 2)\nsec_water = CreateDataSets(mean_water_facilities, 3)\nhigher_water = CreateDataSets(mean_water_facilities, 4)","fe3c6194":"higher_water.drop(columns=[0], inplace=True)","41d77ad0":"higher_water.head(2)","cc5a2b59":"eval_primary_water_fac = CreateDataSets(test_water_fac, 1)\neval_upp_water_fac = CreateDataSets(test_water_fac, 2)\neval_sec_water_fac = CreateDataSets(test_water_fac, 3)\neval_higher_water_fac = CreateDataSets(test_water_fac, 4)","facc5a14":"eval_higher_water_fac.drop(columns=[0], inplace=True)","4c16fefc":"elect.head(3)","3840fdb3":"mean_elect_facilities = PreprocessFiles(elect, elect.columns[2:])","78a7dd14":"test_elect_fac = PreprocessFiles(elect, elect.columns[2:], year_filter=['2015-16'])","b733f5ba":"mean_elect_facilities.head(3)","833e6bae":"primary_elect = CreateDataSets(mean_elect_facilities, 1)\nupp_elect = CreateDataSets(mean_elect_facilities, 2)\nsec_elect = CreateDataSets(mean_elect_facilities, 3)\nhigher_elect = CreateDataSets(mean_elect_facilities, 4)","8dd8c9f3":"higher_elect.drop(columns=[0], inplace=True)","05193a0c":"upp_elect.head(2)","afbf5015":"eval_primary_elect_fac = CreateDataSets(test_elect_fac, 1)\neval_upp_elect_fac = CreateDataSets(test_elect_fac, 2)\neval_sec_elect_fac = CreateDataSets(test_elect_fac, 3)\neval_higher_elect_fac = CreateDataSets(test_elect_fac, 4)","63c48e28":"eval_higher_elect_fac.drop(columns=[0], inplace=True)","d07b8e2b":"comp.head(3)","533cb68e":"mean_comp_facilities = PreprocessFiles(comp, comp.columns[2:])","ac2288e5":"test_comp_fac = PreprocessFiles(comp, comp.columns[2:], year_filter=['2015-16'])","9b815adf":"mean_comp_facilities.head(3)","c398549c":"primary_comp = CreateDataSets(mean_comp_facilities, 1)\nupp_comp = CreateDataSets(mean_comp_facilities, 2)\nsec_comp = CreateDataSets(mean_comp_facilities, 3)\nhigher_comp = CreateDataSets(mean_comp_facilities, 4)","b7c51c30":"higher_comp.drop(columns=[0], inplace=True)","7a85bcb2":"primary_comp.head(2)","72d1f664":"eval_primary_comp_fac = CreateDataSets(test_comp_fac, 1)\neval_upp_comp_fac = CreateDataSets(test_comp_fac, 2)\neval_sec_comp_fac = CreateDataSets(test_comp_fac, 3)\neval_higher_comp_fac = CreateDataSets(test_comp_fac, 4)","66f04baa":"eval_higher_comp_fac.drop(columns=[0], inplace=True)","d188f428":"def GenerateTrain(df_list, indexes=[]):\n    '''\n    df_list:  a list of similar dataframes to join into one\n    '''\n    tmp_train = []\n    if len(indexes) == 0:\n        for col,_ in enumerate(df_list):\n            tmp_train.append(df_list[col])\n    elif len(indexes)>=0:\n        for _,col in enumerate(indexes):\n            tmp_train.append(df_list[col])\n    \n    train = pd.concat(tmp_train, axis=1)\n    return train","8dc21180":"pry_useful = [primary_comp,primary_elect, primary_drop_out, primary_boys,primary_girls, primary_water]\npry_df = GenerateTrain(pry_useful)","06ac2fcd":"upp_useful = [upp_comp,upp_elect, upp_drop_out, upp_boys,upp_girls, upp_water]\nupp_df = GenerateTrain(upp_useful)","4ecc4806":"sec_useful = [sec_comp,sec_elect, sec_drop_out, sec_boys,sec_girls, sec_water]\nsec_df = GenerateTrain(sec_useful)","3583cadf":"high_useful = [higher_comp,higher_elect, higher_drop_out, higher_boys,higher_girls, higher_water]\nhigh_df = GenerateTrain(high_useful)","e0fab8b4":"eval_pry_useful = [eval_primary_comp_fac,eval_primary_elect_fac,eval_primary_drop_out, eval_primary_boys, eval_primary_girls,eval_primary_water_fac]\neval_pry_df = GenerateTrain(eval_pry_useful)","46719520":"eval_upp_useful = [eval_upp_comp_fac,eval_upp_elect_fac,eval_upp_drop_out, eval_upp_boys, eval_upp_girls,eval_upp_water_fac]\neval_upp_df = GenerateTrain(eval_upp_useful)","c7895e86":"eval_sec_useful = [eval_sec_comp_fac,eval_sec_elect_fac,eval_sec_drop_out, eval_sec_boys, eval_sec_girls,eval_sec_water_fac]\neval_sec_df = GenerateTrain(eval_sec_useful)","18fb3608":"eval_high_useful = [eval_higher_comp_fac,eval_higher_elect_fac,eval_higher_drop_out, eval_higher_boys, eval_higher_girls,eval_higher_water_fac]\neval_high_df = GenerateTrain(eval_high_useful)","1719cb02":"enrol['State_UT'].replace({\n    'MADHYA PRADESH':'Madhya Pradesh',\n    'Pondicherry':'Puducherry',\n    'Uttaranchal':'Uttar Pradesh'\n},inplace=True)","aba6a0e0":"enrol.head(3)","b94905f9":"imputer_2 = SimpleImputer(missing_values = 'NR', strategy='constant', fill_value=0)\nimputer_3 = SimpleImputer(missing_values = '@', strategy='constant', fill_value=0)","08b32c84":"enrol_col = enrol.columns.to_list()\nenrol = imputer_2.fit_transform(enrol)\nenrol = pd.DataFrame(imputer_3.fit_transform(enrol), columns=enrol_col)","ccea50e1":"mean_enrol_per_state = PreprocessFiles(enrol, enrol_col[2:], year='Year')","90b064ec":"test_mean_enrol_per_state = PreprocessFiles(enrol, enrol_col[2:], year='Year', year_filter=['2015-16'])","53e7fa18":"mean_enrol_per_state.head(3)","4d5a375b":"enrol_primary = CreateDataSets(mean_enrol_per_state, 1)\nenrol_upper_primary = CreateDataSets(mean_enrol_per_state, 2)\nenrol_secondary = CreateDataSets(mean_enrol_per_state, 3)\nenrol_higher = CreateDataSets(mean_enrol_per_state, 4)","7a2dd1d1":"enrol_primary.head(3)","b49cfa77":"test_enrol_primary = CreateDataSets(test_mean_enrol_per_state, 1)\ntest_enrol_upper_primary = CreateDataSets(test_mean_enrol_per_state, 2)\ntest_enrol_secondary = CreateDataSets(test_mean_enrol_per_state, 3)\ntest_enrol_higher = CreateDataSets(test_mean_enrol_per_state, 4)","df07dc2e":"def CreateTargetFeature(dataframe, gender=2):\n    ''' 0 for male, 1 for female, 2 for both male and female.'''\n    \n    cols = dataframe.columns.to_list()\n    threshold = dataframe.describe().loc['50%'][gender]\n    \n    target = dataframe[cols[gender]]\n    def thresholder(data):\n        if data < threshold:\n            return 0 #BAD\n        elif data >= threshold:\n            return 1 #GOOD\n        else:\n            pass\n    target = target.apply(thresholder)\n    return target","c3fe3d6e":"primary_target = CreateTargetFeature(enrol_primary)\nupper_primary_target = CreateTargetFeature(enrol_upper_primary)\nsecondary_target = CreateTargetFeature(enrol_secondary)\nhigher_target = CreateTargetFeature(enrol_higher)","01f353a0":"eval_primary_target = CreateTargetFeature(test_enrol_primary)\neval_upper_primary_target = CreateTargetFeature(test_enrol_upper_primary)\neval_secondary_target = CreateTargetFeature(test_enrol_secondary)\neval_higher_target = CreateTargetFeature(test_enrol_higher)","03de52bc":"def DropNa(df, target):\n    '''Drop states that may not be present in both datasets'''\n    \n    uncommon = [i for i in df.index if i not in target.index]\n    df.drop([i for i in uncommon], inplace=True)\n    df.fillna(method='ffill', inplace=True)\n    pass","e19b9365":"def blender(x):\n    if x<0.5:return 0 \n    elif x>=0.5: return 1\n    else: pass","c4587e40":"def ModelEvaluator(y_true, y_pred):\n    eval_dict = {}\n    eval_dict['log_loss'] = log_loss(y_true, y_pred)\n    eval_dict['roc_auc_score'] = roc_auc_score(y_true, y_pred,)#multi_class=\"ovr\")\n    true_classes = pd.Series(y_pred).apply(blender)\n    eval_dict['f1_score'] = f1_score(y_true, true_classes,)#average='macro')\n    eval_dict['accuracy_score'] = accuracy_score(y_true, true_classes)\n    return eval_dict","197b1f71":"def Model(train, target, test, algo=RandomForestClassifier):\n    errcb1=[]\n    y_pred_totcb1=[]\n    fold=StratifiedKFold(n_splits=9)\n    i=1\n    cols = train.columns.to_list()\n    for train_index, test_index in fold.split(train,target):\n        print(str(i) + ' iter')\n        X_train, X_test = train.iloc[train_index], train.iloc[test_index]\n        y_train, y_test = target.iloc[train_index], target.iloc[test_index]\n        m1 = algo(n_estimators=100, random_state=2020)\n        m1.fit(X_train, y_train)\n        \n        preds = m1.predict_proba(X_test)[:,1]\n        print('err: ', log_loss(y_test, preds))\n        errcb1.append(log_loss(y_test, preds))\n        p1 = m1.predict_proba(test)[:,1]\n        y_pred_totcb1.append(p1)\n        \n        best_feature = cols[np.argmax(m1.feature_importances_)]\n        worst_feature = cols[np.argmin(m1.feature_importances_)]    \n        print('Best Feature for the {} iteration is {}, while, the worst feature is {}'.format(i, best_feature, worst_feature))\n        \n        i+=1\n    #np.mean(errcb1)\n    return np.mean(y_pred_totcb1, axis=0)","82bcf10d":"metric_list = []                 ### Store the metric results of all Student category types","8a0a380e":"boy_pry_df = GenerateTrain(eval_pry_useful,[0,1,2,3,5])\nboy_eval_pry_df = GenerateTrain(eval_pry_useful,[0,1,2,3,5]) \nboy_primary_target = CreateTargetFeature(enrol_primary,0)    #0 for boys\neval_boy_primary_target = CreateTargetFeature(test_enrol_primary,0) ","76c59046":"DropNa(boy_pry_df,boy_primary_target)          # Run cell only once\nDropNa(boy_eval_pry_df,eval_boy_primary_target)","89da14e8":"boy_primary_target.shape, boy_pry_df.shape, boy_eval_pry_df.shape, eval_boy_primary_target.shape","ed190314":"boy_pry_preds = Model(boy_pry_df, boy_primary_target, boy_eval_pry_df)","5f4d0acf":"ModelEvaluator(eval_boy_primary_target, boy_pry_preds)","66e825c4":"metric_list.append(['Primary Student Boys',ModelEvaluator(eval_boy_primary_target, boy_pry_preds)])   # run only once","1dda01f9":"girl_pry_df = GenerateTrain(eval_pry_useful,[0,1,2,4,5])\ngirl_eval_pry_df = GenerateTrain(eval_pry_useful,[0,1,2,4,5]) \ngirl_primary_target = CreateTargetFeature(enrol_primary,1)    #1 for girls\neval_girl_primary_target = CreateTargetFeature(test_enrol_primary, 1)","f428c4aa":"DropNa(girl_pry_df, girl_primary_target)   # Run  only once\nDropNa(girl_eval_pry_df, eval_girl_primary_target)","b920d4d3":"girl_primary_target.shape, girl_pry_df.shape, girl_eval_pry_df.shape, eval_girl_primary_target.shape","6e8b002d":"girl_pry_preds = Model(girl_pry_df, girl_primary_target, girl_eval_pry_df)","72e1694e":"ModelEvaluator(eval_girl_primary_target, girl_pry_preds)","0a4c02e0":"metric_list.append(['Primary Student Girls', ModelEvaluator(eval_girl_primary_target, girl_pry_preds) ])   # run only once","afe8cb82":"boy_upp_df = GenerateTrain(eval_upp_useful,[0,1,2,3,5,])\nboy_eval_upp_df = GenerateTrain(eval_upp_useful,[0,1,2,3,5]) \nboy_upp_target = CreateTargetFeature(enrol_upper_primary,0)    #0 for boys\neval_boy_upp_target = CreateTargetFeature(test_enrol_upper_primary, 0)","a2a52c12":"DropNa(boy_upp_df, boy_upp_target)   # Run  only once\nDropNa(boy_eval_upp_df, eval_boy_upp_target)","c768c5bb":"boy_upp_target.shape, boy_upp_df.shape, boy_eval_upp_df.shape, eval_boy_upp_target.shape","6781c120":"boy_upp_preds = Model(boy_upp_df, boy_upp_target, boy_eval_upp_df)","2edefa1f":"ModelEvaluator(eval_boy_upp_target, boy_upp_preds)","ced4d05c":"metric_list.append(['Upper Primary Boys',ModelEvaluator(eval_boy_upp_target, boy_upp_preds)])   # run only once ","e398919f":"girl_upp_df = GenerateTrain(eval_upp_useful,[0,1,2,4,5])\ngirl_eval_upp_df = GenerateTrain(eval_upp_useful,[0,1,2,4,5]) \ngirl_upp_target = CreateTargetFeature(enrol_upper_primary,1)    #1 for girls\ngirl_eval_upp_target = CreateTargetFeature(test_enrol_upper_primary, 1)","47f9980a":"DropNa(girl_upp_df, girl_upp_target)   # Run  only once\nDropNa(girl_eval_upp_df, girl_eval_upp_target)","51965d57":"girl_upp_target.shape, girl_upp_df.shape, girl_eval_upp_df.shape, girl_eval_upp_target.shape","653eddff":"girl_upp_preds = Model(girl_upp_df, girl_upp_target, girl_eval_upp_df)","26e70d55":"ModelEvaluator(girl_eval_upp_target, girl_upp_preds)","9d7d98ce":"metric_list.append(['Upper Primary Girls', ModelEvaluator(girl_eval_upp_target, girl_upp_preds)])   #run only once","8066d7a7":"boy_sec_df = GenerateTrain(eval_sec_useful,[0,1,2,3,5])\nboy_eval_sec_df = GenerateTrain(eval_sec_useful,[0,1,2,3,5]) \nboy_sec_target = CreateTargetFeature(enrol_secondary,0)    #0 for boys\nboy_eval_sec_target = CreateTargetFeature(test_enrol_secondary, 0)","20b6563b":"DropNa(boy_sec_df, boy_sec_target)   # Run  only once\nDropNa(boy_eval_sec_df, boy_eval_sec_target)","160e803a":"boy_sec_target.shape, boy_sec_df.shape, boy_eval_sec_df.shape, boy_eval_sec_target.shape","87042132":"boy_sec_preds = Model(boy_sec_df, boy_sec_target, boy_eval_sec_df)","936c31d1":"ModelEvaluator(boy_eval_sec_target, boy_sec_preds)","caad28db":"metric_list.append(['Secondary Student Boys',ModelEvaluator(boy_eval_sec_target, boy_sec_preds)])   # run only once","b71ec601":"girl_sec_df = GenerateTrain(eval_sec_useful,[0,1,2,4,5])\ngirl_eval_sec_df = GenerateTrain(eval_sec_useful,[0,1,2,4,5]) \ngirl_sec_target = CreateTargetFeature(enrol_secondary,1)    #1 for girls\ngirl_eval_sec_target = CreateTargetFeature(test_enrol_secondary,1)","e66429e0":"DropNa(girl_sec_df, girl_sec_target)   # Run  only once\nDropNa(girl_eval_sec_df, girl_eval_sec_target)","4a443165":"girl_sec_target.shape, girl_sec_df.shape, girl_eval_sec_df.shape, girl_eval_sec_target.shape","b70f77ba":"girl_sec_preds = Model(girl_sec_df, girl_sec_target, girl_eval_sec_df)","7e671239":"ModelEvaluator(girl_eval_sec_target, girl_sec_preds)","2a31da9f":"metric_list.append(['Secondary Student Girls', ModelEvaluator(girl_eval_sec_target, girl_sec_preds)])   #run only once","16bd2fbe":"boy_high_df = GenerateTrain(eval_high_useful,[0,1,2,3,5])\nboy_eval_high_df = GenerateTrain(eval_high_useful,[0,1,2,3,5]) \nboy_high_target = CreateTargetFeature(enrol_higher,0)    #0 for boys\nboy_eval_high_target = CreateTargetFeature(test_enrol_higher, 0)","a74aa5f1":"DropNa(boy_high_df, boy_high_target)   # Run  only once\nDropNa(boy_eval_high_df, boy_eval_high_target)","7a39f364":"boy_high_target.shape, boy_high_df.shape, boy_eval_high_df.shape, boy_eval_high_target.shape","ccbf5a49":"boy_high_preds = Model(boy_high_df, boy_high_target, boy_eval_high_df)","4427caf8":"ModelEvaluator(boy_eval_high_target, boy_high_preds)","54c4d5a1":"metric_list.append(['Higher Secondary Student Boys', ModelEvaluator(boy_eval_high_target, boy_high_preds)])  # run only once","fd8ca0cd":"girl_high_df = GenerateTrain(eval_high_useful,[0,1,2,4,5])\ngirl_eval_high_df = GenerateTrain(eval_high_useful,[0,1,2,4,5]) \ngirl_high_target = CreateTargetFeature(enrol_higher,1)    #1 for girls\ngirl_eval_high_target = CreateTargetFeature(test_enrol_higher,1)","c65befcf":"DropNa(girl_high_df, girl_high_target)   # Run  only once\nDropNa(girl_eval_high_df, girl_eval_high_target)","b6eedfc0":"girl_high_target.shape, girl_high_df.shape, girl_eval_high_df.shape, girl_eval_high_target.shape","88ce8cbe":"girl_high_preds = Model(girl_high_df, girl_high_target, girl_eval_high_df)","0009cd68":"ModelEvaluator(girl_eval_high_target, girl_high_preds)","edee02fa":"metric_list.append(['Higher Secondary Student Girls', ModelEvaluator(girl_eval_high_target, girl_high_preds)])  # run only once","5cb11de1":"len(metric_list)   # should be 8","c1b159f1":"student_type = pd.Series( [i[0] for i in metric_list], name='student_type' )\nlog_loss_metric = pd.Series( [i[1]['log_loss'] for i in metric_list], name='log_loss' )\nroc_auc_score_metric = pd.Series( [i[1]['roc_auc_score'] for i in metric_list], name='roc_auc_score' )\nf1_score_metric = pd.Series( [i[1]['f1_score'] for i in metric_list], name='f1_score' )\naccuracy_metric = pd.Series( [i[1]['accuracy_score'] for i in metric_list], name='accuracy_score' )\nmetric_df = pd.DataFrame(pd.concat([student_type, log_loss_metric, roc_auc_score_metric, f1_score_metric, accuracy_metric], axis=1)).set_index('student_type')","87201fd4":"metric_df","8fc53cca":"metric_df.plot(kind='bar', figsize=(27,7), title='Evaluation metrics')","bf9ef534":"metric_df.describe().drop('count').plot(kind='bar', figsize=(26,6), title='Distribution of metric performance')","910a4d9a":"def CreateContinousTargetFeature(dataframe, gender=2):\n    ''' 0 for male, 1 for female, 2 for both male and female.'''\n    cols = dataframe.columns.to_list()\n    \n    target = dataframe[cols[gender]]\n    return target","2e7d3048":"boy_cont_primary_target = CreateContinousTargetFeature(enrol_primary, 0)","d6d25584":"eval_boy_cont_primary_target = CreateContinousTargetFeature(test_enrol_primary,0)","cadfdc5e":"boy_cont_primary_target.shape, boy_pry_df.shape, boy_eval_pry_df.shape, eval_boy_cont_primary_target.shape","98fc4bcf":"regressor = RandomForestRegressor(random_state=1960)","76402d10":"regressor.fit(boy_pry_df, boy_cont_primary_target)","1fde0124":"boy_pry_cont_preds = regressor.predict(boy_eval_pry_df)","a0a81c66":"np.sqrt(mean_squared_error(eval_boy_cont_primary_target, boy_pry_cont_preds))","ee364123":"boy_pry_cont_preds[0], eval_boy_cont_primary_target[0]","986606ba":"!pip list > requirements.txt  ","a4440e8b":"## Primary Student Category\n### datasets are arranged as comp, elect, drop_out, boys, girls, water\n### genders in target are arranged as boys, girls, both_gender","115b1e2d":"## Boys ","82c00e16":"## The Task is to use machine learning to forecast gross enrollment in 2015-16, using data of 2012-15 session, such as dropout rate, water and computer facilities. \n## In this notebook I approached the task as a classification problem and seperated the gross enrollment into 2 groups.","cd506c61":"## Electricity Facilities","9d63058b":"## Drop Out Preprocessing","faf77343":"Test","bede51e4":"## Boys","c9928891":"## Secondary Student Category\n### datasets are arranged as comp, elect, drop_out, boys, girls, water\n### genders in target are arranged as boys, girls, both_gender","322417a9":"## Gross Enrollment\n\nwhat we're going to predict","767e306a":"## Higher Secondary Student Category\n### datasets are arranged as comp, elect, drop_out, boys, girls, water\n### genders in target are arranged as boys, girls, both_gender","301f35af":"## Girls","712e7818":"## Boys","42d01b0d":"## Upper Primary Student Category\n### datasets are arranged as comp, elect, drop_out, boys, girls, water\n### genders in target are arranged as boys, girls, both_gender","928fe4e7":"## Toilet Facilities Preprocessing","b032d89c":"### same steps can be reapeated for other student categories","231da10c":"## Primary Student Boy","24577869":"## Data preprocessing","67c69ac5":"## Girls","1aea1b2d":"## Water Facilities ","47b386e4":"## Girls","7c5b757a":"## Translate Problem as a Regression Task","72598585":"## Computer Facilities","184cfc14":"## Girls","09dbb329":"## Boys"}}