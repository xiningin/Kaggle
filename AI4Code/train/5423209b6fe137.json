{"cell_type":{"e2521e3a":"code","f01a54c5":"code","fb788061":"code","645bd91c":"code","efbd8792":"code","2a1e2857":"code","1e9f0bad":"code","a7126c79":"code","4ad360c6":"code","abb49283":"code","efcf2c34":"code","091406d6":"code","2cb225bf":"code","c5bcba86":"code","87bb630d":"code","a962e2c7":"code","ae49a27c":"code","a7748696":"code","17f228b8":"code","a025da85":"code","7205cb94":"code","8515f612":"code","a1facbc1":"code","52a23b14":"code","35140227":"code","087505ab":"code","9dd0dec7":"code","11366eb0":"code","5dc11729":"code","0c76fff2":"code","3aec1cf7":"code","14313bb3":"code","ad169cf4":"code","003479cb":"code","a507e148":"code","a66a3b09":"markdown","2b87c2e7":"markdown","20c84ec0":"markdown","e9e463d8":"markdown","2b7258a4":"markdown","766ba941":"markdown","4e7247ae":"markdown","54438d05":"markdown","71895d9e":"markdown"},"source":{"e2521e3a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport string\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neural_network import MLPClassifier\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f01a54c5":"# Directory paths to CSV files\nDATA_DIR = '\/kaggle\/input\/titanic\/train.csv'\nSUBMISSION_DIR = '\/kaggle\/input\/titanic\/test.csv'","fb788061":"# Load the dataframe from CSV\ntrain_df = pd.read_csv(DATA_DIR, index_col = \"PassengerId\")\ntest_df = pd.read_csv(SUBMISSION_DIR, index_col = \"PassengerId\")","645bd91c":"# Note the seperation and merge the dataframes\ntrain_index = len(train_df)\ntrain_df = train_df.append(test_df)","efbd8792":"train_df.info()","2a1e2857":"train_df.head(10)","1e9f0bad":"# Calculate correlation of age to determine factors\ntrain_df_corr = train_df.corr().abs().unstack().sort_values(ascending = False).reset_index()\ntrain_df_corr.rename(columns = {\"level_0\" : \"Feature 1\", \"level_1\" : \"Feature 2\", 0 : \"Correlation Coefficient\"}, inplace = True)\ntrain_df_corr[train_df_corr[\"Feature 1\"] == 'Age']","a7126c79":"# Imputing Age according to PClass and Sex\ntrain_df[\"Age\"] = train_df.groupby([\"Sex\", \"Pclass\"])[\"Age\"].apply(lambda x : x.fillna(x.median()))","4ad360c6":"# Imputing Age Feature using Title, Sex and PClass\nage_df = train_df.groupby(['Sex', 'Pclass'])['Age']\ntrain_df['Age'] = age_df.apply(lambda x : x.fillna(x.mean()))","abb49283":"# Only 2 missing values, to check which are those\ntrain_df[train_df[\"Embarked\"].isnull()]","efcf2c34":"# Googling these people shows, they embarked from S\n# Imputing Embarked feature as S\ntrain_df['Embarked'] = train_df[\"Embarked\"].fillna('S')","091406d6":"# Single missing, checking what it is\ntrain_df[train_df[\"Fare\"].isnull()]","2cb225bf":"# Imputing Fare Feature as Male with third class ticket, no family\nmedian_fare = train_df.groupby([\"Pclass\", \"Parch\", \"SibSp\"]).Fare.median()[3][0][0]\ntrain_df[\"Fare\"] = train_df[\"Fare\"].fillna(median_fare)","c5bcba86":"# Imputing Fare with Median value\ntrain_df[\"Fare\"] = train_df[\"Fare\"].fillna(train_df[\"Fare\"].median())","87bb630d":"# Checking distribution of Cabin with Pclass\ntrain_df[\"Deck\"] = train_df[\"Cabin\"].apply(lambda s : s[0] if pd.notnull(s) else 'M')\n\n# Function to get the distribution accoriding to Pclass\ndef get_pclass_distribution(df):\n    # Create a dictionary for every passenger class count in every deck\n    deck_counts = {'A' : {}, 'B': {}, 'C': {}, 'D': {}, 'E': {}, 'F': {}, 'G': {}, 'M': {}, 'T': {}}\n    decks = df.columns.levels[0]\n    \n    for deck in decks:\n        for pclass in range(1, 4):\n            try:\n                count = df[deck][pclass][0]\n                deck_counts[deck][pclass] = count\n            except:\n                deck_counts[deck][pclass] = 0\n                \n    df_decks = pd.DataFrame(deck_counts)\n    deck_percentages = {}\n    \n    # Create a dictionary for every passenger class percentage in every deck\n    for column in df_decks.columns:\n        deck_percentages[column] = [(count \/ df_decks[column].sum()) * 100 for count in df_decks[column]]\n        \n    return deck_counts, deck_percentages\n\n# Function to display the distribution according to Pclass\ndef display_pclass_distribution(percentages):\n    df_percentages = pd.DataFrame(percentages).transpose()\n    deck_names = ('A', 'B', 'C', 'D', 'E', 'F', 'G', 'M', 'T')\n    bar_count = np.arange(len(deck_names))\n    \n    pclass1 = df_percentages[0]\n    pclass2 = df_percentages[1]\n    pclass3 = df_percentages[2]\n    \n    plt.figure(figsize=(10, 5))\n    plt.bar(bar_count, pclass1, label = 'Passenger Class 1')\n    plt.bar(bar_count, pclass2, bottom = pclass1, label = 'Passenger Class 2')\n    plt.bar(bar_count, pclass3, bottom = pclass1 + pclass2, label = 'Passenger Class3')\n    \n    plt.xlabel('Deck')\n    plt.ylabel('Passenger Class Percentage')\n    plt.xticks(bar_count, deck_names)\n    \n    plt.legend()\n    plt.title('Passenger Class Distribution in Decks')\n    plt.show()\n \n\ndeck_df = train_df.groupby(['Deck', 'Pclass']).count().rename(columns={'Name': 'Count'}).transpose()\nall_deck_count, all_deck_per = get_pclass_distribution(deck_df)\ndisplay_pclass_distribution(all_deck_per)","a962e2c7":"# Checking distribution of survival percentage with Deck\n\n# Function to get the survival distribution\ndef get_survived_distribution(df):\n    # Create a dictionary for every survival count in every deck\n    survive_counts = {'A':{}, 'B':{}, 'C':{}, 'D':{}, 'E':{}, 'F':{}, 'G':{}, 'M':{}, 'T': {}}\n    decks = df.columns.levels[0]\n    \n    for deck in decks:\n        for survive in range(0, 2):\n            try:\n                survive_counts[deck][survive] = df[deck][survive][0]\n            except:\n                survive_counts[deck][survive] = 0\n            \n    # Create a dictionary for survival percentage\n    df_survive = pd.DataFrame(survive_counts)\n    survive_percentages = {}\n    \n    for column in df_survive.columns:\n        survive_percentages[column] = [(count \/ df_survive[column].sum()) * 100 for count in df_survive[column]]\n        \n    return survive_counts, survive_percentages\n\n# Function to disply the survival distribution\ndef display_survived_distribution(percentages):\n    df_survived_percentages = pd.DataFrame(percentages).transpose()\n    deck_names = ('A', 'B', 'C', 'D', 'E', 'F', 'G', 'M', 'T')\n    bar_count = np.arange(len(deck_names))\n    \n    not_survived = df_survived_percentages[0]\n    survived = df_survived_percentages[1]\n    \n    plt.figure(figsize=(10, 5))\n    plt.bar(bar_count, not_survived, label = \"Not Survived\")\n    plt.bar(bar_count, survived, bottom = not_survived, label=\"Survived\")\n    \n    plt.xlabel('Deck')\n    plt.ylabel(\"Survival Percentage\")\n    plt.xticks(bar_count, deck_names)\n    \n    plt.legend()\n    plt.title(\"Survival Percentage in Decks\")\n    plt.show()\n    \nsurvive_df = train_df.groupby([\"Deck\", \"Survived\"]).count().transpose()\nsurvive_count, survive_percentage = get_survived_distribution(survive_df)\ndisplay_survived_distribution(survive_percentage)","ae49a27c":"# Imputing Cabin: Create a new Feature Deck\n# Grouping A, B, C (1st class passengers only)\n# Grouping D, E (similar passenger and survival distribution)\n# Grouping F, G (similar passenger and survival distribution)\n# M remains separate\n# T is grouped with A as T has a single member\n\n# Passenger T to A\nidx = train_df[train_df['Deck'] == \"T\"].index\ntrain_df.loc[idx, 'Deck'] = 'A'\n\n# Grouping\ntrain_df[\"Deck\"] = train_df[\"Deck\"].replace([\"A\", 'B', 'C'], 'ABC')\ntrain_df[\"Deck\"] = train_df[\"Deck\"].replace([\"D\", \"E\"], 'DE')\ntrain_df[\"Deck\"] = train_df[\"Deck\"].replace([\"F\", \"G\"], 'FG')\n\n# Final check\ntrain_df[\"Deck\"].value_counts()","a7748696":"# Distribution of Fare (Continous Variables)\nsns.FacetGrid(train_df, hue=\"Survived\", height = 5).map(sns.distplot, \"Fare\").add_legend()","17f228b8":"# Feature Engineering\n# Fare feature is positively skewed and survival rate is high on right side\n# Binning Fare into 13 features, 13 works because the distribution is now divided\ntrain_df[\"Fare\"] = pd.qcut(train_df[\"Fare\"], 13)\n\nsns.countplot(x = 'Fare', hue = 'Survived', data = train_df)","a025da85":"# Distribution of Age (Continous Variables)\nsns.FacetGrid(train_df, hue=\"Survived\", height = 5).map(sns.distplot, \"Age\").add_legend()","7205cb94":"# Feature Engineering\n# Age feature has a normal distribution approximately\n# Binning Age into 10 bins, 10 works because the distribution is now divided\ntrain_df[\"Age\"] = pd.qcut(train_df[\"Age\"], 10)\n\nsns.countplot(x = 'Age', hue = 'Survived', data = train_df)","8515f612":"# Feature Engineering : Create a new feature of Family Size\ntrain_df['FamilySize'] = train_df['Parch'] + train_df['SibSp'] + 1\ntrain_df[\"FamilySize\"].value_counts()","a1facbc1":"# And bin them, according to:\n# 1 : Alone\n# 2, 3, 4 : Small\n# 5, 6 : Medium\n# 7, 8, 11 : Large\nfamily_map = {1 : 'Alone', 2 : 'Small', 3 : 'Small', 4 : 'Small', 5 : 'Medium', 6 : 'Medium', 7 : 'Large', 8 : 'Large', 11 : 'Large'}\ntrain_df['FamilySizeGrouped'] = train_df[\"FamilySize\"].map(family_map)","52a23b14":"# Feature Engineering : Add Ticket Frequency\n# May help in forming groups of people travelling together\ntrain_df[\"TicketFrequency\"] = train_df.groupby('Ticket')['Ticket'].transform('count')","35140227":"# Feature Engineering : Title and Is Married\ntrain_df['Title'] = train_df['Name'].str.split(', ', expand = True)[1].str.split('.', expand = True)[0]\ntrain_df['IsMarried'] = 0\ntrain_df['IsMarried'].loc[train_df['Title'] == 'Mrs'] = 1\n\n# Grouping the Titles\ntrain_df['Title'] = train_df['Title'].replace(['Miss', 'Mrs', 'Ms', 'Mlle', 'Lady', 'Mme', 'the Countess', 'Dona'], 'Miss\/Mrs\/Ms')\ntrain_df['Title'] = train_df['Title'].replace(['Dr', 'Col', 'Major', 'Jonkheer', 'Capt', 'Sir', 'Don', 'Rev'], 'Dr\/Military\/Noble\/Clergy')","087505ab":"# Feature Engineering : Grouping Families\n# Function to extract the surnames of families\ndef extract_surname(data):\n    families = []\n    \n    # Loop over the data\n    for i in range(len(data)):\n        name = data.iloc[i]\n        \n        # Remove brackets from name\n        if '(' in name:\n            name_no_bracket = name.split('(')[0]\n        else:\n            name_no_bracket = name\n            \n        # Extract the name and title\n        family = name_no_bracket.split(',')[0]\n        title = name_no_bracket.split(',')[1].strip().split(' ')[0]\n        \n        # Replace all kinds of punctations\n        for c in string.punctuation:\n            family = family.replace(c, '').strip()\n            \n        families.append(family)\n    \n    return families\n\ntrain_df['Family'] = extract_surname(train_df['Name'])","9dd0dec7":"# Feature Engineering : Survival Rate\n# Determined by taking average of Family Survival Rate and Ticket Survival Rate\n# Some not applicable records for testing data\n\n# Create a list of families and tickets that are occuring in both training and testing set\nnon_unique_families = [x for x in train_df[:train_index]['Family'].unique() if x in train_df[train_index:]['Family'].unique()]\nnon_unique_tickets = [x for x in train_df[:train_index]['Ticket'].unique() if x in train_df[train_index:]['Ticket'].unique()]\n\ndf_family_survival_rate = train_df[:train_index].groupby('Family')['Survived', 'Family', 'FamilySize'].median()\ndf_ticket_survival_rate = train_df[:train_index].groupby('Ticket')['Survived', 'Ticket', 'TicketFrequency'].median()\n\n# Populate the family and ticket survival rates\nfamily_rates = {}\nticket_rates = {}\n\nfor i in range(len(df_family_survival_rate)):\n    # Checking a family exists in both training and test set, and has members more than 1\n    if df_family_survival_rate.index[i] in non_unique_families and df_family_survival_rate.iloc[i, 1] > 1:\n        family_rates[df_family_survival_rate.index[i]] = df_family_survival_rate.iloc[i, 0]\n        \nfor i in range(len(df_ticket_survival_rate)):\n    # Checking a ticket exists in both training and test set, and has members more than 1\n    if df_ticket_survival_rate.index[i] in non_unique_tickets and df_ticket_survival_rate.iloc[i, 1] > 1:\n        ticket_rates[df_ticket_survival_rate.index[i]] = df_ticket_survival_rate.iloc[i, 0]\n        ","11366eb0":"# Assign the Average survival rate based on Family and Ticket\nmean_survival_rate = np.mean(train_df[:train_index][\"Survived\"])\n\ntrain_family_survival_rate = []\ntrain_family_survival_rate_NA = []\ntest_family_survival_rate = []\ntest_family_survival_rate_NA = []\n\n# Determine the Family Survival Rate for Training Set\nfor i in range(len(train_df[:train_index])):\n    if train_df['Family'].iloc[i] in family_rates:\n        train_family_survival_rate.append(family_rates[train_df['Family'].iloc[i]])\n        train_family_survival_rate_NA.append(1)\n    else:\n        train_family_survival_rate.append(mean_survival_rate)\n        train_family_survival_rate_NA.append(0)\n   \n# Determine the Family Survival Rate for Test Set\nfor i in range(len(train_df[train_index:])):\n    if train_df['Family'].iloc[i] in family_rates:\n        test_family_survival_rate.append(family_rates[train_df['Family'].iloc[i]])\n        test_family_survival_rate_NA.append(1)\n    else:\n        test_family_survival_rate.append(mean_survival_rate)\n        test_family_survival_rate_NA.append(0)\n        \n# Assign the survival rates\ntrain_df[\"FamilySurvivalRate\"] = train_family_survival_rate + test_family_survival_rate\ntrain_df[\"FamilySurvivalRateNA\"] = train_family_survival_rate_NA + test_family_survival_rate_NA\n\ntrain_ticket_survival_rate = []\ntrain_ticket_survival_rate_NA = []\ntest_ticket_survival_rate = []\ntest_ticket_survival_rate_NA = []\n\n# Determine the Ticket Survival Rate for Training Set\nfor i in range(len(train_df[:train_index])):\n    if train_df['Ticket'].iloc[i] in ticket_rates:\n        train_ticket_survival_rate.append(ticket_rates[train_df['Ticket'].iloc[i]])\n        train_ticket_survival_rate_NA.append(1)\n    else:\n        train_ticket_survival_rate.append(mean_survival_rate)\n        train_ticket_survival_rate_NA.append(0)\n   \n# Determine the Ticket Survival Rate for Test Set\nfor i in range(len(train_df[train_index:])):\n    if train_df['Ticket'].iloc[i] in ticket_rates:\n        test_ticket_survival_rate.append(ticket_rates[train_df['Ticket'].iloc[i]])\n        test_ticket_survival_rate_NA.append(1)\n    else:\n        test_ticket_survival_rate.append(mean_survival_rate)\n        test_ticket_survival_rate_NA.append(0)\n        \n# Assign the survival rates\ntrain_df[\"TicketSurvivalRate\"] = train_ticket_survival_rate + test_ticket_survival_rate\ntrain_df[\"TicketSurvivalRateNA\"] = train_ticket_survival_rate_NA + test_ticket_survival_rate_NA","5dc11729":"# Create a new survival rate feature\ntrain_df['SurvivalRate'] = (train_df[\"TicketSurvivalRate\"] + train_df[\"FamilySurvivalRate\"]) \/ 2\ntrain_df[\"SurvivalRateNA\"] = (train_df[\"TicketSurvivalRateNA\"] + train_df[\"FamilySurvivalRateNA\"]) \/ 2","0c76fff2":"# Convert features to Categorical\ntrain_df['Sex'] = train_df['Sex'].map({\"male\" : 0, \"female\" : 1})\n\npclass_dummies = pd.get_dummies(train_df[\"Pclass\"], drop_first = True, prefix = 'Pclass')\ntitle_dummies = pd.get_dummies(train_df[\"Title\"], drop_first = True, prefix = 'Title')\nembarked_dummies = pd.get_dummies(train_df[\"Embarked\"], drop_first = True, prefix = 'Embarked')\ndeck_dummies = pd.get_dummies(train_df[\"Deck\"], drop_first = True, prefix = 'Deck')\nage_dummies = pd.get_dummies(train_df[\"Age\"], drop_first = True, prefix = \"Age\")\nfare_dummies = pd.get_dummies(train_df[\"Fare\"], drop_first = True, prefix = \"Fare\")\nfamily_dummies = pd.get_dummies(train_df[\"FamilySizeGrouped\"], drop_first = True, prefix = \"FamilySize\")\n\n# Drop the the categorical fields and concatenate\ntrain_df.drop([\"Deck\", \"FamilySize\", \"FamilySizeGrouped\", \"Parch\", \"SibSp\", \n               \"Pclass\", \"Title\", \"Cabin\", \"Embarked\", \"Name\", \"Ticket\",\n               \"Age\", \"Fare\", \"Family\", \"FamilySurvivalRate\", \"FamilySurvivalRateNA\",\n               \"TicketSurvivalRate\", \"TicketSurvivalRateNA\"], axis = 1, inplace = True)\ntrain_df = pd.concat([train_df, pclass_dummies, title_dummies, embarked_dummies, deck_dummies, age_dummies, fare_dummies, family_dummies], axis = 1)","3aec1cf7":"train_df.columns","14313bb3":"# Split the training and prediction part\nX_train = StandardScaler().fit_transform(train_df[:train_index].drop(columns = ['Survived']))\ny_train = train_df[:train_index]['Survived'].values\n\nX_prediction = StandardScaler().fit_transform(train_df[train_index:].drop(columns = ['Survived']))","ad169cf4":"# Random Forest and it's Optimization\n# Use a model that is to overfit on the Test Data (Why?)\n# Because, final submissions are calculated by taking mean of the probabilites\n# of predictions, therefore good to have slightly larger individual probabilites\nforest = LogisticRegression()\n\n# Stratified K Fold validation\nN = 5; score = 0\nprobs = pd.DataFrame(np.zeros((len(X_prediction), N * 2)), columns=['Fold_{}_Prob_{}'.format(i, j) for i in range(1, N+1) for j in range(2)])\nfprs, tprs, scores = [], [], []\n\nskf = StratifiedKFold(n_splits = N, shuffle = True)\n\nfor fold, (train_index, validation_index) in enumerate(skf.split(X_train, y_train), 1):\n    print('Fold {}\\n'.format(fold))\n    \n    # Fitting the model\n    forest.fit(X_train[train_index], y_train[train_index])\n    \n    # Compute Train AUC score\n    train_fpr, train_tpr, train_thresholds = roc_curve(y_train[train_index], forest.predict_proba(X_train[train_index])[:, 1])\n    train_auc_score = auc(train_fpr, train_tpr)\n    # Compute Validation AUC score\n    validation_fpr, validation_tpr, validation_thresholds = roc_curve(y_train[validation_index], forest.predict_proba(X_train[validation_index])[:, 1])\n    validation_auc_score = auc(validation_fpr, validation_tpr)\n    \n    scores.append((train_auc_score, validation_auc_score))\n    fprs.append(validation_fpr)\n    tprs.append(validation_tpr)\n    \n    # X_test probabilities\n    probs.loc[:, 'Fold_{}_Prob_0'.format(fold)] = forest.predict_proba(X_prediction)[:, 0]\n    probs.loc[:, 'Fold_{}_Prob_1'.format(fold)] = forest.predict_proba(X_prediction)[:, 1]\n    \n    score += forest.score(X_train[validation_index], y_train[validation_index]) \/ N\n    print(\"Fold {} Score: {}\\n\".format(fold, forest.score(X_train[validation_index], y_train[validation_index])))\n    \nprint(\"Average Score: {}\".format(oob))","003479cb":"# ROC Curve\ndef plot_roc_curve(fprs, tprs):\n    tprs_interpolation = []\n    aucs = []\n    mean_fpr = np.linspace(0, 1, 100)\n    f, ax = plt.subplots(figsize = (15, 15))\n    \n    # Plot ROC for each fold and compute AUC scores\n    for i, (fpr, tpr) in enumerate(zip(fprs, tprs), 1):\n        tprs_interpolation.append(np.interp(mean_fpr, fpr, tpr))\n        tprs_interpolation[-1][0] = 0.0\n        roc_auc = auc(fpr, tpr)\n        aucs.append(roc_auc)\n        ax.plot(fpr, tpr, label = 'ROC Fold {} AUC = {:.3f}'.format(i, roc_auc))\n        \n    # Plotting ROC For random guessing\n    plt.plot([0, 1], [0, 1], linestyle = '--', label = \"Random Guessing\")\n    \n    # Plotting mean ROC\n    mean_tpr = np.average(tprs_interpolation, weights=np.array(scores)[:,1], axis = 0)\n    mean_tpr[-1] = 1.0\n    mean_auc = auc(mean_fpr, mean_tpr)\n    std_auc = np.std(aucs)\n    ax.plot(mean_fpr, mean_tpr, color = 'b', label = \"Mean ROC AUC = {:.3f}\".format(mean_auc))\n    \n    ax.set_xlabel('False Positive Rate')\n    ax.set_ylabel('True Positive Rate')\n    \n    ax.set_title('ROC Curves of Folds')\n    ax.legend()\n    \n    plt.show()\n    \nplot_roc_curve(fprs, tprs)","a507e148":"class_survived = [col for col in probs.columns if col.endswith('Prob_1')]\nprobs['1'] = probs[class_survived].sum(axis = 1) \/ N\nclass_survived.append('1')\nprobs['0'] = probs.drop(columns=class_survived).sum(axis = 1) \/ N\nprobs['pred'] = 0\npos = probs[probs['1'] >= 0.5].index\nprobs.loc[pos, 'pred'] = 1\n\ny_pred = probs['pred'].astype(int)\n\nsubmission_df = pd.DataFrame(columns = ['PassengerId', 'Survived'])\nsubmission_df['PassengerId'] = test_df.index.values\nsubmission_df['Survived'] = y_pred.values\nsubmission_df.to_csv('submissions.csv', header = True, index = False)","a66a3b09":"## Data Cleaning and Feature Engineering","2b87c2e7":"## Predictions","20c84ec0":"### Embarked Feature","e9e463d8":"## Machine Learn the Dataset","2b7258a4":"### Cabin Feature","766ba941":"### Age Feature","4e7247ae":"## Load the Data","54438d05":"### Fare Feature","71895d9e":"## Reference Links\n\n- https:\/\/medium.com\/i-like-big-data-and-i-cannot-lie\/how-i-scored-in-the-top-9-of-kaggles-titanic-machine-learning-challenge-243b5f45c8e9\n- https:\/\/www.kaggle.com\/chienhsianghung\/titanic-top-11-starter-i-models-comparing\n- https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial"}}