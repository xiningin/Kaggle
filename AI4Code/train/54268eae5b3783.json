{"cell_type":{"4deebc51":"code","2cd32523":"code","9e267d18":"code","b385a240":"code","a12a592d":"code","919a9d40":"code","857ee32d":"code","db17a6a0":"code","2e0d2a1c":"code","1448e664":"code","8a4325c6":"code","feb599d0":"code","1f162506":"code","224ace8b":"code","98957ec4":"code","d7c42399":"code","f50912d6":"code","6dd65f90":"code","745e0aa4":"code","a2b9bfab":"code","eb53029f":"code","b2f9dad5":"code","61effe01":"code","3d23b534":"code","b8199abe":"code","4e925534":"code","eb9ffa44":"code","05aebdb5":"code","41dcfb55":"code","f9cd1c6d":"code","9a15de3a":"code","49fa2920":"code","80f3f0d8":"code","02297429":"code","cc609f52":"code","9db8eb20":"code","76afd895":"code","b342c090":"code","029f0129":"code","956a2f30":"code","df3aca4f":"code","03839055":"code","0bb98b6e":"markdown","a7ccf24e":"markdown","8bc8b90f":"markdown","3e3e0b83":"markdown","6f0e302f":"markdown","810a5763":"markdown","d4118d3c":"markdown","151a5b0e":"markdown","fcfe4e4a":"markdown","27c37a36":"markdown","ff3492c7":"markdown","d85313d8":"markdown","3f235982":"markdown"},"source":{"4deebc51":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport missingno as msno \nimport scipy\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2cd32523":"#load data file\nlistings = pd.read_csv('..\/input\/seattle\/listings.csv')\n\nlistings.head()","9e267d18":"listings.duplicated().any","b385a240":"df= listings[['id', 'host_is_superhost', 'host_total_listings_count', 'host_has_profile_pic', 'host_identity_verified', 'zipcode', 'room_type','property_type','accommodates', 'bathrooms', 'bedrooms', 'beds', 'bed_type','square_feet', 'price', 'security_deposit', 'cleaning_fee', 'guests_included', 'minimum_nights', \n             'maximum_nights', 'has_availability',  'number_of_reviews', 'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value', 'requires_license', 'instant_bookable','cancellation_policy', 'reviews_per_month']]","a12a592d":"msno.matrix(df) ","919a9d40":"df.drop(['square_feet', 'security_deposit','cleaning_fee'], axis=1,inplace=True)\n\ndf[df.isnull().any(axis=1)]","857ee32d":"df = df.dropna()","db17a6a0":"#drop the irrelevant vars\ndf.drop(['id','zipcode'], axis=1,inplace=True)","2e0d2a1c":"y= df['price']\ndf.drop(['price'], axis=1,inplace=True)\n","1448e664":"X = df.copy()\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state= 42)\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\nX_train.describe()","8a4325c6":"from sklearn.preprocessing import LabelEncoder\n\ntrain_cat = X_train.select_dtypes(include=['object'])\ntest_cat = X_test.select_dtypes(include=['object'])\n\nlabelencoder = LabelEncoder()\n\nx_train_cat_encoded = train_cat.apply(labelencoder.fit_transform)\nx_test_cat_encoded = test_cat.apply(labelencoder.fit_transform)\nx_train_cat_encoded.shape, x_test_cat_encoded.shape\n#x_train_cat_encoded","feb599d0":"from sklearn.preprocessing import StandardScaler\n\nnumerics = ['int64','float64']\n\ntrain_num = X_train.select_dtypes(include=numerics)\ntest_num =  X_test[train_num.columns]\n\n\nscaler = StandardScaler()\nx_train_scaled = pd.DataFrame(scaler.fit_transform(train_num),columns = train_num.columns)\nx_test_scaled = pd.DataFrame(scaler.transform(test_num), columns = test_num.columns)\n\nx_train_scaled.shape,x_test_scaled.shape","1f162506":"#reset index\nx_train_scaled.reset_index(drop=True,inplace=True)\nx_test_scaled.reset_index(drop=True,inplace=True)\nx_train_cat_encoded.reset_index(drop=True,inplace=True)\nx_test_cat_encoded.reset_index(drop=True,inplace=True)\n\n#Concatenate the categorical and numerical data into our final training and testing dataset\nx_train_final =  pd.concat([x_train_scaled, x_train_cat_encoded],axis=1)\nx_test_final =  pd.concat([x_test_scaled, x_test_cat_encoded],axis=1)\n\nx_train_final.shape, x_test_final.shape","224ace8b":"#np.isfinite(x_train_final).all()\n\n#fill the infinity value with mean value\nx_train_final.fillna(x_train_final.mean(), inplace=True)\nx_test_final.fillna(x_test_final.mean(), inplace=True)\n\nx_train_final.shape,x_test_final.shape","98957ec4":"y_train_final = y_train.replace('[\\$,]', '', regex=True).astype(float)\ny_test_final = y_test.replace('[\\$,]', '', regex=True).astype(float)\n#reset index\ny_train_final.reset_index(drop=True,inplace=True)\ny_test_final.reset_index(drop=True,inplace=True)\n\ny_train_final.shape","d7c42399":"from sklearn.ensemble import RandomForestRegressor\n\ndef evaluate(model, test_features, test_labels):\n    predictions = model.predict(test_features)\n    errors = abs(predictions - test_labels)\n    mape = 100 * np.mean(errors \/ test_labels)\n    print('Model Performance')\n    print('Mean Absolute Error: {:0.2f}'.format(np.mean(errors)))\n    print('Mean Absolute Percentage Error: {:0.2f}%'.format(mape))\n    \n    return mape","f50912d6":"base_model = RandomForestRegressor(random_state = 42)\nbase_model.fit(x_train_final, y_train_final)\n\nbase_performance = evaluate(base_model, x_test_final, y_test_final)","6dd65f90":"from sklearn.model_selection import GridSearchCV\n\nrf = RandomForestRegressor(random_state=42)\n\nparam_grid = {\n    'max_depth': [80,100],\n    'min_samples_leaf': [1,3,5],\n    'min_samples_split': [2,8,10],\n    'n_estimators': [100,300,500]\n}\n\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)\n\n# Fit the grid search to the data\ngrid_search.fit(x_train_final, y_train_final)\ngrid_search.best_params_","745e0aa4":"rfgrid = grid_search.best_estimator_\ngrid_performance = evaluate(rfgrid, x_test_final, y_test_final)\ngrid_performance\nprint('Improvement of {:0.2f}%. on the Mean Absolute Percentage Error'.format( 100 * ( base_performance-grid_performance) \/ base_performance))","a2b9bfab":"feature_list = x_train_final.columns\n\n# Get numerical feature importances\nimportances = list(grid_search.best_estimator_.feature_importances_)\n\n# List of tuples with variable and importance\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n\n# Print out the feature and importances \n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];","eb53029f":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef plot_feature_importance(importance,names,model_type):\n\n    #Create arrays from feature importance and feature names\n    feature_importance = np.array(importance)\n    feature_names = np.array(names)\n\n    #Create a DataFrame using a Dictionary\n    data={'feature_names':feature_names,'feature_importance':feature_importance}\n    fi_df = pd.DataFrame(data)\n\n    #Sort the DataFrame in order decreasing feature importance\n    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n     \n    #Define size of bar plot\n    plt.figure(figsize=(10,8))\n    #Plot Searborn bar chart\n    sns.barplot(x=fi_df['feature_importance'][:10], y=fi_df['feature_names'][:10])\n    #Add chart labels\n    plt.title(model_type + ' FEATURE IMPORTANCE')\n    plt.xlabel('FEATURE IMPORTANCE')\n    plt.ylabel('FEATURE NAMES')","b2f9dad5":"plot_feature_importance(grid_search.best_estimator_.feature_importances_,x_train_final.columns,'RANDOM FOREST')","61effe01":"import xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\n\ndtrain = xgb.DMatrix(x_train_final, label=y_train_final)\ndtest = xgb.DMatrix(x_test_final, label=y_test_final)","3d23b534":"params = {\n    'max_depth':6,\n    'min_child_weight': 1,\n    'eta':.3,\n    'subsample': 1,\n    'colsample_bytree': 1,\n    'objective':'reg:linear',\n    'random_state' : 42\n}\nparams['eval_metric'] = \"mae\"\nnum_boost_round = 999\n\n\ncv_results = xgb.cv(\n    params,\n    dtrain,\n    num_boost_round=num_boost_round,\n    seed=42,\n    nfold=5,\n    metrics={'mae'},\n    early_stopping_rounds=10\n)\ncv_results","b8199abe":"from sklearn.metrics import mean_absolute_error\n\nbest_model = xgb.train(\n    params,\n    dtrain,\n    num_boost_round=19,\n    evals=[(dtest, \"Test\")]\n)\n","4e925534":"xg_errors = abs(best_model.predict(dtest)- y_test_final)\nxg_mape = 100 * np.mean(xg_errors \/ y_test_final)\n\nprint('Model Performance')\nprint('Mean Absolute Error: {:0.2f}'.format(np.mean(xg_errors)))\nprint('Mean Absolute Percentage Error: {:0.2f}%'.format(xg_mape))","eb9ffa44":"from xgboost import plot_importance\nfrom matplotlib import pyplot\n\nplot_importance(best_model)\npyplot.show()","05aebdb5":"import matplotlib.pyplot as plt\n\nxgb.plot_tree(best_model,num_trees=0)\nplt.rcParams['figure.figsize'] = [50, 10]\n\nplt.show()","41dcfb55":"import pandas as pd\nimport numpy as np\nfrom sklearn import datasets, linear_model\nfrom sklearn.linear_model import LinearRegression\nimport statsmodels.api as sm\nfrom scipy import stats\n\nX1 = sm.add_constant(x_train_final)\nLR_model = sm.OLS(y_train_final, X1).fit()\nprint(LR_model.summary())","f9cd1c6d":"cols = list(x_train_final.columns)\npmax = 1\nwhile (len(cols)>0):\n    p= []\n    X_1 = x_train_final[cols]\n    X_1 = sm.add_constant(X_1)\n    model = sm.OLS(y_train_final,X_1).fit()\n    p = pd.Series(model.pvalues.values[1:],index = cols)      \n    pmax = max(p)\n    feature_with_p_max = p.idxmax()\n    if(pmax>0.05):\n        cols.remove(feature_with_p_max)\n    else:\n        break\nselected_features_BE = cols\n\nprint(selected_features_BE)","9a15de3a":"x_BE_train_final = x_train_final[selected_features_BE]\n\nX2 = sm.add_constant(x_BE_train_final)\nLR_BE_model = sm.OLS(y_train_final, X2).fit()\nprint(LR_BE_model.summary())","49fa2920":"x_BE_test_final = x_test_final[selected_features_BE]\n\nX3 = sm.add_constant(x_BE_test_final)\nLR_pred = LR_BE_model.predict(X3)\n\nLR_accuracy = evaluate(LR_BE_model, X3, y_test_final)","80f3f0d8":"from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\nLasso_model = LassoCV()\nLasso_model.fit(x_train_final, y_train_final)\nprint(\"Best alpha using built-in LassoCV: %f\" % Lasso_model.alpha_)\nprint(\"Best score using built-in LassoCV: %f\" % Lasso_model.score(x_train_final,y_train_final))\ncoef = pd.Series(Lasso_model.coef_, index = x_train_final.columns)","02297429":"print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")","cc609f52":"import matplotlib\nimport matplotlib.pyplot as plt\n\nimp_coef = coef.sort_values()\n\nmatplotlib.rcParams['figure.figsize'] = (10.0, 6.0)\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Feature importance using Lasso Model\")","9db8eb20":"Lasso_pred = Lasso_model.predict(x_test_final)\nLasso_accuracy = evaluate(Lasso_model, x_test_final, y_test_final)","76afd895":"# Compute paths\nimport matplotlib.pyplot as plt\n\nn_alphas = 500\nalphas = np.logspace(-10, 2, n_alphas)\n\ncoefs = []\nfor a in alphas:\n    lasso = linear_model.Lasso(alpha=a, fit_intercept=False)\n    lasso.fit(x_train_final, y_train_final)\n    coefs.append(lasso.coef_)\n\n# #############################################################################\n# Display results\n\nax = plt.gca()\n\nax.plot(alphas, coefs)\nax.set_xscale('log')\nax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\nplt.xlabel('alpha')\nplt.ylabel('weights')\nplt.title('Lasso coefficients as a function of the regularization')\nplt.axis('tight')\nplt.show()","b342c090":"Ridge_model = RidgeCV()\nRidge_model.fit(x_train_final, y_train_final)\nprint(\"Best alpha using built-in RidgeCV: %f\" % Ridge_model.alpha_)\nprint(\"Best score using built-in RidgeCV: %f\" % Ridge_model.score(x_train_final,y_train_final))\ncoef = pd.Series(Ridge_model.coef_, index = x_train_final.columns)","029f0129":"print(\"Ridge picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")","956a2f30":"imp_coef = coef.sort_values()\n\nmatplotlib.rcParams['figure.figsize'] = (5.0, 10.0)\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Feature importance using Ridge Model\")","df3aca4f":"Ridge_pred = Ridge_model.predict(x_test_final)\nRidge_accuracy = evaluate(Ridge_model, x_test_final, y_test_final)","03839055":"from sklearn.neighbors import KNeighborsRegressor\n\nRegressor= KNeighborsRegressor(n_neighbors = 2)\nRegressor.fit(x_train_final, y_train_final)\ny_pred = Regressor.predict(x_test_final)\nknn_accuracy = evaluate(Regressor, x_test_final, y_test_final)","0bb98b6e":"# **3. Linear Regression**","a7ccf24e":"* **Feature Importance**","8bc8b90f":"# **Models**\n \n# **1. Random Forest Regression**","3e3e0b83":"# 2. XGBoost","6f0e302f":"# 2. Feature transformation","810a5763":"* **Grid Search with 3-fold Cross Validation**","d4118d3c":" * **Base Model**","151a5b0e":"* Standardize Numerical features","fcfe4e4a":"* **Backward Estimation**","27c37a36":"# 1. Data Cleansing ","ff3492c7":"* Convert categorical features to numerics","d85313d8":"# **4. KNN**","3f235982":"* **Ridge Regularization**"}}