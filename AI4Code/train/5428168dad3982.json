{"cell_type":{"85f1e280":"code","89e0c339":"code","53e48eac":"code","0bc9cddb":"code","0813ca47":"code","f1fb4ee8":"code","6fc1e799":"code","11b3692a":"code","9e299dec":"code","96e294e1":"code","18225ef8":"code","8c4df7eb":"code","8a20126a":"code","fa25fe33":"code","9b5780a0":"code","576da178":"code","760774df":"code","41eb782c":"code","a86a066c":"code","2430deff":"code","26b90d96":"code","dab08881":"code","c91b0148":"code","2e103b81":"code","f8f62954":"code","c84e9d27":"code","dffa7a3e":"code","92c9806c":"code","629032f4":"code","7b3ee838":"markdown","2660173d":"markdown","c44b2be3":"markdown","f526f5bd":"markdown","6bf45fc8":"markdown","462c1b6f":"markdown","ae091c5d":"markdown","23d7351b":"markdown","fb7919fa":"markdown","ee5afae6":"markdown","3fd1ad79":"markdown","2293f505":"markdown","58cb97a0":"markdown"},"source":{"85f1e280":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os # to use operating system dependent functionality\nimport librosa # to extract speech features\nimport wave # read and write WAV files\nimport matplotlib.pyplot as plt # to generate the visualizations\n\n# MLP Classifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score\n\n# LSTM Classifier\nimport keras\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import *\nfrom keras.optimizers import rmsprop","89e0c339":"def extract_mfcc(wav_file_name):\n    #This function extracts mfcc features and obtain the mean of each dimension\n    #Input : path_to_wav_file\n    #Output: mfcc_features'''\n    y, sr = librosa.load(wav_file_name,duration=3\n                                  ,offset=0.5)\n    mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T,axis=0)\n    \n    return mfccs","53e48eac":"##### load radvess speech data #####\nradvess_speech_labels = [] # to save extracted label\/file\nravdess_speech_data = [] # to save extracted features\/file\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/ravdess-emotional-speech-audio\/'):\n    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n        radvess_speech_labels.append(int(filename[7:8]) - 1) # the index 7 and 8 of the file name represent the emotion label\n        wav_file_name = os.path.join(dirname, filename)\n        ravdess_speech_data.append(extract_mfcc(wav_file_name)) # extract MFCC features\/file\n        \nprint(\"Finish Loading the Dataset\")","0bc9cddb":"#### convert data and label to array\nravdess_speech_data_array = np.asarray(ravdess_speech_data) # convert the input to an array\nravdess_speech_label_array = np.array(radvess_speech_labels)\nravdess_speech_label_array.shape # get tuple of array dimensions\n\n#### make categorical labels\nlabels_categorical = to_categorical(ravdess_speech_label_array) # converts a class vector (integers) to binary class matrix\nlabels_categorical.shape","0813ca47":"ravdess_speech_data_array.shape","f1fb4ee8":"x_train,x_test,y_train,y_test= train_test_split(np.array(ravdess_speech_data_array),labels_categorical, test_size=0.20, random_state=9)","6fc1e799":"# Split the training, validating, and testing sets\nnumber_of_samples = ravdess_speech_data_array.shape[0]\ntraining_samples = int(number_of_samples * 0.8)\nvalidation_samples = int(number_of_samples * 0.1)\ntest_samples = int(number_of_samples * 0.1)","11b3692a":"# Define the LSTM model\ndef create_model_LSTM():\n    model = Sequential()\n    model.add(LSTM(128, return_sequences=False, input_shape=(40, 1)))\n    model.add(Dense(64))\n    model.add(Dropout(0.4))\n    model.add(Activation('relu'))\n    model.add(Dense(32))\n    model.add(Dropout(0.4))\n    model.add(Activation('relu'))\n    model.add(Dense(8))\n    model.add(Activation('softmax'))\n    \n    # Configures the model for training\n    model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n    return model","9e299dec":"w = np.expand_dims(ravdess_speech_data_array[:training_samples],-1)","96e294e1":"w.shape","18225ef8":"### train using LSTM model\nmodel_A = create_model_LSTM()\nhistory = model_A.fit(np.expand_dims(ravdess_speech_data_array[:training_samples],-1), labels_categorical[:training_samples], validation_data=(np.expand_dims(ravdess_speech_data_array[training_samples:training_samples+validation_samples], -1), labels_categorical[training_samples:training_samples+validation_samples]), epochs=100, shuffle=True)","8c4df7eb":"### loss plots using LSTM model\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(loss) + 1)\n\nplt.plot(epochs, loss, 'ro', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","8a20126a":"### accuracy plots using LSTM model\nplt.clf()                                                \n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nplt.plot(epochs, acc, 'ro', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","fa25fe33":"### evaluate using model A\nmodel_A.evaluate(np.expand_dims(ravdess_speech_data_array[training_samples + validation_samples:], -1), labels_categorical[training_samples + validation_samples:])","9b5780a0":"model_A.save_weights(\"Model_LSTM.h5\")","576da178":"path_ = '\/kaggle\/input\/bhahahahhhahahh\/download.wav'","760774df":"import IPython.display as ipd\nipd.Audio(path_)","41eb782c":"a = extract_mfcc(path_)","a86a066c":"a.shape","2430deff":"a1 = np.asarray(a)","26b90d96":"a1.shape","dab08881":"q = np.expand_dims(a1,-1)","c91b0148":"qq = np.expand_dims(q,0)","2e103b81":"qq.shape","f8f62954":"pred = model_A.predict(qq)","c84e9d27":"pred","dffa7a3e":"preds=pred.argmax(axis=1)\npreds","92c9806c":"data = lb.inverse_transform(pred)","629032f4":"00 = neutral, 01 = calm, 02 = happy, 03 = sad, 04 = angry, 05 = fearful, 06 = disgust, 07 = surprised","7b3ee838":"* **What is Long short-term memory (LSTM)?**\n\nRecurrent neural networks, of which LSTMs are the most powerful and well known subset, are a type of artificial neural network designed to recognize patterns in sequences of data, such as text, genomes, handwriting and the spoken word. **More Details:** (https:\/\/keras.io\/layers\/recurrent\/#lstm) ","2660173d":"* **What is Mel-frequency Cepstrum Coefficients (MFCC)?** \nThe sounds generated by a human are filtered by the shape of the vocal tract including tongue, teeth etc. This shape determines what sound comes out. If we can determine the shape accurately, this should give us an accurate representation of the phoneme being produced. The shape of the vocal tract manifests itself in the envelope of the short time power spectrum, and the job of MFCCs is to accurately represent this envelope. \n![Mel-frequency Cepstrum Coefficients](https:\/\/miro.medium.com\/max\/3430\/1*pzE4i1TXaLCmzTXgdxFZjQ.jpeg)","c44b2be3":"**Speech Emotion Recognition (SER)**\n\nSpeech Emotion Recognition (SER) is one of the most challenging tasks in speech signal analysis domain, it is a research area problem which tries to infer the emotion from the speech signals. The importance of emotion recognition is getting popular with improving user experience and the engagement of Voice User Interfaces (VUIs). For example, customer services, recommender systems, and healthcare applications.\n\n**Objective:** \n\nIn this mini project,Building and training simple Speech Emotion Recognizer that predicts human emotions from audio files using Python, Sci-kit learn, librosa, and Keras. firstly, we will load the data (Ravdess dataset), extract features (MFCC) from it, and split it into training and testing sets. Then, we will initialize two models (MLP and LSTM) as emotion classifiers and train them. Finally, we will calculate the accuracy of our models.","f526f5bd":"**Import Python Packages:**","6bf45fc8":"**(1) Loading the Dataset**\n\nWe will use Speech audio-only files (16bit, 48kHz .wav) from the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) dataset. This portion of the RAVDESS contains 1440 files: 60 trials per actor x 24 actors = 1440. The RAVDESS contains 24 professional actors (12 female, 12 male), vocalizing two lexically-matched statements in a neutral North American accent. Speech emotions includes calm, happy, sad, angry, fearful, surprise, and disgust expressions. Each expression is produced at two levels of emotional intensity (normal, strong), with an additional neutral expression. **for More details:** (https:\/\/www.kaggle.com\/uwrfkaggler\/ravdess-emotional-speech-audio)","462c1b6f":"* **  Save the weights of the model as a HDF5 file **","ae091c5d":"* **What is A multilayer perceptron (MLP)?**\n* A multilayer perceptron (MLP) is a deep, artificial neural network. It is composed of more than one perceptron. They are composed of an input layer to receive the signal, an output layer that makes a decision or prediction about the input, and in between those two, an arbitrary number of hidden layers that are the true computational engine of the MLP. **More Details:** (https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neural_network.MLPClassifier.html)","23d7351b":"*  Split the dataset into training and testing sets: (training set) a subset to train a model, (test set) a subset to test the trained model.\n*  **Note:**(train_test_split) function will split arrays or matrices into random train and test subsets, in our example, training set 80% and testing set 20% ","fb7919fa":"*  **Example 1: MLP Classifier**","ee5afae6":"**Example 2: LSTM Classifier**\n\nIn this example, we will use another model to improve the accuracy which is LSTM model.","3fd1ad79":"*  **(evaluate function): Returns the loss value & metrics values for the model in test mode.**","2293f505":"**The whole pipeline is as follows (as same as any machine learning pipeline):**\n\n* (1)Loading the Dataset: This process is about loading the dataset in Python which involves extracting audio features, such as MFCC\n* (2)Training the Model: After we prepare and load the dataset, we simply train it on a suited model.\n* (3)Testing the Model: Measuring how good our model is doing.","58cb97a0":"**Preparing the dataset for machine learning models:**"}}