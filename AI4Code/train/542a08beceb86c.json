{"cell_type":{"d6b2dc81":"code","f00f5a51":"code","791ee035":"code","79172e9d":"code","3d4e734b":"code","5d24f685":"code","cc475b91":"code","59347a08":"code","7b1a2f4b":"code","57a54331":"code","d156c96f":"code","750842b5":"code","7e3eef31":"code","b0f6ee2b":"code","ac2cc4a8":"code","9c83029b":"code","7e1555d2":"code","ff7b42ed":"code","0bd629d0":"code","57611327":"code","cd0c13ca":"markdown","e7259050":"markdown","7623b421":"markdown"},"source":{"d6b2dc81":"#Libraries \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os,unicodedata, re, io, time, gc, warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom tabulate import tabulate\n\nimport tensorflow as tf\n\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nfrom sklearn.model_selection import train_test_split","f00f5a51":"# Data\nurl = '..\/input\/marathi-english-sentence-pairs\/mar.txt'\ndf = pd.read_csv(url, sep='\\t',header=None, names=['Eng','Mar','Trn'])\n\n#Dropping the Trn Column\ndf.drop('Trn',axis=1, inplace=True)\n\n#Shape\nprint(\"Total Records: \", df.shape[0])","791ee035":"#PreProcessing Function\ndef preprocess(w):\n    w = w.lower().strip()  #lower case & remove white space\n    w = re.sub(r\"([?.!,\u00bf\u0964])\", r\" \\1 \", w)\n    w = w.strip()\n    w = '<start> ' + w + ' <end>'\n    return w\n\n#Applying PreProcess Function to a single sentence\nx = np.random.randint(1,df.shape[0])\nprint(\"English: \", preprocess(df.Eng[x]))\nprint(\"Marathi: \", preprocess(df.Mar[x]))\n\n#applying the preprocess function\ndf['Eng'] = df['Eng'].apply(lambda x: preprocess(x))\ndf['Mar'] = df['Mar'].apply(lambda x: preprocess(x))\n\n#Inspect\ndf.head()","79172e9d":"#Tokenize Function\ndef tokenize(lang):\n    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n    lang_tokenizer.fit_on_texts(lang)\n\n    tensor = lang_tokenizer.texts_to_sequences(lang)\n    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n\n    return tensor, lang_tokenizer\n\n\n'''Tokenize Column Data'''\n# Input = English || # Output = Marathi\ninput_tensor, inp_lang = tokenize(df['Eng'])\ntarget_tensor, targ_lang = tokenize(df['Mar'])","3d4e734b":"# To train faster, we can limit the size of the dataset to 100 sentences \nnum_examples = 100\n\n# Calculate max_length of the target tensors\nmax_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n\n# Creating training and validation sets using an 90-10 split\ninput_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.1)\n\n# Show length\ntb_data = [[\"Eng Train Tensor\", len(input_tensor_train)], [\"Mar Train Tensor\",len(target_tensor_train)], \n           [\"Eng Val Tensor\", len(input_tensor_val)], [\"Mar Val Tensor\",len(target_tensor_val)]] \n\nprint(tabulate(tb_data, headers=['','Lengths']))","5d24f685":"#Create TensorFlow Dataset\n\nBUFFER_SIZE = len(input_tensor_train)\nBATCH_SIZE = 128\nsteps_per_epoch = len(input_tensor_train)\/\/BATCH_SIZE\nembedding_dim = 256\nunits = 1024\nvocab_inp_size = len(inp_lang.word_index)+1\nvocab_tar_size = len(targ_lang.word_index)+1\n\ndataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\ndataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n\n#Example Dataset\nexample_input_batch, example_target_batch = next(iter(dataset))\nexample_input_batch.shape, example_target_batch.shape","cc475b91":"#Encoder Class\n\nclass Encoder(tf.keras.Model):\n    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n        super(Encoder, self).__init__()\n        self.batch_sz = batch_sz\n        self.enc_units = enc_units\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n        self.gru = tf.keras.layers.GRU(self.enc_units,\n                                       return_sequences=True,\n                                       return_state=True,\n                                       recurrent_initializer='glorot_uniform')\n\n    def call(self, x, hidden):\n        x = self.embedding(x)\n        output, state = self.gru(x, initial_state = hidden)\n        return output, state\n\n    def initialize_hidden_state(self):\n        return tf.zeros((self.batch_sz, self.enc_units))","59347a08":"#Encoding \nencoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n\n# Sample Input\nsample_hidden = encoder.initialize_hidden_state()\nsample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n\nprint ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\nprint ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))","7b1a2f4b":"# Bahdanau Attention Class\n\nclass BahdanauAttention(tf.keras.layers.Layer):\n    \n    def __init__(self, units):\n        super(BahdanauAttention, self).__init__()\n        self.W1 = tf.keras.layers.Dense(units)\n        self.W2 = tf.keras.layers.Dense(units)\n        self.V = tf.keras.layers.Dense(1)\n\n    def call(self, query, values):\n        # query hidden state shape == (batch_size, hidden size)\n        # query_with_time_axis shape == (batch_size, 1, hidden size)\n        # values shape == (batch_size, max_len, hidden size)\n        # we are doing this to broadcast addition along the time axis to calculate the score\n        query_with_time_axis = tf.expand_dims(query, 1)\n\n        # score shape == (batch_size, max_length, 1)\n        # we get 1 at the last axis because we are applying score to self.V\n        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n        score = self.V(tf.nn.tanh(\n            self.W1(query_with_time_axis) + self.W2(values)))\n\n        # attention_weights shape == (batch_size, max_length, 1)\n        attention_weights = tf.nn.softmax(score, axis=1)\n\n        # context_vector shape after sum == (batch_size, hidden_size)\n        context_vector = attention_weights * values\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n\n        return context_vector, attention_weights","57a54331":"# Attention\nattention_layer = BahdanauAttention(10)\nattention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n\nprint(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\nprint(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))","d156c96f":"# Decoder Class\n\nclass Decoder(tf.keras.Model):\n    \n    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n        super(Decoder, self).__init__()\n        self.batch_sz = batch_sz\n        self.dec_units = dec_units\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n        self.gru = tf.keras.layers.GRU(self.dec_units,\n                                       return_sequences=True,\n                                       return_state=True,\n                                       recurrent_initializer='glorot_uniform')\n        self.fc = tf.keras.layers.Dense(vocab_size)\n\n        # used for attention\n        self.attention = BahdanauAttention(self.dec_units)\n\n    def call(self, x, hidden, enc_output):\n        # enc_output shape == (batch_size, max_length, hidden_size)\n        context_vector, attention_weights = self.attention(hidden, enc_output)\n\n        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n        x = self.embedding(x)\n\n        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n\n        # passing the concatenated vector to the GRU\n        output, state = self.gru(x)\n\n        # output shape == (batch_size * 1, hidden_size)\n        output = tf.reshape(output, (-1, output.shape[2]))\n\n        # output shape == (batch_size, vocab)\n        x = self.fc(output)\n\n        return x, state, attention_weights","750842b5":"decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n\nsample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n                                      sample_hidden, sample_output)\n\nprint ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))","7e3eef31":"# Optimizer and Loss function\noptimizer = tf.keras.optimizers.Adamax()\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n\ndef loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss_object(real, pred)\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n    return tf.reduce_mean(loss_)","b0f6ee2b":"#Check Points\ncheckpoint_dir = '.\/output\/'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(optimizer=optimizer,encoder=encoder,decoder=decoder)","ac2cc4a8":"# Train Model Function\n\n@tf.function\ndef train_step(inp, targ, enc_hidden):\n    loss = 0\n\n    with tf.GradientTape() as tape:\n        enc_output, enc_hidden = encoder(inp, enc_hidden)\n\n        dec_hidden = enc_hidden\n\n        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n\n        # Teacher forcing - feeding the target as the next input\n        for t in range(1, targ.shape[1]):\n            # passing enc_output to the decoder\n            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n            loss += loss_function(targ[:, t], predictions)\n            # using teacher forcing\n            dec_input = tf.expand_dims(targ[:, t], 1)\n\n    batch_loss = (loss \/ int(targ.shape[1]))\n\n    variables = encoder.trainable_variables + decoder.trainable_variables\n\n    gradients = tape.gradient(loss, variables)\n\n    optimizer.apply_gradients(zip(gradients, variables))\n\n    return batch_loss","9c83029b":"EPOCHS = 10\n\nfor epoch in range(EPOCHS):\n    start = time.time()\n\n    enc_hidden = encoder.initialize_hidden_state()\n    total_loss = 0\n\n    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n        batch_loss = train_step(inp, targ, enc_hidden)\n        total_loss += batch_loss\n\n        if batch % 100 == 0:\n            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n                                                   batch,\n                                                   batch_loss.numpy()))\n    # saving (checkpoint) the model every 2 epochs\n    if (epoch + 1) % 2 == 0:\n        checkpoint.save(file_prefix = checkpoint_prefix)\n\n    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n                                      total_loss \/ steps_per_epoch))\n    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))","7e1555d2":"gc.collect()","ff7b42ed":"# Evaluate Function\n\ndef evaluate(sentence):\n    attention_plot = np.zeros((max_length_targ, max_length_inp))\n\n    sentence = preprocess(sentence)\n\n    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n                                                         maxlen=max_length_inp,\n                                                         padding='post')\n  \n    inputs = tf.convert_to_tensor(inputs)\n\n    result = ''\n\n    hidden = [tf.zeros((1, units))]\n    enc_out, enc_hidden = encoder(inputs, hidden)\n\n    dec_hidden = enc_hidden\n    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n\n    for t in range(max_length_targ):\n        predictions, dec_hidden, attention_weights = decoder(dec_input,\n                                                         dec_hidden,\n                                                         enc_out)\n\n        # storing the attention weights to plot later on\n        attention_weights = tf.reshape(attention_weights, (-1, ))\n        attention_plot[t] = attention_weights.numpy()\n\n        predicted_id = tf.argmax(predictions[0]).numpy()\n\n        result += targ_lang.index_word[predicted_id] + ' '\n\n        if targ_lang.index_word[predicted_id] == '<end>':\n            return result, sentence, attention_plot\n\n        # the predicted ID is fed back into the model\n        dec_input = tf.expand_dims([predicted_id], 0)\n\n    return result, sentence, attention_plot","0bd629d0":"#Function to Translate\ndef translate(sentence):\n    result, sentence, attention_plot = evaluate(sentence)\n\n    print('Input: %s' % (sentence))\n    print('Predicted translation: {}'.format(result))","57611327":"#Translate\ntranslate(\"this is a book\")","cd0c13ca":"# Neural Machine Translation Tutorial (English to Marathi)\n\nIn this notebook, we'll try to perform a neural machine translation using a sequence to sequence (seq2seq) model for English to Hindi translation. The English - Marathi sentence pairs dataset is [here](https:\/\/www.kaggle.com\/kkhandekar\/marathi-english-sentence-pairs).\n\nThis notebook is heavily inspired by the TensorFlow's own [tutorial](https:\/\/www.tensorflow.org\/tutorials\/text\/nmt_with_attention) about Neural Machine Translation.\n\n**Note**: This notebook is intended as a tutorial for Neural Machine Translation\n\n#### Please do consider it to **UPVOTE** if you find it helpful :-)","e7259050":"**Note**: As you may have observed the translation is not very perfect due to the fact that the dataset that we've used is not large and because of that our model is not full trained.\n","7623b421":"### UPDATED - 28-08-2020"}}