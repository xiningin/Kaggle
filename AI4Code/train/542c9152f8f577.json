{"cell_type":{"9ed168a6":"code","e4935cb6":"code","5efc38f9":"code","0b097ea9":"code","26605ec1":"code","063fad03":"code","de6bf548":"code","8a1f96c2":"code","e8378075":"code","f9025785":"code","db57e556":"code","d9f41726":"code","f8626444":"code","5e69f0cc":"code","6edba4a0":"code","d9e667f0":"code","deaa4d1b":"code","448cf897":"markdown","e14116f7":"markdown","fbd7c06e":"markdown","0efa4f58":"markdown","fcd727d9":"markdown","5630c43e":"markdown","9b163d1f":"markdown"},"source":{"9ed168a6":"#Used to make data more uniform across screen.\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:95% !important; }<\/style>\"))","e4935cb6":"#Import packages used here:\n# for initial data exploration:\nimport numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image, display\nimport random\nimport math\n\n#For modeling and model viewing. \nimport tensorflow as tf\nfrom keras.preprocessing.image import load_img,img_to_array,ImageDataGenerator\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.utils import plot_model \nfrom tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Activation,Concatenate\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.utils import to_categorical #Image generator used for transformation to categorical\nfrom tensorflow.keras.optimizers import Adam, SGD\nfrom tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\nfrom tensorflow.keras import backend, models\n#from sklearn.model_selection import train_test_split  #could have used on the consolidated file.\nfrom sklearn.metrics import confusion_matrix\n\nfrom tensorflow.keras.applications import VGG16, MobileNet\n#from keras.applications.vgg16 import decode_predictions\nfrom keras.applications.vgg16 import preprocess_input","5efc38f9":"#This will setup my directories for all of the data files in the 100-bird-species dataset. \nBASE_DIR = '\/kaggle\/input\/100-bird-species'\nprint('BASE_DIR contains ', os.listdir(BASE_DIR))\nTRAIN_DIR = os.path.join(BASE_DIR, 'train')\nVALIDATION_DIR = os.path.join(BASE_DIR, 'valid')\nTEST_DIR = os.path.join(BASE_DIR, 'test')","0b097ea9":"#This will establish the prediction groups for the model.\nCATEGORIES = os.listdir(TRAIN_DIR)\nprint(str(len(CATEGORIES)),'CATEGORIES are ', CATEGORIES)\n\nCategory_count = len(CATEGORIES)\n","26605ec1":"#Load an image and determine image shape for analysis.\nIMAGE = load_img(\"\/kaggle\/input\/100-bird-species\/train\/ANNAS HUMMINGBIRD\/025.jpg\")\nplt.imshow(IMAGE)\nplt.axis(\"off\")\nplt.show()\n\nIMAGEDATA = img_to_array(IMAGE)\nSHAPE = IMAGEDATA.shape\nprint('Figures are ', SHAPE)\n\n\n","063fad03":"#This will be used on training, test, and valid data\nGeneral_datagen = ImageDataGenerator(rescale=1.\/255, )\n","de6bf548":"train_data = General_datagen.flow_from_directory(TRAIN_DIR, target_size=(224,224))\nprint('data groups:', len(train_data)) #Will be used to determine steps_per_epoch in my models.\nTrain_groups = len(train_data)\nvalidation_data = General_datagen.flow_from_directory(VALIDATION_DIR, target_size=(224,224),)\nimage_qty = len(validation_data.filenames)\nprint('data groups:', len(validation_data))\nprint('validation image qty:',str(image_qty))\nValid_groups = len(validation_data)\ntest_data = General_datagen.flow_from_directory(TEST_DIR, target_size=(224,224),)\nprint('data groups:', len(test_data))","8a1f96c2":"#create seperate labels for images \ndef label_images2(DIR, dataset):\n    label = []\n    image = []\n    j=0\n    for i in range (0,30):\n        j = random.randint(0, len(dataset.filenames))\n        label.append(dataset.filenames[j].split('\/')[0])\n        image.append(DIR + '\/' + dataset.filenames[j])\n    return [label,image]\n\n#plot the random images.\ny,x = label_images2(TEST_DIR, test_data)\n\nfor i in range(0,6):\n    X = load_img(x[i])\n    plt.subplot(2,3,+1 + i)\n    plt.axis(False)\n    plt.title(y[i], fontsize=8)\n    plt.imshow(X)\nplt.show()\n","e8378075":"#This was my Sequential model from the CIFAR10 dataset - seemed like a good starting point. -65% accuracy\n#With 2 epochs I got: Test loss: 2.3443613751181243 Test accuracy: 0.4788889\n#With 50 epochs\/stopped at 13 Test loss: 1.7568193797407479, Test accuracy: 0.5733333..Not so great. I will move on to pretrained models.\n#Increased from 32 to 64 nodes in CONV2D layers: Test loss: 4.270853807186258, Test accuracy: 0.5377778\n#Changed from Adam to sgd for optimizer:Test loss: 1.4400342908398858, Test accuracy: 0.65444446 - 65%\nbackend.clear_session()\nmodel = Sequential()\n\nmodel.add(Conv2D(64, (3, 3), padding='same',input_shape=SHAPE)) #224X224\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(64, (3, 3))) #222x222\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2))) #111x111\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.35)) #Doesn't appear to be working in the model summary.\n\nmodel.add(Conv2D(64, (3, 3), padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization()) \n\nmodel.add(Conv2D(64, (3, 3))) #109x109\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2))) #54x54\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.35)) #64 --> 42\n\nmodel.add(Conv2D(64, (3, 3), padding='same')) #54x54\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\n\nmodel.add(Flatten()) \nmodel.add(Dropout(0.5)) \nmodel.add(Dense(512)) \nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(Category_count)) #Updated for number of classes\nmodel.add(Activation('softmax'))\n\nmodel.summary()\n\n#Compile\nmodel.compile(optimizer = tf.keras.optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True),\n               loss = 'categorical_crossentropy',\n               metrics = ['accuracy'])\n#fit model\nhistory = model.fit_generator( \n    train_data, \n    steps_per_epoch = Train_groups, \n    epochs = 50,\n    validation_data = validation_data,\n    validation_steps = Valid_groups,\n    verbose = 1,\n    callbacks=[EarlyStopping(monitor='val_accuracy', patience = 5, restore_best_weights = True),\n               ReduceLROnPlateau(monitor = 'val_loss', factor = 0.7, #0.2 to 0.5 dropped to fast 0.7\n                                 patience = 2, verbose = 1)])","f9025785":"#plot accuracy vs epoch\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot loss values vs epoch\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Evaluate against test data.\nscores = model.evaluate(test_data, verbose=1)\nprint('Test loss:', scores[0])\nprint('Test accuracy:', scores[1])","db57e556":"#Let's try the vgg16 - 86% accuracy\n#Initial run with 2 epochs est loss: 27.959821766820447, Test accuracy: 0.40666667\n# Increased to 50 epochs to Test loss: 26.42553789862271, Test accuracy: 0.79888886 - 80%\n# Added pooling Max to the vgg16 model -78%\n# Removedpooling and add sgd in place of adam optimizer: Test loss: 0.5881293734599804, Test accuracy: 0.8611111\nbackend.clear_session()\n\n\n#Bring in the imagenet dataset training weights for the VGG16 CNN model, remove the classification, the default shape is correct (3,224,224) for my purposes.\nbase_vgg16 = VGG16(weights = 'imagenet', include_top = False, input_shape = SHAPE)\nbase_vgg16.trainable = False # Freeze the VGG16 weights.\n\nmodel = Sequential()\nmodel.add(base_vgg16)\n\nmodel.add(Flatten()) #1024#model.add(Dense(256)) \nmodel.add(Activation('relu'))\nmodel.add(Dense(Category_count)) \nmodel.add(Activation('softmax'))\n\nmodel.summary()\n\n#Compile\nmodel.compile(optimizer = tf.keras.optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True),\n               loss = 'categorical_crossentropy',\n               metrics = ['accuracy'])\nhistory = model.fit_generator( \n    train_data, \n    steps_per_epoch = Train_groups, \n    epochs = 50,\n    validation_data = validation_data,\n    validation_steps = Valid_groups,\n    verbose = 1,\n    callbacks=[EarlyStopping(monitor='val_accuracy', patience = 5, restore_best_weights = True),ReduceLROnPlateau(monitor = 'val_loss', factor = 0.7, #0.2 to 0.5 dropped to fast 0.7\n                                 patience = 2, verbose = 1)])  ","d9f41726":"#plot accuracy vs epoch\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot loss values vs epoch\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Evaluate against test data.\nscores = model.evaluate(test_data, verbose=1)\nprint('Test loss:', scores[0])\nprint('Test accuracy:', scores[1])","f8626444":"#Let's try the mobilenet with ReduceLROnPlateau - 93% accuracy\nbackend.clear_session()\n\n#Bring in the imagenet dataset training weights for the Mobilenet CNN model.\n#Remove the classification top.\nbase_mobilenet = MobileNet(weights = 'imagenet', include_top = False, \n                           input_shape = SHAPE)\nbase_mobilenet.trainable = False # Freeze the mobilenet weights.\n\nmodel = Sequential()\nmodel.add(base_mobilenet)\n\nmodel.add(Flatten()) \nmodel.add(Activation('relu'))\nmodel.add(Dense(Category_count)) \nmodel.add(Activation('softmax'))\n\nmodel.summary()\n\n#Compile\nmodel.compile(optimizer = tf.keras.optimizers.SGD(lr=0.001, \n                                                  momentum=0.9, nesterov=True),\n               loss = 'categorical_crossentropy',\n               metrics = ['accuracy'])\n#fit model\nhistory = model.fit_generator( \n    train_data, \n    steps_per_epoch = Train_groups, \n    epochs = 50,\n    validation_data = validation_data,\n    validation_steps = Valid_groups,\n    verbose = 1,\n    callbacks=[EarlyStopping(monitor = 'val_accuracy', patience = 5, \n                             restore_best_weights = True),\n               ReduceLROnPlateau(monitor = 'val_loss', factor = 0.7, #0.2 to 0.5 dropped to fast 0.7\n                                 patience = 2, verbose = 1)]) \n                # left verbose 1 so I could see the learning rate decay\n\n","5e69f0cc":"#plot accuracy vs epoch\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot loss values vs epoch\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Evaluate against test data.\nscores = model.evaluate(test_data, verbose=1)\nprint('Test loss:', scores[0])\nprint('Test accuracy:', scores[1])","6edba4a0":"#This would only be applied to my training data. looking at the data rotated within 40 degrees would give more data without a change to the mostly vertically sitting birds. \nAugment_datagen = ImageDataGenerator(rescale=1.\/255, rotation_range=40, # Rotate the images randomly by 40 degrees\n    width_shift_range=0.2, # Shift the image horizontally by 20%\n    height_shift_range=0.2, # Shift the image veritcally by 20%\n    zoom_range=0.2, # Zoom in on image by 20% \n    horizontal_flip=True, # Flip image horizontally \n    fill_mode='nearest') \nAugmentation_train = Augment_datagen.flow_from_directory(TRAIN_DIR, target_size=(224,224))\n\nprint('data groups:', len(Augmentation_train)) #Will be used to determine steps_per_epoch in my models.","d9e667f0":"#Let's try the mobilenet with ReduceLROnPlateau with augmentation - 93% accuracy\nbackend.clear_session()\n\n#Bring in the imagenet dataset training weights for the Mobilenet CNN model.\n#Remove the classification top.\nbase_mobilenet = MobileNet(weights = 'imagenet', include_top = False, \n                           input_shape = SHAPE)\nbase_mobilenet.trainable = False # Freeze the mobilenet weights.\n\nmodel = Sequential()\nmodel.add(base_mobilenet)\n\nmodel.add(Flatten()) \nmodel.add(Activation('relu'))\nmodel.add(Dense(Category_count)) \nmodel.add(Activation('softmax'))\n\nmodel.summary()\n\n#Compile\nmodel.compile(optimizer = tf.keras.optimizers.SGD(lr=0.001, \n                                                  momentum=0.9, nesterov=True),\n               loss = 'categorical_crossentropy',\n               metrics = ['accuracy'])\n#fit model\nhistory = model.fit_generator( \n    Augmentation_train, \n    steps_per_epoch = Train_groups, \n    epochs = 50,\n    validation_data = validation_data,\n    validation_steps = Valid_groups,\n    verbose = 1,\n    callbacks=[EarlyStopping(monitor = 'val_accuracy', patience = 5, \n                             restore_best_weights = True),\n               ReduceLROnPlateau(monitor = 'val_loss', factor = 0.7, #0.2 to 0.5 dropped to fast 0.7\n                                 patience = 2, verbose = 1)]) \n                # left verbose 1 so I could see the learning rate decay","deaa4d1b":"#plot accuracy vs epoch\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot loss values vs epoch\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Evaluate against test data.\nscores = model.evaluate(test_data, verbose=1)\nprint('Test loss:', scores[0])\nprint('Test accuracy:', scores[1])","448cf897":"Let's use this model to look at how augmentation might improve the accuracy.","e14116f7":"I choose to load a humming bird image as my favorite bird. I also displayed the shape of the image so that I can use it in my model.","fbd7c06e":"The directories are not direct links to the data so I used the IMAGEDATAGENERATOR in Keras to consolidate the images for each train\/test\/validation set. I left the defaults as follows: batch_size = 32, class_mode = 'categorical', shuffle = TRUE(in flow).","0efa4f58":"An API model is generally used when you have multiple inputs or multiple outputs. This example dataset doesn't appear to have that type of need directly, but I created one to show it is an option.","fcd727d9":"I'll create instances of ImageDataGenerators. One for all of the data being processed and more if I decide to augment my training data.","5630c43e":"So to make certain my new datasets still had images and label seperation, I printed a few more images from the test set.","9b163d1f":"###### Kaggle Project - MSDS686 - Amanda Kimball April 2020\n\n**Overview of the assignment:**\n\nIn this project I will overview the dataset 100-bird-species. I will explore the data, transform it into a format for modeling, and then create classification models to predict the species of birds given a picture of a bird. My models will  \n\n**Description of the data:**\n\nFor this assignment, I choose to create a deep learning CNN model for the classification of 180 bird species (which was continuously increasing so I adjusted the code appropriately). This dataset is located here: https:\/\/www.kaggle.com\/gpiosenka\/100-bird-species. The first step is to add the data at right to your kaggle input. The dataset consists of 2 directories 175 and 180 - (Now changed to a single directory - continuously increasing). Directory 175 is not explored during my analysis, but has the same structure as directory 180. 180 has train, test, valid, consolidated, and a predictor test set. I used the first 3 directories for training my model, validating my model and then a final test of the model. Keeping these 3 datasets seperate is a key part of any good modeling method.  The consolidated file puts all of the photos into one folder, and could verywell be used for a larger trainig set with seperate test\/valid sets or to split the data differently than the original split. Gerry Piosenka (2020) indicates that he compiled the data from photos on the internet, evaluated them so that there are no duplicates, cropped them so that the bird was 50% of the image, and then sized them to the 224x224x3 jpg format. The jpg files are all in thier \nlabel\/species name directory, which you will see that I use to find my favorite bird -- the hummingbird -- early in this tutorial. The author notes a bias in photos of males 80% to females. As an avid bird watcher, I can attest that male birds have better coloring and are easier to speciate. Most female hummingbirds for example are easier to decipher based on bird calls and size. I hypothsize, that a grey-scale evaluation would be better for predicting female birds, but will leave that for future research. \n\n**Summary of Methods:**\n\nFor each model, I used the imagedatagenerator in keras to move the data from the 100-bird-species\/180 directory inot the model. I created each model as described below. Each model is compiled using a batchsize\/samples ratio for the epochs_per_step (on training and validation steps). The compiled models are fit to the generator data for epochs required with patience of 5 (max was 50 but none of the model fit executions went for that long). I graphed the validation and training data loss and accuracy as a function of epochs. Finally, the third dataset - test - was compared to the model to give a true accuracy and loss value for each fit model.\n\n**Summary of Model:**\n\nI present my best from scratch convolution model and the VGG16 model after a few iterations (and MobileNet which performed much better). The different methods attempted are detailed at the top with hash for future investigation. I did find that the SGD optimizer did better than ADAM. \n\n**Analysis of Results:**\n\nThe best accuracy achieved was 86% (94% achieved with MobileNet)."}}