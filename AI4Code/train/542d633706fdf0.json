{"cell_type":{"0d9d003e":"code","9286df19":"code","c62936c9":"code","a02b604e":"code","0b3bb512":"code","57d23c3b":"code","7ae950d7":"code","ad387db0":"code","c99e5178":"code","39e53055":"code","c7031f17":"code","787fbbf8":"code","092f93a1":"code","b7035de8":"code","e5d1aad9":"code","2c610aed":"code","1c2f2e6b":"code","16c16853":"code","4d5fc092":"code","b9fc6403":"code","9c98a951":"code","8e423773":"code","bd0150c5":"code","348f22be":"code","d960f25c":"code","7592bc1d":"code","d30dbcf5":"code","bc58e2ce":"code","463e285b":"code","c3595530":"code","60a32c87":"code","946fa807":"code","18c0d128":"code","35947dde":"code","3a4adf17":"code","90a6c8fc":"code","f35d5b82":"code","e7c090ee":"code","aadf861b":"code","3129dcb9":"code","dd81274b":"code","be44bafe":"code","e2f3be8e":"code","58fac030":"code","c42caa86":"code","61403a60":"code","ee443e90":"code","df24b45f":"code","120eea03":"code","3efa4845":"code","83f7cf85":"code","3277c52b":"code","617094aa":"code","e36e74cc":"markdown","63d8b6c2":"markdown","98620330":"markdown","c4661a64":"markdown","699e4f65":"markdown","c797d745":"markdown","734acf11":"markdown","9870aa54":"markdown","11282313":"markdown","d9becb5c":"markdown","9ac2b711":"markdown","40ac8b3a":"markdown","9da9da34":"markdown","bcb0e583":"markdown","7348972b":"markdown","0db31958":"markdown","c9bf68f1":"markdown","a9a65bd3":"markdown","d40debc3":"markdown","f6c9da90":"markdown","b140d6e0":"markdown","3b4503c7":"markdown","edb8e4bf":"markdown","cbeff39b":"markdown","607401f9":"markdown","b431307c":"markdown"},"source":{"0d9d003e":"# Importing the necessary library\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px \nimport plotly.graph_objs as go\nfrom plotly.offline import iplot\n\nimport nltk\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom wordcloud import WordCloud, STOPWORDS\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,plot_confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom string import punctuation\nfrom bs4 import BeautifulSoup\nimport re,string,unicodedata\nfrom nltk import pos_tag\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import wordnet, stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom keras.models import Sequential\nfrom keras.layers import Dense","9286df19":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nimport collections","c62936c9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a02b604e":"df_train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\ndf_submission = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')","0b3bb512":"df_train.head()","57d23c3b":"df_test.head()","7ae950d7":"df_submission.head()","ad387db0":"df_train.info()","c99e5178":"df_test.info()","39e53055":"df_train['keyword'].unique()","c7031f17":"# Counting the number of unique keywords in trainset\n\ndf_train['keyword'].nunique()","787fbbf8":"# Counting the number of unique keywords in test dataset\n\ndf_test['keyword'].nunique()","092f93a1":"df_train['target'].value_counts()","b7035de8":"labels = df_train['target'].value_counts()[:].index\nvalues = df_train['target'].value_counts()[:].values\n\ncolors=['#2678bf', '#98adbf']\n\nfig = go.Figure(data=[go.Pie(labels = labels, values=values, textinfo=\"label+percent\",\n                            insidetextorientation=\"radial\", marker=dict(colors=colors))])\n\nfig.show()","e5d1aad9":"real = df_train[df_train['target'] == 1]['text']\nreal.values[1:5]","2c610aed":"fake = df_train[df_train['target'] == 0]['text']\nfake.values[1:5]","1c2f2e6b":"# Replacing the ambigious locations name with Standard names\ndf_train['location'].replace({'United States':'USA',\n                           'New York':'USA',\n                            \"London\":'UK',\n                            \"Los Angeles, CA\":'USA',\n                            \"Washington, D.C.\":'USA',\n                            \"California\":'USA',\n                             \"Chicago, IL\":'USA',\n                             \"Chicago\":'USA',\n                            \"New York, NY\":'USA',\n                            \"California, USA\":'USA',\n                            \"FLorida\":'USA',\n                            \"Nigeria\":'Africa',\n                            \"Kenya\":'Africa',\n                            \"Everywhere\":'Worldwide',\n                            \"San Francisco\":'USA',\n                            \"Florida\":'USA',\n                            \"United Kingdom\":'UK',\n                            \"Los Angeles\":'USA',\n                            \"Toronto\":'Canada',\n                            \"San Francisco, CA\":'USA',\n                            \"NYC\":'USA',\n                            \"Seattle\":'USA',\n                            \"Earth\":'Worldwide',\n                            \"Ireland\":'UK',\n                            \"London, England\":'UK',\n                            \"New York City\":'USA',\n                            \"Texas\":'USA',\n                            \"London, UK\":'UK',\n                            \"Atlanta, GA\":'USA',\n                            \"Mumbai\":\"India\"},inplace=True)","16c16853":"bottom_5_location = df_train.sort_values(\"location\", ascending=True).head(5)\n\nfig = px.bar(bottom_5_location,\n            x = \"keyword\",\n            y=\"location\",\n            orientation='v',\n            height=800,\n            title=\"Bottom 5 location \",\n            color=\"keyword\"\n            )\n\nfig.show()","4d5fc092":"labels = df_train[df_train['target'] == 1]['keyword'].value_counts()[:10].index\nvalues = df_train[df_train['target'] == 1]['keyword'].value_counts()[:10].values\n\ncolors = df_train['keyword']\n\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, textinfo='label+percent', \n                             insidetextorientation='radial', marker=dict(colors=colors))])\nfig.show()","b9fc6403":"labels = df_train[df_train['target'] == 0]['keyword'].value_counts()[:10].index\nvalues = df_train[df_train['target'] == 0]['keyword'].value_counts()[:10].values\n\ncolors = df_train['keyword']\n\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, textinfo='label+percent', \n                             insidetextorientation='radial', marker=dict(colors=colors))])\nfig.show()","9c98a951":"labels = df_train['location'].value_counts()[:10].index\nvalues = df_train['location'].value_counts()[:10].values\n\ncolors = df_train['location']\n\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, textinfo='label+percent', \n                             insidetextorientation='radial', marker=dict(colors=colors))])\nfig.show()","8e423773":"df_train['id'].nunique()","bd0150c5":"sns.barplot(y=df_train['location'].value_counts()[:5].index, x=df_train['location'].value_counts()[:5], orient='h')","348f22be":"# Let's have a look at both the trainig and test set data\ndf_train['text'][:5]","d960f25c":"df_test['text'][:5]","7592bc1d":"def clean_text(text):\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n    ","d30dbcf5":"# Now applying clean_text function to both train and test datasets\n\ndf_train['text'] = df_train['text'].apply(lambda x: clean_text(x))\ndf_test['text'] = df_test['text'].apply(lambda x: clean_text(x))","bc58e2ce":"# Let's see how has been our train and test datasets have been changed after applying the clean_text function\n\ndf_train['text'].head()","463e285b":"df_test['text'].head()","c3595530":"\nword_cloud = WordCloud(\n                       width=1600,\n                       height=800,\n                       #colormap='PuRd', \n                       margin=0,\n                       max_words=500, # Maximum numbers of words we want to see \n                       min_word_length=3, # Minimum numbers of letters of each word to be part of the cloud\n                       max_font_size=150, min_font_size=30,  # Font size range\n                       background_color=\"white\").generate(\" \".join(real))\n\nplt.figure(figsize=(10, 16))\nplt.imshow(word_cloud, interpolation=\"gaussian\")\nplt.title('Real Tweets mentioning about Disaster', fontsize = 40)\nplt.axis(\"off\")\nplt.show()","60a32c87":"word_cloud = WordCloud(\n                       width=1600,\n                       height=800,\n                       colormap='PuRd', \n                       margin=0,\n                       max_words=500, # Maximum numbers of words we want to see \n                       min_word_length=3, # Minimum numbers of letters of each word to be part of the cloud\n                       max_font_size=150, min_font_size=30,  # Font size range\n                       background_color=\"white\").generate(\" \".join(fake))\n\nplt.figure(figsize=(10, 16))\nplt.imshow(word_cloud, interpolation=\"gaussian\")\nplt.title('Fake Tweets mentioning about Disaster', fontsize = 40)\nplt.axis(\"off\")\nplt.show()","946fa807":"print()\ntext = \"I love you, don't you\"\n\n# instantiate tokenizer class\ntokenizer1 = nltk.tokenize.WhitespaceTokenizer()\ntokenizer2 = nltk.tokenize.TreebankWordTokenizer()\ntokenizer3 = nltk.tokenize.WordPunctTokenizer()\ntokenizer4 = nltk.tokenize.RegexpTokenizer(r'\\w+')\n\nprint(\"Example Text: \", text)\nprint(\"Tokenization by whitespace: \", tokenizer1.tokenize(text))\nprint(\"Tokenization by words using Treebank Word Tokenizer: \", tokenizer2.tokenize(text))\nprint(\"Tokenization by punctuation: \", tokenizer3.tokenize(text))\nprint(\"Tokenization by regular expression: \", tokenizer4.tokenize(text))","18c0d128":"# Tokenizing the training and the test set\n\n# instantiate tokenizer class\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n\n# Tokenizing the trainig set\ndf_train['text'] = df_train['text'].apply(lambda x: tokenizer.tokenize(x))\ndf_test['text'] = df_test['text'].apply(lambda x: tokenizer.tokenize(x))","35947dde":"print()\nprint('Tokenized string:')\ndf_train['text'].head()","3a4adf17":"print()\nprint('Tokenized string:')\ndf_test['text'].head()","90a6c8fc":"# Definig a function to remove the stopwords\n\ndef remove_stopwords(text):\n    \n    words = [word for word in text if word not in stopwords.words('english')]\n    return words","f35d5b82":"# Removing the stopwords from the train and test set\n\ndf_train['text'] = df_train['text'].apply(lambda x: remove_stopwords(x))\ndf_test['train'] = df_test['text'].apply(lambda x: remove_stopwords(x))","e7c090ee":"df_train.head()","aadf861b":"df_test.head()","3129dcb9":"# Stemming and Lemmatization examples\n\ntext = \"How is the Josh\"\n\ntokenizer = nltk.tokenize.TreebankWordTokenizer()\ntokens = tokenizer.tokenize(text)\n\n# Stemmer \nstemmer = nltk.stem.PorterStemmer()\nprint(\"Stemming the sentence: \", \" \".join(stemmer.stem(token) for token in tokens))\n\n# Lemmatizer\nlemmatizer = nltk.stem.WordNetLemmatizer()\nprint(\"Lemmatizing the sentence: \", \" \".join(lemmatizer.lemmatize(token) for token in tokens))","dd81274b":"# After preprocessing, the text format\ndef combine_text(list_of_text):\n    \n    combined_text = ' '.join(list_of_text)\n    return combined_text\n\ndf_train['text'] = df_train['text'].apply(lambda x : combine_text(x))\ndf_test['text'] = df_test['text'].apply(lambda x : combine_text(x))\ndf_train['text']\ndf_train.head()","be44bafe":"# text preprocessing function\ndef text_preprocessing(text):\n   \n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    \n    nopunc = clean_text(text)\n    tokenized_text = tokenizer.tokenize(nopunc)\n    remove_stopwords = [word for word in tokenized_text if word not in stopwords.words('english')]\n    combined_text = ' '.join(remove_stopwords)\n    return combined_text\n","e2f3be8e":"# CountVectorizer can do all the above task of preprocessing, tokenization, and stop words removal\ncount_vectorizer = CountVectorizer()\ntrain_vectors = count_vectorizer.fit_transform(df_train['text'])\ntest_vectors = count_vectorizer.transform(df_test['text'])\n    \n    \n# Keeping only non-zero elements to preserve spaces\nprint(train_vectors[0].todense())","58fac030":"tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\ntrain_tfidf = tfidf.fit_transform(df_train['text'])\ntest_tfidf = tfidf.transform(df_test['text'])","c42caa86":"# Let's implement simple classifiers\n\nclassifiers = {\n    \"LogisiticRegression\": LogisticRegression(),\n    \"KNearest\": KNeighborsClassifier(),\n    \"Support Vector Classifier\": SVC(),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n    \"MultinimialNB\": MultinomialNB()\n}","61403a60":"# Wow our scores are getting even high scores even when applying cross validation.\n# Lets apply the Classifiers 1st on Countvectoizers\nfrom sklearn.model_selection import cross_val_score\n\n\nfor key, classifier in classifiers.items():\n    classifier.fit(train_vectors, df_train[\"target\"])\n    training_score = cross_val_score(classifier, train_vectors, df_train[\"target\"], cv=5)\n    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")","ee443e90":"# Lets apply the Classifiers tfidf\nfrom sklearn.model_selection import cross_val_score\n\n\nfor key, classifier in classifiers.items():\n    classifier.fit(train_tfidf, df_train[\"target\"])\n    training_score = cross_val_score(classifier, train_tfidf, df_train[\"target\"], cv=5, scoring=\"f1\")\n    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")","df24b45f":"# Fitting a simple Naive Bayes on TFIDF\nfrom sklearn import model_selection\nclf_NB_TFIDF = MultinomialNB()\nscores = model_selection.cross_val_score(clf_NB_TFIDF, train_tfidf, df_train[\"target\"], cv=5, scoring=\"f1\")\nscores","120eea03":"clf_NB_TFIDF.fit(train_tfidf, df_train[\"target\"])","3efa4845":"import xgboost as xgb\nfrom sklearn import model_selection\nclf_xgb = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                           subsample=0.8, nthread=10, learning_rate=0.01)\n\nscores = model_selection.cross_val_score(clf_xgb, train_vectors, df_train[\"target\"], cv=5, scoring=\"f1\")","83f7cf85":"import xgboost as xgb\nclf_xgb_TFIDF = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nscores = model_selection.cross_val_score(clf_xgb_TFIDF, train_tfidf, df_train[\"target\"], cv=5, scoring=\"f1\")\nscores","3277c52b":"def submission(submission_file_path,model,test_vectors):\n    sample_submission = pd.read_csv(submission_file_path)\n    sample_submission[\"target\"] = model.predict(test_vectors)\n    sample_submission.to_csv(\"submission.csv\", index=False)","617094aa":"submission_file_path = \"..\/input\/nlp-getting-started\/sample_submission.csv\"\ntest_vectors=test_tfidf\nsubmission(submission_file_path,clf_NB_TFIDF,test_vectors)\n","e36e74cc":"Using XGBoost on TFIDF","63d8b6c2":"If you found this notebook helpful please upvote it","98620330":"### Exploring the 'location' column","c4661a64":"A lot of missing words in columns 'keyowrd' and 'location'","699e4f65":"Importing Libraries","c797d745":"# Normalizing the Tokens","734acf11":"We can notice from the above 2 outputs all the unnecessary things of the train and test 'text' have been removed","9870aa54":"Above one give output as top 10 commonly used key words during the time of Disaster","11282313":"The next step is to remove stop words. Stop words are words that don't add significant meaning to the text.","d9becb5c":"Above one give output as top 10 commonly used key words during the time of Disaster","9ac2b711":"# Data Preprocessing","40ac8b3a":"Data preprocessing is one of the critical steps in any machine learning project. It includes cleaning and formatting the data before feeding into a machine learning algorithm. For NLP, the preprocessing steps are comprised of the following tasks:\n\n* Tokenizing the string\n* Lowercasing\n* Removing stop words and punctuation\n* Stemming\n* Lemmatization","9da9da34":"By the above plot we may say that these are the locations with maximum number of chances of disasterous and non-disastrous tweets","bcb0e583":"Above one is the plot of location with particular type of keywords used at that place ","7348972b":"All the above tweets mentioned about the disaster tweet","0db31958":"# XGBoost","c9bf68f1":"All the above tweets have no informations about disaster","a9a65bd3":"From the above 2 classification output on countvectorizer and tfidf its clear that counvectorizer is performing much better than the tfidf","d40debc3":"So we can see that there are 4342 disaster tweets and 3271 Non-disaster tweets in train dataset","f6c9da90":"# Building the Final Classification Model\n\nSince all the cleaning and preprocessing has been done so the data is ready to fed up in the model for classification","b140d6e0":"Defining a function to clean text ","3b4503c7":"Using XGBoost on CountVectorizers","edb8e4bf":"Now lets see how our Real and Fake tweets looks like in our dataset","cbeff39b":"Plotting the WordCloud","607401f9":"# Transforming tokens to a Vector","b431307c":"A lot of missing words in columns 'keyowrd' and 'location'"}}