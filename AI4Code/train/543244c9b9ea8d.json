{"cell_type":{"fd39217a":"code","ae95a9ab":"code","30b80577":"code","ae406b7a":"code","8f34434a":"code","8c68c268":"code","d8a86f51":"code","ecd3c6ad":"code","58ff7b14":"code","73d5ae07":"code","56d5dbaa":"code","5e5f2098":"code","70a7eaf5":"code","2a4aeb24":"code","fc16e57a":"code","a47d6366":"code","1c51105b":"code","aabc4c39":"code","e0f5339c":"code","2fa058f7":"code","1c926775":"code","91680d58":"code","7a3ee284":"code","54d0bd5d":"code","15b56571":"code","e9edc0b6":"code","afeac320":"code","679d9577":"code","4acd530b":"code","11de0df9":"code","a3623b41":"code","405667b8":"code","1caadfe1":"code","68fc6e10":"code","ecb072a7":"code","3fb0e70a":"markdown","9dfe313c":"markdown","2b2e0abd":"markdown","6341ac45":"markdown","ece40672":"markdown","a9f0b21c":"markdown","90559ff4":"markdown","41cd5730":"markdown","f9d027ba":"markdown","fcc8fea9":"markdown","d992cf2d":"markdown","be8570ed":"markdown","09ef1a64":"markdown","bb050b8e":"markdown"},"source":{"fd39217a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport warnings # We don't see any error of version\nwarnings.filterwarnings(\"ignore\")","ae95a9ab":"dtypes = {\n        'year'          : 'int16',\n        'mileage'       : 'uint16',\n        'vol_engine'    : 'uint16',\n        'price'         : 'uint32',\n        }\ndf = pd.read_csv(\"..\/input\/car-prices-poland\/Car_Prices_Poland_Kaggle.csv\", dtype = dtypes).drop(columns=[\"Unnamed: 0\" ,\"generation_name\" ])","30b80577":"df.shape # Looking at data shape","ae406b7a":"df.head() # Looking at data's first 5 row ","8f34434a":"df.info() # Getting information about data. ","8c68c268":"df.isnull().sum() # we don't have any null values","d8a86f51":"categorical = df.columns[df.dtypes == \"object\"]\nnumerical = df.columns[df.dtypes != \"object\"]","ecd3c6ad":"df.corr().style.background_gradient(cmap='coolwarm')","58ff7b14":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfig = plt.figure(figsize=(20,20))\n\ngr = fig.add_gridspec(2,2)\n\nax0 = fig.add_subplot(gr[0,0])\nax1 = fig.add_subplot(gr[0,1])\nax2 = fig.add_subplot(gr[1,0])\nax3 = fig.add_subplot(gr[1,1])\naxxes = [ax0,ax1,ax2,ax3]\n\nfor i in range(0,len(numerical)):\n    axxes[i].grid(color='#000000', linestyle=':', axis='y')\n    sns.boxenplot(ax=axxes[i],y=df[numerical[i]],palette=[\"#\"+str(i*2)+\"53000\"],width=0.6)","73d5ae07":"_ = [print(i,\"= \",len(df[i].unique())) for i in categorical]; # how many of the categorical data are unique","56d5dbaa":"n_cat = [\"mark\",\"fuel\",\"province\"] # We visualize some data. We don't visualize \"model and city\" because has too many unique values","5e5f2098":"fig = plt.figure(figsize=(20,20))\n\ngr = fig.add_gridspec(3,1)\n\nax0 = fig.add_subplot(gr[0,0])\nax1 = fig.add_subplot(gr[1,0])\nax2 = fig.add_subplot(gr[2,0])\n\naxxes = [ax0,ax1,ax2]\n\nfor i in range(0,len(n_cat)):\n    axxes[i].grid(color='#000000', linestyle=':', axis='y')\n    sns.countplot(ax=axxes[i],data=df,x=df[n_cat[i]],palette=[\"#\"+str(i)+str(i)+\"5353\"]) #so that the figures change colors in each cycle\n#NOT: if str(i) is greater than 9 it will throw an error, be careful ","70a7eaf5":"for i in categorical: # All unique categorical values.\n    unique, counts = np.unique(df[i], return_counts=True)\n    print(i,\"= \",counts)","2a4aeb24":"unique, counts = np.unique(df[\"fuel\"], return_counts=True)\nprint(unique, counts)\ndf.drop(df[df[\"fuel\"] == unique[0]].index,axis = 0,inplace = True)\nunique, counts = np.unique(df[\"fuel\"], return_counts=True)\nprint(unique,counts)","fc16e57a":"unique, counts = np.unique(df[\"province\"], return_counts=True)\nprint(unique, counts)\nliste = [0,1,8,9,10,16]\nfor i in liste:\n    df.drop(df[df[\"province\"] == unique[i]].index,axis = 0,inplace = True)\nunique, counts = np.unique(df[\"province\"], return_counts=True)\nprint(unique,counts)","a47d6366":"_ = [print(i,\"= \",len(df[i].unique())) for i in categorical]","1c51105b":"unique, counts = np.unique(df[\"mark\"], return_counts=True)\ncounts","aabc4c39":"unique, counts = np.unique(df[\"province\"], return_counts=True)\ncounts","e0f5339c":"unique, counts = np.unique(df[\"fuel\"], return_counts=True)\ncounts","2fa058f7":"fig = plt.figure(figsize=(20,20))\n\ngr = fig.add_gridspec(3,1)\n\nax0 = fig.add_subplot(gr[0,0])\nax1 = fig.add_subplot(gr[1,0])\nax2 = fig.add_subplot(gr[2,0])\n\naxxes = [ax0,ax1,ax2]\n\nfor i in range(0,len(n_cat)):\n    axxes[i].grid(color='#000000', linestyle=':', axis='y')\n    sns.countplot(ax=axxes[i],data=df,x=df[n_cat[i]],palette=[\"#\"+str(i)+\"f9ffb\"]) #so that the figures change colors in each cycle\n#NOT: if str(i) is greater than 9 it will throw an error, be careful ","1c926775":"df = pd.get_dummies(df) # One-hot encoding","91680d58":"df.shape","7a3ee284":"df.columns","54d0bd5d":"y = pd.DataFrame(df.price) # Target\nx = df.drop(columns=[\"price\"]) # data needed for regression","15b56571":"import gc # We get rid of unnecessary variables. We do this for RAM.\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.15, random_state = 53)\n\ndel df,x,y\ngc.collect()","e9edc0b6":"x_train.shape, x_test.shape, y_train.shape, y_test.shape # Looking shape","afeac320":"from sklearn.metrics import mean_squared_error, mean_absolute_error # to calculate error","679d9577":"acc_df = pd.DataFrame(columns=[\"Name\", \"MAE\", \"MSE\", \"RMSE\", \"R2\"]) # to evaluate the models together","4acd530b":"from sklearn.tree import DecisionTreeRegressor\n\nmodel = DecisionTreeRegressor(max_depth=5).fit(x_train, y_train)\ny_pred_DT = pd.DataFrame(model.predict(x_test))\n\nmae =  mean_absolute_error(y_test , y_pred_DT)\nmse = mean_squared_error(y_test, y_pred_DT)\nrmse = np.sqrt(mse)\nr2 = model.score(x_test,y_test)\n\nprint(\"The test MAE error is=\", mae)\nprint(\"The test MSE error is=\", mse)\nprint(\"The test RMSE error is=\",rmse)\nprint(\"The test r2_score is=\",r2)\n\na_series = pd.Series([\"DecisionTreeRegressor\", int(mae), int(mse), int(rmse), r2], index = acc_df.columns)\nacc_df = acc_df.append(a_series, ignore_index=True)","11de0df9":"from sklearn import linear_model\n\nmodel = linear_model.Lasso(alpha=0.2).fit(x_train, y_train)\ny_pred_las = pd.DataFrame(model.predict(x_test))\n\nmae =  mean_absolute_error(y_test , y_pred_las)\nmse = mean_squared_error(y_test, y_pred_las)\nrmse = np.sqrt(mse)\nr2 = model.score(x_test,y_test)\n\nprint(\"The test MAE error is=\", mae)\nprint(\"The test MSE error is=\", mse)\nprint(\"The test RMSE error is=\",rmse)\nprint(\"The test r2_score error is=\",r2)\n\na_series = pd.Series([\"LassoRegressor\",int(mae), int(mse), int(rmse), r2], index = acc_df.columns)\nacc_df = acc_df.append(a_series, ignore_index=True)","a3623b41":"from sklearn.ensemble import RandomForestRegressor\n\nmodel = RandomForestRegressor(max_depth=9, random_state=53).fit(x_train, y_train)\ny_pred_RF = pd.DataFrame(model.predict(x_test))\n\nmae =  mean_absolute_error(y_test , y_pred_RF)\nmse = mean_squared_error(y_test, y_pred_RF)\nrmse = np.sqrt(mse)\nr2 = model.score(x_test,y_test)\n\nprint(\"The test MAE error is=\", mae)\nprint(\"The test MSE error is=\", mse)\nprint(\"The test RMSE error is=\",rmse)\nprint(\"The test r2_score error is=\",r2)\n\na_series = pd.Series([\"RandomForestRegressor\", int(mae), int(mse), int(rmse), r2], index = acc_df.columns)\nacc_df = acc_df.append(a_series, ignore_index=True)","405667b8":"from sklearn.linear_model import Ridge\n\nmodel = Ridge(alpha=1.0).fit(x_train, y_train)\ny_pred_rid = pd.DataFrame(model.predict(x_test))\n\nmae =  mean_absolute_error(y_test , y_pred_rid)\nmse = mean_squared_error(y_test, y_pred_rid)\nrmse = np.sqrt(mse)\nr2 = model.score(x_test,y_test)\n\nprint(\"The test MAE error is=\", mae)\nprint(\"The test MSE error is=\", mse)\nprint(\"The test RMSE error is=\",rmse)\nprint(\"The test r2_score error is=\",r2)\n\na_series = pd.Series([\"Ridge\", int(mae), int(mse), int(rmse), r2], index = acc_df.columns)\nacc_df = acc_df.append(a_series, ignore_index=True)","1caadfe1":"from sklearn.neighbors import KNeighborsRegressor\n\nmodel = KNeighborsRegressor(n_neighbors=5).fit(x_train, y_train)\ny_pred_KNN = pd.DataFrame(model.predict(x_test))\n\nmae =  mean_absolute_error(y_test , y_pred_KNN)\nmse = mean_squared_error(y_test, y_pred_KNN)\nrmse = np.sqrt(mse)\nr2 = model.score(x_test,y_test)\n\nprint(\"The test MAE error is=\", mae)\nprint(\"The test MSE error is=\", mse)\nprint(\"The test RMSE error is=\",rmse)\nprint(\"The test r2_score error is=\",r2)\n\na_series = pd.Series([\"KNeighborsRegressor\", int(mae), int(mse), int(rmse), r2], index = acc_df.columns)\nacc_df = acc_df.append(a_series, ignore_index=True)","68fc6e10":"plt.figure(figsize=(20, 15), layout='constrained')\nplt.plot(np.arange(0,100), y_pred_DT.to_numpy()[0:100], label='DecisionTree')  # Plot some data on the (implicit) axes.\nplt.plot(np.arange(0,100), y_pred_las.to_numpy()[0:100], label='Lasso')\nplt.plot(np.arange(0,100), y_pred_RF.to_numpy()[0:100], label='RandomForest') \nplt.plot(np.arange(0,100), y_pred_rid.to_numpy()[0:100], label='Ridge') \nplt.plot(np.arange(0,100), y_pred_KNN.to_numpy()[0:100], label='KNN') \nplt.plot(np.arange(0,100), y_test.to_numpy()[0:100], label='RealData') \nplt.xlabel('Number of Data')\nplt.ylabel('Predictions')\nplt.title(\"Information about prediction of models first 100 values\")\nplt.legend();","ecb072a7":"acc_df","3fb0e70a":"# **INFORMATION ABOUT DATA**","9dfe313c":"# Now we will learn to discrete data and drop them. ","2b2e0abd":"As we can see random forest is gooder than them. If there is a place I missed or something you do not understand, please let me know. If you have different thoughts, share with me.","6341ac45":"# **IMPORTING LIBRARIES AND INFORMATIONS ABOUT DATA**","ece40672":"# **Data Visualization**","a9f0b21c":"# Numerical values is looking good. We don't touch them.","90559ff4":"![lambo](https:\/\/www.teahub.io\/photos\/full\/37-372221_images-cool-cars-new-car-models-2019-lamborghini.jpg)","41cd5730":"# **\"They're driving right by, they don't even know what they're missing.\"**","f9d027ba":"# **Data Processing and Modelling**","fcc8fea9":"we will delete some outlier data in \"fuel\" and \"province\".\n\ninitial value of fuel ie 47\n\nWe delete the values in 0,1,8,9,10,16 for the province.","d992cf2d":"we know some data types. We set the data types of this data to be the smallest to use less RAM.\n\nint8 = 0-127 bit\n\nuint16 = (-32768)-32767\n\nuint32 = (\u20132147483648)-2147483647","be8570ed":"**Mark:** Brand of car. (string)\n\n**Model:** Model of car. (string)\n\n**Year:** Manufacture date of the car. (int)\n\n**Mileage:** The distance traveled by the car in miles.\n\n**Vol Engine:** Engine volume of car. The high price affects the price positively. (int)\n\n**Fuel:** Fuel type of the car. (string)\n\n**City:** City where the car was sold. (string)\n\n**Province:** Province where the car was sold. (string)\n\n**Price:** Selling price of the car.(int) **(TARGET)**","09ef1a64":"# **Conclusion**","bb050b8e":"# **I tried to explain as best I could. If you've come this far and liked this review, don't forget to hit the arrow in the upper right corner. Hope to see you in different reviews.**"}}