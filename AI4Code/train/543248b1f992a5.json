{"cell_type":{"f7a01a8a":"code","34b3c7a4":"code","6262412a":"code","e2c32c86":"code","8ac3f1c6":"code","e234169a":"code","726f30d5":"code","4c502b93":"code","7100bb1a":"code","71c82ad1":"code","bae74cc0":"code","7a278efa":"code","d4355c38":"code","c4fea971":"code","d1059937":"code","d7652f6a":"code","a3352450":"code","03d8669d":"code","911ff126":"code","1ba3f9b5":"code","f69812e9":"code","8073c30f":"code","e911d979":"code","f4499b73":"code","1da4abb4":"markdown","298dcdf3":"markdown","d1897859":"markdown","7cfefcbb":"markdown","8200ec25":"markdown","32447378":"markdown","71a3990c":"markdown","41323dbf":"markdown","2d0a4d81":"markdown","fc00299a":"markdown"},"source":{"f7a01a8a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import GridSearchCV, train_test_split, KFold \nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.impute import SimpleImputer\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import linear_model, metrics\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport xgboost\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","34b3c7a4":"data=pd.read_csv(\"..\/input\/kc_house_data.csv\")\ndata.head()","6262412a":"\nx=[\"date\",\"price\",\"bedrooms\",\"bathrooms\",\"sqft_living\",\"sqft_lot\",\"floors\",\"waterfront\",\"view\",\"condition\",\"grade\",\"sqft_above\",\"sqft_basement\",\"yr_built\",\"yr_renovated\",\"zipcode\",\"lat\",\"long\",\"sqft_living15\",\"sqft_lot15\"]\n# Cleaning Data\ndf = pd.DataFrame(data, columns=x)\ndf['year'] = pd.DatetimeIndex(df['date']).year\ndf=df.drop(['date'],axis=1)\nmy_imputer = SimpleImputer()\ndf = pd.DataFrame(my_imputer.fit_transform(df))\ndf.columns=[\"price\",\"bedrooms\",\"bathrooms\",\"sqft_living\",\"sqft_lot\",\"floors\",\"waterfront\",\"view\",\"condition\",\"grade\",\"sqft_above\",\"sqft_basement\",\"yr_built\",\"yr_renovated\",\"zipcode\",\"lat\",\"long\",\"sqft_living15\",\"sqft_lot15\",\"year\"]\ndf.describe()","e2c32c86":"df.plot.scatter(x='price',y='grade',c='DarkBlue')","8ac3f1c6":"df.plot.scatter(x='price',y='bedrooms',c='DarkBlue')","e234169a":"df['bedrooms'].value_counts().sort_index().plot.bar()","726f30d5":"df = df[df.bedrooms != 33]\ndf.plot.scatter(x='price',y='bedrooms',c='DarkBlue')","4c502b93":"sns.set(style=\"white\")\ncorr=df.corr()\ncorr","7100bb1a":"mask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","71c82ad1":"y=df.price\ndf=df.drop(['price'],axis=1)\nX_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.33, random_state=42)","bae74cc0":"df['view'].value_counts()","7a278efa":"df['floors'].value_counts()","d4355c38":"df['bathrooms'].value_counts()","c4fea971":"plt.scatter(df['bathrooms'],y)","d1059937":"\nregr = linear_model.LinearRegression()\n\n# Train the model using the training sets\nregr.fit(X_train, y_train)\n\n# Make predictions using the testing set\ny_pred = regr.predict(X_test)\nprint('Coefficients: \\n', regr.coef_)\n# The mean squared error\nprint('Mean Absolute Error',metrics.mean_absolute_error(y_test, y_pred))\nprint('Mean Squared Error',metrics.mean_squared_error(y_test, y_pred))\nprint('Root Mean Squared Error',np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n# Explained variance score: 1 is perfect prediction\nprint('Variance score: %.2f' % r2_score(y_test, y_pred))","d7652f6a":"residuals = y_test - y_pred \nplt.scatter(residuals,y_test)","a3352450":"from sklearn.tree import DecisionTreeRegressor\nregr_1 = DecisionTreeRegressor(max_depth=df.shape[0]-1)\nregr_1.fit(X_train, y_train)\ny_1 = regr_1.predict(X_test)\n","03d8669d":"print('Mean Absolute Error',metrics.mean_absolute_error(y_test, y_1))\nprint('Mean Squared Error',metrics.mean_squared_error(y_test, y_1))\nprint('Root Mean Squared Error',np.sqrt(metrics.mean_squared_error(y_test, y_1)))\n# Explained variance score: 1 is perfect prediction\nprint('Variance score: %.2f' % r2_score(y_test, y_1))","911ff126":"xgb = xgboost.XGBRegressor(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75,colsample_bytree=1, max_depth=7)\nmodel=xgb.fit(X_train,y_train)\npredicted = model.predict(X_test)","1ba3f9b5":"print('Mean Absolute Error',metrics.mean_absolute_error(y_test, predicted))\nprint('Mean Squared Error',metrics.mean_squared_error(y_test, predicted))\nprint('Root Mean Squared Error',np.sqrt(metrics.mean_squared_error(y_test, predicted)))\n# Explained variance score: 1 is perfect prediction\nprint('Variance score: %.2f' % r2_score(y_test, predicted))","f69812e9":"scores = cross_val_score(model,df, y, cv=10)\nscores.mean()","8073c30f":"xgb1 = XGBRegressor()\nparameters = {'nthread':[4],\n              'objective':['reg:linear'],\n              'learning_rate': [.07], \n              'max_depth': [5],\n              'min_child_weight': [4],\n              'silent': [1],\n              'subsample': [0.7],\n              'colsample_bytree': [0.7],\n              'n_estimators': [500]}\n\nxgb_grid = GridSearchCV(xgb1,\n                        parameters,\n                        cv = 2,\n                        n_jobs = 5,\n                        verbose=True)\nxgb_grid.fit(X_train,y_train)\n","e911d979":"predicted = xgb_grid.predict(X_test)\nprint('Mean Absolute Error',metrics.mean_absolute_error(y_test, predicted))\nprint('Mean Squared Error',metrics.mean_squared_error(y_test, predicted))\nprint('Root Mean Squared Error',np.sqrt(metrics.mean_squared_error(y_test, predicted)))\nprint('Variance score: %.2f' % r2_score(y_test, predicted))","f4499b73":"scores = cross_val_score(xgb_grid,df, y, cv=10)\nscores.mean()","1da4abb4":"**House Sales in King County, USA**\n\nPredicting house prices using king county usa dataset!","298dcdf3":"**Cross validation score for model 3**","d1897859":"**EDA**","7cfefcbb":"**CLEANING DATA**","8200ec25":"Since no column of string catergorial type we dont need to use one hot encoding\n","32447378":"**Model 2**","71a3990c":"**1) Base Model - Linear Regression**","41323dbf":"We can fit data into decission trees inorder to reduce rmse","2d0a4d81":"**Model 3**","fc00299a":"**There you go you can see outliers! lets get rid of them**"}}