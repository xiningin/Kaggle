{"cell_type":{"537c5bf2":"code","615870b0":"code","beadd697":"code","485eeee6":"code","99dcc677":"code","a019344c":"code","c697af6d":"code","c879ad8a":"code","95ad9668":"code","4357f333":"code","477067d9":"code","795ea70f":"code","a8bdc9c5":"code","ab7088fa":"code","f835c3bc":"code","675998b5":"code","36f00c68":"code","7f1a82b7":"code","b6b1b28e":"markdown","a3ac6a66":"markdown","c2eec3c5":"markdown","57dfe5ae":"markdown","3ede4c21":"markdown"},"source":{"537c5bf2":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nfrom skopt import BayesSearchCV\nfrom bayes_opt import BayesianOptimization\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import StandardScaler","615870b0":"train = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")","beadd697":"train","485eeee6":"train.isnull().sum()","99dcc677":"# calculating correleation of each column with target class\nx = train.corrwith(train[\"Class\"]).to_dict()","a019344c":"x","c697af6d":"del x['Class']","c879ad8a":"# choosing features which have a absolute correlation value greater than 0.1\nfeatures = []\nfor k,v in x.items():\n    if abs(v)>0.1:\n        print(f\"{k} : {v:.2f}\")\n        features.append(k)","95ad9668":"x_train = train[features]\ny_train = train['Class']","4357f333":"y_train.value_counts()","477067d9":"# scaling the dataset\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)","795ea70f":"X_train,X_val,Y_train,Y_val = train_test_split(x_train,y_train,test_size = 0.1,random_state=26,stratify=y_train)","a8bdc9c5":"%%time\ndef bayes_parameter_opt_lgb(X, y, init_round=15, opt_round=25, n_folds=5, random_seed=6,n_estimators=10000, output_process=False):\n    # prepare data\n    train_data = lgb.Dataset(data=X, label=y, free_raw_data=False)\n    # parameters\n    def lgb_eval(learning_rate,num_leaves, feature_fraction, bagging_fraction, max_depth, max_bin, min_data_in_leaf,min_sum_hessian_in_leaf,subsample):\n        params = {'application':'binary', 'metric':'auc'}\n        params['learning_rate'] = max(min(learning_rate, 1), 0)\n        params[\"num_leaves\"] = int(round(num_leaves))\n        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n        params['max_depth'] = int(round(max_depth))\n        params['max_bin'] = int(round(max_depth))\n        params['min_data_in_leaf'] = int(round(min_data_in_leaf))\n        params['min_sum_hessian_in_leaf'] = min_sum_hessian_in_leaf\n        params['subsample'] = max(min(subsample, 1), 0)\n                \n        cv_result = lgb.cv(params, train_data, nfold=n_folds, seed=random_seed, stratified=True, verbose_eval =200, metrics=['auc'])\n        return max(cv_result['auc-mean'])\n     \n    lgbBO = BayesianOptimization(lgb_eval, {'learning_rate': (0.001, 0.2),\n                                            'num_leaves': (25, 60),\n                                            'feature_fraction': (0.1, 1),\n                                            'bagging_fraction': (0.5, 1),\n                                           'max_depth': (2, 20),\n                                            'max_bin':(20,90),\n                                            'min_data_in_leaf': (20, 80),\n                                            'min_sum_hessian_in_leaf':(0,100),\n                                           'subsample': (0.01, 1.0)}, random_state=200)\n\n\n    lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n    \n    model_auc=[]\n    for model in range(len( lgbBO.res)):\n        model_auc.append(lgbBO.res[model]['target'])\n    \n    # return best parameters\n    return lgbBO.res[pd.Series(model_auc).idxmax()]['target'],lgbBO.res[pd.Series(model_auc).idxmax()]['params']\n\nopt_params = bayes_parameter_opt_lgb(X_train, Y_train, init_round=5, opt_round=10, n_folds=5, random_seed=6,n_estimators=10000)","ab7088fa":"# optimal parameters\nopt_params[1][\"num_leaves\"] = int(round(opt_params[1][\"num_leaves\"]))\nopt_params[1]['max_depth'] = int(round(opt_params[1]['max_depth']))\nopt_params[1]['min_data_in_leaf'] = int(round(opt_params[1]['min_data_in_leaf']))\nopt_params[1]['max_bin'] = int(round(opt_params[1]['max_bin']))\nopt_params[1]['objective']='binary'\nopt_params[1]['metric']='auc'\nopt_params[1]['is_unbalance']=True\nopt_params[1]['boost_from_average']=False\nopt_params=opt_params[1]\nopt_params","f835c3bc":"%%time\nfolds = StratifiedKFold(n_splits=10, shuffle=True, random_state=26)\noof = np.zeros(len(x_train))\npredictions = np.zeros(len(X_val))\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(x_train, y_train)):\n    print(\"Fold {}\".format(fold_))\n    trn_data = lgb.Dataset(train.iloc[trn_idx][features], label=y_train.iloc[trn_idx])\n    val_data = lgb.Dataset(train.iloc[val_idx][features], label=y_train.iloc[val_idx])\n    num_round = 10000\n    clf = lgb.train(opt_params, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=500, early_stopping_rounds = 250,)\n    oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration) \n    predictions += clf.predict(X_val, num_iteration=clf.best_iteration) \/ folds.n_splits\n","675998b5":"print(\"CV score: {:<8.5f}\".format(roc_auc_score(y_train, oof)))","36f00c68":"# putting threshold as 0.5\nbinary_predictions = [i>0.5 for i in predictions]","7f1a82b7":"# Scoring our model\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score,f1_score, precision_score, recall_score\n\n# Confusion Matrix\nprint('Confusion Matrix')\nprint(confusion_matrix(Y_val, binary_predictions))\nprint('--'*50)\n\n# Classification Report\nprint('Classification Report')\nprint(classification_report(Y_val, binary_predictions))\n\n\n# Accuracy of our model\nprint('--'*50)\nbayesOpt_accuracy = round(accuracy_score(Y_val, binary_predictions) * 100,8)\nprint('Accuracy = ', bayesOpt_accuracy,'%')\n","b6b1b28e":"## Model Report","a3ac6a66":"## Importing libraries\n","c2eec3c5":"## Bayesian Optimization","57dfe5ae":"## Prediciton with best parameters and KFold Technique","3ede4c21":"## Importing the data and preprocessing"}}