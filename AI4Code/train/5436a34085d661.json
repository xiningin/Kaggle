{"cell_type":{"deaf03d5":"code","c6e5a095":"code","900794d5":"code","a5641924":"code","82c29b1c":"code","c28bff4f":"code","cf9ccf83":"code","53314cea":"code","1a448aa0":"code","9a9a1943":"code","6df02a24":"code","ef44d816":"code","43288a2b":"code","c4648646":"code","201b7cd4":"code","850206f6":"code","b4d0021c":"code","bf9fd2a1":"code","9639062b":"code","79ef1dd1":"code","8b507ce6":"code","f94daf96":"code","62191d4a":"code","aa61e7c5":"code","cc65113e":"code","c7c1137c":"code","6fbb7a41":"code","eac094dd":"markdown","d3041b95":"markdown","519f94e2":"markdown","be8a9b72":"markdown","fa472f53":"markdown","8cc9fab9":"markdown","36257e34":"markdown","4d3415bb":"markdown"},"source":{"deaf03d5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\nprint(os.listdir(\"..\/input\"))\n# Any results you write to the current directory are saved as output.","c6e5a095":"# Importing Data\ndata = pd.read_csv(\"..\/input\/column_2C_weka.csv\")\nprint(plt.style.available) # look at available plot styles\nplt.style.use('ggplot')","900794d5":"# Showing first five columns\ndata.head()","a5641924":"# Showing last five columns\ndata.tail()","82c29b1c":"data.describe()","c28bff4f":"A = data[data.class_2 == \"Abnormal\"]\nN = data[data.class_2 == \"Normal\"]","cf9ccf83":"# Converting \ndata.class_2 = [1 if each == \"Abnormal\" else 0 for each in data.class_2]\ny = data.class_2.values\nx_data = data.drop([\"class_2\"], axis = 1)","53314cea":"# Normalization\nx = (x_data - np.min(x_data))\/(np.max(x_data) - np.min(x_data)) ","1a448aa0":"plt.scatter(A.pelvic_incidence, A.pelvic_radius, color = \"red\")\nplt.scatter(N.pelvic_incidence, N.pelvic_radius, color = \"green\")\nplt.xlabel(\"Pelvic Incidence\")\nplt.ylabel(\"Pelvic Radius\")\nplt.show()","9a9a1943":"sns.jointplot(x.loc[:,'pelvic_radius'], x.loc[:,'pelvic_incidence'], kind=\"regg\", color=\"#ce1414\")","6df02a24":"sns.countplot(x=\"class_2\", data=data)\ndata.loc[:,'class_2'].value_counts()","ef44d816":"color_list = ['red' if i=='Abnormal' else 'green' for i in data.loc[:,'class_2']]\npd.plotting.scatter_matrix(data.loc[:, data.columns != 'class_2'],\n                                       c=color_list,\n                                       figsize= [15,15],\n                                       diagonal='hist',\n                                       alpha=0.5,\n                                       s = 200,\n                                       marker = '*',\n                                       edgecolor= \"black\")\nplt.show()","43288a2b":"sns.set(style=\"white\")\ndf = x.loc[:,['pelvic_incidence','pelvic_tilt numeric','lumbar_lordosis_angle']]\ng = sns.PairGrid(df, diag_sharey=False)\ng.map_lower(sns.kdeplot, cmap=\"Blues_d\")\ng.map_upper(plt.scatter)\ng.map_diag(sns.kdeplot, lw=3)","c4648646":"# Correlation map\nf,ax = plt.subplots(figsize=(15, 15))\nsns.heatmap(x.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","201b7cd4":"# Histogram\n# bins = number of bar in figure\ndata.pelvic_incidence.plot(kind = 'hist', bins = 50, figsize = (15,15))\ndata.sacral_slope.plot(kind = 'hist', bins = 50, figsize = (15,15))\ndata.pelvic_radius.plot(kind = 'hist', bins = 50, figsize = (15,15))\nplt.show()","850206f6":"# Train and test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.3, random_state = 1)","b4d0021c":"# KNN Implementation \nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(x_train, y_train)\nprediction = knn.predict(x_test)","bf9fd2a1":"print(\"{} nn score: {}\".format(3,knn.score(x_test,y_test)))","9639062b":"# Find Best K Value\nscore_list = []\nfor each in range(1,50):\n    knn_2 = KNeighborsClassifier(n_neighbors = each)\n    knn_2.fit(x_train, y_train)\n    score_list.append(knn_2.score(x_test,y_test))\n\nplt.plot(range(1,50), score_list)\nplt.xlabel(\"K Values\")\nplt.ylabel(\"Accuracy\")\nplt.show()","79ef1dd1":"x_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T\n\nprint(\"X Train: \", x_train.shape)\nprint(\"X Test: \", x_test.shape)\nprint(\"Y Train: \", y_train.shape)\nprint(\"Y Test: \", y_test.shape)","8b507ce6":"# Initialize \n# Let's initialize parameters\n\ndef initialize_weights_and_bias(dimension):\n    w = np.full((dimension,1), 0.01)\n    b = 0.0\n    return w,b","f94daf96":"# Calculation of z\ndef sigmoid(z):\n    y_head = 1\/(1+np.exp(-z))\n    return y_head","62191d4a":"# Forward and Backward Propagation\n# In backward propagation we will use y_head that found in forward progation\n# Therefore instead of writing backward propagation method, lets combine forward propagation and backward propagation\ndef forward_backward_propagation(w,b,x_train,y_train):\n    # forward propagation\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))\/x_train.shape[1]      # x_train.shape[1]  is for scaling\n    # backward propagation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1] # x_train.shape[1]  is for scaling\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]                 # x_train.shape[1]  is for scaling\n    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n    return cost,gradients","aa61e7c5":"# Updating(learning) parameters\ndef update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    # updating(learning) parameters is number_of_iterarion times\n    for i in range(number_of_iterarion):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","cc65113e":"# Prediction\ndef predict(w,b,x_test):\n    # x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction","c7c1137c":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    # initialize\n    dimension =  x_train.shape[0]\n    w,b = initialize_weights_and_bias(dimension)\n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    y_prediction_train = predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n\n    # Print train\/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \nlogistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1, num_iterations = 100) ","6fbb7a41":"# sklearn\nfrom sklearn import linear_model\nlogreg = linear_model.LogisticRegression(random_state = 42,max_iter= 150)\nprint(\"Test accuracy: {} \".format(logreg.fit(x_train.T, y_train.T).score(x_test.T, y_test.T)))\nprint(\"Train accuracy: {} \".format(logreg.fit(x_train.T, y_train.T).score(x_train.T, y_train.T)))","eac094dd":"<a id=\"p5\"><\/a>\n# 5. Initializing, Optimizing, and Predicting\nNow that our data has been processed and formmated properly, and that we understand the general data we're working with as well as the trends and associations, we can start to build our model. We can import different classifiers from sklearn. ","d3041b95":"<a id=\"p1\"><\/a>\n# 1. Importing Libraries and Packages\nWe will use these packages to help us manipulate the data and visualize the features\/labels as well as measure how well our model performed. Numpy and Pandas are helpful for manipulating the dataframe and its columns and cells. We will use matplotlib along with Seaborn to visualize our data.","519f94e2":"# Contents\n1. [Importing Libraries and Packages](#p1)\n2. [Loading and Viewing Data Set](#p2)\n3. [Clean and Normalization Data](#p3)\n4. [Visualization](#p4)\n5. [Initializing, Optimizing, and Predicting](#p5)","be8a9b72":"<a id=\"p2\"><\/a>\n# 2. Loading and Viewing Data Set\nWith Pandas, we can load both the training and testing set that we wil later use to train and test our model. Before we begin, we should take a look at our data table to see the values that we'll be working with. We can use the head and describe function to look at some sample data and statistics.","fa472f53":"<a id=\"p3\"><\/a>\n# 3. Clean and Normalization Data\nWe need to change categorical data to numeric data and we have to normalize the data.","8cc9fab9":"# Logistic Regression Implementation","36257e34":"<a id=\"p4\"><\/a>\n# 4. Visualization\n\nIn order to visualizate the data, we are goingo to use matplotlib and seaborn. Before the visualization don't forget the normalize the data.","4d3415bb":"# KNN Implementation"}}