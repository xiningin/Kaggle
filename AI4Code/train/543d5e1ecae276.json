{"cell_type":{"99b5d74d":"code","010bc989":"code","7e48f852":"code","1c10911c":"code","f8aacdc5":"code","51627907":"code","07f65c88":"code","5f1f68aa":"code","1d1a297b":"code","e73b9fa9":"code","c1f4f153":"code","d94cb362":"code","29ebf989":"code","451b8dca":"code","a907b203":"code","35795088":"code","b0351f27":"code","59f4d9d0":"code","c473981c":"code","047273dd":"code","5972875d":"code","5137871e":"code","7d887172":"code","f173a5cd":"code","2e842987":"code","ed024561":"code","67a5c495":"code","a9224dc7":"code","2b414676":"code","d48a2271":"code","c1405a5a":"code","479a215f":"code","0aa461d1":"code","01b926b7":"code","8823bbca":"code","6b37fb4d":"code","7a210382":"code","fc2031ca":"code","f868dd77":"code","45c339cb":"code","01d7df87":"code","eaec2a95":"code","1c97f9f0":"code","afcd00ed":"code","864515de":"code","de7e3436":"code","974e4295":"code","d84d84b5":"code","d28daa85":"code","89355dc8":"code","7c535a55":"code","8b0f4d18":"code","8f775634":"code","3a21c016":"code","a2414a2a":"code","324c7cd5":"code","bbe93934":"code","7df9bc64":"code","28e580cd":"code","fba51576":"code","f553b3d3":"code","5490af4e":"code","a5e98c61":"code","d0016658":"code","11ce5e34":"code","9c4bf3cb":"code","ef41548c":"code","14cf0a89":"code","fc273534":"code","13d97343":"code","d2090b70":"code","5a3625c2":"code","7b73a433":"code","40f02131":"code","fb496bca":"code","e36601c6":"code","6551b94e":"code","36da9bc3":"code","914188bc":"code","904d13f3":"code","5560e898":"code","56c7a45f":"code","910f372e":"code","5b40f087":"code","c20a7d00":"code","9972fb7c":"code","82f9508a":"code","07026d15":"code","c916023d":"code","f0538eb2":"code","cf7a4b5b":"code","1a4f35b3":"code","d7e10b3c":"code","b9f90e14":"code","768c6eea":"code","b08bbf62":"code","b748ec62":"code","f10c50b3":"code","35aea575":"code","316f8e39":"code","8e0495cd":"code","e5825ebe":"code","9b6e4383":"code","361ff769":"code","b2cf849c":"code","4e6a2563":"code","6c86b67b":"code","07f197b1":"code","7f375bb2":"code","96ecde11":"code","dfe83fc3":"code","f8093ae6":"code","987236ba":"code","71a81bc8":"code","da2397be":"code","552be946":"code","26c4412a":"code","c97143a4":"code","6dc2307e":"code","7f4f8803":"code","7932bcac":"code","47839ff5":"code","a0e62bf3":"code","5baa36f2":"code","9b6661c8":"code","42ad2ace":"code","0f702ab6":"code","90cae074":"code","302d5b7b":"code","8ba42d55":"code","3bc242aa":"code","c49ace96":"code","7fd15a0c":"code","b4582160":"code","a88e8be9":"code","a00b5082":"code","8c6a9450":"code","ec9d0058":"code","eeaec7f3":"code","1ed16a6d":"code","a653aaad":"code","eb043a51":"code","38a3b9f1":"code","98dea869":"code","d51503d6":"code","dc967930":"code","2f012bcb":"code","0de70601":"code","002119de":"code","2f4cfb30":"code","3644bfee":"code","4072f266":"code","fee00e75":"code","2a312730":"code","2f852f9a":"code","92e11e7b":"code","e7b3dc46":"code","b9691ead":"code","f835ad6f":"code","b5e2793f":"code","70c6d559":"code","0a940258":"code","0c12b88e":"code","6fb37121":"code","d2bad575":"code","a3bc9077":"code","a285f584":"code","4e791970":"code","b0585714":"code","cce93100":"code","0127e0c6":"code","97ac533b":"code","6ec4fcd2":"code","beb9f383":"code","83096ca4":"code","7896d98b":"code","bdfb7552":"code","ae32ba5c":"code","9d20f5d1":"code","7c8e4a36":"code","9a76ec2b":"code","8a878616":"code","480449a3":"code","0b9042c2":"code","3701250c":"code","874a3fda":"code","251ba2f0":"code","db29ea65":"code","11321483":"code","537a606e":"code","c0024fcf":"code","eaf61c92":"code","5f4040d3":"code","50ccbac1":"code","2bd5da35":"code","f05a248d":"code","ae85fca0":"code","10547541":"code","8b6931a9":"code","ef7627bf":"code","d51da097":"code","c1e944fa":"code","16c7d1ee":"code","291adf01":"code","1db847a0":"code","9a118675":"code","4e61c802":"code","8b6597d7":"code","9f62dc3c":"markdown","53f94bee":"markdown","b778aacb":"markdown","2400f7a4":"markdown","7c91a779":"markdown","9b50c7d9":"markdown","3829ec4a":"markdown","18917904":"markdown","b3d0a914":"markdown","878b710f":"markdown","7dda2fa1":"markdown","feddede4":"markdown","a8dc5695":"markdown","f5fefd0f":"markdown","90782904":"markdown","5d01d70f":"markdown","33385768":"markdown","4643d8b8":"markdown","0597be39":"markdown","bc140e5a":"markdown","7d1744cc":"markdown","c49b474b":"markdown","b3fcf52e":"markdown","861deacb":"markdown","9630eef3":"markdown","8cf8bdd0":"markdown","b60e7f69":"markdown","bb385b33":"markdown","59c356b8":"markdown","572b35ac":"markdown","89a953a9":"markdown","5fc6871f":"markdown","86962aa8":"markdown","ef792778":"markdown","836e9755":"markdown","b1837268":"markdown","0de71349":"markdown","489c36e4":"markdown","90fd7296":"markdown","abf87062":"markdown","a03e053f":"markdown","27e75e69":"markdown","82978691":"markdown","f3210995":"markdown","3b8be3f0":"markdown","52b810d3":"markdown","c4446539":"markdown","bec57e65":"markdown","d4bbb5d9":"markdown","5b315755":"markdown","534ff80f":"markdown","3f6ac60a":"markdown"},"source":{"99b5d74d":"# pip install -U spacy\n# pip install -U spacy-lookups-data\n# python -m spacy download en_core_web_sm\n# python -m spacy download en_core_web_md\n# python -m spacy download en_core_web_lg","010bc989":"import pandas as pd\nimport numpy as np","7e48f852":"import spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\n","1c10911c":"df = pd.read_csv('..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv', encoding = 'latin1', header = None)","f8aacdc5":"df.head()","51627907":"df = df[[5, 0]]","07f65c88":"df.columns = ['twitts', 'sentiment']\ndf.head()","5f1f68aa":"df['sentiment'].value_counts()","1d1a297b":"sent_map = {0: 'negative', 4: 'positive'}","e73b9fa9":"df['word_counts'] = df['twitts'].apply(lambda x: len(str(x).split()))","c1f4f153":"df.head()","d94cb362":"df['char_counts'] = df['twitts'].apply(lambda x: len(x))","29ebf989":"df.head()","451b8dca":"def get_avg_word_len(x):\n    words = x.split()\n    word_len = 0\n    for word in words:\n        word_len = word_len + len(word)\n    return word_len\/len(words) # != len(x)\/len(words)","a907b203":"df['avg_word_len'] = df['twitts'].apply(lambda x: get_avg_word_len(x))","35795088":"len('this is nlp lesson')\/4","b0351f27":"df.head()","59f4d9d0":"115\/19","c473981c":"print(STOP_WORDS)","047273dd":"x = 'this is text data'","5972875d":"x.split()","5137871e":"len([t for t in x.split() if t in STOP_WORDS])\n","7d887172":"df['stop_words_len'] = df['twitts'].apply(lambda x: len([t for t in x.split() if t in STOP_WORDS]))","f173a5cd":"df.head()","2e842987":"x = 'this #hashtag and this is @mention'\n# x = x.split()\n# x","ed024561":"[t for t in x.split() if t.startswith('@')]\n    ","67a5c495":"df['hashtags_count'] = df['twitts'].apply(lambda x: len([t for t in x.split() if t.startswith('#')]))\ndf['mentions_count'] = df['twitts'].apply(lambda x: len([t for t in x.split() if t.startswith('@')]))","a9224dc7":"df.head()","2b414676":"df['numerics_count'] = df['twitts'].apply(lambda x: len([t for t in x.split() if t.isdigit()]))","d48a2271":"df.head()","c1405a5a":"df['upper_counts'] = df['twitts'].apply(lambda x: len([t for t in x.split() if t.isupper() and len(x)>3]))","479a215f":"df.head()","0aa461d1":"df.loc[96]['twitts']","01b926b7":"df['twitts'] = df['twitts'].apply(lambda x: x.lower())","8823bbca":"df.head(2)","6b37fb4d":"x = \"i don't know what you want, can't, he'll, i'd\"","7a210382":"contractions = { \n\"ain't\": \"am not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he'll've\": \"he will have\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how does\",\n\"i'd\": \"i would\",\n\"i'd've\": \"i would have\",\n\"i'll\": \"i will\",\n\"i'll've\": \"i will have\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it will\",\n\"it'll've\": \"it will have\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she will\",\n\"she'll've\": \"she will have\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so is\",\n\"that'd\": \"that would\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that is\",\n\"there'd\": \"there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they will\",\n\"they'll've\": \"they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\" u \": \" you \",\n\" ur \": \" your \",\n\" n \": \" and \"}","fc2031ca":"def cont_to_exp(x):\n    if type(x) is str:\n        for key in contractions:\n            value = contractions[key]\n            x = x.replace(key, value)\n        return x\n    else:\n        return x","f868dd77":"x = \"hi, i'd be happy\"","45c339cb":"cont_to_exp(x)","01d7df87":"%%time\ndf['twitts'] = df['twitts'].apply(lambda x: cont_to_exp(x))","eaec2a95":"df.head()","1c97f9f0":"import re","afcd00ed":"x = 'hi my email me at email@email.com another@email.com'","864515de":"re.findall(r'([a-zA-Z0-9+._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)', x)","de7e3436":"df['emails'] = df['twitts'].apply(lambda x: re.findall(r'([a-zA-Z0-9+._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)', x))","974e4295":"df['emails_count'] = df['emails'].apply(lambda x: len(x))","d84d84b5":"df[df['emails_count']>0].head()","d28daa85":"re.sub(r'([a-zA-Z0-9+._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)', '', x)","89355dc8":"df['twitts'] = df['twitts'].apply(lambda x: re.sub(r'([a-zA-Z0-9+._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)', '', x))","7c535a55":"df[df['emails_count']>0].head()","8b0f4d18":"x = 'hi, to watch more visit https:\/\/youtube.com\/kgptalkie'","8f775634":"re.findall(r'(http|ftp|https):\/\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\/~+#-]*[\\w@?^=%&\/~+#-])?', x)","3a21c016":"df['urls_flag'] = df['twitts'].apply(lambda x: len(re.findall(r'(http|ftp|https):\/\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\/~+#-]*[\\w@?^=%&\/~+#-])?', x)))","a2414a2a":"re.sub(r'(http|ftp|https):\/\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\/~+#-]*[\\w@?^=%&\/~+#-])?', '', x)","324c7cd5":"df['twitts'] = df['twitts'].apply(lambda x: re.sub(r'(http|ftp|https):\/\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\/~+#-]*[\\w@?^=%&\/~+#-])?', '', x))","bbe93934":"df.head()","7df9bc64":"df.loc[0]['twitts']","28e580cd":"df['twitts'] = df['twitts'].apply(lambda x: re.sub('RT', \"\", x))","fba51576":"df['twitts'] = df['twitts'].apply(lambda x: re.sub('[^A-Z a-z 0-9-]+', '', x))","f553b3d3":"df.head()","5490af4e":"x = 'thanks    for    watching and    please    like this video'","a5e98c61":"\" \".join(x.split())","d0016658":"df['twitts'] = df['twitts'].apply(lambda x: \" \".join(x.split()))","11ce5e34":"df.head(2)","9c4bf3cb":"from bs4 import BeautifulSoup","ef41548c":"x = '<html><h2>Thanks for watching<\/h2><\/html>'","14cf0a89":"BeautifulSoup(x, 'lxml').get_text()","fc273534":"%%time\ndf['twitts'] = df['twitts'].apply(lambda x: BeautifulSoup(x, 'lxml').get_text())","13d97343":"import unicodedata","d2090b70":"x = '\u00c1cc\u011bnt\u011bd t\u011bxt'","5a3625c2":"def remove_accented_chars(x):\n    x = unicodedata.normalize('NFKD', x).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    return x","7b73a433":"remove_accented_chars(x)","40f02131":"import spacy","fb496bca":"x = 'this is stop words removal code is a the an how what'","e36601c6":"\" \".join([t for t in x.split() if t not in STOP_WORDS])","6551b94e":"df['twitts'] = df['twitts'].apply(lambda x: \" \".join([t for t in x.split() if t not in STOP_WORDS]))","36da9bc3":"df.head()","914188bc":"nlp = spacy.load('en_core_web_sm')","904d13f3":"x = 'kenichan dived times ball managed save 50 rest'","5560e898":"# dive = dived, time = times, manage = managed","56c7a45f":"# x = 'i you he she they is am are'","910f372e":"def make_to_base(x):\n    x_list = []\n    doc = nlp(x)\n    \n    for token in doc:\n        lemma = str(token.lemma_)\n        if lemma == '-PRON-' or lemma == 'be':\n            lemma = token.text\n        x_list.append(lemma)\n    print(\" \".join(x_list))\n        ","5b40f087":"make_to_base(x)","c20a7d00":"' '.join(df.head()['twitts'])","9972fb7c":"text = ' '.join(df['twitts'])","82f9508a":"text = text.split()","07026d15":"freq_comm = pd.Series(text).value_counts()","c916023d":"f20 = freq_comm[:20]","f0538eb2":"f20","cf7a4b5b":"df['twitts'] = df['twitts'].apply(lambda x: \" \".join([t for t in x.split() if t not in f20]))","1a4f35b3":"rare20 = freq_comm[-20:]","d7e10b3c":"rare20","b9f90e14":"rare = freq_comm[freq_comm.values == 1]","768c6eea":"rare","b08bbf62":"df['twitts'] = df['twitts'].apply(lambda x: ' '.join([t for t in x.split() if t not in rare20]))","b748ec62":"df.head()","f10c50b3":"# !pip install wordcloud","35aea575":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n%matplotlib inline","316f8e39":"x = ' '.join(text[:20000])","8e0495cd":"len(text)","e5825ebe":"wc = WordCloud(width = 800, height=400).generate(x)\nplt.imshow(wc)\nplt.axis('off')\nplt.show()","9b6e4383":"# !pip install -U textblob\n# !python -m textblob.download_corpora","361ff769":"from textblob import TextBlob","b2cf849c":"x = 'tanks forr waching this vidio carri'","4e6a2563":"x = TextBlob(x).correct()\n","6c86b67b":"x","07f197b1":"x = 'thanks#watching this video. please like it'","7f375bb2":"TextBlob(x).words","96ecde11":"doc = nlp(x)\nfor token in doc:\n    print(token)","dfe83fc3":"x = 'runs run running ran'","f8093ae6":"from textblob import Word","987236ba":"for token in x.split():\n    print(Word(token).lemmatize())","71a81bc8":"doc = nlp(x)\nfor token in doc:\n    print(token.lemma_)","da2397be":"x = \"Breaking News: Donald Trump, the president of the USA is looking to sign a deal to mine the moon\"","552be946":"doc = nlp(x)\nfor ent in doc.ents:\n    print(ent.text + ' - ' + ent.label_ + ' - ' + str(spacy.explain(ent.label_)))","26c4412a":"from spacy import displacy","c97143a4":"displacy.render(doc, style = 'ent')","6dc2307e":"x","7f4f8803":"for noun in doc.noun_chunks:\n    print(noun)","7932bcac":"x","47839ff5":"tb = TextBlob(x)","a0e62bf3":"tb.detect_language()","5baa36f2":"tb.translate(to='bn')","9b6661c8":"from textblob.sentiments import NaiveBayesAnalyzer","42ad2ace":"x = 'we all stands together to fight with corona virus. we will win together'","0f702ab6":"tb = TextBlob(x, analyzer=NaiveBayesAnalyzer())","90cae074":"tb.sentiment","302d5b7b":"x = 'we all are sufering from corona'","8ba42d55":"tb = TextBlob(x, analyzer=NaiveBayesAnalyzer())","3bc242aa":"tb.sentiment","c49ace96":"x = 'thanks for watching'","7fd15a0c":"tb = TextBlob(x)","b4582160":"tb.ngrams(3)","a88e8be9":"x = ['this is first sentence this is', 'this is second', 'this is last']","a00b5082":"from sklearn.feature_extraction.text import CountVectorizer","8c6a9450":"cv = CountVectorizer(ngram_range=(1,1))\ntext_counts = cv.fit_transform(x)","ec9d0058":"text_counts.toarray()","eeaec7f3":"cv.get_feature_names()","1ed16a6d":"bow = pd.DataFrame(text_counts.toarray(), columns = cv.get_feature_names())","a653aaad":"bow","eb043a51":"x","38a3b9f1":"x","98dea869":"bow","d51503d6":"bow.shape","dc967930":"tf = bow.copy()","2f012bcb":"for index, row in enumerate(tf.iterrows()):\n    for col in row[1].index:\n        tf.loc[index, col] = tf.loc[index, col]\/sum(row[1].values)","0de70601":"tf","002119de":"import numpy as np","2f4cfb30":"x_df = pd.DataFrame(x, columns=['words'])","3644bfee":"x_df","4072f266":"bow","fee00e75":"N = bow.shape[0]\nN","2a312730":"bb = bow.astype('bool')\nbb","2f852f9a":"bb['is'].sum()","92e11e7b":"cols = bb.columns\ncols","e7b3dc46":"nz = []\nfor col in cols:\n    nz.append(bb[col].sum())","b9691ead":"nz","f835ad6f":"idf = []\nfor index, col in enumerate(cols):\n    idf.append(np.log((N + 1)\/(nz[index] + 1)) + 1)","b5e2793f":"idf","70c6d559":"bow","0a940258":"from sklearn.feature_extraction.text import TfidfVectorizer","0c12b88e":"tfidf = TfidfVectorizer()\nx_tfidf = tfidf.fit_transform(x_df['words'])","6fb37121":"x_tfidf.toarray()","d2bad575":"tfidf.idf_","a3bc9077":"idf","a285f584":"# !python -m spacy download en_core_web_lg","4e791970":"nlp = spacy.load('en_core_web_lg')","b0585714":"doc = nlp('thank you! dog cat lion dfasaa')","cce93100":"for token in doc:\n    print(token.text, token.has_vector)","0127e0c6":"token.vector.shape","97ac533b":"nlp('cat').vector.shape","6ec4fcd2":"for token1 in doc:\n    for token2 in doc:\n        print(token1.text, token2.text, token1.similarity(token2))\n    print()","beb9f383":"df.shape","83096ca4":"df0 = df[df['sentiment']==0].sample(2000)\ndf4 = df[df['sentiment']==4].sample(2000)","7896d98b":"dfr = df0.append(df4)","bdfb7552":"dfr.shape","ae32ba5c":"dfr_feat = dfr.drop(labels=['twitts','sentiment','emails'], axis = 1).reset_index(drop=True)","9d20f5d1":"dfr_feat","7c8e4a36":"y = dfr['sentiment']","9a76ec2b":"from sklearn.feature_extraction.text import CountVectorizer","8a878616":"cv = CountVectorizer()\ntext_counts = cv.fit_transform(dfr['twitts'])","480449a3":"text_counts.toarray().shape","0b9042c2":"dfr_bow = pd.DataFrame(text_counts.toarray(), columns=cv.get_feature_names())","3701250c":"dfr_bow.head(2)","874a3fda":"from sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.preprocessing import MinMaxScaler","251ba2f0":"sgd = SGDClassifier(n_jobs=-1, random_state=42, max_iter=200)\nlgr = LogisticRegression(random_state=42, max_iter=200)\nlgrcv = LogisticRegressionCV(cv = 2, random_state=42, max_iter=1000)\nsvm = LinearSVC(random_state=42, max_iter=200)\nrfc = RandomForestClassifier(random_state=42, n_jobs=-1, n_estimators=200)","db29ea65":"clf = {'SGD': sgd, 'LGR': lgr, 'LGR-CV': lgrcv, 'SVM': svm, 'RFC': rfc}","11321483":"clf.keys()","537a606e":"def classify(X, y):\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    X = scaler.fit_transform(X)\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)\n    \n    for key in clf.keys():\n        clf[key].fit(X_train, y_train)\n        y_pred = clf[key].predict(X_test)\n        ac = accuracy_score(y_test, y_pred)\n        print(key, \" ---> \", ac)","c0024fcf":"%%time\nclassify(dfr_bow, y)","eaf61c92":"dfr_feat.head(2)","5f4040d3":"%%time\nclassify(dfr_feat, y)","50ccbac1":"X = dfr_feat.join(dfr_bow)","2bd5da35":"%%time\nclassify(X, y)","f05a248d":"from sklearn.feature_extraction.text import TfidfVectorizer","ae85fca0":"dfr.shape","10547541":"tfidf = TfidfVectorizer()\nX = tfidf.fit_transform(dfr['twitts'])","8b6931a9":"%%time\nclassify(pd.DataFrame(X.toarray()), y)","ef7627bf":"def get_vec(x):\n    doc = nlp(x)\n    return doc.vector.reshape(1, -1)","d51da097":"%%time\ndfr['vec'] = dfr['twitts'].apply(lambda x: get_vec(x))","c1e944fa":"X = np.concatenate(dfr['vec'].to_numpy(), axis = 0)","16c7d1ee":"X.shape","291adf01":"classify(pd.DataFrame(X), y)","1db847a0":"def predict_w2v(x):\n    for key in clf.keys():\n        y_pred = clf[key].predict(get_vec(x))\n        print(key, \"-->\", y_pred)","9a118675":"predict_w2v('hi, thanks for watching this video. please like and subscribe')","4e61c802":"predict_w2v('please let me know if you want more video')","8b6597d7":"predict_w2v('congratulation looking good congrats')","9f62dc3c":"### Special Chars removal or punctuation removal ","53f94bee":"### Word Cloud Visualization ","b778aacb":"### Count URLs and Remove it ","2400f7a4":"### Remove Accented Chars ","7c91a779":"### Lower case conversion ","9b50c7d9":"### Remove RT  ","3829ec4a":"### N-Grams ","18917904":"#### TFIDF","b3d0a914":"### ML Algorithms ","878b710f":"### Stop Words Count ","7dda2fa1":"##### Manual Feature","feddede4":"Watch Full Video: https:\/\/youtu.be\/VyDmQggfsZ0\n\nDataset: https:\/\/www.kaggle.com\/kazanova\/sentiment140\/data#","a8dc5695":"### UPPER case words count","f5fefd0f":"idf = log( (1 + N)\/(n + 1)) + 1 used in sklearn when smooth_idf = True","90782904":"### Common words removal ","5d01d70f":"### Rare words removal ","33385768":"### Lemmatization ","4643d8b8":"### Term Frequency ","0597be39":"### Detecting Nouns ","bc140e5a":"Language Code: https:\/\/www.loc.gov\/standards\/iso639-2\/php\/code_list.php","7d1744cc":"#### SpaCy `Word2Vec` ","c49b474b":"### Complete Text Processing for Beginners ","b3fcf52e":"### Translation and Language Detection ","861deacb":"### Inverse Document Frequency IDF","9630eef3":"### Remove Stop Words ","8cf8bdd0":"### Detect Entities using NER of SpaCy ","b60e7f69":"### TFIDF ","bb385b33":"### Tokenization ","59c356b8":"where, N is the total number of rows and n is the number of rows in which the word was present.","572b35ac":"### Count and Remove Emails ","89a953a9":"### Remove multiple spaces `\"hi   hello    \"`","5fc6871f":"Term frequency is simply the ratio of the count of a word present in a sentence, to the length of the sentence.","86962aa8":"### Remove HTML tags ","ef792778":"### Bag of Words `BoW` ","836e9755":"### Word Embeddings ","b1837268":"### Convert into base or root form of word ","0de71349":"### Characters Count ","489c36e4":"### Spelling Correction ","90fd7296":"## Machine Learning Models for Text Classification","abf87062":"### BoW","a03e053f":"### SpaCy and NLP ","27e75e69":"### Word2Vec ","82978691":"### Count #HashTags and @Mentions ","f3210995":"### Use inbuilt sentiment classifier ","3b8be3f0":"### if numeric digits are present in twitts ","52b810d3":"### Contraction to Expansion ","c4446539":"### Average Word Length ","bec57e65":"##### Manual + Bow ","d4bbb5d9":"#### General Feature Extraction\n- File loading\n- Word counts\n- Characters count\n- Average characters per word\n- Stop words count\n- Count #HashTags and @Mentions\n- If numeric digits are present in twitts\n- Upper case word counts\n\n#### Preprocessing and Cleaning\n- Lower case\n- Contraction to Expansion\n- Emails removal and counts\n- URLs removal and counts\n- Removal of `RT`\n- Removal of Special Characters\n- Removal of multiple spaces\n- Removal of HTML tags\n- Removal of accented characters\n- Removal of Stop Words\n- Conversion into base form of words\n- Common Occuring words Removal\n- Rare Occuring words Removal\n- Word Cloud\n- Spelling Correction\n- Tokenization\n- Lemmatization\n- Detecting Entities using NER\n- Noun Detection\n- Language Detection\n- Sentence Translation\n- Using Inbuilt `Sentiment Classifier`\n\n#### Advanced Text Processing and Feature Extraction\n- N-Gram, Bi-Gram etc\n- Bag of Words (BoW)\n- Term Frequency Calculation `TF`\n- Inverse Document Frequency `IDF`\n- `TFIDF` Term Frequency - Inverse Document Frequency\n- Word Embedding `Word2Vec` using SpaCy\n\n#### Machine Learning Models for Text Classification\n- SGDClassifier\n- LogisticRegression\n- LogisticRegressionCV\n- LinearSVC\n- RandomForestClassifier","5b315755":"## Preprocessing and Cleaning ","534ff80f":"## Advanced Text Processing ","3f6ac60a":"### Word Counts "}}