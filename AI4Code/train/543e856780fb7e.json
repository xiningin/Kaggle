{"cell_type":{"7c17c61d":"code","29cfa0b5":"code","c31c8419":"code","0d425b5e":"code","a4752073":"code","bfb1462c":"code","d3753be1":"code","5a2ce52f":"code","6afcd508":"code","6461f40f":"code","34d0f6f9":"code","17cdd4bc":"code","6eb64caa":"code","f7815e43":"code","35d05204":"code","f9a3fe27":"code","f0dd3974":"code","44409ce4":"code","e2fe226f":"markdown","f9be814f":"markdown","3140b536":"markdown","0da343b2":"markdown","85408ef1":"markdown","14e434e7":"markdown"},"source":{"7c17c61d":"#Ho\u015fgeldiniz :) \u00d6zg\u00fcn i\u00e7erikler i\u00e7in l\u00fctfen takip edin --> volkandl\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport tensorflow as tf\nimport tensorflow.keras \nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.utils import to_categorical\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras import backend as K\nfrom keras import optimizers\nimport tensorflow as tf\nimport cv2 \nimport numpy as np \nimport matplotlib.pyplot as plt \nprint(\"tensorflow version is : \", tf.__version__)","29cfa0b5":"path_train=os.path.join(\"\/kaggle\/input\/digit-recognizer\",\"train.csv\")\npath_test=os.path.join(\"\/kaggle\/input\/digit-recognizer\",\"test.csv\")\nprint(path_train),print(path_test)\ntrain=pd.read_csv(path_train)\ntest=pd.read_csv(path_test)\n\nprint(\"train shape: \",train.shape)\nprint(\"test shape: \",test.shape)","c31c8419":"train.head(3)","0d425b5e":"label=train.iloc[:,:1]\ntrain = train.iloc[:,1:]\nprint(\"train shape: \",train.shape),print(\"label shape: \",label.shape)","a4752073":"train = train\/255.\n\nprint(\"bo\u015f de\u011fer var m\u0131?: \",label.isnull().sum().sum())\nprint(\"bo\u015f de\u011fer var m\u0131?: \",train.isnull().sum().sum())","bfb1462c":"#Train - Test ayr\u0131\u015ft\u0131rmas\u0131 yap\u0131yoruz. %90 Train - %10 test olacak \u015fekilde. random_state'i sabit bir de\u011fere at\u0131yoruz ki hyper-parametre tune edebilelim.\n\nx_train, x_test, y_train, y_test = train_test_split(train, label, test_size=0.15, random_state=2 )\nprint(\"x_train shape: \", x_train.shape)\nprint(\"y_train shape: \", y_train.shape)\nprint(\"x_test shape: \", x_test.shape)\nprint(\"y_test shape: \", y_test.shape)\n","d3753be1":"#reshape images\nx_train = x_train.values.reshape(-1,28,28,1)\nx_test = x_test.values.reshape(-1,28,28,1)\n#one-hot encoding\nnum_classes=10\ny_train = tensorflow.keras.utils.to_categorical(y_train, num_classes)\ny_test = tensorflow.keras.utils.to_categorical(y_test, num_classes)\n\n\nprint(\"x_train shape: \", x_train.shape)\nprint(\"y_train shape: \", y_train.shape)\nprint(\"x_test shape: \", x_test.shape)\nprint(\"y_test shape: \", y_test.shape)\n\n","5a2ce52f":"fig = plt.figure(figsize = (11, 12))\n\nfor i in range(16):  \n    plt.subplot(4,4,1 + i)\n    plt.title(np.argmax(y_train[i]),fontname=\"Times New Roman\",fontweight=\"bold\")\n    plt.imshow(x_train[i,:,:,0], cmap=plt.get_cmap('gray'))\nplt.show()","6afcd508":"#Hyper Parameters\nbatch_size = 128\nnum_classes = 10\nepochs = 150\nlearning_rate=0.0001\nactivation = 'relu'\nFully_connected_layer_nodes=4096","6461f40f":"#Rehape the images for AlexNet input such: (28,28,1) --> Giri\u015fleri AlexNet'e g\u00f6re ayarl\u0131yoruz.\nimg_rows = 28\nimg_cols = 28\nif K.image_data_format() == 'channels_first':\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)","34d0f6f9":"print(\"...::MNIST Data set on AlexNet Architecture::...\")\nprint(\"learn rate: \",learning_rate, \",epochs: \", epochs, \",activation: \",activation, \",Fully Connected Layer's Node Number :\", Fully_connected_layer_nodes)\n\n\n#ALEXNET STRUCTURE\nmodel = Sequential()\n#First layer Convolution 96\nmodel.add(Conv2D(96, kernel_size=(3, 3),\n                 activation= activation,padding=\"SAME\",\n                 input_shape=input_shape))\n#MaxPool\nmodel.add(MaxPooling2D(pool_size=(3, 3)))\n#Second layer Convolution 256\nmodel.add(Conv2D(256, (3, 3), activation=activation,padding=\"SAME\"))\n#MaxPool\nmodel.add(MaxPooling2D(pool_size=(3, 3)))\n#Third layer Convolution 384\nmodel.add(Conv2D(384, (3, 3), activation=activation,padding=\"SAME\"))\n#Fourth layer Convolution 384\nmodel.add(Conv2D(384, (3, 3), activation=activation,padding=\"SAME\"))\n#Fifth layer Convolution 384\nmodel.add(Conv2D(256, (3, 3), activation=activation,padding=\"SAME\"))\n#MaxPool\nmodel.add(MaxPooling2D(pool_size=(3, 3)))\n#Drop-out 25%\nmodel.add(Dropout(0.15))\n#Flatten Before Fully Connected Layer\nmodel.add(Flatten())\n#First Fully Connected Layer Originally has 4096 Nodes\nmodel.add(Dense(Fully_connected_layer_nodes, activation=activation))\n#Droput 20%\nmodel.add(Dropout(0.15))\n#First Fully Connected Layer Originally has 4096 Nodes\nmodel.add(Dense(Fully_connected_layer_nodes, activation=activation))\n#Drop-out 20%\nmodel.add(Dropout(0.15))\n#Output layer for 10 Classes --> Softmax\nmodel.add(Dense(num_classes, activation='softmax'))\n#Adam Optimizer, Categorical Crossentropy Loss\nadam=tensorflow.keras.optimizers.Adam(lr=learning_rate)\n\ncallback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam',\n              metrics=['accuracy'])\n\nmodel.summary()","17cdd4bc":"history=model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, callbacks=[callback], verbose=1, \\\nvalidation_data=(x_test, y_test), shuffle=True)\nval_loss, val_acc=model.evaluate(x_test, y_test )\nprint(\"validation loss: \", val_loss)\nprint( \"<3 \")\nprint(\"validation accuracy: \", val_acc)\nprint(\"learn rate: \",learning_rate, \"epochs: \", epochs, \"activation: \",activation, \"Fully Connected Layer's Node Number :\", Fully_connected_layer_nodes)\n","6eb64caa":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.legend(['train', 'validation'], loc='lower right')\nplt.title('accuracy train vs test', fontname=\"Times New Roman\",fontweight=\"bold\")\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.show()\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.legend(['training loss', 'validation loss'], loc = 'upper right')\nplt.title('loss  training vs validation', fontname=\"Times New Roman\",fontweight=\"bold\")\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.show()","f7815e43":"y_pred= model.predict(x_test)\ny_pred = np.argmax(y_pred, axis=1)\ny_test = np.argmax(y_test, axis=1)\nprint(y_pred)\nprint(y_pred.shape)\nprint(y_test.shape)","35d05204":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","f9a3fe27":"import seaborn as sns\ndata= tf.math.confusion_matrix(labels=y_test,predictions=y_pred)\nplt.figure(figsize=(12,8))\nsns.heatmap(data, fmt='d',annot=True)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()","f0dd3974":"submission_data = pd.read_csv(r'..\/input\/digit-recognizer\/test.csv')\n\nsubmission_data = submission_data \/ 255.\n\nsubmission_data = submission_data.values.reshape(-1,28,28,1)\n\n#Sonuclar\u0131 tahmin edelim bakal\u0131m :)\n\noutput = model.predict(submission_data)\nresults = np.argmax(output,axis = 1)","44409ce4":"results = pd.Series(results,name=\"Label\")\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\nsubmission.to_csv(\".\/submission.csv\",index=False)","e2fe226f":"![AlexNet](https:\/\/www.researchgate.net\/publication\/320052364\/figure\/fig1\/AS:543136445198336@1506505227088\/Scheme-of-the-AlexNet-network-used.png)\n\n\n\nAlexNet sekiz katman i\u00e7eriyordu; ilk be\u015fi evri\u015fimli (konvol\u00fcsyonel) katmanlard\u0131, baz\u0131lar\u0131 bunu maksimum havuzlama katmanlar\u0131 izliyordu ve son \u00fc\u00e7\u00fc tamamen ba\u011fl\u0131 katmanlard\u0131 Tanh ve sigmoid \u00fczerinde geli\u015ftirilmi\u015f e\u011fitim performans\u0131 g\u00f6steren doygun olmayan ReLU aktivasyon fonksiyonunu kulland\u0131. Biz de MNSIT data setini AlexNet \u00fcst\u00fcnden e\u011fitmeye \u00e7al\u0131\u015faca\u011f\u0131z. \u00d6dev i\u00e7in kullanacak arkada\u015flar bir be\u011feninizi rica ederim. Amac\u0131m\u0131z y\u00fcksek skor elde etmek de\u011fil basit konseptler ile konuyu anlamakt\u0131r. Yoksa Inception, EfficientNet, EfficientNetv2 hatta daha ge\u00e7en hafta \u00e7\u0131kan CoatNet gibi transformers tabanl\u0131 yap\u0131lar \u00e7ok daha b\u00fcy\u00fck ba\u015far\u0131lar elde edebilir. (?)","f9be814f":"Hiper parametreler bizlerin cross validation ile optimize etmek durumunda oldu\u011fumuz de\u011fi\u015fkenlerdir. Bunlar temel olarak batch size, epoch say\u0131s\u0131, learning rate, aktivasyon fonksiyonlar\u0131 ve fully connected layer'lar i\u00e7in node say\u0131lar\u0131d\u0131r. Modeller b\u00fcy\u00fcd\u00fck\u00e7e bu hiper parametreler'e yenileri eklenmektedir.","3140b536":"\n# **$\\color{purple}{\\text{Derin \u00d6\u011frenmeye Giri\u015f 2 - MNIST DIGIT RECOGNIZER Veri Seti ile Konvol\u00fcsyonel Sinir A\u011flar\u0131  }}$**\n\n*Hi folks, this notebook dedicated to Turkish researchers & deep leaners. Hence, the language is completely Turkish.*\n\nHerkese merhaba, kaggle setlerine ve e\u011fitimlerine devam ediyoruz. Yapay sinir a\u011flar\u0131na giri\u015f yapt\u0131\u011f\u0131m\u0131z [](https:\/\/www.kaggle.com\/volkandl\/mnist-turkce-aciklamali-derin-ogrenmeye-giris-1) 1.mod\u00fcl\u00fcn ard\u0131ndan 2. mod\u00fcl olan konvol\u00fcsyonel Sinir A\u011flar\u0131 (Convolutional Neural Networks) ile kar\u015f\u0131n\u0131zday\u0131m. K\u0131saltma olarak CNN diye adland\u0131r\u0131lan bu yap\u0131lar derin \u00f6\u011frenme d\u00fcnyas\u0131n\u0131n 50 y\u0131ll\u0131k kara baht\u0131n\u0131 de\u011fi\u015ftirmi\u015f olarak bilim d\u00fcnyas\u0131nda da b\u00fcy\u00fck yer tutmaktad\u0131r. AlexNet'in imagenet data seti \u00fczerindeki ba\u015far\u0131s\u0131 ile bu alandaki \u00e7al\u0131\u015fmalar ve yat\u0131r\u0131mlar h\u0131zlanm\u0131\u015ft\u0131r. Sonras\u0131nda da g\u00fcn\u00fcm\u00fczde bolca ba\u015fvurdu\u011fumuz googlenet, resnet, efficentnet gibi yap\u0131lar ile konu derinlik kazanm\u0131\u015ft\u0131r. \n\nPeki nedir bu konvol\u00fcsyon ?\n\n\nKonvol\u00fcsyon, birinin \u015feklinin di\u011feri taraf\u0131ndan nas\u0131l de\u011fi\u015ftirildi\u011fini ifade eden \u00fc\u00e7\u00fcnc\u00fc bir fonksiyonu \u00fcreten iki fonksiyon (f ve g) \u00fczerinde matematiksel bir i\u015flemdir. Konvol\u00fcsyon terimi hem sonu\u00e7 fonksiyonunu hem de onu hesaplama s\u00fcrecini ifade eder. Sinyal i\u015fleme ve sistemler d\u00fc\u015f\u00fcn\u00fcld\u00fc\u011f\u00fcnde temel olarak do\u011frusal ve zamanda de\u011fi\u015fmeyen sitemler i\u00e7in sistemin \u00e7\u0131kt\u0131s\u0131 girdi ile d\u00fcrt\u00fc tepkisinin (impulse response) konvol\u00fcsyonudur. Asl\u0131nda g\u00f6r\u00fcnt\u00fc de bir sinyaldir ve baz\u0131 i\u015flemleri daha kolay kolay yapmak ad\u0131na zaman domain'den frekans domain'ine ge\u00e7ilmesi gerekir. Lakin sinir a\u011flar\u0131nda do\u011frudan kullan\u0131lan sistem bu de\u011fildir yine de \u00e7ok benzemektedir. Bu sayede binlerce pixel'den olu\u015fan bir g\u00f6r\u00fcnt\u00fcy\u00fc hem konvalosyon i\u015flemiyle pixel kom\u015fuluklar\u0131n\u0131 da de\u011ferlendirebilece\u011fiz hem de pooling dedi\u011fimiz sistem ile veri say\u0131s\u0131n\u0131 anlaml\u0131 bir \u015fekilde azaltarak i\u015flem y\u00fck\u00fcnden tasarruf edece\u011fiz. \n\n\n![konv](https:\/\/miro.medium.com\/max\/2184\/0*uXlU8jOFInKCaR7y.png)\n\n\nCNN'lerde kernel dedi\u011fimiz filtreleri \u00f6\u011frenmeye \u00e7al\u0131\u015f\u0131r\u0131z. Hat\u0131rlarsan\u0131z ANN'lerde noronlar aras\u0131ndaki a\u011f\u0131rl\u0131klar\u0131 (weights ,w ) \u00f6\u011frenmeye \u00e7al\u0131\u015f\u0131yorduk. Art\u0131k g\u00fcncelleyece\u011fimiz parametreler bu filtrelerin kat say\u0131lar\u0131 olacak :)\n\nDaha \u00e7ok TensorFlow kullanaca\u011f\u0131z.","0da343b2":"$\\color{purple}{\\text{Data Ne \u0130\u00e7eriyor ? }}$\n\nTrain data setinin ilk s\u00fctunu \"label\" yani etiketlerimiz. (0-9) aras\u0131 10 tane s\u0131n\u0131f bulunmaktad\u0131r. Label'lar\u0131 ayr\u0131 bir data frame'de tutuyoruz, ilk s\u00fctunu .iloc komutu ile etiket isimli df'e kaydediyoruz.","85408ef1":"Sinir a\u011flar\u0131n\u0131n hikayesine bakar isek, ilk ortaya \u00e7\u0131kan perceptron yap\u0131s\u0131ndan sonra \u00f6zellikle XOR problemine \u00e7\u00f6z\u00fcm getirmek amac\u0131yla daha derin sinir a\u011flar\u0131 ortaya \u00e7\u0131km\u0131\u015ft\u0131r. \u0130leri beslemeli sinir a\u011flar\u0131 giri\u015f katman\u0131 olarak bir vekt\u00f6r almaktad\u0131r. Bir g\u00f6r\u00fcnt\u00fcy\u00fc vekt\u00f6r haline getirdi\u011fimiz zaman tahmin edersiniz ki pikseller aras\u0131ndaki ili\u015fkiler, kom\u015fuluklar ve pikseller aras\u0131ndaki \u00f6nemli ge\u00e7i\u015fler kaybolmaktad\u0131r. \u00dcstelik \u00e7ok fazla b\u00fcy\u00fcmesi gereken sinir a\u011flar\u0131 dar bo\u011faz ya\u015famaktad\u0131r. Konvol\u00fcsyon bu duruma bir \u00e7\u00f6z\u00fcm olarak ortaya \u00e7\u0131km\u0131\u015ft\u0131r, ImageNet data seti \u00fczerinde \u00e7ok b\u00fcy\u00fck ba\u015far\u0131 g\u00f6steren AlexNet [1] sonras\u0131nda ConvNet'ler hak etti\u011fi de\u011feri g\u00f6rmeye ba\u015flam\u0131\u015ft\u0131r. Ama unutmayal\u0131mki teknikler hep geli\u015fmektedir. Konvol\u00fcsyon bile eskimek \u00fczere art\u0131k computer vision d\u00fcnyas\u0131nda Vistion Transformers (ViT)'leri konu\u015fmaya ba\u015flad\u0131k bile. Belki bir seri de onun \u00fcst\u00fcne yapar\u0131z.\n\nHer ney ise, CNN'ler i\u00e7in giri\u015f katman\u0131 vekt\u00f6r de\u011fil, g\u00f6r\u00fcnt\u00fcn\u00fcn kendisi yani 28x28'lik yap\u0131d\u0131r.\n\n[1] Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, 1097-1105.","14e434e7":"MNIST veritaban\u0131, \u00e7e\u015fitli g\u00f6r\u00fcnt\u00fc i\u015fleme sistemlerini e\u011fitmek i\u00e7in yayg\u0131n olarak kullan\u0131lan b\u00fcy\u00fck bir **el yaz\u0131s\u0131 rakam** veritaban\u0131d\u0131r. Veritaban\u0131 ayr\u0131ca makine \u00f6\u011frenimi alan\u0131nda e\u011fitim ve test i\u00e7in yayg\u0131n olarak kullan\u0131lmaktad\u0131r. NIST'in orijinal veri setlerinden \u00f6rnekleri \"yeniden kar\u0131\u015ft\u0131rarak\" olu\u015fturulmu\u015ftur. Veri seti i\u00e7erisinde 28 x 28 pixel'li gray imageler'den olu\u015fuyor. Bu demek oluyor ki yatayda 28, dikeyde 28 ve toplamda 784 tane piksel bulunmaktad\u0131r. Ve bu pixellerin her birisi 0 ile 255 ( 2^8 ~ veya 8 bit nas\u0131l derseniz) de\u011ferler alabilmektedir. Haz\u0131r veri seti train ve test olarak ayr\u0131lm\u0131\u015f durumdad\u0131r. The MNIST database  60,000 training imgesi ve 10,000 test imgesi i\u00e7ermektedir. Ama kaggle'da bu 42,000 training ve 28,000 test olarak ayr\u0131lm\u0131\u015ft\u0131r. Ak\u0131llar kar\u0131\u015fmas\u0131n Train - Test - Validation olarak 3 gruba ay\u0131raca\u011f\u0131z datay\u0131. Train - Test sinir a\u011f\u0131n\u0131 e\u011fitirken kullan\u0131lacak, Validation ise e\u011fitilmi\u015f model i\u00e7in daha \u00f6nceden g\u00f6rmedi\u011fi veriler \u00fczerinden performans metrikleri hesaplanacakt\u0131r.\n\n* Train seti; 42000 tane 28x28 gray scale (0-255) imgelerden olu\u015fmaktad\u0131r.\n* Train seti'nin label'lar\u0131 yani etiketleri ise tek boyutlu ve 0-9 aras\u0131ndaki rakamlardan olu\u015fmaktad\u0131r.\n* Test seti: 28000 tane 28x28 gray scale (0-255) imgelerden olu\u015fmaktad\u0131r. Ayn\u0131 \u015fekilde 0-9 aras\u0131nda ground truth yani ger\u00e7ek de\u011ferlerini i\u00e7eren test label seti vard\u0131r."}}