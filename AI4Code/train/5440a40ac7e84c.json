{"cell_type":{"d7fcc774":"code","3243d2c5":"code","dd0c009d":"code","056f7667":"code","b1f2fe1b":"code","a340138a":"code","3b043708":"code","96974dcb":"code","3adbf555":"code","b583c5f7":"code","93988cf6":"code","baf2566b":"code","99c42ade":"code","a2e4b66f":"code","36246cb7":"code","c77a90f8":"markdown","87a63c30":"markdown","a96920e1":"markdown","5475b3d3":"markdown","5152e598":"markdown","67c62d7d":"markdown"},"source":{"d7fcc774":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport plotly.figure_factory as ff\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, f1_score, accuracy_score, classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\npd.set_option(\"plotting.backend\", \"plotly\")","3243d2c5":"df = pd.read_csv('..\/input\/natural-diamonds-prices-images\/Diamonds\/Diamonds\/data_cushion.csv')\ndf","dd0c009d":"df['Price'] = df['Price'].str.replace(',', '').astype(np.float)\n\n# Function to extract all the numbers from the given string\ndef getNumbers(str):\n    import re\n    \n    array = re.findall(r'[0-9]', str)\n    return array\n\ndf['Messurements'] = df['Messurements'].apply(lambda x: getNumbers(x) )\ndf['Messurements'] = df['Messurements'].apply(lambda x: ''.join(x) )\n\ndf['length']= df['Messurements'].str[:3].astype(np.float) \/100\ndf['width'] = df['Messurements'].str[3:6].astype(np.float) \/100\ndf['depth'] = df['Messurements'].str[6:].astype(np.float) \/ 100\n\ndf","056f7667":"def EDA(df):\n    print('\\033[1m' + 'Shape of the data :' + '\\033[0m')\n    print(df.shape, \n          '\\n------------------------------------------------------------------------------------\\n')\n    \n    print('\\033[1m' + 'All columns from the dataframe :' + '\\033[0m')\n    print(df.columns, \n          '\\n------------------------------------------------------------------------------------\\n')\n    \n    print('\\033[1m' + 'Datatpes and Missing values:' + '\\033[0m')\n    print(df.info(), \n          '\\n------------------------------------------------------------------------------------\\n')\n    \n    print('\\033[1m' + 'Missing value count:' + '\\033[0m')\n    print(df.isnull().sum(),\n          '\\n------------------------------------------------------------------------------------\\n')\n    \n    print('\\033[1m' + 'Summary statistics for the data' + '\\033[0m')\n    print(df.describe(include='all'), \n          '\\n------------------------------------------------------------------------------------\\n')\n    \n    print('\\033[1m' + 'Outliers in the data :' + '\\033[0m')\n    Q1 = df.quantile(0.25)\n    Q3 = df.quantile(0.75)\n    IQR = Q3 - Q1\n    outliers = (df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))\n    print(outliers.sum(), \n          '\\n------------------------------------------------------------------------------------\\n')\n        \n    print('\\033[1m' + 'Memory used by the data :' + '\\033[0m')\n    print(df.memory_usage(), \n          '\\n------------------------------------------------------------------------------------\\n')\n    \n    print('\\033[1m' + 'Number of duplicate values :' + '\\033[0m')\n    print(df.duplicated().sum())\n    \nEDA(df)","b1f2fe1b":"df = df.drop(['Id', 'Messurements', 'Data Url'], axis=1)\ndf","a340138a":"features = df[ \n    ['Weight',\n     'Clarity',\n     'Colour',\n     'Cut',\n     'Polish',\n     'Symmetry',\n     'Fluorescence',\n     'length',\n     'width',\n     'depth',\n     ]\n]\n\nlabels= df['Price']","3b043708":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = \\\ntrain_test_split(pd.get_dummies( features ), labels, test_size=0.2, random_state=0)\n\nX_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=0)\n\nX_train.head()","96974dcb":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nscaler.fit(X_train)\n\n# Scale the training, test, and validation sets\nfeatures = X_train.columns\n\nX_train[features] = scaler.transform(X_train[features])\nX_val[features] = scaler.transform(X_val[features])\nX_test[features] = scaler.transform(X_test[features])\n\nX_train.head()","3adbf555":"def make_corr_map(data, title, zmin=-1, zmax=1, height=600, width= 800):\n    \"\"\"\n    data: Your dataframe.\n    title: Title for the correlation matrix.\n    zmin: Minimum number for color scale. (-1 to 1). Default = -1.\n    zmax: Maximum number for color scale. (-1 to 1). Default = 1.\n    height: Default = 600\n    width: Default = 800\n    \"\"\"\n    \n    data = data.corr()\n    mask = np.triu(np.ones_like(data, dtype=bool))\n    rLT = data.mask(mask)\n\n    heat = go.Heatmap(\n        z = rLT,\n        x = rLT.columns.values,\n        y = rLT.columns.values,\n        zmin = zmin, \n            # Sets the lower bound of the color domain\n        zmax = zmax,\n            # Sets the upper bound of color domain\n        xgap = 1, # Sets the horizontal gap (in pixels) between bricks\n        ygap = 1,\n        colorscale = 'RdBu'\n    )\n\n    title = title\n\n    layout = go.Layout(\n        title_text=title, \n        title_x=0.5, \n        width= width, \n        height= height,\n        xaxis_showgrid=False,\n        yaxis_showgrid=False,\n        yaxis_autorange='reversed'\n    )\n\n    fig= go.Figure(data=[heat], layout=layout)\n    return fig","b583c5f7":"Xy_train = pd.concat([X_train, y_train], axis=1)\n\nmake_corr_map(Xy_train, 'Cool title', height=1500, width=1500)","93988cf6":"from sklearn.ensemble import RandomForestRegressor\n\nrfr = RandomForestRegressor(n_estimators=100, oob_score=True)\nrfr_model = rfr.fit(X_train, y_train)","baf2566b":"plt.figure(figsize= (18, 45))\n\nfeature_importance = rfr.feature_importances_\nindices = np.argsort(feature_importance)\n\n\nplt.yticks(range(len(indices)), [X_train.columns[i] for i in indices])\nplt.barh(range(len(indices)), feature_importance[indices], color='b', align='center')\nplt.show()","99c42ade":"# View accuracy score\nprint('Accuracy for Train:', rfr.score(X_train, y_train) )\nprint('Accuracy for Test:', rfr.score(X_test, y_test) )","a2e4b66f":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\n# Make predictions for the test set\ny_pred = rfr_model.predict(X_test)\n\npred_res =pd.DataFrame({'Actual':y_test, 'Predicted':y_pred})\npred_res","36246cb7":"from sklearn import metrics\n\n# Print out the mean absolute error (mae)\nprint('Mean Absolute Error:', round( metrics.mean_absolute_error(y_test, y_pred),2 ))\nprint('Mean Squared Error:', round( metrics.mean_squared_error(y_test, y_pred), 2))\n\n# Calculate mean absolute percentage error (MAPE)\nerrors = abs(y_pred - y_test)\nmape = 100 * (errors \/ y_test)\n\n# Calculate and display accuracy\naccuracy = 100 - np.mean(mape)\nprint('Accuracy:', round(accuracy, 2), '%.')","c77a90f8":"## Regexing columns and creating new columns","87a63c30":"# Exploratory Data Analysis","a96920e1":"#  Price Prediction ","5475b3d3":"# Feature Analysis","5152e598":"# Data\n## Load data (Diamond\/data_cushion)","67c62d7d":"Validation accuracy is terrible, suggesting the problem of overfitting. This is probably due to applying Random Forest Regressor in a dataset with loads of categorical variable. \n\nIf there are any better suggestions please leave it in the comments would really appreciate the learning experience. "}}