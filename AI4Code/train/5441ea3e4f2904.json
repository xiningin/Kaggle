{"cell_type":{"1649190b":"code","9a0ec848":"code","0ef88906":"code","0ec5b79f":"code","bbcdb33b":"code","26624c23":"code","1ac45fdc":"code","8c34f55d":"code","8fdb6630":"code","f58972d7":"code","55a368d4":"code","94dd205c":"code","bb448b8e":"code","cccc07eb":"code","27b76253":"code","cb6eacef":"code","04491203":"code","d1e85913":"code","c56c2825":"code","c5870e8d":"code","9638aa62":"code","f1588e71":"code","36f6b61f":"code","6c9f9994":"code","39bb6487":"code","2ad7f896":"code","8f3dd6c8":"code","ecc78e68":"code","ea85dc7d":"markdown","e783a51c":"markdown","a9d92200":"markdown","936ca6e9":"markdown","cfe05a66":"markdown","050d9821":"markdown","795d473b":"markdown","b24c41ff":"markdown","4a7eee82":"markdown","9106f72d":"markdown","8f9b2822":"markdown","613a3225":"markdown","0411814c":"markdown","a5832c8d":"markdown"},"source":{"1649190b":"import os\nimport time\n\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\nimport random\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold,StratifiedKFold, GroupKFold\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.initializers import RandomUniform","9a0ec848":"class Config:\n    debug = False\n    competition = \"TPS_202111\"\n    seed = 42\n    n_folds = 5\n    batch_size = 1024\n    epochs = 100","0ef88906":"def seed_everything(seed=Config.seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    tf.random.set_seed(seed)","0ec5b79f":"data_dir = Path('..\/input\/tabular-playground-series-nov-2021')","bbcdb33b":"train_df = pd.read_csv(data_dir \/ \"train.csv\", \n#                        nrows=10000\n                      )\ntest_df = pd.read_csv(data_dir \/ \"test.csv\",\n#                      nrows=1000\n                     )\nsample_submission = pd.read_csv(data_dir \/ \"sample_submission.csv\")\n\nprint(f\"train data: Rows={train_df.shape[0]}, Columns={train_df.shape[1]}\")\nprint(f\"test data : Rows={test_df.shape[0]}, Columns={test_df.shape[1]}\")","26624c23":"train_df.head()","1ac45fdc":"features = [col for col in train_df.columns if col not in ('id', 'target')]","8c34f55d":"scaler = StandardScaler()\n\ntrain_df[features] = scaler.fit_transform(train_df[features])\ntest_df[features] = scaler.transform(test_df[features])","8fdb6630":"y = train_df.target\n\ntest = test_df.drop(columns=[\"id\"], axis=1)\nX = train_df.drop(columns=[\"id\", \"target\"], axis=1)","f58972d7":"x_train, x_valid, y_train, y_valid = train_test_split(X, y,\n                                                      test_size=0.2,\n                                                      random_state=Config.seed)","55a368d4":"import keras_tuner as kt\n\ndef make_model(hp):\n    \n    inputs = keras.Input(shape=(X.shape[1]))\n\n\n    x = layers.Dense(units=hp.Int(\"dense_01\",\n                                      min_value=128,\n                                      max_value=256, \n                                      step=32),\n                         activation='relu')(inputs)\n\n    x = layers.Dropout(\n        hp.Float('dense_dropout', min_value=0., max_value=0.7)\n    )(x)\n    \n    num_block = hp.Int('num_block', min_value=1, max_value=3, step=1)\n\n    for i in range(num_block):\n        x = layers.Dense(units=hp.Int(\"units_\" + str(i),\n                                      min_value=32,\n                                      max_value=256, \n                                      step=32),\n                         activation='relu')(x)\n        x = layers.Dropout(\n          hp.Float('dense_dropout', min_value=0., max_value=0.7)\n        )(x)\n#         x = keras.layers.BatchNormalization()(x)\n\n    outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n    \n    model = keras.Model(inputs, outputs)\n    \n    roc_auc = tf.keras.metrics.AUC(name='roc_auc', curve='ROC')\n\n    model.compile(\n        loss=\"binary_crossentropy\",\n        optimizer=keras.optimizers.Adam(\n            hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n        ),\n        metrics=[roc_auc, \"acc\"]\n    )\n\n    model.summary()\n    return model","94dd205c":"tuner = kt.tuners.RandomSearch(\n    make_model,\n    objective='val_acc',\n    max_trials=100, # 100\n    overwrite=True)\n\ntuner.search_space_summary()","bb448b8e":"callbacks=[keras.callbacks.EarlyStopping(monitor='val_acc',\n                                         mode='max',\n                                         patience=3,\n                                         baseline=0.9)]\n\n# Same format as model.fit()\ntuner.search(x_train, y_train, \n             validation_split=0.2, \n             callbacks=callbacks,\n             batch_size=Config.batch_size,\n             verbose=1, \n             epochs=100) #100","cccc07eb":"tuner.results_summary()","27b76253":"best_hp = tuner.get_best_hyperparameters()[0] # Best hyperparameters\nbest_model = make_model(best_hp)\nprint(\"=\"*20, \" Best Model \", \"=\"*20)\nbest_model.summary()","cb6eacef":"# best_model.save(\"best_model\")","04491203":"best_hp = tuner.get_best_hyperparameters()[0]\n\nprint(f\"\"\"\nThe hyperparameter search is complete. The optimal number of units in the first densely-connected\nlayer is {best_hp.get('dense_01')} and the optimal learning rate for the optimizer\nis {best_hp.get('learning_rate')}.\n\"\"\")\n\nprint(f\"First Dense Layer: {best_hp.get('dense_01')}\")\nprint(f\"Zero Layer: {best_hp.get('units_0')}\")\nprint(f\"one Layer: {best_hp.get('units_1')}\")\nprint(f\"Second Layer: {best_hp.get('units_2')}\")\n# print(f\"Third Layer: {best_hp.get('units_3')}\")\n# print(f\"4th Layer: {best_hp.get('units_4')}\")\n\nprint(f\"Best Learning Rate: {best_hp.get('learning_rate')}\")","d1e85913":"best_hp = tuner.get_best_hyperparameters()[0]\nmodel = make_model(best_hp)\nhistory = model.fit(x_train, y_train, validation_split=0.2, epochs=50)","c56c2825":"val_acc_per_epoch = history.history['val_acc']\nbest_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\nprint(f\"Best epoch: {best_epoch}\")","c5870e8d":"best_model = make_model(best_hp)\nbest_model.fit(x_train, y_train, epochs=best_epoch)","9638aa62":"preds_valid = best_model.predict(x_valid)","f1588e71":"auc = roc_auc_score(y_valid,  preds_valid)\nprint(f\"Validation AUC Score: {auc}\")","36f6b61f":"seed_everything()\n\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\nhistories = []\n\nkf = StratifiedKFold(n_splits=Config.n_folds, random_state=Config.seed, shuffle=True)\n\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(X = X, y = y)):\n    print(10*\"=\", f\"Fold={fold+1}\", 10*\"=\")\n    start_time = time.time()\n\n    x_train = X.loc[train_idx, :]\n    x_valid = X.loc[valid_idx, :]\n    \n    y_train = y[train_idx]\n    y_valid = y[valid_idx]\n    \n#     model = build_model02_swish(x_shape=(X.shape[1],))\n    best_model = make_model(best_hp)\n\n    early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_auc\",\n                                                      mode='max',\n                                                      verbose=1,\n                                                      restore_best_weights=True,\n                                                      patience=3)\n    \n    lr_scheduler_cb = keras.callbacks.ReduceLROnPlateau(\n        monitor='val_loss', \n        factor=0.2,\n        patience=10,\n        mode='min'\n    )\n    \n    history = model.fit(X, y,\n              callbacks=[early_stopping_cb, lr_scheduler_cb],\n                  batch_size=Config.batch_size,\n              validation_data=(x_valid, y_valid),\n              epochs=Config.epochs\n             )\n    histories.append(history)\n\n    # Predictions for OOF\n    print(\"--- Predicting OOF ---\")\n    preds_valid = model.predict(x_valid)[:, -1]\n    final_valid_predictions.update(dict(zip(valid_idx, preds_valid)))\n    \n    auc = roc_auc_score(y_valid,  preds_valid)\n    scores.append(auc)\n\n    run_time = time.time() - start_time\n    \n    # Predictions for Test Data\n    print(\"--- Predicting Test Data ---\")\n    test_preds = model.predict(test_df[features])[:, -1]\n    final_test_predictions.append(test_preds)\n    print(f\"Fold={fold+1}, auc: {auc:.8f}, Run Time: {run_time:.2f}\")\n","6c9f9994":"print(f\"Scores -> Adjusted: {np.mean(scores) - np.std(scores):.8f} , mean: {np.mean(scores):.8f}, std: {np.std(scores):.8f}\")","39bb6487":"def plot_history(history, metric, val_metric, title):\n\n    loss = history.history[metric]\n    val_loss = history.history[val_metric]\n\n    epoch = history.epoch\n\n    plt.figure(figsize=(11, 4))\n    \n    plt.plot(epoch, loss, label=metric, color=\"r\")\n    plt.plot(epoch, val_loss, label=val_metric, color=\"b\")\n\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.grid(True)\n    plt.title(f\"Training and Validation {title}\")\n\n    plt.show()\n","2ad7f896":"history.history.keys()","8f3dd6c8":"for fold, h in enumerate(histories):\n    print(20*'=', f\"Fold = {fold+1}\", 20*'=')\n\n    plot_history(h, \"acc\", \"val_acc\", \"Accuracy\")\n\n    plot_history(h, \"loss\", \"val_loss\", \"Loss\")\n    plot_history(h, \"roc_auc\", \"val_roc_auc\", \"AUC\")\n\n    plt.show()\n","ecc78e68":"sample_submission['target'] = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.to_csv(\"test_pred_2.csv\",index=None)\nsample_submission.to_csv(\"submission.csv\",index=None)\nsample_submission","ea85dc7d":"# Cross Validation","e783a51c":"# Scores","a9d92200":"# Standardize\/Normalize the Data","936ca6e9":"# Train the production model","cfe05a66":"# History","050d9821":"# Feature Engineering","795d473b":"# Predict Validation","b24c41ff":"# Best Model","4a7eee82":"# Submission File","9106f72d":"# Configuration","8f9b2822":"# Find the best epoch value","613a3225":"# Keras Tuner\n\nUse the tuner package to determine the best model.\n\n# Versions\n\n- V2: Switching from BatchNormalization to Dropout\n- V1: Original - build_model01()\n\n# References\n\n- https:\/\/www.kaggle.com\/fchollet\/keras-kerastuner-best-practices\n- https:\/\/www.kaggle.com\/fchollet\/moa-keras-kerastuner-best-practices\n- https:\/\/keras.io\/keras_tuner\/\n- https:\/\/keras.io\/guides\/keras_tuner\/getting_started\/","0411814c":"# Models","a5832c8d":"# Extract Target and Drop Unused Columns"}}