{"cell_type":{"d66635dc":"code","e09624ec":"code","c6c60f63":"code","9f6c71e8":"code","a30a8598":"code","b91bbdf0":"code","8102193c":"code","6be6eb8f":"code","d1afb7b8":"code","7c7f8bfb":"code","be277232":"code","d607aa16":"code","e0e2f8b9":"code","9cb543df":"code","0259fe46":"code","39b0286a":"code","22b3bf19":"code","dba46a3a":"code","d0038079":"code","100e9f71":"code","ba11a46f":"code","e96968c9":"code","6fcc2d78":"code","c1b8c375":"code","5fd646b8":"code","5cc41704":"code","19aed6ef":"code","d18dee78":"code","ee8e12a2":"code","f4589fdf":"code","c9225b64":"code","05ad9bb8":"code","15f1c33c":"code","5e97a9c5":"code","81108f1a":"code","49dcf7dd":"code","b9c8cbca":"code","939f6063":"code","c6ae0ba3":"markdown","3a5ad2a0":"markdown","8a04e2e1":"markdown","c7bde895":"markdown","0ffd7f9c":"markdown","e15c8861":"markdown","3d3ab7cf":"markdown"},"source":{"d66635dc":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport nltk\nnltk.download('punkt')\n\n\nfrom wordcloud import WordCloud , STOPWORDS\nimport re\n\nfrom nltk.stem import PorterStemmer , WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize , sent_tokenize\nimport gensim\nfrom gensim.utils import simple_preprocess","e09624ec":"df_true = pd.read_csv('..\/input\/dataminingproject2\/True.csv')\ndf_false = pd.read_csv('..\/input\/dataminingproject2\/Fake.csv')","c6c60f63":"df_true.head()","9f6c71e8":"df_false.head()","a30a8598":"df_true.isnull().sum()","b91bbdf0":"df_false.isnull().sum()","8102193c":"df_true.info()","6be6eb8f":"df_true['isfake'] = 0\n\ndf_false['isfake'] = 1","d1afb7b8":"df_false.head()","7c7f8bfb":"df = pd.concat([df_true , df_false]).reset_index(drop = True)\ndf.head()","be277232":"df.drop(columns = ['date'] , inplace=True)","d607aa16":"df.head()","e0e2f8b9":"df['original'] = df['title'] + ' ' + df['text']\ndf.head()","9cb543df":"df['original'][0][:200]","0259fe46":"nltk.download('stopwords')","39b0286a":"from nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nstop_words.extend(['from' , 'subject' , 're' , 'usa' , 'use' , 'edu'])","22b3bf19":"def preprocess(text):\n    \n    result = []\n    for token in gensim.utils.simple_preprocess(text):\n        \n        if (token not in gensim.parsing.preprocessing.STOPWORDS and \n        len(token) > 3 and \n        token not in stop_words):\n            \n            result.append(token)\n\n    return result","dba46a3a":"df['clean'] = df['original'].apply(preprocess)","d0038079":"df.head()","100e9f71":"list_of_words = []\nfor sent in df.clean:\n    for word in sent:\n        list_of_words.append(word)","ba11a46f":"print(len(list_of_words))","e96968c9":"# obtaining only the unique words \n\ntotal_words = len(list(set(list_of_words)))\nprint(total_words)","6fcc2d78":"df['clean_joined'] = df['clean'].apply(lambda x : \" \".join(x))","c1b8c375":"df.head()","5fd646b8":"#plotting how many subjects do we have in our dataset\n\nplt.figure(figsize = (12 , 12))\nsns.countplot(x = 'subject' , data = df)","5cc41704":"plt.figure(figsize = (8,8))\nsns.countplot(x = 'isfake' , data = df)","19aed6ef":"maxlen = -1\n\nfor doc in df.clean_joined:\n    tok_list = nltk.word_tokenize(doc) # nltk.word_tokenize convert string to a list containig the words in the string\n    if len(tok_list) > maxlen:\n        maxlen = len(tok_list)\n\nprint('the longest doc lenght is : ' , maxlen)","d18dee78":"from sklearn.model_selection import train_test_split\n\nx_train , x_test , y_train , y_test = train_test_split(df.clean_joined , df.isfake , test_size = 0.2)","ee8e12a2":"# from nltk import word_tokenize","f4589fdf":"tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=total_words)\ntokenizer.fit_on_texts(x_train)\ntrain_sequences = tokenizer.texts_to_sequences(x_train)\ntest_sequences = tokenizer.texts_to_sequences(x_test)","c9225b64":"train_sequences[1]","05ad9bb8":"padded_train = tf.keras.preprocessing.sequence.pad_sequences(train_sequences , \n                                                             maxlen=maxlen , \n                                                             padding = 'post' , \n                                                             truncating = 'post')\n\npadded_test = tf.keras.preprocessing.sequence.pad_sequences(test_sequences ,\n                                                            maxlen = maxlen,\n                                                            padding = 'post',\n                                                            truncating = 'post')","15f1c33c":"padded_train[0]","5e97a9c5":"from tensorflow.keras.layers import Dense , Bidirectional , Embedding , LSTM\nfrom tensorflow.keras.models import Sequential\n\nmodel = Sequential()\n\nmodel.add(Embedding( total_words , output_dim = 128 , input_length=maxlen))\n\nmodel.add(Bidirectional(LSTM(128)))\nmodel.add(Dense(128 , activation = 'relu'))\nmodel.add(Dense(1 , activation = 'sigmoid'))\n\nmodel.compile(optimizer = 'adam' , loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n\nmodel.summary()","81108f1a":"y_train = np.asarray(y_train)\n\nmodel.fit(padded_train , y_train , batch_size=128 , epochs = 2 , validation_split=0.2)","49dcf7dd":"pred = model.predict(padded_test)\n\nprediction = []\nfor i in range(len(pred)):\n    if pred[i] > 0.5:\n        \n        prediction.append(1)\n    else:\n        prediction.append(0)","b9c8cbca":"from sklearn.metrics import accuracy_score\n\naccuracy = accuracy_score(list(y_test) , prediction)\n\nprint('the accuracy : ' , accuracy)","939f6063":"from sklearn.metrics import confusion_matrix\n\nconfusion_matrix(list(y_test) , prediction)","c6ae0ba3":"# feature Engineering","3a5ad2a0":"# build and train the model","8a04e2e1":"# padding","c7bde895":"# importing libraries","0ffd7f9c":"# data cleaning","e15c8861":"# tokenization and padding","3d3ab7cf":"# testing the model"}}