{"cell_type":{"695e96c1":"code","babe7b75":"code","63890632":"markdown","6fe0f3fb":"markdown","ba162cd7":"markdown","46400dd6":"markdown","048abf14":"markdown","7a1fa6ef":"markdown","d25267b9":"markdown","1837c841":"markdown","db4baf15":"markdown","5406cc0a":"markdown","ab0866cd":"markdown","0cf7ee49":"markdown","76a0e541":"markdown","f47c0eed":"markdown","4fd2e3e2":"markdown","268910de":"markdown","560a6465":"markdown","42485723":"markdown","2869e60e":"markdown","4b60c22a":"markdown","3cf1066f":"markdown","262397a2":"markdown","c631135d":"markdown","ea556589":"markdown","20d3fe90":"markdown","d4911e92":"markdown","a9dd8870":"markdown","28d805a3":"markdown","2456fb17":"markdown","4053963f":"markdown","bf1dc93b":"markdown","0c99587e":"markdown","3f052bb8":"markdown","e7d006fb":"markdown","de4feb07":"markdown","9ac72eca":"markdown","5187b986":"markdown","06d7f1db":"markdown","625f2d96":"markdown","ed5db90e":"markdown","eea8f31a":"markdown","23597b53":"markdown","bb9fefd6":"markdown","7ca83054":"markdown","e6f1c4e6":"markdown","e60eb282":"markdown","01f6f76b":"markdown","a64b59bb":"markdown","1debf890":"markdown","e01bcb4e":"markdown","bef56024":"markdown","3c5626e8":"markdown","ee0e3dce":"markdown","9b8b36c4":"markdown","fdb913ac":"markdown","c90e3bd6":"markdown","7a5883a3":"markdown","ae509945":"markdown","8930b461":"markdown","ed9103f7":"markdown","5524e98d":"markdown","f803101c":"markdown","e2ed9d06":"markdown","048e2d9b":"markdown","e08ff2d9":"markdown","ae911430":"markdown","4c5d28b7":"markdown","26f65f8b":"markdown","b23f5c12":"markdown","69294a5a":"markdown","88376500":"markdown","55151f65":"markdown","a74d1267":"markdown","fa266699":"markdown","aa0f12e1":"markdown","ed2baab9":"markdown","cd670967":"markdown","bc18d466":"markdown","a44c7333":"markdown"},"source":{"695e96c1":"# The continue statement rejects all the remaining statements in the current iteration of the loop and \n# moves the control back to the top of the loop.\nfor i in range(1, 51):    \n    if (i%3 == 0 and i%5 == 0):\n        print(\"FizzBuzz\")\n        continue\n    if i%3 == 0:\n        print(\"Fizz\")\n        continue\n    if i%5 == 0:\n        print(\"Buzz\")\n        continue\n    print(i)        ","babe7b75":"import math\n\n# define the points\np1 = [6,5]\np2 = [3, 2]\n\neuclidean_distance = math.sqrt( (p1[0]-p2[0])**2 + (p1[1]-p2[1])**2 )\nprint(euclidean_distance)","63890632":"# Explain the difference between train, validation and test set\n* Training set is used for model training \n* Validation set is used for model fine tuning\n* Test set is used for model testing. i.e. evaluating the models predictive power and generalization.","6fe0f3fb":"# Explain Random forest algorithm\n* Random forest is supervised learning algorithm and can be used to solve classification and regression problems. \n* Since decision-tree create only one tree to fit the dataset, it may cause overfitting and model may not generalize well. Unlike decision tree random forest fits multiple decision trees on various sub samples of dataset and make the predictions by averaging the predictions from each tree. \n* Averaging the results from multiple decision trees help to control the overfitting and results in much better prediction accuracy. As you may have noticed, since this algorithm uses multiple trees hence the name \u2018Random Forest\u2019\n* Reference: [Random Forest](https:\/\/satishgunjal.com\/random_forest\/)","ba162cd7":"# What are recommender systems?\n* The purpose of a recommender system is to suggest relevant items or services to users.\n* Two major categories of recommender systems are collaboarative filtering and cotent based filtering methods\n\n### Collaborative Filtering\n* It is based on the past interactions recorded between users and items in order to produce new recommendations.\n* e.g. Music service recommends track that are often played by other users with similar interests\n\n### Content Based Filtering\n* Unlike collaborative methods that only rely on the user-item interactions, content based approaches use additional information about the content consumed by the user to produce new recommendations\n* e.g. Music service recommends new song based on properties of the songs user listens to.\n\nReference: https:\/\/youtu.be\/5JZsSNLXXuE","46400dd6":"# What are confouding varaiables?\n* In statistics, confounder is a variable that influences both the dependent variable and independent avriable.\n* If you are researeching whether a lack of exercise leads to weight gain. In this case 'lack of exercise' is independent variable and 'weight gain' is dependent variable. A confounding varaible in this case would be 'age' which affect both of these variables.","048abf14":"# If it rains on saturday with probability 0.6, and it rains on sunday with probability 0.2, what is the probability that it rains this weekend?\n* Since we know the probability of rain on Saturday and Sunday, the probability of raining on Weekend is combination of both of these events.\n* Trick here is to know the probability of not raining on Saturday and Sunday.\n* If we subtract the intersection(\u2229) of both the events of not raining on Saturday and Sunday from total probability then we get the probability of raining on weekend.\n ```\n = Total probability - (Probability that it will not rain on Saturday) \u2229 (Probability that it will not rain on Sunday)\n = 1 - (1 - 0.6)*(1 - 0.2)\n = 0.68\n ```\n * Reference: https:\/\/youtu.be\/5JZsSNLXXuE\n","7a1fa6ef":"# What is the difference between Bar graph and histogram?\n\n* Bar graph is used for descreate data where as histogram is used for continuous data.\n* In bar graph there is space between the bars and in case of histogram there is no space between the bars(contnuous scale).\n* In bar graph the order of the bars can be changed and in histogram order remains same.\n","d25267b9":"# Explain flase positive an false negative with examples.\n* A false positive is where you receive a positive result for a test, when you should have received a negative results. It\u2019s sometimes called a \u201cfalse alarm\u201d or \u201cfalse positive error.\u201d It\u2019s usually used in the medical field, but it can also apply to other arenas (like software testing). \n* Some examples of false positives:\n    - A pregnancy test is positive, when in fact you aren\u2019t pregnant.\n    - A cancer screening test comes back positive, but you don\u2019t have the disease.\n    - A prenatal test comes back positive for Down\u2019s Syndrome, when your fetus does not have the disorder(1).\n    - Virus software on your computer incorrectly identifies a harmless program as a malicious one.\n* False positives can be worrisome, especially when it comes to medical tests. Researchers are consistently trying to identify reasons for false positives in order to make tests more sensitive.\n* A related concept is a false negative, where you receive a negative result when you should have received a positive one. For example, a pregnancy test may come back negative even though you are in fact pregnant.\n* Reference: https:\/\/www.statisticshowto.com\/false-positive-definition-and-examples\/","1837c841":"# What is regularization. Why it is usefull?\n* Regularization is the process of adding tunning parameter(penalty term) to a model to induce smoothness in order to prevent overfitting.\n* The tunning parameter controls the excessively fluctuating function in such a way that coefficients dont take extreame values.\n* There are two types of regularization as follows:\n    - L1 Regularization or Lasso Regularization.\n    L1 Regularization or Lasso Regularization adds a penalty to the error function. The penalty is the sum of the absolute values of weights.\n    - L2 Regularization or Ridge Regularization.\n    L2 Regularization or Ridge Regularization also adds a penalty to the error function. But the penalty here is the sum of the squared values of weights.","db4baf15":"# Difference between univariate, bivariate and multivariate analysis?\n\n* Univariate Analysis\n\n![Univariate_Analysis](https:\/\/raw.githubusercontent.com\/satishgunjal\/images\/master\/Univariate_Analysis.PNG)\n\n* Bivariate Analysis\n\n![Bivariate_Analysis](https:\/\/raw.githubusercontent.com\/satishgunjal\/images\/master\/Bivariate_Analysis.PNG)\n\n* Multivariate Analysis\n\n![Multivariate_Analysis](https:\/\/raw.githubusercontent.com\/satishgunjal\/images\/master\/Multivariate_Analysis.PNG)","5406cc0a":"# Explain normal distribution of data\nData can be distributed (spread out) in different ways,\n* It can be spread out more on the left\n\n![](https:\/\/www.mathsisfun.com\/data\/images\/normal-distribution-skew-left.gif)\n* More on the right\n\n![](https:\/\/www.mathsisfun.com\/data\/images\/normal-distribution-skew-right.gif)\n* It can be all jumbled up\n\n![](https:\/\/www.mathsisfun.com\/data\/images\/normal-distribution-random.gif)\n\n* But there are many cases where the data tends to be around a central value with no bias left or right, and it gets close to a \"Normal Distribution\" like this:\n\n![](https:\/\/www.mathsisfun.com\/data\/images\/normal-distribution-1.svg)\n\n* The Normal Distribution has:\n    - mean = median = mode\n    - symmetry about the center\n    - 50% of values less than the mean and 50% greater than the mean\n\nReference: https:\/\/www.mathsisfun.com\/data\/standard-normal-distribution.html","ab0866cd":"# What is TF-IDF?\n* TF-IDF is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. This is done by multiplying two metrics: how many times a word appears in a document, and the inverse document frequency of the word across a set of documents\n* It is used in information retrieval and text mining\n* TF-IDF (term frequency-inverse document frequency) was invented for document search and information retrieval. It works by increasing proportionally to the number of times a word appears in a document, but is offset by the number of documents that contain the word. So, words that are common in every document, such as this, what, and if, rank low even though they may appear many times, since they don\u2019t mean much to that document in particular.\n* However, if the word Bug appears many times in a document, while not appearing many times in others, it probably means that it\u2019s very relevant. For example, if what we\u2019re doing is trying to find out which topics some NPS responses belong to, the word Bug would probably end up being tied to the topic Reliability, since most responses containing that word would be about that topic.\n* Reference: https:\/\/monkeylearn.com\/blog\/what-is-tf-idf\/","0cf7ee49":"# What is the difference between machine learning and deep learning?\nDeep Learning out performs traditional ML techniques if the data size is large. But with small data size, traditional Machine Learning algorithms are preferable. Deep Learning really shines when it comes to complex problems such as image classification, natural language processing, and speech recognition. Few important differences are as below,\n\n|Machine Learning|Deep Learning|\n|:-|:-|\n| Machine learning uses algorithms to parse data, learn from that data, and make informed decisions based on what it has learned|Deep learning structures algorithms in layers to create an \"artificial neural network\u201d that can learn and make intelligent decisions on its own|\n|Using handcrafted rules and feature engineering, ML algorithms can work well with small data. But its performance plateau once data increases.|Deep Learning algorithms need large data to understand it perfectly. Deep learning performances increases as data increases.|\n|Traditional ML algorithms can work on less computing power.|DL algorithms need high compute. There also special purpose compute for DL like GPu and TPU.|\n|In case of ML domain experts\/Data scientists needs to do feature engineering in order to enable model o learn all data patterns.|One advantage with DL that it learns high level features from data.No extrnal feature engineering is required.|\n|ML models take less time to train|DL models take more time to train|\n| Ml models are easy to interpret as comare to DL models.|DL models are black box and its very difficult to interpret the results.|","76a0e541":"# Lis the differences between supervised and unsupervised learning\n\n| Supervised learning | Unsupervised leanring\n|:- |:-\nUses labeled data as input | Uses unlabeled data as input\nSupervised learning has feedback mechanism | Unsupervised learning has no feedback mechanism\nCommon supervised learning algorithms are decision tree, logistic regression, support vector machine etc | K Means clustering, hierarchical clustering etc\n\n\nReference: https:\/\/youtu.be\/5JZsSNLXXuE","f47c0eed":"# What is null hypothesis and alternate hypothesis?\n* The null hypothesis states that a population parameter (such as the mean, the standard deviation, and so on) is equal to a hypothesized value. The null hypothesis is often an initial claim that is based on previous analyses or specialized knowledge.\n* The alternative hypothesis states that a population parameter is smaller, greater, or different than the hypothesized value in the null hypothesis. The alternative hypothesis is what you might believe to be true or hope to prove true.\n* So when running a hypothesis test\/experiment, the null hypothesis says that there is no difference or no change between the two tests. The alternate hypothesis is the opposite of the null hypothesis and states that there is a difference between the two tests.","4fd2e3e2":"# Write the equations for the precision and recall?\n\nPrecision = True Positive \/ (True Positive + False Positive)\n\nRecall = True Positive \/(Total Positive + False Negative)","268910de":"# Suppose we have a function -4x^2 + 4x + 3. Find the maximum or minimum of this function.\n* This is quadratic equation,\n  f(x) = -4x^2 + 4x + 13 (for a function: ax^2 + bx + c, when a < 0, then function has maximum value)\n* To find the slope of the function, lets take derivative of it\n\n  f'(x)= -8x + 4\n  \n* At maximum point, slope will be 0\n\n  -8x + 4 = 0\n  \n  x = 0.5\n  \n* Now lets put 0.5 in equation to find the maximum values\n\n  f(0,5) =  -4(0.5)^2 + 4(0.5) + 13 = -1 + 2 +13 = 14\n  \n* This functiona will have concave shape. So the maximum point is (0.5, 14)  \n\n* Reference: https:\/\/www.youtube.com\/watch?v=lRAgottY8XU&list=PLjW9PIyfCennBOprV3CPoqMX8SW-qNlUa","560a6465":"# What are feature selection methods to select right variables?\nFeature selection is the process of reducing the number of input variables when developing a predictive model. There are two methods for feature selection. Filter method and wrapper methods. Best analogy for selecting features is bad data in bad answers out.\n\n### Filter Methods\n* Filter feature selection methods use statistical techniques to evaluate the relationship between each input variable and the target variable, and these scores are used as the basis to choose (filter) those input variables that will be used in the model.\n* These methods are faster and less computationally expensive than wrapper methods.\n\n#### Information Gain\n\nInformation gain calculates the reduction in entropy from the transformation of a dataset. It can be used for feature selection by evaluating the Information gain of each variable in the context of the target variable.\n\n#### Chi-square Test\nThe Chi-square test is used for categorical features in a dataset. We calculate Chi-square between each feature and the target and select the desired number of features with the best Chi-square scores. \n\n#### Correlation Coefficient\nCorrelation is a measure of the linear relationship of 2 or more variables. Through correlation, we can predict one variable from the other. The logic behind using correlation for feature selection is that the good variables are highly correlated with the target. Furthermore, variables should be correlated with the target but should be uncorrelated among themselves.\n\n### Wrapper Methods\n* Wrapper feature selection methods create many models with different subsets of input features and select those features that result in the best performing model according to a performance metric.\n* These methods are unconcerned with the variable types, although they can be computationally expensive.\n* The wrapper methods usually result in better predictive accuracy than filter methods.\n\n#### Forward Feature Selection\nThis is an iterative method wherein we start with the best performing variable against the target. Next, we select another variable that gives the best performance in combination with the first selected variable. This process continues until the preset criterion is achieved.\n\n#### Backward Feature Elimination\nThis method works exactly opposite to the Forward Feature Selection method. Here, we start with all the features available and build a model. Next, we remove the variable from the model which gives the best evaluation measure value. This process is continued until the preset criterion is achieved.\n\n#### Exhaustive Feature Selection\nThis is the most robust feature selection method covered so far. This is a brute-force evaluation of each feature subset. This means that it tries every possible combination of the variables and returns the best performing subset.\n\nReference: https:\/\/www.analyticsvidhya.com\/blog\/2020\/10\/feature-selection-techniques-in-machine-learning\/","42485723":"# What is bias-variance tradeoff\n* As we increase the complexity of the model, error will reduce due to lower bias in the model. However, this will happen until a particular point. If we continue to make our model complex then model will overfit and lead to high variance.\n* The goal of any supervised ML algorithm to have low bias and low variance to achieve good prediction performance. This is referred as bias-variance tradeoff. We can acheive bias-variance tradeoff by selecting optimum model complexity.\n\n<img src=\"https:\/\/raw.githubusercontent.com\/satishgunjal\/images\/master\/Bias_and_variance_contributing_to_total_error.png\" width=\"500\" height=\"400\">\n\n**We can also use hyperparamters to adjust model complexity, few examples are as below**\n\n* The K-NN algorithm has low bias and high variance, tradeoff can be achieved by increasing the value of 'K'. \n    * Higher the value of 'K' means higher the number of neighbours, which in turn increases the bias of the model.\n* The SVM algorithm has low bias and high variance, trade off can be achived by changing the 'C' paramter.\n    * The C parameter tells the SVM optimization how much you want to avoid misclassifying each training example. \n    * For large values of C, the optimization will choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the training points classified correctly. \n    * Conversely, a very small value of C will cause the optimizer to look for a larger-margin separating hyperplane, even if that hyperplane misclassifies more points. \n    * For very tiny values of C, you should get misclassified examples, often even if your training data is linearly separable.\n* The decision trees has low bias and high variance, bias-variance tradeoff can be achived by changing the tree depth.\n    * If the tree is shallow then we're not checking a lot of conditions\/constrains i.e. the logic is simple or less complex, hence it automatically reduces over-fitting. This introduces more bias compared to deeper trees where we overfit the data. It can be imagined as we're deliberately not calculating more conditions means we're making some assumption (introduces bias) while creating the tree.\n* The linear regression has low variance and high bias, bias-variance tradeoff can be acheived by increasing the number of features or by using another regression technique that can fit data better.\n    * If data is not linearly separable then linear regression algorithm will result in low variance and highj bias.","2869e60e":"# Why do we need confusion matrix?\n* We can not rely on a single value of accuracy in classification when the classes are imbalanced. \n* For example, we have a dataset of 100 patients in which 5 have diabetes and 95 are healthy. However, if our model only predicts the majority class i.e. all 100 people are healthy then also we will have a classification accuracy of 95%.\n* Confusion matrices are used to visualize important predictive analytics like recall, specificity, accuracy, and precision. \n* Confusion matrices are useful because they give direct comparisons of values like True Positives, False Positives, True Negatives and False Negatives.","4b60c22a":"# How can AI be used in spam email detection?\n* First we collect spam and ham email data\n* Then we find the statistical relations between words to create feature matrix to train the classification models.\n* Trained classification models can be used to determine whether a piece of text belongs to a certain class.\n* We can use algorithms like Na\u00efve Bayes, RNN and transformers for spam detection","3cf1066f":"# Explain dimensionality reduction, and its benefits?\n* Dimensionality reduction referes to the process of converting a set of data having vast dimensions into data with lesser dimensions(features) to convey similar information concisely.\n* It helps in data compressing and reducing the storage space\n* It reduces computation time as less dimensions lead to less computing\n* It removes redundant features. E.g. There is no point in storing value in two different units\n* Reference: https:\/\/youtu.be\/5JZsSNLXXuE","262397a2":"# Below is the output of a correlation matrix from your Exploratory data. Is using all the features in a model appropriate for predicting\/inferencing Y?\n![](https:\/\/raw.githubusercontent.com\/satishgunjal\/images\/master\/Correlation_Matrix.PNG)\n    \n* We can see from above correlation matrix that there is high correlation(.98) between X1 and X2, also high correlation(.88) between X1 and X3, similarly there is high correlation(.75) between X2 and X3\n* All the variables are correlated to each other. In regression this would result in multicollinearity. We can try methods such as dimension reduction, feature selection, stepwise regression to choose the correct input variables for predictiong Y\n* Second part of question is - should we use all the variables for modeling?\n  - Using multicolnear feature in modeling doesnt help. We should remove all the multicolnear feature and keep unique feature so that explaining the model predictions also becomes easy.\n  - It will also make model less complex and we dont have to store many features.\n\n## Prediction vs Inference\nInference and prediction are two often confused terms, perhaps in part because they are not mutually exclusive.\n![](https:\/\/storage.ning.com\/topology\/rest\/1.0\/file\/get\/4901412886?profile=original)\nReference: https:\/\/www.datasciencecentral.com\/profiles\/blogs\/inference-vs-prediction-in-one-picture\n","c631135d":"# You are given a dataset consisting of variables having more than 30% missing values? How will you deal with them?\n* There are multiple ways to handle missing values in the data\n* If dataset is huge we can simply remove the rows containing the missing data\n* If dataset is small then we have to impute the missing values. There are multiple ways to impute the missing values. In case of categorical data we may use the most common values and in case numerical data we can use mean, median etc.\n* Reference: https:\/\/youtu.be\/5JZsSNLXXuE","ea556589":"# Explain inner working on linear regression\nhttps:\/\/satishgunjal.com\/univariate_lr\/","20d3fe90":"# Explain inner working on logistic regression\nhttps:\/\/satishgunjal.com\/binary_lr\/","d4911e92":"# What is power of hypothesis test? Why is it important?\n\n* Remember that if actual value is positive and our model predicts it as negative then Type II error occuras (False negative). e.g. Calling a guilty person innocent, diaognosing cancer infected person as healthy etc. \n* The probability of not commiting Type II error is called as power of hypothesis test. The higher probability we have of not commiting a type 2 error, the better our hypothesis test is.\n\n\n## More Info: Basic Concepts of Hypothesis Testing\n\n* In simple term hypothesis is a assumption. Since its assumption, after our testing it can hold true may not.\n* If our assumption holds true after testing then it is termed as 'Null Hypothesis' unless there is evidence against it.\n* If our assumption dont hold true and there is claim aganist it, then it is termed as alternate hypothesis.\n* So when our assumption dont hold true then Type I error occurs. Here we are going against the Null Hypothesis.\n* p-value: It is calculated probability of making type I error\n* Reference: https:\/\/www.youtube.com\/watch?v=d0eVIUyt_Uc","a9dd8870":"# You fit two linear models on a dataset. Model 1 has 25 predictors and model 2 has 10 predictors. What performance metric would you use to select the best model based on training dataset?\n\n* First of all model performace is not directly proportional to the number of predictors, so we cant say that model with 25 predictors is better than the model with 10 predictors\n* Here important thing is to understand different evaluation metric for linear regresion and which one of them can help us identify the impact of number of predictors on model performance.\n* Evaluation metric used for linear regression are MSE, MAE, R-squared, Adjusted R-squared, and RMSE.\n* MSE penalizes large errors, MAE does not penalize large errors, RMSE penalizes large errors and R-squared or Coefficient of Determination represent the strength of the relationship between your model and the dependent variable.\n* Though R-squared represent the strength of relationship between model and the dependent variables, it is never used for comparing the models as the value of R\u00b2 increases with the increase in the number of predictors (even if these predictors do not add any value to the model)\n* Now only remaining metric is **Adjusted R-squared**. Unlike R-squared, Adjusted R-squared measures variation explained by only the independent variables that actually affect the dependent variable.\n* So the Adjusted R-squre score will increase only if addition of predictors improve the models performance significantly or else it will decrease. Hence correct answer is Adjusted R-squared\n\n* Reference: https:\/\/www.youtube.com\/watch?v=lRAgottY8XU&list=PLjW9PIyfCennBOprV3CPoqMX8SW-qNlUa","28d805a3":"# After studying the behaviour of population, you have identified four specific individual types who are valueable to your study. You would like find all users who are most similar to each indivdual type. Which algorithm is most approprate for this study?\n```\n- K-means clstering\n- Linear regression\n- Associate rules\n- Decision tress\n```\n\nAnswer is : K-means clustering\n\nReference: https:\/\/youtu.be\/5JZsSNLXXuE","2456fb17":"# Write a SQL query to list all orders with customer information\n![SQL_Join](https:\/\/raw.githubusercontent.com\/satishgunjal\/images\/master\/SQL_Join.PNG)","4053963f":"# How should you maintain your deployed model?\n\n### Monitor\nConstant monitoring of all the models is needed to determine the performance accuracy of the models\n\n### Evaluate\nEvaluation metric of the current model is calculated to determine if new algorithm is needed.\n\n### Compare\nThe new models are compared against each other to determine which model performs the best.\n\n###  Rebuild\nThe best performing model is re-built on the current set of data.\n\nReference: https:\/\/youtu.be\/5JZsSNLXXuE","bf1dc93b":"# What is the angle between the hour and minute hands of clock when the time is half past six?\n![Clock_Puzzle](https:\/\/raw.githubusercontent.com\/satishgunjal\/images\/master\/Clock_Puzzle.PNG)\nReference: https:\/\/youtu.be\/5JZsSNLXXuE","0c99587e":"# Explain the scenario where both false positive and false negative are equally important\n* In banking industry giving loans is the primary source of making money, but at the same time bank can make profit if the repayment rate is good.\n* Bank always try not loose good customers and avoid bad ones. In this case false positive and false negative becomes very important to measure.","3f052bb8":"# How can you select K for K-Means?\nThere are two ways to select the number of clusters in case K-Means clustering algorithm\n\n### Visualization\n* To find the number of clusters manually by data visualization is one of the most common method. \n* Domain knowledge and proper understanding of given data also help to make more informed decisions. \n* Since its manual exercise there is always a scope for ambiguous observations, in such cases we can also use \u2018Elbow Method\u2019\n\n### Elbow Method\n* In Elbow method we run the K-Means algorithm multiple times over a loop, with an increasing number of cluster choice(say from 1 to 10) and then plotting a clustering score as a function of the number of clusters. \n* Clustering score is nothing but sum of squared distances of samples to their closest cluster center. \n* Elbow is the point on the plot where clustering score (distortion) slows down, and the value of cluster at that point gives us the optimum number of clusters to have. \n* But sometimes we don\u2019t get clear elbow point on the plot, in such cases its very hard to finalize the number of clusters.\n\n![K_Means_Elbow_Method](https:\/\/raw.githubusercontent.com\/satishgunjal\/images\/master\/K_Means_Elbow_Method.png)\nReference: https:\/\/satishgunjal.com\/kmeans\/#5","e7d006fb":"# What is a hypothesis test and p-value?\n* A hypothesis test is rule that specifies whether to accept or reject a claim about a population depending on the evidence provided by a sample of data.\n* A hypothesis test examines two opposing hypotheses about a population: the null hypothesis and the alternative hypothesis. The null hypothesis is the statement being tested. Usually the null hypothesis is a statement of \"no effect\" or \"no difference\". The alternative hypothesis is the statement you want to be able to conclude is true based on evidence provided by the sample data.\n* Based on the sample data, the test determines whether to reject the null hypothesis. You use a p-value, to make the determination. If the p-value is less than the significance level (denoted as \u03b1 or alpha), then you can reject the null hypothesis.\n* In laymans term the p-value is the probability that the null hypothesis is true.\n* Consider the example where we are trying to test whether a new marketing campaign generates more revenue, the p-value is the probability that the null hypothesis, which states that there is no change in the revenue as a result of the new marketing campaign, is true. If the value of the p-value is 0.25, then there is a 25% probability that there is no real increase or decrease in revenue as a result of the new marketing campaign. If the value of the p-value is 0.04 then there is a 4% probability that there is no real increase or decrease in revenue as a result of the new marketing campaign. As you can surmise, the lower the p-value, the more confident we are that the alternate hypothesis is true, which, in this case, means that the new marketing campaign causes an increase or decrease in revenue.\n* So what do p-values really tell us? p-values tell us whether an observation is as a result of a change that was made or is a result of random occurrences. In order to accept a test result we want the p-value to be low. How low you ask? Well, that depends on what standard you want to set\/follow. In most fields, acceptable p-values should be under 0.05 while in other fields a p-value of under 0.01 is required. This cut-off number is known in statistics as the alpha, and results from experiments with p-values below this threshold are considered to be statistically significant. So when a result has a p-value of 0.05 or lower we can say that we are 95% confident that there is an actual difference between the two observations as opposed to just differences due to random variations. And as a result, we have reasonable grounds to support the alternate hypothesis and reject the null hypothesis.\n* Ref. https:\/\/support.minitab.com\/en-us\/minitab\/18\/help-and-how-to\/statistics\/basic-statistics\/supporting-topics\/basics\/what-is-a-hypothesis-test\/\n* Ref. https:\/\/dzone.com\/articles\/what-is-p-value-in-layman-terms","de4feb07":"# How to handle missing data?\nData can be missing because of mannual error or can be gennualy missing.\n\n* Delete low quality records completely which have too much missing data\n* Impute the values by educated guess, taking average or regression\n* Use domain knwledge to impute values","9ac72eca":"# What is the Box and Whisker plot? When should use it?\n* Box and whisker plots are ideal for comparing distributions because the centre, spread and overall range are immediately apparent. \n* A box and whisker plot is a way of summarizing a set of data measured on an interval scale. \n* It is often used in explanatory data analysis\n* Boxplots are a standardized way of displaying the distribution of data based on a five number summary (\u201cminimum\u201d, first quartile (Q1), median, third quartile (Q3), and \u201cmaximum\u201d).\n    * median (Q2\/50th Percentile): the middle value of the dataset.\n    * first quartile (Q1\/25th Percentile): the middle number between the smallest number (not the \u201cminimum\u201d) and the median of the dataset.\n    * third quartile (Q3\/75th Percentile): the middle value between the median and the highest value (not the \u201cmaximum\u201d) of the dataset.\n    * interquartile range (IQR): 25th to the 75th percentile.\n    * whiskers (shown in blue)\n    * outliers (shown as green circles)\n    * \u201cmaximum\u201d: Q3 + 1.5*IQR\n    * \u201cminimum\u201d: Q1 -1.5*IQR\n    \n    ![](https:\/\/miro.medium.com\/max\/2400\/1*2c21SkzJMf3frPXPAR_gZA.png)\n    \n    Ref. https:\/\/towardsdatascience.com\/understanding-boxplots-5e2df7bcbd51","5187b986":"# Difference between supervised and unsupervised learning\n|Supervised|Unsupervised|\n|:-|:-|\n|Used for prediction|Used for analysis|\n|Labelled input data|Unlabelled input data|\n|Data need to be splitted into train\/validation\/test sets|No split required|\n|Used in Classification and Regression|Used for clustering, dimension reduction & density estimation|","06d7f1db":"# What is the difference between bias and variance?\n* Bias comes from model underfitting some set of data, whereas variance is the result of model overfitting some set of data.\n* Underfitting models have high error in training as well as test set. This behavior is called as \u2018Low Bias\u2019\n* Consider below example of bias(underfitting) where we are trying to fit linear function for nonlinear data.\n\n![Underfitting](https:\/\/raw.githubusercontent.com\/satishgunjal\/images\/master\/Underfitting.png)\n* Overfitting models have low error in training set but high error in test set. This behavior is called as \u2018High Variance\u2019\n* Consider below example of variance(overfitting) where complicated function creates lots of unnecessary curves and angles that are not related with data.\n\n![Overfitting](https:\/\/raw.githubusercontent.com\/satishgunjal\/images\/master\/Overfitting.png)\n\n* Low bias ML algorithms: Decision Tree, k-NN, SVM\n* High bias ML algorithms: Linear regression, Logistic regression\n* High Variance ML algorithms: Polynimial regression\n* Reference: https:\/\/satishgunjal.com\/underfitting_overfitting\/","625f2d96":"# Explain standard deviation and variance\n![normal-distrubution](https:\/\/www.mathsisfun.com\/data\/images\/normal-distrubution-1sd.svg)\n\n* Standard deviation is measure of how spread out numbers are\n    Formula \u03c3 = square root of the Variance\n* Variance is defined as \u201cThe average of the squared differences from the Mean.\u201d\n* Using the Standard Deviation we have a \"standard\" way of knowing what is normal, and what is extra large or extra small.\n* We can expect about 68% of values to be within plus-or-minus 1 standard deviation.\n* A low standard deviation indicates that the data points tend to be very close to the mean; a high standard deviation indicates that the data points are spread out over a large range of values\n* Ref. https:\/\/www.mathsisfun.com\/data\/standard-deviation.html#Top","ed5db90e":"# What is the difference between precision and accuracy, can you explain in terms of Confusion matrix and Confidence Interval?\n* Accuracy is how close a measured value is to the actual (true) value.\n* Precision is - out of all the predicted positive classes, how much we predicted correctly. Choose Precision if you want to be more confident of your true positives. For example, in case of spam emails, you would rather have some spam emails in your inbox rather than some regular emails in your spam box. You would like to be extra sure that email X is spam before we put it in the spam box\n* Recall is - out of all the positive classes, how much we have predicted correctly. Choose Recall if the occurrence of false negatives is unaccepted\/intolerable. For example, in the case of diabetes that you would rather have some extra false positives (false alarms) over saving some false negatives.\n\n* False Positive (Type 1 Error): You predicted positive and it\u2019s false (actual value is Negative)\n* False Negative (Type 2 Error): You predicted negative and it\u2019s false (actual value is Positive)\n* A confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known.\n* A Confidence Interval(CI) is a range of values we are fairly sure our true value lies in. 95% CI is more precise while 99% CI is more accurate. Here 95% CI is narrow hence more precise and 99% CI is wider hence more data and more accurate.\n","eea8f31a":"# How can you say that the time series data is stationary?\nFor accurate analysis and forecasting trend and seasonality is removed from the time series and converted it into stationary series. Time series data is said to be stationary when statistical properties like mean, standard deviation are constant and there is no seasonality. In other words statistical properties of the time series data should not be a function of time.\n\n![Stationarity](https:\/\/raw.githubusercontent.com\/satishgunjal\/images\/master\/Stationarity.png)\nReference: https:\/\/satishgunjal.com\/time_series\/\n","23597b53":"# Difference between statistics and machine learning\n* The major difference between machine learning and statistics is their purpose. Machine learning models are designed to make the most accurate predictions possible. Statistical models are designed for inference about the relationships between variables.\n* Statistics is mathematical study of data. Lots of statistical models that can make predictions, but predictive accuracy is not their strength.","bb9fefd6":"# In you choice of langauge: Write a program that prints the numbers from 1 to 50. But for multiples of three print \"Fizz\" instaed of the number and for the multiples of five print \"Buzz\". For the numbers which are multiples of both three and five print \"FizzBuzz\".","7ca83054":"# Explain confusion matrix\n![](https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2020\/04\/Basic-Confusion-matrix.png)\n\n* The confusion matrix is one of the most powerful tools for predictive analysis in machine learning. \n* A confusion matrix gives you information about how your machine classifier has performed, pitting properly classified examples against misclassified examples.\n* Confusion matrices are used to visualize important predictive analytics like recall, specificity, accuracy, and precision. \n* Confusion matrices are useful because they give direct comparisons of values like True Positives, False Positives, True Negatives and False Negatives. In contrast, other machine learning classification metrics like \u201cAccuracy\u201d give less useful information, as Accuracy is simply the difference between correct predictions divided by the total number of predictions.\n* All estimation parameters of the confusion matrix are based on 4 basic inputs namely True Positive, False Positive, True Negative and False Negative.\n* Confusion matrices have two types of errors: Type I (False Positive) and Type II (False Negative). False Positive contains one negative word (False) so it\u2019s a Type I error. False Negative has two negative words (False + Negative) so it\u2019s a Type II error.\n* From our confusion matrix, we can calculate five different metrics measuring the validity of our model.\n    * **ACCURACY**\n    \n        Accuracy is the ratio of correctly identified subjects in a pool of subjects.\n\n        Accuracy = (TP+TN)\/(TP+FP+FN+TN)\n\n        Accuracy answers the question: How many patients did we correctly identify out of all patients?\n\n    * **PRECISION**\n    \n        Precision is the ratio of correctly +ve identified subjects by test, against all +ve subjects identified by test.\n\n        Precision = TP\/(TP+FP)\n\n        Precision answers the question: How many patients tested +ve are actually +ve?\n\n        This metric is often used in cases where classification of true positives is a priority. For example, a spam email classifier would rather classify some spam emails as regular emails rather than classify some regular emails as spam. That\u2019s why some spam emails end up in your main inbox, just to be safe.\n\n    * **SPECIFICITY**\n    \n        Specificity is the ratio of correctly -ve identified subjects by test against all -ve subjects in reality.\n\n        Specificity = TN\/(TN+FP)\n\n        Specificity answers the question: Of all the patients that are -ve, how many did the test correctly predict?\n\n        This metric is often used in cases where classification of true negatives is a priority. For example, a doping test will immediately ban an athlete if they are tested positive. We would not want to any drug-free athlete to be wrongly classified and banned.\n\n    * **SENSITIVITY (RECALL)**\n    \n        Sensitivity is the ratio of correctly +ve identified subjects by test against all +ve subjects in reality.\n\n        Sensitivity = TP\/(TP+FN)\n\n        Sensitivity answers the question: Of all the patients that are +ve, how many did the test correctly predict?\n\n        This metric is often used in cases where classification of false negatives is a priority. A good example is the medical test that we used for illustration above. The government would rather have some healthy people labeled +ve than have an infected individual labeled -ve and spread the disease. We would rather be overly cautious and have false positives than risk wrongly identifying false negatives.\n\n    * **F1 SCORE**\n    \n        F1 Score accounts for both precision and sensitivity.\n\n        F1 Score = 2 * (Recall * Precision)\/(Recall + Precision)\n\n        It is often considered a better indicator of a classifier\u2019s performance than a regular accuracy measure as it compensates for uneven class distribution in the training dataset. For example, an uneven class distribution is likely to occur in insurance fraud detection, where a large majority of claims are legitimate and only a very small minority are fraudulent. \n\n**Which metric to use is depends on the problem in hand**","e6f1c4e6":"# What is stepwise regression?\nStepwise regression is a method of fitting regression models in which the choice of predictive variables is carried out by an automatic procedure. In each step, a variable is considered for addition to or subtraction from the set of explanatory variables based on some prespecified criterion.\n\nStepwise regression is classified into backward and forward selection.\n* **Backward selection** starts with a full model, then step by step we reduce the regressor variables and find the model with the least RSS, largest R\u00b2, or the least MSE. The variables to drop would be the ones with high p-values.\n* **Forward selection** starts with a null model, then step by step we increase the regressor variables until we can no longer improve the error performance of the model. We usually pick the model with the highest adjusted R\u00b2.","e60eb282":"# Explain ROC curve and AUC\n* Receiver Operating Characteristics(ROC) curve is very usefull tool for predicting the probability of binary classifier\n* It is a plot of the false positive rate (x-axis) versus the true positive rate (y-axis) for a number of different candidate threshold values between 0.0 and 1.0\n    - False positive rate = FP \/ (FP + TN)\n    - True positive rate (Sensitivity) = TP \/ (TP + FN\n    \nFor more detailed explaination please refer. https:\/\/developers.google.com\/machine-learning\/crash-course\/classification\/roc-and-auc","01f6f76b":"# What are the advantages and disadvantages of neural networks?  \n\n**Here are some advantages of Neural Networks**\n\n* Storing information on the entire network: Information such as in traditional programming is stored on the entire network, not on a database. The disappearance of a few pieces of information in one place does not restrict the network from functioning. \n* The ability to work with inadequate knowledge: After ANN training, the data may produce output even with incomplete information. The lack of performance here depends on the importance of the missing information. \n* It has fault tolerance:  Corruption of one or more cells of ANN does not prevent it from generating output. This feature makes the networks fault-tolerant. \n* Having a distributed memory: For ANN to be able to learn, it is necessary to determine the examples and to teach the network according to the desired output by showing these examples to the network. The network's progress is directly proportional to the selected instances, and if the event can not be shown to the network in all its aspects, the network can produce incorrect output \n* Gradual corruption:  A network slows over time and undergoes relative degradation. The network problem does not immediately corrode.\n* Ability to train machine: Artificial neural networks learn events and make decisions by commenting on similar events. \n* Parallel processing ability:  Artificial neural networks have numerical strength that can perform more than one job at the same time. \n\n**Disadvantages of Neural Networks**\n\n* Hardware dependence:  Artificial neural networks require processors with parallel processing power, by their structure. For this reason, the realization of the equipment is dependent. \n* Unexplained functioning of the network: This is the most important problem of ANN. When ANN gives a probing solution, it does not give a clue as to why and how. This reduces trust in the network. \n* Assurance of proper network structure:  There is no specific rule for determining the structure of artificial neural networks. The appropriate network structure is achieved through experience and trial and error. \n* The difficulty of showing the problem to the network:  ANNs can work with numerical information. Problems have to be translated into numerical values before being introduced to ANN. The display mechanism to be determined here will directly influence the performance of the network. This depends on the user's ability. \n* The duration of the network is unknown: The network is reduced to a certain value of the error on the sample means that the training has been completed. This value does not give us optimum results. ","a64b59bb":"# In a test, students in section A scored with a mean of 75 and standard deviation of 10, while students in section B scored with a mean of 80 and standard deviation of 12? Melissa from section A and Ryan from section B both have scored 90 in this test. Who had a better performance in this test as compared to their classmates?\n\nTo compare the two scores we need to standardize them to the same scale. We do that by calculating the Z score, which allows us to compare the 2 scores in units of standard deviations. \n\n```Z score= (X- mean)\/Standard Deviation```\n\nMelissa's Z score = (90-75)\/10 = 1.5\n\nRyan's Z score = (90-80)\/12 = 0.83\n\nMelissa has performed better.\n","1debf890":"# How do you build random forest model?\n\nRanodm forest is made up of multiple decision trees. Unlike decision tree random forest fits multiple decision trees on various sub samples of dataset and make the predictions by averaging the predictions from each tree. \n\n![](https:\/\/raw.githubusercontent.com\/satishgunjal\/images\/master\/Random_Forest.png)\n\n* Select few random sub sample from given dataset\n* Construct a decision tree for every sub sample and predict the result.\n* Perform the voting on prediction from each tree.\n* At the end select the most voted result as final prediction.\n* Reference: https:\/\/satishgunjal.com\/random_forest\/\n","e01bcb4e":"# How do Random Forest handle missing data?\nNote that handling missing data is one of the advantages of Random Forest algorithm over Decision tree. Please refer below diagram where we have training data set of circle, square and triangle of color red, green and blue respectively. There are total 27 training examples.\n\n![](https:\/\/raw.githubusercontent.com\/satishgunjal\/images\/master\/Random_Forest.png)\n\n* Random forest will create three sub sample of 9 training examples each\n* Random forest algorithm will create three different decision tree for each sub sample\n* Notice that each tree uses different criteria to split the data\n* Now it is straight forward analysis for the algorithm to predict the shape of given figure if its shape and color is known. Let\u2019s check the predictions of each tree for blue color triangle, (here shape input is missing)\n    * Tree 1 will predict: triangle\n    * Tree 2 will predict: square\n    * Tree 2 will predict: triangle \n    \n    Since the majority of voting is for triangle final prediction is \u2018triangle shape\u2019\n    \n* Now, lets check predictions for circle with no color defined (color attribute is missing here)\n    * Tree 1 will predict: triangle\n    * Tree 2 will predict: circle\n    * Tree 2 will predict: circle \n    \n    Since the majority of voting is for circle final prediction is \u2018circle shape\u2019\n\nPlease note this is over simplified example, but you get an idea how multiple tree with different split criteria helps to handle missing features\n\nReference: https:\/\/satishgunjal.com\/random_forest\/","bef56024":"# Suppose you had bank transaction data, and wanted to separate out likely fraudulent transactions. How would you approach it? Why might accuracy be a bad metric for evaluating success? \n* In Machine Learning, problems like fraud detection are usually framed as classification problems. In order to solve this problem we may use different features like amount, merchant, location, time etc associated with each transaction.\n* One of the biggest challenge with fraud transaction detection is- majority of transactions are not fraud, so we have inbalance data!\n* First step will be to do EDA and understand our data and intesity of class inbalance.\n* In order to handle inbalance data problem we can use one of the following method\n    * Oversampling \u2014 SMOTE (ynthetic Minority Over-sampling Technique)\n    * Undersampling \u2014 One simple way of undersampling is randomly selecting a handful of samples from the class that is overrepresented.\n    * Combined Class Methods \u2014 Use SMOTE together with edited nearest-neighbours (ENN). Here, ENN is used as the cleaning method after SMOTE over-sampling to obtain a cleaner space.\n* Test model performane for each of above technique and choose best performing model.\n\n### Why might accuracy be a bad metric for evaluating success? \n* In case of inbalance data accuracy metric is not usefull. Accuracy tells us how close a measured value is to the actual (true) value. But here we are more interested in fraud transactions.\n* We dont mind declaring few good transactions as fruad but failing to identify fraud transaction is not acceptable. In such cases classification of true positives is a priority, hence precision metric make more sense.","3c5626e8":"# References\n* https:\/\/www.youtube.com\/watch?v=k6QWYwOvJs0&t=1149s\n* https:\/\/towardsdatascience.com\/taking-the-confusion-out-of-confusion-matrices-c1ce054b3d3e\n* https:\/\/kambria.io\/blog\/confused-about-the-confusion-matrix-learn-all-about-it\/#:~:text=Confusion%20matrices%20are%20used%20to,True%20Negatives%20and%20False%20Negatives.\n* https:\/\/projects.uplevel.work\/insights\/confusion-matrix-accuracy-sensitivity-specificity-precision-f1-score-how-to-interpret\n* https:\/\/www.analyticsvidhya.com\/blog\/2020\/04\/confusion-matrix-machine-learning\/","ee0e3dce":"# What is outlier? How to handle them?\n\n* An outlier is an observation that lies an abnormal distance from other values in a random sample from a population.\n* Data points above and below 1.5*IQR, are most commonly outliers.\n\nOutliers can drastically change the results of the data analysis and statistical modeling.\n\n    \n## Types of the outliers\n* **Data entry errors**\n* **Measuremental errors**\n* **Intentional outliers**. This is commonly found in self-reported measures that involves sensitive data. For example: Teens would typically under report the amount of alcohol that they consume.\n* **Data processing erros**. Whenever we perform data mining, we extract data from multiple sources. It is possible that some manipulation or extraction errors may lead to outliers in the dataset.\n* **Sampling error**. For instance, we have to measure the height of athletes. By mistake, we include a few basketball players in the sample. This inclusion is likely to cause outliers in the dataset.\n* **Natutal oulier**. When an outlier is not artificial (due to error), it is a natural outlier. For instance: In my problem assignment with one of the renowned insurance company, I noticed that the performance of top 50 financial advisors was far higher than rest of the population. Surprisingly, it was not due to any error. Hence, whenever we perform any data mining activity with advisors, we used to treat this segment separately.\n    \n\n## How to detect Outliers?\nMost commonly used method to detect outliers is visualization.\n* We use various visualization methods, like Box-plot, Histogram, Scatter Plot \n* Use capping methods. Any value which out of range of 5th and 95th percentile can be considered as outlier\n* Data points, three or more standard deviation away from mean are considered outlier\n\nApart from visualization we can also use Z-Score or Extreme Value Analysis (parametric) to detect outliers.\n\n## How to remove outliers?\nMost of the methods used to handle missing values are aslo application in case ot outliers\n\n### Deleting observations\nWe delete outlier values if it is due to data entry error, data processing error or outlier observations are very small in numbers. We can also use trimming at both ends to remove outliers.\n\n### Transforming and binning values\nTransforming variables can also eliminate outliers. Natural log of a value reduces the variation caused by extreme values. Binning is also a form of variable transformation. Decision Tree algorithm allows to deal with outliers well due to binning of variable.\n\n### Imputing\nWe can use mean, median, mode imputation methods.\n\n### Treat separately\nIf there are significant number of outliers, we should treat them separately in the statistical model. One of the approach is to treat both groups as two different groups and build individual model for both groups and then combine the output.\n\nReference: https:\/\/www.linkedin.com\/pulse\/techniques-outlier-detection-treatment-suhas-jk\/","9b8b36c4":"# What is more important model accuracy or model performance?\n* Short answer is: Model accuracy matters the most! inaccurate information is not usefull.\n* Model performance can be improved by increasing the compute resources.\n* Model accuracy and performance can be subjective to the problem in hand. For example, in analysis of medical images to determine if there is a disease (such as cancer), the accuracy extremely critical, even if the models would take minutes or hours to make a prediction.\n* Some applications require real time performance, even if this comes at a cost of accuracy. For example, imagine a machine that views a fast conveyor belt carrying tomatoes, where it must separate the green from the red ones. Though an occasional error is undesired, the success of this machine is more determined by its ability to withstand its throughput.\n* A more common example is face detection for recreational applications. People would expect a fast response from the app, though the occasional missed face would not render it useless.\n* Reference: https:\/\/www.quora.com\/Which-is-more-important-to-you-model-accuracy-or-model-performance","fdb913ac":"# Explain Principal Componenet Analysis?\n* Principal Component Analysis (PCA) is dimensionality reduction method, that is used to reduce dimensionality of large data sets, by transforming large set of variables into a smaller one that still contains most of the information in large set.\n* Principal component analysis is a technique for **feature extraction** \u2014 so it combines our input variables in a specific way, then we can drop the \u201c**least important**\u201d variables while still retaining the most valuable parts of all of the variables! As an added benefit, each of the \u201cnew\u201d variables after PCA are all **independent** of one another\n* Reducing the number of the variables of the datset naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity.\n* By reducing the dimension of your feature space, you have fewer relationships between variables to consider and you are less likely to overfit your model\n\n### When should I use PCA?\n* Do you want to reduce the number of variables, but aren\u2019t able to identify variables to completely remove from consideration?\n* Do you want to ensure your variables are independent of one another?\n* Are you comfortable making your independent variables less interpretable?\n\n* Reference: https:\/\/builtin.com\/data-science\/step-step-explanation-principal-component-analysis, https:\/\/towardsdatascience.com\/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c","c90e3bd6":"# 'People who bought this, also bought...'recommendations seen on Amazon is a result of which algorithm?\n\n* Its done by recommendation system using collaborative filtering approach.\n* In case of collaborative filtering past interactions recorded between users and items are used to produce new recommendations.","7a5883a3":"# For the given point how will you caluclate the Euclidean distance, in Python?\n\nEuclidean distance is calculated as the square root of the sum of the squared differences between the two vectors.\n\n![](https:\/\/predictivehacks.com\/wp-content\/uploads\/2020\/08\/2d_euclidean_distance_illustration.png)\nReference: https:\/\/predictivehacks.com\/tip-how-to-define-your-distance-function-for-hierarchical-clustering\/","ae509945":"# What are the assumptions for linear regression\nLinear regression assumptions are as below\n* Data should have linear relationship between X and Y (actually mean of Y)\n* Data should be normally distributed\n* No or little multicollinearity (observations should be independent of each other)\n* Homoscedasticity- There should not be unequal variance in data","8930b461":"# If deleting outliers is not an option, how will you handle them?\n* I will try differen models. Data detected as outliers by linear model, can be fit by  non-linear model.\n* Try normalizing the data, this way the extreame datapoints are pulled to the similar range.\n* We can use algorithms which are less affected by outliers.\n* We can also create separate model to handle the outlier data points.","ed9103f7":"# What is model overfitting? How can you avoid it?\nOverfitting occurs when your model learns too much from training data and isn't able to generalize the underlying information. When this happens, the model is able to describe training data very accurately but loses precision on every dataset it has not been trained on. Below images represent the overfitting linear and logistic regression models.\n\n![](https:\/\/raw.githubusercontent.com\/satishgunjal\/images\/master\/Overfitting.png)\n\n**How To Avoid Overfitting?**\n* Since overfitting algorithm captures the noise in data, reducing the number of features will help. We can manually select only important features or can use model selection algorithm for same\n* We can also use the \u2018Regularization\u2019 technique. It works well when we have lots of slightly useful features. Sklearn linear model(Ridge and LASSO) uses regularization parameter \u2018alpha\u2019 to control the size of the coefficients by imposing a penalty. \n* K-fold cross validation. In this technique we divide the training data in multiple batches and use each batch for training and testing the model.\n* Increasing the training data also helps to avoid overfitting.\n\nReference: https:\/\/satishgunjal.com\/underfitting_overfitting\/","5524e98d":"# What is the difference betweeen K nearest neighbors and K means\n* KNN or K nearest neighbor is a classification algorithm, while K-Means is clustering technique.\n* KNN is supervised algorithm, K means is unsupervised algorithm.\n* In KNN prediction of the test sample is based on the similarity of its features to its neighbors. The similarity is computed based on the measure such as euclidean distance. Here K referes to the number of neighbors with whom similarity is being compared.\n* K-means is the process of defining clusters or groups around predefined centroids based on the similarity of each data point to each other. Here K referes to the number of centroids around which clusters will be formed.","f803101c":"# What is selection bias?\n* Selection bias is the bias introduced by the selection of individuals, groups, or data for analysis in such a way that proper randomization is not achieved, thereby ensuring that the sample obtained is not representative of the population intended to be analyzed. It is sometimes referred to as the **selection effect**.\n* **Sampling bias** is usually classified as a subtype of selection bias, sampling bias is a bias in which a sample is collected in such a way that some members of the intended population have a lower or higher sampling probability than others.\n* Due to sampling bias, the probability distribution in the collected dataset deviates from its true natural distribution, which may affect ML models performance.","e2ed9d06":"# Expalin covariance and correlation\n* Covariance and Correlation are two mathematical concepts which are commonly used in the field of probability and statistics. Both concepts describe the relationship between two variables.\n* \u201cCovariance\u201d indicates the direction of the linear relationship between variables. \u201cCorrelation\u201d on the other hand measures both the strength and direction of the linear relationship between two variables.\n* In case of High correlation, two sets of data are strongly linked together \n    - Correlation is Positive when the values increase together, and\n    - Correlation is Negative when one value decreases as the other increases\n    \n    \n   ![](https:\/\/www.mathsisfun.com\/data\/images\/correlation-examples.svg)\n   \nReference: https:\/\/www.mathsisfun.com","048e2d9b":"# Important Tips\n* Datascience interview questions can include questions from statistics, math, data visualization, analytics, software engineering, baics ML concepts, ML models etc. Type of questions can also vary from being fiexed answer questions or open ended questions where multiple solutions are possible.\n* Please **DO NOT TRY TO REMEMBER** these question answers. Instead use this notebook to test your knowledge and accordingly work on the weak areas.\n* Try to understand what interviewer is trying to know from each question and explain it in your own words.\n* Best way of learning anything new is to read about it -> understand the concepts -> try it your self -> if possible publish your learning and colloborate with other learners.\n* I will keep updating this notebook as and when I come across interesting questions.","e08ff2d9":"# How can you calculate the accuracy using confusion matrix?\nAccuracy = (True Positive + true Negative) \/ Total Obervations","ae911430":"# What are Eigenvectors and Eigenvalues?\n* Eigenvectors are used for understanding the linear transformations.\n* In data analysis, we usually calculate the eigenvectors for correlation or covariance matrix\n* Eigenvectors are the directions along which a particular linear transformation acts by flipping, compressing or stretching.\n* Eigenvalues can be referred to as the strength of the transaformation in direction of eigenvector or the factor by which compression occurs.\n* Refer. https:\/\/www.youtube.com\/watch?v=glaiP222JWA","4c5d28b7":"# There are 9 balls out of which one ball is heavy in weight and rest are of the same weight. In how many minimum weightings will you find the heavier ball?\n* You will need two weightings\n* Step1: Out of 9 balls, place three balls on each side (you will have three remaining balls)\n    * Scenario: Balance out\n    \n    In balance out scenario, the heaviest ball is definately part of three remaining balls. Out of the remaining three balls from step 1, take two balls and place one ball on each side- if they balance out then the left out ball will be the heavier ball. Otherwise, you will see it in the balance.\n    \n    * Scenario: Not balance out\n    \n    If the balls in step 1 do not balance out, that means heavier side has the heavier ball.\n* Reference: https:\/\/www.youtube.com\/watch?v=5JZsSNLXXuE&list=PLwWVLyefnzgpWxe2WEPrmHqHzwHlyZw1U&index=2&t=1294s","26f65f8b":"# Difference between standardisation and normalization?\n* Normalization typically means rescales the values into a range of [0,1]. This might be useful in some cases where all parameters need to have the same positive scale. However, the outliers from the data set are lost.\n* Standardization typically means rescales data to have a mean of 0 and a standard deviation of 1 (unit variance). For most applications standardization is recommended.","b23f5c12":"# Your organization has a website where visitors randomly receive one of the two coupons. It is also possible that visitors to the website will not receive the coupon. You have been asked to determine if offering a coupon to the visitors to your website has any impact on their purchase decision. Which analysis method should you use?\n```\n- One Way ANOVA\n- K-means clsutering\n- Assiciation rules\n- Student Test\n```\n\nAnswer: One Way ANOVA\n\nReference: https:\/\/youtu.be\/5JZsSNLXXuE","69294a5a":"# Expalin baisc steps in Face Verification\nBelow are the four basic steps in face verification\n\n* Face Detection. Locate one or more faces in the image\n* Face Localization: Draw bounding box around face\n* Face Alignment. Normalize the face to be consistent with the database, such as geometry and photometrics.\n* Feature Extraction. Extract features from the face that can be used for the recognition task.\n* Face Recognition. Perform matching of the face against one or more known faces in a prepared database.","88376500":"# Which of the following machine learning algorithm can be used for imputing missing values of both categorical and continuos variables?\n\n```\n- K-means clustering\n- Linear regression\n- K-NN\n- Decision tress\n```\n\nUsing KNN we can compute the missing variable value by using the nearest neighbors.","55151f65":"# If a drawer containes 12 red socks, 16 blue socks, and 20 white socks, how many must you pull out to be sure of having a amcthing pair?\n* There are three colors of socks- Red, Blue and White. No of socks is irrelevant here.\n* Suppose in our first pull we picked Red color sock\n* In second pull we picked Blue color sock\n* And in third pull we picked White color sock.\n* Now in our fourth pull, if we pick any color, match is guaranteed!! So the answer is 4!\n* Reference: https:\/\/youtu.be\/5JZsSNLXXuE","a74d1267":"# What is K Fold cross validation? Why do you use it?\n* In case of K Fold cross validation input data is divided into \u2018K\u2019 number of folds, hence the name K Fold. Suppose we have divided data into 5 folds i.e. K=5. Now we have 5 sets of data to train and test our model. So the model will get trained and tested 5 times, but for every iteration we will use one fold as test data and rest all as training data. Note that for every iteration, data in training and test fold changes which adds to the effectiveness of this method.\n* This significantly reduces underfitting as we are using most of the data for training(fitting), and also significantly reduces overfitting as most of the data is also being used in validation set.\n* K Fold cross validation helps to generalize the machine learning model, which results in better predictions on unknown data. \n* Reference: [K Fold Cross Validation](https:\/\/satishgunjal.com\/kfold\/)","fa266699":"# You have two buckets - one of 3 liters and other of 5 liters. You are expected to mesure exactly 4 liters. How will you complete the task? Note: There is no thrid bucket.\n \n * Questions like this will test your out of the box thinking\n * Step1: Fill 5 lts bucket and  empty it in 3 ltr bucket. Now we are left with 2 ltr in 5 ltr bucket.\n * Step2: Empty 3 ltr bucket and pour the contents of 5 ltr bucket in 3 ltr bucket. Now our 5 ltr bucket is empty and 3 ltr bucket has 2 ltr content in it.\n * Now fill the 5 ltr bucket again. Remember that our 3 ltr bucket has 2 ltr content in it, so if we pour 1 ltr content from 5 ltr bucket to 3 ltr bucket we are left with 4 ltr content in 5 ltr bucket.\n * Reference: https:\/\/youtu.be\/5JZsSNLXXuE","aa0f12e1":"# Python or R- which one would you prefer for text analytics?\nWe will prefer python for following reasons\n* We can use pandas library which has easy to use data structures and  high performance data analysis tools\n* R is more suitable for ML than text analytics\n* Python is faster for all types of text analytics.","ed2baab9":"# Explain the steps in making decision tree?\n\n![](https:\/\/raw.githubusercontent.com\/satishgunjal\/images\/master\/Decision_Tree.png)\n\n\nBelow are the common steps in decision tree algorithm\n* Take the entire data as input\n* At the root node decision tree selects feature to split the data in two major categories.\n* Different criteria will be used to split the data. We generally use 'entropy' or 'gini' in case of classification and 'mse' or 'mae' in case of regression problems.\n* Features are selected for spliting based on highest information gain.\n* After every split we get decision rules and sub trees.\n* This process will continue until every training example is grouped together or maxinum allowed tree depth is reached.\n* So at the end of decision tree we end up with leaf node. Which represent the class or a continuous value that we are trying predict\n* Reference: https:\/\/satishgunjal.com\/decision_tree\/","cd670967":"# Given a box of matches and two ropes, not necessarily identical, measure a period of 45 minutes? Note: Ropes are not uniform in natire and rope takes exactly 60 minutes to completly burn out\n\n* We have two ropes A and B\n* Ligt A from both the end and B from one end\n* When A finished burning we know that 30 minutes have elapsed and B has 30 minutes remaining\n* Now light the other end of B also, it will now burnout in 15 minutes\n* This we got 30 + 15 = 45 minutes\n* Reference: https:\/\/youtu.be\/5JZsSNLXXuE","bc18d466":"# Explain collinearity and technique to reduce it?\nIn statistics collinearity or multicollinearity is the phenomenon where one or more predictive variables(features) in multiple regression models are highly linearly related to each other.\n## Technique to reduce multicollearity\n* **Remove highly correlated predictors from the model**.  If you have two or more factors with a high collinearity, remove one from the model. Because they supply redundant information, removing one of the correlated factors usually doesn't drastically reduce the R-squared.  Consider using stepwise regression, best subsets regression, or specialized knowledge of the data set to remove these variables. Select the model that has the highest R-squared value. \n* **Principal Components Analysis(PCA)** regression methods that cut the number of predictors to a smaller set of uncorrelated components.\n","a44c7333":"What is Naive Bayes algorithm?\n* Naive Bayes classifier assumes that all the features(predictors) are independent of each other.\n* Naive Bayes model is easy to build and particularly useful for very large data sets. Along with simplicity, Naive Bayes is known to outperform even highly sophisticated classification methods.\n* For more details refer: https:\/\/www.machinelearningplus.com\/predictive-modeling\/how-naive-bayes-algorithm-works-with-example-and-full-code\/"}}