{"cell_type":{"2f135644":"code","6d4c22d7":"code","759310d6":"code","a04593a9":"code","13605567":"code","5716e8e3":"code","3c9d2048":"code","fec243ae":"code","be60538c":"code","45d20ed1":"code","3cc2fc43":"code","93c2f4e2":"code","13d4ed5a":"code","b694660e":"code","7cb9cbcb":"code","a72b5771":"code","d6e88363":"code","3ed0a11c":"code","8b52fade":"code","d56ef6d1":"code","f29a9da7":"code","a5f70924":"code","3874107e":"code","1781b042":"code","48fae8c9":"code","fc5a70cb":"code","e8c8065d":"code","d938badc":"code","2cc66f8b":"code","dc62f124":"code","d6bccb38":"code","559cce79":"code","abe2d5ba":"code","27d7718a":"code","da5467a5":"code","1fdd4b45":"code","396c7166":"code","7cf82c12":"code","992477b0":"code","abaf5fb6":"code","a42eb6fe":"code","7f7084ba":"code","8ff69cec":"code","c2105631":"code","d2c48f3f":"code","44e04a6d":"code","73f8dbf4":"code","71ce3d5a":"code","3df21680":"code","098b80c2":"code","bedb5dc6":"code","d9705994":"code","14bd97f6":"code","26f8636b":"code","bf70edf0":"code","5274b934":"code","90c3f3b3":"code","2a17c53a":"code","6741eafc":"code","01c5f619":"code","d32dac01":"code","063c7e90":"markdown","9969d4f1":"markdown","2e4c46d6":"markdown","1e1cc261":"markdown","99fede84":"markdown","49a20b67":"markdown","586483e0":"markdown","80799963":"markdown","66ca409d":"markdown","e5cc5ce6":"markdown","02aab469":"markdown","13f58639":"markdown","3fc724c8":"markdown","ce51f6fd":"markdown","5da0fb03":"markdown","5a7331d3":"markdown","e305ae18":"markdown","63a788c9":"markdown","57f2bc49":"markdown","67dbfe64":"markdown","a9b36a76":"markdown","2c3ab2e4":"markdown","72548ada":"markdown","fe6793d7":"markdown","be4662b6":"markdown","9887ccfd":"markdown","83354f8e":"markdown","fd989f0c":"markdown","902ea223":"markdown","a01cf87b":"markdown","0dd7b73b":"markdown","10608b5a":"markdown","d7a7fc2b":"markdown","a850dba2":"markdown","1b6fe94a":"markdown","17cb6a7f":"markdown","21076b7e":"markdown","ec74e161":"markdown","da18ac01":"markdown","1ae227a6":"markdown","56ba656e":"markdown","60d005c0":"markdown","ed6b7485":"markdown","91294dd2":"markdown","818bac03":"markdown","1e412c23":"markdown","75a91370":"markdown","2de884cd":"markdown","ea3e914f":"markdown","490abd77":"markdown","f5a9e737":"markdown","d3819b82":"markdown","b88e181d":"markdown","1dfe0104":"markdown","6e586cb3":"markdown","67ac78b5":"markdown","005c2eb1":"markdown"},"source":{"2f135644":"#Importing necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import jaccard_score\nfrom sklearn.metrics import classification_report\nimport itertools\nfrom sklearn.metrics import log_loss\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn import metrics\nfrom sklearn.svm import SVC","6d4c22d7":"#Impporting the datasets (1 for training and 1 for testing)\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")","759310d6":"#Train data overview\ntrain.head()","a04593a9":"#Test data overview\ntest.head()","13605567":"print (\"The shape of the train data is (row, column):\"+ str(train.shape))\nprint (\"The shape of the test data is (row, column):\"+ str(test.shape))","5716e8e3":"#More information on train data\ntrain.info()","3c9d2048":"#More information on test data\ntest.info()","fec243ae":"train.describe().T","be60538c":"test.describe().T","45d20ed1":"#NaN values in train dataset\ntrain.isnull().sum()","3cc2fc43":"#NaN values in test dataset\ntest.isnull().sum()","93c2f4e2":"train[train.Embarked.isnull()]","13d4ed5a":"#Value counts of Embarked feature\ntrain.Embarked.value_counts()","b694660e":"train.Embarked.fillna(\"S\", inplace=True)","7cb9cbcb":"test[test.Fare.isnull()]","a72b5771":"nan_value = test[(test.Pclass == 3) & (test.Embarked == \"S\")].Fare.mean()","d6e88363":"#Replace the mean with the Nan value\ntest.Fare.fillna(nan_value, inplace=True)","3ed0a11c":"test.Cabin","8b52fade":"sur = train.Survived\ntrain.drop([\"Survived\"],axis=1, inplace=True)\n\ncomb_data = pd.concat([train,test], ignore_index=False)","d56ef6d1":"comb_data.Cabin.fillna(\"N\", inplace=True)\ncomb_data.Cabin = [i[0] for i in comb_data.Cabin] #Remove numbers","f29a9da7":"comb_data.groupby(\"Cabin\")['Fare'].mean().sort_values()","a5f70924":"#Cabin classification function\ndef cabin_classifier(f):\n    cab_type = 'unknown'\n    if f<16:\n        cab_type = \"G\"\n    elif f>=16 and f<27:\n        cab_type = \"F\"\n    elif f>=27 and f<38:\n        cab_type = \"T\"\n    elif f>=38 and f<47:\n        cab_type = \"A\"\n    elif f>= 47 and f<53:\n        cab_type = \"E\"\n    elif f>= 53 and f<54:\n        cab_type = \"D\"\n    elif f>=54 and f<116:\n        cab_type = 'C'\n    else:\n        cab_type = \"B\"\n    return cab_type","3874107e":"cab_unknown = comb_data[comb_data.Cabin == \"N\"]\n\ncab_known = comb_data[comb_data.Cabin != \"N\"]\n\ncab_unknown['Cabin'] = cab_unknown.Fare.apply(lambda x: cabin_classifier(x))\n\ncomb_data = pd.concat([cab_unknown, cab_known], axis=0)\n\ncomb_data.sort_values(by = 'PassengerId', inplace=True)\n\ntrain = comb_data[:891]\n\ntest = comb_data[891:]\n\ntrain['Survived'] = sur","1781b042":"#Class 1 survived stats\ntrain.loc[train['Pclass'] == 1].Survived.value_counts()","48fae8c9":"#Class 2 survived stats\ntrain.loc[train['Pclass'] == 2].Survived.value_counts()","fc5a70cb":"#Class 3 survived stats\ntrain.loc[train['Pclass'] == 3].Survived.value_counts()","e8c8065d":"%%capture\n\nct = pd.crosstab(train.Survived, train.Pclass)\n\ncolors = ['tab:blue', 'tab:orange']  \nlabels = [\"not survived\", \"survived\"]  \naxes = ct.plot(kind='pie', autopct='%.1f%%', subplots=True, figsize=(12, 5),\n               legend=False, labels=['', ''], colors=colors)\n\naxes = axes.flat\n\nfig = axes[0].get_figure()\n\nfor ax in axes:\n    yl = ax.get_ylabel()\n    ax.set_ylabel(yl, rotation=0, fontsize=12)\n\nfig.tight_layout()\nfig.suptitle('pclass survival', fontsize=15)","d938badc":"axes = ct.plot(kind='pie', autopct='%.1f%%', subplots=True, figsize=(15, 5), labels=[\"Not survived\", \"Survived\"])","2cc66f8b":"#Female survived stats\ntrain.loc[train['Sex'] == 'female'].Survived.value_counts()","dc62f124":"#Male survived stats\ntrain.loc[train['Sex'] == 'male'].Survived.value_counts()","d6bccb38":"pal = {1:\"green\", 0:\"red\"}\nsns.set(style=\"whitegrid\")\nplt.subplots(figsize = (15,8))\nax = sns.countplot(x = \"Sex\", hue=\"Survived\", data = train, linewidth=4, palette = pal)\n\nplt.title(\"Survival Distribuiton of Males and Females\")\nplt.xlabel(\"Sex\");\nplt.ylabel(\"# of Passenger Survived\")\n\nlegend = ax.get_legend()\nlegend.set_title(\"Survived\")\nlegends = legend.texts\nlegends[0].set_text(\"No\")\nlegends[1].set_text(\"Yes\")\nplt.show()","559cce79":"print('While ' + str(train.loc[train['Sex'] == 'female'].Survived.value_counts(normalize=True)[1]) + '% of the females have survived, only ' + str(train.loc[train['Sex'] == 'male'].Survived.value_counts(normalize=True)[1]) + '% of males have survived.')","abe2d5ba":"fig = plt.figure(figsize=(15,8),)\nax=sns.kdeplot(train.loc[(train['Survived'] == 0),'Age'] , color='red',shade=True,label='not survived')\nax=sns.kdeplot(train.loc[(train['Survived'] == 1),'Age'] , color='blue',shade=True, label='survived')\nplt.legend()\nplt.title('Age Distribution of Survivors')\nplt.xlabel(\"Age\")\nplt.ylabel('Frequency');","27d7718a":"# Placing 0 for female and 1 for male in the \"Sex\" column. \ntrain['Sex'] = train.Sex.apply(lambda x: 0 if x == \"female\" else 1)\ntest['Sex'] = test.Sex.apply(lambda x: 0 if x == \"female\" else 1)","da5467a5":"#Correlations between features\ntrain.corr()","1fdd4b45":"pd.DataFrame(train.corr()['Survived'].sort_values(ascending = False))","396c7166":"#Heatmap of the features\nsns.heatmap(train.corr(), cmap=\"Blues\", annot=True, linewidths=.5)\nsns.set(rc={'figure.figsize':(15,5)})\nplt.title(\"Correlation Heatmap\", fontsize = 20);","7cf82c12":"#Dropping unnecessary features\ntrain.drop(['PassengerId','Name', 'Ticket'], axis=1, inplace=True)\ntest.drop(['PassengerId','Name', 'Ticket'], axis=1, inplace=True)","992477b0":"#Getting the dummy variables for Cabin and Embarked variables\ntrain = pd.get_dummies(train, columns=['Cabin','Embarked'], drop_first=False)\ntest = pd.get_dummies(test, columns=['Cabin','Embarked'], drop_first=False)","abaf5fb6":"train['Age'].fillna(train['Age'].median(), inplace = True)\ntest['Age'].fillna(test['Age'].median(), inplace = True)","a42eb6fe":"#Separating Survived from train dataset\nX = train.drop(['Survived'], axis = 1)\ny = train[\"Survived\"]","7f7084ba":"#Splitting the train data \nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size = .33, random_state=0)","8ff69cec":"#Scaling the values\nX_train = preprocessing.StandardScaler().fit(X_train).transform(X_train)\nX_test = preprocessing.StandardScaler().fit(X_test).transform(X_test)","c2105631":"#Training the linear regression model\nLR = LogisticRegression(C=0.01, solver='liblinear').fit(X_train,y_train)\nLR","d2c48f3f":"#Predicting Survived values\nyhat = LR.predict(X_test)\nyhat","44e04a6d":"jaccard_score(y_test, yhat,pos_label=0)","73f8dbf4":"#A fundction that plots confusion matrix (from IBM Machine Learning Course)\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\nprint(confusion_matrix(y_test, yhat, labels=[1,0]))","71ce3d5a":"cnf_matrix = confusion_matrix(y_test, yhat, labels=[1,0])\nnp.set_printoptions(precision=2)\n\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=['Survived','Not Survived'],normalize= False,  title='Confusion matrix')","3df21680":"#Classification report of the logistic regresison\nprint (classification_report(y_test, yhat))","098b80c2":"yhat_prob = LR.predict_proba(X_test)\n\nlog_loss(y_test, yhat_prob)","bedb5dc6":"k = 4 #Random k value\n#Train Model and Predict  \nneigh = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)\nneigh","d9705994":"yhat = neigh.predict(X_test)\nyhat","14bd97f6":"print(\"Train set Accuracy: \", metrics.accuracy_score(y_train, neigh.predict(X_train)))\nprint(\"Test set Accuracy: \", metrics.accuracy_score(y_test, yhat))","26f8636b":"Ks = 10\nmean_acc = np.zeros((Ks-1))\nstd_acc = np.zeros((Ks-1))\n\nfor n in range(1,Ks):\n    \n    #Train Model and Predict  \n    neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)\n    yhat=neigh.predict(X_test)\n    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)\n\n    \n    std_acc[n-1]=np.std(yhat==y_test)\/np.sqrt(yhat.shape[0])\n\nmean_acc","bf70edf0":"plt.plot(range(1,Ks),mean_acc,'g')\nplt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)\nplt.fill_between(range(1,Ks),mean_acc - 3 * std_acc,mean_acc + 3 * std_acc, alpha=0.10,color=\"green\")\nplt.legend(('Accuracy ', '+\/- 1xstd','+\/- 3xstd'))\nplt.ylabel('Accuracy ')\nplt.xlabel('Number of Neighbors (K)')\nplt.tight_layout()\nplt.show()","5274b934":"k = 7 #Bets k value\n#Train Model and Predict  \nneigh = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)\nneigh","90c3f3b3":"yhat = neigh.predict(X_test)\nyhat","2a17c53a":"print(\"Train set Accuracy: \", metrics.accuracy_score(y_train, neigh.predict(X_train)))\nprint(\"Test set Accuracy: \", metrics.accuracy_score(y_test, yhat))","6741eafc":"#Grid Search of the parameters\nCs = [0.001, 0.01, 0.1, 1,1.5,2,2.5,3,4,5, 10] \ngammas = [0.0001,0.001, 0.01, 0.1, 1]\nparam_grid = {'C': Cs, 'gamma' : gammas}\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\ngrid_search = GridSearchCV(SVC(kernel = 'rbf', probability=True), param_grid, cv=cv)\ngrid_search.fit(X,y)","01c5f619":"print(grid_search.best_score_)\nprint(grid_search.best_params_)\nprint(grid_search.best_estimator_)","d32dac01":"svm_grid = grid_search.best_estimator_\nsvm_grid.score(X,y)","063c7e90":"The log loss value shows us that the prediction waas not bad but also probably not the optimal choice.","9969d4f1":"Now let's check the size and types of our datasets.","2e4c46d6":"The data is split into two parts, one of them is training data and the other one is testing data. We are going to use the train data to train our models and the test data to see how well our model performs.","1e1cc261":"Estime the cabin type of each passengers.","99fede84":"### 2.1.1 Missing Embarked Values","49a20b67":"From the pie charts above we can see that Class 1 has a survival percentage of %63, Class 2 has %47.3, and Class 3 has %24.2. Therefore, these information supports our assumption that hacing a higher class increases chances of that passenger's survival probability. Which implies that the first class passengers had the upper hand during the evacuation of Titanic.","586483e0":"Now, we are going to use logistic regression to classify and estimate the survival vairable using other features.","80799963":"'S' is the most frequent Embarked type, so lets replace the NaN values with 'S'","66ca409d":"## 1. Glimpse of the Dataset","e5cc5ce6":"As there are only 2 nan values (%0.22 of the whole dataset) we can just assign the most frequent Emabarked value to them. Embarked port of each person is independent of any other given feature in the dataset.","02aab469":"SVM with the best parameters will give us the most accurate predicitons.","13f58639":"In this notebook we are going to analyze, visualize and predict some features of the Titanic dataset. This data set includes several different features about the passengers that travelled with the famous cruise ship Titanic, back in 1912. One of the most crucial feature that we are going to be looking for is that whether each passanger has survived the sinking of Titanic. We are going to be using several ML models such as Logistic Regression, K-Nearest Neighbors(KNN), and Support-Vector Machine(SVM) to practice these predictions. But first, we are going to start with importing the libraries and the data and then continue with the analysis and visaulization of some of the aspects of the data.","3fc724c8":"### 2.2 Visualization of Data","ce51f6fd":"From the correlation coeffients above, we can see that Sex and Pclass are the most correlated features with dependent variable Survived. (Negatively correlated) Now, we are going to create the heatmap of corre\u015fations of features to observe more deeply.","5da0fb03":"#### Assumption 3: Younger passengers have a higher survival percentage.","5a7331d3":"Let's evaluate this prediction as well.","e305ae18":"This confusion matrix shows us that while 32 of the passengers were predicted as Survived, they did not actually survive and while 28 of them has survived while they were predicted as Not Survived. Let's calculate the log loss of this prediction to evaluate it's consistency better.","63a788c9":"We see that Age and Cabin features have a considerable amount of NaN values for each dataset. So we can first deal with Embarked feature for the train set and Fare feature for the test set.","57f2bc49":"As we have guessed relatively younger passengers (20-30) have a considerably high survival percentage. Also, we can see the small bump on the left side, which is probably caused by the priority of children and infants.","67dbfe64":"Some basics stats of the numerical values of train and test data.","a9b36a76":"This time we are going to try KNN method to predict the survived variable of passengers.","2c3ab2e4":"Let's propose some assumptions and create some visualizations to see if they are consistent.","72548ada":"For the SVM we won't try the gamma values and we will just calculat the optimal parameters to predict the dependent vairables.","fe6793d7":"Now we have the predictions for each X_test passenger. We can compare them with the y_test values to evaluate the accuracy of the logistic regression prediciton. First let's look at the Jaccard index.","be4662b6":"### 2.1.3 Missing Cabin Values","9887ccfd":"We still have missing age values so this time we are going to use the median with median just for this case.","83354f8e":"# Analysis and Prediction of Titanic Data","fd989f0c":"For the age data we are only going to use the non missing values, as we did not fixed them. Let's create a density plot of the survived passengers by depending on their age.","902ea223":"### 5.1 Logistic Regression","a01cf87b":"From the plot above we can estimate a k value of 9 will give us a more accurate result.","0dd7b73b":"We have to do some preprocessing before we apply the machine learning methods. We need numbers instead of categorical values, as machine leanign algorithms can not understand categorical values such as 'male' and 'female'.","10608b5a":"From above information we can say that the mean fare increases with cabin types from G to B. So we can assign each passengers cabin type depending on where their fare value correspond to the fare range above. (Omit N values since they are dummies for NaN.)","d7a7fc2b":"A value of 0.71 implies that our prediction is relatively a good one. Let's use another method called confusion matrix to investigate deeper.","a850dba2":"### 5.2 K-Nearest Neighbors(KNN)","1b6fe94a":"Let's check how the Survived feature is correlated with other features.","17cb6a7f":"For this value we can use the mean of the Fare values that has a Pclass value of 3 and Emarked value of S, as these 2 features may have an effect on the fare price.","21076b7e":"From the above information, we can see that we have unequal amount of data point in our datasets. Which implies that we probably have some missing values in our datasets. We have to replace or get rid of these missing values so that they dont result in any discrepancies in our calculations. Let's check how many NaN values are present in each data set.","ec74e161":"## 3. Correlation Between Passenger Features","da18ac01":"There is only one missing fare value in the test set.","1ae227a6":"Let's take a look at the missing Embarked values in the train dataset.","56ba656e":"#### Assumption 2: Female passengers have a higher survival percentage.","60d005c0":"Cabin value names start by a letter and followed by a number, the numbers implies the number of cabins each person had booked. So we can just use the letters for the Cabin types. Since there are so many missing cabin values we can't use the most frequent cabin type or any other approach like that. We have to estimate the cabin type for each passenger depending on other features of them. Considering the socio-economic-status of the passengers, we can say that the Cabin type can be associated with the Fare value of each passenger. We can say that passengers with similar Fares may have similar cabin types. But first we should   merge the cabin values and remove the numbers of cabin types.","ed6b7485":"As you can see the test data do not have the Survived status, as we are going to use our model to predict them.","91294dd2":"## 2. Cleaning and Visualization of Data","818bac03":"## 5. Machine Learning Methods","1e412c23":"Let's fill NaN values with dummies so we can change them in the future.","75a91370":"#### Assumption 1: Passengers with a higher Pclass (1>2>3) tends to survive more, compared to a low Pclass passenger.","2de884cd":"## 4. Machine Learning Preprocessing","ea3e914f":"Now, let's look at the association between the cabin types and the mean fare values.","490abd77":"### Column Labels:\n- **PassengerId**: Passenger Identification number.\n- **Survived**: Survival status of the passenger. (0 = No, 1 = Yes)\n- **Pclass**: Ticket class of the passenger. (1 = 1st, 2 = 2nd, 3 = 3rd)\n- **Name**: Name of the passenger.\n- **Sex**: Sex of the passenger.\n- **Age**: Age of the passenger. (in years)\n- **SibSp**: Number of siblings \/ spouses aboard the Titanic.\n- **Parch**: Number of parents \/ children aboard the Titanic.\n- **Ticket**: Ticket number of the passenger.\n- **Fare**: Fare paid by the passenger.\n- **Cabin**: Cabin number of the passenger.\n- **Embarked**: Port of Embarkation. (C = Cherbourg, Q = Queenstown, S = Southampton)","f5a9e737":"Accuracy of the predictions are not bad but maybe we can improve the accuracy with a different k value.","d3819b82":"We don't have a straightforward method to estimate the age of each passenger and since the age features has a consiedrable amount of missing values we can't drop them either. Therefore, we will just leave those missing values as they are.","b88e181d":"### 2.1.2 Missing Fare Value","1dfe0104":"Just like our previous assumption, the plots and statistics supports our assumption. The survival rate of the females are considerably higher than the survival rate of males. This is mostly caused by the priority of the females when the evacuations took place.","6e586cb3":"Which means there are 891 people in our train set and 418 people in our test set.","67ac78b5":"### 5.3 Support-Vector Machine(SVM)","005c2eb1":"As you can see, using the k=9 value gave us a more accurate result. Therefore a better prediction."}}