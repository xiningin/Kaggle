{"cell_type":{"ebf364aa":"code","ff50c959":"code","58fc32cc":"code","365e33aa":"code","4ceed96e":"code","f632285d":"code","5f91bed9":"code","a8294612":"code","916b1ff8":"code","f892c8f6":"code","2cc4daa1":"code","8c2a76de":"code","455ac5fa":"markdown"},"source":{"ebf364aa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport time\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nimport tensorflow as tf","ff50c959":"data = open('\/kaggle\/input\/dinosaur-island\/dinos.txt', 'r').read()\ndata= data.lower()\n# The unique characters in the file\nchars = list(set(data))\ndata_size, vocab_size = len(data), len(chars)\nprint('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))","58fc32cc":"char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\nix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\nprint(ix_to_char)","365e33aa":"def build_data(vocab_size, Tx = 40):\n    \"\"\"\n    Create a training set by scanning a window of size Tx over the text corpus, with stride 3.\n    \n    Arguments:\n    text -- string, corpus of Shakespearian poem\n    Tx -- sequence length, number of time-steps (or characters) in one training example\n    stride -- how much the window shifts itself while scanning\n    \n    Returns:\n    X -- list of training examples\n    Y -- list of training labels\n    \"\"\"\n\n    # Build list of all dinosaur names (training examples)\n    with open(\"\/kaggle\/input\/dinosaur-island\/dinos.txt\") as f:\n        examples = f.readlines()\n    examples = [x.lower().strip() for x in examples]\n    \n    m = len(examples)\n    X = np.zeros((m, Tx, vocab_size))\n    Y = np.zeros((m, Tx, vocab_size))\n    \n    for i, name in enumerate(examples):\n        name_ids = [char_to_ix[ch] for ch in name]\n        name_onehot = tf.one_hot(name_ids, depth=27)\n        \n        X[i,0:len(name),:] = name_onehot\n        X[i,len(name):,0] = 1\n        \n        Y[i,0:len(name)-1,:] = name_onehot[1:,:]\n        Y[i,len(name)-1:,0] = 1\n    \n    print('number of training examples:', m)\n    \n    return X, Y","4ceed96e":"X, Y = build_data(vocab_size, Tx = 40)","f632285d":"Y.shape","5f91bed9":"from keras.models import Sequential\nfrom keras.layers import Input, Dense, GRU  # Embedding","a8294612":"class MyModel(tf.keras.Model):\n  def __init__(self, vocab_size, rnn_units):\n    super().__init__(self)\n    # self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n    self.gru = tf.keras.layers.GRU(rnn_units,\n                                   return_sequences=True,\n                                   return_state=True)\n    self.dense = tf.keras.layers.Dense(vocab_size) #, activation='softmax')\n\n  def call(self, inputs, states=None, return_state=False, training=False):\n    x = inputs\n    # x = self.embedding(x, training=training)\n    if states is None:\n      states = self.gru.get_initial_state(x)\n    x, states = self.gru(x, initial_state=states, training=training)\n    x = self.dense(x, training=training)\n\n    if return_state:\n      return x, states\n    else:\n      return x","916b1ff8":"# Length of the vocabulary in chars\nvocab_size = vocab_size\n\n# Number of RNN units\nrnn_units = 256\n\nmodel = MyModel(\n    vocab_size=vocab_size,\n    rnn_units=rnn_units)\nmodel.build((None, X.shape[1], X.shape[2]))\nmodel.summary()","f892c8f6":"loss = tf.losses.CategoricalCrossentropy(from_logits=True)\nmodel.compile(loss=loss, optimizer='adam')  #, metrics=['accuracy'])\nhistory = model.fit(X, Y, batch_size=64, epochs=300, verbose = 2)","2cc4daa1":"class OneStep(tf.keras.Model):\n  def __init__(self, model):\n    super().__init__()\n    self.model = model\n\n\n  def generate_one_step(self, inputs, states=None, Tx=1, vocab_size=27):\n    # Convert strings to token IDs.\n    input_x = np.zeros((1, len(inputs), vocab_size))\n    name_ids = [char_to_ix[ch] for ch in inputs]\n    name_onehot = tf.one_hot(name_ids, depth=27)\n    input_x[0,:,:] = name_onehot\n    \n    input_x = tf.constant(input_x)\n\n    # Run the model.\n    predicted_logits, states = self.model(inputs=input_x, states=states,\n                                          return_state=True)\n    # Only use the last prediction.\n    predicted_logits = predicted_logits[:, -1, :]\n\n    # Sample the output logits to generate token IDs.\n    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n\n    # Convert from token ids to characters\n    predicted_chars = ''.join(ix_to_char[index] for index in predicted_ids.numpy())\n\n    # Return the characters and model state.\n    return predicted_chars, states\n\none_step_model = OneStep(model)","8c2a76de":"start = time.time()\n\n# Generate 10 dinasaurus names beginning with 'm'\nfor i in range(10):\n    states = None\n    next_char = 'm'\n    result = [next_char]\n    for n in range(20):\n        next_char, states = one_step_model.generate_one_step(next_char, states=states)\n        if next_char=='\\n':\n            break\n        result.append(next_char)\n    result = tf.strings.join(result)\n    result = result.numpy().decode('utf-8')\n    result = result[0].upper() + result[1:]\n    print(result)\n\nend = time.time()\nprint('\\nRun time:', end - start)","455ac5fa":"![](http:\/\/datascience-enthusiast.com\/figures\/dinos3.png)"}}