{"cell_type":{"d25ebd9e":"code","fbd9c2a4":"code","8c06d870":"code","2aade4d5":"code","c34a277e":"code","a95bac81":"code","c6d575d5":"code","6d397c54":"code","04737320":"code","a808c5c2":"code","31aabf39":"code","5a814f64":"code","69d8e227":"code","b366684b":"code","30053247":"code","f515d581":"code","5cb0b02a":"code","a6fd403d":"code","ce8ac170":"code","de578247":"code","c8d9670d":"code","afa3a041":"code","d9e80034":"code","536f6175":"markdown","93c47bc1":"markdown","25160f8d":"markdown","9f759bdc":"markdown","609686fe":"markdown","1e5480b1":"markdown","b5665298":"markdown"},"source":{"d25ebd9e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,VotingClassifier,BaggingClassifier,AdaBoostClassifier,GradientBoostingClassifier \nfrom sklearn.metrics import accuracy_score,roc_auc_score","fbd9c2a4":"#read data\ndf = pd.read_csv('..\/input\/heart-disease-uci\/heart.csv')\ndf.head()","8c06d870":"#Get number of rows and columns of data\ndf.shape","2aade4d5":"df.info()","c34a277e":"#get the statistics from data\ndf.describe()","a95bac81":"#Get count and percentage for independant variable (target)\ndisplay(df.target.value_counts())\ndf.target.value_counts()\/len(df)","c6d575d5":"sns.countplot(x='target',data=df)","6d397c54":"#Get count and percentage for variable (sex)\ndisplay(df.sex.value_counts())\ndf.sex.value_counts()\/len(df)","04737320":"sns.countplot(x='sex', data=df, palette=\"mako_r\")\nplt.xlabel(\"Sex (0 = female, 1= male)\")\nplt.show()","a808c5c2":"def get_ferq_visulaization(x, color , xlabel ,title, figsize= (8,6)):\n    pd.crosstab(x,df.target).plot(kind=\"bar\",figsize=figsize,color=color)\n    plt.title(title)\n    plt.xlabel(xlabel)\n    plt.ylabel('Frequency')\n    plt.show()","31aabf39":"#Heart Disease Frequency for age\nget_ferq_visulaization(df.age,color=['#5bc0de','#d9534f'],xlabel='Age',title='Heart Disease Frequency for Ages',figsize=(15,8))","5a814f64":"#Heart Disease Frequency for sex\nget_ferq_visulaization(df.sex,color=['#11A5AA','#AA1190'],xlabel='Sex',title='Heart Disease Frequency for Sex')","69d8e227":"#Heart Disease Frequency for fbs\nget_ferq_visulaization(df.fbs,color=['#FFC300','#581845'],xlabel='fbs',title='Heart Disease Frequency for FBS')","b366684b":"get_ferq_visulaization(df.slope,color=['#11A5AA','#AA1190'],xlabel='Slope',title='Heart Disease Frequency for Slopes')","30053247":"get_ferq_visulaization(df.cp,color=['#1CA53B','#AA1111'],xlabel='cp',title='Heart Disease Frequency for cp')","f515d581":"sns.pairplot(df)","5cb0b02a":"#take copy from data and apply on in data preprocessing\ndf_copy=df.copy()\ndf_copy.shape","a6fd403d":"#remove outliers by removing data geater than 3 std\nmean = df_copy[['chol','thalach','oldpeak']].mean()\nstd = df_copy[['chol','thalach','oldpeak']].std()\ncut_off = std * 3\nlower, upper = mean - cut_off, mean + cut_off\nnew_df = df_copy[(df_copy[['chol','thalach','oldpeak']] < upper) & (df_copy[['chol','thalach','oldpeak']] > lower)]\nnew_df.shape","ce8ac170":"#get one-hot-encoding to categorical variables\na = pd.get_dummies(df_copy['cp'], prefix = \"cp\")\nb = pd.get_dummies(df_copy['thal'], prefix = \"thal\")\nc = pd.get_dummies(df_copy['slope'], prefix = \"slope\")\n\n\n# concat the one-hot-encoding variables with data\ndata = [df_copy, a, b, c]\ndf_copy = pd.concat(data, axis = 1)\ndf_copy = df_copy.drop(columns = ['cp', 'thal', 'slope'])\ndf_copy.head()","de578247":"y=df_copy['target']\nX=df_copy.drop(['target'],axis=1)\n#split the data to train and test data\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,random_state=0)","c8d9670d":"#scale the data\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test=scaler.transform(X_test)","afa3a041":"#LogisticRegression\nlr=LogisticRegression(random_state = 1)\n# KNN Model\nknn = KNeighborsClassifier(n_neighbors = 2)  \n#DecisionTree model\ndt = DecisionTreeClassifier(random_state=1)\n# Random Forest Classification\nrf = RandomForestClassifier(n_estimators = 1000, random_state = 1)\n# Instantiate a VotingClassifier 'vc'\nvc = VotingClassifier(estimators=classifiers[0:4])\n# Instantiate a BaggingClassifier 'bc'\nbc_knn = BaggingClassifier(base_estimator=knn, n_estimators=300, n_jobs=-1)\n# Instantiate a BaggingClassifier 'bc'\nbc_lr = BaggingClassifier(base_estimator=lr, n_estimators=300, n_jobs=-1)\n# Instantiate a classification-tree 'dt' for AdaBoost\ndt_adb = DecisionTreeClassifier(max_depth=1, random_state=1)\n# Instantiate an AdaBoost classifier 'adab_clf'\nadb_clf = AdaBoostClassifier(base_estimator=dt_adb, n_estimators=100)\n# Instantiate a GradientBoostingRegressor 'gbt'\ngbt = GradientBoostingClassifier(n_estimators=300, max_depth=1, random_state=1)","d9e80034":"\n# Define a list called classifier that contains the tuples (classifier_name, classifier)\nclassifiers = [('Logistic Regression', lr),\n               ('K Nearest Neighbours', knn),\n               ('Classification Tree', dt),\n               ('Random Forest',rf),\n               ('Voting Classifier',vc),\n               ('Bagging Classifier for knn',bc_knn),\n               ('Bagging Classifier for logistic regression',bc_lr),\n               ('AdaBoost Classifier',adb_clf),\n               ('GradientBoosting Classifier',gbt)]\n\n# Iterate over the defined list of tuples containing the classifiers\nfor clf_name, clf in classifiers:\n    #fit clf to the training set\n    clf.fit(X_train, y_train)\n    # Predict the labels of the test set\n    y_pred = clf.predict(X_test)\n    # Evaluate the accuracy of clf on the test set\n    print('{:s} : {:.3f}'.format(clf_name, accuracy_score(y_test, y_pred)))\n","536f6175":"**As seen, all of data's datatypes are numeric values (int, float)\nand there aren't null values in dataset**","93c47bc1":"# Building and comparing classifiers accuracy","25160f8d":"**Our data will need feature scaling since there are big differences in value ranges of features and in std**\n_________________________________________________________________________________","9f759bdc":"# Data Preprocessing & Feature Engineering","609686fe":"All data lies in 3 std. No outliers removed.","1e5480b1":"# **Data Exploration**","b5665298":"# Heart Disease Frequencies"}}