{"cell_type":{"033c4ef6":"code","a958fa81":"code","1bf85b3b":"code","f557eff0":"code","fa0f4906":"code","f259850c":"code","d2e3cb14":"code","635cf8e7":"code","38acd1fc":"code","40dd42fe":"code","294e0e50":"code","7cf01591":"code","8bc2f4c4":"code","30a0bfff":"code","6bb44590":"code","c1d0de57":"code","d131cd5a":"code","96078099":"code","7f1b6926":"code","90580583":"code","d4afeb80":"code","012ba7a7":"code","d59a1b9b":"code","a7d870cb":"code","1d1596f2":"code","c86af4e9":"code","f341b96d":"code","ef5216fa":"code","71e32cf1":"code","aae28934":"code","323b2a9e":"code","9909bb1f":"code","1ba49477":"code","87b4295a":"code","ba416aee":"code","0de047be":"code","a1b7b430":"code","acc16402":"code","26f19a90":"code","32c819a1":"code","ee64b3d6":"code","6db76d76":"code","9be444ab":"code","d56e63f6":"code","501e8021":"code","9bea2f6c":"code","cdbb8712":"code","e4c2a5fa":"code","7618fbd0":"code","bbbde0f3":"code","bf17c693":"code","998f254b":"code","f35d4b1a":"code","b637da01":"code","472831d5":"code","d3a15f45":"code","4d216766":"code","eecccd9a":"code","1826dbb4":"code","c89b2c2a":"code","470d77f2":"code","5296805b":"code","d377e2de":"code","608df7da":"code","840b5bc4":"code","8aeacd50":"code","65aad456":"code","b150993d":"code","2fdfc832":"code","ea170a57":"code","ca6d47ab":"code","9579eb7b":"code","f35a64d5":"code","1ca3f9b1":"code","ed6db2e6":"code","70c6693b":"code","cf552e3b":"code","ec4b8cea":"code","570fe909":"code","6c351cd1":"code","b206f0c5":"code","f17b4c55":"code","ce5686a3":"code","a171e753":"code","4dcaa5f2":"code","73622e05":"code","fe378df3":"code","2937af3a":"code","60809119":"code","d178933c":"code","5678a1de":"code","f16f6b23":"code","19c4a3a3":"code","3a4f9bd9":"code","446d3f47":"code","18f0ed3c":"code","2240d804":"code","71610fdb":"code","3a9363af":"code","97cd1588":"code","a888cebf":"code","bf24e58d":"code","99f3a547":"code","8878e804":"code","10b1a203":"code","14921718":"code","75fff1f0":"markdown","7095716d":"markdown","960be66e":"markdown","9154a868":"markdown","2776807f":"markdown","d37c6100":"markdown","5e56928d":"markdown","c33bcbb9":"markdown","8c769db6":"markdown","565812d7":"markdown","0e2e672e":"markdown","6f7b55b9":"markdown","03331429":"markdown","c5677d7d":"markdown","2ee6f167":"markdown","edaa0b07":"markdown","d4889d26":"markdown","ab509c3e":"markdown","b752759f":"markdown","66e76ff3":"markdown","f2915802":"markdown","f1543aaf":"markdown","fe179c9f":"markdown","dbc591b9":"markdown","ac9e7be1":"markdown","b0b80f7a":"markdown","c335e8bf":"markdown","e8204cae":"markdown","56006b2a":"markdown","a4d88f07":"markdown","7942c5b7":"markdown","640c377c":"markdown","7fd82ea6":"markdown","6776158b":"markdown","05594ee1":"markdown","b32549c1":"markdown","d7b3f7e0":"markdown","49055876":"markdown","b59e5056":"markdown","95078e6b":"markdown","65107752":"markdown","24e22dfa":"markdown","7751795e":"markdown","ec808a2e":"markdown","dd085938":"markdown","5590bb11":"markdown","e2e30e06":"markdown","bd87e0e8":"markdown","1cc390bd":"markdown","7daff27f":"markdown","e120db87":"markdown","f8db6201":"markdown","4d4617c9":"markdown","7cb1f424":"markdown","90031ade":"markdown","2974a750":"markdown","e3b2a2ec":"markdown","c6c68302":"markdown","c9bffd24":"markdown","5312dede":"markdown","515d74c7":"markdown","30e724f3":"markdown","5640b65e":"markdown","737b79b8":"markdown","728b2b25":"markdown","7146938f":"markdown","f768e9ca":"markdown","d28c6042":"markdown","c97ded9b":"markdown","77f82a25":"markdown","ccc40aba":"markdown","8675ae8d":"markdown","927bdc6e":"markdown","bb9a70a5":"markdown","05dab7bf":"markdown","12267fea":"markdown","4521b13f":"markdown","d2683745":"markdown","5c4c974b":"markdown","d88ddb2e":"markdown","25116bfc":"markdown","573203d0":"markdown","a4311633":"markdown","26aff98a":"markdown","cff1c7d1":"markdown","2f3df1b3":"markdown","bfcef2f1":"markdown","37324b5c":"markdown"},"source":{"033c4ef6":"#@title Current Date\nToday = '2021-11-19' #@param {type:\"date\"}\n","a958fa81":"#@markdown ---\n#@markdown ### Enter your details here:\nTeam_Number = \"3\" #@param {type:\"string\"}\nTeam_Name = \"TeamThree\" #@param {type:\"string\"}\nStudent_ID = \"12152854\" #@param {type:\"string\"}\nStudent_full_name = \"Ashley Bentley\" #@param {type:\"string\"}\nStudent_ID = \"19133359\" #@param {type:\"string\"}\nStudent_full_name = \"Eva Radescu\" #@param {type:\"string\"}\nStudent_ID = \"21026416\" #@param {type:\"string\"}\nStudent_full_name = \"Deepika V Chhatpar\" #@param {type:\"string\"}\nStudent_ID = \"21041725\" #@param {type:\"string\"}\nStudent_full_name = \"Johnathan Fernandes\" #@param {type:\"string\"}\nStudent_ID = \"21033951\" #@param {type:\"string\"}\nStudent_full_name = \"Tejaswini Javagal\" #@param {type:\"string\"}\n#@markdown ---","1bf85b3b":"#@title Notebook information\nNotebook_type = 'Assignment' #@param [\"Example\", \"Lab\", \"Practice\", \"Etivity\", \"Assignment\", \"Exam\"]\nVersion = \"Final\" #@param [\"Draft\", \"Final\"] {type:\"raw\"}\nSubmission = True #@param {type:\"boolean\"}","f557eff0":"# Import packages\n\n# Data processing\nimport numpy as np\nimport pandas as pd\nfrom scipy.sparse import hstack # Concatenate sparse matrices \n\n# Data visualization\nimport folium # Create maps\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom folium.plugins import MarkerCluster\n\n# Text Processing\nimport nltk # Natural Language ToolKit\nimport re # Regular expressions\nimport string # To detect punctuation\nfrom nltk.corpus import stopwords # Import stop words list\nfrom nltk.stem import WordNetLemmatizer # Lemmatization\nfrom nltk.tokenize import word_tokenize # tokenization\n\n# Machine learning\nfrom sklearn.preprocessing import OneHotEncoder # Encode categorical variables\nfrom sklearn.preprocessing import MinMaxScaler # Scale numerical data\nfrom sklearn.feature_extraction.text import TfidfVectorizer # Vectorization\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import\tmean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LinearRegression\nfrom xgboost import XGBRegressor # XGBoost Regression\n\n# Deep learning\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau # Model callbacks for training\n\n# File handling\nfrom google.colab import files # Enables direct file uploads in colab","fa0f4906":"# Other setup\nsns.set_theme(style=\"whitegrid\", \n              palette=\"colorblind\")","f259850c":"# Upload data to Google Colab\nuploaded = files.upload()","d2e3cb14":"# Import data\ndf = pd.read_csv(\"HousePrice_Train.csv\")","635cf8e7":"# View data\ndf","38acd1fc":"df.info() # Data types of columns","40dd42fe":"df.describe() # Statistical properties of dataset","294e0e50":"df.shape # Number of (rows, columns)","7cf01591":"df.head() # First few rows","8bc2f4c4":"df.tail() # Last few rows","30a0bfff":"df.isnull().sum() # Check number of missing \/ Nan \/ NULL values per column","6bb44590":"# Draw histograms for data\ncolumn_list = ['Num_Bathrooms', 'Num_Beds', 'BER_class', 'Type','Surface', 'Price']\nfor column in column_list: # Iterate through dataset columns from list\n    plt.figure(figsize=(20,5)) # Set figure size\n    sns.histplot(df[column]) # Create histogram\n    plt.show() # Show histogram","c1d0de57":"# Draw up scatterplot to examine the relationship between numerical variables and price\ncolumn_list = ['Num_Bathrooms', 'Num_Beds', 'Latitude', 'Longitude','Surface']\nfor column in column_list:\n    plt.figure(figsize=(20,5)) \n    sns.scatterplot(df[column],df[\"Price\"])\n    plt.show()","d131cd5a":"# Box plots to look for outliers\ncolumn_list = ['BER_class','Type']\nfor column in column_list:\n    plt.figure(figsize=(20,5)) \n    sns.boxplot(df[column],df[\"Price\"])\n    plt.show()","96078099":"# View correlation of variables\nplt.figure(figsize= (10, 10)) # Define plot size, increase if the graph is crowded\nsns.heatmap(df.iloc[:,2:].corr(),square=True, annot=True)","7f1b6926":"# Visualize locations \n\nlocations = df[['Latitude', 'Longitude']] # Store locations as a set of coordinates\nlocationlist = locations.values.tolist() # convert to a list\n\n# Set map parameters.\nmap = folium.Map(location=[df['Latitude'].median(), df['Longitude'].median()], # Center the default map location around our data\n                 zoom_start=12) \nmarker_cluster = MarkerCluster().add_to(map) # condense functions to create clusters when zoomed out\n\nfor point in range(0, len(locationlist)): # Iterate through list of coordinates\n    folium.Marker(locationlist[point], popup=df['Location'][point]).add_to(marker_cluster) # Add each to map\nmap # Display the map. Takes a while to load!","90580583":"df[df[\"Latitude\"] < 52.6]","d4afeb80":"df[df[\"Longitude\"] > -2]","012ba7a7":"outlier_index = df[df[\"ID\"] == 12270559].index # Store index of row where ID = 12270559\ndf.drop(outlier_index, inplace=True) # Delete it","d59a1b9b":"df[df[\"ID\"] == 12270559] # find rows where ID = 12270559","a7d870cb":"df[df[\"Num_Bathrooms\"] > 10]","1d1596f2":"df[df[\"Num_Beds\"] > 10]","c86af4e9":"fancy_house_IDs = [\"12381836\",\"11780612\",\"12085770\"]\nfor houseID in fancy_house_IDs:\n    outlier_index = df[df[\"ID\"] == houseID].index\n    df.drop(outlier_index, inplace=True)","f341b96d":"outlier_index = df[df[\"Surface\"] > 5000].index \ndf.drop(outlier_index, inplace=True)","ef5216fa":"plt.figure(figsize= (10, 10))\nsns.heatmap(df.iloc[:,2:].corr(),square=True, annot=True)","71e32cf1":"df_numeric = df[[\"Location\",\"Num_Bathrooms\",\"Num_Beds\",\"BER_class\",\"Latitude\",\"Longitude\",\"Type\",\"Surface\",\"Price\"]]","aae28934":"# Initialize (set up) encoder to do the heavy lifting for us\nencoder = OneHotEncoder(drop=\"first\", # Remove the first column\n                        sparse=False, # Set to return an array instead of a matrix\n                        handle_unknown=\"ignore\") # Don't throw errors up if null values are found","323b2a9e":"columns_to_replace = [\"Location\",\"BER_class\",\"Type\"] # List out columns to encode \nencoded_data = encoder.fit_transform(df[columns_to_replace]) # Encode them\nencoded_data = pd.DataFrame(encoded_data) # Convert this to a dataframe\nencoded_data.head() # View data","9909bb1f":"encoded_data.columns = encoder.get_feature_names_out() # Rename encoded columns","1ba49477":"encoded_data.head()","87b4295a":"for column in columns_to_replace: # Iterate through list of columns\n    df_numeric.drop(column ,axis=1, inplace=True) # Drop (delete) the ones we encoded","ba416aee":"df_numeric = encoded_data.join(df_numeric) # Concatenate (join) the dataframes","0de047be":"df_numeric.columns.tolist() # Check column names of final encoded data","a1b7b430":"df_numeric_noscale = df_numeric.copy()","acc16402":"scaler = MinMaxScaler() # Initialize scaler ","26f19a90":"for column in df_numeric.columns:\n    df_numeric[column]=pd.DataFrame(scaler.fit_transform(df_numeric[column].values.reshape(-1,1))) # Apply scaler","32c819a1":"df_numeric.describe()","ee64b3d6":"df_textual = df[[\"Description\",\"Services\",\"Features\",\"Price\"]]","6db76d76":"lemmatizer = WordNetLemmatizer() # Initialize Lemmatizer\nnltk.download('wordnet')# Download lemmatization database\nnltk.download('stopwords') # Download list of stop words\nnltk.download('punkt') # download list of punctuation symbols\nstopwords_set = set(stopwords.words('english')) # Define list of stop words","9be444ab":"stopwords_set","d56e63f6":"def clean(row):\n    input_data = str(row)\n\n    lowertext = input_data.lower() # convert to lower case\n    tokens = word_tokenize(lowertext) # Tokenize\n    df_stopwords=[word for word in tokens if word not in stopwords_set] # Remove stopwords\n    df_punctuation=[re.sub(r'['+string.punctuation+']+', ' ', i) for i in df_stopwords] # Remove Punctuation and split 's, 't, 've with a space for filter\n    df_whitespace = ' '.join(df_punctuation) # Remove multiple whitespace\n    lemmatizer = WordNetLemmatizer() # Initialize lemmatizer\n    df_lemma = lemmatizer.lemmatize(df_whitespace) # Lemmatize\n    df_lemma_tokenized = word_tokenize(df_lemma) # Tokenize again\n    df_lemma_shortwords = [re.sub(r'^\\w\\w?$', '', i) for i in df_lemma_tokenized] # Remove short words\n    df_lemma_whitespace =' '.join(df_lemma_shortwords) # Join whitespace again\n    df_lemma_multiplewhitespace = re.sub(r'\\s\\s+', ' ', df_lemma_whitespace) # Join multiple white spaces\n    df_clean = df_lemma_multiplewhitespace.lstrip(' ') #Remove any whitespace at the front of the sentence\n\n    return df_clean","501e8021":"df_textual[\"Description\"][0] # Original","9bea2f6c":"clean(df_textual[\"Description\"][0]) # Cleaned","cdbb8712":"df_textual_cleaned = df_textual.iloc[:,:-1].applymap(clean) # Apply cleaning to entire dataset","e4c2a5fa":"df_textual_cleaned = pd.concat([df_textual_cleaned,df_textual[\"Price\"]],axis=1) # concatenate price","7618fbd0":"df_textual_cleaned.head() # View cleaned dataset","bbbde0f3":"# Initialize vectorizer\nvectorizer = TfidfVectorizer(max_features= 500, # consider only the top 500 common words\n                             max_df=0.7) # Ignore words that appear in more than 70% of documents","bf17c693":"X0 = vectorizer.fit_transform(df_textual_cleaned.iloc[:,0])\nX1 = vectorizer.fit_transform(df_textual_cleaned.iloc[:,1])\nX2 = vectorizer.fit_transform(df_textual_cleaned.iloc[:,2])","998f254b":"X_textual = hstack((X0, X1, X2))\nX_textual.shape","f35d4b1a":"y_textual = df_textual_cleaned.iloc[:,-1]\ny_textual.shape","b637da01":"X_textual_train, X_textual_val, y_textual_train, y_textual_val = train_test_split(X_textual, y_textual, test_size = 0.20)","472831d5":"# create a model\nMNB = MultinomialNB()\n\n# fit to data\nMNB.fit(X_textual_train, y_textual_train)","d3a15f45":"# Training accuracy\ny_textual_train_pred = MNB.predict(X_textual_train)\naccuracy_score(y_textual_train, y_textual_train_pred)","4d216766":"# Validation accuracy\ny_textual_val_pred = MNB.predict(X_textual_val)\naccuracy_score(y_textual_val, y_textual_val_pred)","eecccd9a":"df_test = pd.read_csv(\"HousePrice_Test.csv\")","1826dbb4":"df_textual_test = df_test[[\"Description\",\"Services\",\"Features\"]]","c89b2c2a":"df_textual_cleaned_test = df_textual_test.iloc[:,:-1].applymap(clean) # Apply cleaning to entire dataset","470d77f2":"X0_test = vectorizer.fit_transform(df_textual_cleaned.iloc[:,0])\nX1_test = vectorizer.fit_transform(df_textual_cleaned.iloc[:,1])\nX2_test = vectorizer.fit_transform(df_textual_cleaned.iloc[:,2])","5296805b":"X_textual_test = hstack((X0_test, X1_test, X2_test))","d377e2de":"y_pred = MNB.predict(X_textual_test)","608df7da":"# save predictions in a file\ndf_id = pd.DataFrame(data=np.arange(1639,2341), columns = ['Index'])\ndf_class = pd.DataFrame(data=y_pred, columns = ['Price'])\ndf_pred = pd.concat([df_id, df_class], axis=1)\n \n# change 'YourTeam' by your team number, for instance: Team-1\ndf_pred.to_csv('CS6501_Kaggle_TeamThree.csv', sep=',', index=False)","840b5bc4":"df_numeric = df_numeric.fillna(-1)","8aeacd50":"X_numeric = df_numeric.iloc[:,:-1]\nX_numeric.shape","65aad456":"y_numeric = df_numeric.iloc[:,-1]\ny_numeric.shape","b150993d":"X_numeric_train, X_numeric_val, y_numeric_train, y_numeric_val = train_test_split(X_numeric, y_numeric, test_size = 0.20)","2fdfc832":"# define the regressor\n\nregressor = Sequential() # Sequential model\n\nnum_units = 300 # Number of units per layer\ndrop_value = 0.65 # Change to drop out connections\n\n# Add LSTM layer\nregressor.add(LSTM(units = num_units, \n                   return_sequences = True, \n                   input_shape = (X_numeric_train.shape[1], 1)))\n\n# Add dropout layer\nregressor.add(Dropout(drop_value))\n\nregressor.add(LSTM(units = num_units, return_sequences = True))\nregressor.add(Dropout(drop_value))\n\nregressor.add(LSTM(units = num_units, return_sequences = True))\nregressor.add(Dropout(drop_value))\n\nregressor.add(LSTM(units = num_units))\nregressor.add(Dropout(drop_value))\n\n# Add dense layer for output predictions\nregressor.add(Dense(units = 1))","ea170a57":"regressor.compile(optimizer='adam', # Configures the model for training using \"Adam\" optimizer\n              loss='mean_squared_error', # Loss function\n              metrics=['accuracy']) # Performance metric","ca6d47ab":"# Define callbacks\ncheckpoint = ModelCheckpoint(\"checkpoints\", # Directory\n                             monitor='accuracy', # Performance metric to monitor\n                             verbose=1, # Print update messages\n                             save_best_only=True, # Save only best performing model\n                             save_weights_only=False, # Save only weights from model\n                             mode='max', # Criteria to replace saved model\n                             save_freq='epoch') # Frequency to save model\n\nearlystop = EarlyStopping(monitor='accuracy',\n                          min_delta=1e-4, # Minimum change in the monitored quantity\n                          patience=7, # Number of epochs with no improvement\n                          verbose=1,\n                          mode='max',\n                          baseline=None, # Baseline value for the monitored quantity\n                          restore_best_weights=True) # restore model weights from the epoch with the best value of the monitored quantity\n\nlrreduction = ReduceLROnPlateau(monitor='accuracy',\n                                factor=0.01, # new lr = lr * factor.\n                                patience = 4, # number of epochs with no improvement\n                                verbose=1,\n                                mode='max',\n                                min_delta=1e-4, # threshold for measuring the new optimum\n                                cooldown=0, # number of epochs to wait before resuming normal operation after lr has been reduced\n                                min_lr=1e-6) # lower bound on the learning rate\n\ncallbacks = [checkpoint, earlystop, lrreduction]","9579eb7b":"num_epochs = 128\nregressor.fit(X_numeric_train, y_numeric_train, \n              epochs = num_epochs, \n              batch_size = 32,\n              callbacks = callbacks)","f35a64d5":"best_model = keras.models.load_model('checkpoints')","1ca3f9b1":"# Training set mean squared error (MSE)\ny_numeric_train_pred = best_model.predict(X_numeric_train)","ed6db2e6":"y_numeric_train_pred = scaler.inverse_transform(y_numeric_train_pred)","70c6693b":"# Validation MSE\ny_numeric_val_pred = best_model.predict(X_numeric_val)","cf552e3b":"y_numeric_val_pred = scaler.inverse_transform(y_numeric_val_pred)","ec4b8cea":"mean_squared_error(y_numeric_val, y_numeric_val_pred)","570fe909":"df_test = pd.read_csv(\"HousePrice_Test.csv\")","6c351cd1":"df_numeric_test = df_test[[\"Location\",\"Num_Bathrooms\",\"Num_Beds\",\"BER_class\",\"Latitude\",\"Longitude\",\"Type\",\"Surface\"]]","b206f0c5":"encoded_data = encoder.transform(df_numeric_test[columns_to_replace]) # Encode them\nencoded_data = pd.DataFrame(encoded_data) # Convert this to a dataframe\nencoded_data.columns = encoder.get_feature_names_out() # Rename encoded columns\n\nfor column in columns_to_replace: # Iterate through list of columns\n    df_numeric_test.drop(column ,axis=1, inplace=True) # Drop (delete) the ones we encoded\ndf_numeric_test = pd.concat([encoded_data,df_numeric_test],axis=1) # Concatenate (join) the dataframes\n\nscaler = MinMaxScaler() # Initialize scaler \nfor column in df_numeric_test.columns:\n    df_numeric_test[column]=pd.DataFrame(scaler.fit_transform(df_numeric_test[column].values.reshape(-1,1))) # Apply scaler","f17b4c55":"X_numeric_test = df_numeric_test","ce5686a3":"y_pred = best_model.predict(X_numeric_test)","a171e753":"y_pred = scaler.inverse_transform(y_pred)","4dcaa5f2":"# save predictions in a file\ndf_id = pd.DataFrame(data=np.arange(1639,2341), columns = ['Index'])\ndf_class = pd.DataFrame(data=y_pred, columns = ['Price'])\ndf_pred = pd.concat([df_id, df_class], axis=1)\n \n# change 'YourTeam' by your team number, for instance: Team-1\ndf_pred.to_csv('CS6501_Kaggle_TeamThree.csv', sep=',', index=False)","73622e05":"data = df_numeric.dropna()","fe378df3":"data_X = data.iloc[:,-6:-1]\ndata_y = data.iloc[:,-1]","2937af3a":"data_X_train, data_X_val, data_y_train, data_y_val = train_test_split(data_X, data_y, test_size = 0.2)","60809119":"Linear_Model = LinearRegression()","d178933c":"Linear_Model.fit(data_X_train, data_y_train)","5678a1de":"# Training score\nLinear_Model.score(data_X_train, data_y_train)","f16f6b23":"# Validation score\nLinear_Model.score(data_X_val, data_y_val)","19c4a3a3":"data = df_numeric.dropna()","3a4f9bd9":"data_X = data.iloc[:,:-1]\ndata_y = data.iloc[:,-1]","446d3f47":"data_X_train, data_X_val, data_y_train, data_y_val = train_test_split(data_X, data_y, test_size = 0.2)","18f0ed3c":"XGBR_Model = XGBRegressor()","2240d804":"XGBR_Model.fit(data_X_train, data_y_train)","71610fdb":"# Training score\nXGBR_Model.score(data_X_train, data_y_train)","3a9363af":"# Validation score\nXGBR_Model.score(data_X_val, data_y_val)","97cd1588":"# TODO","a888cebf":"df_test = pd.read_csv(\"HousePrice_Test.csv\")","bf24e58d":"df_numeric_test = df_test[[\"Location\",\"Num_Bathrooms\",\"Num_Beds\",\"BER_class\",\"Latitude\",\"Longitude\",\"Type\",\"Surface\"]]","99f3a547":"encoded_data = encoder.transform(df_numeric_test[columns_to_replace]) # Encode them\nencoded_data = pd.DataFrame(encoded_data) # Convert this to a dataframe\nencoded_data.columns = encoder.get_feature_names_out() # Rename encoded columns\n\nfor column in columns_to_replace: # Iterate through list of columns\n    df_numeric_test.drop(column ,axis=1, inplace=True) # Drop (delete) the ones we encoded\ndf_numeric_test = pd.concat([encoded_data,df_numeric_test],axis=1) # Concatenate (join) the dataframes\n\nscaler = MinMaxScaler() # Initialize scaler \nfor column in df_numeric_test.columns:\n    df_numeric_test[column]=pd.DataFrame(scaler.fit_transform(df_numeric_test[column].values.reshape(-1,1))) # Apply scaler","8878e804":"y_pred = XGBR_Model.predict(df_numeric_test)","10b1a203":"y_pred = scaler.inverse_transform(y_pred.reshape(-1,1))","14921718":"# save predictions in a file\ndf_id = pd.DataFrame(data=np.arange(1639,2341), columns = ['Index'])\ndf_class = pd.DataFrame(data=y_pred, columns = ['Price'])\ndf_pred = pd.concat([df_id, df_class], axis=1)\n \n# change 'YourTeam' by your team number, for instance: Team-1\ndf_pred.to_csv('CS6501_Kaggle_TeamThree.csv', sep=',', index=False)","75fff1f0":"The aim of this experiment is to design and develop a system to predict the price of a house based on features given in the dataset. The predictions from this system will simultaneously be submitted to a ranked Kaggle competition in addition to the usual submission on SULIS. We are expected to implement the machine learning concepts studied during this week, i.e. Natural Language Processing and Long Short Term Memory. We are also presented with the opportunity to apply prior knowledge to design an effective model.","7095716d":"\u201cNull\u201d or missing values are infamous for messing up prediction models, and should be processed very carefully. We use built in functions to check for null values.","960be66e":"The above code gets rid of 2 houses with an oddly high surface area.","9154a868":"While slightly skewed, the histograms do not immediately signal anything too dangerous. We do note that the \u201cBER_class\u201d column has a value \u201cSINo666of2006exempt\u201d, which is quite a mouthful. ","2776807f":"We apply the vectorizer to each column separately","d37c6100":"First up, the basic multiple linear regression","5e56928d":"### Numerical and categorical data","c33bcbb9":"Going further, we will need to process the data separately. For the next section, we actually separate the data into two sections \u2013 number\/categorical, and textual.","8c769db6":"TODO","565812d7":"## Exploratory Data Analysis (EDA)","0e2e672e":"Note: the score() method of most scikit-learn regressors automatically generate predictions and account for scaling. So while we don\u2019t need to worry about those here, we do need to watch out when generating predictions later.","6f7b55b9":"Looking good so far! It\u2019s boiled down the description to key terms, removed useless punctuation and pesky escape characters. We can apply the function to every cell of the dataset using the applymap() function.\n\nNote that since our dataset also contains the price data, we must be careful to exclude this column and then concatenate it with our cleaned dataset later. Our cleaning function may or may not affect the price rows, and I really don\u2019t want to find out right now.\n","03331429":"For the Kaggle competition, we are required to submit our predictions in a specific format. This helper code takes our predictions and creates the file for us.","c5677d7d":"Note the empty dataframe returned.","2ee6f167":"Taking a look at our data, we note that it has a mix of numerical, categorical and text data. Each of these will have to be processed separately. We start by looking through the data as best as we can, checking for missing values and analyzing for outliers.","edaa0b07":"We can validate our oulier removal by running the plots again. Let's check the heatmap again.","d4889d26":"The other technique studied during this week is \u201cLong Short Term Memory\u201d (LSTM). This is a neural network based model which works best on time series data, but should perform sufficiently on normal numerical data as well. We look forward to observing the time and accuracy tradeoff when using an LSTM based model on non-time series data. Currently, we intend to apply this technique solely on the numerical data. ","ab509c3e":"Graphs plotted and outliers identified, we move on to pre processing our data.","b752759f":"In order to predict data, we must import the given test data and apply the same operations we did on our training data in order for our model to recognize it. In this case, we have to clean and vectorize our data.","66e76ff3":"## Random forest regressor","f2915802":"An alternative to our scatter plot shenanigans is to calculate a correlation matrix and plot it using a heatmap.","f1543aaf":"While the preprocessing steps  technically count as NLP, I chose to include them in the general preprocessing section, leaving this one just for the vectorization and modelling. While we studied multiple vectorization techniques in class, namely \u201cCount Vectorization\u201d and \u201cTerm Frequency Inverse Document Frequency\u201d (TF-IDF), multiple other methods exist,such as \u201cWord2Vec\u201d and \u201cGlobal Vectors for Word Representation\u201d (GloVe). In this section, we will attempt to use a pure NLP approach (textual data only!) to predict house price.","fe179c9f":"The TF-IDF for each term (word) is calculated by \n\n$ {tf} * {idf}$\n\nWhere\n\n${tf} (t,d)={\\frac {f_{t,d}}{\\sum _{t'\\in d}{f_{t',d}}}}$\n\n${idf} (t,D)=\\log {\\frac {N}{|\\{d\\in D:t\\in d\\}|}}$\n\nComplex formulae aside, the term frequency(TF)  is simply the number of times a term(word) occurs in a document(sentence). Inverse Document Frequency(IDF) is the logarithmic inverse of the fraction of documents that contain the term. (meaning, divide total number of sentences by the number of sentences containing that word, and then take the log).\n\n","dbc591b9":"A common mistake we make when working with neural network architecture is not checking for null values. Null values in neural networks make the model collapse, leaving you with an accuracy\/ score of zero that never changes. \n\n\nAs documented by Fran\u00e7ois Chollet (creator of Keras) in his [book]( https:\/\/www.manning.com\/books\/deep-learning-with-python):\n\n\"In general, with neural networks, it\u2019s safe to input missing values as 0, with the condition that 0 isn\u2019t already a meaningful value. The network will learn from exposure to the data that the value 0 means missing data and will start ignoring the value.\"\n\nSince our data is scaled from 0 to 1, we impute(replace) the missing values with -1.","ac9e7be1":"We now define a cleaning function that should be able to reduce entire sentences down to their base forms.","b0b80f7a":"# NLP","c335e8bf":"Much better! Removing the outliers greatly enhances the correlation of our data, allowing us to build a better model for the reasonably prices houses.","e8204cae":"Next up, the industry standard XGBoost Regressor.","56006b2a":"Machine learning algorithms work exclusively with numbers. In order to apply them to categorical data, we must first transform it. \n\nThe na\u00efve approach would be to so something like the following:\n\nBefore:\n\n| ID | Letter | Target |\n|:--:|:------:|:------:|\n|  1 |    A   |    2   |\n|  2 |    B   |    3   |\n|  3 |    C   |    4   |\n|  4 |    B   |    5   |\n\nAfter:\n\n| ID | Letter | Target |\n|:--:|:------:|:------:|\n|  1 |    1   |    2   |\n|  2 |    2   |    3   |\n|  3 |    3   |    4   |\n|  4 |    2   |    5   |\n\nThis method ids known as \u201cLabel encoding\u201d\n\nHowever, simply assigning a number to a value is impossible, as we cannot associate words with numbers. While 1 > 2 is true, saying \u201cA = 1\u201d or \u201cA>B\u201d makes no sense when speaking in general terms.\n\nThe correct method is to treat each category as a column, and indicating the presence of the feature using a binary value.\n\nBefore:\n\n| ID | Letter | Target |\n|:--:|:------:|:------:|\n|  1 |    A   |    2   |\n|  2 |    B   |    3   |\n|  3 |    C   |    4   |\n|  4 |    B   |    5   |\n\nAfter:\n\n| ID | A | B | C | Target |\n|:--:|:-:|:-:|---|:------:|\n|  1 | 1 | 0 | 0 |    2   |\n|  2 | 0 | 1 | 0 |    3   |\n|  3 | 0 | 0 | 1 |    4   |\n|  4 | 0 | 1 | 0 |    5   |\n\nHOWEVER, we cannot directly toss this into a model and call it a day. Observing the table, it is fairly easy to infer to value of column \u201cC\u201d based on the values of column \u201cA\u201d and \u201cB\u201d. This is essentially a problem with machine learning known as \u201cmulticollinearity\u201d, and can cause issues when building the model. So to prevent any issues, we remove one column and finally end up with a table as:\n\n| ID | A | B | Target |\n|:--:|:-:|:-:|:------:|\n|  1 | 1 | 0 |    2   |\n|  2 | 0 | 1 |    3   |\n|  3 | 0 | 0 |    4   |\n|  4 | 0 | 1 |    5   |\n\nThis process is known as \u201cencoding\u201d, specifically \u201cone-hot encoding\u201d.\n","a4d88f07":"There seem to be a few extravagant houses in here that's throwing off our model.","7942c5b7":"Looking through the scatterplots, we note down;\n* There is a slight correlation between number of bathrooms and price, with a few houses having an exorbitant number of them. While technically feasible in real life, it might affect our model adversely if not removed.\n* The same can be said for number of bedrooms.\n* Plotting Latitude and Longitude shows a very slight correlation between location and price (which we know usually happens), but interestingly enough, contains a singular point all by its lonesome in the \u201cLongitude\u201d plot. We can look into this later in more detail.\n* And finally, there is a strong positive correlation between price and surface area. Not surprising.\n","640c377c":"After all that, it is finally time to run our model. We would like to note here that training models are computationally VERY expensive. It is best to run this in Google Colab with a GPU by going to \u201cRuntime\u201d->\u201dChange runtime type\u201d->Select \u201cGPU\u201d under \u201cHardware Accelerator\u201d.","7fd82ea6":"# LSTM","6776158b":"## Predicting on test data","05594ee1":"Since neural networks work with multiple layers, we have to define each layer:","b32549c1":"As insightful as those functions were, staring at raw numbers can only get us so far. Now we start visualizing the data to get a better understanding of its statistical properties, and also hunt down a few outliers, if present.","d7b3f7e0":"## Modelling","49055876":"<div>\n<img src=\"https:\/\/drive.google.com\/uc?export=view&id=1vK33e_EqaHgBHcbRV_m38hx6IkG0blK_\" width=\"350\"\/>\n<\/div> \n\n#**Artificial Intelligence - MSc**\n##CS6501 - MACHINE LEARNING APPLICATIONS \n\n###Instructor: Enrique Naredo\n###CS6501_Kaggle","b59e5056":"To start off, we grab a subset of the original dataframe containing only numerical and categorical data.","95078e6b":"The One Hot Encoder from the scikit -learn library [(documentation available here)]( https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OneHotEncoder.html) handles most of the work for us. It doesn\u2019t, however, retain column names, leaving us to fetch them from the \u201cget_feature_names_out\u201d function, and apply them to our new dataframe. It automatically converts the old column name into a prefix, so if the \"Type\" was \"Apartment\", the new column is \"Type_Apartment\".","65107752":"Looks like this is it!","24e22dfa":"Viewing the data on the map shows us that most of the data is in and around Dublin. Zooming out, however, reveals our outlier point. A singular point, all the way over in the United Kingdom! Considering we\u2019re looking at Ireland housing data, it\u2019s safe to say this point was included in our dataset accidently. We\u2019ll exclude it from our data before processing any further.","7751795e":"## Testing on test data","ec808a2e":"To start off, we initialize our lemmatizer object.\nLemmatizing is the process of reducing a word to its base form. It is an advanced form of stemming, taking into consideration the context of the word instead of blindly removing suffixes.\n\nFor example:\n\n|   Word   | Stemming | Lemmatizing |\n|:--------:|:--------:|:-----------:|\n|    was   |    wa    |      be     |\n|  studies |   studi  |    study    |\n| studying |   study  |    study    |\n\n\nAs we see from the example, lemmatizing is far superior to stemming when reducing words to their base form. The fewer words we have, the smaller our data will be and the faster our models will learn.\n\nWe also download a few sets:\n* Wordnet: the Lemmatization dataset.\n* Stopwords: Common \u201cstop\u201d words that do not add much to the context of a sentence.\n* Punkt: Punctuation marks.\n","dd085938":"Taking it step by step:\n* Lowercase: easier to process everything when you don\u2019t have to worry about case sensitivity.\n* Tokenize: Split sentences into words, with each word being a \u201ctoken\u201d.\n* Remove stopwords: Scans through the tokens of each sentence, checks them against the big list of stopwords, and only keeps them if the aren\u2019t present in the stopwords list.\n* Remove punctuation: Punctuation usually doesn\u2019t add much to the context of a sentence, and can be very ambiguous depending on usage. It\u2019s better to remove them altogether.\n* Whitespace: A lot of these operations don\u2019t actually remove the offending characters, but rather replace them with a space. We join multiple spaces together quite often here.\n* Lemmatization: Reduces words to their base form.\n\nThe odd symbols you see around the re.sub() functions are known as \u201cregular expressions\u201d. They are a way to scan through and identify patterns in blocks of text.","5590bb11":"Coming up next, we have scatterplots. We use these to examine the relationship between the predictor and target variables. A high correlation (data points forming a line) indicated that that feature would be a very good predictor.","e2e30e06":"### Textual data","bd87e0e8":"## Imports","1cc390bd":"The first thing we do is remove the outlier we just spotted. We do this by simply finding the ID of the specific point, and simply dropping it. ","7daff27f":"And then combine them all together","e120db87":"## Vectorization","f8db6201":"We can actually view the list of stop words!","4d4617c9":"Similar to the previous section, when predicting on new data, we first transform the input data to a format similar to our training data. For now, that entails encoding and scaling.\n\nA key detail to remember is that while the encoder is fit on training data and applied to test data with transform(), the scaler must be fit to training data too. We must also remember to use the scaler to inverse transform the data in order to get non-scaled predictions.\n","7cb1f424":"We can, observe the results of scaling by using the describe() function again.","90031ade":"The \u201cfolium\u201d package allows us to visualize a set of coordinates on an interactive map on interactive python notebooks, so we use it here to draw up a map of our houses. This should also help us investigate the outlier we observed earlier.","2974a750":"## Preprocessing","e3b2a2ec":"# OTHERS","c6c68302":"Now we work on removing the other outliers.","c9bffd24":"We can verify that this point is gone by searching for the same ID again.","5312dede":"# SUMMARY","515d74c7":"Now that our categorical features are all encoded, we move on to scaling. Scaling multiplies the data points by a specific value so that the resulting data lies between a set of values, usually 0 and 1. Doing so makes the model much more computationally efficient. Other methods of scaling also exist, applying different operations for specific use cases. We use the [scikit-learn MinMax scaler]( https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.MinMaxScaler.html) here as it benefits us when working with neural networks. (We\u2019ll get into that later).","30e724f3":"After designing the network, we \u201ccompile\u201d it together while specifying an optimization algorithm, a loss metric to minimize, and a performance metric to measure and report. ","5640b65e":"The correlation heatmap shows us at a glance that price is highly correlated to the number of bedrooms and bathrooms, while not very correlated with location and surface are, although this could be caused due to outliers. This is why even though it might be faster,  we observe the relationship between variables using scatter plots.","737b79b8":"## Predicting on test data","728b2b25":"We load the \"best\" model defined from our callbacks:","7146938f":"<div><center>\n<img src=\"https:\/\/starecat.com\/content\/wp-content\/uploads\/the-numbers-mason-what-do-they-mean.jpg\" width=\"350\"\/>\n<\/center><\/div> ","f768e9ca":"As mentioned earlier, this experiment also encourages us to attempt building our own systems by considering different approaches.","d28c6042":"The info() function presents us with a quick glance at the data, showcasing the features and their data types. The numerical data is either integer (int64) or floating point (float64), while the categorical and text data are simply stored as objects.","c97ded9b":"* The box plots for BER class showcases quite a few outliers, namely a house with an exorbitantly high price with a BER rating of \u201cB2\u201d\n* There are plenty of outliers for the \u201cdetached\u201d home too, but there might be other reasons for these houses to have such high prices, like location. We shouldn\u2019t be too quick to discard these data points. We might come back to take care of them if they heavily affect the model performance.\n","77f82a25":"# INTRODUCTION","ccc40aba":"We decided to go with the [TF-IDF Vectorizer] (https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfVectorizer.html), due to its ease of use and decent efficiency (considering accuracy vs time).","8675ae8d":"Our numerical data is all scaled and ready for the model now.","927bdc6e":"## Modelling","bb9a70a5":"We note that the \u201cBER_class\u201d and \u201cServices\u201d columns have several missing values, which we shall deal with when processing this data.","05dab7bf":"Let\u2019s apply this to the first description and compare the results:","12267fea":"## Multivariate linear reression","4521b13f":"Note that this huge table is only the encoded features, we still have to:\n* Drop the original columns from the dataframe\n* Join the encoded columns with the actual dataframe\n\nWe could technically directly encode the data in the main dataframe directly, but that brings the risk of having the encoder thinking our continuous values are categorical and encoding them too! Better be safe than sorry. This method also allows us to check the data at every step to make sure it is going smoothly.\n","d2683745":"We can also directly check the condition and drop the outliers by index as such:","5c4c974b":"In order to make training more efficient, we can define \u201ccallbacks\u201d, which are conditions for the network to change certain parameters or stop training altogether in order to save time.\n\nThe callbacks we implement are:  \n* ModelCheckpoint \u2013 Saves the best model (whichever has the highest metric-accuracy in our case)\n* EarlyStopping \u2013 Stops the model if performance does not improve for a certain number of epochs\n* ReduceLROnPlateau \u2013 Reduces the learning rate if performance stagnates.\n","d88ddb2e":"Since we primarily use Google Colab, this allows us to select our data and easily upload it.","25116bfc":"## XGBoost Regression","573203d0":"### Removing outliers","a4311633":"So far so good! Now all we have next is to vectorize the data and apply a model!","26aff98a":"After the ordeal we just went through with categorical data, processing full blown textual data seems like an even bigger hurdle. We can\u2019t possibly encode each and every word from each and every review, can we?\n\nWell actually, we can! The process follows the same principle, transforming words into numbers to we can feed them into our traditional machine learning algorithms. A major difference here is that we initially have to clean the data and extract the most significant words, before assigning values to them, a process known as \u201cvectorizing\u201d.\n\nLet\u2019s walk through cleaning first. \n","cff1c7d1":"Once we\u2019re done with our vectorization process, we can finally apply the usual machine learning models we\u2019re familiar with. Here, we go with a Multinomial Na\u00efve Bayes model.","2f3df1b3":"The describe() function gives us a little more insight about the statistical properties of the data. It conveniently excludes the text based data, giving us stats about numerical data only.","bfcef2f1":"Histograms are usually the primary go-to when working on data viz. They depict the frequency of data points, enabling us to quickly check for outliers.\n\nHistograms obviously aren\u2019t applicable to textual data, so we define a custom list of columns to plot them for:","37324b5c":"While scatter plots are also applicable to categorical variables, they're more useful in spotting outliers than anything. Box plots are more suited to this."}}