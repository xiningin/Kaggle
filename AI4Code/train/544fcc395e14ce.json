{"cell_type":{"195d629c":"code","3b41da36":"code","9cf5872a":"code","4a2eeaf6":"code","08368173":"code","2d614eb2":"code","83186905":"code","56d07d8b":"code","aa24af7d":"code","86b775e1":"code","e2757746":"code","c2765526":"code","e81dd526":"code","f0002ca0":"code","1659cbd5":"code","00a4b1d5":"code","7e5b4ae1":"code","de0528cf":"code","129b884c":"code","11945917":"code","a8574202":"code","7276bb38":"code","50522f71":"code","498bc17f":"code","d48d14e6":"code","fe4e061c":"code","57193b3e":"code","b4ff8a3a":"code","59b5e748":"code","fd7532fe":"code","83994ce0":"code","a829ef6c":"code","9cff1971":"code","d5d93990":"code","24198ded":"code","5b8ce2c7":"code","424c2755":"code","5a821fc3":"markdown","7d32d1ae":"markdown","3208f450":"markdown","94edd29e":"markdown","1fa12600":"markdown","84a23e5e":"markdown","b2cec245":"markdown","0c53b89f":"markdown","812561c8":"markdown","e2577d44":"markdown","e6691dd5":"markdown","fa4a540f":"markdown","dbaa44ce":"markdown","f2d132d4":"markdown","59ecf67a":"markdown","678ed67c":"markdown","eaa1cb0d":"markdown","f0b7f47a":"markdown","8586d98e":"markdown","8209b34a":"markdown","a874a134":"markdown","0ecc602d":"markdown","8882ee31":"markdown","55a83f9f":"markdown","e9353aa0":"markdown","9ea2da40":"markdown","a30dcf91":"markdown","ba5c59bd":"markdown","bbf1e49a":"markdown","dc95d08c":"markdown","51e525b0":"markdown","a035570e":"markdown","1ecdfb49":"markdown","1e496f1b":"markdown","97a327f3":"markdown","37f1ea65":"markdown","94254177":"markdown","cf231555":"markdown","d995b50b":"markdown","84cfda17":"markdown","eb2968d5":"markdown"},"source":{"195d629c":"import numpy as np\nimport pandas as pd\nimport re\nimport warnings\n\nimport seaborn as sn\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import FuncFormatter\n!pip install adjustText\nfrom adjustText import adjust_text\nfrom textwrap import wrap\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectKBest, f_classif, SelectFromModel\nfrom eli5.sklearn import PermutationImportance\nfrom catboost import Pool, cv, CatBoostClassifier\nfrom sklearn.metrics import  classification_report, log_loss, roc_auc_score\nimport shap  \n\npd.set_option('display.max_columns', None)\n\ngames = pd.read_csv('..\/input\/nba-games\/games.csv')\ndetails = pd.read_csv('..\/input\/nba-games\/games_details.csv')\nteams = pd.read_csv('..\/input\/nba-games\/teams.csv')\nplayers = pd.read_csv('..\/input\/nba-games\/players.csv')\nranking = pd.read_csv('..\/input\/nba-games\/ranking.csv')\n\ndetails['PLAYER_NAME_SHORT'] = details['PLAYER_NAME'].str.replace('^(.).*\\s(.*)', '\\\\1.\\\\2')\ndetails[[\"MINS\", \"SECS\"]] = details.MIN.str.extract(r\"([^:]+):(.*)\")\ndetails.loc[(~details.MIN.str.contains(':', na = True)), 'SECS'] = details.MIN\ndetails.MINS = pd.to_numeric(details.MINS)\ndetails.SECS = pd.to_numeric(details.SECS)\ndetails['PLAY_TIME'] = np.round(details.MINS.fillna(0) + details.SECS\/60)\n\ngames = games.loc[~games[['GAME_ID', 'GAME_DATE_EST']].duplicated()] #Leaving one entry per game\ngames['GAME_DATE_EST'] = pd.to_datetime(games.GAME_DATE_EST)\n","3b41da36":"def get_season(date):\n    '''\n    Returns the season based on the month and year of a date\n    '''\n    date = pd.to_datetime(date, format='%Y-%m-%d')\n    if (date.month >= 10):\n        season = date.year\n    else:\n        season = date.year - 1\n    return season\n\ndef get_season_data(season):\n    ''' \n    Aggregates the details data into yearly figures\n    Returns the aggregated data and the standardised aggregated data    \n    '''\n    \n    temp = details.loc[details.GAME_ID.isin(games.loc[games.SEASON == season, 'GAME_ID'])]\n    temp = temp.loc[~temp['PLAY_TIME'].isnull()]\n    agg_df = temp.groupby(['PLAYER_ID', 'PLAYER_NAME', 'PLAYER_NAME_SHORT'])[['FGA', 'FGM', 'FG3A', 'FG3M', 'FTA', 'FTM', 'OREB', 'DREB', 'AST', 'STL', 'BLK', 'TO', 'PF', 'PLAY_TIME']].sum().reset_index()\n    agg_df = agg_df.loc[agg_df['PLAY_TIME'] >= 1500].reset_index(drop = True)\n    st_agg_df = StandardScaler().fit_transform(agg_df.drop(columns = ['PLAYER_ID', 'PLAYER_NAME', 'PLAY_TIME', 'PLAYER_NAME_SHORT']))\n    return(agg_df, st_agg_df)\n\npca = PCA(n_components = 2)\nagg_df, st_agg_df = get_season_data(2020)\npcomp = pca.fit_transform(st_agg_df)\ndf = pd.DataFrame(pca.components_, \n             index = ['PC 1', 'PC 2'],\n             columns = agg_df.drop(columns = ['PLAYER_ID', 'PLAYER_NAME', 'PLAY_TIME', 'PLAYER_NAME_SHORT']).columns).round(2)\nstyles = [dict(selector=\"caption\", props=[(\"caption-side\", \"bottom\")])]\ndf.style.set_table_styles(styles).set_properties(**{'font-size':'8pt'}).background_gradient('Greens').set_precision(2).set_caption('Breakdown of the principal components for the 2020 season')\n","9cf5872a":"def pca_plots(season, ax = None):\n    '''\n    Performs PCA on season data\n    Calculates the top players based on the 2 PCs\n    Creates a scatter plot for visualising the season data\n    '''\n    \n    pca = PCA(n_components = 2)\n    agg_df, st_agg_df = get_season_data(season)\n    pcomp = pca.fit_transform(st_agg_df)\n    \n    overall = pcomp.sum(axis = 1)\n    top_overall = np.argpartition(overall, -4)[-4:]\n    top_pc1 = np.argpartition(pcomp[:,0], -4)[-4:]\n    top_pc2 = np.argpartition(pcomp[:,1], -4)[-4:]\n    top_pc = list(set(list(top_pc1) + list(top_pc2) + list(top_overall)))\n\n    alphas = np.zeros(pcomp.shape[0]) + 0.3\n    alphas[top_pc] = 1\n    \n    ax.scatter(pcomp[:, 0], pcomp[:, 1], alpha = alphas)\n    ax.axhline(y = 0, color ='green', linestyle='--', lw=1)\n    ax.axvline(x = 0, color ='green', linestyle='--', lw=1)\n    texts = [ax.text(x=pcomp[i, 0],y=pcomp[i, 1],s=agg_df.PLAYER_NAME_SHORT[i]) for i in top_pc]\n    adjust_text(texts, ax = ax, arrowprops=dict(arrowstyle=\"-\", color='k', lw=0.5))\n    _ = ax.set_title(str(season))\n    ","4a2eeaf6":"fig,axes = plt.subplots(4, 4, figsize=(15,15))\nfor i,season in enumerate(range(games.SEASON.max(), games.SEASON.min()+1, -1)):\n    pca_plots(season, ax = axes.ravel()[i])    \n    axes.ravel()[i].tick_params(bottom=False, left=False, labelbottom = False, labelleft = False)\n    if (i % 5 == 0 ):\n        axes.ravel()[i].set_xlabel('Offensive component')\n        axes.ravel()[i].set_ylabel('Defensive component')\n        axes.ravel()[i].text(x = 0.1, y = -3, s = 'Avg off.',rotation=90)\n        axes.ravel()[i].text(x = -3, y = 0.1, s = 'Avg def.')\nfig.suptitle('Outstanding players by principal components per season', fontsize = 20)\nfig.tight_layout()\nfig.subplots_adjust(top=0.92)\nplt.show()","08368173":"pca_all = PCA()\npca_all.fit(st_agg_df)\n\ncum_var = np.cumsum(pca_all.explained_variance_ratio_)\ncum_var = np.insert(cum_var, 0, 0)\ncum_var = cum_var[:-1]\n\ncomp = [str(x + 1) for x in range(pca_all.n_components_)]","2d614eb2":"fig, ax = plt.subplots()\n\nax.bar(comp, cum_var, align='center', width=0.7, label='_nolegend_')\nax.bar(comp, pca_all.explained_variance_ratio_, bottom=cum_var, align='center', width=0.7, label= '\\n'.join(wrap('Additional var. explained', 20)))\n\nax.set_ylabel('Variance')\nax.yaxis.set_major_formatter(FuncFormatter('{0:.0%}'.format))\nax.set_xlabel('PC')\nax.set_title('Dataset variance explained by principal components')\nax.legend()\n\nplt.show()","83186905":"fig,axes = plt.subplots(1, 4, figsize=(16,4))\nagg_df, st_agg_df = get_season_data(2020)\nfor i in range(2,6):\n    kmeans = KMeans(n_clusters = i, random_state = 0).fit(st_agg_df)\n    axes.ravel()[i-2].scatter(pcomp[:, 0], pcomp[:, 1], c = kmeans.labels_)\n    axes.ravel()[i-2].set_title(str(i) + ' clusters', size = 15)\nfig.suptitle('K-Means clustering for the 2020 season', fontsize = 20)\nfig.tight_layout()\nfig.subplots_adjust(top=0.75)\nplt.show()","56d07d8b":"ranking['STANDINGSDATE'] = pd.to_datetime(ranking['STANDINGSDATE'])\nranking.sort_values('STANDINGSDATE', inplace = True)\nranking = ranking.loc[ranking.SEASON_ID \/\/ 10000 == 2]\nranking['SEASON_ID'] = ranking['SEASON_ID'] % 10000\nranking['MAX_S_GAMES'] = ranking.groupby('SEASON_ID').G.transform(max)\n\n# Creating a df with season start and end for excluding preseason and playoff games\nstart_dates = ranking.loc[ranking.SEASON_ID > 2002].groupby('SEASON_ID')['STANDINGSDATE'].min().to_frame('FIRST_GAME').reset_index()\nt = ranking.loc[(ranking.G == ranking.MAX_S_GAMES)].groupby(['SEASON_ID', 'STANDINGSDATE'])['TEAM_ID'].nunique().to_frame('TEAMS').reset_index()\nplayoff_dates = t.loc[(t.TEAMS == 30) | \n                      ((t.TEAMS == 28) & (t.SEASON_ID == 2012)) | \n                      ((t.TEAMS == 29) & (t.SEASON_ID == 2003))].groupby('SEASON_ID')['STANDINGSDATE'].min().to_frame('LAST_GAME').reset_index()\ndates = start_dates.merge(playoff_dates, how = 'left')\ndates.loc[dates.SEASON_ID == 2019, 'LAST_GAME'] = pd.to_datetime('2020-03-12') #Manually adding the end of regular season\n\n# Filtering out details for non-regular season games\ndetails = details.merge(games[['GAME_ID', 'GAME_DATE_EST', 'SEASON']], how = 'left')\ndetails = details.merge(dates, left_on = 'SEASON', right_on = 'SEASON_ID', how = 'left')\ndetails = details.loc[(details.LAST_GAME.isnull()) | ((details.GAME_DATE_EST <= details.LAST_GAME) & (details.GAME_DATE_EST > details.FIRST_GAME))]\n\n# Filtering out games for non-regular season games\ngames = games.merge(dates, left_on = 'SEASON', right_on = 'SEASON_ID', how = 'inner')\ngames = games.loc[games.LAST_GAME.isnull() | ((games.GAME_DATE_EST <= games.LAST_GAME) & (games.GAME_DATE_EST > games.FIRST_GAME))]\ngames.drop(columns = ['SEASON_ID', 'FIRST_GAME', 'LAST_GAME'], inplace=True)\n\n# Excluding the 2020 season as it was plagued by Covid related results\ndetails = details.loc[details.SEASON != 2020]\ngames = games.loc[games.SEASON != 2020]\ndates = dates.loc[dates.SEASON_ID != 2020]\n\n# Creating a new ranking df with regular season only and values that can be used for modeling \nranking_short = ranking[['TEAM_ID', 'SEASON_ID', 'STANDINGSDATE', 'CONFERENCE', 'TEAM', 'G', 'W', 'L', 'HOME_RECORD', 'ROAD_RECORD']].merge(dates)\nranking_short = ranking_short.loc[(ranking_short.STANDINGSDATE >= ranking_short.FIRST_GAME) & (ranking_short.STANDINGSDATE <= ranking_short.LAST_GAME)]\nranking_short = ranking_short.loc[ranking_short.G > 0]\nranking_short[['HOME_W', 'HOME_L']] = ranking_short.HOME_RECORD.str.split('-', expand = True)\nranking_short[['AWAY_W', 'AWAY_L']] = ranking_short.ROAD_RECORD.str.split('-', expand = True)\nranking_short[['HOME_W', 'HOME_L', 'AWAY_W', 'AWAY_L']] = ranking_short[['HOME_W', 'HOME_L', 'AWAY_W', 'AWAY_L']].apply(pd.to_numeric)\nranking_short.drop(columns = ['SEASON_ID', 'FIRST_GAME', 'LAST_GAME', 'HOME_RECORD', 'ROAD_RECORD'], inplace = True)\nranking_short.sort_values('STANDINGSDATE', inplace = True)\n\n# Creating a new games df with regular season only\ngames_short = pd.merge(games[['GAME_ID', 'GAME_DATE_EST', 'SEASON', 'HOME_TEAM_ID', 'VISITOR_TEAM_ID', 'HOME_TEAM_WINS']], dates, left_on = 'SEASON', right_on = 'SEASON_ID')\ngames_short = games_short.loc[(games_short.GAME_DATE_EST > games_short.FIRST_GAME) & (games_short.GAME_DATE_EST <= games_short.LAST_GAME)]\ngames_short.drop(columns = ['SEASON_ID', 'FIRST_GAME', 'LAST_GAME'], inplace = True)\ngames_short.sort_values('GAME_DATE_EST', inplace = True)","aa24af7d":"def mov_mult(mov, elo_diff):\n    return (mov + 3)**0.8\/(7.5 + 0.006*(elo_diff))\n\ndef win_prob(elo_diff):\n    return 1\/(10**(-elo_diff\/400) + 1)\n\ndef update_elo(team_elo, game_data, k=20):\n    if game_data.MOV < 0:        \n        mult = mov_mult(-game_data.MOV, -game_data.ELO_DIFF)\n        elo_change =  k*(game_data.HOME_WIN_PR)*mult\n        team_elo.loc[team_elo.TEAM == game_data.HOME_TEAM_ID, 'ELO'] -= elo_change\n        team_elo.loc[team_elo.TEAM == game_data.VISITOR_TEAM_ID, 'ELO'] += elo_change\n    else:\n        mult = mov_mult(game_data.MOV, game_data.ELO_DIFF)\n        elo_change =  k*(1-game_data.HOME_WIN_PR)*mult\n        team_elo.loc[team_elo.TEAM == game_data.HOME_TEAM_ID, 'ELO'] += elo_change\n        team_elo.loc[team_elo.TEAM == game_data.VISITOR_TEAM_ID, 'ELO'] -= elo_change\n        \nelo_data = games[['GAME_DATE_EST', 'GAME_ID', 'HOME_TEAM_ID', 'VISITOR_TEAM_ID', 'SEASON', 'PTS_home', 'PTS_away']].copy()\nelo_data['MOV'] = elo_data['PTS_home'] - elo_data['PTS_away']\nelo_data.sort_values('GAME_DATE_EST', inplace=True)\nelo_data = elo_data.dropna()\nelo_data[['HOME_ELO', 'VISITOR_ELO', 'ELO_DIFF', 'HOME_WIN_PR', 'VISITOR_WIN_PR']] = 0\nelo_data.reset_index(inplace=True, drop=True)\n\n\nteams_dict = dict(zip(teams.TEAM_ID, teams.ABBREVIATION))\nteam_elo = pd.DataFrame({'TEAM':pd.unique(elo_data[['HOME_TEAM_ID', 'VISITOR_TEAM_ID']].values.ravel('K')), \n                         'ELO':1500})\nteam_elo['NAME'] = team_elo.TEAM.map(teams_dict)\nteam_elo_l = []\n\ncurrent_season = 2003\nhome_elo_col = elo_data.columns.get_loc(\"HOME_ELO\")\nvisitor_elo_col = elo_data.columns.get_loc(\"VISITOR_ELO\")\nhome_team_col = elo_data.columns.get_loc('HOME_TEAM_ID')\nvisitor_team_col = elo_data.columns.get_loc('VISITOR_TEAM_ID')\nelo_diff_col = elo_data.columns.get_loc('ELO_DIFF')\nhome_win_col = elo_data.columns.get_loc('HOME_WIN_PR')\nvisitor_win_col = elo_data.columns.get_loc('VISITOR_WIN_PR')\n\nfor i in range(len(elo_data)):\n    if elo_data.iloc[i, elo_data.columns.get_loc('SEASON')] != current_season:\n        team_elo_l.append(team_elo.sort_values(by = 'ELO', ascending = False).head(5).assign(SEASON = current_season))\n        team_elo['ELO'] = 0.75*team_elo.ELO + 0.25*1500 # Reverting back to the mean for the start of each season\n        current_season = elo_data.iloc[i, elo_data.columns.get_loc('SEASON')]\n        \n    elo_data.iloc[i, home_elo_col] = team_elo.loc[team_elo.TEAM == elo_data.iloc[i, home_team_col], 'ELO'].values + 100\n    elo_data.iloc[i, visitor_elo_col] = team_elo.loc[team_elo.TEAM == elo_data.iloc[i, visitor_team_col], 'ELO'].values\n    elo_data.iloc[i, elo_diff_col] = elo_data.iloc[i, home_elo_col] - elo_data.iloc[i, visitor_elo_col]\n    elo_data.iloc[i, home_win_col] = win_prob(elo_data.iloc[i, elo_diff_col])\n    elo_data.iloc[i, visitor_win_col] = 1-elo_data.iloc[i, home_win_col]\n    update_elo(team_elo, elo_data.iloc[i])\nteam_elo_l.append(team_elo.sort_values(by = 'ELO', ascending = False).head(5).assign(SEASON = current_season))\n\n## Preparing chart data\n\nteam_nick_dict = dict(zip(teams.TEAM_ID, teams.NICKNAME)) \nelo_plot_df = pd.concat(team_elo_l)\nelo_plot_df['NICKNAME'] = elo_plot_df.TEAM.map(team_nick_dict)\ntop_teams = elo_plot_df.groupby('NAME').size().nlargest(4, keep='all').index.tolist()\nall_teams = elo_plot_df.NAME.unique()\ncolors = len(all_teams)\ncolor_dict = {}\n\n#cm = plt.get_cmap('gist_rainbow')\ncm = plt.get_cmap('Accent')\ncolor_counter = 0\nfor i in range(colors):\n    if all_teams[i] in top_teams:\n        color_dict[all_teams[i]] = np.array(cm(1.*color_counter\/len(top_teams)))\n        color_counter += 1\n    else:\n        color_dict[all_teams[i]] = 'white'\n        ","86b775e1":"fig,axes = plt.subplots(4, 4, figsize=(15,15))\nfor i,season in enumerate(range(elo_plot_df.SEASON.max(), elo_plot_df.SEASON.min(), -1)):\n    data = elo_plot_df.loc[elo_plot_df.SEASON == season].sort_values(by='ELO')\n    axes.ravel()[i].tick_params(left=False, labelleft = False)\n    axes.ravel()[i].barh(y=data.NAME, width=data.ELO, color=data.NAME.map(color_dict))\n    axes.ravel()[i].set_title(str(season))\n    for j,name in enumerate(data.NICKNAME):\n        axes.ravel()[i].text(s=name, x=600, y=j, color=\"black\", verticalalignment=\"center\",size=16)\n        \nfig.suptitle('Best teams by ELO rating per season', fontsize = 20)\nfig.tight_layout()\nfig.subplots_adjust(top=0.92)\nplt.show()","e2757746":"details['FGMISSES'] = details.FGA - details.FGM\ndetails['FTMISSES'] = details.FTA - details.FTM\ndetails['EFF'] = details.PTS + details.REB + details.AST + details.STL + details.BLK - (details.TO + details.FGMISSES + details.FTMISSES)\n\n# Creating the efficiency dataset\neff = details[['PLAYER_NAME', 'PLAYER_ID', 'SEASON', 'EFF']].copy()\neff.dropna(inplace=True)\neff = eff.groupby(['PLAYER_NAME', 'PLAYER_ID', 'SEASON'], as_index=False).sum()\neff.sort_values('EFF', ascending=False, inplace=True)\ntop_players = eff.groupby('SEASON', as_index=False).head(30).copy()\ntop_players['NEXT_SEASON'] = top_players.SEASON + 1\neff['NEXT_SEASON'] = eff.SEASON + 1\n\n# Efficiency per game per team\nper_game_eff = details.loc[details.COMMENT.isnull()][['GAME_ID', 'TEAM_ID', 'PLAYER_ID', 'SEASON']].merge(eff, left_on=['PLAYER_ID', 'SEASON'], right_on=['PLAYER_ID', 'NEXT_SEASON'], how='left')\npge = per_game_eff.groupby(['GAME_ID', 'TEAM_ID'], as_index=False).EFF.mean()\npge.dropna(inplace= True)\n\n# Top players based on efficiecy for the 2019 season\ntop_players.loc[top_players.SEASON == 2019, ['PLAYER_NAME', 'EFF']]","c2765526":"top_players_per_game = details.loc[details.COMMENT.isnull()][['GAME_ID', 'TEAM_ID', 'PLAYER_ID', 'SEASON']].merge(top_players, left_on=['PLAYER_ID', 'SEASON'], right_on=['PLAYER_ID', 'NEXT_SEASON'], how='left')\n\ntop_players_per_game = top_players_per_game.groupby(['GAME_ID', 'TEAM_ID', 'SEASON_x']).PLAYER_NAME.agg(lambda x: x.notnull().sum()).reset_index()\ntop_players_per_game.columns = ['GAME_ID', 'TEAM_ID', 'SEASON', 'TOP_PLAYERS']\ntop_players_per_game.drop(columns='SEASON', inplace=True)","e81dd526":"games_played = pd.melt(games_short, id_vars = ['GAME_ID', 'GAME_DATE_EST'], value_vars = ['HOME_TEAM_ID', 'VISITOR_TEAM_ID']).set_index('GAME_DATE_EST')\ngames_played['HOME_GAME'] = np.where(games_played.variable == \"HOME_TEAM_ID\", True, False)\ngames_played.drop(columns = 'variable', inplace = True)\ngames_played.rename(columns = {'value':'TEAM_ID'}, inplace = True)\ngames_played.sort_values(['TEAM_ID', 'GAME_DATE_EST'], inplace = True)\n\n# Games in the last week\ngames_played['HG_7days'] = games_played.groupby('TEAM_ID').HOME_GAME.apply(lambda x: x.rolling(window = \"7d\", closed='left', min_periods=0).sum())\ngames_played['AG_7days'] = games_played.groupby('TEAM_ID').HOME_GAME.apply(lambda x: (~x).rolling(window = \"7d\", closed='left', min_periods=0).sum())\ngames_played['G_7days'] = games_played['HG_7days'] + games_played['AG_7days']\n\n# Back to back games\ngames_played.reset_index(inplace=True)\ngames_played['PAST_GAME'] = games_played.groupby(['TEAM_ID']).GAME_DATE_EST.transform(lambda x: x.shift(periods=1))\ngames_played['BACK2BACK'] = np.where((games_played.GAME_DATE_EST - games_played.PAST_GAME).dt.days == 1, 1, 0)\ngames_played.drop(columns = ['PAST_GAME', 'GAME_DATE_EST', 'HOME_GAME'], inplace=True)\n\n","f0002ca0":"missing_players = details.groupby(['GAME_ID', 'TEAM_ID'], as_index=False)['COMMENT'].agg(lambda x: ((x.notnull()) & (~x.str.contains('coach', case=False, na=False))).sum())\nmissing_players.rename(columns = {'COMMENT':'MISSING_PLAYERS'}, inplace = True)","1659cbd5":"# Combining all the data to create the full dataset\ninput_data = pd.merge_asof(games_short, ranking_short, left_on = 'GAME_DATE_EST', right_on = 'STANDINGSDATE', \n                     left_by = 'HOME_TEAM_ID', right_by = 'TEAM_ID', allow_exact_matches = False)\ninput_data = input_data.loc[~input_data.TEAM_ID.isnull()]\ninput_data = pd.merge_asof(input_data, ranking_short.add_suffix(\"_VISITOR\"), left_on = 'GAME_DATE_EST', right_on = 'STANDINGSDATE_VISITOR', \n                     left_by = 'VISITOR_TEAM_ID', right_by = 'TEAM_ID_VISITOR', allow_exact_matches = False)\n\ninput_data.loc[(input_data.GAME_DATE_EST.dt.month <= 12) & (input_data.GAME_DATE_EST.dt.month > 9) & (input_data.G > 50), ['G', 'W', 'L', 'HOME_W', 'HOME_L', 'AWAY_W', 'AWAY_L']] = 0\ninput_data.loc[(input_data.GAME_DATE_EST.dt.month <= 12) & (input_data.GAME_DATE_EST.dt.month > 9) & (input_data.G_VISITOR > 50), ['G_VISITOR', 'W_VISITOR', 'L_VISITOR', 'HOME_W_VISITOR', 'HOME_L_VISITOR', 'AWAY_W_VISITOR', 'AWAY_L_VISITOR']] = 0\n\n#Adding top players\ninput_data = input_data.merge(top_players_per_game, left_on=['HOME_TEAM_ID', 'GAME_ID'], right_on=['TEAM_ID', 'GAME_ID'], suffixes=('','_something'))\ninput_data = input_data.merge(top_players_per_game, left_on=['VISITOR_TEAM_ID', 'GAME_ID'], right_on=['TEAM_ID', 'GAME_ID'], suffixes=('','_VISITOR'))\ninput_data.drop(columns=['TEAM_ID_something', 'TEAM_ID_VISITOR'], inplace=True)\n\n#Adding player efficiency\ninput_data = input_data.merge(pge, left_on=['HOME_TEAM_ID', 'GAME_ID'], right_on=['TEAM_ID', 'GAME_ID'], suffixes=('','_something'))\ninput_data = input_data.merge(pge, left_on=['VISITOR_TEAM_ID', 'GAME_ID'], right_on=['TEAM_ID', 'GAME_ID'], suffixes=('','_VISITOR'))\ninput_data.drop(columns=['TEAM_ID_something', 'TEAM_ID_VISITOR'], inplace=True)\n\n#Adding team fatigue\ninput_data = input_data.merge(games_played, left_on=['HOME_TEAM_ID', 'GAME_ID'], right_on=['TEAM_ID', 'GAME_ID'], suffixes=('','_something'))\ninput_data = input_data.merge(games_played, left_on=['VISITOR_TEAM_ID', 'GAME_ID'], right_on=['TEAM_ID', 'GAME_ID'], suffixes=('','_VISITOR'))\ninput_data.drop(columns=['TEAM_ID_something', 'TEAM_ID_VISITOR'], inplace=True)\n\n#Adding missing players\ninput_data = input_data.merge(missing_players, left_on=['GAME_ID', 'HOME_TEAM_ID'], right_on=['GAME_ID', 'TEAM_ID'])\ninput_data = input_data.merge(missing_players, left_on=['GAME_ID', 'VISITOR_TEAM_ID'], right_on=['GAME_ID', 'TEAM_ID'], suffixes=(None, '_VISITOR'))\n\n# Adding ELO\ninput_data = input_data.merge(elo_data[['GAME_ID', 'HOME_ELO', 'VISITOR_ELO']])\n\ninput_data['HOME_TEAM'] = input_data['HOME_TEAM_ID'].map(teams_dict)\ninput_data['VISITOR_TEAM'] = input_data['VISITOR_TEAM_ID'].map(teams_dict)\n\ninput_data = input_data.drop(columns = ['HOME_TEAM_ID', 'VISITOR_TEAM_ID', 'TEAM_ID', 'TEAM', 'TEAM_ID_x', 'TEAM_ID_y', 'STANDINGSDATE', 'TEAM_VISITOR', 'STANDINGSDATE_VISITOR'])\ninput_data['ELO_DIFF'] = input_data.HOME_ELO - input_data.VISITOR_ELO\ninput_data['TOP_PLAYER_DIFF'] = input_data.TOP_PLAYERS - input_data.TOP_PLAYERS_VISITOR\ninput_data['MISSING_PLAYER_DIFF'] = input_data.MISSING_PLAYERS - input_data.MISSING_PLAYERS_VISITOR\ninput_data['EFF_DIFF'] = input_data.EFF - input_data.EFF_VISITOR\ninput_data['MONTH'] = input_data.GAME_DATE_EST.dt.month\n\ninput_data.drop(columns = ['HOME_TEAM', 'VISITOR_TEAM', 'CONFERENCE', 'CONFERENCE_VISITOR', 'GAME_DATE_EST'], inplace=True)","00a4b1d5":"train_data = input_data.loc[(input_data.SEASON < 2018) & (input_data.SEASON > 2005)]\nvalid_data = input_data.loc[input_data.SEASON == 2018]\ntest_data = input_data.loc[input_data.SEASON == 2019]\nfull_train_data = pd.concat([train_data, valid_data], axis = 0)\n\nX, y = train_data.drop(columns = ['HOME_TEAM_WINS']), train_data.HOME_TEAM_WINS\nvalid_X, valid_y = valid_data.drop(columns = ['HOME_TEAM_WINS']), valid_data.HOME_TEAM_WINS\ntest_X, test_y = test_data.drop(columns = ['HOME_TEAM_WINS']), test_data.HOME_TEAM_WINS\nfull_train_X, full_train_y = full_train_data.drop(columns = ['HOME_TEAM_WINS', 'SEASON', 'GAME_ID']), full_train_data.HOME_TEAM_WINS\n\ntrain_games = X[['SEASON', 'GAME_ID']]\nvalid_games = valid_X[['SEASON', 'GAME_ID']]\ntest_games = test_X[['SEASON', 'GAME_ID']]\n\nX.drop(columns = ['SEASON', 'GAME_ID'], inplace=True)\nvalid_X.drop(columns = ['SEASON', 'GAME_ID'], inplace = True)\ntest_X.drop(columns = ['SEASON', 'GAME_ID'], inplace = True)","7e5b4ae1":"def calc_accuracy(model, test_X, test_y):\n    return(np.round(accuracy_score(model.predict(test_X), test_y)*100,2))\n    \ndef add_accuracy(model, model_name, test_X, test_y):\n    test_acc = calc_accuracy(model, test_X, test_y)\n    results_l.append([model_name, test_acc, len(test_X.columns)])\n    print(model_name + ' test accuracy: ' + str(test_acc) + ' %')\n    \nresults_l = []","de0528cf":"games_short.sort_values(\"GAME_DATE_EST\", inplace=True)\nnaive = pd.merge_asof(games_short, games_short[['GAME_DATE_EST', 'HOME_TEAM_ID', 'VISITOR_TEAM_ID', 'SEASON', 'HOME_TEAM_WINS']], on = 'GAME_DATE_EST', by = ['HOME_TEAM_ID', 'VISITOR_TEAM_ID'], allow_exact_matches = False, suffixes=(['', '_p']))\nnaive = naive.dropna()\n\nprint('Same result prediction accuracy: ' + str(np.round(accuracy_score(naive.HOME_TEAM_WINS, naive.HOME_TEAM_WINS_p)*100,2)) + ' %')\nprint('Home team wins prediction accuracy: ' + str(np.round(accuracy_score(games_short.HOME_TEAM_WINS, np.repeat(1, games_short.shape[0]))*100,2)) + ' %')\n\nresults_l.append(['Same result', np.round(accuracy_score(naive.HOME_TEAM_WINS, naive.HOME_TEAM_WINS_p)*100,2), 1])\nresults_l.append(['Home team wins', np.round(accuracy_score(games_short.HOME_TEAM_WINS, np.repeat(1, games_short.shape[0]))*100,2), 0])\n","129b884c":"print('ELO accuracy: ' + str(np.round(accuracy_score(test_X.ELO_DIFF >= 0, test_y)*100,2)) + ' %')\nresults_l.append(['ELO', accuracy_score(test_X.ELO_DIFF >= 0, test_y)*100, 1])","11945917":"catb = CatBoostClassifier(verbose=False)\ncatb.fit(full_train_X, full_train_y)\nadd_accuracy(catb, 'Full model', test_X, test_y)","a8574202":"games['HOME_TEAM'] = games.HOME_TEAM_ID.map(teams_dict)\ngames['VISITOR_TEAM'] = games.VISITOR_TEAM_ID.map(teams_dict)\n\ngames.loc[games.GAME_ID == valid_games.GAME_ID.iloc[0], ['GAME_DATE_EST', 'HOME_TEAM', 'VISITOR_TEAM', 'PTS_home', 'PTS_away']]","7276bb38":"explainer = shap.TreeExplainer(catb)\nshap_values = explainer.shap_values(valid_X)\nshap.initjs()\nshap.force_plot(explainer.expected_value, shap_values[0,], valid_X.iloc[0,:])","50522f71":"games.loc[games.GAME_ID == valid_games.GAME_ID.iloc[1], ['GAME_DATE_EST', 'HOME_TEAM', 'VISITOR_TEAM', 'PTS_home', 'PTS_away']]","498bc17f":"shap.force_plot(explainer.expected_value, shap_values[1,], valid_X.iloc[1,:])","d48d14e6":"pipe = Pipeline([\n    ('reduce_dim', SelectKBest(f_classif)),\n    ('classify', CatBoostClassifier(verbose=False))\n])\n\nparam_grid = [\n      { 'reduce_dim__k': [8, 10, 12],\n        'classify__depth':[3, 5, 7]}]\n\nsf_grid = GridSearchCV(pipe, n_jobs=-1, param_grid=param_grid)\n_ = sf_grid.fit(X, y)","fe4e061c":"features_selected = sf_grid.best_params_['reduce_dim__k']\nprint(\"The GridSearch selected \" + str(features_selected) + ' features with a tree depth of ' + str(sf_grid.best_params_['classify__depth']))\nfeature_score_df = pd.DataFrame({'Feature':X.columns, 'Score':sf_grid.best_estimator_['reduce_dim'].scores_}).sort_values(by='Score', ascending=False).head(features_selected)\nfeature_score_df","57193b3e":"full_train_X_sf = full_train_X[feature_score_df.Feature]\nsf_model = CatBoostClassifier(verbose=False)\nsf_model.fit(full_train_X_sf, full_train_y)\n\ntest_X_sf = test_X[feature_score_df.Feature]\nadd_accuracy(sf_model, 'Simple filtering', test_X_sf, test_y)","b4ff8a3a":"with warnings.catch_warnings():\n    warnings.filterwarnings(\"ignore\")\n    best_valid_acc = 0\n    for features in [8, 10, 12]:\n        for depth in [3,5,7]:\n            pi_catb = CatBoostClassifier(verbose=False, depth=depth)\n            perm = PermutationImportance(estimator=pi_catb, cv=3).fit(X.reset_index(drop=True), y.reset_index(drop=True))\n            selected_columns = X.columns[np.argpartition(perm.feature_importances_, -features)[-features:]].tolist()\n\n            valid_X_pi = valid_X[selected_columns]\n            pi_catb.fit(X[selected_columns], y)\n\n            valid_acc = np.round(accuracy_score(pi_catb.predict(valid_X_pi), valid_y),3)\n            if valid_acc > best_valid_acc:\n                pi_features = selected_columns\n                pi_depth = depth\n                best_valid_acc = valid_acc    \n\nfull_train_X_pi = full_train_X[pi_features]\ntest_X_pi = test_X[pi_features]\npi_model = CatBoostClassifier(verbose=False, depth=pi_depth)\npi_model.fit(full_train_X_pi, full_train_y)","59b5e748":"pd.DataFrame(pi_features[::-1], columns = ['Features'])","fd7532fe":"add_accuracy(pi_model, 'Permutation importance', test_X_pi, test_y)","83994ce0":"train_pool = Pool(X, y)\nbest_valid_acc = 0\n\nfor features in [8, 10, 12]:\n    for depth in [3, 5, 7]:\n        fi_catb = CatBoostClassifier(verbose=False, depth = depth)\n        summary = fi_catb.select_features(X=train_pool, \n                                features_for_select=X.columns.to_list(),\n                                num_features_to_select = features,\n                                steps = 3,\n                                logging_level='Silent',\n                                train_final_model=True,\n                                algorithm = 'RecursiveByLossFunctionChange',\n                                plot=False)\n        valid_acc = np.round(accuracy_score(fi_catb.predict(valid_X), valid_y),3)\n        if valid_acc > best_valid_acc:\n            fi_model = fi_catb\n            fi_depth = depth\n            fi_features = summary['selected_features_names']\n            best_valid_acc = valid_acc\n\nfull_train_X_fi = full_train_X[fi_features]\ntest_X_fi = test_X[fi_features]\nfi_model = CatBoostClassifier(verbose=False, depth=fi_depth)\nfi_model.fit(full_train_X_fi, full_train_y)","a829ef6c":"pd.DataFrame(fi_features[::-1], columns = ['Features'])","9cff1971":"add_accuracy(fi_model, 'Feature importance', test_X_fi, test_y)","d5d93990":"train_pool = Pool(X, y)\n\nvalid_acc = 0\n\nfor features in [8, 10, 12]:\n    for depth in [3, 5, 7]:\n        si_catb = CatBoostClassifier(verbose=False, depth = depth)\n        summary = si_catb.select_features(X=train_pool, \n                                features_for_select=X.columns.to_list(),\n                                num_features_to_select = features,\n                                steps = 3,\n                                logging_level='Silent',\n                                algorithm = 'RecursiveByShapValues',\n                                train_final_model=True,\n                                plot=False)\n        valid_acc = np.round(accuracy_score(si_catb.predict(valid_X), valid_y),3)\n        if valid_acc > best_valid_acc:\n            si_depth = depth\n            si_features = summary['selected_features_names']\n            best_valid_acc = valid_acc\n ","24198ded":"pd.DataFrame(si_features[::-1], columns=['Features'])","5b8ce2c7":"full_train_X_si = full_train_X[si_features]\ntest_X_si = test_X[si_features]\nsi_model = CatBoostClassifier(verbose=False, depth=si_depth)\nsi_model.fit(full_train_X_si, full_train_y)\n\nadd_accuracy(si_model, 'Shap importance', test_X_si, test_y)","424c2755":"pd.DataFrame(results_l, columns = ['Model name', 'Test accuracy', 'Features']).sort_values(by='Test accuracy', ascending=False)","5a821fc3":"## Feature importance\n\nBy feature importance in the context of a boosting tree, what is meant is the following:\nHow effective (in terms of reducing the loss or node impurity) are the splits based on each particular attribute?\nThere are multiple factors to take into account here:\n* The effectiveness is weighted by the number of observations at each split so splits that occur higher up the tree are more impactful (the lower we get on the decision tree the less observations there are to be split)\n* The number of trees that is using an attribute for a split is important here. More trees using the attribute for a split --> Higher score for the attribute\n\n| \u2795      | \u2796 |\n| ----------- | ----------- |\n| Can capture more complex relationships      |   Collinearity in the data might confound actual feature importance     |\n| Effective and efficient   | | \n\n\nLuckily, CatBoost has a built-in method for doing feature selection that allows using the feature importance as a selection criterion. The method includes Recursive Feature Elimination which starts with a model using the full set of features, calculates the feature importance and eliminates the least important of the features","7d32d1ae":"## Naive prediction\n\nBefore starting to do feature selection, modelling or any other calculation, we need to establish what is the baseline accuracy for our predictions. Two relatively simple ways of predicting the outcome came to mind:  \n*1) The outcome of the game will be the same as the outcome of the previous encounter between the same teams*  \nFor example, if the Miami Heat are playing the Lakers the naive prediction would be to predict that the result of the game will be the same as the result of the previous encounter between those two teams.  \n*2) The home team will win. Generally, the home team does have a statistical advantage in the NBA (as in most sports)*\n\nGiven that both these options are reasonable, let's try both of them out and pick the one that scores the highest as the prediction benchmark.","3208f450":"The ELO rating looks in line with the results of the respective seasons. The most consistent teams in terms of seasons in the top 5 have their own colour. It becomes evident that the Spurs have dominated the last 15 years and are consistently one of the best teams in the league. (Hopefully they will rise again!\ud83e\udd1e)  \nI found also very interesting that out of the most consistent teams only the Lakers and the Spurs have actually won a championship in those years. Jazz, Rockets, Thunder and Clippers have yet to win the \ud83c\udfc6\n\nNote that the ELO rating was calculated only based on regular season results and not playoff success. This becomes more important in later seasons as teams do load management and have understood that having their players healthy and rested in the playoffs is more important than a couple of ranking spots in the regular season.\n","94edd29e":"## Full Catboost model\n\nOne of the selling points of `CatBoost` is that the model can achieve high accuracy without much parameter tuning. Let's see how that works for the NBA dataset.\n\n| \u2795      | \u2796 |\n| ----------- | ----------- |\n| Easy to train      |   Features that negatively impact the accuracy of the model are not removed     |\n| Uses all the features from the dataset   | Low model interpretability|","1fa12600":"## ELO only prediction\n\nThe ELO rating of the two teams is a very good predictor of match results (as we shall see also later on in the notebook). How accurate would the predictions be if we only relied on the ELO of the two teams?","84a23e5e":"## Permutation importance\n\nThe idea behind permutation importance for a single feature is very simple:  \n1) Calculate the overall error of the model  \n2) Shuffle the values of the feature randomly in order to break any association between the feature and the outcome  \n3) Calculate by how much does the error of the model change with the new shuffling\n\nHigher values would mean that the feature was important for the predictions of the model.\n\n| \u2795      | \u2796 |\n| ----------- | ----------- |\n| Easy to implement      |   Issues with capturing more complex relationships in the data     |\n| Easy to understand and explain   | Can lead to impossible combinations | \n\nThe first issue of the method is very similar to the simple filtering case. The second requires some further explanation:\nIn the NBA data scenario, we have three ELO related variables, ELO home, ELO visitor, ELO Diff (difference between home and visitor ELO). If we do a permutation of the ELO Diff variable, the permutation will lead to some impossible combinations where we have for example ELO Home 1500, ELO visitor 1500 and ELO Diff 200 just because we selected the ELO Diff values randomly.\n","b2cec245":"# Introduction\n\nThe NBA-games dataset is a really great dataset with lots of information about players, teams and results that can be put together in order to create a predictive model for future games. I found this project very interesting as I really like basketball \ud83c\udfc0\ud83c\udfc0\ud83c\udfc0 and there was room to play around and experiment with the data. The main focus of the notebook is to create a dataset for predictive modelling and at a second step to trim it down to few features in order to enhance interpretability. Let's get started! \ud83c\udf88\ud83c\udf88\ud83c\udf88","0c53b89f":"# EDA\nWe have 5 datasets at our disposal:\n* Games -> Information about each game and the stats of the teams playing\n* Games details -> More detailed information about the individual player stats\n* Players -> Information about the name of the player and his team\n* Ranking -> Information about the standings of each team on individual days throughout the season\n* Teams --> Information about the team including ownership, arena, when it was established etc.\n\nFor the EDA we are mainly going to focus on the details dataset in order to have a more granular viewpoint.  Specifically, the focus will be on the yearly stats of the players and how we can use PCA and KMEANS to visualise the differences between individual player performances.","812561c8":"## Calculating ELO for each team\n\nThe [ELO statistic](https:\/\/en.wikipedia.org\/wiki\/Elo_rating_system) was originally divised by Arpad Elo for ranking players in chess games. In order to adjust the calculation for NBA games, the methodology was adjusted by [538](https:\/\/fivethirtyeight.com\/).  \nThe main points of the calculation:\n* Winning a game increases the ELO of the team and losing a game decreases it\n* The amount of increase\/decrease depends on a) the margin of victory b) the pre-game expectations for the outcome of the game (which are based on the ELO rating of the two teams)\n* Ratings are adjusted after each game\n\nOne of the easiest way to assess the ELO figures is to have a look at the end-season results and see which teams finished at the top.","e2577d44":"The SHAP values indicate that the model was considering Detroit slightly bigger favourite than the base value (the fact that the base value is higher than 0 indicates that the model has understood that the home team has the advantage generally).  \nThe main feature in favour of Detroit winning was the ELO difference between the two teams (biggest red components). On the other hand, the fact that two players were not available for Detroit pushed in the opposite direction (biggest blue component).","e6691dd5":"# Feature selection\n\nIn any machine learning model feature selection is very important in order to use only the most useful features and remove useless ones. This can help the model generalise better for new input data. This becomes clear also from looking at the SHAP charts above where very few variables are the main drivers of the decisions while the rest are contributing very little.\n\nThere are multiple ways of doing feature selection:\n* Using a statistical test to determine features that are related to the output variable \n* Using permutation importance to select the most relevant features\n* Using the feature importance of the model to remove less important features\n* Using the SHAP values importance\n\nIn the following parts of the notebook, we are going to try all those options. Please note that for more accurate results, a wider choice of grid search could be selected but in the interest of time (and lack of patience \ud83d\ude05) the parameter selection was limited.\n","fa4a540f":"## Splitting the data\n\nThe datasets are split as follows:  \n1) A train set in order to train the classifier (all seasons after 2005 and before 2018)    \n2) A validation set in order to tune the hyperparameters and evaluate feature selection  (2018 season)  \n3) A test set in order to assess how well the classifier works out of sample (2019 season)\n\nI did not include the 2020 season as it would require some careful recalibration of multiple parameters such as:\n* Player availability due to Covid\n* Empty stadiums\n* Season starting right after the playoffs finished","dbaa44ce":"Now comes the important part: **How good is the model?**  \nUnfortunately, there is no clear answer to this as we do not know exactly what kind of accuracy can be achieved. I could not find any models that try to predict regular season games (most focus on the playoffs which are significantly more predictable). But we can do some assessment!\nDoes the model improve on the floor accuracy?  \n1) The model is better than the baseline \u2705  \n2) The model is better than simple ELO based predictions \u2705\n\n**How high is the accuracy ceiling?**  \nThe easiest answer to this question is that an all omnicient being can predict with 100% accuracy and therefore that is the ceiling. A more realistic answer would be to try to see *how much inherent unpredictability is there in the NBA*.  \nThe most reliable source of data I could find for this is this perfectly named paper [Luck is hard to beat](https:\/\/arxiv.org\/pdf\/1706.02447.pdf). The researchers performed an analysis accross all sports to try and understand to what extent does luck play a role in the results.  \nFor basketball the skill coefficient is pretty high as it is a high scoring game which gives plenty of opportunity for the highly skilled teams to shine. They did however perform an analysis accross the NBA regular season to see **how often an underdog wins the game**. The figure they came up with (average accross seasons 2012-2016) is **around 36% of the time**. This gives us a figure of roughly speaking the unpredictability of the regular season.  \nPlease note that despite the analysis in this paper, there are always improvements to be made in any prediction algorithm!","f2d132d4":"Efficiency appears to be the most important feature with ELO and missing players being in the top 5. There are quite some differences in the last spots compared to the simple filtering algorithm but we have to see how these translate in the model's ability to generalise:","59ecf67a":"# Resources\n\nPER: https:\/\/en.wikipedia.org\/wiki\/Player_efficiency_rating  \nNBA ELO: https:\/\/fivethirtyeight.com\/features\/how-we-calculate-nba-elo-ratings\/#:~:text=One%20hundred%20Elo%20points%20is,and%20then%20divide%20by%2028  \nChess ELO https:\/\/en.wikipedia.org\/wiki\/Elo_rating_system  \nLuck is hard to beat: https:\/\/arxiv.org\/pdf\/1706.02447.pdf  \nModel interpretability: https:\/\/christophm.github.io\/interpretable-ml-book\nCatBoost: https:\/\/catboost.ai\/","678ed67c":"The first game in the validation dataset is the game between the [Brooklyn Nets and the Detroit Pistons](https:\/\/www.youtube.com\/watch?v=ch5G4fGvXaE).","eaa1cb0d":"## PCA\nThe details dataset contains many variables that can be used for analysis. Naturally, some variable combinations might be more interesting than others or offer more unique insights.  \nFor a more hollistic view of the data though, we can perform Principal Components Analysis.  \nThe aim of the analysis is to reduce the individual player stats into 2 components and try to see if these components can be interpreted meaningfully.","f0b7f47a":"The features selected based on RFE and feature importance are again mostly focused on the efficiency and the ELO rating of the two teams. ","8586d98e":"The features selected by the permutation importance calculation are the following:","8209b34a":"## Lessons learnt\n\nAnalysing and working with this dataset took quite a while and resulted in many (but really many!\ud83d\ude02) unsuccessful attempts. These have been educational as I learnt some valuable lessons that might be useful for somebody who is relatively new to predictive modelling. I think these points are mostly relevant for someone trying to establish a good model and not aiming to get that extra 0.01 increase in accuracy that will push their submission to the top of the leaderboard.\n\n**1) Before anything establish a reasonable baseline to measure the model against**    \n**2) Understanding which parts of the data are useful for prediction is crucial, start out by training simple models on the data that you are interested in and then focus on the parts that show most promise**  \n**3) Feature engineering is crucial and good features will get you much much further than hyperparameter tuning. This is really important as hyperparameter tuning is relatively straightforward (simply try out values) but feature engineering requires putting much more thought into the issue and understanding the data better**","a874a134":"## SHAP importance\n\nThe last algorithm for feature selection is selection by SHAP values. The idea behind the algoritm is fairly similar to the feature importance algorithm above with the only difference being that instead of selecting the features that result in the biggest decrease in losses, we select the features that have on average the highest SHAP values.\n\n","0ecc602d":"### How many principal components are enough?\nUsing 2 principal components allows for an easy and interpretable visualisation but is not necessarily the optimal number of components.  \nWe can assess how 'complete' is the breakdown into principal components by visualising what percentage of the total dataset variance is captured.","8882ee31":"## Incorporating player efficiency\n\nBasketball is a sport where key players can have substantial impact on the team's success. Therefore, it is crucial to incorporate this information in the modelling dataset.  \nThis sounds straightforward in theory but in practice there is an important issue to be adressed: **There is no all-encompassing metric for efficiency in basketball**. \nThe most popular one is the [Player efficiency rating (PER)](https:\/\/en.wikipedia.org\/wiki\/Player_efficiency_rating) developed by John Hollinger.  \nGiven that not all figures that are required in the calculation are available in the dataset, a different efficiency measure was used: (taken from [Breakthroughbasketball](https:\/\/www.breakthroughbasketball.com\/stats\/definitions.html))  \n**Efficiency = Pts + Rebs + Ast + Stl + Blk \u2013 (TO + FG Misses + FT Misses)**  \nOne benefit that I found in this statistic is that it does filter for games played as the number of all of these figures increases with more games. This means that there is no need for an arbitrary cut-off for season games. \nUsing this efficiency measure the following features were added to the dataset:  \n1) Players in a team that had a top 30 efficiency rating last season.  \n2) A measure of \"team\" efficiency by summing the last season efficiency of all players per team per game\n\nThe player efficiency figures for the 2019 season can be seen below:\n","55a83f9f":"### Which are the features that were selected from the GridSearch model?  \nWe can find it by checking the best parameters of the grid.","e9353aa0":"It can be beneficial to investigate which are the top players according to the efficiency statistic for the 2019 season to get a feel for how well it filters the top players of the league.\nFor the 2019 season the results look quite reasonable, as most of the players selected in the top 30 are some of the best in the league. There are cases that looked strange to me, for example Hassan Whiteside and Andre Drummond but they are both included also in the [top PER ratings](http:\/\/insider.espn.com\/nba\/hollinger\/statistics\/_\/year\/2020) by Hollinger himself.","9ea2da40":"## Using SHAP values to understand predictions\n\nAfter obtaining a basic model, we can use it to understand how predictions are made by using the SHAP values. It is important to note that SHAP values offer prediction explanations for individual examples. An appropriate interpretation of the values is the following:  \n > Given the current set of feature values, the contribution of a feature value to the difference between the actual prediction and the mean prediction is the estimated Shapley value.  [Interpretable ML book](https:\/\/christophm.github.io\/interpretable-ml-book\/shap.html)","a30dcf91":"Looking more closely at the values, it becomes clear that the first principal component is more oriented towards offence (as FG, FT, TO and AST are the most important contributors) while the second principal component is more defence oriented (OREB, DREB, BLK and PF contribute the most).\n\nLet's see which players score the highest in each of the components.","ba5c59bd":"Looking at the chart, it becomes clear that the first two principal components explain roughly 70% of the total dataset variance.  \nTherefore, our dimensionality reduction is quite meaningful and indeed distills patterns but there is still some nuance in the data that is not fully captured.","bbf1e49a":"Interpreting the clustering is very subjective but to my mind they split the dataset into the following groups:   \n2 clusters --> Bad offence - Good offence  \n3 clusters --> Good offence - Good defence - Rest  \n4 clusters --> Great offence - Good defence - Mediocre offence - Rest  \n5 clusters --> ????\ud83d\ude05  \n\nClearly the groups for the 5 clusters are tougher to interpret...","dc95d08c":"## K-Means\n\nAn additional interesting exploratory data visualisation is to try to cluster our dataset. One of the easiest to use clustering algorithms is the KMeans algorithm, which aims to find the most homogenious groups by minimising intra-group variance.  \nOne of the drawbacks of K-Means is that it the optimal number of clusters is tough to determine. Nevertheless, we are going to visualise the results of K-Means for various cluster sizes and see how they relate to the PC analysis from above.","51e525b0":"The second game in the validation dataset is the game between the [Indiana Pacers and the Memphis Grizzlies](https:\/\/www.youtube.com\/watch?v=cAP5Lqppqwg). None of the two teams had dramatic changes in their lineups in the offseason that would flip the balance of power. In the actual game Indiana crushed the Grizzlies to almost 30 points difference.","a035570e":"# Preparing the predictive modelling dataset\n\nAs always, the crucial task when given NBA stats is to manage to predict the outcome of future games based on the already available stats. But while we could use the datasets as they are, it is much better to try to create new meaningful features to help with prediction accuracy.\n\nThe modelling dataset consists of the following components:\n* Team ELO rating \n* Team record information (wins, losses, percentages)\n* Missing players (due to short term injuries)\n* Team player efficiency\n* Schedule information (whether the team gave back to back games, how many games in the last 5 days)","1ecdfb49":"## Incorporating team fatigue\n\nPlayers are humans and do get tired throughout the season. In order to measure this in the dataset the following variables are added:\n* HG_7days: Home games in the last 7 days\n* AG_7days: Away games in the last 7 days\n* G_7days: Total games in the last 7 days\n* BACK2BACK: Whether a team played a game the night before","1e496f1b":"## Missing players\n\nIn the details dataset there is a field called `comment` which indicates the reason a player is absent for the game. Unfortunately, this figure is not filled in for long-term injuries (for example for Kevin Durant in the 2019 season) but it can be used to calculate how many day-to-day injuries a team has in a given game.","97a327f3":"Despite the change in the way that the values are selected, the selected features do not differ substantially between the Feature importance and the SHAP importance selection.","37f1ea65":"# Evaluating the results\n\nWe have tried various predictive strategies and it can be good to put together all the scores for each of the methods.\n\n","94254177":"# Game predictions\n\nGiven the data at our disposal let's try to see how well we can predict the outcome of the games in the 2019 season. In order to take full advantage of the dataset, we are going to tune the parameters based on the validation set and after selecting the optimal ones we are going to retrain the model on the validation and train set before making the final predictions.  \nFor generating the predictions we are going to use [CatBoost](https:\/\/catboost.ai\/) a very popular gradient boosting library.","cf231555":"The full model with all the features available achieves the best test accuracy. Nevertheless the feature selection methods (especially the Permutation and Shap importance ones) achieve comparable accuracy with significantly smaller number of features. ","d995b50b":"Indeed the interpretation of the principal components seems to be correct.  \nFor example in the 2019 season the offensive machine that is James Harden, scores the highest for the first component (but not very well on the defensive side\ud83d\ude05) while the defensive component is dominated by players such as Gobert, Whiteside and Drummond.  \nPlayers such as Giannis, Davis and Adebayo offer the best of both worlds!","84cfda17":"## Simple filtering\n\nThe simplest approach to take, calculating how well each feature is separating the output (win or loss) and selecting the top features. The approach is using an ANOVA test to determine a measure of separation of the dependent variable for each feature. This is a really [nice post](https:\/\/datascience.stackexchange.com\/questions\/74465\/how-to-understand-anova-f-for-feature-selection-in-python-sklearn-selectkbest-w) explaining it in more detail.\n\n| \u2795      | \u2796 |\n| ----------- | ----------- |\n| Relatively quick to calculate      |   Missing interactions and more complicated relationships     |\n| Easy to understand and explain selection  | |\n\nIn order to better explain the drawback of the method, imagine a scenario where 1 variable is very predictive for a small number of observations in the dataset. For example in the NBA scenario, let's say that all other things being equal, the team that has a back to back game is less likely to win. This separation will come into play for that particular subset of games (small) where the teams are evenly matched. But the `Back to back` feature is not very predictive for the dataset as a whole and therefore it will score low overall.","eb2968d5":"The drivers of the model prediction are the difference between the ELO rating of the two teams and their efficiency. The model is very confident in the prediction that home team would win the game, which indeed it did.  \n\n\nEven from these two examples we can see that there are lots of features that are not contributing that much to the prediction, while there are some that contribute quite significantly. In the next parts of the notebook we are going to try to see if we can achieve similar accuracy by reducing the features of the model."}}