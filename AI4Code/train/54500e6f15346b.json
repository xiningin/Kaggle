{"cell_type":{"1d68498c":"code","e0a45251":"code","56f71282":"code","8872f00b":"code","8bc40902":"code","a75b2517":"code","88bc7e7a":"code","abd0e9ac":"code","406eef39":"code","c3ee936b":"code","119dc768":"code","be1edccf":"code","20304a37":"code","23f607a4":"code","31c6b5a4":"code","5b5483a2":"code","0ada0d74":"code","cefea103":"code","d966106d":"code","07eb8dd5":"code","83bf74dd":"code","bf62fa83":"code","310f47de":"code","1b5a7481":"code","602ae19e":"code","9dfb198d":"code","38fc9536":"code","878ca5b5":"code","6fa5d324":"code","0217b399":"code","548629b2":"code","bff5de12":"markdown","6b1b9e03":"markdown","46d0c988":"markdown","dc55bd0c":"markdown","c76f537f":"markdown","5d133895":"markdown","10a38084":"markdown","83ad52d7":"markdown","3100d793":"markdown","c5486bef":"markdown","b4f3e265":"markdown","5bed9592":"markdown","f7fdc090":"markdown","5db84bd1":"markdown","1588a2d1":"markdown","121387e0":"markdown"},"source":{"1d68498c":"# from IPython.display import HTML\n\n# HTML('''<script>\n# code_show=true; \n# function code_toggle() {\n#  if (code_show){\n#  $('div.input').hide();\n#  } else {\n#  $('div.input').show();\n#  }\n#  code_show = !code_show\n# } \n# $( document ).ready(code_toggle);\n# <\/script>\n# The raw code for this IPython notebook is by default hidden for easier reading.\n# To toggle on\/off the raw code, click <a href=\"javascript:code_toggle()\">here<\/a>.''')","e0a45251":"# HYPER PARAMS\nmax_boosting_rounds = 5500\n\nimport time\nnotebookstart= time.time()\n\nimport datetime\nimport pandas as pd\nimport numpy as np\nimport random\nimport time\n\n# Viz\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport os\nfrom wordcloud import WordCloud\nimport missingno as mn\nfrom yellowbrick.text import TSNEVisualizer\n\n# Hide Warnings\nWarning = True\nif Warning is False:\n    import warnings\n    warnings.filterwarnings(action='ignore')\n    warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n    warnings.filterwarnings(action='ignore', category=FutureWarning)\n\n# Modeling..\nimport eli5\nimport lightgbm as lgb\nimport shap\nshap.initjs()\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_validate\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.pipeline import make_pipeline\nimport scikitplot as skplt\nfrom sklearn import preprocessing\n\n# Tf-Idf\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import FeatureUnion\nfrom scipy.sparse import hstack, csr_matrix\n\nnp.random.seed(2018)\n\nfrom contextlib import contextmanager\nimport re\nimport string\nimport gc\n\n@contextmanager\ndef timer(name):\n    \"\"\"\n    Taken from Konstantin Lopuhin https:\/\/www.kaggle.com\/lopuhin\n    in script named : Mercari Golf: 0.3875 CV in 75 LOC, 1900 s\n    https:\/\/www.kaggle.com\/lopuhin\/mercari-golf-0-3875-cv-in-75-loc-1900-s\n    \"\"\"\n    t0 = time.time()\n    yield\n    print(f'[{name}] done in {time.time() - t0:.0f} s')\n    \n# Data Visualization\ndef cloud(text, title, size = (10,7)):\n    # Processing Text\n    wordcloud = WordCloud(width=800, height=400,\n                          collocations=False\n                         ).generate(\" \".join(text))\n    \n    # Output Visualization\n    fig = plt.figure(figsize=size, dpi=80, facecolor='k',edgecolor='k')\n    plt.imshow(wordcloud,interpolation='bilinear')\n    plt.axis('off')\n    plt.title(title, fontsize=25,color='w')\n    plt.tight_layout(pad=0)\n    plt.show()","56f71282":"train = pd.read_csv(\"..\/input\/train.csv\", index_col= 'qid')#.sample(50000)\ntest = pd.read_csv(\"..\/input\/test.csv\", index_col= 'qid')#.sample(5000)\ntestdex = test.index\n\ntarget_names = [\"Sincere\",\"Insincere\"]\ny = train['target'].copy()","8872f00b":"print(train.shape)\ntrain.head()","8bc40902":"print(\"Class Balance..\")\ntrain.target.value_counts(normalize=True)","a75b2517":"for i,name in [(0,\"Sincere\"),(1,\"Insincere\")]:\n     cloud(train.loc[train.target == i,\"question_text\"].str.title(), title=\"{} WordCloud\".format(name), size=[8,5])","88bc7e7a":"test['target'] = np.nan\nall_text = pd.concat([train['question_text'],test['question_text']], axis =0)\n\nword_vect = TfidfVectorizer(\n            sublinear_tf=True,\n            strip_accents='unicode',\n            analyzer='word',\n            token_pattern=r'\\w{1,}',\n            stop_words='english',\n            ngram_range=(1, 2),\n            max_features=20000)\n\nwith timer(\"Word Grams TFIDF\"):\n    word_vect.fit(all_text)\n    X  = word_vect.transform(train['question_text'])\n    testing  = word_vect.transform(test['question_text'])","abd0e9ac":"# Train Test Split\nX_train, X_valid, y_train, y_valid = train_test_split(\n        X, y, test_size=0.20, random_state=23, stratify=y)","406eef39":"# Create the visualizer and draw the vectors\nplt.figure(figsize = [15,9])\ntsne = TSNEVisualizer()\nn = 20000\ntsne.fit(X[:n], train.target[:n].map({1: target_names[1],0:target_names[0]}))\ntsne.poof()","c3ee936b":"# Fit Model\nmodel = LogisticRegression(solver = 'sag')\nmodel.fit(X_train, y_train)\n\n# Predict..\nvalid_logistic_pred = model.predict(X_valid)\ntrain_logistic_pred = model.predict(X_train)\nvalid_logistic_pred_proba = model.predict_proba(X_valid)\nvalid_logistic_pred_proba = [x[1] for x in valid_logistic_pred_proba]","119dc768":"print(\"Train Set Accuracy: {}\".format(metrics.accuracy_score(train_logistic_pred, y_train)))\nprint(\"Train Set ROC: {}\".format(metrics.roc_auc_score(train_logistic_pred, y_train)))\nprint(\"Train Set F1 Score: {}\\n\".format(metrics.f1_score(train_logistic_pred, y_train)))\n\nprint(\"Validation Set Accuracy: {}\".format(metrics.accuracy_score(valid_logistic_pred, y_valid)))\nprint(\"Validation Set ROC: {}\".format(metrics.roc_auc_score(valid_logistic_pred, y_valid)))\nprint(\"Validation Set F1 Score: {}\\n\".format(metrics.f1_score(valid_logistic_pred, y_valid)))\n\nprint(metrics.classification_report(valid_logistic_pred, y_valid))\n\n# Confusion Matrix\nskplt.metrics.plot_confusion_matrix(valid_logistic_pred, y_valid)\nplt.show()","be1edccf":"eli5.show_weights(model, vec = word_vect, top=(50,50),\n                  target_names=target_names)","20304a37":"def eli5_plotter(df, n = 5):\n    for iteration in range(n):\n        samp = random.randint(1,df.shape[0]-1)\n        print(\"Ground Truth: {} \\nPredicted: {}\".format(\n            pd.Series(df.target.iloc[samp]).map({0:'Sincere', 1: 'Insincere'})[0],\n            pd.Series(df.predicted.iloc[samp]).map({0:'Sincere', 1: 'Insincere'})[0]))\n        display(eli5.show_prediction(model, df.question_text.iloc[samp], vec=word_vect,\n                             target_names=target_names))\n        \n# Prepare Validation Set\nraw_valid = train.loc[train.index.isin(y_valid.index), :]\nraw_valid['predicted'] = valid_logistic_pred\nraw_valid['predicted_proba'] = valid_logistic_pred_proba\n\nraw_valid['wrong_degree'] = abs(raw_valid['predicted_proba'] - raw_valid['target'])","23f607a4":"temp = raw_valid.loc[raw_valid.sort_values(by='wrong_degree', ascending = False).index[:20], [\"question_text\", \"target\", \"predicted\"]]\neli5_plotter(temp, n=16)","31c6b5a4":"eli5_plotter(temp, n=30)","5b5483a2":"temp = raw_valid.loc[raw_valid.question_text.str.contains(\"(?i)trump\"),\n                     [\"question_text\", \"target\", \"predicted\"]]\neli5_plotter(temp, n=10)","0ada0d74":"# submit_df = pd.DataFrame({\"qid\": test.index, \"prediction\": model.predict(testing).astype(int)})\n# submit_df.to_csv(\"submission.csv\", index=False)\n# print(submit_df.head())\n# del submit_df","cefea103":"###########################################################################################\n### Upvote this :) - https:\/\/www.kaggle.com\/ogrellier\/lgbm-with-words-and-chars-n-gram ####\n###########################################################################################\n\n# The better written the code, the easier the copy pasta\n\n# Contraction replacement patterns\ncont_patterns = [\n    (b'(W|w)on\\'t', b'will not'),\n    (b'(C|c)an\\'t', b'can not'),\n    (b'(I|i)\\'m', b'i am'),\n    (b'(A|a)in\\'t', b'is not'),\n    (b'(\\w+)\\'ll', b'\\g<1> will'),\n    (b'(\\w+)n\\'t', b'\\g<1> not'),\n    (b'(\\w+)\\'ve', b'\\g<1> have'),\n    (b'(\\w+)\\'s', b'\\g<1> is'),\n    (b'(\\w+)\\'re', b'\\g<1> are'),\n    (b'(\\w+)\\'d', b'\\g<1> would'),\n]\npatterns = [(re.compile(regex), repl) for (regex, repl) in cont_patterns]\n\ndef prepare_for_char_n_gram(text):\n    \"\"\" Simple text clean up process\"\"\"\n    # 1. Go to lower case (only good for english)\n    # Go to bytes_strings as I had issues removing all \\n in r\"\"\n    clean = bytes(text.lower(), encoding=\"utf-8\")\n    # 2. Drop \\n and  \\t\n    clean = clean.replace(b\"\\n\", b\" \")\n    clean = clean.replace(b\"\\t\", b\" \")\n    clean = clean.replace(b\"\\b\", b\" \")\n    clean = clean.replace(b\"\\r\", b\" \")\n    # 3. Replace english contractions\n    for (pattern, repl) in patterns:\n        clean = re.sub(pattern, repl, clean)\n    # 4. Drop puntuation\n    # I could have used regex package with regex.sub(b\"\\p{P}\", \" \")\n    exclude = re.compile(b'[%s]' % re.escape(bytes(string.punctuation, encoding='utf-8')))\n    clean = b\" \".join([exclude.sub(b'', token) for token in clean.split()])\n    # 5. Drop numbers - as a scientist I don't think numbers are toxic ;-)\n    clean = re.sub(b\"\\d+\", b\" \", clean)\n    # 6. Remove extra spaces - At the end of previous operations we multiplied space accurences\n    clean = re.sub(b'\\s+', b' ', clean)\n    # Remove ending space if any\n    clean = re.sub(b'\\s+$', b'', clean)\n    # 7. Now replace words by words surrounded by # signs\n    # e.g. my name is bond would become #my# #name# #is# #bond#\n    # clean = re.sub(b\"([a-z]+)\", b\"#\\g<1>#\", clean)\n    clean = re.sub(b\" \", b\"# #\", clean)  # Replace space\n    clean = b\"#\" + clean + b\"#\"  # add leading and trailing #\n\n    return str(clean, 'utf-8')\n\ndef count_regexp_occ(regexp=\"\", text=None):\n    \"\"\" Simple way to get the number of occurence of a regex\"\"\"\n    return len(re.findall(regexp, text))\n\ndef get_indicators_and_clean_comments(df, text_var):\n    \"\"\"\n    Check all sorts of content as it may help find toxic comment\n    Though I'm not sure all of them improve scores\n    \"\"\"\n    # Count number of \\n\n    df[\"ant_slash_n\"] = df[text_var].apply(lambda x: count_regexp_occ(r\"\\n\", x))\n    # Get length in words and characters\n    df[\"raw_word_len\"] = df[text_var].apply(lambda x: len(x.split()))\n    df[\"raw_char_len\"] = df[text_var].apply(lambda x: len(x))\n    # Check number of upper case, if you're angry you may write in upper case\n    df[\"nb_upper\"] = df[text_var].apply(lambda x: count_regexp_occ(r\"[A-Z]\", x))\n    # Number of F words - f..k contains folk, fork,\n    df[\"nb_fk\"] = df[text_var].apply(lambda x: count_regexp_occ(r\"[Ff]\\S{2}[Kk]\", x))\n    # Number of S word\n    df[\"nb_sk\"] = df[text_var].apply(lambda x: count_regexp_occ(r\"[Ss]\\S{2}[Kk]\", x))\n    # Number of D words\n    df[\"nb_dk\"] = df[text_var].apply(lambda x: count_regexp_occ(r\"[dD]ick\", x))\n    # Number of occurence of You, insulting someone usually needs someone called : you\n    df[\"nb_you\"] = df[text_var].apply(lambda x: count_regexp_occ(r\"\\W[Yy]ou\\W\", x))\n    # Just to check you really refered to my mother ;-)\n    df[\"nb_mother\"] = df[text_var].apply(lambda x: count_regexp_occ(r\"\\Wmother\\W\", x))\n    # Just checking for toxic 19th century vocabulary\n    df[\"nb_ng\"] = df[text_var].apply(lambda x: count_regexp_occ(r\"\\Wnigger\\W\", x))\n    # Some Sentences start with a <:> so it may help\n    df[\"start_with_columns\"] = df[text_var].apply(lambda x: count_regexp_occ(r\"^\\:+\", x))\n    # Check for time stamp\n    df[\"has_timestamp\"] = df[text_var].apply(lambda x: count_regexp_occ(r\"\\d{2}|:\\d{2}\", x))\n    # Check for dates 18:44, 8 December 2010\n    df[\"has_date_long\"] = df[text_var].apply(lambda x: count_regexp_occ(r\"\\D\\d{2}:\\d{2}, \\d{1,2} \\w+ \\d{4}\", x))\n    # Check for date short 8 December 2010\n    df[\"has_date_short\"] = df[text_var].apply(lambda x: count_regexp_occ(r\"\\D\\d{1,2} \\w+ \\d{4}\", x))\n    # Check for http links\n    df[\"has_http\"] = df[text_var].apply(lambda x: count_regexp_occ(r\"http[s]{0,1}:\/\/\\S+\", x))\n    # check for mail\n    df[\"has_mail\"] = df[text_var].apply(\n        lambda x: count_regexp_occ(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+', x)\n    )\n    # Looking for words surrounded by == word == or \"\"\"\" word \"\"\"\"\n    df[\"has_emphasize_equal\"] = df[text_var].apply(lambda x: count_regexp_occ(r\"\\={2}.+\\={2}\", x))\n    df[\"has_emphasize_quotes\"] = df[text_var].apply(lambda x: count_regexp_occ(r\"\\\"{4}\\S+\\\"{4}\", x))\n\n    # Now clean comments\n    df[\"clean_comment\"] = df[text_var].apply(lambda x: prepare_for_char_n_gram(x))\n\n    # Get the new length in words and characters\n    df[\"clean_word_len\"] = df[\"clean_comment\"].apply(lambda x: len(x.split()))\n    df[\"clean_char_len\"] = df[\"clean_comment\"].apply(lambda x: len(x))\n    # Number of different characters used in a comment\n    # Using the f word only will reduce the number of letters required in the comment\n    df[\"clean_chars\"] = df[\"clean_comment\"].apply(lambda x: len(set(x)))\n    df[\"clean_chars_ratio\"] = df[\"clean_comment\"].apply(lambda x: len(set(x))) \/ df[\"clean_comment\"].apply(\n        lambda x: 1 + min(99, len(x)))\n    \ndef char_analyzer(text):\n    \"\"\"\n    This is used to split strings in small lots\n    I saw this in an article (I can't find the link anymore)\n    so <talk> and <talking> would have <Tal> <alk> in common\n    \"\"\"\n    tokens = text.split()\n    return [token[i: i + 3] for token in tokens for i in range(len(token) - 2)]\n\nall_text = pd.concat([train['question_text'],test['question_text']], axis =0)\n\nword_vect = TfidfVectorizer(\n            sublinear_tf=True,\n            strip_accents='unicode',\n            analyzer='word',\n            token_pattern=r'\\w{1,}',\n            stop_words='english',\n            ngram_range=(1, 2),\n            max_features=20000)\n\nchar_vectorizer = TfidfVectorizer(\n            sublinear_tf=True,\n            strip_accents='unicode',\n            tokenizer=char_analyzer,\n            analyzer='word',\n            ngram_range=(1, 1),\n            max_features=50000)\n\nwith timer(\"Word Grams TFIDF\"):\n    word_vect.fit(all_text)\n    train_word_features  = word_vect.transform(train['question_text'])\n    test_word_features  = word_vect.transform(test['question_text'])\n\nwith timer(\"Character Grams TFIDF\"):\n    char_vectorizer.fit(all_text)\n    train_char_features = char_vectorizer.transform(train['question_text'])\n    test_char_features = char_vectorizer.transform(test['question_text'])\n\nwith timer(\"Performing basic NLP\"):\n    get_indicators_and_clean_comments(train, 'question_text')\n    get_indicators_and_clean_comments(test,  'question_text')\n    \n    num_features = [f_ for f_ in train.columns\n                if f_ not in [\"question_text\", \"clean_comment\", \"remaining_chars\",\n                              'has_ip_address', 'target']]\n    \n# Get Sparse Matrix Feature Names..\nfeature_names = word_vect.get_feature_names() + char_vectorizer.get_feature_names() + num_features\ndel all_text; gc.collect()\n\nwith timer(\"Sparse Combine\"):\n    X = hstack(\n        [\n            train_char_features,\n            train_word_features,\n            train[num_features]\n        ]\n    ).tocsr()\n\n    del train_char_features\n    gc.collect()\n\n    testing = hstack(\n        [\n            test_char_features,\n            test_word_features,\n            test[num_features]\n        ]\n    ).tocsr()\n    del test_char_features; gc.collect()","d966106d":"# Create the visualizer and draw the vectors\nplt.figure(figsize = [15,9])\ntsne = TSNEVisualizer()\nn = 20000\ntsne.fit(X[:n], train.target[:n].map({1: target_names[1],0:target_names[0]}))\ntsne.poof()","07eb8dd5":"X_train, X_valid, y_train, y_valid = train_test_split(\n    X, y, test_size=0.20, random_state=23, stratify=y)\n\nprint(\"Light Gradient Boosting Classifier: \")\nlgbm_params = {\n        \"objective\": \"binary\",\n        'metric': {'auc'},\n        \"boosting_type\": \"gbdt\",\n        \"num_threads\": 4,\n        \"bagging_fraction\": 0.8,\n        \"feature_fraction\": 0.8,\n        \"learning_rate\": 0.1,\n        \"num_leaves\": 31,\n        \"min_split_gain\": .1,\n        \"reg_alpha\": .1\n    }\n\nmodelstart= time.time()\n# LGBM Dataset Formatting \nlgtrain = lgb.Dataset(X_train, y_train,\n                feature_name=feature_names)\nlgvalid = lgb.Dataset(X_valid, y_valid,\n                feature_name=feature_names)\n\n# Go Go Go\nlgb_clf = lgb.train(\n    lgbm_params,\n    lgtrain,\n    num_boost_round= max_boosting_rounds,\n    valid_sets=[lgtrain, lgvalid],\n    valid_names=['train','valid'],\n    early_stopping_rounds=150,\n    verbose_eval=100\n)\n\ndel lgtrain, lgvalid ;  gc.collect();","83bf74dd":"valid_pred = lgb_clf.predict(X_valid)\n_thresh = []\nfor thresh in np.arange(0.1, 0.501, 0.01):\n    _thresh.append([thresh, metrics.f1_score(y_valid, (valid_pred>thresh).astype(int))])\n#     print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(y_valid, (valid_pred>thresh).astype(int))))\n\n_thresh = np.array(_thresh)\nbest_id = _thresh[:,1].argmax()\nbest_thresh = _thresh[best_id][0]\nprint(\"Best Threshold: {}\\nF1 Score: {}\".format(best_thresh, _thresh[best_id][1]))","bf62fa83":"allmodelstart= time.time()\n# Run Model with different Seeds\nmulti_seed_pred = dict()\nall_feature_importance_df  = pd.DataFrame()\n\noptimal_rounds = lgb_clf.best_iteration\n\nlgtrain = lgb.Dataset(X, y, feature_name=feature_names)\n\nall_seeds = [27,22]\nfor seeds_x in all_seeds:\n    modelstart= time.time()\n    print(\"Seed: \", seeds_x,)\n    # Go Go Go\n    lgbm_params[\"seed\"] = seeds_x\n    lgb_seed_clf = lgb.train(\n        lgbm_params,\n        lgtrain,\n        num_boost_round = optimal_rounds + 1,\n        verbose_eval=200)\n\n    # Feature Importance\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = feature_names\n    fold_importance_df[\"importance\"] = lgb_seed_clf.feature_importance()\n    all_feature_importance_df = pd.concat([all_feature_importance_df, fold_importance_df], axis=0)\n\n    multi_seed_pred[seeds_x] =  list(lgb_seed_clf.predict(testing))\n    print(\"Model Runtime: %0.2f Minutes\"%((time.time() - modelstart)\/60))\n    print(\"###########################################################################################\")\n    del lgb_seed_clf\n    \ndel lgtrain, X","310f47de":"cols = all_feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n    by=\"importance\", ascending=False)[:50].index\nbest_features = all_feature_importance_df.loc[all_feature_importance_df.feature.isin(cols)]\nplt.figure(figsize=(8,10))\nsns.barplot(x=\"importance\", y=\"feature\", \n            data=best_features.sort_values(by=\"importance\", ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances.png')\nprint(\"All Model Runtime: %0.2f Minutes\"%((time.time() - allmodelstart)\/60))\n\n# To DataFrame\nsub_preds = pd.DataFrame.from_dict(multi_seed_pred).replace(0,0.000001)\ndel multi_seed_pred; gc.collect();\n\n# Correlation Plot\nf, ax = plt.subplots(figsize=[8,6])\nsns.heatmap(sub_preds.corr(),\n            annot=True, fmt=\".2f\",cbar_kws={'label': 'Percentage %'},cmap=\"plasma\",ax=ax)\nax.set_title(\"Correlation Plot for Seed Diversified Models\")\nplt.show()","1b5a7481":"# Take Mean over Seed prediction\ntarget_var = 'prediction'\nmean_sub = sub_preds.mean(axis=1).rename(target_var)\nmean_sub = (mean_sub > best_thresh).astype(int)\nmean_sub.index = testdex\n\n# Submit\nmean_sub.to_csv('submission.csv',index = True, header=True)\nprint(mean_sub.value_counts(normalize=True))\nmean_sub.head()","602ae19e":"non_sparse = pd.DataFrame(X_valid[:400].toarray(), columns = feature_names)\nprint(non_sparse.shape)","9dfb198d":"explainer = shap.TreeExplainer(lgb_clf)\nshap_values = explainer.shap_values(non_sparse)","38fc9536":"# summarize the effects of all the features\nshap.summary_plot(shap_values, non_sparse)","878ca5b5":"# visualize the first prediction's explanation\nshap.force_plot(explainer.expected_value, shap_values[0,:], non_sparse.iloc[0,:])","6fa5d324":"# valid_data = train.loc[train.index.isin(y_valid.index), :]\n\n# # visualize the first prediction's explanation\n# for iteration in range(15):\n#     samp = random.randint(1,non_sparse.shape[0])\n#     print(\"Real Label: {}\".format(valid_data.target.iloc[samp]))\n#     display(valid_data.question_text.iloc[samp])\n#     display(shap.force_plot(explainer.expected_value, shap_values[samp,:], non_sparse.iloc[samp,:]))","0217b399":"shap.dependence_plot(\"trump\", shap_values, non_sparse)","548629b2":"print(\"Notebook Runtime: %0.2f Minutes\"%((time.time() - notebookstart)\/60))","bff5de12":"**SHAP** <br>\nHaving big memory issues here.","6b1b9e03":"**TSNE - Visual Clustering**","46d0c988":"**Expand to see more Cases..:**","dc55bd0c":"**Submit**","c76f537f":"**Model and Model Evaluation:**","5d133895":"***\n\n## Logistic Regression and Count Vectorizer\n\nLets start with a simple model. <br>\n\n**TF-IDF**","10a38084":"**Has to do with Trump:**","83ad52d7":"## Seed Diversification\n\nHere I build two final models with different seeds in order to reach better generalization.","3100d793":"**Take a Glimpse**","c5486bef":"## ELI5 & SHAP - LGBM\/LR - Interpretable ML\n\n_By Nick Brooks, November 2018_\n\nText Processing and LGBM parameters taken from @Olivier, please give him credit where it is due! <br>\nhttps:\/\/www.kaggle.com\/ogrellier\/lgbm-with-words-and-chars-n-gram","b4f3e265":"**Load Packages and Data**","5bed9592":"### Light Gradient Boosting Binary Classifier","f7fdc090":"## TF-IDF - Word and Character Grams & Regular NLP\n\nUpvote this :) - https:\/\/www.kaggle.com\/ogrellier\/lgbm-with-words-and-chars-n-gram","5db84bd1":"**Get Optimal Threshold**","1588a2d1":"**Model Evaluation**","121387e0":"## Visualize Logistic Regression Predictions that are MOST wrong with ELI5\n\n**NOTE:** For the sentence highlight, the greener the highlight, the more the model believes it contributes to the y= CLASS. Sometimes green may mean its a high contributor to INSINCERE.."}}