{"cell_type":{"258a92e3":"code","214f804c":"code","eb4ea25c":"code","8198dae9":"code","c8c152f8":"code","e06ba2ce":"code","d61a075d":"code","3178cd7c":"code","2750667b":"code","5d96cfa3":"code","2b63ca60":"code","8c1e6a77":"code","1ccc8e6f":"code","40bef9c2":"code","33fd5a54":"code","180ac9af":"code","8013b8a7":"code","f02ef729":"code","6e0aedfc":"code","dd5cf8c0":"code","c7b4cc51":"code","45f4e30b":"markdown","9c71f205":"markdown","f0ca9e3d":"markdown","f2da9f36":"markdown","17888884":"markdown","d213dc20":"markdown","98027eca":"markdown","e971d406":"markdown","109236da":"markdown","0725fb90":"markdown","b235b736":"markdown","02a0d726":"markdown","d1f264c0":"markdown"},"source":{"258a92e3":"from IPython.core.display import display, HTML\n\nimport pandas as pd\nimport numpy as np\nimport glob\nimport os\nimport gc\n\nfrom joblib import Parallel, delayed\n\nfrom sklearn import preprocessing, model_selection\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.metrics import r2_score\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport numpy.matlib\n\n\nfrom numpy.random import seed\nseed(114)\nimport tensorflow as tf\ntf.random.set_seed(114)\nfrom tensorflow import keras\nimport numpy as np\nfrom keras import backend as K\n\n\nfrom sklearn.cluster import KMeans\n\n\n#https:\/\/bignerdranch.com\/blog\/implementing-swish-activation-function-in-keras\/\nfrom keras.backend import sigmoid\ndef swish(x, beta = 1):\n    return (x * sigmoid(beta * x))\nfrom keras.utils.generic_utils import get_custom_objects\nfrom keras.layers import Activation\nget_custom_objects().update({'swish': Activation(swish)})\n\n","214f804c":"path_submissions = '\/'\n# data directory\ndata_dir = '..\/input\/optiver-realized-volatility-prediction\/'\ntrain_filename = '..\/input\/optiver-realized-volatility-prediction\/train.csv'\ntest_filename = '..\/input\/optiver-realized-volatility-prediction\/test.csv'\n\ntarget_name = 'target'\nscores_folds = {}\n\n# Model Parameters\nlearning_rate = 0.0011\nnum_epochs = 200 # 1000","eb4ea25c":"# Function to calculate first WAP\ndef wap1(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n    return wap\n\n# to calculate second WAP\ndef wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) \/ (df['bid_size2'] + df['ask_size2'])\n    return wap\n\n# to calculate the log of the return\n# log(xi, xi-1) = log(xi) - log(xi-1)\ndef log_return(series):\n    return np.log(series).diff()\n\n# to calculate the realized volatility\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n\n# Function to count unique elements of a series\ndef count_unique(series):\n    return len(np.unique(series))\n\n# Function to read our base train and test set\ndef read_train_test(train_filename, test_filename):\n    ##TOREMOVE: _train = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\n    ##TOREMOVE: _test = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\n    _train = pd.read_csv(train_filename)\n    _test = pd.read_csv(test_filename)\n    # Create a key to merge with book and trade data\n    _train['row_id'] = _train['stock_id'].astype(str) + '-' + _train['time_id'].astype(str)\n    _test['row_id'] = _test['stock_id'].astype(str) + '-' + _test['time_id'].astype(str)\n    print(f'The training set has {_train.shape[0]} rows')\n    print(f'The test set has {_train.shape[0]} rows')\n          \n    ##TODO BORRAR\n    #d = _train[_train['stock_id']==1]\n    #return d, _test\n    \n    return _train, _test\n          \n\ndef root_mean_squared_per_error(y_true, y_pred):\n    return K.sqrt(K.mean(K.square( (y_true - y_pred)\/ y_true )))","8198dae9":"# to preprocess book data (for each stock id)\ndef book_preprocessor(file_path):\n    \n    print ('Init: book_preprocessor')\n    \n    df = pd.read_parquet(file_path)\n    \n    # Calculate Wap\n    df['wap1'] = wap1(df)\n    df['wap2'] = wap2(df)\n    \n    # Calculate log returns\n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n    # Calculate wap balance\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    # Calculate spread\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1']) \/ 2)\n    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) \/ ((df['ask_price2'] + df['bid_price2']) \/ 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'wap1': [np.sum, np.mean, np.std],\n        'wap2': [np.sum, np.mean, np.std],\n        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n        'wap_balance': [np.sum, np.mean, np.std],\n        'price_spread':[np.sum, np.mean, np.std],\n        'price_spread2':[np.sum, np.mean, np.std],\n        'bid_spread':[np.sum, np.mean, np.std],\n        'ask_spread':[np.sum, np.mean, np.std],\n        'total_volume':[np.sum, np.mean, np.std],\n        'volume_imbalance':[np.sum, np.mean, np.std],\n        \"bid_ask_spread\":[np.sum, np.mean, np.std],\n    }\n    \n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    #df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n    #df_feature_500 = get_stats_window(seconds_in_bucket = 500, add_suffix = True)\n    df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n    #df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n\n    # Merge all\n    #df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n    #df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n    #df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    # Drop unnecesary time_ids\n    #df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150'], axis = 1, inplace = True)\n    df_feature.drop(['time_id__400', 'time_id__300', 'time_id__200'], axis = 1, inplace = True)\n    \n    # Create row_id so we can merge\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n    \n    print ('End: book_preprocessor')\n    \n    return df_feature","c8c152f8":"# Function to preprocess trade data (for each stock id)\ndef trade_preprocessor(file_path):\n    print ('Init: trade_preprocessor')\n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum, realized_volatility, np.mean, np.std, np.max, np.min],\n        'order_count':[np.mean,np.sum,np.max],\n    }\n    \n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n\n    # Get the stats for different windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    #df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n    #df_feature_500 = get_stats_window(seconds_in_bucket = 500, add_suffix = True)\n    df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n    #df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n    \n    def tendency(price, vol):    \n        df_diff = np.diff(price)\n        val = (df_diff\/price[1:])*100\n        power = np.sum(val*vol[1:])\n        return(power)\n    \n    lis = []\n    for n_time_id in df['time_id'].unique():\n        df_id = df[df['time_id'] == n_time_id]        \n        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n        # new\n        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n        energy = np.mean(df_id['price'].values**2)\n        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n        \n        # vol vars\n        \n        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n        energy_v = np.sum(df_id['size'].values**2)\n        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n        \n        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n    \n    df_lr = pd.DataFrame(lis)\n        \n   \n    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n    \n    # Merge all\n    #df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    #df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n    # Drop unnecesary time_ids\n    #df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150','time_id'], axis = 1, inplace = True)\n    df_feature.drop(['time_id__400', 'time_id__300', 'time_id__200','time_id'], axis = 1, inplace = True)\n    \n    \n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n    \n    print ('End: trade_preprocessor')\n    \n    return df_feature\n\n","e06ba2ce":"# Function to get group stats for the stock_id and time_id\ndef get_time_stock(df):\n    # Get realized volatility columns\n    #vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_450', 'log_return2_realized_volatility_450', \n    #            'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_150', 'log_return2_realized_volatility_150', \n    #           'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_450', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_150']\n    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n    #     vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility',\n    #                 'log_return1_realized_volatility_600', 'log_return2_realized_volatility_600', \n    #                 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400',\n    # #                 'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', \n    #                 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200',\n    # #                 'log_return1_realized_volatility_100', 'log_return2_realized_volatility_100', \n    #                 'trade_log_return_realized_volatility',\n    #                 'trade_log_return_realized_volatility_600', \n    #                 'trade_log_return_realized_volatility_400',\n    # #                 'trade_log_return_realized_volatility_300',\n    # #                 'trade_log_return_realized_volatility_100',\n    #                 'trade_log_return_realized_volatility_200']\n\n    # Group by the stock id\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n    # Group by the stock id\n    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n    \n    # Merge with original dataframe\n    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n    return df\n    \n# Funtion to make preprocessing function in parallel (for each stock id)\ndef preprocessor(list_stock_ids, is_train = True):\n    \n    # Parrallel for loop\n    def for_joblib(stock_id):\n        # Train\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet\/stock_id=\" + str(stock_id)\n        # Test\n        else:\n            file_path_book = data_dir + \"book_test.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet\/stock_id=\" + str(stock_id)\n    \n        # Preprocess book and trade data and merge them\n        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n        \n        # Return the merge dataframe\n        return df_tmp\n    \n    # Use parallel api to call paralle for loop\n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n    \n    # Concatenate all the dataframes that return from Parallel\n    df = pd.concat(df, ignore_index = True)\n    return df\n\n# Function to calculate the root mean squared percentage error\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n\n# Function to early stop with root mean squared percentage error\n# TODO: Rreview....\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False","d61a075d":"# Funtion to make preprocessing function in parallel (for each stock id)\ndef prepreprocessor(list_stock_ids, is_train = True):\n    \n    # Parrallel for loop\n    def for_joblib(stock_id):\n        # Train\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet\/stock_id=\" + str(stock_id)\n        # Test\n        else:\n            file_path_book = data_dir + \"book_test.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet\/stock_id=\" + str(stock_id)\n    \n        # Preprocess book and trade data and merge them\n        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n        \n        # Return the merge dataframe\n        return df_tmp\n    \n    # Use parallel api to call paralle for loop\n    ##df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n    \n    for stock_id in list_stock_ids:\n        print('stock_id: {}'.format(stock_id))\n        d1 = for_joblib(stock_id)\n        # Concatenate all the dataframes that return from Parallel\n        ##df = pd.concat(d1, ignore_index = True)\n        if stock_id==0:\n            df = d1\n        else:\n            df.append(d1, ignore_index = True)\n        \n        ##if stock_id==0: break\n            \n    return df","3178cd7c":"def train_test_readisness(train_filename, test_filename):\n    # Read train and test\n    print ('Reading files ...')\n    train, test = read_train_test(train_filename, test_filename )\n\n    print ('Train Preprocessing ...')\n\n    # Get unique stock ids \n    train_stock_ids = train['stock_id'].unique()\n\n    # Preprocess them using Parallel and our single stock id functions\n    ##train_ = preprocessor(train_stock_ids, is_train = True)\n    train_ = prepreprocessor(train_stock_ids, is_train = True)\n    train = train.merge(train_, on = ['row_id'], how = 'left')\n\n    print ('Test Preprocessing ...')\n    # Get unique stock ids \n    test_stock_ids = test['stock_id'].unique()\n\n    # Preprocess them using Parallel and our single stock id functions\n    ##test_ = preprocessor(test_stock_ids, is_train = False)\n    test_ = prepreprocessor(test_stock_ids, is_train = False)\n    test = test.merge(test_, on = ['row_id'], how = 'left')\n\n    print ('Grouping stats ...')\n    # Get group stats of time_id and stock_id\n    train = get_time_stock(train)\n    test = get_time_stock(test)\n\n    print('Pre-Procesor done.')\n    \n    # replace by order sum (tau)\n    train['size_tau'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_count_unique'] )\n    test['size_tau'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_count_unique'] )\n    train['size_tau_400'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_count_unique_400'] )\n    test['size_tau_400'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_count_unique_400'] )\n    train['size_tau_300'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_count_unique_300'] )\n    test['size_tau_300'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_count_unique_300'] )\n    train['size_tau_200'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_count_unique_200'] )\n    test['size_tau_200'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_count_unique_200'] )\n    \n    # tau2 \n    train['size_tau2'] = np.sqrt( 1\/ train['trade_order_count_sum'] )\n    test['size_tau2'] = np.sqrt( 1\/ test['trade_order_count_sum'] )\n    train['size_tau2_400'] = np.sqrt( 0.25\/ train['trade_order_count_sum'] )\n    test['size_tau2_400'] = np.sqrt( 0.25\/ test['trade_order_count_sum'] )\n    train['size_tau2_300'] = np.sqrt( 0.5\/ train['trade_order_count_sum'] )\n    test['size_tau2_300'] = np.sqrt( 0.5\/ test['trade_order_count_sum'] )\n    train['size_tau2_200'] = np.sqrt( 0.75\/ train['trade_order_count_sum'] )\n    test['size_tau2_200'] = np.sqrt( 0.75\/ test['trade_order_count_sum'] )\n\n    # delta tau\n    train['size_tau2_d'] = train['size_tau2_400'] - train['size_tau2']\n    test['size_tau2_d'] = test['size_tau2_400'] - test['size_tau2']\n    \n    print('Function: train_test_readisness done.')\n    \n    return train, test","2750667b":"def kfold_knn():\n    # kfold based on the knn++ algorithm\n\n    out_train = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\n    out_train = out_train.pivot(index='time_id', columns='stock_id', values='target')\n\n    #out_train[out_train.isna().any(axis=1)]\n    out_train = out_train.fillna(out_train.mean())\n    out_train.head()\n\n    # code to add the just the read data after first execution\n\n    # data separation based on knn ++\n    nfolds = 5 # number of folds\n    index = []\n    totDist = []\n    values = []\n    # generates a matriz with the values of \n    mat = out_train.values\n\n    scaler = MinMaxScaler(feature_range=(-1, 1))\n    mat = scaler.fit_transform(mat)\n\n    nind = int(mat.shape[0]\/nfolds) # number of individuals\n\n    # adds index in the last column\n    mat = np.c_[mat,np.arange(mat.shape[0])]\n\n    lineNumber = np.random.choice(np.array(mat.shape[0]), size=nfolds, replace=False)\n    lineNumber = np.sort(lineNumber)[::-1]\n\n    for n in range(nfolds):\n        totDist.append(np.zeros(mat.shape[0]-nfolds))\n\n    # saves index\n    for n in range(nfolds):\n        values.append([lineNumber[n]])    \n\n\n    s=[]\n    for n in range(nfolds):\n        s.append(mat[lineNumber[n],:])\n        mat = np.delete(mat, obj=lineNumber[n], axis=0)\n\n    for n in range(nind-1):    \n        luck = np.random.uniform(0,1,nfolds)\n\n        for cycle in range(nfolds):\n             # saves the values of index           \n\n            s[cycle] = np.matlib.repmat(s[cycle], mat.shape[0], 1)\n\n            sumDist = np.sum( (mat[:,:-1] - s[cycle][:,:-1])**2 , axis=1)   \n            totDist[cycle] += sumDist        \n\n            # probabilities\n            f = totDist[cycle]\/np.sum(totDist[cycle]) # normalizing the totdist\n            j = 0\n            kn = 0\n            for val in f:\n                j += val        \n                if (j > luck[cycle]): # the column was selected\n                    break\n                kn +=1\n            lineNumber[cycle] = kn\n\n            # delete line of the value added    \n            for n_iter in range(nfolds):\n\n                totDist[n_iter] = np.delete(totDist[n_iter],obj=lineNumber[cycle], axis=0)\n                j= 0\n\n            s[cycle] = mat[lineNumber[cycle],:]\n            values[cycle].append(int(mat[lineNumber[cycle],-1]))\n            mat = np.delete(mat, obj=lineNumber[cycle], axis=0)\n\n\n    for n_mod in range(nfolds):\n        values[n_mod] = out_train.index[values[n_mod]]\n\n    \n    return values","5d96cfa3":"def nnnn(train, test):\n    \n    colNames = list(train)\n\n    colNames.remove('time_id')\n    colNames.remove('target')\n    colNames.remove('row_id')\n    colNames.remove('stock_id')\n\n    train.replace([np.inf, -np.inf], np.nan,inplace=True)\n    test.replace([np.inf, -np.inf], np.nan,inplace=True)\n    qt_train = []\n\n    for col in colNames:\n        #print(col)\n        qt = QuantileTransformer(random_state=21,n_quantiles=2000, output_distribution='normal')\n        train[col] = qt.fit_transform(train[[col]])\n        test[col] = qt.transform(test[[col]])    \n        qt_train.append(qt)\n    \n    # making agg features\n    train_p = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\n    train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n\n    corr = train_p.corr()\n\n    ids = corr.index\n\n    kmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n    print('------------------------------AAAAAAAAAAAAAA--------------------------')\n    print(kmeans.labels_)\n    print('------------------------------AAAAAAAAAAAAAA--------------------------')\n\n    l = []\n    for n in range(7):\n        l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n\n    mat = []\n    matTest = []\n\n    n = 0\n    for ind in l:\n        print(ind)\n        newDf = train.loc[train['stock_id'].isin(ind) ]\n        newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n        newDf.loc[:,'stock_id'] = str(n)+'c1'\n        mat.append ( newDf )\n\n        newDf = test.loc[test['stock_id'].isin(ind) ]    \n        newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n        newDf.loc[:,'stock_id'] = str(n)+'c1'\n        matTest.append ( newDf )\n\n        n+=1\n\n    mat1 = pd.concat(mat).reset_index()\n    mat1.drop(columns=['target'],inplace=True)\n\n    mat2 = pd.concat(matTest).reset_index()\n    \n    \n    \n    matTest = []\n    mat = []\n    kmeans = []\n\n    mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\n\n    mat1 = mat1.pivot(index='time_id', columns='stock_id')\n    mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n    mat1.reset_index(inplace=True)\n\n    mat2 = mat2.pivot(index='time_id', columns='stock_id')\n    mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n    mat2.reset_index(inplace=True)\n    \n    nnn = ['time_id',\n         'log_return1_realized_volatility_0c1',\n         'log_return1_realized_volatility_1c1',     \n         'log_return1_realized_volatility_3c1',\n         'log_return1_realized_volatility_4c1',     \n         'log_return1_realized_volatility_6c1',\n         'total_volume_mean_0c1',\n         'total_volume_mean_1c1', \n         'total_volume_mean_3c1',\n         'total_volume_mean_4c1', \n         'total_volume_mean_6c1',\n         'trade_size_mean_0c1',\n         'trade_size_mean_1c1', \n         'trade_size_mean_3c1',\n         'trade_size_mean_4c1', \n         'trade_size_mean_6c1',\n         'trade_order_count_mean_0c1',\n         'trade_order_count_mean_1c1',\n         'trade_order_count_mean_3c1',\n         'trade_order_count_mean_4c1',\n         'trade_order_count_mean_6c1',      \n         'price_spread_mean_0c1',\n         'price_spread_mean_1c1',\n         'price_spread_mean_3c1',\n         'price_spread_mean_4c1',\n         'price_spread_mean_6c1',   \n         'bid_spread_mean_0c1',\n         'bid_spread_mean_1c1',\n         'bid_spread_mean_3c1',\n         'bid_spread_mean_4c1',\n         'bid_spread_mean_6c1',       \n         'ask_spread_mean_0c1',\n         'ask_spread_mean_1c1',\n         'ask_spread_mean_3c1',\n         'ask_spread_mean_4c1',\n         'ask_spread_mean_6c1',   \n         'volume_imbalance_mean_0c1',\n         'volume_imbalance_mean_1c1',\n         'volume_imbalance_mean_3c1',\n         'volume_imbalance_mean_4c1',\n         'volume_imbalance_mean_6c1',       \n         'bid_ask_spread_mean_0c1',\n         'bid_ask_spread_mean_1c1',\n         'bid_ask_spread_mean_3c1',\n         'bid_ask_spread_mean_4c1',\n         'bid_ask_spread_mean_6c1',\n         'size_tau2_0c1',\n         'size_tau2_1c1',\n         'size_tau2_3c1',\n         'size_tau2_4c1',\n         'size_tau2_6c1'] \n\n    train = pd.merge(train,mat1[nnn],how='left',on='time_id')\n    test = pd.merge(test,mat2[nnn],how='left',on='time_id')\n    mat1= []\n    mat2= []\n    \n    return train, test","2b63ca60":"def save_datasets(train, test, trained_filename, tested_filename):\n    train.to_csv(trained_filename)\n    test.to_csv(tested_filename)\n\ndef load_datasets(trained_filename, tested_filename):\n    _train = pd.DataFrame()\n    _test = pd.DataFrame()\n    _train.read_csv(trained_filename)\n    _test.pd.read_csv(tested_filename)\n    return _train, _test\n\n#trained_filename = 'mytrained.csv'\n#tested_filename = 'mytested.csv'\n#save_datasets(train, test, trained_filename, tested_filename)","8c1e6a77":"def define_neural_network():\n    \n    # INPUTS\n    stock_id_input = keras.Input(shape=(1,), name='stock_id')\n    num_input = keras.Input(shape=(362,), name='num_data')\n\n    cat_data = train['stock_id']\n    stock_embedding_size = 24\n    hidden_units = (128,64,32)\n\n    # Embedding, flatenning and concatenating\n    stock_embedded = keras.layers.Embedding(max(cat_data)+1, stock_embedding_size, \n                                           input_length=1, name='stock_embedding')(stock_id_input)\n    stock_flattened = keras.layers.Flatten()(stock_embedded)\n    out = keras.layers.Concatenate()([stock_flattened, num_input])\n    \n    # Add one or more hidden layers\n    for n_hidden in hidden_units:\n        out = keras.layers.Dense(n_hidden, activation='swish')(out)\n        out = keras.layers.Dropout(rate=0.02,seed=111)(out)\n        \n    #out = keras.layers.Concatenate()([out, num_input])\n\n    # A single output: our predicted rating\n    out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n    \n    model = keras.Model(\n        inputs = [stock_id_input, num_input],\n        outputs = out,\n    )\n    \n    return model","1ccc8e6f":"def prediction(train, test):\n\n    model_name = 'NN'\n    pred_name = 'pred_{}'.format(model_name)\n\n    # https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.KFold.html\n    # K-Folds cross-validator\n    # Provides train\/test indices to split data in train\/test sets. Split dataset into k consecutive folds.\n    n_folds = 5\n    kf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state=2020)\n    scores_folds[model_name] = []\n    counter = 1\n\n    features_to_consider = list(train)\n    features_to_consider.remove('time_id')\n    features_to_consider.remove('target')\n    features_to_consider.remove('row_id')\n    try:\n        features_to_consider.remove('pred_NN')\n    except:\n        pass\n\n    # Fill na values with the mean in train & test\n    train[features_to_consider] = train[features_to_consider].fillna(train[features_to_consider].mean())\n    test[features_to_consider] = test[features_to_consider].fillna(train[features_to_consider].mean())\n\n    train[pred_name] = 0\n    test['target'] = 0\n\n    # looping folds\n    for n_count in range(n_folds):\n        print('CV {}\/{}'.format(counter, n_folds))\n\n        ##indexes = np.arange(nfolds).astype(int)\n        indexes = np.arange(n_folds).astype(int)    \n        indexes = np.delete(indexes,obj=n_count, axis=0) \n\n        indexes = np.r_[values[indexes[0]],values[indexes[1]],values[indexes[2]],values[indexes[3]]]\n\n        X_train = train.loc[train.time_id.isin(indexes), features_to_consider]\n        y_train = train.loc[train.time_id.isin(indexes), target_name]\n        X_test = train.loc[train.time_id.isin(values[n_count]), features_to_consider]\n        y_test = train.loc[train.time_id.isin(values[n_count]), target_name]\n\n        # NN   \n        model = define_neural_network()\n\n        es = tf.keras.callbacks.EarlyStopping(\n                monitor='val_loss', patience=20, verbose=0,\n                mode='min',restore_best_weights=True)\n\n        plateau = tf.keras.callbacks.ReduceLROnPlateau(\n                monitor='val_loss', factor=0.2, patience=7, verbose=0,\n                mode='min')\n\n        print('Compile Model')\n        model.compile(\n            keras.optimizers.Adam(learning_rate=learning_rate),\n            loss=root_mean_squared_per_error\n        )\n\n        try:\n            features_to_consider.remove('stock_id')\n        except:\n            pass\n\n        num_data = X_train[features_to_consider]\n\n        scaler = MinMaxScaler(feature_range=(-1, 1))         \n        num_data = scaler.fit_transform(num_data.values)    \n\n        cat_data = X_train['stock_id']    \n        target =  y_train\n\n        num_data_test = X_test[features_to_consider]\n        num_data_test = scaler.transform(num_data_test.values)\n        cat_data_test = X_test['stock_id']\n\n        print('Fit Model')\n        model.fit([cat_data, num_data], \n                  target,               \n                  batch_size=2048,\n                  epochs=num_epochs,\n                  validation_data=([cat_data_test, num_data_test], y_test),\n                  callbacks=[es, plateau],\n                  validation_batch_size=len(y_test),\n                  shuffle=True,\n                  verbose = 1)\n\n        print('Predict Model')\n        preds = model.predict([cat_data_test, num_data_test]).reshape(1,-1)[0]\n\n        score = round(rmspe(y_true = y_test, y_pred = preds),5)\n        print('Fold {} {}: {}'.format(counter, model_name, score))\n        scores_folds[model_name].append(score)\n\n        tt =scaler.transform(test[features_to_consider].values)\n        test[target_name] += model.predict([test['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)\n        #test[target_name] += model.predict([test['stock_id'], test[features_to_consider]]).reshape(1,-1)[0].clip(0,1e10)\n\n        counter += 1\n        features_to_consider.append('stock_id')","40bef9c2":"def train_model(train, test, model):\n\n    model_name = 'NN'\n    pred_name = 'pred_{}'.format(model_name)\n\n    features_to_consider = list(train)\n    features_to_consider.remove('time_id')\n    features_to_consider.remove('target')\n    features_to_consider.remove('row_id')\n    try:\n        features_to_consider.remove('pred_NN')   ## No existe!!!!\n    except:\n        pass\n\n    # Fill na values with the mean in train & test\n    train[features_to_consider] = train[features_to_consider].fillna(train[features_to_consider].mean())\n    test[features_to_consider] = test[features_to_consider].fillna(train[features_to_consider].mean())\n\n    train[pred_name] = 0\n    test['target'] = 0\n\n    ##X_train = train.loc[train.time_id.isin(indexes), features_to_consider]\n    ##y_train = train.loc[train.time_id.isin(indexes), target_name]\n    ##X_test = train.loc[train.time_id.isin(values[n_count]), features_to_consider]\n    ##y_test = train.loc[train.time_id.isin(values[n_count]), target_name]\n\n    X_train = train.loc[train, features_to_consider]\n    y_train = train.loc[train, target_name]\n    X_test = train.loc[train, features_to_consider]\n    y_test = train.loc[train, target_name]    \n    \n\n    monitor = tf.keras.callbacks.EarlyStopping(\n          monitor='val_loss', patience=20, verbose=0,\n          mode='min',restore_best_weights=True)\n\n    plateau = tf.keras.callbacks.ReduceLROnPlateau(\n                monitor='val_loss', factor=0.2, patience=7, verbose=0,\n                mode='min')\n\n    print('Compile Model')\n    model.compile(\n            keras.optimizers.Adam(learning_rate=learning_rate),\n            loss=root_mean_squared_per_error\n        )\n\n    try:\n        features_to_consider.remove('stock_id')\n    except:\n        pass\n\n    num_data = X_train[features_to_consider]\n\n    scaler = MinMaxScaler(feature_range=(-1, 1))         \n    num_data = scaler.fit_transform(num_data.values)    \n\n    cat_data = X_train['stock_id']    \n    target =  y_train\n\n    num_data_test = X_test[features_to_consider]\n    num_data_test = scaler.transform(num_data_test.values)\n    cat_data_test = X_test['stock_id']\n\n    print('Fit Model')\n    model.fit([cat_data, num_data], \n                  target,               \n                  batch_size=2048,\n                  epochs=num_epochs,\n                  validation_data=([cat_data_test, num_data_test], y_test),\n                  callbacks=[monitor, plateau],\n                  validation_batch_size=len(y_test),\n                  shuffle=True,\n                  verbose = 1)\n\n\ndef predict_model(model, x, y_test, model_name):\n    print('Predict Model')\n    ##preds = model.predict([cat_data_test, num_data_test]).reshape(1,-1)[0]\n    preds = model.predict(x).reshape(1,-1)[0]\n\n    score = round(rmspe(y_true = y_test, y_pred = preds),5)\n    print('Score {}: {}'.format(model_name, score))\n\n    tt =scaler.transform(test[features_to_consider].values)\n    test[target_name] += model.predict([test['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)\n","33fd5a54":"def submission(train, test):\n    test[target_name] = test[target_name]\/n_folds\n\n    score = round(rmspe(y_true = train[target_name].values, y_pred = train[pred_name].values),5)\n    print('RMSPE {}: {} - Folds: {}'.format(model_name, score, scores_folds[model_name]))\n\n    display(test[['row_id', target_name]].head(2))\n    test[['row_id', target_name]].to_csv('submission.csv',index = False)\n    ","180ac9af":"def pre_submission(train, test):\n    #test[target_name] = test[target_name]\/n_folds\n\n    score = round(rmspe(y_true = train[target_name].values, y_pred = train[pred_name].values),5)\n    print('RMSPE {}: {} - Folds: {}'.format(model_name, score, scores_folds[model_name]))\n\n    display(test[['row_id', target_name]].head(2))\n    test[['row_id', target_name]].to_csv('submission.csv',index = False)","8013b8a7":"%%time\n##############################################################\ntrain, test = train_test_readisness(train_filename, test_filename)\n#values = kfold_knn()\n#train, test = nnnn(train, test)","f02ef729":"\n  #  train, test = read_train_test(train_filename, test_filename )\n\n#train","6e0aedfc":"#test","dd5cf8c0":"#model = define_neural_network()\n\n#predict_model(train, test, model)\n\n#model.fit([cat_data, num_data], \n#                  target,               \n#                  batch_size=2048,\n#                  epochs=num_epochs,\n#                  validation_data=([cat_data_test, num_data_test], y_test),\n#                  callbacks=[es, plateau],\n#                  validation_batch_size=len(y_test),\n#                  shuffle=True,\n#                  verbose = 1)\n    \n    \n","c7b4cc51":"#s = submission(train, test)\n#print(np.mean(s[model_name]))","45f4e30b":"# preprocessor Function","9c71f205":"# Base Model\nembedding, flatenning and concatenating","f0ca9e3d":"# Submission","f2da9f36":"# Train and Test","17888884":"# Realized Volatility Prediction\n\n\nA NN using Embedding.\n\n**References**\n* NOTE: Idea code based on https:\/\/www.kaggle.com\/syerwin\/stock-embedding-ffnn-my-features-a5d6b0\n* https:\/\/www.kaggle.com\/alexioslyon\/stock-embedding-ffnn-my-features\n\n\n* https:\/\/www.kaggle.com\/jiashenliu\/introduction-to-financial-concepts-and-data\n* https:\/\/www.kaggle.com\/vbmokin\/data-science-for-tabular-data-advanced-techniques\n* Embedding layer from : https:\/\/www.kaggle.com\/colinmorris\/embedding-layers\n\n**My Work**\n* Code learning, refactoring and understanding.\n","d213dc20":"# Training model and making predictions","98027eca":"# trade_preprocessor Function","e971d406":"# book_preprocessor Function","109236da":"# Prediction","0725fb90":"# Variables & Parameters","b235b736":"## Functions","02a0d726":"# Main","d1f264c0":"# Import Libraries"}}