{"cell_type":{"1c6a64f2":"code","3ec93232":"code","468e15a2":"code","d853f20c":"code","3a4512df":"code","efe6a8b0":"code","05fdb337":"code","493a0bb6":"code","b4849437":"code","2f7cd712":"code","8bb68569":"code","3820d01e":"code","4150e686":"code","0b142ed0":"code","dc3d097d":"code","b0453c35":"code","de7427f4":"code","1f4f4ccd":"code","26128213":"code","3d871f73":"code","81946b01":"code","99653d59":"code","d1ed9cfb":"code","1aadb8dc":"code","7e9909b5":"code","bb9efaeb":"code","e6c99da2":"code","fb763383":"code","1c8c9cac":"code","13409877":"code","e49b41c1":"code","84027884":"code","a13202b5":"code","90cf954a":"code","ace62fdb":"code","7d3a426f":"code","648dbe72":"code","38647ec0":"code","8d62af8a":"code","3422b1a6":"code","742a2fef":"code","48d999d8":"code","45ce419b":"code","d8714968":"code","569b03c9":"code","4a232024":"code","11ad5f3f":"code","d3bfce4d":"code","ad439f9c":"code","e675f355":"code","d2fd2b9c":"code","0bc29270":"code","032263fa":"code","b5364152":"code","b953d593":"code","36c6e80d":"code","541caec0":"code","c1e8037d":"code","70c8efbb":"code","f1e16274":"code","acbcba57":"code","3eb56dbb":"code","f5ebceca":"code","55639881":"code","be952955":"code","36204c42":"code","bf94334c":"code","3db0faf0":"code","9e478e02":"code","ec20329c":"code","adec7fa7":"code","348589eb":"markdown","f470f489":"markdown","16eedb1f":"markdown","448bed0f":"markdown","4d7eab55":"markdown","b4c95891":"markdown","94faf201":"markdown","d6bcb064":"markdown","aa66aa00":"markdown","4946a1a8":"markdown","c4131161":"markdown","7d12d272":"markdown","a8da47c2":"markdown","5fdfbd37":"markdown","877d8919":"markdown","44e4c67d":"markdown","9a1a68ff":"markdown","08b2e7be":"markdown","7c6b783d":"markdown","cc2eb41d":"markdown","22cd3c3e":"markdown","31fd2a05":"markdown","fc6e9180":"markdown","3f4fca93":"markdown","282f4a8f":"markdown","523bbfd5":"markdown","d9956c44":"markdown","7c7d2306":"markdown","fabe971f":"markdown","73795c4c":"markdown","74caa302":"markdown","782b66c9":"markdown","284454bc":"markdown","38c526e5":"markdown","fd3a0d9b":"markdown","05b73a9e":"markdown","6f72ecca":"markdown","64fa01df":"markdown","e26363d0":"markdown","8e80885b":"markdown","a7680d0e":"markdown","c69a88f9":"markdown","68ed9217":"markdown","aa5882be":"markdown","9cebd3b1":"markdown","fdd2a1ca":"markdown"},"source":{"1c6a64f2":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn import svm\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import StrMethodFormatter\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3ec93232":"#import data\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ndata = pd.concat([train,test]).reset_index(drop=True)\ndata","468e15a2":"data.dtypes","d853f20c":"data.isna().sum()","3a4512df":"data[data.Embarked.isna()]","efe6a8b0":"data['Embarked'] = data['Embarked'].fillna('S')","05fdb337":"data.groupby(['Pclass','Sex'])['Age'].median()","493a0bb6":"sns.violinplot(x='Pclass',y='Age',hue='Sex',data=data,split = True, palette = 'RdBu_r')\nplt.title('Age Distribution of Passengers based on Class')","b4849437":"def fill_age(df,age,sex,pclass):\n    \"\"\"This function finds all passengers with NaN age values, looks at the passengers class\/sex,\n    computes the mean age for passengers of the same class\/sex, and\n    populates the missing age with the mean age for that respective class\/sex combination \"\"\"\n    for i in range(0,len(age)):\n        if np.isnan(age[i]):\n            age[i] = df.loc[(df.Sex == sex[i]) & (df.Pclass == pclass[i])].Age.median()\n\n    df.Age = age\n    return df    ","2f7cd712":"age = list(data.Age)\npclass = list(data.Pclass)\nsex = list(data.Sex)\n\n#apply previously written function to populate missing age data\nfill_age(data,age,sex,pclass)","8bb68569":"def fill_fare(df,fare,pclass):\n    \"\"\"performs same job as fill_age, but populates missing fare data instead.\n    Populates based on mean Pclass fare price.\"\"\"\n    for i in range(0,len(fare)):\n        if np.isnan(fare[i]):\n            fare[i] = df.loc[(df.Pclass == pclass[i])].Fare.mean()\n\n    df.Fare = fare\n    return df  ","3820d01e":"fare=list(data.Fare)\npclass=list(data.Pclass)\n\n#apply previously written function to populate missing fare information\nfill_fare(data,fare,pclass)","4150e686":"#analyze missing cabin data\n#label any missing cabins as 'M'\ndata.Cabin = data.Cabin.fillna('M')\n\n#export cabin numbers to a list to process\ncabinlist = list(data.Cabin)\n\n#loop through cabin lists, keeping only the first character \n#(the letter signifying the cabin the passenger is in)\nfor i in range(0,len(cabinlist)):\n    if len(cabinlist[i]) > 1:\n        cabinlist[i] = cabinlist[i][0]\n\n#put modified list back in the dataframe\ndata.Cabin = cabinlist\ndata","0b142ed0":"#visualize survivors in each cabin\nsns.countplot('Cabin',hue='Survived',data=data, palette = 'RdYlGn',edgecolor = 'dimgrey')\nplt.legend(loc='upper right')\nplt.title('Survivor Breakdown by Cabin')","dc3d097d":"data.isna().sum()","b0453c35":"#splits name into separate components to locate title\ndata['title'] = data.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\n\n#prints unique titles for all passengers aboard\nset(data.title)","de7427f4":"#create a dictionary to bin passenger titles into more general categories\nnewtitles = {'Capt':'Officer','Col':'Officer','Don':'Officer','Dr':'Officer','Jonkheer':'Noble','Lady':'Noble',\\\n            'Major':'Officer','Master':'Master','Miss':'Miss','Mlle':'Mlle','Mme':'Mme','Mr':'Mr',\\\n            'Mrs':'Mrs','Ms':'Ms','Rev':'Officer','Sir':'Noble','the Countess':'Noble','Dona':'Miss'}\n#assign new title to each passenger\ndata.title = data.title.map(newtitles)\ndata.sample(3)","1f4f4ccd":"def visualize_passengers(df,features,colors):\n    \"\"\"This function calculates the number of passengers for each categorical \n    feature and generates a graph to visualize this data. \n    It does this by grouping the dataframe for each feature, and sorting \n    in descending order, which makes visualization cleaner\"\"\"\n    plt.figure(figsize = [10,8])\n    \n    for i in range(0,len(features)):\n        if (features[i] != 'SibSp') & (features[i] != 'Parch'):\n            feature = df.groupby(['{}'.format(features[i])]).count().mean(axis=1).sort_values()\n            plt.subplot(4,3,i+1)\n            feature.plot(kind='barh',x=list(feature.index),y=list(feature.values),\\\n                         color=colors[i],width=0.8,edgecolor='dimgrey')\n            plt.xticks(rotation = 0)\n            plt.xlabel('Number of Passengers')\n            plt.title('Passengers by {}'.format(features[i]))\n        else:\n            feature = df.groupby(['{}'.format(features[i])]).count().mean(axis=1)\n            plt.subplot(4,3,i+1)\n            feature.plot(kind='bar',x=list(feature.index),y=list(feature.values),\\\n                         color=colors[i],width=0.8,edgecolor='dimgrey')\n            plt.xticks(rotation = 0)\n            plt.ylabel('Number of Passengers')\n            plt.title('Passengers by {}'.format(features[i]))\n        plt.tight_layout()\n    return feature","26128213":"#visualize training data\nfeatures = ['Pclass','title','Sex','SibSp','Parch','Cabin','Embarked','Survived']\ncolors = ['lightcoral','navajowhite','khaki','lightgreen','powderblue','plum','lightpink','dimgray']\nvisualize_passengers(data[data.Survived.isna()==0],features,colors)","3d871f73":"#visualize test data\nfeatures = ['Pclass','title','Sex','SibSp','Parch','Cabin','Embarked']\ncolors = ['lightcoral','navajowhite','khaki','lightgreen','powderblue','plum','lightpink']\nvisualize_passengers(data[data.Survived.isna()],features,colors)","81946b01":"#calculate correlation between all variables\ncorrelations = data[data.Survived.isna()==0][['Survived','Pclass','Sex','Age','SibSp','Parch',\\\n                                           'Fare']].corr()\n\n#create a dataframe that sorts variables by how well-correlated they are to the 'Survived' target feature\nordered_correlations = pd.DataFrame(correlations.Survived).sort_values(by='Survived')","99653d59":"mask = np.zeros_like(correlations)\nmask[np.triu_indices_from(mask)] = True\n\nplt.subplots(figsize=(10,5))\ncorr_plot = sns.heatmap(correlations,annot = True,cmap = sns.diverging_palette(10,240, n=100),\\\n                        cbar = True,mask = mask, linewidth = 0.5)\nplt.yticks(rotation = 0)\nplt.title('Correlation of Raw Numerical Features',fontsize = 25)","d1ed9cfb":"plt.subplots(figsize=(15,5))\ncorr_plot = sns.heatmap(ordered_correlations.transpose(),annot = True,cmap = sns.diverging_palette(10,240, n=100),\\\n                        cbar_kws = dict(use_gridspec=False,location=\"bottom\"),\\\n                        cbar = True,square = True,linewidth = 0.5)\nplt.yticks(rotation = 0, fontsize = 17)\nplt.title('Correlation of Features with Passenger Survival\\n',fontsize = 25)","1aadb8dc":"def survivor_histogram(df,features):\n    \"\"\"This function counts the number of passengers for each \n    feature and generates a graph to visualize the survivors vs non-survivors.\"\"\"\n    plt.figure(figsize = [15,15])\n    \n    for i in range(0,len(features)):\n        plt.subplot(3,3,i+1)\n        sns.countplot(features[i],hue='Survived',data=data, palette = 'RdYlGn'\\\n                     ,edgecolor = 'dimgrey')\n        plt.xticks(rotation = 0)\n        plt.ylabel('Number of Passengers')\n        plt.title('Survivorship by {}'.format(features[i]))\n        plt.legend(loc='upper right')\n        plt.tight_layout()\n    return plt.show()","7e9909b5":"#visualize survivors by feature for training data\nfeatures = ['Pclass','title','Sex','SibSp','Parch','Cabin','Embarked']\nsurvivor_histogram(data[data.Survived.isna()==0],features)","bb9efaeb":"sns.distplot(data[data.Survived==0].Age,color='r', kde = False)\nsns.distplot(data[data.Survived==1].Age,color='g', kde = False)\nplt.ylabel('Number of Passengers')\nplt.title('Age Distribution of Passengers')","e6c99da2":"sns.distplot(data[data.Survived==0].Fare,color='r', kde = False)\nsns.distplot(data[data.Survived==1].Fare,color='g', kde = False)\nplt.ylabel('Number of Passengers')\nplt.title('Fare Distribution of Passengers')","fb763383":"def visualize_survivorship(df,features):\n    \"\"\"This function calculates the survivorship percentage for each categorical \n    feature and generates a graph to visualize this data. \n    It does this by creating a dataframe for each feature, and sorts the dataframe \n    in descending order, which makes visualization easier\"\"\"\n    sorted_dict = {}\n    plt.figure(figsize = [15,10])\n    \n    for i in range(0,len(features)):\n        sorted_dict.update({'Survived':(df.groupby(['{}'.format(features[i])]).Survived.mean())})\n        sorted_df = pd.DataFrame(sorted_dict).sort_values(by = 'Survived',ascending = False)\n        plt.subplot(5,3,i+1)\n        sns.barplot(sorted_df.index,'Survived',data=sorted_df, palette = 'Greens_d',\\\n                   edgecolor = 'dimgrey')\n        plt.title('Survivorship Percentage based on {}'.format(features[i]))\n        plt.tight_layout()\n    return sorted_df","1c8c9cac":"features1=['Pclass','title','Sex','SibSp','Parch','Cabin','Embarked']\nvisualize1=visualize_survivorship(data[data.Survived.isna()==0],features1)","13409877":"#add a feature that counts how many family members a given passenger has onboard\ndata['Family'] = data['SibSp'] + data['Parch'] \n\n#add a feature to count how many times a given ticket appears.\n#this is a useful feature to gauge passengers who were not travelling alone, but didn't necessary have family onboard\n#(ie. Passengers with maids)\ntickets = list(set(data.Ticket))\ndata['TicketFreq'] = [(list(data.Ticket).count(ticket)) for ticket in data.Ticket]\n\n#add a feature for married women\ndata['Married'] = [1 if title == 'Mrs' else 0 for title in data.title]","e49b41c1":"#split Name feature to isolate the surname of each passenger\ndata['surname'] = data.Name.apply(lambda x: x.split(',')[0].strip())","84027884":"#filter for unique surnames in the training set\ntraining_names = data[data.Survived.isna()==0].surname.unique()\n\n#filter for unique surnames in the test set\ntest_names = data[data.Survived.isna()].surname.unique()\n\n#find the surnames that appear in both the training and test set\ncommon_names = [name for name in training_names if name in test_names]\n\n#find the names that appear exclusively in the training set\ntraining_only_names = [name for name in training_names if name not in common_names]\n\n#find the names that appear exclusively in the test set\ntest_only_names = [name for name in test_names if name not in common_names]","a13202b5":"#calculate the mean survival rate for passengers in the training set\nmean_survival_rate = np.mean(data[data.Survived.isna() == 0].Survived)","90cf954a":"#filter for families that have more than 1 member, and calculate the survival rate for each of those families\nfamily_survival_rates = data[data.Family>0].groupby('surname').Survived.mean()\n\n#fill NaN values with 0 for familes where the mean was not computable\nfamily_survival_rates = family_survival_rates.fillna(0)\n\n\nfamsurv = []\nfamsurvna = []\n\n#loop through passengers, and if their surname is in both the training and test sets, assign the corresponding\n#family survival rate.\nfor name in data.surname:\n    if name in list(family_survival_rates.index) and name in common_names:\n        famsurv.append(family_survival_rates[name])\n        famsurvna.append(0)\n    else:\n        #if the passenger surname is not in both the training and test set, assign them the mean survival rate\n        famsurv.append(mean_survival_rate)\n        famsurvna.append(1)\n\n#add the calculated family survival rates to the main dataframe\ndata['famsurv'] = famsurv\ndata['famsurvNA'] = famsurvna\ndata[data.famsurv==mean_survival_rate]","ace62fdb":"#filter for unique ticket numbers in the training set\ntraining_tickets = list(set(data[data.Survived.isna()==0].Ticket))\n\n#filter for unique ticket numbers in the test set\ntest_tickets = list(set(data[data.Survived.isna()].Ticket))\n\n#find ticket numbers that appear in both the training and test set\ncommon_tickets = [ticket for ticket in training_tickets if ticket in test_tickets]\n\n#find ticket numbers exclusive to the training set\ntraining_only_tickets = [ticket for ticket in training_tickets if ticket not in common_tickets]\n\n#find ticket numbers exclusive to the test set\ntest_only_tickets = [ticket for ticket in test_tickets if ticket not in common_tickets]","7d3a426f":"#for ticket numbers that appear more than once, calculate the survival rate for each ticket number\nticket_survival_rates = data[data.TicketFreq>1].groupby('Ticket').Survived.mean()\n\n##fill NaN values with 0 for ticket numbers where the mean was not computable\nticket_survival_rates = ticket_survival_rates.fillna(0)\n\nticksurv = []\nticksurvna = []\n\n#loop through passengers, and if their ticket number is in both the training and test sets, assign the corresponding\n#ticket survival rate.\nfor ticket in data.Ticket:\n    if ticket in list(ticket_survival_rates.index) and ticket in common_tickets:\n        ticksurv.append(ticket_survival_rates[ticket])\n        ticksurvna.append(0)\n    else:\n        #if the passenger ticket number is not in both the training and test set, assign them the mean survival rate\n        ticksurv.append(mean_survival_rate)\n        ticksurvna.append(1)\n\n#add calculated ticket survival rates back to the main dataframe\ndata['ticksurv'] = ticksurv\ndata['ticksurvNA'] = ticksurvna\ndata","648dbe72":"#average the family survival rate and the ticket survival rate to reduce dimensionality of data\ndata['survrate'] = round(((data['famsurv'] + data['ticksurv']) \/ 2),2)\ndata['survrateNA'] = round(((data['famsurvNA'] + data['ticksurvNA']) \/ 2), 2)\ndata.sample(7)","38647ec0":"#combine female passenger titles together. High survival rates across all passengers with these titles\ndata.title = data.title.replace(['Mlle','Mme','Ms','Miss','Mrs'],'Ms\/Mrs')","8d62af8a":"#bucket cabins together that had similar survival rates\ndata.Cabin = data.Cabin.replace(['D','E','B'],'DEB')\ndata.Cabin = data.Cabin.replace(['F','C','G','A'],'FCGA')\ndata.Cabin = data.Cabin.replace(['M','T'],'M')","3422b1a6":"#bucket family sizes into 0 (traveller was alone), small families (1-3 people), and large families (>3 people)\nfor i in range(0,len(data.Family)):\n    if data.Family[i] == 0:\n        data.Family[i] = 0\n    elif data.Family[i] <=3 and data.Family[i] > 0:\n        data.Family[i] = 1\n    else:\n        data.Family[i] = 2","742a2fef":"#use qcut() function discretize age\/fare data into equal-sized buckets\ndata.Age = pd.qcut(data.Age,7,precision=1,duplicates='drop')\ndata.Fare = pd.qcut(data.Fare,4,precision=2)","48d999d8":"fig, axs = plt.subplots(figsize=(10, 6))\nsns.countplot(x='Age',hue='Survived',data=data, palette = 'RdYlGn',edgecolor='dimgrey')\nplt.title('Survivor Count by Age Bucket')","45ce419b":"fig, axs = plt.subplots(figsize=(10, 6))\nsns.countplot(x='Fare',hue='Survived',data=data, palette = 'RdYlGn',edgecolor='dimgrey')\nplt.title('Survivor Count by Fare Bucket')","d8714968":"#visualize survivorship rates of new features\nfeatures=['title','Family','Cabin','survrate','Married']\ntestdf=visualize_survivorship(data[data.Survived.isna()==0],features)","569b03c9":"features = ['Family','TicketFreq']\nsurvivor_histogram(data[data.Survived.isna()==0],features)","4a232024":"#drop features that are no longer needed (we used these features as stepping stones to other features)\ndata = data.drop(['SibSp','Parch','surname','famsurvNA','famsurv','ticksurvNA','ticksurv'],axis = 1)\ndata.sample(3)","11ad5f3f":"le = LabelEncoder()\nbinned_data = data.copy()\n\n#label encode discretized age\/fare features\nbinned_data.Age = le.fit_transform(binned_data.Age)\nbinned_data.Fare = le.fit_transform(binned_data.Fare)\n\n#create list of categorical variables we want to one hot encode\ncategorical_features = ['Pclass','Sex','Cabin','Embarked','title','Family']\n\n#use Pandas get_dummies() function to one hot encode categorical variables\ncatdf = pd.get_dummies(binned_data[categorical_features].astype(str))\n\ncatdf","d3bfce4d":"#create dataframe of ordinal features\norddf = binned_data[['Age','Fare','TicketFreq']]\n\n#create dataframe of ratio features\nratiodf = binned_data['survrate']\n\n#create dataframe of other categorical variables that did not need to be one hot encoded with get_dummies()\ncatdf2 = binned_data[['survrateNA','Married']]\n\n#concatenate our features into one dataframe\nbinned_data = pd.concat([binned_data[['Survived']],catdf,orddf,ratiodf,catdf2],axis=1)\nbinned_data","ad439f9c":"binned_data.columns","e675f355":"#separate dataframe into training and test sets\ntraining_set = binned_data[binned_data.Survived.isna()==0]\n\ntest_set = binned_data[binned_data.Survived.isna()].drop(['Survived'],axis=1)","d2fd2b9c":"training_set","0bc29270":"test_set","032263fa":"#scale\/normalize the training set\nx_data = training_set.drop(['Survived'],axis=1)\nX = preprocessing.StandardScaler().fit_transform(x_data)\n\n#create target variable\ny = training_set['Survived']","b5364152":"#trial run with a logistic regression model\nlr_model = LogisticRegression(solver='liblinear')\n\n#compute cross validation score for logistic regression model\nlr_score = np.mean(cross_val_score(lr_model,X,y,scoring = 'accuracy'))\nlr_score","b953d593":"model_params = {\n    'LogisticRegression':{\n        'model':LogisticRegression(),\n        'params':{\n            'solver':['liblinear','lbfgs'], \n            'C':[0.01,0.1,0.5,0.75,1,10] #regularization parameter to dampen impact of features on model\n        }\n    },\n    'svm':{\n        'model':svm.SVC(),\n        'params':{\n            'kernel':['rbf','linear'],\n            'C':[0.01,0.1,0.5,1,1.5,10]\n        }\n    },\n    'decision_tree':{\n        'model':DecisionTreeClassifier(),\n        'params':{\n            'criterion':['gini','entropy']\n        }\n    },\n    'random_forest':{\n        'model':RandomForestClassifier(),\n        'params':{\n            'n_estimators':[1,5,20,50,100,1000]\n        }\n    }\n}","36c6e80d":"scores = []\n\n#loop through different ML models and return the parameters that yield the highest cross validation score for each model\nfor model_name,mp in model_params.items():\n    clf = GridSearchCV(mp['model'],mp['params'],return_train_score = False)\n    clf.fit(X,y)\n    scores.append({\n        'model':model_name,\n        'best_score':clf.best_score_,\n        'best_params':clf.best_params_\n    })","541caec0":"#convert results from GridSearchCV into a dataframe\nmodels_df = pd.DataFrame(scores)\nmodels_df","c1e8037d":"#train SVM model using tuned parameters from GridSearchCV\nsvm_model = svm.SVC(C = 0.1, kernel = 'linear')\nsvm_model.fit(X,y)\n           \n#compute cross validation score\nsvm_cv_score = np.mean(cross_val_score(svm_model,X,y,scoring = 'accuracy'))\nsvm_cv_score","70c8efbb":"#create learning curves to diagnose bias\/variance\ntrain_sizes, train_scores, cv_scores = learning_curve(estimator = svm_model,X = X, y = y,shuffle = True, \\\n                                                     train_sizes = np.linspace(0.01,1,100),\\\n                                                         scoring = 'neg_mean_squared_error')\n\n#compute mean scores for training and cross-validation set to generate learning curves\nmean_train_score = train_scores.mean(axis = 1) * -1\nmean_cv_score = cv_scores.mean(axis = 1) * -1\n\n#plot learning curves\nplt.plot(train_sizes, mean_train_score, label = 'Training error')\nplt.plot(train_sizes, mean_cv_score, label = 'Cross Validation error')\nplt.ylabel('MSE', fontsize = 14)\nplt.xlabel('Training set size', fontsize = 14)\nplt.title('Learning curves for Tuned SVM Model', fontsize = 18)\nplt.legend()","f1e16274":"#define and fit selector object\nselector = SelectKBest(f_classif, k=14)\nselector.fit(X,y)\n\n#get indices of best features\nbest_features = selector.get_support(indices = True)\nbest_features ","acbcba57":"#get names of best performing features\nx_data.columns[best_features]","3eb56dbb":"#reduce feature set\nreduced_feat_set = x_data.iloc[:,best_features]\n\n#normalize reduced feature set\nX_reduced = preprocessing.StandardScaler().fit_transform(reduced_feat_set)","f5ebceca":"scores1 = []\n\n#loop through different ML models and return the parameters that yield the highest cross validation score for each model\nfor model_name,mp in model_params.items():\n    clf1 = GridSearchCV(mp['model'],mp['params'],return_train_score = False)\n    clf1.fit(X_reduced,y)\n    scores1.append({\n        'model':model_name,\n        'best_score':clf1.best_score_,\n        'best_params':clf1.best_params_\n    })\n\n#convert results from GridSearchCV into a dataframe\nmodels_df1 = pd.DataFrame(scores1)\nmodels_df1","55639881":"#get cross validation score for SVM model using reduced feature set\nclf2 = svm.SVC(C = 1, kernel = 'rbf')\n\ncv_score2 = np.mean(cross_val_score(clf2,X_reduced,y,scoring = 'accuracy'))\ncv_score2","be952955":"#create learning curves to diagnose bias\/variance\ntrain_sizes, train_scores, cv_scores = learning_curve(estimator = clf2,X = X_reduced, y = y,shuffle = True, \\\n                                                     train_sizes = np.linspace(0.01,1,100),\\\n                                                         scoring = 'neg_mean_squared_error')\n\n#compute mean scores for training and cross-validation set to generate learning curves\nmean_train_score = train_scores.mean(axis = 1) * -1\nmean_cv_score = cv_scores.mean(axis = 1) * -1\n\n#plot learning curves\nplt.plot(train_sizes, mean_train_score, label = 'Training error')\nplt.plot(train_sizes, mean_cv_score, label = 'Cross Validation error')\nplt.ylabel('MSE', fontsize = 14)\nplt.xlabel('Training set size', fontsize = 14)\nplt.title('Learning curves for SVM Model with Reduced Feature Set', fontsize = 18)\nplt.legend()","36204c42":"#plot ROC curve for SVM model\nmetrics.plot_roc_curve(clf2.fit(X_reduced,y), X_reduced, y)\nplt.show() ","bf94334c":"#test SVM model with reduced feature set:\ntest_set2 = test_set.iloc[:,best_features]\nX_test2 = preprocessing.StandardScaler().fit(test_set2).transform(test_set2.astype(float))\n\n#generate predictions for test set\nyhat = clf2.fit(X_reduced,y).predict(X_test2).astype(int)","3db0faf0":"#generate new dataframe with only test set PassengerId's and predicted survival classification\npredicted = pd.DataFrame(data[data.Survived.isna()].PassengerId)\npredicted['Survived'] = yhat\n\n#export results to .csv file for submission without indices or headers\npredicted.to_csv('\/kaggle\/working\/titanic_results2.csv',index=False,header=True)\npredicted.head(10)","9e478e02":"check = data[data.Survived.isna()].copy()\ncheck['predicted'] = predicted.Survived\nsample_men_live = [956,981,1053,1086,1088,1199,1284,1309]\nsample_women_die = [910,925,929,1024,1032,1080,1172,1176,1257,1259]","ec20329c":"#all the men in the table below survived the Titanic\ncheck[check.PassengerId.isin(sample_men_live)]","adec7fa7":"#all the women in the table below did not survive the Titanic\ncheck[check.PassengerId.isin(sample_women_die)]","348589eb":"The last variable we have to populate missing data for is the Cabin variable. There is a lot of missing data here, but it makes logical sense that there would be a relationship between what a passenger's cabin assignment and their survival.\n\nThe first character of the cabin number is useful to indicate what cabin each passenger was located in:","f470f489":"## 2.3 Data Visualization 2:\nNow that we've added a few more features, and modified some of the existing features, we can perform more visualizations.","16eedb1f":"Although GridSearchCV is telling us that a Random Forest classifier would perform slightly better, we will stick with SVMs because I understand them better. It's worth noting that we are not heavily regularizing the features in our new SVM model, and we are using an 'rbf' kernel in our SVM.","448bed0f":"Now we see we've populated all of the missing data, and delved deeper into our dataset, we can go ahead and start to visualize the data to better understand it.\n## 1.4 Data Visualization 1:","4d7eab55":"This new learning curve looks pretty good, judging by the narrowing gap between the training and cross validation error. It still looks like the training and cross validation curves are convergent. One more technique we have to evaluate model performance is the ROC curve.","b4c95891":"As we can see from the sample tables above, the model isn't doing too bad! It correctly predicts that all 8 of the men survive, and it correctly predicts 8\/10 non-survivals amongst the women listed.\n\n# 5. Conclusion:\nMy highest test score is *80.1*%! My goal was to score over 80%, so I'm very happy that I've achieved my goal! I tried the chronicle the path I took to solve this problem, and the tools I used to evaluate my model along the way. This was my no means a linear progression, and I hope that I highlighted my struggles in a way that helps other beginners diagnose their models. Thanks for reading!\n\n-**Evan Cheng**","94faf201":"Let's take care of the low-hanging fruit. There are only 2 missing values in our Embarked variable. We can filter for these samples:","d6bcb064":"There is a really interesting notebook on a rules-based classification model for this dataset: https:\/\/www.kaggle.com\/c\/titanic\/discussion\/57447. In the notebook, Chris Deotte provides two sample list of passengers from the test set: one list of men that survived the Titanic, and one list of women that did not survive. We will use these lists to do a quick visual inspection of how our model did on correctly making these predictions.","aa66aa00":"What we see from the plots above is that the passenger characteristics seem fairly similar across both the training and test sets. Some general observations:\n* Most passengers are male\n* Most passengers are Pclass 3 (lowest class)\n* Most passengers have no siblings\/spouses\/parents\/children aboard, meaning they are travelling alone\n* Most passengers are Cabin M, meaning their cabin information was not provided\n* Most passengers embarked from Southampton\n\nNow we need to explore the relationship between all these variables and passenger survival:","4946a1a8":"## 1.3 Data Cleaning:\nThere is a lot of useful information we could deduce from the names of passengers. Their title indicates their marital status, role aboard the ship, and economic status. Let's break this variable down: ","c4131161":"From Encyclopedia Titanica, we know both Amelie Icard and Martha Evelyn Stone embarked from Southampton, so we can manually input missing embarkment data","7d12d272":"# 1.1 Data Collection:\nNormally, this stage in the data science methodology would take a while, but because this is a convenient Kaggle competition, our data has been collected for us. \nImport the training and test data, and merge them into one dataframe. This will allow us to clean the training and test data together at once:","a8da47c2":"The qcut() method was great to bucket the continuous variables into equally sized groups. Binning the data this way highlighted the fact that young passengers, and passengers that paid a higher ticket fare were more likely to survive, as we will see in the visualizations below.","5fdfbd37":"Our cross validation score with the reduced feature set (14 features) is *84.8*%. Our cross validation score with the full feature set (25 features) was *85.2*%. So we've lost almost no performance, while managing to reduce our number of features by >*40*%! This not only hopefully solves our problem of overfitting, but also reduces compute time.\n\nLet's examine the learning curves for the model using the reduced feature set:","877d8919":"## 4.6 Submission:","44e4c67d":"# 3. Final Clean-Up \/ Munging Before Modeling:","9a1a68ff":"It would be extremely useful to know how many of passenger's family members or travel companions (ie. same ticket number) survived. This feature that helps connect the training set to the test set through the passenger surnames or the ticket number. If we are predicting the survival of a passenger in the test set, knowing how many of his family members and\/or travel companions survived is very useful. To create features that convey this information will take a bit of work:","08b2e7be":"## 4.5 ROC Curves:\n**ROC Curves**: A curve that shows the true positive rate and false positive rate as the threshold for classification of positive\/negative class is varied. As you lower the threshold for positive classification (ie. make it easier for the model to predict a positive class), you improve your true positive rate, but your false positive rate also increases. Essentially, you are casting a wider net, so while you will catch more true positive values, you will also increase the likelihood that your model mistakes a negative class for a positive class. The vice is also versa. Increasing your threshold for positive classification (ie. making the model more selective about what it chooses to classify as a positive example) means that you will make more negative classifications, but your total number of true positive classifications will also decrease. You are tightening your net so that only examples you are very confident are positive will be classified as positive.  An ideal ROC curve hugs the top left corner of the graph as closely as possible. \n\n**AUC (Area Under the Curve)**: A perfect classifier algorithm would have an AUC = 1. ","7c6b783d":"## 4.2 Selecting a Classfier Algorithm:\nWith an untuned logistic regression classifier, we are achieving a cross validation score of *84.6*%, which is encouraging. But it would be useful to compare a variety of ML models\/algorithms without too much code. Here's a **brief** novice synopsis of each supervised learning (classification) algorithm we will be testing:\n\n* **Logistic Regression**: uses a special mathematical function (the sigmoid function) to output a probability between 0 and 1 of whether or not a training example is classified positively. \n* **Support Vector Machines (SVMs)**: fits a hyperplane (decision boundary) between the positive and negative class in the dataset.\n* **Decision Tree**: Splits data repeatedly based on features in the dataset to sort training examples into their positive\/negative classes.\n* **Random Forest**: An ensemble of decision trees, with very high computational power.\n\n### 4.2.1 Parameter Tuning with GridSearchCV:\nWe will create a dictionary of different models and tuning parameters to pass to Scikit-learn's GridSearchCV module. The GridSearchCV module will iterate through all the different parameters we've given it for all the different classification algorithms. It will compute the cross validation score for each algorithm using the different combinations of parameters we've provided, and will return the best model for us to use, as well as the tuned parameters that optimize the performance of that model.","cc2eb41d":"Some takeaways from the above histograms:\n* 1st class passengers have the highest survival rate\n* Passengers with Mrs, Ms, Miss etc. (ie. women) have very high survival rates. Masters (boys) also have high survival rates\n* Men have very low survival rates\n* Passengers without siblings, spouses, chldren, or parents onboard (ie. solo travellers) have low survival rates\n* Certain cabins (D,E,B) have noticeably higher survival rates than other cabins\n* Passengers embarking from Cherbourg have a better survival rate compared to other points of embarkment\n\nContinuing with the visualization, let's look at the distribution of continuous variables (age and ticket fare):","22cd3c3e":"We can see from the violin plot that the age distributions by sex for the different passenger classes is quite different. We will therefore fill the missing age data by calculating the mean age for each sex, by passenger class:","31fd2a05":"From the plot above, it looks like **Pclass** and **Fare** are well correlated to survival. But this plot fails to show us how categorical variables relate to passenger survival. So here are more visualizations:","fc6e9180":"# Focus of this Notebook:\nMassive thanks to Gunes Evitan and his fantastic feature engineering tutorial, which was instrumental in the development of this notebook:\nhttps:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial\n\nMy notebook borrows a lot of the features from Gunes' tutorial, but in my notebook I try to emphasize model evaluation and diagnostics, which caused me the biggest headache during this project.\nWith this notebook, I will tell the story of how I solved this problem, and the challenges I faced along the way. I will be using the Data Science Methodology template coined by IBM's John Rollins as a guide for the problem solving process.\n(https:\/\/www.ibmbigdatahub.com\/blog\/why-we-need-methodology-data-science)\n\n","3f4fca93":"#### Modeling Attempt 2:\nAfter creating all those new features (and a couple ones that I removed from this notebook), I tried modeling again. Creating so many new features led to a high variance model (overfit). You can check out my overfit models here: https:\/\/www.kaggle.com\/bethecheng\/titanic-ml-competition-take-3. My overfit models didn't perform much better than my initial attempt at the problem. Throwing some high power algorithms like Random Forests at the data only got me a score of *78*%. A slight improvement, but still not quite satisfying...\n\n## 2.2 Binning \/ Bucketing Features:\nPart of the solution to address this overfitting was to bin our features to reduce the variance within these features. The visualizations made in \"**Data Visualization 1**\" provide insight into how we might bin our variables.","282f4a8f":"From the plot above, it's clear that different cabins have different survival rates. Below, we can see that we have no more missing variables (aside from the Survived feature for our test set):","523bbfd5":"## 3.1 Encoding Categorical Features:\nWe need to encode the categorical features into numerical features so that they can be processed by our model.\n\nNote there are two types of encoding: \n1. **Label encoding**: converts string data into numerical data by assigning a unique number to each label. \n2. **One hot encoding**: splits categorical data into multiple columns, where each column contains only 1's or 0's to signify if that data sample has that particular categorical attribute.\n\nIn my earlier attempts at the project, I made the mistake of using **label encoding** for all my variables, without realizing that each encoding technique is best suited for certain kinds of data.\n\n**One hot encoding** should be used on categorical variables, where the numerical order has no significance. **Label encoding** should be used when the order and magnitude of number values is meaningful for your variable. Here's a super helpful article that articulates this idea better than I do: https:\/\/towardsdatascience.com\/choosing-the-right-encoding-method-label-vs-onehot-encoder-a4434493149b","d9956c44":"## 1.2 Data Understanding\/Preparation:\n### 1.2.1 Variable Types:\nLet's take a look at what kind of information we are given in this dataset:","7c7d2306":"Now we have a reduced feature set, we can plug it back into GridSearchCV to see how our model should be re-tuned to optimize to the new feature set.","fabe971f":"The heatmap above is a great tool to see correlation between variables, but we can focus specifically on visualizing the correlation of the variables with the target '**Survived**' variable:","73795c4c":"Now that our feature set is scaled\/normalized, we can plug it into a model. We don't know what kind of classifier algorithm will work best, so let's just try logistic regression:","74caa302":"We are working with the following variable types:\n\nCategorical\/Nominal Variables:\n* Survived (0 = didn't survive, 1 = survived)\n* Pclass (1 = first class, 2 = second class, 3 = third class)\n* Sex (male or female)\n* Embarked (C = Cherbourg, S = Southampton, Q = Queenstown)\n\nRatio Variables:\n* Age\n* Fare \n* SibSp (number of siblings\/spouses onboard with passenger)\n* Parch (number of parents\/children onboard with passenger)\n\nString Variables:\n* Name\n* Ticket\n* Cabin","782b66c9":"This learning curve seemed to be performing well. The training and cross validation error converged, but they converge at a fairly high value of mean squared error (MSE) which might indicate high bias (underfit). This could be due to the regularization parameter (C) that we tuned our model with. With this model, I was still only achieving a score of *78*%, which was not an improvement from my previous attempts. \n\nI continued modifying my model, adding features because the learning curves were indicating high bias. After submitting my model several times, and realizing that my performance on the test set was continuing to fall, I realized I was starting to overtrain the model. As my cross validation score improved (up to ~*89*%), my test scores fell to ~*73*%. It was clear to me my model was now overfit, so I set out to further refine the model.\n\nAs mentioned earlier, one technique to correct overfitting is to reduce the number of features. I will use Scikit-learn's SelectKBest module to help me choose which features are most important to the model. This will help me decide which features to keep.\n\n## 4.4 Feature Selection with SelectKBest:\nSelectKBest takes a score function parameter that it uses to evaluate which features are best for the model. I randomly started by selecting the best 5 features using an F-test as my scoring parameter. I tinkered with the k parameter, and found that **k = 14** parameters seemed to produce the best test score.\n\n**F-test**: compares the variance between a given feature and the target, with the variance within the feature \/ target. A large F-statistic means the variance between groups is high, while the variance within groups is low. High between-group variance means that the variable has a strong impact on the model.\nHere is a great tutorial on F-statistics: https:\/\/towardsdatascience.com\/fisher-test-for-regression-analysis-1e1687867259","284454bc":"We can fill missing Fare information by simply calculating the mean fare for the passenger class of the missing sample:","38c526e5":"Import the required libraries:","fd3a0d9b":"There is a lot of missing Age data, so the way we impute missing Age data must be given careful thought. We can analyze the age distribution of passengers based on their class and sex:","05b73a9e":"### 1.2.2 Impute Missing Values:\nCheck to see how many missing values are in each feature:","6f72ecca":"We've thrown four different classfiers at the dataset, and it looks like support vector machines (SVMs) are our best option, so we will fit a model that has had its parameters tuned for us by GridSearchCV. Note that the C parameter given to us by GridSearchCV is <1, which means we are regularizing the features, which indicates that our model was overfit.","64fa01df":"Some notes on the parameters we are asking GridSearchCV to iterate through:\n* The **solver** parameter in logistic regression refers to the method by which the algorithm performs gradient descent to find the minima of the logistic regression cost function.  \n* The **C** value (for both logistic regression and SVMs) is a regularization parameter. Regularization is a means of reducing the impact that a feature has on the algorithm by reducing the magnitude of the values in that feature. A low value of C (C < 1) indicates heavy regularization. Regularization is helpful to correct overfit (high variance) models.\n* The **kernel** in SVMs is a function that is used to create a \"similarity\", which is a transformation of the original feature set. These kernels help SVMs learn complex, non-linear decision boundaries. A linear kernel is a fancy way of saying no kernel is being applied. An 'rbf' or radial basis function kernel is a common type of kernel, of which the most famous example is the Gaussian kernel.\n* The **entropy** criterion for decision trees tells the algorithm to make splits in the data based on the purity of information that results from each split (ie. How cleanly does the split separate training examples into positive\/negative classes?).\n* As a machine learning novice, random forests are still a bit of black hole to me, so don't ask about **n_estimators** or any other random forest parameters...\n","e26363d0":"Now that we've selected the best performing model based on cross-validation score, we need to evaluate the model to determine if any further refinements can be made.\n\n### 4.3 Diagnosing Bias\/Variance with Learning Curves:\n#### 4.3.1 Underfit (High Bias) & Overfit (High Variance):\n**Underfit (High Bias)**: Describes models that are too general\/simplistic for the training dataset. Underfit models lack specificity to correctly model the training data, and also do not model the cross validation data well. Underfit models can be corrected by adding more features, and removing regularization parameters. \n\n**Overfit (High Variance)**: Describes models that fit the training set so well that they do not generalize well to data outside the training set. Overfit models can be corrected by reducing the number of features, increasing the amount of regularization, or by providing more training data to the model.\n\n#### 4.3.2 Learning Curves:\nWe need to plot the **learning curves** for this model to diagnose if we have underfit\/overfit the model. Scikit-learn has a learning_curve() module to help us. \nA **learning curve** is a graph of the training error and cross validation error plotted as a function of training set size. The idea is that a very small training set is easy to overfit (high cross validation error, low training error), but as we increase the training set size, if our model is well fit, the cross validation error will decrease, the training error will increase, and hopefully there will be a sort of convergence between the two errors. If we see the training and cross validation error converging at a very high error value, we know we have a high bias (underfit) model. If we see the training error remain low, and there is a large gap between the training and cross validation errors, even as the training set size increases, we know we have a high variance (overfit) model.\n\nHere's a great tutorial on learning curves: https:\/\/www.dataquest.io\/blog\/learning-curves-machine-learning\/","8e80885b":"Now that we've determined our best features, we can reduce our feature set:","a7680d0e":"## 2.1 Feature Creation:\nInitally, I created a host of new features, some more meaningful than others. The features that remain in this notebook are left over from what I felt were the most effective and logical features.","c69a88f9":"Takeaways from the distribution plots above:\n* Young passengers have a higher survival rate\n* Low fare seems to result in low survival. This is logical, as higher class \/ more important passengers would pay higher fares\n\nThere is another way we can visualize data, to get a better sense of the actual rates of survival:","68ed9217":"# 1. Exploratory Data Analysis","aa5882be":"#### Model Attempt 1:\nAt this stage in my first attempt at this competition, I created a machine learning (ML) model without doing anything else. I had only taken IBM's Data Science specialization on Coursera, so I didn't have a great understanding of Python and the common ML libraries (NOTE: I do realize that my understanding is still VERY RUDIMENTARY). \n\nWithout working any more with the dataset, I normalized\/scaled the data, and threw a bunch of algorithms at it. That got me a score of *76*% (~18,000th of ~23,000 teams). \nThis first pass at the model can be found here: https:\/\/www.kaggle.com\/bethecheng\/titanic-ml-competition-take-1\n\nI realized I was still missing a bunch of skills\/knowledge to handle this problem, so I took a break from Kaggle, and enrolled in Andrew Ng's Machine Learning MOOC.\n\n#### Evaluation 1:\n\nAfter completing the MOOC, I had learned a bit about model evaluation, and diagnosing bias\/variance. I plotted the learning curves for this very basic model, and realized the model was underfit (high bias). \nYou can check out the learning curves and the rest of the notebook here: https:\/\/www.kaggle.com\/bethecheng\/titanic-ml-competition-take-2\n\nTo correct the underfit model, I set out to add some features...I will discuss model evaluation and high bias\/variance in the **Modeling** section of this notebook.\n# 2 Feature Engineering:\n","9cebd3b1":"Now we have fully numerical training and test sets we can use for modeling. Our training set has one extra column because it contains the target '**Survived**' variable.\n\n# 4. Modeling:\nI really focused on this portion of the data science methodology because I felt like there wasn't much attention given to it in other notebooks. I found it helpful to be able to diagnose how the models were performing, and how I could use that information to update my feature set and model.\n## 4.1 Normalizing the Feature Set:\nNormalization of the feature set is required to make sure that variables with different magnitudes are not treated differently. It's not a huge problem with this dataset, but imagine we had a feature that had a range of values from 0 - 10,000, and all the other features ranged only from 0 - 10. In that case, the classification algorithm would assign too much weight\/importance to the feature with the large magnitude, and that variable would have too large of an impact on the model. Normalization helps mitigate this issue. ","fdd2a1ca":"The bar plots above confirm our inferences from the previous plots, but present a more digestable visual that indicates survival rates of different variables, rather than showing the absolute count of survivors\/non-survivors. "}}