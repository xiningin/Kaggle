{"cell_type":{"26b82aa1":"code","81fa0606":"code","22ad8910":"code","cabf4497":"code","d9d99696":"code","a67b6125":"code","c18bf28b":"code","8498b2be":"code","4351b04c":"code","c9a8201a":"code","e6294771":"code","6f6e104e":"code","2fc90f64":"code","739c0efa":"code","0a6950c9":"code","50c82f90":"code","4ee741f7":"code","ae59a47c":"code","7de42289":"code","398a7718":"code","f1e80298":"code","8c59076b":"code","b6aa29ae":"code","e7813df8":"code","6efb1439":"code","edb788f2":"code","2b7518e0":"code","ec87ea6b":"code","a9060252":"code","692fd28d":"code","bcce2748":"code","474ba693":"code","909a7b5c":"code","eb3c5404":"code","070340cb":"code","04df9f06":"code","ff26a0e4":"code","78df76e1":"code","344228c6":"code","9e39ac82":"code","289eb402":"code","767236d3":"code","c7ab295e":"code","83c785c3":"code","9be4b582":"code","aa91d2cb":"markdown","0155e730":"markdown","be4d1231":"markdown","f1806904":"markdown","b66cf4d1":"markdown","d1a07b06":"markdown","b3da2644":"markdown","8c1fb69b":"markdown","7fd4c6a5":"markdown","b3463d77":"markdown","4b626e20":"markdown","65f0e393":"markdown"},"source":{"26b82aa1":"#import important libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom torch import nn, optim\nfrom torch.autograd import Variable\nimport json\nimport os\nimport torch\nimport torch.nn.functional as F\nfrom torchvision import datasets, models, transforms\nfrom torch.utils.data import DataLoader\nfrom collections import OrderedDict\nfrom PIL import Image\nfrom torch import Tensor\nfrom importlib import reload\n#reload(helper)\nfrom glob import glob","81fa0606":"#Make helper utility","22ad8910":"def test_network(net, trainloader):\n\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(net.parameters(), lr=0.001)\n\n    dataiter = iter(trainloader)\n    images, labels = dataiter.next()\n\n    # Create Variables for the inputs and targets\n    inputs = Variable(images)\n    targets = Variable(images)\n\n    # Clear the gradients from all Variables\n    optimizer.zero_grad()\n\n    # Forward pass, then backward pass, then update weights\n    output = net.forward(inputs)\n    loss = criterion(output, targets)\n    loss.backward()\n    optimizer.step()\n\n    return True","cabf4497":"def imshow(image, ax=None, title=None, normalize=True):\n    \"\"\"Imshow for Tensor.\"\"\"\n    if ax is None:\n        fig, ax = plt.subplots()\n    image = image.numpy().transpose((1, 2, 0))\n\n    if normalize:\n        mean = np.array([0.485, 0.456, 0.406])\n        std = np.array([0.229, 0.224, 0.225])\n        image = std * image + mean\n        image = np.clip(image, 0, 1)\n\n    ax.imshow(image)\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.tick_params(axis='both', length=0)\n    ax.set_xticklabels('')\n    ax.set_yticklabels('')\n\n    return ax","d9d99696":"def view_recon(img, recon):\n    ''' Function for displaying an image (as a PyTorch Tensor) and its\n        reconstruction also a PyTorch Tensor\n    '''\n\n    fig, axes = plt.subplots(ncols=2, sharex=True, sharey=True)\n    axes[0].imshow(img.numpy().squeeze())\n    axes[1].imshow(recon.data.numpy().squeeze())\n    for ax in axes:\n        ax.axis('off')\n        ax.set_adjustable('box-forced')","a67b6125":"def view_classify(img, ps, version=\"MNIST\"):\n    ''' Function for viewing an image and it's predicted classes.\n    '''\n    ps = ps.data.numpy().squeeze()\n\n    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n    ax1.axis('off')\n    ax2.barh(np.arange(10), ps)\n    ax2.set_aspect(0.1)\n    ax2.set_yticks(np.arange(10))\n    if version == \"MNIST\":\n        ax2.set_yticklabels(np.arange(10))\n    elif version == \"Fashion\":\n        ax2.set_yticklabels(['T-shirt\/top',\n                            'Trouser',\n                            'Pullover',\n                            'Dress',\n                            'Coat',\n                            'Sandal',\n                            'Shirt',\n                            'Sneaker',\n                            'Bag',\n                            'Ankle Boot'], size='small');\n    ax2.set_title('Class Probability')\n    ax2.set_xlim(0, 1.1)\n\n    plt.tight_layout()","c18bf28b":"#Load and preprocess dataset for training and validation","8498b2be":"data_dir='..\/input\/flower_data\/flower_data' \ntrain_dir = data_dir + '\/train'\nvalid_dir = data_dir + '\/valid'\ntest_dir='..\/input\/test set'","4351b04c":"# Define transforms for the training, validation, and testing sets\ntrain_transforms = transforms.Compose([\n    transforms.RandomRotation(30),\n    transforms.RandomResizedCrop(size=224),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomVerticalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\nvalidation_transforms = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n# Load the datasets with ImageFolder\ntrain_dataset = datasets.ImageFolder(train_dir, transform=train_transforms)\nvalidation_dataset = datasets.ImageFolder(valid_dir, transform=validation_transforms)\ntest_dataset = datasets.ImageFolder(test_dir, transform=validation_transforms)\n\n# Using the image datasets and the transforms, define the dataloaders\ntrain_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=64, num_workers=4)\nvalid_dataloader = DataLoader(validation_dataset, shuffle=True, batch_size=64, num_workers=4)\ntest_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=16, num_workers=4)","c9a8201a":"len(test_dataset)","e6294771":"#Visualizing few images\nimage, label = next(iter(train_dataloader))\nimshow(image[10])\n\n#Visualizing few images\nimage, label = next(iter(test_dataloader))\nimshow(image[10])","6f6e104e":"#Label mapping \ntrain_class_names = train_dataset.classes\nvalid_class_names=validation_dataset.classes\n","2fc90f64":"import json\nwith open('..\/input\/cat_to_name.json', 'r') as f: \n    cat_to_name = json.load(f)\ncat_to_name['102']","739c0efa":"category_map = sorted(cat_to_name.items(), key=lambda x: int(x[0]))\n\ncategory_names = [cat[1] for cat in category_map]","0a6950c9":"# changing categories to their actual names \nfor i in range(0,len(train_class_names)):\n    train_class_names[i] = cat_to_name.get(train_class_names[i])\n    \nfor i in range(0,len(valid_class_names)):\n    valid_class_names[i] = cat_to_name.get(valid_class_names[i])","50c82f90":"#Building and training the classifier\n#Transfer learning::Rather than building a model and training from scratch,\n#we can leverage a pretrained model, and adjust the classifier (the last part of it) as needed to fit our needs. This saves a huge amount of time and effort. We using vgg model.","4ee741f7":"vgg16 = models.vgg16(pretrained=True)","ae59a47c":"vgg16","7de42289":"#Training:: We want to train the final layers of the model. \n#The following functions will run forward and backward propogation with pytorch against the training set, and then test against the validation set.","398a7718":"# Use GPU if it's available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","f1e80298":"# Freeze parameters so we don't backprop through them\nfor param in vgg16.parameters():\n    param.requires_grad = False\n\nclassifier = nn.Sequential(OrderedDict([\n                          ('fc1', nn.Linear(25088, 4096)),\n                          ('relu', nn.ReLU()),\n                          ('dropout', nn.Dropout(p=0.5)),\n                          ('fc2', nn.Linear(4096, 102)),\n                          ('output', nn.LogSoftmax(dim=1))\n                          ]))\n    \nvgg16.classifier = classifier\nvgg16.class_idx_mapping = train_dataset.class_to_idx","8c59076b":"criterion = nn.NLLLoss()\noptimizer = optim.Adam(vgg16.classifier.parameters(), lr=0.0001)","b6aa29ae":"def validation(model, testloader, criterion, device):\n    test_loss = 0\n    accuracy = 0\n    model.to(device)\n    for images, labels in testloader:\n        images, labels = images.to(device), labels.to(device)\n        # images.resize_(images.shape[0], 3, 224, 224)\n\n        output = model.forward(images)\n        test_loss += criterion(output, labels).item()\n\n        ps = torch.exp(output)\n        equality = (labels.data == ps.max(dim=1)[1])\n        accuracy += equality.type(torch.FloatTensor).mean()\n    \n    return test_loss, accuracy","e7813df8":"def train(model, trainloader, validloader, epochs, print_every, criterion, optimizer, device='cuda'): #cuda in kernel\n    steps = 0\n    \n    # Change to train mode if not already\n    model.train()\n    # change to cuda\n    model.to(device)\n\n    for e in range(epochs):\n        running_loss = 0\n\n        for (images, labels) in trainloader:\n            steps += 1\n\n            images, labels = images.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n\n            # Forward and backward passes\n            outputs = model.forward(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n\n            if steps % print_every == 0:\n                \n                # Make sure network is in eval mode for inference\n                model.eval()\n\n                # Turn off gradients for validation, saves memory and computations\n                with torch.no_grad():\n                    validation_loss, accuracy = validation(model, validloader, criterion, device)\n\n                print(\"Epoch: {}\/{}.. \".format(e+1, epochs),\n                      \"Training Loss: {:.3f}.. \".format(running_loss\/print_every),\n                      \"Validation Loss: {:.3f}.. \".format(validation_loss\/len(validloader)),\n                      \"Validation Accuracy: {:.3f}\".format((accuracy\/len(validloader))*100))\n\n                model.train()\n                \n                running_loss = 0","6efb1439":"#Testing the network::\n#train(model=vgg16, \n  ##      trainloader=train_dataloader, \n    ##    validloader=valid_dataloader,\n      #  epochs=3, \n     #   print_every=20, \n      #  criterion=criterion,\n       # optimizer=optimizer,\n       # device=\"cuda\") # cuda in kernel\n","edb788f2":"#Testing the network::\ntrain(model=vgg16, \n        trainloader=train_dataloader, \n        validloader=valid_dataloader,\n        epochs=9, \n        print_every=40, \n        criterion=criterion,\n        optimizer=optimizer,\n        device=\"cuda\") # cuda in kernel","2b7518e0":"#Testing on test set  can not be done as no test labels given.We can go for predicting class\ndef check_accuracy_on_test(testloader, model):    \n    correct = 0\n    total = 0\n    model.to(device)\n    with torch.no_grad():\n        for data in testloader:\n            images, labels = data\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            #print(labels)\n            #print(outputs)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            print(total)\n            #print(predicted)\n            correct += (predicted == labels).sum().item()\n            print(correct)\n    #print(correct)\n\n    return  correct \/ total","ec87ea6b":"#Save the check point\nvgg16.class_idx_mapping = train_dataset.class_to_idx\nvgg16.class_idx_mapping","a9060252":"def save_checkpoint(state, filename='checkpoint.pth'):\n    torch.save(state, filename)","692fd28d":"save_checkpoint({\n            'epoch': 9,\n            'classifier': vgg16.classifier,\n            'state_dict': vgg16.state_dict(),\n            'optimizer' : optimizer.state_dict(),\n            'class_idx_mapping': vgg16.class_idx_mapping,\n            'arch': \"vgg16\"\n            })","bcce2748":"#Loading check point\ndef load_model(model_checkpoint):\n    \"\"\"\n    Loads the model from a checkpoint file.\n\n    Arguments: \n        model_checkpoint: Path to checkpoint file\n    \n    Returns: \n        model: Loaded model.\n        idx_class_mapping: Index to class mapping for further evaluation.\n    \"\"\"\n\n    checkpoint = torch.load(model_checkpoint)\n    \n    model = models.vgg16(pretrained=True)\n    \n    for param in model.parameters():\n        param.requires_grad = False\n\n    model.classifier = checkpoint[\"classifier\"]\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    \n    return model","474ba693":"model = load_model(model_checkpoint=\"checkpoint.pth\")","909a7b5c":"#Class inference\n\nimg = Image.open(\"..\/input\/flower_data\/flower_data\/train\/61\/image_06281.jpg\")\nprint(\"Original image with size: {}\".format(img.size))\nplt.imshow(img)","eb3c5404":"def process_image(img_path):\n    ''' Scales, crops, and normalizes a PIL image for a PyTorch model,\n        returns an Numpy array\n    '''\n    img = Image.open(img_path)\n    w, h = img.size\n    if w<h:\n        size = 256, 999999999\n    else:\n        size = 999999999, 256\n\n    img.thumbnail(size=size)\n    \n    w, h = img.size\n    left = (w - 224) \/ 2\n    right = (w + 224) \/ 2\n    top = (h - 224) \/ 2\n    bottom = (h + 224) \/ 2\n    \n    img = img.crop((left, top, right, bottom))\n    \n    # Convert to numpy array\n    np_img = np.array(img)\/255\n    \n    # Normalize\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    np_img = (np_img - mean) \/ std\n    \n    np_img = np_img.transpose(2, 0, 1)\n    \n    return np_img","070340cb":"img = process_image(\"..\/input\/flower_data\/flower_data\/train\/61\/image_06281.jpg\")\nprint(\"After resizing, cropping and normalizing, size: {}\".format(img.shape))\n","04df9f06":"#Creating Dataframe to get test images names and its corresponding lablel names","ff26a0e4":"import pandas as pd","78df76e1":"#Class prediction\nclass_idx_mapping = train_dataset.class_to_idx\nidx_class_mapping = {v: k for k, v in class_idx_mapping.items()}\nidx_class_mapping[1]","344228c6":"#Prediction Method\ndef predict(test_loader, model_checkpoint, topk=1, device=\"cpu\", idx_class_mapping=idx_class_mapping):\n    # Build the model from the checkpoint\n    model = load_model(model_checkpoint)\n    # No need for GPU\n    model.to(device)\n    \n    model.eval()\n    \n    labels=[]\n    \n    with torch.no_grad():\n        for images, _ in test_dataloader:\n            images = images.to(device)\n            output = model(images)\n            ps = torch.exp(output)\n            top_p, top_class = ps.topk(1, dim=1) \n            for i in top_class:\n                #print(idx_class_mapping[i.item()])\n                labels.append(idx_class_mapping[i.item()] )\n                #print(i.item())\n                      \n               \n    return labels","9e39ac82":"class_label=predict(test_dataloader,\"checkpoint.pth\", idx_class_mapping=idx_class_mapping)","289eb402":"print(len(class_label))","767236d3":"class_label[0]","c7ab295e":"#Methods to get list of Images names in test floder::\nimage_col=[]\nfor img_filename in os.listdir('..\/input\/test set\/test set'):\n    image_col.append(img_filename)\nprint(len(image_col))","83c785c3":"category_map = sorted(cat_to_name.items(), key=lambda x: int(x[0]))\nplant_name=[]\nfor label in class_label:\n    name=cat_to_name[label]\n    #print(name)\n    plant_name.append(name)\n    #plant_name+=category_map[int(label)][1]\n    #print(plant_name)\nprint(len(plant_name))\n#print(category_map[int(class_label[3])][1] )","9be4b582":"flower_dataframe = pd.DataFrame({'image_name': image_col, 'class_label': class_label,'plant_name': plant_name})\nflower_dataframe.sort_values('image_name')","aa91d2cb":"This project is about classification of #102 images .We will build an image classifier to recognize different piecesof flowers.","0155e730":"****Hackathon Blossom (Flower Classification)\nThis data set contains images of flowers from 102 different species.****","be4d1231":"**Algoritm Optimization:: We iincreased model accuracy by changing parameters like epoc.We changed epoc from 3 to 9.Accurancy increased from 84 to 94.**","f1806904":"#Description of Dataset\nHere we have two groups :\n1. input\/flower_data\/train\n2. input\/flower_data\/valid\n3. input\/test set\/test set","b66cf4d1":"For creating Dataframe with image names and their corresponding labels like 1,2..,102 etc::\n1. Making prediction method to predict labels for each test images.\n2. Make list to get names of all test images in test set folder\n3. I will append images name and prediction in columns named 'image_name' and 'class_label' in flower_dataframe.","d1a07b06":"****Then we pre-process the dataset, using pytorch, rotate,resize , normailze etc method****s.","b3da2644":"**My work approach::** I used pytorch.imageloader becuase it directly loads dataset from given path like\ninput\/flower_data\/train or input\/flower_data\/valid. Earlier i tried to load it using numpy\/pandas,\nthen i need to write code for making train and valid set, each having images of 102 categories with proper labeling.\nLike.. 1 folder will have all images inside 1 floder and label as 1, which we will replace with name(help from json cat_name).\nBut, using pytorch, it was easy to do so with only few lines of code.","8c1fb69b":"**Within each train\/valid floder, we have separate folder for each of the 102 flower classes.**","7fd4c6a5":"Epoch: 1\/3..  Training Loss: 4.139..  Validation Loss: 3.158..  Validation Accuracy: 35.654\nEpoch: 1\/3..  Training Loss: 3.112..  Validation Loss: 2.168..  Validation Accuracy: 51.303\nEpoch: 1\/3..  Training Loss: 2.437..  Validation Loss: 1.571..  Validation Accuracy: 64.298\nEpoch: 1\/3..  Training Loss: 2.005..  Validation Loss: 1.186..  Validation Accuracy: 71.231\nEpoch: 1\/3..  Training Loss: 1.734..  Validation Loss: 0.963..  Validation Accuracy: 77.962\nEpoch: 2\/3..  Training Loss: 1.192..  Validation Loss: 0.815..  Validation Accuracy: 82.577\nEpoch: 2\/3..  Training Loss: 1.372..  Validation Loss: 0.743..  Validation Accuracy: 81.803\nEpoch: 2\/3..  Training Loss: 1.272..  Validation Loss: 0.702..  Validation Accuracy: 82.909\nEpoch: 2\/3..  Training Loss: 1.221..  Validation Loss: 0.726..  Validation Accuracy: 81.995\nEpoch: 2\/3..  Training Loss: 1.124..  Validation Loss: 0.566..  Validation Accuracy: 85.976\nEpoch: 3\/3..  Training Loss: 0.754..  Validation Loss: 0.553..  Validation Accuracy: 86.197\nEpoch: 3\/3..  Training Loss: 1.013..  Validation Loss: 0.531..  Validation Accuracy: 86.490\nEpoch: 3\/3..  Training Loss: 1.010..  Validation Loss: 0.575..  Validation Accuracy: 84.587\nEpoch: 3\/3..  Training Loss: 0.921..  Validation Loss: 0.489..  Validation Accuracy: 88.447\nEpoch: 3\/3..  Training Loss: 0.936..  Validation Loss: 0.537..  Validation Accuracy: 85.620","b3463d77":"Epoch: 1\/9..  Training Loss: 0.830..  Validation Loss: 0.440..  Validation Accuracy: 89.856\nEpoch: 1\/9..  Training Loss: 0.796..  Validation Loss: 0.441..  Validation Accuracy: 88.413\nEpoch: 2\/9..  Training Loss: 0.308..  Validation Loss: 0.409..  Validation Accuracy: 90.096\nEpoch: 2\/9..  Training Loss: 0.761..  Validation Loss: 0.405..  Validation Accuracy: 90.115\nEpoch: 2\/9..  Training Loss: 0.736..  Validation Loss: 0.405..  Validation Accuracy: 89.736\nEpoch: 3\/9..  Training Loss: 0.606..  Validation Loss: 0.418..  Validation Accuracy: 90.197\nEpoch: 3\/9..  Training Loss: 0.724..  Validation Loss: 0.388..  Validation Accuracy: 89.856\nEpoch: 4\/9..  Training Loss: 0.167..  Validation Loss: 0.350..  Validation Accuracy: 91.385\nEpoch: 4\/9..  Training Loss: 0.659..  Validation Loss: 0.322..  Validation Accuracy: 91.264\nEpoch: 4\/9..  Training Loss: 0.661..  Validation Loss: 0.347..  Validation Accuracy: 91.486\nEpoch: 5\/9..  Training Loss: 0.426..  Validation Loss: 0.338..  Validation Accuracy: 91.024\nEpoch: 5\/9..  Training Loss: 0.589..  Validation Loss: 0.355..  Validation Accuracy: 90.630\nEpoch: 6\/9..  Training Loss: 0.073..  Validation Loss: 0.335..  Validation Accuracy: 91.111\nEpoch: 6\/9..  Training Loss: 0.567..  Validation Loss: 0.304..  Validation Accuracy: 92.327\nEpoch: 6\/9..  Training Loss: 0.604..  Validation Loss: 0.345..  Validation Accuracy: 91.212\nEpoch: 7\/9..  Training Loss: 0.286..  Validation Loss: 0.308..  Validation Accuracy: 93.375\nEpoch: 7\/9..  Training Loss: 0.565..  Validation Loss: 0.291..  Validation Accuracy: 93.014\nEpoch: 7\/9..  Training Loss: 0.542..  Validation Loss: 0.290..  Validation Accuracy: 92.808\nEpoch: 8\/9..  Training Loss: 0.533..  Validation Loss: 0.278..  Validation Accuracy: 92.841\nEpoch: 8\/9..  Training Loss: 0.504..  Validation Loss: 0.286..  Validation Accuracy: 93.082\nEpoch: 9\/9..  Training Loss: 0.205..  Validation Loss: 0.306..  Validation Accuracy: 92.567","4b626e20":"The project is broken down into multiple steps:\n1. Load and preprocess the image dataset\n2. Train the image classifier on your dataset\n3. Use the trained classifier to predict image content","65f0e393":"**Algorithm used for this image classification::**\nTransfer learning becuase making model from scratch would be very complex and computation expensive, would require lot of time.\nTransfer learning allows us to train deep networks using significantly less data then we would need if we had to train from scratch.\nIn recent years, a number of models have been created for reuse in computer vision problems. Using these pre-trained models is known as transfer learning\nThese pre-trained models allow others to quickly obtain cutting edge results in computer vision without needing the large amounts of compute power, time, and patience in finding the right training technique to optimize the weights.\nHere, i am using vggnet model, though we can use any other model."}}