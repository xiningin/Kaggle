{"cell_type":{"d5eb3a8e":"code","55dc65ab":"code","1b374954":"code","f24686b0":"code","1f9d65e6":"code","95f57858":"code","82670c1d":"code","a78f8514":"code","a3b57c8f":"code","4a7ea378":"code","a03ddbc2":"code","c5319096":"code","44b69562":"code","3cf9705b":"code","a91ebd40":"code","5126267d":"code","53a02a52":"code","a3d2afd3":"code","e634c86c":"code","3724be5c":"code","b5537c3d":"code","14c781fd":"code","f30bec9c":"code","d6c8ef3a":"code","e88dec60":"code","fcc659cf":"code","518c6249":"code","3812171d":"code","7bb7b07d":"code","a26fe91a":"code","38ced907":"code","c1e6e149":"markdown","26febb5f":"markdown","76bf75d2":"markdown","4e9ce217":"markdown","f566f0f7":"markdown","d42a9d17":"markdown","c816f0c3":"markdown","45c24666":"markdown","5570cccf":"markdown","f6576ac5":"markdown","1547f3ff":"markdown","c2fea9a8":"markdown","309cdb5f":"markdown"},"source":{"d5eb3a8e":"%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm   # tqdm is for printing the status bar\nfrom bs4 import BeautifulSoup\n\n# library for splitting the dataset\nfrom sklearn.model_selection import train_test_split\n\n# libraries for featurization  \nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import Normalizer\n\n# library for modeling \nfrom sklearn.naive_bayes import MultinomialNB\n\n#  library for hyperparameter tuning\nfrom sklearn.model_selection import GridSearchCV\n\n# evaluation of model \nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\n","55dc65ab":"data = pd.read_csv('..\/input\/amazon-fine-food-reviews\/Reviews.csv')\ndata.head()","1b374954":"data=data[data['Score']!=3]  #  we will not consider reviews with 'Score' 3, so we are droping all the rows with 'Score' feature equals 3 \ndata.shape","f24686b0":"# Give reviews with Score>3 will be considered as having positive rating, and reviews with a score<3 as negative rating.\ndef partition(x):  # given x it returns 1 if x>3 else returns 0\n    if x < 3:\n        return 0\n    return 1\n\n#changing reviews with score less than 3 to be negative(0) and greater the 3 to be positive(1) \nactual_score = data['Score']  \npositive_negative = actual_score.map(partition)  \ndata['Score'] = positive_negative\ndata.head()","1f9d65e6":"data['Score'].value_counts()","95f57858":"# code is taken form : https:\/\/matplotlib.org\/3.1.1\/gallery\/pie_and_polar_charts\/pie_features.html\n\nlabels = 'Positive Reviews', 'Negative Reviews'\nsizes = [443777, 82037]\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\nplt.show()","82670c1d":"# Sorting data according to ProductId in ascending order\nsorted_data = data.sort_values('ProductId', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\n\n# Droping Deduplication of entries\nfinal=sorted_data.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"}, keep='first', inplace=False)\nfinal.shape","a78f8514":"# code taken from  https:\/\/stackoverflow.com\/a\/47091490\/4084039\n\ndef decontracted(phrase):  # this function expands english language contraction such as (that's) to ('that is')\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\n\n# https:\/\/gist.github.com\/sebleier\/554280\n# we are removing the words from the stop words list: 'no', 'nor', 'not'\nstopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"])","a3b57c8f":"## Preprocessing 'Text' column\n# The Below code removes url's , html tags, words with numbers, special character, stopwords and decontracts words in the Text for each review   \n\npreprocessed_reviews = []    \nfor sentance in tqdm(final['Text'].values):  \n    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n    sentance = decontracted(sentance)\n    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n    # https:\/\/gist.github.com\/sebleier\/554280\n    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords)\n    preprocessed_reviews.append(sentance.strip())","4a7ea378":"# Preprocessing Summary column\n# The Below code removes url's , html tags, words with numbers, special character, stopwords and decontracts words in the Summary for each review    \n\npreprocessed_Summary = []\n# tqdm is for printing the status bar\nfor sentance in tqdm(final['Summary'].values):\n    sentance = re.sub(r\"http\\S+\", \"\", str(sentance))\n    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n    sentance = decontracted(sentance)\n    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n    # https:\/\/gist.github.com\/sebleier\/554280\n    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords)\n    preprocessed_Summary.append(sentance.strip())","a03ddbc2":"# let's replace the 'Summary' and 'Text' column with the preprocessed data.  \nfinal[\"Summary\"] = preprocessed_Summary\nfinal['Text'] = preprocessed_reviews\nfinal.drop(['Id', 'ProductId', 'UserId', 'ProfileName'], axis = 1, inplace=True)  # not considering these columns for classification.  ","c5319096":"final.reset_index(inplace=True)","44b69562":"final.drop(['index'], axis=1, inplace=True)\nfinal.head()  # this is our final dataset","3cf9705b":"# seperating the class column from the dataset\ny = final['Score'].values\nX = final.drop(['Score'], axis=1)\n\n# splitting the data and class labels in to train set and test set  \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, stratify=y)","a91ebd40":"print('train data shape', X_train.shape)\nprint('train data labels shape ', y_train.shape)\nprint('test data shape', X_test.shape)\nprint('test data labels shape', y_test.shape)","5126267d":"### Text feature \n# calling the CountVectorizer class with three parameters \nvectorizer = CountVectorizer(min_df=10, ngram_range=(1,4), max_features=5000)\nvectorizer.fit(X_train['Text'].values) # fitting the model with train data\n\n# we use the fitted CountVectorizer to convert the text to vector\nX_train_text = vectorizer.transform(X_train['Text'].values)\nX_test_text = vectorizer.transform(X_test['Text'].values)\n\n# getting the features \nfeatures_text = vectorizer.get_feature_names() # we will be using this afterwords \n\nprint(\"After vectorizations using BOW\")\nprint(X_train_text.shape, y_train.shape)\nprint(X_test_text.shape, y_test.shape)","53a02a52":"################# Summary feature ##################\n# calling the CountVectorizer class with three parameters \nvectorizer = CountVectorizer(min_df=10, ngram_range=(1,4), max_features=5000)\nvectorizer.fit(X_train['Summary'].values) # fitting the model with train data\n\n# we use the fitted CountVectorizer to convert the text to vector\nX_train_summary = vectorizer.transform(X_train['Summary'].values)\nX_test_summary = vectorizer.transform(X_test['Summary'].values)\n\n# getting the features of bow vectorization \nfeatures_summary = vectorizer.get_feature_names() # we will be using this afterwords \n\nprint(\"After vectorizations using BOW\")\nprint(X_train_summary.shape, y_train.shape)\nprint(X_test_summary.shape, y_test.shape)","a3d2afd3":"# \"HelpfulnessNumerator\" feature\nnormalizer = Normalizer() # normalizing numerical features such that all numerical features are in same range. \n\nnormalizer.fit(X_train['HelpfulnessNumerator'].values.reshape(-1,1))\n\nX_train_help_num_norm = normalizer.transform(X_train['HelpfulnessNumerator'].values.reshape(1,-1)).reshape(-1,1)\nX_test_help_num_norm = normalizer.transform(X_test['HelpfulnessNumerator'].values.reshape(1,-1)).reshape(-1,1)\n\n\nprint(\"After normalization of price feature\")\nprint(X_train_help_num_norm.shape, y_train.shape)\nprint(X_test_help_num_norm.shape, y_test.shape)\nprint(\"=\"*100)","e634c86c":"# \"HelpfulnessDenominator\" feature\nnormalizer = Normalizer()\n\nnormalizer.fit(X_train['HelpfulnessDenominator'].values.reshape(-1,1))\n\nX_train_help_den_norm = normalizer.transform(X_train['HelpfulnessDenominator'].values.reshape(1,-1)).reshape(-1,1)\nX_test_help_den_norm = normalizer.transform(X_test['HelpfulnessDenominator'].values.reshape(1,-1)).reshape(-1,1)\n\n\nprint(\"After normalization of price feature\")\nprint(X_train_help_den_norm.shape, y_train.shape)\nprint(X_test_help_den_norm.shape, y_test.shape)\nprint(\"=\"*100)","3724be5c":"# \"Time\" feature\nnormalizer = Normalizer()\n\nnormalizer.fit(X_train['Time'].values.reshape(-1,1))\n\nX_train_time_norm = normalizer.transform(X_train['Time'].values.reshape(1,-1)).reshape(-1,1)\nX_test_time_norm = normalizer.transform(X_test['Time'].values.reshape(1,-1)).reshape(-1,1)\n\n\nprint(\"After normalization of price feature\")\nprint(X_train_time_norm.shape, y_train.shape)\nprint(X_test_time_norm.shape, y_test.shape)\nprint(\"=\"*100)","b5537c3d":"# reference : merge two sparse matrices: https:\/\/stackoverflow.com\/a\/19710648\/4084039\nfrom scipy.sparse import hstack\n\nX_tr = hstack((X_train_text, X_train_summary, X_train_help_num_norm,  # train data after BOW representation for 'Text' and 'Summary' feature.\n                   X_train_help_den_norm, X_train_time_norm)).tocsr()\n\nX_te = hstack((X_test_text, X_test_summary, X_test_help_num_norm,  # test data after BOW representation for 'Text' and 'Summary' feature.\n                   X_test_help_den_norm, X_test_time_norm)).tocsr()\n\n\nprint(\"Final Data matrix with BOW representation for essay\")\nprint(X_tr.shape, y_train.shape)\nprint(X_te.shape, y_test.shape)\nprint(\"=\"*100)","14c781fd":"# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html\n\n# using grid search for finding the best alpha for laplace smoothing\/additive smoothing \nfrom sklearn.model_selection import GridSearchCV\n\nNB_classifier = MultinomialNB(class_prior=[0.5, 0.5]) \n \nparameters = {'alpha': [0.001, 0.05, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 1, 2, 3, 4, 5, 10, 20, 25, 30, 50, 70, 100]}  # various values of alhap's to choose from.\nclf = GridSearchCV(NB_classifier, parameters, cv=5, scoring='roc_auc', return_train_score=True)  # gridsearchCV with 5 fold cross validation .\nclf.fit(X_tr, y_train)\n\nresults = pd.DataFrame.from_dict(clf.cv_results_) \nresults = results.sort_values(['param_alpha'])\n","f30bec9c":"cv_auc = results['mean_test_score']      # mean test scores for every 'alpha'\ntrain_auc = results['mean_train_score']  # mean train scores for every 'alpha\n\nalpha =  list(results['param_alpha']) \nalpha=np.log(alpha)   # taking log of alphas so to make the plot more readable\n\nplt.plot(alpha, train_auc, label='Train AUC')\nplt.plot(alpha, cv_auc, label='CV AUC')\n\nplt.scatter(alpha, train_auc, label='Train AUC points')\nplt.scatter(alpha, cv_auc, label='CV AUC points')\n\n\nplt.legend()\nplt.xlabel(\"log(alpha): hyperparameters\")\nplt.ylabel(\"AUC\")\nplt.title(\"Hyper parameters Vs AUC plot\")\nplt.grid()\nplt.show()","d6c8ef3a":"clf.best_estimator_ # using this estimator lets predict the labels of test dataset","e88dec60":"NBclassifier = MultinomialNB(alpha=0.2, class_prior=[0.5, 0.5], fit_prior=True)\nNBclassifier.fit(X_tr, y_train)\n# roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n# not the predicted outputs\n\ny_train_pred = NBclassifier.predict_proba(X_tr)[:,1]     # predicted probabilities of train datapoints belonging to positive class\ny_test_pred = NBclassifier.predict_proba(X_te)[:,1]      # predicted probabilities of test datapoints belonging to positive class\n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)   # fpr and tpr for train data\ntest_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)       # fpr and tpr for test data\n\nplt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(auc(train_fpr, train_tpr)))\nplt.plot(test_fpr, test_tpr, label=\"test AUC =\"+str(auc(test_fpr, test_tpr)))\nplt.legend()\nplt.xlabel(\"fpr\")\nplt.ylabel(\"tpr\")\nplt.title(\"ROC curve\")\nplt.grid()\nplt.show()","fcc659cf":"# reference : Applied AI course lectures\n# we are writing our own function for predict, with defined thresould\n# we will pick a threshold that will give the least fpr\ndef find_best_threshold(threshould, fpr, tpr):\n    t = threshould[np.argmax(tpr*(1-fpr))]\n    # (tpr*(1-fpr)) will be maximum if your fpr is very low and tpr is very high\n    print(\"the maximum value of tpr*(1-fpr)\", max(tpr*(1-fpr)), \"for threshold\", np.round(t,3))\n    return t\n\ndef predict_with_best_t(proba, threshould):\n    predictions = []\n    for i in proba:\n        if i>=threshould:\n            predictions.append(1)\n        else:\n            predictions.append(0)\n    return predictions","518c6249":"from sklearn.metrics import confusion_matrix\n\nbest_t = find_best_threshold(tr_thresholds, train_fpr, train_tpr)  # getting the best threshold for separating the positive classes form negative \n\ntest_confusion_matrix = confusion_matrix(y_test, predict_with_best_t(y_test_pred, best_t))  # calculates the confusion matrix\n","3812171d":"# below code is taken from  https:\/\/medium.com\/@dtuk81\/confusion-matrix-visualization-fc31e3f30fea\n\ngroup_names = ['True Neg','False Pos','False Neg','True Pos']\n\ngroup_counts = ['{0:0.0f}'.format(value) for value in\n                test_confusion_matrix.flatten()]\n\nlabels = [f'{v1}\\n{v2}' for v1, v2 in\n          zip(group_names,group_counts)]\nlabels = np.asarray(labels).reshape(2,2)\n\nprint(\"Test confusion matrix\")\nsns.heatmap(test_confusion_matrix, annot=labels, fmt='', cmap='Oranges',cbar=False, xticklabels=['Prediction:Negative', 'Prediction:Positive'], yticklabels=['Actal:Negative', 'Actual:Positive'])","7bb7b07d":"list_of_features = features_text + features_summary + ['HelpfulnessNumerator', 'HelpfulnessDenominator', 'Time']","a26fe91a":"# Top 20 features form negative class\nfeatures = np.argsort(NBclassifier.feature_log_prob_[0]) # sorting the features log probability for negtive class\n                                                             #  form low probability to high probability and getting its indice \n\nfeatures = features[::-1]  # reversing it form high probability to low probability indice\n\nfor i in features[:20]:  # printing top 20 features from negative calss\n    print(list_of_features[i])","38ced907":"# Top 20 features form negative class\nfeatures = np.argsort(NBclassifier.feature_log_prob_[1]) # sorting the features log probability for negtive class\n                                                             #  form low probability to high probability and getting its indice \n\nfeatures = features[::-1]  # reversing it form high probability to low probability indice\n\nfor i in features[:20]:  # printing top 20 features from negative calss\n    print(list_of_features[i])","c1e6e149":"### Top 20 features for classifying reviews as negative","26febb5f":"#### Summary and Text are the two text features in the data\n","76bf75d2":"Data Source: https:\/\/www.kaggle.com\/snap\/amazon-fine-food-reviews <br>\n\nThe Amazon Fine Food Reviews dataset consists of reviews of fine foods from Amazon.<br>\n\nNumber of reviews: 568,454<br>\nNumber of users: 256,059<br>\nNumber of products: 74,258<br>\nTimespan: Oct 1999 - Oct 2012<br>\nNumber of Attributes\/Columns in data: 10 \n\nAttribute Information:\n\n1. Id\n2. ProductId - unique identifier for the product\n3. UserId - unqiue identifier for the user\n4. ProfileName\n5. HelpfulnessNumerator - number of users who found the review helpful\n6. HelpfulnessDenominator - number of users who indicated whether they found the review helpful or not\n7. Score - rating between 1 and 5\n8. Time - timestamp for the review\n9. Summary - brief summary of the review\n10. Text - text of the review\n","4e9ce217":"In Preprocessing we do the following in the order below:-\n\n1. Begin by removing the html tags\n2. Remove any punctuations or limited set of special characters like , or . or # etc.\n3. Check if the word is made up of english letters and is not alpha-numeric\n4. Check to see if the length of the word is greater than 2 (as it was researched that there is no adjective in 2-letters)\n5. Convert the word to lowercase\n6. Remove Stopwords\n7. Finally Snowball Stemming the word (it was obsereved to be better than Porter Stemming)<br>\n\nAfter which we collect the words used to describe positive and negative reviews","f566f0f7":"### <font color='#339933'> Conclustion : \n<ul> Using BOW featurization on 'Text' and 'Summary' features and using Multinomial naive bayes model we got a auc score of 0.96 for testdata and from the confusion matrix we can say that model is doing fairly well on classifying positive reviews from negative reviews but here the model is also predicting many positive reviews as negative which may be due to the class imbalance.<\/ul>","d42a9d17":"## Training the Multinomial naive bayes Model\nUsing multinomial naive bayes classifier for classification.","c816f0c3":"### Encoding nemurical features","45c24666":"### Top 20 features for classifying reviews as positive\n","5570cccf":"# Text preprocessing","f6576ac5":"\n### <font color='#339933'> Objective:\nGiven a review, determine whether the review is positive (Rating of 4 or 5) or negative (rating of 1 or 2).","1547f3ff":"# Amazon Fine Food Reviews Analysis","c2fea9a8":"## Encoding Text and Summary features using BOW (bag of words)","309cdb5f":"we see that the data is imbalanced with positive(1) dominating over negative class(0)"}}