{"cell_type":{"40679954":"code","62b73253":"code","066b3fa3":"code","cbd46efd":"code","48bbaf5b":"code","03fef2eb":"code","1d084206":"code","81d2039c":"code","8b381c08":"code","e1ce0da9":"code","4454146f":"code","3198cafb":"code","7770e608":"markdown","dddd351d":"markdown"},"source":{"40679954":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","62b73253":"df1=pd.read_csv(\"\/kaggle\/input\/student-performance-data-set-competition-form\/X_test.csv\")\ndf3=pd.read_csv(\"\/kaggle\/input\/student-performance-data-set-competition-form\/test_label\/y_test.csv\")\ndf2=pd.read_csv(\"\/kaggle\/input\/student-performance-data-set-competition-form\/X_train.csv\")\ndf4=pd.read_csv(\"\/kaggle\/input\/student-performance-data-set-competition-form\/y_train.csv\")\n\n# concatening table to clean the entire data instead of having to loop through each training and testing dataset\n\ndata_X=pd.concat([df1,df2])\ndata_Y=pd.concat([df3,df4])\n\ndata = pd.merge(data_X, data_Y, on = 'StudentID')\n\n# Number of missing values\/infinite values in column of data\nmissing_val_count_by_column = (data.isnull().sum())\ninfinite_val_count_by_column=(data.isin([np.inf, -np.inf]).values.sum())\nprint(missing_val_count_by_column[missing_val_count_by_column > 0], infinite_val_count_by_column[infinite_val_count_by_column> 0])\n\n# Get list of categorical variables\ncat=[col for col in data.columns if data[col].dtype == 'object']\n\nprint(\"Categorical variables:\")\nprint(cat)\n\n#find unique values for each categories\nfor x in cat:\n    print(x,':', data[x].unique())\n\n#Get list of numerical variables\nnum=[col for col in data.columns if data[col].dtype != 'object']\nprint(\"Numerical variables:\")\nprint(num)\n\n\n","066b3fa3":"#ordinal encoding because one-hot encoding will create too many entries since the greatest number of nominal is 5. \nfrom sklearn.preprocessing import OrdinalEncoder\n# Make copy to avoid changing original data \nnew_data = data.copy()\n\n\n# Apply ordinal encoder to each column with categorical data\nordinal_encoder = OrdinalEncoder()\nnew_data[cat] = ordinal_encoder.fit_transform(data[cat])\n\n","cbd46efd":"#find correlation between all the features\nimport matplotlib.pyplot as plt\ncorr = new_data.corr()\nfig = plt.figure()\nfig.set_size_inches(18.5, 10.5)\nax = fig.add_subplot(111)\ncax = ax.matshow(corr,cmap='coolwarm', vmin=-1, vmax=1)\nfig.colorbar(cax)\nticks = np.arange(0,len(new_data.columns),1)\nax.set_xticks(ticks)\nplt.xticks(rotation=90)\nax.set_yticks(ticks)\nax.set_xticklabels(new_data.columns)\nax.set_yticklabels(new_data.columns)\n\nplt.show()\n\n","48bbaf5b":"student_features=['school', 'sex', 'age', 'address', 'famsize',\n       'Pstatus', 'Medu', 'Fedu','Mjob','Fjob','reason', 'guardian',\n       'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup',\n       'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic',\n       'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health',\n       'absences', 'G1', 'G2']\n\nfeatures=data.columns.values #retrieve all features in dataframe\nfeatures\n\nX=new_data[student_features] #exclude student id and G3\nY=new_data.G3 #set target to G3\n\n","03fef2eb":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\n#train_test_split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test,Y_train, y_test = train_test_split(X, Y, random_state = 0)\n\ndef get_mae (n_node,X_train,Y_train,X_test,y_test):\n    model=DecisionTreeRegressor(max_leaf_nodes=n_node, random_state=0)\n    model.fit(X_train,Y_train)\n    pred_val=model.predict(X_test)\n    mae= mean_absolute_error(y_test,pred_val)\n    return mae\n\nleaf=np.arange(2,20) #number of leaf\nscore ={ x: get_mae(x,X_train,Y_train,X_test,y_test) for x in leaf}\n\nprint(score)\n","1d084206":"from sklearn.ensemble import RandomForestRegressor\nforest_model=RandomForestRegressor(random_state=0)\nforest_model.fit(X_train,Y_train)\npred_forest=forest_model.predict(X_test)\nmae_rf=mean_absolute_error(pred_forest, y_test)\n\n#RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                     # max_depth=None, max_features='auto', max_leaf_nodes=None,\n                     # max_samples=None, min_impurity_decrease=0.0,\n                     # min_impurity_split=None, min_samples_leaf=1,\n                     # min_samples_split=2, min_weight_fraction_leaf=0.0,\n                     # n_estimators=100, n_jobs=None, oob_score=False,\n                     # random_state=None, verbose=0, warm_start=False)","81d2039c":"#the random forrest reveals a lower MAE comparable to 7 leaf of the decision tree.\nimport matplotlib.pyplot as plt\nmyList = score.items()\nmyList = sorted(myList)\nx, y = zip(*myList)\nplt.plot(x,y, label='Decision Tree')\nplt.axhline(y=mae_rf, color='r', linestyle='-', label='Random Forrest') #value of random forrest regressor\nplt.xlabel('Num of leaf')\nplt.ylabel('MAE (Mean absolute error)')\nplt.legend()\n","8b381c08":"#The difference between random forrest and decision tree is that random forrest picks the average over all the possible permutation of the decision tree to return the best mean absolute score.\n#The next step is to retrieve the tree path that the random forrest had selected to check the features that gave the best prediction. \n\n\n# On average the max_depth of the random forrest regressor is more than 7 leaves. \nmax_depth=[estimator.tree_.max_depth for estimator in forest_model.estimators_]\nprint(np.mean(max_depth))\n\n","e1ce0da9":"#random forrest model on test data with depth 7\nforest_model=RandomForestRegressor(max_depth=7,random_state=0)\nforest_model.fit(X_train,Y_train)\npred_forest=forest_model.predict(X_test)\nmae_rf=mean_absolute_error(pred_forest, y_test)\nprint(mae_rf)\n\n#visualisation of path split for maxdepth=7\nfrom sklearn import tree\nfig = plt.figure()\nfig.set_size_inches(18.5, 10.5)\nax=tree.plot_tree(forest_model.estimators_[0], feature_names=X.columns, filled=True)","4454146f":"#permutation importance test: randomly shuffle the rows in each column and see which features strongly affects the target. \nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(forest_model, random_state=0).fit(X_train, Y_train)\neli5.show_weights(perm, feature_names = X_train.columns.tolist())\n\n# G2 strongly affects the target. It seems if a student doesnt do well in G2, naturally doesnt do well in G3. \n#the rest of the features has no importance to the target. \n","3198cafb":"#SHAP Values (an acronym from SHapley Additive exPlanations) break down a prediction to show the impact of each feature.\n\nimport shap  # package used to calculate Shap values\n\n# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(forest_model)\n\nshap_values = explainer.shap_values(X_train)\n\n#print(shap_values.flatten())\nshap.summary_plot(shap_values, X_train)","7770e608":"We expect student ID is assigned randomly and so does not correlate with any features. \nIntersting correlation:\n1. Parents education correlates with parents job. \n1. And alcoholic consumption correlates with health\n1. Past failures has a negative correlation to G1, G2, G3. One must wonder, repeatition of class actually works? Professionally as an ex algebra teacher I do not think so especially for math class. \nThere is also an indication that an educated parent will provide education resources:\nextra classes, extra activities, nursery, internet, and student are more inclined to go into higher education\n\nAt the end of the notebook, I identify the important features in making a random forrest regressor prediction. \nThis will allow us to reduce uneccesary features, (from eyeballing) such as student ID, family relationship\/size, parent's cohabitation status in our random forest analysis. ","dddd351d":"The following is a list of attributes provided from the main source\nAttributes for both student-mat.csv (Math course) and student-por.csv (Portuguese language course) datasets:\n1. 1 school - student's school (binary: 'GP' - Gabriel Pereira or 'MS' - Mousinho da Silveira)\n1. 2 sex - student's sex (binary: 'F' - female or 'M' - male)\n1. 3 age - student's age (numeric: from 15 to 22)\n1. 4 address - student's home address type (binary: 'U' - urban or 'R' - rural)\n1. 5 famsize - family size (binary: 'LE3' - less or equal to 3 or 'GT3' - greater than 3)\n1. 6 Pstatus - parent's cohabitation status (binary: 'T' - living together or 'A' - apart)\n1. 7 Medu - mother's education (numeric: 0 - none, 1 - primary education (4th grade), 2 \u00e2\u20ac\u201c 5th to 9th grade, 3 \u00e2\u20ac\u201c secondary education or 4 \u00e2\u20ac\u201c higher education)\n1. 8 Fedu - father's education (numeric: 0 - none, 1 - primary education (4th grade), 2 \u00e2\u20ac\u201c 5th to 9th grade, 3 \u00e2\u20ac\u201c secondary education or 4 \u00e2\u20ac\u201c higher education)\n1. 9 Mjob - mother's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'athome' or 'other') 10 Fjob - father's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'athome' or 'other')\n1. 11 reason - reason to choose this school (nominal: close to 'home', school 'reputation', 'course' preference or 'other')\n1. 12 guardian - student's guardian (nominal: 'mother', 'father' or 'other')\n1. 13 traveltime - home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)\n1. 14 studytime - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)\n1. 15 failures - number of past class failures (numeric: n if 1<=n<3, else 4)\n1. 16 schoolsup - extra educational support (binary: yes or no)\n1. 17 famsup - family educational support (binary: yes or no)\n1. 18 paid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)\n1. 19 activities - extra-curricular activities (binary: yes or no)\n1. 20 nursery - attended nursery school (binary: yes or no)\n1. 21 higher - wants to take higher education (binary: yes or no)\n1. 22 internet - Internet access at home (binary: yes or no)\n1. 23 romantic - with a romantic relationship (binary: yes or no)\n1. 24 famrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)\n1. 25 freetime - free time after school (numeric: from 1 - very low to 5 - very high)\n1. 26 goout - going out with friends (numeric: from 1 - very low to 5 - very high)\n1. 27 Dalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)\n1. 28 Walc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)\n1. 29 health - current health status (numeric: from 1 - very bad to 5 - very good)\n1. 30 absences - number of school absences (numeric: from 0 to 93)\n\nthese grades are related with the course subject, Math or Portuguese:\n1. 31 G1 - first period grade (numeric: from 0 to 20)\n1. 31 G2 - second period grade (numeric: from 0 to 20)\n1. 32 G3 - final grade (numeric: from 0 to 20, output target)\n\n*Notice, the Fjob and Mjob nominal entry 'athome' is missing an underscore in comparison to the data previous inspection*\n\nI decided to not hardwire transform \n#Transforming binary and nominal to int 1,0\nX['sex']=X['sex'].map({'M':1,'F':0})\nX['school']=X['school'].map({'GP':1,'MS':0})\nX['address']=X['address'].map({'U':1,'R':0})\nX['famsize']=X['famsize'].map({'LE3':1,'GT3':0})\nX['Pstatus']=X['Pstatus'].map({'T':1,'A':0})\nX['guardian']=X['guardian'].map({'mother':1,'father':2,'other':3})\nX['schoolsup']=X['schoolsup'].map({'yes':1,'no':0})\nX['famsup']=X['famsup'].map({'yes':1,'no':0})\nX['paid']=X['paid'].map({'yes':1,'no':0})\nX['activities']=X['activities'].map({'yes':1,'no':0})\nX['nursery']=X['nursery'].map({'yes':1,'no':0})\nX['higher']=X['higher'].map({'yes':1,'no':0})\nX['internet']=X['internet'].map({'yes':1,'no':0})\nX['romantic']=X['romantic'].map({'yes':1,'no':0})\n\n#nominal data\nX['Mjob']=X['Mjob'].map({'teacher':1,'health':2,'services':3, 'at_home':4,'other':5})\nX['Fjob']=X['Fjob'].map({'teacher':1,'health':2,'services':3, 'at_home':4,'other':5})\nX['reason']=X['reason'].map({'home':1,'reputation':2,'course':3,'other':4})\n"}}