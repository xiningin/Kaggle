{"cell_type":{"e623f213":"code","9271c3e2":"code","ed3d29e9":"code","609a5d1c":"code","2d2af923":"code","d5d83bdb":"code","57b7541c":"code","38a589f0":"code","b7fe8882":"code","f2bcf50e":"code","6fcb8fd0":"code","6bb8c03a":"code","a4ffda03":"code","8cafe62e":"code","6688f969":"code","fc3c2022":"code","56482607":"code","3828cf36":"code","b73c8634":"code","2da95af5":"code","f66dbe16":"code","6c6f3037":"markdown","2c1498b4":"markdown","6fc40f14":"markdown","ca02156e":"markdown","9b20e388":"markdown","2bd49147":"markdown","9003649d":"markdown","d086f587":"markdown"},"source":{"e623f213":"from datetime import date, datetime, timedelta\nimport numpy as np\nimport pandas as pd\n\nconfirmed = pd.read_csv(\"..\/input\/jhucovid19\/csse_covid_19_data\/csse_covid_19_time_series\/time_series_covid19_confirmed_global.csv\")\ndeaths   = pd.read_csv(\"..\/input\/jhucovid19\/csse_covid_19_data\/csse_covid_19_time_series\/time_series_covid19_deaths_global.csv\")","9271c3e2":"launch_date = date(2020, 4, 9)\nlatest_train_date = date(2020, 4, 8)\n\npublic_leaderboard_start_date = launch_date - timedelta(7)\nclose_date = launch_date + timedelta(7)\nfinal_evaluation_start_date = launch_date + timedelta(8)","ed3d29e9":"confirmed.columns = list(confirmed.columns[:4]) + [datetime.strptime(d, \"%m\/%d\/%y\").date().strftime(\"%Y-%m-%d\") for d in confirmed.columns[4:]]\ndeaths.columns    = list(deaths.columns[:4])    + [datetime.strptime(d, \"%m\/%d\/%y\").date().strftime(\"%Y-%m-%d\") for d in deaths.columns[4:]]","609a5d1c":"# Filter out problematic data points (The West Bank and Gaza had a negative value, cruise ships were associated with Canada, etc.)\nremoved_states = \"Recovered|Grand Princess|Diamond Princess\"\nremoved_countries = \"US|The West Bank and Gaza\"\n\nconfirmed.rename(columns={\"Province\/State\": \"Province_State\", \"Country\/Region\": \"Country_Region\"}, inplace=True)\ndeaths.rename(columns={\"Province\/State\": \"Province_State\", \"Country\/Region\": \"Country_Region\"}, inplace=True)\nconfirmed = confirmed[~confirmed[\"Province_State\"].replace(np.nan, \"nan\").str.match(removed_states)]\ndeaths    = deaths[~deaths[\"Province_State\"].replace(np.nan, \"nan\").str.match(removed_states)]\nconfirmed = confirmed[~confirmed[\"Country_Region\"].replace(np.nan, \"nan\").str.match(removed_countries)]\ndeaths    = deaths[~deaths[\"Country_Region\"].replace(np.nan, \"nan\").str.match(removed_countries)]\n\nconfirmed.drop(columns=[\"Lat\", \"Long\"], inplace=True)\ndeaths.drop(columns=[\"Lat\", \"Long\"], inplace=True)","2d2af923":"confirmed","d5d83bdb":"deaths","57b7541c":"us_keys = pd.read_csv(\"..\/input\/jhucovid19\/csse_covid_19_data\/csse_covid_19_daily_reports\/04-01-2020.csv\")\nus_keys = us_keys[us_keys[\"Country_Region\"]==\"US\"]\nus_keys = us_keys.groupby([\"Province_State\", \"Country_Region\"])[[\"Confirmed\", \"Deaths\"]].sum().reset_index()\n\nus_keys = us_keys[~us_keys.Province_State.str.match(\"Diamond Princess|Grand Princess|Recovered|Northern Mariana Islands|American Samoa\")].reset_index(drop=True)\nus_keys","38a589f0":"confirmed = confirmed.append(us_keys[[\"Province_State\", \"Country_Region\"]], sort=False).reset_index(drop=True)\ndeaths = deaths.append(us_keys[[\"Province_State\", \"Country_Region\"]], sort=False).reset_index(drop=True)","b7fe8882":"for col in confirmed.columns[2:]:\n    confirmed[col].fillna(0, inplace=True)\n    deaths[col].fillna(0, inplace=True)","f2bcf50e":"confirmed","6fcb8fd0":"us_start_date = date(2020, 3, 10)\nday_date = us_start_date\n\nwhile day_date <= latest_train_date:\n    day = pd.read_csv(\"..\/input\/jhucovid19\/csse_covid_19_data\/csse_covid_19_daily_reports\/%s.csv\" % day_date.strftime(\"%m-%d-%Y\"))\n    \n    if \"Country\/Region\" in day.columns:\n        day.rename(columns={\"Country\/Region\": \"Country_Region\", \"Province\/State\": \"Province_State\"}, inplace=True)\n    \n    us = day[day[\"Country_Region\"]==\"US\"]\n    us = us.groupby([\"Province_State\", \"Country_Region\"])[[\"Confirmed\", \"Deaths\"]].sum().reset_index()\n    \n    unused_data = []\n    untouched_states = set(confirmed[confirmed[\"Country_Region\"]==\"US\"][\"Province_State\"])\n    \n    for (i, row) in us.iterrows():\n        if confirmed[(confirmed[\"Country_Region\"]==\"US\") & (confirmed[\"Province_State\"]==row[\"Province_State\"])].shape[0]==1:\n            confirmed.loc[(confirmed[\"Country_Region\"]==\"US\") & (confirmed[\"Province_State\"]==row[\"Province_State\"]), day_date.strftime(\"%Y-%m-%d\")] = row[\"Confirmed\"]\n            deaths.loc[(deaths[\"Country_Region\"]==\"US\") & (deaths[\"Province_State\"]==row[\"Province_State\"]), day_date.strftime(\"%Y-%m-%d\")] = row[\"Deaths\"]\n            untouched_states.remove(row[\"Province_State\"])\n        else:\n            unused_data.append(row[\"Province_State\"])\n            \n    print(day_date, \"Untouched\", untouched_states)\n    print(day_date, \"Unused\", unused_data)\n\n    day_date = day_date + timedelta(1)","6bb8c03a":"confirmed","a4ffda03":"deaths","8cafe62e":"dates_on_after_launch = [col for col in confirmed.columns[4:] if col>=launch_date.strftime(\"%Y-%m-%d\")]\nprint(\"Removing %d columns: %s\" % (len(dates_on_after_launch), str(dates_on_after_launch)))\n\ncols_to_keep = [col for col in confirmed.columns if col not in dates_on_after_launch]\n\nconfirmed = confirmed[cols_to_keep]\ndeaths = deaths[cols_to_keep]","6688f969":"for i in range(36):\n    this_date = (launch_date + timedelta(i)).strftime(\"%Y-%m-%d\")\n    confirmed.insert(len(confirmed.columns), this_date, np.NaN)\n    deaths.insert(len(deaths.columns), this_date, np.NaN)","fc3c2022":"confirmed_melted = confirmed.melt(confirmed.columns[:2], confirmed.columns[2:], \"Date\", \"ConfirmedCases\")\n#confirmed_melted.insert(5, \"Type\", \"Confirmed\")\ndeaths_melted = deaths.melt(deaths.columns[:2], deaths.columns[2:], \"Date\", \"Fatalities\")\n#deaths_melted.insert(5, \"Type\", \"Deaths\")\n\nconfirmed_melted.sort_values(by=[\"Country_Region\", \"Province_State\", \"Date\"], inplace=True)\ndeaths_melted.sort_values(by=[\"Country_Region\", \"Province_State\", \"Date\"], inplace=True)\n\nassert confirmed_melted.shape==deaths_melted.shape\nassert list(confirmed_melted[\"Province_State\"])==list(deaths_melted[\"Province_State\"])\nassert list(confirmed_melted[\"Country_Region\"])==list(deaths_melted[\"Country_Region\"])\nassert list(confirmed_melted[\"Date\"])==list(deaths_melted[\"Date\"])\n\ncases = confirmed_melted.merge(deaths_melted, on=[\"Province_State\", \"Country_Region\", \"Date\"], how=\"inner\")\ncases = cases[[\"Country_Region\", \"Province_State\", \"Date\", \"ConfirmedCases\", \"Fatalities\"]]\n\ncases.sort_values(by=[\"Country_Region\", \"Province_State\", \"Date\"], inplace=True)\ncases.insert(0, \"Id\", range(1, cases.shape[0]+1))\ncases","56482607":"forecast = cases.loc[cases[\"Date\"]>=public_leaderboard_start_date.strftime(\"%Y-%m-%d\")].copy()\nforecast.drop(columns=\"Id\", inplace=True)\nforecast.insert(0, \"ForecastId\", range(1, forecast.shape[0]+1))\nforecast.insert(6, \"Usage\", \"Ignored\")\nforecast.loc[forecast[\"Date\"]<launch_date.strftime(\"%Y-%m-%d\"),\"Usage\"]=\"Public\"\nforecast.loc[forecast[\"Date\"]>=final_evaluation_start_date.strftime(\"%Y-%m-%d\"),\"Usage\"]=\"Private\"\nforecast","3828cf36":"train = cases[cases[\"Date\"]<launch_date.strftime(\"%Y-%m-%d\")]\ntrain.to_csv(\"train.csv\", index=False)\ntrain","b73c8634":"test = forecast[forecast.columns[:-3]]\ntest.to_csv(\"test.csv\", index=False)\ntest","2da95af5":"solution = forecast[[\"ForecastId\", \"ConfirmedCases\", \"Fatalities\", \"Usage\"]].copy()\nsolution[\"ConfirmedCases\"].fillna(1, inplace=True)\nsolution[\"Fatalities\"].fillna(1, inplace=True)\nsolution.to_csv(\"solution.csv\", index=False)\nsolution","f66dbe16":"submission = forecast[[\"ForecastId\", \"ConfirmedCases\", \"Fatalities\"]].copy()\nsubmission[\"ConfirmedCases\"] = 1\nsubmission[\"Fatalities\"] = 1\nsubmission.to_csv(\"submission.csv\", index=False)\n\nsubmission","6c6f3037":"Adding the rows to be forecast","2c1498b4":"Adding in daily US state data","6fc40f14":"Starting to pull in US state data, since this was saved separately","ca02156e":"## Global competition data","9b20e388":"Move to ISO 8601 dates","2bd49147":"### COVID-19 Forecasting Challenge (Week 4) Data Prep\n\nThis notebook prepared the data in Kaggle's [COVID-19 Global Forecasting Competition (Week 4)](https:\/\/www.kaggle.com\/c\/covid19-global-forecasting-week-4) that was used to launch the competition. The source data comes from [JHU CSSE's COVID-19 data repository on GitHub](https:\/\/github.com\/CSSEGISandData\/COVID-19).\n\nI re-ran this notebook on updated data to add descriptive comments, so it won't output precisely the same as the original launch data. I saved the original launch data [to this dataset](https:\/\/www.kaggle.com\/inversion\/covid19-forecasting-week-four-launch-data).\n\nThe data for the submission period for the forecasting challenges is also updated every day, alongside leaderboard rescores. I use [this notebook](https:\/\/www.kaggle.com\/benhamner\/covid-19-forecasting-ongoing-data-updates\/) to run the ongoing data updates.","9003649d":"Melting the data to a version that will be friendlier to Kaggle's evaluation system.","d086f587":"Filtering out any data on or after the launch date for the competition"}}