{"cell_type":{"3b84bbda":"code","f999680f":"code","3ca5b96d":"code","1e94bc53":"code","28d2825c":"code","ca09ca1a":"code","ab77ed7a":"code","d3d16d5e":"code","ac0fee3c":"code","31c9e148":"code","5663d226":"code","d4187e22":"code","8867127d":"code","be08ceed":"code","e8e60185":"code","dafca0f4":"code","0d761521":"code","e412b849":"code","6cb9e83f":"code","ddd480f2":"code","75ffcef2":"code","8ccd3423":"code","20f7911c":"code","7a4da6f9":"code","4fafde04":"code","d62c6699":"code","fbbcaa53":"code","3f22e839":"code","ba65bcef":"code","75b62ca8":"code","11119f4f":"code","ff6acd48":"code","11d1e0f1":"code","9296aeed":"code","59a315b3":"code","f36a0dfe":"code","a3c680c8":"code","edf39c0f":"code","1113e568":"code","b974a2e2":"code","673c5eb6":"code","1e89e8e5":"code","90c231a6":"code","6131c8b8":"code","3b957e68":"code","c4b91fbb":"code","557a0625":"code","7c889ea0":"code","16abba0c":"code","c1553d7b":"code","13dce4e3":"code","5b9eb935":"code","e6975349":"code","5877675a":"code","7fef25b5":"code","5667f48a":"code","bc93edd4":"code","f3715ee2":"code","5b5f5053":"code","de991f72":"code","8be10c01":"code","b63be2b3":"code","8ff4591c":"code","b2410c1e":"code","ecc794bd":"code","8219703f":"code","a1a0da5f":"code","2cf66a24":"code","e0c4a934":"code","bd20581b":"code","21886137":"code","6665dabe":"code","6fd99678":"code","696b6c01":"code","ec0f2e32":"code","fe8325f6":"code","bd5847be":"code","ef5ae5d0":"code","f662c85b":"code","8be2b5d5":"code","41e8541d":"code","21ba2f12":"code","0723a5a5":"code","0b86fe4e":"code","dad493e8":"code","bb71079a":"code","2071cb13":"code","eec2f0f3":"code","d97706f6":"code","15b5c74b":"code","f7cd5fce":"code","c5894df9":"code","727f804a":"code","02283655":"code","c11ad922":"code","c488b238":"code","c402771b":"code","b39c14ad":"code","9cc54ac8":"code","379717d6":"code","83ed56d0":"code","d5860fa6":"code","2f7555c9":"code","9ec9d44e":"code","26c6b532":"code","263aad3d":"code","f248be57":"markdown","c7619018":"markdown","d17d0b92":"markdown","9ea0f53b":"markdown","2c9390fa":"markdown","d9fecdd5":"markdown","21a530cd":"markdown","a991acd5":"markdown","463c2d29":"markdown","316b7ff5":"markdown","ebe58d02":"markdown","83d86d16":"markdown","285dafa2":"markdown","97b7372f":"markdown","5abec426":"markdown","22b5383e":"markdown","2fc6e9d8":"markdown","f680e9a0":"markdown","69bac04a":"markdown","25d60807":"markdown","89d5e713":"markdown","e8ed41fb":"markdown","6db74fae":"markdown","1a2cdfa5":"markdown","53c8661f":"markdown","128277d2":"markdown","87e1f155":"markdown","176bfa5a":"markdown","f77d676e":"markdown","6087fa21":"markdown","4af69547":"markdown","8199879f":"markdown","dcf44068":"markdown","8eb34a5d":"markdown","df49ef06":"markdown","8f3c3c98":"markdown","c637abb5":"markdown","1c8025f9":"markdown","25e900df":"markdown","56235b2d":"markdown","94b79040":"markdown","10f6fca7":"markdown","8794b6ce":"markdown","f164fec7":"markdown","d756263c":"markdown","c4ec7c16":"markdown","d07b9814":"markdown","fe0b22a9":"markdown","04a60fca":"markdown","1525e4eb":"markdown","2d5f01ea":"markdown","743c7dbd":"markdown","12d71872":"markdown","aedbad51":"markdown","26c630f3":"markdown","3098bb10":"markdown","50bd7eba":"markdown","0a2dd04e":"markdown","64da7be6":"markdown","fd54c831":"markdown","24d9d786":"markdown","b911fd57":"markdown","71e41e43":"markdown","507bf35f":"markdown","c4172b79":"markdown","8312ed39":"markdown","debaf9d0":"markdown","29e9e1a4":"markdown","16673f48":"markdown","5d862e77":"markdown","bc01abf1":"markdown","5b5d2f58":"markdown","b9594cfe":"markdown","f8fd730f":"markdown","a33f4e59":"markdown","8fdb7b33":"markdown","fb0c6905":"markdown","09078a6c":"markdown","5a99dcf3":"markdown","b356fcb3":"markdown","6b56d7f3":"markdown","5be9ad84":"markdown","9d4427bd":"markdown","54daef46":"markdown","aeb88f6d":"markdown","e549a636":"markdown","24bd156c":"markdown","b3695d04":"markdown","eca6be77":"markdown","93eabacc":"markdown","2ef9a87b":"markdown","9b6b2350":"markdown","2c7f4cdd":"markdown","7a5786e1":"markdown"},"source":{"3b84bbda":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import register_matplotlib_converters\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split, cross_val_score, learning_curve\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.preprocessing import MinMaxScaler, PolynomialFeatures, LabelEncoder, OneHotEncoder\nfrom sklearn.metrics import mean_squared_error\nimport xgboost\nimport warnings\n\nregister_matplotlib_converters()\nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.max_columns', 2500, 'display.max_rows', 2500, 'display.width', None)","f999680f":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntrain.name = 'Training set'\n\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntest.name = 'Test set'\n\ndf = pd.concat([train, test]).reset_index(drop=True)\ndf.name = 'Total database'","3ca5b96d":"def correlation(value):\n    correlation = df.drop(['Id'], axis=1).apply(lambda x: x.factorize()[0]).corr().abs().unstack().sort_values(kind='quicksort', ascending=False).drop_duplicates(keep='first')\n    correlation = correlation.reset_index().rename(columns={0: 'Correlation'})\n    correlation = correlation[correlation['level_0'].str.contains(value) |\n                              correlation['level_1'].str.contains(value)]\n    return correlation[:5]\n\ndef countplot(feature, data):\n    sns.set(style='darkgrid')\n    sns.countplot(data[feature])\n    plt.title('{0} ({1})'.format(feature, data.name))\n    if len(df.groupby(['Exterior1st']).sum()) > 6:\n        plt.xticks(rotation=45, ha='right')\n    plt.show()\n\ndef catplot(feature, data):\n    sns.catplot(x=feature, y='SalePrice', data=data, kind=\"bar\")\n    plt.title('Effect of {} on SalePrice'.format(feature)), plt.show()","1e94bc53":"print('Training set:', train.shape)\nprint('Test set:', test.shape)\nprint('\\nColumns:\\n', list(df.columns))\n\n# Data types\nprint('\\nData types:\\n{}'.format(df.dtypes))\n\n# Descriptive statistics\ndf.describe()","28d2825c":"for dataset in (train, test, df):\n    dataset['MSSubClass'] = dataset['MSSubClass'].astype('str')","ca09ca1a":"print('Missing values in training set: {}'.format(train['SalePrice'].isna().sum()))\n\ntrain['SalePrice'].describe()","ab77ed7a":"sns.set(style='darkgrid')\nsns.distplot(df['SalePrice'], 20),\nplt.xticks(rotation=45, ha='right')\nplt.show()","d3d16d5e":"sns.set(style='darkgrid')\nsns.boxplot(x=train['SalePrice'])\nplt.title('Boxplot SalePrice', fontsize=12), plt.xlabel('SalePrice', fontsize=10), plt.xticks(fontsize=10, rotation=90)\nplt.show()","ac0fee3c":"df[df['SalePrice'] > 700000]","31c9e148":"highest_correlation_target = df.drop(['Id'], axis=1).corr().abs().unstack().sort_values(kind='quicksort', ascending=False)#.drop_duplicates(keep='first')\nhighest_correlation_target = highest_correlation_target.reset_index().rename(columns={0: 'Correlation'})\nhighest_correlation_target = highest_correlation_target[highest_correlation_target['level_0'].str.contains('SalePrice') |\n                                                        highest_correlation_target['level_1'].str.contains('SalePrice')]\nhighest_correlation_target = highest_correlation_target[highest_correlation_target['Correlation'] < 1]\nhighest_correlation_target.drop_duplicates(subset='Correlation')[:10]","5663d226":"correlation_matrix = df.drop(['Id'], axis=1).corr()#.drop_duplicates(keep='first')\n\nplt.figure(figsize=(12,12))\nsns.set(font_scale=0.75)\nax = sns.heatmap(correlation_matrix, vmin=-1, vmax=1, center=0, linewidths=0.5, cmap='coolwarm', square=True, annot=False)","d4187e22":"catplot('OverallQual', train)","8867127d":"sns.lmplot(x=\"GrLivArea\", y=\"SalePrice\", data=train)\nplt.show()","be08ceed":"train = train.drop(train[(train['GrLivArea'] > 4000) & (train['SalePrice'] < 300000)].index)#.reset_index(drop=False)\ndf = df.drop(df[(df['GrLivArea'] > 4000) & (df['SalePrice'] < 300000)].index)#.reset_index(drop=False)","e8e60185":"# Relplot\nsns.relplot(x='YearBuilt', y='SalePrice', hue='TotRmsAbvGrd', data=train)\nplt.show()","dafca0f4":"mean_sale_price_neighborhood = train.groupby('Neighborhood')['SalePrice'].mean().sort_values()\n\nsns.pointplot(x =mean_sale_price_neighborhood.index, y =mean_sale_price_neighborhood.values, data=train,\n              order=mean_sale_price_neighborhood.index)\nplt.xticks(rotation=45)\nplt.show()","0d761521":"central_air = train.groupby(['CentralAir'])['SalePrice'].mean()\ncentral_air = central_air.sort_index(ascending=False)\n\nplt.figure()\nsns.barplot(x=central_air.index, y=central_air.values)\nplt.title('Effect of CentralAir on SalePrice')\nplt.show()\n\nsns.set(style='darkgrid')\nsns.countplot(x=pd.qcut(train['SalePrice'], 5), hue='CentralAir', data=train)\nplt.xticks(ha='right', rotation=45)\nplt.show()","e412b849":"missing = df.isna().sum()\nmissing = missing[missing.values != 0].sort_values(ascending=False)\n\nplt.figure(figsize=(14,8))\nsns.barplot(missing.index, missing.values)\nplt.xticks(rotation=90), plt.ylabel('Missing values'), plt.title('Missing values by feature\\n(total dataset: {} observations)'.format(len(df)))\nfor i, v in enumerate(np.around(np.array(missing.values), 4)):\n    plt.text(i, v+20, str('%.0f' % v), ha='center', fontsize=8)\nplt.show()","6cb9e83f":"for dataset in (train, test, df):\n    # Categorial features\n    for column in ['Alley', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PoolQC', 'Fence', 'MiscFeature']:\n        dataset[column] = dataset[column].fillna('Not available')\n    \n    # Numerical features\n    for column in ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF']:\n        dataset[column] = dataset[column].fillna(0)","ddd480f2":"sns.set(style='darkgrid')\nsns.scatterplot(x='GarageYrBlt', y='YearBuilt', data=df)\nplt.show()\n\ncorrelation('GarageYrBlt')","75ffcef2":"df.loc[:, ('Id', 'GarageYrBlt', 'YearBuilt', 'YearRemodAdd')][df['GarageYrBlt'] == 2207]","8ccd3423":"df['GarageYrBlt'] = df['GarageYrBlt'].replace({2207: 2007})\n\nsns.set(style='darkgrid')\nsns.scatterplot(x='GarageYrBlt', y='YearBuilt', data=df)\nplt.show()","20f7911c":"print('Missing values:', len(df[df['GarageYrBlt'].isna()]))","7a4da6f9":"for dataset in (train, test, df):\n    dataset['GarageYrBlt'] = dataset['GarageYrBlt'].fillna(0)","4fafde04":"df.loc[:, ('Id', 'GarageCars', 'GarageQual', 'GarageCond')][df['GarageCars'].isna()]","d62c6699":"for dataset in (train, test, df):\n    dataset['GarageCars'] = dataset['GarageCars'].fillna(0)","fbbcaa53":"df.loc[:, ('Id', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual', 'GarageCond')][df['GarageArea'].isna()]","3f22e839":"for dataset in (train, test, df):\n    dataset['GarageArea'] = dataset['GarageArea'].fillna(0)","ba65bcef":"for feature in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    print('{0}: {1}'.format(feature, len(df[df[feature] == 'Not available'])))","75b62ca8":"basement = df\nbasement['Count'] = basement['BsmtQual'].str.count('Not available') + basement['BsmtCond'].str.count('Not available') + basement['BsmtExposure'].str.count('Not available') + basement['BsmtFinType1'].str.count('Not available') + basement['BsmtFinType2'].str.count('Not available')\nbasement.loc[:, ('Id', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'TotalBsmtSF')][(basement['Count'] < 5) & (basement['Count'] > 0)]","11119f4f":"correlation('BsmtQual')","ff6acd48":"df.loc[2217:2218, ('BsmtQual', 'OverallQual')]","11d1e0f1":"print('Mean OverallQual: {}'.format(round(df['OverallQual'].mean(), 2)))\n\nfor dataset in (train, test, df):\n    for value in (2218, 2219):\n        dataset.loc[dataset['Id'] == value, 'BsmtQual'] = 'TA'","9296aeed":"correlation('BsmtCond')","59a315b3":"for dataset in (train, test, df):\n    dataset['BsmtCond'] = np.where((dataset['BsmtCond'] == 'Not available') &\n                                   (dataset['BsmtQual'] != 'Not available'), dataset['BsmtQual'], dataset['BsmtCond'])\n        \ndf.loc[(2040, 2185, 2524), ('BsmtQual', 'BsmtCond')]","f36a0dfe":"correlation('BsmtExposure')","a3c680c8":"df.loc[(948, 1487, 2348), ('BsmtExposure', 'HouseStyle')]","edf39c0f":"correlation('BsmtExposure')\nsns.catplot(x='HouseStyle', hue='BsmtExposure', data=df, kind='count')\nplt.show()","1113e568":"for dataset in (train, test, df):\n    for value in (949, 1488, 2349):\n        dataset.loc[dataset['Id'] == value, 'BsmtExposure'] = 'No'","b974a2e2":"correlation('BsmtFinType2')","673c5eb6":"df.loc[[332], ('Id', 'BsmtFinType2', 'BsmtFinSF2')]","1e89e8e5":"df['BsmtFinSF2Grouped'] = pd.cut(df['BsmtFinSF2'], 5)\n\nsns.catplot(x='BsmtFinSF2Grouped', data=df, kind='count')\nplt.xticks(rotation=45, ha='right')\nplt.title('Number of houses with BsmtFinSF2 (305.2, 610.4)')\nfor i, v in enumerate(np.array(df.groupby('BsmtFinSF2Grouped')['BsmtFinType2'].count())):\n    plt.text(i, v+20, v, ha='center', fontsize=10) \nplt.show()","90c231a6":"df_new = df[df['BsmtFinSF2Grouped'] == pd.Interval(305.2, 610.4)]\n\npercentage = 100 * df_new['BsmtFinType2'].value_counts() \/ df_new['BsmtFinType2'].value_counts().sum()\nplt.figure(figsize=(10,6))\nplt.pie(percentage)\nplt.legend(['{0} - {1:1.2f} %'.format(i, j) for i, j in zip(percentage.index, percentage)], loc='best', fontsize=10, frameon=True)\nplt.title('Distribution of BsmtFinType2 in the BsmtFinSF2 group (305.2, 610.4)')\nplt.show()\n\nfor dataset in (train, test, df):\n    dataset.loc[dataset['Id'] == 333, 'BsmtFinType2'] = 'Rec'\n","6131c8b8":"for feature in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    print('{0}: {1}'.format(feature, len(df[df[feature] == 'Not available'])))","3b957e68":"df.name = 'Total database'\ncountplot('MSZoning', df)\n\nprint('Missing values in MSZoning: {}'.format(df['MSZoning'].isna().sum()))\n\ncorrelation('MSZoning')","c4b91fbb":"df.loc[:, ('Id', 'MSZoning', 'Alley')][df['MSZoning'].isna()]","557a0625":"sns.catplot(x='Alley', hue='MSZoning', data=df, kind='count')\nplt.title('MSZoning for Alley values')\nplt.show()\n\nfor dataset in (train, test, df):\n    dataset['MSZoning'] = dataset['MSZoning'].fillna('RL')","7c889ea0":"print('Missing values in Utilities: {}'.format(df['Utilities'].isna().sum()))\n\ncountplot('Utilities', df)\n\nprint(df['Utilities'].value_counts())\n\nfor dataset in (train, test, df):\n    dataset['Utilities'] = dataset['Utilities'].fillna('AllPub')","16abba0c":"print('Missing values in Exterior1st: {}'.format(df['Exterior1st'].isna().sum()))\nprint('Missing values in Exterior2nd: {}'.format(df['Exterior2nd'].isna().sum()))\n\nfig, ax  = plt.subplots(nrows=1, ncols=2, figsize=(12,5))\nsns.countplot(df['Exterior1st'], ax=ax[0])\nsns.countplot(df['Exterior2nd'], ax=ax[1])\nfor ax in fig.axes:\n    plt.sca(ax)\n    plt.xticks(rotation=45)\nfig.show()","c1553d7b":"correlation('Exterior1st|Exterior2nd')","13dce4e3":"df.loc[:, ('Id', 'Exterior1st', 'Exterior2nd', 'Foundation')][df['Exterior1st'].isnull()]","5b9eb935":"sns.catplot(x='Foundation', data=df, hue='Exterior2nd', kind='count')\nplt.xticks(rotation=45, ha='right')\nplt.show()\n\nfor dataset in (train, test, df):\n    dataset['Exterior1st'] = dataset['Exterior1st'].fillna('VinylSd')\n    dataset['Exterior2nd'] = dataset['Exterior2nd'].fillna('VinylSd')","e6975349":"print('Missing values in MasVnrType: {}'.format(df['MasVnrType'].isna().sum()))\nprint('Missing values in MasVnrArea: {}'.format(df['MasVnrArea'].isna().sum()))\n\nsns.set(style='darkgrid')\nsns.countplot(df['MasVnrType'])\nplt.title('{0} ({1})'.format('MasVnrType', df.name))\nplt.show()","5877675a":"df['YearBuiltGrouped'] = pd.cut(df['YearBuilt'], 10)\ncountplot('YearBuiltGrouped', df)","7fef25b5":"sns.set(style='darkgrid')\nsns.catplot(x='YearBuiltGrouped', data=df, hue='MasVnrType', kind='count')\nplt.xticks(rotation=45, ha='right')\nplt.title('MasVnrType per YearBuiltGrouped')\nplt.show()\n\nmissing_MasVnrType = df[df[\"MasVnrType\"].isnull()]\nsns.set(style='darkgrid')\nsns.countplot(missing_MasVnrType['YearBuiltGrouped'])\nplt.title('YearBuiltGrouped for missing values in MasVnrType')\nplt.xticks(rotation=45, ha='right')\nplt.show()","5667f48a":"correlation('MasVnrArea')","bc93edd4":"sns.catplot(x='Fireplaces', data=df, hue='MasVnrType', kind='count')\nplt.xticks(rotation=45, ha='right')\nplt.show()\n\ndf.loc[:, ('Id', 'MasVnrType', 'Fireplaces')][df['MasVnrType'].isnull()]","f3715ee2":"train = train.drop(['MasVnrType', 'MasVnrArea'], axis=1)\ntest = test.drop(['MasVnrType', 'MasVnrArea'], axis=1)\ndf = df.drop(['MasVnrType', 'MasVnrArea'], axis=1)","5b5f5053":"print('Missing values in LotFrontage: {}'.format(df['LotFrontage'].isna().sum()))\n\ndf['LotFrontageGrouped'] = pd.cut(df['LotFrontage'], 10)\n\nsns.countplot(df['LotFrontageGrouped'])\nplt.xticks(rotation=45, ha='right')\nplt.show()\n\ncorrelation('LotFrontage')","de991f72":"missing_LotFrontage = df.loc[:, ('Id', 'LotFrontage', 'BldgType')][df['LotFrontage'].isna()]\nsns.countplot(missing_LotFrontage['BldgType'])\nplt.title('Missing values in LotFrontage grouped by BldgType')\nplt.show()\n\nfor dataset in (train, test, df):\n    dataset['LotFrontage'] = dataset['LotFrontage'].fillna(df.groupby('BldgType')['LotFrontage'].transform('mean'))","8be10c01":"print('Missing values in Electrical: {}'.format(df['Electrical'].isna().sum()))\n\nsns.countplot(df['Electrical'])\nplt.xticks(rotation=45, ha='right')\nplt.show()\n\ncorrelation('Electrical')","b63be2b3":"df.loc[:, ('Id', 'Electrical', 'CentralAir')][df['Electrical'].isna()]","8ff4591c":"sns.catplot(x='CentralAir', data=df, hue='Electrical', kind='count')\nplt.xticks(rotation=45, ha='right')\nplt.show()\n\ndistribution = df.groupby('CentralAir')['Electrical'].value_counts()[4:]\nlegend = ['SBrkr', 'FuseA', 'FuseF', 'FuseP', 'Mix']    \npercentage = 100 * distribution.values \/ distribution.values.sum()\n\nplt.pie(distribution.values \/ distribution.values.sum(), wedgeprops=dict(edgecolor='black', linewidth=0.25))\nplt.legend(['{0} - {1:1.2f} %'.format(i, j) for i, j in zip(legend, percentage)], loc='best', fontsize=10, frameon=True)\nplt.title('Distribution of Electrical for CentralAir=Y', fontsize=12)\nplt.show()\n\nfor dataset in (train, test, df):\n    dataset['Electrical'] = dataset['Electrical'].fillna('Sbrkr')","b2410c1e":"print('Missing values in BsmtFullBath: {}'.format(df['BsmtFullBath'].isna().sum()))\nprint('Missing values in BsmtHalfBath: {}'.format(df['BsmtHalfBath'].isna().sum()))","ecc794bd":"df.loc[:, ('Id', 'BsmtQual', 'BsmtFullBath', 'BsmtHalfBath')][df['BsmtFullBath'].isna()|df['BsmtHalfBath'].isna()]","8219703f":"for dataset in (train, test, df):\n    for feature in ('BsmtFullBath', 'BsmtHalfBath'):\n        dataset[feature] = dataset[feature].fillna(0)","a1a0da5f":"print('Missing values in KitchenQual: {}'.format(df['KitchenQual'].isna().sum()))\n\ncorrelation('KitchenQual')","2cf66a24":"df.loc[:, ('KitchenQual','ExterQual', 'BsmtQual', 'OverallQual')][df['KitchenQual'].isna()]","e0c4a934":"for dataset in (train, test, df):\n    dataset['KitchenQual'] = dataset['KitchenQual'].fillna(dataset['ExterQual'])\n\ndf.loc[[1555], ('KitchenQual','ExterQual', 'OverallQual')]","bd20581b":"print('Missing values in Functional: {}'.format(df['Functional'].isna().sum()))\n\nplt.figure(figsize=(8, 6))\nplt.pie(df['Functional'].value_counts() \/ df['Functional'].value_counts().sum(),\n        wedgeprops=dict(edgecolor='black', linewidth=0.25))\npercentage = 100. * df['Functional'].value_counts() \/ df['Functional'].value_counts().sum()\nplt.legend(['{0} - {1:1.2f} %'.format(i, j) for i, j in zip(percentage.index, percentage)],\n           loc='best', fontsize=10, frameon=True)\nplt.title('Distribution of Functional', fontsize=14)\nplt.show()\n\n# Replace NaN with 'Typ'\nfor dataset in (train, test, df):\n    dataset['Functional'] = dataset['Functional'].fillna('Typ')","21886137":"print('Missing values in SaleType: {}'.format(df['SaleType'].isna().sum()))\n\ndf.loc[:, ('SalePrice', 'SaleType')].apply(lambda x: x.factorize()[0]).corr()\n\ntrain = train.drop('SaleType', axis=1)\ntest = test.drop('SaleType', axis=1)\ndf = df.drop('SaleType', axis=1)","6665dabe":"for column in ['Count', 'BsmtFinSF2Grouped', 'YearBuiltGrouped', 'LotFrontageGrouped']:\n    df = df.drop(column, axis=1)\n\nfor dataset in (train, test,df):\n    for column in ['MoSold', 'SaleCondition']:\n        dataset = dataset.drop(column, axis=1)","6fd99678":"df.columns[df.isnull().any()].tolist()","696b6c01":"for dataset in (train, test, df):\n    dataset['TotalSF'] = dataset['TotalBsmtSF'] + dataset['1stFlrSF'] + dataset['2ndFlrSF']","ec0f2e32":"catplot('FullBath', train)\ncatplot('HalfBath', train)","fe8325f6":"for dataset in (train, test, df):\n    dataset['Bath'] = dataset['FullBath'] + dataset['HalfBath']\n\ncatplot('Bath', train)","bd5847be":"df.loc[:, ('YearBuilt', 'YearRemodAdd')].corr()","ef5ae5d0":"fig, ax  = plt.subplots(nrows=1, ncols=2, figsize=(14,4))\nsns.scatterplot(x='YearBuilt', y='SalePrice', data=train, ax=ax[0])\nsns.scatterplot(x='YearRemodAdd', y='SalePrice', data=train, ax=ax[1])\n\nfor ax in fig.axes:\n    plt.sca(ax)\n    plt.xticks(fontsize=8), plt.yticks(fontsize=8)\nfig.show()","f662c85b":"train = train.drop(['YearRemodAdd'], axis=1)\ntest = test.drop(['YearRemodAdd'], axis=1)\ndf = df.drop(['YearRemodAdd'], axis=1)","8be2b5d5":"X = df.drop(['SalePrice'], axis=1)\n\nprint('Columns before one-hot encoding:', X.shape[1])\n        \nfor column in X:\n    if X[column].dtypes == 'object':\n        one_hot_encoding = pd.get_dummies(X[column])\n        one_hot_encoding.columns = column + '_' + one_hot_encoding.columns.astype('str')\n        X = pd.concat([X, one_hot_encoding], ignore_index=False, axis=1, sort=False)\n        X = X.drop(column, axis=1)\n\nprint('Columns after one-hot encoding:', X.shape[1])","41e8541d":"highest_correlation_target = df.drop(['Id'], axis=1).corr().abs().unstack().sort_values(kind='quicksort', ascending=False)#.drop_duplicates(keep='first')\nhighest_correlation_target = highest_correlation_target.reset_index().rename(columns={0: 'Correlation'})\nhighest_correlation_target = highest_correlation_target[highest_correlation_target['level_0'].str.contains('SalePrice') |\n                                                        highest_correlation_target['level_1'].str.contains('SalePrice')]\nhighest_correlation_target = highest_correlation_target[highest_correlation_target['Correlation'] < 1]\nhighest_correlation_target.drop_duplicates(subset='Correlation')[:10]","21ba2f12":"poly = PolynomialFeatures(degree=3)\npolynomial = pd.DataFrame(poly.fit_transform(X.loc[:, ('TotalSF', 'OverallQual', 'GrLivArea', 'TotalBsmtSF', 'GarageCars', '1stFlrSF', 'GarageArea', 'Bath', 'FullBath', 'TotRmsAbvGrd')]),\n                          columns=poly.get_feature_names(X.loc[:, ('TotalSF', 'OverallQual', 'GrLivArea', 'TotalBsmtSF', 'GarageCars', '1stFlrSF', 'GarageArea', 'Bath', 'FullBath','TotRmsAbvGrd')].columns))\n\nX = pd.concat([X, polynomial.loc[:, 'TotalSF^2':'TotRmsAbvGrd^3']], ignore_index=False, axis=1, sort=False)","0723a5a5":"for column in X:\n    if column != 'Id':\n        X[column] = MinMaxScaler().fit_transform(X[[column]])","0b86fe4e":"X_trainval = X.loc[X['Id'] < 1461]\nX_trainval = X_trainval.drop(['Id'], axis=1)\nX_trainval.name = 'X_trainval'\n\nX_test = X.loc[X['Id'] >= 1461]  \nX_test = X_test.drop(['Id'], axis=1)\nX_test.name = 'X_test'","dad493e8":"X_training, X_validation = train_test_split(X_trainval, test_size=0.2, shuffle=False)\nX_training.name = 'X_training'\nX_validation.name = 'X_validation'","bb71079a":"print('Datasets for estimating generalization performance:')\nprint('X_training: {}'.format(X_training.shape))\nprint('X_validation: {}\\n'.format(X_validation.shape))\n\nprint('Datasets for predicting test set:')\nprint('X_trainval: {}'.format(X_trainval.shape))\nprint('X_test: {}'.format(X_test.shape))","2071cb13":"X_trainval[:5]","eec2f0f3":"y_trainval = np.ravel(train[['SalePrice']])\ny_training = y_trainval[:len(X_training)]\ny_validation = y_trainval[len(X_training):]","d97706f6":"model_comparison = pd.DataFrame({'Model': [], 'RMSE': []})\n\nxg_boost = xgboost.XGBRegressor(n_estimators=500, max_depth=3, learning_rate=0.1)\nxg_boost.name ='XGBoost'\ngradient_boosting = GradientBoostingRegressor(learning_rate=0.1, n_estimators=500, max_depth=3, alpha=0.9)\ngradient_boosting.name = 'Gradient boosting'\nrandom_forest = RandomForestRegressor(n_estimators=200, max_features=40, max_depth=40)\nrandom_forest.name = 'Random forest'\nlasso = LassoCV(alphas=None, n_alphas=50, cv=10)\nlasso.name = 'LASSO'\n\nfor model in [xg_boost, gradient_boosting, random_forest, lasso]:\n    # Train the model with polynomials\n    model.fit(X_training, y_training)\n    rmse_poly = round(mean_squared_error(np.log(y_validation), np.log(model.predict(X_validation))) ** (1\/2), 5)\n\n    model_results = pd.DataFrame({'Model': [model.name + ' (polynomials)'], 'RMSE': [rmse_poly]})\n    model_comparison = model_comparison.append(model_results, ignore_index=True)\n\n    # Train the model without polynomials\n    X_training_no_poly = X_training.loc[:, :'SaleCondition_Partial']\n    X_validation_no_poly = X_validation.loc[:, :'SaleCondition_Partial']\n\n    model.fit(X_training_no_poly, y_training)\n\n    rmse_no_poly = round(mean_squared_error(np.log(y_validation), np.log(model.predict(X_validation_no_poly))) ** (1\/2), 5)\n\n    model_results = pd.DataFrame({'Model': [model.name + ' (no polynomials)'], 'RMSE': [rmse_no_poly]})\n    model_comparison = model_comparison.append(model_results, ignore_index=True)\n    \nmodel_comparison.sort_values(by='RMSE', ascending=True).reset_index(drop=True)","15b5c74b":"def learning(model):\n    train_sizes, train_scores, test_scores = learning_curve(model, X_trainval, y_trainval, cv=5)\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n\n    plt.style.use('seaborn-darkgrid')\n    plt.plot(train_sizes, train_mean, color='#1f77b4', label='Training set', linewidth=2)  # Draw lines train\n    plt.plot(train_sizes, test_mean, color='#d62728', label='Validation set', linewidth=2)  # Draw lines validation\n    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color='#1f77b4', alpha=0.25)  # Draw band train\n    plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color='#d62728', alpha=0.25)  # Draw band validation\n    plt.title('Learning curve {}'.format(model.name)), plt.xlabel('Training set size'), plt.ylabel('Score'), plt.ylim(0, 1)\n    plt.legend(loc='best')\n    plt.show()","f7cd5fce":"X_training_no_poly = X_training.loc[:, :'SaleCondition_Partial']\nX_validation_no_poly = X_validation.loc[:, :'SaleCondition_Partial']\n\nxg_boost.fit(X_training_no_poly, y_training)\n\nfeature_importance = pd.DataFrame({'Feature': X_training_no_poly.columns, 'Relative Importance': xg_boost.feature_importances_})\nfeature_importance = feature_importance.iloc[feature_importance['Relative Importance'].abs().argsort()[::-1]].reset_index(drop=True)\nfeature_importance[:10]","c5894df9":"learning(xg_boost)","727f804a":"gradient_boosting.fit(X_training_no_poly, y_training)\n\nfeature_importance = pd.DataFrame({'Feature': X_training_no_poly.columns, 'Relative Importance': gradient_boosting.feature_importances_})\nfeature_importance = feature_importance.iloc[feature_importance['Relative Importance'].abs().argsort()[::-1]].reset_index(drop=True)\nfeature_importance[:10]","02283655":"learning(gradient_boosting)","c11ad922":"random_forest.fit(X_training_no_poly, y_training)\n\nfeature_importance = pd.DataFrame({'Feature': X_training_no_poly.columns, 'Relative Importance': random_forest.feature_importances_})\nfeature_importance = feature_importance.iloc[feature_importance['Relative Importance'].abs().argsort()[::-1]].reset_index(drop=True)\nfeature_importance[:10]","c488b238":"learning(random_forest)","c402771b":"lasso.fit(X_training_no_poly, y_training)\n\nalphas = pd.DataFrame(list(lasso.alphas_), columns=['Alpha'])\ncoefficient_path = lasso.path(X_training_no_poly, y_training, alphas=alphas)\ncoefficients = pd.DataFrame(coefficient_path[1], index=X_training_no_poly.columns).T.iloc[::-1].reset_index(drop=True)\nresult = pd.concat([alphas, coefficients], axis=1, sort=False)\nbest_alpha = result[result['Alpha'] == lasso.alpha_].tail(1)\n\nused_features = best_alpha.columns[(best_alpha != 0).iloc[0]].tolist()\nused_features.remove('Alpha')\n\nbest_coefficients = best_alpha[best_alpha['Alpha'] == lasso.alpha_].reset_index(drop=True)\nbest_coefficients = best_coefficients.drop(['Alpha'], axis=1)\nbest_coefficients = best_coefficients.iloc[0]\nbest_coefficients = pd.DataFrame({'Feature': best_coefficients.index, 'Coefficient': best_coefficients.values},\n                                columns=['Feature', 'Coefficient'])\n\nbest_coefficients = best_coefficients.iloc[(-best_coefficients['Coefficient'].abs()).argsort()].reset_index(drop=True)\n\nprint('At the best alpha {0}, LASSO set {1} of the total {2} features equal to zero.'.format(\n    round(lasso.alpha_, 5),  sum(best_coefficients['Coefficient'] == 0), len(best_coefficients['Coefficient'])))\n\nbest_coefficients[:10]","b39c14ad":"learning(lasso)","9cc54ac8":"prediction_validation_xg_boost = pd.DataFrame({'Id': X['Id'][:len(X_validation_no_poly)], 'Actual': y_validation, 'SalePrice': xg_boost.predict(X_validation_no_poly)})\nprediction_validation_gradient_boosting = pd.DataFrame({'Id': X['Id'][:len(X_validation_no_poly)], 'Actual': y_validation, 'SalePrice': gradient_boosting.predict(X_validation_no_poly)})\nprediction_validation_random_forest = pd.DataFrame({'Id': X['Id'][:len(X_validation_no_poly)], 'Actual': y_validation, 'SalePrice': random_forest.predict(X_validation_no_poly)})\nprediction_validation_lasso = pd.DataFrame({'Id': X['Id'][:len(X_validation_no_poly)], 'Actual': y_validation, 'SalePrice': lasso.predict(X_validation_no_poly)})","379717d6":"plt.figure(figsize=(16,10))\nplt.plot(prediction_validation_gradient_boosting['Id'][:50], prediction_validation_gradient_boosting['Actual'].sort_values()[:50])\nplt.plot(prediction_validation_gradient_boosting['Id'][:50], prediction_validation_gradient_boosting['SalePrice'].sort_values()[:50])\nplt.plot(prediction_validation_xg_boost['Id'][:50], prediction_validation_xg_boost['SalePrice'].sort_values()[:50])\nplt.plot(prediction_validation_lasso['Id'][:50], prediction_validation_lasso['SalePrice'].sort_values()[:50])\nplt.plot(prediction_validation_random_forest['Id'][:50], prediction_validation_random_forest['SalePrice'].sort_values()[:50])\nplt.legend(['True SalePrice', 'XG boost', 'Gradient boosting', 'Random forest', 'LASSO']), plt.ylabel('SalePrice'), plt.xlabel('Observation')\nplt.title('True SalePrice vs. predictions (cheapest houses)')\nplt.show()","83ed56d0":"plt.figure(figsize=(16,10))\nplt.plot(prediction_validation_gradient_boosting['Id'][240:], prediction_validation_gradient_boosting['Actual'].sort_values()[240:])\nplt.plot(prediction_validation_gradient_boosting['Id'][240:], prediction_validation_gradient_boosting['SalePrice'].sort_values()[240:])\nplt.plot(prediction_validation_xg_boost['Id'][240:], prediction_validation_xg_boost['SalePrice'].sort_values()[240:])\nplt.plot(prediction_validation_random_forest['Id'][240:], prediction_validation_random_forest['SalePrice'].sort_values()[240:])\nplt.plot(prediction_validation_lasso['Id'][240:], prediction_validation_lasso['SalePrice'].sort_values()[240:])\nplt.legend(['True SalePrice', 'Gradient boosting', 'XG boost', 'Random forest', 'LASSO']), plt.ylabel('SalePrice'), plt.xlabel('Observation')\nplt.title('True SalePrice vs. predictions (most expensive houses)')\nplt.show()","d5860fa6":"prediction_validation_average = ((gradient_boosting.predict(X_validation_no_poly) +\n                                  xg_boost.predict(X_validation_no_poly) +\n                                  random_forest.predict(X_validation_no_poly) +\n                                  lasso.predict(X_validation_no_poly)) \/ 4)\n\nrmse_average = round(mean_squared_error(np.log(y_validation), (np.log(prediction_validation_average))) ** (1\/2), 5)\nprint('XGBoost, Gradient Boosting, Random Forest & LASSO:')\nprint('RMSE =', rmse_average)","2f7555c9":"xg_boost.fit(X_training_no_poly, y_training)\ngradient_boosting.fit(X_training_no_poly, y_training)\nrandom_forest.fit(X_training_no_poly, y_training)\nlasso.fit(X_training_no_poly, y_training)\n\n# Index has to be reset to match indices of X_validation_no_poly and prediction_validation_average (since observations have been dropped earlier)\nX_validation_no_poly.set_index([list(range(len(X_training)+1, len(X_training)+1+len(X_validation_no_poly)))], inplace=True)\n\nfor index, value in enumerate(prediction_validation_average):\n    if value > 400000:\n        print('Index:', index)\n        print(value)\n        prediction_validation_average[index] = (gradient_boosting.predict(X_validation_no_poly.loc[[index+1167], :]) +\n                                                xg_boost.predict(X_validation_no_poly.loc[[index+1167], :])) \/ 2\n        print(prediction_validation_average[index])","9ec9d44e":"rmse_average = round(mean_squared_error(np.log(y_validation), (np.log(prediction_validation_average))) ** (1\/2), 5)\nprint('Outliers only XGBoost, Gradient Boosting, Random Forest & LASSO:')\nprint('RMSE =', rmse_average)","26c6b532":"X_trainval_no_poly = X_trainval.loc[:, :'SaleCondition_Partial']\nX_test = X_test.loc[:, :'SaleCondition_Partial']\n\nxg_boost.fit(X_trainval_no_poly, y_trainval)\ngradient_boosting.fit(X_trainval_no_poly, y_trainval)\nrandom_forest.fit(X_trainval_no_poly, y_trainval)\nlasso.fit(X_trainval_no_poly, y_trainval)\n\nprediction_test = pd.DataFrame({'Id': test['Id'], 'SalePrice': ((xg_boost.predict(X_test) +\n                                                                 gradient_boosting.predict(X_test) +\n                                                                 random_forest.predict(X_test) +\n                                                                 lasso.predict(X_test)) \/ 4)})\n\n# Make XGBoost and gradient boosting predict houses with predicted SalePrice >400K\nfor index, column in prediction_test.iterrows():\n    if column['SalePrice'] > 400000:\n        print('Index:', index)\n        print(column['SalePrice'])\n        prediction_test.loc[index, 'SalePrice'] = ((xg_boost.predict(X_test.loc[[1460+index], :]) + \n                                                   gradient_boosting.predict(X_test.loc[[1460+index], :]))\/2)\n        print(prediction_test.loc[index, 'SalePrice'])","263aad3d":"prediction_test.to_csv('my_submission.csv', index=False)","f248be57":"## 4.1 XGBoost\n\n**4.1.1 Feature importance**","c7619018":"**2.1.3 Feature *GarageArea***\n\nThere is one NaN in the numerical column *GarageArea*. The missing value addresses a house without garage. Therefore, the NaN can be replaced with 0.","d17d0b92":"- Some columns have lots of missing values others have only a few missing values.\n- The columns with missing values might simply be dropped. By doing so, information would get lost though. Thereby, every column with a missing value will be considered in detail.\n- In the housing price data, missing values are not completely meaningless. In *PoolQC* and many other columns, a missing value indicates that this item, here a pool, is not avaialable in this house. An additional group 'Not available' has been created.\n- This should only be done for categorical features (dtype 'object'). Missing values in numerical columns should be replace with a number.","9ea0f53b":"**Make test set predictions:**","2c9390fa":"**c) Features *YearBuilt* & *TotRmsAbvGrd***\n\nThe following relational plots captures the effects of *YearBuilt* and *TotRmsAbvGrd* on *SalePrice* in one figure.","d9fecdd5":"**2.2.3 Feature *YearRemodAdd***\n\n*YearBuilt* is highly correlated with *YearRemodAdd*. There is a clear positive effect on *SalePrice*. The feature *YearRemodAdd* may be biased due to the fact that it is same as *YearBuilt* if there was no remodeling or addition. Therefore, only *YearBuilt* will be used.","21a530cd":"There is no way to reliably replace the NaN values in *MasVnrType* and *MasVnrArea*. Thus, both columns are dropped from the database.","a991acd5":"# 1. Exploratory Data Analysis\n\n## 1.1 Overview","463c2d29":"**Read the data:**","316b7ff5":"**2.1.2 Feature *GarageCars***\n\nThere is one house with a missing value in *GarageCars*. It has no garage and thus no car capacity so that the NaN is replaced with 0.","ebe58d02":"**2.1.4 Features *BsmtQual*, *BsmtCond*, *BsmtExposure*, *BsmtFinType1*, *BsmtFinType2***\n\nThe missing value overview has shown that houses without basement have not consistently 'Not available' in the 'object' columns addressing the basement (*BsmtQual*, *BsmtCond*, *BsmtExposure*, *BsmtFinType1*, *BsmtFinType2*). See output below.","83d86d16":"**Define functions:**","285dafa2":"Older houses have mostly 'None' value while newer houses have different *MasVnrType*. The missing values only address newer houses so that they cannot be replaced reliably based on *YearBuilt*.","97b7372f":"**2.2.1 Feature *TotalSF***\n\nA feature indicating the total square feet of the house has been engineered.","5abec426":"**Import libraries:**","22b5383e":"The features are highly correlated and *Exterior2nd* correlates most with *Foundation*.","2fc6e9d8":"**c) Feature *BsmtExposure***\n\n*BsmtExposure* correlates most with *HouseStyle* and the houses with missing values have the style '2Story' or '1Story'.","f680e9a0":"Drop columns that have been created for data visualization.","69bac04a":"# 4. Model Diagnostics\n\nThe predictions of the better performing model for every algorithm are analyzed more in-depth:\n- GXBoost (no polynomials)\n- Gradient boosting (no polynomials)\n- Random forest (no polynomials)\n- LASSO (no polynomials)\n\nThe following diagnostics will be adressed:\n- Feature importance (10 most important features)\n- Learning curve\n- True vs. predicted *SalePrice*\n\nLearning curves are a powerful technique to get insights into the generalization performance of the model. They show whether the algorithm suffers from overfitting and whether more training data is expected to improve the performance of the algorithm.","25d60807":"Overview of the final trainval set:","89d5e713":"# 2. Data Preprocessing","e8ed41fb":"**b) Feature *GrLivArea*:**","6db74fae":"From the 6 *BsmtFinType1* groups, 'Rec' which indicates an average rec room is the most common one (see pie chart below). The missing value has been replaced with 'Rec'.","1a2cdfa5":"There are 2 outliers in *GrLivArea* with a large *GrLivArea* but a low *SalePrice*. They will be removed.","53c8661f":"The following column ***MSSubClass*** has a numerical data type. It needs to be converted to categorical columns. Otherwise the model would compare the values which may yield poor results.","128277d2":"Correlations may yield better insights. *MasVnrArea* has correlates most with *Fireplaces*.","87e1f155":"**4.4.2 Learning curve**","176bfa5a":"**Define functions:**","f77d676e":"**4.5.1 Lowest *SalePrice***\n\nFindings:\n- XGBoost and Gradient boosting are very robust to outliers.\n- LASSO overestimates and Random forest underestimates the *SalePrice*.\n- Random forest performs best for houses with a *SalePrice* between 60K and 100K.","6087fa21":"Over 94% of  houses with central air conditioning have a 'Standard Circuit Breakers & Romex' electrical sytstem. Therefore, the house with the missing value is expected to have the same electrical system. The missing value has been replaced by 'Sbrkr'.","4af69547":"# Introduction\n\nThis kernel addresses the **House Prices: Advanced Regression Techniques** competition.\n\n**Focus:**\n- Feature analysis with respect to missing values\n- Comparison of different algorithms:\n - **XGBoost**\n - **Gradient Boosting Regressor**\n - **Random Forest Regressor**\n - **LASSO**\n\nThe kernel consists of 4 sections:\n- **Exploratory Data Analysis**\n- **Data Preprocessing**\n- **Model Selection**\n- **Model Diagnostics**","8199879f":"## 2.5 Feature scaling for numerical features","dcf44068":"**4.1.2 Learning curve**","8eb34a5d":"Most houses with the style '2Story' and '1Story' have no basement exposure. The *BsmtExposure* value for the 3 houses was changed from 'Not available' (since the houses clearly have a basement) to 'No'. ","df49ef06":"The feature *ExterQual* which *KitchenQual* correlates most with has the same column values so that missing value can be replaced easily. Missing value after cleaning:","8f3c3c98":"**2.1.1 Feature *GarageYrBlt***\n\nFirst of all, the scatterplot reveals an incorrect value in *GarageYrBlt*. According to this, the garage was\/will be built in 2207 which does not make sense. This house was built in 2006 and remodeled in 2007. Therefore, 2207 is expected to be a typing error that should mean 2007. The value has been replaced.","c637abb5":"In the group of 'PConc', most houses have Exterior 'VinylSd'. The missing value has thus been replaced with the most common value 'VinylSd'.\n","1c8025f9":"## 2.1 Missing values\n\nColumns with missing values will be analyzed and cleaned in detail. The outputs below provide a first overview of missing values.","25e900df":"**b) Feature *CentralAir*:**\n\n*CentralAir* has a positive effect on the target value. This is most relevant when it comes to predicting cheaper houses. ","56235b2d":"**4.3.2 Learning curve**","94b79040":"**4.4.1 Feature importance (highest coefficients)**","10f6fca7":"The house has average values for the remaining quality features.","8794b6ce":"Split trainval set into training and validation set to be able to estimate the generalization performance.","f164fec7":"## 2.6 Prepare the datasets\n\n**2.6.1 Estimators**\n\nThe column *Id* is not used for model training.","d756263c":"## 1.2 Target value *SalePrice*\n\nThere are no missing values in the training set for *SalePrice*.","c4ec7c16":"\nThe target value looks normally distributed.","d07b9814":"**2.1.13 Feature *Functional***\n\nThis feature indicates the home functionality. 'Typ' is assumed unless deductions are warranted. Two houses have no information for its functionality. In fact, 93% of the houses have a typical functionality. The two missing values were replaced by 'Typ'.","fe0b22a9":"## 4.5 True vs. predicted *SalePice*\n\nThe predictions of the algorithms are plotted below to get more insights into how robust the models are. For better visibility, the first plot displays only the least expensive houses and the second plot the most expensive houses. ","04a60fca":"The house with a missing value in *Electrical* has a central air conditioning.","1525e4eb":"**2.1.14 Feature *SaleType***\n\n*SaleType* contains different types, such as conventional, cash or loan. There is only 1 missing value in *SaleType*. However, this feature is assumed to have no effect on the price of the house. Indeed, there is no correlation between *SaleType* and *SalePrice*. The feature has thus been dropped from the database.","2d5f01ea":"**2.1.8 Features *MasVnrType* & *MasVnrArea***\n\nThe features *MasVnrType* & *MasVnrArea* indicate the masonry veneer type and area. There are 23 and 24 missing values. The values are not trivial according to the countplot.","743c7dbd":"**2.1.11 Features *BsmtFullBath* & *BsmtHalfBath***\n\nThere are 2 observations which have a missing value in both *BsmtFullBath* and *BsmtHalfBath*.","12d71872":"Summarizing *FullBath* and *HalfBath* into a feature *Bath* could yield better results.","aedbad51":"# House Prices: Advanced Regression Techniques","26c630f3":"## 2.2 Feature engineering","3098bb10":"There are 107 houses with similar BsmtFinSF2 size, as the bar chart below shows.","50bd7eba":"There are 159 missing values in 'GarageYrBlt' which indicate that 159 houses have no garage. By replacing these values with 'Not available', the column would become data type 'object'. This could be critical when it comes to feature scaling and one-hot encoding. NaN values have been replaced with 0. Thereby, the column remains data type 'float' and the feature has no effect on houses without garage.","0a2dd04e":"*MSZoning* correlates most with *Alley*. The houses with missing values in *MSZoning* have no alley.","64da7be6":"**2.2.2 Feature *Bath***\n\nThere are houses without a *FullBath*. Surprisingly, houses with 0 full bathromms are more expensive than houses with 1 full bathroom. There is an additional column *HalfBath*. The effect of both features on the sale price is not trivial.","fd54c831":"**a) Feature *BsmtQual***\n\nThis feature correlates most with *OverallQual*. The two 'Not available' values that should be cleaned might be replaced based on the *OverallQual* values.","24d9d786":"**2.1.6 Feature *Utilities***\n\nThere are 2 missing values in *Utilities*. The column has 2916 observations with 'AllPub' and 1 observation with 'NoSeWa'. The missing value has been replaced by 'AllPub'.","b911fd57":"Most of the houses without alley turned out to have 'RL' in *MSZoning*. The missing values have thus been replaced by 'RL'.","71e41e43":"**2.1.5 Feature *MSZoning***","507bf35f":"## 4.2 Gradient boosting\n\n**4.2.1 Feature importance**","c4172b79":"The output below shows that these houses have no basement. Thus, they obviously have 0 full and 0 half bathrooms in the basement. The vales have been replaced with 0.","8312ed39":"Houses with missing values in *MasVnrType* have between 0 and 2 fire places. There is no clear relationship also between *MasVnrType* and *Fireplaces*.","debaf9d0":"**2.1.10 Feature *Electrical***\n\nThere is only 1 missing value in *Electrical*. The feature shows a considerable correlation with *CentralAir* which indicates whether the house has a central air conditioning.","29e9e1a4":"## 2.4 Polynomial features for numerical features\n\nPolynomial features can be powerful for identifying interaction effects in the data. Given two features *a* and *b*, the degree-3 polynomial features are *a*, *b*, *a^2*, *ab*, *b^2*, *a^3*, *a^2b*, *ab^2* and *b^3*. Care must be taken since high degree polynomials make the model prone to overfitting.\n\nIn addition, a large number of features can make models computationally expensive. Therefore, degree-3 polynomials have only been created for the 10 features that correlate most with the target value *SalePrice*.","16673f48":"**2.1.9 Feature *LotFrontage***\n\nThere are 486 missing values in *LotFrontage*. Due to the large number of missing values the feature might simply be dropped. The feature has been grouped to get better insights into how it is distributed. There are no irregularities in its distribution. *LotFrontage* correlates most with *BldgType*.","5d862e77":"The final output shows that there are consistently 79 houses without a basement after the basement columns have been cleaned.","bc01abf1":"The plots below illustrate the effect of the highest correlating features *OverallQual* and *GrLivArea* on the *SalePrice*. These features are expected to be important for the sales predictions.","5b5d2f58":"**2.1.12 Feature *KitchenQual***\n\nThere is only 1 NaN in *KitchenQual*. This house correlates highly with other features indicating the quality of the house.","b9594cfe":"The missing value has a *Foundation* of 'PConc' ('Poured Contrete').","f8fd730f":"**4.5.2 Houses with highest *SalePrice***\n\nFinding:\n- Gradient Boosting and XGBoost seem to be very robust to outliers.","a33f4e59":"**b) Feature *BsmtCond***\n\nResults have shown earlier that there is 'Not available' in the *BsmtCond* of three houses, although these houses have a basement. *BsmtCond* correlates most with *BsmtQual*. Since both features have the same values, the missing values in *BsmtCond* are thus replaced by the values for *BsmtQual*.","8fdb7b33":"There are some outliers, as the boxplot shows, but the prices of the most expensive houses seem not to be unrealistic (*OverallQual*=10, *CentralAir*=Y, *FullBath*=3, *KitchenQual*=Ex, *TotRmsAbvGrd*=10 etc.).","fb0c6905":"Both houses have an *OverallQual* of 4 while the mean *OverallQual* is round about 6. *BsmtQual* is thus not replaced by 'TA' (='Typical') which is supposed to indicate the default value, but by 'FA' (='Fair') which is one class below 'TA'.","09078a6c":"## 2.3 One-hot encoding for categorical features","5a99dcf3":"**1.3.1 Effect of selected numerical features on *SalePrice***\n\n**a) Feature *OverallQual*:**","b356fcb3":"In light of the finding that XGBoost and Gradient boosting predict outliers most accurately, houses with a predicted prices greater than 400K were predicted again only using XGBoost and Gradient boosting. The RMSE further decreased due to this.\n\n- Indeed, XGBoost and gradient boosting make higher predictions for the outliers and the RMSE decreases.","6b56d7f3":"**d) Feature *BsmtFinType2***\n\n*BsmtFinType2* shows the highest correlation with *BsmtFinSF2*. The house with the incorrect 'Not available' in *BsmtFinType2* has 479.0 type 2 finished square feet in the basement.","5be9ad84":"**1.3.2 Effect of selected categorical features on *SalePrice***\n\nFor most of the categorical features, there are considerable differences in the *SalePrice* among the groups. They are expected to have predictive power for the target as well. This is illustrated based on the features *Neighborhood* and *CentralAir*.\n\n**a) Feature *Neighborhood*:**","9d4427bd":"**4.2.2 Learning curve**","54daef46":"## 4.3 Random forest\n\n**4.3.1 Feature importance**","aeb88f6d":"# 3. Model Selection: Comparison of generalization performance\n\nFindings:\n- XGBoost and Gradient boosting perform best in terms of RMSE.\n- Polynomial features do not yield better results.","e549a636":"**2.1.7 Features *Exterior1st* & *Exterior2nd***\n\nThere is 1 missing value in 'Exterior1st' and 'Exterior2nd'.","24bd156c":"## 4.4 LASSO","b3695d04":"The masonry veneer may depend on *YearBuilt*. The numeric feature *YearBuilt* has been grouped into 10 bins, as the plot shows.","eca6be77":"### 2.6.2 Target value\n\nSimilar to the estimators, the array which captures the *SalePrice* is split into an array y_trainval and an array y_validation to be able to estimate the generalization performance.","93eabacc":"It becomes clear that there are incorrect 'Not available' values which need to be cleaned.\nThe following columns need to be considered in detail:\n\na) *BsmtQual*\n\nb) *BsmtCond*\n\nc) *BsmtExposure*\n\nd) *BsmtFinType2*","2ef9a87b":"## 1.3 Correlations\n\nThe target value *SalePrice* is highly correlated with some of the feature which indicates that the housing prices highly depend on these features. In addition, there are considerable correlations across some of the features.","9b6b2350":"The missing values in *LotFrontage* have different building types. Since *LotFrontage* is a numeric feature, it has been replaced with the mean *LotFrontage* for the respective *BldgType*.","2c7f4cdd":"Columns with missing values after cleaning:","7a5786e1":"With these insights, the average predictions of the algorithms were calculated. Thereby, the RMSE in the validation set was further reduced."}}