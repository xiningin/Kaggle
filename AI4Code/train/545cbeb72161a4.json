{"cell_type":{"c1fa7435":"code","02253fda":"code","f7d08203":"code","31b4ff14":"code","af3f28de":"code","863ca0e0":"code","47a74e7f":"code","590a8a95":"code","20c03d42":"code","ca674488":"code","ad85e2ff":"code","53af3809":"code","5e1b1e2b":"code","01c71cf1":"markdown","8e6b23cf":"markdown","1429e089":"markdown","0ba42851":"markdown","a4d09f1e":"markdown","fe750fcf":"markdown","8b5fb1bf":"markdown","a8296a1f":"markdown","afce0dfc":"markdown","5a41e5f5":"markdown"},"source":{"c1fa7435":"import nltk\nimport numpy as np\nimport random\nimport string\n\nimport bs4 as bs\nimport urllib.request\nimport re","02253fda":"# Scrap sentences from wikipedia on desired topic\nraw_html = urllib.request.urlopen('https:\/\/en.wikipedia.org\/wiki\/Deep_learning')\nraw_html = raw_html.read()\n\n# Read complete page paragraphs\narticle_html = bs.BeautifulSoup(raw_html, 'lxml')\narticle_paragraphs = article_html.find_all('p')\narticle_text = ''\n\n# concat all paragraphs\nfor para in article_paragraphs:\n    article_text += para.text\n\narticle_text = article_text.lower()\nprint(article_text[:1000])","f7d08203":"# Cleaning sentences - remove everything other than characters, full-stop and a space.\narticle_text = re.sub(r'[^A-Za-z. ]', '', article_text)","31b4ff14":"len(article_text)","af3f28de":"# ngram length\nn = 5\n\n# dictionary, which will contain n-grams combinations as keys and next character, after each occurance, as an item in list \nngrams = {}\n\n\n# iterate over (length of chars-n)\nfor i in range(len(article_text)-n):\n    # incrementaly find ngram sequences     \n    seq = article_text[i:i+n]\n    \n    # insert in dictionary as key\n    if seq not in ngrams.keys():\n        # prepare empty list to insert next possible chars\n        ngrams[seq] = []\n\n    # push a next character\n    ngrams[seq].append(article_text[i+n])","863ca0e0":"list(ngrams.items())[:2]","47a74e7f":"# assuming very first ngram as a search sequence\nsearch_sequence = article_text[0:n]\n\n# init the suggestion output\noutput = search_sequence\n\n# lenth of max chars in obtained suggestion\nsuggestion_len = 100\n\nfor i in range(suggestion_len):\n    # break, if search sequence is not present in prepared ngram dictionary\n    if search_sequence not in ngrams.keys():\n        break\n        \n    # if ngram key is available, then find the list of next possible characters\n    possible_chars = ngrams[search_sequence]\n    print(f'possible_chars:{possible_chars}')\n    \n    # Randomly select the next possbile character, most common will have more chances\n    next_char = possible_chars[random.randrange(len(possible_chars))]\n    print(f'next_char:{next_char}')\n    \n    # Update the suggestion output\n    output += next_char\n    print(f'updated complete suggestion: {output}')\n    \n    # update the search sequence now, excluding first char as we move forward\n    search_sequence = output[len(output)-n:len(output)]","590a8a95":"print(f'Search sequence: ',article_text[0:n])\nprint(f'\\nSuggestion: {output}')","20c03d42":"# ngram dictionary to keep the possible ngrams as keys and next occuring words as list items\nngrams = {}\n\n# words in a single ngram\nwords = 3\n\n# Word Tokenization\nwords_tokens = nltk.word_tokenize(article_text)\n\n# Iterate over words tokens\nfor i in range(len(words_tokens)-words):\n    # incrementaly find ngram word sequences  \n    seq = ' '.join(words_tokens[i:i+words])\n    print(seq)\n\n    # insert in dictionary as key\n    if  seq not in ngrams.keys():\n        # prepare empty list to insert next possible chars\n        ngrams[seq] = []\n\n    # push a next word\n    ngrams[seq].append(words_tokens[i+words])","ca674488":"list(ngrams.items())[:20]","ad85e2ff":"# assuming very first ngram as a search sequence\nsearch_sequence = article_text[0:n]\n\n# init the suggestion output\noutput = search_sequence\n\n# lenth of max chars in obtained suggestion\nsuggestion_len = 100\n\nfor i in range(suggestion_len):\n    # break, if search sequence is not present in prepared ngram dictionary\n    if search_sequence not in ngrams.keys():\n        break\n        \n    # if ngram key is available, then find the list of next possible characters\n    possible_chars = ngrams[search_sequence]\n    print(f'possible_chars:{possible_chars}')\n    \n    # Randomly select the next possbile character, most common will have more chances\n    next_char = possible_chars[random.randrange(len(possible_chars))]\n    print(f'next_char:{next_char}')\n    \n    # Update the suggestion output\n    output += next_char\n    print(f'updated complete suggestion: {output}')\n    \n    # update the search sequence now, excluding first char as we move forward\n    search_sequence = output[len(output)-n:len(output)]","53af3809":"# assuming very first ngram as a search sequence\nsearch_sequence = ' '.join(words_tokens[0:words])\n\n# init the suggestion output\noutput = search_sequence\n\n# lenth of max words in obtained suggestion\nfor i in range(50):\n    # break, if search sequence is not present in prepared ngram dictionary\n    if search_sequence not in ngrams.keys():\n        break\n    \n    # if ngram key is available, then find the list of next possible characters\n    possible_words = ngrams[search_sequence]\n    \n    # Randomly select the next possbile character, most common will have more chances\n    next_word = possible_words[random.randrange(len(possible_words))]\n\n    # Update the suggestion output\n    output += ' ' + next_word\n    seq_words = nltk.word_tokenize(output)\n\n    # update the search sequence now, excluding first char as we move forward\n    search_sequence = ' '.join(seq_words[len(seq_words)-words:len(seq_words)])","5e1b1e2b":"print(f'Search sequence: ',' '.join(words_tokens[0:words]))\nprint(f'\\nSuggestion: {output}')","01c71cf1":"Auto-complete or Text filler is one of the outstanding features in search bars available on various search engines and websites. It's a must to have feature on such sites because when users starts entering their searc keywords, they exactly don't know the best best word or combination of words required to produce the best search result.\n\nHere, we are going to look into the most easiest method for auto complete. Previously, we've looked into BOW and TF-IDF techniques for feature extraction from text based data, now in this kernel you'll see a practicle usecase of chars and words based N-Grams feature creation. Even though we won't be training any ML or RNN model to solve our problem, we will be producing a decent result using a simple python and text processing techniques.\n\nLet's get started","8e6b23cf":"You see, it's a lot better than char based N-Grams. Both N-Grams have their significance in different kind of problems.","1429e089":"For a desired sequence or search *search_sequence*, find a complete suggestion of specified length suggestion_len.\n \nTo do that, we'll have to keep appending characters based on ngrams keys and item list. From item list, we'll be performing selection based on probality, i.e. mostly occured next character will have a better chance of getting picked.","0ba42851":"Here idea is to find all n-gram continious char combinations and have their next occuring words (for best possible suggestion) ready in our ngram dictionary","a4d09f1e":"Please also take a look at this amazing article by Usman Malik:\n    https:\/\/stackabuse.com\/python-for-nlp-developing-an-automatic-text-filler-using-n-grams\/\n\n<br>\nLater we'll solve this auto-complete problem, using a large amount of data, using GRU and LSTM Models.","fe750fcf":"## Words based N-grams","8b5fb1bf":"## Character based N-grams","a8296a1f":"In Words N-grams, we follow the same approach, only difference is, here we consider a word as a single entity.","afce0dfc":"Mainly N-grams models can be implemented in two ways:\n    - Character based N-grams\n    - Word based N-grams\n\nLet's look at them one by one","5a41e5f5":"First few words in this sentence will make a sense, but considering it as a whole won't make much sense. Considering, we haven't used any typical RNN sentence generator, this is a decent output."}}