{"cell_type":{"2dc3c1fd":"code","f1afc59e":"code","c76bd4e2":"code","d1a2973d":"code","0c65fbed":"code","1eb5b366":"code","3872a907":"code","3071df8a":"code","0417a32d":"code","a95c8a81":"code","d5b64d0d":"code","d5b7fffe":"code","2f0df890":"code","2725b3fe":"code","963c42de":"code","c04eb128":"code","27c8b319":"code","7b5be1ed":"code","3902ec47":"code","5f83e944":"code","b6887587":"code","9110cd0a":"code","f0caee18":"code","d42d8481":"code","6b15fd4b":"code","cc2e070e":"code","9a939b75":"code","ce75504c":"code","2d95a5f3":"code","088762ec":"code","ac2c5437":"code","726b9de0":"code","6c6e42c7":"code","1a909624":"code","e1a80b3f":"code","100ac20d":"code","e40fc4cd":"code","dccc4546":"code","958d1f0b":"code","c5ddebae":"code","9634c3df":"code","07d9d012":"code","5cbbf6ec":"code","82f23a27":"code","f3b70760":"code","f15024ab":"code","9a19b54f":"markdown","3aa189f2":"markdown","e6c0fc00":"markdown","426ead77":"markdown","661ee78b":"markdown","dfbc05bd":"markdown","0b3fcbb8":"markdown","a5c7902f":"markdown","959e90c9":"markdown","efae6214":"markdown"},"source":{"2dc3c1fd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f1afc59e":"train = pd.read_csv('\/kaggle\/input\/demand-forecasting\/train_0irEZ2H.csv')\ntest = pd.read_csv('\/kaggle\/input\/demand-forecasting\/test_nfaJ3J5.csv')\nsubmit = pd.read_csv('\/kaggle\/input\/demand-forecasting\/sample_submission_pzljTaX.csv')","c76bd4e2":"train.head()","d1a2973d":"print(train.shape, test.shape, submit.shape)","0c65fbed":"train['store_sku'] = (train['store_id'].astype('str') + \"_\" + train['sku_id'].astype('str'))\ntest['store_sku'] = (test['store_id'].astype('str') + '_' + train['sku_id'].astype('str'))","1eb5b366":"train['store_sku'].head()","3872a907":"test['store_sku'].head()","3071df8a":"len(train['store_sku'].unique()) - len(test['store_sku'].unique()) ## checking if the combination of store and sku is same across train and test.","0417a32d":"len(np.intersect1d(train['store_sku'].unique(), test['store_sku'].unique())) == len(test['store_sku'].unique())","a95c8a81":"assert len(np.intersect1d(train['store_sku'].unique(), test['store_sku'].unique())) == len(test['store_sku'].unique())","d5b64d0d":"train.info()","d5b7fffe":"temp = train[train['total_price'].isnull()]['base_price']\ntemp","2f0df890":"train['total_price'] = train['total_price'].fillna(temp)","2725b3fe":"#Appending train and test together for faster manipulation of data\ntest['units_sold'] = -1\ndata = train.append(test, ignore_index = True)","963c42de":"test.head()","c04eb128":"data.head()","27c8b319":"data.info()","7b5be1ed":"print('Checking Data distribution for Train! \\n')\nfor col in train.columns:\n    print(f'Distinct entries in {col}: {train[col].nunique()}')\n    print(f'Common # of {col} entries in test and train: {len(np.intersect1d(train[col].unique(), test[col].unique()))}')","3902ec47":"data.describe()","5f83e944":"train.units_sold.describe()","b6887587":"(train[train.units_sold <= 200].units_sold).hist()","9110cd0a":"train['units_sold'].hist()","f0caee18":"np.log1p(train['units_sold']).hist()","d42d8481":"data[['base_price', 'total_price']].plot.box()","6b15fd4b":"#Making price based new features\n\ntrain['diff']=train['base_price']-train['total_price']\ntrain['relative_diff_base'] = train['diff']\/train['base_price']\ntrain['relative_diff_total'] = train['diff']\/train['total_price']\n\ntrain.head()","cc2e070e":"test['diff']=test['base_price']-test['total_price']\ntest['relative_diff_base'] = test['diff']\/test['base_price']\ntest['relative_diff_total'] = test['diff']\/test['total_price']\n\ntest.head()","9a939b75":"#Studying correlation between features and target variable\n\ntrain.columns\ncols = ['total_price', 'base_price',\n       'is_featured_sku', 'is_display_sku', 'units_sold', 'store_sku', 'diff',\n       'relative_diff_base', 'relative_diff_total']\ntrain[cols].corr()","ce75504c":"train[cols].corr().loc['units_sold']","2d95a5f3":"print(f'current # of features in cols: {len(cols)}')\ncols.remove('units_sold')\nprint(f'current # of features to be used: {len(cols)}')","088762ec":"from sklearn.model_selection import train_test_split\n\nX = train[cols]\ny = np.log1p(train['units_sold']) # Transforming target into normal via logarithmic operation\n\nXtrain, Xval, ytrain, yval = train_test_split(X, y, test_size = 0.2, random_state = 1)\nprint(Xtrain.shape, ytrain.shape, Xval.shape, yval.shape)","ac2c5437":"Xtrain.isnull().sum()","726b9de0":"from sklearn.ensemble import RandomForestRegressor\nreg = RandomForestRegressor()\nreg.fit(Xtrain, ytrain)","6c6e42c7":"def RMSLE(actual, predicted):\n\n    predicted = np.array([np.log(np.abs(x+1.0)) for x in predicted])  # doing np.abs for handling neg values  \n    actual = np.array([np.log(np.abs(x+1.0)) for x in actual])\n    log_err = actual-predicted\n    \n    return 1000*np.sqrt(np.mean(log_err**2))","1a909624":"preds = reg.predict(Xval)\nprint(f'The validation RMSLE error for baseline model is : {RMSLE(np.exp(yval), np.exp(preds))}')","e1a80b3f":"sub_preds = reg.predict(test[cols])\nsubmit['units_sold'] = np.exp(sub_preds)\nsubmit.head(2)","100ac20d":"submit.to_csv('sub_baseline_v1.csv', index = False)","e40fc4cd":"from category_encoders import TargetEncoder, MEstimateEncoder\nencoder = MEstimateEncoder()\nencoder.fit(train['store_id'], train['units_sold'])\ntrain['store_encoded'] = encoder.transform(train['store_id'], train['units_sold'])\ntest['store_encoded'] = encoder.transform(test['store_id'], test['units_sold'])","dccc4546":"encoder.fit(train['sku_id'], train['units_sold'])\ntrain['sku_encoded'] = encoder.transform(train['sku_id'], train['units_sold'])\ntest['sku_encoded'] = encoder.transform(test['sku_id'], test['units_sold'])","958d1f0b":"skus = train.sku_id.unique() #unique sku_ids\nprint(skus)","c5ddebae":"skus = train.sku_id.unique() #unique sku_ids\nprint(skus[:2])","9634c3df":"test_preds = test.copy()\ntest_preds.tail(2)","07d9d012":"def sku_model(sku, cols_to_use, reg):\n    X = train[train['sku_id'] == sku][cols_to_use]\n    y = train[train['sku_id'] == sku]['units_sold']\n    \n    Xtrain, Xval, ytrain, yval = train_test_split(X, y, test_size = 0.2, random_state = 1)\n    reg.fit(X,np.log1p(y))\n    \n    y_pred = reg.predict(Xval)\n    err = RMSLE(yval, np.exp(y_pred))\n    print(f'RMSLE for {sku} is: {err}')\n    \n    preds = reg.predict(test[test['sku_id'] == sku][cols_to_use])    \n    temp_df =  pd.DataFrame.from_dict({'record_ID': test_preds[test_preds['sku_id'] == sku]['record_ID'],\n                                       'units_sold':  np.exp(preds)})\n    return err, temp_df","5cbbf6ec":"cols_to_use = cols + ['store_encoded', 'sku_encoded']","82f23a27":"cols_to_use","f3b70760":"err = dict() # for documenting error for each sku type\nsub = pd.DataFrame(None, columns = ['record_ID', 'units_sold'])\nreg = RandomForestRegressor(random_state = 2288)\n\nfor sku in skus:\n    err[sku], temp = sku_model(sku, cols_to_use, reg)\n    sub = sub.append(temp)\n\nprint(np.mean(list(err.values())))","f15024ab":"sub.sort_values(by = ['record_ID']).to_csv('sub_sku_RF_v2.csv', index = False)","9a19b54f":" Since, the data is on (store X sku) level, let's make a separate identifier to pick it later. \n1. Below, I have concatenated the store and sku id by making new column store_sku.\n2. I have also checked if the number of such combinations is same across the train and test set. By making sure it is 0, we can rest assured that no cold-start needs to be done.","3aa189f2":"Notice that the target data is highly skewed. To make accurate predictions, we must normalise it before using.","e6c0fc00":"Comparing for base and total price. Let's see if we can gain some insights about the target data from these two.","426ead77":"Not a bad start! The time series data that we have behaves differently for different sku and hence we should try fitting multiple models for each sku\/store, i.e., different models for different store_sku combination. Let's try that out and see if that makes any improvement.","661ee78b":"Before fitting the model, we would like to encode our store and sku information. There are multiple ways of doing this:\n    1. One-hot encoding\n    2. Label encoding\n    3. Category encoding\nSince, the features here have high cardinality, we should go for Category encoding. I'll be using `MEstimateEncoder` for this purpose.\nYou can find its documentation at this link: http:\/\/contrib.scikit-learn.org\/category_encoders\/mestimate.html","dfbc05bd":"# SKU level base model fitting","0b3fcbb8":"We have good correlation of features with the target variable, hence doing a baseline regression won't be a bad start.","a5c7902f":"Notice that one entry for total_price is null in the train set. Lets replace it with the base_price for now.","959e90c9":"Your private score for this submission is : 603.706491130597, Had it been a live contest, your rank would be : 186","efae6214":"# Baseline Regression"}}