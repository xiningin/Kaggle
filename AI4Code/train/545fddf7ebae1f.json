{"cell_type":{"1a1150d7":"code","39acbe00":"code","82ff806b":"code","781f700a":"code","a3fd3724":"code","8002f20c":"code","bf771751":"code","0b8ef7e0":"code","cb17ae52":"code","0f7334e0":"code","20431239":"code","9190212b":"code","a14fef0e":"code","9aa0c0c6":"code","e79792a9":"code","01de4061":"code","69711015":"code","4cada511":"code","47130540":"code","86495b7b":"code","17bb5db2":"code","9e2cb975":"markdown","7e31f150":"markdown","a0766b9b":"markdown","c4b4ab53":"markdown","34a56ad5":"markdown","e9aca4bf":"markdown","9dafec83":"markdown","883bb7d7":"markdown","2022c627":"markdown","ec499227":"markdown","bd72afde":"markdown"},"source":{"1a1150d7":"import json\n\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport tensorflow.keras.layers as L\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split","39acbe00":"tf.random.set_seed(2020)\nnp.random.seed(2020)","82ff806b":"# This will tell us the columns we are predicting\npred_cols = ['reactivity', 'deg_Mg_pH10', 'deg_Mg_50C', 'deg_pH10', 'deg_50C']","781f700a":"y_true = tf.random.normal((32, 68, 3))\ny_pred = tf.random.normal((32, 68, 3))","a3fd3724":"def MCRMSE(y_true, y_pred):\n    colwise_mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=1)\n    return tf.reduce_mean(tf.sqrt(colwise_mse), axis=1)","8002f20c":"def gru_layer(hidden_dim, dropout):\n    return L.Bidirectional(L.GRU(\n        hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer='orthogonal'))","bf771751":"def build_model(embed_size, seq_len=107, pred_len=68, dropout=0.5, \n                sp_dropout=0.2, embed_dim=200, hidden_dim=256, n_layers=3):\n    inputs = L.Input(shape=(seq_len, 3))\n    embed = L.Embedding(input_dim=embed_size, output_dim=embed_dim)(inputs)\n    \n    reshaped = tf.reshape(\n        embed, shape=(-1, embed.shape[1],  embed.shape[2] * embed.shape[3])\n    )\n    hidden = L.SpatialDropout1D(sp_dropout)(reshaped)\n    \n    for x in range(n_layers):\n        hidden = gru_layer(hidden_dim, dropout)(hidden)\n    \n    # Since we are only making predictions on the first part of each sequence, \n    # we have to truncate it\n    truncated = hidden[:, :pred_len]\n    out = L.Dense(5, activation='linear')(truncated)\n    \n    model = tf.keras.Model(inputs=inputs, outputs=out)\n    model.compile(tf.optimizers.Adam(), loss=MCRMSE)\n    \n    return model","0b8ef7e0":"def pandas_list_to_array(df):\n    \"\"\"\n    Input: dataframe of shape (x, y), containing list of length l\n    Return: np.array of shape (x, l, y)\n    \"\"\"\n    \n    return np.transpose(\n        np.array(df.values.tolist()),\n        (0, 2, 1)\n    )","cb17ae52":"def preprocess_inputs(df, token2int, cols=['sequence', 'structure', 'predicted_loop_type']):\n    return pandas_list_to_array(\n        df[cols].applymap(lambda seq: [token2int[x] for x in seq])\n    )","0f7334e0":"data_dir = '\/kaggle\/input\/stanford-covid-vaccine\/'\ntrain = pd.read_json(data_dir + 'train.json', lines=True)\ntest = pd.read_json(data_dir + 'test.json', lines=True)\nsample_df = pd.read_csv(data_dir + 'sample_submission.csv')","20431239":"train = train.query(\"signal_to_noise >= 1\")","9190212b":"# We will use this dictionary to map each character to an integer\n# so that it can be used as an input in keras\ntoken2int = {x:i for i, x in enumerate('().ACGUBEHIMSX')}\n\ntrain_inputs = preprocess_inputs(train, token2int)\ntrain_labels = pandas_list_to_array(train[pred_cols])","a14fef0e":"x_train, x_val, y_train, y_val = train_test_split(\n    train_inputs, train_labels, test_size=.1, random_state=34, stratify=train.SN_filter)","9aa0c0c6":"public_df = test.query(\"seq_length == 107\")\nprivate_df = test.query(\"seq_length == 130\")\n\npublic_inputs = preprocess_inputs(public_df, token2int)\nprivate_inputs = preprocess_inputs(private_df, token2int)","e79792a9":"model = build_model(embed_size=len(token2int))\nmodel.summary()","01de4061":"history = model.fit(\n    x_train, y_train,\n    validation_data=(x_val, y_val),\n    batch_size=64,\n    epochs=75,\n    verbose=2,\n    callbacks=[\n        tf.keras.callbacks.ReduceLROnPlateau(patience=5),\n        tf.keras.callbacks.ModelCheckpoint('model.h5')\n    ]\n)","69711015":"fig = px.line(\n    history.history, y=['loss', 'val_loss'],\n    labels={'index': 'epoch', 'value': 'MCRMSE'}, \n    title='Training History')\nfig.show()","4cada511":"# Caveat: The prediction format requires the output to be the same length as the input,\n# although it's not the case for the training data.\nmodel_public = build_model(seq_len=107, pred_len=107, embed_size=len(token2int))\nmodel_private = build_model(seq_len=130, pred_len=130, embed_size=len(token2int))\n\nmodel_public.load_weights('model.h5')\nmodel_private.load_weights('model.h5')","47130540":"public_preds = model_public.predict(public_inputs)\nprivate_preds = model_private.predict(private_inputs)","86495b7b":"preds_ls = []\n\nfor df, preds in [(public_df, public_preds), (private_df, private_preds)]:\n    for i, uid in enumerate(df.id):\n        single_pred = preds[i]\n\n        single_df = pd.DataFrame(single_pred, columns=pred_cols)\n        single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n\n        preds_ls.append(single_df)\n\npreds_df = pd.concat(preds_ls)\npreds_df.head()","17bb5db2":"submission = sample_df[['id_seqpos']].merge(preds_df, on=['id_seqpos'])\nsubmission.to_csv('submission.csv', index=False)","9e2cb975":"## Set seed to ensure reproducibility","7e31f150":"## Evaluate training history\n\nLet's use Plotly to quickly visualize the training and validation loss throughout the epochs.","a0766b9b":"Public and private sets have different sequence lengths, so we will preprocess them separately and load models of different tensor shapes. This is possible because RNN models can accept sequences of varying lengths as inputs.","c4b4ab53":"For each sample, we take the predicted tensors of shape (107, 5) or (130, 5), and convert them to the long format (i.e. $629 \\times 107, 5$ or $3005 \\times 130, 5$):","34a56ad5":"## Load models and make predictions","e9aca4bf":"## Post-processing and submit","9dafec83":"## Load and preprocess data","883bb7d7":"Public and private sets have different sequence lengths, so we will preprocess them separately and load models of different tensor shapes.","2022c627":"## Helper functions and useful variables","ec499227":"This notebook shows you how to build a model for predicting degradation at various locations along RNA sequence. \n* We will first pre-process and tokenize the sequence, secondary structure and loop type. \n* Then, we will use all the information to train a model on degradations recorded by the researchers from OpenVaccine. \n* Finally, we run our model on the public test set (shorter sequences) and the private test set (longer sequences), and submit the predictions.\n\nFor more details, please see [this discussion post](https:\/\/www.kaggle.com\/c\/stanford-covid-vaccine\/discussion\/182303).\n\n---\n\nUpdates:\n\n* V7: Updated kernel initializer, embedding dimension, and added spatial dropout. Changed epochs and validation split. All based on [Tucker's excellent kernel](https:\/\/www.kaggle.com\/tuckerarrants\/openvaccine-gru-lstm#Training) (go give them an upvote!).\n* V8-9: Changed loss from MSE to M-MCRMSE, which is the official competition metric. See [this post](https:\/\/www.kaggle.com\/c\/stanford-covid-vaccine\/discussion\/183211) for more details. Changed number of epochs to 100.\n* V10: loss function: M-MCRMSE -> MCRMSE\n* V11: Filter `signal_to_noise` to be greater than 1, since the same was applied to private set. See [this post](https:\/\/www.kaggle.com\/c\/stanford-covid-vaccine\/discussion\/183992). \n* V12: Loss function: MCRMSE -> M-MCRMSE.\n* V13: Decrease and stratify validation set based on `SN_filter`. Increase embedding size, GRU dimensions, number of layers. All inspired from Tucker's work again.","bd72afde":"## Build and train model\n\nWe will train a bi-directional GRU model. It has three layer and has dropout. To learn more about RNNs, LSTM and GRU, please see [this blog post](https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/)."}}