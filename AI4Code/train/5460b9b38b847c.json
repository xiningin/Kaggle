{"cell_type":{"199a9ebe":"code","e8bab2bc":"code","4b826c2d":"code","2831c954":"code","2e1780b1":"code","89262061":"code","041042db":"code","fcc25393":"code","dfc59ced":"code","532dfe61":"code","bf8f728b":"code","0cbb7d51":"code","e964ed81":"code","c32ae8ed":"code","137f68e3":"code","94d2ba11":"code","bcd80b64":"code","3d39127f":"code","21e6fdc2":"code","88843ac8":"code","f2f0fb1c":"markdown","984a9522":"markdown","d028ba3e":"markdown","19e87da1":"markdown","6ce616bc":"markdown","ffa4df0f":"markdown","2a59369c":"markdown","e5076302":"markdown","30f93566":"markdown","de0e4c7f":"markdown","226bc790":"markdown","124c0cbe":"markdown","b1b083a3":"markdown","c2e14cba":"markdown","697c95b8":"markdown","f557c164":"markdown","f99f8827":"markdown"},"source":{"199a9ebe":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e8bab2bc":"data=pd.read_csv(\"..\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv\",index_col='sl_no')","4b826c2d":"data.head()","2831c954":"x=data.iloc[:,:12].values\ny=data.iloc[:,-2].values","2e1780b1":"data.isnull().sum()","89262061":"data['ssc_b'].unique()","041042db":"data['hsc_b'].unique()","fcc25393":"data['hsc_s'].unique()","dfc59ced":"data['degree_t'].unique()","532dfe61":"data['specialisation'].unique()","bf8f728b":"from sklearn.preprocessing import OneHotEncoder\nencoder=OneHotEncoder()\nx=encoder.fit_transform(x)","0cbb7d51":"from sklearn.preprocessing import LabelEncoder\nenc=LabelEncoder()\ny=enc.fit_transform(y)","e964ed81":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,random_state=20,test_size=0.2)","c32ae8ed":"from sklearn.ensemble import RandomForestClassifier\nrf=RandomForestClassifier(n_estimators=10, max_depth=5, random_state=1)\n\nrf.fit(x_train,y_train)\naccuracy=rf.score(x_test,y_test)\n\nprint(\"Random Forest accuracy is :{}\".format(accuracy))","137f68e3":"from sklearn.naive_bayes import MultinomialNB\n# Instantiate the classifier\nmnb = MultinomialNB()\n\n# Train classifier\nmnb.fit( x_train,y_train)\naccuracy=mnb.score(x_test,y_test)\nprint(\"Naive bayes accuracy is :{}\".format(accuracy))","94d2ba11":"from sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors=29)            #n_neighbors optimal value should be suqare root of n\nknn.fit(x_train,y_train)\ny_pred_knn=knn.predict(x_test)\n\n#finding accuracy and confusion matrix\naccuracy=knn.score(x_test,y_test)\nprint(\"KNN accuracy is :{}\".format(accuracy))","bcd80b64":"from sklearn import svm    \t\t\t\nC = 0.6  # SVM regularization parameter\nsvc = svm.SVC(kernel='linear', C=C).fit(x_train, y_train)\n#svc = svm.LinearSVC(C=C).fit(X, y)\nrbf_svc = svm.SVC(kernel='rbf', gamma=0.5, C=C).fit(x_train, y_train)\n# SVC with polynomial (degree 3) kernel\npoly_svc = svm.SVC(kernel='poly', degree=2, C=C).fit(x_train, y_train)\naccuracy1=svc.score(x_test,y_test)\nprint(\"SVM accuracy is :{}\".format(accuracy1))\naccuracy2=rbf_svc.score(x_test,y_test)\nprint(\"SVM rbf accuracy is :{}\".format(accuracy2))\naccuracy3=poly_svc.score(x_test,y_test)\nprint(\"SVM poly accuracy is :{}\".format(accuracy3))","3d39127f":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier(max_depth = 6,random_state = 99, max_features = None, min_samples_leaf = 5)\ndtree.fit(x_train,y_train)\naccuracy=dtree.score(x_test,y_test)\nprint(\"Decision tree accuracy is :{}\".format(accuracy))","21e6fdc2":"from sklearn.linear_model import LogisticRegression\nlr=LogisticRegression()\nlr.fit(x_train,y_train)\naccuracy=lr.score(x_test,y_test)\nprint(\"Decision tree accuracy is :{}\".format(accuracy))\n","88843ac8":"from sklearn.naive_bayes import MultinomialNB\nmodel = MultinomialNB()\nmodel.fit(x,y)\n","f2f0fb1c":"***Naive Bayes Classifier***","984a9522":"***Encode the categorical features using OneHotEncoding***","d028ba3e":"Fit the entire dataset with the Naive Bayes Classifier","19e87da1":"***Load the dataset***","6ce616bc":"***Let's check for missing values***","ffa4df0f":"# ***A model to predict if a student gets placed or not***","2a59369c":"***Clearly, Naive Bayes Clasifier gives the highest accuracy of 83.72%***","e5076302":"***Suppor Vectore Machine with different kernels***","30f93566":"***Random Forest Classifier***","de0e4c7f":"***K-Nearest Neighbors Classifier***","226bc790":"***Decision Tree Classifier***","124c0cbe":"Split the training data to test and train so as to find the best algorithm based on accuracy.","b1b083a3":"**Categorical Features:**\n* Gender\n* ssc_b\n* hsc_b\n* hsc_s\n* degree_t\n* workex\n* specialisation\n* status","c2e14cba":"***Let's now explore the features***","697c95b8":"***Logistic Regression***","f557c164":"**Numerical Features:**\n* ssc_p\n* hsc_p\n* degree_p\n* etest_p\n* mba_p\n* salary","f99f8827":"# **Campus Recruitment - Academic and Employability Factors influencing placement**"}}