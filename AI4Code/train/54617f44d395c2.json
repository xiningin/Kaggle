{"cell_type":{"5d4d509a":"code","51e4cb32":"code","a35643b1":"code","09a9ee6c":"code","60952c5a":"code","80dd85ae":"code","c66332e0":"code","391a286e":"code","d5bc32a4":"code","091c4bdf":"code","a15deabe":"code","2af205aa":"code","b6713fa1":"code","045aad1d":"code","ef831530":"code","9d93e50a":"code","b7b6a654":"code","bfad2b63":"code","a59fdee7":"code","bb2603c6":"code","bf7c71fc":"code","f3a821cc":"code","a8111c8e":"code","7ab7420b":"code","ac3af775":"code","fdfcee30":"code","29745983":"code","3d5330e2":"code","1fb93498":"code","3bcf4fac":"code","dfc3bb3a":"code","314112da":"code","3db0d7a9":"code","a3bd2398":"code","f88ae3c2":"code","e819473c":"code","a8ae0760":"code","d55279e2":"code","574fc2ac":"code","fa599553":"code","2d13f00b":"code","61fa2503":"code","c4dc3431":"code","83241391":"code","30529d28":"code","adfc7efc":"code","f24a361d":"code","598f45be":"code","81349cf0":"code","df5046f8":"code","47e89ba2":"code","59a8f160":"code","8abbf964":"code","9a27ad08":"code","66b28838":"code","217767d9":"code","5758bbc9":"code","f36769b2":"code","dbe66b70":"code","7bc8ec15":"code","0cd22c51":"code","1d9626ad":"code","a1ba0579":"code","d9468540":"code","f05f3d0b":"code","e0e78674":"code","0845d6d7":"code","89aae7d2":"code","3d38d7c9":"code","68f548d8":"code","4928c61b":"code","31935679":"code","78826508":"code","529e2c43":"code","56833200":"code","5502724c":"code","540c1821":"code","782d8ccf":"code","f6da713f":"code","d300e0ab":"code","0f487fc2":"code","251e419a":"code","feeb2938":"code","2cc95b1e":"code","35e5da03":"code","ce1ff495":"code","fb86b116":"code","3daee822":"code","b6d72ecc":"code","d73a7be9":"code","1196cca4":"code","26bdd32f":"code","f6272b6e":"code","941f90f1":"code","fb9f294f":"code","6785d371":"code","5df0e73e":"code","2e828477":"code","028ac61e":"code","7dac0d95":"code","592a0f9b":"code","2994d1bd":"code","4dbea72f":"code","5d75c593":"markdown","9649b009":"markdown","3b373ed7":"markdown","4c85bdad":"markdown","07729801":"markdown","a4de05fb":"markdown","ec179e43":"markdown","055400cc":"markdown","bad3c7f2":"markdown","2b948223":"markdown","e93405ef":"markdown","f8878f4a":"markdown","d79af667":"markdown","f90b9f44":"markdown","def8508e":"markdown","381a6c9b":"markdown","ca0e6a23":"markdown","91e3e9f3":"markdown","041a14f2":"markdown","d8e9ea5c":"markdown","905a2354":"markdown","2b5f3773":"markdown","083a5e55":"markdown","055d1f7d":"markdown","bcc95d7d":"markdown","33adb7f5":"markdown","b910f46f":"markdown","e2e4782b":"markdown","ef47f4d6":"markdown","2cac691f":"markdown","ceddabf9":"markdown","2c95d30e":"markdown","b6de26fa":"markdown","6d449aaa":"markdown","5384308a":"markdown","237146ce":"markdown","6128c4f3":"markdown","496028f4":"markdown","aa54802f":"markdown","60625e2d":"markdown","ed423ba4":"markdown","6bea50bb":"markdown","e34b703d":"markdown","522d0905":"markdown","5ed51f19":"markdown","72274428":"markdown","3136217f":"markdown","813c7459":"markdown","94be9099":"markdown","4406a477":"markdown","4ea13fec":"markdown","745744e2":"markdown","182b4e9e":"markdown","ff01e41f":"markdown","36a08043":"markdown","77d60b71":"markdown","e08c30d4":"markdown","7a8dbb84":"markdown","b006bddf":"markdown","17741f5e":"markdown","7ae8591c":"markdown","3f560189":"markdown","1ffd2b2e":"markdown","3ca580c0":"markdown","b0eda912":"markdown","24e08c68":"markdown","9d4bd5bc":"markdown","4b56ee28":"markdown","64eee093":"markdown","760668c9":"markdown","bf6b2275":"markdown","c2c99960":"markdown"},"source":{"5d4d509a":"#Import the necessary libraries(This was run on Kaggle as Colabs was crashing and getting disconnected)\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport math\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n#from google.colab import drive\n#import os\n#drive.mount('\/content\/drive',force_remount=True)","51e4cb32":"#Change to the drive location having the data file\n#os.chdir('\/content\/drive\/My Drive')","a35643b1":"#Read the csv file and assign column names as per problem statement description\nratings = pd.read_csv(\"..\/input\/amazon-electronics-data\/ratings_Electronics.csv\",\n                      names=['userId', 'productId','Rating','timestamp'])","09a9ee6c":"#Check data snapshot to see if everything looks fine, timestamp column needs to be dropped later on\nratings.head()","60952c5a":"#Number of rows is 7.82MM and number of columns is 4\nratings.shape","80dd85ae":"#Check the datatypes\n#userID, productID are object while Rating is float, timestamp is integer\nratings.dtypes","c66332e0":"#The dataset is utilizing almost 240MB of disk space due to 7.82MM rows and 4 columns\n#There will be memory issues unless we make the dataset more dense\nratings.info()","391a286e":"#Countplot of the ratings, maximum user-products have got rating as 5 \nsns.countplot(data=ratings, x='Rating');\n#ratings[\"Rating\"].value_counts().sort_values(ascending=False).plot(kind=\"bar\")","d5bc32a4":"#Find the minimum and maximum ratings - It is between 1 and 5\nprint('Minimum rating is: %d' %(ratings.Rating.min()))\nprint('Maximum rating is: %d' %(ratings.Rating.max()))","091c4bdf":"#Check for missing values - There are no missing values, so no imputation required\nprint('Number of missing values across columns: \\n',ratings.isna().sum())","a15deabe":"#Number of products (~476K) is less than number of users(~4.2MM), so item-item colaborative filtering would make sense\n#instead of user-user colaborative filtering\nprint(\"Electronic Data Summary\")\nprint(\"=\"*100)\nprint(\"\\nTotal # of Ratings :\",ratings.shape[0])\nprint(\"Total # of Users   :\", len(np.unique(ratings.userId)))\nprint(\"Total # of Products  :\", len(np.unique(ratings.productId)))\nprint(\"\\n\")\nprint(\"=\"*100)","2af205aa":"#Dropping the Timestamp column\nratings.drop(['timestamp'], axis=1,inplace=True)","b6713fa1":"#Check and find the max ratings given by user for a particular item\nmax_ratings = ratings.groupby(['userId','productId'])['Rating'].max().sort_values(ascending=False)","045aad1d":"max_ratings.head()","ef831530":"#Check and find the min ratings given by user for a particular item\nmin_ratings = ratings.groupby(['userId','productId'])['Rating'].min().sort_values(ascending=False)","9d93e50a":"min_ratings.head()","b7b6a654":"#From above min and max calculation, we see that the ratings are identical for the sample\n#However for consistency let us remove duplicates if any just to be sure\nratings.drop_duplicates(inplace=True)","bfad2b63":"#Analysis of how many product rating given by a particular user \nno_of_rated_products_per_user = ratings.groupby(by='userId')['Rating'].count().sort_values(ascending=False)\nno_of_rated_products_per_user.head()","a59fdee7":"#We have certain users who have rated only 1 product and few users have rated upto 520 products\n#However the number of rated products per user is fairly skewed seeing the 5 point summary\n#Max is 520 and 75% percentile is at 2\nno_of_rated_products_per_user.describe().astype(int).T","bb2603c6":"#Boxplot shows that we have few users who rate many items (appearing in outliers) but majority rate very few items\nsns.boxplot(data=no_of_rated_products_per_user);","bf7c71fc":"#Let us look at the quantile view to understand where the ratings are concentrated\nquantiles = no_of_rated_products_per_user.quantile(np.arange(0,1.01,0.01), interpolation='higher')","f3a821cc":"#We can see that all the ratings are clustered at the top end of the quantile\n#Basically the outliers that we saw earlier are reflected here in the peak\nplt.figure(figsize=(10,10))\nplt.title(\"Quantiles and their Values\")\nquantiles.plot()\n# quantiles with 0.05 difference\nplt.scatter(x=quantiles.index[::5], y=quantiles.values[::5], c='red', label=\"quantiles with 0.05 intervals\")\n# quantiles with 0.25 difference\nplt.scatter(x=quantiles.index[::25], y=quantiles.values[::25], c='green', label = \"quantiles with 0.25 intervals\")\nplt.ylabel('# ratings by user')\nplt.xlabel('Value at the quantile')\nplt.legend(loc='best')\nplt.show()","a8111c8e":"#We have 1,540 users who have rated more than or equal to 50 products\nprint('\\n # of rated product more than 50 per user : {}\\n'.format(sum(no_of_rated_products_per_user >= 50)) )","7ab7420b":"#Getting the new dataframe which contains users who has given 50 or more ratings\nnew_df=ratings.groupby(\"userId\").filter(lambda x:x['Rating'].count() >=50)","ac3af775":"#Products also have skewed ratings with majority of the products having very few ratings\nno_of_ratings_per_product = new_df.groupby(by='productId')['Rating'].count().sort_values(ascending=False)\n\nfig = plt.figure(figsize=plt.figaspect(.5))\nax = plt.gca()\nplt.plot(no_of_ratings_per_product.values)\nplt.title('# RATINGS per Product')\nplt.xlabel('Product')\nplt.ylabel('# ratings per product')\nax.set_xticklabels([])\n\nplt.show","fdfcee30":"#Boxplot shows that we have few products with large number of ratings, but majority have very low ratings\nsns.boxplot(data=no_of_ratings_per_product);","29745983":"#Let us look at the quantile view to understand where the ratings are concentrated\nquantiles = no_of_ratings_per_product.quantile(np.arange(0,1.01,0.01), interpolation='higher')","3d5330e2":"#We can see that all the ratings are clustered at the top end of the quantile\n#This reflects our finding above in the boxplot\nplt.figure(figsize=(10,10))\nplt.title(\"Quantiles and their Values\")\nquantiles.plot()\n# quantiles with 0.05 difference\nplt.scatter(x=quantiles.index[::5], y=quantiles.values[::5], c='red', label=\"quantiles with 0.05 intervals\")\n# quantiles with 0.25 difference\nplt.scatter(x=quantiles.index[::25], y=quantiles.values[::25], c='green', label = \"quantiles with 0.25 intervals\")\nplt.ylabel('# ratings by user')\nplt.xlabel('Value at the quantile')\nplt.legend(loc='best')\nplt.show()","1fb93498":"#Average rating of the product across users\nnew_df.groupby('productId')['Rating'].mean().head()","3bcf4fac":"new_df.groupby('productId')['Rating'].mean().sort_values(ascending=False).head()","dfc3bb3a":"#Total no of rating for product\nnew_df.groupby('productId')['Rating'].count().sort_values(ascending=False).head()","314112da":"ratings_mean_count = pd.DataFrame(new_df.groupby('productId')['Rating'].mean())","3db0d7a9":"ratings_mean_count['rating_counts'] = pd.DataFrame(new_df.groupby('productId')['Rating'].count())","a3bd2398":"#Products which have high rating have fewer user reviews as seen below\nratings_mean_count.head()","f88ae3c2":"#The maximum number of ratings received for a product is 206\nratings_mean_count['rating_counts'].max()","e819473c":"#Majority of the products have received 1 rating only and it is a right skewed distribution\nplt.figure(figsize=(8,6))\n#plt.rcParams['patch.force_edgecolor'] = True\nratings_mean_count['rating_counts'].hist(bins=100)","a8ae0760":"#We see a left skewed distribution for the ratings\n#There are clusters at each of the points 1,2,3,4,5 as that is where the means are concentrated\nplt.figure(figsize=(8,6))\nplt.rcParams['patch.force_edgecolor'] = True\nratings_mean_count['Rating'].hist(bins=100)","d55279e2":"#From the joint plot below it seems that popular products (higher ratings) tend to be rated more frequently\n#To make people more engaged (bottom of the chart) we can start by recommending them based on popularity based system and then\n#slowly graduate them to collaborative system once we have sufficient number of data points to giver personlized recommendation\nplt.figure(figsize=(8,6))\nplt.rcParams['patch.force_edgecolor'] = True\nsns.jointplot(x='Rating', y='rating_counts', data=ratings_mean_count, alpha=0.4)","574fc2ac":"#PDF and CDF for the number of ratings per product\n#PDF is left skewed as majority of the products have very few ratings\nax1 = plt.subplot(121)\nsns.kdeplot(no_of_ratings_per_product, shade=True, ax=ax1)\nplt.xlabel('No of ratings by product')\nplt.title(\"PDF\")\n\nax2 = plt.subplot(122)\nsns.kdeplot(no_of_ratings_per_product, shade=True, cumulative=True,ax=ax2)\nplt.xlabel('No of ratings by product')\nplt.title('CDF')\n\nplt.show()","fa599553":"no_of_ratings_per_user = new_df.groupby(by='userId')['Rating'].count().sort_values(ascending=False)","2d13f00b":"#PDF and CDF for the number of ratings per user\n#PDF is left skewed as majority of the users have given very few ratings\nax1 = plt.subplot(121)\nsns.kdeplot(no_of_ratings_per_user, shade=True, ax=ax1)\nplt.xlabel('No of ratings by user')\nplt.title(\"PDF\")\n\nax2 = plt.subplot(122)\nsns.kdeplot(no_of_ratings_per_user, shade=True, cumulative=True,ax=ax2)\nplt.xlabel('No of ratings by user')\nplt.title('CDF')\n\nplt.show()","61fa2503":"#Below is the bar graph showing product list of top 30 most popular products\npopular_products = pd.DataFrame(new_df.groupby('productId')['Rating'].count())\nmost_popular = popular_products.sort_values('Rating', ascending=False)\nmost_popular.head(30).plot(kind = \"bar\")","c4dc3431":"#Split the data into 70% train and 30% test\ntrain_data, test_data = train_test_split(new_df, test_size = 0.30, random_state=0)\nprint(train_data.head(5))","83241391":"#Count of user_id for each unique product as recommendation score \ntrain_data_grouped = train_data.groupby('productId').agg({'userId': 'count'}).reset_index()\ntrain_data_grouped.rename(columns = {'userId': 'score'},inplace=True)\ntrain_data_grouped.head()","30529d28":"#Sort the products on recommendation score \ntrain_data_sort = train_data_grouped.sort_values(['score', 'productId'], ascending = [0,1])     \n#Generate a recommendation rank based upon score \ntrain_data_sort['Rank'] = train_data_sort['score'].rank(ascending=0, method='first') \n#Get the top 5 recommendations \npopularity_recommendations = train_data_sort.head(5) \npopularity_recommendations ","adfc7efc":"# Use popularity based recommender model to make predictions for a user\n# As we note this list will be same for all the users\ndef recommend(user_id):     \n    user_recommendations = popularity_recommendations \n          \n    #Add user_id column for which the recommendations are being generated \n    user_recommendations['userId'] = user_id \n      \n    #Bring user_id column to the front \n    cols = user_recommendations.columns.tolist() \n    cols = cols[-1:] + cols[:-1] \n    user_recommendations = user_recommendations[cols] \n          \n    return user_recommendations ","f24a361d":"# This list is user choice, since this is popularity based recommendation method irrespective of user \n# same products will be suggested\nfind_recom = ['A15BHBF0L0HV1F','A3VVJIZXLL1QFP','AFHY3XJJ6NCAI','A2WPY1SNQPCC00','AJMJREC90WJVP']   \nfor i in find_recom:\n    print(\"Here is the recommendation for the userId: %s\\n\" %(i))\n    print(recommend(i))    \n    print(\"\\n\") ","598f45be":"train_data_sort.head()\n#print(pred)","81349cf0":"test_data.head()","df5046f8":"#Calculating the RMSE of the popularity based recommendation system\n#Rating present in the test data is the actual rating (Act_rating)\ntest_data2 = test_data.copy()\n#ratings.drop(['timestamp'], axis=1,inplace=True)\ntest_data2.drop(['userId'],axis=1,inplace=True)\ntest_data2.rename(columns = {'Rating':'Act_rating'}, inplace = True)","47e89ba2":"test_data2.head()","59a8f160":"#Count of user_id for each unique product as recommendation score \ntrain_data_grouped2 = train_data.groupby('productId').agg({'Rating': 'sum'}).reset_index()\ntrain_data_grouped2.rename(columns = {'Rating': 'Sum_rating'},inplace=True)\ntrain_data_grouped2.head()","8abbf964":"train_data_inner = pd.merge(train_data_grouped2, train_data_sort)","9a27ad08":"train_data_inner.head()","66b28838":"#Obtain the average rating of the product across users\ntrain_data_inner[\"Avg_Rating\"] = train_data_inner[\"Sum_rating\"]\/train_data_inner[\"score\"]","217767d9":"train_data_inner.head()","5758bbc9":"#Merge the train data having average rating with the test data having actual rating at product level\ntest_data_inner = pd.merge(train_data_inner, test_data2)","f36769b2":"test_data_inner.head()","dbe66b70":"#Now the merged data has both actual rating (Act_rating) and predicted rating (Avg_rating)\n#Now RMSE can be calculated\ntest_data_inner.head()","7bc8ec15":"#RMSE for popularity based recommender system is 1.09\nmse = mean_squared_error(test_data_inner[\"Act_rating\"], test_data_inner[\"Avg_Rating\"])\nrmse = math.sqrt(mse)\nprint(\"RMSE for popularity based recommendation system:\", rmse)","0cd22c51":"ratings.head()","1d9626ad":"#Upgrading pip as pakage update for turicreate was giving issues\n!pip install --upgrade pip","a1ba0579":"!pip install -U --use-feature=2020-resolver turicreate","d9468540":"#Importing turicreate\n#This package takes SFrame instead of dataframe so typecasting accordingly\nimport turicreate\ntrain_data2 = turicreate.SFrame(train_data)\ntest_data2 = turicreate.SFrame(test_data)","f05f3d0b":"#Build the popularity recommender system \npopularity_model = turicreate.popularity_recommender.create(train_data2, user_id='userId', item_id='productId', target='Rating')","e0e78674":"#Recommend for a given set of users, since there are top 5 recommendation for 5 users, total rows will be 25\npopularity_recomm = popularity_model.recommend(users=['AKM1MP6P0OYPR',\n'A2CX7LUOHB2NDG',\n'A2NWSAGRHCP8N5',\n'A2WNBOD3WNDNKT',\n'A1GI0U4ZRJA8WN'],k=5)\npopularity_recomm.print_rows(num_rows=25)","0845d6d7":"#m = turicreate.recommender.create(train_data2, user_id='userId', item_id='productId', target='Rating')\npop_rmse = popularity_model.evaluate_rmse(test_data2,'Rating')","89aae7d2":"#Get the rmse value\nprint(pop_rmse)","3d38d7c9":"#The RMSE value is 1.11 for the popularity based model\npop_rmse[\"rmse_overall\"]","68f548d8":"#Training the model for item-item similarity recommender\nitem_sim_model = turicreate.item_similarity_recommender.create(train_data2, user_id='userId', item_id='productId', target='Rating', similarity_type='cosine')","4928c61b":"#Making top 5 recommendations based on item-item similarity recommender system\nitem_sim_recomm = item_sim_model.recommend(users=['AKM1MP6P0OYPR',\n'A2CX7LUOHB2NDG',\n'A2NWSAGRHCP8N5',\n'A2WNBOD3WNDNKT',\n'A1GI0U4ZRJA8WN'],k=5)\nitem_sim_recomm.print_rows(num_rows=25)","31935679":"#Get the item-item recommender RMSE value, value is 4.39 which is much worse than popularity based recommender system\nitem_rmse = item_sim_model.evaluate_rmse(test_data2,'Rating')","78826508":"#for key in pop_rmse2.keys():\n#    print(key, '->', pop_rmse2[key])\nitem_rmse[\"rmse_overall\"]","529e2c43":"#Build a matrix based factorization model recommender system\nfactorization_model = turicreate.factorization_recommender.create(train_data2, user_id='userId', item_id='productId', target='Rating')\n","56833200":"#Recommend top 5 products for 5 users basis matrix factorization method\nfactorization_recomm = factorization_model.recommend(users=['AKM1MP6P0OYPR',\n'A2CX7LUOHB2NDG',\n'A2NWSAGRHCP8N5',\n'A2WNBOD3WNDNKT',\n'A1GI0U4ZRJA8WN'],k=5)\nfactorization_recomm.print_rows(num_rows=25)","5502724c":"fcm_rmse2 = factorization_model.evaluate_rmse(test_data2,'Rating')","540c1821":"#Matrix factorization has a better RMSE than item-item recommender but little worse than popularity based recommender system\nfcm_rmse2[\"rmse_overall\"]","782d8ccf":"#Importing Surprise and relevant packages to do some hyper parameter tuning through Grid Search\nfrom surprise import Dataset\nfrom surprise import Reader\nfrom surprise.model_selection import cross_validate\nfrom surprise.model_selection import GridSearchCV\nfrom surprise import KNNBasic, KNNWithMeans, KNNWithZScore\nfrom surprise import SVD, SVDpp, NMF\nfrom surprise import SlopeOne, CoClustering","f6da713f":"rts_gp = ratings.groupby(by=['Rating']).agg({'userId': 'count'}).reset_index()\nrts_gp.columns = ['Rating', 'Count']","d300e0ab":"#We can see that majority have rated products in the higher range\nplt.barh(rts_gp.Rating, rts_gp.Count, color='royalblue')\nplt.title('Overall Count of Ratings', fontsize=15)\nplt.xlabel('Count', fontsize=15)\nplt.ylabel('Rating', fontsize=15)\nplt.grid(ls='dotted')\nplt.show()","0f487fc2":"#Subsetting the data to keep products having at least 100 ratings\nprod_ge_100=ratings.groupby(\"productId\").filter(lambda x:x['Rating'].count() >= 100)","251e419a":"prod_ge_100.head()","feeb2938":"#Subsetting the data to keep users who have given at least 100 ratings\nuser_ge_100=ratings.groupby(\"userId\").filter(lambda x:x['Rating'].count() >= 100)","2cc95b1e":"user_ge_100.head()","35e5da03":"user_ge_100.drop(['Rating'],inplace=True,axis=1)","ce1ff495":"user_prod_ge_100 = pd.merge(prod_ge_100,user_ge_100)","fb86b116":"#Merging the datasets to get data where each product has atleast 100 ratings and each user has given atleast 100 ratings\n#This makes the matrix more dense and also allows GridSearch algo to run, else it is failing\nuser_prod_ge_100.shape","3daee822":"#Taking a 10% random sample with full data as the SVD decomposition is failing\nnew_df2 = user_prod_ge_100.sample(frac=0.1, replace=False, random_state=0)","b6d72ecc":"#train_data.shape","d73a7be9":"# Set Rating Scale from 1 to 5\n#We are running basic algorithms to check which one works best\nreader = Reader(rating_scale=(1, 5))\n\n# Load data with rating scale\n#data = Dataset.load_from_df(new_df, reader)\ndata = Dataset.load_from_df(new_df2,reader)","1196cca4":"knnbasic_cv = cross_validate(KNNBasic(), data, cv=5, n_jobs=5, verbose=False)\nknnmeans_cv = cross_validate(KNNWithMeans(), data, cv=5, n_jobs=5, verbose=False)\nknnz_cv = cross_validate(KNNWithZScore(), data, cv=5, n_jobs=5, verbose=False)","26bdd32f":"svd_cv = cross_validate(SVD(), data, cv=5, n_jobs=5, verbose=False)\nsvdpp_cv = cross_validate(SVDpp(), data, cv=5, n_jobs=5, verbose=False)\nnmf_cv = cross_validate(NMF(), data, cv=5, n_jobs=5, verbose=False)","f6272b6e":"slope_cv = cross_validate(SlopeOne(), data, cv=5, n_jobs=5, verbose=False)\ncoclus_cv = cross_validate(CoClustering(), data, cv=5, n_jobs=5, verbose=False)","941f90f1":"print('Algorithm\\t RMSE\\t\\t MAE')\nprint()\nprint('KNN Basic', '\\t', round(knnbasic_cv['test_rmse'].mean(), 4), '\\t', round(knnbasic_cv['test_mae'].mean(), 4))\nprint('KNN Means', '\\t', round(knnmeans_cv['test_rmse'].mean(), 4), '\\t', round(knnmeans_cv['test_mae'].mean(), 4))\nprint('KNN ZScore', '\\t', round(knnz_cv['test_rmse'].mean(), 4), '\\t', round(knnz_cv['test_mae'].mean(), 4))\nprint()\nprint('SVD', '\\t\\t', round(svd_cv['test_rmse'].mean(), 4), '\\t', round(svd_cv['test_mae'].mean(), 4))\nprint('SVDpp', '\\t\\t', round(svdpp_cv['test_rmse'].mean(), 4), '\\t', round(svdpp_cv['test_mae'].mean(), 4))\nprint('NMF', '\\t\\t', round(nmf_cv['test_rmse'].mean(), 4), '\\t', round(nmf_cv['test_mae'].mean(), 4))\nprint()\nprint('SlopeOne', '\\t', round(slope_cv['test_rmse'].mean(), 4), '\\t', round(slope_cv['test_mae'].mean(), 4))\nprint('CoClustering', '\\t', round(coclus_cv['test_rmse'].mean(), 4), '\\t', round(coclus_cv['test_mae'].mean(), 4))\nprint()","fb9f294f":"x_algo = ['KNN Basic', 'KNN Means', 'KNN ZScore', 'SVD', 'SVDpp', 'NMF', 'SlopeOne', 'CoClustering']\nall_algos_cv = [knnbasic_cv, knnmeans_cv, knnz_cv, svd_cv, svdpp_cv, nmf_cv, slope_cv, coclus_cv]\n\nrmse_cv = [round(res['test_rmse'].mean(), 4) for res in all_algos_cv]\nmae_cv = [round(res['test_mae'].mean(), 4) for res in all_algos_cv]\n\nplt.figure(figsize=(20,5))\n\nplt.subplot(1, 2, 1)\nplt.title('Comparison of Algorithms on RMSE', loc='center', fontsize=15)\nplt.plot(x_algo, rmse_cv, label='RMSE', color='darkgreen', marker='o')\nplt.xlabel('Algorithms', fontsize=15)\nplt.ylabel('RMSE Value', fontsize=15)\nplt.legend()\nplt.grid(ls='dashed')\n\nplt.subplot(1, 2, 2)\nplt.title('Comparison of Algorithms on MAE', loc='center', fontsize=15)\nplt.plot(x_algo, mae_cv, label='MAE', color='navy', marker='o')\nplt.xlabel('Algorithms', fontsize=15)\nplt.ylabel('MAE Value', fontsize=15)\nplt.legend()\nplt.grid(ls='dashed')\n\nplt.show()","6785d371":"# Parameter space\nsvd_param_grid = {'n_epochs': [20, 25], \n                  'lr_all': [0.007, 0.009, 0.01],\n                  'reg_all': [0.4, 0.6]}\n\nsvdpp_gs = GridSearchCV(SVDpp, svd_param_grid, measures=['rmse', 'mae'], cv=5, n_jobs=5)\nsvdpp_gs.fit(data)\n\nsvd_gs = GridSearchCV(SVD, svd_param_grid, measures=['rmse', 'mae'], cv=5, n_jobs=5)\nsvd_gs.fit(data)","5df0e73e":"print('SVDpp - RMSE:', round(svdpp_gs.best_score['rmse'], 4), '; MAE:', round(svdpp_gs.best_score['mae'], 4))\nprint('SVD   - RMSE:', round(svd_gs.best_score['rmse'], 4), '; MAE:', round(svd_gs.best_score['mae'], 4))","2e828477":"print('RMSE =', svdpp_gs.best_params['rmse'])\nprint('MAE =', svdpp_gs.best_params['mae'])","028ac61e":"print('RMSE =', svd_gs.best_params['rmse'])\nprint('MAE =', svd_gs.best_params['mae'])","7dac0d95":"param_grid = {'k': [15, 20, 25, 30, 40, 50, 60]}\n\nknnbasic_gs = GridSearchCV(KNNBasic, param_grid, measures=['rmse', 'mae'], cv=5, n_jobs=5)\nknnbasic_gs.fit(data)\n\nknnmeans_gs = GridSearchCV(KNNWithMeans, param_grid, measures=['rmse', 'mae'], cv=5, n_jobs=5)\nknnmeans_gs.fit(data)\n\nknnz_gs = GridSearchCV(KNNWithZScore, param_grid, measures=['rmse', 'mae'], cv=5, n_jobs=5)\nknnz_gs.fit(data)","592a0f9b":"x = [15, 20, 25, 30, 40, 50, 60]\ny1 = knnbasic_gs.cv_results['mean_test_rmse']\ny2 = knnbasic_gs.cv_results['mean_test_mae']\n\ny3 = knnmeans_gs.cv_results['mean_test_rmse']\ny4 = knnmeans_gs.cv_results['mean_test_mae']\n\ny5 = knnz_gs.cv_results['mean_test_rmse']\ny6 = knnz_gs.cv_results['mean_test_mae']","2994d1bd":"plt.figure(figsize=(18,5))\n\nplt.subplot(1, 2, 1)\nplt.title('K Neighbors vs RMSE', loc='center', fontsize=15)\nplt.plot(x, y1, label='KNNBasic', color='lightcoral', marker='o')\nplt.plot(x, y5, label='KNNWithZScore', color='indianred', marker='o')\nplt.plot(x, y3, label='KNNWithMeans', color='darkred', marker='o')\nplt.xlabel('K Neighbor', fontsize=15)\nplt.ylabel('RMSE Value', fontsize=15)\nplt.legend()\nplt.grid(ls='dotted')\n\nplt.subplot(1, 2, 2)\nplt.title('K Neighbors vs MAE', loc='center', fontsize=15)\nplt.plot(x, y2, label='KNNBasic', color='lightcoral', marker='o')\nplt.plot(x, y4, label='KNNWithMeans', color='indianred', marker='o')\nplt.plot(x, y6, label='KNNWithZScore', color='darkred', marker='o')\nplt.xlabel('K Neighbor', fontsize=15)\nplt.ylabel('MAE Value', fontsize=15)\nplt.legend()\nplt.grid(ls='dotted')\n\nplt.show()","4dbea72f":"from collections import defaultdict\ndef get_top_n(predictions, n=5):\n    \"\"\"Return the top-N recommendation for each user from a set of predictions.\n\n    Args:\n        predictions(list of Prediction objects): The list of predictions, as\n            returned by the test method of an algorithm.\n        n(int): The number of recommendation to output for each user. Default\n            is 5.\n\n    Returns:\n    A dict where keys are user (raw) ids and values are lists of tuples:\n        [(raw item id, rating estimation), ...] of size n.\n    \"\"\"\n\n    # First map the predictions to each user.\n    top_n = defaultdict(list)\n    for uid, iid, true_r, est, _ in predictions:\n        top_n[uid].append((iid, est))\n\n    # Then sort the predictions for each user and retrieve the k highest ones.\n    for uid, user_ratings in top_n.items():\n        user_ratings.sort(key=lambda x: x[1], reverse=True)\n        top_n[uid] = user_ratings[:n]\n\n    return top_n","5d75c593":"#### From the joint plot below it seems that popular products (higher ratings) tend to be rated more frequently\n#### To make people more engaged (bottom of the chart) we can start by recommending them based on popularity based system and then slowly graduate them to collaborative system once we have sufficient number of data points to giver personlized recommendation\n","9649b009":"#### Obtain the average rating of the product across users","3b373ed7":"## To buy or not to buy is the question...","4c85bdad":"#### Dropping the Timestamp column","07729801":"#### Analysis of how many product rating given by a particular user ","a4de05fb":"#### Let us look at the quantile view to understand where the ratings are concentrated\n","ec179e43":"#### Check the datatypes\n#### userID, productID are object while Rating is float, timestamp is integer","055400cc":"#### Importing turicreate\n#### This package takes SFrame instead of dataframe so typecasting accordingly","bad3c7f2":"# 5.Matrix based factorization model\n#### Build a matrix based factorization model","2b948223":"#### Countplot of the ratings, maximum user-products have got rating as 5 ","e93405ef":"#### Descriptive statistics on the ratings","f8878f4a":"#### Use popularity based recommender model to make predictions for a user\n#### As we note this list will be same for all the users","d79af667":"#### Count of user_id for each unique product as recommendation score ","f90b9f44":"#### Merging the datasets to get data where each product has atleast 100 ratings and each user has given atleast 100 ratings\n#### This makes the matrix more dense and also allows GridSearch algo to run, else it is failing\n#### Taking a 10% random sample with full data as the SVD decomposition is failing\n\n#### * Use K nearest neighbour (basic, with means, and normalized (z-score))\n#### * Support Vector Decomposition (SVD)\n#### * Co-clustering","def8508e":"![](https:\/\/www.theindianwire.com\/business\/amazon-prime-online-nets-over-4000-sellers-millions-in-2-days-282475\/)","381a6c9b":"#### This list is user choice, since this is popularity based recommendation method irrespective of user same products will be suggested\n","ca0e6a23":"#### We can see that all the ratings are clustered at the top end of the quantile\n#### Basically the outliers that we saw earlier are reflected here in the peak","91e3e9f3":"#### Majority of the products have received 1 rating only and it is a right skewed distribution","041a14f2":"#### We have made both popularity and collaborative recommendation system here\n#### Popularity based system had RMSE of 1.09 while collaborative item-item based system had RMSE of 4.39\n#### So the collaborative item-item based recommendation system was worse although \n#### we had to create a more dense matrix.The matrix factorization gave RMSE of 1.16\n#### This shows that popularity based and matrix factorization based system are comparable if \n#### user ratings are skewed to few items only\n#### For the rest of the portfolio we can have popularity based recommendations as they are not that\n#### much engaged with electronic products\n#### The dense matrix consists of users who have rated atlest 50 products and products having at least 50 ratings\n#### This was done to avoid system crashes and restrict the recommendation to the top users for \n#### for multiple models as it is computation intensive\n#### We can also use hyperparameter tuning to get better results in terms of RMSE \n#### Further improvement can be made by doing GridSearchCV or RandomSearchCV to understand what is\n#### the best k to reduce RMSE\n#### We are not able to plot distortion vs K value as Surprise package converts dataframe into a different format\n#### However with grid search we see that there is not much difference with k values for k means\n#### SVD gives much better results in turns of RMSE as seen from the charts above ->0.93","d8e9ea5c":"#### Merge the train data having average rating with the test data having actual rating at product level","905a2354":"#### Get the item-item recommender RMSE value, value is 4.39 which is much worse than popularity based recommender system\n","2b5f3773":"#### Count of user_id for each unique product as recommendation score ","083a5e55":"#### Matrix factorization has a better RMSE than item-item recommender but little worse than popularity based recommender system\n","055d1f7d":"#### Recommend for a given set of users, since there are top 5 recommendation for 5 users, total rows will be 25\n","bcc95d7d":"#### Build the popularity recommender system ","33adb7f5":"# 3. Popularity Based Method","b910f46f":"#### Split the data into 70% train and 30% test","e2e4782b":"#### Number of products (476K) is less than number of users(4.2MM), so item-item colaborative filtering would make sense\n#### instead of user-user colaborative filtering","ef47f4d6":"#### We can see that all the ratings are clustered at the top end of the quantile\n#### This reflects our finding above in the boxplot","2cac691f":"#### Let us look at the quantile view to understand where the ratings are concentrated","ceddabf9":"#### Below is the bar graph showing product list of top 30 most popular products","2c95d30e":"#### We have 1,540 users who have rated more than or equal to 50 products","b6de26fa":"## To recommend is the answer ...","6d449aaa":"#### Find the minimum and maximum ratings - It is between 1 and 5","5384308a":"#### Number of rows is 7.82MM and number of columns is 4","237146ce":"#### From above min and max calculation, we see that the ratings are identical for the sample\n#### However for consistency let us remove duplicates if any just to be sure","6128c4f3":"#### The maximum number of ratings received for a product is 206","496028f4":"#### We see a left skewed distribution for the ratings\n#### There are clusters at each of the points 1,2,3,4,5 as that is where the means are concentrated\n","aa54802f":"#### Reference function to get top n recommendations","60625e2d":"#### Making top 5 recommendations based on item-item similarity recommender system","ed423ba4":"#### Recommend top 5 products for 5 users basis matrix factorization method","6bea50bb":"#### Check data snapshot to see if everything looks fine, timestamp column needs to be dropped later on","e34b703d":"#### The dataset is utilizing almost 240MB of disk space due to 7.82MM rows and 4 columns\n#### There will be memory issues unless we make the dataset more dense","522d0905":"#### PDF (Probability distribution function) and CDF (Cumulative distribution function) for the number of ratings per product\n#### PDF is left skewed as majority of the products have very few ratings","5ed51f19":"#### We can see that majority have rated products in the higher range","72274428":"## 0. Import necessary libraries","3136217f":"#### We have certain users who have rated only 1 product and few users have rated upto 520 products\n#### However the number of rated products per user is fairly skewed seeing the 5 point summary\n#### Max is 520 and 75% percentile is at 2","813c7459":"#### Upgrading pip as pakage update for turicreate was giving issues","94be9099":"# 4.Item-Item based method","4406a477":"#### Subsetting the data to keep products having at least 100 ratings","4ea13fec":"#### Check and find the max and min ratings given by user for a particular item","745744e2":"#### Average rating of the product across users","182b4e9e":"![image.png](attachment:image.png)","ff01e41f":"#### Calculating the RMSE of the popularity based recommendation system\n#### Rating present in the test data is the actual rating (Act_rating)","36a08043":"#### Sort the products on recommendation score \n#### Get the top 5 recommendations","77d60b71":"#### Importing Surprise and relevant packages to do some hyper parameter tuning through Grid Search","e08c30d4":"![image.png](attachment:image.png)","7a8dbb84":"#### Check for missing values - There are no missing values, so no imputation required","b006bddf":"#### Get the rmse value which is 1.11 in this case","17741f5e":"#### Boxplot shows that we have few products with large number of ratings, but majority have very low ratings\n","7ae8591c":"#### Training the model for item-item similarity recommender","3f560189":"# Recommendation Systems on Amazon electronic data  \n\n## Data Description:  \n\nAmazon Reviews data (data source) The repository has several datasets. For this case study, we are using the Electronics dataset.\n\n## Domain: \nE-commerce\n\n## Context: \nOnline E-commerce websites like Amazon, Flipkart uses different recommendation models to provide different suggestions to different users. Amazon currently uses item-to-item collaborative filtering, which scales to massive data sets and produces high-quality recommendations in real-time. \n#### Attribute Information: \n#### \u25cf userId  : Every user identified with a unique id \n#### \u25cf productId : Every product identified with a unique id \\n \n#### \u25cf Rating  : Rating of the corresponding product by the corresponding user  \n#### \u25cf timestamp : Time of the rating ( ignore this column for this exercise) \n\n## Learning Outcomes:  \n#### \u25cf Exploratory Data Analysis \n#### \u25cf Creating a Recommendation system using real data \n#### \u25cf Collaborative filtering  \n\n## Objective:  \n\nBuild a recommendation system to recommend products to customers based on the their  previous ratings for other products. \n\n## Steps and tasks: \n\n#### 1. Read and explore the given dataset.  (Rename column\/add headers, plot histograms, find data characteristics) \n#### 2. Take a subset of the dataset to make it less sparse\/ denser. ( For example, keep the users only who has given 50 or more number of ratings )  \n#### 3. Split the data randomly into train and test dataset. ( For example, split it in 70\/30 ratio) \n#### 4. Build Popularity Recommender model.  \n#### 5. Build Collaborative Filtering model. \n#### 6. Evaluate both the models. ( Once the model is trained on the training data, it can be used to compute the error (RMSE) on predictions made on the test data.  \n#### 7. Get top - K ( K = 5) recommendations. Since our goal is to recommend new products for each user based on his\/her habits, we will recommend 5 new products.  \n#### 8. Summarise your insights.  \n\n","1ffd2b2e":"#### RMSE for popularity based recommender system is 1.09","3ca580c0":"#### Getting the new dataframe which contains users who has given 50 or more ratings","b0eda912":"![Amazon-1.jpg](attachment:Amazon-1.jpg)","24e08c68":"## 1. Read the input file","9d4bd5bc":"#### Subsetting the data to keep users who have given at least 100 ratings","4b56ee28":"#### PDF and CDF for the number of ratings per user\n#### PDF is left skewed as majority of the users have given very few ratings","64eee093":"#### Boxplot shows that we have few users who rate many items (appearing in outliers) but majority rate very few items\n","760668c9":"#### Products also have skewed ratings with majority of the products having very few ratings\n","bf6b2275":"#### Read the csv file and assign column names as per problem statement description","c2c99960":"#### Now the merged data has both actual rating (Act_rating) and predicted rating (Avg_rating)\n#### Now RMSE can be calculated"}}