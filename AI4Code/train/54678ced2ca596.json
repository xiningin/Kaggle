{"cell_type":{"f657a95c":"code","946b216b":"code","9af46dc3":"code","4b9b2d02":"code","b690de6c":"code","8407222f":"code","8740ce2e":"code","f721feb0":"code","eb7d6dd9":"code","d75285b4":"code","2a853f08":"code","cb1d88f0":"code","803b9088":"code","923c0a2e":"code","c8cdd50d":"code","89712448":"code","017f4533":"code","e22ce322":"code","b34bba7a":"code","a8b80a05":"code","3836a690":"code","03816ad3":"code","c21409ae":"code","f7c351f3":"code","9e0f08c1":"code","5c033827":"code","e7c8998f":"code","61058ab0":"code","3364095e":"code","dc65119c":"code","8f78b8d3":"code","e9bf463b":"code","3f95cb96":"code","b964eb28":"code","8e9c6d2b":"code","b29d250e":"code","66646ee7":"code","730a76cd":"code","d372dc55":"code","4e5a08e1":"code","b38c0c87":"code","fd86452a":"code","8399af72":"code","c891e35c":"code","cbbc5b73":"code","7f5569e1":"code","884c1bd6":"markdown","bbf4dd6f":"markdown","21cb961b":"markdown","59adfc55":"markdown","af8e3265":"markdown","a197c041":"markdown","fe94fab8":"markdown","0bd9eca2":"markdown","fd5c5869":"markdown","2e79a2fe":"markdown","99bf21f8":"markdown","46413729":"markdown","5a7c3dc8":"markdown","fc76e0e0":"markdown","fb4bafaf":"markdown","0297470f":"markdown","51c57491":"markdown","dd080072":"markdown","1695f844":"markdown","95b4a1fa":"markdown","8b8c4051":"markdown","30a2025d":"markdown","ad7cce43":"markdown","31d05d99":"markdown","7bdaf8e1":"markdown","92369225":"markdown","b0b6b904":"markdown","c8de316c":"markdown","05ef4086":"markdown","fc4b6197":"markdown","cd60eb68":"markdown","2c2851a0":"markdown","73f265a8":"markdown"},"source":{"f657a95c":"!pip install pytorch_model_summary","946b216b":"import tqdm\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport seaborn as sns\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport sklearn.metrics as skm\n\nfrom pytorch_model_summary import summary\nfrom sklearn.model_selection import train_test_split\nfrom transformers import BertModel, BertTokenizer, AutoConfig, pipeline\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"   # \"last_expr\"\nrandom_seed=42","9af46dc3":"train_df = pd.read_csv('..\/input\/stumbleupon\/train.tsv', delimiter='\\t')\ntrain_df.head()","4b9b2d02":"print(train_df.shape) # Shape of train dataset","b690de6c":"train_df.replace(to_replace='?', value=np.nan, inplace=True)","8407222f":"train_df.info()","8740ce2e":"train_df['alchemy_category_score'] = train_df['alchemy_category_score'].astype(dtype='float')\ntrain_df['news_front_page'] = train_df['news_front_page'].astype(dtype='float')\ntrain_df['is_news'] = train_df['is_news'].astype(dtype='float')","f721feb0":"train_df.describe()","eb7d6dd9":"train_df['boilerplate'] = train_df['boilerplate'].replace(to_replace=':null', value=':\"\"', regex=True) # replace null with empty strings","d75285b4":"train_df['label'].value_counts()\ntrain_df['label'].value_counts().plot(kind='barh', color='g')","2a853f08":"def concat_text(boilerplate_dict):\n    text = ''\n    for key in boilerplate_dict:\n        text += f\" {boilerplate_dict[key]}\"\n    return text.strip()","cb1d88f0":"train_df['total_text'] = train_df['boilerplate'].apply(lambda x: concat_text(eval(x)))","803b9088":"train_df['total_text_length (words)'] = train_df['total_text'].apply(lambda x: len(x.split()))","923c0a2e":"train_df['total_text_length (words)'].describe()\ntrain_df['total_text_length (words)'].plot(kind='hist', color='b')","c8cdd50d":"cat2idx = {label: i for i, label in enumerate(sorted(train_df['label'].unique()))}\nidx2cat = {i: label for i, label in enumerate(sorted(train_df['label'].unique()))}","89712448":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)","017f4533":"class Dataset(torch.utils.data.Dataset):\n    def __init__(self, df, source_column, target_column, max_length, transform=None):\n        self.df = df\n        self.transform = transform\n        \n        # get source and target texts\n        self.source_texts = [tokenizer(text, padding='max_length', max_length = max_length, truncation=True,\n                                return_tensors=\"pt\") for text in self.df[source_column]]\n        self.targets = self.df[target_column].map(cat2idx)\n    \n    def classes(self):\n        return self.targets\n    \n    def __len__(self):\n        return len(self.targets)\n    \n    def get_batch_labels(self, idx):\n        # Fetch a batch of labels\n        return np.array(self.targets[idx])\n    \n    def get_batch_texts(self, idx):\n        # Fetch a batch of inputs\n        return self.source_texts[idx]\n    \n    def __getitem__(self, idx):\n\n        batch_texts = self.get_batch_texts(idx)\n        batch_y = self.get_batch_labels(idx)\n\n        return batch_texts, batch_y","e22ce322":"y_train = train_df['label'] \nX_train = train_df.drop(['label'], axis=1)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, stratify=y_train,random_state=random_seed)","b34bba7a":"X_train.reset_index(drop=True, inplace=True)\ny_train.reset_index(drop=True, inplace=True)\nX_val.reset_index(drop=True, inplace=True)\ny_val.reset_index(drop=True, inplace=True)","a8b80a05":"X_train.shape\ny_train.shape\nX_val.shape\ny_val.shape","3836a690":"y_train.value_counts(normalize=True)*100\n\ny_train.value_counts().plot(kind='barh',color='green')\n\ny_val.value_counts(normalize=True)*100\n\ny_val.value_counts().plot(kind='barh',color='orange')","03816ad3":"X_train['label'] = y_train\nX_train.head()","c21409ae":"X_val['label'] = y_val\nX_val.head()","f7c351f3":"train_iter = Dataset(X_train, 'total_text', 'label', 512)\nX_train['total_text_length (words)'][0]\ntrain_iter[0]","9e0f08c1":"val_iter = Dataset(X_val, 'total_text', 'label', 512)\nX_val['total_text_length (words)'][0]\nval_iter[0]","5c033827":"train_dataloader = torch.utils.data.DataLoader(train_iter, batch_size=32, shuffle=True)\nval_dataloader = torch.utils.data.DataLoader(val_iter, batch_size=32)","e7c8998f":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() \/ len(preds))\n\ndef f1_score(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(skm.f1_score(preds.cpu(), labels.cpu(), average='weighted'))\n","61058ab0":"use_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")","3364095e":"input_size = 768\nnum_classes = len(cat2idx)","dc65119c":"class LogisticRegression(nn.Module):\n    def __init__(self):\n        super(LogisticRegression, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased', )\n        self.linear = nn.Linear(input_size, num_classes)\n    \n    def forward(self, input_id, mask):\n        # pooled outout is 768 dimension vector , _ is rest of vectors of bert model\n        _, pooled_output = self.bert(input_ids=input_id, attention_mask=mask,return_dict=False) \n        out = self.linear(pooled_output)\n        return out \n    \n    def training_step(self, batch):\n        bert_dict, labels = batch \n        out = self(bert_dict['input_ids'].squeeze(1).to(device), \n                                bert_dict['attention_mask'].to(device)) # Generate predictions\n        labels = labels.to(device)\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        bert_dict, labels = batch \n        out = self(bert_dict['input_ids'].squeeze(1).to(device), \n                                bert_dict['attention_mask'].to(device)) # Generate predictions\n        labels = labels.to(device)\n        loss = F.cross_entropy(out, labels)                             # Calculate loss\n        acc = accuracy(out, labels)                                     # Calculate accuracy\n        f1_sc = f1_score(out, labels)\n        return {'val_loss': loss, 'val_acc': acc, 'f1_score': f1_sc}\n    \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()                    # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()                       # Combine accuracies\n        batch_f1 = [x['f1_score'] for x in outputs]\n        epoch_f1 = torch.stack(batch_f1).mean()                          # Combine f1 scores\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item(), 'f1_score': epoch_f1.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(f\"Epoch [{epoch}], val_loss: {result['val_loss']:.4f}, val_acc: {result['val_acc']:.4f} val_f1_score: {result['f1_score']:.4f}\")\n    ","8f78b8d3":"def evaluate(model, val_loader):\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)","e9bf463b":"model = LogisticRegression()","3f95cb96":"for param in model.bert.parameters():\n    param.requires_grad = False\n    #print(param.shape)","b964eb28":"if torch.cuda.is_available():\n    model.cuda()","8e9c6d2b":"batch, labels  = next(iter(train_dataloader))\nmask = batch['attention_mask'].to(device)\ninput_id = batch['input_ids'].squeeze(1).to(device)\nprint(summary(model, input_id, mask))","b29d250e":"evaluate(model, val_dataloader)","66646ee7":"def fit(epochs, learning_rate, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n    optimizer=opt_func(model.parameters(), learning_rate)\n    history = []\n    for epoch in range(epochs):\n        \n        # Training Phase\n        for batch in train_loader:\n            loss = model.training_step(batch)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            \n        # Validation Phase\n        result = evaluate(model, val_loader)\n        model.epoch_end(epoch, result)\n        history.append(result)\n        \n    return history","730a76cd":"history1 = fit(5, 0.001, model, train_dataloader, val_dataloader)","d372dc55":"history2 = fit(5, 0.001, model, train_dataloader, val_dataloader)","4e5a08e1":"history3 = fit(5, 0.001, model, train_dataloader, val_dataloader)","b38c0c87":"history4 = fit(5, 0.001, model, train_dataloader, val_dataloader)","fd86452a":"test_df = pd.read_csv('..\/input\/stumbleupon\/test.tsv', delimiter='\\t')\ntest_df.head()","8399af72":"test_df['boilerplate'] = test_df['boilerplate'].replace(to_replace=':null', value=':\"\"', regex=True) # replace null with empty strings","c891e35c":"test_df['total_text'] = test_df['boilerplate'].apply(lambda x: concat_text(eval(x)))\ntest_df['total_text_length (words)'] = test_df['total_text'].apply(lambda x: len(x.split()))","cbbc5b73":"predictions = []\n\nfor text in test_df['total_text']:\n    bert_dict = tokenizer(text, padding='max_length', max_length = 512, truncation=True,\n                                return_tensors=\"pt\")\n    mask = bert_dict['attention_mask'].to(device)\n    input_id = bert_dict['input_ids'].squeeze(1).to(device)\n    output = model(input_id, mask)\n    pred = output.argmax(dim=1)\n    predictions.append(pred.cpu().numpy()[0])\n","7f5569e1":"test_df['label'] = predictions\ntest_df.to_csv('submission.csv',columns=['urlid','label'],index=False)","884c1bd6":"* We will be only using the text column for classifing the data\n* We can use python `eval` function to convert to `dict` object and access as keys\n* There are null text in `boilerplate` column replace null with empty string so that we don't have any error while using `eval` function","bbf4dd6f":"## Training Data Iterator","21cb961b":"## Desribe Data","59adfc55":"## Intialize  Model","af8e3265":"## Train and Validation Split","a197c041":"## Model Summary","fe94fab8":"# Data preprocessing and DataLoader ","0bd9eca2":"## Concatenate all the boiler plate text","fd5c5869":"## Data info","2e79a2fe":"## Validation Data Iterator","99bf21f8":"# Model Defintion","46413729":"# EDA","5a7c3dc8":"# Train Model","fc76e0e0":"# Load DataSet","fb4bafaf":"## DataLoader","0297470f":"## Clean the `boilerplate` text column","51c57491":"## Train Function","dd080072":"## Stats of text length in words","1695f844":"## Check Label Count Distrubtion","95b4a1fa":"## Replace `?` with nan as per dataset `?` is null","8b8c4051":"# Input Size to model and number of classes","30a2025d":"## Evaluate Model","ad7cce43":"## Disable training of bert embedding layer","31d05d99":"## Model Architecture","7bdaf8e1":"## Intial Model Loss and accuracy using model evaulation","92369225":"* We will take bert default 512 sequence length for tokenizer because 75% of data is near 632 length this would be better fit\n* While using tokenizer we will truncate sequence bigger than this.","b0b6b904":"# Import Libraries","c8de316c":"## Metrics","05ef4086":"## Defining Text Tokenizer using `BertTokenizer`","fc4b6197":"## Target Label Encoding","cd60eb68":"## Dataset iterator","2c2851a0":"# Test Model and Submission","73f265a8":"## Use GPU or not "}}