{"cell_type":{"75f47b2f":"code","e7eaf11e":"code","8cc3621d":"code","83aa151f":"code","6e6447f4":"code","ea1d8594":"code","dc5c6120":"code","2c1d78de":"code","372388d0":"code","9a19271a":"code","7025edfc":"code","3604f9e3":"code","c10faf78":"code","f2799d05":"code","47bdd471":"code","12c302e0":"code","cc9649d0":"code","3fd8c7b3":"code","59e0ac5b":"code","7f9f4131":"code","09060408":"code","2b210a56":"code","e84a577f":"code","d5494a5e":"code","b25f4945":"code","d0dbcaf8":"code","0ed68909":"code","e195e1a0":"code","c06e3861":"code","fdb5a196":"code","734e9977":"code","9d018753":"code","3f572153":"code","01f2c8d3":"code","06706b01":"markdown","e77da4b3":"markdown","983cdb07":"markdown","7c24769f":"markdown","4e7a52ee":"markdown","eb8eb0b7":"markdown"},"source":{"75f47b2f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e7eaf11e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n!pip install openpyxl\n%matplotlib inline\n","8cc3621d":"#Display all columns and 10 rows no shrink\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 10)","83aa151f":"anz = pd.read_excel(\"..\/input\/anz-synthesised-transaction-dataset\/ANZ synthesised transaction dataset.xlsx\")","6e6447f4":"anz.head(10)","ea1d8594":"anz.info()","dc5c6120":"#Check if null values present in the dataframe\nanz.isnull().sum()","2c1d78de":"###locate NaN value###\n\n#find rows which are all features(columns) are null \nanz[anz.isnull()]\n\n#target on certain column\nanz[anz['merchant_state'].isnull()]\n","372388d0":"\n##### Create consumer long lat #####\n#create new dataframe with split value columns\nlong_lat = anz['long_lat'].str.split(\" \", n=1, expand=True) #expand parameter is set to True, which means it will return a data frame with all separated strings in different columns\n                                                            #n=1 means separated at the first occurrence of \u201ct\u201d and not on the later occurrence since the n parameter was set to 1 (Max 1 separation in a string)\nlong_lat.head()\n#create new column to add to the anz dataframe from new created dataframe\nanz['consumer_longitude'] = long_lat[0] \nanz['consumer_latitude'] = long_lat[1]\nanz.head()\n\n\n\n##### seperate merchant long lat #####\nmerchant_lg_lt = anz['merchant_long_lat'].str.split(\" \", n=1, expand=True)\nmerchant_lg_lt.head()\nanz['merchant_longitude'] = merchant_lg_lt[0] \nanz['merchant_latitude'] = merchant_lg_lt[1]\nanz.head()\n\n#anz['consumer_longitude'].unique()\n#anz['consumer_latitude'].unique()\n#anz['merchant_longitude'].unique()\n#anz['merchant_latitude'].unique()\n\n\n#convert columns to numeric\nanz[['consumer_longitude', 'consumer_latitude', 'merchant_longitude', 'merchant_latitude']] = anz[['consumer_longitude', 'consumer_latitude', 'merchant_longitude', 'merchant_latitude']].apply(pd.to_numeric, errors='coerce')\nanz.info()\n","9a19271a":"#the dataset only contain records for 91 days, one day is missing\n\nprint(len(anz['date'].unique()))\n\n\nmin_date = anz['date'].min()\nmax_date = anz['date'].max()\ndate_range = pd.date_range(start=min_date, end=max_date)\n\n# print(date_range)\n\nfor x in anz['date'].unique():\n    if x not in date_range:\n        print(f'the missing date is {x}')\n    \nprint(f'the missing date is {x}')\n\n","7025edfc":"#confirm the one-to-one link of account_id and customer_id\n\n#create new column concat customer id and account id\nanz['cust_id_acc_id_compiled'] = anz['account'] + \"-\" + anz['customer_id']\nanz.head()\n#check unique account\nprint(len(anz['cust_id_acc_id_compiled'].unique()))\n#the length return to be 100, which means there are 100 unique combination of cust_id and acc_id\n#if more than one customers have different acc, the combination will exceed 100\n\n","3604f9e3":"##check the range of customer location\n#filtering out transactions for those who dont reside in Australia\n\ncondition_1 = ((anz['consumer_longitude'] <113) & (anz['consumer_longitude'] >154))\ncondition_2 = ((anz['consumer_latitude'] <(-44)) & (anz['consumer_latitude'] >(-10)))\n\nanz[condition_1 & condition_2]\n\n\n","c10faf78":"#visualise the overall transaction amount\nanz_purchase_overall = anz['amount']\nplt.hist(anz_purchase_overall, bins=[0,10,20,30,40,50,60,70,80,90,100,110], rwidth= 0.95)\nplt.xlabel('Overall Amount')\nplt.ylabel('Frequency')","f2799d05":"#visualise customers' average monthly transaction volume\n\ncustomer = anz.groupby('customer_id')\n#print group \n#for x, y in customer:\n#    print(x)\n#    print(y)\ncustomer['amount'].count()\n","47bdd471":"anz_avg_transac_vol = anz.groupby('customer_id')['amount'].count()","12c302e0":"plt.hist(anz_avg_transac_vol, bins=10, rwidth=0.95)\nplt.xlabel('Monthly Transaction Volume')\nplt.ylabel('No of Customers')","cc9649d0":"#segment the dataset by transaction date and time\n#visualise transaction volume over an average week\nto_date = anz['date'].dt.day_name()\nanz['Weekday'] = to_date\n#anz_transac_vol_weekday = \nww = anz.groupby('Weekday')['amount'].count()\nplt.plot(ww)\n","3fd8c7b3":"to_date = anz['date'].dt.day_name()\nanz['Weekday'] = to_date\n#ww = anz.groupby('Weekday')['amount'].count()\n\n#ww = anz.groupby('Weekday')['amount'].count()\n#yy = anz.groupby('Weekday')['date'].count()\n#print(yy)\n#print(f'{ww} is ww')\n\nday_order=[\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n\n\n#First way of converting groupby series object to dataframe\nanz_date = anz.groupby(['date', 'Weekday'])['amount'].count().reset_index()\n# print(anz_date)\n#print(type(anz_date))\nanz_weekday = anz_date.groupby(['Weekday'])['amount'].count().reset_index()                 #note here\nprint(type(anz_weekday))\nprint(anz_weekday)\n\nanz_amount = anz_date.groupby('Weekday')['amount'].sum().reset_index()                    #note here\n#print(anz_amount)\n\n#success version\nanz_weekday['average_vol'] = anz_amount['amount']\/anz_weekday['amount']\n# print(anz_weekday)\n\n\n#new column adding into the existing dataframe using transform (but by using this method the dataframe size will still maintain the same, just that avg_col column, the results for each row depends on the weekday column, eg. all wednesday will have the same result)\nanz['avg_vol'] = anz.groupby(['date', 'Weekday'])['amount'].transform('count')\n#print(anz)\n#print(anz[['date', 'Weekday', 'avg_vol']])\n#print(type(anz))\n\n\n#perfect solution\nanz_test = anz_date.groupby('Weekday')['amount'].count().reset_index(name='total_day')                 \n#print(anz_test)\n\nanz_test['total_transac'] = anz_date.groupby('Weekday')['amount'].transform('sum')  #note here\nprint(anz_test)\n\n#calculation can be performed with other dataframes also\nanz_test['avg_transac'] = anz_date.groupby('Weekday')['amount'].transform('sum')\/anz_test['total_day']  #note here\n#print(anz_test)\n\n#rearrange\n\n#anz_test['Weekday'] = anz_test['Weekday'].astype('category', categories= day_order, ordered= True)\n\nanz_test = anz_test.set_index('Weekday').reindex(day_order).reset_index()\nprint(type(anz_test))\n\"\"\"\n#notes\nthere are 3 types of object to take note\n\n1. dataframe groupby object\n ->    anz_date.groupby('Weekday')['amount']\n  \n2. dataframe series\n ->    anz_date.groupby('Weekday')['amount'].count()\n \n3. dataframe\n ->    anz_date.groupby('Weekday')['amount'].count().reset_index(name='total_day')\n \n ->    anz_weekday = anz_date.groupby(['Weekday'])----this is still a dataframe, just that weekday became the index\n--> tranform can be used to add new column to a dataframe\n\n\"\"\"\n\n\n\n\n#this method not working as in anz_weekday, the amount column already stated .count(), so the code stated below will work in the way 13 sum()\/1 count()\n# anz_weekday['avg_vol'] = anz_weekday['amount'].sum()\/anz_weekday['amount'].count()\n# print(f'anz_weekday compiled = {anz_weekday}' )\n\n\n#Plot Graph in order\nplt.plot(anz_test['Weekday'], anz_test['avg_transac'])\nplt.xlabel('Weekday')\nplt.ylabel('Average Transaction')\nplt.title('Average Weekday Transaction Volume')\n\n","59e0ac5b":"anz.head()","7f9f4131":"print(anz['extraction'].dtypes)","09060408":"print(type(anz['extraction']))","2b210a56":"#convert object to datetime\nanz['extraction_time'] =anz['extraction'].astype('datetime64[ns]')\n\nanz.head()\nanz['extraction'].dtypes\nanz.head()\n\n","e84a577f":"anz['hour'] = anz['extraction_time'].dt.strftime('%H')","d5494a5e":"anz.head()","b25f4945":"anz_hour_summary = anz.groupby('hour')['Weekday'].count().reset_index(name='transac_vol')\nanz_hour_summary.info()","d0dbcaf8":"anz['hour'].unique()","0ed68909":"anz_hour_summary['avg_transaction'] = anz_hour_summary['transac_vol'] \/ anz_hour_summary['hour'].count()\nanz_hour_summary\n","e195e1a0":"#Plot Graph in order\nplt.plot(anz_hour_summary['hour'], anz_hour_summary['avg_transaction'])\nplt.xlabel('hour')\nplt.ylabel('Average Transaction')\nplt.title('Hourly Average Transaction Volume')","c06e3861":"anz.head()","fdb5a196":"#Calculate distance based on longitude and latitude \nfrom numpy import cos, sin, arcsin, sqrt\nfrom math import radians\n\ndef haversine(row):\n    lon1 = row['consumer_longitude']\n    lat1 = row['consumer_latitude']\n    lon2 = row['merchant_longitude']\n    lat2 = row['merchant_latitude']\n    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n    dlon = lon2 - lon1 \n    dlat = lat2 - lat1 \n    a = sin(dlat\/2)**2 + cos(lat1) * cos(lat2) * sin(dlon\/2)**2\n    c = 2 * arcsin(sqrt(a)) \n    km = 6367 * c\n    return km\n\nanz['distance(km)'] = anz.apply(lambda row: haversine(row), axis=1)\nanz.head()","734e9977":"plt.hist(anz['distance(km)'], rwidth= 0.95, bins=20)\nplt.xlabel('Distance(km)')\nplt.ylabel('Frequency')\nplt.title('Distance between customer and merchants')\nplt.show()\n\n","9d018753":"anz['distance(km)'].max()","3f572153":"\nanz['geometry'] = 'POINT' + '(' + anz['merchant_long_lat'] +')'\nanz.head()\n\n# geometry = [Point(xy) for xy in zip(anz['merchant_longitude'], anz['merchant_latitude'])]\n# geometry[:3]\n\n\n","01f2c8d3":"###\n\n# import descartes\n# import geopandas as gpd\n# from shapely.geometry import Point, Polygon\n\n# fig, ax = plt.subplots(figsize = (15,15))\n# street_map.plot(ax= ax, alpha= 0.4, color= 'grey')\n# # anz[anz['WnvPresent'] ==0].plot(ax= ax, marksize= 20, color= 'blue', marker= 'o', label= 'Neg')\n# # anz[anz['WnvPresent'] ==1].plot(ax= ax, marksize= 20, color= 'red', marker= '^', label= 'Pos')\n# plt.legend(prop={'size': 15})\n\n\n","06706b01":"## 1.2 Data Preparation\n\n\nThe purpose of doing so is to \n1. Make sure that each column feature is in its correct datatype (ex. date in datetime, age in integer etc..),otherwise, then we need to tranform the wrong datatype to the correct datatype.\n2. Locate missing values\n3. Gather & understand information in the datset.\n-> The dataset contains 12043 transactions for 100 customers who have one bank account each.\n\n-> Transaction period is from 01\/08\/2018-31\/10\/2018 (92 days duration)\n\n-> The data entries are unique and have consistent formats for analysis.\n\n-> For each row, information is complete for majority of columns. Some columns contain missing data, \n   which is likely due to the nature of transaction (i.e merchants are not involved for Interbank transfer or salary payments)\n\n-> It is also noticed that there is only 91 unique dates in the dataset, suggesting the transaction records for one day are missing (2018-08-16)\n\n-> The range of each feature should also be examined which shows that there is one customer that resides outside Australia.\n\n\n\n","e77da4b3":"## 1.3 Gather some interesting overall insights about the data","983cdb07":"## 1.1 Import Dataset","7c24769f":"To validate, we could further plot the location of the customer and the merchants he\/she trades with on a map\n####Plotfailed.....failed to import geometry package####\nref: https:\/\/towardsdatascience.com\/geopandas-101-plot-any-data-with-a-latitude-and-longitude-on-a-map-98e01944b972\n","4e7a52ee":"## 1.5 Challenge: Exploring location information\n\nWe could firstly see the distribution of distance between a customer and the merchant he\/she trades with","eb8eb0b7":"## 1.4 Segment the dataset by transaction date and time"}}