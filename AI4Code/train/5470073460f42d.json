{"cell_type":{"c8997962":"code","f9aeb946":"code","c581f1a0":"code","4b4a021f":"code","7f347091":"code","99d588aa":"code","c85bfd1d":"code","2aa60269":"code","64a2a57c":"code","7d7cae1a":"code","f3bafd92":"code","9b9734d4":"code","4c6b92e7":"code","22a3a44e":"code","4e98026a":"markdown","bd98f642":"markdown","a2ed61c1":"markdown","5bda9057":"markdown","7a2c8264":"markdown","b12a1995":"markdown","81111310":"markdown","ab0f20b9":"markdown","9b9a2bf6":"markdown","7e60398e":"markdown","215f694e":"markdown","49bb0935":"markdown","ab13f352":"markdown","7702b97f":"markdown"},"source":{"c8997962":"# The libraries needed in the project\nimport pandas as pd\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report\nfrom statistics import mean\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import preprocessing\n\ndata = pd.read_csv(\"..\/input\/covid19-csv\/dataset_csv.CSV\", encoding ='latin1')\n\n# Initial contact with the data\ndata.dtypes\ndata.info()\ndata.describe()","f9aeb946":"nonnumeric_data = data.select_dtypes(include='object')\n\n# Run, for each column, and gather the string values, and their appropriate numeric substitutes.\n# There are many options available, the ones used in here are only 2.\n\n# Either the value is probabilistic treated (ie. probability of testing positive)\n# or the feature is simple, and it can be mapped directly to any number (synthetic category)\n# given the fact that those columns can easily be normalized and standardized\n\n# Analysis of common values for all non-numeric features present in the data\nfor column in nonnumeric_data.columns:\n    print (nonnumeric_data[column].value_counts())\n\n    \n# Indequate strings that will be mapped to numeric values:\n\ndata = data.replace('positive', 1) # applies to all features\ndata = data.replace('negative', 0) # applies to all features\ndata = data.replace('identified', 1) # applies to all features\ndata = data.replace('not_identified', 0) # applies to all features\ndata = data.replace('absent', 0) # applies to all features\ndata = data.replace('present', 1) # applies to all features\ndata = data.replace('not_done', 0.5) # applies to all features. The test value could either be positive or negative. \n\n# We will assume, for 'not_done' that the probabilities of outcome of the test are uniform, that is, \n# that P(positive) = P(negative), as we don't have enough data (or time) to perform distribution estimation\n# in order to acquire a better tool for guessing\/approximating the real probabilities.\n\ndata = data.replace('detected', 1) # applies to all features\ndata = data.replace('not_detected', 0) # applies to all features\ndata = data.replace('N\u00e3o Realizado', 0) # applies to all features\ndata = data.replace('normal', 1) # applies to all features\ndata['Urine - Aspect'] = data['Urine - Aspect'].astype('category')\ndata['Urine - Aspect'] = data['Urine - Aspect'].cat.codes # simple mapping suffices\ndata['Urine - Density'] = data['Urine - Density'].replace('normal', 1)\ndata['Urine - Sugar'] = data['Urine - Sugar'].replace('<1000', 500) # mean between 0 and first other category (1000)\ndata['Urine - Leukocytes'] = data['Urine - Leukocytes'].astype('category')\ndata['Urine - Leukocytes'] = data['Urine - Leukocytes'].cat.codes # simple mapping suffices\ndata['Urine - Yeasts'] = data['Urine - Yeasts'].astype('category')\ndata['Urine - Yeasts'] = data['Urine - Yeasts'].cat.codes # simple mapping suffices\ndata['Urine - Crystals'] = data['Urine - Crystals'].astype('category')\ndata['Urine - Crystals'] = data['Urine - Crystals'].cat.codes # simple mapping suffices\ndata['Urine - Color'] = data['Urine - Color'].astype('category')\ndata['Urine - Color'] = data['Urine - Color'].cat.codes # simple mapping suffices\n\n# Resolving NaN values, feature-by-feature, for the nonnumeric_data columns\ndata['Urine - pH'] = data['Urine - pH'].fillna(0)\ndata['Urine - pH'] = data['Urine - pH'].astype('float64')\n\n# simple mapping suffices is justified as follows: the data will be categorized without creating a bias, specifically, \n# as we will scale the data. Where 'scaling' is normalization and standardization of the data.\n# The data has to be scaled in order to avoid creating the well known magnitude and centrality biases.","c581f1a0":"# Removing the patient's ID, and admittance data.\n\n# The patient's ID carry no useful information, as individual points that might never repeat (the patient might never)\n# be tested for COVID-19 again, do not generalize well.\n\n# At this point in time, the patient has just arrive at the ER. There is no data on wether or not he was admitted\n# to the general ward, or to any other facility.\n\ndel data['Patient ID']\ndel data['Patient addmited to regular ward (1=yes. 0=no)']\ndel data['Patient addmited to semi-intensive unit (1=yes. 0=no)']\ndel data['Patient addmited to intensive care unit (1=yes. 0=no)']","4b4a021f":"data = data.fillna(0)","7f347091":"data = data.loc[:, data.std(axis = 0) > 0]","99d588aa":"target_feature = 'SARS-Cov-2 exam result'\nfeatures = data.loc[:, data.columns != target_feature]\ntarget = data.loc[:, data.columns == target_feature]\ntarget = target.to_numpy()\ntarget = target.ravel()\n\nfeatures = preprocessing.scale(features)\nlinear_estimator_feature_importance = LinearSVC(C=0.02, penalty=\"l1\", dual=False).fit(features, target)\nsvm_model = SelectFromModel(linear_estimator_feature_importance, prefit = True)\nrelevant_features = svm_model.transform(features)\n\nprint ('Basis of learning hyperspace has', relevant_features.shape[1], 'dimensions.')\nprint ('The reduction in use of features was', (relevant_features.shape[1]\/features.shape[1])*100, '%.')\nprint ('From', features.shape[1], 'to', relevant_features.shape[1], 'features.')\n\n","c85bfd1d":"# 1. Repeated training and testing, using probabilistic choosing of train and test data\n\n# Storing experiment results, and setting its parameters\nexperiment_results = []\nrounds_of_experiment = 33 # Gauss's theorem condition\ntest_proportion = 0.33 # 1\/3 of the data should be used as test data, for each round of experiment\nregularization_intensity = 1.0 # L2 penalization, the more distant from 1, the more agressive\n\n# It is very important to notice that several hyper-parameter optimization experiments were conducted, and they have not\n# presented improvements over the parameter values that are shown here.\n# To the curious reader, the optimization method used was the Randomized search on hyper parameters.\n\nfor k in range (1, rounds_of_experiment):\n    # Segmenting the data into training and testing data\n    X_train, X_test, Y_train, Y_test = train_test_split(relevant_features, target, test_size = test_proportion)\n    \n    # Now that we know which dimensions are more important to the learning process, we can use them to train\n    # a classifier that deals well with high dimensional sparse data, that might follow a non-linear process.\n    # As both the disease synthomatic manifestations and its evolution can be non-linear over time.\n    # For such task, the literature suggests a Support Vector Machine.\n    \n    SVM = SVC(C = regularization_intensity,\n              kernel = 'rbf', \n              gamma = 'auto', \n              shrinking = False)\n    \n    SVM.fit(X_train,Y_train)\n    predictions = SVM.predict(X_test)\n    \n    result = SVM.score(X_test, Y_test)\n    experiment_results.append(result)\n    \n    print(classification_report(Y_test, predictions))\n\n    \n# The overall score is the mean of all the scores obtained, as the score values are real numbers, generated by\n# a complex algorithm that operates on probabilistically chosen data, they are not very likely to repeat themselves.\n# Therefore, for consistency, we choose to use the mean instead of the median.\nprint(' >> Overall accuracy with', rounds_of_experiment, ' rounds of experimentation:', mean(experiment_results))\n","2aa60269":"# 2. Stratified K-fold cross-validation\n\nSVM = SVC(C = regularization_intensity, kernel = 'rbf', gamma = 'auto', shrinking = False)\n\nX = relevant_features\nY = target\n\nskf = StratifiedKFold(n_splits=33)\nfor train, test in skf.split(X, Y):\n    SVM.fit(X[train], Y[train])\n\nprint (SVM.score(X,Y))\nprint(classification_report(Y,SVM.predict(X)))\n","64a2a57c":"# 3. Single split into training and testing data\n\nSVM = SVC(C = regularization_intensity, kernel = 'rbf', gamma = 'auto', shrinking = False)\nSVM.fit(X,Y)\n\nprint (SVM.score(X,Y))\nprint(classification_report(Y,SVM.predict(X)))\n","7d7cae1a":"data = pd.read_csv(\"..\/input\/covid19-csv\/dataset_csv.CSV\", encoding ='latin1')\nnonnumeric_data = data.select_dtypes(include='object')\n\nfor column in nonnumeric_data.columns:\n    print (nonnumeric_data[column].value_counts())\n\n# Indequate strings that will be mapped to numeric values\ndata = data.replace('positive', 1) # applies to all features\ndata = data.replace('negative', 0) # applies to all features\ndata = data.replace('identified', 1) # applies to all features\ndata = data.replace('not_identified', 0) # applies to all features\ndata = data.replace('absent', 0) # applies to all features\ndata = data.replace('present', 1) # applies to all features\ndata = data.replace('not_done', 0.5) # applies to all features.\ndata = data.replace('detected', 1) # applies to all features\ndata = data.replace('not_detected', 0) # applies to all features\ndata = data.replace('N\u00e3o Realizado', 0) # applies to all features\ndata = data.replace('normal', 1) # applies to all features\ndata['Urine - Aspect'] = data['Urine - Aspect'].astype('category')\ndata['Urine - Aspect'] = data['Urine - Aspect'].cat.codes # simple mapping suffices\ndata['Urine - Density'] = data['Urine - Density'].replace('normal', 1)\ndata['Urine - Sugar'] = data['Urine - Sugar'].replace('<1000', 500) # mean between 0 and first other category (1000)\ndata['Urine - Leukocytes'] = data['Urine - Leukocytes'].astype('category')\ndata['Urine - Leukocytes'] = data['Urine - Leukocytes'].cat.codes\ndata['Urine - Yeasts'] = data['Urine - Yeasts'].astype('category')\ndata['Urine - Yeasts'] = data['Urine - Yeasts'].cat.codes\ndata['Urine - Crystals'] = data['Urine - Crystals'].astype('category')\ndata['Urine - Crystals'] = data['Urine - Crystals'].cat.codes\ndata['Urine - Color'] = data['Urine - Color'].astype('category')\ndata['Urine - Color'] = data['Urine - Color'].cat.codes\n\n# Resolving NaN values, feature-by-feature, for the nonnumeric_data columns\ndata['Urine - pH'] = data['Urine - pH'].fillna(0)\ndata['Urine - pH'] = data['Urine - pH'].astype('float64')\n\n# ------------------------------------------------------------------------------\n# From this point on, this approach differs from the solution to the first task.\n# ------------------------------------------------------------------------------\n\n# Structuring the outcome of the patient's admittance (or he's lack thereof) as a multiclass problem\ndata['Patient addmited to regular ward (1=yes. 0=no)'] = data['Patient addmited to regular ward (1=yes. 0=no)'] .replace(1,1) \ndata['Patient addmited to semi-intensive unit (1=yes. 0=no)'] = data['Patient addmited to semi-intensive unit (1=yes. 0=no)'].replace(1,2)\ndata['Patient addmited to intensive care unit (1=yes. 0=no)'] = data['Patient addmited to intensive care unit (1=yes. 0=no)'].replace(1,3)\ndata['target'] = data['Patient addmited to regular ward (1=yes. 0=no)'] + data['Patient addmited to semi-intensive unit (1=yes. 0=no)'] +                  data['Patient addmited to intensive care unit (1=yes. 0=no)']\n\n# Removing the patient's ID, and admittance data\ndel data['Patient ID']\ndel data['SARS-Cov-2 exam result']\ndel data['Patient addmited to regular ward (1=yes. 0=no)']\ndel data['Patient addmited to semi-intensive unit (1=yes. 0=no)']\ndel data['Patient addmited to intensive care unit (1=yes. 0=no)']\n\ndata = data.fillna(0)","f3bafd92":"data = data.loc[:, data.std(axis = 0) > 0]\n\ntarget_feature = 'target'\nfeatures = data.loc[:, data.columns != target_feature]\ntarget = data.loc[:, data.columns == target_feature]\ntarget = target.to_numpy()\ntarget = target.ravel()\n\nfeatures = preprocessing.scale(features)\nlinear_estimator_feature_importance = LinearSVC(C=0.02, penalty=\"l1\", dual=False).fit(features, target)\nsvm_model = SelectFromModel(linear_estimator_feature_importance, prefit = True)\nrelevant_features = svm_model.transform(features)\n\nprint ('Basis of learning hyperspace has', relevant_features.shape[1], 'dimensions.')\nprint ('The reduction in use of features was', (relevant_features.shape[1]\/features.shape[1])*100, '%.')\nprint ('From', features.shape[1], 'to', relevant_features.shape[1], 'features.')\n","9b9734d4":"# 1. Repetead train and test\n\n# Storing experiment results, and setting its parameters\nexperiment_results = []\nrounds_of_experiment = 33 # Gauss's theorem condition\ntest_proportion = 0.33 # 1\/3 of the data should be used as test data, for each round of experiment\nregularization_intensity = 1.0 # L2 penalization, the more distant from 1, the more agressive\n\nfor k in range (1, rounds_of_experiment):\n    # Segmenting the data into training and testing data\n    X_train, X_test, Y_train, Y_test = train_test_split(relevant_features, target, test_size = test_proportion)\n    \n    # Now that we know which dimensions are more important to the learning process, we can use them to train\n    # a classifier that deals well with high dimensional sparse data, that might follow a non-linear process.\n    # As both the disease synthomatic manifestations and its evolution can be non-linear over time.\n    # For such task, the literature suggests a Support Vector Machine.\n    \n    SVM = SVC(C = regularization_intensity,\n              kernel = 'rbf', \n              gamma = 'scale', \n              shrinking = False,\n              class_weight = 'balanced',\n              decision_function_shape = 'ovr')\n    \n    SVM.fit(X_train,Y_train)\n    predictions = SVM.predict(X_test)\n    \n    result = SVM.score(X_test, Y_test)\n    experiment_results.append(result)\n    \n    print(classification_report(Y_test, predictions))\n\nprint(' >> Overall accuracy with', rounds_of_experiment, ' rounds of experimentation:', mean(experiment_results))\n","4c6b92e7":"# 2. Stratified K-Fold cross-validation\n\nSVM = SVC(C = regularization_intensity,\n              kernel = 'rbf', \n              gamma = 'scale', \n              shrinking = False,\n              class_weight = 'balanced',\n              decision_function_shape = 'ovr')\n    \nX = relevant_features\nY = target\n\nskf = StratifiedKFold(n_splits=33)\nfor train, test in skf.split(X, Y):\n    SVM.fit(X[train], Y[train])\n\nprint (SVM.score(X,Y))\nprint(classification_report(Y,SVM.predict(X)))\n","22a3a44e":"# 3. Simple single train and test split\n\nSVM = SVC(C = regularization_intensity,\n              kernel = 'rbf', \n              gamma = 'scale', \n              shrinking = False,\n              class_weight = 'balanced',\n              decision_function_shape = 'ovr')  \nSVM.fit(X,Y)\n\nprint(SVM.score(X,Y))\nprint(classification_report(Y,SVM.predict(X)))\n","4e98026a":"# Results and Expectations\n\nThis notebook presents a simple albeit effective approach to classfiying COVID-19 clinical data in the absense of a great number of desirable data. I hope the reader find it both interesting and informative.\n\nWe were able to obtain reasonable accuracy for both tasks. For the first task, our method performed with a 91% accuracy using stratified K-Fold cross-validation. As for the second task, our method (a very close variant of the method used to solve first task) was able to perform with a 96% accuracy, also using stratified K-fold cross-validation.\n\nI hope this little effort can be somewhat useful to the team at Albert Einstein, and for the medical community as a whole.\n\nMy very best wishes,\nPedro.","bd98f642":"# Resolving NaN values for all strictly numerical features.\nAs this is a degenerate dataset, with a high number of dimensions and an alarming number of nan entries, it requires special attention to its data treatment.\n\nIt is important to notice that changing all nan values to a zero-magnitude synthetic value, will allow the chosen  mathematical models to operate on said column and will alter none of the explicability of the dataset itself, as it remains sparse, with same coefficient of variation.\n\nThis property **does NOT hold** for entries with magnitudes different than 0.","a2ed61c1":"# High-dimensional, sparse data\n\nAs the **dataset is high-dimensional, and sparse**, it presents a **great challenge**, classification-wise, as many techniques will produce bad outcomes as a result of the known high dimensionality problems, or the sparseness problem.\n\nThere are 2 obvious, related, but very different solutions: **dimensionality reduction** and **attribute selection**.\n\n**Dimensionality reduction** consists of mathematical transformations that in essence alter the available data, rendering the maximum amount of information that a lesser number of dimensions can convey.\n\n**Attribute selection** also delivers a dataset with reduced dimensionality, however, it does not transform any of the data. It uses mathematical methods for selecting which features convey the most information and should be preserved.\nAnd therefore, eliminates the other features.\n\nThis subtle, but *extremely important* distinction is of particular interest for the case in hand.\n> As this entire effort is done with the intent of being for medical use and medical care, and the global amount of clinical data of COVID-19 is nothing short of lacking, we cannot afford to alter\/transform the only reliable data available. **Therefore, we will use Attribute Selection**, instead of dimensionality reduction techniques.\n\nA very good initial reference for better understanding attribute selection and its intricacies, is:\nhttp:\/\/jmlr.csail.mit.edu\/papers\/volume3\/guyon03a\/guyon03a.pdf","5bda9057":"The second method we will use for feature selection is based on statistical learning theory, specifically, on support vector machines. \n\nWe will use the regularization parameter as a selecting statistic. \n\nTherefore, we will be able to find which dimensions (features) contribute the most for the learning process. The smaller the value of the regularization parameter (C), the fewer the dimensions. This is a foundational aspect of this entire approach, and therefore it deserves to be discussed a little more carefuly.\n\nWhen linear models are penalized using the L1 norm, they only have sparse solutions. This implies that a great number of coefficients they estimated are equal to zero. If we, then, perform this learning process and select only the non-zero features, given that we have used an optimum method for solving the optimization problem, we have an optimum attribute selecting algorithm.\n\nIt is important to notice that this only holds for L1 spaces (both regularization and learning). Therefore, the kernel of the SVM, the chosen technique, has to be strictly linear, and the penalty has to be the L1 norm.","7a2c8264":"# Second Task\n\nPredicting the patient's admittance: wheter or not he was admitted, and if indeed he was, to which facility (general ward, semi-intensive care, or ICU) he was assigned.\n\n","b12a1995":"# Attribute Selection\n\nExactly the same approach used for the first task.","81111310":"# Experiment designs\n\nIn order to fully support the generalization abilities of the proposed method, we will present three distinct experimental methods: repetead training and testing with probabilistic choosing of train and test sets, stratified K-fold cross-validation, and a simple training-testing using a single train and test split on all the data.\n\nThis will ensure the reader that the results obtained are reproducible, statistically significant, and could not de improved using the same amount of data and the same method. **But more importantly, it strongly argues that the learning process is not presenting results that are tainted with overfitting problems.**","ab0f20b9":"# Diagnosis of COVID-19 and its clinical spectrum\n\nProfessor Yasser Abu-Mostafa, of Caltech, famously said \"if you have data, we're in business. If not, well, then you're out of luck\". This anedotical quote beautifully sumarises the most significant shortcoming of the entire field of applied sciences: data is critical. Its abundance or shotness can be the decisive difference between a sucessful or a failed experiment. The field of AI\/ML is no exception: the more data we have, the more options we've got.\n\nThis somewhat verbose introduction was intended to prepare the reader for a very simple fact. In the world, today, we do not have an abundant amount of clinical data of COVID-19 patients. Therefore, the success of our endeavours in trying to classify COVID-19 data have a clear upper bound: the amount of information contained within our very limited available data. We may not have all possible behaviors in our datasets. We may not even have most of them. In this notebook, we present our attempt of working despite those (strong) possibilities.\n\nCan we remedy this scenario? We can certainly try. The literature proposes many approaches for investigating problems with small amounts of available data: synthetic data generation based on known statistics of the real data, data augmentation, re-sampling, and many other ingenious techniques.\n\nIn this approach, however, we will not use any of those techniques. We resort only to math. Indeed, this is a mathematical approach to trying to design a comprehensive, and most of all useful, workaround our shortness of data.\n\nThis somewhat uncommon choice can be justified. This is a medical application, its results will hopefully help Physicians design efficient protocols for helping people, those who have fallen ill of COVID-19, and those who haven't, alike. Hence, accuracy is very important, but as important are our work hypothesis. We cannot assume far-reaching logics, nor can we 'improvise and see what we get', as the byproduct of the work has to be (ideally) trust worthy.\nFor this reason alone, we will forbid ourselves of using any technique that cannot be rigourously (to the best of our own limited mathematical ability) justified.\n\nA few very important sources will be presented as we go along the code. Moreover, we discuss the reasons behind each of our choices as they are made along the design of the presented solution.","9b9a2bf6":"# Data treatment\n\nUp to this point in the code, everything is exactly the same as for the first task.","7e60398e":"# Experiment designs\n\nAgain, exactly the same as before, for the same reasons.","215f694e":"# First task\n\nPredicting how the patient will test for SARS-COV-2.","49bb0935":"# Understanding the data - Pre-processing\n\nThe first and foremost step into any sort of analysis process is to identify, measure and quantify the available data.\nThus, such is the purpose of this initial script, to perform what the literature calls pre-processing.\nFirst, we must be able to reand and understand the contents of the data file. To simplify what can be simplified, we can\nexport the data from excel file format to a csv file format.","ab13f352":"For our purposes, as **the tasks can both be interpreted as classification problems**, in the** ideal scenario the entire dataset is strictly numeric**. Therefore, we must first identify all non-numeric data, and treat it accordingly.","7702b97f":"# Attribute Selection\n\nShannon-Nyquist's theorem ensures that information is contained within variation.\nThis allows us to use the first method to select usefull attributes: **Variance Threshold**\nIf a feature has zero variance, it offers zero information, and ought to be removed.\n"}}