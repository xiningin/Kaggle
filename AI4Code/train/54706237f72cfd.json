{"cell_type":{"8330b678":"code","71c98ac9":"code","9273df8d":"code","ab2bd21f":"code","62bb96b9":"code","49e905ad":"code","b7abe223":"code","f194ab75":"code","a52e5f12":"code","faaee0ee":"code","e92a0d8d":"code","f3bd1000":"code","e7c716c6":"code","458b54e7":"code","3a091b7b":"code","a77e29d4":"code","0e42a2fd":"code","5cf77630":"code","7c843fe1":"code","d014b7f3":"code","aa335320":"code","96ade8cb":"code","0ec5ae5c":"code","4010f1a6":"code","0cf76977":"code","f91c8e4f":"code","a4e41e3e":"code","b958751c":"code","bb9f0a15":"code","3bd0b103":"code","49d5b155":"code","e64f8ad7":"code","620ed8d3":"code","2e2c89e2":"code","0d1f8c05":"code","49368626":"code","6a554418":"code","98fd04ce":"code","d3609ef2":"code","fa0ae252":"code","d0811cee":"code","6c43763d":"markdown","112da366":"markdown","ba9320d7":"markdown","0d57b71e":"markdown","1d421da3":"markdown","f05b3ff8":"markdown","0d388d81":"markdown","b02d05bd":"markdown","eb4e4f2c":"markdown","a70d96f4":"markdown","7132f16e":"markdown","cc84fa89":"markdown","b1d39cfe":"markdown"},"source":{"8330b678":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","71c98ac9":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\nimport os\nimport numpy as np\nfrom datetime import datetime\nfrom collections  import Counter\nfrom nltk import word_tokenize\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport calendar\nfrom wordcloud import WordCloud ,STOPWORDS\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nprint(os.listdir(\"..\/input\"))\nfrom gensim.models import word2vec\nfrom sklearn.manifold import TSNE\nfrom nltk import pos_tag\nfrom nltk.help import upenn_tagset\nimport gensim\nimport matplotlib.colors as mcolors\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tag import pos_tag\nimport xml.etree.cElementTree as ET\nfrom collections import OrderedDict\nimport json\nfrom nltk import jaccard_distance\nfrom nltk import ngrams\n#import textstat\nplt.style.use('ggplot')","9273df8d":"import os\nimport re #search in strings.\n\nimport plotly.plotly as py\nimport cufflinks as cf\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nfrom wordcloud import WordCloud\n#import textstat\n\npd.set_option('max_colwidth', 10000)  # this is important because the requirements are sooooo long \n\nimport warnings\nwarnings.filterwarnings('ignore')   # get rid of the matplotlib warnings","ab2bd21f":"import os\nfiles=[dir for dir in os.walk('..\/input\/cityofla')]\nfor file in files:\n    print(os.listdir(file[0]))\n    print(\"\\n\")","62bb96b9":"for subfold in os.listdir(\"..\/input\/cityofla\/CityofLA\/\"):\n    print(subfold)","49e905ad":"job_dir= '..\/input\/cityofla\/CityofLA\/Job Bulletins'\nlistOfFile = os.listdir(job_dir)\nlistOfFile","b7abe223":"bulletin_dir = '..\/input\/cityofla\/CityofLA\/Job Bulletins\/'\nadditional_data_dir = '..\/input\/cityofla\/CityofLA\/Additional data\/'\n","f194ab75":"#Add 'FILE_NAME', 'JOB_CLASS_TITLE', 'JOB_CLASS_NO' ,'OPEN_DATE'\ndata_list = []\nfor filename in os.listdir(bulletin_dir):\n    with open(bulletin_dir + \"\/\" + filename, 'r', errors='ignore') as f:\n        job_class_title = ''\n        for line in f.readlines():\n            #Insert code to parse job bulletins\n            if \"Open Date:\" in line:\n                job_bulletin_date = line.split(\"Open Date:\")[1].split(\"(\")[0].strip()\n            if \"Class Code:\" in line:\n                job_class_no = line.split(\"Class Code:\")[1].strip()\n            if len(job_class_title)<2 and len(line.strip())>1:\n                job_class_title = line.strip()\n        data_list.append([filename, job_bulletin_date, job_class_title, job_class_no])","a52e5f12":"import pandas as pd\nimport numpy as np\ndf = pd.DataFrame(data_list)\ndf.columns = [\"START FILE_NAME\", \"OPENING_DATE\", \"JOB_CLASS_TITLE\", \"JOB_CLASS_NO\"]\ndf.head()","faaee0ee":"import pandas as pd\nimport numpy as np\ninput_dir =  '..\/input\/cityofla\/CityofLA\/Job Bulletins'\ndef getListOfFiles(dirName):\n    \n#list of file and sub directories and names in the given directory \n    listOfFile = os.listdir(dirName)\n    allFiles = list()\n    # Iterate all the entries\n    for entry in listOfFile:\n    # Create full path\n        fullPath = os.path.join(dirName, entry)\n        # If entry is a directory then get the list of files in this directory \n        if os.path.isdir(fullPath):\n            allFiles = allFiles + getListOfFiles(fullPath)\n        else:\n            allFiles.append(fullPath)\n    return allFiles\nlistOfFiles = getListOfFiles(input_dir)\ndf_bulletins = pd.DataFrame(listOfFiles, columns = ['job_position'])\ndf_bulletins.head()","e92a0d8d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\nimport os\nimport numpy as np\n# Clean up of the job_position name\ndf_positions = pd.DataFrame()\ndf_positions['job_position'] = (df_bulletins['job_position']\n                                .str.replace(input_dir, '', regex=False)\n                                .str.replace('.txt', '', regex=False)\n                                .str.replace('\\d+', '')\n                                .str.replace(r\"\\s+\\(.*\\)\",\"\")\n                                .str.replace(r\"REV\",\"\"))\n\n#Remove the numbers\ndf_positions['class_code'] = (df_bulletins['job_position']\n                              .str.replace(input_dir, '', regex=False)\n                              .str.replace('.txt', '', regex=False)\n                              .str.extract('(\\d+)'))\n\ndisplay(df_positions.head())\n# Add the Text fields of Salary, Duties and Minimum REQ","f3bd1000":"headings = {}\nfor filename in os.listdir(bulletin_dir):\n    with open(bulletin_dir + \"\/\" + filename, 'r', errors='ignore') as f:\n        for line in f.readlines():\n            line = line.replace(\"\\n\",\"\").replace(\"\\t\",\"\").replace(\":\",\"\").strip()\n            \n            if line.isupper():\n                if line not in headings.keys():\n                    headings[line] = 1\n                else:\n                    count = int(headings[line])\n                    headings[line] = count+1\n","e7c716c6":"del headings['$103,606 TO $151,484'] #This is not a heading, it's an Annual Salary component\nheadingsFrame = []\nfor i,j in (sorted(headings.items(), key = lambda kv:(kv[1], kv[0]), reverse = True)):\n    headingsFrame.append([i,j])\nheadingsFrame = pd.DataFrame(headingsFrame)\nheadingsFrame.columns = [\"Heading\",\"Count\"]\n#headingsFrame.head()","458b54e7":"#Convert the txt files to a table:\nimport glob\npath = input_dir =  '..\/input\/cityofla\/CityofLA\/Job Bulletins'\nall_files = glob.glob(path + \"\/*.txt\")\nli = []\n\nfor filename in all_files:\n    with open (filename, \"r\",errors='replace') as myfile:\n        data=pd.DataFrame(myfile.readlines())\n        #df = pd.read_csv(filename, header=0,error_bad_lines=False, encoding='latin-1')\n    li.append(data)\nframe = pd.concat(li, axis=1, ignore_index=True)\n#pd.read_csv(listOfFiles,header = None)\nframe = frame.replace('\\n','', regex=True)\n","3a091b7b":"frame.head(16)","a77e29d4":"import tkinter\nfrom tkinter import Frame\nimport pandas as pd\nimport numpy as np\n #Here the loop should start, for each text file do:\ndef getString(col_i, frame):\n    try:\n        filter = frame[col_i] != \"\"\n        bulletin = frame[col_i][filter]\n        #display(salary)\n        isal = min(bulletin[bulletin.str.contains('SALARY',na=False)].index.values) #take the sum to convert the array to an int...TO CHANGE\n        inot = min(bulletin[bulletin.str.contains('NOTES',na=False)].index.values) # NOTES\n        idut = min(bulletin[bulletin.str.contains('DUTIES',na=False)].index.values) # DUTIES\n        ireq = min(bulletin[bulletin.str.contains('REQUIREMENT',na=False)].index.values) #REQUIREMENTS\n        ipro = min(bulletin[bulletin.str.contains('PROCESS',na=False)].index.values) # PROCESS NOTES\n\n        #isal = sum(bulletin.loc[bulletin == 'ANNUAL SALARY'].index.values) #take the sum to convert the array to an int...TO CHANGE\n        #inot = sum(bulletin.loc[bulletin == 'NOTES:'].index.values) # NOTES\n        #idut = sum(bulletin.loc[bulletin == 'DUTIES'].index.values) # DUTIES\n        #ireq = sum(bulletin.loc[bulletin == '(.*)REQUIREMENTS(.*)'].index.values) #REQUIREMENTS\n        #ipro = sum(bulletin.loc[bulletin == '(.*)PROCESS(.*)'].index.values) # PROCESS NOTES\n\n        icode = min(bulletin[bulletin.str.contains('Class Code',na=False)].index.values)\n        class_code = sum(bulletin.str.extract('(\\d+)').iloc[icode].dropna().astype('int'))\n        salary = (bulletin.loc[isal+1:inot-1]).to_string()\n        duties = (bulletin.loc[idut+1:ireq-1]).to_string()\n        requirements = (bulletin.loc[ireq+1:ipro-1]).to_string()\n        return (class_code, salary, duties, requirements)\n    except:\n        return (np.nan,np.nan,np.nan,np.nan)\njobsections = pd.DataFrame()\n   #getString(0,bulletin)\nfor col_i in range(frame.shape[1]):\n    #print(col_i)\n    #print(list(getString(col_i,frame)))\n    prop = getString(col_i,frame)\n    prop = pd.DataFrame(list(prop)).T\n    jobsections = jobsections.append(prop)","0e42a2fd":"jobsections.head()","5cf77630":"import pandas as pd\nimport numpy as np\njobsections.columns = ['class_code','salary','duties','requirements']\njobsections['class_code'] = pd.to_numeric(jobsections['class_code'],downcast='integer')\ndf_positions['class_code'] = pd.to_numeric(df_positions['class_code'], downcast='integer')\ndf_positions['class_code']\ndf_jobs = df_positions.merge(jobsections, left_on='class_code',right_on='class_code', how='outer')\ndisplay(df_jobs.dropna())","7c843fe1":"#Add 'REQUIREMENT_SET_ID','REQUIREMENT_SUBSET_ID'\nrequirements = []\nrequirementHeadings = [k for k in headingsFrame['Heading'].values if 'requirement' in k.lower()]\nfor filename in os.listdir(bulletin_dir):\n    with open(bulletin_dir + \"\/\" + filename, 'r', errors='ignore') as f:\n        readNext = 0\n        isNumber=0\n        prevNumber=0\n        prevLine=''\n        \n        for line in f.readlines():\n            clean_line = line.replace(\"\\n\",\"\").replace(\"\\t\",\"\").replace(\":\",\"\").strip()   \n            if readNext == 0:                         \n                if clean_line in requirementHeadings:\n                    readNext = 1\n            elif readNext == 1:\n                if clean_line in headingsFrame['Heading'].values:\n                    if isNumber>0:\n                        requirements.append([filename,prevNumber,'',prevLine])\n                    break\n                elif len(clean_line)<2:\n                    continue\n                else:\n                    rqrmntText = clean_line.split('.')\n                    if len(rqrmntText)<2:\n                        requirements.append([filename,'','',clean_line])\n                    else:                        \n                        if rqrmntText[0].isdigit():\n                            if isNumber>0:\n                                requirements.append([filename,prevNumber,'',prevLine])\n                            isNumber=1\n                            prevNumber=rqrmntText[0]\n                            prevLine=clean_line\n                        elif re.match('^[a-z]$',rqrmntText[0]):\n                            requirements.append([filename,prevNumber,rqrmntText[0],prevLine+'-'+clean_line])\n                            isNumber=0\n                        else:\n                            requirements.append([filename,'','',clean_line])","d014b7f3":"import pandas as pd\nimport numpy as np\ndf_requirements = pd.DataFrame(requirements)\ndf_requirements.columns = ['FILE_NAME','REQUIREMENT_SET_ID','REQUIREMENT_SUBSET_ID','REQUIREMENT_TEXT']\ndf_requirements.head()","aa335320":"#Check for one sample file \ndf_requirements.loc[df_requirements['FILE_NAME']=='SYSTEMS ANALYST 1596 102717.txt']","96ade8cb":"#Check for salary components\nsalHeadings = [k for k in headingsFrame['Heading'].values if 'salary' in k.lower()]\nsal_list = []\nfor filename in os.listdir(bulletin_dir):\n    with open(bulletin_dir + \"\/\" + filename, 'r', errors='ignore') as f:\n        readNext = 0\n        for line in f.readlines():\n            clean_line = line.replace(\"\\n\",\"\").replace(\"\\t\",\"\").replace(\":\",\"\").strip()  \n            if clean_line in salHeadings:\n                readNext = 1\n            elif readNext == 1:\n                if clean_line in headingsFrame['Heading'].values:\n                    break\n                elif len(clean_line)<2:\n                    continue\n                else:\n                    sal_list.append([filename, clean_line])\n","0ec5ae5c":"df_salary = pd.DataFrame(sal_list)\ndf_salary.columns = ['FILE_NAME','SALARY_TEXT']\ndf_salary.head()","4010f1a6":"files = []\nfor filename in os.listdir(bulletin_dir):\n    files.append(filename)","0cf76977":"#Add 'ENTRY_SALARY_GEN','ENTRY_SALARY_DWP'\npattern = r'\\$?([0-9]{1,3},([0-9]{3},)*[0-9]{3}|[0-9]+)(.[0-9][0-9])?'\ndwp_salary_list = {}\ngen_salary_list = {}\nfor filename in files:\n    for sal_text in df_salary.loc[df_salary['FILE_NAME']==filename]['SALARY_TEXT']:\n        if 'department of water' in sal_text.lower():\n            if filename in dwp_salary_list.keys():\n                continue\n            matches = re.findall(pattern+' to '+pattern, sal_text) \n            if len(matches)>0:\n                salary_dwp = ' - '.join([x for x in matches[0] if x and not x.endswith(',')])\n            else:\n                matches = re.findall(pattern, sal_text)\n                if len(matches)>0:\n                    salary_dwp = matches[0][0]\n                else:\n                    salary_dwp = ''\n            dwp_salary_list[filename]= salary_dwp\n        else:\n            if filename in gen_salary_list.keys():\n                continue\n            matches = re.findall(pattern+' to '+pattern, sal_text)\n            if len(matches)>0:\n                salary_gen = ' - '.join([x for x in matches[0] if x and not x.endswith(',')])\n            else:\n                matches = re.findall(pattern, sal_text)\n                if len(matches)>0:\n                    salary_gen = matches[0][0]\n                else:\n                    salary_gen = ''\n            if len(salary_gen)>1:\n                gen_salary_list[filename]= salary_gen","f91c8e4f":"df_salary_dwp = pd.DataFrame(list(dwp_salary_list.items()), columns=['FILE_NAME','ENTRY_SALARY_DWP'])\ndf_salary_gen = pd.DataFrame(list(gen_salary_list.items()), columns=['FILE_NAME','ENTRY_SALARY_GEN'])\n","a4e41e3e":"def preprocess(txt):\n    txt = nltk.word_tokenize(txt)\n    txt = nltk.pos_tag(txt)\n    return txt","b958751c":"import nltk\ndef getEducationMajor(row):\n    txt = row['REQUIREMENT_TEXT']\n    txtMajor = ''\n    if 'major in' not in txt.lower() and ' majoring ' not in txt.lower():\n        return txtMajor\n    result = []\n    \n    istart = txt.lower().find(' major in ')\n    if istart!=-1:\n        txt = txt[istart+10:]\n    else:\n        istart = txt.lower().find(' majoring ')\n        if istart==-1:\n            return txtMajor\n        txt = txt[istart+12:]\n    \n    txt = txt.replace(',',' or ').replace(' and\/or ',' or ').replace(' a closely related field',' related field')\n    sent = preprocess(txt)\n    pattern = \"\"\"\n            NP: {<DT>? <JJ>* <NN.*>*}\n           BR: {<W.*>|<V.*>} \n        \"\"\"\n    cp = nltk.RegexpParser(pattern)\n    cs = cp.parse(sent)\n    #print(cs)\n    checkNext = 0\n    for subtree in cs.subtrees():\n        if subtree.label()=='NP':\n            result.append(' '.join([w for w, t in subtree.leaves()]))\n            checkNext=1\n        elif checkNext==1 and subtree.label()=='BR':\n            break\n    return '|'.join(result)\n","bb9f0a15":"#Add EDUCATION_MAJOR\ndf_requirements['EDUCATION_MAJOR']=df_requirements.apply(getEducationMajor, axis=1)\n","3bd0b103":"df_requirements.loc[df_requirements['EDUCATION_MAJOR']!=''].head()","49d5b155":"def getValues(searchText, COL_NAME):\n    data_list = []\n    dataHeadings = [k for k in headingsFrame['Heading'].values if searchText in k.lower()]\n\n    for filename in os.listdir(bulletin_dir):\n        with open(bulletin_dir + \"\/\" + filename, 'r', errors='ignore') as f:\n            readNext = 0 \n            datatxt = ''\n            for line in f.readlines():\n                clean_line = line.replace(\"\\n\",\"\").replace(\"\\t\",\"\").replace(\":\",\"\").strip()   \n                if readNext == 0:                         \n                    if clean_line in dataHeadings:\n                        readNext = 1\n                elif readNext == 1:\n                    if clean_line in headingsFrame['Heading'].values:\n                        break\n                    else:\n                        datatxt = datatxt + ' ' + clean_line\n            data_list.append([filename,datatxt.strip()])\n    result = pd.DataFrame(data_list)\n    result.columns = ['FILE_NAME',COL_NAME]\n    return result\n","e64f8ad7":"#Add JOB_DUTIES\ndf_duties = getValues('duties','JOB_DUTIES')","620ed8d3":"print(df_duties['JOB_DUTIES'].loc[df_duties['FILE_NAME'] == 'AIRPORT POLICE SPECIALIST 3236 063017 (2).txt'].values)","2e2c89e2":"#Function to retrieve values that match with pre-defined values \ndef section_value_extractor( document, section, subterms_dict, parsed_items_dict ):\n    retval = OrderedDict()\n    single_section_lines = document.lower()\n    \n    for node_tag, pattern_string in subterms_dict.items():\n        pattern_list = re.split(r\",|:\", pattern_string[0])#.sort(key=len)\n        pattern_list=sorted(pattern_list, key=len, reverse=True)\n        #print (pattern_list)\n        matches=[]\n        for pattern in pattern_list:\n            if pattern.lower() in single_section_lines:\n                matches.append(pattern)\n                single_section_lines = single_section_lines.replace(pattern.lower(),'')\n        #print (matches)\n        if len(matches):\n            info_string = \", \".join(list(matches)) + \" \"\n            retval[node_tag] = info_string\n    return retval","0d1f8c05":"#Function to read xml configuration to return json formatted string\ndef read_config( configfile ):\n    root = ET.fromstring(configfile)\n    config = []\n    for child in root:\n        term = OrderedDict()\n        term[\"Term\"] = child.get('name', \"\")\n        for level1 in child:\n            term[\"Method\"] = level1.get('name', \"\")\n            term[\"Section\"] = level1.get('section', \"\")\n            for level2 in level1:\n                term[level2.tag] = term.get(level2.tag, []) + [level2.text]\n\n        config.append(term)\n    json_result = json.dumps(config, indent=4)\n    return config","49368626":"def parse_document(document, config):\n    parsed_items_dict = OrderedDict()\n\n    for term in config:\n        term_name = term.get('Term')\n        extraction_method = term.get('Method')\n        extraction_method_ref = globals()[extraction_method]\n        section = term.get(\"Section\")\n        subterms_dict = OrderedDict()\n        \n        for node_tag, pattern_list in list(term.items())[3:]:\n            subterms_dict[node_tag] = pattern_list\n        parsed_items_dict[term_name] = extraction_method_ref(document, section, subterms_dict, parsed_items_dict)\n\n    return parsed_items_dict\n","6a554418":"#Read job_titles to use them to find patterns in the requirement text to extract job_class_titles\njob_titles = pd.read_csv(additional_data_dir+'\/job_titles.csv', header=None)\n\njob_titles = ','.join(job_titles[0])\njob_titles = job_titles.replace('\\'','').replace('&','and')\n","98fd04ce":"configfile = r'''\n<Config-Specifications>\n<Term name=\"Requirements\">\n        <Method name=\"section_value_extractor\" section=\"RequirementSection\">\n            <SchoolType>College or University,High School,Apprenticeship,Certificates<\/SchoolType>\n            <JobTitle>'''+job_titles+'''<\/JobTitle>\n        <\/Method>\n    <\/Term>\n<\/Config-Specifications>\n'''","d3609ef2":"config = read_config(configfile)\nresult = df_requirements['REQUIREMENT_TEXT'].apply(lambda k: parse_document(k,config))\ni=0\ndf_requirements['EXP_JOB_CLASS_TITLE']=''\ndf_requirements['SCHOOL_TYPE']=''\nfor item in (result.values):\n    for requirement,dic in list(item.items()):        \n        if 'JobTitle' in dic.keys():\n            df_requirements.loc[i,'EXP_JOB_CLASS_TITLE'] = dic['JobTitle']\n        if 'SchoolType' in dic.keys():\n            df_requirements.loc[i,'SCHOOL_TYPE'] = dic['SchoolType']\n    i=i+1","fa0ae252":"#Let's check the result for one sample file\ndf_requirements[df_requirements['FILE_NAME']=='SYSTEMS ANALYST 1596 102717.txt'][['FILE_NAME','EXP_JOB_CLASS_TITLE','SCHOOL_TYPE']]","d0811cee":"result.drop(columns=['REQUIREMENT_TEXT'], inplace=True)\nresult[result['FILE_NAME']=='SYSTEMS ANALYST 1596 102717.txt']","6c43763d":"DUTIES: search dutY parameter and return values","112da366":"Preparation : Convert the information in the txt files in a table","ba9320d7":"                              Activity 1: Results ( loading necessary files and converting in table) main task is give job recommendations based on resumes.\n","0d57b71e":"SETTING THE REQUIREMENT ( SET-ID AND SUBSET-ID)","1d421da3":"CHECKING SALARY PARAMETER","f05b3ff8":"CHECKING FILES","0d388d81":"PARSING THE DATA: Parse different fields ","b02d05bd":"NLP: CREATING PART OF SPEECH AND MAJORS(EDUCATIONS)","eb4e4f2c":"Problem statement:\n\nThe goal is to convert a folder full of plain-text job postings into a single structured CSV file and then to use this data to:\n\nidentify language that can negatively bias the pool of applicants; improve the diversity and quality of the applicant pool; and\/or make it easier to determine which promotions are available to employees in each job class.\n\nHypothesis of City of LA:\nthe main ain of this hypothesis is to adjust the readability-level of different job postings based on educationnal background. the requirements of that post will increase interest in job postings for those written English comprehension skills are limited.\n\n\"The goal aims to convert a folder full of plain-text job postings into a single structured CSV file and perform the following activity: (1) identify negatively bias language that is the pool of applicants; (2) improve the diversity level and quality of the applicant pool(language); and\/or (3) determine which promotions are available to employees in each job class and give recommendation based on that promotion\"\n\nActivity 1: Results, The files I am creating are inputs to my analysis and recommendations.\n\nActivity 2 : Improving the diversity and quality grade level too high - Simplify and Readability score\nThe Flesch\u2013Kincaid readability tests are readability tests designed to indicate how difficult a passage in English is to understand.\n\nHigher the scores, the more readable is the passage.Scores below 30 are considered to be very difficult to read.\n\nWe visualize the top 10 Flesch\u2013Kincaid readability scores and find that they are very low.It is very evident that the Job Bulletins need to be revisited for readability\n\nActivity 3 : Results and Recommendations\nThe City of LA should maintain a database of Course Subjects ,Apprenticeship and Education Majors. The dataset City Of LA Additional Datasets has been made by reading and cleaning through the Job Bulletins. This can be improved further. This can also help in standardizing the Job Bulletins.\nReadability scores calculated using Flesch.Kincaid scores have been low. It is recommended to make them more readable\n\nThe City of LA should also mainatain a database of Certifications. This can be used by job bulletins to have a very comphrensive list. Building a web portal or a mobile app with the structured dataset in the back end would also be very useful for job seekers\nPromotion Pathways as illustrated can be incorporated in the Job Bulletins and also the number of years to reach a position would also be beneficial\n\nVisualization and final recommendation of suitable candidates is predicted at last.\n","a70d96f4":"Preparation : Look for keywords, and append the following strings to the final dataframe","7132f16e":"Preparation for data:  cleaning the file names space unkown entries and null char.","cc84fa89":"Defining bulletin and additional directory and appending following field: file name, job class title, class no and opening date","b1d39cfe":"Retriving the values"}}