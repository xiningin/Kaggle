{"cell_type":{"cc3582c8":"code","f51b758f":"code","37bb61f4":"code","0e985bf7":"code","ed61144a":"code","b4eb8e88":"code","45e77f5c":"code","ec708061":"code","ab9d7ae1":"code","acb1cedc":"code","5b784c51":"code","c0e14b9f":"code","208f4246":"code","14ffe48e":"code","4020c310":"code","c34794c8":"code","1d44c335":"code","98b78c6d":"code","df8d79ba":"code","fdf125fa":"code","a9f6656b":"code","4e05d3c3":"code","43ac3694":"code","73ce200d":"code","e7751a76":"code","81116508":"code","2c4438b3":"code","0e756140":"code","e91a8ce1":"code","0a15dae4":"code","a2e3af72":"markdown","8add8391":"markdown","d9a63418":"markdown","33c52d45":"markdown","69fbd6d9":"markdown","f0a68971":"markdown","ba84df97":"markdown","ab0dcdd7":"markdown","8378de1b":"markdown","e2120f5e":"markdown","c5fc84dd":"markdown","ee306da2":"markdown","78927afb":"markdown","eee9216c":"markdown","f5690637":"markdown","5b102ea2":"markdown","5096cab7":"markdown","b04e0889":"markdown","76582e24":"markdown"},"source":{"cc3582c8":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns # data visualization library  \nimport matplotlib.pyplot as plt","f51b758f":"%matplotlib inline","37bb61f4":"data = pd.read_csv('..\/input\/data.csv')","0e985bf7":"data.head()","ed61144a":"data.columns","b4eb8e88":"# y includes our labels and x includes our features\ny = data.diagnosis   # M or B \nlist = ['Unnamed: 32','id','diagnosis']\nx = data.drop(list,axis = 1 )  #drop two columns\nx.head()","45e77f5c":"ax = sns.countplot(y,label=\"Count\") \nB, M = y.value_counts()\nprint('Number of Benign: ',B)\nprint('Number of Malignant : ',M)","ec708061":"x.describe()","ab9d7ae1":"# first ten features\ndata_dia = y\ndata = x\ndata_n_2 = (data - data.mean()) \/ (data.std())              # standardization\ndata = pd.concat([y,data_n_2.iloc[:,0:10]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data,split=True, inner=\"quart\")\nplt.xticks(rotation=90)","acb1cedc":"# Second ten features\ndata = pd.concat([y,data_n_2.iloc[:,10:20]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data,split=True, inner=\"quart\")\nplt.xticks(rotation=90)","5b784c51":"# Second ten features\ndata = pd.concat([y,data_n_2.iloc[:,20:31]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data,split=True, inner=\"quart\")\nplt.xticks(rotation=90)","c0e14b9f":"# As an alternative of violin plot, box plot can be used\n# box plots are also useful in terms of seeing outliers\n# I do not visualize all features with box plot\n# If you want, you can visualize other features as well.\nplt.figure(figsize=(10,10))\nsns.boxplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\nplt.xticks(rotation=90)","208f4246":"sns.jointplot(x.loc[:,'concavity_worst'], x.loc[:,'concave points_worst'], kind=\"regg\", color=\"#ce1414\")","14ffe48e":"sns.set(style=\"white\")\ndf = x.loc[:,['radius_worst','perimeter_worst','area_worst']]\ng = sns.PairGrid(df, diag_sharey=False)\ng.map_lower(sns.kdeplot, cmap=\"Blues_d\")\ng.map_upper(plt.scatter)\ng.map_diag(sns.kdeplot, lw=3)","4020c310":"sns.set(style=\"whitegrid\", palette=\"muted\")\ndata_dia = y\ndata = x\ndata_n_2 = (data - data.mean()) \/ (data.std())              # standardization\ndata = pd.concat([y,data_n_2.iloc[:,0:10]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\ntic = time.time()\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\n\nplt.xticks(rotation=90)","c34794c8":"data = pd.concat([y,data_n_2.iloc[:,10:20]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\nplt.xticks(rotation=90)","1d44c335":"data = pd.concat([y,data_n_2.iloc[:,20:31]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\ntoc = time.time()\nplt.xticks(rotation=90)\nprint(\"swarm plot time: \", toc-tic ,\" s\")","98b78c6d":"#correlation map\nf,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(x.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","df8d79ba":"from sklearn.linear_model import LogisticRegression\n\nfrom sklearn.model_selection import train_test_split\n\n# split data train 70 % and test 30 %\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n\nclf = LogisticRegression(random_state = 2)\nclf.fit(x_train, y_train)","fdf125fa":"y_pred = clf.predict(x_test)","a9f6656b":"from sklearn.metrics import accuracy_score","4e05d3c3":"accuracy_score(y_true= y_test, y_pred= y_pred)","43ac3694":"clf.score(x_test, y_test)","73ce200d":"from sklearn.metrics import f1_score,confusion_matrix","e7751a76":"cm = confusion_matrix(y_test, y_pred)\nprint(cm)","81116508":"cm = confusion_matrix(y_test,clf.predict(x_test))\nsns.heatmap(cm,annot=True,fmt=\"d\")","2c4438b3":"#from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\n# split data train 70 % and test 30 %\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n\n#random forest classifier with n_estimators=10 (default)\nclf_rf = RandomForestClassifier(random_state=43)      \nclr_rf = clf_rf.fit(x_train,y_train)\n\nac = accuracy_score(y_test,clf_rf.predict(x_test))\nprint('Accuracy is: ',ac)\ncm = confusion_matrix(y_test,clf_rf.predict(x_test))\nsns.heatmap(cm,annot=True,fmt=\"d\")","0e756140":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n# find best scored 5 features\nselect_feature = SelectKBest(chi2, k=5).fit(x_train, y_train)","e91a8ce1":"print('Score list:', select_feature.scores_)\nprint('Feature list:', x_train.columns)","0a15dae4":"x_train_2 = select_feature.transform(x_train)\nx_test_2 = select_feature.transform(x_test)\n#random forest classifier with n_estimators=10 (default)\nclf_rf_2 = RandomForestClassifier()      \nclr_rf_2 = clf_rf_2.fit(x_train_2,y_train)\nac_2 = accuracy_score(y_test,clf_rf_2.predict(x_test_2))\nprint('Accuracy is: ',ac_2)\ncm_2 = confusion_matrix(y_test,clf_rf_2.predict(x_test_2))\nsns.heatmap(cm_2,annot=True,fmt=\"d\")","a2e3af72":"# Accuracy\n\nAccuracy is the most intuitive performance Metric. It's the number of correct predictions over all predictions made. It's the default performance metric in scikit Learn. We can compute the accuracy in two ways: using the accuracy_score() method or the score() method:\n\n","8add8391":"# let's try swarm plot","d9a63418":"# Logistic Regression","33c52d45":"to compare two features deeper, we can use joint plot.it correlate.\n0.86 is enough to say they correlate.\nNote: we are not choosing feature, we are just looking to have an idea about them.","69fbd6d9":"# Confusion Matrix\n\nA much better way to evaluate the performance is to look at the confusion matrix. The confusion matrix give us information about the correct predictions and the errors. It's a two-by-two array, where the rows correspond to the true classes and the columns correspond to the predicted classes.\n\nLet's look at the code to output confusion matrix for our prediction task and explain what TP, TN, FP, FN means","f0a68971":"violin plot to see similarities and check for correlation and \nswarm plot to see variance clearly for feature selection due to classification","ba84df97":"# Random Forest & Confusion Matrix","ab0dcdd7":"# Heatmap to observe all correlation between features","8378de1b":"Accuracy is almost 95% and as it can be seen in confusion matrix, we make few wrong prediction. Now lets see other feature selection methods to find better results.","e2120f5e":"# Univariate feature selection and random forest classification","c5fc84dd":"# We are good to go with these!!!\n\nThe rest should be figures and business impact\n\nI will forward my findings as soon as possible.\n\nThanks","ee306da2":"What about three or more feauture comparision ? we can use pair grid plot. Notice that radius_worst, perimeter_worst and area_worst are correlated as it can be seen pair grid plot.\nWe'll use these discoveries for feature selection.","78927afb":"# visualization","eee9216c":"from the plots above, variable of concavity_worst and concave point_worst looks like similar but how can we decide whether they are correlated with each other or not. (Not always true but, basically if the features are correlated with each other we can drop one of them)","f5690637":"Best 5 feature to classify is that area_mean, area_se, texture_mean, concavity_worst and concavity_mean. So lets se what happens if we use only these best scored 5 feature.","5b102ea2":"observation\n\nThe radius, parameter and area are highly correlated as expected from their relation so from these we will use anyone of them\n\nCompactness_mean, concavity_mean and concavepoint_mean are highly correlated so we will use compactness_mean from here\n\nSo selected Parameter for use is perimeter_mean, texture_mean, compactness_mean, symmetry_mean*","5096cab7":"Here,you can see variance clearly.  This help us to know features that are good for classification. above, area_worst seperates malignant and benign to a reasonable extent. Hovewer, smoothness_se are mixed so it is not the best to be used for classification.","b04e0889":"In univariate feature selection, we will use SelectKBest that removes all but the k highest scoring features. http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest","76582e24":"In this method we need to choose how many features we will use. For example, will k (number of features) be 5 or 10 or 15? let'schoose k = 5 and find best 5 features."}}