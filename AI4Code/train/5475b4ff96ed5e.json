{"cell_type":{"47f5191b":"code","9e2f6494":"code","4c3c9808":"code","bdc27061":"code","541407fe":"code","26fbf7f8":"code","1e9f9edf":"code","dc5a5183":"code","7957564c":"code","b86bbbaf":"code","ba78e784":"code","8ccd5a37":"code","5f3509b5":"code","94cf0f25":"code","76864cf6":"code","7b776d5c":"code","e984d61f":"code","bfffa14e":"code","e834e394":"code","7752c85c":"code","53ad92d3":"code","c4400bb9":"code","058f6cd5":"code","02126799":"code","1ba74875":"code","69ebd86f":"code","50c62741":"code","167d03e5":"code","23d9fc75":"code","6d375b17":"code","b7c81460":"code","dc7ac9c3":"code","22ad3d13":"code","cfd01914":"code","7456af0c":"code","79793557":"code","ff27f7e8":"code","d7864cde":"code","202974d9":"code","980630fc":"code","0447ab77":"code","99cdeccf":"code","20d03d09":"code","08fc1a0a":"code","2c683973":"code","0d43b486":"code","4dd0ca31":"code","a83d2e07":"code","cf3b0774":"code","10813798":"code","cfd09105":"code","d17dc6e8":"code","b1ca174e":"code","7902cd31":"code","d7818bbb":"code","ad94f604":"code","e343e75e":"code","42a9915a":"code","972b435d":"code","fe32d561":"code","492138c5":"code","654443b7":"markdown","78cb1529":"markdown","aa683e4c":"markdown","151ef0dc":"markdown","90e6d956":"markdown","08829f58":"markdown","1ab71f03":"markdown","5cb3fdcc":"markdown","a8952c0f":"markdown","87cebe54":"markdown"},"source":{"47f5191b":"import warnings, sys\nwarnings.filterwarnings(\"ignore\")\n\n# Thanks to Chris's RAPIDS dataset, it only takes around 1 min to install offline\n!cp ..\/input\/rapids\/rapids.0.15.0 \/opt\/conda\/envs\/rapids.tar.gz\n!cd \/opt\/conda\/envs\/ && tar -xzvf rapids.tar.gz > \/dev\/null\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\/site-packages\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\"] + sys.path \n!cp \/opt\/conda\/envs\/rapids\/lib\/libxgboost.so \/opt\/conda\/lib\/","9e2f6494":"#libraries for SVC\nimport os\nimport gc\nimport pickle\nimport datetime\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom tqdm.notebook import tqdm\nfrom time import time","4c3c9808":"#libraries for neural network\n\nimport random\n\nimport matplotlib.pyplot as plt\n\nimport copy\nimport seaborn as sns\nimport collections\n\n\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler,QuantileTransformer,PowerTransformer,RobustScaler,Normalizer\n\nsys.path.append('..\/input\/rank-gauss')\nfrom gauss_rank_scaler import GaussRankScaler\n\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.feature_selection import VarianceThreshold\n\nfrom joblib import dump, load\n\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import StepLR","bdc27061":"train_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\ntrain_targets_nonscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\n\n#copies from dataframes\ndf_train=train_features.copy()\ndf_train_targets=train_targets.copy()\ndf_test=test_features.copy()\ndf_train_targets_nonscored=train_targets_nonscored.copy()\nss = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')\n\ncols = [c for c in ss.columns.values if c != 'sig_id']","541407fe":"#elimination of controls\ndf_train = df_train[df_train['cp_type']!='ctl_vehicle']\ndf_test = df_test[df_test['cp_type']!='ctl_vehicle']\n\ndf_train_targets = df_train_targets.iloc[df_train.index]\ndf_train_targets_nonscored=df_train_targets_nonscored.iloc[df_train.index]\n\ndf_train.reset_index(drop=True, inplace=True)\ndf_test.reset_index(drop=True, inplace=True)\ndf_train_targets.reset_index(drop=True, inplace=True)\ndf_train_targets_nonscored.reset_index(drop=True, inplace=True)\n\ndel df_train_targets['sig_id']\ndel df_train_targets_nonscored['sig_id']","26fbf7f8":"def preprocess(df):\n    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n    del df['sig_id']\n    return df\n\ndef log_loss_metric(y_true, y_pred):\n    metrics = []\n    for _target in train_targets.columns:\n        metrics.append(log_loss(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float), labels = [0,1]))\n    return np.mean(metrics)\n\ndef log_loss_metric_n(y_true, y_pred):\n    metrics = []\n    for _target in df_train_targets_nonscored.columns:\n        metrics.append(log_loss(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float), labels = [0,1]))\n    return np.mean(metrics)\n\n\ntrain = preprocess(df_train)\ntest = preprocess(df_test)\n\n#del train_targets['sig_id']\n#del train_targets_nonscored['sig_id']\n","1e9f9edf":"top_feats = [  0,   1,   2,   3,   5,   6,   8,   9,  10,  11,  12,  14,  15,\n        16,  18,  19,  20,  21,  23,  24,  25,  27,  28,  29,  30,  31,\n        32,  33,  34,  35,  36,  37,  39,  40,  41,  42,  44,  45,  46,\n        48,  50,  51,  52,  53,  54,  55,  56,  57,  58,  59,  60,  61,\n        63,  64,  65,  66,  68,  69,  70,  71,  72,  73,  74,  75,  76,\n        78,  79,  80,  81,  82,  83,  84,  86,  87,  88,  89,  90,  92,\n        93,  94,  95,  96,  97,  99, 100, 101, 103, 104, 105, 106, 107,\n       108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120,\n       121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 132, 133, 134,\n       135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147,\n       149, 150, 151, 152, 153, 154, 155, 157, 159, 160, 161, 163, 164,\n       165, 166, 167, 168, 169, 170, 172, 173, 175, 176, 177, 178, 180,\n       181, 182, 183, 184, 186, 187, 188, 189, 190, 191, 192, 193, 195,\n       197, 198, 199, 202, 203, 205, 206, 208, 209, 210, 211, 212, 213,\n       214, 215, 218, 219, 220, 221, 222, 224, 225, 227, 228, 229, 230,\n       231, 232, 233, 234, 236, 238, 239, 240, 241, 242, 243, 244, 245,\n       246, 248, 249, 250, 251, 253, 254, 255, 256, 257, 258, 259, 260,\n       261, 263, 265, 266, 268, 270, 271, 272, 273, 275, 276, 277, 279,\n       282, 283, 286, 287, 288, 289, 290, 294, 295, 296, 297, 299, 300,\n       301, 302, 303, 304, 305, 306, 308, 309, 310, 311, 312, 313, 315,\n       316, 317, 320, 321, 322, 324, 325, 326, 327, 328, 329, 330, 331,\n       332, 333, 334, 335, 338, 339, 340, 341, 343, 344, 345, 346, 347,\n       349, 350, 351, 352, 353, 355, 356, 357, 358, 359, 360, 361, 362,\n       363, 364, 365, 366, 368, 369, 370, 371, 372, 374, 375, 376, 377,\n       378, 379, 380, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n       392, 393, 394, 395, 397, 398, 399, 400, 401, 403, 405, 406, 407,\n       408, 410, 411, 412, 413, 414, 415, 417, 418, 419, 420, 421, 422,\n       423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435,\n       436, 437, 438, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450,\n       452, 453, 454, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465,\n       466, 468, 469, 471, 472, 473, 474, 475, 476, 477, 478, 479, 482,\n       483, 485, 486, 487, 488, 489, 491, 492, 494, 495, 496, 500, 501,\n       502, 503, 505, 506, 507, 509, 510, 511, 512, 513, 514, 516, 517,\n       518, 519, 521, 523, 525, 526, 527, 528, 529, 530, 531, 532, 533,\n       534, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547,\n       549, 550, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563,\n       564, 565, 566, 567, 569, 570, 571, 572, 573, 574, 575, 577, 580,\n       581, 582, 583, 586, 587, 590, 591, 592, 593, 595, 596, 597, 598,\n       599, 600, 601, 602, 603, 605, 607, 608, 609, 611, 612, 613, 614,\n       615, 616, 617, 619, 622, 623, 625, 627, 630, 631, 632, 633, 634,\n       635, 637, 638, 639, 642, 643, 644, 645, 646, 647, 649, 650, 651,\n       652, 654, 655, 658, 659, 660, 661, 662, 663, 664, 666, 667, 668,\n       669, 670, 672, 674, 675, 676, 677, 678, 680, 681, 682, 684, 685,\n       686, 687, 688, 689, 691, 692, 694, 695, 696, 697, 699, 700, 701,\n       702, 703, 704, 705, 707, 708, 709, 711, 712, 713, 714, 715, 716,\n       717, 723, 725, 727, 728, 729, 730, 731, 732, 734, 736, 737, 738,\n       739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751,\n       752, 753, 754, 755, 756, 758, 759, 760, 761, 762, 763, 764, 765,\n       766, 767, 769, 770, 771, 772, 774, 775, 780, 781, 782, 783, 784,\n       785, 787, 788, 790, 793, 795, 797, 799, 800, 801, 805, 808, 809,\n       811, 812, 813, 816, 819, 820, 821, 822, 823, 825, 826, 827, 829,\n       831, 832, 833, 834, 835, 837, 838, 839, 840, 841, 842, 844, 845,\n       846, 847, 848, 850, 851, 852, 854, 855, 856, 858, 860, 861, 862,\n       864, 867, 868, 870, 871, 873, 874]\n\nprint(len(top_feats))","dc5a5183":"scaler = StandardScaler()\nX = scaler.fit_transform(train.values[:, top_feats])\nx_tt = scaler.transform(test.values[:, top_feats])","7957564c":"from cuml.svm import SVC\n\nN_STARTS =1 #3\nN_SPLITS =5 #5\n\nres = df_train_targets_nonscored.copy()\nres.loc[:, df_train_targets_nonscored.columns] = 0\n\n#make a dataframe to collect the sample data with dimensions of sample and columns of train_targets_nonscored\nss_non=pd.DataFrame(np.zeros((df_test.shape[0],len(df_train_targets_nonscored.columns))),columns=df_train_targets_nonscored.columns)\ncollect_tar_failure=[]\ncollect_tar_log012=[]\ncollect_tar=[]\n\nfor tar in tqdm(range(df_train_targets_nonscored.shape[1])):\n    print(tar)\n    \n    start_time = time()\n    targets = df_train_targets_nonscored.values[:, tar]\n    \n    if targets.sum() >= N_SPLITS:\n        \n        for seed in range(N_STARTS):\n\n            skf = StratifiedKFold(n_splits = N_SPLITS, random_state = seed, shuffle = True)\n\n            for n, (tr, te) in enumerate(skf.split(targets, targets)):\n\n                x_tr, x_val = X[tr], X[te]\n                y_tr, y_val = targets[tr], targets[te]\n                \n                if y_tr.sum() >= 5:\n\n                    model = SVC(probability = True, cache_size = 2000)\n                    model.fit(x_tr, y_tr)\n                    ss_non.loc[:, df_train_targets_nonscored.columns[tar]] += model.predict_proba(x_tt)[:, 1] \/ (N_SPLITS * N_STARTS)\n                    res.loc[te, df_train_targets_nonscored.columns[tar]] += model.predict_proba(x_val)[:, 1] \/ N_STARTS\n\n                else:\n\n                    print(f'Target {tar}: Seed {seed}: Fold {n}: SVC probabilistic output failure.')\n                    collect_tar_failure.append(tar)\n                    model = SVC(cache_size = 2000)\n                    model.fit(x_tr, y_tr)\n                    ss_non.loc[:, df_train_targets_nonscored.columns[tar]] += model.predict(x_tt) \/ (N_SPLITS * N_STARTS)\n                    res.loc[te, df_train_targets_nonscored.columns[tar]] += model.predict(x_val) \/ N_STARTS\n    \n        score = log_loss(df_train_targets_nonscored.loc[:, df_train_targets_nonscored.columns[tar]], res.loc[:, df_train_targets_nonscored.columns[tar]])\n        if (score<=0.02) and (tar not in collect_tar_failure):\n            collect_tar_log012.append(tar)\n        if tar not in collect_tar_failure:\n            collect_tar.append(tar)\n        \n    print(f'[{str(datetime.timedelta(seconds = time() - start_time))[2:7]}] Target {tar}:', score)","b86bbbaf":"print(f'Model OOF Metric: {log_loss_metric_n(df_train_targets_nonscored, res)}')\n","ba78e784":"print(len(collect_tar_log012))","8ccd5a37":"ss_non.shape,res.shape","5f3509b5":"#synthetic variables from SVC\ndf_train_svc = pd.DataFrame(np.array(res.iloc[:,collect_tar_log012]), columns=[f'svc-{i}' for i in range(len(collect_tar_log012))])\ndf_test_svc = pd.DataFrame(np.array(ss_non.iloc[:,collect_tar_log012]), columns=[f'svc-{i}' for i in range(len(collect_tar_log012))])","94cf0f25":"\n#svc datasets\ndf_train_svc=df_train_svc.iloc[df_train.index]\ndf_test_svc=df_test_svc.iloc[df_test.index]\ndf_train_svc.reset_index(drop=True, inplace=True)\ndf_test_svc.reset_index(drop=True, inplace=True)\n\nprint(df_train.shape,df_train_targets.shape,df_test.shape,df_train_svc.shape, df_test_svc.shape)","76864cf6":"df_train.head()","7b776d5c":"df_test.head()","e984d61f":"df_train_targets.head()","bfffa14e":"data_all = pd.concat([df_train, df_test], ignore_index=True)\nprint(data_all.shape)\ndata_all.head()","e834e394":"def process_data(data):\n    #one hot encoding\n    #no controls\n    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n    #with controls\n    #data = pd.get_dummies(data, columns=['cp_type','cp_time','cp_dose'])\n    return data","7752c85c":"#data_all = preprocess(data_all)\n#one_hot\ndata_all=process_data(data_all)\ndata_all.head()","53ad92d3":"#change ","c4400bb9":"#change numericals for predictors tstudent significant\nnumerical_features=list(train_features.columns[4:])\n\nprint(numerical_features[:10],numerical_features[860:])","058f6cd5":"data_all_numerical=data_all.loc[:,numerical_features]","02126799":"data_all_numerical.head()","1ba74875":"def variance_threshold(dataframe):   \n    'return dataframe containing all features with higher variance are the threshold'\n    #return numpy data\n    selector=VarianceThreshold(threshold=0.90)\n    selector.fit_transform(dataframe)\n    mask=selector.get_support(indices=False)\n    dataframe_s=dataframe.iloc[:,mask]\n    print('number of features removed',dataframe.shape[1]-dataframe_s.shape[1])\n    return dataframe_s\n\ndef scaling_data(scaler_name,dataframe):\n    'Apply a scaling method to a dataframe, which include exclusively numerical data' \n    'quantile_normal, rank_gauss, min_max, standard, gaussian_yeo, normal_l2, robust_scaler'\n    #return numpy data\n    if scaler_name=='rank_gauss':\n        #?same quantile transform normal\n        scaler =GaussRankScaler()\n        \n    elif scaler_name=='standard':\n        scaler= StandardScaler()\n        \n    elif scaler_name=='min_max':\n        scaler= MinMaxScaler()\n        \n    elif scaler_name=='gaussian_yeo':\n        scaler=PowerTransformer(method='yeo-johnson', standardize=True, copy=False)\n        \n    elif  scaler_name=='robust_scaler':\n        scaler=RobustScaler(with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0), copy=False)\n        #scaler=RobustScaler(with_centering=True, with_scaling=True, quantile_range=(10.0, 90.0), copy=False)\n    \n    elif scaler=='quantile_normal':\n        scaler=QuantileTransformer(n_quantiles=1000, output_distribution='normal', random_state=None, copy=False)\n        \n    elif scaler=='normal_l2':\n        scaler=Normalizer(norm='l2',copy=False)\n                \n    dataframe.loc[:,:]=scaler.fit_transform( dataframe.loc[:,:])\n    \n    return dataframe","69ebd86f":"#features\ndata_all_var=variance_threshold(data_all_numerical)\nscaler_name='rank_gauss'\n#scaler_name='standard'\ndata_all_sc=scaling_data(scaler_name,data_all_numerical)\n","50c62741":"data_all_sc.head()","167d03e5":"#check if there are nan values\n#np.isnan(data_all_sc).any()\ndata_all_sc.isnull().values.any()","23d9fc75":"def PCA_descriptors(data,ncompo_genes,ncompo_cells):\n    'introduce PCA descriptors'\n    data_all=data.copy()\n    #base_seed = 2020\n\n    GENES = [col for col in data_all.columns if col.startswith('g-')]\n    CELLS = [col for col in data_all.columns if col.startswith('c-')]\n\n    pca_genes = PCA(n_components=ncompo_genes, random_state=42).fit_transform(data_all[GENES])\n    pca_cells = PCA(n_components=ncompo_cells, random_state=42 ).fit_transform(data_all[CELLS])\n    #pca_genes = PCA(n_components=ncompo_genes).fit_transform(data_all[GENES])\n    #pca_cells = PCA(n_components=ncompo_cells).fit_transform(data_all[CELLS])\n    pca_genes = pd.DataFrame(pca_genes, columns=[f'pca_g-{i}' for i in range(ncompo_genes)])\n    pca_cells = pd.DataFrame(pca_cells, columns=[f'pca_c-{i}' for i in range(ncompo_cells)])\n    data_pca = pd.concat([pca_genes, pca_cells], axis=1)\n    \n    return data_pca","6d375b17":"#2 add PCA features\n\nscaler_name='gaussian_yeo'\n\ndata_all_g=scaling_data(scaler_name,data_all_numerical)\n\nncompo_genes = 70#70\nncompo_cells = 15\n\ndata_pca=PCA_descriptors(data_all_g,ncompo_genes,ncompo_cells)\n#data_pca=data_all_pca.iloc[:,872:]","b7c81460":"data_pca.head()","dc7ac9c3":"#one hot categorical data\ndata_cat=data_all.iloc[:,873:]\ndata_cat.head()","22ad3d13":"#join all data\ndata_all_new=pd.concat([data_cat,data_all_sc, data_pca],axis=1)\n\ndata_all_new.head()","cfd01914":"#Preparation for the neural network: separate data in train and test\nrows_train=df_train.shape[0]\ntrain=data_all_new[:rows_train]\ntest=data_all_new[df_train.shape[0]:]\ntest.reset_index(drop=True,inplace=True)","7456af0c":"#join scv data\ntrain=pd.concat([train,df_train_svc],axis=1)\n\ntest=pd.concat([test,df_test_svc],axis=1)\ntrain.tail()","79793557":"test.head()","ff27f7e8":"train.shape,df_train_targets.shape","d7864cde":"#preparation of the data. concatenate targets to the train data \n\nsample_submission = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')\n#del df_train_targets['sig_id']\ntrain=pd.concat([train,df_train_targets],axis=1)\ntarget = train[df_train_targets.columns]\n\ntarget_cols = target.columns.values.tolist()\n","202974d9":"train.head()","980630fc":"len(target_cols)","0447ab77":"train.shape,target.shape","99cdeccf":"#cv folds\nfolds = train.copy()\n\nmskf = MultilabelStratifiedKFold(n_splits=7)\n\nfor f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n    folds.loc[v_idx, 'kfold'] = int(f)\n\nfolds['kfold'] = folds['kfold'].astype(int)\nfolds","20d03d09":"\nprint(train.shape)\nprint(folds.shape)\nprint(test.shape)\nprint(target.shape)\n\nprint(sample_submission.shape)","08fc1a0a":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","2c683973":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct\n    ","0d43b486":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n#         print(inputs.shape)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        final_loss += loss.item()\n        \n    final_loss \/= len(dataloader)\n    return final_loss\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        \n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss \/= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds\n","4dd0ca31":"import torch\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","a83d2e07":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(0.26)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(0.26)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n        \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = F.leaky_relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.dense2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        return x\n    \n\nclass LabelSmoothingLoss(nn.Module):\n    def __init__(self, classes, smoothing=0.0, dim=-1):\n        super(LabelSmoothingLoss, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.cls = classes\n        self.dim = dim\n\n    def forward(self, pred, target):\n        pred = pred.log_softmax(dim=self.dim)\n        with torch.no_grad():\n            # true_dist = pred.data.clone()\n            true_dist = torch.zeros_like(pred)\n            true_dist.fill_(self.smoothing \/ (self.cls - 1))\n            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))    ","cf3b0774":"#to change\n\nfeature_cols = [c for c in folds.columns if c not in target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['kfold','sig_id']]\nlen(feature_cols)","10813798":"# HyperParameters\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS =25 #25\nBATCH_SIZE = 128\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nNFOLDS = 7           \nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = False\n\nnum_features=len(feature_cols)\nnum_targets=len(target_cols)\nhidden_size=1300\nprint(num_features,num_targets)","cfd09105":"len(train)","d17dc6e8":"def run_training(fold, seed):\n    \n    seed_everything(seed)\n    \n    train = folds\n    test_ = test\n    \n    trn_idx = train[train['kfold'] != fold].index\n    val_idx = train[train['kfold'] == fold].index\n    \n    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n    \n    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n    \n    train_dataset = MoADataset(x_train, y_train)\n    valid_dataset = MoADataset(x_valid, y_valid)\n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n    )\n    \n    model.to(DEVICE)\n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                              max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    \n    loss_fn = nn.BCEWithLogitsLoss()\n    loss_tr = SmoothBCEwLogits(smoothing =0.001)\n    \n    early_stopping_steps = EARLY_STOPPING_STEPS\n    early_step = 0\n   \n    oof = np.zeros((len(train), target.shape[1]))\n    best_loss = np.inf\n    for epoch in range(EPOCHS):\n        \n        train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}\")\n        \n        if valid_loss < best_loss:\n            \n            best_loss = valid_loss\n            oof[val_idx] = valid_preds\n            torch.save(model.state_dict(), f\"FOLD{fold}_.pth\")\n            \n        elif(EARLY_STOP == True):\n            \n            early_step += 1\n            if (early_step >= early_stopping_steps):\n                break\n            \n    \n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n    )\n    \n    model.load_state_dict(torch.load(f\"FOLD{fold}_.pth\"))\n    model.to(DEVICE)\n    \n    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return oof, predictions","b1ca174e":"def run_k_fold(NFOLDS, seed):\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    for fold in range(NFOLDS):\n        oof_, pred_ = run_training(fold, seed)\n        \n        predictions += pred_ \/ NFOLDS\n        oof += oof_\n        \n    return oof, predictions","7902cd31":"# Averaging on multiple SEEDS\n\nSEED = [0,1,2] #<-- Update\n\n\noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n#SEED = [0,1,2,3,4,5,6]\nfor seed in SEED:\n    \n    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n    oof += oof_ \/ len(SEED)\n    predictions += predictions_ \/ len(SEED)\n\ntrain[target_cols] = oof\ntest[target_cols] = predictions","d7818bbb":"oof.shape","ad94f604":"df_train_targets[target_cols].shape","e343e75e":"test[target_cols].shape","42a9915a":"#valid_results = df_train_targets.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\ndef log_loss_metric(y_true, y_pred):\n    metrics = []\n    for _target in df_train_targets.columns:\n        metrics.append(log_loss(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float), labels = [0,1]))\n    return np.mean(metrics)\n\n\ny_true = df_train_targets[target_cols].values\ny_pred = oof\n\nscore = 0\nfor i in range(len(target_cols)):\n    score_ = log_loss(y_true[:, i], y_pred[:, i])\n    print(i,target_cols[i],score_ )\n    score += score_ \/ target.shape[1]\n    \nprint(\"CV log_loss: \", score)","972b435d":"# submit\ntest_pred=test[target_cols]\n\nsig_id = test_features[test_features['cp_type']!='ctl_vehicle'].sig_id.reset_index(drop=True)\n\ntest_pred['sig_id'] = sig_id\n\nsub = pd.merge(test_features[['sig_id']], test_pred, on='sig_id', how='left')\nsub.fillna(0, inplace=True)\n\nsub.to_csv('submission.csv', index=False)","fe32d561":"sub.shape","492138c5":"sub","654443b7":"# Can we use non-scored targets to create synthetic variables?\n \nI will try to use Rapids-SVC models to trained non-scored targets and the results of the predictions will be concatenate to the features, including g- c- PCA  and categorical. Then, a neural network (NN) model will be trained exclusively for the scored targets.\n\nReferenced notebook for rapids and beginning of this notebook is forked from:\nhttps:\/\/www.kaggle.com\/gogo827jz\/rapids-svm-on-gpu-6000-models-in-1-hour\n\nNN referenced notebooks:\nhttps:\/\/www.kaggle.com\/kushal1506\/moa-pytorch-0-01859-rankgauss-pca-nn\nhttps:\/\/www.kaggle.com\/riadalmadani\/pytorch-cv-0-0145-lb-0-01839\n\npredictors\nhttps:\/\/www.kaggle.com\/demetrypascal\/t-test-pca-rfe-logistic-regression\n","78cb1529":"# Try to use the nonscored targets to make synthetic variables","aa683e4c":"Apply variance threshold of 0.9 to the numerical data, then apply scaling a scaler and then PCA","151ef0dc":"Add training model (NN) -Riad  & Kushal-","90e6d956":"# 3090 CuML SVC Models","08829f58":"Prepare data for Neural Network.\nMake analysis without controls","1ab71f03":"Make everything without controls: SVC of nonscored targets and NN integrating synthetic variables","5cb3fdcc":"# RAPIDS SVC for MoA\n\nRAPIDS cuML is a great library alows training sklearn models on GPU. Available classification models include Logistic Regresssion, SVC, Random Forest and KNN, etc..\n\nKonrad has tried to train SVR models in [SVR Modeds][1]. In this notebook, I try training 3090 SVC models in 2 hours on GPU, which should take forever on CPU...\n\n[1]: https:\/\/www.kaggle.com\/konradb\/build-model-svm","a8952c0f":"# Data Preparation","87cebe54":"V3- No controls were included in the complete analysis. Only a selection of the non-scored predicted features, based on log-loss lower or equal to 0.02 were concatenated.\n\nIn v5, I will concatenate all possible non-scored predicted features."}}