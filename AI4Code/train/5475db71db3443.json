{"cell_type":{"75dd02d2":"code","ae3cb523":"code","7d957d59":"code","07f17663":"code","a6f65634":"code","b9c44a53":"code","ec07aba5":"code","d966d07e":"code","a7e69e98":"code","91345c75":"code","55371af9":"code","39ad3773":"code","d8f3cac4":"code","4adbcb5f":"code","4a9da5c2":"code","35c9b59e":"code","15251445":"code","b48c3c30":"code","ebeb2819":"code","fe4cd9a2":"code","745e86c9":"code","2dc5841b":"code","e3e80ea7":"code","2d1832d1":"code","9aecd42e":"code","0e969cb8":"code","bccfb078":"code","f7aeeb25":"code","02f86ea5":"code","485429e8":"code","a9205476":"code","e21cde84":"code","3295b79a":"code","af3dd7b3":"code","a9082390":"code","8234ddb6":"code","a26fc3f5":"markdown","2ecba7d7":"markdown","96f6a3f5":"markdown","62fe83e8":"markdown","d1abdb2a":"markdown"},"source":{"75dd02d2":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","ae3cb523":"df=pd.read_csv('..\/input\/seed-from-uci\/Seed_Data.csv')\ndf=df.sample(frac=0.5,random_state=3)\ndf.head()","7d957d59":"df.info()","07f17663":"df=df.sample(frac=1,random_state=3)\ny=df['target']\nx=df.drop('target',axis=1)","a6f65634":"df['target'].value_counts()","b9c44a53":"df['target'].value_counts(normalize=True).plot.bar()","ec07aba5":"df.describe()","d966d07e":"sns.pairplot(df)","a7e69e98":"from sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=3)\n\n","91345c75":"from sklearn.preprocessing import StandardScaler\nss=StandardScaler()\nXs=ss.fit_transform(x)\n\nX_trains=ss.fit_transform(X_train)\nX_tests=ss.transform(X_test)","55371af9":"from sklearn.metrics import confusion_matrix,accuracy_score,roc_auc_score,roc_curve\nfrom sklearn.ensemble import RandomForestClassifier\nrfc=RandomForestClassifier(n_estimators=100)\n\nrfc.fit(X_trains,y_train)\ny_train_pred =rfc.predict(X_trains)\ny_train_prob = rfc.predict_proba(X_trains)[:,1]\n\nprint('Confusion Matrix - Train: \\n', confusion_matrix(y_train, y_train_pred))\nprint('\\n')\nprint('Overall Accuracy - Train: ', accuracy_score(y_train, y_train_pred))\n#print('AUC - Train: ', roc_auc_score(y_train, y_train_prob))\n\ny_test_pred = rfc.predict(X_tests)\ny_test_prob = rfc.predict_proba(X_tests)[:,1]\n\nprint('\\n')\nprint('Confusion Matrix - Test: \\n', confusion_matrix(y_test, y_test_pred))\nprint('\\n')\nprint('Overall Accuracy - Test: ', accuracy_score(y_test, y_test_pred))\n'''print('AUC - Test: ', roc_auc_score(y_test, y_test_prob))\n\n\nfpr,tpr,th=roc_curve(y_test, y_test_prob)\nfig,ax=plt.subplots()\nplt.plot(fpr,tpr)\nplt.plot(fpr,fpr)\nax1=ax.twinx()'''","39ad3773":"from sklearn.naive_bayes import GaussianNB\ngnb=GaussianNB()\ngnb.fit(X_trains,y_train)","d8f3cac4":"y_train_pred = gnb.predict(X_trains)\ny_train_prob = gnb.predict_proba(X_trains)[:,1]\n\nprint('Confusion Matrix - Train: \\n', confusion_matrix(y_train, y_train_pred))\nprint('\\n')\nprint('Overall Accuracy - Train: ', accuracy_score(y_train, y_train_pred))\n\n\ny_test_pred = gnb.predict(X_tests)\ny_test_prob = gnb.predict_proba(X_tests)[:,1]\n\nprint('\\n')\nprint('Confusion Matrix - Test: \\n', confusion_matrix(y_test, y_test_pred))\nprint('\\n')\nprint('Overall Accuracy - Test: ', accuracy_score(y_test, y_test_pred))","4adbcb5f":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import RandomizedSearchCV,GridSearchCV\nfrom scipy.stats import randint as sp_randint\n\nknn=KNeighborsClassifier()\n\nparams={'n_neighbors':sp_randint(1,20),'p':sp_randint(1,5)}\n\nrsearch_knn=RandomizedSearchCV(knn,param_distributions=params,cv=3,n_iter=50,return_train_score=True,random_state=3,n_jobs=-1)\nrsearch_knn.fit(Xs,y)","4a9da5c2":"rsearch_knn.best_params_","35c9b59e":"from sklearn.metrics import confusion_matrix,accuracy_score,classification_report,roc_auc_score,roc_curve\n\n\nknn=KNeighborsClassifier(**rsearch_knn.best_params_)\n\n# done above\nknn.fit(X_trains,y_train)\ny_train_pred = knn.predict(X_trains)\ny_train_prob = knn.predict_proba(X_trains)[:,1]\n\nprint('Confusion Matrix - Train: \\n', confusion_matrix(y_train, y_train_pred))\nprint('\\n')\nprint('Overall Accuracy - Train: ', accuracy_score(y_train, y_train_pred))\n#print('AUC - Train: ', roc_auc_score(y_train, y_train_prob))\n\ny_test_pred = knn.predict(X_tests)\ny_test_prob = knn.predict_proba(X_tests)[:,1]\n\nprint('\\n')\nprint('Confusion Matrix - Test: \\n', confusion_matrix(y_test, y_test_pred))\nprint('\\n')\nprint('Overall Accuracy - Test: ', accuracy_score(y_test, y_test_pred))\nprint('Classification Report-Test: \\n', classification_report(y_test,y_test_pred))\n#print('AUC - Test: ', roc_auc_score(y_test, y_test_prob))\n","15251445":"from sklearn.cluster import KMeans","b48c3c30":"model = KMeans(n_clusters = 3)","ebeb2819":"from scipy.stats import zscore\ndf_scaled=df.apply(zscore)","fe4cd9a2":"cluster_range = range( 1, 10 )\ncluster_errors = []\nfor num_clusters in cluster_range:\n  clusters = KMeans( num_clusters, n_init = 10 )\n  clusters.fit(df_scaled)\n  cluster_errors.append( clusters.inertia_ )\nclusters_df = pd.DataFrame( { \"num_clusters\":cluster_range, \"cluster_errors\": cluster_errors } )\nclusters_df[0:15]","745e86c9":"plt.figure(figsize=(12,6))\nplt.plot( clusters_df.num_clusters, clusters_df.cluster_errors, marker = \"o\" )","2dc5841b":"kmeans=KMeans(n_clusters=3, n_init=15,random_state=3)","e3e80ea7":"kmeans.fit(df_scaled)","2d1832d1":"centroids=kmeans.cluster_centers_","9aecd42e":"pd.DataFrame(centroids,columns=df.columns)","0e969cb8":"df_scaled['Class']=kmeans.labels_.astype('object')","bccfb078":"df_scaled['Class'].value_counts()","f7aeeb25":"df_k=df_scaled.copy()\ndf_k.head()","02f86ea5":"from mpl_toolkits.mplot3d import Axes3D","485429e8":"fig = plt.figure(figsize=(8, 6))\nax = Axes3D(fig, rect=[0, 0, .95, 1], elev=20, azim=100)\nkmeans.fit(df)\nlabels = kmeans.labels_\nax.scatter(df_scaled.iloc[:, 0], df_scaled.iloc[:, 1], df_scaled.iloc[:, 3],c=labels.astype(np.float), edgecolor='k')\nax.w_xaxis.set_ticklabels([])\nax.w_yaxis.set_ticklabels([])\nax.w_zaxis.set_ticklabels([])\nax.set_xlabel('Length')\nax.set_ylabel('Height')\nax.set_zlabel('Weight')\nax.set_title('3D plot of KMeans Clustering')","a9205476":"from scipy.cluster.hierarchy import linkage, dendrogram\nplt.figure(figsize=[10,10])\nmerg = linkage(df, method='ward')\ndendrogram(merg, leaf_rotation=90)\nplt.title('Dendrogram')\nplt.xlabel('Data Points')\nplt.ylabel('Euclidean Distances')\nplt.show()","e21cde84":"from sklearn.cluster import AgglomerativeClustering\n\nhie_clus = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')\ncluster2 = hie_clus.fit_predict(df)\n\ndf_h = df.copy(deep=True)\ndf_h['label'] = cluster2","3295b79a":"df_h","af3dd7b3":"print('K-Means Predicted Data Classes:')\nprint(df_k['Class'].value_counts())\nprint('-' * 30)\nprint('Hierarchical Predicted Data Classes:')\nprint(df_h['label'].value_counts())","a9082390":"sns.pairplot(df_h,hue='label')","8234ddb6":"from __future__ import print_function\n%matplotlib inline\n\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport numpy as np\n\nprint(__doc__)\n\n# Generating the sample data from make_blobs\n# This particular setting has one distinct cluster and 3 clusters placed close\n# together.\nX=np.array(df.drop('target',axis=1))\ny=np.array(df['target'])\n\nrange_n_clusters = [2, 3, 4, 5, 6]\n\nfor n_clusters in range_n_clusters:\n    # Create a subplot with 1 row and 2 columns\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    fig.set_size_inches(18, 7)\n\n    # The 1st subplot is the silhouette plot\n    # The silhouette coefficient can range from -1, 1 but in this example all\n    # lie within [-0.1, 1]\n    ax1.set_xlim([-0.1, 1])\n    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n    # plots of individual clusters, to demarcate them clearly.\n    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n\n    # Initialize the clusterer with n_clusters value and a random generator\n    # seed of 10 for reproducibility.\n    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n    cluster_labels = clusterer.fit_predict(X)\n\n    # The silhouette_score gives the average value for all the samples.\n    # This gives a perspective into the density and separation of the formed\n    # clusters\n    silhouette_avg = silhouette_score(X, cluster_labels)\n    print(\"For n_clusters =\", n_clusters,\n          \"The average silhouette_score is :\", silhouette_avg)\n\n    # Compute the silhouette scores for each sample\n    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n\n    y_lower = 10\n    for i in range(n_clusters):\n        # Aggregate the silhouette scores for samples belonging to\n        # cluster i, and sort them\n        ith_cluster_silhouette_values = \\\n            sample_silhouette_values[cluster_labels == i]\n\n        ith_cluster_silhouette_values.sort()\n\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n\n        color = cm.Spectral(float(i) \/ n_clusters)\n        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                          0, ith_cluster_silhouette_values,\n                          facecolor=color, edgecolor=color, alpha=0.7)\n\n        # Label the silhouette plots with their cluster numbers at the middle\n        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n        # Compute the new y_lower for next plot\n        y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax1.set_title(\"The silhouette plot for the various clusters.\")\n    ax1.set_xlabel(\"The silhouette coefficient values\")\n    ax1.set_ylabel(\"Cluster label\")\n\n    # The vertical line for average silhouette score of all the values\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n\n    ax1.set_yticks([])  # Clear the yaxis labels \/ ticks\n    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n\n    # 2nd Plot showing the actual clusters formed\n    colors = cm.Spectral(cluster_labels.astype(float) \/ n_clusters)\n    ax2.scatter(X[:,0], X[:,1], marker='.', s=30, lw=0, alpha=0.7,c=colors)\n\n    # Labeling the clusters\n    centers = clusterer.cluster_centers_\n    # Draw white circles at cluster centers\n    ax2.scatter(centers[:, 0], centers[:, 1],\n                marker='o', c=\"white\", alpha=1, s=200)\n\n    for i, c in enumerate(centers):\n        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)\n\n    ax2.set_title(\"The visualization of the clustered data.\")\n    ax2.set_xlabel(\"Feature space for the 1st feature\")\n    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n\n    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n                  \"with n_clusters = %d\" % n_clusters),\n                 fontsize=14, fontweight='bold')\n\n    plt.show()","a26fc3f5":"# Building predictive algorithm using random forest","2ecba7d7":"# KNN","96f6a3f5":"# Do Upvote if you like my work!!!","62fe83e8":"# Clustering","d1abdb2a":"# Naive Bayes"}}