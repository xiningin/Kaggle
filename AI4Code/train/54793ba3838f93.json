{"cell_type":{"767c23c4":"code","14496bbb":"code","098c08de":"code","6f4a10ba":"code","1b2a1050":"code","f536773b":"code","da490d74":"code","7b38ac6f":"code","a3754324":"code","329590ab":"code","611aa362":"code","222c31ec":"code","6e4809ae":"code","59f206b1":"code","2b549ae0":"code","2c212e0e":"code","f725cf4d":"code","a6311dda":"code","aac29b40":"code","c9a8a3fb":"code","20232e38":"code","5feff3b5":"code","40837687":"code","c2c30cee":"code","28dca5a7":"code","13ff3e28":"code","a957fd1d":"code","9372db8a":"code","b8319b67":"code","64542e53":"code","5d5efeb2":"code","d01d3e71":"code","1a3b8506":"code","6d07728f":"code","43eaf1ae":"code","3dbc82f3":"code","01c8436e":"code","1de18413":"code","ef0269d5":"code","3a32e128":"code","674c9d02":"code","cf58655b":"code","c2067977":"code","eacc7ca8":"markdown","07a9812e":"markdown","0f4a8483":"markdown","c06e1d2c":"markdown","26218a1b":"markdown","8d4de04d":"markdown","5e5e2712":"markdown","035926ed":"markdown","b22df13f":"markdown","218dadd7":"markdown","1e8b748a":"markdown","f6a999a2":"markdown","6b2acb74":"markdown","968a2f30":"markdown","b3388921":"markdown"},"source":{"767c23c4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # plotting graphs\nimport sklearn\n%matplotlib inline\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\/ai-academy-intermediate-class-competition-1\"))\n\n# Any results you write to the current directory are saved as output.","14496bbb":"TRAIN_PATH = os.path.join(\"..\/input\/ai-academy-intermediate-class-competition-1\", \"BBC News Train.csv\")\n\n#Load the data using pandas : Create a DataFrame named df, that contains the training data \ndf = pd.read_csv(TRAIN_PATH)","098c08de":"# List first 5 entries in dataframe to make sure it was loaded properly\n# and review the various colums in the dataframe\n\ndf.head()","6f4a10ba":"# Associate Category names with numerical index and save it in new column category_id\ndf['category_id'] = df['Category'].factorize()[0]\n\n#View first 10 entries of category_id, as a sanity check\ndf['category_id'][0:10]","1b2a1050":"# Create a new pandas dataframe \"category_id_df\", which only has unique Categories, also sorting this list in order of category_id values\ncategory_id_df = df[['Category', 'category_id']].drop_duplicates().sort_values('category_id')","f536773b":"category_id_df","da490d74":"# Create a dictionary ( python datastructure - like a lookup table) that \n# can easily convert category names into category_ids and vice-versa\ncategory_to_id = dict(category_id_df.values)\nid_to_category = dict(category_id_df[['category_id', 'Category']].values)","7b38ac6f":"id_to_category","a3754324":"# Pick 5 random samples from the dataframe\ndf.sample(5, random_state=0)","329590ab":"# Group the dataframe by categories and count items ( number of news articles) in each category\ndf.groupby('Category').category_id.count()\n","611aa362":"#Plot the distribution of news articles by category\ndf.groupby('Category').category_id.count().plot.bar(ylim=0)","222c31ec":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n\nfeatures = tfidf.fit_transform(df.Text).toarray() # Remaps the words in the 1490 articles in the text column of \n                                                  # data frame into features (superset of words) with an importance assigned \n                                                  # based on each words frequency in the document and across documents\n\nlabels = df.category_id                           # represents the category of each of the 1490 articles\n","6e4809ae":"#Get a feel of the features identified by tfidf\nfeatures.shape # How many features are there ? ","59f206b1":"# Remember the dictionary created to map category names to a number ? \ncategory_to_id.items()","2b549ae0":"# The sorted function Converts dictionary items into a (sorted) list. \n# In subsequent steps - We will use this list to iterate over the categories\nsorted(category_to_id.items())","2c212e0e":"# Use chi-square analysis to find corelation between features (importantce of words) and labels(news category) \nfrom sklearn.feature_selection import chi2\n\nN = 3  # We are going to look for top 3 categories\n\n#For each category, find words that are highly corelated to it\nfor Category, category_id in sorted(category_to_id.items()):\n  features_chi2 = chi2(features, labels == category_id)                   # Do chi2 analyses of all items in this category\n  indices = np.argsort(features_chi2[0])                                  # Sorts the indices of features_chi2[0] - the chi-squared stats of each feature\n  feature_names = np.array(tfidf.get_feature_names())[indices]            # Converts indices to feature names ( in increasing order of chi-squared stat values)\n  unigrams = [v for v in feature_names if len(v.split(' ')) == 1]         # List of single word features ( in increasing order of chi-squared stat values)\n  bigrams = [v for v in feature_names if len(v.split(' ')) == 2]          # List for two-word features ( in increasing order of chi-squared stat values)\n  print(\"# '{}':\".format(Category))\n  print(\"  . Most correlated unigrams:\\n       . {}\".format('\\n       . '.join(unigrams[-N:]))) # Print 3 unigrams with highest Chi squared stat\n  print(\"  . Most correlated bigrams:\\n       . {}\".format('\\n       . '.join(bigrams[-N:]))) # Print 3 bigrams with highest Chi squared stat","f725cf4d":"features_chi2","a6311dda":"from sklearn.manifold import TSNE\n\n# Sampling a subset of our dataset because t-SNE is computationally expensive\nSAMPLE_SIZE = int(len(features) * 0.3)\nnp.random.seed(0)\nindices = np.random.choice(range(len(features)), size=SAMPLE_SIZE, replace=False)          # Randomly select 30 % of samples\nprojected_features = TSNE(n_components=2, random_state=0).fit_transform(features[indices]) # Array of all projected features of 30% of Randomly chosen samples \n","aac29b40":"type(projected_features)","c9a8a3fb":"my_id = 0 # Select a category_id\nprojected_features[(labels[indices] == my_id).values]","20232e38":"colors = ['pink', 'green', 'midnightblue', 'orange', 'darkgrey']\n\n# Find points belonging to each category and plot them\nfor category, category_id in sorted(category_to_id.items()):\n    points = projected_features[(labels[indices] == category_id).values]\n    plt.scatter(points[:, 0], points[:, 1], s=30, c=colors[category_id], label=category)\nplt.title(\"tf-idf feature vector for each article, projected on 2 dimensions.\",\n          fontdict=dict(fontsize=15))\nplt.legend()","5feff3b5":"features.shape","40837687":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\n\nfrom sklearn.model_selection import cross_val_score\n\n\nmodels = [\n    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n    MultinomialNB(),\n    LogisticRegression(random_state=0),\n]\n","c2c30cee":"CV = 5  # Cross Validate with 5 different folds of 20% data ( 80-20 split with 5 folds )\n\n#Create a data frame that will store the results for all 5 trials of the 3 different models\ncv_df = pd.DataFrame(index=range(CV * len(models)))\nentries = [] # Initially all entries are empty","28dca5a7":"#For each Algorithm \nfor model in models:\n  model_name = model.__class__.__name__\n  # create 5 models with different 20% test sets, and store their accuracies\n  accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n  # Append all 5 accuracies into the entries list ( after all 3 models are run, there will be 3x5 = 15 entries)\n  for fold_idx, accuracy in enumerate(accuracies):\n    entries.append((model_name, fold_idx, accuracy))","13ff3e28":"# Store the entries into the results dataframe and name its columns    \ncv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])","a957fd1d":"import seaborn as sns\n\nsns.boxplot(x='model_name', y='accuracy', data=cv_df)\nsns.stripplot(x='model_name', y='accuracy', data=cv_df, \n              size=8, jitter=True, edgecolor=\"gray\", linewidth=2)","9372db8a":"# Mean accuracy of each algorithm\ncv_df.groupby('model_name').accuracy.mean()","b8319b67":"cv_df","64542e53":"from sklearn.model_selection import train_test_split\n\nmodel = LogisticRegression(random_state=0)\n\n#Split Data \nX_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, df.index, test_size=0.33, random_state=0)\n\n#Train Algorithm\nmodel.fit(X_train, y_train)\n\n# Make Predictions\ny_pred_proba = model.predict_proba(X_test)\ny_pred = model.predict(X_test)","5d5efeb2":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\nconf_mat = confusion_matrix(y_test, y_pred)\nsns.heatmap(conf_mat, annot=True, fmt='d',\n            xticklabels=category_id_df.Category.values, yticklabels=category_id_df.Category.values)\nplt.ylabel('Actual')\nplt.xlabel('Predicted')","d01d3e71":"from IPython.display import display\n\nfor predicted in category_id_df.category_id:\n  for actual in category_id_df.category_id:\n    if predicted != actual and conf_mat[actual, predicted] >= 2:\n      print(\"'{}' predicted as '{}' : {} examples.\".format(id_to_category[actual], id_to_category[predicted], conf_mat[actual, predicted]))\n      display(df.loc[indices_test[(y_test == actual) & (y_pred == predicted)]]['Text'])\n      print('')","1a3b8506":"model.fit(features, labels)","6d07728f":"# model.coef_ contains the importance of each feature for each category\nmodel.coef_","43eaf1ae":"from sklearn.feature_selection import chi2\n\nN = 5\nfor Category, category_id in sorted(category_to_id.items()):\n  indices = np.argsort(model.coef_[category_id])   # This time using the model co-eficients \/ weights\n  feature_names = np.array(tfidf.get_feature_names())[indices]\n  unigrams = [v for v in reversed(feature_names) if len(v.split(' ')) == 1][:N]\n  bigrams = [v for v in reversed(feature_names) if len(v.split(' ')) == 2][:N]\n  print(\"# '{}':\".format(Category))\n  print(\"  . Top unigrams:\\n       . {}\".format('\\n       . '.join(unigrams)))\n  print(\"  . Top bigrams:\\n       . {}\".format('\\n       . '.join(bigrams)))","3dbc82f3":"texts = [\"Hooli stock price soared after a dip in PiedPiper revenue growth.\",\n         \"Captain Tsubasa scores a magnificent goal for the Japanese team.\",\n         \"Merryweather mercenaries are sent on another mission, as government oversight groups call for new sanctions.\",\n         \"Beyonc\u00e9 releases a new album, tops the charts in all of south-east Asia!\",\n         \"You won't guess what the latest trend in data analysis is!\"]\ntext_features = tfidf.transform(texts)\npredictions = model.predict(text_features)\nfor text, predicted in zip(texts, predictions):\n  print('\"{}\"'.format(text))\n  print(\"  - Predicted as: '{}'\".format(id_to_category[predicted]))\n  print(\"\")","01c8436e":"import os\nprint(os.listdir(\"..\/input\/bbc-test\"))","1de18413":"TEST_PATH = os.path.join(\"..\/input\/bbc-test\", \"BBC News Test.csv\")\n\n#Load the data using pandas : Create a DataFrame\ntest_df = pd.read_csv(TEST_PATH)\n\n","ef0269d5":"test_features = tfidf.transform(test_df.Text.tolist())\n\nY_pred = model.predict(test_features)\n\nY_pred","3a32e128":"# Since all predictions are in terms of \"Category IDs (numbers)\", need to convert back to Category name\nY_pred_name =[]\nfor cat_id in Y_pred :\n    Y_pred_name.append(id_to_category[cat_id])","674c9d02":"Y_pred_name","cf58655b":"#Create Submission Dataframe\nsubmission = pd.DataFrame({\n        \"ArticleId\": test_df[\"ArticleId\"],\n        \"Category\": Y_pred_name\n    })","c2067977":"# Convert submission dataframe to csv \n# you could use any filename. We choose submission here\nsubmission.to_csv('submission.csv', index=False)","eacc7ca8":"### Store results in the results dataframe","07a9812e":"### Print confusion matrix in test data using seaborn","0f4a8483":"### Create a data frame that will store the results of various models.\nEach model will be run 5 times with different test sets of 20%","c06e1d2c":"# Finally - Use all the data to train the model ","26218a1b":"### Use seaborn to plot the results\n\nseaborn is a library that runs on top of matplotlib and makes drawing fancier plots easier\n","8d4de04d":"## Use t-SNE : A  Dimensionality reduction  technique to visualize ( in 2 diemnsions), a high dimensional space\n### t-Distributed Stochastc neighbor Embedding : Keeps similar instances close and dissimilar instances apart","5e5e2712":"# Optional\n### Study the failing scenrios \n### ****Print the cases where article was miscategorised in same way at least 2 or more times","035926ed":"## Run each Algorithm 5 times and store accuracy results in \"entries\"","b22df13f":"### Print top 5 words \/ two-word combos for each Category","218dadd7":"### Plot the 2-dimensional ditribution identified by  t-SNE","1e8b748a":"## Convert words in the news articles into numerical features using tfdif \n\nsklearn.feature_extraction.text.TfidfVectorizer will be used to calculate a tf-idf vector for each of our documents. \nNote that we are passing a number of parameters to this class:\n\n*  **sublinear_df** is set to True to use a logarithmic form for frequency, to give diminishing returns as the frequency of a word increases. This is usually preferable for a number of reasons, one of which being Zipf's Law.\n*  **min_df** is the minimum numbers of documents a word must be present in to be kept, and we are setting it to 5. This is to avoid rare words, which drastically increase the size of our features and might cause overfitting.\n*  **norm** is set to l2, to ensure all our feature vectors have a euclidian norm of 1. This is helpful for visualizing these vectors, and can also improve (or deteriorate) the performance of some models.\n* **encoding** is set to latin-1 which is used by our input text.\n*  **ngram_range** is set to (1, 2) to indicate that we want to consider both unigrams and bigrams, or in other terms: we want to consider single words (\"prices\", \"player\") and pairs of words (\"stock prices\", \"football player\").\n*  **stop_words** is set to \"english\" to remove all common pronouns (\"a\", \"the\", ...) and further reduce the number of noisy features.","f6a999a2":"# Model fit Logistic regression with 33% of data randomly chosen for test","6b2acb74":"# Model Training and Evaluation\n### We will try 3 different classification models on the data : \n            Logistic Regression\n            RandomForestClassifier\n            MultinomialNB ( Naive Bayes)","968a2f30":"## Load and Explore the Data","b3388921":"1. # Submitting your work "}}