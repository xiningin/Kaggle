{"cell_type":{"1c25a8b6":"code","02d41d26":"code","ad0efc23":"code","21a7d327":"code","369f9c1b":"code","708cde88":"code","4ee3c70c":"code","aa0ad883":"code","96edc542":"code","e1fa7e37":"code","27c3265b":"code","fba85793":"code","3137d422":"code","e6b931cd":"code","f75a5024":"code","3661e2a4":"code","e7bf7dbc":"code","6e946dcd":"code","3365190b":"code","734e3312":"markdown"},"source":{"1c25a8b6":"import os\nimport glob\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport time\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nimport PIL\nfrom IPython import display\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow import GradientTape","02d41d26":"gpus = tf.config.experimental.list_physical_devices('GPU') \nfor gpu in gpus: \n    tf.config.experimental.set_memory_growth(gpu, True)","ad0efc23":"len(os.listdir('..\/input\/cat-and-dog\/training_set\/training_set\/cats\/'))","21a7d327":"BUFFER_SIZE=4001\nBatch_size=32\n#Convert train_images to a tf.data.Dataset\npath='..\/input\/cat-and-dog\/training_set\/training_set\/cats\/cat.*.jpg'\nprint(path)\ntrain_dataset=tf.data.Dataset.list_files(tf.io.gfile.glob(path)).shuffle(BUFFER_SIZE)","369f9c1b":"def decode_img(img):\n    img = tf.image.decode_jpeg(img, channels=3) #color images\n    img = tf.image.convert_image_dtype(img, tf.float32) \n    #convert unit8 tensor to floats in the [0,1]range\n    return tf.image.resize(img, [128, 128])\n#resize the image into 224*224 \ndef process_path(file_path):\n    img = tf.io.read_file(file_path)\n    img = decode_img(img)\n    return img","708cde88":"images=[]\nfor i in train_dataset:\n    images.append(process_path(i))","4ee3c70c":"train_dataset=tf.data.Dataset.from_tensor_slices(images).shuffle(BUFFER_SIZE).batch(Batch_size)","aa0ad883":"train_dataset #Shape","96edc542":"def make_Genrator_Model():\n    model=tf.keras.Sequential()\n    model.add(layers.Dense(8*8*1024,use_bias=False,input_shape=(256,)))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    model.add(layers.Reshape((8,8,1024)))\n    \n    assert model.output_shape==(None,8,8,1024) #Debug\n    \n    model.add(layers.Conv2DTranspose(512,(16,16),strides=(2,2),padding='same',use_bias=False))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n        \n    model.add(layers.Conv2DTranspose(128,(64,64),strides=(2,2),padding='same',use_bias=False))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    \n    model.add(layers.Conv2DTranspose(64,(128,128),strides=(2,2),padding='same',use_bias=False))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    \n    model.add(layers.Conv2DTranspose(3,(128,128),strides=(1,1),padding='same',use_bias=False))\n    model.add(layers.Activation(tf.nn.tanh))\n    \n    return model","e1fa7e37":"def make_decriminator_model():\n    model=tf.keras.Sequential()\n    \n    model.add(layers.Conv2D(64,(64,64),strides=(2,2),padding='same',input_shape=[128,128,3]))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n    \n    model.add(layers.Conv2D(128,(32,32),strides=(2,2),padding='same'))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n          \n    model.add(layers.Conv2D(512,(16,16),strides=(2,2),padding='same'))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n    \n    model.add(layers.Conv2D(1024,(8,8),strides=(2,2),padding='same'))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n    \n    model.add(layers.Flatten())\n    model.add(layers.Dense(1))\n    \n    return model","27c3265b":"cross_entropy=tf.keras.losses.BinaryCrossentropy(from_logits=True,reduction=tf.keras.losses.Reduction.NONE)","fba85793":"def discriminator_loss(real_output,fake_output):\n    real_loss=cross_entropy(tf.ones_like(real_output),real_output)\n    fake_loss=cross_entropy(tf.zeros_like(fake_output),fake_output)\n    total_loss=real_loss+fake_loss\n    return total_loss","3137d422":"def generator_loss(fake_output):\n       return cross_entropy(tf.ones_like(fake_output),fake_output)","e6b931cd":"generator_optimizer=tf.keras.optimizers.Adam(1e-4)\ndiscriminator_optimizer=tf.keras.optimizers.Adam(1e-4)","f75a5024":"EPOCHS=20\nnoise_dim=256\nnum_ex_to_generate=16\ntest_random_vectors=tf.random.normal([num_ex_to_generate,noise_dim])\nprint(test_random_vectors.shape)","3661e2a4":"def train(dataset,epochs):\n    for epoch in range(epochs):\n        start=time.time()\n\n        for image_batch in dataset:\n            train_step(image_batch)\n            \n        display.clear_output(wait=True)\n        generate_and_save_images(generator,epoch+1,test_random_vectors)\n        \n        if (epoch+1)%10==0:\n            checkpoint.save(file_prefix=checkpoint_prefix)\n            \n        print('Time for epoch {} is {} sec'.format(epoch+1,time.time()-start))\n    display.clear_output(wait=True)\n    generate_and_save_images(generator,epochs,test_random_vectors)","e7bf7dbc":"def generate_and_save_images(model,epoch,test_input):\n    predictions=model(test_input,training=False)\n    fig=plt.figure(figsize=(4,4))\n    for i in range(predictions.shape[0]):\n        plt.subplot(4,4,i+1)\n        plt.imshow(predictions[i,:,:,0]*127.5+127.5,cmap='gray')\n        plt.axis('off')\n    plt.savefig('image_at_epoch{:04d}.png'.format(epoch))\n    plt.show()","6e946dcd":"@tf.function\ndef train_step(images):\n        noise=tf.random.normal([Batch_size,noise_dim])\n    \n        with GradientTape(persistent=True) as gen_tape,GradientTape(persistent=True) as disc_tape:\n            genrated_images=generator(noise,training=True)\n\n            real_output=discriminator(images,training=True)\n            fake_output=discriminator(genrated_images,training=True)\n\n            gen_loss=generator_loss(fake_output)\n            disc_loss=discriminator_loss(real_output,fake_output)\n\n        gradient_of_generator=gen_tape.gradient(gen_loss,generator.trainable_variables)\n        gradient_of_discriminator=gen_tape.gradient(disc_loss,discriminator.trainable_variables)\n\n        generator_optimizer.apply_gradients(zip(gradient_of_generator,generator.trainable_variables))\n        generator_optimizer.apply_gradients(zip(gradient_of_discriminator,discriminator.trainable_variables))","3365190b":"discriminator=make_decriminator_model()\ngenerator=make_Genrator_Model()\ncheckpoint_dir='.\/training_checkpoints_Gan'\ncheckpoint_prefix=os.path.join(checkpoint_dir,'ckpt')\ncheckpoint=tf.train.Checkpoint(generator_optimizer=generator_optimizer,discriminator_optimizer=discriminator_optimizer,\n                              generator=generator,discriminator=discriminator)\ntrain(train_dataset,EPOCHS)","734e3312":"Code was Referanced from the Course  : https:\/\/www.appliedaicourse.com\/"}}