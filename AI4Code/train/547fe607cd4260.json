{"cell_type":{"f0588dc5":"code","24c785c2":"code","1ed47119":"code","edb8a879":"code","62b38b6b":"code","2f019744":"code","2a4ccb01":"code","70d9c237":"code","646bd3be":"code","6183a6be":"code","50e6a25d":"code","d4fac5f7":"code","bfb90541":"code","3bcc7416":"code","5a71cfea":"code","d10bd806":"code","1efea63a":"code","92915e23":"code","db1da579":"code","a05b3e29":"code","ee2a67b1":"code","042b8116":"code","09d25c69":"code","eea894f7":"code","ffbf7162":"code","298ac377":"markdown","d551fe86":"markdown","12f8fc0c":"markdown","b9450153":"markdown","855511ff":"markdown","3f29e4a6":"markdown","5fa93afa":"markdown","00c82ef9":"markdown","9628f38f":"markdown","2694a673":"markdown","8909672c":"markdown","5a68f31d":"markdown"},"source":{"f0588dc5":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport time\nfrom datetime import datetime\n\nfrom sklearn.model_selection import train_test_split\n\nfrom google.cloud import storage\nfrom google.cloud import automl_v1beta1 as automl\n\nfrom automlwrapper import AutoMLWrapper","24c785c2":"# Set your own values for these. bucket_name should be the project_id + '-lcm'.\nGOOGLE_APPLICATION_CREDENTIALS={\n  \"type\": \"service_account\",\n  \"project_id\": \"cloudml-demo-263508\",\n  \"private_key_id\": \"91e776d3f8e4f6231db2e7d666fb357ced9c48bb\",\n  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvAIBADANBgkqhkiG9w0BAQEFAASCBKYwggSiAgEAAoIBAQCglGJG+EK6MZUK\\nSUYvcB6wGdvsOh5BN54a66ZdY+oeQh94ifLxloUNrNe4YRuUFHOS9tQE09Sa9Pn8\\no8SD8\/cjKZ6d0lv9YlVxn\/RmQc0yAk2kPU2I1JYVrZwmSx4O939OgBJyUaBAx\/x+\\nsX7PdbNgS0se5MG0iuO9bH8XNNa7fuZ08P49OASmBfj0gaLheBunvAbeyQGcYn8O\\nL3Cf0Tu7dMUYrTOUf9\/d2+\/exUDa66BRBQel+4EfyHy\/2yLYICKuvkuhF0lY49e9\\n1z4PWfeDHIn\/ZBFXkYI3oRM5Adt9HTsoImpk8zz17827xIP6nYKRhIJHHasMYF2U\\nkGBIQDELAgMBAAECggEARpCeVWCgdhD21UPY71y+Z5WdgsQRaohl1p+qhoyzI6Pc\\nD4zTcV27T4SecXxe8aZBGZqVFh\/+ZC\/2MRLId1H8zvtppCH+Ya1WLe3kKRGOegTw\\nGJYF3fTJlXIq1dsZtOsHCziVop4DJaScbRJTzxgCgPTlaIiPJOcJ6EJm4QxLtq19\\npvwxLvdoHDpqVYm5SLAssMO+6pEBmnPAAsU2HAb1TCpRdmQti8zbsr2CATXyGwVC\\nSMz8OHByuEryhWRdapgdaqTkPM4tU9uvdLJP97k7NiIrse8DqVgtL7IvJerL4lqI\\nYzyfenAiFsDstnXO1HweJ1cGVKxaUvwqV1J4OgO1EQKBgQDLDb+6\/C\/3ZoRpsjaQ\\nZpclvljhFvXMC4MaXY4s7t91nxqRlc9MNyRcJZOLN1zckon1S75mE2E\/JfPFhUzP\\nw6gD5msGhWrh5cbV6yhqLPNtMHGow7UqBpNUjnEEiADTIHoGPnomX4NcDigyTFXx\\nYJuzty9PcbU58bboqDSzheatewKBgQDKc2clhxSsayRlvjuP+Mrb66+Ise+7kGzP\\nN1CTQB194tyG24HReuRlnJ5FwhRtbgusOFhy9GqFqxOzYZa40L9tlOLWg2jK+Jhz\\nb1UHcwu1TXozU7Q5CyKlODs6VM6v1+ufovnY8Xrp\/tVVB9fg6nidAORS8Ggk4Kpc\\nSrZoyOkNsQKBgBmDucLaK4DP3fWWEisk6PsyJuLpyqsAC1JFzTayMVwLSQBgLbMq\\n\/qOhiR+mLkH9G965zMvwxXh04WPBczxsHnGXA33HR\/3orIp2yRZGXbQrsSx+owYL\\n23l1iJgEKu5cl1ivgxPNTqlDkiN32hjBhe8YJpa5+Aa267\/iWvTv72N\/AoGAHTUf\\nCXP+kI14l9ElKLU9FxNepLr\/GjFQ8WhKkKPsXVAQfshGZ2FyeBJrZycD6XG572PF\\npvgKMjSnR\/bpVsqPA5E6VLZqtPW+kNDcFJINe1+tJKCL3Lr1iZ6eNOEtXIMrNEHA\\nCAN2tzGTt4vFImbRdd0+E5UpgUCEc4I35NAvh1ECgYBIkMxeocrAqL3m9Yvydoew\\nC0uskSlZdBw0Zn8cQsETOtx00PlkVcBVFsH455Om8CFncFJLoQH5TaEKyswQL6lR\\nlgy95MpfQbL\/mUvQ9Cv\/QnCPyvjnZ0ZW7URkeBRGk4qa7owT7x\/CrvBJOw\/T0Nl6\\nNkqnU9iZBBDrY9Z5aNNRpw==\\n-----END PRIVATE KEY-----\\n\",\n  \"client_email\": \"girijesh21@cloudml-demo-263508.iam.gserviceaccount.com\",\n  \"client_id\": \"109765275647273349353\",\n  \"auth_uri\": \"https:\/\/accounts.google.com\/o\/oauth2\/auth\",\n  \"token_uri\": \"https:\/\/oauth2.googleapis.com\/token\",\n  \"auth_provider_x509_cert_url\": \"https:\/\/www.googleapis.com\/oauth2\/v1\/certs\",\n  \"client_x509_cert_url\": \"https:\/\/www.googleapis.com\/robot\/v1\/metadata\/x509\/girijesh21%40cloudml-demo-263508.iam.gserviceaccount.com\"\n}\n\nfrom google.cloud import storage\n\n    # If you don't specify credentials when constructing the client, the\n    # client library will look for credentials in the environment.\nstorage_client = storage.Client()\n\n    # Make an authenticated API request\nbuckets = list(storage_client.list_buckets())\nprint(buckets)\n#project_id = 'cloudml-demo-263508'\n#bucket_name = 'cloudml-demo-263508-lcm'\n\n#region = 'us-central1' # Region must be us-central1\n#dataset_display_name = 'kaggle_tweets'\n#model_display_name = 'kaggle_starter_model1'\n\n#storage_client = storage.Client(project=project_id)\n#client = automl.AutoMlClient()","1ed47119":"# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","edb8a879":"nlp_train_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\nnlp_test_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\ndef callback(operation_future):\n    result = operation_future.result()","62b38b6b":"nlp_train_df.tail()","2f019744":"nlp_train_df.loc[nlp_train_df['text'].str.contains('fire', na=False, case=False)]","2a4ccb01":"nlp_train_df.loc[nlp_train_df['text'].str.contains('fire', na=False, case=False)].target.value_counts()","70d9c237":"def upload_blob(bucket_name, source_file_name, destination_blob_name):\n    \"\"\"Uploads a file to the bucket. https:\/\/cloud.google.com\/storage\/docs\/ \"\"\"\n    bucket = storage_client.get_bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n    blob.upload_from_filename(source_file_name)\n    print('File {} uploaded to {}'.format(\n        source_file_name,\n        'gs:\/\/' + bucket_name + '\/' + destination_blob_name))\n    \ndef download_to_kaggle(bucket_name,destination_directory,file_name,prefix=None):\n    \"\"\"Takes the data from your GCS Bucket and puts it into the working directory of your Kaggle notebook\"\"\"\n    os.makedirs(destination_directory, exist_ok = True)\n    full_file_path = os.path.join(destination_directory, file_name)\n    blobs = storage_client.list_blobs(bucket_name,prefix=prefix)\n    for blob in blobs:\n        blob.download_to_filename(full_file_path)","646bd3be":"bucket = storage.Bucket(storage_client, name=bucket_name)\nif not bucket.exists():\n    bucket.create(location=region)","6183a6be":"# Select the text body and the target value, for sending to AutoML NL\nnlp_train_df[['text','target']].to_csv('train.csv', index=False, header=False) ","50e6a25d":"nlp_train_df[['id','text','target']].head()","d4fac5f7":"training_gcs_path = 'uploads\/kaggle_getstarted\/full_train.csv'\nupload_blob(bucket_name, 'train.csv', training_gcs_path)","bfb90541":"amw = AutoMLWrapper(client=client, \n                    project_id=PROJECT_ID, \n                    bucket_name=bucket_name, \n                    region='us-central1', \n                    dataset_display_name=dataset_display_name, \n                    model_display_name=model_display_name)\n       ","3bcc7416":"if not amw.get_dataset_by_display_name(dataset_display_name):\n    print('dataset not found')\n    amw.create_dataset()\n    amw.import_gcs_data(training_gcs_path)\n\namw.dataset","5a71cfea":"if not amw.get_model_by_display_name():\n    amw.train_model()\namw.deploy_model()\namw.model","d10bd806":"amw.model_full_path","1efea63a":"nlp_test_df.head()","92915e23":"# Create client for prediction service.\nprediction_client = automl.PredictionServiceClient()\namw.set_prediction_client(prediction_client)\n\npredictions_df = amw.get_predictions(nlp_test_df, \n                                     input_col_name='text', \n#                                      ground_truth_col_name='target', # we don't have ground truth in our test set\n                                     limit=None, \n                                     threshold=0.5,\n                                     verbose=False)\n","db1da579":"amw.undeploy_model()","a05b3e29":"predictions_df.head()","ee2a67b1":"submission_df = pd.concat([nlp_test_df['id'], predictions_df['class']], axis=1)\nsubmission_df.head()","042b8116":"# predictions_df['class'].iloc[:10]\n# nlp_test_df['id']","09d25c69":"submission_df = submission_df.rename(columns={'class':'target'})\nsubmission_df.head()","eea894f7":"submission_df.to_csv(\"submission.csv\", index=False, header=True)","ffbf7162":"! ls -l submission.csv","298ac377":"This notebook utilizes a utility script that wraps much of the AutoML Python client library, to make the code in this notebook easier to read. Feel free to check out the utility for all the details on how we are calling the underlying AutoML Client Library!","d551fe86":"### Data spelunking\nHow often does 'fire' come up in this dataset?","12f8fc0c":"## Create our class instance","b9450153":"## Create submission output","855511ff":"## Create (or retreive) dataset\nCheck to see if this dataset already exists. If not, create it","3f29e4a6":"## Prediction\nNote that prediction will not run until deployment finishes, which takes a bit of time.\nHowever, once you have your model deployed, this notebook won't re-train the model, thanks to the various safeguards put in place. Instead, it will take the existing (trained) model and make predictions and generate the submission file.","5fa93afa":"## Kick off the training for the model\nAnd retrieve the training info after completion. \nStart model deployment.","00c82ef9":"## (optional) Undeploy model\nUndeploy the model to stop charges","9628f38f":"### Export to CSV and upload to GCS","2694a673":"### GCS upload\/download utilities\nThese functions make upload and download of files from the kernel to Google Cloud Storage easier. This is needed for AutoML","8909672c":"Does the presence of the word 'fire' help determine whether the tweets here are real or false?","5a68f31d":"## Submit predictions to the competition!"}}