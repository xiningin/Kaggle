{"cell_type":{"d20b9ab7":"code","df80519b":"code","623ef504":"code","50b4e5fc":"code","8e338858":"code","a4fc154d":"code","95076a0f":"code","be3c16ab":"code","cb9ae256":"markdown","97d362c0":"markdown","e57cd23e":"markdown","5c8d50b6":"markdown","dc36dd34":"markdown","cc053e38":"markdown"},"source":{"d20b9ab7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"..\/input\"))\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 12, 12\nimport matplotlib.pyplot as plt","df80519b":"\ntrain_csv = pd.read_csv(\"..\/input\/photometric-redshift-estimation-2019\/train.csv\")\ntest_csv = pd.read_csv(\"..\/input\/photometric-redshift-estimation-2019\/test.csv\")\nmytry_csv = pd.read_csv(\"..\/input\/ontrainredshifts\/redshifTest.csv\")\nnames = list(train_csv)\ntrain = np.copy(train_csv.values)\ntest = np.copy(test_csv.values)\npred = np.copy(mytry_csv.values)\nnames","623ef504":"def feat_distros(sets,reso = 50):\n    hists = []\n    for i in range(sets[0].shape[1]):\n        plt.subplot(sets[0].shape[1],1,i+1)\n        bins = np.linspace(min(sets[0][:,i].min(), sets[1][:,i].min()), max(sets[0][:,i].max(), sets[1][:,i].max()), reso )\n        for sett in sets:\n            hists.append(plt.hist(sett[:,i], histtype=\"step\", bins=bins, alpha= 0.7))\n    plt.show()\n    return hists","50b4e5fc":"hists = feat_distros([test[:,:-1], train[:,:-1]], 100)","8e338858":"\nreds_hist = plt.hist(train[:,9], 100, (0,0.8),histtype=\"step\",alpha= 1.0)\nplt.title(\"The actual redshifts of the training set and the results of my model on it\")\npred_red_hist = plt.hist(pred[:,1], 100, (0,0.8),histtype=\"step\",alpha= 1.0)\nplt.legend([\"real labels\", \"estimated\"])\nplt.xlabel(\"redshift\", fontsize = 14)\nplt.ylabel(\"count\",  fontsize = 14)","a4fc154d":"miss = pred[:,1]-train[:,9]\nhist = plt.hist(miss, 400)\nplt.xlabel(\"miss\", fontsize = 14)\nplt.ylabel(\"count\",  fontsize = 14)\nmiss_plot = plt.plot([0,0], [0,45000])\n","95076a0f":"histsq = np.histogram((miss)**2, 4000)\nprint(miss.shape[0])\nn = histsq[0].cumsum()\nplt.xlabel(\"N best results\", fontsize = 14)\nplt.ylabel(\"RMSE\",  fontsize = 14)\ncumu_hist = plt.plot(n, np.sqrt((histsq[0]*histsq[1][:-1]).cumsum()\/n) )\n","be3c16ab":"bad = miss**2 > 0.08\nprint(bad.sum())\nbounds = None\nfor idn in range(0, 10):\n    plt.xlabel( names[idn]+\" parameter\", fontsize = 14)\n    plt.ylabel(\"count\",  fontsize = 14)\n    bh = plt.hist(train[bad,idn], 100, range=bounds,histtype=\"step\", alpha= 0.7, density=True)\n    ah = plt.hist(train[:,idn],  100, range=bounds,histtype=\"step\", alpha= 0.7, density = True)\n    plt.legend([\"bad misses\", \"whole trainig set\"])\n    plt.show()","cb9ae256":"Hisztogrammot k\u00e9sz\u00edtve a galaxisok egyes tulajdons\u00e1gaib\u00f3l, l\u00e1that\u00f3, hogy a teszt \u00e9s a tanul\u00f3 adathalmazban ezeknek az eloszl\u00e1sa l\u00e9nyeg\u00e9ben azonos. Ez j\u00f3 h\u00edr a modell sz\u00e1m\u00e1ra, de \u00e9rthet\u0151 is, ha a k\u00e9t halmazt \"fair\" m\u00f3don v\u00e9letlenszer\u0171en v\u00e1lasztott\u00e1k sz\u00e9t a feladat kit\u0171z\u0151i.","97d362c0":"Now we select those pieces of the training set, on which the miss is particularly great, and histogram the density of these against the parameters of galaxies","e57cd23e":"Next we make a histogram of the squares of the misses, and plot a cumulative RMSE over it in such a manner, that we consider the lowest misses first, and the greatest last. The result confirms that we had already suspected: **On nine-tenth of the data the model is quite good, but on the remainder it fouls its performance by blunders**\nI again ephasize that this is on the training set (as the actual labels of the test set are not known to us), but if there are test set elements close to those pieces of the training set on which our model failed badly, we would also fail on them. It should also be very easy to set aside a part of our training set as validation set, and use those to perform a similar analysis","5c8d50b6":"I've run a trained model on the **trainig set**, and histogrammed its result against the labels of the trainig set. The two distributions do not match. It is important to note that even if the two distributions **would** match very closely, it does not mean that the model is all good and would perform brillinatly on the test set. It could easily be the result of excessive overfitting. However if the model reproduces the labels of the training set with such a systematic bias as mine, it is probably not too good.","dc36dd34":"Now we calculate the difference of the models's output on the training set, and the actual labels. We get a neat gaussian-like curve, more or less centered on 0, indicating that our predictions are usually accuarate, but we see that sometimes there are big mistakes.","cc053e38":"Performance is slightly better on upper part of the ra parameter (one of the sky coordinates)\nThe intensity parameters display a similar pattern: At greater intensities the probability of grave miss are greater. \nOur model also performs better on bigger sizes and lower ellipticities\n\nThe most dramatic results we get on the redshift (the target variable itself) There is a range between 0.3 and 0.6 where the probability of grave miss is very small, and there are places where it is disproportionately high\n\nArmed with this knowledge we can start to think and tinker on corrective action: Perhaps we should craft a more sophisticated input normalisation strategy? I am even toying with the idea of splitting both the training and the test set to pieces according to the parameters, and fitting different models to them."}}