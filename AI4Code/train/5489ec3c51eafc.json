{"cell_type":{"fa353f92":"code","829ce519":"code","b7a75169":"code","fe6ef4bc":"code","30e3d4e4":"code","b981ffff":"code","8270517a":"code","3addbdb8":"code","23522f71":"markdown","54c6ed3b":"markdown","ee90044d":"markdown","a93a1d79":"markdown","33b4e5f3":"markdown","d742ccf5":"markdown","defc1157":"markdown","d0489037":"markdown"},"source":{"fa353f92":"%%capture\n!pip install ..\/input\/sentencetransformer\/sentence-transformers-1.0.4","829ce519":"import os\nfrom typing import List, Optional, Tuple\n\nimport numpy as np\nimport pandas as pd\nfrom cuml.neighbors import NearestNeighbors\nfrom PIL import Image\nfrom sentence_transformers import SentenceTransformer\nfrom tqdm.notebook import tqdm","b7a75169":"SUBMIT = False\n\ndf = pd.read_csv(\"..\/input\/shopee-product-matching\/test.csv\")\nif len(df) > 3:\n    SUBMIT = True\n\n    \nclass CFG:\n    data_dir = \"..\/input\/shopee-product-matching\/\"\n    csv = \"test.csv\" if SUBMIT else \"train.csv\"\n    images_dir = \"test_images\/\" if SUBMIT else \"train_images\/\"\n    model_name = \"..\/input\/clip-model\/clip\"\n    batch_size = 512","fe6ef4bc":"def create_dataset(\n    data_dir: str, \n    csv: str, \n    images_dir: str\n) -> Tuple[pd.DataFrame, List[str], List[str]]:\n\n    df = pd.read_csv(os.path.join(data_dir, csv))\n    df[\"image_path\"] = df.image.apply(lambda x: os.path.join(data_dir, images_dir, x))\n    tmp = df.groupby(\"image_phash\").posting_id.agg(\"unique\").to_dict()\n    df[\"preds_phash\"] = df.image_phash.map(tmp)\n    df[\"preds_phash\"] = df.preds_phash.apply(lambda x: \" \".join(x))\n    return df, df.image_path.to_list(), df.title.to_list()","30e3d4e4":"def create_embeddings(\n    model: SentenceTransformer,\n    batch_size: int,\n    data: List[str],\n    is_image: Optional[bool] = False,\n) -> np.ndarray:\n\n    embeddings = np.empty((0, 512))\n    CTS = int(np.ceil(len(data) \/ batch_size))\n    for i in tqdm(range(CTS), total=CTS):\n        a = i * batch_size\n        b = (i + 1) * batch_size\n        b = min(b, len(data))\n        batch_data = data[a:b]\n        if is_image:\n            batch_data = [Image.open(filepath) for filepath in batch_data]\n        batch_emb = model.encode(batch_data, convert_to_numpy=True, show_progress_bar=False)\n        embeddings = np.concatenate((embeddings, batch_emb), axis=0)\n    return embeddings","b981ffff":"def get_neighbors(\n    df: pd.DataFrame,\n    embeddings: np.ndarray,\n    n_neighbors: int,\n    metric: str,\n    threshold: float,\n) -> List[str]:\n\n    model = NearestNeighbors(n_neighbors=n_neighbors, metric=metric)\n    model.fit(embeddings)\n    distances, indices = model.kneighbors(embeddings)\n    predictions = []\n    for k in tqdm(range(embeddings.shape[0])):\n        idx = np.where(distances[k, ] < threshold)[0]\n        ids = indices[k, idx]\n        posting_ids = \" \".join(df[\"posting_id\"].iloc[ids].values)\n        predictions.append(posting_ids)\n    return predictions","8270517a":"def combine_predictions(row: pd.Series) -> str:\n    x = \" \".join((row[\"preds_phash\"], row[\"preds_images\"], row[\"preds_titles\"]))\n    return \" \".join([*{*x.split()}])\n\n\ndef f1_score(y_true: pd.Series, y_pred: pd.Series) -> float:\n    y_true = y_true.apply(lambda x: set(x.split()))\n    y_pred = y_pred.apply(lambda x: set(x.split()))\n    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n    len_y_pred = y_pred.apply(lambda x: len(x)).values\n    len_y_true = y_true.apply(lambda x: len(x)).values\n    f1 = 2 * intersection \/ (len_y_pred + len_y_true)\n    return f1\n\n\ndef print_f1_score(df: pd.DataFrame) -> None:\n    tmp = df.groupby([\"label_group\"]).posting_id.unique().to_dict()\n    df[\"target\"] = df.label_group.map(tmp)\n    df[\"target\"] = df.target.apply(lambda x: \" \".join(x))\n    for column in [\"preds_phash\", \"preds_images\", \"preds_titles\", \"matches\"]:\n        df[\"f1\"] = f1_score(df[column], df[\"target\"])\n        score = df[\"f1\"].mean()\n        print(f\"\\tF1 score associated with columns {column} is: {score}\")","3addbdb8":"print(\"Loading data and model...\")\ndf, images, titles = create_dataset(CFG.data_dir, CFG.csv, CFG.images_dir)\nmodel = SentenceTransformer(CFG.model_name)\n\nprint(\"Gettings images embeddings...\")\nimages_emb = create_embeddings(model, CFG.batch_size, images, is_image=True)\n\nprint(\"Gettings titles embeddings...\")\ntitles_emb = create_embeddings(model, CFG.batch_size, titles)\n\nprint(\"Gettings images predictions...\")\ndf[\"preds_images\"] = get_neighbors(df, images_emb, n_neighbors=50, metric=\"euclidean\", threshold=4.5)\n\nprint(\"Getting titles predictions...\")\ndf[\"preds_titles\"] = get_neighbors(df, titles_emb, n_neighbors=50, metric=\"euclidean\", threshold=3)\n\ndf[\"matches\"] = df.apply(combine_predictions, axis=1)\n\nif not SUBMIT:\n    print(\"Getting scores...\")\n    print_f1_score(df)\n\ndf[[\"posting_id\", \"matches\"]].to_csv(\"submission.csv\", index=False)\nprint(\"\\nFile 'submission.csv' successfully saved.\")","23522f71":"# Neighbors","54c6ed3b":"In the end we compare the F1 score for 4 approaches:\n1. Using the *image_phash* values\n2. Using KNN on the images embeddings\n3. Using KNN on the titles embeddings\n4. Combining the previous 3 predictions, which yields a surprisingly passable score\n\nThe next steps for developping this notebook would be to:\n- Try different letrics (i. e. cosine) for the ```NearestNeighbors``` model\n- Add a function to find the optimal threshold defining embeddings representing the same product\n- And obviously fine-tune CLIP :P\n  \n  \nIf you have other ideas and\/or suggestions, please leave a comment below. ","ee90044d":"# Score","a93a1d79":"# CLIP embeddings","33b4e5f3":"# Zero-shot learning for images and text with CLIP\n\nIn this very minimalist notebook I use the [Sentence-transformers](https:\/\/www.sbert.net\/examples\/applications\/image-search\/README.html) implementation [OpenAI's CLIP](https:\/\/openai.com\/blog\/clip\/) model to generate embeddings for both text and images, and then use KNN to find similar products in the resulting embedding space. \n\nThis is an example of zero-shot learning, an \"extreme\" version of transfer learning where the model isn't fine-tuned on the target task before inference. \n\nThis approach is useful to quickly get results and assess the difficulty of the task at hand while going a step further than a baseline model, which in this case would be simply predicting products with identical ```image_phash``` values as identical.  \n\nPlus it's fun to experiment with cutting-edge models and the possibility they offer, like projecting images and sentences in a same embedding space as shown in the Sentence-transformers [documentation](https:\/\/www.sbert.net\/examples\/applications\/image-search\/README.html):\n> SentenceTransformers provides models that allow to embed images and text into the same vector space.  \n> This allows to find similar images as well as to implement image search.  \n>  \n> ![](https:\/\/raw.githubusercontent.com\/UKPLab\/sentence-transformers\/master\/docs\/img\/ImageSearch.png)","d742ccf5":"# Setup","defc1157":"# Data","d0489037":"# Engine"}}