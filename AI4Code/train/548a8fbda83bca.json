{"cell_type":{"3519c168":"code","8b4fdd1c":"code","284424b5":"code","8c4c8bae":"code","b1977ee4":"code","5d64e436":"code","d7155067":"code","2f723fe9":"code","4068752c":"code","621a03d5":"code","1a1e9ee0":"code","a03385cf":"code","fabfc70b":"code","35104204":"code","1a5a72af":"code","ccea28c8":"code","5272ec84":"code","dceaea4a":"code","55f10e88":"code","18d03542":"code","e4aef463":"code","1f6d0b52":"code","563c1c42":"code","4ccc05a6":"code","ca117af4":"code","9cb436d2":"code","91c8e041":"markdown"},"source":{"3519c168":"import os\nimport time\nimport progressbar\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom keras.backend.tensorflow_backend import set_session\nimport keras\nimport sys, time, os, warnings \nimport numpy as np\nimport pandas as pd \nfrom collections import Counter \nfrom keras.preprocessing.image import load_img\nfrom nltk.tokenize import word_tokenize\nwarnings.filterwarnings(\"ignore\")\n\n","8b4fdd1c":"## The location of the caption file\n#dir_Flickr_text = \"..\/input\/flickr-image-dataset\/flickr30k_images\/flickr30k_images\/results.csv\"\n#dir_Flickr_jpg = \"..\/input\/flickr-image-dataset\/flickr30k_images\/flickr30k_images\/flickr30k_images\"\ndir_Flickr_text = \"..\/input\/flickr8k-sau\/flickr8k-sau\/Flickr_Data\/Flickr_TextData\/Flickr8k.token.txt\"\ndir_Flickr_jpg = \"..\/input\/flickr8k-sau\/flickr8k-sau\/Flickr_Data\/Images\"\n\njpgs = os.listdir(dir_Flickr_jpg)\nprint(\"The number of jpg flies in Flicker30k: {}\".format(len(jpgs)))","284424b5":"## loading as dataframe\ndef load_csv(directory):\n    desc=dict()\n    text = pd.read_csv(directory, delimiter='|',header=None,names=[\"filename\",\"index\",\"caption\"])\n    text = text.iloc[1:,:]\n    df_new = text[text.iloc[:,2].notnull()]\n    print(df_new.iloc[:5,:])\n    return df_new  ","8c4c8bae":"\n\nfile = open(dir_Flickr_text,'r')\ntext = file.read()\nfile.close()\n\n\ndatatxt = []\nfor line in text.split('\\n'):\n    col = line.split('\\t')\n    if len(col) == 1:\n        continue\n    w = col[0].split(\"#\")\n    datatxt.append(w + [col[1].lower()])\n\ndf_txt = pd.DataFrame(datatxt,columns=[\"filename\",\"index\",\"caption\"])\n\n\nuni_filenames = np.unique(df_txt.filename.values)\nprint(\"The number of unique file names : {}\".format(len(uni_filenames)))\nprint(\"The distribution of the number of captions for each image:\")\nCounter(Counter(df_txt.filename.values).values())\n\n","b1977ee4":"from keras.preprocessing.image import load_img, img_to_array\n\nnpic = 5\nnpix = 224\ntarget_size = (npix,npix,3)\n\ncount = 1\nfig = plt.figure(figsize=(10,20))\nfor jpgfnm in uni_filenames[:npic]:\n    filename = dir_Flickr_jpg + '\/' + jpgfnm\n    captions = list(df_txt[\"caption\"].loc[df_txt[\"filename\"]==jpgfnm].values)\n    image_load = load_img(filename, target_size=target_size)\n    \n    ax = fig.add_subplot(npic,2,count,xticks=[],yticks=[])\n    ax.imshow(image_load)\n    count += 1\n    \n    ax = fig.add_subplot(npic,2,count)\n    plt.axis('off')\n    ax.plot()\n    ax.set_xlim(0,1)\n    ax.set_ylim(0,len(captions))\n    for i, caption in enumerate(captions):\n        ax.text(0,i,caption,fontsize=20)\n    count += 1\nplt.show()","5d64e436":"def df_word(df_txt):\n    vocabulary = []\n    for i in range(len(df_txt)):\n        temp=df_txt.iloc[i,2]\n        vocabulary.extend(temp.split())\n    print('Vocabulary Size: %d' % len(set(vocabulary)))\n    ct = Counter(vocabulary)\n    dfword = pd.DataFrame({\"word\":list(ct.keys()),\"count\":list(ct.values())})\n    dfword = dfword.sort_values(\"count\",ascending=False)\n    dfword = dfword.reset_index()[[\"word\",\"count\"]]\n    return(dfword)\ndfword = df_word(df_txt)\ndfword.head(3)","d7155067":"topn = 50\n\ndef plthist(dfsub, title=\"The top 50 most frequently appearing words\"):\n    plt.figure(figsize=(20,3))\n    plt.bar(dfsub.index,dfsub[\"count\"])\n    plt.yticks(fontsize=20)\n    plt.xticks(dfsub.index,dfsub[\"word\"],rotation=90,fontsize=20)\n    plt.title(title,fontsize=20)\n    plt.show()\n\nplthist(dfword.iloc[:topn,:],\n        title=\"The top 50 most frequently appearing words\")\nplthist(dfword.iloc[-topn:,:],\n        title=\"The least 50 most frequently appearing words\")","2f723fe9":"import string\ndef remove_punctuation(text_original):\n    text_no_punctuation = text_original.translate(str.maketrans('','',string.punctuation))\n    return(text_no_punctuation)\n\ndef remove_single_character(text):\n    text_len_more_than1 = \"\"\n    for word in text.split():\n        if len(word) > 1:\n            text_len_more_than1 += \" \" + word\n    return(text_len_more_than1)\n\ndef remove_numeric(text,printTF=False):\n    text_no_numeric = \"\"\n    for word in text.split():\n        isalpha = word.isalpha()\n        if printTF:\n            print(\"    {:10} : {:}\".format(word,isalpha))\n        if isalpha:\n            text_no_numeric += \" \" + word\n    return(text_no_numeric)\n","4068752c":"def text_clean(text_original):\n    text = remove_punctuation(text_original)\n    text = remove_single_character(text)\n    text = remove_numeric(text)\n    return(text)\n\nwith progressbar.ProgressBar(max_value=len(df_txt.caption.values)) as bar:\n    for i, caption in enumerate(df_txt.caption.values):\n        newcaption = text_clean(caption)\n        df_txt[\"caption\"].iloc[i] = newcaption\n        bar.update(i)\n    ","621a03d5":"dfword = df_word(df_txt)\nplthist(dfword.iloc[:topn,:],\n        title=\"The top 50 most frequently appearing words\")\nplthist(dfword.iloc[-topn:,:],\n        title=\"The least 50 most frequently appearing words\")","1a1e9ee0":"from copy import copy\ndef add_start_end_seq_token(captions):\n    caps = []\n    for txt in captions:\n        txt = 'startseq ' + txt + ' endseq'\n        caps.append(txt)\n    return(caps)\ndf_txt0 = copy(df_txt)\ndf_txt0[\"caption\"] = add_start_end_seq_token(df_txt[\"caption\"])\ndf_txt0.head(5)\ndel df_txt\n","a03385cf":"from keras.applications import VGG16\n\nmodelvgg = VGG16(include_top=True,weights=None)\n## load the locally saved weights \nmodelvgg.load_weights(\"..\/input\/vgg16-weights-image-captioning\/vgg16_weights_tf_dim_ordering_tf_kernels.h5\")\nmodelvgg.summary()","fabfc70b":"from keras import models\nmodelvgg.layers.pop()\nmodelvgg = models.Model(inputs=modelvgg.inputs, outputs=modelvgg.layers[-1].output)\n## show the deep learning model\nmodelvgg.summary()","35104204":"from keras.preprocessing.image import load_img, img_to_array\nfrom keras.applications.vgg16 import preprocess_input\nfrom collections import OrderedDict\n\nimages = OrderedDict()\nnpix = 224\ntarget_size = (npix,npix,3)\nwith progressbar.ProgressBar(max_value=len(jpgs)) as bar:\n    for i,name in enumerate(jpgs):\n        # load an image from file\n        filename = dir_Flickr_jpg + '\/' + name\n        image = load_img(filename, target_size=target_size)\n        # convert the image pixels to a numpy array\n        image = img_to_array(image)\n        nimage = preprocess_input(image)\n        y_pred = modelvgg.predict(nimage.reshape( (1,) + nimage.shape[:3]))\n        images[name] = y_pred.flatten()\n        bar.update(i)\n    #print(i,filename)","1a5a72af":"dimages, keepindex = [],[]\nnd=(df_txt0[\"index\"].values)\nb = [(int(i)==0) for i in nd]\n#for i in nd:\n #   print(int(i)==0)\n#df_txt0 = df_txt0.loc[b,: ]\ndf_txt0 = df_txt0.loc[df_txt0[\"index\"].values == \"0\",: ]\n\nfor i, fnm in enumerate(df_txt0.filename):\n    if fnm in images.keys():\n        dimages.append(images[fnm])\n        keepindex.append(i)\n        \nfnames = df_txt0[\"filename\"].iloc[keepindex].values\ndcaptions = df_txt0[\"caption\"].iloc[keepindex].values\ndimages = np.array(dimages)\nprint(df_txt0[\"index\"][:5])","ccea28c8":"from keras.preprocessing.text import Tokenizer\n## the maximum number of words in dictionary\ncount_words=22000\n#nb_words = 31782\ntokenizer = Tokenizer(num_words=8000)\ntokenizer.fit_on_texts(dcaptions)\nvocab_size = len(tokenizer.word_index) + 1\nprint(\"vocabulary size : {}\".format(vocab_size))\ndtexts = tokenizer.texts_to_sequences(dcaptions)\nprint(dtexts[:5])","5272ec84":"prop_test, prop_val = 0.2, 0.2 \n\nN = len(dtexts)\nNtest, Nval = int(N*prop_test), int(N*prop_val)\n\ndef split_test_val_train(dtexts,Ntest,Nval):\n    return(dtexts[:Ntest], \n           dtexts[Ntest:Ntest+Nval],  \n           dtexts[Ntest+Nval:])\n\ndt_test,  dt_val, dt_train   = split_test_val_train(dtexts,Ntest,Nval)\ndi_test,  di_val, di_train   = split_test_val_train(dimages,Ntest,Nval)\nfnm_test,fnm_val, fnm_train  = split_test_val_train(fnames,Ntest,Nval)","dceaea4a":"maxlen = np.max([len(text) for text in dtexts])","55f10e88":"from keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\n\ndef preprocessing(dtexts,dimages):\n    N = len(dtexts)\n    print(\"# captions\/images = {}\".format(N))\n\n    assert(N==len(dimages))\n    Xtext, Ximage, ytext = [],[],[]\n    for text,image in zip(dtexts,dimages):\n\n        for i in range(1,len(text)):\n            in_text, out_text = text[:i], text[i]\n            in_text = pad_sequences([in_text],maxlen=maxlen).flatten()\n            out_text = to_categorical(out_text,num_classes = vocab_size)\n\n            Xtext.append(in_text)\n            Ximage.append(image)\n            ytext.append(out_text)\n\n    Xtext  = np.array(Xtext)\n    Ximage = np.array(Ximage)\n    ytext  = np.array(ytext)\n    print(\" {} {} {}\".format(Xtext.shape,Ximage.shape,ytext.shape))\n    return(Xtext,Ximage,ytext)\n\n\nXtext_train, Ximage_train, ytext_train = preprocessing(dt_train,di_train)\nXtext_val,   Ximage_val,   ytext_val   = preprocessing(dt_val,di_val)\n# pre-processing is not necessary for testing data\n#Xtext_test,  Ximage_test,  ytext_test  = preprocessing(dt_test,di_test)","18d03542":"from keras import layers\nprint(vocab_size)\n## image feature\n\ndim_embedding = 64\n\ninput_image = layers.Input(shape=(Ximage_train.shape[1],))\nfimage = layers.Dense(256,activation='relu',name=\"ImageFeature\")(input_image)\n## sequence model\ninput_txt = layers.Input(shape=(maxlen,))\nftxt = layers.Embedding(vocab_size,dim_embedding, mask_zero=True)(input_txt)\nftxt = layers.LSTM(256,name=\"CaptionFeature\")(ftxt)\n## combined model for decoder\ndecoder = layers.add([ftxt,fimage])\ndecoder = layers.Dense(256,activation='relu')(decoder)\noutput = layers.Dense(vocab_size,activation='softmax')(decoder)\nmodel = models.Model(inputs=[input_image, input_txt],outputs=output)\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\nprint(model.summary())","e4aef463":"start = time.time()\n#checkpoint_path = \"training_1\/cp.ckpt\"\n#checkpoint_dir = os.path.dirname(checkpoint_path)\n\n# Create checkpoint callback\n#cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n #                                                save_weights_only=True,\n  #                                               verbose=2)\n\nhist = model.fit([Ximage_train, Xtext_train], ytext_train, \n                  epochs=7, verbose=2, \n                  batch_size=64,\n                  validation_data=([Ximage_val, Xtext_val], ytext_val))\n                #callbacks = [cp_callback])\nend = time.time()\nprint(\"TIME TOOK {:3.2f}MIN\".format((end - start )\/60))","1f6d0b52":"print(Ximage_train.shape,Xtext_train.shape,ytext_train.shape)","563c1c42":"for label in [\"loss\",\"val_loss\"]:\n    plt.plot(hist.history[label],label=label)\nplt.legend()\nplt.xlabel(\"epochs\")\nplt.ylabel(\"loss\")\nplt.show()","4ccc05a6":"index_word = dict([(index,word) for word, index in tokenizer.word_index.items()])\ndef predict_caption(image):\n    '''\n    image.shape = (1,4462)\n    '''\n\n    in_text = 'startseq'\n\n    for iword in range(maxlen):\n        sequence = tokenizer.texts_to_sequences([in_text])[0]\n        sequence = pad_sequences([sequence],maxlen)\n        yhat = model.predict([image,sequence],verbose=0)\n        yhat = np.argmax(yhat)\n        newword = index_word[yhat]\n        in_text += \" \" + newword\n        if newword == \"endseq\":\n            break\n    return(in_text)\n\n\n\nnpic = 5\nnpix = 224\ntarget_size = (npix,npix,3)\n\ncount = 1\nfig = plt.figure(figsize=(10,20))\nfor jpgfnm, image_feature in zip(fnm_test[:npic],di_test[:npic]):\n    ## images \n    filename = dir_Flickr_jpg + '\/' + jpgfnm\n    image_load = load_img(filename, target_size=target_size)\n    ax = fig.add_subplot(npic,2,count,xticks=[],yticks=[])\n    ax.imshow(image_load)\n    count += 1\n\n    ## captions\n    caption = predict_caption(image_feature.reshape(1,len(image_feature)))\n    ax = fig.add_subplot(npic,2,count)\n    plt.axis('off')\n    ax.plot()\n    ax.set_xlim(0,1)\n    ax.set_ylim(0,1)\n    ax.text(0,0.5,caption,fontsize=20)\n    count += 1\n\nplt.show()","ca117af4":"from nltk.translate.bleu_score import sentence_bleu\nindex_word = dict([(index,word) for word, index in tokenizer.word_index.items()])\n\n\nnkeep = 5\npred_good, pred_bad, bleus = [], [], [] \ncount = 0 \nfor jpgfnm, image_feature, tokenized_text in zip(fnm_test,di_test,dt_test):\n    count += 1\n    if count % 200 == 0:\n        print(\"  {:4.2f}% is done..\".format(100*count\/float(len(fnm_test))))\n    \n    caption_true = [ index_word[i] for i in tokenized_text ]     \n    caption_true = caption_true[1:-1] ## remove startreg, and endreg\n    ## captions\n    caption = predict_caption(image_feature.reshape(1,len(image_feature)))\n    caption = caption.split()\n    caption = caption[1:-1]## remove startreg, and endreg\n    \n    bleu = sentence_bleu([caption_true],caption)\n    bleus.append(bleu)\n    if bleu > 0.7 and len(pred_good) < nkeep:\n        pred_good.append((bleu,jpgfnm,caption_true,caption))\n    elif bleu < 0.3 and len(pred_bad) < nkeep:\n        pred_bad.append((bleu,jpgfnm,caption_true,caption))","9cb436d2":"\ndef plot_images(pred_bad):\n    def create_str(caption_true):\n        strue = \"\"\n        for s in caption_true:\n            strue += \" \" + s\n        return(strue)\n    npix = 224\n    target_size = (npix,npix,3)    \n    count = 1\n    fig = plt.figure(figsize=(10,20))\n    npic = len(pred_bad)\n    for pb in pred_bad:\n        bleu,jpgfnm,caption_true,caption = pb\n        ## images \n        filename = dir_Flickr_jpg + '\/' + jpgfnm\n        image_load = load_img(filename, target_size=target_size)\n        ax = fig.add_subplot(npic,2,count,xticks=[],yticks=[])\n        ax.imshow(image_load)\n        count += 1\n\n        caption_true = create_str(caption_true)\n        caption = create_str(caption)\n        \n        ax = fig.add_subplot(npic,2,count)\n        plt.axis('off')\n        ax.plot()\n        ax.set_xlim(0,1)\n        ax.set_ylim(0,1)\n        ax.text(0,0.7,\"true:\" + caption_true,fontsize=20)\n        ax.text(0,0.4,\"pred:\" + caption,fontsize=20)\n        ax.text(0,0.1,\"BLEU: {}\".format(bleu),fontsize=20)\n        count += 1\n    plt.show()\n\nprint(\"Bad Caption\")\nplot_images(pred_bad)\nprint(\"Good Caption\")\nplot_images(pred_good)\n ","91c8e041":"\ndf_txt= load_csv(dir_Flickr_text)\nuni_filenames = np.unique(df_txt.filename.values)\nprint(\"The number of unique file names : {}\".format(len(uni_filenames)))\nprint(\"The distribution of the number of captions for each image:\")\nCounter(Counter(df_txt.filename.values).values())"}}