{"cell_type":{"757895eb":"code","f6503630":"code","2aca6e2a":"code","535e9af9":"code","752d1961":"code","646abcfb":"code","3abb797c":"code","6848b61c":"code","f3b035fc":"code","f29a4bf9":"code","6a0975d5":"code","ce9b03c2":"code","8af424d4":"code","2f0fad04":"code","0cdda40a":"code","f5212eb3":"code","fce1add2":"code","861940e1":"code","a1f2db3d":"code","4c861a67":"code","046a60b9":"code","26823bae":"code","a014d13c":"code","b305d7ee":"code","5324fd20":"code","b8cdc989":"code","00ca9c77":"code","b8498f0c":"code","6052e846":"code","3130d896":"code","4ea625e9":"code","9bd431c0":"markdown","41920918":"markdown","f7cdf80d":"markdown","7f524769":"markdown","2179349f":"markdown","b960788c":"markdown","562d5564":"markdown","713ef00a":"markdown","b02ad360":"markdown","ee18fb05":"markdown","d8f41151":"markdown","111e2fb5":"markdown","7cc0f35c":"markdown","2ceec568":"markdown","ef405b07":"markdown","ed3654cd":"markdown","451a6f52":"markdown","5e717cf1":"markdown","d1cdfb42":"markdown"},"source":{"757895eb":"# Check the files in Data\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f6503630":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport time\n\nimport torch\nfrom torchtext import data\nimport torch.nn as nn\n","2aca6e2a":"# Import Data\n\ntest = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")","535e9af9":"# Shape of dataset\ntrain.shape","752d1961":"train.head()","646abcfb":"# drop 'id' , 'keyword' and 'location' columns.\ntrain.drop(columns=['id','keyword','location'], inplace=True)","3abb797c":"# to clean data\ndef normalise_text (text):\n    text = text.str.lower() # lowercase\n    text = text.str.replace(r\"\\#\",\"\") # replaces hashtags\n    text = text.str.replace(r\"http\\S+\",\"URL\")  # remove URL addresses\n    text = text.str.replace(r\"@\",\"\")\n    text = text.str.replace(r\"[^A-Za-z0-9()!?\\'\\`\\\"]\", \" \")\n    text = text.str.replace(\"\\s{2,}\", \" \")\n    return text","6848b61c":"train[\"text\"]=normalise_text(train[\"text\"])","f3b035fc":"train['text'].head()","f29a4bf9":"# split data into train and validation \ntrain_df, valid_df = train_test_split(train)\n","6a0975d5":"train_df.head()","ce9b03c2":"valid_df.head()","8af424d4":"SEED = 42\n\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n\n","2f0fad04":"TEXT = data.Field(tokenize = 'spacy', include_lengths = True)\nLABEL = data.LabelField(dtype = torch.float)","0cdda40a":"# source : https:\/\/gist.github.com\/lextoumbourou\/8f90313cbc3598ffbabeeaa1741a11c8\n# to use DataFrame as a Data source\n\nclass DataFrameDataset(data.Dataset):\n\n    def __init__(self, df, fields, is_test=False, **kwargs):\n        examples = []\n        for i, row in df.iterrows():\n            label = row.target if not is_test else None\n            text = row.text\n            examples.append(data.Example.fromlist([text, label], fields))\n\n        super().__init__(examples, fields, **kwargs)\n\n    @staticmethod\n    def sort_key(ex):\n        return len(ex.text)\n\n    @classmethod\n    def splits(cls, fields, train_df, val_df=None, test_df=None, **kwargs):\n        train_data, val_data, test_data = (None, None, None)\n        data_field = fields\n\n        if train_df is not None:\n            train_data = cls(train_df.copy(), data_field, **kwargs)\n        if val_df is not None:\n            val_data = cls(val_df.copy(), data_field, **kwargs)\n        if test_df is not None:\n            test_data = cls(test_df.copy(), data_field, True, **kwargs)\n\n        return tuple(d for d in (train_data, val_data, test_data) if d is not None)","f5212eb3":"fields = [('text',TEXT), ('label',LABEL)]\n\ntrain_ds, val_ds = DataFrameDataset.splits(fields, train_df=train_df, val_df=valid_df)","fce1add2":"# Lets look at a random example\nprint(vars(train_ds[15]))\n\n# Check the type \nprint(type(train_ds[15]))","861940e1":"MAX_VOCAB_SIZE = 25000\n\nTEXT.build_vocab(train_ds, \n                 max_size = MAX_VOCAB_SIZE, \n                 vectors = 'glove.6B.200d',\n                 unk_init = torch.Tensor.zero_)\n\n","a1f2db3d":"LABEL.build_vocab(train_ds)","4c861a67":"BATCH_SIZE = 128\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ntrain_iterator, valid_iterator = data.BucketIterator.splits(\n    (train_ds, val_ds), \n    batch_size = BATCH_SIZE,\n    sort_within_batch = True,\n    device = device)","046a60b9":"# Hyperparameters\nnum_epochs = 25\nlearning_rate = 0.001\n\nINPUT_DIM = len(TEXT.vocab)\nEMBEDDING_DIM = 200\nHIDDEN_DIM = 256\nOUTPUT_DIM = 1\nN_LAYERS = 2\nBIDIRECTIONAL = True\nDROPOUT = 0.2\nPAD_IDX = TEXT.vocab.stoi[TEXT.pad_token] # padding","26823bae":"class LSTM_net(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n                 bidirectional, dropout, pad_idx):\n        \n        super().__init__()\n        \n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n        \n        self.rnn = nn.LSTM(embedding_dim, \n                           hidden_dim, \n                           num_layers=n_layers, \n                           bidirectional=bidirectional, \n                           dropout=dropout)\n        \n        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)\n        \n        self.fc2 = nn.Linear(hidden_dim, 1)\n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, text, text_lengths):\n        \n        # text = [sent len, batch size]\n        \n        embedded = self.embedding(text)\n        \n        # embedded = [sent len, batch size, emb dim]\n        \n        #pack sequence\n        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n        \n        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n        \n        #unpack sequence\n        # output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n\n        # output = [sent len, batch size, hid dim * num directions]\n        # output over padding tokens are zero tensors\n        \n        # hidden = [num layers * num directions, batch size, hid dim]\n        # cell = [num layers * num directions, batch size, hid dim]\n        \n        # concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n        # and apply dropout\n        \n        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n        output = self.fc1(hidden)\n        output = self.dropout(self.fc2(output))\n                \n        #hidden = [batch size, hid dim * num directions]\n            \n        return output","a014d13c":"#creating instance of our LSTM_net class\n\nmodel = LSTM_net(INPUT_DIM, \n            EMBEDDING_DIM, \n            HIDDEN_DIM, \n            OUTPUT_DIM, \n            N_LAYERS, \n            BIDIRECTIONAL, \n            DROPOUT, \n            PAD_IDX)","b305d7ee":"pretrained_embeddings = TEXT.vocab.vectors\n\nprint(pretrained_embeddings.shape)\nmodel.embedding.weight.data.copy_(pretrained_embeddings)","5324fd20":"#  to initiaise padded to zeros\nmodel.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n\nprint(model.embedding.weight.data)","b8cdc989":"model.to(device) #CNN to GPU\n\n\n# Loss and optimizer\ncriterion = nn.BCEWithLogitsLoss()\n\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)","00ca9c77":"def binary_accuracy(preds, y):\n    \"\"\"\n    Returns accuracy per batch, i.e. if you get 8\/10 right, this returns 0.8, NOT 8\n    \"\"\"\n\n    #round predictions to the closest integer\n    rounded_preds = torch.round(torch.sigmoid(preds))\n    correct = (rounded_preds == y).float() #convert into float for division \n    acc = correct.sum() \/ len(correct)\n    return acc","b8498f0c":"# training function \ndef train(model, iterator):\n    \n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.train()\n    \n    for batch in iterator:\n        text, text_lengths = batch.text\n        \n        optimizer.zero_grad()\n        predictions = model(text, text_lengths).squeeze(1)\n        loss = criterion(predictions, batch.label)\n        acc = binary_accuracy(predictions, batch.label)\n\n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n        \n\n    return epoch_loss \/ len(iterator), epoch_acc \/ len(iterator)","6052e846":"def evaluate(model, iterator):\n    \n    epoch_acc = 0\n    model.eval()\n    \n    with torch.no_grad():\n        for batch in iterator:\n            text, text_lengths = batch.text\n            predictions = model(text, text_lengths).squeeze(1)\n            acc = binary_accuracy(predictions, batch.label)\n            \n            epoch_acc += acc.item()\n        \n    return epoch_acc \/ len(iterator)","3130d896":"t = time.time()\nloss=[]\nacc=[]\nval_acc=[]\n\nfor epoch in range(num_epochs):\n    \n    train_loss, train_acc = train(model, train_iterator)\n    valid_acc = evaluate(model, valid_iterator)\n    \n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\t Val. Acc: {valid_acc*100:.2f}%')\n    \n    loss.append(train_loss)\n    acc.append(train_acc)\n    val_acc.append(valid_acc)\n    \nprint(f'time:{time.time()-t:.3f}')","4ea625e9":"plt.xlabel(\"runs\")\nplt.ylabel(\"normalised measure of loss\/accuracy\")\nx_len=list(range(len(acc)))\nplt.axis([0, max(x_len), 0, 1])\nplt.title('result of LSTM')\nloss=np.asarray(loss)\/max(loss)\nplt.plot(x_len, loss, 'r.',label=\"loss\")\nplt.plot(x_len, acc, 'b.', label=\"accuracy\")\nplt.plot(x_len, val_acc, 'g.', label=\"val_accuracy\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.2)\nplt.show\n","9bd431c0":"### Declare Hyperparameters","41920918":"#### Let us get a glimpse at the data table","f7cdf80d":"### Setting up the LSTM model","7f524769":"## LSTM architecture","2179349f":"The following will help make the results reproducible later.\n","b960788c":"#### The `target` column marks the label of the text:\n* **\n* **label==1** : If the Tweet is about Disasters.\n* **label==0** : If the Tweet is not about disasters. \n\nWe are only interested in the `text` and `target` columns. So we drop the rest. ","562d5564":"We need to create `Field` objects to process the text data. These field objects will contain information for converting the texts to Tensors.\nWe will set two parameters:\n* `tokenize=spacy` and \n* `include_arguments=True`\nWhich implies that SpaCy will be used to tokenize the texts and that the field objects should include length of the texts - which will be needed to pad the texts. \nWe will later use methods of these objects to create a vocabulary, which will help us create a numerical representation for every token.\n\nThe `LabelField` is a shallow wrappper around field, useful for data labels. ","713ef00a":"We will now build the vocabulary using only the training dataset. This can be accessed through `TEXT.vocab` and will be shared by the validation dataset.\n\nWe will use pretrainied 200 dimensional vectors to represent the tokens. Any unknown token will have a zero vector. These vectors will be later loaded as the embedding layer. ","b02ad360":"Let us look at the cleaned text once","ee18fb05":"loading the pretrained vectors into the embedding matrix. ","d8f41151":"We build the iterators. ","111e2fb5":"### Plot a graph to trace model performance","7cc0f35c":"### Training the model","2ceec568":"Next we create a `DataFrameDataset` class which will allow us to load the data and the target-labels as a `DataSet` using a DataFrame as a source of data.\nWe will create a vocabulary using the training dataset and then pass the training and validation datasets to the iterator later. \n","ef405b07":"### Next we clean and modify the texts, so that the classification algorithm does not get confused with irrelevant information. ","ed3654cd":"* We will first create a list called _field_, where the elements will be a tuple of string (name) and `Field` object. The `Field` object for the text should be placed with name 'text'  and the object for label should be placed with name 'label'\n\nThen we will use the `splits` method of `DataFrameDataset`, which will return the training and validation datasets, which will be composed of Examples of the tokenized texts and labels. The texts and labels will have the name that we provide to the _field_.  ","451a6f52":"Split the data into training and validation sets","5e717cf1":"## **Data Pre Processing**\nCleaning the text data ","d1cdfb42":"# Sentence Classification\n\n\nIn this notebook, We will be classifying text (The data-set used here contains tweets, but the process shown here can be adapted for other text classification tasks too.)\nThe content is arranged as follows:\n* Cleaninig and basic pre-processing of text\n* Building a vocabulary, and creating iterators using TorchText\n* Building a sequence model - LSTM using Pytorch to predict labels\n\n**_Notebook is still under construction...._**\n"}}