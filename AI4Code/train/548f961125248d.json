{"cell_type":{"eb76b3bd":"code","b61f864b":"code","7552a8d6":"code","70e7a3ea":"code","ae7709c1":"code","4542608c":"code","6ce3e253":"code","c3d79b98":"code","7367bf0c":"code","466e4d24":"code","18cf249c":"code","44f52534":"code","73198819":"code","08b58fe4":"code","74982c25":"code","dad4e050":"code","82229839":"code","694dce65":"code","c82f85f2":"code","c57aeea4":"code","ea6e7117":"code","80f9f5c9":"code","a643d0f2":"code","710cd7cd":"code","72821f20":"code","9439f971":"code","16b4fb7d":"code","1e941c3b":"code","3d7ae19d":"code","05f44ca2":"code","a1ab3a0f":"code","c2d6d106":"code","d69f1ef7":"code","9708a002":"code","fb771313":"code","3e8ecf7b":"code","892b5c57":"code","6a3b0772":"code","b4968817":"code","f4a96f55":"code","b2b07857":"markdown","7ed52b9c":"markdown","3513cae7":"markdown","df2a67dc":"markdown","d490cba6":"markdown","d77aeaa7":"markdown","a1c6f4da":"markdown","82a40a08":"markdown","f9e5137c":"markdown","d1be9567":"markdown","b4c4dbd7":"markdown","738c166a":"markdown","6a6f1c52":"markdown","d63fcf29":"markdown","f0a4f41b":"markdown","b46fa56e":"markdown","af585285":"markdown","3ef880a1":"markdown","660229b9":"markdown","edbc8fb2":"markdown","d36b8e8c":"markdown","5c762534":"markdown","fe0fbd69":"markdown","a2f7e3d7":"markdown","fd1d2e85":"markdown","791c9954":"markdown","ffc97fa2":"markdown","4b94a978":"markdown","07ed2bda":"markdown"},"source":{"eb76b3bd":"import shap\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report,confusion_matrix, precision_recall_curve, roc_curve, roc_auc_score, log_loss\nfrom sklearn.model_selection import train_test_split\nfrom catboost import CatBoostClassifier, cv, Pool\nfrom hyperopt import hp, fmin, tpe, Trials, STATUS_OK\nfrom itertools import combinations\n\n%matplotlib inline\nsns.set(style='ticks')\npd.options.display.max_columns = 500\npd.options.display.max_rows = 500","b61f864b":"# Read in data\ntest = pd.read_csv(\"..\/input\/amazon-employee-access-challenge\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/amazon-employee-access-challenge\/train.csv\")","7552a8d6":"def performance(model, X_test, y_test):\n    \n    \"\"\"\n    Accepts a fitted model and an evaluation dataset at input.\n    Prints the confusion matrix, classification_report & auc score. \n    Also, displays Precision-Recall curve & ROC curve.\n    \"\"\"\n    \n    # Make predictions on test set\n    y_pred=model.predict(X_test)\n    y_pred=np.round(y_pred)\n    \n    # Confusion matrix\n    print(confusion_matrix(y_test, y_pred))\n    \n    # AUC score\n    y_pred_prob = model.predict_proba(X_test)\n    print(\"AUC score: \", roc_auc_score(y_test, y_pred_prob[:,1]))\n    \n    # Logloss\n    print(\"Logloss : \", log_loss(y_test, y_pred_prob))\n\n    # Accuracy, Precision, Recall, F1 score\n    print(classification_report(y_test, y_pred))\n    \n    # Precision-Recall curve\n    precisions, recalls, thresholds = precision_recall_curve(y_test, y_pred)\n    plt.plot(recalls, precisions, \"b-\", linewidth=2)\n    plt.xlabel(\"Recall\", fontsize=16)\n    plt.ylabel(\"Precision\", fontsize=16)\n    plt.axis([0, 1, 0, 1])\n    plt.grid(True)\n    plt.show()\n\n    # ROC curve\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob[:,1])\n    plt.plot([0, 1], [0, 1],'k--')\n    plt.plot(fpr, tpr, label='Neural Network')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    plt.show()\n","70e7a3ea":"print(\"Train shape: {}, Test shape: {}\".format(train.shape, test.shape))","ae7709c1":"train.head()","4542608c":"print(train.isnull().any()) \nprint(test.isnull().any())","6ce3e253":"# Compare number of Unique Categorical labels for train and test\n\nunique_train= pd.DataFrame([(col,train[col].nunique()) for col in train.columns], \n                           columns=['Columns', 'Unique categories'])\nunique_test=pd.DataFrame([(col,test[col].nunique()) for col in test.columns],\n                columns=['Columns', 'Unique categories'])\nunique_train=unique_train[1:]\nunique_test=unique_test[1:]\n\nfig, ax = plt.subplots(2, 1, sharex=True, sharey=True)\nax[0].bar(unique_train.Columns, unique_train['Unique categories'])\nax[1].bar(unique_test.Columns, unique_test['Unique categories'])\nplt.xticks(rotation=90)","c3d79b98":"sns.countplot(train.ACTION)","7367bf0c":"# Check for duplicated rows\n\nif (sum(train.duplicated()), sum(test.duplicated())) == (0,0):\n    print('No duplicated rows')\nelse: \n    print('train: ',sum(train.duplicated()))\n    print('test: ',sum(train.duplicated()))","466e4d24":"# Check for duplicated columns                          \n\nfor col1,col2 in combinations(train.columns, 2):\n    condition1=len(train.groupby([col1,col2]).size())==len(train.groupby([col1]).size())\n    condition2=len(train.groupby([col1,col2]).size())==len(train.groupby([col2]).size())\n    condition3=(train[col1].nunique()==train[col2].nunique())\n    if (condition1 | condition2) & condition3:\n        print(col1,col2)\n        print('Potential Categorical column duplication')","18cf249c":"train.groupby(['ROLE_TITLE', 'ROLE_CODE']).mean()","44f52534":"np.random.seed(123)","73198819":"# Drop duplicated column\ntrain.drop('ROLE_CODE', axis=1, inplace=True)\ntest.drop('ROLE_CODE', axis=1, inplace=True)","08b58fe4":"# Split into features and target\ny = train['ACTION']\nX = train.drop('ACTION', axis=1)\n\n# Split into train & validation set\nX_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.8)","74982c25":"cat_features = [*range(8)]","dad4e050":"model = CatBoostClassifier(custom_metric=['TotalF1'], early_stopping_rounds=100, eval_metric='AUC')\n\nmodel.fit(X_train, y_train, cat_features=cat_features,\n          eval_set=(X_val, y_val), plot=True, verbose=False, use_best_model=True)","82229839":"performance(model, X_val, y_val)","694dce65":"feat_imp=model.get_feature_importance(prettified=True)\nplt.bar(feat_imp['Feature Id'], feat_imp['Importances'])\nplt.xlabel('Features')\nplt.ylabel('Feature Importance')\nplt.xticks(rotation=90)","c82f85f2":"sub=pd.read_csv(\"..\/input\/amazon-employee-access-challenge\/sampleSubmission.csv\")\nsum(test.id==sub.Id), test.shape\n\n#sub.to_csv('amazon1.csv', index=False, header=True)","c57aeea4":"y_pred=model.predict_proba(test.drop('id', axis=1))\nsub.Action=y_pred[:,1]","ea6e7117":"sub.to_csv('amazon1.csv', index=False, header=True)\nsub.head()","80f9f5c9":"model.get_all_params()","a643d0f2":"\"\"\"\nCOmmented out as it takes too long to run. \nUnder construction, some things can be improved.\nspace = {\n    'depth': hp.quniform(\"depth\", 1, 16, 1),\n    'border_count': hp.quniform('border_count', 32, 255, 1),\n    'l2_leaf_reg': hp.uniform('l2_leaf_reg', 3, 8),\n    #'rsm': hp.uniform('rsm', 0.1, 1), # use only when task_type is default CPU\n    'scale_pos_weight': hp.uniform('scale_pos_weight', 0.06, 1), # Can be set only when loss_function is default Logloss\n    #'loss_function' : hp.choice('loss_function', ['Logloss', 'CrossEntropy'])\n}\n\n\ndef hyperparameter_tuning(space):\n    model = CatBoostClassifier(depth=int(space['depth']),\n                               border_count=space['border_count'],\n                               l2_leaf_reg=space['l2_leaf_reg'],\n                               #rsm=space['rsm'],\n                               scale_pos_weight=space['scale_pos_weight']\n                               #loss_function=space['loss_function'],\n                               task_type='GPU', # change to CPU when working on personal system\n                               eval_metric='AUC'\n                               early_stopping_rounds=100,\n                              thread_count=-1)\n\n    model.fit(X_train, y_train, cat_features=cat_features,use_best_model=True,\n              verbose=False, eval_set=(X_val, y_val))\n\n    preds_class = model.predict_proba(X_val)\n    #score = classification_report(y_val, preds_class, output_dict=True)['0']['f1-score']\n    score = roc_auc_score(y_val, preds_class[:,1])\n    return{'loss': 1-score, 'status': STATUS_OK}\n\n\nbest = fmin(fn=hyperparameter_tuning,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=50)\n\nprint(best)\n\"\"\"","710cd7cd":"# Best of the tuned models\nmodel = CatBoostClassifier(border_count=248, depth=4, l2_leaf_reg=4.830204209625978,\n                           scale_pos_weight=0.4107081177319144, \n                           eval_metric='AUC',\n                           use_best_model=True,\n                          early_stopping_rounds=100)\nbest=model.fit(X_train, y_train, cat_features=cat_features, eval_set=(X_val, y_val), use_best_model=True,\n          verbose=False, plot=False)","72821f20":"performance(model, X_val, y_val)","9439f971":"model = CatBoostClassifier(border_count=248, depth=4, l2_leaf_reg=4.830204209625978,\n                           scale_pos_weight=0.4107081177319144, iterations = 400)\nmodel.fit(X_train, y_train, cat_features=cat_features,\n          verbose=False, plot=False)\nshap.initjs()\nexplainer = shap.TreeExplainer(model)","16b4fb7d":"print('Probability of class 1 = {:.4f}'.format(model.predict_proba(X_train.iloc[2:3])[0][1]))\n#print('Formula raw prediction = {:.4f}'.format(model.predict(X_train.iloc[0:1], prediction_type='RawFormulaVal')[0]))","1e941c3b":"shap_values = explainer.shap_values(Pool(X_train, y_train, cat_features=cat_features))\nshap.force_plot(explainer.expected_value, shap_values[2,:], X_train.iloc[2,:])","3d7ae19d":"shap.summary_plot(shap_values, X_train)","05f44ca2":"model = CatBoostClassifier(border_count=248, depth=4, l2_leaf_reg=4.830204209625978,\n                           scale_pos_weight=0.4107081177319144,\n                           loss_function='Logloss',\n                           eval_metric='AUC',\n                           use_best_model=True,\n                          early_stopping_rounds=100)\ncv_data = cv(Pool(X_train, y_train, cat_features=cat_features), params=model.get_params(),\n             verbose=False)","a1ab3a0f":"score = np.max(cv_data['test-AUC-mean'])\nprint('AUC score from cross-validation: ', score)","c2d6d106":"cv_data['test-AUC-mean'].plot()\nplt.xlabel('Iterations')\nplt.ylabel('test-AUC-Mean')","d69f1ef7":"clf = xgb.XGBClassifier()\nclf.fit(X_train, y_train)","9708a002":"performance(clf, X_val, y_val)","fb771313":"train_data = lgb.Dataset(X_train, label=y_train)","3e8ecf7b":"param = {'objective': 'binary'}\nparam['metric'] = 'auc'\nbst = lgb.train(train_set=train_data, params=param)","892b5c57":"y_pred_prob=bst.predict(X_val)\ny_pred=np.round(y_pred_prob)\n\n# Confusion matrix\nprint(confusion_matrix(y_val, y_pred))","6a3b0772":"# AUC score\nprint(\"AUC score: \", roc_auc_score(y_val, y_pred_prob))","b4968817":"# Logloss\nprint(\"Logloss : \", log_loss(y_val, y_pred_prob))","f4a96f55":"# Accuracy, Precision, Recall, F1 score\nprint(classification_report(y_val, y_pred))","b2b07857":"### Shap\n\n* Refer to https:\/\/www.kaggle.com\/dansbecker\/shap-values for explanation of SHAP. \n* \"SHAP values interpret the impact of having a certain value for a given feature in comparison to the prediction we'd make if that feature took some baseline value.\"","7ed52b9c":"* The best model has a **loss of 0.137** and an **AUC score of 89.7%**. \n\n\n* RESOURCE & ROLE_DEPTNAME are the most important features. \n\n\n* Let us look at model performance of the test set now by making a submission to Kaggle. \n","3513cae7":"* It is important to tell CatBoost which columns are categorical and which ones are text. If no information is provided - CatBoost assumes all features are numerical. \n\n\n* Default values of CatBoostClassifier() parameters depend of the type of input data - CatBoost automatically applies the best settings. Catboost can distinguish between binary & multiclass problems - it will appropriately assign 'Logloss' as the 'loss_function' for Binary problems, 'MultiClass' for multiclass problems and 'RMSE' for regression problems. \n\n\n* Default number of 'iterations' is 1000. I set early stopping rounds to 100 (for boosting algos high patience values give best models) for the first run and I also selected 'use_best_model'= True . (When we fit using the model, we want to use the best model, rather than the potentially substandard model saved in memory at the end of training). 'custom_metric' provides an additional plot to moniter while CatBoost fits (It does not change training performance). 'eval_metric' is the metric used for 'best model' selection. \n\n\n* When fitting 'eval_set' is optional. If we provide 'eval_metric' and 'use_best_model' (metric used for overfit detection), we will need to provide 'eval_set'. \n\n\n* AUC selected as parameter of choice - as Kaggle competition requires this. Logloss also studied to prove Yandex's claims about Catboost's superior performance. \n\n\n* Input data can be in many different tabular forms. \n    - If only a dataframe is provided, first column is assumed to be the target. Rest of the columns are assumed to be features. \n    - We can provide a dataframe of features and a dataframe\/array of target values, as we do in Sklearn. \n    - The Pool() class is specific to CatBoost. ","df2a67dc":"## Other Boosting Algorithms <a class=\"anchor\" id=\"ninth\"><\/a>","d490cba6":"## Imports & Read in file <a class=\"anchor\" id=\"first\"><\/a>","d77aeaa7":"* The dataset represents a case of Imbalanced classes. The 0 label has fewer values.","a1c6f4da":"### Cross validation to check for overfitting\n\n* Explanation of what cv() function in Catboost does: \"The dataset is split into N folds. N\u20131 folds are used for training, and one fold is used for model performance estimation. N models are updated on each iteration K. Each model is evaluated on its' own validation dataset on each iteration. This produces N metric values on each iteration K. \"","82a40a08":"## Test set performance <a class=\"anchor\" id=\"eighth\"><\/a>","f9e5137c":"* There is 97% likelihood for positive label - for this instance.\n\n\n* From the plot we see that the base value is 2.638. \n* The SHAP values of all features sum up to explain why our prediction is different from the baseline (value of +3.46 - a positive value).\n* The contribution of each of the features towards change from base values is shown in the plot. \n* 'RESOURCES' contributes towards a more positive values. All the other features contribute to more negative value for the prediction in row 2. ","d1be9567":"### Balance of target labels","b4c4dbd7":"* The Training and Test data have different subsets of categorical variables.","738c166a":"### LightGBM","6a6f1c52":"### Null values","d63fcf29":"\n\n* Target: ACTION\n* 9 categorical features (represented as numbers for privacy reasons)\n\n","f0a4f41b":"### Functions","b46fa56e":"# Catboost & Hyperopt : Amazon employees dataset\n\n* **Information from [Kaggle](https:\/\/www.kaggle.com\/c\/amazon-employee-access-challenge\/overview)**\n    - When an employee at any company starts work, they first need to obtain the computer access necessary to fulfill their role.\n    - **Given data about current employees and their provisioned access, models can be built that automatically determine access privileges as employees enter and leave roles within a company.** These auto-access models seek to minimize the human involvement required to grant or revoke employee access.\n    \n    \n* **Catboost**: \n    - **Yandex, the developers of Catboost, claim that default Catboost provides ~20% logloss improvement over LightGMB & XGBoost. Tuning further improves performance of the model.**\n    - **I will be testing these claims.**\n    - Catboost uses gradient boosted trees. Great for working on catgorical data and mixed data (with both categorical and numerical features)\n    - Data is quantized into bins. The algorithm decides bin 'borders'(We can set our own values too). This quantization supports faster integration into parallel processing workflows. \n    - Symmetric gradient boosted trees are built, each subsequent tree improves the performance of the previous set of trees. \n    - Categorical preprocessing steps like One-Hot-Encoding, text preprocessing steps like tokenization, Bag of Words models can be performed within the Catboost algorithm (No need for additional preprocessing.)  \n    \n    \n* **RESULT**:\n    - **One of the columns had duplicated information. After removing this column - the default algorithm gave the best loss publicised by Yandex (~0.137). A kaggle submission showed 90% AUC score.**\n    - Hyperopt tuning did not improve scores. \n    - Yandex's claims were proven. It had the best loss among the boosting models as shown in table below. \n    \n    \n<table>\n  <tr>\n    <th>Model<\/th>\n    <th>Logloss from default<\/th>\n  <\/tr>\n  <tr>\n    <td>Catboost<\/td>\n    <td>0.13516505504697254<\/td>\n  <\/tr>\n  <tr>\n    <td>Xgboost<\/td>\n    <td>0.1554555542790197<\/td>\n  <\/tr>\n  <tr>\n    <td>LightGBM<\/td>\n    <td>0.16383632381872779<\/td>\n  <\/tr>\n<\/table>\n    \n    \n    \n## Table of Contents\n* [Imports & Read in file](#first)\n* [Explore data](#second)\n* [Preprocessing](#third)\n* [Baseline Model](#fifth)\n* [Test set performance](#eighth)\n* [Hyperparameter tuning](#sixth)\n* [Model validation](#seventh)\n* [Other Boosting Algorithms](#ninth)\n\n<img src=\"https:\/\/images.freeimages.com\/images\/large-previews\/753\/go-to-work-1189863.jpg\" alt=\"Drawing\" style=\"width: 300px;\"\/>\n\n\n* **Description of Features:**\n    \n<table>\n  <tr>\n    <th>Label<\/th>\n    <th>Description<\/th>\n  <\/tr>\n  <tr>\n    <td>ACTION<\/td>\n    <td>ACTION is 1 if the resource was approved, 0 if the resource was not<\/td>\n  <\/tr>\n  <tr>\n    <td>RESOURCE<\/td>\n    <td>An ID for each resource<\/td>\n  <\/tr>\n  <tr>\n    <td>EMPLOYEE ID<\/td>\n    <td>The EMPLOYEE ID of the manager of the current EMPLOYEE ID record; an employee may have only one manager at a time<\/td>\n  <\/tr>\n  <tr>\n    <td>ROLE_ROLLUP_1<\/td>\n    <td>Company role grouping category id 1 (e.g. US Engineering)<td>\n  <\/tr>\n  <tr>\n    <td>ROLE_ROLLUP_2<\/td>\n    <td>Company role grouping category id 2 (e.g. US Retail)<\/td>\n  <\/tr>\n  <tr>\n    <td>ROLE_DEPTNAME<\/td>\n    <td>Company role department description (e.g. Retail)<\/td>\n  <\/tr>\n  <tr>\n    <td>ROLE_TITLE<\/td>\n    <td>Company role business title description (e.g. Senior Engineering Retail Manager)<\/td>\n  <\/tr>\n  <tr>\n    <td>ROLE_FAMILY_DESC<\/td>\n    <td>Company role family extended description (e.g. Retail Manager, Software Engineering)<\/td>\n  <\/tr>\n  <tr>\n    <td>ROLE_FAMILY<\/td>\n    <td>Company role family description (e.g. Retail Manager)<\/td>\n  <\/tr>\n  <tr>\n    <td>ROLE_CODE<\/td>\n    <td>Company role code; this code is unique to each role (e.g. Manager)<\/td>\n<\/table>","af585285":"## Model Validation <a class=\"anchor\" id=\"seventh\"><\/a>","3ef880a1":"## Explore data <a class=\"anchor\" id=\"second\"><\/a>","660229b9":"## Preprocessing <a class=\"anchor\" id=\"third\"><\/a>","edbc8fb2":"Contribution of all the features and all the instances of the features towards predictions are shown using the red\/blue dots. ","d36b8e8c":"* We will look at SHAP values for a single row of the dataset (we arbitrarily chose row 2). For context, we'll look at the raw predictions before looking at the SHAP values. ","5c762534":"* Some differnt sets of parameters returned as 'best' by hyperopt. \n* {'border_count': 209.0, 'depth': 8.0, 'l2_leaf_reg': 7.476976878626717, 'loss_function': 0, 'rsm': 0.7556557996868841}\n* {'border_count': 248.0, 'depth': 4.0, 'l2_leaf_reg': 4.830204209625978, 'scale_pos_weight': 0.4107081177319144}\n* {'border_count': 129.0, 'depth': 10.0, 'l2_leaf_reg': 4.450385969436819, 'scale_pos_weight': 0.1034646048953394}","fe0fbd69":"* ROLE_TITLE and ROLE_CODE represent the same data. One of the two features can be dropped. ","a2f7e3d7":"\n## Hyperparameter tuning <a class=\"anchor\" id=\"sixth\"><\/a>","fd1d2e85":"### Xgboost","791c9954":"### Set random seed","ffc97fa2":"### Correlations","4b94a978":"## Baseline Model <a class=\"anchor\" id=\"fifth\"><\/a>","07ed2bda":"* The default model gives an **AUC score of 0.90373**. Not bad for the first model!! \n* The winning submission has a score of 92%. Lets see if we can tune the model to squeeze out the last 2%. \n\n\n* We will study the paramaters of the default model and try to provide a sensible range for hyperopt to tune on. "}}