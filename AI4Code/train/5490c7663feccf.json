{"cell_type":{"8de11f59":"code","52244a69":"code","2af4034e":"code","81b217d5":"code","ba243c1a":"code","ab0f6583":"code","b8c01fc5":"code","2b8798c0":"code","a6e5c30d":"code","22ba2f14":"code","4e23a409":"code","60285300":"code","f3b97494":"code","0f5eece5":"code","59b2a0a1":"code","81ed0090":"code","d24adea8":"code","eb1c3919":"code","9ceb655a":"code","4d7a8577":"code","01c56cdf":"code","142442bc":"code","51877865":"code","6d4f99e8":"code","9a201784":"code","dc957bbf":"code","d84e9acc":"code","d51acbef":"code","909aae84":"code","85bccccb":"code","c4fc6aeb":"code","09a9b280":"code","75bacda3":"code","2515e4aa":"code","d5cd618e":"code","64660cba":"code","46c861bc":"code","c371b2e7":"code","6c256c05":"code","3addb808":"markdown","977dfbb1":"markdown","7be31e2c":"markdown","25c49ee8":"markdown"},"source":{"8de11f59":"\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nimport warnings\nwarnings.simplefilter(action='ignore', category=Warning)\n\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV, cross_val_score, cross_validate\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBRegressor\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport joblib\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfrom pandas.core.common import SettingWithCopyWarning\nfrom sklearn.exceptions import ConvergenceWarning\n\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action=\"ignore\", category=ConvergenceWarning)\nwarnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n\npd.set_option('display.max_columns', None)\npd.set_option('display.width', 170)\npd.set_option('display.max_rows', 20)\npd.set_option('display.float_format', lambda x: '%.3f' % x)","52244a69":"############################################\n# EDA ANALIZI\n############################################\n\n########################################################################################\n# HELPERS ADINDA BIR DOSYANIN ICINE KOYUP KULLANABILIRSINIZ ASAGIDAKI FONKSIYONLARI\n########################################################################################\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\n\ndef check_df(dataframe, head=5):\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    print(\"##################### Head #####################\")\n    print(dataframe.head(head))\n    print(\"##################### Tail #####################\")\n    print(dataframe.tail(head))\n    print(\"##################### NA #####################\")\n    print(dataframe.isnull().sum())\n    print(\"##################### Quantiles #####################\")\n    print(dataframe.quantile([0, 0.05, 0.50, 0.60, 0.75, 0.95, 0.99, 1]).T)\n\n\ndef cat_summary(dataframe, col_name, plot=False):\n    print(pd.DataFrame({col_name: dataframe[col_name].value_counts(),\n                        \"Ratio\": 100 * dataframe[col_name].value_counts() \/ len(dataframe)}))\n    print(\"##########################################\")\n    if plot:\n        sns.countplot(x=dataframe[col_name], data=dataframe)\n        plt.show()\n\n\ndef num_summary(dataframe, numerical_col, plot=False):\n    quantiles = [0.05, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.99]\n    print(dataframe[numerical_col].describe(quantiles).T)\n\n    if plot:\n        dataframe[numerical_col].hist(bins=20)\n        plt.xlabel(numerical_col)\n        plt.title(numerical_col)\n        plt.show()\n\n\n\ndef grab_col_names(dataframe, cat_th=10, car_th=20):\n    \"\"\"\n\n    Veri setindeki kategorik, numerik ve kategorik fakat kardinal de\u011fi\u015fkenlerin isimlerini verir.\n    Not: Kategorik de\u011fi\u015fkenlerin i\u00e7erisine numerik g\u00f6r\u00fcn\u00fcml\u00fc kategorik de\u011fi\u015fkenler de dahildir.\n\n    Parameters\n    ------\n        dataframe: dataframe\n                De\u011fi\u015fken isimleri al\u0131nmak istenilen dataframe\n        cat_th: int, optional\n                numerik fakat kategorik olan de\u011fi\u015fkenler i\u00e7in s\u0131n\u0131f e\u015fik de\u011feri\n        car_th: int, optinal\n                kategorik fakat kardinal de\u011fi\u015fkenler i\u00e7in s\u0131n\u0131f e\u015fik de\u011feri\n\n    Returns\n    ------\n        cat_cols: list\n                Kategorik de\u011fi\u015fken listesi\n        num_cols: list\n                Numerik de\u011fi\u015fken listesi\n        cat_but_car: list\n                Kategorik g\u00f6r\u00fcn\u00fcml\u00fc kardinal de\u011fi\u015fken listesi\n\n    Examples\n    ------\n        import seaborn as sns\n        df = sns.load_dataset(\"iris\")\n        print(grab_col_names(df))\n\n\n    Notes\n    ------\n        cat_cols + num_cols + cat_but_car = toplam de\u011fi\u015fken say\u0131s\u0131\n        num_but_cat cat_cols'un i\u00e7erisinde.\n        Return olan 3 liste toplam\u0131 toplam de\u011fi\u015fken say\u0131s\u0131na e\u015fittir: cat_cols + num_cols + cat_but_car = de\u011fi\u015fken say\u0131s\u0131\n\n    \"\"\"\n\n\n    # cat_cols, cat_but_car\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n                   dataframe[col].dtypes != \"O\"]\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n                   dataframe[col].dtypes == \"O\"]\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    # num_cols\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n    return cat_cols, num_cols, cat_but_car\n\n\ndef target_summary_with_cat(dataframe, target, categorical_col):\n    print(pd.DataFrame({\"TARGET_MEAN\": dataframe.groupby(categorical_col)[target].mean()}), end=\"\\n\\n\\n\")\n\n\ndef target_summary_with_num(dataframe, target, numerical_col):\n    print(dataframe.groupby(target).agg({numerical_col: \"mean\"}), end=\"\\n\\n\\n\")\n\n\n\ndef high_correlated_cols(dataframe, plot=False, corr_th=0.90):\n    corr = dataframe.corr()\n    cor_matrix = corr.abs()\n    upper_triangle_matrix = cor_matrix.where(np.triu(np.ones(cor_matrix.shape), k=1).astype(np.bool))\n    drop_list = [col for col in upper_triangle_matrix.columns if any(upper_triangle_matrix[col] > corr_th)]\n    if plot:\n        import seaborn as sns\n        import matplotlib.pyplot as plt\n        sns.set(rc={'figure.figsize': (15, 15)})\n        sns.heatmap(corr, cmap=\"RdBu\")\n        plt.show()\n    return drop_list\n\n\ndef outlier_thresholds(dataframe, col_name, q1=0.05, q3=0.95):\n    quartile1 = dataframe[col_name].quantile(q1)\n    quartile3 = dataframe[col_name].quantile(q3)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit\n\n\ndef replace_with_thresholds(dataframe, variable, q1=0.05, q3=0.95):\n    low_limit, up_limit = outlier_thresholds(dataframe, variable, q1=0.05, q3=0.95)\n    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit\n\n\ndef check_outlier(dataframe, col_name):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):\n        return True\n    else:\n        return False\n\n\ndef grab_outliers(dataframe, col_name, index=False):\n    low, up = outlier_thresholds(dataframe, col_name)\n    if dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))].shape[0] > 10:\n        print(dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))].head())\n    else:\n        print(dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))])\n\n    if index:\n        outlier_index = dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))].index\n        return outlier_index\n\n\ndef remove_outlier(dataframe, col_name):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    df_without_outliers = dataframe[~((dataframe[col_name] < low_limit) | (dataframe[col_name] > up_limit))]\n    return df_without_outliers\n\n\ndef missing_values_table(dataframe, na_name=False):\n    na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n    n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)\n    ratio = (dataframe[na_columns].isnull().sum() \/ dataframe.shape[0] * 100).sort_values(ascending=False)\n    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])\n    print(missing_df, end=\"\\n\")\n    if na_name:\n        return na_columns\n\n\ndef missing_vs_target(dataframe, target, na_columns):\n    temp_df = dataframe.copy()\n    for col in na_columns:\n        temp_df[col + '_NA_FLAG'] = np.where(temp_df[col].isnull(), 1, 0)\n    na_flags = temp_df.loc[:, temp_df.columns.str.contains(\"_NA_\")].columns\n    for col in na_flags:\n        print(pd.DataFrame({\"TARGET_MEAN\": temp_df.groupby(col)[target].mean(),\n                            \"Count\": temp_df.groupby(col)[target].count()}), end=\"\\n\\n\\n\")\n\n\ndef label_encoder(dataframe, binary_col):\n    labelencoder = LabelEncoder()\n    dataframe[binary_col] = labelencoder.fit_transform(dataframe[binary_col])\n    return dataframe\n\n\ndef one_hot_encoder(dataframe, categorical_cols, drop_first=False):\n    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first)\n    return dataframe\n\n\ndef rare_analyser(dataframe, target, cat_cols):\n    for col in cat_cols:\n        print(col, \":\", len(dataframe[col].value_counts())) # S\u0131n\u0131f Say\u0131s\u0131\n        print(pd.DataFrame({\"COUNT\": dataframe[col].value_counts(),\n                            \"RATIO\": dataframe[col].value_counts() \/ len(dataframe),\n                            \"TARGET_MEAN\": dataframe.groupby(col)[target].mean()}), end=\"\\n\\n\\n\")\n\n\ndef rare_encoder(dataframe, rare_perc, cat_cols):\n    temp_df = dataframe.copy()\n    rare_columns = [col for col in cat_cols if (temp_df[col].value_counts() \/ len(temp_df) < rare_perc).sum() > 1]\n\n    for col in rare_columns:\n        tmp = temp_df[col].value_counts() \/ len(temp_df)\n        rare_labels = tmp[tmp < rare_perc].index\n        temp_df[col] = np.where(temp_df[col].isin(rare_labels), 'Rare', temp_df[col])\n\n    return temp_df\n","2af4034e":"############################################\n# EDA ANALIZI\n############################################\ndf = pd.read_csv(\"..\/input\/hitters\/Hitters.csv\")\ncheck_df(df)","81b217d5":"# BA\u011eIMLI DE\u011e\u0130\u015eKEN ANAL\u0130Z\u0130\n\ndf[\"Salary\"].describe()\nsns.distplot(df.Salary)\nplt.show()\n\n# Maa\u015f yo\u011funlu\u011funun 500 civar\u0131nda oldu\u011funu g\u00f6zlemliyoruz","ba243c1a":"# KATEGOR\u0130K VE NUMER\u0130K DE\u011e\u0130\u015eKENLER\u0130N SE\u00c7\u0130LMES\u0130\ncat_cols, num_cols, cat_but_car = grab_col_names(df)\n\n# KATEGOR\u0130K DE\u011e\u0130\u015eKEN ANAL\u0130Z\u0130\nrare_analyser(df, \"Salary\", cat_cols);","ab0f6583":"# SAYISAL DE\u011e\u0130\u015eKEN ANAL\u0130Z\u0130\nfor col in num_cols:\n    num_summary(df, col, plot=False)","b8c01fc5":"# AYKIRI G\u00d6ZLEM ANAL\u0130Z\u0130\nfor col in num_cols:\n    print(col, check_outlier(df, col))\n    \n# Ayk\u0131r\u0131 g\u00f6zlem olmad\u0131\u011f\u0131n\u0131 g\u00f6zlemledik","2b8798c0":"df.isnull().sum()\n# Salary de\u011fi\u015fkeninde 59 tane bo\u015f g\u00f6zlem oldu\u011funu g\u00f6zlemledik","a6e5c30d":"missing_values_table(df)\n# Salary de\u011fi\u015fkenindeki bo\u015f de\u011ferlerin toplam\u0131 veri setinin %18'ine denk geliyor","22ba2f14":"def hitter_first_preprocessing(dataframe, model):\n    new_df = dataframe.copy()\n    new_df.columns = [col.upper() for col in new_df.columns]\n    \n    # KATEGOR\u0130K VE NUMER\u0130K DE\u011e\u0130\u015eKENLER\u0130N SE\u00c7\u0130LMES\u0130\n    cat_cols, num_cols, cat_but_car = grab_col_names(new_df)\n    \n    new_df.dropna(inplace=True)\n\n    new_df = pd.get_dummies(new_df, columns=cat_cols, drop_first=True)\n    \n    y = new_df['SALARY']\n    X = new_df.drop(\"SALARY\", axis=1)\n\n    model_tuned = model(random_state=46).fit(X, y)\n\n    # Salary de\u011fi\u015fkenini en \u00e7ok a\u00e7\u0131klayan de\u011fi\u015fkeni ara\u015ft\u0131ran fonksiyon.\n    # Bu de\u011fere g\u00f6re feature \u00fcretece\u011fiz\n    def plot_importance(model, features, num=len(X), save=False):\n        feature_imp = pd.DataFrame({'Value': model.feature_importances_, 'Feature': features.columns})\n        plt.figure(figsize=(10, 10))\n        sns.set(font_scale=1)\n        sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\",\n                                                                         ascending=False)[0:num])\n        plt.title('Features')\n        plt.tight_layout()\n        plt.show()\n\n\n    plot_importance(model_tuned, X)\n\n    # Salary ile en \u00e7ok korelasyona sahip olanlar\u0131 g\u00f6steriyoruz\n    high_corr = new_df.corr()\n    print(high_corr['SALARY'].sort_values(ascending=False)[1:20])","4e23a409":"hitter_first_preprocessing(df, LGBMRegressor)\n# Feature importance de\u011ferlerini inceliyoruz. Salary de\u011fi\u015fkenini en \u00e7ok a\u00e7\u0131klayan de\u011ferleri bulmaya \u00e7al\u0131\u015f\u0131yoruz\n# Buna g\u00f6re feature \u00fcretip veya \u00f6nemsizleri modelden kald\u0131raca\u011f\u0131z","60285300":"def hitter_data_preprocessing():\n    df = pd.read_csv(\"..\/input\/hitters\/Hitters.csv\")\n    \n    ############################################\n    # VER\u0130 \u00d6N\u0130\u015eLEME\n    ############################################\n    df.columns = [col.upper() for col in df.columns]\n    \n    df['NEW_HITRATIO'] = df['HITS'] \/ df['ATBAT']\n    df['NEW_RUNRATIO'] = df['HMRUN'] \/ df['RUNS']\n    df['NEW_CHITRATIO'] = df['CHITS'] \/ df['CATBAT']\n    df['NEW_CRUNRATIO'] = df['CHMRUN'] \/ df['CRUNS']\n\n    df['NEW_AVG_ATBAT'] = df['CATBAT'] \/ df['YEARS']\n    df['NEW_AVG_HITS'] = df['CHITS'] \/ df['YEARS']\n    df['NEW_AVG_HMRUN'] = df['CHMRUN'] \/ df['YEARS']\n    df['NEW_AVG_RUNS'] = df['CRUNS'] \/ df['YEARS']\n    df['NEW_AVG_RBI'] = df['CRBI'] \/ df['YEARS']\n    df['NEW_AVG_WALKS'] = df['CWALKS'] \/ df['YEARS']\n\n    cat_cols, num_cols, cat_but_car = grab_col_names(df)\n    \n    df = df.drop(\n    ['ATBAT', 'HITS', 'HMRUN',\n     'RUNS', 'RBI', 'WALKS', 'ASSISTS', 'ERRORS', \"PUTOUTS\", 'LEAGUE', 'NEWLEAGUE',\n     'DIVISION'], axis=1)\n    \n    ############################################\n    # MODELLEME\n    ############################################\n    df_eksik = df[df[\"SALARY\"].isnull()]\n    df_eksik= df_eksik.drop([\"SALARY\"], axis=1)\n\n    df.dropna(inplace=True)\n\n    y = df['SALARY']\n    X = df.drop(\"SALARY\", axis=1)\n    \n    return df, df_eksik, X, y\n    ","f3b97494":"# df, df_eksik, X, y = hitter_data_preprocessing()","0f5eece5":"# En iyi 5 skoru d\u00f6nd\u00fcren fonk\ndef best_mse_scores(X, y, random_state = 1, best_model_size = 5):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n    models = [('LR', LinearRegression().fit(X_train, y_train)),\n              (\"Ridge\", Ridge(random_state=random_state).fit(X_train, y_train)),\n              (\"Lasso\", Lasso(random_state=random_state).fit(X_train, y_train)),\n              (\"ElasticNet\", ElasticNet(random_state=random_state).fit(X_train, y_train)),\n              ('KNN', KNeighborsRegressor().fit(X_train, y_train)),\n              ('CART', DecisionTreeRegressor(random_state=random_state).fit(X_train, y_train)),\n              ('RF', RandomForestRegressor(random_state=random_state).fit(X_train, y_train)),\n              ('SVR', SVR().fit(X_train, y_train)),\n              ('GBM', GradientBoostingRegressor(random_state=random_state).fit(X_train, y_train)),\n              (\"XGBoost\", XGBRegressor(objective='reg:squarederror', random_state=random_state).fit(X_train, y_train)),\n              (\"LightGBM\", LGBMRegressor(random_state=random_state).fit(X_train, y_train))\n              ]\n    \n    all_models = []\n    for name, regressor in models:\n        rmse = np.mean(np.sqrt(-cross_val_score(regressor, X_test, y_test, cv=40, scoring=\"neg_mean_squared_error\")))\n        values = dict(name=name,RMSE = rmse)\n        all_models.append(values)\n        \n    all_models_df = pd.DataFrame(all_models)\n    all_models_df = all_models_df.sort_values(all_models_df.columns[1])\n    best_models_df = all_models_df[0:best_model_size]\n    return best_models_df\n","59b2a0a1":"# best_models_df, X_train, X_test, y_train, y_test = best_mse_scores(X, y, random_state= 1, best_model_size = 2)\n# best_models_df\n","81ed0090":"def hyperparameter_str_to_int_converting(converting_list= []):\n    converted_list = []\n    for i in converting_list:\n        if len(str(i)) != 0:\n            if str(i).isdigit():\n                converted_list.append(int(i))\n            elif i is None:\n                converted_list.append(i)\n            else:\n                try:\n                    float(i)\n                    converted_list.append(float(i))\n                except:\n                    # string de\u011ferdir 'auto' gibi onu direk ekle\n                    if str(i) == 'None':\n                        converted_list.append(9)\n                    else:\n                        converted_list.append(i)\n                \n    return converted_list","d24adea8":"def set_hyperparameters(best_models_df, is_set_custom_hyperparameter = False):\n    hyperparameter_list = []\n    \n    gbm_params = {\"learning_rate\": [0.1],\n              \"max_depth\": [3],\n              \"n_estimators\": [100],\n              \"subsample\": [1]}\n\n    rf_params = {\"max_depth\": [None],\n                 \"max_features\": [\"auto\"],\n                 \"min_samples_split\": [2],\n                 \"n_estimators\": [100]}\n\n\n    xgboost_params = {\"learning_rate\": [None],\n                      \"max_depth\": [None],\n                      \"n_estimators\": [100],\n                      \"colsample_bytree\": [None]}\n\n    lightgbm_params = {\"learning_rate\": [0.1],\n                       \"n_estimators\": [100],\n                       \"colsample_bytree\": [1.0]}\n\n    elastic_params = {\"alpha\": [1]}\n\n    lasso_params = {\"alpha\": [1]}\n\n    knn_params = {'n_neighbors': [5]}\n\n    cart_params = {\"max_depth\": [None],\n                  \"min_samples_split\": [2]}\n\n    svr_params = {\"kernel\": \"rbf\",\n                 \"max_iter\": -1}\n\n\n    if is_set_custom_hyperparameter:\n        for col in best_models_df['name']:\n            print(f\"{col} modeli i\u00e7in hiperparametreleri giriniz, birden \u00e7ok girmek i\u00e7in virg\u00fclle di\u011fer say\u0131y\u0131 yaz\u0131n\u0131z\")\n            if (col == \"GBM\"):\n                new_params = {}\n                for model, value in gbm_params.items():\n                    print(\"Parametre Ad\u0131: {}, \u00d6n tan\u0131ml\u0131 de\u011feri: {}\".format(model, value))\n                    values = input()\n                    splitted_value = values.split(',')\n                    converted_list = hyperparameter_str_to_int_converting(splitted_value)\n                    new_params[model] = converted_list\n                gbm_param = new_params\n                hyperparameter_list.append(dict(gbm_paramms= gbm_param))\n\n            elif col == \"RF\":\n                new_params = {}\n                for model, value in rf_params.items():\n                    print(\"Parametre Ad\u0131: {}, \u00d6n tan\u0131ml\u0131 de\u011feri: {}\".format(model, value))\n                    values = input()\n                    splitted_value = values.split(',')\n                    converted_list = hyperparameter_str_to_int_converting(splitted_value)\n                    new_params[model] = converted_list\n                rf_param = new_params\n                hyperparameter_list.append(dict(rf_params= rf_param))\n            elif col == \"XGBoost\":\n                new_params = {}\n                for model, value in xgboost_params.items():\n                    print(\"Parametre Ad\u0131: {}, \u00d6n tan\u0131ml\u0131 de\u011feri: {}\".format(model, value))\n                    values = input()\n                    splitted_value = values.split(',')\n                    converted_list = hyperparameter_str_to_int_converting(splitted_value)\n                    new_params[model] = converted_list\n                xgboost_param = new_params\n                hyperparameter_list.append(dict(xgboost_params = xgboost_param))\n\n            elif col == \"LightGBM\":\n                new_params = {}\n                for model, value in lightgbm_params.items():\n                    print(\"Parametre Ad\u0131: {}, \u00d6n tan\u0131ml\u0131 de\u011feri: {}\".format(model, value))\n                    values = input()\n                    splitted_value = values.split(',')\n                    converted_list = hyperparameter_str_to_int_converting(splitted_value)\n                    new_params[model] = converted_list\n                lightgbm_param = new_params\n                hyperparameter_list.append(dict(lightgbm_params = lightgbm_param))\n\n            elif col == \"ElasticNet\":\n                new_params = {}\n                for model, value in elastic_params.items():\n                    print(\"Parametre Ad\u0131: {}, \u00d6n tan\u0131ml\u0131 de\u011feri: {}\".format(model, value))\n                    values = input()\n                    splitted_value = values.split(',')\n                    converted_list = hyperparameter_str_to_int_converting(splitted_value)\n                    new_params[model] = converted_list\n                elastic_param = new_params\n                hyperparameter_list.append(dict(elastic_params = elastic_param))\n\n            elif col == \"Lasso\":\n                new_params = {}\n                for model, value in lasso_params.items():\n                    print(\"Parametre Ad\u0131: {}, \u00d6n tan\u0131ml\u0131 de\u011feri: {}\".format(model, value))\n                    values = input()\n                    splitted_value = values.split(',')\n                    converted_list = hyperparameter_str_to_int_converting(splitted_value)\n                    new_params[model] = converted_list\n                lasso_param = new_params\n                hyperparameter_list.append(dict(lasso_params = lasso_param))\n\n            elif col == \"KNN\":\n                new_params = {}\n                for model, value in knn_params.items():\n                    print(\"Parametre Ad\u0131: {}, \u00d6n tan\u0131ml\u0131 de\u011feri: {}\".format(model, value))\n                    values = input()\n                    splitted_value = values.split(',')\n                    converted_list = hyperparameter_str_to_int_converting(splitted_value)\n                    new_params[model] = converted_list\n                knn_param = new_params\n                hyperparameter_list.append(dict(knn_params = knn_param))\n\n            elif col == \"CART\":\n                new_params = {}\n                for model, value in cart_params.items():\n                    print(\"Parametre Ad\u0131: {}, \u00d6n tan\u0131ml\u0131 de\u011feri: {}\".format(model, value))\n                    values = input()\n                    splitted_value = values.split(',')\n                    converted_list = hyperparameter_str_to_int_converting(splitted_value)\n                    new_params[model] = converted_list\n                cart_param = new_params\n                hyperparameter_list.append(dict(cart_params = cart_param))\n\n            elif col == \"SVR\":\n                new_params = {}\n                for model, value in svr_params.items():\n                    print(\"Parametre Ad\u0131: {}, \u00d6n tan\u0131ml\u0131 de\u011feri: {}\".format(model, value))\n                    values = input()\n                    splitted_value = values.split(',')\n                    converted_list = hyperparameter_str_to_int_converting(splitted_value)\n                    new_params[model] = converted_list\n                svr_param = new_params\n                hyperparameter_list.append(dict(svr_params = svr_param))\n    else:\n        for col in best_models_df['name']:\n            if (col == \"GBM\"):\n                hyperparameter_list.append(dict(gbm_paramms= gbm_params))\n\n            elif col == \"RF\":\n                hyperparameter_list.append(dict(rf_params= rf_params))\n                \n            elif col == \"XGBoost\":\n                hyperparameter_list.append(dict(xgboost_params = xgboost_params))\n\n            elif col == \"LightGBM\":\n                hyperparameter_list.append(dict(lightgbm_params = lightgbm_params))\n\n            elif col == \"ElasticNet\":\n                hyperparameter_list.append(dict(elastic_params = elastic_params))\n\n            elif col == \"Lasso\":\n                hyperparameter_list.append(dict(lasso_params = lasso_params))\n\n            elif col == \"KNN\":\n                hyperparameter_list.append(dict(knn_params = knn_params))\n\n            elif col == \"CART\":\n                hyperparameter_list.append(dict(cart_params = cart_params))\n\n            elif col == \"SVR\":\n                hyperparameter_list.append(dict(svr_params = svr_params))\n                \n    return hyperparameter_list\n    \n        ","eb1c3919":"# hyperparameter_list = set_hyperparameters(best_models_df)\n# hyperparameter_list","9ceb655a":"def final_models_with_hyperparameters(hyperparameter_list, random_state = 1, X_train=[], X_test=[], y_train=[], y_test=[]):\n    regressors = []\n    \n    for col in hyperparameter_list:\n        for model_params, params_value in col.items():\n            if model_params == \"gbm_paramms\":\n                 regressors.append(('GBM', GradientBoostingRegressor(random_state = random_state).fit(X_train, y_train), params_value))\n                    \n            elif model_params == \"rf_params\":\n                 regressors.append((\"RF\", RandomForestRegressor(random_state = random_state).fit(X_train, y_train), params_value))\n                    \n            elif model_params == \"xgboost_params\":\n                 regressors.append(('XGBoost', XGBRegressor(random_state = random_state,objective='reg:squarederror').fit(X_train, y_train), params_value))\n                    \n            elif model_params == \"lightgbm_params\":\n                 regressors.append(('LightGBM', LGBMRegressor(random_state = random_state).fit(X_train, y_train), params_value))\n                    \n            elif model_params == \"elastic_params\":\n                 regressors.append((\"ElasticNet\", ElasticNet(random_state = random_state).fit(X_train,y_train), params_value))\n                    \n            elif model_params == \"lasso_params\":\n                regressors.append((\"Lasso\", Lasso(random_state = random_state).fit(X_train,y_train), params_value))\n            \n            elif model_params == \"knn_params\":\n                regressors.append(('KNN', KNeighborsRegressor().fit(X_train,y_train), params_value))\n            \n            elif model_params == \"cart_params\":\n                regressors.append(('CART', DecisionTreeRegressor(random_state = random_state).fit(X_train,y_train), params_value))\n            \n            elif model_params == \"svr_params\":\n                regressors.append(('SVR', SVR().fit(X_train,y_train), params_value))\n    \n    best_models = {}\n\n    for name, regressor, params in regressors:\n        print(f\"########## {name} ##########\")\n        rmse = np.mean(np.sqrt(-cross_val_score(regressor, X_test, y_test, cv=20, scoring=\"neg_mean_squared_error\")))\n        print(f\"RMSE (Before): {round(rmse, 4)} ({name}) \")\n\n        gs_best = GridSearchCV(regressor, params, cv=5, n_jobs=-1, verbose=False).fit(X_train, y_train)\n\n        final_model = regressor.set_params(**gs_best.best_params_)\n\n        rmse = np.mean(np.sqrt(-cross_val_score(final_model, X_test, y_test, cv=40, scoring=\"neg_mean_squared_error\")))\n        print(f\"RMSE (After): {round(rmse, 4)} ({name}) \")\n\n        print(f\"{name} best params: {gs_best.best_params_}\", end=\"\\n\\n\")\n        best_models[name] = final_model\n    \n    return best_models","4d7a8577":"# final_best_models = final_models_with_hyperparameters(hyperparameter_list)","01c56cdf":"# final_best_models","142442bc":"# dataframe_na_values = Salary de 59 tane bo\u015f de\u011fer vard\u0131\n# bunlar\u0131 tahmin de\u011feriyle doldurmak icin bu dataframe_na_values de\u011feri fonksiyona g\u00f6nderirsek\n# bo\u015f de\u011ferleri doldurur ve hata de\u011feriyle geri d\u00f6nd\u00fcr\u00fcr\n\n# dataframe_na_values bu de\u011fer girilmezse sadece tahmin modelini geriye d\u00f6nd\u00fcr\u00fcr\n\ndef voting_model_with_hyperparameter(best_models, dataframe_na_values = [], na_target_name = '', original_dataframe = [], X_train=[], X_test=[], y_train=[], y_test=[]):\n    ######################################################\n    # # Stacking & Ensemble Learning\n    ######################################################\n    estimators = []\n    \n    for key, value in best_models.items():\n        estimators.append((key, value))\n\n    voting_reg = VotingRegressor(estimators)\n\n    voting_reg.fit(X_train, y_train)\n\n    rmse = np.mean(np.sqrt(-cross_val_score(voting_reg, X_test, y_test, cv=40,\n                                     scoring=\"neg_mean_squared_error\")))\n    print(\"RMSE : \", rmse)\n    \n    if len(dataframe_na_values) > 0:\n        dataframe_na_values[na_target_name] = voting_reg.predict(dataframe_na_values)\n        final_df = original_dataframe.merge(dataframe_na_values, how='outer')\n        return final_df, rmse\n    \n    return voting_reg","51877865":"# eksik salary de\u011ferlerini dolduruyoruz\n# final_df, rmse = voting_model_with_hyperparameter(final_best_models, dataframe_na_values = df_eksik,na_target_name = \"SALARY\", original_dataframe=df )\n# final_df.head()","6d4f99e8":"# hitter_first_preprocessing(final_df, GradientBoostingRegressor)","9a201784":"# de\u011fi\u015fkenler aras\u0131 korelasyonu inceliyoruz\n# birbiriyle korelasyonu y\u00fcksek olanlar asl\u0131nda ayn\u0131 \u015feyi ifade etti\u011fi i\u00e7in\n# y\u00fcksek olanlar\u0131 drop etmek modelin performans\u0131n\u0131 artt\u0131racakt\u0131r.\n\n\n# high_corr=final_df.corr()\n# for i in high_corr.columns:\n#     print(high_corr[i].sort_values(ascending=False)[1:10])","dc957bbf":"# Korelasyonlara bakt\u0131k, feature importance'lara bakt\u0131k. Buna g\u00f6re baz\u0131 de\u011fi\u015fkenleri modelimizden \u00e7\u0131kartt\u0131k\n# final_df = final_df.drop(['CATBAT','CWALKS','CHMRUN','NEW_HITRATIO','NEW_RUNRATIO','NEW_AVG_ATBAT',\n#            'NEW_AVG_HMRUN','NEW_AVG_WALKS'], axis=1)\n","d84e9acc":"# y = final_df['SALARY']\n# X = final_df.drop(\"SALARY\", axis=1)\n\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)","d51acbef":"# best_models_df, X_train, X_test, y_train, y_test = best_mse_scores(X, y, random_state= 1, best_model_size = 5)\n# best_models_df","909aae84":"# hyperparameter_list = set_hyperparameters(best_models_df)\n# hyperparameter_list","85bccccb":"# final_best_models = final_models_with_hyperparameters(hyperparameter_list)\n# final_best_models","c4fc6aeb":"# voting_reg = voting_model_with_hyperparameter(final_best_models )\n# voting_reg","09a9b280":"\n######################################################\n# Prediction for a New Observation\n######################################################\n\n# random_user = X.sample(1,random_state=5)\n# voting_reg.predict(random_user)\n","75bacda3":"# final_df[final_df.index== random_user.index[0]]['SALARY']","2515e4aa":"def hitter_custom_pipeline(random_state=1, best_model_size= 2, is_set_custom_hyperparameter = False, save_model=False):\n    print('1')\n    df, df_eksik, X, y = hitter_data_preprocessing()\n    \n    print('2')\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n    \n    best_models_df = best_mse_scores(X, y, random_state= random_state, best_model_size = best_model_size)\n    print('3')\n    hyperparameter_list = set_hyperparameters(best_models_df, is_set_custom_hyperparameter)\n    print('4')\n    final_best_models = final_models_with_hyperparameters(hyperparameter_list, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)\n    print('5')\n    # eksik salary de\u011ferlerini dolduruyoruz\n    final_df, rmse = voting_model_with_hyperparameter(final_best_models, dataframe_na_values = df_eksik,na_target_name = \"SALARY\", original_dataframe=df, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)\n    print('6')\n    # Korelasyonlara bakt\u0131k, feature importance'lara bakt\u0131k. Buna g\u00f6re baz\u0131 de\u011fi\u015fkenleri modelimizden \u00e7\u0131kartt\u0131k\n    final_df = final_df.drop(['CATBAT','CWALKS','CHMRUN','NEW_HITRATIO','NEW_RUNRATIO','NEW_AVG_ATBAT',\n           'NEW_AVG_HMRUN','NEW_AVG_WALKS'], axis=1)\n    y = final_df['SALARY']\n    X = final_df.drop(\"SALARY\", axis=1)\n    print('7')\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n    print('8')\n    best_models_df = best_mse_scores(X, y, random_state= 1, best_model_size = best_model_size)\n    print('9')\n    hyperparameter_list = set_hyperparameters(best_models_df, is_set_custom_hyperparameter)\n    print('10')\n    final_best_models = final_models_with_hyperparameters(hyperparameter_list, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)\n    print('11')\n    voting_reg = voting_model_with_hyperparameter(final_best_models, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)\n    \n    if save_model:\n        os.chdir(\".\/\")\n        joblib.dump(voting_reg, \"voting_reg_hitters.pkl\")\n        print(\"voting_reg_hitters has been created\")\n\n    return voting_reg, X, y, final_df","d5cd618e":"# is_set_custom_hyperparameter = False hiperparametre girmek i\u00e7in bunu True yapmak laz\u0131m\n# Di\u011fer t\u00fcrl\u00fc \u00f6n tan\u0131ml\u0131 hiperparametreler ile \u00e7al\u0131\u015facak pipeline\n\n\nvoting_reg, X, y, final_df = hitter_custom_pipeline(best_model_size= 2, is_set_custom_hyperparameter = False, save_model=True)\nvoting_reg\n","64660cba":"######################################################\n# Prediction for a New Observation\n######################################################\n## pickle olarak modelimizi \nvoting_reg = joblib.load(\".\/voting_reg_hitters.pkl\")\n\nrandom_user = X.sample(1,random_state=2)\nvoting_reg.predict(random_user)","46c861bc":"final_df[final_df.index== random_user.index[0]]['SALARY']","c371b2e7":"# Hiperparemetre girelim\n\n\n# voting_reg, X, y, final_df = hitter_custom_pipeline(best_model_size= 2, is_set_custom_hyperparameter = True)\n# voting_reg","6c256c05":"# voting_reg, X, y, final_df = hitter_custom_pipeline(best_model_size= 2, is_set_custom_hyperparameter = True)\n# voting_reg\n# Bunu \u00e7al\u0131\u015ft\u0131rd\u0131\u011f\u0131m\u0131zda a\u015fa\u011f\u0131daki tahmin de\u011ferlerine ula\u015f\u0131yoruz\n\n\n# 1\n# Observations: 322\n# Variables: 30\n# cat_cols: 3\n# num_cols: 27\n# cat_but_car: 0\n# num_but_cat: 0\n# 2\n# 3\n# GBM modeli i\u00e7in hiperparametreleri giriniz, birden \u00e7ok girmek i\u00e7in virg\u00fclle di\u011fer say\u0131y\u0131 yaz\u0131n\u0131z\n# Parametre Ad\u0131: learning_rate, \u00d6n tan\u0131ml\u0131 de\u011feri: [0.1]\n#  0.1,0.01\n# Parametre Ad\u0131: max_depth, \u00d6n tan\u0131ml\u0131 de\u011feri: [3]\n#  3,5,7\n# Parametre Ad\u0131: n_estimators, \u00d6n tan\u0131ml\u0131 de\u011feri: [100]\n#  100,300,500,600\n# Parametre Ad\u0131: subsample, \u00d6n tan\u0131ml\u0131 de\u011feri: [1]\n#  1,0.5,0.6,0.7\n# RF modeli i\u00e7in hiperparametreleri giriniz, birden \u00e7ok girmek i\u00e7in virg\u00fclle di\u011fer say\u0131y\u0131 yaz\u0131n\u0131z\n# Parametre Ad\u0131: max_depth, \u00d6n tan\u0131ml\u0131 de\u011feri: [None]\n#  5,8,15,None\n# Parametre Ad\u0131: max_features, \u00d6n tan\u0131ml\u0131 de\u011feri: ['auto']\n#  2,5,7,9,auto\n# Parametre Ad\u0131: min_samples_split, \u00d6n tan\u0131ml\u0131 de\u011feri: [2]\n#  2,4,5,7\n# Parametre Ad\u0131: n_estimators, \u00d6n tan\u0131ml\u0131 de\u011feri: [100]\n#  100,200,300,400,500\n# 4\n# ########## GBM ##########\n# RMSE (Before): 199.1793 (GBM) \n# RMSE (After): 177.0177 (GBM) \n# GBM best params: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 600, 'subsample': 0.5}\n\n# ########## RF ##########\n# RMSE (Before): 201.3148 (RF) \n# RMSE (After): 167.831 (RF) \n# RF best params: {'max_depth': 15, 'max_features': 5, 'min_samples_split': 2, 'n_estimators': 100}\n\n# 5\n# RMSE :  170.76784502310116\n# 6\n# 7\n# 8\n# 9\n# KNN modeli i\u00e7in hiperparametreleri giriniz, birden \u00e7ok girmek i\u00e7in virg\u00fclle di\u011fer say\u0131y\u0131 yaz\u0131n\u0131z\n# Parametre Ad\u0131: n_neighbors, \u00d6n tan\u0131ml\u0131 de\u011feri: [5]\n#  1,2,3,4,5,6,7,8,9,10,11,12,13,14,15\n# RF modeli i\u00e7in hiperparametreleri giriniz, birden \u00e7ok girmek i\u00e7in virg\u00fclle di\u011fer say\u0131y\u0131 yaz\u0131n\u0131z\n# Parametre Ad\u0131: max_depth, \u00d6n tan\u0131ml\u0131 de\u011feri: [None]\n#  5,8,15,None\n# Parametre Ad\u0131: max_features, \u00d6n tan\u0131ml\u0131 de\u011feri: ['auto']\n#  2,5,7,9,auto\n# Parametre Ad\u0131: min_samples_split, \u00d6n tan\u0131ml\u0131 de\u011feri: [2]\n#  2,4,5,7\n# Parametre Ad\u0131: n_estimators, \u00d6n tan\u0131ml\u0131 de\u011feri: [100]\n#  100,200,300,400,500\n# 10\n# ########## KNN ##########\n# RMSE (Before): 212.3876 (KNN) \n# RMSE (After): 178.4536 (KNN) \n# KNN best params: {'n_neighbors': 15}\n\n# ########## RF ##########\n# RMSE (Before): 215.2845 (RF) \n# RMSE (After): 184.2157 (RF) \n# RF best params: {'max_depth': 9, 'max_features': 2, 'min_samples_split': 2, 'n_estimators': 100}\n\n# 11\n# RMSE :  176.65639968646988\n# VotingRegressor(estimators=[('KNN', KNeighborsRegressor(n_neighbors=15)),\n#                             ('RF',\n#                              RandomForestRegressor(max_depth=9, max_features=2,\n#                                                    random_state=1))])","3addb808":"### Pickle olarak modelimizi kaydettik","977dfbb1":"##### Tahmin edilen de\u011fer 247.992. Ger\u00e7ek de\u011fer ise 228.414\n##### RMSE De\u011ferimiz 171.842 olarak bulduk. Bu kadar y\u00fcksek olmas\u0131n\u0131n sebebi ayk\u0131r\u0131 Salary de\u011ferlerini bask\u0131lama i\u015flemine sokmad\u0131k ki \n##### modelin yap\u0131s\u0131n\u0131 bozmayal\u0131m\n##### Netice itibariyle tahminler ile ger\u00e7ek de\u011ferler aras\u0131nda ortalama 171 birimlik bir sapmaya kadar modelimiz tahminlemesini do\u011fru yapmaktad\u0131r.","7be31e2c":"# CUSTOM PIPELINE","25c49ee8":"###################################################\n### PROJECT: SALARY PREDICT\u0130ON WITH MACHINE LEARNING\n###################################################\n\n### \u0130\u015f Problemi\n\n Maa\u015f bilgileri ve 1986 y\u0131l\u0131na ait kariyer istatistikleri payla\u015f\u0131lan beyzbol\n oyuncular\u0131n\u0131n maa\u015f tahminleri i\u00e7in bir makine \u00f6\u011frenmesi projesi ger\u00e7ekle\u015ftirilebilir mi?\n\n ### Veri seti hikayesi\n\n Bu veri seti orijinal olarak Carnegie Mellon \u00dcniversitesi'nde bulunan StatLib k\u00fct\u00fcphanesinden al\u0131nm\u0131\u015ft\u0131r.\n Veri seti 1988 ASA Grafik B\u00f6l\u00fcm\u00fc Poster Oturumu'nda kullan\u0131lan verilerin bir par\u00e7as\u0131d\u0131r.\n Maa\u015f verileri orijinal olarak Sports Illustrated, 20 Nisan 1987'den al\u0131nm\u0131\u015ft\u0131r.\n 1986 ve kariyer istatistikleri, Collier Books, Macmillan Publishing Company, New York taraf\u0131ndan yay\u0131nlanan\n 1987 Beyzbol Ansiklopedisi G\u00fcncellemesinden elde edilmi\u015ftir.\n\n\n* AtBat: 1986-1987 sezonunda bir beyzbol sopas\u0131 ile topa yap\u0131lan vuru\u015f say\u0131s\u0131\n* Hits: 1986-1987 sezonundaki isabet say\u0131s\u0131\n* HmRun: 1986-1987 sezonundaki en de\u011ferli vuru\u015f say\u0131s\u0131\n* Runs: 1986-1987 sezonunda tak\u0131m\u0131na kazand\u0131rd\u0131\u011f\u0131 say\u0131\n* RBI: Bir vurucunun vuru\u015f yapt\u0131g\u0131nda ko\u015fu yapt\u0131rd\u0131\u011f\u0131 oyuncu say\u0131s\u0131\n* Walks: Kar\u015f\u0131 oyuncuya yapt\u0131r\u0131lan hata say\u0131s\u0131\n* Years: Oyuncunun major liginde oynama s\u00fcresi (sene)\n* CAtBat: Oyuncunun kariyeri boyunca topa vurma say\u0131s\u0131\n* CHits: Oyuncunun kariyeri boyunca yapt\u0131\u011f\u0131 isabetli vuru\u015f say\u0131s\u0131\n* CHmRun: Oyucunun kariyeri boyunca yapt\u0131\u011f\u0131 en de\u011ferli say\u0131s\u0131\n* CRuns: Oyuncunun kariyeri boyunca tak\u0131m\u0131na kazand\u0131rd\u0131\u011f\u0131 say\u0131\n* CRBI: Oyuncunun kariyeri boyunca ko\u015fu yapt\u0131rd\u0131rd\u0131\u011f\u0131 oyuncu say\u0131s\u0131\n* CWalks: Oyuncun kariyeri boyunca kar\u015f\u0131 oyuncuya yapt\u0131rd\u0131\u011f\u0131 hata say\u0131s\u0131\n* League: Oyuncunun sezon sonuna kadar oynad\u0131\u011f\u0131 ligi g\u00f6steren A ve N seviyelerine sahip bir fakt\u00f6r\n* Division: 1986 sonunda oyuncunun oynad\u0131\u011f\u0131 pozisyonu g\u00f6steren E ve W seviyelerine sahip bir fakt\u00f6r\n* PutOuts: Oyun icinde tak\u0131m arkada\u015f\u0131nla yard\u0131mla\u015fma\n* Assits: 1986-1987 sezonunda oyuncunun yapt\u0131\u011f\u0131 asist say\u0131s\u0131\n* Errors: 1986-1987 sezonundaki oyuncunun hata say\u0131s\u0131\n* Salary: Oyuncunun 1986-1987 sezonunda ald\u0131\u011f\u0131 maa\u015f(bin uzerinden)\n* NewLeague: 1987 sezonunun ba\u015f\u0131nda oyuncunun ligini g\u00f6steren A ve N seviyelerine sahip bir fakt\u00f6r"}}