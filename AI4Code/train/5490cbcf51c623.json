{"cell_type":{"94139caf":"code","1dfecc97":"code","da7daee3":"code","cdcbfc7f":"code","a85e5949":"code","2611f6d9":"code","3cd8a70a":"code","451ebf3c":"code","953bfb42":"code","dc074c23":"code","37d066a8":"code","225630e2":"code","5c82febd":"code","64c76169":"code","c56f9837":"code","c09f693e":"code","354c4807":"code","1dc138b9":"code","c79c126f":"code","f1964946":"code","1adf94ab":"code","b795ff65":"code","d10dfe1c":"code","187f62b1":"code","a60ee424":"code","28cbd60f":"code","5db3de46":"code","642c29db":"code","d01c80f0":"code","8ea144be":"code","f7cc8d2b":"code","38813ec0":"code","8f236e8d":"code","24ab9f0f":"code","4a1a0819":"code","d80d8799":"code","643612b5":"code","a9c02215":"code","d556bc79":"code","8c6d3ecf":"code","0d6a4877":"code","ea0ba34d":"code","1cb73bc7":"code","e1563f11":"code","2a50f896":"code","3230c161":"code","bf8bbfe5":"code","ec9a1087":"code","b5e70c4f":"code","42c59b80":"code","edf45cd0":"code","ec27a2ae":"code","f494b270":"code","8a80558c":"code","4f50a88f":"code","51ef5a55":"code","739a095a":"code","f1230a70":"code","e21d0ccd":"code","5eec6b85":"code","a536590c":"code","09c17d0c":"code","17f4c584":"code","b1902cd5":"code","61eb4456":"code","34bccff9":"code","e751dea4":"code","32b1f39c":"code","134e24cc":"code","9c21a169":"code","074bfa80":"code","f1137c08":"code","828223e7":"code","1fd35046":"code","8e12d92b":"code","099cd87f":"code","73e88665":"code","a24725e1":"code","072f3bee":"code","bf7a252b":"code","91b90cd8":"code","a9deeacb":"code","3ca46d46":"code","a622dce2":"code","b574ea6b":"markdown","9083f53d":"markdown","7cf2427e":"markdown","87d704ca":"markdown","894d81cd":"markdown","6f29f6b0":"markdown","2983af75":"markdown","51e45563":"markdown","0e586544":"markdown","f093391f":"markdown","9137bad5":"markdown","f418d382":"markdown","c35349fb":"markdown","a44ee702":"markdown","87c08e4a":"markdown","111c819d":"markdown","3bd00235":"markdown","ccb335bb":"markdown","075b921d":"markdown","44d172e5":"markdown","d35aacfc":"markdown","d1f61f80":"markdown","2f7c32f9":"markdown"},"source":{"94139caf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfile_path = []\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        file_path.append(os.path.join(dirname, filename))\nfile_path[:10]\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1dfecc97":"import gc\ngc.collect()\n","da7daee3":"# import libraries\n\nimport os\nimport pandas as pd\nimport numpy as np\nimport time\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n%matplotlib inline\nimport seaborn as sns\nimport collections, random, re\nfrom collections import Counter\nfrom PIL import Image\nimport glob\nfrom sklearn.model_selection import train_test_split \n\n#model building \nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nfrom nltk.translate.bleu_score import sentence_bleu\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.preprocessing import image, sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.image import load_img, img_to_array\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector,\\\n                         Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\nfrom keras.optimizers import Adam, RMSprop\nfrom keras.layers.wrappers import Bidirectional\nfrom keras.layers.merge import add\nfrom keras.applications.inception_v3 import InceptionV3, preprocess_input\nfrom keras.models import Model\nfrom keras import Input, layers\nfrom keras import optimizers\nfrom tqdm import tqdm\n","cdcbfc7f":"# Defining paths\nINPUT_PATH = \"..\/input\/flickr8k\/\"\nIMAGE_PATH = INPUT_PATH+'Images\/'\nCAPTIONS_FILE = INPUT_PATH+'captions.txt'\nOUTPUT_IMAGE_PATH = \"..\/working\/Image\/\"","a85e5949":"images=\"..\/input\/flickr8k\/Images\"\nall_imgs = glob.glob(images + '\/*.jpg',recursive=True)\nprint(\"The total images present in the dataset: {}\".format(len(all_imgs)))","2611f6d9":"#Visualise both the images present in the dataset\nplt.imshow(mpimg.imread(all_imgs[0]))","3cd8a70a":"def load_doc(filename):\n    \n    with open(filename,'r') as f_in:\n        text = list(filter(None, (line.rstrip() for line in f_in)))\n    f_in.close()\n\n    col=[]\n    for line in text:\n        col.append(line.split(\",\",maxsplit=1)) #Maxsplit 1 used to handle the captions with comma\n\n    return col\n\ndoc = load_doc(CAPTIONS_FILE)\ndoc[:30]","451ebf3c":"import glob\nimport random\nimport base64\nimport pandas as pd\n\nfrom PIL import Image\nfrom io import BytesIO\nfrom IPython.display import HTML\nimport io\n\npd.set_option('display.max_colwidth', -1)\n\n\ndef get_thumbnail(path):\n    #path = \"\\\\\\\\?\\\\\"+path # This \"\\\\\\\\?\\\\\" is used to prevent problems with long Windows paths\n    i = Image.open(path)    \n    return i\n\ndef image_base64(im):\n    if isinstance(im, str):\n        im = get_thumbnail(im)\n    with BytesIO() as buffer:\n        im.save(buffer, 'jpeg')\n        return base64.b64encode(buffer.getvalue()).decode()\n\ndef image_formatter(im):\n    return f'<img src=\"data:image\/jpeg;base64,{image_base64(im)}\" style=\"max-height:124px;\">'","953bfb42":"all_img_id = []\nall_img_vector = []\nannotations = []\nimage_visual = []\n# get image_id, image_caption and image_path\n\n\nall_img_id = [i[0] for i in doc[1:]] #store all the image id here\nall_img_vector= [IMAGE_PATH+i[0] for i in doc[1:]] #store all the image path here\nannotations= [i[1] for i in doc[1:]] #store all the captions here\n#image_visual = [get_thumbnail(IMAGE_PATH+i[0]) for i in doc[1:]] \n#\n\n\n# Commenting as this \n#doc = doc.splitlines()[1:]\n#for line in doc:\n    #com_id =line.index(\",\")\n    #img_id = line[:com_id]\n    #all_img_id.append(img_id)\n\n    #annotations.append(line[com_id +1:])\n    #all_img_vector.append(IMAGE_PATH + \"\/\" + img_id)\n    #image_visual.append(get_thumbnail(IMAGE_PATH + \"\/\" + img_id))\n    \n#    all_img_id.append(line[:line.index(\",\")])\n#    annotations.append(line[line.index(\",\") +1:])\n#    all_img_vector.append(IMAGE_PATH + \"\/\" + line[:line.index(\",\")])\n#    image_visual.append(get_thumbnail(IMAGE_PATH + \"\/\" + line[:line.index(\",\")]))\n    \ndf= pd.DataFrame(zip(all_img_id, all_img_vector, annotations), columns=['ID', 'Path', 'Captions'])\ndf.head()\n","dc074c23":"df_img = pd.DataFrame(zip([get_thumbnail(i) for i in df.Path.head().to_list()], df.Captions.head()), columns=['Images', 'Captions'])","37d066a8":"#Mapping of Image and Caption\nHTML(df_img.to_html(formatters={'Images': image_formatter}, escape=False))","225630e2":"df.info()","5c82febd":"#Create a list which contains all the captions\nannotations = []\n\n#add the <start> & <end> token to all those captions as well\nfor cap in df.Captions:\n    cap='<start> '+ cap + ' <end>'\n    annotations.append(cap)\n\n\n#Create a list which contains all the path to the images\nall_img_path=df.Path.to_list()\n\nprint(\"Total captions present in the dataset: \"+ str(len(annotations)))\nprint(\"Total images present in the dataset: \" + str(len(all_img_path)))","64c76169":"uni_filenames= np.unique(df.ID.values)\nprint(\"The number of unique file names : {}\".format(len(uni_filenames)))\nprint(\"The distribution of the number of captions for each image:\", Counter(Counter(df.ID.values).values()))","c56f9837":"# get the vocabulary from the captions\nvocabulary = []\nfor cap in df.Captions.values:\n        vocabulary.extend(cap.split())\n        \nval_count = Counter(vocabulary)\nprint(\"Size of Vocab : \",len(set(vocabulary)))","c09f693e":"val_count.most_common(30)","354c4807":"stopwords.words('english')","1dc138b9":"sorted_list = val_count.most_common(30)\n\ndef plot_word_count(wordCountList):\n    word_list,wc_list = [],[]\n    for word, count in wordCountList:\n        word_list.append(word)\n        wc_list.append(count)\n    plt.figure(figsize=(20,6))\n    sns.barplot(x = word_list, y = wc_list, orient='v').set_title('Top 30 occurinng words')\n        ","c79c126f":"plot_word_count(sorted_list)","f1964946":"# create the tokenizer\ntop_word_count = 5000\n\nfilter_chars = '!\"#$%&()*+.,-\/:;=?@[\\]^_`{|}~ '\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_word_count,\n                                                  oov_token=\"<unk>\",\n                                                  filters=filter_chars)\ntokenizer.fit_on_texts(annotations)\ntrain_seqs = tokenizer.texts_to_sequences(annotations)\nprint(train_seqs[:5])\nprint(annotations[:5])\n","1adf94ab":"train_seqs[:5]","b795ff65":"annotations[:5]","d10dfe1c":"# printing token\ntokenizer.oov_token","187f62b1":"# Create word-to-index and index-to-word mappings.\n\nword_index = tokenizer.word_index\nindex_word = tokenizer.index_word\n\ntokenizer.word_index['<pad>'] = 0\ntokenizer.index_word[0] = '<pad>'\n\n# Encode training data sentences into sequences\ntrain_seqs  = tokenizer.texts_to_sequences(annotations)","a60ee424":"print(train_seqs[1])\nprint(annotations[1])","28cbd60f":"# Create a word count of your tokenizer to visulize the Top 30 occuring words after text processing\n\nsort_word_by_count = sorted(tokenizer.word_counts.items(), key=lambda kv : kv[1], reverse= True)\nplot_word_count(sort_word_by_count[:30])","5db3de46":"# Pad each vector to the max_length of the captions ^ store it to a vairable\n\nmax_length = max([len(t) for t in train_seqs])\nprint(\"The maximum length of a sentence in the annotation dataset is: \" + str(max_length))\n\ncaption_vector = keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post', maxlen=max_length)\nprint(\"The shape of Caption vector is :\" + str(caption_vector.shape))\n\n","642c29db":"caption_vector","d01c80f0":"print(caption_vector[0])\nprint(annotations[0])","8ea144be":"# write your code here for creating the function. This function should return images & their path\n# We will use tensorflow to resize\ndef load_image(image_path):\n    image = tf.io.read_file(image_path)\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.resize(image, (299, 299))\n    image = tf.keras.applications.inception_v3.preprocess_input(image)\n    \n    return image, image_path","f7cc8d2b":"#Load function test \nprint(\"Path -\",(all_img_vector[20]))\nprint(\"Shape after resize :\", load_image(all_img_vector[20])[0].shape)\nplt.imshow(load_image(all_img_vector[20])[0])","38813ec0":"#write your code here for applying the function to the image path dataset, such that the transformed dataset should contain images & their path\n#Get unique images\nall_img_vector_uniq = sorted(set(all_img_vector))\n\n#Updating Batch Size to 64\nBATCH_SIZE = 64\nimage_dataset = tf.data.Dataset.from_tensor_slices(all_img_vector_uniq).map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(BATCH_SIZE)\n\nprint(\"Unique number of Images:\", len(all_img_vector_uniq))","8f236e8d":"image_dataset","24ab9f0f":"sample_img_batch, sample_cap_batch = next(iter(image_dataset))\nprint(sample_img_batch.shape) #(batch_size, 299, 299, 3)\nprint(sample_cap_batch.shape) #(batch_size, max_len)","4a1a0819":"#Inception V3 Model \nimage_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n\nnew_input = image_model.input\nhidden_layer = image_model.layers[-1].output\n\n# Getting the Pretrained Model Weights\nimage_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n","d80d8799":"image_features_extract_model.summary()","643612b5":"keras.utils.plot_model(image_model, \"image_model.png\", show_shapes=True)","a9c02215":"import gc\ngc.collect()\n","d556bc79":"# code to apply the feature_extraction model to your earlier created dataset which contained images & their respective paths\n# Once the features are created, you need to reshape them such that feature shape is in order of (batch_size, 8*8, 2048)\n\nimage_dict = {}\nfrom tqdm import tqdm\nfor img, path in tqdm(image_dataset):\n    batch_features = image_features_extract_model(img)\n    batch_features = tf.reshape(batch_features,(batch_features.shape[0], -1, batch_features.shape[3]))\n\n    for bf, p in zip(batch_features, path):\n        path_of_feature = p.numpy().decode(\"utf-8\")\n        #image_dict[path_of_feature] =  bf.numpy()\n        path_of_feature1 = os.path.basename(path_of_feature)\n        path_of_feature2 = '.\/'+path_of_feature1\n        np.save(path_of_feature2, bf.numpy())","8c6d3ecf":"import gc\ngc.collect()\n","0d6a4877":"len(image_dict)","ea0ba34d":"# Code for test train split\npath_train, path_test, cap_train, cap_test = train_test_split(all_img_vector,caption_vector,test_size=0.2,random_state=42)","1cb73bc7":"print(\"Training data for images: \" + str(len(path_train)))\nprint(\"Testing data for images: \" + str(len(path_test)))\nprint(\"Training data for Captions: \" + str(len(cap_train)))\nprint(\"Testing data for Captions: \" + str(len(cap_test)))","e1563f11":"# Lets see the image\nprint(cap_test[5])\nprint(\" \".join(tokenizer.index_word[i] for i in cap_test[5]))\nplt.imshow(load_image(path_test[5])[0])\n","2a50f896":"# Create a function which maps the image path to their feature. \n# This function will take the image_path & caption and return it's feature & respective caption.\n\ndef map_func(image, caption):\n    #Memory utilization\n    #img_tensor = image_dict[image.decode('utf-8')]\n    img_name_temp = os.path.basename(image.numpy().decode('utf-8'))\n    img_name_ip = '.\/'+img_name_temp\n    img_tensor = np.load(img_name_ip+'.npy')\n    return img_tensor, caption","3230c161":"# create a builder function to create dataset which takes in the image path & captions as input\n# This function should transform the created dataset(img_path,cap) to (features,cap) using the map_func created earlier\n\ndef data_generator(image,caption, batch_size=32, buffer_size=1000):\n    dataset = tf.data.Dataset.from_tensor_slices((image, caption))\n    \n    # Use map to load the numpy files in parallel\n    #dataset = dataset.map(lambda item1, item2: tf.numpy_function(map_func, [item1, item2], [tf.float32, tf.int32]),num_parallel_calls=tf.data.experimental.AUTOTUNE)\n    dataset = dataset.map(lambda item1, item2: tf.py_function(map_func, [item1, item2], [tf.float32, tf.int32]), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n    dataset = dataset.shuffle(buffer_size)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n\n    return dataset","bf8bbfe5":"train_dataset=data_generator(path_train,cap_train,64)\ntest_dataset=data_generator(path_test,cap_test,64)","ec9a1087":"sample_img_batch, sample_cap_batch = next(iter(train_dataset))\nprint(sample_img_batch.shape) #(batch_size, 299, 299, 3)\nprint(sample_cap_batch.shape) #(batch_size, max_len)","b5e70c4f":"BATCH_SIZE = 32\nembedding_dim = 256 \nunits = 512\nvocab_size = 5001 #top 5,000 words +1\ntrain_num_steps = len(path_train) \/\/ BATCH_SIZE\ntest_num_steps = len(path_test) \/\/ BATCH_SIZE\n# Shape of the vector extracted from InceptionV3 is (64, 2048)\n# These two variables represent that vector shape\nfeatures_shape = bf.shape[1]\nattention_features_shape = bf.shape[0]","42c59b80":"class Encoder(Model):\n    def __init__(self,embed_dim):\n        super(Encoder, self).__init__()\n        self.dense = tf.keras.layers.Dense(embed_dim)\n        self.dropout = tf.keras.layers.Dropout(0.5)\n        \n    def call(self, features):\n        # extract the features from the image shape: (batch, 8*8, embed_dim)\n        features =  self.dense(features) \n        features = tf.nn.relu(features)\n        \n        return features","edf45cd0":"encoder=Encoder(embedding_dim)","ec27a2ae":"class Attention_model(Model):\n    def __init__(self, units):\n        super(Attention_model, self).__init__()\n        self.W1 = tf.keras.layers.Dense(units)\n        self.W2 = tf.keras.layers.Dense(units) #build your Dense layer\n        self.V = tf.keras.layers.Dense(1) #build your final Dense layer with unit 1\n        self.units=units\n\n    def call(self, features, hidden):\n        #features shape: (batch_size, 8*8, embedding_dim)\n        # hidden shape: (batch_size, hidden_size)\n        hidden_with_time_axis = tf.expand_dims(hidden, 1) # Expand the hidden shape to shape: (batch_size, 1, hidden_size)\n        score = tf.keras.activations.tanh(self.W1(features) + self.W2(hidden_with_time_axis)) # build your score funciton to shape: (batch_size, 8*8, units)\n        attention_weights =  tf.keras.activations.softmax(self.V(score), axis=1) # extract your attention weights with shape: (batch_size, 8*8, 1)\n        context_vector = attention_weights * features #shape: create the context vector with shape (batch_size, 8*8,embedding_dim)\n        context_vector = tf.reduce_sum(context_vector, axis = 1) # reduce the shape to (batch_size, embedding_dim)\n        \n\n        return context_vector, attention_weights","f494b270":"class Decoder(Model):\n    def __init__(self, embed_dim, units, vocab_size):\n        super(Decoder, self).__init__()\n        self.units=units\n        self.attention = Attention_model(self.units) #iniitalise your Attention model with units\n        self.embed = tf.keras.layers.Embedding(vocab_size, embed_dim, mask_zero =  False) #build your Embedding layer\n        self.gru = tf.keras.layers.GRU(self.units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n        self.d1 = tf.keras.layers.Dense(self.units) #build your Dense layer\n        self.d2 = tf.keras.layers.Dense(vocab_size) #build your Dense layer\n        self.dropout = tf.keras.layers.Dropout(0.5) #Adding Dropouts\n        \n\n    def call(self,x,features, hidden):\n        context_vector, attention_weights = self.attention(features, hidden) #create your context vector & attention weights from attention model\n        embed =  self.dropout(self.embed(x))# embed your input to shape: (batch_size, 1, embedding_dim)\n        mask = self.embed.compute_mask(x)\n        embed = tf.concat([tf.expand_dims(context_vector, 1), embed], axis=-1) # Concatenate your input with the context vector from attention layer. Shape: (batch_size, 1, embedding_dim + embedding_dim)\n        output,state = self.gru(embed, mask) # Extract the output & hidden state from GRU layer. Output shape : (batch_size, max_length, hidden_size)\n        output = self.d1(output)\n        output = tf.reshape(output, (-1, output.shape[2])) # shape : (batch_size * max_length, hidden_size)\n        output = self.d2(output) # shape : (batch_size * max_length, vocab_size)\n        \n        return output,state, attention_weights\n    \n    def init_state(self, batch_size):\n        return tf.zeros((batch_size, self.units))","8a80558c":"decoder=Decoder(embedding_dim, units, vocab_size)","4f50a88f":"features=encoder(sample_img_batch)\n\nhidden = decoder.init_state(batch_size=sample_cap_batch.shape[0])\ndec_input = tf.expand_dims([tokenizer.word_index['<start>']] * sample_cap_batch.shape[0], 1)\n\npredictions, hidden_out, attention_weights= decoder(dec_input, features, hidden)\nprint('Feature shape from Encoder: {}'.format(features.shape)) #(batch, 8*8, embed_dim)\nprint('Predcitions shape from Decoder: {}'.format(predictions.shape)) #(batch,vocab_size)\nprint('Attention weights shape from Decoder: {}'.format(attention_weights.shape)) #(batch, 8*8, embed_dim)\n","51ef5a55":"optimizer = tf.keras.optimizers.Adam() #define the optimizer\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none') #define your loss object","739a095a":"def loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss_object(real, pred)\n\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n\n    return tf.reduce_mean(loss_)","f1230a70":"checkpoint_path = \".\/checkpoints\/train\"\nckpt = tf.train.Checkpoint(encoder=encoder,\n                           decoder=decoder,\n                           optimizer = optimizer)\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)","e21d0ccd":"start_epoch = 0\nif ckpt_manager.latest_checkpoint:\n    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])","5eec6b85":"@tf.function\ndef train_step(img_tensor, target):\n    loss = 0\n    hidden = decoder.init_state(batch_size=target.shape[0])\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n    \n    with tf.GradientTape() as tape:\n        #write your code here to do the training steps\n        features = encoder(img_tensor)\n        for i in range(1, target.shape[1]):\n            # passing the features through the decoder\n            predictions, hidden, _ = decoder(dec_input, features, hidden)\n            loss += loss_function(target[:, i], predictions)\n            # using teacher forcing\n            dec_input = tf.expand_dims(target[:, i], 1)\n        avg_loss = (loss\/int(target.shape[1]))\n        trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n        gradients = tape.gradient(loss, trainable_variables)\n        optimizer.apply_gradients(zip(gradients, trainable_variables))\n        \n    return loss, avg_loss","a536590c":"print(tf.__version__)","09c17d0c":"# Define Test\/Validation Step that does not use Teacher Forcing\n#Adding this to fetch numpy from TF\n#tf.config.run_functions_eagerly(True)\n\n@tf.function\ndef test_step(img_tensor, target):\n    loss = 0\n    hidden = decoder.init_state(batch_size=target.shape[0])\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n\n    with tf.GradientTape() as tape:\n        features = encoder(img_tensor)\n\n        for i in range(1, target.shape[1]):\n            predictions, hidden, _ = decoder(dec_input, features, hidden)\n            loss += loss_function(target[:, i], predictions)\n            \n            #predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n            predicted_id = tf.argmax(predictions,1)\n            # Not using teacher forcing since test dataset\n            #dec_input = tf.expand_dims([predicted_id]* target.shape[0], 1)\n            #dec_input = tf.expand_dims(target[:, i], 1)\n            dec_input = tf.expand_dims(predicted_id, 1)\n\n        avg_loss = (loss \/ int(target.shape[1]))\n\n    return loss, avg_loss","17f4c584":"def test_loss_cal(test_dataset):\n    total_loss = 0\n    \n    for (batch, (img_tensor, target)) in enumerate(test_dataset):\n        batch_loss, t_loss = test_step(img_tensor, target)\n        total_loss += t_loss\n    avg_test_loss=total_loss\/test_num_steps\n    \n    return avg_test_loss","b1902cd5":"import sys\ndef sizeof_fmt(num, suffix='B'):\n    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n        if abs(num) < 1024.0:\n            return \"%3.1f %s%s\" % (num, unit, suffix)\n        num \/= 1024.0\n    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n\nfor name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),\n                         key= lambda x: -x[1])[:20]:\n    print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))","61eb4456":"del df \ndel caption_vector\ngc.collect()","34bccff9":"import psutil\nprocess = psutil.Process(os.getpid())\nprocess.memory_info()","e751dea4":"tf.executing_eagerly()","32b1f39c":"#Model Training \n\nloss_plot = []\ntest_loss_plot = []\nEPOCHS = 20\n\nbest_test_loss=100\nfor epoch in tqdm(range(0, EPOCHS)):\n    start = time.time()\n    total_loss = 0\n\n    for (batch, (img_tensor, target)) in enumerate(train_dataset):\n        batch_loss, t_loss = train_step(img_tensor, target)\n        total_loss += t_loss\n        avg_train_loss=total_loss \/ train_num_steps\n        if batch % 100 == 0:\n            average_batch_loss = batch_loss.numpy()\/int(target.shape[1])\n            #print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')\n        \n    loss_plot.append(avg_train_loss)    \n    test_loss = test_loss_cal(test_dataset)\n    test_loss_plot.append(test_loss)\n    \n    print ('For epoch: {}, the train loss is {:.3f}, & test loss is {:.3f}'.format(epoch+1,avg_train_loss,test_loss))\n    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n    \n    if test_loss < best_test_loss:\n        print('Test loss has been reduced from %.3f to %.3f' % (best_test_loss, test_loss))\n        best_test_loss = test_loss\n        ckpt_manager.save()","134e24cc":"gc.collect()","9c21a169":"plt.plot(loss_plot)\nplt.plot(test_loss_plot)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Loss Plot')\nplt.show()","074bfa80":"def evaluate(image):\n    attention_plot = np.zeros((max_length, attention_features_shape))\n\n    hidden = decoder.init_state(batch_size=1)\n\n    temp_input = tf.expand_dims(load_image(image)[0], 0) #process the input image to desired format before extracting features\n    img_tensor_val = image_features_extract_model(temp_input)\n    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n\n    features = encoder(img_tensor_val)\n\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 1)\n    print(\"dec_input Shape\", dec_input.shape)\n    result = []\n\n    for i in range(max_length):\n        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n\n        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n\n        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n        #print(\"Predicted id\", predicted_id)\n        #print(\"result\", tokenizer.index_word[predicted_id])\n\n        result.append(tokenizer.index_word[predicted_id])\n\n        if tokenizer.index_word[predicted_id] == '<end>':\n            return result, attention_plot,predictions\n\n        dec_input = tf.expand_dims([predicted_id], 0)\n\n    attention_plot = attention_plot[:len(result), :]\n    return result, attention_plot,predictions","f1137c08":"# Using Beam Search Evaluation\ndef beam_evaluate(image, beam_index = 3):\n    start = [tokenizer.word_index['<start>']]\n    result = [[start, 0.0]]\n\n    attention_plot = np.zeros((max_length, attention_features_shape))\n\n    hidden = decoder.init_state(batch_size=1)\n\n    temp_input = tf.expand_dims(load_image(image)[0], 0)\n    img_tensor_val = image_features_extract_model(temp_input)\n    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n\n    features = encoder(img_tensor_val)\n\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n\n    while len(result[0][0]) < max_length:\n        i=0\n        temp = []\n        for s in result:\n            predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n            attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n            i=i+1\n            word_preds = np.argsort(predictions[0])[-beam_index:]\n          \n            for w in word_preds:\n                next_cap, prob = s[0][:], s[1]\n                next_cap.append(w)\n            \n                prob += np.log(predictions[0][w])\n                    \n                temp.append([next_cap, prob])\n        result = temp\n        result = sorted(result, reverse=False, key=lambda l: l[1])\n        result = result[-beam_index:]\n        \n        \n        predicted_id = result[-1]\n        pred_list = predicted_id[0]\n        \n        prd_id = pred_list[-1] \n        if(prd_id!=3):\n            dec_input = tf.expand_dims([prd_id], 0)  \n        else:\n            break\n    \n    \n    result2 = result[-1][0]\n    \n    intermediate_caption = [tokenizer.index_word[i] for i in result2]\n    final_caption = []\n    for i in intermediate_caption:\n        if i != '<end>':\n            final_caption.append(i)\n            \n        else:\n            break\n\n    attention_plot = attention_plot[:len(result), :]\n    final_caption = ' '.join(final_caption[1:])\n    return final_caption","828223e7":"# Plotting different parts of the image which are used for captioning\ndef plot_attmap(caption, weights, image):\n\n    fig = plt.figure(figsize=(10, 10))\n    temp_img = np.array(Image.open(image))\n    \n    len_cap = len(caption)\n    for cap in range(len_cap):\n        weights_img = np.reshape(weights[cap], (8,8))\n        weights_img = np.array(Image.fromarray(weights_img).resize((224, 224), Image.LANCZOS))\n        \n        ax = fig.add_subplot(len_cap\/\/2, len_cap\/\/2, cap+1)\n        ax.set_title(caption[cap], fontsize=15)\n        \n        img=ax.imshow(temp_img)\n        \n        ax.imshow(weights_img, cmap='gist_heat', alpha=0.6,extent=img.get_extent())\n        ax.axis('off')\n    plt.subplots_adjust(hspace=0.2, wspace=0.2)\n    plt.show()","1fd35046":"# Removing start unk and end tag from the caption\ndef filt_text(text):\n    filt=['<start>','<unk>','<end>'] \n    temp= text.split()\n    [temp.remove(j) for k in filt for j in temp if k==j]\n    text=' '.join(temp)\n    return text","8e12d92b":"from nltk.translate.bleu_score import sentence_bleu\ndef Testing_Model(img_test):\n    #Testing on test image\n    rid = np.random.randint(0, len(img_test))\n    test_image = img_test[rid]\n    # test_image = '.\/images\/413231421_43833a11f5.jpg'\n    # real_caption = '<start> black dog is digging in the snow <end>'\n\n    real_caption = ' '.join([tokenizer.index_word[i] for i in cap_test[rid] if i not in [0]])\n    result, attention_plot,pred_test = evaluate(test_image)\n    real_caption=filt_text(real_caption)      \n    pred_caption=' '.join(result).rsplit(' ', 1)[0]\n\n    real_appn = []\n    real_appn.append(real_caption.split())\n    reference = real_appn\n    candidate = pred_caption.split()\n\n    print ('Real Caption      :', real_caption)\n    print ('Prediction Caption:', pred_caption)\n    print ('')\n    score1 = sentence_bleu(reference, candidate, weights=(1,0,0,0))\n    score2 = sentence_bleu(reference, candidate, weights=(0,1,0,0))\n    score3 = sentence_bleu(reference, candidate, weights=(0,0,1,0))\n    score4 = sentence_bleu(reference, candidate, weights=(0,0,0,1))\n\n    print(\"\\nBELU score: \")\n    print(f\"Individual 1-gram: {score1*100}\")\n    print(f\"Individual 2-gram: {score2*100}\")\n    print(f\"Individual 3-gram: {score3*100}\")\n    print(f\"Individual 4-gram: {score4*100}\")\n    plot_attmap(result, attention_plot, test_image)\n    \n    return test_image, pred_caption","099cd87f":"from PIL import Image\ndef Prediction_Unknown_data(test_image):\n    #Testing on test image\n    openImg = test_image\n    print(test_image)\n    #real_caption = ' '.join([tokenizer.index_word[i] for i in cap_test[rid] if i not in [0]])\n    result, attention_plot,pred_test = evaluate(test_image)\n    pred_caption=' '.join(result).rsplit(' ', 1)[0]\n\n    candidate = pred_caption.split()\n\n    print ('Prediction Caption:', pred_caption)\n    print ('')\n    #plot_attmap(result, attention_plot, test_image)\n    #plt.imshow(load_image(test_image)[0])\n    \n    im = Image.open(openImg)\n    width, height = im.size\n    print(width,height)\n    div=3\n    if width > 3000:\n        div=10\n    im = im.resize((width\/\/div, height\/\/div))\n    \n#    newsize = (200, 200)\n    #openImg = openImg.resize(200,200)\n    #return Image.open(openImg).resize(newsize)\n    return im\n    ","73e88665":"Prediction_Unknown_data(path_test[10])","a24725e1":"Prediction_Unknown_data(path_test[1])","072f3bee":"test_image, pred_caption = Testing_Model(path_test)\nImage.open(test_image)","bf7a252b":"test_image, pred_caption = Testing_Model(path_test)\nImage.open(test_image)","91b90cd8":"beam_caption=beam_evaluate(test_image)\nprint(beam_caption)","a9deeacb":"pip install gTTS","3ca46d46":"# Libraries to convert text into audio\nfrom gtts import gTTS\nfrom IPython import display","a622dce2":"#Caption to audio conversion\nsoundFile = 'pred_caption.mp3'\n\ntts = gTTS(pred_caption, slow = False)\ntts.save(soundFile)\n\ndisplay.display(display.Audio(soundFile))\n\n","b574ea6b":"Before Training the model, let's clear the unused variable and free up some CPU.","9083f53d":"## Pre-processing the images\n\n1.Resize them into the shape of (299, 299)\n\n3.Normalize the image within the range of -1 to 1, such that it is in correct format for InceptionV3. ","7cf2427e":"## Encoder","87d704ca":"# Converting Text into Audio","894d81cd":"## Model training & optimization\n1.Set the optimizer & loss object\n\n2.Create your checkpoint path\n\n3.Create your training & testing step functions\n\n4.Create your loss function for the test dataset","6f29f6b0":"#### NOTE: \n* Since there is a difference between the train & test steps ( Presence of teacher forcing), you may observe that the train loss is decreasing while your test loss is not. \n* This doesn't mean that the model is overfitting, as we can't compare the train & test results here, as both approach is different.\n* Also, if you want to achieve better results you can run it more epochs, but the intent of this capstone is to give you an idea on how to integrate attention mechanism with E-D architecture for images. The intent is not to create the state of art model. ","2983af75":"## Decoder","51e45563":"Here we will call the tensor function to load the image and corresponding path to a tensor. ","0e586544":"## Dataset creation\n1.Apply train_test_split on both image path & captions to create the train & test list. Create the train-test spliit using 80-20 ratio & random state = 42\n\n2.Create a function which maps the image path to their feature. \n\n3.Create a builder function to create train & test dataset & apply the function created earlier to transform the dataset\n\n2.Make sure you have done Shuffle and batch while building the dataset\n\n3.The shape of each image in the dataset after building should be (batch_size, 8*8, 2048)\n\n4.The shape of each caption in the dataset after building should be(batch_size, max_len)\n","f093391f":"## Model Building\n1.Set the parameters\n\n2.Build the Encoder, Attention model & Decoder","9137bad5":"## Pre-Processing the captions\n1.Create the tokenized vectors by tokenizing the captions fore ex :split them using spaces & other filters. \nThis gives us a vocabulary of all of the unique words in the data. Keep the total vocaublary to top 5,000 words for saving memory.\n\n2.Replace all other words with the unknown token \"UNK\" .\n\n3.Create word-to-index and index-to-word mappings.\n\n4.Pad all sequences to be the same length as the longest one.","f418d382":"### FAQs on how to load the features:\n* You can load the features using the dictionary created earlier OR\n* You can store using numpy(np.load) to load the feature vector.","c35349fb":"## Greedy Search","a44ee702":"## Data understanding\n1.Import the dataset and read image & captions into two seperate variables\n\n2.Visualise both the images & text present in the dataset\n\n3.Create word-to-index and index-to-word mappings.\n\n4.Create a dataframe which summarizes the image, path & captions as a dataframe\n\n5.Visualise the top 30 occuring words in the captions\n\n6.Create a list which contains all the captions & path","87c08e4a":"## Beam Search","111c819d":"* While creating the training step for your model, you will apply Teacher forcing.\n* Teacher forcing is a technique where the target\/real word is passed as the next input to the decoder instead of previous prediciton.","3bd00235":"**Inception Model Image**","ccb335bb":"### FAQs on how to store the features:\n* You can store the features using a dictionary with the path as the key and values as the feature extracted by the inception net v3 model OR\n* You can store using numpy(np.save) to store the resulting vector.","075b921d":"## Attention Model","44d172e5":"## Model Evaluation\n1.Define your evaluation function using greedy search\n\n2.Define your evaluation function usininit_stateeam search ( optional)\n\n3.Test it on a sample data using BLEU score","d35aacfc":"**Eye For Blind**\n\n**Problem Statement:**\nCreate a model which is able to caption an image to help blind people, so that they can also get the feel of image.\n\n**Notebook Description:**\nThis notebook will contain code which are used to create audio text from an given image which very useful for blind people.\n\nAttention model mechenism has been used with Encoder and Decorder mechenism.\n**Greedy Search** and **Beam Search** has been used\n\n**Encoder:** It will get create the features of the image<\/br>\n**Decoder:** It use the features created by Encoder and output from attention model.<\/br>\n**Attention Mechenism:** It is used to overcome the problem of Encoder and Decoder architecture. By using this we need to focus only the specific part of the image while generating caption (as human do i.e when we want to describe any paerticular object we only focus on that object as that time).","d1f61f80":"## Load the pretrained Imagenet weights of Inception net V3\n\n1.To save the memory(RAM) from getting exhausted, extract the features of the images using the last layer of pre-trained model. Including this as part of training will lead to higher computational time.\n\n2.The shape of the output of this layer is 8x8x2048. \n\n3.Use a function to extract the features of each image in the train & test dataset such that the shape of each image should be (batch_size, 8*8, 2048)\n\n","2f7c32f9":"### FAQs on how to resize the images::\n* Since you have a list which contains all the image path, you need to first convert them to a dataset using <i>tf.data.Dataset.from_tensor_slices<\/i>. Once you have created a dataset consisting of image paths, you need to apply a function to the dataset which will apply the necessary preprocessing to each image. \n* This function should resize them and also should do the necessary preprocessing that it is in correct format for InceptionV3.\n"}}