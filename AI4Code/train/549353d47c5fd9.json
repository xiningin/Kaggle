{"cell_type":{"f47c70ce":"code","c824b999":"code","5a32c294":"code","859b94c7":"code","8b7efbc4":"code","312b8cb1":"code","49a102c3":"code","64d9d79f":"code","142b87f4":"code","c08b5341":"code","569417f2":"code","deb79b85":"code","f9cc7d9c":"code","8f7d1434":"code","73f6db73":"code","a01bfe70":"code","546c35c1":"code","23765269":"code","df40927f":"code","b53e39c4":"code","2494fd2d":"code","7f26686f":"code","87c516eb":"code","a7506fe9":"code","33036f41":"code","0faa0f15":"code","77f3440c":"code","573b699f":"code","9784268f":"markdown","695e1caa":"markdown","807c9279":"markdown","c7ec21c3":"markdown","d53abae4":"markdown","bdfe42b4":"markdown","86f82192":"markdown","6b430f06":"markdown","12bbaff4":"markdown","e5123daf":"markdown","235d659e":"markdown","f929e73f":"markdown","15dd8361":"markdown","bdfa6c34":"markdown","1aa021ad":"markdown","ac8c844e":"markdown","935e41ee":"markdown","b8cdd31f":"markdown","1811507b":"markdown","0c65274c":"markdown","bdfba1aa":"markdown","4220ac95":"markdown","fc951137":"markdown"},"source":{"f47c70ce":"!pip install \/kaggle\/input\/adjusttext\n!pip install \/kaggle\/input\/bioinfokit","c824b999":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, KFold, GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import log_loss\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# for feature importance study\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom pdpbox import pdp\nimport shap\n\n# for PCA\nfrom bioinfokit.visuz import cluster\nfrom sklearn.decomposition import PCA\n\n# Custom theme\nplt.style.use('fivethirtyeight')\n\nfigure = {'dpi': '200'}\nfont = {'family': 'serif'}\ngrid = {'linestyle': ':', 'alpha': .9}\naxes = {'titlecolor': 'black', 'titlesize': 20, 'titleweight': 'bold',\n        'labelsize': 12, 'labelweight': 'bold'}\n\nplt.rc('font', **font)\nplt.rc('figure', **figure)\nplt.rc('grid', **grid)\nplt.rc('axes', **axes)\n\nmy_colors = ['#DC143C', '#FF1493', '#FF7F50', '#FFD700', '#32CD32', \n             '#00FFFF', '#1E90FF', '#663399', '#708090']\n\ncaption = \"\u00a9 maksymshkliarevskyi\"\n\n# Show our custom palette\nsns.palplot(sns.color_palette(my_colors))\nplt.title('Custom palette')\nplt.text(6.9, 0.75, caption, size = 8)\nplt.show()","5a32c294":"train = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/train.csv', \n                    index_col = 0)\ntest = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/test.csv', \n                   index_col = 0)\nss = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/sample_submission.csv', \n                 index_col = 0)\n\ny = train.target\ntarget_names = np.sort(train.target.unique())\ntarget = LabelEncoder().fit_transform(train.target) + 1\ntrain.drop(['target'], axis = 1, inplace = True)","859b94c7":"sum(train.columns != test.columns)","8b7efbc4":"train.describe().T.style.background_gradient(subset = ['count'], cmap = 'viridis') \\\n    .bar(subset = ['mean', '50%'], color = my_colors[6]) \\\n    .bar(subset = ['std'], color = my_colors[0])","312b8cb1":"test.describe().T.style.background_gradient(subset = ['count'], cmap = 'viridis') \\\n    .bar(subset = ['mean', '50%'], color = my_colors[6]) \\\n    .bar(subset = ['std'], color = my_colors[0])","49a102c3":"dtypes = train.dtypes.value_counts().reset_index()\n\nplt.figure(figsize = (12, 1))\nplt.title('Data types\\n')\nplt.barh(str(dtypes.iloc[0, 0]), dtypes.iloc[0, 1],\n         label = str(dtypes.iloc[0, 0]), color = my_colors[4])\nplt.legend(loc = 'upper center', ncol = 3, fontsize = 13,\n           bbox_to_anchor = (0.5, 1.45), frameon = False)\nplt.yticks('')\nplt.text(65, -0.9, caption, size = 8)\nplt.show()","64d9d79f":"# Concatenate train and test datasets\nall_data = pd.concat([train, test], axis = 0)\n\n# columns with missing values\ncols_with_na = all_data.isna().sum()[all_data.isna().sum() > 0].sort_values(ascending = False)\ncols_with_na","142b87f4":"fig = plt.figure(figsize = (20, 70))\nfor i in range(len(train.columns)):\n    fig.add_subplot(np.ceil(len(train.columns)\/5), 5, i+1)\n    all_data.iloc[:, i].hist(bins = 20)\n    plt.title('feature_{}'.format(i))\nplt.text(25, -50000, caption, size = 12)\nplt.show()","c08b5341":"plt.figure(figsize = (12, 4))\nplt.title('Target feature')\nsns.countplot(x = y, edgecolor = 'black', \n              palette = sns.color_palette(my_colors))\nplt.xlabel('')\nplt.text(7.5, -8000, caption, size = 8)\nplt.show()","569417f2":"corr = train.corr()\nmask = np.triu(np.ones_like(corr, dtype = bool))\n\nplt.figure(figsize = (15, 15))\nplt.title('Corelation matrix')\nsns.heatmap(corr, mask = mask, cmap = 'Spectral_r', linewidths = .5)\nplt.text(77, 83, caption, size = 8)\nplt.show()","deb79b85":"# Create data sets for training (80%) and validation (20%)\nX_train, X_valid, y_train, y_valid = train_test_split(train, target, \n                                                      test_size = 0.2,\n                                                      random_state = 0)","f9cc7d9c":"# The basic model\nparams = {'random_state': 0,\n          'predictor': 'gpu_predictor',\n          'tree_method': 'gpu_hist',\n          'eval_metric': 'logloss'}\n\nmodel = XGBClassifier(**params)\n\nmodel.fit(X_train, y_train, verbose = False)\n\npreds = model.predict_proba(X_valid)\nprint('Valid log_loss of the basic model: {}'.format(log_loss(y_valid, preds)))","8f7d1434":"pi = PermutationImportance(model, random_state = 0).fit(X_valid, y_valid)\neli5.show_weights(pi, feature_names = X_valid.columns.tolist())","73f6db73":"pdp_f = pdp.pdp_isolate(model = model, dataset = X_valid, \n                        model_features = X_valid.columns.tolist(),\n                        feature = 'feature_43')\npdp.pdp_plot(pdp_f, 'feature_43')\nplt.text(125, -0.12, caption, size = 8)\nplt.show()","a01bfe70":"pdp_f = pdp.pdp_isolate(model = model, dataset = X_valid, \n                        model_features = X_valid.columns.tolist(),\n                        feature = 'feature_56')\npdp.pdp_plot(pdp_f, 'feature_56')\nplt.text(90, -0.1, caption, size = 8)\nplt.show()","546c35c1":"pdp_f = pdp.pdp_isolate(model = model, dataset = X_valid, \n                        model_features = X_valid.columns.tolist(),\n                        feature = 'feature_21')\npdp.pdp_plot(pdp_f, 'feature_21')\nplt.text(70, -0.07, caption, size = 8)\nplt.show()","23765269":"features = ['feature_43', 'feature_56']\ninter = pdp.pdp_interact(model = model, dataset = X_valid,\n                         model_features = X_valid.columns.tolist(),\n                         features = features)\npdp.pdp_interact_plot(inter, feature_names = features,\n                      plot_type = 'contour')\nplt.text(0.55, 0.118, caption, size = 8)\nplt.show()","df40927f":"explainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_valid)\n\nfor i in range(len(target_names)):\n    plt.title('Class_{}'.format(i+1))\n    shap.summary_plot(shap_values[i], X_valid)","b53e39c4":"pca = PCA()\npca_out = pca.fit(train)","2494fd2d":"comp = pca_out.components_\nnum_pc = pca_out.n_features_\npc_list = [\"PC\" + str(i) for i in list(range(1, num_pc + 1))]\ncomp_df = pd.DataFrame.from_dict(dict(zip(pc_list, comp)))\ncomp_df['variable'] = train.columns.values\ncomp_df = comp_df.set_index('variable')\n\ncomp_df.head(10).style.background_gradient(cmap = 'viridis')","7f26686f":"cluster.screeplot(obj = [pc_list[:20], pca_out.explained_variance_ratio_[:20]], \n                  show = True, dim = (16, 5), axlabelfontsize = 13)","87c516eb":"# PCA loadings plots\n# 2D\ncluster.pcaplot(x = comp[0], y = comp[1], \n                labels = range(num_pc), \n                var1 = round(pca_out.explained_variance_ratio_[0]*100, 2),\n                var2 = round(pca_out.explained_variance_ratio_[1]*100, 2),\n                show = True, dim = (10, 8), axlabelfontsize = 13)\n\n# 3D\ncluster.pcaplot(x = comp[0], y = comp[1], z = comp[2],  \n                labels = range(num_pc), \n                var1 = round(pca_out.explained_variance_ratio_[0]*100, 2), \n                var2 = round(pca_out.explained_variance_ratio_[1]*100, 2), \n                var3 = round(pca_out.explained_variance_ratio_[2]*100, 2),\n                show = True, dim = (14, 10), axlabelfontsize = 13)","a7506fe9":"X_pca = pd.DataFrame(pca.transform(train), columns = pc_list)\ntrain_new = pd.concat([train, X_pca.iloc[:, :5]], axis = 1)\n\n# Create data sets for training (80%) and validation (20%)\nX_train, X_valid, y_train, y_valid = train_test_split(train_new, target, \n                                                      test_size = 0.2,\n                                                      random_state = 0)","33036f41":"# The basic model\nparams = {'random_state': 0,\n          'predictor': 'gpu_predictor',\n          'tree_method': 'gpu_hist',\n          'eval_metric': 'logloss'}\n\nmodel = XGBClassifier(**params)\n\nmodel.fit(X_train, y_train, verbose = False)\n\npreds = model.predict_proba(X_valid)\nprint('Valid log_loss of the basic model: {}'.format(log_loss(y_valid, preds)))","0faa0f15":"FOLDS = 7\n\nss.iloc[:, :] = np.zeros((len(test), 9))\n\nparams = {'n_estimators': 400,\n          'max_depth': 6,\n          'min_child_weight': 3,\n          'learning_rate': 0.03,\n          'subsample': 0.7,\n          'random_state': 0,\n          'predictor': 'gpu_predictor',\n          'tree_method': 'gpu_hist',\n          'eval_metric': 'logloss'}\n\nkfold = KFold(n_splits = FOLDS, random_state = 0, shuffle = True)\ni = 1\nfor train_idx, test_idx in kfold.split(train_new):\n    print('Training {} fold...'.format(i))\n    X_train, y_train = train_new.iloc[train_idx, :], target[train_idx]\n    X_valid, y_valid = train_new.iloc[test_idx, :], target[test_idx]\n    \n    model = XGBClassifier(**params)\n    model.fit(X_train, y_train, verbose = False)\n    preds = model.predict_proba(X_valid)\n    print('Valid log_loss for {} fold: {}'.format(i, log_loss(y_valid, preds)))\n    i += 1\n    ss.iloc[:, :] += model.predict_proba(test) \/ FOLDS","77f3440c":"ss","573b699f":"ss.to_csv('submission.csv', index = True)","9784268f":"<a id=\"section-5\"><\/a>\n<h2 style='color:white; background:#008294; border:0'><center>3.2. Feature importance<\/center><\/h2>\n\n[**Back to the table of contents**](#section-start)","695e1caa":"<a id=\"section-1\"><\/a>\n<h1 style='color:white; background:#008294; border:0'><center>1. Loading libraries and data<\/center><\/h1>\n\n[**Back to the table of contents**](#section-start)","807c9279":"As we can see, in the context of classes, these features have different effects. An increase in the values of these features leads to an increase in the prediction probability for some classes and a decrease in others.\n\nLet's also take a look at the relationship between the two most important features.","c7ec21c3":"### Test data","d53abae4":"Some features are clearly different from others. This will also need to be taken into account in the subsequent modeling.\n\nLet's add the components as new features and train the base model again.","bdfe42b4":"Almost all features have approximately the same importance for the model. The most important are 'feature_43', 'feature_56' and 'feature_21'. Let's take a closer look and visualize them with the Partial Dependence Plot ([pdpbox library](https:\/\/pdpbox.readthedocs.io\/en\/latest\/)).","86f82192":"<a id=\"section-2\"><\/a>\n<h1 style='color:white; background:#008294; border:0'><center>2. EDA<\/center><\/h1>\n\n[**Back to the table of contents**](#section-start)\n\n","6b430f06":"<h2 style='color:white; background:#008294; border:0'><center>WORK IN PROGRESS...<\/center><\/h2>\n\n[**Back to the table of contents**](#section-start)","12bbaff4":"All features are weakly correlated.\n\n<a id=\"section-3\"><\/a>\n<h1 style='color:white; background:#008294; border:0'><center>3. Baseline<\/center><\/h1>\n\n[**Back to the table of contents**](#section-start)\n\nThe algorithm of our actions will be as follows:\n- first, we'll train a very simple basic XGBClassifier,\n- then we'll look at the importance of features and build some interesting visualizations,\n- and then we'll make a PCA.","e5123daf":"<h1 style='color:white; background:#008294; border:0'><center>TPS-Jun: starting point (EDA, Baseline)<\/center><\/h1>\n\n![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/26480\/logos\/header.png?t=2021-04-09-00-57-05)\n\n<a id=\"section-start\"><\/a>\n\nThe goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition.\n\nThe dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the category on an eCommerce product given various attributes about the listing. Although the features are anonymized, they have properties relating to real-world features.\n\n<h3 style='color:white; background:#008294; border:0'><center>Table of contents:<\/center><\/h3>\n\n\n1. [**Loading libraries and data**](#section-1) <br>\n2. [**EDA**](#section-2) <br>\n3. [**Baseline**](#section-3) <br>\n 3.1. [Simple model](#section-4) <br>\n 3.2. [Feature importance](#section-5) <br>\n 3.3. [Principal Component Analysis](#section-6) <br>\n 3.4. [Test prediction](#section-7) <br>","235d659e":"This competition dataset is similar to the Tabular Playground Series - May 2021 dataset, but with increased observations, increased features, and increased class labels.","f929e73f":"<a id=\"section-4\"><\/a>\n<h2 style='color:white; background:#008294; border:0'><center>3.1. Simple model<\/center><\/h2>\n\n[**Back to the table of contents**](#section-start)\n\nAt first, we'll split our data into train and validation sets.","15dd8361":"This practically did not affect the accuracy of our model, but do not forget that this is a very basic, almost untuned model. In the future, we will experiment with different models and their parameters. For a starting point, this, I think, is enough.\n\nIn the future, we will expand our data analysis and experiment with machine learning.\n\n<a id=\"section-7\"><\/a>\n<h2 style='color:white; background:#008294; border:0'><center>3.4. Test prediction<\/center><\/h2>\n\n[**Back to the table of contents**](#section-start)\n\nNow let's train the model with cross-validation and some parameters and then predict the test data.","bdfa6c34":"Good sign! Let's take a look at the general statistical information about our data. We'll use a built-in function describe with some visual features.","1aa021ad":"Let's see if columns in train and test datasets are the same.\nThe result must be zero.","ac8c844e":"Now, we'll see at the permutation importance of features.","935e41ee":"### Train data","b8cdd31f":"The nature of the interaction varies greatly from class to class. For some, the prediction probability increases with large values of both features, for others, vice versa.","1811507b":"We should also look at the correlation between features","0c65274c":"The contribution of various features to prediction varies greatly, but we can see a completely logical pattern: large feature values most often lead to a stronger change in the prediction probability for some classes.\n\n<a id=\"section-6\"><\/a>\n<h2 style='color:white; background:#008294; border:0'><center>3.3. Principal Component Analysis<\/center><\/h2>\n\n[**Back to the table of contents**](#section-start)\n\nNow, let's do a principal component analysis. We'll look at the correlation between components and features, as well as study the proportion of variance for the first components. We'll use the bioinfokit package to visualize the PCA results.","bdfba1aa":"It's also important to see if our data has missing values.","4220ac95":"As before, our data has no missing values. Now, let's look at the feature distributions.","fc951137":"In my opinion, for a start, we can try to use the first five components as additional features."}}