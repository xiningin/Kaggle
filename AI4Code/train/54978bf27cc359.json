{"cell_type":{"831e1fe4":"code","c0c7ca42":"code","2feb6a5b":"code","71097bf2":"code","13650e1b":"code","ca189b10":"code","53624f2b":"code","71ce91c0":"code","c87183a8":"code","b8b4188a":"code","bfe9175d":"code","3af35c9a":"code","7cd65b78":"code","11970482":"code","fccee113":"code","ac4abee4":"code","9ad8f983":"code","747414f2":"code","5fc02aff":"code","164012ef":"code","4129af2b":"markdown","5829c4c3":"markdown","a54115e2":"markdown","4d8e1475":"markdown","18da504a":"markdown","6de0e5e8":"markdown","b1a72c53":"markdown","8da7001d":"markdown","47605223":"markdown","b7144574":"markdown","c1ec107a":"markdown","3a32611f":"markdown","da18c766":"markdown","a80cde73":"markdown"},"source":{"831e1fe4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c0c7ca42":"\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom scipy.stats import skew\nfrom scipy.stats import uniform\n\nfrom xgboost import XGBRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.metrics import make_scorer \n\n# ignore Deprecation Warning\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) ","2feb6a5b":"df_train = pd.read_csv(r'..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv(r'..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ndf = df_train.append(df_test , ignore_index = True)\ndf_train.shape, df_test.shape, df_train.columns.values","71097bf2":"df.SalePrice = np.log(df.SalePrice)","13650e1b":"#!pip install pandas_profiling\n#from pandas_profiling import ProfileReport\n#report = ProfileReport(df_train, title=\"Pandas Profiling Report\")","ca189b10":"quan = list( df_test.loc[:,df_test.dtypes != 'object'].drop('Id',axis=1).columns.values )\nqual = list( df_test.loc[:,df_test.dtypes == 'object'].columns.values )","53624f2b":"# Find out how many missing values there are for the quantitative and categorical features\nhasNAN = df[quan].isnull().sum()\nhasNAN = hasNAN[hasNAN > 0]\nhasNAN = hasNAN.sort_values(ascending=False)\nprint(hasNAN)\nprint('**'*40)\nhasNAN = df[qual].isnull().sum()\nhasNAN = hasNAN[hasNAN > 0]\nhasNAN = hasNAN.sort_values(ascending=False)\nprint(hasNAN)","71ce91c0":"df.LotFrontage.fillna(df.LotFrontage.median(), inplace=True)\n# NAN should mean no garage. I temporarily use yr = 0 here. Will come back to this later. \ndf.GarageYrBlt.fillna(0, inplace=True)\n# Use zero\ndf.MasVnrArea.fillna(0, inplace=True)    \ndf.BsmtHalfBath.fillna(0, inplace=True)\ndf.BsmtFullBath.fillna(0, inplace=True)\ndf.GarageArea.fillna(0, inplace=True)\ndf.GarageCars.fillna(0, inplace=True)    \ndf.TotalBsmtSF.fillna(0, inplace=True)   \ndf.BsmtUnfSF.fillna(0, inplace=True)     \ndf.BsmtFinSF2.fillna(0, inplace=True)    \ndf.BsmtFinSF1.fillna(0, inplace=True)","c87183a8":"\ndf.PoolQC.fillna('NA', inplace=True)\ndf.MiscFeature.fillna('NA', inplace=True)    \ndf.Alley.fillna('NA', inplace=True)          \ndf.Fence.fillna('NA', inplace=True)         \ndf.FireplaceQu.fillna('NA', inplace=True)    \ndf.GarageCond.fillna('NA', inplace=True)    \ndf.GarageQual.fillna('NA', inplace=True)     \ndf.GarageFinish.fillna('NA', inplace=True)   \ndf.GarageType.fillna('NA', inplace=True)     \ndf.BsmtExposure.fillna('NA', inplace=True)     \ndf.BsmtCond.fillna('NA', inplace=True)        \ndf.BsmtQual.fillna('NA', inplace=True)        \ndf.BsmtFinType2.fillna('NA', inplace=True)     \ndf.BsmtFinType1.fillna('NA', inplace=True)     \ndf.MasVnrType.fillna('None', inplace=True)   \ndf.Exterior2nd.fillna('None', inplace=True) \n\n# These are general properties that all houses should have, so NANs probably \n# just mean the values were not recorded. I therefore use \"mode\", the most \n# common value to fill in\ndf.Functional.fillna(df.Functional.mode()[0], inplace=True)       \ndf.Utilities.fillna(df.Utilities.mode()[0], inplace=True)          \ndf.Exterior1st.fillna(df.Exterior1st.mode()[0], inplace=True)        \ndf.SaleType.fillna(df.SaleType.mode()[0], inplace=True)                \ndf.KitchenQual.fillna(df.KitchenQual.mode()[0], inplace=True)        \ndf.Electrical.fillna(df.Electrical.mode()[0], inplace=True)    \n\n# MSZoning should highly correlate with the location, so I use the mode values of individual \n# Neighborhoods\nfor i in df.Neighborhood.unique():\n    if df.MSZoning[df.Neighborhood == i].isnull().sum() > 0:\n        df.loc[df.Neighborhood == i,'MSZoning'] = \\\n        df.loc[df.Neighborhood == i,'MSZoning'].fillna(df.loc[df.Neighborhood == i,'MSZoning'].mode()[0])","b8b4188a":"\ndf.Alley = df.Alley.map({'NA':0, 'Grvl':1, 'Pave':2})\ndf.BsmtCond =  df.BsmtCond.map({'NA':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\ndf.BsmtExposure = df.BsmtExposure.map({'NA':0, 'No':1, 'Mn':2, 'Av':3, 'Gd':4})\ndf['BsmtFinType1'] = df['BsmtFinType1'].map({'NA':0, 'Unf':1, 'LwQ':2, 'Rec':3, 'BLQ':4, 'ALQ':5, 'GLQ':6})\ndf['BsmtFinType2'] = df['BsmtFinType2'].map({'NA':0, 'Unf':1, 'LwQ':2, 'Rec':3, 'BLQ':4, 'ALQ':5, 'GLQ':6})\ndf.BsmtQual = df.BsmtQual.map({'NA':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\ndf.ExterCond = df.ExterCond.map({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\ndf.ExterQual = df.ExterQual.map({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\ndf.FireplaceQu = df.FireplaceQu.map({'NA':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\ndf.Functional = df.Functional.map({'Sal':1, 'Sev':2, 'Maj2':3, 'Maj1':4, 'Mod':5, 'Min2':6, 'Min1':7, 'Typ':8})\ndf.GarageCond = df.GarageCond.map({'NA':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\ndf.GarageQual = df.GarageQual.map({'NA':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\ndf.HeatingQC = df.HeatingQC.map({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\ndf.KitchenQual = df.KitchenQual.map({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\ndf.LandSlope = df.LandSlope.map({'Sev':1, 'Mod':2, 'Gtl':3}) \ndf.PavedDrive = df.PavedDrive.map({'N':1, 'P':2, 'Y':3})\ndf.PoolQC = df.PoolQC.map({'NA':0, 'Fa':1, 'TA':2, 'Gd':3, 'Ex':4})\ndf.Street = df.Street.map({'Grvl':1, 'Pave':2})\ndf.Utilities = df.Utilities.map({'ELO':1, 'NoSeWa':2, 'NoSewr':3, 'AllPub':4})\n\n# Update my lists of numerical and categorical features\nnewquan = ['Alley','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','BsmtQual',\n           'ExterCond','ExterQual','FireplaceQu','Functional','GarageCond',\n           'GarageQual','HeatingQC','KitchenQual','LandSlope','PavedDrive','PoolQC',\n           'Street','Utilities']\nquan = quan + newquan \nfor i in newquan: qual.remove(i)\n\n\n# This is actually a categorical feature\ndf.MSSubClass = df.MSSubClass.map({20:'class1', 30:'class2', 40:'class3', 45:'class4',\n                                   50:'class5', 60:'class6', 70:'class7', 75:'class8',\n                                   80:'class9', 85:'class10', 90:'class11', 120:'class12',\n                                   150:'class13', 160:'class14', 180:'class15', 190:'class16'})\n\n# Keeping \"YrSold\" is enough\ndf=df.drop('MoSold',axis=1)\n\n# Update my lists of numerical and categorical features\nquan.remove('MoSold')\nquan.remove('MSSubClass')\nqual.append('MSSubClass')","bfe9175d":"\ndf['Age'] = df.YrSold - df.YearBuilt\ndf['AgeRemod'] = df.YrSold - df.YearRemodAdd\ndf['AgeGarage'] = df.YrSold - df.GarageYrBlt\n\n# For the houses without a Garage, I filled the NANs with zeros, which makes AgeGarage ~ 2000\n# Here I replace their AgeGarage with the maximum value among the houses with Garages\nmax_AgeGarage = np.max(df.AgeGarage[df.AgeGarage < 1000])\ndf['AgeGarage'] = df['AgeGarage'].map(lambda x: max_AgeGarage if x > 1000 else x)\n\n# Some of the values are negative because the work was done after the house \n# was sold. In these cases, I change them to zero to avoid negative ages.\ndf.Age = df.Age.map(lambda x: 0 if x < 0 else x)\ndf.AgeRemod = df.AgeRemod.map(lambda x: 0 if x < 0 else x)\ndf.AgeGarage = df.AgeGarage.map(lambda x: 0 if x < 0 else x)\n\n# drop the original time variables \ndf=df.drop(['YrSold','YearBuilt','YearRemodAdd','GarageYrBlt'],axis=1)\n\n# update my list of numerical feature\nfor i in ['YrSold','YearBuilt','YearRemodAdd','GarageYrBlt']: quan.remove(i)\nquan = quan + ['Age','AgeRemod','AgeGarage']","3af35c9a":"index_drop = df.LotFrontage[df.LotFrontage > 300].index\nindex_drop = np.append(index_drop, df.LotArea[df.LotArea > 100000].index)\nindex_drop = np.append(index_drop, df.BsmtFinSF1[df.BsmtFinSF1 > 4000].index)\nindex_drop = np.append(index_drop, df.TotalBsmtSF[df.TotalBsmtSF > 6000].index)\nindex_drop = np.append(index_drop, df['1stFlrSF'][df['1stFlrSF'] > 4000].index)\nindex_drop = np.append(index_drop, df.GrLivArea[(df.GrLivArea > 4000) & (df.SalePrice < 13)].index)\nindex_drop = np.unique(index_drop)\n\n# make sure we only remove data from the training set\nindex_drop = index_drop[index_drop < 1460] \n\ndf = df.drop(index_drop).reset_index(drop=True)\nprint(\"{} examples in the training set are dropped.\".format(len(index_drop)))","7cd65b78":"\n# print the skewness of each numerical feature\nfor i in quan:\n    print(i+': {}'.format(round(skew(df[i]),2)))\n    \n# transform those with skewness > 0.5\nskewed_features = np.array(quan)[np.abs(skew(df[quan])) > 0.5]\ndf[skewed_features] = np.log1p(df[skewed_features])","11970482":"\ndummy_drop = []\nfor i in qual:\n    dummy_drop += [ i+'_'+str(df[i].unique()[-1]) ]\n\n# create dummy variables\ndf = pd.get_dummies(df,columns=qual) \n# drop the last column generated from each categorical feature\ndf = df.drop(dummy_drop,axis=1)","fccee113":"\nX_train  = df[:-1459].drop(['SalePrice','Id'], axis=1)\ny_train  = df[:-1459]['SalePrice']\nX_test  = df[-1459:].drop(['SalePrice','Id'], axis=1)\n\n# fit the training set only, then transform both the training and test sets\nscaler = RobustScaler()\nX_train[quan]= scaler.fit_transform(X_train[quan])\nX_test[quan]= scaler.transform(X_test[quan])\n\nX_train.shape, X_test.shape # now we have 220 features!","ac4abee4":"xgb = XGBRegressor()\nxgb.fit(X_train, y_train)\nimp = pd.DataFrame(xgb.feature_importances_ ,columns = ['Importance'],index = X_train.columns)\nimp = imp.sort_values(['Importance'], ascending = False)\n\nprint(imp)","9ad8f983":"# Define a function to calculate RMSE\ndef rmse(y_true, y_pred):\n    return np.sqrt(np.mean((y_true-y_pred)**2))\n\n# Define a function to calculate negative RMSE (as a score)\ndef nrmse(y_true, y_pred):\n    return -1.0*rmse(y_true, y_pred)\n\nneg_rmse = make_scorer(nrmse)\n\nestimator = XGBRegressor()\nselector = RFECV(estimator, cv = 3, n_jobs = -1, scoring = neg_rmse)\nselector = selector.fit(X_train, y_train)\n\nprint(\"The number of selected features is: {}\".format(selector.n_features_))\n\nfeatures_kept = X_train.columns.values[selector.support_] \nX_train = X_train[features_kept]\nX_test = X_test[features_kept]","747414f2":"\n# These are the selected features \nfeatures_kept","5fc02aff":"ridge = KernelRidge()\n\nparameters = {'alpha': uniform(0.05, 1.0), 'kernel': ['polynomial'], \n              'degree': [2], 'coef0':uniform(0.5, 3.5)}\n\nrandom_search = RandomizedSearchCV(estimator = ridge,\n                                   param_distributions = parameters,\n                                   n_iter = 1000,\n                                   cv = 3,\n                                   scoring = neg_rmse,\n                                   n_jobs = -1,\n                                   random_state=0)\n\nrandom_search = random_search.fit(X_train, y_train)\n\nprint(\"Parameters of the best_estimator:\")\nprint(random_search.best_params_)\nprint(\"Mean cross-validated RMSE of the best_estimator: {}\".format(-random_search.best_score_))\nmodel = random_search.best_estimator_\nprint(\"RMSE of the whole training set: {}\".format(rmse(y_train, model.predict(X_train))))","164012ef":"\n# Make predictions on the test set\ny_pred = np.exp(model.predict(X_test))\noutput = pd.DataFrame({'Id': df_test['Id'], 'SalePrice': y_pred})\noutput.to_csv('prediction.csv', index=False)","4129af2b":"## Importing necessary libraries","5829c4c3":"# What is XGBRegressor?\nXGBoost stands for \"Extreme Gradient Boosting\" and it is an implementation of gradient boosting trees algorithm. The XGBoost is a popular supervised machine learning model with characteristics like computation speed, parallelization, and performance. ","a54115e2":"# Capturing the difference of the years rather than the year itself","4d8e1475":"# In order to select the required features we make use of the RFECV i.e Feature ranking with recursive feature elimination and cross-validated selection of the best number of features.","18da504a":"# Creating a list of dummy variables that I will drop, which will be the last column generated from each categorical feature","6de0e5e8":"# What is KernelRidge ?\nKernel ridge regression (KRR) combines ridge regression (linear least squares with l2-norm regularization) with the kernel trick. It thus learns a linear function in the space induced by the respective kernel and the data. For non-linear kernels, this corresponds to a non-linear function in the original space.\n\nThe form of the model learned by KRR is identical to support vector regression (SVR). However, different loss functions are used: KRR uses squared error loss while support vector regression uses epsilon-insensitive loss, both combined with l2 regularization. In contrast to SVR, fitting a KRR model can be done in closed-form and is typically faster for medium-sized datasets. On the other hand, the learned model is non-sparse and thus slower than SVR, which learns a sparse model for epsilon > 0, at prediction-time.\n\nThis estimator has built-in support for multi-variate regression (i.e., when y is a 2d-array of shape [n_samples, n_targets]).","b1a72c53":"# Splitting data into numerical and categorical","8da7001d":"# Skewness","47605223":"\n# These categorical features are \"rank\", so they can be transformed tonumerical features","b7144574":"\n# Filling missing values for categorical features","c1ec107a":"# Loading the data","3a32611f":"**Filling missing values for numerical features. Most of the NAN should mean that the corresponding facillity\/structure doesn't exist, so I use zero for most cases**","da18c766":"## Note :- Here I would like to put forward that the evaluation metric at the end would be RMSE of the logarithm of predictions of the final sale price. For exploration we log-transform the sale price and convert it back at the end.","a80cde73":"# Let's now Filter the NaN values"}}