{"cell_type":{"381a9de5":"code","23bc4436":"code","6e767a54":"code","bff313ae":"code","b2c7d16c":"markdown","394fe36a":"markdown","e7c085d6":"markdown"},"source":{"381a9de5":"%%writefile submission.py\n\n\"\"\"Agent that samples the estimated payout ratio distribution.\n\nThe a posteriori distribution of potential payout ratios for each machine\nis tracked and updated based on the results of each pull. The a priori\ndistribution for each machine is a uniform distribution from 0.0 to 1.0.\n\nWhen selecting a machine to pull, each distribution is sample a configurable\nnumber of times.  The machine with the sample(s) that generate the highest\nexpected reward is selected for the next pull.\n\n\"\"\"\nimport random\n\nimport numpy as np\n\n# Parameters\nNUM_SAMPLES = 5\nPRIOR_DISTRIBUTION = np.array([0.01] * 100)\n\n\nclass MonteCarloStragegy:\n    \"\"\"Implements strategy to maximize expected value\n\n    - Tracks estimated likelihood of payout ratio for each machine\n    - Tracks number of pulls on each machine\n    - Chooses machine based on maximum reward from a limited Monte-Carlo \n      simulation based on the estimated distribution of payout ratios\n    \n    \n    \"\"\"\n    def __init__(self, name, agent_num, initial_dist, ev_rounds, n_machines):\n        \"\"\"Initialize with simple distribution of payout probabilities\n\n        Args:\n           name (str):   Name for the agent\n           agent_num (int):   Assigned player number\n           initial_dist (np.array, (100,)):   a priori payout distribution for\n               each machine.\n           ev_rounds (int):   number of samples to average for monte-carlo\n               expected value calculation\n           n_machines (int):   number of machines in the game\n        \n        \"\"\"\n        # Record inputs\n        self.name = name\n        self.agent_num = agent_num\n        self.initial_dist = initial_dist\n        self.ev_rounds = ev_rounds  # Num rounds to base MC choice on\n        self.n_machines = n_machines\n        \n        # Initialize discrete set of payout ratios\n        self.p_ratios = np.linspace(0, 0.99, 100)\n        \n        # Initialize distributions for all machines\n        self.n_pulls = [0 for _ in range(n_machines)]\n        self.dist = [initial_dist for m_index in range(n_machines)]\n        self.cum_dist = [self.updateCumDist(m_index)\n                         for m_index in range(n_machines)]\n        \n        # Track winnings!\n        self.last_reward_count = 0\n\n    def __call__(self):\n        \"\"\"Choose machine based on maximum Monte-Carlo return\n\n        Returns:\n           <result> (int):  index of machine to pull\n        \n        \"\"\"\n        # Select machine with highest return on limited Monte Carlo\n        est_return = np.array([self.estimatedReturn(m_index)\n                               for m_index in range(self.n_machines)])\n        return int(np.argmax(est_return))\n\n    def samplePayoutRatio(self, m_index):\n        \"\"\"Pull a weighted sample from the distribution\"\"\"\n        x = random.random()\n        return self.p_ratios[np.where(x <= self.cum_dist[m_index])[0][0]]\n\n    def estimatedReturn(self, m_index):\n        \"\"\"Expected return from a Monte-Carlo sample of payout ratios\"\"\"\n        n_pulls = self.n_pulls[m_index]\n        est_p = sum([self.samplePayoutRatio(m_index)\n                     for ii in range(self.ev_rounds)])\n        return est_p \/ self.ev_rounds * 0.97**n_pulls\n        \n    def updateDist(self, curr_total_reward, last_m_indices):\n        \"\"\"Updates estimated distribution of payouts\"\"\"\n        # Compute last reward\n        last_reward = curr_total_reward - self.last_reward_count\n        self.last_reward_count = curr_total_reward\n\n        if len(last_m_indices) == 2:\n            # Update number of pulls for both machines\n            self.n_pulls[last_m_indices[0]] += 1\n            self.n_pulls[last_m_indices[1]] += 1\n\n            # Update estimated probabilities for this agent's pull\n            m_index = last_m_indices[self.agent_num]\n            n_pulls = self.n_pulls[m_index]\n            if last_reward == 1:\n                curr_prob = self.p_ratios * 0.97**n_pulls\n            else:\n                curr_prob = (1 - self.p_ratios * 0.97**n_pulls)\n\n            self.dist[m_index] = curr_prob * self.dist[m_index]\n            self.dist[m_index] = self.dist[m_index] \/ self.dist[m_index].sum()\n            self.cum_dist[m_index] = self.updateCumDist(m_index)\n\n    def updateCumDist(self, m_index):\n        \"\"\"Updates cumulative payout ratio distribution\"\"\"\n        return np.cumsum(self.dist[m_index])\n\n\n# DEFINE AGENT ----------------------------------------------------------------\n\ndef agent(observation, configuration):\n    global curr_agent\n    \n    if observation.step == 0:\n        # Initialize agent\n        curr_agent = MonteCarloStragegy(\n            'Mr. Agent %i' % observation['agentIndex'],\n            observation['agentIndex'],\n            PRIOR_DISTRIBUTION,\n            NUM_SAMPLES,\n            configuration['banditCount'])\n    \n    # Update payout ratio distribution with:\n    # - which machines were pulled by both players\n    # - result from previous pull\n    curr_agent.updateDist(observation['reward'], observation['lastActions'])\n\n    return curr_agent()\n    \n# -----------------------------------------------------------------------------\n","23bc4436":"%%writefile random.py\n\n\"\"\"Implements an agent that selects machines randomly\n\n\"\"\"\nimport random\n\ndef agent(observation, configuration):\n    machine = random.randint(0, configuration['banditCount'] - 1)\n    return machine","6e767a54":"!pip install kaggle-environments --upgrade -q\nfrom kaggle_environments import make\n\nenv = make(\"mab\", debug=True)","bff313ae":"\nenv.reset()\nenv.configuration['episodeSteps'] = 1\nresult = env.run(['submission.py', 'random.py'])\n#env.render(mode=\"ipython\", width=800, height=500)\n\nprint('-------------------')\nprint('FINAL RESULTS')\nprint('-------------------')\nprint('Agent 0: %i rewards' % result[-1][0]['reward'])\nprint('Agent 1: %i rewards' % result[-1][1]['reward'])\nif result[-1][0]['reward'] > result[-1][1]['reward']:\n    print('\\nAgent 0 is the winner!!!')\nelif result[-1][0]['reward'] < result[-1][1]['reward']:\n    print('\\nAgent 1 is the winner!!!')\nelse:\n    print('\\nIts a tie!!!')\n","b2c7d16c":"# Simulation","394fe36a":"# Write Submission File","e7c085d6":"# Random Agent"}}