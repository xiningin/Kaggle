{"cell_type":{"0a9f0390":"code","a4266943":"code","ded7e559":"code","2a500658":"code","2cb38a5d":"code","84ebd07d":"code","0f5ac35c":"code","c18fb3f3":"code","57b8f65c":"code","53cdb7a4":"code","65ed6e3b":"code","d9c0d275":"code","1bf6d879":"code","ce3fcc53":"code","5d2a7671":"code","db6adede":"code","0be44b49":"code","f460eaa3":"code","2f874a11":"code","c539b96d":"code","b3ce0e50":"code","eb63abc1":"code","c3108afb":"code","6d82c3cb":"code","a72e7b19":"code","39f775df":"code","0bd106d5":"code","f3f0b746":"code","f744500f":"code","3b743cbb":"code","48b9474f":"code","2c4501e3":"code","9ab1e7fc":"code","d513ddcb":"code","6a0d0bdb":"code","4360e329":"code","b5626a8b":"code","4c2183b4":"code","d2b94ba6":"code","36a13372":"code","e3b6e50a":"code","0ae9c39d":"code","27fd4b28":"code","265f065f":"code","52160d23":"code","8aead6af":"code","1c5c5488":"code","a50cf3b6":"code","38695f33":"code","a4abba59":"code","2adf1762":"code","423fccbe":"code","b66665df":"code","caee2d09":"code","e53c2b9d":"code","20b00f83":"code","557d4ae4":"code","86413b3e":"code","2d4238e1":"code","9f4e8bb7":"code","abbb59ff":"code","e4093a8b":"markdown","1b84320f":"markdown","6686a402":"markdown","57dc9e12":"markdown","8727db24":"markdown","f8874ae4":"markdown","d419994b":"markdown","e3fb7aeb":"markdown","b2113057":"markdown","df75b84f":"markdown","c56f74d0":"markdown","ad9c2836":"markdown","a299b998":"markdown","3dcf66d3":"markdown"},"source":{"0a9f0390":"import tensorflow as tf\nimport pandas as pd\nimport  numpy as np\ntf.config.run_functions_eagerly(True)","a4266943":"dataset=pd.read_csv('..\/input\/emotions-dataset-for-nlp\/train.txt',sep=';',names=['sentence','emotion'])","ded7e559":"test_dataset=pd.read_csv('..\/input\/emotions-dataset-for-nlp\/test.txt',sep=';',names=['sentence','emotion'])","2a500658":"val_dataset=pd.read_csv('..\/input\/emotions-dataset-for-nlp\/val.txt',sep=';',names=['sentence','emotion'])","2cb38a5d":"dataset.head()","84ebd07d":"dataset.tail()","0f5ac35c":"dataset.isnull().sum()","c18fb3f3":"import random \nfor i in range(10):\n  index=random.randint(0,len(dataset)-2)\n  print(f\"sentence is '{dataset['sentence'][index]}' and emotion is : {dataset['emotion'][index]}\")\n","57b8f65c":"dataset.describe()","53cdb7a4":"dataset.groupby(['emotion']).describe()","65ed6e3b":"possible_labels = dataset.emotion.unique()\n\nlabel_dict = {}\nfor index, possible_label in enumerate(possible_labels):\n    label_dict[possible_label] = index\nlabel_dict","d9c0d275":"dataset.emotion = dataset.emotion.replace(label_dict)","1bf6d879":"val_dataset.emotion = val_dataset.emotion.replace(label_dict)","ce3fcc53":"test_dataset.emotion=test_dataset.emotion.replace(label_dict)","5d2a7671":"dataset.head()","db6adede":"val_dataset.head()","0be44b49":"train_sentence=dataset['sentence']\ntrain_labels=dataset['emotion']\n","f460eaa3":"val_sentence=val_dataset['sentence']\nval_label=val_dataset['emotion']","2f874a11":"test_sentence=test_dataset['sentence']\ntest_label=test_dataset['emotion']","c539b96d":"train_labels_onehot=tf.one_hot(train_labels,depth=6)","b3ce0e50":"val_label_onehot=tf.one_hot(val_label,depth=6)","eb63abc1":"test_label_onehot=tf.one_hot(test_label,depth=6)","c3108afb":"round(sum([len(i.split()) for i in train_sentence])\/len(train_sentence))","6d82c3cb":"import tensorflow as tf\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization","a72e7b19":"max_vocab_length = 10000 # max number of words to have in our vocabulary\nmax_length = 19 # max length our sequences will be (e.g. how many words from a Tweet does our model see?)\n\ntext_vectorizer = TextVectorization(max_tokens=max_vocab_length,\n                                    output_mode=\"int\",\n                                    output_sequence_length=max_length)","39f775df":"text_vectorizer.adapt(train_sentence)","0bd106d5":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\n\n# Create tokenization and modelling pipeline\nmodel_0 = Pipeline([\n                    (\"tfidf\", TfidfVectorizer()), # convert words to numbers using tfidf\n                    (\"clf\", MultinomialNB()) # model the text\n])\n\n# Fit the pipeline to the training data\nmodel_0.fit(train_sentence, train_labels)","f3f0b746":"baseline_score = model_0.score(val_sentence, val_label)\nprint(f\"Our baseline model achieves an accuracy of: {baseline_score*100:.2f}%\")","f744500f":"baseline_score = model_0.score(test_sentence, test_label)\nprint(f\"Our baseline model achieves an accuracy of: {baseline_score*100:.2f}%\")","3b743cbb":"# Set random seed and create embedding layer (new embedding layer for each model)\ntf.random.set_seed(42)\nfrom tensorflow.keras import layers\nmodel_2_embedding = layers.Embedding(input_dim=max_vocab_length,\n                                     output_dim=128,\n                                     embeddings_initializer=\"uniform\",\n                                     input_length=max_length,\n                                     name=\"embedding_2\")\n\n\n# Create LSTM model\ninputs = layers.Input(shape=(1,), dtype=\"string\")\nx = text_vectorizer(inputs)\nx = model_2_embedding(x)\n\n\nx = layers.LSTM(64)(x) # return vector for whole sequence\n\n\noutputs = layers.Dense(6, activation=\"softmax\")(x)\nmodel_2 = tf.keras.Model(inputs, outputs, name=\"model_2_LSTM\")","48b9474f":"model_2.summary()","2c4501e3":"# Compile model\nmodel_2.compile(loss=\"CategoricalCrossentropy\",\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[\"accuracy\"])\n","9ab1e7fc":"model_2_history = model_2.fit(train_sentence,\n                              train_labels_onehot,\n                              epochs=5,\n                              validation_data=(val_sentence, val_label_onehot))","d513ddcb":"import matplotlib.pyplot as plt\nplt.plot(model_2_history.history['loss'],label='train_loss')\nplt.plot(model_2_history.history['val_loss'],label='val_loss')\nplt.plot(model_2_history.history['accuracy'],label='train_acc')\nplt.plot(model_2_history.history['val_accuracy'],label='val_acc')\nplt.legend()","6a0d0bdb":"# Set random seed and create embedding layer (new embedding layer for each model)\ntf.random.set_seed(42)\nfrom tensorflow.keras import layers\nmodel_3_embedding = layers.Embedding(input_dim=max_vocab_length,\n                                     output_dim=128,\n                                     embeddings_initializer=\"uniform\",\n                                     input_length=max_length,\n                                     name=\"embedding_2\")\n\n\n# Create LSTM model\ninputs = layers.Input(shape=(1,), dtype=\"string\")\nx = text_vectorizer(inputs)\nx = model_2_embedding(x)\n\n\nx = tf.keras.layers.Bidirectional(layers.LSTM(64))(x) # return vector for whole sequence\n\n\noutputs = layers.Dense(6, activation=\"softmax\")(x)\nmodel_3 = tf.keras.Model(inputs, outputs, name=\"model_2_LSTM\")","4360e329":"# Compile model\nmodel_3.compile(loss=\"CategoricalCrossentropy\",\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[\"accuracy\"])\n","b5626a8b":"model_3_history = model_3.fit(train_sentence,\n                              train_labels_onehot,\n                              epochs=5,\n                              validation_data=(val_sentence, val_label_onehot))","4c2183b4":"import matplotlib.pyplot as plt\nplt.plot(model_3_history.history['loss'],label='train_loss')\nplt.plot(model_3_history.history['val_loss'],label='val_loss')\nplt.plot(model_3_history.history['accuracy'],label='train_acc')\nplt.plot(model_3_history.history['val_accuracy'],label='val_acc')\nplt.legend()","d2b94ba6":"# Set random seed and create embedding layer (new embedding layer for each model)\ntf.random.set_seed(42)\nfrom tensorflow.keras import layers\nmodel_4_embedding = layers.Embedding(input_dim=max_vocab_length,\n                                     output_dim=128,\n                                     embeddings_initializer=\"uniform\",\n                                     input_length=max_length,\n                                     name=\"embedding_2\")\n\n\n# Create LSTM model\ninputs = layers.Input(shape=(1,), dtype=\"string\")\nx = text_vectorizer(inputs)\nx = model_4_embedding(x)\n\n\nx = tf.keras.layers.Bidirectional(layers.LSTM(32,return_sequences=True))(x) # return vector for whole sequence\nx= tf.keras.layers.Bidirectional(layers.LSTM(64))(x)\n\noutputs = layers.Dense(6, activation=\"softmax\")(x)\nmodel_4 = tf.keras.Model(inputs, outputs, name=\"model_2_LSTM\")","36a13372":"# Compile model\nmodel_4.compile(loss=\"CategoricalCrossentropy\",\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[\"accuracy\"])\n","e3b6e50a":"model_4_history = model_4.fit(train_sentence,\n                              train_labels_onehot,\n                              epochs=5,\n                              validation_data=(val_sentence, val_label_onehot))","0ae9c39d":"import matplotlib.pyplot as plt\nplt.plot(model_4_history.history['loss'],label='train_loss')\nplt.plot(model_4_history.history['val_loss'],label='val_loss')\nplt.plot(model_4_history.history['accuracy'],label='train_acc')\nplt.plot(model_4_history.history['val_accuracy'],label='val_acc')\nplt.legend()","27fd4b28":"# Set random seed and create embedding layer (new embedding layer for each model)\ntf.random.set_seed(42)\nfrom tensorflow.keras import layers\nmodel_5_embedding = layers.Embedding(input_dim=max_vocab_length,\n                                     output_dim=128,\n                                     embeddings_initializer=\"uniform\",\n                                     input_length=max_length,\n                                     name=\"embedding_2\")\n\n\n# Create LSTM model\ninputs = layers.Input(shape=(1,), dtype=\"string\")\nx = text_vectorizer(inputs)\nx = model_5_embedding(x)\n\n\nx = tf.keras.layers.Bidirectional(layers.GRU(8,return_sequences=True))(x) # return vector for whole sequence\nx= tf.keras.layers.Bidirectional(layers.LSTM(32))(x)\n\noutputs = layers.Dense(6, activation=\"softmax\")(x)\nmodel_5 = tf.keras.Model(inputs, outputs, name=\"model_2_LSTM\")","265f065f":"# Compile model\nmodel_5.compile(loss=\"CategoricalCrossentropy\",\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[\"accuracy\"])\n","52160d23":"model_5_history = model_5.fit(train_sentence,\n                              train_labels_onehot,\n                              epochs=5,\n                              validation_data=(val_sentence, val_label_onehot))","8aead6af":"import matplotlib.pyplot as plt\nplt.plot(model_5_history.history['loss'],label='train_loss')\nplt.plot(model_5_history.history['val_loss'],label='val_loss')\nplt.plot(model_5_history.history['accuracy'],label='train_acc')\nplt.plot(model_5_history.history['val_accuracy'],label='val_acc')\nplt.legend()","1c5c5488":"# Set random seed and create embedding layer (new embedding layer for each model)\ntf.random.set_seed(42)\nfrom tensorflow.keras import layers\nmodel_6_embedding = layers.Embedding(input_dim=max_vocab_length,\n                                     output_dim=128,\n                                     embeddings_initializer=\"uniform\",\n                                     input_length=max_length,\n                                     name=\"embedding_2\")\n\n\n# Create LSTM model\ninputs = layers.Input(shape=(1,), dtype=\"string\")\nx = text_vectorizer(inputs)\nx = model_6_embedding(x)\n\n\nx = tf.keras.layers.Bidirectional(layers.GRU(6,return_sequences=True))(x) # return vector for whole sequence\nx= tf.keras.layers.Bidirectional(layers.LSTM(6))(x)\n\noutputs = layers.Dense(6, activation=\"softmax\")(x)\nmodel_6 = tf.keras.Model(inputs, outputs, name=\"model_2_LSTM\")","a50cf3b6":"# Compile model\nmodel_6.compile(loss=\"CategoricalCrossentropy\",\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[\"accuracy\"])\n","38695f33":"model_6_history = model_6.fit(train_sentence,\n                              train_labels_onehot,\n                              epochs=5,\n                              validation_data=(val_sentence, val_label_onehot))","a4abba59":"import matplotlib.pyplot as plt\nplt.plot(model_6_history.history['loss'],label='train_loss')\nplt.plot(model_6_history.history['val_loss'],label='val_loss')\nplt.plot(model_6_history.history['accuracy'],label='train_acc')\nplt.plot(model_6_history.history['val_accuracy'],label='val_acc')\nplt.legend()","2adf1762":"model_0.score(test_sentence,test_label)","423fccbe":"model_2.evaluate(test_sentence,test_label_onehot)","b66665df":"model_3.evaluate(test_sentence,test_label_onehot)","caee2d09":"model_4.evaluate(test_sentence,test_label_onehot)","e53c2b9d":"model_5.evaluate(test_sentence,test_label_onehot)","20b00f83":"model_6.evaluate(test_sentence,test_label_onehot)","557d4ae4":"# Set random seed and create embedding layer (new embedding layer for each model)\ntf.random.set_seed(42)\nfrom tensorflow.keras import layers\nmodel_7_embedding = layers.Embedding(input_dim=max_vocab_length,\n                                     output_dim=128,\n                                     embeddings_initializer=\"uniform\",\n                                     input_length=max_length,\n                                     name=\"embedding_2\")\n\n\n# Create LSTM model\ninputs = layers.Input(shape=(1,), dtype=\"string\")\nx = text_vectorizer(inputs)\nx = model_7_embedding(x)\n\n\nx = tf.keras.layers.Bidirectional(layers.GRU(6))(x) # return vector for whole sequence\n\n\noutputs = layers.Dense(6, activation=\"softmax\")(x)\nmodel_7 = tf.keras.Model(inputs, outputs, name=\"model_2_LSTM\")","86413b3e":"# Compile model\nmodel_7.compile(loss=\"CategoricalCrossentropy\",\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[\"accuracy\"])\n","2d4238e1":"model_7_history = model_7.fit(train_sentence,\n                              train_labels_onehot,\n                              epochs=5,\n                              validation_data=(val_sentence, val_label_onehot))","9f4e8bb7":"import matplotlib.pyplot as plt\nplt.plot(model_7_history.history['loss'],label='train_loss')\nplt.plot(model_7_history.history['val_loss'],label='val_loss')\nplt.plot(model_7_history.history['accuracy'],label='train_acc')\nplt.plot(model_7_history.history['val_accuracy'],label='val_acc')\nplt.legend()","abbb59ff":"model_7.evaluate(test_sentence,test_label_onehot)","e4093a8b":"> ****Becoming onw with the data","1b84320f":"Seeing random sentence and its label","6686a402":"Applying one hot encoding ","57dc9e12":"There is no missing data!!:)","8727db24":"> **visualizing data **\n","f8874ae4":"Now we will make our model less complex as our model is overfiting we can clearly see form val loss","d419994b":"> **Making a baseline model**","e3fb7aeb":"We will try to imporve the accuracy above the baseline model.....\nmodel 2: trying hello world of LSTM model","b2113057":"The last model performs the best which  is less complex.","df75b84f":"As we can see val loss is increasing after 3 epoch that means our model is overfitting....but we will try with several model let's see","c56f74d0":"That does not worked well but it outperform more complex model that was previously trained.\nour best model will be model 6 which is also less comples than previous ones.\nHope you like it please give a thumbs up","ad9c2836":"Replacing all categorical label with numberical ones","a299b998":"Giving last shot with least complex model ","3dcf66d3":"getting avg length of sentence to use it for max length for input sequence"}}