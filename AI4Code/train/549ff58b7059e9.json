{"cell_type":{"cb3b994a":"code","28dba27a":"code","0751abe5":"code","2c6762ba":"code","69a7e3e5":"code","70ab3cd1":"code","e9c898e3":"code","88671b31":"code","fc3306b4":"code","bfae967f":"code","e19c4f83":"code","d1c73016":"code","78fa0902":"code","1fb2fe87":"code","b6f7eb98":"code","b62c9266":"code","5adfda9e":"code","0341d520":"code","5381bde3":"code","f475d84d":"code","691ce920":"code","3485e077":"code","7cf43a92":"code","bab197da":"code","3278e11a":"code","2633a47d":"code","3d97b176":"code","2c6afb6c":"code","e847e49f":"code","ac49a58b":"code","cc4debbd":"code","4466f731":"code","76f4193b":"markdown","69e6fadc":"markdown","b740065a":"markdown","f6da34da":"markdown","686a3b08":"markdown","bf8b180f":"markdown","a9104f2f":"markdown","17fd3ff7":"markdown","aaee8f06":"markdown","8d56e1b6":"markdown","33344556":"markdown","ebe65d86":"markdown","0cad999b":"markdown","29c16995":"markdown","e0fa46b4":"markdown","c4d326b0":"markdown","c32d6f44":"markdown","975fa5d3":"markdown","7b75a418":"markdown","3e45829a":"markdown","e5b3123e":"markdown","94a5123f":"markdown","9494503b":"markdown","58a70e79":"markdown","6d7a14f4":"markdown","007ab98a":"markdown","1f044fdd":"markdown","a44b50ee":"markdown","087ffe32":"markdown","6869bda6":"markdown","168d4055":"markdown","3b023ed2":"markdown","e69828e6":"markdown","9133641c":"markdown","b5061f01":"markdown","68a9f55a":"markdown","7706662b":"markdown","65e0c411":"markdown"},"source":{"cb3b994a":"!pip install -q datascience                                                       # Package that is required by pandas profiling\n!pip install -q pandas-profiling  ","28dba27a":"!pip install -q --upgrade pandas-profiling","0751abe5":"#-------------------------------------------------------------------------------------------------------------------------------\nimport pandas as pd                                                               # Importing for panel data analysis\nfrom pandas_profiling import ProfileReport                                        # Import Pandas Profiling (To generate Univariate Analysis) \npd.set_option('display.max_columns', None)                                        # Unfolding hidden features if the cardinality is high      \npd.set_option('display.max_colwidth', None)                                       # Unfolding the max feature width for better clearity      \npd.set_option('display.max_rows', None)                                           # Unfolding hidden data points if the cardinality is high\npd.set_option('mode.chained_assignment', None)                                    # Removing restriction over chained assignments operations\n#-------------------------------------------------------------------------------------------------------------------------------\nimport numpy as np                                                                # Importing package numpys (For Numerical Python)\nimport string                                                                     # For string related operations\nimport pprint                                                                     # For printing of Collections line by line\nfrom collections import Counter                                                   # For estimating frequency\n#-------------------------------------------------------------------------------------------------------------------------------\nimport matplotlib.pyplot as plt                                                   # Importing pyplot interface using matplotlib                                              \nimport seaborn as sns                                                             # Importing seaborm library for interactive visualization\n%matplotlib inline\n#-------------------------------------------------------------------------------------------------------------------------------\nfrom sklearn.metrics import precision_recall_curve,accuracy_score                 # For precision and recall metric estimation\nfrom sklearn.metrics import classification_report                                 # To generate complete report of evaluation metrics\nfrom sklearn.metrics import plot_confusion_matrix,roc_auc_score                   # To plot confusion matrix \n#-------------------------------------------------------------------------------------------------------------------------------\nfrom sklearn.model_selection import train_test_split                              # To split the data in training and testing part \nfrom sklearn.preprocessing import StandardScaler                                  # Importing Standard Scaler library from preprocessing.     \nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier             # Loading differernt ensemble models\nfrom sklearn.ensemble import AdaBoostClassifier,BaggingClassifier                 # Loading differernt ensemble models\nfrom sklearn.tree import DecisionTreeClassifier                                   # Loading decision tree classifier from tree\nfrom sklearn.svm import SVC                                                       # Loading SVM from  Sklearn\nfrom sklearn.linear_model import LogisticRegression                               # Loading Logistic regression from linear model\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB                            \nfrom sklearn.model_selection import cross_val_score                               # Importing cross validation score from model selection\nfrom sklearn import model_selection\nfrom mlxtend.classifier import StackingClassifier\n#-------------------------------------------------------------------------------------------------------------------------------\nimport warnings                                                                   # Importing warning to disable runtime warnings\nwarnings.filterwarnings(\"ignore\")     ","2c6762ba":" data = pd.read_csv(filepath_or_buffer='https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/breast-cancer-wisconsin\/breast-cancer-wisconsin.data',\n                    names=['Sample code number', 'Clump Thickness','Uniformity of Cell Size', 'Uniformity of Cell Shape', 'Marginal Adhesion', \n                           'Single Epithelial Cell Size','Bare Nuclei','Bland Chromatin', 'Normal Nucleoli', 'Mitosis','Class'])\nprint('Data Shape:', data.shape)\ndata.head()","69a7e3e5":"data.describe()","70ab3cd1":"data.info()","e9c898e3":"# profile = ProfileReport(df=data)\n# profile.to_file(output_file='Pre Profiling Report.html')\n# print('Accomplished!')","88671b31":"# from google.colab import files                   # Use only if you are using Google Colab, otherwise remove it\n# files.download('Pre Profiling Report.html')      # Use only if you are using Google Colab, otherwise remove it","fc3306b4":"def drop_duplicate_data(data):\n  before_shape = data.shape\n  print('Data Shape [Before]:', before_shape)\n\n  data.drop_duplicates(inplace = True)\n\n  after_shape = data.shape\n  print('Data Shape [After]:', after_shape)\n\n  drop_nums = before_shape[0] - after_shape[0]\n  drop_percent = np.round(drop_nums \/ before_shape[0], decimals = 3) * 100\n\n  print('Drop Ratio:', drop_percent, '%')","bfae967f":"drop_duplicate_data(data)","e19c4f83":"def drop_features():\n  data.drop(columns=['Sample code number'], inplace=True)","d1c73016":"drop_features()\ndata.head()","78fa0902":"def replace_unwanted():\n    data.replace('?','0',inplace=True)\n    print(data['Bare Nuclei'].unique())\n","1fb2fe87":"replace_unwanted()","b6f7eb98":"def plot_class(data):\n   plt.figure(figsize = (15, 7))\n   malignant = len(data[data['Class']==4])\n   benign = len(data[data['Class']==2])\n\n   ax=sns.countplot(x = 'Class', data=data)\n   ax.set_title('Number of Cases by Class', size=16)\n   ax.text(x = 0, y = 200, s = benign, size=14, \n         color = 'black', horizontalalignment='center')\n   ax.text(x = 1, y = 100, s = malignant, size=14,\n        color='black', horizontalalignment='center')\n   plt.show()","b62c9266":"plot_class(data)","5adfda9e":"def plot_corr():\n  correlation_matrix = data.corr()\n  fig = plt.figure(figsize=(10, 7))\n  sns.heatmap(correlation_matrix)\n  plt.show()","0341d520":"plot_corr()","5381bde3":"def data_prep(data):\n  X=data.drop(columns=['Class'])\n  y=data['Class']\n  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n  return X_train,X_test,y_train, y_test, X, y","f475d84d":"X_train, X_test, y_train, y_test, X, y = data_prep(data)","691ce920":"def scale_data(X_train,X_test):\n    sc = StandardScaler()\n    X_train = sc.fit_transform(X_train)\n    X_test = sc.transform(X_test)\n    return X_train,X_test","3485e077":"X_train,X_test = scale_data(X_train,X_test)","7cf43a92":"def ensemble_log():\n  bagClf = BaggingClassifier(LogisticRegression(), n_estimators=500, random_state=42)\n  bagClf.fit(X_train, y_train)\n  return bagClf","bab197da":"def random_forest():\n    rfm = RandomForestClassifier(n_estimators=100, random_state=42)\n    rfm.fit(X_train, y_train)\n    return rfm","3278e11a":"def adaBoost():\n    abm = AdaBoostClassifier(n_estimators=70, random_state=42)\n    abm.fit(X_train, y_train)\n    return abm","2633a47d":"def votingClass():\n  #create the sub models\n    estimators =[]\n\n    model1 = LogisticRegression()\n    estimators.append(('logistic',model1))\n\n    model2 = DecisionTreeClassifier()\n    estimators.append(('cart', model2))\n\n    model3 = SVC()\n    estimators.append(('svm', model3))\n\n    Vm = VotingClassifier(estimators)\n    Vm.fit(X_train, y_train)\n    return Vm","3d97b176":"def predict_scores():\n  # Intialize a list of classifier objects\n  clf_list = [ensemble_log(),random_forest(),adaBoost(), votingClass()]\n\n  # Create an empty list to append scores and classifier name\n  train_scores = []\n  test_scores = []\n  clf_names = ['BaggingClassifier','RandomForest','AdaBoost','VotingClassifier']\n\n  # Train classifier over train data and append scores to empty list\n  for clf in clf_list:\n    # Fit the train data over the classifier object\n    clf.fit(X_train, y_train)\n\n  # Append train and test score to the empty list\n  train_scores.append(np.round(a=clf.score(X_train, y_train), decimals=2))\n  test_scores.append(np.round(a=clf.score(X_test, y_test), decimals=2))\n  return   train_scores, test_scores, clf_names","2c6afb6c":" train_scores, test_scores, clf_names = predict_scores()","e847e49f":"def create_frame():\n  # Create an accuracy dataframe from scores and names list\n  accuracy_frame = pd.DataFrame(data={'Train Accuracy': train_scores, 'Test Accuracy': test_scores}, index=clf_names)\n\n  # View the accuracy of all the classifiers\n  accuracy_frame = accuracy_frame.transpose()\n  return accuracy_frame","ac49a58b":"create_frame()","cc4debbd":"def stacking_clf(X,y):\n  clf1 = KNeighborsClassifier(n_neighbors=1)\n  clf2 = RandomForestClassifier(random_state=1)\n  clf3 = GaussianNB()\n  lr=LogisticRegression()\n  sclf = StackingClassifier(classifiers=[clf1, clf2, clf3], \n                          meta_classifier=lr)\n  for clf, label in zip([clf1, clf2, clf3, sclf], \n                      ['KNN', \n                       'Random Forest', \n                       'Naive Bayes',\n                       'StackingClassifier']):\n     scores = model_selection.cross_val_score(clf, X, y, \n                                              cv=3, scoring='accuracy')\n     print(\"Accuracy: %0.2f  [%s]\" \n          % (scores.mean(), label))","4466f731":"stacking_clf(X,y)","76f4193b":"**Observations**\n\n- **25%** of the cancer cell have **Clump Thickness** less than equal to **2**.\n\n- **50%** of the cancer cell have **Clump Thickness** less than equal to **4**.\n\n- **75%** of the cancer cell have **Clump Thickness** less than equal to **6**.\n\n- **25%** of the cancer cell have **Bland Chromatin** less than equal to **2**.\n\n- **50%** of the cancer cell have **Bland Chromatin** less than equal to **3**.\n\n- **75%** of the cancer cell have **Bland Chromatin** less than equal to **5**.\n\n- Similarly we can understand  the statistical inference for other features.","69e6fadc":"<a name = Section41><\/a>\n### **2.1 Data Description**\n\n- In this section we will get **description** and **statistics** about the data.","b740065a":"---\n**<h4>Question 5:** Prepare the  data according to the model requirements.<\/h4>\n\n---\n- Write a function  data_prep() to structure the  data according to model requirement.\n\n- **Split** the data into **80:20** inside train_test_split.\n\n- Make sure to set the **random_state = 42**.\n","f6da34da":"<a name = Section31><\/a>\n### **3.1 Pre Profiling Report**","686a3b08":"**Observation:**\n- We can see that stacking classifier gives us pretty **good accuracy** scores the other models.","bf8b180f":"---\n**<h4>Question 1:** Create a function that drops the redundant rows and displays the frequency of final rows.<\/h4>\n\n---\n- You can plot the frequency distribution and proportion using seaborn countplot function.","a9104f2f":"<a name = Section=11><\/a>\n### **1.1 Installing Libraries**","17fd3ff7":"---\n**<h4>Question 4:** Create a function that displays the correlation between different features.<\/h4>\n\n---\n\n- Write a function  plot_corr() function to create a heatmap of size (10, 7).\n\n- Use .corr() function to find the correlation between the features in the data.\n\n- Use seaborn heatmap function to plot the following.","aaee8f06":"**Observation:**\n- We can see that **RandomForest** model has the **best accuracy** scores among the models.\n\n- We can see that all models generalize well on the test set without overfitting.","8d56e1b6":"<a name = Section9><\/a>\n\n---\n# **7. Conclusion**\n---\n","33344556":"**Observation:**\n- We can see that the data is quite **imbalanced**.\n\n- We will resolve this issue in our further analysis.","ebe65d86":"<a name = Section3><\/a>\n\n---\n# **3. Data Pre-Processing**\n---","0cad999b":"---\n**<h4>Question 8:** Create a function to  instantiate RandomForest Classifier.<\/h4>\n\n---\n\n- You have to use the hyperparameters setting of the model as following:\n  - `n_neghbors` = 100 ,and\n  - `random_state` = 42,  ","29c16995":"---\n**<h4>Question 11:** Create a function to predict accuracy score in both train and test dataset using different models.<\/h4>\n\n---\n- Create a list with different models and use for-loop to predict on train and test set.\n- Store the result of train and test scores in different list. ","e0fa46b4":"**Observation**\n- We can see that there is **1 features** with **object** datatype and **10 features** as **int** datatype.","c4d326b0":"---\n**<h4>Question 2:** Create a function that drops **Sample code number** feature  from the dataset.<\/h4>\n\n---","c32d6f44":"---\n**<h4>Question 13:** Create a function to stack different models to create a stacking classifier.<\/h4>\n\n---\n\n- Use StackingClassifier from **mlxtend** to create the model.\n\n- Build stacking classifier using **KNN**, **RadomForest** and **Naive Bayes**, use **LogisticRegression** as meta_classifier.\n\n- Use cross validation score to predict on the entire dataset with following parameters:\n  - `estimator` : clf (stacked models)\n  - `X` : X\n  - `y` : y\n  - `cv` : 3\n  - `scoring` : 'accuracy'\n\n- Note that use the whole dataset.\n\n- Print accuracy for different models with the model name.\n","975fa5d3":"---\n**<h4>Question 6:** Create a function to normalize the data.<\/h4>\n\n---\n\n- Using the StandardScaler() function  transform(scale) the data in suitable format for the model.\n","7b75a418":"---\n**<h4>Question 9:** Create a function to  instantiate AdaBoost Classifier.<\/h4>\n\n---\n\n- You have to use the hyperparameters setting of the model as following:\n  - `n_neghbors` = 70 , and\n  - `random_state` = 42,  ","3e45829a":"**Observation:**\n- We can observe that their is a unique feature that uniquely identifies each data point.\n\n- We can drop this feature because it won't provide any necessary infomation.\n","e5b3123e":"---\n**<h4>Question 12:** Create a function to create a dataframe to compare the scores of different models.<\/h4>\n\n---\n\n- Using pandas **DataFrame** method create a data frame to compare accuracy score of different models.\n\n- Use **transpose** method to make the data frame more readable.","94a5123f":"<a name = Section12><\/a>\n### **1.2 Upgrading Libraries**\n\n- **After upgrading** the libraries, you need to **restart the runtime** to make the libraries in sync. \n\n- Make sure not to execute the cell above (3.1) and below (3.2) again after restarting the runtime.\n\n","9494503b":"- We **analyzed** and understood different **factors** to classify if the tumor is benign or malignant. \n\n- We approached this problem using build **different** ensemble methods where the **voting classifer** and **random forest** gave us the highest accuracy scores.\n\n- We can also try experimenting with various **hyperparameter** to improve model accuracy.\n\n- Their are plethora of other ensemble techniques you can try on to build a better model with better accuracy score. ","58a70e79":"<a name = Section8><\/a>\n\n---\n# **6. Model Development & Evaluation**\n---\n\n- In this section we will develope different **ensemble methods**.\n\n- Then we will **analyze the results** obtained and **make our observation**.\n\n- For **evaluation purpose** we will **focus** on **accuracy score**.\n\n- **Remember** that **we want generalize results** i.e. same results or error on testing data as that of training data.","6d7a14f4":"<a name = Section7><\/a>\n\n---\n# **5. Post Data Processing & Feature Selection**\n---\n","007ab98a":"---\n<a name = Section2><\/a>\n# **2. Data Acquisition & Description**\n---\n- This corpus has been collected from free research sources on the Internet and it can be retrieved from the attached <a href = \"https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/breast-cancer-wisconsin\/breast-cancer-wisconsin.data\">**link**<\/a>.\n\n| Records | Features | Dataset Size |\n| :-- | :-- | :-- |\n| \t699 | 11 | \t88.2 B| \n\n|Id|Feature|Description|\n|:--|:--|:--|\n|01|**Sample code number**| Unique code for each sample.|\n|02|**Clump Thickness**|Benign cells tend to be grouped in monolayers, while cancerous cells are often grouped in multilayers.|\n|03|**Uniformity of Cell Size**|Cancer cells tend to vary in size and shape.|\n|04|**Uniformity of Cell Shape**| Cancer cells tend to vary in size and shape.|\n|05|**Marginal Adhesion**|Normal cells tend to stick together. Cancer cells tend to lose this ability. So the loss of adhesion is a sign of malignancy.|\n|06|**Single Epithelial Cell Size**| Epithelial cells that are significantly enlarged may be a malignant cell.|\n|07|**Bare Nuclei**|This is a term used for nuclei not surrounded by cytoplasm (the rest of the cell). Those are typically seen in benign tumors.|\n|08|**Bland Chromatin**|Describes a uniform \"texture\" of the nucleus seen in benign cells. In cancer cells, the chromatin tends to be more coarse.|\n|08|**Normal Nucleoli**|In cancer cells, the nucleoli become more prominent, and sometimes there are more of them.|\n|09|**Mitosis**|Cancer is essentially a disease of uncontrolled mitosis.|\n|10|**Class**| Benign (non-cancerous) or malignant (cancerous) lump in a breast.|\n","1f044fdd":"<a name = Section13><\/a>\n### **1.3 Importing Libraries**","a44b50ee":"**Observation**\n- We can see that `Mitosis` column is **negatively correlated** with `clump Thickness` and `Bland Chromatin` columns.\n\n- We can also see that `uniformity of Cell Shape` is **higly correlated** with `uniformity of Cell Size`.","087ffe32":"---\n<a name = Section1><\/a>\n# **1. Installing & Importing Libraries**\n---","6869bda6":"---\n**<h4>Question 6:** Create a function to replace '?' in 'Bare Nuclei' with 0 ? <\/h4>\n\n---\n\n- Print the unique values in 'Bare Nuclei' after the opreation is performed.","168d4055":"---\n**<h4>Question 7:** Create a function to  instantiate Bagging classifer.<\/h4>\n\n---\n- Use 500 logistic regression classifiers to create a bagging method.\n\n- You have to use the hyperparameters setting of the model as following:\n  - `base_estimator` = Logistic Regression\n  - `n_neghbors` = 500 and\n  - `random_state` = 42,  \n","3b023ed2":"<a name = Section22><\/a>\n### **2.2 Data Information**\n\n - In this section, we will get **information about the data** and see some observations.\n","e69828e6":"**Observations**\n\n- We can see that the '?' value in the 'Bare nuclei' is replaced with 0.","9133641c":"---\n**<h4>Question 3:** Create a function that displays the frequency of Malignant and Benign tumors in the class column?<\/h4>\n\n---\n- You can plot the frequency distribution using seaborn countplot function.\n\n- You can also add proportions onto your plot.","b5061f01":"<a name = Section6><\/a>\n\n---\n# **4. Exploratory Data Analysis**\n---","68a9f55a":"---\n# **Table of Contents**\n---\n\n**1.** [**Installing & Importing Libraries**](#Section1)<br>\n  - **1.1** [**Installing Libraries**](#Section11)\n  - **1.2** [**Upgrading Libraries**](#Section12)\n  - **1.3** [**Importing Libraries**](#Section13)\n\n**2.** [**Data Acquisition & Description**](#Section2)<br>\n  - **2.1** [**Data Description**](#Section21)<br>\n\n**3.** [**Data Pre-processing**](#Section3)<br>\n  - **3.1** [**Pre-Profiling Report**](#Section31)<br>\n\n**4.** [**Exploratory Data Analysis**](#Section4)<br>\n**5.** [**Post Data Processing & Feature Selection**](#Section5)<br>\n**6.** [**Model Development & Evaluation**](#Section6)<br>\n**7.** [**Conclusion**](#Section7)<br>\n","7706662b":"---\n**<h4>Question 10:** Create a function to create an ensemble model using Voting Classifier.<\/h4>\n\n---\n- We can use different model like \"LogisticRegression\", \"DecisionTreeClassifier\", \"SVC\" to create a voting classifier.","65e0c411":"**Observation:**\n\n-  The report shows that there are **11 features** out of which **2 is categorical**, and **9 is numerical**.\n\n- There exists **0 missing values** and **8 redundant data points**.\n\n- Uniformity of Cell Shape is **highly correlated** with Uniformity of Cell Size\n\n- Similarly, we can explore information for rest of features from the report. "}}