{"cell_type":{"af221a89":"code","caef52de":"code","dba463ee":"code","61ffb236":"code","0a67b7f7":"code","e5096fa1":"code","f122e6a1":"code","a771204d":"code","0a0ef893":"code","f8adc387":"code","03a1c9ce":"code","f76a9a77":"code","16c8b8cd":"code","deb85f2f":"code","72a89284":"code","7e8f7119":"code","b513e542":"code","54400751":"code","398b1862":"code","12bdccf8":"code","8aa9137b":"code","10c5f393":"code","9e088765":"code","129ff0d6":"code","fb00b252":"code","2457c249":"code","c813e57e":"code","e5b0f60e":"code","016677c2":"code","60c586ee":"code","e77050fe":"code","8e92ef12":"code","5ea55a8c":"code","b5ed13cf":"code","d4fd3b2e":"code","28eb1fff":"code","70da9a08":"markdown","4d0bac94":"markdown","7a3c131e":"markdown","e7622466":"markdown","ffb70ba2":"markdown"},"source":{"af221a89":"import numpy as np \nimport pandas as pd \nimport scipy.optimize as opt\nimport math\nfrom pathlib import Path\nfrom tqdm.notebook import tqdm\nimport pyproj\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nimport plotly.express as px\nimport copy\nimport plotly.graph_objects as go\nimport json\nimport pickle","caef52de":"with open('..\/input\/region-classification\/region_type_train.json') as f:\n    region_type_train = json.load(f)\nwith open('..\/input\/region-classification\/region_type_test.json') as f:\n    region_type_test = json.load(f)","dba463ee":"def visualize_trafic(df, center={\"lat\":37.423576, \"lon\":-122.094132}, zoom=9):\n    fig = px.scatter_mapbox(df,\n                            \n                            # Here, plotly gets, (x,y) coordinates\n                            lat=\"latDeg\",\n                            lon=\"lngDeg\",\n                            \n                            #Here, plotly detects color of series\n                            color=\"phoneName\",\n                            labels=\"phoneName\",\n                            \n                            zoom=zoom,\n                            center=center,\n                            height=400,\n                            width=600)\n    fig.update_layout(mapbox_style='stamen-terrain')\n    fig.update_layout(margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0})\n    fig.update_layout(title_text=\"GPS trafic\")\n    fig.show()\n    \ndef visualize_collection(df, collection, center={\"lat\":37.423576, \"lon\":-122.094132}):\n    df_traj = df[df['collectionName'] == collection]\n    center = {\"lat\":37.423576, \"lon\":-122.094132}\n    visualize_trafic(df_traj, center)","61ffb236":"def calc_haversine(lat1, lon1, lat2, lon2):\n    \"\"\"Calculates the great circle distance between two points\n    on the earth. Inputs are array-like and specified in decimal degrees.\n    \"\"\"\n    RADIUS = 6_367_000\n    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat\/2)**2 + \\\n          np.cos(lat1) * np.cos(lat2) * np.sin(dlon\/2)**2\n    dist = 2 * RADIUS * np.arcsin(a**0.5)\n    return dist\n\ndef percentile50(x):\n    return np.percentile(x, 50)\ndef percentile95(x):\n    return np.percentile(x, 95)\n\ndef get_train_score(df, gt):\n    gt = gt.rename(columns={'latDeg':'latDeg_gt', 'lngDeg':'lngDeg_gt'})\n    df = df.merge(gt, on=['collectionName', 'phoneName', 'millisSinceGpsEpoch'], how='inner')\n    # calc_distance_error\n    df['err'] = calc_haversine(df['latDeg_gt'], df['lngDeg_gt'], df['latDeg'], df['lngDeg'])\n    # calc_evaluate_score\n    df['phone'] = df['collectionName'] + '_' + df['phoneName']\n    res = df.groupby('phone')['err'].agg([percentile50, percentile95])\n    res['p50_p90_mean'] = (res['percentile50'] + res['percentile95']) \/ 2 \n    score = res['p50_p90_mean'].mean()\n    return score\n\ndef eval_all(df_pred, df_gt):\n    scores = []\n    compared_cols = [\"latDeg_truth\",\"lngDeg_truth\",\"latDeg_pred\",\"lngDeg_pred\"]\n    collections = sorted(df_gt['collectionName'].unique())\n    for collection in collections:\n        df_pred_col = df_pred[df_pred['collectionName'] == collection]\n        df_gt_col = df_gt[df_gt['collectionName'] == collection]\n        \n        score = get_train_score(df_pred_col, df_gt_col)\n        \n        df_merged = pd.merge_asof(df_gt_col.sort_values('millisSinceGpsEpoch'), df_pred_col.sort_values('millisSinceGpsEpoch'), \n                                  on=\"millisSinceGpsEpoch\", by=[\"collectionName\", \"phoneName\"], \n                                  direction='nearest',tolerance=100000, suffixes=('_truth', '_pred'))\n        df_merged = df_merged.sort_values(by=[\"collectionName\", \"phoneName\", \"millisSinceGpsEpoch\"], ignore_index=True)\n\n        haversine = calc_haversine(*df_merged[compared_cols].to_numpy().transpose()).mean()\n        scores.append([collection, haversine, score])\n        \n    score = get_train_score(df_pred, df_gt)\n    df_merged = pd.merge_asof(df_gt.sort_values('millisSinceGpsEpoch'), df_pred.sort_values('millisSinceGpsEpoch'), \n                              on=\"millisSinceGpsEpoch\", by=[\"collectionName\", \"phoneName\"], \n                              direction='nearest',tolerance=100000, suffixes=('_truth', '_pred'))\n    haversine = calc_haversine(*df_merged[compared_cols].to_numpy().transpose()).mean()\n    scores.append(['all', haversine, score])\n    \n    df_scores = pd.DataFrame(scores, columns=['collection', 'haversine', 'score'])\n    return df_scores","0a67b7f7":"def ecef2lla(x, y, z):\n    # x, y and z are scalars or vectors in meters\n    x = np.array([x]).reshape(np.array([x]).shape[-1], 1)\n    y = np.array([y]).reshape(np.array([y]).shape[-1], 1)\n    z = np.array([z]).reshape(np.array([z]).shape[-1], 1)\n\n    a=6378137\n    a_sq=a**2\n    e = 8.181919084261345e-2\n    e_sq = 6.69437999014e-3\n\n    f = 1\/298.257223563\n    b = a*(1-f)\n\n    # calculations:\n    r = np.sqrt(x**2 + y**2)\n    ep_sq  = (a**2-b**2)\/b**2\n    ee = (a**2-b**2)\n    f = (54*b**2)*(z**2)\n    g = r**2 + (1 - e_sq)*(z**2) - e_sq*ee*2\n    c = (e_sq**2)*f*r**2\/(g**3)\n    s = (1 + c + np.sqrt(c**2 + 2*c))**(1\/3.)\n    p = f\/(3.*(g**2)*(s + (1.\/s) + 1)**2)\n    q = np.sqrt(1 + 2*p*e_sq**2)\n    r_0 = -(p*e_sq*r)\/(1+q) + np.sqrt(0.5*(a**2)*(1+(1.\/q)) - p*(z**2)*(1-e_sq)\/(q*(1+q)) - 0.5*p*(r**2))\n    u = np.sqrt((r - e_sq*r_0)**2 + z**2)\n    v = np.sqrt((r - e_sq*r_0)**2 + (1 - e_sq)*z**2)\n    z_0 = (b**2)*z\/(a*v)\n    h = u*(1 - b**2\/(a*v))\n    phi = np.arctan((z + ep_sq*z_0)\/r)\n    lambd = np.arctan2(y, x)\n\n    return phi*180\/np.pi, lambd*180\/np.pi, h","e5096fa1":"\nEARTH_ROTATION_RATE = 7.2921151467e-005\nSPEED_OF_LIGHT = 2.99792458e8\nEARTH_RADIUS = 6_371_000\n\ndef calc_pos_fix(sat_pos, pr, weights=1, x0=[-2.69456739e+06, -4.29648247e+06,  3.85481182e+06, 0], loss='cauchy'):\n    '''\n    Calculates gps fix with WLS optimizer\n    returns:\n    0 -> list with positions\n    1 -> pseudorange errs\n    '''\n    n = len(pr)\n    if n < 3:\n        return x0, []\n    Fx_pos = pr_residual(sat_pos, pr, weights=weights)\n    opt_pos = opt.least_squares(Fx_pos, x0, loss=loss).x\n    return opt_pos, Fx_pos(opt_pos, weights=1)\n\ndef pr_residual(sat_pos, pr, weights=1):\n    # solve for pos\n    def Fx_pos(x_hat, weights=weights):\n        theta = EARTH_ROTATION_RATE * (pr - x_hat[3]) \/ SPEED_OF_LIGHT\n        # theta = EARTH_ROTATION_RATE * (pr) \/ SPEED_OF_LIGHT\n        dx = np.sqrt(\n            (sat_pos[:, 0] * np.cos(theta) + sat_pos[:, 1] * np.sin(theta) - x_hat[0])**2 + \n            (-sat_pos[:, 0] * np.sin(theta) + sat_pos[:, 1] * np.cos(theta) - x_hat[1])**2 +\n            (sat_pos[:, 2] - x_hat[2]) ** 2\n        )\n        rows = weights * (dx - pr + x_hat[3])\n        return rows\n    return Fx_pos","f122e6a1":"def estimation_pipeline(df_trails, region_type, eldeg_high = 50, eldeg_mid = 25, eldeg_low = 15):\n    \"\"\" simple pipeline to estimate the GNSS receiver location by least square\n    Args:\n    df_trails: the df read from derived file\n\n    Returns:\n    result df with estimated degrees and heights\n    \"\"\"\n    \n    elev_deg_a_downtown = 10\n    elev_deg_b_downtown = 45\n    elev_deg_a_tree = 10\n    elev_deg_b_tree = 10\n    \n    df_trails[\"correctedPrM\"] = df_trails[\"rawPrM\"] + df_trails[\"satClkBiasM\"] - df_trails[\"isrbM\"] - df_trails[\"ionoDelayM\"] - df_trails[\"tropoDelayM\"]\n\n    results = []\n    results_loss = []\n    x = [0, 0, 0, 0]\n\n    df_epochs = df_trails.groupby([\"collectionName\", \"phoneName\", \"millisSinceGpsEpoch\"])\n    for i, (indices, df_epoch) in enumerate(tqdm(df_epochs, desc=\"Estimate location by LS for epoch\")):\n        sat_pos = df_epoch[[\"xSatPosM\",\"ySatPosM\",\"zSatPosM\"]].to_numpy()\n        pseudoranges = np.squeeze(df_epoch[[\"correctedPrM\"]].to_numpy())\n        pseudoranges_sigma = np.squeeze(df_epoch[[\"rawPrUncM\"]].to_numpy())\n        x, _ = calc_pos_fix(sat_pos, pseudoranges, 1\/pseudoranges_sigma, x)\n        \n        phone2sat_pos = sat_pos - x[:3]\n        theta = np.arccos(np.sum(x[:3] * phone2sat_pos, axis=1) \/ np.linalg.norm(phone2sat_pos, axis=1) \/ np.linalg.norm(x[:3]))\n\n        # downtown\n        if 'downtown' in region_type[indices[0]]:\n            lane_vec = df_epoch[['dx', 'dy', 'dz']].values[0]\n            cond = ~ get_sat_ids(sat_pos, x[:3], lane_vec, elev_deg_a = elev_deg_a_downtown, elev_deg_b = elev_deg_b_downtown)\n            # while np.sum(cond) < sat_pos.shape[0] * 0.6: # iterate until the phone can see enough satellites.\n            #     elev_deg_b_downtown -= 5\n            #     cond = ~ get_sat_ids(sat_pos, x[:3], lane_vec, elev_deg_a = elev_deg_a_downtown, elev_deg_b = elev_deg_b_downtown)\n        # tree\n        elif 'tree' in region_type[indices[0]]:\n            lane_vec = df_epoch[['dx', 'dy', 'dz']].values[0]\n            cond = ~ get_sat_ids(sat_pos, x[:3], lane_vec, elev_deg_a = elev_deg_a_tree, elev_deg_b = elev_deg_b_tree)\n        # highway & \"other\"\n        else:\n            elevation_deg = eldeg_low\n            threshold_theta = (90 - elevation_deg) * np.pi \/ 180\n            cond = np.abs(theta) > threshold_theta\n\n        pseudoranges_sigma += cond * 10\n        x, loss = calc_pos_fix(sat_pos, pseudoranges, 1\/pseudoranges_sigma, x)\n\n        values = np.squeeze(ecef2lla(*x[:3]))\n        results.append([*indices, *values])\n        results_loss.append([*indices, loss])\n    df_estimate = pd.DataFrame(results, columns=[\"collectionName\", \"phoneName\", \"millisSinceGpsEpoch\", \"latDeg\", \"lngDeg\", \"heightAboveWgs84EllipsoidM\"])\n    df_residuals = pd.DataFrame(results_loss, columns=['collectionName', 'phoneName', 'millisSinceGpsEpoch', 'residual'])\n    return df_estimate, results_loss","a771204d":"def get_sat_ids(pos_sat, pos_phone, vec_lane, elev_deg_a = 15, elev_deg_b = 15):\n    pos_phone = pos_phone[:3]\n    e_a = vec_lane \/ np.linalg.norm(vec_lane)\n    e_b = np.cross(e_a, pos_phone \/ np.linalg.norm(pos_phone))\n    pos_sat2phone = pos_sat - pos_phone\n    a = np.dot(pos_sat2phone, e_a.reshape(-1, 1))\n    b = np.dot(pos_sat2phone, e_b.reshape(-1, 1))\n    l = np.dot(pos_sat2phone, pos_phone) \/ np.linalg.norm(pos_phone)\n    theta_a = (90 - elev_deg_a) * np.pi \/ 180\n    theta_b = (90 - elev_deg_b) * np.pi \/ 180\n    cond = (a.reshape(-1)\/np.tan(theta_a))**2 + (b.reshape(-1)\/np.tan(theta_b))**2 <= l**2\n    \n    return cond","0a0ef893":"def add_dxyz(df):\n    ecef = pyproj.Proj(proj='geocent', ellps='WGS84', datum='WGS84')\n    lla = pyproj.Proj(proj='latlong', ellps='WGS84', datum='WGS84')\n    x, y, z = pyproj.transform(lla, ecef, *df[['lngDeg', 'latDeg', 'heightAboveWgs84EllipsoidM']].values.transpose(), radians=False)\n    df[['x', 'y', 'z']] = np.array([x, y, z]).transpose()\n\n    ans = []\n    for (collection, phone), df_traj in df.groupby(['collectionName', 'phoneName']):\n        # if 'SJC' not in collection:\n        #     continue\n        positions = df_traj[['x', 'y', 'z']].values\n        times = df_traj['millisSinceGpsEpoch'].values\n        for idx, pos in enumerate(positions):\n            i = 1\n            dist = np.linalg.norm(positions[max(idx - i, 0)] - positions[min(idx + i, len(positions)-1)])\n            while dist < 1e1:\n                i += 1\n                dist = np.linalg.norm(positions[max(idx - i, 0)] - positions[min(idx + i, len(positions)-1)])\n            ans.append([collection, phone, times[idx], *(positions[max(idx - i, 0)] - positions[min(idx + i, len(positions)-1)])])\n    dxyz = pd.DataFrame(ans, columns=['collectionName', 'phoneName', 'millisSinceGpsEpoch', 'dx', 'dy', 'dz'])\n    df = df.merge(dxyz, how='left')\n    return df","f8adc387":"datapath = Path(\"..\/input\/google-smartphone-decimeter-challenge\/\")\n\ncollection = '2021-04-29-US-SJC-2'\nphone = 'SamsungS20Ultra'\nepoch_time = 1303758010000\n\n# collection = '2021-04-28-US-SJC-1'\n# phone = 'Pixel4'\n# epoch_time = 1303676004438\n\n# collection = '2020-05-14-US-MTV-1'\n# phone = 'Pixel4'\n# epoch_time = 1273529477442\n\ndf_baseline = pd.read_csv(datapath\/\"baseline_locations_train.csv\")\ndf_sample_trail_gt = pd.read_csv(datapath\/\"train\/{0}\/{1}\/ground_truth.csv\".format(collection, phone))\ndf_sample_trail_gt = add_dxyz(df_sample_trail_gt)\n# print(df_sample_trail_gt.millisSinceGpsEpoch[1000:1010])\ndf_sample_trail = pd.read_csv(datapath\/\"train\/{0}\/{1}\/{1}_derived.csv\".format(collection, phone))\ndf_sample_trail[\"correctedPrM\"] = df_sample_trail[\"rawPrM\"] + df_sample_trail[\"satClkBiasM\"] - df_sample_trail[\"isrbM\"] - df_sample_trail[\"ionoDelayM\"] - df_sample_trail[\"tropoDelayM\"] \n\ndf_sample_epoch = df_sample_trail[df_sample_trail.millisSinceGpsEpoch == epoch_time]\ndf_sample_epoch_gt = df_sample_trail_gt[df_sample_trail_gt.millisSinceGpsEpoch == epoch_time]\ndf_sample_epoch_baseline = df_baseline[(df_baseline.collectionName == collection) & (df_baseline.phoneName == phone) & (df_baseline.millisSinceGpsEpoch == epoch_time)]\n\nsat_pos = df_sample_epoch[[\"xSatPosM\",\"ySatPosM\",\"zSatPosM\"]].to_numpy()\npseudoranges = np.squeeze(df_sample_epoch[[\"correctedPrM\"]].to_numpy())\npseudoranges_sigma = np.squeeze(df_sample_epoch[[\"rawPrUncM\"]].to_numpy())\n\nvisualize_trafic(df_sample_epoch_gt)","03a1c9ce":"x, dp = calc_pos_fix(sat_pos, pseudoranges, 1.0 \/ pseudoranges_sigma, loss='linear')\ngt = df_sample_epoch_gt[[\"latDeg\",\"lngDeg\",\"heightAboveWgs84EllipsoidM\"]].to_numpy()[0]\nest = np.array([hoge[0][0] for hoge in ecef2lla(*x[:3])])\nprint(\"Ground truth:\", gt)\nprint(\"Simple Least Square Estimation:\", est)\nprint(\"Baseline:\", df_sample_epoch_baseline[[\"latDeg\",\"lngDeg\",\"heightAboveWgs84EllipsoidM\"]].to_numpy())\nprint(\"Error: \", np.linalg.norm(gt[:2]-est[:2]))\nprint(\"normalized loss\", np.sqrt(np.sum((np.linalg.norm(x[:3] - sat_pos, axis=1) - pseudoranges)**2) \/ (len(sat_pos) - 4)))\n\ndf_satellites = df_sample_epoch[['constellationType', 'svid', 'signalType', 'rawPrUncM']]\ndf_satellites['residual'] = np.linalg.norm(sat_pos - x[:3], axis=1) - pseudoranges + x[3]\nphone2sat_pos = sat_pos - x[:3]\ndf_satellites['theta'] = np.arccos(np.sum(x[:3] * phone2sat_pos, axis=1) \/ np.linalg.norm(phone2sat_pos, axis=1) \/ np.linalg.norm(x[:3]))\ndf_satellites","f76a9a77":"x_next = x + np.array([-20, 20, 25, 0])\ne_a = df_sample_epoch_gt[['dx', 'dy', 'dz']].values[0]\nmarker_small = dict(\n        size=1,\n        colorscale='Viridis',   # choose a colorscale\n        opacity=0.8\n    )\nmarker = dict(\n        size=4,\n        colorscale='Viridis',   # choose a colorscale\n        opacity=0.8\n    )\nmarker_large = dict(\n        size=16,\n        colorscale='Viridis',   # choose a colorscale\n        opacity=0.8\n    )\n\n# theta = np.arccos(np.sum(x[:3] * (sat_pos - x[:3]), axis=1) \/ np.linalg.norm((sat_pos - x[:3]), axis=1) \/ np.linalg.norm(x[:3]))\n# threshold_theta = (90 - 25) * np.pi \/ 180\n# sat_ids_chosen = np.abs(theta) < threshold_theta\n\nsat_ids_chosen = get_sat_ids(sat_pos, x[:3], df_sample_epoch_gt[['dx', 'dy', 'dz']].values[0], elev_deg_a = 10, elev_deg_b = 45)\n# residual = np.linalg.norm(sat_pos - x[:3], axis=1) - pseudoranges + x[3]\n# sat_ids_chosen = np.abs(residual) < 40\n\nfig = go.Figure(data=[\n    go.Scatter3d(x=sat_pos[sat_ids_chosen, 0], y=sat_pos[sat_ids_chosen, 1], z=sat_pos[sat_ids_chosen, 2],\n                 mode='markers', opacity=0.9, name='satellite', marker=marker),\n    go.Scatter3d(x=sat_pos[~sat_ids_chosen, 0], y=sat_pos[~sat_ids_chosen, 1], z=sat_pos[~sat_ids_chosen, 2],\n                 mode='markers', opacity=0.9, name='satellite (not chosen)', marker=marker),\n    go.Scatter3d(x=[x[0]], y=[x[1]], z=[x[2]], mode='markers', opacity=1, name='phone', marker=marker),\n    go.Scatter3d(x=[x[0], x[0] + e_a[0]*1000000 ], y=[x[1], x[1] + e_a[1]*1000000 ], z=[x[2], x[2] + e_a[2]*1000000 ], opacity=1, name='lane', marker=marker_small),\n    go.Scatter3d(x=[0], y=[0], z=[0], mode='markers', opacity=0.5, name='Center of the Earth', marker=marker_large),\n])\nfig.show()","16c8b8cd":"residual = pseudoranges - np.linalg.norm(sat_pos - x[:3], axis=1) - x[3]\nx, dp = calc_pos_fix(sat_pos[sat_ids_chosen], pseudoranges[sat_ids_chosen], 1.0 \/ (pseudoranges_sigma[sat_ids_chosen]), loss='linear')\n\ngt = df_sample_epoch_gt[[\"latDeg\",\"lngDeg\",\"heightAboveWgs84EllipsoidM\"]].to_numpy()[0]\nest = np.array([hoge[0][0] for hoge in ecef2lla(*x[:3])])\nprint(\"Ground truth:\", gt)\nprint(\"Simple Least Square Estimation:\", est)\nprint(\"Error: \", np.linalg.norm(gt[:2]-est[:2]))\nprint(\"normalized loss\", np.sqrt(np.sum((np.linalg.norm(x[:3] - sat_pos[sat_ids_chosen], axis=1) - pseudoranges[sat_ids_chosen])**2) \/ (len(sat_pos[sat_ids_chosen]) - 4)))","deb85f2f":"datapath = Path(\"..\/input\/google-smartphone-decimeter-challenge\/\")\nground_truths = (datapath \/ \"train\").rglob(\"ground_truth.csv\")\ndrived_files = (datapath \/ \"train\").rglob(\"*_derived.csv\")\n\ndf_gt = pd.concat([pd.read_csv(filepath) for filepath in tqdm(ground_truths, total=73, desc=\"Reading ground truth data\")], ignore_index=True)\ndf_baseline_train = pd.read_csv(datapath \/ 'baseline_locations_train.csv')\ndf_derived_train = pd.concat([pd.read_csv(filepath) for filepath in tqdm(drived_files, total=73, desc=\"Reading drived data\")], ignore_index=True)","72a89284":"df_gt[\"receivedSvTimeInGpsNanos\"] = df_gt.millisSinceGpsEpoch*int(1e6)\ndf_derived_raw_train = df_derived_train.drop(\"millisSinceGpsEpoch\", axis=1)\ndf_gt = add_dxyz(df_gt)\n\ndf_merge_train = pd.merge_asof(df_derived_raw_train.sort_values('receivedSvTimeInGpsNanos'), df_gt.sort_values('receivedSvTimeInGpsNanos'), \n                                           on=\"receivedSvTimeInGpsNanos\", by=[\"collectionName\", \"phoneName\"], direction='nearest',tolerance=int(1e8))\ndf_merge_train = df_merge_train.sort_values(by=[\"collectionName\", \"phoneName\", \"millisSinceGpsEpoch\"], ignore_index=True)","7e8f7119":"delta_millis = df_merge_train['millisSinceGpsEpoch'] - df_merge_train['receivedSvTimeInGpsNanos'] \/ 1e6\nwhere_good_signals = (0 < delta_millis) & (delta_millis < 300)\ndf_merge_train_filtered = df_merge_train[where_good_signals].copy()","b513e542":"# df_estimate_train = estimation_pipeline_iterative_version(df_merge_train, region_type_train, 10, 10, 10)","54400751":"df_estimate_train, df_residuals_train = estimation_pipeline(df_merge_train_filtered, region_type_train, 10, 10, 10)","398b1862":"df_estimate_train.millisSinceGpsEpoch = df_estimate_train.millisSinceGpsEpoch.astype(np.int64)","12bdccf8":"df_merged_baseline_train = pd.merge_asof(df_gt.sort_values('millisSinceGpsEpoch'),\n                                         df_baseline_train.sort_values('millisSinceGpsEpoch'), \n                                         on=\"millisSinceGpsEpoch\", by=[\"collectionName\", \"phoneName\"], \n                                         direction='nearest',tolerance=100000, suffixes=('_truth', '_pred'))\ndf_merged_baseline_train = df_merged_baseline_train.sort_values(by=[\"collectionName\", \"phoneName\", \"millisSinceGpsEpoch\"], ignore_index=True)\n\ndf_merged_SL_train = pd.merge_asof(df_gt.sort_values('millisSinceGpsEpoch'), \n                                   df_estimate_train.sort_values('millisSinceGpsEpoch'), \n                                   on=\"millisSinceGpsEpoch\", by=[\"collectionName\", \"phoneName\"], \n                                   direction='nearest',tolerance=100000, suffixes=('_truth', '_pred'))\ndf_merged_SL_train = df_merged_SL_train.sort_values(by=[\"collectionName\", \"phoneName\", \"millisSinceGpsEpoch\"], ignore_index=True)\n\ncompared_cols = [\"latDeg_truth\",\"lngDeg_truth\",\"latDeg_pred\",\"lngDeg_pred\"]\n# print(\"Weighted Least Square (baseline) haversine distance (M):\", calc_haversine(*df_merged_baseline_train[compared_cols].to_numpy().transpose()).mean())\nprint(\"Weighted Least Square haversine distance (M):\", calc_haversine(*df_merged_SL_train[compared_cols].to_numpy().transpose()).mean())","8aa9137b":"# df_pred_tmp = pd.DataFrame(df_merged_baseline_train[['latDeg_pred', 'lngDeg_pred']].values, columns=['latDeg', 'lngDeg'])\n# df_pred_tmp['collectionName'] = df_merged_baseline_train['collectionName']\n# df_pred_tmp['phoneName'] = df_merged_baseline_train['phoneName']\n# df_pred_tmp['millisSinceGpsEpoch'] = df_merged_baseline_train['millisSinceGpsEpoch']\neval_all(df_baseline_train, df_gt)","10c5f393":"df_pred_tmp = pd.DataFrame(df_merged_SL_train[['latDeg_pred', 'lngDeg_pred']].values, columns=['latDeg', 'lngDeg'])\ndf_pred_tmp['collectionName'] = df_merged_SL_train['collectionName']\ndf_pred_tmp['phoneName'] = df_merged_SL_train['phoneName']\ndf_pred_tmp['millisSinceGpsEpoch'] = df_merged_SL_train['millisSinceGpsEpoch']\neval_all(df_gt, df_pred_tmp)","9e088765":"df_estimate_train['millisSinceGpsEpoch'] = df_estimate_train['millisSinceGpsEpoch'].astype(np.int64)\n\ndf_baseline_train = df_baseline_train.drop([\"latDeg\",\"lngDeg\",\"heightAboveWgs84EllipsoidM\"], axis=1)\ndf_merged = pd.merge_asof(df_baseline_train.sort_values('millisSinceGpsEpoch'), \n                        df_estimate_train.sort_values('millisSinceGpsEpoch'), \n                        on=\"millisSinceGpsEpoch\", by=[\"collectionName\", \"phoneName\"], direction='nearest', tolerance=100000)\ndf_merged = df_merged.sort_values(by=[\"phone\", \"millisSinceGpsEpoch\"], ignore_index=True)\n\ndf_submission = df_merged[[\"phone\", \"millisSinceGpsEpoch\", \"latDeg\", \"lngDeg\"]].copy()\ndf_submission.to_csv('train_submission.csv', index=False)\nwith open('residuals_train.pickle', 'wb') as f:\n    pickle.dump(df_residuals_train, f)","129ff0d6":"df_baseline_train_onlysanjose = df_baseline_train.copy()\nis_sjc = lambda x: 'SJC' in x\ndf_baseline_train_onlysanjose[df_baseline_train.phone.apply(is_sjc)] = df_submission[df_submission.phone.apply(is_sjc)]\ndf_baseline_train_onlysanjose.to_csv('train_submission_sanjose_estimated.csv')","fb00b252":"from pathlib import Path\n# from tqdm import tqdm\nfrom tqdm.notebook import tqdm\n\n# datapath = Path(\".\/data\")\ndrived_files = (datapath \/ \"test\").rglob(\"*_derived.csv\")\n\ndf_baseline_test = pd.read_csv('..\/input\/210706-state-of-the-art\/submission.csv')\ndf_baseline_test['collectionName'] = df_baseline_test['phone'].apply(lambda x: x.split('_')[0])\ndf_baseline_test['phoneName'] = df_baseline_test['phone'].apply(lambda x: x.split('_')[1])\ndf_baseline_test['heightAboveWgs84EllipsoidM'] = pd.read_csv('..\/input\/google-smartphone-decimeter-challenge\/baseline_locations_test.csv')['heightAboveWgs84EllipsoidM']\ndf_derived_test = pd.concat([pd.read_csv(filepath) for filepath in tqdm(drived_files, total=48, desc=\"Reading drived data\")], ignore_index=True)","2457c249":"df_baseline_test[\"receivedSvTimeInGpsNanos\"] = df_baseline_test.millisSinceGpsEpoch*int(1e6)\ndf_raw_test = df_derived_test.drop(\"millisSinceGpsEpoch\", axis=1)\ndf_baseline_test = add_dxyz(df_baseline_test)\ndf_merge_test = pd.merge_asof(df_raw_test.sort_values('receivedSvTimeInGpsNanos'), df_baseline_test.sort_values('receivedSvTimeInGpsNanos'), \n                                           on=\"receivedSvTimeInGpsNanos\", by=[\"collectionName\", \"phoneName\"], direction='nearest',tolerance=int(1e9))\ndf_merge_test = df_merge_test.sort_values(by=[\"collectionName\", \"phoneName\", \"millisSinceGpsEpoch\"], ignore_index=True)","c813e57e":"df_estimate_test, df_residuals_test = estimation_pipeline(df_merge_test, region_type_test, 10, 10, 10)\n# df_estimate_test = estimation_pipeline_iterative_version(df_merge_test, region_type_test, 10, 10, 10)","e5b0f60e":"df_estimate_test['millisSinceGpsEpoch'] = df_estimate_test['millisSinceGpsEpoch'].astype(np.int64)\n\ndf_baseline_test = df_baseline_test.drop([\"latDeg\",\"lngDeg\",\"heightAboveWgs84EllipsoidM\"], axis=1)\ndf_merged_test = pd.merge_asof(df_baseline_test.sort_values('millisSinceGpsEpoch'), \n                        df_estimate_test.sort_values('millisSinceGpsEpoch'), \n                        on=\"millisSinceGpsEpoch\", by=[\"collectionName\", \"phoneName\"], direction='nearest', tolerance=100000)\ndf_merged_test = df_merged_test.sort_values(by=[\"phone\", \"millisSinceGpsEpoch\"], ignore_index=True)\n\ndf_submission_test = df_merged_test[[\"phone\", \"millisSinceGpsEpoch\", \"latDeg\", \"lngDeg\"]].copy()\ndf_submission_test.to_csv('submission.csv', index=False)\nwith open('residuals_test.pickle', 'wb') as f:\n    pickle.dump(df_residuals_test, f)","016677c2":"df_baseline_test_kaggle = pd.read_csv('..\/input\/google-smartphone-decimeter-challenge\/baseline_locations_test.csv')\ndf_baseline_test_kaggle = df_baseline_test_kaggle[['phone', 'millisSinceGpsEpoch', 'latDeg', 'lngDeg']]\nis_sjc = lambda x: 'SJC' in x\ndf_baseline_test_kaggle[df_baseline_test.phone.apply(is_sjc)] = df_submission_test[df_submission_test.phone.apply(is_sjc)]\ndf_baseline_test_kaggle.to_csv('submission_sanjose_estimated.csv', index=False)","60c586ee":"visualize_trafic(df_merged_test)","e77050fe":"df_merge_train.groupby([\"collectionName\", \"phoneName\", \"millisSinceGpsEpoch\"]).size().reset_index().groupby([\"collectionName\"]).size()\n# df_merge_train.groupby([\"collectionName\"]).size()","8e92ef12":"pd.read_csv('..\/input\/google-smartphone-decimeter-challenge\/baseline_locations_train.csv').groupby([\"collectionName\"]).size()","5ea55a8c":"df_merge_test.groupby([\"collectionName\", \"phoneName\", \"millisSinceGpsEpoch\"]).size().reset_index().groupby([\"collectionName\"]).size()\n# df_merge_test.groupby([\"collectionName\"]).size()","b5ed13cf":"publics = ['2020-05-28-US-MTV-2',\n'2020-06-10-US-MTV-1',\n'2020-06-10-US-MTV-2',\n'2020-08-13-US-MTV-1',\n'2021-03-16-US-MTV-2',\n'2021-04-02-US-SJC-1',\n'2021-04-21-US-MTV-1',\n'2021-04-26-US-SVL-2',\n'2021-04-28-US-MTV-2', \n'2021-04-29-US-MTV-2',\n'2021-04-29-US-SJC-3']","d4fd3b2e":"hoge = pd.DataFrame()\nhoge['original'] = pd.read_csv('..\/input\/google-smartphone-decimeter-challenge\/baseline_locations_test.csv').groupby([\"collectionName\"]).size()\nhoge['gnss_available_points'] = df_merge_test.groupby([\"collectionName\", \"phoneName\", \"millisSinceGpsEpoch\"]).size().reset_index().groupby([\"collectionName\"]).size()\nhoge['null_points'] = hoge['original'] - hoge['gnss_available_points']\nhoge['is_public'] = hoge.index.isin(publics)","28eb1fff":"hoge","70da9a08":"# GNSS-only localization using WLS\n\nHere I demonstrate how to improve the position estimation from the original baseline provided by Kaggle Team. \nBefore going into the code, I would like to thank [@YangLiu](https:\/\/www.kaggle.com\/foreveryoung) for [the great notebook[1]](https:\/\/www.kaggle.com\/foreveryoung\/least-squares-solution-from-gnss-derived-data)! Also, I am new to GNSS field, so feel free to let me know if you find any mistakes in the notebook.\n\nThe major problem in GNSS is a multi-path problem.\nAs most of the competitors would observed, WLS solution from raw GNSS in city areas such as 2021-04-XX-US-SJC-Y are significantly worse compared to other areas. This noise is mainly due to the satellite signals reflected by the surrounding buildings. Since WLS algorithm calculate the optimal position based on the (pseudo-)range between the GNSS receiver and the satellites, this reflection can have a large impact on the estimation accuracy.\n\nTo mitigate the adverse effects of this multi-path phenomena, I introduced the following techniques in the estimation algorithm.\n1. cauchy loss in least-square optimization (instead of linear loss)\n2. elevation mask\n\nThe score was **LB: 8.179** before introducing these two algorithms. By adopting these techniques, the score improved to **LB: 6.776**. By ensembling with the provided baseline, the score further improved to **LB: 6.665**.\n\n## 1. cauchy loss in least-square optimization\nCauchy loss is a function defined as\n    rho(z) = ln(1 + z)\n. This function can weaken the effect of outliers in least-square optimization.\n\n## 2. elevation mask\nElevation mask is a frequently used filtering in GNSS-based localization based on elevation angle (= angle from the GNSS receiver to the satellite with respect to the ground surface). The idea here is very simple: the filter masks out a signal from which the satellite is close to the ground from the GNSS receiver perspective. In other words, the algorithm ignores the satellite signal if the elevation angle of the satellite is close to 0.\n\nIn this notebook, I increased the uncertainty of pseudorange of the low-angle satellites instead of ignoring them (for some reason, I found it is more effective through some experiments).\n\n![elvmask.png](attachment:149cbf19-7559-451f-a42a-07cff5bf4071.png)\n\n\n## What did not work\nI spent some time investigating RTKLIB. The software incorporates elevation mask as well as plenty of other algorithms (SNR-mask, Real-time kineamtics, etc). I thought that RTK can further improve the estimation, but the results were worse than that of this notebook.\n","4d0bac94":"https:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?tp=&arnumber=4724789","7a3c131e":"# Estimation (test data)","e7622466":"# Estimation (train data)","ffb70ba2":"# Visualize some examples"}}