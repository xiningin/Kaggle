{"cell_type":{"421f7cf2":"code","fd753a00":"code","d7ac02ef":"code","efb6940a":"code","67bf023b":"code","bf79635b":"code","1446ae50":"code","4b73fb49":"code","7b2e3621":"code","16296cc9":"code","edb26fa6":"code","76ef966f":"code","ba7bf0c5":"code","45015c39":"code","de654182":"code","b5f9e196":"code","b2a56eb6":"code","e3d9e889":"code","af5cd471":"markdown","dbb74bfb":"markdown"},"source":{"421f7cf2":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D,AveragePooling2D\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.callbacks import LearningRateScheduler,ReduceLROnPlateau\nfrom keras.optimizers import Adam # I believe this is better optimizer for our case\nfrom keras.preprocessing.image import ImageDataGenerator # to augmenting our images for increasing accuracy\nfrom keras.utils.vis_utils import plot_model\nimport scipy\nfrom sklearn.model_selection import train_test_split # to split our train data into train and validation sets\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nnp.random.seed(13) # My lucky number","fd753a00":"num_classes = 10 # We have 10 digits to identify\nbatch_size = 128 # Handle 128 pictures at each round\nepochs = 700 \nimg_rows, img_cols = 28, 28 # Image dimensions 28 pixels in height&width\ninput_shape = (img_rows, img_cols,1) # We'll use this while building layers","d7ac02ef":"# Load some date to rock'n roll\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","efb6940a":"# Drop the label from the data and move it to real label part\ny_train = train[\"label\"]\nx_train = train.drop(labels = [\"label\"],axis = 1 )","67bf023b":"# Normalize both sets\nx_train \/= 255\ntest \/= 255","bf79635b":"print(x_train.shape[0], 'train samples')\nprint(test.shape[0], 'test samples')","1446ae50":"# Images should be in shape of height,width and color channel so it will be 28x28x1\nx_train = x_train.values.reshape(-1,img_rows,img_cols,1).astype('float32')\ntest = test.values.reshape(-1,img_rows,img_cols,1).astype('float32')","4b73fb49":"# Class vectors needs to be binary so we use \"to_catogorical\" function of keras utilities for one-hot-encoding\ny_train = keras.utils.to_categorical(y_train, num_classes = num_classes)","7b2e3621":"# Lets split our train set into train and validation test sets with my lucky number 13 :)\nx_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size = 0.1)","16296cc9":"def model_cnn(input_shape=input_shape, num_classes=num_classes):   \n    model = Sequential()\n\n    # Add convolutional layer consisting of 32 filters and shape of 3x3 with ReLU activation\n    # We want to preserve more information for following layers so we use padding\n    # 'Same' padding tries to pad evenly left and right, \n    # but if the amount of columns to be added is odd, it will add the extra column to the right\n    model.add(Conv2D(32, kernel_size = (3,3), activation='relu', input_shape = input_shape))\n    model.add(BatchNormalization())\n    model.add(Conv2D(32, kernel_size = (3,3), activation='relu'))\n    model.add(BatchNormalization())\n\n    # Add convolutional layer consisting of 32 filters and shape of 5x5 with ReLU activation\n    # We give strides=2 for space between each sample on the pixel grid\n    model.add(Conv2D(32, kernel_size = (5,5), strides=2, padding='same', activation='relu'))\n    model.add(BatchNormalization())\n    # Dropping %40 of neurons\n    model.add(Dropout(0.4))\n    \n    model.add(Conv2D(64, kernel_size = (3,3), activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(64, kernel_size = (3,3), activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(64, kernel_size = (5,5), strides=2, padding='same', activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.4))\n\n    model.add(Conv2D(128, kernel_size = 4, activation='relu'))\n    model.add(BatchNormalization())\n    # To be able to merge into fully connected layer we have to flatten\n    model.add(Flatten())\n    model.add(Dropout(0.4))\n    # Lets add softmax activated neurons as much as number of classes\n    model.add(Dense(num_classes, activation = \"softmax\"))\n    # Compile the model with loss and metrics\n    model.compile(optimizer =  Adam() , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n    \n    return model","edb26fa6":"def LeNet5(input_shape=input_shape,num_classes=num_classes):\n    model = Sequential()\n    model.add(Conv2D(6, kernel_size=(5, 5), strides=(1, 1), activation='relu', input_shape=input_shape, padding=\"same\"))\n    model.add(AveragePooling2D(pool_size=(2, 2), strides=(1, 1), padding='valid'))\n    model.add(Conv2D(16, kernel_size=(5, 5), strides=(1, 1), activation='relu', padding='valid'))\n    model.add(AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n    model.add(Conv2D(120, kernel_size=(5, 5), strides=(1, 1), activation='relu', padding='valid'))\n    model.add(Flatten())\n    model.add(Dense(84, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n    model.compile(optimizer =  Adam() , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n    return model","76ef966f":"print(\"My Custom CNN Network:\")\nplot_model(model_cnn(), to_file='custom-cnn.png', show_shapes=True, show_layer_names=True)","ba7bf0c5":"print(\"Master Yann LeCun's LeNet-5 Network:\")\nplot_model(LeNet5(), to_file='lenet-5.png', show_shapes=True, show_layer_names=True)","45015c39":"model = []\nmodel.append(model_cnn())\nmodel.append(LeNet5())","de654182":"# Generate batches of tensor image data with real-time data augmentation more detail: https:\/\/keras.io\/preprocessing\/image\/\ndatagen = ImageDataGenerator(rotation_range=10, zoom_range = 0.1, width_shift_range=0.1, height_shift_range=0.1)\ndatagen.fit(x_train)","b5f9e196":"# Start multiple model training with the batch size\nmodels = []\nfor i in range(len(model)):\n    model[i].fit_generator(datagen.flow(x_train,y_train, batch_size=batch_size),\n                                        epochs = epochs, steps_per_epoch=x_train.shape[0] \/\/ batch_size,\n                                        validation_data = (x_test,y_test), \n                                        callbacks=[ReduceLROnPlateau(monitor='loss', patience=3, factor=0.1)], \n                                        verbose=2)\n    models.append(model[i])","b2a56eb6":"# Predict labels with models\nlabels = []\nfor m in models:\n    predicts = np.argmax(m.predict(test), axis=1)\n    labels.append(predicts)\n    \n# Ensemble with voting\nlabels = np.array(labels)\nlabels = np.transpose(labels, (1, 0))\nlabels = scipy.stats.mode(labels, axis=-1)[0]\nlabels = np.squeeze(labels)","e3d9e889":"# Dump predictions into submission file\npd.DataFrame({'ImageId' : np.arange(1, predicts.shape[0] + 1), 'Label' : labels }).to_csv('submission.csv', index=False)","af5cd471":"<img src=\"custom-cnn.png\">","dbb74bfb":"<img src=\"lenet-5.png\">"}}