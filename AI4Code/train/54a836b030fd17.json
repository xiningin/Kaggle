{"cell_type":{"88392b57":"code","a812a7ca":"code","6139278e":"code","0dcd8cb1":"code","5b70b932":"code","08737bd8":"code","f5aeea36":"code","e6f903eb":"code","062f10c8":"code","2bdc2239":"code","e34bf16b":"code","dc5409cf":"code","7c9e728c":"markdown"},"source":{"88392b57":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport random\nfrom typing import Union\nfrom math import ceil\nfrom os import mkdir","a812a7ca":"UNK = '<UNK>' # Unknown word\nEOS = '<EOS>' # End of sentence\n\ndef build_vocabulary(sentences: list, words_to_keep: int) -> list:\n    # builds a vocabulary using 'words_to_keep' most frequent words\n    # encountered in the list of sentences\n    vocabulary = {}\n    n = len(sentences)\n    for i, s in enumerate(sentences):\n        print('Creating vocabulary: %05.2f%%' % (100*(i+1)\/n,), end='\\r')\n        for word in s.strip().split():\n            vocabulary[word] = vocabulary.get(word, 0) + 1\n    vocabulary = list(vocabulary.items())\n    vocabulary.sort(reverse=True, key=lambda e: e[1])\n    vocabulary = vocabulary[0:words_to_keep]\n    vocabulary = [e[0] for e in vocabulary]\n    vocabulary.sort()\n    vocabulary.append(UNK)\n    vocabulary.append(EOS)\n    print('Done'+(50*' '))\n    return vocabulary\n\ndef build_sentences(vocabulary: list, sentences: list) -> list:\n    # transforms the list of sentences into a list of lists of words\n    # replacing words that are not in the vocabulary with <UNK>\n    # and appending <EOS> at the end of each sentence\n    processed_sent = []\n    n = len(sentences)\n    for i, sent in enumerate(sentences):\n        print('Creating sentences list: %05.2f%%' % (100*(i+1)\/n,), end='\\r')\n        s = []\n        for word in sent.strip().split():\n            if word not in vocabulary:\n                word = UNK\n            s.append(word)\n        s.append(EOS)\n        processed_sent.append(s)\n    print('Done'+(50*' '))\n    return processed_sent\n\ndef word2index(vocabulary: list, word: str) -> int:\n    # returns the index of 'word' in the vocabulary\n    return vocabulary.index(word)\n\ndef words2onehot(vocabulary: list, words: list) -> np.ndarray:\n    # transforms the list of words given as argument into\n    # a one-hot matrix representation using the index in the vocabulary\n    n_words = len(words)\n    n_voc = len(vocabulary)\n    indices = np.array([word2index(vocabulary, word) for word in words])\n    a = np.zeros((n_words, n_voc))\n    a[np.arange(n_words), indices] = 1\n    return a\n\ndef sample_word(vocabulary: list, prob: np.ndarray) -> str:\n    # sample a word from the vocabulary according to 'prob'\n    # probability distribution (the softmax output of our model)\n    # until it is != <UNK>\n    while True:\n        word = np.random.choice(vocabulary, p=prob)\n        if word != UNK:\n            return word","6139278e":"class Model:\n    def __init__(self, vocabulary: list = [], a_size: int = 0):\n        self.vocab = vocabulary\n        self.vocab_size = len(vocabulary)\n        self.a_size = a_size\n        self.combined_size = self.vocab_size + self.a_size\n        \n        # weights and bias used to compute the new a\n        # (a = vector that is passes to the next time step)\n        self.wa = tf.Variable(tf.random.normal(\n            stddev=1.0\/(self.combined_size+self.a_size),\n            shape=(self.combined_size, self.a_size),\n            dtype=tf.double))\n        self.ba = tf.Variable(tf.random.normal(\n            stddev=1.0\/(1+self.a_size),\n            shape=(1, self.a_size),\n            dtype=tf.double))\n        \n        # weights and bias used to compute y (the softmax predictions)\n        self.wy = tf.Variable(tf.random.normal(\n            stddev=1.0\/(self.a_size+self.vocab_size),\n            shape=(self.a_size, self.vocab_size),\n            dtype=tf.double))\n        self.by = tf.Variable(tf.random.normal(\n            stddev=1.0\/(1+self.vocab_size),\n            shape=(1, self.vocab_size),\n            dtype=tf.double))\n        \n        self.weights = [self.wa, self.ba, self.wy, self.by]\n        self.optimizer = tf.keras.optimizers.Adam()\n    \n    def __call__(self,\n                 a: Union[np.ndarray, tf.Tensor],\n                 x: Union[np.ndarray, tf.Tensor],\n                 y: Union[np.ndarray, tf.Tensor, None] = None) -> tuple:\n        \n        a_new = tf.math.tanh(tf.linalg.matmul(tf.concat([a, x], axis=1), self.wa)+self.ba)\n        y_logits = tf.linalg.matmul(a_new, self.wy)+self.by\n        if y is None:\n            # during prediction return softmax probabilities\n            return (a_new, tf.nn.softmax(y_logits))\n        else:\n            # during training return loss\n            return (a_new, tf.math.reduce_mean(\n                        tf.nn.softmax_cross_entropy_with_logits(y, y_logits)))\n    \n    def fit(self,\n            sentences: list,\n            batch_size: int = 128,\n            epochs: int = 10) -> None:\n        \n        n_sent = len(sentences)\n        num_batches = ceil(n_sent \/ batch_size)\n        \n        for epoch in range(epochs):\n            \n            random.shuffle(sentences)\n            start = 0\n            batch_idx = 0\n            \n            while start < n_sent:\n                \n                print('Training model: %05.2f%%' %\n                      (100*(epoch*num_batches+batch_idx+1)\/(epochs*num_batches),),\n                      end='\\r')\n                \n                batch_idx += 1\n                end = min(start+batch_size, n_sent)\n                batch_sent = sentences[start:end]\n                start = end\n                batch_sent.sort(reverse=True, key=lambda s: len(s))\n                \n                init_num_words = len(batch_sent)\n                a = np.zeros((init_num_words, self.a_size))\n                x = np.zeros((init_num_words, self.vocab_size))\n                \n                time_steps = len(batch_sent[0])\n                \n                with tf.GradientTape() as tape:\n                \n                    losses = []\n                    for t in range(time_steps):\n                        words = []\n                        for i in range(init_num_words):\n                            if t >= len(batch_sent[i]):\n                                break\n                            words.append(batch_sent[i][t])\n\n                        y = words2onehot(self.vocab, words)\n                        n = y.shape[0]\n                        a, loss = self(a[0:n], x[0:n], y)\n                        losses.append(loss)\n                        x = y\n                    \n                    loss_value = tf.math.reduce_mean(losses)\n                \n                grads = tape.gradient(loss_value, self.weights)\n                self.optimizer.apply_gradients(zip(grads, self.weights))\n\n    def sample(self) -> str:\n        # sample a new sentence from the learned model\n        sentence = ''\n        a = np.zeros((1, self.a_size))\n        x = np.zeros((1, self.vocab_size))\n        while True:\n            a, y_hat = self(a, x)\n            word = sample_word(self.vocab, tf.reshape(y_hat, (-1,)))\n            if word == EOS:\n                break\n            sentence += ' '+word\n            x = words2onehot(self.vocab, [word])\n        return sentence[1:]\n    \n    def predict_next(self, sentence: str) -> str:\n        # predict the next part of the sentence given as parameter\n        a = np.zeros((1, self.a_size))\n        for word in sentence.strip().split():\n            if word not in vocabulary:\n                word = UNK\n            x = words2onehot(self.vocab, [word])\n            a, y_hat = self(a, x)\n        s = ''\n        while True:\n            word = sample_word(self.vocab, tf.reshape(y_hat, (-1,)))\n            if word == EOS:\n                break\n            s += ' '+word\n            x = words2onehot(self.vocab, [word])\n            a, y_hat = self(a, x)\n        return s\n    \n    def save(self, name: str) -> None:\n        mkdir(f'.\/{name}')\n        with open(f'.\/{name}\/vocabulary.txt', 'w') as f:\n            f.write(','.join(self.vocab))\n        with open(f'.\/{name}\/a_size.txt', 'w') as f:\n            f.write(str(self.a_size))\n        np.save(f'.\/{name}\/wa.npy', self.wa.numpy())\n        np.save(f'.\/{name}\/ba.npy', self.ba.numpy())\n        np.save(f'.\/{name}\/wy.npy', self.wy.numpy())\n        np.save(f'.\/{name}\/by.npy', self.by.numpy())\n    \n    def load(self, name: str) -> None:\n        with open(f'.\/{name}\/vocabulary.txt', 'r') as f:\n            self.vocab = f.read().split(',')\n        with open(f'.\/{name}\/a_size.txt', 'r') as f:\n            self.a_size = int(f.read())\n            \n        self.vocab_size = len(self.vocab)\n        self.combined_size = self.vocab_size + self.a_size\n        \n        self.wa = tf.Variable(np.load(f'.\/{name}\/wa.npy'))\n        self.ba = tf.Variable(np.load(f'.\/{name}\/ba.npy'))\n        self.wy = tf.Variable(np.load(f'.\/{name}\/wy.npy'))\n        self.by = tf.Variable(np.load(f'.\/{name}\/by.npy'))\n        self.weights = [self.wa, self.ba, self.wy, self.by]","0dcd8cb1":"df = pd.read_csv('..\/input\/million-headlines\/abcnews-date-text.csv')\ndf","5b70b932":"vocabulary = build_vocabulary(df['headline_text'].values.tolist(), words_to_keep=10000)","08737bd8":"len(vocabulary)","f5aeea36":"sentences = build_sentences(vocabulary, df['headline_text'].values.tolist())","e6f903eb":"model = Model(vocabulary, 1024)","062f10c8":"model.fit(sentences, batch_size=128, epochs=10)","2bdc2239":"model.save('news_headlines_model')\n# model.load('news_headlines_model')","e34bf16b":"for i in range(20):\n    print(model.sample())","dc5409cf":"s = 'scientists just discovered'\ns += model.predict_next(s)\ns","7c9e728c":"### A simple RNN language model implemented from scratch with TensorFlow"}}