{"cell_type":{"be15d9f6":"code","306815ce":"code","f262f101":"code","3bfd14aa":"code","8073f8e1":"code","b35671b2":"code","cbbca48d":"code","3c41efc6":"code","7cfeb79e":"code","ae678ae2":"code","0fe75d40":"code","9e7771fd":"code","d2d32176":"code","d6ca150d":"code","98204873":"markdown","fe554836":"markdown","3e59e854":"markdown","cefab287":"markdown","d3a6aa09":"markdown"},"source":{"be15d9f6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","306815ce":"import os\nimport numpy as np\nimport pandas as pd \nimport random\nimport cv2\nimport matplotlib.pyplot as plt","f262f101":"import keras\nfrom keras.models import Model,Sequential\nfrom keras.layers import Input,Dense,BatchNormalization,Flatten,Dropout\nfrom keras.layers import Conv2D,SeparableConv2D,MaxPool2D,LeakyReLU,Activation\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint,ReduceLROnPlateau,EarlyStopping\nimport tensorflow as tf\n\nseed=232\nnp.random.seed(seed)\ntf.random.set_seed(seed)","3bfd14aa":"tf.__version__","8073f8e1":"input_path=\"..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/\"\n\nfig,ax=plt.subplots(2,3,figsize=(15,7))\nax=ax.ravel()       #to flatten into 1D array\nplt.tight_layout() #to pad between the images from their subplot images\n\nfor i,set_ in enumerate(['train','test','val']):\n    set_path=input_path+set_\n    ax[i].imshow(plt.imread(set_path+'\/NORMAL\/'+os.listdir(set_path+'\/NORMAL\/')[0]),cmap='gray')\n    ax[i].set_title('Set: {},Condition:NORMAL'.format(set_))\n    ax[i+3].imshow(plt.imread(set_path+'\/PNEUMONIA\/'+os.listdir(set_path+'\/PNEUMONIA')[0]),cmap='gray')\n    ax[i+3].set_title('Set: {},Condition:PNEUMONIA'.format(set_))","b35671b2":"for set_ in ['train','val','test']:\n    normal=len(os.listdir(input_path+set_+'\/NORMAL'))\n    pneum=len(os.listdir(input_path+set_+'\/PNEUMONIA'))\nprint('Normal images: ',normal)\nprint('Affected images: ',pneum)","cbbca48d":"input_path","3c41efc6":"def process_data(img_dims,batch_size):\n    \n    #Data generation objects\n    train_datagen=ImageDataGenerator(rescale=1.\/255,zoom_range=0.3,vertical_flip=True)\n    test_datagen=ImageDataGenerator(rescale=1.\/255)\n    \n    #To fed to network of batch_size and correct image dimension\n    train_gen=train_datagen.flow_from_directory(\n        directory=input_path+'train',\n        target_size=(img_dims,img_dims),\n        batch_size=batch_size,\n        class_mode='binary',\n        shuffle=True\n    )\n    \n    test_gen=test_datagen.flow_from_directory(\n        directory=input_path+'test',\n        target_size=(img_dims,img_dims),\n        batch_size=batch_size,\n        class_mode='binary',\n        shuffle=True\n    )\n    \n    #Making one batch of prediction to test set\n    test_data=[]\n    test_label=[]\n    \n    for cond in ['\/NORMAL\/','\/PNEUMONIA\/']:\n        for img in (os.listdir(input_path+'test'+cond)):\n            img=plt.imread(input_path+'test'+cond+img)\n            img=cv2.resize(img,(img_dims,img_dims))\n            img=np.dstack([img,img,img])\n            img=img.astype('float32')\/255\n            if cond=='\/NORMAL\/':\n                label=0\n            elif cond=='\/PNEUMONIA\/':\n                label=1\n            test_data.append(img)\n            test_label.append(label)\n                \n    test_data=np.array(test_data)\n    test_label=np.array(test_label)\n    \n    return train_gen,test_gen,test_data,test_label","7cfeb79e":"# Hyperparameters\nepoch=10\nbatch_size=64\nimg_dims=150\n\n#Get the data\ntrain_gen,test_gen,test_data,test_label=process_data(img_dims,batch_size)","ae678ae2":"#Input\ninputs=Input(shape=(img_dims,img_dims,3))\n\nx=Conv2D(filters=16,kernel_size=(3,3),activation='relu',padding='same')(inputs)\nx=Conv2D(filters=16,kernel_size=(3,3),activation='relu',padding='same')(x)\n\n# Second conv block\nx = SeparableConv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = SeparableConv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = BatchNormalization()(x)\nx = MaxPool2D(pool_size=(2, 2))(x)\n\n# Third conv block\nx = SeparableConv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = SeparableConv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = BatchNormalization()(x)\nx = MaxPool2D(pool_size=(2, 2))(x)\n\n# Fourth conv block\nx = SeparableConv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = SeparableConv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = BatchNormalization()(x)\nx = MaxPool2D(pool_size=(2, 2))(x)\nx = Dropout(rate=0.2)(x)\n\n# Fifth conv block\nx = SeparableConv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = SeparableConv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = BatchNormalization()(x)\nx = MaxPool2D(pool_size=(2, 2))(x)\nx = Dropout(rate=0.2)(x)\n\n#Fully connected layer\nx=Flatten()(x)\nx = Dense(units=512, activation='relu')(x)\nx = Dropout(rate=0.7)(x)\nx = Dense(units=128, activation='relu')(x)\nx = Dropout(rate=0.5)(x)\nx = Dense(units=64, activation='relu')(x)\nx = Dropout(rate=0.3)(x)\n\n#Output layer\noutput=Dense(1,activation='sigmoid')(x)\n\n#Model creation & compiling\nmodel=Model(inputs=inputs,outputs=output)\nmodel.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n\n#Callback\ncheckpoint = ModelCheckpoint(filepath='best_weights.hdf5', save_best_only=True, save_weights_only=True)\nlr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=2, verbose=2, mode='max')\nearly_stop = EarlyStopping(monitor='val_loss', min_delta=0.1, patience=1, mode='min')","0fe75d40":"hist=model.fit_generator(train_gen,steps_per_epoch=train_gen.samples\/\/batch_size,\n              epochs=epoch,validation_data=test_gen,validation_steps=test_gen.samples\/\/batch_size,\n              callbacks=[checkpoint,lr_reduce])","9e7771fd":"print(hist.set_params)","d2d32176":"fig, ax = plt.subplots(1, 2, figsize=(10, 3))\nax = ax.ravel()\n\nfor i, met in enumerate(['accuracy', 'loss']):\n    ax[i].plot(hist.history[met])\n    ax[i].plot(hist.history['val_' + met])\n    ax[i].set_title('Model {}'.format(met))\n    ax[i].set_xlabel('epochs')\n    ax[i].set_ylabel(met)\n    ax[i].legend(['train', 'val'])","d6ca150d":"from sklearn.metrics import accuracy_score, confusion_matrix\n\npreds = model.predict(test_data)\n\nacc = accuracy_score(test_label, np.round(preds))*100\ncm = confusion_matrix(test_label, np.round(preds))\ntn, fp, fn, tp = cm.ravel()\n\nprint('CONFUSION MATRIX ------------------')\nprint(cm)\n\nprint('\\nTEST METRICS ----------------------')\nprecision = tp\/(tp+fp)*100\nrecall = tp\/(tp+fn)*100\nprint('Accuracy: {}%'.format(acc))\nprint('Precision: {}%'.format(precision))\nprint('Recall: {}%'.format(recall))\nprint('F1-score: {}'.format(2*precision*recall\/(precision+recall)))\n\nprint('\\nTRAIN METRIC ----------------------')\nprint('Train acc: {}'.format(np.round((hist.history['accuracy'][-1])*100, 2)))","98204873":"# Building CNN model","fe554836":"# Fitting the model","3e59e854":"# Validation performance","cefab287":"**Setting the seed to run the same set of data at every run**","d3a6aa09":"# Train & Test data"}}