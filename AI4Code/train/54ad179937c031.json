{"cell_type":{"1fb1ead2":"code","8c438c36":"code","9d6d7046":"code","0554913f":"code","0bc6b4c3":"code","8e4a32bf":"code","89bc261a":"code","1cd2915a":"code","eb152fc4":"code","12475a5f":"markdown","95911bb0":"markdown","9d6f7fc7":"markdown","aebb6162":"markdown","309099bd":"markdown"},"source":{"1fb1ead2":"# load popular packages\nimport numpy as np, pandas as pd, os, matplotlib.pyplot as plt,re, regex, string, pprint\nimport unicodedata, sys, pickle\nfrom pprint import pprint\nfrom time import time\nfrom tqdm import tqdm\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom gensim.models import Word2Vec, KeyedVectors\nimport seaborn as sns\n# import boto3, os\n# s3 = boto3.resource('s3')\npd.set_option('display.max_rows', 40)\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_colwidth', 500)\npd.set_option('display.width', 10000)\nnp.set_printoptions(linewidth=1000, precision=4, edgeitems=20, suppress=True)\ntqdm.pandas(mininterval=3)\n\nclass Timer():\n    from time import time\n    def __init__(self):\n        self.is_localPC = os.path.isdir('C:\/Users\/Oleg Melnikov\/Downloads\/')\n        # self.is_interactive = 'SHLVL' not in os.environ\n        self.set()\n    def set(self): self.t0 = time()\n    def show(self, msg='', reset=True):\n        print(f'{time() - self.t0:.1f} {msg}')\n        if reset: self.set()\n\ndef display_local_variables(msg='', topN=5, lcl=locals().items()):\n    def sizeof_fmt(num, suffix='B'): #https:\/\/stackoverflow.com\/a\/1094933\/1870254\n        for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n            if abs(num) < 1024.0: return \"%3.1f%s%s\" % (num, unit, suffix)\n            num \/= 1024.0\n        return \"%.1f%s%s\" % (num, 'Yi', suffix)\n\n    if msg!='': print(msg)\n    for var, sz in sorted(((k, sys.getsizeof(v)) for k,v in lcl), key= lambda x: -x[1])[:topN]:\n        print(\"{:>30}: {:>8}\".format(var,sizeof_fmt(sz)))\n# !pip install paramiko\n# print(os.listdir(\"..\/input\"))","8c438c36":"# class for abstracting away loading and basic operations on word embeddings (from different providers)\nclass Emb():\n    def __init__(self, emb_dir=''):\n        self.emb_dir = emb_dir\n        self.X = None\n        self.KeyedVectors = False\n        self._isLower = None\n\n    def _readEmb_txt2dict(self, filename='glove.6B.50d.txt'):\n        # read embeddings from a text file. Store as dictionary\n        from tqdm import tqdm\n        assert filename[-4:]=='.txt'\n        def get_coefs(word, *arr):\n            return word, np.asarray(arr, dtype='float32')  # word followed by word vector\n        with open(self.emb_dir+filename, 'r', encoding=\"utf8\") as f:\n            self.X = dict(get_coefs(*line.strip().split(' ')) for line in tqdm(f, mininterval=1))\n        self.KeyedVectors = False\n        return self\n\n    def _readEmb_pickle2dict(self, filename='glove.840B.300d.pkl'):\n        # read embeddings from a pkl file (fast!). Store as dictionary\n        assert filename[-4:] == '.pkl'\n        with open(self.emb_dir+filename, 'rb') as f:\n            self.X = pickle.load(f)\n        self.KeyedVectors = False\n        return self\n\n    def _readEmb_bin2KeyedVectors(self, filename='GoogleNews-vectors-negative300.bin'):\n        # read embeddings from a binary file. Store as gensim.models.keyedvectors.Word2VecKeyedVectors\n        assert filename[-4:]=='.bin'\n        from gensim.models import KeyedVectors\n\n        self.X = KeyedVectors.load_word2vec_format(self.emb_dir + filename, binary=True)\n\n        self.KeyedVectors = True\n        return self.X\n\n    def readEmb(self, filename):\n        # Different type of word embedding file requires different loading procedure\n        if filename[-4:]=='.txt':\n            self._readEmb_txt2dict(filename)\n        elif filename[-4:]=='.pkl':\n            self._readEmb_pickle2dict(filename)\n        elif filename[-4:]=='.bin':  # binary file from Gensim word2vec (up to 3 million words)\n            self._readEmb_bin2KeyedVectors(filename)\n        else:\n            print('unknown file type. Must be .txt, .pkl, or .bin')\n            return None\n\n        self._lowerCount = None  # reset for the freshly loaded embedding\n        # self.info()\n        return self\n\n    def isLower(self):\n        if self._isLower is None:  # this check takes a few sec, so cache the results\n            words = set(self.vocab())\n            words_lower = set(w.lower() for w in words)\n            self._lowerCount = len(words_lower)\n            self._isLower = len((words - words_lower)) == 0  # whether embedding uses strictly lower case\n        return self._isLower\n\n    def lowerCount(self):  # returns count of strictly lower-cased words\n        self.isLower()  # also counts lower case words\n        return self._lowerCount\n\n    def valueType(self):\n        if self.KeyedVectors:\n            print(type(self.X))\n            vec = self.X[next(iter(self.X.vocab))]\n        else:\n            vec = next(iter(self.X.values()))  # get any one vector\n        return type(vec[0])\n\n    def shape(self):\n        if self.KeyedVectors:\n            return (len(self.X.index2word), self.X.vector_size)\n        else:\n            vec = next(iter(self.X.values()))  # get any one vector\n            return (len(self.X), len(vec))\n\n    def vocab(self, numWords=-1, castAs=set, seed=None):\n        words = self.X.vocab.keys() if self.KeyedVectors else self.X.keys()\n        if numWords >=0:\n            np.random.seed(seed)\n            words = np.random.choice(list(words), size=numWords)\n        return np.array(list(words)) if castAs==np.array else castAs(words)\n\n    def info(self, numWords=10):\n        from pprint import pprint\n        if self.X is None:\n            print('embedding is not loaded')\n        else:\n            caseMsg = \"lower\" if self.isLower() else \"varying\"\n            n,m = self.shape()\n            print(f'{n} words, {self.lowerCount()} in lower case, {m} embedding size, {self.valueType()} type')\n            pprint(f'{numWords} random word vectors:')\n            pprint({k: self.X[k] for k in np.random.choice(list(self.vocab()), size=numWords)})\n\n    def to_frame(self, wordsAsRows=True):\n        import pandas as pd\n        if not self.KeyedVectors:\n            return pd.DataFrame.from_dict(self.X, orient=('index' if wordsAsRows else 'columns'))\n\n    def findWords(self, lstToFind, ignoreCase=True, wholeWord=True, isRegEx=False, asDF=True):\n        vocab = vocab_orig = self.vocab(castAs=np.array)\n\n        if ignoreCase:\n            lstToFind = np.char.lower(lstToFind)\n            vocab = np.array([w.lower() for w in vocab_orig])\n\n        if wholeWord and not isRegEx:\n            found_lists = [[vocab == w] for w in lstToFind]\n            # found = vocab_orig[ np.any([[vocab == w] for w in lstToFind], 0)[0]]\n        elif not isRegEx:\n            found_lists = [[np.core.defchararray.find(vocab, w) != -1] for w in lstToFind]\n            # found = vocab_orig[ np.any( [[np.core.defchararray.find(vocab, w) != -1] for w in lstToFind],0)[0]]\n        else:\n            import re\n            flags = re.IGNORECASE if ignoreCase else 0\n            found_lists = [[bool(re.match(w, x, flags=flags)) for x in vocab] for w in lstToFind]\n            # found = vocab_orig[np.any([[re.match(w, x, flags=flags) is not None for x in vocab] for w in lstToFind], 0)]\n        found = vocab_orig[ np.any( found_lists,0).flatten()]\n\n        if len(found)>0:\n            emb = np.concatenate([self.X[w][:, None] for w in found], 1, out=None).T\n        else:\n            emb = np.array([])\n        if asDF:\n            return pd.DataFrame(emb, found)\n        return found, emb\n\n    def cosine_similarity(self, lstToFind, ignoreCase=True, wholeWord=True, isRegEx=False, k2plot=100, k2annot=25, clustermap=False):\n        '''\n\n        :param lstToFind:\n        :param ignoreCase:\n        :param wholeWord:\n        :param k2plot: add a plot if fewer than k words are found (to avoid overplotting\n        :param clustermap: whether the heatmap should be orderd by words or clustered by similarity values\n        :param k2annot: add annotations if fewer than k words are found (to avoid overplotting\n        :return:\n        '''\n        words, mtx = self.findWords(lstToFind, ignoreCase=ignoreCase, wholeWord=wholeWord, isRegEx=isRegEx, asDF=False)\n\n        if len(mtx)==0:\n            print(f'No match found.')\n            return None\n        dfSim = pd.DataFrame(cosine_similarity(mtx), index=words, columns=words)\n\n        ann = len(words) < k2annot\n        plot = len(words)<k2plot\n        col=\"YlGnBu\"\n\n        if plot:\n            import seaborn as sns\n            if clustermap:\n                sns.clustermap(dfSim, cmap=col, xticklabels=words, yticklabels=words, annot=ann).fig.suptitle('Cosine similarities')\n            else:\n                import matplotlib.pyplot as plt\n                # cols_lower = [w.lower() for w in dfSim.columns]\n                # cols_sorted = [x for _, x in sorted(zip(dfSim.columns, cols_lower))]\n                # dfSim['order']=cols_lower\n                # dfSim.sort_values('order', inplace=True)\n                # dfSim.drop('order',1,inplace=True)\n                # dfSim = dfSim.reindex(cols_lower, axis=1)\n                # dfSim.sort_values(by=cols_lower, ascending=False)\n                # dfSim.sort_index(inplace=True)\n                ax = plt.axes()\n                sns.heatmap(dfSim, cmap=col, xticklabels=words, yticklabels=words, annot=ann)\n                ax.set_title('Cosine similarities')\n                plt.show()\n        return dfSim\n\n    def rankSynonyms(self, w, synLst):\n        if not w in self.X:\n            print(f'{w} is not found in word embedding')\n        synLst = [w for w in synLst if w in self.X]\n        synVecs = [self.X[v] for v in synLst]\n\n        cosSim = lambda x,y: (x @ y) \/ np.linalg.norm(x)\/ np.linalg.norm(y)\n        cosSim1 = lambda y: cosSim(self.X[w],y)\n        return  pd.DataFrame(map(cosSim1, synVecs), index=synLst, columns=[w]).sort_values(w, ascending=False)","9d6d7046":"# Lists of common words (tokens).\nsectors = ['Energy','Telecommunication', 'Industrial','Industrials',\n           'Financial','Financial_Services', 'Consumer_Staples',\n            'Utility', 'Utilities', 'Materials', 'Consumer_Discretionary',\n           'Telecommunication Services', 'Health_Care', 'Information_Technology']\n\ncontinents=('North_America','Asia','Europe','Australia','South_America','Africa','SOUTH_AMERICA','NORTH_AMERICA')\n\ncountries=('United_Kingdom','United_States','Japan','China','Germany','Switzerland','Spain','Belarus',\n           'Philippines','United_Arab_Emirates','Ireland','France','India','Thailand','Ukraine',\n           'Taiwan','Netherlands','Russia','Belgium','Australia','Canada','Bahrain','Hong_Kong',\n           'Turkey','Saudi_Arabia','Mexico','Greece','Malaysia','South_Korea','Chile','Jordan',\n           'Luxembourg','Bermuda','Sweden','Italy','Morocco','Portugal','Brazil','Colombia',\n           'Venezuela','Lebanon','Indonesia','Israel','Oman','Peru','South_Africa','Singapore',\n           'Denmark','Argentina','Czech_Republic','Vietnam','Qatar','Egypt','Nigeria','Norway',\n           'Austria','Finland','Poland','Pakistan','Mongolia','Kuwait','Hungary','Puerto_Rico',\n           'Kazakhstan','Togo','Mauritius','Cayman_Islands','Panama','Channel_Island','Liberia','New_Zealand')\n\nindustries=('Aerospace_Defense','AEROSPACE_DEFENSE','catalogers_retailers','cataloger_retailer',\n            'Consumer_Finances','CONSUMER_FINANCE','CONSUMER_FINANCE_INDUSTRY','THE_CONSUMER_FINANCE',\n            'Conglomerates','Regional_Banks','Financial_Services',\n            'Thrifts_&_Mortgage_Finance','Pharmaceuticals',\n            'Other_Transportation','Computer_Services','Construction_Services','Hotels_&_Motels',\n            'Business_&_Personal_Services','Apparel\/Accessories','Software_&_Programming',\n            'Specialty_Stores','Telecommunications_services','Semiconductors','Diversified_Insurance',\n            'Discount_Stores','Electric_Utilities','Managed_Health_Care','Investment_Services',\n            'Life_&_Health_Insurance','Real_Estate','Electronics','Specialized_Chemicals','Airline',\n            'Auto_&_Truck_Parts','Food_Processing','Diversified_Chemicals','Aluminum','Biotechs',\n            'Property_&_Casualty_Insurance','Diversified_Metals_&_Mining','Tobacco',\n            'Internet_&_Catalog_Retail','Containers_&_Packaging',#'Consumer_Financial_Services',\n            'Diversified_Utilities','Electrical_Equipment','Oil_&_Gas_Operations','Iron_&_Steel',\n            'Beverages','Construction_Materials','Major_Banks','Insurance_Brokers','Computer_Hardware',\n            'Other_Industrial_Equipment','Rental_&_Leasing','Communications_Equipment',\n            'Aerospace_&_Defense','Recreational_Products','Oil_Services_&_Equipment',\n            'Medical_Equipment_&_Supplies','Household\/Personal_Care','Natural_Gas_Utilities',\n            'Apparel\/Footwear_Retail','Computer_&_Electronics_Retail','Auto_&_Truck_Manufacturers',\n            'Broadcasting_&_Cable','Railroads','Business_Products_&_Supplies','Food_Retail',\n            'Heavy_Equipment','Healthcare_Services','Trading_Companies','Restaurants','Casinos_&_Gaming',\n            'Printing_&_Publishing','Advertising','Air_Courier','Trucking','Department_Stores',\n            'Household_Appliances','Home_Improvement_Retail','Consumer_Electronics','Security_Systems',\n            'Paper_&_Paper_Products','Furniture_&_Fixtures','Computer_Storage_Devices',\n            'Environmental_&_Waste','Drug_Retail','Precision_Healthcare_Equipment')","0554913f":"emb50=Emb(\"..\/input\/nlpword2vecembeddingspretrained\/\").readEmb('glove.6B.50d.txt')","0bc6b4c3":"emb50.cosine_similarity(['python','programming'], ignoreCase=True, wholeWord=False, isRegEx=False, k2annot=5)","8e4a32bf":"# loads a 300-dimensional Word2Vec embedding from a text file (from Google). Takes ~1-2 minute\nemb300wv=Emb(\"..\/input\/nlpword2vecembeddingspretrained\/\").readEmb('GoogleNews-vectors-negative300.bin')","89bc261a":"emb300wv.cosine_similarity(['citi[_].*'], ignoreCase=True, wholeWord=False, isRegEx=True)","1cd2915a":"# emb300wv.rankSynonyms('Chase', sectors).plot.barh(grid=True)\nemb300wv.rankSynonyms('Microsoft', ['countries']).plot.barh(grid=True)\n# emb300wv.rankSynonyms('Google', countries).plot.barh(grid=True)\n# emb300wv.rankSynonyms('JPMOrgan_Chase', industries).plot.barh(grid=True)\n# emb300wv.rankSynonyms('Aboitiz', industries).plot.barh(grid=True)\n# emb300wv.rankSynonyms('Aboitiz', industries).plot.barh(grid=True)\n# emb300wv.rankSynonyms('Agile_Property', industries).plot.barh(grid=True)\n# emb300wv.rankSynonyms('Alexion', industries).plot.barh(grid=True)\n# emb300wv.rankSynonyms('Alexion_Pharmaceuticals', industries).plot.barh(grid=True)\n# emb300wv.rankSynonyms('AmerisourceBergen', industries).plot.barh(grid=True)\n# emb300wv.rankSynonyms('Chugoku', industries).plot.barh(grid=True) \n# emb300wv.rankSynonyms('Chugoku', countries).plot.barh(grid=True) \n# emb300wv.rankSynonyms('Unibanco', countries).plot.barh(grid=True) \n# emb300wv.rankSynonyms('Jazz_Pharmaceuticals', countries).plot.barh(grid=True) \n# emb300wv.rankSynonyms('Jeronimo_Martins', countries).plot.barh(grid=True)\n\n# emb300wv.rankSynonyms('JetBlue_Airways', countries).plot.barh(grid=True) \n# emb300wv.rankSynonyms('JetBlue', countries).plot.barh(grid=True) \n# emb300wv.rankSynonyms('JetBlue_Airways_Corp.', countries).plot.barh(grid=True) \n# emb300wv.rankSynonyms('Jetblue', countries).plot.barh(grid=True) \n# emb300wv.rankSynonyms('Jetblue_Airways_Corp', countries).plot.barh(grid=True) \n# emb300wv.rankSynonyms('JetBlue_Airways_Nasdaq_JBLU', countries).plot.barh(grid=True) \n# emb300wv.rankSynonyms('www.jetblue.com', countries).plot.barh(grid=True)\n\n# emb300wv.rankSynonyms('Putin', countries).plot.barh(grid=True) \n# emb300wv.rankSynonyms('Putin', industries).plot.barh(grid=True) ","eb152fc4":"sectors","12475a5f":"Some operations to search for a company (or any word, in general) in the word embedding.","95911bb0":"Here we compute a matrix of cosine similarities among all given words (the search for these words in embedding matrix allows varying casing, whole word match or not, and regex search). ","9d6f7fc7":"Here are some operations you can do to identify the industry of the company. You provide a list of industries and rank them according to their similarity to the company name you provide. Keep in mind that casing can make a difference. Also, company names with multiple words,or symbols: &, (, ',...), need to be in the embedding. So, if not getting any results, search the embedding for the company name to make sure it is there.","aebb6162":"Load a 50-dimensional [GloVe embedding](https:\/\/nlp.stanford.edu\/projects\/glove\/) from a text file (from Stanford). It has 400K words (tokens) in lower case. It's the smalest embedding from GloVe with the file size of 170MB.","309099bd":"Now, let's load a larger, 300-dimensional, [Word2Vec embedding](https:\/\/radimrehurek.com\/gensim\/models\/word2vec.html) from a binary file (from Google). It has 3M words (tokens) in varying casing. It is among the largest embeddings, ~4GB."}}