{"cell_type":{"f97b40cc":"code","34d8c8db":"code","28da81cf":"code","8d6f47d7":"code","982aef47":"code","a466c78e":"code","83057d13":"code","02a98f5a":"code","1055024c":"code","5679cf3b":"code","2b815dce":"code","64094b0e":"code","8c403348":"code","f1f752f2":"code","d4f5e4d4":"code","77234a25":"code","6c9afb67":"code","70c946bd":"code","6f992337":"code","3e9e8c87":"code","8dd84912":"code","5ba73022":"code","84e35c5e":"code","e04581ea":"markdown","3af60e1c":"markdown","bce0fee4":"markdown","133d0cba":"markdown","df8e1f2f":"markdown","61e0a879":"markdown","65f6f56e":"markdown","ff75ba57":"markdown","441aed69":"markdown","8185351c":"markdown","0fd64036":"markdown","c79b0bcd":"markdown","1af21af5":"markdown","31a3ad3f":"markdown","372a57f9":"markdown","811db163":"markdown","dba5bcba":"markdown","2a90ebe4":"markdown","74e73b4c":"markdown"},"source":{"f97b40cc":"import os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom bs4 import BeautifulSoup\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport matplotlib.patches as mpatches","34d8c8db":"# Define Directory \nimages_directory = \"..\/input\/simple-object-detection\/datasets\/images\"\nannotations_directory=\"..\/input\/simple-object-detection\/datasets\/annotations\"\n\nimg_paths = sorted([os.path.join(images_directory, fname) for fname in os.listdir(images_directory) if fname.endswith(\".jpg\")])\nlabel_paths = sorted([os.path.join(annotations_directory, fname) for fname in os.listdir(annotations_directory) if fname.endswith(\".xml\")])\n\nk = len(img_paths)\n\ndata_list = []\n\nfor i in range(k):  \n    annotation_file=label_paths[i]\n    ds = BeautifulSoup(open(annotation_file).read(), \"html.parser\")\n\n    # Iterating over each object elements\n    for o in ds.find_all(\"object\"):\n        \n        x_min = max(0, int(float(o.find(\"xmin\").string)))\n        y_min = max(0, int(float(o.find(\"ymin\").string)))\n        x_max = min(int(ds.find(\"width\").string), int(float(o.find(\"xmax\").string)))\n        y_max = min(int(ds.find(\"height\").string), int(float(o.find(\"ymax\").string)))\n        \n        # in case the boundary goes above its limis, providing some restrictions.\n        if x_min >= x_max or y_min >= y_max:\n            continue\n        elif x_max <= x_min or y_max <= y_min:\n            continue\n        \n        sample = [str(img_paths[i]), x_min, y_min, x_max, y_max]\n        \n        data_list.append(sample)\n        \ndata = pd.DataFrame(data_list)","28da81cf":"data.head()","8d6f47d7":"img = plt.imread(data[0][0])\nplt.imshow(img)\nprint('xmin', data[1][0], 'ymin',data[2][0],'xmax',data[3][0],'ymax',data[4][0])","982aef47":"print(img.shape)","a466c78e":"width=5\nheight=5\nrows = 3\ncols = 3\naxes=[]\nfig=plt.figure()\nfor a in range(rows*cols):\n    c = np.random.randint(100)\n    img = plt.imread(data[0][c])\n    axes.append( fig.add_subplot(rows, cols, a+1) )\n    subplot_title=(\"Subplot\"+str(a))\n    axes[-1].set_title(subplot_title)  \n    plt.imshow(img)\nfig.tight_layout()    \nplt.show()","83057d13":"sns.histplot(data[1])","02a98f5a":"sns.histplot(data[2])","1055024c":"sns.histplot(data[3])","5679cf3b":"sns.histplot(data[4])","2b815dce":"def cm(xmin, xmax):\n  return (xmin+xmax)\/2\n\ndata[['cx']] = data.apply(lambda r:cm(r[1],r[3]), axis=1) \ndata[['cy']] = data.apply(lambda r:cm(r[2],r[4]), axis=1) ","64094b0e":"sns.scatterplot(data=data, x=\"cx\", y=\"cy\")","8c403348":"def cm(xmin, ymin, xmax, ymax):\n    w = xmax - xmin\n    h = ymax - ymin \n    return (h+w)\/2\n\ndata[['size']] = data.apply(lambda r:cm(r[1],r[2],r[3],r[4]), axis=1) ","f1f752f2":"sns.histplot(data['size'])","d4f5e4d4":"sns.scatterplot(data=data, x=\"cx\", y=\"cy\", size = 'size', sizes=(0, 224))","77234a25":"def proc_image(path, xmin, ymin, xmax, ymax):\n    image = tf.io.read_file(path)\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.cast(image, tf.float64)\n    return image\/255.0, (xmin, ymin, xmax, ymax)","6c9afb67":"np.random.seed(127)\nbs = 16 #batch size\nvalid_mask = np.random.rand(len(data)) < 0.2\nval = data[valid_mask]\ntrain = data[~valid_mask]","70c946bd":"dataset = tf.data.Dataset.from_tensor_slices((train[0].values, train[1].values, train[2].values ,train[3].values, train[4].values))\ndataset = dataset.map(proc_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\ndataset = dataset.shuffle(241).cache().repeat().batch(bs).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n\ndataset_valid = tf.data.Dataset.from_tensor_slices((val[0].values, val[1].values, val[2].values, val[3].values, val[4].values))\ndataset_valid = dataset_valid.map(proc_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\ndataset_valid = dataset_valid.shuffle(241).cache().batch(bs).repeat().prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n\nsteps_per_epoch_train = len(train) \/\/ bs\nvalidation_steps = len(val) \/\/ bs","6f992337":"tf.keras.backend.clear_session()\ndatasetGen = iter(dataset)\nbatch2 = next(datasetGen)\nimg = np.array(batch2[0][0],dtype='float32')\n# print(np.array(batch2[1],dtype='float32'))\nplt.figure(figsize=(6,6))\nplt.imshow(img[:,:,0],cmap='gray')","3e9e8c87":"# Model Setup\nx0 = tf.keras.Input(shape=(224,224,3))\nx = tf.keras.layers.Conv2D(filters=64,kernel_size=(2,2), activation=tf.nn.relu)(x0)\nx = tf.keras.layers.Conv2D(filters=32,kernel_size=(3,3), activation=tf.nn.relu)(x)\nx = tf.keras.layers.Conv2D(filters=32,kernel_size=(4,4), activation=tf.nn.relu)(x)\nx = tf.keras.layers.Conv2D(filters=32,kernel_size=(5,5), activation=tf.nn.relu)(x)\nx = tf.keras.layers.Conv2D(filters=16,kernel_size=(6,6), activation=tf.nn.relu)(x)\nx = tf.keras.layers.Conv2D(filters=16,kernel_size=(7,7), activation=tf.nn.relu)(x)\nx = tf.keras.layers.Conv2D(filters=64,kernel_size=(8,8), activation=tf.nn.relu)(x)\nx = tf.keras.layers.Conv2D(filters=64,kernel_size=(9,9), activation=tf.nn.relu)(x)\nx = tf.keras.layers.Conv2D(filters=32,kernel_size=(10,10), activation=tf.nn.relu)(x)\nx = tf.keras.layers.Conv2D(filters=32,kernel_size=(15,15), activation=tf.nn.relu)(x)\nx = tf.keras.layers.Conv2D(filters=32,kernel_size=(20,20), activation=tf.nn.relu)(x)\nx = tf.keras.layers.Conv2D(filters=32,kernel_size=(25,25), activation=tf.nn.relu)(x)\nx = tf.keras.layers.Conv2D(filters=16,kernel_size=(30,30), activation=tf.nn.relu)(x)\nx = tf.keras.layers.Conv2D(filters=16,kernel_size=(40,40), activation=tf.nn.relu)(x)\nx = tf.keras.layers.Conv2D(filters=16,kernel_size=(45,45), activation=tf.nn.relu)(x)\nx = tf.keras.layers.Conv2D(filters=16,kernel_size=(5,5), activation=tf.nn.relu)(x)\nx = tf.keras.layers.GlobalMaxPooling2D()(x)\n\nx = tf.keras.layers.Dense(256, activation=tf.nn.relu)(x)\nx =tf.keras.layers.Dropout(0.2)(x)\n\nx1 = tf.keras.layers.Dense(64, activation=tf.nn.relu)(x)\nx1 =tf.keras.layers.Dropout(0.2)(x1)\ny1 = tf.keras.layers.Dense(1, activation=\"linear\")(x1)\n\nx2 = tf.keras.layers.Dense(64, activation=tf.nn.relu)(x)\nx2 =tf.keras.layers.Dropout(0.2)(x2)\ny2 = tf.keras.layers.Dense(1, activation=\"linear\")(x2)\n\nx3 = tf.keras.layers.Dense(64, activation=tf.nn.relu)(x)\nx3 =tf.keras.layers.Dropout(0.2)(x3)\ny3 = tf.keras.layers.Dense(1, activation=\"linear\")(x3)\n\nx4 = tf.keras.layers.Dense(64, activation=tf.nn.relu)(x)\nx4 =tf.keras.layers.Dropout(0.2)(x4)\ny4 = tf.keras.layers.Dense(1, activation=\"linear\")(x4)\n\nmodel = tf.keras.Model(inputs=x0, outputs=[y1, y2, y3, y4])\nmodel.compile(tf.keras.optimizers.Adam(lr=0.0001),loss=['mse','mse','mse','mse'])\nmodel.summary()","8dd84912":"hist = model.fit(dataset,epochs = 50,steps_per_epoch=steps_per_epoch_train, validation_data=dataset_valid, validation_steps=validation_steps)","5ba73022":"plt.figure(figsize=(8,5))\nplt.plot(hist.history['loss'], label='loss')\nplt.plot(hist.history['val_loss'], label='val_loss')\nplt.grid(True)\nplt.legend()","84e35c5e":"path = \"..\/input\/simple-object-detection\/datasets\/images\/a (1).jpg\"\nimage = tf.io.read_file(path)\nimage = tf.image.decode_jpeg(image, channels=3)\nimage = tf.cast(image, tf.float64)\nimage = image\/255.0\nres = model.predict(tf.expand_dims(image,0))\n\nw = res[2] - res[0]\nh = res[3] - res[1]\nrect=mpatches.Rectangle((res[0],res[1]),w,h, fill = False,color = \"purple\",linewidth = 2)\nprint(res[0])\nplt.imshow(image)\nplt.gca().add_patch(rect)","e04581ea":"**4. Training**\n\nwe have trained this model upto 100 epoch but it could be trainned further to improve the accuracy bit more. here we have used mean squre error criteria for evaluation and tarining as problem seems bit of regression. ","3af60e1c":"x_max samples","bce0fee4":"above we see the different box sizes. where we have enough samples for each size to cover diversity in size. ","133d0cba":"**5.Visulisation**","df8e1f2f":"visulise data loader","61e0a879":"This notebook covers the following topics for single object detection problem. we have used lite weight datasets to analyze this problem with minimal setup. \n\n1. Data Analysis\n2. Data Loading\n3. Model Setup\n4. Training\n5. Visulisation ","65f6f56e":"here we can observer that samples have covered most of the detection region to establish the generalised approaximations in model training. it could have been better we apply some sort of augmentation to enahnce the diversity and varience in data","ff75ba57":"y_min samples","441aed69":"we can observe from the distributions that some locations are not having less sample than the other. lets visulise more clearly with center points observations","8185351c":"let's re visulise this distrution with size and locations","0fd64036":"**Training and Validation Split**\n\nit is required to split data between training and validation to avoid overfitting in model training. for the case we have split data between 80-20 where 80 percentage data is used for training and remaining 20 percentage is used for the validation.","c79b0bcd":"**Model Setup**\n\nlets create basic model for bounding box regression like problem with input size of 224x224 and output of 4 channel. the model is deigned with inceasing feature map size over each layer to improve the scope of detection. linear layer at end for fitting with bounding box number is producing quite amazing results for the regression case. ","1af21af5":"**box size distribution**","31a3ad3f":"x_min samples","372a57f9":"**2. Data Loading**\n\nnow its time to implemet efficeint data loading technic to boost the training time. tf.data seems the popular efficient approach for such task. applying it to our selected data.","811db163":"y_max samples","dba5bcba":"**1. Data Analysis**\n\nlet's disucss about the data first, how its distributed and what kind of balancing is required or not. plotting and data visulisation will help you to resolve this problem. the data should be well crafted to assist model training with adequate diversity to meet generlisation need. ","2a90ebe4":"**Visulisation**\n\nlet's visulise the data to see how it has been disctributed or balanced. ","74e73b4c":"**Transforming row data to dataframe**\n\nhere the data format for annotations are quit unsual than people used to so better to first convert this in suitable format for smoother operations in training."}}