{"cell_type":{"9fe7fe33":"code","42ac46fb":"code","86b5f24d":"code","0316d75a":"code","174cace5":"code","1ba16de8":"code","3b03315d":"code","14e44e4c":"code","496d5160":"code","d04cbf42":"code","200276b9":"code","1f29a81d":"code","a0829be8":"code","a5f8f2f0":"code","a43928e6":"code","97e29eef":"code","2e49a229":"code","5b2389c1":"code","91a26cc8":"code","5c663d9f":"code","7c2b6481":"code","969004fc":"code","e929d987":"code","83627300":"code","0d0e0511":"code","c7fd8918":"code","73f578cd":"code","970ae85e":"code","8ae78d66":"code","dce91ef9":"code","ddc64c9b":"code","0ae9bccb":"code","168043f6":"code","92cc2085":"code","f1603200":"code","c96182bf":"code","15464306":"code","f99f89e7":"code","bee6141b":"code","fb5bd722":"code","3dff15a7":"code","4ef6842b":"code","e03652c3":"code","68eca07d":"code","58d63f03":"code","c8254db1":"code","547240ec":"markdown","fa29b8f4":"markdown","064e45d0":"markdown","0c521bf1":"markdown","1e989762":"markdown","493f482e":"markdown","e11d555d":"markdown","620751ea":"markdown","705a3c16":"markdown","053a8fce":"markdown","7be144c4":"markdown","e81565e9":"markdown","b5e6dc4b":"markdown","83c881f0":"markdown","ddf795a7":"markdown"},"source":{"9fe7fe33":"import tarfile\nmy_tar = tarfile.open('..\/input\/california-house-price-prediction\/housing.tgz')\nmy_tar.extractall('.\/data') # specify which folder to extract to\nmy_tar.close()","42ac46fb":"import pandas as pd\nhousing =pd.read_csv('.\/data\/housing.csv')\nhousing","86b5f24d":"housing.info()","0316d75a":"housing['ocean_proximity'].value_counts()","174cace5":"housing.describe()","1ba16de8":"%matplotlib inline\nimport matplotlib.pyplot as plt\nhousing.hist(bins=50, figsize=(20,15))","3b03315d":"import numpy as np\nnp.random.seed(42)","14e44e4c":"import numpy as np\n\n# For illustration only. Sklearn has train_test_split()\ndef split_train_test(data, test_ratio):\n    shuffled_indices = np.random.permutation(len(data))\n    test_set_size = int(len(data) * test_ratio)\n    test_indices = shuffled_indices[:test_set_size]\n    train_indices = shuffled_indices[test_set_size:]\n    return data.iloc[train_indices], data.iloc[test_indices]\n","496d5160":"train_set, test_set = split_train_test(housing, 0.2)\nprint(len(train_set), \"train +\", len(test_set), \"test\")","d04cbf42":"from zlib import crc32\n\ndef test_set_check(identifier, test_ratio):\n    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32\n\ndef split_train_test_by_id(data, test_ratio, id_column):\n    ids = data[id_column]\n    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\n    return data.loc[~in_test_set], data.loc[in_test_set]\n# loc is label-based, which means that you have to specify rows and columns based on their row and column labels.\n# iloc is integer position-based, so you have to specify rows and columns by their integer position values \n# split train test\n# Also you can make longitude and latitude as unique feature","200276b9":"import hashlib\n\ndef test_set_check(identifier, test_ratio, hash=hashlib.md5):\n    return hash(np.int64(identifier)).digest()[-1] < 256 * test_ratio\ndef test_set_check(identifier, test_ratio, hash=hashlib.md5):\n    return bytearray(hash(np.int64(identifier)).digest())[-1] < 256 * test_ratio\n\nhousing_with_id = housing.reset_index()   # adds an `index` column\ntrain_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"index\")\n\nhousing_with_id[\"id\"] = housing[\"longitude\"] * 1000 + housing[\"latitude\"]\ntrain_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"id\")","1f29a81d":"from sklearn.model_selection import train_test_split\ntrain, test=train_test_split(housing,test_size=0.2,random_state=42)","a0829be8":"housing['income_cat']=pd.cut(housing['median_income'], bins=[0., 1.5,3.,4.5,6.,np.inf], labels=[1,2,3,4,5])\nhousing['income_cat'].hist()","a5f8f2f0":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n    strat_train_set = housing.loc[train_index]\n    strat_test_set = housing.loc[test_index]\n    \nstrat_test_set[\"income_cat\"].value_counts() \/ len(strat_test_set)\nhousing[\"income_cat\"].value_counts() \/ len(housing)\n\ndef income_cat_proportions(data):\n    return housing[\"income_cat\"].value_counts() \/ len(housing)\n\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n\ncompare_props = pd.DataFrame({\n    \"Overall\": income_cat_proportions(housing),\n    \"Stratified\": income_cat_proportions(strat_test_set),\n    \"Random\": income_cat_proportions(test_set),\n}).sort_index()\ncompare_props[\"Rand. %error\"] = 100 * compare_props[\"Random\"] \/ compare_props[\"Overall\"] - 100\ncompare_props[\"Strat. %error\"] = 100 * compare_props[\"Stratified\"] \/ compare_props[\"Overall\"] - 100","a43928e6":"for set_ in (strat_train_set, strat_test_set):\n    set_.drop(\"income_cat\", axis=1, inplace=True)","97e29eef":"housing = strat_train_set.copy()\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)","2e49a229":"plt.scatter(housing.longitude, housing.latitude, alpha =0.1)","5b2389c1":"housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n    s=housing[\"population\"]\/100, label=\"population\", figsize=(10,7),\n    c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n    sharex=False)\nplt.legend()\n","91a26cc8":"# Looking for Correlation\n\ncorr_matrix=housing.corr()\ncorr_matrix","5c663d9f":"# Another way to check correlation\n\nfrom pandas.plotting import scatter_matrix\nattributes=['median_house_value', 'median_income', 'total_rooms','housing_median_age']\nscatter_matrix(housing[attributes], figsize=(12,8))","7c2b6481":"\nhousing[\"rooms_per_household\"] = housing[\"total_rooms\"]\/housing[\"households\"]\nhousing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]\/housing[\"total_rooms\"]\nhousing[\"population_per_household\"]=housing[\"population\"]\/housing[\"households\"]\n#Looking correlation\ncorr_matrix=housing.corr()\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)","969004fc":"corr_matrix = housing.corr()\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)\nhousing.plot(kind=\"scatter\", x=\"rooms_per_household\", y=\"median_house_value\",\n             alpha=0.2)\nplt.axis([0, 5, 0, 520000])\nplt.show()","e929d987":"housing = strat_train_set.drop(\"median_house_value\", axis=1) # drop labels for training set\nhousing_labels = strat_train_set[\"median_house_value\"].copy()\n\nsample_incomplete_rows = housing[housing.isnull().any(axis=1)].head()\nsample_incomplete_rows\n\n\nsample_incomplete_rows.dropna(subset=[\"total_bedrooms\"])\nsample_incomplete_rows.drop(\"total_bedrooms\", axis=1)   \n\nmedian = housing[\"total_bedrooms\"].median()\nsample_incomplete_rows[\"total_bedrooms\"].fillna(median, inplace=True) # option 3\nsample_incomplete_rows","83627300":"try:\n    from sklearn.impute import SimpleImputer # Scikit-Learn 0.20+\nexcept ImportError:\n    from sklearn.preprocessing import Imputer as SimpleImputer\n\nimputer = SimpleImputer(strategy=\"median\")\n\nhousing_num = housing.drop('ocean_proximity', axis=1)\n\nhousing_num = housing.drop('ocean_proximity', axis=1)\nimputer.fit(housing_num)\nX = imputer.transform(housing_num)\nhousing_tr = pd.DataFrame(X, columns=housing_num.columns,\n                          index=housing.index)\n\nhousing_tr.loc[sample_incomplete_rows.index.values]\n\nhousing_tr = pd.DataFrame(X, columns=housing_num.columns,\n                          index=housing_num.index)\nhousing_tr.head()\n\nhousing_cat = housing[['ocean_proximity']]\nhousing_cat.head(10)","0d0e0511":"try:\n    from sklearn.preprocessing import OrdinalEncoder\nexcept ImportError:\n    from future_encoders import OrdinalEncoder \n    \nordinal_encoder = OrdinalEncoder()\nhousing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\nhousing_cat_encoded[:10]\nordinal_encoder.categories_\n\ntry:\n    from sklearn.preprocessing import OrdinalEncoder # just to raise an ImportError if Scikit-Learn < 0.20\n    from sklearn.preprocessing import OneHotEncoder\nexcept ImportError:\n    from future_encoders import OneHotEncoder # Scikit-Learn < 0.20\n\ncat_encoder = OneHotEncoder()\nhousing_cat_1hot = cat_encoder.fit_transform(housing_cat)\nhousing_cat_1hot\n\nhousing_cat_1hot.toarray()\n\ncat_encoder = OneHotEncoder(sparse=False)\nhousing_cat_1hot = cat_encoder.fit_transform(housing_cat)\nhousing_cat_1hot\n\nhousing.columns","c7fd8918":"from sklearn.base import BaseEstimator, TransformerMixin\n\n# get the right column indices: safer than hard-coding indices 3, 4, 5, 6\nrooms_ix, bedrooms_ix, population_ix, household_ix = [\n    list(housing.columns).index(col)\n    for col in (\"total_rooms\", \"total_bedrooms\", \"population\", \"households\")]\n\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, add_bedrooms_per_room = True): # no *args or **kwargs\n        self.add_bedrooms_per_room = add_bedrooms_per_room\n    def fit(self, X, y=None):\n        return self  # nothing else to do\n    def transform(self, X, y=None):\n        rooms_per_household = X[:, rooms_ix] \/ X[:, household_ix]\n        population_per_household = X[:, population_ix] \/ X[:, household_ix]\n        if self.add_bedrooms_per_room:\n            bedrooms_per_room = X[:, bedrooms_ix] \/ X[:, rooms_ix]\n            return np.c_[X, rooms_per_household, population_per_household,\n                         bedrooms_per_room]\n        else:\n            return np.c_[X, rooms_per_household, population_per_household]\n\nattr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\nhousing_extra_attribs = attr_adder.transform(housing.values)","73f578cd":"from sklearn.preprocessing import FunctionTransformer\n\ndef add_extra_features(X, add_bedrooms_per_room=True):\n    rooms_per_household = X[:, rooms_ix] \/ X[:, household_ix]\n    population_per_household = X[:, population_ix] \/ X[:, household_ix]\n    if add_bedrooms_per_room:\n        bedrooms_per_room = X[:, bedrooms_ix] \/ X[:, rooms_ix]\n        return np.c_[X, rooms_per_household, population_per_household,\n                     bedrooms_per_room]\n    else:\n        return np.c_[X, rooms_per_household, population_per_household]\n\nattr_adder = FunctionTransformer(add_extra_features, validate=False,\n                                 kw_args={\"add_bedrooms_per_room\": False})\nhousing_extra_attribs = attr_adder.fit_transform(housing.values)","970ae85e":"housing_extra_attribs = pd.DataFrame(\n    housing_extra_attribs,\n    columns=list(housing.columns)+[\"rooms_per_household\", \"population_per_household\"],\n    index=housing.index)\nhousing_extra_attribs.head()","8ae78d66":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nnum_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('attribs_adder', FunctionTransformer(add_extra_features, validate=False)),\n        ('std_scaler', StandardScaler()),\n    ])\n\nhousing_num_tr = num_pipeline.fit_transform(housing_num)\n\n\ntry:\n    from sklearn.compose import ColumnTransformer\nexcept ImportError:\n    from future_encoders import ColumnTransformer # Scikit-Learn < 0.20\n    \n\nnum_attribs = list(housing_num)\ncat_attribs = [\"ocean_proximity\"]\n\nfull_pipeline = ColumnTransformer([\n        (\"num\", num_pipeline, num_attribs),\n        (\"cat\", OneHotEncoder(), cat_attribs),\n    ])\n\nhousing_prepared = full_pipeline.fit_transform(housing)","dce91ef9":"from sklearn.base import BaseEstimator, TransformerMixin\n\n# Create a class to select numerical or categorical columns \nclass OldDataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names].values\n    \nnum_attribs = list(housing_num)\ncat_attribs = [\"ocean_proximity\"]\n\nold_num_pipeline = Pipeline([\n        ('selector', OldDataFrameSelector(num_attribs)),\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('attribs_adder', FunctionTransformer(add_extra_features, validate=False)),\n        ('std_scaler', StandardScaler()),\n    ])\n\nold_cat_pipeline = Pipeline([\n        ('selector', OldDataFrameSelector(cat_attribs)),\n        ('cat_encoder', OneHotEncoder(sparse=False)),\n    ])","ddc64c9b":"from sklearn.pipeline import FeatureUnion\n\nold_full_pipeline = FeatureUnion(transformer_list=[\n        (\"num_pipeline\", old_num_pipeline),\n        (\"cat_pipeline\", old_cat_pipeline),\n    ])\n\nold_housing_prepared = old_full_pipeline.fit_transform(housing)\nold_housing_prepared","0ae9bccb":"np.allclose(housing_prepared, old_housing_prepared)","168043f6":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(housing_prepared, housing_labels)","92cc2085":"\n# let's try the full preprocessing pipeline on a few training instances\nsome_data = housing.iloc[:5]\nsome_labels = housing_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\n\nprint(\"Predictions:\", lin_reg.predict(some_data_prepared))\n\nprint(\"Labels:\", list(some_labels))","f1603200":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\n\n\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse\n\n\nlin_mae = mean_absolute_error(housing_labels, housing_predictions)\nlin_mae","c96182bf":"from sklearn.tree import DecisionTreeRegressor\n\ntree_reg = DecisionTreeRegressor(random_state=42)\ntree_reg.fit(housing_prepared, housing_labels)\n\nhousing_predictions = tree_reg.predict(housing_prepared)\ntree_mse = mean_squared_error(housing_labels, housing_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse","15464306":"from sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n                         scoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-scores)","f99f89e7":"def display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())\n\ndisplay_scores(tree_rmse_scores)\n\nlin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n                             scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)","bee6141b":"from sklearn.ensemble import RandomForestRegressor\n\nforest_reg = RandomForestRegressor(n_estimators=10, random_state=42)\nforest_reg.fit(housing_prepared, housing_labels)\n\nhousing_predictions = forest_reg.predict(housing_prepared)\nforest_mse = mean_squared_error(housing_labels, housing_predictions)\nforest_rmse = np.sqrt(forest_mse)\nforest_rmse","fb5bd722":"from sklearn.model_selection import cross_val_score\n\nforest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n                                scoring=\"neg_mean_squared_error\", cv=10)\nforest_rmse_scores = np.sqrt(-forest_scores)\ndisplay_scores(forest_rmse_scores)","3dff15a7":"scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\npd.Series(np.sqrt(-scores)).describe()","4ef6842b":"from sklearn.svm import SVR\n\nsvm_reg = SVR(kernel=\"linear\")\nsvm_reg.fit(housing_prepared, housing_labels)\nhousing_predictions = svm_reg.predict(housing_prepared)\nsvm_mse = mean_squared_error(housing_labels, housing_predictions)\nsvm_rmse = np.sqrt(svm_mse)\nsvm_rmse","e03652c3":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    # try 12 (3\u00d74) combinations of hyperparameters\n    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n    # then try 6 (2\u00d73) combinations with bootstrap set as False\n    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n  ]\n\nforest_reg = RandomForestRegressor(random_state=42)\n# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n                           scoring='neg_mean_squared_error', return_train_score=True)\ngrid_search.fit(housing_prepared, housing_labels)","68eca07d":"grid_search.best_params_\ngrid_search.best_estimator_\ncvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)\n    \npd.DataFrame(grid_search.cv_results_)","58d63f03":"final_model = grid_search.best_estimator_\n\nX_test = strat_test_set.drop(\"median_house_value\", axis=1)\ny_test = strat_test_set[\"median_house_value\"].copy()\n\nX_test_prepared = full_pipeline.transform(X_test)\nfinal_predictions = final_model.predict(X_test_prepared)\n\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)","c8254db1":"final_rmse","547240ec":"# MAIN STEPS\n1. Look at the big picture for that particular topic on which you are going to apply Machine learning\n2. Get the data\n3. Analyse \/ Visualize the data you get.\n4. Prepare the Data to feed into Machine Learning algorithm\n5. Select a model based on the problem(Classification, regression etc.) and train it.\n6. Fine tune your model \n7. Deploy ","fa29b8f4":"Above code can cause test data different every time which covers all data points. Now because we want our test data different from training data. So we will use here **Hash VAlue**","064e45d0":"Here in data we have 10 attributes( 10 columns). \nWe get a quick description of data by info().\n","0c521bf1":"Next step is to do some visualizations. for that we will use matplotlib.\n","1e989762":"The data is extracted and is in output folder \nNow Load House Data using pandas:","493f482e":"# Visualize the data","e11d555d":"# Create test dataset","620751ea":"# Select and train over algorithm","705a3c16":"These graphs gives information and tells which attributes required rescaling, where is outliers etc.\n\nAfter that Split X(features), Y(label).","053a8fce":"# Fine tuning","7be144c4":"HERE we are going to choose California Housing Prices Dataset\n\nThis Dataset was based on 1990 California census. We are choosing it only for learning purpose.","e81565e9":"As Median_income is an important feature but have broad values mostly it lie bw 1.5->6 but some value go beyond 6. So we use pd.cut to create income category attribute","b5e6dc4b":"Here we can see that there are 20640 instances in our dataset. All attributes have 20640 values except total bedrooms which shows that it has 207 missing values. \n\nAlso all attributes are numerical except **ocean proximity**\n\nNow we will find categories in ocean proximity because it's not numerical. We will find categories by using **value_counts()**","83c881f0":"Here we do stratified sampling based on income category","ddf795a7":"There are 5 fields in ocean_proximity.\n\nNow, Looking at other fields:\n**Describe()** method shows summary of numerical attributes. \n\n*NOTE: Null values are ignored*"}}