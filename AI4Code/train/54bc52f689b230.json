{"cell_type":{"d0ec5630":"code","5ba2ea3c":"code","30fffb21":"code","5c3e5d7b":"code","bd2c99d9":"code","5a29355f":"code","6a03e3a5":"code","52448f99":"code","6975ff4f":"code","6e767fc5":"code","4db98f39":"code","929a92f2":"code","f46984d0":"code","08e528e7":"code","81af3172":"code","7c6f53d1":"code","9696b825":"code","07d1509c":"code","5f0e76df":"code","3f6e0156":"code","ccf02654":"code","59b9c36f":"code","77b593ed":"code","feb3f30f":"code","ad914ffc":"code","c5584e2e":"code","6cbce4e1":"code","dd1a2307":"code","de1b542d":"code","e3cc170c":"code","e82b6a1f":"code","bdc8b326":"code","c7840812":"code","70d61adc":"code","63236f0e":"code","09d3b753":"code","1add46f7":"code","8a7bf976":"code","942ace04":"code","ae4ec58e":"code","a051b3f9":"code","1733b435":"code","1febb18b":"code","2cd768ab":"code","9d8d1826":"code","01867aae":"code","b19e4a8e":"code","ffae5ebe":"code","b12071b2":"code","e06cd0df":"code","e9d18db0":"code","0ce3048f":"code","c4a4bfe0":"code","973cc91c":"code","21810762":"code","5239ddb3":"markdown","b832f1ae":"markdown","e7cde502":"markdown","b7b57bb9":"markdown","9eeef35d":"markdown","c1a6a884":"markdown","d457b41c":"markdown","cf1cbd69":"markdown","1ba60f86":"markdown","4d1ec1df":"markdown","59bfb6b2":"markdown","be8e6332":"markdown","3de35fa3":"markdown","96637cee":"markdown","0ef8a72c":"markdown","5c1c5f3c":"markdown","2ffe8c14":"markdown","6cf6512a":"markdown","4c53ccf8":"markdown","e31cebbc":"markdown"},"source":{"d0ec5630":"import os\nprint(os.listdir(\"..\/input\"))","5ba2ea3c":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport re, csv, string\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Bidirectional, GlobalMaxPool1D, Bidirectional\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nimport gensim\nimport gensim.models.keyedvectors as word2vec\nimport nltk.data\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer, PorterStemmer, SnowballStemmer\nfrom nltk.stem.lancaster import LancasterStemmer\nimport heapq\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nimport gc","30fffb21":"train = pd.read_csv('..\/input\/bad-comments\/train.csv')\ntrain.head(5)","5c3e5d7b":"train.isnull().any()","bd2c99d9":"train = train[:10000]","5a29355f":"train.drop(['id'], axis=1).describe()","6a03e3a5":"list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]","52448f99":"COMMENT = 'comment_text'\ntrain[COMMENT].fillna(\"unknown\", inplace=True)","6975ff4f":"def loadStemmer(stemmer):\n    choice = {'porter': PorterStemmer, 'lancaster': LancasterStemmer, 'snowball': SnowballStemmer}\n    return choice[stemmer]()\n","6e767fc5":"contractions = {\"isn't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\ncontractions_re = re.compile('(%s)' % '|'.join(contractions.keys()))","4db98f39":"def replace_contraction(match):\n    return contractions[match.group(0)]","929a92f2":"STOP_WORDS = set(stopwords.words('english'))","f46984d0":"def clean_data():\n    for i in range(train.shape[0]):\n        text = str(train.loc[i,'comment_text'])\n        text=text.lower()\n        text = contractions_re.sub(replace_contraction, text)\n        text = re.sub('[^A-Za-z]', ' ', text)\n        tokens = word_tokenize(text)\n        tokens = [ word for word in tokens if word not in STOP_WORDS ]\n        train.loc[i,'comment_text'] = ' '.join(tokens)","08e528e7":"clean_data()","81af3172":"model_google_news = gensim.models.KeyedVectors.load_word2vec_format('..\/input\/googlenewsvectorsnegative300\/GoogleNews-vectors-negative300.bin.gz', \n                                                        binary=True)","7c6f53d1":"words = model_google_news.index2word","9696b825":"def build_vocab(texts):\n    sentences = texts.apply(lambda x: x.split()).values\n    vocab = {}\n    for sentence in sentences:\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab","07d1509c":"vocab_train = build_vocab(train.comment_text)\n\nWORDS = {}\nfor i,word in enumerate(words):\n    WORDS[word] = i","5f0e76df":"def words(text): return re.findall(r'\\w+', text)\n\ndef frequency(word):\n    return - WORDS.get(word, 0)\n\ndef correction(word):\n    # \u0442\u043e\u043b\u044c\u043a\u043e \u0434\u043b\u044f \u0441\u043b\u043e\u0432, \u0434\u043b\u0438\u043d\u0430 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0431\u043e\u043b\u044c\u0448\u0435 1, \u0434\u043b\u044f \u0434\u0440\u0443\u0433\u0438\u0445 \u043d\u0435\u0442 \u0441\u043c\u044b\u0441\u043b\u0430 \u0432\u044b\u0437\u044b\u0432\u0430\u0442\u044c \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\n    if len(word)>1:\n        return max(candidates(word), key=frequency)\n    return word\n\ndef candidates(word):\n    return (known([word]) or known(edits(word)) or [word])\n\ndef known(words):\n    return set(w for w in words if w in WORDS)\n\ndef edits(word):\n    letters    = 'abcdefghijklmnopqrstuvwxyz'\n    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n    deletes    = [L + R[1:]               for L, R in splits if R]\n    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n    inserts    = [L + c + R               for L, R in splits for c in letters]\n    return set(deletes + replaces + inserts)","3f6e0156":"corrected_words_train = {}\nfor word in vocab_train:\n    corrected_word = correction(word)\n    if corrected_word != word:\n        corrected_words_train[word] = corrected_word","ccf02654":"print('humour',corrected_words_train['humour'])","59b9c36f":"print('centres',corrected_words_train['centres'])","77b593ed":"print('lner',corrected_words_train['lner'])","feb3f30f":"print('wulf',corrected_words_train['wulf'])","ad914ffc":"def replace_misspell(text, mispell_dict ):\n    text = str(text)\n    for mispell in mispell_dict.keys():\n        text=text.replace(mispell, mispell_dict[mispell])\n    return text","c5584e2e":"stemmer = loadStemmer('lancaster')\nlemmer = WordNetLemmatizer()","6cbce4e1":"def remove_misspellings_and_normalize():\n    for i in range(train.shape[0]):\n        text = str(train.loc[i,'comment_text'])\n        text = replace_misspell(text, corrected_words_train)\n        tokens = word_tokenize(text)\n        tokens = [stemmer.stem(w) for w in tokens]    \n        tokens = [lemmer.lemmatize(w) for w in tokens]\n        train.loc[i,'comment_text'] = ' '.join(tokens)","dd1a2307":"remove_misspellings_and_normalize()","de1b542d":"y_train = train[list_classes].values\nx_train = train[COMMENT]\nmax_features = 20000\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(x_train))\nx_train_seq = tokenizer.texts_to_sequences(x_train)","e3cc170c":"len(vocab_train.keys())","e82b6a1f":"maxlen = 300\nx_train_seq = pad_sequences(x_train_seq, maxlen=maxlen)","bdc8b326":"emb_size = 300","c7840812":"def creat_embedding_matrix(emb_type, emb_size, dict_model=None):\n        emb_size_real = emb_size\n        if emb_type==\"glove\":\n            EMBEDDING_FILE='..\/input\/glovetwitter27b100dtxt\/glove.twitter.27B.200d.txt'\n        elif emb_type==\"word2vec\":\n            word2vecDict = model_google_news\n        elif emb_type==\"fasttext\":\n            EMBEDDING_FILE='..\/input\/fasttext\/wiki.simple.vec'\n\n        emb_index = {}\n        if emb_type==\"glove\" or emb_type==\"fasttext\":\n            emb_file = open(EMBEDDING_FILE, encoding='utf-8')\n            for line in emb_file:\n                values = line.split()\n                word=values[0]\n                counter=1\n                # \u043c\u043e\u0436\u0435\u0442 \u0432\u0441\u0442\u0440\u0435\u0442\u0438\u0442\u044c\u0441\u044f \u0441\u043b\u043e\u0432\u043e\u0441\u043e\u0447\u0435\u0442\u0430\u043d\u0438\u0435, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u043c \u0434\u043e \u043f\u0435\u0440\u0432\u043e\u0433\u043e \u0444\u043b\u043e\u0442\u0430\n                while True:\n                    try:\n                        test= float(values[counter])\n                        break\n                    except ValueError:\n                        word+=f' {values[counter]}'\n                        counter+=1\n                        pass\n                word = values[0]\n                # \u0431\u0435\u0440\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u0442\u0435 \u0441\u043b\u043e\u0432\u0430, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0435\u0441\u0442\u044c \u0432 \u043d\u0430\u0448\u0438\u0445 \u0442\u0435\u043a\u0441\u0442\u0430\u0445\n                if word in vocab_train.keys():\n                    size_coefs = len(values[counter:emb_size+counter])\n                    # \u043f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u043c \u0441\u043e\u0432\u043f\u0430\u0434\u0430\u0435\u0442 \u043b\u0438 \u0437\u0430\u043f\u0440\u043e\u0448\u0435\u043d\u043d\u0430\u044f \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c \u0441 \u0440\u0435\u0430\u043b\u044c\u043d\u043e\u0439\n                    # \u0435\u0441\u043b\u0438 \u0437\u0430\u043f\u0440\u043e\u0448\u0435\u043d\u043d\u0430\u044f \u0431\u043e\u043b\u044c\u0448\u0435, \u0442\u043e \u043f\u0435\u0447\u0430\u0442\u0430\u0435\u043c \u043e\u0448\u0438\u0431\u043a\u0443 \u0438 \u0434\u0435\u043b\u0430\u0435\u043c \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0433\u043e \u0440\u0430\u0437\u043c\u0435\u0440\u0430\n                    if size_coefs < emb_size:\n                        emb_size_real = size_coefs\n                        print(f'Warning!\\nDesired size {emb_size} is not possible, returned size {emb_size_real}')\n                        emb_size = emb_size_real\n                    coefs = np.asarray(values[counter:emb_size+counter], dtype='float32')\n                    emb_index[word] = coefs\n            emb_file.close()\n            print(f'Loaded {len(emb_index)} word vectors.')\n        elif emb_type==\"word2vec\":\n            for word in word2vecDict.wv.vocab:\n                # \u0431\u0435\u0440\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u0442\u0435 \u0441\u043b\u043e\u0432\u0430, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0435\u0441\u0442\u044c \u0432 \u043d\u0430\u0448\u0438\u0445 \u0442\u0435\u043a\u0441\u0442\u0430\u0445\n                if word in vocab_train.keys():\n                    size_coefs = word2vecDict.word_vec(word).shape[0]\n                    if size_coefs < emb_size:\n                        emb_size_real = size_coefs\n                        print(f'Warning!\\nDesired size {emb_size} is not possible, returned size {emb_size_real}')\n                        emb_size = emb_size_real\n                    coefs = word2vecDict.word_vec(word)[:emb_size+1]\n                    emb_index[word] = coefs\n            print(f'Loaded {len(emb_index)} word vectors.')\n        elif emb_type==\"own\":\n            if dict_model is None:\n                print(f\"Dict was not received!\")\n                return None, None\n            for word in dict_model.keys():\n                    size_coefs = dict_model[word].shape[0]\n                    # \u043f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u043c \u0441\u043e\u0432\u043f\u0430\u0434\u0430\u0435\u0442 \u043b\u0438 \u0437\u0430\u043f\u0440\u043e\u0448\u0435\u043d\u043d\u0430\u044f \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c \u0441 \u0440\u0435\u0430\u043b\u044c\u043d\u043e\u0439\n                    # \u0435\u0441\u043b\u0438 \u0437\u0430\u043f\u0440\u043e\u0448\u0435\u043d\u043d\u0430\u044f \u0431\u043e\u043b\u044c\u0448\u0435, \u0442\u043e \u043f\u0435\u0447\u0430\u0442\u0430\u0435\u043c \u043e\u0448\u0438\u0431\u043a\u0443 \u0438 \u0434\u0435\u043b\u0430\u0435\u043c \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0433\u043e \u0440\u0430\u0437\u043c\u0435\u0440\u0430\n                    if size_coefs < emb_size:\n                        emb_size_real = size_coefs\n                        print(f'Warning!\\nDesired size {emb_size} is not possible, returned size {emb_size_real}')\n                        emb_size = emb_size_real\n                    coefs = dict_model[word][:emb_size+1]\n                    emb_index[word] = coefs\n            print(f'Loaded {len(emb_index)} word vectors.')\n        else:\n            print(f\"Wrong type {emb_type}!\")\n            return None, None\n        \n        all_embeddings = np.stack(list(emb_index.values()))\n        emb_mean, emb_std = all_embeddings.mean(), all_embeddings.std()\n        \n        #\u043f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u0441\u043e\u0437\u0434\u0430\u0435\u043c \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u0438 \u0437\u0430\u0431\u0438\u0432\u0430\u0435\u043c \u0435\u0435 \u0441\u0440\u0435\u0434\u043d\u0438\u043c\u0438 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\u043c\u0438\n        emb_matrix = np.random.normal(emb_mean, emb_std, (len(vocab_train.keys()), emb_size_real))\n        gc.collect()\n\n        # \u0438\u0437 \u0441\u043b\u043e\u0432\u0430\u0440\u044f \u0434\u0435\u043b\u0430\u0435\u043c \u043c\u0430\u0442\u0440\u0438\u0446\u0443\n        counter = 0\n        for word in vocab_train.keys():\n            try:\n                # \u0435\u0441\u043b\u0438 \u0441\u043b\u043e\u0432\u043e \u0435\u0441\u0442\u044c \u0432 \u043d\u0430\u0448\u0435\u043c \u0441\u043b\u043e\u0432\u0430\u0440\u0435, \u0442\u043e \u0431\u0435\u0440\u0435\u043c \u043e\u0442\u0442\u0443\u0434\u0430 \u0432\u0435\u043a\u0442\u043e\u0440 \u0438 \u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u043c \u0435\u0433\u043e \u0432 \u043d\u0430\u0448\u0443 \u043c\u0430\u0442\u0440\u0438\u0446\u0443\n                embedding_vector=emb_index[word]\n                emb_matrix[counter] = embedding_vector\n            except KeyError:\n                # \u0435\u0441\u043b\u0438 \u0441\u043b\u043e\u0432\u0430 \u043d\u0435 \u043e\u043a\u0430\u0437\u0430\u043b\u043e\u0441\u044c- \u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435\n                pass\n            counter+=1\n        print(f'total embedded: {len(emb_matrix)} common words')\n        \n        del(emb_index)\n        gc.collect()\n        \n        return emb_matrix, emb_size_real","70d61adc":"embedding_matrix_w2v, emb_size = creat_embedding_matrix('word2vec',emb_size)\n","63236f0e":"def get_emb_model(emb_matrix):\n    inp = Input(shape=(maxlen, ))\n    x = Embedding(emb_matrix.shape[0], emb_matrix.shape[1],weights=[emb_matrix],trainable=False)(inp)\n    x = Bidirectional(LSTM(60, return_sequences=True,name='lstm_layer',dropout=0.1,recurrent_dropout=0.1))(x)\n    x = GlobalMaxPool1D()(x)\n    x = Dropout(0.1)(x)\n    x = Dense(50, activation=\"relu\")(x)\n    x = Dropout(0.1)(x)\n    x = Dense(6, activation=\"sigmoid\")(x)\n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n    return model","09d3b753":"model_w2v = get_emb_model(embedding_matrix_w2v)","1add46f7":"model_w2v.summary()","8a7bf976":"batch_size = 32\nepochs = 4","942ace04":"fold = KFold(n_splits=2, shuffle=True)\nfor train_index, test_index in fold.split(train):\n    train_data = x_train_seq[train_index]\n    test_data = x_train_seq[test_index]\n    train_label = y_train[train_index]\n    test_label = y_train[test_index]\n    hist = model_w2v.fit(train_data,train_label, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n    test_score = model_w2v.evaluate(test_data, test_label)\n    print('Test loss {:.4f}, accuracy {:.2f}%'.format(test_score[0], test_score[1] * 100))","ae4ec58e":"emb_size=200\nembedding_matrix_glove, emb_size = creat_embedding_matrix('glove',emb_size)","a051b3f9":"model = get_emb_model(embedding_matrix_glove)","1733b435":"fold = KFold(n_splits=2, shuffle=True)\nfor train_index, test_index in fold.split(train):\n    train_data = x_train_seq[train_index]\n    test_data = x_train_seq[test_index]\n    train_label = y_train[train_index]\n    test_label = y_train[test_index]\n    hist = model.fit(train_data,train_label, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n    test_score = model.evaluate(test_data, test_label)\n    print('Test loss {:.4f}, accuracy {:.2f}%'.format(test_score[0], test_score[1] * 100))","1febb18b":"emb_size = 300\nembedding_matrix_wiki, emb_size = creat_embedding_matrix('fasttext',emb_size)","2cd768ab":"model_wiki=get_emb_model(embedding_matrix_wiki)","9d8d1826":"fold = KFold(n_splits=2, shuffle=True)\nfor train_index, test_index in fold.split(train):\n    train_data = x_train_seq[train_index]\n    test_data = x_train_seq[test_index]\n    train_label = y_train[train_index]\n    test_label = y_train[test_index]\n    hist = model_wiki.fit(train_data,train_label, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n    test_score = model_wiki.evaluate(test_data, test_label)\n    print('Test loss {:.4f}, accuracy {:.2f}%'.format(test_score[0], test_score[1] * 100))","01867aae":"tokenizer = nltk.data.load(\"tokenizers\/punkt\/english.pickle\")\ntrain_words=[]\nfor i in range(train.shape[0]):\n    train_words.append(train.loc[i][COMMENT].split())","b19e4a8e":"train_words[0]","ffae5ebe":"emb_size = 300\nmodel_w2v = gensim.models.Word2Vec(size=emb_size, window=5, workers=4,negative=100)","b12071b2":"model_w2v.build_vocab(train_words)","e06cd0df":"len(model_w2v.wv.vocab)","e9d18db0":"model_w2v.train(train_words, total_examples=model_w2v.corpus_count, epochs=model_w2v.iter)","0ce3048f":"w2v = dict(zip(model_w2v.wv.index2word, model_w2v.wv.vectors))","c4a4bfe0":"embedding_matrix_own, emb_size = creat_embedding_matrix('own', emb_size, w2v)","973cc91c":"model_own_emb = get_emb_model(embedding_matrix_own)","21810762":"fold = KFold(n_splits=2, shuffle=True)\nfor train_index, test_index in fold.split(train):\n    train_data = x_train_seq[train_index]\n    test_data = x_train_seq[test_index]\n    train_label = y_train[train_index]\n    test_label = y_train[test_index]\n    hist = model_own_emb.fit(train_data,train_label, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n    test_score = model_own_emb.evaluate(test_data, test_label)\n    print('Test loss {:.4f}, accuracy {:.2f}%'.format(test_score[0], test_score[1] * 100))","5239ddb3":"#### \u041d\u0435 \u0432\u0441\u0435 \u0437\u0430\u043c\u0435\u043d\u044b \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u0438\u0441\u044c \u0443\u0434\u0430\u0447\u043d\u044b\u043c\u0438","b832f1ae":"#### \u0421\u043b\u043e\u0432 \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u043e\u0441\u044c \u043d\u0435\u043c\u043d\u043e\u0433\u043e \u043c\u0435\u043d\u044c\u0448\u0435","e7cde502":"#### \u0421\u043b\u043e\u0432 \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u043e\u0441\u044c \u043c\u0430\u043b\u043e","b7b57bb9":"#### \u041f\u0440\u043e\u043f\u0443\u0441\u043a\u043e\u0432 \u043d\u0435\u0442","9eeef35d":"#### \u0417\u0430\u0433\u0440\u0443\u0437\u0438\u043c \u0433\u043e\u0442\u043e\u0432\u044b\u0439 \u0441\u043b\u043e\u0432\u0430\u0440\u044c, \u043e\u043d \u043f\u043e\u043d\u0430\u0434\u043e\u0431\u0438\u0442\u0441\u044f \u0434\u043b\u044f \u0438\u0441\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u043e\u0448\u0438\u0431\u043e\u043a \u0432 \u0441\u043b\u043e\u0432\u0430\u0445. \u041e\u0448\u0438\u0431\u043a\u0438 \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u0440\u0430\u0432\u043b\u044f\u0442\u044c \u043f\u043e \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0435\u043c\u0443 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0443:\n#### 1)\u0421\u043e\u0437\u0434\u0430\u0435\u043c \u0441\u043b\u043e\u0432\u0430\u0440\u044c \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043d\u0430\u0448\u0438\u0445 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u043e\u0432\n#### 2)\u0411\u0435\u0440\u0435\u043c \u043a\u0430\u0436\u0434\u043e\u0435 \u0441\u043b\u043e\u0432\u043e \u0438 \u043f\u0440\u043e\u0431\u0443\u0435\u043c \u0432\u0441\u0435\u0432\u043e\u0437\u043c\u043e\u0436\u043d\u044b\u0435 \u0437\u0430\u043c\u0435\u043d\u044b, \u043f\u0435\u0440\u0435\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0438 \u0438 \u0442.\u0434.\n#### 3)\u0418\u0449\u0435\u043c \u043f\u043e\u043b\u0443\u0447\u0438\u0432\u0448\u0438\u0435\u0441\u044f \u0441\u043b\u043e\u0432\u0430 \u0432 \u0433\u043e\u0442\u043e\u0432\u043e\u043c \u0437\u0430\u0433\u0440\u0443\u0436\u0435\u043d\u043d\u043e\u043c \u0441\u043b\u043e\u0432\u0430\u0440\u0435, \u0438 \u0432\u044b\u0431\u0438\u0440\u0430\u0435\u043c \u0438\u0437 \u043d\u0438\u0445 \u0441\u0430\u043c\u043e\u0435 \u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u043e\u0435\n#### 4)\u0417\u0430\u043c\u0435\u043d\u044f\u0435\u043c \u0441\u043b\u043e\u0432\u043e \u043d\u0430 \u0432\u044b\u0431\u0440\u0430\u043d\u043d\u043e\u0435 \u043d\u0430\u043c\u0438","c1a6a884":"#### \u0412\u0441\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u0438\u0441\u044c \u043f\u0440\u0438\u043c\u0435\u0440\u043d\u043e \u043e\u0434\u0438\u043d\u0430\u043a\u043e\u0432\u044b\u043c\u0438","d457b41c":"#### \u0422\u0430\u043a \u043a\u0430\u043a \u044d\u0442\u043e \u0442\u043e\u043b\u044c\u043a\u043e \u043f\u0440\u0438\u043c\u0435\u0440 \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u044f \u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u043e\u0432\u044b\u0445 \u043c\u0430\u0442\u0440\u0438\u0446, \u0434\u043b\u044f \u0431\u044b\u0441\u0442\u0440\u043e\u0442\u044b \u0432\u0441\u0435\u0445 \u043f\u043e\u0434\u0441\u0447\u0435\u0442\u043e\u0432 \u044f \u0432\u043e\u0437\u044c\u043c\u0443 \u043d\u0435\u0431\u043e\u043b\u044c\u0448\u0443\u044e \u0432\u0441\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438","cf1cbd69":"#### \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u043f\u0440\u0438\u043c\u0435\u0440\u043d\u043e \u0442\u0430\u043a\u043e\u0439 \u0436\u0435. \u0422\u0435\u043f\u0435\u0440\u044c \u043f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433 \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u0441\u0430\u043c\u043e\u0441\u0442\u043e\u044f\u0442\u0435\u043b\u044c\u043d\u043e \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 word2vec","1ba60f86":"\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c KFold \u0434\u043b\u044f \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u044f \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u0443\u044e \u0438 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0443\u044e \u0432\u044b\u0431\u043e\u0440\u043a\u0438","4d1ec1df":"#### \u0421\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u0441\u043b\u043e\u0432\u0430\u0440\u044c \u0441\u043e \u0441\u043b\u043e\u0432\u0430\u043c\u0438 \u0438 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0432\u0443\u044e\u0449\u0438\u043c\u0438 \u0438\u043c \u0432\u0435\u043a\u0442\u043e\u0440\u0430\u043c\u0438, \u0437\u0430\u0442\u0435\u043c \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043f\u043e\u043b\u0443\u0447\u0438\u0432\u0448\u0435\u0433\u043e\u0441\u044f \u0441\u043b\u043e\u0432\u0430\u0440\u044f \u043f\u043e\u0441\u0442\u0440\u043e\u0438\u043c \u043c\u0430\u0442\u0440\u0438\u0446\u0443, \u043f\u043e\u0441\u0442\u043e\u0440\u0438\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u0438 \u043e\u0431\u0443\u0447\u0438\u043c.","59bfb6b2":"#### \u041f\u0440\u0438\u043c\u0435\u0440\u044b \u0437\u0430\u043c\u0435\u043d","be8e6332":"#### \u0411\u0443\u0434\u0435\u043c \u0441\u0442\u0440\u043e\u0438\u0442\u044c \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0442\u0440\u0435\u0445 \u0433\u043e\u0442\u043e\u0432\u044b\u0445 \u043c\u0430\u0442\u0440\u0438\u0446 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 word2vec, glove, fasttext","3de35fa3":"#### \u0421\u0434\u0435\u043b\u0430\u0435\u043c \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433 \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0443\u0436\u0435 \u0433\u043e\u0442\u043e\u0432\u044b\u0445.","96637cee":"#### \u0414\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u043d\u0435\u043f\u043b\u043e\u0445\u043e\u0439 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442, \u043e\u0441\u043e\u0431\u0435\u043d\u043d\u043e \u0435\u0441\u043b\u0438 \u0443\u0447\u0435\u0441\u0442\u044c, \u0447\u0442\u043e \u043c\u044b \u043e\u0431\u0443\u0447\u0430\u043b\u0438\u0441\u044c \u043d\u0430 \u043d\u0435\u0431\u043e\u043b\u044c\u0448\u043e\u0439 \u043f\u043e \u0440\u0430\u0437\u043c\u0435\u0440\u0443 \u0432\u044b\u0431\u043e\u0440\u043a\u0435. \n#### \u041f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 glove.","0ef8a72c":"####  \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u0447\u0443\u0442\u044c-\u0447\u0443\u0442\u044c \u0445\u0443\u0436\u0435, \u043d\u043e \u043f\u0440\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u0442\u0430\u043a\u043e\u0439 \u0436\u0435. \u0422\u0435\u043f\u0435\u0440\u044c \u043f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u043d\u0430 \u043e\u0441\u043d\u043e\u0435 wiki \u0441\u043b\u043e\u0432\u0430\u0440\u044f.","5c1c5f3c":"#### \u0421\u0434\u0435\u043b\u0430\u0435\u043c \u0447\u0438\u0441\u0442\u043a\u0443 \u043d\u0430\u0448\u0438\u0445 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u043e\u0432: \u0443\u0431\u0435\u0440\u0435\u043c \u0441\u0441\u044b\u043b\u043a\u0438, \u043d\u0435\u043d\u0443\u0436\u043d\u044b\u0435 \u0437\u043d\u0430\u043a\u0438, \u0437\u0430\u043c\u0435\u043d\u0438\u043c \u0441\u043e\u043a\u0440\u0430\u0449\u0435\u043d\u0438\u044f, \u0443\u0431\u0435\u0440\u0435\u043c \u0446\u0438\u0444\u0440\u044b, \u043f\u0435\u0440\u0435\u0445\u043e\u0434\u044b \u0441\u0442\u0440\u043e\u043a \u0438 \u0441\u0442\u043e\u043f \u0441\u043b\u043e\u0432\u0430","2ffe8c14":"#### \u0421\u043b\u043e\u0432 \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u043e\u0441\u044c \u0431\u043e\u043b\u044c\u0448\u0435, \u043d\u043e \u0434\u043b\u0438\u043d\u0430 \u0432\u0435\u043a\u0442\u043e\u0440\u043e\u0432 \u043c\u0435\u043d\u044c\u0448\u0435","6cf6512a":"> #### \u0414\u043b\u044f \u0441\u0442\u0435\u043c\u043c\u0438\u043d\u0433\u0430 \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c lancaster stemmer \u0438 WordNetLemmatizer \u0434\u043b\u044f \u043b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0430\u0446\u0438\u0438, \u043d\u043e \u0434\u043b\u044f \u043d\u0430\u0447\u0430\u043b\u0430 \u0437\u0430\u043c\u0435\u043d\u0438\u043c \u043e\u0448\u0438\u0431\u043a\u0438 \u0432\u0441\u043b\u043e\u0432\u0430\u0445","4c53ccf8":"#### \u0421\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u0441\u043b\u043e\u0432\u0430\u0440\u044c \u0441\u043e\u043a\u0440\u0430\u0449\u0435\u043d\u0438\u0439, \u0447\u0442\u043e\u0431\u044b \u0432 \u043f\u043e\u0441\u043b\u0435\u0434\u0441\u0442\u0432\u0438\u0438 \u0438\u0445 \u0437\u0430\u043c\u0435\u043d\u0438\u0442\u044c","e31cebbc":"#### \u041f\u043e\u0441\u0442\u0440\u043e\u0438\u043c \u043c\u043e\u0434\u0435\u043b\u044c"}}