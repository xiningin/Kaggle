{"cell_type":{"0d44718f":"code","f4a9f69b":"code","834c30c3":"code","da155ede":"code","2090038f":"code","0298c059":"code","0cf3f738":"code","2ba75bda":"code","d9ab99c4":"code","a0a04cc5":"code","0ac77385":"code","ed47ea05":"code","80fb7d68":"code","31f1e21e":"code","133b1b10":"code","da6023a6":"code","d98b6039":"code","1f45ef27":"code","5703b3fa":"code","d5cfaf1a":"code","9517ed46":"code","9d75d5c3":"code","c1744e52":"code","f74b49b1":"code","c75708e2":"code","06831277":"markdown","cc6c3eaa":"markdown","4e9a1c15":"markdown","2fd178b5":"markdown","fffaf0b9":"markdown","9bec8ccc":"markdown","1fbc97da":"markdown","838fd61b":"markdown","127abc86":"markdown","a21f7ad6":"markdown","310f454c":"markdown","b2489390":"markdown","20187061":"markdown","bdf9a05d":"markdown"},"source":{"0d44718f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f4a9f69b":"import seaborn as sns;\nimport matplotlib.pyplot as plt\nimport scipy\n\nfrom sklearn.preprocessing import OneHotEncoder, KBinsDiscretizer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import log_loss\n\nimport lightgbm as lgbm","834c30c3":"col_names_train = ['Label'] + \\\n[\"I\"+str(i) for i in range(1, 14)] + \\\n['C'+str(i) for i in range(1,27)]\n\n#col_names_test = col_names_train[1:]\n\ndf_train = pd.read_csv('\/kaggle\/input\/criteo-dataset\/dac\/train.txt', \n                       sep='\\t', names=col_names_train,\n                       chunksize=100000) # ten chunks: first 1,000,000\n\n# df_test = pd.read_csv('\/kaggle\/input\/criteo-dataset\/dac\/test.txt', \n#                       sep='\\t', names=col_names_test,\n#                       chunksize=100000)\n\n# don't re-run, getting without replacement\ndf_train_100 = df_train.get_chunk(1000000)\ndf_test_25 = df_train.get_chunk(250000)\n\n# Using the first one million records for analysis\n# use the next 250,000 as testing data","da155ede":"# for simplicity of model fitting, no cross-validation process\n# directly train on df_train_100 and test for performance on df_test_25 \n\ndf_train_100 = df_train_100.convert_dtypes()\ndf_test_25 = df_test_25.convert_dtypes()","2090038f":"# I12 0.770057\np1 = sns.boxplot(x=\"Label\", y=\"I12\", data=df_train_100)\np2 = sns.displot(df_train_100, x=\"I12\", hue=\"Label\")","0298c059":"I12_temp = df_train_100[df_train_100.I12.between(1,5)]\np1 = sns.boxplot(x=\"Label\", y=\"I12\", data=I12_temp)\np2 = sns.displot(I12_temp, x=\"I12\", hue=\"Label\")","0cf3f738":"I12_temp = df_train_100[df_train_100.I12.between(5,25)]\np1 = sns.boxplot(x=\"Label\", y=\"I12\", data=I12_temp)\np2 = sns.displot(I12_temp, x=\"I12\", hue=\"Label\")","2ba75bda":"# C22      0.738959\np1 = sns.catplot(x='C22', hue='Label', kind='count', data=df_train_100)\nplt.xticks(rotation=90)\nplt.show()","d9ab99c4":"def pre1_gbdt(df_train, df_test):\n    '''\n    Function for preprocessing dataframes for gbdt model with methods in \n    Team 3idiots' solution.\n    '''\n    \n    my_dict = dict.fromkeys(col_names_train[1:14], -10)\n    my_dict.update(dict.fromkeys(col_names_train[14:], 'NA'))\n\n    df_train = df_train.fillna(my_dict) # inplace=True will change global var\n    df_test = df_test.fillna(my_dict)\n    \n#     df1 = pd.get_dummies(df_train_100, columns=df_train_100.columns[14:],\n#                          prefix=col, prefix_sep='-',\n#                          dummy_na=True, sparse=True)\n    \n    y_train = df_train.Label.values.astype('int')\n    y_test = df_test.Label.values.astype('int')\n    \n    ct = ColumnTransformer(transformers=[('encoder',\n                                      OneHotEncoder(handle_unknown='ignore'), \n                                      col_names_train[14:])],\n                       remainder='passthrough')\n    X_train = ct.fit_transform(df_train.iloc[:, 1:]) # sparse matrix\n    X_test = ct.transform(df_test.iloc[:, 1:]) # sparse matrix\n    \n    target_feats = ['encoder__x8_a73ee510', 'encoder__x21_NA',\n                    'encoder__x16_e5ba7672', 'encoder__x25_NA', \n                    'encoder__x22_32c7478e', 'encoder__x5_7e0ccccf',\n                    'encoder__x13_b28479f6', 'encoder__x18_21ddcdc9',\n                    'encoder__x13_07d13a8f', 'encoder__x9_3b08e48b',\n                    'encoder__x5_fbad5c96', 'encoder__x22_3a171ecb',\n                    'encoder__x19_b1252a9d', 'encoder__x19_5840adea',\n                    'encoder__x5_fe6b92e5', 'encoder__x19_a458ea53', \n                    'encoder__x13_1adce6ef', 'encoder__x24_001f3601',\n                    'encoder__x21_ad3062eb', 'encoder__x16_07c540c4',\n                    'encoder__x5_NA', 'encoder__x22_423fab69', \n                    'encoder__x16_d4bb7bd8', 'encoder__x1_38a947a1',\n                    'encoder__x24_e8b83407', 'encoder__x8_7cc72ec2'] \\\n    + [\"I\"+str(i) for i in range(1, 14)]\n    index_selected = [i for i, x in enumerate(ct.get_feature_names())\\\n                      if x in target_feats]\n    \n    X_train = X_train[:, index_selected]\n    X_test = X_test[:, index_selected]\n    \n    return X_train, X_test, y_train, y_test\n\nX_train, X_test, y_train, y_test = pre1_gbdt(df_train_100, df_test_25)","a0a04cc5":"# GBDT model only, pre1_gbdt\ngrd = GradientBoostingClassifier() # with default setting\ngrd.fit(X_train, y_train)\n\ny_pred_train_grd = grd.predict_proba(X_train)[:, 1] # prob of y=1\ny_pred_test_grd = grd.predict_proba(X_test)[:, 1] # prob of y=1\nscore_train = log_loss(y_train, y_pred_train_grd)\nscore_test = log_loss(y_test, y_pred_test_grd)\nnp.round(score_train, 4), np.round(score_test, 4)","0ac77385":"def pre2_gbdt(df_train, df_test):\n    '''\n    Function for preprocessing dataframes for gbdt model with modification on\n    methods in Team 3idiots' solution:\n    - dropping I12;\n    - one-hot encodded features with appearance more than ..%.\n    '''\n    \n    my_dict = dict.fromkeys(col_names_train[1:14], -10)\n    my_dict.update(dict.fromkeys(col_names_train[14:], 'NA'))\n\n    df_train = df_train.fillna(my_dict) # inplace=True will change global var\n    df_test = df_test.fillna(my_dict)\n    \n    y_train = df_train.Label.values.astype('int')\n    y_test = df_test.Label.values.astype('int')\n    \n    ct = ColumnTransformer(transformers=[('encoder',\n                                      OneHotEncoder(handle_unknown='ignore'), \n                                      col_names_train[14:])],\n                       remainder='passthrough')\n    X_train = ct.fit_transform(df_train.iloc[:, 1:]) # sparse matrix\n    X_test = ct.transform(df_test.iloc[:, 1:]) # sparse matrix\n    \n    # select one-hot encodded features with appearance more than ..%\n    selected_cat = np.asarray(X_train.sum(axis=0)[:,:-13] > 500000).reshape(-1)\n    # exclude I12\n    target_feats = [i for (i, v) in zip(ct.get_feature_names(), selected_cat) if v] \\\n    + [\"I\"+str(i) for i in range(1, 12)] + ['I13']\n    index_selected = [i for i, x in enumerate(ct.get_feature_names())\\\n                      if x in target_feats]\n    \n    X_train = X_train[:, index_selected]\n    X_test = X_test[:, index_selected]\n    \n    return X_train, X_test, y_train, y_test\n\nX_train, X_test, y_train, y_test = pre2_gbdt(df_train_100, df_test_25)","ed47ea05":"# GBDT model only, pre2_gbdt\ngrd = GradientBoostingClassifier() # with default setting\ngrd.fit(X_train, y_train)\n\ny_pred_train_grd = grd.predict_proba(X_train)[:, 1] # prob of y=1\ny_pred_test_grd = grd.predict_proba(X_test)[:, 1] # prob of y=1\nscore_train = log_loss(y_train, y_pred_train_grd)\nscore_test = log_loss(y_test, y_pred_test_grd)\nnp.round(score_train, 4), np.round(score_test, 4)","80fb7d68":"def pre3_gbdt(df_train, df_test):\n    '''\n    Function for preprocessing dataframes for gbdt model with modification on\n    methods in Team 3idiots' solution:\n    - quantile discretization & one-hot on numerical features;\n    - one-hot encodded features with appearance more than 50%.\n    '''\n    \n    my_dict = dict.fromkeys(col_names_train[1:14], -10)\n    my_dict.update(dict.fromkeys(col_names_train[14:], 'NA'))\n\n    df_train = df_train.fillna(my_dict) # inplace=True will change global var\n    df_test = df_test.fillna(my_dict)\n    \n    y_train = df_train.Label.values.astype('int')\n    y_test = df_test.Label.values.astype('int')\n    \n    # transformers: discretization and one-hot encoding\n    numeric_features = col_names_train[1:14]\n    numeric_transformer = KBinsDiscretizer(n_bins=10, encode='onehot',\n                                           strategy='quantile')\n\n    categorical_features = col_names_train[14:]\n    categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n\n    ct = ColumnTransformer(\n        transformers=[\n            ('num', numeric_transformer, numeric_features),\n            ('cat', categorical_transformer, categorical_features)])\n\n    X_train = ct.fit_transform(df_train.iloc[:, 1:]) # sparse matrix\n    X_test = ct.transform(df_test.iloc[:, 1:]) # sparse matrix\n    \n    # select one-hot encodded features with appearance more than 50%\n    selected_cat = np.asarray(X_train.sum(axis=0) > 500000).reshape(-1)\n    \n    X_train = X_train[:, selected_cat]\n    X_test = X_test[:, selected_cat]\n    \n    return X_train, X_test, y_train, y_test\n\nX_train, X_test, y_train, y_test = pre3_gbdt(df_train_100, df_test_25)","31f1e21e":"# GBDT model only, pre3_gbdt\ngrd = GradientBoostingClassifier() # with default setting\ngrd.fit(X_train, y_train)\n\ny_pred_train_grd = grd.predict_proba(X_train)[:, 1] # prob of y=1\ny_pred_test_grd = grd.predict_proba(X_test)[:, 1] # prob of y=1\nscore_train = log_loss(y_train, y_pred_train_grd)\nscore_test = log_loss(y_test, y_pred_test_grd)\nnp.round(score_train, 4), np.round(score_test, 4)","133b1b10":"# using pre3_gbdt preprocessed features directly in LR\n#X_train, X_test, y_train, y_test = pre3_gbdt(df_train_100, df_test_25)\n\nlr = LogisticRegression(random_state=0, solver='sag', max_iter=1000) \n# stochastic average gradient, l2 penalty\nlr.fit(X_train, y_train)\n\ny_pred_train_lr = lr.predict_proba(X_train)[:, 1] # prob of y=1\ny_pred_test_lr = lr.predict_proba(X_test)[:, 1] # prob of y=1\nscore_train = log_loss(y_train, y_pred_train_lr)\nscore_test = log_loss(y_test, y_pred_test_lr)\nnp.round(score_train, 4), np.round(score_test, 4)","da6023a6":"def pre3_gbdt_temp(df_train, df_test):\n    '''\n    Function for preprocessing dataframes for gbdt model with modification on\n    methods in Team 3idiots' solution:\n    - kmeans discretization & one-hot on numerical features;\n    - one-hot encodded features with appearance more than 50%.\n    '''\n    \n    my_dict = dict.fromkeys(col_names_train[1:14], -10)\n    my_dict.update(dict.fromkeys(col_names_train[14:], 'NA'))\n\n    df_train = df_train.fillna(my_dict) # inplace=True will change global var\n    df_test = df_test.fillna(my_dict)\n    \n    y_train = df_train.Label.values.astype('int')\n    y_test = df_test.Label.values.astype('int')\n    \n    # transformers: discretization and one-hot encoding\n    numeric_features = col_names_train[1:14]\n    numeric_transformer = KBinsDiscretizer(n_bins=5, encode='onehot',\n                                           strategy='kmeans')\n\n    categorical_features = col_names_train[14:]\n    categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n\n    ct = ColumnTransformer(\n        transformers=[\n            ('num', numeric_transformer, numeric_features),\n            ('cat', categorical_transformer, categorical_features)])\n\n    X_train = ct.fit_transform(df_train.iloc[:, 1:]) # sparse matrix\n    X_test = ct.transform(df_test.iloc[:, 1:]) # sparse matrix\n    \n    # select one-hot encodded features with appearance more than 50%\n    selected_cat = np.asarray(X_train.sum(axis=0) > 500000).reshape(-1)\n    \n    X_train = X_train[:, selected_cat]\n    X_test = X_test[:, selected_cat]\n    \n    return X_train, X_test, y_train, y_test\n\nX_train, X_test, y_train, y_test = pre3_gbdt_temp(df_train_100, df_test_25)","d98b6039":"# using pre3_gbdt_temp preprocessed features in LR\nlr = LogisticRegression(random_state=0, solver='sag', max_iter=1000) # stochastic average gradient\nlr.fit(X_train, y_train)\n\ny_pred_train_lr = lr.predict_proba(X_train)[:, 1] # prob of y=1\ny_pred_test_lr = lr.predict_proba(X_test)[:, 1] # prob of y=1\nscore_train = log_loss(y_train, y_pred_train_lr)\nscore_test = log_loss(y_test, y_pred_test_lr)\nnp.round(score_train, 4), np.round(score_test, 4)","1f45ef27":"def pre1_lr(df_train, df_test):\n    '''\n    Function for preprocessing dataframes to get GBDT features for lr model.\n    - quantile discretization & one-hot on numerical features;\n    - one-hot encodded features with appearance more than ..%.\n    '''\n    \n    my_dict = dict.fromkeys(col_names_train[1:14], -10)\n    my_dict.update(dict.fromkeys(col_names_train[14:], 'NA'))\n\n    df_train = df_train.fillna(my_dict) # inplace=True will change global var\n    df_test = df_test.fillna(my_dict)\n    \n    y_train = df_train.Label.values.astype('int')\n    y_test = df_test.Label.values.astype('int')\n    \n    # transformers: discretization and one-hot encoding\n    numeric_features = col_names_train[1:14]\n    numeric_transformer = KBinsDiscretizer(n_bins=10, encode='onehot',\n                                           strategy='quantile')\n\n    categorical_features = col_names_train[14:]\n    categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n\n    ct = ColumnTransformer(\n        transformers=[\n            ('num', numeric_transformer, numeric_features),\n            ('cat', categorical_transformer, categorical_features)])\n\n    X_train = ct.fit_transform(df_train.iloc[:, 1:]) # sparse matrix\n    X_test = ct.transform(df_test.iloc[:, 1:]) # sparse matrix\n    \n    # select one-hot encodded features with appearance more than 30%\n    selected_cat = np.asarray(X_train.sum(axis=0) > 300000).reshape(-1)\n    \n    X_train = X_train[:, selected_cat]\n    X_test = X_test[:, selected_cat]\n    \n    return X_train, X_test, y_train, y_test\n\nX_train, X_test, y_train, y_test = pre1_lr(df_train_100, df_test_25)","5703b3fa":"# \u4e2d\u95f4\u68c0\u67e5\uff0cGBDT\u6a21\u578b\u6548\u679c\ngrd = GradientBoostingClassifier() # n_estimators=100, max_depth=3\ngrd.fit(X_train, y_train)\n\ny_pred_train_grd = grd.predict_proba(X_train)[:, 1] # prob of y=1\ny_pred_test_grd = grd.predict_proba(X_test)[:, 1] # prob of y=1\nscore_train = log_loss(y_train, y_pred_train_grd)\nscore_test = log_loss(y_test, y_pred_test_grd)\nnp.round(score_train, 4), np.round(score_test, 4)","d5cfaf1a":"# GBDT\u7279\u5f81\n#grd = GradientBoostingClassifier() # n_estimators=100, max_depth=3\ngrd_enc = OneHotEncoder()\n#grd.fit(X_train, y_train)\ngrd_enc.fit(grd.apply(X_train)[:, :, 0]) # OHE on indexes\nX_train_part2 = grd_enc.transform(grd.apply(X_train)[:, :, 0])\nX_test_part2 = grd_enc.transform(grd.apply(X_test)[:, :, 0])","9517ed46":"# # LR\n# grd_lm = LogisticRegression(random_state=0, solver='saga', \n#                             penalty='l1', max_iter=1000)\n# # feed into LR\n# grd_lm.fit(X_train_part2, y_train)\n\n# y_pred_train_grd_lm = grd_lm.predict_proba(X_train_part2)[:, 1]\n# y_pred_test_grd_lm = grd_lm.predict_proba(X_test_part2)[:, 1]\n\n# score_train = log_loss(y_train, y_pred_train_grd_lm)\n# score_test = log_loss(y_test, y_pred_test_grd_lm)\n# np.round(score_train, 4), np.round(score_test, 4)","9d75d5c3":"# \u4eba\u5de5\u7279\u5f81, continuing from `pre1_lr`\n# X_train, X_test, y_train, y_test = pre1_lr(df_train_100, df_test_25)\n# select one-hot encodded features with appearance more than 30%\n\n# GBDT\u7279\u5f81: X_train_part2, X_test_part2\n\n# combined\nX_train_new = scipy.sparse.hstack((X_train, X_train_part2))\nX_test_new = scipy.sparse.hstack((X_test, X_test_part2))","c1744e52":"# feed into LR\ngrd_lm = LogisticRegression(random_state=0, solver='saga', \n                            penalty='l1', max_iter=2000)\ngrd_lm.fit(X_train_new, y_train)\n\ny_pred_train_grd_lm = grd_lm.predict_proba(X_train_new)[:, 1]\ny_pred_test_grd_lm = grd_lm.predict_proba(X_test_new)[:, 1]\n\nscore_train = log_loss(y_train, y_pred_train_grd_lm)\nscore_test = log_loss(y_test, y_pred_test_grd_lm)\nnp.round(score_train, 4), np.round(score_test, 4)","f74b49b1":"# \u4eba\u5de5\u7279\u5f81, continuing from `pre3_gbdt`\n# select one-hot encodded features with appearance more than 50%\nX_train, X_test, y_train, y_test = pre3_gbdt(df_train_100, df_test_25)\n\n# GBDT\u7279\u5f81: X_train_part2, X_test_part2\ngrd = GradientBoostingClassifier() # n_estimators=100, max_depth=3\ngrd_enc = OneHotEncoder()\ngrd.fit(X_train, y_train)\ngrd_enc.fit(grd.apply(X_train)[:, :, 0]) # ohe on indexes\nX_train_part2 = grd_enc.transform(grd.apply(X_train)[:, :, 0])\nX_test_part2 = grd_enc.transform(grd.apply(X_test)[:, :, 0])\n\n# combined\nX_train_new = scipy.sparse.hstack((X_train, X_train_part2))\nX_test_new = scipy.sparse.hstack((X_test, X_test_part2))","c75708e2":"# feed into LR\ngrd_lm = LogisticRegression(random_state=0, solver='sag', \n                            penalty='l2', max_iter=1000)\ngrd_lm.fit(X_train_new, y_train)\n\ny_pred_train_grd_lm = grd_lm.predict_proba(X_train_new)[:, 1]\ny_pred_test_grd_lm = grd_lm.predict_proba(X_test_new)[:, 1]\n\nscore_train = log_loss(y_train, y_pred_train_grd_lm)\nscore_test = log_loss(y_test, y_pred_test_grd_lm)\nnp.round(score_train, 4), np.round(score_test, 4)","06831277":"## \u6709\u5927\u91cf\u7f3a\u5931\u7684\u7279\u5f81\u548c\u6807\u7b7e\u95f4\u7684\u76f8\u5173\u6027\u8c03\u67e5\n\n- above 70% missing: I12, C22","cc6c3eaa":"training, testing : (0.4982, 0.4968) - 13 + 26 variables","4e9a1c15":"- GBDT\u7279\u5f81 + LR: taking the index of the prediction leaf node for every tree as the sparse input for logistic regression (ref: https:\/\/towardsdatascience.com\/next-better-player-gbdt-lr-for-binary-classification-f8dc6f32628e) (ref: https:\/\/scikit-learn.org\/stable\/auto_examples\/ensemble\/plot_feature_transformation.html#example-ensemble-plot-feature-transformation-py)\n\n    1. \u5ef6\u7528\u4e4b\u524d\u7684GBDT\u6846\u67b6\uff08pre3_gbdt, 6 variables) -> 800 (100 tree, depth 3) leaf index feature into LR(random_state=0, solver='sag', max_iter=1000, penalty='l2') -> (0.5609, 0.5565)\uff0c \u548c\u4e4b\u524d\u4e00\u6837\n    2. \u4e4b\u524d\u7684GBDT\u6846\u67b6+\u5176\u4ed6\u7b5b\u9009\u6761\u4ef6\uff08pre1_lr) (LR: random_state=0, solver='saga', penalty='l1', max_iter=1000):\n        - 10% : 103 variables, GBDT score (0.504, 0.5019) -> LR score (0.4979, 0.4967)\n        - 30% : 19 variables, GBDT score (0.5392, 0.5358) -> LR score (0.5386, 0.5355)","2fd178b5":"- training, testing : (0.4987, 0.4976) - dropping I12 (12 + 26 variables)\n- training, testing : (0.5038, 0.5027) - dropping I12 & above 40% dummies only (12 + 9 variables - `['encoder__x0_05db9164',\n 'encoder__x4_25c83c98',\n 'encoder__x7_0b153874',\n 'encoder__x8_a73ee510',\n 'encoder__x16_e5ba7672',\n 'encoder__x18_NA',\n 'encoder__x19_NA',\n 'encoder__x21_NA',\n 'encoder__x24_NA',\n 'encoder__x25_NA']`)\n- training, testing : (0.5054, 0.5044) - dropping I12 & above 50% dummies only (12 + 5 variables - `['encoder__x0_05db9164',\n 'encoder__x4_25c83c98',\n 'encoder__x7_0b153874',\n 'encoder__x8_a73ee510',\n 'encoder__x21_NA']`)","fffaf0b9":"- \u4eba\u5de5\u7279\u5f81 + GBDT\u7279\u5f81 + LR\n    - pre1_lr: \n        - 19+800 vars\n        - `(random_state=0, solver='saga', penalty='l1', max_iter=1000)`\n        - `(random_state=0, solver='saga', penalty='l1', max_iter=2000)`\n        - didn't converge in 1000\/2000, (0.5386, 0.5355)\n    - pre3_gbdt: \n        - 6+800 vars \n        - `(random_state=0, solver='saga', penalty='l1', max_iter=1000)`\n        - `(random_state=0, solver='sag', penalty='l2', max_iter=1000)`\n        - (0.5609, 0.5565)","9bec8ccc":"### 3-idiots methods","1fbc97da":"### modification on 3-idiots","838fd61b":"- \u4eba\u5de5\u7279\u5f81 + LR: same\/similar results as gbdt (ref: https:\/\/datascience.stackexchange.com\/questions\/18081\/gradient-boosting-vs-logistic-regression-for-boolean-features)\n    - kmeans: (0.5428, 0.5397)\n    - quantile: (0.5609, 0.5565)\n    - uniform: (0.5506, 0.5469)","127abc86":"### \u6570\u503c\u578b\u7279\u5f81\u79bb\u6563\u5316\n\n- the value in replace with na may matter here","a21f7ad6":"## \u7279\u5f81\u5de5\u7a0b\uff1aGBDT+LR\n","310f454c":"- training, testing : (0.5506, 0.5468) - uniform discretization (-10 for NA, n_bins=10) + appearance more than 50% (17 variables)\n- training, testing : (0.541, 0.538) - kmeans discretization (-10 for NA, n_bins=5) + appearance more than 50% (17 variables)\n- training, testing : (0.5609, 0.5565) - quantile discretization (-1\/-10\/-100 for NA, n_bins=10\/5) + appearance more than 50% (6 variables)","b2489390":"## \u7279\u5f81\u5de5\u7a0b - GBDT\n- basing on 3idiots-preA\n- https:\/\/github.com\/ycjuan\/kaggle-2014-criteo\n- https:\/\/www.kaggle.com\/c\/criteo-display-ad-challenge\/discussion\/10555\n- replace all numerical missing values with -10; one-hot encoding with selection (only include dense variables)\n`target_cat_feats = ['C9-a73ee510', 'C22-', 'C17-e5ba7672', 'C26-', 'C23-32c7478e', 'C6-7e0ccccf', 'C14-b28479f6', 'C19-21ddcdc9', 'C14-07d13a8f', 'C10-3b08e48b', 'C6-fbad5c96', 'C23-3a171ecb', 'C20-b1252a9d', 'C20-5840adea', 'C6-fe6b92e5', 'C20-a458ea53', 'C14-1adce6ef', 'C25-001f3601', 'C22-ad3062eb', 'C17-07c540c4', 'C6-', 'C23-423fab69', 'C17-d4bb7bd8', 'C2-38a947a1', 'C25-e8b83407', 'C9-7cc72ec2']`\n- \u7279\u5f81\u6784\u5efa\u903b\u8f91: \u6709\u7f3a\u5931\u7684\u6570\u503c\u578b\u7279\u5f81\u90fd\u53ea\u542b\u975e\u8d1f\u503c\uff08\u9664\u4e86I2\uff0c\u6700\u5c0f\u503c-2\u4f46\u65e0\u7f3a\u5931\uff09\uff0c-10\u662f\u7ed9NA\u8d4b\u4e86\u4e00\u4e2a\u7279\u6b8a\u503c\uff0c\u5e76\u4e14\u65e0\u7f3a\u5931\u9879\u6709\u8d1f\u503c\uff0c\u6240\u4ee5\u7ed9NA\u8d4b\u8d1f\u503c\u8f83\u4e3a\u5408\u7406\uff08**\u53ef\u7528\u5176\u4ed6\u503c\uff0c\u8303\u56f4\u5916\u4e0d\u5f71\u54cdscore**\uff09\uff1b\u5bf9\u6240\u6709\u7c7b\u522b\u578b\u7279\u5f81\u505aone-hot\u540e\u53ea\u4fdd\u7559\u7a20\u5bc6\uff08\u5728\u539f\u5b8c\u6574\u8bad\u7ec3\u96c6\u4e2d\u51fa\u73b0\u8d85\u8fc7\u56db\u767e\u4e07\u6b21\uff09\u7684\u7279\u5f81\uff0c\u56e0\u4e3a\u4f20\u7edfgbdt\u4e0d\u9002\u5408\u8fc7\u5206\u7a00\u758f\u7684\u7279\u5f81\u77e9\u9635\uff08prohibitively expensive computation & memory problem\uff09\uff0c\u539f\u59cbone-hot\u4e4b\u540e\u7684\u7279\u5f81\u9700\u505a\u7b5b\u9009\uff08**\u53ef\u6839\u636e\u6b64\u8bad\u7ec3\u96c6\u5c1d\u8bd5\u5176\u4ed6\u7b5b\u9009**\uff09","20187061":"- I12\u4e0eLabel\u95f4\u65e0\u660e\u663e\u76f8\u5173\u6027\uff08\u5404\u4e2a\u503c\u4e0a\u90fd\u51fa\u73b0\u4e24\u79cd\u6807\u7b7e\uff09\uff0c77%\u7f3a\u5c11 -> \u53ef\u8003\u8651\u820d\u5f03\u8be5\u7279\u5f81\n- \u6216\u5bf9\u6570\u503c\u7c7b\u7279\u5f81\u5206\u7bb1\/\u79bb\u6563\u5316","bdf9a05d":"## Notes\n- Using the first one million records for analysis\n- Using the next 250,000 as testing data\n- for simplicity of model fitting, no cross-validation process\n- directly train on df_train_100 and test for performance on df_test_25"}}