{"cell_type":{"41a72aa5":"code","c4e06a98":"code","f540ddab":"code","71e53248":"code","8564a7bf":"code","fe6567c5":"code","69104f1c":"code","7e5f88d6":"code","32397f55":"code","a9bdc624":"code","fbdc5151":"code","7911beb0":"code","d40f2ad4":"code","429b069a":"code","cbf61850":"code","0d64c553":"code","2b582e4e":"code","7a40d6e7":"code","6c6b5568":"code","8b988e92":"code","900cf657":"code","37e35a78":"code","3299d2c2":"code","4e0b4c78":"code","0ae5695e":"code","52f66015":"code","d32fd9c5":"code","302f453c":"code","0363dc20":"markdown","ae1c68c3":"markdown","367b6fd8":"markdown","094ddd3e":"markdown","04b6d7fa":"markdown","0caff6d3":"markdown","339f6a10":"markdown","2408484e":"markdown","852900ce":"markdown","d069f0fb":"markdown","a7074926":"markdown","042d1b6b":"markdown","5688f286":"markdown","1face393":"markdown","0d91a0e8":"markdown","5ac65e15":"markdown","e18679a9":"markdown","5e689c21":"markdown","8c8bf269":"markdown","eb4ff23d":"markdown","bcebb21b":"markdown","098ed2a1":"markdown","a7b4924a":"markdown","70985585":"markdown","66f79a4b":"markdown","756f749c":"markdown","6a73e2d4":"markdown","62615fd1":"markdown"},"source":{"41a72aa5":"import os #paths to file\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\nimport warnings# warning filter\n\n\n#ploting libraries\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\n#relevant ML libraries\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\n\n#ML models\nfrom xgboost import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n#default theme\nsns.set(context='notebook', style='darkgrid', palette='deep', font='sans-serif', font_scale=1, color_codes=False, rc=None)\n\n#warning hadle\nwarnings.filterwarnings(\"ignore\")","c4e06a98":"#list all files under the input directory\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f540ddab":"#path for the training set\ntr_path = \"\/kaggle\/input\/loan-prediction-problem-dataset\/train_u6lujuX_CVtuZ9i.csv\"\n#path for the testing set\nte_path = \"\/kaggle\/input\/loan-prediction-problem-dataset\/test_Y3wMUE5_7gLdaTN.csv\"","71e53248":"# read in csv file as a DataFrame\ntr_df = pd.read_csv(tr_path)\n# explore the first 5 rows\ntr_df.head()","8564a7bf":"# read in csv file as a DataFrame\nte_df = pd.read_csv(te_path)\n# explore the first 5 rows\nte_df.head()","fe6567c5":"print(f\"training set (row, col): {tr_df.shape}\\n\\ntesting set (row, col): {te_df.shape}\")","69104f1c":"#column information\ntr_df.info(verbose=True, null_counts=True)","7e5f88d6":"#summary statistics\ntr_df.describe()","32397f55":"#the Id column is not needed, let's drop it for both test and train datasets\ntr_df.drop('Loan_ID',axis=1,inplace=True)\nte_df.drop('Loan_ID',axis=1,inplace=True)\n#checking the new shapes\nprint(f\"training set (row, col): {tr_df.shape}\\n\\ntesting set (row, col): {te_df.shape}\")","a9bdc624":"#missing values in decsending order\ntr_df.isnull().sum().sort_values(ascending=False)","fbdc5151":"#filling the missing data\nprint(\"Before filling missing values\\n\\n\",\"#\"*50,\"\\n\")\nnull_cols = ['Credit_History', 'Self_Employed', 'LoanAmount','Dependents', 'Loan_Amount_Term', 'Gender', 'Married']\n\n\nfor col in null_cols:\n    print(f\"{col}:\\n{tr_df[col].value_counts()}\\n\",\"-\"*50)\n    tr_df[col] = tr_df[col].fillna(\n    tr_df[col].dropna().mode().values[0] )   \n\n    \ntr_df.isnull().sum().sort_values(ascending=False)\nprint(\"After filling missing values\\n\\n\",\"#\"*50,\"\\n\")\nfor col in null_cols:\n    print(f\"\\n{col}:\\n{tr_df[col].value_counts()}\\n\",\"-\"*50)","7911beb0":"#list of all the columns.columns\n#Cols = tr_df.tolist()\n#list of all the numeric columns\nnum = tr_df.select_dtypes('number').columns.to_list()\n#list of all the categoric columns\ncat = tr_df.select_dtypes('object').columns.to_list()\n\n#numeric df\nloan_num =  tr_df[num]\n#categoric df\nloan_cat = tr_df[cat]","d40f2ad4":"print(tr_df[cat[-1]].value_counts())\n#tr_df[cat[-1]].hist(grid = False)\n\n#print(i)\ntotal = float(len(tr_df[cat[-1]]))\nplt.figure(figsize=(8,10))\nsns.set(style=\"whitegrid\")\nax = sns.countplot(tr_df[cat[-1]])\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,height + 3,'{:1.2f}'.format(height\/total),ha=\"center\") \nplt.show()","429b069a":"for i in loan_num:\n    plt.hist(loan_num[i])\n    plt.title(i)\n    plt.show()\n","cbf61850":"for i in cat[:-1]: \n    plt.figure(figsize=(15,10))\n    plt.subplot(2,3,1)\n    sns.countplot(x=i ,hue='Loan_Status', data=tr_df ,palette='plasma')\n    plt.xlabel(i, fontsize=14)","0d64c553":"#converting categorical values to numbers\n\nto_numeric = {'Male': 1, 'Female': 2,\n'Yes': 1, 'No': 2,\n'Graduate': 1, 'Not Graduate': 2,\n'Urban': 3, 'Semiurban': 2,'Rural': 1,\n'Y': 1, 'N': 0,\n'3+': 3}\n\n# adding the new numeric values from the to_numeric variable to both datasets\ntr_df = tr_df.applymap(lambda lable: to_numeric.get(lable) if lable in to_numeric else lable)\nte_df = te_df.applymap(lambda lable: to_numeric.get(lable) if lable in to_numeric else lable)\n\n# convertind the Dependents column\nDependents_ = pd.to_numeric(tr_df.Dependents)\nDependents__ = pd.to_numeric(te_df.Dependents)\n\n# dropping the previous Dependents column\ntr_df.drop(['Dependents'], axis = 1, inplace = True)\nte_df.drop(['Dependents'], axis = 1, inplace = True)\n\n# concatination of the new Dependents column with both datasets\ntr_df = pd.concat([tr_df, Dependents_], axis = 1)\nte_df = pd.concat([te_df, Dependents__], axis = 1)\n\n# checking the our manipulated dataset for validation\nprint(f\"training set (row, col): {tr_df.shape}\\n\\ntesting set (row, col): {te_df.shape}\\n\")\nprint(tr_df.info(), \"\\n\\n\", te_df.info())","2b582e4e":"#plotting the correlation matrix\nsns.heatmap(tr_df.corr() ,cmap='cubehelix_r')","7a40d6e7":"#correlation table\ncorr = tr_df.corr()\ncorr.style.background_gradient(cmap='coolwarm').set_precision(2)","6c6b5568":"y = tr_df['Loan_Status']\nX = tr_df.drop('Loan_Status', axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)","8b988e92":"DT = DecisionTreeClassifier()\nDT.fit(X_train, y_train)\n\ny_predict = DT.predict(X_test)\n\n#  prediction Summary by species\nprint(classification_report(y_test, y_predict))\n\n# Accuracy score\nDT_SC = accuracy_score(y_predict,y_test)\nprint(f\"{round(DT_SC*100,2)}% Accurate\")","900cf657":"Decision_Tree=pd.DataFrame({'y_test':y_test,'prediction':y_predict})\nDecision_Tree.to_csv(\"Dection Tree.csv\")     ","37e35a78":"RF = RandomForestClassifier()\nRF.fit(X_train, y_train)\n\ny_predict = RF.predict(X_test)\n\n#  prediction Summary by species\nprint(classification_report(y_test, y_predict))\n\n# Accuracy score\nRF_SC = accuracy_score(y_predict,y_test)\nprint(f\"{round(RF_SC*100,2)}% Accurate\")","3299d2c2":"Random_Forest=pd.DataFrame({'y_test':y_test,'prediction':y_predict})\nRandom_Forest.to_csv(\"Random Forest.csv\")     ","4e0b4c78":"XGB = XGBClassifier()\nXGB.fit(X_train, y_train)\n\ny_predict = XGB.predict(X_test)\n\n#  prediction Summary by species\nprint(classification_report(y_test, y_predict))\n\n# Accuracy score\nXGB_SC = accuracy_score(y_predict,y_test)\nprint(f\"{round(XGB_SC*100,2)}% Accurate\")","0ae5695e":"XGBoost=pd.DataFrame({'y_test':y_test,'prediction':y_predict})\nXGBoost.to_csv(\"XGBoost.csv\")     ","52f66015":"LR = LogisticRegression()\nLR.fit(X_train, y_train)\n\ny_predict = LR.predict(X_test)\n\n#  prediction Summary by species\nprint(classification_report(y_test, y_predict))\n\n# Accuracy score\nLR_SC = accuracy_score(y_predict,y_test)\nprint('accuracy is',accuracy_score(y_predict,y_test))","d32fd9c5":"Logistic_Regression=pd.DataFrame({'y_test':y_test,'prediction':y_predict})\nLogistic_Regression.to_csv(\"Logistic Regression.csv\")     ","302f453c":"score = [DT_SC,RF_SC,XGB_SC,LR_SC]\nModels = pd.DataFrame({\n    'n_neighbors': [\"Decision Tree\",\"Random Forest\",\"XGBoost\", \"Logistic Regression\"],\n    'Score': score})\nModels.sort_values(by='Score', ascending=False)","0363dc20":"Each value will be replaced by the most frequent value (mode).\n\nE.G. `Credit_History` has 50 null values and has 2 unique values `1.0` (475 times) or `0.0` (89 times) therefore each null value will be replaced by the mode `1.0` so now it will show in our data 525 times. ","ae1c68c3":"## Random Forest\n\n![](https:\/\/miro.medium.com\/max\/1280\/1*9kACduxnce_JdTrftM_bsA.gif)","367b6fd8":"## Decision Tree\n\n![](https:\/\/i.pinimg.com\/originals\/eb\/08\/05\/eb0805eb6e34bf3eac5ab4666bbcc167.gif)","094ddd3e":"### Csv results of the test for our model:\n\n<table>\n  <tr><td>\n    <img src=\"https:\/\/miro.medium.com\/max\/900\/1*a99bY1VkmfXhqW-5uAX28w.jpeg\"\n         width=\"200\" height=\"300\">\n      <tr><td align=\"center\">\n  <\/td><\/tr>\n  <\/td><\/tr>\n<\/table>\n\nYou can see each predition and true value side by side by the csv created in the output directory.","04b6d7fa":"Categorical (split by Loan status):","0caff6d3":"We can clearly see that `Credit_History` has the highest correlation with `Loan_Status` (a positive correlation of `0.54`).\nTherefore our target value is highly dependant on this column.","339f6a10":"## Missing values \ud83d\udeab\nAs you can see we have some missing data, let's have a look how many we have for each column:","2408484e":"## Correlation matrix ","852900ce":"## Loan status distribution","d069f0fb":"## XGBoost\n\n![](https:\/\/f-origin.hypotheses.org\/wp-content\/blogs.dir\/253\/files\/2015\/06\/boosting-algo-3.gif)","a7074926":"# Machine learning models\n\nFirst of all we will divide our dataset into two variables `X` as the features we defined earlier and `y` as the `Loan_Status` the target value we want to predict.\n\n## Models we will use:\n\n* **Decision Tree** \n* **Random Forest**\n* **XGBoost**\n* **Logistic Regression**\n\n## The Process of Modeling the Data:\n\n1. Importing the model\n\n2. Fitting the model\n\n3. Predicting Loan Status\n\n4. Classification report by Loan Status\n\n5. Overall accuracy\n","042d1b6b":"### Now the focus is shifted for the preprocessing of the training dataset.","5688f286":"Size of each data set:","1face393":"# File path \ud83d\udcc2","0d91a0e8":"Let's plot our data\n\nNumeric:","5ac65e15":"Testing set:","e18679a9":"### Correlation table for a more detailed analysis:","5e689c21":"Firstly we need to split our data to categorical and numerical data,\n\n\nusing the `.select_dtypes('dtype').columns.to_list()` combination.","8c8bf269":"### Csv results of the test for our model:\n\n<table>\n  <tr><td>\n    <img src=\"https:\/\/miro.medium.com\/max\/900\/1*a99bY1VkmfXhqW-5uAX28w.jpeg\"\n         width=\"200\" height=\"300\">\n      <tr><td align=\"center\">\n  <\/td><\/tr>\n  <\/td><\/tr>\n<\/table>\n\nYou can see each predition and true value side by side by the csv created in the output directory.","eb4ff23d":"### Csv results of the test for our model:\n\n<table>\n  <tr><td>\n    <img src=\"https:\/\/miro.medium.com\/max\/900\/1*a99bY1VkmfXhqW-5uAX28w.jpeg\"\n         width=\"200\" height=\"300\">\n      <tr><td align=\"center\">\n  <\/td><\/tr>\n  <\/td><\/tr>\n<\/table>\n\nYou can see each predition and true value side by side by the csv created in the output directory.","bcebb21b":"# Conclusion\n\n1. `Credit_History` is a very important variable  because of its high correlation with `Loan_Status` therefor showind high Dependancy for the latter.\n2. The Logistic Regression algorithm is the most accurate: **approximately 83%**.","098ed2a1":"## Encoding data to numeric","a7b4924a":"## Logistic Regression\nNow, I will explore the Logistic Regression model.\n\n<table>\n  <tr><td>\n    <img src=\"https:\/\/files.realpython.com\/media\/log-reg-2.e88a21607ba3.png\"\n          width=\"500\" height=\"400\">\n      <tr><td align=\"center\">\n  <\/td><\/tr>\n  <\/td><\/tr>\n<\/table>","70985585":"# Introduction\n\n<table>\n  <tr><td>\n    <img src=\"https:\/\/pas-wordpress-media.s3.us-east-1.amazonaws.com\/content\/uploads\/2015\/12\/loan-e1450497559334.jpg\"\n         width=\"400\" height=\"600\">\n      <tr><td align=\"center\">\n  <\/td><\/tr>\n  <\/td><\/tr>\n<\/table>\n\nIn finance, a loan is the lending of money by one or more individuals, organizations, or other entities to other individuals, organizations etc. The recipient (i.e., the borrower) incurs a debt and is usually liable to pay interest on that debt until it is repaid as well as to repay the principal amount borrowed. ([wikipedia](https:\/\/en.wikipedia.org\/wiki\/Loan))\n\n### **The major aim of this notebook is to predict which of the customers will have their loan approved.**\n\n![](https:\/\/i.pinimg.com\/originals\/41\/b0\/08\/41b008395e8e7f888666688915750d1f.gif)\n\n# Data Id \ud83d\udccb\n\nThis dataset is named [Loan Prediction Dataset](https:\/\/www.kaggle.com\/altruistdelhite04\/loan-prediction-problem-dataset) data set. The dataset contains a set of **613** records under **13 attributes**:\n\n![](http:\/\/miro.medium.com\/max\/795\/1*cAd_tqzgCWtCVMjEasWmpQ.png)\n\n## The main objective for this dataset:\nUsing machine learning techniques to predict loan payments.\n\n### target value: `Loan_Status`\n\n# Libraries \ud83d\udcd5\ud83d\udcd7\ud83d\udcd8","66f79a4b":"### Csv results of the test for our model:\n\n<table>\n  <tr><td>\n    <img src=\"https:\/\/miro.medium.com\/max\/900\/1*a99bY1VkmfXhqW-5uAX28w.jpeg\"\n         width=\"200\" height=\"300\">\n      <tr><td align=\"center\">\n  <\/td><\/tr>\n  <\/td><\/tr>\n<\/table>\n\nYou can see each predition and true value side by side by the csv created in the output directory.","756f749c":"### If you found this notebook interesting please upvote!\n\n![](https:\/\/i.pinimg.com\/originals\/e2\/11\/cc\/e211ccb9e3a579ba8a4a8e25d68b4897.gif)\n\n","6a73e2d4":"# Preprocessing and Data Analysis \ud83d\udcbb\n\n## First look at the data:\n\nTraining set:","62615fd1":"## Data visalization \ud83d\udcca"}}