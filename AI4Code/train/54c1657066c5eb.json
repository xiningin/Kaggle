{"cell_type":{"f92bc692":"code","e666cb27":"code","b0f10385":"code","005f5fd0":"code","30482de6":"code","022e5185":"code","cc23d68f":"code","d3aa404c":"code","16388f29":"code","dcb6c683":"code","6ea352e7":"code","20c2dcc8":"code","68e09a70":"code","b243e8f0":"code","30c0b913":"code","a0429246":"code","4724c5b5":"code","9d8f1164":"code","91a9256f":"code","79d0d909":"code","20b5bea9":"code","b9e95c6b":"code","7948a4c4":"code","c04a3fc0":"code","1af14f8d":"code","6a804c42":"code","a3350c9a":"code","bfc15929":"code","5ddb753f":"code","44e0401c":"code","63c468d3":"code","233b783e":"code","60cb7903":"code","c6f8ad57":"code","b418cb32":"code","1a773fbf":"code","e707658b":"code","8201e4b9":"code","5f422b16":"code","f2037bce":"code","de7af7b3":"code","51b40c35":"code","c98ca428":"code","adef2edc":"code","ecc0a087":"code","8c2ed8b3":"code","1d9b1bdf":"code","f1a70e1c":"code","565d1b3a":"code","5d8a6e2d":"code","428f866e":"code","2f5eaabd":"code","4f73a934":"code","74176968":"code","c2a7adf8":"code","76034417":"code","8597f3ca":"code","ffb7338f":"code","d80bb973":"code","f2ae718d":"code","e993be80":"code","5de89d21":"code","d65b3c8a":"code","35c7896a":"code","90650c52":"code","99755206":"code","81ca0689":"code","adef1b7f":"code","360bbb14":"code","6b802ce5":"code","85f0a208":"code","184ae6e2":"code","7d18d2ae":"code","a71d3328":"code","f58aa759":"code","aad91fc8":"code","6ff6f5f3":"code","1e55892b":"code","d0b0f27c":"code","49c1550d":"code","69af14e9":"code","ff1e3a9b":"code","163c62e8":"code","15cd1f2d":"code","70e72634":"code","8e5a752f":"code","1894e5bc":"code","617ef751":"code","a2eabfb7":"code","455db7b0":"code","f8826438":"code","4bf2c558":"code","3de02327":"code","8bf6ffd7":"code","e6a5ab0c":"code","ee5503a7":"code","6dbf60bf":"code","ea165a99":"code","860fa10d":"code","ac35ea93":"code","7e5191dd":"code","ee0bcded":"code","54169b6b":"code","a7514b23":"code","e3babe3f":"code","e1b7c8aa":"code","0b8bf998":"code","2f365a66":"code","309d2bc0":"code","b32dd17e":"code","a8923daa":"code","4b77babc":"code","c2334a41":"markdown","605a9a28":"markdown","8f35bcb0":"markdown","9640db7f":"markdown","f5ace746":"markdown","a84df4e5":"markdown","73159165":"markdown","addfe9f0":"markdown","cac04bc9":"markdown","d8d59a12":"markdown","22da133e":"markdown","14998035":"markdown","cc3bfc9b":"markdown","2d6e4af7":"markdown","9c6b932e":"markdown","5c0b2dec":"markdown","21438475":"markdown","bb3f8eb8":"markdown","89df1576":"markdown","2b7a9b51":"markdown","1880def9":"markdown","c2bffddb":"markdown","768d35e8":"markdown","bcd9fc4a":"markdown","c40ed46b":"markdown","858686e6":"markdown","6684d71e":"markdown","7ab119be":"markdown","1e6830fb":"markdown","7d177b5e":"markdown","0c29121c":"markdown","2a9d9a58":"markdown","bba3f7bb":"markdown","4e2c4496":"markdown","7ce586f9":"markdown","94c899fa":"markdown","3f8bcee6":"markdown","35dc2c7c":"markdown","ceacf30e":"markdown","3dc60ac0":"markdown","b46ee0fc":"markdown","f5562895":"markdown","7007401a":"markdown"},"source":{"f92bc692":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e666cb27":"import pandas as pd\ndataset = pd.read_csv(\"..\/input\/cottonprices\/cotton-prices-historical-chart-data.csv\")","b0f10385":"dataset.info()","005f5fd0":"dataset.head(10)","30482de6":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(15,8))\nplt.plot(dataset)","022e5185":"training_set = dataset.iloc[:9898].values\ntest_set = dataset.iloc[9898:].values","cc23d68f":"from sklearn.preprocessing import MinMaxScaler\nsc = MinMaxScaler(feature_range = (0, 1))\ntraining_set_scaled = sc.fit_transform(training_set)","d3aa404c":"# Creating a data structure with 60 timesteps and 1 output\nX_train = []\ny_train = []\nfor i in range(60, len(training_set_scaled)):\n    X_train.append(training_set_scaled[i-60:i, 0])\n    y_train.append(training_set_scaled[i, 0])\nX_train, y_train = np.array(X_train), np.array(y_train)","16388f29":"# Reshaping\nX_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))","dcb6c683":"# Importing the Keras libraries and packages\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout","6ea352e7":"regressor = Sequential()","20c2dcc8":"# Adding the first LSTM layer and some Dropout regularisation\nregressor.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\nregressor.add(Dropout(0.2))","68e09a70":"# Adding a second LSTM layer and some Dropout regularisation\nregressor.add(LSTM(units = 50, return_sequences = True))\nregressor.add(Dropout(0.2))","b243e8f0":"# Adding a third LSTM layer and some Dropout regularisation\nregressor.add(LSTM(units = 50, return_sequences = True))\nregressor.add(Dropout(0.2))\n\n# Adding a fourth LSTM layer and some Dropout regularisation\nregressor.add(LSTM(units = 50))\nregressor.add(Dropout(0.2))","30c0b913":"# Adding the output layer\nregressor.add(Dense(units = 1))\n\n# Compiling the RNN\nregressor.compile(optimizer = 'adam', loss = 'mean_squared_error')","a0429246":"# Fitting the RNN to the Training set\nregressor.fit(X_train, y_train, epochs = 100, batch_size = 32)","4724c5b5":"X_test = []\ndataset = dataset.iloc[:].values\nfor i in range(len(training_set), len(dataset)):\n    X_test.append(dataset[i-60:i, 0])\nX_test = np.array(X_test)\nX_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))","9d8f1164":"predicted_test_values = regressor.predict(X_test)\npredicted_test_values = sc.inverse_transform(predicted_test_values)","91a9256f":"predicted_test_values = predicted_test_values","79d0d909":"# Visualising the results\nimport matplotlib.pyplot as plt\n\nplt.plot(test_set, color = 'red', label = 'Real Cotton Price')\nplt.plot(predicted_test_values, color = 'blue', label = 'Predicted Cotton Price')\nplt.title('Cotton Price Prediction')\nplt.xlabel('Time')\nplt.ylabel('Cotton Price')\nplt.legend()\nplt.savefig('prices.jpg')","20b5bea9":"model_json = regressor.to_json()\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)\nregressor.save_weights(\"model.h5\")","b9e95c6b":"# from keras.models import model_from_json\n# # load json and create model\n# json_file = open('..\/input\/trained-values\/model.json', 'r')\n# loaded_model_json = json_file.read()\n# json_file.close()\n# regressor = model_from_json(loaded_model_json)\n# # load weights into new model\n# regressor.load_weights(\"..\/input\/trained-values\/model.h5\")\n# print(\"Loaded model from disk\")","7948a4c4":"regressor2 = Sequential()","c04a3fc0":"# Adding the first LSTM layer and some Dropout regularisation\nregressor2.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\nregressor2.add(Dropout(0.2))","1af14f8d":"# Adding a second LSTM layer and some Dropout regularisation\nregressor2.add(LSTM(units = 50, return_sequences = True))\nregressor2.add(Dropout(0.2))","6a804c42":"# Adding a third LSTM layer and some Dropout regularisation\nregressor2.add(LSTM(units = 50, return_sequences = True))\nregressor2.add(Dropout(0.2))\n\n# Adding a fourth LSTM layer and some Dropout regularisation\nregressor2.add(LSTM(units = 50))\nregressor2.add(Dropout(0.2))","a3350c9a":"# Adding the output layer\nregressor2.add(Dense(units = 1))\n\n# Compiling the RNN\nregressor2.compile(optimizer = 'rmsprop', loss = 'mean_squared_error')","bfc15929":"# Fitting the RNN to the Training set\nregressor2.fit(X_train, y_train, epochs = 100, batch_size = 32)","5ddb753f":"X_test2 = []\ndataset = dataset.iloc[:].values\nfor i in range(len(training_set), len(dataset)):\n    X_test2.append(dataset[i-60:i, 0])\nX_test2 = np.array(X_test2)\nX_test2 = np.reshape(X_test2, (X_test2.shape[0], X_test2.shape[1], 1))","44e0401c":"predicted_test_values2 = regressor2.predict(X_test2)\npredicted_test_values2 = sc.inverse_transform(predicted_test_values2)","63c468d3":"predicted_test_values2 = predicted_test_values2","233b783e":"# Visualising the results\nimport matplotlib.pyplot as plt\n\nplt.plot(test_set, color = 'red', label = 'Real Cotton Price')\nplt.plot(predicted_test_values2, color = 'blue', label = 'Predicted Cotton Price')\nplt.title('Cotton Price Prediction')\nplt.xlabel('Time')\nplt.ylabel('Cotton Price')\nplt.legend()\nplt.savefig('prices.jpg')","60cb7903":"model_json = regressor2.to_json()\nwith open(\"model2.json\", \"w\") as json_file:\n    json_file.write(model_json)\nregressor2.save_weights(\"model2.h5\")","c6f8ad57":"# from keras.models import model_from_json\n# # load json and create model\n# json_file = open('..\/input\/model2\/model2.json', 'r')\n# loaded_model_json = json_file.read()\n# json_file.close()\n# regressor2 = model_from_json(loaded_model_json)\n# # load weights into new model\n# regressor2.load_weights(\"..\/input\/model2\/model2.h5\")\n# print(\"Loaded model from disk\")","b418cb32":"#Import the libraries\nimport math\nimport pandas_datareader as web\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM\nimport matplotlib.pyplot as plt\nfrom keras.layers import Dropout\nplt.style.use('fivethirtyeight')","1a773fbf":"df = pd.read_csv(\"..\/input\/cottonprices\/cotton-prices-historical-chart-data.csv\")\ndf.info()","e707658b":"df.shape","8201e4b9":"plt.figure(figsize=(16,8))\nplt.title('Cotton Price History')\nplt.plot(df)\nplt.ylabel('Close Price USD ($)',fontsize=18)\nplt.show()","5f422b16":"data = df\ndataset = data.values#Get \/Compute the number of rows to train the model on\ntraining_data_len = math.ceil( len(dataset) *.8) ","f2037bce":"#Scale the all of the data to be values between 0 and 1 \nscaler = MinMaxScaler(feature_range=(0, 1)) \nscaled_data = scaler.fit_transform(dataset)","de7af7b3":"#Create the scaled training data set \ntrain_data = scaled_data[0:training_data_len  , : ]#Split the data into x_train and y_train data sets\nx_train=[]\ny_train = []\nfor i in range(60,len(train_data)):\n    x_train.append(train_data[i-60:i,0])\n    y_train.append(train_data[i,0])","51b40c35":"#Convert x_train and y_train to numpy arrays\nx_train, y_train = np.array(x_train), np.array(y_train)","c98ca428":"#Reshape the data into the shape accepted by the LSTM\nx_train = np.reshape(x_train, (x_train.shape[0],x_train.shape[1],1))","adef2edc":"#Build the LSTM network model\nmodel = Sequential()\nmodel.add(LSTM(units=50, return_sequences=True,input_shape=(x_train.shape[1],1)))\nmodel.add(LSTM(units=50, return_sequences=False))\nmodel.add(Dense(units=25))\nmodel.add(Dense(units=1))","ecc0a087":"#Compile the model\nmodel.compile(optimizer='adam', loss='mean_squared_error')","8c2ed8b3":"#Train the model\nmodel.fit(x_train, y_train, batch_size=4, epochs=10)","1d9b1bdf":"#Test data set\ntest_data = scaled_data[training_data_len - 60: , : ]#Create the x_test and y_test data sets\nx_test = []\ny_test =  dataset[training_data_len : , : ] #Get all of the rows from index 1603 to the rest and all of the columns (in this case it's only column 'Close'), so 2003 - 1603 = 400 rows of data\nfor i in range(60,len(test_data)):\n    x_test.append(test_data[i-60:i,0])","f1a70e1c":"x_test[0]","565d1b3a":"#Convert x_test to a numpy array \nx_test = np.array(x_test[0:2])\n#Reshape the data into the shape accepted by the LSTM\nx_test = np.reshape(x_test, (x_test.shape[0],x_test.shape[1],1))","5d8a6e2d":"#Convert x_test to a numpy array \nx_test = np.array(x_test)","428f866e":"#Reshape the data into the shape accepted by the LSTM\nx_test = np.reshape(x_test, (x_test.shape[0],x_test.shape[1],1))","2f5eaabd":"#Getting the models predicted price values\npredictions = model.predict(x_test) \npredictions = scaler.inverse_transform(predictions)#Undo scaling","4f73a934":"#Calculate\/Get the value of RMSE\nrmse=np.sqrt(np.mean(((predictions- y_test)**2)))\nrmse","74176968":"# Visualising the results\nimport matplotlib.pyplot as plt\n\nplt.plot(data[training_data_len:], color = 'red', label = 'Real Cotton Price')\nplt.plot(predictions, color = 'blue', label = 'Predicted Cotton Price')\nplt.title('Cotton Price Prediction')\nplt.xlabel('Time')\nplt.ylabel('Cotton Price')\nplt.legend()\nplt.savefig('prices.jpg')","c2a7adf8":"#Plot\/Create the data for the graph\ntrain = data[:training_data_len]\nvalid = data[training_data_len:]\nvalid['Predictions'] = predictions#Visualize the data\nplt.figure(figsize=(16,8))\nplt.title('Model')\nplt.xlabel('Date', fontsize=18)\nplt.ylabel('Close Price USD ($)', fontsize=18)\nplt.plot(train)\nplt.plot(valid)\nplt.legend(['Train', 'Val', 'Predictions'], loc='lower right')\nplt.show()","76034417":"#Show the valid and predicted prices\nvalid","8597f3ca":"model_json = model.to_json()\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)\nmodel.save_weights(\"model2.h5\")","ffb7338f":"#Import the libraries\nimport math\nimport pandas_datareader as web\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM\nimport matplotlib.pyplot as plt\nfrom keras.layers import Dropout\nplt.style.use('fivethirtyeight')","d80bb973":"df = pd.read_csv(\"..\/input\/cottonprices\/cotton-prices-historical-chart-data.csv\")\ndf.info()","f2ae718d":"plt.figure(figsize=(16,8))\nplt.title('Cotton Price History')\nplt.plot(df)\nplt.ylabel('Close Price USD ($)',fontsize=18)\nplt.show()","e993be80":"def modelTraining(time,neurons,optimizer,batch,epochs,train_data):\n    \n    x_train=[]\n    y_train = []\n    for i in range(time,len(train_data)):\n        x_train.append(train_data[i-time:i,0])\n        y_train.append(train_data[i,0])\n    #Convert x_train and y_train to numpy arrays\n    x_train, y_train = np.array(x_train), np.array(y_train)\n    \n    #Reshape the data into the shape accepted by the LSTM\n    x_train = np.reshape(x_train, (x_train.shape[0],x_train.shape[1],1))\n    \n    #Build the LSTM network model\n    model = Sequential()\n    model.add(LSTM(units=neurons, return_sequences=True,input_shape=(x_train.shape[1],1)))\n    model.add(Dropout(0.2))\n    model.add(LSTM(units=neurons, return_sequences=False))\n    model.add(Dropout(0.2))\n    model.add(Dense(units=25))\n    model.add(Dense(units=1))\n    \n    #Compile the model\n    model.compile(optimizer=optimizer, loss='mean_squared_error')\n    \n    #Train the model\n    model.fit(x_train, y_train, batch_size=batch, epochs=epochs)\n    \n    #Test data set\n    test_data = scaled_data[training_data_len - time: , : ]#Create the x_test and y_test data sets\n    x_test = []\n    y_test =  dataset[training_data_len : , : ] #Get all of the rows from index 1603 to the rest and all of the columns (in this case it's only column 'Close'), so 2003 - 1603 = 400 rows of data\n    for i in range(time,len(test_data)):\n        x_test.append(test_data[i-time:i,0])\n        \n    #Convert x_test to a numpy array \n    x_test = np.array(x_test)\n    \n    #Reshape the data into the shape accepted by the LSTM\n    x_test = np.reshape(x_test, (x_test.shape[0],x_test.shape[1],1))\n    \n    #Getting the models predicted price values\n    predictions = model.predict(x_test) \n    predictions = scaler.inverse_transform(predictions)#Undo scaling\n    \n    #Plot\/Create the data for the graph\n    train = df[:training_data_len]\n    valid = df[training_data_len:]\n    valid['Predictions'] = predictions#Visualize the data\n    plt.figure(figsize=(16,8))\n    plt.title('Model')\n    plt.xlabel('Date', fontsize=18)\n    plt.ylabel('Close Price USD ($)', fontsize=18)\n    plt.plot(train)\n    plt.plot(valid)\n    plt.legend(['Train', 'Val', 'Predictions'], loc='lower right')\n    plt.savefig('img'+str(time)+str(neurons)+str(optimizer)+str(batch)+str(epochs)+'.png')\n    plt.show()\n    \n    #save the model\n    model_json = model.to_json()\n    with open(\"model\"+str(time)+str(neurons)+str(optimizer)+str(batch)+str(epochs)+\".json\", \"w\") as json_file:\n        json_file.write(model_json)\n    model.save_weights(\"model\"+str(time)+str(neurons)+str(optimizer)+str(batch)+str(epochs)+\".h5\")\n    \n    #Calculate\/Get the value of RMSE\n    rmse=np.sqrt(np.mean(((predictions- y_test)**2)))\n    print(rmse)\n    return rmse","5de89d21":"result_error = []\nresult_parameters = []\n\ntime_values = [30,45,60]\nneurons_values = [40,50]\noptimizer_values = ['adam','rmsprop']\nbatch_values = [4,16,32]\nepochs_values = [10,15]\n\ndataset = df.values#Get \/Compute the number of rows to train the model on\ntraining_data_len = math.ceil( len(dataset) *.8) \n\n#Scale the all of the data to be values between 0 and 1 \nscaler = MinMaxScaler(feature_range=(0, 1)) \nscaled_data = scaler.fit_transform(dataset)\n\n#Create the scaled training data set \ntrain_data = scaled_data[0:training_data_len  , : ]#Split the data into x_train and y_train data sets\n\n\nfor t in time_values:\n    for n in neurons_values:\n        for o in optimizer_values:\n            for b in batch_values:\n                for e in epochs_values:\n                    result_parameters.append([t,n,o,b,e])\n                    result_error.append(modelTraining(t,n,o,b,e,train_data))\n","d65b3c8a":"print(result_parameters)\nprint(result_error)","35c7896a":"min_index = result_error.index(min(result_error))","90650c52":"result_parameters[min_index]","99755206":"plt.plot(result_error)","81ca0689":"for i in range(len(result_error)):\n    result_parameters[i].append(result_error[i])","adef1b7f":"conclusion = pd.DataFrame(result_parameters,columns=['learning_time','neurons','optimizer','batch_size','epochs','RMS'])","360bbb14":"conclusion","6b802ce5":"conclusion.sort_values('RMS')","85f0a208":"# Training on 45 days\nresult_error_45 = []\nresult_parameter_45 = [[45,50,'rmsprop',4,10],[45,50,'adam',16,15]]\n\nresult_error_45.append(modelTraining(45,50,'rmsprop',4,10,train_data))\nresult_error_45.append(modelTraining(45,50,'adam',16,15,train_data))","184ae6e2":"print(result_parameter_45)\nprint(result_error_45)","7d18d2ae":"for i in range(len(result_error_45)):\n    result_parameter_45[i].append(result_error_45[i])","a71d3328":"conclusion_45 = pd.DataFrame(result_parameter_45,columns=['learning_time','neurons','optimizer','batch_size','epochs','RMS'])","f58aa759":"conclusion_45","aad91fc8":"# Training on 60 days\nresult_error_60 = []\nresult_parameter_60 = [[60,50,'rmsprop',4,10],[60,50,'adam',16,15]]\n\nresult_error_60.append(modelTraining(60,50,'rmsprop',4,10,train_data))\nresult_error_60.append(modelTraining(60,50,'adam',16,15,train_data))","6ff6f5f3":"print(result_parameter_60)\nprint(result_error_60)","1e55892b":"for i in range(len(result_error_60)):\n    result_parameter_60[i].append(result_error_60[i])","d0b0f27c":"conclusion_60 = pd.DataFrame(result_parameter_60,columns=['learning_time','neurons','optimizer','batch_size','epochs','RMS'])","49c1550d":"conclusion_60","69af14e9":"results = pd.concat([conclusion,conclusion_45,conclusion_60], axis=0)\nresults","ff1e3a9b":"results.sort_values('RMS')","163c62e8":"from keras.models import model_from_json\n# load json and create model\njson_file = open('..\/input\/bestrnnparameters\/model3040rmsprop410.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nregressor2 = model_from_json(loaded_model_json)\n# load weights into new model\nregressor2.load_weights(\"..\/input\/bestrnnparameters\/model3040rmsprop410.h5\")\nprint(\"Loaded model from disk\")","15cd1f2d":"#Getting the models predicted price values\npredictions = regressor2.predict(x_test) \nprint(predictions)\npredictions = scaler.inverse_transform(predictions)#Undo scaling","70e72634":"#Import the libraries\nimport math\nimport pandas_datareader as web\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM\nimport matplotlib.pyplot as plt\nfrom keras.layers import Dropout\nplt.style.use('fivethirtyeight')","8e5a752f":"df = pd.read_csv(\"..\/input\/dataset3\/Price.csv\")\ndf.info()","1894e5bc":"df.head(5)","617ef751":"df = df[df['Total Value (Lacs)']!=0]\ndf['price'] = df['Total Value (Lacs)']\/df[\"Quantity (000's)\"]","a2eabfb7":"plt.figure(figsize=(16,8))\nplt.title('Cotton Price History')\nplt.plot(df['price'])\n# plt.xticks(list(range(0,len(df))),df['Date'])\nplt.ylabel('Price',fontsize=18)\nplt.show()","455db7b0":"def modelTraining(time,neurons,optimizer,batch,epochs,train_data):\n    \n    x_train=[]\n    y_train = []\n    for i in range(time,len(train_data)):\n        x_train.append(train_data[i-time:i,0])\n        y_train.append(train_data[i,0])\n    #Convert x_train and y_train to numpy arrays\n    x_train, y_train = np.array(x_train), np.array(y_train)\n    \n    #Reshape the data into the shape accepted by the LSTM\n    x_train = np.reshape(x_train, (x_train.shape[0],x_train.shape[1],1))\n    \n    #Build the LSTM network model\n    model = Sequential()\n    model.add(LSTM(units=neurons, return_sequences=True,input_shape=(x_train.shape[1],1)))\n    model.add(Dropout(0.2))\n    model.add(LSTM(units=neurons, return_sequences=False))\n    model.add(Dropout(0.2))\n#     model.add(LSTM(units=neurons, return_sequences=False))\n#     model.add(Dropout(0.2))\n    model.add(Dense(units=25))\n    model.add(Dense(units=1))\n    \n    #Compile the model\n    model.compile(optimizer=optimizer, loss='mean_squared_error')\n    print(\"Reached here\")\n    \n    #Train the model\n    model.fit(x_train, y_train, batch_size=batch, epochs=epochs)\n    \n    #Test data set\n    test_data = scaled_data[training_data_len - time: , : ]#Create the x_test and y_test data sets\n    x_test = []\n    y_test =  dataset[training_data_len : , : ] #Get all of the rows from index 1603 to the rest and all of the columns (in this case it's only column 'Close'), so 2003 - 1603 = 400 rows of data\n    for i in range(time,len(test_data)):\n        x_test.append(test_data[i-time:i,0])\n        \n    #Convert x_test to a numpy array \n    x_test = np.array(x_test)\n    \n    #Reshape the data into the shape accepted by the LSTM\n    x_test = np.reshape(x_test, (x_test.shape[0],x_test.shape[1],1))\n    \n    #Getting the models predicted price values\n    predictions = model.predict(x_test) \n    predictions = scaler.inverse_transform(predictions)#Undo scaling\n    \n    #Plot\/Create the data for the graph\n    train = df[:training_data_len]\n    valid = df[training_data_len:]\n    valid['Predictions'] = predictions#Visualize the data\n    plt.figure(figsize=(16,8))\n    plt.title('Model')\n    plt.xlabel('Date', fontsize=18)\n    plt.ylabel('Price', fontsize=18)\n    plt.plot(train)\n    plt.plot(valid)\n    plt.legend(['Train', 'Val', 'Predictions'], loc='lower right')\n    plt.savefig('img2-'+str(time)+str(neurons)+str(optimizer)+str(batch)+str(epochs)+'.png')\n    plt.show()\n    \n    #save the model\n    model_json = model.to_json()\n    with open(\"model2-\"+str(time)+str(neurons)+str(optimizer)+str(batch)+str(epochs)+\".json\", \"w\") as json_file:\n        json_file.write(model_json)\n    model.save_weights(\"model2-\"+str(time)+str(neurons)+str(optimizer)+str(batch)+str(epochs)+\".h5\")\n    \n    #Calculate\/Get the value of RMSE\n    rmse=np.sqrt(np.mean(((predictions- y_test)**2)))\n    print(rmse)\n    return rmse","f8826438":"df = pd.DataFrame(df['price'])\ndataset = df.values#Get \/Compute the number of rows to train the model on\ntraining_data_len = math.ceil( len(dataset) *.8) \n\n#Scale the all of the data to be values between 0 and 1 \nscaler = MinMaxScaler(feature_range=(0, 1)) \nscaled_data = scaler.fit_transform(dataset)\n\n#Create the scaled training data set \ntrain_data = scaled_data[0:training_data_len  , : ]#Split the data into x_train and y_train data sets\nprint(\"Root mean Square error =\",modelTraining(60,50,'rmsprop',4,10,train_data))\n","4bf2c558":"result_error = []\nresult_parameters = []\n\ntime_values = [60]\nneurons_values = [40,50]\noptimizer_values = ['adam','rmsprop']\nbatch_values = [4,16,32]\nepochs_values = [10,15]\n\n# dataset = df.values#Get \/Compute the number of rows to train the model on\n# training_data_len = math.ceil( len(dataset) *.8) \n\n# #Scale the all of the data to be values between 0 and 1 \n# scaler = MinMaxScaler(feature_range=(0, 1)) \n# scaled_data = scaler.fit_transform(dataset)\n\n# #Create the scaled training data set \n# train_data = scaled_data[0:training_data_len  , : ]#Split the data into x_train and y_train data sets\n\n\nfor t in time_values:\n    for n in neurons_values:\n        for o in optimizer_values:\n            for b in batch_values:\n                for e in epochs_values:\n                    result_parameters.append([t,n,o,b,e])\n                    result_error.append(modelTraining(t,n,o,b,e,train_data))\n","3de02327":"print(result_parameters)\nprint(result_error)","8bf6ffd7":"min_index = result_error.index(min(result_error))\nresult_parameters[min_index]","e6a5ab0c":"plt.plot(result_error)","ee5503a7":"for i in range(len(result_error)):\n    result_parameters[i].append(result_error[i])","6dbf60bf":"result_parameters","ea165a99":"conclusion = pd.DataFrame(result_parameters,columns=['learning_time','neurons','optimizer','batch_size','epochs','RMS'])","860fa10d":"conclusion","ac35ea93":"conclusion.sort_values('RMS')","7e5191dd":"# Training on 45 days\nresult_error_45 = []\nresult_parameter_45 = [[45,40,'rmsprop',4,15],[45,50,'adam',4,15]]\n\nresult_error_45.append(modelTraining(45,40,'rmsprop',4,15,train_data))\nresult_error_45.append(modelTraining(45,50,'adam',4,15,train_data))","ee0bcded":"print(result_parameter_45)\nprint(result_error_45)","54169b6b":"for i in range(len(result_error_45)):\n    result_parameter_45[i].append(result_error_45[i])","a7514b23":"conclusion_45 = pd.DataFrame(result_parameter_45,columns=['learning_time','neurons','optimizer','batch_size','epochs','RMS'])","e3babe3f":"conclusion_45","e1b7c8aa":"# Training on 30 days\nresult_error_30 = []\nresult_parameter_30 = [[30,40,'rmsprop',4,15],[30,50,'adam',4,15]]\n\nresult_error_30.append(modelTraining(30,40,'rmsprop',4,15,train_data))\nresult_error_30.append(modelTraining(30,50,'adam',4,15,train_data))","0b8bf998":"print(result_parameter_30)\nprint(result_error_30)","2f365a66":"for i in range(len(result_error_30)):\n    result_parameter_30[i].append(result_error_30[i])","309d2bc0":"conclusion_30 = pd.DataFrame(result_parameter_30,columns=['learning_time','neurons','optimizer','batch_size','epochs','RMS'])","b32dd17e":"conclusion_30","a8923daa":"results = pd.concat([conclusion,conclusion_45,conclusion_30], axis=0)\nresults","4b77babc":"results.sort_values('RMS') ","c2334a41":"# Top 3 model parameters with least error","605a9a28":"# **Changing the optimizer**","8f35bcb0":"**First DataSet found at [macrotrends](https:\/\/www.macrotrends.net\/2533\/cotton-prices-historical-chart-data)**","9640db7f":"## Prediction","f5ace746":"## Model Architecture","a84df4e5":"# Version 2","73159165":"## Preprocessing","addfe9f0":"Changing the optimizer doesnot solve our problem. So lets try with some other architecture design.","cac04bc9":"**As can be seen from above visualization, the model is following the trend properly but the values are not accurate**","d8d59a12":"# 2. RMS = 1.889552\n\n## Parameters\n\n\n## Time = 60, Neurons = 50, Optimizer =ADAM, Batch_size = 4, Epochs = 15\n![img2-6050adam415.png](attachment:img2-6050adam415.png)","22da133e":"## Loading above trained model ","14998035":"# 2. RMS = 0.018285\n\n## Parameters\n### Time = 45, Neurons = 50, Optimizer = Adam, Batch_size = 16, Epochs = 15\n\n![img4550adam1615.png](attachment:img4550adam1615.png)\n","cc3bfc9b":"**Results in Tabular form**","2d6e4af7":"# 1. RMS = 0.017960\n\n\n## Parameters\n### Time = 30, Neurons = 40, Optimizer = RMSPROP, Batch_size = 4, Epochs = 10\n\n![img3040rmsprop410.png](attachment:img3040rmsprop410.png)\n","9c6b932e":"# 3. RMS = 0.018588\n\n## Parameters\n### Time = 60, Neurons = 50, Optimizer = RMSPROP, Batch_size = 4, Epochs = 10\n![img6050rmsprop410.png](attachment:img6050rmsprop410.png)","5c0b2dec":"## New model architecture","21438475":"# Choosing best parameters","bb3f8eb8":"## Prediction","89df1576":"## Saving the model","2b7a9b51":"## Predicting values","1880def9":"## Preprocessing","c2bffddb":"## Training on 30 days as lookup time","768d35e8":"# 3. RMS = 1.893024\n\n## Parameters\n\n## Time = 30, Neurons = 40, Optimizer = RMSPROP, Batch_size = 4, Epochs = 15\n![img2-3040rmsprop415.png](attachment:img2-3040rmsprop415.png)","bcd9fc4a":"## Training model on training data","c40ed46b":"# 1. RMS = 1.834648\n\n## Parameters\n\n### Time = 60, Neurons = 40, Optimizer = RMSPROP, Batch_size = 4, Epochs = 15\n![img2-6040rmsprop415.png](attachment:img2-6040rmsprop415.png)","858686e6":"## Spliting data as training and test data","6684d71e":"**Prameter Tunning result for 60 days time lookup**","7ab119be":"## Applying Parameter Tuning on this dataset","1e6830fb":"# Training above 2 choosen parameters sets on Time = 45days ","7d177b5e":"## Training on 45 days as lookup Time","0c29121c":"# Training above 2 choosen parameters sets on Time = 60 days ","2a9d9a58":"## Saving the model","bba3f7bb":"# Top 3 model parameters with least error","4e2c4496":"# Loading any above trained model","7ce586f9":"## Results in tabular form","94c899fa":"**Parameter Tuning Results for 30 days of time set in LSTM**","3f8bcee6":"## Overall results","35dc2c7c":"## Loading above trained model","ceacf30e":"## Saving the model","3dc60ac0":"# New Dataset","b46ee0fc":"## Cotton price values","f5562895":"# Observations\n\n* **The Best Result comes with parameters: Neurons = 40, Optimizer = RMSPROP, Batch_size = 4, Epochs = 15**\n\n* So we will consider above 2 best parametrs and test for time parameters: 45 and 30 to see if we can increse the accuracy\n\n* We could have added the Time parameter in parameter tuning but it then require 72 combinations. Our current hardware and kaggle use limit does not permit this","7007401a":"# Observations\n\n* The Best Result comes with parameters: **Neurons = 40**, **Optimizer = RMSPROP**, **Batch_size = 4**, **Epochs = 10**\n* Overall Results show that using **Neurons = 50**, **Optimizer = Adam**, **Epochs = 15** gives optimal results with all batch size\n* So we will consider above 2 parametrs (In second case the batch size 16 will be used) and test for time parameters: 45 and 60 to see if we can increse the accuracy\n* We could have added the Time parameter in parameter tuning but it then require 72 combinations. Our current hardware and kaggle use limit does not permit this"}}