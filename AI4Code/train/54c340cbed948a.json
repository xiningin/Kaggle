{"cell_type":{"60d98f77":"code","fb49af86":"code","0ccf470c":"code","1781b0a7":"code","9160505e":"code","37a15f59":"code","c4d490a2":"code","2812db77":"code","6383d463":"code","60c4f65d":"code","f16f3975":"code","af8e2867":"code","4a90a6a8":"code","baf16048":"code","fc5089a0":"code","59329ae1":"code","775d9b30":"code","d0fab60b":"code","084103aa":"code","4babdf73":"code","900ddfa9":"code","5b1d4f88":"code","bf7ed72f":"code","1c4c33a8":"code","68dda02a":"code","c5dd6ed1":"code","937b40b6":"code","ab361695":"code","23a6fa3c":"code","daa0fdae":"code","c9f27b57":"code","920fa0f0":"code","9df03342":"code","691c02fa":"code","dd4f0450":"code","242a8876":"code","2a493314":"code","042b57c9":"code","86f84db1":"code","85546f22":"code","4bbc3cfd":"code","fe253034":"code","8dcfa75a":"code","b7683b53":"code","fe4d26b7":"code","1d652968":"code","b86a97d3":"code","3e8d074f":"code","201da153":"code","6894a402":"code","14911f52":"code","4886f2c4":"code","51b74a41":"code","7c7f1343":"code","ac9342b6":"code","9851c772":"code","38064ba5":"code","772d5f57":"code","39dd1a4c":"code","8f73f813":"code","3206cd3a":"code","0073516d":"code","f94ba37f":"markdown","f069e5b8":"markdown","c71678e4":"markdown","e072166f":"markdown","4c1d7c45":"markdown"},"source":{"60d98f77":"!pip install tf-nightly","fb49af86":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dropout, Dense, Conv2D, MaxPool2D, Flatten\nfrom tensorflow.keras.callbacks import TensorBoard\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nimport time\nprint(tf.__version__)\n\nimport os\nimport csv\nimport sys\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras.preprocessing.image import load_img\nimport cv2\nfrom PIL import Image\nfrom skimage.transform import resize\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import Sequence,to_categorical\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, TensorBoard\nfrom tensorflow.keras import optimizers, losses, activations, models\nfrom tensorflow.keras.layers import Convolution2D, Dense, Input, Flatten, Dropout, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D, Concatenate\nfrom tensorflow.keras import applications\n\n%matplotlib inline","0ccf470c":"! ls \/kaggle\/input\/inception-weights-v1\/inception","1781b0a7":"IMGS_PER_CLASS = 13333\nENFORCE_COUNT_UNTOUCHED = 7000 # set to None to deactivate \n\nM1 = '\/kaggle\/input\/ft-tl-inception\/checkpoints\/15-epochs-64-0.3-sgd-8e-05.h5'\nM2 = '\/kaggle\/input\/inception-weights-v1\/inception\/2.h5'\nM3 = '\/kaggle\/input\/inception-weights-v1\/inception\/3.h5'\nMf1 = '\/kaggle\/input\/inception-weights-v1\/inception\/f1-15-epochs-64-0.3-rmsprop-0.0001.h5'\nMf2 = '\/kaggle\/input\/inception-weights-v1\/inception\/f2-7-epochs-64-0.4-adam-0.0006.h5'\nM_EN = '\/kaggle\/input\/good-efficientnet-b3-adam-0-001-14-ep\/checkpoints\/15-epochs-32-0.2-adam-0.001.h5'\n\nNCLASS = 3\nBATCH_SIZE = 32\nSEED = 123\nrandom.seed(SEED)","9160505e":"# root_path = \"\/kaggle\/input\/selfie-classification-wiki\"\n# image_names_csv = os.path.join(root_path, \"merged_big.csv\")\n# images_folder = os.path.join(root_path, \"merged_big\")\n\n# p = pd.read_csv(image_names_csv)\n# #print(p.head(5),'\\n_____________________________')\n\n# ppl_df = p[p['class']== 1]\n# slf_df = p[p['class']== 0]\n# rnd_df = p[p['class']== 2]\n\n# ppl_len = IMGS_PER_CLASS #ppl_df.shape[0]\n# ppl_len = min(IMGS_PER_CLASS, min(slf_df.shape[0], rnd_df.shape[0], ppl_df.shape[0]))\n# if ppl_len != IMGS_PER_CLASS:\n#     print('WARNING: adjuested IMGS_PER_CLASS to the max={}'.format(ppl_len))\n\n# slf_full_df = slf_df\n# slf_df = slf_df.sample(n = ppl_len, random_state = SEED)\n# rnd_df = rnd_df.sample(n = ppl_len, random_state = SEED)\n# ppl_df = ppl_df.sample(n = ppl_len, random_state = SEED)","37a15f59":"# slf_test_df = slf_full_df[~slf_full_df.apply(tuple,1).isin(slf_df.apply(tuple,1))]\n# slf_test_df.reset_index(inplace=True, drop=True)\n# if ENFORCE_COUNT_UNTOUCHED: slf_test_df = slf_test_df.sample(n=ENFORCE_COUNT_UNTOUCHED, random_state=SEED)\n# # slf_test_df","c4d490a2":"# frames = [slf_test_df]\n# p = pd.concat(frames)\n\n# p = p[(p['filename'] != 'BGRibeseya.jpg') & (p['filename'] != '.DS_Store') & (p['filename'] != '.cache')]\n\n# p['filename'] = p['filename'].apply(lambda x: os.path.join(images_folder,x))\n\n# p = p.sample(frac=1, axis=0,random_state = SEED)\n# p.reset_index(drop=True, inplace = True)\n\n# p","2812db77":"# X = np.array(p['filename'])\n# y = np.array(p['class'])\n# labels = { p['filename'][i] : p['class'][i] for i in range(p.shape[0])}","6383d463":"# ! ls '\/kaggle\/input\/portraits-with-demography-dataset\/data'","60c4f65d":"bad_paths = [\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/ad60fd8f0c651d0b0d6db29c562006c9.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/e9660d78a34fc8173d339ab7f8360e58.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/fc4310a49d6bee0b1bc7337963cf53ae.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/3c5189fb065e9a7cb016e2da109644f5.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/c33d9fb2124166c3e0acd3f3288d2261.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/01d34d3444f7f7d65ff5bea005a94f55.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/67a0ad40ba8545e2e1d3505db26f3e83.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/317fe9c3591817b78b7493bdac560e46.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/67d2a626d82c21b4fc2abe61b45ce3d7.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/f51ad5bc75d9d1625b2633746c11e89b.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/ab31a56c0afdfc419845f7f625adf749.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/f303a9fcdf5551a19599923295d35336.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/c00a28d511400d9ceeeb2d193d7dcd45.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/cce3e968c6a7d8cfe1752481f832a3e4.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/a2aaefcd61c8ede2aaea34ffacb01f79.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/5ebbeb0258b87199550d8c89fece7f95.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/49cb99e84145cecebc5d251b59ed25b0.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/8c4b4509ad7c5ead02a2a39f9646567f.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/7bbb6873a1b496a30f1e326a5363daf5.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/35762e9f9a824c591c46044f914fb824.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/f968ab7a9db29ed4e9d969b327a7b674.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/bb1254adfecea28ddf38068fe52b1230.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/92ca6cc6e5c1f66fa716f0fa26256e95.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/5d192b5b1a74b0f45c0e5fa2578f3486.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/5fdd32ffa2ed2a440d377b07aee286db.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/4170eb6ed9331bb01e1f4ee5d53b5f31.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/7bab8e2952ea2b055cb100ea3490a777.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/7bbb6873a1b496a30f1e326a5363daf5.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/2fc542888302f620054818fa7c3d47f0.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/e50e1150d4b98fd1e8603959256956dd.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/8a5d9e49da42ef0710643ce2088277d2.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/bba4f9398e896288f2249e24724cac91.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/99edafe3d87966efe65203c91fa4c082.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/b4430b6ebb99fbe5253947caa3e03ed8.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/9bcf9d55097b2b5bf89e3630375903b2.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/6b914a1382608b39ee766d97a048656a.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/f816ed7d960c0b8467f18e8c729db06d.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/0fb43489a985c57259a18da1e47c7131.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/075799efad3db7d83160b5b7edb541b2.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/b18f07e20a8d027d94e7625f949c9dfd.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/3c5189fb065e9a7cb016e2da109644f5.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/c8146ca312a31e8d9766968e4d0a26cc.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/ff48aaa5836f351414812123b6cd59d4.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/ad63c319049dbca259f53174c0a412c2.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/451789ea6f8508f775ac62b90c06880c.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/5b9b05ab1bb56de8d122cda00ef6bda1.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/2208b106eb22a7be1eb66fd53dacd0f7.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/03f1fd0ffd99041f26a8932c8505f148.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/85c5d45c69a44cbf042aee38cce02442.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/2208b106eb22a7be1eb66fd53dacd0f7.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/d423c62229f0b3c57515012d5e28a96f.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/a2aaefcd61c8ede2aaea34ffacb01f79.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/98aae8f1dc10ba7de84fc0fba91c3fd6.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/95b65fb69bb6075fb2f82a94fe76e1ec.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/c5aad77941347601bfb58c669312e9b1.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/89ef21dba367584534edc53a2cf88002.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/04b2f080fe1219e0b9ceaf68a1a45907.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/d2ce613cd8ebd6d00d111ee2baee5f7e.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/4de022a61ea980977a06784cab507254.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/7bbb6873a1b496a30f1e326a5363daf5.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/c1b8e33980b97514baf4875db1d2bb0e.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/0e534bbf80f0c20638d8f588bc459717.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/5e00b1d2bd1c0333affe506a22f8c071.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/f2908d12eb93f9b2edab9867167baee8.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/3c5189fb065e9a7cb016e2da109644f5.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/9804da5e32cfcf44e2867b3e7c05c9f8.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/9804da5e32cfcf44e2867b3e7c05c9f8.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/3e912e4b3fe2a03f2e7c6a66b6b54965.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/56040dfca039460c106a11bd2ba3a38f.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/5ebbeb0258b87199550d8c89fece7f95.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/c0db22029cc5087c5876832257c705c4.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/63c6e1f02fcf3c5c68db0dfc286d58bb.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/70bbe5649fe7d174a7de3d3dcabae597.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/9133271c36552f8fff41a5dcd44e34b7.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/23d9120f1d24741717c9b47e6cde1951.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/e1c515044185c383e5e59aa64ad91c8e.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/567a8217a70b286acb96cc699ea09d0b.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/fe9c46b3a76efe1aeb460888685c039b.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/55a19b9061aba5edbbf6e69ca5c3c898.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/635104c9f54815eaa33aaacbe3fa0f7f.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/4262450b2bd698ce91a1b5d5b161b13c.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/42fbe356285cb224d9081fb677d7df3b.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/ea296090404a7a78d65a2998cc384519.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/4cae38bb08eb074e34a1bebf92ff18bd.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/66c09a29d5ceeb49cb8f9c69ace5a162.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/54e8d3d737fdafac218174d1533c5830.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/b68e1d9dc979ace02fc8bed3d4b3aed6.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/ea296090404a7a78d65a2998cc384519.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/c40534e69e96dcca960166bc05d536f9.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/f2908d12eb93f9b2edab9867167baee8.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/c9bb4d835a2f22b71c0543c8045c05b0.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/54e8d3d737fdafac218174d1533c5830.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/74f339ab4bcceaeb365b33653d05ce22.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/fee951aea90e212fef059c1adc0c0962.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/87fd832501bfd8daa8ee533cfc068706.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/d8cf2965d958b8ff41e86d55ed221972.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/c98217a225f8ea9b41dfbfb6497621e5.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/7b6753dfa8e61db3dcdba13f4ed9c0a9.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/f7a78dfe46bea90ceffbf8c0f16d9d3e.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/c99041a608b6a87e584040359521e9df.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/ff48aaa5836f351414812123b6cd59d4.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/da963c69ed7b83b4fafd2d793a82dfe8.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/27ced020c11ddbbd8d3c72fc523230c0.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/7a5ea41f83e36cbb7508301409f35558.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/15cae4d5d36dc77c5b9848b0cd3d0fd3.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/5ebbeb0258b87199550d8c89fece7f95.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/1d88929c5a1abc9528d773ff1405b64b.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/1c703bdb7a37f06b76f85928caacd523.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/99a476097f7a3f9a8a94ecb2b977fc5d.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/ed0aba81629ba5261e133afd9ad55b21.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/c56be9f162f26e83e517dd212660d01d.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/00eec877b4fd685e910bf3183a4379e9.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/f968ab7a9db29ed4e9d969b327a7b674.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/5e2b23b7ad482482750d80024f163c58.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/755c3d665a64db2954841239d1abefeb.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/412d55f723bc68f901c9252a1dad7357.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/bc9a65fdd6c56d9f8dc0caa6d4731586.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/f8adabce022dde0c03ab2641b47a88cf.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/1ebe4af3b7495d95e83d0a4e1507fa69.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/ecced7e084139b34b2b5af6d02a26225.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/597a09f1c23e2c0b63ddcfd9a3b60444.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/e78365175669e91bda703b1b2a3c2175.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/58ce13c63c4e99f1353d656c212ec4dc.jpg\",\n\"\/kaggle\/input\/portraits-with-demography-dataset\/data\/a8299811920c723bbcb7b0b8f0d894dc.jpg\",\n]\n\nlen(bad_paths)","f16f3975":"images_folder = '\/kaggle\/input\/portraits-with-demography-dataset\/data'\nl = pd.read_csv('\/kaggle\/input\/portraits-with-demography-dataset\/bias_filename.tsv', sep='\\t')\n\ngend_ethn_df = l[[\"filename\",\"genderLabel\", \"ethnicityLabel\"]]\ngend_ethn_df = gend_ethn_df.sample(n = 29226, random_state = SEED)\n\n# gend_ethn_df = gend_ethn_df[(gend_ethn_df['filename'] != 'a8299811920c723bbcb7b0b8f0d894dc.jpg')]\n\ngend_ethn_df['filename'] = gend_ethn_df['filename'].apply(lambda x: os.path.join(images_folder,x))\ngend_ethn_df = gend_ethn_df[[x not in bad_paths for x in gend_ethn_df['filename']]]\n\ngend_ethn_df.reset_index(inplace=True, drop=True)\ngend_ethn_df","af8e2867":"X = np.array(gend_ethn_df['filename'])\ny = np.array([1] * len(X))\nlabels = { x : 1 for x in X}","4a90a6a8":"#m = gend_ethn_df[\"ethnicityLabel\"].unique().tolist()\n\nj = gend_ethn_df[\"ethnicityLabel\"].value_counts()\nprint(j.head(55))\n\n    \n# ethnicityLabel 1086 1089\n# dobLabel 19961\n# genderLabel 6 \n# 55 over 50\n# 32 over 100","baf16048":"def load_img_as_arr(img_path):\n    img = cv2.imread(img_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    return cv2.resize(img, (299, 299), interpolation = cv2.INTER_AREA)\n\ndef preprocess_img(img_path):\n    try:\n        img = load_img_as_arr(img_path)\n    except:\n        print(img_path)\n        return None\n    img = tf.cast(img, tf.float32)\n    img = (img \/ 255.)\n    return img","fc5089a0":"class DataGenerator(Sequence):\n    'Generates data for Keras'\n    def __init__(self, list_IDs, labels, batch_size=32, dim=(299,299), n_channels=3,\n                 n_classes=3, shuffle=True):\n        'Initialization'\n        self.dim = dim\n        self.batch_size = batch_size\n        self.labels = labels\n        self.list_IDs = list_IDs\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.shuffle = shuffle\n        self.on_epoch_end()\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_IDs) \/ self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        # Find list of IDs\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n\n        # Generate data\n        X, y = self.__data_generation(list_IDs_temp)\n\n        return X, y\n    \n    def get_filenames(self):\n        n = len(self) * self.batch_size\n        return [self.list_IDs[k] for k in self.indexes[:n]]\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n            \n    def get_list_IDs(self):\n        return [self.list_IDs[i] for i in self.indexes]\n\n    def __data_generation(self, list_IDs_temp):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        # X = np.empty((self.batch_size, *self.dim, self.n_channels))\n        X = np.zeros((self.batch_size, *self.dim, self.n_channels), dtype=np.float32)\n        y = np.empty((self.batch_size), dtype=int)\n        s = 0\n\n        for i, ID in enumerate(list_IDs_temp):\n            img_resize = preprocess_img(ID)\n            if img_resize == None:\n                s += 1\n                continue\n            if img_resize.shape != (299,299,3):\n                continue\n            # X[i,] = np.load('data\/' + ID + '.npy')\n            X[i - s] = np.dstack([img_resize])\n            y[i - s] = self.labels[ID]\n        return  X, to_categorical(y, num_classes=self.n_classes)","59329ae1":"pre_trained_model = applications.InceptionV3(\n    input_shape = (299, 299, 3), # Shape of our images\n    include_top = False, # Leave out the last fully connected layer\n    weights = 'imagenet'\n)\n\nnot_trained_model = applications.InceptionV3(\n    input_shape = (299, 299, 3), # Shape of our images\n    include_top = False, # Leave out the last fully connected layer\n    weights = None\n)\n\n\nefnet_pre_trained_model = applications.EfficientNetB3(\n    input_shape = (299, 299, 3), # Shape of our images\n    include_top = False, # Leave out the last fully connected layer\n    weights = None\n)","775d9b30":"def get_exp_pred_vals(model, test_generator):\n    print(\"=============================================================================================\")\n    y_true = []\n    y_pred = []\n    x = []\n\n    y_pred_ = []\n    y_p = []\n    count = 0\n    for X, y in test_generator:\n        count += 1\n        pred = model.predict(X)\n\n#         x += list(X)\n        y_true += list(np.argmax(y, axis=1))\n        y_pred += list(np.argmax(pred, axis=1))\n        y_p = list(np.argmax(pred, axis=1))\n        y_pred_ += [pred[i][y_p[i]] for i in range(len(y_p))]\n    \n    x = test_generator.get_filenames()\n    print(len(x), len(y_true), len(y_pred), len(y_pred_))\n    d = {\"x\": x, \"y_true\": y_true, \"y_pred\": y_pred, \"y_pred_\": y_pred_}\n    df = pd.DataFrame(d, columns=[\"x\", \"y_true\", \"y_pred\", \"y_pred_\"])\n    df = df.sort_values('y_pred_', ascending = True)\n    \n    return list(df[\"x\"]), list(df.y_true), list(df.y_pred)","d0fab60b":"from tensorboard.plugins.hparams import api as hp\n\n%load_ext tensorboard\n# Clear any logs from previous runs\n!rm -rf .\/logs\/ \n\n    \nHP_BATCH_SIZE = hp.HParam('batch_size', hp.Discrete([64]))\nHP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.001])) #0.0001,0.001,\nHP_DROPOUT = hp.HParam('dropout', hp.Discrete([0.2])) #0.3, 0.4, 0.35, 0.5\nHP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam']))#, 'rmsprop','adam', 'adadelta']))\n\nMETRIC_ACCURACY = 'accuracy'","084103aa":"def create_model(hparams, base_model = pre_trained_model, weights = None):\n    nclass = NCLASS\n    add_model = Sequential()\n    add_model.add(base_model)\n    add_model.add(GlobalAveragePooling2D())\n    if weights == (M1 or M3):\n        add_model.add(Dense(1024, activation='relu'))\n    add_model.add(Dropout(hparams[HP_DROPOUT]))\n    add_model.add(Dense(nclass, activation='softmax'))\n    model = add_model\n    \n    optimizer = None\n    if hparams[HP_OPTIMIZER] == 'adam':\n        optimizer = optimizers.Adam(learning_rate=hparams[HP_LEARNING_RATE])\n    elif hparams[HP_OPTIMIZER] == 'sgd':\n        optimizer = optimizers.SGD(learning_rate=hparams[HP_LEARNING_RATE], momentum=0.9)\n    elif hparams[HP_OPTIMIZER] == 'rmsprop':\n        optimizer = optimizers.RMSprop(learning_rate=hparams[HP_LEARNING_RATE])\n    elif hparams[HP_OPTIMIZER] == 'adadelta':\n        optimizer = optimizers.Adadelta(learning_rate=hparams[HP_LEARNING_RATE])\n    else:\n        raise Exception(\"Unknown HP_OPTIMIZER value =\", hparams[HP_OPTIMIZER])\n    \n    if weights != None:\n        #model = tf.keras.models.load_model(weights)\n        if weights == (M1):\n            train_from_layer = 249\n            model.layers[0].trainable = True\n            for layer in model.layers[0].layers[:train_from_layer]:\n                layer.trainable =  False\n            model.load_weights(weights)\n        elif weights == Mf1 or weights == Mf2 or weights == M_EN:\n            #model.layers[0].trainable = True\n            model.load_weights(weights)\n        else:\n            model = tf.keras.models.load_model(weights)\n    \n    model.compile(\n        loss='categorical_crossentropy', \n        optimizer=optimizer,\n        metrics=['accuracy']\n\n    )\n    \n    return model","4babdf73":"def results(fine_tuned_model, base_model):\n    hparams = {\n            HP_BATCH_SIZE: 64,\n            HP_DROPOUT: 0.4,\n            HP_OPTIMIZER: 'adam',\n            HP_LEARNING_RATE: 0.0001}\n    params = {\n            'dim': (299, 299),\n            'batch_size': hparams[HP_BATCH_SIZE],\n            'n_classes': 3,\n            'n_channels': 3,\n        }\n    model = create_model(hparams,base_model, fine_tuned_model)\n    test_generator = DataGenerator(\n        X, labels, **params, shuffle = False\n    )\n\n\n#     loss, acc = model.evaluate_generator(test_generator)\n#     print('Loaded model loss: ', loss)\n#     print('Loaded model accuracy: ', str(acc))\n\n    x, y_true, y_pred = get_exp_pred_vals(model, test_generator)\n    \n    np.set_printoptions(precision=3)\n#     cm = confusion_matrix(y_true, y_pred, normalize='true')\n#     print(\"\\nConfusion matrix of class accuracies\\n\", cm)\n    return np.array(x), np.array(y_true), np.array(y_pred)","900ddfa9":"# def create_model(hparams, base_model=pre_trained_model, weights=None):\n#     nclass = NCLASS\n#     add_model = Sequential()\n#     add_model.add(base_model)\n#     add_model.add(GlobalAveragePooling2D())\n#     if weights == (M1 or M3):\n#         add_model.add(Dense(1024, activation='relu'))\n#     add_model.add(Dropout(0.4))\n#     add_model.add(Dense(nclass, activation='softmax'))\n#     model = add_model\n        \n#     if weights != None:\n#         if weights == (M1):\n# #             train_from_layer = 249\n# #             model.layers[0].trainable = True\n# #             for layer in model.layers[0].layers[:train_from_layer]:\n# #                 layer.trainable =  False\n#             model.load_weights(weights)\n#         elif weights == Mf1 or weights == Mf2 or weights == M_EN:\n#             model.load_weights(weights)\n#         else:\n#             model = tf.keras.models.load_model(weights)\n    \n# #     model.compile(\n# #         loss='categorical_crossentropy', \n# # #         optimizer=optimizers.Adam(learning_rate=0.0001),\n# #         metrics=['accuracy']\n\n# #     )\n# #     \n#     return model","5b1d4f88":"# def get_exp_pred_vals(model, test_generator):\n#     y_true = []\n#     y_pred = []\n#     count = 0\n#     for X, y in test_generator:\n#         if count % 10 == 0: print('get_exp_pred_vals iteration', count)\n#         count += 1\n#         pred = model.predict(X)\n\n#         y_true += list(np.argmax(y, axis=1))\n#         y_pred += list(np.argmax(pred, axis=1))\n#     print(\"Number of generations: \", count)\n#     return np.array(y_true), np.array(y_pred)\n    ","bf7ed72f":"def get_attr(filename, pred, meta_df):\n    name = os.path.splitext(filename)[0]\n    row = meta_df[meta_df['img_name'] == name]\n    return pred(row)\n\ndef get_attr_persons(filename, pred, meta_df):\n    row = meta_df[meta_df['filename'] == filename]\n#     print(type(row))\n    #print(\"--------------get_attr_persons: \",row)\n    return pred(row)\n\n\ndef get_stats(idx, y_true, y_pred):\n    percent = lambda x, total: '{:.2%}'.format(x\/total)\n    if len(idx) == 0: return [0] + ['N\/A'] * 3\n    \n    CLASS_IDX = 0\n#     print(y_pred[idx])\n    cm = confusion_matrix(y_true[idx], y_pred[idx])\n#     print(cm)\n    cm = cm[CLASS_IDX]\n    \n    total = cm.sum()\n    correct = cm[CLASS_IDX]\n    false_person = cm[1] if len(cm) > 1 else 0\n    false_random = cm[2] if len(cm) > 2 else 0\n    return [total, percent(correct, total), percent(false_person, total), percent(false_random, total)]\n\ndef get_stats_person(idx, y_true, y_pred):\n    percent = lambda x, total: '{:.2%}'.format(x\/total)\n    if len(idx) == 0: return [0] + ['N\/A'] * 3\n\n    cm = confusion_matrix(y_true[idx], y_pred[idx])\n#     print(cm)\n    if cm.shape[0] == 3:\n        cm = cm[1]\n        false_selfie, correct, false_random = cm\n    elif cm.shape[0] == 2:\n        cm = cm[0]\n        correct, false_random = cm\n        false_selfie = 0\n    else:\n        cm = cm[0]\n        correct = cm[0]\n        false_selfie, false_random = 0,0\n        \n    total = cm.sum()\n    return [total, percent(correct, total), percent(false_selfie, total), percent(false_random, total)]","1c4c33a8":"def person_bias_analysis(x_filenames, y_true, y_pred, attrs, meta_df):\n#     x_filenames = [os.path.basename(f) for f in x_filenames]\n    #x_filenames = [f for f in x_filenames]\n    bias_df = pd.DataFrame(columns=['attribute', 'total', 'correct', 'false_selfie', 'false_random'])\n    #print(len(x_filenames))\n\n    print('Evaluating model biases...')\n    for i, (name, pred) in enumerate(attrs):\n        print(name, pred, i)\n        idx = [k for k,f in enumerate(x_filenames) if get_attr_persons(f, pred, meta_df)]\n#         print(idx)\n        \n        print(\"idx\", len(idx))\n        bias_df.loc[i] = [name] + get_stats_person(idx, y_true, y_pred)\n    print(bias_df)\n\n    return bias_df","68dda02a":"'TEST SUBSET SIZE = {}'.format(X.shape[0])","c5dd6ed1":"attrs = [\n    ('African Americans', lambda row: row['ethnicityLabel'].values[0] == 'African Americans'),\n    ('Armenians', lambda row: row['ethnicityLabel'].values[0] == 'Armenians' or row['ethnicityLabel'].values[0] == 'Armenian American'),\n    ('Greeks', lambda row: row['ethnicityLabel'].values[0] == 'Greeks'),\n    ('Czechs', lambda row: row['ethnicityLabel'].values[0] == 'Czechs'),\n    ('Jews', lambda row: row['ethnicityLabel'].values[0] == 'Jewish people' or row['ethnicityLabel'].values[0] == 'American Jews'),\n    ('Serbs', lambda row: row['ethnicityLabel'].values[0] == 'Serbs'),\n    ('Bulgarians', lambda row: row['ethnicityLabel'].values[0] == 'Bulgarians'),\n    ('Ukrainians', lambda row: row['ethnicityLabel'].values[0] == 'Ukrainians'),\n    ('Swedish-speaking population of Finland', lambda row: row['ethnicityLabel'].values[0] == 'Swedish-speaking population of Finland'),\n    ('Albanians', lambda row: row['ethnicityLabel'].values[0] == 'Albanians'),\n    ('Germans', lambda row: row['ethnicityLabel'].values[0] == 'Germans' or row['ethnicityLabel'].values[0] == 'German Americans'),\n    ('French people', lambda row: row['ethnicityLabel'].values[0] == 'French people'),\n    ('English people', lambda row: row['ethnicityLabel'].values[0] == 'English people' or row['ethnicityLabel'].values[0] == 'English American'),\n    ('Poles', lambda row: row['ethnicityLabel'].values[0] == 'Poles'),\n    ('Yoruba people', lambda row: row['ethnicityLabel'].values[0] == 'Yoruba people'),\n    ('Russians', lambda row: row['ethnicityLabel'].values[0] == 'Russians'),\n    ('Italians', lambda row: row['ethnicityLabel'].values[0] == 'Italians' or  row['ethnicityLabel'].values[0] == 'Italian American'),\n    #('American Jews', lambda row: row['ethnicityLabel'].values[0] == 'American Jews'),\n    ('Japanese people', lambda row: row['ethnicityLabel'].values[0] == 'Japanese people'),\n    ('Americans', lambda row: row['ethnicityLabel'].values[0] == 'Americans of the United States' or  row['ethnicityLabel'].values[0] == 'White American'),\n    ('Han Chinese people', lambda row: row['ethnicityLabel'].values[0] == 'Han Chinese people'),\n    ('Tibetan people', lambda row: row['ethnicityLabel'].values[0] == 'Tibetan people'),\n    ('Sinhala people', lambda row: row['ethnicityLabel'].values[0] == 'Sinhala people'),\n    ('British people', lambda row: row['ethnicityLabel'].values[0] == 'British people'),\n    ('Arabs', lambda row: row['ethnicityLabel'].values[0] == 'Arabs'),\n    ('Irish people', lambda row: row['ethnicityLabel'].values[0] == 'Irish people'),\n    ('Swedes', lambda row: row['ethnicityLabel'].values[0] == 'Swedish American' or  row['ethnicityLabel'].values[0] == 'Swedes'),\n    #('White American', lambda row: row['ethnicityLabel'].values[0] == 'White American'),\n    #('Swedes', lambda row: row['ethnicityLabel'].values[0] == 'Swedes'),\n    ('Georgians', lambda row: row['ethnicityLabel'].values[0] == 'Georgians'),\n    ('Hungarians', lambda row: row['ethnicityLabel'].values[0] == 'Hungarians'),\n    ('other', lambda row: row['ethnicityLabel'].values[0] not in ['African Americans', 'Armenians', 'Greeks', 'Czechs', 'Jewish people', 'Serbs', 'Bulgarians', 'Ukrainians', 'Swedish-speaking population of Finland', 'Albanians', 'Germans', 'French people', 'English people', 'Poles', 'Yoruba people', 'Russians', 'American Jews', 'Italians', 'Armenian American', 'Japanese people', 'Americans of the United States', 'Han Chinese people', 'Tibetan people', 'Sinhala people', 'British people', 'Arabs', 'Irish people', 'Swedish American', 'White American', 'Swedes', 'Georgians', 'Hungarians']),\n\n#     ('Ashkenazi Jews', lambda row: row['ethnicityLabel'].values[0] == 'Ashkenazi Jews'),\n#     ('Scottish people', lambda row: row['ethnicityLabel'].values[0] == 'Scottish people'),\n#     #('Italian American', lambda row: row['ethnicityLabel'].values[0] == 'Italian American'),\n#     ('Irish Americans', lambda row: row['ethnicityLabel'].values[0] == 'Irish Americans'),\n#     ('Croats', lambda row: row['ethnicityLabel'].values[0] == 'Croats'),\n#     ('Norwegians', lambda row: row['ethnicityLabel'].values[0] == 'Norwegians'),\n#     ('Spaniards', lambda row: row['ethnicityLabel'].values[0] == 'Spaniards'),\n#     ('Manchu', lambda row: row['ethnicityLabel'].values[0] == 'Manchu'),\n#     ('Belarusians', lambda row: row['ethnicityLabel'].values[0] == 'Belarusians'),\n#     ('Koreans', lambda row: row['ethnicityLabel'].values[0] == 'Koreans'),    \n#     ('Romanians', lambda row: row['ethnicityLabel'].values[0] == 'Romanians'),\n#     ('Bengali people', lambda row: row['ethnicityLabel'].values[0] == 'Bengali people'),\n#     ('Vietnamese people', lambda row: row['ethnicityLabel'].values[0] == 'Vietnamese people'),\n#     ('Dutch people', lambda row: row['ethnicityLabel'].values[0] == 'Dutch people'),\n#     #('German Americans', lambda row: row['ethnicityLabel'].values[0] == 'German Americans'),\n#     ('Austrians', lambda row: row['ethnicityLabel'].values[0] == 'Austrians'),\n#     #('English American', lambda row: row['ethnicityLabel'].values[0] == 'English American'),\n#     ('Kurds', lambda row: row['ethnicityLabel'].values[0] == 'Kurds'),\n#     ('Indian people', lambda row: row['ethnicityLabel'].values[0] == 'Indian people'),\n#     ('Macedonians', lambda row: row['ethnicityLabel'].values[0] == 'Macedonians'),\n#     ('Ottoman Greeks', lambda row: row['ethnicityLabel'].values[0] == 'Ottoman Greeks'),\n#     ('Mongols', lambda row: row['ethnicityLabel'].values[0] == 'Mongols'),\n#     ('Slovaks', lambda row: row['ethnicityLabel'].values[0] == 'Slovaks')\n]\nlen(attrs)","937b40b6":"attrs_gen = [\n    ('male', lambda row: row['genderLabel'].values[0] == 'male'),\n    ('female', lambda row: row['genderLabel'].values[0] == 'female'),\n    ('other', lambda row: row['genderLabel'].values[0] != 'male' and row['genderLabel'].values[0] != 'female'),\n]\nlen(attrs_gen)","ab361695":"%%time\nx, y_true, y_pred = results(M1, pre_trained_model)","23a6fa3c":"%%time\ndf1 = person_bias_analysis(x, y_true, y_pred, attrs, gend_ethn_df)\ndf1","daa0fdae":"%%time\ndf1 = person_bias_analysis(x, y_true, y_pred, attrs_gen, gend_ethn_df)\ndf1","c9f27b57":"cm = confusion_matrix(y_true, y_pred)\ncm[1]","920fa0f0":"'accuracy = {}'.format(round(cm[1][1] \/ sum(cm[1]), 3))","9df03342":"%%time\nx, y_true, y_pred = results(M_EN, efnet_pre_trained_model)\ndf1 = person_bias_analysis(x, y_true, y_pred, attrs, gend_ethn_df)\ndf1","691c02fa":"df1 = person_bias_analysis(x, y_true, y_pred, attrs_gen, gend_ethn_df)\ndf1","dd4f0450":"cm = confusion_matrix(y_true, y_pred)\ncm[1]","242a8876":"'accuracy = {}'.format(round(cm[1][1] \/ sum(cm[1]), 3))","2a493314":"%%time\nx, y_true, y_pred = results(Mf2, not_trained_model)","042b57c9":"%%time\ndf1 = person_bias_analysis(x, y_true, y_pred, attrs, gend_ethn_df)\ndf1","86f84db1":"df1 = person_bias_analysis(x, y_true, y_pred, attrs_gen, gend_ethn_df)\ndf1","85546f22":"cm = confusion_matrix(y_true, y_pred)\ncm[1]","4bbc3cfd":"'accuracy = {}'.format(round(cm[1][1] \/ sum(cm[1]), 3))","fe253034":"def selfie_bias_analysis(x_filenames, y_true, y_pred, attrs):\n#     params = {\n#         'dim': (299, 299),\n#         'batch_size': BATCH_SIZE,\n#         'n_classes': NCLASS,\n#         'n_channels': 3,\n#     }\n\n#     test_generator = DataGenerator(X, labels, **params, shuffle=False)\n    \n#     print('Evaluating model performance...')\n#     loss, acc = model.evaluate_generator(test_generator)\n#     print('Loaded model loss: ', loss)\n#     print('Loaded model accuracy: ', str(acc))\n    \n#     x_filenames = test_generator.get_list_IDs()\n    x_filenames = [os.path.basename(f) for f in x_filenames]\n    \n#     print('Evaluating model predictions...')\n#     y_true, y_pred = get_exp_pred_vals(model, test_generator)\n    \n    columns = \"img_name popularity_score partial_faces is_female baby child teenager youth middle_age senior white black asian oval_face round_face heart_face smiling mouth_open frowning wearing_glasses wearing_sunglasses wearing_lipstick tongue_out duck_face black_hair blond_hair brown_hair red_hair curly_hair straight_hair braid_hair showing_cellphone using_earphone using_mirror braces wearing_hat harsh_lighting dim_lighting\"\n    meta_df =  pd.read_csv('\/kaggle\/input\/selfie-dataset-metadata\/selfie_dataset.txt', names=columns.split(), sep=' ')\n    \n    bias_df = pd.DataFrame(columns=['attribute', 'total', 'correct', 'false_person', 'false_random'])\n\n    print('Evaluating model biases...')\n    selfie_idx = [i for i,y in enumerate(y_true) if y == 0]\n    for i, (name, pred) in enumerate(attrs):\n        idx = [k for k,f in enumerate(x_filenames) if k in selfie_idx and get_attr(f, pred, meta_df)]\n        bias_df.loc[i] = [name] + get_stats(idx, y_true, y_pred)\n        print(i)\n\n    return bias_df","8dcfa75a":"'TEST SUBSET SIZE = {}'.format(X.shape[0])","b7683b53":"attrs = [\n    ('female', lambda row: (row['is_female'] == 1).values[0]),\n    ('male', lambda row: (row['is_female'] == -1).values[0]),\n    ('white', lambda row: (row['white'] == 1).values[0]),\n    ('black', lambda row: (row['black'] == 1).values[0]),\n    ('asian', lambda row: (row['asian'] == 1).values[0]),\n    ('other_races', lambda row: ((row['white'] == -1) & (row['black'] == -1) & (row['asian'] == -1)).values[0]),\n    ('baby', lambda row: (row['baby'] == 1).values[0]),\n    ('child', lambda row: (row['child'] == 1).values[0]),\n    ('teenager', lambda row: (row['teenager'] == 1).values[0]),\n    ('youth', lambda row: (row['youth'] == 1).values[0]),\n    ('middle_age', lambda row: (row['middle_age'] == 1).values[0]),\n    ('senior', lambda row: (row['senior'] == 1).values[0]),\n    ('other_ages', lambda row: ((row['baby'] == -1) & (row['child'] == -1) & (row['teenager'] == -1) & (row['youth'] == -1) & (row['middle_age'] == -1) & (row['senior'] == -1)).values[0]),\n]","fe4d26b7":"# %%time\n# x, y_true, y_pred = results(M1, pre_trained_model)\n# df1 = selfie_bias_analysis(x, y_true, y_pred, attrs)\n# df1","1d652968":"# %%time\n# x, y_true, y_pred = results(M_EN, efnet_pre_trained_model)\n# df2 = selfie_bias_analysis(x, y_true, y_pred, attrs)\n# df2","b86a97d3":"# %%time\n# x, y_true, y_pred = results(Mf2, not_trained_model)\n# df3 = selfie_bias_analysis(x, y_true, y_pred, attrs)\n# df3","3e8d074f":"# # intialise data of lists. \n# data = {'Name':['Tom', 'nick', 'krish', 'jack'], 'Age':[20, 21, 19, 18]} \n  \n# # Create DataFrame \n# df = pd.DataFrame(data) \n# df","201da153":"# import matplotlib.pyplot as plt\n# import pandas as pd\n# from pandas.plotting import table # EDIT: see deprecation warnings below\n\n# ax = plt.subplot(111, frame_on=False) # no visible frame\n# ax.xaxis.set_visible(False)  # hide the x axis\n# ax.yaxis.set_visible(False)  # hide the y axis\n\n# table(ax, df)  # where df is your data frame\n\n# plt.savefig('mytable.png')","6894a402":"# %%time\n\n# model = create_model(M_EN, efnet_pre_trained_model)\n# selfie_bias_analysis(model, attrs)","14911f52":"# %%time\n# model = create_model(M1, pre_trained_model)\n# selfie_bias_analysis(model, attrs)","4886f2c4":"# %%time\n# model = create_model(M2, pre_trained_model)\n# selfie_bias_analysis(model, attrs)","51b74a41":"# %%time\n# model = create_model(M3, pre_trained_model)\n# selfie_bias_analysis(model, attrs)","7c7f1343":"# %%time\n# model = create_model(Mf1, not_trained_model)\n# selfie_bias_analysis(model, attrs)","ac9342b6":"# %%time\n# model = create_model(Mf2,not_trained_model)\n# selfie_bias_analysis(model, attrs)","9851c772":"# model = tf.keras.models.load_model(M1)\n\n# params = {\n#     'dim': (299, 299),\n#     'batch_size': 64,\n#     'n_classes': NCLASS,\n#     'n_channels': 3,\n# }\n\n# test_generator = DataGenerator(\n#     partition['test'], labels, **params, shuffle = False\n# )\n\n# loss, acc = model.evaluate_generator(test_generator)\n# print('Loaded model loss: ', loss)\n# print('Loaded model accuracy: ', str(acc))","38064ba5":"# x_filenames = test_generator.get_list_IDs()\n# x_filenames = [os.path.basename(f) for f in x_filenames]","772d5f57":"# x, y_true, y_pred = get_exp_pred_vals(model, test_generator)","39dd1a4c":"# columns = \"img_name popularity_score partial_faces is_female baby child teenager youth middle_age senior white black asian oval_face round_face heart_face smiling mouth_open frowning wearing_glasses wearing_sunglasses wearing_lipstick tongue_out duck_face black_hair blond_hair brown_hair red_hair curly_hair straight_hair braid_hair showing_cellphone using_earphone using_mirror braces wearing_hat harsh_lighting dim_lighting\"\n# meta_df =  pd.read_csv('\/kaggle\/input\/selfie-dataset-metadata\/selfie_dataset.txt', names=columns.split(), sep=' ')\n# meta_df","8f73f813":"# def get_attr(filename, pred, meta_df):\n#     name = os.path.splitext(filename)[0]\n#     row = meta_df[meta_df['img_name'] == name]\n#     return pred(row)\n\n# def get_stats(idx, y_true, y_pred):\n#     percent = lambda x, total: '{:.2%}'.format(x\/total)\n#     if len(idx) == 0: return [0] + ['N\/A'] * 3\n    \n#     CLASS_IDX = 0\n#     cm = confusion_matrix(y_true[idx], y_pred[idx])[CLASS_IDX]\n    \n#     total = cm.sum()\n#     correct = cm[CLASS_IDX]\n#     false_person = cm[1] if len(cm) > 1 else 0\n#     false_random = cm[2] if len(cm) > 2 else 0\n#     return [total, percent(correct, total), percent(false_person, total), percent(false_random, total)]","3206cd3a":"# attrs = [\n#     ('female', lambda row: (row['is_female'] == 1).values[0]),\n#     ('male', lambda row: (row['is_female'] == -1).values[0]),\n#     ('white', lambda row: (row['white'] == 1).values[0]),\n#     ('black', lambda row: (row['black'] == 1).values[0]),\n#     ('asian', lambda row: (row['asian'] == 1).values[0]),\n#     ('other_races', lambda row: ((row['white'] == -1) & (row['black'] == -1) & (row['asian'] == -1)).values[0]),\n#     ('baby', lambda row: (row['baby'] == 1).values[0]),\n#     ('child', lambda row: (row['child'] == 1).values[0]),\n#     ('teenager', lambda row: (row['teenager'] == 1).values[0]),\n#     ('youth', lambda row: (row['teenager'] == 1).values[0]),\n#     ('middle_age', lambda row: (row['teenager'] == 1).values[0]),\n#     ('senior', lambda row: (row['teenager'] == 1).values[0]),\n#     ('other_ages', lambda row: ((row['baby'] == -1) & (row['child'] == -1) & (row['teenager'] == -1) & (row['youth'] == -1) & (row['middle_age'] == -1) & (row['senior'] == -1)).values[0]),\n# ]","0073516d":"# bias_df = pd.DataFrame(columns=['attribute', 'total', 'correct', 'false_person', 'false_random'])\n\n# selfie_idx = [i for i,y in enumerate(y_true) if y == 0]\n# for i, (name, pred) in enumerate(attrs):\n#     idx = [k for k,f in enumerate(x_filenames) if k in selfie_idx and get_attr(f, pred, meta_df)]\n#     bias_df.loc[i] = [name] + get_stats(idx, y_true, y_pred)\n\n# bias_df","f94ba37f":"## DataGenerator","f069e5b8":"## Debug Code","c71678e4":"## Unrelated","e072166f":"## Selfie Bias Analysis","4c1d7c45":"## CSV to DataFrame"}}