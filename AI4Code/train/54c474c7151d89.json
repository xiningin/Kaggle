{"cell_type":{"482d114d":"code","1a991a4f":"code","07c63d52":"code","da8dfd7c":"code","fddbdf9e":"code","0dedcc89":"code","82d8d866":"code","a7e0af54":"code","e031f800":"code","900767cc":"code","012aa783":"code","d834ce7b":"code","50043bf8":"code","8cfb4145":"code","3e16685a":"code","f65ef88a":"code","90df595e":"code","53791d9f":"code","f0569743":"code","f1d194c0":"code","d256c594":"markdown","71668f52":"markdown","4697a530":"markdown","956afeca":"markdown","2c33feaf":"markdown","034a8284":"markdown","f713d3a9":"markdown","7a002d9e":"markdown","faa28b4d":"markdown","10048723":"markdown","88124b8a":"markdown","ea2f293a":"markdown","a2a08bb3":"markdown","bd7476f9":"markdown","c9fb3f6a":"markdown","4bb3265d":"markdown","464eb754":"markdown","6b6e846f":"markdown"},"source":{"482d114d":"import os\nimport glob\n\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\n\nimport random\nfrom tqdm.notebook import tqdm\nimport pydicom # Handle MRI images\n\nimport cv2  # OpenCV - https:\/\/docs.opencv.org\/master\/d6\/d00\/tutorial_py_root.html\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import layers\n","1a991a4f":"data_dir = Path('..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/')\n\nmri_types = [\"FLAIR\", \"T1w\", \"T2w\", \"T1wCE\"]\nexcluded_images = [109, 123, 709] # Bad images","07c63d52":"train_df = pd.read_csv(data_dir \/ \"train_labels.csv\",\n#                        index='id',\n#                       nrows=100000\n                      )\ntest_df = pd.read_csv(data_dir \/ \"sample_submission.csv\")\nsample_submission = pd.read_csv(data_dir \/ \"sample_submission.csv\")\n\ntrain_df = train_df[~train_df.BraTS21ID.isin(excluded_images)]\n\nprint(f\"train data: Rows={train_df.shape[0]}, Columns={train_df.shape[1]}\")\n# print(f\"test data : Rows={test_df.shape[0]}, Columns={test_df.shape[1]}\")","da8dfd7c":"def load_dicom(path, size = 512):\n    ''' \n    Reads a DICOM image, standardizes so that the pixel values are between 0 and 1, then rescales to 0 and 255\n    \n    Not super sure if this kind of scaling is appropriate, but everyone seems to do it. \n    '''\n    dicom = pydicom.read_file(path)\n    data = dicom.pixel_array\n    # transform data into black and white scale \/ grayscale\n#     data = data - np.min(data)\n    if np.max(data) != 0:\n        data = data \/ np.max(data)\n    data = (data * 255).astype(np.uint8)\n    return cv2.resize(data, (size, size))","fddbdf9e":"def get_all_image_paths(brats21id, image_type, folder='train'): \n    '''\n    Returns an arry of all the images of a particular type for a particular patient ID\n    '''\n    assert(image_type in mri_types)\n    \n    patient_path = os.path.join(\n        \"..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/%s\/\" % folder, \n        str(brats21id).zfill(5),\n    )\n\n    paths = sorted(\n        glob.glob(os.path.join(patient_path, image_type, \"*\")), \n        key=lambda x: int(x[:-4].split(\"-\")[-1]),\n    )\n    \n    num_images = len(paths)\n    \n    start = int(num_images * 0.25)\n    end = int(num_images * 0.75)\n\n    interval = 3\n    \n    if num_images < 10: \n        interval = 1\n    \n    return np.array(paths[start:end:interval])\n\ndef get_all_images(brats21id, image_type, folder='train', size=225):\n    return [load_dicom(path, size) for path in get_all_image_paths(brats21id, image_type, folder)]","0dedcc89":"def get_all_data_for_train(image_type, image_size=400):\n    global train_df\n    \n    X = []\n    y = []\n    train_ids = []\n\n    for i in tqdm(train_df.index):\n        x = train_df.loc[i]\n        images = get_all_images(int(x['BraTS21ID']), image_type, 'train', image_size)\n        label = x['MGMT_value']\n\n        X += images\n        y += [label] * len(images)\n        train_ids += [int(x['BraTS21ID'])] * len(images)\n        assert(len(X) == len(y))\n    return np.array(X), np.array(y), np.array(train_ids)","82d8d866":"def get_all_data_for_test(image_type, image_size=434):\n    global test_df\n    \n    X = []\n    test_ids = []\n\n    for i in tqdm(test_df.index):\n        x = test_df.loc[i]\n        images = get_all_images(int(x['BraTS21ID']), image_type, 'test', image_size)\n        X += images\n        test_ids += [int(x['BraTS21ID'])] * len(images)\n\n    return np.array(X), np.array(test_ids)","a7e0af54":"X, y, trainidt = get_all_data_for_train('T1wCE', image_size=32)\nX_test, testidt = get_all_data_for_test('T1wCE', image_size=32)","e031f800":"X.shape, y.shape, trainidt.shape","900767cc":"X_train, X_valid, y_train, y_valid, trainidt_train, trainidt_valid = train_test_split(X, y, trainidt, random_state=12)\n\n## Remove dimension","012aa783":"X_train = tf.expand_dims(X_train, axis=-1)\nX_valid = tf.expand_dims(X_valid, axis=-1)","d834ce7b":"y_train = to_categorical(y_train)\ny_valid = to_categorical(y_valid)","50043bf8":"# Define, train, and evaluate model\n# source: https:\/\/keras.io\/examples\/vision\/3D_image_classification\/\ndef get_model01(width=128, height=128, depth=64, name='3dcnn'):\n    \"\"\"Build a 3D convolutional neural network model.\"\"\"\n\n    inputs = tf.keras.Input((width, height, depth, 1))\n\n    x = tf.keras.layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(inputs)\n    x = tf.keras.layers.MaxPool3D(pool_size=2)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n\n    x = tf.keras.layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(x)\n    x = tf.keras.layers.MaxPool3D(pool_size=2)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n\n    x = tf.keras.layers.Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n    x = tf.keras.layers.MaxPool3D(pool_size=2)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n\n    x = tf.keras.layers.Conv3D(filters=256, kernel_size=3, activation=\"relu\")(x)\n    x = tf.keras.layers.MaxPool3D(pool_size=2)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n\n    x = tf.keras.layers.GlobalAveragePooling3D()(x)\n    x = tf.keras.layers.Dense(units=512, activation=\"relu\")(x)\n    x = tf.keras.layers.Dropout(0.3)(x)\n\n    outputs = tf.keras.layers.Dense(units=1, activation=\"sigmoid\")(x)\n\n    # Define the model.\n    model = tf.keras.Model(inputs, outputs, name=name)\n    \n    # Compile model.\n    initial_learning_rate = 0.0003\n    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n    )\n    model.compile(\n        loss=\"binary_crossentropy\",\n        optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n        metrics=[\"acc\"],\n    )\n    \n    return model\n\n","8cfb4145":"def get_model02():\n    np.random.seed(0)\n    random.seed(12)\n    tf.random.set_seed(12)\n\n    inpt = keras.Input(shape=X_train.shape[1:])\n\n    h = keras.layers.experimental.preprocessing.Rescaling(1.0 \/ 255)(inpt)\n\n    h = keras.layers.Conv2D(64, kernel_size=(4, 4), activation=\"relu\", name=\"Conv_1\")(h)\n    h = keras.layers.MaxPool2D(pool_size=(2, 2))(h)\n\n    h = keras.layers.Conv2D(32, kernel_size=(2, 2), activation=\"relu\", name=\"Conv_2\")(h)\n    h = keras.layers.MaxPool2D(pool_size=(1, 1))(h)\n\n    h = keras.layers.Dropout(0.1)(h)\n\n    h = keras.layers.Flatten()(h)\n    h = keras.layers.Dense(32, activation=\"relu\")(h)\n\n    output = keras.layers.Dense(2, activation=\"softmax\")(h)\n\n    model = keras.Model(inpt, output)\n\n    model.compile(\n        loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[tf.keras.metrics.AUC()]\n    )\n    return model","3e16685a":"checkpoint_filepath = \"best_model.h5\"\n\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=False,\n    monitor=\"val_auc\",\n    mode=\"max\",\n    save_best_only=True,\n    save_freq=\"epoch\",\n    verbose=1,\n)","f65ef88a":"model = get_model02()\n\nhistory = model.fit(x=X_train, y = y_train, epochs=200, callbacks=[model_checkpoint_callback], validation_data= (X_valid, y_valid))","90df595e":"model_best = tf.keras.models.load_model(filepath=checkpoint_filepath)","53791d9f":"y_pred = model_best.predict(X_valid)\n\npred = np.argmax(y_pred, axis=1)\n\nresult = pd.DataFrame(trainidt_valid)\nresult[1] = pred\n\nresult.columns = [\"BraTS21ID\", \"MGMT_value\"]\nresult2 = result.groupby(\"BraTS21ID\", as_index=False).mean()\n\nresult2 = result2.merge(train_df, on=\"BraTS21ID\")\nauc = roc_auc_score(\n    result2.MGMT_value_y,\n    result2.MGMT_value_x,\n)\nprint(f\"Validation AUC={auc}\")","f0569743":"y_pred = model_best.predict(X_test)\n\npred = np.argmax(y_pred, axis=1) #\n\nresult = pd.DataFrame(testidt)\nresult[1] = pred\npred","f1d194c0":"result.columns=['BraTS21ID','MGMT_value']\n\nresult2 = result.groupby('BraTS21ID',as_index=False).mean()\nresult2['BraTS21ID'] = sample_submission['BraTS21ID']\n\n# Rounding...\nresult2['MGMT_value'] = result2['MGMT_value'].apply(lambda x:round(x*10)\/10)\nresult2.to_csv('submission.csv',index=False)\nresult2","d256c594":"# Configuration, Constants, Setup","71668f52":"# Tensorflow Models","4697a530":"# Submission File","956afeca":"I'm reading through several existing notebooks and trying to distill down the information into a new notebook to help me understand the project.  All help appreciated!\n\n# References\n\n- [Advanced EDA - Brain Tumor Data](https:\/\/www.kaggle.com\/smoschou55\/advanced-eda-brain-tumor-data)\n- [Team 9 Second Week](https:\/\/www.kaggle.com\/evanyao27\/team-9-second-week)\n  - The only model that is working. get_model02()\n- [Dataset to Model with Tensorflow](https:\/\/www.kaggle.com\/ohbewise\/dataset-to-model-with-tensorflow)\n- [Brain Tumer Train Class Flair](https:\/\/www.kaggle.com\/lucamtb\/brain-tumer-train-class-flair)\n  - Uses TPU\n  - Generates a Tensorflow model: Brain_flair_model_effect_3e-05_0.0001.h5\n- [Brain Tumor very basic inference](https:\/\/www.kaggle.com\/lucamtb\/brain-tumor-very-basice-inference)\n  - Uses the above mentioned model: Brain_flair_model_effect_3e-05_0.0001.h5\n  - Add this Kaggle Dataset: https:\/\/www.kaggle.com\/lucamtb\/effect0-brain","2c33feaf":"# Predictions on Validation Set","034a8284":"## Model from: https:\/\/www.kaggle.com\/evanyao27\/team-9-second-week\/notebook\n\n- Validation AUC=0.9148664856146349","f713d3a9":"# Predictions on the Test Set","7a002d9e":"# Load Datasets","faa28b4d":"# Utility Functions","10048723":"# Load Libraries","88124b8a":"## One-hot encode labels","ea2f293a":"## Set up Model Checkpoint","a2a08bb3":"# Load Our Best Model","bd7476f9":"# Load Images We Will Need","c9fb3f6a":"# Train\/Validation Split","4bb3265d":"## Model from:  https:\/\/www.kaggle.com\/ohbewise\/dataset-to-model-with-tensorflow","464eb754":"### There's a version that converts into grayscale: \n\n- https:\/\/www.kaggle.com\/smoschou55\/advanced-eda-brain-tumor-data\n","6b6e846f":"### Note that rerunning the cell below will change val_acc to val_acc_N and the model will not be saved."}}