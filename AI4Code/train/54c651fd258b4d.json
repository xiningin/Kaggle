{"cell_type":{"9c2da4f2":"code","6e6aa2bc":"code","dbc7df07":"code","1e9d70cb":"code","a8c298df":"code","12f5d530":"code","8aeecdde":"code","f2d4ac17":"code","84e3ff79":"code","1c91facc":"code","923dd82a":"code","0a3df63a":"code","96bc7918":"code","c55aed75":"code","0dff44a5":"code","efbdce55":"code","5a5dd65f":"code","d25c5443":"code","a6f700a5":"code","e2307d53":"code","64454e02":"code","6b1087c2":"code","f759bb59":"code","f8322b72":"code","89e3256e":"code","ee4304ec":"code","55964448":"code","d5aca0e5":"code","6676d86c":"code","e9447cf2":"code","09b67355":"code","43319b3c":"code","e9537c30":"code","231e87cf":"code","1c408c47":"code","3e8901d9":"code","cd7d44c5":"code","19429629":"code","52cbf7a4":"code","a6266bfe":"code","003ac5bc":"code","b3a2dba5":"code","31bf3fb3":"markdown","10f9811d":"markdown","c2f66ba7":"markdown","876c9f3b":"markdown","576ffc5e":"markdown","ab9b82d8":"markdown","953bd0d2":"markdown","b3c9f05b":"markdown","db2c9fc7":"markdown","a879efec":"markdown","ad3cb44d":"markdown","4263b626":"markdown","8188c413":"markdown","800592fd":"markdown","3ed90588":"markdown","f9a06405":"markdown","c2e30b46":"markdown","e823d8b1":"markdown","06a64794":"markdown","3b824d9e":"markdown","d02b785c":"markdown","25dd5c71":"markdown","41e9b0a1":"markdown","df8b9312":"markdown","b8da31a9":"markdown","ee5ef487":"markdown"},"source":{"9c2da4f2":"# load basic neccesities\nimport os\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set()\n\npd.options.display.max_columns = 200","6e6aa2bc":"from sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split","dbc7df07":"train = pd.read_csv(\"..\/input\/learn-together\/train.csv\")\ntest = pd.read_csv(\"..\/input\/learn-together\/test.csv\")","1e9d70cb":"print(train.shape)\nprint(test.shape)","a8c298df":"train.head(5)","12f5d530":"train.describe()","8aeecdde":"test.describe()","f2d4ac17":"classes = set(train.Cover_Type)\nn_classes = len(classes)\nprint(classes, ' - ', n_classes)","84e3ff79":"sns.countplot(train.Cover_Type)","1c91facc":"# check if there are NaN cells\ntrain.count()","923dd82a":"test.head(5)","0a3df63a":"test.count()","96bc7918":"df_train = train.drop('Id', axis=1).copy()\ntrain_cols = df_train.columns.tolist()\ntrain_cols = train_cols[-1:] + train_cols[:-1]\ndf_train = df_train[train_cols]\ncorr = df_train.corr()","c55aed75":"# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(15, 15))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","0dff44a5":"df_train = train.drop('Id', axis=1).copy()\ntrain_cols = df_train.columns.tolist()\ntrain_cols = train_cols[-1:] + train_cols[:-1]\ndf_train[train_cols[15:]].sum()\/df_train.shape[0]*100","efbdce55":"test_cols = test.columns.tolist()\ntest[test_cols[14:]].sum()\/test.shape[0]*100","5a5dd65f":"# lets start with Light GBM\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier","d25c5443":"# almost basic parameters. some adjustments to prevent overfitting\nLGB_PARAMS_1 = {\n    'objective': 'multiclass',\n    \"num_class\" : n_classes,\n    'metric': 'multiclass',\n    'verbosity': -1,\n    'boosting_type': 'gbdt',\n    'learning_rate': 0.1,\n    'num_leaves': 64,\n#    'num_leaves': 128,\n    'max_depth': -1,\n#    'subsample_freq': 10,\n    'subsample_freq': 1,\n    'subsample': 0.5,\n    'bagging_seed': 1970,\n    'reg_alpha': 0.3,\n#    'reg_alpha': 0.1,\n    'reg_lambda': 0.3,\n    'colsample_bytree': 0.90\n}","a6f700a5":"# basic parameters\nLGB_PARAMS = {\n    'objective': 'multiclass',\n    \"num_class\" : n_classes,\n    'metric': 'multiclass',\n    'reg_alpha': 0.9,\n    'reg_lambda': 0.9,\n    'verbosity': -1\n}","e2307d53":"X_data = train.drop(['Id','Cover_Type','Soil_Type7','Soil_Type15'], axis = 1).copy()\ny_data = train.Cover_Type\ncols = X_data.columns\nprint(X_data.shape)\nprint(y_data.shape)","64454e02":"# split train set into train and validation\nX_train, X_val, y_train, y_val = train_test_split(X_data, y_data, test_size=0.2, random_state=128)","6b1087c2":"# number of samples in Validation dataset\nN_valid = y_val.shape[0]","f759bb59":"LB_model = LGBMClassifier(**LGB_PARAMS, n_estimators=1500, n_jobs = -1)","f8322b72":"LB_model.fit(X_train, y_train, \n        eval_set=[(X_train, y_train), (X_val, y_val)], eval_metric='multiclass',\n        verbose=10, early_stopping_rounds=50)","89e3256e":"LG_pred = LB_model.predict(X_val)\nprint(accuracy_score(LG_pred, y_val))","ee4304ec":"confusion_matrix(y_val, LG_pred)","55964448":"df_importance = pd.DataFrame({'feature': cols, 'importance': LB_model.feature_importances_})","d5aca0e5":"# Lets plot it\nplt.figure(figsize=(6, 10))\nsns.barplot(x=\"importance\", y=\"feature\", data=df_importance.sort_values('importance', ascending=False));\n","6676d86c":"new_features = df_importance.loc[df_importance.importance>100].sort_values('importance', ascending=False)\nnew_columns = new_features.feature","e9447cf2":"KNN_clf = KNeighborsClassifier(n_neighbors = 5, weights ='distance', metric = 'minkowski', p = 2)\nKNN_clf.fit(X_train, y_train)\nyk_pred = KNN_clf.predict(X_val)\ncm = confusion_matrix(y_val, yk_pred)\nprint(accuracy_score(yk_pred, y_val))","09b67355":"cm","43319b3c":"from sklearn.ensemble import RandomForestClassifier\n\n# just default settings\n#RF_clf = RandomForestClassifier(n_estimators=1000, max_depth=10,\nRF_clf = RandomForestClassifier(n_estimators=1000, \n                              random_state=1970)\nRF_clf.fit(X_train, y_train) ","e9537c30":"RF_pred = RF_clf.predict(X_val)\naccuracy_score(y_val, RF_pred)","231e87cf":"confusion_matrix(y_val, RF_pred)","1c408c47":"df_importance = pd.DataFrame({'feature': cols, 'importance': RF_clf.feature_importances_})","3e8901d9":"# Lets plot it\nplt.figure(figsize=(6, 10))\nsns.barplot(x=\"importance\", y=\"feature\", data=df_importance.sort_values('importance', ascending=False));","cd7d44c5":"submission_id = test.Id\ntest.drop(['Id','Soil_Type7','Soil_Type15'], axis = 1, inplace = True)\nprediction = LB_model.predict(test)\nsubmission = pd.DataFrame({'Id': submission_id, 'Cover_Type': prediction})\nsubmission.to_csv('submission_LGBM.csv', index = False)","19429629":"#lets check Ensemble prediction on validation set\nRF_val = RF_clf.predict_proba(X_val)\nKNN_val = KNN_clf.predict_proba(X_val)\nLG_val = LB_model.predict_proba(X_val)\n\nval_probs = (RF_val + LG_val + KNN_val)\/3\nval_preds = np.argmax(val_probs, axis=1)+1\n\nprint(accuracy_score(y_val, val_preds))\n# lets check the distribution of predicted classes\n# as we know all classes shall be evenly split. at least they shall be close to it\nsns.countplot(val_preds)","52cbf7a4":"# final Test set prediction\nRF_probs = RF_clf.predict_proba(test)\nKNN_probs = KNN_clf.predict_proba(test)\nLG_probs = LB_model.predict_proba(test)","a6266bfe":"# averaging the prediction\nfinal_probs = (RF_probs + LG_probs + KNN_probs)\/3\nfinal_preds = np.argmax(final_probs, axis=1)+1","003ac5bc":"# lets see the distribution of predicted classes\n# hint - as we know class 1 shall be around 37%, and as I've checked it class 2 shall be around 50% actually\nsns.countplot(final_preds)","b3a2dba5":"submission = pd.DataFrame({'Id': submission_id, 'Cover_Type': final_preds})\nsubmission.to_csv('submission_ensemble.csv', index = False)\n","31bf3fb3":"Lets check if the dataset is balanced","10f9811d":"We can relax a bit - categorical features already have been encoded for us: Soil type and Wilderness Area. All other features are integers and nothing is missing. We would only need to scale them if using Neural Networks sensitive to the scale of numbers. Don't need to do that for most of tree based algorithms.","c2f66ba7":"## Light GBM","876c9f3b":"## Features\n\nLets see quick feature importance analysis , LGBM version.","576ffc5e":"Comparing with the test data - everything looks pretty close, similar. Looks like we shall not have any problems with predictions. ","ab9b82d8":"Lets have a look at the training data stats. ","953bd0d2":"# Import basics","b3c9f05b":"# Features analysis","db2c9fc7":"# Submission \nNeed to create a dataframe for submission with 2 columns, make a prediction on test data and write it into the submission csv file. Will use LightGBM for now.","a879efec":"LightGBM seems preforming slightly better, than RandomForest. And the KNN is the worst so far.","ad3cb44d":"# First impression","4263b626":"## Conclusion about the features\nRandom Forest gave slightly different evalution of feature importances, however location features and Wilderness type are still most important.\nAnd it seems that most of Soil types were not really useful for RF as well as for LGBM.","8188c413":"## Soil types distribution\nWhich soil types are more common in train dataset? ","800592fd":"Looks like we don't have significant correlation between any features and the target. There are just a few features with more or less correlation between them, but nothing significant enough that would prompt to drop some of them.","3ed90588":"## KNN\nLets see how K Nearest Neighbors will do","f9a06405":"Looks like a bit smaller prediction for class 2. Shall be around 283000. Class 1 looks more or less good. The rest of the classes shall be smaller.","c2e30b46":"Clearly there are some feautures of outmost importance while some of the features have 0 weight in decision making.\nWe have to reduce number of features, and try to engineer some new ones as well.","e823d8b1":"And here we have a significantly different distribution of soil types. Should probably avoid to rely on soil types as major features for predictions? \nAlso we definitely shall drop Soil types 7 and 15 since there are just a few samples in test set with them (and we won't be able to relate to prediction since there is no data in train for these features).","06a64794":"A few soil types are quite abundant, while about half of them are less than 1% of cases. Need to check correlation with the target. It could be that rare soil types will help with certain class prediction. Or otherwise will just mess the calculations.\nIn any case Soil type 7 and 15 dont have any samples in train dataset. \n\n**Lets check test dataset now.**","3b824d9e":"# Ensemble","d02b785c":"Perfect, we have absolutely evenly split dataset in terms of classes.","25dd5c71":"# Things to try\n\n* shall try to engeneer new spatial features\n* drop unimportant features\n* try different models including Neural Nets","41e9b0a1":"Accuracy seems better than for separate models as expected. \nHowever the distribution is slightly skewed. I'm afraid we will have much lower LeaderBoard score","df8b9312":"# Base model","b8da31a9":"# Loading everything","ee5ef487":"## Random Forest\n\nNow lets see how RandomForest will work."}}