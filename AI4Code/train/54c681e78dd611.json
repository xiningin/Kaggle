{"cell_type":{"a629429f":"code","16c2e8bd":"code","ee5090fd":"code","2985b396":"code","054f8b77":"code","728f0d65":"code","d7f1683d":"code","4cd77809":"code","957ec16a":"code","222ca159":"code","c4a0d6a2":"code","402b3472":"code","833cacf8":"code","098da066":"code","d8998ca5":"code","829c46e7":"code","b8d36b48":"code","a80c7ed3":"code","2e2f35d0":"code","9147c9fe":"code","6ab27cfa":"code","133830bc":"code","e0e78921":"code","afe766ef":"code","c58b87fe":"code","351b115a":"code","6e62034c":"code","def01108":"code","f472c067":"code","afb7d0c5":"code","95fc6f74":"code","4a5f4925":"code","b6c656ec":"code","b072307e":"code","18c9ff56":"code","92a160c5":"code","4e5a0ce2":"code","74d1a841":"code","844cb83e":"code","cdd5f4cc":"code","ac4ffedd":"code","2bbc4900":"code","1b84bbee":"code","e2f6e0a5":"code","879a6c9d":"code","9fc75390":"code","dff91e5f":"code","f7ca7419":"code","121318db":"code","222bc05b":"code","d94c6784":"code","7a5a5827":"code","a2d3b9ef":"code","304ea116":"code","414404c7":"code","a6e5314d":"code","79eeffb4":"code","d514ccf3":"code","3402246f":"code","72c27188":"code","b5538d4d":"code","476df978":"code","cf961f24":"code","dcea12a0":"code","082949e4":"code","1f159152":"code","fd09a04d":"code","bfd690b9":"code","704e38b1":"code","d6e1ed71":"code","6c2194e1":"code","67883331":"code","65648dc9":"code","ca058560":"code","f1771fad":"code","5441db6a":"code","81c10d56":"code","fea45ea7":"code","2713a104":"markdown","ed0e35fd":"markdown","eaed9f60":"markdown","dc78127e":"markdown","7434cb0f":"markdown","0d48b20d":"markdown","a1b95c2f":"markdown","2683e79e":"markdown","8d758609":"markdown","802e6f71":"markdown","4f338bce":"markdown","e150ecd3":"markdown","9881538e":"markdown","714e0827":"markdown","0f1d102a":"markdown","2fe28861":"markdown","99ab14b1":"markdown","8a870184":"markdown","7481e5a4":"markdown","0eacf1b6":"markdown","bdd21aaa":"markdown"},"source":{"a629429f":"import re\nimport csv\nimport codecs\nimport numpy as np\nimport pandas as pd\nimport operator\nimport string\nimport time\nimport matplotlib.pyplot as plt\n\nimport nltk\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom string import punctuation\nfrom collections import defaultdict\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nnltk.download('stopwords')\nnltk.download('punkt')\neng_stopwords = set(stopwords.words(\"english\"))\nimport sys","16c2e8bd":"import io\ndata = pd.read_csv('\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv')","ee5090fd":"print(\"Data shape of metadata.csv: \", data.shape)\n# remove articles that were published before November 2019\nmeta_data = data.loc[data[\"publish_time\"] >= \"2019-11-01\"]\n# only keep title, abstracts, doi and url\nmeta_data = meta_data[[\"cord_uid\", \"title\", \"abstract\", \"doi\", \"url\"]]\nmeta_data = meta_data.reset_index(drop=True)\nprint(\"New data shape: \", meta_data.shape)","2985b396":"# clean abstract\ndef clean_abstract(text):\n\n  text = text.lower()\n  word_len = len(\"abstract\")\n  if text[:word_len] == \"abstract\":\n    text = text[word_len:]\n  if \"risk factor\" in text:\n    text = text.replace(\"risk factor\", \"riskfactor\")\n  elif \"risk factors\" in text:\n    text = text.replace(\"risk factors\", \"riskfactor\")\n  return text","054f8b77":"#check na\nmeta_data.isna().sum()","728f0d65":"# remove rows with NAs in title and abstract\ncomplete_cases = meta_data.dropna(subset=['title', 'abstract'])\nprint(complete_cases.isna().sum())\nprint(\"Data shape: \", complete_cases.shape)","d7f1683d":"# clean abstracts\nabstracts = complete_cases['abstract']\ncleaned_abstract = [clean_abstract(text) for text in abstracts]\ncomplete_cases.abstract = cleaned_abstract","4cd77809":"def tokenize(text):\n    '''\n    Convert the text corpus to lower case, remove all punctuations and numbers which lead to\n    a final cleaned corpus with only tokens where all characters in the string are alphabets.\n    '''\n    # convert the text to lower case and replace all new line characters by an empty string\n    lower_text = text.lower().replace('\\n', ' ')\n    table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n    punct_text = lower_text.translate(table)\n    # use NLTK's word tokenization to tokenize the text \n    # remove numbers and empty tokens, only keep tokens with all characters in the string are alphabets\n    tokens = [word for word in word_tokenize(punct_text) if word.isalpha()]\n    return tokens","957ec16a":"def remove_stopwords(word_list, sw=stopwords.words('english')):\n    \"\"\" \n    Filter out all stop words from the text corpus.\n    \"\"\"\n    # It is important to keep words like no and not. Since the meaning of the text will change oppositely\n    # if they are removed.\n    if 'not' in sw:\n        sw.remove('not')\n    if 'no' in sw:\n        sw.remove('no')\n    \n    cleaned = []\n    for word in word_list:\n        if word not in sw:\n            cleaned.append(word)\n    return cleaned","222ca159":"def stem_words(word_list):\n    stemmer = SnowballStemmer('english')\n    stemmed_words = [stemmer.stem(word) for word in word_list]\n    text = \" \".join(stemmed_words)\n    return stemmed_words, text","c4a0d6a2":"def preprocess(text):\n    \"\"\"\n    Combine all preprocess steps together.\n    Clean each text into tokenized stemmed word list and also return concatenated string\n    \"\"\"\n    tokenized = tokenize(text)\n    stopword_removed = remove_stopwords(tokenized)\n    tokenized_str, cleaned_str = stem_words(stopword_removed)\n    return stopword_removed, tokenized_str, cleaned_str","402b3472":"# clean abstracts\nabstracts = complete_cases[\"abstract\"].values.tolist()\ntokenized_abstracts = []\nstr_abstracts = []\n# create a word dictionary to store all words and their stemmed results\nword_dict_abstract = {}\nfor abstract in abstracts:\n  result = preprocess(abstract)\n  tokenized = result[0]\n  stemmed = result[1]\n  for i in range(0,len(stemmed)):\n    if stemmed[i] not in word_dict_abstract:\n      word_dict_abstract[stemmed[i]] = tokenized[i]\n  tokenized_abstracts.append(stemmed)\n  str_abstracts.append(result[2])","833cacf8":"print(\"Number of Unique Words in Abstracts:\", len(word_dict_abstract))","098da066":"complete_cases[\"tokenized_abstract\"] = tokenized_abstracts\ncomplete_cases[\"cleaned_abstracts\"] = str_abstracts","d8998ca5":"# cleaned dataset\ncomplete_cases.head()","829c46e7":"import gensim\nimport itertools\nimport random\nfrom gensim.models import LdaModel\nfrom gensim import models, corpora, similarities\nfrom scipy.stats import entropy","b8d36b48":"# 20 topics, on abstracts\ndef train_model_lda(data):\n    \n    num_topics = 20\n    chunksize = 300\n    dictionary = corpora.Dictionary(data['tokenized_abstract'])\n    corpus = [dictionary.doc2bow(doc) for doc in data['tokenized_abstract']]\n\n    lda = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary,\n                   alpha=1e-3, eta=0.5e-3, chunksize=chunksize, minimum_probability=0.0, passes=2)\n    return dictionary, corpus, lda","a80c7ed3":"np.random.seed(2020)\ndictionary, corpus, lda = train_model_lda(complete_cases)","2e2f35d0":"for i in range(0,20):\n    print(\"--------Topic: \", i, \"--------\")\n    topic = lda.show_topic(topicid=i, topn=50)\n    # use word dictionary to translate stemmed words back to original words\n    print([word_dict_abstract[word[0]] for word in topic])\n    print()\n    print()","9147c9fe":"# read in task description, the task description was edited in order to cover more keywords\ntask_description = '''\nSmoking, chronic and pre-existing pulmonary disease\nCo-infections and co-morbidities\nNeonates, pregnant women, transmission during pregnancy \nAge and sex\/gender difference, women, men\nSocio-economic, psychological, behavioral and environmental factors\nTransmission dynamics\nSeverity of disease, fatality, mortality and high-risk patient groups\nSusceptibility\nPublic health mitigation measures\n'''","6ab27cfa":"task_complete_w, tokenized_list, cleaned_desc = preprocess(task_description)\nbow = dictionary.doc2bow(tokenized_list)","133830bc":"doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=bow)])\n# bar plot of topic distribution of this document\nfig, ax = plt.subplots(figsize=(12,6));\n# the histogram of the data\npatches = ax.bar(np.arange(len(doc_distribution)), doc_distribution)\nax.set_xlabel('Topic ID', fontsize=15)\nax.set_ylabel('Topic Distribution', fontsize=15)\nax.set_title(\"Topic Distribution of Task Description\", fontsize=20)\nax.set_xticks(np.linspace(0,19,20))\nfig.tight_layout()\nplt.show()","e0e78921":"for i in doc_distribution.argsort()[-5:][::-1]:\n    print(i, [word_dict_abstract[item[0]] for item in lda.show_topic(topicid=i, topn=50)], \"\\n\")","afe766ef":"def assign_article_topics(text):\n    \"\"\"\n    Find the top 6 topics that are most relevant to each abstract in data.\n    \"\"\"\n    bow = dictionary.doc2bow(text)\n    doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=bow)])\n    top6_topics = doc_distribution.argsort()[-6:][::-1]\n    return list(top6_topics)","c58b87fe":"top6_topics = []\nfor abstract in list(complete_cases[\"tokenized_abstract\"]):\n    top6_topics.append(assign_article_topics(abstract))\ncomplete_cases['top6_topics'] = top6_topics","351b115a":"complete_cases.head()","6e62034c":"# topic distribution of all abstracts in data\nfrom collections import Counter\n\ntopics_total = list(itertools.chain.from_iterable(complete_cases['top6_topics']))\ncount = Counter(topics_total)\ndf = pd.DataFrame.from_dict(count, orient='index')\nax = df.sort_index().plot(kind='bar', legend=None, title=\"Topic Distribution Frequency Plot\")\nax.set_xlabel(\"Topic ID\")\nax.set_ylabel(\"Frequency Count\")","def01108":"# Most popular 5 topics in data\nsorted_count = {k: v for k, v in sorted(count.items(), key=lambda item: item[1], reverse = True)}\nfor i in list(sorted_count.keys())[:5]:\n    print(i, [word_dict_abstract[item[0]] for item in lda.show_topic(topicid=i, topn=20)], \"\\n\")","f472c067":"target = list(doc_distribution.argsort()[-5:][::-1])\ntarget = sorted(target)\nprint(target)","afb7d0c5":"match_4 = complete_cases[[len(set(item).intersection(set(target)))>=4 for item in complete_cases['top6_topics'].tolist()]]\nmatch_4_title_abs = match_4.drop([\"top6_topics\"], axis = 1)\n\n# filter out articles that are not related to COVID-19 or general infectious diseases\ncov19_names = [\"ncov\", \"covid-19\", \"coronavirus\", \"sars-cov-2\"]\nrelated = []\nfor i in range(0, match_4_title_abs.shape[0]):\n  r = any([(keyword in match_4_title_abs[\"title\"].tolist()[i].lower()) or (keyword in match_4_title_abs[\"abstract\"].tolist()[i].lower()) for keyword in cov19_names])\n  related.append(r)\n\n# relevant target articles\ntarget_data = match_4_title_abs[related]\n# evaluate results\nsample = target_data.sample(5, random_state=2)\nfor i in range(0,sample.shape[0]):\n  print(\"----------Article\", i, \"----------\")\n  print(\"Title: \", \"\\n\", sample['title'].tolist()[i])\n  print(\"Abstracts: \", \"\\n\", sample['abstract'].tolist()[i]) \n  print()\n  print()","95fc6f74":"keyword = [\"smok\", \"preexisting\", \"pre-existing\", \"chronic\", \"underlying\", # smoking, pre-existing pulmonary disease\n           \"co-infection\", \"coinfection\", \"co-morbidities\", \"comorbidities\", # co-infections\n           \"neonat\", \"pregnancy\", \"pregnant\", \"newborn\", \"uterine\", \"infant\", # neorates and pregnant women\n           \"socioeconomic\", \"socio-economic\", # socio-economic\n           \"susceptibility\", \"environmental\", \"psychological\", \"stress\", \"mental\", \"frustration\", # mental\n           \"mitigation measures\", #government mitigation measures\n           \"riskfactor\", \"riskfactors\"\n]","4a5f4925":"found = []\ncomplete_cases = complete_cases.reset_index(drop=True)\nfor i in range(0, complete_cases.shape[0]):\n  r = any([(word in complete_cases[\"title\"][i].lower()) or (word in complete_cases[\"abstract\"][i].lower()) for word in keyword])\n  found.append(r)","b6c656ec":"# keyword search results\nkeyword_search_output = complete_cases[found].reset_index(drop=True)","b072307e":"# filter out articles that are not related to covid-19 from keyword search results\nrelated_2 = []\nfor i in range(0, keyword_search_output.shape[0]):\n  r = any([(keyword in keyword_search_output[\"title\"][i].lower()) or (keyword in keyword_search_output[\"abstract\"][i].lower()) for keyword in cov19_names])\n  related_2.append(r)\nkeyword_found = keyword_search_output[related_2].reset_index(drop=True)\n\n# evaluate results\nsample = keyword_found.sample(5, random_state=0)\nfor i in range(0,sample.shape[0]):\n  print(\"----------Article\", i, \"----------\")\n  print(\"Title: \", \"\\n\", sample['title'].tolist()[i])\n  print(\"Abstracts: \", \"\\n\", sample['abstract'].tolist()[i]) \n  print()\n  print()","18c9ff56":"keyword_search_ids = set(keyword_found['cord_uid'].tolist())\nlda_search_ids = set(target_data['cord_uid'].tolist())\nall_found = keyword_search_ids.union(lda_search_ids)\nfinal_articles = complete_cases[[uid in list(all_found) for uid in complete_cases['cord_uid']]]","92a160c5":"#!pip install numpy==1.16.1\nimport numpy as np\nimport pandas as pd\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import sent_tokenize\nfrom keras.models import load_model\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import pairwise_distances_argmin_min","4e5a0ce2":"# clone the skip-thoughts github repository\n# !git clone https:\/\/github.com\/ryankiros\/skip-thoughts","74d1a841":"# pre-trained models and word embeddings (wikipedia data)\n#!mkdir ..\/input\/skipthoughtspackage\/skip-thoughts-master\/models\n#!wget -P ..\/input\/skipthoughtspackage\/skip-thoughts-master\/models http:\/\/www.cs.toronto.edu\/~rkiros\/models\/dictionary.txt\n#!wget -P ..\/input\/skipthoughtspackage\/skip-thoughts-master\/models http:\/\/www.cs.toronto.edu\/~rkiros\/models\/utable.npy\n#!wget -P ..\/input\/skipthoughtspackage\/skip-thoughts-master\/models http:\/\/www.cs.toronto.edu\/~rkiros\/models\/btable.npy\n#!wget -P ..\/input\/skipthoughtspackage\/skip-thoughts-master\/models http:\/\/www.cs.toronto.edu\/~rkiros\/models\/uni_skip.npz\n#!wget -P ..\/input\/skipthoughtspackage\/skip-thoughts-master\/models http:\/\/www.cs.toronto.edu\/~rkiros\/models\/uni_skip.npz.pkl\n#!wget -P ..\/input\/skipthoughtspackage\/skip-thoughts-master\/models http:\/\/www.cs.toronto.edu\/~rkiros\/models\/bi_skip.npz\n#!wget -P ..\/input\/skipthoughtspackage\/skip-thoughts-master\/models http:\/\/www.cs.toronto.edu\/~rkiros\/models\/bi_skip.npz.pkl","844cb83e":"# import os\n# os.chdir('..\/..\/')\n# !pwd\n# import skipthoughts","cdd5f4cc":"def skipthought_encode(text):\n    \"\"\"\n    Sentences are encoded into fixed-dimensional vector representation\n    \"\"\"\n    \n    enc_text = [None]*len(text)\n    cum_sum_sentences = [0]\n    sent_count = 0\n    for txt in text:\n        sent_count += len(txt)\n        cum_sum_sentences.append(sent_count)\n        all_sentences = [sent for txt in text for sent in txt]\n    \n    print('Loading pre-trained models...')\n    model = skipthoughts.load_model()\n    encoder = skipthoughts.Encoder(model)\n    print('Encoding sentences...')\n    enc_sentences = encoder.encode(all_sentences, verbose=False)\n\n    for i in range(len(text)):\n        begin = cum_sum_sentences[i]\n        end = cum_sum_sentences[i+1]\n        enc_text[i] = enc_sentences[begin:end]\n    return enc_text","ac4ffedd":"def MakeSummary(texts):\n    \"\"\"\n    Produce final summary of each text.\n    \"\"\"\n    n_topics = len(texts)\n    summary = [None]*n_topics\n    print('Starting to encode...')\n    # sentence encoding...\n    enc_texts= skipthought_encode(texts)\n    print('Encoding Finished')\n    # perform K-Means clustering\n    for i in range(n_topics):\n        enc_txt = enc_texts[i]\n        # number of clusters\n        n_clusters = int(np.ceil(len(enc_txt)**0.5))\n        kmeans = KMeans(n_clusters=n_clusters, random_state=1)\n        kmeans = kmeans.fit(enc_txt)\n        avg = []\n        closest = []\n        for j in range(n_clusters):\n            idx = np.where(kmeans.labels_ == j)[0]\n            avg.append(np.mean(idx))\n        closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_,\\\n                                                   enc_txt)\n        ordering = sorted(range(n_clusters), key=lambda k: avg[k])\n        summary[i] = ' '.join([texts[i][closest[idx]] for idx in ordering])\n    print('Clustering Finished')\n\n    return summary","2bbc4900":"all_abstracts = final_articles['abstract'].tolist()\ntokenized_abstracts = [sent_tokenize(abstract) for abstract in all_abstracts]","1b84bbee":"# like 2-gram words, a contiguous sequence of 2 words\n# here I created a list of 2-gram sentences in order to preserve sentence completeness\n\n# Chronic diseases, especially cardiovescular diseases were found to be the most popular underlying conditions that high-risk patients have.\n# We inferred that it might be a risk factor for 2019-ncov.\n\ntwogram_abstracts = []\nfor abstract in tokenized_abstracts:\n    abs_list = []\n    if len(abstract) == 1:\n        abs_list = abstract\n    else:\n        for i in range(len(abstract)-1):\n            abs_list.append(abstract[i]+abstract[i+1])\n        twogram_abstracts.append(abs_list)\n","e2f6e0a5":"#abstract_summary = MakeSummary(twogram_abstracts)\nimport pickle\nwith open(\"\/kaggle\/input\/skipthoughts-results\/abstract_summary.pkl\", 'rb') as handle:\n    abstract_summary = pickle.load(handle)","879a6c9d":"twogram_abstracts[999]","9fc75390":"sent_tokenize(abstract_summary[999])","dff91e5f":"# build a large sentence corpus\n# Dump all the sentences in all abstract summaries into the large list\nlarge_list = []\nfrom nltk.tokenize import sent_tokenize\nnltk.download('stopwords')\nnltk.download('punkt')\n\nfor abstract in abstract_summary:\n  large_list += sent_tokenize(abstract)\nlen(large_list)","f7ca7419":"import re\nimport csv\nimport codecs\nimport numpy as np\nimport pandas as pd\nimport operator\nimport string\nimport time\nimport matplotlib.pyplot as plt\n\nimport nltk\nnltk.download('stopwords')\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom string import punctuation\nfrom collections import defaultdict\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\neng_stopwords = set(stopwords.words(\"english\"))\nimport sys","121318db":"def tokenize(text):\n    '''\n    Convert the text corpus to lower case, remove all punctuations and numbers which lead to\n    a final cleaned corpus with only tokens where all characters in the string are alphabets.\n    '''\n    # convert the text to lower case and replace all new line characters by an empty string\n    lower_text = text.lower().replace('\\n', ' ')\n    table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n    punct_text = lower_text.translate(table)\n    # use NLTK's word tokenization to tokenize the text \n    # remove numbers and empty tokens, only keep tokens with all characters in the string are alphabets\n    tokens = [word for word in word_tokenize(punct_text) if word.isalpha()]\n    return tokens\n\n    \ndef remove_stopwords(word_list, sw=stopwords.words('english')):\n    \"\"\" \n    Filter out all stop words from the text corpus.\n    \"\"\"\n    # It is important to keep words like no and not. Since the meaning of the text will change oppositely\n    # if they are removed.\n\n    rm_words = ['covid', 'cov', 'sars', 'ncov', 'coronavirus', 'coronaviruses', 'mers', 'corona', 'virus', 'disease', 'diseases', 'viral',\n                'jan', 'january', 'feb', 'february', 'march', 'wuhan', 'china', 'hubei', 'december', 'chinese', 'province', 'article', 'protection',\n                'copyright', 'abstract', 'background', 'conclusion', 'summary']\n    sw += rm_words\n    if 'not' in sw:\n        sw.remove('not')\n    if 'no' in sw:\n        sw.remove('no')\n    \n    cleaned = []\n    for word in word_list:\n        if word not in sw:\n            cleaned.append(word)\n    return cleaned\n\n    \ndef stem_words(word_list):\n    stemmer = SnowballStemmer('english')\n    stemmed_words = [stemmer.stem(word) for word in word_list]\n    text = \" \".join(stemmed_words)\n    return stemmed_words, text\n\n    \ndef preprocess(text):\n    \"\"\"\n    Combine all preprocess steps together.\n    Clean each text into tokenized stemmed word list and also return concatenated string\n    \"\"\"\n    tokenized = tokenize(text)\n    stopword_removed = remove_stopwords(tokenized)\n    tokenized_str, cleaned_str = stem_words(stopword_removed)\n    return stopword_removed, tokenized_str, cleaned_str","222bc05b":"# clean sentences\nsentences = large_list\ntokenized_sent = []\nstr_sent = []\nword_dict = {}\nfor sent in sentences:\n  result = preprocess(sent)\n  tokenized = result[0]\n  stemmed = result[1]\n  for i in range(0,len(stemmed)):\n    if stemmed[i] not in word_dict:\n      word_dict[stemmed[i]] = tokenized[i]\n  tokenized_sent.append(stemmed)\n  str_sent.append(result[2])","d94c6784":"new_data = pd.DataFrame(\n    {\"sentence\": large_list,\n     \"tokenized_sent\": tokenized_sent,\n     \"string_sent\": str_sent}\n)\nnew_data.head()","7a5a5827":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.cluster import KMeans\ntfidf_vectorizer = TfidfVectorizer(min_df=5, norm=\"l2\", sublinear_tf=True, strip_accents='unicode', \n                                                  analyzer='word', token_pattern=r'\\w{1,}', stop_words='english', \n                                                  ngram_range=(1,3))\n%time tfidf_matrix = tfidf_vectorizer.fit_transform(new_data[\"string_sent\"])\nprint(tfidf_matrix.shape)\nterms_title = tfidf_vectorizer.get_feature_names()","a2d3b9ef":"num_clusters = 5\nkm = KMeans(n_clusters = num_clusters, random_state=1)\n\n%time km.fit(tfidf_matrix)\nclusters = km.labels_.tolist()","304ea116":"print(\"Top Vocabularies of each cluster:\")\n\norder_centroids = km.cluster_centers_.argsort()[:, ::-1]\n\nfor i in range(num_clusters):\n  print(\"Cluster %d vocabs\" % i, end = \"\\n\")\n\n  for ind in order_centroids[i, :50]:\n    top_word = \"\"\n    for word in terms_title[ind].split(\" \"):\n      top_word += word_dict[word] + \" \"\n\n    print(\" %s\" % top_word.encode(\"utf-8\", \"ignore\"), end = \"\\n\")\n  print(\"\")\n  print(\"\")","414404c7":"# remove cluster 3,4\nnew_data['cluster_id'] = clusters\ndata_sub = new_data[(new_data['cluster_id'] != 3) & (new_data['cluster_id'] != 4)]","a6e5314d":"from nltk import FreqDist\nimport gensim\nfrom gensim.models import LdaModel\nfrom gensim import models, corpora, similarities\nfrom scipy.stats import entropy\n\ndef train_model_lda(data):\n    \n    num_topics = 20\n    chunksize = 300\n    dictionary = corpora.Dictionary(new_data['tokenized_sent'])\n    corpus = [dictionary.doc2bow(doc) for doc in data['tokenized_sent']]\n\n    lda = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary,\n                   alpha=0.005, eta=0.001, chunksize=chunksize, minimum_probability=0.0, passes=2)\n    return dictionary, corpus, lda","79eeffb4":"np.random.seed(1)\ndictionary, corpus, lda = train_model_lda(data_sub)","d514ccf3":"for i in range(0,20):\n  print(\"--------Topic: \", i, \"--------\")\n  topic = lda.show_topic(topicid=i, topn=50)\n  print([word_dict[word[0]] for word in topic])\n  print()\n  print()","3402246f":"def assign_topic_importance(text, topic):\n  bow = dictionary.doc2bow(text)\n  doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=bow)])\n  return doc_distribution[topic]","72c27188":"for i in [1,2,18,19]:\n  print(\"--------Topic: \", i, \"--------\")\n  topic = lda.show_topic(topicid=i, topn=50)\n  print([word_dict[word[0]] for word in topic])\n  print()\n  print()","b5538d4d":"topic_importance = []\nfor topic in [1,2,18,19]:\n    temp = []\n    for sent in list(new_data['tokenized_sent']):\n        temp.append(assign_topic_importance(sent, topic))\n    topic_importance.append(temp)","476df978":"# calculate topic coverage scores for each sentence\nnew_data['pregnancy and neonates'] = topic_importance[0]\nnew_data['comorbidities and pre-existing disease'] = topic_importance[1]\nnew_data['gender and age'] = topic_importance[2]\nnew_data['psychological and mitigation measures'] = topic_importance[3]","cf961f24":"# clean\nfilter_out = []\nsentences = new_data['sentence'].tolist()\ndel_keywords = ['http', 'copy', 'right', 'copyrights', 'reserved', 'www']\nfor sent in sentences:\n    if any([word in sent for word in del_keywords]):\n        filter_out.append(False)\n    else:\n        filter_out.append(True)","dcea12a0":"new_data = new_data[filter_out]\nnew_data = new_data.drop_duplicates(subset=\"sentence\")\nprint(new_data.shape)\nnew_data[['pregnancy and neonates', \n          'comorbidities and pre-existing disease', \n          'gender and age', \n          'psychological and mitigation measures']].describe()","082949e4":"pd.qcut(new_data['pregnancy and neonates'], 20)","1f159152":"# Pregnancy and Neonates\npreg_neo = new_data.sort_values(by=['pregnancy and neonates'], ascending=False)\n# high importance\npreg_neo = preg_neo[preg_neo['pregnancy and neonates']>=0.214]\nkeyword_check = ['pregnan', 'newborn', 'neonat', 'babies', 'baby', 'birth', 'mother', 'delivery',\n                'breastmilk', 'perinatal', 'placental', 'fetal', 'maternal', 'breast', 'uterine']\nbackground_check = ['background', 'aim', 'in this', 'would like to', 'objective', 'introduction', 'this paper',\n                   'review', 'emerge', 'cite', 'record', 'setting', 'recent', 'january', 'february', 'march',\n                    'method', 'approach', 'unpublished']\npreg_check = []\nfor sent in preg_neo['sentence']:\n    if len(sent.split(' ')) < 30:\n        preg_check.append(False)\n    else:\n        key = any([word in sent for word in keyword_check])\n        drop = not any([word in sent for word in background_check])\n        filtr = key & drop\n        preg_check.append(filtr) \n\npreg_neo = preg_neo[preg_check]\nprint(\"Number of sentences found: \", preg_neo.shape[0])\nprint()\nprint(\"Top 30 Answers:\")\n# show the most relevant 30 sentences\nfor i in range(30):\n    print(preg_neo['sentence'].tolist()[i])\n    print(round(preg_neo['pregnancy and neonates'].tolist()[i],5))\n    print()\n    print()\n","fd09a04d":"sw = stopwords.words('english') + ['conclusions', 'conclusion', 'summary', 'data', 'sars', 'cov', 'of', 'and',\n                                   'with', 'covid', 'therefore', 'many', 'data', 'also', 'but', 'however']","bfd690b9":"from wordcloud import WordCloud\n# Generate a word cloud image\ntext =  ' '.join(preg_neo['sentence'].tolist())\n#wordcloud = WordCloud(stopwords = stopwords.words('english')).generate(text)\n\nimport matplotlib.pyplot as plt\n\n# lower max_font_size\nwordcloud = WordCloud(width=800, height=400, background_color='black', stopwords = sw, max_font_size=60, random_state=4).generate(text)\nplt.figure(figsize=(30,30), facecolor = 'k')\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","704e38b1":"pd.qcut(new_data['comorbidities and pre-existing disease'], 20)","d6e1ed71":"# Comorbidities and Pre-existing Diseases\ncomorbidity = new_data.sort_values(by=['comorbidities and pre-existing disease'], ascending=False)\n# high importance\ncomorbidity = comorbidity[comorbidity['comorbidities and pre-existing disease']>=0.447]\nkeyword_check = ['diabete', 'comorbid', 'cancer', 'cardio', 'pulmonary',\n                'smok', 'condition', 'chronic', 'underlying', 'hypertension', 'pneumonia',\n                 'severe', 'preexisting', 'pre-existing', 'obesity', 'co-infection', 'coinfection',\n                'kidney', 'liver', 'severity']\nbackground_check = ['background', 'aim', 'in this', 'would like to', 'objective', 'introduction', 'this paper',\n                   'review', 'emerge', 'cite', 'record', 'setting', 'recent', 'january', 'february', 'march',\n                    'method', 'approach', 'unpublished', 'mers', 'middle east', 'investigat', 'explor', '2019',\n                    '2020']\ncomorb_check = []\nfor sent in comorbidity['sentence']:\n    if len(sent.split(' ')) < 30:\n        comorb_check.append(False)\n    else:\n        key = any([word in sent for word in keyword_check])\n        drop = not any([word in sent for word in background_check])\n        filtr = key & drop\n        comorb_check.append(filtr) \n\ncomorbidity = comorbidity[comorb_check]\nprint(\"Number of sentences found: \", comorbidity.shape[0])\nprint()\nprint(\"Top 30 Answers:\")\n# show the most relevant 30 sentences\nfor i in range(30):\n    print(comorbidity['sentence'].tolist()[i])\n    print(round(comorbidity['comorbidities and pre-existing disease'].tolist()[i],5))\n    print()\n    print()\n","6c2194e1":"sw = stopwords.words('english') + ['conclusions', 'conclusion', 'summary', 'data', 'sars', 'cov', 'of', 'and',\n                                   'with', 'covid', 'therefore', 'many', 'data', 'also', 'but', 'however', 'or ci',\n                                   'ci and', 'ci to', 'including'\n                                  ]","67883331":"\n# Generate a word cloud image\ntext =  ' '.join(comorbidity['sentence'].tolist())\n\n# lower max_font_size\nwordcloud = WordCloud(width=800, height=400, background_color='black', stopwords = sw, max_font_size=60, random_state=4).generate(text)\nplt.figure(figsize=(30,30), facecolor = 'k')\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","65648dc9":"pd.qcut(new_data['gender and age'], 20)","ca058560":"# Gender and Age\ngender_age = new_data.sort_values(by=['gender and age'], ascending=False)\n# high importance\ngender_age = gender_age[gender_age['gender and age']>=0.273]\nkeyword_check = ['gender', 'male', 'female', 'woman', 'women', 'man', 'men', 'newborn', 'infant', 'child', \n                 'adult', 'old', 'elder', 'young', 'sex', 'advanced age', 'age', 'years old']\nbackground_check = ['background', 'aim', 'in this', 'would like to', 'objective', 'introduction', 'this paper',\n                   'review', 'emerge', 'cite', 'record', 'setting', 'recent', 'january', 'february', 'march',\n                    'method', 'approach', 'unpublished', 'mers', 'middle east', 'investigat', 'explor', '2019',\n                    '2020', 'studies']\nagegender_check = []\nfor sent in gender_age['sentence']:\n    if len(sent.split(' ')) < 30:\n        agegender_check.append(False)\n    else:\n        key = any([word in sent for word in keyword_check])\n        drop = not any([word in sent for word in background_check])\n        filtr = key & drop\n        agegender_check.append(filtr) \n\ngender_age = gender_age[agegender_check]\nprint(\"Number of sentences found: \", gender_age.shape[0])\nprint()\nprint(\"Top 30 Answers:\")\n# show the most relevant 30 sentences\nfor i in range(30):\n    print(gender_age['sentence'].tolist()[i])\n    print(round(gender_age['gender and age'].tolist()[i],5))\n    print()\n    print()","f1771fad":"pd.qcut(new_data['psychological and mitigation measures'], 20)","5441db6a":"# Psychological and Mitigation Measures\npsy_measures = new_data.sort_values(by=['psychological and mitigation measures'], ascending=False)\n\n# high importance\npsy_measures = psy_measures[psy_measures['psychological and mitigation measures']>=0.255]\nkeyword_check = ['mental', 'psycholog', 'environmental', 'anxiety', 'socioeconomic',\n                'socio-economic', 'econom', 'measure', 'lockdown', 'quarantine', 'isolation',\n                'mitigation', 'distancing', 'emotional', 'regulation', 'order', 'behav']\nbackground_check = ['background', 'aim', 'in this', 'would like to', 'objective', 'introduction', 'this paper',\n                   'review', 'emerge', 'cite', 'record', 'setting', 'recent', 'january', 'february', 'march',\n                    'method', 'approach', 'capital', 'unpublished', 'mers', 'middle east', 'investigat', \n                    'explor', '2019',\n                    '2020', 'studies']\npsy_check = []\nfor sent in psy_measures['sentence']:\n    if len(sent.split(' ')) < 30:\n        psy_check.append(False)\n    else:\n        key = any([word in sent for word in keyword_check])\n        drop = not any([word in sent for word in background_check])\n        filtr = key & drop\n        psy_check.append(filtr) \n\npsy_measures = psy_measures[psy_check]\nprint(\"Number of sentences found: \", psy_measures.shape[0])\nprint()\nprint(\"Top 30 Answers:\")\n# show the most relevant 30 sentences\nfor i in range(30):    \n    print(psy_measures['sentence'].tolist()[i])\n    print(round(psy_measures['psychological and mitigation measures'].tolist()[i],5))\n    print()\n    print()","81c10d56":"sw = stopwords.words('english') + ['conclusions', 'conclusion', 'summary', 'data', 'sars', 'cov', 'of', 'and',\n                                   'with', 'covid', 'therefore', 'many', 'data', 'also', 'but', 'however', \n                                   'including', 'among', 'due to', 'may', 'could'\n                                  ]","fea45ea7":"# Generate a word cloud image\ntext =  ' '.join(psy_measures['sentence'].tolist())\n\n# lower max_font_size\nwordcloud = WordCloud(width=800, height=400, background_color='black', stopwords = sw, max_font_size=60, random_state=3).generate(text)\nplt.figure(figsize=(30,30), facecolor = 'k')\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","2713a104":"## Topics we need:\n\n1. Topic 1 - Pregnancy and Neonates\n2. Topic 2 - Co-morbidities and Pre-existing Diseases\n3. Topic 18 - Gender and Age Difference\n4. Topic 19 - Psychological, Behavioral Factors and Government Mitigation Measures\n","ed0e35fd":"Created by a [TransUnion](https:\/\/www.transunion.com\/) data scientist that believes that information can be used to **change our world for the better**. #InformationForGood","eaed9f60":"# Step 5: Match Sentences to Bullet Points\n\n* Use LDA to help find 20 topics covered in the corpus.\n* Match topics to bullet points under task description.\n* Use topic coverage percentage to sort all the sentences based on relevance.\n* Output the most relevant answers to bullet points.\n* Use word cloud to visualize the results.\n\n","dc78127e":"Check the related abstracts found using LDA:","7434cb0f":"## What do we know about *Psychological and Mitigation Measures*?","0d48b20d":"## What do we know about *Pregnancy and Neonates*?","a1b95c2f":"Use LDA to find target topics:","2683e79e":"## What do we know about *Comorbidities and Pre-existing Disease*?","8d758609":"## What do we know about *Age and Gender*?","802e6f71":"# Step 4: Text Summarization\n- Summarize each selected abstract using ***skip-thoughts*** \n\n    Reference: \n* https:\/\/medium.com\/jatana\/unsupervised-text-summarization-using-sentence-embeddings-adb15ce83db1\n* https:\/\/github.com\/ryankiros\/skip-thoughts\n\n- Why Text Summarization:\n\n\n>> In the end, we want to find key senteces that are most relevant to each sub-task, not just relevant abstracts\/articles.\n\n>> So we want to keep each abstract clear and concise. \n\n**Skip-Thoughts**\n\n\n*  Similar to sent2vec, skip-thoughts learns to encode input sentences into a fixed-dimensional vector representation. \n*  Encoder Network: The encoder is typically a GRU-RNN which generates a fixed length vector representation for each sentence in the input\n*  The decoder is expected to generate the previous and next sentences, word by word. \n*  The encoder-decoder network is trained to minimize the sentence reconstruction loss.\n*  These learned representations are embeddings of semantically similar sentences are closer to each other in vector space.\n\n\n\n---\n\n### **Pipeline**\n\n**Step 1** Preprocessing\n\n- To lower case, remove punctuations, remove stop words.... (Complete)\n\n**Step 2** Skip-thoughts Encoder\n- Sentences are encoded into fixed-dimensional vector representation\n\n**Step 3** Clustering\n- The encoded sentences are clustered (K-Means)\n\n**Step 4** Summarization\n- The sentences corresponding to sentence embeddings that are closest to the cluster centers are chosen and combined as final summary\n\n","4f338bce":"# Step3-2: Final Target Articles\n- Use keyword search to find more related articles\n- Filter out all articles that are not related to COVID-19\n","e150ecd3":"# Task 2 What do we know about COVID-19 risk factors?\n\n\n***Task Details***\n\nWhat do we know about COVID-19 risk factors? What have we learned from epidemiological studies?\n\nSpecifically, we want to know what the literature reports about:\n\nData on potential risks factors\n\n- Smoking, pre-existing pulmonary disease\n- Co-infections (determine whether co-existing respiratory\/viral infections make the virus more transmissible or virulent) and other co-morbidities\n- Neonates and pregnant women\n- Socio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences.\n- Transmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors\n- Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups\n- Susceptibility of populations\n- Public health mitigation measures that could be effective for control","9881538e":"* pregnant women\n* vertical transmission\n* no evidence\n* similar to\n* limited data\n","714e0827":"\n# Step 2: Preprocessing\n- Remove all punctuations, numbers and all other non-alphabets.\n- Convert all texts to lower case.\n- Word Tokenization\n- Remove stopwords\n- Stemming\n\n","0f1d102a":"# Step 3: Find Related Articles\n1. Topic Modeling: Latent Dirichlet Allocation (LDA)\n    \n    Reference: https:\/\/www.kaggle.com\/ktattan\/lda-and-document-similarity\n2. Keyword Search","2fe28861":"Topic Distribution of all the abstracts we have in data.","99ab14b1":"* mental health\n* social distancing\n* stress\n* lockdown\n* social support\n* pyschological distress\n* depression anxiety","8a870184":"Check related abstracts found using keyword search:","7481e5a4":"# Step1: Import Data\n- metadata.csv\n- Remove articles that were published before November 2019","0eacf1b6":"# Final Step: Answer Questions","bdd21aaa":"* obesity\n* hypertension\n* diabetes\n* kidney diseases\n* cardiovascular, heart diseases\n* cancer"}}