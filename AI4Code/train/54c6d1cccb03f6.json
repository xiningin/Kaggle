{"cell_type":{"b8eda217":"code","be3cc434":"code","3d702707":"code","19e0e08a":"code","6fa11c08":"code","2bfbe64f":"code","f02189a8":"code","cdbdd39a":"code","4d7fad71":"code","6e3e92b4":"code","57c817b8":"code","38266152":"code","7c6812b5":"code","803ac58b":"code","13fa68e2":"code","c691b0af":"markdown","641e94b7":"markdown","60e9d492":"markdown","ed056b39":"markdown","00af9088":"markdown","b16f4e55":"markdown","e2b9e1f7":"markdown","a0560855":"markdown","e0ccad9a":"markdown","9d450c3d":"markdown","7d113705":"markdown","7130155d":"markdown","83b8020b":"markdown","bbf0e123":"markdown","03fc3092":"markdown","9f6c6df9":"markdown","154f6036":"markdown"},"source":{"b8eda217":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","be3cc434":"df = pd.read_csv(\"..\/input\/Iris.csv\")   # reading the data","3d702707":"df.head()    # first 5 rows","19e0e08a":"df.drop([\"Id\"],axis=1,inplace=True)    # dropped\n\ndf.tail()   # last 5 rows","6fa11c08":"df.info()   # all non-null and numeric [except the labels]","2bfbe64f":"import matplotlib.pyplot as plt\nimport seaborn as sns","f02189a8":"sns.pairplot(data=df,hue=\"Species\",palette=\"Set2\")\nplt.show()","cdbdd39a":"features = df.loc[:,[\"SepalLengthCm\",\"SepalWidthCm\",\"PetalLengthCm\",\"PetalWidthCm\"]]","4d7fad71":"from sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=99)        # 99 CLUSTERS ?????","6e3e92b4":"from sklearn.cluster import KMeans\nwcss = []\n\nfor k in range(1,15):\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(features)\n    wcss.append(kmeans.inertia_)\n\n\nplt.figure(figsize=(20,8))\nplt.title(\"WCSS \/ K Chart\", fontsize=18)\nplt.plot(range(1,15),wcss,\"-o\")\nplt.grid(True)\nplt.xlabel(\"Amount of Clusters\",fontsize=14)\nplt.ylabel(\"Inertia\",fontsize=14)\nplt.xticks(range(1,20))\nplt.tight_layout()\nplt.show()\n","57c817b8":"plt.figure(figsize=(24,4))\n\nplt.suptitle(\"K Means Clustering\",fontsize=20)\n\n\nplt.subplot(1,5,1)\nplt.title(\"K = 1\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nplt.ylabel(\"PetalWidthCm\")\nplt.scatter(features.PetalLengthCm,features.PetalWidthCm)\n\n\nplt.subplot(1,5,2)\nplt.title(\"K = 2\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nkmeans = KMeans(n_clusters=2)\nfeatures[\"labels\"] = kmeans.fit_predict(features)\nplt.scatter(features.PetalLengthCm[features.labels == 0],features.PetalWidthCm[features.labels == 0])\nplt.scatter(features.PetalLengthCm[features.labels == 1],features.PetalWidthCm[features.labels == 1])\n\n# I drop labels since we only want to use features.\nfeatures.drop([\"labels\"],axis=1,inplace=True)\n\nplt.subplot(1,5,4)\nplt.title(\"K = 3\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nkmeans = KMeans(n_clusters=3)\nfeatures[\"labels\"] = kmeans.fit_predict(features)\nplt.scatter(features.PetalLengthCm[features.labels == 0],features.PetalWidthCm[features.labels == 0])\nplt.scatter(features.PetalLengthCm[features.labels == 1],features.PetalWidthCm[features.labels == 1])\nplt.scatter(features.PetalLengthCm[features.labels == 2],features.PetalWidthCm[features.labels == 2])\n\n# I drop labels since we only want to use features.\nfeatures.drop([\"labels\"],axis=1,inplace=True)\n\nplt.subplot(1,5,3)\nplt.title(\"K = 4\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nkmeans = KMeans(n_clusters=4)\nfeatures[\"labels\"] = kmeans.fit_predict(features)\nplt.scatter(features.PetalLengthCm[features.labels == 0],features.PetalWidthCm[features.labels == 0])\nplt.scatter(features.PetalLengthCm[features.labels == 1],features.PetalWidthCm[features.labels == 1])\nplt.scatter(features.PetalLengthCm[features.labels == 2],features.PetalWidthCm[features.labels == 2])\nplt.scatter(features.PetalLengthCm[features.labels == 3],features.PetalWidthCm[features.labels == 3])\n\n# I drop labels since we only want to use features.\nfeatures.drop([\"labels\"],axis=1,inplace=True)\n\nplt.subplot(1,5,5)\nplt.title(\"Original Labels\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nplt.scatter(df.PetalLengthCm[df.Species == \"Iris-setosa\"],df.PetalWidthCm[df.Species == \"Iris-setosa\"])\nplt.scatter(df.PetalLengthCm[df.Species == \"Iris-versicolor\"],df.PetalWidthCm[df.Species == \"Iris-versicolor\"])\nplt.scatter(df.PetalLengthCm[df.Species == \"Iris-virginica\"],df.PetalWidthCm[df.Species == \"Iris-virginica\"])\n\nplt.subplots_adjust(top=0.8)\nplt.show()","38266152":"from sklearn.cluster import AgglomerativeClustering\nhc_cluster = AgglomerativeClustering(n_clusters=99)    # 99 CLUSTERS ????? ","7c6812b5":"from scipy.cluster.hierarchy import dendrogram, linkage\n\nmerg = linkage(features,method=\"ward\")\n\nplt.figure(figsize=(18,6))\ndendrogram(merg, leaf_rotation=90)\nplt.xlabel(\"data points\")\nplt.ylabel(\"euclidian distance\")\n\nplt.suptitle(\"DENDROGRAM\",fontsize=18)\nplt.show()","803ac58b":"plt.figure(figsize=(24,4))\n\nplt.suptitle(\"Hierarchical Clustering\",fontsize=20)\n\nplt.subplot(1,5,1)\nplt.title(\"K = 1\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nplt.ylabel(\"PetalWidthCm\")\nplt.scatter(features.SepalLengthCm,features.SepalWidthCm)\n\n\nplt.subplot(1,5,2)\nplt.title(\"K = 2\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nhc_cluster = AgglomerativeClustering(n_clusters=2)\nfeatures[\"labels\"] = hc_cluster.fit_predict(features)\nplt.scatter(features.SepalLengthCm[features.labels == 0],features.SepalWidthCm[features.labels == 0])\nplt.scatter(features.SepalLengthCm[features.labels == 1],features.SepalWidthCm[features.labels == 1])\n\n# I drop labels since we only want to use features.\nfeatures.drop([\"labels\"],axis=1,inplace=True)\n\nplt.subplot(1,5,3)\nplt.title(\"K = 4\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nhc_cluster = AgglomerativeClustering(n_clusters=4)\nfeatures[\"labels\"] = hc_cluster.fit_predict(features)\nplt.scatter(features.SepalLengthCm[features.labels == 0],features.SepalWidthCm[features.labels == 0])\nplt.scatter(features.SepalLengthCm[features.labels == 1],features.SepalWidthCm[features.labels == 1])\nplt.scatter(features.SepalLengthCm[features.labels == 2],features.SepalWidthCm[features.labels == 2])\nplt.scatter(features.SepalLengthCm[features.labels == 3],features.SepalWidthCm[features.labels == 3])\n\n# I drop labels since we only want to use features.\nfeatures.drop([\"labels\"],axis=1,inplace=True)\n\nplt.subplot(1,5,4)\nplt.title(\"K = 3\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nhc_cluster = AgglomerativeClustering(n_clusters=3)\nfeatures[\"labels\"] = hc_cluster.fit_predict(features)\nplt.scatter(features.SepalLengthCm[features.labels == 0],features.SepalWidthCm[features.labels == 0])\nplt.scatter(features.SepalLengthCm[features.labels == 1],features.SepalWidthCm[features.labels == 1])\nplt.scatter(features.SepalLengthCm[features.labels == 2],features.SepalWidthCm[features.labels == 2])\n\n\nplt.subplot(1,5,5)\nplt.title(\"Original Labels\",fontsize=16)\nplt.xlabel(\"PetalLengthCm\")\nplt.scatter(df.SepalLengthCm[df.Species == \"Iris-setosa\"],df.SepalWidthCm[df.Species == \"Iris-setosa\"])\nplt.scatter(df.SepalLengthCm[df.Species == \"Iris-versicolor\"],df.SepalWidthCm[df.Species == \"Iris-versicolor\"])\nplt.scatter(df.SepalLengthCm[df.Species == \"Iris-virginica\"],df.SepalWidthCm[df.Species == \"Iris-virginica\"])\n\nplt.subplots_adjust(top=0.8)\nplt.show()","13fa68e2":"# I drop labels since we only want to use features.\nfeatures.drop([\"labels\"],axis=1,inplace=True)\n\n# kmeans\nkmeans = KMeans(n_clusters=3)\nkmeans_predict = kmeans.fit_predict(features)\n\n# cross tabulation table for kmeans\ndf1 = pd.DataFrame({'labels':kmeans_predict,\"Species\":df['Species']})\nct1 = pd.crosstab(df1['labels'],df1['Species'])\n\n\n# hierarchy\nhc_cluster = AgglomerativeClustering(n_clusters=3)\nhc_predict = hc_cluster.fit_predict(features)\n\n# cross tabulation table for Hierarchy\ndf2 = pd.DataFrame({'labels':hc_predict,\"Species\":df['Species']})\nct2 = pd.crosstab(df2['labels'],df2['Species'])\n\n\nplt.figure(figsize=(24,8))\nplt.suptitle(\"CROSS TABULATIONS\",fontsize=18)\nplt.subplot(1,2,1)\nplt.title(\"KMeans\")\nsns.heatmap(ct1,annot=True,cbar=False,cmap=\"Blues\")\n\nplt.subplot(1,2,2)\nplt.title(\"Hierarchy\")\nsns.heatmap(ct2,annot=True,cbar=False,cmap=\"Blues\")\n\nplt.show()","c691b0af":"### Again, Our double checking method showed that **3** is more accurate than ** 2 ** value by simply looking to the graph above. \n\n### Reason behind this is basically \"iris-setosa\" being too easy to separate while the other two is quite mixed and it made our Dendrogram method a bit unclear.","641e94b7":"<div id=\"1\"><\/div>\n# ** Adjusting the Dataset for Unsupervised Learning **\n\n### I will simply do not use labels column on my *\"new\"* Dataset","60e9d492":"### Kmeans visibly did an amazing job with **3** clusters. Except few data points, I can say prediction is identical to the original with labels. Which shows that our ELBOW chart was right.\n\n<div id=\"4\"><\/div>\n# ** Implemeting the Hierarchical Clustering**\n\n### Again, Super Simple with SciKit-Learn. ","ed056b39":"### We see that iris-setosa is easily separable from the other two. Especially when we can see in different colors for corresponding Labels like above.\n\n### But our mission was finding the Labels that we didn't knew at all, So Let's create a suitable scenario.","00af9088":"# **Conclusion**\n\n### The both Failed on 16 data points over 150 data points, which is equal to 90%\n\n### We also see that clustering \"iris-setosa\" was easy for both of them (50\/50 success) because it's data points are all easily differentiable\n\n### 15 mistakes of all 16 is coming from \"iris-virginica\". Which shows that it was hard to cluster for my models.\n\n### ***Thanks for reading this far. If you enjoyed please be sure to vote, or if you have some issues don't be shy to comment. Best Regards. Efe***","b16f4e55":"### WHY 99 ????? Because I don't know the right amount of Labels. Don't worry, There is a solution for it.\n\n\n<div id=\"3\"><\/div>\n# **Finding the best amount of clusters to get most accurate results (KMeans) **\n\n### I will use ELBOW RULE, which is basically looking for a plot line that respectively has a slope nearest to 90 degrees compared to y axis and be smallest possible. (yes, looks like an elbow )","e2b9e1f7":"## Double Check!","a0560855":"### we see that longest vertical line without any perpendecular matching lines (euclidian distances). If we draw a horizontal line between that values, we will have ** 2 or 3 ** interceptions which are representing ideal amount of labels.\n\n","e0ccad9a":"Id column is not a real feature of our flowers. I will drop it","9d450c3d":"<div id=\"6\"><\/div>\n# ** Evaluating the Results and Comparing them**","7d113705":"<div id=\"eda\"><\/div>\n# **BASIC EDA**","7130155d":"### **3 or 2** seems to be our ** Best ** value(s) for clusters. (By the ** Elbow Rule**)","83b8020b":"### AGAIN 99 ????? Yes, Because I don't know the right amount of Labels. Again, There is also solution for it.\n<div id=\"5\"><\/div>\n# **Finding the best amount of clusters to get most accurate results (Hierarchy)**\n\n### Longest Vertical line between Horizontal Lines.","bbf0e123":"<div id=\"2\"><\/div>\n# ** Implemeting the K Means Clustering **\n\n### SciKit-Learn implementation is very simple, it only takes 2 lines of code and 1 parameter","03fc3092":"### From now on, we don't know the real labels or amount of labels anymore (Shhh!)","9f6c6df9":"# Hi, this notebook is all about learning and experimenting Unsupervised Learning (on famous Iris Dataset)\n## You'll find: \n\n1- <a href=\"#eda\">EDA<\/a>\n\n2-  <a href=\"#1\">Adjusting the Dataset for Unsupervised Learning<\/a>\n\n3-  <a href=\"#2\">Implemeting the K Means Clustering<\/a>\n\n4-  <a href=\"#3\">Finding the best amount of clusters to get most accurate results (KMeans)<\/a>\n\n5-  <a href=\"#4\">Implemeting the Hierarchical Clustering<\/a>\n\n6-  <a href=\"#5\">Finding the best amount of clusters to get most accurate results (Hierarchy)<\/a>\n\n7-  <a href=\"#6\">Evaluating the Results and Comparing them<\/a>","154f6036":"## Let's Double Check it "}}