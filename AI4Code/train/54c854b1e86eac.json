{"cell_type":{"b1406e58":"code","c39a927e":"code","dcf2bcaf":"code","36612251":"code","d1593601":"code","4b0447dc":"code","3176b7ba":"code","225e7008":"code","422dfa0e":"code","cc7b9e70":"code","43e30628":"code","7e28ed09":"code","adc6523e":"code","a126aacb":"code","6049284d":"code","9e5c2e4f":"code","dbabb35c":"code","e838ff09":"code","102d8731":"code","fdae5c51":"code","0da98d12":"code","e3853073":"code","4ecac74e":"code","a58d7089":"code","fd7feefd":"code","c13aff53":"markdown","d690ac67":"markdown","cbd338d5":"markdown","679d122d":"markdown","da5afbd5":"markdown","40cac420":"markdown"},"source":{"b1406e58":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nprint(os.listdir(\"..\/input\/2019-2nd-ml-month-with-kakr\"))\nprint(os.listdir(\"..\/input\/2019-2nd-ml-month-with-kakr-neighbor-stat\"))\nprint(os.listdir(\"..\/input\/11th-solution-data-public-98316-private-99336\"))\n\n# Any results you write to the current directory are saved as output.","c39a927e":"from IPython.display import Image\nImage('..\/input\/11th-solution-data-public-98316-private-99336\/stacking_in_practice_1.png')","dcf2bcaf":"Image('..\/input\/11th-solution-data-public-98316-private-99336\/stacking_in_practice_2.png')","36612251":"import sys\nimport os\nimport re\nimport warnings\nimport time\nimport gc\nimport random as rn\n\nfrom timeit import default_timer as timer\nfrom datetime import date, datetime, timedelta\nfrom functools import wraps\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder, Imputer, StandardScaler, RobustScaler\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor\nfrom sklearn.linear_model import ElasticNet, Lasso, Ridge\nfrom sklearn.svm import SVR\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\n\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost as cat\n\nimport keras\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Embedding, Reshape, Concatenate, Input, Flatten\nfrom keras import optimizers\nfrom keras import initializers\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom keras import backend as K\nimport tensorflow as tf\nfrom tensorflow import set_random_seed\n\nimport scipy\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\nfrom scipy.cluster import hierarchy as hc\nfrom scipy.special import boxcox1p\n\nfrom tqdm import tqdm_notebook as tqdm\n\n#RANDOM_SEED = 42\nos.environ['PYTHONHASHSEED'] = '0'\nRANDOM_SEED = 0\nrn.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\nset_random_seed(RANDOM_SEED)\n\nsession_conf = tf.ConfigProto(intra_op_parallelism_threads=1,\n                              inter_op_parallelism_threads=1)\nsess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\nK.set_session(sess)\n\ndef plot_numeric_for_regression(df, field, target_field='price'):\n    df = df[df[field].notnull()]\n\n    fig = plt.figure(figsize = (16, 7))\n    ax1 = plt.subplot(121)\n    \n    sns.distplot(df[df['data'] == 'train'][field], label='Train', hist_kws={'alpha': 0.5}, ax=ax1)\n    sns.distplot(df[df['data'] == 'test'][field], label='Test', hist_kws={'alpha': 0.5}, ax=ax1)\n\n    plt.xlabel(field)\n    plt.ylabel('Density')\n    plt.legend()\n    \n    ax2 = plt.subplot(122)\n    \n    df_copy = df[df['data'] == 'train'].copy()\n\n    sns.scatterplot(x=field, y=target_field, data=df_copy, ax=ax2)\n    \n    plt.show()\n    \ndef plot_categorical_for_regression(df, field, target_field='price', show_missing=True, missing_value='NA'):\n    df_copy = df.copy()\n    if show_missing: df_copy[field] = df_copy[field].fillna(missing_value)\n    df_copy = df_copy[df_copy[field].notnull()]\n\n    ax1_param = 121\n    ax2_param = 122\n    fig_size = (16, 7)\n    if df_copy[field].nunique() > 30:\n        ax1_param = 211\n        ax2_param = 212\n        fig_size = (16, 10)\n    \n    fig = plt.figure(figsize = fig_size)\n    ax1 = plt.subplot(ax1_param)\n    \n    sns.countplot(x=field, hue='data', order=np.sort(df_copy[field].unique()), data=df_copy)\n    plt.xticks(rotation=90, fontsize=11)\n    \n    ax2 = plt.subplot(ax2_param)\n    \n    df_copy = df_copy[df_copy['data'] == 'train']\n\n    sns.boxplot(x=field, y=target_field, data=df_copy, order=np.sort(df_copy[field].unique()), ax=ax2)\n    plt.xticks(rotation=90, fontsize=11)\n    \n    plt.show()\n\ndef get_prefix(group_col, target_col, prefix=None):\n    if isinstance(group_col, list) is True:\n        g = '_'.join(group_col)\n    else:\n        g = group_col\n    if isinstance(target_col, list) is True:\n        t = '_'.join(target_col)\n    else:\n        t = target_col\n    if prefix is not None:\n        return prefix + '_' + g + '_' + t\n    return g + '_' + t\n    \ndef groupby_helper(df, group_col, target_col, agg_method, prefix_param=None):\n    try:\n        prefix = get_prefix(group_col, target_col, prefix_param)\n        #print(group_col, target_col, agg_method)\n        group_df = df.groupby(group_col)[target_col].agg(agg_method)\n        group_df.columns = ['{}_{}'.format(prefix, m) for m in agg_method]\n    except BaseException as e:\n        print(e)\n    return group_df.reset_index()\n\ndef rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))\n\ndef rmse_exp(y_true, y_pred):\n    return np.sqrt(mean_squared_error(np.expm1(y_true), np.expm1(y_pred)))\n\ndef time_decorator(func): \n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        print(\"\\nStartTime: \", datetime.now() + timedelta(hours=9))\n        start_time = time.time()\n        \n        df = func(*args, **kwargs)\n        \n        print(\"EndTime: \", datetime.now() + timedelta(hours=9))  \n        print(\"TotalTime: \", time.time() - start_time)\n        return df\n        \n    return wrapper\n\nclass SklearnWrapper(object):\n    def __init__(self, clf, params=None, **kwargs):\n        #if isinstance(SVR) is False:\n        #    params['random_state'] = kwargs.get('seed', 0)\n        self.clf = clf(**params)\n        self.is_classification_problem = True\n        self.use_avg_oof = kwargs.get('use_avg_oof', False)\n    @time_decorator\n    def train(self, x_train, y_train, x_cross=None, y_cross=None):\n        if len(np.unique(y_train)) > 30:\n            self.is_classification_problem = False\n            \n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        if self.is_classification_problem is True:\n            return self.clf.predict_proba(x)[:,1]\n        else:\n            return self.clf.predict(x)\n    \nclass XgbWrapper(object):\n    def __init__(self, params=None, **kwargs):\n        self.param = params\n        self.use_avg_oof = kwargs.get('use_avg_oof', False)\n        self.num_rounds = kwargs.get('num_rounds', 1000)\n        self.early_stopping = kwargs.get('ealry_stopping', 100)\n\n        self.eval_function = kwargs.get('eval_function', None)\n        self.verbose_eval = kwargs.get('verbose_eval', 100)\n        self.best_round = 0\n    \n    @time_decorator\n    def train(self, x_train, y_train, x_cross=None, y_cross=None):\n        need_cross_validation = True\n       \n        if isinstance(y_train, pd.DataFrame) is True:\n            y_train = y_train[y_train.columns[0]]\n            if y_cross is not None:\n                y_cross = y_cross[y_cross.columns[0]]\n\n        if x_cross is None:\n            dtrain = xgb.DMatrix(x_train, label=y_train, silent= True)\n            train_round = self.best_round\n            if self.best_round == 0:\n                train_round = self.num_rounds\n            \n            self.clf = xgb.train(self.param, dtrain, train_round)\n            del dtrain\n        else:\n            dtrain = xgb.DMatrix(x_train, label=y_train, silent=True)\n            dvalid = xgb.DMatrix(x_cross, label=y_cross, silent=True)\n            watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n\n            self.clf = xgb.train(self.param, dtrain, self.num_rounds, watchlist, feval=self.eval_function,\n                                 early_stopping_rounds=self.early_stopping,\n                                 verbose_eval=self.verbose_eval)\n            self.best_round = max(self.best_round, self.clf.best_iteration)\n\n    def predict(self, x):\n        return self.clf.predict(xgb.DMatrix(x), ntree_limit=self.best_round)\n\n    def get_params(self):\n        return self.param    \n    \nclass LgbmWrapper(object):\n    def __init__(self, params=None, **kwargs):\n        self.param = params\n        self.use_avg_oof = kwargs.get('use_avg_oof', False)\n        self.num_rounds = kwargs.get('num_rounds', 1000)\n        self.early_stopping = kwargs.get('ealry_stopping', 100)\n\n        self.eval_function = kwargs.get('eval_function', None)\n        self.verbose_eval = kwargs.get('verbose_eval', 100)\n        self.best_round = 0\n        \n    @time_decorator\n    def train(self, x_train, y_train, x_cross=None, y_cross=None):\n        \"\"\"\n        x_cross or y_cross is None\n        -> model train limted num_rounds\n        \n        x_cross and y_cross is Not None\n        -> model train using validation set\n        \"\"\"\n        if isinstance(y_train, pd.DataFrame) is True:\n            y_train = y_train[y_train.columns[0]]\n            if y_cross is not None:\n                y_cross = y_cross[y_cross.columns[0]]\n\n        if x_cross is None:\n            dtrain = lgb.Dataset(x_train, label=y_train, silent= True)\n            train_round = self.best_round\n            if self.best_round == 0:\n                train_round = self.num_rounds\n                \n            self.clf = lgb.train(self.param, train_set=dtrain, num_boost_round=train_round)\n            del dtrain   \n        else:\n            dtrain = lgb.Dataset(x_train, label=y_train, silent=True)\n            dvalid = lgb.Dataset(x_cross, label=y_cross, silent=True)\n            self.clf = lgb.train(self.param, train_set=dtrain, num_boost_round=self.num_rounds, valid_sets=[dtrain, dvalid],\n                                  feval=self.eval_function, early_stopping_rounds=self.early_stopping,\n                                  verbose_eval=self.verbose_eval)\n            self.best_round = max(self.best_round, self.clf.best_iteration)\n            del dtrain, dvalid\n            \n        gc.collect()\n    \n    def predict(self, x):\n        return self.clf.predict(x, num_iteration=self.clf.best_iteration)\n    \n    def plot_importance(self, importance_type='gain', max_num_features=20):\n        lgb.plot_importance(self.clf, importance_type=importance_type, max_num_features=max_num_features, height=0.7, figsize=(10,30))\n        plt.show()\n        \n    def get_params(self):\n        return self.param\n\nclass CatWrapper(object):\n    def __init__(self, params=None, **kwargs):\n        self.param = params\n        self.use_avg_oof = kwargs.get('use_avg_oof', False)\n        self.num_rounds = kwargs.get('num_rounds', 1000)\n        self.param['iterations'] = kwargs.get('num_rounds', 1000)\n        self.early_stopping = kwargs.get('ealry_stopping', 100)\n\n        self.eval_function = kwargs.get('eval_function', None)\n        self.verbose_eval = kwargs.get('verbose_eval', 100)\n        self.best_round = 0\n        \n    @time_decorator\n    def train(self, x_train, y_train, x_cross=None, y_cross=None, cat_features=None):\n        \"\"\"\n        x_cross or y_cross is None\n        -> model train limted num_rounds\n        \n        x_cross and y_cross is Not None\n        -> model train using validation set\n        \"\"\"\n        if isinstance(y_train, pd.DataFrame) is True:\n            y_train = y_train[y_train.columns[0]]\n            if y_cross is not None:\n                y_cross = y_cross[y_cross.columns[0]]\n\n        if x_cross is None:\n            dtrain = cat.Pool(x_train, y_train, cat_features=cat_features)\n            train_round = self.best_round\n            if self.best_round == 0:\n                train_round = self.num_rounds\n                \n            self.clf = cat.CatBoost(params=self.param)\n            self.clf.fit(dtrain, verbose_eval=self.verbose_eval)\n            del dtrain   \n        else:\n            dtrain = cat.Pool(x_train, y_train, cat_features=cat_features)\n            dvalid = cat.Pool(x_cross, y_cross, cat_features=cat_features)\n            \n            self.clf = cat.CatBoost(params=self.param)\n            self.clf.fit(dtrain, eval_set=[dvalid], early_stopping_rounds=self.early_stopping, verbose_eval=self.verbose_eval)\n            self.best_round = max(self.best_round, self.clf.best_iteration_)\n            del dtrain, dvalid\n            \n        gc.collect()\n    \n    def predict(self, x):\n        if self.clf.best_iteration_ is None: return self.clf.predict(x, ntree_end=self.best_round)\n        else: return self.clf.predict(x, ntree_end=self.clf.best_iteration_)\n        \n    def get_params(self):\n        return self.param\n    \nclass KerasWrapper(object):\n    def __init__(self, model_func, params=None, **kwargs):\n        self.model_func = model_func\n        self.param = params\n        self.use_avg_oof = kwargs.get('use_avg_oof', False)\n        self.epochs = kwargs.get('epochs', 20)\n        self.batch_size = kwargs.get('batch_size', 16)\n        self.callbacks = kwargs.get('callbacks', None)\n        self.shuffle = kwargs.get('shuffle', True)\n        self.best_epochs = 0\n    @time_decorator\n    def train(self, x_train, y_train, x_cross=None, y_cross=None):\n        self.model = self.model_func(x_train.shape[1])\n        if x_cross is None:\n            train_epochs = self.best_epochs\n            if self.best_epochs == 0:\n                train_epochs = self.epochs\n                \n            self.model.fit(x_train, y_train, epochs=train_epochs, batch_size=self.batch_size,\n                           shuffle=self.shuffle, callbacks=self.callbacks, verbose=0)\n        else:\n            hist = self.model.fit(x_train, y_train, epochs=self.epochs, batch_size=self.batch_size,\n                                  shuffle=self.shuffle, validation_data=(x_cross, y_cross),\n                                  callbacks=self.callbacks, verbose=0)\n            self.best_epochs = max(self.best_epochs, len(hist.history['val_loss']))\n\n    def predict(self, x):\n        if isinstance(x, pd.DataFrame):\n            return self.model.predict(x.values).ravel()\n        else:\n            return self.model.predict(x).ravel()\n    \n    def get_params(self):\n        return self.param\n\nclass KerasEmbeddingWrapper(object):\n    def __init__(self, model_func, params=None, **kwargs):\n        self.model_func = model_func\n        self.param = params\n        self.use_avg_oof = kwargs.get('use_avg_oof', False)\n        self.epochs = kwargs.get('epochs', 20)\n        self.batch_size = kwargs.get('batch_size', 16)\n        self.callbacks = kwargs.get('callbacks', None)\n        self.embedding_cols = kwargs.get('embedding_cols', None)\n        self.shuffle = kwargs.get('shuffle', True)\n        self.best_epochs = 0\n    @time_decorator\n    def train(self, x_train, y_train, x_cross=None, y_cross=None):\n        non_embedding_cols = [col for col in x_train.columns if col not in self.embedding_cols]\n        self.model = self.model_func(x_train, self.embedding_cols)\n        if x_cross is None:\n            train_epochs = self.best_epochs\n            if self.best_epochs == 0:\n                train_epochs = self.epochs\n                \n            x_tr_list = []\n            x_tr_list.append(x_train[non_embedding_cols])\n            for col in self.embedding_cols:\n                x_tr_list.append(x_train[col])\n            self.model.fit(x_tr_list, y_train, epochs=train_epochs, batch_size=self.batch_size,\n                           shuffle=self.shuffle, callbacks=self.callbacks, verbose=0)\n        else:\n            x_tr_list = []\n            x_tr_list.append(x_train[non_embedding_cols])\n            for col in self.embedding_cols:\n                x_tr_list.append(x_train[col])\n\n            x_cr_list = []\n            x_cr_list.append(x_cross[non_embedding_cols])\n            for col in self.embedding_cols:\n                x_cr_list.append(x_cross[col])\n            hist = self.model.fit(x_tr_list, y_train, epochs=self.epochs, batch_size=self.batch_size, shuffle=self.shuffle,\n                                  validation_data=(x_cr_list, y_cross), callbacks=self.callbacks, verbose=0)\n            self.best_epochs = max(self.best_epochs, len(hist.history['val_loss']))\n\n    def predict(self, x):\n        non_embedding_cols = [col for col in x.columns if col not in self.embedding_cols]\n        x_list = []\n        x_list.append(x[non_embedding_cols])\n        for col in self.embedding_cols:\n            x_list.append(x[col])\n        return self.model.predict(x_list).ravel()\n    \n    def get_params(self):\n        return self.param\n\n\n@time_decorator\ndef get_oof(clf, x_train, y_train, x_test, eval_func, **kwargs):\n    nfolds = kwargs.get('NFOLDS', 5)\n    kfold_shuffle = kwargs.get('kfold_shuffle', True)\n    kfold_random_state = kwargs.get('kfold_random_state', 0)\n    stratified_kfold_ytrain = kwargs.get('stratifed_kfold_y_value', None)\n    ntrain = x_train.shape[0]\n    ntest = x_test.shape[0]\n    \n    kf_split = None\n    if stratified_kfold_ytrain is None:\n        kf = KFold(n_splits=nfolds, shuffle=kfold_shuffle, random_state=kfold_random_state)\n        kf_split = kf.split(x_train)\n    else:\n        kf = StratifiedKFold(n_splits=nfolds, shuffle=kfold_shuffle, random_state=kfold_random_state)\n        kf_split = kf.split(x_train, stratified_kfold_ytrain)\n        \n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n\n    cv_sum = 0\n    \n    # before running model, print model param\n    # lightgbm model and xgboost model use get_params()\n    try:\n        if clf.clf is not None:\n            print(clf.clf)\n    except:\n        print(clf)\n        print(clf.get_params())\n\n    for i, (train_index, cross_index) in enumerate(kf_split):\n        x_tr, x_cr = None, None\n        y_tr, y_cr = None, None\n        if isinstance(x_train, pd.DataFrame):\n            x_tr, x_cr = x_train.iloc[train_index], x_train.iloc[cross_index]\n            y_tr, y_cr = y_train.iloc[train_index], y_train.iloc[cross_index]\n        else:\n            x_tr, x_cr = x_train[train_index], x_train[cross_index]\n            y_tr, y_cr = y_train[train_index], y_train[cross_index]\n\n        clf.train(x_tr, y_tr, x_cr, y_cr)\n        \n        oof_train[cross_index] = clf.predict(x_cr)\n        if hasattr(clf, 'use_avg_oof') and clf.use_avg_oof:\n            oof_test += clf.predict(x_test)\/nfolds\n\n        cv_score = eval_func(y_cr, oof_train[cross_index])\n        \n        print('Fold %d \/ ' % (i+1), 'CV-Score: %.6f' % cv_score)\n        cv_sum = cv_sum + cv_score\n        \n        del x_tr, x_cr, y_tr, y_cr\n        \n    gc.collect()\n    \n    score = cv_sum \/ nfolds\n    print(\"Average CV-Score: \", score)\n\n    # Using All Dataset, retrain\n    if not hasattr(clf, 'use_avg_oof') or clf.use_avg_oof is False:\n        clf.train(x_train, y_train)\n        oof_test = clf.predict(x_test)\n\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1), score\n\n@time_decorator\ndef stacking(data_list, y_train, model_list, eval_func=None, nfolds=5, kfold_random_state=RANDOM_SEED):\n    \n    oof_train_list = []\n    oof_test_list = []\n    oof_cv_score_list = []\n    \n    for X_train, X_test in data_list:\n        print(X_train.shape, X_test.shape, y_train.shape)\n        for model in model_list:\n            oof_train, oof_test, oof_cv_score = get_oof(model, X_train, y_train, X_test, eval_func,\n                                                        NFOLDS=nfolds, kfold_random_state=kfold_random_state)\n            oof_train_list.append(oof_train)\n            oof_test_list.append(oof_test)\n            oof_cv_score_list.append(oof_cv_score)\n        \n    X_train_next = pd.DataFrame(np.concatenate(oof_train_list, axis=1))\n    X_test_next = pd.DataFrame(np.concatenate(oof_test_list, axis=1))\n    \n    print(X_train_next.shape, X_test_next.shape)\n    \n    return X_train_next, X_test_next, oof_cv_score_list\n\n\ndef load_data(nb_1km=True, nb_3km=True, nb_5km=True,\n              n_5_nb=True, n_10_nb=True, n_20_nb=True,\n              original=True, do_scale=False, fix_skew=False, do_ohe=True):\n    train = pd.read_csv('..\/input\/2019-2nd-ml-month-with-kakr\/train.csv')\n    test = pd.read_csv('..\/input\/2019-2nd-ml-month-with-kakr\/test.csv')\n\n    train_copy = train.copy()\n    train_copy['data'] = 'train'\n    test_copy = test.copy()\n    test_copy['data'] = 'test'\n    test_copy['price'] = np.nan\n    \n    # remove outlier\n    train_copy = train_copy[~((train_copy['sqft_living'] > 12000) & (train_copy['price'] < 3000000))].reset_index(drop=True)\n\n    # concat train, test data to preprocess\n    data = pd.concat([train_copy, test_copy]).reset_index(drop=True)\n    data = data[train_copy.columns]\n    \n    # fix skew feature\n    skew_columns = ['price']\n\n    for c in skew_columns:\n        data[c] = np.log1p(data[c])\n    \n    if original:\n        # feature engineering\n        data['date'] = pd.to_datetime(data['date'])\n        data['yr_mo_sold'] = data['date'].dt.strftime('%Y-%m')\n        data['yr_sold'] = data['date'].dt.year\n        data['qt_sold'] = data['date'].dt.quarter\n        data['week_sold'] = data['date'].dt.week\n        data['dow_sold'] = data['date'].dt.dayofweek\n        data['yr_sold - yr_built'] = data['yr_sold'] - data['yr_built']\n        data['yr_sold - yr_renovated'] = data['yr_sold'] - data['yr_renovated']\n        data['yr_renovated - yr_built'] = data['yr_renovated'] - data['yr_built']\n        \n        data['yr_sold'] = data['yr_sold'].astype(str)\n        data['qt_sold'] = data['qt_sold'].astype(str)\n        data['week_sold'] = data['week_sold'].astype(str)\n        data['dow_sold'] = data['dow_sold'].astype(str)\n        data.drop(['date'], axis=1, inplace=True)\n\n        data['bedrooms + bathrooms'] = data['bedrooms'] + data['bathrooms']\n        data['bathrooms \/ bedrooms'] = data['bathrooms'] \/ data['bedrooms']\n        data.loc[np.isinf(data['bathrooms \/ bedrooms']), 'bathrooms \/ bedrooms'] = 0\n        data['bathrooms \/ bedrooms'].fillna(0, inplace=True)\n        data['sqft_living \/ bedrooms'] = data['sqft_living'] \/ data['bedrooms']\n        data.loc[np.isinf(data['sqft_living \/ bedrooms']), 'sqft_living \/ bedrooms'] = 0\n        data['sqft_living \/ bathrooms'] = data['sqft_living'] \/ data['bathrooms']\n        data.loc[np.isinf(data['sqft_living \/ bathrooms']), 'sqft_living \/ bathrooms'] = 0\n        data['sqft_living \/ floors'] = data['sqft_living'] \/ data['floors']\n        data.loc[np.isinf(data['sqft_living \/ floors']), 'sqft_living \/ floors'] = 0\n        data['sqft_lot \/ sqft_living'] = data['sqft_lot'] \/ data['sqft_living']\n        data['sqft_basement \/ sqft_above'] = data['sqft_basement'] \/ data['sqft_above']\n        data['sqft_lot15 \/ sqft_living15'] = data['sqft_lot15'] \/ data['sqft_living15']\n        data['has_basement'] = data['sqft_basement'] > 0\n        data['is_renovated'] = data['yr_renovated'] > 0\n        data['sqft_living_changed'] = data['sqft_living'] != data['sqft_living15']\n        data['sqft_lot_changed'] = data['sqft_lot'] != data['sqft_lot15']\n        data['sqft_living * grade'] = data['sqft_living'] * data['grade']\n        data['overall'] = data['grade'] + data['view'] + data['condition'] + data['waterfront'] + data['has_basement'] + data['is_renovated']\n        data['sqft_living * overall'] = data['sqft_living'] * data['overall']\n\n        data['zipcode'] = data['zipcode'].astype(str)\n        data['zipcode-3'] = data['zipcode'].str[2:3]\n        data['zipcode-4'] = data['zipcode'].str[3:4]\n        data['zipcode-5'] = data['zipcode'].str[4:5]\n        data['zipcode-34'] = data['zipcode'].str[2:4]\n        data['zipcode-45'] = data['zipcode'].str[3:5]\n        data['zipcode-35'] = data['zipcode-3'] + data['zipcode-5']\n\n        # pca for lat, long\n        coord = data[['lat','long']]\n        pca = PCA(n_components=2)\n        pca.fit(coord)\n\n        coord_pca = pca.transform(coord)\n\n        data['coord_pca1'] = coord_pca[:, 0]\n        data['coord_pca2'] = coord_pca[:, 1]\n\n        # kmeans for lat, long\n        kmeans = KMeans(n_clusters=72, random_state=RANDOM_SEED).fit(coord)\n        coord_cluster = kmeans.predict(coord)\n        data['coord_cluster'] = coord_cluster\n        data['coord_cluster'] = data['coord_cluster'].map(lambda x: 'cluster_' + str(x).rjust(2, '0'))\n\n        lat_med = data['lat'].median()\n        long_med = data['long'].median()\n\n        lat2 = data['lat'].values\n        long2 = data['long'].values\n\n        bearing_arr = bearing_array(lat_med, long_med, lat2, long2)\n\n        data['bearing_from_center'] = bearing_arr\n\n        qcut_count = 10\n        data['qcut_bearing'] = pd.qcut(data['bearing_from_center'], qcut_count, labels=range(qcut_count))\n        data['qcut_bearing'] = data['qcut_bearing'].astype(str)\n\n        # calculate grouped price\n        group_cols = ['grade','bedrooms','bathrooms','view','condition','waterfront']\n\n        for col in group_cols:\n            group_df = groupby_helper(data[data['data'] == 'train'], col, 'price', ['mean']).fillna(0)\n            data = data.merge(group_df, on=col, how='left').fillna(0)\n            \n        cat_cols = [\n            'yr_mo_sold','yr_sold','qt_sold','week_sold','dow_sold','coord_cluster',\n            'zipcode','zipcode-3','zipcode-4','zipcode-5','zipcode-34','zipcode-45','zipcode-35',\n            'qcut_bearing'\n        ]\n            \n        if do_ohe:\n            for col in cat_cols:\n                ohe_df = pd.get_dummies(data[[col]], prefix='ohe_'+col)\n                data.drop(col, axis=1, inplace=True)\n                data = pd.concat([data, ohe_df], axis=1)\n        else:\n            for col in cat_cols:\n                le = LabelEncoder()\n                data[col] = le.fit_transform(data[col])\n    else:\n        data = data[['id','price','data']]\n        \n    if nb_1km:\n        neighbor_1km_stat = pd.read_csv('..\/input\/2019-2nd-ml-month-with-kakr-neighbor-stat\/neighbor_1km_stat.csv')\n        data = data.merge(neighbor_1km_stat, on='id', how='left').fillna(0)\n        \n        if original:\n            data['sqft_living - nb_1km_sqft_living_mean'] = data['sqft_living'] - data['nb_1km_sqft_living_mean']\n            data['sqft_lot - nb_1km_sqft_lot_mean'] = data['sqft_lot'] - data['nb_1km_sqft_lot_mean']\n            data['bedrooms - nb_1km_bedrooms_mean'] = data['bedrooms'] - data['nb_1km_bedrooms_mean']\n            data['bathrooms - nb_1km_bathrooms_mean'] = data['bathrooms'] - data['nb_1km_bathrooms_mean']\n            data['grade - nb_1km_grade_mean'] = data['grade'] - data['nb_1km_grade_mean']\n            data['view - nb_1km_view_mean'] = data['view'] - data['nb_1km_view_mean']\n            data['condition - nb_1km_condition_mean'] = data['condition'] - data['nb_1km_condition_mean']\n    if nb_3km:\n        neighbor_3km_stat = pd.read_csv('..\/input\/2019-2nd-ml-month-with-kakr-neighbor-stat\/neighbor_3km_stat.csv')\n        data = data.merge(neighbor_3km_stat, on='id', how='left').fillna(0)\n        \n        if original:\n            data['sqft_living - nb_3km_sqft_living_mean'] = data['sqft_living'] - data['nb_3km_sqft_living_mean']\n            data['sqft_lot - nb_3km_sqft_lot_mean'] = data['sqft_lot'] - data['nb_3km_sqft_lot_mean']\n            data['bedrooms - nb_3km_bedrooms_mean'] = data['bedrooms'] - data['nb_3km_bedrooms_mean']\n            data['bathrooms - nb_3km_bathrooms_mean'] = data['bathrooms'] - data['nb_3km_bathrooms_mean']\n            data['grade - nb_3km_grade_mean'] = data['grade'] - data['nb_3km_grade_mean']\n            data['view - nb_3km_view_mean'] = data['view'] - data['nb_3km_view_mean']\n            data['condition - nb_3km_condition_mean'] = data['condition'] - data['nb_3km_condition_mean']\n    if nb_5km:\n        neighbor_5km_stat = pd.read_csv('..\/input\/2019-2nd-ml-month-with-kakr-neighbor-stat\/neighbor_5km_stat.csv')\n        data = data.merge(neighbor_5km_stat, on='id', how='left').fillna(0)\n        \n        if original:\n            data['sqft_living - nb_5km_sqft_living_mean'] = data['sqft_living'] - data['nb_5km_sqft_living_mean']\n            data['sqft_lot - nb_5km_sqft_lot_mean'] = data['sqft_lot'] - data['nb_5km_sqft_lot_mean']\n            data['bedrooms - nb_5km_bedrooms_mean'] = data['bedrooms'] - data['nb_5km_bedrooms_mean']\n            data['bathrooms - nb_5km_bathrooms_mean'] = data['bathrooms'] - data['nb_5km_bathrooms_mean']\n            data['grade - nb_5km_grade_mean'] = data['grade'] - data['nb_5km_grade_mean']\n            data['view - nb_5km_view_mean'] = data['view'] - data['nb_5km_view_mean']\n            data['condition - nb_5km_condition_mean'] = data['condition'] - data['nb_5km_condition_mean']\n    if n_5_nb:\n        nearest_5_neighbor_stat = pd.read_csv('..\/input\/2019-2nd-ml-month-with-kakr-neighbor-stat\/nearest_5_neighbor_stat.csv')\n        data = data.merge(nearest_5_neighbor_stat, on='id', how='left').fillna(0)\n        \n        if original:\n            data['sqft_living - n_5_nb_sqft_living_mean'] = data['sqft_living'] - data['n_5_nb_sqft_living_mean']\n            data['sqft_lot - n_5_nb_sqft_lot_mean'] = data['sqft_lot'] - data['n_5_nb_sqft_lot_mean']\n            data['bedrooms - n_5_nb_bedrooms_mean'] = data['bedrooms'] - data['n_5_nb_bedrooms_mean']\n            data['bathrooms - n_5_nb_bathrooms_mean'] = data['bathrooms'] - data['n_5_nb_bathrooms_mean']\n            data['grade - n_5_nb_grade_mean'] = data['grade'] - data['n_5_nb_grade_mean']\n            data['view - n_5_nb_view_mean'] = data['view'] - data['n_5_nb_view_mean']\n            data['condition - n_5_nb_condition_mean'] = data['condition'] - data['n_5_nb_condition_mean']\n    if n_10_nb:\n        nearest_10_neighbor_stat = pd.read_csv('..\/input\/2019-2nd-ml-month-with-kakr-neighbor-stat\/nearest_10_neighbor_stat.csv')\n        data = data.merge(nearest_10_neighbor_stat, on='id', how='left').fillna(0)\n        \n        if original:\n            data['sqft_living - n_10_nb_sqft_living_mean'] = data['sqft_living'] - data['n_10_nb_sqft_living_mean']\n            data['sqft_lot - n_10_nb_sqft_lot_mean'] = data['sqft_lot'] - data['n_10_nb_sqft_lot_mean']\n            data['bedrooms - n_10_nb_bedrooms_mean'] = data['bedrooms'] - data['n_10_nb_bedrooms_mean']\n            data['bathrooms - n_10_nb_bathrooms_mean'] = data['bathrooms'] - data['n_10_nb_bathrooms_mean']\n            data['grade - n_10_nb_grade_mean'] = data['grade'] - data['n_10_nb_grade_mean']\n            data['view - n_10_nb_view_mean'] = data['view'] - data['n_10_nb_view_mean']\n            data['condition - n_10_nb_condition_mean'] = data['condition'] - data['n_10_nb_condition_mean']\n    if n_20_nb:\n        nearest_20_neighbor_stat = pd.read_csv('..\/input\/2019-2nd-ml-month-with-kakr-neighbor-stat\/nearest_20_neighbor_stat.csv')\n        data = data.merge(nearest_20_neighbor_stat, on='id', how='left').fillna(0)\n        \n        if original:\n            data['sqft_living - n_20_nb_sqft_living_mean'] = data['sqft_living'] - data['n_20_nb_sqft_living_mean']\n            data['sqft_lot - n_20_nb_sqft_lot_mean'] = data['sqft_lot'] - data['n_20_nb_sqft_lot_mean']\n            data['bedrooms - n_20_nb_bedrooms_mean'] = data['bedrooms'] - data['n_20_nb_bedrooms_mean']\n            data['bathrooms - n_20_nb_bathrooms_mean'] = data['bathrooms'] - data['n_20_nb_bathrooms_mean']\n            data['grade - n_20_nb_grade_mean'] = data['grade'] - data['n_20_nb_grade_mean']\n            data['view - n_20_nb_view_mean'] = data['view'] - data['n_20_nb_view_mean']\n            data['condition - n_20_nb_condition_mean'] = data['condition'] - data['n_20_nb_condition_mean']\n                \n    if fix_skew:\n        ordinal_cols = [\n            'id','price','data',\n            'grade','overall','view','condition','waterfront','is_renovated','has_basement',\n        ]\n        if do_ohe:\n            exclude_cols = [col for col in data.columns if 'ohe_' in col] + ordinal_cols\n        else:\n            exclude_cols = cat_cols + ordinal_cols\n        numeric_feats = [col for col in data.columns if col not in exclude_cols]\n        skewed_feats = data[numeric_feats].apply(lambda x : skew(x.dropna())).sort_values(ascending=False)\n        skewness = pd.DataFrame({'skew' :skewed_feats})\n        skewness = skewness[abs(skewness) > 0.75]\n        skewed_features = skewness.index\n        lam = 0.15\n        for feat in skewed_features:\n            data[feat] = boxcox1p(data[feat], lam)\n            data[feat] = data[feat].fillna(0)\n    \n    df = data.drop(['id','price','data'], axis=1).copy()\n\n    train_len = data[data['data'] == 'train'].shape[0]\n    X_train = df.iloc[:train_len]\n    X_test = df.iloc[train_len:]\n    y_train = data[data['data'] == 'train']['price']\n    \n    if do_scale:\n        if do_ohe:\n            non_numeric_cols = [col for col in X_train.columns if 'ohe_' in col]\n            numeric_cols = [col for col in X_train.columns if 'ohe_' not in col]\n        else:\n            non_numeric_cols = cat_cols\n            numeric_cols = [col for col in X_train.columns if col not in cat_cols]\n        X_train_rb = X_train[numeric_cols].copy()\n        X_test_rb = X_test[numeric_cols].copy()\n\n        rb = RobustScaler()\n        X_train_rb = rb.fit_transform(X_train_rb)\n        X_test_rb = rb.transform(X_test_rb)\n\n        X_train_rb = pd.DataFrame(X_train_rb, index=X_train.index, columns=X_train[numeric_cols].columns)\n        X_test_rb = pd.DataFrame(X_test_rb, index=X_test.index, columns=X_test[numeric_cols].columns)\n        \n        X_train_rb = pd.concat([X_train[non_numeric_cols], X_train_rb], axis=1)\n        X_test_rb = pd.concat([X_test[non_numeric_cols], X_test_rb], axis=1)\n        \n        print(X_train_rb.shape, X_test_rb.shape, y_train.shape)\n        \n        return X_train_rb, X_test_rb, y_train\n    else :\n        print(X_train.shape, X_test.shape, y_train.shape)\n        return X_train, X_test, y_train\n\ndef haversine_array(lat1, lng1, lat2, lng2): \n    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2)) \n    AVG_EARTH_RADIUS = 6371 # in km \n    lat = lat2 - lat1 \n    lng = lng2 - lng1 \n    d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2 \n    h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d)) \n    return h\n\ndef bearing_array(lat1, lng1, lat2, lng2): \n    AVG_EARTH_RADIUS = 6371 # in km \n    lng_delta_rad = np.radians(lng2 - lng1) \n    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2)) \n    y = np.sin(lng_delta_rad) * np.cos(lat2) \n    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad) \n    return np.degrees(np.arctan2(y, x))\n","d1593601":"warnings.filterwarnings('ignore')\n\n# Set a few plotting defaults\nplt.style.use('ggplot')\nplt.rcParams['figure.figsize'] = (12, 8)\nplt.rcParams['font.size'] = 12\n\npd.options.display.max_rows = 10000\npd.options.display.max_columns = 10000\npd.options.display.max_colwidth = 1000","4b0447dc":"data_list = []\n\nX_train_1km, X_test_1km, y_train = load_data(nb_1km=True, nb_3km=False, nb_5km=False,\n                                             n_5_nb=False, n_10_nb=False, n_20_nb=False,\n                                             original=False, do_scale=False, do_ohe=True)\ndata_list.append((X_train_1km, X_test_1km))\n\nX_train_3km, X_test_3km, y_train = load_data(nb_1km=False, nb_3km=True, nb_5km=False,\n                                             n_5_nb=False, n_10_nb=False, n_20_nb=False,\n                                             original=False, do_scale=False, do_ohe=True)\ndata_list.append((X_train_3km, X_test_3km))\n\nX_train_5km, X_test_5km, y_train = load_data(nb_1km=False, nb_3km=False, nb_5km=True,\n                                             n_5_nb=False, n_10_nb=False, n_20_nb=False,\n                                             original=False, do_scale=False, do_ohe=True)\ndata_list.append((X_train_5km, X_test_5km))\n\nX_train_5_nn, X_test_5_nn, y_train = load_data(nb_1km=False, nb_3km=False, nb_5km=False,\n                                               n_5_nb=True, n_10_nb=False, n_20_nb=False,\n                                               original=False, do_scale=False, do_ohe=True)\ndata_list.append((X_train_5_nn, X_test_5_nn))\n\nX_train_10_nn, X_test_10_nn, y_train = load_data(nb_1km=False, nb_3km=False, nb_5km=False,\n                                                 n_5_nb=False, n_10_nb=True, n_20_nb=False,\n                                                 original=False, do_scale=False, do_ohe=True)\ndata_list.append((X_train_10_nn, X_test_10_nn))\n\nX_train_20_nn, X_test_20_nn, y_train = load_data(nb_1km=False, nb_3km=False, nb_5km=False,\n                                                 n_5_nb=False, n_10_nb=False, n_20_nb=True,\n                                                 original=False, do_scale=False, do_ohe=True)\ndata_list.append((X_train_20_nn, X_test_20_nn))\n\nX_train_ori, X_test_ori, y_train = load_data(nb_1km=False, nb_3km=False, nb_5km=False,\n                                             n_5_nb=False, n_10_nb=False, n_20_nb=False,\n                                             original=True, do_scale=False, do_ohe=True)\ndata_list.append((X_train_ori, X_test_ori))\n\nX_train_full, X_test_full, y_train = load_data(nb_1km=True, nb_3km=True, nb_5km=True,\n                                               n_5_nb=True, n_10_nb=True, n_20_nb=True,\n                                               original=True, do_scale=False, do_ohe=True)\ndata_list.append((X_train_full, X_test_full))","3176b7ba":"run_flag = False\n\nif run_flag:\n    model_list = []\n\n    lgb_param = {\n        'objective': 'regression',\n        'learning_rate': 0.01,\n        'max_depth': 20,\n        'num_leaves': 63,\n        'min_data_in_leaf': 30,\n        'bagging_fraction': 0.7,\n        'bagging_freq': 1,\n        'feature_fraction': 0.2,\n        'seed': RANDOM_SEED,\n        'metric': ['rmse'],\n    }\n    lgb_model = LgbmWrapper(params=lgb_param, num_rounds=100000, ealry_stopping=200, verbose_eval=False)\n    model_list.append(lgb_model)\n\n    lgb_param2 = {\n        'objective': 'regression',\n        'learning_rate': 0.01,\n        'max_depth': 10,\n        'num_leaves': 31,\n        'min_data_in_leaf': 30,\n        'bagging_fraction': 0.7,\n        'bagging_freq': 1,\n        'feature_fraction': 0.2,\n        'seed': RANDOM_SEED,\n        'metric': ['rmse'],\n    }\n    lgb_model2 = LgbmWrapper(params=lgb_param2, num_rounds=100000, ealry_stopping=200, verbose_eval=False)\n    model_list.append(lgb_model2)\n\n    lgb_param3 = {\n        'objective': 'regression',\n        'learning_rate': 0.01,\n        'max_depth': 3,\n        'num_leaves': 7,\n        'min_data_in_leaf': 30,\n        'bagging_fraction': 0.7,\n        'bagging_freq': 1,\n        'feature_fraction': 0.2,\n        'seed': RANDOM_SEED,\n        'metric': ['rmse'],\n    }\n    lgb_model3 = LgbmWrapper(params=lgb_param3, num_rounds=100000, ealry_stopping=200, verbose_eval=False)\n    model_list.append(lgb_model3)\n\n    xgb_param = {\n        'eval_metric': 'rmse',\n        'seed': RANDOM_SEED,\n        'eta': 0.01,\n        'max_depth': 20,\n        'subsample': 0.7,\n        'colsample_bytree': 0.5,\n        'silent': 1,\n    }\n    xgb_model = XgbWrapper(params=xgb_param, num_rounds=100000, ealry_stopping=200, verbose_eval=False)\n    model_list.append(xgb_model)\n\n    xgb_param2 = {\n        'eval_metric': 'rmse',\n        'seed': RANDOM_SEED,\n        'eta': 0.01,\n        'max_depth': 10,\n        'subsample': 0.7,\n        'colsample_bytree': 0.5,\n        'silent': 1,\n    }\n    xgb_model2 = XgbWrapper(params=xgb_param2, num_rounds=100000, ealry_stopping=200, verbose_eval=False)\n    model_list.append(xgb_model2)\n\n    xgb_param3 = {\n        'eval_metric': 'rmse',\n        'seed': RANDOM_SEED,\n        'eta': 0.01,\n        'max_depth': 3,\n        'subsample': 0.7,\n        'colsample_bytree': 0.5,\n        'silent': 1,\n    }\n    xgb_model3 = XgbWrapper(params=xgb_param3, num_rounds=100000, ealry_stopping=200, verbose_eval=False)\n    model_list.append(xgb_model3)\n\n    rf_param = {\n        'n_estimators': 1000,\n        'max_depth': 20,\n        'max_features': 0.6,\n        'n_jobs': -1,\n        'random_state': RANDOM_SEED\n    }\n    rf_model = SklearnWrapper(RandomForestRegressor, params=rf_param)\n    model_list.append(rf_model)\n\n    rf_param2 = {\n        'n_estimators': 1000,\n        'max_depth': 3,\n        'max_features': 0.6,\n        'n_jobs': -1,\n        'random_state': RANDOM_SEED\n    }\n    rf_model2 = SklearnWrapper(RandomForestRegressor, params=rf_param2)\n    model_list.append(rf_model2)\n\n    ridge_param = {'alpha': 1e-3, 'normalize': True, 'max_iter': 1e7, 'random_state': RANDOM_SEED}\n    ridge_model = SklearnWrapper(Ridge, params=ridge_param)\n    model_list.append(ridge_model)\n\n    X_train_single, X_test_single, cv_score_single = stacking(data_list, y_train, model_list, eval_func=rmse_exp)","225e7008":"X_train_single = pd.read_csv('..\/input\/11th-solution-data-public-98316-private-99336\/x_train_single.csv')\nX_test_single = pd.read_csv('..\/input\/11th-solution-data-public-98316-private-99336\/x_test_single.csv')\ncv_score_single_df = pd.read_csv('..\/input\/11th-solution-data-public-98316-private-99336\/cv_score_single.csv')","422dfa0e":"cv_score_single_df","cc7b9e70":"cv_score_single_df.set_index('name').plot.bar(figsize=(16,8))\nplt.ylabel('Single Model CV Score');","43e30628":"X_train_single.shape, X_test_single.shape","7e28ed09":"X_train_stage1 = pd.concat([\n    X_train_single,\n    X_train_full[['nb_1km_price_mean','nb_3km_price_mean','nb_5km_price_mean',\n                  'n_5_nb_price_mean','n_10_nb_price_mean','n_20_nb_price_mean']]\n], axis=1)\n\nX_test_stage1 = pd.concat([\n    X_test_single,\n    X_test_full[['nb_1km_price_mean','nb_3km_price_mean','nb_5km_price_mean',\n                 'n_5_nb_price_mean','n_10_nb_price_mean','n_20_nb_price_mean']].reset_index(drop=True)\n], axis=1)\n\nX_train_stage1.shape, X_test_stage1.shape","adc6523e":"def create_model(input_dim):\n    model = Sequential()\n    model.add(Dense(64, activation='selu', input_dim=input_dim,\n                    kernel_initializer=initializers.he_normal(seed=RANDOM_SEED),\n                    bias_initializer=initializers.Constant(0.01)))\n    model.add(Dense(32, activation='selu', \n                    kernel_initializer=initializers.he_normal(seed=RANDOM_SEED),\n                    bias_initializer=initializers.Constant(0.01)))\n    model.add(Dense(16, activation='selu',\n                    kernel_initializer=initializers.he_normal(seed=RANDOM_SEED),\n                    bias_initializer=initializers.Constant(0.01)))\n    model.add(Dense(8, activation='selu',\n                    kernel_initializer=initializers.he_normal(seed=RANDOM_SEED),\n                    bias_initializer=initializers.Constant(0.01)))\n    model.add(Dense(1,\n                    kernel_initializer=initializers.he_normal(seed=RANDOM_SEED),\n                    bias_initializer=initializers.Constant(0.01)))\n\n\n    optimizer = optimizers.RMSprop(lr=0.001)\n    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mse'])\n    \n    return model\n\npatient = 200\n\ncallbacks = [\n    EarlyStopping(monitor='val_loss', patience=patient, mode='min', verbose=1),\n    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=patient\/2, min_lr=0.00001, verbose=1, mode='min')\n]","a126aacb":"if run_flag:\n    model_list_second = []\n\n    keras_model = KerasWrapper(create_model, epochs=100000, batch_size=512, callbacks=callbacks, use_avg_oof=True)\n    model_list_second.append(keras_model)\n\n    et_param = {\n        'n_estimators': 1000,\n        'max_depth': 20,\n        'max_features': 0.6,\n        'n_jobs': -1,\n        'random_state': RANDOM_SEED\n    }\n    et_model = SklearnWrapper(ExtraTreesRegressor, params=et_param)\n    model_list_second.append(et_model)\n\n    et_param2 = {\n        'n_estimators': 1000,\n        'max_depth': 3,\n        'max_features': 0.6,\n        'n_jobs': -1,\n        'random_state': RANDOM_SEED\n    }\n    et_model2 = SklearnWrapper(ExtraTreesRegressor, params=et_param2)\n    model_list_second.append(et_model2)\n\n    lgb_param = {\n        'objective': 'regression',\n        'learning_rate': 0.01,\n        'max_depth': 20,\n        'num_leaves': 63,\n        'min_data_in_leaf': 30,\n        'bagging_fraction': 0.7,\n        'bagging_freq': 1,\n        'feature_fraction': 0.2,\n        'seed': RANDOM_SEED,\n        'metric': ['rmse'],\n    }\n    lgb_model = LgbmWrapper(params=lgb_param, num_rounds=100000, ealry_stopping=200, verbose_eval=False)\n    model_list_second.append(lgb_model)\n\n    lgb_param2 = {\n        'objective': 'regression',\n        'learning_rate': 0.01,\n        'max_depth': 10,\n        'num_leaves': 31,\n        'min_data_in_leaf': 30,\n        'bagging_fraction': 0.7,\n        'bagging_freq': 1,\n        'feature_fraction': 0.2,\n        'seed': RANDOM_SEED,\n        'metric': ['rmse'],\n    }\n    lgb_model2 = LgbmWrapper(params=lgb_param2, num_rounds=100000, ealry_stopping=200, verbose_eval=False)\n    model_list_second.append(lgb_model2)\n\n    lgb_param3 = {\n        'objective': 'regression',\n        'learning_rate': 0.01,\n        'max_depth': 3,\n        'num_leaves': 7,\n        'min_data_in_leaf': 30,\n        'bagging_fraction': 0.7,\n        'bagging_freq': 1,\n        'feature_fraction': 0.2,\n        'seed': RANDOM_SEED,\n        'metric': ['rmse'],\n    }\n    lgb_model3 = LgbmWrapper(params=lgb_param3, num_rounds=100000, ealry_stopping=200, verbose_eval=False)\n    model_list_second.append(lgb_model3)\n\n    xgb_param = {\n        'eval_metric': 'rmse',\n        'seed': RANDOM_SEED,\n        'eta': 0.01,\n        'max_depth': 20,\n        'subsample': 0.7,\n        'colsample_bytree': 0.5,\n        'silent': 1,\n    }\n    xgb_model = XgbWrapper(params=xgb_param, num_rounds=100000, ealry_stopping=200, verbose_eval=False)\n    model_list_second.append(xgb_model)\n\n    xgb_param2 = {\n        'eval_metric': 'rmse',\n        'seed': RANDOM_SEED,\n        'eta': 0.01,\n        'max_depth': 10,\n        'subsample': 0.7,\n        'colsample_bytree': 0.5,\n        'silent': 1,\n    }\n    xgb_model2 = XgbWrapper(params=xgb_param2, num_rounds=100000, ealry_stopping=200, verbose_eval=False)\n    model_list_second.append(xgb_model2)\n\n    xgb_param3 = {\n        'eval_metric': 'rmse',\n        'seed': RANDOM_SEED,\n        'eta': 0.01,\n        'max_depth': 3,\n        'subsample': 0.7,\n        'colsample_bytree': 0.5,\n        'silent': 1,\n    }\n    xgb_model3 = XgbWrapper(params=xgb_param3, num_rounds=100000, ealry_stopping=200, verbose_eval=False)\n    model_list_second.append(xgb_model3)\n\n    rf_param = {\n        'n_estimators': 1000,\n        'max_depth': 20,\n        'max_features': 0.6,\n        'n_jobs': -1,\n        'random_state': RANDOM_SEED\n    }\n    rf_model = SklearnWrapper(RandomForestRegressor, params=rf_param)\n    model_list_second.append(rf_model)\n\n    rf_param2 = {\n        'n_estimators': 1000,\n        'max_depth': 3,\n        'max_features': 0.6,\n        'n_jobs': -1,\n        'random_state': RANDOM_SEED\n    }\n    rf_model2 = SklearnWrapper(RandomForestRegressor, params=rf_param2)\n    model_list_second.append(rf_model2)\n\n    ridge_param = {'alpha': 1e-10, 'normalize': True, 'max_iter': 1e7, 'random_state': RANDOM_SEED}\n    ridge_model = SklearnWrapper(Ridge, params=ridge_param)\n    model_list_second.append(ridge_model)\n\n    gbr_param = {\n        'n_estimators': 1000,\n        'learning_rate':0.1,\n        'max_depth': 20,\n        'subsample': 0.7,\n        'max_features': 0.6,\n        'random_state': RANDOM_SEED\n    }\n    gbr_model = SklearnWrapper(GradientBoostingRegressor, params=gbr_param)\n    model_list_second.append(gbr_model)\n\n    gbr_param2 = {\n        'n_estimators': 1000,\n        'learning_rate':0.1,\n        'max_depth': 10,\n        'subsample': 0.7,\n        'max_features': 0.6,\n        'random_state': RANDOM_SEED\n    }\n    gbr_model2 = SklearnWrapper(GradientBoostingRegressor, params=gbr_param2)\n    model_list_second.append(gbr_model2)\n\n    gbr_param3 = {\n        'n_estimators': 1000,\n        'learning_rate':0.1,\n        'max_depth': 2,\n        'subsample': 0.7,\n        'max_features': 0.6,\n        'random_state': RANDOM_SEED\n    }\n    gbr_model3 = SklearnWrapper(GradientBoostingRegressor, params=gbr_param3)\n    model_list_second.append(gbr_model3)\n\n    lasso_param = {'alpha':1e-6, 'normalize':True, 'max_iter':1e7, 'random_state':RANDOM_SEED}\n    lasso_model = SklearnWrapper(Lasso, params=lasso_param)\n    model_list_second.append(lasso_model)\n\n    elastic_param = {'alpha':1e-6, 'normalize':True, 'max_iter':1e5, 'random_state':RANDOM_SEED, 'l1_ratio':0.8}\n    elastic_model = SklearnWrapper(ElasticNet, params=elastic_param)\n    model_list_second.append(elastic_model)\n\n    svr_param = {'C':1e3, 'epsilon':0.001, 'gamma':1e-4}\n    svr_model = SklearnWrapper(SVR, params=svr_param)\n    model_list_second.append(svr_model)","6049284d":"if run_flag:\n    data_list_stage1 = [(X_train_stage1, X_test_stage1)]\n    X_train_stage2, X_test_stage2, cv_score_stage1 = stacking(data_list_stage1, y_train, model_list_second,\n                                                              eval_func=rmse_exp)","9e5c2e4f":"X_train_stage2 = pd.read_csv('..\/input\/11th-solution-data-public-98316-private-99336\/x_train_stage2.csv')\nX_test_stage2 = pd.read_csv('..\/input\/11th-solution-data-public-98316-private-99336\/x_test_stage2.csv')\ncv_score_stage1_df = pd.read_csv('..\/input\/11th-solution-data-public-98316-private-99336\/cv_score_stage1.csv')","dbabb35c":"cv_score_stage1_df","e838ff09":"cv_score_stage1_df.set_index('name').plot.bar(figsize=(16,8))\nplt.ylabel('Stage 1 CV Score');","102d8731":"ridge_l3_param = {\n    'alpha': 1e-10,\n    'normalize': True,\n    'max_iter': 1e7,\n    'random_state': RANDOM_SEED\n}\n\nridge_l3_model = SklearnWrapper(Ridge, params=ridge_l3_param, use_avg_oof=True)\n\nridge_l3_train, ridge_l3_test, ridge_l3_cv_score = get_oof(ridge_l3_model, X_train_stage2, y_train, X_test_stage2,\n                                                           rmse_exp, NFOLDS=5, kfold_random_state=RANDOM_SEED)","fdae5c51":"lasso_l3_param = {'alpha':1e-7, 'normalize':True, 'max_iter':1e7, 'random_state':RANDOM_SEED}\n\nlasso_l3_model = SklearnWrapper(Lasso, params=lasso_l3_param, use_avg_oof=True)\nlasso_l3_train, lasso_l3_test, lasso_l3_cv_score = get_oof(lasso_l3_model, X_train_stage2, y_train, X_test_stage2,\n                                                           rmse_exp, NFOLDS=5, kfold_random_state=RANDOM_SEED)","0da98d12":"elastic_l3_param = {'alpha':1e-8, 'normalize':True, 'max_iter':1e6, 'random_state':RANDOM_SEED, 'l1_ratio':0.8}\n\nelastic_l3_model = SklearnWrapper(ElasticNet, params=elastic_l3_param, use_avg_oof=True)\nelastic_l3_train, elastic_l3_test, elastic_l3_cv_score = get_oof(elastic_l3_model, X_train_stage2, y_train, X_test_stage2,\n                                                                 rmse_exp, NFOLDS=5, kfold_random_state=RANDOM_SEED)","e3853073":"svr_l3_param = {'C':1e3, 'epsilon':0.001, 'gamma':1e-4}\n\nsvr_l3_model = SklearnWrapper(SVR, params=svr_l3_param, use_avg_oof=True)\nsvr_l3_train, svr_l3_test, svr_l3_cv_score = get_oof(svr_l3_model, X_train_stage2, y_train, X_test_stage2,\n                                                     rmse_exp, NFOLDS=5, kfold_random_state=RANDOM_SEED)","4ecac74e":"cv_score_stage2_df = pd.read_csv('..\/input\/11th-solution-data-public-98316-private-99336\/cv_score_stage2.csv')\ncv_score_stage2_df","a58d7089":"cv_score_stage2_df.set_index('name').plot.bar(figsize=(16,8))\nplt.ylabel('Stage 2 CV Score');","fd7feefd":"# public score: 98316.65734, private score: 99336.33652\n\navg_pred = elastic_l3_test\n\ntest = pd.read_csv('..\/input\/2019-2nd-ml-month-with-kakr\/test.csv')\n\noutput = f'stacking_{datetime.now().strftime(\"%Y%m%d%H%M%S\")}.csv'\nprint(output)\n\nsubmission = pd.DataFrame({'id': test['id'], 'price': np.expm1(avg_pred.ravel())})\nsubmission.to_csv(output, index=False)","c13aff53":"# Kaggle Korea 2nd Competition - House Price Prediction\n\uc548\ub155\ud558\uc138\uc694. \uc57d \ud55c\ub2ec\uac04 2\ud68c \ub300\ud68c\uc5d0 \ubab0\uc785\ud574\uc11c \ub9ce\uc740 \uac83\uc744 \ubc30\uc6b0\uace0 \uc990\uacbc\uc2b5\ub2c8\ub2e4. \uc774\ub7f0 \uc88b\uc740 \uae30\ud68c\ub97c \ub9c8\ub828\ud574\uc8fc\uc2e0 \uce90\uae00 \ucf54\ub9ac\uc544 \uc6b4\uc601\uc9c4\uc5d0\uac8c \uc9c4\uc2ec\uc73c\ub85c \uac10\uc0ac\ub4dc\ub9bd\ub2c8\ub2e4.\n\n\uc800\ub294 \ucd5c\uc885 \uc2a4\ucf54\uc5b4 \ud504\ub77c\uc774\ube57 \ub9ac\ub354\ubcf4\ub4dc \uae30\uc900\uc73c\ub85c 99336.33652\uc774 \ub098\uc654\uace0 \uc21c\uc704\ub294 11\uc704\ub97c \ud588\ub294\ub370\uc694.\n\n\ucee4\ub110 \uacf5\uac1c \uc758\ubb34\uc5d0 \ub530\ub77c \uc81c \uc194\ub8e8\uc158\uc744 \uacf5\uc720\ud569\ub2c8\ub2e4.\n\n\uc790\uc138\ud55c \uc124\uba85\uc744 \uc4f0\uc9c0\ub294 \ubabb\ud588\ub294\ub370, \ud639\uc2dc \uad81\uae08\ud55c \uc810 \ubb38\uc758\uc8fc\uc2dc\uba74 \ub2f5\ubcc0 \ub4dc\ub9ac\uaca0\uc2b5\ub2c8\ub2e4. \n\n\ubcf4\uc2dc\uace0 \ub3c4\uc6c0\uc774 \ub410\ub2e4\uba74 \uc81c Kernel\uc5d0 Vote \ud574\uc8fc\uc2dc\uba74 \uac10\uc0ac\ud558\uaca0\uc2b5\ub2c8\ub2e4!\n\n# Lessons Learned\n* Regression \ubaa8\ub378 \uc2e4\uc804 \uccab \uacbd\ud5d8\n* Geo Data Feature Engineering : https:\/\/www.kaggle.com\/tmheo74\/geo-data-eda-and-feature-engineering\n* Stacking Ensemble\ub85c \uc720\uc758\ubbf8\ud55c \ubaa8\ub378 \uc131\ub2a5 \ud5a5\uc0c1\n* Deep Learning \uc2e4\uc804 \uc801\uc6a9\n\n# Feature Engineering\n* House Feature Engineering : grade, sqft_living \ub4f1\n* Date Feature Engineering\n* Zipcode Feature Engineering\n* Lat, Long Feature Engineering\n  * PCA Transformation\n  * K-Means Clustering\n  * Haversine Distance\n    * \ub098\uc640 \uac00\uae4c\uc6b4 \uac70\ub9ac\uc5d0 \uc788\ub294 \uc774\uc6c3\uc9d1\uc758 Feature \uc0dd\uc131 : 1km, 3km, 5km\n    * \ub098\uc640 \uac00\uae4c\uc6b4 \uac70\ub9ac\uc5d0 \uc788\uc73c\uba74\uc11c \ud2b9\uc131\uc774 \ube44\uc2b7\ud55c \uc774\uc6c3\uc9d1\uc758 Feature \uc0dd\uc131 : \ub098\uc640 \uac00\uc7a5 \ud2b9\uc131\uc774 \ube44\uc2b7\ud55c \uc774\uc6c3\uc9d1 5\uac1c, 10\uac1c, 20\uac1c\n  * Bearing\n    * Lat, Long\uc758 Median \uae30\uc900\uc73c\ub85c 10\uac1c quantile cut\ud574\uc11c \ubc29\ud5a5\uc5d0 \ub530\ub978 Feature \uc0dd\uc131\n\n# Stacking In Practice\n* Reference\n  * Kaggle Kernel\n    * \uae40\uc5f0\ubbfc\ub2d8\uc758 [Default EDA - Stacking Introduction](https:\/\/www.kaggle.com\/yeonmin\/default-eda-stacking-introduction)\n  * Article\n    * [Stacking Made Easy: An Introduction to StackNet by Competitions Grandmaster Marios Michailidis \\(KazAnova\\)](http:\/\/blog.kaggle.com\/2017\/06\/15\/stacking-made-easy-an-introduction-to-stacknet-by-competitions-grandmaster-marios-michailidis-kazanova\/)\n* Inspiration\n> As a quick note, one should try a few diverse models. To my experience, a good stacking solution is often composed of at least:\n> - 2 or 3 GBMs (one with low depth, one with medium and one with high)\n> - 1 or 2 Random Forests (again as diverse as possible\u2013one low depth, one high)\n> - 1 or 2 NNs (one deeper, one smaller)\n> - 1 linear model","d690ac67":"\uc544\ub798\ub294 Feature Engineering\uacfc Stacking\uc5d0 \uc0ac\uc6a9\ud55c \uc804\uccb4 \ucf54\ub4dc\uc785\ub2c8\ub2e4. Folding \ud574\ub1a8\uc73c\ub2c8 \ucf54\ub4dc\ub97c \ubcf4\uae30 \uc6d0\ud558\uc2dc\uba74 \uc624\ub978\ucabd \uc544\ub798\uc758 Code \ubc84\ud2bc\uc744 \ud074\ub9ad\ud574\uc8fc\uc138\uc694.","cbd338d5":"# Datasets\n\uc804\uccb4 \ucf54\ub4dc\ub97c \uc2e4\ud589\ud558\ub294\ub370 \ub108\ubb34 \uc2dc\uac04\uc774 \uac78\ub824\uc11c Neighbor Feature\ub97c \uc0dd\uc131\ud558\ub294 Kernel\uc740 \ubcc4\ub3c4\ub85c \ub9cc\ub4e4\uc5c8\uace0, Stacking Layer \uc2e4\ud589\uc740 \ub85c\uceec\uc5d0\uc11c \ub3cc\ub824\uc11c \uacb0\uacfc \uc0dd\uc131\ud588\uc2b5\ub2c8\ub2e4.\nNeighbor Feature Data\uc640 Stacking Layer \uc2e4\ud589 \uacb0\uacfc\ub294 Public Datasets\uc73c\ub85c \ub9cc\ub4e4\uc5c8\uc2b5\ub2c8\ub2e4.\n\uc544\ub798\ub294 Neighbor Feature Kernel, Datasets, Stacking Layer \uacb0\uacfc Datasets \ub9c1\ud06c\uc785\ub2c8\ub2e4.\n* Neighbor Feature\n  * Kernel\n    * https:\/\/www.kaggle.com\/tmheo74\/generate-neighbor-data\n    * https:\/\/www.kaggle.com\/tmheo74\/generate-neighbor-stat\n  * Datasets\n    * https:\/\/www.kaggle.com\/tmheo74\/2019-2nd-ml-month-with-kakr-neighbor-info\n    * https:\/\/www.kaggle.com\/tmheo74\/2019-2nd-ml-month-with-kakr-neighbor-stat\n* Stacking Layer\n  * Datasets : https:\/\/www.kaggle.com\/tmheo74\/11th-solution-data-public-98316-private-99336","679d122d":"# GitHub Link\n\uc804\uccb4 \ucf54\ub4dc\ub294 \uc544\ub798 GitHub\uc5d0\ub3c4 \uc62c\ub838\uc2b5\ub2c8\ub2e4.\n\n* https:\/\/github.com\/tmheo\/2019-2nd-ML-month-with-KaKR-Solution","da5afbd5":"\uc544\ub798 \uc774\ubbf8\uc9c0 \ucd9c\ucc98\ub294 \uc791\ub144 DataBreak 2018 \ud589\uc0ac\uc5d0\uc11c \uc774\uc815\uc724\ub2d8\uc758 \ubc1c\ud45c \uc2ac\ub77c\uc774\ub4dc\uc5d0\uc11c \ubc1c\ucdcc\ud588\uc2b5\ub2c8\ub2e4. Stacking Layer\uc758 \uacbd\uc6b0 \uc81c\uac00 \uad6c\uc131\ud55c Stacking\uc5d0 \ub9de\uac8c \ub2e4\uc2dc \uadf8\ub838\uc2b5\ub2c8\ub2e4.\n* \uc6d0\ubcf8 \uc790\ub8cc \ub9c1\ud06c : https:\/\/github.com\/KaggleBreak\/databreak\/raw\/master\/2018\/Keynote\/%EC%9D%B4%EC%A0%95%EC%9C%A4%EB%8B%98_Mastering%20ML%20with%20Competitions.pdf","40cac420":"* Stacking Layer\n  * First Layer\n    * 8 Feature Sets\n      * Neighbor 1km\n      * Neighbor 3km\n      * Neighbor 5km\n      * Nearest Neighbor 5\n      * Nearest Neighbor 10\n      * Nearest Neighbor 20\n      * Original Feature\n      * Full Feature : Neighbor 1km, 3km, 5km, Nearest Neighbor 5, 10, 20, Original Feature\n    * 9 Single Models\n      * LightGBM High, Medium, Low Depth\n      * XGBoost High, Medium, Low Depth\n      * RandomForest High, Low Depth\n      * Ridge\n  * Second Layer\n    * 18 Models\n      * Neural Net\n      * ExtraTree High, Low Depth\n      * LightGBM High, Medium, Low Depth\n      * XGBoost High, Medium, Low Depth\n      * RandomForest High, Low Depth\n      * Ridge\n      * Gradient Boosting Tree High, Medium, Low Depth\n      * Lasso\n      * ElasticNet\n      * SVM\n  * Third Layer\n    * 1 Model\n      * ElasticNet"}}