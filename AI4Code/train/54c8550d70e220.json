{"cell_type":{"ac11ab27":"code","d7f7a7e2":"code","1b85a06d":"code","f465c37b":"code","f504b903":"code","784dd955":"code","4a52cec7":"code","0d9d8624":"code","43df9ce6":"code","bd61e075":"code","b59ae31b":"code","9aed3cd7":"code","d5217b4a":"code","3c0baa1a":"code","9e52bf77":"code","a751bd9d":"code","b3dab6dc":"code","9838db7a":"code","7f145e67":"code","ef71afc6":"code","e7001b17":"markdown","da6465b6":"markdown","a6d04f62":"markdown","2b22b528":"markdown","1a7a5712":"markdown","ac37bea1":"markdown","08817e05":"markdown","1b712348":"markdown","9e0cd2d8":"markdown"},"source":{"ac11ab27":"!apt install aptitude -y\n!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n!pip install mecab-python3==0.996.6rc2\n\n!pip install unidic-lite\n!pip install neologdn\n\n#!git clone --depth 1 https:\/\/github.com\/neologd\/mecab-ipadic-neologd.git\n#!echo yes | mecab-ipadic-neologd\/bin\/install-mecab-ipadic-neologd -n -a","d7f7a7e2":"import re\nimport gc\nimport os\nimport json\nimport pickle\nimport random\n\nimport pandas as pd\nimport numpy as np\n\nimport tqdm\nfrom matplotlib import pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nfrom torch.nn import functional as F\n\nimport MeCab\nfrom transformers import *\nimport neologdn\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndevice = torch.device('cuda')\nm = MeCab.Tagger (\"-Ochasen\")\n#cmd = 'echo `mecab-config --dicdir`\"\/mecab-ipadic-neologd\"'\n#path = (subprocess.Popen(cmd, stdout=subprocess.PIPE,\n#                           shell=True).communicate()[0]).decode('utf-8')\n#m = MeCab.Tagger(\"-d {0}\".format(path))","1b85a06d":"!ls \/kaggle\/input\/d\/takamichitoda\/aio-my-prediction","f465c37b":"class config:\n    ENTITY_PATH = \"\/kaggle\/input\/k\/takamichitoda\/aio-final-submission-dataset\/\"\n    MODEL_DIR = \"\/kaggle\/input\/d\/takamichitoda\/aio-my-prediction\"\n    MODELS = [\n        #\"model_2.bin\",  # LB BEST\n        #\"model_2_lb_hard_best.bin\",\n        \"model_2.bin\",\n        \"model_2_very_hard_best_prev.bin\",\n        \"model_2_dev1_best.bin\",\n        \"model_2_dev2_best.bin\",\n        #\"model_2_dev_avg_best.bin\",\n    ]\n    SEED = 0\n    MODEL_TYPE = \"cl-tohoku\/bert-base-japanese-whole-word-masking\"\n    TOKENIZER = BertJapaneseTokenizer.from_pretrained(MODEL_TYPE)\n    MAX_LEN = 512\n    N_TTA = 3","f504b903":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","784dd955":"!wget --no-check-certificate https:\/\/www.nlp.ecei.tohoku.ac.jp\/projects\/AIP-LB\/static\/aio_leaderboard.json","4a52cec7":"df_test = pd.read_json(\"aio_leaderboard.json\", orient='records', lines=True)\nprint(df_test.shape)\ndf_test.head(3)","0d9d8624":"lst = []\nfor answer_candidates in df_test[\"answer_candidates\"]:\n    lst += answer_candidates\nuse_candidates = list(set(lst))\nuse_candidates_dict = {c: None for c in use_candidates}\nlen(use_candidates_dict)","43df9ce6":"entities_dict = {}\nwith open(f\"{config.ENTITY_PATH}\/all_entities.json\", \"r\") as f:\n    for line in tqdm.tqdm_notebook(f, total=920172):\n        d = json.loads(line)\n        title = d[\"title\"]\n        text = d[\"text\"]\n        try:\n            use_candidates_dict[title]\n        except KeyError:\n            continue\n        entities_dict[title] = text\n        \nlen(entities_dict)","bd61e075":"del use_candidates, use_candidates_dict\ngc.collect()","b59ae31b":"in_q_lst = []\nfor _, row in df_test.iterrows():\n    question = row[\"question\"]\n    answer_candidates = row[\"answer_candidates\"]\n    in_q = [c in question for c in answer_candidates]\n    in_q_lst.append(in_q)","9aed3cd7":"class BertForAIO(nn.Module):\n    def __init__(self):\n        super(BertForAIO, self).__init__()\n\n        bert_conf = BertConfig(config.MODEL_TYPE)\n        bert_conf.output_hidden_states = True\n        bert_conf.vocab_size = config.TOKENIZER.vocab_size\n\n        self.n_use_layer = 1\n        self.dropout_sample = 5\n\n        self.bert = AutoModel.from_pretrained(config.MODEL_TYPE, config=bert_conf)\n        \n        self.dropout = nn.Dropout(0.2)\n        n_weights = bert_conf.num_hidden_layers + 1\n        weights_init = torch.zeros(n_weights).float()\n        weights_init.data[:-1] = -3\n        self.layer_weights = torch.nn.Parameter(weights_init)\n        \n        self.dense1 = nn.Linear(bert_conf.hidden_size*self.n_use_layer, bert_conf.hidden_size*self.n_use_layer)\n        self.dense2 = nn.Linear(bert_conf.hidden_size*self.n_use_layer, bert_conf.hidden_size*self.n_use_layer)\n        \n        self.dropouts = nn.ModuleList([nn.Dropout(0.2) for _ in range(self.dropout_sample)])\n    \n        self.fc = nn.Linear(bert_conf.hidden_size*self.n_use_layer, 1)\n\n\n    def forward(self, ids, mask, token_type_ids):\n        n_choice = ids.shape[1]\n        \n        ids = ids.view(-1, ids.size(-1))\n        mask = mask.view(-1, mask.size(-1))\n        token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1))\n\n        _, _, h = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n\n        cat_output = torch.stack([self.dropout(layer[:, 0, :]) for layer in h], dim=2)\n        cat_output = (torch.softmax(self.layer_weights, dim=0) * cat_output).sum(-1)\n\n        cat_output = self.dense1(cat_output)\n        cat_output = nn.ReLU()(cat_output)\n        cat_output = self.dense2(cat_output)\n        cat_output = nn.ReLU()(cat_output)\n\n        logits = sum([self.fc(dropout(cat_output)) for dropout in self.dropouts])\/self.dropout_sample\n\n        logits = logits.view(-1, n_choice)\n\n        return logits","d5217b4a":"models = []\nfor _m in config.MODELS:\n    print(_m)\n    model = BertForAIO()\n    model.load_state_dict(torch.load(f\"{config.MODEL_DIR}\/{_m}\", map_location=torch.device('cpu')))\n    model.to(device)\n    model.eval()\n    models.append(model)\n    del model\n    gc.collect()\n\nprint(\"*** model load complete ***\")","3c0baa1a":"with open(\"\/kaggle\/input\/aio-stacking\/stacking_lr_model.pkl\", \"rb\") as f:\n    lr_models = pickle.load(f)","9e52bf77":"def extract_header(title, text):\n    lines = text.split(\"\u3002\")\n    exp = \"\"\n    lst = []\n    for line in lines:\n        if \"^\" in line[:3]:\n            break\n        if line == \"\":\n            continue\n        lst.append(line)\n    for line in lst:\n        sp_line = line.split(\"\u306f\u3001\")\n        if len(sp_line) > 1:\n            exp = sp_line[1]\n            break\n    if exp == \"\":\n        exp = lines[0]\n    return f\"{title}\u306f\u3001{exp}\", \"\u3002\".join(lst[1:])\n\ndef wakati_mecab(text):\n    wakati = [w.split(\"\\t\") for w in m.parse (text).split(\"\\n\")[:-2]]\n    return [w[0] for w in wakati if w[3].split(\"-\")[0] in [\"\u540d\u8a5e\", \"\u52d5\u8a5e\", \"\u5f62\u5bb9\u8a5e\"]]\n\ndef split_contents(contents, n_char=64, duplicate=5):\n    n_roop = len(contents) #\/\/n_char\n    lst = []\n    head_i = 0\n    for idx in range(n_roop):\n        tail_i = head_i + n_char\n        line = contents[head_i:tail_i]\n        if len(line) == 0:\n            break\n        lst.append(line)\n        head_i += n_char - duplicate\n    return lst\n\ndef extract_kakko(text):\n    re_text1 = re.findall(r\"\u300c.*?\u300d\", text)\n    re_text2 = re.findall(r\"\u300e.*?\u300f\", text)\n    re_text1 = [t.replace(\"\u300c\", \"\").replace(\"\u300d\", \"\") for t in re_text1]\n    re_text2 = [t.replace(\"\u300e\", \"\").replace(\"\u300f\", \"\") for t in re_text2]\n    return re_text1 + re_text2\n\ndef extract_key_word_line(contents_sp, key_words, is_tta=False):\n    lst1, lst2 = [], []\n    for line in contents_sp:\n        for w in key_words:\n            if w in line:\n                lst1.append(line)\n        if line not in lst1:\n            lst2.append(line)\n    lst1 = list(set(lst1))\n    if is_tta:        \n        random.shuffle(lst1)\n    return \"\u3002\".join(lst1), lst2\n\n\ndef extract_new_contents(title, question, is_tta=False):\n    contents = entities_dict[title]\n\n    header, contents_tail = extract_header(title, contents)\n    contents_sp = split_contents(contents_tail)\n\n    key_words = extract_kakko(question)\n    key_lines, contents_sp = extract_key_word_line(contents_sp, key_words, is_tta)\n\n    q_set = set(wakati_mecab(question))\n    c_sets = [set(wakati_mecab(c)) for c in contents_sp]\n\n    n_common = [len(c & q_set) for c in c_sets]\n    idx = np.array(n_common).argsort()[::-1]\n\n    new_contents = key_lines + \"\u3002\".join(np.array(contents_sp)[idx].tolist())\n    return f\"{header}[SEP]{new_contents}\"\n\ndef process_but(text):\n    sp_q = text.split(\"\u3067\u3059\u304c\u3001\")\n    tail_text = sp_q[-1]\n    sp_head = sp_q[0].split(\"\u3001\")\n    if len(sp_head) > 1:\n        text = sp_head[0] + \"\u3001\" + tail_text\n    else:\n        text = tail_text\n    return text","a751bd9d":"def tokenize_text(question, answer_candidates, is_tta=False):\n    if \"\u3067\u3059\u304c\u3001\" in question:\n        question = process_but(question)\n        \n    _ids, _mask, _id_type = [], [], []\n    for candidate in answer_candidates:\n        contents = extract_new_contents(candidate, question, is_tta)\n        \n        tok = config.TOKENIZER.encode_plus(contents,\n                                           question,\n                                           add_special_tokens=True,\n                                           max_length=config.MAX_LEN,\n                                           truncation_strategy=\"only_first\",\n                                           pad_to_max_length=True)\n        _ids.append(tok[\"input_ids\"])\n        _mask.append(tok[\"attention_mask\"])\n        _id_type.append(tok[\"token_type_ids\"])\n    \n    d = {\n        \"input_ids\": torch.tensor(_ids),\n        \"attention_mask\": torch.tensor(_mask),\n        \"token_type_ids\": torch.tensor(_id_type),\n    }\n    return d","b3dab6dc":"seed_everything(config.SEED)\n\npredicts = []\ntk = tqdm.tqdm_notebook(enumerate(df_test[[\"qid\", \"question\", \"answer_candidates\"]].values), total=len(df_test))\nfor idx, (qid, question, answer_candidates) in tk:\n    in_q = np.array(in_q_lst[idx]).astype(int)\n    \n    input_ids, attention_mask, token_type_ids = [], [], []\n    for _ in range(config.N_TTA):\n        d = tokenize_text(question, answer_candidates, is_tta=True)\n        _input_ids = d[\"input_ids\"].to(device)\n        _attention_mask = d[\"attention_mask\"].to(device)\n        _token_type_ids = d[\"token_type_ids\"].to(device)\n        input_ids.append(_input_ids)\n        attention_mask.append(_attention_mask)\n        token_type_ids.append(_token_type_ids)\n    input_ids = torch.stack(input_ids)\n    attention_mask = torch.stack(attention_mask)\n    token_type_ids = torch.stack(token_type_ids)\n    \n    all_preds = []\n    for model in models:\n        with torch.no_grad():\n            preds = model(input_ids, attention_mask, token_type_ids)\n        preds = preds.cpu().detach()\n        all_preds.append(preds)\n    all_preds = np.stack(all_preds)\n\n    #pred = all_preds.max(0).max(0) # (ensemble, TTA, label)\n    \n    # Stacking\n    lr_preds = []\n    for lr_model in lr_models:\n        lr_pred = lr_model.predict_proba(all_preds.max(1).T)[:, 1]\n        lr_preds.append(lr_pred)\n    pred = np.array(lr_preds).mean(0)\n\n    pred = pred + (-99999 * in_q)\n    \n    label_idx = pred.argmax(0)   \n    label = answer_candidates[label_idx]\n    \n    hyp = {\"qid\": qid,\"answer_entity\": label}\n    predicts.append(hyp)","9838db7a":"df_predicts = pd.DataFrame(predicts)\ndf_predicts.to_json(f'test_predict.jsonl', orient='records', force_ascii=False, lines=True)\nprint(df_predicts.shape)\ndf_predicts.head(3)","7f145e67":"!head \"test_predict.jsonl\"","ef71afc6":"!ls","e7001b17":"##### \u30e1\u30e2\u30ea\u524a\u6e1b","da6465b6":"# AI\u738b \u6700\u7d42\u63d0\u51fa\u7528\n## \u63a8\u8ad6\n### \u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u8aad\u307f\u8fbc\u307f","a6d04f62":"### \u30c7\u30fc\u30bf\u8aad\u307f\u8fbc\u307f","2b22b528":"### \u30e2\u30c7\u30eb","1a7a5712":"#### \u6700\u7d42\u554f\u984c(\u73fe\u72b6\u516c\u958b\u554f\u984c)","ac37bea1":"#### \u5019\u88dcWiki\u30c7\u30fc\u30bf","08817e05":"### \u30d1\u30e9\u30e1\u30fc\u30bf","1b712348":"Utility","9e0cd2d8":"##### \u554f\u984c\u6587\u306b\u3042\u308b\u5019\u88dc\u3092\u62bd\u51fa"}}