{"cell_type":{"889d8aa3":"code","6112655a":"code","18c4b839":"code","4a5f45e5":"code","54fcc51c":"code","0f289cf3":"code","a53f7c3f":"code","43ce30b1":"code","6e0a978c":"code","5149e168":"code","7321c8ed":"code","d7db9e6e":"code","415e62db":"code","fac474fd":"code","f2dd0b1a":"code","c368eba2":"code","e4298af3":"code","b2f634b3":"code","daf28f4f":"code","34f36c5c":"code","bc3b27c7":"code","cabdce02":"code","2b989988":"code","49233c1b":"code","12933deb":"code","bd00123b":"code","4d58ff74":"code","6534447f":"code","a15a8c15":"markdown","4011caf3":"markdown","e7b022ed":"markdown","a71e1f7b":"markdown","1ffd3085":"markdown","a4ab54cb":"markdown","19fc01c5":"markdown","366b51a2":"markdown","202e8bab":"markdown","c187b101":"markdown","b162f7cf":"markdown","8f25c8fe":"markdown","8685ed80":"markdown","5d5e3f94":"markdown","57c453d3":"markdown","ccb02959":"markdown","8cc91486":"markdown","eda97880":"markdown","fb8621a4":"markdown","faede11b":"markdown","6a2951e2":"markdown","b13bb991":"markdown","0d80ad6c":"markdown","3c01103b":"markdown","855f909f":"markdown"},"source":{"889d8aa3":"import numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nimport matplotlib\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold,RepeatedKFold\nimport warnings\nfrom six.moves import urllib\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nplt.style.use('seaborn')\nfrom scipy.stats import norm, skew\nrandom_state = 42\nnp.random.seed(random_state)","6112655a":"#Load the Data\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nfeatures = [c for c in train.columns if c not in ['ID_code', 'target']]","18c4b839":"train.describe()","4a5f45e5":"train.info()","54fcc51c":"train.shape","0f289cf3":"train.head(5)","a53f7c3f":"#Check for Missing Values after Concatination\n\nobs = train.isnull().sum().sort_values(ascending = False)\npercent = round(train.isnull().sum().sort_values(ascending = False)\/len(train)*100, 2)\npd.concat([obs, percent], axis = 1,keys= ['Number of Observations', 'Percent'])","43ce30b1":"target = train['target']\ntrain = train.drop([\"ID_code\", \"target\"], axis=1)\nsns.set_style('whitegrid')\nsns.countplot(target)","6e0a978c":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of mean values per row in the train and test set\")\nsns.distplot(train[features].mean(axis=1),color=\"black\", kde=True,bins=120, label='train')\nsns.distplot(test[features].mean(axis=1),color=\"red\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","5149e168":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of mean values per column in the train and test set\")\nsns.distplot(train[features].mean(axis=0),color=\"black\", kde=True,bins=120, label='train')\nsns.distplot(test[features].mean(axis=0),color=\"red\", kde=True,bins=120, label='test')\nplt.legend();plt.show()","7321c8ed":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of std values per rows in the train and test set\")\nsns.distplot(train[features].std(axis=1),color=\"blue\",kde=True,bins=120, label='train')\nsns.distplot(test[features].std(axis=1),color=\"green\", kde=True,bins=120, label='test')\nplt.legend(); plt.show()","d7db9e6e":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of mean values per column in the train and test set\")\nsns.distplot(train[features].mean(axis=0),color=\"blue\", kde=True,bins=120, label='train')\nsns.distplot(test[features].mean(axis=0),color=\"green\", kde=True,bins=120, label='test')\nplt.legend();plt.show()","415e62db":"t0 = train.loc[target == 0]\nt1 = train.loc[target == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of mean values per row in the train set\")\nsns.distplot(t0[features].mean(axis=1),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].mean(axis=1),color=\"green\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","fac474fd":"t0 = train.loc[target == 0]\nt1 = train.loc[target == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of mean values per column in the train set\")\nsns.distplot(t0[features].mean(axis=0),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].mean(axis=0),color=\"green\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","f2dd0b1a":"t0 = train.loc[target == 0]\nt1 = train.loc[target == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of standard deviation values per row in the train set\")\nsns.distplot(t0[features].std(axis=1),color=\"blue\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].std(axis=1),color=\"red\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","c368eba2":"t0 = train.loc[target  == 0]\nt1 = train.loc[target  == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of standard deviation values per column in the train set\")\nsns.distplot(t0[features].std(axis=0),color=\"blue\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].std(axis=0),color=\"red\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","e4298af3":"t0 = train.loc[target == 0]\nt1 = train.loc[target == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of skew values per row in the train set\")\nsns.distplot(t0[features].skew(axis=1),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].skew(axis=1),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","b2f634b3":"t0 = train.loc[target == 0]\nt1 = train.loc[target == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of skew values per column in the train set\")\nsns.distplot(t0[features].skew(axis=0),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].skew(axis=0),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","daf28f4f":"t0 = train.loc[target == 0]\nt1 = train.loc[target == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of kurtosis values per row in the train set\")\nsns.distplot(t0[features].kurtosis(axis=1),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].kurtosis(axis=1),color=\"green\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","34f36c5c":"t0 = train.loc[target == 0]\nt1 = train.loc[target == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of kurtosis values per column in the train set\")\nsns.distplot(t0[features].kurtosis(axis=0),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].kurtosis(axis=0),color=\"green\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","bc3b27c7":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\ntrain_scaled = scaler.fit_transform(train)         \nPCA_train_x = PCA(2).fit_transform(train_scaled)\nplt.scatter(PCA_train_x[:, 0], PCA_train_x[:, 1], c=target, cmap=\"copper_r\")\nplt.axis('off')\nplt.colorbar()\nplt.show()","cabdce02":"from sklearn.decomposition import KernelPCA\n\nlin_pca = KernelPCA(n_components = 2, kernel=\"linear\", fit_inverse_transform=True)\nrbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.0433, fit_inverse_transform=True)\nsig_pca = KernelPCA(n_components = 2, kernel=\"sigmoid\", gamma=0.001, coef0=1, fit_inverse_transform=True)\n\n\nplt.figure(figsize=(11, 4))\nfor subplot, pca, title in ((131, lin_pca, \"Linear kernel\"), (132, rbf_pca, \"RBF kernel, $\\gamma=0.04$\"), \n                            (133, sig_pca, \"Sigmoid kernel, $\\gamma=10^{-3}, r=1$\")):\n       \n    PCA_train_x = PCA(2).fit_transform(train_scaled)\n    plt.subplot(subplot)\n    plt.title(title, fontsize=14)\n    plt.scatter(PCA_train_x[:, 0], PCA_train_x[:, 1], c=target, cmap=\"nipy_spectral_r\")\n    plt.xlabel(\"$z_1$\", fontsize=18)\n    if subplot == 131:\n        plt.ylabel(\"$z_2$\", fontsize=18, rotation=0)\n    plt.grid(True)\n\nplt.show()","2b989988":"def augment(x,y,t=2):\n    xs,xn = [],[]\n    for i in range(t):\n        mask = y>0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        for c in range(x1.shape[1]):\n            np.random.shuffle(ids)\n            x1[:,c] = x1[ids][:,c]\n        xs.append(x1)\n\n    for i in range(t\/\/2):\n        mask = y==0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        for c in range(x1.shape[1]):\n            np.random.shuffle(ids)\n            x1[:,c] = x1[ids][:,c]\n        xn.append(x1)\n\n    xs = np.vstack(xs)\n    xn = np.vstack(xn)\n    ys = np.ones(xs.shape[0])\n    yn = np.zeros(xn.shape[0])\n    x = np.vstack([x,xs,xn])\n    y = np.concatenate([y,ys,yn])\n    return x,y","49233c1b":"param = {\n    \"objective\" : \"binary\",\n    \"metric\" : \"auc\",\n    \"boosting\": 'gbdt',\n    \"max_depth\" : -1,\n    \"num_leaves\" : 31,\n    \"learning_rate\" : 0.01,\n    \"bagging_freq\": 5,\n    \"bagging_fraction\" : 0.4,\n    \"feature_fraction\" : 0.05,\n    \"min_data_in_leaf\": 150,\n    \"min_sum_heassian_in_leaf\": 10,\n    \"tree_learner\": \"serial\",\n    \"boost_from_average\": \"false\",\n    \"bagging_seed\" : random_state,\n    \"verbosity\" : 1,\n    \"seed\": random_state}","12933deb":"train.shape","bd00123b":"num_folds = 11\nfeatures = [c for c in train.columns if c not in ['ID_code', 'target']]\n\nfolds = StratifiedKFold(n_splits=num_folds, shuffle=False, random_state=2319)\noof = np.zeros(len(train))\ngetVal = np.zeros(len(train))\npredictions = np.zeros(len(target))\nfeature_importance_df = pd.DataFrame()\n\nprint('Light GBM Model')\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n    \n    X_train, y_train = train.iloc[trn_idx][features], target.iloc[trn_idx]\n    X_valid, y_valid = train.iloc[val_idx][features], target.iloc[val_idx]\n    \n    X_tr, y_tr = augment(X_train.values, y_train.values)\n    X_tr = pd.DataFrame(X_tr)\n    \n    print(\"Fold idx:{}\".format(fold_ + 1))\n    trn_data = lgb.Dataset(X_tr, label=y_tr)\n    val_data = lgb.Dataset(X_valid, label=y_valid)\n    \n    clf = lgb.train(param, trn_data, 1000000, valid_sets = [trn_data, val_data], verbose_eval=5000, early_stopping_rounds = 4000)\n    oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    getVal[val_idx]+= clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration) \/ folds.n_splits\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions += clf.predict(test[features], num_iteration=clf.best_iteration) \/ folds.n_splits\n\nprint(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))","4d58ff74":"cols = (feature_importance_df[[\"feature\", \"importance\"]]\n        .groupby(\"feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:1000].index)\nbest_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n\nplt.figure(figsize=(14,26))\nsns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('LightGBM Features (averaged over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances.png')","6534447f":"num_sub = 34\nprint('Saving the Submission File')\nsub = pd.DataFrame({\"ID_code\": test.ID_code.values})\nsub[\"target\"] = predictions\nsub.to_csv('submission{}.csv'.format(num_sub), index=False)\ngetValue = pd.DataFrame(getVal)\ngetValue.to_csv(\"Validation.csv\")","a15a8c15":"<pre>Since PCA hasn't been useful, I decided to proceed with the existing dataset<\/pre>","4011caf3":"<h1><center><font size=\"6\">Santander EDA, PCA and Light GBM Classification Model<\/font><\/center><\/h1>\n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/4\/4a\/Another_new_Santander_bank_-_geograph.org.uk_-_1710962.jpg\/640px-Another_new_Santander_bank_-_geograph.org.uk_-_1710962.jpg\"><\/img>\n\n<br>\n<b>\nIn this challenge, Santander invites Kagglers to help them identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data they have available to solve this problem. \nThe data is anonimyzed, each row containing 200 numerical values identified just with a number.<\/b>\n\n<b>Inspired by Jiwei Liu's Kernel. I added Data Augmentation Segment to my kernel<\/b>\n\n<pre>\n<a id='0'><b>Content<\/b><\/a>\n- <a href='#1'><b>Import the Data<\/b><\/a>\n- <a href='#11'><b>Data Exploration<\/b><\/a>  \n- <a href='#2'><b>Check for the missing values<\/b><\/a>  \n- <a href='#3'><b>Visualizing the Satendar Customer Transactions Data<\/b><\/a>   \n - <a href='#31'><b>Check for Class Imbalance<\/b><\/a>   \n - <a href='#32'><b>Distribution of Mean and Standard Deviation<\/b><\/a>   \n - <a href='#33'><b>Distribution of Skewness<\/b><\/a>   \n - <a href='#34'><b>Distribution of Kurtosis<\/b><\/a>   \n- <a href='#4'><b>Principal Component Analysis<\/b><\/a>\n - <a href='#41'><b>Kernel PCA<\/b><\/a>\n- <a href = \"#16\"><b>Data Augmentation<\/b><\/a>\n- <a href='#6'><b>Build the Light GBM Model<\/b><\/a><\/pre>","e7b022ed":"<a id=1><pre><b>Import the Data<\/b><\/pre><\/a>","a71e1f7b":"<pre>Let's see now the distribution of skewness on columns in train separated for values of target 0 and 1.<\/pre>","1ffd3085":"<pre>There are no missing values in the dataset<\/pre>","a4ab54cb":"<pre><a id = 32 ><b>Distribution of Mean and Standard Deviation<\/b><\/a><\/pre>\n\n<pre>EDA Reference : https:\/\/www.kaggle.com\/gpreda\/santander-eda-and-prediction<\/pre>","19fc01c5":"<pre>Distribution for Standard Deviation<\/pre>","366b51a2":"<a id=2><b><pre>Check for the Missing Values.<\/pre><\/b><\/a> ","202e8bab":"<pre><a id = 16><b>Data Augmentation<\/b><\/a><\/pre>","c187b101":"<pre>Let's check the distribution of the standard deviation of values per columns in the train and test datasets.<\/pre>","b162f7cf":"<pre>Let's see now the distribution of kurtosis on rows in train separated for values of target 0 and 1. We found the distribution to be Leptokurtic<\/pre>","8f25c8fe":"<pre><a id = 3><b>Visualizing the Satendar Customer Transactions Data<\/b><\/a><\/pre>","8685ed80":"<pre>Let's check the distribution of the mean of values per columns in the train and test datasets.<\/pre>","5d5e3f94":"<a id=11><pre><b>Data Exploration<\/b><\/pre><\/a>","57c453d3":"<pre><a id = 34 ><b>Distribution of Kurtosis<\/b><\/a><\/pre>","ccb02959":"<pre><a id = 31 ><b>Check for Class Imbalance<\/b><\/a><\/pre>","8cc91486":"<pre>Let's check now the distribution of the mean value per row in the train dataset, grouped by value of target<\/pre>","eda97880":"<pre><a id = 6><b>Build the Light GBM Model<\/b><\/a><\/pre>","fb8621a4":"<pre>Let's see now the distribution of kurtosis on columns in train separated for values of target 0 and 1.<\/pre>","faede11b":"<pre><a id = 41><b>Kernel PCA (Since the Graph above doesn't represent meaningful analysis)<\/b><\/a><\/pre>","6a2951e2":"<pre>Let's check now the distribution of the mean values per columns in the train and test datasets.<\/pre>","b13bb991":"<pre>Let's check now the distribution of standard deviation per columns in the train and test datasets.<\/pre>","0d80ad6c":"<a id=4><pre><b>Principal Component Analysis to check Dimentionality Reduction<b><\/pre><\/a>","3c01103b":"<pre><a id = 33 ><b>Distribution of Skewness<\/b><\/a><\/pre>\n\n<pre>Let's see now the distribution of skewness on rows in train separated for values of target 0 and 1. We found the distribution is left skewed<\/pre>","855f909f":"<pre>Let's check now the distribution of the standard deviation  per row in the train dataset, grouped by value of target<\/pre>"}}