{"cell_type":{"5c8a56d3":"code","9d0f96b3":"code","645dea8f":"code","23c11656":"code","eede6c16":"code","45ae778c":"code","7c36faf1":"code","b42bd60d":"code","7c830ec9":"code","ebb7bdf0":"code","166892d4":"code","e35c7aa2":"code","c4eefaa0":"code","9f8ac6a2":"code","c11654f7":"code","cf96226d":"code","78fabc03":"markdown","2e94a010":"markdown","fdcbaa60":"markdown","f085e142":"markdown","98f25c28":"markdown","d8323d47":"markdown","feccd621":"markdown","73d0ddae":"markdown","dab86e90":"markdown"},"source":{"5c8a56d3":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications import VGG16\nfrom keras import models, layers, optimizers\nimport matplotlib.image as mpimg\nimport math\n\nimport os","9d0f96b3":"BASE_DIR = '..\/input\/intel-image-classification'\nTRAIN_DIR = os.path.join(BASE_DIR, 'seg_train\/seg_train')\nTEST_DIR = os.path.join(BASE_DIR, 'seg_test\/seg_test')\nBATCH_SIZE = 20\nEPOCHS = 30\n\n#ImageDataGenerator for training data\nTRAIN_DATAGEN = ImageDataGenerator(\n                rescale = 1.0\/255,\n                validation_split = 0.2\n                )\n\n#ImageDataGenerator for testing data\nTEST_DATAGEN = ImageDataGenerator(\n                rescale = 1.0\/255\n                )","645dea8f":"j=0\nfig=plt.figure(figsize=(10, 10))\nfor i in os.listdir(TRAIN_DIR):\n        img = mpimg.imread(TRAIN_DIR+\"\/\"+i+\"\/\"+os.listdir(TRAIN_DIR+\"\/\"+i)[0])\n        fig.add_subplot(2,3,j+1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.grid(False)\n        plt.imshow(img, cmap=plt.cm.binary)\n        plt.xlabel(i)\n        j += 1\nplt.show()","23c11656":"print(\"TRAINING DATA:\")\nfor i in os.listdir(TRAIN_DIR):\n    files = os.listdir(TRAIN_DIR+\"\/\"+i)\n    print(f'{i} : {len(files)} files')\n    \nprint(\"\\n\\nTEST DATA:\")\nfor i in os.listdir(TEST_DIR):\n    files = os.listdir(TEST_DIR+\"\/\"+i)\n    print(f'{i} : {len(files)} files')","eede6c16":"train_generator = TRAIN_DATAGEN.flow_from_directory(\n                    TRAIN_DIR,\n                    target_size = (150,150),\n                    batch_size = BATCH_SIZE,\n                    class_mode = 'categorical',\n                    subset='training'\n                  )\n\nvalidation_generator = TRAIN_DATAGEN.flow_from_directory(\n                    TRAIN_DIR,\n                    target_size = (150,150),\n                    batch_size = BATCH_SIZE,\n                    class_mode = 'categorical',\n                    subset='validation'\n                  )\n\ntest_generator = TEST_DATAGEN.flow_from_directory(\n                    TEST_DIR,\n                    target_size = (150,150),\n                    batch_size = BATCH_SIZE,\n                    class_mode = 'categorical'\n                  )","45ae778c":"conv_base = VGG16(\n                    weights = 'imagenet',\n                    include_top = False,\n                    input_shape = (150,150,3)\n                 )","7c36faf1":"conv_base.summary()","b42bd60d":"model = models.Sequential()\nmodel.add(conv_base)\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(250, activation='relu'))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(6, activation='softmax'))","7c830ec9":"model.compile(loss='categorical_crossentropy', optimizer=optimizers.Nadam(lr=2e-5), metrics=['acc'])","ebb7bdf0":"history = model.fit_generator(\n                                train_generator,\n                                steps_per_epoch = train_generator.samples \/\/ BATCH_SIZE,\n                                epochs = EPOCHS,\n                                validation_data = validation_generator,\n                                validation_steps = validation_generator.samples \/\/ BATCH_SIZE,\n                             )","166892d4":"def plot_acc_loss(history):\n\n    train_acc = history.history['acc']\n    val_acc = history.history['val_acc']\n    train_loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    epochs = range(1, len(train_acc)+1)\n\n    plt.plot(epochs, train_acc, 'bo', label='Training accuracy')\n    plt.plot(epochs, val_acc, 'bo', label='Validation accuracy', color='red')\n    plt.title(\"Training and Validation Accuracy\")\n    plt.legend()\n    plt.figure()\n\n    plt.plot(epochs, train_loss, 'bo', label='Training loss')\n    plt.plot(epochs, val_loss, 'bo', label='Validation loss', color='red')\n    plt.title(\"Training and Validation loss\")\n    plt.legend()\n    plt.show()\n\n    print(\"Average Validation accuracy: \", np.mean(val_acc))\n    print(\"Average Validation loss: \", np.mean(val_loss))","e35c7aa2":"plot_acc_loss(history)","c4eefaa0":"conv_base.trainable = True\n\nset_trainable = False\nfor layer in conv_base.layers:\n    if layer.name=='block5_conv1':\n        set_trainable = True\n    if set_trainable == True:\n        layer.trainable = True\n    else:\n        layer.trainable = False","9f8ac6a2":"model.compile(loss='categorical_crossentropy', optimizer=optimizers.Nadam(lr=1e-5), metrics=['acc'])\nhistory = model.fit_generator(\n                                train_generator,\n                                steps_per_epoch = train_generator.samples \/\/ BATCH_SIZE,\n                                epochs = EPOCHS,\n                                validation_data = validation_generator,\n                                validation_steps = validation_generator.samples \/\/ BATCH_SIZE,\n                             )","c11654f7":"plot_acc_loss(history)","cf96226d":"test_acc, test_loss = model.evaluate_generator(test_generator, steps=test_generator.samples \/\/ BATCH_SIZE)\nprint(\"Test Accuracy: \", math.ceil(test_acc*100)\/100)","78fabc03":"Let's push the performance a bit further by **fine-tuning** the VGG16 model.\n\nIn fine-tuning we slightly adjust the more abstract representations of the model being reused, in order to make them more relevant to the problem at hand. We are going to unfreez last block convolution layers of the model. To limit the magnitude of modification made to pretrained representations, let's keep small learning rate.","2e94a010":"Let's define the model with it's own classifeir on top of VGG16 convolutional base.","fdcbaa60":"Almost same number of samples for each class, hence **no class-imbalance** problem.\n\nWe have two options to build a model\n1. **Convolutional Neural Network from scratch**\n2. **Feature Extraction using pretrained networks**\n\nOn an average, for every class we have 2300 images in training data, which is less. If we build a convnet from scratch, there are high chances that it will overfit the training data. So option-1 does not seem to be a good one.\n\nLet's see second option.\n\nA pretrained network is a saved network that was previously trained on a large dataset, typically on large-scale image classification task. If the original dataset is large enough and general enough, then the spatial hierarchy of features learned by the network can effectively act as a generic model of the visual world and hence it's features can be useful for many different image classification tasks.\n\nWe will use pretrained **VGG16 model**. This model was trained on ImageNet dataset for 1000 classes. Features learned by the initial layers of VGG16 model are generic enough to apply for other image classification tasks.\n\nLet's go with feature extraction using pretrained VGG16 model.\n","f085e142":"Let's apply the model on test data.","98f25c28":"Load the VGG16 model's **convolutional base**.\n\nparameter include_top=False lets us **drop the trained classifier** from VGG16 model.","d8323d47":"# Intel Image Classification\n\nIn this notebook we will use convolutional networks to perform single-label multi-class classification. The dataset contains train, test and prediction images for 6 classes ('buildings','forest','glacier','mountain','sea','street').\n\nLet's see the data and decide what model to use.\n\n### Import all the required packages","feccd621":"This approach achieves an accuracy of 90%, Not bad!\n\nFrom the above plots we see that our model is overfitting the training data, let's address this issue in next version.","73d0ddae":"### Set directory paths and initialize parameters","dab86e90":"Let's see images from each of the classes."}}