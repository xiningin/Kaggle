{"cell_type":{"f9cba204":"code","05dad2b5":"code","4ba5312a":"code","6550c7eb":"code","ae97eec0":"code","3f615c54":"code","5070de97":"code","1bcfffbb":"code","06392814":"code","93a98571":"code","afd68f78":"code","db230402":"code","0785aec5":"code","2ba619cd":"code","600f692a":"code","ea300616":"code","1203464b":"code","d602edfb":"code","b8c90625":"code","ce30ccc0":"code","7cc7acc0":"code","f7e43313":"code","c5379c24":"code","3b1ef75d":"code","f3253874":"code","c90611fc":"markdown","d99491dc":"markdown","c5b39b03":"markdown","5de711d4":"markdown","c01abdba":"markdown"},"source":{"f9cba204":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport nltk\nfrom nltk.corpus import stopwords\n\nfrom bs4 import BeautifulSoup\n\nfrom wordcloud import WordCloud\n\nimport re,string,unicodedata\nfrom keras.preprocessing import text, sequence\n\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom string import punctuation\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Embedding,LSTM,Dropout,Bidirectional,GRU\nimport tensorflow as tf","05dad2b5":"df = pd.read_json(\"..\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset_v2.json\", lines=True)\ndf.head()","4ba5312a":"del df['article_link'] # Deleting this column as it is of no use","6550c7eb":"df.head()","ae97eec0":"sns.set_style(\"dark\")\nsns.countplot(df.is_sarcastic)\n## We can see that the data is balanced","3f615c54":"### Removing Stopwords and Punctuations\n\nstop = set(stopwords.words('english'))\npunctuation = list(string.punctuation)\nstop.update(punctuation)","5070de97":"def strip_html(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    return soup.get_text()\n\n#Removing the square brackets\ndef remove_between_square_brackets(text):\n    return re.sub('\\[[^]]*\\]', '', text)\n# Removing URL's\ndef remove_between_square_brackets(text):\n    return re.sub(r'http\\S+', '', text)\n#Removing the stopwords from text\ndef remove_stopwords(text):\n    final_text = []\n    for i in text.split():\n        if i.strip().lower() not in stop:\n            final_text.append(i.strip())\n    return \" \".join(final_text)\n#Removing the noisy text\ndef denoise_text(text):\n    text = strip_html(text)\n    text = remove_between_square_brackets(text)\n    text = remove_stopwords(text)\n    return text\n#Apply function on review column\ndf['headline']=df['headline'].apply(denoise_text)","1bcfffbb":"## Wordcloud (for label = 1)\n\nplt.figure(figsize = (20,20)) # Text that is Sarcastic\nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.is_sarcastic == 1].headline))\nplt.imshow(wc , interpolation = 'bilinear')","06392814":"## Wordcloud (for label = 0)\n\nplt.figure(figsize = (20,20)) # Text that is Not Sarcastic\nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.is_sarcastic == 0].headline))\nplt.imshow(wc , interpolation = 'bilinear')","93a98571":"x_train,x_test,y_train,y_test = train_test_split(df.headline,df.is_sarcastic, test_size = 0.3 , random_state = 0)","afd68f78":"max_features = 35000\nmaxlen = 200","db230402":"tokenizer = text.Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(x_train)\ntokenized_train = tokenizer.texts_to_sequences(x_train)\nx_train = sequence.pad_sequences(tokenized_train, maxlen=maxlen)","0785aec5":"tokenized_test = tokenizer.texts_to_sequences(x_test)\nX_test = sequence.pad_sequences(tokenized_test, maxlen=maxlen)","2ba619cd":"EMBEDDING_FILE = '..\/input\/glove-twitter\/glove.twitter.27B.200d.txt'","600f692a":"def get_coefs(word, *arr): \n    return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))","ea300616":"all_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","1203464b":"batch_size = 128\nepochs = 2\nembed_size = 200","d602edfb":"#Defining Neural Network\nmodel = Sequential()\n#Non-trainable embeddidng layer\nmodel.add(Embedding(nb_words, output_dim=embed_size, weights=[embedding_matrix], input_length=200, trainable=True))\n#LSTM \nmodel.add(Bidirectional(LSTM(units=128 , recurrent_dropout = 0.5 , dropout = 0.5)))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer=keras.optimizers.Adam(lr = 0.01), loss='binary_crossentropy', metrics=['acc'])","b8c90625":"model.summary()","ce30ccc0":"history = model.fit(x_train, y_train, batch_size = batch_size , validation_data = (X_test,y_test) , epochs = 10)","7cc7acc0":"print(\"Accuracy of the model on Testing Data is - \" , model.evaluate(X_test,y_test)[1]*100)","f7e43313":"pred = model.predict_classes(X_test)\npred[:5]","c5379c24":"print(classification_report(y_test, pred, target_names = ['Not Sarcastic','Sarcastic']))","3b1ef75d":"cm = confusion_matrix(y_test,pred)\ncm","f3253874":"cm = pd.DataFrame(cm , index = ['Not Sarcastic','Sarcastic'] , columns = ['Not Sarcastic','Sarcastic'])\nplt.figure(figsize = (7,7))\nsns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='' , xticklabels = ['Not Sarcastic','Sarcastic'] , yticklabels = ['Not Sarcastic','Sarcastic'])","c90611fc":"### ANALYSIS AFTER TRAINING OF GLOVE EMBEDDINGS MODEL","d99491dc":"### TRAINING GLOVE EMBEDDINGS MODEL","c5b39b03":"### GLoVe Encodings","5de711d4":"**Tokenizing Text -> Repsesenting each word by a number**\n\n**Mapping of orginal word to number is preserved in word_index property of tokenizer**\n\n**Tokenized applies basic processing like changing it to lower case, explicitely setting that as False**\n\n**Lets keep all news to 200, add padding to news with less than 200 words and truncating long ones**","c01abdba":"### Reading and Preprocessing the data"}}