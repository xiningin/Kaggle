{"cell_type":{"d02ebec6":"code","1a0056dd":"code","fe34cde5":"code","f0720e0c":"code","b7b4b85f":"code","05bd4845":"code","a4a71f58":"code","61ab63fd":"markdown","c6e51696":"markdown","e2289b79":"markdown","b8f7ac3c":"markdown"},"source":{"d02ebec6":"import gc\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom torch import nn\n\nfrom pathlib import Path\nfrom transformers.file_utils import ModelOutput\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AdamW, AutoTokenizer, RobertaModel\nfrom transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel","1a0056dd":"COMPETITION_DATA_PATH = Path('..\/input\/commonlitreadabilityprize')\nTEST_DATA_PATH = COMPETITION_DATA_PATH \/ 'test.csv'","fe34cde5":"class PredictionDataset(Dataset):\n    def __init__(self, text_excerpts):\n        self.text_excerpts = text_excerpts\n        \n    def __len__(self):\n        return len(self.text_excerpts)\n    \n    def __getitem__(self, idx):\n        sample = {'text_excerpt': self.text_excerpts[idx]}\n        return sample\n    \ndef create_prediction_dataloader(data, batch_size):\n    text_excerpts = data['excerpt'].tolist()\n    dataset = PredictionDataset(text_excerpts=text_excerpts)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n    return dataloader","f0720e0c":"class RegressorOutput(ModelOutput):\n    loss = None\n    logits = None\n    hidden_states = None\n    attentions = None","b7b4b85f":"class RobertaPoolerRegressor(RobertaPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        \n        self.roberta = RobertaModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.regressor = nn.Linear(config.hidden_size, 1)\n        self.loss_fct = nn.MSELoss()\n        self.init_weights()\n        \n    def forward(self, input_ids=None, attention_mask=None, labels=None):\n        bert_outputs = self.roberta(input_ids=input_ids,\n                                    attention_mask=attention_mask)\n        pooler_output = bert_outputs['pooler_output']\n        pooler_output = self.dropout(pooler_output)\n        logits = self.regressor(pooler_output)\n        loss = self.loss_fct(labels, logits) if labels is not None else None\n        return RegressorOutput(loss=loss, logits=logits)","05bd4845":"def predict(dataloader, model, tokenizer, device):\n    model.eval()\n    predictions = []\n    for batch_num, batch in enumerate(dataloader):\n        inputs = tokenizer(batch['text_excerpt'], padding=True, truncation=True, return_tensors=\"pt\")\n        inputs = {key: value.to(device) for key, value in inputs.items()}\n        with torch.no_grad():\n            outputs = model(**inputs)\n        batch_predictions = outputs.logits.detach().cpu().numpy()\n        predictions.append(batch_predictions)\n    predictions = np.vstack(predictions)\n    return predictions","a4a71f58":"BATCH_SIZE = 16\n\ntest_data = pd.read_csv(TEST_DATA_PATH)\ntest_dataloader = create_prediction_dataloader(test_data, batch_size=4)\nTOKENIZER_PATH = '..\/input\/commonlit-data-download\/roberta-base'\npredictions = []\nfor fold_num in range(5):\n    model_path = '..\/input\/commonlit-easy-transformer-finetuner\/additional-pretrained-roberta-base-pooler-regressor\/' + str(fold_num)\n    model = RobertaPoolerRegressor.from_pretrained(model_path)\n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)\n    \n    gc.collect()\n    torch.cuda.empty_cache()\n    model.to(device)\n    \n    fold_predictions = predict(test_dataloader, model, tokenizer, device)\n    predictions.append(fold_predictions)\n\npredictions = np.hstack(predictions)\nmean_predictions = np.mean(predictions, axis=1)\ntest_data['target'] = mean_predictions\ntest_data[['id','target']].to_csv('submission.csv', index=False)","61ab63fd":"# Define Dataset and Dataloader","c6e51696":"# Commonlit Finetuned Roberta inference\nA Pretrained Roberta-base transformer is finetuned with the Competition dataset. This inference notebook is based on the training notebook here: https:\/\/www.kaggle.com\/vigneshbaskaran\/commonlit-easy-transformer-finetuner","e2289b79":"# Define prediction loop","b8f7ac3c":"# Define Model"}}