{"cell_type":{"2fbcef3e":"code","33a61a65":"code","ae2c05c1":"code","a41fbc98":"code","b8e0422e":"code","298e8a04":"code","e1001347":"code","c0698bf7":"markdown","29bc080a":"markdown","de430b0c":"markdown","ab0362f1":"markdown"},"source":{"2fbcef3e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport math\nfrom collections import Counter\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","33a61a65":"data = pd.read_csv('..\/input\/mushrooms.csv')\ndata.head()","ae2c05c1":"data.isnull().sum()","a41fbc98":"print('We have {} features in our data'.format(len(data.columns)))","b8e0422e":"def entropy(labels):\n    entropy=0\n    label_counts = Counter(labels)\n    for label in label_counts:\n        prob_of_label = label_counts[label] \/ len(labels)\n        entropy -= prob_of_label * math.log2(prob_of_label)\n    return entropy\n\ndef information_gain(starting_labels, split_labels):\n    info_gain = entropy(starting_labels)\n    for branched_subset in split_labels:\n        info_gain -= len(branched_subset) * entropy(branched_subset) \/ len(starting_labels)\n    return info_gain","298e8a04":"def split(dataset, column):\n    split_data = []\n    col_vals = data[column].unique() # This tree generation method only works with discrete values\n    for col_val in col_vals:\n        split_data.append(dataset[dataset[column] == col_val])\n    return(split_data)","e1001347":"def find_best_split(dataset):\n    best_gain = 0\n    best_feature = 0\n    features = list(dataset.columns)\n    features.remove('class')\n    for feature in features:\n        split_data = split(dataset, feature)\n        split_labels = [dataframe['class'] for dataframe in split_data]\n        gain = information_gain(dataset['class'], split_labels)\n        if gain > best_gain:\n            best_gain, best_feature = gain, feature\n    print(best_feature, best_gain)\n    return best_feature, best_gain\n\nnew_data = split(data, find_best_split(data)[0]) # contains a list of dataframes after splitting","c0698bf7":"Let's start by creating 2 functions, one that calculates the entropy and one that calculaten the information gain.\n","29bc080a":"**Gini Impurity:**\n\nWe start at 1 (maximum impurity) and subtract the squared percentage of each label in the data set. As an example, if a data set had 5 items of class (1) and 5 item of class (0), the Gini impurity of the set would be\n\n\\\\(G = 1 - (5\/10)^2 - (5\/10)^2  \\\\)\n\nThat is the impurity at any given instance\/leaf, to find the Weighted Information Gain, you start with the old (root) Gini Impurity and subtract the sum of all weighted Gini Impurities.\nThe weighted Gini Impurity is the same as the Gini Impurity but multiplied by a ratio (weight) which is the number of data points in the new leaf divided by the number of points in the original root.\n![](https:\/\/drive.google.com\/uc?id=1Kf855C45vVGh-1-FOVaCTgYp6MdEyz3M)\nThe Weighted Gain for this example \\\\( = 0.5 - (2\/10)*0.5 - (5\/10)*0.48 - (4\/10)*0.44 = 0.026  \\\\)\n\n\n**Entropy:**\nSimilar to the Gini Impurity but follows a different formula for the calculation:\n![](https:\/\/drive.google.com\/uc?id=1m0W6QKmGbS6GTSDo1zyogpOo9eDpqIZp)\nWe will still find the information gain, using weighted entropies and pick the attribute which provided the maximum information gain.\n![](https:\/\/drive.google.com\/uc?id=14xKt2BkbU1qjFwLZ-k16SLf3IJtTrPWA)","de430b0c":"Now, a recursive call is needed to keep finding best split for every new data subset. I will stop here as it would get rather complex and the goal of this kernel was just to practice the idea behind decsion trees in general and calculating the entropy.","ab0362f1":"The idea behind building trees is, finding the best feature to split on that generates the largest information gain or provides the least uncertainity in the following leafs.\nThe 2 most known ways to find these features are:\n1. Gini Impurity\n2. Entropy\n\nI will give a brief about the Gini Impurity method, but will implement the tree using calculated Entropy."}}