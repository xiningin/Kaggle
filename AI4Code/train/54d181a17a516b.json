{"cell_type":{"18840c04":"code","6f5a074d":"code","0356dbb4":"code","8e4bfa68":"code","6bf1a0d5":"code","2fedacf8":"code","a53c7392":"code","ccab90d4":"code","934a45a2":"code","671cc7fe":"code","9f01fb83":"code","b43b4e1d":"code","e2e547eb":"code","d1d3ee8c":"code","149bde67":"code","48a60b0d":"code","226c9ae2":"markdown","08bb128e":"markdown","a8e729fc":"markdown","1dd9d2ec":"markdown","f7bb2ed9":"markdown","b089da62":"markdown","237874f9":"markdown","1d65b3a0":"markdown","a75c73ad":"markdown","e406d8ea":"markdown","8454cb88":"markdown","f519aa3b":"markdown","ceaf3386":"markdown","eba3fbe8":"markdown","805a936b":"markdown","c88848e6":"markdown","436e8f46":"markdown","beaf6d7b":"markdown","27464e40":"markdown","7163546f":"markdown","42c7851a":"markdown","431760d7":"markdown","8ca2c8b1":"markdown","69ea95b1":"markdown","c2f8175f":"markdown","66556504":"markdown"},"source":{"18840c04":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","6f5a074d":"data = pd.read_csv('..\/input\/weatherAUS.csv')\ndata.sample(5)","0356dbb4":"# Getting rid of the columns with objects which will not be used in our model:\ndata.drop(['Date', 'Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RISK_MM'], axis=1, inplace=True)\ndata.head(5)","8e4bfa68":"# And we need to replace NaN values with mean values of each column:\ndata.fillna(data.mean(), inplace=True)\ndata.head(5)","6bf1a0d5":"# Now we can change that day and next days'predictions (yes and no) to 1 and 0:\ndata.RainToday = [1 if each == 'Yes' else 0 for each in data.RainToday]\ndata.RainTomorrow = [1 if each == 'Yes' else 0 for each in data.RainTomorrow]\ndata.sample(3)","2fedacf8":"y = data.RainTomorrow.values\nx_data = data.drop('RainTomorrow', axis=1)\nx_data.head()","a53c7392":"# In order to scale all the features between 0 and 1:\nx = (x_data - np.min(x_data)) \/ (np.max(x_data) - np.min(x_data))\nx.head(5)","ccab90d4":"# importing sklearn's library for splitting our dataset:\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=75)\n\n# For our matrix calculations we need to transpose our matrixis:\nx_train = x_train.T\ny_train = y_train.T\nx_test = x_test.T\ny_test = y_test.T\n\nprint('x_train shape is: ', x_train.shape)\nprint('y_train shape is: ', y_train.shape)\nprint('x_test shape is: ', x_test.shape)\nprint('y_test shape is: ', y_test.shape)","934a45a2":"def initialize_weight_bias(dimension):\n    w = np.full((dimension,1), 0.01)    # Create a matrix by the size of (dimension,1) and fill it with the values of 0.01\n    b = 0.0\n    return w,b","671cc7fe":"def sigmoid(z):\n    y_head = 1 \/ (1 + np.exp(-z))\n    return y_head","9f01fb83":"def forward_backward_propagation(w, b, x_train, y_train):\n    # forward propagation:\n    z = np.dot(w.T, x_train) + b\n    y_head = sigmoid(z)\n    \n    loss = -(1 - y_train) * np.log(1 - y_head) - y_train * np.log(y_head)     # loss function formula\n    cost = (np.sum(loss)) \/ x_train.shape[1]                               # cost function formula\n    \n    # backward propagation:\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1]\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]\n    \n    gradients = {'derivative_weight': derivative_weight, 'derivative_bias': derivative_bias}\n    \n    return cost, gradients","b43b4e1d":"def update(w, b, x_train, y_train, learning_rate, nu_of_iteration):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    # Initialize for-back propagation for the number of iteration times. Then updating w and b values and writing the cost values to a list:  \n    for i in range(nu_of_iteration):\n        cost, gradients = forward_backward_propagation(w, b, x_train, y_train)\n        cost_list.append(cost)\n    \n        # Update weight and bias values:\n        w = w - learning_rate * gradients['derivative_weight']\n        b = b - learning_rate * gradients['derivative_bias']\n        # Show every 20th value of cost:\n        if i % 20 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print('Cost after iteration %i: %f' %(i,cost))\n    \n    parameters = {'weight': w, 'bias':b}\n    \n    # Visulization of cost values:\n    plt.plot(index, cost_list2)\n    plt.xlabel('Nu of Iteration')\n    plt.ylabel('Cost Function Value')\n    plt.show()\n    \n    return parameters, gradients, cost_list","e2e547eb":"def prediction(w, b, x_test):\n    z = sigmoid(np.dot(w.T, x_test) + b)\n    y_prediction = np.zeros((1,x_test.shape[1]))\n    \n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            y_prediction[0,i] = 0\n        else:\n            y_prediction[0,i] = 1\n            \n    return y_prediction","d1d3ee8c":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate, nu_of_iteration):\n    dimension = x_train.shape[0]\n    w, b = initialize_weight_bias(dimension)    # Creating an initial weight matrix of (x_train data[0] x 1)\n    \n    # Updating our w and b by using update method. \n    # Update method contains our forward and backward propagation.\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate, nu_of_iteration)\n    \n    # Lets use x_test for predicting y:\n    y_test_predictions = prediction(parameters['weight'], parameters['bias'], x_test) \n    \n    # Investigate the accuracy:\n    print('Test accuracy: {}%'.format(100 - np.mean(np.abs(y_test_predictions - y_test))*100))","149bde67":"logistic_regression(x_train, y_train, x_test, y_test, learning_rate=1, nu_of_iteration=400)","48a60b0d":"# Importing sklearn library for logistic regression:\nfrom sklearn.linear_model import LogisticRegression\n\n# Creating our model named 'lr'\nlr = LogisticRegression()\n\n# Training it by using our train data:\nlr.fit(x_train.T, y_train.T)\n\n# Printing our accuracy by using our trained model and test data:\nprint('Test accuracy of sklearn logistic regression library: {}'.format(lr.score(x_test.T, y_test.T)))","226c9ae2":"**2.3. Converting Predictions to Binary for Logistic Regression**","08bb128e":"**4. LOGISTIC REGRESSION WITH SKLEARN LIBRARY**","a8e729fc":"**2.5. Normalization Progress**","1dd9d2ec":"Sigmoid Function is used to scale z values between 0 and 1. But this is not the same thing as the normalization.\n\nSigmoid function is used for explaining propability.\n\nLet's say we have used our weight, bias and x values in this formula: **z = w*x + b**\n\nAnd let's say our result for **z=5**. And when we apply it to sigmoid function we have **y_head=0.8**\n\n**That means our model's prediction's result is 1. Because all y_head values above 0.5 (treshold value) on the graph are 1 in the sigmoid function graph.** If we have y_head = 0.4 that means our model's prediction is 0. \n\nHere is the sigmoid function's graph:\n![](https:\/\/imgur.com\/z1hx3dc.png)","f7bb2ed9":"**2.2. Cleaning Data**","b089da62":"Let's test our model. Use logistic_regression method, train and test data, with a learning rate of 1 and run the forward backward propagation 400 times to train our model. And then show us our cost function (which should decline by the number of iterations) and accuracy of the model.","237874f9":"**1. IMPORTING LIBRARIES**","1d65b3a0":"**IS IT WHETHER OR NOT GOING TO RAIN IN AUSTRALIA?**\n\nIn this kernel I use rain in Australia dataset. This dataset contains daily weather observations from numerous Australian weather stations.\n\nThe target variable RainTomorrow means: Did it rain the next day? Yes or No.\n\nI used **logistic regression model** for predictions. Before using sklearn library for logistic regression, I explained all the steps with the help of DATAI Team's wonderful kernel (https:\/\/www.kaggle.com\/kanncaa1\/deep-learning-tutorial-for-beginners) and Udemy course (https:\/\/www.udemy.com\/machine-learning-ve-python-adan-zye-makine-ogrenmesi-4\/).\n\nIn order to create a logistic regression model, I followed these steps:\n* Importing libraries\n* Importing dataset\n* Cleaning dataset\n* Converting Predictions to Binary form (0 or 1)\n* Excluding Tomorrow's Prediction from the Dataset (Because I wouldn't want my model to cheat)\n* Normalization Progress (Scaling all the values between 0 and 1)\n* Splitting Dataset for Training and Testing\n* Creating the Initial Parameters (Weight and Bias)\n* Defining the Sigmoid Function\n* Defining Forward and Backward Propagation\n* Defining Update Parameters Method\n* Defining Prediction Method\n* Implementing Logistic Regression Using Test Data\n\nAnd at the end of this kernel I showed an easier way of doing logistic regression which is using sklearn library.","a75c73ad":"**2.4. Excluding Tomorrow's Prediction from the Dataset**","e406d8ea":"**2.1. Importing Data**","8454cb88":"**3.1. Creating the Initial Parameters (Weight and Bias)**","f519aa3b":"**3.6. Implementing Logistic Regression Using Test Data**","ceaf3386":"Loss function tells us if our model's prediction is correct or wrong. If the value of loss function is 1 that means our model's prediction is wrong.\n\nLoss (error) function formula = -(1 - y) * log(1 - y_head) - y * log(y_head)\n\nCost function is the summation of all the loss functions. If cost function is high that means our model makes more mistakes in it's predictions.","eba3fbe8":"80% of the dataset will be used for training the model and 20% will be used later for testing our model's accuracy.","805a936b":"**3.4. Defining Update Parameters Method**","c88848e6":"Now it's time for using our x_test data and defined methods to predict y_prediction values. \n\nThen we will use our predictions (y_test_predictions) and real y values (y_test) to compare and calculate our model's accuracy.","436e8f46":"**3. LOGISTIC REGRESSION**","beaf6d7b":"**2.6. Dividing Dataset for Training and Testing the Model**","27464e40":"**5. CONCLUSION**\n\nLogistic regression is a very important subject for understanding machine learning and first step to deep learning.\nThat's why it's so important to understand logic behind the logistic regression before using library.\n\nKey steps for logistic regression are:\n\n        Clean and fill the data\n        \n            Seperate x and y values\n        \n                Normalize values\n            \n                    Split train and test data\n                \n                        Define the initial weight and bias values\n                    \n                            Apply sigmoid function\n                        \n                                Implement forward and backward propagations\n                            \n                                    Update w and b \n                                \n                                        Predict the y test values by using x test and updated w and b\n                                    \n                                            Compare predicted y values and y test values for accuracy\n\n\nI hope you understand and liked my logistic regression kernel. If you liked it please vote it and comment your questions or ideas.\n\nMelih..","7163546f":"**3.3. Defining Forward and Backward Propagation**","42c7851a":"**3.2. Defining the Sigmoid Function**","431760d7":"In order to be able to analyse and visualize all feature values we need to scale all the values between 0 and 1. This progress is called normalisation.\n\nNormalization formula = (x - min(x)) \/ (max(x) - min(x))","8ca2c8b1":"We are going to tell our model how many times to initiate forward and backward propagation.\n\nFor this purpose we will use weight, bias, training datasets (x_train, y_train), number of iteration (how many times to go for-back) and our model's learning rate (how many steps our model uses while doing for-back propagation).","69ea95b1":"**2. DATA PREPARATION**","c2f8175f":"**3.5. Defining Prediction Method**","66556504":"Now we can define our prediction method. For our model's training we have used training data but for prediction we will be using our test data.\n\n"}}