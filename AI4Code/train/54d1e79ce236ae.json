{"cell_type":{"ada7685c":"code","df8fea85":"code","15dc1c2e":"code","04b09f34":"code","429593ce":"code","a3208ae2":"code","7c9fa1e2":"code","77a59990":"code","8f1304c5":"code","5327d284":"code","78ed85f5":"code","9fe41c82":"code","de5927fa":"code","84ae4fed":"code","e351e636":"code","f9942ad8":"code","7dacf3bb":"code","524b849b":"code","ab4d74c2":"code","981f4678":"code","45c0b69d":"code","7ca1366d":"code","125d4463":"code","0dd15377":"code","dd875bc3":"code","7fd3e310":"code","b00231a2":"code","ec2cdd84":"code","f6b23601":"markdown","4b59c045":"markdown","ec39f8cc":"markdown","63540e4a":"markdown","86f18b8f":"markdown","65a96c8f":"markdown","afd93fb8":"markdown","81bca22a":"markdown","d1434279":"markdown","1e0a064d":"markdown","246affbc":"markdown","2197784d":"markdown","75f40f75":"markdown","768b05ec":"markdown","a577339a":"markdown","d4a875d3":"markdown","352efcac":"markdown","09651a38":"markdown","48f60021":"markdown","79fb5fd7":"markdown"},"source":{"ada7685c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# setting the font size\nimport matplotlib\nmatplotlib.rcParams.update({'font.size': 15})\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","df8fea85":"iris = pd.read_csv('\/kaggle\/input\/iris\/Iris.csv')\niris.head()","15dc1c2e":"iris.drop('Id', axis=1, inplace=True)\niris.columns","04b09f34":"target = 'Species'\nfeatures = [val for val in iris.columns if val != target]","429593ce":"from sklearn.model_selection import train_test_split","a3208ae2":"X = iris[features]\ny = iris[target]\nprint(X.shape)\nprint(y.shape)","7c9fa1e2":"X = iris[features]\ny = iris[target]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.3, random_state=12)\nfor data in ['X_train', 'X_test', 'y_train', 'y_test']:\n    print(f'{data}: {eval(data).shape}')","77a59990":"from sklearn.metrics import accuracy_score","8f1304c5":"from sklearn.svm import SVC\nmodel = SVC()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)","5327d284":"print(f\"Accuracy SVM: {accuracy_score(y_pred, y_test)}\")","78ed85f5":"from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nprint(f\"Accuracy Logitic Regression: {accuracy_score(y_pred, y_test)}\")","9fe41c82":"from sklearn.tree import DecisionTreeClassifier\n\nmodel = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nprint(f\"Accuracy Decision Tree: {accuracy_score(y_pred, y_test)}\")","de5927fa":"from sklearn.neighbors import KNeighborsClassifier\n\nmodel = KNeighborsClassifier()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nprint(f\"Accuracy KNN: {accuracy_score(y_pred, y_test)}\")","84ae4fed":"accuracies = []\nfor n_neighors in range(1, 100, 5):\n    model = KNeighborsClassifier(n_neighbors=n_neighors)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    print(f\"Accuracy {n_neighors} neighbors: {accuracy_score(y_pred, y_test)}\")\n    accuracies.append(accuracy_score(y_pred, y_test))","e351e636":"fig = plt.gcf()\nfig.set_size_inches(15, 20)","f9942ad8":"fig = plt.gcf()\nfig.set_size_inches(15, 10)\n\nplt.grid()\nplt.xlabel('n_neighbors')\nplt.ylabel('Accuracy')\nplt.plot(accuracies)","7dacf3bb":"def calculate_accuracy(model_name, X_train, y_train, X_test, y_test, print_result=True, **hyperparameters):\n    \"\"\"\n    model_name: the name of the model that we want to train and test\n    X_train: The training dataframe of variables\n    y_train: The training series: the target\n    X_test: The testing dataframe of variables\n    y_test: The testing series: the target\n    **hyperparameters: potential hyperparameters of the model\n    \"\"\"\n    model = eval(model_name)(**hyperparameters)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_pred, y_test)\n    if print_result:\n        print(f\"Accuracy {model_name} with {hyperparameters}: \\n{accuracy}\")\n    return accuracy","524b849b":"  _ = calculate_accuracy('KNeighborsClassifier', \n                         X_train,\n                         y_train, \n                         X_test, \n                         y_test, \n                         n_neighbors=2)","ab4d74c2":"sepal_features = ['SepalLengthCm', 'SepalWidthCm']\npetal_features = ['PetalLengthCm', 'PetalWidthCm']\n\n\nsepal = iris[sepal_features]\npetal = iris[petal_features]\ny = iris[target]\n","981f4678":"sepal.head()","45c0b69d":"petal.head()","7ca1366d":"sepal_train, sepal_test, y_train, y_test = train_test_split(sepal, y, \n                                                    test_size=0.3, random_state=12)\npetal_train, petal_test, y_train, y_test = train_test_split(petal, y, \n                                                    test_size=0.3, random_state=12)","125d4463":"for model_name in ['SVC', 'DecisionTreeClassifier', 'KNeighborsClassifier']:\n    print(model_name)\n    print('==================================')\n    print('\\n\\nFor petals:')\n    _ = calculate_accuracy(model_name, \n                           X_train=petal_train,\n                          X_test=petal_test,\n                          y_train=y_train,\n                          y_test=y_test)\n    print('\\nFor sepeals:')   \n    _ = calculate_accuracy(model_name, \n                       X_train=sepal_train,\n                      X_test=sepal_test,\n                      y_train=y_train,\n                      y_test=y_test)\n    print('\\n')","0dd15377":"from sklearn.model_selection import cross_val_score","dd875bc3":"# Looping over the three models\nfor model_name in ['SVC', 'DecisionTreeClassifier', 'KNeighborsClassifier']:\n    print(model_name)\n    print('==================================')\n    clf = eval(model_name)()\n    accuracies = cross_val_score(clf, petal, y, cv=10)\n    print('\\nResults:')\n    mean = sum(accuracies)\/len(accuracies)\n    print(f'Average accuracy: {mean}')\n    var = sum((i - mean) ** 2 for i in accuracies) \/ len(accuracies)\n    print(f'Variance accuracy: {var}')\n    print('\\n')","7fd3e310":"avg_accuracies = {}\n\nfor n_neighbors in range(1, 100):\n    clf= KNeighborsClassifier(n_neighbors=n_neighbors)\n    accuracies_kfold = cross_val_score(clf, X, y, cv=10)\n    # Calculate the average accuracy over the k-folds\n    mean_accuracy = sum(accuracies_kfold)\/len(accuracies_kfold)\n    avg_accuracies[n_neighbors] = mean_accuracy\n","b00231a2":"avg_accuracies_list = sorted(avg_accuracies.items())\n\nn_neighbors,accuracy = zip(*avg_accuracies_list) # unpack a list of pairs into two tuples\n\nfig = plt.gcf()\nfig.set_size_inches(15, 10)\n\nplt.grid()\nplt.xlabel('n_neighbors')\nplt.ylabel('Accuracy')\nplt.plot(n_neighbors,accuracy)","ec2cdd84":"best_n_neighbors = max(avg_accuracies, key=avg_accuracies.get)\nprint('The value n_neighbors with the maximum accuracy is: ')\nprint(f'Max_n_neighbors: {best_n_neighbors} --> Max_accuracy: {avg_accuracies[best_n_neighbors]}')      ","f6b23601":"We can see that after a value of **n_neighbors=11**, the accuracy drops ","4b59c045":"The **Id** column won't be useful, we will remove it","ec39f8cc":"# Data preprocessing","63540e4a":"Let's split both of the dataframes into training and testing sets.\n\n***Note:***\n*We set the same seed for both of them*","86f18b8f":"## Observation\n\n- Training the dataset with Petal variables rather than Sepals variables gives a better accuracy\n- Let's only keep the petals variables thereafter","65a96c8f":"# Model testing\n\nWe are going to test a few well-known models of classification","afd93fb8":"## Perform classification algorithms","81bca22a":"Let's now perform the three models that we saw before with Petals and Sepals:\n- Support Vector Machine\n- Logistic Regression\n- K nearest neighbors\n","d1434279":"We are going to save the accuracy scoring for a specific model in a function called ***calculate_accuracy***","1e0a064d":"## Observations\n\n- The three of these models seem to perform really well\n- For all the models the variance is insignificant\n- KNN seems to be the best","246affbc":"Let's test it","2197784d":"## Support Vector Machine","75f40f75":"## Import data","768b05ec":"## K Nearest Neighbors","a577339a":"# Cross-validation","d4a875d3":"## Sub-datasets\n\nThe variables can be divided between those of:\n- petals\n- sepals\n\nLet's see the performances of the two sub-datasets respectively with the variables:\n- PetalLengthCm and PetalWidthCm\n\nand\n\n- SepalLengthCm and SepalWidthCm\n","352efcac":"## Decision Tree","09651a38":"Split the dataset into training and testing datasets","48f60021":"Let's test different values of neighbors with K-Fold cross-validation with the hole dataset iris","79fb5fd7":"## Logistic Regression"}}