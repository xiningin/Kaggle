{"cell_type":{"1086fd4d":"code","8bbbe415":"code","90663577":"code","48a739c1":"code","287e461c":"code","db11b2f5":"code","98a9ff4b":"code","268ddce7":"code","b5ffe336":"code","08f5de57":"code","0aec373f":"code","fbaa4127":"markdown","2a86c3e8":"markdown","459c3e56":"markdown","fb6a1243":"markdown","53568b26":"markdown","07316122":"markdown","47dd6377":"markdown","890d681a":"markdown","6f5be5a6":"markdown","1159b233":"markdown","17146620":"markdown"},"source":{"1086fd4d":"from pandas import read_csv\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n#import tensorflow as tf\nimport tensorflow.compat.v1 as tf\nimport keras\nfrom keras.models import Sequential\ntf.disable_v2_behavior()","8bbbe415":"url = \"\/kaggle\/input\/airfoil-selfnoise-dataset\/AirfoilSelfNoise.csv\"\ndata = read_csv(url)\nprint(data)","90663577":"data = data.rename(columns={\"f\": 'Frequency',\"alpha\":'Angle of attack',\"c\":'Chord length',\"U_infinity\":'Free-stream velocity',\"delta\":\"Section side displacement thickness\",\"SSPL\":\"Scaled Sound Pressure Level\"})\nprint(data)","48a739c1":"print(data.corr())","287e461c":"pca = PCA(n_components=1)\npca.fit(np.array(data[[\"Angle of attack\",\"Section side displacement thickness\"]]))\nprint(\"variance explained : \",pca.explained_variance_ratio_)\n\ny = pca.transform(np.array(data[[\"Angle of attack\",\"Section side displacement thickness\"]]))\nnew_component = pd.DataFrame(y)\nnew_component = new_component.rename(columns={0: 'New component'})\ndata[\"New component\"] = new_component \ndel data[\"Angle of attack\"]\ndel data[\"Section side displacement thickness\"]","db11b2f5":"print(data.corr())","98a9ff4b":"x = np.array(data[[\"Frequency\",\"Chord length\",\"Free-stream velocity\",\"New component\"]])\ny = np.array(data[[\"Scaled Sound Pressure Level\"]])\nx_train, x_test, y_train, y_test = train_test_split(x, y,train_size=0.70)\nscaler = MinMaxScaler()\n# transform data\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.fit_transform(x_test)\nprint(x_train)\nprint(x_test)","268ddce7":"model = tf.keras.models.Sequential()\nmodel.add(tf.keras.Input(shape=(4,)))\nmodel.add(tf.keras.layers.Dense(units=3,activation=tf.nn.sigmoid))\nmodel.add(tf.keras.layers.Dense(units=1))\nmodel.summary()\n","b5ffe336":"model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.002),loss=\"mean_squared_error\")\nhistory = model.fit(x_train, y_train,batch_size=100,epochs=100,validation_data=(x_test, y_test))\nplt.plot(history.history['loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')","08f5de57":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nx_train, x_test, y_train, y_test = train_test_split(x, y,train_size=0.70)\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.fit_transform(x_test)\nprint(x_train)\nprint(x_test)","0aec373f":"model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.002),loss=\"mean_squared_error\")\nhistory = model.fit(x_train, y_train,batch_size=100,epochs=100,validation_data=(x_test, y_test))\nplt.plot(history.history['loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')","fbaa4127":"As we can see the new component omitted some of the colinearity.","2a86c3e8":"It's time to start splitting the data into training (70% of the total sets), test sets (30% of the total sets). While we are at it, we scale data using the minmax technique, all of our feature will range between (0 and 1)","459c3e56":"# ****3. Modelling****\n\nWe prepare a simple neural network model, made up of :\n\n- An input layer which is a vector of size 4 (number of feature)\n- A hidden layer composed of 3 neurons equiped with sigmoid activation\n- An output layer equiped with identity function f(x) = x","fb6a1243":"In this last section we try another scaling technique which standarization. The concept here is to center the data around 0, and scale it in a way so it becomes distributed equally and symetrically around 0 (it would resemble a gaussian distribution).\nThis configuration gave a better when applying the same NN architecture.","53568b26":"We apply a dimension reduction technique to replace the feature Angle of attack, Section side displacement thickness by a principle component that best represents them, as it's clear,this new componenent explains almost all variance of these two features and best represents them, it would make sense to omit these two feature and introduce the principle component instead.","07316122":"Next we train the model on the training dataset using the SGD solver. the loss function evolution plot shows a quick convergence to the minimum. In fact only 20 epochs are needed to achieve this convergence, still the loss function best value equals 48.3528 at the end of the training process which means the models didn't completely fitted the data. The risk of overfitting here isn't iminent because the loss function values for both training and validation sets aren't too far from each other. We choose a model that as complex as the problem itself.","47dd6377":"# 1. ***Introduction*** \nThis notebook is intended to provide a simple model to predict the Scaled sound pressure using 4 features.\n\nFirst we import all the necessary packages","890d681a":"This configuration gave a better model when applying the same NN architecture. It's noticeable that the loss function considerably dropped during the first 5 iterations, then kept dropping progressively over the iterations, and took all 100 epochs to progressively attains a better model that the previous one (a smaller loss functions value). Smooth evolutions of loss functions usually lead to good results.","6f5be5a6":"We notice a relatively strong corrolation between the feature Angle of attack,Section side displacement thickness.","1159b233":"We start by importing the data and identifying the potential opportunities to reduce dimensionality","17146620":"# 2. ***Feature extraction***  "}}