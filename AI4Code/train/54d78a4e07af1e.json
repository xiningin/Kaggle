{"cell_type":{"08ca653e":"code","90c62d23":"code","76414cb5":"code","e585f6e8":"code","c5c1e1b1":"code","1e827e31":"code","d0826dd3":"code","030d15d3":"code","49b6079e":"code","4a9bd3af":"code","909b7560":"code","23251c6e":"code","c62c5319":"code","5d85547e":"code","1ea10374":"code","bf00070e":"code","d3bc7a4c":"code","b65c294c":"code","b47211b3":"code","e9941100":"code","ae3a02fe":"code","cdd6babe":"code","3c53694e":"code","2a29677c":"code","023cda33":"code","33b74f94":"code","8a7ad50d":"code","c3037a98":"code","c35117ad":"code","c5d235a1":"code","076a5527":"code","d2b1c5e5":"code","19838555":"code","83985234":"code","ff1889d2":"code","dbc28a6e":"code","a1872281":"code","130bf440":"code","a2af0551":"code","c23095aa":"code","c581b936":"code","8be28f10":"code","2b2fcfcf":"code","bfe80d8b":"code","4b069214":"code","93085582":"code","05b5461b":"code","4f574c4b":"code","98945d9f":"code","903e1d46":"code","d43d47ea":"code","ed330cd8":"code","e91eb281":"code","e5358bf2":"code","95042256":"code","db45d820":"code","a7763a84":"code","21b6015b":"code","ee2174f3":"code","f2409e19":"code","15fce196":"code","69369128":"code","a8300a44":"code","7c8b1e8b":"code","65095f30":"code","2c638e7a":"code","aa238150":"code","cb63eba0":"code","963ab0cd":"code","251850d0":"code","e689e67b":"code","52e4eb48":"code","71c0bb75":"code","74ac7915":"code","5411f947":"code","3010f8a4":"code","d25c5cd6":"markdown","d309eeb2":"markdown","5898e9a5":"markdown","ff88ad2e":"markdown","b0beee8d":"markdown","c6aeb2cc":"markdown","2b88701e":"markdown","d243282e":"markdown","1899f8fb":"markdown","0c526f58":"markdown","5c053459":"markdown","d526b200":"markdown","ec9e717e":"markdown","e22507f4":"markdown","cb350b91":"markdown","c804607e":"markdown","4706daf7":"markdown","deb635a6":"markdown"},"source":{"08ca653e":"pip install Levenshtein","90c62d23":"#Gi\u1ea3i n\u00e9n c\u00e1c file d\u1eef li\u1ec7u v\u00e0 l\u01b0u v\u00e0o output\n!unzip ..\/input\/home-depot-product-search-relevance\/product_descriptions.csv.zip\n!unzip ..\/input\/home-depot-product-search-relevance\/train.csv.zip\n!unzip ..\/input\/home-depot-product-search-relevance\/test.csv.zip\n!unzip ..\/input\/home-depot-product-search-relevance\/attributes.csv.zip","76414cb5":"#Khai b\u00e1o c\u00e1c th\u01b0 vi\u1ec7n c\u1ea7n thi\u1ebft\n%matplotlib inline\nimport re\nimport nltk\nimport numpy as np \nimport json\nimport os\nimport pandas as pd\nimport seaborn as sns # v\u1eeba \u0111\u1ebfm s\u1ed1 l\u01b0\u1ee3ng v\u1eeba v\u1ebd l\u00ean bi\u1ec3u \u0111\u1ed3\nimport Levenshtein # \u0110o kho\u1ea3ng c\u00e1ch gi\u1eefa 2 string (\u0111\u1ed9 t\u01b0\u01a1ng \u0111\u1ed3ng)\nfrom matplotlib import pyplot as plt # v\u1ebd bi\u1ec3u \u0111\u1ed3\nfrom matplotlib_venn import venn2 # v\u1ebd bi\u1ec3u \u0111\u1ed3 nh\u01b0ng l\u00e0 d\u1ea1ng tr\u00f2n\nfrom nltk.corpus import stopwords # c\u00e1c stopwords (the, or, and, ...)\nfrom nltk.stem.snowball import SnowballStemmer # k\u1ef9 thu\u1eadt stemmer \u0111\u01b0a v\u1ec1 t\u1eeb g\u1ed1c\nfrom nltk.stem import WordNetLemmatizer # k\u1ef9 thu\u1eadt Lemmatizer \u0111\u01b0a v\u1ec1 t\u1eeb g\u1ed1c\nstemmer = SnowballStemmer('english') # stemer v\u1edbi ng\u00f4n ng\u1eef l\u00e0 ti\u1ebfng anh\nnltk.download('wordnet') # dowload b\u1ed9 t\u1eeb \u0111i\u1ec3n cho Lemmatizer\nnltk.download('stopwords') # dowload b\u1ed9 c\u00e1c stopwords\nstop_words = set(stopwords.words('english')) # c\u00e1c stopwords l\u00e0 ti\u1ebfng anh ","e585f6e8":"#\u0111\u1ecdc file d\u1eef li\u1ec7u\ndt_train = pd.read_csv(\"train.csv\", encoding=\"ISO-8859-1\")\ndt_test = pd.read_csv(\"test.csv\", encoding=\"ISO-8859-1\")\ndt_attributes = pd.read_csv(\"attributes.csv\")\ndt_descriptions = pd.read_csv(\"product_descriptions.csv\")","c5c1e1b1":"#check s\u1ed1 gi\u00e1 tr\u1ecb null trong c\u00e1c file d\u1eef li\u1ec7u\nprint(f\"train data has {dt_train.isnull().values.sum()} null values:\")\nprint(f\"test data has {dt_test.isnull().values.sum()} null values:\")\nprint(f\"attributes data has {dt_attributes.isnull().values.sum()} null values:\")\nprint(f\"descriptions data has {dt_descriptions.isnull().values.sum()} null values:\")","1e827e31":"# h\u00e0m show th\u00f4ng tin c\u1ee7a t\u1eadp data\ndef show_data_info(dt):\n    print(f\"df shape: \\n\",dt.shape,\"\\n\")\n    print(\"df columns: \\n\",dt.columns,\"\\n\")\n    dt.info()\n    return dt.head(5)","d0826dd3":"#show th\u00f4ng tin t\u1eadp dt_train\nshow_data_info(dt_train)","030d15d3":"print(\"There are in total {} products \".format(len(dt_train.product_title.unique())))\nprint(\"There are in total {} search query \".format(len(dt_train.search_term.unique())))\nprint(\"There are in total {} product_uid\".format(len(dt_train.product_uid.unique())))","49b6079e":"# Th\u1ed1ng k\u00ea s\u1ed1 l\u01b0\u1ee3ng c\u00e1c rating trong t\u1eadp dt_train\ndef count_SR():\n    high_SR = [i for i in dt_train['relevance'] if i == 3.00]\n    midle_SR = [i for i in dt_train['relevance'] if i > 1.00 and i < 3.00]\n    low_SR = [i for i in dt_train['relevance'] if i == 1.00]\n    return [len(high_SR), len(midle_SR), len(low_SR)] ","4a9bd3af":"plt.bar(['high_sr', 'midle_sr', 'low_sr'], count_SR(), label='Relevance score', color='green')\nplt.xlabel('Ratings')\nplt.ylabel('Numbers')\nplt.title('Numbers of these ratings')\nplt.legend()\nplt.show()","909b7560":"# chi ti\u1ebft h\u01a1n v\u1ec1 s\u1ed1 l\u01b0\u1ee3ng relevance\nsns.countplot(x=\"relevance\", data=dt_train)\nplt.show()","23251c6e":"#show th\u00f4ng tin t\u1eadp dt_train\nshow_data_info(dt_test)","c62c5319":"print(\"There are in total {} products \".format(len(dt_test.product_title.unique())))\nprint(\"There are in total {} search query \".format(len(dt_test.search_term.unique())))\nprint(\"There are in total {} product_uid\".format(len(dt_test.product_uid.unique())))","5d85547e":"#bi\u1ec3u \u0111\u1ed3 bi\u1ec3u th\u1ecb s\u1ed1 l\u01b0\u1ee3ng c\u1ee7a c\u00e1c product_uid xu\u1ea5t hi\u1ec7n \u1edf 2 t\u1eadp dt_test v\u00e0 dt_train\nvenn2([set(dt_train[\"product_uid\"]), set(dt_test[\"product_uid\"])], set_labels=('train', 'test'), set_colors=('green', 'blue'))\nplt.show()","1ea10374":"# bi\u1ec3u \u0111\u1ed3 bi\u1ec3u th\u1ecb s\u1ed1 l\u01b0\u1ee3ng c\u1ee7a c\u00e1c search_term xu\u1ea5t hi\u1ec7n \u1edf 2 t\u1eadp dt_test v\u00e0 dt_train\nvenn2([set(dt_train[\"search_term\"]), set(dt_test[\"search_term\"])], set_labels=('train', 'test'), set_colors=('green', 'blue'))\nplt.show()","bf00070e":"#show th\u00f4ng tin t\u1eadp dt_descriptions\nshow_data_info(dt_descriptions)","d3bc7a4c":"print(\"There are in total {} product_uid \".format(len(dt_descriptions.product_uid.unique())))\nprint(\"There are in total {} product_descriptions \".format(len(dt_descriptions.product_description.unique())))","b65c294c":"#show th\u00f4ng tin t\u1eadp dt_attributes\nshow_data_info(dt_attributes)","b47211b3":"# h\u00e0m lo\u1ea1i b\u1ecf duplicates\ndef remove_duplicates(string):\n    lits_tokens = [] \n    [lits_tokens.append(str(_)) for _ in string.split() if _ not in lits_tokens]\n    return ' '.join(lits_tokens)","e9941100":"# L\u1ea5y ph\u1ea7n chung l\u1edbn nh\u1ea5t trong tr\u01b0\u1eddng name l\u00e0 bullet thay v\u00ec bullet0 x y z\ndt_attributes['name'] = [_[:6] if 'bullet' in str(_).lower() else _ for _ in dt_attributes['name'].tolist()]\n\n# t\u1ea1o n\u00ean tr\u01b0\u1eddng product_attributes t\u1eeb name v\u00e0 value \ndt_attributes['product_attributes'] = dt_attributes['name'] + ' ' +  dt_attributes['value']\ndt_attributes = dt_attributes.drop(['name', 'value'], axis=1)\n\n# g\u1ed9p c\u00e1c thu\u1ed9c t\u00ednh c\u00f3 chung id l\u1ea1i th\u00e0nh m\u1ed9t v\u00e0 n\u1ed1i c\u00e1c product_attributes t\u01b0\u01a1ng \u1ee9ng l\u1ea1i v\u1edbi nhau\n# s\u1eed d\u1ee5ng th\u00eam h\u00e0m astype() \u0111\u1ec3 \u00e9p t\u1ea5t c\u00e1c tokens v\u1ec1 string v\u00ec c\u00f3 ch\u1ee9a s\u1ed1 trong tr\u01b0\u1eddng value.\n# \u0111\u1ec3 n\u1ed1i c\u00e1c gi\u00e1 tr\u1ecb sau khi nh\u00f3m th\u00ec d\u00f9ng h\u00e0m aggregate(), c\u00e1c gi\u00e1 tr\u1ecb \u1edf c\u1ed9t value c\u00f3 chung id s\u1ebd \u0111\u01b0\u1ee3c n\u1ed1i v\u1edbi nhau c\u00e1ch nhau m\u1ed9t kho\u1ea3ng tr\u1eafng\ndt_attributes = dt_attributes.groupby('product_uid').aggregate({'product_attributes': lambda _ : ' '.join(_.astype(str))})\n\n# l\u1ea5y ph\u1ea7n chung c\u1ee7a name v\u00e0 g\u1ee1 c\u00e1c tokens b\u1ecb l\u1eb7p.\ndt_attributes['product_attributes'] = [remove_duplicates(_) for _ in dt_attributes.product_attributes.tolist()]","ae3a02fe":"# N\u1ed1i b\u1ea3ng dt_attributes v\u1edbi b\u1ea3ng dt_descriptions b\u1eb1ng thu\u1ed9c t\u00ednh chung l\u00e0 product_id\ndt_des_attr = pd.merge(dt_descriptions, dt_attributes, on='product_uid', how='left')","cdd6babe":"#show th\u00f4ng tin b\u1ea3ng v\u1eeba n\u1ed1i\ndt_des_attr.info()","3c53694e":"# t\u00ecm v\u00e0 thay t\u1ea5t c\u1ea3 c\u00e1c gi\u00e1 tr\u1ecb Null b\u1eb1ng ''\ndt_des_attr['product_attributes'].fillna('', inplace = True)","2a29677c":"# x\u00f3a c\u00e1c c\u00e2u b\u1ecb copy nh\u1ea7m trong tr\u01b0\u1eddng product_descriptions bao g\u1ed3m c\u00e1c \u0111o\u1ea1n m\u00f4 t\u1ea3 trong html, link.\nstrings = ['br',\n           'src',\n           'href',\n           'alt',\n           'please visit'\n           'Click here to review our return policy for additional information regarding returns', \n           'Click here to see Home Depot', \n           'Click here for our Project Guide', \n           'Click here for our Buying Guide', \n           'Click on the More Info tab to download',\n           'CLICK HERE to create your own collection',\n           'Click Here for details on the services',\n           'Click Here for Ideas and Designs',\n           'Click Here for a Demo of the Design',\n           'Click Here to learn more about',\n           'CLICK HERE to view our',\n           'Click below to visit our',\n           'Click here to purchase a sample of this',\n           'click on the link to get started',\n           'Click image to enlarge',\n           'https:\/\/www.ryobitools.com\/nation',\n           'http:\/\/www.homedepot.com\/ApplianceDeliveryandInstallation',\n           'http:\/\/itemvideo-dev.microsite.homedepot.com\/111414\/26P\/online_BB_banner_111114.jpg',\n           'http:\/\/www.homedepot.com\/p\/Rev-A-Shelf-Door-Mounting-Kit-5WB-DMKIT\/202855698']\n\nfor string in strings:\n    dt_des_attr['product_description'] = [_.lower().replace(string.lower(), '') for _ in dt_des_attr.product_description]","023cda33":"#show 5 h\u00e0ng \u0111\u1ea7u\ndt_des_attr.head(5)","33b74f94":"# t\u1ea1o m\u1ed9t tr\u01b0\u1eddng m\u1edbi ch\u1ee9a c\u1ea3 th\u00f4ng tin m\u00f4 t\u1ea3 s\u1ea3n ph\u1ea9m v\u00e0 gi\u00e1 tr\u1ecb thu\u1ed9c t\u00ednh c\u1ee7a n\u00f3.\n# tr\u01b0\u1eddng product_description_attributes = product_description + product_attributes\ndt_des_attr['product_description_attributes'] = dt_des_attr['product_description'] + ' ' + dt_des_attr['product_attributes']\n\n# x\u00f3a 2 tr\u01b0\u1eddng c\u0169\ndt_des_attr = dt_des_attr.drop(['product_description', 'product_attributes'], axis=1)","8a7ad50d":"# k\u1ebft qu\u1ea3 sau khi n\u1ed1i.\ndt_des_attr.head(5)","c3037a98":"# n\u1ed1i t\u1eadp dt_train v\u1edbi dt_des_attr\ndt_train = pd.merge(dt_train, dt_des_attr, on='product_uid', how='left')\n\n# n\u1ed1i t\u1eadp dt_test v\u1edbi t\u1eadp dt_des_attr\ndt_test = pd.merge(dt_test, dt_des_attr, on='product_uid', how='left')","c35117ad":"#show k\u1ebft qu\u1ea3\ndt_train.head(5)","c5d235a1":"#show k\u1ebft qu\u1ea3\ndt_test.head(5)","076a5527":"#l\u1ea5y d\u1eef li\u1ec7u t\u1eeb file .json \u0111\u1ec3 s\u1eeda l\u1ed7i ch\u00ednh t\u1ea3 \n#thay c\u00e1c gi\u00e1 tr\u1ecb gi\u1ed1ng tr\u01b0\u1eddng key b\u1eb1ng tr\u01b0\u1eddng value\nspell_check = json.load(open('..\/input\/spell-check-dict-kimoanh29\/spell_check.json', 'r'))\ndef speel_fix(string):\n    for (k,v) in spell_check.items():\n        string = string.replace(k,v)\n    return string\n","d2b1c5e5":"def standardized_sentences(string):\n    string = string.lower()\n\n    # g\u1ee1 c\u00e1c k\u00fd hi\u1ec7u s\u1ea3n ph\u1ea9m kh\u00f4ng li\u00ean quan\n    string = string.replace('-', '')\n    string = string.replace('in.', 'inch')\n    string = string.replace('ft.', 'foot')\n    string = string.replace('-oz. ', ' ')\n    string = string.replace('oz.', ' ')\n    string = string.replace('sq.', ' ')\n    string = string.replace('Gal.', ' ')\n    string = string.replace('lb.', ' ')\n    string = string.replace('cu.', ' ')\n    string = string.replace('O.D.', ' ')\n    string = string.replace('sq.', ' ')\n    string = string.replace('st.', ' ')\n    string = string.replace('lb.', ' ')\n    string = string.replace('Dia.', ' ')\n    string = string.replace('dia.', ' ')\n\n    # Thay th\u1ebf t\u1ea5t c\u1ea3 c\u00e1c k\u00fd t\u1ef1 \u0111\u1eb7c bi\u1ec7c b\u1eb1ng 1 space\n    string = re.sub(r'[^a-zA-Z0-9]+', ' ', string)\n\n    # Thay th\u1ebf c\u00e1c k\u00fd t\u1ef1 \u0111\u1ee9ng m\u1ed9t m\u00ecnh b\u1eb1ng 1 space\n    string = re.sub(r'\\b[a-zA-Z]\\b', ' ', string)\n\n    # G\u1ed9p c\u00e1c space \u0111\u1ee9ng li\u1ec1n nhau th\u00e0nh 1 space\n    string = re.sub(r'\\s+', ' ', string)\n   \n    return string","19838555":"def standardized_search_term(string):\n    string = string.lower()\n\n    # g\u1ee1 c\u00e1c k\u00fd hi\u1ec7u s\u1ea3n ph\u1ea9m kh\u00f4ng li\u00ean quan\n    string = string.replace('in.', 'inch')\n    string = string.replace('ft.', 'foot')\n    string = string.replace('ft', 'foot')\n\n    # key word b\u1ecb vi\u1ebft ri\u00eang l\u1ebb\n    string = string.replace('r ', 'r')\n\n    # Thay th\u1ebf t\u1ea5t c\u1ea3 c\u00e1c k\u00fd t\u1ef1 \u0111\u1eb7c bi\u1ec7c b\u1eb1ng 1 space\n    string = re.sub(r'[^a-zA-Z0-9]+', ' ', string)\n\n    # G\u1ed9p c\u00e1c space \u0111\u1ee9ng li\u1ec1n nhau th\u00e0nh 1 space\n    string = re.sub(r'\\s+', ' ', string)\n\n    return string","83985234":"# \u0110\u01b0a c\u00e1c t\u1eeb v\u1ec1 t\u1eeb g\u1ed1c c\u1ee7a n\u00f3.\n# \u00e1p d\u1ee5ng h\u00e0m n\u00e0y cho c\u1ea3 3 tr\u01b0\u1eddng d\u1eef li\u1ec7u title, search_term v\u00e0 des_attr\ndef set_root_form(string):\n    lemmatizer = WordNetLemmatizer()\n    return ' '.join(map(lambda x: lemmatizer.lemmatize(x), list(map(lambda x: stemmer.stem(x), string.split()))))","ff1889d2":"# g\u1ee1 c\u00e1c stopwords\n# h\u00e0m n\u00e0y ch\u1ec9 th\u1ef1c hi\u1ec7n \u1edf tr\u01b0\u1eddng m\u00f4 t\u1ea3 v\u00e0 thu\u1ed9c t\u00ednh\ndef remove_stopwords(string):\n    return ' '.join([w for w in string.split() if not w in stop_words])","dbc28a6e":"# S\u1eeda l\u1ed7i ch\u00ednh t\u1ea3 cho c\u00e1c tr\u01b0\u1eddng search_term, product_title v\u00e0 product_description_attributes\ndt_train['search_term'] = dt_train['search_term'].map(lambda x:speel_fix(x))\ndt_train['product_title'] = dt_train['product_title'].map(lambda x:speel_fix(x))\ndt_train['product_description_attributes'] = dt_train['product_description_attributes'].map(lambda x:speel_fix(x))","a1872281":"# chu\u1ea9n h\u00f3a tr\u01b0\u1eddng search_term\ndt_train['search_term'] = [standardized_search_term(_) for _ in dt_train.search_term]","130bf440":"# chu\u1ea9n h\u00f3a tr\u01b0\u1eddng product_title v\u00e0 product_description_attributes\ndt_train['product_title'] = [standardized_sentences(_) for _ in dt_train.product_title]\ndt_train['product_description_attributes'] = [standardized_sentences(_) for _ in dt_train.product_description_attributes]","a2af0551":"# g\u1ee1 c\u00e1c stopwords trong tr\u01b0\u1eddng product_description_attributes\ndt_train['product_description_attributes'] = [remove_stopwords(_) for _ in dt_train.product_description_attributes]","c23095aa":"# \u0111\u01b0a c\u00e1c t\u1eeb v\u1ec1 d\u1ea1ng g\u1ed1c c\u1ee7a n\u00f3\ndt_train['search_term'] = [set_root_form(_) for _ in dt_train.search_term] \ndt_train['product_title'] = [set_root_form(_) for _ in dt_train.product_title]\ndt_train['product_description_attributes'] = [set_root_form(_) for _ in dt_train.product_description_attributes] ","c581b936":"#show k\u1ebft qu\u1ea3\ndt_train.head(5)","8be28f10":"# S\u1eeda l\u1ed7i ch\u00ednh t\u1ea3 cho c\u00e1c tr\u01b0\u1eddng search_term, product_title v\u00e0 product_description_attributes\ndt_test['search_term'] = dt_test['search_term'].map(lambda x:speel_fix(x))\ndt_test['product_title'] = dt_test['product_title'].map(lambda x:speel_fix(x))\ndt_test['product_description_attributes'] = dt_test['product_description_attributes'].map(lambda x:speel_fix(x))","2b2fcfcf":"# chu\u1ea9n h\u00f3a tr\u01b0\u1eddng search_term\ndt_test['search_term'] = [standardized_search_term(_) for _ in dt_test.search_term]","bfe80d8b":"# chu\u1ea9n h\u00f3a tr\u01b0\u1eddng product_title v\u00e0 product_description_attributes\ndt_test['product_title'] = [standardized_sentences(_) for _ in dt_test.product_title]\ndt_test['product_description_attributes'] = [standardized_sentences(_) for _ in dt_test.product_description_attributes]","4b069214":"# g\u1ee1 c\u00e1c stopwords trong tr\u01b0\u1eddng product_description_attributes\ndt_test['product_description_attributes'] = [remove_stopwords(_) for _ in dt_test.product_description_attributes]","93085582":"# \u0111\u01b0a c\u00e1c c\u00e1c t\u1eeb v\u1ec1 d\u1ea1ng g\u1ed1c c\u1ee7a n\u00f3\ndt_test['search_term'] = [set_root_form(_) for _ in dt_test.search_term]\ndt_test['product_title'] = [set_root_form(_) for _ in dt_test.product_title]\ndt_test['product_description_attributes'] = [set_root_form(_) for _ in dt_test.product_description_attributes]","05b5461b":"#show k\u1ebft qu\u1ea3\ndt_test.head(5)","4f574c4b":"def correlation(dt_sample, dt_field, transform=True):\n    # L\u1ea5y \u0111\u1ed9 d\u00e0i c\u1ee7a tr\u01b0\u1eddng th\u00f4ng tin\n    # \u0110\u1eb7t transform=True th\u00ec sau n\u00e0y  ch\u00fang ta c\u00f3 th\u1ec3 s\u1eed d\u1ee5ng transform=False \u0111\u1ec3 l\u1ea5y ch\u00ednh d\u1eef li\u1ec7u \u1edf tr\u01b0\u1eddng \u0111\u00f3 m\u00e0 kh\u00f4ng ph\u1ea3i l\u00e0 \u0111\u1ed9 d\u00e0i :) do \u0111\u00f3 m\u00e0 c\u00f3 th\u1ec3 xem \u0111\u01b0\u1ee3c s\u1ef1 \u1ea3nh h\u01b0\u1edfng c\u1ee7a ch\u00ednh tr\u01b0\u1eddng d\u1eef li\u1ec7u \u0111\u00f3 l\u00ean relevance\n    x_ar = np.array(dt_sample[dt_field].map(lambda x:len(str(x).split())).astype(np.int64)) if transform else dt_sample[dt_field]\n   \n    # L\u1ea5y relevance t\u01b0\u01a1ng \u1ee9ng\n    y_ar = np.array(dt_sample['relevance'])\n   \n    # V\u1ebd c\u00e1c \u0111i\u1ec3m (\u0111\u1ed9 d\u00e0i, relevance) l\u00ean \u0111\u1ed3 th\u1ecb\n    plt.plot(x_ar, y_ar, 'bo')\n   \n    # t\u00ednh to\u00e1n m v\u00e0 b \u0111\u1ec3 v\u1ebd \u0111\u01b0\u1eddng regression\n    m, b = np.polyfit(x_ar, y_ar, 1)\n   \n    # V\u1ebd regression\n    plt.plot(x_ar, m * x_ar + b,'r')","98945d9f":"# t\u1eadp dt_train raw ch\u01b0a b\u1ecb bi\u1ebfn \u0111\u1ed5i\ndt_raw_train = pd.read_csv(\".\/train.csv\", encoding=\"ISO-8859-1\")\n\n# bi\u1ec3u di\u1ec5n v\u1edbi tr\u01b0\u1eddng product_title\ncorrelation(dt_raw_train, 'product_title')","903e1d46":"# bi\u1ec3u di\u1ec5n v\u1edbi tr\u01b0\u1eddng search_term\ncorrelation(dt_raw_train, 'search_term')","d43d47ea":"# bi\u1ec3u di\u1ec5n v\u1edbi tr\u01b0\u1eddng product_description_attributes\ncorrelation(dt_train, 'product_description_attributes')","ed330cd8":"# T\u00ecm c\u00e1c tokens chung \n# t\u00e1ch tokens c\u1ee7a 2 c\u00e2u b\u1eb1ng h\u00e0m split() r\u1ed3i l\u1ea7n l\u01b0\u1ee3t t\u00ednh s\u1ef1 xu\u1ea5t hi\u1ec7n c\u1ee7a tokens \u1edf c\u00e2u 1 trong c\u00e2u 2\ndef str_common_tokens(sentence_1, sentence_2):\n    return sum(1 for word in str(sentence_2).split() if word in set(str(sentence_1).split()))","e91eb281":"# C\u00e1c t\u1eeb chung m\u1ed9t ph\u1ea7n n\u00e0o \u0111\u1ea5y\n# \u0111\u1ec3 l\u1ea5y c\u00e1c t\u1eeb chung m\u1ed9t ph\u1ea7n th\u00ec ta kh\u00f4ng t\u00e1ch tokens (ko d\u00f9ng h\u00e0m split() \u1edf \u0111\u00e2y)\n# l\u1ea7n l\u01b0\u1ee3t t\u00ednh s\u1ef1 xu\u1ea5t hi\u1ec7n c\u1ee7a c\u00e1c t\u1eeb trong c\u00e2u 1 \u1edf c\u00e2u 2\ndef str_common_word(sentence_1, sentence_2):\n    return sum(1 for word in str(sentence_2) if word in set(sentence_1))","e5358bf2":"# T\u00ednh t\u1ed5ng t\u1ea5t c\u1ea3 c\u00e1c Tokens xu\u1ea5t hi\u1ec7n \"to\u00e0n ph\u1ea7n\"\ndef set_shared_words_whole(row_data):\n    return str_common_tokens(row_data[0], row_data[1])","95042256":"# T\u00ednh t\u1ed5ng t\u1ea5t c\u1ea3 c\u00e1c T\u1eeb xu\u1ea5t hi\u1ec7n \"m\u1ed9t ph\u1ea7n\"\ndef set_shared_words_part(row_data):\n    return str_common_word(row_data[0], row_data[1])","db45d820":"# t\u00ednh \u0111\u1ed9 d\u00e0i c\u1ee7a search_term\ndt_train['len_of_querry'] = [len(_.split()) for _ in dt_train['search_term'].values]","a7763a84":"# t\u00ednh t\u1ed5ng s\u1ed1 l\u1ea7n c\u00e1c tokens \u1edf tr\u01b0\u1eddng search_term xu\u1ea5t hi\u1ec7n to\u00e0n ph\u1ea7n \u1edf tr\u01b0\u1eddng product_title v\u00e0 product_description_attributes\ndt_train['shared_words_whole_st_pt'] = [set_shared_words_whole(_) for _ in  dt_train[['search_term','product_title']].values]\ndt_train['shared_words_whole_st_pdat'] = [set_shared_words_whole(_) for _ in  dt_train[['search_term','product_description_attributes']].values]","21b6015b":"# t\u00ednh t\u1ed5ng s\u1ed1 l\u1ea7n c\u00e1c tokens \u1edf tr\u01b0\u1eddng search_term xu\u1ea5t hi\u1ec7n m\u1ed9t ph\u1ea7n \u1edf tr\u01b0\u1eddng product_title v\u00e0 product_description_attributes\ndt_train['shared_words_part_st_pt'] = [set_shared_words_part(_) for _ in dt_train[['search_term', 'product_title']].values]\ndt_train['shared_words_part_st_pdat'] = [set_shared_words_part(_) for _ in dt_train[['search_term', 'product_description_attributes']].values]","ee2174f3":"# t\u00ednh \u0111\u1ed9 t\u01b0\u01a1ng \u0111\u1ed3ng c\u1ee7a search_term v\u00e0 product_title\ndt_train['similarity'] = [Levenshtein.ratio(_[0], _[1]) for _ in dt_train[['search_term', 'product_title']].values]","f2409e19":"#show k\u1ebft qu\u1ea3\ndt_train.head(5)","15fce196":"# t\u00ednh \u0111\u1ed9 d\u00e0i c\u1ee7a search_term\ndt_test['len_of_querry'] = [len(_.split()) for _ in dt_test['search_term'].values]","69369128":"# t\u00ednh t\u1ed5ng s\u1ed1 l\u1ea7n c\u00e1c tokens \u1edf tr\u01b0\u1eddng search_term xu\u1ea5t hi\u1ec7n to\u00e0n ph\u1ea7n \u1edf tr\u01b0\u1eddng product_title v\u00e0 product_description_attributes\ndt_test['shared_words_whole_st_pt'] = [set_shared_words_whole(_) for _ in dt_test[['search_term', 'product_title']].values]\ndt_test['shared_words_whole_st_pdat'] = [set_shared_words_whole(_) for _ in dt_test[['search_term', 'product_description_attributes']].values]","a8300a44":"# t\u00ednh t\u1ed5ng s\u1ed1 l\u1ea7n c\u00e1c tokens \u1edf tr\u01b0\u1eddng search_term xu\u1ea5t hi\u1ec7n m\u1ed9t ph\u1ea7n \u1edf tr\u01b0\u1eddng product_title v\u00e0 product_description_attributes\ndt_test['shared_words_part_st_pt'] = [set_shared_words_part(_) for _ in dt_test[['search_term', 'product_title']].values]\ndt_test['shared_words_part_st_pdat'] = [set_shared_words_part(_) for _ in dt_test[['search_term', 'product_description_attributes']].values]","7c8b1e8b":"# t\u00ednh \u0111\u1ed9 t\u01b0\u01a1ng \u0111\u1ed3ng c\u1ee7a search_term v\u00e0 product_title\ndt_test['similarity'] = [Levenshtein.ratio(_[0], _[1]) for _ in dt_test[['search_term', 'product_title']].values]","65095f30":"#show k\u1ebft qu\u1ea3\ndt_test.head(5)","2c638e7a":"correlation(dt_train, 'len_of_querry', transform=False)","aa238150":"correlation(dt_train, 'shared_words_whole_st_pt', transform=False)","cb63eba0":"correlation(dt_train, 'shared_words_whole_st_pdat', transform=False)","963ab0cd":"correlation(dt_train, 'shared_words_part_st_pt', transform=False)","251850d0":"correlation(dt_train, 'shared_words_part_st_pdat', transform=False)","e689e67b":"correlation(dt_train, 'similarity', transform=False)","52e4eb48":"# g\u1ee1 feature shared_words_part_st_pdat \u1edf 2 t\u1eadp train v\u00e0 test\ndt_train = dt_train.drop(['shared_words_part_st_pdat'],axis=1)\ndt_test = dt_test.drop(['shared_words_part_st_pdat'],axis=1)","71c0bb75":"# drop c\u00e1c c\u1ed9t text ch\u1ec9 \u0111\u1ec3 l\u1ea1i c\u00e1c c\u1ed9t ch\u1ee9a features\ndt_train = dt_train.drop(['product_title','search_term','product_description_attributes'],axis=1)\n\n# set x_train v\u00e0 y_train\ny_train = dt_train['relevance'].values\nX_train = dt_train.drop(['id','relevance'], axis=1).values","74ac7915":"#show k\u1ebft qu\u1ea3\ndt_train.head(5)","5411f947":"# \u1edf t\u1eadp test c\u0169ng g\u1ee1 h\u1ebft c\u00e1c tr\u01b0\u1eddng kh\u00e1c ch\u1ec9 \u0111\u1ec3 l\u1ea1i c\u00e1c tr\u01b0\u1eddng mang features\nX_test = dt_test.drop(['id','product_title','search_term','product_description_attributes'],axis=1).values\n\n# id test \u1edf \u0111\u00e2y ch\u00ednh l\u00e0 id c\u1ee7a c\u00e1c c\u1eb7p search_term v\u00e0 product title\nid_test = dt_test['id']","3010f8a4":"from sklearn.ensemble import RandomForestRegressor\n# b\u1ed9 tham s\u1ed1 \u0111\u01b0\u1ee3c tham kh\u1ea3o t\u1eeb ngu\u1ed3n kh\u00e1c\nrfr = RandomForestRegressor(n_estimators=30, n_jobs=-1, random_state=17, verbose=1.0, max_depth=10)\n\n# fit\nrfr.fit(X_train, y_train)\n\n# d\u1ef1 \u0111o\u00e1n v\u1edbi m\u00f4 h\u00ecnh v\u1eeba fit\ny_pred = rfr.predict(X_test)\n\n# xu\u1ea5t file csv\npd.DataFrame({\"id\": id_test, \"relevance\": y_pred}).to_csv('submission_ntko.csv',index=False)","d25c5cd6":"**\u0110\u00e1nh gi\u00e1 l\u1ea1i ch\u1ea5t l\u01b0\u1ee3ng c\u1ee7a c\u00e1c features b\u1eb1ng h\u00e0m correlation s\u1eed d\u1ee5ng t\u1eadp dt_train**","d309eeb2":"# **Vi\u1ebft h\u00e0m cho ph\u00e9p quan s\u00e1t s\u1ef1 \u1ea3nh h\u01b0\u1edfng c\u1ee7a \u0111\u1ed9 d\u00e0i c\u00e1c tr\u01b0\u1eddng t\u1edbi relevance**","5898e9a5":"# **Training data**\n* Ch\u1ee9a c\u00e1c tr\u01b0\u1eddng d\u1eef li\u1ec7u product_uid, product_title, search_term v\u00e0 relevance","ff88ad2e":"# **Random forest regressor**","b0beee8d":"* Ti\u1ebfn h\u00e0nh train t\u1eadp d\u1eef li\u1ec7u v\u1edbi m\u00f4 h\u00ecnh \"Random forest\" \u0111\u01b0\u1ee3c cung c\u1ea5p trong th\u01b0 vi\u1ec7n sklearn","c6aeb2cc":"# **Attributes data**\n* Ch\u1ee9a c\u00e1c tr\u01b0\u1eddng product_uid, name v\u00e0 value\n* Cung c\u1ea5p th\u00f4ng tin m\u1edf r\u1ed9ng v\u1ec1 m\u1ed9t t\u1eadp h\u1ee3p con c\u1ee7a c\u00e1c s\u1ea3n ph\u1ea9m (th\u01b0\u1eddng \u0111\u1ea1i di\u1ec7n cho c\u00e1c th\u00f4ng s\u1ed1 k\u1ef9 thu\u1eadt chi ti\u1ebft). Kh\u00f4ng ph\u1ea3i m\u1ecdi s\u1ea3n ph\u1ea9m s\u1ebd c\u00f3 c\u00e1c thu\u1ed9c t\u00ednh.","2b88701e":"# **Testing data**\n* Ch\u1ee9a c\u00e1c tr\u01b0\u1eddng d\u1eef li\u1ec7u product_uid, product_title, search_term\n","d243282e":"**C\u00e1c h\u00e0m Clean data**","1899f8fb":"# **Descriptions data**\n* Ch\u1ee9a c\u00e1c tr\u01b0\u1eddng product_uid v\u00e0 product_description\n* Ch\u1ee9a m\u00f4 t\u1ea3 v\u0103n b\u1ea3n c\u1ee7a t\u1eebng s\u1ea3n ph\u1ea9m.\n","0c526f58":"# **Clean t\u1eadp test**","5c053459":"**Vi\u1ebft c\u00e1c h\u00e0m l\u1ea5y \u0111\u1ec3 t\u1ea1o features t\u1eeb \u0111\u1ed9 xu\u1ea5t hi\u1ec7n c\u1ee7a c\u00e1c t\u1eeb trong search_term**","d526b200":"# **Extract features \u1edf t\u1eadp test**","ec9e717e":"# **Clean t\u1eadp train**","e22507f4":"# **Extract features \u1edf t\u1eadp train**","cb350b91":"# **Clean data**\n* Bi\u1ebfn \u0111\u1ed5i t\u1eadp dt_description v\u00e0 dt_attributes","c804607e":"**Nguy\u1ec5n Th\u1ecb Kim Oanh**\n\n**MSSV: 18020989**\n\n**L\u1edbp: K63K1**\n","4706daf7":"# **T\u00e1ch d\u1eef li\u1ec7u**","deb635a6":"# **M\u1ed9t s\u1ed1 h\u00e0m clean data kh\u00e1c**"}}