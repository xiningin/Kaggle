{"cell_type":{"30bd6a5b":"code","e67d50e2":"code","c5bc556b":"code","e8e602de":"code","5d8077fb":"code","3228e848":"code","e2c123ec":"code","9cdd4699":"code","04354676":"code","54cb2d7b":"code","c2f45c69":"code","4be975e3":"code","04530f2e":"code","8696aca3":"code","0ec9cc0a":"code","61b6c6f9":"code","116add46":"code","44a673e9":"code","1fc04eda":"code","8ed16413":"code","7a596c94":"code","68a352f0":"code","f22bf6a1":"code","0dee2a8d":"code","7a7ebb15":"code","b1c96d3d":"code","6c0f2369":"code","c3eb1037":"code","5cd8e183":"code","75cc68e3":"code","4e406c6b":"code","6efeddf6":"code","760934f8":"code","eba6bcf0":"code","e39f0785":"code","d808a2d8":"code","7dc21fb9":"code","8ffa807e":"code","c6835d20":"code","91282864":"code","29428437":"code","8ae3cd18":"code","6d08cdba":"code","e6236b9e":"code","febf44a3":"code","f10eaeba":"code","86b98510":"code","1811f7cb":"code","37606274":"code","6fd73b69":"code","1ec8e754":"code","f157c996":"code","436fdd14":"code","74625780":"code","dd5fd022":"code","4762f2ab":"markdown","218c5c20":"markdown","aa53d5b1":"markdown","a06ffc81":"markdown","ef0202fa":"markdown","572da020":"markdown","7387e978":"markdown","f9e3f011":"markdown","9ba7601c":"markdown","cb399e53":"markdown","71442336":"markdown","f974d5d7":"markdown","2887a490":"markdown","de6ab11a":"markdown","f83b72b7":"markdown","45fd544e":"markdown","bb256940":"markdown","ca19df59":"markdown","70a7c767":"markdown","e8c66054":"markdown","5bc4f38b":"markdown","19c3522b":"markdown","54cf4b88":"markdown","ace5a552":"markdown","23f2d19c":"markdown","5ff38693":"markdown","f53b87f1":"markdown","53c6114f":"markdown","efe78cc0":"markdown","f1f083e9":"markdown","3f95d999":"markdown","182001a6":"markdown","98270519":"markdown","8704dff7":"markdown","dba4c8db":"markdown","fa681b94":"markdown","4e71c002":"markdown","8f4f75a5":"markdown","ca60917c":"markdown","377d5d70":"markdown","5c6f93f5":"markdown","97f7b7d3":"markdown","337488fb":"markdown","6b4cabb8":"markdown","3a752ef2":"markdown","ccc1fabc":"markdown","9a126cc4":"markdown","d0533afd":"markdown","7dd7bf93":"markdown","1d69349d":"markdown","f60b4c81":"markdown","2e27cf15":"markdown","a3522d4e":"markdown","108b2054":"markdown","98c70044":"markdown","978365c9":"markdown","18d99bdc":"markdown","92d6dc52":"markdown","d86562ea":"markdown","b8806f66":"markdown"},"source":{"30bd6a5b":"# import useful libraries\nimport numpy as np #\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom time import time\nfrom sklearn.metrics import roc_auc_score\n\n\n# Import the supervised learning models from sklearn\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n#disable warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","e67d50e2":"data_train = pd.read_csv(\"..\/input\/census.csv\")\ndata_test = pd.read_csv(\"..\/input\/test_census.csv\").drop('Unnamed: 0',1)\n#make a copy of data_train to overwrite during feature engineering\ntrain = data_train[:]","c5bc556b":"# train and test set shape\nprint(\"Train set shape:\", train.shape)\nprint(\"Test set shape:\", data_test.shape)","e8e602de":"# first glance on train set\ntrain.head()","5d8077fb":"#first glance on test data\ndata_test.head()","3228e848":"# Check info for train and test dataset\ndata_train.info()\nprint(\"----------------------------------\")\ndata_test.info()","e2c123ec":"# Total number of records\nn_records = train.shape[0]\n\n# Print the results\nprint(\"Total number of records: {}\".format(n_records))\n","9cdd4699":"#explore unique values for income, to use it below \ndata_train.income.unique()","04354676":"# Number of records where individual's income is more than $50,000\nn_greater_50k = train[train.income == '>50K' ].shape[0]\n\n# Number of records where individual's income is at most $50,000\nn_at_most_50k = train[train.income == '<=50K' ].shape[0]\n\n# Percentage of individuals whose income is more than $50,000\ngreater_percent = round((n_greater_50k\/n_records)*100 ,2)\n\n# Print the results\nprint(\"Individuals making more than $50,000: {}\".format(n_greater_50k))\nprint(\"Individuals making at most $50,000: {}\".format(n_at_most_50k))\nprint(\"Percentage of individuals making more than $50,000: {}%\".format(greater_percent))","54cb2d7b":"# transform target into new variable called income:\nincome=data_train.income.map({'<=50K': 0, '>50K':1})\nincome.head()","c2f45c69":"# check how many unique values each feature has:\nfor column in data_train.columns:\n    print(column, len(train[column].unique()))","4be975e3":"categorical = ['workclass', 'education_level', 'marital-status', 'occupation', 'relationship', \n               'race', 'sex', 'native-country']\ncontinues = ['age', 'capital-gain', 'capital-loss', 'hours-per-week', 'education-num']","04530f2e":"# for each categorical features pring unique values:\nfor column in categorical:\n    print(column, train[column].unique())","8696aca3":"# plot disctribution and check skewness:\nfor column in continues:\n    a = sns.FacetGrid(train, aspect=4 )\n    a.map(sns.kdeplot, column, shade= True )\n    a.add_legend()\n    print('Skew for ',str(column), train[column].skew())","0ec9cc0a":"skewed = ['capital-gain', 'capital-loss']\n# Log-transform the skewed features (create function to use later for test set)\ndef log_transform(data):\n    return data[skewed].apply(lambda x: np.log(x + 1))\n    \ntrain[skewed] = log_transform(train)\n\n# Visualize the new log distributions\nfor column in skewed:\n    a = sns.FacetGrid(train, aspect=4 )\n    a.map(sns.kdeplot, column, shade= True )\n    a.add_legend()\n    print('Skew for ',str(column), train[column].skew())","61b6c6f9":"for column in continues:\n    sns.boxplot(train[column])\n    plt.show()","116add46":"from sklearn.preprocessing import MinMaxScaler\n#normalizing numerical features. Create function to use later on test data\n\ndef normalize(data):\n    \n    scaler = MinMaxScaler()\n    data=scaler.fit_transform(data[continues])\n    return data\n\ntrain[continues]= normalize(train)\ntrain.head(100)","44a673e9":"# One-hot encode thedata using pandas.get_dummies()\nfeatures_final = pd.get_dummies(train.drop(['income'],1))\n\n# Print the number of features after one-hot encoding\nencoded = list(features_final.columns)\nprint(\"{} total features after one-hot encoding.\".format(len(encoded)))\n","1fc04eda":"# check correlation between features: \ndata = pd.concat([features_final, income], axis =1)\nplt.figure(figsize=(30,28))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(data.corr(),linewidths=0.1,vmax=1.0, \n            square=True,linecolor='white')","8ed16413":"# Import train_test_split\nfrom sklearn.model_selection import train_test_split\n\n# Split the 'features' and 'income' data into training and testing sets\nX_train, X_val, y_train, y_val = train_test_split(features_final, \n                                                    income, \n                                                    test_size = 0.2, \n                                                    random_state = 0)\n\n# Show the results of the split\nprint(\"Training set has {} samples.\".format(X_train.shape[0]))\nprint(\"Testing set has {} samples.\".format(X_val.shape[0]))","7a596c94":"def evaluate(results):\n    \"\"\"\n    Visualization code to display results of various learners.\n    \n    inputs:\n      - learners: a list of supervised learners\n      - stats: a list of dictionaries of the statistic results from 'train_predict()'\n      - accuracy: The score for the naive predictor\n      - f1: The score for the naive predictor\n    \"\"\"\n  \n    # Create figure\n    fig, ax = plt.subplots(2, 2, figsize = (18,10))\n\n    # Constants\n    bar_width = 1\n    colors = ['r','g','b','c', 'm', 'y']\n    \n    # Super loop to plot four panels of data\n    for k, learner in enumerate(results.keys()):\n        for j, metric in enumerate(['train_time', 'roc_train',  'pred_time', 'roc_test']):\n\n                ax[j\/\/2, j%2].bar(k*bar_width, results[learner][metric], width = bar_width, color = colors[k])\n    \n    # Add unique y-labels\n    ax[0, 0].set_ylabel(\"Time (in seconds)\")\n    ax[0, 1].set_ylabel(\"ROC-AUC Score\")\n    ax[1, 0].set_ylabel(\"Time (in seconds)\")\n    ax[1, 1].set_ylabel(\"ROC-AUC Score\")\n    \n    # Add titles\n    ax[0, 0].set_title(\"Model Training\")\n    ax[0, 1].set_title(\"ROC-AUC Score on Training Subset\")\n    ax[1, 0].set_title(\"Model Predicting\")\n    ax[1, 1].set_title(\"ROC-AUC Score on Testing Set\")\n       \n    # Set y-limits for score panels\n    ax[0, 1].set_ylim((0, 1))\n    ax[1, 1].set_ylim((0, 1))\n\n    # Create patches for the legend\n    patches = []\n    for i, learner in enumerate(results.keys()):\n        patches.append(mpatches.Patch(color = colors[i], label = learner))\n    ax[1, 0].legend(handles = patches)\n    \n    # Aesthetics\n    plt.suptitle(\"Performance Metrics for Supervised Learning Models\", fontsize = 16, y = 1.10)\n    plt.tight_layout()\n    plt.show()","68a352f0":"# Display inline matplotlib plots with IPython\nfrom IPython import get_ipython\nget_ipython().run_line_magic('matplotlib', 'inline')\n\n\ndef train_predict(learner, X_train, y_train, X_test, y_test): \n    '''\n    inputs:\n       - learner: the learning algorithm to be trained and predicted on\n       - sample_size: the size of samples (number) to be drawn from training set\n       - X_train: features training set\n       - y_train: income training set\n       - X_val: features testing set\n       - y_val: income testing set\n    '''\n    \n    results = {}\n    \n    # Fit the learner to the training data \n    start = time() # Get start time\n    learner.fit(X_train, y_train)\n    end = time() # Get end time\n    \n    # Calculate the training time\n    results['train_time'] = end - start\n        \n    # Get the predictions on the test set(X_test),\n    #       then get predictions on the first 300 training samples(X_train) using .predict()\n    start = time() # Get start time\n    predictions_test = learner.predict(X_val)\n    predictions_train = learner.predict(X_train[:300])\n    end = time() # Get end time\n    \n    # Calculate the total prediction time\n    results['pred_time'] = end - start\n            \n    # Compute accuracy on the first 300 training samples which is y_train[:300]\n    results['roc_train'] = roc_auc_score(y_train[:300], predictions_train)\n        \n    # Compute accuracy on test set using accuracy_score()\n    results['roc_test'] = roc_auc_score(y_val, predictions_test)\n              \n    # Return the results\n    return results","f22bf6a1":"random_state =42\nn_estimators =100\n\n# Initialize the three models\nclf_A = GaussianNB()\nclf_B = KNeighborsClassifier()\nclf_C = LogisticRegression(random_state= random_state)\nclf_D = RandomForestClassifier(random_state= random_state, n_estimators = n_estimators)\nclf_E = GradientBoostingClassifier(n_estimators = n_estimators, random_state = random_state)\nclf_F = AdaBoostClassifier(n_estimators = n_estimators, random_state = random_state)\n\n# Collect results on the learners\nresults = {}\nfor clf in [clf_A, clf_B, clf_C, clf_D, clf_E, clf_F]:\n    clf_name = clf.__class__.__name__\n    results[clf_name] = {}\n    results[clf_name] = train_predict(clf, X_train, y_train, X_val, y_val)","0dee2a8d":"# Run metrics visualization for the three supervised learning models chosen\nevaluate(results)","7a7ebb15":"# Import 'GridSearchCV', 'make_scorer', and any other necessary libraries\nfrom sklearn.metrics import make_scorer \nfrom sklearn.model_selection import GridSearchCV","b1c96d3d":"clf = AdaBoostClassifier(random_state = random_state)\n\n# Create the parameters list you wish to tune, using a dictionary if needed.\nparameters = {'n_estimators': range(20,1021,100)}\n\n#Make an fbeta_score scoring object using make_scorer()\nscorer = make_scorer(roc_auc_score)\n\n# Perform grid search on the classifier using 'scorer' as the scoring method using GridSearchCV()\ngrid_obj = GridSearchCV(clf, param_grid = parameters, scoring= scorer,  verbose=1, n_jobs =-1)\n\n#  Fit the grid search object to the training data and find the optimal parameters using fit()\ngrid_fit = grid_obj.fit(X_train, y_train)\n\n# Get the estimator\nbest_clf = grid_fit.best_estimator_\n\n# Make predictions using the unoptimized and model\npredictions = (clf.fit(X_train, y_train)).predict(X_val)\nbest_predictions_val = best_clf.predict(X_val)\nbest_predictions_train = best_clf.predict(X_train)\n\n# Report the before-and-afterscores\nprint(\"Unoptimized model\\n------\")\nprint(\"ROC-AUC on validation data: {:.4f}\".format(roc_auc_score(y_val, predictions)))\nprint(\"\\nOptimized Model\\n------\")\nprint(\"Final ROC-AUC on the validation data: {:.4f}\".format(roc_auc_score(y_val, best_predictions_val)))\nprint(\"Final ROC-AUC on the training data: {:.4f}\".format(roc_auc_score(y_train, best_predictions_train)))\nprint(\"Optimal parameters:\", grid_obj.best_params_)","6c0f2369":"clf = AdaBoostClassifier(random_state = random_state)\n\n# Create the parameters list you wish to tune, using a dictionary if needed.\nparameters = { 'n_estimators': range(1000,1501,100)}\n\n# Perform grid search on the classifier using 'scorer' as the scoring method using GridSearchCV()\ngrid_obj = GridSearchCV(clf, param_grid = parameters, scoring= scorer,  verbose=1, n_jobs =-1)\n\n#  Fit the grid search object to the training data and find the optimal parameters using fit()\ngrid_fit = grid_obj.fit(X_train, y_train)\n\n# Get the estimator\nbest_clf = grid_fit.best_estimator_\n\n# Make predictions using the unoptimized and model\npredictions = (clf.fit(X_train, y_train)).predict(X_val)\nbest_predictions_val = best_clf.predict(X_val)\nbest_predictions_train = best_clf.predict(X_train)\n\n# Report the before-and-afterscores\nprint(\"Unoptimized model\\n------\")\nprint(\"ROC-AUC on validation data: {:.4f}\".format(roc_auc_score(y_val, predictions)))\nprint(\"\\nOptimized Model\\n------\")\nprint(\"Final ROC-AUC on the validation data: {:.4f}\".format(roc_auc_score(y_val, best_predictions_val)))\nprint(\"Final ROC-AUC on the training data: {:.4f}\".format(roc_auc_score(y_train, best_predictions_train)))\nprint(\"Optimal parameters:\", grid_obj.best_params_)","c3eb1037":"clf = AdaBoostClassifier(random_state = random_state)\n\n# Create the parameters list you wish to tune, using a dictionary if needed.\nparameters = { 'n_estimators': range(2000,3001,200),\n              'learning_rate': [0.5]}\n\n# Perform grid search on the classifier using 'scorer' as the scoring method using GridSearchCV()\ngrid_obj = GridSearchCV(clf, param_grid = parameters, scoring= scorer,  verbose=1, n_jobs =-1)\n\n#  Fit the grid search object to the training data and find the optimal parameters using fit()\ngrid_fit = grid_obj.fit(X_train, y_train)\n\n# Get the estimator\nbest_clf = grid_fit.best_estimator_\n\n# Make predictions using the unoptimized and model\npredictions = (clf.fit(X_train, y_train)).predict(X_val)\nbest_predictions_val = best_clf.predict(X_val)\nbest_predictions_train = best_clf.predict(X_train)\n\n# Report the before-and-afterscores\nprint(\"Unoptimized model\\n------\")\nprint(\"ROC-AUC on validation data: {:.4f}\".format(roc_auc_score(y_val, predictions)))\nprint(\"\\nOptimized Model\\n------\")\nprint(\"Final ROC-AUC on the validation data: {:.4f}\".format(roc_auc_score(y_val, best_predictions_val)))\nprint(\"Final ROC-AUC on the training data: {:.4f}\".format(roc_auc_score(y_train, best_predictions_train)))\nprint(\"Optimal parameters:\", grid_obj.best_params_)","5cd8e183":"clf = AdaBoostClassifier(n_estimators=4000,random_state = random_state, learning_rate = 0.5)\nclf.fit(X_train, y_train)\nbest_predictions_val_ab = clf.predict(X_val)\nbest_predictions_train_ab = clf.predict(X_train)\nprint(\"Final ROC-AUC on the validation data: {:.4f}\".format(roc_auc_score(y_val, best_predictions_val_ab)))\nprint(\"Final ROC-AUC on the training data: {:.4f}\".format(roc_auc_score(y_train, best_predictions_train_ab)))","75cc68e3":"#fit model with optimal parameters found during gridsearch:\nclf_AB = AdaBoostClassifier(n_estimators=1200,random_state = random_state)\nclf_AB.fit(X_train, y_train)\n# predict outcome using predict_probe instead of predict function:\nprobs_train_ab = clf_AB.predict_proba(X_train)[:, 1]\nprobs_val_ab = clf_AB.predict_proba(X_val)[:, 1]\nprint(\"score train: {}\".format(roc_auc_score(y_train, probs_train_ab)))\nprint(\"score validation: {}\".format(roc_auc_score(y_val, probs_val_ab)))","4e406c6b":"# Initialize the classifier\nclf = GradientBoostingClassifier(random_state = random_state)\n\n# Create the parameters list you wish to tune, using a dictionary if needed.\nparameters = {  'n_estimators': range(20,101,20),\n                'learning_rate':[0.2],\n                'min_samples_split': [500],\n                'min_samples_leaf' : [50],\n                'max_depth' : [8],\n                'subsample' : [0.8]}\n\n#Make an fbeta_score scoring object using make_scorer()\nscorer = make_scorer(roc_auc_score)\n\n# Perform grid search on the classifier using 'scorer' as the scoring method using GridSearchCV()\ngrid_obj = GridSearchCV(clf, param_grid = parameters, scoring= scorer,verbose=1, n_jobs =-1)\n\n#  Fit the grid search object to the training data and find the optimal parameters using fit()\ngrid_fit = grid_obj.fit(X_train, y_train)\n\n# Get the estimator\nbest_clf = grid_fit.best_estimator_\n\n# Make predictions using the unoptimized and model\npredictions = (clf.fit(X_train, y_train)).predict(X_val)\nbest_predictions_val = best_clf.predict(X_val)\nbest_predictions_train = best_clf.predict(X_train)\n\n# Report the before-and-afterscores\nprint(\"Unoptimized model\\n------\")\nprint(\"ROC-AUC on validation data: {:.4f}\".format(roc_auc_score(y_val, predictions)))\nprint(\"\\nOptimized Model\\n------\")\nprint(\"Final ROC-AUC on the validation data: {:.4f}\".format(roc_auc_score(y_val, best_predictions_val)))\nprint(\"Final ROC-AUC on the validation data: {:.4f}\".format(roc_auc_score(y_train, best_predictions_train)))\nprint(\"Optimal parameters:\", grid_obj.best_params_)","6efeddf6":"# Initialize the classifier\nclf = GradientBoostingClassifier(random_state = random_state)\n\n# Create the parameters list you wish to tune, using a dictionary if needed\nparameters = {'max_depth':range(2,12,2), \n              'min_samples_split':range(100,601,100),\n              'n_estimators': [80],\n              'learning_rate':[0.2],                \n              'min_samples_leaf' : [50],\n              'subsample' : [0.8]\n              }\n\n#Make an fbeta_score scoring object using make_scorer()\nscorer = make_scorer(roc_auc_score)\n\n# Perform grid search on the classifier using 'scorer' as the scoring method using GridSearchCV()\ngrid_obj = GridSearchCV(clf, param_grid = parameters, scoring= scorer,  verbose=1, n_jobs =-1)\n\n#  Fit the grid search object to the training data and find the optimal parameters using fit()\ngrid_fit = grid_obj.fit(X_train, y_train)\n\n# Get the estimator\nbest_clf = grid_fit.best_estimator_\n\n# Make predictions using the unoptimized and model\npredictions = (clf.fit(X_train, y_train)).predict(X_val)\nbest_predictions_val = best_clf.predict(X_val)\nbest_predictions_train = best_clf.predict(X_train)\n\n# Report the before-and-afterscores\nprint(\"Unoptimized model\\n------\")\nprint(\"ROC-AUC on validation data: {:.4f}\".format(roc_auc_score(y_val, predictions)))\nprint(\"\\nOptimized Model\\n------\")\nprint(\"Final ROC-AUC on the validation data: {:.4f}\".format(roc_auc_score(y_val, best_predictions_val)))\nprint(\"Final ROC-AUC on the training data: {:.4f}\".format(roc_auc_score(y_train, best_predictions_train)))\nprint(\"Optimal parameters:\", grid_obj.best_params_)","760934f8":"# Initialize the classifier\nclf = GradientBoostingClassifier(random_state = random_state)\n\n# Create the parameters list you wish to tune, using a dictionary if needed.\nparameters = {'min_samples_leaf':range(10,71,10),\n              'max_depth': [6], \n              'min_samples_split': [200],\n              'n_estimators': [80],\n              'learning_rate':[0.2],\n              'subsample' : [0.8]}\n\n#Make an fbeta_score scoring object using make_scorer()\nscorer = make_scorer(roc_auc_score)\n\n# Perform grid search on the classifier using 'scorer' as the scoring method using GridSearchCV()\ngrid_obj = GridSearchCV(clf, param_grid = parameters, scoring= scorer, verbose=1, n_jobs =-1)\n\n\n# Fit the grid search object to the training data and find the optimal parameters using fit()\ngrid_fit = grid_obj.fit(X_train, y_train)\n\n# Get the estimator\nbest_clf = grid_fit.best_estimator_\n\n# Make predictions using the unoptimized and model\npredictions = (clf.fit(X_train, y_train)).predict(X_val)\nbest_predictions_val = best_clf.predict(X_val)\nbest_predictions_train = best_clf.predict(X_train)\n\n# Report the before-and-afterscores\nprint(\"Unoptimized model\\n------\")\nprint(\"ROC-AUC on validation data: {:.4f}\".format(roc_auc_score(y_val, predictions)))\nprint(\"\\nOptimized Model\\n------\")\nprint(\"Final ROC-AUC on the validation data: {:.4f}\".format(roc_auc_score(y_val, best_predictions_val)))\nprint(\"Final ROC-AUC on the validation data: {:.4f}\".format(roc_auc_score(y_train, best_predictions_train)))\nprint(\"Optimal parameters:\", grid_obj.best_params_)","eba6bcf0":"# Initialize the classifier\nclf = GradientBoostingClassifier(random_state = random_state)\n\n# Create the parameters list you wish to tune, using a dictionary if needed.\nparameters = {'subsample':[0.6,0.7,0.75,0.8,0.85,0.9],\n              'min_samples_leaf': [50],\n              'max_depth': [6], \n              'min_samples_split': [200],\n              'n_estimators': [80],\n              'learning_rate':[0.2]}\n\n#Make an fbeta_score scoring object using make_scorer()\nscorer = make_scorer(roc_auc_score)\n\n# Perform grid search on the classifier using 'scorer' as the scoring method using GridSearchCV()\ngrid_obj = GridSearchCV(clf, param_grid = parameters, scoring= scorer, verbose=1, n_jobs =-1)\n\n# Fit the grid search object to the training data and find the optimal parameters using fit()\ngrid_fit = grid_obj.fit(X_train, y_train)\n\n# Get the estimator\nbest_clf = grid_fit.best_estimator_\n\n# Make predictions using the unoptimized and model\npredictions = (clf.fit(X_train, y_train)).predict(X_val)\nbest_predictions_val = best_clf.predict(X_val)\nbest_predictions_train = best_clf.predict(X_train)\n\n# Report the before-and-afterscores\nprint(\"Unoptimized model\\n------\")\nprint(\"ROC-AUC on validation data: {:.4f}\".format(roc_auc_score(y_val, predictions)))\nprint(\"\\nOptimized Model\\n------\")\nprint(\"Final ROC-AUC on the validation data: {:.4f}\".format(roc_auc_score(y_val, best_predictions_val)))\nprint(\"Final ROC-AUC on the validation data: {:.4f}\".format(roc_auc_score(y_train, best_predictions_train)))\nprint(\"Optimal parameters:\", grid_obj.best_params_)","e39f0785":"# Initialize the classifier\nclf = GradientBoostingClassifier(random_state = random_state)\n\n# Create the parameters list you wish to tune, using a dictionary if needed.\nparameters = {'subsample':[0.8],\n              'min_samples_leaf': [50],\n              'max_depth': [6], \n              'min_samples_split': [200],\n              'n_estimators': range(140, 241, 20),\n              'learning_rate':[0.1]}\n\n#Make an fbeta_score scoring object using make_scorer()\nscorer = make_scorer(roc_auc_score)\n\n# Perform grid search on the classifier using 'scorer' as the scoring method using GridSearchCV()\ngrid_obj = GridSearchCV(clf, param_grid = parameters, scoring= scorer, verbose=1, n_jobs =-1)\n\n#  Fit the grid search object to the training data and find the optimal parameters using fit()\ngrid_fit = grid_obj.fit(X_train, y_train)\n\n# Get the estimator\nbest_clf = grid_fit.best_estimator_\n\n# Make predictions using the unoptimized and model\npredictions = (clf.fit(X_train, y_train)).predict(X_val)\nbest_predictions_val = best_clf.predict(X_val)\nbest_predictions_train = best_clf.predict(X_train)\n\n# Report the before-and-afterscores\nprint(\"Unoptimized model\\n------\")\nprint(\"ROC-AUC on validation data: {:.4f}\".format(roc_auc_score(y_val, predictions)))\nprint(\"\\nOptimized Model\\n------\")\nprint(\"Final ROC-AUC on the validation data: {:.4f}\".format(roc_auc_score(y_val, best_predictions_val)))\nprint(\"Final ROC-AUC on the validation data: {:.4f}\".format(roc_auc_score(y_train, best_predictions_train)))\nprint(\"Optimal parameters:\", grid_obj.best_params_)","d808a2d8":"# Initialize the classifier\nclf = GradientBoostingClassifier(random_state = random_state)\n\n# Create the parameters list you wish to tune, using a dictionary if needed.\nparameters = {'subsample':[0.8],\n              'min_samples_leaf': [50],\n              'max_depth': [6], \n              'min_samples_split': [200],\n              'n_estimators': range(360, 401, 20) ,\n              'learning_rate':[0.05]}\n\n#Make an fbeta_score scoring object using make_scorer()\nscorer = make_scorer(roc_auc_score)\n\n# Perform grid search on the classifier using 'scorer' as the scoring method using GridSearchCV()\ngrid_obj = GridSearchCV(clf, param_grid = parameters, scoring= scorer, verbose=1, n_jobs =-1)\n\n#  Fit the grid search object to the training data and find the optimal parameters using fit()\ngrid_fit = grid_obj.fit(X_train, y_train)\n\n# Get the estimator\nbest_clf = grid_fit.best_estimator_\n\n# Make predictions using the unoptimized and model\npredictions = (clf.fit(X_train, y_train)).predict(X_val)\nbest_predictions_val = best_clf.predict(X_val)\nbest_predictions_train = best_clf.predict(X_train)\n\n# Report the before-and-afterscores\nprint(\"Unoptimized model\\n------\")\nprint(\"ROC-AUC on validation data: {:.4f}\".format(roc_auc_score(y_val, predictions)))\nprint(\"\\nOptimized Model\\n------\")\nprint(\"Final ROC-AUC on the validation data: {:.4f}\".format(roc_auc_score(y_val, best_predictions_val)))\nprint(\"Final ROC-AUC on the validation data: {:.4f}\".format(roc_auc_score(y_train, best_predictions_train)))\nprint(\"Optimal parameters:\", grid_obj.best_params_)","7dc21fb9":"# fitting GradientBoostingClassifier with optimal parameters:\nclf_GB = GradientBoostingClassifier(random_state = random_state, subsample = 0.8, min_samples_leaf = 50,\n              max_depth = 6, min_samples_split = 200, n_estimators = 180, learning_rate = 0.1 )\nclf_GB.fit(X_train, y_train)\n\nbest_predictions_val_gb = clf_GB.predict(X_val)\nbest_predictions_train_gb = clf_GB.predict(X_train)\nprint(\"Final ROC-AUC on the validation data: {:.4f}\".format(roc_auc_score(y_val, best_predictions_val_gb)))\nprint(\"Final ROC-AUC on the training data: {:.4f}\".format(roc_auc_score(y_train, best_predictions_train_gb)))","8ffa807e":"# predict outcome using predict_probe instead of predict function:\nprobs_train_gb = clf_GB.predict_proba(X_train)[:, 1]\nprobs_val_gb = clf_GB.predict_proba(X_val)[:, 1]\nprint(\"score train: {}\".format(roc_auc_score(y_train, probs_train_gb)))\nprint(\"score test: {}\".format(roc_auc_score(y_val, probs_val_gb)))","c6835d20":"clf = LogisticRegression(random_state= random_state)\n\n# Create the parameters list you wish to tune, using a dictionary if needed.\nparameters = { 'C': [0.001, 0.01, 0.05, 0.1, 0.5, 0.7, 0.8, 0.9, 1, 5, 10, 20, 50]}\n\n#Make an fbeta_score scoring object using make_scorer()\nscorer = make_scorer(roc_auc_score)\n\n# Perform grid search on the classifier using 'scorer' as the scoring method using GridSearchCV()\ngrid_obj = GridSearchCV(clf, param_grid = parameters, scoring= scorer,  verbose=1, n_jobs =-1)\n\n#  Fit the grid search object to the training data and find the optimal parameters using fit()\ngrid_fit = grid_obj.fit(X_train, y_train)\n\n# Get the estimator\nclf_LR = grid_fit.best_estimator_\n\n# Make predictions using the unoptimized and model\npredictions = (clf.fit(X_train, y_train)).predict(X_val)\nbest_predictions_val_lr = clf_LR.predict(X_val)\nbest_predictions_train_lr = clf_LR.predict(X_train)\n\n# Report the before-and-afterscores\nprint(\"Unoptimized model\\n------\")\nprint(\"ROC-AUC on validation data: {:.4f}\".format(roc_auc_score(y_val, predictions)))\nprint(\"\\nOptimized Model\\n------\")\nprint(\"Final ROC-AUC on the validation data: {:.4f}\".format(roc_auc_score(y_val, best_predictions_val_lr)))\nprint(\"Final ROC-AUC on the training data: {:.4f}\".format(roc_auc_score(y_train, best_predictions_train_lr)))\nprint(\"Optimal parameters:\", grid_obj.best_params_)","91282864":"# predict outcome using predict_probe instead of predict function:\nprobs_train_lr = clf_LR.predict_proba(X_train)[:, 1]\nprobs_val_lr = clf_LR.predict_proba(X_val)[:, 1]\nprint(\"score train: {}\".format(roc_auc_score(y_train, probs_train_lr)))\nprint(\"score test: {}\".format(roc_auc_score(y_val, probs_val_lr)))","29428437":"clf = RandomForestClassifier(random_state= random_state)\n\n# Create the parameters list you wish to tune, using a dictionary if needed.\nparameters = { 'n_estimators': range(20,1020,100),\n                'max_depth': range(2, 10, 1)}\n\n#Make an fbeta_score scoring object using make_scorer()\nscorer = make_scorer(roc_auc_score)\n\n# Perform grid search on the classifier using 'scorer' as the scoring method using GridSearchCV()\ngrid_obj = GridSearchCV(clf, param_grid = parameters, scoring= scorer,  verbose=1, n_jobs =-1)\n\n#  Fit the grid search object to the training data and find the optimal parameters using fit()\ngrid_fit = grid_obj.fit(X_train, y_train)\n\n# Get the estimator\nclf_RF = grid_fit.best_estimator_\n\n# Make predictions using the unoptimized and model\npredictions = (clf.fit(X_train, y_train)).predict(X_val)\nbest_predictions_val_rf = clf_RF.predict(X_val)\nbest_predictions_train_rf = clf_RF.predict(X_train)\n\n# Report the before-and-afterscores\nprint(\"Unoptimized model\\n------\")\nprint(\"ROC-AUC on validation data: {:.4f}\".format(roc_auc_score(y_val, predictions)))\nprint(\"\\nOptimized Model\\n------\")\nprint(\"Final ROC-AUC on the validation data: {:.4f}\".format(roc_auc_score(y_val, best_predictions_val_rf)))\nprint(\"Final ROC-AUC on the training data: {:.4f}\".format(roc_auc_score(y_train, best_predictions_train_rf)))\nprint(\"Optimal parameters:\", grid_obj.best_params_)","8ae3cd18":"# predict outcome using predict_probe instead of predict function:\nprobs_train_rf = clf_RF.predict_proba(X_train)[:, 1]\nprobs_val_rf = clf_RF.predict_proba(X_val)[:, 1]\nprint(\"score train: {}\".format(roc_auc_score(y_train, probs_train_rf)))\nprint(\"score test: {}\".format(roc_auc_score(y_val, probs_val_rf)))","6d08cdba":"print(\"score train for Adaboost: {}\".format(roc_auc_score(y_train, probs_train_ab)))\nprint(\"score test for Adaboost: {}\".format(roc_auc_score(y_val, probs_val_ab)))\nprint(\"score train for Gradient Boosting: {}\".format(roc_auc_score(y_train, probs_train_gb)))\nprint(\"score test for Gradient Boosting: {}\".format(roc_auc_score(y_val, probs_val_gb)))\nprint(\"score train for Logistic Regression: {}\".format(roc_auc_score(y_train, probs_train_lr)))\nprint(\"score test for Logistic Regression: {}\".format(roc_auc_score(y_val, probs_val_lr)))\nprint(\"score train for Random Forest: {}\".format(roc_auc_score(y_train, probs_train_rf)))\nprint(\"score test for Random Forest: {}\".format(roc_auc_score(y_val, probs_val_rf)))\nprint(\"score train for top2 models: {}\".format(roc_auc_score(y_train, (probs_train_gb+probs_train_ab)\/2)))\nprint(\"score test for top2 models: {}\".format(roc_auc_score(y_val, (probs_val_gb+probs_val_ab)\/2)))\nprint(\"score train for top3 models: {}\".format(roc_auc_score(y_train, (probs_train_gb+probs_train_ab+probs_train_rf)\/3)))\nprint(\"score test for top3 models: {}\".format(roc_auc_score(y_val, (probs_val_gb+probs_val_ab+probs_val_rf)\/3)))\nprint(\"score train for all models: {}\".format(roc_auc_score(y_train, (probs_train_gb+probs_train_ab+\n                                                                      probs_train_rf+probs_train_lr)\/4)))\nprint(\"score test for all models: {}\".format(roc_auc_score(y_val, (probs_val_gb+probs_val_ab+\n                                                                      probs_val_rf+ probs_val_lr)\/4)))","e6236b9e":"# make a copy of data_test to overwrite duting feature engineering:\nX_test = data_test[:]","febf44a3":"X_test.info()","f10eaeba":"# fill missing values for numeric variables with approximatelly gaussian dictribution:\nfor col in ['age', 'education-num', 'hours-per-week']:\n    X_test[col]= X_test[col].fillna(data_train[col].mean())\n\n# fill missing values for numeric variables with skewed dictribution:\nfor col in ['capital-gain', 'capital-loss']:\n    X_test[col]= X_test[col].fillna(data_train[col].median())\n\n#fill missing categorical values with most freaquent category:\nfor col in categorical:\n    X_test[col]= X_test[col].fillna(data_train.groupby([col])[col].count().sort_values(ascending=False).index[0])\n    ","86b98510":"#check for missing values in X_test after filling them in:\nX_test.info()","1811f7cb":"#log transform skewed data\nX_test[skewed] = log_transform(X_test)\n\n#scale continues variables:\nX_test[continues]= normalize(X_test)\n\n# One-hot encode thedata using pandas.get_dummies()\nX_test_final = pd.get_dummies(X_test)\n\n# Print the number of features after one-hot encoding\nencoded = list(X_test_final.columns)\nprint(\"{} total features after one-hot encoding.\".format(len(encoded)))","37606274":"best_model = clf_GB\ntest = pd.read_csv(\"..\/input\/test_census.csv\")\n\ntest['id'] = test.iloc[:,0] \ntest['income'] = best_model.predict_proba(X_test_final)[:, 1]\n\ntest[['id', 'income']].to_csv(\"submissionGB.csv\", index=False)","6fd73b69":"best_model = clf_AB\ntest = pd.read_csv(\"..\/input\/test_census.csv\")\n\ntest['id'] = test.iloc[:,0] \ntest['income'] = best_model.predict_proba(X_test_final)[:, 1]\n\ntest[['id', 'income']].to_csv(\"submissionAB.csv\", index=False)","1ec8e754":"best_model = clf_LR\ntest = pd.read_csv(\"..\/input\/test_census.csv\")\n\ntest['id'] = test.iloc[:,0] \ntest['income'] = best_model.predict_proba(X_test_final)[:, 1]\n\ntest[['id', 'income']].to_csv(\"submissionLR.csv\", index=False)","f157c996":"best_model = clf_RF\ntest = pd.read_csv(\"..\/input\/test_census.csv\")\n\ntest['id'] = test.iloc[:,0] \ntest['income'] = best_model.predict_proba(X_test_final)[:, 1]\n\ntest[['id', 'income']].to_csv(\"submissionRF.csv\", index=False)","436fdd14":"test = pd.read_csv(\"..\/input\/test_census.csv\")\n\ntest['id'] = test.iloc[:,0] \ntest['income'] = (clf_GB.predict_proba(X_test_final)[:, 1] + clf_AB.predict_proba(X_test_final)[:, 1])\/2\n\ntest[['id', 'income']].to_csv(\"submission_top2.csv\", index=False)","74625780":"test = pd.read_csv(\"..\/input\/test_census.csv\")\n\ntest['id'] = test.iloc[:,0] \ntest['income'] = (clf_GB.predict_proba(X_test_final)[:, 1] + clf_AB.predict_proba(X_test_final)[:, 1]+\n                  clf_RF.predict_proba(X_test_final)[:, 1])\/3\n\ntest[['id', 'income']].to_csv(\"submission_top3.csv\", index=False)","dd5fd022":"test = pd.read_csv(\"..\/input\/test_census.csv\")\n\ntest['id'] = test.iloc[:,0] \ntest['income'] = (clf_GB.predict_proba(X_test_final)[:, 1] + clf_AB.predict_proba(X_test_final)[:, 1]+\n                clf_RF.predict_proba(X_test_final)[:, 1] + clf_LR.predict_proba(X_test_final)[:, 1])\/4\n\ntest[['id', 'income']].to_csv(\"submission_all.csv\", index=False)","4762f2ab":"## 5.1. Categorical Variables\n<a id=\"cat\"><\/a>\n","218c5c20":"Test data has multiple missing values that needs to be filled. \nFor different variable types we will use different missing value strategy:\n1. Numeric variables with appr gaussian disctribution: fill missing values with mean value from train set\n2. Numeric variables with skewed dictribution: fill missing values with median values from train set\n3. Catogorical data: fill values with most frequent cateory in train set\n","aa53d5b1":"#### Summary \n* **age**: continuous. \n* **workclass**: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked. \n* **education**: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool. \n* **education-num**: continuous. \n* **marital-status**: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse. \n* **occupation**: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces. \n* **relationship**: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried. \n* **race**: Black, White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other. \n* **sex**: Female, Male. \n* **capital-gain**: continuous. \n* **capital-loss**: continuous. \n* **hours-per-week**: continuous. \n* **native-country**: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.","a06ffc81":"Typically, learning algorithms expect input to be numeric, which requires that non-numeric features (called categorical variables) be converted.  \n\nThere are multiple ways to perform encoding:\n\n1. One-hot encoding:  \nkeeps all information about variable, but create a lot of new features  \n\n2. Binary encoding:  \nkeeps all information about variable, creates new dimensions, but less than one-hot encoder        \n             \n3. Ordinal numbering encoding:    \nkeeps semantical info about variable, but treats 2-1 as 3-2 and can be missleading, doesn't create new dimensions  \n\n4. Frequency encoding:  \ndoesn't create new dimensions, but depends on frequency rather than relation with target and that can lead to wrong predictions  \n\n5. Target guided encoding: ordinal, mean, and probability ratio encoding, Weight of evidence:  \ncreates monotonic relationship between variable and target, but can cause overfitting \n\nAll of this models have advantages and disadvantagies, and performance of particular encoding strategy for categorical features depends on amount of instances and features in data set, variable type and chosen prediction model. \n\nAccording to the info about unique values for categorical variables in current dataset, there are not so many unique values for each feature. Therefore we will use one-hot encoding strategy, where all categories of variables are converted into separate features. ","ef0202fa":"A dataset may sometimes contain at least one feature whose values tend to lie near a single number, but will also have a non-trivial number of vastly larger or smaller values than that single number.  Algorithms can be sensitive to such distributions of values and can underperform if the range is not properly normalized.  \n\nWe will perform three steps to preprocess continues data:\n    - check skew of data and perform log transform on skewed variables\n    - find outliers\n    - normalize data","572da020":"[](http:\/\/)With lower learning rate performance is worse, as the model is overfitting to the training set. Therefore we will leave learning rate of 0.1 and n_estimators = 180","7387e978":"For learning rate = 1 (default parmeter for Adaboost) optimal n_estimators is 1020- that is on the edge of search parameters. I will increase n_estimators further","f9e3f011":"## Part 8: Prediction and Submission\n<a id=\"pred\"><\/a>","9ba7601c":"Outliers can shift decision boundry for linear models significanlty, thats why is it inportant to handle them. Tree models are not sensitive for ourliers, but shifting outliers will not effect them in any way, so we will perform shifting for all models.  \nIn order to check for outliers we will make box plots for continues features.","cb399e53":"To properly evaluate the performance of each model chosen, it's important that you create a training and predicting pipeline that allows to quickly and effectively train models using various sizes of training data and perform predictions on the testing data. ","71442336":"### 6.3.2. Search for optimal Tree parameters\nNow lets move onto tuning the tree parameters. I plan to do this in following stages:\n\nTune max_depth and num_samples_split\nTune min_samples_leaf","f974d5d7":"## 6.2. Choosing optimal model\n<a id=\"choise\"><\/a>","2887a490":"Optimal max_depth = 6 and  min_samples_split =200 . We will freezee this values and proceed to min_samples_leaf grid search.","de6ab11a":"Split the data (both features and their labels) into training and test sets. 80% of the data will be used for training and 20% for testing.","f83b72b7":"### Transforming Skewed Continuous Features","45fd544e":"## Table of contents\n***\n- [Introduction](#introduction)\n- [Part 1: Importing Libraries](#Importing_Libraries)\n- [Part 2: Importing Dataset](#Importing_Dataset)\n- [Part 3: Explore Dataset](#Explore_Dataset)\n- [Part 4: Explore Target Variable](#target)\n- [Part 5: Explore and Engineer Features](#features)\n    - [5.1. Continues Variables](#continues)\n    - [5.2. Categorical Variables](#cat)\n- [Part 6: Modeling](#modeling)\n    - [6.1. Split Dataset](#split)\n    - [6.2. Choosing Optimal Model](#choise)\n    - [6.3. Parameter Tuning for AdaBoost Classifier](#tuneA)\n    - [6.4. Parameter Tuning for Gradient Boosting Classifier](#tuneB)\n    - [6.5. Logistic Regression](#knn)\n    - [6.6. Random Forest](#forest)\n    - [6.7. Comparison of Classifiers](#compare)\n- [Part 7: Preprocessing of Test Data](#test)\n- [Part 8: Prediction and Submission](#pred)","bb256940":"In this chaper, we will investigate six different algorithms that are suitable for Charity ML prediction, and determine which 3 are the best at modeling the data. Four of these algorithms will be supervised learners of choice for further parameter tuning and prediction.","ca19df59":"![](http:\/\/)For learning rate = 1 (default parmeter for Adaboost) optimal n_estimators is 1200. In order to make model better I will try to further decrease learning rate by half and increase n_estimates a liitle bit more than twice","70a7c767":"Applying log transformation on capital-gain and capital-loss helped to reduce skew, however, skew is still pretty high for this two features. ","e8c66054":"# Part 5: Explore and Engineer Features\n<a id=\"features\"><\/a>","5bc4f38b":"From the graphs it can be seen that all continues features have outliers. Outliers can be a problem for linear model performance. I will leave this outliers without preprocessing them, as they do not effect tree based models. ","19c3522b":"AdaBoost classifier doesn't have much parameters to tune, however, it is critical to find proper balance between n-estimators and learning rate. We will try to increase n-estimators and decrease learning rate slowly until we observe no further improvement in performance.","54cf4b88":"No missing values in X_test -->success\n\nNow we need to log transform, scale and convert X_test features to dummies:","ace5a552":"### Conclusion: \n\nThe best performance can be achieved by blend of best two models models: Gradient Boosting and Adaboost.   \nI will try to submit all four combinations though, to prove the concept and see how consistent is public leaderbord results with cross validation ","23f2d19c":"### Find outliers","5ff38693":"# Part 6: Modeling\n<a id=\"Importing_Libraries\"><\/a>","f53b87f1":"From claculated skew and plots it can be stated that capital-gain and capital-loss features are highly skewed. For highly-skewed feature distributions, it is common practice to apply a logarithmic transformation on the data so that the very large and very small values do not negatively affect the performance of a learning algorithm. Using a logarithmic transformation significantly reduces the range of values caused by outliers. Care must be taken when applying this transformation, however: the logarithm of `0` is undefined, so we must translate the values by a small amount above `0` to apply the the logarithm successfully.","53c6114f":"# Part 9: Results and Conclusion\n<a id=\"results\"><\/a>\n\n### Score on the Public Leaderboard\n\n1. Best model: 0.92542\n\n2. average between top 2 models: 0.93687 \n\n3. average between top 3 models: 0.93160\n\n4. average between all models: 0.93684\n\n### Conclusion\n\nThe best performance on Public Leaderbord is achieved by combination of best two models models: Gradien boost and Ada Boost. This result is consistent with result achived during crossvalidation. \n\n","efe78cc0":"## 6.6. Random Forest\n<a id=\"forest\"><\/a>","f1f083e9":"## 6.5. Logistic Regression\n<a id=\"logistic\"><\/a>","3f95d999":"50% decrease of learning rate improved model performance of validation set. We will try to decrease learning rate more, i.e to 0.05 and increase n_estimators slightly more tham twice","182001a6":"# Part 4: Explore Target Variable\n<a id=\"target\"><\/a>","98270519":"## 5.1. Continues Variables\n<a id=\"continues\"><\/a>","8704dff7":"# Part 7: Preprocessing of Test Data\n<a id=\"test\"><\/a>","dba4c8db":"Age, capital_gain, capital_loss, hours_per_week and education_num are continues features, rest are categorical. ","fa681b94":"Optimal n_estimators = 80 for learning_rate = 0.2. We will freezee this values for now and proceed to tree parameters grid search.","4e71c002":"### 6.3.1. Search optimal n_estimates","8f4f75a5":"### 6.3.3. Search for optimal subsample","ca60917c":"#### Information discovered about dataset:\n- dataset size: (45222, 103)\n- feature correlation - mostly low --> low linear relation between variables\n- income is unballanced, however, not so much, enough data for both classes\n- most of the features have low correlation with target, but around 10% of features have more than 0.4 correlation with target\n- continues features have outliers - negative effect on linear models performance","377d5d70":"A cursory investigation of the dataset will determine how many individuals fit into either group, and will tell us about the percentage of these individuals making more than \\$50,000. ","5c6f93f5":"## 6.7. Comparison of Classifiers\n<a id=\"compare\"><\/a>","97f7b7d3":"Performance did not improve by decreasing learning rate and increasing n_estimators. I will use learning_rate of 1 and n_estimators of 1200","337488fb":"According to information discovered about dataset:\n\n1. Hypothesis about computation complexity of models: \n    - Dataset is more than 10000 samples - too big to use SVC due to hight computation complexity of the model: O(n_samples^2 * n_features) for RBF kernel \n    - Random forest - low computation complexity (trees can be constracted in parallel)\n    - KNN - very big computation complexity in test time (each test point needs to be compared with each point in training set to find nearest neighbor) \n    - Gradient Boost - computation complexity depends on n_estimators, as it is sequential model\n    - Decision tree, logictic regression and Naive Bayes - low computation complexity \n\n    \n2. Hypothesis about model performance:\n    - Linear models - medium performance as there is not so much correlation in dataset \n    - Ensemble models are in general good performers, especially with optimal parameters\n    - KNN - usually good performer\n    - Decision tree - in general good performer, but has overfitting problem\n    - Naive Bayes - low performance due to assumption for independence in data - not realistic in real world\n    \nAccording to all mentioned above, my assumption is that ensemble models will outperform all other models in performance and still provide feasible comptutation complexity.   \nI will check 3 of them : AdaBoost, Random Forest, and Gradient Boosting.\nHowever, I will also check 3 other models to confirm correctness of my assumptions: KNN, Logictic regression and Naive Bayes.\nI will leave SVC out of scope due to very high computation complexity.\n\n1. Ensemble Methods (Bagging, AdaBoost, Random Forest, Gradient Boosting): \n    - Pros: no overfitting; no need to worry about data linearity; runs efficiently on large datasets \n    - Cons: only predicts in range of data available in train set; doesn't train well on small datasets; training time can be huge as needs to train multiple trees, but possible to reduce running parallel processing \n    - Dataset is big enough to use ensemble; can deal with non-linear data; in general ensemble models are top performance models \n       \n2. Logistic regression: \n    - Pros: simple to understand and explain model; fast to train; good to predict probability of events\n    - Cons: Needs linear inputs --> extensive feature engineering; suffers from ourliers   \n    - Performance will be lower as there is not so much correlation between variables in dataset  \n\n3. K-Nearest Neighbors:\n    - Pros: lazy training, good performance\n    - Cons: slow in predicting phase, susceptible to high dimensional dataset    \n    - Good relative performance, but will be slow in testing time. I would not use this algorithm in applications that need fast decision, but for this project this information is missing from initial conditions, so I decided to try this algorithm as well\n    \n3. Gaussian Naive Bayes (GaussianNB):\n   - Pros: performs well with categorical variables; converges fast; good with moderate and large datasets\n   - Cons: independence assumption- correlated features effect performance    \n   - Can deal with medium and large datasets and is fast","6b4cabb8":"# Part 2: Importing Dataset\n<a id=\"Importing_Dataset\"><\/a>","3a752ef2":"## 6.4. Parameter Tuning for Gradient Boosting Classifier\n<a id=\"tuneB\"><\/a>\n","ccc1fabc":"Before proceeding to modeling stage it is very important to explore and preprocess features. Proper feature engineering can improve model performance significantly.","9a126cc4":"Most of the algorithms can't work with categorical strings and transformation of string categories into numeric values is required. Since there are only two possible categories for this label (\"<=50K\" and \">50K\"), we can avoid using one-hot encoding and simply encode these two categories as 0 and 1","d0533afd":"according to the information on dataset, there are no missing values in train dataset, however, there are values missing in test data.","7dd7bf93":"# Part 3: Explore Dataset\n<a id=\"Explore_Dataset\"><\/a>","1d69349d":"## 6.1. Split Dataset\n<a id=\"split\"><\/a>","f60b4c81":"### Normalizing Numerical Features\nNormalizing numerical features is important for a lot of algorithms. Proper feature scaling can help gradient descent to converge faster. Moreover, algorithms that are using eucledian disctance (such as K-meand and KNN) and regression coefficients of linear models  are effected by scaling.  \nApplying scaling to the data does not change the shape of each feature's distribution, however, normalization ensures that each feature is treated equally when applying supervised learners.  \nThere are multiple ways to scale features: like MinMaxScaler, StandardScaler, RobustScaler and etc.  \nWe will use [`MinMaxScaler`](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.MinMaxScaler.html) from sklearn","2e27cf15":"### Choosing Optimal Models: Summary\n\n1. Metrics:   \n    - As expected good predictors are ensemble models. All off them provide high F-score for training\/test set. Random forest has lower F-score for test data, however, very high F-score for train data. This shows that default Random Forest Classier is overfitting training data. However, by choosing correct parameters, Random Forest model will have better balance between train\/test data  \n\n2. Comutation complexity:  \n    - NaiveBayes, KNN, Logistic regression and Random Forest are the fastest models to train\n    - KNN is very slow in prediction, other models are relatively fast \n    - From ensemble models that use default parameters Random forest is the fastest, as it can train multiple trees at the same time. Gradient boosting classifier seams to be the slowest, as trees are trained using cascaded approach. However, all mentined above is subjective information as computation complexity depends on model parameters. \n    \n3. Suitability for the data:\n    - dataset size: (45222, 103)\n    - as correlation between features are relatively low and also ensemple models perform significantly better, I assume that non-linear relationships between features exsist in this dataset and they are important for correct prediction. The dataset is already 103 dimensions, so additional non-linear features will expand feature space tremendously and result in very high computational complexity.\n\nTop 4 models based on performance: Gradient Boosting, Adaboost, random Forest and Logistic regression. \nGradient Boosting Classifier  high training time, but still managable, low prediction time, and superior performance.  \nAdaboost Classifier , has  lower training time, slightly bigger prediction time and superior performance. Moreover, Adaboost has less parameters than Gradient Boosting Classifier and is much easier to tune.  ","a3522d4e":"### Parameters Tuning Strategy:\n\nGradient Boosting Classifier model is prone to overfit and needs smart parameter tuning. As the model has a high training computational complexity I will perforsm parameters tuning in steps, to make it faster: \n\n1. I will freeze learning_rate and all tree parameters of the model, and search for the optimal n_estimates for this learning rate\n2. Experiment with tree parameters to get optimal setting (max_depth, max_samples_leaf, min_samples_split, min_weight_fraction_leaf, max_leaf_nodes, max_features)\n3. Search for optimal 'subsample' \n4. Lower the learning rate and increase the estimators proportionally to get more robust models \n\n\nReferences: \n[Complete Guide to Parameter Tuning in Gradient Boosting (GBM) in Python](https:\/\/www.analyticsvidhya.com\/blog\/2016\/02\/complete-guide-parameter-tuning-gradient-boosting-gbm-python\/)","108b2054":"### 6.3.4.Find optimal learning_rate\nNow lets reduce learning rate to half of the original value, i.e. 0.1. To match with learning rate n_estomators need to be increased approximatelly twice, i.e. values around 160 and more","98c70044":"# Introduction\n<a id=\"introduction\"><\/a>\n\nIn this project, I will apply supervised learning techniques and an analytical mind on data collected for the U.S. census to help CharityML (a fictitious charity organization) identify people most likely to donate to their cause. I will first explore the data to learn how the census data is recorded. Next, I will apply a series of transformations and preprocessing techniques to manipulate the data into a workable format. I will then evaluate several supervised learners of your choice on the data, and consider which is best suited for the solution. Afterwards, I will optimize the model selected and present it as a solution to CharityML. Finally, you will explore the chosen model and its predictions under the hood, to see just how well it's performing when considering the data it's given.\n\nThe success of your model will be determined based on models AUC or area under the curve associated with ROC curves. ","978365c9":"Optimal subsample = 0.8. We will freezee this value and proceed to finding optimal learning_rate.","18d99bdc":"# Part 1: Importing Libraries\n<a id=\"Importing_Libraries\"><\/a>","92d6dc52":"1. AdaBoost classifier only has couple of parameters to tune, therefore, it was much faster to find optimal parameters than for Gradiend Boosting.  \n2. In term of performance, Gradient Boosting achieved better performance.\n3. Logistic regression is performing worse than tree based ensemble models, however, it can be useful to combine it with tree based models as it has very different approach to prediction and can potentially catch dependencies that three based models can't\n\nI will try three subbmissions:\n    - top model: GB\n    - average between top 2 models: GB and AB\n    - average between top 3 models: GB, AB and RF\n    - average between all models","d86562ea":"## 6.3. Parameter Tuning for AdaBosst Classifier\n<a id=\"tuneA\"><\/a>","b8806f66":"Optimal min_samples_leaf = 50. We will freezee this value and proceed to subsamples grid search."}}