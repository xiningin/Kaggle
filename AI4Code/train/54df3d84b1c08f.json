{"cell_type":{"0807f85f":"code","0284ecaf":"code","9c72fc72":"code","de72b0ec":"code","a57ee913":"code","c2ac6ac8":"code","0b5087c4":"code","551ea7e2":"code","85e20a7e":"code","4848f0ef":"code","66f3cac6":"code","45a0aa58":"code","5119207a":"code","4c03dae1":"code","97b1e9ac":"code","939963b6":"code","d916d993":"code","8abb0d45":"code","703fcc49":"code","c2ae0796":"code","3d81967b":"code","5b3bde99":"code","22386077":"code","bcd57f0f":"code","f124f4a5":"code","46973e00":"code","dd0e0ebe":"code","c6b8ddab":"code","e6bc61eb":"code","bf4c32ee":"code","c42924a0":"code","204adf14":"code","d0a85fed":"markdown","387be8e3":"markdown","003f5b6b":"markdown","26eb5136":"markdown","fa8668c7":"markdown","2cb34c13":"markdown","0115fead":"markdown","c7fb389a":"markdown","74ce8ee9":"markdown","a4349fe7":"markdown","f5cab05e":"markdown","bc84ca2d":"markdown","99c0b0ad":"markdown","bc2673ab":"markdown","5314b060":"markdown","e5d53fdc":"markdown","f97369b3":"markdown","e2f51e9e":"markdown","085b9a8d":"markdown","169fb074":"markdown","f0cc47c9":"markdown","2252770a":"markdown","dce02ea6":"markdown","32b7d623":"markdown","0dac798d":"markdown","e766e89f":"markdown","659c4134":"markdown","ba119ec7":"markdown","86e7afe4":"markdown","a2c0de47":"markdown","e1907a99":"markdown","b0cfdf9c":"markdown","937f504a":"markdown","924c30cb":"markdown","6e51e2a0":"markdown","8b2538fe":"markdown"},"source":{"0807f85f":"#Basic imports\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.utils import shuffle","0284ecaf":"!pip install pomegranate\nimport pomegranate as pg\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.semi_supervised import LabelPropagation, LabelSpreading\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.datasets import load_breast_cancer","9c72fc72":"import warnings\nwarnings.simplefilter('ignore') #we don't wanna see that\nnp.random.seed(1) #i'm locking seed at the begining since we will use some heavy RNG stuff, be aware","de72b0ec":"data = load_breast_cancer()\ndf = pd.DataFrame(data.data, columns=data.feature_names)\ndf['target'] = data['target']","a57ee913":"df.head()","c2ac6ac8":"df.info()","0b5087c4":"df.describe()","551ea7e2":"df = shuffle(df, random_state=1)\nX = df.drop(['target', 'radius error', 'texture error', 'perimeter error', 'area error', 'smoothness error', \n             'compactness error', 'concavity error', 'concave points error', 'symmetry error', 'fractal dimension error', \n             'worst radius', 'worst texture', 'worst perimeter', 'worst area', 'worst smoothness', \n             'worst compactness', 'worst concavity', 'worst concave points', 'worst symmetry', \n             'worst fractal dimension', 'mean area', 'mean perimeter', 'mean concave points'], axis=1)\ny = df['target']","85e20a7e":"sns.pairplot(X)\n#sns.pairplot(df)","4848f0ef":"X_1, X_2, X_3  = np.split(X, [int(.1*len(X)), int(.5*len(X))])\ny_1, y_2, y_3  = np.split(y, [int(.1*len(y)), int(.5*len(y))])\ny_1_2 = np.concatenate((y_1, y_2.apply(lambda x: -1)))\nX_1_2 = np.concatenate((X_1, X_2))","66f3cac6":"index = ['Algorithm', 'ROC AUC']\nresults = pd.DataFrame(columns=index)","45a0aa58":"logreg = LogisticRegression(random_state=1, class_weight='balanced')\nlogreg.fit(X_1, y_1)\nresults = results.append(pd.Series(['Logistic Regression', roc_auc_score(y_3, logreg.predict_proba(X_3)[:,1])], \n                                   index=index), ignore_index=True)\nresults","5119207a":"%%timeit\nlogreg_test = LogisticRegression(random_state=1, class_weight='balanced')\nlogreg_test.fit(df, y)\nlogreg_test.predict_proba(df);","4c03dae1":"def label_prop_test(kernel, params_list, X_train, X_test, y_train, y_test):\n    plt.figure(figsize=(20,10))\n    n, g = 0, 0\n    roc_scores = []\n    if kernel == 'rbf':\n        for g in params_list:\n            lp = LabelPropagation(kernel=kernel, n_neighbors=n, gamma=g, max_iter=100000, tol=0.0001)\n            lp.fit(X_train, y_train)\n            roc_scores.append(roc_auc_score(y_test, lp.predict_proba(X_test)[:,1]))\n    if kernel == 'knn':\n        for n in params_list:\n            lp = LabelPropagation(kernel=kernel, n_neighbors=n, gamma=g, max_iter=100000, tol=0.0001)\n            lp.fit(X_train, y_train)\n            roc_scores.append(roc_auc_score(y_test, lp.predict_proba(X_test)[:,1]))\n    plt.figure(figsize=(16,8));\n    plt.plot(params_list, roc_scores)\n    plt.title('Label Propagation ROC AUC with ' + kernel + ' kernel')\n    plt.show()\n    print('Best metrics value is at {}'.format(params_list[np.argmax(roc_scores)]))","97b1e9ac":"gammas = [9e-6, 1e-5, 2e-5, 3e-5, 4e-5, 5e-5, 6e-5, 7e-5, 8e-5, 9e-5]\nlabel_prop_test('rbf', gammas, X_1_2, X_3, y_1_2, y_3)","939963b6":"ns = np.arange(50,60)\nlabel_prop_test('knn', ns, X_1_2, X_3, y_1_2, y_3)","d916d993":"lp_rbf = LabelPropagation(kernel='rbf', gamma=9e-6, max_iter=100000, tol=0.0001)\nlp_rbf.fit(X_1_2, y_1_2)\nresults = results.append(pd.Series(['Label Propagation RBF', \n                                    roc_auc_score(y_3, lp_rbf.predict_proba(X_3)[:,1])], index=index), ignore_index=True)\n\nlp_knn = LabelPropagation(kernel='knn', n_neighbors=53, max_iter=100000, tol=0.0001)\nlp_knn.fit(X_1_2, y_1_2)\nresults = results.append(pd.Series(['Label Propagation KNN', \n                                    roc_auc_score(y_3, lp_knn.predict_proba(X_3)[:,1])], index=index), ignore_index=True)","8abb0d45":"results","703fcc49":"%%timeit\nrbf_lp_test = LabelPropagation(kernel='rbf')\nrbf_lp_test.fit(df, y)\nrbf_lp_test.predict_proba(df);","c2ae0796":"%%timeit\nknn_lp_test = LabelPropagation(kernel='knn')\nknn_lp_test.fit(df, y)\nknn_lp_test.predict_proba(df);","3d81967b":"def labels_spread_test(kernel, hyperparam, alphas, X_train, X_test, y_train, y_test):\n    plt.figure(figsize=(20,10))\n    n, g = 0, 0\n    roc_scores = []\n    if kernel == 'rbf':\n        g = hyperparam\n    if kernel == 'knn':\n        n = hyperparam\n    for alpha in alphas:\n        ls = LabelSpreading(kernel=kernel, n_neighbors=n, gamma=g, alpha=alpha, max_iter=1000, tol=0.001)\n        ls.fit(X_train, y_train)\n        roc_scores.append(roc_auc_score(y_test, ls.predict_proba(X_test)[:,1]))\n    plt.figure(figsize=(16,8));\n    plt.plot(alphas, roc_scores);\n    plt.title('Label Spreading ROC AUC with ' + kernel + ' kernel')\n    plt.show();\n    print('Best metrics value is at {}'.format(alphas[np.argmax(roc_scores)]))","5b3bde99":"alphas = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]  \nlabels_spread_test('rbf', 1e-5, alphas, X_1_2, X_3, y_1_2, y_3)","22386077":"alphas = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09]  \nlabels_spread_test('knn', 53, alphas, X_1_2, X_3, y_1_2, y_3)","bcd57f0f":"ls_rbf = LabelSpreading(kernel='rbf', gamma=9e-6, alpha=0.6, max_iter=1000, tol=0.001)\nls_rbf.fit(X_1_2, y_1_2)\nresults = results.append(pd.Series(['Label Spreading RBF', \n                                    roc_auc_score(y_3, ls_rbf.predict_proba(X_3)[:,1])], index=index), ignore_index=True)\nls_knn = LabelSpreading(kernel='knn', n_neighbors=53, alpha=0.08, max_iter=1000, tol=0.001)\nls_knn.fit(X_1_2, y_1_2)\nresults = results.append(pd.Series(['Label Spreading KNN', \n                                    roc_auc_score(y_3, ls_knn.predict_proba(X_3)[:,1])], index=index), ignore_index=True)","f124f4a5":"results","46973e00":"%%timeit\nknn_ls_test = LabelSpreading(kernel='rbf')\nknn_ls_test.fit(df, y)\nknn_ls_test.predict_proba(df);","dd0e0ebe":"%%timeit\nknn_ls_test = LabelSpreading(kernel='knn')\nknn_ls_test.fit(df, y)\nknn_ls_test.predict_proba(df);","c6b8ddab":"nb = pg.NaiveBayes.from_samples(pg.ExponentialDistribution, X_1_2, y_1_2, verbose=True)\nroc_auc_score(y_3, nb.predict_proba(X_3)[:,1])","e6bc61eb":"d = [pg.ExponentialDistribution, pg.PoissonDistribution, pg.NormalDistribution, \n     pg.ExponentialDistribution, pg.ExponentialDistribution, pg.PoissonDistribution, pg.NormalDistribution]\nnb = pg.NaiveBayes.from_samples(d, X_1_2, y_1_2, verbose=True)\nresults = results.append(pd.Series(['Naive Bayes ICD Prior', \n                                    roc_auc_score(y_3, nb.predict_proba(X_3)[:,1])], index=index), ignore_index=True)","bf4c32ee":"results","c42924a0":"logReg_coeff = pd.DataFrame({'feature_name': list(X.columns.values), \n                             'model_coefficient': logreg.coef_.transpose().flatten()});\nlogReg_coeff = logReg_coeff.sort_values('model_coefficient',ascending=False);\n\nfg = sns.barplot(x='feature_name', y='model_coefficient',data=logReg_coeff);\nfg.set_xticklabels(rotation=35, labels=logReg_coeff['feature_name']);","204adf14":"%%timeit\nnb_test = pg.NaiveBayes.from_samples(pg.ExponentialDistribution, df, y, verbose=False)\nnb_test.predict_proba(df);","d0a85fed":"Last but not least - timings","387be8e3":"The whole idea of SSL is kinda bayesian in its nature, but we won't dive deep.\n\nAs analogy to SSL [some resources](https:\/\/deepai.org\/machine-learning-glossary-and-terms\/semi-supervised-learning) give human inductive logic: we know just some instances of the phenomenon, but we try to infer general principles and same here: algorithms are using few reference points (our labeled data) to find the general pattern that will suit our unlabeled data best. But it's not always the case. Some algorithms will try to find generalization for the given data (infer a function, that describes our data best), but some algorithms will use thing called [transductive learning](https:\/\/en.wikipedia.org\/wiki\/Transduction_(machine_learning)) ([more here](http:\/\/www.cs.cornell.edu\/courses\/cs4780\/2009fa\/lecture\/13-transduction.pdf)). We will use models that take this approach to solve the classification task. \n\nAll in all inductive learning is trying to get generalization from the given data and only then predict new data. Transductive learning will just try to predict new data given training data, skipping generalization part.\n\nLet's code a bit. I will add comments and explain what I'm doing. I guess most of the time it won't be necessary but still.","003f5b6b":"## Bonus","26eb5136":"## Naive Bayes","fa8668c7":"Just for reference LR timings.\n\n*Reminder: df is 569x31 dense matrix*","2cb34c13":"Now I will define a little function that will give us a plot of ROC AUC of our algorithm depending on our kernel and parameter list.","0115fead":"And the last one, but not the least. Pseudo-Labeling. I won't copy-paste it, you can do it yourself. This is very simple approach. We will just separate our labeled and unlabeled data, train model on labelled. Then we will sample unlabeled data and predict these samples and add them to labeled data as new ground truth. That's it.\nYou can literally use everything with it: regression models or classification models. In fact original it was designed for neural nets, but it is very versatile.","c7fb389a":"It's still bad since our data is correlated and not equal at all. At least our `logreg` doesn't think so.\nBut still NB is could be useful in some cases since it's simple, **very** fast and has interpretability. You can use it as a baseline or as \"scouting\" algorithm.","74ce8ee9":"<center><p><img src=\"https:\/\/s3-ap-south-1.amazonaws.com\/av-blog-media\/wp-content\/uploads\/2017\/09\/20182516\/dataiku-hadoop-summit-semisupervised-learning-with-hadoop-for-understanding-user-web-behaviours-12-638.jpg\" title=\"Types of learning\"\/><\/p><\/center>  \n\nImg 1 - Types of learning [[source]](https:\/\/www.slideshare.net\/Dataiku\/dataiku-hadoop-summit-semisupervised-learning-with-hadoop-for-understanding-user-web-behaviours)","a4349fe7":"Close to random. FeelsBad. Let's try some crazy things. We will construct our independent component distribution with n-dimensions (in this case 7). We will cheat a bit, since we already now how our data is distributed (see `sns.pairplot` step). My take on this is below","f5cab05e":"## Label Propagation","bc84ca2d":"They seem reasonable enough though it's not a problem to find counterarguments.\nWhy do we need all these complications? Well, because it's working (in theory).","99c0b0ad":"Now you can define your model separately with the best (or whatever) hyperparameter value and check its score","bc2673ab":"## Label Spreading","5314b060":"Since we have only seven features now we can do something called [pairplot](https:\/\/seaborn.pydata.org\/generated\/seaborn.pairplot.html): it will plot pointplots between each features and their distribution on diagonal. This will help us find correlations and see how we can normalize our features.\nThis will take a time. More features and objects = more time, so don't try it on large datasets. If you want you can try it on whole data set `df` (it will take more time and every graph will be smaller), you will see highly correlated features that I dropped above.","e5d53fdc":"Now we will initiate our NB model directly from our data. `from_samples` is the method that allow us to do it and set distribution that will model our data.\nAnother way is to directly pre-initialize distributions and its parameters and then just predict samples classes.\nHere I picked `ExponentialDistribution` but you can pick whatever you want. The list of supported distributions is [here](https:\/\/pomegranate.readthedocs.io\/en\/latest\/Distributions.html). The ones you want here are: `ExponentialDistribution`, `NormalDistribution`, `PoissonDistribution`. You can check others but it will give you an error since algorithm won't converge.","f97369b3":"And since we are dealing with unlabeled data (obviously, unknown instances) we grab some assumptions from unsupervised learning:\n \n - Continuity assumption (*Points which are close to each other are more likely to share a label*)\n - Cluster assumption (*The data tend to form discrete clusters, and points in the same cluster are more likely to share a label*)\n - Manifold assumption (*The data lie approximately on a manifold of much lower dimension than the input space*)","e2f51e9e":"Another assumption that Naive Bayes make use of is equality of all our features. It's wrong too, but with it we don't need to set weights to our features. Although some libs give you that possibility.\n\nThere a lot of articles about NB, I like this [one](https:\/\/jakevdp.github.io\/PythonDataScienceHandbook\/05.05-naive-bayes.html) and [wiki's one](https:\/\/en.wikipedia.org\/wiki\/Naive_Bayes_classifier).\n`pomegranate` bayes classifier user docs are [here](https:\/\/pomegranate.readthedocs.io\/en\/latest\/NaiveBayes.html)","085b9a8d":"Now let's talk about time. You can just look at the results or run command on your machine. On dense matrixes it's okeish, but on sparse representation it's a catastrophy (knn kernel is bareable). But for this we have [Sparse Label Propagation](https:\/\/arxiv.org\/abs\/1612.01414) or some dimensionality reduction technique (PCA, ICA, RCA etc.)","169fb074":"<center><p><img src=\"https:\/\/datawhatnow.com\/wp-content\/uploads\/2017\/08\/pseudo-labeling-683x1024.png\" title=\"Pseudo-Labeling\"\/><\/p><\/center>\n\nImg 5 - Pseudo-Labeling explanation [[source]](https:\/\/datawhatnow.com\/pseudo-labeling-semi-supervised-learning\/)","f0cc47c9":"And timings.","2252770a":"Sometimes pressing Enter in TensorFlow is not enough to get good results. Sometimes you have too little data to work with and maximum likelihood estimation gives you some bizarre answer. Or even more common - data and labeling are *expensive* and the relationship between the number of instances in your dataset and quality of the final model is not linear at all.\nThat's when semi-supervised learning (SSL) come to play.\n \n > Semi-supervised learning is a class of machine learning tasks and techniques that also make use of unlabeled data for training \u2013 typically a small amount of labeled data with a large amount of unlabeled data [[source]](https:\/\/en.wikipedia.org\/wiki\/Semi-supervised_learning).\n \nThese types of ML algorithms are using both labeled and unlabeled data to get more accurate predictions (or in rare cases to get them at all).  ","dce02ea6":"We will shuffle our dataset since it has order and we don't want that. \nAlso, I will reduce dimensionality by dropping features to simplify everything.\n\n*Disclaimer: yes, if you train LR on all features, you will surpass the results of LP and LS. But in this case I only wanted to show you the models not give the best results. Perfromance of metric models will degrade with larger number of dimensions (it depends on metric that you are using but still).*\n\nThen we will create our `X` and `Y` and split them into three parts: labeled train data (1), unlabeled train data (2) and test data (3). We will drop a lot of features (se and worst columns; area\/perimeter is redundant since it's highly correlated to the radius; concave points feature is redundant too and correlates to concavity and compactness).\n\nLittle warning: result reproduction is only achievable if you shuffle data from its original order (order that it has in csv file). If you try to shuffle from another state (shuffle when you've already shuffled it, etc.) will give you different results. Solutions of some models are not stable, since they are metric models in their core.","32b7d623":"<center><p><img src=\"https:\/\/www.saedsayad.com\/images\/Bayes_rule.png\" title=\"NB explanation 2\"\/><\/p><\/center>\n\nImg 3 - Naive Bayes explanation [[source]](https:\/\/www.saedsayad.com\/images\/Bayes_rule.png)","0dac798d":"<center><p><img src=\"https:\/\/cdn-images-1.medium.com\/max\/800\/1*0izKZvefo1pCCVXK3fmLhg.png\" title=\"NB optimization\"\/><\/p><\/center>\n\nImg 4 - Naive Bayes classifier [[source]](https:\/\/towardsdatascience.com\/introduction-to-naive-bayes-classification-4cffabb1ae54)","e766e89f":"Now we will merge labeled and unlabeled data.\nThe common practice (most libs will need it that way) is to label unlabeled data `-1`. Some libs will only accept `NaN` as label for unlabeled data. Always check documentation and user guides.","659c4134":"`sklearn` has two SSL algorithms: Label Propagation and Label Spreading. Let's import them.\n\nWe will also use [`pomegranate`](https:\/\/pomegranate.readthedocs.io\/en\/latest\/index.html). It's a bayesian lib with lots of features but we will take only one-two models from it. It's written in so called skl notation - it uses same syntax and methods names as `sklearn`, so you will understand it pretty quickly. To install it run:\n\nthrough pip\n>pip install pomegranate\n\nthrough Anaconda\n>conda install pomegranate","ba119ec7":"Now let us move to `pomegranate` lib. It supports SSL for some models: Naive Bayes, Bayes Classificator, Hidden Markov Models.\nWe will use only Naive Bayes (NB), it's plain and simple.\n\n>Naive Bayes classifiers are a family of simple \"probabilistic classifiers\" based on applying Bayes' theorem with strong (naive) independence assumptions between the features [[source]](https:\/\/en.wikipedia.org\/wiki\/Naive_Bayes_classifier)\n\n<center><p><img src=\"https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/52bd0ca5938da89d7f9bf388dc7edcbd546c118e\" title=\"NB explanation\"\/><\/p><\/center>\n\nWe will use [Bayes theorem](https:\/\/en.wikipedia.org\/wiki\/Bayes%27_theorem) for our predictions. So, we need to find $p(c|x)$ (probability of the class c given sample x). To do it we need to calculate $p(c)$ - just a class probability in general, $p(x)$ - our *evidence* and $p(x|c)$. The last one is very hard to compute since in general we need to take into account conditional dependencies in our data.\n\nBut what if we assume that all our features are conditionally independent given class c (very strong and mostly wrong assumption)? Well, this would make our life **a lot** easier.\nWe can now just calculate $p(c|x)$ as a product of $p(x_i|c)$.","86e7afe4":"Now we will just briefly look through our dataset","a2c0de47":"For every sample we will pick the most probable class (this is known as the maximum a posteriori or MAP decision rule). Then we can write down our optimization task","e1907a99":"# BASIC SEMI-SUPERVISED LEARNING MODELS","b0cfdf9c":"Next one is `LabelSpreading` (LS). The algorithm is very similar to spectral clustering algorithm *normalized cuts algorithm* ([look here](https:\/\/en.wikipedia.org\/wiki\/Spectral_clustering), [here](https:\/\/towardsdatascience.com\/spectral-clustering-for-beginners-d08b7d25b4d8) and [here](https:\/\/papers.nips.cc\/paper\/2092-on-spectral-clustering-analysis-and-an-algorithm.pdf)).\nLS will create a affinity matrix (same as LP weights calculation step): \n\n$$W_{i,j} = exp(\\frac{-||x_i - x_j||^2}{\\sigma^2})$$\n\nfor every $i\\neq j$ and $W_{i,j} = 0$ for every $i = j$\n\nThen we will construct the matrix (Laplacian):\n\n$$S = D^{-1\/2}WD^{\u22121\/2}$$\n\nwhere D - diagonal matrix with its *(i,i)*-element equal to the sum of the i-th row of W.\n\nThese two steps is just a spectral clustering on all our data. Next two is more interesting:\n \nIterate $F(t+1) = \u03b1SF(t)+(1\u2212\u03b1)Y$ until convergence, where \u03b1 is a parameter in (0, 1), F(t) - classifying function\n\nLet $F^*$ denote the limit of the sequence {F(t)}. Label each point $x_i$ as a label $y_i = argmax\u2264F^*_{i,j}$\n\nDuring each iteration of the third step each point receives the information from its neighbors (first term), and also retains its initial information (second term). The parameter \u03b1 specifies the relative amount of the information from its neighbors and its initial label information. Finally, the label of each unlabeled point is set to be the class of which it has received most information during the iteration process.\n\nYou can read article about Label Spreading [here](http:\/\/citeseer.ist.psu.edu\/viewdoc\/download;jsessionid=19D11D059FCEDAFA443FB135B4065A6A?doi=10.1.1.115.3219&rep=rep1&type=pdf), `sklearn` user docs are [here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.semi_supervised.LabelSpreading.html).","937f504a":"We will use this [dataset](https:\/\/scikit-learn.org\/stable\/datasets\/index.html#breast-cancer-dataset). Here is `sklearn` [loader](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.datasets.load_breast_cancer.html)\n\nAtributes:\n\n- ID number \n- Diagnosis (M = malignant, B = benign) \n\nTen real-valued features are computed for each cell nucleus: \n\n- radius (mean of distances from center to points on the perimeter) \n- texture (standard deviation of gray-scale values) \n- perimeter \n- area \n- smoothness (local variation in radius lengths) \n- compactness (perimeter^2 \/ area - 1.0) \n- concavity (severity of concave portions of the contour) \n- concave points (number of concave portions of the contour) \n- symmetry \n- fractal dimension (\"coastline approximation\" - 1)\n\nThe mean, standard error and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\n\nAll feature values are recoded with four significant digits.\n\nMissing attribute values: none\n\nClass distribution: 357 benign, 212 malignant","924c30cb":"Time to use our first SSL model - `LabelPropagation` (LP). It's quite simple. Intuitively, it looks and sounds like a clustering algorithm that just \"propagate\" labels to nearest data points. But we need to go a little bit deeper.\n\nY is our label matrix, it has size of $(l+u)\\times C$, where l - amount of labeled data points, u - amount of unlabeled data points, C - number of classes. So in binary classification task we will get 2 columns. It's kind of list that we will get from `predict_proba` method.\n\nThe algorithm will create a fully connected graph where nodes are **all** the data points. The edges between nodes will be weights $w_{i,j}$: \n\n$$w_{i,j} = exp(\\frac{d_{i,j}^2}{\\sigma^2})$$\n\nwhere *d* - distance function (euclidean in this case, but in general could be any distance function you want), $\\sigma$ - hyperparameter, that control (shrink) weights.\n\nThen we build probabilistic transition matrix T: \n\n$$T_{i,j} = \\frac{w_{i,j}}{\\sum_{k=1}^{l+u}w_{i,j}}$$\n\nT is just a matrix with probability of every data point to be in class C. Our labeled data points has 1.0 probability to be in class C (since we know their classes) and unlabeled data will get their classes from neighbours (we calculated weights earlier, they depend on distance between 2 points).\n\nThe whole algorithm looks like this:\n1. Propagate Y <- TY (we \"propagate\" labels from labeled data to unlabeled)\n2. Row-normalize Y (value of element in row \/ sum of all elements values in row)\n3. Clamp the labeled data (we fixinig our labeled data, so our algorithm won't change probability of class, or in another words change the label)\n4. Repeat from step 1 until Y converges (we recalculate or distances and weights, that will give us different transition matrix that will change our belief in assigned labels, repeat until process converge).\n\nIn case of `sklearn` implementation we have a choice of weightening function: RBF (see formula for $w_{i,j}$) or KNN ($1(x' \\in kNN(x))$). KNN is faster and gives sparse representation. RBF is harder to compute and store dense transition matrix in memory, but has more options for tuning. Also sklearn RBF implementation instead of dividing by $\\sigma^2$ multiply it by `gamma`, so it should be float, not integer.\n\nTo learn more you can read [this](http:\/\/pages.cs.wisc.edu\/~jerryzhu\/pub\/CMU-CALD-02-107.pdf) and [this](https:\/\/scikit-learn.org\/stable\/modules\/label_propagation.html#label-propagation). `sklearn` also has some cool examples of `LabelPropagation`: [SVM vs LP](https:\/\/scikit-learn.org\/stable\/auto_examples\/semi_supervised\/plot_label_propagation_versus_svm_iris.html#sphx-glr-auto-examples-semi-supervised-plot-label-propagation-versus-svm-iris-py), [LP demo](https:\/\/scikit-learn.org\/stable\/auto_examples\/semi_supervised\/plot_label_propagation_structure.html#sphx-glr-auto-examples-semi-supervised-plot-label-propagation-structure-py), [LP digits recognition](https:\/\/scikit-learn.org\/stable\/auto_examples\/semi_supervised\/plot_label_propagation_digits.html#sphx-glr-auto-examples-semi-supervised-plot-label-propagation-digits-py) and [LP digits active learning](https:\/\/scikit-learn.org\/stable\/auto_examples\/semi_supervised\/plot_label_propagation_digits_active_learning.html#sphx-glr-auto-examples-semi-supervised-plot-label-propagation-digits-active-learning-py).","6e51e2a0":"<center><p><img src=\"https:\/\/ai2-s2-public.s3.amazonaws.com\/figures\/2017-08-08\/19bb0dce99466077e9bc5a2ad4941607fc28b40c\/4-Figure1-1.png\" title=\"SSL explanation\"\/><\/p><\/center>\n    \nImg 2 - Semi-supervised explanation [[source]](http:\/\/people.cs.uchicago.edu\/~niyogi\/papersps\/BNSJMLR.pdf)","8b2538fe":"Pseudo-Labeling described [here](https:\/\/datawhatnow.com\/pseudo-labeling-semi-supervised-learning\/) with code and graphs, [copypaste on another blog](https:\/\/www.analyticsvidhya.com\/blog\/2017\/09\/pseudo-labelling-semi-supervised-learning-technique\/).\nOriginal article is [here](http:\/\/deeplearning.net\/wp-content\/uploads\/2013\/03\/pseudo_label_final.pdf)\nYou can copy it below and play with it!"}}