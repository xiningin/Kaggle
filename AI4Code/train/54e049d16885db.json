{"cell_type":{"eaaf1c70":"code","5f2bb564":"code","cabf6d1b":"code","fc08a231":"code","7fd9bb42":"code","818ab87c":"code","9025f138":"code","96d7e2dd":"code","ddd50321":"code","88d981e6":"code","493c5846":"code","35c648e6":"code","906920e0":"code","9f1551bc":"code","07b81629":"code","71c79a37":"code","a8b95d77":"code","4f3951e4":"code","5f6e3c30":"code","64802a17":"code","6e3203be":"code","0a35170d":"code","930f3153":"code","9d36ce9e":"code","d38e2f69":"code","0663032e":"code","b4ab8ad0":"code","6675cc4a":"code","cc3cb606":"code","60d71a0f":"code","2c8069d0":"code","97b7d9c1":"code","ad3169fb":"code","bb16eed2":"code","6d471f5e":"code","92d730a0":"code","490baf4d":"code","2a2ad399":"code","b98eca24":"code","aeb584d9":"code","3995b49f":"code","bf7ca2ab":"code","30c017aa":"code","b139a1fd":"code","68d70ed5":"code","77980154":"code","5a59c036":"code","6b7dc6e4":"code","c29b2704":"code","2c9f3c6a":"code","34b88d69":"code","238dde5a":"code","6b094074":"code","fcc5adb0":"code","c4db273a":"markdown","46ebcc5b":"markdown","739ba902":"markdown","9d069c69":"markdown","f1aceae2":"markdown","d0ac91b5":"markdown","3f4b8eeb":"markdown","8963c3e7":"markdown","8eb2a6fa":"markdown","fae36c83":"markdown","4557071a":"markdown","6d7c0375":"markdown","1b8c70de":"markdown","4610280b":"markdown","068eca40":"markdown","bd8c69b4":"markdown","6a92ebc4":"markdown","95f645f5":"markdown","cb987c5c":"markdown","898fc5f1":"markdown","cc78c253":"markdown","d5a6b043":"markdown","1e92890c":"markdown","1e9c8e76":"markdown","a8f4bfdc":"markdown","34334bda":"markdown","006fae95":"markdown","92773212":"markdown","416562d5":"markdown","0a9ad2fd":"markdown","07ea371f":"markdown","d851e15a":"markdown","5b027ae5":"markdown"},"source":{"eaaf1c70":"#Acquisition des bibliotheques\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing, model_selection, metrics\nimport warnings\nfrom sklearn.model_selection import train_test_split\nwarnings.filterwarnings(\"ignore\")\nimport seaborn as sns\n\nimport plotly as pl\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n%matplotlib inline","5f2bb564":"#Lecture des donnees\ndtst = pd.read_csv(\"..\/input\/energydata_complete.csv\")\ndtst.head()","cabf6d1b":"#Visualisation des informations sur les donnees\ndtst.info()","fc08a231":"#Description des donnees\ndtst.describe()","7fd9bb42":"#Visualisation du nombre de lignes et colonnes\nprint('Nombre de lignes : ' , dtst.shape[0])\nprint('Nombre de colonnes : ' , dtst.shape[1])","818ab87c":"#Verification des valeurs nulles\ndtst.isnull().sum().sort_values(ascending = True)","9025f138":"#Division des donnes en donnees donnees test et entrainements\n# 75% des donnees pour la formation de models et 25 test\ntrain, test = train_test_split(dtst,test_size=0.25,random_state=40)\ntrain.describe()","96d7e2dd":"#Description des donnees d'entrainement\ntrain.describe()","ddd50321":"#Division des colonnes de leur type\n\ncol_temp = [\"T1\",\"T2\",\"T3\",\"T4\",\"T5\",\"T6\",\"T7\",\"T8\",\"T9\"]\n\ncol_hum = [\"RH_1\",\"RH_2\",\"RH_3\",\"RH_4\",\"RH_5\",\"RH_6\",\"RH_7\",\"RH_8\",\"RH_9\"]\n\ncol_weather = [\"T_out\", \"Tdewpoint\",\"RH_out\",\"Press_mm_hg\",\n                \"Windspeed\",\"Visibility\"] \ncol_light = [\"lights\"]\n\ncol_randoms = [\"rv1\", \"rv2\"]\n\ncol_target = [\"Appliances\"]","88d981e6":"#Separation variables dependantes et variables independantes\nfeature = train[col_temp + col_hum + col_weather + col_light + col_randoms ]\ntarget = train[col_target]","493c5846":"feature.describe()","35c648e6":"#Verification des valeurs dans lights\nfeature.lights.value_counts()","906920e0":"target.describe()","9f1551bc":"_ = feature.drop(['lights'], axis=1 , inplace= True) ;\nfeature.head(2)","07b81629":"#Comprehension de la variation chronologique de la consommation d'\u00e9nergie de l'appareil\nvisData = go.Scatter( x= dtst.date  ,  mode = \"lines\", y = dtst.Appliances )\nlayout = go.Layout(title = 'Mesure de la consommation d_\u00e9nergie des appareils' , xaxis=dict(title='Date'), yaxis=dict(title='(Wh)'))\nfig = go.Figure(data=[visData],layout=layout)\niplot(fig)","71c79a37":"dtst['WEEKDAY'] = ((pd.to_datetime(dtst['date']).dt.dayofweek)\/\/ 5 == 1).astype(float)\ndtst['WEEKDAY'].value_counts()","a8b95d77":"#Jour de la semaine\ntemp_weekday =  dtst[dtst['WEEKDAY'] == 0]\n#Comprehension de la variation chronologique\nvisData = go.Scatter( x= temp_weekday.date  ,  mode = \"lines\", y = temp_weekday.Appliances )\nlayout = go.Layout(title = 'Mesure de la consommation d_\u00e9nergie des appareils en semaine' , xaxis=dict(title='Date'), yaxis=dict(title='(Wh)'))\nfig = go.Figure(data=[visData],layout=layout)\n\niplot(fig)","4f3951e4":"#Jour de la semaine\ntemp_weekday =  dtst[dtst['WEEKDAY'] == 0]\n#Comprehension de la variation chronologique\nvisData = go.Scatter( x= temp_weekday.date  ,  mode = \"lines\", y = temp_weekday.Appliances )\nlayout = go.Layout(title = 'Mesure de la consommation d_\u00e9nergie des appareils en semaine' , xaxis=dict(title='Date'), yaxis=dict(title='(Wh)'))\nfig = go.Figure(data=[visData],layout=layout)\n\niplot(fig)","5f6e3c30":"#Histogramme des fonctionnalites \nfeature.hist(bins = 20, figsize= (12,16)) ;","64802a17":"f, ax = plt.subplots(2,2,figsize=(12,8))\nv1 = sns.distplot(feature[\"RH_6\"],bins=10, ax= ax[0][0])\nv2 = sns.distplot(feature[\"RH_out\"],bins=10, ax=ax[0][1])\nv3 = sns.distplot(feature[\"Visibility\"],bins=10, ax=ax[1][0])\nv4 = sns.distplot(feature[\"Windspeed\"],bins=10, ax=ax[1][1])","6e3203be":"#Distribution des valeurs de la colonne Appliances en fonction de la frequence\nf = plt.figure(figsize=(12,5))\nplt.xlabel('Consommation d_appareils en Wh')\nplt.ylabel('Frequence')\nsns.distplot(target , bins=10 ) ;","0a35170d":"print('Le pourcentage de la consommation de l_appareil est inf\u00e9rieur \u00e0 200 Wh')\nprint(((target[target <= 200].count()) \/ (len(target)))*100 )","930f3153":"# Correlation entre les colonnes m\u00e9t\u00e9o, temp\u00e9rature, appareils\ntrain_corr = train[col_temp + col_hum + col_weather +col_target+col_randoms]\ncorr = train_corr.corr()\n# Masquage des valeurs repetees\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n  \nf, ax = plt.subplots(figsize=(16, 14))\n#G\u00e9n\u00e9ration d'un une carte de chaleur \nsns.heatmap(corr, annot=True, fmt=\".2f\" , mask=mask,)\n    #Application de xticks\nplt.xticks(range(len(corr.columns)), corr.columns);\n    #Application de yticks\nplt.yticks(range(len(corr.columns)), corr.columns)\n    #Affichage\nplt.show()","9d36ce9e":"def get_redundant_pairs(df):\n    '''Get diagonal and lower triangular pairs of correlation matrix'''\n    pairs_to_drop = set()\n    cols = df.columns\n    for i in range(0, df.shape[1]):\n        for j in range(0, i+1):\n            pairs_to_drop.add((cols[i], cols[j]))\n    return pairs_to_drop\n\n# Fonction d'obtention de meilleures corr\u00e9lations \n\ndef get_top_abs_correlations(df, n=5):\n    au_corr = df.corr().abs().unstack()\n    labels_to_drop = get_redundant_pairs(df)\n    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n    return au_corr[0:n]\n\nprint(\"Top Absolute Correlations\")\nprint(get_top_abs_correlations(train_corr, 40))","d38e2f69":"#Division l'ensemble de donn\u00e9es d'apprentissage en variables ind\u00e9pendantes et d\u00e9pendantes\ntrain_X = train[feature.columns]\ntrain_y = train[target.columns]\n#Pour eviter confusion faite ci-dessus, ces colonnes sont supprim\u00e9es\ntrain_X.drop([\"rv1\",\"rv2\",\"Visibility\",\"T6\",\"T9\"],axis=1 , inplace=True)","0663032e":"#Division l'ensemble de donn\u00e9es de test en variables ind\u00e9pendantes et d\u00e9pendantes\ntest_X = test[feature.columns]\ntest_y = test[target.columns]\n#Pour eviter confusion faite ci-dessus, ces colonnes sont supprim\u00e9es\ntest_X.drop([\"rv1\",\"rv2\",\"Visibility\",\"T6\",\"T9\"], axis=1, inplace=True)","b4ab8ad0":"train_X.columns","6675cc4a":"test_X.columns","cc3cb606":"from sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\n\n# Creation d'un ensemble de tests et de formation en incluant la colonne Appliances\n\ntrain = train[list(train_X.columns.values) + col_target ]\n\ntest = test[list(test_X.columns.values) + col_target ]\n\n# Creation d'un test factice et un ensemble de formation pour contenir des valeurs mises \u00e0 l'\u00e9chelle\n\nsc_train = pd.DataFrame(columns=train.columns , index=train.index)\n\nsc_train[sc_train.columns] = sc.fit_transform(train)\n\nsc_test= pd.DataFrame(columns=test.columns , index=test.index)\n\nsc_test[sc_test.columns] = sc.fit_transform(test)","60d71a0f":"sc_train.head()","2c8069d0":"sc_test.head()","97b7d9c1":"sc_test.head()","ad3169fb":"#Suppression de la colonne Appliances de l'ensemble de formation\ntrain_X =  sc_train.drop(['Appliances'] , axis=1)\ntrain_y = sc_train['Appliances']\n\ntest_X =  sc_test.drop(['Appliances'] , axis=1)\ntest_y = sc_test['Appliances']","bb16eed2":"train_X.head()","6d471f5e":"train_y.head()","92d730a0":"from sklearn.linear_model import Ridge, Lasso\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn import neighbors\nfrom sklearn.svm import SVR","490baf4d":"models = [\n           ['Lasso: ', Lasso()],\n           ['Ridge: ', Ridge()],\n           ['KNeighborsRegressor: ',  neighbors.KNeighborsRegressor()],\n           ['SVR:' , SVR(kernel='rbf')],\n           ['RandomForest ',RandomForestRegressor()],\n           ['ExtraTreeRegressor :',ExtraTreesRegressor()],\n           ['GradientBoostingClassifier: ', GradientBoostingRegressor()] ,\n           ['MLPRegressor: ', MLPRegressor(  activation='relu', solver='adam',learning_rate='adaptive',max_iter=1000,learning_rate_init=0.01,alpha=0.01)]\n         ]","2a2ad399":"#Ex\u00e9cution des mod\u00e8les et mettre \u00e0 jour des informations dans une liste model_data\n\nimport time\nfrom math import sqrt\nfrom sklearn.metrics import mean_squared_error\n\nmodel_data = []\nfor name,curr_model in models :\n    curr_model_data = {}\n    curr_model.random_state = 78\n    curr_model_data[\"Name\"] = name\n    start = time.time()\n    curr_model.fit(train_X,train_y)\n    end = time.time()\n    curr_model_data[\"Train_Time\"] = end - start\n    curr_model_data[\"Train_R2_Score\"] = metrics.r2_score(train_y,curr_model.predict(train_X))\n    curr_model_data[\"Test_R2_Score\"] = metrics.r2_score(test_y,curr_model.predict(test_X))\n    curr_model_data[\"Test_RMSE_Score\"] = sqrt(mean_squared_error(test_y,curr_model.predict(test_X)))\n    model_data.append(curr_model_data)\n","b98eca24":"model_data","aeb584d9":"dtst = pd.DataFrame(model_data)\ndtst","3995b49f":"#Affichage des resultat de R2\ndtst.plot(x=\"Name\", y=['Test_R2_Score' , 'Train_R2_Score' , 'Test_RMSE_Score'], kind=\"bar\" , \n        title = 'Resultats Score R2' , figsize= (10,8)) ;","bf7ca2ab":"from sklearn.model_selection import GridSearchCV\nparam_grid = [{\n              'max_depth': [80, 150, 200,250],\n              'n_estimators' : [100,150,200,250],\n              'max_features': [\"auto\", \"sqrt\", \"log2\"]\n            }]\nreg = ExtraTreesRegressor(random_state=40)\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = reg, param_grid = param_grid, cv = 5, n_jobs = -1 , scoring='r2' , verbose=2)\ngrid_search.fit(train_X, train_y)","30c017aa":"#Jeu de param\u00e8tres accord\u00e9\ngrid_search.best_params_","b139a1fd":"#Meilleurs param\u00e8tres possibles pour ExtraTreesRegressor\ngrid_search.best_estimator_","68d70ed5":"#Score R2 sur l'ensemble d'entra\u00eenement avec param\u00e8tres r\u00e9gl\u00e9s\ngrid_search.best_estimator_.score(train_X,train_y)","77980154":"#Score R2 sur l'ensemble de test avec param\u00e8tres r\u00e9gl\u00e9s\ngrid_search.best_estimator_.score(test_X,test_y)","5a59c036":"## Score RMSE sur l'ensemble de test avec param\u00e8tres r\u00e9gl\u00e9s\nnp.sqrt(mean_squared_error(test_y, grid_search.best_estimator_.predict(test_X)))","6b7dc6e4":"#Obtention une liste tri\u00e9e des fonctionnalit\u00e9s par ordre d'importance\nfeature_indices = np.argsort(grid_search.best_estimator_.feature_importances_)","c29b2704":"importances = grid_search.best_estimator_.feature_importances_\nindices = np.argsort(importances)[::-1]\nnames = [train_X.columns[i] for i in indices]\n# Create plot\nplt.figure(figsize=(10,6))\n\n# Create plot title\nplt.title(\"Importance des Fonctionnalites\")\n\n# Add bars\nplt.bar(range(train_X.shape[1]), importances[indices])\n\n# Add feature names as x-axis labels\nplt.xticks(range(train_X.shape[1]), names, rotation=90)\n\n# Show plot\nplt.show()","2c9f3c6a":"#Les 5 fonctionnalit\u00e9s les plus importantes sont :\nnames[0:5]","34b88d69":"#Les 5 fonctionnalit\u00e9s les moins importantes sont : 'T7', 'Tdewpoint', 'Windspeed', 'T1', 'T5'\nnames[-5:]","238dde5a":"train_important_feature = train_X[names[0:5]]\ntest_important_feature = test_X[names[0:5]]","6b094074":"from sklearn.base import clone\ncloned_model = clone(grid_search.best_estimator_)\ncloned_model.fit(train_important_feature , train_y)","fcc5adb0":"print('Training set R2 Score - ', metrics.r2_score(train_y,cloned_model.predict(train_important_feature)))\nprint('Testing set R2 Score - ', metrics.r2_score(test_y,cloned_model.predict(test_important_feature)))\nprint('Testing set RMSE Score - ', np.sqrt(mean_squared_error(test_y, cloned_model.predict(test_important_feature))))","c4db273a":"Dans la Colonne lights, on constate que cette colonne n\u2019est pas en mesure de fournir des informations utiles. Donc, avec 11438, 0 (z\u00e9ro) entr\u00e9es sur 14801 lignes, cette colonne n'ajoutera aucune valeur au mod\u00e8le. De ce fait, on peut laisser de cot\u00e9 cette colonne.","46ebcc5b":"**Fig. 1.-**  Toutes les valeurs d'humidit\u00e9 sauf RH_6 et RH_out suivent une distribution normale, c'est-\u00e0-dire que toutes les lectures des capteurs \u00e0 l'int\u00e9rieur de la maison proviennent d'une distribution normale.","739ba902":"**Nous aurons a utiliser les methodes suivantes**\n\n1. Mod\u00e8les lin\u00e9aires :\n  - Lasso Regression\n  - Ridge Regression\n \n2. Support Vector Machine\n  - Support Vector Regression\n\n3. Nearest Neighbor Regressor\n - KNeighborsRegressor\n\n4. Tree based Regression models\n  - Randoms Forests\n  - Gradient Boosting Machines\n  - ExtraTreeRegression\n\n5. R\u00e9seaux Neuronaux\n  - MLP","9d069c69":"Dans la colonne Appareils, on constate que 75% de la consommation d'appareils est inf\u00e9rieure \u00e0 100 Wh. Mais, il peut y avoir une consommation maximale de 1080 Wh. Avec une telle consommation, il y aura des valeurs distantes dans cette colonne et le nombre de cas o\u00f9 la consommation est tr\u00e8s \u00e9lev\u00e9e est tr\u00e8s petit.","f1aceae2":"**Mod\u00e8le de r\u00e9f\u00e9rence**\n-\tScore de formation R2 - 0,97\n-\tTest du score R2 - 0,57\n-\tRMSE sur les donn\u00e9es de test - 0,66","d0ac91b5":"**Focalisation des d\u00e9placements pour RH_6, RH_out, visibility, windspeed en raison d'une distribution irr\u00e9guli\u00e8re**","3f4b8eeb":"On a enregistre 5472","8963c3e7":"Dans les colonnes de temp\u00e9rature, on constate que la temp\u00e9rature \u00e0 l'int\u00e9rieur de la maison varie entre 14,89 degr\u00e9s et 29,85 degr\u00e9s (T2 et T9), la temp\u00e9rature ext\u00e9rieure (T6) varie entre -6,06 degr\u00e9s et 28,29 degr\u00e9s. On peut dire que cette variation est li\u00e9e \u00e0 la conservation des capteurs qui sont \u00e0 l'ext\u00e9rieur de la maison.","8eb2a6fa":"Le score R2 de l'ensemble d'entra\u00eenement peut \u00eatre le signe d'un sur-ajustement sur l'ensemble d'entra\u00eenement","fae36c83":"D\u2019apr\u00e8s ce graphe, on constate que : RH_out, RH_8, RH_1, T3, RH_3","4557071a":"# R\u00e9glage des param\u00e8tres\nD\u2019apr\u00e8s les \u00e9tudes r\u00e9alis\u00e9es, on peut d\u00e9crire les param\u00e8tres ainsi :\nExtraTreeRegressor a obtenu les meilleurs r\u00e9sultats avec les param\u00e8tres par d\u00e9faut. On a utilis\u00e9 la validation crois\u00e9e de recherche de grille \u00e0 l'aide de la fonction GridSearchCV de la biblioth\u00e8que sklearn.model_selection. Les param\u00e8tres qui ont \u00e9t\u00e9 r\u00e9gl\u00e9s :\n-\tn_estimators : le nombre d'arbres \u00e0 utiliser.\n-\tmax_features : le nombre de fonctionnalit\u00e9s \u00e0 consid\u00e9rer \u00e0 chaque division.\n-\tmax_depth : la profondeur maximale de l'arbre, si aucun param\u00e8tre n'est fourni, alors la division continuera jusqu'\u00e0 ce que toutes les feuilles soient pures ou contiennent moins le min_samples_split sp\u00e9cifi\u00e9.\n","6d7c0375":"On constate que, avec le score R2, par rapport au mod\u00e8le de r\u00e9glage 0,63, le score R2 est descendu \u00e0 0,47, soit une diminution de 16%, le score des donn\u00e9es d\u2019entrainement est de 100%. La r\u00e9duction du score R2 est \u00e9lev\u00e9e et on ne doit pas utiliser un ensemble de fonctionnalit\u00e9s r\u00e9duit pour cet ensemble de donn\u00e9es.","1b8c70de":"Le score de l\u2019Erreur quadratique moyenne (RMSE) de l'ensemble de tests est une am\u00e9lioration de 0,60 par rapport \u00e0 0,65 obtenu \u00e0 l'aide d'un mod\u00e8le non ajust\u00e9","4610280b":"**Fig. 3.-** Toutes les lectures de temp\u00e9rature suivent une distribution normale \u00e0 l'exception de T9","068eca40":"**Fig. 5.-** La colonne Visibility est biais\u00e9e n\u00e9gativement et la colonne windspeed est asym\u00e9trique","bd8c69b4":"# Gammes\n\nDe l\u00e0, on peut calculer la gamme d'appareils avec une consommation inf\u00e9rieure \u00e0 200 Wh.\n\n - ((target[target <= 200].count()) \/ (len(target)))*100\n - 90% des appareils consomment moins de 200Wh\n\n","6a92ebc4":"# Conclusion\n\nEn guise de conclusion, on r\u00e9alise que les trois principales caract\u00e9ristiques importantes sont les attributs d'humidit\u00e9, ce qui conduit \u00e0 la conclusion que l'humidit\u00e9 affecte plus la consommation d'\u00e9nergie que la temp\u00e9rature. La vitesse du vent est moins importante car la vitesse du vent n'affecte pas la consommation d'\u00e9nergie \u00e0 l'int\u00e9rieur de la maison. Ainsi, contr\u00f4ler l'humidit\u00e9 \u00e0 l'int\u00e9rieur de la maison peut entra\u00eener des \u00e9conomies d'\u00e9nergie.","95f645f5":"# Mod\u00e9lisation et rep\u00e8res\nLa r\u00e9solution de notre sujet caract\u00e9rise un probl\u00e8me de r\u00e9gression. Cette analyse est une forme de technique de mod\u00e9lisation pr\u00e9dictive qui \u00e9tudie la relation entre une variable d\u00e9pendante (cible) et une ou plusieurs variables ind\u00e9pendantes (pr\u00e9dicteur). Pour notre cas, on aura \u00c0 utiliser les m\u00e9thodes suivantes :\n\n# Pr\u00e9traitement et mise en \u0153uvre des donn\u00e9es\nL'ensemble de fonctionnalit\u00e9s contient des donn\u00e9es dans diff\u00e9rentes plages. Temp\u00e9rature (-6 \u00e0 30), humidit\u00e9 (1\u2013100), vitesse du vent (0 \u00e0 14), visibilit\u00e9 (1 \u00e0 66), pression (729\u2013772) et consommation d'\u00e9nergie de l'application (10\u20131080). En raison de diff\u00e9rentes gammes de fonctionnalit\u00e9s, il est possible que certaines fonctionnalit\u00e9s dominent l'algorithme de r\u00e9gression. Pour \u00e9viter cette situation, toutes les fonctionnalit\u00e9s doivent \u00eatre mises \u00e0 l'\u00e9chelle. Ainsi, les donn\u00e9es ont \u00e9t\u00e9 mises \u00e0 l'\u00e9chelle \u00e0 la moyenne et \u00e0 la variance unitaire \u00e0 l'aide de la classe StandardScaler dans le module sklearn.preprocessing.\n\n# Impl\u00e9mentation ","cb987c5c":"Le score R2 de l'ensemble de tests, une am\u00e9lioration par rapport \u00e0 0,57 obtenu \u00e0 l'aide d'un mod\u00e8le non ajust\u00e9","898fc5f1":"On utilise la colonne **Date** seulement afin de comprendre le comportement de la consommation par rapport \u00e0 la date et qui va \u00eatre supprim\u00e9e \u00e0 la suite","cc78c253":"# **Pr\u00e9vision \u00e9nerg\u00e9tique des appareils**","d5a6b043":"On constate que le jeu de donnees n'a pas de valeurs nulles","1e92890c":"**Fig. 4.-** On constate que les variables al\u00e9atoires rv1 et rv2 ont plus ou moins les m\u00eames valeurs pour tous les enregistrements.","1e9c8e76":"**Fig. 6.-** Cette colonne est asym\u00e9trique de mani\u00e8re post\u00e9rieure, la plupart des valeurs se situent en moyenne autour de 100 Wh. Il y a des valeurs aberrantes dans cette colonne. Aucune colonne n'a une distribution comme la variable cible Appliances.","a8f4bfdc":"# Plages de fonctionnalit\u00e9s \nNotre \u00e9tude est bas\u00e9e sur les plages de fonctionnalit\u00e9s d\u00e9crites ci-apr\u00e8s.\n\n    Temp\u00e9rature: -6 \u00e0 30 degr\u00e9s\n    Humidit\u00e9: 1 \u00e0 100%\n    Vitesse du vent: 0 \u00e0 14 m \/ s\n    Visibilit\u00e9: 1 \u00e0 66 km\n    Pression: 729 \u00e0 772 mm Hg\n    Consommation d'\u00e9nergie de l'appareil: 10 \u00e0 1080 Wh\n\n# Visualisation des donn\u00e9es \nPour mieux visualiser notre jeu de donn\u00e9es, nous allons passer \u00e0 la phase de visualisation afin de mieux comprendre la variation chronologique de la consommation d'\u00e9nergie de l'appareil.\n","34334bda":"# **Enonce du probleme**\nCe projet concerne la pr\u00e9vision de la consommation d\u2019\u00e9nergie des appareils \u00e9lectrom\u00e9nagers en utilisant diff\u00e9rentes sources de donn\u00e9es et param\u00e8tres environnementaux tels que : la temp\u00e9rature, l'humidit\u00e9, les vibrations, la lumi\u00e8re et le bruit afin de pouvoir r\u00e9duire la consommation de l\u2019\u00e9nergie et les \u00e9missions de carbones\n    \n    \nEnonc\u00e9 du probl\u00e8me\nLa tendance \u00e0 la hausse de la consommation d'\u00e9nergie devient une source de pr\u00e9occupation pour le monde entier, car la consommation d'\u00e9nergie augmente d'ann\u00e9e en ann\u00e9e, tout comme les \u00e9missions de carbone et de gaz \u00e0 effet de serre, la majeure partie de l'\u00e9lectricit\u00e9 produite est consomm\u00e9e par le secteur industriel mais une quantit\u00e9 consid\u00e9rable est \u00e9galement consomm\u00e9e par le secteur r\u00e9sidentiel. Il est important d'\u00e9tudier le comportement de consommation d'\u00e9nergie dans le secteur r\u00e9sidentiel et de pr\u00e9dire la consommation d'\u00e9nergie des appareils \u00e9lectrom\u00e9nagers car ils consomment le maximum d'\u00e9nergie dans la r\u00e9sidence. Notre \u00c9tude est bas\u00e9e sur la pr\u00e9vision de consommation d'\u00e9nergie des appareils pour une maison en fonction de facteurs tels que la temp\u00e9rature, l'humidit\u00e9 et la pression. Pour y parvenir, on aura \u00e0 d\u00e9velopper un mod\u00e8le d'apprentissage supervis\u00e9 \u00e0 l'aide d'algorithmes de r\u00e9gression. On utilisera des algorithmes de r\u00e9gression car les donn\u00e9es sont constitu\u00e9es d'entit\u00e9s continues et les appareils ne sont pas identifi\u00e9s dans l'ensemble de donn\u00e9es\n\n\nLes donnees\nDescription de donn\u00e9es disponibles\nPour la r\u00e9alisation de notre projet, nous avons utilis\u00e9 le jeu de donn\u00e9es : \u00ab energydata_complete \u00bb qui est d\u00e9crit comme suit :\n-\tNombre d\u2019instance : 19735\n-\tNombre d\u2019attributs : 29\n-\tVariables ind\u00e9pendantes: 28 (11 temp\u00e9rature, 10 humidit\u00e9, 1 pression, 2 al\u00e9as)\n-\tVariable d\u00e9pendante: 1 (appareils)\n-\tPas de valeurs manquantes\n-\tDate d\u2019utilisation : Depuis 2017-02-15\n    \nDescription des attributs\n1.\tdate: heure ann\u00e9e-mois-jour heure: minute: seconde\n2.\tlumi\u00e8res: utilisation d'\u00e9nergie des luminaires dans la maison de Wh\n3.\tT1: temp\u00e9rature dans le coin cuisine, en degr\u00e9s Celsius\n4.\tT2: temp\u00e9rature dans le salon, en degr\u00e9s Celsius\n5.\tT3: Temp\u00e9rature dans la buanderie\n6.\tT4: Temp\u00e9rature dans le bureau, en degr\u00e9s Celsius\n7.\tT5: Temp\u00e9rature dans la salle de bain, en degr\u00e9s Celsius\n8.\tT6: Temp\u00e9rature \u00e0 l'ext\u00e9rieur du b\u00e2timent (c\u00f4t\u00e9 nord), en degr\u00e9s Celsius\n9.\tT7: Temp\u00e9rature dans la salle de repassage, en degr\u00e9s Celsius\n10.\tT8: Temp\u00e9rature dans la chambre adolescent 2, en Celsius\n11.\tT9: Temp\u00e9rature dans la chambre des parents, en degr\u00e9s Celsius\n12.\tT_out: Temp\u00e9rature ext\u00e9rieure (depuis la station m\u00e9t\u00e9o de Chi\u00e8vres), en Celsius\n13.\tPoint de ros\u00e9e: (depuis la station m\u00e9t\u00e9o de Chi\u00e8vres), \u00b0 C\n14.\tRH_1: Humidit\u00e9 dans le coin cuisine, en%\n15.\tRH_2: Humidit\u00e9 dans le salon, en%\n16.\tRH_3: Humidit\u00e9 dans la buanderie, en%\n17.\tRH_4: Humidit\u00e9 dans le bureau, en%\n18.\tRH_5: Humidit\u00e9 dans la salle de bain, en%\n19.\tRH_6: Humidit\u00e9 \u00e0 l'ext\u00e9rieur du b\u00e2timent (c\u00f4t\u00e9 nord), en%\n20.\tRH_7: Humidit\u00e9 dans la salle de repassage, en%\n21.\tRH_8: Humidit\u00e9 dans la chambre d'adolescent 2, en%\n22.\tRH_9: Humidit\u00e9 dans la chambre des parents, en%\n23.\tRH_out: Humidit\u00e9 ext\u00e9rieure (depuis la station m\u00e9t\u00e9o de Chi\u00e8vres), en%\n24.\tPression: (depuis la station m\u00e9t\u00e9o de Chi\u00e8vres), en mm Hg\n25.\tVitesse du vent: (depuis la station m\u00e9t\u00e9o de Chi\u00e8vres), en m \/ s\n26.\tVisibilit\u00e9: (depuis la station m\u00e9t\u00e9o de Chi\u00e8vres), en km\n27.\tRv1: variable al\u00e9atoire 1, non dimensionnelle\n28.\tRv2: variable al\u00e9atoire 2, non dimensionnelle\n29.\tAppareils: \u00e9nergie totale utilis\u00e9e par les appareils, en Wh","006fae95":"**#Ajout d'une colonne pour marquer les jours de la semaine (0) et les week-ends (1) pour l'\u00e9valuation des s\u00e9ries chronologiques**","92773212":"# Correlation\nOn va donc faire la corr\u00e9lation entre les variables en Utilisant la m\u00e9t\u00e9o, la temp\u00e9rature, les appareils et la colonne al\u00e9atoire pour voir la corr\u00e9lation.","416562d5":"# Fonction d'obtention de meilleures corr\u00e9lations ","0a9ad2fd":"**En raison du grand nombre d'entr\u00e9es nulles, cette colonne n'est pas tr\u00e8s utile et sera ignor\u00e9e dans le reste du mod\u00e8le.**","07ea371f":"**Fig. 2.-** Toutes les lectures de temp\u00e9rature suivent une distribution normale \u00e0 l'exception de T9","d851e15a":"# **Exploration des donnees**","5b027ae5":"# Observations des r\u00e9sultats\nD\u2019apr\u00e8s les r\u00e9sultats recueillis, on constate que :\n-\tExtraTreeRegressor avec un score R2 de 57%, fonctionne mieux que tous les autres r\u00e9gresseurs en termes de toutes les mesures, sauf pour le temps de formation.\n-\tLe score RMSE le plus faible est \u00e9galement obtenu par ExtraTreeRegressor est 65%.\n-\tLa r\u00e9gularisation au lasso sur la r\u00e9gression lin\u00e9aire \u00e9tait le mod\u00e8le le moins performant.\n"}}