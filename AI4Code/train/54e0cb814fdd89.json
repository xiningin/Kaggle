{"cell_type":{"6f7f5cfc":"code","0634879b":"code","28f8d674":"code","38ec90ea":"code","6f37c888":"code","276a78d3":"code","60d1193e":"code","8246b34f":"code","8ec85a7b":"code","404597c1":"code","e48ef992":"code","149e43c9":"code","0fbbc23b":"code","99beca39":"code","fa2e4414":"code","022329f7":"code","a2773af5":"code","b99b1459":"code","42b815a8":"code","4cdbaaac":"code","31ffb4d7":"code","d45b2665":"code","e0dd76e2":"code","2f8ebad8":"code","9ffb6d61":"code","40786177":"code","c6311a81":"code","2466bcd2":"code","f0a53755":"markdown","c7c2ca7d":"markdown","043012b1":"markdown","c0bacd98":"markdown","e516e583":"markdown","67665624":"markdown","03287447":"markdown","e166991f":"markdown","9443bc74":"markdown","e97b663c":"markdown","65858861":"markdown","acf9fbb3":"markdown","cc3606b4":"markdown","cbd3db7d":"markdown","ae51cfde":"markdown","e71fdef9":"markdown","cd60fe3b":"markdown","9fe9cd73":"markdown","54f1c42f":"markdown","c5244167":"markdown","93d3857a":"markdown","07676e7a":"markdown","a417a144":"markdown","ddd6dd11":"markdown","dd856b2f":"markdown","7a8b248f":"markdown","7ecdcf43":"markdown","3403b0e2":"markdown","39f40ba9":"markdown"},"source":{"6f7f5cfc":"#Import Libraries\nimport numpy as np\nimport pandas as pd \nfrom scipy import stats\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom numpy import sort\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV,RandomizedSearchCV\nfrom sklearn.utils import resample\nfrom sklearn.feature_selection import SelectFromModel,SelectKBest,f_classif\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, precision_score,recall_score,classification_report,roc_curve,roc_auc_score","0634879b":"data = pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")\ndata.head()","28f8d674":"data.info()","38ec90ea":"#Sample proportion of diabetes:\nprop = 100*data['Outcome'].value_counts()\/len(data)\n\nlabels = ['No Diabetes '+ str(round(prop[0],2)) + \"%\",'Diabetes '+ str(round(prop[1],2)) + \"%\"]\ncolormap = {'tab:orange','lightgrey'}\ndata['Outcome'].value_counts().plot.pie(startangle=90, colors=colormap, labels=labels)\nplt.title(\"Diabetes Overall Sample Proportion\", fontsize=15)\nplt.ylabel('')\ncircle = plt.Circle((0,0),0.7,color=\"white\")\np = plt.gcf()\np.gca().add_artist(circle)\nplt.show()","6f37c888":"data.describe()","276a78d3":"#Distributions of independent variable:\nfeatures = data.columns.drop('Outcome')\n\nnrows=4\nncols=2\nplt.subplots(nrows,ncols,figsize=(8,8))\nplt.tight_layout()\n\nj=1\nfor i in features:\n    plt.subplot(nrows,ncols,j)\n    sns.distplot(data[i],bins=50, kde=False, color='teal')\n    j+=1","60d1193e":"#Create missing markers:\nmiss_markers = []\n\nfor i in features:\n    if i in ['Pregnancies','Age','DiabetesPedigreeFunction']:\n        pass\n    else:\n        data['missing_'+i]  = data[i].apply(lambda x: 1 if x==0 else 0)\n        miss_markers.append('missing_'+i)\n        \n #Create DF of proportions:\nprops = pd.DataFrame(index=[0,1])\n\nfor i in miss_markers:\n    props[i] = data.groupby(i).Outcome.sum()\/data.groupby(i).Outcome.count()\n\nprops['Total']=1\n\n#Proportion of missing values by variable:\nfor i in miss_markers:\n    percent_missing = 100*data[i].value_counts()\/len(data)\n    print(\"Percentage missing in \", i, \": \",round(percent_missing[1],2),\"%\")\n    \n","8246b34f":"#Chart proportion diabetic by missing status:\n\nfor i in miss_markers:\n    plt.figure(figsize=(4,4))\n    sns.barplot(props.index,props['Total'],color='lightgrey')\n    sns.barplot(props.index,props[i],color='tab:orange')\n    plt.title(i,fontsize=20)\n    plt.ylabel(\"Proportion Diabetic\")\n    plt.show()\n    \ndata.drop(miss_markers,axis=1,inplace=True)","8ec85a7b":"#Boxplots of features by diabetes status:\nnrows=3\nncols=3\nplt.subplots(nrows,ncols,figsize=(9,9))\nplt.tight_layout()\n\nj=1\nfor i in features:\n    if i in ['Pregnancies','Age','DiabetesPedigreeFunction']:\n        plt.subplot(nrows,ncols,j)\n        sns.boxplot(data['Outcome'],data[i], palette={'lightgrey','tab:orange'})\n        plt.xlabel(\"\")\n        plt.title(i)\n        j+=1\n    else:\n        mini = data[data[i]!=0]\n        plt.subplot(nrows,ncols,j)\n        sns.boxplot(mini['Outcome'],mini[i], palette={'lightgrey','tab:orange'})\n        plt.xlabel(\"\")\n        plt.title(i)\n        j+=1\n        ","404597c1":"#Recode missing values as nan:\nmiss_data = data.copy()\n\nfor i in ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n       'BMI']:\n    miss_data[i] = miss_data[i].replace(0,np.nan)\n    \nmiss_data['SkinThickness'] = miss_data['SkinThickness'].replace(99,np.nan)","e48ef992":"#Feature correlation excluding missing:\ncorr_mat = miss_data[features].corr()\n\nmask = np.zeros_like(corr_mat,dtype=np.bool)\nmask[np.triu_indices_from(mask)]=True\nsns.heatmap(corr_mat,annot=True,mask=mask,cmap=sns.diverging_palette(240,10,as_cmap=True),vmin=-1,vmax=1)\nplt.title(\"Feature Correlation Matrix\",fontsize=20)\nplt.show()","149e43c9":"#Pairplots of features:\nsns.pairplot(miss_data, hue='Outcome', palette={'darkgrey','tab:orange'})\nplt.show()","0fbbc23b":"#Upscale diabetes group: miss_data.Outcome.value_counts()\n\ndf_min = miss_data[miss_data['Outcome']==1] \ndf_maj = miss_data[miss_data['Outcome']==0]\n\ndf_min_upscaled = resample(df_min, replace=True,\n                           n_samples=500,\n                           random_state=0) \nmiss_data = pd.concat([df_maj,df_min_upscaled]) \nmiss_data.Outcome.value_counts()\n","99beca39":"#Create a test set:\ny = miss_data['Outcome'].copy()\nX = miss_data[features].copy()\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nprint(\"X\",X.shape)\nprint(\"X_train\",X_train.shape)\nprint(\"X_test\",X_test.shape)","fa2e4414":"#Create a modeling pipeline:\n# Create num transformer for imputing and scaling then bundle into a preprocessor:\n\nnum_transformer = Pipeline( steps=[('imputer',SimpleImputer(strategy='median')),\n                                        ('scaler',MinMaxScaler())\n                                       ])\n\n\npreprocessor = ColumnTransformer( transformers = [('num',num_transformer, features)\n                                                 ])","022329f7":"#Define the L1 Logistic Regression model and\n#bundle into a processing and modelling pipeline:\n\nclf = LogisticRegression(penalty=\"l1\",solver='liblinear',random_state=0)\n\npipe = Pipeline(steps = [('preprocessing',preprocessor),\n                            ('modelling', clf)\n                            ])","a2773af5":"#Tune C with GridSearchCV:\n\nhparams = {'modelling__C':[0.001,0.003,0.01,0.03,0.1,0.3,1,3,10,30,100]}\n\ngrid = GridSearchCV(pipe,param_grid = hparams,cv=5,scoring='roc_auc',n_jobs=3,verbose=1)\ngrid.fit(X_train,y_train)\nprint(\"Best params\",grid.best_params_)\nprint(\"Best score:\",grid.best_score_)","b99b1459":"#Plot kept coefficients:\ncoefs = pd.Series(grid.best_estimator_.named_steps['modelling'].coef_[0],index = X.columns)\nkept = coefs[coefs!=0]\nkept.sort_values().plot.barh()\n\nplt.title(\"L1 Regularised Coefficients\", fontsize=15);","42b815a8":"#Create table of results:\nmodel_table = pd.DataFrame(columns=[\"Model\",\"AUC\"])\n\n\n#Fit baseline models:\nfor i in [LogisticRegression(),KNeighborsClassifier(), SVC(kernel='linear'), SVC(kernel='rbf'),RandomForestClassifier(random_state=0),XGBClassifier(random_state=0)]:\n    #define processing pipeline:\n    num_transformer = Pipeline( steps=[('imputer',SimpleImputer(strategy='median')),\n                                        ('scaler',MinMaxScaler())\n                                       ])\n    preprocessor = ColumnTransformer( transformers = [('num',num_transformer, features)\n                                                 ])\n    #Fit model:\n    clf = i\n    \n    pipe = Pipeline(steps = [('preprocessing',preprocessor),\n                            ('modelling', clf)\n                            ])\n    \n    scores = cross_val_score(pipe,X_train,y_train,cv=5,scoring='roc_auc')\n    \n    \n    #Add to model table:\n    model_table = model_table.append({\"Model\":i,\"AUC\":scores.mean()},ignore_index=True)\n\nround(model_table,3)","4cdbaaac":"#Tune RF model with GridSearchCV:\n#Create pipeline:\nnum_transformer = Pipeline( steps=[('imputer',SimpleImputer(strategy='median')),\n                                        ('scaler',MinMaxScaler())\n                                       ])\npreprocessor = ColumnTransformer( transformers = [('num',num_transformer, features)\n                                                 ])\nclf = RandomForestClassifier(random_state=0)   \n    \npipe = Pipeline(steps = [('preprocessing',preprocessor),\n                            ('modelling', clf)\n                            ])\n\nhparams = {'modelling__n_estimators':[n for n in range(100,501,100)],\n          'modelling__max_depth':[4,6,8]}\n\ngrid = GridSearchCV(pipe,param_grid = hparams,cv=5,scoring='roc_auc',n_jobs=3,verbose=1)\ngrid.fit(X_train,y_train)\nprint(\"Best params\",grid.best_params_)\nprint(\"Best score:\",grid.best_score_)\n","31ffb4d7":"#Add Tuned RF model to table:\nclf = RandomForestClassifier(n_estimators=100,max_depth=8, random_state=0)\n\npipe = Pipeline(steps = [('preprocessing',preprocessor),\n                            ('modelling', clf)\n                            ])\n    \nscores = cross_val_score(pipe,X_train,y_train,cv=5,scoring='roc_auc')\n    \n#Add to model table:\nmodel_table = model_table.append({\"Model\":clf,\"AUC\":scores.mean()},ignore_index=True)\n  \n\nround(model_table,3)","d45b2665":"#Tune XGBoost model with GridSearchCV:\n#Create pipeline:\nnum_transformer = Pipeline( steps=[('imputer',SimpleImputer(strategy='median')),\n                                        ('scaler',MinMaxScaler())\n                                       ])\npreprocessor = ColumnTransformer( transformers = [('num',num_transformer, features)\n                                                 ])\nclf = XGBClassifier(random_state=0)   \n    \npipe = Pipeline(steps = [('preprocessing',preprocessor),\n                            ('modelling', clf)\n                            ])\n\nhparams = {'modelling__learning_rate':[0.001,0.003,0.01,0.03,0.1,0.3],\n           'modelling__n_estimators':[n for n in range(100,501,100)],\n          'modelling__max_depth':[4,6,8]}\n\ngrid = GridSearchCV(pipe,param_grid = hparams,cv=5,scoring='roc_auc',n_jobs=-1,verbose=3)\ngrid.fit(X_train,y_train)\nprint(\"Best params\",grid.best_params_)\nprint(\"Best score:\",grid.best_score_)","e0dd76e2":"#Fit the tuned XGBoost model:\nclf = XGBClassifier(learning_rate=0.03,n_estimators=500,max_depth=6, random_state=0)\n\npipe = Pipeline(steps = [('preprocessing',preprocessor),\n                            ('modelling', clf)\n                            ])\n    \nscores = cross_val_score(pipe,X_train,y_train,cv=5,scoring='roc_auc')\n\n#Add to table of results:\nmodel_table = model_table.append({\"Model\":clf,\"AUC\":scores.mean()},ignore_index=True)\nround(model_table,3)","2f8ebad8":"#Refit tuned RF model:\nclf = RandomForestClassifier(n_estimators=100,max_depth=8, random_state=0)\n\npipe = Pipeline(steps = [('preprocessing',preprocessor),\n                            ('modelling', clf)\n                            ])\n\npipe.fit(X_train,y_train)\ny_preds = pipe.predict(X_test)\ny_probs = pipe.predict_proba(X_test)[:,1]\n\n#Save tuned RF model Feature Importances:\ntuned_feat_imps = pipe.named_steps['modelling'].feature_importances_","9ffb6d61":"#Plot Tuned Random Forest Feature Importances:\n\nfeat_imp = pd.Series(tuned_feat_imps,index=X.columns)\nfeat_imp = feat_imp.sort_values(ascending=False)\nsns.barplot(feat_imp,feat_imp.index)\nplt.title(\"Feature Importances in Tuned Random Forest Model\",fontsize=15);","40786177":"#Confusion matrix for tuned RF model:\ncon_mat = pd.crosstab(y_test,y_preds,rownames=['Actual'],colnames=['Predicted'])\nsns.heatmap(con_mat,annot=True,cmap='Blues',vmin=0)\nplt.title(\"Confusion Matrix for tuned RF Model\", fontsize=15)\nplt.show()","c6311a81":"#ROC curve:\nfpr,tpr,_ = roc_curve(y_test,y_probs)\nauc = roc_auc_score(y_test,y_probs)\nplt.plot(fpr,tpr,label=\"RF Model,auc=\" +  str(round(auc,4)))\nplt.title(\" Random Forest Model ROC Curve\",fontsize=15)\nplt.legend(loc=4);","2466bcd2":"print(classification_report(y_test, y_preds))","f0a53755":"## 2.3 Feature Selection Using L1 Regularization\n\nWe will make an initial selection of the best features using L1 regularisation. We will further select features for the logistic regression model and Feature Importance for the Random  Forest and XGBoost models.","c7c2ca7d":"We can see that the diabetic group have higher glucose levels, age, BMI, pregnancies and insulin measures.","043012b1":"## 3.3 Tuned RF Model Feature Importances\nRefit the tuned RF model to all the training data to obtain Feature Importances and predictions.","c0bacd98":"## 2.3.1 Examine Selected Features","e516e583":"34.9% of the women in the sample have diabetes. This means our classes are imbalanced.","67665624":"## 2.2 Imputation and Scaling\n","03287447":"We can clearly see the correlation between some of the features, such as skin thickness and BMI. We can also see the difference across features of the diabetic status, clearest with glucose and more subtly with features like BMI.","e166991f":"## 3.2 Tuned XGB Model","9443bc74":"## 3.1 Tuned Random Forest Model","e97b663c":"We have 768 subjects in the dataset and 8 features, all of which are shown as numeric. We have a target variable, Outcome, which we know to be a binary indicator of the presence or absence of diabetes.There are no missing values.","65858861":"# 4.Summary\n\nThe Random Forest Classifier model is a good predictor of diabetes in Pima Indian women aged over 21, AUC score = 90.1%, sensitivity = 82.0%. The mose important feature for prediction is Glucose.\n\nNOTE: Almost half the subjects had missing insulin values, 48.7%, and almost a third have missing skin thickness measurements, 29.6%.","acf9fbb3":"In the tuned Random Forest model Glucose is the most important feature while SkinThickness is the least important feature. ","cc3606b4":"## 1.2 Features Relationship with Outcome and Each Other\n\nWe will examine the features against outcome, excluding the missing values identified earlier.","cbd3db7d":"We will replace the missing - previously zero - values with the variable medians as some of the distributions are skewed. We don't generally need to scale for logistic regression, but, by default, Python uses regularisation which requires scaling. We will also be fitting a KNN model which also requres scaling.  We will create a modelling pipeline to do this.","ae51cfde":"The tuned RF model is the best so far. ","e71fdef9":"Our baseline Random Forest and XGBoost models achieve the highest AUC scores so we will tune them:","cd60fe3b":"## 1.1 Missing Values\n\nWe will examine whether there is a diffence in diabetes status for the missing and non-missing data. We will have to assume that there are no missing pregnancy values.","9fe9cd73":"Our tuned Random Forest Classifier performed worse than our default RF classifier. This is likely to be because the default RF model has no max_depth, which can lead to overfitting.","54f1c42f":"With the exception of the number of pregnancies, which is discrete, all other features are continuous numeric values. It is evident that missing values have been coded as 0 in the dataset because zero values in the tests are not possible. There is also a value of 99 in the skin thickness which is likely to be another missing value as the subject's BMI is only average. \nBefore we explore the relationship between outcome and independent variables we will examine the relationship between outcome and missing status. ","c5244167":"We can see that a few of the features are moderately correlated - Age and number of pregnancies, Insulin and glucose levels, skin thickness and BMI - but not so much as to cause concern. ","93d3857a":"# 1. EDA\n\nFirst we will examine the distributions of the dependent and independent variables.","07676e7a":"**Context**\n\nThis dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases, Maryland, USA. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females, at least 21 years old and of Pima Indian heritage.\n\n**Dataset**\n\nThe datasets consists of several medical predictor variables and one binary target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level and age.\n\n**Notebook Contents**\n\n- 1. Exploratory Data Analysis\n- 2. Feature Engineering\n- 3. Machine Learning\n- 4. Summary","a417a144":"# 3. Machine Learning\n\nWe will start by fitting baseline models. We will fit the following models:\n- L2 Logistic Regression,\n- KNN,\n- SVCs,\n- Random Forest,\n- XGBoost","ddd6dd11":"## 2.1 Upscale Diabetes Group\n\nWe saw earlier that our diabetes classes are not balanced so we will upscales the diabetes group.","dd856b2f":"None of the features were dropped through L1 regularisation. Glucose and BMI are the features most strongly associated with diabetes status.  ","7a8b248f":"# Diabetes Classification Predictive Modelling","7ecdcf43":"Almost half the subjects have missing insulin values, 48.7%, and almost a third have missing skin thickness measurements, 29.6%.","3403b0e2":"The proportion of diabetics are, with the exception of BMI, similar across missing status so we will impute with variable means or medians where appropriate. The proportion of diabetics is lower in the missing BMI group, but only 11(1.4%) of the BMI values are missing so this difference is not significant( Fisher's Exact test, p=0.35).","39f40ba9":"# 2. Feature Engineering\n\nWe will balance the diabetes classes. We will also impute missing values and scale the data using a pipeline."}}