{"cell_type":{"20aa2251":"code","bf61385d":"code","59d85101":"code","44ae422d":"code","8b9e0bce":"code","68a48ee2":"code","287f5166":"code","66aa8e48":"code","d30743ae":"code","c043a256":"code","fb1132d1":"markdown","707adfc4":"markdown","0593b75d":"markdown","52d683de":"markdown","685fbd4e":"markdown"},"source":{"20aa2251":"import os\nimport numpy as np\nimport pandas as pd\nimport soundfile as sf\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook as tqdm\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset, sampler, Subset\nfrom torchvision.datasets import DatasetFolder, ImageFolder\nfrom torchvision.transforms import Compose\nimport torchvision.models as models\nimport torchaudio\nfrom torchaudio.transforms import MelSpectrogram, AmplitudeToDB, Resample\nfrom fastai.vision.all import *\nimport librosa\n%matplotlib inline","bf61385d":"path = \"..\/input\/birdclef-2021\/\"\n\nclasses = os.listdir(path+\"train_short_audio\")\nclasses.sort()\nprint(classes, len(classes))","59d85101":"# Source: https:\/\/www.kaggle.com\/drcapa\/birdclef-2021-starter\ndef plot_audio_file(data):\n    \"\"\" Plot the audio data\"\"\"\n    \n    sr = 32000\n    fig = plt.figure(figsize=(8, 4))\n    x = range(len(data))\n    y = data\n    plt.plot(x, y)\n    plt.plot(x, y, color='red')\n    plt.legend(loc='upper center')\n    plt.grid()","44ae422d":"def to_tensor(x):\n    return torch.Tensor(x)\n\ndef expand_dim(x):\n    return x.unsqueeze(0)\n\ndef padding(x):\n    t_size = 32000*5\n    size = t_size\/\/len(x) + 1\n    x = torch.cat([x]*size, 0)\n    return x[0:t_size]\n    \n\ntransforms = Compose([\n    to_tensor,\n    padding,\n    MelSpectrogram(sample_rate=32000, n_mels=128),\n    AmplitudeToDB(),\n    expand_dim\n])\n\n\ndef loader(file):\n    data, samplerate = sf.read(file)\n    return data\n\nclass BirdClefShort(DatasetFolder):\n    #https:\/\/pytorch.org\/vision\/stable\/_modules\/torchvision\/datasets\/folder.html#DatasetFolder\n    #Modify code to return a One Hot vector instead of the ID of the class\n    def _find_classes(self, dir: str):\n        \"\"\"\n        Finds the class folders in a dataset.\n\n        Args:\n            dir (string): Root directory path.\n\n        Returns:\n            tuple: (classes, class_to_idx) where classes are relative to (dir), and class_to_idx is a dictionary.\n\n        Ensures:\n            No class is a subdirectory of another.\n        \"\"\"\n        classes = [d.name for d in os.scandir(dir) if d.is_dir()]\n        classes.sort()\n        class_to_idx = {cls_name: np.eye(len(classes))[i] for i, cls_name in enumerate(classes)}\n        return classes, class_to_idx\n\nmain_ds = BirdClefShort(\"..\/input\/birdclef-2021\/train_short_audio\", loader=loader, extensions=\".ogg\", transform=transforms, target_transform=to_tensor)","8b9e0bce":"plt.figure(figsize=(16,16))\nplt.imshow(main_ds[12][0].numpy().transpose(1,2,0), cmap='hot')\n","68a48ee2":"train_ids, valid_ids = train_test_split(range(len(main_ds)), test_size=0.33, random_state=2021)\ntrain_ds, valid_ds = Subset(main_ds, train_ids), Subset(main_ds, valid_ids)\n\ndls = DataLoaders.from_dsets(train_ds, valid_ds, batch_size=64, num_workers=4, shuffle=True)","287f5166":"model = models.resnet18(pretrained=True)\nmodel.conv1= nn.Conv2d(1, model.conv1.out_channels, \n                      kernel_size=model.conv1.kernel_size[0], \n                      stride=model.conv1.stride[0], \n                      padding=model.conv1.padding[0])\n\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, len(classes))\n","66aa8e48":"cbs=[SaveModelCallback(monitor='f1_score',comp=np.greater)]\n\nif torch.cuda.is_available(): dls.cuda(), model.cuda()\nlearn = Learner(dls, model, metrics= [accuracy_multi, F1ScoreMulti()], loss_func= nn.BCEWithLogitsLoss(), opt_func=ranger, cbs=cbs)\n#if mixed_precision_training: learn.to_fp16()","d30743ae":"learn.fit_one_cycle(3, lr_max=1e-3)","c043a256":"learn.save(\"last\", with_opt=False)\ntorch.save(learn.model.state_dict(),f'model_classic.pth')","fb1132d1":"# Dataset and Transformations\n\nWe will leverage the functionality of DatasetFolder from torchvision and expand it to read audio files from the \"train_short_audio\" folder.","707adfc4":"# Helper Functions","0593b75d":"# Model","52d683de":"# Train","685fbd4e":"# Classes"}}