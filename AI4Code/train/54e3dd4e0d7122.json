{"cell_type":{"4f2acd35":"code","46aff4f6":"code","0d7d0e13":"code","4038e3c9":"code","1e8c1c74":"code","be06a6eb":"code","b3ececf8":"code","a03e7795":"code","7246c23e":"code","f4c25a4c":"markdown","cd2d4ab3":"markdown","df833c9a":"markdown","7fa3f35a":"markdown","89427f13":"markdown","1053600d":"markdown","a261406b":"markdown","4cbb75ad":"markdown","f1df9d75":"markdown","d646a2b0":"markdown","54b07025":"markdown","61c32b3a":"markdown","092f105c":"markdown"},"source":{"4f2acd35":"import numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nfrom sklearn.model_selection import GridSearchCV\n\n%matplotlib inline\nimport os\nprint(os.listdir(\"..\/input\"))","46aff4f6":"file = '..\/input\/airfoil.csv'\nairfoil_data = pd.read_csv(file, header=None)\nairfoil_data.columns = ['frequency', 'angle_of_attack', 'chord', 'velocity', 'suc_displacement', 'sound_pressure']\nsns.pairplot(data=airfoil_data)","0d7d0e13":"y = airfoil_data.sound_pressure\nfeatures = ['frequency', 'angle_of_attack', 'chord', 'velocity', 'suc_displacement']\nX = airfoil_data[features]\ny = y.to_frame()","4038e3c9":"from sklearn import preprocessing\nscaler_x = preprocessing.MinMaxScaler()\nX_scaled = scaler_x.fit_transform(X)\nscaler_y = preprocessing.MinMaxScaler()\ny_scaled = scaler_y.fit_transform(np.array(y).reshape(-1,1))\ny_scaled = y_scaled.reshape(-1)\ntrain_X, val_X, train_y, val_y = train_test_split(X_scaled, y_scaled, random_state=1)\nval_y = scaler_y.inverse_transform(np.array(val_y).reshape(-1,1))","1e8c1c74":"def compare_models(a,b,c,d):\n    print('\\nCompare Multiple Classifiers:')\n    print('\\nK-Fold Cross-Validation Accuracy:\\n')\n    models = []\n    models.append(('LR', LinearRegression()))\n    models.append(('RF', RandomForestRegressor(n_estimators=10)))\n    models.append(('DT', DecisionTreeRegressor()))\n    models.append(('XGB', XGBRegressor()))\n    models.append(('SVR RBF', SVR(gamma='auto')))\n    models.append(('SVR Lin', SVR(kernel='linear', gamma='auto')))\n    resultsAccuracy = []\n    names = []\n    for name, model in models:\n        model.fit(a,b)\n        kfold = model_selection.KFold(n_splits=10, random_state=7)\n        accuracy_results = model_selection.cross_val_score(model, a,b, cv=kfold)\n        resultsAccuracy.append(accuracy_results)\n        names.append(name)\n        accuracyMessage = \"%s: %f (%f)\" % (name, accuracy_results.mean(), accuracy_results.std())\n        print(accuracyMessage)\n        \n    # boxplot algorithm comparison\n    fig = plt.figure()\n    fig.suptitle('Algorithm Comparison: Accuracy')\n    ax = fig.add_subplot(111)\n    plt.boxplot(resultsAccuracy)\n    ax.set_xticklabels(names)\n    ax.set_ylabel('Cross-Validation: Accuracy Score')\n    plt.show()\n    return\n\ncompare_models(train_X, train_y, val_X, val_y)","be06a6eb":"rf_model = RandomForestRegressor(n_estimators=100, random_state=1)\nrf_model.fit(train_X, train_y)\nrf_val_predictions = rf_model.predict(val_X)\nrf_val_predictions = scaler_y.inverse_transform(np.array(rf_val_predictions).reshape(-1,1))\n\nrf_val_mae = mean_absolute_error(rf_val_predictions, val_y)\nprint(\"Validation MAE when using Random Forest {}\".format(rf_val_mae))\nval_r2 = r2_score(val_y, rf_val_predictions)\nprint(\"Validation R^2: {}\".format(val_r2))\nkfold = model_selection.KFold(n_splits=10, random_state=7)\naccuracy_results = model_selection.cross_val_score(rf_model, train_X,train_y, cv=kfold)\naccuracyMessage = \"%s: %f (%f)\" % ('RF Cross Validation', accuracy_results.mean(), accuracy_results.std())\nprint(accuracyMessage)","b3ececf8":"fig, ax = plt.subplots()\nax.scatter(val_y, rf_val_predictions)\nax.plot([val_y.min(), val_y.max()], [rf_val_predictions.min(), rf_val_predictions.max()], 'k--', lw=4)\nax.set_xlabel('Measured')\nax.set_ylabel('Predicted')\nplt.show()","a03e7795":"xg_model = xgb.XGBRegressor(learning_rate=0.2, max_depth=6, n_estimators=200, random_state=1)\nxg_model.fit(train_X, train_y)\nxg_preds = xg_model.predict(val_X)\nxg_preds = scaler_y.inverse_transform(np.array(xg_preds).reshape(-1,1))\nxg_mae = mean_squared_error(xg_preds, val_y)\nprint(\"Validation MSE when using Gradient Boost {}\".format(xg_mae))\nval_r2 = r2_score(val_y, xg_preds)\nprint(\"Validation R^2: {}\".format(val_r2))\n\nkfold = model_selection.KFold(n_splits=10, random_state=7)\naccuracy_results = model_selection.cross_val_score(xg_model, train_X,train_y, cv=kfold)\naccuracyMessage = \"%s: %f (%f)\" % ('XGB Cross Validation', accuracy_results.mean(), accuracy_results.std())\nprint(accuracyMessage)","7246c23e":"fig, ax = plt.subplots()\nax.scatter(val_y, xg_preds)\nax.plot([val_y.min(), val_y.max()], [xg_preds.min(), xg_preds.max()], 'k--', lw=4)\nax.set_xlabel('Measured')\nax.set_ylabel('Predicted')\nplt.show()","f4c25a4c":"Here, the data is imported and plots between different variables are plotted in order to observe correlations between variables.\n\nThe independent variables are as follows:\n* Frequency (Hz)\n* Angle of Attack (deg)\n* Chord Length (m)\n* Free-stream velocity (m\/s)\n* Suction side displacement thickness (m)\n\nThe dependent variable is:\n* Scaled sound pressure level (dB)","cd2d4ab3":"This exploration demonstrated that hyperparameter tuning of certain models can increase the accuracy of simple models significantly. Finally, we are now able to predict Airfoil Self-Noise to a reasonable degree of accuracy without overfitting\/underfitting or using a complex Neural Network. These models have scope for improvement, and more precise hyperparameter tuning can lead to a more accurate model.","df833c9a":"# XGBoost","7fa3f35a":"# Conclusion","89427f13":"In this notebook, we will be exploring and using different machine learning models to analyze and predict self-noise generated by the NACA 0012 Airfoil due to flow of air around it. A further explanation of the mechanism is detailed in the paper by Brooks et. al. and subsequent papers where the authors attemp to predict self-noise using Artificial Neural Networks. Using this method yields a highly accurate model, however, it takes significant computational resources to train. This ANN model also has the potential to overfit the data.\n\nHence, I decided to test the effectiveness of using simpler models such as Random Forest Regression and Extreme Gradient Boosting Regression to develop an accurate model. Through an iterative approach by tuning the hyperparameters, I was able to obtain ~94% cross-validated accuracy for the XG Boost model and ~92% cross-validated accuracy for the Random Forest model. This demonstrates the viability of using such models to model complex data.","1053600d":"# Comparing Different Models","a261406b":"Since the features vary in terms of orders of magnitude, feature scaling is applied to yield a better fit to the model and increase efficiency of gradient descent. Since the data is not normally distributed (or close to it), a simple min max scaling is applied. The data is also split in terms of input and output variables.","4cbb75ad":"The second algorithm I decided to evaluate is the Extreme Gradient Boosting Regressor algorithm. Here, tuning the hyperparameters resulted in a much higher cross validated accuracy by more than 10%, exceeding the Random Forest Regressor accuracy. Increasing the learning rate and the number of estimators yielded this result.","f1df9d75":"# Random Forest Regressor","d646a2b0":"Here, different regression models are compared in terms of their accuracy. The predicted results are 10 fold cross validated to demonstrate their robustness.","54b07025":"# Splitting and Scaling Data","61c32b3a":"Since it can be observed that the Random Forest Regressor performed the best, I decided to further explore the algorithm's capabilities by tuning the hyperparameters. By increasing the number of estimators, the maximum absolute error decreased considerably.","092f105c":"# Importing Data"}}