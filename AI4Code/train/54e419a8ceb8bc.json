{"cell_type":{"3a4245d5":"code","1b2fecc1":"code","7997b72e":"code","731c8b91":"code","e3c9cac2":"code","c1e70f1e":"code","efc2e0d6":"code","7a51f95e":"code","e840eb74":"code","9cebb8a5":"code","542e3eb4":"code","301e696b":"code","7515f7cf":"code","7d2de392":"code","618dc53e":"code","e6425fe6":"code","972509e5":"code","48daff24":"code","8b054523":"code","a740e28d":"code","03abc38d":"code","a9944843":"code","7a931f72":"code","1f001104":"code","ce1fc618":"code","00ae3332":"code","28d6a1d8":"code","d9bbf745":"code","e1d50226":"code","2625931f":"code","237a67b9":"code","4346a1d5":"code","6d7e4c0c":"code","e5df5c2f":"code","ce24b566":"code","6bbe41e3":"code","43ed68fb":"code","8e3a7332":"code","b173095a":"code","770eeea0":"code","56ad873e":"code","584fa0e8":"code","387ed51f":"code","9da0eb1b":"code","13337362":"code","ef6271a1":"code","e083447e":"code","c83d1e79":"code","b4620850":"code","ac25013a":"code","430d1930":"code","56682109":"code","156a3098":"code","ce367aa6":"code","c0703053":"code","be10507d":"code","13ee4c88":"code","24c71dce":"code","13baf67c":"markdown","51b64a11":"markdown","5784137b":"markdown","a8b9e556":"markdown","e51ee974":"markdown","d0d59b8a":"markdown","49913c49":"markdown","794f01a1":"markdown","19f45384":"markdown","828d479b":"markdown","6c1c7322":"markdown","acb33707":"markdown","86a98758":"markdown","49482adc":"markdown","ff9453c9":"markdown","13fdd7a0":"markdown","82163ad0":"markdown","94bf04f3":"markdown","a387d10c":"markdown","057fcb97":"markdown","69226b2d":"markdown","48e51ccb":"markdown","7d0ddcb2":"markdown","6e362fe3":"markdown","6f92ce84":"markdown","25765d8b":"markdown","38cd9c36":"markdown","2c3ee06d":"markdown","173d5c1d":"markdown","020a5226":"markdown","c1af1be0":"markdown","3e9f523d":"markdown","5759ac95":"markdown","ab696c55":"markdown","7f003f7f":"markdown","a28171fc":"markdown","58c40712":"markdown","d220eae8":"markdown","af40cf8b":"markdown","aac3f096":"markdown","3e190c73":"markdown","094848d8":"markdown","6f453587":"markdown","fc8a8b20":"markdown","0f57b221":"markdown","1abfdf60":"markdown","005f3497":"markdown","8dfb565a":"markdown","6510ac02":"markdown","8a9fa3b5":"markdown","9377cb22":"markdown","0d69db18":"markdown","acd3393c":"markdown","08850469":"markdown","3487773a":"markdown"},"source":{"3a4245d5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings  \nwarnings.filterwarnings('ignore')\nsns.set(font_scale=1.3)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1b2fecc1":"df = pd.read_csv('\/kaggle\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')","7997b72e":"df.head()","731c8b91":"df.info()","e3c9cac2":"df.isnull().sum()","c1e70f1e":"df.duplicated().sum()","efc2e0d6":"df.drop_duplicates(inplace=True)","7a51f95e":"plt.figure(figsize=(8,7))\n\nmy_ticks = ['Less Chance', 'More Chance']\n\nax = sns.countplot(x='output', data=df, palette='rocket')\nax.set_xticklabels(my_ticks)\nplt.xlabel('Output')\nplt.ylabel('Count')\nplt.title('Chances of Heart Attack', fontsize=18)\n\nfor p in ax.patches:\n    width = p.get_width()\n    height = p.get_height()\n    x, y = p.get_xy() \n    ax.annotate(f'{round(height*100\/302,2)}%', (x + width\/2, y + height*1.01), ha='center')\n\nplt.show()","e840eb74":"plt.figure(figsize=(8,7))\n\nmy_ticks = ['Female', 'Male']\n\nax = sns.countplot(x='sex', data=df, palette='viridis')\nax.set_xticklabels(my_ticks)\nplt.xlabel('Sex')\nplt.ylabel('Count')\nplt.title('Gender Distribution', fontsize=18)\n\nfor p in ax.patches:\n    width = p.get_width()\n    height = p.get_height()\n    x, y = p.get_xy() \n    ax.annotate(f'{round(height*100\/302,2)}%', (x + width\/2, y + height*1.01), ha='center')\n\nplt.show()","9cebb8a5":"plt.figure(figsize=(9,7))\n\nmy_ticks = ['No Heart Attack', 'Heart Attack']\nmy_legends = ['Female', 'Male']\n\nsns.set(font_scale=1.3)\nax = sns.countplot(x = 'output', hue='sex', data=df, palette='husl')\nax.set_xticklabels(my_ticks)\n\nplt.xlabel('Output')\nplt.ylabel('Count')\nplt.legend(title = 'Gender', labels = my_legends)\nplt.title('Gender Distribution wrt Heart Attack', fontsize=18)\n\n\nplt.show()","542e3eb4":"plt.figure(figsize=(8,7))\n\nsns.set_theme()\nsns.set(font_scale=1.3)\nsns.displot(x='age', data=df, color='Chocolate', height=7, aspect=1.1)\n\nplt.xlabel('Age')\nplt.ylabel('Count')\nplt.title('Age Distribution', fontsize=18)\n\nplt.show()","301e696b":"v = pd.crosstab(df[\"age\"],df[\"output\"]).reset_index()\nv.columns = ['Age', 'Low Risk', 'High Risk']\n\nv.head()","7515f7cf":"plt.figure(figsize=(12,8))\n\nsns.lineplot(x = 'Age', y='Low Risk', data=v, \n             marker = 'o' ,color = 'darkgreen', lw=2)\nsns.lineplot(x = 'Age', y='High Risk', data=v, \n             marker = 'o', color = 'red', lw=2)\n\nplt.legend(title = 'Risk', labels = ['Low Risk', 'High Risk'])\nplt.xlabel('Age')\nplt.ylabel('No. of People')\nplt.title('Low Risk vs High Risk wrt Age', fontsize=18)","7d2de392":"cat = ['sex', 'cp', 'fbs', 'restecg', 'exng', 'slp', 'caa', 'thall']\n\n\n\nfor i in range(len(cat)):\n    \n    plt.figure(figsize=(20,55))\n    \n    plt.subplot(len(cat),2,1)\n    sns.countplot(x=cat[i] , data=df, palette='mako')\n    plt.xlabel(cat[i].capitalize(), fontsize=20)\n    plt.ylabel('Count', fontsize=20)\n    if i == 0:\n        plt.title('Countplot of Categorical Features', fontsize=22)\n    \n    plt.subplot(len(cat),2,2)\n    sns.countplot(x=cat[i], hue='output', data = df, palette='magma_r')\n    plt.xlabel(cat[i].capitalize(), fontsize=20)\n    plt.ylabel('Count', fontsize=20)\n    plt.legend(title = 'Output')\n    if i == 0:\n        plt.title('Countplot of Categorical Features wrt Target', fontsize=22)\n    \n    plt.show()","618dc53e":"cont = ['age', 'trtbps', 'chol', 'thalachh', 'oldpeak']\n\n\nfor i in range(len(cont)):\n    \n    plt.figure(figsize=(20,40))\n    \n    plt.subplot(len(cont),2,1)\n    sns.histplot(x=cont[i] , data=df, color='Olive', kde=True)\n    plt.xlabel(cont[i].capitalize(), fontsize=20)\n    plt.ylabel('Count', fontsize=20)\n    if i == 0:\n        plt.title('Histplot of Continuous Features', fontsize=22)\n    \n    plt.subplot(len(cont),2,2)\n    sns.histplot(x=cont[i], hue='output', data = df, palette='magma_r', kde=True)\n    plt.xlabel(cont[i].capitalize(), fontsize=20)\n    plt.ylabel('Count', fontsize=20)\n    if i == 0:\n        plt.title('Histplot of Continuous Features wrt Target', fontsize=22)\n    \n    plt.show()","e6425fe6":"pair = df[['age', 'trtbps', 'chol', 'thalachh', 'oldpeak', 'output']]\n\n\nax = sns.pairplot(pair, hue='output', palette='rocket')\nax._legend.remove()\nax.fig.legend(title='Output', labels = ['Heart Attack', 'No Heart Attack'])\nax.fig.subplots_adjust(right=0.8, top=1)\n\nplt.show()","972509e5":"plt.figure(figsize=(20,10))\n\nsns.heatmap(df.corr(), annot=True, cmap='YlGnBu')\n\nplt.show()","48daff24":"df.info()","8b054523":"df = df.astype(float)","a740e28d":"df.info()","03abc38d":"df.head()","a9944843":"norm_df = (df - df.mean()) \/ df.std()","7a931f72":"norm_df.head()","1f001104":"norm_df['output'] = df['output']","ce1fc618":"norm_df.head()","00ae3332":"norm_df = norm_df.sample(frac=1, random_state=123)","28d6a1d8":"print(norm_df.shape)\nprint(302*0.75)","d9bbf745":"train = norm_df[:226]\ntest = norm_df[226:]","e1d50226":"abs(norm_df.corr()['output'])","2625931f":"plt.figure(figsize=(5,10))\n\ncor_df = pd.DataFrame({'Output' : norm_df.corr()['output'].values},\n                     index = norm_df.corr()['output'].index)\n\nsns.heatmap(cor_df, annot=True, cmap='viridis')\n\nplt.show()","237a67b9":"corr = abs(norm_df.corr()['output'])\ncorr[corr>0.25]","4346a1d5":"corr[corr>0.25].index","6d7e4c0c":"features = ['sex', 'cp', 'thalachh', 'exng', 'oldpeak', 'slp', 'caa', 'thall']\ntarget = ['output']","e5df5c2f":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error","ce24b566":"model_KNN = KNeighborsClassifier()\nmodel_RF = RandomForestClassifier()\nmodel_GB = GradientBoostingClassifier()\nmodel_LG = LogisticRegression()\n\nmodel_RF.fit(train[features], train[target])\nacc_RF = model_RF.score(test[features], test[target])\n\nmodel_KNN.fit(train[features], train[target])\nacc_KNN = model_KNN.score(test[features], test[target])\n\nmodel_GB.fit(train[features], train[target])\nacc_GB = model_GB.score(test[features], test[target])\n\nmodel_LG.fit(train[features], train[target])\nacc_LG = model_LG.score(test[features], test[target])\n\nind = ['Random Forest', 'Gradient Boost', 'K Nearest Neighbor', 'Logistic Regression']\npd.DataFrame({'Models': ind, 'Accuracy':[round(acc_RF,2), \n                                         round(acc_GB,2), \n                                         round(acc_KNN,2),\n                                        round(acc_LG,2)],\n              'Mean Sqaured Error' : [mean_squared_error(test['output'], model_RF.predict(test[features])),\n                                      mean_squared_error(test['output'], model_GB.predict(test[features])),\n                                      mean_squared_error(test['output'], model_KNN.predict(test[features])),\n                                      mean_squared_error(test['output'], model_LG.predict(test[features]))\n                  \n              ] }).set_index('Models')","6bbe41e3":"n_est = {}\nfor i in range(10,100):\n    for k in range(1,25):\n        model_RF = RandomForestClassifier(n_estimators=i, max_depth=k, criterion='entropy', random_state=123)\n        model_RF.fit(train[features], train[target])\n        pred = model_RF.predict(test[features])\n        accuracy = model_RF.score(test[features], test[target])\n        n_est[i, k] = round(accuracy*100,2)","43ed68fb":"print(f'(n_estimator, max_depth) @ max accuracy: {max(n_est , key = n_est.get)}')\nprint(f'Max Accuracy: {n_est[max(n_est , key = n_est.get)]}%')","8e3a7332":"ls_a = []\nls_b = []\nfor i in n_est:\n    ls_a.append(i[0])\n    ls_b.append(i[1])  ","b173095a":"plt.figure(figsize=(15,8))\n\nsns.lineplot(x = ls_b, y = list(n_est.values()), color = 'darkgreen', \n             lw=2, ci=0.1, estimator=None, label='max_depth')\n\nsns.lineplot(x = ls_a, y = list(n_est.values()),\n             color = 'darkblue', lw=2, ci=0.1, estimator=None, label='n_estimator')\n\nplt.axhline(y = 89.47, linestyle='-.',color='red')\nplt.axvline(x = 26, linestyle='-.',color='red')\nplt.axvline(x = 2, linestyle='-.',color='red')\n\n\n\n\nplt.text(x=29, y = 74, s='@ n_estimator=26 & max_depth=2, Max Accuracy = 89.47%', fontsize=19)\nplt.title('Accuracy vs n_estimator & max_depth', fontsize=20)\nplt.xlabel('n_estimator & max_depth')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\n\nplt.show()","770eeea0":"gb = {}\nfor i in range(10,100):\n    for k in range(1,25):\n        model_GB = GradientBoostingClassifier(n_estimators=i, max_depth=k, random_state=123)\n        model_GB.fit(train[features], train[target])\n        pred = model_GB.predict(test[features])\n        accuracy = model_GB.score(test[features], test[target])\n        gb[i, k] = round(accuracy*100,2)","56ad873e":"print(f'(n_estimator, max_depth) @ max accuracy: {max(gb , key = gb.get)}')\nprint(f'Max Accuracy: {gb[max(gb , key = gb.get)]}%')","584fa0e8":"ls_a = []\nls_b = []\nfor i in gb:\n    ls_a.append(i[0])\n    ls_b.append(i[1])    ","387ed51f":"\n\n\nplt.figure(figsize=(15,8))\n\nsns.lineplot(x = ls_b, y = list(gb.values()), color = 'darkgreen', \n             lw=2, ci=0.1, estimator=None, label='max_depth')\n\nsns.lineplot(x = ls_a, y = list(gb.values()),\n             color = 'darkblue', lw=2, ci=0.1, estimator=None, label='n_estimator')\n\nplt.axhline(y = 90.79, linestyle='-.',color='red')\nplt.axvline(x = 10, linestyle='-.',color='red')\nplt.axvline(x = 1, linestyle='-.',color='red')\n\n\n\n\nplt.text(x=29, y = 89, s='@ n_estimator=11 & max_depth=1, Max Accuracy = 90.79%', fontsize=18)\nplt.title('Accuracy vs n_estimator & max_depth', fontsize=20)\nplt.xlabel('n_estimator & max_depth')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\n\nplt.show()","9da0eb1b":"gb_lr = {}\nfor k in range(1,21):\n    model_GB = GradientBoostingClassifier(n_estimators=11, max_depth=1, learning_rate= k\/10,\n                                                  random_state=123)\n    model_GB.fit(train[features], train[target])\n    pred = model_GB.predict(test[features])\n    accuracy = model_GB.score(test[features], test[target])\n    gb_lr[k\/10] = round(accuracy*100,2)","13337362":"print(f'learning_rate @ max accuracy: {max(gb_lr , key = gb_lr.get)}')\nprint(f'Max Accuracy: {gb_lr[max(gb_lr , key = gb_lr.get)]}%')","ef6271a1":"plt.figure(figsize=(13,8))\n\nsns.lineplot(x = list(gb_lr.keys()), y = list(gb_lr.values()),\n            color = 'darkgreen', lw=2)\nplt.axhline(y = 90.79, linestyle='-.',color='red', xmax=0.05)\nplt.axvline(x = 0.1, linestyle='-.',color='red', ymax=0.95 )\nplt.text(x=0.3, y = 70, s='@ learning_rate = 0.1, Max Accuracy = 90.79%', fontsize=20)\nplt.title('Accuracy vs Learning_rate', fontsize=20)\nplt.xlabel('Learning_rate')\nplt.ylabel('Accuracy')\n\nplt.show()","e083447e":"knn = {}\nfor k in range(1,100):\n    model_KNN = KNeighborsClassifier(n_neighbors=k)\n    model_KNN.fit(train[features], train[target])\n    pred = model_KNN.predict(test[features])\n    accuracy = model_KNN.score(test[features], test[target])\n    knn[k] = round(accuracy*100,2)","c83d1e79":"print(f'n_neighbor at max accuracy: {max(knn, key = knn.get)}')\nprint(f'Max Accuracy: {knn[18]}%')","b4620850":"plt.figure(figsize=(13,8))\n\nsns.lineplot(x = list(knn.keys()), y = list(knn.values()),\n            color = 'darkgreen', lw=2)\nplt.axhline(y = 88.16, linestyle='-.',color='red', xmax=0.20)\nplt.axvline(x = 18, linestyle='-.',color='red', ymax=0.95 )\nplt.text(x=30, y = 83, s='@ n_neighbors = 18, Accuracy = 88.16%', fontsize=20)\nplt.title('Accuracy vs n_neighbors', fontsize=20)\nplt.xlabel('n_neighbors')\nplt.ylabel('Accuracy')\n\nplt.show()","ac25013a":"lg = {}\nfor i in range(1,200):\n    model_LR = LogisticRegression(max_iter=i)\n    model_LR.fit(train[features], train[target])\n    pred = model_LR.predict(test[features])\n    accuracy = model_LR.score(test[features], test[target])\n    lg[i] = round(accuracy*100,2)","430d1930":"print(f'max_iteration at max accuracy: {max(lg, key = lg.get)}')\nprint(f'Max Accuracy: {lg[max(lg, key = lg.get)]}%')","56682109":"plt.figure(figsize=(13,8))\n\nsns.lineplot(x = list(lg.keys()), y = list(lg.values()),\n            color = 'darkgreen', lw=2)\nplt.axhline(y = 88.16, linestyle='-.',color='red', xmax=0.05)\nplt.axvline(x = 2, linestyle='-.',color='red', ymax=0.95 )\nplt.text(x=50, y = 87.75, s='@ max_iterations = 2, Accuracy = 88.16%', fontsize=20)\nplt.title('Accuracy vs max_iterations', fontsize=20)\nplt.xlabel('max_iterations')\nplt.ylabel('Accuracy')\n\nplt.show()","156a3098":"model_KNN = KNeighborsClassifier(n_neighbors= 18)\nmodel_RF = RandomForestClassifier(n_estimators=26, max_depth=2, random_state=1, criterion='entropy')\nmodel_GB = GradientBoostingClassifier(n_estimators=11, learning_rate=0.1,max_depth=1, random_state=1)\nmodel_LG = LogisticRegression(max_iter=2)\n\n\nmodel_RF.fit(train[features], train[target])\nacc_RF = model_RF.score(test[features], test[target])\n\nmodel_KNN.fit(train[features], train[target])\nacc_KNN = model_KNN.score(test[features], test[target])\n\nmodel_GB.fit(train[features], train[target])\nacc_GB = model_GB.score(test[features], test[target])\n\nmodel_LG.fit(train[features], train[target])\nacc_LG = model_LG.score(test[features], test[target])\n\nind = ['Random Forest', 'Gradient Boost', 'K Nearest Neighbor', 'Logistic Regression']\npd.DataFrame({'Models': ind, 'Accuracy':[round(acc_RF,2), \n                                         round(acc_GB,2), \n                                         round(acc_KNN,2),\n                                        round(acc_LG,2)],\n              'Mean Sqaured Error' : [mean_squared_error(test['output'], model_RF.predict(test[features])),\n                                      mean_squared_error(test['output'], model_GB.predict(test[features])),\n                                      mean_squared_error(test['output'], model_KNN.predict(test[features])),\n                                      mean_squared_error(test['output'], model_LG.predict(test[features]))\n                  \n              ] }).set_index('Models')","ce367aa6":"prediction = model_GB.predict(test[features])","c0703053":"test['Prediction'] = prediction","be10507d":"test[['output', 'Prediction']]","13ee4c88":"from sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(test['output'],test['Prediction'])\ncm","24c71dce":"plt.figure(figsize=(8,6))\nsns.heatmap(cm,annot=True,fmt=\"d\")\n\nplt.show()","13baf67c":"# **MACHINE LEARNING & PREDICTION**","51b64a11":"### **Plotting the parameters for Maximum Accuracy**","5784137b":"#### **If you find this notebook helpful, kindly upvote. If you have any query related to this notebook, do comment below, it will be my pleasure to answer them.**\n### **Thank You**","a8b9e556":"The parameters optimizations performed above is very necessay, as it brings you to choose the optimim values for your variables, further it also helps to minimize the computation time as well in some cases.\n\nSo after performing parameters optimization as shown above, it can be concluded that:\n- K Nearest Neighbors will give its maximum accuracy for this dataset when n_neighbors = 18.\n- Random Forest will give its maximum accuracy for this dataset, when n_estimators=26 & max_depth=2.\n- Gradient Boost will give its maximum accuracy for this dataset, when n_estimators=11, max_depth=1 & learning_rate=0.1.\n- Logistic Regression will give its maximum accuracy for this dataset, when max_iteration=2.","e51ee974":"### **Plotting Accuracy vs max_iterations**","d0d59b8a":"# **Exploratory Data Analysis**","49913c49":"#### **True Values vs Predicted Values**","794f01a1":"**At n_neighbors = 18, we can achieve maximum accuracy=88.16%. Same is also shown in the plot below**","19f45384":"### **Percentage of Heart Attack Chances**","828d479b":"# **Hyperparamter Optimization** ","6c1c7322":"### **Plotting n_neighbors for Max Accuracy**","acb33707":"### **Gender Distribution in DataSet**","86a98758":"As we can see our maximum dataset ranges of age between 50 - 60.","49482adc":"### **Plotting the parameters for Maximum Accuracy**","ff9453c9":"**Shuffling the DataSet before splitting**","13fdd7a0":"### **CountPlot of Categorical Features**","82163ad0":"**It can be seen that with n_estimator=26 and max_depth=2, we will have maximum accuracy=89.47%.**","94bf04f3":"**Determining the Correlation of Features wrt to the Target Feature**","a387d10c":"## **Models Comparison after Tuning**","057fcb97":"### **Histplot of Continuous Features**","69226b2d":"### **Data Preparation**","48e51ccb":"**At maximum iterations=2, we will have maximum accuracy=88.16% for Logistic Regression**","7d0ddcb2":"**We've to take the features having correlation with output > 0.25**","6e362fe3":"### **Pairplot of Continuous Features**","6f92ce84":"### **Random Forest Classifier**","25765d8b":"As we can see above that there are 54.3% of dataset which has high chance of Heart Attack where 45.7% are with Low Chance.","38cd9c36":"### **Comparing Models with their Default Parameters**","2c3ee06d":"**At learning_rate = 0.1, we will have maximum accuracy=90.79%.**","173d5c1d":"#### *For learninig_rate*","020a5226":"#### **Plotting Correlation Heatmap wrt to Output (Target Label)**","c1af1be0":"**It can be seen that with n_estimator=11 and max_depth=1, we will have maximum accuracy=90.79%.**","3e9f523d":"## **CONTINUOUS FEATURES**","5759ac95":"**Splitting 75% of the data into training set and 25% into test set**","ab696c55":"**Converting the data to similar typecast**","7f003f7f":"### **KNearest Neighbors Classifier**","a28171fc":"#### *For n_estimators and max_depth*","58c40712":"**We cannot perform normalization to our target variable so replacing with the original one**","d220eae8":"## **CATEGORICAL FEATURES**","af40cf8b":"### **Checking Null Values**","aac3f096":"#### Plotting Correlation Heatmap to find the correlation among columns","3e190c73":"- Here we can see there is no significant relation of Age with Heart Attack Risk.\n- One point can be seen here is that there are more number of people between age 55-60 who has Low Risk of Heart Attack.","094848d8":"### **Gender Distribution wrt Heart Attack**","6f453587":"### **Checking Duplicates and Removing**","fc8a8b20":"We have 68.21% Male and 31.79% Female in our DataSet. (Assuming, 1=Male and 0=Female)","0f57b221":"Plotting pairplots for Continuous features to see any significant correlation among the features.","1abfdf60":"**Estimating Optimum values for n_estimator and max_depth**","005f3497":"### **Age Distribution in our DataSet**","8dfb565a":"### **Gradient Boost Classifier**","6510ac02":"### **Logistic Regression**","8a9fa3b5":"### ****Getting Information from the Data****","9377cb22":"### **As we can see that our Gradient Boost works best on our data set showing Maximum Accuracy of 91% and Mean Squared Error = 0.092**","0d69db18":"### **Plotting No of People vs Age with Low or High Risk of Heart Attack**","acd3393c":"### ****Loading and Understanding the Data****","08850469":"### **Importing Necessary Libraries for ML**","3487773a":"**Performing Standardization (Normalizing the data)**"}}