{"cell_type":{"78c76a6f":"code","e867b73f":"code","5a5e0d80":"code","57f1dc10":"code","d5da7339":"code","ab17afe4":"code","0bac1293":"code","87483bbe":"code","0cb35c11":"code","490a4720":"code","cfd92396":"code","15009b8c":"code","1d4026f8":"code","37a82518":"code","1c3eacd7":"code","b70c9b04":"code","113b0e27":"code","ef703d98":"code","8bc55499":"code","5f7cb4ab":"code","6d3ac913":"code","0b9ddc0c":"code","64e127fb":"code","06b48e92":"code","1800929a":"code","67517c38":"code","1ec4ef89":"code","ed291ce2":"code","a088b90c":"code","47e1380d":"code","147390ae":"code","34d75ace":"code","f8eb22a9":"code","0ded586d":"code","578eba19":"code","cb51a8b5":"code","9ae2856d":"code","2cdc5793":"code","86c7b75e":"code","431b8e0d":"code","453baa52":"code","8ea804ad":"code","3e1514fc":"code","00bcd69a":"code","23ac4d0e":"code","8fe86913":"code","38fa832b":"code","923ca866":"code","36e20639":"code","66dbb16f":"code","b025df8f":"code","76e32600":"code","505d3fb3":"code","1381ecc1":"code","14bf0a1c":"code","bc756950":"code","1bc8359a":"code","871748a2":"code","07b85d57":"code","baad2d12":"code","41d94faf":"code","950e3a46":"code","b5f91067":"code","b5d4fb40":"code","f36cac65":"markdown","9677334e":"markdown","455dbbfa":"markdown","e869de9a":"markdown","df7014e9":"markdown","662365ca":"markdown","d0d0a258":"markdown","39712d05":"markdown","990b8228":"markdown","a4000933":"markdown","3a6d1e23":"markdown","ef739a9f":"markdown","9af65c4b":"markdown","6b4686db":"markdown","8fd0827a":"markdown"},"source":{"78c76a6f":"!pip install pyod","e867b73f":"import numpy as np\nimport pandas as pd\n\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.cluster import KMeans\nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import KFold, train_test_split\n\nimport optuna\nfrom optuna.samplers import TPESampler\n\nfrom pyod.models.copod import COPOD\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.utils import to_categorical\n\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier","5a5e0d80":"train = pd.read_csv('\/kaggle\/input\/health-insurance-cross-sell-prediction\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/health-insurance-cross-sell-prediction\/test.csv')","57f1dc10":"train","d5da7339":"train = train.drop(['id'], axis=1)\ntest = test.drop(['id'], axis=1)","ab17afe4":"fig = make_subplots(rows=1, cols=2)\n\ntraces = [\n    go.Bar(\n        x=['Male', 'Female'], \n        y=[\n            len(train[train['Gender']=='Male']),\n            len(train[train['Gender']=='Female'])\n        ], \n        name='Train Gender',\n        text = [\n            str(round(100 * len(train[train['Gender']=='Male']) \/ len(train), 2)) + '%',\n            str(round(100 * len(train[train['Gender']=='Female']) \/ len(train), 2)) + '%'\n        ],\n        textposition='auto'\n    ),\n    go.Bar(\n        x=['Male', 'Female'], \n        y=[\n            len(test[test['Gender']=='Male']),\n            len(test[test['Gender']=='Female'])\n        ], \n        name='Test Gender',\n        text=[\n            str(round(100 * len(test[test['Gender']=='Male']) \/ len(test), 2)) + '%',\n            str(round(100 * len(test[test['Gender']=='Female']) \/ len(test), 2)) + '%'\n        ],\n        textposition='auto'\n    ),\n\n]\n\nfor i in range(len(traces)):\n    fig.append_trace(\n        traces[i], \n        (i \/\/ 2) + 1, \n        (i % 2)  +1\n    )\n\nfig.update_layout(\n    title_text='Train\/test gender column',\n    height=400,\n    width=700\n)\n\nfig.show()","0bac1293":"fig = make_subplots(rows=1, cols=2)\n\ntraces = [\n    go.Bar(\n        x=['Yes', 'No'], \n        y=[\n            len(train[train['Driving_License']==1]),\n            len(train[train['Driving_License']==0])\n        ], \n        name='Train Driving_License',\n        text = [\n            str(round(100 * len(train[train['Driving_License']==1]) \/ len(train), 2)) + '%',\n            str(round(100 * len(train[train['Driving_License']==0]) \/ len(train), 2)) + '%'\n        ],\n        textposition='auto'\n    ),\n    go.Bar(\n        x=['Yes', 'No'], \n        y=[\n            len(test[test['Driving_License']==1]),\n            len(test[test['Driving_License']==0])\n        ], \n        name='Test Driving_License',\n        text=[\n            str(round(100 * len(test[test['Driving_License']==1]) \/ len(test), 2)) + '%',\n            str(round(100 * len(test[test['Driving_License']==0]) \/ len(test), 2)) + '%'\n        ],\n        textposition='auto'\n    ),\n\n]\n\nfor i in range(len(traces)):\n    fig.append_trace(\n        traces[i], \n        (i \/\/ 2) + 1, \n        (i % 2)  + 1\n    )\n\nfig.update_layout(\n    title_text='Train\/test Driving_License column',\n    height=400,\n    width=700\n)\n\nfig.show()","87483bbe":"fig = make_subplots(rows=1, cols=2)\n\ntraces = [\n    go.Bar(\n        x=['Yes', 'No'], \n        y=[\n            len(train[train['Previously_Insured']==1]),\n            len(train[train['Previously_Insured']==0])\n        ], \n        name='Train Previously_Insured',\n        text = [\n            str(round(100 * len(train[train['Previously_Insured']==1]) \/ len(train), 2)) + '%',\n            str(round(100 * len(train[train['Previously_Insured']==0]) \/ len(train), 2)) + '%'\n        ],\n        textposition='auto'\n    ),\n    go.Bar(\n        x=['Yes', 'No'], \n        y=[\n            len(test[test['Previously_Insured']==1]),\n            len(test[test['Previously_Insured']==0])\n        ], \n        name='Test Previously_Insured',\n        text = [\n            str(round(100 * len(test[test['Previously_Insured']==1]) \/ len(test), 2)) + '%',\n            str(round(100 * len(test[test['Previously_Insured']==0]) \/ len(test), 2)) + '%'\n        ],\n        textposition='auto'\n    ),\n\n]\n\nfor i in range(len(traces)):\n    fig.append_trace(\n        traces[i], \n        (i \/\/ 2) + 1, \n        (i % 2)  + 1\n    )\n\nfig.update_layout(\n    title_text='Train\/test Previously_Insured column',\n    height=400,\n    width=700\n)\n\nfig.show()","0cb35c11":"fig = make_subplots(rows=1, cols=2)\n\ntraces = [\n    go.Bar(\n        x=['Yes', 'No'], \n        y=[\n            len(train[train['Vehicle_Damage']=='Yes']),\n            len(train[train['Vehicle_Damage']=='No'])\n        ], \n        name='Train Vehicle_Damage',\n        text = [\n            str(round(100 * len(train[train['Vehicle_Damage']=='Yes']) \/ len(train), 2)) + '%',\n            str(round(100 * len(train[train['Vehicle_Damage']=='No']) \/ len(train), 2)) + '%'\n        ],\n        textposition='auto'\n    ),\n    go.Bar(\n        x=['Yes', 'No'], \n        y=[\n            len(test[test['Vehicle_Damage']=='Yes']),\n            len(test[test['Vehicle_Damage']=='No'])\n        ], \n        name='Test Vehicle_Damage',\n        text = [\n            str(round(100 * len(test[test['Vehicle_Damage']=='Yes']) \/ len(test), 2)) + '%',\n            str(round(100 * len(test[test['Vehicle_Damage']=='No']) \/ len(test), 2)) + '%'\n        ],\n        textposition='auto'\n    ),\n\n]\n\nfor i in range(len(traces)):\n    fig.append_trace(\n        traces[i], \n        (i \/\/ 2) + 1, \n        (i % 2)  + 1\n    )\n\nfig.update_layout(\n    title_text='Train\/test Vehicle_Damage column',\n    height=400,\n    width=700\n)\n\nfig.show()","490a4720":"fig = make_subplots(rows=1, cols=2)\n\ntraces = [\n    go.Bar(\n        x=['> 2 Years', '1-2 Year', '< 1 Year'], \n        y=[\n            len(train[train['Vehicle_Age']=='> 2 Years']),\n            len(train[train['Vehicle_Age']=='1-2 Year']),\n            len(train[train['Vehicle_Age']=='< 1 Year'])\n        ], \n        name='Train Vehicle_Age',\n        text = [\n            str(round(100 * len(train[train['Vehicle_Age']=='> 2 Years']) \/ len(train), 2)) + '%',\n            str(round(100 * len(train[train['Vehicle_Age']=='1-2 Year']) \/ len(train), 2)) + '%',\n            str(round(100 * len(train[train['Vehicle_Age']=='< 1 Year']) \/ len(train), 2)) + '%'\n        ],\n        textposition='auto'\n    ),\n    go.Bar(\n        x=['> 2 Years', '1-2 Year', '< 1 Year'], \n        y=[\n            len(test[test['Vehicle_Age']=='> 2 Years']),\n            len(test[test['Vehicle_Age']=='1-2 Year']),\n            len(test[test['Vehicle_Age']=='< 1 Year'])\n        ], \n        name='Test Vehicle_Age',\n        text = [\n            str(round(100 * len(test[test['Vehicle_Age']=='> 2 Years']) \/ len(test), 2)) + '%',\n            str(round(100 * len(test[test['Vehicle_Age']=='1-2 Year']) \/ len(test), 2)) + '%',\n            str(round(100 * len(test[test['Vehicle_Age']=='< 1 Year']) \/ len(test), 2)) + '%'\n        ],\n        textposition='auto'\n    ),\n\n]\n\nfor i in range(len(traces)):\n    fig.append_trace(\n        traces[i], \n        (i \/\/ 2) + 1, \n        (i % 2)  + 1\n    )\n\nfig.update_layout(\n    title_text='Train\/test Vehicle_Age column',\n    height=400,\n    width=700\n)\n\nfig.show()","cfd92396":"fig = make_subplots(rows=1, cols=2)\n\ntraces = [\n    go.Histogram(\n        x=train['Age'], \n        name='Train Age'\n    ),\n    go.Histogram(\n        x=test['Age'], \n        name='Test Age'\n    ),\n\n]\n\nfor i in range(len(traces)):\n    fig.append_trace(\n        traces[i], \n        (i \/\/ 2) + 1, \n        (i % 2)  + 1\n    )\n\nfig.update_layout(\n    title_text='Train\/test Age column distribution',\n    height=500,\n    width=900\n)\n\nfig.show()","15009b8c":"fig = make_subplots(rows=1, cols=2)\n\ntraces = [\n    go.Histogram(\n        x=train['Annual_Premium'], \n        name='Train Annual_Premium'\n    ),\n    go.Histogram(\n        x=test['Annual_Premium'], \n        name='Test Annual_Premium'\n    ),\n\n]\n\nfor i in range(len(traces)):\n    fig.append_trace(\n        traces[i], \n        (i \/\/ 2) + 1, \n        (i % 2)  + 1\n    )\n\nfig.update_layout(\n    title_text='Train\/test Annual_Premium column distribution',\n    height=500,\n    width=800\n)\n\nfig.show()","1d4026f8":"fig = make_subplots(rows=1, cols=2)\n\ntraces = [\n    go.Histogram(\n        x=train['Policy_Sales_Channel'], \n        name='Train Policy_Sales_Channel'\n    ),\n    go.Histogram(\n        x=test['Policy_Sales_Channel'], \n        name='Test Policy_Sales_Channel'\n    ),\n\n]\n\nfor i in range(len(traces)):\n    fig.append_trace(\n        traces[i], \n        (i \/\/ 2) + 1, \n        (i % 2)  +1\n    )\n\nfig.update_layout(\n    title_text='Train\/test Policy_Sales_Channel column distribution',\n    height=500,\n    width=800\n)\n\nfig.show()","37a82518":"fig = make_subplots(rows=1, cols=2)\n\ntraces = [\n    go.Histogram(\n        x=train['Vintage'], \n        name='Train Vintage'\n    ),\n    go.Histogram(\n        x=test['Vintage'], \n        name='Test Vintage'\n    ),\n\n]\n\nfor i in range(len(traces)):\n    fig.append_trace(\n        traces[i], \n        (i \/\/ 2) + 1, \n        (i % 2)  + 1\n    )\n\nfig.update_layout(\n    title_text='Train\/test Vintage column distribution',\n    height=500,\n    width=800\n)\n\nfig.show()","1c3eacd7":"tr = train['Region_Code'].value_counts().reset_index()\nx_tr = tr['index'].tolist()\ny_tr = tr['Region_Code'].tolist()\nte = test['Region_Code'].value_counts().reset_index()\nx_te = te['index'].tolist()\ny_te = te['Region_Code'].tolist()\n\nfig = make_subplots(rows=2, cols=1)\n\ntraces = [\n    go.Bar(\n        x=x_tr, \n        y=y_tr, \n        name='Train Region_Code'\n    ),\n    go.Bar(\n        x=x_te, \n        y=y_te, \n        name='Test Region_Code'\n    )\n]\n\nfor i in range(len(traces)):\n    fig.append_trace(\n        traces[i], \n        (i \/\/ 1) + 1, \n        (i % 1)  + 1\n    )\n\nfig.update_layout(\n    title_text='Train \/ test Region_Code',\n    height=900,\n    width=800\n)\n\nfig.show()","b70c9b04":"fig = make_subplots(rows=1, cols=1)\n\ntraces = [\n    go.Bar(\n        x=['Yes', 'No'], \n        y=[\n            len(train[train['Response']==1]),\n            len(train[train['Response']==0])\n        ], \n        name='Train Response'\n    ),\n]\n\nfor i in range(len(traces)):\n    fig.append_trace(\n        traces[i], \n        (i \/\/ 2) + 1, \n        (i % 2)  + 1\n    )\n\nfig.update_layout(\n    title_text='Train Response column',\n    height=400,\n    width=400\n)\n\nfig.show()","113b0e27":"fig = px.histogram(\n    train, \n    \"Age\", \n    color='Response',\n    nbins=100, \n    title='Age & Response ditribution', \n    width=700,\n    height=500\n)\n\nfig.show()","ef703d98":"fig = px.histogram(\n    train[train['Response'] == 1], \n    \"Age\", \n    nbins=100, \n    title='Age distribution for positive response', \n    width=700,\n    height=500\n)\n\nfig.show()","8bc55499":"fig = make_subplots(\n    rows=1, \n    cols=2\n)\n\ntraces = [\n    go.Bar(\n        x=['Declined', 'Accepted'], \n        y=[\n            len(train[(train['Gender']=='Male') & (train['Response']==0)]),\n            len(train[(train['Gender']=='Male') & (train['Response']==1)])\n        ], \n        name='Gender: Male'\n    ),\n    go.Bar(\n        x=['Declined', 'Accepted'],  \n        y=[\n            len(train[(train['Gender']=='Female') & (train['Response']==0)]),\n            len(train[(train['Gender']=='Female') & (train['Response']==1)])\n        ], \n        name='Gender: Female'\n    ),\n\n]\n\nfor i in range(len(traces)):\n    fig.append_trace(\n        traces[i], \n        (i \/\/ 2) + 1, \n        (i % 2)  + 1\n    )\n\nfig.update_layout(\n    title_text='Train gender\/response dependencies',\n    height=400,\n    width=700\n)\n\nfig.show()","5f7cb4ab":"fig = make_subplots(\n    rows=1, \n    cols=2\n)\n\ntraces = [\n    go.Bar(\n        x=['Declined', 'Accepted'], \n        y=[\n            len(train[(train['Previously_Insured']==0) & (train['Response']==0)]),\n            len(train[(train['Previously_Insured']==0) & (train['Response']==1)])\n        ], \n        name='Previously_Insured: Previously Not Insured'\n    ),\n    go.Bar(\n        x=['Declined', 'Accepted'],  \n        y=[\n            len(train[(train['Previously_Insured']==1) & (train['Response']==0)]),\n            len(train[(train['Previously_Insured']==1) & (train['Response']==1)])\n        ], \n        name='Previously_Insured: Previously Insured'\n    ),\n\n]\n\nfor i in range(len(traces)):\n    fig.append_trace(\n        traces[i], \n        (i \/\/ 2) + 1, \n        (i % 2)  + 1\n    )\n\nfig.update_layout(\n    title_text='Train previously_insured\/response dependencies',\n    height=400,\n    width=700\n)\n\nfig.show()","6d3ac913":"fig = make_subplots(\n    rows=1, \n    cols=2\n)\n\ntraces = [\n    go.Bar(\n        x=['Declined', 'Accepted'], \n        y=[\n            len(train[(train['Vehicle_Damage']=='No') & (train['Response']==0)]),\n            len(train[(train['Vehicle_Damage']=='No') & (train['Response']==1)])\n        ], \n        name='Vehicle_Damage: No'\n    ),\n    go.Bar(\n        x=['Declined', 'Accepted'],  \n        y=[\n            len(train[(train['Vehicle_Damage']=='Yes') & (train['Response']==0)]),\n            len(train[(train['Vehicle_Damage']=='Yes') & (train['Response']==1)])\n        ], \n        name='Vehicle_Damage: Yes'\n    ),\n\n]\n\nfor i in range(len(traces)):\n    fig.append_trace(\n        traces[i], \n        (i \/\/ 2) + 1, \n        (i % 2)  + 1\n    )\n\nfig.update_layout(\n    title_text='Train vehicle_damage\/response dependencies',\n    height=400,\n    width=700\n)\n\nfig.show()","0b9ddc0c":"fig = make_subplots(\n    rows=1, \n    cols=3\n)\n\ntraces = [\n    go.Bar(\n        x=['Declined', 'Accepted'], \n        y=[\n            len(train[(train['Vehicle_Age']=='> 2 Years') & (train['Response']==0)]),\n            len(train[(train['Vehicle_Age']=='> 2 Years') & (train['Response']==1)])\n        ], \n        name='Vehicle_Age: > 2 Years'\n    ),\n    go.Bar(\n        x=['Declined', 'Accepted'], \n        y=[\n            len(train[(train['Vehicle_Age']=='1-2 Year') & (train['Response']==0)]),\n            len(train[(train['Vehicle_Age']=='1-2 Year') & (train['Response']==1)])\n        ], \n        name='Vehicle_Age: 1-2 Year'\n    ),\n    go.Bar(\n        x=['Declined', 'Accepted'], \n        y=[\n            len(train[(train['Vehicle_Age']=='< 1 Year') & (train['Response']==0)]),\n            len(train[(train['Vehicle_Age']=='< 1 Year') & (train['Response']==1)])\n        ], \n        name='Vehicle_Age: < 1 Year'\n    ),\n\n]\n\nfor i in range(len(traces)):\n    fig.append_trace(traces[i], (i \/\/ 3) + 1, (i % 3)  +1)\n\nfig.update_layout(\n    title_text='Train\/test Vehicle_Age\/Response dependencies',\n    height=400,\n    width=800\n)\n\nfig.show()","64e127fb":"fig = px.histogram(\n    train, \n    \"Annual_Premium\", \n    color='Response',\n    nbins=100, \n    title='Annual_Premium & Response ditribution', \n    width=700,\n    height=500\n)\nfig.show()","06b48e92":"fig = px.histogram(\n    train[train['Response'] == 1], \n    \"Annual_Premium\", \n    nbins=100, \n    title='Annual_Premium distribution for positive response', \n    width=700,\n    height=500\n)\n\nfig.show()","1800929a":"fig = px.histogram(\n    train, \n    \"Vintage\", \n    color='Response',\n    nbins=100, \n    title='Vintage & Response ditribution', \n    width=700,\n    height=500\n)\n\nfig.show()","67517c38":"fig = px.histogram(\n    train[train['Response'] == 1], \n    \"Vintage\", \n    nbins=100, \n    title='Vintage distribution for positive response', \n    width=700,\n    height=500\n)\nfig.show()","1ec4ef89":"train.loc[train['Gender'] == 'Male', 'Gender'] = 1\ntrain.loc[train['Gender'] == 'Female', 'Gender'] = 0\ntest.loc[test['Gender'] == 'Male', 'Gender'] = 1\ntest.loc[test['Gender'] == 'Female', 'Gender'] = 0\n\ntrain.loc[train['Vehicle_Age'] == '> 2 Years', 'Vehicle_Age'] = 2\ntrain.loc[train['Vehicle_Age'] == '1-2 Year', 'Vehicle_Age'] = 1\ntrain.loc[train['Vehicle_Age'] == '< 1 Year', 'Vehicle_Age'] = 0\ntest.loc[test['Vehicle_Age'] == '> 2 Years', 'Vehicle_Age'] = 2\ntest.loc[test['Vehicle_Age'] == '1-2 Year', 'Vehicle_Age'] = 1\ntest.loc[test['Vehicle_Age'] == '< 1 Year', 'Vehicle_Age'] = 0\n\ntrain.loc[train['Vehicle_Damage'] == 'Yes', 'Vehicle_Damage'] = 1\ntrain.loc[train['Vehicle_Damage'] == 'No', 'Vehicle_Damage'] = 0\ntest.loc[test['Vehicle_Damage'] == 'Yes', 'Vehicle_Damage'] = 1\ntest.loc[test['Vehicle_Damage'] == 'No', 'Vehicle_Damage'] = 0","ed291ce2":"for col in train.columns:\n    train[col] = train[col].astype(np.int32)\n\ntrain","a088b90c":"f = plt.figure(\n    figsize=(13, 11)\n)\n\nplt.matshow(\n    train.corr(), \n    fignum=f.number\n)\n\nplt.xticks(\n    range(train.shape[1]), \n    train.columns, \n    fontsize=14, \n    rotation=75\n)\n\nplt.yticks(\n    range(train.shape[1]), \n    train.columns, \n    fontsize=14\n)\n\ncb = plt.colorbar()\n\ncb.ax.tick_params(\n    labelsize=14\n)","47e1380d":"for col in train.columns:\n    if col == 'Response':\n        continue\n    print(col, train[col].corr(train['Response']))","147390ae":"fig = px.scatter(\n    train, \n    x=\"Annual_Premium\", \n    y=\"Age\", \n    color=\"Response\",\n    width=600,\n    height=600,\n    title='Annual_premium vs Age scatter'\n)\n\nfig.show()","34d75ace":"X = train.drop(['Response'], axis=1)\ny = train['Response']","f8eb22a9":"kmeans = KMeans(\n    n_clusters=2, \n    random_state=666\n).fit(X)","0ded586d":"train['cluster'] = kmeans.labels_\ntrain","578eba19":"train['cluster'].value_counts()","cb51a8b5":"print('Kmeans accuracy: ', accuracy_score(train['Response'], train['cluster']))\nprint('Kmeans f1_score: ', f1_score(train['Response'], train['cluster']))","9ae2856d":"response = train['Response']\ntrain = train.drop(['Response', 'cluster'], axis=1)","2cdc5793":"clf = COPOD(\n    contamination=0.15\n)\nclf.fit(train)","86c7b75e":"cluster = clf.predict(train)\ntrain['cluster'] = cluster\ntrain['Response'] = response\ntrain","431b8e0d":"train['cluster'].value_counts()","453baa52":"print('COPOD accuracy: ', accuracy_score(train['Response'], train['cluster']))\nprint('COPOD f1_score: ', f1_score(train['Response'], train['cluster']))","8ea804ad":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=666)","3e1514fc":"print('Positive cases % in validation set: ', round(100 * len(y_test[y_test == 1]) \/ len(y_test), 3), '%')\nprint('Positive cases % in train set: ', round(100 * len(y_train[y_train == 1]) \/ len(y_train), 3), '%')","00bcd69a":"model = LogisticRegression(random_state=666)\nmodel.fit(X_train, y_train)","23ac4d0e":"preds = model.predict(X_test)\nprint('Simple Logistic Regression accuracy: ', accuracy_score(y_test, preds))\nprint('Simple Logistic Regression f1_score: ', f1_score(y_test, preds))","8fe86913":"def plot_confusion_matrix(y_real, y_pred):\n    cm = confusion_matrix(y_real, y_pred)\n\n    ax= plt.subplot()\n    sns.heatmap(cm, annot=True, ax = ax, fmt='g')\n\n    ax.set_xlabel('Predicted labels')\n    ax.set_ylabel('True labels')","38fa832b":"plot_confusion_matrix(y_test, preds)","923ca866":"X_train = X_train.drop(['Region_Code', 'Vintage', 'Driving_License'], axis=1)\nX_test = X_test.drop(['Region_Code', 'Vintage', 'Driving_License'], axis=1)","36e20639":"model = LogisticRegression(random_state=666)\nmodel.fit(X_train, y_train)","66dbb16f":"preds = model.predict(X_test)\nprint('Simple Logistic Regression accuracy: ', accuracy_score(y_test, preds))\nprint('Simple Logistic Regression f1_score: ', f1_score(y_test, preds))","b025df8f":"plot_confusion_matrix(y_test, preds)","76e32600":"model = LGBMClassifier(random_state=666)\nmodel.fit(X_train, y_train)\n\npreds = model.predict(X_test)\nprint('Simple LGBM accuracy: ', accuracy_score(y_test, preds))\nprint('Simple LGBM Regression f1_score: ', f1_score(y_test, preds))","505d3fb3":"np.random.seed(666)\nsampler = TPESampler(seed=0)\n\ndef create_model(trial):\n    max_depth = trial.suggest_int(\"max_depth\", 2, 20)\n    n_estimators = trial.suggest_int(\"n_estimators\", 1, 400)\n    learning_rate = trial.suggest_uniform('learning_rate', 0.0000001, 1)\n    gamma = trial.suggest_uniform('gamma', 0.0000001, 1)\n    scale_pos_weight = trial.suggest_int(\"scale_pos_weight\", 1, 20)\n    model = XGBClassifier(\n        learning_rate=learning_rate, \n        n_estimators=n_estimators, \n        max_depth=max_depth, \n        gamma=gamma, \n        scale_pos_weight=scale_pos_weight, \n        random_state=0\n    )\n    return model\n\ndef objective(trial):\n    model = create_model(trial)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    score = f1_score(y_test, preds)\n    return score\n\n#study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n#study.optimize(objective, n_trials=500)\n\n#xgb_params = study.best_params\nxgb_params = {\n    'max_depth': 4, \n    'n_estimators': 372, \n    'learning_rate': 0.09345905554110154, \n    'gamma': 0.6641238000625036, \n    'scale_pos_weight': 4\n}\nxgb_params['random_state'] = 0\nxgb = XGBClassifier(**xgb_params)\nxgb.fit(X_train, y_train)\npreds = xgb.predict(X_test)\nprint('Optimized XGBClassifier accuracy: ', accuracy_score(y_test, preds))\nprint('Optimized XGBClassifier f1-score', f1_score(y_test, preds))","1381ecc1":"plot_confusion_matrix(y_test, preds)","14bf0a1c":"def create_model(trial):\n    max_depth = trial.suggest_int(\"max_depth\", 2, 7)\n    n_estimators = trial.suggest_int(\"n_estimators\", 2, 200)\n    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n    model = RandomForestClassifier(\n        min_samples_leaf=min_samples_leaf, \n        n_estimators=n_estimators, \n        max_depth=max_depth, \n        random_state=0\n    )\n    return model\n\ndef objective(trial):\n    model = create_model(trial)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    score = f1_score(y_test, preds)\n    return score\n\nstudy = optuna.create_study(direction=\"maximize\", sampler=sampler)\nstudy.optimize(objective, n_trials=100)\nrf_params = study.best_params\nrf = RandomForestClassifier(**rf_params)\nrf.fit(X_train, y_train)\npreds = rf.predict(X_test)\nprint('Optimized RF accuracy: ', accuracy_score(y_test, preds))\nprint('Optimized RF f1-score:', f1_score(y_test, preds))","bc756950":"plot_confusion_matrix(y_test, preds)","1bc8359a":"def recall_score(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    return recall\n\ndef precision_score(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    return precision\n\ndef keras_f1_score(y_true, y_pred):\n    precision = precision_score(y_true, y_pred)\n    recall = recall_score(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","871748a2":"def create_model():\n    model = tf.keras.Sequential([\n        tf.keras.layers.Input(7),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(30, activation=\"relu\"),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.3),\n        tf.keras.layers.Dense(2, activation='softmax')\n    ])\n    model.compile(\n        loss=tf.keras.losses.binary_crossentropy, \n        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n        metrics=[keras_f1_score]\n    )\n    return model","07b85d57":"y_nn_train = to_categorical(y_train)","baad2d12":"class_weight = {\n    0: 1.,\n    1: 8.\n}","41d94faf":"model = create_model()\nmodel.fit(X_train, y_nn_train, validation_split=0.2, epochs=35, batch_size=256, verbose=2, class_weight=class_weight)","950e3a46":"preds = model.predict(X_test)\npreds = np.argmax(preds, axis=1)","b5f91067":"print('NN accuracy: ', accuracy_score(y_test, preds))\nprint('NN f1-score', f1_score(y_test, preds))","b5d4fb40":"plot_confusion_matrix(y_test, preds)","f36cac65":"#### Let's build LightGBM with default parameters","9677334e":"\n<h1><center>Insurance Prediction. Data analysis and modeling.<\/center><\/h1>\n\n<center><img src=\"https:\/\/www.outlookindia.com\/outlookmoney\/public\/uploads\/article\/gallery\/9f5518fc9b70672aaba65aa3af600c32.jpg\"><\/center>\n","455dbbfa":"<a id=\"2\"><\/a>\n<h2 style='background:blue; border:0; color:white'><center>2. Feature Engineering<\/center><h2>","e869de9a":"Let's build our first version of classifier and use Logistic Regression","df7014e9":"#### After removing some columns predictions become better but still not good.","662365ca":"<a id=\"3\"><\/a>\n<h2 style='background:blue; border:0; color:white'><center>3. Modeling<\/center><h2>","d0d0a258":"Correlation for every feature with target","39712d05":"Now let's try to use COPOD anomaly detection model and check results","990b8228":"Let's try unsupervised learning first. We will us kmeans clustering algorithm to check scores.","a4000933":"1) Convert columns with text values","3a6d1e23":"As we can see from initial analysis all columns presented in dataset have exactly the same ditribution. Let's do feature engineering and modeling next.","ef739a9f":"So we can see that our sets are well balanced by target column and we can use our validation set for testing.","9af65c4b":"Now we will create validation set","6b4686db":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:blue; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Quick navigation<\/center><\/h2>\n\n* [1. Basic Data Analysis](#1)\n* [2. Feature engineering (In progress)](#2)\n* [3. Modeling (In progress)](#3)","8fd0827a":"<a id=\"1\"><\/a>\n<h2 style='background:blue; border:0; color:white'><center>1. Basic Data Analysis<\/center><h2>"}}