{"cell_type":{"e4ef47fc":"code","2ea2addf":"code","53a63a24":"code","e7f572ee":"code","8d573cbf":"code","241887c8":"code","8f3be30a":"code","5dd67d6e":"code","34b1efde":"code","d330b2b7":"code","71e3da4b":"code","60bc6a6b":"code","19731c7e":"code","ab3e23fa":"code","d994e69c":"code","4624e6b5":"code","ee6c0466":"code","65202905":"code","ddf440d0":"code","5e256a6b":"code","c3c6891e":"code","b44e1969":"code","97cdd2e2":"code","420128a1":"code","649321f1":"code","291317cd":"code","7b0790a3":"code","840fec64":"code","899ca9f2":"code","c12243f6":"code","1886a555":"code","244c09a1":"code","bc3d0faa":"code","201fafe7":"code","ab294dad":"code","9a626cde":"code","73a247b1":"code","90278076":"code","24acf8d3":"code","9e310634":"code","5c0b54f9":"code","4af0e9ec":"code","7ffc98e9":"code","d3464884":"code","4546c7ed":"markdown","2d374f08":"markdown","65cb6d27":"markdown","d5ce5a08":"markdown","13b9802d":"markdown","ec8c322c":"markdown","13249a61":"markdown","467d3282":"markdown","12c402e6":"markdown","d6b870a9":"markdown","66cbcfe2":"markdown","16ea6ad5":"markdown","e3ab011e":"markdown","31f21dee":"markdown","94727116":"markdown","13415e5e":"markdown","15bc6df4":"markdown","a3d1f6ed":"markdown","860cf4fe":"markdown","8fd74912":"markdown","7c50b700":"markdown","a04308a2":"markdown","ec211f65":"markdown","deb0cbcc":"markdown","b073380e":"markdown","081e321e":"markdown","3fec01dd":"markdown","47b24b45":"markdown","19537845":"markdown","68045313":"markdown","41313f49":"markdown","d6c9c282":"markdown","f56a92fd":"markdown"},"source":{"e4ef47fc":"import numpy as np \nimport pandas as pd\nimport tensorflow as tf\nimport os\nimport matplotlib.pyplot as plt\nfrom PIL import Image, ImageDraw, ImageEnhance\nimport albumentations as albu\nfrom tqdm.notebook import tqdm","2ea2addf":"labels = pd.read_csv('..\/input\/global-wheat-detection\/train.csv')\nlabels.head()","53a63a24":"def group_boxes(group):\n    boundaries = group['bbox'].str.split(',', expand=True)\n    boundaries[0] = boundaries[0].str.slice(start=1)\n    boundaries[3] = boundaries[3].str.slice(stop=-1)\n    \n    return boundaries.values.astype(float)\n\nlabels = labels.groupby('image_id').apply(group_boxes)","e7f572ee":"labels['b6ab77fd7'][0:5]","8d573cbf":"train_image_ids = np.unique(labels.index.values)[0:3363]\nval_image_ids = np.unique(labels.index.values)[3363:3373]","241887c8":"def load_image(image_id):\n    image = Image.open('..\/input\/global-wheat-detection\/train\/' + image_id + \".jpg\")\n    image = image.resize((256, 256))\n    \n    return np.asarray(image)","8f3be30a":"train_pixels = {}\ntrain_labels = {}\n\nfor image_id in tqdm(train_image_ids):\n    train_pixels[image_id] = load_image(image_id)\n    train_labels[image_id] = labels[image_id].copy() \/ 4","5dd67d6e":"val_pixels = {}\nval_labels = {}\n\nfor image_id in tqdm(val_image_ids):\n    val_pixels[image_id] = load_image(image_id)    \n    val_labels[image_id] = labels[image_id].copy() \/ 4","34b1efde":"def draw_bboxes(image_id, bboxes, source='train'):  \n    image = Image.open('..\/input\/global-wheat-detection\/' + source +'\/' + image_id + \".jpg\")\n    image = image.resize((256,256))\n    \n    draw = ImageDraw.Draw(image)\n            \n    for bbox in bboxes:\n        draw_bbox(draw, bbox)\n    \n    return np.asarray(image)\n\n\ndef draw_bbox(draw, bbox):\n    x, y, width, height = bbox\n    draw.rectangle([x, y, x + width, y + height], width=2, outline='red')","d330b2b7":"def show_images(image_ids, bboxes, source='train'):\n    pixels = []\n    \n    for image_id in image_ids:\n        pixels.append(\n            draw_bboxes(image_id, bboxes[image_id], source)\n        )\n    \n    num_of_images = len(image_ids)\n    fig, axes = plt.subplots(\n        1, \n        num_of_images, \n        figsize=(5 * num_of_images, 5 * num_of_images)\n    )\n    \n    for i, image_pixels in enumerate(pixels):\n        axes[i].imshow(image_pixels)","71e3da4b":"show_images(train_image_ids[0:4], train_labels)","60bc6a6b":"tiny_bboxes = []\n\nfor i, image_id in enumerate(train_image_ids):\n    for label in train_labels[image_id]:\n        if label[2] * label[3] <= 10 and label[2] * label[3] != 0:\n            tiny_bboxes.append(i)\n\n            \nprint(str(len(tiny_bboxes)) + ' tiny bounding boxes found')","19731c7e":"huge_bboxes = []\n\nfor i, image_id in enumerate(train_image_ids):\n    for label in train_labels[image_id]:\n        if label[2] * label[3] > 8000:\n            huge_bboxes.append(i)\n\n            \nprint(str(len(huge_bboxes)) + ' huge bounding boxes found')","ab3e23fa":"show_images(train_image_ids[562:564], train_labels)","d994e69c":"def clean_labels(train_image_ids, train_labels):\n    good_labels = {}\n    \n    for i, image_id in enumerate(train_image_ids):\n        good_labels[image_id] = []\n        \n        for j, label in enumerate(train_labels[image_id]):\n\n            # remove huge bbox\n            if label[2] * label[3] > 8000 and i not in [1079, 1371, 2020]:\n                continue\n\n            # remove tiny bbox\n            elif label[2] < 5 or label[3] < 5:\n                continue\n                \n            else:\n                good_labels[image_id].append(\n                    train_labels[image_id][j]\n                )\n                \n    return good_labels\n\ntrain_labels = clean_labels(train_image_ids, train_labels)","4624e6b5":"class DataGenerator(tf.keras.utils.Sequence):\n\n    def __init__(self, image_ids, image_pixels, labels=None, batch_size=1, shuffle=False, augment=False):\n        self.image_ids = image_ids\n        self.image_pixels = image_pixels\n        self.labels = labels\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.augment = augment\n        self.on_epoch_end()\n        \n        self.image_grid = self.form_image_grid()\n        \n        \n    def form_image_grid(self):    \n        image_grid = np.zeros((32, 32, 4))\n\n        # x, y, width, height\n        cell = [0, 0, 256 \/ 32, 256 \/ 32] \n\n        for i in range(0, 32):\n            for j in range(0, 32):\n                image_grid[i,j] = cell\n\n                cell[0] = cell[0] + cell[2]\n\n            cell[0] = 0\n            cell[1] = cell[1] + cell[3]\n\n        return image_grid","ee6c0466":"def __len__(self):\n    return int(np.floor(len(self.image_ids) \/ self.batch_size))\n\n\ndef on_epoch_end(self):\n    self.indexes = np.arange(len(self.image_ids))\n\n    if self.shuffle == True:\n        np.random.shuffle(self.indexes)\n\n\nDataGenerator.__len__ = __len__\nDataGenerator.on_epoch_end = on_epoch_end","65202905":"DataGenerator.train_augmentations = albu.Compose([\n        albu.RandomSizedCrop(\n            min_max_height=(200, 200), \n            height=256, \n            width=256, \n            p=0.8\n        ),\n        albu.OneOf([\n            albu.Flip(),\n            albu.RandomRotate90(),\n        ], p=1),\n        albu.OneOf([\n            albu.HueSaturationValue(),\n            albu.RandomBrightnessContrast()\n        ], p=1),\n        albu.OneOf([\n            albu.GaussNoise(),\n            albu.GlassBlur(),\n            albu.ISONoise(),\n            albu.MultiplicativeNoise(),\n        ], p=0.5),\n        albu.Cutout(\n            num_holes=8, \n            max_h_size=16, \n            max_w_size=16, \n            fill_value=0, \n            p=0.5\n        ),\n        albu.CLAHE(p=1),\n        albu.ToGray(p=1),\n    ], \n    bbox_params={'format': 'coco', 'label_fields': ['labels']})\n\nDataGenerator.val_augmentations = albu.Compose([\n    albu.CLAHE(p=1),\n    albu.ToGray(p=1),\n])","ddf440d0":"def __getitem__(self, index):\n    indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n\n    batch_ids = [self.image_ids[i] for i in indexes]\n\n    X, y = self.__data_generation(batch_ids)\n\n    return X, y\n\n\ndef __data_generation(self, batch_ids):\n    X, y = [], []\n\n    # Generate data\n    for i, image_id in enumerate(batch_ids):\n        pixels = self.image_pixels[image_id]\n        bboxes = self.labels[image_id]\n\n        if self.augment:     \n            pixels, bboxes = self.augment_image(pixels, bboxes)\n        else:\n            pixels = self.contrast_image(pixels)\n            bboxes = self.form_label_grid(bboxes)\n\n        X.append(pixels)\n        y.append(bboxes)\n\n    return np.array(X), np.array(y)\n\n\ndef augment_image(self, pixels, bboxes):\n    bbox_labels = np.ones(len(bboxes))\n\n    aug_result = self.train_augmentations(image=pixels, bboxes=bboxes, labels=bbox_labels)\n\n    bboxes = self.form_label_grid(aug_result['bboxes'])\n\n    return np.array(aug_result['image']) \/ 255, bboxes\n\n\ndef contrast_image(self, pixels):        \n    aug_result = self.val_augmentations(image=pixels)\n    return np.array(aug_result['image']) \/ 255\n\n\nDataGenerator.__getitem__ = __getitem__\nDataGenerator.__data_generation = __data_generation\nDataGenerator.augment_image = augment_image\nDataGenerator.contrast_image = contrast_image","5e256a6b":"def form_label_grid(self, bboxes):\n    label_grid = np.zeros((32, 32, 10))\n\n    for i in range(0, 32):\n        for j in range(0, 32):\n            cell = self.image_grid[i,j]\n            label_grid[i,j] = self.rect_intersect(cell, bboxes)\n\n    return label_grid\n\n\ndef rect_intersect(self, cell, bboxes): \n    cell_x, cell_y, cell_width, cell_height = cell\n    cell_x_max = cell_x + cell_width \n    cell_y_max = cell_y + cell_height\n    \n    anchor_one = np.array([0, 0, 0, 0, 0])\n    anchor_two = np.array([0, 0, 0, 0, 0])\n\n    # check all boxes\n    for bbox in bboxes:\n        box_x, box_y, box_width, box_height = bbox\n        box_x_centre = box_x + (box_width \/ 2)\n        box_y_centre = box_y + (box_height \/ 2)\n\n        if(box_x_centre >= cell_x and box_x_centre < cell_x_max and box_y_centre >= cell_y and box_y_centre < cell_y_max):\n            \n            if anchor_one[0] == 0:\n                anchor_one = self.yolo_shape(\n                    [box_x, box_y, box_width, box_height], \n                    [cell_x, cell_y, cell_width, cell_height]\n                )\n            \n            elif anchor_two[0] == 0:\n                anchor_two = self.yolo_shape(\n                    [box_x, box_y, box_width, box_height], \n                    [cell_x, cell_y, cell_width, cell_height]\n                )\n                \n            else:\n                break\n\n    return np.concatenate((anchor_one, anchor_two), axis=None)\n\n\ndef yolo_shape(self, box, cell):\n    box_x, box_y, box_width, box_height = box\n    cell_x, cell_y, cell_width, cell_height = cell\n\n    # top left x,y to centre x,y\n    box_x = box_x + (box_width \/ 2)\n    box_y = box_y + (box_height \/ 2)\n\n    # offset bbox x,y to cell x,y\n    box_x = (box_x - cell_x) \/ cell_width\n    box_y = (box_y - cell_y) \/ cell_height\n\n    # bbox width,height relative to cell width,height\n    box_width = box_width \/ 256\n    box_height = box_height \/ 256\n\n    return [1, box_x, box_y, box_width, box_height]\n\n\nDataGenerator.form_label_grid = form_label_grid\nDataGenerator.rect_intersect = rect_intersect\nDataGenerator.yolo_shape = yolo_shape","c3c6891e":"train_generator = DataGenerator(\n    train_image_ids,\n    train_pixels,\n    train_labels, \n    batch_size=6, \n    shuffle=True,\n    augment=True\n)\n\nval_generator = DataGenerator(\n    val_image_ids, \n    val_pixels,\n    val_labels, \n    batch_size=10,\n    shuffle=False,\n    augment=False\n)\n\nimage_grid = train_generator.image_grid","b44e1969":"x_input = tf.keras.Input(shape=(256,256,3))\n\nx = tf.keras.layers.Conv2D(32, (3, 3), strides=(1, 1), padding='same')(x_input)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n########## block 1 ##########\nx = tf.keras.layers.Conv2D(64, (3, 3), strides=(1, 1), padding='same')(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\nx_shortcut = x\n\nfor i in range(2):\n    x = tf.keras.layers.Conv2D(32, (3, 3), strides=(1, 1), padding='same')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n    x = tf.keras.layers.Conv2D(64, (3, 3), strides=(1, 1), padding='same')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n    x = tf.keras.layers.Add()([x_shortcut, x])\n    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n    x_shortcut = x\n\n\n########## block 2 ##########\nx = tf.keras.layers.Conv2D(128, (3, 3), strides=(1, 1), padding='same')(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\nx_shortcut = x\n\nfor i in range(2):\n    x = tf.keras.layers.Conv2D(64, (3, 3), strides=(1, 1), padding='same')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n    x = tf.keras.layers.Conv2D(128, (3, 3), strides=(1, 1), padding='same')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n    x = tf.keras.layers.Add()([x_shortcut, x])\n    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n    x_shortcut = x\n\n########## block 3 ##########\nx = tf.keras.layers.Conv2D(256, (3, 3), strides=(2, 2), padding='same')(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\nx_shortcut = x\n\nfor i in range(8):\n    x = tf.keras.layers.Conv2D(128, (3, 3), strides=(1, 1), padding='same')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n    x = tf.keras.layers.Conv2D(256, (3, 3), strides=(1, 1), padding='same')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n    x = tf.keras.layers.Add()([x_shortcut, x])\n    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n    x_shortcut = x\n\n    \n########## block 4 ##########\nx = tf.keras.layers.Conv2D(512, (3, 3), strides=(2, 2), padding='same')(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\nx_shortcut = x\n\nfor i in range(8):\n    x = tf.keras.layers.Conv2D(256, (3, 3), strides=(1, 1), padding='same')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n    x = tf.keras.layers.Conv2D(512, (3, 3), strides=(1, 1), padding='same')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n    x = tf.keras.layers.Add()([x_shortcut, x])\n    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n    x_shortcut = x\n\n########## block 5 ##########\nx = tf.keras.layers.Conv2D(1024, (3, 3), strides=(2, 2), padding='same')(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\nx_shortcut = x\n\nfor i in range(4):\n    x = tf.keras.layers.Conv2D(512, (3, 3), strides=(1, 1), padding='same')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n    x = tf.keras.layers.Conv2D(1024, (3, 3), strides=(1, 1), padding='same')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n    x = tf.keras.layers.Add()([x_shortcut, x])\n    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\n    x_shortcut = x\n\n########## output layers ##########\nx = tf.keras.layers.Conv2D(512, (3, 3), strides=(1, 1), padding='same')(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\nx = tf.keras.layers.Conv2D(256, (3, 3), strides=(1, 1), padding='same')(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\nx = tf.keras.layers.Conv2D(128, (3, 3), strides=(1, 1), padding='same')(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n\npredictions = tf.keras.layers.Conv2D(10, (1, 1), strides=(1, 1), activation='sigmoid')(x)\n\nmodel = tf.keras.Model(inputs=x_input, outputs=predictions)","97cdd2e2":"def custom_loss(y_true, y_pred):\n    binary_crossentropy = prob_loss = tf.keras.losses.BinaryCrossentropy(\n        reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE\n    )\n    \n    prob_loss = binary_crossentropy(\n        tf.concat([y_true[:,:,:,0], y_true[:,:,:,5]], axis=0), \n        tf.concat([y_pred[:,:,:,0], y_pred[:,:,:,5]], axis=0)\n    )\n    \n    xy_loss = tf.keras.losses.MSE(\n        tf.concat([y_true[:,:,:,1:3], y_true[:,:,:,6:8]], axis=0), \n        tf.concat([y_pred[:,:,:,1:3], y_pred[:,:,:,6:8]], axis=0)\n    )\n    \n    wh_loss = tf.keras.losses.MSE(\n        tf.concat([y_true[:,:,:,3:5], y_true[:,:,:,8:10]], axis=0), \n        tf.concat([y_pred[:,:,:,3:5], y_pred[:,:,:,8:10]], axis=0)\n    )\n    \n    bboxes_mask = get_mask(y_true)\n    \n    xy_loss = xy_loss * bboxes_mask\n    wh_loss = wh_loss * bboxes_mask\n    \n    return prob_loss + xy_loss + wh_loss\n\n\ndef get_mask(y_true):\n    anchor_one_mask = tf.where(\n        y_true[:,:,:,0] == 0, \n        0.5, \n        5.0\n    )\n    \n    anchor_two_mask = tf.where(\n        y_true[:,:,:,5] == 0, \n        0.5, \n        5.0\n    )\n    \n    bboxes_mask = tf.concat(\n        [anchor_one_mask,anchor_two_mask],\n        axis=0\n    )\n    \n    return bboxes_mask","420128a1":"optimiser = tf.keras.optimizers.Adam(learning_rate=0.0001)\n\nmodel.compile(\n    optimizer=optimiser, \n    loss=custom_loss\n)","649321f1":"callbacks = [\n    tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', patience=2, verbose=1),\n    tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5, verbose=1, restore_best_weights=True),\n]","291317cd":"history = model.fit_generator(\n    train_generator,\n    validation_data=val_generator,\n    epochs=80,\n    callbacks=callbacks\n)","7b0790a3":"def prediction_to_bbox(bboxes, image_grid):    \n    bboxes = bboxes.copy()\n    \n    im_width = (image_grid[:,:,2] * 32)\n    im_height = (image_grid[:,:,3] * 32)\n    \n    # descale x,y\n    bboxes[:,:,1] = (bboxes[:,:,1] * image_grid[:,:,2]) + image_grid[:,:,0]\n    bboxes[:,:,2] = (bboxes[:,:,2] * image_grid[:,:,3]) + image_grid[:,:,1]\n    bboxes[:,:,6] = (bboxes[:,:,6] * image_grid[:,:,2]) + image_grid[:,:,0]\n    bboxes[:,:,7] = (bboxes[:,:,7] * image_grid[:,:,3]) + image_grid[:,:,1]\n    \n    # descale width,height\n    bboxes[:,:,3] = bboxes[:,:,3] * im_width \n    bboxes[:,:,4] = bboxes[:,:,4] * im_height\n    bboxes[:,:,8] = bboxes[:,:,8] * im_width \n    bboxes[:,:,9] = bboxes[:,:,9] * im_height\n    \n    # centre x,y to top left x,y\n    bboxes[:,:,1] = bboxes[:,:,1] - (bboxes[:,:,3] \/ 2)\n    bboxes[:,:,2] = bboxes[:,:,2] - (bboxes[:,:,4] \/ 2)\n    bboxes[:,:,6] = bboxes[:,:,6] - (bboxes[:,:,8] \/ 2)\n    bboxes[:,:,7] = bboxes[:,:,7] - (bboxes[:,:,9] \/ 2)\n    \n    # width,heigth to x_max,y_max\n    bboxes[:,:,3] = bboxes[:,:,1] + bboxes[:,:,3]\n    bboxes[:,:,4] = bboxes[:,:,2] + bboxes[:,:,4]\n    bboxes[:,:,8] = bboxes[:,:,6] + bboxes[:,:,8]\n    bboxes[:,:,9] = bboxes[:,:,7] + bboxes[:,:,9]\n    \n    return bboxes","840fec64":"def non_max_suppression(predictions, top_n):\n    probabilities = np.concatenate((predictions[:,:,0].flatten(), predictions[:,:,5].flatten()), axis=None)\n    \n    first_anchors = predictions[:,:,1:5].reshape((32*32, 4))\n    second_anchors = predictions[:,:,6:10].reshape((32*32, 4))\n    \n    bboxes = np.concatenate(\n        (first_anchors,second_anchors),\n        axis=0\n    )\n    \n    bboxes = switch_x_y(bboxes)\n    bboxes, probabilities = select_top(probabilities, bboxes, top_n=top_n)\n    bboxes = switch_x_y(bboxes)\n    \n    return bboxes\n\n\ndef switch_x_y(bboxes):\n    x1 = bboxes[:,0].copy()\n    y1 = bboxes[:,1].copy()\n    x2 = bboxes[:,2].copy()\n    y2 = bboxes[:,3].copy()\n    \n    bboxes[:,0] = y1\n    bboxes[:,1] = x1\n    bboxes[:,2] = y2\n    bboxes[:,3] = x2\n    \n    return bboxes\n\n\ndef select_top(probabilities, boxes, top_n=10):\n    top_indices = tf.image.non_max_suppression(\n        boxes = boxes, \n        scores = probabilities, \n        max_output_size = top_n, \n        iou_threshold = 0.3,\n        score_threshold = 0.3\n    )\n    \n    top_indices = top_indices.numpy()\n    \n    return boxes[top_indices], probabilities[top_indices]","899ca9f2":"def process_predictions(predictions, image_ids, image_grid):\n    bboxes = {}\n    \n    for i, image_id in enumerate(image_ids):\n        predictions[i] = prediction_to_bbox(predictions[i], image_grid)\n        bboxes[image_id] = non_max_suppression(predictions[i], top_n=100)\n        \n        # back to coco shape\n        bboxes[image_id][:,2:4] = bboxes[image_id][:,2:4] - bboxes[image_id][:,0:2]\n    \n    return bboxes","c12243f6":"val_predictions = model.predict(val_generator)\nval_predictions = process_predictions(val_predictions, val_image_ids, image_grid)","1886a555":"show_images(val_image_ids[0:4], val_predictions)","244c09a1":"show_images(val_image_ids[4:8], val_predictions)","bc3d0faa":"print('Epochs: ' + str(len(history.history['loss'])))\nprint('Final training loss: ' + str(history.history['loss'][-1]))\nprint('Final validation loss: ' + str(history.history['val_loss'][-1]))","201fafe7":"fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n\nax[0].set_title('Training Loss')\nax[0].plot(history.history['loss'])\n\nax[1].set_title('Validation Loss')\nax[1].plot(history.history['val_loss'])","ab294dad":"layer_outputs = [layer.output for layer in model.layers]\nevaluation_model = tf.keras.Model(inputs=model.input, outputs=layer_outputs)","9a626cde":"image = Image.open('..\/input\/global-wheat-detection\/train\/' + train_image_ids[1] + \".jpg\")\nimage = image.resize((256, 256))\n\npixels = np.asarray(image) \/ 255\npixels = np.expand_dims(pixels, axis=0)","73a247b1":"num_of_layers = len(layer_outputs)\n\nfig, axes = plt.subplots(2, 6, figsize=(20, 10))\n\nlayer = 0\nfor i in range(0, 2):\n    for j in range(0, 6):\n        layer_output = evaluation_model.predict(pixels)[layer]\n        axes[i, j].imshow(layer_output[0, :, :, 1], cmap='inferno')\n        \n        layer = layer + 1","90278076":"test_image_ids = os.listdir('\/kaggle\/input\/global-wheat-detection\/test\/')\ntest_image_ids = [image_id[:-4] for image_id in test_image_ids]","24acf8d3":"test_predictions = []\n\nfor i, image_id in enumerate(test_image_ids):\n    image = Image.open('\/kaggle\/input\/global-wheat-detection\/test\/' + image_id + \".jpg\")\n    image = image.resize((256, 256))            \n\n    pixels = np.asarray(image)\n\n    val_augmentations = albu.Compose([\n        albu.CLAHE(p=1),\n        albu.ToGray(p=1)\n    ])\n\n    aug_result = val_augmentations(image=pixels)\n    pixels = np.array(aug_result['image']) \/ 255\n    \n    pixels = np.expand_dims(pixels, axis=0)\n    \n    bboxes = model.predict(pixels)\n\n    test_predictions.append(bboxes)\n    \ntest_predictions = np.concatenate(test_predictions)","9e310634":"test_predictions = process_predictions(test_predictions, test_image_ids, image_grid)","5c0b54f9":"show_images(test_image_ids[0:4], test_predictions, source='test')","4af0e9ec":"show_images(test_image_ids[4:8], test_predictions, source='test')","7ffc98e9":"show_images(test_image_ids[8:10], test_predictions, source='test')","d3464884":"model.save_weights('wheat_detection_model')","4546c7ed":"Now loop through the images, loading each one as a numpy array, applying augmentations to it and feeding it into the model. Save the bounding box predictions as a numpy array.","2d374f08":"Finally save the weights so that the model can be used in an inference notebook.","65cb6d27":"I did some more manual inspection of the bad labels picked out of this code that I have not included in this notebook. I found that some huge bounding boxes were actually okay as they bound a very zoomed in image. To this end I have listed a few to be kept (1079, 1371, 2020). Otherwise the below code throws out any bounding boxes whose area is larger than 8000 or smaller than 5.","d5ce5a08":"While a high learning rate at the beginning of a training run is great at the start, it can cause issues as the model approaches convergence when smaller, more careful steps are needed. I considered using learning rate decay to handle this but decided on a callback to reduce the learning rate when it plateaus (or increases) over the space of two epochs. This allows the model to make the most of a higher rate until it that rate is too high at which point the model wuicky reduces it.\n\nIn addition to this I have added an early stopping callback to stop the model training if is no longer able to reduce the loss. This reduces any waste processing and provides faster feedback if the model just isn't training very well.","13b9802d":"## Visualise images\n\nBefore going on it is worth having a look at some of the images and bounding boxes in the dataset. For that a few helper functions will be required. The below functions take an image id and the corresponding bounding boxes and return the bounding boxes drawn onto the image.","ec8c322c":"Here's a sample of five bounding boxes for one of the images.","13249a61":"Regarding the augmentations a number of transformations will be applied to each training image before they are fed into the model. This helps to add some diversity to a small dataset effectively growing it to a much larger one:\n\n- **random sized crop:** The model needs to be able to detect a wheat head regardless of how close or far away the head is to the camera. To produce more zoom levels in the dataset the crop method will take a portion of the image and zoom in to create a new image with larger wheat heads.\n- **flip amd rotate**: The wheat heads can point in any direction. To create more examples of wheat heads pointing in different directions the image will randomly be flipped both horizontally and vertically or rotated.\n- **hue saturation and brightness:** these are various methods that will alter the lighting of the image which will help to create different lighting scenarios. This helps as the test pictures are from various countries each with their own lighting levels.\n- **noise:** Some wheat heads aren't quite in focus. Adding some noise to the images helps to catch these wheat heads while also forcing the model to learn more abstract wheat head shapes. This helps a lot with over-fitting.\n- **cutout**: randomly remove small squares of pixels in the image. This prevents the model simply memorizing certain wheat heads and instead forces it to learn the patterns that represent a wheat head.\n- **clahe:** this is a must have. In many images the wheat heads are a similar colour to the grass in the background making it tricky for the model to differentiate between them. CLAHE helps to exemplify the colour difference between the two.\n- **grey scale:** I found that there were a few images with a yellow\/gold tint. My model was learning to detect wheat heads without a tint (as most images do not contain a tint) and was really struggling to detect anything on the yellow images. By converting all images to grey scale the model is forced to ignore these tints making it much more effective at identifying wheat heads regardless of tint.\n\nI also greyscale and apply CLAHE to each validation image as the model has learnt on grey images where the wheat heads are given a lighter shade of grey.","467d3282":"With the labels extracted from the data I now need the images loaded as numpy arrays. At this point it is worth splitting the data into a training and validation dataset. As the dataset is small I will keep the vast majority of the images in the training dataset and only put the last ten images aside as a validation dataset. This may not be the best size to perform accurate validation but I think it is the right compromise considering the number of images available and the complexity of the task.","12c402e6":"Let's see how the model did by producing predictions for some images in the validation dataset.","d6b870a9":"Next the bounding boxes with low confidence need removing. I also need to remove any boxes that overlap another box. Luckily Tensorflow has a non-max suppression function that filters out both low confidence boxes and removes one box if any two overlap. There's just a little bit of reshaping to prepare the bounding boxes for this function one of which includes switching the position of the x and y dimensions.","66cbcfe2":"Then pick an image and cycle through the layers making a prediction and visualising the features outputted by the layer. The warm colours represent where the features lie in this image.\n\nNote that I have not visualised all the layers here due to the large number of them.","16ea6ad5":"## Model\n\nWith the data ready to go I'll define and train the model. As mentioned before this model is inspired by yolo, specifically yolo v3. This is a large and at times complex model. Below is an outline of the model. Basically the model begins with a convolutional layer with 32 filters which doubles in size in the next layer. The filters are then halved in size before doubling every layer up to 128 layers. The filters are then halved again while a larger stride reduces the size of the input image. This pattern of doubling and halving filter sizes continues with a few repeated blocks until we reach a size of 1024. A few resnet skip layers are added in as well to stabilise the large number of layers and reduce the chance of vanishing gradients.\n\n![Screenshot%202020-07-13%20at%2010.42.35.png](attachment:Screenshot%202020-07-13%20at%2010.42.35.png)\n*figure 3, An outline of the model taken from the yolov3 [paper](https:\/\/pjreddie.com\/media\/files\/papers\/YOLOv3.pdf).*\n\nBelow is my keras implementation of the model. The model is mostly in line with the yolov3 architecture though I have removed a few layers and altered some others. ","e3ab011e":"With the ids split it's now time to load the images. To keep the training of the model relatively fast I will resize each image from (1024,1024) to (256,256). I experimented with larger images and with the model I am using I didn't see a good enough lift in training accuracy to make up for the extra time it took to train a model with larger images.","31f21dee":"## Load data\n\nI'll begin by loading the labels data and extracting the bounding boxes (which I refer to as \"bboxes\" in the code) from the table.","94727116":"In the raw data each bounding box is in an array of length 4 but formatted as a string. The below block of code groups the bounding boxes by image id and places the bounding boxes as numpy arrays next to each image id. The image id can then be used to quickly retrieve all of the bounding boxes.","13415e5e":"Next I will add some methods to the class that keras needs to operate the data generation. length is used to determine how many images there are in the dataset. on_epoch_end is called at the end of each epoch (as well once before training starts) to get the index of all images in the dataset. It is also has the opportunity to shuffle the dataset per epoch if the generator was configured to do so.","15bc6df4":"The next functions load an image and the corresponding bounding boxes depending on randomly picked image ids. As well as loading the images the above augmentations are added to an image as it is loaded. As the albumentaitons library was used to apply these augmentations I get the bounding boxes re-sized for free.","a3d1f6ed":"## Prediction post processing\n\nThe model outputs the predicted bounding boxes as a label grid. However to visualise the bounding boxes on an image or submit them to the competition the shape for one images bounding boxes need changing from (16,16,10) to (m, 4) where m represents the number of bounding boxes that have a high confidence.\n\nThis first function transforms the boxes from the yolo format to the coco format. It does this through the following:\n\n- return the scale of the boxes from 0-1 to 0-256\n- change the x,y from the centre of the box to the top left corner\n- change width and height to x_max, y_max i.e. change to voc shape","860cf4fe":"## Test Images\n\nFinally predict bounding boxes for the test set. I wanted to see how easy\/difficult it would be to use the model without a data generator class so I have inputted the images into the model as numpy arrays.\n\nFirst, load the test image ids.","8fd74912":"## Data pipeline\n\nUsually I would use Tensorflows data api or keras data generators to build a pipeline to get data into the model. However the pre-processing that needs to be done for this model is not trivial and it turned out to be easier to create a custom data generator. This takes the form of a class that is passed to keras' fit generator function. It contains the following functionality:\n\n- define the size of the dataset. Keras needs this to work out how long an epoch is.\n- shuffle the dataset.\n- get an image and augment it to add variety to the dataset. This includes amending bounding boxes when a head of wheat has changed in the image.\n- reshape the bounding boxes to a label grid.\n\nI'll start by initialising the class.","7c50b700":"I experimented with a few optimisers including FTRL and SGD but in the end Adam was the fastest and most reliable function. I have kept the learning rate reasonably high as it can take a number of steps to get the model moving quickly towards convergence. A higher rate helps to reduce the number of steps needed to get the model going.","a04308a2":"Then apply the typical post processing functions to the predictions.","ec211f65":"Wrap these post-processing functions into one and output the predicted bounding boxes as a dictionary where the image id is the key.","deb0cbcc":"## Clean bounding boxes\n\nThere are a small number of bounding boxes in this dataset that do not bound a head of wheat. While the number is small enough that the model can still learn how to detect the heads of wheat they still cause a little bit of inaccuracy. Below I'll search for tiny bounding boxes that cannot possibly fit a head of wheat inside them and huge bounding boxes that miss the head of wheat they are aimed at.","b073380e":"I'll also add a wrapper function to call this function multiple times for multiple images.","081e321e":"One issue with yolo is that it is likely to contain more cells in its label grid that contain no objects than cells that do contain objects. It is easy then for the model to focus too much on learning to reduce no object cells to zero and not focus enough on getting the bounding boxes to the right shape. To overcome this the yolo [paper](https:\/\/pjreddie.com\/media\/files\/papers\/YOLOv3.pdf) suggests weighting the cells containing bounding boxes five times higher and the cells with no bounding boxes by half. \n\nI have defined a custom loss function to do just this. I have also split the loss function into three parts. The first takes care of the confidence score that is trying to work out if a label grid cell contains a head of wheat or not. Binary cross entropy is used here as that is a binary classification task. The second part looks at the x,y position of the bounding boxes while the third looks at the width,height of the bounding boxes. MSE (mean squared loss) is used for the second and third parts as they are regression tasks. ","3fec01dd":"Finally the model is ready to be trained. The data generators are passed into the fit generator method of the model alongside the callbacks and the maximum number of epochs to take. Be warned that with 100 or less images this model can train at an okay speed on CPU. Any more images than that will need the GPU (which could still run for a few hours).","47b24b45":"I entered this competition as an opportunity to implement an object detection model, something I know how to do in theory but have not yet put into practice. As yolo is the model I am most familiar with I will be implementing a model inspired by yolov3 (though not exactly the same) using Tensorflow.","19537845":"The tiny bounding boxes are actually too small to show when visualised on an image. However we can take a peak at one of the huge bounding boxes.","68045313":"## Evaluate Model\n\nWith the model trained it's time to look at the quality of the model. Begin by plotting the loss curve.","41313f49":"Then visualise the first few layers of the model to see how each layer influences the bounding boxes. Start by copying the model and configuring the new one to return each layers output when a prediction is made.","d6c9c282":"Although only a submission to the competition will provide a final score on how good the model is, I'll visualise each test image with their predicted boxes to get an idea of the models quality. This involves a slightly different visualisation function just to accomodate the larger images.","f56a92fd":"The final part of the data generator class re-shapes the bounding box labels. It's worth mentioning here that there are a number of ways to represent a bounding box with four numbers. Some common ways are coco (the shape the boxes are in the raw data), voc-pascal and yolo.\n\n![Screenshot%202020-07-13%20at%2010.29.09.png](attachment:Screenshot%202020-07-13%20at%2010.29.09.png)\n*Figure 1, some common ways to represent bounding boxes*\n\nI'll be using the yolo shape for this model. In addition to the above shape, yolo detects objects by placing a grid over the image and asking if an object (such as a wheat head) is present in any of the cells of the grid. I've decided to use a 32x32 grid for this challenge which I'll refer to as a label grid. The bounding boxes are reshaped to be offset within the relevant cells of the image. Then all four variables (x, y, width and height) are scaled down to a 0-1 scale using the width and height of the image.\n\n![Screenshot%202020-06-12%20at%2016.29.19.png](attachment:Screenshot%202020-06-12%20at%2016.29.19.png)\n*Figure 2, a bounding box is put in the cell most at the centre of an object. It's x and y are offset to the cell while width and height remain the same*\n\nAny cells in the grid that have no objects within them contain a bounding box with the dimensions [0, 0, 0, 0].\n\nEach bounding box gets a confidence score where a value of 1 tells us that an object (wheat head) is present in the cell and a value of 0 tells us that no object is present. So a cell with an object present could contain a value like this: [1, 0.5, 0.5, 0.2, 0.2] telling us that there is an object present (due to the confidence score of 1, the centre of the bounding box is exactly in the middle of the cell and the box is 20% of the images total width and height.\n\nAs a cell could contain two overlapping heads of wheat I have configured the grid to contain up to two bounding boxes. These are known as anchor boxes.\n\nThe below code takes the list of bounding boxes for an image and puts them into the yolo label grid shape."}}