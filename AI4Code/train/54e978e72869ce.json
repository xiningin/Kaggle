{"cell_type":{"f897b800":"code","4e2c3c1e":"code","0cbed23e":"code","8c71d6c9":"code","905f5514":"code","9d70c27b":"code","fbbbb8da":"code","570df91e":"code","ce11e481":"code","f1548558":"code","c46e14d7":"code","58f173ed":"code","39e066dd":"code","32e1184a":"code","ab5d6266":"code","7862a467":"code","5c98821e":"markdown","9374ea51":"markdown","417d2233":"markdown","e37e36c9":"markdown","e0b4ee57":"markdown","76f75715":"markdown","dfd0b81f":"markdown","9c893878":"markdown","bdeb0465":"markdown","533e70bf":"markdown","339464ad":"markdown","9fa1405f":"markdown","68fb543e":"markdown","1380dcaf":"markdown","2a1e590c":"markdown","a88c4abe":"markdown","56b85640":"markdown","7d7d4931":"markdown","7954d18a":"markdown","d4d063ab":"markdown","5ae25b13":"markdown","237528b3":"markdown","911d2c93":"markdown","6a55dc8d":"markdown","135e72ea":"markdown","91ba6ec3":"markdown","15a4e301":"markdown","6c4567d1":"markdown","2b85a37f":"markdown","716641d1":"markdown","9f8979a3":"markdown","397108d6":"markdown","f932a5c5":"markdown","1503de00":"markdown"},"source":{"f897b800":"# Import libraries necessary for this project\nimport numpy as np\nimport pandas as pd\nfrom time import time\nfrom IPython.display import display # Allows the use of display() for DataFrames\nimport warnings\n#warnings.filterwarnings(\"ignore\", category = UserWarning, module = \"matplotlib\")\nwarnings.filterwarnings(\"ignore\")\n# Display inline matplotlib plots with IPython\nfrom IPython import get_ipython\nget_ipython().run_line_magic('matplotlib', 'inline')\nimport matplotlib.pyplot as pl\nimport matplotlib.patches as mpatches\nfrom sklearn.metrics import f1_score, accuracy_score\n\n# Pretty display for notebooks\n%matplotlib inline\n\n# Load the Census dataset\ndata = pd.read_csv(\"..\/input\/census\/census.csv\")\n\n# Success - Display the first record\ndisplay(data.head(n=1))","4e2c3c1e":"# Total number of records\nn_records = len(data)\n\n# Number of records where individual's income is more than $50,000\nn_greater_50k = len(data[data.income==\">50K\"])\n\n# Number of records where individual's income is at most $50,000\nn_at_most_50k = len(data[data.income==\"<=50K\"])\n\n# Percentage of individuals whose income is more than $50,000\ngreater_percent = n_greater_50k\/n_records*100.0\n\n# Print the results\nprint(\"Total number of records: {}\".format(n_records))\nprint(\"Individuals making more than $50,000: {}\".format(n_greater_50k))\nprint(\"Individuals making at most $50,000: {}\".format(n_at_most_50k))\nprint(\"Percentage of individuals making more than $50,000: {:.2f}%\".format(greater_percent))","0cbed23e":"def distribution(data, transformed = False):\n    \"\"\"\n    Visualization code for displaying skewed distributions of features\n    \"\"\"\n    \n    # Create figure\n    fig = pl.figure(figsize = (11,5));\n\n    # Skewed feature plotting\n    for i, feature in enumerate(['capital-gain','capital-loss']):\n        ax = fig.add_subplot(1, 2, i+1)\n        ax.hist(data[feature], bins = 25, color = '#00A0A0')\n        ax.set_title(\"'%s' Feature Distribution\"%(feature), fontsize = 14)\n        ax.set_xlabel(\"Value\")\n        ax.set_ylabel(\"Number of Records\")\n        ax.set_ylim((0, 2000))\n        ax.set_yticks([0, 500, 1000, 1500, 2000])\n        ax.set_yticklabels([0, 500, 1000, 1500, \">2000\"])\n\n    # Plot aesthetics\n    if transformed:\n        fig.suptitle(\"Log-transformed Distributions of Continuous Census Data Features\", \\\n            fontsize = 16, y = 1.03)\n    else:\n        fig.suptitle(\"Skewed Distributions of Continuous Census Data Features\", \\\n            fontsize = 16, y = 1.03)\n\n    fig.tight_layout()\n    fig.show()\n","8c71d6c9":"# Split the data into features and target label\nincome_raw = data['income']\nfeatures_raw = data.drop('income', axis = 1)\n# Visualize skewed continuous features of original data\ndistribution(data)","905f5514":"# Log-transform the skewed features\nskewed = ['capital-gain', 'capital-loss']\nfeatures_log_transformed = pd.DataFrame(data = features_raw)\nfeatures_log_transformed[skewed] = features_raw[skewed].apply(lambda x: np.log(x + 1))\n\n# Visualize the new log distributions\ndistribution(features_log_transformed, transformed = True)","9d70c27b":"# Import sklearn.preprocessing.StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Initialize a scaler, then apply it to the features\nscaler = MinMaxScaler() # default=(0, 1)\nnumerical = ['age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n\nfeatures_log_minmax_transform = pd.DataFrame(data = features_log_transformed)\nfeatures_log_minmax_transform[numerical] = scaler.fit_transform(features_log_transformed[numerical])\n\n# Show an example of a record with scaling applied\ndisplay(features_log_minmax_transform.head(n = 5))","fbbbb8da":"# One-hot encode the 'features_log_minmax_transform' data using pandas.get_dummies()\nfeatures_final = pd.get_dummies(features_log_minmax_transform)\n\n# Encode the 'income_raw' data to numerical values\nincome = income_raw.replace({'<=50K':0, '>50K':1})\n\n# Print the number of features after one-hot encoding\nencoded = list(features_final.columns)\nprint(\"{} total features after one-hot encoding.\".format(len(encoded)))\n\n# Uncomment the following line to see the encoded feature names\nprint(encoded)","570df91e":"# Import train_test_split\nfrom sklearn.model_selection import train_test_split\n\n# Split the 'features' and 'income' data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(features_final, \n                                                    income, \n                                                    test_size = 0.2, \n                                                    random_state = 0)\n\n# Show the results of the split\nprint(\"Training set has {} samples.\".format(X_train.shape[0]))\nprint(\"Testing set has {} samples.\".format(X_test.shape[0]))","ce11e481":"'''\nTP = np.sum(income) # Counting the ones as this is the naive case. Note that 'income' is the 'income_raw' data \nencoded to numerical values done in the data preprocessing step.\nFP = income.count() - TP # Specific to the naive case\n\nTN = 0 # No predicted negatives in the naive case\nFN = 0 # No predicted negatives in the naive case\n'''\n# Calculate accuracy, precision and recall\nTP = np.sum(income)  \nFP = income.count() - TP\nTN = 0\nFN = 0\naccuracy =float(TP)\/(TP+FP)\nrecall =float(TP)\/(TP+FN)\nprecision = TP \/ (TP + FP)\n\n# Calculate F-score using the formula above for beta = 0.5 and correct values for precision and recall.\n\nfscore = (1+0.5**2)*(precision*recall)\/(0.5**2*precision+recall)\n\n# Print the results \nprint(\"Naive Predictor: [Accuracy score: {:.4f}, F-score: {:.4f}]\".format(accuracy, fscore))","f1548558":"# Import two metrics from sklearn - fbeta_score and accuracy_score\n\nfrom sklearn.metrics import fbeta_score\nfrom sklearn.metrics import accuracy_score\n \n\ndef train_predict(learner, sample_size, X_train, y_train, X_test, y_test): \n    '''\n    inputs:\n       - learner: the learning algorithm to be trained and predicted on\n       - sample_size: the size of samples (number) to be drawn from training set\n       - X_train: features training set\n       - y_train: income training set\n       - X_test: features testing set\n       - y_test: income testing set\n    '''\n    \n    results = {}\n    \n    # Fit the learner to the training data using slicing with 'sample_size' using .fit(training_features[:], training_labels[:])\n    \n    start = time() # Get start time\n    learner = learner.fit(X_train[:sample_size], y_train[:sample_size])\n    end = time() # Get end time\n    \n    # Calculate the training time\n    \n    results['train_time'] = end - start\n        \n    # Get the predictions on the test set(X_test),\n    #       then get predictions on the first 300 training samples(X_train) using .predict()\n    \n    start = time() # Get start time\n    predictions_test = learner.predict(X_test)\n    predictions_train = learner.predict(X_train[:300])\n    end = time() # Get end time\n    \n    # Calculate the total prediction time\n    \n    results['pred_time'] = end-start\n            \n    # Compute accuracy on the first 300 training samples which is y_train[:300]\n   \n    results['acc_train'] = accuracy_score(y_train[:300],predictions_train)\n        \n    # Compute accuracy on test set using accuracy_score()\n    \n    results['acc_test'] = accuracy_score(y_test,predictions_test)\n    \n    # Compute F-score on the the first 300 training samples using fbeta_score()\n    \n    results['f_train'] = fbeta_score(y_train[:300],predictions_train,beta=0.5)\n        \n    # Compute F-score on the test set which is y_test\n    \n    results['f_test'] = fbeta_score(y_test,predictions_test,beta=0.5)\n       \n    # Success\n    print(\"{} trained on {} samples.\".format(learner.__class__.__name__, sample_size))\n        \n    # Return the results\n    return results","c46e14d7":"def evaluate(results, accuracy, f1):\n    \"\"\"\n    Visualization code to display results of various learners.\n    \n    inputs:\n      - learners: a list of supervised learners\n      - stats: a list of dictionaries of the statistic results from 'train_predict()'\n      - accuracy: The score for the naive predictor\n      - f1: The score for the naive predictor\n    \"\"\"\n  \n    # Create figure\n    fig, ax = pl.subplots(2, 3, figsize = (11,7))\n\n    # Constants\n    bar_width = 0.3\n    colors = ['#A00000','#00A0A0','#00A000']\n    \n    # Super loop to plot four panels of data\n    for k, learner in enumerate(results.keys()):\n        for j, metric in enumerate(['train_time', 'acc_train', 'f_train', 'pred_time', 'acc_test', 'f_test']):\n            for i in np.arange(3):\n                \n                # Creative plot code\n                ax[j\/\/3, j%3].bar(i+k*bar_width, results[learner][i][metric], width = bar_width, color = colors[k])\n                ax[j\/\/3, j%3].set_xticks([0.45, 1.45, 2.45])\n                ax[j\/\/3, j%3].set_xticklabels([\"1%\", \"10%\", \"100%\"])\n                ax[j\/\/3, j%3].set_xlabel(\"Training Set Size\")\n                ax[j\/\/3, j%3].set_xlim((-0.1, 3.0))\n    \n    # Add unique y-labels\n    ax[0, 0].set_ylabel(\"Time (in seconds)\")\n    ax[0, 1].set_ylabel(\"Accuracy Score\")\n    ax[0, 2].set_ylabel(\"F-score\")\n    ax[1, 0].set_ylabel(\"Time (in seconds)\")\n    ax[1, 1].set_ylabel(\"Accuracy Score\")\n    ax[1, 2].set_ylabel(\"F-score\")\n    \n    # Add titles\n    ax[0, 0].set_title(\"Model Training\")\n    ax[0, 1].set_title(\"Accuracy Score on Training Subset\")\n    ax[0, 2].set_title(\"F-score on Training Subset\")\n    ax[1, 0].set_title(\"Model Predicting\")\n    ax[1, 1].set_title(\"Accuracy Score on Testing Set\")\n    ax[1, 2].set_title(\"F-score on Testing Set\")\n    \n    # Add horizontal lines for naive predictors\n    ax[0, 1].axhline(y = accuracy, xmin = -0.1, xmax = 3.0, linewidth = 1, color = 'k', linestyle = 'dashed')\n    ax[1, 1].axhline(y = accuracy, xmin = -0.1, xmax = 3.0, linewidth = 1, color = 'k', linestyle = 'dashed')\n    ax[0, 2].axhline(y = f1, xmin = -0.1, xmax = 3.0, linewidth = 1, color = 'k', linestyle = 'dashed')\n    ax[1, 2].axhline(y = f1, xmin = -0.1, xmax = 3.0, linewidth = 1, color = 'k', linestyle = 'dashed')\n    \n    # Set y-limits for score panels\n    ax[0, 1].set_ylim((0, 1))\n    ax[0, 2].set_ylim((0, 1))\n    ax[1, 1].set_ylim((0, 1))\n    ax[1, 2].set_ylim((0, 1))\n\n    # Create patches for the legend\n    patches = []\n    for i, learner in enumerate(results.keys()):\n        patches.append(mpatches.Patch(color = colors[i], label = learner))\n    pl.legend(handles = patches, bbox_to_anchor = (-.80, 2.53), \\\n               loc = 'upper center', borderaxespad = 0., ncol = 3, fontsize = 'x-large')\n    \n    # Aesthetics\n    pl.suptitle(\"Performance Metrics for Three Supervised Learning Models\", fontsize = 16, y = 1.10)\n    pl.tight_layout()\n    pl.show()\n    ","58f173ed":"# Import the three supervised learning models from sklearn\n\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\n\n\n# Initialize the three models\n\nclf_A = LogisticRegression(random_state=1) # Logisitic Regression Classifier\nclf_B = AdaBoostClassifier(random_state=1) # Support Vector Machines Classifier\nclf_C = DecisionTreeClassifier(random_state=1) # Decision Tree Classifier\n\n# Calculate the number of samples for 1%, 10%, and 100% of the training data\n# HINT: samples_100 is the entire training set i.e. len(y_train)\n# HINT: samples_10 is 10% of samples_100 (ensure to set the count of the values to be `int` and not `float`)\n# HINT: samples_1 is 1% of samples_100 (ensure to set the count of the values to be `int` and not `float`)\n\nsamples_100 = len(y_train)\nsamples_10 = int(samples_100 * 0.1)\nsamples_1 = int(samples_100 * 0.01)\n\n# Collect results on the learners\nresults = {}\nfor clf in [clf_A, clf_B, clf_C]:\n    clf_name = clf.__class__.__name__\n    results[clf_name] = {}\n    for i, samples in enumerate([samples_1, samples_10, samples_100]):\n        results[clf_name][i] = \\\n        train_predict(clf, samples, X_train, y_train, X_test, y_test)\n\n# metrics visualization for the three supervised learning models chosen\nevaluate(results, accuracy, fscore)","39e066dd":"# Import 'GridSearchCV', 'make_scorer', and any other necessary libraries\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\n\n# Initialize the classifier\nclf = LogisticRegression(random_state=1)\n\n# Create the parameters list you wish to tune, using a dictionary if needed.\n# HINT: parameters = {'parameter_1': [value1, value2], 'parameter_2': [value1, value2]}\n\nparameters ={'C': [0.01, 0.1, 1, 10, 100], 'penalty': ['l1', 'l2']}\n\n\n# Make an fbeta_score scoring object using make_scorer()\nscorer = make_scorer(fbeta_score, beta=0.5)\n\n# Perform grid search on the classifier using 'scorer' as the scoring method using GridSearchCV()\ngrid_obj = GridSearchCV(clf, param_grid = parameters,scoring=scorer)\n\n# Fit the grid search object to the training data and find the optimal parameters using fit()\ngrid_fit = grid_obj.fit(X_train, y_train)\n\n# Get the estimator\nbest_clf = grid_fit.best_estimator_\n\n# Make predictions using the unoptimized and model\npredictions = (clf.fit(X_train, y_train)).predict(X_test)\nbest_predictions = best_clf.predict(X_test)\n\n# Report the before-and-afterscores\nprint(\"Unoptimized model\\n------\")\nprint(\"Accuracy score on testing data: {:.4f}\".format(accuracy_score(y_test, predictions)))\nprint(\"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, predictions, beta = 0.5)))\nprint(\"\\nOptimized Model\\n------\")\nprint(\"Final accuracy score on the testing data: {:.4f}\".format(accuracy_score(y_test, best_predictions)))\nprint(\"Final F-score on the testing data: {:.4f}\".format(fbeta_score(y_test, best_predictions, beta = 0.5)))","32e1184a":"def feature_plot(importances, X_train, y_train):\n    \n    # Display the five most important features\n    indices = np.argsort(importances)[::-1]\n    columns = X_train.columns.values[indices[:5]]\n    values = importances[indices][:5]\n\n    # Creat the plot\n    fig = pl.figure(figsize = (9,5))\n    pl.title(\"Normalized Weights for First Five Most Predictive Features\", fontsize = 16)\n    pl.bar(np.arange(5), values, width = 0.6, align=\"center\", color = '#00A000', \\\n          label = \"Feature Weight\")\n    pl.bar(np.arange(5) - 0.3, np.cumsum(values), width = 0.2, align = \"center\", color = '#00A0A0', \\\n          label = \"Cumulative Feature Weight\")\n    pl.xticks(np.arange(5), columns)\n    pl.xlim((-0.5, 4.5))\n    pl.ylabel(\"Weight\", fontsize = 12)\n    pl.xlabel(\"Feature\", fontsize = 12)\n    \n    pl.legend(loc = 'upper center')\n    pl.tight_layout()\n    pl.show()  \n","ab5d6266":"# Import a supervised learning model that has 'feature_importances_'\n\nclf = AdaBoostClassifier(random_state=1)\n\n# Train the supervised model on the training set using .fit(X_train, y_train)\nmodel = clf.fit(X_train,y_train)\n\n# Extract the feature importances using .feature_importances_ \nimportances = clf.feature_importances_\n\n# Plot\nfeature_plot(importances, X_train, y_train)","7862a467":"# Import functionality for cloning a model\nfrom sklearn.base import clone\n\n# Reduce the feature space\nX_train_reduced = X_train[X_train.columns.values[(np.argsort(importances)[::-1])[:5]]]\nX_test_reduced = X_test[X_test.columns.values[(np.argsort(importances)[::-1])[:5]]]\n\n# Train on the \"best\" model found from grid search earlier\nclf = (clone(best_clf)).fit(X_train_reduced, y_train)\n\n# Make new predictions\nreduced_predictions = clf.predict(X_test_reduced)\n\n# Report scores from the final model using both versions of data\nprint(\"Final Model trained on full data\\n------\")\nprint(\"Accuracy on testing data: {:.4f}\".format(accuracy_score(y_test, best_predictions)))\nprint(\"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, best_predictions, beta = 0.5)))\nprint(\"\\nFinal Model trained on reduced data\\n------\")\nprint(\"Accuracy on testing data: {:.4f}\".format(accuracy_score(y_test, reduced_predictions)))\nprint(\"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, reduced_predictions, beta = 0.5)))","5c98821e":"\nAdditionally, identifying someone that does not make more than*$50,000* as someone who does would be detrimental to our imaginary charity non-profit organization, since they are looking to find individuals willing to donate.","9374ea51":"Looking at the distribution of classes ( those who make at most *$50,000*, and those who make more), it's clear most individuals do not make more than \\$50,000.","417d2233":"### Extracting Feature Importance\nUsing a `scikit-learn` supervised learning algorithm that has a `feature_importance_` attribute availble for it. This attribute is a function that ranks the importance of each feature when making predictions based on the chosen algorithm.\n\nIn the code below, I will implement the following:\n - Import a supervised learning model from sklearn if it is different from the three used earlier.\n - Train the supervised model on the entire training set.\n - Extract the feature importances using `'.feature_importances_'`.","e37e36c9":"----\n## Improving Results\nIn this final section, we will choose from the three supervised learning models the *best* model to use on the data. we will then perform a grid search optimization for the model over the entire training set (`X_train` and `y_train`) by tuning at least one parameter to improve upon the untuned model's F-score. ","e0b4ee57":"### Choosing the Best Model\n\nFrom our initial model evaluation and based on the resulting performance metrics for the three proposed learning models and inspecting the used dataset ,I could conclude that using <font color='green'><b>Logistic regression<\/b><\/font> is the right choice in our case because when using 100% of the training set (in the testing phase) it gives robust performance indicated by the <b>high accuracy<\/b> (about 0.84 ) and <b>moderate F-Score<\/b> (about 0.68) as well as its <b>resonable prediction<\/b> compared to the other two algorithms .Note, it looksthat AdBoost performs better but it takes long time to respond than others but since the diffierence in accuarcy and performance between it and logistic regression on this model is negliable we could ignore it and depend on logistic regression to get best results.","76f75715":"# Supervised Learning\n## By Yosry Negm\n\n<br><hr><br>\nIn this project, I will use several supervised learning algorithms to accurately model individuals' income using data collected from the 1994 U.S. Census. Then choose the best candidate algorithm from preliminary results and further optimize this algorithm to best model the data. The goal with this implementation is to construct a model that accurately predicts whether an individual makes more than $50,000. This sort of task can arise in a non-profit setting, where organizations survive on donations.  Understanding an individual's income can help a non-profit better understand how large of a donation to request, or whether or not they should reach out to begin with.  While it can be difficult to determine an individual's general income bracket directly from public sources, we can infer this value from other publically available features. \n\nThe dataset for this project originates from the [UCI Machine Learning Repository](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Census+Income). The datset was donated by Ron Kohavi and Barry Becker, after being published in the article _\"Scaling Up the Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybrid\"_. As in the article by Ron Kohavi [online](https:\/\/www.aaai.org\/Papers\/KDD\/1996\/KDD96-033.pdf). The data investigated here consists of small changes to the original dataset, such as removing the `'fnlwgt'` feature and records with missing or ill-formatted entries.","dfd0b81f":"### Data Preprocessing\n\nFrom the table in **Exploring the Data** above, we can see there are several features for each record that are non-numeric. Typically, learning algorithms expect input to be numeric, which requires that non-numeric features (called *categorical variables*) be converted. One popular way to convert categorical variables is by using the **one-hot encoding** scheme. One-hot encoding creates a _\"dummy\"_ variable for each possible category of each non-numeric feature. For example, assume `someFeature` has three possible entries: `A`, `B`, or `C`. We then encode this feature into `someFeature_A`, `someFeature_B` and `someFeature_C`.\n\n|   | someFeature |                    | someFeature_A | someFeature_B | someFeature_C |\n| :-: | :-: |                            | :-: | :-: | :-: |\n| 0 |  B  |  | 0 | 1 | 0 |\n| 1 |  C  | ----> one-hot encode ----> | 0 | 0 | 1 |\n| 2 |  A  |  | 1 | 0 | 0 |\n\nAdditionally, as with the non-numeric features, we need to convert the non-numeric target label, `'income'` to numerical values for the learning algorithm to work. Since there are only two possible categories for this label (\"<=50K\" and \">50K\"), we can avoid using one-hot encoding and simply encode these two categories as `0` and `1`, respectively. In code below, we will implement the following:\n - Use [`pandas.get_dummies()`](http:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.get_dummies.html?highlight=get_dummies#pandas.get_dummies) to perform one-hot encoding on the `'features_log_minmax_transform'` data.\n - Convert the target label `'income_raw'` to numerical entries.\n   - Set records with \"<=50K\" to `0` and records with \">50K\" to `1`.","9c893878":"### Effects of Feature Selection\n\nIt appears that reducing No. of features will in turn reduce the performance of the model.As we noted that the Accuracy and F-score values of the reduced features is less than when the model including all the features and I propose that we should work on the entire features set unless we are working in quick response applications in which training time is more important than accuracy.Here I think this less important features reduction will also affect on the ability of the model to generalize well and if performance doesn't matter we could proceed with that otherwise no.Finally, we could say it is a Time\/Space Trade-Off.\n","bdeb0465":"#### Note: Recap of accuracy, precision, recall\n\n** <font color=green>Accuracy<\/font> ** measures how often the classifier makes the correct prediction. It\u2019s the ratio of the number of correct predictions to the total number of predictions (the number of test data points).\n\n** <font color=green>Precision<\/font> ** tells us what proportion of messages we classified as spam, actually were spam.\nIt is a ratio of true positives(words classified as spam, and which are actually spam) to all positives(all words classified as spam, irrespective of whether that was the correct classificatio), in other words it is the ratio of\n\n`[True Positives\/(True Positives + False Positives)]`\n\n** <font color=green>Recall (sensitivity)<\/font> ** tells us what proportion of messages that actually were spam were classified by us as spam.\nIt is a ratio of true positives(words classified as spam, and which are actually spam) to all the words that were actually spam, in other words it is the ratio of\n\n`[True Positives\/(True Positives + False Negatives)]`\n\nFor classification problems that are skewed in their classification distributions like in our case, for example if we had a 100 text messages and only 2 were spam and the rest 98 weren't, accuracy by itself is not a very good metric. We could classify 90 messages as not spam(including the 2 that were spam but we classify them as not spam, hence they would be false negatives) and 10 as spam(all 10 false positives) and still get a reasonably good accuracy score. For such cases, precision and recall come in very handy. These two metrics can be combined to get the F1 score, which is weighted average(harmonic mean) of the precision and recall scores. This score can range from 0 to 1, with 1 being the best possible F1 score(we take the harmonic mean as we are dealing with ratios).","533e70bf":" Therefore, a model's ability to precisely predict those that make more than *$50,000* is *more important* than the model's ability to **recall** those individuals. We can use <font color=green>F-beta score<\/font> as a metric that considers both <font color=green>precision<\/font> and <font color=green>recall<\/font>:","339464ad":"----\n## Evaluating Model Performance\nIn this section, I will investigate four different algorithms, and determine which is best at modeling the data. ","9fa1405f":"> **Note**: Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an HTML document. You can do this by using the menu above and navigating to  \n**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission.","68fb543e":"### Transforming Skewed Continuous Features\nA dataset may sometimes contain at least one feature whose values tend to lie near a single number, but will also have a non-trivial number of vastly larger or smaller values than that single number.  Algorithms can be sensitive to such distributions of values and can underperform if the range is not properly normalized. With the census dataset two features fit this description: '`capital-gain'` and `'capital-loss'`. \n\nThe code below plots a histogram of these two features. Note the range of the values present and how they are distributed.","1380dcaf":"### Metrics and the Naive Predictor\n\nWe assume that our imaginary charity non-profit organization, equipped with their research knows individuals that make more than \\$50,000 are most likely to donate to their charity. \nLet us also suppose that our virtual organization is particularly interested in predicting who makes more than *$50,000* accurately. It would seem that using <font color=green>accuracy<\/font> as a metric for evaluating a particular model's performace would be appropriate. ","2a1e590c":"### Model Tuning\nNow I will fine tune the chosen model using grid search (`GridSearchCV`) with at least one important parameter tuned with at least 3 different values. I will use the entire training set for this. In the code below, I implemented the following:\n- Import [`sklearn.grid_search.GridSearchCV`](http:\/\/scikit-learn.org\/0.17\/modules\/generated\/sklearn.grid_search.GridSearchCV.html) and [`sklearn.metrics.make_scorer`](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.make_scorer.html).\n- Initialize the classifier you've chosen and store it in `clf`.\n - Set a `random_state` if one is available to the same state you set before.\n- Create a dictionary of parameters you wish to tune for the chosen model.\n - Example: `parameters = {'parameter' : [list of values]}`.\n - **Note:** Avoid tuning the `max_features` parameter of your learner if that parameter is available!\n- Use `make_scorer` to create an `fbeta_score` scoring object (with $\\beta = 0.5$).\n- Perform grid search on the classifier `clf` using the `'scorer'`, and store it in `grid_obj`.\n- Fit the grid search object to the training data (`X_train`, `y_train`), and store it in `grid_fit`.","a88c4abe":"----\n## Preparing the Data\nBefore data can be used as input for machine learning algorithms, it often must be cleaned, formatted, and restructured \u2014 this is typically known as **preprocessing**. Fortunately, for this dataset, there are no invalid or missing entries we must deal with, however, there are some qualities about certain features that must be adjusted. This preprocessing can help tremendously with the outcome and predictive power of nearly all learning algorithms.","56b85640":"### Describing the Model in Layman's Terms\n\nwe will use the <font color='green'><b>Logistic Regression<\/font><\/b> algorithm which is a <b>classification<\/b> algorithm for analyzing a dataset in which there are one or more nominal, ordinal, interval or ratio-level independent variables(<b>Features<\/b>) that determine an outcome which in turn is a <b>binary dependent variable<\/b> which has two possible values(can be generalized to more i.e. is categorical),often labelled as \"0\" and \"1\" to represent outcomes such as pass\/fail, win\/lose, alive\/dead or healthy\/sick and specifically, in our case <u> \"More than 50k Dollars\" or not <\/u> or in other words <b> \"may Donate\" or may not\" <\/b> .You can think of logistic regression as a special case of linear regression when the outcome  variable is <i>categorical<\/i>, where we are <b>predicting the probability<\/b> of occurrence of an event by fitting data to a certain function i.e. To predict which class a data belongs,a threshold can be set. Based upon this threshold,the obtained estimated probability is classified into classes.Say, if predicted_value \u2265 50k dollars,then classify \"May Donate\" else \"may not \". Decision boundary can be linear or nonlinear(Polynomial order can be increased to get complex decision boundary). We could accomplish this by using a mathematical function called the logistic function(sigmoid function), it can take any real-valued number and map it into a value between 0 and 1.Making predictions with a logistic regression model is as simple\nas inputting in numbers into the logistic regression function and calculating a result.","7d7d4931":"### Creating a Training and Predicting Pipeline\nTo properly evaluate the performance of each model I've chosen, it's important that we create a training and predicting pipeline that allows us to quickly and effectively train models using various sizes of training data and perform predictions on the testing data. our implementation here will be used in the following section.\nIn the code block below, we will need to implement the following:\n - Import `fbeta_score` and `accuracy_score` from [`sklearn.metrics`](http:\/\/scikit-learn.org\/stable\/modules\/classes.html#sklearn-metrics-metrics).\n - Fit the learner to the sampled training data and record the training time.\n - Perform predictions on the test data `X_test`, and also on the first 300 training points `X_train[:300]`.\n   - Record the total prediction time.\n - Calculate the accuracy score for both the training subset and testing set.\n - Calculate the F-score for both the training subset and testing set.\n   - Make sure that you set the `beta` parameter!","7954d18a":"### Final Model Evaluation\n\n#### Results:\n\n|     Metric     | Unoptimized Model | Optimized Model |\n| :------------: | :---------------: | :-------------: | \n| Accuracy Score |       0.8419      |     0.8420      |\n| F-score        |       0.6832      |     0.6842      |\n\n\nFrom the above result we could note that the <b><font color='green'>optimized<\/font><\/b> values of Accuracy (<b><font color='green'>0.8420<\/font><\/b>) and F-score (<b><font color='green'>0.6842<\/font><\/b>) for our selected model is<b> enhanced<\/b> a little bit and it is <b>better<\/b> than the <b><font color='red'>unoptimized<\/font><\/b> model's values of Accuracy (<b><font color='red'>0.8419<\/font><\/b>) and F-score (<b><font color='red'>0.6832<\/font><\/b>).Also we could note that they are <b><font color='blue'>greatly imporoved<\/font><\/b> compared to the Naive Byes Classifer used in Question 1 (benchmark predictor) which were Accuracy (<b><font color='blue'>0.2478<\/font><\/b>) and F-score (<b><font color='blue'>0.2917<\/font><\/b>),we could conclude from that:<i>it is useful to always optimize your model<\/i>.","d4d063ab":"----\n## Exploring the Data\nThe code below will load necessary Python libraries and load the census data. Note that the last column from this dataset, `'income'`, will be our target label (whether an individual makes more than, or at most, $50,000 annually). All other columns are features about each individual in the census database.","5ae25b13":"A cursory investigation of the dataset will determine how many individuals fit into either group, and will tell us about the percentage of these individuals making more than \\$50,000. The code below will compute the following:\n- The total number of records, `'n_records'`\n- The number of individuals making more than \\$50,000 annually, `'n_greater_50k'`.\n- The number of individuals making at most \\$50,000 annually, `'n_at_most_50k'`.\n- The percentage of individuals making more than \\$50,000 annually, `'greater_percent'`.","237528b3":"### Normalizing Numerical Features\nIn addition to performing transformations on features that are highly skewed, it is often good practice to perform some type of scaling on numerical features. Applying a scaling to the data does not change the shape of each feature's distribution (such as `'capital-gain'` or `'capital-loss'` above); however, normalization ensures that each feature is treated equally when applying supervised learners. Note that once scaling is applied, observing the data in its raw form will no longer have the same original meaning, as exampled below.\n\nThe code below will normalize each numerical feature. I will use [`sklearn.preprocessing.MinMaxScaler`](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.MinMaxScaler.html) for this.","911d2c93":"----\n## Feature Importance\n\nAn important task when performing supervised learning on a dataset like the census data we study here is determining which features provide the most predictive power. By focusing on the relationship between only a few crucial features and the target label we simplify our understanding of the phenomenon, which is most always a useful thing to do. In the case of this project, that means we wish to identify a small number of features that most strongly predict whether an individual makes at most or more than \\$50,000.\n\nI've Chosen a scikit-learn classifier that has a `feature_importance_` attribute, which is a function that ranks the importance of features according to the chosen classifier.  In the next code  fiting this classifier to training set and use this attribute to determine the top 5 most important features for the census dataset.\n\n### Feature Relevance Observation\n\nAfter looking back at data exploration section, I think the most significant five feature for our model prediction are <b>in order<\/b> as following :-\n1. <b>Education<\/b> : we could think about well educated person may have good income that allow him to donate and also he more aware of the importance of donation rather than less educated one.\n2. <b>Capital Gain<\/b> : this reflects his profit and hence the income ,if it is high this may encourage the person to donate freely.\n3. <b>Capital Loss<\/b> : this feature representing a financial issues which may lead to lower income and block the person from donation. \n4. <b>Occupation<\/b> : Notably the income is depending on the persons field of work and hence his ability to donate either by plenty of money or the culture related to his work environment.\n5. <b>Age<\/b> : I think Mature people tends to donate more than younger ones ,the are wise and have enough experience to valuate the importance of donation.  ","6a55dc8d":"### Naive Predictor Performace\n* If we chose a model that always predicted an individual made more than $50,000, what would  that model's accuracy and F-score be on this dataset? The code below assigns values to `'accuracy'` and `'fscore'` to be used later.\n\n** note ** that the the purpose of generating a naive predictor is simply to show what a base model without any intelligence would look like. In the real world, ideally our base model would be either the results of a previous model or could be based on a research paper upon which we are looking to improve. When there is no benchmark model set, getting a result better than random choice is a place we could start from.\n\n* When we have a model that always predicts '1' (i.e. the individual makes more than 50k) then our model will have no True Negatives(TN) or False Negatives(FN) as we are not making any negative('0' value) predictions. Therefore our Accuracy in this case becomes the same as our Precision(True Positives\/(True Positives + False Positives)) as every prediction that we have made with value '1' that should have '0' becomes a False Positive; therefore our denominator in this case is the total number of records we have in total. \n* Our Recall score(True Positives\/(True Positives + False Negatives)) in this setting becomes 1 as we have no False Negatives.","135e72ea":"###  Supervised Learning Models\n**The following are some of the supervised learning models that are currently available in** [`scikit-learn`](http:\/\/scikit-learn.org\/stable\/supervised_learning.html) **that you may choose from:**\n- Gaussian Naive Bayes (GaussianNB)\n- Decision Trees\n- Ensemble Methods (Bagging, AdaBoost, Random Forest, Gradient Boosting)\n- K-Nearest Neighbors (KNeighbors)\n- Stochastic Gradient Descent Classifier (SGDC)\n- Support Vector Machines (SVM)\n- Logistic Regression","91ba6ec3":"### Initial Model Evaluation\nIn the code below, we are implementing the following:\n- Import the three supervised learning models I've discussed in the previous section.\n- Initialize the three models and store them in `'clf_A'`, `'clf_B'`, and `'clf_C'`.\n  - Use a `'random_state'` for each model you use, if provided.\n  - **Note:** Use the default settings for each model \u2014 you will tune one specific model in a later section.\n- Calculate the number of records equal to 1%, 10%, and 100% of the training data.\n  - Store those values in `'samples_1'`, `'samples_10'`, and `'samples_100'` respectively.","15a4e301":"For highly-skewed feature distributions such as `'capital-gain'` and `'capital-loss'`, it is common practice to apply a <a href=\"https:\/\/en.wikipedia.org\/wiki\/Data_transformation_(statistics)\">logarithmic transformation<\/a> on the data so that the very large and very small values do not negatively affect the performance of a learning algorithm. Using a logarithmic transformation significantly reduces the range of values caused by outliers. Care must be taken when applying this transformation however: The logarithm of `0` is undefined, so we must translate the values by a small amount above `0` to apply the the logarithm successfully.\n\nThe code below performs a transformation on the data and visualize the results. Again, note the range of values and how they are distributed. ","6c4567d1":"### Shuffle and Split Data\nNow all _categorical variables_ have been converted into numerical features, and all numerical features have been normalized. As always, we will now split the data (both features and their labels) into training and test sets. 80% of the data will be used for training and 20% for testing.\n\nThe code below performs this split.","2b85a37f":"$$ F_{\\beta} = (1 + \\beta^2) \\cdot \\frac{precision \\cdot recall}{\\left( \\beta^2 \\cdot precision \\right) + recall} $$\n\nIn particular, when $\\beta = 0.5$, more emphasis is placed on precision. This is called the **F$_{0.5}$ score** (or F-score for simplicity).","716641d1":"** Featureset Exploration **\n\n* **age**: continuous. \n* **workclass**: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked. \n* **education**: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool. \n* **education-num**: continuous. \n* **marital-status**: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse. \n* **occupation**: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces. \n* **relationship**: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried. \n* **race**: Black, White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other. \n* **sex**: Female, Male. \n* **capital-gain**: continuous. \n* **capital-loss**: continuous. \n* **hours-per-week**: continuous. \n* **native-country**: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.","9f8979a3":"### Model Application\n\n**after investigating the underlying data in this project and trying to figure out the suitable model to fullfill the requirements asked by our charity non-profit organization I could propose that the following three algorithms may be good candidates to build the required model:-**\n\n## (A) Logistic Regression :\n- Most common algorithm which could be used as binary classifier in a variety of real world applications such as detecting wether a patient is subject to lung cancer or not based on some information from his medical records. Also, could be used to predict heart attack (expected or not ) based on the patient's medical history. refer to [this link](https:\/\/hub.packtpub.com\/healthcare-analytics-logistic-regression-to-reduce-patient-readmissions\/) and [this](http:\/\/www.iaeng.org\/IJAM\/issues_v40\/issue_2\/IJAM_40_2_08.pdf) for a practical medical application to logistic regression.  \n- Its strength resides in being <b>straightforward<\/b> to understand and explain,<b>can be regularized to avoid overfitting<\/b> nd its tendency to give <b>calibrated predictions<\/b>. Also,because the gradient is sparse (or has a sparse approximation in the regularized case), it is a good fit for domains with sparse features such we are using as well as it has Outputs which give a <b>nice probabilistic interpretation<\/b> \n- one of the weakness of the logistic regression algorithm is its tendency to perform poorly when there are<b> multiple<\/b> or <b>nonlinear decision boundaries<\/b>.it is not flexible enough to naturally capture more<b>complex relationships<\/b>. check [this link](http:\/\/www.academicstar.us\/UploadFile\/Picture\/2014-6\/201461494819669.pdf) for more info.\n- Reasons for nominate logistic regression to model our problem are that we have clean data and we require binary classification to be performed on it as well as it could give reasonable response in a reasonable time.\n\n## (B) Ensemble Methods - AdaBoost  :\n- as a real life example of using ensemble methods is to use AdaBoost algorithm to analyze basketball game i.e. to recognize face and body parts for player detection during basketball games on a video footage obtained from the single moving camera. ([check this link](https:\/\/www.uni-obuda.hu\/journal\/Markoski_Ivankovic_Ratgeber_Pecev_Glusac_57.pdf))  \n- Its strength that it is <b>fast and very adaptive<\/b> classification algorithm and <b>less prone to overfitting<\/b>.It is  valid for ery complex decision boundaries.\n- its weaknesse is that it is slow in training and tends to be sensitive for noisy data and outliers.\n- Reasons for suggesting this algorithm for the current problem are that our dataset is clean and contains 45222 data points and this could be considered reaonable for that algorithm if we not considered the long training time and slowness.\n\n## (C) Decision Trees :\n- Decision tree algorithm could be used in IT security Intrusion Detection system (<b>IDS<\/b>) that monitors a network for malicious activities or policy violations. ([check this link](https:\/\/www.researchgate.net\/publication\/312296143_AN_APPLICATION_OF_DECISION_TREES_IN_INTRUSION_DETECTION))  \n- Its strength that it is able to handle <b>categorical and numerical<\/b> data, help <b>avoiding irrelevant features<\/b> by using information gain,fairly robust to outliers,Doesn\u2019t require much data pre-processing,can handle data which hasn\u2019t been normalized ,reduce time complexity and could be utilized in nonlinear relationships. In addition, it is Easy to use ,interpret and visualise the decision boundary.\n- its weaknesses are that it is prone to overfitting if there are too many features (they can keep branching until they memorize the training data) or is prone to underfitting if there is too little data ,hence complex decision trees do not generalize well.Also it is unstable, as small variations in the data can result in a different decision tree.as well as it can create biased trees if some classes dominate.\n- Reason for being candidate for our problem is to make the process fast since decision tree doesn't require specific domain knowledge.","397108d6":"### Feature Selection\nHow does a model perform if we only use a subset of all the available features in the data? With less features required to train, the expectation is that training and prediction time is much lower \u2014 at the cost of performance metrics. From the visualization above, we see that the top five most important features contribute more than half of the importance of **all** features present in the data. This hints that we can attempt to *reduce the feature space* and simplify the information required for the model to learn. The code below uses the same optimized model we found earlier, and train it on the same training set *with only the top five important features*. ","f932a5c5":"### Extracting Feature Importance\n\nObserving the visualization created above which displays the five most relevant features for predicting if an individual makes at most or above \\$50,000.  \nI've missed <b>only<\/b> the \"hours-per-week\" in place of \"Capital-Gain\" and I think they are related some how and have the same effect i.e as \"hours-per-week\" increases this means more work that leads to more \"Capital gain\" and hence more  income and vice versa, so they are equal in importance and one of them enough.\n<b>Another difference<\/b> in my answer is the rank of feature \"Age\" ,I put it at last but the visualization put it before last,and again in my point of view \"Age\" is less effective on donation tendency than Capital Loss\/Gain but \"Age\" of course affects income (here's the point). <b>the remaining features and ranking I've made the same as the algorithm had made<\/b>.It Looks like that the algoritms was matching my thinking in somehow.","1503de00":" This can greatly affect <font color=green>accuracy<\/font>, since we could simply say \"this person does not make more than \\$50,000\" and generally be right, without ever looking at the data! Making such a statement would be called <font color=green>naive<\/font>, since we have not considered any information to substantiate the claim. It is always important to consider the <font color=green>naive prediction<\/font> for our data, to help establish a benchmark for whether a model is performing well. That been said, using that prediction would be pointless: If we predicted all people made less than $50,000, imaginary charity non-profit organization would identify no one as donors."}}