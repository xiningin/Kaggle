{"cell_type":{"ba7e6b73":"code","93263913":"code","725086ee":"code","224f5c2f":"code","239cc31b":"code","b126452e":"code","8bbf388f":"code","b09a5553":"code","640013a9":"code","c5309716":"code","5448a497":"code","950979b6":"code","4c01d1fb":"code","9c1b2c17":"code","5e2ceb4d":"code","82120ee0":"code","de793e11":"code","299154ba":"code","b82544d3":"code","12461d84":"code","5238faac":"code","996283e7":"code","c9501966":"code","cb5ea5fa":"code","60bc943a":"code","18e0b1c4":"code","124ddff9":"code","f8a02af6":"code","574e8964":"code","37b1d9e4":"code","e633bf0f":"code","910a34ac":"code","bcfe2784":"code","2676271c":"code","c7564d48":"code","91aa2e64":"code","b75cb04d":"code","67f219b8":"code","d002fc23":"code","13bd23d2":"code","d4899ae9":"code","dbb20661":"code","831132c7":"code","aa900399":"code","270d83d8":"code","70b91ba0":"code","4924fa9b":"code","ce70606c":"code","83326019":"code","006d5cff":"code","ac8a0213":"code","0aac8e9c":"code","50a88314":"markdown"},"source":{"ba7e6b73":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","93263913":"df = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/train.csv')","725086ee":"df.head(5)","224f5c2f":"df.describe()","239cc31b":"df.isnull().sum()","b126452e":"df = df.drop(['id', 'url_legal', 'license'], axis = 1)","8bbf388f":"df.head()","b09a5553":"df['excerpt'][0]","640013a9":"# remove the numbers\n# tokenise the excerpt \n# remove the stopwords\n# remove the contractions\n# lemitization","c5309716":"# remove the numbers\ndf['excerpt'] = df['excerpt'].apply(lambda x: x.replace('\\d+', ''))","5448a497":"df['excerpt'][0]","950979b6":"# tokenise the excerpt\nfrom nltk.tokenize import RegexpTokenizer\ntokenizer=RegexpTokenizer(r'\\w+')","4c01d1fb":"df['excerpt']=df['excerpt'].apply(lambda x:tokenizer.tokenize(x.lower()))","9c1b2c17":"#remove the stopwords\n!pip install nltk\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstop_words  = set(stopwords.words('english'))","5e2ceb4d":"df['excerpt'] = df['excerpt'].apply(lambda x:[item for item in x if item not in stop_words])","82120ee0":"df['excerpt'][0]","de793e11":"# remove the contractions\n!pip install contractions\nimport contractions","299154ba":"df['excerpt'] = df['excerpt'].apply(lambda x: [contractions.fix(word) for word in x])","b82544d3":"df['excerpt'][0]","12461d84":"# lemitization","5238faac":"from nltk.stem import WordNetLemmatizer\nlemmatizer=WordNetLemmatizer()","996283e7":"def word_lemmatizer(text):\n    lem_text=' '.join([lemmatizer.lemmatize(i) for i in text])\n    return lem_text","c9501966":"df['excerpt']=df['excerpt'].apply(lambda x:word_lemmatizer(x))","cb5ea5fa":"df.head(5)","60bc943a":"x_train = df['excerpt']\ny_train = df['target']","18e0b1c4":"print(x_train.shape)\ny_train.shape","124ddff9":"test_df = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/test.csv')","f8a02af6":"test_df.head()","574e8964":"x_test = test_df['excerpt']","37b1d9e4":"# Linear Regreesion\n# Bayesian Regression\n# Support Vector Machine\n# Nearest neighbour regression\n# Decision Tree Regressor","e633bf0f":"import joblib","910a34ac":"# Linear Regreesion\nfrom sklearn import linear_model\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_extraction.text import TfidfVectorizer\npipeline_1 = Pipeline([\n    ('vectorizer', TfidfVectorizer(max_df=0.85, norm='l2')),\n    ('ligreg', linear_model.LinearRegression()),\n])","bcfe2784":"pipeline_1.fit(x_train, y_train)","2676271c":"#save the pipeline\nfilename = 'submission.csv'\njoblib.dump(pipeline_1, filename)","c7564d48":"model_1 = joblib.load('submission.csv')\ny_pred = model_1.predict(x_test)\nprint(y_pred)","91aa2e64":"from sklearn.base import TransformerMixin\nclass DenseTransformer(TransformerMixin):\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def transform(self, X, y=None, **fit_params):\n        return X.todense()","b75cb04d":"# Bayesian Regression\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.feature_extraction.text import TfidfVectorizer\npipeline_2 = Pipeline([\n    ('vectorizer', TfidfVectorizer(max_df=0.85, norm='l2')),\n    ('to_dense', DenseTransformer()),\n    ('basreg', linear_model.BayesianRidge()),\n])","67f219b8":"pipeline_2.fit(x_train, y_train)","d002fc23":"#save the pipeline\nfilename = 'pipeline_2.sav'\njoblib.dump(pipeline_2, filename)","13bd23d2":"model_2 = joblib.load('pipeline_2.sav')\ny_pred_2 = model_2.predict(x_test)\nprint(y_pred_2)","d4899ae9":"# Support Vector Machine\nfrom sklearn import svm\nfrom sklearn.svm import LinearSVR\npipeline_3 = Pipeline([\n    ('vectorizer', TfidfVectorizer(max_df=0.85, norm='l2')),\n    ('LinearSvr', svm.LinearSVR()),\n])","dbb20661":"pipeline_3.fit(x_train, y_train)","831132c7":"#save the pipeline\nfilename = 'pipeline_3.sav'\njoblib.dump(pipeline_3, filename)","aa900399":"model_3 = joblib.load('pipeline_3.sav')\ny_pred_3 = model_3.predict(x_test)\nprint(y_pred_3)","270d83d8":"# Nearest neighbour regression\nfrom sklearn import neighbors\nfrom sklearn.neighbors import KNeighborsRegressor\npipeline_4 = Pipeline([\n    ('vectorizer', TfidfVectorizer(max_df=0.85, norm='l2')),\n    ('KNN', neighbors.KNeighborsRegressor()),\n])","70b91ba0":"pipeline_4.fit(x_train, y_train)","4924fa9b":"#save the pipeline\nfilename = 'pipeline_4.sav'\njoblib.dump(pipeline_4, filename)","ce70606c":"model_4 = joblib.load('pipeline_4.sav')\ny_pred_4 = model_4.predict(x_test)\nprint(y_pred_4)","83326019":"# Decision Tree Regressor\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeRegressor\npipeline_5 = Pipeline([\n    ('vectorizer', TfidfVectorizer(max_df=0.85, norm='l2')),\n    ('DTR', tree.DecisionTreeRegressor()),\n])","006d5cff":"pipeline_5.fit(x_train, y_train)","ac8a0213":"#save the pipeline\nfilename = 'pipeline_5.sav'\njoblib.dump(pipeline_5, filename)","0aac8e9c":"model_5 = joblib.load('pipeline_5.sav')\ny_pred_5 = model_5.predict(x_test)\nprint(y_pred_5)","50a88314":"We will drop the first three columns"}}