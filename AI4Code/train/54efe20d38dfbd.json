{"cell_type":{"9b96a340":"code","dd62c617":"code","f900aba0":"code","1cec601c":"code","b7c00c3f":"code","c37ee64f":"code","460003f3":"code","d2ba9880":"code","59e372e4":"code","7633b52d":"code","4c718562":"code","1a172c71":"code","56a6d289":"code","618d96bb":"code","6c342870":"code","10d3b0b4":"code","4bd8d4ea":"code","cb7536d4":"code","3a392f12":"code","604b8f70":"code","c8e807a8":"code","d6f4137e":"code","ca2132b1":"code","10263a1d":"code","fbe67e51":"code","29188691":"code","83f5c1e7":"code","27dcc837":"code","57b9bdbf":"code","0be06775":"markdown","adcddf08":"markdown","f1783c56":"markdown","52af944f":"markdown","3676de62":"markdown","7d9ae907":"markdown","0c082c41":"markdown","54b0e2c5":"markdown"},"source":{"9b96a340":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dd62c617":"import plotly.express as px\nimport pandas as pd\nfrom IPython.display import display\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, median_absolute_error, mean_squared_error,accuracy_score\nfrom xgboost import XGBRegressor","f900aba0":"df_train = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ndf_test = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\ndf_train_scored = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ndf_train_unscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')","1cec601c":"df_train[:6]","b7c00c3f":"df_train['cp_type'].value_counts()","c37ee64f":"df_train_scored[:5]","460003f3":"df_train_unscored[:6]","d2ba9880":"from pandas_profiling import ProfileReport","59e372e4":"pd.set_option('display.max_columns', None)","7633b52d":"df_train.head(5)","4c718562":"ProfileReport(df_train.iloc[:,4:776],minimal =True)","1a172c71":"ProfileReport(df_train.iloc[:,776:],minimal =True)","56a6d289":"\ndf_train.cp_type.value_counts(normalize=True)","618d96bb":"df_train.cp_time.value_counts(normalize=True)","6c342870":"df_train.cp_dose.value_counts(normalize=True)","10d3b0b4":"#### Target values distribution\u00b6\n","4bd8d4ea":"plt.hist(df_train_scored.mean())\nplt.title('Distribution of mean target in each target column');","cb7536d4":"df_train_scored.describe()","3a392f12":"df_train_scored.mean().min(),df_train_scored.mean().max(),df_train_scored.mean().mean()\n#We have a high imbalance as  the max target rate is 0.03, the min is very low","604b8f70":"df_train[:2]","c8e807a8":"plt.plot(df_train.loc[df_train['sig_id'] == 'id_000644bb2',[col for col in df_train if 'g-' in col]].values.reshape(-1,1));\nplt.title('g- value of id_000644bb2');","d6f4137e":"plt.plot(sorted(df_train.loc[df_train['sig_id'] == 'id_000644bb2', [col for col in df_train if 'g-' in col]].values.reshape(-1, 1)))\nplt.title('sorted g- value of id_000644bb2');","ca2132b1":"#Sample\nsample = pd.read_csv(\"..\/input\/lish-moa\/sample_submission.csv\")\n\n#Test\ntest_features = pd.read_csv(\"..\/input\/lish-moa\/test_features.csv\",index_col='sig_id')\n\n#Train\ntrain_features = pd.read_csv(\"..\/input\/lish-moa\/train_features.csv\",index_col='sig_id')\ntrain_nonscore = pd.read_csv(\"..\/input\/lish-moa\/train_targets_nonscored.csv\",index_col='sig_id')\ntrain_score = pd.read_csv(\"..\/input\/lish-moa\/train_targets_scored.csv\",index_col='sig_id')","10263a1d":"g_features = [feature for feature in train_features.columns if feature.startswith('g-')]\nc_features = [feature for feature in train_features.columns if feature.startswith('c-')]\nother_features = [feature for feature in train_features.columns if feature not in g_features and feature not in c_features]\n                                                            \n\nprint(f'Number of g- Features: {len(g_features)}')\nprint(f'Number of c- Features: {len(c_features)}')\nprint(f'Number of other Features: {len(other_features)} ({other_features})')","fbe67e51":"cols = train_score.columns\nsubmission = pd.DataFrame({'sig_id': test_features.index})\ntotal_loss = 0\n\nSEED = 42","29188691":"y = train_score[column]\nprint(y)","83f5c1e7":"for c, column in enumerate(cols,1):\n    \n    y = train_score[column]\n    \n    # Split\n    X_train, X_valid, y_train, y_valid = train_test_split(train_features, y, train_size=0.9, test_size=0.1, random_state=SEED)\n   \n    X_test = test_features.copy()\n\n    # One-hot encoding\n    X_train = pd.get_dummies(X_train)\n    X_test = pd.get_dummies(X_test)\n    X_valid = pd.get_dummies(X_valid)\n    \n    X_train, X_test = X_train.align(X_test, join='left', axis=1)\n    X_train, X_valid = X_train.align(X_valid, join='left', axis=1)\n    \n    model = XGBRegressor(\n                         tree_method = 'gpu_hist',\n                         min_child_weight = 31.580,\n                         learning_rate = 0.055,\n                         colsample_bytree = 0.655,\n                         gamma = 3.705,\n                         max_delta_step = 2.080,\n                         max_depth = 25,\n                         n_estimators = 170,\n                         #subsample =  0.864, \n                         subsample =  0.910,\n                         booster='dart',\n                         validate_parameters = True,\n                         grow_policy = 'depthwise',\n                         predictor = 'gpu_predictor'\n                              \n                        )\n                        \n    # Train Model\n    model.fit(X_train, y_train)\n    pred = model.predict(X_valid)\n    \n    \n    # Loss\n    mae = mean_absolute_error(y_valid,pred)\n    mdae = median_absolute_error(y_valid,pred)\n    mse = mean_squared_error(y_valid,pred)\n    \n    total_loss += mae\n    \n    # Prediction\n    predictions = model.predict(X_test)\n    submission[column] = predictions\n    \n    print(\"Regressing through col-\"+str(c)+\", Mean Abs Error: \"+str(mae)+\", Median Abs Error: \"+str(mdae)+\", Mean Sqrd Error: \"+str(mse))\n    \n    ","27dcc837":"submission.to_csv('submission.csv', index=False)","57b9bdbf":"print(\"Loss: \", total_loss\/206)","0be06775":"## Pandas Profiling:\n### The display.max_columns option controls the number of columns to be printed.It receives an int or None (to print all the columns):","adcddf08":"\n\n### Data Description\n\nIn this competition, you will be predicting multiple targets of the Mechanism of Action (MoA) response(s) of different samples (sig_id), given various inputs such as gene expression data and cell viability data.\n\nTwo notes:\n\nThe training data has an additional (optional) set of MoA labels that are not included in the test data and not used for scoring.\nThe re-run dataset has approximately 4x the number of examples seen in the Public test.\n\n### Files\n\n**train_features.csv** - Features for the training set. Features g- signify gene expression data, and c- signify cell viability data. cp_type indicates samples treated with a compound (cp_vehicle) or with a control perturbation (ctrl_vehicle); control perturbations have no MoAs; cp_time and cp_dose indicate treatment duration (24, 48, 72 hours) and dose (high or low).\ntrain_targets_scored.csv - The binary MoA targets that are scored.\ntrain_targets_nonscored.csv - Additional (optional) binary MoA responses for the training data. These are not predicted nor scored.\ntest_features.csv - Features for the test data. You must predict the probability of each scored MoA for each row in the test data.\nsample_submission.csv - A submission file in the correct format.\n\n","f1783c56":"### Profile Report for Features g- which signify gene expression data","52af944f":"### Target values distribution\u00b6\n","3676de62":"#### G-307, 3200 distinct counts, 8383 are Zeros (35.2 % of Zeros) which has 13 % of unique values.\n\n#### G-707, 4730 distinct counts.\n#### G- 307, is the highest(35.2 % of zeros) and  G-707 is the 2nd highest(31% of zeros) \n","7d9ae907":"### Profile Report for Features c- signify cell viability data","0c082c41":"### Obervation: Minimum for c- signify cell viability feature is -10","54b0e2c5":"###\nFeatures\n\n    sig_id is the unique sample id\n    Features with g- prefix are gene expression features and there are 772 of them (from g-0 to g-771)\n    Features with c- prefix are cell viability features and there are 100 of them (from c-0 to g-99)\n    cp_type is a binary categorical feature which indicates the samples are treated with a compound or with a control perturbation (trt_cp or ctl_vehicle)\n    cp_time is a categorical feature which indicates the treatment duration (24, 48 or 72 hours)\n    cp_dose is a binary categorical feature which indicates the dose is low or high (D1 or D2)\n\n"}}