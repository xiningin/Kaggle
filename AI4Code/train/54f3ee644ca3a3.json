{"cell_type":{"b52e42f3":"code","2483478a":"code","b317bddd":"code","412be640":"code","409313d0":"code","b5fab1ea":"code","d5323225":"code","431344b5":"code","0e632b76":"code","3dc09f9e":"code","55cbdd76":"code","703a23e2":"code","29fdc1fc":"code","0fc229ff":"code","66d8bdb9":"code","8f585875":"markdown","ae6aa033":"markdown","900f7978":"markdown","794e9dce":"markdown","deba6529":"markdown","4e4f7705":"markdown","5bf767f0":"markdown"},"source":{"b52e42f3":"import os\nimport json\nimport math\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, KFold\nfrom typing import NamedTuple, Optional\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\ndef seed_everything(seed=2020):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything()    ","2483478a":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nTRAIN = \"..\/input\/stanford-covid-vaccine\/train.json\"\nTEST = \"..\/input\/stanford-covid-vaccine\/test.json\"\nSS = \"..\/input\/stanford-covid-vaccine\/sample_submission.csv\"\ntrain = pd.read_json(TRAIN, lines=True)\ntest = pd.read_json(TEST, lines=True)\nsample_sub = pd.read_csv(SS)\nprint(f\"Using {device}\")","b317bddd":"inp_seq_cols = ['sequence', 'structure', 'predicted_loop_type']\ntarget_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']\nvocab = {\n    'sequence': {x:i for i, x in enumerate(\"A C G U\".split())},\n    'structure': {x:i for i, x in enumerate(\"( . )\".split())},\n    'predicted_loop_type': {x:i for i, x in enumerate(\"B E H I M S X\".split())},\n}","412be640":"def preprocess_inputs(df, cols=inp_seq_cols):\n    \n    def f(x):\n        return [vocab['sequence'][x] for x in x[0]],\\\n                [vocab['structure'][x] for x in x[1]],\\\n                [vocab['predicted_loop_type'][x] for x in x[2]],\n\n    return np.array(\n            df[cols]\n            .apply(f, axis=1)\n            .values\n            .tolist()\n        )","409313d0":"train_filtered = train.loc[train.SN_filter == 1]\ntrain_inputs = torch.tensor(preprocess_inputs(train_filtered)).to(device)\nprint(\"input shape: \", train_inputs.shape)\n\ntrain_labels = torch.tensor(\n    np.array(\n        train_filtered[target_cols]\n        .values.tolist()\n    ).transpose(0, 2, 1)\n).float().to(device)\nprint(\"output shape: \", train_labels.shape)","b5fab1ea":"def split_last(x, shape):\n    \"split the last dimension to given shape\"\n    shape = list(shape)\n    assert shape.count(-1) <= 1\n    if -1 in shape:\n        shape[shape.index(-1)] = int(x.size(-1) \/ -np.prod(shape))\n    return x.view(*x.size()[:-1], *shape)\n\ndef merge_last(x, n_dims):\n    \"merge the last n_dims to a dimension\"\n    s = x.size()\n    assert n_dims > 1 and n_dims < len(s)\n    return x.view(*s[:-n_dims], -1)\n\nclass Config(NamedTuple):\n    \"Configuration for BERT model\"\n    dim: int = 768 # Dimension of Hidden Layer in Transformer Encoder\n    n_layers: int = 12 # Numher of Hidden Layers\n    n_heads: int = 12 # Numher of Heads in Multi-Headed Attention Layers\n    dim_ff: int = 768*4 # Dimension of Intermediate Layers in Positionwise Feedforward Net\n    #activ_fn: str = \"gelu\" # Non-linear Activation Function Type in Hidden Layers\n    p_drop_hidden: float = 0.1 # Probability of Dropout of various Hidden Layers\n    p_drop_attn: float = 0.1 # Probability of Dropout of Attention Layers\n    max_len: int = 130\n    n_bases: int = 4\n    n_structures: int = 3\n    n_loop: int = 7\n\n    @classmethod\n    def from_json(cls, file):\n        return cls(**json.load(open(file, \"r\")))\n\n\ndef gelu(x):\n    \"Implementation of the gelu activation function by Hugging Face\"\n    return x * 0.5 * (1.0 + torch.erf(x \/ math.sqrt(2.0)))\n\n\nclass LayerNorm(nn.Module):\n    \"A layernorm module in the TF style (epsilon inside the square root).\"\n    def __init__(self, cfg, variance_epsilon=1e-12):\n        super().__init__()\n        self.gamma = nn.Parameter(torch.ones(cfg.dim))\n        self.beta  = nn.Parameter(torch.zeros(cfg.dim))\n        self.variance_epsilon = variance_epsilon\n\n    def forward(self, x):\n        u = x.mean(-1, keepdim=True)\n        s = (x - u).pow(2).mean(-1, keepdim=True)\n        x = (x - u) \/ torch.sqrt(s + self.variance_epsilon)\n        return self.gamma * x + self.beta\n\n\nclass MultiHeadedSelfAttention(nn.Module):\n    \"\"\" Multi-Headed Dot Product Attention \"\"\"\n    def __init__(self, cfg):\n        super().__init__()\n        self.proj_q = nn.Linear(cfg.dim, cfg.dim)\n        self.proj_k = nn.Linear(cfg.dim, cfg.dim)\n        self.proj_v = nn.Linear(cfg.dim, cfg.dim)\n        self.drop = nn.Dropout(cfg.p_drop_attn)\n        self.scores = None # for visualization\n        self.n_heads = cfg.n_heads\n\n    def forward(self, x, mask):\n        \"\"\"\n        x, q(query), k(key), v(value) : (B(batch_size), S(seq_len), D(dim))\n        mask : (B(batch_size) x S(seq_len))\n        * split D(dim) into (H(n_heads), W(width of head)) ; D = H * W\n        \"\"\"\n        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n        q, k, v = self.proj_q(x), self.proj_k(x), self.proj_v(x)\n        q, k, v = (split_last(x, (self.n_heads, -1)).transpose(1, 2)\n                   for x in [q, k, v])\n        # (B, H, S, W) @ (B, H, W, S) -> (B, H, S, S) -softmax-> (B, H, S, S)\n        scores = q @ k.transpose(-2, -1) \/ np.sqrt(k.size(-1))\n        if mask is not None:\n            mask = mask[:, None, None, :].float()\n            scores -= 10000.0 * (1.0 - mask)\n        scores = self.drop(F.softmax(scores, dim=-1))\n        # (B, H, S, S) @ (B, H, S, W) -> (B, H, S, W) -trans-> (B, S, H, W)\n        h = (scores @ v).transpose(1, 2).contiguous()\n        # -merge-> (B, S, D)\n        h = merge_last(h, 2)\n        self.scores = scores\n        return h\n\n\nclass PositionWiseFeedForward(nn.Module):\n    \"\"\" FeedForward Neural Networks for each position \"\"\"\n    def __init__(self, cfg):\n        super().__init__()\n        self.fc1 = nn.Linear(cfg.dim, cfg.dim_ff)\n        self.fc2 = nn.Linear(cfg.dim_ff, cfg.dim)\n        #self.activ = lambda x: activ_fn(cfg.activ_fn, x)\n\n    def forward(self, x):\n        # (B, S, D) -> (B, S, D_ff) -> (B, S, D)\n        return self.fc2(gelu(self.fc1(x)))\n\n\nclass Block(nn.Module):\n    \"\"\" Transformer Block \"\"\"\n    def __init__(self, cfg):\n        super().__init__()\n        self.attn = MultiHeadedSelfAttention(cfg)\n        self.proj = nn.Linear(cfg.dim, cfg.dim)\n        self.norm1 = LayerNorm(cfg)\n        self.pwff = PositionWiseFeedForward(cfg)\n        self.norm2 = LayerNorm(cfg)\n        self.drop = nn.Dropout(cfg.p_drop_hidden)\n\n    def forward(self, x, mask=None):\n        h = self.attn(x, mask)\n        h = self.norm1(x + self.drop(self.proj(h)))\n        h = self.norm2(h + self.drop(self.pwff(h)))\n        return h\n    \ndef generate_original_PE(length: int, d_model: int) -> torch.Tensor:\n    \"\"\"Generate positional encoding as described in original paper.  :class:`torch.Tensor`\n    Parameters\n    ----------\n    length:\n        Time window length, i.e. K.\n    d_model:\n        Dimension of the model vector.\n    Returns\n    -------\n        Tensor of shape (K, d_model).\n    \"\"\"\n    PE = torch.zeros((length, d_model))\n\n    pos = torch.arange(length).unsqueeze(1)\n    PE[:, 0::2] = torch.sin(\n        pos \/ torch.pow(1000, torch.arange(0, d_model, 2, dtype=torch.float32)\/d_model))\n    PE[:, 1::2] = torch.cos(\n        pos \/ torch.pow(1000, torch.arange(1, d_model, 2, dtype=torch.float32)\/d_model))\n\n    return PE\n\n\ndef generate_regular_PE(length: int, d_model: int, period: Optional[int] = 24) -> torch.Tensor:\n    \"\"\"Generate positional encoding with a given period.\n    Parameters\n    ----------\n    length:\n        Time window length, i.e. K.\n    d_model:\n        Dimension of the model vector.\n    period:\n        Size of the pattern to repeat.\n        Default is 24.\n    Returns\n    -------\n        Tensor of shape (K, d_model).\n    \"\"\"\n    PE = torch.zeros((length, d_model))\n\n    pos = torch.arange(length, dtype=torch.float32).unsqueeze(1)\n    PE = torch.sin(pos * 2 * np.pi \/ period)\n    PE = PE.repeat((1, d_model))\n\n    return PE","d5323225":"class Embeddings(nn.Module):\n    \"\"\"Modified Embeddings for mRNA degradation\"\"\"\n    def __init__(self, cfg):\n        super().__init__()\n        \n        self.dim = cfg.dim\n        self.base_embed = nn.Embedding(cfg.n_bases, cfg.dim)\n        self.struct_embed = nn.Embedding(cfg.n_structures, cfg.dim)\n        self.loop_embed = nn.Embedding(cfg.n_loop, cfg.dim) \n        self.pos_embed = nn.Embedding(cfg.max_len, cfg.dim)\n        self.norm = LayerNorm(cfg)\n        self.drop = nn.Dropout(cfg.p_drop_hidden)\n        self.norm1 = LayerNorm(cfg)\n        self.norm2 = LayerNorm(cfg)\n        \n        \n        self.embed = nn.Embedding(84, cfg.dim)\n\n    def forward(self, x, flip=False):\n        seq_len = x.size(2)\n        base_seq, struct_seq, loop_seq = x[:, 0, :], x[:, 1, :], x[:, 2, :]\n        \n        if flip:\n            base_seq = base_seq.flip(1)\n            struct_seq = struct_seq.flip(1)\n            loop_seq = loop_seq.flip(1)\n        \n        pos = torch.arange(seq_len, dtype=torch.long, device=x.device)\n        pos = pos.unsqueeze(0).expand_as(base_seq) # (S,) -> (B, S)\n        pe = self.pos_embed(pos)\n        \n#         pe = generate_regular_PE(seq_len, self.dim).to(x.device)\n        \n        se = self.struct_embed(struct_seq)\n        le = self.loop_embed(loop_seq)\n        be = self.base_embed(base_seq)\n    \n        e =  self.norm1(be + se + le) + self.norm2(pe)\n#         print(f\"Embed: {e.shape}\")\n            \n        return self.drop(self.norm(e))\n  ","431344b5":"  \n    \nclass Transformer(nn.Module):\n    \"\"\" The BERT transformer with slight modifications \n        for the use-case of mRNA degradation\"\"\"\n    def __init__(self, cfg, pred_len=68):\n        super().__init__()\n        self.embed = Embeddings(cfg)\n        self.blocks = nn.ModuleList([Block(cfg) for _ in range(cfg.n_layers)])\n        self.pred_len = pred_len\n        self.out = nn.Linear(cfg.dim, 5)\n\n    def forward(self, x):\n        h = self.embed(x)\n        for block in self.blocks:\n            h = block(h)\n        truncated = h[: , :self.pred_len]\n        out = self.out(truncated)\n        return out    ","0e632b76":"config = \"\"\"\n{\n    \"dim\": 192,\n    \"dim_ff\": 384,\n    \"n_layers\": 6,\n    \"p_drop_attn\": 0.3,\n    \"n_heads\": 6,\n    \"p_drop_hidden\": 0.25,\n    \"n_bases\": 4,\n    \"n_structures\": 3,\n    \"n_loop\": 7,\n    \"max_len\": 130\n}\n\"\"\"\nCFILE = \"config.json\"\nwith open(CFILE, 'w') as handle:\n    handle.write(config)\n    \ncfg = Config().from_json(CFILE)","3dc09f9e":"def compute_loss(batch_X, batch_Y, model, optimizer=None, is_train=True, ret_pred=False):\n    \"\"\"custom MCRMSE\"\"\"\n    model.train(is_train)\n\n    pred_Y = model(batch_X)\n\n    loss = torch.pow(\n            torch.pow(\n            batch_Y-pred_Y, \n            2).mean(dim=1), \n            .5).mean()\n\n    if is_train:\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    if ret_pred:\n        return loss.item(), pred_Y\n    return loss.item()","55cbdd76":"FOLDS = 3\nEPOCHS = 120\nPRINT_FREQ = EPOCHS\/\/10\nBATCH_SIZE = 32\nLR = 1e-4","703a23e2":"public_df = test.query(\"seq_length == 107\").copy()\nprivate_df = test.query(\"seq_length == 130\").copy()\n\npublic_inputs = torch.tensor(preprocess_inputs(public_df)).to(device)\nprivate_inputs = torch.tensor(preprocess_inputs(private_df)).to(device)\n\npublic_loader = DataLoader(TensorDataset(public_inputs), shuffle=False, batch_size=BATCH_SIZE)\nprivate_loader = DataLoader(TensorDataset(private_inputs), shuffle=False, batch_size=BATCH_SIZE)\n\nbert_private_preds = np.zeros((private_df.shape[0], 130, 5))\nbert_public_preds = np.zeros((public_df.shape[0], 107, 5))\n\nkfold = KFold(FOLDS, shuffle=True, random_state=2020)\nbert_histories = []\n\nfor k, (train_index, val_index) in enumerate(kfold.split(train_inputs)):\n    train_dataset = TensorDataset(train_inputs[train_index], train_labels[train_index])\n    val_dataset = TensorDataset(train_inputs[val_index], train_labels[val_index])\n\n    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n    val_loader = DataLoader(val_dataset, shuffle=False, batch_size=BATCH_SIZE)\n\n    seed_everything()\n    model = Transformer(cfg).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=LR) #AdamW \n\n    train_losses = []\n    val_losses = []\n    \n    for epoch in tqdm(range(EPOCHS)):\n        train_losses_batch = []\n        val_losses_batch = []\n        for (batch_X, batch_Y) in train_loader:\n            model.train()\n            train_loss = compute_loss(batch_X, batch_Y, model, optimizer=optimizer, is_train=True)\n            train_losses_batch.append(train_loss)\n        for (batch_X, batch_Y) in val_loader:\n            model.eval()\n            val_loss = compute_loss(batch_X, \n                                batch_Y, model, \n                                optimizer=optimizer, \n                                is_train=False)\n            val_losses_batch.append(val_loss)\n        avg_train_loss = sum(train_losses_batch) \/ len(train_losses_batch)\n        avg_val_loss = sum(val_losses_batch) \/ len(val_losses_batch)\n        train_losses.append(avg_train_loss)\n        val_losses.append(avg_val_loss)\n        \n        if not (epoch+1) % PRINT_FREQ:\n            print(f\"[{epoch+1}\/{EPOCHS}] | Train Loss: {avg_train_loss:.5f} | Val Loss: {avg_val_loss:.5f}\")\n        \n    model_state = model.state_dict()\n    del model\n            \n    bert_histories.append({'train_loss': train_losses, 'val_loss': val_losses})\n\n\n    bert_short = Transformer(cfg, pred_len=107).to(device)\n    bert_short.load_state_dict(model_state)\n    bert_short.eval()\n    bert_public_pred = np.ndarray((0, 107, 5))\n    for batch in public_loader:\n        batch_X = batch[0]\n        pred = bert_short(batch_X).detach().cpu().numpy()\n        bert_public_pred = np.concatenate([bert_public_pred, pred], axis=0)\n    bert_public_preds += bert_public_pred \/ FOLDS\n\n    bert_long = Transformer(cfg, pred_len=130).to(device)\n    bert_long.load_state_dict(model_state)\n    bert_long.eval()\n    bert_private_pred = np.ndarray((0, 130, 5))\n    \n    for batch in private_loader:\n        batch_X = batch[0]\n        pred = bert_long(batch_X).detach().cpu().numpy()\n        bert_private_pred = np.concatenate([bert_private_pred, pred], axis=0)\n    bert_private_preds += bert_private_pred \/ FOLDS\n    \n    del bert_short, bert_long","29fdc1fc":"print(f\" BERT mean fold validation loss: {np.mean([min(history['val_loss']) for history in bert_histories])}\")\n      \nfor i, hist in enumerate(bert_histories):\n    plt.title(f\"Model #{i+1}\")\n    plt.plot(hist['train_loss'])\n    plt.plot(hist['val_loss']) \n    plt.legend(['Train', 'Val'])\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.show()","0fc229ff":"public_df = test.query(\"seq_length == 107\").copy()\nprivate_df = test.query(\"seq_length == 130\").copy()\n\npublic_inputs = preprocess_inputs(public_df)\nprivate_inputs = preprocess_inputs(private_df)\n\npreds_bert = []\n\nfor df, preds in [(public_df, bert_public_preds), (private_df, bert_private_preds)]:\n    for i, uid in enumerate(df.id):\n        single_pred = preds[i]\n\n        single_df = pd.DataFrame(single_pred, columns=target_cols)\n        single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n\n        preds_bert.append(single_df)\n\npreds_bert_df = pd.concat(preds_bert)\npreds_bert_df.head()","66d8bdb9":"submission = sample_sub[['id_seqpos']].merge(preds_bert_df, on=['id_seqpos'])\nsub_fname = \"submission.csv\"\nsubmission.to_csv(sub_fname, index=False)\nprint(f\"Submission saved in {sub_fname}\")\nsubmission.head()","8f585875":"# BERT Helper Classes\/Functions","ae6aa033":"# Imports","900f7978":"# Submission","794e9dce":"# Config\nFollowing is the original BERT-BASE config:\n```\n{\n\t\"dim\": 768,\n\t\"dim_ff\": 3072,\n\t\"n_layers\": 12,\n\t\"p_drop_attn\": 0.1,\n\t\"n_heads\": 12,\n\t\"p_drop_hidden\": 0.1,\n\t\"max_len\": 512,\n\t\"n_segments\": 2,\n\t\"vocab_size\": 30522\n}\n```\nHowever, as you can see in the following code section, I am using a different configuration.","deba6529":"# BERT Transformer","4e4f7705":"# BERT (PyTorch) Model for mRNA Degradation\n## Disclaimer: \n1. I do not take any credit for this notebook. Almost everything in this notebook has been taken from either [1] or [2]. \n2. I am no expert, there might be mistakes. Also, the results aren't good for now.\n\n## About\nBERT is a very powerful NLP language model, it might be an overkill for the current task. Therefore, I have used the BERT architecture with less number of layers and dimension size. Please see the config section for these changes.\n\n\n\n- [1]: https:\/\/github.com\/dhlee347\/pytorchic-bert\n- [2]: https:\/\/www.kaggle.com\/hiroshun\/pytorch-implementation-gru-lstm\n\n","5bf767f0":"# Read and pre-process the data"}}