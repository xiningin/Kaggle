{"cell_type":{"63f70af2":"code","44c482bc":"code","db67ff86":"code","d732ae24":"code","bd1d0ac5":"code","c18b2ab7":"code","ff5f22b7":"code","1179e19d":"code","5367e5af":"code","19836269":"code","3584e922":"code","236c240c":"code","1f38351b":"code","42e8133c":"code","c21aee49":"code","dfbddbe1":"code","ad9d40b1":"code","6609db56":"code","9395923b":"code","185933fd":"code","2524132d":"code","27b6d7f7":"code","0b414d9f":"code","e63422f2":"code","95522ff9":"code","d260e86c":"code","ebb711c8":"code","80da1777":"code","dda0bb42":"code","52c99bd3":"code","1fdec99a":"code","387d93b1":"code","9ee67b22":"code","c1f232b8":"code","8caded42":"code","e92c741f":"code","c22d4445":"code","6468ae63":"code","7bfaef71":"code","9dac64b4":"code","6e751d5d":"code","1d66eae1":"code","8aaeb7bf":"code","73ec12ec":"code","1a258b40":"code","d4cd6d7b":"code","f1db8067":"code","e931dc26":"code","9deb84a2":"code","ae15d6f1":"code","fea12ff0":"code","ee0ba39a":"code","ecc72e7a":"code","f1a923ca":"code","030f8fe8":"code","4684230d":"code","cfae0a5c":"code","4a78e6bb":"code","86fd9116":"code","8ff2f116":"code","91e04ce4":"code","e1cd565a":"code","87f32efe":"code","f29b024e":"code","df3387f1":"code","932f36f0":"code","d81acc20":"code","27f58c92":"code","3151bdfb":"code","e655eb21":"code","53ae69ba":"code","83cda840":"code","90bbf8f5":"code","02a3a449":"code","ad9f0f10":"code","fda70b3e":"code","4b6d7a69":"markdown","d18448c0":"markdown","37084b31":"markdown","d3266b8e":"markdown","fad7ac7b":"markdown","ab44baa4":"markdown","85d75e5d":"markdown","65ef2274":"markdown","e16ba78f":"markdown","8d6fe55b":"markdown","c19b682b":"markdown","df231579":"markdown","4cb45477":"markdown","d34cb80b":"markdown","5d350947":"markdown","67542d28":"markdown","d3fc447c":"markdown","c148a63c":"markdown","733467bd":"markdown","175e33ad":"markdown","f151a160":"markdown","1945a80a":"markdown","0dc6ce3d":"markdown"},"source":{"63f70af2":"!pip install iso3166\nfrom iso3166 import countries_by_name","44c482bc":"import spacy\nspacy_model = spacy.load(\"en_core_web_lg\")","db67ff86":"from IPython.display import HTML\nimport pickle\nimport re\nimport string\nfrom tqdm.notebook import tqdm\n\nimport numpy as np\nimport pandas as pd\n\nimport plotly.graph_objects as go\nfrom plotly.offline import init_notebook_mode, iplot, plot\ninit_notebook_mode(connected=True)\n\nimport nltk\nnltk.download('stopwords')\nnltk.download('vader_lexicon')\nfrom nltk.corpus import stopwords\nfrom nltk.sentiment import vader\n\nfrom sklearn.metrics.pairwise import cosine_similarity","d732ae24":"import requests\n\ndef download_file_from_google_drive(id, destination):\n    URL = \"https:\/\/docs.google.com\/uc?export=download\"\n\n    session = requests.Session()\n\n    response = session.get(URL, params = { 'id' : id }, stream = True)\n    token = get_confirm_token(response)\n\n    if token:\n        params = { 'id' : id, 'confirm' : token }\n        response = session.get(URL, params = params, stream = True)\n\n    save_response_content(response, destination)    \n\ndef get_confirm_token(response):\n    for key, value in response.cookies.items():\n        if key.startswith('download_warning'):\n            return value\n\n    return None\n\ndef save_response_content(response, destination):\n    CHUNK_SIZE = 1000\n\n    with open(destination, \"wb\") as f:\n        for chunk in response.iter_content(CHUNK_SIZE):\n            if chunk: # filter out keep-alive new chunks\n                f.write(chunk)\n","bd1d0ac5":"class Paper:\n    def __init__(self,\n               paper_id,\n               doi,\n               publish_time,\n               journal,\n               affiliations,\n               source_x,\n               title,\n               authors,\n               abstract,\n               text,\n               **extras\n               ):\n        self.paper_id = str(paper_id)\n        self.doi = 'https:\/\/doi.org\/' + str(doi)\n        self.publish_time = publish_time\n        self.journal = journal\n        self.affiliations = affiliations\n        self.source_x = source_x\n        self.title = title\n        self.title_embedding = None\n        self.authors = authors.split(',')\n        self.abstract = abstract\n        self.abstract_embedding = None\n        self.text = text\n        self.text_embedding = None","c18b2ab7":"class TextProcessor:\n    DOMAIN_STOP_WORDS = {\"abstract\", \"al\", \"article\", \"arxiv\", \"author\", \"biorxiv\", \"background\", \"chapter\", \"conclusions\", \"copyright\", \"da\", \"dei\", \"del\", \"dell\", \"della\",\n                          \"delle\", \"di\", \"doi\", \"edition\", \"et\", \"fig\", \"figure\", \"funder\", \"holder\", \"http\", \"https\",\n                        'i', 'ii', 'iii','iv','v', 'vii', 'viii', 'ix', 'x', 'xi', 'xii','xiii', 'xiv' \"il\", \"la\", \"le\", \"introduction\",\n                          \"license\", \"medrxiv\", \"non\", \"org\", \"peer\", \"peer-reviewed\", \"permission\", \"preprint\", \"publication\",\n                          \"pubmed\", \"reserved\", \"reviewed\", \"rights\", \"section\", \"summary\", \"si\", \"table\", \"una\"}\n\n    STOP_WORDS = set(stopwords.words(\"english\")) | DOMAIN_STOP_WORDS - set(['non', 'no'])\n\n    @staticmethod\n    def clean_text(text, spacy_model):\n        if len(text) > spacy_model.max_length:\n            text = text[:spacy_model.max_length]\n        doc = spacy_model(text)\n        raw_sentences = []\n        clean_sentences = []\n        tokenized_sentences = []\n        for sent in list(doc.sents):\n            raw_sentences.append(sent.string)\n            clean_sentence = \"\"\n            tokens = []\n            for token in spacy_model(sent.string.strip()):\n                if token.pos_ not in ('PUNCT', 'SYM', 'X') \\\n                and re.match(r\"^\\d*[a-z][\\-.0-9:_a-z]{1,}$\", token.text.lower()):\n                    clean_sentence += f\"{token.text} \"\n                    if token.text.lower() not in TextProcessor.STOP_WORDS:\n                        tokens.append(token.text.lower())\n\n            clean_sentences.append(clean_sentence.strip())\n            tokenized_sentences.append(tokens) \n            for raw, cleaned, tokenized in zip(raw_sentences, clean_sentences, tokenized_sentences):\n                if len(tokenized) > 3:\n                    yield (raw, cleaned, tokenized)\n","ff5f22b7":"reseach_papers_file_id = '1ZGRWUIEm9VHfW4JraJ6ECUoNdNX5Q9mW'\ndownload_file_from_google_drive(reseach_papers_file_id, 'reseach_papers.pkl')\nwith open('reseach_papers.pkl', 'rb') as f:\n    research_papers = pickle.load(f) ","1179e19d":"type(research_papers[0].text)","5367e5af":"research_papers[0].text[0]","19836269":"def get_df(list_of_paper_obj):\n    df = pd.DataFrame(data={'title': [\" \".join([title[1] for title in paper.title]) for paper in list_of_paper_obj],\n                          'abstract': [\" \".join([abstract[1] for abstract in paper.abstract]) for paper in list_of_paper_obj],\n                          'text': [\" \".join([text[1] for text in paper.text]) for paper in list_of_paper_obj],\n                          'affiliations': [paper.affiliations for paper in list_of_paper_obj],\n                          'publish_time': [paper.publish_time for paper in list_of_paper_obj]\n                          })\n    df['publish_time'] = df['publish_time'].map(lambda x: re.search('\\d{4}', str(x)).group())\n    df['publish_time'] = pd.to_datetime(df['publish_time'])\n\n    return df\n\ndef count_relative_number_ethics_articles(x):\n    articles_matches = [re.findall('ethical|ethician|ethicist|ethics', record) for record in x['title'] + x['abstract']]\n    count_ethics_articles = sum([len(match) != 0 for match in articles_matches])\n    return pd.Series([count_ethics_articles \/ x['title'].count(),\n                      count_ethics_articles,\n                      x['title'].count()\n                      ],\n                      index=['relative_count', 'ethics_count', 'total_count']\n                    )","3584e922":"df = get_df([paper for paper in research_papers \n             if paper.title != [] and paper.publish_time != '' and paper.abstract != []\n             ])","236c240c":"grouped = df.groupby(df.publish_time.dt.year).apply(count_relative_number_ethics_articles)\ngrouped.index = pd.to_datetime(grouped.index, format='%Y')","1f38351b":"def plot_relative_number_ethics_articles(grouped_by_year_df, title):\n    grouped_data = [go.Scatter(x=grouped_by_year_df.index,\n                            y=grouped_by_year_df['relative_count'],\n                            text = [f'ethics count: {row[\"ethics_count\"]}<br>total count: {row[\"total_count\"]}' \n                                    for idx, row in grouped_by_year_df.iterrows()],\n                            mode='lines+markers',\n                            name='ethics articles count'\n                            )]\n    grouped_data.append(go.Scatter(x=[\"2002-01-01\", \"2019-01-01\", \"2014-01-01\"],\n                                 y=[-0.005, -0.005, -0.005],\n                                 text=[\"SARS\", \"COVID-19\", \"Ebola\"],\n                                 mode=\"text\",\n                                 name='outbreaks'\n                                 ))\n    layout = go.Layout(title=title, title_x=0.5, xaxis=dict(title='Date'),\n                   yaxis=dict(title='Ratio'))\n    shapes=[dict(type=\"line\",\n            xref=\"x\",\n            yref=\"paper\",\n            x0=\"2002-01-01\",\n            y0=0,\n            x1=\"2002-01-01\",\n            y1=1,\n            line=dict(width=4, dash='dot', color=\"LightSeaGreen\")\n        ),\n        dict(type=\"line\",\n            xref=\"x\",\n            yref=\"paper\",\n            x0=\"2019-01-01\",\n            y0=0,\n            x1=\"2019-01-01\",\n            y1=1,\n            line=dict(width=4, dash='dot', color=\"MediumPurple\")\n        ),\n        dict(type=\"line\",\n            xref=\"x\",\n            yref=\"paper\",\n            x0=\"2014-01-01\",\n            y0=0,\n            x1=\"2014-01-01\",\n            y1=1,\n            line=dict(width=4, dash='dot', color=\"RoyalBlue\")\n        )\n        ]\n    layout['shapes'] = shapes\n    layout['template'] = 'plotly_white'\n    fig = go.Figure(data=grouped_data, layout=layout)\n    return fig","42e8133c":"fig = plot_relative_number_ethics_articles(grouped, title='Ratio of Ethics Articles')","c21aee49":"display(HTML('<iframe width=\"1000\" height=\"500\" frameborder=\"0\" scrolling=\"no\" src=\"https:\/\/plotly.com\/~kfrid\/1.embed\"><\/iframe>'))","dfbddbe1":"def map_tags(string, regex_dict):\n    matches = []\n    for regex, phrase in regex_dict.items():\n        for match in re.findall(regex, string):\n            matches.append(phrase)\n    return list(set(matches))","ad9d40b1":"ethical_principles = {re.compile(r'\\bmax[\\w\\s]+benefit[s]?\\b|\\bincreas[\\w\\s]+benefit[s]?\\b|\\bbenefit[s\\s]+max[\\w]*\\b|\\bbeneficen[\\w]+\\b', re.I ): 'beneficence',\n                      re.compile(r'\\bconflict[sing\\s]+interest[s]?\\b|\\bconflict[sing\\s]+obligation[\\w]*\\b', re.I): 'conflicts of interest',\n                      re.compile(r'\\binclusiv[\\w]*\\b', re.I): 'inclusiveness',\n                      re.compile(r'\\binform[ed\\s]+consent\\b|\\binform[ed\\s]+choice[s]?\\b|\\bindividual[\\s]+consent[s]?\\b', re.I): 'informed consent',\n                      re.compile(r'\\bpublic[\\s]+trust[\\w]*\\b', re.I): 'public trust',\n                      re.compile(r'\\breduc[\\w\\s]+risk[s]?\\b|\\bmin[\\w\\s]+risk[s]?\\b|\\brisk[s\\s]+min[\\w]+\\b', re.I): 'minimizing risk',\n                      re.compile(r'\\bconfidentiality\\b|\\bprivacy\\b', re.I): 'confidentiality',\n                      re.compile(r'\\binterest[s]+defense\\b', re.I): 'interest defense',\n                      re.compile(r'\\bsafeguard[\\w]*\\b', re.I): 'safeguarding',\n                      re.compile(r'\\bscientific[\\s]+objectiv[\\w]*\\b', re.I): 'scientific objectivity', \n                      re.compile(r'\\btransparency\\b', re.I): 'transparency',\n                      re.compile(r'\\bno[n\\s-]*maleficence\\b|\\bno[n\\s-]*abandonment\\b|\\b[no\\s-]*harm[\\w]\\b', re.I): 'nonmaleficence',\n                      re.compile(r'\\bjustice\\b', re.I): 'justice',\n                      re.compile(r'\\bself[\\s-]*care\\b|\\bself[\\s-]*protect[ion]*\\b|\\bself[\\s-]*defens[es]+\\b', re.I): 'self-care'\n                     }","6609db56":"def count_tags(x):\n    data = []\n    indexes = []\n    for column in x.columns:\n        data.append(x[column].sum())\n        indexes.append(f'{column}')\n    return pd.Series(data=data,\n                    index=indexes\n                  ).fillna(0)","9395923b":"df['ethical_principles_tags'] = df['text'].apply(lambda x: map_tags(\"\".join(x), ethical_principles))","185933fd":"df = pd.concat([df['publish_time'], df['ethical_principles_tags'].str.join('|').str.get_dummies()], axis=1)","2524132d":"df = df.set_index('publish_time', drop=True)","27b6d7f7":"grouped = df.groupby(df.index.year).apply(count_tags)\ngrouped.index = pd.to_datetime(grouped.index, format='%Y')","0b414d9f":"grouped = grouped.reset_index()\ndf_melt = pd.melt(grouped, id_vars=['publish_time'], value_vars=list(grouped.columns)[1:], var_name=['tag'])\ndf_melt['year'] = df_melt['publish_time'].dt.year","e63422f2":"df_melt[df_melt['year'] == 1990]","95522ff9":"def plot_tags_per_year(df, title):\n    fig = go.Figure()\n    for step in df['year'].unique():\n        sample_df_year = df[df['year'] == step].reset_index(drop=True)\n        sample_df_year['relative'] = sample_df_year['value'] \/ sample_df_year['value'].sum()\n        sample_df_year['relative'] = sample_df_year['relative'].fillna(0)\n        colors = ['lightslategray',] * len(set(sample_df_year['tag']))\n\n\n        colors[sample_df_year['relative'].idxmax()] = 'crimson'\n        fig.add_trace(\n            go.Bar(\n                visible=False,\n                x=sample_df_year['tag'],\n                y=sample_df_year['relative'],\n                text=sample_df_year['value'],\n                textposition='outside',\n                marker_color=colors),\n                )\n\n# Make 0 trace visible\n    fig.data[0].visible = True\n    steps = []\n    for i in range(len(fig.data)):\n        step = dict(\n          method=\"restyle\",\n          label=str(df['year'].unique()[i]),\n          args=[\"visible\", [False] * len(fig.data)],\n      )\n        step[\"args\"][1][i] = True  # Toggle i'th trace to \"visible\"\n        steps.append(step)\n\n    sliders = [dict(\n      active=0,\n      currentvalue={\"prefix\": \" \", \"xanchor\": \"right\", \"font\": {\n            \"color\": '#888',\n            \"size\": 32\n          }},\n      pad={\"t\": 1},\n      steps=steps\n    )]\n\n    fig.update_layout(\n        sliders=sliders,\n        title=title,\n        title_x=0.5,\n        template='plotly_white'\n    )\n\n    fig.update_xaxes(\n      showgrid=False,\n      ticks=\"inside\",\n      tickson=\"boundaries\",\n      ticklen=10,\n      tickangle=-10\n    )\n\n    fig['layout']['yaxis'].update(title='Ratio of Mentions', range=[0, 1], autorange=False)\n    return fig","d260e86c":"fig = plot_tags_per_year(df_melt, title='Mentions of Ethical Principles')\niplot(fig)","ebb711c8":"display(HTML('<iframe width=\"1000\" height=\"550\" frameborder=\"0\" scrolling=\"no\" src=\"https:\/\/plotly.com\/~kfrid\/18.embed?width=1000&height=550\"><\/iframe>'))","80da1777":"health_measure = {re.compile(r'\\bhygiene\\b', re.I): 'hygiene',\n                  re.compile(r'\\bfac[\\w\\s]+mask[s]?\\b|\\bmask[sfor\\s]+fac[es]+\\b', re.I): 'face mask',\n                  re.compile(r'\\bsurfac[es\\s]+clean[ing]?|object[s\\s]+clean[ing]\\b', re.I): 'objects cleaning',\n                  re.compile(r'\\bultraviolet[\\s]+light[\\w]*\\b', re.I): 'ultraviolet light',\n                  re.compile(r'\\bmodif[\\w\\s]+humidity\\b|\\bincreas[\\w\\s]+humidity\\b', re.I): 'humidity',\n                  re.compile(r'\\bcontact[s\\s]+trac[\\w]*\\b|\\btrac[eingof\\s]+contact[s]?\\b', re.I): 'contact tracing',\n                  re.compile(r'\\b[self-]*isolation\\b', re.I): 'isolation',\n                  re.compile(r'\\bquarantin[\\w]*\\b', re.I): 'quarantine',\n                  re.compile(r'\\bsocial[\\s]+distans[\\w]*\\b', re.I): 'social distancing',\n                  re.compile(r'\\bavoid[ing\\s]+crowd[\\w]*\\b', re.I): 'avoiding crowds',\n                  re.compile(r'\\bschool[s\\s]+measure[s]?\\b|\\bschool[s\\s]+clos[\\w]*\\b', re.I): 'school closure',\n                  re.compile(r'\\bentr[yies\\s]+screen[ing]?\\b|\\bexit[s\\s]+screen[ing]?\\b', re.I): 'entry\/exit screening',\n                  re.compile(r'\\btravel[lsing\\s]+restrict[\\w]*\\b|\\bvisit[sing\\s]?restricti[\\w]*\\b', re.I): 'travel restrictions',\n                  re.compile(r'\\bborder[s\\s]+clos[\\w]*\\b', re.I): 'border closure',\n}","dda0bb42":"df = get_df([paper for paper in research_papers \n             if paper.title != [] and paper.publish_time != '' and paper.abstract != []\n             ])","52c99bd3":"df['health_measure_tags'] = df['text'].apply(lambda x: map_tags(x, health_measure))","1fdec99a":"df = pd.concat([df['publish_time'], df['health_measure_tags'].str.join('|').str.get_dummies()], axis=1)","387d93b1":"df = df.set_index('publish_time', drop=True)","9ee67b22":"grouped = df.groupby(df.index.year).apply(count_tags)\ngrouped.index = pd.to_datetime(grouped.index, format='%Y')","c1f232b8":"grouped = grouped.reset_index()\ndf_melt = pd.melt(grouped, id_vars=['publish_time'], value_vars=list(grouped.columns)[1:], var_name=['tag'])\ndf_melt['year'] = df_melt['publish_time'].dt.year","8caded42":"fig = plot_tags_per_year(df_melt, title='Mentions of Public Health Measures')\niplot(fig)","e92c741f":"display(HTML('<iframe width=\"1000\" height=\"500\" frameborder=\"0\" scrolling=\"no\" src=\"https:\/\/plotly.com\/~kfrid\/20.embed\"><\/iframe>'))","c22d4445":"country_name = {\n    re.compile('^(?:(A[KLRZ]|C[AOT]|D[CE]|FL|GA|HI|I[ADLN]|K[SY]|LA|M[ADEINOST]|N[CDEHJMVY]|O[HKR]|P[AR]|RI|S[CD]|T[NX]|UT|V[AIT]|W[AIVY]))$'):'United States of America',\n                re.compile(r'\\bUnited[\\s]+States[\\w]*\\b|Chapel[\\s]Hill|Irvine|Los Angeles|Baltimore|East Lansing|San Francisco|USA', re.I): 'United States of America',\n                re.compile(r'Alabama|Alaska|Arizona|Arkansas|California|Colorado|Connecticut|Delaware|Florida|Georgia|Hawaii|Idaho|Illinois|Indiana|Iowa|Kansas|Kentucky|Louisiana|Maine|Maryland|Massachusetts|Michigan|Minnesota|Mississippi|Missouri|Montana|Nebraska|Nevada|New\\sHampshire|New\\sJersey|New\\sMexico|New\\sYork|North\\sCarolina|North\\sDakota|Ohio|Oklahoma|Oregon|Pennsylvania|Rhode\\sIsland|South\\sCarolina|South\\sDakota|Tennessee|Texas|Utah|Vermont|Virginia|Washington|West\\sVirginia|Wisconsin|Wyoming', re.I) : 'United States of America',\n                re.compile(r'\\bUnited[\\s]+Kingdom\\b|\\bGreat[\\s]Britain\\b|\\bLondon\\b|Cambridge|Liverpool|England|UK', re.I): 'UNITED KINGDOM OF GREAT BRITAIN AND NORTHERN IRELAND',\n                re.compile(r'\\b[\\w\\s]*China\\b|\\bROC\\b|\\bWuhan\\b|\\bHubei\\b|\\bBeijing\\b', re.I): 'China',\n                re.compile(r'\\b[\\w\\s]*Korea\\b|Seoul|Suwon|Seongnam', re.I): 'KOREA, REPUBLIC OF',\n                re.compile(r'[\\w\\s]*Netherlands|Utrecht|Leiden|Amsterdam|Rotterdam|Bilthoven', re.I): 'Netherlands',\n                re.compile(r'\\b[\\w\\s]*Singapore', re.I): 'Singapore',\n                re.compile(r'\\b[\\w\\s]*Germany', re.I): 'Germany',\n                re.compile(r'Riyadh|[\\w\\s]*Saudi Arabia', re.I): 'Saudi Arabia',\n                re.compile(r'Hong|Shatin|Kowloon|Pokfulam', re.I): 'Hong Kong',\n                re.compile(r'Espa\u00f1a|Spain', re.I): 'Spain',\n                re.compile(r'M[\u00e9e]+xico', re.I): 'Mexico',\n                re.compile(r'Viet[\\s]?nam', re.I): 'Viet Nam',\n                re.compile(r'Ouro Preto|Bra[sz]il|Belo Horizonte|Porto Alegre', re.I): 'BRAZIL',\n                re.compile(r'Paris|France|Lyon', re.I): 'France',\n                re.compile(r'Cape Town', re.I): 'South Africa',\n                re.compile(r'Brno|Czechia|Czech Republic', re.I): 'Czechia',\n                re.compile(r'IRAN', re.I): 'IRAN, ISLAMIC REPUBLIC OF',\n                re.compile(r'Russia|Russian Federation', re.I): 'RUSSIAN FEDERATION',\n                re.compile(r'Taiwan', re.I): 'TAIWAN, PROVINCE OF CHINA',\n                re.compile(r'Tan[sz]ania', re.I): 'TANZANIA, UNITED REPUBLIC OF'}","6468ae63":"def map_geo_tags(text, regex_dict):\n    matches = re.findall(r', [\\w\\s]+\\)', text)\n    clean_matches = set([re.sub('[\\d,\\)]', \"\", text).strip(' ') for text in matches])\n    clean_matches_2 = []\n    for text in clean_matches:\n        for regex, phrase in regex_dict.items():\n            if re.match(regex, text):\n                text = phrase\n            clean_matches_2.append(text.upper())\n    return list(set(clean_matches_2))","7bfaef71":"df = get_df([paper for paper in research_papers \n                          if paper.title != [] and paper.publish_time != '' and paper.text != [] and paper.affiliations != '']\n                         )","9dac64b4":"df.shape","6e751d5d":"df['geo_tags'] = df['affiliations'].apply(lambda x: map_geo_tags(x, country_name))","1d66eae1":"df = pd.concat([df[['publish_time']], df['geo_tags'].str.join('|').str.get_dummies()], axis=1)","8aaeb7bf":"df.shape","73ec12ec":"df = df.loc[~(df.iloc[:, 1:]==0).all(axis=1)]","1a258b40":"df = df.set_index('publish_time', drop=True)","d4cd6d7b":"grouped = df.iloc[:, 1:].groupby(df.index.year).apply(count_tags)\ngrouped.index = pd.to_datetime(grouped.index, format='%Y')","f1db8067":"df_paper_per_country = grouped.transpose().cumsum(axis=1)\ndf_paper_per_country = df_paper_per_country.transpose().reset_index()","e931dc26":"df_paper_per_country.columns","9deb84a2":"df_paper_melt = pd.melt(df_paper_per_country, id_vars=['publish_time'], value_vars=list(df_paper_per_country.columns)[1:], var_name=['country'])","ae15d6f1":"df_paper_melt['iso'] = df_paper_melt['country'].map(lambda x: countries_by_name.get(x, 'Unknown code')[2])","fea12ff0":"df_paper_melt = df_paper_melt[df_paper_melt['iso'] != 'k'].reset_index(drop=True)","ee0ba39a":"df_paper_melt['year'] = df_paper_melt['publish_time'].dt.year","ecc72e7a":"df_paper_melt","f1a923ca":"import plotly.express as px\nfig = px.choropleth(df_paper_melt,\n                    locations=\"iso\",\n                    color='value',\n                    hover_name=\"country\",\n                    animation_frame='year',\n                    range_color=[0, 1000],  color_continuous_scale='deep'\n                    )\nfig.show()","030f8fe8":"def vectorize(list_of_triplets):\n    print(\"--start--\")\n    embeddings = embed([\" \".join(text) for _, _, text in list_of_triplets], signature=\"default\", as_dict=True)[\"default\"]\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        sess.run(tf.tables_initializer())\n        x = sess.run(embeddings)\n    print('--end--')\n    return x","4684230d":"import tensorflow.compat.v1 as tf\ntf.compat.v1.disable_eager_execution()\nimport tensorflow_hub as hub","cfae0a5c":"tf.test.gpu_device_name()","4a78e6bb":"url = \"https:\/\/tfhub.dev\/google\/elmo\/3\"\nembed = hub.Module(url)","86fd9116":"covid_sample_papers = [paper for paper in research_papers \n if len(re.findall('COVID[-\\d]*|2019-nCov|SARS[-\\s]?cov[-\\s]?2',\n                   \" \".join([title[0] for title in paper.title]) + \" \".join([abstract[0] for abstract in paper.abstract]),\n                   re.I)\n        ) != 0]","8ff2f116":"for idx in tqdm(range(len(covid_sample_papers)), total=len(covid_sample_papers)):\n    covid_sample_papers[idx].text_embedding = vectorize(covid_sample_papers[idx].text)","91e04ce4":"del research_papers","e1cd565a":"covid_sample_file_id = '1H_onqws0ccTAn7a2Xe7S4CTcxfImvEBW'\ndownload_file_from_google_drive(covid_sample_file_id, 'covid_sample.pkl')\nwith open('covid_sample.pkl', 'rb') as f:\n    covid_sample_papers = pickle.load(f) ","87f32efe":"len(covid_sample_papers)","f29b024e":"text_processor = TextProcessor()\nsid = vader.SentimentIntensityAnalyzer()\n\n\ndef color_sid(phrase):\n    scores = sid.polarity_scores(phrase)\n    pos = scores['pos']\n    neu = scores['neu']\n    neg = scores['neg']\n    return 'rgb(' + f'{neg * 255},'  + f'{pos * 255},' + f'{neu * 255})'","df3387f1":"def search_similarity(search_string, papers, n_results, threshold=0.7):\n    processed_search_string = list(text_processor.clean_text(search_string, spacy_model))\n    search_vect = vectorize(processed_search_string)\n    output = \"\"\n    sentences = []\n    for paper in papers:\n        cosine_similarities = pd.Series(cosine_similarity(search_vect,\n                                                      paper.text_embedding\n                                                      ).flatten())\n        if any(cosine_similarities > threshold):\n            output += '<p style=\"font-family:verdana; font-size:110%;\"> '\n            title = str(paper.title[0][0]) if paper.title != [] else \"Empty\"\n            output += \" <b>\" + f'{title}' + \" <\/b>\" + \"<\/br>\"\n            for i, j in cosine_similarities.nlargest(int(n_results)).iteritems():\n                if j > threshold:\n                    output += \" <b>\"+str(round(j, 3))+\" <\/b>\"\n                    similar_sentences = [paper.text[i][0]]\n                    colored_string = \"\".join([\" \".join(['<span style=' + f'\"color:{color_sid(word)}; font-size:100%\"' + '>' + f\"{word}\" + '<\/span>' for word in sentence.split()]) + '<\/br>' \n                                  for sentence in similar_sentences])\n                    output += colored_string\n              # output += \" \".join([f'<p style=\"color: \"red\">\" ' + sentence + ' <\/p>' \n              #                     for sentence in similar_sentences]) \\\n              #                     + \"<\/br>\" \n              # output += \" \".join(similar_sentences) + \"<\/br>\"\n                    sentences.append(similar_sentences)\n\n            output += \"<\/p><\/hr>\"\n        \n    output = '<h3>Results:<\/h3>' + output\n    return output, sentences","932f36f0":"query = \"emotional response and psychological health of workers during an outbreak\" \nN = \"5\" \n\noutput, sim_sentences = search_similarity(query, covid_sample_papers, N, 0.7)\n","d81acc20":"display(HTML(output))","27f58c92":"\nimport itertools\nimport networkx as nx","3151bdfb":"SUBJ_DEPS = {\"agent\", \"csubj\", \"csubjpass\", \"expl\", \"nsubj\", \"nsubjpass\"}\nOBJ_DEPS = {\"attr\", \"dobj\", \"dative\", \"oprd\"}\nAUX_DEPS = {\"aux\", \"auxpass\", \"neg\",  \"advmod\"}\n\ndef get_span_for_compound_noun(noun):\n    \n    min_i = noun.i - sum(\n        1\n        for _ in itertools.takewhile(\n            lambda x: x.dep_ in {\"compound\", \"acomp\"} , reversed(list(noun.lefts))\n        )\n    )\n\n    max_i = noun.i + sum(\n        1\n        for _ in itertools.takewhile(\n            lambda x: x.dep_ in {\"compound\", \"acomp\"}, noun.rights\n        )\n    )\n    return (min_i, max_i)\n\n\ndef get_span_for_verb_auxiliaries(verb):\n\n\n    min_i = verb.i - sum(\n        1\n        for _ in itertools.takewhile(\n            lambda x: x.dep_ in AUX_DEPS, reversed(list(verb.lefts))\n        )\n    )\n    max_i = verb.i + sum(\n        1\n        for _ in itertools.takewhile(\n            lambda x: x.dep_ in AUX_DEPS, verb.rights\n        )\n    )\n    return (min_i, max_i)\n\ndef subject_verb_object_triples(doc):\n  \n    sents = doc.sents\n\n    for sent in sents:\n        start_i = sent[0].i\n        verbs = get_main_verbs_of_sent(sent)\n        for verb in verbs:\n            subjs = get_subjects_of_verb(verb)\n            if not subjs:\n                continue\n            objs = get_objects_of_verb(verb)\n            if not objs:\n                continue\n\n            verb_span = get_span_for_verb_auxiliaries(verb)\n            verb = sent[verb_span[0] - start_i : verb_span[1] - start_i + 1]\n            for subj in subjs:\n                if subj not in text_processor.STOP_WORDS:\n                    subj = sent[\n                    get_span_for_compound_noun(subj)[0]\n                    - start_i : subj.i\n                    - start_i\n                    + 1\n                ]\n            for obj in objs:\n                if obj.pos_ == \"VERB\":\n                    continue\n                elif obj.pos_ == \"NOUN\" and obj not in text_processor.STOP_WORDS:\n                    span = get_span_for_compound_noun(obj)\n            \n                    obj = sent[span[0] - start_i : span[1] - start_i + 1]\n\n                    if len(obj) != 0 and len(subj) != 0:\n                        yield (subj, verb, obj)\n\n\n\n\ndef _get_conjuncts(tok):\n    \"\"\"\n    Return conjunct dependents of the leftmost conjunct in a coordinated phrase,\n    e.g. \"Burton, [Dan], and [Josh] ...\".\n    \"\"\"\n    return [right for right in tok.rights if right.dep_ == \"conj\"]\n\ndef get_main_verbs_of_sent(sent):\n    \"\"\"Return the main (non-auxiliary) verbs in a sentence.\"\"\"\n    return [\n        tok for tok in sent if tok.pos_ == 'VERB' \n        and tok.dep_ not in AUX_DEPS\n    ]\n\n\ndef get_subjects_of_verb(verb):\n    \"\"\"Return all subjects of a verb according to the dependency parse.\"\"\"\n    subjs = [tok for tok in verb.lefts if tok.dep_ in SUBJ_DEPS]\n    # get additional conjunct subjects\n    subjs.extend(tok for subj in subjs for tok in _get_conjuncts(subj))\n    return subjs\n\n\ndef get_objects_of_verb(verb):\n    \"\"\"\n    Return all objects of a verb according to the dependency parse,\n    including open clausal complements.\n    \"\"\"\n    objs = [tok for tok in verb.rights if tok.dep_ in OBJ_DEPS]\n    # get open clausal complements (xcomp)\n    objs.extend(tok for tok in verb.rights if tok.dep_ == \"xcomp\")\n    # get additional conjunct objects\n    objs.extend(tok for obj in objs for tok in _get_conjuncts(obj))\n    return objs","e655eb21":"def plot_graph(df, node_filter, depth=2, seed=3):\n    shells = [[node_filter], list(set(df[['subject', 'object']].values.flatten()) - {node_filter})]\n    if depth == -1:\n        G = nx.from_pandas_edgelist(df, 'subject', 'object',  create_using=nx.MultiDiGraph())\n    else:\n        G_full = nx.from_pandas_edgelist(df, 'subject', 'object',  create_using=nx.MultiDiGraph())\n        G = nx.ego_graph(G_full, node_filter, radius=depth)\n  \n    pos = nx.drawing.layout.spring_layout(G, scale=2, k=1.5, seed=seed)\n    for node in G.nodes:\n        G.nodes[node]['pos'] = list(pos[node])\n\n    traceRecode = []  \n  \n\n    index = 0\n    for edge in G.edges:\n        x0, y0 = G.nodes[edge[0]]['pos']\n        x1, y1 = G.nodes[edge[1]]['pos']\n        trace = go.Scatter(x=tuple([x0, x1, None]), y=tuple([y0, y1, None]),\n                          mode='lines',\n                          line={'width': 1.5},\n                          marker=dict(color='LightGrey'),\n                          line_shape='spline',\n                          opacity=1\n                          )\n        traceRecode.append(trace)\n        index = index + 1\n\n\n    middle_hover_trace = go.Scatter(x=[], y=[], hovertext=[], mode='markers', hoverinfo=\"text\",\n                                    marker={'size': 6, 'color': 'DarkGrey', 'symbol': \"circle-dot\"},\n                                    opacity=1)\n\n    index = 0\n    for edge in G.edges:\n        x0, y0 = G.nodes[edge[0]]['pos']\n        x1, y1 = G.nodes[edge[1]]['pos']\n        hovertext = df[(df['subject'] == edge[0]) & (df['object'] == edge[1])]['relation'].values[0]\n        middle_hover_trace['x'] += tuple([(x0 + x1) \/ 2])\n        middle_hover_trace['y'] += tuple([(y0 + y1) \/ 2])\n        middle_hover_trace['hovertext'] += tuple([edge[0] + '---' + hovertext + '---'+ edge[1]])\n        index = index + 1\n\n    traceRecode.append(middle_hover_trace)\n\n    node_trace = go.Scatter(x=[], y=[], hovertext=[], text=[], mode='markers+text', textposition=\"middle center\",\n                            hoverinfo=\"text\", marker={ 'size': [], 'color': ['LightBlue'] * len(G.nodes())}, opacity=1)\n    index = 0\n    for node in G.nodes():\n        x, y = G.nodes[node]['pos']\n        \n        text = node\n        node_trace['x'] += tuple([x])\n        node_trace['y'] += tuple([y])\n        node_trace['text'] += tuple([\"<br>\".join(text.split())])\n        node_trace['marker']['size'] += tuple([max(np.log(len(nx.descendants(G, node))) * 10, 10)])\n        index = index + 1\n\n    traceRecode.append(node_trace)\n\n    figure = {\n        \"data\": traceRecode,\n        \"layout\": go.Layout(title=f'{node_filter} connections',\n                            title_x=0.5,\n                            showlegend=False, hovermode='closest',\n                            template = 'plotly_white',\n                            margin={'b': 40, 'l': 40, 'r': 40, 't': 40},\n                            xaxis={'showgrid': False, 'zeroline': False, 'showticklabels': False},\n                            yaxis={'showgrid': False, 'zeroline': False, 'showticklabels': False},\n                            height=1000,\n                            clickmode='select+event',\n                            annotations=[\n                                dict(\n                                    x=(G.nodes[edge[0]]['pos'][0] + G.nodes[edge[1]]['pos'][0]) \/ 2,\n                                    y=(G.nodes[edge[0]]['pos'][1] + G.nodes[edge[1]]['pos'][1]) \/ 2, axref='x', ayref='y',\n                                    ax=(G.nodes[edge[0]]['pos'][0] + G.nodes[edge[1]]['pos'][0]) \/ 2 ,\n                                    ay=(G.nodes[edge[0]]['pos'][1]  + G.nodes[edge[1]]['pos'][1]) \/ 2, xref='x', yref='y',\n                                    showarrow=True,\n                                    arrowhead=2,\n                                    arrowsize=10,\n                                    arrowwidth=1,\n                                    arrowcolor=\"LightGrey\",\n                                    opacity=1,\n                                    \n                                ) for edge in G.edges]\n                            )}\n\n    return figure","53ae69ba":"svo = []\nfor paper in tqdm(covid_sample_papers, total=(len(covid_sample_papers))):\n    sentence = \". \".join([triplet[1] for triplet in paper.text])\n    if len(sentence) > 1000000:\n        for i in range(0, len(sentence), 1000000):\n            svo += list(subject_verb_object_triples(spacy_model(sentence[i:i+1000000])))\n    else:\n        svo += list(subject_verb_object_triples(spacy_model(sentence)))","83cda840":"data = [(\" \".join([t.text for t in triplet[0]]).strip(),\n           \" \".join([t.text for t in triplet[1]]).strip(),\n           \" \".join([t.text for t in triplet[2]]).strip()\n           ) for triplet in svo]\ndf = pd.DataFrame(data=data, columns=['subject', 'relation', 'object'])\ndf = df.drop_duplicates(keep='first').reset_index(drop=True)","90bbf8f5":"df[df['subject'].str.find('WHO') != -1]","02a3a449":"configure_plotly_browser_state()\nfig = plot_graph(df, 'WHO', depth=1, seed=8)","ad9f0f10":"display(HTML('<iframe width=\"1000\" height=\"1000\" frameborder=\"0\" scrolling=\"no\" src=\"https:\/\/plotly.com\/~kfrid\/79.embed\"><\/iframe>'))","fda70b3e":"display(HTML('<iframe width=\"1000\" height=\"1000\" frameborder=\"0\" scrolling=\"no\" src=\"https:\/\/plotly.com\/~kfrid\/85.embed\"><\/iframe>'))","4b6d7a69":"The following implementation of SVO extraction is based on textacy method but extendent to get phrases modified by adjectives. Also, objects consisting of verbs were removed","d18448c0":"'text' attributes consists of triplet (raw, cleaned, tokenized)","37084b31":"The number of scientific papers related to virusology is growing every year.\n        The United States of America have been obtaining a leading position in the virusology research area from the beggining of observations alongside with Canada until 2002. After outbreak of SARS in 2002, China plays a crucial role in the number of publications. As we can see, the SARS outbreak became a reason for the increasing interest in research in many other countries e.g. Japan, India, Taiwan, Viet Nam, Hong Kong, Singapore.\n        The 2008 H1N1 outbreak has much strengthened positions of mentioned countries. Also, it had impact on the investigations in Brasil and European countries. \n        The next significant date is related to 2013 Ebola outbreak. It made a huge impact on the epidemiology research in South Africa.","d3266b8e":"The most popular measure during the whole period of research is obviously <i>isolation<\/i>. But after pandemic outbreaks, more severe restictions gain popularity, particularly <i>quarantine<\/i> and <i>travel restrictions<\/i>. After COVID-19 outbreak, <i>contact tracing<\/i> has rose in the 3rd place among other measures. Interesting that such common measures as face <i>masks<\/i> and <i>hygiene<\/i> strenghened theirs positions only after SARS 2002 outbreak.","fad7ac7b":"This notebook aims to provide tools for data mining from texts rather than give the answers on tasks' questions.\nHere you can get tools for building statistics plots, search engine (inspired by [medium post by Josh Taylor\n](https:\/\/towardsdatascience.com\/elmo-contextual-language-embedding-335de2268604)) and knowledge graph (inspired by [medim post by Jiahui Wang](http:\/\/https:\/\/towardsdatascience.com\/python-interactive-network-visualization-using-networkx-plotly-and-dash-e44749161ed7).\n\nThis notebook is based on the 1st version of cleaned and parsed data from [the output data](https:\/\/www.kaggle.com\/xhlulu\/cord-19-eda-parse-json-and-generate-clean-csv). Data has been already cleaned from duplicates and papers with both empty titles and abstracts. \n ","ab44baa4":"Extract SVO triplets","85d75e5d":"As you can see, the response on a query <i>emotional response and psychological health of people during an outbreak<\/i> contains up to 5 sentences from each paper that have cosine similarity more than 0.7. There are <i>anxiety<\/i>, <i>fear<\/i>, <i>depressive<\/i>, <i>suicide<\/i> and <i>stress<\/i> in founded sentences. Color highlightening helps to find out words with polar sentiment.","65ef2274":"Knowledge graphs are useful structures for capturing information in machine readable format, that can be used for higher level tasks such as question answering.","e16ba78f":"Imports","8d6fe55b":"Let's find out what preventive public health measures are mentioned in research papers using regular expressions similarly to ethical concepts checking.","c19b682b":"Free space","df231579":"Embeddings are already shared","4cb45477":"Data has already been preprocessed and here you can download it Paper's objects from Google Drive","d34cb80b":"Functions for loading files from Google Drive","5d350947":"In the beginning, let's see how the research of ethics guidelines was evolved? \nLet's count the number of papers related to ethics (i.e. ones containing words ethics, ethical in a title or an abstract), also it would be interesting to see the ratio of such papers to ones published in the same year.","67542d28":"Now let's use papers' metadata to discover patterns in research development in the world. We will tag each paper with locations of authors' institutions, after that we could count the number of published papers for each location tag per year. \n        ","d3fc447c":"Now let's get embeddings of papers' text and try to find similar sentences for search queary.\n\nIn order to get and embedding for a sentence, we will clean it (remove stopwords, noise characters) and split on tokens (i.e. words). After that we can pass a sentence to the pretrained ELMo model stored on Tensorflow Hub and retrieve vector of 1024 length.\n        The advantages of such representation we will use in a construction of a simple semantic search engine. This will allow us to search through the paper text not by keywords but by semantic closeness to our search query. ","c148a63c":"As we know, there are key concepts in healthcare ethics and particulary in epidemiological ethics. They include <i>beneficence<\/i>,<i> minimising risk<\/i>, <l>justice<\/l> and <l>informed consent<\/l> (<a href='https:\/\/www.who.int\/ethics\/indigenous_peoples\/en\/index13.html'>WHO guidelines<\/a>). \n        Using simple regular expressions, let's find out which of principles are mentioned in papers annually. \n        Ethical concepts start to be widely used in 1990s. Top 3 the most popular concepts before SARS outbreak in 2002 include <i>minimizing risk<\/i>, <i>beneficence<\/i> and <i>informed consent<\/i>. \n        After SARS and Ebola outbreaks we can observe the diversity of concepts, e.g. transparency, confidentiality and safeguarding have appeared.","733467bd":"The first plot contains SVO triplets where <i>'WHO'<\/i> (World Health Organization) is a subject (or the othes nodes are subjects). Here we can see that <i>'WHO recommended measures'<\/i>, <i>'WHO have recently established framework'<\/i>, <i>'WHO released protocol'<\/i>. Also, such phrases as <i>'countries'<\/i>, <i>'cases'<\/i>, <i>'outbreak'<\/i>, <i>'disease'<\/i>, <i>'pneumonia'<\/i> much more connections than other prases in papers.","175e33ad":"The second plot is dedicated to <i>'COVID-19'<\/i> connections. Here the most popular phrases are <i>'health emergency'<\/i>, <i>'symptoms'<\/i>, <i>'incubation period'<\/i>. We can make insights about the virus: <i>'COVID-19 could damage placenta'<\/i>, <i>'COVID-19 is causing losses'<\/i>, <i>'countries will experience deaths'<\/i>.","f151a160":"Classes Paper and TextProcessor for storing and processing papers","1945a80a":"The following plot shows that the ratio of papers related to ethics did not exceed 3%, but it is increasing last 6 years. There is an obvious direct dependency between amount  of ethics papers and amount of research papers at all. The more researches are conducted, the more ethical guidelines are developed.","0dc6ce3d":"Sometimes it's important to know sentiment of a phrase: is it positive, negative or neutral. In such cases we can use nltk implementation of sentiment analyzer from VADER (Valence Aware Dictionary and sEntiment Reasoner) \u2013 a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media. It provides us with sentiment scores to 4 classes of phrase sentiments: 'pos' (positive), 'neg' (negative), 'neu' (neutral) and 'compound'. The compound score is computed by summing the valence scores of each word in the lexicon, adjusted according to the rules, and then normalized to be between -1 (most extreme negative) and +1 (most extreme positive).\n        To illustrate sentiment score of a phrase we can interpret sentiment values as RGB components (negative-positive-neutral).\n        On the following screen record we included title of a paper with similar sentences, similarity score and sentence with sentiment analyzed words."}}