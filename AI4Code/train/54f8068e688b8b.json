{"cell_type":{"f8cfd9de":"code","a943d5fd":"code","b4e143c3":"code","3f7889b5":"code","36fcfd5a":"code","3ce0a8f6":"code","17d4ca12":"code","eb5c018e":"code","b9fe49a8":"code","d70d0efb":"code","92e855e4":"code","136fa6cb":"code","74a6c3c5":"code","bb3a56c3":"code","42c4be44":"code","865626cf":"code","fb2544d5":"code","8d6e2572":"code","676e8eb6":"code","4045a30f":"code","78f420b5":"code","35d2f3ba":"code","1f69a651":"code","560edfb5":"code","011bd0c0":"code","039d833d":"code","b259faa9":"code","00b55667":"code","5afc9913":"code","50286645":"code","b22e1e00":"code","89831558":"code","21a4473e":"code","e1630a57":"code","9101391c":"code","1a564dc0":"code","3156e163":"code","87602590":"code","b425b231":"code","447abeea":"code","65867b7a":"code","401aa363":"code","64c0bf0a":"code","43676c74":"code","75e1af86":"code","d5839608":"code","9cd79041":"code","dd94c637":"markdown","15eeb6b9":"markdown","e44149b3":"markdown","3f3583c4":"markdown","722e0dbe":"markdown","5befaed3":"markdown","73a43e69":"markdown","02d4b3cf":"markdown","0b160be1":"markdown","30643aea":"markdown","6af109e9":"markdown","97c3e70c":"markdown","6d6e1294":"markdown","111325ad":"markdown","540c4288":"markdown","2dc6bed6":"markdown","d4a51d66":"markdown","b309c394":"markdown","65b71513":"markdown","1740ca39":"markdown","07dfdbf0":"markdown","44dda5a0":"markdown","e72bba8c":"markdown","6cc427f1":"markdown","dd362c18":"markdown","0b2328ac":"markdown","33e8b69e":"markdown","6f4eae39":"markdown","7f9e5a2c":"markdown","36e99885":"markdown","408ba45d":"markdown","4bd819a1":"markdown","3559baf6":"markdown","4d8a3e8b":"markdown","2f9befe7":"markdown","0a0a6fc7":"markdown","5c283308":"markdown","b8ec98c2":"markdown","40c50d6f":"markdown","f9076488":"markdown","0f8bc06d":"markdown","bd12c02b":"markdown","6bb5e330":"markdown","b1fea08d":"markdown","3d4b99bb":"markdown","08ae5562":"markdown","84937e6a":"markdown","70c63bdc":"markdown","d3e7ab13":"markdown","ebfd2f59":"markdown","e7e2853e":"markdown","1ad08eeb":"markdown"},"source":{"f8cfd9de":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor as rf_reg\nfrom sklearn.model_selection import RandomizedSearchCV as randomCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn import feature_selection\nfrom sklearn.linear_model import LinearRegression as l_reg\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import r2_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_validate\nfrom plotnine import *\nfrom matplotlib import gridspec\nfrom sklearn.preprocessing import StandardScaler\nimport pprint","a943d5fd":"energy_df=pd.read_csv(r'..\/input\/eergy-efficiency-dataset\/ENB2012_data.csv')\nenergy_df.head()","b4e143c3":"energy_df.columns=[\"relative_compactness\",\"surface_area\",\"wall_area\",\"roof_area\",\"overall_height\",\"orientaion\",\n                   \"glazing_area\",\"glazing_area_dist\",\"heating_load\",\"cooling_load\"]","3f7889b5":"energy_df.describe()","36fcfd5a":"energy_df.loc[energy_df[\"glazing_area\"]==0].describe()","3ce0a8f6":"energy_df.loc[energy_df[\"glazing_area_dist\"]==0].describe()","17d4ca12":"energy_df.hist(figsize=(15,15))\nplt.show()","eb5c018e":"energy_df[\"log_heating_load\"]=np.log(energy_df[\"heating_load\"])\nenergy_df[\"log_heating_load\"].hist(bins=6)\nplt.show()","b9fe49a8":"energy_df[\"log_cooling_load\"]=np.log(energy_df[\"cooling_load\"])\nenergy_df[\"log_cooling_load\"].hist(bins=6)\nplt.show()","d70d0efb":"sns.pairplot(energy_df)\nplt.show()","92e855e4":"corr = energy_df.corr()\nmask = np.zeros_like(corr, dtype=bool)\nmask[np.triu_indices_from(mask)] = True\nf, ax = plt.subplots(figsize=(12, 10))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, mask=mask,cmap=cmap, vmax=.9, center=0, square=True, linewidths=.5, annot=True,cbar_kws={\"shrink\": .5})\nplt.show()","136fa6cb":"energy_df_f=energy_df.copy()\nenergy_df_f.drop([\"heating_load\",\"cooling_load\"],axis=1,inplace=True)\n#energy_df_f.drop([\"log_heating_load\",\"cooling_load\"],axis=1,inplace=True)\n\nenergy_X=energy_df_f.iloc[:,:-2]\nenergy_Y=energy_df_f.loc[:,[\"log_heating_load\"]]\n#energy_Y=energy_df_f.loc[:,[\"heating_load\"]]\n\nenergy_train_X,energy_test_X,energy_train_Y,energy_test_Y=\\\ntrain_test_split(energy_X,energy_Y,test_size=0.20,random_state=48)\n\nprint(energy_train_X.shape)\nprint(energy_test_X.shape)","74a6c3c5":"def rf_regr_cv_model(min_sample_split_in,min_sample_leaf_in,max_feature_in):\n    rf_grid={\"min_samples_split\":min_sample_split_in,\n             \"min_samples_leaf\":min_sample_leaf_in,\"max_features\":max_feature_in}\n    regr = rf_reg(max_depth=3, random_state=48)\n    rf_reg_cv = randomCV(regr, rf_grid, random_state=48,scoring='neg_root_mean_squared_error',cv=5)\n    return rf_reg_cv","bb3a56c3":"min_sample_split=np.arange(10,35,5)\nmin_sample_leaf=np.arange(10,35,5)\nmax_feature=np.arange(3,7,1)\n\nrf_reg_search=rf_regr_cv_model(min_sample_split_in=min_sample_split,min_sample_leaf_in=min_sample_leaf,\n                            max_feature_in=max_feature)","42c4be44":"rf_reg_search.fit(energy_train_X,np.ravel(energy_train_Y))","865626cf":"print(\"Best parameters set:\",rf_reg_search.best_params_)\nprint(\"Best score:\",rf_reg_search.best_score_)","fb2544d5":"regr_rf_best=rf_reg(min_samples_split= 30, min_samples_leaf=10, max_features=6,max_depth=3,random_state=48)\nregr_rf_best.fit(energy_train_X,np.ravel(energy_train_Y))","8d6e2572":"predicted_train_Y=regr_rf_best.predict(energy_train_X)\npredicted_test_Y=regr_rf_best.predict(energy_test_X)\nprint(\"RMSE for Train set:\",MSE(predicted_train_Y,energy_train_Y,squared=False))\nprint(\"RMSE for Test set:\",MSE(predicted_test_Y,energy_test_Y,squared=False))","676e8eb6":"feature_list=list(energy_train_X.columns)\nfeature_impt=list(regr_rf_best.feature_importances_)\nfeature_impt_dict=dict(zip(feature_list,feature_impt))\nfeature_impt_dict=dict(sorted(feature_impt_dict.items(), key=lambda item: item[1],reverse=True))\nfeature_impt_dict","4045a30f":"final_feature_list=[\"relative_compactness\",\"overall_height\",\"glazing_area\",\"wall_area\",\"roof_area\",\"surface_area\"]","78f420b5":"def l_reg_cv(train_X,train_Y,feature_list):\n    rmse_list_train=[]\n    rmse_list_test=[]\n    r2_list_train=[]\n    r2_list_test=[]\n    for i in range(1,len(feature_list)+1):\n        train_X_temp=train_X.loc[:,feature_list[:i]]\n        cv_results_temp = cross_validate(l_reg(), train_X.loc[:,final_feature_list[:i]],train_Y, \n                            cv=5,scoring=[\"neg_root_mean_squared_error\",\"r2\"],return_train_score=True)\n        mean_rmse_train=np.mean(cv_results_temp[\"train_neg_root_mean_squared_error\"])\n        mean_r2_train=np.mean(cv_results_temp[\"train_r2\"])\n        mean_rmse_test=np.mean(cv_results_temp[\"test_neg_root_mean_squared_error\"])\n        mean_r2_test=np.mean(cv_results_temp[\"test_r2\"])\n        rmse_list_train.append(mean_rmse_train)\n        r2_list_train.append(mean_r2_train)\n        rmse_list_test.append(mean_rmse_test)\n        r2_list_test.append(mean_r2_test)\n        rmse_df=pd.DataFrame(zip(rmse_list_train,rmse_list_test,r2_list_train,r2_list_test))\n        rmse_df.columns=[\"Mean RMSE Train\",\"Mean RMSE Test\",\"Mean R2 Train\",\"Mean R2 Test\"]\n        rmse_df.index=rmse_df.index+1\n    return rmse_df","35d2f3ba":"rmse_cv=l_reg_cv(energy_train_X,np.ravel(energy_train_Y),final_feature_list)\nrmse_cv","1f69a651":"fig,ax=plt.subplots(1,2,figsize=(10,5))\nsns.lineplot(data=rmse_cv.iloc[:,:2],ax=ax[0])\nsns.lineplot(data=rmse_cv.iloc[:,2:],ax=ax[1])\nax[0].set_title(\"Mean Negative RMSE \\n Based on Number of Features\")\nax[1].set_title(\"Mean R2 Based on Number of Features\")\nplt.show()","560edfb5":"l_reg_best=l_reg()\nl_reg_best.fit(energy_train_X.loc[:,final_feature_list[:5]],np.ravel(energy_train_Y))\npred_train_Y_best=l_reg_best.predict(energy_train_X.loc[:,final_feature_list[:5]])\npred_test_Y_best=l_reg_best.predict(energy_test_X.loc[:,final_feature_list[:5]])","011bd0c0":"print(\"RMSE for Train set:\",MSE(pred_train_Y_best,energy_train_Y,squared=False))\nprint(\"RMSE for Test set:\",MSE(pred_test_Y_best,energy_test_Y,squared=False))","039d833d":"print(\"R2 for Train set:\",r2_score(pred_train_Y_best,energy_train_Y))\nprint(\"R2 for Test set:\",r2_score(pred_test_Y_best,energy_test_Y))","b259faa9":"dict(zip(final_feature_list[:5],np.exp(l_reg_best.coef_)))","00b55667":"def predictVSactual(actual_y,y_predict,title_label):\n    fig,ax=plt.subplots(1,len(actual_y),figsize=(15,15))\n    for i,col in enumerate(actual_y,0):\n        ax[i].plot(np.ravel(actual_y[i]),\n                   np.ravel(y_predict[i]),'o',markeredgecolor=\"black\")\n        ax[i].set_title(title_label[i])\n        ax[i].set_xlabel('Actual Values')\n        ax[i].set_ylabel('Predicted Values')\n        ax[i].set(aspect='equal')\n        x=ax[i].get_xlim()\n        y=ax[i].get_xlim()\n        ax[i].plot(x,y, ls=\"--\", c=\".3\")\n    return fig,ax\n#+np.random.normal(0.1, 0.005,len(actual_y[i]))","5afc9913":"actual_y_energy=[energy_train_Y,energy_test_Y]\npred_y_energy=[pred_train_Y_best,pred_test_Y_best]","50286645":"predictVSactual(actual_y_energy,pred_y_energy,\n                [\"Scatter Plot: Prediction Comparison (Train)\",\"Scatter Plot: Prediction Comparison (Test)\"])\nplt.show()","b22e1e00":"def residual_plot(actual_y,predict_y,title_label):\n    fig,ax=plt.subplots(1,len(actual_y),figsize=(10,5))\n    for i,col in enumerate(actual_y,0):\n        sns.residplot(x=actual_y[i], y=predict_y[i], lowess=True, color=\"g\",ax=ax[i])\n        ax[i].set_title(title_label[i])\n    return fig,ax","89831558":"residual_plot(actual_y_energy,pred_y_energy,[\"Train\",\"Test\"])\nplt.show()","21a4473e":"raw_pred_err_list=[]\n\nfor i in range(0,len(actual_y_energy)):\n    list_temp=[]\n    list_temp=actual_y_energy[i].to_numpy().ravel()-pred_y_energy[i]\n    raw_pred_err_list.append(list_temp)\nraw_pred_err_label=[\"Raw Prediction Errors (Train)\",\"Raw Prediction Errors (Test)\"]","e1630a57":"def raw_predict_err_hist(err_predict_list,bin_no,title_label):\n    fig,ax=plt.subplots(1,len(err_predict_list),figsize=(10,5))\n    for i,col in enumerate(err_predict_list,0):\n        sns.histplot(x=err_predict_list[i],bins=bin_no,kde=True,ax=ax[i])\n        ax[i].set_title(title_label[i])\n    return fig,ax","9101391c":"raw_predict_err_hist(raw_pred_err_list,bin_no=7,title_label=raw_pred_err_label)\nplt.show()","1a564dc0":"energy2_X=energy_df_f.iloc[:,:-2]\nenergy2_Y=energy_df_f.loc[:,[\"log_cooling_load\"]]\n#energy_Y=energy_df_f.loc[:,[\"heating_load\"]]\n\nenergy2_train_X,energy2_test_X,energy2_train_Y,energy2_test_Y=\\\ntrain_test_split(energy2_X,energy2_Y,test_size=0.20,random_state=48)","3156e163":"rmse_cv2=l_reg_cv(energy2_train_X,np.ravel(energy2_train_Y),final_feature_list)\nrmse_cv2","87602590":"fig,ax=plt.subplots(1,2,figsize=(10,5))\nsns.lineplot(data=rmse_cv2.iloc[:,:2],ax=ax[0])\nsns.lineplot(data=rmse_cv2.iloc[:,2:],ax=ax[1])\nax[0].set_title(\"Mean Negative RMSE \\n Based on Number of Features\")\nax[1].set_title(\"Mean R2 Based on Number of Features\")\nplt.show()","b425b231":"l_reg2_best=l_reg()\nl_reg2_best.fit(energy2_train_X.loc[:,final_feature_list[:5]],np.ravel(energy2_train_Y))\npred2_train_Y_best=l_reg2_best.predict(energy2_train_X.loc[:,final_feature_list[:5]])\npred2_test_Y_best=l_reg2_best.predict(energy2_test_X.loc[:,final_feature_list[:5]])","447abeea":"print(\"RMSE for Train set:\",MSE(pred2_train_Y_best,energy2_train_Y,squared=False))\nprint(\"RMSE for Test set:\",MSE(pred2_test_Y_best,energy2_test_Y,squared=False))","65867b7a":"print(\"R2 for Train set:\",r2_score(pred2_train_Y_best,energy2_train_Y))\nprint(\"R2 for Test set:\",r2_score(pred2_test_Y_best,energy2_test_Y))","401aa363":"actual2_y_energy=[energy2_train_Y,energy2_test_Y]\npred2_y_energy=[pred2_train_Y_best,pred2_test_Y_best]\n\npredictVSactual(actual2_y_energy,pred2_y_energy,\n                [\"Scatter Plot: Prediction Comparison (Train)\",\"Scatter Plot: Prediction Comparison (Test)\"])\nplt.show()","64c0bf0a":"residual_plot(actual2_y_energy,pred2_y_energy,[\"Train\",\"Test\"])\nplt.show()","43676c74":"raw_pred2_err_list=[]\n\nfor i in range(0,len(actual2_y_energy)):\n    list_temp=[]\n    list_temp=actual2_y_energy[i].to_numpy().ravel()-pred2_y_energy[i]\n    raw_pred2_err_list.append(list_temp)\nraw_pred2_err_label=[\"Raw Prediction Errors (Train)\",\"Raw Prediction Errors (Test)\"]","75e1af86":"raw_predict_err_hist(raw_pred2_err_list,bin_no=8,title_label=raw_pred2_err_label)\nplt.show()","d5839608":"print(\"Log heating load as dependent variable:\")\ndict(zip(final_feature_list[:5],np.exp(l_reg_best.coef_)))","9cd79041":"print(\"Log cooling load as dependent variable:\")\ndict(zip(final_feature_list[:5],np.exp(l_reg2_best.coef_)))","dd94c637":"# Data Loading","15eeb6b9":"# Data Exploration & Transformation","e44149b3":"Looking at the table and graphs above, the appropriate number of features are 5 as using 6 features do not show any great improvement on RMSE and R2 for both train and test datasets. ","3f3583c4":"Based on the 2 summary tables above, when glazing area or glazing area distribution is 0, the other also contain value 0. So, if glazing area or glazing area distribution are valued at 0 for a building, it can be assumed that the building itself do not have glazing area. ","722e0dbe":"## Forward Selection Linear Regression","5befaed3":"Similar to using log heating load as dependent variable, the model seems to be underestimated the cooling load as most points are at the right side of the diagonal line. ","73a43e69":"## Feature Selection","02d4b3cf":"relative compactness is highly correlated to surface area, roof area and overall height. Therefore, feature selection is required to reduce the number of features that are highly correlated.\n\nheating load is highly correlated to cooling load which suggested that only 1 of them can be used as dependent factor to determine energy efficiency of the building. Therefore, log heating load is selected as dependent variable to determine energy efficiency in term of heating load.  ","0b160be1":"Based on the coefficients above:\n\n1. Cooling load will be reduced by a multiplicative factor of 0.36 when relative compactness increases by 1.\n2. Cooling load will be increased by a multiplicative factor of 1.23 when overall height increases by 1.\n3. Cooling load will be increased by a multiplicative factor of 1.88 when glazing area increases by 1.\n4. Wall area and roof area do affect the cooling load but lesser magnitude compared to the previous 3 factors.\n\nTherefore, lower building with high relative compactness and small glazing area requires less energy to cool down the indoor environment. Smaller wall area and bigger roof area also reduce energy required to cool down the indoor environment.","30643aea":"Looking at heating load and cooling, they seem to be heavily skewed to the right. Therefore, log transformation will be done on heating load and cooling load to make them more normalised in term of distribution","6af109e9":"The range of the residuals is between -0.3 to 0.3. The residual plots do not show any particular trends in the residuals. ","97c3e70c":"Looking at the scatter plot, the model seems to be underestimated the heating load as more points situated at the right side of the diagonal line. ","6d6e1294":"Looking at the list above, relative compactness, surface area, overall height, roof area and glazing area are the top 5 features. However, there are 2 pairs of variables that are high correlated:\n1. relative compactness with surface area\n2. roof area with surface area\n3. overall height with roof area\n4. relative compactness with overall height\n","111325ad":"R2 for test is lower when using log cooling load as dependent variable instead of log heating load. ","540c4288":"The function above is to do a linear regression model fitting based on the feature list. The data with independent variables (train_X) will be sliced according to the variables in the feature list and fit into the model with dependent variable. Then, RMSE and R2 will be calculated for each set of independent variables fitted into the model to find out which set of independent variables is the most optimal to fit into the model. ","2dc6bed6":"For this section, cooling load is used to find out the relationships for the same features with cooling load. ","d4a51d66":"The function above is to plot two scatter plots side by side with train on the left and test on the right using actual values and predicted values that store in list as inputs. ","b309c394":"## Randomised Grid Search for Random Forest Regressor","65b71513":"# Interpretation on Regression Coefficients","1740ca39":"This section will explore data through summary table, histogram and correlation matrix to understand the data, explore the relationship between the variables and check whether the data contain any missing values. ","07dfdbf0":"Based on the randomised CV search result above, the model performs the best when:\n1. the sample split at 30 \n2. each leaf has 10 instances\n3. 6 features are used\n\nThe model is refitted with the best set of hyperparameters found in randomised CV search.","44dda5a0":"Using the dataset above with log heating load as the main dependent variable, the data is split into train and test datasets with the ratio of 80:20. After splitting, feature selection and model fitting using linear regression will be done to test the performance of linear regression after feature selection.","e72bba8c":"Based on the coefficients above:\n1. Heating load will be increased by a multiplicative factor of 2.43 when relative compactness increases by 1. \n2. Heating load will be increased by a multiplicative factor of 1.30 when overall height increases by 1. \n3. Heating load will be increased by a multiplicative factor of 2.86 when glazing area increases by 1. \n4. Wall area and roof area affect the heating load but lesser magnitude compared to the previous 3 factors. ","6cc427f1":"Based on the coefficients above:\n\n1. Heating load will be increased by a multiplicative factor of 2.43 when relative compactness increases by 1.\n2. Heating load will be increased by a multiplicative factor of 1.30 when overall height increases by 1.\n3. Heating load will be increased by a multiplicative factor of 2.86 when glazing area increases by 1.\n4. Wall area and roof area do affect the heating load but lesser magnitude compared to the previous 3 factors.\n\nTherefore, lower building with low relative compactness and small glazing area requires less energy to warm up the indoor environment. Smaller wall area and roof area also reduce energy required to warm up the indoor environment.","dd362c18":"Looking at the histograms above, the residuals are approximately normally distributed. The prediction errors seem to be larger using log cooling load compared to log heating load.","0b2328ac":"Train dataset has 614 instances while test dataset has 154 instances with 8 variables including log heating load after dropping heating load and cooling load. ","33e8b69e":"Based on the counts in the summary table above, there are no missing values for each variable as the counts for each variable are the same. However, glazing area and glazing area distribution have values 0 for some instances. ","6f4eae39":"This section will conduct feature selection using weight importance calculated from the random forest regressor to select the features to be used in linear regression","7f9e5a2c":"Looking at the residual plots, they indicated that the residuals are in the range of -0.35 to 0.4. There are some outliers in train and test datasets as there are some instances with residuals greater than 0.3 or lesser than -0.3. The residual plots do not show any particular trends in the residuals. ","36e99885":"Using the rf_regr_cv_model and the hyperparameters declared, train dataset will be fitted into the model to determine the best set of hyperparameters for the model. ","408ba45d":"# Using Energy Efficiency Dataset for Linear Regression","4bd819a1":"The difference between test and train in RMSE is at least 0.01 which is quite small. ","3559baf6":"This section will be refitted the linear regression model using the top 5 features in the previous section.","4d8a3e8b":"This section will look at the prediction performance of the model by plotting scatter plot for comparison between actual and predicted values, histogram for prediction errors and residual plot. ","2f9befe7":"This section is to load the data into the notebook for data exploration and statistical modelling purposes.","0a0a6fc7":"Adding column name for each column in the dataframe for clearer understanding and easier data slicing using pandas.","5c283308":"## Best Fit Linear Regression Model","b8ec98c2":"# Using Raw Data (Log_heating_load)","40c50d6f":"After log transformation on heating and cooling loads, both variables' distributions look better but both show bimodal distribution as two peaks are formed. ","f9076488":"The difference between test and train is at least 0.02 which is quite small.\n\nTherefore, the current model should be sufficient to predict the energy efficiency of a building in term of log heating load as the model can explain 90% of the variation in the data according to R2 and has small RMSE. ","0f8bc06d":"## Model Diagnostics","bd12c02b":"# Using Raw Data (Log_cooling_load)","6bb5e330":"Looking at the model performance in term of RMSE, the model seems to be overfitting as RMSE in test is higher than in train. But, the difference is quite small, around 0.02.","b1fea08d":"## Model Diagnostics","3d4b99bb":"Looking at the graphs and the table above, linear regression with 5 features seems to be better as it has higher negative mean RMSE and higher mean R2 in test set compared to others. Furthermore, means for negative RMSE and R2 in test slightly decreased when using 6 features. \n\nTherefore,the most optimal number of features to be used in linear regression is 5 and the selected 5 features are relative_compactness, overall_height, glazing_area, wall_area and roof_area.","08ae5562":"Looking at the histogram above, the residuals are normally distributed with long left tails. Most prediction errors are in the range of -0.1 to 0.1.","84937e6a":"This section will conduct linear regression model fitting using forward selection by adding variables one by one into the model. The best model will be the one with the biggest negative RMSE and biggest r2. ","70c63bdc":"The difference between test and train in RMSE is at least 0.01.","d3e7ab13":"This section will use the data with log heating load as the main dependent variable to do a randomised cross validation search for random forest regressor to tune the hyperparameters for random forest regressor. The random forest regressor is used as feature selection model as the model can calculate the weight importance for each variable based on the proportion of each variable is used in the model to partition the data in a way that the predicted value is closer to the actual value in dependent variable.","ebfd2f59":"The data source is from https:\/\/archive.ics.uci.edu\/ml\/datasets\/Energy+efficiency\n\nThis notebook will explore the dataset and use linear regression to explain the relationship between the independent variables and dependent variable (heating load) with random forest regressor to be used as a model for feature selection. ","e7e2853e":"A feature list is created with the roof area and surface area as the last 2 features as linear regression will be done using forward selection by adding variables one by one according to the feature list and selecting the smallest RMSE and biggest r2.","1ad08eeb":"The function above is to create a randomised cross-validation search process using random forest regressor as base model to tune the hyperparameters in the model. The hyperparameters to be tuned are minimum sample split, minimum sample size in the leaf and maximum features to be used in each regression tree. The model is regularised with maximum depth of 3 and using seed number 48 to prevent overfitting. Root mean squared error (RMSE) is used as the scoring criteria to determine the best set of hyperparameters."}}