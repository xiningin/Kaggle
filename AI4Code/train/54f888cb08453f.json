{"cell_type":{"048cc53d":"code","e76bef80":"code","ae9e6e42":"code","46838cdb":"code","588c9a02":"code","335b346c":"code","cd30c365":"code","94bf3e9a":"code","c7181bb3":"code","3214d551":"code","370c723e":"code","6a8ed4c3":"code","92927beb":"code","545590b7":"code","724def56":"code","c772aa02":"code","894a3207":"code","0d9eca6d":"code","bfb8014c":"code","30dc5da5":"code","bbec282a":"code","27c8238a":"code","b5814e73":"code","c83cb82d":"code","7d22890d":"code","8743c784":"code","0728b774":"code","276d96db":"code","938e060d":"code","3102b281":"code","e45881fa":"code","f396e5af":"code","0b685ce5":"code","7aa407b5":"code","8c8ae96e":"code","ff7da368":"code","5dffc6b0":"markdown","b7a80527":"markdown","87e53159":"markdown","e4f542f2":"markdown","3e699fb3":"markdown","d9f07514":"markdown","10320341":"markdown","0d5b8439":"markdown","d2f36298":"markdown","62d6e040":"markdown","8643aad0":"markdown","ea36c678":"markdown","d74eca47":"markdown","a094aeaf":"markdown","f05e2b71":"markdown","9b813cb6":"markdown","03fe98ce":"markdown","f0666915":"markdown","123b5a9a":"markdown","00bc8e44":"markdown","11603a0f":"markdown","4bb9b2db":"markdown","adc7f1e8":"markdown","3fbd9edc":"markdown","53018bc1":"markdown","f209f684":"markdown","bf140f8a":"markdown","4bf798a9":"markdown","3f778f3c":"markdown","b472c007":"markdown","395eaab9":"markdown","79203bb4":"markdown","69096694":"markdown","dccf0cf8":"markdown"},"source":{"048cc53d":"# Supress Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#core imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly.offline import plot, iplot, init_notebook_mode\nimport plotly_express as px\ninit_notebook_mode(connected=True)\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","e76bef80":"#load dataset and assign it to a variable\nwine=pd.read_csv(\"..\/input\/wine-reviews\/winemag-data-130k-v2.csv\")","ae9e6e42":"#use the 'head' method to show the first five rows of the table as well as their names. \nwine.head() ","46838cdb":"# shape of the wine-reviews dataframe\nwine.shape","588c9a02":"wine.info()","335b346c":"# checking out total number of rows with same title and description values.[dUPLICATE cHECKER] \nwine[wine.duplicated(subset=['title','description'])].shape[0]","cd30c365":"wine.drop_duplicates(subset=['title','description'],inplace=True)\nwine.shape[0]","94bf3e9a":"# dropping columns'description' since we are not gonna implement NLP in this kernel for now.\nwine.drop(columns=['description'],inplace=True)\n# dropping 'Unnamed: 0' since the ids won't contribute or affect the result of regression model.\nwine.drop(columns=['Unnamed: 0'],inplace=True)\nwine.head(5)","c7181bb3":"# Getting the insight of null values present in the feature columns\nwine.isnull().sum()","3214d551":"wine_price=wine.groupby('variety')['price'].describe()\n\n#filling null values in price column with the median price value for each variety\ndef fill_pricenull(x):\n    var=x[0]\n    price=x[1]\n    if pd.isnull(price):\n        return round(wine_price['mean'].loc[var],2)\n    else:\n        return round(price,2)\n\nwine['price']=wine[['variety','price']].apply(fill_pricenull,axis=1)\nwine.head()","370c723e":"wine.isnull().sum()","6a8ed4c3":"wine=wine[wine['price'].notna()]\n#wine.head()","92927beb":"print('Avg. Wine Points: ',wine['points'].mean())\n","545590b7":"plt.figure(figsize=(12,8))\nfig = sns.countplot(wine['points'], color=\"darkred\")\nfig.set_xlabel(\"Points\",size=15)\nfig.set_ylabel(\"#\",size=15)\nplt.title('Wine Points Distribution',size = 25)\nplt.show()","724def56":"print('Avg. Wine Price: ',wine['price'].mean())\n","c772aa02":"plt.figure(figsize=(12,8))\nfig = sns.distplot(wine['price'], color=\"darkorange\")\nfig.set_xlabel(\"Price\",size=15)\nfig.set_ylabel(\"Probability Density\",size=15)\nplt.title('Wine Price Distribution',size = 25)\nplt.show()","894a3207":"plt.figure(figsize=(12,8))\nfig = sns.distplot(wine[wine['price']<250.00]['price'], color=\"darkorange\")\nfig.set_xlabel(\"Price<250.00\",size=15)\nfig.set_ylabel(\"Probability Density\",size=15)\nplt.title('Wine Price Distribution-Scaled',size = 25)\nplt.show()","0d9eca6d":"plt.figure(figsize=(12,8))\nfig=sns.regplot(x='price',y='points',data=wine,color='darkblue')\nfig.set_xlabel(\"Price\",size=15)\nfig.set_ylabel(\"Points\",size=15)\nplt.xticks(size=10)\nplt.yticks(size=10)\nplt.title('Price vs Points',size = 20)\nplt.tight_layout()","bfb8014c":"#temp dataframe to get the grouped countrywise mean wine-price\nwi_country=wine.groupby('country').mean().reset_index()\n\nplt.figure(figsize=(12,8))\nfig=sns.barplot(wi_country['country'],wi_country['price'],palette='Reds')\nfig.set_xlabel(\"Country\",size=15)\nfig.set_ylabel(\"Mean Price\",size=15)\nplt.xticks(size=10,rotation=90)\nplt.yticks(size=10)\nplt.title('Mean Wine Price[Country-wise Distribution]',size = 20)\nplt.tight_layout()","30dc5da5":"plt.figure(figsize=(12,8))\nfig=sns.barplot(wi_country['country'],wi_country['points'],palette='Reds')\nfig.set_xlabel(\"Country\",size=15)\nfig.set_ylabel(\"Mean Points\",size=15)\nplt.xticks(size=10,rotation=90)\nplt.yticks(size=10)\nplt.title('Mean Wine Points[Country-wise Distribution]',size = 20)\nplt.tight_layout()","bbec282a":"plt.figure(figsize=(12,8))\nfig=sns.barplot(x='country',y='points',data=wi_country.sort_values(by=['points'],ascending=False).head(5),palette='Reds')\nfig.set_xlabel(\"Country\",size=15)\nfig.set_ylabel(\"Mean Points\",size=15)\nplt.xticks(size=10,rotation=90)\nplt.yticks(size=10)\nplt.title('Mean Wine Points[Top 5 countries]',size = 20)\nplt.tight_layout()","27c8238a":"wi_variety=wine.groupby('variety')[['price']].mean().reset_index().sort_values(by=['price'],ascending=False)\nwi_variety=wi_variety.head(10)\n\nplt.figure(figsize=(12,8))\nfig=sns.barplot(wi_variety['price'],wi_variety['variety'],palette='Reds')\nfig.set_ylabel(\"variety\",size=15)\nfig.set_xlabel(\"Mean Price\",size=15)\nplt.yticks(size=10)\nplt.xticks(size=10)\nplt.title('Mean Wine Price[Top 10 Varieties]',size = 20)\nplt.tight_layout()","b5814e73":"wi_variety=wine.groupby('variety')[['points']].mean().reset_index().sort_values(by=['points'],ascending=False)\nwi_variety=wi_variety.head(10)\n\nplt.figure(figsize=(12,8))\nfig=sns.barplot(wi_variety['points'],wi_variety['variety'],palette='Reds')\nfig.set_ylabel(\"variety\",size=15)\nfig.set_xlabel(\"Mean Points\",size=15)\nplt.yticks(size=10)\nplt.xticks(size=10)\nplt.title('Mean Wine Points[Top 10 Varieties]',size = 20)\nplt.tight_layout()","c83cb82d":"# We're splitting up our data set into groups called 'train' and 'test'\nfrom sklearn.model_selection import train_test_split\n\n# we will toss out the target variable 'points' from our input data features\nX=wine.drop(columns=['points'])\n\n#Filling the null values since CatBoost can't handle missing null values\nX=X.fillna(0)\n\n#To be used during Catboost's Feature Importance extractor\ncategorical_features_indices =np.where(X.dtypes == np.object)[0]\n\ny=wine['points']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n                                                    random_state=101)\n","7d22890d":"categorical_features_indices","8743c784":"from catboost import Pool, CatBoostRegressor, cv\n\nmodel = CatBoostRegressor(random_seed = 350,loss_function = 'RMSE',iterations=350)\n#fitting the train data\nmodel.fit(X_train, y_train,cat_features = categorical_features_indices,verbose=False)","0728b774":"predictions=model.predict(X_test)","276d96db":"fig=plt.figure(figsize=(8,8))\n\nplt.scatter(y_test,predictions,color='darkred')\n# Plot-label\nfig.suptitle('y_test vs predictions',fontsize = 20)\n\n#X-label\nplt.xlabel('y_test')\n\n# Y-label\nplt.ylabel('predcitions')\n","938e060d":"fig=plt.figure(figsize=(8,8))\n  \nsns.distplot((y_test-predictions),bins=20,color='darkred')\n\n#Plot Label\nfig.suptitle('Residual Analysis', fontsize = 20)   ","3102b281":"from sklearn import metrics\nprint('Mean Absolute Error     MAE:', metrics.mean_absolute_error(y_test, predictions))\nprint('Mean Squared Error      MSE:', metrics.mean_squared_error(y_test, predictions))\nprint('Root Mean Squared Error RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))","e45881fa":"errors = abs(predictions - y_test)\nmape = 100 * (errors \/ y_test)\n# Calculate and display accuracy\naccuracy = 100 - np.mean(mape)\n\n\nprint('Accuracy_CatBoost:', round(accuracy, 2))","f396e5af":"# Let's import the shap package before starting our interpretation\nimport shap","0b685ce5":"shap_values = model.get_feature_importance(Pool(X_test, label=y_test, cat_features=categorical_features_indices),type=\"ShapValues\")\nshap_values = shap_values[:, :-1]\nshap.summary_plot(shap_values, X_test, plot_type=\"bar\")","7aa407b5":"shap.summary_plot(shap_values, X_test)","8c8ae96e":"shap_values = model.get_feature_importance(Pool(X_test, label=y_test, cat_features=categorical_features_indices),type=\"ShapValues\",)\nexpected_value = shap_values[0, -1]\nshap_values = shap_values[:, :-1]\nshap.initjs()  \n \nshap.force_plot(expected_value, shap_values[20, :], X_test.iloc[20, :])","ff7da368":"shap.force_plot(expected_value, shap_values[150, :], X_test.iloc[150, :])","5dffc6b0":"> Looking at the above 5 rows of data we need to develop an understanding of every column to efficiently continue exploring the dataset further.\n> We need to have a clear understanding of every feature defined in the dataset and what it is trying to convey!","b7a80527":"# Explaining SHAP with Wine Analysis[EDA,Comparisons and Points Predictions]\n\n![](https:\/\/media-cdn.tripadvisor.com\/media\/photo-s\/10\/28\/86\/6f\/wine-cheers.jpg)\n\nIn this tutorial we will be predicting number of points, based on input features from 'wine-reviews' dataset. The model of choice is CATBoost Regressor. I will explain and analyze the outcome and predicitons using SHAP package implementation.\n\n### What is SHAP?\nTo make sure that we know what the algorithms we use actually do, we have to take a closer look at what we are actually predicting. New methods of explainable machine learning open up the possibility to explore which factors were used exhaustively by the algorithm to come to the predictions.\n\n> SHAP (SHapley Additive exPlanations) is the package by Scott M. Lundberg that is the approach to interpret machine learning outcomes.","87e53159":"> Since the difference is very hard to interpret we will analyze the top 5 countries with highest mean wine points","e4f542f2":"> Swiss wines are the most expensive because goods in that country are generally over-priced, however ***English and Indian Wines*** taste the best as per the data!! ","3e699fb3":"# Thank You\n\n> 1. **Please do highlight any irregularities found in the kernel below in the comments!**\n> 2. **Do hit upvote if you like my work on this project !!**\n> 3. **Do check for the updated version soon!**","d9f07514":"### Regression Evaluation Metrics\nHere are three common evaluation metrics for regression problems:\n\n**Mean Absolute Error** (MAE) is the mean of the absolute value of the errors:\n\n$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n\n**Mean Squared Error** (MSE) is the mean of the squared errors:\n\n$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n\n**Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\n\n$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n\nComparing these metrics:\n\n- **MAE** is the easiest to understand, because it's the average error.\n- **MSE** is more popular than MAE, because MSE \"punishes\" larger errors, which tends to be useful in the real world.\n- **RMSE** is even more popular than MSE, because RMSE is interpretable in the \"y\" units.\n\nAll of these are **loss functions**, because we want to minimize them.","10320341":"# Data Cleaning\n\n> Now we will take some steps in the direction of preparing our data for training on our CatBoost regression model. We will drop-columns that are not needed or can't be applied to model training, drop duplicate row values and overcome null values present in our feature columns.","0d5b8439":"> To get a deeper estimation and a better distribution we need to scale our data of observation.  We will drop all wines that are priced above 250","d2f36298":"### Visualizinng Row 150 using force_plot","62d6e040":"We observe that there are still null values inside the price Feature. Dropping the same.","8643aad0":"# Fetching the Data\n**Using Pandas to load the dataset into this notebook. Using pandas we can read our datafile (winemag-data-130k-v2.csv) with the line below. Data-set loaded will be assigned to the variable \"wine\"**.","ea36c678":"### 5. Variety Comparisons","d74eca47":"### 3. 'price' vs 'points' ","a094aeaf":"> 1. ***Ramisco wines*** are the most expensive on an average however mean best points goes to **Tinta del Pais and Gelber Traminer** wines.\n> 2. This simply means Ramsico is produced in countries where average price of goods is simply more and by taste there are better options.","f05e2b71":"### 3. Predictions and Evaluations\n\n> Let's grab predictions off our test set and see how well it did!\n","9b813cb6":"***Until now, the SHAP package did not show anything other algorithm libraries cannot do. Showing feature importances has already been implemented in XGBoost and CatBoost some versions ago.\nBut now let\u2019s get SHAP to shine. We enter shap.summary_plot(shap_values, X_test) and receive the following summary plot.***","03fe98ce":"> 1. ***Biggest pink block here is the \"price\" feature that is 50.***\n> 2. ***So, now we got a better look at our model with this  dataset.*** \n> 3. ***One could also explore the false predictions and get an even deeper understanding of the model***","f0666915":"> Above is a concise summary of our dataframe returning columns' data-type,index data-type and number of non-null values !","123b5a9a":"# Let us make SHAP shine!\n\nWe will now explain our prediction and interpret the model with the help of SHAP package!!","00bc8e44":"***Force Plots have been the default method for visualizing individual model predictions via the shap package.***\n\n> 1. ***In the above force plot , we can see the row at position 20 of our test dataset.*** \n> 2. ***Features that are pink contribute to the model output being higher, that means predicting a success of the Wine-Points predictions.*** \n> 3. ***Blue parts of the visualization indicate a lower model output, predicting a failed project. So the biggest block here is the feature \u2018price\u2019, which in this case is 12.*** \n> 4. ***Therefore, with this particular set of information, the price of the Wine being 12 is the most informative feature for the model.***","11603a0f":"> Ramisco wines are very ***expensive*** on an average!!","4bb9b2db":"# Imports\n\n\n> 1. Let's get our environment ready with the libraries we'll need and then import the relevant ones beforehand!\n> 2. Pandas is one of the most widely used python libraries in data science. It provides high-performance, easy to use structures and data analysis tools.\n> 3. Matplotlib is a plotting library for the Python programming language\n> 4. Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.","adc7f1e8":"> 1. Now we will drop duplicate values in our dataset.\n> 2. Duplicates are an extreme case of nonrandom sampling, and they bias our fitted model. Including them will essentially lead to the model overfitting this subset of points.","3fbd9edc":"### 4. Country-wise Comparisons","53018bc1":"> 1. ***In the above plot, the order of the columns still represents the amount of information the column is accountable for in the prediction.***\n> 2. ***Each dot in the visualization represents one prediction. The color is related to the real data point. If the actual value in the dataset was high, the color is pink; blue indicates the actual value being low. Grey represents the categorical values which cannot be scaled in high or low.*** \n> 3. ***The x-axis represents the SHAP value, which is the impact on the model output. The model output 1 equates to the prediction of successful; 0 the prediction that the project is going to fail.***\n\n### The great thing about the SHAP package is that it gives the opportunity to dive even deeper into the exploration of our model. Namely, it will give us the feature contributions for every single prediction\n\n>> 1. ***price feature contributes most to the prediction result***\n>> 2. ***country,region_2 and title the least*** ","f209f684":"> 1. Most of the wines are priced below 1000.\n> 2. Highly Priced wines score more points[Not the most smoothest trends]","bf140f8a":"> Points has closer to a normal distribution. Most number of wines have 88 points.","4bf798a9":"## Check out the Data\n**We will run some exploratory analysis on our \"wine-reviews\" dataset now that it is loaded in the wine variable.We would check for the shape of the dataset, any missing or null values and will try to find out the correlation amongst the dataset features.**\n\n**Skol!**","3f778f3c":"# Exploratory Data Analysis,Comparisons & Projections\n\nLet's create some simple plots to check out the data and relations amongs the feature columns in the Wine-reviews dataset!","b472c007":"# Training our CatBoost Regressor\nLet's now begin to train our regression model! We will need to first split up our input data into a train and test seperate arrays that contains the part of dataset used for training data and test data.\n\n### 1. Train\/Test Split","395eaab9":"> Swiss-Origin wines are highest priced on an average!!","79203bb4":"### 2. Creating and Training The Model","69096694":"### 1. Wine Points Distribution","dccf0cf8":"### 2. Wine Price Distribution"}}