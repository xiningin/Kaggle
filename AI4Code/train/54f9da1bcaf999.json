{"cell_type":{"c078cd51":"code","894ad851":"code","57309671":"code","a653c511":"code","600f7100":"code","1759a836":"code","28db2835":"code","2d914ffc":"code","8cf5089e":"code","c92d38d1":"code","fbc04448":"code","a52b41f0":"code","7cd21907":"code","7cd9868b":"code","a276bd3c":"code","c8eb64bf":"code","ea13f44b":"code","0f717b9c":"code","021281c9":"code","5447ac73":"code","dadd66c8":"code","e12a94ca":"code","a3f3e92a":"code","165280e0":"code","1e9e8d99":"code","8d211a23":"code","71382a5d":"code","3eae7abe":"code","42c2244a":"code","113bae6d":"code","60077756":"code","9ffab76e":"code","bd91c048":"code","b5dd7540":"code","93431313":"code","7932141d":"code","eb365ca4":"code","e569afb5":"code","316a3baa":"code","d0e35889":"code","5efe89a5":"code","b1dcbe58":"code","0eb79f5c":"code","1d86e235":"code","589f7c36":"code","ca1d5ee4":"code","89e213ad":"code","3b5e2669":"code","7ab0864b":"code","0c2dda54":"code","f3122f2e":"code","c258bcdf":"code","43fa966c":"code","643c3598":"code","a128334a":"code","af67973b":"code","cd22935b":"code","bddb4d65":"code","331292e0":"code","ef7af210":"code","e1b82e09":"code","e87b88f2":"code","146c8ff4":"code","871b3ff9":"code","89ae1d86":"code","e311c270":"code","17934e9a":"code","33b8b68c":"code","c402fcea":"code","2b608e78":"code","37ec7667":"code","d45e6bb2":"markdown","07fb04fb":"markdown","51477dc9":"markdown","93ba1772":"markdown","0d3293a1":"markdown","bcd52dd7":"markdown","e0770bc6":"markdown","e9e0e69c":"markdown","a8baffd4":"markdown","d0b4d6f4":"markdown","02f20b32":"markdown","1411c864":"markdown","152cc6dd":"markdown"},"source":{"c078cd51":"# See dataset description on kaggle","894ad851":"# Importing requrired packages\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC  \nfrom sklearn import svm\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport numpy as np \n%matplotlib inline ","57309671":"#Loading dataset\nwine = pd.read_csv('..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')","a653c511":"# Plot the data type\nprint('Data type: ', type(wine))\n# Plot the first five rows of the dataset\nwine.head()","600f7100":"# Check the number of values and datatypes for each column\nwine.info()","1759a836":"# Check the dataset structure\nprint(f'Number of rows: {wine.shape[0]}\\nNumber of columns: {wine.shape[1]}')","28db2835":"# Plot the statistical values for each column\nwine.describe()","2d914ffc":"#Detect missing values\nwine.isnull().sum()","8cf5089e":"# Plot each feature against quality\nfig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'fixed acidity', data = wine)","c92d38d1":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'volatile acidity', data = wine)","fbc04448":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'citric acid', data = wine)","a52b41f0":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'residual sugar', data = wine)","7cd21907":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'chlorides', data = wine)","7cd9868b":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'free sulfur dioxide', data = wine)","a276bd3c":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'total sulfur dioxide', data = wine)","c8eb64bf":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'sulphates', data = wine)","ea13f44b":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'alcohol', data = wine)","0f717b9c":"# Histograms\nwine.hist(bins=20, figsize=(15,15));","021281c9":"# Plot scatter matrix\npd.plotting.scatter_matrix(wine,  figsize=(20,20));","5447ac73":"# Plot correlation heatmap with \ncorr = wine.corr()\nplt.figure(figsize=(15,15))\nax = sns.heatmap(corr, vmin=-1, vmax=1, center=0, annot=True, square=False)\nax.set_xticklabels(ax.get_xticklabels(),rotation=45, horizontalalignment='right')\nax.set_ylim(len(corr)-0.5, -0.5)\n\n# fix for mpl bug that cuts off top\/bottom of seaborn visualization\nb, t = plt.ylim() # discover the values for bottom and top\nb += 0.5 # Add 0.5 to the bottom\nt -= 0.5 # Subtract 0.5 from the top\nplt.ylim(b, t) # update the ylim(bottom, top) values\nplt.show()","dadd66c8":"# Preprocessing Data\nbins = (0, 6.5, 8)  # Define bins\ngroup_names = ['bad', 'good'] # Define names of classes\nwine['quality'] = pd.cut(wine['quality'], bins=bins, labels=group_names) # Bin quality values into the specified bins\n# This function is also useful for going from a continuous variable to a categorical variable\nwine['quality'].unique()","e12a94ca":"label_quality = LabelEncoder() # Create instance of LabelEncoder","a3f3e92a":"wine['quality'] = label_quality.fit_transform(wine['quality']) # Transform data: bad --> 0, good --> 1","165280e0":"wine.head(10)","1e9e8d99":"# Anzahl an Werte Gut & Schlecht\nwine['quality'].value_counts()","8d211a23":"sns.countplot(wine['quality'])","71382a5d":"# Values() = von Pandas DataFrame zur\u00fcck in ein Array\nX = wine.drop('quality', axis=1).values\ny = wine['quality'].values","3eae7abe":"type(X)","42c2244a":"X.shape","113bae6d":"y.shape","60077756":"# 20% test 80% training data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","9ffab76e":"print(f'Training data: X: {X_train.shape} y: {y_train.shape}')\nprint(f'Test data: X: {X_test.shape} y: {y_test.shape}')","bd91c048":"#Applying Standard scaling\nfeature_scaler = StandardScaler()\nX_train = feature_scaler.fit_transform(X_train)\nX_test = feature_scaler.transform(X_test)","b5dd7540":"RFC_base_classifier = RandomForestClassifier(n_estimators=100)   # Define base estimator","93431313":"RFC_base_classifier.get_params()  # Show all changeable parameters","7932141d":"# Define parameter grid\n\ngrid_param_RFC = {\n    'n_estimators': [100, 120, 140, 160, 180, 200, 230, 260, 300, 500, 800, 1000],\n    'criterion': ['gini', 'entropy'],\n    'max_depth': [None, 3, 4, 5, 7, 9, 10, 15, 20],\n    'bootstrap': [True, False]\n}","eb365ca4":"# Grid Search Instanz definieren (cv=5)\n\nfrom sklearn.model_selection import GridSearchCV\n\ngrid_search_RFC = GridSearchCV(estimator=RFC_base_classifier,\n                           param_grid=grid_param_RFC,\n                           scoring='accuracy',\n                           cv=5,                     # CV=5 --> 5-fold cross-validation\n                           n_jobs=-1)","e569afb5":"grid_search_RFC.fit(X_train, y_train)","316a3baa":"# Show best parameter outcome\n\nbest_parameters_RFC = grid_search_RFC.best_params_\nprint(f'Best parameters: {best_parameters_RFC}')","d0e35889":"# Mean cross-validation score of best model\n\nbest_result_RFC = grid_search_RFC.best_score_\nprint(f'Mean cross-validation score of best model: {best_result_RFC}')","5efe89a5":"SVC_base_classifier = SVC()   # Define base estimator","b1dcbe58":"SVC_base_classifier.get_params()  # Show all changeable parameters","0eb79f5c":"# Due to the fact that the calculation and therefore the grid search with SVM is computationally very\n# expensive, we get ourselves a first look at good parameter values with help of the RandomizedSearch\n# Instead of specifiing discrete values for the parameters 'C', 'gamma' and 'degree', we have to specify a \n# distribution of values. The aim of this process is to find the best kernel for the SVM and to get a first look\n# of a good parameter values for the other parameters. Therefore you can run the randomized_search_SVC.fit()\n# command multiple times and observe the output of the best parameters.\n\ndist_param_SVC = {\n    'C': np.arange(0.1, 10, 0.2),\n    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n    'gamma': np.arange(0.01, 1, 0.01),\n    'degree': np.arange(3, 10, 1)\n}","1d86e235":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\nrandomized_search_SVC = RandomizedSearchCV(estimator=SVC_base_classifier,\n                           param_distributions=dist_param_SVC,\n                           scoring='accuracy',\n                           cv=5,\n                           n_jobs=-1)","589f7c36":"randomized_search_SVC.fit(X_train, y_train)","ca1d5ee4":"# Show best parameter outcome\n\nbest_parameters_SVC = randomized_search_SVC.best_params_\nprint(f'Best parameters: {best_parameters_SVC}')","89e213ad":"# Mean cross-validation score\n\nbest_result_SVC = randomized_search_SVC.best_score_\nprint(f'Mean cross-validation score of best model: {best_result_SVC}')","3b5e2669":"# Now we want to get the exact best parameter values of the model with the normal GridSearch\n# With RandomizedSearch we have seen that the best kernel is the rbf-kernel. With this knowledge we can now compute\n# the other parameter values much faster, because we do not have to calculate every kernel.\n# For the other parameters you have to test a few intervals to find the best value.\n\ngrid_param_SVC = {\n    'C': [2, 2.5, 3, 3.5, 4, 4.5, 5, 6, 6.5, 7],\n    'kernel': ['rbf'],\n    'gamma': ['scale', 'auto', 0.2, 0.5, 0.6, 0.7, 0.8],\n    'degree': [4, 5, 6, 7, 8, 9]\n}","7ab0864b":"grid_search_SVC = GridSearchCV(estimator=SVC_base_classifier,\n                           param_grid=grid_param_SVC,\n                           scoring='accuracy',\n                           cv=5,\n                           n_jobs=-1)","0c2dda54":"grid_search_SVC.fit(X_train, y_train)","f3122f2e":"# Show best parameter outcome\n\nbest_parameters_SVC = grid_search_SVC.best_params_\nprint(f'Best parameters: {best_parameters_SVC}')","c258bcdf":"# Mean cross-validation score of best model\n# Now we found the best parameter values for the SVC-model\n\nbest_result_SVC = grid_search_SVC.best_score_\nprint(f'Mean cross-validation score of best model: {best_result_SVC}')","43fa966c":"knn_base_classifier = KNeighborsClassifier(n_neighbors=5) # Define base estimator","643c3598":"knn_base_classifier.get_params() # Show all changeable parameters","a128334a":"# Define parameter grid\n\ngrid_param_knn = {\n    'n_neighbors': [3, 5, 7, 9, 11, 13],\n    'weights': ['uniform', 'distance'],\n    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n}","af67973b":"grid_search_knn = GridSearchCV(estimator=knn_base_classifier,\n                           param_grid=grid_param_knn,\n                           scoring='accuracy',\n                           cv=5,\n                           n_jobs=-1)","cd22935b":"grid_search_knn.fit(X_train, y_train)","bddb4d65":"# Show best parameter outcome\n\nbest_parameters_knn = grid_search_knn.best_params_\nprint(f'Best parameters: {best_parameters_knn}')","331292e0":"# Mean cross-validation score of best model\n\nbest_result_knn = grid_search_knn.best_score_\nprint(f'Mean cross-validation score of best model: {best_result_knn}')","ef7af210":"# Initialize RFC with the calculated best parameters\n# Note that a max_depth value of None can easily lead to overfitting\n\nRFC = RandomForestClassifier(n_estimators=200, max_depth=None, criterion='gini', bootstrap=True)\nRFC.fit(X_train, y_train)\npred_RFC = RFC.predict(X_test)","e1b82e09":"# Performance of model\nprint(classification_report(y_test, pred_RFC))\nconfmat_RFC = confusion_matrix(y_test, pred_RFC)","e87b88f2":"# Plot confusion matrix\n\nfig, ax = plt.subplots(figsize=(2.5, 2.5))\nax.matshow(confmat_RFC, cmap=plt.cm.Blues, alpha=0.3)\nfor i in range(confmat_RFC.shape[0]):\n    for j in range(confmat_RFC.shape[1]):\n        ax.text(x=j, y=i, \n               s=confmat_RFC[i, j],\n               va='center', ha='center')\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.show()","146c8ff4":"# Initialize SVC with the calculated best parameters\n\nSVC_clf = SVC(kernel='rbf', C=2, gamma=0.6, degree=4)\nSVC_clf.fit(X_train, y_train)\npred_SVC = SVC_clf.predict(X_test)","871b3ff9":"# Performance of model\nprint(classification_report(y_test, pred_SVC))\nconfmat_SVC = confusion_matrix(y_test, pred_SVC)","89ae1d86":"# Plot confusion matrix\nfig, ax = plt.subplots(figsize=(2.5, 2.5))\nax.matshow(confmat_SVC, cmap=plt.cm.Blues, alpha=0.3)\nfor i in range(confmat_SVC.shape[0]):\n    for j in range(confmat_SVC.shape[1]):\n        ax.text(x=j, y=i, \n               s=confmat_SVC[i, j],\n               va='center', ha='center')\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.show()","e311c270":"# Initialize KNN with the calculated best parameters\n\nknn_clf = KNeighborsClassifier(n_neighbors=5, algorithm='auto', weights='distance')\nknn_clf.fit(X_train, y_train)\npred_knn = knn_clf.predict(X_test)","17934e9a":"# Performance of model\nprint(classification_report(y_test, pred_knn))\nconfmat_knn = confusion_matrix(y_test, pred_knn)","33b8b68c":"# Plot confusion matrix\nfig, ax = plt.subplots(figsize=(2.5, 2.5))\nax.matshow(confmat_knn, cmap=plt.cm.Blues, alpha=0.3)\nfor i in range(confmat_knn.shape[0]):\n    for j in range(confmat_knn.shape[1]):\n        ax.text(x=j, y=i, \n               s=confmat_knn[i, j],\n               va='center', ha='center')\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.show()","c402fcea":"# Now we want to use the model on new, unseen data. Therefore we create a new, randomly specified data point\n# You can change the values of pH and alcohol to see different outcomes\n\npH = 3\nalcohol = 12\nXnew = [[7.8, 0.22, 0.99, 2.0, 0.01, 9.0, 18.0, 0.9968, pH, 1.8, alcohol]]","2b608e78":"Xnew = feature_scaler.transform(Xnew) #Use same transformer and model","37ec7667":"ynew_RFC = RFC.predict(Xnew)\nynew_SVC = SVC_clf.predict(Xnew)\nynew_knn = knn_clf.predict(Xnew)\n\nif ynew_RFC==0:\n    label_RFC = 'bad'\nelse:\n    label_RFC = 'good'\n    \nif ynew_SVC==0:\n    label_SVC = 'bad'\nelse:\n    label_SVC = 'good'\n    \nif ynew_knn==0:\n    label_knn = 'bad'\nelse:\n    label_knn = 'good'\n\nprint('Result of classification: ')\nprint(f'Random Forest Classifier: Label = {ynew_RFC} --> {label_RFC} wine')\nprint(f'Support Vector Classifier: Label = {ynew_SVC} --> {label_SVC} wine')\nprint(f'K-Nearest Neighbors Classifier: Label = {ynew_knn} --> {label_knn} wine')","d45e6bb2":"## 4.2. Support Vector Machine","07fb04fb":"## 4.1. Random Forest Classifier","51477dc9":"# 3. Data Preprocessing","93ba1772":"# 1. Business Understanding","0d3293a1":"# Thank you for opening this notebook !\n### In this notebook we will classify the wine quality using the red wine quality dataset. We will do the steps leaned on the CRISP-DM model, which is a standard for industry machine learning projects We will use the following models and compare their performance:\n1. Random Forest Classifier\n2. Support Vector Classifier\n3. K-Neighbors Classifier\n\n### Note that for a higher performance of the models you schould consider using some feature extraction\/selection methods which we not used here.","bcd52dd7":"## 5.2. Support Vector Machine","e0770bc6":"## 4.3. K-Nearest Neighbors","e9e0e69c":"# 5. Evaluation of the optimized models on test data","a8baffd4":"## 5.1. Random Forest Classifier","d0b4d6f4":"# 4. Modeling and Evaluation - Grid Search with Cross Validation","02f20b32":"## 5.3. K-Nearest Neighbors","1411c864":"# 2. Data Understanding","152cc6dd":"# 6. Deployment"}}