{"cell_type":{"498cac4d":"code","0a042eb0":"code","a8eef9ff":"code","7cae72cb":"code","6001c528":"code","bbdb791d":"code","88f5bbb0":"code","de380371":"code","61060995":"code","68fe0651":"code","36c63b34":"code","c2d038ce":"code","26bf7f20":"code","5056a892":"code","418bd7ca":"code","e179dbf1":"code","c70f516e":"code","24ef0fa1":"code","ca53f89f":"code","343c0185":"code","1e4d7724":"code","544c7e74":"code","30d08e1e":"code","897a5876":"code","7224026d":"code","64624ad7":"code","3004b6bc":"code","f7bef023":"code","991349dc":"code","d93d0a39":"code","52097b1c":"code","4a7f29cd":"code","e066a679":"code","a9dd50f7":"code","f24cdb11":"code","fd245326":"code","e9ab14e0":"code","2ea4a819":"code","fbd353bf":"code","9d5f850b":"code","df65a87a":"code","bf35b087":"code","f332dc15":"code","0cf784ac":"code","b6591955":"markdown","9b5fc293":"markdown","5da59e13":"markdown","cda6062d":"markdown","b605097b":"markdown","17b823d7":"markdown","e7f82700":"markdown","9776be5c":"markdown","638036bf":"markdown","c040dc22":"markdown","0a48403c":"markdown","ef84f2af":"markdown","c236373e":"markdown","dae04c6a":"markdown","a193fbb2":"markdown","e006e4dc":"markdown","ac57caf9":"markdown","afc9fc2b":"markdown","338c3af9":"markdown","ee5bd53a":"markdown","e6c960da":"markdown","a0c7e24b":"markdown","9dc727bd":"markdown","2869c0ea":"markdown","e81392b9":"markdown","0ccb7c67":"markdown","f7c4943f":"markdown","8c56d23b":"markdown","a7d62701":"markdown","0dbf493a":"markdown","ba1d4571":"markdown","237c0d00":"markdown","9375c414":"markdown","54e1ccc4":"markdown","51fa0928":"markdown","163b0242":"markdown","8c340884":"markdown","76f54192":"markdown"},"source":{"498cac4d":"import numpy as np \nimport pandas as pd \nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n\nimport hyperopt\nfrom hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n\nimport string\nimport warnings\nwarnings.filterwarnings('ignore')\n\nSEED = 42","0a042eb0":"# load data\ntrain=pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest=pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n\n# merge train and test\ntitanic = train.append(test, ignore_index=True)\n\n# create indexes to separate data later on\ntrain_idx = len(train)\ntest_idx = len(titanic) - len(test)\n\n# save PassengerId for final submission\npassengerId = test.PassengerId","a8eef9ff":"# preview the data\ntrain.head()","7cae72cb":"# count of missing values by column\ntitanic.isnull().sum()[titanic.isnull().sum()>0]","6001c528":"# create a new feature to extract title names from the Name column\ntitanic['Title'] = titanic.Name.apply(lambda name: name.split(',')[1].split('.')[0].strip())\nprint(titanic.Title.value_counts())","bbdb791d":"# create standardized Title mapping\nstandardized_titles = {\n    \"Capt\":       \"Officer\",\n    \"Col\":        \"Officer\",\n    \"Major\":      \"Officer\",\n    \"Jonkheer\":   \"Aristorcat\",\n    \"Don\":        \"Aristorcat\",\n    \"Sir\" :       \"Aristorcat\",\n    \"Dr\":         \"Officer\",\n    \"Rev\":        \"Officer\",\n    \"the Countess\":\"Aristorcat\",\n    \"Dona\":       \"Aristorcat\",\n    \"Mme\":        \"Mrs\",\n    \"Mlle\":       \"Miss\",\n    \"Ms\":         \"Mrs\",\n    \"Mr\" :        \"Mr\",\n    \"Mrs\" :       \"Mrs\",\n    \"Miss\" :      \"Miss\",\n    \"Master\" :    \"Master\",\n    \"Lady\" :      \"Aristorcat\"\n}\n\n# map the normalized titles to the current titles \ntitanic.Title = titanic.Title.map(standardized_titles)\n\n# view value counts for the normalized titles\nprint(titanic.Title.value_counts())","88f5bbb0":"# group by Sex, Pclass, and Title \ngrouped = titanic.groupby(['Sex','Pclass', 'Title']) \n\n# view the median Age by the grouped features \ngrouped.Age.median()","de380371":"# apply the grouped median value on the Age NaN\ntitanic.Age = grouped.Age.apply(lambda x: x.fillna(x.median()))","61060995":"# fill Embarked NaN with most frequent Embarked value\ntitanic.Embarked = titanic.Embarked.fillna(titanic.Embarked.mode()[0])","68fe0651":"# Filling the missing value in Fare with the median Fare of 3rd class alone passenger\nmed_fare = titanic.groupby(['Pclass', 'Parch', 'SibSp']).Fare.median()[3][0][0]\ntitanic['Fare'] = titanic['Fare'].fillna(med_fare)","36c63b34":"titanic['Cabin'] = titanic['Cabin'].fillna('U')\ntitanic[\"Cabin\"] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in titanic['Cabin'] ])","c2d038ce":"# Count of missing values by column after filling in.\n# Should only be the 418 test set samples with no Survived value.\ntitanic.isnull().sum()[titanic.isnull().sum()>0]","26bf7f20":"# Sex\ngrouped = titanic.groupby(['Sex'])  \ngrouped.Survived.mean()","5056a892":"# Pclass\ngrouped = titanic.groupby(['Pclass'])  \ngrouped.Survived.mean()","418bd7ca":"# Age\ng = sns.kdeplot(titanic[\"Age\"][(titanic[\"Survived\"] == 0) & (titanic[\"Age\"].notnull())], color=\"Red\", shade = True)\ng = sns.kdeplot(titanic[\"Age\"][(titanic[\"Survived\"] == 1) & (titanic[\"Age\"].notnull())], ax =g, color=\"Blue\", shade= True)\ng.set_xlabel(\"Age\")\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Not Survived\",\"Survived\"])","e179dbf1":"# SibSp - # of siblings \/ spouses aboard the Titanic\ngrouped = titanic.groupby(['SibSp'])  \ngrouped.Survived.mean()","c70f516e":"# Parch - # of parents \/ children aboard the Titanic\ngrouped = titanic.groupby(['Parch'])  \ngrouped.Survived.mean()","24ef0fa1":"# Fare \ng = sns.kdeplot(titanic[\"Fare\"][(titanic[\"Survived\"] == 0) & (titanic[\"Fare\"].notnull())], color=\"Red\", shade = True)\ng = sns.kdeplot(titanic[\"Fare\"][(titanic[\"Survived\"] == 1) & (titanic[\"Fare\"].notnull())], ax =g, color=\"Blue\", shade= True)\ng.set_xlabel(\"Fare\")\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Not Survived\",\"Survived\"])","ca53f89f":"# Cabin\ngrouped = titanic.groupby(['Cabin'])  \ngrouped.Survived.mean()","343c0185":"# Embarked\ngrouped = train.groupby(['Embarked'])  \ngrouped.Survived.mean()","1e4d7724":"titanic['Ticket_Frequency'] = titanic.groupby('Ticket')['Ticket'].transform('count')\ngrouped = titanic.groupby(['Ticket_Frequency'])  \ngrouped.Survived.mean()","544c7e74":"titanic[\"NameLength\"] = titanic[\"Name\"].apply(lambda x: len(x))\n\ng = sns.kdeplot(titanic[\"NameLength\"][(titanic[\"Survived\"] == 0) & (titanic[\"NameLength\"].notnull())], color=\"Red\", shade = True)\ng = sns.kdeplot(titanic[\"NameLength\"][(titanic[\"Survived\"] == 1) & (titanic[\"NameLength\"].notnull())], ax =g, color=\"Blue\", shade= True)\ng.set_xlabel(\"NameLength\")\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Not Survived\",\"Survived\"])","30d08e1e":"# Age\ntitanic['Age'] = pd.qcut(titanic['Age'], 11)\n\nfig, axs = plt.subplots(figsize=(22, 9))\nsns.countplot(x='Age', hue='Survived', data=titanic, palette = ['firebrick', 'blue'])\n\nplt.xlabel('Age', size=15, labelpad=20)\nplt.ylabel('Passenger Count', size=15, labelpad=20)\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\nplt.title('Survival Counts in {} Feature'.format('Age'), size=15, y=1.05)\n\nplt.show()","897a5876":"# Create family size feature from SisSp and Parch\ntitanic['Family_Size'] = titanic['SibSp'] + titanic['Parch'] + 1\n\n# standardize family size by grouping \/ mapping\nfamily_map = {1: 'Alone', 2: 'Small', 3: 'Small', 4: 'Small', 5: 'Medium', 6: 'Medium', 7: 'Large', 8: 'Large', 11: 'Large'}\ntitanic['Family_Size_Grouped'] = titanic['Family_Size'].map(family_map)\n\nfig, axs = plt.subplots(figsize=(22, 9))\nsns.countplot(x='Family_Size_Grouped', hue='Survived', data=titanic, palette = ['firebrick', 'blue'])\n\nplt.xlabel('Family_Size_Grouped', size=15, labelpad=20)\nplt.ylabel('Passenger Count', size=15, labelpad=20)\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\nplt.title('Survival Counts in {} Feature'.format('Age'), size=15, y=1.05)\n\nplt.show()","7224026d":"titanic['Fare'] = (titanic['Fare'] \/ titanic['Ticket_Frequency'])","64624ad7":"titanic['Fare'] = pd.qcut(titanic['Fare'], 13)\n\nfig, axs = plt.subplots(figsize=(22, 9))\nsns.countplot(x='Fare', hue='Survived', data=titanic, palette = ['firebrick', 'blue'])\n\nplt.xlabel('Fare', size=15, labelpad=20)\nplt.ylabel('Passenger Count', size=15, labelpad=20)\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\nplt.title('Count of Survival in {} Feature'.format('Fare'), size=15, y=1.05)\n\nplt.show()","3004b6bc":"titanic['Deck'] = titanic['Cabin']\ntitanic['Deck'] = titanic['Deck'].replace(['A', 'B', 'C', 'T'], 'ABC')\ntitanic['Deck'] = titanic['Deck'].replace(['D', 'E'], 'DE')\ntitanic['Deck'] = titanic['Deck'].replace(['F', 'G'], 'FG')\n\nfig, axs = plt.subplots(figsize=(22, 9))\nsns.countplot(x='Deck', hue='Survived', data=titanic, palette = ['firebrick', 'blue'])\n\nplt.xlabel('Deck', size=15, labelpad=20)\nplt.ylabel('Passenger Count', size=15, labelpad=20)\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\nplt.title('Count of Survival in {} Feature'.format('Fare'), size=15, y=1.05)\n\nplt.show()","f7bef023":"# a function to extract the surname from each passanger name\n\ndef extract_surname(data):    \n    \n    families = []\n    \n    for i in range(len(data)):        \n        name = data.iloc[i]\n\n        if '(' in name:\n            name_no_bracket = name.split('(')[0] \n        else:\n            name_no_bracket = name\n            \n        family = name_no_bracket.split(',')[0]\n        title = name_no_bracket.split(',')[1].strip().split(' ')[0]\n        \n        for c in string.punctuation:\n            family = family.replace(c, '').strip()\n            \n        families.append(family)\n            \n    return families\n\ntitanic['Family'] = extract_surname(titanic['Name'])\ndf_train = titanic.loc[:890]\ndf_test = titanic.loc[891:]\ndfs = [df_train, df_test]","991349dc":"# Creating a list of families and tickets that are occuring in both training and test set\nnon_unique_families = [x for x in df_train['Family'].unique() if x in df_test['Family'].unique()]\nnon_unique_tickets = [x for x in df_train['Ticket'].unique() if x in df_test['Ticket'].unique()]\n\ndf_family_survival_rate = df_train.groupby('Family')['Survived', 'Family','Family_Size'].median()\ndf_ticket_survival_rate = df_train.groupby('Ticket')['Survived', 'Ticket','Ticket_Frequency'].median()\n\nfamily_rates = {}\nticket_rates = {}\n\nfor i in range(len(df_family_survival_rate)):\n    # Checking a family exists in both training and test set, and has members more than 1\n    if df_family_survival_rate.index[i] in non_unique_families and df_family_survival_rate.iloc[i, 1] > 1:\n        family_rates[df_family_survival_rate.index[i]] = df_family_survival_rate.iloc[i, 0]\n\nfor i in range(len(df_ticket_survival_rate)):\n    # Checking a ticket exists in both training and test set, and has members more than 1\n    if df_ticket_survival_rate.index[i] in non_unique_tickets and df_ticket_survival_rate.iloc[i, 1] > 1:\n        ticket_rates[df_ticket_survival_rate.index[i]] = df_ticket_survival_rate.iloc[i, 0]","d93d0a39":"mean_survival_rate = np.mean(df_train['Survived'])\n\ntrain_family_survival_rate = []\ntrain_family_survival_rate_NA = []\ntest_family_survival_rate = []\ntest_family_survival_rate_NA = []\n\nfor i in range(len(df_train)):\n    if df_train['Family'][i] in family_rates:\n        train_family_survival_rate.append(family_rates[df_train['Family'][i]])\n        train_family_survival_rate_NA.append(1)\n    else:\n        train_family_survival_rate.append(mean_survival_rate)\n        train_family_survival_rate_NA.append(0)\n        \nfor i in range(len(df_test)):\n    if df_test['Family'].iloc[i] in family_rates:\n        test_family_survival_rate.append(family_rates[df_test['Family'].iloc[i]])\n        test_family_survival_rate_NA.append(1)\n    else:\n        test_family_survival_rate.append(mean_survival_rate)\n        test_family_survival_rate_NA.append(0)\n        \ndf_train['Family_Survival_Rate'] = train_family_survival_rate\ndf_train['Family_Survival_Rate_NA'] = train_family_survival_rate_NA\ndf_test['Family_Survival_Rate'] = test_family_survival_rate\ndf_test['Family_Survival_Rate_NA'] = test_family_survival_rate_NA\n\ntrain_ticket_survival_rate = []\ntrain_ticket_survival_rate_NA = []\ntest_ticket_survival_rate = []\ntest_ticket_survival_rate_NA = []\n\nfor i in range(len(df_train)):\n    if df_train['Ticket'][i] in ticket_rates:\n        train_ticket_survival_rate.append(ticket_rates[df_train['Ticket'][i]])\n        train_ticket_survival_rate_NA.append(1)\n    else:\n        train_ticket_survival_rate.append(mean_survival_rate)\n        train_ticket_survival_rate_NA.append(0)\n        \nfor i in range(len(df_test)):\n    if df_test['Ticket'].iloc[i] in ticket_rates:\n        test_ticket_survival_rate.append(ticket_rates[df_test['Ticket'].iloc[i]])\n        test_ticket_survival_rate_NA.append(1)\n    else:\n        test_ticket_survival_rate.append(mean_survival_rate)\n        test_ticket_survival_rate_NA.append(0)\n        \ndf_train['Ticket_Survival_Rate'] = train_ticket_survival_rate\ndf_train['Ticket_Survival_Rate_NA'] = train_ticket_survival_rate_NA\ndf_test['Ticket_Survival_Rate'] = test_ticket_survival_rate\ndf_test['Ticket_Survival_Rate_NA'] = test_ticket_survival_rate_NA","52097b1c":"for df in [df_train, df_test]:\n    df['Survival_Rate'] = (df['Ticket_Survival_Rate'] + df['Family_Survival_Rate']) \/ 2\n    df['Survival_Rate_NA'] = (df['Ticket_Survival_Rate_NA'] + df['Family_Survival_Rate_NA']) \/ 2 ","4a7f29cd":"# NameLength\ntitanic['NameLength'] = pd.qcut(titanic['NameLength'], 11)\n\nfig, axs = plt.subplots(figsize=(22, 9))\nsns.countplot(x='NameLength', hue='Survived', data=titanic, palette = ['firebrick', 'blue'])\n\nplt.xlabel('NameLength', size=15, labelpad=20)\nplt.ylabel('Passenger Count', size=15, labelpad=20)\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\nplt.title('Survival Counts in {} Feature'.format('Age'), size=15, y=1.05)\n\nplt.show()","e066a679":"non_numeric_features = ['Embarked', 'Sex', 'Deck', 'Title', 'Family_Size_Grouped', 'Age', 'Fare', 'NameLength']\n\nfor df in dfs:\n    for feature in non_numeric_features:        \n        df[feature] = LabelEncoder().fit_transform(df[feature])","a9dd50f7":"cat_features = ['Pclass', 'Sex', 'Deck', 'Embarked', 'Title', 'Family_Size_Grouped']\nencoded_features = []\n\nfor df in dfs:\n    for feature in cat_features:\n        encoded_feat = OneHotEncoder().fit_transform(df[feature].values.reshape(-1, 1)).toarray()\n        n = df[feature].nunique()\n        cols = ['{}_{}'.format(feature, n) for n in range(1, n + 1)]\n        encoded_df = pd.DataFrame(encoded_feat, columns=cols)\n        encoded_df.index = df.index\n        encoded_features.append(encoded_df)\n\ndf_train = pd.concat([df_train, *encoded_features[:6]], axis=1)\ndf_test = pd.concat([df_test, *encoded_features[6:]], axis=1)","f24cdb11":"titanic =  pd.concat([df_train, df_test], sort=True).reset_index(drop=True)\ndrop_cols = ['Deck', 'Cabin', 'Embarked', 'Family', 'Family_Size', 'Family_Size_Grouped', 'Survived',\n             'Name', 'Parch', 'PassengerId', 'Pclass', 'Sex', 'SibSp', 'Ticket', 'Title',\n            'Ticket_Survival_Rate', 'Family_Survival_Rate', 'Ticket_Survival_Rate_NA', 'Family_Survival_Rate_NA']\n\ntitanic.drop(columns=drop_cols, inplace=True)\n\ntitanic.head()","fd245326":"titanic.columns","e9ab14e0":"#correlation matrix\ncorrmat = titanic.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True, cmap=\"BuPu\");","2ea4a819":"X_train = StandardScaler().fit_transform(df_train.drop(columns=drop_cols))\ny_train = df_train['Survived'].values\nX_test = StandardScaler().fit_transform(df_test.drop(columns=drop_cols))\n\nprint('X_train shape: {}'.format(X_train.shape))\nprint('y_train shape: {}'.format(y_train.shape))\nprint('X_test shape: {}'.format(X_test.shape))","fbd353bf":"#RandomForestClassifier\n\nmax_evals = 2\n\nspace={\n        'n_estimators': hp.uniform(\"n_estimators\", 500, 2000),\n        'max_features': hp.choice(\"max_features\", [\"auto\", \"sqrt\"]),\n        'max_depth': hp.uniform(\"max_depth\", 2, 12),\n        'min_samples_split': hp.uniform(\"min_samples_split\", 2, 11),\n        'min_samples_leaf': hp.uniform(\"min_samples_leaf\", 2, 11),\n    }\n\n# Regression: \ndef hyperparameter_tuning(space):\n    model=RandomForestClassifier(\n        n_estimators =int(space['n_estimators']),\n        max_depth =int(space['max_depth']),\n        max_features = space['max_features'],\n        min_samples_split =int(space['min_samples_split']),\n        min_samples_leaf =int(space['min_samples_leaf']),\n        random_state = SEED\n        )\n    \n    scores = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=StratifiedKFold(n_splits=5,shuffle=True))\n    # print(scores)\n    \n    # mse= metrics.mean_squared_error(y_test, pred)\n    print (\"SCORE: %0.2f\" % (-scores.mean()))\n    #change the metric if you like\n    return {'loss':-scores.mean(), 'status': STATUS_OK }\n\ntrials = Trials()\nbest = fmin(fn=hyperparameter_tuning,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=max_evals,\n            trials=trials)\n\nprint (best)","9d5f850b":"model = RandomForestClassifier(criterion='gini',\n                                           n_estimators=1750,\n                                           max_depth=7,\n                                           min_samples_split=6,\n                                           min_samples_leaf=6,\n                                           max_features='auto',\n                                           oob_score=True,\n                                           random_state=SEED,\n                                           n_jobs=-1,\n                                           verbose=1) ","df65a87a":"N = 5\noob = 0\nprobs = pd.DataFrame(np.zeros((len(X_test), N * 2)), columns=['Fold_{}_Prob_{}'.format(i, j) for i in range(1, N + 1) for j in range(2)])\nimportances = pd.DataFrame(np.zeros((X_train.shape[1], N)), columns=['Fold_{}'.format(i) for i in range(1, N + 1)], index=titanic.columns)\n\nskf = StratifiedKFold(n_splits=N, random_state=N, shuffle=True)\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(X_train, y_train), 1):\n    print('Fold {}\\n'.format(fold))\n    \n    # Fitting the model\n    model.fit(X_train[trn_idx], y_train[trn_idx]) \n    \n    # X_test probabilities\n    probs.loc[:, 'Fold_{}_Prob_0'.format(fold)] = model.predict_proba(X_test)[:, 0]\n    probs.loc[:, 'Fold_{}_Prob_1'.format(fold)] = model.predict_proba(X_test)[:, 1]\n    importances.iloc[:, fold - 1] = model.feature_importances_\n        \n    oob += model.oob_score_ \/ N\n    print('Fold {} OOB Score: {}\\n'.format(fold, model.oob_score_))   \n    \nprint('Average OOB Score: {}'.format(oob))","bf35b087":"importances['Mean_Importance'] = importances.mean(axis=1)\nimportances.sort_values(by='Mean_Importance', inplace=True, ascending=False)\n\nplt.figure(figsize=(15, 20))\nsns.barplot(x='Mean_Importance', y=importances.index, data=importances, palette = 'PRGn')\n\nplt.xlabel('')\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\nplt.title('Random Forest Classifier Mean Feature Importance Between Folds', size=15)\n\nplt.show()","f332dc15":"class_survived = [col for col in probs.columns if col.endswith('Prob_1')]\nprobs['1'] = probs[class_survived].sum(axis=1) \/ N\nprobs['0'] = probs.drop(columns=class_survived).sum(axis=1) \/ N\nprobs['pred'] = 0\npos = probs[probs['1'] >= 0.50].index\nprobs.loc[pos, 'pred'] = 1\n\ny_pred = probs['pred'].astype(int)\n\nsubmission_df = pd.DataFrame(columns=['PassengerId', 'Survived'])\nsubmission_df['PassengerId'] = df_test['PassengerId']\nsubmission_df['Survived'] = y_pred.values","0cf784ac":"import os\nos.chdir('\/kaggle\/working')\n    \nsubmission_df.to_csv('titanic_submissions.csv', header=True, index=False)","b6591955":"# 6. Modeling","9b5fc293":"**4.5 Ticket\/Family Survival Rate**\n\nGrouping passangers into 'unique ticket' and family groups by surname. Will then create a feature out of the average of the survival rate of these two groups.","5da59e13":"**3.6 Cabin**\n\nEssentially looks like if you have a cabin value, then you have better survival rate. This is probably due to 'named' cabins being upper class accomodation in the higher levels, closer to the deck and life boats.\n\nThere is only one passanger in a 'T' cabin. Given that this passanger has Pclass 1, we'll assume that cabin T belonngs with the other predominantly 1st class cabins A, B, and C.","cda6062d":"**3.7 Embarked**\n\nPassangers who embarked at Cherbourg seem to have a much higher survival rate. It's likely that this is mostly down to other factors, such as a higher proportion of them being 1st or 2nd class. However, this feature will stay in as there may be some connection between embarking city and cabin or deck position.","b605097b":"Family\/Ticket Survival Rates are calculated from family\/ticket groups in the training set.\n\nFamily name \/ ticket groups that occur in both traning and test set are determined and the survival rates are applied to them. \n\nThe features Family\/Ticket Survival Rate NA signifies that a family or ticket group appears only in the test set and so this feature does not apply to them.","17b823d7":"**2.1 Age**\n\nThere are 263 missing Age values. Filling these with median values (28) is a fairly blunt approach and so I'll fill with median values after grouping rows by Sex, Pclass and Title. \n\nWe don't have a Title column yet. Title will go on to become a very useful feature but it makes sense to create it here and use it to fill in missing ages. For example, there are significant age differences between passangers with the title 'Miss' and 'Mrs' or 'Mr' and 'Master'.","e7f82700":"**5.3 Create final feature set**","9776be5c":"**Model training and score prediction:** The cross-validation strategy is a k-fold scheme. For each fold, we make the trained model predict probabilities for the test set. With 5 folds, this means each row in the test set recieves 5 predictions probabilities from 5 versions of the model, fitted on slightly different training datasets. This gives a better score than just training on the full train set and predicting for test.","638036bf":"The following cell uses Bayesian hyperparameter optimization to search for the best parameters for a Random Forest model. The best parameters here are not neccesarily the parameters than will give the highest leaderboard score.\n\nNote - At the moment, the max_evals parameter is set to 20. To find genuinely optimal features, set this to at least 20.","c040dc22":"# 4. Feature Engineering","0a48403c":"Feature Importance","ef84f2af":"**4.6 Namelength**\n\nBin name lengths.","c236373e":"## **Introduction**\n\nThe Titanic competition is a nice introduction to machine learning. Due to come very powerful predictors (Age, Sex, and a few features which indicate wealth), it's relatively simple to produce a 'good' model with some simple features and any appropriate machine learning model. The baseline score for this dataset is around 62% i.e. around 62% of passangers in the dataset die and so it's what you'd score if you just predicted everyone dies. A relatively simple, 'good' model should improve that score up to somehwere in the mid 70s. However, it's possible, with some more advanced feature engineering and deeper exploration of model tuning to improve that score further up to around the low 80s. The journey from 77 to 81 is a lot harder than the journey from 0 to 75 and in this way, it very much captures the diminishing returns nature of 'real world' machine learning.\n\nAt the time or writing, this kernel scores **0.81578** accuracy which scores in the **top 4%** on the public leaderboard. The sections of this kernel are;\n\n**1. Preprocessing** - Load in libraries, data and take an initial look at the data.\n\n**2. Missing Values** - A few columns have Null or missing values. Come up with strategies to fill this data in.\n\n**3. Exploratory Data Analysis** - Explore each feature and determine whether it is a good predictor for survival. I will usually explore the data in pivot form rather than visualizing everything to keep an already long notebook as brief as possible.\n\n**4. Feature Engineering** - most features need to be transformed before they can be used by a model. This will be a mix of transforming original features (e.g. grouping, binning) and creating new features.\n\n**5. Feature Transformation** - label and one-hot encoding of features.\n\n**6. Modeling** - Using and tuning a Random Forest classifier.\n\n**7. Submission** - create a file to be submitted to Kaggle for scoring.","dae04c6a":"**3.8 Name Length**\n\nLength of string in Name column. Passangers who have more chatacters in their Name column have a better survival rate. It's likely this is tied in to social class. Upper classes have longer titles, more middle names, double-barraled surnames etc.","a193fbb2":"Plotting **feature correlations** with a heat map.","e006e4dc":"**4.1 Age**\n\nPassangers are binned into 11 groups based on age. This helps capture the clear structure in the age distributions whilst generalising to reduce overfitting.","ac57caf9":"**2.4 Cabin**\n\nThere are over 1000 missing values here. For the moment, fill these with 'U' (unknown). Missing cabin values are more likely to be from lower class passangers and are less likely to survive so this is actually relevant information.","afc9fc2b":"**5.2 One-hot encoding categorical features**\n\n","338c3af9":"**2.3 Fare**\n\nOnly one missing fare value. This is a passanger travelling alone in 3rd class - so fill with median of this group.","ee5bd53a":"Create train and test arrays and apply standard scaling.","e6c960da":"**3.3 Age**\n\nChildren have a higher survival rate than adults. Generally younger and older adults see lower survival rates than middle aged adults. To be used as a feature, age will be binned.","a0c7e24b":"# 5. Feature Transformations","9dc727bd":"# 2. Missing Values","2869c0ea":"**3.4 SibSp and Parch**\n\nSiblings \/ spouses and parents \/ children. These will be combined into a family size feature. It is significant. With lone travelers and large families having lower survival rates.","e81392b9":"# 3. Exploratory Data Analysis","0ccb7c67":"**3.1 Sex**\n\n74% of females survived compared to only 19% of males. This is clearly extremely significant and most likely down to a 'women and children first' approach to who gets on the lifeboats. This will obviously be an extremely powerful predictor.","f7c4943f":"# 1. Preprocessing","8c56d23b":"**2.2 Embarked**\n\nThere are only two missing values here - use most common Embarked value - S.","a7d62701":"Submission","0dbf493a":"Model definition","ba1d4571":"**3.7 Ticket Frequency**\n\nDescribes how many passangers are travelling on a joint ticket. Will catch some non-family groupings e.g. friend groups or upper class passangers and their staff. This is the value we will later divide Fare by to capture 'true' Fare per passanger.","237c0d00":"**4.4 Cabin**\n\nCabin feature is going to be changed to a 'Deck' feature where cabin types are grouped by similar class make ups and survival rates.","9375c414":"**4.2 Family Size**\n\nFamily Size is created by adding the number of siblings, spouses, children and parents a passanger is travelling with +1 so that solo travellers have family size 1. Family size is then grouped to aid overfitting.","54e1ccc4":"**5.1 Label encoding non-numeric features**\n\nLabel encoding will transform non-numeric features into numbered lists. e.g. Embarked is transformed from ['S', 'C', 'Q'] to [1,2,3]. Age and Fare stay as their label encoded form as these are ordinal features i.e. they have some ordering that makes sense (a passanger with age group 6 is older than one in age group 2). Features such as Embarked and Deck and further one-hot encoded as they are categorical not ordinal, so the label encoded order has no meaning.","51fa0928":"**3.2 PClass**\n\nPassenger class is also a very important feature. This correlates strongly with other variables such as Fare and Cabin.","163b0242":"Let's look at the median ages of passangers grouped by Sex, class and title...","8c340884":"**4.3 Fare**\n\nFare is divided by ticket frequency, as described above, and then binned into 13 groups.","76f54192":"**3.5 Fare**\n\nVery skewed but very significant feature - lower fare passangers have lower survival rates. So we'll bin fare values to allow our model to generalise better - we don't want it learning that passangers whose fare is \u00a34.29 have a better chance of surviving that those passangers whose fare is \u00a34.89, for example.\n\nA note about fare - I've noticed that a lot of kernels use Fare 'as is' and take it to mean the cost of a ticket per passanger but this isn't the case. Many tickets are bought in groups. So if the average 3rd class ticket is \u00a35 and a family of five buy tickets, the Fare will be \u00a325 for each passanger. Fare needs to be divided by the number of instances of each ticket id."}}