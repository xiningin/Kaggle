{"cell_type":{"b0cf4c80":"code","6e589834":"code","1befb3d8":"code","dfc81638":"code","428a6c0b":"code","f6400257":"code","56051ac0":"code","0f663a4a":"code","fd52815d":"code","db528e62":"code","87eedd20":"code","06cd0711":"code","0ad2eb06":"code","e60e8d3a":"code","45d3a1df":"code","f3951849":"code","b8d895b1":"code","1930db93":"code","4b648d09":"code","955ecff2":"code","01f039e2":"code","33ca5f13":"code","9f23bfd6":"code","bc5f3273":"code","ec024030":"code","c11aa021":"code","a025cfcc":"code","aeb11643":"code","b7d9d196":"code","2cbb638a":"code","360b99ad":"code","294370d7":"code","c4708535":"code","cf3449bc":"code","37777ffd":"code","ab3fd86e":"code","7a9ccbfa":"code","49b4bcbe":"code","ebf3495c":"code","149e83bc":"code","dca0d51e":"code","f960dc6c":"code","cea1a988":"code","1bc1ae9d":"code","cd446580":"markdown","95cbfa94":"markdown","3ac6634f":"markdown","b68adeaf":"markdown","c52ed1fe":"markdown","6e9edc21":"markdown","1274cc61":"markdown","0548d793":"markdown","06f8771b":"markdown","5ca8686d":"markdown","3b9dd9fd":"markdown","997bc941":"markdown","2b1e55f6":"markdown","e5edf716":"markdown","d0064b7c":"markdown","9525ac51":"markdown","b0f4223a":"markdown","db67989a":"markdown","7f3b1c7e":"markdown","266a94d3":"markdown","f384d96c":"markdown","e5867fa9":"markdown","40f6ab97":"markdown","7eefe13f":"markdown","53e8ce3d":"markdown","c1112c57":"markdown","e3dfcfce":"markdown","7139a4f1":"markdown","4ba2e79d":"markdown","62969518":"markdown","0728cb17":"markdown","b909b92c":"markdown","667bd564":"markdown","acd1a59f":"markdown"},"source":{"b0cf4c80":"import os\nimport numpy as np # linear algebra\nfrom numpy.linalg import inv, det\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport scipy.stats as st\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.stats import diagnostic\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom patsy import dmatrix\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\n\n\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\nimport plotly.io as pio \n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nnp.random.seed(125)\npio.templates.default = 'none'\npio.renderers.default='iframe'","6e589834":"%matplotlib inline","1befb3d8":"filename = 'bmw_pricing_challenge.csv'\ndirname = '..\/input\/bmw-pricing-challenge'\n\ndf = pd.read_csv(os.path.join(dirname, filename), parse_dates=['registration_date','sold_at'])\ndf.head(3)","dfc81638":"#Calculate age ~ the time in years between registering the car and selling it\ndf['age'] = (df.sold_at - df.registration_date).dt.days \/ 365\n\n\n#Assign categorical variables\ncat_cols = ['model_key', 'fuel','paint_color','car_type',\n            'feature_1','feature_2','feature_3','feature_4',\n            'feature_5','feature_6','feature_7','feature_8']\ncont_cols = ['mileage','engine_power','price','age']","428a6c0b":"df[cont_cols].describe()","f6400257":"# change the sign of mileage to positive\n\ndf.mileage = df.mileage.apply(lambda x: abs(x))","56051ac0":"# fix the record with zero engine horse power\ndf[df.engine_power <=0]","0f663a4a":"# replace zero engine power with the median of car\ndf.engine_power[df.engine_power==0] = \\\ndf[(df.model_key =='X1')&(df.fuel =='diesel')&(df.age < 3.5)].engine_power.median()","fd52815d":"# check if there is any N\/As in the dataset\nprint(df.isna().sum())","db528e62":"# We only examin outliers in continuous variables\nfig = px.box(df[cont_cols].melt(var_name='variable'),\n             color='variable',\n             facet_col='variable',\n             boxmode='overlay')\nfig.update_yaxes(matches=None)\nfig.show()","87eedd20":"fig = go.Figure()\nfor col in cat_cols:\n    d = df.groupby(col).size().to_frame(col).T\n    f = px.bar(d, orientation='h')\n    for o in f.data:\n        fig.add_trace(o)\nfig.update_layout(showlegend=False, barmode='stack', title='Distribution of Data across Categorical Variables')\nfig.show()","06cd0711":"# Roll up model_key into model by series\ndf['model'] = 'Other'\nfor i in range(1,8):\n    idx = df[df.model_key.str.startswith(str(i))].index\n    df.model.loc[idx] = str(i) + ' series'\ncat_cols.append('model')\n\n# Merge minority colors\ndf['color'] = np.where(df.paint_color.isin(['beige','green','orange','red']), 'other', df.paint_color)\ncat_cols.remove('paint_color')\ncat_cols.append('color')\n# Drop electric and hybrid cars\n\ndf.drop(index=df[df.fuel.isin(['hybrid_petrol','electro'])].index, inplace=True)\ndf.reset_index(drop=True, inplace=True)","0ad2eb06":"fig = go.Figure()\ncat_cols.remove('model_key')\nfor col in cat_cols:\n    d = df.groupby(col).size().to_frame(col).T\n    f = px.bar(d, orientation='h')\n    for o in f.data:\n        fig.add_trace(o)\nfig.update_layout(showlegend=False, barmode='stack', title='Distribution of Data across Categorical Variables')\nfig.show()","e60e8d3a":"scatter = ff.create_scatterplotmatrix(df[cont_cols],\n                                      diag='histogram',\n                                      height=900, width=900,\n                                      title='BMW Scatter Plot')\nscatter.show()","45d3a1df":"# pd.plotting.scatter_matrix(df[cont_cols], figsize=(12,12));","f3951849":"mask = np.ones_like(df[cont_cols].corr(), dtype=bool)\nmask[np.tril_indices_from(mask)] = False\nsns.heatmap(df[cont_cols].corr(), annot=True, mask=mask, annot_kws=dict(fontsize=12));","b8d895b1":"# Split the data into train and test sets\n\ntrain_idx, test_idx = train_test_split(df.index, test_size=.25)\ndf_train = df.iloc[train_idx].reset_index(drop=True)\ndf_test = df.iloc[test_idx].reset_index(drop=True)\n\nprint(f'Size of training set:{len(df_train)}, size of test set: {len(df_test)}\\n')","1930db93":"# Scale the data (optional)\n\nfrom sklearn.preprocessing import MinMaxScaler\ncont_x = ['mileage','engine_power','age']\nscalar = MinMaxScaler()\nscalar.fit(df[cont_x])\ndfc = df.copy()\ndfc[cont_x] = scalar.transform(dfc[cont_x])\n\ndfc_train = dfc.iloc[train_idx].reset_index(drop=True)\ndfc_test = dfc.iloc[test_idx].reset_index(drop=True)","4b648d09":"def pred(res, x, y, log=False):\n    y_hat = res.predict(x)\n    if log:\n        y_hat = np.exp(y_hat)\n    r2 = r2_score(y,y_hat)\n    print(f'R-squared: {r2:.2f}')","955ecff2":"formula_manual = '''\nnp.log(price)~age+np.power(age,2)+mileage+np.power(mileage,2)+engine_power+model+fuel+color\n+feature_1+feature_2+feature_3+feature_4\n+feature_5+feature_6+feature_7+feature_8\n'''\n\nols_manual = smf.ols(formula_manual, df_train)\nres_manual = ols_manual.fit()\nprint(res_manual.summary())","01f039e2":"# R2 of test-data\n\npred(res_manual, x=df_test, y=df_test.price, log=True)","33ca5f13":"from collections import defaultdict\nfrom itertools import combinations\nfrom tqdm import tqdm\n\nclass ForwardModelSelector:\n    \"\"\"\n    Ordinary Least Square forward selection algorithm. It tries every possible combination of variables in  \n    x_cols and stores the resulting metrices in summary_frame\n    \"\"\"\n    def __init__(self, x_cols , y_col, df, subset=None, metrics=['aic','bic','rsquared_adj']):\n        \"\"\"\n        x_cols: list of the independent variables\n        y_col : str represents the dependent variable or a transformation version of it. Takes np.log, center\n        or a custom function\n        df    : pandas dataframe contains x's and y\n        subset: numpy array or pandas indexes to train on subset of the data (training set)\n        metrics: list of metrics to save in summary_frame. Default is aic, bic and rsquared_adj\n        \n        \"\"\"\n        self.x_cols = x_cols\n        self.y_col = y_col\n        self.df = df\n        self.subset = subset\n        self.metrics = metrics\n        self.res = defaultdict(list)\n        self.left = str(y_col) + '~'\n        self.n = 0\n        pass\n    \n    def generate_formulas(self):\n        \"\"\"Generate a list of formulas using all possible combinations of x variables\"\"\"\n        self.formulas = []\n        for i in range(1,len(self.x_cols)):\n            self.formulas.extend(list(map(' + '.join, combinations(x_cols, r=i))))\n        pass\n    \n    def fit(self):\n        \"\"\"Regress using generated formulas. Results are saved in dataframe\"\"\"\n        self.generate_formulas()\n        for right in tqdm(self.formulas):\n            self.n += 1\n            f_tmp = self.left + right\n            self._update_(f_tmp)\n        print(f'Finished trying {self.n} combinations')\n        self.frame = pd.DataFrame(self.res, index=self.metrics+['n_variables']).T.reset_index().rename(columns={'index':'formula'})\n    \n    def _update_(self, formula):\n        ols_tmp = smf.ols(formula, self.df, subset=self.subset).fit()\n        for m in self.metrics:\n            self.res[formula].append(getattr(ols_tmp,m))\n        self.res[formula].append(ols_tmp.model.exog.shape[1])\n        pass\n    \n    def summary_frame(self, sort_by=None, ascending=None):\n        if sort_by is None:\n            return self.frame.sort_values(self.metrics)\n        return self.frame.sort_values(sort_by, ascending=ascending)\n","9f23bfd6":"x_cols = ['age','np.power(age,2)','mileage','np.power(mileage,2)','engine_power'] + cat_cols\n\nm = ForwardModelSelector(x_cols, 'np.log(price)', df, subset=train_idx, metrics=['aic','bic','rsquared_adj','fvalue'])\nm.fit()","bc5f3273":"top = m.summary_frame(sort_by=['rsquared_adj','bic','aic'],\n                ascending=[False,True,True]).nlargest(n=10, columns='rsquared_adj')\ntop","ec024030":"formula = top.loc[top.n_variables.idxmin()].formula\nformula","c11aa021":"ols = smf.ols(formula, data=df, subset=train_idx)\nres = ols.fit()\nprint(res.summary())","a025cfcc":"# R-squared of test-data\n\npred(res, x=df_test, y=df_test.price, log=True)","aeb11643":"ols_ordered_tmp = smf.ols('np.log(price)~mileage+age+engine_power', data=df_train).fit()\nhc = {'F-statistic':[],'p-value':[]}\nidx = []\nfor col in cont_cols:\n    if col =='price':continue\n    try:\n        h = diagnostic.linear_harvey_collier(ols_ordered_tmp, order_by=np.argsort(df_train[col]))\n        idx.append(col)\n        hc['F-statistic'].append(h[0])\n        hc['p-value'].append(h[1])\n        \n    except:\n        pass\npd.DataFrame(hc, index=idx).round(4)","b7d9d196":"rainbow = {'F-statistic':[],'p-value':[]}\nidx = []\nfor col in cont_cols:\n    if col =='price': continue    \n    try:\n        h = diagnostic.linear_rainbow(ols_ordered_tmp, order_by=col)\n        rainbow['F-statistic'].append(h[0])\n        rainbow['p-value'].append(h[1])\n        idx.append(col)\n    except:\n        pass\npd.DataFrame(rainbow, index=idx).style.format('{:.6f}')","2cbb638a":"# Scatter age and price again and draw a log-normal terndline to visualize the non-linear relation between them\n\npx.scatter(dfc_train, x='age', y='price', trendline='ols', log_y=True, log_x=False, trendline_color_override='red')","360b99ad":"plot_kwargs = dict(mode='markers',\n                   marker=dict(symbol='circle-open', opacity=0.4, size=6, #color='orange', \n                               line=dict(color='orange', width=1)\n                              )\n                  )\n\nfig = make_subplots(2,3,vertical_spacing=.15,horizontal_spacing=.05,\n                   subplot_titles=['Run-Seq','Price Y','Pred Price','Mileage','Age','Engine Power'])\nfig.append_trace(go.Scatter(y=res.resid,name='seq-run', **plot_kwargs), 1,1)\nfig.append_trace(go.Scatter(x=res.model.endog,name='price (y)', y=res.resid, **plot_kwargs), 1,2)\nfig.append_trace(go.Scatter(x=res.fittedvalues,name='predicted price', y=res.resid, **plot_kwargs), 1,3)\nfig.append_trace(go.Scatter(x=df.iloc[train_idx].mileage, name='mileage', y=res.resid, **plot_kwargs), 2,1)\nfig.append_trace(go.Scatter(x=df.iloc[train_idx].age, name='age', y=res.resid, **plot_kwargs), 2,2)\nfig.append_trace(go.Scatter(x=df.iloc[train_idx].engine_power, name='engine_power', y=res.resid, **plot_kwargs), 2,3)\n\nfig.update_layout(height=600)\nfig.show()\n\n# plt.plot(res_algo.model.endog,res_algo.resid, 'rx')","294370d7":"def GQ_test(ols, start_idx=None, alpha=0.05):\n    GQ = lambda ols,i: diagnostic.het_goldfeldquandt(ols.endog, ols.exog, idx=i, alternative='two-sided')\n    gq_df = pd.DataFrame([GQ(ols, i=i) for i in [None]+list(range(start_idx,ols.exog.shape[1]))],\n             index=['run-seq']+ols.exog_names[start_idx:],\n             columns=['F-statistic','p-value','alternative']\n            )\n    gq_df['result'] = np.where(gq_df['p-value']< alpha , 'Reject Null Hypothesis', '')\n    return gq_df\nGQ_test(ols, start_idx=23, alpha=.05)","c4708535":"bp = list(diagnostic.het_breuschpagan(res.resid, df.iloc[train_idx][['age','mileage']].values))\nprint(pd.DataFrame(bp, index=['LM statistic','p-value','F statistic','p-value'], columns=['Breusch-Pagan']))","cf3449bc":"white_mat = np.column_stack([np.ones(len(train_idx)), df.iloc[train_idx].age,df.iloc[train_idx].age**2])\ndiagnostic.het_white(res.resid, white_mat)","37777ffd":"print(res.get_robustcov_results('HC0').summary().tables[1])","ab3fd86e":"# Calculate weights array: h_est\nlog_e2 = np.log(res.resid**2)\ne2_mat = df.iloc[train_idx].copy()\ne2_mat['log_e2'] = log_e2\nf_e2 = 'log_e2~age + np.power(age,2) + mileage + np.power(mileage,2) + engine_power + car_type + feature_1 + feature_2 + feature_4 + feature_5 + feature_6 + feature_7 + feature_8 + model'\nh_est = np.exp(smf.ols(formula=f_e2, data=e2_mat).fit().fittedvalues)\n\n# Build GLS model using residual weights\nwls = smf.wls(ols.formula, data=df.iloc[train_idx],weights=1\/h_est)\nres_wls = wls.fit()\n\n# Test Heteroskedactisity in WLS\nbp_wls = list(diagnostic.het_breuschpagan(res_wls.wresid, df.iloc[train_idx][['age','mileage']].values))\nprint(pd.DataFrame(bp_wls, index=['LM statistic','p-value','F statistic','p-value'], columns=['Breusch-Pagan']))","7a9ccbfa":"# 1. determinant matrix\nX = res.model.exog\ndet(X.T@X)","49b4bcbe":"# 2. (X.T@X)^-1 @(X.T@X) = I\nnp.diag(np.round(inv(X.T@X)@(X.T@X), 7))","ebf3495c":"#Let's define a function to calculate Variance Influence Factor\ndef vif_test(ols, cont_cols:list =None):\n    \"\"\"\n    VIF test for collinearity of specific continuous variables\n    Inputs:\n    ols: ordinary least square model (statsmodels)\n    cont_cols: list contains the column numbers of variables you want to test\n    \"\"\"\n    if cont_cols is None:\n        cont_cols = list(range(ols.exog.shape[1]))\n    index = [ols.exog_names[n] for n in cont_cols]\n    vif = lambda ols, i: variance_inflation_factor(ols.exog, exog_idx=i)\n    return pd.DataFrame([vif(ols,i) for i in cont_cols], index=index, columns=['VIF'])\n\nvif_test(ols, [22,24,26])","149e83bc":"def norm(x):\n    return (x-x.mean())\/x.std()\n\nformula_n = '''\nnp.log(price) ~ norm(age) + np.power(norm(age),2) + norm(mileage) + np.power(norm(mileage),2) +\nnorm(engine_power) + car_type + feature_1 + feature_2 + feature_4 + feature_5 + feature_6 +\nfeature_7 + feature_8 + model\n''' \nols_norm = smf.ols(formula=formula_n, data=df, subset=train_idx)\nres_norm = ols_norm.fit()\nsummary = res_norm.summary()\nprint(summary.tables[1],'\\n',summary.extra_txt)\nvif_test(ols_norm, [22,24,26])","dca0d51e":"# Testing R2\npred(res_norm, x=df_test, y=df_test.price, log=True)","f960dc6c":"# There is a built in fuction in statsmodel and another in pandas plotting\nfig = plt.figure(figsize=(8,4))\nplot_acf(res_norm.resid, lags=25, zero=False, ax=fig.add_subplot(111));\nplt.show()","cea1a988":"names = ['LM-stat','LM:p-value','F-stat','F:p-value']\npd.DataFrame(diagnostic.acorr_breusch_godfrey(res_norm, nlags=20), index=names, columns=['Breusch-Godfrey'])","1bc1ae9d":"# Durbin Watson:\nfrom statsmodels.stats.stattools import durbin_watson\ndurbin_watson(res_norm.resid)","cd446580":"# Introduction:\n\nThis notebook is a practical example of the steps taken in multivariate linear regression. I tried to be as generic as possible, but in reality, every dataset is unique and requires a slightly different treatment. If you noticed any missing or unnecessary steps don't hesitate to leave a comment :) \n\nLet's denote the dependent variable as $Y$, and the independent variables (features) $X ~ (x_1, x_2\\dots x_n)$.  \n \n\n# 1. Ask the Right Question:\n   \nThe first and foremost is to state the objective of the analysis and be very clear about it. Here we're building a model to predict the price of used BMW given a list of features (age, mileage, color...etc)  \nLet's begin by importing the libraries needed. I kept the libraries in a `txt` file so the notebook is tidy!","95cbfa94":"\n#### 4.1.2.5 How to fix it?\n#### 4.1.2.5.A Heteroskadastic-Consistent Errors (HCE):\nAlso known as white errors. Use them to correct SE, and assess the coefficient's significance. This is a quick and dirty way that often produces variables larger than model-based estimators like WLS, GLS if the residual covariance matrix is correctly specified. However, it's very useful when the shape of heteroskedasticity is unknown.  \nAll we need to do is fit the model using one of the robust `cov_type`s, then use the new coefficient standard errors and p-values","3ac6634f":"The algorithm tried 131,070 combinations of $x$ variables, which are continuous, quadratic, and categorical. Let's list the top 10 by by `rsquared_adj`, `bic` and `aic`, then one that best fits the criteria. In this case, I would choose the model with the highest R-squared adjusted and lowest AIC, BIC with the least number of variables!","b68adeaf":"### 3.3 Model Selection\nThe most challenging step in any analysis is specifying the model to use; is it linear, log-linear, quadratic, or non-linear!? Which variables to omit and which to transform! What kind of transformation (polynomial, log)...etc.   \nWhen it comes to model selection keep in mind that the model's job is to approximate, and not perfectly represent reality. So keep it simple, but not too simple! \nYou must be able to explain the relationship in terms of how a unit increase in $X_i$ changes $Y$. And explain the economic sense of the model and how it adheres to its assumptions.  \n\n#### 3.3.1 Manual  approach\nIt's always good to examine the scatter plot and correlation matrix first to get a sense of the relationship between variables. From there write down an initial formula for the model, and build on it. ","c52ed1fe":"Right off the bat, mileage cannot be negative and engine power can't be zero! So let's fix them assuming this was a data-gathering error ","6e9edc21":"Bar plots across categorical variables reveal some underrepresented categories. An obvious example is the small number of petrol and electric cars vs. diesel, which doesn't represent the population!  \nAnother example is the imbalance between the `paint_color` sub-categories. This is justifiable because some car colors are more popular. Same goes for `key_model` and `car_type` (Convertable, Coupe, subcompacts, and vans)    \n\n\n**Conclusion**  \nSample data should reflect the population irrespective of how extreme we perceive it. It should also make economic sense! \nOne of the consequences is failing coefficient *t-test*, and ending up with insignificant coefficients. To avoid that, I'm going to merge the underrepresented categories into a single one.","1274cc61":"**Conclusion:** No N\/As to worry about.\n\n## 2.2 Outliers\n\nSometimes outliers are bad data that you must get rid of, and sometimes they're Michael Jordan or Shaquille O'Neal and you should keep. Either way, outliers must be identified before dealing with them.  \nThe two most common definitions of outliers are: \n1. Observations that are more than 3 standard deviations away from the mean.\n2. Observations that are more than 1.5 * Inter-Quartile-Range (IQR) below\/above the 25th\/75th percentiles (ends of the boxplot).\n\n**More on outliers detection**   \nThe most common techniques used to identify outliers by data types is:  \n- Univariate data -> boxplot. outside of 1.5 times the inter-quartile range is an outlier.  \n- Bivariate -> scatterplot with confidence ellipse. outside of, say, 95% confidence ellipse is an outlier.  \n- Multivariate -> Mahalanobis D2 distance   \n\nOnce defined, mark those observations as outliers. Then run a logistic regression to see if there are any systematic patterns. ","0548d793":"#### 4.1.2.5.B Weighted Least Squares (WLS)\nI'm not going to get into the math of WLS, but it is a special case of Generalized Least Squares (GLS) that assumes no autocorrelation. i.e. covariance matrix is diagonal that has *some* form of heteroskedasticity! Feasible GLS Procedure:\n1. Estimate the regression via OLS\n2. Calculate $log(\\epsilon^2)$\n3. estimate the regression $log(\\epsilon^2)=X\\alpha+v_i$ and calculate the fitted values.\n4. Take the exponent of fitted values $\\hat{h_i}=e^{\\hat{log(\\epsilon_i^2)}}$\n5. Estimate regression $Y=X\\beta + \\epsilon$ via WLS using weights $w_i^{-1}=1\/\\sqrt{\\hat{h_i}}$","06f8771b":"**Conclusion:**\nThere are some outliers in $X$ and $Y$, but at this point, I'm going to leave them as is. Because they appear to be extreme readings. I will revisit this assumption after building the model though \n\n## 2.3 Imbalances:\n\nIn general, having balanced data results in higher accuracy models. An underrepresented group can lead to an insignificant coefficient.  \nThis is true for all regression types and machine learning algorithms. ","5ca8686d":"# 2. Read & Clean Data:\nThe second step is to gather data. But in our case we're using a ready multivariate dataset that contains selling prices of +4800 used BMW cars and their models, mileage, color, fuel type...etc","3b9dd9fd":"**Conclusion**  \nVisually, I don't see a particular increasing or decreasing pattern to confirm or deny heteroskedasticity. So let's run some tests\n\n#### 4.1.2.2 Goldfeld-Quandt test:\nIt's based on two subsamples with two different variances.  \n$H_0:$ no heteroskadasticity","997bc941":"**Conclusion**: normalizing data has fixed the condition number and improved the significance of the `age` coefficient. It also reduced its VIF number below 5 which is within the normal range","2b1e55f6":"The next step is to examine the continuous $X$ and $Y$ variables:","e5edf716":"**Impression**  \nComparing manual and systematic models shows that the forward-selection method has dropped `fuel` and `color` because they didn't improve the chosen metrics. If you remember, in the manual model the two variables had insignificant coefficients. I would've dropped them had I chosen to continue with that approach! This confirms that the algorithm adds value. We can reduce exploration time by building a more efficient search algorithm instead of the brute force one applied here. Like Monte Carlo Tree or A* search to name a few. \nIn any event, it doesn't matter which model we begin with. The next steps are very similar in terms of improving and validating it.\n\nThat said, let's use the algorithmic output as a base formula for our model.","d0064b7c":"## 2.1 Examin data types & corrupt data:\n\nLooking at the table above I noticed the following:\n- There are 12 categorical and 3 continuous variables (including the dependent variable)\n- We can calculate the age of each car at the time of sale from the two date fields `registration_date` and `sold_at`\n- I think the car's model is relevant when it comes to predicting price. But the `model_key` column has way too many categories, so I'm going to combine them into fewer values in a new field using the BMW series model.","9525ac51":"#### 3.2.3 Predictor function\nIt convenient to write an $R^2$ function to test data with later","b0f4223a":"# 4. Validating the Model\n\nFor the model to be valid, it should not violate the assumptions of linear regression. And it has to make economic sense, has significant coefficients, and acceptable performance metrics! In other words, it must be BLUE (Best Linear Unbiased Efficient)\n\n## 4.1 Model Assumptions\nThe assumptions of Multivariate Linear Regression are:\n- MR.1: The population is described by linear model $Y = X\\beta + \\epsilon$\n- MR.2: **Strict Exogeneity**: conditional expectation of error term given all observations is zero: $E(\\epsilon |X)=0$\n- MR.3: **Conditional homoskedasticity** the variance-covariance matrix of the error term conditional on X is constant\n$$Var(\\epsilon|X) =\\sigma^2.I $$\n- MR.4: **Conditional uncorrelated errors** the covariance between different error term pairs, conditional on X is zero:\n- MR.5 There exists no exact linear relationship between the explanatory variables. This means that:\n$$ rank(X) = K+1$$\n$$det(X^TX) \\neq 0$$\n- MR.6: **(Optional)** the residulas are normally distributed i.e:\n$\\epsilon|X \\in N(0,\\sigma^2I$\n\nEach of these assumptions must be validated\/tested.  \n\n\n### 4.1.1 Linearity Tests:\nThis is the first assumption of linear regression. Breaching it is considered very series because it means the model is not the best representation of the data.  \nIf the scatter plot between $X$ and $y$ doesn't clearly show the relationship, we revert to numerical methods to test linearity.\n\n#### 4.1.1.2 Harvey-Collier Test for Linearity:\nHarvey-Collier performs a t-test on the recursive residuals. If the true relationship is non-linear (i.e. convex or concave) the mean of the recursive residuals is significantly different from zero.  \n\n*$H_0:$ The model is correctly specified as linear*","db67989a":"**Conclusion**Since the p-value is less than $\\alpha$ (0.05) We reject the null hypothesis and conclude the presence of heteroskedasticity in `age` and `mileage`\n\n*Note: \n\n#### 4.1.2.4 White test\nEven more generalized test. Its design allows it to include not only exogenous variables but also their polynomial and interaction terms. Its null is: $H_0:$ No heteroskedasticity\n","7f3b1c7e":"### 3.3.1 Forward Model Selection\nAnother approach is to use a *forward selection* algorithm. It applies brute force to fit all the possible combinations of variables and pick the one\/s with the best metrics (AIC, BIC, R2 Adjusted ... etc.)   \nThe approach is greedy by nature, which means we'll end up omitting some variables the algorithm had included!","266a94d3":"**Impression**\nThe relationship between `price` and both `mileage` and `engine_power` is correctly specified as linear. However, its relationship with `age` does not appear linear, as the scatter plot shows. That said, we can still build a linear model since the other variables are linear and either omit `age` altogether or leave it as is. The alternative is to build a non-linear algorithm, which is out of the scope of this notebook","f384d96c":"**Conclusion** examining the correlogram plot, BG and DW number, shows no signs of autocorrelation in our residuals at the 20th lag\n\n","e5867fa9":"## 2.4 Relationship between Variables\nTo examine the relationship between variables we can:\n1. scatter plot them\n2. calculate their correlation\nLet's do both","40f6ab97":"# Appendex\nHere are some important definitions and explanations of regression outputs often used in statistical analysis.\n\n#### Bias of Estimator\nBias is a measure of the difference between the estimator's expected and true values. As such, a biased estimator is one with coefficients that are very different from the true values \n\n#### Efficiency of Estimator\nAn efficient estimator is the 'best possible' estimator of the parameter of interest that minimizes the loss function. In linear regression, it's the parameter that provides the smallest variance.\n\n#### 7.1 R-squared\nR-squared\/Adj.R-squared: 90.4% of the variability in the dependent variable is explained by the independent variables. This is high enough for the training set, but let's see how well the model deals with data it hasn't seen (test_set)\nTest R-squared at 90.5% is as high as train R-squared, which means the model generalizes well\n\n\n#### 7.2 F-statistic|Prob(F)\nIt tests how well the independent variables as a group explain the variations in the dependent variable (i.e. tests the null hypothesis $H_0: \\beta_1=\\beta_2=\\dots \\beta_n=0$   \nP-value of F-statistic, or the probability of type I error (observation happening giving the null hypothesis is true)  \nIn this case, F-statistic is very large and Prob(F) is zero, which is great!  \n\n#### 7.3 AIC:\nThe Akaike Information Criterion (AIC) measures overfit. It rewards the model for goodness-of-fit and penalizes it if it became overly complicated.  \nAIC is useful when comparing models but very hard to interpret standalone!  \n\n#### 7.4 Omnibus\/Prob(Omnibus):\nTests the normality of the residuals. We hope to see a value close to zero for the Omnibus number and a value close to one for the Prob (Omnibus).\n**Omnibus: the closer to zero the better***  \n**Prob(Omnibus): The closer to one the better***  \n\nIn this case, Omnibus is relatively high and the Prob (Omnibus) is relatively low so the data is far from normal. \n\n#### 7.5 Skew \nMeasures data symmetry. We want to see something close to zero, indicating the residual distribution is normal. Note that this value also drives the Omnibus.  \n**The closer to zero the better***   \n\n#### 7.6 Kurtosis\nA measure of peakiness, or data curvature. Higher peaks lead to greater Kurtosis, and in the case of residuals, a greater Kurtosis means tighter clustering around zero, which implies a better model with few outliers.  \n**The higher the better**  \n\n#### 7.7 Durbin-Watson\nTests for serial correlation. A value between 1.5 and 2.5 means there is no serial correlation. In this case, the number is within the acceptable range.  \n\n#### 7.8 Jarque-Bera (JB)\/Prob(JB)\nLike the Omnibus test, it measures both skewness and kurtosis. We hope to see in this test confirmation of the Omnibus test.   \n\n#### 7.9 Condition Number\nMeasures the sensitivity of a function's output as compared to its input. When we have multicollinearity, we can expect much higher fluctuations to small changes in the data, hence, we hope to see a relatively small number, something below 30.  \n**The smaller below 30 the better**\n\n\nGood reading on regression violations and how to correct them:  \nhttps:\/\/people.duke.edu\/~rnau\/testing.htm\n","7eefe13f":"### 4.1.4 Autocorrelation\nSerial or autocorrelation is when error terms are correlated with a lagged version of themselves. In this case the covariance matrix remains diagonal with $\\sigma$ but the $Cov(\\epsilon_i,\\epsilon_j) \\neq 0$  \nThe consequences of this are:\n- OLS estimators are not efficient (do not minimize the loss function) \n- The variances of the estimators are biased and inconsistent\n- Invalid *t-statistics* of OLS estimators (i.e. wrong p-values)\n\nHowever, the estimators themselves remain unbiased (i.e. the distance between expected and true values)   \n\nThe **General approach** to deal with autocorrelation usually follows the below steps:   \n\n1. Assume the true model is $Y=\\beta.X+\\epsilon$\n2. Create a new model using OLS residuals. For example, the residual auxiliary regression in the Breusch-Godfrey test includes the explanatory variables from the original model and n lagged residuals: \n$$\\hat{\\epsilon_i}^2 = \\alpha_0 + \\alpha_1X_{1,i}+\\dots +\\alpha_k X_{k,i}+\\rho_1 \\hat{\\epsilon}_{i-1}^2+\\rho_2 \\hat{\\epsilon}_{i-2}^2+\\dots+ \\rho_p \\hat{\\epsilon}_{i-p}^2 + \\mu_t$$\n3. Test the null hypothesis that the residuals are serially correlated using OLS residual auxiliary regression:\n$$H_0: \\rho_1=\\rho_2=\\dots=\\rho_p=0$$\n4. If we fail to reject the null hypothesis -> use the usual OLS estimators from step 1.\n5. If we reject the null hypothesis there are three ways to go about it:  \n    a. Continue to use the OLS estimators but correct their variance (i.e. make them consistent)  \n    b. Instead of OLS, use FGLS (and its variations).  \n    c. Specify a different model that, hopefully, accounts for serial correlation\n\n#### 4.1.4.1 How to detect it:\nWe start with a residual correlogram plot, and if it didn't show clear relation we move to numerical tests. The most common of which are **Durbin-Watson** and **Breusch-Godfrey** tests. Both assume no serial correlation at $\\alpha$ level","53e8ce3d":"`age` is showing moderate collinearity but before we take an action let's normalize that data to see if that helps. To do that we can either use ","c1112c57":"The first two tests don't show any signs of multicollinearity but to be on the safe side let's run a variance influence factor test on the continuous variables `age`, `mileage`, and `engine_power`. We are not concerned about the artificial multicollinearity that results from a polynomial transformation of independent variables","e3dfcfce":"#### 4.1.2.3 Breusch-Pagan test\nThis is a general heteroskedasticity test that returns two statistics `LM` and `F-statistic`. Its null hypothesis is: \n$H_0:$ no heterskadasticity at $\\alpha$ level.  \nWe run the test between the residuals and the group of $X$ variables we suspect of being related to heteroskedasticity. In our case, it's `age` and `mileage`, as shown by the GQ test\n\n*Note: p-value can change with different training sets. Sometimes it confirms heteroskedasticity in `age` and `mileage`, other times only `age`*","7139a4f1":"### 4.1.2 Heteroskadasticity:\n\nHeteroskedasticity occurs when the variance of the error term is not constant across observations. This breaches assumption MR.3 of linear regression and causes:\n\n- the variance of the estimators to become biased and inconsistent\n- the OLS estimator no longer has Student $t$ distribution, which renders *t-test* invalid. Even for large data samples\nHowever, OLS estimators themselves remain unbiased\n\n#### 4.1.2.1 Residual Plot diagram:\nRun sequence and vs predicted values, and against each of the independent variable","4ba2e79d":"The second to last regression have used the smallest number of variables providing almost the same $R^2$, BIC, and AIC. ","62969518":"**Conclusion**: using WLS decreased heteroskedasticity but it didn't eliminate it. This is mainly because we don't know the true form of heteroskedasticity in residuals. In practice, it's acceptable to use white corrected standard errors (Hansen method) \n\n### 4.1.3 Multicollinearity:\n\nIs the situation in which two or more explanatory variables are highly **linearly** correlated. This is a violation of assumption MR.5 of linear regression that causes the determinant of $(X^T.X)$ matrix to be zero, which results in an invertible matrix. However, In real life, such a relationship is almost always approximate (never perfect), and therefore $det(X^TX)$ is close to zero. The consequences are:\n\n- Imprecise estimators\n- Large coefficient standard errors -> failure to reject their significance\n- Small changes to input data can lead to big changes in the model (even different signs of the coefficients). This can be examined using cv training\n- The usual interpretation of the change in one variable holding all else equal does not hold anymore\n- Overfitting, since the variables contain the same data (or partially the same)\n\n\n#### 4.1.3.1 How to detect it?\nThe obvious red flags are large F-statistic accompanied by insignificant coefficients (small p-values) and big changes in coefficients when training on different data blocks.\n\n1. $det(X^TX) \\neq 0$  \n2. $(X^TX)^{-1}(X^TX)=I \\neq 0$\n3. Correlation between variables\n4. **Variance Inflation Factor**: runs a regression between the selected $X_i$ and other independent variables and claculates $VIF=1\/(1-R^2)$. Any value above 10 is not good, below 5 is acceptable\n\nWe've examined the correlation matrix in step 2.4 and found a moderate positive association between `age` and `mileage`, which might indicate multicollinearity!  \nHowever, what I'm more concerned about is the second warning of regression summary:  \n`[2] The condition number is large, 1.04e+12. This might indicate that there are\nstrong multicollinearity or other numerical problems.`  \n\nSo let's run the standard collinearity tests then try to address the condition number by normalizing data. This can address collinearity and other numerical problems.","0728cb17":"#### 3.2.2 Normalize data (optional but important)\nNormalizing data improves numerical stability and produces cleaner results. It doesn't improve accuracy or prediction power though","b909b92c":"\n# 3. Analyzing Data\n\nThis step should answer the question in step (1).i.e. predict BMW cars' prices given a set of their specifications and features. \n\n### 3.2 Pre-Processing\n#### 3.2.1 Split Data\n\nIt's standard practice to split the data into training and test sets to see how the model behaves when presented with data it hasn't seen. ","667bd564":"**Conclusion**:  \n1. None of the continuous variables is normally distributed. Therefore, I don't expect the residuals to be normally distributed either (not without transforming variables that is)\n2. There appear to be negative relationships between `price` and each of `mileage` and `age` albeit not straight lines! This is confirmed by their moderate negative correlation of `-0.41` and `-0.45` respectively.\n\n3. Engine power and price have a moderate positive relationship that appears to be linear. This makes economic sense since higher engine power is indicative of more expensive sport or luxury models.\n4. There's a moderate positive correlation between the explanatory variables `age` and `mileage`. This makes sense as we expect older cars to have more mileage. This is something to keep an eye on when testing\/correcting multicollinearity","acd1a59f":"#### 4.1.1.3 Rainbow Test for Linearity:\nRainbow test assums that the **true** model is non-linear with additional parameters $Z.\\theta$. Where $Z$ is an $r$ x $K$ matrix.  \nThe hypothesis test to prove linearity is\n\n*$H_0:\\theta = 0$ (The model is correctly specificed as linear)*  \n*$H_1:\\theta \\neq 0$*"}}