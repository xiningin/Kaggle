{"cell_type":{"00773636":"code","a383f552":"code","e2fbd0ca":"code","9fb2eeec":"code","314071f9":"code","6e866581":"code","3db2a1d1":"code","f967dacf":"code","3470c09f":"code","0269110e":"code","21c2ab8c":"code","c53338e0":"code","0cf879c5":"code","35f7eb9d":"code","93937a7d":"code","50fa52b2":"code","ad080609":"code","b66a6267":"code","9501208c":"code","6657cbc4":"code","8ed5b5f2":"code","41af39e8":"code","9920343d":"code","1009e993":"code","800a25b2":"code","22867627":"code","72b68ba5":"code","2a603e77":"code","8c483d3b":"code","02b0f4d6":"code","75062524":"code","8d39441d":"code","d4bfc9ef":"code","f1695177":"code","33073976":"code","58a7b8d7":"code","fd42b14b":"code","f7b24516":"code","f4b8558b":"code","1f40b55a":"code","d0e18b45":"code","6704dceb":"code","74bb0511":"code","59ab43a1":"code","b286e39e":"code","47f6021c":"code","cb4f328f":"code","e416cd5e":"code","6f365c62":"code","2baabbf8":"code","ff2ef09a":"code","5de58ac8":"code","ca8aea2e":"code","450cadd8":"code","e8a85fc8":"code","80bf0958":"code","f65a2d8b":"code","3ab416b4":"code","05fb29fc":"markdown","64d8c257":"markdown","0c995f05":"markdown","d524a4b6":"markdown","5d648cf5":"markdown","489cc6ad":"markdown","12a5c5bc":"markdown","253fe751":"markdown","ee7e1fd2":"markdown","24d6cdb1":"markdown","e3366463":"markdown","17e1961c":"markdown","2c84ad4c":"markdown","00516c59":"markdown","e6216756":"markdown","0c4cb96a":"markdown","3541b829":"markdown","8a997ef1":"markdown","36dfad21":"markdown","f2c15b88":"markdown","bbe94d11":"markdown","9d1e2f12":"markdown","efab4a27":"markdown","02b3618d":"markdown","404987ee":"markdown","3835909c":"markdown","e1c463ae":"markdown","0017f979":"markdown","4953f598":"markdown","5d02662a":"markdown","1427105d":"markdown","a4f60f96":"markdown","da9960e5":"markdown","6480f44f":"markdown"},"source":{"00773636":"!pip install pyspark","a383f552":"import os\nimport pandas as pd\nimport numpy as np\n\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession, SQLContext\n\nfrom pyspark.sql.types import *\nimport pyspark.sql.functions as F\nfrom pyspark.sql.functions import udf, col\n\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.mllib.evaluation import RegressionMetrics\n\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\nfrom pyspark.ml.feature import VectorAssembler, StandardScaler\nfrom pyspark.ml.evaluation import RegressionEvaluator","e2fbd0ca":"import seaborn as sns\nimport matplotlib.pyplot as plt","9fb2eeec":"# Visualization\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\npd.set_option('display.max_columns', 200)\npd.set_option('display.max_colwidth', 400)\n\nfrom matplotlib import rcParams\nsns.set(context='notebook', style='whitegrid', rc={'figure.figsize': (18,4)})\nrcParams['figure.figsize'] = 18,4\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'","314071f9":"# setting random seed for notebook reproducability\nrnd_seed=23\nnp.random.seed=rnd_seed\nnp.random.set_state=rnd_seed","6e866581":"spark = SparkSession.builder.master(\"local[2]\").appName(\"Linear-Regression-California-Housing\").getOrCreate()","3db2a1d1":"spark","f967dacf":"sc = spark.sparkContext\nsc","3470c09f":"sqlContext = SQLContext(spark.sparkContext)\nsqlContext","0269110e":"HOUSING_DATA = '..\/input\/cal_housing.data'","21c2ab8c":"# define the schema, corresponding to a line in the csv data file.\nschema = StructType([\n    StructField(\"long\", FloatType(), nullable=True),\n    StructField(\"lat\", FloatType(), nullable=True),\n    StructField(\"medage\", FloatType(), nullable=True),\n    StructField(\"totrooms\", FloatType(), nullable=True),\n    StructField(\"totbdrms\", FloatType(), nullable=True),\n    StructField(\"pop\", FloatType(), nullable=True),\n    StructField(\"houshlds\", FloatType(), nullable=True),\n    StructField(\"medinc\", FloatType(), nullable=True),\n    StructField(\"medhv\", FloatType(), nullable=True)]\n)","c53338e0":"# Load housing data\nhousing_df = spark.read.csv(path=HOUSING_DATA, schema=schema).cache()","0cf879c5":"# Inspect first five rows\nhousing_df.take(5)","35f7eb9d":"# Show first five rows\nhousing_df.show(5)","93937a7d":"# show the dataframe columns\nhousing_df.columns","50fa52b2":"# show the schema of the dataframe\nhousing_df.printSchema()","ad080609":"# run a sample selection\nhousing_df.select('pop','totbdrms').show(10)","b66a6267":"# group by housingmedianage and see the distribution\nresult_df = housing_df.groupBy(\"medage\").count().sort(\"medage\", ascending=False)","9501208c":"result_df.show(10)","6657cbc4":"result_df.toPandas().plot.bar(x='medage',figsize=(14, 6))\n","8ed5b5f2":"housing_df.describe()","41af39e8":"(housing_df.describe().select(\n                    \"summary\",\n                    F.round(\"medage\", 4).alias(\"medage\"),\n                    F.round(\"totrooms\", 4).alias(\"totrooms\"),\n                    F.round(\"totbdrms\", 4).alias(\"totbdrms\"),\n                    F.round(\"pop\", 4).alias(\"pop\"),\n                    F.round(\"houshlds\", 4).alias(\"houshlds\"),\n                    F.round(\"medinc\", 4).alias(\"medinc\"),\n                    F.round(\"medhv\", 4).alias(\"medhv\"))\n                    .show())","9920343d":"# Adjust the values of `medianHouseValue`\nhousing_df = housing_df.withColumn(\"medhv\", col(\"medhv\")\/100000)","1009e993":"# Show the first 2 lines of `df`\nhousing_df.show(2)","800a25b2":"housing_df.columns","22867627":"# Add the new columns to `df`\nhousing_df = (housing_df.withColumn(\"rmsperhh\", F.round(col(\"totrooms\")\/col(\"houshlds\"), 2))\n                       .withColumn(\"popperhh\", F.round(col(\"pop\")\/col(\"houshlds\"), 2))\n                       .withColumn(\"bdrmsperrm\", F.round(col(\"totbdrms\")\/col(\"totrooms\"), 2)))","72b68ba5":"# Inspect the result\nhousing_df.show(5)","2a603e77":"# Re-order and select columns\nhousing_df = housing_df.select(\"medhv\", \n                              \"totbdrms\", \n                              \"pop\", \n                              \"houshlds\", \n                              \"medinc\", \n                              \"rmsperhh\", \n                              \"popperhh\", \n                              \"bdrmsperrm\")","8c483d3b":"featureCols = [\"totbdrms\", \"pop\", \"houshlds\", \"medinc\", \"rmsperhh\", \"popperhh\", \"bdrmsperrm\"]","02b0f4d6":"# put features into a feature vector column\nassembler = VectorAssembler(inputCols=featureCols, outputCol=\"features\") ","75062524":"assembled_df = assembler.transform(housing_df)","8d39441d":"assembled_df.show(10, truncate=False)","d4bfc9ef":"# Initialize the `standardScaler`\nstandardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")","f1695177":"# Fit the DataFrame to the scaler\nscaled_df = standardScaler.fit(assembled_df).transform(assembled_df)","33073976":"# Inspect the result\nscaled_df.select(\"features\", \"features_scaled\").show(10, truncate=False)","58a7b8d7":"# Split the data into train and test sets\ntrain_data, test_data = scaled_df.randomSplit([.8,.2], seed=rnd_seed)","fd42b14b":"train_data.columns","f7b24516":"# Initialize `lr`\nlr = (LinearRegression(featuresCol='features_scaled', labelCol=\"medhv\", predictionCol='predmedhv', \n                               maxIter=10, regParam=0.3, elasticNetParam=0.8, standardization=False))","f4b8558b":"# Fit the data to the model\nlinearModel = lr.fit(train_data)","1f40b55a":"# Coefficients for the model\nlinearModel.coefficients","d0e18b45":"featureCols","6704dceb":"# Intercept for the model\nlinearModel.intercept","74bb0511":"coeff_df = pd.DataFrame({\"Feature\": [\"Intercept\"] + featureCols, \"Co-efficients\": np.insert(linearModel.coefficients.toArray(), 0, linearModel.intercept)})\ncoeff_df = coeff_df[[\"Feature\", \"Co-efficients\"]]","59ab43a1":"coeff_df","b286e39e":"# Generate predictions\npredictions = linearModel.transform(test_data)","47f6021c":"# Extract the predictions and the \"known\" correct labels\npredandlabels = predictions.select(\"predmedhv\", \"medhv\")","cb4f328f":"predandlabels.show()","e416cd5e":"# Get the RMSE\nprint(\"RMSE: {0}\".format(linearModel.summary.rootMeanSquaredError))","6f365c62":"print(\"MAE: {0}\".format(linearModel.summary.meanAbsoluteError))","2baabbf8":"# Get the R2\nprint(\"R2: {0}\".format(linearModel.summary.r2))","ff2ef09a":"evaluator = RegressionEvaluator(predictionCol=\"predmedhv\", labelCol='medhv', metricName='rmse')\nprint(\"RMSE: {0}\".format(evaluator.evaluate(predandlabels)))","5de58ac8":"evaluator = RegressionEvaluator(predictionCol=\"predmedhv\", labelCol='medhv', metricName='mae')\nprint(\"MAE: {0}\".format(evaluator.evaluate(predandlabels)))","ca8aea2e":"evaluator = RegressionEvaluator(predictionCol=\"predmedhv\", labelCol='medhv', metricName='r2')\nprint(\"R2: {0}\".format(evaluator.evaluate(predandlabels)))","450cadd8":"# mllib is old so the methods are available in rdd\nmetrics = RegressionMetrics(predandlabels.rdd)","e8a85fc8":"print(\"RMSE: {0}\".format(metrics.rootMeanSquaredError))","80bf0958":"print(\"MAE: {0}\".format(metrics.meanAbsoluteError))","f65a2d8b":"print(\"R2: {0}\".format(metrics.r2))","3ab416b4":"spark.stop()","05fb29fc":"There's definitely some improvements needed to our model! If we want to continue with this model, we can play around with the parameters that we passed to your model, the variables that we included in your original DataFrame.","64d8c257":"## 1. Understanding the Data Set\n\nThe data contains one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people). In this sample a block group on average includes 1425.5 individuals living in a geographically compact area.\n\nThese spatial data contain 20,640 observations on housing prices with 9 economic variables:\n\n<p style=\"text-align: justify;\"><\/p>\n<pre><strong>Longitude:<\/strong>refers to the angular distance of a geographic place north or south of the earth\u2019s equator for each block group\n<strong>Latitude :<\/strong>refers to the angular distance of a geographic place east or west of the earth\u2019s equator for each block group\n<strong>Housing Median Age:<\/strong>is the median age of the people that belong to a block group. Note that the median is the value that lies at the midpoint of a frequency distribution of observed values\n<strong>Total Rooms:<\/strong>is the total number of rooms in the houses per block group\n<strong>Total Bedrooms:<\/strong>is the total number of bedrooms in the houses per block group\n<strong>Population:<\/strong>is the number of inhabitants of a block group\n<strong>Households:<\/strong>refers to units of houses and their occupants per block group\n<strong>Median Income:<\/strong>is used to register the median income of people that belong to a block group\n<strong>Median House Value:<\/strong>is the dependent variable and refers to the median house value per block group\n<\/pre>\n\nWhat's more, we also learn that all the block groups have zero entries for the independent and dependent variables have been excluded from the data.\n\nThe Median house value is the dependent variable and will be assigned the role of the target variable in our ML model.\n","0c995f05":"### The differences between mllib and ml:\n\n#### Spark Mllib\n\nspark.mllib contains the legacy API built on top of RDDs.\n\n#### Spark ML\n\nspark.ml provides higher-level API built on top of DataFrames for constructing ML pipelines.\n\nAs of Spark 2.0, the RDD-based APIs in the spark.mllib package have entered maintenance mode. The primary Machine Learning API for Spark is now the DataFrame-based API in the spark.ml package. Apache spark is recommended to use spark.ml\nMLlib will still support the RDD-based API in spark.mllib with bug fixes.\nAfter reaching feature parity (roughly estimated for Spark 2.3), the RDD-based API will be deprecated.\nWhy is MLlib switching to the DataFrame-based API?\nDataFrames provide a more user-friendly API than RDDs. The many benefits of DataFrames include Spark Datasources, SQL\/DataFrame queries, Tungsten and Catalyst optimizations, and uniform APIs across languages.\nThe DataFrame-based API for MLlib provides a uniform API across ML algorithms and across multiple languages.\nDataFrames facilitate practical ML Pipelines, particularly feature transformations. See the Pipelines guide for details.","d524a4b6":"### 4.1 Distribution of the median age of the people living in the area:","5d648cf5":"**Using the RegressionEvaluator from pyspark.ml package:**","489cc6ad":"## 5. Data Preprocessing\n\nWith all this information that we gathered from our small exploratory data analysis, we know enough to preprocess our data to feed it to the model.\n\n+ we shouldn't care about missing values; all zero values have been excluded from the data set.\n+ We should probably standardize our data, as we have seen that the range of minimum and maximum values is quite big.\n+ There are possibly some additional attributes that we could add, such as a feature that registers the number of bedrooms per room or the rooms per household.\n+ Our dependent variable is also quite big; To make our life easier, we'll have to adjust the values slightly.","12a5c5bc":"Specifying the schema when loading data into a DataFrame will give better performance than schema inference.","253fe751":"### 7. Building A Machine Learning Model With Spark ML\n\nWith all the preprocessing done, it's finally time to start building our Linear Regression model! Just like always, we first need to split the data into training and test sets. Luckily, this is no issue with the `randomSplit()` method:","ee7e1fd2":"## 3. Load The Data From a File Into a Dataframe","24d6cdb1":"**Using the RegressionMetrics from pyspark.mllib package:**","e3366463":"We pass in a list with two numbers that represent the size that we want your training and test sets to have and a seed, which is needed for reproducibility reasons.\n\n**Note** that the argument `elasticNetParam` corresponds to $\\alpha$ or the vertical intercept and that the `regParam` or the regularization paramater corresponds to $\\lambda$.","17e1961c":"**Use a VectorAssembler to put features into a feature vector column:**","2c84ad4c":"### 6.1 Feature Extraction\n\nNow that we have re-ordered the data, we're ready to normalize the data. We will choose the features to be normalized.","00516c59":"### 5.1 Preprocessing The Target Values\nFirst, let's start with the `medianHouseValue`, our dependent variable. To facilitate our working with the target values, we will express the house values in units of 100,000. That means that a target such as `452600.000000` should become `4.526`:","e6216756":"### 6.2 Standardization\n\nNext, we can finally scale the data using `StandardScaler`. The input columns are the `features`, and the output column with the rescaled that will be included in the scaled_df will be named `\"features_scaled\"`:","0c4cb96a":"If you like it, please vote :)","3541b829":"### About the models and imported classes:\n\n#### General concepts\nBriefly, feature is input; label is output. This applies to both classification and regression problems. A feature is one column of the data in your input set. For instance, if you're trying to predict the type of pet someone will choose, your input features might include age, home region, family income, etc.\nThe label is the final choice, such as dog, fish, iguana, rock, etc.\n\nOnce you've trained your model, you will give it sets of new input containing those features; it will return the predicted \"label\" (pet type) for that person.\n\n\n#### LinerRegression\nLinear regression is a linear model, e.g. a model that assumes a linear relationship between the input variables (x) and the single output variable (y). More specifically, that y can be calculated from a linear combination of the input variables (x).\n\n#### RegressionMetrics\nRegression predictive modeling are problems that involve predicting a numeric value. Metrics for regression involve calculating an error score to summarize the predictive skill of a model. How to calculate and report mean squared error, root mean squared error, and mean absolute error.\n\n#### ParamGridBuilder\nBuilder for a param grid used in grid search-based model selection. It can do many things such as add a boolean param to true or false, set the given parameters in this grid to fixed values and more. Finally, it builds and returns all combinations of parameters specified by the param grid.\n\n#### cross validation concept\nCross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample. The procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into. As such, the procedure is often called k-fold cross-validation.\n\n#### CrossValidator class\nK-fold cross validation performs model selection by splitting the dataset into a set of non-overlapping randomly partitioned folds which are used as separate training and test datasets e.g., with k=3 folds, K-fold cross validation will generate 3 (training, test) dataset pairs, each of which uses 2\/3 of the data for training and 1\/3 for testing. Each fold is used as the test set exactly once.\n\n#### CrossValidatorModel\nCrossValidatorModel contains the model with the highest average cross-validation metric across folds and uses this model to transform input data. CrossValidatorModel also tracks the metrics for each param map evaluated.\n\n#### VectorAssembler\nA feature transformer that merges multiple columns into a vector column. This requires one pass over the entire dataset. In case we need to infer column lengths from the data we require an additional call to the 'first' Dataset method, see 'handleInvalid' parameter.\n\n#### StandardScaler\nStandardizes features by removing the mean and scaling to unit variance using column summary statistics on the samples in the training set.\nThe \u201cunit std\u201d is computed using the corrected sample standard deviation, which is computed as the square root of the unbiased sample variance.\n\n#### RegressionEvaluator\nEvaluator for Regression, which expects input columns prediction, label and an optional weight column.","8a997ef1":"### 4.2 Summary Statistics:\n\nSpark DataFrames include some built-in functions for statistical processing. The describe() function performs summary statistics calculations on all numeric columns and returns them as a DataFrame.","36dfad21":"We can see that, for the first row, there are about 6.98 rooms per household, the households in the block group consist of about 2.5 people and the amount of bedrooms is quite low with 0.14:","f2c15b88":"## 6. Feature Engineering\nNow that we have adjusted the values in medianHouseValue, we will now add the following columns to the data set:\n\n+ Rooms per household which refers to the number of rooms in households per block group;\n+ Population per household, which basically gives us an indication of how many people live in households per block group; And\n+ Bedrooms per room which will give us an idea about how many rooms are bedrooms per block group;\n\nAs we're working with DataFrames, we can best use the `select()` method to select the columns that we're going to be working with, namely `totalRooms`, `households`, and `population`. Additionally, we have to indicate that we're working with columns by adding the `col()` function to our code. Otherwise, we won't be able to do element-wise operations like the division that we have in mind for these three variables:","bbe94d11":"## 8. Evaluating the Model\n\nWith our model in place, we can generate predictions for our test data: use the `transform()` method to predict the labels for our `test_data`. Then, we can use RDD operations to extract the predictions as well as the true labels from the DataFrame.","9d1e2f12":"### 8.3 Inspect the Metrics\n\nLooking at predicted values is one thing, but another and better thing is looking at some metrics to get a better idea of how good your model actually is.\n\n**Using the `LinearRegressionModel.summary` attribute:**\n\nNext, we can also use the `summary` attribute to pull up the `rootMeanSquaredError` and the `r2`.","efab4a27":"**Create an ElasticNet model:**\n\nElasticNet is a linear regression model trained with L1 and L2 prior as regularizer. This combination allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge. We control the convex combination of L1 and L2 using the l1_ratio parameter.\n\nElastic-net is useful when there are multiple features which are correlated with one another. Lasso is likely to pick one of these at random, while elastic-net is likely to pick both.\n\nA practical advantage of trading-off between Lasso and Ridge is it allows Elastic-Net to inherit some of Ridge\u2019s stability under rotation.\n\nThe objective function to minimize is in this case:\n\\begin{align}\nmin_w\\frac{1}{2n_{samples}}{\\parallel{X_w - y}\\parallel}^2_2 + \\alpha\\lambda{\\parallel{X_w - y}\\parallel}_1 + \\frac{\\alpha(1-\\lambda)}{2}{\\parallel{w}\\parallel}^2_2\n\\end{align}\n\nhttp:\/\/scikit-learn.org\/stable\/modules\/linear_model.html#elastic-net","02b3618d":"## 2. Creating the Spark Session","404987ee":"Most of the residents are either in their youth or they settle here during their senior years. Some data are showing median age < 10 which seems to be out of place.","3835909c":"Look at the minimum and maximum values of all the (numerical) attributes. We see that multiple attributes have a wide range of values: we will need to normalize your dataset.","e1c463ae":"## 4. Data Exploration","0017f979":"+ The RMSE measures how much error there is between two datasets comparing a predicted value and an observed or known value. The smaller an RMSE value, the closer predicted and observed values are.\n\n+ The R2 (\"R squared\") or the coefficient of determination is a measure that shows how close the data are to the fitted regression line. This score will always be between 0 and a 100% (or 0 to 1 in this case), where 0% indicates that the model explains none of the variability of the response data around its mean, and 100% indicates the opposite: it explains all the variability. That means that, in general, the higher the R-squared, the better the model fits our data.","4953f598":"Since we don't want to necessarily standardize our target values, we'll want to make sure to isolate those in our data set. Note also that this is the time to leave out variables that we might not want to consider in our analysis. In this case, let's leave out variables such as longitude, latitude, housingMedianAge and totalRooms.\n\nIn this case, we will use the `select()` method and passing the column names in the order that is more appropriate. In this case, the target variable medianHouseValue is put first, so that it won't be affected by the standardization.","5d02662a":"### 8.1 Inspect the Model Co-efficients","1427105d":"# Predicting House Prices with Apache Spark\n\n## LINEAR REGRESSION\n\nDataset: [California Housing](http:\/\/www.dcc.fc.up.pt\/~ltorgo\/Regression\/cal_housing.html).","a4f60f96":"### 8.2 Generating Predictions","da9960e5":"We can clearly see that the values have been adjusted correctly when we look at the result of the show() method:","6480f44f":"All the features have transformed into a Dense Vector."}}