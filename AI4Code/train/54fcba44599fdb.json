{"cell_type":{"86e66e95":"code","86c34bfb":"code","0344347c":"code","2e1f27fd":"code","78fae002":"code","a3158163":"code","aa43eb35":"code","093ef108":"code","728a18dd":"code","bff3862b":"code","056e5423":"code","96aeb445":"code","1c37e0be":"code","62482044":"code","331201b6":"code","4f8e563e":"code","0bc431c9":"code","05975960":"code","787937f4":"code","779e6ff3":"code","a5d3a4cd":"code","ed9797fb":"code","77fcf396":"code","67e96683":"code","59a85e04":"code","2edb75d8":"code","1ae4667a":"code","671ac472":"code","cf007459":"code","e7ae17c2":"code","24204d53":"code","79b6c43b":"code","2f33c183":"code","cb8b70ca":"code","05ddb2d9":"code","20ccf15d":"code","58525837":"code","4d39a588":"code","91c8d20f":"code","f0c9005c":"code","c4b7cfd6":"code","63a36219":"code","410c7819":"code","36c7a56e":"code","0375c496":"code","dc4d5d82":"code","c867885a":"code","eb7a2ad2":"code","0d7b4089":"code","7187e317":"code","3e0b5726":"code","2fe0d69b":"code","57c949be":"code","2dea806a":"code","5296482b":"code","a67021df":"code","131ce220":"code","7a003795":"markdown","f364a36e":"markdown","24f1e4c5":"markdown","e200f25b":"markdown","611b2fb0":"markdown","36ea84fd":"markdown","c8957489":"markdown","0deb6fae":"markdown","754c8303":"markdown","6d034fbf":"markdown","8f028835":"markdown","bfbc3f79":"markdown","66d5dbdf":"markdown","302f6eaf":"markdown","78d84682":"markdown","8ae247a5":"markdown","77c55c3f":"markdown","a12b6339":"markdown","3f433c99":"markdown","99111dc3":"markdown","2ad3e82f":"markdown","6c069299":"markdown","7c0d6b3f":"markdown","ccabd2dd":"markdown","11edf2c3":"markdown"},"source":{"86e66e95":"# install DeepForest\n\n!mkdir -p \/tmp\/pip\/cache\/\n\nimport os\nfrom shutil import copyfile\nfrom tqdm.notebook import tqdm\n\nsrc = '..\/input\/deep-forest-files\/'\ndst = '\/tmp\/pip\/cache\/'\nfor filename in tqdm(os.listdir(src)):\n    if '.xyz' in filename:\n        f = filename.split('.xyz')[0]\n        copyfile(src + filename, dst + f + '.tar.gz')\n    else:\n        copyfile(src + filename, dst + filename)\n        \n!pip install --no-index --find-links \/tmp\/pip\/cache\/ deep-forest\n\nfrom deepforest import CascadeForestRegressor as CFR\n\n# install TabNet\n!pip -q install ..\/input\/pytorchtabnet\/pytorch_tabnet-3.1.1-py3-none-any.whl","86c34bfb":"import numpy as np \nimport numpy.matlib\nimport datatable as dt\nimport pandas as pd\nimport glob\nimport gc\nimport pickle\nfrom collections import defaultdict\nfrom tqdm.auto import tqdm\nfrom numba import njit\nfrom joblib import Parallel, delayed, dump, load\n\nfrom sklearn.preprocessing import StandardScaler, PowerTransformer\nfrom sklearn.preprocessing import MinMaxScaler, QuantileTransformer, LabelEncoder\n\n\nfrom numba_functions import *\n\n# TF\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.models import Model\n\n# Torch and TabNet\nfrom pytorch_tabnet.metrics import Metric\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\nimport torch\nfrom torch.optim import Adam, SGD\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts","0344347c":"N_MINS = 5\nMIN_SIZE = 600 \/\/ N_MINS\nDEBUG = 0\nDEBUG_STOCK_ID = 31\n\n# CONSTANT\nMEAN = -5.762330803300896\nSTD = 0.6339307835941186\nEPS = 1e-9\n\n# path\npreprocessor_path = '..\/input\/optiver-final-preprocessors'","2e1f27fd":"if DEBUG:\n    df_result = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\n    df_result['row_id'] = [f'{x[0]}-{x[1]}' for x in df_result[['stock_id', 'time_id']].values]\nelse:\n    df_result = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\ndf_result.head()","78fae002":"if DEBUG:\n    list_train_book_data = glob.glob('..\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/*')\n    list_stock_id = sorted([int(path.split('=')[1]) for path in list_train_book_data])\n#     list_stock_id = [0, 31]\nelse:\n    list_test_book_data = glob.glob('..\/input\/optiver-realized-volatility-prediction\/book_test.parquet\/*')\n    list_stock_id = sorted([int(path.split('=')[1]) for path in list_test_book_data])","a3158163":"def calc_log_return(prices):\n    return np.log(prices).diff()\n\n\ndef transform_target(target):\n    return (np.log(target + EPS) - MEAN) \/ STD\n\n\ndef inverse_target(target):\n    return np.exp(MEAN + STD * target) - EPS\n\n\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n\n\ndef mspe_loss(y_true, y_pred):\n    y_true = K.exp(MEAN + STD * y_true) - EPS\n    y_pred = K.exp(MEAN + STD * y_pred) - EPS\n    return K.sqrt(K.mean(K.square((y_true - y_pred) \/ y_true)))\n\n\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False","aa43eb35":"def load_pickle(path):\n    with open(path, 'rb') as f:\n        data = pickle.load(f)\n    return data\n\n\ndef gen_row_id(df):\n    df['row_id'] = [f'{x[0]}-{x[1]}' for x in df[['stock_id', 'time_id']].values]\n    return df\n\n\ndef get_path_by_id(data_type, stock_id):\n    if DEBUG:\n        return f'..\/input\/optiver-realized-volatility-prediction\/{data_type}_train.parquet\/stock_id={stock_id}'\n    return f'..\/input\/optiver-realized-volatility-prediction\/{data_type}_test.parquet\/stock_id={stock_id}'","093ef108":"def unstack_agg(df, agg_col):\n    df = df.unstack(level=1)\n    df.columns = [f'{agg_col}_{k}' for k in df.columns]\n    return df.reset_index()\n\ndef init_feature_df(df_book, stock_id):\n    df_feature = pd.DataFrame(df_book['time_id'].unique())\n    df_feature['stock_id'] = stock_id\n    df_feature.columns = ['time_id', 'stock_id']\n    return df_feature[['stock_id', 'time_id']]\n\ndef add_stats(df, cols, data_name, suffix='', axis=0):\n    unwrap = lambda x: x.item() if len(cols) == 1 else x\n    df[f'{data_name}_{suffix}_mean'] = unwrap(df[cols].mean(axis=axis).values)\n    df[f'{data_name}_{suffix}_std'] = unwrap(df[cols].std(axis=axis).values)\n    df[f'{data_name}_{suffix}_skew'] = unwrap(df[cols].skew(axis=axis).values)\n    df[f'{data_name}_{suffix}_min'] = unwrap(df[cols].min(axis=axis).values)\n    df[f'{data_name}_{suffix}_q1'] = unwrap(df[cols].quantile(q=0.25, axis=axis).values)\n    df[f'{data_name}_{suffix}_q2'] = unwrap(df[cols].quantile(q=0.50, axis=axis).values)\n    df[f'{data_name}_{suffix}_q3'] = unwrap(df[cols].quantile(q=0.75, axis=axis).values)\n    df[f'{data_name}_{suffix}_max'] = unwrap(df[cols].max(axis=axis).values)\n    return df\n\ndef add_feature_min(df_feature, df, configs):\n    df['min_id'] = df['seconds_in_bucket'] \/\/ MIN_SIZE\n    df_gb_min = df.groupby(['time_id', 'min_id'])\n    for data_col, agg_func, agg_col in configs:\n        # agg by min\n        df_ = df_gb_min[data_col].agg(agg_func, engine='numba')\n        df_ = unstack_agg(df_, agg_col)\n        df_feature = df_feature.merge(df_, on=['time_id'], how='left')\n        # gen stats by min and by time\n        cols = [f'{agg_col}_{k}' for k in range(N_MINS)]\n        for c in cols:\n            if c not in df_feature:\n                df_feature[c] = 0\n        df_feature = add_stats(df_feature, cols=cols, data_name=agg_col, suffix='min', axis=1)\n    return df_feature.fillna(0.0)\n\ndef add_feature_time(df_feature, df, configs):\n    df_gb_time = df.groupby(['time_id'])\n    for data_col, agg_func, agg_col in configs:\n        # agg by time\n        df_ = df_gb_time[data_col].agg(agg_func, engine='numba')\n        df_.name = f'{agg_col}_time'\n        df_feature = df_feature.merge(df_, on=['time_id'], how='left')\n    return df_feature.fillna(0.0)\n\ndef ffill_book(df_book):\n    list_time_id_book = df_book.time_id.unique()\n    df_ = pd.DataFrame()\n    df_['time_id'] = np.matlib.repeat(list_time_id_book, 600)\n    df_['seconds_in_bucket'] = np.matlib.repmat(range(600), 1, len(list_time_id_book)).ravel()\n    df_book = df_.merge(df_book, on=['time_id', 'seconds_in_bucket'], how='left')\n    df_book = df_book.set_index('time_id').groupby(level='time_id').ffill().bfill().reset_index() \n    return df_book","728a18dd":"book_configs = [\n    ('log_return1', rv_numba, 'B_RV1'),\n    ('log_return2', rv_numba, 'B_RV2'),\n    ('seconds_in_bucket', count_numba, 'B_NROW'),\n    ('bid_vol1', sum_numba, 'B_BVOL1'),\n    ('bid_vol2', sum_numba, 'B_BVOL2'),\n    ('ask_vol1', sum_numba, 'B_AVOL1'),\n    ('ask_vol2', sum_numba, 'B_AVOL2'),\n]\n\nbook_configs_ffill = [\n    ('bid_price1', mean_numba, 'B_BP1'),\n    ('bid_price2', mean_numba, 'B_BP2'),\n    ('ask_price1', mean_numba, 'B_AP1'),\n    ('ask_price2', mean_numba, 'B_AP2'),\n    ('bid_size1', mean_numba, 'B_BS1'),\n    ('bid_size2', mean_numba, 'B_BS2'),\n    ('ask_size1', mean_numba, 'B_AS1'),\n    ('ask_size2', mean_numba, 'B_AS2'),\n    # new features\n    ('price1_diff', mean_numba, 'Z_P1-DIFF'),\n    ('price2_diff', mean_numba, 'Z_P2-DIFF'),\n    ('price1_dabs', mean_numba, 'Z_P1-DABS'),\n    ('price2_dabs', mean_numba, 'Z_P2-DABS'),\n    ('price_spread1', mean_numba, 'Z_SPREAD1'),\n]\n\ntrade_configs = [\n    ('vol', sum_numba, 'T_VOL'),\n    ('order_count', sum_numba, 'T_OC'),\n    ('size', sum_numba, 'T_SIZE'),\n    ('seconds_in_bucket', count_numba, 'T_NROW'),\n]","bff3862b":"def gen_df_feature(stock_id):\n    # -----------------------------------------------------------------\n    # Book data (no ffill)\n    book_parquet_path = get_path_by_id('book', stock_id)\n    df_book = pd.read_parquet(book_parquet_path)\n    df_book.iloc[:, 2:] = df_book.iloc[:, 2:].astype('float64')\n    df_book_ff = df_book.copy()\n    df_feature = init_feature_df(df_book, stock_id)\n    # add wap and log_return\n    df_book['wap1'] = calc_wap_njit(\n        df_book.bid_price1.values,\n        df_book.ask_price1.values,\n        df_book.bid_size1.values,\n        df_book.ask_size1.values\n    )\n    df_book['wap2'] = calc_wap_njit(\n        df_book.bid_price2.values,\n        df_book.ask_price2.values,\n        df_book.bid_size1.values + df_book.bid_size2.values,\n        df_book.ask_size1.values + df_book.ask_size2.values\n    )\n    df_book['log_return1'] = df_book.groupby(['time_id'])['wap1'].apply(calc_log_return).fillna(0)\n    df_book['log_return2'] = df_book.groupby(['time_id'])['wap2'].apply(calc_log_return).fillna(0)\n    # add vols\n    df_book['bid_vol1'] = prod_njit(df_book['bid_price1'].values, df_book['bid_size1'].values)\n    df_book['bid_vol2'] = prod_njit(df_book['bid_price2'].values, df_book['bid_size2'].values)\n    df_book['ask_vol1'] = prod_njit(df_book['ask_price1'].values, df_book['ask_size1'].values)\n    df_book['ask_vol2'] = prod_njit(df_book['ask_price2'].values, df_book['ask_size2'].values)\n    # generate book features\n    df_feature = add_feature_min(df_feature, df_book, book_configs)\n    df_feature = add_feature_time(df_feature, df_book, book_configs)\n\n\n    # -----------------------------------------------------------------\n    # Book data (ffill) \n    df_book_ff = ffill_book(df_book_ff)\n    # new features\n    df_book_ff['price1_diff'] = df_book_ff['ask_price1'] - df_book_ff['bid_price1']\n    df_book_ff['price2_diff'] = df_book_ff['ask_price2'] - df_book_ff['bid_price2']\n    df_book_ff['price1_dabs'] = df_book_ff['price1_diff'].abs()\n    df_book_ff['price2_dabs'] = df_book_ff['price2_diff'].abs()\n    df_book_ff['price_spread1'] = (df_book_ff['ask_price1'] - df_book_ff['bid_price1']) \/ (df_book_ff['ask_price1'] + df_book_ff['bid_price1'])\n    # generate book features\n    df_feature = add_feature_min(df_feature, df_book_ff, book_configs_ffill)\n    df_feature = add_feature_time(df_feature, df_book_ff, book_configs_ffill)\n    \n\n    # -----------------------------------------------------------------\n    # Trade data\n    trade_parquet_path = get_path_by_id('trade', stock_id)\n    df_trade = pd.read_parquet(trade_parquet_path)\n    df_trade.iloc[:, 2:] = df_trade.iloc[:, 2:].astype('float64')\n    # add vol\n    df_trade['vol'] = prod_njit(df_trade['price'].values, df_trade['size'].values)\n    # generate trade features\n    df_feature = add_feature_min(df_feature, df_trade, trade_configs)\n    df_feature = add_feature_time(df_feature, df_trade, trade_configs)\n\n\n    # -----------------------------------------------------------------\n    # Combined feature\n    log_return = df_trade.merge(df_book, on=['time_id', 'seconds_in_bucket'], how='left').groupby('time_id')['log_return1'].agg(lambda x: np.sum(np.square(x)))\n    total_log_return = df_book.groupby('time_id')['log_return1'].agg(lambda x: np.sum(np.square(x)))\n    df_feature['Z_RATIO'] = (log_return \/ total_log_return).values\n    df_feature['Z_RATIO'] = df_feature['Z_RATIO'].fillna(0.0)\n    return df_feature","056e5423":"list_dfs = Parallel(n_jobs=-1)(delayed(gen_df_feature)(stock_id) for stock_id in tqdm(list_stock_id))\ndf_train = pd.concat(list_dfs).reset_index(drop=True)\ndf_train = df_train.sort_values(['stock_id', 'time_id']).reset_index(drop=True)\ndf_train_nn = df_train.copy() # prepare for NN\n\nfea_cols = [c for c in df_train.columns if c.startswith('B_') or c.startswith('T_') or c.startswith('Z_')]\nfea_cols_TA = [f for f in fea_cols if 'min_' not in f]\ndf_time_mean = df_train.groupby('time_id')[fea_cols_TA].mean()\ndf_time_mean.columns = [f'{c}_TA_mean' for c in df_time_mean.columns]\ndf_time_mean = df_time_mean.reset_index()\ndf_train = df_train.merge(df_time_mean, on='time_id', how='left')\n\n# Save data for LGB\n# dt.Frame(df_train).to_csv('test_501_LGB.csv')","96aeb445":"def add_time_stats(df_train):\n    time_cols = [f for f in df_train.columns if f.endswith('_time')]\n    df_gp_stock = df_train.groupby('stock_id')\n    #\n    df_stats = df_gp_stock[time_cols].mean().reset_index()\n    df_stats.columns = ['stock_id'] + [f'{f}_mean' for f in time_cols]\n    df_train = df_train.merge(df_stats, on=['stock_id'], how='left')\n    #\n    df_stats = df_gp_stock[time_cols].std().reset_index()\n    df_stats.columns = ['stock_id'] + [f'{f}_std' for f in time_cols]\n    df_train = df_train.merge(df_stats, on=['stock_id'], how='left')\n    #\n    df_stats = df_gp_stock[time_cols].skew().reset_index()\n    df_stats.columns = ['stock_id'] + [f'{f}_skew' for f in time_cols]\n    df_train = df_train.merge(df_stats, on=['stock_id'], how='left')\n    #\n    df_stats = df_gp_stock[time_cols].min().reset_index()\n    df_stats.columns = ['stock_id'] + [f'{f}_min' for f in time_cols]\n    df_train = df_train.merge(df_stats, on=['stock_id'], how='left')\n    #\n    df_stats = df_gp_stock[time_cols].max().reset_index()\n    df_stats.columns = ['stock_id'] + [f'{f}_max' for f in time_cols]\n    df_train = df_train.merge(df_stats, on=['stock_id'], how='left')\n    #\n    df_stats = df_gp_stock[time_cols].quantile(0.25).reset_index()\n    df_stats.columns = ['stock_id'] + [f'{f}_q1' for f in time_cols]\n    df_train = df_train.merge(df_stats, on=['stock_id'], how='left')\n    #\n    df_stats = df_gp_stock[time_cols].quantile(0.50).reset_index()\n    df_stats.columns = ['stock_id'] + [f'{f}_q2' for f in time_cols]\n    df_train = df_train.merge(df_stats, on=['stock_id'], how='left')\n    #\n    df_stats = df_gp_stock[time_cols].quantile(0.75).reset_index()\n    df_stats.columns = ['stock_id'] + [f'{f}_q3' for f in time_cols]\n    df_train = df_train.merge(df_stats, on=['stock_id'], how='left')\n    return df_train.fillna(0.0)","1c37e0be":"# Power transformation\ndf_train = df_train_nn\npipe = load_pickle(f'..\/input\/{preprocessor_path}\/pipe.pkl')\ndf_train[fea_cols] = pipe.transform(df_train[fea_cols].values)\ndf_train[fea_cols] = df_train[fea_cols].clip(-10, 10, axis=1)\n\n# Add groupby time_id features\nfea_cols_TA = [f for f in fea_cols if 'min_' not in f]\ndf_time_mean = df_train.groupby('time_id')[fea_cols_TA].mean()\ndf_time_mean.columns = [f'{c}_TA_mean' for c in df_time_mean.columns]\ndf_time_mean = df_time_mean.reset_index()\ndf_train = df_train.merge(df_time_mean, on='time_id', how='left')\n\n# Add groupby stock_id features\ndf_train = add_time_stats(df_train)\n\n# Save data for NN\ndt.Frame(df_train).to_csv('test_501_NN.csv')","62482044":"del df_time_mean, list_dfs, df_train, df_train_nn\nx = gc.collect()","331201b6":"# df_test = dt.fread('test_501_LGB.csv').to_pandas()\n# fea_cols = ['stock_id'] + [f for f in df_test.columns if f.startswith('B_') or f.startswith('T_') or f.startswith('Z_')]\n# X_test = df_test[fea_cols].values\n# for i in range(N_FOLD):\n#     lgb_model_path = f'{model_path}\/lgb_501_{i}.pkl'\n#     lgb_model = load_pickle(lgb_model_path)\n#     df_test[f'lgb_{i}'] = lgb_model.predict(X_test)\n# df_test['pred_501-lgb'] = df_test[[f'lgb_{i}' for i in range(N_FOLD)]].mean(axis=1)\n# df_result = df_result.merge(df_test[['stock_id', 'time_id', 'pred_501-lgb']], on=['stock_id', 'time_id'])\n# df_result.head()","4f8e563e":"# del df_test, X_test\n# x = gc.collect()","0bc431c9":"N_SEED = 5\nN_FOLD = 10\n\ndf_test = dt.fread('test_501_NN.csv').to_pandas()\nfea_cols = [f for f in df_test.columns if f.startswith('B_') or f.startswith('T_') or f.startswith('Z_')]\nX_test = df_test[fea_cols].values\nmodel_folder = '..\/input\/optiver-final-501-mlp'\nfor i_seed in range(N_SEED):\n    for i_fold in range(N_FOLD):\n        model_path = f'{model_folder}\/model_{i_seed}_{i_fold}.hdf5'\n        model = tf.keras.models.load_model(model_path, custom_objects={'mspe_loss': mspe_loss})\n        df_test[f'pred_{i_seed}_{i_fold}'] = model.predict(X_test, batch_size=1024)\n\ndf_test['pred_501_mlp'] = df_test[[f'pred_{i_seed}_{i_fold}' for i_seed in range(N_SEED) for i_fold in range(N_FOLD)]].mean(axis=1)\ndf_test['pred_501_mlp'] = inverse_target(df_test['pred_501_mlp'])\ndf_result = df_result.merge(df_test[['stock_id', 'time_id', 'pred_501_mlp']], on=['stock_id', 'time_id'])\ndf_result.head()","05975960":"N_SEED = 5\nN_FOLD = 10\n\ndf_test = dt.fread('test_501_NN.csv').to_pandas()\nfea_cols = [f for f in df_test.columns if f.startswith('B_') or f.startswith('T_') or f.startswith('Z_')]\nX_test = df_test[fea_cols].values\nmodel_folder = '..\/input\/optiver-final-501-1dcnn'\nfor i_seed in range(N_SEED):\n    for i_fold in range(N_FOLD):\n        model_path = f'{model_folder}\/model_{i_seed}_{i_fold}.hdf5'\n        model = tf.keras.models.load_model(model_path, custom_objects={'mspe_loss': mspe_loss})\n        df_test[f'pred_{i_seed}_{i_fold}'] = model.predict(X_test, batch_size=1024)\n\ndf_test['pred_501_1dcnn'] = df_test[[f'pred_{i_seed}_{i_fold}' for i_seed in range(N_SEED) for i_fold in range(N_FOLD)]].mean(axis=1)\ndf_test['pred_501_1dcnn'] = inverse_target(df_test['pred_501_1dcnn'])\ndf_result = df_result.merge(df_test[['stock_id', 'time_id', 'pred_501_1dcnn']], on=['stock_id', 'time_id'])\ndf_result.head()","787937f4":"N_SEED = 3\nN_FOLD = 5\n\ndf_test = dt.fread('test_501_NN.csv').to_pandas()\nfea_cols = [f for f in df_test.columns if f.startswith('B_') or f.startswith('T_') or f.startswith('Z_')]\nX_test = df_test[fea_cols].values\nmodel_folder = '..\/input\/optiver-final-501-tabnet'\nfor i_seed in range(N_SEED):\n    for i_fold in range(N_FOLD):\n        model_path = f'{model_folder}\/model_{i_seed}_{i_fold}.xyz'\n        model = TabNetRegressor()\n        model.load_model(model_path)\n        df_test[f'pred_{i_seed}_{i_fold}'] = model.predict(X_test)\n\ndf_test['pred_501_tabnet'] = df_test[[f'pred_{i_seed}_{i_fold}' for i_seed in range(N_SEED) for i_fold in range(N_FOLD)]].mean(axis=1)\ndf_test['pred_501_tabnet'] = inverse_target(df_test['pred_501_tabnet'])\ndf_result = df_result.merge(df_test[['stock_id', 'time_id', 'pred_501_tabnet']], on=['stock_id', 'time_id'])\ndf_result.head()","779e6ff3":"N_SEED = 3\nN_FOLD = 5\n\ndf_test = dt.fread('test_501_NN.csv').to_pandas()\nfea_cols = [f for f in df_test.columns if f.startswith('B_') or f.startswith('T_') or f.startswith('Z_')]\nX_test = df_test[fea_cols].values\nmodel_folder = '..\/input\/optiver-final-501-unet'\nfor i_seed in range(N_SEED):\n    for i_fold in range(N_FOLD):\n        model_path = f'{model_folder}\/model_{i_seed}_{i_fold}.hdf5'\n        model = tf.keras.models.load_model(model_path, custom_objects={'mspe_loss': mspe_loss})\n        df_test[f'pred_{i_seed}_{i_fold}'] = inverse_target(model.predict(X_test, batch_size=1024))\n\ndf_test['pred_501_unet'] = df_test[[f'pred_{i_seed}_{i_fold}' for i_seed in range(N_SEED) for i_fold in range(N_FOLD)]].mean(axis=1)\ndf_result = df_result.merge(df_test[['stock_id', 'time_id', 'pred_501_unet']], on=['stock_id', 'time_id'])\ndf_result.head()","a5d3a4cd":"N_SEED = 3\nN_FOLD = 5\n\ndf_test = dt.fread('test_501_NN.csv').to_pandas()\nfea_cols = [f for f in df_test.columns if f.startswith('B_') or f.startswith('T_') or f.startswith('Z_')]\nX_test = df_test[fea_cols].values\nmodel_folder = '..\/input\/optiver-final-501-wavenet'\nfor i_seed in range(N_SEED):\n    for i_fold in range(N_FOLD):\n        model_path = f'{model_folder}\/model_{i_seed}_{i_fold}.hdf5'\n        model = tf.keras.models.load_model(model_path, custom_objects={'mspe_loss': mspe_loss})\n        df_test[f'pred_{i_seed}_{i_fold}'] = inverse_target(model.predict(X_test, batch_size=1024))\n\ndf_test['pred_501_wavenet'] = df_test[[f'pred_{i_seed}_{i_fold}' for i_seed in range(N_SEED) for i_fold in range(N_FOLD)]].mean(axis=1)\ndf_result = df_result.merge(df_test[['stock_id', 'time_id', 'pred_501_wavenet']], on=['stock_id', 'time_id'])\ndf_result.head()","ed9797fb":"del df_test, X_test\nx = gc.collect()","77fcf396":"# for pandas aggregation\n@njit\ndef rv_fast(x): return np.sqrt(np.nansum(x**2))\nfast_rv = lambda x: rv_fast(x.values)\nfast_rv.__name__ = 'realized_volatility'\n\n@njit\ndef sum_fast(x): return np.sum(x)\nfast_sum = lambda x: sum_fast(x.values)\nfast_sum.__name__ = 'sum'\n\n@njit\ndef mean_fast(x): return np.mean(x)\nfast_mean = lambda x: mean_fast(x.values)\nfast_mean.__name__ = 'mean'\n\n# @njit\ndef std_fast(x): return np.std(x, ddof=1)\nfast_std = lambda x: std_fast(x.values)\nfast_std.__name__ = 'std'\n\n@njit\ndef min_fast(x): return np.min(x)\nfast_min = lambda x: min_fast(x.values)\nfast_min.__name__ = 'min'\n\n@njit\ndef max_fast(x): return np.max(x)\nfast_max = lambda x: max_fast(x.values)\nfast_max.__name__ = 'max'\n\n@njit\ndef count_fast(x): return len(np.unique(x))\nfast_count = lambda x: count_fast(x.values)\nfast_count.__name__ = 'count_unique'","67e96683":"data_dir = '..\/input\/optiver-realized-volatility-prediction\/'\n\n# Function to calculate first WAP\ndef calc_wap1(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n    return wap\n\n# Function to calculate second WAP\ndef calc_wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) \/ (df['bid_size2'] + df['ask_size2'])\n    return wap\n\ndef calc_wap3(df):\n    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef calc_wap4(df):\n    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) \/ (df['bid_size2'] + df['ask_size2'])\n    return wap\n\n# Function to calculate the log of the return\n# Remember that logb(x \/ y) = logb(x) - logb(y)\ndef log_return(series):\n    return np.log(series).diff()\n\n# Function to read our base train and test set\ndef read_train_test():\n    train = pd.read_csv(f'{data_dir}\/train.csv')\n    test = pd.read_csv(f'{data_dir}\/test.csv')\n    # Create a key to merge with book and trade data\n    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    print(f'Our training set has {train.shape[0]} rows')\n    return train, test\n\n# Function to preprocess book data (for each stock id)\ndef book_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    df.iloc[:, 2:] = df.iloc[:, 2:].astype('float64')\n    # Calculate Wap\n    df['wap1'] = calc_wap1(df)\n    df['wap2'] = calc_wap2(df)\n    df['wap3'] = calc_wap3(df)\n    df['wap4'] = calc_wap4(df)\n    # Calculate log returns    \n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n    df['log_return3'] = df.groupby(['time_id'])['wap3'].apply(log_return)\n    df['log_return4'] = df.groupby(['time_id'])['wap4'].apply(log_return)\n    # Calculate wap balance\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    # Calculate spread\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1']) \/ 2)\n    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) \/ ((df['ask_price2'] + df['bid_price2']) \/ 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'wap1': [fast_sum, fast_std],\n        'wap2': [fast_sum, fast_std],\n        'wap3': [fast_sum, fast_std],\n        'wap4': [fast_sum, fast_std],\n        'log_return1': [fast_rv],\n        'log_return2': [fast_rv],\n        'log_return3': [fast_rv],\n        'log_return4': [fast_rv],\n        'wap_balance': [fast_sum, fast_max],\n        'price_spread':[fast_sum, fast_max],\n        'price_spread2':[fast_sum, fast_max],\n        'bid_spread':[fast_sum, fast_max],\n        'ask_spread':[fast_sum, fast_max],\n        'total_volume':[fast_sum, fast_max],\n        'volume_imbalance':[fast_sum, fast_max],\n        \"bid_ask_spread\":[fast_sum, fast_max],\n    }\n    create_feature_dict_time = {\n        'log_return1': [fast_rv],\n        'log_return2': [fast_rv],\n        'log_return3': [fast_rv],\n        'log_return4': [fast_rv],\n    }\n    \n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n\n    # Merge all\n    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id__100'], axis = 1, inplace = True)\n    \n    \n    # Create row_id so we can merge\n    stock_id = file_path.split('=')[1].split('\/')[0]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n    return df_feature\n\n# Function to preprocess trade data (for each stock id)\ndef trade_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    df.iloc[:, 2:] = df.iloc[:, 2:].astype('float64')\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    df['amount']=df['price']*df['size']\n    # Dict for aggregations\n    create_feature_dict = {\n        'log_return':[fast_rv],\n        'seconds_in_bucket':[fast_count],\n        'size':[fast_sum, fast_max, fast_min],\n        'order_count':[fast_sum,fast_max],\n        'amount':[fast_sum,fast_max,fast_min],\n    }\n    create_feature_dict_time = {\n        'log_return':[fast_rv],\n        'seconds_in_bucket':[fast_count],\n        'size':[fast_sum],\n        'order_count':[fast_sum],\n    }\n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n\n    # Get the stats for different windows\n    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n    \n    def tendency(price, vol):    \n        df_diff = np.diff(price)\n        val = (df_diff\/price[1:])*100\n        power = np.sum(val*vol[1:])\n        return(power)\n    \n    lis = []\n    for n_time_id in df['time_id'].unique():\n        df_id = df[df['time_id'] == n_time_id]        \n        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n        # new\n        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n        energy = np.mean(df_id['price'].values**2)\n        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n        \n        # vol vars\n        \n        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n        energy_v = np.sum(df_id['size'].values**2)\n        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n        \n        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n    \n    df_lr = pd.DataFrame(lis)\n        \n   \n    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n    \n    # Merge all\n    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id','time_id__100'], axis = 1, inplace = True)\n    \n    \n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1].split('\/')[0]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n    return df_feature\n\n# Function to get group stats for the stock_id and time_id\ndef get_time_stock(df):\n    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n\n    # Group by the stock id\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n    # Group by the stock id\n    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n    \n    # Merge with original dataframe\n    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n    return df\n    \n\n# Funtion to make preprocessing function in parallel (for each stock id)\ndef preprocessor(list_stock_ids, is_train = True):\n    \n    # Parrallel for loop\n    def for_joblib(stock_id):\n        # Train\n        if is_train:\n            file_path_book = glob.glob(f'..\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/stock_id={stock_id}\/*')[0]\n            file_path_trade = glob.glob(f'..\/input\/optiver-realized-volatility-prediction\/trade_train.parquet\/stock_id={stock_id}\/*')[0]\n        # Test\n        else:\n            file_path_book = glob.glob(f'..\/input\/optiver-realized-volatility-prediction\/book_test.parquet\/stock_id={stock_id}\/*')[0]\n            file_path_trade = glob.glob(f'..\/input\/optiver-realized-volatility-prediction\/trade_test.parquet\/stock_id={stock_id}\/*')[0]\n    \n        # Preprocess book and trade data and merge them\n        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n        \n        # Return the merge dataframe\n        return df_tmp\n    \n    # Use parallel api to call paralle for loop\n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in tqdm(list_stock_ids))\n    # Concatenate all the dataframes that return from Parallel\n    df = pd.concat(df, ignore_index = True)\n    return df","59a85e04":"# Read train and test\ntrain, test = read_train_test()\n\n# Get unique stock ids \n# train_stock_ids = train['stock_id'].unique()\n# Preprocess them using Parallel and our single stock id functions\n# train_ = preprocessor(train_stock_ids, is_train = True)\n# train = train.merge(train_, on = ['row_id'], how = 'left')\n\n# Get unique stock ids \ntest_stock_ids = test['stock_id'].unique()\n# Preprocess them using Parallel and our single stock id functions\ntest_ = preprocessor(test_stock_ids, is_train = False)\ntest = test.merge(test_, on = ['row_id'], how = 'left')\n\n# # Get group stats of time_id and stock_id\n# train = get_time_stock(train)\ntest = get_time_stock(test)","2edb75d8":"# replace by order sum (tau)\n# train['size_tau'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_count_unique'] )\ntest['size_tau'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_count_unique'] )\n# train['size_tau_400'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_count_unique_400'] )\ntest['size_tau_400'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_count_unique_400'] )\n# train['size_tau_300'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_count_unique_300'] )\ntest['size_tau_300'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_count_unique_300'] )\n# train['size_tau_200'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_count_unique_200'] )\ntest['size_tau_200'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_count_unique_200'] )","1ae4667a":"# train['size_tau2'] = np.sqrt( 1\/ train['trade_order_count_sum'] )\ntest['size_tau2'] = np.sqrt( 1\/ test['trade_order_count_sum'] )\n# train['size_tau2_400'] = np.sqrt( 0.33\/ train['trade_order_count_sum'] )\ntest['size_tau2_400'] = np.sqrt( 0.33\/ test['trade_order_count_sum'] )\n# train['size_tau2_300'] = np.sqrt( 0.5\/ train['trade_order_count_sum'] )\ntest['size_tau2_300'] = np.sqrt( 0.5\/ test['trade_order_count_sum'] )\n# train['size_tau2_200'] = np.sqrt( 0.66\/ train['trade_order_count_sum'] )\ntest['size_tau2_200'] = np.sqrt( 0.66\/ test['trade_order_count_sum'] )\n\n# delta tau\n# train['size_tau2_d'] = train['size_tau2_400'] - train['size_tau2']\ntest['size_tau2_d'] = test['size_tau2_400'] - test['size_tau2']","671ac472":"train = dt.fread('..\/input\/optiver-501-train\/public_train_601.csv').to_pandas()\ntrain.head()","cf007459":"test.head()","e7ae17c2":"colNames = [col for col in list(train.columns)\n            if col not in {\"stock_id\", \"time_id\", \"target\", \"row_id\"}]\nlen(colNames)","24204d53":"from sklearn.cluster import KMeans\n# making agg features\n\ntrain_p = pd.read_csv(f'{data_dir}\/train.csv')\ntrain_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n\ncorr = train_p.corr()\n\nids = corr.index\n\nkmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\nprint(kmeans.labels_)\n\nl = []\nfor n in range(7):\n    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n\n\nmat = []\nmatTest = []\n\nn = 0\nfor ind in l:\n    print(ind)\n    newDf = train.loc[train['stock_id'].isin(ind) ]\n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    newDf.loc[:,'stock_id'] = str(n)+'c1'\n    mat.append ( newDf )\n    \n    newDf = test.loc[test['stock_id'].isin(ind) ]    \n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    newDf.loc[:,'stock_id'] = str(n)+'c1'\n    matTest.append ( newDf )\n    \n    n+=1\n    \nmat1 = pd.concat(mat).reset_index()\nmat1.drop(columns=['target'],inplace=True)\n\nmat2 = pd.concat(matTest).reset_index()","79b6c43b":"mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\nmat1 = mat1.pivot(index='time_id', columns='stock_id')\nmat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\nmat1.reset_index(inplace=True)\n\nmat2 = mat2.pivot(index='time_id', columns='stock_id')\nmat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\nmat2.reset_index(inplace=True)","2f33c183":"nnn = ['time_id',\n     'log_return1_realized_volatility_0c1',\n     'log_return1_realized_volatility_1c1',     \n     'log_return1_realized_volatility_3c1',\n     'log_return1_realized_volatility_4c1',     \n     'log_return1_realized_volatility_6c1',\n     'total_volume_sum_0c1',\n     'total_volume_sum_1c1', \n     'total_volume_sum_3c1',\n     'total_volume_sum_4c1', \n     'total_volume_sum_6c1',\n     'trade_size_sum_0c1',\n     'trade_size_sum_1c1', \n     'trade_size_sum_3c1',\n     'trade_size_sum_4c1', \n     'trade_size_sum_6c1',\n     'trade_order_count_sum_0c1',\n     'trade_order_count_sum_1c1',\n     'trade_order_count_sum_3c1',\n     'trade_order_count_sum_4c1',\n     'trade_order_count_sum_6c1',      \n     'price_spread_sum_0c1',\n     'price_spread_sum_1c1',\n     'price_spread_sum_3c1',\n     'price_spread_sum_4c1',\n     'price_spread_sum_6c1',   \n     'bid_spread_sum_0c1',\n     'bid_spread_sum_1c1',\n     'bid_spread_sum_3c1',\n     'bid_spread_sum_4c1',\n     'bid_spread_sum_6c1',       \n     'ask_spread_sum_0c1',\n     'ask_spread_sum_1c1',\n     'ask_spread_sum_3c1',\n     'ask_spread_sum_4c1',\n     'ask_spread_sum_6c1',   \n     'volume_imbalance_sum_0c1',\n     'volume_imbalance_sum_1c1',\n     'volume_imbalance_sum_3c1',\n     'volume_imbalance_sum_4c1',\n     'volume_imbalance_sum_6c1',       \n     'bid_ask_spread_sum_0c1',\n     'bid_ask_spread_sum_1c1',\n     'bid_ask_spread_sum_3c1',\n     'bid_ask_spread_sum_4c1',\n     'bid_ask_spread_sum_6c1',\n     'size_tau2_0c1',\n     'size_tau2_1c1',\n     'size_tau2_3c1',\n     'size_tau2_4c1',\n     'size_tau2_6c1'] \n\ntrain = pd.merge(train,mat1[nnn],how='left',on='time_id')\ntest = pd.merge(test,mat2[nnn],how='left',on='time_id')","cb8b70ca":"if DEBUG:\n    dt.Frame(train).to_csv(f'test_601_LGB.csv')    \nelse:\n    dt.Frame(test).to_csv(f'test_601_LGB.csv')","05ddb2d9":"train.replace([np.inf, -np.inf], np.nan,inplace=True)\ntest.replace([np.inf, -np.inf], np.nan,inplace=True)\nqt_train = load_pickle(f'{preprocessor_path}\/qt_train.pkl')\ntrain_nn=train[colNames].copy()\ntest_nn=test[colNames].copy()\nfor i, col in enumerate(colNames):\n    #print(col)\n    qt = qt_train[i]\n    train_nn[col] = qt.transform(train_nn[[col]])\n    test_nn[col] = qt.transform(test_nn[[col]])","20ccf15d":"train_nn[['stock_id','time_id','target']]=train[['stock_id','time_id','target']]\ntest_nn[['stock_id','time_id']]=test[['stock_id','time_id']]","58525837":"# Kmean: making agg features\nfrom sklearn.cluster import KMeans\ntrain_p = pd.read_csv(f'{data_dir}\/train.csv')\ntrain_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n\ncorr = train_p.corr()\n\nids = corr.index\n\nkmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\nprint(kmeans.labels_)\n\nl = []\nfor n in range(7):\n    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n    \n\nmat = []\nmatTest = []\n\nn = 0\nfor ind in l:\n    print(ind)\n    newDf = train_nn.loc[train_nn['stock_id'].isin(ind) ]\n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    newDf.loc[:,'stock_id'] = str(n)+'c1'\n    mat.append ( newDf )\n    \n    newDf = test_nn.loc[test_nn['stock_id'].isin(ind) ]    \n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    newDf.loc[:,'stock_id'] = str(n)+'c1'\n    matTest.append ( newDf )\n    \n    n+=1\n    \nmat1 = pd.concat(mat).reset_index()\nmat1.drop(columns=['target'],inplace=True)\n\nmat2 = pd.concat(matTest).reset_index()\nmat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])","4d39a588":"nnn = ['time_id',\n     'log_return1_realized_volatility_0c1',\n     'log_return1_realized_volatility_1c1',     \n     'log_return1_realized_volatility_3c1',\n     'log_return1_realized_volatility_4c1',     \n     'log_return1_realized_volatility_6c1',\n     'total_volume_sum_0c1',\n     'total_volume_sum_1c1', \n     'total_volume_sum_3c1',\n     'total_volume_sum_4c1', \n     'total_volume_sum_6c1',\n     'trade_size_sum_0c1',\n     'trade_size_sum_1c1', \n     'trade_size_sum_3c1',\n     'trade_size_sum_4c1', \n     'trade_size_sum_6c1',\n     'trade_order_count_sum_0c1',\n     'trade_order_count_sum_1c1',\n     'trade_order_count_sum_3c1',\n     'trade_order_count_sum_4c1',\n     'trade_order_count_sum_6c1',      \n     'price_spread_sum_0c1',\n     'price_spread_sum_1c1',\n     'price_spread_sum_3c1',\n     'price_spread_sum_4c1',\n     'price_spread_sum_6c1',   \n     'bid_spread_sum_0c1',\n     'bid_spread_sum_1c1',\n     'bid_spread_sum_3c1',\n     'bid_spread_sum_4c1',\n     'bid_spread_sum_6c1',       \n     'ask_spread_sum_0c1',\n     'ask_spread_sum_1c1',\n     'ask_spread_sum_3c1',\n     'ask_spread_sum_4c1',\n     'ask_spread_sum_6c1',   \n     'volume_imbalance_sum_0c1',\n     'volume_imbalance_sum_1c1',\n     'volume_imbalance_sum_3c1',\n     'volume_imbalance_sum_4c1',\n     'volume_imbalance_sum_6c1',       \n     'bid_ask_spread_sum_0c1',\n     'bid_ask_spread_sum_1c1',\n     'bid_ask_spread_sum_3c1',\n     'bid_ask_spread_sum_4c1',\n     'bid_ask_spread_sum_6c1',\n     'size_tau2_0c1',\n     'size_tau2_1c1',\n     'size_tau2_3c1',\n     'size_tau2_4c1',\n     'size_tau2_6c1'] ","91c8d20f":"mat1 = mat1.pivot(index='time_id', columns='stock_id')\nmat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\nmat1.reset_index(inplace=True)\n\nmat2 = mat2.pivot(index='time_id', columns='stock_id')\nmat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\nmat2.reset_index(inplace=True)\n\ntrain_nn = pd.merge(train_nn,mat1[nnn],how='left',on='time_id')\ntest_nn = pd.merge(test_nn,mat2[nnn],how='left',on='time_id')","f0c9005c":"# fill mean\nfeatures_to_consider = list(train_nn)\n\nfeatures_to_consider.remove('time_id')\nfeatures_to_consider.remove('target')\ntry:\n    features_to_consider.remove('pred_NN')\nexcept:\n    pass\n\ntrain_nn[features_to_consider] = train_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\ntest_nn[features_to_consider] = test_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\n\nif DEBUG:\n    dt.Frame(train_nn).to_csv(f'test_601_NN.csv')    \nelse:\n    dt.Frame(test_nn).to_csv(f'test_601_NN.csv')","c4b7cfd6":"del mat1, mat2, train, test, train_nn, test_nn\n# del train,test\nx = gc.collect()","63a36219":"N_SEED = 5\nN_FOLD = 10\n\ndf_test = dt.fread('test_601_LGB.csv').to_pandas()\nfea_cols = ['stock_id'] + [f for f in df_test if f not in ['time_id', 'target', 'stock_id', 'row_id']]\nX_test = df_test[fea_cols].values\nmodel_folder = '..\/input\/optiver-final-601-lgb'\nfor i_seed in range(N_SEED):\n    for i_fold in range(N_FOLD):\n        model_path = f'{model_folder}\/LGB_{i_seed}_{i_fold}.pkl'\n        model = load_pickle(model_path)\n        df_test[f'pred_{i_seed}_{i_fold}'] = model.predict(X_test)\ndf_test['pred_601_lgb'] = df_test[[f'pred_{i_seed}_{i_fold}' for i_seed in range(N_SEED) for i_fold in range(N_FOLD)]].mean(axis=1)\ndf_result = df_result.merge(df_test[['stock_id', 'time_id', 'pred_601_lgb']], on=['stock_id', 'time_id'])\ndf_result.head()","410c7819":"N_SEED = 5\nN_FOLD = 10\n\ndf_test = dt.fread('test_601_LGB.csv').to_pandas()\nfea_cols = ['stock_id'] + [f for f in df_test if f not in ['time_id', 'target', 'stock_id', 'row_id']]\nX_test = df_test[fea_cols]\nmodel_folder = '..\/input\/optiver-final-601-cat'\nfor i_seed in range(N_SEED):\n    for i_fold in range(N_FOLD):\n        model_path = f'{model_folder}\/CAT_{i_seed}_{i_fold}.pkl'\n        model = load_pickle(model_path)\n        df_test[f'pred_{i_seed}_{i_fold}'] = model.predict(X_test)\ndf_test['pred_601_cat'] = df_test[[f'pred_{i_seed}_{i_fold}' for i_seed in range(N_SEED) for i_fold in range(N_FOLD)]].mean(axis=1)\ndf_result = df_result.merge(df_test[['stock_id', 'time_id', 'pred_601_cat']], on=['stock_id', 'time_id'])\ndf_result.head()","36c7a56e":"N_FOLD = 5\n\ndf_test = dt.fread('test_601_LGB.csv').to_pandas()\nfea_cols = ['stock_id'] + [f for f in df_test if f not in ['time_id', 'target', 'stock_id', 'row_id']]\nX_test = df_test[fea_cols].values\nmodel_folder = '..\/input\/optiver-601-cfr'\nfor i_fold in tqdm(range(N_FOLD)):\n    model_path = f'{model_folder}\/cfr_601_{i_fold}.pkl'\n    model = load_pickle(model_path)\n    df_test[f'pred_{i_fold}'] = model.predict(X_test)\ndf_test['pred_601_cfr'] = df_test[[f'pred_{i_fold}' for i_fold in range(N_FOLD)]].mean(axis=1)\ndf_result = df_result.merge(df_test[['stock_id', 'time_id', 'pred_601_cfr']], on=['stock_id', 'time_id'])\ndf_result.head()","0375c496":"N_SEED = 5\nN_FOLD = 10\n\ndf_test = dt.fread('test_601_NN.csv').to_pandas()\nfea_cols = [f for f in df_test if f not in ['time_id', 'target', 'pred_NN', 'stock_id', 'row_id']]\nS_test = df_test['stock_id'].values\nX_test = df_test[fea_cols].values\nmodel_folder = '..\/input\/optiver-final-601-mlp'\nfor i_seed in range(N_SEED):\n    for i_fold in range(N_FOLD):\n        scaler = load_pickle(f'{model_folder}\/minmax_scaler_{i_seed}_{i_fold}.pkl')\n        X_test_scaled = scaler.transform(X_test)\n        model_path = f'{model_folder}\/model_{i_seed}_{i_fold}.hdf5'\n        model = tf.keras.models.load_model(model_path, custom_objects={'mspe_loss': mspe_loss})\n        df_test[f'pred_{i_seed}_{i_fold}'] = model.predict([S_test, X_test_scaled], batch_size=1024)\n\ndf_test['pred_601_mlp'] = df_test[[f'pred_{i_seed}_{i_fold}' for i_seed in range(N_SEED) for i_fold in range(N_FOLD)]].mean(axis=1)\ndf_test['pred_601_mlp'] = inverse_target(df_test['pred_601_mlp'])\ndf_result = df_result.merge(df_test[['stock_id', 'time_id', 'pred_601_mlp']], on=['stock_id', 'time_id'])\ndf_result.head()","dc4d5d82":"N_SEED = 5\nN_FOLD = 10\n\ndf_test = dt.fread('test_601_NN.csv').to_pandas()\nfea_cols = [f for f in df_test if f not in ['time_id', 'target', 'pred_NN', 'stock_id', 'row_id']]\nS_test = df_test['stock_id'].values\nX_test = df_test[fea_cols].values\nmodel_folder = '..\/input\/optiver-final-601-1dcnn'\nfor i_seed in range(N_SEED):\n    for i_fold in range(N_FOLD):\n        scaler = load_pickle(f'{model_folder}\/minmax_scaler_{i_seed}_{i_fold}.pkl')\n        X_test_scaled = scaler.transform(X_test)\n        model_path = f'{model_folder}\/model_{i_seed}_{i_fold}.hdf5'\n        model = tf.keras.models.load_model(model_path, custom_objects={'mspe_loss': mspe_loss})\n        df_test[f'pred_{i_seed}_{i_fold}'] = model.predict([S_test, X_test_scaled], batch_size=1024)\n\ndf_test['pred_601_1dcnn'] = df_test[[f'pred_{i_seed}_{i_fold}' for i_seed in range(N_SEED) for i_fold in range(N_FOLD)]].mean(axis=1)\ndf_test['pred_601_1dcnn'] = inverse_target(df_test['pred_601_1dcnn'])\ndf_result = df_result.merge(df_test[['stock_id', 'time_id', 'pred_601_1dcnn']], on=['stock_id', 'time_id'])\ndf_result.head()","c867885a":"N_SEED = 3\nN_FOLD = 5\n\ndf_test = dt.fread('test_601_NN.csv').to_pandas()\ndf_test['stock_id'] = df_test['stock_id'].astype('int64')\nfea_cols = [f for f in df_test if f not in ['time_id', 'target', 'pred_NN', 'stock_id', 'row_id']]\nS_test = df_test['stock_id'].values\nX_test = df_test[['stock_id']+fea_cols].values\nmodel_folder = '..\/input\/optiver-final-601-tabnet'\nfor i_seed in range(N_SEED):\n    for i_fold in range(N_FOLD):\n        scaler = load_pickle(f'{model_folder}\/minmax_scaler_{i_seed}_{i_fold}.pkl')\n        X_test_scaled = X_test.copy()\n        X_test_scaled[:, 1:] = scaler.transform(X_test[:, 1:])\n        model_path = f'{model_folder}\/model_{i_seed}_{i_fold}.xyz'\n        model = TabNetRegressor()\n        model.load_model(model_path)\n        df_test[f'pred_{i_seed}_{i_fold}'] = model.predict(X_test_scaled)\n\ndf_test['pred_601_tabnet'] = df_test[[f'pred_{i_seed}_{i_fold}' for i_seed in range(N_SEED) for i_fold in range(N_FOLD)]].mean(axis=1)\ndf_test['pred_601_tabnet'] = inverse_target(df_test['pred_601_tabnet'])\ndf_result = df_result.merge(df_test[['stock_id', 'time_id', 'pred_601_tabnet']], on=['stock_id', 'time_id'])\ndf_result.head()","eb7a2ad2":"del df_test, X_test\nx = gc.collect()","0d7b4089":"def read_train_test():\n    # Function to read our base train and test set\n    \n    train = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\n    test = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\n\n    # Create a key to merge with book and trade data\n    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    print(f'Our training set has {train.shape[0]} rows')\n    print(f'Our test set has {test.shape[0]} rows')\n    print(f'Our training set has {train.isna().sum().sum()} missing values')\n    print(f'Our test set has {test.isna().sum().sum()} missing values')\n    \n    return train, test","7187e317":"train, test = read_train_test()","3e0b5726":"# data directory\ndata_dir = '..\/input\/optiver-realized-volatility-prediction\/'\n\ndef calc_wap1(df):\n    # Function to calculate first WAP\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef calc_wap2(df):\n    # Function to calculate second WAP\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) \/ (df['bid_size2'] + df['ask_size2'])\n    return wap\n\ndef log_return(series):\n    # Function to calculate the log of the return\n    return np.log(series).diff()\n\ndef realized_volatility(series):\n    # Calculate the realized volatility\n    return np.sqrt(np.sum(series**2))\n\ndef count_unique(series):\n    # Function to count unique elements of a series\n    return len(np.unique(series))\n\ndef book_preprocessor(file_path):\n    # Function to preprocess book data (for each stock id)\n    \n    df = pd.read_parquet(file_path)\n    \n    # Calculate Wap\n    df['wap1'] = calc_wap1(df)\n    df['wap2'] = calc_wap2(df)\n    \n    # Calculate log returns\n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n    \n    # Calculate wap balance\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    \n    # Calculate spread\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1']) \/ 2)\n    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) \/ ((df['ask_price2'] + df['bid_price2']) \/ 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'wap1': [np.sum, np.mean, np.std],\n        'wap2': [np.sum, np.mean, np.std],\n        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n        'wap_balance': [np.sum, np.mean, np.std],\n        'price_spread':[np.sum, np.mean, np.std],\n        'price_spread2':[np.sum, np.mean, np.std],\n        'bid_spread':[np.sum, np.mean, np.std],\n        'ask_spread':[np.sum, np.mean, np.std],\n        'total_volume':[np.sum, np.mean, np.std],\n        'volume_imbalance':[np.sum, np.mean, np.std],\n        \"bid_ask_spread\":[np.sum, np.mean, np.std],\n    }\n    \n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Function to get group stats for different windows (seconds in bucket)\n        \n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        \n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        \n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n    \n    # Merge all\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__400', 'time_id__300', 'time_id__200'], axis = 1, inplace = True)\n    \n    \n    # Create row_id so we can merge\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n    \n    return df_feature\n\n\ndef trade_preprocessor(file_path):\n    # Function to preprocess trade data (for each stock id)\n    \n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum, realized_volatility, np.mean, np.std, np.max, np.min],\n        'order_count':[np.mean,np.sum,np.max],\n    }\n    \n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Function to get group stats for different windows (seconds in bucket)\n        \n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        \n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        \n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n    \n    def tendency(price, vol):    \n        df_diff = np.diff(price)\n        val = (df_diff\/price[1:])*100\n        power = np.sum(val*vol[1:])\n        return(power)\n    \n    lis = []\n    for n_time_id in df['time_id'].unique():\n        df_id = df[df['time_id'] == n_time_id]        \n        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n        energy = np.mean(df_id['price'].values**2)\n        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n        energy_v = np.sum(df_id['size'].values**2)\n        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n        \n        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n    \n    df_lr = pd.DataFrame(lis)\n        \n   \n    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n    \n    # Merge all\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__400', 'time_id__300', 'time_id__200','time_id'], axis = 1, inplace = True)\n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n    \n    def order_sum(df, sec:str):\n        new_col = 'size_tau' + sec\n        bucket_col = 'trade_seconds_in_bucket_count_unique' + sec\n        df[new_col] = np.sqrt(1\/df[bucket_col])\n        \n        new_col2 = 'size_tau2' + sec\n        order_col = 'trade_order_count_sum' + sec\n        df[new_col2] = np.sqrt(1\/df[order_col])\n        \n        if sec == '400_':\n            df['size_tau2_d'] = df['size_tau2_400'] - df['size_tau2']\n        \n\n    \n    for sec in ['','_200','_300','_400']:\n        order_sum(df_feature, sec)\n        \n    df_feature['size_tau2_d'] = df_feature['size_tau2_400'] - df_feature['size_tau2']\n    \n    return df_feature\n\n\ndef get_time_stock(df):\n    # Function to get group stats for the stock_id and time_id\n    \n    # Get realized volatility columns\n    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n\n    # Group by the stock id\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    \n    # Rename columns joining suffix\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n    # Group by the stock id\n    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    \n    # Rename columns joining suffix\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n    \n    # Merge with original dataframe\n    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n    \n    return df\n\ndef create_agg_features(train, test):\n\n    # Making agg features\n\n    train_p = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\n    train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n    corr = train_p.corr()\n    ids = corr.index\n    kmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n    l = []\n    for n in range(7):\n        l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n\n    mat = []\n    matTest = []\n    n = 0\n    for ind in l:\n        newDf = train.loc[train['stock_id'].isin(ind) ]\n        newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n        newDf.loc[:,'stock_id'] = str(n)+'c1'\n        mat.append ( newDf )\n        newDf = test.loc[test['stock_id'].isin(ind) ]    \n        newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n        newDf.loc[:,'stock_id'] = str(n)+'c1'\n        matTest.append ( newDf )\n        n+=1\n\n    mat1 = pd.concat(mat).reset_index()\n    mat1.drop(columns=['target'],inplace=True)\n    mat2 = pd.concat(matTest).reset_index()\n    \n    mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\n    \n    mat1 = mat1.pivot(index='time_id', columns='stock_id')\n    mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n    mat1.reset_index(inplace=True)\n    \n    mat2 = mat2.pivot(index='time_id', columns='stock_id')\n    mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n    mat2.reset_index(inplace=True)\n    \n    prefix = ['log_return1_realized_volatility', 'total_volume_mean', 'trade_size_mean', 'trade_order_count_mean','price_spread_mean','bid_spread_mean','ask_spread_mean',\n              'volume_imbalance_mean', 'bid_ask_spread_mean','size_tau2']\n    selected_cols=mat1.filter(regex='|'.join(f'^{x}.(0|1|3|4|6)c1' for x in prefix)).columns.tolist()\n    selected_cols.append('time_id')\n    \n    train_m = pd.merge(train,mat1[selected_cols],how='left',on='time_id')\n    test_m = pd.merge(test,mat2[selected_cols],how='left',on='time_id')\n    \n    # filling missing values with train means\n\n    features = [col for col in train_m.columns.tolist() if col not in ['time_id','target','row_id']]\n    train_m[features] = train_m[features].fillna(train_m[features].mean())\n    test_m[features] = test_m[features].fillna(train_m[features].mean())\n\n    return train_m, test_m\n    \n    \ndef preprocessor(list_stock_ids, is_train = True):\n    # Funtion to make preprocessing function in parallel (for each stock id)\n    \n    # Parrallel for loop\n    def for_joblib(stock_id):\n        # Train\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet\/stock_id=\" + str(stock_id)\n        # Test\n        else:\n            file_path_book = data_dir + \"book_test.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet\/stock_id=\" + str(stock_id)\n    \n        # Preprocess book and trade data and merge them\n        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n        \n        # Return the merge dataframe\n        return df_tmp\n    \n    # Use parallel api to call paralle for loop\n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n    \n    # Concatenate all the dataframes that return from Parallel\n    df = pd.concat(df, ignore_index = True)\n    \n    return df","2fe0d69b":"# Get unique stock ids \ntrain_stock_ids = train['stock_id'].unique()\n\n# Preprocess them using Parallel and our single stock id functions\n# train_ = preprocessor(train_stock_ids, is_train = True)\ntrain_ = pd.read_csv('..\/input\/optiver-public-kfold-train\/kfold_train_.csv')\ntrain = train.merge(train_, on = ['row_id'], how = 'left')\n\n# Get unique stock ids \ntest_stock_ids = test['stock_id'].unique()\n\n# Preprocess them using Parallel and our single stock id functions\ntest_ = preprocessor(test_stock_ids, is_train = False)\ntest = test.merge(test_, on = ['row_id'], how = 'left')\n\n# Get group stats of time_id and stock_id\ntrain = get_time_stock(train)\ntest = get_time_stock(test)\n\n# Fill inf values\ntrain.replace([np.inf, -np.inf], np.nan,inplace=True)\ntest.replace([np.inf, -np.inf], np.nan,inplace=True)\n\n# Aggregating some features\ntrain, test = create_agg_features(train,test)\ntest = test.loc[test['row_id'].isin(df_result['row_id'])]","57c949be":"X = train.drop(['row_id', 'target', 'time_id'], axis = 1)\ny = train['target']\nX_test=test.copy()\nX_test.drop(['time_id','row_id'], axis=1,inplace=True)","2dea806a":"nunique = X.nunique()\ntypes = X.dtypes\n\ncategorical_columns = []\ncategorical_dims =  {}\n\nscalers = dict()\nfor col in X.columns:\n    if col == 'stock_id':\n        l_enc = LabelEncoder()\n        X[col] = l_enc.fit_transform(X[col].values)\n        X_test[col] = l_enc.transform(X_test[col].values)\n        categorical_columns.append(col)\n        categorical_dims[col] = len(l_enc.classes_)\n        dump(l_enc, 'l_enc')\n    else:\n        scaler = StandardScaler()\n        X[col] = scaler.fit_transform(X[col].values.reshape(-1, 1))\n        X_test[col] = scaler.transform(X_test[col].values.reshape(-1, 1))\n        scalers[col] = scaler\n\ndump(scalers, 'scalers')\ncat_idxs = [ i for i, f in enumerate(X.columns.tolist()) if f in categorical_columns]\ncat_dims = [ categorical_dims[f] for i, f in enumerate(X.columns.tolist()) if f in categorical_columns]","5296482b":"N_FOLD = 5\n\ntest_predictions = np.zeros(X_test.shape[0])\nmodel_folder = '..\/input\/optiver-public-kfold-tabnet'\nfor i_fold in range(N_FOLD):\n    model_path = f'{model_folder}\/fold{i_fold}.xyz'\n    model = TabNetRegressor()\n    model.load_model(model_path)\n    test_predictions += model.predict(X_test.values).flatten() \/ N_FOLD\n\ndf_result['pred_kfold_tabnet'] = test_predictions\ndf_result.head()","a67021df":"pred_cols = ['pred_501_mlp', 'pred_501_wavenet', 'pred_501_1dcnn', 'pred_501_tabnet', 'pred_501_unet']\ncoef_ = [ 0.4, 0, 0.4, 0.1, 0.1 ]\ndf_result[pred_cols] = transform_target(df_result[pred_cols])\ndf_result['fpred_501'] = np.sum(coef_ * df_result[pred_cols].values, axis=1)\ndf_result['fpred_501'] = inverse_target(df_result['fpred_501'])\n\npred_cols = ['pred_601_tabnet', 'pred_601_mlp', 'pred_601_lgb', 'pred_601_cat', 'pred_601_1dcnn', 'pred_601_cfr']\ncoef_ = [ 0.2, 0.4, 0.15, 0, 0.25, 0 ]\ndf_result[pred_cols] = transform_target(df_result[pred_cols])\ndf_result['fpred_601'] = np.sum(coef_ * df_result[pred_cols].values, axis=1)\ndf_result['fpred_601'] = inverse_target(df_result['fpred_601'])\n\npred_cols = ['fpred_501', 'fpred_601', 'pred_kfold_tabnet']\ncoef_ = [ 0.6, 0.3, 0.1 ]\ndf_result[pred_cols] = transform_target(df_result[pred_cols])\ndf_result['target'] = np.sum(coef_ * df_result[pred_cols].values, axis=1)\ndf_result['target'] = inverse_target(df_result['target'])\n\ndf_result.head()","131ce220":"# Make submission\ndf_submission = df_result[['row_id', 'target']]\ndf_submission.to_csv('submission.csv', index=False)\nprint(df_submission.shape)\ndf_submission.head()","7a003795":"## Prediction - LGB","f364a36e":"## Feature Generation","24f1e4c5":"## Models:\n### - 501: MLP \/ 1dCNN \/ TabNet \/ UNet \/ WaveNet\n### - 601: LGB \/ Catboost \/ DeepForest(CFR) \/ MLP \/ 1dCNN \/ TabNet","e200f25b":"## Preprocessing - NN","611b2fb0":"## Prediction - 601 - LGB","36ea84fd":"## Prediction - 501 - MLP","c8957489":"## Prediction - 601 - Catboost","0deb6fae":"## Kmean for LGB","754c8303":"## Prediction - 501 - UNet","6d034fbf":"## Prediction - 601 - MLP","8f028835":"## Prediction - KFold - TabNet","bfbc3f79":"## Preprocessing - LGB","66d5dbdf":"## Prediction - 501 - WaveNet","302f6eaf":"## Prediction - 601 - 1dCNN","78d84682":"# 601","8ae247a5":"# Functions","77c55c3f":"# 501","a12b6339":"# Kmean + Quantile transformation for NN","3f433c99":"## Prediction - 601 - TabNet","99111dc3":"## Feature Generation","2ad3e82f":"# Public - KFold","6c069299":"## Prediction - 501 - TabNet","7c0d6b3f":"# Make submission","ccabd2dd":"## Prediction - 501 - 1dCNN","11edf2c3":"## Prediction - 601 - DeepForest (CFR)"}}