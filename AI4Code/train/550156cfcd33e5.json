{"cell_type":{"7572a577":"code","693a9e6f":"code","3cb909e8":"code","278059d3":"code","6dcb8fbc":"code","8e87fcb6":"code","d38da2c9":"code","46d6fcd3":"code","9b31a401":"code","5c63bf5c":"code","210e5d87":"code","687faf8f":"code","4136f789":"code","165604e6":"code","fcfe69ce":"code","e398ba2f":"code","7b924665":"code","542a9470":"code","9472f2a4":"code","84af1e6a":"code","f0b9d026":"code","74be7d99":"code","017b77cb":"code","4390faf7":"code","96538da7":"code","e3ee7947":"code","67874dad":"code","df12bbe0":"code","b6a30d9b":"markdown","0273a106":"markdown","56326557":"markdown","b80a9433":"markdown","fb8ff874":"markdown","8db56b31":"markdown","b2764d59":"markdown","48325cef":"markdown","50bc3f8c":"markdown","d198a627":"markdown","7351d36f":"markdown","e0a4c04b":"markdown","98dbf10d":"markdown","296d97fd":"markdown","5769ba8d":"markdown","2ce93491":"markdown","e83d0d84":"markdown","5909253d":"markdown","e6ae4930":"markdown","7aef57a0":"markdown","ed7d06a4":"markdown","72670088":"markdown","20660e1f":"markdown","d6b22070":"markdown","d10594f0":"markdown","bea851b7":"markdown","c41b9afb":"markdown","e28a29c4":"markdown","caad285d":"markdown"},"source":{"7572a577":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfrom keras.utils.np_utils import to_categorical # Label encoding\n\nfrom sklearn.model_selection import train_test_split # Split Data\n\n# Modelling\nfrom tensorflow import keras\nfrom keras import layers\n\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n\nfrom keras import models\n\nfrom sklearn.metrics import confusion_matrix","693a9e6f":"X_train = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\nX_test = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\n\nprint('Shape of the training data: ', X_train.shape)\nprint('Shape of the test data: ', X_test.shape)","3cb909e8":"f = plt.figure(figsize=(10,6))\ncounts = X_train['label'].value_counts().sort_index()\nsns.barplot(counts.index, counts.values)\n\nfor i in counts.index:\n    plt.text(i,counts.values[i]+50,str(counts.values[i]),horizontalalignment='center',fontsize=14)\n\n\nplt.tick_params(labelsize = 14)\nplt.xticks(counts.index)\nplt.xlabel(\"Digits\",fontsize=16)\nplt.ylabel(\"Frequency\",fontsize=16)\nplt.title(\"Frequency Graph training set\",fontsize=20)\n\nplt.show()","278059d3":"y_train = X_train['label']\nX_train.drop(labels = ['label'], axis=1, inplace=True)","6dcb8fbc":"print('Null values in training data: ',X_train.isna().any().sum())\nprint('Null values in test data: ',X_test.isna().any().sum())\n\nX_train = X_train \/ 255.0\nX_test = X_test \/ 255.0","8e87fcb6":"X_train = X_train.values.reshape(-1, 28, 28, 1)\nX_test = X_test.values.reshape(-1, 28, 28, 1)","d38da2c9":"y_train = to_categorical(y_train, num_classes=10)","46d6fcd3":"X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.1)","9b31a401":"model = keras.models.Sequential([layers.Conv2D(32, (3,3), activation=\"relu\",padding='same', input_shape=(28,28,1)),\n                                 layers.MaxPooling2D(2,2),\n                                 layers.Dropout(0.25),\n                                  layers.Conv2D(64, (3,3), activation=\"relu\",padding='same'),\n                                 layers.MaxPooling2D(2,2),\n                                 layers.Dropout(0.25),\n                                 layers.Flatten(),\n                                  layers.BatchNormalization(),\n                                 layers.Dense(128, activation=\"elu\", kernel_initializer=\"he_normal\"),\n                                  layers.BatchNormalization(),\n                                 layers.Dense(32, activation=\"elu\", kernel_initializer=\"he_normal\"),\n                                  layers.BatchNormalization(),\n                                 layers.Dense(10, activation=\"softmax\")])\nmodel.summary()","5c63bf5c":"optimizer = Adam(learning_rate=0.001, epsilon=1e-07)\n\nmodel.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics=['accuracy'])\n\nearlyStopping = EarlyStopping(monitor='val_accuracy', patience=10, verbose=0, mode='auto')\nmcp = ModelCheckpoint('.mdl_wts.hdf5', save_best_only=True, monitor='val_accuracy', mode='auto')\nreduce_lr_loss = ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='auto')","210e5d87":"datagen = ImageDataGenerator(\n    rotation_range=5,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    shear_range=5,\n    zoom_range=0.1)\n\ndatagen.fit(X_train)","687faf8f":"Batch_size=100\nEpochs = 100\nhistory = model.fit_generator(datagen.flow(X_train, y_train, batch_size=Batch_size),\n                              epochs = Epochs, \n                              validation_data = (X_val,y_val),\n                              verbose = 2, \n                              steps_per_epoch=X_train.shape[0]\/\/Batch_size, \n                             callbacks = [earlyStopping, mcp, reduce_lr_loss])","4136f789":"f = plt.figure(figsize=(20,7))\n\nf.add_subplot(121)\n# plt.figure(figsize=(6, 4))\nplt.plot(history.history['accuracy'], color='b', label=\"Train\")\nplt.plot(history.history['val_accuracy'], color='r',label=\"Validation\")\nplt.legend(loc='best',fontsize=18)\nplt.title('Accuracy Curve',fontsize=25)\n\n\n\nf.add_subplot(122)\n# plt.figure(figsize=(6, 4))\nplt.plot(history.history['loss'], color='b', label=\"Train\")\nplt.plot(history.history['val_loss'], color='r', label=\"validation\")\nplt.legend(loc='best',fontsize=18)\nplt.title('Loss Curve',fontsize=25)\nplt.show()","165604e6":"plt.figure(figsize=(10,8.5))\ny_pred = model.predict(X_val)\ny_pred = np.argmax(y_pred, axis=1)\ny_val_test = np.argmax(y_val, axis=1)\ncm = confusion_matrix(y_val_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='g', cbar=False)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix for predicted and true labels')\nplt.show()","fcfe69ce":"model.load_weights(filepath = '.mdl_wts.hdf5')\n\nscores = model.evaluate(X_val, y_val, callbacks = [earlyStopping, mcp, reduce_lr_loss])","e398ba2f":"cnt_error = []\nfor (a, b) in zip(y_val_test, y_pred):\n    if a == b: continue\n    cnt_error.append( a )\n\n    \ncnt_error = np.unique(cnt_error, return_counts = True)\ncnt_error = pd.Series(cnt_error[1],index=cnt_error[0])\nf = plt.figure(figsize=(10,6))\nsns.barplot(cnt_error.index, cnt_error.values)\n\n# for i in counts.index:\n#     plt.text(i,cnt_error.values[i]+0.2,str(cnt_error.values[i]),horizontalalignment='center',fontsize=14)\n\n\nplt.tick_params(labelsize = 14)\nplt.xticks(cnt_error.index)\nplt.xlabel(\"Digits\",fontsize=16)\nplt.ylabel(\"Frequency\",fontsize=16)\nplt.title(\"Worng Predictions\",fontsize=20)\n\nplt.show()","7b924665":"rows = 3\ncols = 4\n\nplt.figure(figsize=(10,7))\nfor i in range(rows*cols):\n    ax = plt.subplot(rows, cols, i+1)\n    plt.axis('off')\n    ax.set_xticks([])\n    ax.set_yticks([])\n    plt.imshow(X_val[i].reshape(28, 28), cmap='Blues')\n    plt.title('Predicted: {}   True: {}'.format(y_pred[i], y_val_test[i])\n              ,y=-0.15)","542a9470":"ls = np.array(y_pred - y_val_test) # Subtract true values from predicted --> All non-zero results were incorrectly predicted\n\nnonzero_pred = np.nonzero(ls)[0]\n\nrows = 3\ncols = 4\n\nplt.figure(figsize=(10,7))\nfor i in range(rows*cols):\n    ax = plt.subplot(rows, cols, i+1)\n    plt.axis('off')\n    ax.set_xticks([])\n    ax.set_yticks([])\n    plt.imshow(X_val[nonzero_pred[i]].reshape(28, 28), cmap='Reds')\n    plt.title('Predicted: {}  True: {}'.format(y_pred[nonzero_pred[i]], y_val_test[nonzero_pred[i]]), y=-0.15)","9472f2a4":"test = X_test[0:30,:,:,:]\ntest_pred = np.argmax(model.predict(X_test), axis=1)\nrows = 3\ncols = 4\nplt.figure(figsize=(10,7))\nfor i in range(rows*cols):\n    ax = plt.subplot(rows,cols, i+1)\n    plt.axis('off')\n    ax.set_xticks([])\n    ax.set_yticks([])\n    plt.imshow(X_test[i].reshape(28, 28), cmap='bone')\n    plt.title('Predicted: {}'.format(test_pred[i]),y=-0.15)","84af1e6a":"for layer in model.layers:\n    if'conv' in layer.name:\n        filters, biases = layer.get_weights()\n        print('Layer: ', layer.name, filters.shape)\n#         f_min, f_max = filters.min(), filters.max()\n#         filters = (filters - f_min) \/ (f_max - f_min)\n        \n        print('Filter size: (', filters.shape[0], ',', filters.shape[1], ')')\n        print('Channels in this layer: ', filters.shape[2])\n        print('Number of filters: ', filters.shape[3])\n        \n        count = 1\n        plt.figure(figsize = (18, 4))\n        \n        for i in range(filters.shape[3]):\n            ax= plt.subplot(4, filters.shape[3]\/4, count)\n            ax.set_xticks([])\n            ax.set_yticks([])\n            plt.imshow(filters[:,:,0, i], cmap=plt.cm.binary)\n            count+=1\n            \n        plt.show()","f0b9d026":"print('total number of layers',len(model.layers))","74be7d99":"# Extract outputs of top 6 layers\nlayer_outputs = [layer.output for layer in model.layers[0:6]]\n\n# Create a model that will return these outputs given the model input\nactivation_model = models.Model(inputs = model.input, outputs = layer_outputs)","017b77cb":"plt.imshow(X_test[4].reshape(28, 28), cmap = plt.cm.binary)\nplt.xticks([])\nplt.yticks([])\nplt.show()","4390faf7":"img_tensor = X_test[4].reshape(-1, 28, 28, 1)\nactivations = activation_model.predict(img_tensor)","96538da7":"for i in range(len(activations)):\n    print(activations[i].shape)","e3ee7947":"first_layer_activation = activations[0]\n\nax = plt.subplot(1, 2, 1)\nax.set_xticks([])\nax.set_yticks([])\nplt.imshow(first_layer_activation[0,:,:,0], cmap = plt.cm.binary)\nplt.title('1st Filter o\/p')\n\nax = plt.subplot(1, 2, 2)\nax.set_xticks([])\nax.set_yticks([])\nplt.imshow(first_layer_activation[0,:,:,16], cmap = plt.cm.binary)\nplt.title('16th Filter o\/p')\n    \nplt.show()","67874dad":"layer_names = []\nfor layer in model.layers[:6]:\n    layer_names.append(layer.name)\n\nfor activation_layer, layer_name in zip(activations, layer_names):\n\n    n_features = activation_layer.shape[3]\n    feat_per_row = 16\n    rows = n_features\/\/feat_per_row\n    size = activation_layer.shape[1]\n    \n    print(layer_name)\n    plt.figure(figsize=(20, rows))\n    for i in range(n_features):\n        ax = plt.subplot(rows, feat_per_row, i+1)\n        ax.set_xticks([])\n        ax.set_yticks([])\n        plt.imshow(activation_layer[:,:,:,i].reshape(size, size), cmap = plt.cm.binary)\n    \n    plt.show()","df12bbe0":"submissions = pd.DataFrame({\"ImageId\": list(range(1,len(test_pred)+1)),\n    \"Label\": test_pred})\nsubmissions.to_csv(\"submission.csv\", index=False, header=True)","b6a30d9b":"# Libraries","0273a106":"# Output of these filters for each layer","56326557":"lets observe the outputs of 1st and 16th filters","b80a9433":"layers shapes in our NN from top","fb8ff874":"Number of wrong predictions in validation for each digit","8db56b31":"### Fit model","b2764d59":"### Image Data Generator","48325cef":"#### Validation set images","50bc3f8c":"### Label Encoding","d198a627":"#### Test Set predictions","7351d36f":"## Model Performance at glance","e0a4c04b":"### Train-Validation Split","98dbf10d":"lets consider all the layers above the flatten layer\n\nlets use keras **Model** class to get the output state of each layer for an input","296d97fd":"### Confusion Matrix","5769ba8d":"# FILTERS\n\n* This neural net uses two conv2d layers each of size 32 and 64 respectively\n\n* These filters are learnable filters that learn some visual feature as the model gets trained over images across epochs.","2ce93491":"## Read data","e83d0d84":"### Choosing the best model weights","5909253d":"### Nulls=? & Normalization","e6ae4930":"### keras-NN Base","7aef57a0":"### keras optimizer, loss-func, metrics & callbacks","ed7d06a4":"### Re-Shape data","72670088":"consider a sample from test set","20660e1f":"## Visualization","d6b22070":"as we are using early stopping, model will converge early, so we can go for larger epoches","d10594f0":"### data distribution","bea851b7":"# Modeling data\n<img src=\"https:\/\/i.imgur.com\/PFn0LrM.jpg\">","c41b9afb":"# Data Preprocessing","e28a29c4":"#### Wrong Predictions","caad285d":"## All Activation Layers"}}