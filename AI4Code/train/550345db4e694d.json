{"cell_type":{"735ce083":"code","a62ed9ef":"code","26b99aec":"code","82915e02":"code","f78155b6":"code","076ff5d3":"code","a2e31ef2":"code","96109a7c":"code","c91d4157":"code","0483371d":"code","de1abf09":"code","0c80abd0":"code","198e78f7":"code","ff737995":"code","de1c1eb2":"code","5335ca64":"code","4f5cc99a":"markdown","76032342":"markdown","34ff74c9":"markdown","5193fbda":"markdown","efa15391":"markdown"},"source":{"735ce083":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a62ed9ef":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport os\nimport random\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer","26b99aec":"train=pd.read_csv('\/kaggle\/input\/bnbr-kenya\/Train_health.csv')\ntest=pd.read_csv('\/kaggle\/input\/bnbr-kenya\/Test_health.csv')\nsub = pd.read_csv('\/kaggle\/input\/sample-submission\/ss_health.csv')","82915e02":"print('Train shape:',train.shape,'and number of null values are:',train.isnull().sum())\ntrain.head()","f78155b6":"print('Test shape:',test.shape,'and number of null values are:',test.isnull().sum())\ntest.head()","076ff5d3":"train['label'].value_counts()","a2e31ef2":"cate = train['label']\ncate.head()","96109a7c":"from sklearn.preprocessing import OneHotEncoder\ncate_one = pd.get_dummies(data=cate)\ncate_one.head()","c91d4157":"train_df =train.join(cate_one)\ntrain_df.head()","0483371d":"train_df = train_df.drop(['label'], axis=1)","de1abf09":"train_df.head()","0c80abd0":"train_df.shape","198e78f7":"!pip install --upgrade transformers\n!pip install simpletransformers\n# memory footprint support libraries\/code\n!ln -sf \/opt\/bin\/nvidia-smi \/usr\/bin\/nvidia-smi\n!pip install gputil\n!pip install psutil\n!pip install humanize","ff737995":"train_df.shape, test.shape, sub.shape","de1c1eb2":"label = ['Depression','Alcohol','Suicide','Drugs']","5335ca64":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom scipy.sparse import hstack\n\nclass_names = ['Depression','Alcohol','Suicide','Drugs']\n\ntrain_df.fillna(' ')\ntest.fillna(' ')\n\n#train = pd.read_csv('..\/input\/train.csv').fillna(' ')\n#test = pd.read_csv('..\/input\/test.csv').fillna(' ')\n\ntrain_df_text = train['text']\ntest_text = test['text']\nall_text = pd.concat([train_df_text, test_text])\n\nword_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='word',\n    token_pattern=r'\\w{1,}',\n    stop_words='english',\n    ngram_range=(1, 1),\n    max_features= 1050)\nword_vectorizer.fit(all_text)\ntrain_word_features = word_vectorizer.transform(train_df_text)\ntest_word_features = word_vectorizer.transform(test_text)\n\nchar_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='char',\n    stop_words='english',\n    ngram_range=(2, 6),\n    max_features= 1050 )\nchar_vectorizer.fit(all_text)\ntrain_char_features = char_vectorizer.transform(train_df_text)\ntest_char_features = char_vectorizer.transform(test_text)\n\ntrain_features = hstack([train_char_features, train_word_features])\ntest_features = hstack([test_char_features, test_word_features])\n\nscores = []\nsubmission = pd.DataFrame.from_dict({'ID': test['ID']})\nfor class_name in class_names:\n    train_target = train_df[class_name]\n    classifier = LogisticRegression(C=0.1, solver='sag')\n\n    cv_score = np.mean(cross_val_score(classifier, train_features, train_target, cv=3, scoring='roc_auc'))\n    scores.append(cv_score)\n    print('CV score for class {} is {}'.format(class_name, cv_score))\n\n    classifier.fit(train_features, train_target)\n    submission[class_name] = classifier.predict_proba(test_features)[:, 1]\n\nprint('Total CV score is {}'.format(np.mean(scores)))\n\nsubmission.to_csv('submission.csv', index=False)","4f5cc99a":"**Merging One-Hot encoding dataframe to train data**","76032342":"**Now let's drop the formal label in the train dataset**","34ff74c9":"**Performing One-Hot Encoding for the train label**","5193fbda":"# SIMPLE TRANSFORMERS","efa15391":"**Hence, the newly adjusted train data will look like this:**"}}