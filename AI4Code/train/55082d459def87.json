{"cell_type":{"a97d9562":"code","05a279af":"code","c1d9b218":"code","c5f7e493":"code","1894d738":"code","66aa0bc8":"code","db9f9acd":"code","4b6ec626":"code","b93111aa":"code","7e39147b":"code","72bd4e89":"code","5497cead":"code","4e1e54ed":"code","62764ee0":"code","e898e2f8":"code","0d5da837":"code","85380b50":"code","c958022c":"code","b66d712d":"code","3b225b4f":"code","55a25582":"code","af61f2a3":"code","cc366e9c":"code","67f08a6b":"code","f4e6d9d4":"code","7aef8b6a":"code","8a514e90":"code","1e4f5eea":"code","0c8d1da7":"code","d3ba2ca3":"code","7e592cb7":"code","e94f70e6":"code","183c943a":"code","0a877a16":"code","d76add46":"code","0269a1b1":"code","a70f3ad9":"code","ff96e594":"code","bad0d126":"code","8aa2128f":"code","769e8fdb":"code","641e2bc8":"code","9eb01d9b":"code","fa689e02":"code","e1542ff6":"code","f0ab9e78":"code","2d1c9a0f":"code","5115cf49":"code","15fb4fe7":"code","9c246113":"code","fea741cd":"code","45433b79":"code","6c4312ef":"code","112e9116":"code","978fb4e7":"markdown","f18438df":"markdown","f03a893f":"markdown","91895aad":"markdown","05af16e1":"markdown","cc99c694":"markdown","47c55dbd":"markdown"},"source":{"a97d9562":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","05a279af":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import accuracy_score, classification_report,f1_score,roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import KFold, cross_val_score,StratifiedKFold\nfrom sklearn.model_selection import train_test_split, GridSearchCV,RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier","c1d9b218":"data = pd.read_csv(\"..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\")","c5f7e493":"data.head()","1894d738":"data.info()","66aa0bc8":"data.describe()","db9f9acd":"data.isna().sum()","4b6ec626":"data1 = data.groupby(\"quality\").mean()","b93111aa":"data1.head()","7e39147b":"sns.barplot(x= data1.index,y = \"alcohol\", data = data1)","72bd4e89":"sns.barplot(x= data1.index,y = \"chlorides\", data = data1)","5497cead":"data.corr()","4e1e54ed":"sns.heatmap(data.corr(),annot=True)","62764ee0":"df = data.copy()","e898e2f8":"df.columns","0d5da837":"cols = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n       'pH', 'sulphates', 'alcohol']","85380b50":"df = df.drop([\"fixed acidity\",\"free sulfur dioxide\",\"volatile acidity\",\"citric acid\"],axis = 1)","c958022c":"sns.heatmap(df.corr(),annot=True)","b66d712d":"data.info()","3b225b4f":"df.info()","55a25582":"y = df.quality","af61f2a3":"df = df.drop(\"quality\", axis = 1 )","cc366e9c":"from sklearn.model_selection import train_test_split","67f08a6b":"X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=41)","f4e6d9d4":"X_train.shape","7aef8b6a":"X_test.shape","8a514e90":"cols = df.columns","1e4f5eea":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test =scaler.transform(X_test) ","0c8d1da7":"X_train = pd.DataFrame(data = X_train, columns=cols)\nX_test = pd.DataFrame(data = X_test, columns=cols)","d3ba2ca3":"X_train","7e592cb7":"from sklearn.linear_model import LogisticRegression","e94f70e6":"parameters = {\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}","183c943a":"logmodel = LogisticRegression(max_iter = 1000)","0a877a16":"rsc = RandomizedSearchCV(estimator=logmodel,param_distributions= parameters ,cv=5,) \n\nrsc.fit(X_train,y_train)\nprint(\"tuned hpyerparameters :(best parameters) \",rsc.best_params_)\nprint(\"Accuracy :\",rsc.best_score_)","d76add46":"logmodel = LogisticRegression(max_iter = 1000)","0269a1b1":"from xgboost import XGBClassifier","a70f3ad9":"estimator = XGBClassifier(random_state = 42)","ff96e594":"parameters = {\n    'max_depth': range (6,36 , 6),\n    'n_estimators': range(10, 50, 10),\n    'learning_rate': [0.01,0.1,0.3,0.5,0.8,1,2],\n     'penalty':['l2', 'l1']\n    \n}","bad0d126":"rsc = RandomizedSearchCV(estimator=estimator,param_distributions= parameters ,cv=5, n_iter = 6, verbose=1, scoring='accuracy')","8aa2128f":"rsc.fit(X_train,y_train)","769e8fdb":"rsc.best_params_","641e2bc8":"rsc.best_score_","9eb01d9b":"xgb = XGBClassifier(\n   max_depth= 30,\n    n_estimators= 30,\n    learning_rate= 0.8,\n    penalty = \"l2\",\n    random_state = 42\n)","fa689e02":"X_train.columns","e1542ff6":"from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\n","f0ab9e78":"from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\n\n\nefs = EFS(xgb, \n           min_features=1,\n           max_features=7,\n           scoring='accuracy',\n           print_progress=True,\n           cv=5)\n\nfeature_names = ('residual sugar', 'chlorides', 'total sulfur dioxide', 'density', 'pH',\n       'sulphates', 'alcohol')\nefs = efs.fit(X_train, y_train, custom_feature_names=feature_names)\n\nprint('Best accuracy score: %.2f' % efs.best_score_)\nprint('Best subset (indices):', efs.best_idx_)\nprint('Best subset (corresponding names):', efs.best_feature_names_)","2d1c9a0f":"pd.DataFrame.from_dict(efs.get_metric_dict()).T.sort_values(by =\"avg_score\",ascending = False).head(15)","5115cf49":"xgb.fit(X_train,y_train)","15fb4fe7":"y_pred = xgb.predict(X_test)","9c246113":"print(\"accuracy:\", accuracy_score(y_test,y_pred))","fea741cd":"feature_important = xgb.get_booster().get_score(importance_type='weight')\nkeys = list(feature_important.keys())\nvalues = list(feature_important.values())\n\ndata = pd.DataFrame(data=values, index=keys, columns=[\"score\"]).sort_values(by = \"score\", ascending=False)\ndata.nlargest(50, columns=\"score\").plot(kind='barh', figsize = (20,10)) ## plot top 40 features","45433b79":"pred = pd.DataFrame({\"ypred\":y_pred,\"ytest\" : y_test})","6c4312ef":"pred[\"difference\"] = pred.ytest-pred.ypred\n","112e9116":"pred","978fb4e7":"# **Train-Test Split**","f18438df":"# **Scailing**","f03a893f":"# Feature Selection","91895aad":"# ****EDA****","05af16e1":"for i in cols:\n    mean= df[i].mean()\n    std = df[i].std()\n    num = mean + 3*std\n    num2= mean - 3*std\n    df = df[(df[i] <= num) & (df[i] >= mean - 3*std)]","cc99c694":"# **Modelling**","47c55dbd":"# **Outliers**"}}