{"cell_type":{"5dccaa93":"code","e3b28915":"code","5addd198":"code","f59626d7":"code","984665b4":"code","36765d51":"code","2008dce5":"code","5bb027c8":"code","31dfe312":"code","25ee2924":"code","4b64d225":"code","d4ba39c7":"code","7e0a6e45":"code","8e068629":"code","0ac56aac":"code","9b51e5a1":"code","8b32dfa9":"code","ec117df1":"code","860b5f67":"code","b7ee093c":"code","bba92191":"code","326e08fe":"code","e6654c49":"code","f421cfb9":"markdown","c0b9e036":"markdown","012a5bfd":"markdown"},"source":{"5dccaa93":"import numpy as np \nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\nfrom sklearn.metrics import r2_score\n\nimport torch\nimport torch.nn as nn","e3b28915":"def r2(y_true, y_pred):\n    score = r2_score(y_true.cpu().detach().numpy(), y_pred.cpu().detach().numpy())\n    return score","5addd198":"torch.manual_seed(0)\n\nif torch.cuda.is_available():\n    device = 'cuda'\n    torch.cuda.manual_seed_all(0)\nelse:\n    device = 'cpu'","f59626d7":"df_train = pd.read_csv('..\/input\/sejongai-19011755\/x_train.csv')\ndf_test = pd.read_csv('..\/input\/sejongai-19011755\/x_test.csv')","984665b4":"df_train.shape, df_test.shape","36765d51":"df_train.head()","2008dce5":"# for i in range(len(df_train)):\n#     month = df_train.loc[i, '\uc6d4']\n    \n#     if month >= 3 and month <= 5:\n#         season = 0\n#     elif month >= 6 and month <= 8:\n#         season = 1\n#     elif month >= 9 and month <= 11:\n#         season = 2\n#     else:\n#         season = 3\n        \n#     df_train.loc[i, '\uacc4\uc808'] = season\n\n    \n# for i in range(len(df_test)):\n#     month = df_test.loc[i, '\uc6d4']\n    \n#     if month >= 3 and month <= 5:\n#         season = 0\n#     elif month >= 6 and month <= 8:\n#         season = 1\n#     elif month >= 9 and month <= 11:\n#         season = 2\n#     else:\n#         season = 3\n        \n#     df_test.loc[i, '\uacc4\uc808'] = season","5bb027c8":"# \ud604\uc2e4\uc801\uc778 \uacc4\uc808\nfor i in range(len(df_train)):\n    month = df_train.loc[i, '\uc6d4']\n    \n    if month >= 3 and month <= 4:\n        season = 0\n    elif month >= 5 and month <= 9:\n        season = 1\n    elif month >= 10 and month <= 11:\n        season = 2\n    else:\n        season = 3\n        \n    df_train.loc[i, '\uacc4\uc808'] = season\n\n    \nfor i in range(len(df_test)):\n    month = df_test.loc[i, '\uc6d4']\n    \n    if month >= 3 and month <= 4:\n        season = 0\n    elif month >= 5 and month <= 9:\n        season = 1\n    elif month >= 10 and month <= 11:\n        season = 2\n    else:\n        season = 3\n        \n    df_test.loc[i, '\uacc4\uc808'] = season","31dfe312":"enc = LabelEncoder()\n\ncategory_cols = ['\uc9c0\uad6c\ubcc4', '\uc8fc\ucc28\uc7a5\uba85']\n\nfor col in category_cols:\n    data_concat = pd.concat([df_train[col], df_test[col]], axis=0)\n    enc.fit(data_concat)\n    df_train[col] = enc.transform(df_train[col])\n    df_test[col] = enc.transform(df_test[col])","25ee2924":"X = df_train.drop(['\uc8fc\ucc28\ub300\uc218'], axis=1)\ny = df_train['\uc8fc\ucc28\ub300\uc218']\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25)\n\n\n# Scaling\nscaler = RobustScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)\n\n\n# To Tensor\nX_train = torch.FloatTensor(X_train).to(device)\nX_val = torch.FloatTensor(X_val).to(device)\n\ny_train = torch.FloatTensor(y_train.values).to(device)\ny_val = torch.FloatTensor(y_val.values).to(device)","4b64d225":"# Hyperparameter\nn_epochs = 5000\nlearning_rate = 1e-2\ndrop_prob = 0.3\n\n\n# Model\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        \n        self.fc1 = nn.Linear(X_train.shape[1], 512)\n        self.fc2 = nn.Linear(512, 512)\n        self.fc3 = nn.Linear(512, 512)\n        self.fc4 = nn.Linear(512, 128)\n        self.fc5 = nn.Linear(128, 1)\n        \n        self.leaky = nn.LeakyReLU()\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=drop_prob)\n        \n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight.data)\n                \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.leaky(out)\n        out = self.dropout(out)\n        \n        out = self.fc2(out)\n        out = self.leaky(out)\n        out = self.dropout(out)\n        \n        out = self.fc3(out)\n        out = self.leaky(out)\n        out = self.dropout(out)\n        \n        out = self.fc4(out)\n        out = self.leaky(out)\n        out = self.dropout(out)\n        \n        out = self.fc5(out)\n        return out","d4ba39c7":"model = Net().to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\nloss_fn = nn.MSELoss()\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.7)","7e0a6e45":"for epoch in range(1, n_epochs+1):\n    model.train()\n    H = model(X_train)\n    loss = loss_fn(H, y_train.reshape(-1, 1))\n    \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    scheduler.step()\n    \n    model.eval()\n    with torch.no_grad():\n        H_val = model(X_val)\n        loss_val = loss_fn(H_val, y_val.reshape(-1, 1))\n    \n    if epoch % 50 == 0:\n        print('Epoch {:4d} \/ {},   Loss : {:12.4f},   Val Loss : {:12.4f}'.format(\n            epoch, n_epochs, loss.item(), loss_val.item()))","8e068629":"print('Train R2 : {:.4f}\\nVal R2 : {:.4f}'.format(r2(y_train, H), r2(y_val, H_val)))","0ac56aac":"# X = df_train.drop(['\uc8fc\ucc28\ub300\uc218'], axis=1)\n# y = df_train['\uc8fc\ucc28\ub300\uc218']\n\n# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25)\n\n# X_train['\ub144'] = (X_train['\ub144'] == 2021).astype(np.float)\n# X_val['\ub144'] = (X_val['\ub144'] == 2021).astype(np.float)\n\n# con_feature = ['\uc774\uc6a9\uc2dc\uac04', '\ud3c9\uade0\uae30\uc628(\u2103)', '\ucd5c\uc800\uae30\uc628(\u2103)', '\ucd5c\uace0\uae30\uc628(\u2103)']\n\n# # Scaling\n# scaler = StandardScaler()\n\n# X_train.loc[:, con_feature] = scaler.fit_transform(X_train.loc[:, con_feature])\n# X_val.loc[:, con_feature] = scaler.transform(X_val.loc[:, con_feature])\n\n# # To Tensor\n# X_train = torch.FloatTensor(X_train.values).to(device)\n# X_val = torch.FloatTensor(X_val.values).to(device)\n\n# y_train = torch.FloatTensor(y_train.values).to(device)\n# y_val = torch.FloatTensor(y_val.values).to(device)\n\n# # Hyperparameter\n# n_epochs = 9000\n# learning_rate = 1e-3\n# drop_prob = 0.3\n\n\n# # Model\n# class Net(nn.Module):\n#     def __init__(self):\n#         super(Net, self).__init__()\n        \n#         self.fc1 = nn.Linear(X_train.shape[1], 512)\n#         self.fc2 = nn.Linear(512, 512)\n#         self.fc3 = nn.Linear(512, 256)\n#         self.fc4 = nn.Linear(256, 128)\n#         self.fc5 = nn.Linear(128, 1)\n        \n#         self.relu = nn.ReLU()\n#         self.dropout = nn.Dropout(p=drop_prob)\n        \n#         for m in self.modules():\n#             if isinstance(m, nn.Linear):\n#                 nn.init.xavier_uniform_(m.weight.data)\n                \n#     def forward(self, x):\n#         out = self.fc1(x)\n#         out = self.relu(out)\n#         out = self.dropout(out)\n        \n#         out = self.fc2(out)\n#         out = self.relu(out)\n#         out = self.dropout(out)\n        \n#         out = self.fc3(out)\n#         out = self.relu(out)\n#         out = self.dropout(out)\n        \n#         out = self.fc4(out)\n#         out = self.relu(out)\n#         out = self.dropout(out)\n        \n#         out = self.fc5(out)\n#         return out\n\n# model = Net().to(device)\n\n# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n# loss_fn = nn.MSELoss()\n# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1200, gamma=0.3)\n\n# for epoch in range(1, n_epochs+1):\n#     model.train()\n#     H = model(X_train)\n#     loss = loss_fn(H, y_train.reshape(-1, 1))\n    \n#     optimizer.zero_grad()\n#     loss.backward()\n#     optimizer.step()\n#     scheduler.step()\n    \n#     model.eval()\n#     with torch.no_grad():\n#         H_val = model(X_val)\n#         loss_val = loss_fn(H_val, y_val.reshape(-1, 1))\n    \n#     if epoch % 50 == 0:\n#         print('Epoch {:4d} \/ {},   Loss : {:12.4f},   Val Loss : {:12.4f}'.format(\n#             epoch, n_epochs, loss.item(), loss_val.item()))\n\n# print('Train R2 : {:.4f}\\nVal R2 : {:.4f}'.format(r2(y_train, H), r2(y_val, H_val)))","9b51e5a1":"X_train = df_train.drop(['\uc8fc\ucc28\ub300\uc218'], axis=1)\ny_train = df_train['\uc8fc\ucc28\ub300\uc218']\nX_test = df_test.copy()\n\n\n# Scaling\nscaler = RobustScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n\n# To Tensor\nX_train = torch.FloatTensor(X_train).to(device)\nX_test = torch.FloatTensor(X_test).to(device)\n\ny_train = torch.FloatTensor(y_train.values).to(device)","8b32dfa9":"model = Net().to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\nloss_fn = nn.MSELoss()\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.7)","ec117df1":"for epoch in range(1, n_epochs+1):\n    model.train()\n    H = model(X_train)\n    loss = loss_fn(H, y_train.reshape(-1, 1))\n    \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    scheduler.step()\n \n    if epoch % 50 == 0:\n        print('Epoch {:4d} \/ {},   Loss : {:12.4f}'.format(\n            epoch, n_epochs, loss.item()))","860b5f67":"r2(y_train, H)","b7ee093c":"model.eval()\nwith torch.no_grad():\n    pred = model(X_test)","bba92191":"submit = pd.read_csv('..\/input\/sejongai-19011755\/y_test_submit.csv')","326e08fe":"submit['\uc8fc\ucc28\ub300\uc218'] = pred.cpu()\nsubmit.head()","e6654c49":"submit.to_csv('submission.csv', index=False)","f421cfb9":"# Submission","c0b9e036":"# Not Scaling Categorical Features","012a5bfd":"# Train with Validation"}}