{"cell_type":{"db625eeb":"code","ddfaf35a":"code","b1ecddc7":"code","37b52e6e":"code","b3bd03fc":"code","16bd3cc3":"code","3ee02dbd":"code","285c16d0":"code","f14c788a":"code","4af9f88f":"code","94cb3285":"code","e92e6ddf":"code","769b9afe":"code","c55431ab":"code","d8054593":"code","56ff36aa":"code","3ceff78e":"code","59d6cfba":"code","da385102":"code","f9d0442a":"code","7ae2a934":"code","410175fb":"code","6342dee2":"code","63926d89":"code","e7db505b":"code","eebadfd6":"code","1a328df2":"code","eeb6c95c":"code","f015b479":"code","dde96cbe":"code","e0c96fdc":"code","ae40e44a":"code","560acf5f":"code","a1913559":"code","efbed0e1":"code","3aeff12a":"code","13218287":"code","76b4ecb7":"code","bbdd35ec":"code","71fc802f":"code","ce3794e1":"code","271e1828":"code","379b5203":"code","7a4733e2":"code","6c16a7ad":"code","45f49131":"markdown","50b5ccb2":"markdown","9763a61e":"markdown","05958f3e":"markdown","620904c8":"markdown","f9b38730":"markdown","6a70f281":"markdown","640602a6":"markdown","89309989":"markdown","f42eb3b7":"markdown","65f19083":"markdown","a4d75815":"markdown","d54126ca":"markdown"},"source":{"db625eeb":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.linear_model import LogisticRegression\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\nimport optuna\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import auc\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV , GridSearchCV\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ddfaf35a":"df_train_ogi = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-sep-2021\/train.csv\")\ndf_test_ogi = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-sep-2021\/test.csv\")   ","b1ecddc7":"df_train = df_train_ogi.copy()\ndf_test = df_test_ogi.copy()","37b52e6e":"#df_train.head()","b3bd03fc":"df_train.describe()","16bd3cc3":"df_train.info()","3ee02dbd":"pd.set_option(\"max_columns\", None)","285c16d0":"df_train.isna().sum()","f14c788a":"len(df_train)","4af9f88f":"# df = pd.concat([df_train.drop([\"id\", \"claim\"], axis=1), df_test.drop(\"id\", axis=1)], axis=0)\n# columns = df.columns.values\n\n# cols = 4\n# rows = len(columns) \/\/ cols + 1\n\n# fig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(25,230), sharex=False)\n\n# plt.subplots_adjust(hspace = 0.3)\n# i=0\n\n# for r in np.arange(0, rows, 1):\n#     for c in np.arange(0, cols, 1):\n#         if i >= len(columns):\n#             axs[r, c].set_visible(False)\n#         else:\n#             hist1 = axs[r, c].hist(df_train[columns[i]].values,\n#                                    range=(df[columns[i]].min(),\n#                                           df[columns[i]].max()),\n#                                    bins=40,\n#                                    color=\"blue\",\n#                                    edgecolor=\"black\",\n#                                    alpha=0.7,\n#                                    label=\"Train Dataset\")\n#             hist2 = axs[r, c].hist(df_test[columns[i]].values,\n#                                    range=(df[columns[i]].min(),\n#                                           df[columns[i]].max()),\n#                                    bins=40,\n#                                    color=\"yellow\",\n#                                    edgecolor=\"black\",\n#                                    alpha=0.7,\n#                                    label=\"Test Dataset\")\n#             axs[r, c].set_title(columns[i], fontsize=12, pad=5)\n#             axs[r, c].set_yticks(axs[r, c].get_yticks())\n#             axs[r, c].set_yticklabels([str(int(i\/1000))+\"k\" for i in axs[r, c].get_yticks()])\n#             axs[r, c].tick_params(axis=\"y\", labelsize=10)\n#             axs[r, c].tick_params(axis=\"x\", labelsize=10)\n#             axs[r, c].grid(axis=\"y\")\n#             axs[r, c].legend(fontsize=13)\n                                  \n#         i+=1\n# plt.show();","94cb3285":"# for x in df_train.columns : \n#     plt.hist(df_train[x])\n#     plt.show()","e92e6ddf":"# for x in df_train.columns : \n#     sns.boxplot(df_train[x] , orient = 'Vertical', color = 'yellow')\n#     plt.show();","769b9afe":"df_train['claim'].value_counts()","c55431ab":"df_train = df_train.drop(\"id\", axis = 1)","d8054593":"features = [col for col in df_test.columns if 'f' in col]\ndf_target = 'claim'\n\ntarget = df_train[df_target].copy()\ndf_train['n_missing'] = df_train[features].isna().sum(axis=1)\ndf_train['mean'] = df_train[features].mean(axis=1)\ndf_train['median'] = df_train[features].median(axis=1)\ndf_train['std'] = df_train[features].std(axis=1)\ndf_train['min'] = df_train[features].min(axis=1)\ndf_train['max'] = df_train[features].max(axis=1)\ndf_train['sem']= df_train[features].sem(axis=1)\ndf_train['skew'] = df_train[features].skew(axis = 1)\ndf_train['mad'] = df_train[features].mad(axis = 1)\n\ndf_test['n_missing'] = df_test[features].isna().sum(axis=1)\ndf_test['mean'] = df_test[features].mean(axis=1)\ndf_test['median'] = df_test[features].median(axis=1)\ndf_test['std'] = df_test[features].std(axis=1)\ndf_test['min'] = df_test[features].min(axis=1)\ndf_test['max'] = df_test[features].max(axis=1)\ndf_test['sem']= df_test[features].sem(axis=1)\ndf_test['skew'] = df_test[features].skew(axis=1)\ndf_test['mad'] = df_test[features].mad(axis = 1)","56ff36aa":"features += ['n_missing','mean','median','std','min','max','sem','skew','mad']","3ceff78e":"from tqdm import tqdm\nfill_value_dict = {\n    'f1': 'Mean', \n    'f2': 'Median', \n    'f3': 'Median', \n    'f4': 'Median', \n    'f5': 'Mode', \n    'f6': 'Mean', \n    'f7': 'Median', \n    'f8': 'Median', \n    'f9': 'Median', \n    'f10': 'Median', \n    'f11': 'Mean', \n    'f12': 'Median', \n    'f13': 'Mean', \n    'f14': 'Median', \n    'f15': 'Mean', \n    'f16': 'Median', \n    'f17': 'Median', \n    'f18': 'Median', \n    'f19': 'Median', \n    'f20': 'Median', \n    'f21': 'Median', \n    'f22': 'Mean', \n    'f23': 'Mode', \n    'f24': 'Median', \n    'f25': 'Median', \n    'f26': 'Median', \n    'f27': 'Median', \n    'f28': 'Median', \n    'f29': 'Mode', \n    'f30': 'Median', \n    'f31': 'Median', \n    'f32': 'Median', \n    'f33': 'Median', \n    'f34': 'Mean', \n    'f35': 'Median', \n    'f36': 'Mean', \n    'f37': 'Median', \n    'f38': 'Median', \n    'f39': 'Median', \n    'f40': 'Mode', \n    'f41': 'Median', \n    'f42': 'Mode', \n    'f43': 'Mean', \n    'f44': 'Median', \n    'f45': 'Median', \n    'f46': 'Mean', \n    'f47': 'Mode', \n    'f48': 'Mean', \n    'f49': 'Mode', \n    'f50': 'Mode', \n    'f51': 'Median', \n    'f52': 'Median', \n    'f53': 'Median', \n    'f54': 'Mean', \n    'f55': 'Mean', \n    'f56': 'Mode', \n    'f57': 'Mean', \n    'f58': 'Median', \n    'f59': 'Median', \n    'f60': 'Median', \n    'f61': 'Median', \n    'f62': 'Median', \n    'f63': 'Median', \n    'f64': 'Median', \n    'f65': 'Mode', \n    'f66': 'Median', \n    'f67': 'Median', \n    'f68': 'Median', \n    'f69': 'Mean', \n    'f70': 'Mode', \n    'f71': 'Median', \n    'f72': 'Median', \n    'f73': 'Median', \n    'f74': 'Mode', \n    'f75': 'Mode', \n    'f76': 'Mean', \n    'f77': 'Mode', \n    'f78': 'Median', \n    'f79': 'Mean', \n    'f80': 'Median', \n    'f81': 'Mode', \n    'f82': 'Median', \n    'f83': 'Mode', \n    'f84': 'Median', \n    'f85': 'Median', \n    'f86': 'Median', \n    'f87': 'Median', \n    'f88': 'Median', \n    'f89': 'Median', \n    'f90': 'Mean', \n    'f91': 'Mode', \n    'f92': 'Median', \n    'f93': 'Median', \n    'f94': 'Median', \n    'f95': 'Median', \n    'f96': 'Median', \n    'f97': 'Mean', \n    'f98': 'Median', \n    'f99': 'Median', \n    'f100': 'Mode', \n    'f101': 'Median', \n    'f102': 'Median', \n    'f103': 'Median', \n    'f104': 'Median', \n    'f105': 'Median', \n    'f106': 'Median', \n    'f107': 'Median', \n    'f108': 'Median', \n    'f109': 'Mode', \n    'f110': 'Median', \n    'f111': 'Median', \n    'f112': 'Median', \n    'f113': 'Mean', \n    'f114': 'Median', \n    'f115': 'Median', \n    'f116': 'Mode', \n    'f117': 'Median', \n    'f118': 'Mean'\n}\n\n\nfor col in tqdm(features):\n    if fill_value_dict.get(col)=='Mean':\n        fill_value = df_train[col].mean()\n    elif fill_value_dict.get(col)=='Median':\n        fill_value = df_train[col].median()\n    elif fill_value_dict.get(col)=='Mode':\n        fill_value = df_train[col].mode().iloc[0]\n    \n    df_train[col].fillna(fill_value, inplace=True)\n    df_test[col].fillna(fill_value, inplace=True)","59d6cfba":"X = df_train.drop('claim', axis = 1)\ny = df_train['claim']\nX_test = df_test.drop('id',axis =1 )","da385102":"# for x in df_train.columns: \n#     df_train[x] = df_train[x].fillna(df_train[x].mean())","f9d0442a":"#df_train.head()","7ae2a934":"#train.head()","410175fb":"#y.head()","6342dee2":"xtrain, xtest , ytrain , ytest = train_test_split(X , y , test_size = 0.2 , random_state  = 0)","63926d89":"scaler = MinMaxScaler()\ntrain_scaled = scaler.fit_transform(xtrain)\ntest_scaled = scaler.transform(xtest)","e7db505b":"# %%time\n# lr = LogisticRegression(solver='saga', penalty = 'elasticnet', random_state = 0 ,max_iter = 500 , l1_ratio = 0.6 )\n# model1 = lr.fit(train_scaled , ytrain)","eebadfd6":"# from sklearn import metrics\n# pred= model1.predict(test_scaled)\n# print(pred)\n# #\n# ytest\n# print(accuracy_score(pred , ytest))\n# fpr, tpr, thresholds = metrics.roc_curve(pred, ytest, pos_label=1)\n# print(metrics.auc(fpr, tpr))","1a328df2":"#!pip install scikit-learn-intelex --progress-bar off >> \/tmp\/pip_sklearnex.log","eeb6c95c":"# from sklearnex import patch_sklearn\n# patch_sklearn()","f015b479":"# %%time\n# lr = RandomForestClassifier()\n# model = lr.fit(train_scaled,ytrain)","dde96cbe":"# pred= model.predict(test_scaled)\n# accuracy_score(pred , ytest)\n# fpr, tpr, thresholds = metrics.roc_curve(pred, ytest, pos_label=1)\n# print(metrics.auc(fpr, tpr))","e0c96fdc":"# from sklearn.metrics import roc_auc_score\n# pred= model.predict(test_scaled)\n# print(roc_auc_score(pred,ytest))","ae40e44a":"# def objective(trial,data=train_scaled,target=ytrain):\n    \n#     param = {\n\n#         'lambda': trial.suggest_uniform('lambda',0.001,0.1),\n#         'alpha': trial.suggest_uniform('alpha',0.1,0.5),\n#         'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.1,1.0),\n#         'subsample': trial.suggest_uniform('subsample', 0.5,0.9),\n#         'learning_rate': trial.suggest_uniform('learning_rate', 0.05,0.10),\n#         'n_estimators': trial.suggest_int('n_estimators', 1000,30000),\n#         'max_depth': trial.suggest_int('max_depth', 3,8),\n#         'min_child_weight': trial.suggest_int('min_child_weight', 10,100),        \n#         'objective': trial.suggest_categorical('objective',['binary:logistic']), \n#         'tree_method': trial.suggest_categorical('tree_method',['gpu_hist']),  # 'gpu_hist','hist'\n#         'eval_metric' : 'logloss'\n#     }\n#     model = xgb.XGBClassifier(**param)      \n#     model.fit(train_scaled,ytrain,eval_set=[(test_scaled,ytest)],early_stopping_rounds=100,verbose=False)\n#     preds = model.predict(test_scaled)\n#     auc = roc_auc_score(ytest, preds)\n    \n#     return auc","560acf5f":"# import optuna\n# from optuna.samplers import TPESampler\n# import sklearn\n# sampler = TPESampler(seed=0)\n# study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n# study.optimize(objective, n_trials=200)\n# params = study.best_params #getting best params from study\n# print('Number of finished trials:', len(study.trials))\n# print('Best trial:', study.best_trial.params)\n","a1913559":"# params = {'lambda': 0.01053687237713984, \n#           'alpha': 0.4495749711394468, \n#           'colsample_bytree': 0.8378659612970323, \n#           'subsample': 0.5939624665626383, \n#           'learning_rate': 0.05402737627291443 ,\n#           'n_estimators': 2564, \n#           'max_depth': 3, \n#           'random_state': 0, \n#           'min_child_weight': 24, \n#           'objective': 'binary:logistic', \n#           'tree_method': 'gpu_hist', \n#           'use_label_encoder': False}","efbed0e1":"# %%time\n# xgb = xgb.XGBClassifier(**params)\n# model = xgb.fit(train_scaled,ytrain)","3aeff12a":"# from sklearn.metrics import roc_auc_score\n# pred= model.predict(test_scaled)\n# print(roc_auc_score(pred,ytest))","13218287":"# def create_model(trial):\n#     num_leaves = trial.suggest_int(\"num_leaves\", 100, 200)\n#     n_estimators = trial.suggest_int(\"n_estimators\", 30000, 50000)\n#     min_child_samples = trial.suggest_int('min_child_samples', 100, 200)\n#     min_child_weight = trial.suggest_int('min_child_weight', 10, 200)\n#     learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.1)\n#     reg_alpha = trial.suggest_uniform('reg_alpha', 10, 100)\n#     reg_lambda = trial.suggest_uniform('reg_lambda', 10, 100)\n#     colsample_bytree = trial.suggest_uniform('colsample_bytree', 0.50, 1.0)\n#     device =  'gpu'\n\n#     model = lgb.LGBMClassifier(\n#         objective='binary',\n#         metric='binary_logloss',\n#         num_leaves = num_leaves,\n#         n_estimators = n_estimators,\n#         min_child_samples = min_child_samples,\n#         min_child_weight = min_child_weight,\n#         learning_rate = learning_rate,\n#         reg_alpha = reg_alpha,\n#         reg_lambda = reg_lambda,\n#         colsample_bytree = colsample_bytree,\n#         device =  'gpu',\n#         random_state=0,\n#         verbosity = -1\n#     )\n    \n#     return model\n\n# def objective(trial):\n#     model = create_model(trial)\n#     model.fit(train_scaled,ytrain,eval_set=[(test_scaled,ytest)],early_stopping_rounds=10,verbose=False)\n#     preds = model.predict(test_scaled)\n#     auc = roc_auc_score(ytest, preds)\n# #     model.fit(train_scaled, ytrain)\n# #     score = sklearn.metrics.roc_auc_score(test_scaled, model.predict_proba(ytest)[:,1])\n# #     return score\n#     return auc\n","76b4ecb7":"# import optuna\n# from optuna.samplers import TPESampler\n# import sklearn\n# sampler = TPESampler(seed=0)\n# study = optuna.create_study(direction=\"maximize\", sampler=sampler,pruner=optuna.pruners.HyperbandPruner())\n# study.optimize(objective, n_trials=100)\n# params = study.best_params #getting best params from study\n# print('Number of finished trials:', len(study.trials))\n# print('Best trial:', study.best_trial.params)","bbdd35ec":"lgbm_params = {'objective': 'binary',\n                'boosting_type': 'gbdt',\n                'num_leaves': 156,\n                'max_depth': 4,\n                'learning_rate': 0.01,\n                'n_estimators': 40026,\n                'reg_alpha': 25.5,\n                'reg_lambda': 96.6,\n                'random_state': 0,\n                'bagging_seed': 0, \n                'feature_fraction_seed': 0,\n                'n_jobs': 4,\n                'subsample': 0.6,\n                'subsample_freq': 1,\n                'colsample_bytree': 0.95,\n                'min_child_samples': 95,\n                'min_child_weight': 145,\n                'metric': 'AUC',\n                'verbosity': -1,\n                'device' : 'gpu'\n              }","71fc802f":"# import lightgbm as lgb\n# model = lgb.LGBMClassifier(**params, device ='gpu')\n# model.fit(train_scaled , ytrain)","ce3794e1":"# from sklearn.metrics import roc_auc_score\n# pred= model.predict(test_scaled)\n# print(roc_auc_score(pred,ytest))","271e1828":"X = df_train.drop('claim', axis = 1)\ny = df_train['claim']\nX_test = df_test.drop('id', axis= 1 )\n\nscaler = MinMaxScaler()\nX = scaler.fit_transform(X)\nX_test = scaler.transform(X_test)","379b5203":"%%time\nfrom sklearn.model_selection import StratifiedKFold\nimport gc\nSEED = 0\nsplits = 3\nkf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=SEED)\n\npreds = np.zeros(len(X_test))\n\nfor train_idx, test_idx in kf.split(X, y):    \n    train = lgb.Dataset(X[train_idx], y[train_idx], free_raw_data=False)\n    test = lgb.Dataset(X[test_idx], y[test_idx], free_raw_data=False)\n    \n    lgbm_params['learning_rate'] = 0.01\n    \n    model = lgb.train(lgbm_params,\n                      train,\n                      verbose_eval=-1,\n                      early_stopping_rounds=10,\n                      valid_sets=[test])\n    \n    preds += model.predict(X_test) \/ splits\n    gc.collect()","7a4733e2":"submission = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv', index_col='id')\nsubmission['claim'] = preds\nsubmission.to_csv('submission.csv')","6c16a7ad":"df1 = pd.read_csv(\"submission.csv\")\ndf1.head()","45f49131":"# Splitting the data into train and test","50b5ccb2":"### Uncomment the below cells to use Intel Extension","9763a61e":"## Using XGBoost","05958f3e":"## Using LightGBM","620904c8":"# Uni-variate Analysis","f9b38730":"# Handling the Missing Values","6a70f281":"## Model Building","640602a6":"### We will not deleted the outlier values except scale them ","89309989":"# Scaling Data","f42eb3b7":"##### Idea taken from https:\/\/www.kaggle.com\/realtimshady\/single-simple-lightgbm","65f19083":"## Importing Libraries","a4d75815":"### Without Intel Extension","d54126ca":"# Using Intel Extension for Scikit-Learn"}}