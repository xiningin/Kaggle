{"cell_type":{"cfde8f65":"code","24163675":"code","a4877def":"code","e4ad6e81":"code","c150ad32":"code","deb67ffc":"code","c348c357":"code","1b9ba1c5":"code","cc09cd38":"code","81e78722":"code","5c96d95e":"code","1e1f8e87":"code","5bf63537":"code","65be371d":"code","7f95ed9b":"code","e37a4536":"code","713c05dd":"code","799a07cb":"code","bddfe952":"code","03cad952":"code","7ce20344":"code","1139166b":"code","a5bb39c5":"code","3abd2d67":"code","465a5cb3":"code","95964e5d":"code","572c9504":"code","9ff04e06":"code","9b79a03f":"code","6b52299a":"code","ec1ed476":"code","e3ece6ef":"code","9dc6444f":"code","f09bbc7d":"code","c4e6d7d8":"code","79fa36e8":"code","4ec25252":"code","3db55d8d":"code","eddc267a":"code","39b4524e":"code","7e0718dd":"code","e05a0cf8":"code","6d2ca6bf":"code","9735e48c":"code","96923421":"code","e2f8fa21":"code","4875ae7d":"code","132f7d36":"code","803edcc8":"code","83204a3d":"code","134b2464":"code","1f9fe678":"code","a55b0df1":"code","148d91b3":"code","6da34e40":"code","dd80cb31":"code","f47f88b7":"code","bac671c6":"code","06330b88":"code","f3ec3c45":"code","a5068653":"code","ac2ecbe0":"code","e00072fa":"code","eaf4fd00":"code","bace4e2e":"code","6816323e":"code","4b64df42":"code","f6e3703a":"code","68b60f49":"code","57cdbed4":"code","b8c73a55":"code","b32ab7db":"code","eb6113bd":"code","3a0a8ed6":"code","50c9192b":"code","570a1f78":"code","a9cf2258":"code","b3ed685a":"code","f33e3229":"code","4f5a34e3":"code","9051f386":"markdown","f5b04627":"markdown","0dc6b8ec":"markdown","06294f06":"markdown","190990cf":"markdown","1ed8f26a":"markdown","e37c8578":"markdown","1068b7d2":"markdown","272029a1":"markdown","3fb1c722":"markdown","b63e74ce":"markdown","048717eb":"markdown","6f8e5a62":"markdown","34ebdfac":"markdown","4f5d40c7":"markdown","4f26cbdf":"markdown","c2bb24a5":"markdown","cc972460":"markdown","3f553b04":"markdown","c31a7aa0":"markdown","6472cc9c":"markdown","0f56cf1b":"markdown","a182e74a":"markdown","baf9e07b":"markdown","8bcb4fb6":"markdown","8cd0b516":"markdown","81c01b10":"markdown","e18e6517":"markdown","e2cc1f06":"markdown","e77c1c5e":"markdown","0451b7a9":"markdown","cb7a5281":"markdown","1340cc9b":"markdown","f9d2c7b0":"markdown","e7fce2ce":"markdown","c591dcd7":"markdown","3efb269a":"markdown","358c846d":"markdown","ba5876bc":"markdown","2ea57326":"markdown","21a32a7a":"markdown","010cb9b2":"markdown","c4dc7d89":"markdown"},"source":{"cfde8f65":"import os\nprint(os.listdir(\"..\/input\"))","24163675":"train_filepath='..\/input\/train.csv'\ntest_filepath='..\/input\/test.csv'","a4877def":"import numpy as np\nimport pandas as pd\ndf_train0 = pd.read_csv(train_filepath,decimal=\",\")\ndf_test0 = pd.read_csv(test_filepath,decimal=\",\")","e4ad6e81":"df_train0.head()","c150ad32":"df_test0.head()","deb67ffc":"df_train0['Survived'].value_counts(normalize=True)\n","c348c357":"df_test0['Survived']=-1\ndf_train0.columns\ndf_test0=df_test0.reindex(columns=df_train0.columns)\ndf_test0.head()","1b9ba1c5":"df_all=pd.concat([df_train0,df_test0]).copy()\ndf_all.head()","cc09cd38":"df_all.dtypes","81e78722":"df_all.isnull().sum()","5c96d95e":"df_all['Age'].describe()","1e1f8e87":"dg=df_all[['Sex','Pclass','Age']].dropna().copy()","5bf63537":"dg.head()","65be371d":"dg['Age']=pd.to_numeric(dg['Age'])","7f95ed9b":"import seaborn as sns","e37a4536":"sns.boxplot(x='Pclass', y='Age', data=dg)","713c05dd":"sns.boxplot(x='Sex', y='Age', data=dg)","799a07cb":"def input_age(cols):\n    Age=cols[0]\n    Pclass=cols[1]\n    if pd.isnull(Age):\n        if Pclass==1: return 38\n        elif Pclass==2: return 30\n        else: return 25\n    else: return Age","bddfe952":"df_all['Age']=df_all[['Age','Pclass']].apply(input_age,axis=1)","03cad952":"df_all['Age']=df_all['Age'].astype(float)","7ce20344":"df_all['Fare'].describe()","1139166b":"bool_val_not_null=np.invert(df_all['Fare'].isnull())\nfares_num=pd.to_numeric(df_all['Fare'][bool_val_not_null])\nnp.average(fares_num)","a5bb39c5":"df_all.loc[df_all['Fare'].isnull(),'Fare']=33.3","3abd2d67":"df_all['Fare']=df_all['Fare'].astype(float)","465a5cb3":"df_all['Embarked'].value_counts(normalize=True)","95964e5d":"df_all.loc[df_all['Embarked'].isnull(),'Embarked']='S'","572c9504":"df_all.head(10)","9ff04e06":"df_all['Cabin'].unique()","9b79a03f":"df_all.loc[df_all['Cabin'].isnull(),'Cabin']='XX'","6b52299a":"df_all.isnull().sum()","ec1ed476":"df_all['Cabin']=df_all['Cabin'].str.extract('(.)')","e3ece6ef":"df_all.head()","9dc6444f":"n=df_all['Name'].str.extract('.+?,\\s(.+?).\\s')[0]","f09bbc7d":"n.value_counts()","c4e6d7d8":"df_all[n=='Jonkheer']","79fa36e8":"df_all[n=='th']","4ec25252":"df_all[n=='Master'].head()","3db55d8d":"n[n=='Jonkheer']='Nh'","eddc267a":"n[n=='Sir']='Nh'","39b4524e":"n[n=='th']='Nh'","7e0718dd":"n[n=='Mlle']='Miss'","e05a0cf8":"n[n=='Mme']='Mrs'","6d2ca6bf":"n[n=='Lady']='Miss'","9735e48c":"n[n=='Dona']='Miss'","96923421":"n[n=='Don']='Rev'","e2f8fa21":"n[n=='Ms']='Miss'","4875ae7d":"n[n=='Major']='Nh'","132f7d36":"n[n=='Capt']='Nh'","803edcc8":"n[n=='Col']='Nh'","83204a3d":"n.value_counts()","134b2464":"df_all['Title']=n.copy()","1f9fe678":"df_all['Parch'].value_counts()","a55b0df1":"df_all['Fam']=df_all['SibSp']+df_all['Parch']","148d91b3":"df_all.head()","6da34e40":"df=df_all.copy()\ndf.head()","dd80cb31":"df.to_csv('titanic_full.csv',index=False)","f47f88b7":"df.drop('Name', axis=1, inplace=True)\ndf.drop('Ticket', axis=1, inplace=True)\ndf.drop('SibSp', axis=1, inplace=True)\ndf.drop('Parch', axis=1, inplace=True)\ndf.head()","bac671c6":"df.to_csv('titanic_clean.csv',index=False)","06330b88":"sex=pd.get_dummies(df['Sex'],drop_first=True)","f3ec3c45":"embarked=pd.get_dummies(df['Embarked'],drop_first=True)","a5068653":"title=pd.get_dummies(df['Title'],drop_first=True)","ac2ecbe0":"cabin=pd.get_dummies(df['Cabin'],drop_first=True)","e00072fa":"df=pd.concat([df,sex,embarked,title,cabin], axis=1)\ndf.drop('Embarked', axis=1, inplace=True)\ndf.drop('Sex', axis=1, inplace=True)\ndf.drop('Title', axis=1, inplace=True)\ndf.drop('Cabin', axis=1, inplace=True)\ndf.head()","eaf4fd00":"train=df[df['Survived']>=0].copy()\ntest=df[df['Survived']==-1].copy()","bace4e2e":"test=test.drop('Survived',axis=1)","6816323e":"train.head()","4b64df42":"test.head()","f6e3703a":"y = train['Survived']\nX = train.drop('Survived',axis=1)","68b60f49":"from sklearn.model_selection import train_test_split","57cdbed4":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=123)","b8c73a55":"from sklearn.linear_model import LogisticRegression","b32ab7db":"lrm = LogisticRegression()","eb6113bd":"lrm.fit(X_train,y_train)","3a0a8ed6":"y_test_pred=lrm.predict(X_test)","50c9192b":"from sklearn.metrics import confusion_matrix","570a1f78":"C = confusion_matrix(y_test,y_test_pred)\nprint(C)\naccuracy=(C[0,0]+C[1,1])\/(C[0,0]+C[1,0]+C[0,1]+C[1,1])\nprint(accuracy)","a9cf2258":"lrm.fit(X,y)","b3ed685a":"y_pred=lrm.predict(test)","f33e3229":"subm=pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": y_pred\n    })\nsubm.head()","4f5a34e3":"subm.to_csv('titanic_third.csv', index=False)","9051f386":"The case is quite similar but we have only one NaN, therefore using the basic mean it's enough.","f5b04627":"Missing Data\n-----\n\nTo deal with the missing data you can use the isnull() function. There is also an isna() but in this case it's easy to verify that it gives the same result.","0dc6b8ec":"We have the names in the form \"XXX, [Title] YYY\" so our regexp simply says: \"find the comma, ignore XXX and put in the extracted value what you find between the two spaces\". These are the different values found in this way:","06294f06":"We can say that SibSp and Parch are both related to family and we think that the only importat thing is this: if you have a familiar on boad, probably he\/she and you will collaborate to survive, so surviving chances of both of you, reasonably increase. Then let's create a new feature called \"Fam\" (for number of FAMily members):","190990cf":"Now let's consider the Fare column:","1ed8f26a":"The Name can be interesting mainly because it contains the title, which is reasonably related to the economic status of the person and to a probably better accomodation. We'll use a regular expression to do that in this way: ","e37c8578":"With this tool we reached accuracy=84% starting from the accuracy=78% of the gender submission.","1068b7d2":"It's a classification problem, so to understand if our model performs well or not, an usefult tool is the **confusion matrix**: how many real Survived=1 are (wrongly) predicted as Survived=0 and so on?","272029a1":"Let's check that we have no more null Cabin values ...","3fb1c722":"And now finally, we **import and call our classifier**, chosen in the family \"Logistic Regression\":","b63e74ce":".. and this is the conversion in a floating point variable:","048717eb":"### Dummy Variables","6f8e5a62":"Another backup in the filesystem:","34ebdfac":"First of all, we'll work on a backup copy of the cleaned dataframe (just to be protected in case of wrong updates, or to have a checkpoint to restart from):","4f5d40c7":"Embarked is a categorial variable so we cannot calculate its mean. But we can find the frequencies of the distinct values and that is exactly what we are going to do:","4f26cbdf":"We save it into file, too.","c2bb24a5":"The **test set above defined** is the one to use to do the **submission to Kaggle**. We haven't a Survived values for this one (calculating it is our task!). We have, instead, the real Survived value for all the train set. So we'll split it into two further dataframes: 90% for the training and 10% for the validation. The Scikit-Learn package offers us an affordable function which we now import and use:","cc972460":"Probably \"Master\" is the older male son in the richest families. \n\nThe following are considered synonims of noble people:","3f553b04":"So we have about 38% if survived. It's useful concatenating the two dataframes, train and test. In the test part wel'll add a Survived column, set to -1. We give the command reindex to preserve the columns sort between the first and the second dataframe.","c31a7aa0":"We choose to ignore ticket and focus only on the Cabin column, which has these distinc values:","6472cc9c":"Now let's treat the various columns\/variables one by one, starting from Age.","0f56cf1b":"### Final Fitting and Submission","a182e74a":"Other translations (from different classifications and\/or languages):","baf9e07b":"Let's split df into two new sets, one for the train and one for the validation. Why now, after a previous join of the same set? We previously concatenated them to be sure to have applied to both the **same transformation**.","8bcb4fb6":"Previously we filled the missing cells of the Age column with a unique default\/mean value. This is ok, but we can also go beyond filling the empty spaces with values depending on other higly related cells. So let's try to find what are the most important variables for Age.\n\nProbably Pclass and Sex (both categorial can play a role), so we'll use for the exploration the boxplot diagram, after the conversion in numeric of Age, which is now a string variable.","8cd0b516":"The prediction is different int 35 records out of 418. In the leatherboard we reach the score of 77.5%. We improve of about 3000 positions. Other suggestions to improve further:\n* more sophisticated and\/or optimtimizable classificators (such as Random Forest, Xgboost...)\n* better use of Ticket and Cabin columns\n* more intense featuring engineering\n* ...","81c01b10":"Now we have to deal with some remaining \"strange\" cases. Let's see them one by one.","e18e6517":"The output variable (Survived) is a binary variable. It doesn't make sense using commands like df_train0['Survived'].describe(), .skew(), .kurt() which would be very useful if it was a continue one. In this case, the best point to start from is an histogram or, even more simply, a count of the unique values and their occurrences.","e2cc1f06":"Now it's the turn of Cabin and Ticket which have too many distinct values to convert them in dummy varables. This is our dataset:","e77c1c5e":"To sum up we can say that different Sexes don't imply too different Ages. On the contrary, PClass and Age can be considered as highly related. Therefore we'll assign default values not as constants but as functions of the PClass variable. We can do that using a simple function like this:","0451b7a9":"And now, the concatenation:","cb7a5281":"Now let's proceed with the ML optimization. At first, we delete some useless columns:","1340cc9b":"Preliminary Operations\n------","f9d2c7b0":"Titanic for dummies (Pandas,Logistic Regression)\n=======","e7fce2ce":"We can simpy assing the 'S' to the missing values.","c591dcd7":"The idea is to delete nulls and then take only the first char, mainly to cut off multiple tickets:","3efb269a":"... and then extract the first letter of that string variable:","358c846d":"### Test\/Train separation and first training","ba5876bc":"This kernel is sort of first step beyond the \"**gender submission**\" and a **first approch** to **Pyhton** and **ML**.  It introduces the most simple classificator, **logistic regression**. \n\nThe notebook is dedicated to any beginner but especially to my students at [MaCSIS](http:\/\/http:\/\/www.colpodiscienza.it\/). I am **not** a data scientist, they aren't too, but we are **discovering together** the fantastic world of **data science** and it's very exciting!","2ea57326":"Dummy variable, i.e. one column with binary values for all the different values of the source column. We use drop_first just because, for example in the Sex case, if we keep two column Male=0\/1 and Female=0\/1, we introduce a relationship in the dataset (Male=Not(Female)) which can crash our previsional model (based on the hypotesis of indipendent input variables).","21a32a7a":"First Modeling\n-----","010cb9b2":"To sum up this is the head of our dataset, after data preparation:","c4dc7d89":"This is the command:"}}