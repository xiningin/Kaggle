{"cell_type":{"f1d3bb05":"code","fb48dd5e":"code","1145a1b7":"code","ad265a3f":"code","c84eca2b":"code","b7510311":"code","6487320f":"code","3d1d9c03":"code","d797c04b":"code","6305ac51":"code","d9c95b31":"code","092a5f77":"code","e6f55ac7":"code","6c257aa6":"code","d8fb6d3e":"code","d73c2e4f":"code","609b3a24":"code","36a1d676":"code","ebb8fb19":"code","2872e501":"code","aae1fd4d":"code","6035387e":"code","717ca961":"code","9c161eac":"code","2a3c1a61":"code","85ca18fd":"code","295ea0f7":"code","2f81815e":"code","c7aefbff":"code","df9fc295":"code","4700c05a":"markdown","c2e8968b":"markdown","ff814a2f":"markdown","16c96fbc":"markdown"},"source":{"f1d3bb05":"import gc\nimport os\nimport random\n\nimport lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport itertools\n\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, GroupKFold\nfrom sklearn.cluster import KMeans\n\nsns.set(style='darkgrid')\nSEEDS = 42","fb48dd5e":"def rmse(y_true, y_pred):\n    return (mean_squared_error(y_true, y_pred))** .5","1145a1b7":"# treemodel_wrapper\nclass TreeModel:\n    def __init__(self, model_type):\n        self.model_type = model_type\n        self.tr_data = None\n        self.vl_data = None\n        self.model = None\n    \n    def train(self, params, train_x, train_y, valid_x=None, valid_y=None, num_round=None, early_stopping=None, verbose=None):\n        if self.model_type == 'lgb':\n            self.tr_data = lgb.Dataset(train_x, label=train_y)\n            self.vl_data = lgb.Dataset(valid_x, label=valid_y)\n            self.model = lgb.train(params, self.tr_data, valid_sets=[self.tr_data, self.vl_data],\n                                   num_boost_round=num_round, early_stopping_rounds=early_stopping,verbose_eval=verbose)\n            \n        if self.model_type == 'rf_reg':\n            self.train_x = train_x\n            self.train_y = train_y\n            self.model = RandomForestRegressor(**params).fit(self.train_x, self.train_y)\n            \n        if self.model_type == 'xgb':\n            self.tr_data = xgb.DMatrix(train_x, train_y)\n            self.vl_data = xgb.DMatrix(valid_x, valid_y)\n            self.model = xgb.train(params, self.tr_data, num_boost_round=num_round,\n                                   evals=[(self.tr_data, 'train'), (self.vl_data, 'val')], \n                                   verbose_eval=verbose, early_stopping_rounds=early_stopping)\n            \n        if self.model_type == 'cat':\n            params['num_boost_round'] = num_round\n            self.cat_cols = list(train_x.select_dtypes(include='object').columns)\n            self.tr_data = Pool(train_x, train_y, cat_features=self.cat_cols)\n            self.vl_data = Pool(valid_x, valid_y, cat_features=self.cat_cols)\n            self.model = CatBoost(params).fit(self.tr_data, eval_set=self.vl_data,\n                                                early_stopping_rounds=early_stopping, verbose=verbose, use_best_model=True)\n            \n            return self.model\n            \n    \n    def predict(self,X):\n        if self.model_type == 'lgb':\n            return self.model.predict(X, num_iteration=self.model.best_iteration)\n        \n        if self.model_type == 'rf_reg':\n            return self.model.predict(X)\n        \n        if self.model_type == 'xgb':\n            X_DM = xgb.DMatrix(X)\n            return self.model.predict(X_DM)\n        \n        if self.model_type == 'cat':\n            X_pool = Pool(X, cat_features=self.cat_cols)\n            return self.model.predict(X_pool)\n    \n    @property\n    def feature_names_(self):\n        if self.model_type == 'lgb':\n            return self.model.feature_name()\n        \n        if self.model_type == 'rf_reg':\n            return self.train_x.columns\n        \n        if self.model_type == 'xgb':\n            return list(self.model.get_score(importance_type='gain').keys())\n        \n        if self.model_type == 'cat':\n            return self.model.feature_names_\n    \n    @property\n    def feature_importances_(self):\n        if self.model_type == 'lgb':\n            return self.model.feature_importance(importance_type='gain')\n        \n        if self.model_type == 'rf_reg':\n            return self.model.feature_importances_\n        \n        if self.model_type == 'xgb':\n            return list(self.model.get_score(importance_type='gain').values())\n        \n        if self.model_type == 'cat':\n            return self.model.feature_importances_","ad265a3f":"PATH = '..\/input\/stanford-covid-vaccine\/'\ntrain = pd.read_json(PATH+'train.json',lines=True)\ntest = pd.read_json(PATH+'test.json', lines=True)\nsubmission = pd.read_csv(PATH+'sample_submission.csv')","c84eca2b":"train[train['signal_to_noise'] > 1].shape","b7510311":"train[train['SN_filter'] == 1].shape","6487320f":"train[train['SN_filter'] == 1].head(5)","3d1d9c03":"train = train[train['SN_filter'] == 1] \ntrain.shape","d797c04b":"print(train.sequence.values)","6305ac51":"#Additional features\n#Basic ideas is that GC (or CG) is strongest pair, AU (or UA) is weaker and GU(or UG) is the weakest. hence counting the pairs occurance and adding\n#as a feature will provide better signals to LGBM\ntrain['GCcount1']=train['sequence'].map(lambda x: x.count('GC'))\ntrain['GCcount2']=train['sequence'].map(lambda x: x.count('CG'))\n\ntrain['AUcount1']=train['sequence'].map(lambda x: x.count('AU'))\ntrain['AUcount2']=train['sequence'].map(lambda x: x.count('UA'))\n\ntrain['GUcount1']=train['sequence'].map(lambda x: x.count('GU'))\ntrain['GUcount2']=train['sequence'].map(lambda x: x.count('UG'))\n\ntrain['GCcount']=train['GCcount1']+train['GCcount2']\ntrain['AUcount']=train['AUcount1']+train['AUcount2']\ntrain['GUcount']=train['GUcount1']+train['GUcount2']","d9c95b31":"train = train.drop(['GCcount1','GCcount2','AUcount1','AUcount2','GUcount1','GUcount2'], axis=1)\ntrain.head(3)","092a5f77":"test.columns\n","e6f55ac7":"train_data = []\nfor mol_id in train['id'].unique():\n    sample_data = train.loc[train['id'] == mol_id]\n    sample_seq_length = sample_data.seq_length.values[0]\n    \n    for i in range(68):\n        sample_dict = {'id' : sample_data['id'].values[0],\n                       'id_seqpos' : sample_data['id'].values[0] + '_' + str(i),\n                       'sequence' : sample_data['sequence'].values[0][i],\n                       'structure' : sample_data['structure'].values[0][i],\n                       'predicted_loop_type' : sample_data['predicted_loop_type'].values[0][i],\n                       'reactivity' : sample_data['reactivity'].values[0][i],\n                       'reactivity_error' : sample_data['reactivity_error'].values[0][i],\n                       'deg_Mg_pH10' : sample_data['deg_Mg_pH10'].values[0][i],\n                       'deg_error_Mg_pH10' : sample_data['deg_error_Mg_pH10'].values[0][i],\n                       'deg_pH10' : sample_data['deg_pH10'].values[0][i],\n                       'deg_error_pH10' : sample_data['deg_error_pH10'].values[0][i],\n                       'deg_Mg_50C' : sample_data['deg_Mg_50C'].values[0][i],\n                       'deg_error_Mg_50C' : sample_data['deg_error_Mg_50C'].values[0][i],\n                       'deg_50C' : sample_data['deg_50C'].values[0][i],\n                       'deg_error_50C' : sample_data['deg_error_50C'].values[0][i],\n                       'GCcount':sample_data['GCcount'].values[0],\n                       'AUCcount':sample_data['AUcount'].values[0],\n                       'GUcount':sample_data['GUcount'].values[0]}\n        \n        \n        shifts = [1,2,3,4,5]\n        shift_cols = ['sequence', 'structure', 'predicted_loop_type']\n        #shift_cols = ['sequence']\n        for shift,col in itertools.product(shifts, shift_cols):\n            if i - shift >= 0:\n                sample_dict['b'+str(shift)+'_'+col] = sample_data[col].values[0][i-shift]\n            else:\n                sample_dict['b'+str(shift)+'_'+col] = -1\n            \n            if i + shift <= sample_seq_length - 1:\n                sample_dict['a'+str(shift)+'_'+col] = sample_data[col].values[0][i+shift]\n            else:\n                sample_dict['a'+str(shift)+'_'+col] = -1\n        \n        \n        train_data.append(sample_dict)\ntrain_data = pd.DataFrame(train_data)\ntrain_data.head()","6c257aa6":"#Additional features\n#Basic ideas is that GC (or CG) is strongest pair, AU (or UA) is weaker and GU(or UG) is the weakest. hence counting the pairs occurance and adding\n#as a feature will provide better signals to LGBM\ntest['GCcount1']=test['sequence'].map(lambda x: x.count('GC'))\ntest['GCcount2']=test['sequence'].map(lambda x: x.count('CG'))\n\ntest['AUcount1']=test['sequence'].map(lambda x: x.count('AU'))\ntest['AUcount2']=test['sequence'].map(lambda x: x.count('UA'))\n\ntest['GUcount1']=test['sequence'].map(lambda x: x.count('GU'))\ntest['GUcount2']=test['sequence'].map(lambda x: x.count('UG'))\n\ntest['GCcount']=test['GCcount1']+test['GCcount2']\ntest['AUcount']=test['AUcount1']+test['AUcount2']\ntest['GUcount']=test['GUcount1']+test['GUcount2']\n\ntest = test.drop(['GCcount1','GCcount2','AUcount1','AUcount2','GUcount1','GUcount2'], axis=1)\ntest.head(3)","d8fb6d3e":"test_data = []\nfor mol_id in test['id'].unique():\n    sample_data = test.loc[test['id'] == mol_id]\n    sample_seq_length = sample_data.seq_length.values[0]\n    for i in range(sample_seq_length):\n        sample_dict = {'id' : sample_data['id'].values[0],\n                       'id_seqpos' : sample_data['id'].values[0] + '_' + str(i),\n                       'sequence' : sample_data['sequence'].values[0][i],\n                       'structure' : sample_data['structure'].values[0][i],\n                       'predicted_loop_type' : sample_data['predicted_loop_type'].values[0][i],\n                       'GCcount':sample_data['GCcount'].values[0],\n                       'AUCcount':sample_data['AUcount'].values[0],\n                       'GUcount':sample_data['GUcount'].values[0]}\n        \n        \n        shifts = [1,2,3,4,5]\n        shift_cols = ['sequence', 'structure', 'predicted_loop_type']\n        #shift_cols = ['sequence']\n        for shift,col in itertools.product(shifts, shift_cols):\n            if i - shift >= 0:\n                sample_dict['b'+str(shift)+'_'+col] = sample_data[col].values[0][i-shift]\n            else:\n                sample_dict['b'+str(shift)+'_'+col] = -1\n            \n            if i + shift <= sample_seq_length - 1:\n                sample_dict['a'+str(shift)+'_'+col] = sample_data[col].values[0][i+shift]\n            else:\n                sample_dict['a'+str(shift)+'_'+col] = -1\n        \n        test_data.append(sample_dict)\ntest_data = pd.DataFrame(test_data)\ntest_data.head()","d73c2e4f":"# label_encoding\nsequence_encmap = {'A': 0, 'G' : 1, 'C' : 2, 'U' : 3}\nstructure_encmap = {'.' : 0, '(' : 1, ')' : 2}\nlooptype_encmap = {'S':0, 'E':1, 'H':2, 'I':3, 'X':4, 'M':5, 'B':6}\n\nenc_targets = ['sequence', 'structure', 'predicted_loop_type']\nenc_maps = [sequence_encmap, structure_encmap, looptype_encmap]\n\nfor t,m in zip(enc_targets, enc_maps):\n    for c in [c for c in train_data.columns if t in c]:\n        train_data[c] = train_data[c].astype(str).replace(m)\n        test_data[c] = test_data[c].astype(str).replace(m)","609b3a24":"print(train_data.shape)\nprint(train_data.dtypes)\ntrain_data.head(3)","36a1d676":"print(test_data.shape)\ntest_data.head(3)","ebb8fb19":"not_use_cols = ['id', 'id_seqpos']\nfeatures = [c for c in test_data.columns if c not in not_use_cols]\ntargets = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']\n\n    \n","2872e501":"for f in features:\n    if test_data[f].dtype == 'object':\n        train_data[f]= train_data[f].astype(str).astype(int)\n        test_data[f]= test_data[f].astype(str).astype(int)\nlen(features)","aae1fd4d":"FOLD_N = 5\ngkf = GroupKFold(n_splits=FOLD_N)","6035387e":"\"\"\"\nparams = {'objective': 'regression',\n          'boosting': 'gbdt',\n          'metric': 'rmse',\n          'learning_rate': 0.01,\n          'seed' : SEEDS}\n\"\"\"\nn_fold =5\nparams ={'boosting': 'gbdt',\n        'objective': 'regression',\n        'metric': 'rmse',\n        'learning_rate': 0.04,\n        'subsample': 0.7,\n        'max_depth': -1,\n        'num_leaves': 81,\n        'colsample_bytree': 0.8,\n        'verbose': -1,\n        'seed':int(2**n_fold),\n        'bagging_seed':int(2**n_fold)\n        }","717ca961":"feature_importances = pd.DataFrame()\nresult = {}\noof_df = pd.DataFrame(train_data.id_seqpos)\n\nfor target in targets:\n    oof = pd.DataFrame()\n    preds = np.zeros(len(test_data))\n    scores = 0.0\n    \n    for n, (tr_idx, vl_idx) in enumerate(gkf.split(train_data[features], train_data['reactivity'], train_data['id'])):\n        tr_x, tr_y = train_data[features].iloc[tr_idx], train_data[target].iloc[tr_idx]\n        vl_x, vl_y = train_data[features].iloc[vl_idx], train_data[target].iloc[vl_idx]\n        vl_id = train_data['id_seqpos'].iloc[vl_idx]\n\n        model = TreeModel(model_type='lgb')\n        model.train(params, tr_x, tr_y, vl_x, vl_y,\n                    num_round=20000, early_stopping=100,verbose=1000)\n\n        fi_tmp = pd.DataFrame()\n        fi_tmp['feature'] = model.feature_names_\n        fi_tmp['importance'] = model.feature_importances_\n        fi_tmp['fold'] = n\n        fi_tmp['target'] = target\n        feature_importances = feature_importances.append(fi_tmp)\n\n        vl_pred = model.predict(vl_x)\n        score = rmse(vl_y, vl_pred)\n        scores += score \/ FOLD_N\n        print(f'score : {score}')\n\n        oof = oof.append(pd.DataFrame({'id_seqpos':vl_id, target:vl_pred}))\n\n        pred = model.predict(test_data[features])\n        preds += pred \/ FOLD_N\n    \n    oof_df = oof_df.merge(oof, on='id_seqpos', how='inner')\n    submission[target] = preds\n    \n    print(f'{target}_rmse : {scores}')\n    result[target] = scores","9c161eac":"display(result)\ndisplay(f'total : {np.mean(list(result.values()))}')","2a3c1a61":"# feature_importances\nfor target in targets:\n    tmp = feature_importances[feature_importances.target==target]\n    order = list(tmp.groupby('feature').mean().sort_values('importance', ascending=False).index)\n\n    plt.figure(figsize=(10, 5))\n    sns.barplot(x=\"importance\", y=\"feature\", data=tmp, order=order)\n    plt.title(target)\n    plt.tight_layout()","85ca18fd":"oof_df.head()","295ea0f7":"submission.head()","2f81815e":"display(oof_df.shape)\ndisplay(submission.shape)","c7aefbff":"oof_df.to_csv('oof_df.csv', index=False)\nsubmission.to_csv('submission_lgb.csv', index=False)","df9fc295":"sub_col= submission.columns\n","4700c05a":"#Attribution: great notebook from T88, which is used  (https:\/\/www.kaggle.com\/t88take\/openvaccine-simple-lgb-baseline)\n\n9\/21:changes made:\n1) Did hyper parameter tuning \n2) Added pair featuresnew features","c2e8968b":"# preprocess","ff814a2f":"# train & predict","16c96fbc":"# load data"}}