{"cell_type":{"db77bf3b":"code","60de24d7":"code","57c135db":"code","b56e5bd9":"code","8e3809ad":"code","90a527b6":"code","3d9f509f":"code","0ce91ab1":"code","b7f87f45":"code","8d6add6b":"code","6016cb10":"code","3c05d298":"code","964fc325":"code","c6d9a450":"code","7bb396c4":"code","d5ca8608":"code","c345b89e":"code","fa1a8986":"code","78db69d0":"code","f72d3f81":"code","5c7733dd":"code","e2e300b6":"code","afec7f29":"code","b66fb95f":"code","1c32189b":"code","5de24756":"code","ca140802":"code","359cc506":"code","c0a7437c":"code","cf5f15f2":"code","5ac4be11":"code","24e23ee2":"code","8e982641":"code","266a4d09":"code","7556b04d":"code","ee14dc8c":"code","c5fafb78":"code","985523c2":"code","80a1a2bf":"code","b15a0371":"code","18fa622f":"code","9a4d2a96":"code","4fd73eb5":"code","ff5989f1":"code","090f5f55":"code","f0dc094b":"code","34355e26":"code","f396328f":"code","7f836e90":"code","38c81790":"code","e22a8830":"code","9ac7bc26":"code","1f1dfb35":"code","93b56539":"markdown","81ffcabe":"markdown","988ddc59":"markdown","f3767fa0":"markdown","b083c7ec":"markdown","26b9419e":"markdown","2d15d5eb":"markdown","8265bff1":"markdown","432be115":"markdown","8693bf63":"markdown","683f42f6":"markdown","c905648e":"markdown","d29e66c3":"markdown","06cfa4b4":"markdown","d24a65e0":"markdown","8f039a48":"markdown","ff889128":"markdown","3680c038":"markdown","7561c4a4":"markdown","96c06f0a":"markdown","ee35f4b8":"markdown","7d1530ff":"markdown","1516d60d":"markdown","ea36d8c3":"markdown","c2d0dcf2":"markdown","ee844df7":"markdown","bb867881":"markdown","1d4dc256":"markdown","0a1a4c46":"markdown","5aa3aa3c":"markdown","197bf8d6":"markdown","d1d74f6e":"markdown","3dc35784":"markdown","748ac258":"markdown","068d7b6f":"markdown","8c477cbf":"markdown","63b9afec":"markdown","03acc7b2":"markdown","bd30edd8":"markdown","8dd22199":"markdown","df14bcc5":"markdown"},"source":{"db77bf3b":"import re\nimport sys\nimport math\nimport string\nimport zipfile\nimport unicodedata\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom collections import Counter\nfrom gensim.models import KeyedVectors\nfrom sklearn import metrics\nfrom sklearn.metrics import f1_score, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom wordcloud import WordCloud\nimport nltk\nfrom nltk.corpus import stopwords\nfrom tqdm import tqdm\nfrom tqdm.notebook import tqdm_notebook\nimport gc\n\ntqdm_notebook.pandas()","60de24d7":"## C\u1ea5u tr\u00fac th\u01b0 m\u1ee5c\n!ls ..\/input\/quora-insincere-questions-classification\/","57c135db":"# Load d\u1eef li\u1ec7u\ntrain_df = pd.read_csv('..\/input\/quora-insincere-questions-classification\/train.csv')\ntest_df = pd.read_csv('..\/input\/quora-insincere-questions-classification\/test.csv')","b56e5bd9":"## T\u1ed5ng quan d\u1eef li\u1ec7u train\ntrain_df.head()","8e3809ad":"train_df.info()","90a527b6":"# T\u1ed5ng quan d\u1eef li\u1ec7u test\ntest_df.head()","3d9f509f":"test_df.info()","0ce91ab1":"## Bi\u1ec3u \u0111\u1ed3 bar plot\ntrain_df['target'].value_counts().plot(kind='bar')","b7f87f45":"# Pie Chart\nplt.subplot(1, 2, 2)\nvalues = [train_df[train_df['target']==0].shape[0], train_df[train_df['target']==1].shape[0]]\nlabels = ['Sincere questions', 'Insincere questions']\n\nplt.pie(values, labels=labels, autopct='%1.1f%%', shadow=True)\nplt.title('Target Distribution')\nplt.tight_layout()\nplt.subplots_adjust(right=1.9)\nplt.show()","8d6add6b":"# Stopword t\u1eeb nltk\nstop_words = set(stopwords.words('english'))\n\n# T\u1ea1o t\u1eeb \u0111i\u1ec3n t\u1ea7n su\u1ea5t\ndef word_freq_dict(text):\n    # Convert text into word list\n    wordList = text.split()\n    # Generate word freq dictionary\n    vocab = Counter(wordList) \n    freq_dict = dict(vocab.most_common())\n    return freq_dict\n    \n# Plot a wordcloud from a word frequency dictionary\ndef word_cloud_frequency(word_freq_dict, title, figure_size=(10,6)):\n    for w in stop_words:\n        if w in word_freq_dict:\n            word_freq_dict.pop(w)\n    wordcloud.generate_from_frequencies(word_freq_dict)\n    plt.figure(figsize=figure_size)\n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.title(title)\n    plt.show()","6016cb10":"# Wordcloud of insincere questions\ninsincere_questions = train_df.question_text[train_df['target'] == 1]\ninsincere_sample = \" \".join(insincere_questions)\ninsincere_word_freq = word_freq_dict(insincere_sample)\ninsincere_word_freq = dict(list(insincere_word_freq.items()))\nwordcloud = WordCloud(width= 5000,\n    height=3000,\n    max_words=200,\n    colormap='Reds',\n    background_color='white')\n\nword_cloud_frequency(insincere_word_freq, \"Most Frequent Words insincere\")","3c05d298":"# Wordcloud of insincere questions\ninsincere_questions = train_df.question_text[train_df['target'] == 0]\ninsincere_sample = \" \".join(insincere_questions)\ninsincere_word_freq = word_freq_dict(insincere_sample)\ninsincere_word_freq = dict(list(insincere_word_freq.items()))\nwordcloud = WordCloud(width= 5000,\n    height=3000,\n    max_words=200,\n    colormap='Greens',\n    background_color='white')\n\nword_cloud_frequency(insincere_word_freq, \"Most Frequent Words sincere\")","964fc325":"# Gi\u1ea3i n\u00e9n v\u00e0 load embeddings\nzip_file = zipfile.ZipFile('\/kaggle\/input\/quora-insincere-questions-classification\/embeddings.zip', 'r')\npath=zip_file.open('GoogleNews-vectors-negative300\/GoogleNews-vectors-negative300.bin', 'r')\n\nembeddings_index = KeyedVectors.load_word2vec_format(path, binary=True)","c6d9a450":"# H\u00e0m ki\u1ec3m tra \u0111\u1ed9 bao ph\u1ee7 embedding\ndef check_coverage(vocab, embeddings_index):\n    embeddings_vocab = 0\n    embeddings_text = 0\n    oov_text = 0\n    oov = Counter()\n    \n    for word in vocab:\n        if word in embeddings_index:\n            embeddings_vocab += 1\n            embeddings_text += vocab[word]    \n        else:\n            oov[word] = vocab[word]\n            oov_text += vocab[word]\n\n    print('Embeddings coverage for {:.2%} of vocab'.format(embeddings_vocab \/ len(vocab)))\n    print('Embeddings coverage for {:.2%} of all text'.format(embeddings_text \/ (embeddings_text + oov_text)))\n    \n    return oov","7bb396c4":"## Ki\u1ec3m tra \u0111\u1ed9 bao ph\u1ee7\ndata = \" \".join(train_df.question_text)\nvocab = word_freq_dict(data)\noov = check_coverage(vocab, embeddings_index)","d5ca8608":"## Hi\u1ec3n th\u1ecb 20 t\u1eeb OOV\nprint('Out of vocab')\noov.most_common(20)","c345b89e":"# X\u00e2y d\u1ef1ng t\u1eadp punctuations\npunctuation = [chr(i) for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('P')]\nfor punct in string.punctuation:\n    if punct not in punctuation:\n        punctuation.append(punct)\npunctuation_in_embeddings = [punct for punct in punctuation if punct in embeddings_index]\npunctuation_not_in_embeddings = [punct for punct in punctuation if punct not in embeddings_index]\n\n# Lo\u1ea1i b\u1ecf punctuations kh\u1ecfi text\ndef remove_punct(sen):\n    for punct in punctuation_not_in_embeddings:\n        sen = sen.replace(punct, ' ')\n    for punct in punctuation_in_embeddings:\n        sen = sen.replace(punct, f' {punct} ')\n    return sen","fa1a8986":"# Check \u0111\u1ed9 bao ph\u1ee7\nsentens = train_df['question_text'].progress_apply(lambda x: remove_punct(x))\ndata = \" \".join(sentens)\nvocab = word_freq_dict(data)\noov = check_coverage(vocab, embeddings_index)\nprint('Out of vocab')\noov.most_common(20)","78db69d0":"## Ki\u1ec3m tra top 10 t\u1eeb xu\u1ea5t hi\u1ec7n nhi\u1ec1u nh\u1ea5t\nfor i in range(10):\n    print(embeddings_index.index_to_key[i])","f72d3f81":"## Ki\u1ec3m tra k\u00ed t\u1ef1 s\u1ed1 trong t\u1eadp embedding\nprint('1' in embeddings_index)\nprint('22' in embeddings_index)","5c7733dd":"# H\u00e0m chu\u1ea9n ho\u00e1 k\u00ed t\u1ef1 s\u1ed1\ndef standard_numbers(x):\n    ## S\u1eed d\u1ee5ng regex\n    x = re.sub('[0-9]{5,}', '#####', x)\n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    return x","e2e300b6":"# Check \u0111\u1ed9 bao ph\u1ee7\n\nsentens = sentens.progress_apply(lambda x: standard_numbers(x))\ndata = \" \".join(sentens)\nvocab = word_freq_dict(data)\noov = check_coverage(vocab, embeddings_index)\nprint('Out of vocab')\noov.most_common(20)","afec7f29":"# T\u1eeb \u0111i\u1ec3n mispell\nmispell_dict = {\n    'grey': 'gray',\n    'litre': 'liter',\n    'labour': 'labor',\n    'travelling':'traveling',\n    'favour': 'favor',\n    'colour': 'color',\n    'centre': 'center',\n    'honours': 'honor',\n    'theatre': 'theater',\n    'realise': 'realize',\n    'defence': 'defense',\n    'licence': 'license',\n    'analyse': 'analyze',\n    'practise': 'practice',\n    'behaviour': 'behavior',\n    'neighbour': 'neighbor',\n    'recognise': 'recognize',\n    'organisation':'organization',  \n    'Qoura': 'Quora',\n    'quora': 'Quora',\n    'Quorans': 'Quoran',\n    'infty': 'infinity',\n    'judgement': 'judge',   \n    'isnt': 'is not',\n    'didnt': 'did not',\n    'Whatis': 'what is',\n    'doesnt': 'does not',  \n    'learnt': 'learn',\n    'modelling': 'model',\n    'cancelled': 'cancel',\n    'travelled': 'travell',\n    'travelling': 'travel',\n    'aluminium': 'alumini',\n    'counselling':'counseling',\n    '\u20b9': 'rupee',\n    'Brexit': 'Britain exit',\n    'Paytm': 'Pay Through Mobile',\n    'KVPY': 'Kishore Vaigyanik Protsahan Yojana',\n    'GDPR': 'General Data Protection Regulation',\n    'INTJ': 'Introversion Intuition Thinking Judgment',   \n    'cheque': 'bill',\n    'upvote': 'agree',\n    'upvotes': 'agree',\n    'vape': 'cigarette',\n    'jewellery': 'jewell',\n    'Fiverr': 'freelance',\n    'programd': 'program',\n    'programme': 'program',\n    'programr': 'programer',\n    'programrs': 'programer',\n    'WeChat': 'socialmedia',\n    'Snapchat': 'socialmedia',\n    'Redmi': 'cellphone',\n    'Xiaomi': 'cellphone',\n    'OnePlus': 'cellphone',\n    'cryptos': 'crypto',\n    'bitcoin': 'crypto',\n    'Coinbase': 'crypto',\n    'bitcoins': 'crypto',\n    'ethereum': 'crypto',\n    'Ethereum': 'crypto',\n    'Blockchain': 'crypto',\n    'blockchain': 'crypto',\n    'cryptocurrency': 'crypto',\n    'cryptocurrencies': 'crypto',\n}\n## Chu\u1ea9n ho\u00e1 ng\u1eef ph\u00e1p v\u00e0 t\u1eeb m\u1edbi\ndef correct_spelling(sen):\n    for word in mispell_dict.keys():\n        sen = sen.replace(word, mispell_dict[word])\n    return sen","b66fb95f":"## Xo\u00e1 b\u1ecf a, to, of\ndef replace_useless(sen):\n    text_list = sen.split()\n    text_list = [text for text in text_list if text not in ['a', 'to', 'of', 'and']]\n    return \" \".join(text_list)","1c32189b":"# Check \u0111\u1ed9 bao ph\u1ee7\n\nsentens = sentens.progress_apply(lambda x: correct_spelling(x))\nsentens = sentens.progress_apply(lambda x: replace_useless(x))\n\ndata = \" \".join(sentens)\nvocab = word_freq_dict(data)\noov = check_coverage(vocab, embeddings_index)\nprint('Out of vocab')\noov.most_common(20)","5de24756":"## T\u1ed5ng h\u1ee3p l\u1ea1i clean data\ndef data_cleaning(sen):\n  sen = remove_punct(sen)\n  sen = standard_numbers(sen)\n  sen = correct_spelling(sen)\n  sen = replace_useless(sen)\n\n  return sen","ca140802":"## Clean d\u1eef li\u1ec7u\ntrain_df['question_text_cleaned'] = train_df['question_text'].progress_apply(lambda x: data_cleaning(x))\ntest_df['question_text_cleaned'] = test_df['question_text'].progress_apply(lambda x: data_cleaning(x))","359cc506":"## Xo\u00e1 c\u00e1c bi\u1ebfn kh\u00f4ng d\u00f9ng \u0111\u1ebfn\ndel oov, sentens\ngc.collect()","c0a7437c":"## Chia t\u1eadp d\u1eef li\u1ec7u\ntrain_df, val_df = train_test_split(train_df, test_size = 0.1, stratify = train_df['target'], random_state = 42)","cf5f15f2":"## Hi\u1ec3n th\u1ecb ph\u00e2n b\u1ed1 nh\u00e3n trong hai t\u1eadp\nprint(train_df.shape)\nprint(train_df['target'].value_counts())\nprint(val_df.shape)\nprint(val_df['target'].value_counts())","5ac4be11":"SEQ_LEN = 30\nbatch_size = 128\nEMB_SIZE = 300\n\n# H\u00e0m chuy\u1ec3n text th\u00e0nh vector embedding\ndef text_to_array(sen):\n    empyt_emb = np.zeros(EMB_SIZE)\n    sen = sen[:-1].split()[:SEQ_LEN]\n    embeds = [embeddings_index[x] for x in sen if x in embeddings_index]\n    embeds+= [empyt_emb] * (SEQ_LEN - len(embeds))\n    return np.array(embeds)\n\n# H\u00e0m sinh d\u1eef li\u1ec7u\ndef batch_train_gen(train_df):\n    n_batches = math.ceil(len(train_df) \/ batch_size)\n    while True: \n        train_df = train_df.sample(frac=1.) \n        for i in range(n_batches):\n            texts = train_df.iloc[i * batch_size: (i + 1) * batch_size, -1]\n            text_arr = np.array([text_to_array(text) for text in texts])\n            yield text_arr, np.array(train_df[\"target\"][i * batch_size:(i + 1) * batch_size])","24e23ee2":"# train_gen = batch_train_gen(train_df)\n# val_gen = batch_val_gen(val_df)","8e982641":"## Khai b\u00e1o h\u00e0m sinh d\u1eef li\u1ec7u\ntrain_gen = batch_train_gen(train_df)\nval_size = 10000\nval_vects = np.array([text_to_array(X_text) for X_text in tqdm(val_df[\"question_text_cleaned\"][:val_size],position=0)], dtype=float)\nval_y = np.array(val_df[\"target\"][:val_size], dtype='int32')\nvalidation_data=(val_vects, val_y)","266a4d09":"## Ki\u1ec3m tra \u0111\u1ecbnh d\u1ea1ng\nnext(iter(train_gen))[0].shape","7556b04d":"from keras.models import Sequential,Model\nfrom keras.layers import CuDNNLSTM, Dense, Bidirectional, Input,Dropout, LSTM\nfrom keras import backend as K\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau","ee14dc8c":"# H\u00e0m hi\u1ec3n th\u1ecb history\ndef plot_his(history):\n    # plot history for loss\n    plt.figure(figsize=(15, 5)).add_subplot(1, 3, 1)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'val'], loc='upper left')\n\n    # plot history for accuracy\n    plt.figure(figsize=(15, 5)).add_subplot(1, 3, 2)\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'val'], loc='upper left')\n    plt.show()","c5fafb78":"# S\u1eed d\u1ee5ng sequential \u0111\u1ec3 x\u00e2y d\u1ef1ng\nlstm_model = Sequential()\n\n# Th\u00eam Lstm v\u1edbi unit l\u00e0 128\nlstm_model.add(LSTM(units=128))\n\n# Th\u00eam fullyconnected layer c\u00f9ng v\u1edbi sigmoid \u0111\u1ec3 d\u1ef1 \u0111o\u00e1n \nlstm_model.add(Dense(1, activation=\"sigmoid\"))\n\n# Compile neural network\nlstm_model.compile(loss='binary_crossentropy', # Cross-entropy\n              optimizer='adam', # Adam optimization\n              metrics=['accuracy']) # Accuracy performance metric\n","985523c2":"filepath = 'Lstm-{epoch:02d}-{val_loss:.4f}.hdf5'\n## Gi\u1ea3m lr\nreduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.20, min_delta=1e-15 ,patience=2, \n                                   verbose=1, mode='min')\n## L\u01b0u m\u00f4 h\u00ecnh \n# checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n## Early stop\nearlystopper = EarlyStopping(monitor='val_loss', mode='min', patience=3, verbose=1, restore_best_weights=True)\ncallbacks = [reduceLROnPlat, earlystopper]","80a1a2bf":"lstm_history = lstm_model.fit(train_gen,\n                    epochs=30,\n                    steps_per_epoch=1000, \n                    validation_data=validation_data,\n                    callbacks=callbacks,\n                    verbose=True)","b15a0371":"## Hi\u1ec3n th\u1ecb history\nplot_his(lstm_history)","18fa622f":"## X\u00e2y d\u1ef1ng sequential\nmodel = Sequential()\n## Layer1\nmodel.add(Bidirectional(CuDNNLSTM(64, return_sequences=True),\n                        input_shape=(30, 300)))\n## Layer 2\nmodel.add(Bidirectional(CuDNNLSTM(64)))\nmodel.add(Dense(1, activation=\"sigmoid\"))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","9a4d2a96":"model.summary()","4fd73eb5":"filepath = 'BiLstm-{epoch:02d}-{val_loss:.4f}.hdf5'\nreduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.20, min_delta=1e-15 ,patience=2, \n                                   verbose=1, mode='min')\n# checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\nearlystopper = EarlyStopping(monitor='val_loss', mode='min', patience=3, verbose=1, restore_best_weights=True)\ncallbacks = [reduceLROnPlat, earlystopper]","ff5989f1":"# train_step = math.ceil(len(train_df) \/ batch_size)\n# val_step = math.ceil(len(val_df) \/ batch_size)\n\n# history = model.fit(train_gen, batch_size=batch_size, epochs= 10, steps_per_epoch=train_step, validation_data = val_gen, validation_steps=val_step, verbose=True, callbacks=callbacks)","090f5f55":"history = model.fit(train_gen,\n                    epochs=30,\n                    steps_per_epoch=1000, \n                    validation_data=validation_data,\n                    callbacks=callbacks,\n                    verbose=True)","f0dc094b":"plot_his(history)","34355e26":"pred_val = lstm_model.predict(val_vects, verbose=False)\ndef best_threshold(pred, truth):\n    thresholds = []\n    scores = []\n    for thresh in np.arange(0.1, 0.701, 0.01):\n        thresh = np.round(thresh, 2)\n        thresholds.append(thresh)\n        score = f1_score(truth, (pred>thresh).astype(int))\n        scores.append(score)\n    return np.max(scores), thresholds[np.argmax(scores)]\n\ny_val_truth = np.array(val_df['target'])\nscore_val, threshold_val = best_threshold(pred_val, val_y)\n\n\nprint(\"The best threshold is {0} with f1_score {1} of LSTM\".format(score_val, threshold_val))","f396328f":"pred_val_bi_lstm = model.predict(val_vects, verbose=False)\nscore_val_bi, threshold_val_bi = best_threshold(pred_val_bi_lstm, val_y)\nprint(\"The best threshold is {0} with f1_score {1} of Bi-LSTM model\".format(score_val_bi, threshold_val_bi))","7f836e90":"def batch_test_gen(test_df):\n    n_batches = math.ceil(len(test_df) \/ batch_size)\n    for i in range(n_batches):\n        texts = test_df.iloc[i * batch_size: (i + 1) * batch_size, 1]\n        text_arr = [text_to_array(text) for text in texts]\n        yield np.array(text_arr, dtype=float)","38c81790":"all_preds = []\nfinal_thres = 0\nres = 0\nfor x in tqdm(batch_test_gen(test_df)):\n    if score_val_bi > score_val:\n        res = model.predict(x, verbose=False)\n        final_thres = threshold_val_bi\n    else:\n        res = lstm_model.predict(x, verbose=False)\n        final_thres = threshold_val\n    res = (res > final_thres ).astype(int)\n    for i in res:\n        all_preds.append(i[0])","e22a8830":"print(final_thres)","9ac7bc26":"submit_df = pd.DataFrame({\"qid\": test_df[\"qid\"], \"prediction\": all_preds})\nsubmit_df.head(5)","1f1dfb35":"submit_df.to_csv(\"submission.csv\", index=False)","93b56539":"![](https:\/\/miro.medium.com\/max\/1098\/1*xdvd-PJewtzFF6vP55ObwA.png)\nM\u00f4 h\u00ecnh **bi-lstm** v\u1edbi ph\u1ea7n x\u01b0\u01a1ng s\u1ed1ng v\u1eabn l\u00e0 **lstm**\n\nTuy nhi\u00ean n\u00f3 h\u1ecdc ng\u1eef c\u1ea3nh theo hai chi\u1ec1u (bi-directional).\n\nV\u00ec v\u1eady n\u00f3 s\u1ebd h\u1ecdc ng\u1eef c\u1ea3nh c\u1ee7a t\u1eeb so v\u1edbi c\u00e1c t\u1eeb trong qu\u00e1 kh\u1ee9 v\u00e0 t\u01b0\u01a1ng lai.\n\nTa s\u1ebd x\u00e2y d\u1ef1ng m\u00f4 h\u00ecnh **Bi-LSTM** v\u1edbi 2 layer ch\u1ed3ng l\u00ean nhau.","81ffcabe":"Kh\u1ea3o s\u00e1t v\u1ec1 k\u00ed t\u1ef1 s\u1ed1 ta th\u1ea5y r\u1eb1ng embeddings c\u1ee7a GoogleNews s\u1ebd thay th\u1ebf c\u00e1c s\u1ed1 c\u00f3 hai ch\u1eef s\u1ed1 tr\u1edf l\u00ean b\u1eb1ng k\u00ed **#** <\/br>\nV\u1edbi **22** s\u1ebd l\u00e0 **##**, **333** s\u1ebd l\u00e0 **###** v\u00e0 t\u01b0\u01a1ng t\u1ef1 \n\nV\u00ec v\u1eady ta s\u1ebd \u0111\u1ecbnh ngh\u0129a h\u00e0m chu\u1ea9n ho\u00e1 `standard_numbers` \u0111\u1ec3 chu\u1ea9n ho\u00e1 l\u1ea1i c\u00e1c k\u00ed t\u1ef1 s\u1ed1\n","988ddc59":"**V\u1ea5n \u0111\u1ec1:** \nDo b\u1ed9 d\u1eef li\u1ec7u qu\u00e1 l\u1edbn ta s\u1ebd t\u1ea1o h\u00e0m generator \u0111\u1ec3 sinh d\u1eef li\u1ec7u theo batch. \u0110i\u1ec1u n\u00e0y gi\u00fap cho qu\u00e1 tr\u00ecnh train kh\u00f4ng b\u1ecb t\u1ed1n b\u1ed9 nh\u1edb.\n\n\u1ede m\u1ed7i batch s\u1ebd tr\u1ea3 v\u1ec1 cho model 2 th\u00f4ng tin \u0111\u00f3 l\u00e0 t\u1eadp embedding c\u00e2u v\u00e0 t\u1eadp nh\u00e3n t\u01b0\u01a1ng \u1ee9ng\n\nTa s\u1eed d\u1ee5ng h\u00e0m `text_to_array` \u0111\u1ec3 sinh ra b\u1ed9 ma tr\u1eadn em bedding c\u1ee7a m\u1ed9t batch c\u00e1c c\u00e2u v\u1edbi SEQ_LEN = 30, EMBED_DIM = 300 ch\u00ednh l\u00e0 chi\u1ec1u vector c\u1ee7a **GoogleNews**\n\nH\u00e0m `batch_train_gen` \u0111\u1ec3 sinh c\u00e1c batch v\u1edbi batch-size l\u00e0 128 l\u00e0 h\u00e0m m\u0169 c\u1ee7a hai gi\u00fap t\u1eadn d\u1ee5ng kh\u1ea3 n\u0103ng t\u00ednh to\u00e1n song song","f3767fa0":"## Tr\u1ef1c quan ho\u00e1 d\u1eef li\u1ec7u","b083c7ec":"Ta s\u1ebd x\u00e2y d\u1ef1ng m\u1ed9t list c\u00e1c k\u00ed t\u1ef1 \u0111\u1eb7c bi\u1ec7t v\u00e0 d\u1ea5u t\u00e1ch c\u00e2u t\u1eeb string c\u1ee7a python v\u00e0 sys.\n\n\u0110\u1ed1i v\u1edbi c\u00e1c k\u00ed t\u1ef1 n\u1eb1m trong embedding ta s\u1ebd cho n\u00f3 n\u1eb1m ri\u00eang r\u1ebd ra, c\u00f2n \u0111\u1ed1i v\u1edbi k\u00ed t\u1ef1 kh\u00f4ng n\u1eb1m trong embedding ta s\u1ebd thay th\u1ebf b\u1eb1ng d\u1ea5u c\u00e1ch.\n\nS\u1eed d\u1ee5ng h\u00e0m `remove_punct`","26b9419e":"## X\u1eed l\u00ed ng\u1eef ph\u00e1p v\u00e0 c\u00e1c t\u1eeb m\u1edbi","2d15d5eb":"Ta s\u1ebd s\u1eed d\u1ee5ng **barplot** v\u00e0 **piechart** \u0111\u1ec3 c\u00f3 c\u00e1i nh\u00ecn tr\u1ef1c quan v\u1ec1 ph\u00e2n b\u1ed1 d\u1eef li\u1ec7u.","8265bff1":"# D\u1ef1 \u0111o\u00e1n d\u1eef li\u1ec7u test v\u00e0 submission","432be115":"# Data Generator","8693bf63":"# **B\u00e1o c\u00e1o b\u00e0i t\u1eadp l\u1edbn m\u00f4n H\u1ecdc m\u00e1y**\n**Gi\u1ea3ng vi\u00ean:** Tr\u1ea7n Qu\u1ed1c Long<br>\n**L\u1edbp m\u00f4n h\u1ecdc:** INT3405E_20<br>\n**Sinh vi\u00ean:** T\u1ea7n L\u00ea Ngh\u0129a<br>\n**MSSV:** 18020949<br>","683f42f6":"## Chu\u1ea9n ho\u00e1 k\u00ed t\u1ef1 s\u1ed1","c905648e":"**V\u1ea5n \u0111\u1ec1**: \n\nS\u1ed1 l\u01b0\u1ee3ng c\u00e2u h\u1ecfi thi\u1ebfu ch\u00e2n th\u00e0nh (nh\u00e3n 1) **th\u1ea5p h\u01a1n r\u1ea5t nhi\u1ec1u** so v\u1edbi c\u00e1c c\u00e2u h\u1ecfi ch\u00e2n th\u00e0nh v\u00e0 ch\u1ec9 chi\u1ebfm **6.2%** tr\u00ean to\u00e0n b\u1ed9 d\u1eef li\u1ec7u\n\n=> D\u1eef li\u1ec7u b\u1ecb m\u1ea5t c\u00e2n b\u1eb1ng\n\n\u0110i\u1ec1u n\u00e0y c\u00f3 th\u1ec3 d\u1eabn \u0111\u1ebfn m\u1ed9t s\u1ed1 v\u1ea5n \u0111\u1ec1:\n\nM\u00f4 h\u00ecnh d\u1ef1 \u0111o\u00e1n k\u00e9m: M\u00f4 h\u00ecnh s\u1ebd c\u00f3 thi\u00ean h\u01b0\u1edbng d\u1ef1 \u0111o\u00e1n thi\u00ean l\u1ec7ch v\u1ec1 nh\u00e3n \u0111a s\u1ed1.\n\n\u0110\u00e1nh gi\u00e1 sai ch\u1ea5t l\u01b0\u1ee3ng m\u00f4 h\u00ecnh: Ta s\u1ebd kh\u00f4ng th\u1ec3 \u0111\u00e1nh gi\u00e1 m\u00f4 h\u00ecnh b\u1eb1ng \u0111\u1ed9 \u0111o **accuracy** hay **recall** b\u1edfi m\u00f4 h\u00ecnh c\u00f3 th\u1ec3 d\u1ef1 \u0111o\u00e1n to\u00e0n b\u1ed9 m\u1eabu l\u00e0 0 m\u1eb7c d\u00f9 trong khi \u0111\u00f3 accuracy v\u00e0 recall l\u1ea1i \u0111\u1ea1t gi\u00e1 tr\u1ecb cao.\n\nV\u00ec v\u1eady ta s\u1ebd l\u1ef1a ch\u1ecdn h\u00e0m \u0111\u00e1nh gi\u00e1 l\u00e0 **F1_score** \u0111\u1ec3 \u0111\u00e1nh gi\u00e1 ch\u1ea5t l\u01b0\u1ee3ng m\u00f4 h\u00ecnh","d29e66c3":"# M\u00f4 h\u00ecnh ","06cfa4b4":"Ta s\u1ebd l\u1ef1a ch\u1ecdn threshold t\u1ed1t nh\u1ea5t cho hai m\u00f4 h\u00ecnh sao cho gi\u00e1 tr\u1ecb **f1_score** l\u00e0 cao nh\u1ea5t sau \u0111\u00f3 ch\u1ecdn m\u00f4 h\u00ecnh t\u1ed1t nh\u1ea5t s\u1eed d\u1ee5ng h\u00e0m **best_threshold**.\n\nTa s\u1ebd \u0111\u1eb7t kho\u1ea3ng thres t\u1eeb 0.1 \u0111\u1ebfn 0.7 b\u1edfi ch\u00fang ta mu\u1ed1n t\u1ec9 l\u1ec7 false negative cao h\u01a1n m\u1ed9t ch\u00fat do d\u1eef li\u1ec7u b\u1ecb l\u1ec7ch.","d24a65e0":"Check l\u1ea1i \u0111\u1ed9 bao ph\u1ee7 ta th\u1ea5y:\n\n\u0110\u1ed9 bao ph\u1ee7 \u0111\u00e3 t\u0103ng l\u00ean **63.30%**\n\n**V\u1ea5n \u0111\u1ec1:**\n\nCh\u00fang ta th\u1ea5y oov \u0111ang xu\u1ea5t hi\u1ec7n nhi\u1ec1u l\u1ed7i ng\u1eef ph\u00e1p v\u00e0 c\u00e1c t\u1eeb m\u1edbi.","8f039a48":"## M\u00f4 h\u00ecnh BiLSTM","ff889128":"Set up callback t\u01b0\u01a1ng t\u1ef1 LSTM","3680c038":"Ta s\u1ebd chu\u1ea9n ho\u00e1 l\u1ea1i v\u1ec1 ng\u1eef ph\u00e1p c\u00f9ng m\u1ed9t s\u1ed1 t\u1eeb m\u1edbi:\n* Ng\u1eef ph\u00e1p Anh-Anh v\u1ec1 Anh-M\u1ef9 nh\u01b0 **labour** th\u00e0nh **labor**\n* T\u1eeb m\u1edbi **Ethereum** th\u00e0nh **crypto**\n* C\u00e1c t\u1eeb vi\u1ebft t\u1eaft **INTJ** th\u00e0nh **Introversion Intuition Thinking Judgment**\n\n\u0110\u1ecbnh ngh\u0129a h\u00e0m `correct_spelling` \u0111\u1ec3 x\u1eed l\u00ed vi\u1ec7c n\u00e0y\n\nB\u00ean c\u1ea1nh \u0111\u00f3 ta s\u1ebd \u0111\u1ecbnh ngh\u0129a h\u00e0m `replace_useless` \u0111\u1ec3 lo\u1ea1i b\u1ecf c\u00e1c k\u00ed t\u1ef1 **a, of** \u0111ang xu\u1ea5t hi\u1ec7n nhi\u1ec1u nh\u01b0ng kh\u00f4ng qu\u00e1 nhi\u1ec1u \u00fd ngh\u0129a","7561c4a4":"**V\u1ea5n \u0111\u1ec1:**\n\n\u0110\u1ed9 bao ph\u1ee7 c\u1ee7a t\u1eadp embeddings \u0111ang r\u1ea5t th\u1ea5p ch\u1ec9 chi\u1ebfm **24.31%** v\u00e0 **78.75%**\n\nQua OOV ta c\u00f3 th\u1ec3 th\u1ea5y c\u00e1c k\u00ed t\u1ef1 \u0111\u1eb7c bi\u1ec7t c\u1ee5 th\u1ec3 l\u00e0 d\u1ea5u h\u1ecfi \u0111ang l\u1eabn v\u00e0o c\u00e1c t\u1eeb, c\u00e1c k\u00ed t\u1ef1 n\u00e0y kh\u00f4ng c\u00f3 qu\u00e1 nhi\u1ec1u \u00fd ngh\u0129a. \n\n=> V\u00ec v\u1eady ta s\u1ebd lo\u1ea1i b\u1ecf ch\u00fang\n","96c06f0a":"# **M\u00f4 t\u1ea3 b\u00e0i to\u00e1n**\n**Quora** l\u00e0 m\u1ed9t n\u1ec1n t\u1ea3ng cho ph\u00e9p m\u1ecdi ng\u01b0\u1eddi h\u1ecdc h\u1ecfi l\u1eabn nhau. Tr\u00ean **Quora**, m\u1ecdi ng\u01b0\u1eddi c\u00f3 th\u1ec3 \u0111\u1eb7t c\u00e2u h\u1ecfi v\u00e0 k\u1ebft n\u1ed1i v\u1edbi nh\u1eefng ng\u01b0\u1eddi kh\u00e1c, m\u1ecdi ng\u01b0\u1eddi c\u00f9ng nhau \u0111\u00f3ng g\u00f3p th\u00f4ng tin chi ti\u1ebft \u0111\u1ed9c \u0111\u00e1o v\u00e0 c\u00e2u tr\u1ea3 l\u1eddi ch\u1ea5t l\u01b0\u1ee3ng. Tuy nhi\u00ean c\u00f3 m\u1ed9t th\u1ef1c tr\u1ea1ng \u0111\u00f3 l\u00e0 xu\u1ea5t hi\u1ec7n nh\u1eefng c\u00e2u h\u1ecfi thi\u1ebfu ch\u00e2n th\u00e0nh - nh\u1eefng c\u00e2u h\u1ecfi \u0111\u01b0\u1ee3c \u0111\u1eb7t ra mang thi\u00ean h\u01b0\u1edbng ti\u00ean c\u1ef1c, qu\u1ea5y r\u1ed1i.\n\n**Quora Insincere Question Classification** l\u00e0 m\u1ed9t b\u00e0i to\u00e1n c\u1ee7a **Quora** \u0111\u1eb7t ra, s\u1eed d\u1ee5ng s\u1ef1 tr\u1ee3 gi\u00fap t\u1eeb c\u1ed9ng \u0111\u1ed3ng, gi\u00fap h\u1ecd ph\u00e2n lo\u1ea1i nh\u1eefng c\u00e2u h\u1ecfi kh\u00f4ng ch\u00e2n th\u00e0nh.\n\nNhi\u1ec7m v\u1ee5 c\u1ee7a b\u00e0i to\u00e1n l\u00e0 s\u1eed d\u1ee5ng t\u1eadp d\u1eef li\u1ec7u m\u00e0 **Quora** cung c\u1ea5p \u0111\u1ec3 ph\u00e2n lo\u1ea1i \u0111\u00e2u l\u00e0 nh\u1eefng c\u00e2u h\u1ecfi mang h\u00e0m \u00fd kh\u00f4ng ch\u00e2n th\u00e0nh, mang n\u1ed9i dung x\u1ea5u \u0111\u1ed9c, g\u00e2y hi\u1ec3u l\u1ea7m.","ee35f4b8":"## T\u1ed5ng h\u1ee3p h\u00e0m ti\u1ec1n x\u1eed l\u00ed d\u1eef li\u1ec7u","7d1530ff":"D\u1eef li\u1ec7u ban \u0111\u1ea7u l\u00e0 d\u1eef li\u1ec7u th\u00f4 ch\u01b0a qua x\u1eed l\u00ed l\u00e0 d\u1eef li\u1ec7u m\u00e0 m\u00f4 h\u00ecnh kh\u00f4ng th\u1ec3 hi\u1ec3u \u0111\u01b0\u1ee3c v\u00ec v\u1eady ta c\u1ea7n nh\u00fang n\u00f3 v\u00e0o chi\u1ec1u kh\u00f4ng gian vector.\n\n\u1ede \u0111\u00e2y ta s\u1ebd s\u1eed d\u1ee5ng b\u1ed9 **embeddings** c\u1ee7a **GoogleNews** \u0111\u01b0\u1ee3c cu\u1ed9c thi cung c\u1ea5p l\u00e0 b\u1ed9 embedding \u0111\u01b0\u1ee3c Google hu\u1ea5n luy\u1ec7n b\u1eb1ng hai ph\u01b0\u01a1ng ph\u00e1p **Word2Vec** l\u00e0 Cbow v\u00e0 Skipgram.\n\n![](https:\/\/www.samyzaf.com\/ML\/nlp\/word2vec2.png)\n\nC\u00e1c t\u1eeb khi \u0111\u01b0\u1ee3c nh\u00fang v\u00e0o chi\u1ec1u kh\u00f4ng gian v\u1eabn s\u1ebd gi\u1eef \u0111\u01b0\u1ee3c quan h\u1ec7 ng\u1eef ngh\u0129a gi\u1eefa c\u00e1c t\u1eeb v\u1edbi nhau","1516d60d":"Check l\u1ea1i \u0111\u1ed9 bao ph\u1ee7 ta th\u1ea5y:\n\n\u0110\u1ed9 bao ph\u1ee7 \u0111\u00e3 t\u0103ng l\u00ean \u0111\u00e1ng k\u1ec3 t\u1eeb **24%** l\u00ean **60.58%**\n\n**V\u1ea5n \u0111\u1ec1:**\n\nCh\u00fang ta th\u1ea5y oov \u0111ang xu\u1ea5t hi\u1ec7n nhi\u1ec1u **k\u00ed t\u1ef1 s\u1ed1**","ea36d8c3":"# Ti\u1ec1n x\u1eed l\u00ed d\u1eef li\u1ec7u","c2d0dcf2":"Sau khi load \u0111\u01b0\u1ee3c b\u1ed9 **embeddings**, ch\u00fang ta s\u1ebd ki\u1ec3m tra \u0111\u1ed9 bao ph\u1ee7 c\u1ee7a t\u1eadp n\u00e0y tr\u00ean d\u1eef li\u1ec7u.\n\nS\u1eed d\u1ee5ng h\u00e0m `check_coverage` v\u00e0 t\u1eeb \u0111i\u1ec3n t\u1ea7n su\u1ea5t nh\u01b0 \u1edf ph\u1ea7n WordCloud","ee844df7":"# Ph\u00e2n t\u00edch d\u1eef li\u1ec7u","bb867881":"Ta s\u1ebd t\u1ed5ng h\u1ee3p l\u1ea1i c\u00e1c ph\u01b0\u01a1ng ph\u00e1p tr\u00ean trong h\u00e0m `data_cleaning` \u0111\u1ec3 x\u1eed l\u00ed d\u1eef li\u1ec7u ","1d4dc256":"\u1ede \u0111\u00e2y ta s\u1ebd th\u1eed nghi\u1ec7m hai m\u00f4 h\u00ecnh \u0111\u00f3 **LSTM** v\u00e0 **BiLSTM**\n\nLSTM l\u00e0 bi\u1ebfn th\u1ec3 c\u1ee7a RNN l\u00e0 m\u00f4 h\u00ecnh x\u1eed l\u00ed d\u1eef li\u1ec7u d\u1ea1ng chu\u1ed7i. \n\nTuy nhi\u00ean RNN c\u00f3 hai nh\u01b0\u1ee3c \u0111i\u1ec3m \u0111\u00f3 l\u00e0 **vanishing gradient** v\u00e0 h\u1ea1n ch\u1ebf trong vi\u1ec7c **l\u01b0u tr\u1eef th\u00f4ng tin** gi\u1eefa c\u00e1c t\u1eeb xa nhau trong c\u00e2u.\n\nLSTM ra \u0111\u1eddi \u0111\u1ec3 h\u1ea1n ch\u1ebf nh\u1eefng h\u1ea1n ch\u1ebf n\u00e0y c\u1ee7a RNN.\n\n![](https:\/\/i.stack.imgur.com\/aTDpS.png)\n\nBao g\u1ed3m hidden state **h(t)** ch\u1ee9a th\u00f4ng tin short-term v\u00e0 cell state **c(t)** ch\u1ee9a th\u00f4ng tin long-term\n\nV\u1edbi m\u1ed7i h(t) s\u1ebd g\u1ed3m ba c\u1ed5ng l\u00e0m nhi\u1ec7m v\u1ee5 qu\u1ea3n l\u00ed th\u00f4ng tin:\n* Forget gate f(t): ki\u1ec3m so\u00e1t th\u00f4ng tin n\u00ean qu\u00ean v\u00e0 gi\u1eef t\u1eeb cell state tr\u01b0\u1edbc \u0111\u00f3\n* Input gate i(t): ki\u1ec3m so\u00e1t ch\u1ecdn l\u1ecdc nh\u1eefng th\u00f4ng tin m\u1edbi \u0111\u1ec3 ghi v\u00e0o cell state hi\u1ec7n t\u1ea1i\n* Output gate o(t): X\u00e1c \u0111\u1ecbnh th\u00f4ng tin n\u00e0o t\u1eeb cell state hi\u1ec7n t\u1ea1i \u0111\u1ec3 truy\u1ec1n t\u1edbi c\u00e1c hidden state ti\u1ebfp theo","0a1a4c46":"## Lo\u1ea1i b\u1ecf k\u00ed t\u1ef1 \u0111\u1eb7c bi\u1ec7t v\u00e0 chu\u1ea9n ho\u00e1 t\u1eeb vi\u1ebft t\u1eaft","5aa3aa3c":"Ta chia t\u1eadp d\u1eef li\u1ec7u th\u00e0nh hai t\u1eadp d\u1eef li\u1ec7u **train** v\u00e0 **validation**\n\nV\u1edbi t\u1eadp train t\u00e1c d\u1ee5ng t\u00ecm tham s\u1ed1 cho m\u00f4 h\u00ecnh\n\nV\u1edbi t\u1eadp validation gi\u00fap t\u00ecm si\u00eau tham s\u1ed1, \u0111\u1ed9 ph\u1ee9c t\u1ea1p cho m\u00f4 h\u00ecnh tr\u00e1nh m\u00f4 h\u00ecnh b\u1ecb **overfitting**\n\nTh\u00eam v\u00e0o \u0111\u00f3 d\u1eef li\u1ec7u n\u00e0y l\u00e0 d\u1eef li\u1ec7u b\u1ecb l\u1ec7ch n\u00ean ta s\u1ebd chia m\u1eabu ph\u00e2n t\u1ea7ng **stratify** \u0111\u1ea3m b\u1ea3o t\u1ec9 l\u1ec7 gi\u1eefa hai nh\u00e3n trong hai t\u1eadp l\u00e0 nh\u01b0 nhau","197bf8d6":"\u0110\u1ed1i v\u1edbi hai m\u00f4 h\u00ecnh ta s\u1ebd setup c\u00e1c h\u00e0m callbacks h\u1ed7 tr\u1ee3 cho m\u00f4 h\u00ecnh t\u1ed1i \u01b0u t\u1ed1t h\u01a1n:\n* `checkpoint`: l\u01b0u m\u00f4 h\u00ecnh n\u1ebfu validation_loss gi\u1ea3m sau m\u1ed7i epoch\n* `reduceLROnPlat`: ta s\u1ebd gi\u1ea3m learning rate v\u1edbi c\u00f4ng th\u1ee9c `newlr = oldlr * 0.2` n\u1ebfu sau 2 epoch val_loss kh\u00f4ng gi\u1ea3m\n* `EarlyStopping`: D\u1eebng h\u1ecdc m\u00f4 h\u00ecnh n\u1ebfu sau 3 epoch val_loss kh\u00f4ng gi\u1ea3m","d1d74f6e":"**K\u1ebft lu\u1eadn:**\n\nTa th\u1ea5y \u0111\u00f4i v\u1edbi c\u00e1c c\u00e2u h\u1ecfi kh\u00f4ng ch\u00e2n th\u00e0nh c\u00e1c t\u1eeb xu\u1ea5t hi\u1ec7n c\u00f3 thi\u00ean h\u01b0\u1edbng ti\u00eau c\u1ef1c, ph\u00e2n bi\u1ec7t v\u00e0 ph\u1ea3n c\u1ea3m v\u00ed d\u1ee5 nh\u01b0: **muslims, sex, black, white ...**\n\n\u0110\u1ed1i v\u1edbi c\u00e1c c\u00e2u h\u1ecfi ch\u00e2n th\u00e0nh c\u00e1c t\u1eeb xu\u1ea5t hi\u1ec7n mang \u00fd ngh\u0129a t\u00edch c\u1ef1c, mang h\u00e0m \u00fd l\u00e0 m\u1ed9t c\u00e2u h\u1ecfi h\u01a1n: **best, when, what, like ...**","3dc35784":"D\u1eef li\u1ec7u cu\u1ed9c thi cung c\u1ea5p g\u1ed3m:\n* `train.csv` d\u1eef li\u1ec7u hu\u1ea5n luy\u1ec7n\n* `test.csv` d\u1eef li\u1ec7u ki\u1ec3m th\u1eed\n* `sample_submission.csv` file m\u1eabu n\u1ed9p l\u00ean cu\u1ed9c thi\n* `embeddings.zip` file ch\u1ee9a t\u1eadp embeddings cu\u1ed9c thi cung c\u1ea5p do cu\u1ed9c thi kh\u00f4ng cho s\u1eed d\u1ee5ng data b\u00ean ngo\u00e0i","748ac258":"## Load embedding vector","068d7b6f":"## Chu\u1ea9n b\u1ecb d\u1eef li\u1ec7u","8c477cbf":"\u0110\u1ed9 bao ph\u1ee7 \u0111\u00e3 t\u0103ng l\u00ean **63.33%** v\u00e0 **99,15%** so v\u1edbi to\u00e0n b\u1ed9 d\u1eef li\u1ec7u\n\n=> Ta s\u1ebd s\u1eed d\u1ee5ng c\u00e1c ph\u01b0\u01a1ng ph\u00e1p n\u00e0y \u0111\u1ec3 l\u00e0m s\u1ea1ch d\u1eef li\u1ec7u","63b9afec":"## LSTM","03acc7b2":"## WordCloud","bd30edd8":"D\u1eef li\u1ec7u ki\u1ec3m th\u1eed g\u1ed3m **375806** d\u00f2ng d\u1eef li\u1ec7u.\n\nD\u1eef li\u1ec7u ki\u1ec3m th\u1eed bao g\u1ed3m 2 c\u1ed9t th\u00f4ng tin:\n* `qid`: ID c\u1ee7a c\u00e2u h\u1ecfi\n* `question_text`: ti\u00eau \u0111\u1ec1 c\u00e2u h\u1ecfi, d\u1eef li\u1ec7u ta c\u1ea7n ph\u00e2n lo\u1ea1i\n\nKh\u00f4ng c\u00f3 tr\u01b0\u1eddng d\u1eef li\u1ec7u n\u00e0o c\u00f3 **gi\u00e1 tr\u1ecb thi\u1ebfu ho\u1eb7c null**","8dd22199":"**WordCloud** s\u1ebd gi\u00fap ch\u00fang ta c\u00f3 c\u00e1i nh\u00ecn tr\u1ef1c quan v\u1ec1 c\u00e1c t\u1eeb s\u1ebd ch\u1ee7 y\u1ebfu xu\u1ea5t hi\u1ec7n trong c\u00e1c c\u00e2u kh\u00f4ng ch\u00e2n th\u00e0nh v\u00e0 ch\u00e2n th\u00e0nh\n\nC\u00e1c t\u1eeb xu\u1ea5t hi\u1ec7n t\u1ea7n su\u1ea5t c\u00e0ng nhi\u1ec1u th\u00ec t\u1eeb \u0111\u00f3 s\u1ebd \u0111\u01b0\u1ee3c hi\u1ec3n th\u1ecb to v\u00e0 \u0111\u1eadm\n\nTa s\u1ebd \u0111\u1ecbnh ngh\u0129a h\u00e0m: \n* `word_freq_dict` t\u1ea1o t\u1eeb \u0111i\u1ec3n v\u1ec1 t\u1ea7n su\u1ea5t xu\u1ea5t hi\u1ec7n c\u00e1c t\u1eeb\n* `word_cloud_frequency` h\u00e0m hi\u1ec3n th\u1ecb WordCloud t\u1eeb \u0111i\u1ec3n t\u1ea7n su\u1ea5t. Trong h\u00e0m n\u00e0y s\u1ebd \u0111\u01b0\u1ee3c lo\u1ea1i b\u1ecf c\u00e1c stopword d\u1ef1a v\u00e0o th\u01b0 vi\u1ec7n nltk \u0111\u1ec3 l\u1ea5y ra nh\u1eefng t\u1eeb \u0111\u1eb7c tr\u01b0ng nh\u1ea5t cho hai nh\u00e3n","df14bcc5":"D\u1eef li\u1ec7u hu\u1ea5n luy\u1ec7n g\u1ed3m **1306122** d\u00f2ng d\u1eef li\u1ec7u.\n\nD\u1eef li\u1ec7u hu\u1ea5n luy\u1ec7n \u0111\u1ea7u v\u00e0o g\u1ed3m 3 c\u1ed9t th\u00f4ng tin:\n* `qid`: ID c\u1ee7a c\u00e2u h\u1ecfi\n* `question_text`: ti\u00eau \u0111\u1ec1 c\u00e2u h\u1ecfi, d\u1eef li\u1ec7u ta c\u1ea7n ph\u00e2n lo\u1ea1i\n* `target`: k\u1ebft qu\u1ea3 cho th\u1ea5y c\u00e2u h\u1ecfi c\u00f3 thi\u1ebfu ch\u00e2n th\u00e0nh kh\u00f4ng (1 - thi\u1ebfu ch\u00e2n th\u00e0nh, 0 - ch\u00e2n th\u00e0nh)\n\nKh\u00f4ng c\u00f3 tr\u01b0\u1eddng d\u1eef li\u1ec7u n\u00e0o c\u00f3 **gi\u00e1 tr\u1ecb thi\u1ebfu ho\u1eb7c null**"}}