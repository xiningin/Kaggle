{"cell_type":{"9cd68796":"code","42d50e8c":"code","c7520ea8":"code","0c8e8300":"code","65503493":"code","f5d383dd":"code","a337f053":"code","61375b30":"code","8d5ec851":"code","8621778b":"code","6f66643e":"code","30f44bbd":"code","511cab58":"code","c7783687":"code","62831b04":"code","4be6f3a7":"markdown","39091117":"markdown","abb245c5":"markdown","ef1271e5":"markdown","6b373565":"markdown","ec943aae":"markdown","aeed781d":"markdown"},"source":{"9cd68796":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","42d50e8c":"import gensim\n\nimport lightgbm as lgb\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error","c7520ea8":"df = pd.read_csv(\"\/kaggle\/input\/immanuel-kant-bibliography\/Kant_works_corpus.csv\")\ndf.head(6)","0c8e8300":"data_dir = '..\/input\/immanuel-kant-bibliography\/'\nsample_submission = pd.read_csv(data_dir + 'Kant_works_corpus.csv')\ntarget = df['publishing_date'].to_numpy()","65503493":"#Code by Syurenuko https:\/\/www.kaggle.com\/syurenuko\/clrp-word2vec-lightgbm-baseline\/data\n\nword2vec_model = gensim.models.KeyedVectors.load_word2vec_format('..\/input\/googlenewsvectorsnegative300\/GoogleNews-vectors-negative300.bin', binary=True)\nprint(word2vec_model.vectors.shape)","f5d383dd":"#Code by Syurenuko https:\/\/www.kaggle.com\/syurenuko\/clrp-word2vec-lightgbm-baseline\/data\n\ndef avg_feature_vector(sentence, model, num_features):\n    words = sentence.replace('\\n',\" \").replace(',',' ').replace('.',\" \").split()\n    feature_vec = np.zeros((num_features,),dtype=\"float32\")#\u7279\u5fb4\u30d9\u30af\u30c8\u30eb\u306e\u521d\u671f\u5316\n    i=0\n    for word in words:\n        try:\n            feature_vec = np.add(feature_vec, model[word])\n        except KeyError as error:\n            feature_vec \n            i = i + 1\n    if len(words) > 0:\n        feature_vec = np.divide(feature_vec, len(words)- i)\n    return feature_vec","a337f053":"#Code by Syurenuko https:\/\/www.kaggle.com\/syurenuko\/clrp-word2vec-lightgbm-baseline\/data\n\nword2vec_df = np.zeros((len(df.index),300),dtype=\"float32\")#\u7279\u5fb4\u30d9\u30af\u30c8\u30eb\u306e\u521d\u671f\u5316\n#word2vec_test = np.zeros((len(test.index),300),dtype=\"float32\")\n\nfor i in range(len(df.index)):\n    word2vec_df[i] = avg_feature_vector(df[\"text_clean\"][i],word2vec_model, 300)\n    \n#for i in range(len(test.index)):\n #   word2vec_test[i] = avg_feature_vector(test[\"text_clean\"][i],word2vec_model, 300) ","61375b30":"print(word2vec_df.shape)\nprint(target.shape)\n#print(word2vec_test.shape)","8d5ec851":"#Code by Syurenuko https:\/\/www.kaggle.com\/syurenuko\/clrp-word2vec-lightgbm-baseline\/data\n\n#parameter settings\nparams = {\n    'boosting_type': 'gbdt',\n    'metric': 'rmse',\n    'objective': 'regression',\n    'seed': 42,\n    'learning_rate': 0.01,\n    \"n_jobs\": -1,\n    \"verbose\": -1\n}\n\npred = np.zeros(df.shape[0])","8621778b":"#KFold \u3000n_splits=5\nfrom sklearn.model_selection import KFold\nfold = KFold(n_splits=5, shuffle=True, random_state=42)\ncv=list(fold.split(word2vec_df, target))","6f66643e":"#Code by Syurenuko https:\/\/www.kaggle.com\/syurenuko\/clrp-word2vec-lightgbm-baseline\/data\n\nrmses = []\nfor tr_idx, val_idx in cv: \n    x_tr, x_va = word2vec_df[tr_idx], word2vec_df[val_idx]\n    y_tr, y_va = target[tr_idx], target[val_idx]\n        \n    df_set = lgb.Dataset(x_tr, y_tr)\n    val_set = lgb.Dataset(x_va, y_va, reference=df_set)\n        \n    # Training\n    model = lgb.train(params, df_set, num_boost_round=10000, early_stopping_rounds=100,\n                      valid_sets=[df_set, val_set], verbose_eval=-1)#lgb.train is module train. Don't write df\n        \n    y_pred = model.predict(x_va)\n    rmse =  np.sqrt(mean_squared_error(y_va, y_pred))\n    rmses.append(rmse)\n        \n    #Inference\n   # test_pred = model.predict(word2vec_test)\n    #pred += test_pred \/ 5  \n        \nprint(\"\\n\", \"Mean Fold RMSE:\", np.mean(rmses))","30f44bbd":"#Saving for the next competition\n\nsample_submission.target = pred\nsample_submission.to_csv('Kant_works_corpus.csv',index=False)","511cab58":"#sample_submission","c7783687":"#5th row. And 5th column, text_clean \n\ndf.iloc[5,4]","62831b04":"#Code by Olga Belitskaya https:\/\/www.kaggle.com\/olgabelitskaya\/sequential-data\/comments\nfrom IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#eb3434','#eb3446','Akronim','Smokum',30,15\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https:\/\/fonts.googleapis.com\/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';<\/style>\n    <h1 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s<\/h1>\"\"\"%string))\n    \n    \ndhtml('Thank you Syurenuko @syurenuko for the script' )","4be6f3a7":"#This notebook is a LightGBM learning & inference model using Word2vec. It's a very light model so it can be run on a CPU.\n\nWord2vec represents words in 300 dimensions. By averaging the 300-dimensional vectors of the words in the sentence, the sentence was represented in 300 dimensions.","39091117":"#Embedding by Word2vec","abb245c5":"#Training & Inference\n\nlightgbm (KFold=5)","ef1271e5":"![](https:\/\/i.ytimg.com\/vi\/qzo9FZwIq1M\/maxresdefault.jpg)ciouliralisy.gq","6b373565":"##Code by Syurenuko https:\/\/www.kaggle.com\/syurenuko\/clrp-word2vec-lightgbm-baseline\/data","ec943aae":"<center style=\"font-family:verdana;\"><h1 style=\"font-size:200%; padding: 20px; background: #2B3A67;\"><i><b style=\"color:orange;\">Of The Injustice of Counterfeiting Books by Immanuel Kant<\/b><\/i><\/h1><\/center>\n\nImmanuel Kant (1724 - 1804)\n\nTranslated by John Richardson ( - 19th Cent.)\n\n\"This essay of Kant\u2019s on copyright argues that the unlicensed copying of books cannot possibly be permissible, due to the fact that it assumes a consent on the part of the author which it is logically impossible for the author to give. The argument is dependent upon an assumption that the writings be commodified, for the reason why the author is unable to possibly give consent to multiple publishers is due to the author\u2019s will \u2013 to communicate with the public \u2013 necessitating the profitability of the publisher, for, it is assumed, there is no way to communicate with the public at large without a great expense which can only be borne by a publishing firm. This is, of course, no longer a necessary assumption.\" (Summary by D.E. Wittkower)\n\nhttps:\/\/librivox.org\/of-the-injustice-of-counterfeiting-books-by-immanuel-kant\/","aeed781d":"#Of the Injustice of Counterfeiting Books"}}