{"cell_type":{"be0504f3":"code","16df3273":"code","15ee1621":"code","ea306162":"code","f0b55652":"code","8cb0a094":"code","2679d029":"code","7bc0c17c":"code","0f4d2193":"code","35fbfb3b":"code","220354d8":"code","75711d6a":"code","c04e35e1":"code","facdb51a":"code","679e2408":"code","ed924aaa":"code","93852783":"code","dcfef036":"code","7461cec1":"code","eabe1aa3":"code","415be734":"code","83e1dbfb":"code","ae39057b":"code","cd8f4870":"code","b6924816":"code","bb366e24":"code","8255b8d5":"code","91fcec57":"code","93e75bb4":"code","15a88ddc":"code","a4f0a618":"code","ed995bf9":"code","7f44e1b6":"code","2ff52261":"code","d54043b6":"code","42613492":"code","474d9fbc":"code","863c9906":"code","be7f1978":"code","d55774b5":"markdown"},"source":{"be0504f3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n","16df3273":"\nimport pandas as pd \ntrain = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\nprint('Data Loading is done!')","15ee1621":"train.head()","ea306162":"print(test.info())","f0b55652":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nprint(train['target'].value_counts())\nnews_class = train['target'].value_counts()\nlabels = ['Non-Disaster', 'Disaster']\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.bar(labels, news_class, color=['green', 'orange'])\n\nfig.show()","8cb0a094":"disaster_tweet_len = train[train['target']==1]['text'].str.len()\nnon_disaster_tweet_len = train[train['target']==0]['text'].str.len()\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 6))\nax[0].hist(disaster_tweet_len, color='green')\nax[0].set_title(\"Disaster Tweet Length\")\n\nax[1].hist(non_disaster_tweet_len, color='orange')\nax[1].set_title(\"Non Disaster Tweet Length\")\n\nfig.suptitle('All words in Tweets')\nplt.show()","2679d029":"fig,ax=plt.subplots(1,2,figsize=(12,6))\nax[0].boxplot(disaster_tweet_len,labels=['counts'],showmeans=True)\nax[0].set_title(\"Disaster Tweet Length\")\n\nax[1].boxplot(non_disaster_tweet_len,labels=['counts'],showmeans=True)\nax[1].set_title(\"Non Disaster Tweet Length\")#\uadf8\ub9bc \uc704 \uc81c\ubaa9\n\nfig.suptitle(\"All words in Tweets\") #\uc804\uccb4 \uc81c\ubaa9 \nplt.show","7bc0c17c":"import numpy as np\ndisaster_tweet_len = train[train['target']==1]['text'].str.len()\nnon_disaster_tweet_len = train[train['target']==0]['text'].str.len()\n\nprint(\"Max Length of Disaster Tweet: {}\".format(np.max(disaster_tweet_len)))\nprint(\"Min Length of Disaster Tweet: {}\".format(np.min(disaster_tweet_len)))\nprint(\"Mean Length of Disaster Tweet: {:.2f}\".format(np.mean(disaster_tweet_len)))\nprint(\"Median Length of Disaster Tweet: {}\".format(np.median(disaster_tweet_len)))\n\nprint(\"Max Length of Non Disaster Tweet: {}\".format(np.max(non_disaster_tweet_len)))\nprint(\"Min Length of Non Disaster Tweet: {}\".format(np.min(non_disaster_tweet_len)))\nprint(\"Mean Length of Non Disaster Tweet: {:.2f}\".format(np.mean(non_disaster_tweet_len)))\nprint(\"Median Length of Non Disaster Tweet: {}\".format(np.median(non_disaster_tweet_len)))","0f4d2193":"from wordcloud import WordCloud, STOPWORDS\n\ndisaster_tweet_keywords = dict(train[train['target']==1]['keyword'].value_counts())\nnon_disaster_tweet_keywords = dict(train[train['target']==0]['keyword'].value_counts())\n\nstopwords = set(STOPWORDS)\ndisaster_wordcloud = WordCloud(stopwords=stopwords, width=800, height=400, background_color=\"white\").\\\ngenerate_from_frequencies(disaster_tweet_keywords)\nnon_disaster_wordcloud = WordCloud(stopwords=stopwords, width=800, height=400, background_color=\"white\").\\\ngenerate_from_frequencies(non_disaster_tweet_keywords)\n\n\nfig, ax = plt.subplots(1, 2, figsize=(16, 10))\nax[0].imshow(disaster_wordcloud, interpolation='bilinear')\nax[0].axis('off')\nax[0].set_title(\"Disaster Tweet\")\nax[1].imshow(non_disaster_wordcloud, interpolation='bilinear')\nax[1].axis('off')\nax[1].set_title(\"Non Disaster Tweet\")\nfig.show()","35fbfb3b":"import pandas as pd\ndef check_na(data):\n  isnull_na = (data.isnull().sum() \/ len(data)) * 100\n  data_na = isnull_na.drop(isnull_na[isnull_na == 0].index).sort_values(ascending=False)\n  missing_data = pd.DataFrame({'Missing Ratio': data_na, \n                               'Data Type': data.dtypes[data_na.index]})\n  print(\"\uacb0\uce21\uce58 \ub370\uc774\ud130 \uceec\ub7fc\uacfc \uac74\uc218:\\n\", missing_data)\n\ncheck_na(train)\ncheck_na(test)","220354d8":"test_id = test['id']\n\nfor datas in [train, test]:\n  datas = datas.drop(['id', 'keyword', 'location'], axis=1, inplace=True)\n\ntrain.shape, test.shape","75711d6a":"import re\n\ndef remove_url(text):\n  url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n  return url.sub(r'', text)\n\nsample_text = \"\uc0c8\ub85c\uc6b4 \uce90\uae00 \ub300\ud68c\uac00 \uc5f4\ub838\uc2b5\ub2c8\ub2e4. \uc8fc\uc18c: https:\/\/www.kaggle.com\/c\/nlp-getting-started\"\nremove_url(sample_text)","c04e35e1":"def remove_html(text):\n  html = re.compile(r'<.*?>')\n  return html.sub(r'', text)\n\nsample_text =\"\"\"<div>\n<h1> Real News or Fake News <\/h1>\n<p> Kaggle Machine Learning <\/p>\n<\/div>\"\"\"\n\nprint(remove_html(sample_text))","facdb51a":"!pip install emoji --upgrade","679e2408":"import emoji\nprint(emoji.emojize('Phd is very easy!!! :thumbs_up: \ud83d\ude09\ud83d\ude09'))","ed924aaa":"def remove_emoji(text):\n  emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n  #\ud574\ub2f9 \uc694\uc18c(\uac10\uc815, \uae43\ubc1c, \uc0c1\uc9d5 \ub4f1 \uc774\ubaa8\ud2f0\ucf58)\uc774 \uac80\ucd9c\ub418\ub294 \uacbd\uc6b0 \n  return emoji_pattern.sub(r'', text) \n  #\ud574\ub2f9 \uc694\uc18c \ubd80\ubd84\uc744 \uacf5\ubc31(\uc5ec\ubc31\uc5c6\ub294)\uc73c\ub85c \ub300\uccb4\ud558\uc5ec \ucd9c\ub825\ud558\ub3c4\ub85d \ud55c\ub2e4.\n\nremove_emoji(\"Hello, \ud83d\udc4d\ud83d\ude01\")","93852783":"def remove_punct(text):\n  return re.sub(\"[^a-zA-Z]\", \" \", text)\n\nsample_text = \"Hello!, Can I have one question?.., Is it #Outbreak?\"\nremove_punct(sample_text)","dcfef036":"import nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\n\nprint(\"Total Length of stopwords:\", len(stopwords.words('english')))\nprint(stopwords.words('english')[:10])","7461cec1":"import string\nimport re\nfrom nltk.corpus import stopwords\nimport nltk\nnltk.download('stopwords')\n\ndef data_cleansing(text, remove_stopwords = False):\n  # remove url \n  url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n  cleaned_text = url.sub(r'', text) #\uc704\uc758 \ubb38\uc790\uac00 \ub098\uc62c \uacbd\uc6b0 ''\uc73c\ub85c \ubcc0\ud658\n\n  # remove html\n  html = re.compile(r'<.*?>')\n  cleaned_text = html.sub(r'', cleaned_text)\n\n  # remove emoji\n  emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n  cleaned_text = emoji_pattern.sub(r'', cleaned_text)\n\n  # Special Letters to empty space\n  cleaned_text = re.sub(\"[^a-zA-Z]\", \" \", cleaned_text)\n\n  # Lowercase\n  cleaned_text = cleaned_text.lower().split()\n\n  if remove_stopwords:\n    stops = set(stopwords.words(\"english\"))\n    cleaned_text = [word for word in cleaned_text if not word in stops]\n    clean_review = ' '.join(cleaned_text)\n  else:\n    clean_review = ' '.join(cleaned_text)\n\n  return clean_review","eabe1aa3":"clean_train_reviews = []\nfor datas in [train, test]:\n    datas['cleaned_text'] = datas['text'].apply(lambda x : data_cleansing(x, remove_stopwords=True))\n\ntrain.head(5)","415be734":"from sklearn.feature_extraction.text import CountVectorizer\ncorpus = ['As you know, I want to be with you']\nvector = CountVectorizer()\nprint(vector.fit_transform(corpus).toarray()) \nprint(vector.vocabulary_)","83e1dbfb":"from sklearn.feature_extraction.text import CountVectorizer\ncorpus = ['As you know, I want to be with you', \n          'Thank you, but I cannot be with you']\nvector = CountVectorizer()\nprint(vector.fit_transform(corpus).toarray()) \nprint(vector.vocabulary_)","ae39057b":"from sklearn.feature_extraction.text import TfidfVectorizer\ncorpus = ['Can I have lunch with you?', \n          'No, I cannot have it with you.', \n          'Because, I need to study later']\ntfidfv = TfidfVectorizer().fit(corpus)\nprint(np.round(tfidfv.transform(corpus).toarray(), 2))\nprint(tfidfv.vocabulary_)","cd8f4870":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc","b6924816":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(min_df = 0.0, analyzer='char', sublinear_tf=True, ngram_range=(1, 3), max_features = 10000)\nX = vectorizer.fit_transform(train['cleaned_text']).todense()\ny = train['target'].values","bb366e24":"print(X.shape)\nprint(y.shape)","8255b8d5":"from sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=0)\nX_train.shape, X_valid.shape, y_train.shape, y_valid.shape","91fcec57":"from sklearn.linear_model import LogisticRegression\nlgs = LogisticRegression(class_weight = 'balanced')\nlgs.fit(X_train, y_train)","93e75bb4":"import numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nX_testset = vectorizer.transform(test['cleaned_text']).todense()\nprint(\"The Shape of Test Dataset:\", X_testset.shape)\n\ny_test_pred = lgs.predict(X_testset)\nprint(\"The Predict Value:\", y_test_pred)\ny_test_pred = np.where(y_test_pred >= 0.5, 1, 0)\nprint(\"The Predict Class:\", y_test_pred)\n\nsubmission_file = pd.DataFrame({'id': test_id, 'target': y_test_pred})\nprint(submission_file.head())\n\nsubmission_file.to_csv('submission.csv', index = False)","15a88ddc":"y_pred_lgs = lgs.predict(X_valid)","a4f0a618":"confusion_matrix(y_valid, y_pred_lgs)","ed995bf9":"print(\"Accuracy: {}\".format(accuracy_score(y_valid, y_pred_lgs)))","7f44e1b6":"f1_score(y_valid, y_pred_lgs)","2ff52261":"fpr, tpr, _ = roc_curve(y_valid, y_pred_lgs)\nauc_score = auc(fpr, tpr)\n\n# clear current figure\nplt.clf()\n\nplt.title('ROC Curve')\nplt.plot(fpr, tpr, marker='.', label='AUC = {:.2f}'.format(auc_score))\n\n# it's helpful to add a diagonal to indicate where chance \n# scores lie (i.e. just flipping a coin)\nplt.plot([0,1],[0,1],'r--')\n\nplt.xlim([-0.1,1.1])\nplt.ylim([-0.1,1.1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.legend(loc='lower right')\nplt.show()","d54043b6":"X_testset =vectorizer.transform(test['cleaned_text']).todense()\nprint(X_testset)","42613492":"y_test_pred=lgs.predict(X_testset)","474d9fbc":"import numpy as np\ny_test_pred=np.where(y_test_pred>=0.5,1,0)\ny_test_pred","863c9906":"submission_file=pd.DataFrame({'id':test_id,'target':y_test_pred})\nsubmission_file","be7f1978":"submission_file.to_csv('submission.csv',index=False)","d55774b5":"# EDA"}}