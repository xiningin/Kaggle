{"cell_type":{"ad977cf0":"code","9f2bd955":"code","18d744a0":"code","5500cce4":"code","90d3e775":"code","66b7f433":"code","acb1d607":"code","72ae1ebe":"code","ed214c69":"code","50af18bf":"code","13c3e447":"code","aa3f7400":"code","68063e35":"code","c4046eff":"code","6f7f4849":"code","78fb4b3b":"markdown","764ac810":"markdown"},"source":{"ad977cf0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport string\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer,WordNetLemmatizer\nfrom tqdm import tqdm\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.metrics import accuracy_score,f1_score,roc_auc_score,confusion_matrix\nfrom keras.models import Sequential,load_model\nfrom keras import layers\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport pickle\nfrom keras.optimizers import Adam\nimport tensorflow.compat.v1.keras.layers as kl\nfrom keras.callbacks import ModelCheckpoint","9f2bd955":"train_df=pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest_df=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","18d744a0":"train_df.head(10)","5500cce4":"wl=WordNetLemmatizer()\nps=PorterStemmer()\nsp=stopwords.words('english')\ndef clean_text(tweets):\n    final_tmp=[]\n    for tweet in tweets:\n        #lower and remove punctuation\n        tweet=tweet.translate(str.maketrans('','',string.punctuation)).lower()\n        \n        #Remove Hyperlinks\n        tweet=re.sub(r'http\\S+','',tweet)\n        \n        #Remove numbers and words containing numbers\n        tweet=' '.join([i for i in tweet.split() if i.isalpha()])\n        \n        #Normalize words\n        tweet=' '.join(wl.lemmatize(i,pos='a') for i in tweet.split())\n        \n        #Now stop words\n        tweet=' '.join(i for i in tweet.split() if i not in sp)\n        \n        final_tmp.append(tweet)\n    return final_tmp","90d3e775":"cleaned_tweets=clean_text(train_df['text'])\nprint('Cleaned tweets :')\nprint(cleaned_tweets[:5])","66b7f433":"X_train,X_val,y_train,y_val=train_test_split(cleaned_tweets,list(train_df['target']),test_size=0.1)","acb1d607":"num_words=5000\nmaxlen=50\ntokenizer = Tokenizer(num_words=num_words)\ntokenizer.fit_on_texts(X_train)\nX_train = tokenizer.texts_to_sequences(X_train)\nX_val = tokenizer.texts_to_sequences(X_val)\nX_train = pad_sequences(X_train, maxlen=maxlen)\nX_val = pad_sequences(X_val, maxlen=maxlen)\nword_index=tokenizer.word_index\n","72ae1ebe":"embedding_path='..\/input\/pickled-glove840b300d-for-10sec-loading\/glove.840B.300d.pkl'\nwith open(embedding_path,'rb') as f:\n    embedding_dict=pickle.load(f)\nprint('Found %s word vectors.' % len(embedding_dict))\n","ed214c69":"embedding_matrix=np.zeros((num_words,300))\nprint('Loading Embedding Matrix..\\n')\nfor word,ix in tqdm(word_index.items()):\n    if ix<num_words:\n        embed_vec=embedding_dict.get(word)\n        if embed_vec is not None:\n            embedding_matrix[ix]=embed_vec\n        \nprint('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))","50af18bf":"model=Sequential()\nmodel.add(layers.Embedding(num_words,300,input_length=maxlen))\nmodel.layers[0].set_weights([embedding_matrix])\nmodel.layers[0].trainable = True\nmodel.add(layers.GRU(16,return_sequences=True))\nmodel.add(layers.GlobalMaxPooling1D())\nmodel.add(layers.Dense(1,activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',optimizer=Adam(0.0005),metrics=['acc'])\nmodel.summary()","13c3e447":"mc=ModelCheckpoint('classifier_0.h5',save_best_only=True,period=1,verbose=1)","aa3f7400":"history = model.fit(X_train, y_train,\n                    epochs=10,\n                    validation_data=(X_val, y_val),\n                    batch_size=32,callbacks=[mc])","68063e35":"import matplotlib.pyplot as plt\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, acc, 'b',color='red', label='Training acc')\nplt.plot(epochs, val_acc, 'b',color='blue', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, 'b', color='red', label='Training loss')\nplt.plot(epochs, val_loss, 'b',color='blue', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","c4046eff":"#Confusion matrix\nmodel=load_model('classifier_0.h5')\ny_preds=model.predict(X_train)\ny_preds=[1 if i>0.5 else 0 for i in y_preds]\ncm=confusion_matrix(y_train,y_preds)\nsns.heatmap(cm,annot=True)","6f7f4849":"#test data\ncleaned_test=clean_text(test_df['text'])\ncleaned_test=tokenizer.texts_to_sequences(cleaned_test)\ncleaned_test=pad_sequences(cleaned_test,maxlen=maxlen)\ntest_predictions=model.predict(cleaned_test)\ntest_predictions=[1 if i>0.5 else 0 for i in test_predictions]\nsub=pd.DataFrame({'id':test_df['id'],'target':test_predictions})\nsub.to_csv('submission.csv',index=False)","78fb4b3b":"**Text Cleaning**\n* Remove punctuations \n* Remove hyperlinks\n* stop words\n* lower case \n* number and words containing numbers\n* Normalize Words","764ac810":"**Split Dataset**"}}