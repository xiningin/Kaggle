{"cell_type":{"c0508580":"code","76c73540":"code","535d4531":"code","6fef4fd8":"code","73443534":"code","abdee065":"code","2514838e":"code","95dc51fb":"code","e1cb8e89":"code","f17fe296":"code","34764cdf":"code","4862fb91":"code","eaa09ab8":"code","747781a0":"code","385961f8":"code","d7c5c639":"code","412ca794":"code","e1c82608":"code","bca6009d":"code","8befb2bb":"code","971f20d8":"code","a925923f":"code","ccc87a36":"code","ec3dbcda":"code","dbfaf32c":"code","dd81e1d5":"code","27334ef8":"code","2e5941ce":"markdown","b5d9e6a8":"markdown","bc3a860b":"markdown","d0165c56":"markdown","89efad5e":"markdown","18ec2208":"markdown","cb39d0ea":"markdown"},"source":{"c0508580":"!pip install efficientnet","76c73540":"# ==================\n# Library\n# ==================\nimport pandas as pd\nimport numpy as np\nimport os\nfrom tqdm import tqdm, tqdm_notebook\nimport tensorflow as tf, re, math\nimport tensorflow.keras.backend as K\nimport efficientnet.tfkeras as efn\nfrom efficientnet.tfkeras import preprocess_input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Input, Lambda, AveragePooling1D\nfrom tensorflow.keras.preprocessing.image import load_img, save_img, img_to_array, array_to_img\nimport glob","535d4531":"# ==================\n# Constant\n# ==================\nTRAIN_PATH = '..\/input\/data-science-autumn-2021\/train.csv'\nTEST_PATH = '..\/input\/data-science-autumn-2021\/test.csv'\nTRAIN_IMG_PATH = \"..\/input\/data-science-autumn-2021\/train\/train\"\nTEST_IMG_PATH = \"..\/input\/data-science-autumn-2021\/test\/test\"\n# SAVE_PATH = \"train_eff0.npy\"","6fef4fd8":"# ===============\n# Settings\n# ===============\nimg_size = 224\nbatch_size = 16","73443534":"# ====================\n# Function\n# ====================\n\n\ndef build_model(dim=256):\n    inp = tf.keras.layers.Input(shape=(dim,dim,3))\n    base = efn.EfficientNetB0(input_shape=(dim,dim,3),weights='imagenet',include_top=False) #\u4eca\u56de\u306feEfficientNet B0\u3092\u4f7f\u3044\u307e\u3059\n    x = base(inp)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    model = tf.keras.Model(inputs=inp,outputs=x)\n    return model\n\n\ndef load_images(img_path):\n    img = load_img(img_path,target_size=(img_size,img_size))\n    img = img_to_array(img)\n    img = preprocess_input(img)\n    return img","abdee065":"# ======================\n# Main\n# ======================\ntrain_df = pd.read_csv(TRAIN_PATH)\nm = build_model(dim=img_size)\nhome_ids = train_df['homeImage'].values\nn_batches = len(home_ids) \/\/ batch_size + 1","2514838e":"m.summary()","95dc51fb":"features = np.zeros((len(train_df), 1280)) #model\u306e\u5927\u304d\u3055\u306b\u5fdc\u3058\u3066\u3001features\u306e\u5217\u6570\u3092\u5909\u3048\u3066\u304f\u3060\u3055\u3044\nn = 0\nfor b in tqdm_notebook(range(n_batches)):\n    start = b*batch_size\n    end = (b+1)*batch_size\n    batch_home = home_ids[start:end]\n    batch_images = np.zeros((len(batch_home),img_size,img_size,3))\n    for i,home_id in enumerate(batch_home):\n        try:\n            batch_images[i] = load_images(f\"{TRAIN_IMG_PATH}{home_id}.jpg\")\n        except:\n            pass\n    batch_preds = m.predict(batch_images)\n    for i,home_id in enumerate(batch_home):\n        features[n,:] = batch_preds[i]\n        n += 1","e1cb8e89":"# ======================\n# Main\n# ======================\ntest_df = pd.read_csv(TEST_PATH)\nm = build_model(dim=img_size)\nhome_ids = test_df['homeImage'].values\nn_batches = len(home_ids) \/\/ batch_size + 1","f17fe296":"features2 = np.zeros((len(test_df), 1280)) #model\u306e\u5927\u304d\u3055\u306b\u5fdc\u3058\u3066\u3001features\u306e\u5217\u6570\u3092\u5909\u3048\u3066\u304f\u3060\u3055\u3044\nn = 0\nfor b in tqdm_notebook(range(n_batches)):\n    start = b*batch_size\n    end = (b+1)*batch_size\n    batch_home = home_ids[start:end]\n    batch_images = np.zeros((len(batch_home),img_size,img_size,3))\n    for i,home_id in enumerate(batch_home):\n        try:\n            batch_images[i] = load_images(f\"{TEST_IMG_PATH}{home_id}.jpg\")\n        except:\n            pass\n    batch_preds = m.predict(batch_images)\n    for i,home_id in enumerate(batch_home):\n        features2[n,:] = batch_preds[i]\n        n += 1","34764cdf":"from sklearn.decomposition import TruncatedSVD","4862fb91":"svd = TruncatedSVD(n_components=1279)\nX = svd.fit_transform(features)\ndf_train = pd.DataFrame(X, columns=[f\"image_feats_{i}\" for i in range(1279)])","eaa09ab8":"svd2 = TruncatedSVD(n_components=1279)\nX2 = svd2.fit_transform(features2)\ndf_test= pd.DataFrame(X2, columns=[f\"image_feats_{i}\" for i in range(1279)])","747781a0":"df_train_ = pd.read_csv(\"..\/input\/data-science-autumn-2021\/train.csv\") \ndf_test_ = pd.read_csv(\"..\/input\/data-science-autumn-2021\/test.csv\") ","385961f8":"df_train2 = pd.concat([df_train_,df_train],axis=1)","d7c5c639":"df_train2.to_csv(\"train_feats_add.csv\",index=False)","412ca794":"df_test2 = pd.concat([df_test_,df_test],axis=1)","e1c82608":"df_test2.to_csv(\"test_feats_add.csv\",index=False)","bca6009d":"df_train.to_csv(\"train_feats.csv\",index=False) #\u305d\u306e\u307e\u307e\u306e\u3082\u51fa\u529b\u3057\u3066\u304a\u304f","8befb2bb":"df_test.to_csv(\"test_feats.csv\",index=False) #\u305d\u306e\u307e\u307e\u306e\u3082\u51fa\u529b\u3057\u3066\u304a\u304f","971f20d8":"from sklearn.decomposition import PCA #\u4e3b\u6210\u5206\u5206\u6790\u5668\nfrom sklearn.preprocessing import StandardScaler","a925923f":"#\u4e3b\u6210\u5206\u5206\u6790\u3000n_components=**\u3092\u5909\u66f4\u3059\u308c\u3070\u6b21\u5143\u524a\u6e1b\u6570\u304c\u5909\u308f\u308b\nfeature = PCA(n_components=50).fit_transform(df_train)\nsc = StandardScaler()\nfeature = sc.fit_transform(feature)\n\nfeature_train = pd.DataFrame(feature, columns=[\"img_PCA{}\".format(x + 1) for x in range(50)])\nfeature_train.head()","ccc87a36":"#\u4e3b\u6210\u5206\u5206\u6790\u3000n_components=**\u3092\u5909\u66f4\u3059\u308c\u3070\u6b21\u5143\u524a\u6e1b\u6570\u304c\u5909\u308f\u308b\nfeature2 = PCA(n_components=50).fit_transform(df_test)\nsc = StandardScaler()\nfeature2 = sc.fit_transform(feature2)\n\nfeature_test = pd.DataFrame(feature2, columns=[\"img_PCA{}\".format(x + 1) for x in range(50)])\nfeature_test.head()","ec3dbcda":"df_train3 = pd.concat([df_train_,feature_train],axis=1)","dbfaf32c":"df_train3.to_csv(\"train_imgPCA.csv\",index=False)","dd81e1d5":"df_test3 = pd.concat([df_test_,feature_test],axis=1)","27334ef8":"df_test3.to_csv(\"test_imgPCA.csv\",index=False)","2e5941ce":"\u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u4f7f\u3063\u3066\u3001\u753b\u50cf\u304b\u3089\u7279\u5fb4\u91cf\u3092\u62bd\u51fa\u3059\u308b\u65b9\u6cd5\u3067\u3059\u3002  \u4e0b\u8a18notebook\u3092\u53c2\u8003\u306b\u3057\u3066\u3044\u307e\u3059\u3002  \nhttps:\/\/www.kaggle.com\/christofhenkel\/extract-image-features-from-pretrained-nn\n\nSettings\u3067\u4e0b\u8a18\u8a2d\u5b9a\u306b\u5909\u66f4\u3057\u3066\u304f\u3060\u3055\u3044\n- Accelerator\u3092GPU\n- Internet\u3092ON","b5d9e6a8":"# \u6700\u521d\u306e\u5c64\u306f\u3001\u30a8\u30c3\u30b8\u306a\u3069\u306e\u5c40\u6240\u7684\u306a\u7279\u5fb4\u306e\u62bd\u51fa\u304c\u884c\u308f\u308c\u3001\u968e\u5c64\u304c\u4e0a\u306b\u306a\u308b\u307b\u3069\u3001\u3088\u308a\u5927\u57df\u7684\u306a\u7279\u5fb4\uff08\u6982\u5ff5\uff09\u3092\u8b58\u5225\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308b\u3002","bc3a860b":"# \u904e\u53bb\u30b3\u30f3\u30da\u3092\u53c2\u8003\u306b\u3057\u307e\u3057\u305f\uff01\n\u3000https:\/\/www.kaggle.com\/takoihiraokazu\/pretrained-model","d0165c56":"# \u753b\u50cf\u304b\u3089\u62bd\u51fa\u3057\u305f\u7279\u5fb4\u91cf\u3092\u3001\u305d\u306e\u307e\u307e\u4f7f\u3046\u5834\u5408","89efad5e":"# \u3063\u3066\u3053\u3068\u306f\u3001\u5f8c\u534a\u90e8\u5206\u3060\u3051\u3092\u62bd\u51fa\u3057\u3066\u4f7f\u7528\u3002\u307e\u305f\u306f\u5f8c\u534a\u90e8\u5206\u306e\u307f\u6b21\u5143\u5727\u7e2e\u3059\u308b\u306e\u304c\u3044\u3044\u304b\u3082\uff1f\uff1f","18ec2208":"# \u6b21\u5143\u524a\u6e1b\u3057\u3066\u4f7f\u3046\u5834\u5408","cb39d0ea":"## \u8a66\u3057\u3066\u307f\u3066\u3001\u3069\u3046\u3060\u3063\u305f\u304b\u3001\u5171\u6709\u304a\u9858\u3044\u3057\u307e\u3059\u3002"}}