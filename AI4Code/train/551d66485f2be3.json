{"cell_type":{"b9e4efc2":"code","647b021d":"code","a9b25e07":"code","e6b20ac0":"code","a49379fc":"code","aac6e74a":"code","3b5e10ee":"code","5068e257":"code","a5494d91":"code","bd249e22":"code","ae03abbe":"code","310d83fa":"code","0e606cdc":"code","50839b61":"code","737b37c2":"code","3dbaaf09":"code","298c95f9":"code","425edbdf":"code","f19c0ccb":"code","230a1da4":"markdown","396d38dd":"markdown","c6014e1a":"markdown","b399cd7a":"markdown","09abebb8":"markdown","362a69a3":"markdown","5eca1d40":"markdown","e0a8946f":"markdown","f1ece39e":"markdown","089c40fa":"markdown","c0f300c8":"markdown","9b448646":"markdown","3a28cbaa":"markdown","7fc8dd6f":"markdown","08e1457e":"markdown","43fae005":"markdown","b9c41b2f":"markdown","496d2055":"markdown","d7c45398":"markdown","e1b346fb":"markdown"},"source":{"b9e4efc2":"import pandas as pd\nimport numpy as np\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","647b021d":"df_train = pd.read_csv('..\/input\/titanic\/train.csv')\ndf_test = pd.read_csv('..\/input\/titanic\/test.csv')","a9b25e07":"df_train.head()","e6b20ac0":"df_train.tail()","a49379fc":"df_test.head()","aac6e74a":"df_test.tail()","3b5e10ee":"df_train.describe()","5068e257":"from matplotlib import pyplot as plt\nplt.style.use('seaborn')\n\nfig1, ax1 = plt.subplots(nrows=1, ncols=3,figsize=(10,5))\ndf_train.groupby('Pclass')['Survived'].mean().plot.bar(ax=ax1[0],rot=0, title='Passenger Class',edgecolor=\"k\", xlabel='')\ndf_train.groupby('Sex')['Survived'].mean().plot.bar(ax=ax1[1],rot=0, title = 'Female or Male',edgecolor=\"k\", xlabel='')\ndf_train.groupby('Embarked')['Survived'].mean().plot.bar(ax=ax1[2],rot=0, title = 'Port of Embarkation',edgecolor=\"k\",xlabel='')\n\nplt.tight_layout()\nplt.show()","a5494d91":"#Age\nage_bins = np.arange(0, 100, 20, dtype='int')\nage_labels = [f'Under {i}' for i in age_bins[1:]]\nage_group = pd.cut(df_train['Age'], bins=age_bins, labels=age_labels, right=False).rename(None)\n\n#Fare\nfare_bins = np.arange(0, 601, 200, dtype='int')\nfare_labels = [f'Under {i}' for i in fare_bins[1:]]\nfare_group = pd.cut(df_train['Fare'], bins=fare_bins, labels=fare_labels, right=False).rename(None)\n\nfig2, ax2 = plt.subplots(nrows=2, ncols=2,figsize=(15,10))\ndf_train.groupby(age_group)['Survived'].mean().plot.bar(ax=ax2[0][0], rot=0, title='Age',edgecolor=\"k\")\ndf_train.groupby('SibSp')['Survived'].mean().plot.bar(ax=ax2[0][1],rot=0, title = '# of siblings \/ spouses', edgecolor=\"k\",xlabel='')\ndf_train.groupby('Parch')['Survived'].mean().plot.bar(ax=ax2[1][0],rot=0, title = '# of parents \/ children',edgecolor=\"k\",xlabel='')\ndf_train.groupby(fare_group)['Survived'].mean().plot.bar(ax=ax2[1][1], rot=0, title='Passenger Fare',edgecolor=\"k\")\n\nplt.show()","bd249e22":"df_train.info()","ae03abbe":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\n\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n  def __init__(self, attribute_names):\n    self.attribute_names = attribute_names\n  def fit(self, X, y=None):\n    return self\n  def transform(self, X):\n    return X[self.attribute_names].values","310d83fa":"df_train_drop = df_train.dropna(subset=['Age', 'Embarked']).drop('Cabin', axis=1)\ndf_test_drop = (df_test.dropna(subset=['Age', 'Embarked'])).drop('Cabin', axis=1)\nX_train = df_train_drop.drop('Survived', axis=1)\ny_train = df_train_drop['Survived']\nX_test = df_test_drop.copy()\n\n\ncat_attribute = ['Pclass', 'Sex', 'Embarked']\ncat_pipeline = Pipeline([\n    ('selector', DataFrameSelector(cat_attribute)),\n    ('cat_encoder', OneHotEncoder(categories='auto', sparse=False))\n])","0e606cdc":"num_attribute = ['Age', 'SibSp', 'Parch', 'Fare']\nnum_pipeline = Pipeline([\n    ('selector', DataFrameSelector(num_attribute)),\n    ('Imputer', SimpleImputer()),\n    ('num_scale', StandardScaler())\n])","50839b61":"from sklearn.compose import ColumnTransformer\n\nfull_pipeline = ColumnTransformer([\n    ('num_pipeline', num_pipeline, num_attribute),\n    ('cat_pipeline', cat_pipeline, cat_attribute)\n])\n\ntrain_prepared = full_pipeline.fit_transform(X_train)\ntrain_prepared.shape","737b37c2":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nimport math\n\nlog_reg = LogisticRegression(random_state=42, n_jobs=-1)\nlog_params = {'tol':[1e-4, 1e-2, 1e-1], 'C':[1,3,5]}\nlog_grid = GridSearchCV(log_reg, log_params, cv=3)\nlog_grid.fit(train_prepared, y_train)\nlog_predict = log_grid.predict(train_prepared)\nlog_graph = log_grid.predict_proba(train_prepared)[:,1]","3dbaaf09":"from sklearn.svm import SVC\n\nsvc = SVC(random_state=42)\nsvc_params = {'kernel':('poly', 'rbf'), 'C':[1,3,5], 'tol':[1e-4, 1e-2, 1e-1]}\nsvc_grid = GridSearchCV(svc, svc_params, cv=3)\nsvc_grid.fit(train_prepared, y_train)\nsvc_predict = svc_grid.predict(train_prepared)","298c95f9":"from sklearn.metrics import accuracy_score\n\nlog_score = str(round(accuracy_score(df_train_drop['Survived'], log_predict) * 100, 2))\nsvc_score = str(round(accuracy_score(df_train_drop['Survived'], svc_predict) * 100, 2))\n\nprint('The accuracy score for Logistic Regression is ' + log_score + '%')\nprint('The accuracy score for SVC is ' + svc_score + '%')","425edbdf":"test_prepared = full_pipeline.fit_transform(df_test)\nY_pred = svc_grid.predict(test_prepared)","f19c0ccb":"submission = pd.DataFrame({\n        \"PassengerId\": df_test[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv('titanic.csv', index=False)","230a1da4":"<font size=3>\nBased on the Data Dictionary in 'Titanic - Machine Learning from Disaster', Categorical features are pclass, Sex, and embarked. In addition, Numerical features are Age, sibsp, parch, ticket, fare, and cabin. <br \/>\nCategorical and Numerical features will be divided separately. <br \/>\nAfter splitting each other, all the data engineering will be happened to apply at the model.","396d38dd":"**Logistic Regression**","c6014e1a":"# 4. Model Selection & GridSearch","b399cd7a":"# Titanic\n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/a\/a2\/Titanic_lifeboat.jpg\" width=\"500\">","09abebb8":"**Numerical Features** <br \/>\nUsing StandardScaler can increase accuracy of machine learning model!","362a69a3":"**Categorical Features** <br \/>\nChanging Age's null data to median or mean can lead wrong solution to model, so I deleted null data. <br \/>\nUsing OneHotEncoder can apply categorical features to machine learning model.\n","5eca1d40":"**SVM**","e0a8946f":"<font size=3>\nSince scikit-learn cannot use dataframe directly, I made new class, DataFrameSelector, which can take data as a dataframe.","f1ece39e":"<font size=3>\nNow, it's time to submit our machine model!","089c40fa":"<font size=3>\n1. Age = Under 20 years old has highest ratio and Under 80 years old has lowest ratio. <br \/>\n2. # of siblings \/ spouses = Less siblings or spouses has more chance to survive <br \/>\n3. # of parents \/ children = This result is kind of tricky for me. 3 parents of children has highest percentage of survive. <br \/>\n4. Passenger Fare = This result has very close relation with Passenger Class","c0f300c8":"<font size=3>\nI am going to use Logistic Regression and SVM. By using gridsearch, each model will be tuned. After finishing tuning, we will check which model has higher accuracy.","9b448646":"<font size=3>\nBased on the describe, only 38% of the people in the titanic were survived. <br \/>\nPclass(=Ticket class) is divided into three classes which are 1st, 2nd, 3rd class.<br \/>\nOldest person in the titanic was 80 years old and yougest person was less than one year. <br \/>\nMore than 50% of people did not come with any siblings or spouses<br \/>\nAlso, more than 75% of people came along to the titanic <br \/>\nLastly, highest price for ticket was \\$512.3 and lowest price was $0!","3a28cbaa":"<font size=3>\nAs you can see, SVC is almost 4% more accurate than Logistic Regression. <br \/>\nTherefore, I am going to use SVC to test data sets.","7fc8dd6f":"# 5. Submission","08e1457e":"<font size=3>\n1. Passenger Class = More than a half people in 1st class survived! Unfortunately, lots of 2nd and 3rd class people did not make it... <br \/>\n2. Female or Male = Interestingly, the ratio of female has 3 times higher than the ration of male <br \/>\n3. Port of Embarkation = People who were embarked from C(=Cherbourg) was highest, Q(=Queenstown) was second, and S(=Southampton) was third.","43fae005":"# 1. Load Data & Check Information","b9c41b2f":"<font size=\"3\">\nAs you can see, data is divided into two groups, which are train and test sets. <br \/> Train set contains 891 personal information, and test set contains 417 personal information based on PassengerId. Each person has different information.","496d2055":"# 2. Data Overview","d7c45398":"**ColumnTransformer** <br \/>\nAfter handling Categorical and Numerical features, both are going to combined together to apply into machine learning model.","e1b346fb":"# 3. Data Engineering"}}