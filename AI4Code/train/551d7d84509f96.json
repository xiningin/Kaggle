{"cell_type":{"00514a99":"code","b4f58291":"code","b1066d2b":"code","1f542530":"code","9e1565f4":"code","7bc15509":"code","39806b51":"code","eea7d7fc":"code","83406acb":"code","d71c37b7":"code","1782e7d1":"code","6a746f80":"code","bc0d41cf":"code","67b695bb":"code","3f437e22":"code","2c4f30f8":"code","377a38bb":"code","5ce544d0":"code","3341b9e7":"code","87486ff7":"code","33866f06":"code","14e06395":"code","e8872c91":"code","a8b3a0f5":"code","078bdb65":"code","038084c3":"code","0351992c":"code","52215d47":"code","dc6feaec":"code","e795e66a":"code","d807c351":"code","665b7345":"code","cb62636b":"code","3a9b0491":"code","ae761624":"code","f73dd2be":"code","d0e9155e":"code","1bd82b48":"code","9974dfda":"code","e789eca5":"code","3f6163ad":"code","92cdc533":"code","a0989cf9":"code","4466c478":"code","f166d8a4":"code","e482f2cb":"code","42ad0f64":"code","45281b62":"code","f80b063f":"code","f4bad98f":"code","1ee461d9":"code","cea84576":"code","a36267d5":"code","4bb73c7e":"code","b63d1920":"code","efad2c05":"code","04ff0486":"code","17bbddc9":"code","b5743041":"code","62b5757c":"code","5255b666":"code","824394cf":"code","5896242f":"code","f239bc85":"code","1ba551e6":"code","54d5fdaf":"code","0319a775":"code","81757d5a":"code","b3d7082d":"code","93a5fdfb":"code","b876e47f":"code","bbaedb84":"code","5239693d":"code","b72cf2d2":"markdown","625e8d34":"markdown","3c6bf523":"markdown","25b6c7cf":"markdown","f80d26c1":"markdown","82c87c46":"markdown","80a2cca2":"markdown","63a1e8e3":"markdown","0b238f02":"markdown","f684ad2d":"markdown","50882087":"markdown","bf847617":"markdown","636149a1":"markdown","22eebedb":"markdown","59df050e":"markdown","3b7fed3b":"markdown","21bcefc0":"markdown","bd22359b":"markdown","c37a2e59":"markdown","d6d71361":"markdown","e5559747":"markdown","4d39a855":"markdown","0dd851b2":"markdown","2bb241fe":"markdown","7b91c47a":"markdown","99a11377":"markdown","27253b41":"markdown","448d9106":"markdown","695afa50":"markdown","2bafd708":"markdown","31eb3126":"markdown","b7757628":"markdown","7cb30ae0":"markdown","4573d755":"markdown","b9e634a1":"markdown","2161d674":"markdown","65705885":"markdown","e2673351":"markdown","7e273b2c":"markdown","a82446b3":"markdown","5a48e4ec":"markdown","37925b9e":"markdown","6d31291b":"markdown","b57e448d":"markdown","317d075c":"markdown","0941c2e7":"markdown","f4ef964f":"markdown","24a17be8":"markdown","abef2866":"markdown","9b3ef597":"markdown","a09d8819":"markdown","0c09fbf1":"markdown","3593e48d":"markdown","935bac4a":"markdown","ce4a8b29":"markdown","b4fa11c5":"markdown","80d81366":"markdown","3c8fd2bd":"markdown","0fb94952":"markdown","9dfaa449":"markdown","a884d908":"markdown","19316f37":"markdown","8d845f08":"markdown","279b69e8":"markdown","3704bac6":"markdown","0cd6ed09":"markdown","d83564a8":"markdown","f6e46cab":"markdown","ca7722d3":"markdown","455d2172":"markdown","30e8811e":"markdown","7bb5c961":"markdown","afb80773":"markdown","e75a4293":"markdown","ab819e80":"markdown","03b5d309":"markdown","7addf89a":"markdown","f0c6ec8f":"markdown"},"source":{"00514a99":"# Importing essential starter libraries\nimport numpy as np      # vectors and matrices\nimport pandas as pd     # tables and data manipulations\nimport seaborn as sns\n\n# For Dates\nimport datetime\nfrom scipy.optimize import minimize                 # for function minimization\nfrom dateutil.relativedelta import relativedelta    # working with dates with style\n\n# Sklearn libraries\nfrom sklearn import metrics\nfrom sklearn import datasets\nfrom sklearn import linear_model\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV, RandomizedSearchCV\n\n# statistics and econometrics\nimport scipy.stats as scs\nimport statsmodels.api as sm\nimport statsmodels.tsa.api as smt\nimport statsmodels.formula.api as smf\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\n# Visualization Libraries\nimport seaborn as sns # more plots\nimport matplotlib.pyplot as plt # plots\n# plt.style.use('fivethirtyeight')\n# Above is a special style template for matplotlib, highly useful for visualizing time series data\n\n# some useful functions\nfrom itertools import product\nfrom tqdm import tqdm_notebook\nimport warnings # `do not disturb` mode\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\nsns.set(rc={'figure.figsize':(15,12)})\n\n# To have graphs embedded in the notebook\n%matplotlib inline\n\n# Print the graphs as PDF and PNG\nfrom IPython.display import set_matplotlib_formats\nset_matplotlib_formats('png', 'pdf')\n\n# Center graph outputs\nfrom IPython.core.display import HTML as Center\n\nCenter(\"\"\" <style>\n.output_png {\n    display: table-cell;\n    text-align: center;\n    vertical-align: middle;\n}\n<\/style> \"\"\")","b4f58291":"# Import data\ndf = pd.read_csv('..\/input\/car-price-prediction\/CarPrice_Assignment.csv')","b1066d2b":"# View the first 5 rows\ndf.head()","1f542530":"# Dataframe shape: how many rows and columns in the given dataset?\nprint(\"There are\", df.shape[0], \"rows\/entries and\", df.shape[1], \"features in our dataframe.\")","9e1565f4":"# Data set Info\ndf.info()","7bc15509":"# check if there are missing values in our data\ndf.isnull().sum()","39806b51":"# Are there any rowns with missing values?\ndf[df.isnull().any(axis=1)]","eea7d7fc":"# summary statistics\ndf.describe()","83406acb":"#Identifying the unique number of values in the dataset\ndf.nunique()","d71c37b7":"# Drop car_ID \ndf = df.drop('car_ID', axis=1)\ndf.head()","1782e7d1":"# Outlier Analysis\nplt.figure(figsize = [15,5])\nsns.boxplot(data=df['price'], orient=\"h\", palette=\"Set1\", color=\"red\")\nplt.title(\"Price Variable Distribution\", fontsize = 20, fontweight = 'bold')\n#plt.xlabel(\"Price Range\", fontsize = 15, fontweight= 'bold')\n#plt.ylabel(\"Continuous Variable\", fontsize = 15, fontweight= 'bold')\nplt.show()","6a746f80":"# Extracting Auto Company names from carName\ndf['CarName'] = df['CarName'].str.split(' ', expand=True)\ndf.head()","bc0d41cf":"# Unique car companies\narr = df['CarName'].unique()\n\n# Turn the above into a DataFrame\ndf1 = pd.DataFrame(arr, columns=[\"Auto Company Name\"])\ndf1.reset_index(drop=True, inplace=True)\ndf1","67b695bb":"# Correcting the typing errors observed\ndf['CarName'] = df['CarName'].replace({'maxda': 'mazda', 'nissan': 'Nissan', 'porcshce': 'porsche', 'toyouta': 'toyota', 'vokswagen': 'volkswagen', 'vw': 'volkswagen'})","3f437e22":"# Unique car companies\narr1 = df['CarName'].unique()\n\n# Turn the above into a DataFrame\ndf2 = pd.DataFrame(arr1, columns=[\"Updated Auto Company Name\"])\ndf3 = pd.concat([df1, df2], axis=1)\ndf3","2c4f30f8":"# Datatype change\ndf['symboling'] = df['symboling'].astype(str)\n\n# To check if the change took effect\nif df['symboling'].dtype == 'object':\n    print(\"Symboling datatype has been changed successfully.\")\nelse:\n    print(\"Changes failed\")","377a38bb":"# Check for Duplicates\nprint(\"There are\", df.duplicated().sum(),\"duplicates in this dataset.\")","5ce544d0":"# Numerical Segregation\nnum_cols = df.select_dtypes(exclude=['object']).columns\ndf_num = df[num_cols]\ndf_num.head()","3341b9e7":"# Numerical Segregation\ncat_cols = df.select_dtypes(include=['object']).columns\ndf_cat = df[cat_cols]\ndf_cat.head()","87486ff7":"print(\"Numerical Category has\", df_num.shape[1], \"features, while Categorical segregation has\", df_cat.shape[1],\".\")","33866f06":"# Minimum price\nprint(\"The Minimum price of the car in this dataset is $\", round(df['price'].min()))\n# Maximum price\nprint(\"The Maximum price of the car in this dataset is $\", round(df['price'].max()))","14e06395":"# Cars sold by each Auto Company\n#df.CarName.value_counts()\n# Turn into DataFrame\ndf_carnames = pd.DataFrame(df['CarName'].value_counts()).reset_index().rename(columns={'index':'car_name','CarName': 'count'})\ndf_carnames","e8872c91":"df_carnames = pd.DataFrame(df['CarName'].value_counts()).reset_index().rename(columns={'index':'car_name','CarName': 'count'})\ndf_carnames","a8b3a0f5":"# Visualizing Number of car per automaker\nnum_cars = sns.barplot(y='car_name', x='count', data=df_carnames)\nnum_cars = plt.setp(num_cars.get_xticklabels(), rotation=75)\nplt.xlabel(\"Count\", fontsize = 15, fontweight= 'bold')\nplt.ylabel(\"Car Name\", fontsize = 15, fontweight= 'bold')\nplt.title(\"Number of Cars by Each Manufacturer\", fontname ='Times New Roman', size = 12, color ='purple')","078bdb65":"plt.style.use('seaborn')\nplt.figure(figsize=(10, 4))\n# Regression Plot\nsns.regplot(x='wheelbase', y='price', data=df_num, \n            scatter_kws = {'color': 'purple', 'alpha':0.3}, \n            line_kws = {'color': '#CCCC00', 'alpha':0.89, 'lw':6})\nplt.title(\"Wheelbase vs. Price\", fontname ='Times New Roman', size = 25, color ='purple')\nplt.xlabel(\"Wheel Base\", fontname ='Times New Roman', size = 15, color ='blue')\nplt.ylabel(\"Price\", fontname ='Times New Roman', size = 15, color ='blue')\nsns.set_style('whitegrid')\nplt.show()","038084c3":"# pairplot without hue\nplt.figure(figsize=(10, 4))\nsns.pairplot(df_num, x_vars=['wheelbase', 'price', ], \n             y_vars=['wheelbase', 'price'],\n             diag_kind=\"kde\", \n             plot_kws=dict(marker=\"o\", linewidth=1))\n# to show\nplt.show()\nprint(\"Correlation of wheel base to price is\", round(df_num['wheelbase'].corr(df_num['price']), 4))","0351992c":"plt.style.use('seaborn')\n# pairplot of the car length, width and height\nauto_dims = sns.pairplot(df_num, x_vars=['carlength', 'carwidth','carheight', 'price', ], \n             y_vars=['carlength', 'carwidth', 'carheight', 'price'],\n             #diag_kind=\"auto\",\n             diag_kws=dict(fill=True, color=\"magenta\"),\n             plot_kws=dict(marker=\"o\", linewidth=1, color='red', alpha=0.3))\nauto_dims.map_upper(sns.regplot, color=\"green\")\nauto_dims.map_lower(sns.kdeplot, cmap=\"Reds\")\nauto_dims.map_diag(plt.hist, density ='True', bins = 10, color = 'purple')\n# to show\nplt.show()","52215d47":"# Select carlength, carwidth, carheight only\ndf_cardims = df_num.iloc[:,1:4]\n# Add price column to the selected columns\ndf_cardims['price'] = df_num['price']\n# Find their correlation matrix\ndf_cardims_corr = df_cardims.corr().round(5)\n# Make a heatmap\n#sns.heatmap(df_cardims_corr, annot = True, center=1, linewidths=1)\nsns.set(rc={'figure.figsize':(10,6)})\nax = sns.heatmap(df_cardims_corr, cbar=True, square= False, \n                 fmt='.1f',\n                 annot=True, annot_kws={'size':15}, cmap='Accent')\nplt.title(\"Car Dimensions correlation to Price\", size=25, fontname ='Times New Roman', color ='purple')\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right')","dc6feaec":"df_cardims_corr\n# Correlation of features vs price only\n# Set thrashhold: anything greater or equal to 0.5 is good\nthreshold = 0.5 \nc = abs(df_cardims_corr['price'])\noutput_c = pd.DataFrame(c[c>threshold])\noutput_c","e795e66a":"plt.style.use('seaborn')\nplt.figure(figsize=(15, 5))\n# Regression Plot\nsns.regplot(x='carheight', y='price', data=df_num, \n            scatter_kws = {'color': 'red', 'alpha':1,\"s\":30, \"cmap\":\"plasma\"},\n            line_kws = {'color': 'k', 'alpha':0.89, 'lw':4})\n#sns.scatterplot(x=df_num['carheight'], y=df_num['price'], cmap=\"red\")\nsns.kdeplot( df_num['carheight'], df_num['price'], cmap=\"Blues\")\nplt.title(\"Car Height vs. Price\", fontname ='Helvetica', size = 25, color ='purple')\nplt.xlabel(\"Car Height \", fontname ='Times New Roman', size = 15, color ='blue')\nplt.ylabel(\"Price\", fontname ='Times New Roman', size = 15, color ='blue')\nsns.set_style('whitegrid')\nplt.show()\n# print correlation \nprint(\"Correlation between curb weight and price is\", df_num['carheight'].corr(df_num['price']))","d807c351":"plt.style.use('seaborn')\nplt.figure(figsize=(15, 5))\n# Regression Plot\nsns.regplot(x='curbweight', y='price', data=df_num, \n            scatter_kws = {'color': 'red', 'alpha':1,\"s\":50, \"cmap\":\"plasma\"},\n            line_kws = {'color': 'k', 'alpha':0.89, 'lw':4})\n#sns.scatterplot(x=df_num['carheight'], y=df_num['price'], cmap=\"red\")\nsns.kdeplot(df_num['curbweight'], df_num['price'], cmap=\"Blues\")\nplt.title(\"Curb Weight vs. Price\", fontname ='Times New Roman', size = 25, color ='purple')\nplt.xlabel(\"Curb Weight\", fontname ='Times New Roman', size = 15, color ='blue')\nplt.ylabel(\"Price\", fontname ='Times New Roman', size = 15, color ='blue')\nsns.set_style('whitegrid')\nplt.show()\n\n# print correlation \nprint(\"Correlation between curb weight and price is\", df_num['curbweight'].corr(df_num['price']))","665b7345":"plt.style.use('seaborn')\nplt.figure(figsize=(15, 5))\n# Regression Plot\nsns.regplot(x='enginesize', y='price', data=df_num, \n            scatter_kws = {'color': 'red', 'alpha':0.4,\"s\":100, \"cmap\":\"plasma\"},\n            line_kws = {'color': 'k', 'alpha':0.89, 'lw':4})\n#sns.scatterplot(x=df_num['carheight'], y=df_num['price'], cmap=\"red\")\nsns.kdeplot(df_num['enginesize'], df_num['price'], cmap=\"Blues\")\nplt.title(\"Engine Size vs. Price\", fontname ='Times New Roman', size = 25, color ='purple')\nplt.xlabel(\"Curb Weight\", fontname ='Times New Roman', size = 15, color ='blue')\nplt.ylabel(\"Price\", fontname ='Times New Roman', size = 15, color ='blue')\nsns.set_style('whitegrid')\nplt.show()\nplt.tight_layout()\n# print correlation \nprint(\"Correlation between engine size and price is\", df_num['enginesize'].corr(df_num['price']))","cb62636b":"plt.style.use('seaborn')\nplt.figure(figsize=(15, 10))\n\nplt.subplot(2, 2, 1)\n# Regression Scatter Plot for Bore Ratio\n# get coeffs of linear fit\n#slope, intercept, r_value, p_value, std_err = stats.linregress(df_num['boreratio'], df_num['price'])\nsns.regplot(x='boreratio', y='price', data=df_num, \n            scatter_kws = {'color': 'red', 'alpha':0.4,\"s\":80, \"cmap\":\"colorblind\", 'marker':'+'},\n            line_kws = {'color': 'k', 'alpha':0.89, 'lw':4,\n                       'label':'Bore Ratio'})\nplt.legend()\nsns.kdeplot(df_num['boreratio'], df_num['price'], cmap=\"Blues\")\n\n# Regression Scatter Plot for Stroke\nsns.regplot(x='stroke', y='price', data=df_num, \n            scatter_kws = {'color': 'blue', 'alpha':0.5,\"s\":100, \"cmap\":\"plasma\", 'marker':\"+\"},\n            line_kws = {'color': 'maroon', 'alpha':0.4, 'lw':4,\n                       'label':'Stroke'})\nplt.legend()\nsns.kdeplot(df_num['stroke'], df_num['price'], cmap=\"Blues\")\n\n#sns.scatterplot(x=df_num['carheight'], y=df_num['price'], cmap=\"red\")\nplt.title(\"Bore Ratio & Stroke vs. Price\", fontname ='Times New Roman', size = 18, color ='purple')\n#plt.xlabel(\"Bore Ratio and Stroke\", fontname ='Times New Roman', size = 15, color ='blue')\n#plt.ylabel(\"Price\", fontname ='Times New Roman', size = 15, color ='blue')\n\nplt.subplot(2, 2, 2)\n# Regression Scatter Plot for Compression Ratio\nsns.regplot(x='compressionratio', y='price', data=df_num, \n            scatter_kws = {'color': 'maroon', 'alpha':0.3,\"s\":100, \"cmap\":\"plasma\", 'marker':'+'},\n            line_kws = {'color': 'darkgreen', 'alpha':0.4, 'lw':4})\nsns.kdeplot(df_num['compressionratio'], df_num['price'], cmap=\"Blues\")\n\n# graph aethetics\nplt.title(\"Compression vs. Price\", fontname ='Times New Roman', size = 18, color ='purple')\n#plt.xlabel(\"Compression\", fontname ='Times New Roman', size = 15, color ='blue')\n#plt.ylabel(\"Price\", fontname ='Times New Roman', size = 15, color ='blue')\n\n# Set graph styles\nsns.set_style('whitegrid')\nplt.show()\nplt.tight_layout()\n\n# print correlation \nprint(\"Correlation Coefficient between Bore Ratio and Price is\", round(df_num['boreratio'].corr(df_num['price']), 4))\nprint(\"Correlation Coefficient between Stroke and Price is\", round(df_num['stroke'].corr(df_num['price']), 4))\nprint(\"Correlation Coefficient between Compression Ratio and Price is\", round(df_num['compressionratio'].corr(df_num['price']), 5))\n","3a9b0491":"plt.style.use('seaborn')\nplt.figure(figsize=(15, 5))\n# Regression Scatter Plot for Compression Ratio\nsns.regplot(x='horsepower', y='price', data=df_num, \n            scatter_kws = {'color': 'maroon', 'alpha':0.3,\"s\":100, \"cmap\":\"plasma\", 'marker':'+'},\n            line_kws = {'color': 'darkgreen', 'alpha':0.4, 'lw':4})\nsns.kdeplot(df_num['horsepower'], df_num['price'], cmap=\"Blues\")\n\n# graph aethetics\nplt.title(\"Horsepower vs. Price\", fontname ='Times New Roman', size = 25, color ='purple')\nplt.xlabel(\"Horsepower\", fontname ='Times New Roman', size = 15, color ='blue')\nplt.ylabel(\"Price\", fontname ='Times New Roman', size = 15, color ='blue')\n\n# Set graph styles\nsns.set_style('whitegrid')\nplt.show()\nplt.tight_layout()\n\n# print correlation \nprint(\"Correlation Coefficient between horsepower and Price is\", round(df_num['horsepower'].corr(df_num['price']), 4))\n","ae761624":"plt.style.use('seaborn')\nplt.figure(figsize=(15, 5))\n# Regression Scatter Plot for Compression Ratio\nx = df_num.peakrpm\ny = df_num.price\ndata = df_num\nsns.regplot(x=x, y=y, data=df_num, \n            scatter_kws = {'color': 'maroon', 'alpha':0.3,\"s\":100, \"cmap\":\"plasma\", 'marker':'+'},\n            line_kws = {'color': 'darkgreen', 'alpha':0.4, 'lw':4})\nsns.kdeplot(df_num['peakrpm'], df_num['price'], cmap=\"Blues\")\n\n# graph aethetics\nplt.title(\"Peak RPM vs. Price\", fontname ='Times New Roman', size = 25, color ='purple')\nplt.xlabel(\"Peark RPM\", fontname ='Times New Roman', size = 15, color ='blue')\nplt.ylabel(\"Price\", fontname ='Times New Roman', size = 15, color ='blue')\n\n# Set graph styles\nsns.set_style('whitegrid')\nplt.show()\nplt.tight_layout()\n\n# print correlation \nprint(\"Correlation Coefficient between horsepower and Price is\", round(df_num['peakrpm'].corr(df_num['price']), 4))\n","f73dd2be":"plt.style.use('seaborn')\nplt.figure(figsize=(15, 10))\n# first subplot\nplt.subplot(2, 2, 1)\n\nsns.regplot(x='citympg', y='price', data=df_num, \n            scatter_kws = {'color': 'green', 'alpha':0.4,\"s\":80, \"cmap\":\"colorblind\", 'marker':'+'},\n            line_kws = {'color': 'k', 'alpha':0.5, 'lw':4, 'label':'City MPG'})\nsns.kdeplot(df_num['citympg'], df_num['price'], cmap=\"Reds\")\nplt.legend()\n\n# Graph aesthetics\nplt.title(\"City MPG vs. Price\", fontname ='Times New Roman', size = 18, color ='purple')\nplt.xlabel(\"City MPG\", fontname ='Times New Roman', size = 15, color ='blue')\nplt.ylabel(\"Price\", fontname ='Times New Roman', size = 15, color ='blue')\n\n# 2nd subplot\nplt.subplot(2, 2, 2)\n# Regression Scatter Plot for Compression Ratio\nsns.regplot(x='highwaympg', y='price', data=df_num, \n            scatter_kws = {'color': 'maroon', 'alpha':0.3,\"s\":100, \"cmap\":\"plasma\", 'marker':'+'},\n            line_kws = {'color': 'darkgreen', 'alpha':0.4, 'lw':4, 'label':'Highway MPG'})\nsns.kdeplot(df_num['highwaympg'], df_num['price'], cmap=\"Blues\")\nplt.legend()\n\n# graph aethetics\nplt.title(\"Highway MPG vs. Price\", fontname ='Times New Roman', size = 18, color ='purple')\nplt.xlabel(\"Highway\", fontname ='Times New Roman', size = 15, color ='blue')\nplt.ylabel(\"Price\", fontname ='Times New Roman', size = 15, color ='blue')\n\n# Set graph styles\nsns.set_style('whitegrid')\nplt.show()\nplt.tight_layout()\n\n\n# print out correlation coefficients\nprint(\"Correlation Coefficient between City MPG and Price is\", round(df_num['citympg'].corr(df_num['price']), 4))\nprint(\"Correlation Coefficient between Highway MPG and Price is\", round(df_num['highwaympg'].corr(df_num['price']), 4))","d0e9155e":"df_num['fueleconomy'] = (0.55*df_num['citympg'])+ (0.45*df_num['highwaympg'])\ndf_num.head()","1bd82b48":"plt.style.use('seaborn')\nplt.figure(figsize=(15, 10))\n\n# Regression Scatter Plot for Compression Ratio\nsns.regplot(x='fueleconomy', y='price', data=df_num, \n            scatter_kws = {'color': 'green', 'alpha':0.3,\"s\":100, \"cmap\":\"plasma\", 'marker':'+'},\n            line_kws = {'color': 'red', 'alpha':0.5, 'lw':4, 'label':'Fuel Economy'})\nsns.kdeplot(df_num['fueleconomy'], df_num['price'], cmap=\"Blues\")\nplt.legend()\n\n# graph aethetics\nplt.title(\"Fuel Economy vs. Price\", fontname ='Times New Roman', size = 18, color ='purple')\n#plt.xlabel(\"Highway\", fontname ='Times New Roman', size = 15, color ='blue')\n#plt.ylabel(\"Price\", fontname ='Times New Roman', size = 15, color ='blue')\n\n# Set graph styles\nsns.set_style('whitegrid')\nplt.show()\n\n\nprint(\"Correlation Coefficient between Fuel Economy and Price is\", round(df_num['fueleconomy'].corr(df_num['price']), 4))","9974dfda":"# Correlation of features vs price\n# Set thrashhold: anything greater or equal to 0.7 is good\nhi_threshold = 0.5\na = abs(df_num.corr()['price'])\noutput_a = pd.DataFrame(a[a >= hi_threshold])\noutput_a","e789eca5":"print(\"Categorical Features are\", list(df_cat.columns))\n\n#export_graphviz(clf, out_file=dot_data, feature_names=feature_names)","3f6163ad":"# Box Plot of All the categorical features\nplt.figure(figsize=(20, 15))\nplt.tight_layout()\nplt.suptitle(\"Features vs Price Box Plots\", fontname ='Times New Roman', size = 50, color ='purple')\nplt.subplot(3, 3, 1)\nsns.boxplot(x = 'doornumber', y = 'price', data = df)\nplt.title(\"Number of Doors vs Price\")\nplt.xlabel(\"Number of Doors\")\nplt.subplot(3,3,2)\nsns.boxplot(x = 'fueltype', y = 'price', data = df)\n#sns.regplot(x='enginesize', y='price', data=df)\nplt.title(\"Fuel Type vs Price\")\nplt.xlabel(\"Fuel Type\")\nplt.subplot(3,3,3)\nsns.boxplot(x = 'aspiration', y = 'price', data = df)\nplt.title(\"Aspiration vs Price\")\nplt.xlabel(\"Aspiration\")\nplt.subplot(3,3,4)\nsns.boxplot(x = 'carbody', y = 'price', data = df)\nplt.title(\"Car Body vs Price\")\nplt.xlabel(\"Car Body\")\nplt.subplot(3,3,5)\nsns.boxplot(x = 'enginelocation', y = 'price', data = df)\nplt.title(\"Engine Location vs Price\")\nplt.xlabel(\"Engine Location\")\nplt.subplot(3,3,6)\nsns.boxplot(x = 'drivewheel', y = 'price', data = df)\n#sns.regplot(x='drivewheel', y='price', data=df)\nplt.title(\"Drive Wheel Type vs Price\")\nplt.xlabel(\"Drive Wheel Type\")\nplt.subplot(3,3,7)\nsns.boxplot(x = 'enginetype', y = 'price', data = df)\nplt.title(\"Engine Type vs Price\")\nplt.xlabel(\"Engine Type\")\nplt.subplot(3,3,8)\nsns.boxplot(x = 'cylindernumber', y = 'price', data = df)\nplt.title(\"Num of Cylinders vs Price\")\nplt.xlabel(\"Number of Cylinders\")\nplt.subplot(3,3,9)\nsns.boxplot(x = 'fuelsystem', y = 'price', data = df)\nplt.title(\"Fuel System vs Price\")\nplt.xlabel(\"Fuel System\")\n\nplt.show()\n","92cdc533":"list(df_cat.columns)","a0989cf9":"# Set figure size\nplt.style.use('seaborn')\nplt.figure(figsize=(15,15))\nplt.tight_layout()\n\n#Set Plot Title for both subplots\nplt.suptitle(\"Symboling vs. Price\", fontname ='Times New Roman', size = 30, color ='purple')\n\n# box Plots\nplt.subplot(3,3,1)\nsns.boxplot(x = 'symboling', y = 'price', data = df, palette=(\"gist_earth_r\"))\n\n# Count \/Bar Plots\nplt.subplot(3,3,2)\nsns.countplot(df.symboling, palette=(\"gist_earth_r\"))\n\n# Pie Chart\nplt.subplot(3,3,3)\nexplode = (0.1, 0.1, 0.1, 0.1, 0.1, 0.4)\nlabels=[\"-1\", \"-2\", \"0\", \"1\", \"2\", \"3\"]\npie = plt.pie(df['symboling'].value_counts(),\n              labels=labels,\n              autopct='%1.0f%%', \n              explode=explode, \n              startangle=90, shadow=True, \n              wedgeprops={'edgecolor': 'red'})\nplt.show()","4466c478":"# Make a dataframe of fuel prices\ndf_fuel = pd.DataFrame(df['fueltype'].value_counts())\n# print out df_fuel\ndf_fuel","f166d8a4":"plt.style.use('seaborn')\nexplode = (0.1, 0.2)\ndf_fuel.plot.pie(y='fueltype', figsize=(6,10), autopct='%1.0f%%', explode=explode, \n                 startangle=90, shadow=True, wedgeprops={'edgecolor': 'red'})\nplt.title(\"Fuel Type\", fontname ='Times New Roman', size = 30, color ='purple')\nplt.tight_layout()","e482f2cb":"# Make a dataframe of fuel prices\ndf_carbody = pd.DataFrame(df['carbody'].value_counts())\n# print out df_fuel\ndf_carbody","42ad0f64":"df_body=pd.DataFrame(df['carbody'].value_counts()).reset_index()\n# create first subplot on the left (1 row, 2 columns, position 1)\nplt.subplot(121)\nexplode = (0.1, 0.1, 0.1, 0.1, 0.4)\nlabels=[\"Sedan\", \"Hatchback\", \"Wagon\", \"Hardtop\", \"Convertible\"]\npie = plt.pie(df['carbody'].value_counts(),labels=labels, \n                autopct='%1.0f%%', explode=explode, \n                startangle=90, shadow=True, wedgeprops={'edgecolor': 'yellow'})\n\n# create second subplot on the right (1 row, 2 columns, position 2)\nmy_colors = 'rgbkymc'  #red, green, blue, black, etc.\nplt.subplot(122)\nbar = df['carbody'].value_counts().plot(kind='bar', color=list(my_colors))\nplt.title(\"Car Body Types in numbers\")\n#plt.xlabel(\"Body type\")\n#plt.ylabel(\"Quantity\")\nplt.show()","45281b62":"# visualize fule type\nplt.style.use('seaborn')\nexplode = (0.1, 0.1, 0.1, 0.1, 0.4)\ndf_carbody.plot.pie(y='carbody', figsize=(8,8), autopct='%1.0f%%', explode=explode, startangle=90, shadow=True, wedgeprops={'edgecolor': 'yellow'})\nplt.title(\"Car Body Type Percentages\", fontname ='Times New Roman', size = 30, color ='purple')","f80b063f":"plt.figure(figsize=(15,10))\nplt.style.use('seaborn')\nsns.pairplot(df[['enginesize', 'price', 'carbody']], hue=\"carbody\");","f4bad98f":"sns.scatterplot(x = car_data['fueleconomy'], y = car_data['price'], hue = car_data['carbody'])\n#plt.xlabel('Fuel Economy')\n#plt.ylabel('Price')\nplt.show()","1ee461d9":"# Import label encoder\nfrom sklearn.preprocessing import LabelEncoder\n\n# Instantiate labelencoder \nle = LabelEncoder()\n\n# Apply le on categorical feature columns\ndf_cat_enc = df_cat.apply(le.fit_transform)","cea84576":"car_data = pd.concat([df_num, df_cat_enc], axis=1, ignore_index=False)\ncar_data.head(2)","a36267d5":"#df[['wheelbase', 'carlength', 'carwidth', 'curbweight','enginesize', 'boreratio', 'horsepower', 'citympg', 'highwaympg', 'price']].corr().T\n\n#df_cardims_corr\nstrong_corr = df_num.corr().round(5)\n\n# Correlation of features vs price only\n# Set thrashhold: anything greater or equal to 0.65 is good\nnum_threshold = 0.5\nf = abs(strong_corr['price'])\noutput_f = pd.DataFrame(f[f>num_threshold])\noutput_f","4bb73c7e":"auto = car_data[['wheelbase', 'carlength', 'carwidth', 'curbweight', \n                 'enginesize', 'boreratio', 'horsepower', 'aspiration', \n                 'carbody', 'enginetype', 'drivewheel', 'cylindernumber', \n                 'fueltype', 'fueleconomy', 'price',]]","b63d1920":"auto.head()","efad2c05":"# Split into target variable and independent variables\nX = auto.drop(['price'], axis = 1)\n\n# target\ny = auto['price']","04ff0486":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=0)","17bbddc9":"#print('-'*48)\nprint('X_train has', X_train.shape[0], 'rows, y_train also has', y_train.shape[0], 'rows')\nprint('-'*48)\nprint('X_test has', X_test.shape[0], 'rows, y_test also has', y_test.shape[0], 'rows')\nprint('-'*48)\nprint('X_train has', X_train.shape[1], 'features; X_test also has', X_test.shape[1], 'rows')","b5743041":"# Create a linear regressor\nlin_reg_mod = LinearRegression()\n\n# Train the model using the X_train  and y_train sets\nlin_reg_mod.fit(X_train, y_train)","62b5757c":"# Model prediction on test data\nlin_reg_y_test_pred = lin_reg_mod.predict(X_test)\n#y_pred_train = lin_reg_mod.predict(X_train)b","5255b666":"# Evaluation Metrics\n# RSquare\nfrom sklearn.metrics import r2_score\n\n# Print RSquare Score for train data\n#print(\"Train data R-squared score: {}\".format(r2_score(y_true = y_train, y_pred = y_pred_train)))\n\n# Print RSquare Score for test data\nprint(\"Test data R-squared score for Linear Regression Model is {}\".format(r2_score(y_test, lin_reg_y_test_pred)))","824394cf":"# Assign DecisionTreeRegressor to the variable \"dt_regressor\"\ndt_regressor = DecisionTreeRegressor(random_state=0)\n# Fit the training data to the model\ndt_regressor.fit(X_train, y_train)","5896242f":"#dt_y_train_pred = dt_regressor.predict(X_train)\n# Predict the results\ndt_y_test_pred = dt_regressor.predict(X_test)","f239bc85":"# Evaluate the Model\ndt_regressor.score(X_test, y_test)\n# Evaluate the model\nprint(\"Test dataset df_regressor score for Decision Tree Regression Model is {}\".format(dt_regressor.score(X_test, y_test)))","1ba551e6":"# Assign the RandomForestRegressor to rf\nrf_regressor = RandomForestRegressor(n_estimators = 100,\n                           criterion = 'mse', \n                           random_state = 0,\n                           n_jobs = -1)\n# Fit the Model with training data\nrf_regressor.fit(X_train, y_train)","54d5fdaf":"# Model Prediction\n#rf_train_pred = rfr.predict(X_train)\nrf_y_test_pred = rf_regressor.predict(X_test)","0319a775":"# Evaluate the model\nprint(\"Test dataset R-squared score for Random Forest Regressor Model is {}\".format(rf_regressor.score(X_test, y_test)))","81757d5a":"print(\"Linear Regression RSquared score is {}\".format(r2_score(y_test, lin_reg_y_test_pred)))\nprint(\"Decision Tree Regression Score is   {}\".format(dt_regressor.score(X_test, y_test)))\nprint(\"Random Forest Regressor Score is    {}\".format(rf_regressor.score(X_test, y_test)))","b3d7082d":"lr_df = pd.DataFrame({'Real Values': y_test, 'Predicted Values': lin_reg_y_test_pred})\ndt_df = pd.DataFrame({'Real Values': y_test, 'Predicted Values': dt_y_test_pred})\nrf_df = pd.DataFrame({'Real Values': y_test, 'Predicted Values': rf_y_test_pred})","93a5fdfb":"fig = plt.figure(figsize=(25, 10))\n#plt.title()\n#plt.subplot(2, 2, 1)\n# Regression Scatter Plot for Compression Ratio\nsns.regplot(x=lr_df['Real Values'], y=lr_df['Predicted Values'], data=lr_df, \n            scatter_kws = {'color': 'green', 'alpha':0.5,\"s\":80, \"cmap\":\"plasma\", 'marker':'+'},\n            line_kws = {'color': 'red', 'alpha':0.5, 'lw':4, 'label':'Linear Reg'})\nsns.kdeplot(lr_df['Real Values'], lr_df['Predicted Values'], cmap=\"Blues\")\nplt.legend()\nsns.regplot(x=dt_df['Real Values'], y=dt_df['Predicted Values'], data=dt_df, \n            scatter_kws = {'color': 'maroon', 'alpha':0.6,\"s\":90, \"cmap\":\"Blues\", 'marker':'+'},\n            line_kws = {'color': 'darkblue', 'alpha':0.7, 'lw':4, 'label':'Decision Tree'})\nsns.kdeplot(lr_df['Real Values'], lr_df['Predicted Values'], cmap=\"Blues\")\nplt.legend()\nsns.regplot(x=rf_df['Real Values'], y=rf_df['Predicted Values'], data=rf_df, \n            scatter_kws = {'color': 'cyan', 'alpha':0.8,\"s\":10, \"cmap\":\"Blues\", 'marker':'+'},\n            line_kws = {'color': 'magenta', 'alpha':0.8, 'lw':4, 'label':'Random Forest'})\nsns.kdeplot(lr_df['Real Values'], lr_df['Predicted Values'], cmap=\"Greens\", shade=False)\nplt.legend()\n\n#plt.subplot(2, 2, 2)\n#plt.bar(lin_reg, lin_reg.values())","b876e47f":"plt.style.use('ggplot')\nfig, ax = plt.subplots(figsize=(20, 5))\n# Linear Reg Model Predictions \nsns.lineplot(x = y_test.index, y = y_test, label = \"Actual\", color = \"blue\", ax = ax)\nsns.lineplot(x = y_test.index, y = lin_reg_y_test_pred, label = \"Predictions\", color = \"red\", ax = ax)\nax.set_title(\"Price: Actual vs Predicted values for Linear Reg Model\", fontname ='Times New Roman', size = 30, color ='purple')\nax.set_xlabel(\"Index\")\nax.set_ylabel(\"Price\")","bbaedb84":"plt.style.use('ggplot')\nfig, ax = plt.subplots(figsize=(20, 5))\n#\nsns.lineplot(x = y_test.index, y = y_test, label = \"Actual\", color = \"blue\", ax = ax)\nsns.lineplot(x = y_test.index, y = dt_y_test_pred, label = \"Predictions\", color = \"red\", ax = ax)\nax.set_title(\"Price: Actual vs Predicted values for Decision Tree Model\", fontname ='Times New Roman', size = 30, color ='purple')\nax.set_xlabel(\"Index\")\nax.set_ylabel(\"Price\")","5239693d":"plt.style.use('ggplot')\nfig, ax = plt.subplots(figsize=(20, 5))\n\nsns.lineplot(x = y_test.index, y = y_test, label = \"Actual\", color = \"blue\", ax = ax)\nsns.lineplot(x = y_test.index, y = rf_y_test_pred, label = \"Predictions\", color = \"red\", ax = ax)\nax.set_title(\"Price: Actual vs Predicted values for Random Forest Model\", fontname ='Times New Roman', size = 30, color ='purple')\nax.set_xlabel(\"Index\")\nax.set_ylabel(\"Price\")","b72cf2d2":"### Different Car Names & Visualization\n#### Number of Cars Sold By Each Auto Company\n- **Which Auto Company seems to be famous or selling more according to our dataset?**","625e8d34":"It becomes apparent from the above DataFrame of \"unique\" Auto Companies that some company names have been mispelled, or short-cut (vw for instance). So we need to rename such accordingly. \n\nHere is the list of typing mistakes:\n1. maxda $\\implies$ mazda \n2. Nissan $\\implies$ nissan\n3. porcshce $\\implies$ porsche\n4. toyouta $\\implies$ toyota\n5. vokswagen $\\implies$ vw $\\implies$ volkswagen\n\nSo let's go ahead and correct the mistakes.","3c6bf523":"We see from the above under the **`Non-Null Count`** that there are 205 entries, as we already saw and that there no missing values for each feature. \n`dtypes` nicely summarizes for us that there are\n> * 8 floating data type features\n\n> * 8 features with integer values and \n\n> * 10 of object data type.\n\n#### MISSING DATA\nLet us check if there are missing values for each feature.\n","25b6c7cf":"#### $Observation$\n- It looks like the prices of cars increase with increasing horsepower.\n- The graph above shows an upward positive trend between horsepower and prices; the correlation coefficient is also approximately 0.8081, which is above the threshold of 0.7.\n- With all these observations, we can safely conclude that horsepower is another feature that directly affects car prices. ","f80d26c1":"---\n### Comparing Real $vs.$ Predicted Values\n- Let's display the values of *y_test* as __\"Real Values\"__ and *lin_reg_y_test_pred* as __\"Predicted Values\"__ in a Pandas DataFrame.","82c87c46":"### Splitting the Data into Training and Testing sets","80a2cca2":"## Data Categorization\nAs we have already seen, our dataset comprises categorical and numerical data; so we want to split features into such for better analysis. \n\n> #### Extracting Numerical Features\nLet's get those features that are of numerical datatype.","63a1e8e3":"#### $Observations$\n   1. -  **`Number of Doors`**: Price seems to not rise based on Number of doors a car has. \n   2. - **`Fuel Type`**: Cars that use Diesel are considerably more expensive; so fuel type does have an effect on price.\n   3. - **`Car Body Type`**: Covertible and Hardbody Car Body types seem relatively expensive compared to hatchback, sedan and wagon.\n   4. - **`Engine location`**: Cars with rear engine location are way more expensive, it's not a lot of them but their prices are way high.\n   5. - **`Drive Wheel Type`**: 4-wheel-drive and forward-wheel-drive cars are cheaper. \n   6. - **`Engine Type`**: $ohcv$ engine type is comparatively more expensive.","0b238f02":"$Observation$:\n\nThere are no NULL values under each feature.","f684ad2d":"# <center>Data Modeling<\/center>\nTaking into account the above, let's have them grouped into one dataframe.","50882087":"### Duplicates\nLet's check if there are duplicate in our dataset.","bf847617":"#### About the features\n-  **What do the features mean?**\n\n * **Car_ID**          : Unique `id` of each car - `Integer`\n * **Symboling**       : Assigned insurance risk rating- +3 indicates risky auto; -3 indicates a less risky auto - `Categorical`\n * **CarName**\t       : Name of the car - `Categorical`\n * **Fueltype**\t       : Type of Fuel a car uses - `Categorical`\n * **Aspiration**      : Aspiration used in a car - `Categorical`\n * **Doornumber**      : Number of doors in a car - `Categorical` \n * **Carbody**         : The body of the car - `Categorical`\n * **Drivewheel**      : Type of drive wheel - `Categorical`\t\n * **Enginelocation**  : Location of a car engine - `Categorical`\t\n * **Wheelbase**\t   : Wheelbase of a car - `Numeric`\n * **Carlength**\t   : Length of the car - `Numeric`\n * **Carwidth**\t       : Width of the car - `Numeric`\n * **Carheight**       : Height of car - `Numeric`\n * **Curbweight**      : The weight of a car without occupants or baggage - `Numeric`\n * **Enginetype**\t   : Type of engine - `Categorical`\n * **Cylindernumber**  : Cylinder placed in the car - `Categorical`\t\n * **Enginesize**      : Size of the car engine - `Numeric`\n * **Fuelsystem**\t   : Fuel system of car - `Categorical`\n * **Boreratio**\t   : Boreratio of a car - `Numeric`\n * **Stroke**\t       : Stroke or volume inside the engine - `Numeric`\n * **Compressionratio**: Compression ratio of a car - `Numeric`\n * **Horsepower**\t   : Horsepower - `Numeric`\n * **Peakrpm**\t       : Car peak rpm - `Numeric`\n * **Citympg**\t       : Mileage in the city - `Numeric`\n * **Highwaympg**\t   : Mileage on highway - `Numeric`\n * **Price**           : Price of the car - `Numeric`; **Dependent variable**\n \n #### Let's try understand the shape of the dataframe\n - i.e, *how many rows and columns in the given dataset?*\n","636149a1":" ###  2. Model Prediction: Predicting the Results\n> - Here we predict the results of the test set with which the model trained on the training set values; \n> - We use `dt_regressor.predict()` function and assign it to the variable \"***dtr_y_test_pred***\".","22eebedb":"### <align right>Bivariate Data Analysis<\/align>","59df050e":"---\n## Random Forest Regressor\n### 1. Training the Decision Tree Regression Model on the training dataset\n> - Assign `RandomForestRegressor` to the variable **\"rf_regressor\"**\n> - Then we fit the ***$X$_$train$*** and ***$y$_$train$*** data to the model using `rf_regressor.fit()` function.","3b7fed3b":"$Observation$:\n\nWe see that regression plots for the predicted values $vs.$ actual price values are pretty similar- more so Decision Tree and Random Forest regressors. ","21bcefc0":"---\n### Symboling\nIn the auto-insurance and auto business, symboling tells us the riskiness of the car.\n- <font color='red'>$\\implies$<\/font> $+3$ indicates that the car is pretty risky\n- <font color='red'>$\\implies$<\/font> $-1$ indicates that the car is pretty safe","bd22359b":"---\n### Peak RMP $vs.$ Price\nRecall that RMP stands for \"Revolutions Per Minute\" and that this is a way of measuring the speed at which the engine revolves or spins, in simplistic terms so to speak. RPM is a measure of two things: \n1. the number of times the engine's crankshaft makes one complete rotation each minute, and simultaneously, \n2. the number of times each piston goes up and down in its cylinder. \nSometimes RPM is called *\u201c$engine$ $speed$.\u201d*\n\n> **$Hypothesis$**: <font color='blue'>Generally, a faster spinning engine (peak speed) generates more power; hence a faster car. We can expect car prices to increase with higher Peak RMP's.<\/font>\n\nLet's go ahead and visualize the relationship and see if it supports our hypothesis.","c37a2e59":"### 3. Model Evaluation","d6d71361":"#### $Observation$ \nWe see the above numerical features can be used in predicting the target. \n\n---\nWe can safely say that the following features play the biggest part in predicting car prices:\n1. Wheelbase\n2. Car length\n3. Car width\n4. Curb weight\n5. Engine size\n6. Horsepower\n7. Fuel economy\n8. Aspiration\n9. Car Body\n10. Engine Type\n11. Brand \n12. Fuel Type\n13. Drive Wheel\n14. Cylinder Number\n15. Bore Ratio","e5559747":"#### $Observations$\n- `carwidth`, `curbweight`, `enginesize`, and `horsepower` seem to have a strong positive correlation with `price`, as per our threshold. \n","4d39a855":"### 3. Model Evaluation\n\n- **R-Square** - $R^2$","0dd851b2":"### Visualizing Models Results","2bb241fe":"#### New Feature \nLet's create a new feature called `fueleconomy` from `citympg` and `highwaympg`. ","7b91c47a":"#### Any missing Values?","99a11377":"---\n## Decision Tree Regressor\n\n### 1. Training the Decision Tree Regression Model\n   > - Assign `DecisionTreeRegressor` to the variable **\"dt_regressor\"**\n   > - Then we fit the ***$X$_$train$*** and ***$y$_$train$*** data to the model using `dt_regressor.fit()` function.","27253b41":"- We indeed see from the above that `car_ID` variable has been dropped from our dataframe. \n\n---\n### Target Variable Outlier Analysis\/Detection\n* **What is Outlier Analysis**\n  - This is the data analysis process of identifying the anomalous observations in a dataset. An outlier is an extreme value that deviates from from other observations in the dataset. \n  \n  - This process removes inaccurate or erroneous observations which might otherwise lead to skewed conclusions, leading to poor business decisions and a potential loss of revenue. ","448d9106":"Now that we have segregated our dataset into different datatypes, let's see how many features are in each categorization.","695afa50":"### 3. Model Evaluation","2bafd708":"#### Unique Entries\n- Let's unique entries in our dataset","31eb3126":"> #### Extracting Categorical Features\nLet's get those features that are of categorical        datatype\n","b7757628":"---\n# <center>Problem Statement<\/center>\n\nA Lesotho automobile company Thabure Autos aspires to expand operations and enter the South African auto market; so the company plans on setting up their manufacturing unit there and producing cars locally to give competition to the already available counterparts in South Africa.\n\nIn their efforts to understand the factors on which the pricing of cars depends, Thabure Autos has contracted Deiv Data Consultancy (DDC), a data consulting firm. Specifically, they want to understand the factors affecting the pricing of cars in the South African market, since those may be very different from the Lesotho market. So the company wants to know:\n\n- Which variables are significant in predicting the price of a car\n- How well those variables describe the price of a car \n- Can we visualize the predicted vs. actual price values\n- Which model yields better scores\n\nBased on various market surveys, the consulting firm has gathered a large data set of different types of cars across the South African market.\n\n\n## Project Goal\n- To predict the price of cars using the available independent variables.\n\nWe could ask questions like:\n * What features affect the price of a car? \n     * Is it the color, the brand, or year of make?\n     * Could it be the horsepower, or perhaps something else?\n\nThe model to predict the prices of cars will be used by management to understand exactly how prices vary with independent variables. \n\nWith the model at hand, Thabure Autos can manipulate the design of the cars, their business strategies, and many more, to meet certain price levels. In addition, the model will be a good way for management to understand the pricing dynamics of a new, foreign market. \n\n\n---\n# Data Preprocessing \n\nThe technique used to transform raw data into useful, understandable and efficient format.\n\nIt can comprise of the following\n1. Importing necessary libraries\n2. Importing the dataset\n3. Data Cleaning\n4. Data Transformation\n5. Data Reduction\n\n## 1. Importing Libraries\nLet's go ahead and import some libraries that we will make use of in our quest to archieve our goal. \n","7cb30ae0":"We see that the correlation is $0.05778$, which is a moderate correlation- this would imply that wheel base does little to pricing a car, but can't be ignored entirely- at least not yet.\n\nThe pairs plot builds on the kde and the scatter plot. \n- The kde on the diagonal allows us to see the distribution of a single variable(price in this case) while \n- The scatter plots on the upper and lower triangles show the relationship (or lack thereof) between two variables. For example, the left-most plot in the last row shows the scatter plot of `wheelbase` $vs.$ `price`.\n","4573d755":"#### $Observations$\n- <font color='red'>$\\implies$<\/font>$90$% of the cars use gas. Now, does fuel type affect the price of a car? Yes, we saw in the boc plots above that it does- diesel cars cost more than gas cars, and the inference we can make even before is from the fact that there are more gas cars than diesel cars, and we know that the cheaper the product, the more users it will attract.","b9e634a1":"- We see that there are outliers in our target `price`: some car prices are above $30, 000$.\n--- \n### Auto Comapy Names \nWe'll notice that in our dataframe, there is no column specifying car company names, just car names. Because we want to know which auto company is famous or sells more, we will want to **extract company name from car names**. Let's go ahead and do just that. ","2161d674":"#### $Observation$\n- We see that car height has very weak correlation with price- we could say that car pricing does not depend much on car height.\n- Let's get a visualization for better understanding:","65705885":"## 3. Data Cleaning \n**What is Data Cleaning?**\n\nIn Data Science, data cleaning can be described in many ways, one of them being: ***`the process of fixing and\/or removing incorrect, corrupted, wrongly-formatted, incomplete or duplicate data within a dataset`***. \n\nQuite mouthful and yet comprehensive. We want to ensure that our dataset has no duplicate data or does not contain any corrupted entries that will otherwise lead to wrong\/less useful models. \n\nAs we make use of multiple data sources in data analysis, the chances of duplicating data are very high, and so are those of mislabeling the data. It goes without saying that incorrect data leads to unreliable algorithms and predictions or outcomes. While there are no sure steps of going about data cleaning due to the nature of different datasets, it is however vital to build some sort of a framework or template for data cleaning process for future references so as to at least be close to doing it right each time. \n\n### 3.1 Exploring The Dataset\n- **Understanding the data**\n\nWith the data fed into our notebook, it's time we take a look at it: we need to spend some time exploring it in order to understand what the features represent in each column. We want to avoid or at least minimize mistakes in the data analysis and the modeling process. \n\nWithout any further ado, let's dive into our loaded dataset.","e2673351":"--- \n### Bore Ratio $vs.$ Price\n**<font color='red'>What is bore-hole ratio, really?**\n- To better understand how this feature might be connected to pricing, we might want to have a little background on it first- what it really means!\n\nWhile there are many factors that contribute to an engine\u2019s efficiency, the primary factor that needs to be considered is the engine geometry itself. Not only does the overall size of the engine matter, but the aspect ratio of the engine cylinders \u2014 defined by the `stroke-to-bore ratio` \u2014 also matters. According to [wikipedia](https:\/\/en.wikipedia.org\/wiki\/Stroke_ratio), stroke ratio, defined by either **bore\/stroke ratio** or **stroke\/bore ratio**, is a term used to describe the ratio between cylinder bore diameter and piston stroke length. \n- Do notice that we don't speak about bore without the mention of stroke, so these two must be very close. \n![Bore_vs_Stroke_Ration](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/b\/bf\/Bore_Stroke_Ratio_Animation.gif)\n\nSo let's ask ourselves this in order to make some hypothesis about Bore Ratio and Stroke: <font color=\"red\">How does bore and stroke affect engine performance?<\/font>\n\n> ***$Hypothesis$***: <font color='blue'> A bigger bore with a shorter stroke also allows an engine to rev higher, which creates more horsepower, leading to a faster car that's not so fuel efficient. Conversely, a long stroke is generally better for fuel efficiency, because it reduces surface area during combustion.<\/font>","7e273b2c":"--- \n### Car Body Type Visualization","a82446b3":"#### $Observation$\nOn average, hardtop cars are more expensive, closely followed by convertibles. This supports our earlier assumption that hatchbacks sell more because they are cheap, maybe not because of safety or other factors.\n","5a48e4ec":"#### $Observation$\n- The higher the fuel consumption rate on either the highway or in the city, the less is the price. It makes sense that there's negative correlations in these features: Higher values imply the car isn't fuel efficient, or lower values imply that the car is fuel efficient so the price will be higher. We all want cars that can save us some gas when we are driving in the city with lots of stops and *go's*; so it makes sense to put higher price tags on such cars. \n- But from the plots above we see that for city mpg most consumers prefer anything in the $[15-35]$ mpg range, and for highway, $[20-45]$ mpg range. ","37925b9e":"##### Available Auto Companies in our Dataset\nLet's take a look at which auto companies with have in  our dataset. \n","6d31291b":"#### $Observation$\n- We notice a very weak negative correlation beween price and peak rpm. Now, this is not to say RPM plays no part in pricing, it affects other features that directly affect pricing. So in a way it indirectly affects car prices.\n\nSo we reject our hypothesis.","b57e448d":"## Correlations\n### Meaning Of Positive, Negative, and Zero Correlation Coefficients \n\nCorrelation coefficients are indicators of the strength of the linear relationship between two different variables, $x$ and $y$. \n   - A linear correlation coefficient that is greater than zero indicates a positive relationship. \n   - A value that is less than zero signifies a negative relationship. \n   - Finally, a value of zero indicates no relationship between the two variables $x$ and $y$.\n\n**IMPORTANT**: When interpreting correlation, it's important to remember that just because two variables are correlated, it $does$ $not$ mean that one causes the other.\n![image.png](attachment:image.png)\n\n\n### Features Correlation\nLet us dig deeper: let us try find the correlation between the features with respect to `price`. \n\nWe will pick some features at random and try uncover some useful insights, not losing focus of our end-goal: **Car Pricing.**\n \n#### <center>Visualizing Numerical Features<\/center>\nLet's list features we classified as numerical and some explanations of some we may not fully understand:\n<ol>\n     <li>$wheelbase$: <i><font color='blue'>the horizontal distance between the centers of the front and rear wheels<\/font>\n         <\/i>\n     <\/li>\n     <li>$carlength$<\/li>\n     <li>$carwidth$<\/li>\t\n     <li>$carheight$<\/li>\t\n     <li>$curbweight$: <i><font color='blue'>simply the weight of the vehicle while it's resting on the curb and not in use;<\/font>\n     <\/i><\/li>\n     <li>$enginesize$: <i><font color='blue'>also known as engine capacity,  the total volume of the cylinders in the engine;<\/font>\n     <\/i>\n     <\/li>\n     <li>$boreratio$: <i><font color='blue'>the cylinder bore is the diameter of each cylinder<\/font>\n     <\/i>\n    <\/li>\n    <li>$stroke$: <i><font color='blue'>the shorter the stroke, the more power it can make and so the faster is the car. We'll see if that makes the cost of the car higher or lower<\/font>\n     <li>$compressionratio$: <i><font color= 'blue'>Higher compression ratios and combustion efficiency mean more power with less fuel, so an energy-saving car. Will those kinds be more costly or cheap?<\/font>   \n      <\/i>\n        <\/li>\n      <li>$horsepower$<\/li>\n      <li>$peakrpm$: <i><font color='blue'>$rmp$ measures how many times the engine\u2019s crankshaft makes one full rotation every minute, and along with it, how many times each piston goes up and down in its cylinder; in simple terms, rmp stands for revolutions per minute, and it\u2019s used as a measure of how fast any machine is operating at a given time. Will it affect the price? Let's find out.<\/font>\n      <\/i><\/li>\n      <li>$citympg$:<i><font color='blue'>$mpg$ stands for miles per gallon- the distance, measured in miles, that a car can travel per gallon of fuel. It measures car's fuel efficiency, higher a car's MPG, the more fuel efficient it is. So $City$ $MPG$ is the score a car will get on average in city conditions, with stopping and starting at lower speeds.<\/font>\n      <\/i><\/li>\n      <li>$highwaympg$: <i><font color='blue'>the average a car will get while driving on an open stretch of road without stopping or starting, typically at a higher speed.<\/font>\n      <\/i><\/li>\n<ol>\n    \n    \n<body>With a better understanding of what all this auto jagon means, let's try to see how these impact the price. $Remember$, we want to see which features affect the price so we can keep or discard them. \n<\/body>\n","317d075c":"---\n### Feature Filtering\n#### Dropping Features Not Related to the Target\/Dependent Variable\n\nNot all features are are correlated with the dependent variable, and from our list of features `car_ID` stands out. So, we will have to drop it as it serves as a mere reference; thus can be dropped.","0941c2e7":"- Indeed! We have successfully corrected the typing mistakes. ","f4ef964f":"#### Visualization of the above","24a17be8":"### Wrapping Up on Numerical Features\n#### Features with Strong Correlations to Price\nBelow is a dataframe showing features with Strong Correlations to Price.\n\n><font color='red'>Statistically, or generally, a value greater than $0.7$ is considered a **strong correlation**. Anything between $0.5$ and $0.7$ is a **moderate correlation**, and anything less than $0.4$ is considered a **weak** or **no correlation**.<\/font>\n\nSo it will make sense to set our thrashhold at 0.5 for moderate to strong correlations","abef2866":"#### $Observation$\n- <font color='red'>$\\implies$<\/font> We notice that cars rated $0$ and $1$ sell more\n- <font color='red'>$\\implies$<\/font> We also notice that cars rated $-1$ are high-priced as those are highly regarded in terms of risk (they are \"very safe\" so to speak).\n- <font color='red'>$\\implies$<\/font> Not many cars rated $-2$ sold much, perhaps because they are also very expensive as -2 insurance rating is also pretty high.\n","9b3ef597":"### 2. Model Prediction on Test Data\n> - Here we predict the results of the test set \n> - We use `lin_reg_mod.predict()` function and assign it to the variable \"***lin_reg_y_test_pred***\".","a09d8819":"### Wheel Base $vs.$ Price\nLet's see how wheel base impacts car pricing:\n\n**<font color='red'>Scatter Plot and Linear Regression Model Fit<\/font>**","0c09fbf1":"# <center>Conclusion<\/center>\n- After using 3 different models- Linear Regression Model, Decision Tree Regression Model and Random Forest Regression Model, we see that Random Forest Regressor scored about 90.82%, which would be the model of choice for this project. \n\n---\n---","3593e48d":"#### $Observation$\nNow that we have successfully encoded our categorical data and concartenated them with numerical data, we shall go ahead into the next big step... Feature Selection!","935bac4a":"--- \n---\n# <center>Visualizing Categorical Features $vs.$ Price<\/center>\nWe will print out what we set as categorical features first, then use Box plots to further see which features might be affecting the price.\n\n### Box Plot of All the categorical features $vs.$ Price\nFirst, let's try understand box plots. The illustration below serves to offer labels, which will be followed by descriptive narrations.\n\n\n![Labelled Box Plot](attachment:image.png)\n\n#### Definitions\n1. **Median**\n   \n   The median (middle quartile) marks the mid-point of the data and is shown by the line that divides the box into two parts. Half the scores are greater than or equal to this value and half are less.\n   \n   \n2. **Inter-Quartile Range**\n\n    The middle \"box\" represents the middle 50% of scores for the group. The range of scores from lower to upeer quartile is referred to as the inner-quartile range. The middle 50% of the scores fall within the inter-quartile range.\n   \n   \n3. **Upper Quartile**\n\n   75% of the scores all fall below the upper quartile.\n   \n   \n4. **Lower Quartile**\n\n   Twenty-five percent of the scores fall below the lower quartile.\n  \n  \n5. **Whiskers**\n\n   The upper and lower whiskers represent scores outside the middle 50%. Whiskers often (but not always) stretch over a wider range of scores than the middle quartile groups. \n  \n\nNow that we have an idea of what the labels mean in box plots, let us try visualize all our categorical features to get an idea of their effect on pricing.","ce4a8b29":"---\n---\n## <center>Feature Engineering<\/center>\nAs we have already seen, we have categorical features in our dataset. However, our machine learning algorithm can only read numerical values; so it is of paramount importance to encode those features into numerical values. \n\nWe will make use of LabelEncoding and OneHotEncoder for their various benefits over other ways like <i><font color='red'>DictVectorizer<\/font><\/i> and <i><font color='red'>Pandas `get_dummies`<\/font><\/i>.\n\nLet's get on it right away.","b4fa11c5":"### Horsepower $vs.$ Price\n**<font color='red'>What is horsepower in the auto industry?**\n\n- According to [Toyota](https:\/\/www.toyota.ca\/toyota\/en\/connect\/3887\/what-is-horsepower#:~:text=Horsepower%20refers%20to%20the%20power,takes%20to%20do%20the%20work), horsepower is the power that an engine produces.\n- So at this point we ask questions like: <font color='red'>Is it more expensive to make an engine with more horsepower, which in turn makes the cars more expensive? Or horsepower doesn't matter?<\/font> \n- > ***$Hypothesis$***: <font color='blue'>The more power, the faster the car, and so the more it will cost.<\/font>\n\nLet's find out with the help of visualization. ","80d81366":"Luckily enough, this dataset has no missing values; this rearely happens however- data rarely come ready to use. Datasets, whether big or small, come with myriad kinds of issues- it be missing values, invalid fields, repeated fields or entries, wrong datatype compared to the rest of other entries in the same column, you name them. \n\nThat said, in this dataset there seems to be no immediate work to do to clean data, so we move on.\n#### Statistical Summary of the dataset","3c8fd2bd":"- <font color='red'>Indeed, the regression line above is almost horizontal, which means auto prices are not too dependent on the car height.<\/font>","0fb94952":"#### Information About the Dataset\nNow that we know how entries and features we are looking at, let's try learn more about the features themselves- their data types: are there integer values, object or floating data types?","9dfaa449":"## 2. Loading the dataset","a884d908":"So train sets match, with $143$ entries, and <i>`X_train`<\/i> and <i>`X_test`<\/i> also match, with $14$ rows. \n\n---\n# <center>Data Modeling<\/center>\n## Linear Regression Model\n### 1. Training the Decision Tree Regression Model\n   > - Assign `LinearRegresson` to the variable **\"lin_reg-mod\"**\n   > - Then we fit the ***$X$_$train$*** and ***$y$_$train$*** data to the model using `lin_reg_mod.fit()` function.","19316f37":"--- \n### Curb Weight $vs.$ Price\n- Let's see if there is any causal effect from curb weight to pricing of cars. \n- We want to find out of weight of a car makes it cheap or expensive. ","8d845f08":"---\n---\n# <center>Exploratory Data Analysis<\/center>\nExploratory Data Analysis (EDA) is the philosophy of analyzing datasets to to summerize their main characteristics, usually employing graphs and data vizualization techniques[ [1] ](https:\/\/en.wikipedia.org\/wiki\/Exploratory_data_analysis).\n\nThis is the critical first step in analyzing the data; here are some reasons for using EDA:\n  * Detection of mistakes\n  * Determining relationships among the features\n  * Assessing assumptions on which statistical inference will be based, etc. \n  \nIt is worth noting that EDA is not really a formal process with rigid set of rules or path to follow, it is waht one makes it. The aim is to uncover whatever may be hidden in the data, so one should feel free to investigate whatever idea that comes to mind. Of course some ideas will yield some positive outcomes, others not so much. To successfully perform data cleaning, we'll need to deploy EDA tools such as visualisation, transformation and modelling. \n  \n","279b69e8":"---\n### Engine Size $vs.$ Price\n- Let's see if the size of engine has some impact on the pricing of the car. \n- Just a reminder of what we are talking about when referring to the engine size: <font color='red'>also known as engine capacity,  the total volume of the cylinders in the engine.<\/font>\n- We ask ourselves: do cars with bigger engine sizes cost more? Do we recommend the company to build cars with smaller or bigger engines in order to attract buyers, or all those don't matter based on how little the engine size may impact the cost? Let's find out. \n    \n    \nAs we have been borrowing the aide of visual plots to see some interesting patterns in our data before, we will continue that even in this section, combined with correlation coefficient(s). ","3704bac6":"### Changing datatypes of some features\n- **`Symboling`**\n> It is categorical in nature so we will have to change the entries from integer to string datatype","0cd6ed09":"---\n### Car Length, Width & Height $vs.$ Price\n#### Pairplots","d83564a8":"#### $Observation$\n- We notice that Bore Ratio has a medium correlation coeficient, which means Bore Ration doesn't have much impact on the pricing of cars.\n- We also notice that Stroke and Compression have weakest correlation coefficients to price: their impact on the pricing is almost negligible- in the feature selection we can choose to leave them out as they show almost 0 impact on pricing. Thier regression lines tell as a lot as they seem to be almost horizontal, which is a sign of 0 correlation by standard. ","f6e46cab":"With the above visualization, seaborn calculated and plotted a linear regression model line of fit, along with a translucent 95% confidence interval band. We can set the confidence interval to any integer in $[0, 100]$, or None.","ca7722d3":"#### $Observation$\n- We can observe that the regression line is a bit steep, which implies there is a positive correlation between curb weight and price. \n- We see that the correlation coefficient is about $0.8741$, which is above the normal threshold of $0.7$.\n- **Inference**: So we see that the bigger the engine size, the more a car costs.","455d2172":"---\n### Gas Mileage $vs.$ Price\nUnder gas mileage, we will look at the following features from our dataset:\n> 1. City MPG, \n> 2. Highway MPG\n\nWe want to see if gas mileage affects car pricing: what are these \"good mileage\" cars we always hear other people talk about? If mileage is important to consumers, will pricing respond the same- good mileage leading to higher prices?\n","30e8811e":"#### Heat Maps","7bb5c961":"Let's check if the changes took effect, let's concatenate them together for easier comparison so we can see if the changes happened. ","afb80773":"Let's check the sizes of the training and testing sets. ","e75a4293":"We see that:\n- Toyota is the highest selling car, a competition to beat for many manufacturers \n- Mercury isn't so popular. ","ab819e80":"---\n### Fuel Type Visualization","03b5d309":"### 2. Model Prediction: Predicting the Results\n> - Here we predict the results of the test set with which the model trained on the training set values; \n> - We use `regressor.predict()` function and assign it to the variable \"***rf_y_test_pred***\".","7addf89a":"#### $Observation$\n- We can observe that the regression line is a bit steep, which implies there is a positive correlation between curb weight and price. \n- We see that the correlation coefficient is about $0.8353$, which is above the normal threshold of $0.7$.\n- So, maybe later on in our feature selection we will have to keep this feature to better predict what prices can be set by a new Auto Firm we are consulting for. ","f0c6ec8f":"---\n## <center>Feature Selection<\/center>\nThis is the process of selecting features which are most relevant in predicting the output variable.\n- <font color='red'>$\\implies$<\/font> It helps reduce data dimensionaluty and \n- <font color='red'>$\\implies$<\/font> Ensures that models' accuracy can be trusted when those features are out. Learn more [here](https:\/\/en.wikipedia.org\/wiki\/Feature_selection).\n\nAfter looking at both numerical and categorical variables, we can be certain about how some of those features affect car pricing.\nWe saw that the following numerical features have medium to storng correlations with price:\n\n- \n    1. Wheelbase\n    2. Car length\n    3. Car Width\n    4. Curb weight\n    5. Engine Size\n    6. Bore Ratio \n    7. Horsepower\n    8. Fuel Economy\n\n### <i>Variance Threshold<\/i>\nIn this project we will set threshold to 0.65; so any feature with a correlation coefficient greater than or equal to $0.65$ shall be labelled a **<font color='red'>$Strong$ $Feature$<\/font>** and make a cut in our feature selection process. \n\nLet's go ahead and see which features have a correlation coefficient $\\geq$ $0.5$ among the numerical features."}}