{"cell_type":{"e38e4543":"code","3d051c62":"code","14908302":"code","50de6ec5":"code","a14f88a9":"code","27502e73":"code","77f64f93":"code","8f1943f3":"code","7d00ffb1":"code","e70cbb4c":"code","db2e9ba8":"code","fdac7d86":"code","09c47c37":"code","95de6dcb":"code","ce893a94":"code","aba96d14":"code","1c30828c":"code","d736448d":"code","57c34f53":"code","d9696522":"code","0e0e263e":"code","299b6f34":"code","a7c46a83":"markdown","bb8764d9":"markdown","ced5e1a9":"markdown","18d99ca9":"markdown","bef2d386":"markdown","2728389a":"markdown","f7bd0172":"markdown","d1e041f0":"markdown"},"source":{"e38e4543":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\nimport pickle\nimport zipfile\nimport csv\nimport sys\nimport os\nimport cv2\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint\nfrom tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras import optimizers\n\nfrom tensorflow.keras.models import Model\nimport tensorflow.keras.models as M\nfrom tensorflow.keras.layers import *\nimport tensorflow.keras.layers as L\n\nfrom tensorflow.keras.applications.xception import Xception\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\n\nimport PIL\nfrom PIL import Image, ImageOps, ImageFilter\n\n#\u0443\u0432\u0435\u043b\u0438\u0447\u0438\u043c \u0434\u0435\u0444\u043e\u043b\u0442\u043d\u044b\u0439 \u0440\u0430\u0437\u043c\u0435\u0440 \u0433\u0440\u0430\u0444\u0438\u043a\u043e\u0432\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 8, 4\n\n#\u0433\u0440\u0430\u0444\u0438\u043a\u0438 \u0432 svg \u0432\u044b\u0433\u043b\u044f\u0434\u044f\u0442 \u0431\u043e\u043b\u0435\u0435 \u0447\u0435\u0442\u043a\u0438\u043c\u0438\n%config InlineBackend.figure_format = 'svg' \n%matplotlib inline\n\nprint(os.listdir(\"..\/input\"))\nprint('Python       :', sys.version.split('\\n')[0])\nprint('Numpy        :', np.__version__)\nprint('Tensorflow   :', tf.__version__)\nprint('Keras        :', tf.keras.__version__)","3d051c62":"!pip freeze > requirements.txt","14908302":"EPOCHS               = 20 # \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u044d\u043f\u043e\u0445 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f\nBATCH_SIZE           = 128 # \u0435\u0441\u043b\u0438 \u0441\u0435\u0442\u044c \u0431\u043e\u043b\u044c\u0448\u0430\u044f, \u0443\u043c\u0435\u043d\u044c\u0448\u0430\u0435\u043c batch, \u0438\u043d\u0430\u0447\u0435 \u043d\u0435 \u0432\u043b\u0435\u0437\u0435\u0442 \u0432 \u043f\u0430\u043c\u044f\u0442\u044c \u043d\u0430 GPU\nLR                   = 1e-4 # learning rate\nVAL_SPLIT            = 0.2 # \u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043f\u0440\u043e\u0446\u0435\u043d\u0442\u043e\u0432 \u0432\u044b\u0434\u0435\u043b\u044f\u0435\u043c \u043d\u0430 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044e\n\nCLASS_NUM            = 100 # \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u043b\u0430\u0441\u0441\u043e\u0432\nIMG_SIZE             = 200 # \u0440\u0430\u0437\u043c\u0435\u0440 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438\nIMG_CHANNELS         = 3 # \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u0430\u043d\u0430\u043b\u043e\u0432 (3, \u0435\u0441\u043b\u0438 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0430 \u0446\u0432\u0435\u0442\u043d\u0430\u044f)\ninput_shape          = (IMG_SIZE, IMG_SIZE, IMG_CHANNELS)\n\ndata_emo = '..\/input\/emotion-detection-fer\/'\nPATH = \"..\/working\/flower\/\"","50de6ec5":"os.makedirs(PATH,exist_ok=True)\n\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)  \nPYTHONHASHSEED = 0","a14f88a9":"os.listdir(data_emo)","27502e73":"# \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u0435 \u0444\u043e\u0442\u043e\u0433\u0440\u0430\u0444\u0438\u0438\n\ndef see_sample_images(age, number_of_images):\n    plt.figure(figsize=(6,6))\n    age_folder = data_emo +'train\/surprised'\n    images = os.listdir(age_folder)[:number_of_images]\n    for i in range(number_of_images):\n        file = mpimg.imread(age_folder +'\/'+ images[i])\n        plt.subplot(number_of_images\/2,2,i+1)\n        plt.imshow(file)","77f64f93":"see_sample_images('034', 4)","8f1943f3":"image = PIL.Image.open(data_emo +'train\/surprised\/im2.png')\nimgplot = plt.imshow(image)\nplt.show()\nimage.size","7d00ffb1":"train_datagen = ImageDataGenerator(rescale=1. \/ 255, \n                                    rotation_range = 30,\n                                    shear_range=0.1,\n                                    #zoom_range=[0.75,1.25],\n                                    brightness_range=[0.5, 1.5],\n                                    width_shift_range=0.1,\n                                    height_shift_range=0.1,\n                                    horizontal_flip=True,\n                                    #validation_split=0.2\n                                  )\n\n\ntest_datagen = ImageDataGenerator(rescale=1. \/ 255)","e70cbb4c":"# data generators\ntrain_generator = train_datagen.flow_from_directory(\n        data_emo + 'train\/',\n        target_size=(IMG_SIZE, IMG_SIZE),\n        batch_size=BATCH_SIZE,\n        class_mode='categorical',\n        shuffle=True, \n        seed=RANDOM_SEED,\n        #subset='training'\n)\n\ntest_generator = test_datagen.flow_from_directory(\n        data_emo + 'test\/',\n        target_size=(IMG_SIZE, IMG_SIZE),\n        batch_size=BATCH_SIZE,\n        class_mode='categorical',\n        shuffle=True, \n        seed=RANDOM_SEED,\n        #subset='validation'\n)","db2e9ba8":"from skimage import io\n\ndef imshow(image_RGB):\n  io.imshow(image_RGB)\n  io.show()\n\nx,y = train_generator.next() # \u0432\u044b\u0437\u044b\u0432\u0430\u0435\u043c \u0442\u0440\u0435\u0439\u043d-\u0433\u0435\u043d\u0435\u0440\u0430\u0442\u043e\u0440\nprint('\u041f\u0440\u0438\u043c\u0435\u0440 \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a \u0438\u0437 train_generator')\nplt.figure(figsize=(12,8))\n\nfor i in range(0,6):\n    image = x[i]\n    plt.subplot(3,3, i+1)\n    plt.imshow(image)\n\nplt.show()","fdac7d86":"x,y = test_generator.next() # \u0432\u044b\u0437\u044b\u0432\u0430\u0435\u043c test-\u0433\u0435\u043d\u0435\u0440\u0430\u0442\u043e\u0440\nprint('\u041f\u0440\u0438\u043c\u0435\u0440 \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a \u0438\u0437 test_generator')\nplt.figure(figsize=(12,8))\n\nfor i in range(0,6):\n    image = x[i]\n    plt.subplot(3,3, i+1)\n    plt.imshow(image)\n\nplt.show()","09c47c37":"\nfrom keras.applications.mobilenet import MobileNet","95de6dcb":"checkpoint = ModelCheckpoint('best_model_e.hdf5', \n                             monitor = ['val_accuracy'], \n                             verbose = 1, \n                             mode = 'max')\n\nearlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n                                             patience=3, \n                                             restore_best_weights=True)\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.25,\n                              patience=2, min_lr=0.0000001, verbose=1,\n                             mode='auto')\n\ncallbacks_list = [checkpoint,earlystop,reduce_lr]","ce893a94":"base_mobilenet_model = MobileNet(weights = None,\n                                 include_top = False,\n                                 input_shape =  input_shape \n                                 )\n\nmodel_e = M.Sequential()\nmodel_e.add(L.BatchNormalization(input_shape =  input_shape))\nmodel_e.add(base_mobilenet_model)\nmodel_e.add(L.BatchNormalization())\nmodel_e.add(L.GlobalAveragePooling2D())\nmodel_e.add(L.Dropout(0.25))\nmodel_e.add(L.Dense(1, activation='linear'))\n\nmodel_e.compile(loss=\"categorical_crossentropy\", \n               optimizer=optimizers.Adam(lr=LR), \n               metrics=[\"accuracy\"],\n               )","aba96d14":"model_e.summary()","1c30828c":"LR = 0.01\n\nhistory = model_e.fit(\n            train_generator,\n        steps_per_epoch = train_generator.samples\/\/train_generator.batch_size,\n        validation_data = test_generator, \n        validation_steps = test_generator.samples\/\/test_generator.batch_size,\n        epochs = EPOCHS,\n        callbacks = callbacks_list)","d736448d":"# \u0441\u043e\u0445\u0440\u0430\u043d\u0438\u043c \u0438\u0442\u043e\u0433\u043e\u0432\u0443\u044e \u0441\u0435\u0442\u044c \u0438 \u043f\u043e\u0434\u0433\u0440\u0443\u0437\u0438\u043c \u043b\u0443\u0447\u0448\u0443\u044e \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u044e \u0432 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0438 (best_model)\nmodel_e.save('..\/working\/model_e_fold_1')\nmodel_e.save('..\/working\/model_e_last_1.hdf5')\nmodel_e.load_weights('best_model_e.hdf5')","57c34f53":"scores = model_e.evaluate_generator(test_generator, steps=len(test_generator), verbose=1)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))","d9696522":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n \nepochs = range(len(acc))\n \nplt.plot(epochs, acc, 'b', label='Training acc')\nplt.plot(epochs, val_acc, 'r', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n \nplt.figure()\n\nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","0e0e263e":"import numpy as np\nfrom sklearn import metrics\n\nx, y = test_generator.next()\nprediction = model_e.predict(x)\n\npredict_label1 = np.argmax(prediction, axis=-1)\ntrue_label1 = np.argmax(y, axis=-1)\n\ny = np.array(true_label1)\n\nscores = np.array(predict_label1)\nfpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=9)\nroc_auc = metrics.auc(fpr, tpr)","299b6f34":"plt.figure()\nlw = 2\nplt.plot(fpr, tpr, color='darkorange',\n lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic (ROC)')\nplt.legend(loc=\"lower right\")\nplt.show()","a7c46a83":"# Emotion recognition","bb8764d9":"## Load and EDA","ced5e1a9":"\u0420\u0430\u0437\u043c\u0435\u0440 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0445 \u043a\u0430\u0440\u0438\u0442\u043d\u043e\u043a - 48 x 48","18d99ca9":"\u0410\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044f \u043d\u0430 \u043c\u0435\u0441\u0442\u0435","bef2d386":"\u0410\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u043d\u0435\u0442 - \u0432\u0441\u0435 \u043d\u043e\u0440\u043c","2728389a":"# Making model","f7bd0172":"### Model","d1e041f0":"### MobileNet"}}