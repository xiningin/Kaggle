{"cell_type":{"e42e77b7":"code","1bca33db":"code","3ba820ca":"code","289e94c6":"code","99690775":"code","d7732a46":"code","40bb5d3a":"code","a9161698":"code","43c57223":"code","08bf8383":"code","bd17f672":"code","30917ef2":"code","e5fd2803":"code","9312069e":"code","ccb0ad37":"code","46dc4b80":"code","cee0142c":"code","e77b1751":"code","56411509":"code","e8e64b83":"code","2c2b7da8":"code","1e214b0e":"code","f75defab":"code","681d799d":"code","553bcb81":"code","6df478ce":"code","f8f88584":"code","a49c95cd":"code","90ae1d1f":"code","86b84223":"code","40b43755":"code","f14cf273":"code","59e5287b":"code","f8e1be6c":"code","5349be41":"code","1f644a5d":"code","4745c340":"code","0b1e47c8":"code","4f3b850c":"code","ae04ce06":"code","305866eb":"markdown","fe3d1105":"markdown","0aa2e44a":"markdown","c3340fbb":"markdown","0305ac21":"markdown","0e904383":"markdown","f06c3f22":"markdown","28300b85":"markdown","a0ae4d1d":"markdown","e45095b9":"markdown","047db49e":"markdown","ced905af":"markdown","8f3da2fa":"markdown","9f3a8ff5":"markdown","abecbc5c":"markdown","09f0f4b8":"markdown","f726da10":"markdown","94eae612":"markdown","023270d8":"markdown","d53041d5":"markdown","2e7846f8":"markdown","82d8b3e0":"markdown","58ed9213":"markdown","b79985a9":"markdown","74c4bfd6":"markdown","03bf11ff":"markdown","4838952c":"markdown"},"source":{"e42e77b7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1bca33db":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport missingno as msno\n\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor, \\\n    AdaBoostRegressor, BaggingRegressor, ExtraTreesRegressor, HistGradientBoostingRegressor, StackingRegressor\nfrom sklearn.preprocessing import RobustScaler, MinMaxScaler, LabelEncoder, StandardScaler\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score, cross_validate, train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport time\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\n#from ngboost import NGBRegressor\n#from ngboost.distns import Normal\n\n\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_colwidth', 100)\npd.set_option('display.max_rows', 50)\npd.set_option('display.width', 5000)\npd.set_option('display.expand_frame_repr', False)\npd.set_option('display.float_format', lambda x: '%.4f' % x)\n","3ba820ca":"def load_house():\n    global train, test, df\n    train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\n    test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\n    df = train.append(test).reset_index(drop=True)","289e94c6":"load_house()","99690775":"df.head()","d7732a46":"def check_df(dataframe, head=5):\n    \n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n    \n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    \n    print(\"##################### NA #####################\")\n    print(dataframe.isnull().sum())\n    \n    print(\"##################### Quantiles #####################\")\n    print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)","40bb5d3a":"check_df(df)","a9161698":"def grab_col_names(dataframe, cat_th=10, car_th=20, excluded=None):\n\n    excluded = [] if excluded is None else excluded\n    df_cols  = [i for i in dataframe.columns if i not in excluded]\n\n    # cat_cols, cat_but_car\n    cat_cols = [col for col in df_cols if dataframe[col].dtypes == \"O\"]\n    num_but_cat = [col for col in df_cols if dataframe[col].nunique() < cat_th and\n                   dataframe[col].dtypes != \"O\"]\n    cat_but_car = [col for col in df_cols if dataframe[col].nunique() > car_th and\n                   dataframe[col].dtypes == \"O\"]\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    # num_cols\n    num_cols = [col for col in df_cols if dataframe[col].dtypes != \"O\"]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n    return cat_cols, num_cols, cat_but_car, num_but_cat","43c57223":"cat_cols, num_cols, cat_but_car, num_but_cat = grab_col_names(df)\n\ndfx = df.copy()\ndfx['SalePrice'] = pd.qcut(dfx['SalePrice'], 3, ['low', 'mid', 'high'])\n","08bf8383":"cat_cols, num_cols, cat_but_car, num_but_cat = grab_col_names(df)\n\ndfx = df.copy()\ndfx['SalePrice'] = pd.qcut(dfx['SalePrice'], 3, ['low', 'mid', 'high'])\nnum_cols = [col for col in num_cols if col not in [\"SalePrice\", \"Id\"]]\n\nfor i in num_cols:\n    for ii in num_cols:\n        if i != ii and i:\n            sns.scatterplot(x=i, y=ii, data=dfx, hue=dfx['SalePrice'].tolist(), s=50, alpha=0.7)\n            plt.show()\n            \nlist(sorted(num_cols))","bd17f672":"corr_matrix = df.corr()\nsns.clustermap(corr_matrix, annot=True, figsize=(20, 15), fmt=\".2f\")\nplt.title(\"Correlation Between Features\")\nplt.show()\n\nthreshold = 0\nfiltre = np.abs(corr_matrix[\"SalePrice\"]) > threshold\ncorr_features = corr_matrix.columns[filtre].tolist()\nsns.clustermap(df[corr_features].corr(), annot=True, fmt=\".2f\")\nplt.title(\"Correlation Between Features w\/ Corr Threshold 0.75)\")\nplt.show()","30917ef2":"def find_correlation(dataframe, numeric_cols, corr_limit=0.60):\n    \n    \"\"\"\n    Correlations of target and independent variables\n    \n    \"\"\"\n    high_correlations = []\n    low_correlations = []\n    for col in numeric_cols:\n        if col == \"SalePrice\":\n            pass\n        else:\n            correlation = dataframe[[col, \"SalePrice\"]].corr().loc[col, \"SalePrice\"]\n            print(col, correlation)\n            if abs(correlation) > corr_limit:\n                high_correlations.append(col)\n            else:\n                low_correlations.append(col)\n    return low_correlations, high_correlations","e5fd2803":"num_cols = [col for col in list(set(num_cols + num_but_cat)) if col != \"Id\"]\nlow_corrs, high_corrs = find_correlation(df, num_cols)","9312069e":"#######################\n# Skewed to normal dist trials.\n#######################\n\nfor col in num_cols:\n    df[col].hist(bins=40)\n    plt.title(col)\n    plt.show()\n\nnp.log(df['BsmtUnfSF']).hist(bins=40)\nplt.show()\nnp.log1p(df['BsmtUnfSF']).hist(bins=40)\nplt.show()\nnp.sqrt(df['BsmtUnfSF']).hist(bins=40)\nplt.show()\nnp.power(df['BsmtUnfSF'],2).hist(bins=40)\nplt.show()","ccb0ad37":"df.loc[:, 'new1'] = 0\ndf.loc[(df['BsmtFinSF1'] > 1000) & (df['LotFrontage'] > 100), 'new1'] = 1\ndf.loc[:, 'new2'] = 0\ndf.loc[(df['OverallQual'] < 5) & (df['MSSubClass'] < 100), 'new2'] = 1\ndf.loc[:, 'new3'] = 0\ndf.loc[(df['YearBuilt'] > 1990) & (df['BsmtFinSF1'] > 1000), 'new3'] = 1\ndf.loc[:, 'new4'] = 0\ndf.loc[(df['GrLivArea'] > 2000) & (df['LotFrontage'] > 100), 'new4'] = 1\n\ncat_cols, num_cols, cat_but_car, num_but_cat = grab_col_names(df)\ndf[\"NEW_POOLQC_BOOL\"] = df[\"PoolQC\"].notnull().astype('int')\ndf.drop(\"PoolQC\", axis=1, inplace=True)\ndf[\"NEW_FENCE_BOOL\"] = df[\"Fence\"].notnull().astype('int')\ndf[\"NEW_FENCE_BOOL\"].value_counts(dropna=False)\ndf.drop(\"Fence\", axis=1, inplace=True)\ndf[\"NEW_HOUSE_AGE\"] = df[\"YearBuilt\"].apply(lambda x: 2021 - x)\ndf.drop(\"YearBuilt\", axis=1, inplace=True)\ndf[\"NEW_HOUSE_RENO_AGE\"] = df[\"YearRemodAdd\"].apply(lambda x: 2021 - x)\ndf.drop(\"YearRemodAdd\", axis=1, inplace=True)\ndf[\"GarageYrBlt\"].fillna(0, inplace=True)\ndf[\"NEW_GARAGE_AGE\"] = df[\"GarageYrBlt\"].apply(lambda x: 2021 - x if x > 0 else (2021 - 1899))\ndf.drop(\"GarageYrBlt\", axis=1, inplace=True)\ndf[\"NEW_ALLEY_BOOL\"] = df[\"Alley\"].notnull().astype('int')\ndf.drop(\"Alley\", axis=1, inplace=True)\ndf[\"NEW_MiscFeature_BOOL\"] = df[\"MiscFeature\"].notnull().astype('int')\ndf.drop(\"MiscFeature\", axis=1, inplace=True)\n\ndf[\"N_WholeArea\"] = df[\"GrLivArea\"] + df[\"TotalBsmtSF\"] + df[\"GarageArea\"] + df[\"WoodDeckSF\"] + df[\"OpenPorchSF\"] + \\\n                    df[\"EnclosedPorch\"] + df[\"3SsnPorch\"] + df[\"ScreenPorch\"] + df[\"PoolArea\"]\n\ndf[\"N_GnrlConQual\"] = df[\"OverallQual\"] * df[\"OverallCond\"]\ndf[\"N_GnrlConQual_Sum\"] = df[\"OverallQual\"] + df[\"OverallCond\"]\ndf[\"N_TotalPorchDeck\"] = df[\"WoodDeckSF\"] + df[\"OpenPorchSF\"] + df[\"EnclosedPorch\"] + df[\"3SsnPorch\"] + df[\n    \"ScreenPorch\"]\ndf[\"N_Firepl_Bool\"] = df[\"FireplaceQu\"].apply(lambda x: 0 if x == 1 else 1)\ndf[\"N_Porch_Bool\"] = df[\"N_TotalPorchDeck\"].apply(lambda x: 0 if x == 0 else 1)\ndf[\"N_TotalRoomsBaths\"] = df[\"TotRmsAbvGrd\"] * 2 + df[\"FullBath\"] * 2 + df[\"HalfBath\"]\ndf[\"N_SaleCondNormal_Bool\"] = df[\"SaleCondition\"].apply(lambda x: 1 if x == \"Normal\" else 0)\ndf.loc[df[\"MSZoning\"].isin([\"RM\", \"RH\"]), \"MSZoning\"] = \"RM_RH\"\ndf.loc[df[\"MSZoning\"].isnull(), \"MSZoning\"] = \"RL\"\ndf.loc[df[\"LotShape\"].isin([\"IR2\", \"IR3\"]), \"LotShape\"] = \"IR2_IR3\"\ndf.loc[df[\"LotConfig\"].isin([\"FR2\", \"FR3\"]), \"LotConfig\"] = \"FR2_3\"\ndf.loc[df[\"Condition1\"].isin([\"Artery\", \"RRAe\", \"Feedr\"]), \"Condition1\"] = \"Artery_Feedr_RRAe\"\ndf.loc[df[\"Condition1\"].isin([\"Norm\", \"RRNe\", \"RRAn\"]), \"Condition1\"] = \"Norm_RRNe_RRAn\"\ndf.loc[df[\"Condition1\"].isin([\"PosA\", \"PosN\", \"RRNn\"]), \"Condition1\"] = \"PosA_PosN_RRNn\"\n\n# Condition1 : 9    Being close to the main road or railway\ndf.loc[df[\"Condition1\"].isin([\"Artery\", \"RRAe\", \"Feedr\"]), \"Condition1\"] = \"Artery_Feedr_RRAe\"\ndf.loc[df[\"Condition1\"].isin([\"Norm\", \"RRNe\", \"RRAn\"]), \"Condition1\"] = \"Norm_RRNe_RRAn\"\ndf.loc[df[\"Condition1\"].isin([\"PosA\", \"PosN\", \"RRNn\"]), \"Condition1\"] = \"PosA_PosN_RRNn\"\n\n# Condition2 : 8    Being close to the main road or railway (if there is another way)\ndf.loc[df[\"Condition2\"].isin([\"Artery\", \"RRAn\", \"Feedr\", \"RRNn\"]), \"Condition2\"] = \"Artery_Feedr_RRAn_RRNn\"\ndf.loc[df[\"Condition2\"].isin([\"RRAe\"]), \"Condition2\"] = \"Norm\"\ndf.loc[df[\"Condition2\"].isin([\"PosN\", \"PosA\"]), \"Condition2\"] = \"PosN_PosA\"\n\n# BldgType : 5    Housing type\ndf.loc[df[\"BldgType\"].isin([\"2fmCon\", \"Duplex\", \"Twnhs\"]), \"BldgType\"] = \"2fmCon_Duplex_Twnhs\"\n\n# HouseStyle : 8    Housing Style\ndf.loc[df[\"HouseStyle\"].isin([\"1.5Unf\", \"SFoyer\"]), \"HouseStyle\"] = \"1.5Unf_SFoyer\"\ndf.loc[df[\"HouseStyle\"].isin([\"2Story\", \"2.5Fin\"]), \"HouseStyle\"] = \"2Story_2.5Fin\"\ndf.loc[df[\"HouseStyle\"].isin([\"SLvl\", \"2.5Unf\"]), \"HouseStyle\"] = \"SLvl_2.5Unf\"\n\n# RoofStyle : 6    Roof type\ndf.loc[df[\"RoofStyle\"].isin([\"Gable\", \"Gambrel\"]), \"RoofStyle\"] = \"Gable_Gambrel\"\ndf.loc[df[\"RoofStyle\"].isin([\"Flat\", \"Mansard\", \"Shed\"]), \"RoofStyle\"] = \"Flat_Mansard_Shed\"\n\n# RoofMatl : 8    Roofing material\ndf.loc[df[\"RoofMatl\"].isin([\"WdShake\", \"WdShngl\", \"Membran\"]), \"RoofMatl\"] = \"WdShake_WdShngl_Membran\"\ndf.loc[df[\"RoofMatl\"].isin([\"Tar&Grv\", \"Metal\", \"ClyTile\", \"Roll\"]), \"RoofMatl\"] = \"Other\"\n\n# Exterior1st : 15\ndf.loc[\n    df[\"Exterior1st\"].isin([\"AsbShng\", \"AsphShn\", \"CBlock\", \"BrkComm\"]), \"Exterior1st\"] = \"AsbShng_AspShn_CBl_BrkC\"\ndf.loc[df[\"Exterior1st\"].isin([\"CemntBd\", \"ImStucc\", \"Stone\"]), \"Exterior1st\"] = \"CemntBd_ImSt_St\"\ndf[\"Exterior1st\"].fillna(\"VinylSd\", inplace=True)\n\n# Exterior2nd : 16   Exterior cladding in the house (if there is more than one material)\ndf.loc[df[\"Exterior2nd\"].isin([\"Other\", \"ImStucc\", \"CmentBd\"]), \"Exterior2nd\"] = \"CmentBd_ImStucc_Oth\"\ndf.loc[df[\"Exterior2nd\"].isin([\"Brk Cmn\", \"AsbShng\", \"CBlock\"]), \"Exterior2nd\"] = \"Brk Cmn_AsbShng_CBl\"\ndf.loc[df[\"Exterior2nd\"].isin([\"Stucco\", \"Stone\"]), \"Exterior2nd\"] = \"Stucco_Stone\"\ndf.loc[df[\"Exterior2nd\"].isin([\"Wd Sdng\", \"AsphShn\"]), \"Exterior2nd\"] = \"Wd Sdng_AsphShn\"\ndf[\"Exterior2nd\"].fillna(\"VinylSd\", inplace=True)\n\n# MasVnrType : 4\ndf.loc[df[\"MasVnrType\"].isnull(), \"MasVnrType\"] = \"None\"\ndf.loc[df[\"MasVnrType\"].isin([\"None\", \"BrkCmn\"]), \"MasVnrType\"] = \"None_BrkCmn\"\n\n# Foundation : 6    Institution type\ndf.loc[df[\"Foundation\"].isin([\"Stone\", \"Wood\"]), \"Foundation\"] = \"Stone_Wood\"\n\ndf[\"NEW_HasGarage\"] = df[\"GarageType\"].apply(lambda x: 0 if x == \"None\" else 1)\ndf[\"NEW_HasBasement\"] = df[\"BsmtQual\"].apply(lambda x: 0 if x == \"None\" else 1)\ndf[\"NEW_HasFirePlace\"] = df[\"FireplaceQu\"].apply(lambda x: 0 if x == \"None\" else 1)\ndf[\"NEW_Has2ndFloor\"] = df[\"HouseStyle\"].apply(lambda x: 1 if \"2\" in x else 0)\ndf[\"NEW_Has_WodDeck\"] = df[\"WoodDeckSF\"].apply(lambda x: 0 if x == 0 else 1)\ndf[\"NEW_HAS_OPENPorch\"] = df[\"OpenPorchSF\"].apply(lambda x: 0 if x == 0 else 1)\ndf[\"NEW_HAS_EnclosedPorch\"] = df[\"EnclosedPorch\"].apply(lambda x: 0 if x == 0 else 1)\ndf[\"NEW_HAS_ScreenPorch\"] = df[\"ScreenPorch\"].apply(lambda x: 0 if x == 0 else 1)\ndf[\"NEW_OverallGrade\"] = df[\"OverallQual\"] * df[\"OverallCond\"]\ndf['NEW_TotalPorchSF'] = (df['OpenPorchSF'] +\n                          df['3SsnPorch'] +\n                          df['EnclosedPorch'] +\n                          df['ScreenPorch'])\ndf[\"NEW_TOTALSF\"] = df[\"1stFlrSF\"] + df[\"2ndFlrSF\"] + df[\"TotalBsmtSF\"]\ndf['NEW_TotalBathrooms'] = (df['FullBath'] +\n                            (0.5 * df['HalfBath']) +\n                            df['BsmtFullBath'] +\n                            (0.5 * df['BsmtHalfBath']))\ndf[\"NEW_TotalPorch_SF\"] = df[\"OpenPorchSF\"] + df[\"EnclosedPorch\"] + df[\"3SsnPorch\"] + \\\n                          df[\"ScreenPorch\"]\n\ndf[\"NEW_SqFtPerRoom\"] = df[\"GrLivArea\"] \/ (df[\"TotRmsAbvGrd\"] +\n                                           df[\"FullBath\"] +\n                                           df[\"HalfBath\"] +\n                                           df[\"KitchenAbvGr\"])\n\ndf['NEW_Total_Home_Quality'] = df['OverallQual'] + df['OverallCond']\n\ndf['NEW_Total_Bathrooms'] = (df['FullBath'] + (0.5 * df['HalfBath']) +\n                             df['BsmtFullBath'] + (0.5 * df['BsmtHalfBath']))\n\ndf[\"NEW_HighQualSF\"] = df[\"1stFlrSF\"] + df[\"2ndFlrSF\"]\n\n\n# df[\"MSZoning\"].value_counts()\n# target_summary_with_cat(df, \"SalePrice\", \"MSZoning\")\ndf.loc[(df[\"MSZoning\"] == \"RH\"), \"MSZoning\"] = \"RM\"\n\n# df[\"LotShape\"].value_counts()\n# target_summary_with_cat(df, \"SalePrice\", \"LotShape\")\ndf.loc[(df[\"LotShape\"] == \"IR2\"), \"LotShape\"] = \"IR1\"\ndf.loc[(df[\"LotShape\"] == \"IR3\"), \"LotShape\"] = \"IR1\"\n\n# df[\"LotConfig\"].value_counts()\n# target_summary_with_cat(df, \"SalePrice\", \"LotConfig\")\ndf.loc[(df[\"LotConfig\"] == \"Corner\"), \"LotConfig\"] = \"FR2\"\ndf.loc[(df[\"LotConfig\"] == \"Inside\"), \"LotConfig\"] = \"FR2\"\ndf.loc[(df[\"LotConfig\"] == \"CulDSac\"), \"LotConfig\"] = \"FR3\"\n\n#df[\"LandSlope\"].value_counts()\n#target_summary_with_cat(df, \"SalePrice\", \"LandSlope\")\ndf.loc[(df[\"LandSlope\"] == \"Mod\"), \"LandSlope\"] = \"Sev\"\n\n#df[\"Condition1\"].value_counts()\n#target_summary_with_cat(df, \"SalePrice\", \"Condition1\")\ndf.loc[(df[\"Condition1\"] == \"Feedr\"), \"Condition1\"] = \"Artery\"\ndf.loc[(df[\"Condition1\"] == \"RRAe\"), \"Condition1\"] = \"Artery\"\ndf.loc[(df[\"Condition1\"] == \"RRAn\"), \"Condition1\"] = \"Norm\"\ndf.loc[(df[\"Condition1\"] == \"PosN\"), \"Condition1\"] = \"PosA\"\ndf.loc[(df[\"Condition1\"] == \"RRNe\"), \"Condition1\"] = \"PosA\"\ndf.loc[(df[\"Condition1\"] == \"RRNn\"), \"Condition1\"] = \"PosA\"\n\ndf.loc[(df[\"HouseStyle\"] == \"1.5Fin\"), \"HouseStyle\"] = \"1.5Unf\"\ndf.loc[(df[\"HouseStyle\"] == \"2.5Unf\"), \"HouseStyle\"] = \"1.5Unf\"\ndf.loc[(df[\"HouseStyle\"] == \"SFoyer\"), \"HouseStyle\"] = \"1.5Unf\"\ndf.loc[(df[\"HouseStyle\"] == \"SLvl\"), \"HouseStyle\"] = \"1Story\"\ndf.loc[(df[\"HouseStyle\"] == \"2.5Fin\"), \"HouseStyle\"] = \"2Story\"\n\ndf.loc[(df[\"MasVnrType\"] == \"BrkCmn\"), \"MasVnrType\"] = \"None\"\n\n#df[\"GarageType\"].value_counts()\n#target_summary_with_cat(df, \"SalePrice\", \"GarageType\")\ndf.loc[(df[\"GarageType\"] == \"2Types\"), \"GarageType\"] = \"Basment\"\ndf.loc[(df[\"GarageType\"] == \"CarPort\"), \"GarageType\"] = \"Not\"\n\n","46dc4b80":"# Shows the total number of bathrooms\ndf[\"NEW_TotalBath\"] = df['BsmtFullBath'] + df['BsmtHalfBath'] * 0.5 + df['FullBath'] + df['HalfBath'] * 0.5\n\n# Total Number of Floors\ndf['NEW_TotalSF'] = (df['BsmtFinSF1'] + df['BsmtFinSF2'] + df['1stFlrSF'] + df['2ndFlrSF'])\n\n# Square meters of first floor and basement\ndf[\"NEW_SF\"] = df[\"1stFlrSF\"] + df[\"TotalBsmtSF\"]\n\n# Garage area and sum of square meters\ndf[\"NEW_SF_G\"] = df[\"NEW_SF\"] + df[\"GarageArea\"]\n\n# Total Patio Area\ndf['NEW_TotalPorchSF'] = (\n        df['OpenPorchSF'] + df['3SsnPorch'] + df['EnclosedPorch'] + df['ScreenPorch'] + df['WoodDeckSF'])\n\n# a variable related to the age and state of the house\n# The variable \"YearBuilt\" must remain in the dataset.\n\n# We divided the age of the house into categories\ndf[\"NEW_YEAR_BUILT_CAT\"] = pd.qcut(df['NEW_HOUSE_AGE'], 4, labels=[1, 2, 3, 4])\n# df['NEW_HOUSE_AGE'].hist(bins=20)\n# plt.show()\n\n# total square meters\ndf[\"NEW_TOTAL_M^2\"] = df[\"NEW_SF\"] + df[\"2ndFlrSF\"]\n\n# Qualities\ndf[\"NEW_QUAL_COND\"] = df['OverallQual'] + df['OverallCond']\n\ndf[\"NEW_BSMT_QUAL_COND\"] = df['BsmtQual'] + df['BsmtCond']\n\ndf[\"NEW_EX_QUAL_COND\"] = df['ExterQual'] + df['ExterCond']\n\ndf[\"NEW_QUAL_COND*\"] = df['OverallQual'] * df['OverallCond']\n\n\n# HOUSES WITH POOL\ndf[\"NEW_HAS_POOL\"] = (df['PoolArea'] > 0).astype('int')\n\n# LUXURIOUS HOUSES\ndf.loc[(df['Fireplaces'] > 0) & (df['GarageCars'] >= 3), \"NEW_LUX\"] = 1\ndf[\"NEW_LUX\"].fillna(0, inplace=True)  # eksiklere 0 verildi.\ndf[\"NEW_LUX\"] = df[\"NEW_LUX\"].astype(int)\n\ndf[\"NEW_AREA\"] = df[\"GrLivArea\"] + df[\"GarageArea\"]\n##############################################################################################################\n","cee0142c":"# Ordinal Cols change with points:\nordinal_cols = ['GarageCond', 'GarageQual', 'BsmtQual', 'BsmtCond', 'ExterQual', 'ExterCond', 'HeatingQC',\n                'KitchenQual', 'FireplaceQu']\nfor col in ordinal_cols:\n    df[col] = df[col].replace({'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1}).fillna(0)\n\n# other Ordinal  cols to change\ndf['BsmtExposure'] = df['BsmtExposure'].replace({'Ex': 5, 'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1}).fillna(0)\ndf['BsmtFinType1'] = df['BsmtFinType1'].replace(\n    {'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1}).fillna(0)\ndf['BsmtFinType2'] = df['BsmtFinType2'].replace(\n    {'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1}).fillna(0)\ndf['LandSlope'] = df['LandSlope'].replace({'Gtl': 3, 'Mod': 2, 'Sev': 1}).fillna(0)\ndf['GarageFinish'] = df['GarageFinish'].replace({'Fin': 3, 'RFn': 2, 'Unf': 1}).fillna(0)\n\n# i dont want these cols to be threaded as cat cols at grab cols part. i will remove them.\nordinal_cols = ['GarageCond', 'GarageQual', 'BsmtQual', 'BsmtCond', 'ExterQual', 'ExterCond', 'HeatingQC',\n                'KitchenQual', 'FireplaceQu'\n    , 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'LandSlope', 'GarageFinish'\n    , 'OverallCond', 'BedroomAbvGr', 'GarageCars']\n","e77b1751":"def replace_with_thresholds(dataframe, variable, q1=0.25, q3=0.75):\n    low_limit, up_limit = outlier_thresholds(dataframe, variable, q1=q1, q3=q3)\n    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit","56411509":"def outlier_thresholds(dataframe, col_name, q1=0.25, q3=0.75):\n    quartile1 = dataframe[col_name].quantile(q1)\n    quartile3 = dataframe[col_name].quantile(q3)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit","e8e64b83":"# Change outliers to low and up limits.\n\ncat_cols, num_cols, cat_but_car, num_but_cat = grab_col_names(df, excluded=ordinal_cols)\n\nnum_cols.remove('SalePrice')\nfor col in num_cols:\n    replace_with_thresholds(df, col, q1=0.15, q3=0.85)","2c2b7da8":"cat_cols, num_cols, cat_but_car, num_but_cat = grab_col_names(df)\ndf[\"GarageType\"].fillna('NO_GARAGE', inplace=True)\ndf[\"MasVnrType\"].fillna('NO_Masonry', inplace=True)\ndf[\"MasVnrArea\"].fillna('NO_Masonry', inplace=True)\ndf['LotFrontage'].fillna(df['LotArea'] \/ 170,\n                         inplace=True)  # there is around 1\/170 between 2 columns so filled with it.\n\ndf.isnull().sum().sum()\ndf.apply(lambda x: x.fillna(x.mode()[0], inplace=True) if (x.dtype == \"O\" and len(x.unique()) <= 10) else x,\n         axis=0)  # col type is int interestingly.\ndf = df.apply(lambda x: x.fillna(x.mode()[0]) if (x.dtype == \"O\" and len(x.unique()) <= 10) else x, axis=0)\ndf.isnull().sum().sum()\ndf['Electrical'].dtype == \"O\"\nlen(df['Electrical'].unique()) <= 10\ndf['Electrical'].mode()[0]\n\ndf['Electrical'].isnull().sum()\ndf['Electrical'].value_counts(dropna=False)\n\ndf[\"Electrical\"].fillna('SBrkr', inplace=True)\ndf['SaleType'].fillna('WD', inplace=True)\ndf['GarageCars'].fillna(2, inplace=True)\ndf['Functional'].fillna('Typ', inplace=True)\ndf['BsmtHalfBath'].fillna(0.000, inplace=True)\ndf['BsmtFullBath'].fillna(0.000, inplace=True)\ndf['BsmtFinSF2'].fillna(0.000, inplace=True)\ndf['Exterior1st'].fillna('VinylSd', inplace=True)\ndf['Exterior2nd'].fillna('VinylSd', inplace=True)\ndf['Utilities'].fillna('AllPub', inplace=True)\ndf['MSZoning'].fillna('RL', inplace=True)\ndf['GarageArea'].fillna(0, inplace=True)\ndf['TotalBsmtSF'].fillna(29.500, inplace=True)\ndf['BsmtUnfSF'].fillna(0, inplace=True)\ndf['BsmtFinSF1'].fillna(0, inplace=True)\n\ncat_cols, num_cols, cat_but_car, num_but_cat = grab_col_names(df, excluded=ordinal_cols)\nna_cols_cat = [col for col in cat_cols if df[col].isnull().sum() > 0 and \"SalePrice\" not in col]\ndf[na_cols_cat] = df[na_cols_cat].apply(lambda x: x.fillna(x.mode()), axis=0)\n\nna_cols_num = [col for col in num_cols if df[col].isnull().sum() > 0 and \"SalePrice\" not in col]\ndf[na_cols_num] = df[na_cols_num].apply(lambda x: x.fillna(x.median()), axis=0)\n\n","1e214b0e":"def label_encoder(dataframe, binary_col):\n    labelencoder = LabelEncoder()\n    dataframe[binary_col] = labelencoder.fit_transform(dataframe[binary_col])\n    return dataframe","f75defab":"binary_cols = [col for col in df.columns if df[col].dtype not in [int, float] and df[col].nunique() == 2]\n\nfor col in binary_cols:\n    label_encoder(df, col)","681d799d":"def rare_analyser(dataframe, target, cat_cols):\n    for col in cat_cols:\n        print(col, \":\", len(dataframe[col].value_counts()))\n        print(pd.DataFrame({\"COUNT\": dataframe[col].value_counts(),\n                            \"RATIO\": dataframe[col].value_counts() \/ len(dataframe),\n                            \"TARGET_MEAN\": dataframe.groupby(col)[target].mean()}), end=\"\\n\\n\\n\")\n\n\ndef rare_encoder(dataframe, rare_perc):\n    temp_df = dataframe.copy()\n\n    rare_columns = [col for col in temp_df.columns if temp_df[col].dtypes == 'O'\n                    and (temp_df[col].value_counts() \/ len(temp_df) < rare_perc).any(axis=None)]\n\n    for var in rare_columns:\n        tmp = temp_df[var].value_counts() \/ len(temp_df)\n        rare_labels = tmp[tmp < rare_perc].index\n        temp_df[var] = np.where(temp_df[var].isin(rare_labels), 'Rare', temp_df[var])\n\n    return temp_df","553bcb81":"cat_cols, num_cols, cat_but_car, num_but_cat = grab_col_names(df, excluded=ordinal_cols)\ndf = rare_encoder(df, 0.01)","6df478ce":"def one_hot_encoder(dataframe, categorical_cols, drop_first=False):\n    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first)\n    return dataframe","f8f88584":"# on some columns, I don't want to apply one-hot encoder...\n\nadditional_exceptions = ['Street', 'Utilities', 'BsmtFinSF2', 'CentralAir', 'LowQualFinSF', 'BsmtFullBath',\n                         'BsmtHalfBath', 'FullBath', 'HalfBath', 'KitchenAbvGr', 'Fireplaces', '3SsnPorch',\n                         'ScreenPorch', 'PoolArea', 'MiscVal', 'YrSold', 'NEW_POOLQC_BOOL', 'NEW_FENCE_BOOL',\n                         'NEW_ALLEY_BOOL', 'NEW_MiscFeature_BOOL']\n\nordinal_cols = ['GarageCond', 'GarageQual', 'BsmtQual', 'BsmtCond', 'ExterQual', 'ExterCond', 'HeatingQC',\n                'KitchenQual', 'FireplaceQu'\n    , 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'LandSlope', 'GarageFinish'\n    , 'OverallCond', 'BedroomAbvGr', 'GarageCars']\n\nexcluded_list = []\nexcluded_list.extend(ordinal_cols)\nexcluded_list.extend(additional_exceptions)\ncat_cols, num_cols, cat_but_car, num_but_cat = grab_col_names(df, excluded=excluded_list)\ncat_cols = cat_cols + cat_but_car\n\n\n\ndf = pd.get_dummies(df, drop_first=1)\n# df = one_hot_encoder(df, cat_cols)\ncat_cols, num_cols, cat_but_car, num_but_cat = grab_col_names(df, excluded=excluded_list)\n\nuseless_cols = [col for col in df.columns if df[col].nunique() == 2 and\n                (df[col].value_counts() \/ len(df) < 0.01).any(axis=None)]\n\ndf.drop(useless_cols, axis=1, inplace=True)","a49c95cd":"num_cols = [col for col in num_cols if \"Id\" not in col]\nnum_cols = [col for col in num_cols if \"SalePrice\" not in col]\nscaler = StandardScaler()\ndf[num_cols] = scaler.fit_transform(df[num_cols])","90ae1d1f":"global train_df, test_df\ntrain_df = df[df['SalePrice'].notnull()]\ntest_df = df[df['SalePrice'].isnull()].drop(\"SalePrice\", axis=1)\n\nglobal X, y\n# y = train_df[\"SalePrice\"]\ny = np.log1p(train_df['SalePrice'])  # to increase train speed.\nX = train_df.drop([\"Id\", \"SalePrice\"], axis=1)","86b84223":"def run_single_model():\n    models = [('LR', LinearRegression()),\n              (\"Ridge\", Ridge()),\n              (\"Lasso\", Lasso()),\n              (\"ElasticNet\", ElasticNet()),\n              ('KNN', KNeighborsRegressor()),\n              ('CART', DecisionTreeRegressor()),\n              ('RF', RandomForestRegressor()),\n              ('SVR', SVR()),\n              ('GBM', GradientBoostingRegressor()),\n              (\"XGBoost\", XGBRegressor(objective='reg:squarederror')),\n              (\"LightGBM\", LGBMRegressor()),\n              (\"CatBoost\", CatBoostRegressor(verbose=False))\n        , (\"AdaBoost\", AdaBoostRegressor())\n        , (\"Bagging\", BaggingRegressor())\n        , (\"ExtraTrees\", ExtraTreesRegressor())\n        , (\"HistGradient\", HistGradientBoostingRegressor())\n              ]\n\n    global output_df\n    output_df = pd.DataFrame(models, columns=[\"MODEL_NAME\", \"MODEL_BASE\"])\n    output_df.drop('MODEL_BASE', axis=1, inplace=True)\n    for name, regressor in models:\n        rmse_cv = np.mean(np.sqrt(-cross_val_score(regressor, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n        print(f\"RMSE: {round(rmse_cv, 4)} ({name}) \")\n        output_df.loc[output_df['MODEL_NAME'] == name, \"RMSE_BASE_CV_ALL\"] = rmse_cv\n        \n    rf_params = {\"max_depth\": [5, 15, 20, None],\n                 \"max_features\": [5, 7, 9, \"auto\"],\n                 \"min_samples_split\": [6, 8, 15],\n                 \"n_estimators\": [150, 200, 300]}\n\n    xgboost_params = {\"learning_rate\": [0.05, 0.1], #[0.01, 0.05, 0.1, 0.15],\n                      \"max_depth\": [3, 5,], # 8],\n                      \"n_estimators\": [300], # [100, 200, 300],\n                      \"colsample_bytree\": [0.3], #[0.3, 0.5, 0.8]\n                     }\n\n    lightgbm_params = {\"learning_rate\": [0.001, 0.01, 0.1],\n                       \"n_estimators\": [100, 300, 500],\n                       \"colsample_bytree\": [0.1, 0.3, 0.7, 1],\n                       #\"num_leaves\": [5,31],\n                       #\"max_bin\": [55, 255],\n                       #\"bagging_fraction\": [0.8, 1.0],\n                       #\"bagging_freq\": [0, 5],\n                       #\"feature_fraction\": [0.2319, 1.0],\n                       #\"feature_fraction_seed\": [2, 9],\n                       #\"bagging_seed\": [3, 9],\n                       #\"min_data_in_leaf\": [6, 20],\n                       #\"min_sum_hessian_in_leaf\": [0.003, 11]\n                       }\n\n    extraTrees_params = {\n        'n_estimators': [10, 50, 100],\n        'max_depth': [2, 16, 50],\n        'min_samples_split': [2, 6],\n        'min_samples_leaf': [1, 2],\n        'max_features': ['auto', 'sqrt', 'log2'],\n        'bootstrap': [True, False],\n        'warm_start': [True, False],\n    }\n\n    HistGradient_params = {\"learning_rate\": [0.01, 0.05],\n                           \"max_iter\": [20, 100],\n                           \"max_depth\": [None, 25],\n                           \"l2_regularization\": [0.0, 1.5],\n                           }\n\n    catboost_params = {\"iterations\": [ 200, 500, 1000],\n                       \"learning_rate\": [ 0.01, 0.1, 0.3],\n                       \"depth\": [3, 6]}\n    regressors = [\n        (\"RF\", RandomForestRegressor(), rf_params),\n        ('XGBoost', XGBRegressor(objective='reg:squarederror'), xgboost_params),\n        ('LightGBM', LGBMRegressor(), lightgbm_params),\n        ('CatBoost', CatBoostRegressor(verbose=False), catboost_params),\n        ('ExtraTrees', ExtraTreesRegressor(), extraTrees_params),\n        ('HistGradient', HistGradientBoostingRegressor(), HistGradient_params),\n\n    ]\n    global best_models\n    best_models = {}\n\n    for name, regressor, params in regressors:\n        # GridSearch\n        gs_best = GridSearchCV(regressor, params, cv=3, n_jobs=-1, verbose=False).fit(X, y)\n        final_model = regressor.set_params(**gs_best.best_params_)\n        rmse = np.mean(np.sqrt(-cross_val_score(final_model, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n        print(f\"RMSE (After): {round(rmse, 4)} ({name}) \")\n        output_df.loc[output_df['MODEL_NAME'] == name, \"RMSE_TUNED_CV_ALL\"] = rmse\n        print(f\"{name} best params: {gs_best.best_params_}\", end=\"\\n\\n\")\n        output_df.loc[output_df['MODEL_NAME'] == name, \"BEST_PARAMS\"] = str(gs_best.best_params_)\n\n        best_models[name] = final_model","40b43755":"def run_multiple_model():\n    \n    \"\"\"\n    Stacking & Ensemble Learning\n    \n    \"\"\"\n    \n    best_models\n\n    model_name = 'voting_reg_XG_Cat'\n    model = VotingRegressor(estimators=[('XGBoost', best_models[\"XGBoost\"]), ('CatBoost', best_models[\"CatBoost\"])])\n    model.fit(X, y)\n    rmse = np.mean(np.sqrt(-cross_val_score(model, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE ({model_name}): {round(rmse, 4)} \")\n    global output_df\n    output_df = output_df.append({'MODEL_NAME': {model_name}}, ignore_index=True)\n    output_df.loc[output_df['MODEL_NAME'] == {model_name}, \"RMSE_TUNED_CV_ALL\"] = rmse\n    best_models[model_name] = model\n\n    model_name = 'stacking_reg_XG_Cat'\n    estimators = [('XGBoost', best_models[\"XGBoost\"]), ('CatBoost', best_models[\"CatBoost\"])]\n    model = StackingRegressor(estimators=estimators, final_estimator=best_models[\"XGBoost\"])\n    model.fit(X, y)\n    rmse = np.mean(np.sqrt(-cross_val_score(model, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE ({model_name}): {round(rmse, 4)} \")\n    output_df = output_df.append({'MODEL_NAME': {model_name}}, ignore_index=True)\n    output_df.loc[output_df['MODEL_NAME'] == {model_name}, \"RMSE_TUNED_CV_ALL\"] = rmse\n    best_models[model_name] = model\n\n    model_name = 'voting_reg_RF_LGBM'\n    model = VotingRegressor(estimators=[('RF', best_models[\"RF\"]), ('LightGBM', best_models[\"LightGBM\"])])\n    model.fit(X, y)\n    rmse = np.mean(np.sqrt(-cross_val_score(model, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE ({model_name}): {round(rmse, 4)} \")\n    output_df = output_df.append({'MODEL_NAME': {model_name}}, ignore_index=True)\n    output_df.loc[output_df['MODEL_NAME'] == {model_name}, \"RMSE_TUNED_CV_ALL\"] = rmse\n    best_models[model_name] = model\n\n    estimators = [('RF', best_models[\"RF\"]), ('XGBoost', best_models[\"XGBoost\"])]\n    stacking_reg = StackingRegressor(estimators=estimators, final_estimator=best_models[\"LightGBM\"])\n    stacking_reg.fit(X, y)\n    stacking_reg_rmse = np.mean(np.sqrt(-cross_val_score(stacking_reg, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE (stacking_reg_RF_XG): {round(stacking_reg_rmse, 4)} \")\n    output_df = output_df.append({'MODEL_NAME': \"stacking_reg_rmse\"}, ignore_index=True)\n    output_df.loc[output_df['MODEL_NAME'] == \"stacking_reg_rmse\", \"RMSE_TUNED_CV_ALL\"] = stacking_reg_rmse\n    best_models['stacking_reg_RF_XG'] = stacking_reg\n\n    voting_reg_RF_XG_LGBM = VotingRegressor(estimators=[('RF', best_models[\"RF\"]), ('XGBoost', best_models[\"XGBoost\"]),\n                                                        ('LightGBM', best_models[\"LightGBM\"])])\n    voting_reg_RF_XG_LGBM.fit(X, y)\n    voting_reg_rmse = np.mean(np.sqrt(-cross_val_score(voting_reg_RF_XG_LGBM, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE (voting_reg_RF_XG_LGBM): {round(voting_reg_rmse, 4)} \")\n    output_df = output_df.append({'MODEL_NAME': \"voting_reg_RF_XG_LGBM\"}, ignore_index=True)\n    output_df.loc[output_df['MODEL_NAME'] == \"voting_reg_RF_XG_LGBM\", \"RMSE_TUNED_CV_ALL\"] = voting_reg_rmse\n    best_models['voting_reg_RF_XG_LGBM'] = voting_reg_RF_XG_LGBM\n\n\n    voting_reg_RF_XG_LGBM_Cat = VotingRegressor(estimators=[('RF', best_models[\"RF\"]), ('XGBoost', best_models[\"XGBoost\"]),\n                                                        ('LightGBM', best_models[\"LightGBM\"]), ('CatBoost', best_models[\"CatBoost\"])])\n    voting_reg_RF_XG_LGBM_Cat.fit(X, y)\n    voting_reg_rmse = np.mean(np.sqrt(-cross_val_score(voting_reg_RF_XG_LGBM_Cat, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE (voting_reg_RF_XG_LGBM): {round(voting_reg_rmse, 4)} \")\n    output_df = output_df.append({'MODEL_NAME': \"voting_reg_RF_XG_LGBM_Cat\"}, ignore_index=True)\n    output_df.loc[output_df['MODEL_NAME'] == \"voting_reg_RF_XG_LGBM_Cat\", \"RMSE_TUNED_CV_ALL\"] = voting_reg_rmse\n    best_models['voting_reg_RF_XG_LGBM_Cat'] = voting_reg_RF_XG_LGBM_Cat","f14cf273":"def run_selected_cols(model, thr=1):\n    feature_imp = pd.DataFrame({'Value': model.feature_importances_, 'Feature': X.columns})\n    feature_imp[feature_imp[\"Value\"] > 0].shape\n    feature_imp[feature_imp[\"Value\"] > 1].shape\n    feature_imp[feature_imp[\"Value\"] < 1].shape\n    zero_imp_cols = feature_imp[feature_imp[\"Value\"] < thr][\"Feature\"].values\n    global selected_cols\n    selected_cols = [col for col in X.columns if col not in zero_imp_cols]\n\n    global model_selected_cols\n    model_selected_cols = LGBMRegressor(random_state=1)\n    rmse_cv = np.mean(\n        np.sqrt(-cross_val_score(model_selected_cols, X[selected_cols], y, cv=10, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE: {round(rmse_cv, 4)} ({model_selected_cols}) \")","59e5287b":"def plot_importance(model, features, num=20, save=False):\n    feature_imp = pd.DataFrame({'Value': model.feature_importances_, 'Feature': features.columns})\n    plt.figure(figsize=(10, 10))\n    sns.set(font_scale=1)\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\",\n                                                                     ascending=False)[0:num])\n    plt.title('Features')\n    plt.tight_layout()\n    plt.show()\n    if save:\n        plt.savefig('importances.png')","f8e1be6c":"def create_submission(final_model):\n    \n    \"\"\"\n    Uploading Results\n    \n    \"\"\"\n   \n    submission_df = pd.DataFrame()\n    submission_df['Id'] = test_df[\"Id\"]\n    y_pred_sub = final_model.predict(test_df.drop(\"Id\", axis=1))\n    y_pred_sub = np.expm1(y_pred_sub)\n    submission_df['SalePrice'] = y_pred_sub\n    submission_df['Id'] = submission_df['Id'].astype('int')\n    submission_df.to_csv('submission.csv', index=False)","5349be41":"run_single_model()","1f644a5d":"run_multiple_model()","4745c340":"output_df.sort_values(['RMSE_TUNED_CV_ALL','RMSE_BASE_CV_ALL'])","0b1e47c8":"best_models","4f3b850c":"final_model1 = best_models['voting_reg_XG_Cat'].fit(X, y)","ae04ce06":"create_submission(final_model1)","305866eb":"* Any observations which had missing values were also removed from the dataset. Below are a few feature engineering processes which were done  to cleanse the dataset:\n\n\n* [1. Proximity to main road or railway](#0)   \n* [2. Proximity to the main road or railway (if a second location is available)](#1)\n* [3. Housing type](#2)\n* [4. Housing style](#3)\n* [5. Roof type](#4)\n* [6. Roofing material](#5)\n* [7. Exterior cladding in the house (if there is more than one material)](#6)\n* [8. Institution type](#7)\n* [9. Total Number of Floors](#7)\n* [10. 1st floor and basement](#7)\n* [11. Garage area and sum of square meters](#7)\n* [12. Total Patio Area](#7)\n* [13. Houses With Pool](#7)\n* [14. Luxurious Houses](#7)","fe3d1105":"<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">4. Conclusions<\/h1>","0aa2e44a":"<h4 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">2.8. Rare Encoding<\/h4>","c3340fbb":"<h4 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">2.3. Producing New Columns for Feature Engineering<\/h4>","0305ac21":"<a id=\"toc\"><\/a>\n\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">Table of Contents<\/h3>\n    \n* [1. Introduction](#0) \n\n* [2. Explatory Data Analysis](#0) \n    * [2.1 Feature Enginnering](#0) \n    * [2.2 Correlation Analysis](#0) \n    * [2.3 Producing New Columns for Feature Engineering](#0) \n    * [2.4 Feature Extractions](#0) \n    * [2.5 Outliers](#0) \n    * [2.6 Missing Values](#0) \n    * [2.7 Label Encoding](#0) \n    * [2.8 Rare Encoding](#0) \n    * [2.9 One-Hot Encoding](#0) \n        \n* [3. Machine Learning Model](#0) \n    * [3.1 Standart Scaler](#0) \n    * [3.2 Modeling](#0) \n    * [3.3 Hyper Parameters Tunning](#0)\n\n* [4. Conclusions](#0) \n* [5. References](#0) \n","0e904383":"<h4 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">2.7. Label Encoding<\/h4>","f06c3f22":"<h4 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">2.1. Feature Enginnering<\/h4>","28300b85":"<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">2. Explatory Data Analysis<\/h1>","a0ae4d1d":"![8-2-2021%202-30-44%20PM-2.jpg](attachment:8-2-2021%202-30-44%20PM-2.jpg)","e45095b9":"<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">1. Introduction<\/h1>","047db49e":"* In this dataset, I have tried to minimize the rmse of the house prices. As usual we start with EDA. Tried to add new features to identify the data to models in a better way. I have tried so many different model from sklearn, ligthgbm, catboost, xgboost and as well as ngboost. On top of it, I have applied Voting and Stacking Models to improve the model predictions.","ced905af":"* In this study machine learning algorithms were applied against each other in order to investigate which one is more successful in predicting housing prices. ","8f3da2fa":"<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">3. Machine Learning Model<\/h1>","9f3a8ff5":"![Untitled.jpg](attachment:Untitled.jpg)","abecbc5c":"* [[1] Chen, X., Wei, L. and Xu, J., 2017. House price prediction using LSTM. arXiv preprint arXiv:1709.08432.](#0)\n* [[2] Engstr\u00f6m, I. and Ihre, A., 2019. Predicting house prices with machine learning methods.](#0)\n* [[3] Ho, W.K., Tang, B.S. and Wong, S.W., 2021. Predicting property prices with machine learning algorithms. Journal of Property Research, 38(1), pp.48-70.](#0)\n* [[4] Alfiyatin, A.N., Febrita, R.E., Taufiq, H. and Mahmudy, W.F., 2017. Modeling house price prediction using regression analysis and particle swarm optimization. International Journal of Advanced Computer Science and Applications, 8(10), pp.323-326.](#0)\n* [[5] Truong, Q., Nguyen, M., Dang, H. and Mei, B., 2020. Housing price prediction via improved machine learning techniques. Procedia Computer Science, 174, pp.433-442.](#0)\n","09f0f4b8":"* Location is an important factor in shaping the price of a home. This is because the location determines the current land price. The location also determines the ease of access to public facilities such as schools, campuses, hospitals and health centers, as well as family leisure facilities such as shopping malls, food tours, and even offers a beautiful view.","f726da10":"<h4 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">2.6. Missing Values<\/h4>","94eae612":"<h4 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">2.9. One-Hot Encoding<\/h4>","023270d8":"<h4 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">2.2. Correlation Analysis<\/h4>","d53041d5":"<h4 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">3.2. Modeling<\/h4>","2e7846f8":"Accurately estimating the financial value of real estate is important to many people, including homeowners, home buyers, agents, creditors and investors. While it is common knowledge that factors such as size, number of rooms and location affect the price, there are also many other parameters that affect the price. In addition, prices can be other parameters that affect the price, such as changes in market demand and the urgent need to sell a property. \n    \n    \n* Problem\n    \nIt is desired to carry out a machine learning project regarding the prices of different types of houses by using the data set containing the features and house prices of each house.\n\n* Feature Description\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\n\n* Variables\n\nThe Train dataset consists of 1460 observations and 81 variables, and the Test dataset consists of 1459 observation units.\n\n* Purpose\n\nHouse prices are left null in the test dataset, and you are expected to estimate these values. Carrying out a machine learning project that predicts house prices with minimum error on the data set we have.\n","82d8b3e0":"<h4 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">2.5. Outliers<\/h4>","58ed9213":"<h4 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">2.4. Feature Extractions<\/h4>","b79985a9":"<h4 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">3.3. Hyper Parameters Tunning<\/h4>","74c4bfd6":"* The data used in the experiment will be processed using a combination of preprocessing methods to improve the prediction accuracy. Also, some factors will be added to the local dataset to examine the relationship between these factors and the selling price in Iowa.","03bf11ff":"<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">5. References<\/h1>","4838952c":"<h4 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">3.1. Standart Scaler<\/h4>"}}