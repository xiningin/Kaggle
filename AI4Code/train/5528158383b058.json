{"cell_type":{"91a6f474":"code","440f4492":"code","cdbcb77e":"code","fbd08223":"code","0d35911e":"code","9708aaff":"code","c8f95d6e":"code","979ee432":"code","772c1f25":"code","2b84dd67":"code","51d59a44":"code","6f2028df":"code","7968cfe5":"code","b046da67":"code","3008cc09":"code","d95ff338":"code","87e9ea73":"code","a4ed9b50":"code","009443f6":"code","385a8091":"code","b583ac29":"code","7f220289":"markdown","d2508a05":"markdown","9329f152":"markdown","8465614d":"markdown","a1aa7aa8":"markdown","eb67a7d1":"markdown"},"source":{"91a6f474":"#!pip install optuna \nimport optuna","440f4492":"import numpy as np\nimport pandas as pd\n","cdbcb77e":"from lightgbm import LGBMRegressor\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt","fbd08223":"train = pd.read_csv('..\/input\/30days-folds\/train_folds.csv')\ntest  = pd.read_csv('..\/input\/30-days-of-ml\/test.csv')\nsub = pd.read_csv('..\/input\/30-days-of-ml\/sample_submission.csv')","0d35911e":"cat_cols = ['cat'+str(i) for i in range(10)]\ncon_cols = ['cont'+str(i) for i in range(14)]","9708aaff":"imp_cols = cat_cols+con_cols\ndata=train[imp_cols]\ntarget=train['target']","c8f95d6e":"def objective(trial,data=data,target=target):\n    \n    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.2,random_state=42)\n    ordinal_encoder = OrdinalEncoder()\n    train_x[cat_cols] = ordinal_encoder.fit_transform(train_x[cat_cols])\n    test_x[cat_cols] = ordinal_encoder.transform(test_x[cat_cols])\n    \n    std_scaler = StandardScaler()\n    train_x[con_cols] = std_scaler.fit_transform(train_x[con_cols])\n    test_x[con_cols] = std_scaler.transform(test_x[con_cols])\n    \n    param = {\n        'metric': 'rmse', \n        'random_state': 42,\n        'n_estimators': 20000,\n        'device_type':'gpu',\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.006,0.008,0.01,0.014,0.017,0.02]),\n        'max_depth': trial.suggest_categorical('max_depth', [10,20,100]),\n        'num_leaves' : trial.suggest_int('num_leaves', 1, 1000),\n        'min_child_samples': trial.suggest_int('min_child_samples', 1, 300),\n        'cat_smooth' : trial.suggest_int('min_data_per_groups', 1, 100)\n    }\n    model = LGBMRegressor(**param)  \n    \n    model.fit(train_x,train_y,eval_set=[(test_x,test_y)],early_stopping_rounds=100,verbose=False)\n    \n    preds = model.predict(test_x)\n    \n    rmse = mean_squared_error(test_y, preds,squared=False)\n    \n    return rmse","979ee432":"study = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=50)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","772c1f25":"study.trials_dataframe()","2b84dd67":"#plot_optimization_histor: shows the scores from all trials as well as the best score so far at each point.\noptuna.visualization.plot_optimization_history(study)","51d59a44":"#plot_parallel_coordinate: interactively visualizes the hyperparameters and scores\noptuna.visualization.plot_parallel_coordinate(study)","6f2028df":"'''plot_slice: shows the evolution of the search. You can see where in the hyperparameter space your search\nwent and which parts of the space were explored more.'''\noptuna.visualization.plot_slice(study)","7968cfe5":"#plot_contour: plots parameter interactions on an interactive chart. You can choose which hyperparameters you would like to explore.\noptuna.visualization.plot_contour(study, params=['num_leaves',\n                            'max_depth',\n                            'subsample',\n                            'learning_rate',\n                            'subsample'])","b046da67":"#Visualize parameter importances.\noptuna.visualization.plot_param_importances(study)","3008cc09":"#Visualize empirical distribution function\noptuna.visualization.plot_edf(study)","d95ff338":"params=study.best_params   \nparams['random_state'] = 42\nparams['n_estimators'] = 20000 \nparams['metric'] = 'rmse'\nparams['device_type'] = 'gpu'","87e9ea73":"# change the param name, differnt in optuna and lgbm\nparams['cat_smooth'] = params.pop('min_data_per_groups')","a4ed9b50":"params","009443f6":"preds = np.zeros(test.shape[0])\npreds_all = np.zeros((test.shape[0], 5))\nrmse=[]  # list contains rmse for each fold\n\nfor fold in range(5):\n    X_tr =  train[train.kfold != fold].reset_index(drop=True)\n    X_val = train[train.kfold == fold].reset_index(drop=True)\n    y_tr = X_tr.target\n    y_val = X_val.target \n    xtest = test.copy()\n    \n    X_tr = X_tr[imp_cols]\n    X_val = X_val[imp_cols]\n    xtest = xtest[imp_cols]\n    \n    ordinal_encoder = OrdinalEncoder()\n    X_tr[cat_cols] = ordinal_encoder.fit_transform(X_tr[cat_cols])\n    X_val[cat_cols] = ordinal_encoder.transform(X_val[cat_cols])\n    xtest[cat_cols] = ordinal_encoder.transform(xtest[cat_cols])\n    \n    std_scaler = StandardScaler()\n    X_tr[con_cols] = std_scaler.fit_transform(X_tr[con_cols])\n    X_val[con_cols] = std_scaler.transform(X_val[con_cols])\n    xtest[con_cols] = std_scaler.transform(xtest[con_cols])\n    \n    model = LGBMRegressor(**params)\n    model.fit(X_tr,y_tr,eval_set=[(X_val,y_val)],early_stopping_rounds=100,verbose=False)\n    preds = model.predict(xtest)\n    \n    preds_all[:, fold] = preds\n\n    rmse.append(mean_squared_error(y_val, model.predict(X_val), squared=False))\n    print(fold+1,rmse[fold])\n","385a8091":"from optuna.integration import lightgbm as lgb\nlgb.plot_importance(model, max_num_features=10, figsize=(10,10))\nplt.show()\n","b583ac29":"np.savetxt(\"all_data.csv\", preds_all, delimiter=\",\")\n\nsub['target']=np.mean(preds_all, axis = 1)\nsub.to_csv('submission.csv', index=False)","7f220289":"# LGBMRegressor model with the best hyperparameters","d2508a05":"## Encode categorical features","9329f152":"* Objective of our fuction is to minimize the RMSE hence direction='minimize', n_trials = number of executions","8465614d":"# Optuna: Hyperparameter Tuning with LGBM","a1aa7aa8":"# Submission","eb67a7d1":"# Quick Visualization for Hyperparameter Optimization Analysis"}}