{"cell_type":{"61c3d334":"code","19157715":"code","973e2d52":"code","866b9c98":"code","c6943c46":"code","c82f8dd1":"code","925aec02":"code","df4201dc":"code","ceb7af8d":"code","1093d5ce":"code","095e5990":"code","f90e50a0":"code","6eaafd2f":"code","83bee001":"markdown","8a89f940":"markdown","ecf63888":"markdown","f06e8095":"markdown","32f96f95":"markdown","94c231c4":"markdown"},"source":{"61c3d334":"!apt-get install python-opengl -y\n!pip install pyvirtualdisplay","19157715":"import gym\nimport os\nimport numpy as np\nimport random\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom pyvirtualdisplay import Display\nimport tensorflow as tf\nfrom tensorflow.keras import models, layers\n\n\ndisplay = Display(visible=0, size=(1400, 900))\ndisplay.start()","973e2d52":"STATE_DIM, ACTION_DIM = 4, 2  # State \u7ef4\u5ea6 4, Action \u7ef4\u5ea6 2\n\nmodel = models.Sequential([\n    layers.Dense(64, input_dim=STATE_DIM, activation='relu'),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(32, activation='relu'),\n    layers.Dense(1, activation='sigmoid')\n])\nmodel.summary()  # \u6253\u5370\u795e\u7ecf\u7f51\u7edc\u4fe1\u606f","866b9c98":"env = gym.make(\"CartPole-v0\")  # \u52a0\u8f7d\u6e38\u620f\u73af\u5883","c6943c46":"def generate_data_one_episode():\n    '''\u751f\u6210\u5355\u6b21\u6e38\u620f\u7684\u8bad\u7ec3\u6570\u636e'''\n    x = []\n    y = []\n    score = 0\n    env.reset()\n    while True:\n        action = random.randrange(0, 2)\n        x.append(env.state)\n        y.append(action) # \u8bb0\u5f55\u6570\u636e\n        state, reward, done, _ = env.step(action) # \u6267\u884c\u52a8\u4f5c\n        score += reward\n        if done:\n            break\n    return x, y, score","c82f8dd1":"from tqdm import tqdm\n\ndef generate_training_data(expected_score=100):\n    '''# \u751f\u6210N\u6b21\u6e38\u620f\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u8fdb\u884c\u7b5b\u9009\uff0c\u9009\u62e9 > 200 \u7684\u6570\u636e\u4f5c\u4e3a\u8bad\u7ec3\u96c6'''\n    data_X, data_Y, scores = [], [], []\n    pbr = tqdm(range(30000))\n    for i in pbr:\n        x, y, score = generate_data_one_episode()\n        if score > expected_score:\n            data_X += x\n            data_Y += y\n            scores.append(score)\n        pbr.set_description('len(data_X) = %d' % (len(data_X)))\n    return np.array(data_X), np.array(data_Y)","925aec02":"data_X, data_Y = generate_training_data()","df4201dc":"data_X.shape,data_Y.shape","ceb7af8d":"loss_fc = tf.keras.losses.BinaryCrossentropy(from_logits=False)\nopt = tf.keras.optimizers.Adam(learning_rate=0.01)\n\nmodel.compile(loss=loss_fc,optimizer=opt,metrics=['acc'])","1093d5ce":"model.fit(data_X,data_Y,epochs=100)","095e5990":"env.reset()\nscore = 0\n# images\u4e2d\u7684\u5143\u7d20\u7c7b\u578b\u4e3aImage\nimages = []\nwhile True:\n    # time.sleep(0.1)\n    screen = env.render(mode='rgb_array')\n    im = Image.fromarray(screen)\n    images.append(im)\n    state = env.state\n    state = tf.expand_dims(state,axis=0)\n    action = model.predict(state)\n    action = tf.cast(action>0.5,tf.int32).numpy()[0,0]\n    state, reward, done, _ = env.step(action)  # \u6267\u884c\u8fd9\u4e2a\u52a8\u4f5c\n    score += reward     # \u6bcf\u56de\u5408\u7684\u5f97\u5206\n    if done:       # \u6e38\u620f\u7ed3\u675f\n        print('score: ', score)  # \u6253\u5370\u5206\u6570\n        break\nenv.close()","f90e50a0":"from matplotlib import animation , rc\n\nfig = plt.figure()\n\nframes = []\nimages_count = len(images)\n\nfor i in range(images_count):\n    img = plt.imshow(np.array(images[i]))\n    frames.append([img])\n\nan = animation.ArtistAnimation(fig, frames, interval=100, repeat_delay=1000, blit=True)\nrc('animation', html='jshtml')\nan","6eaafd2f":"image_file = 'cartpole-v0.gif'\nimages[0].save(image_file, save_all=True, append_images=images[1:], loop=0, duration=1)","83bee001":"# **\u8bad\u7ec3\u6a21\u578b**","8a89f940":"# **\u83b7\u53d6\u8bad\u7ec3\u6570\u636e**  \n**\u5728\u8fc7\u7a0b\u4e2d\u8ba1\u7b97Score\uff0c\u5982\u679c\u6700\u7ec8\u5f97\u5206\u8fbe\u5230\u8bbe\u5b9a\u7684\u6807\u51c6\uff0c\u8fd9\u4e2a\u5206\u6570\u6240\u5bf9\u5e94\u7684\u6240\u6709State\u548cAction\u5c31\u53ef\u4ee5\u4f5c\u4e3a\u6211\u4eec\u7684\u8bad\u7ec3\u6570\u636e**\n\n","ecf63888":"# **\u642d\u5efaActor**","f06e8095":"# **\u7f51\u9875\u7aef\u663e\u793a\u52a8\u6001\u56fe**","32f96f95":"# **\u6a21\u578b\u6d4b\u8bd5\/\u9884\u6d4b**","94c231c4":"# **\u751f\u6210Gif**"}}