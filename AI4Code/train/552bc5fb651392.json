{"cell_type":{"6bbfef38":"code","8a2b3302":"code","86770669":"code","67fcd99b":"code","20f92e9b":"code","fc363739":"code","7daff307":"code","439296db":"code","1630085c":"code","e1ec5814":"code","a9b66cc1":"code","58ee0a8f":"code","dbacd309":"code","ffedf299":"code","55739d23":"code","706b42ad":"code","0633feb8":"code","4fd7c536":"code","6669be5b":"code","f4a4b0bc":"code","b2aaa60c":"code","4d394deb":"code","acfe30c9":"code","c16e212e":"code","4b0c701b":"code","81083342":"code","da8588d3":"code","8ff9c2d2":"code","be931f21":"code","b4df843c":"code","7709f63d":"code","4e676e37":"code","7ab8f217":"code","9487668d":"code","a190dce6":"code","b8ccffe4":"code","3f141da8":"code","35436db9":"code","7ef1fac6":"code","a7526e18":"code","81508be5":"code","348a3502":"code","a06c0a10":"code","f4957e11":"code","9435e429":"code","73387951":"code","50a34a6e":"code","a18f5332":"code","d89652f7":"code","df7dda43":"code","e77671fd":"code","9a003f65":"code","df4eafcc":"code","a8786d60":"code","e0d55560":"code","6368608f":"code","ec5f68a7":"code","6e22ac13":"code","7deed27d":"code","c0be406b":"code","473c1803":"code","705d4ff9":"code","55360b03":"code","c5380d5a":"code","b23c9b46":"code","a087bcf0":"code","f2993854":"code","826026fe":"code","650bc815":"code","e3843557":"code","14a93740":"code","88e2491a":"code","7f37ae7f":"code","480293cf":"code","9e3b3f0b":"code","248de1de":"code","2a90fba5":"code","0bb0a733":"code","d4e8d91c":"markdown","89220c5a":"markdown","4ad4a179":"markdown","aa5ff059":"markdown","ff700bb2":"markdown","9b15a6af":"markdown","d7ed8a8d":"markdown","585f8725":"markdown","f4a203ae":"markdown","e531a1e9":"markdown","b4beb3d8":"markdown","5baf10da":"markdown","fec7b1a0":"markdown","48619d77":"markdown","be12c247":"markdown","b905e054":"markdown","bf382cb6":"markdown","695104a5":"markdown","131fb17a":"markdown","ad21ba10":"markdown","db3eb60e":"markdown","626b5964":"markdown","f2aa5cc0":"markdown","2cd1b670":"markdown","67a46467":"markdown","6b3b7f92":"markdown","81c56215":"markdown","e1b52c4e":"markdown","8acda2d7":"markdown","449b4267":"markdown","f97238d4":"markdown","91d8cef6":"markdown","9a36a0ff":"markdown","8a54e803":"markdown","8139c393":"markdown","affb563f":"markdown","14a28d09":"markdown","876d5407":"markdown","660c4399":"markdown","02dcafd9":"markdown","f71f4e54":"markdown","b5fc894a":"markdown","8b48b068":"markdown","6fa9399f":"markdown","0cf4b3ba":"markdown","8b49d70f":"markdown","b88d4710":"markdown","832cbbae":"markdown","e24afabd":"markdown","d368746a":"markdown","c34f9205":"markdown","6e5df05b":"markdown","1e76f56c":"markdown","b4d33e89":"markdown","45e00dc9":"markdown","21e33643":"markdown","42ce6535":"markdown","0f2cc6b9":"markdown","3df81cd0":"markdown","822eef35":"markdown","314020a3":"markdown","4ece4a7c":"markdown","a97e0a00":"markdown"},"source":{"6bbfef38":"# For Data Visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.colors import ListedColormap\n\n# For Data Manipulation\nimport numpy as np \nimport pandas as pd\nimport sklearn\nfrom itertools import cycle\n\n\n# For Data Preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# For Classification Results\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.preprocessing import label_binarize\nfrom scipy import interp\nfrom sklearn.exceptions import NotFittedError\n\n# Dimensionality Reduction\nfrom sklearn.decomposition import PCA\n\n# Importing Models\nfrom sklearn.linear_model import LogisticRegression #Logistic Regression\nfrom sklearn.neighbors import KNeighborsClassifier as KNN #K-Nearest Neighbors\nfrom sklearn.svm import SVC #Support Vector Classifier\nfrom sklearn.naive_bayes import GaussianNB #Naive Bayes\nfrom sklearn.tree import DecisionTreeClassifier #Decision Tree Classifier\nfrom sklearn.ensemble import RandomForestClassifier #Random Forest Classifier\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.model_selection import GridSearchCV","8a2b3302":"df = pd.read_csv(\"\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\")\ndf.head()","86770669":"ax = df[\"quality\"].value_counts().plot.bar(figsize=(7,5))\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.02, p.get_height() * 1.02))\n    \nprint(df[\"quality\"].value_counts(normalize=True)*100)","67fcd99b":"df.describe()","20f92e9b":"df.isnull().sum() #No missing values","fc363739":"df[\"is good\"] = 0\ndf.loc[df[\"quality\"]>=7,\"is good\"] = 1","7daff307":"ax = df[\"is good\"].value_counts().plot.bar(figsize=(7,5))\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x(), p.get_height() * 0.5), color=\"white\")\n    \nprint(df[\"is good\"].value_counts(normalize=True)*100)","439296db":"features = df.columns[:-2]\noutput = df.columns[-1]\nprint(\"Features: \\n{}, \\n\\nLabels: \\n{}\".format(features.values,output))","1630085c":"# sns.pairplot(df[features],palette='coolwarm')\n# plt.show()","e1ec5814":"for f in features:\n    print('Feature:{}\\n Skew = {} \\n\\n'.format(f,df[f].skew()))","a9b66cc1":"corr = df[features].corr()\nplt.figure(figsize=(16,16))\nsns.heatmap(corr, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 15},\n           xticklabels= features, yticklabels= features, alpha = 0.7,   cmap= 'coolwarm')\nplt.show()","58ee0a8f":"for f in features:\n    df.boxplot(column=f, by=output)\n    plt.title(f)\nplt.show()","dbacd309":"X = df[features].values\ny = df[output].values","ffedf299":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.30)\nprint('Training size: {}, Testing size: {}'.format(X_train.size,X_test.size))","55739d23":"sc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)","706b42ad":"def get_probabilty_output(X_test, model_fitted, value_count=10):\n    def highlight_max(data, color='yellow'):\n        attr = 'background-color: {}'.format(color)\n        if data.ndim == 1:  # Series from .apply(axis=0) or axis=1\n            is_max = data == data.max()\n            return [attr if v else '' for v in is_max]\n        else:  # from .apply(axis=None)\n            is_max = data == data.max().max()\n            return pd.DataFrame(np.where(is_max, attr, ''), index=data.index, columns=data.columns)\n        \n    y_scores = model_fitted.predict_proba(X_test)\n    prob_df = pd.DataFrame(y_scores*100).head(value_count)\n    styled_df = prob_df.style.background_gradient(cmap='Reds')\n    styled_df = styled_df.highlight_max(axis=1, color='green')\n    return styled_df","0633feb8":"def get_classification_report(y_test,predictions,average=\"macro\"):\n    #Confusion Matrix\n    cm = confusion_matrix(y_test, predictions)\n    sns.heatmap(cm, annot=True)\n    plt.title(\"Confusion Matrix\")\n    \n    acc = accuracy_score(y_test, predictions)\n    pre = precision_score(y_test, predictions, average=average)\n    rec = recall_score(y_test, predictions, average=average)\n    # Prediction Report\n    print(classification_report(y_test, predictions, digits=3))\n    print(\"Overall Accuracy:\", acc)\n    print(\"Overall Precision:\", pre)\n    print(\"Overall Recall:\", rec)\n    \n    return acc,pre,rec\n    ","4fd7c536":"def get_classification_ROC(X,y,model,test_size,model_fitted=False,random_state=0):\n    \n    def check_fitted(clf): \n        return hasattr(clf, \"classes_\")\n    \n    if(len(np.unique(y)) == 2):\n        #Binary Classifier\n        if not check_fitted(model):\n            model = model.fit(X,y)\n        \n        plot_roc_curve(model, X, y)\n        y_score = model.predict_proba(X)[:, 1]\n        fpr, tpr, threshold = roc_curve(y, y_score)\n        auc = roc_auc_score(y, y_score)\n        return auc\n#         print(\"False Positive Rate: {} \\nTrue Positive Rate: {} \\nThreshold:{}\".format(fpr,tpr,threshold))\n    \n    else:\n        #Multiclass Classifier\n        y_bin = label_binarize(y, classes=np.unique(y))\n        n_classes = y_bin.shape[1]\n\n        # shuffle and split training and test sets\n        X_train, X_test, y_train, y_test = train_test_split(X, y_bin, test_size=test_size, random_state=random_state)\n\n        # Learn to predict each class against the other\n        classifier = OneVsRestClassifier(model)\n        model_fitted = classifier.fit(X_train, y_train)\n        try:\n            y_score = model_fitted.decision_function(X_test)\n        except:\n            y_score = model_fitted.predict_proba(X_test)\n\n\n\n        # Compute ROC curve and ROC area for each class\n        fpr = dict()\n        tpr = dict()\n        roc_auc = dict()\n        for i in range(n_classes):\n            fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n            roc_auc[i] = auc(fpr[i], tpr[i])\n\n\n        # Compute micro-average ROC curve and ROC area\n        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\n\n        plt.figure()\n        lw = 2\n        plt.plot(fpr[2], tpr[2], color='darkorange',\n                 lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])\n        plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.05])\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title('Receiver operating characteristic averaged')\n        plt.legend(loc=\"lower right\")\n        plt.show()\n\n\n\n        # First aggregate all false positive rates\n        all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n\n        # Then interpolate all ROC curves at this points\n        mean_tpr = np.zeros_like(all_fpr)\n        for i in range(n_classes):\n            mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n\n        # Finally average it and compute AUC\n        mean_tpr \/= n_classes\n\n        fpr[\"macro\"] = all_fpr\n        tpr[\"macro\"] = mean_tpr\n        roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\n        # Plot all ROC curves\n        plt.figure(figsize=(10,10))\n        plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n                 label='micro-average ROC curve (area = {0:0.2f})'\n                       ''.format(roc_auc[\"micro\"]),\n                 color='deeppink', linestyle=':', linewidth=4)\n\n        plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n                 label='macro-average ROC curve (area = {0:0.2f})'\n                       ''.format(roc_auc[\"macro\"]),\n                 color='navy', linestyle=':', linewidth=4)\n\n        colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'red', 'blue', 'purple', 'green'])\n        for i, color in zip(range(n_classes), colors):\n            plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n                     label='ROC curve of class {0} (area = {1:0.2f})'\n                     ''.format(i, roc_auc[i]))\n\n        plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.05])\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title('multi-class ROC (One vs All)')\n        plt.legend(loc=\"lower right\")\n        plt.show()","6669be5b":"def visualisation_through_PCA(X_PCA, y, model_PCA, model_name=\"Classification Model\"):\n    X_set, y_set = X_PCA, y\n    X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                         np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n    plt.contourf(X1, X2, model_PCA.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n                 alpha = 0.75, cmap = ListedColormap(('red', 'green', 'blue', 'yellow', 'purple', 'grey')))\n    plt.xlim(X1.min(), X1.max())\n    plt.ylim(X2.min(), X2.max())\n    for i, j in enumerate(np.unique(y_set)):\n        plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                    c = ListedColormap(('red', 'green', 'blue', 'yellow', 'purple', 'grey'))(i), label = j)\n    plt.title(model_name)\n    plt.xlabel('Principal Component 1')\n    plt.ylabel('Principal Component 2')\n    plt.legend()\n    plt.show()","f4a4b0bc":"pca = PCA(n_components = 2)\nX_train_PCA_2 = pca.fit_transform(X_train)\nX_test_PCA_2 = pca.transform(X_test)\nexplained_variance = pca.explained_variance_ratio_\nprint(\"Variance Explained by each of the Principal Components: {:.{prec}f}% and {:.{prec}f}%, \\nTotal Variance Explained: {:.{prec}f}%\".format((explained_variance*100)[0],\n                                                                                                                                               (explained_variance*100)[1],\n                                                                                                                                                  explained_variance.sum()*100,prec=3))","b2aaa60c":"parameters_LR = {\n    \"solver\" : ('newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'),\n    \"penalty\" : ('l1', 'l2', 'elasticnet', 'none'),\n    \"C\" : [0.01, 0.1, 1, 10, 1000]\n    \n}\n\nmodel_LR = LogisticRegression()\nmodel_LR_with_best_params = GridSearchCV(model_LR, parameters_LR)\nmodel_LR_with_best_params.fit(X_train,y_train)\nmodel_LR_best_params = model_LR_with_best_params.best_params_","4d394deb":"model_LR_best_params","acfe30c9":"predictions_LR = model_LR_with_best_params.predict(X_test)\nprint(\"Predictions:\",predictions_LR[:10])\nprint(\"Actual:\",y_test[:10])","c16e212e":"get_probabilty_output(X_test=X_test, model_fitted=model_LR_with_best_params, value_count=15)","4b0c701b":"acc_LR,pre_LR,rec_LR = get_classification_report(y_test,predictions_LR)","81083342":"auc_LR = get_classification_ROC(X_test,y_test,model_LR_with_best_params,test_size=0.3,random_state=0)","da8588d3":"# model_LR_PCA = LogisticRegression(random_state = 0)\n# model_LR_PCA.fit(X_train_PCA_2, y_train)\n# predictions_LR_PCA = model_LR_PCA.predict(X_test_PCA_2)","8ff9c2d2":"# visualisation_through_PCA(X_train_PCA_2, y_train, model_LR_PCA, model_name=\"Logisitic Regression (Training Set)\")","be931f21":"# visualisation_through_PCA(X_test_PCA_2, y_test, model_LR_PCA, model_name=\"Logisitic Regression (Test Set)\")","b4df843c":"parameters_KNN = {\n    \"n_neighbors\" : [2,5,7,15],\n    \"weights\" : ('uniform','distance'),\n    \"algorithm\" : ('auto','ball_tree','kd_tree','brute'),\n    'p': [1,2,5]\n    \n    \n}\n\nmodel_KNN = KNN(n_jobs=-1)\nmodel_KNN_with_best_params = GridSearchCV(model_KNN, parameters_KNN)\nmodel_KNN_with_best_params.fit(X_train,y_train)\nmodel_KNN_best_params = model_KNN_with_best_params.best_params_","7709f63d":"model_KNN_best_params","4e676e37":"predictions_KNN = model_KNN_with_best_params.predict(X_test)\nprint(\"Predictions:\",predictions_KNN[:10])\nprint(\"Actual:\",y_test[:10])","7ab8f217":"get_probabilty_output(X_test=X_test, model_fitted=model_KNN_with_best_params)","9487668d":"acc_KNN,pre_KNN,rec_KNN = get_classification_report(y_test,predictions_KNN)","a190dce6":"auc_KNN = get_classification_ROC(X_test,y_test,model_KNN_with_best_params,test_size=0.3,random_state=0)","b8ccffe4":"# model_KNN_PCA = KNN(5)\n# model_KNN_PCA.fit(X_train_PCA_2, y_train)\n# predictions_KNN_PCA = model_KNN_PCA.predict(X_test_PCA_2)","3f141da8":"# visualisation_through_PCA(X_train_PCA_2, y_train, model_KNN_PCA, model_name=\"k-Nearest Neighbors (Training Set)\")","35436db9":"# visualisation_through_PCA(X_test_PCA_2, y_test, model_KNN_PCA, model_name=\"k-Nearest Neighbors (Test Set)\")","7ef1fac6":"parameters_SVC = {\n    \"C\": [0.1, 1, 10],\n    \"kernel\": ('linear','poly','rbf'),\n    \"degree\": [2,4] \n    \n}\n\nmodel_SVC = SVC(probability=True)\nmodel_SVC_with_best_params = GridSearchCV(model_SVC, parameters_SVC)\nmodel_SVC_with_best_params.fit(X_train,y_train)\nmodel_SVC_best_params = model_SVC_with_best_params.best_params_","a7526e18":"model_SVC_best_params","81508be5":"predictions_SVC = model_SVC_with_best_params.predict(X_test)\nprint(\"Predictions:\",predictions_SVC[:10])\nprint(\"Actual:\",y_test[:10])","348a3502":"get_probabilty_output(X_test=X_test, model_fitted=model_SVC_with_best_params)","a06c0a10":"acc_SVC,pre_SVC,rec_SVC = get_classification_report(y_test,predictions_SVC)","f4957e11":"auc_SVC = get_classification_ROC(X_test,y_test,model_SVC_with_best_params,test_size=0.3,random_state=0)","9435e429":"# model_SVC_PCA = model_SVC = SVC(kernel=kernel, random_state=random_state, probability=True)\n# model_SVC_PCA.fit(X_train_PCA_2, y_train)\n# predictions_SVC_PCA = model_SVC_PCA.predict(X_test_PCA_2)","73387951":"# visualisation_through_PCA(X_train_PCA_2, y_train, model_SVC_PCA, model_name=\"Support Vector Classifier (Training Set)\")","50a34a6e":"# visualisation_through_PCA(X_test_PCA_2, y_test, model_SVC_PCA, model_name=\"Support Vector Classifier (Test Set)\")","a18f5332":"model_NB = GaussianNB()\nmodel_NB.fit(X_train, y_train)","d89652f7":"predictions_NB = model_NB.predict(X_test)\nprint(\"Predictions:\",predictions_NB[:10])\nprint(\"Actual:\",y_test[:10])","df7dda43":"get_probabilty_output(X_test=X_test, model_fitted=model_NB)","e77671fd":"acc_NB,pre_NB,rec_NB = get_classification_report(y_test,predictions_NB)","9a003f65":"auc_NB = get_classification_ROC(X_test,y_test,model_NB,test_size=0.3,random_state=0)","df4eafcc":"# model_NB_PCA = GaussianNB()\n# model_NB_PCA.fit(X_train_PCA_2, y_train)\n# predictions_NB_PCA = model_NB_PCA.predict(X_test_PCA_2)","a8786d60":"# visualisation_through_PCA(X_train_PCA_2, y_train, model_NB_PCA, model_name=\"Naive Bayes Classifier (Training Set)\")","e0d55560":"# visualisation_through_PCA(X_test_PCA_2, y_test, model_NB_PCA, model_name=\"Naive Bayes Classifier (Test Set)\")","6368608f":"parameters_DT = {\n    'criterion':('gini','entropy'),\n    'max_features': ('auto','sqrt','log2')\n}\n\n\nmodel_DT = DecisionTreeClassifier()\nmodel_DT_with_best_params = GridSearchCV(model_DT, parameters_DT)\nmodel_DT_with_best_params.fit(X_train,y_train)\nmodel_DT_best_params = model_DT_with_best_params.best_params_\nmodel_DT_with_best_params.fit(X_train,y_train)","ec5f68a7":"model_DT_best_params","6e22ac13":"predictions_DT = model_DT_with_best_params.predict(X_test)\nprint(\"Predictions:\",predictions_DT[:10])\nprint(\"Actual:\",y_test[:10])","7deed27d":"get_probabilty_output(X_test=X_test, model_fitted=model_DT_with_best_params)","c0be406b":"acc_DT,pre_DT,rec_DT = get_classification_report(y_test,predictions_DT)","473c1803":"auc_DT = get_classification_ROC(X_test,y_test,model_DT_with_best_params,test_size=0.3,random_state=0)","705d4ff9":"# model_DT_PCA = DecisionTreeClassifier(criterion=\"entropy\", random_state=0)\n# model_DT_PCA.fit(X_train_PCA_2, y_train)\n# predictions_DT_PCA = model_DT_PCA.predict(X_test_PCA_2)","55360b03":"# visualisation_through_PCA(X_train_PCA_2, y_train, model_DT_PCA, model_name=\"Decision Tree Classifier (Training Set)\")","c5380d5a":"# visualisation_through_PCA(X_test_PCA_2, y_test, model_DT_PCA, model_name=\"Decision Tree Classifier (Test Set)\")","b23c9b46":"parameters_RF = {\n    'criterion':('gini','entropy'),\n    'max_features': ('auto','sqrt','log2'),\n    'n_estimators': [100,150,200,250,300]\n}\n\n\nmodel_RF = RandomForestClassifier(n_jobs=-1)\nmodel_RF_with_best_params = GridSearchCV(model_RF, parameters_RF)\nmodel_RF_with_best_params.fit(X_train,y_train)\nmodel_RF_best_params = model_RF_with_best_params.best_params_\nmodel_RF_with_best_params.fit(X_train,y_train)","a087bcf0":"model_RF_best_params","f2993854":"predictions_RF = model_RF_with_best_params.predict(X_test)\nprint(\"Predictions:\",predictions_DT[:10])\nprint(\"Actual:\",y_test[:10])","826026fe":"get_probabilty_output(X_test=X_test, model_fitted=model_RF_with_best_params)","650bc815":"acc_RF,pre_RF,rec_RF = get_classification_report(y_test,predictions_RF)","e3843557":"auc_RF = get_classification_ROC(X_test,y_test,model_RF_with_best_params,test_size=0.3,random_state=0)","14a93740":"# model_RF_PCA = RandomForestClassifier(n_estimators = 10, criterion=\"entropy\", random_state=0)\n# model_RF_PCA.fit(X_train_PCA_2, y_train)\n# predictions_RF_PCA = model_RF_PCA.predict(X_test_PCA_2)","88e2491a":"# visualisation_through_PCA(X_train_PCA_2, y_train, model_RF_PCA, model_name=\"Random Forest Classifier (Training Set)\")","7f37ae7f":"# visualisation_through_PCA(X_test_PCA_2, y_test, model_RF_PCA, model_name=\"Random Forest Classifier (Test Set)\")","480293cf":"result = pd.DataFrame(\n    [[\"LogisticRegression\",auc_LR,acc_LR,pre_LR,rec_LR],\n    [\"kNearestNeighbor\",auc_KNN,acc_KNN,pre_KNN,rec_KNN],\n    [\"SupportVectorClassifier\",auc_SVC,acc_SVC,pre_SVC,rec_SVC],\n    [\"NaiveBayes\",auc_NB,acc_NB,pre_NB,rec_NB],\n    [\"DecisionTree\",auc_DT,acc_DT,pre_DT,rec_DT],\n    [\"RandomForest\",auc_RF,acc_RF,pre_RF,rec_RF]],\n    columns=[\"Classifier\",\"AUC\",\"Accuracy\",\"Precision\",\"Recall\"]\n)\n\nresult","9e3b3f0b":"fig = plt.figure(figsize=(10,5))\nax = fig.add_axes([0,0,1,1])\nx = result.Classifier\ny = result.AUC\nsns.barplot(x=x, y=y)\nplt.title(\"AUC Score Comparision\")\nplt.show()","248de1de":"fig = plt.figure(figsize=(10,5))\nax = fig.add_axes([0,0,1,1])\nx = result.Classifier\ny = result.Accuracy\nsns.barplot(x=x, y=y)\nplt.title(\"Accuracy Comparision\")\nplt.show()","2a90fba5":"fig = plt.figure(figsize=(10,5))\nax = fig.add_axes([0,0,1,1])\nx = result.Classifier\ny = result.Precision\nsns.barplot(x=x, y=y)\nplt.title(\"Precision Comparision\")\nplt.show()","0bb0a733":"fig = plt.figure(figsize=(10,5))\nax = fig.add_axes([0,0,1,1])\nx = result.Classifier\ny = result.Recall\nsns.barplot(x=x, y=y)\nplt.title(\"Recall Comparision\")\nplt.show()","d4e8d91c":"### Predicting","89220c5a":"## Checking for Dataset skewness","4ad4a179":"### Visualising through PCA","aa5ff059":"Let's start defining classification models <br> Models which are implemented: <br>\n* Logitic Regression\n* k-Nearest Neighbors\n* Support Vector Classifier\n* Naive Bayes\n* Decision Tree\n* Random Forest","ff700bb2":"### Predicting","9b15a6af":"## Importing Libraries","d7ed8a8d":"It is suggested to make the ```quality``` a binary variable. Let's say for ```quality``` >= 7 is good quality or ```is good``` = 1 and for ```quality``` < 7 is not of good quality or ```is good``` = 0","585f8725":"### Visualisation through PCA","f4a203ae":"### Visualising PCA","e531a1e9":"### Analysing Feature Distribution","b4beb3d8":"## Data Splitting","5baf10da":"# Random Forest Classifier","fec7b1a0":"### Analysing Feature Correlation","48619d77":"### Results","be12c247":"# Support Vector Classifier (SVC)","b905e054":"# k-Nearest Neighbor Classifier","bf382cb6":"# Wine Quality Classification ","695104a5":"# Functions for Analysing Results","131fb17a":"We can see the dataset is skewed (unbalanced). <br>\nOf whole dataset **~5% belong to class 4, 8 and 3 combined**","ad21ba10":"### Fitting model on Principal Component dataset","db3eb60e":"### Fitting model on Principal Component dataset","626b5964":"## Classification ROC","f2aa5cc0":"### Fitting model on Principal Component dataset","2cd1b670":"### Fitting model on Principal Component dataset","67a46467":"## Model Comparision","6b3b7f92":"### Results","81c56215":"# Final Results","e1b52c4e":"## Classification Report","8acda2d7":"### Visualising through PCA","449b4267":"# Decision Tree Classifier ","f97238d4":"### Fitting model on Principal Component dataset","91d8cef6":"### Creating Model","9a36a0ff":"Although, we can see the explained variance is ~46% hence, it is **NOT** suggested to go for just 2 components. But since we need to visualise the the dataset and classification boundries, we will go for 2 components. ","8a54e803":"### Creating Model","8139c393":"## Generating Principal Components of the feature dataset ","affb563f":"### Creating Model","14a28d09":"## Checking for missing values","876d5407":"### Result","660c4399":"### Predicting","02dcafd9":"### Predicting ","f71f4e54":"In this notebook, I will be implementing multiple classfication algorithms on a binary (or multiclass if required) classification problem. I created simple functions <br>for generating ROC curve for both binary or multiclass classification (using One-vs-Rest),\n<br>for visualising results in 2 dimensions via Principal Component Analysis,\n<br>for getting classification report,\n<br>for looking classification results row wise along with each class' prediction probabilty.<br><br>\nFeel free to use it as a template for any other classification problem. Do upvote :)","b5fc894a":"### Results","8b48b068":"# Classification Models","6fa9399f":"## Reading Dataset","0cf4b3ba":"### Visualising through PCA","8b49d70f":"### Fitting model on Principal Component dataset","b88d4710":"### Creating Model","832cbbae":"### Predicting","e24afabd":"### Creating Model","d368746a":"## Data Scaling","c34f9205":"# Naive Bayes Classifier","6e5df05b":"### Results","1e76f56c":"### Predicting","b4d33e89":"#### by Logistic Regression, k-Nearest Neighbor, Support Vector Classifier, Naive Bayes, Decision Tree and Random Forest","45e00dc9":"## Probability Output","21e33643":"### Visualising through PCA","42ce6535":"We can see there are many ","0f2cc6b9":"# Logisitic Regression ","3df81cd0":"## Visualisation Through PCA","822eef35":"### Result","314020a3":"## Exploring Features ","4ece4a7c":"There are no missing values. Pretty clean dataset!","a97e0a00":"### Creating Model"}}