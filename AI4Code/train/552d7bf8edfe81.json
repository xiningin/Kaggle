{"cell_type":{"b162e3d7":"code","504741ca":"code","d0f9b727":"code","f2e6a0ab":"code","fadda9ee":"code","83bccd29":"code","7b758b3a":"code","063137a2":"markdown","8b4c5e93":"markdown","8c4d457b":"markdown","aaeb1679":"markdown","d2f01e3e":"markdown","50530c8c":"markdown"},"source":{"b162e3d7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","504741ca":"import numpy as np\nimport pandas as pd\npd.set_option('display.max_columns', 70)\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.ensemble import RandomForestClassifier\nimport warnings\nwarnings.simplefilter(action='ignore')","d0f9b727":"train = pd.read_csv('..\/input\/learn-together\/train.csv')\ntest = pd.read_csv('..\/input\/learn-together\/test.csv')","f2e6a0ab":"cover_type = {1:'Spruce\/Fir', 2:'Lodgepole Pine',3:'Ponderosa Pine',4:'Cottonwood\/Willow',5:'Aspen',6:'Douglas-fir',7:'Krummholz'}\ntrain['Cover_type_description'] = train['Cover_Type'].map(cover_type)\n\ncombined_data = [train, test]\n\ndef distance(a,b):\n    return np.sqrt(np.power(a,2)+np.power(b,2))\n\nextremely_stony = [1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1]\nstony = [0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\nrubbly = [0,0,1,1,1,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\nother = [0,0,0,0,0,0,1,1,0,0,0,0,0,1,0,1,1,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0]\n    \n\nfor data in combined_data:\n    \n    # grouping of Aspect feature\n    data['Azimut'] = 0\n    data['Azimut'].loc[(data['Aspect'] == 0)] = 'north'\n    data['Azimut'].loc[(data['Aspect']>0 ) & (data['Aspect']< 90)] = 'north_east'\n    data['Azimut'].loc[data['Aspect']==90] = 'east'\n    data['Azimut'].loc[(data['Aspect'] >90) & (data['Aspect'] <180)] = 'south_east'\n    data['Azimut'].loc[data['Aspect']==180] = 'south'\n    data['Azimut'].loc[(data['Aspect']>180) & (data['Aspect']<270)] = 'south_west'\n    data['Azimut'].loc[data['Aspect']== 270] = 'west'\n    data['Azimut'].loc[(data['Aspect']> 270) & (data['Aspect']< 360)] = 'noth_west'\n    data['Azimut'].loc[data['Aspect']== 360] = 'north'\n    \n    #grouping of Elevation feature\n    \n    data['Elevation_bins'] = 0\n    data['Elevation_bins'].loc[data['Elevation']<= 2000] = 'less than 2000'\n    data['Elevation_bins'].loc[(data['Elevation'] > 2000) & (data['Elevation']< 2500)] = 'between 2000 and 2500'\n    data['Elevation_bins'].loc[(data['Elevation'] > 2500) & (data['Elevation'] <= 3000)] = 'between 2000 and 3000'\n    data['Elevation_bins'].loc[(data['Elevation'] > 3000) & (data['Elevation'] <= 3500)] = 'between 3000 and 3500'\n    data['Elevation_bins'].loc[data['Elevation'] > 3500] = 'greater than 3500'\n    \n    # grouping for slope\n    data['Slope_category'] = 0\n    data['Slope_category'].loc[(data['Slope'] <= 10)] = 'slope less than 10'\n    data['Slope_category'].loc[(data['Slope'] > 10) & (data['Slope'] <= 20)] = 'slope between 10 and 20'\n    data['Slope_category'].loc[(data['Slope'] > 20) & (data['Slope'] <= 30)] = 'slope between 20 and 30'\n    data['Slope_category'].loc[(data['Slope'] > 30)] = 'slope greater than 30'\n    \n    data['mean_Hillshade'] = (data['Hillshade_9am']+ data['Hillshade_Noon']+data['Hillshade_3pm'])\/3\n    \n    data['Distance_to_hidrology'] = distance(data['Horizontal_Distance_To_Hydrology'],data['Vertical_Distance_To_Hydrology'])\n    \n\n    # there are 40 differents type of soil in Roosvelt National Forest but there are some common features between them\n    # we can try to group soils according to some common features like stony \n    \n    data['extremely_stony_level'] = data[[col for col in data.columns if col.startswith(\"Soil\")]]@extremely_stony\n    data['stony'] = data[[col for col in data.columns if col.startswith(\"Soil\")]]@stony\n    data['rubbly'] = data[[col for col in data.columns if col.startswith(\"Soil\")]]@rubbly\n    data['other'] = data[[col for col in data.columns if col.startswith(\"Soil\")]]@other\n   \n    data['Hillshade_noon_3pm'] = data['Hillshade_Noon']- data['Hillshade_3pm']\n    data['Hillshade_3pm_9am'] = data['Hillshade_3pm']- data['Hillshade_9am']\n    data['Hillshade_9am_noon'] = data['Hillshade_9am']- data['Hillshade_Noon']\n    \n    data['Up_the_water'] = data['Vertical_Distance_To_Hydrology'] > 0\n    data['Total_horizontal_distance'] = data['Horizontal_Distance_To_Hydrology']+ data['Horizontal_Distance_To_Roadways']+ data['Horizontal_Distance_To_Fire_Points']\n    data['Elevation_of_hydrology'] = data['Elevation']+ data['Vertical_Distance_To_Hydrology']\n    data['Distance_to_firepoints plus Distance_to_roads'] = data['Horizontal_Distance_To_Fire_Points']+ data['Horizontal_Distance_To_Roadways']\n    data['Distance_to_roads plus distance_to_hydrology'] = data['Horizontal_Distance_To_Roadways'] + data['Horizontal_Distance_To_Hydrology']\n    data['Distance_to_firepoints minus Distance_to_roads'] = data['Horizontal_Distance_To_Fire_Points']- data['Horizontal_Distance_To_Roadways']\n    data['Distance_to_roads minus distance_to_hydrology'] = data['Horizontal_Distance_To_Roadways'] - data['Horizontal_Distance_To_Hydrology']\n\n\nsoil_columns = [col for col in train.columns if col.startswith('Soil')]\n\n\n# we can drop soil columns because we have group each soil according to their stoyness\n\nfor data in combined_data:\n    data.drop(columns = soil_columns, inplace=True)\n    ","fadda9ee":"# for categorical values we can encode them into dummy values \n    \ncolumns_to_encode = ['Azimut','Elevation_bins','Slope_category','Up_the_water']\n    \ntrain = pd.get_dummies(train,columns = columns_to_encode)\ntest = pd.get_dummies(test,columns = columns_to_encode)\n\n# once we have encoded the columns we can drop them from both training and testing dataset\n\nfor data in combined_data:\n    data.drop(columns= columns_to_encode, inplace=True)\n\n# after encoding all variables are numerical but, as we can see, thier scale is very different; there are variable encoded whose value is 0 or 1 and there are some\n# others variables like Elevation who are much bigger\n# we need to scale variable in order to make them confrontable\n\ncolumns_to_scale = ['Elevation', 'Aspect', 'Slope',\n       'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology',\n       'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon',\n       'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points',\n       'mean_Hillshade', 'Distance_to_hidrology','Hillshade_noon_3pm', 'Hillshade_3pm_9am',\n       'Hillshade_9am_noon', 'Total_horizontal_distance',\n       'Elevation_of_hydrology',\n       'Distance_to_firepoints plus Distance_to_roads',\n       'Distance_to_roads plus distance_to_hydrology',\n       'Distance_to_firepoints minus Distance_to_roads',\n       'Distance_to_roads minus distance_to_hydrology']\n    \n\ns_scaler = preprocessing.StandardScaler()\ntrain_columns_to_scale = train[columns_to_scale]\ntest_columns_to_scale = test[columns_to_scale]\n\ntrain_scaled = s_scaler.fit_transform(train_columns_to_scale)\ntest_scaled = s_scaler.fit_transform(test_columns_to_scale)\n\ntrain_scaled_df = pd.DataFrame(data = train_scaled, columns = columns_to_scale)\ntest_scaled_df = pd.DataFrame(data = test_scaled, columns = columns_to_scale)\n\n# dropping columns scaled from training and testing dataset\ntrain.drop(columns = columns_to_scale, inplace = True)\ntest.drop(columns = columns_to_scale, inplace = True)\n\n#now we can concatenate scaled columns to both training and testing dataset\n    \ntrain_final = pd.concat([train , train_scaled_df],axis = 1)\ntest_final = pd.concat([test  ,test_scaled_df],axis = 1)","83bccd29":"\n#defining targer variable and features\ntarget = 'Cover_Type'\nfeatures = [ col for col in train_final.columns if col not in ['Id','Cover_Type','Cover_type_description']]\n\nX = train_final[features]\ny = train_final[target]\n\n#BUILDING THE MODEL\n\nX_train, X_test,y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1)\n\nfrom sklearn.model_selection import KFold\n\nKfold = KFold(n_splits= 50,shuffle = True,  random_state= 1)\n\n#GRID_SEARCH IN ORDER TO FIND OUT BEST PARAMETERS FOR THE MODEL\n    \nrandom_forest  = RandomForestClassifier()\nparams_decision_random_forest = {'n_estimators':[100,150,200], 'criterion':['gini','entropy'],'max_features':['auto','sqrt','log2']}\ngrid_search_random_forest = GridSearchCV(random_forest, param_grid =params_decision_random_forest, cv=Kfold,scoring= 'neg_mean_squared_error', n_jobs=-1)\ngrid_search_random_forest.fit(X_train, y_train)\nprint('Best parameters for random forest classifier:', grid_search_random_forest.best_params_)\n\ndecision_random_forest = RandomForestClassifier(n_estimators = grid_search_random_forest.best_params_['n_estimators'],criterion = grid_search_random_forest.best_params_['criterion'], max_features = grid_search_random_forest.best_params_['max_features']) \ndecision_random_forest.fit(X_train,y_train)\n","7b758b3a":"X_test = test_final[features]\ny_pred = decision_random_forest.predict(X_test)\n\nsub = pd.DataFrame({'Id': test.Id, 'Cover_Type': y_pred})\nsub.to_csv('submission_csv', index = False)","063137a2":"# CLASSIFY FOREST TYPES\n\n## 1 - Features Enginering\n## 2 - Data Preprocessing \n## 3 - Model Building with Gridsearch\n## 4 - Submitting","8b4c5e93":"## 3 - Model Building with Gridsearch","8c4d457b":"## 1 - Features Enginering","aaeb1679":"## 2 - Data Preprocessing ","d2f01e3e":"## 4 - Submitting","50530c8c":"# --------------------------------------------\n\n\n\n"}}