{"cell_type":{"a820447f":"code","183e4109":"code","ea4611f0":"code","36e8dd2f":"code","253f0e80":"code","a1aefad7":"code","85d0d61c":"code","5d789bf9":"code","11bc7957":"code","8f53dc37":"code","b4727ace":"code","962e4271":"code","84d891c0":"code","7370ac06":"code","36ccf33b":"code","c499e7f8":"code","74019bec":"code","edcaa3a1":"code","32539551":"code","8cc72420":"code","5f12881d":"code","3634c997":"code","0f8b7bb0":"code","ceaec62c":"code","0e02cfa6":"code","d6a21762":"code","91b5944e":"code","ed18da8a":"code","8163559b":"code","f4c15df1":"code","dced14bf":"code","47f4d9e8":"code","1f3a7777":"code","18a423fb":"code","0bdf51d7":"code","8e3b911d":"code","2b6b460c":"code","a21523c6":"code","5acd33cb":"code","246d5016":"code","e15ddbda":"markdown","1cf94ef0":"markdown","07badd58":"markdown","bde49a51":"markdown","0bae4c7b":"markdown","ecba65ff":"markdown","37f59adb":"markdown","0b008516":"markdown","02f8eed4":"markdown","29c26767":"markdown","b60f50b8":"markdown","3cf8f862":"markdown","962868e2":"markdown","8fc04b49":"markdown","39592b4d":"markdown","293df826":"markdown","06a95e8b":"markdown","402a0353":"markdown","6d8efad4":"markdown","e1be2f84":"markdown","a43d57ee":"markdown","6c809bf9":"markdown","d46518e5":"markdown","6cc8f22e":"markdown","75d764e4":"markdown","1b164cee":"markdown","2fcce358":"markdown","cb296118":"markdown","f0dafdfe":"markdown","f27c032a":"markdown","82c5fdd5":"markdown","e5268fd8":"markdown","ca1c7523":"markdown","746add6e":"markdown","ad16280f":"markdown","7225cf65":"markdown","a60d1698":"markdown","ac7eb708":"markdown","e379733f":"markdown","013be823":"markdown","069de9b9":"markdown","73a5c910":"markdown","6c97b0b5":"markdown","463bf2f6":"markdown","eefeac2d":"markdown","c821f80c":"markdown","c23e3ef2":"markdown","a0231952":"markdown","6d7e2ea4":"markdown","dc51b66e":"markdown","99ac6fdd":"markdown","f0edc321":"markdown","d34358ad":"markdown","c5e4ed11":"markdown","7bda6b4b":"markdown","d91f2abe":"markdown"},"source":{"a820447f":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport math \nfrom scipy import stats as ss\n","183e4109":"df = pd.read_csv('\/kaggle\/input\/heart-disease-uci\/heart.csv')\n\npd.options.display.max_columns = None\npd.options.display.max_rows = None\ndf.head(10)","ea4611f0":"columns_names = ['age', 'sex_male', 'cp', 'rest_bp', 'chol', 'fbs_above_120', 'rest_ecg',\n       'max_bps', 'ex_angina', 'st_depression', 'ecg_st_slope', 'cardiac_fluoro', 'thallium', 'target']\ndf.columns = columns_names","36e8dd2f":"columns_names_reordered = ['age', 'sex_male', 'rest_bp', 'chol', 'fbs_above_120',\n       'max_bps', 'cp', 'ex_angina', 'rest_ecg', 'st_depression', 'ecg_st_slope', 'thallium', 'cardiac_fluoro', 'target']\ndf = df[columns_names_reordered]","253f0e80":"#plot settings\nsns.set(style = 'darkgrid')\nplt.figure(figsize = (6,3))\n\n#countplot\nsns.countplot(x = \"target\", data = df, palette=\"Set2\")\nplt.title('Heart Disease Count')","a1aefad7":"#plot settings\nsns.set(style = 'darkgrid')\nfig, (ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8) = plt.subplots(1,8,figsize=(16,8))\nfig.subplots_adjust(hspace=0.4, wspace=0.7)\n\n#countplots\nsns.countplot(x = 'sex_male', hue ='target', data = df, ax = ax1, palette = 'Set2')\nsns.countplot(x = 'cp', hue = 'target', data = df, ax = ax2, palette = 'Set2')\nsns.countplot(x = 'fbs_above_120', hue = 'target', data = df, ax = ax3, palette = 'Set2')\nsns.countplot(x = 'rest_ecg', hue = 'target', data = df, ax = ax4, palette = 'Set2')\nsns.countplot(x = \"ex_angina\", hue = 'target', data = df, ax = ax5, palette = 'Set2')\nsns.countplot(x = \"cardiac_fluoro\", hue = 'target', data = df, ax = ax6, palette = 'Set2')\nsns.countplot(x = \"thallium\", hue = 'target', data = df, ax = ax7, palette = 'Set2')\nsns.countplot(x = \"ecg_st_slope\", hue = 'target', data = df, ax = ax8, palette = 'Set2')","85d0d61c":"#plot settings\nsns.set(style = 'darkgrid')\nfig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1,5,figsize=(16,8))\nfig.subplots_adjust(hspace=0.4, wspace=0.7)\n\n#boxplots\nsns.boxplot(x = 'target', y ='age', data = df, ax = ax1, palette = 'Set2')\nsns.boxplot(x = 'target', y ='rest_bp', data = df, ax = ax2, palette = 'Set2')\nsns.boxplot(x = 'target', y ='chol', data = df, ax = ax3, palette = 'Set2')\nsns.boxplot(x = 'target', y ='max_bps', data = df, ax = ax4, palette = 'Set2')\nsns.boxplot(x = 'target', y ='st_depression', data = df, ax = ax5, palette = 'Set2')","5d789bf9":"#plot settings\nplt.figure(figsize = (14,12))\n\n#plot pairwise graphs\nax = sns.PairGrid(df, vars=['age', 'rest_bp','chol', 'max_bps', 'st_depression'], hue=\"target\", palette = 'Set2')\nax.map_upper(plt.scatter)\nax.map_lower(sns.kdeplot)\nax.map_diag(sns.kdeplot, lw=3, legend=False)\nax.add_legend()","11bc7957":"#specify the numerical variables\ndf_num = df[['age', 'rest_bp', 'chol', 'max_bps', 'st_depression', 'cardiac_fluoro']]","8f53dc37":"#add a mask to hide identical pairwise correlations\nmask = np.zeros_like(df_num.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n#plot settings\nsns.set(font_scale=1.25)\nplt.figure(figsize = (15,10))\n\n#correlation matrix\ncorrMatrix = df_num.corr()\ng = sns.heatmap(corrMatrix, vmin = -1,cmap='coolwarm', fmt='.2f', annot = True,\n                square = True, linewidths = .5,mask = mask)\ng.set_xticklabels(g.get_xticklabels(), rotation=45, horizontalalignment='right')\ng.set_yticklabels(g.get_yticklabels(), rotation=45, horizontalalignment='right')\n","b4727ace":"#specify the categorical variables\ndf_cramer = df[['cp', 'sex_male', 'fbs_above_120', 'ex_angina', 'rest_ecg', 'ecg_st_slope', 'thallium', 'target']]","962e4271":"#Cramer's V correlation matrix, used for categorical variables\n#https:\/\/towardsdatascience.com\/the-search-for-categorical-correlation-a1cf7f1888c9\ndef cramers_v(x, y):\n    confusion_matrix = pd.crosstab(x,y)\n    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2\/n\n    r,k = confusion_matrix.shape\n    phi2corr = max(0, phi2-((k-1)*(r-1))\/(n-1))\n    rcorr = r-((r-1)**2)\/(n-1)\n    kcorr = k-((k-1)**2)\/(n-1)\n    return np.sqrt(phi2corr\/min((kcorr-1),(rcorr-1)))","84d891c0":"#create the cramers correlation matrix\nrows= []\n\nfor var1 in df_cramer:\n    col = []\n    for var2 in df_cramer:\n        cramers =cramers_v(df_cramer[var1], df_cramer[var2])\n        col.append(round(cramers,2)) \n    rows.append(col)\n\ncramers_results = np.array(rows)\ndf_cramer_corr = pd.DataFrame(cramers_results, columns = df_cramer.columns, index =df_cramer.columns)\n\n#mask the diagonial and top triangle\nmask = np.zeros_like(df_cramer_corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n#set correlation graph size\nsns.set(font_scale=1)\nplt.figure(figsize = (10,10))\n\n#correlation graph settings\nax = sns.heatmap(df_cramer_corr, vmin=0., vmax=1,cmap='coolwarm', fmt='.2f', annot = True, square = True,\n                 linewidths = .5,mask = mask)\nplt.show()","7370ac06":"df.info()","36ccf33b":"#Create dummy variables for the following categorical attributes\n#cp, rest_ecg, ecg_st_slope, cardiac_fluoro, thallium\ncp_dummy = pd.get_dummies(df['cp'], drop_first = True)\ncp_dummy.columns = ['cp_typical', 'cp_atypical', 'cp_non_anginal']\n\nrest_ecg_dummy = pd.get_dummies(df['rest_ecg'], drop_first = True)\nrest_ecg_dummy.columns = ['rest_ecg_st_abnormal', 'rest_ecg_left_vent_hyper']\n\necg_st_slope_dummy = pd.get_dummies(df['ecg_st_slope'], drop_first = True)\necg_st_slope_dummy.columns = ['ecg_st_slope_1', 'ecg_st_slope_2']\n\nthallium_dummy = pd.get_dummies(df['thallium'], drop_first = True)\nthallium_dummy.columns = ['thallium_normal', 'thallium_abnormal', 'thallium_rev_defect']\n","c499e7f8":"#add the dummy variables to our df\ndf = pd.concat([df, cp_dummy, rest_ecg_dummy, ecg_st_slope_dummy, thallium_dummy], axis =1)","74019bec":"#remove the original attributes of the dummy variables\ndf = df.drop(columns = ['cp', 'rest_ecg', 'ecg_st_slope', 'thallium'])","edcaa3a1":"df.head(10)","32539551":"#randomize the sample set\ndf = df.sample(frac=1).reset_index(drop=True)","8cc72420":"df1 = df.copy()","5f12881d":"#specify independent and dependent variables\ny = df1['target']\nX = df1[['age', 'sex_male', 'rest_bp', 'chol', 'fbs_above_120', 'max_bps', 'ex_angina', 'st_depression',\n         'cardiac_fluoro', 'cp_typical', 'cp_atypical', 'cp_non_anginal', 'rest_ecg_st_abnormal',\n         'rest_ecg_left_vent_hyper', 'ecg_st_slope_1', 'ecg_st_slope_2', 'thallium_abnormal', \n         'thallium_rev_defect']]\n","3634c997":"#from imblearn.over_sampling import SMOTE\n\n#smt = SMOTE()\n#X, y = smt.fit_sample(X, y)","0f8b7bb0":"from sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n\n#model\nmodel = LogisticRegression(max_iter=3000)\n\n#performance metrics\naccuracy = cross_val_score(model, X, y, cv=4)\navg_accuracy = np.mean(accuracy)\nprint('Accuracy:', round((avg_accuracy)*100,2), '%', '+\/-', round((max(accuracy)-avg_accuracy)*100,2), '%')\n\nrecall = cross_val_score(model, X, y, scoring = 'recall', cv= 4)\navg_recall = np.mean(recall)\nprint('Recall:', round((avg_recall)*100,2), '%', '+\/-', round((max(recall)-avg_recall)*100,2), '%')\n\nprecision = cross_val_score(model, X, y, scoring = 'precision', cv= 4)\navg_precsion = np.mean(precision)\nprint('Precision:', round((avg_precsion)*100,2), '%', '+\/-', round((max(precision)-avg_precsion)*100,2), '%')\n","ceaec62c":"import statsmodels.api as sm\n\n#run logistic regression\nlogit_model = sm.Logit(y,X)\nresult=logit_model.fit()\nresult.summary()","0e02cfa6":"from sklearn.feature_selection import RFE\n\n# the model\nmodel = LogisticRegression(max_iter=3000)\n\n#run RFE\nrfe = RFE(model, 10)\nrfe = rfe.fit(X, y)\n\n#display the ranking of each variable\nseries1 = pd.Series(X.columns.values)\nseries2 = pd.Series(rfe.ranking_)\n\nrank = pd.DataFrame(data={'Variables': series1, 'Ranking' : series2})\nrank.sort_values(by='Ranking')\n","d6a21762":"X_adj = X[['sex_male', 'max_bps', 'ex_angina', 'st_depression',\n         'cardiac_fluoro', 'cp_typical', 'cp_atypical', 'cp_non_anginal', 'rest_ecg_st_abnormal', \n         'thallium_rev_defect']]","91b5944e":"#model\nmodel = LogisticRegression(max_iter=3000)\n\n#performance metrics\naccuracy = cross_val_score(model, X_adj, y, cv=4)\navg_accuracy = np.mean(accuracy)\nprint('Accuracy:', round((avg_accuracy)*100,2), '%', '+\/-', round((max(accuracy)-avg_accuracy)*100,2), '%')\n\nrecall = cross_val_score(model, X_adj, y, scoring = 'recall', cv= 4)\navg_recall = np.mean(recall)\nprint('Recall:', round((avg_recall)*100,2), '%', '+\/-', round((max(recall)-avg_recall)*100,2), '%')\n\nprecision = cross_val_score(model, X_adj, y, scoring = 'precision', cv= 4)\navg_precsion = np.mean(precision)\nprint('Precision:', round((avg_precsion)*100,2), '%', '+\/-', round((max(precision)-avg_precsion)*100,2), '%')","ed18da8a":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\n\n#split the data\nX_train, X_test, y_train, y_test = train_test_split(X_adj, y, test_size=0.3, random_state=0)\n\n#the model\nmodel = LogisticRegression(max_iter=3000)\n      \n#fit and predict\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\ny_pred_quant = model.predict_proba(X_test)[:, 1]\n\n#ROC graph x and y axis\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_quant)\nprint('AUC:', round((auc(fpr, tpr))*100,2), '%')\n\n#plot the ROC graph\nfig, ax = plt.subplots(figsize = (8,8))\nax.plot(fpr, tpr)\nax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\".3\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.rcParams['font.size'] = 12\nplt.title('ROC curve')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.grid(True)","8163559b":"from sklearn.metrics import roc_curve, auc\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom statistics import mean \nfrom scipy import stats as ss\n\n#store the performance values for aggregation of our loop\nspecificity_store = []\nrecall_store = []\nprecision_store = []\nf_measure_store = []\naccuracy_store = []\nauc_store = []\n\n#ROC plot settings\nfig, ax = plt.subplots(figsize = (8,8))\nax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\".3\")\nplt.xlim([0.0, 1.0])    \nplt.ylim([0.0, 1.0])\nplt.rcParams['font.size'] = 12\nplt.title('ROC curve')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.grid(True)\n    \n\n#specificy how many times to run the model\nx = 10\n\n#run the model x times\nfor i in range(x):\n    #randomize the dataset and specify X and y\n    df1_rand = df1.sample(frac=1).reset_index(drop=True)\n    y = df1_rand['target']\n    X_adj = df1_rand[['sex_male', 'max_bps', 'ex_angina', 'st_depression',\n                      'cardiac_fluoro', 'cp_typical', 'cp_atypical', 'cp_non_anginal', 'rest_ecg_st_abnormal',\n                      'thallium_rev_defect']]\n    #split the data\n    X_train, X_test, y_train, y_test = train_test_split(X_adj, y, test_size=0.3, random_state=0)\n    \n    #run the model\n    model = LogisticRegression(max_iter=3000) \n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    \n    #ROC plot\n    y_pred_quant = model.predict_proba(X_test)[:, 1]\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred_quant)\n    ax.plot(fpr, tpr)\n    \n    #calculate and store performance metrics\n    TN, FP, FN, TP = confusion_matrix(y_test, y_pred).ravel()\n    \n    specificity = TN \/ (TN + FP)\n    specificity_store.append(specificity)\n\n    recall = TP\/ (TP + FN)\n    recall_store.append(recall)\n    \n    precision = TP \/ (TP + FP)\n    precision_store.append(precision)\n    \n    f_measure = (2 * TP) \/ ((2 * TP) + FP + FN)\n    f_measure_store.append(f_measure)\n\n    accuracy = (TP + TN)\/ (TN + FP + FN + TP)\n    accuracy_store.append(accuracy)\n\n    auc_store.append(auc(fpr, tpr))\n\n#performance metrics\navg_accuracy = np.mean(accuracy_store)\nprint('Accuracy:', round((avg_accuracy)*100,2), '%', '+\/-', round(np.std(accuracy_store) * 1.96 * 100,2), '%')\n\navg_specificity = np.mean(specificity_store)\nprint('Specificity:', round((avg_specificity)*100,2), '%', '+\/-', round(np.std(specificity_store) * 1.96 * 100,2), '%')\n\navg_recall = np.mean(recall_store)\nprint('Recall:', round((avg_recall)*100,2), '%', '+\/-', round(np.std(recall_store) * 1.96 * 100,2), '%')\n\navg_precision = np.mean(precision_store)\nprint('Precision:', round((avg_precision)*100,2), '%', '+\/-', round(np.std(precision_store) * 1.96 * 100,2), '%')\n\navg_f_measure = np.mean(f_measure_store)\nprint('F Measure:', round((avg_f_measure)*100,2), '%', '+\/-', round(np.std(f_measure_store) * 1.96 * 100,2), '%')\n\navg_auc = np.mean(auc_store)\nprint('AUC:', round((avg_auc)*100,2), '%', '+\/-', round(np.std(auc_store) * 1.96 * 100,2), '%')\n\n","f4c15df1":"from pdpbox import pdp, get_dataset, info_plots\n\n#specify the variables to grab from\nvar_names = pd.Series(X_adj.columns.values)\n\n# Create the data that we will plot\npdp_sex_male = pdp.pdp_isolate(model=model, dataset = df1, model_features=var_names, feature = 'sex_male')\n\n# plot it\npdp.pdp_plot(pdp_sex_male, 'Sex Male')\nplt.show()","dced14bf":"# Create the data that we will plot\npdp_max_bps = pdp.pdp_isolate(model=model, dataset = df1, model_features=var_names, feature='max_bps')\n\n# plot it\npdp.pdp_plot(pdp_max_bps, 'max_bps (Max Heartrate)')\nplt.show()","47f4d9e8":"# Create the data that we will plot\npdp_cardiac_fluoro = pdp.pdp_isolate(model = model, dataset = df1,\n                                     model_features = var_names, feature = 'cardiac_fluoro')\n\n# plot it\npdp.pdp_plot(pdp_cardiac_fluoro, 'cardiac_fluoro')\nplt.show()","1f3a7777":"# Create the data that we will plot\npdp_st_depression = pdp.pdp_isolate(model = model, dataset = df1,\n                                     model_features = var_names, feature = 'st_depression')\n\n# plot it\npdp.pdp_plot(pdp_st_depression, 'st_depression')\nplt.show()","18a423fb":"\nfeatures_to_plot = ['max_bps', 'sex_male']\ninter1  =  pdp.pdp_interact(model=model, dataset=df1, model_features=var_names,\n                            features=features_to_plot,num_grid_points=[10, 10])\n\nfig, axes = pdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot,\n                                  plot_type='grid')","0bdf51d7":"\nfeatures_to_plot = ['cardiac_fluoro', 'st_depression']\ninter1  =  pdp.pdp_interact(model=model, dataset=df1, model_features=var_names,\n                            features=features_to_plot,num_grid_points=[10, 10])\n\nfig, axes = pdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot,\n                                  plot_type='grid')","8e3b911d":"\nfeatures_to_plot = ['max_bps', 'st_depression']\ninter1  =  pdp.pdp_interact(model=model, dataset=df1, model_features=var_names,\n                            features=features_to_plot,num_grid_points=[10, 10])\n\nfig, axes = pdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot,\n                                  plot_type='grid')","2b6b460c":"#create a contingency table for sex_male and target\ndata_crosstab = pd.crosstab(df1['sex_male'], df1['target'], margins = True)\ndata_crosstab","a21523c6":"#calculate odds ratio\nodds_ratio = (93\/114)\/(72\/24)\nodds_ratio","5acd33cb":"#step 1 calculate natural log of odds ratio\nln_odds_ratio = math.log(odds_ratio)\n\n#step 2 calculate standard error of log \na, b, c, d = 93, 114, 72, 24\nse_log = math.sqrt((1\/a)+(1\/b)+(1\/c)+(1\/d))\n\n#step 3 calculate the 95% CI on the natual log scale\nlower_CI_log = ln_odds_ratio - (1.96*se_log)\nupper_CI_log = ln_odds_ratio + (1.96*se_log)\n\n#step 4 convert back to normal scale\nmath.exp(lower_CI_log), math.exp(upper_CI_log)","246d5016":"#columns \nodds_ratio_storage = []\nci_lower_storage = []\nci_upper_storage = []\n\n#set a copy of the dataframe\nX2_adj = X_adj.copy()\n\n#specify cut off points\nX2_adj.loc[X2_adj['max_bps'] >= 150, 'max_bps_greater_150'] = 1\nX2_adj.max_bps_greater_150 = X2_adj.max_bps_greater_150.fillna(0)\n\nX2_adj.loc[X2_adj['cardiac_fluoro'] >= 1, 'cardiac_fluoro_greater_0'] = 1\nX2_adj.cardiac_fluoro_greater_0 = X2_adj.cardiac_fluoro_greater_0.fillna(0)\n\nX2_adj.loc[X2_adj['st_depression'] >= 1, 'st_depression_greater_1'] = 1\nX2_adj.st_depression_greater_1 = X2_adj.st_depression_greater_1.fillna(0)\n\nX2_adj = X2_adj.drop(columns = ['max_bps' ,'cardiac_fluoro', 'st_depression'])\nX2_adj = X2_adj.astype(int)\n\n#variable names\nvar_names_X2_adj = pd.Series(X2_adj.columns.values)\n\n#put the dataframe back together\ndf1_adj = pd.concat([y, X2_adj],axis=1)\n\n#loop through each variable\nfor i in var_names_X2_adj:\n    data_crosstab = pd.crosstab(df1_adj[str(i)], df1_adj['target'], margins = True)\n\n    odds_ratio = (data_crosstab.iloc[1 , 1]\/data_crosstab.iloc[1 , 0])\/(data_crosstab.iloc[0 , 1]\/data_crosstab.iloc[0 , 0])\n\n    #step 1 calculate natural log of odds ratio\n    ln_odds_ratio = math.log(odds_ratio)\n\n    #step 2 calculate standard error of log \n    a = data_crosstab.iloc[1 , 1]\n    b = data_crosstab.iloc[1 , 0]\n    c = data_crosstab.iloc[0 , 1]\n    d = data_crosstab.iloc[0 , 0]\n    se_log = math.sqrt((1\/a)+(1\/b)+(1\/c)+(1\/d))\n\n    #step 3 calculate the 95% CI on the natual log scale\n    lower_CI_log = ln_odds_ratio - (1.96*se_log)\n    upper_CI_log = ln_odds_ratio + (1.96*se_log)\n\n    #step 4 convert back to normal scale\n    lower_ci = math.exp(lower_CI_log) \n    upper_ci = math.exp(upper_CI_log)\n    \n    #append the odds ratios and ci \n    odds_ratio_storage.append(odds_ratio)\n    ci_lower_storage.append(lower_ci)\n    ci_upper_storage.append(upper_ci)\n    \n#odds ratio table\nodds_table = pd.DataFrame(data={'Variables': var_names_X2_adj, 'Odds Ratios' : odds_ratio_storage,\n                                '95% CI Lower' : ci_lower_storage, '95% CI Upper' : ci_upper_storage})\nodds_table.sort_values(by='Odds Ratios')","e15ddbda":"## 10. Conclusion<a id = 'conclusion'><\/a>","1cf94ef0":"Not too much improvement in the average values but the variation is a lot tighter. I'm happy with that.\n\nLet\u2019s look at the ROC curve to see the prediction power of our model. The more the curve is skewed towards the upper left hand corner, the better. The closer it is to the diagonal, the more random the model is.\n","07badd58":"Let\u2019s do an exploratory analysis of the dataset to get a general sense of what we are working with and how each attribute relates to the presence of heart disease. First let\u2019s check if our target class is skewed or not. We don't want our model accuracy being 95% and to find out that our target variable was 95% true for heart disease!","bde49a51":"The variables kept for the final model is shown below. I used a combination of both methods.","0bae4c7b":"## 11. References<a id = 'ref'><\/a>","ecba65ff":"Lets check for missing values in the dataset, all the value counts for each column should be 303.","37f59adb":"It is interesting to see how the performance metrics are different. The variation is higher than in the cross validate method but specificity should be around 79%. So our model is better at tell us who has heart disease (recall) than as oppose to not having heart disease (specificity).","0b008516":"Run the initial model with all the variables. We'll be using cross validation to measure our performance with 95% confidence intervals. I prefer this method of validation because the whole data set will have a chance to be used as either the training and testing set.","02f8eed4":"Congrats for making it this far! We covered a fair amount and learned a lot along the way. Several of the most important factors were sex, max_bps, cardiac_fluoro, and the ECG related variables. For cardiac_fluoro and the ECG tests, we should expect these since they are tests for abnormalities of the heart. These variables were also noticeable in the EDA. From the logistic regression, since the performance was relatively good, we can say that there is a linear relationship between the variables to the presence of heart disease. However, we did not see cholesterol as an important factor in the presence of heart disease which is contradicting to what we typically hear from our doctors, at least for this dataset anyways. In short, I'm generally content with the analysis we did but there are more rabbit holes we can delve into such as black box models. I hope you found this insightful, thanks!","29c26767":"Now, we can start removing variables that do not provide much information to our model or are not statistically significant. We'll be going over two methods, the first method is to look at the p-values. The logistic regression provides p-values, from that we can determine the significance of each variable. Less than .05 is statistically significant.","b60f50b8":"Many medical publications are descriptive as opposed to predictive and often display variables in terms of odds ratios. Odds ratios tells us the likelihood of having heart disease for a given value in a variable as oppose to those that are not of the given value of that variable. Logistic regression is also based on the odds ratio as well. We'll look at one odds ratio below, then create some code to loop it through all the variables. \n","3cf8f862":"Heart Disease Image\n\nhttps:\/\/www.webmd.com\/heart-disease\/ss\/slideshow-heart-disease-affects-body\n\nCorrelation \n\nhttps:\/\/medium.com\/@outside2SDs\/an-overview-of-correlation-measures-between-categorical-and-continuous-variables-4c7f85610365\n    \nhttps:\/\/towardsdatascience.com\/the-search-for-categorical-correlation-a1cf7f1888c9\n\nLogistic Regression\n\nhttps:\/\/en.wikipedia.org\/wiki\/Logistic_regression\n\nPerformance\/ROC\/etc.\n\nhttps:\/\/en.wikipedia.org\/wiki\/Receiver_operating_characteristic\n\nOdds Ratio\n\nhttps:\/\/en.wikipedia.org\/wiki\/Odds_ratio\n\nKaggle Notebook by Rob Harrand\n\nhttps:\/\/www.kaggle.com\/tentotheminus9\/what-causes-heart-disease-explaining-the-model","962868e2":"There seems to be the most distinction of those that do and do not have heart disease when looking at thalach and oldpeak.\n\nLet\u2019s visualize all our numerical data in pair-wise grid plot to get a different view. Hopefully we can see some clustering.\n","8fc04b49":"## Heart Disease Analysis","39592b4d":"We'll concentrate on logistic regression for this process. Logistic regression is a traditional well-known supervised machine learning model that provides an extremely high level of interpretability, as opposed to black box models. Also, odds ratios are widely used in medical research and logistic regression is based on it.\n\nLogistic regression is a linear optimization problem, when solved produces a sample equation below. The betas are the slopes, the x's are the unit values of the variables, and p is the probability. A sample univariate graph looking at the probability of passing an exam vs hours of studying from Wiki is displayed below. \n\n\nSee the Wiki page for more information.\nhttps:\/\/en.wikipedia.org\/wiki\/Logistic_regression","293df826":"1. [Introduction](#intro)\n2. [Overview](#overview)\n3. [Import Data](#import)\n4. [Exploratory Data Analysis](#eda)\n5. [Correlation Matrix](#corr)\n6. [Preprocessing](#preprocessing)\n7. [Model Building](#model)\n8. [Visualizations](#vis)\n9. [Odds Ratios](#odds)\n10. [Conclusion](#conclusion)\n11. [References](#ref)","06a95e8b":"## 9. Odds Ratios<a id = 'odds'><\/a>","402a0353":"## 7. Model Building <a id = 'model'><\/a>","6d8efad4":"Lets go over the variables in the data set.\n\nage: how old a person is in years\n\nsex: gender (1 = male, 0 = female)\n\ncp: chest pain type. This category is vague, there's no description of each type. Lets assume the following:  \n    (0 = no chest pain, 1 = typical angina, 2 = atypical angina, 3 = non-anginal pain)\n    Note, angina is chest pain when there is reduced blood flow to the heart.\n\ntrestbps: resting blood pressure on admission to hospital (measured in mm Hg)\n\nchol: total serum cholesterol calculated by adding the HDL, LDL, and 20 percent of the triglyceride \n    levels. (measured in mg\/dl)\n\nfbs: fasting blood sugar > 120 mg\/dl (1 = true, 0 = false)\n\nrestecg: resting electrocardiographic results \n    (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular \n    hypertrophy by Estes' criteria)\n\nthalach: max heart rate achieved (bps)\n\nexang: exercise induced angina (0 = no, 1 = yes)\n\noldpeak: ST depression induced by exercise relative to rest ('ST' relates to positions on the ECG plot. See more here)\n\nslope: the slope of the ST segment in an ecg during exercise. This attribute is vague, there's no \n    description of each value, Lets assume the following:\n    (0 = positive slope, 1 = flat, 2 = negative slope)\n\nca: number of major vessels colored during a fluoroscopy (0-3)\n\nthal: thallium scintigraphy is an alternate non-invasive method to test for heart disease\n    (1 = normal; 2 = fixed defect; 3 = reversable defect)\n\ntarget: Is heart disease present? (0 = no, 1 = yes)","e1be2f84":"So, the columns represent the presence of heart disease and the rows represent if the patient is male or female. For example, we have 93 patients that are male and have heart disease. The ALL rows and columns are summations. We have 303 in the bottom most right most number, same as the total number of rows we have in the dataset.\n\nLet\u2019s compute the odds ratio:\n\nPatients that are male are x times more likely of having heart disease compared to those that are female.","a43d57ee":"St_depression (oldpeak) provides the most change in probability. We also saw this variable showing the most separation in the pairwise plots in the exploratory data analysis.","6c809bf9":"It looks like the p-values are too high for the following variables: age, fbs_above_120, rect_ecg_left_vent_hyper, ecg_st_slope_1, ecg_st_slope_2, thallium_abnormal.\n\nWe can remove them from use as they do not provide much useful information for our model. \n\nAnother method is Recursive Feature Elimination where the model is initially run with all the variables. Then an importance coefficient is obtained for each variable. Then the least important features are removed from the model. We can specify how many features we want to keep.\n","d46518e5":"From the simple bar charts, we can tell there is a clear distinction between those that do and do not have heart disease when looking at the variables sex, cp and ca. \n\nOkay, lets take a look at our numerical variables using boxplots.","6cc8f22e":"![logistic_curve.jpeg](attachment:logistic_curve.jpeg)","75d764e4":"The logistic model seems to have a high prediction power and can be deemed an appropriate fit.\n\nOkay for healthcare data, an extremely important performance metric is specificity. That is the true negative rate. The higher it is, the less false negatives we will have. A false negative is when a patient has a condition, but the prediction was false, leaving the patient going untreated, a Type II error. \n\nCross validate from the sklearn library does not provide a specificity scoring method. We'll create it ourselves and also try a different custom performance method for this one. The data will be randomized, split into a 70\/30 and the performance values will be calculated. We'll repeat this for x amount of times. Then we can get an average value and compute a 95% confident interval. I expect the variation to be higher since we are running it a large amount of times and the model to follow normality (bell curve). It's interesting to play around with how many times we run the model and see the variation in performance values and the ROC curves.\n","1b164cee":"Dummy variables for categorical variables work better with machine learning models. Lets transform the categorical variables to dummy variables.","2fcce358":"Let's also change the order to the following:\n\ndemographics - age, sex\n\nbasic info - rest_bp, chol, fbs, max_bps, cp, ex_angina\n\necg related - restecg, oldpeak, ecg_st_slope\n\nalt ecg method - thallium, cardiac_fluoro\n\ntarget","cb296118":"Lets import the dataset, make some adjustments interpretation and take a look at what we have.","f0dafdfe":"For the odds ratios less than 1, if the variable is true than it would point towards less chance of heart disease. For the odds ratios greater than 1, if the variable is true that it would point towards the presence of heart disease.","f27c032a":"## 5. Correlation Matrix<a id = 'corr'><\/a>","82c5fdd5":"We can also see how the change of two variables affect the probability. Yellow is the highest probability of heart disease. The darker, it is the less chance of heart disease.","e5268fd8":"Looking at the target column, all the values are '1' are on the top of the dataset, we'll need to randomize it the dataset. ","ca1c7523":"Now we'll rerun the model and see what the difference is.","746add6e":"![heart.jpg](attachment:heart.jpg)","ad16280f":"## 1. Introduction <a id = 'intro'><\/a>","7225cf65":"## 4. Exploratory Data Analysis<a id = 'eda'><\/a>","a60d1698":"On to the more aesthetically pleasing part of the analysis. Visualizations provide us a way of conveying the model to the audience. We'll select some top variables and see the probability of heart disease changes as the variable increases or decreases. We'll be using the pdpbox library, I don't think it is a standard library so you may need to install it.","ac7eb708":"Interesting... there is a notice difference between people with and without heart disease across the numerical variable pair plots. We can see that the bivariate concentrations clearly in the density plots, with the most distinction in st_depression (oldpeak).","e379733f":"Heart disease is consistently the leading cause of death in the United States year after year. According to Centers for Diseases Control and Prevention (CDC), about 647,000 Americans die from heart disease each year \u2014 that\u2019s 1 in every 4 deaths. In this notebook, we'll model the dataset using logistic regression and visualize the results.","013be823":"## Table of Contents","069de9b9":"## 8. Visualizations<a id = 'vis'><\/a>","73a5c910":"We'll be setting a copy of our preprocessed dataset. It is always good to have a checkpoint just in case you inadvertently changed it during the model building process. All you need to do is to come back here and reset it.","6c97b0b5":"Now we can compute the 95% Confidence Interval for the odds ratio.","463bf2f6":"Lets change the variable names to be a bit clearer and easier to interpret later on.\n\ntrestbps to rest_bp, thalach to max_bps, exang to ex_angina, oldpeak to st_depression, slope to ecg_st_slope, ca to cardiac_fluoro, thal to thallium","eefeac2d":"## 6. Preprocessing<a id = 'preprocessing'><\/a>","c821f80c":"![log_equation.PNG](attachment:log_equation.PNG)","c23e3ef2":"Variables that are highly correlated with each other, that is if one increases the other increases at the same rate, can produce noise and redundancy in our model. Let\u2019s look to see if there are any and removed them if present. We can also see what factors correlates to having heart disease.\n\nCorrelation between numerical and categorical variables can be tricky and there isn't as much information on it as numerical to numerical and categorical to categorical correlation. We'll use Pearson's for the numerical variables and Cramer's V for the categorical variables. Cramer's can handle ranked and non-ranked categorical variables.\n","a0231952":"Nothing alarming, it looks like there are no highly correlated variables (>.8). ","6d7e2ea4":"## Checkpoint","dc51b66e":"Okay. Looks okay, not too skewed. If a dataset is highly imbalanced, we might not have enough information for the machine learning model. There is an interesting method called upscaling used for imbalanced datasets. One such method is SMOTE, take a look at the link if you want to know more.\n\nhttps:\/\/medium.com\/@saeedAR\/smote-and-near-miss-in-python-machine-learning-in-imbalanced-datasets-b7976d9a7a79\n\nLets move on to our categorical variables: sex, cp, fbs,  restecg, exang, ca, thal.","99ac6fdd":"## 2. Overview <a id = 'overview'><\/a>\n","f0edc321":"First, we'll have to assess which variables impact the presence of the heart disease the most. We don't want insignificant variables introducing noise into our final model. There are numerous ways to do this; iterations, correlation graphs, and even a brute force approach (try every single combination of variables).","d34358ad":"## 3. Import Data<a id = 'import'><\/a>","c5e4ed11":"Patients that are male are 0.27 times more likely of having heart disease compared to those that are female (about 1 to 4, male being less likely). We are 95% confident that the odds ratio lies between 0.16 to 0.47. ","7bda6b4b":"Odds ratios only take binomial categorical variables. For the numerical and multi categorical values, we have to specify cut off point that separates the data. I generally chose it where there was the most distinction. For example, in a numerical variable, I chose it at the median.","d91f2abe":"Remember in the beginning when we looked at our dataset and saw that it was slightly imbalanced? We can fix that using SMOTE, a machine learning algorithm that generates extra synthetic rows of the balanced target variable. In this case, it was a coin toss on the level of performance improvement but I\u2019ll leave the code here."}}