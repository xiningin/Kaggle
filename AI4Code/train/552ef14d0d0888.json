{"cell_type":{"5b7e611d":"code","a570c093":"code","eae31a7c":"code","a4119d69":"code","4e679e90":"code","6ca46692":"code","214b28b2":"code","b022c2fe":"code","84d55da9":"code","f694eef4":"code","800a4447":"code","e27c9082":"code","bfc664dc":"code","db750256":"code","c8deccda":"code","58beea64":"code","06f342d4":"code","d46c5bd2":"code","c1f1e71e":"code","8d1dd4ec":"code","327dc777":"code","ea782ce4":"code","4bd26ff7":"code","f4858280":"code","f0d402ef":"code","eea6bf14":"code","76d16d28":"code","6c190b8b":"markdown","4fbdd1dd":"markdown"},"source":{"5b7e611d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport os\nfrom scipy.io import wavfile\nfrom collections import defaultdict, Counter\nfrom scipy import signal\nimport numpy as np\nimport librosa\nimport random as rn\nimport tensorflow as tf\nfrom tensorflow import keras\n\nDATA_DIR = '\/kaggle\/input\/spoken-digit-dataset\/free-spoken-digit-dataset-master\/free-spoken-digit-dataset-master\/recordings\/'\n# random_file = rn.choice(os.listdir(DATA_DIR))\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a570c093":"wav, sr = librosa.load('\/kaggle\/input\/spoken-digit-dataset\/free-spoken-digit-dataset-master\/free-spoken-digit-dataset-master\/recordings\/0_jackson_14.wav')\nprint(sr,wav.shape)\nplt.plot(wav)","eae31a7c":"librosa.get_duration(y=wav,sr=sr)","a4119d69":"11265\/22050","4e679e90":"import librosa.display\nD = librosa.amplitude_to_db(np.abs(librosa.stft(wav)), ref=np.max)\nlibrosa.display.specshow(D, y_axis='linear')","6ca46692":"pad1d = lambda a, i: a[0: i] if a.shape[0] > i else np.hstack((a, np.zeros(i - a.shape[0])))\npad2d = lambda a, i: a[:, 0: i] if a.shape[1] > i else np.hstack((a, np.zeros((a.shape[0],i - a.shape[1]))))\n","214b28b2":"spectrogram = np.abs(librosa.stft(wav))\npadded_spectogram = pad2d(spectrogram,40)\n\nD = librosa.amplitude_to_db(padded_spectogram, ref=np.max)\nlibrosa.display.specshow(D, y_axis='linear')","b022c2fe":"padded_spectogram = pad2d(spectrogram,60)\n\nD = librosa.amplitude_to_db(padded_spectogram, ref=np.max)\nlibrosa.display.specshow(D, y_axis='linear')","84d55da9":"mel_spectrogram = librosa.feature.melspectrogram(wav)\n\n\nD = librosa.amplitude_to_db(mel_spectrogram, ref=np.max)\nlibrosa.display.specshow(D, y_axis='linear')","f694eef4":"padded_mel_spectrogram = pad2d(mel_spectrogram,40)\nD = librosa.amplitude_to_db(padded_mel_spectrogram, ref=np.max)\nlibrosa.display.specshow(D)","800a4447":"mfcc = librosa.feature.mfcc(wav)\npadded_mfcc = pad2d(mfcc,40)\nD = librosa.amplitude_to_db(mfcc, ref=np.max)\nlibrosa.display.specshow(D)","e27c9082":"D = librosa.amplitude_to_db(padded_mfcc, ref=np.max)\nlibrosa.display.specshow(D)","bfc664dc":"from keras.utils import to_categorical\nimport os\n\ntest_speaker = 'theo'\ntrain_X = []\ntrain_spectrograms = []\ntrain_mel_spectrograms = []\ntrain_mfccs = []\ntrain_y = []\n\ntest_X = []\ntest_spectrograms = []\ntest_mel_spectrograms = []\ntest_mfccs = []\ntest_y = []\ntrain_mfccs_no_padding=[]\ntest_mfccs_no_padding=[]\n\n\nfor fname in os.listdir(DATA_DIR):\n    try:\n        if '.wav' not in fname or 'dima' in fname:\n            continue\n        struct = fname.split('_')\n        digit = struct[0]\n        speaker = struct[1]\n        wav, sr = librosa.load(DATA_DIR + fname)\n#         wav, sr = librosa.load(DATA_DIR + fname, mono=True, sr=8000, duration = 1.024)\n#         wav, sr = librosa.load(DATA_DIR + fname, mono=True, sr=8000)\n        wav = librosa.util.normalize(wav) #normalize \n    \n        padded_x = pad1d(wav, 30000)\n        spectrogram = np.abs(librosa.stft(wav))\n        padded_spectogram = pad2d(spectrogram,40)\n\n        mel_spectrogram = librosa.feature.melspectrogram(wav)\n        padded_mel_spectrogram = pad2d(mel_spectrogram,40)\n\n        mfcc = librosa.feature.mfcc(wav)\n#         duration = wav.shape[0]\/sr\n#         speed_change = 2.0* duration\/1.024\n#         print('duration',duration)\n#         print('speed_change',speed_change)\n#         wav = librosa.effects.time_stretch(wav, speed_change)\n#         wav = wav[:4096]\n#         mfcc = librosa.feature.mfcc(wav, sr=sr, n_mfcc=40, hop_length=int(0.048*sr), n_fft=int(0.096*sr))\n        mfcc -= (np.mean(mfcc, axis=0) + 1e-8)\n\n#         print(\"shape=\",mfcc.shape[1], wav.shape[0])\n        max_pad_len=11\n#         pad_width = max_pad_len - mfcc.shape[1]\n        padded_mfcc = pad2d(mfcc,40)\n#         mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n\n        if speaker == test_speaker:\n            test_X.append(padded_x)\n            test_spectrograms.append(padded_spectogram)\n            test_mel_spectrograms.append(padded_mel_spectrogram)\n            test_mfccs.append(padded_mfcc)\n            test_y.append(digit)\n            test_mfccs_no_padding.append(mfcc)\n        else:\n            train_X.append(padded_x)\n            train_spectrograms.append(padded_spectogram)\n            train_mel_spectrograms.append(padded_mel_spectrogram)\n            train_mfccs.append(padded_mfcc)\n            train_mfccs_no_padding.append(mfcc)\n            train_y.append(digit)\n    except Exception as e:\n        print(fname, e)\n        raise\n\ntrain_X = np.vstack(train_X)\ntrain_spectrograms = np.array(train_spectrograms)\ntrain_mel_spectrograms = np.array(train_mel_spectrograms)\ntrain_mfccs = np.array(train_mfccs)\n# train_mfccs_no_padding=np.array(train_mfccs_no_padding)\ntrain_y = to_categorical(np.array(train_y))\n\ntest_X = np.vstack(test_X)\ntest_spectrograms = np.array(test_spectrograms)\ntest_mel_spectrograms = np.array(test_mel_spectrograms)\ntest_mfccs = np.array(test_mfccs)\n# test_mfccs_no_padding=np.array(test_mfccs_no_padding)\ntest_y = to_categorical(np.array(test_y))\n\n","db750256":"print('train_X:', train_X.shape)\nprint('train_y:', train_y.shape)\nprint('test_X:', test_X.shape)\nprint('test_y:', test_y.shape)","c8deccda":"np.shape(train_mfccs)","58beea64":"len(train_mfccs_no_padding)","06f342d4":"train_mfccs.shape","d46c5bd2":"train_X_ex = np.expand_dims(train_mfccs, -1)\ntest_X_ex = np.expand_dims(test_mfccs, -1)\nprint('train X shape:', train_X_ex.shape)\nprint('test X shape:', test_X_ex.shape)","c1f1e71e":"model = tf.keras.models.Sequential([\n  tf.keras.layers.Conv2D(32, (3,3), activation='relu',\n                         kernel_regularizer=tf.keras.regularizers.l2(0.002), \n                         input_shape=(20, 40, 1)),\n  tf.keras.layers.MaxPooling2D(2, 2),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Conv2D(64, (1,1), activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.002)),\n  tf.keras.layers.MaxPooling2D(2,2),\n#   tf.keras.layers.Conv2D(64, (1,1), activation=tf.keras.layers.LeakyReLU()),\n#   tf.keras.layers.MaxPooling2D(2,2),  \n#   tf.keras.layers.Dropout(0.1),  \n    \n  tf.keras.layers.Flatten(),\n  tf.keras.layers.Dense(64, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.002)),\n  tf.keras.layers.Dense(10, activation='softmax')\n])\nmodel.compile(optimizer = 'adam',\n              loss = 'categorical_crossentropy',metrics=['accuracy'])\n\n# model.fit(train_X, train_y, epochs=5,verbose =1)\nhistory = model.fit(train_X_ex,\n          train_y,\n          epochs=20,\n          batch_size=64,\n          validation_data=(test_X_ex, test_y))","8d1dd4ec":"predictions = model.predict(test_X_ex)\nresults=np.argmax(predictions,axis=1)","327dc777":"test_X_ex.shape","ea782ce4":"# results","4bd26ff7":"from sklearn.metrics import classification_report\nprint(classification_report(test_y, to_categorical(results)))","f4858280":"print(train_mfccs.shape)\nprint(test_mfccs.shape)","f0d402ef":"train_mfccs.shape[1],train_mfccs.shape[2]","eea6bf14":"input_shape=(20, 40)   \n\nlstm_model = tf.keras.models.Sequential([\ntf.keras.layers.LSTM(64, input_shape=(20, 40), return_sequences=True, \n                     kernel_initializer=tf.keras.initializers.he_uniform(),\n#                      kernel_regularizer=tf.keras.regularizers.l2(0.002)\n                    ),\n   \ntf.keras.layers.LSTM(32,return_sequences=False, \n                      kernel_initializer=tf.keras.initializers.he_uniform(),\n                      kernel_regularizer=tf.keras.regularizers.l2(0.01)\n                    ),\n# tf.keras.layers.BatchNormalization() ,    \ntf.keras.layers.Dropout(0.5),   \ntf.keras.layers.Dense(32, activation='relu', \n                       kernel_initializer=tf.keras.initializers.he_uniform(),\n                       kernel_regularizer=tf.keras.regularizers.l2(0.001)\n                     ),\ntf.keras.layers.Dropout(0.3),\ntf.keras.layers.Dense(10, activation='softmax')\n])\n\nlstm_model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.008),\n          loss = 'categorical_crossentropy',metrics=['accuracy'])\n\n# model.fit(train_X, train_y, epochs=5,verbose =1)\nhistory = lstm_model.fit(train_mfccs,\n      train_y,\n      epochs=30,\n      batch_size=32,\n      validation_data=(test_mfccs, test_y))","76d16d28":"predictions = lstm_model.predict(test_mfccs)\nresults=np.argmax(predictions,axis=1)\nfrom sklearn.metrics import classification_report\nprint(classification_report(test_y, to_categorical(results)))","6c190b8b":"Links for reference :\n    [Speech Processing](https:\/\/haythamfayek.com\/2016\/04\/21\/speech-processing-for-machine-learning.html)\n     [[Audio Classification](https:\/\/github.com\/shudima\/notebooks\/blob\/master\/Speech%20Classification.ipynb)]\n     [Audio Handling](https:\/\/medium.com\/behavioral-signals-ai\/basic-audio-handling-d4cc9c70d64d)","4fbdd1dd":"Using RNN-LSTM Network"}}