{"cell_type":{"affb925f":"code","08ba41e4":"code","ed8a4d62":"code","602b3615":"code","5129e559":"code","9b7e4eaf":"code","186af5ca":"code","947b87ef":"code","1ebfa575":"code","4ea237c7":"code","a4e1fed6":"code","3555b06f":"code","22c642dc":"code","d510632c":"code","10c61afb":"code","d84070c4":"code","c5b1442a":"code","2aa48556":"code","f7441583":"code","47785b25":"code","af7bd173":"code","7d92bf39":"code","3b7cc328":"code","cfc67448":"code","4d433bef":"code","b5519401":"code","b860bd2b":"code","3cd5e872":"code","961a481b":"code","94e19e21":"code","52bfb04e":"code","bb35595d":"code","f9cad5f5":"code","d862ae71":"code","9fa2f79a":"code","ea66cc41":"code","1a38539d":"code","a405e251":"code","5bb61755":"code","24a058b0":"code","5a8d891e":"code","f1f3c05a":"code","42b1c653":"code","6fa6ae18":"code","0cb65677":"code","cbfa6b57":"markdown","531a3b48":"markdown","3ea77bb2":"markdown","8f1bb8bc":"markdown","5e222c96":"markdown","4ddf9499":"markdown","feb40bda":"markdown","079a39ba":"markdown","1b6fca72":"markdown","af77ab74":"markdown","a9b017dd":"markdown","11ca09fb":"markdown","5ec07291":"markdown","f2aaac9a":"markdown","8e36c24a":"markdown","b04bbb04":"markdown","84129fd7":"markdown","83d85fad":"markdown","36870f9b":"markdown","88690de2":"markdown","a5f5d49a":"markdown","ec52927e":"markdown","ebbac0e0":"markdown","e308d024":"markdown","6faea53b":"markdown","df3973e1":"markdown","da99e0d5":"markdown","d910235a":"markdown","917fcc1d":"markdown","6a2d3447":"markdown","544443a7":"markdown","006d44ee":"markdown","283ed574":"markdown","4618d5d0":"markdown","5463c3a5":"markdown","17c3f22d":"markdown","7c5d0687":"markdown","dd7079e0":"markdown","0e4d85c0":"markdown","4e5cab83":"markdown","6327c936":"markdown"},"source":{"affb925f":"import warnings\nimport random\nimport os\nimport gc\nimport torch","08ba41e4":"import pandas            as pd\nimport numpy             as np\nimport matplotlib.pyplot as plt \nimport seaborn           as sns\nimport joblib            as jb\nimport xgboost           as xgb","ed8a4d62":"from sklearn.model_selection import train_test_split,  KFold, StratifiedKFold\nfrom sklearn.preprocessing   import StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler\nfrom sklearn.preprocessing   import QuantileTransformer, LabelEncoder\nfrom sklearn.impute          import SimpleImputer\nfrom sklearn                 import metrics\nfrom datetime                import datetime","602b3615":"def jupyter_setting():\n    \n    %matplotlib inline\n      \n    #os.environ[\"WANDB_SILENT\"] = \"true\" \n    #plt.style.use('bmh') \n    #plt.rcParams['figure.figsize'] = [20,15]\n    #plt.rcParams['font.size']      = 13\n     \n    pd.options.display.max_columns = None\n    #pd.set_option('display.expand_frame_repr', False)\n\n    warnings.filterwarnings(action='ignore')\n    warnings.simplefilter('ignore')\n    warnings.filterwarnings('ignore')\n    #warnings.filterwarnings(category=UserWarning)\n\n    warnings.filterwarnings('ignore', category=DeprecationWarning)\n    warnings.filterwarnings('ignore', category=FutureWarning)\n    warnings.filterwarnings('ignore', category=RuntimeWarning)\n    warnings.filterwarnings('ignore', category=UserWarning)\n    #warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n\n    pd.set_option('display.max_rows', 150)\n    pd.set_option('display.max_columns', 500)\n    pd.set_option('display.max_colwidth', None)\n\n    icecream = [\"#00008b\", \"#960018\",\"#008b00\", \"#00468b\", \"#8b4500\", \"#582c00\"]\n    #sns.palplot(sns.color_palette(icecream))\n    \n    return icecream\n\nicecream = jupyter_setting()\n\n# Colors\ndark_red = \"#b20710\"\nblack    = \"#221f1f\"\ngreen    = \"#009473\"\nmyred    = '#CD5C5C'\nmyblue   = '#6495ED'\nmygreen  = '#90EE90'\n\ncols= [myred, myblue,mygreen]","5129e559":"colors = [\"lightcoral\", \"sandybrown\", \"darkorange\", \"mediumseagreen\",\n          \"lightseagreen\", \"cornflowerblue\", \"mediumpurple\", \"palevioletred\",\n          \"lightskyblue\", \"sandybrown\", \"yellowgreen\", \"indianred\",\n          \"lightsteelblue\", \"mediumorchid\", \"deepskyblue\"]","9b7e4eaf":"def missing_zero_values_table(df):\n        mis_val         = df.isnull().sum()\n        mis_val_percent = round(df.isnull().mean().mul(100), 2)\n        mz_table        = pd.concat([mis_val, mis_val_percent], axis=1)\n        mz_table        = mz_table.rename(columns = {df.index.name:'col_name', \n                                                     0 : 'Valores ausentes', \n                                                     1 : '% de valores totais'})\n        \n        mz_table['Tipo de dados'] = df.dtypes\n        mz_table                  = mz_table[mz_table.iloc[:,1] != 0 ]. \\\n                                     sort_values('% de valores totais', ascending=False)\n        \n        msg = \"Seu dataframe selecionado tem {} colunas e {} \" + \\\n              \"linhas. \\nExistem {} colunas com valores ausentes.\"\n            \n        print (msg.format(df.shape[1], df.shape[0], mz_table.shape[0]))\n        \n        return mz_table.reset_index()","186af5ca":"def describe(df):\n    var = df.columns\n\n    # Medidas de tend\u00eancia central, m\u00e9dia e mediana \n    ct1 = pd.DataFrame(df[var].apply(np.mean)).T\n    ct2 = pd.DataFrame(df[var].apply(np.median)).T\n\n    # Dispens\u00e3o - str, min , max range skew, kurtosis\n    d1 = pd.DataFrame(df[var].apply(np.std)).T\n    d2 = pd.DataFrame(df[var].apply(min)).T\n    d3 = pd.DataFrame(df[var].apply(max)).T\n    d4 = pd.DataFrame(df[var].apply(lambda x: x.max() - x.min())).T\n    d5 = pd.DataFrame(df[var].apply(lambda x: x.skew())).T\n    d6 = pd.DataFrame(df[var].apply(lambda x: x.kurtosis())).T\n    d7 = pd.DataFrame(df[var].apply(lambda x: (3 *( np.mean(x) - np.median(x)) \/ np.std(x) ))).T\n\n    # concatenete \n    m = pd.concat([d2, d3, d4, ct1, ct2, d1, d5, d6, d7]).T.reset_index()\n    m.columns = ['attrobutes', 'min', 'max', 'range', 'mean', 'median', 'std','skew', 'kurtosis','coef_as']\n    \n    return m","947b87ef":"def reduce_memory_usage(df, verbose=True):\n    \n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    \n    for col in df.columns:\n        \n        col_type = df[col].dtypes\n        \n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            \n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) \/ start_mem\n            )\n        )\n        \n    return df","1ebfa575":"def plot_roc_curve(fpr, tpr, label=None):\n    fig, ax = plt.subplots()\n    ax.plot(fpr, tpr, \"r-\", label=label)\n    ax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\".3\")\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.0])\n    plt.rcParams['font.size'] = 12\n    plt.title('ROC curve for TPS 09')\n    plt.xlabel('False Positive Rate (1 - Specificity)')\n    plt.ylabel('True Positive Rate (Sensitivity)')\n    plt.legend(loc=\"lower right\")\n    plt.grid(True)","4ea237c7":"def confusion_plot(matrix, labels = None, title = None):\n        \n    labels = labels if labels else ['Negative (0)', 'Positive (1)']    \n    \n    fig, ax = plt.subplots(nrows=1, ncols=1)\n    \n    sns.heatmap(data        = matrix, \n                cmap        = 'Blues', \n                annot       = True, \n                fmt         = 'd',\n                xticklabels = labels, \n                yticklabels = labels, \n                ax          = ax);\n    \n    ax.set_xlabel('\\n PREVISTO', fontsize=15)\n    ax.set_ylabel('REAL \\n', fontsize=15)\n    ax.set_title(title)\n    \n    plt.close();\n    \n    return fig;","a4e1fed6":"def df_corr(df, annot_=False):\n    \n    df = df.corr(method ='pearson').round(5)\n\n    # M\u00e1scara para ocultar a parte superior direita do gr\u00e1fico, pois \u00e9 uma duplicata\n    mask = np.zeros_like(df)\n    mask[np.triu_indices_from(mask)] = True\n\n    # Making a plot\n    plt.figure(figsize=(15,12))\n    ax = sns.heatmap(df, annot=annot_, mask=mask, cmap=\"RdBu\", annot_kws={\"weight\": \"bold\", \"fontsize\":13})\n\n    ax.set_title(\"Mapa de calor de correla\u00e7\u00e3o das vari\u00e1vel\", fontsize=17)\n\n    plt.setp(ax.get_xticklabels(), \n             rotation      = 90, \n             ha            = \"right\",\n             rotation_mode = \"anchor\", \n             weight        = \"normal\")\n\n    plt.setp(ax.get_yticklabels(), \n             weight        = \"normal\",\n             rotation_mode = \"anchor\", \n             rotation      = 0, \n             ha            = \"right\");","3555b06f":"def graf_outlier(df, feature):\n    col = [(0,4), (5,9)]\n\n    df_plot = ((df[feature] - df[feature].min())\/\n               (df[feature].max() - df[feature].min()))\n\n    fig, ax = plt.subplots(len(col), 1, figsize=(15,7))\n\n    for i, (x) in enumerate(col): \n        sns.boxplot(data = df_plot.iloc[:, x[0]:x[1] ], ax = ax[i]); ","22c642dc":"def diff(t_a, t_b):\n    from dateutil.relativedelta import relativedelta\n    t_diff = relativedelta(t_b, t_a)  # later\/end time comes first!\n    return '{h}h {m}m {s}s'.format(h=t_diff.hours, m=t_diff.minutes, s=t_diff.seconds)","d510632c":"def free_gpu_cache():\n    \n    # https:\/\/www.kaggle.com\/getting-started\/140636\n    #print(\"Initial GPU Usage\")\n    #gpu_usage()                             \n\n    #cuda.select_device(0)\n    #cuda.close()\n    #cuda.select_device(0)   \n    \n    gc.collect()\n    torch.cuda.empty_cache()","10c61afb":"path   = '..\/input\/tabular-playground-series-dec-2021\/'\ntarget = 'Cover_Type'","d84070c4":"df1_train     = pd.read_csv(path + 'train.csv')\ndf1_test      = pd.read_csv(path + 'test.csv')\ndf_submission = pd.read_csv(path + 'sample_submission.csv')\n\ndf1_train.drop('Id', axis=1, inplace=True)\ndf1_test.drop('Id', axis=1, inplace=True)\n\ndf1_train.shape, df1_test.shape, df_submission.shape","c5b1442a":"df1_train.head()","2aa48556":"df1_test.head()","f7441583":"df1_train = reduce_memory_usage(df1_train)\ndf1_test  = reduce_memory_usage(df1_test)","47785b25":"#jb.dump(df1_train, path + 'Data\/df_train_nb_01.pkl.z')\n#jb.dump(df1_test, path + 'Data\/df_test_nb_01.pkl.z')","af7bd173":"print('TREINO')\nprint('Number of Rows: {}'.format(df1_train.shape[0]))\nprint('Number of Columns: {}'.format(df1_train.shape[1]), end='\\n\\n')\n\nprint('TESTE')\nprint('Number of Rows: {}'.format(df1_test.shape[0]))\nprint('Number of Columns: {}'.format(df1_test.shape[1]))","7d92bf39":"df1_train.info()","3b7cc328":"df1_test.info()","cfc67448":"print(f'{3*\"=\"} For Pandas {10*\"=\"}\\n{(df1_train.dtypes).value_counts()}')\nprint(f'\\n{3*\"=\"} For Datatable {7*\"=\"}\\n{(df1_test.dtypes).value_counts()}')","4d433bef":"missing = missing_zero_values_table(df1_train)\nmissing[:].style.background_gradient(cmap='Reds')","b5519401":"missing = missing_zero_values_table(df1_test)\nmissing[:].style.background_gradient(cmap='Reds')","b860bd2b":"feature_float = []\nfeature_cat = df1_test.filter(regex=r'Wilderness_Area').columns.to_list() + \\\n              df1_test.filter(regex=r'Soil_Type').columns.to_list()\n\nfor col in df1_test.columns:\n    if col not in feature_cat: \n        feature_float.append(col)   \n\nfeature_float_test = feature_float.copy()\nfeature_float.append('Cover_Type')\n\nprint('Temos {} vari\u00e1vies num\u00e9ricas e {} categ\u00f3ricas.'.format(len(feature_float), \n                                                              len(feature_cat)))","3cd5e872":"df1_train[feature_float].describe().T.style.background_gradient(cmap='YlOrRd')","961a481b":"df1_test[feature_float_test].describe().T.style.background_gradient(cmap='YlOrRd')","94e19e21":"df1_train[feature_cat].columns","52bfb04e":"df1_train.filter(regex=r'Wil').head()","bb35595d":"df_corr(df1_train[feature_float], annot_=True)","f9cad5f5":"df_corr(df1_train)","d862ae71":"fig, ax = plt.subplots(figsize=(5, 5))\n\npie = ax.pie([len(df1_train), len(df1_test)],\n             labels   = [\"Train dataset\", \"Test dataset\"],\n             colors   = [\"salmon\", \"teal\"],\n             textprops= {\"fontsize\": 15},\n             autopct  = '%1.1f%%')\n\nax.axis(\"equal\")\nax.set_title(\"Compara\u00e7\u00e3o de comprimento do conjunto de dados \\n\", fontsize=18)\nfig.set_facecolor('white')\nplt.show();","9fa2f79a":"fig, ax = plt.subplots(figsize=(5, 5))\n\nplt.pie([ len(feature_cat), len(feature_float)], \n        labels=['Categorical', 'Continuos' ],\n        textprops={'fontsize': 13},\n        autopct='%1.1f%%')\n\n#ax.axis(\"equal\")\nax.set_title(\"Compara\u00e7\u00e3o vari\u00e1veis continuas\/categ\u00f3ricas \\n Dataset Treino\/Teste\", fontsize=18)\nfig.set_facecolor('white')\nplt.show()","ea66cc41":"\nplt.figure(figsize=(8, 4))\nax = sns.countplot(x=df1_train['Cover_Type'], palette='viridis')\nax.set_title('Distribui\u00e7\u00e3o da vari\u00e1vel Cover_Type', fontsize=20, y=1.05)\n\nsns.despine(right=True)\nsns.despine(offset=10, trim=True)","1a38539d":"col = [(0,5), (5,9)]\n       \nfor x in col:\n       \n    L    = len(df1_train[feature_float_test].columns[x[0]:x[1]])\n    nrow = int(np.ceil(L\/6))\n    ncol = 5\n    i    = 1\n\n    remove_last = (nrow * ncol) - L\n    fig, ax     = plt.subplots(nrow, ncol,figsize=(20, 4))\n    \n    fig.subplots_adjust(top=0.8)\n    \n    for feature in df1_train[feature_float_test].columns[x[0]:x[1]]:\n        plt.subplot(nrow, ncol, i)\n        ax = sns.kdeplot(df1_train[feature], shade=True, color='salmon',  alpha=0.5, label='train')\n        ax = sns.kdeplot(df1_test[feature], shade=True, color='teal',  alpha=0.5, label='test')\n        plt.xlabel(feature, fontsize=10)\n        plt.legend()\n\n        i += 1\n        \n        gc.collect()\n    \n    plt.suptitle('DistPlot: train & test data de {} \u00e0 {}'.format(x[0],x[1]), fontsize=15)\n    plt.show();","a405e251":"fig, axes = plt.subplots(10, 4, figsize=(20, 16))\n\ntarget_order = sorted(df1_train['Cover_Type'].unique())\nmean         = df1_train.groupby('Cover_Type').mean().sort_index()\nstd          = df1_train.groupby('Cover_Type').std().sort_index()\n\nfor idx, ax in zip(range(1,40), axes.flatten()):\n    \n    ax.bar(mean[f'Soil_Type{idx}'].index, \n           mean[f'Soil_Type{idx}'],           \n           width=0.6)\n    \n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_xlabel('')\n    ax.set_ylabel('')\n    ax.margins(0.1)\n    \n    ax.spines['left'].set_visible(False)\n    ax.set_title(f'Soil_Type_{idx}', loc='center', weight='bold', fontsize=11)\n\naxes.flatten()[-1].axis('off')    \naxes.flatten()[-2].axis('off')\n\n#fig.supxlabel('M\u00e9dia por classe (por vari\u00e1vel)', ha='center', fontweight='bold')\n\nfig.tight_layout()\nplt.show()","5bb61755":"graf_outlier(df1_train, feature_float)","24a058b0":"graf_outlier(df1_test, feature_float_test)","5a8d891e":"%%time\ndf1_train[target].iloc[df1_train[df1_train[target]==5].index] = 4\ndf1_train[target].iloc[df1_train[df1_train[target]==4].index] = 6\ndf1_train[target].value_counts()\n\nX      = df1_train.drop([target], axis=1)\ny      = df1_train[target].astype(int)\nX_test = df1_test\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, \n                                                      test_size    = 0.3,\n                                                      shuffle      = True,                                                       \n                                                      random_state = 12359)\n\ndel df1_train,df1_test\n\nX_train.shape, y_train.shape, X_valid.shape, y_valid.shape , X_test.shape","f1f3c05a":"seed   = 12359\nparams = {\"objective\"     :\"multi:softmax\",    \n          'eval_metric'   : 'mlogloss',         \n          'random_state'  : seed}\n\nif torch.cuda.is_available():           \n    params.update({'tree_method' : 'gpu_hist',                    \n                   'predictor'   : 'gpu_predictor'\n                   })\nparams","42b1c653":"%%time \n\nmodel_baseline = xgb.XGBClassifier(**params)\n\nscalers = [StandardScaler(), \n           RobustScaler(), \n           MinMaxScaler(), \n           MaxAbsScaler(), \n           QuantileTransformer(output_distribution='normal', random_state=0)]\n\nfor scaler in scalers: \n    \n    if scaler!=None:\n        X_train_s = scaler.fit_transform(X_train)\n        X_valid_s = scaler.fit_transform(X_valid)\n    else:\n        X_train_s = X_train\n        X_valid_s = X_valid\n                \n    model_baseline.fit(X_train_s, y_train, verbose=False)\n\n    y_pred = model_baseline.predict(X_valid_s)  \n    acc    = metrics.accuracy_score(y_valid, y_pred)    \n        \n    print('Valida\u00e7ao ACC: {:2.5f} => {}'.format(acc, scaler))\n\n    free_gpu_cache()\n\nprint()","6fa6ae18":"def cross_val_model(model_, model_name_, X_, y_, X_test_, target_, scalers_, fold_=5, path_='', seed_=12359):\n    \n    n_estimators = model_.get_params()['n_estimators']\n    score        = []     \n    taco         = 50 \n    acc_best     = 0\n\n    for scaler_ in scalers_: \n\n        time_start   = datetime.now()\n                \n        if scaler_!=None:\n            string_scaler = str(scaler_)        \n            string_scaler = string_scaler[:string_scaler.index('(')]\n            X_tst = scaler_.fit_transform(X_test_.copy())\n        else:\n            string_scaler = None \n            X_tst = X_test_.copy()\n            \n        y_pred_test = np.zeros(len(X_test_))\n\n        folds = KFold(n_splits=fold_, random_state=seed_, shuffle=True)\n\n        print('='*taco)\n        print('Scaler: {} - n_estimators: {}'.format(string_scaler, n_estimators))\n        print('='*taco)\n\n        for fold, (trn_idx, val_idx) in enumerate(folds.split(X_)):\n\n            time_fold_start = datetime.now()\n\n            # ---------------------------------------------------- \n            # Separar dados para treino \n            # ----------------------------------------------------     \n            X_trn, X_val = X_.iloc[trn_idx], X_.iloc[val_idx]\n            y_trn, y_val = y_.iloc[trn_idx], y_.iloc[val_idx]   \n            \n            # ---------------------------------------------------- \n            # Processamento \n            # ----------------------------------------------------     \n            if scaler_!=None:                \n                X_trn = scaler_.fit_transform(X_trn)\n                X_val = scaler_.fit_transform(X_val)\n                \n            # ---------------------------------------------------- \n            # Treinar o modelo \n            # ---------------------------------------------------- \n            model_.fit(X_trn, y_trn,\n                    eval_set              = [(X_val, y_val)],                    \n                    early_stopping_rounds = n_estimators*.2,\n                    verbose               = False)\n            \n            # ---------------------------------------------------- \n            # Predi\u00e7\u00e3o \n            # ----------------------------------------------------     \n            y_pred_val   = model_.predict(X_val)    \n            y_pred_test += model_.predict(X_tst) \/ folds.n_splits\n            \n            # ---------------------------------------------------- \n            # Score \n            # ---------------------------------------------------- \n            acc  = metrics.accuracy_score(y_val, y_pred_val)\n            score.append(acc)\n\n            # ---------------------------------------------------- \n            # Print resultado  \n            # ---------------------------------------------------- \n            time_fold_end = diff(time_fold_start, datetime.now())\n            \n            print('[Fold {}] ACC: {:2.5f} - {}'.format(fold+1, acc, time_fold_end))\n\n        acc_mean = np.mean(score) \n        acc_std  = np.std(score)\n\n        if acc_mean > acc_best:     \n            acc_best      = acc_mean           \n            model_best    = model_    \n            scaler_best   = scaler_\n\n        time_end = diff(time_start, datetime.now())   \n\n        print('-'*taco)\n        print('[Mean Fold] ACC: {:2.5f} std: {:2.5f} - {}'.format(acc_mean, acc_std, time_end))\n        print('='*taco)\n        print()\n\n        df_submission[target_] = y_pred_test.astype('int')\n        name_file_sub = model_name_ + '_' + str(scaler_).lower()[:4] + '.csv'\n        df_submission.to_csv(path_ + name_file_sub.format(acc_mean), index = False)\n\n    print('-'*taco)\n    print('Scaler Best: {}'.format(scaler_best))\n    print('Score      : {:2.5f}'.format(acc_best))\n    print('-'*taco)\n    print()","0cb65677":"%%time \nscalers = [None,\n           StandardScaler(), \n           RobustScaler(), \n           MinMaxScaler(), \n           MaxAbsScaler(), \n           QuantileTransformer(output_distribution='normal', random_state=0)\n          ] \n\ncross_val_model(model_      = xgb.XGBClassifier(**params, learning_rate=0.4),\n                model_name_ = 'xgb_baseline_score_{:2.5f}',\n                X_          = X,\n                y_          = y,\n                X_test_     = X_test,\n                target_     = target,\n                scalers_    = scalers,\n                fold_       = 5, \n                path_       = '',\n                seed_       = seed\n                )","cbfa6b57":"### 2.6.3. Vari\u00e1vel predidora","531a3b48":"<div class=\"alert alert-info\" role=\"alert\"> \n \n**`NOTA:`** <br>\nVari\u00e1veis `Soil_Type_7` e `Soil_Type_15` n\u00e3o tem informa\u00e7\u00f5es, neste caso podemos excluir.    \n    \n<\/div>","3ea77bb2":"## 3.1. Split Train\/Test","8f1bb8bc":"### 1.3.1. Carregar Dados\nS\u00e3o dois arquivos que vamos utilizar para an\u00e1lise e treinanmento dos modelos, e um arquivo para submiss\u00e3o na competi\u00e7\u00e3o.\n\n- `train.csv`: arquivo com dados de treinamento;  \n- `test.csv`: arquivo que ser\u00e1 utilizado para previs\u00e3o; \n- `sample_submission.csv`: arquivo utlizado para envio das previs\u00f5es.  \n","5e222c96":"## 2.4 Estat\u00edstica Descritiva\nAbaixo est\u00e3o as estat\u00edsticas b\u00e1sicas para cada vari\u00e1vel que cont\u00e9m informa\u00e7\u00f5es sobre contagem, m\u00e9dia, desvio padr\u00e3o, m\u00ednimo, 1\u00ba quartil, mediana, 3\u00ba quartil e m\u00e1ximo.","4ddf9499":"## 1.1. Bibliotecas ","feb40bda":"- Test","079a39ba":"<div class=\"alert alert-info\" role=\"alert\"> \n \n**`NOTA:`** <br>\n    \nAcima observamos que temos que fazer um tratamento de outliers em algumas vari\u00e1veis, na etapa de processamento vamos fazer o tramento ou remo\u00e7\u00e3o dos outliers para ajudar na previs\u00e3o dos modelos. \n\n<\/div>","1b6fca72":"## 1.3. Dataset","af77ab74":"### 2.4.1 Atributos num\u00e9ricos","a9b017dd":"## 2.1. Dimens\u00e3o do DataSet","11ca09fb":"### 2.6.2. Propor\u00e7\u00e3o de vari\u00e1veis","5ec07291":"## 2.6.6. Detec\u00e7\u00e3o de Outlier","f2aaac9a":"<div class=\"alert alert-info\" role=\"alert\"> \n \n**`NOTA:`** <br>\nObservamos no gr\u00e1fico acima que a classe 2 \u00e9 a mais representativa, seguida pela classe 1. As classes raras s\u00e3o 4, 5 e 6, a classe 5 com apenas um \u00fanico exemplo podemos fazer a exclus\u00e3o ou associ\u00e1-la a outra classe, mas para isso devemos analisar a associa\u00e7\u00e3o dessa classe, pois ter\u00e1 um impacto m\u00ednimo na classifica\u00e7\u00e3o.  \n    \n<\/div>","8e36c24a":"<div class=\"alert alert-info\" role=\"alert\"> \n    \n**`NOTA:`** <br>\n\nN\u00e3o temos dados faltantes.\n    \n<\/div>","b04bbb04":"# Descri\u00e7\u00e3o de dados\n\nPara esta competi\u00e7\u00e3o, voc\u00ea vai prever se um cliente fez uma reclama\u00e7\u00e3o sobre uma ap\u00f3lice de seguro. A verdade fundamental claimtem valor bin\u00e1rio, mas uma previs\u00e3o pode ser qualquer n\u00famero de 0.0 para 1.0, representando a probabilidade de uma reclama\u00e7\u00e3o. Os recursos neste conjunto de dados foram tornados an\u00f4nimos e podem conter valores ausentes.\narquivos\n\n- `train.csv`: os dados de treinamento com o alvo claimcoluna\n- `test.csv`: o conjunto de teste; voc\u00ea estar\u00e1 prevendo o claimpara cada linha neste arquivo\n- `sample_submission.csv`:  um arquivo de envio de amostra no formato correto","84129fd7":"<div class=\"alert alert-info\" role=\"alert\"> \n    \n**`NOTA:`** <br>\n    \nTemos quatros vari\u00e1veis que podemos transformar em dammy, isso pode ajudar a melhor a performance dos modelos. \n\n    \n<\/div> ","83d85fad":"<div class=\"alert alert-info\" role=\"alert\"> \n \n**`NOTA:`** <br>\nTivemos uma redu\u00e7\u00e3o de 85% em ambos os datasets.\n    \n<\/div>","36870f9b":"## 2.2. Tipo de dados","88690de2":"## 2.5. Correla\u00e7\u00e3o\nVamos examinar a correla\u00e7\u00e3o entre as vari\u00e1veis.","a5f5d49a":"### 2.6.6.2. Data Test","ec52927e":"### 2.6.4. Distribui\u00e7\u00e3o Train x Test","ebbac0e0":"<div class=\"alert alert-info\" role=\"alert\"> \n \n**`NOTA:`** <br>\n    \nCom scaler  RobustScaler obtivemos uma ACC de 0.92970, como estamos fazer apenas uma valida\u00e7\u00e3o simples, neste caso a pontua\u00e7\u00e3o do score pode ser afetada por aleatoriedade dos dados, sendo assim, vamos fazer uma valida\u00e7\u00e3o cruzada para termos uma estimativa robusta.  <br>\n    \n<\/div>","e308d024":"## 3.2. Sele\u00e7\u00e3o de Scaler","6faea53b":"## 1.2. Fun\u00e7\u00f5es\nAqui centralizamos todas as fun\u00e7\u00f5es desenvolvidas durante o projeto para melhor organiza\u00e7\u00e3o do c\u00f3digo.","df3973e1":"<div class=\"alert alert-info\" role=\"alert\"> \n    \n**`NOTA:`** <br>\n\nO que me chamou aten\u00e7\u00e3o \u00e9 valor negativo para a maioria das vari\u00e1veis exceto para `Hillshade_Noon` em ambos os dataset e para a vari\u00e1vel `Hillshade_9am` apenas no dataset de test, pode ser um indicativo de algum problema na coleta de informa\u00e7\u00f5es, mas com se trata de informa\u00e7\u00f5es de `Cobertura Florestal` pode fazer sentido. \u00c9 um ponto que temos que levar em considera\u00e7\u00e3o no processamento de dados.  \n    \n<\/div>","da99e0d5":"### 2.6.5.  Vari\u00e1veis categoricas","d910235a":"<div class=\"alert alert-info\" role=\"alert\"> \n \n**`NOTA:`** <br>\n    \nComo podemos observar, a correla\u00e7\u00e3o est\u00e1 entre -0.6 e 0.2, o que \u00e9 muito pequeno, portanto, as vari\u00e1veis s\u00e3o fracamente correlacionados.\n    \n<\/div>","917fcc1d":"<h1 div class='alert alert-success'><center> Ponto de partida (EDA)<\/center><\/h1>\n\n![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/26480\/logos\/header.png?t=2021-04-09-00-57-05)","6a2d3447":"### 2.6.6.1. Data Train ","544443a7":"### 2.6.1. Train \/ Test","006d44ee":"# <div class=\"alert alert-success\"> 2. An\u00e1lise Explorat\u00f3ria de Dados (EDA)  <\/div> ","283ed574":"### 1.3.1. Redu\u00e7\u00e3o dos datasets","4618d5d0":"## 2.6. Distribui\u00e7\u00e3o","5463c3a5":"## 2.3. identificar vari\u00e1veis ausentes (NA)","17c3f22d":"### 2.4.2. Atributos categ\u00f3ricos","7c5d0687":"## 3.2. Parametros do modelo","dd7079e0":"<div class=\"alert alert-info\" role=\"alert\"> \n \n**`CONCLUS\u00c3O:`** <br>\n    \n1. Verificar os valores negativos identificados na an\u00e1lise descritiva dos dados; \n2. Transforma\u00e7\u00e3o das vari\u00e1veis `Wilderness_Area` em dammy;\n3. Tratamento de outliers; \n4. Exclus\u00e3o das vari\u00e1veis `Soil_Type_7` e `Soil_Type_15` sem informa\u00e7\u00f5es para modelagem. \n    \n    \n<\/div>","0e4d85c0":"# <div class=\"alert alert-success\">  1. IMPORTA\u00c7\u00d5ES <\/div> ","4e5cab83":"# <div class=\"alert alert-success\"> 3. Modelagem (baseline) <\/div> ","6327c936":"- Train"}}