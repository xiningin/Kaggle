{"cell_type":{"707c9a1d":"code","7690a531":"code","2b265ddc":"code","624f6e33":"code","4ceb16d0":"code","ecdd36dd":"code","07f06860":"code","c2946ad2":"code","47c7468a":"code","64208524":"code","da7fef73":"code","90e5d10d":"code","c08854d6":"code","1d9badb4":"code","40a81e2b":"code","ddfcd779":"code","e04c9197":"code","7b7e6d0b":"code","13e99fad":"code","9b6cb192":"code","e03cc8d6":"code","e1e47d4a":"code","443fa9a6":"code","720b9d07":"code","bfcec73d":"code","e8e746aa":"code","32db102a":"code","31a8ad40":"code","3d8070ac":"code","ddbb68ff":"code","cf8fde86":"code","56200096":"code","7db15868":"code","eb6eedcb":"code","6698f9a4":"code","191e560e":"code","318f9b7d":"code","16195443":"code","7fc98eb4":"code","3c5640ff":"code","4d88ee5b":"code","70775643":"code","f8864473":"code","eefbd1d7":"code","ebb8ad7e":"code","7b5a0f36":"code","52d61b9f":"code","a5716d2d":"code","0ca38054":"code","173da671":"code","c5464c50":"code","680faa18":"code","cdeaa3fe":"code","a87417a6":"markdown","338ebe88":"markdown","31a41aa6":"markdown","d90801f7":"markdown","9bb42896":"markdown","b7b9f56a":"markdown","d865c65d":"markdown","6fe8af16":"markdown","9c6ea2c1":"markdown","bc47ebd3":"markdown","2229cea0":"markdown","f3da621e":"markdown","2b401aae":"markdown","529e76a9":"markdown","a8106add":"markdown","c591c6df":"markdown","07a8fd5a":"markdown","6d2e6f16":"markdown","6a472437":"markdown","c55768b3":"markdown","673145eb":"markdown","da8281c6":"markdown","e75de6db":"markdown","f1fe545c":"markdown","c90d1256":"markdown","2db3b0d4":"markdown","949e0099":"markdown","4470c5ef":"markdown","0ce33795":"markdown","15e0d89f":"markdown","4a79fe54":"markdown","e486985b":"markdown","fdc335b7":"markdown","afb467de":"markdown","0bcd4387":"markdown","66e448b1":"markdown","fa91717c":"markdown","c43737a3":"markdown","609b6e8f":"markdown","1401b4cb":"markdown","341fe2ad":"markdown","569e0a8b":"markdown","d82a5019":"markdown","404cdaf6":"markdown","5e2724eb":"markdown","9c600cb5":"markdown","86914ee6":"markdown","85c55fd9":"markdown","ffc59e62":"markdown"},"source":{"707c9a1d":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7690a531":"#Data Wrangling:\nimport numpy as np\nimport pandas as pd\n\n#Data Visualization:\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n%matplotlib inline","2b265ddc":"df = pd.read_csv('..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')\ndf.head(10)","624f6e33":"df.shape","4ceb16d0":"df.info()","ecdd36dd":"plt.figure(figsize=(12,6))\nplt.title('Searching for missing values:')\nsns.heatmap(data=df.isnull(),cmap = 'coolwarm', cbar = False)","07f06860":"df.describe()","c2946ad2":"plt.style.use('ggplot')\nfig, axis  = plt.subplots(nrows = 3, ncols = 2, figsize = (15,9))\n\nax0, ax1, ax2, ax3, ax4, ax5 = axis.flatten()\n\nax0.hist(df['age'])\nax0.set_xlabel('Age')\n\nax1.hist(df['creatinine_phosphokinase'])\nax1.set_xlabel('CPK Enzyme')\n\nax2.hist(df['platelets'], bins = 15)\nax2.set_xlabel('Platelets')\n\nax3.hist(df['serum_creatinine'])\nax3.set_xlabel('Serum Creatinine')\n\nax4.hist(df['serum_sodium'], bins = 15)\nax4.set_xlabel('Serum Sodium')\n\nax5.hist(df['ejection_fraction'])\nax5.set_xlabel('Ejection Fraction')\n\nplt.tight_layout()","47c7468a":"fig, ax  = plt.subplots(nrows = 3, ncols = 2, figsize = (12,6))\nplt.tight_layout()\nsns.countplot(df['anaemia'], ax=ax[0,0])\nsns.countplot(df['diabetes'], ax=ax[0,1])\nsns.countplot(df['high_blood_pressure'], ax=ax[1,0])\nsns.countplot(df['sex'], ax=ax[1,1])\nsns.countplot(df['smoking'], ax=ax[2,0])\nfig.delaxes(ax[2,1])","64208524":"x1 = (len(df[df['anaemia'] == 1]))\/len(df['anaemia'])\nx2 = (len(df[df['diabetes'] == 1]))\/len(df['diabetes'])\nx3 = (len(df[df['high_blood_pressure'] == 1]))\/len(df['high_blood_pressure'])\nx4 = (len(df[df['sex'] == 1]))\/len(df['sex'])\nx5 = (len(df[df['smoking'] == 1]))\/len(df['smoking'])\n\ndata = {'Anaemia': x1, 'Diabetes': x2, 'High Blood Pressure': x3, 'Sex': x4, \n       'Smoking': x5}\ncateg_zeros = pd.DataFrame(data, index=[1])\ncateg_zeros","da7fef73":"df[['sex', 'DEATH_EVENT']].groupby(['sex'], as_index = False).mean()","90e5d10d":"df[['smoking', 'DEATH_EVENT']].groupby(['smoking'], as_index = False).mean()","c08854d6":"df[['anaemia', 'DEATH_EVENT']].groupby(['anaemia'], as_index = False).mean()","1d9badb4":"df[['diabetes', 'DEATH_EVENT']].groupby(['diabetes'], as_index = False).mean()","40a81e2b":"df[['high_blood_pressure', 'DEATH_EVENT']].groupby(['high_blood_pressure'], \n                                                   as_index = False).mean()","ddfcd779":"plt.figure(figsize = (12,6))\nsns.heatmap(df[['age', 'creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium', 'time', 'DEATH_EVENT']].corr(), cmap = 'coolwarm', annot = True)","e04c9197":"g = sns.FacetGrid(df, col = 'DEATH_EVENT')\ng.map(plt.hist, 'age')","7b7e6d0b":"g = sns.FacetGrid(df, col = 'DEATH_EVENT')\ng.map(plt.hist, 'time')","13e99fad":"g = sns.FacetGrid(df, col = 'DEATH_EVENT')\ng.map(plt.hist, 'ejection_fraction')","9b6cb192":"g = sns.FacetGrid(df, col = 'DEATH_EVENT')\ng.map(plt.hist, 'serum_creatinine')","e03cc8d6":"g = sns.FacetGrid(df, col = 'DEATH_EVENT', row = 'sex')\ng.map(plt.hist, 'age')","e1e47d4a":"print(\"Before\", df.shape)\ndf = df.drop(['platelets', 'creatinine_phosphokinase', 'serum_sodium', 'sex', 'smoking'], axis = 1)\nprint(\"After\", df.shape)","443fa9a6":"df['UnderCon'] = 0\ndf.loc[((df['anaemia'] == 1) | (df['diabetes'] == 1) | df['high_blood_pressure'] == 1), 'UnderCon'] = 1\ndf.drop(['anaemia', 'diabetes', 'high_blood_pressure'], axis = 1, inplace = True)\ndf.head()","720b9d07":"df.shape","bfcec73d":"df[['DEATH_EVENT', 'UnderCon']].groupby('UnderCon', as_index = False).mean()","e8e746aa":"df = df.drop('UnderCon', axis = 1)","32db102a":"df.head()","31a8ad40":"fig, ax = plt.subplots(nrows = 2, ncols = 2, figsize = (12,6))\nplt.tight_layout()\n\nax0,ax1,ax2,ax3 = ax.flatten()\n\nax0.boxplot(df['serum_creatinine'])\nax0.set_title('Serum Creatinine')\n\nax1.boxplot(df['ejection_fraction'])\nax1.set_title('Ejection Fraction')\n\nax2.boxplot(df['age'])\nax2.set_title('Age')\n\nax3.boxplot(df['time'])\nax3.set_title('time')","3d8070ac":"serum_cmode = df['serum_creatinine'].median()\ndf.loc[(df['serum_creatinine'] > 4), 'serum_creatinine'] = serum_cmode\ndf.head(10)","ddbb68ff":"df['AgeBand'] = pd.cut(df['age'],5)\ndf[['AgeBand', 'DEATH_EVENT']].groupby('AgeBand', as_index = False).mean()","cf8fde86":"#Mapping:\n\ndf.loc[df['age'] <= 51, 'age'] = 0\ndf.loc[(df['age'] > 51) & (df['age'] <= 62), 'age'] = 1\ndf.loc[(df['age'] > 62) & (df['age'] <= 73), 'age'] = 2\ndf.loc[(df['age'] > 73) & (df['age'] <= 84), 'age'] = 3\ndf.loc[(df['age'] > 84) & (df['age'] <= 95), 'age'] = 4\n\ndf.drop('AgeBand', axis = 1, inplace = True)","56200096":"df['TimeBand'] = pd.cut(df['time'],5)\ndf[['TimeBand', 'DEATH_EVENT']].groupby('TimeBand', as_index = False).mean()","7db15868":"#Mapping:\n\ndf.loc[df['time'] <= 60.2, 'time'] = 0\ndf.loc[(df['time'] > 60.2) & (df['time'] <= 116.4), 'time'] = 1\ndf.loc[(df['time'] > 116.4) & (df['time'] <= 172.6), 'time'] = 2\ndf.loc[(df['time'] > 172.6) & (df['time'] <= 228.8), 'time'] = 3\ndf.loc[(df['time'] > 228.8) & (df['time'] <= 285), 'time'] = 4\n\ndf.drop('TimeBand', axis = 1, inplace = True)","eb6eedcb":"df['SCBand'] = pd.cut(df['serum_creatinine'],5)\ndf[['SCBand', 'DEATH_EVENT']].groupby('SCBand', as_index = False).mean()","6698f9a4":"#Mapping:\n\ndf.loc[df['serum_creatinine'] <= 1.2, 'serum_creatinine'] = 0\ndf.loc[(df['serum_creatinine'] > 1.2) & (df['serum_creatinine'] <= 1.9), 'serum_creatinine'] = 1\ndf.loc[(df['serum_creatinine'] > 1.9) & (df['serum_creatinine'] <= 2.6), 'serum_creatinine'] = 2\ndf.loc[(df['serum_creatinine'] > 2.6) & (df['serum_creatinine'] <= 3.3), 'serum_creatinine'] = 3\ndf.loc[(df['serum_creatinine'] > 3.3) & (df['serum_creatinine'] <= 4), 'serum_creatinine'] = 4\n\ndf.drop('SCBand', axis = 1, inplace = True)","191e560e":"df['EJBand'] = pd.cut(df['ejection_fraction'],5)\ndf[['EJBand', 'DEATH_EVENT']].groupby('EJBand', as_index = False).mean()","318f9b7d":"#Mapping:\n\ndf.loc[df['ejection_fraction'] <= 27.2, 'ejection_fraction'] = 0\ndf.loc[(df['ejection_fraction'] > 27.2) & (df['ejection_fraction'] <= 40.4), 'ejection_fraction'] = 1\ndf.loc[(df['ejection_fraction'] > 40.4) & (df['ejection_fraction'] <= 53.6), 'ejection_fraction'] = 2\ndf.loc[(df['ejection_fraction'] > 53.6) & (df['ejection_fraction'] <= 66.8), 'ejection_fraction'] = 3\ndf.loc[(df['ejection_fraction'] > 66.8) & (df['ejection_fraction'] <= 80), 'ejection_fraction'] = 4\n\ndf.drop('EJBand', axis = 1, inplace = True)","16195443":"#Converting age and serum creatinine to integers:\ndf['age'] = df['age'].astype(int)\ndf['serum_creatinine'] = df['serum_creatinine'].astype(int)\ndf.head()","7fc98eb4":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","3c5640ff":"train, test = train_test_split(df, test_size = 0.2, random_state = 42)\nprint(train.head())\nprint('_'*40)\nprint(test.head())","4d88ee5b":"X_train = train.drop('DEATH_EVENT', axis = 1)\nY_train = train['DEATH_EVENT']\nX_test = test.drop('DEATH_EVENT', axis=1).copy()\nY_test = test['DEATH_EVENT']\n\nX_train.shape,Y_train.shape,X_test.shape, Y_test.shape","70775643":"logreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\n\nacc_log_train = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log_test = round(accuracy_score(Y_pred, Y_test) * 100,2)\n\nprint('Training Score:',acc_log_train,'%')\nprint('-'*25)\nprint('Test Score:',acc_log_test,'%')","f8864473":"svc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\n\nacc_svc_train = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc_test = round(accuracy_score(Y_pred, Y_test) * 100,2)\n\nprint('Training Score:',acc_svc_train,'%')\nprint('-'*25)\nprint('Test Score:',acc_svc_test,'%')\n","eefbd1d7":"error_rate = []\nfor i in range(1,40):\n knn = KNeighborsClassifier(n_neighbors=i)\n knn.fit(X_train,Y_train)\n pred_i = knn.predict(X_test)\n error_rate.append(np.mean(pred_i != Y_test))\n\nplt.figure(figsize=(10,6))\nplt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', \n         marker='o',markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')\nprint(\"Minimum error:-\",min(error_rate),\"at K =\",error_rate.index(min(error_rate)))","ebb8ad7e":"knn = KNeighborsClassifier(n_neighbors = error_rate.index(min(error_rate)))\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\n\nacc_knn_train = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn_test = round(accuracy_score(Y_pred, Y_test) * 100,2)\n\nprint('Training Score:',acc_knn_train,'%')\nprint('-'*25)\nprint('Test Score:',acc_knn_test,'%')","7b5a0f36":"gaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\n\nacc_gaussian_train = round(gaussian.score(X_train, Y_train) * 100, 2)\nacc_gaussian_test = round(accuracy_score(Y_pred, Y_test) * 100,2)\n\nprint('Training Score:',acc_gaussian_train,'%')\nprint('-'*25)\nprint('Test Score:',acc_gaussian_test,'%')","52d61b9f":"perceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nY_pred = perceptron.predict(X_test)\n\nacc_perceptron_train = round(perceptron.score(X_train, Y_train) * 100, 2)\nacc_perceptron_test = round(accuracy_score(Y_pred, Y_test) * 100,2)\n\nprint('Training Score:',acc_perceptron_train,'%')\nprint('-'*25)\nprint('Test Score:',acc_perceptron_test,'%')","a5716d2d":"linear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\nY_pred = linear_svc.predict(X_test)\n\nacc_linear_svc_train = round(linear_svc.score(X_train, Y_train) * 100, 2)\nacc_linear_svc_test = round(accuracy_score(Y_pred, Y_test) * 100,2)\n\nprint('Training Score:',acc_linear_svc_train,'%')\nprint('-'*25)\nprint('Test Score:',acc_linear_svc_test,'%')","0ca38054":"sgd = SGDClassifier()\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\n\nacc_sgd_train = round(sgd.score(X_train, Y_train) * 100, 2)\nacc_sgd_test = round(accuracy_score(Y_pred, Y_test) * 100,2)\n\nprint('Training Score:',acc_sgd_train,'%')\nprint('-'*25)\nprint('Test Score:',acc_sgd_test,'%')","173da671":"decision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\n\nacc_decision_tree_train = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree_test = round(accuracy_score(Y_pred, Y_test) * 100,2)\n\nprint('Training Score:',acc_decision_tree_train,'%')\nprint('-'*25)\nprint('Test Score:',acc_decision_tree_test,'%')","c5464c50":"random_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\n\nacc_random_forest_train = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest_test = round(accuracy_score(Y_pred, Y_test) * 100,2)\n\nprint('Training Score:',acc_random_forest_train,'%')\nprint('-'*25)\nprint('Test Score:',acc_random_forest_test,'%')","680faa18":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree'],\n    'Score': [acc_svc_train, acc_knn_train, acc_log_train, \n              acc_random_forest_train, acc_gaussian_train, acc_perceptron_train, \n              acc_sgd_train, acc_linear_svc_train, acc_decision_tree_train]})\nmodels.sort_values(by='Score', ascending=False)","cdeaa3fe":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree'],\n    'Score': [acc_svc_test, acc_knn_test, acc_log_test, \n              acc_random_forest_test, acc_gaussian_test, acc_perceptron_test, \n              acc_sgd_test, acc_linear_svc_test, acc_decision_tree_test]})\nmodels.sort_values(by='Score', ascending=False)","a87417a6":"# Analyzing relationships between categorical and numerical variables:","338ebe88":"## Analyzing relationships between survival and categorical variables via grouping:","31a41aa6":"## Gaussian Naive-Bayes","d90801f7":"### Assumptions based on data analysis\n\nWe arrive at following assumptions based on data analysis done so far. We may validate these assumptions further before taking appropriate actions.\n\n**Completing.**\n\n- No variables require completing\n\n**Correcting.**\n\n1. We may want to correct the above mentioned variables in the event we find that there is some correlation with survival\n\n\n**Creating.**\n\n1. We may want create bands for age given that patients become more vulnerable as the get older.\n2. Given how broad the time feature is in terms of values - we may want to also create a band for this if there is indeed a relationships with survival\n3. We may want to create a new column denoting whether the patient had any sort of underlying condition i.e. anaemia, diabetes or high blood pressure\n\n**Classifying.**\n\nAdditional assumptions:\n\n1. Women may have higher chances of survival than men\n2. Those who had less time in follow up days had higher chances of survival\n3. Those who were younger than a certain age had higher chances of survival","9bb42896":"Time\n- Similar Conclusion.","b7b9f56a":"Ejection Fraction\n- Similar Conclusion.","d865c65d":"# Modelling and Prediction:\n\nThis is a classification and a regression problem since we are predicting an output based off of other independent features. Furthermore - this is a supervised problem as we will have a training dataset to train our model against.\n\nPossible models:\n- Logistic Regression\n- KNN or k-Nearest Neighbors\n- Support Vector Machines\n- Naive Bayes classifier\n- Decision Tree\n- Random Forrest\n- Perceptron\n- Artificial neural network\n- RVM or Relevance Vector Machine","6fe8af16":"## Logistic Regression","9c6ea2c1":"## Conclusion:\n\nThere doesn't appear to be any correlation here, so we can just go ahead and drop this leaving us with our 4 main features to use in our model training - now we just need to deal with our outliers.","bc47ebd3":"## Conclusions:\n\nWhat we notice here is that there does not seem to be any correlation between  whether the patients had any underlying health conditions, their sex or whether they smoked against whether they died or not.\nIn conclusion we can say assumptions 1 and 4 made above are false.\n\nBefore we can discard these variables - it is best to see if we see if we can create a new feature out of these before dropping them. For example - we could create a new categorical feature denoting whether the patient has an underlying condition or not (diabetes, anaemia, high blood pressure). For sex and smoking - it seems so that we should drop them but before we do this, we do so we should check for underlying relationships with numerical variables.","2229cea0":"We can see all features are numerical variables","f3da621e":"## Random Forest","2b401aae":"# Data Wrangling:\n\n1. From the above - we have overall said that our only useful features at the moment are: age, time, serum creatinine and the ejection fraction. \n2. At the same time we have said that we can create a new column denoting whether a patient has any underlying condition whether it be anaemia, high blood pressure or diabetes.\n3. The remaining features can be dropped\n\nSo all we need to do is create the new feature - assess how useful it is and decide whether we need to drop this, we need to fix for outliers for our useful features and drop the relevant features we no longer require before we can begin the prediction stage. We also need to band relevant features too.","529e76a9":"## Linear SVC","a8106add":"## Analyzing relationships between survival and numerical variables:\n\nFirstly a heatmap will come in very useful.\n\nWe immediately notice that there appears to be correlation between death with either age, no. of follow up days, ejection fraction and serum sodium.","c591c6df":"## Conclusion:\n\nBefore we even go about removing outliers we need to understand typical values for these features - one just can't remove outliers! \n\nFor example - the ejection fraction of a healthy person is typically between 50 -70% so a patient having a percentage between 70-80% which was once thought as an outlier isn't necessarily even though the boxplot suggests this. \n\nFor Serum Creatinine - it seems there are very large values distant from the rest of the datapoints suggesting the existence of outliers (in this case values larger than say 3.5.) - to correct this - we'll replace values larger than 4 with the median of the rest of the values. \nNOTE - whilst the outliers occur at values higher than around 2 rather than 3.5 as shown by the boxplot and using the fact that the internet says a normal human being has level at around 1.2 - I could have removed all those outliers after 2 - however I took into account that there may be some patients with underlying heart conditions not taken into account by the data.\n\nWe have no outliers for Age and Time.\n\nHence, we only need to fix values for serum creatinine levels.","07a8fd5a":"# HEART FAILURE ANALYSIS - PYTHON\n\nHi everyone, I'm Tasdeed and this is my first submission on Kaggle and this is my first real hands Data Science project as well as first time using Python after an online course.\nI have never done machine learning before, even more so in Python and with help from other notebooks and online resources I managed to implement some methods, so the accuracies of the different algorithms I used could most definitely be improved once I learn the different sorts of parameters these models take!\nI would love for some feedback as well from more experienced people on certain things I can implement or further resources!\nIf this notebook is good enough I would appreciate it if the reader upvotes this!","6d2e6f16":"### Converting age and serum creatinine to integers:","6a472437":"# Conclusions:\n\n- It would seem that the Linear SVC model produces our best results of 78.3% accuracy on the test set with the best model under the training set being the Random Forest with 94% accuracy\n- There is clear room for improvement if we can adjust hyperparameters using cross-validation (currently unsure of how to do this with current knowledge)\n    (Note to self: come back to this later and attempt to improve scores!)\n    \n- This concludes this notebook, feel free to give some feedback on improvements or any available to resources to learn from!\n- Up-vote if you think this is good!","c55768b3":"We have completely filled data - that's not to say however that we don't have any outliers!","673145eb":"# Model Evaluation:","da8281c6":"## Decision Tree","e75de6db":"## Meaning of Variables:\n\nAge - Self Explanatory\n\nAnaemia - Is 1 or 0 with 1 being the patient does have this condition.\nAnemia is a condition in which you lack enough healthy red blood cells to carry adequate oxygen to your body's tissues.\n\nCreatinine Phosphokinase - Level of CPK enzyme in the blood\n\nDiabetes - Is a 1 or 0 - whether the patient suffers from diabetes or not - similar to anaemia\n\nEjection Fraction - Is a percentage (numerical between 0 to 100)\nEjection fraction is a measurement of the percentage of blood leaving your heart each time it contracts.\n\nHigh Blood Pressure - Is a 1 or 0 - whether patient suffers from high blood pressure\n\nPlatelets - no. of platelets in the blood\n\nSerum Creatine - Level of Creatine produced from the kidneys in the blood\n\nSerum Sodium - Level of Sodium in the blood\n\nSex - Self Explanatory - assuming 1 is male and 0 is female\n\nSmoking - Self Explanatory - assuming 1 is smokes and 2 is doesn't smoke\n\nTime - Follow up days\n\nDeath Event - whether patient died during follow up period","f1fe545c":"### Conclusions:\n\n- The majority of patients were aged between 50-65 - with around a third of patients above 65 \n- Our remarks from above regarding the possibility of outliers have been confirmed as shown by the plots for the CPK enzyme, platelet counts, creatinine, sodium and ejection fraction\n- Most patients CPK in fact were below 1000\n- Serum Creatinine was less than 2 (>75%)\n- Platelets varied between 150K-400K\n- Sodium varied between 132-142\n- Ejection Fraction varied significantly amongst patients with some as low as 15% as high as 55% excluding outliers","c90d1256":"## Assessing for null-values:","2db3b0d4":"## Conclusion:\n\n- The majority of patients in this sample (65%) are male by assumption that 1 = Male\n- Around a third of patients suffered from either High Blood Pressure, Smoking or both\n- Around 40% of the sample either had anaemia or diabeters or both too.","949e0099":"## Creating new underlying condion column (UnderCon):","4470c5ef":"## About the Dataset\n\nCardiovascular diseases (CVDs) are the number 1 cause of death globally, taking an estimated 17.9 million lives each year, which accounts for 31% of all deaths worlwide.\nHeart failure is a common event caused by CVDs and this dataset contains 12 features that can be used to predict mortality by heart failure.\n\nMost cardiovascular diseases can be prevented by addressing behavioural risk factors such as tobacco use, unhealthy diet and obesity, physical inactivity and harmful use of alcohol using population-wide strategies.\n\nPeople with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease) need early detection and management wherein a machine learning model can be of great help.","0ce33795":"## Conclusions: \n\n1. Those over 70 had higher chances of dying, whilst many under 50 had higher chances of surival suggesting the need to band ages. So assumption #3 is true!\n2. Those with a larger amount of follow up days tended to survive whilst those with less (<50) ended up with a higher mortality - for model building purposes we would like to band this too. This was in fact the opposite our assumption #2.\n3. With regards to the ejection fraction, what we can see it that with high percentages had lower deaths. Those with lower percentages (<30%) had more deaths - again suggesting that we should create some bands.\n4. Patients with higher creatinine levels suffered more deaths whist the vast majority (75%) who had percentages less than 1.4 has greater survival rates.\n5. Platelets and CPK enzymes do not seem to have an effect with survival as well as with other variables - it makes sense to drop these\n\nWe can conclude that we have 4 numerical features which are all useful for our model building and that these features need to be sorted out in terms of outliers should they contain any. We would also like to create bands on the four of these.","15e0d89f":"## Stochastic Gradient Descent","4a79fe54":"## Score under the training set:","e486985b":"## Now looking at distributions of the categorical variables:","fdc335b7":"# Importing Relevant Libraries for analysis:","afb467de":"## Support Vector Machines","0bcd4387":"## Conclusion\n\n1. Looking at the plot above - for smoking against the age - we either don't see much relationship or we reach similar conclusions from above even when we split into whether a patient smokes or not - the same can be said for smoking against the other 3 relevant numerical variables. This means assumption #1 is false.\n2. The same can be said for sex - overall this strengthens our case for dropping the two variables.","66e448b1":"Serum Creatinine\n- Similar Conclusion.","fa91717c":"## Banding features:","c43737a3":"## Feature Dropping:","609b6e8f":"## What sort of datatypes do we have?","1401b4cb":"## Looking for features which may contain errors\/typos","341fe2ad":"## Score under the test set:","569e0a8b":"Age\n\n- As we can see banding was effective - we see clear correlation!","d82a5019":"## Assessing the distribution of the features:\n\n## Continuous Features:","404cdaf6":"### Conclusions:\n\n1. We seem to have outliers for CPK - since the max value is way higher than expected from the 75% quartile. With the same reasoning, this is the same for Platelets (+ fact that min is 25100) and Serum Creatinine and ejection fraction and possibly time. These need to be corrected\n2. The remaining variables: Continuous (Age, Serum Sodium, Time) and Categoricals (remaining 5 variables) are fine - in total we've accounted for all 12 variables - excluding Death Event.\n\nAside from this:\n1. 96 of the 299 (32%) patients in the sample passed away","5e2724eb":"## Outlier Removal\n\nAs mentioned at the beginning of the analysis - considering the remaining features we have left - the ones that contain possible outliers are ejection fraction, time and serum creatinine","9c600cb5":"# Workflow Stage 1: Data Importation","86914ee6":"## What types of features do we have?\n\nContinuous: Age, CPK, Ejection Fraction, Platelets, Serum Creatine, Serum Sodium and Time\n\nCategorical: Anaemia, Diabetes, High Blood Pressue, Sex, Smoking, Death Event\n\nWe do not have any variables with mixed data types.","85c55fd9":"## Perceptron","ffc59e62":"## K Nearest-Neighhbours (KNN)"}}