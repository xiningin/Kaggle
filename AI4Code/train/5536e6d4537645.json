{"cell_type":{"68fc9c5a":"code","06354abe":"code","edcf6f4d":"code","1f557148":"code","ba5d7ce5":"code","2aa6d2a4":"code","f8ca0855":"code","f6f317a6":"code","07e0cee5":"code","c77400b8":"code","e5097a04":"code","36fb7476":"code","6809293d":"code","f4f15644":"markdown","98237059":"markdown","700b6ede":"markdown","ffd681d6":"markdown","13054909":"markdown","71d33004":"markdown","4f62bf64":"markdown","fd8d4513":"markdown","57891ca7":"markdown","a7952d65":"markdown"},"source":{"68fc9c5a":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprint(\"numpy version:\", np.__version__)\nprint(\"pandas version:\", pd.__version__)\n","06354abe":"data = pd.read_csv(\"..\/input\/aps_failure_training_set.csv\", na_values=\"na\")\ndata.head()","edcf6f4d":"fig, ax = plt.subplots(figsize=(15,5))\n\n# calculating pct of missing values for each feature\nmissing = data.isna().sum().div(data.shape[0]).mul(100).to_frame().sort_values(by=0, ascending = False)\n\nax.bar(missing.index, missing.values.T[0])\nplt.xticks([])\nplt.ylabel(\"Percentage missing\")\nplt.show()","1f557148":"cols_missing = missing[missing[0]>75]\nprint(\"There are {} columns with NaNs above 75%.\".format(len(cols_missing)))","ba5d7ce5":"cols_to_drop = list(cols_missing.index) # list with columns to drop\ncols_to_drop.append('class')\ncols_to_drop","2aa6d2a4":"X = data.drop(cols_to_drop, axis=1)\ny = data.loc[:,\"class\"]\ny = pd.get_dummies(y).drop(\"neg\",axis=1)","f8ca0855":"X.fillna(X.mean(), inplace=True)","f6f317a6":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)","07e0cee5":"from sklearn.metrics.scorer import make_scorer\nfrom sklearn.metrics import confusion_matrix\n\ndef my_scorer(y_true,y_pred):\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n    cost = 10*fp+500*fn\n    return cost\n\nmy_func = make_scorer(my_scorer, greater_is_better=False)","c77400b8":"from sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\n\nclf = SVC(probability = False, class_weight=\"balanced\", gamma=\"auto\") # initialising SVC classifier\npca = PCA() # initialising PCA component\n\npipe = Pipeline(steps=[(\"pca\",pca), (\"clf\",clf)]) # creating pipeline\n\n# hyperparameters grid to be investigated\nparam_grid = {\n    'pca__n_components': range(10,24),\n    'clf__C': np.arange(0.2,0.5,0.05),\n}\n\nsearch = GridSearchCV(pipe, param_grid, iid=False, cv=3, return_train_score=False, scoring=my_func, n_jobs=-1, verbose=3) # Grid Search with 3-fold CV\nsearch.fit(X_scaled, np.ravel(y))\n\n# Plotting best classificator\nprint(\"Best parameters (CV score: {:0.3f}):\".format(search.best_score_))\nprint(search.best_params_)","e5097a04":"pca.fit(X_scaled)\n\nfig, ax0 = plt.subplots(nrows=1, sharex=True, figsize=(12, 6))\nax0.plot(pca.explained_variance_ratio_, linewidth=2)\nax0.set_ylabel('PCA explained variance')\nax0.axvline(search.best_estimator_.named_steps['pca'].n_components, linestyle=':', label='n_components chosen')\nax0.legend(prop=dict(size=12))\nplt.show()","36fb7476":"search.best_estimator_","6809293d":"fig, ax1 = plt.subplots(nrows=1, sharex=True, figsize=(12, 6))\n\nresults = pd.DataFrame(search.cv_results_)\ncomponents_col = 'param_pca__n_components'\nbest_clfs = results.groupby(components_col).apply(lambda g: g.nlargest(1, 'mean_test_score'))\n\nbest_clfs.plot(x=components_col, y='mean_test_score', yerr='std_test_score',legend=False, ax=ax1)\n\nax1.set_ylabel('Classification accuracy (val)')\nax1.set_xlabel('n_components')\n\nplt.tight_layout()\nplt.show()","f4f15644":"**READING DATA**\n\nFirst I will load basic libraries and raw data. Additional libraries I will be loading as necessary to increase readibility.","98237059":"<a id='Top'><\/a>\n<center>\n<h1><u>Scania Air Pressure System Failures Prediction<\/u><\/h1>\n<\/center>\n<br>\n\n<!-- Start of Unsplash Embed Code - Centered (Embed code by @BirdyOz)-->\n<div style=\"width:60%; margin: 20px 20% !important;\">\n    <img src=\"https:\/\/images.unsplash.com\/photo-1540852360777-5f6fa7752aeb?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=720&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjEyMDd9\" class=\"img-responsive img-fluid img-med\" alt=\"trailer on road \" title=\"trailer on road \">\n    <div class=\"text-muted\" style=\"opacity: 0.5\">\n        <small><a href=\"https:\/\/unsplash.com\/photos\/mS2ngGq6VO4\" target=\"_blank\">Photo<\/a> by <a href=\"https:\/\/unsplash.com\/@vanveenjf\" target=\"_blank\">@vanveenjf<\/a> on <a href=\"https:\/\/unsplash.com\" target=\"_blank\">Unsplash<\/a>, accessed 21\/02\/2020<\/small>\n    <\/div>\n<\/div>\n<!-- End of Unsplash Embed code -->\n                \nIn this challange we are asked to predict if there is truck's APS failure based on sensors telemetry data. \nFor demonstartion purposes a Support Vector Machine for classification (SVC) will be used. It will be used together PCA to reduce dimensionality, GridSearch for hyperparameters optimisation and a custom scoring function.\n\nThis analysis is composed from the following steps:\n1. reading data\n2. dealing with missing values\n3. data standarisation (normalisation)\n4. creating custom scorer\n5. creating pipeline of PCA, grid search and SVM\n6. visualisation of results","700b6ede":"Graph above shows that we have significant amount of missing data. I will drop columns containing more than 75% of missing values.","ffd681d6":"**CREATING CUSTOM SCORER**\n\nHere I will create a custom scorer accoring to the challange guidelines. C=10FP + 500 FN. We can see that a cost for False Negative is much more than for False Positive.","13054909":"From a visual inspection of raw data it is obvious that some columns contain missing values. The first column named \"class\" is our target set (labels).","71d33004":"**PCA AND PARAMETERS OPTIMISATION PIPELINED**\n\nI will chain PCA and classification model with a pipeline to perform a grid search optimisation. In the cell below I will use Support Vector Machine Classifier (SVC). ","4f62bf64":"**DATA STANDARISATION**\n\nI am going to use the Support Vector Machine Classifier. To work properly it requires standarisation of data.","fd8d4513":"I will drop columns with a significant amount of missing data.","57891ca7":"Remaining ones will be filledwith a mean.","a7952d65":"**DEALING WITH MISSING DATA**"}}