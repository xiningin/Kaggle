{"cell_type":{"7c7b1783":"code","5c574089":"code","b5c58d0b":"code","9e606f0b":"code","c947e950":"code","6c807aec":"code","fac77e34":"code","bca4c811":"code","0f27f964":"code","11a7c734":"code","77fa7d17":"code","2af0329c":"code","34945abf":"code","d51c4ad1":"code","b720ee0d":"code","2105f9bf":"code","af755565":"code","10db06a1":"code","08698806":"code","1f057435":"code","93af03d6":"code","d41e7bf6":"code","e4f1595f":"code","dc27e695":"code","872319a8":"code","f9b06111":"code","2d773cd5":"code","2fe9a477":"code","cf5659a7":"code","88e2f257":"code","7c585cf8":"code","4f22c236":"code","3198a192":"code","22c799e5":"code","d91213e5":"code","6e96de55":"code","b96587b1":"code","7e43f829":"code","799beb93":"code","9f71db1b":"code","cbd5a484":"code","0e7143fc":"code","b39a027a":"code","14e320a1":"code","aae0659a":"markdown","10becd3e":"markdown","2203a5da":"markdown","62160a50":"markdown","ea5d4fac":"markdown","dae9ec6a":"markdown","ad82a801":"markdown","7a884071":"markdown","09bdd83a":"markdown","1f41c0e0":"markdown","f323c342":"markdown","55bf48b9":"markdown","f5dad8d4":"markdown","d814810a":"markdown","084706c4":"markdown","b540b06c":"markdown"},"source":{"7c7b1783":"import pandas as pd\nimport json\nimport matplotlib.pyplot as plt\n\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.decomposition import NMF\n\nimport gensim\nimport pyLDAvis\nimport pyLDAvis.gensim_models as gensimvis","5c574089":"nbReviews = 50000\nchunks = pd.read_json('..\/input\/yelp-dataset\/yelp_academic_dataset_review.json', lines=True, chunksize = nbReviews)","b5c58d0b":"for chunk in chunks:\n    df = chunk\n    break","9e606f0b":"df.head(5)","c947e950":"df_badreviews = df.loc[df.stars < 2]\ndf_badreviews.shape","6c807aec":"texts = []\nfor review in df_badreviews.text:\n    texts.append(review)","fac77e34":"def tokenize(texts):\n    tokenizer = nltk.RegexpTokenizer(r'\\w+')\n\n    texts_tokens = []\n    for i, val in enumerate(texts):\n        text_tokens = tokenizer.tokenize(val.lower())\n\n        for i in range(len(text_tokens) - 1, -1, -1):\n            if len(text_tokens[i]) < 4:\n                del(text_tokens[i])\n\n        texts_tokens.append(text_tokens)\n        \n    return texts_tokens","bca4c811":"%%time\n\ntexts_tokens = tokenize(texts)\n\ntexts_tokens[:1]","0f27f964":"def removeSW(texts_tokens):\n    stopWords = set(stopwords.words('english'))\n    texts_filtered = []\n\n    for i, val in enumerate(texts_tokens):\n        text_filtered = []\n        for w in val:\n            if w not in stopWords:\n                text_filtered.append(w)\n        texts_filtered.append(text_filtered)\n        \n    return texts_filtered","11a7c734":"%%time\ntexts_filtered = removeSW(texts_tokens)\n\n\ntexts_filtered[:1]","77fa7d17":"len(texts_filtered)","2af0329c":"def lemma(texts_filtered):\n    wordnet_lemmatizer = WordNetLemmatizer()\n    texts_lem = []\n\n    for i, val in enumerate(texts_filtered):\n        text_lem = []\n        for word in val:\n            text_lem.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n        texts_lem.append(text_lem)\n    \n    return texts_lem","34945abf":"%%time\n\ntexts_lem = lemma(texts_filtered)\n\ntexts_lem[:1]","d51c4ad1":"len(texts_lem)","b720ee0d":"texts_string = []\nfor text in texts_lem:\n    string = ' '\n    string = string.join(text)\n    texts_string.append(string)","2105f9bf":"texts_string[:10]","af755565":"def plot_top_words(model, feature_names, n_top_words, title):\n    #Modified from SKlearn\n    fig, axes = plt.subplots(2, 5, figsize=(15, 7))\n    axes = axes.flatten()\n    for topic_idx, topic in enumerate(model.components_):\n        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n        top_features = [feature_names[i] for i in top_features_ind]\n        weights = topic[top_features_ind]\n\n        ax = axes[topic_idx]\n        ax.barh(top_features, weights, height=0.7)\n        ax.set_title(f'Topic {topic_idx +1}',\n                     fontdict={'fontsize': 15})\n        ax.invert_yaxis()\n        ax.tick_params(axis='both', which='major', labelsize=20)\n        for i in 'top right left'.split():\n            ax.spines[i].set_visible(False)\n        fig.suptitle(title, fontsize=15)\n        ax.tick_params(bottom=False)\n        ax.set(xticklabels=[])\n\n    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n    plt.show()\n","10db06a1":"%%time\n\nvectorizer = CountVectorizer(max_df=0.90, min_df=5)\nX = vectorizer.fit_transform(texts_string)\nfeature_names =  vectorizer.get_feature_names()\n\nX.toarray().shape","08698806":"%%time\nn_topics = 10\n\nlda = LatentDirichletAllocation(\n        n_components=n_topics,\n        max_iter=12\n)\n\nlda.fit_transform(X)","1f057435":"lda.n_iter_","93af03d6":"def display_topics(model, feature_names, no_top_words):\n    for topic_idx, topic in enumerate(model.components_):\n        print(\"Topic {}:\".format(topic_idx))\n        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n\nno_top_words = 10\ndisplay_topics(lda, feature_names, no_top_words)\n","d41e7bf6":"plot_top_words(lda, feature_names, no_top_words,'Topics in LDa')","e4f1595f":"%%time\ntfidf_vectorizer = TfidfVectorizer(max_df=0.90, \n    min_df=5,  \n    stop_words='english')\ntfidf = tfidf_vectorizer.fit_transform(texts_string)\ntfidf_feature_names = tfidf_vectorizer.get_feature_names()","dc27e695":"%%time\nno_topics = 10\n\n# Run NMF\nnmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd')\nnmf.fit_transform(tfidf)","872319a8":"no_top_words = 10\ndisplay_topics(nmf, tfidf_feature_names, no_top_words)","f9b06111":"plot_top_words(nmf, tfidf_feature_names, no_top_words,'Topics in NMF')","2d773cd5":"dictionary = gensim.corpora.Dictionary(texts_lem)\nlen(dictionary.cfs)","2fe9a477":"dictionary.filter_extremes(no_below=15, no_above=0.5)\nlen(dictionary.cfs)","cf5659a7":"bow_corpus = [dictionary.doc2bow(doc) for doc in texts_lem]","88e2f257":"print('Number of unique tokens: %d' % len(dictionary))\nprint('Number of documents: %d' % len(bow_corpus))","7c585cf8":"tfidf = gensim.models.TfidfModel(bow_corpus)\ncorpus_tfidf = tfidf[bow_corpus]","4f22c236":"%%time\nlda_model_coherence = []\nfor i in range (2,15):\n    lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=i, id2word=dictionary, passes=2, workers=4)\n    cm = gensim.models.CoherenceModel(model=lda_model, corpus=bow_corpus, coherence='u_mass')\n    coherence = cm.get_coherence()\n    lda_model_coherence.append(coherence)","3198a192":"plt.plot(range(2, 15),lda_model_coherence)\nplt.xlabel('Number of topics')\nplt.ylabel('Coherence score')\nplt.title('How many topics ? (Closer to 0 = better)')\nplt.show()","22c799e5":"lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=8, id2word=dictionary, passes=2, workers=4)","d91213e5":"for idx, topic in lda_model.print_topics(-1):\n    print('Topic: {} Word: {}'.format(idx, topic))","6e96de55":"vis = gensimvis.prepare(topic_model=lda_model, corpus=bow_corpus, dictionary=dictionary)\npyLDAvis.enable_notebook()\npyLDAvis.display(vis)","b96587b1":"%%time\nlda_model_tfidf_coherence = []\nfor i in range (2,15):\n    lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=i, id2word=dictionary, passes=2, workers=4)\n    cm = gensim.models.CoherenceModel(model=lda_model_tfidf, corpus=bow_corpus, coherence='u_mass')\n    coherence = cm.get_coherence()\n    lda_model_tfidf_coherence.append(coherence)","7e43f829":"plt.plot(range(2, 15),lda_model_coherence)\nplt.xlabel('Number of topics')\nplt.ylabel('Coherence score')\nplt.title('How many topics ? (Closer to 0 = better)')\nplt.show()","799beb93":"lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=8, id2word=dictionary, passes=2, workers=4)\nfor idx, topic in lda_model_tfidf.print_topics(-1):\n    print('Topic: {} Word: {}'.format(idx, topic))","9f71db1b":"vis = gensimvis.prepare(topic_model=lda_model_tfidf, corpus=corpus_tfidf, dictionary=dictionary)\npyLDAvis.enable_notebook()\npyLDAvis.display(vis)","cbd5a484":"for i in range(0,3):\n    print('### Scoring the document', i)\n    for index, score in sorted(lda_model[bow_corpus[i]], key=lambda tup: -1*tup[1]):\n        print(\"Score: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 5)), '\\n')","0e7143fc":"for i in range(0,3):\n    print('### Scoring the document', i)\n    for index, score in sorted(lda_model_tfidf[bow_corpus[i]], key=lambda tup: -1*tup[1]):\n        print(\"Score: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 5)), '\\n')","b39a027a":"def preprocess(raw_text):\n    x = tokenize(raw_text)\n    x = removeSW(x)\n    x = lemma(x)\n    return x","14e320a1":"unseen_document = ['I had to wait 2 hours. It was so long. I will never come back here.', 'Food was horrible. My pizza was burn and was late.', 'It tasted bad. It is not good quality. Who is cooking here ?']\npreprocessed_doc = preprocess(unseen_document)\nbow_corpus = [dictionary.doc2bow(doc) for doc in preprocessed_doc]\n\nprint('There is', len(preprocessed_doc), 'reviews in the unseen document. We are going to calculate the score for the best topic of each of them.\\n---------------\\n')\n\nfor i in range(0, len(preprocessed_doc)):\n    for index, score in sorted(lda_model[bow_corpus[i]], key=lambda tup: -1*tup[1]):\n        print(\"# Review\", i, \": best score: {}\\t For topic: {}\".format(score, lda_model.print_topic(index, 5)), '\\n')\n        break","aae0659a":"### LDA using TF-IDF","10becd3e":"Displaying result of the analysis.","2203a5da":"### Joinning","62160a50":"## LDA analysis with Sklearn\nVectorizing texts with Count Vectorizer. Filtering too rate or too common words.","ea5d4fac":"## LDA BoW and LDA TF-IDF with gensim\nGenerating dictionnary, filtering it. Generating bag-of-words. Generating TF-IDF.","dae9ec6a":"## New reviews testing","ad82a801":"## NMF analysis with Sklearn\nVectorizing the texts using TF IDF Vectorizer. Filtering too rate or too common words.","7a884071":"Displaying result of the NMF","09bdd83a":"### Cleaning techniques #3 : Lemma","1f41c0e0":"## Loading reviews","f323c342":"## Scoring topics on documents\nAnalysing topics's score on 3 documents for both methods.","55bf48b9":"### LDA using BoW","f5dad8d4":"Fitting LDA model.","d814810a":"Fitting NMF model","084706c4":"### Cleaning techniques #2 : Removing stopwords","b540b06c":"### Cleaning techniques #1 : Tokenization"}}