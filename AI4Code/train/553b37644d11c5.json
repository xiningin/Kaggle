{"cell_type":{"04751767":"code","8af6da12":"code","12161480":"code","6c17a543":"code","73937cd2":"code","1c374029":"code","adacff7b":"code","cd8ea124":"code","2185a06b":"code","3ce781fc":"code","359cff3c":"code","de7f36e3":"code","c12566c2":"code","a8603f30":"code","5c47b0ae":"code","b4a62935":"code","22ffb277":"code","b41e5b8c":"code","159a8838":"code","7486b222":"code","1df0a006":"code","7884a45e":"code","23da56e7":"code","c1e7678f":"code","4f18d7ce":"code","3555450d":"code","219a5015":"code","4ee43cf0":"code","9a1f6b55":"code","272bd07a":"code","b01877f1":"code","9c0412ec":"code","3302189a":"code","7985e65f":"markdown","eb99587d":"markdown","6c8dcd5e":"markdown","51b57bd5":"markdown","c9b39f9f":"markdown","0b0eafa4":"markdown","4d9416a5":"markdown","b77d8d5f":"markdown","64f605fc":"markdown"},"source":{"04751767":"import numpy as np # numeric operations\nimport pandas as pd # data manipulation \nimport matplotlib.pyplot as plt # visuals\nimport tensorflow as tf # tensorflow\n\nfrom tensorflow.keras.losses import sparse_categorical_crossentropy # loss function\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import Embedding, GRU, Dense, Dropout","8af6da12":"# read the data  \npath = '..\/input\/shakespeare\/shakespeare.txt'\ntext = open(path, 'r').read()","12161480":"print(text[:500])","6c17a543":"print(text[80000:81000])","73937cd2":"# unique characters\nvocab = sorted(set(text))\nvocab","1c374029":"# length of unique characters\nlen(vocab)","adacff7b":"# assign number for each unique character\nfor pair in enumerate(vocab):\n    print(pair)","cd8ea124":"# assign number for each unique character\nchar_to_ind = {char:ind for ind, char in enumerate(vocab)}\nchar_to_ind","2185a06b":"# the index of the letter H\nchar_to_ind['H']","3ce781fc":"# going from the character to the index \nind_to_char = np.array(vocab)\nind_to_char","359cff3c":"# the character at the index 33\nind_to_char[33]","de7f36e3":"# encode all text to integers \nencoded_text = np.array([char_to_ind[c] for c in text])","c12566c2":"# check the result \nencoded_text[:500]","a8603f30":"# number of characters in our array \nencoded_text.shape","5c47b0ae":"# number of characters to consider before predicting the next character\nseq_len = 120","b4a62935":"# total number of sequences in our text \ntotal_num_seq = len(text) \/\/ (seq_len+1)\ntotal_num_seq","22ffb277":"# create the training sequences \nchar_dataset = tf.data.Dataset.from_tensor_slices(encoded_text); ","b41e5b8c":"# look at the generated dataset \nfor item in char_dataset.take(500):\n    print(item.numpy())","159a8838":"# create sequences\nsequences = char_dataset.batch(seq_len+1, drop_remainder=True)","7486b222":"# generate the same sequence shifted over one time stip \ndef create_seq_targets(seq):\n    input_txt = seq[:-1] # Hello my nam\n    target_txt = seq[1:] # ello my name \n    return input_txt, target_txt","1df0a006":"# map tha fuction to the sequences\ndataset = sequences.map(create_seq_targets) ","7884a45e":"# have a look at what we have created so far \n# show how the very first batch look like  \nfor input_txt, target_txt in dataset.take(1):\n    print(input_txt.numpy())\n    print(\"\".join(ind_to_char[input_txt.numpy()]))\n    print('\\n')\n    print(target_txt.numpy())\n    print(\"\".join(ind_to_char[target_txt.numpy()]));","23da56e7":"batch_size = 128 # number of sequences to be fed to the model at a time \nbuffer_size = 10000 # to aviod shuffling the whole dataset at once\ndataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)","c1e7678f":"dataset","4f18d7ce":"vocab_size = len(vocab) # 84\nembed_dim = 64\nrnn_neurons = 1026","3555450d":"# Customized loss function \ndef sparse_cat_loss(y_true, y_pred):\n    return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)","219a5015":"# create the model\ndef create_model(vocab_size, embed_dim, rnn_neurons, batch_size):\n    model = Sequential()\n    model.add(Embedding(vocab_size, embed_dim, batch_input_shape = [batch_size, None]))\n    model.add(GRU(rnn_neurons, return_sequences = True, stateful = True, recurrent_initializer = 'glorot_uniform'))\n    model.add(Dense(vocab_size))\n    \n    model.compile('adam', loss = sparse_cat_loss)\n    return model","4ee43cf0":"# create the model\nmodel = create_model(vocab_size = vocab_size,\n                     embed_dim = embed_dim,\n                     rnn_neurons = rnn_neurons,\n                     batch_size = batch_size)","9a1f6b55":"model.summary()","272bd07a":"# fit the model\nepochs = 2\nmodel.fit(dataset, epochs = epochs);","b01877f1":"# load pre-trained weights  \nmodel = create_model(vocab_size, embed_dim, rnn_neurons, batch_size=1)\nmodel.load_weights('..\/input\/shakespeare\/shakespeare_gen.h5')\nmodel.build(tf.TensorShape([1, None]))","9c0412ec":"def generate_text(model, start_seed,gen_size=100,temp=1.0):\n    '''\n    model: Trained Model to Generate Text\n    start_seed: Intial Seed text in string form\n    gen_size: Number of characters to generate\n\n    Basic idea behind this function is to take in some seed text, format it so\n    that it is in the correct shape for our network, then loop the sequence as\n    we keep adding our own predicted characters. Similar to our work in the RNN\n    time series problems.\n    '''\n\n    # Number of characters to generate\n    num_generate = gen_size\n\n    # Vecotrizing starting seed text\n    input_eval = [char_to_ind[s] for s in start_seed]\n\n    # Expand to match batch format shape\n    input_eval = tf.expand_dims(input_eval, 0)\n\n    # Empty list to hold resulting generated text\n    text_generated = []\n\n    # Temperature effects randomness in our resulting text\n    # The term is derived from entropy\/thermodynamics.\n    # The temperature is used to effect probability of next characters.\n    # Higher probability == lesss surprising\/ more expected\n    # Lower temperature == more surprising \/ less expected\n\n    temperature = temp\n\n    # Here batch size == 1\n    model.reset_states()\n\n    for i in range(num_generate):\n\n        # Generate Predictions\n        predictions = model(input_eval)\n\n        # Remove the batch shape dimension\n        predictions = tf.squeeze(predictions, 0)\n\n        # Use a cateogircal disitribution to select the next character\n        predictions = predictions \/ temperature\n        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n\n        # Pass the predicted charracter for the next input\n        input_eval = tf.expand_dims([predicted_id], 0)\n\n        # Transform back to character letter\n        text_generated.append(ind_to_char[predicted_id])\n\n    return (start_seed + ''.join(text_generated))","3302189a":"# generate predictions \nprint(generate_text(model,\"year\",gen_size=1000))","7985e65f":"# Read and Understand the Data ","eb99587d":"# Text Processing - Encoding","6c8dcd5e":"# Imports ","51b57bd5":"# Creating Batches ","c9b39f9f":"# Creating the Model","0b0eafa4":"> Well done! see you in the next project","4d9416a5":"> buffer_size is used to avoid shuffling the whole dataset at once which is very memory intensive","b77d8d5f":"> The batch method arranges the consecutive elements of this data in batches that we can then feed into the model later on","64f605fc":"> We will be feeding the model 128 sequences at a time. Each sequence is 120 characters long. "}}