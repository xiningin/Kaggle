{"cell_type":{"bd7a4999":"code","a66e3d91":"code","18672777":"code","a069aeb3":"code","bb33e802":"code","3f08b84c":"code","e7538d8e":"code","07e14b79":"code","c8be642d":"code","c8096d74":"code","eaaae260":"code","f537e09c":"code","76ddd8bd":"code","3833631c":"code","f110c5ad":"code","dfed117f":"code","de437403":"code","36bf5870":"code","865bbf6c":"code","277fb1ba":"markdown","a7f3e174":"markdown","4bab2352":"markdown","2b426d33":"markdown","2fd6e332":"markdown","be64351a":"markdown","1416011c":"markdown","7b0e33cd":"markdown","abb5102f":"markdown","3b7373b4":"markdown","3b5847a6":"markdown","64ed5363":"markdown","35a6f95b":"markdown","0c9c4d6d":"markdown","475a19b2":"markdown","f1e48844":"markdown","084c5164":"markdown","de20eb63":"markdown","690bf0ea":"markdown","ac8d0b00":"markdown"},"source":{"bd7a4999":"import numpy as np\nimport pandas as pd ","a66e3d91":"X_train = pd.read_csv(\"..\/input\/saas-2021-spring-cx-kaggle-compeition\/train_features.csv\")\nX_test = pd.read_csv(\"..\/input\/saas-2021-spring-cx-kaggle-compeition\/test_features.csv\")\ny_train = pd.read_csv(\"..\/input\/saas-2021-spring-cx-kaggle-compeition\/train_targets.csv\")\ndel y_train['Id']\nsample_submission = pd.read_csv(\"..\/input\/saas-2021-spring-cx-kaggle-compeition\/sample_submission.csv\")","18672777":"#as mentioned in lecture, when doing feature engineering, it's better to merge train and test datasets first and do operations on the entire merged dataset\nfull_data = pd.concat([X_train, X_test]).reset_index(drop=True)","a069aeb3":"#checking if the merged dataset has the correct number of rows\nassert full_data.shape[0] == X_train.shape[0]+X_test.shape[0]","bb33e802":"full_data.columns","3f08b84c":"# Your Code Here","e7538d8e":"# original full_data has 22 columns, after you drop some columns, the below expression will return a number less than 23\nlen(full_data.columns)","07e14b79":"# below expression shows the number of missing values in each column\nfull_data.isna().sum()","c8be642d":"# Your code here","c8096d74":"# Your code here","eaaae260":"# optional\nfrom sklearn.decomposition import PCA\n# Your code here","f537e09c":"# Splitting up our engineered df back into training and test\nX_train = full_data[:X_train.shape[0]]\nX_test = full_data[X_train.shape[0]:]","76ddd8bd":"X_train.shape","3833631c":"X_test.shape","f110c5ad":"from sklearn.metrics import mean_absolute_error\ndef evaluate(y_pred, y_true):\n    \"\"\"Returns the MAE(y_pred, y_true)\"\"\"\n    return mean_absolute_error(y_true, y_pred)","dfed117f":"# Build a simple random forest model here\n# Don't worry if you don't how a random forest works. We will cover that in lecture. \n# The below code serves as a demonstration of how you generally create a model\nfrom sklearn.ensemble import RandomForestClassifier\n# Instantiate a model \nclf = RandomForestClassifier(max_depth=1, random_state=0, verbose=1)\n# Train the model using our train_features and train_targets\nclf.fit(X_train.to_numpy(), np.ravel(y_train.to_numpy()))\n# Use the trained model to predict y_train!\npredictions = clf.predict(X_test)","de437403":"assert predictions.shape[0] == 25124","36bf5870":"sample_submission['RainTomorrow'] = predictions","865bbf6c":"sample_submission.to_csv(\"submission.csv\", index=False)","277fb1ba":"In our Visualization + Data Cleaning HW, you have explored and got familiar the dataset. Now is the time to do some feature engineering! ","a7f3e174":"### Fill in the Missing Values\nIn Visualization + Data Cleaning HW, we have explored techniques of dealing with missing values. Perform the same techniques on all columns of full_data that contain missing values. ","4bab2352":"## Cross Validation\n![cross-validation-graphic](https:\/\/i.stack.imgur.com\/1fXzJ.png)","2b426d33":"## Ensemble","2fd6e332":"### One Hot Encoding\nOne-hot encode categorical columns","be64351a":"### Feel Free to do more feature engineering on df! All the methods listed above are ones to help you get started.","1416011c":"### Optional: Dimensionality Reduction\nWhen the data has high dimensions(a lot of columns), it is very useful to use PCA to lower the dimension of the data during feature engineering. \nSince we only have around 20 features, this is not necessary. But it could potentially help with your kaggle score.\nPS: PCA is designed for continuous variables, so maybe you should try ignore categorical columns for PCA.\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.PCA.html","7b0e33cd":"### Note: submission.csv can be found within the \/kaggle\/working file on the right side of your screen. Download the csv file and use it for submission!","abb5102f":"The above shows all the column names of the training dataset. Do you think all columns are useful in doing rain prediction? For instance, is knowing the date really going to help us?","3b7373b4":"## Loading Data","3b5847a6":"# Career Exploration Kaggle Competition: Rain in Australia Prediction\n\n### Hosted by and maintained by the [Students Association of Applied Statistics (SAAS)](https:\/\/saas.berkeley.edu).  Authored by Derek Cai(dcai@berkeley.edu).","64ed5363":"Create another model that scores decently well and create an ensemble of these 2 models. You can simply average the output predictions of these 2 models or take a weighted average of the output predictions of of these 2 models. Screenshot the code of your second model as well as the final ensemble process to Derek. Your submission should generally be better than your single submission.","35a6f95b":"Our Kaggle competition uses MAE(Mean Absolute Error) as our metric!","0c9c4d6d":"## Feature Engineering","475a19b2":"Task 1:\nIn Lecture, we have discussed the cross-validtion scheme. Set up your cross validation below. Perform a 5-fold cross-validation on the rain dataset below. You should use 20% of your training data as your validation data. Print out the accuracy of each of your 5 experiment! Feel free to keep the random forest model or use any models of your choosing. DM the screenshot of your code and the printed out 5 experiment accuracy to Derek! ","f1e48844":"## Import Libraries","084c5164":"### Dropping Features\nNot all features are useful. Drop some features of full_data if you want!\nResource: https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.drop.html","de20eb63":"## Baseline Model\nCongrats! You have feature engineered the train and test dataset such that they can be used to build models!","690bf0ea":"## Submission","ac8d0b00":"Task2: For people scoring above 0.17 on the leaderboard, beat the score 0.17! For people scoring below 0.17, beat your current score! "}}