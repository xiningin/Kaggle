{"cell_type":{"afcc0c75":"code","83bb9b61":"code","20427fb8":"code","3eec7316":"code","d89fe1ec":"code","4c5b7728":"code","4bc55570":"code","9afe4be4":"code","d69997c2":"code","0a509653":"code","f4fda801":"markdown"},"source":{"afcc0c75":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport os\nprint(os.listdir(\"..\/input\"))\nfrom matplotlib import pyplot as plt\nimport cv2\nfrom PIL import Image\nimport random\nfrom imgaug import augmenters as iaa\n\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential, load_model,Model\nfrom keras.layers import Activation,Dropout,Flatten,Dense,Input,BatchNormalization,Conv2D\nfrom keras.applications.inception_resnet_v2 import preprocess_input\nfrom keras.applications import InceptionResNetV2\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.callbacks import LambdaCallback\nfrom keras.callbacks import Callback\nfrom keras import metrics\nfrom keras.optimizers import Adam \nfrom keras import backend as K\nimport tensorflow as tf\nimport keras\n","83bb9b61":"df = pd.read_csv(\"..\/input\/train.csv\")\nprint(\"Total number of unique ids:\",df.Id.count())\nprint(\"Total number of images:\", df.Id.count()*4)","20427fb8":"df.head(2)","3eec7316":"path_to_train = '\/kaggle\/input\/train\/'\ndata = pd.read_csv('\/kaggle\/input\/train.csv')\n\ntrain_dataset_info = []\nfor name, labels in zip(data['Id'], data['Target'].str.split(' ')):\n    train_dataset_info.append({\n        'path':os.path.join(path_to_train, name),\n        'labels':np.array([int(label) for label in labels])})\ntrain_dataset_info = np.array(train_dataset_info)\n\nfrom sklearn.model_selection import train_test_split\ntrain_ids, test_ids, train_targets, test_target = train_test_split(\n    data['Id'], data['Target'], test_size=0.2, random_state=42)","d89fe1ec":"class data_generator:\n    \n    def create_train(dataset_info, batch_size, shape, augument=True):\n        while True:\n            random_indexes = np.random.choice(len(dataset_info), batch_size)\n            batch_images = np.empty((batch_size, shape[0], shape[1], shape[2]))\n            batch_labels = np.zeros((batch_size, 28))\n            for i, idx in enumerate(random_indexes):\n                image = data_generator.load_image(\n                    dataset_info[idx]['path'], shape)   \n                if augument:\n                    image = data_generator.augment(image)\n                batch_images[i] = image\n                batch_labels[i][dataset_info[idx]['labels']] = 1\n            yield batch_images, batch_labels\n    \n    def load_image(path, shape):\n        R = np.array(Image.open(path+'_red.png'))\n        G = np.array(Image.open(path+'_green.png'))\n        B = np.array(Image.open(path+'_blue.png'))\n        Y = np.array(Image.open(path+'_yellow.png'))\n\n        image = np.stack((\n            R,\n            G, \n            (B+Y)\/2),-1)\n\n        image = cv2.resize(image, (shape[0], shape[1]))\n        image = np.divide(image, 255)\n        return image        \n    \n    def augment(image):\n        augment_img = iaa.Sequential([\n            iaa.OneOf([\n                iaa.Affine(rotate=0),\n                iaa.Affine(rotate=90),\n                iaa.Affine(rotate=180),\n                iaa.Affine(rotate=270),\n                iaa.Fliplr(0.5),\n                iaa.Flipud(0.5),\n            ])], random_order=True)\n        \n        image_aug = augment_img.augment_image(image)\n        return image_aug","4c5b7728":"input_shape=(299,299,3)\ntrain_datagen = data_generator.create_train(\n    train_dataset_info, 5, input_shape, augument=True)\n\nimages, labels = next(train_datagen)\n\nfig, ax = plt.subplots(1,5,figsize=(25,5))\nfor i in range(5):\n    ax[i].imshow(images[i])\nprint('min: {0}, max: {1}'.format(images.min(), images.max()))","4bc55570":"def create_model(input_shape, n_out):\n    input_tensor = Input(shape=input_shape)\n    base_model = InceptionResNetV2(include_top=False,\n                   weights='imagenet',\n                   input_shape=input_shape)\n    bn = BatchNormalization()(input_tensor)\n    x = base_model(bn)\n    x = Conv2D(32, kernel_size=(1,1), activation='relu')(x)\n    x = Flatten()(x)\n    x = Dropout(0.5)(x)\n    x = Dense(1024, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    output = Dense(n_out, activation='sigmoid')(x)\n    model = Model(input_tensor, output)\n    \n    return model","9afe4be4":"def f1(y_true, y_pred):\n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n    p = tp \/ (tp + fp + K.epsilon())\n    r = tp \/ (tp + fn + K.epsilon())\n    f1 = 2*p*r \/ (p+r+K.epsilon())\n    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n    return K.mean(f1)\ndef show_history(history):\n    fig, ax = plt.subplots(1, 3, figsize=(15,5))\n    ax[0].set_title('loss')\n    ax[0].plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n    ax[0].plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\n    ax[1].set_title('f1')\n    ax[1].plot(history.epoch, history.history[\"f1\"], label=\"Train f1\")\n    ax[1].plot(history.epoch, history.history[\"val_f1\"], label=\"Validation f1\")\n    ax[2].set_title('acc')\n    ax[2].plot(history.epoch, history.history[\"acc\"], label=\"Train acc\")\n    ax[2].plot(history.epoch, history.history[\"val_acc\"], label=\"Validation acc\")\n    ax[0].legend()\n    ax[1].legend()\n    ax[2].legend()","d69997c2":"model = create_model(\n    input_shape, \n    n_out=28)\n\ncheckpointer = ModelCheckpoint(\n    '\/kaggle\/working\/InceptionResNetV2.model',\n    verbose=2, save_best_only=True)\n\nBATCH_SIZE = 10\nINPUT_SHAPE = (299,299,3)\n\ntrain_generator = data_generator.create_train(\n    train_dataset_info[train_ids.index], BATCH_SIZE, INPUT_SHAPE, augument=False)\nvalidation_generator = data_generator.create_train(\n    train_dataset_info[test_ids.index], 256, INPUT_SHAPE, augument=False)\n\nmodel.layers[0].trainable = True\n\nmodel.compile(\n    loss='binary_crossentropy',  \n    optimizer=Adam(1e-4),\n    metrics=['acc', f1])\n\nhistory = model.fit_generator(\n    train_generator,\n    steps_per_epoch=500,\n    validation_data=next(validation_generator),\n    epochs=35, \n    verbose=1,\n    callbacks=[checkpointer])\nshow_history(history)","0a509653":"from tqdm import tqdm\nsubmit = pd.read_csv('..\/input\/sample_submission.csv')\npredicted = []\nfor name in tqdm(submit['Id']):\n    path = os.path.join('..\/input\/test\/', name)\n    image = data_generator.load_image(path, INPUT_SHAPE)\n    score_predict = model.predict(image[np.newaxis])[0]\n    label_predict = np.arange(28)[score_predict>=0.1]\n    str_predict_label = ' '.join(str(l) for l in label_predict)\n    predicted.append(str_predict_label)\n    \nsubmit['Predicted'] = predicted\nsubmit.to_csv('submission.csv', index=False)","f4fda801":"Let's train a classifier for baseline. I'm choosing InceptionResnet50 but another interesting candidate is NasNet."}}