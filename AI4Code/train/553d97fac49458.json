{"cell_type":{"da4277a8":"code","cf057351":"code","baec825b":"code","ff096951":"code","9d31a66d":"code","757ed798":"code","8ba78e19":"code","3310e6ab":"code","bb247cad":"code","4aa0bbda":"code","6aeef15f":"code","4def727f":"code","1cd2f73e":"code","a25e6136":"code","960f5d7b":"code","c9712e91":"code","8444a11f":"markdown","ae513124":"markdown","ac697434":"markdown","f116036a":"markdown","e6c319e0":"markdown","ceea0ec3":"markdown","41e5b342":"markdown","736f3b19":"markdown","18223a9c":"markdown"},"source":{"da4277a8":"import os\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.figure_factory as ff\nimport plotly.graph_objects as go\nfrom matplotlib.ticker import FormatStrFormatter\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","cf057351":"train = pd.read_csv(r'\/kaggle\/input\/tabular-playground-series-nov-2021\/train.csv', index_col='id')\ntest = pd.read_csv(r'\/kaggle\/input\/tabular-playground-series-nov-2021\/test.csv', index_col='id')\nsubmission= pd.read_csv(r'\/kaggle\/input\/tabular-playground-series-nov-2021\/sample_submission.csv', index_col='id')","baec825b":"print('shape')\nprint(train.shape)\nprint(test.shape)\n\nprint('Nullvalues')\ndisplay(train.isna().sum().sum())\ndisplay(test.isna().sum().sum())","ff096951":"target = train['target']","9d31a66d":"pal = ['#6495ED','#ff355d']\nplt.figure(figsize=(8, 6))\nax = sns.countplot(x=target, palette=pal)\nax.set_title('Target variable distribution', fontsize=20, y=1.05)\n\nsns.despine(right=True)\nsns.despine(offset=10, trim=True)","757ed798":"train_ = train.sample(10000, random_state=1121)\ntest_ = test.sample(5000, random_state=1121)\n\nfeatures = train.columns\nnum_features = features[:-1]","8ba78e19":"#Features which have peculiar distribution\nff = ['f0', 'f2',  'f4', 'f9', 'f12',  'f16', 'f19', 'f20', 'f23', 'f24',  'f27', 'f28',  'f30','f31', 'f32', 'f33', 'f35', 'f36', 'f39', \n'f42',  'f44', 'f46', 'f48', 'f49', 'f51', 'f52', 'f53', 'f56', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f68', 'f69', \n'f72', 'f73', 'f75', 'f76', 'f78', 'f79', 'f81', 'f83', 'f84',  'f87', 'f88', 'f89', 'f90', 'f92', 'f93', 'f94', 'f95', 'f98', 'f99']","3310e6ab":"def density_plotter(a, b, title):    \n    L = len(num_features[a:b])\n    nrow= int(np.ceil(L\/10))\n    ncol= 10\n    fig, ax = plt.subplots(nrow, ncol,figsize=(24, 12), sharey=False, facecolor='#dddddd')\n\n    fig.subplots_adjust(top=0.90)\n    i = 1\n    for feature in num_features[a:b]:\n        plt.subplot(nrow, ncol, i)\n        ax = sns.kdeplot(train_[feature], shade=True,  color='#6495ED',  alpha=0.85, label='train')\n        ax = sns.kdeplot(test_[feature], shade=True, color='#ff355d',  alpha=0.85, label='test')\n        ax.yaxis.set_major_formatter(FormatStrFormatter('%.0f'))\n        ax.xaxis.set_label_position('top')\n        ax.set_ylabel('')\n        ax.set_yticks([])        \n        ax.set_xticks([])\n        \n        if feature in ff:\n            ax = sns.kdeplot(train_[feature], shade=True,  color='black',  alpha=0.85, label='train')\n            ax = sns.kdeplot(test_[feature], shade=True, color='gold',  alpha=0.85, label='test')\n            ax.set_facecolor('#dddddd')\n        \n        i += 1\n\n    lines, labels = fig.axes[-1].get_legend_handles_labels()    \n    fig.legend(lines, labels, loc = 'upper center',borderaxespad= 4.0) \n\n    plt.suptitle(title, fontsize=20)\n    plt.show()","bb247cad":"density_plotter(a=0, b=50, title='Density plot: train & test data (f0 -f50)')","4aa0bbda":"density_plotter(a=50, b=100, title='Density plot: train & test data (f50 - f100)')","6aeef15f":"fig, ax = plt.subplots(1, 1, figsize=(16 , 16), facecolor='#dddddd')\ncorr = train.sample(600000, random_state=2021).corr()\n\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nsns.heatmap(corr, ax=ax, square=True, center=0, linewidth=1, vmax=0.2, vmin=-0.2,\n        cmap=sns.diverging_palette(240, 10, as_cmap=True),\n        cbar_kws={\"shrink\": .85}, mask=mask ) \n\nax.set_title('Correlation heatmap: Numerical features', fontsize=24, y= 1.05);\n#ax.set_facecolor(None);","4def727f":"## Code from kaggles starter notebook of TPS_september\n\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.model_selection import cross_validate\nimport warnings \nwarnings.filterwarnings('ignore')\n\ndata_dir = Path('..\/input\/tabular-playground-series-nov-2021\/')\n\ndf_train = pd.read_csv(\n    data_dir \/ \"train.csv\",\n    index_col='id',\n    #nrows=25000, \n)\n\nX_test = pd.read_csv(data_dir \/ \"test.csv\", index_col='id')\n\nFEATURES = df_train.columns[:-1]\nTARGET = df_train.columns[-1]\n\nX = df_train.loc[:, FEATURES]\ny = df_train.loc[:, TARGET]\n\nseed = 0\nfold = 5","1cd2f73e":"model_xgb = XGBClassifier(max_depth=3,\n    subsample=.85,\n    colsample_bytree=.1,\n    n_jobs=-1,\n    tree_method='gpu_hist',\n    sampling_method='gradient_based', \n    random_state= seed,\n)\ndef score(X, y, model_xgb, cv):\n    scoring = [\"roc_auc\"]\n    scores = cross_validate(\n        model_xgb, X, y, scoring=scoring, cv=cv, return_train_score=True\n    )\n    scores = pd.DataFrame(scores).T\n    return scores.assign(\n        mean = lambda x: x.mean(axis=1),\n        std = lambda x: x.std(axis=1),\n    )\n\nscores = score(X, y, model_xgb, cv=fold)\ndisplay(scores)","a25e6136":"model_lgbm = LGBMClassifier(\n    num_iterations=100,\n    objective = \"binary\",\n    feature_pre_filter = False,  \n    device_type = 'gpu',\n    )\ndef score(X, y, model_lgbm, cv):\n    scoring = [\"roc_auc\"]\n    scores = cross_validate(\n        model_lgbm, X, y, scoring=scoring, cv=cv, return_train_score=True\n    )\n    scores = pd.DataFrame(scores).T\n    return scores.assign(\n        mean = lambda x: x.mean(axis=1),\n        std = lambda x: x.std(axis=1),\n    )\n\nscores = score(X, y, model_lgbm, cv=fold)\ndisplay(scores)","960f5d7b":"model_xgb.fit(X, y, eval_metric='auc')\nX_test = pd.read_csv(data_dir \/ \"test.csv\", index_col='id')\n\ny_pred_xgb = pd.Series(\n    model_xgb.predict_proba(X_test)[:, 1],\n    index=X_test.index,\n    name=TARGET,\n)\ny_pred_xgb.to_csv(\"submission_xgb.csv\")","c9712e91":"y_pred_xgb","8444a11f":"## **5. Correlation Heatmap**","ae513124":"### 5.3: Submission","ac697434":"## **4. Features Distibution**","f116036a":"## **5. Base Models** \n\n### 5.1: xgboost","e6c319e0":"## **2. Dataset Overview**\n\n### Data size\n- Train data has 600000 rows and 101 features including the target variable\n- Test dataset has 540000 rows and 100 features.\n\n### Missing Values\n- No missing values in both train and test datasets!\n\n### Features\n- All features area numerical features.\n\n### Target\n- Binary target (1, 0)\n- Target distribution is balanced.","ceea0ec3":"## **3. Target Distribution**","41e5b342":"## Thank you for visiting this notebook! ","736f3b19":"## **1. Introduction**\n\nStarting from January this year, the kaggle competition team is offering a month-long tabulary playground competitions. This series aims to bridge between inclass competition and featured competitions with a friendly and approachable datasets.\n\nFor this competition, you will be predicting a binary target based on 100 feature columns given in the data. All columns are continuous.\n\nThe data is synthetically generated by a GAN that was trained on a real-world dataset used to identify spam emails via various extracted features from the email.\n\n*Files to work with*:\n\ntrain.csv - the training data with the target column\n\ntest.csv - the test set; you will be predicting the target for each row in this file (the probability of the binary target)\n\nsample_submission.csv - a sample submission file in the correct format\n\n*Evaluation*:\n\nSubmissions are evaluated on area under the ROC curve between the predicted probability and the observed target.","18223a9c":"### 5.2: Lgbm"}}