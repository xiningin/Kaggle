{"cell_type":{"86a52bad":"code","2b56d403":"code","201d4c46":"code","e999e6d4":"code","2d706401":"code","0112c8e2":"code","26e9aeec":"code","e9e8f631":"code","e474495e":"code","2e321385":"code","9376977f":"code","4c09d67c":"markdown","1344c89e":"markdown","e8643674":"markdown","5e7b109d":"markdown","32730be7":"markdown","bffb0035":"markdown","8a81f32e":"markdown"},"source":{"86a52bad":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","2b56d403":"train = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/test.csv')\nsub = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/sample_submission.csv')","201d4c46":"# load oof\noof1 = pd.read_csv(\"..\/input\/tps-jun2021-r-nn-residual-keras\/oof_nnresidual.csv\", index_col='id')\noof2 = pd.read_csv(\"..\/input\/tps-jun2021-r-wide-deep-keras\/oof_widedeep.csv\", index_col='id')\noof3 = pd.read_csv(\"..\/input\/tps-jun2021-catboost-optuna\/oof_cat_default.csv\", index_col='id')\noof4 = pd.read_csv(\"..\/input\/tps-jun2021-lightautoml\/oof_lightautoml.csv\", index_col='id')\noof5 = pd.read_csv(\"..\/input\/tps-jun2021-lightgbm-optuna\/oof_lgb_optuned.csv\", index_col='id')\noof6 = pd.read_csv(\"..\/input\/tps-jun2021-tabnetclassifier\/oof_tab_optuned.csv\", index_col='id')\noof7 = pd.read_csv(\"..\/input\/tps-jun2021-r-nn-gbt-blend-optim-keras\/oof_nn_gbt_blend.csv\", index_col='id')\noof8 = pd.read_csv(\"..\/input\/tps-jun2021-r-wide-deep-residual-optblend-keras\/oof_widedeep_res.csv\", index_col='id')\n\n# load submission\nsub1 = pd.read_csv(\"..\/input\/tps-jun2021-r-nn-residual-keras\/sub_nnresidual.csv\", index_col='id')\nsub2 = pd.read_csv(\"..\/input\/tps-jun2021-r-wide-deep-keras\/sub_widedeep.csv\", index_col='id')\nsub3 = pd.read_csv(\"..\/input\/tps-jun2021-catboost-optuna\/sub_cat_default.csv\", index_col='id')\nsub4 = pd.read_csv(\"..\/input\/tps-jun2021-lightautoml\/sub_lightautoml.csv\", index_col='id')\nsub5 = pd.read_csv(\"..\/input\/tps-jun2021-lightgbm-optuna\/sub_lgb_optuned.csv\", index_col='id')\nsub6 = pd.read_csv(\"..\/input\/tps-jun2021-tabnetclassifier\/sub_tab_default.csv\", index_col='id')\nsub7 = pd.read_csv(\"..\/input\/tps-jun2021-r-nn-gbt-blend-optim-keras\/sub_nn_gbt_blend.csv\", index_col='id')\nsub8 = pd.read_csv(\"..\/input\/tps-jun2021-r-wide-deep-residual-optblend-keras\/sub_widedeep_res.csv\", index_col='id')\n\n# External subs\nsub1_ext = pd.read_csv(\"..\/input\/3-tps-jun-21-smart-ensembling-for-classifier\/submission_ens.csv\", index_col='id')\nsub2_ext = pd.read_csv(\"..\/input\/tps-6-21-new-ensemble\/blend_ens.csv\", index_col='id')\nsub3_ext = pd.read_csv(\"..\/input\/tps06-nns-gbts-optimization\/submission.csv\", index_col='id')","e999e6d4":"# made in R:\n\n#log_weight_loss <- function(w){\n#    weighted_pred = as.matrix((oof1*w[1]+oof2*w[2]+oof3*w[3]+oof7*w[4]) \/ sum(w))\n#    \n#  r = yardstick::mn_log_loss_vec(\n#    truth = factor(train$target),\n#    estimate = weighted_pred)\n#    \n#    return(r)\n#}\n\n# results <- optim(rep(10, 4), log_weight_loss, method = \"Nelder-Mead\")\n# w <- results$par","2d706401":"w = [11.9028565343769, 6.57314783814548, 8.19327453286444, 12.3950366481336]","0112c8e2":"oof_my_blend = ((oof1*w[0])+(oof2*w[1])+(oof3*w[2])+(oof7*w[3])) \/ np.sum(w)\nsub_my_blend = ((sub1*w[0])+(sub2*w[1])+(sub3*w[2])+(sub7*w[3])) \/ np.sum(w)","26e9aeec":"def generation(main, support, coeff): \n    sub1  = support.copy()\n    sub1v = sub1.values    \n    \n    sub2  = main.copy() \n    sub2v = sub2.values\n       \n    imp  = main.copy()    \n    impv = imp.values  \n    NCLASS = 9\n    number = 0\n    \n    for i in range (len(main)):               \n        \n        row1 = sub1v[i,:]\n        row2 = sub2v[i,:]\n        row1_sort = np.sort(row1)\n        row2_sort = np.sort(row2) \n        \n        row = (row2 * coeff) + (row1 * (1.0 - coeff))\n        row_sort = np.sort(row)        \n        \n        for j in range (NCLASS):             \n            if ((row2[j] == row2_sort[8]) and (row1[j] != row1_sort[8])):                                \n                row = row2 \n                number = number + 1            \n#               print(number, i)\n        \n        impv[i, :] = row\n    \n    imp.iloc[:, :] = impv[:, :]\n    print(f'Number of rows unchanged: {number}')\n    print(30 * '=')                                        \n    return imp","e9e8f631":"sub_gen = generation(sub3_ext, sub_my_blend, 0.85)\nsub_gen = generation(sub2_ext, sub_gen, 0.85)\nsub_gen = generation(sub1_ext, sub_gen, 0.85)","e474495e":"def improve(sub1, sub2, sub3, sub4, sub_ens, majority, m_majority):  \n    \n    sub1v = sub1.values\n    sub2v = sub2.values\n    sub3v = sub3.values\n    sub4v = sub4.values\n    \n    imp = sub_ens.copy()\n    impv = imp.values\n    NCLASS = 9\n    number = 0\n\n    for i in range (len(sub_ens)):\n        c_count = 0  \n        row = impv[i,:]\n        row_sort = np.sort(row)        \n        \n        row1 = sub1v[i,:]\n        row2 = sub2v[i,:]\n        row3 = sub3v[i,:]\n        row4 = sub4v[i,:]\n        row1_sort = np.sort(row1)\n        row2_sort = np.sort(row2)\n        row3_sort = np.sort(row3)\n        row4_sort = np.sort(row4)\n                      \n        for j in range (NCLASS): \n            count = 0\n            \n            for k in range (NCLASS):                \n                if (row4[j] == row4_sort[k]): \n                    \n                    if (row1[j] == row1_sort[k]):\n                        count = count + 1\n                    if (row2[j] == row2_sort[k]):\n                        count = count + 1                   \n                    if (row3[j] == row3_sort[k]):\n                        count = count + 1   \n                        \n            if (count >= majority):\n                c_count = c_count + 1\n        \n        if ((c_count >= m_majority) and (row4_sort[8] >= row_sort[8])): \n            impv[i, :] = row4            \n            number = number + 1            \n#           print (number, i, c_count)                         \n                    \n    imp.iloc[:, 1:] = impv[:, 1:]\n    p_number = round(((number \/ 100000) * 100),2)\n    print('>>>  R  E  T  U  R  N  S  <<<')\n    print(30 * '=')\n    print(f'Number of changes: {number}\\n')\n    print(f'Percentage of changes: {p_number} %')\n    print(30 * '=')\n    return imp  ","2e321385":"sub_imp = improve(sub_my_blend, sub3_ext, sub2_ext, sub1_ext, sub_gen, 3, 5)","9376977f":"sub.iloc[:, 1:] = sub_imp.values\nsub.to_csv(\"sub_blend_imp.csv\", index=False)","4c09d67c":"<p align=\"right\"><span style=\"color:firebrick\">If you liked it, please don't forget to vote for the mentioned notebooks too! \ud83d\ude80 <\/p>\n","1344c89e":"# Blend optimization with Nelder-Mead","e8643674":"# Smart Ensembling for Classifier\n\nWith external submissions. Thank you very much for sharing:\n\n- https:\/\/www.kaggle.com\/mehrankazeminia\/3-tps-jun-21-smart-ensembling-for-classifier by [@mehrankazeminia](https:\/\/www.kaggle.com\/mehrankazeminia) and [@somayyehgholami](https:\/\/www.kaggle.com\/somayyehgholami); \n- https:\/\/www.kaggle.com\/tunguz\/tps-6-21-new-ensemble by [@tunguz](https:\/\/www.kaggle.com\/tunguz);\n- https:\/\/www.kaggle.com\/hiro5299834\/tps06-nns-gbts-optimization by [@hiro5299834](https:\/\/www.kaggle.com\/hiro5299834)\n\n\nsee: https:\/\/www.kaggle.com\/c\/tabular-playground-series-jun-2021\/discussion\/248454","5e7b109d":"# Load dependencies","32730be7":"# Comparative Method for Classifier\n\nsee: https:\/\/www.kaggle.com\/mehrankazeminia\/2-tps-jun-21-comparative-method-for-classifier (many thanks for sharing)","bffb0035":"# Sub","8a81f32e":"# Problem definition\n\nFrom description:\n\n\"The dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the category on an eCommerce product given various attributes about the listing. Although the features are anonymized, they have properties relating to real-world features.\"\n\n# Blend\n\nFirstly the optimization of the blend weights with my submissions will be done using the `optim` (R base) function using the Nelder-Mead method\n\nThis blend will then be combined with external subsmissions using (the amazing) functions shared by [@mehrankazeminia](https:\/\/www.kaggle.com\/mehrankazeminia) and [@somayyehgholami](https:\/\/www.kaggle.com\/somayyehgholami) (thanks so much for sharing!)\n\n<br>\n\n<p align=\"right\"><span style=\"color:firebrick\">Dont forget the upvote if you liked the notebook! <i class=\"fas fa-hand-peace\"><\/i><\/span> <\/p>"}}