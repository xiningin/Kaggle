{"cell_type":{"2ceffa53":"code","feaaf6ef":"code","f9e9afb3":"code","d99a9750":"code","7d98535b":"code","7770cc63":"code","a042f305":"code","1d03c73d":"code","102e74e1":"code","a61a8cd5":"code","0f53b0e8":"markdown","18a1c3a0":"markdown","382e99c2":"markdown","53635d17":"markdown","d43d3f45":"markdown","ef3b475e":"markdown","305557a1":"markdown","59e2f238":"markdown","d79ac0f4":"markdown","7cb70bb1":"markdown","b00a4111":"markdown"},"source":{"2ceffa53":"import pandas as pd\nimport numpy as np\nimport string\n\n# keras module for building LSTM \nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Embedding, LSTM, Dense, Dropout\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nimport keras.utils as ku\n\nfrom keras.callbacks import EarlyStopping","feaaf6ef":"trip_advisor_data = pd.read_csv('..\/input\/trip-advisor-hotel-reviews\/tripadvisor_hotel_reviews.csv')\ntrip_advisor_data = trip_advisor_data['Review']\ntrip_advisor_data = trip_advisor_data.iloc[:1000]","f9e9afb3":"def clean_text(txt):\n    txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n    return txt \n\ncorpus = [clean_text(x) for x in trip_advisor_data]\ncorpus[0]","d99a9750":"tokenizer = Tokenizer()\n\ndef get_sequence_of_tokens(corpus):\n    ## tokenization\n    tokenizer.fit_on_texts(corpus)\n    total_words = len(tokenizer.word_index) + 1\n    \n    ## convert data to sequence of tokens \n    input_sequences = []\n    for line in corpus:\n        token_list = tokenizer.texts_to_sequences([line])[0]\n        for i in range(1, len(token_list)):\n            n_gram_sequence = token_list[:i+1]\n            input_sequences.append(n_gram_sequence)\n    return input_sequences, total_words\n\ninp_sequences, total_words = get_sequence_of_tokens(corpus)\ninp_sequences[:10]","7d98535b":"def generate_padded_sequences(input_sequences):\n    max_sequence_len = max([len(x) for x in input_sequences])\n    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n    \n    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n    label = ku.to_categorical(label, num_classes=total_words)\n    return predictors, label, max_sequence_len\n\npredictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)","7770cc63":"early_stopping = EarlyStopping(\n    monitor='loss', min_delta=0.001, patience=30, verbose=0,\n    mode='min', baseline=None, restore_best_weights=True\n)","a042f305":"def create_model(max_sequence_len, total_words):\n    input_len = max_sequence_len - 1\n    model = Sequential()\n    \n    # Add Input Embedding Layer\n    model.add(Embedding(total_words, 10, input_length=input_len))\n    \n    # Add Hidden Layer 1 - LSTM Layer\n    model.add(LSTM(100,return_sequences=True))\n    model.add(Dropout(0.1))\n    \n    # Add Hidden Layer 2 - LSTM Layer\n    model.add(LSTM(100))\n    model.add(Dropout(0.1))\n    \n    # Add Output Layer\n    model.add(Dense(total_words, activation='softmax'))\n\n    model.compile(loss='categorical_crossentropy', optimizer='adam')\n    \n    return model\n\nmodel = create_model(max_sequence_len, total_words)\nmodel.summary()","1d03c73d":"model.fit(predictors, label, epochs=200, callbacks=[early_stopping], verbose=False)","102e74e1":"def generate_text(seed_text, next_words, model, max_sequence_len):\n    for _ in range(next_words):\n        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n        predicted = model.predict_classes(token_list, verbose=0)\n        \n        output_word = \"\"\n        for word,index in tokenizer.word_index.items():\n            if index == predicted:\n                output_word = word\n                break\n        seed_text += \" \"+output_word\n    return seed_text.title()","a61a8cd5":"print(generate_text(\"this hotel\", 20, model, max_sequence_len))\nprint(generate_text(\"we liked\", 20, model, max_sequence_len))\nprint(generate_text(\"the place\", 20, model, max_sequence_len))\nprint(generate_text(\"everything\", 20, model, max_sequence_len))\nprint(generate_text(\"I think\",20, model, max_sequence_len))","0f53b0e8":"# Text Generating","18a1c3a0":"# Data preparation","382e99c2":"Word sequences may have different lengths. We need to pad them and equalize their lengths. We use the pad_sequence function of Keras for this purpose. To train a template to generate text, we need to create labels. So for each given word sequence, the goal is to predict the next word.","53635d17":"The LSTMs are implemented, and the data prepared for the model thanks to the Keras library. The first step is to clean the data, by removing for example the punctuation present in the corpus.","d43d3f45":"# LSTMs","ef3b475e":"![lstm.png](attachment:lstm.png)","305557a1":"The goal of this notebook is to generate TripAdvisor reviews with Long Short Term Memory units.","59e2f238":"This notebook is inspired from [@shivamb's notebook on text generation](https:\/\/www.kaggle.com\/shivamb\/beginners-guide-to-text-generation-using-lstms) and [@thebrownviking20's work on LSTMs](https:\/\/www.kaggle.com\/thebrownviking20\/intro-to-recurrent-neural-networks-lstm-gru)","d79ac0f4":"The next step is tokenization. It is a process of extracting tokens from a corpus. Thus, each sequence of words is converted into a sequence of tokens.","7cb70bb1":"# Creating and training the model","b00a4111":"The LSTM block learns which elements it should retain over the long term and which elements it should forget. We can consider the states of the Ct memory as the states of a memory acting on the long term and we can see the hidden states ht as the states of a memory acting on the short term.\n\nIt first takes the previous memory state Ct-1 and performs a multiplication with the forget gate f to decide at which degree Ct-1 should be forgotten. The value of the forget gate is between 0 and 1. For example, if it is equal to 0, then Ct-1 is completely forgotten; and if it is equal to 1, then Ct-1 is completely passed on to the rest of the block.\n\nIt then adds the part of Ct-1 retained and the information retained by the I gate.\nThe Ct memory state thus obtained is transferred directly to the next LSTM block. On the other hand, Ct is copied. The tanh function is applied to this copy, then the result is coupled with xt and ht-1, and the final result obtained is filtered by the output gate O. This gives us the hidden state ht of the LSTM block, which is then transferred to the next LSTM block."}}