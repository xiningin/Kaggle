{"cell_type":{"a9e71bae":"code","87c2c2d1":"code","61d90e45":"code","36b27f99":"code","401bc022":"code","d8809cff":"code","332cd188":"code","e630c277":"code","4836d6e5":"code","0abdc466":"code","2672c63f":"code","f35102aa":"code","6394c7ee":"code","5a90e0eb":"code","9db63ed3":"code","a62a32ce":"code","637a9653":"code","0b116342":"code","2446d829":"code","9f75890d":"code","47ce7e01":"code","0884fc5f":"markdown","ba1f64a8":"markdown","3aecd925":"markdown","90cc1ec1":"markdown","ce9943bc":"markdown","6b536f02":"markdown"},"source":{"a9e71bae":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nimport gc\ngc.collect()\n# Any results you write to the current directory are saved as output.","87c2c2d1":"import pandas as pd\ntrain_df=pd.read_csv('..\/input\/train.csv',iterator=True,chunksize=150_000, dtype={'acoustic_data':np.int16,'time_to_failure':np.float32})","61d90e45":"submission=pd.read_csv('..\/input\/sample_submission.csv')\nprint(\"No. of rows in submission :{}\".format(submission.shape[0]))","36b27f99":"len(os.listdir('..\/input\/test'))","401bc022":"def gen_features(X):\n    strain = []\n    strain.append(X.mean())\n    strain.append(X.std())\n    strain.append(X.max())\n    strain.append(X.min())\n    strain.append(X.kurtosis())\n    strain.append(X.skew())\n    strain.append(np.quantile(X,0.01))\n    strain.append(np.quantile(X,0.05))\n    strain.append(np.quantile(X,0.95))\n    strain.append(np.quantile(X,0.99))\n    strain.append(np.abs(X).max())\n    strain.append(np.abs(X).mean())\n    strain.append(np.abs(X).std())\n    return pd.Series(strain)\n","d8809cff":"#train = pd.read_csv('train.csv', iterator=True, chunksize=150_000, dtype={'acoustic_data': np.int16, 'time_to_failure': np.float64})\n\nX_train = pd.DataFrame()\ny_train = pd.Series()\nfor df in train_df:\n    ch = gen_features(df['acoustic_data'])\n    X_train = X_train.append(ch, ignore_index=True)\n    y_train = y_train.append(pd.Series(df['time_to_failure'].values[-1]))","332cd188":"X_train.columns=['ave', 'std', 'max', 'min','kurtosis','skew','1%_quantile','5%_quantile','95%_quantile',\n                                '99%_quantile','abs_max','abs_mean','abs_std']","e630c277":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)","4836d6e5":"y_train.head()","0abdc466":"import matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize=(10,10))\nsns.heatmap(X_train.corr(),cmap='YlGnBu')\nplt.show()","2672c63f":"import xgboost as xgb \nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error\n# Create the training and test sets\nX_trai, X_tes, y_trai, y_tes = train_test_split(X_train_scaled, y_train, test_size=0.2, random_state=123)\n\n# Instantiate the XGBRegressor: xg_reg\nxg_reg = xgb.XGBRegressor(objective='reg:linear',n_estimators=150,seed=123)\n#booster=\"gbtree\" is default base learner in XGBoost\n\n# Fit the regressor to the training set\nxg_reg.fit(X_trai,y_trai)\n\n# Predict the labels of the test set: preds\npreds = xg_reg.predict(X_tes)\n\n# Compute the rmse: rmse\nmae = (mean_absolute_error(y_tes, preds))\nprint(\"MAE: %f\" % (mae))","f35102aa":"import xgboost as xgb\nfrom sklearn.metrics import mean_absolute_error\ndmatrix = xgb.DMatrix(data=X_train_scaled, label=y_train)\n\nreg_params = [0.05, 0.0001, 0.0005]\n\n# Create the initial parameter dictionary for varying l2 strength: params\nparams = {\"objective\":\"reg:linear\",\"max_depth\":3}\n\n# Create an empty list for storing rmses as a function of l2 complexity\nmae_l2 = []\n\n# Iterate over reg_params\nfor reg in reg_params:\n\n    # Update l2 strength\n    params[\"lambda\"] = reg\n    \n    # Pass this updated param dictionary into cv\n    cv_results_mae = xgb.cv(dtrain=dmatrix, params=params, nfold=10, num_boost_round=100, metrics=\"mae\", as_pandas=True, seed=123)\n    \n    # Append best rmse (final round) to rmses_l2\n    mae_l2.append(cv_results_mae[\"test-mae-mean\"].tail(1).values[0])\n\n# Look at best rmse per l2 param\nprint(\"Best mae as a function of l2:\")\nprint(pd.DataFrame(list(zip(reg_params, mae_l2)), columns=[\"l2\",\"mae\"]))","6394c7ee":"from catboost import CatBoostRegressor,Pool\ntrain_pool = Pool(X_train, y_train)\nm = CatBoostRegressor(iterations=10000, loss_function='MAE', boosting_type='Ordered')\nm.fit(X_train_scaled, y_train, silent=True)\nm.best_score_","5a90e0eb":"'''from sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import NuSVR, SVR\n\n\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\n\nparameters = [{'gamma': [0.001, 0.005, 0.01, 0.02, 0.05, 0.1],\n               'C': [0.1, 0.2, 0.25, 0.5, 1, 1.5, 2]}]\n               #'nu': [0.75, 0.8, 0.85, 0.9, 0.95, 0.97]}]\n\nreg1 = GridSearchCV(SVR(kernel='rbf', tol=0.01), parameters, cv=5, scoring='neg_mean_absolute_error')\nreg1.fit(X_train_scaled, y_train.values.flatten())\ny_pred1 = reg1.predict(X_train_scaled)\n\nprint(\"Best CV score: {:.4f}\".format(reg1.best_score_))\nprint(reg1.best_params_)'''","9db63ed3":"'''import keras \nfrom keras.layers import Dense\nfrom keras.model import Sequential\n\n# Saving Number of predictors in ncols\nncols='''","a62a32ce":"submission = pd.read_csv('..\/input\/sample_submission.csv', index_col='seg_id')","637a9653":"X_test = pd.DataFrame(columns=X_train.columns, dtype=np.float64, index=submission.index)","0b116342":"for seg_id in X_test.index:\n    seg = pd.read_csv('..\/input\/test\/' + seg_id + '.csv')\n    \n    x = seg['acoustic_data']#.values\n    \n    X_test.loc[seg_id, 'ave'] = x.mean()\n    X_test.loc[seg_id, 'std'] = x.std()\n    X_test.loc[seg_id, 'max'] = x.max()\n    X_test.loc[seg_id, 'min'] = x.min()\n    X_test.loc[seg_id, 'kurtosis'] = x.kurtosis()\n    X_test.loc[seg_id, 'skew'] = x.skew()\n    X_test.loc[seg_id, '1%_quantile'] = np.quantile(x,0.01)\n    X_test.loc[seg_id, '5%_quantile'] = np.quantile(x,0.05)\n    X_test.loc[seg_id, '95%_quantile'] = np.quantile(x,0.95)\n    X_test.loc[seg_id, '99%_quantile'] = np.quantile(x,0.99)\n    X_test.loc[seg_id, 'abs_max'] = np.abs(x).max()\n    X_test.loc[seg_id, 'abs_mean'] = np.abs(x).mean()\n    X_test.loc[seg_id, 'abs_std'] = np.abs(x).std()","2446d829":"X_test.head()","9f75890d":"X_test_scaled = scaler.transform(X_test)\nsubmission['time_to_failure'] = m.predict(X_test_scaled)\nsubmission.to_csv('submission.csv')","47ce7e01":"submission.head()","0884fc5f":"### Using Catboost","ba1f64a8":"### No of files in test.zip ","3aecd925":"### Importing Data by changing bits so as to fully Load the data in memory","90cc1ec1":"### Checking no of rows in sample_submission.csv","ce9943bc":"### Using Keras ","6b536f02":"#### since we have same No. of folders in test directory as No. of rows in submission, means we have to predict time_to_failure for all of the test files"}}