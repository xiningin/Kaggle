{"cell_type":{"3241140d":"code","ead15f0c":"code","30043047":"code","8e9491fa":"code","33158330":"code","6d30d28f":"code","4ec3859c":"code","01044c01":"code","ddbf6793":"code","12ea8359":"code","f5924332":"code","dec8d432":"code","f152be14":"code","d6f27e6d":"code","8aa92d0a":"code","01c53f5f":"code","c53c7bb0":"code","53996930":"code","8ad3ca13":"code","41ad67d6":"code","7aee0e04":"code","54418f42":"code","d07fc2b7":"code","a45c5ea8":"code","dadff66b":"code","54846c2d":"code","545418c3":"code","b95b64ab":"code","d6bcfad8":"code","d0efce3b":"code","e19b7e9b":"code","759ca7e7":"code","9001dd89":"code","4da280c1":"code","a5f136f3":"code","8cc26956":"code","51e54aef":"markdown","52bf0fbd":"markdown","457309ef":"markdown","0c155d2c":"markdown","f331162a":"markdown","2e496ba8":"markdown","8b7f4482":"markdown","190749b7":"markdown","409a76f3":"markdown","a8326026":"markdown","ed625881":"markdown","28339e64":"markdown","6b317a3e":"markdown","14b32435":"markdown","aca7bf98":"markdown","aad19543":"markdown","1a8eb8cf":"markdown","9b6ba413":"markdown","0765c84b":"markdown","02e97938":"markdown"},"source":{"3241140d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ead15f0c":"from IPython.display import Image","30043047":"Image(r'\/kaggle\/input\/into-to-cnn\/images\/CNN-01.PNG', width=400)","8e9491fa":"Image(r'\/kaggle\/input\/into-to-cnn\/images\/CNN-02.PNG', width=400)","33158330":"Image(r'\/kaggle\/input\/into-to-cnn\/images\/CNN-03.gif', width=400)","6d30d28f":"Image(r'\/kaggle\/input\/into-to-cnn\/images\/CNN-04.gif', width=800)","4ec3859c":"Image(r'\/kaggle\/input\/into-to-cnn\/images\/CNN-05.gif', width=400)","01044c01":"Image(r'\/kaggle\/input\/into-to-cnn\/images\/CNN-06.gif', width=400)","ddbf6793":"Image(r'\/kaggle\/input\/into-to-cnn\/images\/CNN-07.gif', width=700)","12ea8359":"import keras\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras import backend as K","f5924332":"# input image dimensions\nimg_rows, img_cols = 28, 28","dec8d432":"from keras.datasets import mnist","f152be14":"#download mnist data and split into train and test sets\n(X_train, y_train), (X_test, y_test) = mnist.load_data()","d6f27e6d":"print(\"Training set images shape: {}\".format(X_train.shape))\nprint(\"Training set labels shape: {}\".format(y_train.shape))\n\nprint(\"Test set images shape: {}\".format(X_test.shape))\nprint(\"Test set labels shape: {}\".format(y_test.shape))","8aa92d0a":"X_train.shape","01c53f5f":"X_train[2].shape","c53c7bb0":"import matplotlib.pyplot as plt\n\n#plot the first image in the dataset\nplt.imshow(X_train[3]);\nprint(\"Digit in the image is -\",y_train[3])","53996930":"#check image shape\nX_train[0].shape","8ad3ca13":"#reshape data to fit model\nX_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\nX_test  = X_test.reshape( X_test.shape[0],  img_rows, img_cols, 1)","41ad67d6":"X_train.shape","7aee0e04":"if K.image_data_format() == 'channels_first':\n    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n    X_test  = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n    \n    input_shape = (1, img_rows, img_cols)\nelse:\n    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n    X_test  = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n    \n    input_shape = (img_rows, img_cols, 1)","54418f42":"x_train = X_train.astype('float32')\nx_test  = X_test.astype('float32')","d07fc2b7":"x_train \/= 255\nx_test  \/= 255","a45c5ea8":"print('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')","dadff66b":"# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train)\ny_test  = keras.utils.to_categorical(y_test)","54846c2d":"y_train[3]","545418c3":"model = Sequential()\n\n#add model layers\nmodel.add(Conv2D(32, kernel_size=3, activation='relu', input_shape=(28,28,1)))\n\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(512, activation='relu'))\n\nmodel.add(Dense(128, activation='relu'))\n\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.summary()","b95b64ab":"9*32 + 32","d6bcfad8":"model.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.Adam(),\n              metrics=['accuracy'])","d0efce3b":"60000\/1024","e19b7e9b":"496*12","759ca7e7":"batch_size  = 60000\nnum_classes = 10\nepochs = 12\n\nhistory = model.fit(x_train, y_train,\n                  batch_size=batch_size,\n                  epochs=epochs,\n                  verbose=1,\n                  validation_data=(x_test, y_test))","9001dd89":"score = model.evaluate(x_test, y_test, verbose=0)\n\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","4da280c1":"import matplotlib.pyplot as plt\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(16, 5))\n\n# Plot training & validation accuracy values\nax1.plot(history.history['accuracy'])\nax1.plot(history.history['val_accuracy'])\n\nax1.set_title('Model performance')\nax1.set_ylabel('accuracy')\nax1.set_xlabel('Epoch')\nax1.legend(['Train', 'Val'], loc='upper left')\n\n# Plot training & validation loss values\nax2.plot(history.history['loss'])\nax2.plot(history.history['val_loss'])\n\nax2.set_title('Model loss')\nax2.set_ylabel('Loss')\nax2.set_xlabel('Epoch')\nax2.legend(['Train', 'Val'], loc='upper left');","a5f136f3":"#predict first 4 images in the test set\nmodel.predict(X_test[:9])","8cc26956":"#actual results for first 4 images in test set\ny_test[:4]","51e54aef":"#### Compiling the model\nNext, we need to compile our model. Compiling the model takes three parameters: optimizer, loss and metrics.","52bf0fbd":"## various architectures of CNNs available\n\n1. LeNet\n2. AlexNet\n3. VGGNet\n4. GoogLeNet\n5. ResNet\n6. ZFNet","457309ef":"#### Data pre-processing\n\nNext, we need to reshape our dataset inputs (X_train and X_test) to the shape that our model expects when we train the model. \n\nThe first number is the number of images (60,000 for X_train and 10,000 for X_test). \n\nThen comes the shape of each image (28x28). \n\nThe last number is 1, which signifies that the images are greyscale.","0c155d2c":"#### Exploratory data analysis\nNow let\u2019s take a look at one of the images in our dataset to see what we are working with. We will plot the first image in our dataset and check its size using the \u2018shape\u2019 function.","f331162a":"- Convolution operation on a MxNx3 image matrix with a 3x3x3 Kernel\n\n- The objective of the Convolution Operation is to extract the high-level features such as edges, from the input image. \n\n- ConvNets need not be limited to only 1 Convolutional Layer. \n\n- Conventionally, the first ConvLayer is responsible for capturing the Low-Level features such as edges, color, gradient orientation, etc. \n\n- With added layers, the architecture adapts to the High-Level features as well, giving us a network which has the wholesome understanding of images in the dataset, similar to how we would.","2e496ba8":"## <CENTER> CONVOLUTION RECAP","8b7f4482":"#### Summary\n\n- we have a __input__, such as an image of pixel values, \n- we have a __filter__, which is a set of __weights__, and the __filter__ is systematically applied to the __input__ data to create a __feature map__.\n--------","190749b7":"### Interactive CNN\n\n* An Intuitive Explanation of Convolutional Neural Networks - https:\/\/ujjwalkarn.me\/2016\/08\/11\/intuitive-explanation-convnets\/\n\n* Visualize CNN - http:\/\/scs.ryerson.ca\/~aharley\/vis\/conv\/\n\n* Convolution (Feature Detection) - https:\/\/www.youtube.com\/watch?v=KiftWz544_8\n\n* How do Convolutional Neural Networks work? - https:\/\/brohrer.github.io\/how_convolutional_neural_networks_work.html\n\n* Interactice Convolution Neural Network on MNIST - https:\/\/cs.stanford.edu\/people\/karpathy\/convnetjs\/demo\/mnist.html\n\n* Interactive CNN with CIFAR-10 dataset - https:\/\/cs.stanford.edu\/people\/karpathy\/convnetjs\/demo\/cifar10.html\n\n* Neural Net for Handwritten Digit Recognition in JavaScript - http:\/\/myselph.de\/neuralNet.html\n\n* CNN with Tensorflow|Keras for Fashion MNIST - https:\/\/www.kaggle.com\/gpreda\/cnn-with-tensorflow-keras-for-fashion-mnist","409a76f3":"## How Do Convolutional Layers Work\n\n- used for image classification. \n- major building blocks used in convolutional neural networks.\n- A convolution is the simple application of a __filter__ to an input that results in an activation.\n- Repeated application of the same __filter__ to an input results in a map of activations called a __feature map__, indicating the locations and strength of a detected feature in an input, such as an image.\n\n- The __filter__ is _smaller_ than the __input data__ and the type of multiplication applied between a __filter-sized patch of the input__ and the filter is a dot product. \n\n- A dot product is the __element-wise multiplication__ between the __filter-sized patch of the input__ and __filter__, which is then summed, always resulting in a single value. \n\n- Because it results in a single value, the operation is often referred to as the __\u201cscalar product\u201c__.\n\n- Once a __feature map__ is created, we can pass each value in the feature map through a __nonlinearity__, such as a __ReLU__, much like we do for the outputs of a fully connected layer.","a8326026":"![CNN-00.PNG](attachment:df132e34-8cd2-4890-a9af-b8820351dd41.PNG)","ed625881":"Next __flatten__ the final output and feed it to a regular Neural Network for classification purposes.","28339e64":"# <CENTER> CNN DEMO - MNIST DATASET","6b317a3e":"- Our first 2 layers are Conv2D layers. These are convolution layers that will deal with our input images, which are seen as 2-dimensional matrices.\n\n- 64 in the first layer and 32 in the second layer are the number of nodes in each layer. \n\n- Kernel size is the size of the filter matrix for our convolution. So a kernel size of 3 means we will have a 3x3 filter matrix.\n\n- Our first layer also takes in an input shape. This is the shape of each input image, 28,28,1 as seen earlier on, with the 1 signifying that the images are greyscale.\n\n- In between the Conv2D layers and the dense layer, there is a \u2018Flatten\u2019 layer. Flatten serves as a connection between the convolution and dense layers.\n\n- \u2018Dense\u2019 is the layer type we will use in for our output layer. Dense is a standard layer type that is used in many cases for neural networks.\n\n- We will have 10 nodes in our output layer, one for each possible outcome (0\u20139).\n\n- The activation is \u2018softmax\u2019. Softmax makes the output sum up to 1 so the output can be interpreted as probabilities. The model will then make its prediction based on which option has the highest probability.","14b32435":"# <CENTER> INTRODUCTION TO CNN","aca7bf98":"- Image Dimensions = 5 (Height) x 5 (Breadth) x 1 (Number of channels, eg. RGB)\n\n- the __green__ section resembles our 5x5x1 input image, I. \n\n- The element involved in carrying out the convolution operation in the first part of a Convolutional Layer is called the Kernel\/Filter, K, represented in the __color__ yellow. We have selected K as a 3x3x1 matrix.","aad19543":"- the shape of every image in the mnist dataset is 28 x 28\n\n- When using real-world datasets, 28 x 28 is a fairly small size","1a8eb8cf":"## Convolution\n\n- We have an RGB image which has been separated by its three color planes \u2014 Red, Green, and Blue.\n\n- Imagine how computationally intensive things would get once the images reach dimensions, say 8K (7680\u00d74320). \n\n- The role of the ConvNet is to reduce the images into a form which is easier to process, without losing features which are critical for getting a good prediction. ","9b6ba413":"We need to \u2018one-hot-encode\u2019 our target variable. This means that a column will be created for each output category and a binary variable is inputted for each category. For example, we saw that the first image in the dataset is a 5. This means that the sixth number in our array will have a 1 and the rest of the array will be filled with 0.","0765c84b":"- The __flattened__ output is fed to a __feed-forward neural network__ and __backpropagation__ applied to every iteration of training. \n\n- Over a series of __epochs__, the model is able to distinguish between dominating and certain low-level features in images and classify them using the __Softmax__ Classification technique.","02e97938":"## Pooling Layer\n\n- Similar to the Convolutional Layer, the __Pooling layer__ is responsible for reducing the spatial size of the Convolved Feature. \n\n- This is to decrease the computational power required to process the data through __dimensionality reduction__. \n\n- Furthermore, it is useful for extracting dominant features which are rotational and positional invariant, thus maintaining the process of effectively training of the model.\n\nThere are 2 types of Pooling: \n\n- Max Pooling \n- Average Pooling. \n\n- __Max Pooling__ returns the maximum value from the portion of the image covered by the Kernel. On the other hand, __Average Pooling__ returns the average of all the values from the portion of the image covered by the Kernel.\n\n- __Max Pooling__ also performs as a __Noise__ Suppressant. It discards the noisy activations altogether and also performs de-noising along with dimensionality reduction. \n\n- __Average Pooling__ simply performs dimensionality reduction as a noise suppressing mechanism. \n\nHence, we can say that Max Pooling performs a lot better than Average Pooling"}}