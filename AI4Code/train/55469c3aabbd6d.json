{"cell_type":{"de6a81dc":"code","955c4b17":"code","8ae40269":"code","be026a73":"code","e1bbbb22":"code","cd70cc33":"code","012e6bff":"code","33b0aa51":"code","d06ae2d8":"code","d85a357e":"code","33eeac8e":"code","cbe0e626":"code","c38347e0":"code","fe00875d":"code","0a73171a":"code","df901aa5":"code","1d232d89":"code","d6be50a1":"code","b60d92c4":"code","1018a5cb":"code","2e9f6a44":"code","6b8e0b4c":"code","c778264e":"code","94e05e5e":"code","ad5102e4":"code","6f8ef6ac":"code","88385569":"code","2095d14c":"code","6076b5d7":"code","77e25548":"code","4456261a":"code","feb9adc5":"code","67a6e26a":"code","d0eb7115":"code","20673d74":"code","1b1af421":"code","b3db6924":"code","6ad36621":"code","acea77ce":"code","39e3dc7f":"code","196153e0":"code","693568ba":"markdown","b374db8b":"markdown","f00ab786":"markdown","edff32d7":"markdown","553605bd":"markdown","a1b853ba":"markdown","8f3e1f92":"markdown","321a6da0":"markdown","0b1789ee":"markdown","889439c0":"markdown","0c9eeba7":"markdown","a34a524f":"markdown","bea6e3a7":"markdown","93ea5cf3":"markdown","3427a887":"markdown","9d3b7a3e":"markdown"},"source":{"de6a81dc":"#import modules\n%matplotlib inline\n\nimport time\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier \nfrom urllib.request import urlopen \n\n\nplt.style.use('ggplot')\npd.set_option('display.max_columns', 500) ","955c4b17":"breast_cancer = pd.read_csv(\"\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv\")","8ae40269":"names = ['id', 'diagnosis', 'radius_mean', \n         'texture_mean', 'perimeter_mean', 'area_mean', \n         'smoothness_mean', 'compactness_mean', \n         'concavity_mean','concave_points_mean', \n         'symmetry_mean', 'fractal_dimension_mean',\n         'radius_se', 'texture_se', 'perimeter_se', \n         'area_se', 'smoothness_se', 'compactness_se', \n         'concavity_se', 'concave_points_se', \n         'symmetry_se', 'fractal_dimension_se', \n         'radius_worst', 'texture_worst', \n         'perimeter_worst', 'area_worst', \n         'smoothness_worst', 'compactness_worst', \n         'concavity_worst', 'concave_points_worst', \n         'symmetry_worst', 'fractal_dimension_worst']\n\ndx = ['Benign', 'Malignant']","be026a73":"# Setting 'id_number' as our index\n\nbreast_cancer.set_index(['id'], inplace=True)\n\n# Converted to binary to help later on with models and plots\n\nbreast_cancer['diagnosis'] = breast_cancer['diagnosis'].map({'M': 1, 'B': 0})\nbreast_cancer.diagnosis","e1bbbb22":"breast_cancer.apply(lambda x: x.isnull().sum())","cd70cc33":" #For later use in CART models\nnames_index = names[2:]","012e6bff":"del breast_cancer['Unnamed: 32']","33b0aa51":"breast_cancer.head()","d06ae2d8":"print(\"Here's the dimensions of our data frame:\\n\", \n     breast_cancer.shape)\nprint(\"Here's the data types of our columns:\\n\",\n     breast_cancer.dtypes)","d85a357e":"breast_cancer.describe()","33eeac8e":"feature_space = breast_cancer.iloc[:,breast_cancer.columns != 'diagnosis']\nfeature_class = breast_cancer.iloc[:,breast_cancer.columns == 'diagnosis']\n\ntraining_set, test_set, class_set, test_class_set = train_test_split(feature_space,feature_class,test_size = 0.20,random_state = 42)","cbe0e626":" # Cleaning test sets to avoid future warning messages\nclass_set = class_set.values.ravel() \ntest_class_set = test_class_set.values.ravel() ","c38347e0":"fit_rf = RandomForestClassifier(random_state = 42)","fe00875d":"np.random.seed(42)\nstart = time.time()\nparam_dist = {'max_depth':[2,3,4],\n              'bootstrap':[True,False],\n              'max_features':['auto','sqrt','log2',None],\n              'criterion': ['gini','entropy']}\ncv_rf = GridSearchCV(fit_rf,cv=10,\n                    param_grid = param_dist,\n                    n_jobs = 3)\ncv_rf.fit(training_set,class_set)\n\nprint('Best Parameters using grid search: \\n', \n      cv_rf.best_params_)\nend = time.time()\nprint('Time taken in grid search: {0: .2f}'.format(end - start))","0a73171a":"# Set best parameters given by grid search \nfit_rf.set_params(criterion = 'entropy',\n                  max_features = 'log2', \n                  max_depth = 4)","df901aa5":"fit_rf.set_params(warm_start=False, \n                  oob_score=True)\n\nmin_estimators = 15\nmax_estimators = 1000\n\nerror_rate = {}\n\nfor i in range(min_estimators, max_estimators + 1):\n    fit_rf.set_params(n_estimators=i)\n    fit_rf.fit(training_set,class_set)\n\n    oob_error = 1 - fit_rf.oob_score_\n    error_rate[i] = oob_error\n    \n","1d232d89":"# Convert dictionary to a pandas series for easy plotting \noob_series = pd.Series(error_rate)\n\nprint(oob_series)","d6be50a1":"fig, ax =plt.subplots(figsize=(10,10))\n\nax.set_facecolor('#fafafa')\noob_series.plot(kind = 'line',\n               color = 'red')\nplt.axhline(0.055,\n           color = '#875FDB',\n           linestyle = '--')\nplt.xlabel('n_estimators')\nplt.ylabel('OOB Error Rate')\nplt.title('OOB Error Rate Across various Forest sizes \\n(From 15 to 1000 trees)')\n\n\n","b60d92c4":"print('OOB error rate for 400 tree is: {0:5f}'.format(oob_series[400]))","1018a5cb":"fit_rf.set_params(n_estimators = 400,\n                 bootstrap = True,\n                 warm_start = False,\n                 oob_score = False)","2e9f6a44":"fit_rf.fit(training_set, class_set)","6b8e0b4c":"def variable_importance(fit):\n    try:\n        if not hasattr(fit, 'fit'):\n            return print(\"'{0}' is not an instantiated model from scikit-learn\".format(fit)) \n                # Captures whether the model has been trained\n        if not vars(fit)[\"estimators_\"]:\n            return print(\"Model does not appear to be trained.\")\n    except KeyError:\n        print(\"Model entered does not contain 'estimators_' attribute.\")\n\n    importances = fit.feature_importances_\n    indices = np.argsort(importances)[::-1]\n    return {'importance': importances,\n            'index': indices}","c778264e":"var_imp_rf = variable_importance(fit_rf)\n\nimportances_rf = var_imp_rf['importance']\n\nindices_rf = var_imp_rf['index']","94e05e5e":"def print_var_importance(importance, indices, name_index):\n    \n    print(\"Feature ranking:\")\n    \n\n    for f in range(0, indices.shape[0]):\n        i = f\n        print(\"{0}. The feature '{1}' has a Mean Decrease in Impurity of {2:.5f}\"\n              .format(f + 1,\n                      names_index[indices[i]],\n                      importance[indices[f]]))\n","ad5102e4":"print_var_importance(importances_rf, indices_rf, names_index)","6f8ef6ac":"def variable_importance_plot(importance, indices, name_index):\n    \"\"\"\n    Purpose\n    ----------\n    Prints bar chart detailing variable importance for CART model\n    NOTE: feature_space list was created because the bar chart\n    was transposed and index would be in incorrect order.\n\n    Parameters\n    ----------\n    * importance: Array returned from feature_importances_ for CART\n                models organized by dataframe index\n    * indices: Organized index of dataframe from largest to smallest\n                based on feature_importances_\n    * name_index: Name of columns included in model\n\n    Returns:\n    ----------\n    Returns variable importance plot in descending order\n    \"\"\"\n    index = np.arange(len(names_index))\n\n    importance_desc = sorted(importance)\n    feature_space = []\n    for i in range(indices.shape[0] - 1, -1, -1):\n        feature_space.append(names_index[indices[i]])\n\n    fig, ax = plt.subplots(figsize=(10, 10))\n\n    #ax.set_axis_bgcolor('#fafafa')\n    plt.title('Feature importances for Random Forest Model\\\n    \\nBreast Cancer (Diagnostic)')\n    plt.barh(index,\n             importance_desc,\n             align=\"center\",\n             color = '#875FDB')\n    plt.yticks(index,\n               feature_space)\n\n    plt.ylim(-1, 30)\n    plt.xlim(0, max(importance_desc) + 0.01)\n    plt.xlabel('Mean Decrease in Impurity')\n    plt.ylabel('Feature')\n\n    plt.show()\n    plt.close()","88385569":"variable_importance_plot(importances_rf, indices_rf, names_index)","2095d14c":"def cross_val_metrics(fit, training_set, class_set, estimator, print_results = True):\n    \"\"\"\n    Purpose\n    ----------\n    Function helps automate cross validation processes while including \n    option to print metrics or store in variable\n\n    Parameters\n    ----------\n    fit: Fitted model \n    training_set:  Data_frame containing 80% of original dataframe\n    class_set:     data_frame containing the respective target vaues \n                      for the training_set\n    print_results: Boolean, if true prints the metrics, else saves metrics as \n                      variables\n\n    Returns\n    ----------\n    scores.mean(): Float representing cross validation score\n    scores.std() \/ 2: Float representing the standard error (derived\n                from cross validation score's standard deviation)\n    \"\"\"\n    my_estimators = {\n    'rf': 'estimators_',\n    'nn': 'out_activation_',\n    'knn': '_fit_method'\n    }\n    try:\n        # Captures whether first parameter is a model\n        if not hasattr(fit, 'fit'):\n            return print(\"'{0}' is not an instantiated model from scikit-learn\".format(fit)) \n\n        # Captures whether the model has been trained\n        if not vars(fit)[my_estimators[estimator]]:\n            return print(\"Model does not appear to be trained.\")\n\n    except KeyError as e:\n        print(\"'{0}' does not correspond with the appropriate key inside the estimators dictionary. \\\n\\nPlease refer to function to check `my_estimators` dictionary.\".format(estimator))\n        raise\n\n    n = KFold(n_splits=10)\n    scores = cross_val_score(fit, \n                         training_set, \n                         class_set, \n                         cv = n)\n    if print_results:\n        for i in range(0, len(scores)):\n            print(\"Cross validation run {0}: {1: 0.3f}\".format(i, scores[i]))\n        print(\"Accuracy: {0: 0.3f} (+\/- {1: 0.3f})\"\\\n              .format(scores.mean(), scores.std() \/ 2))\n    else:\n        return scores.mean(), scores.std() \/ 2","6076b5d7":"cross_val_metrics(fit_rf, \n                  training_set, \n                  class_set, \n                  'rf',\n                  print_results = True)","77e25548":"predictions_rf = fit_rf.predict(test_set)\nprint(predictions_rf)","4456261a":"def create_conf_mat(test_class_set, predictions):\n    \"\"\"Function returns confusion matrix comparing two arrays\"\"\"\n    if (len(test_class_set.shape) != len(predictions.shape) == 1):\n        return print('Arrays entered are not 1-D.\\nPlease enter the correctly sized sets.')\n    elif (test_class_set.shape != predictions.shape):\n        return print('Number of values inside the Arrays are not equal to each other.\\nPlease make sure the array has the same number of instances.')\n    else:\n        # Set Metrics\n        test_crosstb_comp = pd.crosstab(index = test_class_set,\n                                        columns = predictions)\n        # Changed for Future deprecation of as_matrix\n        test_crosstb = test_crosstb_comp.values\n        return test_crosstb","feb9adc5":"conf_mat = create_conf_mat(test_class_set, predictions_rf)\nsns.heatmap(conf_mat, annot=True, fmt='d', cbar=False)\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.title('Actual vs. Predicted Confusion Matrix')\nplt.show()","67a6e26a":"accuracy_rf = fit_rf.score(test_set, test_class_set)\n\nprint(\"Here is our mean accuracy on the test set:\\n {0:.3f}\"\\\n      .format(accuracy_rf))","d0eb7115":"# Here we calculate the test error rate!\ntest_error_rate_rf = 1 - accuracy_rf\nprint(\"The test error rate for our model is:\\n {0: .4f}\"\\\n      .format(test_error_rate_rf))","20673d74":"# We grab the second array from the output which corresponds to\n# to the predicted probabilites of positive classes \n# Ordered wrt fit.classes_ in our case [0, 1] where 1 is our positive class\npredictions_prob = fit_rf.predict_proba(test_set)[:, 1]\n\nfpr2, tpr2, _ = roc_curve(test_class_set,\n                          predictions_prob,\n                          pos_label = 1)","1b1af421":"auc_rf = auc(fpr2, tpr2)","b3db6924":"def plot_roc_curve(fpr, tpr, auc, estimator, xlim=None, ylim=None):\n    \"\"\"\n    Purpose\n    ----------\n    Function creates ROC Curve for respective model given selected parameters.\n    Optional x and y limits to zoom into graph\n\n    Parameters\n    ----------\n    * fpr: Array returned from sklearn.metrics.roc_curve for increasing\n            false positive rates\n    * tpr: Array returned from sklearn.metrics.roc_curve for increasing\n            true positive rates\n    * auc: Float returned from sklearn.metrics.auc (Area under Curve)\n    * estimator: String represenation of appropriate model, can only contain the\n    following: ['knn', 'rf', 'nn']\n    * xlim: Set upper and lower x-limits\n    * ylim: Set upper and lower y-limits\n    \"\"\"\n    my_estimators = {'knn': ['Kth Nearest Neighbor', 'deeppink'],\n              'rf': ['Random Forest', 'red'],\n              'nn': ['Neural Network', 'purple']}\n\n    try:\n        plot_title = my_estimators[estimator][0]\n        color_value = my_estimators[estimator][1]\n    except KeyError as e:\n        print(\"'{0}' does not correspond with the appropriate key inside the estimators dictionary. \\\n\\nPlease refer to function to check `my_estimators` dictionary.\".format(estimator))\n        raise\n\n    fig, ax = plt.subplots(figsize=(10, 10))\n    ax.set_facecolor('#fafafa')\n\n    plt.plot(fpr, tpr,\n             color=color_value,\n             linewidth=1)\n    plt.title('ROC Curve For {0} (AUC = {1: 0.3f})'\\\n              .format(plot_title, auc))\n\n    plt.plot([0, 1], [0, 1], 'k--', lw=2) # Add Diagonal line\n    plt.plot([0, 0], [1, 0], 'k--', lw=2, color = 'black')\n    plt.plot([1, 0], [1, 1], 'k--', lw=2, color = 'black')\n    if xlim is not None:\n        plt.xlim(*xlim)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.show()\n    plt.close()","6ad36621":"plot_roc_curve(fpr2, tpr2, auc_rf, 'rf',\n               xlim=(-0.01, 1.05), \n               ylim=(0.001, 1.05))","acea77ce":"plot_roc_curve(fpr2, tpr2, auc_rf, 'rf', \n               xlim=(-0.01, 0.2), \n               ylim=(0.85, 1.01))","39e3dc7f":"def print_class_report(predictions, alg_name):\n    \"\"\"\n    Purpose\n    ----------\n    Function helps automate the report generated by the\n    sklearn package. Useful for multiple model comparison\n\n    Parameters:\n    ----------\n    predictions: The predictions made by the algorithm used\n    alg_name: String containing the name of the algorithm used\n    \n    Returns:\n    ----------\n    Returns classification report generated from sklearn. \n    \"\"\"\n    print('Classification Report for {0}:'.format(alg_name))\n    print(classification_report(predictions, \n            test_class_set, \n            target_names = dx))","196153e0":"class_report = print_class_report(predictions_rf, 'Random Forest')","693568ba":"# Training Algorithm\nNext we train the algorithm utilizing the training and target class set we had made earlier.","b374db8b":"# OOB Rate","f00ab786":"# Cleaning:","edff32d7":"# Confusion Matrix","553605bd":"# Classification Report","a1b853ba":"The OOB error rate starts to oscilate at around 400 trees, so I will go ahead and use my judgement to use 400 trees in my forest. Using the pandas series object I can easily find the OOB error rate for the estimator as follows:","8f3e1f92":"# Creating Training and Test Sets","321a6da0":"Once we are given the best parameter combination, we set the parameters to our model.\n\nNotice how we didn't utilize the bootstrap: True parameter, this will make sense in the following section.","0b1789ee":"# Traditional Training and Test Set Split\u00b6\nIn order for this methodology to work we will set the number of trees calculated using the OOB error rate, and removing the warm_start and oob_score parameters. Along with including the bootstrap parameter.","889439c0":"# Test Set Metrics","0c9eeba7":"# Variable Importance","a34a524f":"# Missing Values","bea6e3a7":"# ROC Curve Metrics","93ea5cf3":"# Fitting Random Forest","3427a887":"# LOAD DATA\n\nload data into panda dataframe","9d3b7a3e":"# Hyperparameters Optimization"}}