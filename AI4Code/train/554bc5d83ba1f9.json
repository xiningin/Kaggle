{"cell_type":{"89c22e76":"code","571b8c04":"code","f94e7e66":"code","c3b2799c":"code","914107ee":"code","649d8706":"code","076a37cb":"code","f7220072":"code","41d070dc":"code","59bf2408":"code","e9ba8af3":"code","18fab1cb":"code","8fc62b30":"code","b5d7847c":"code","ffa36a2c":"code","ee9927da":"code","dddcfd39":"code","79698645":"code","0833c0e5":"code","ea54e8be":"code","83cb0928":"code","76eea24a":"code","95f4834e":"code","61989691":"code","37a92b1e":"code","f2b34c3c":"code","72aa1624":"code","4c5b7303":"code","52315e77":"code","5b613cc9":"markdown","12673895":"markdown","1aba4393":"markdown","c5203e6d":"markdown","cbc4161d":"markdown","2fe68283":"markdown","d7a0798b":"markdown","2c23769b":"markdown","01ea6633":"markdown","50700198":"markdown","44853a01":"markdown","67d350f8":"markdown","9ac3de63":"markdown","722f0c15":"markdown","8cb391b2":"markdown","d831201f":"markdown","7db41e56":"markdown","583681e8":"markdown","67dac45b":"markdown","42485aa4":"markdown","b63d7df6":"markdown","bce77d98":"markdown","ae4272c1":"markdown"},"source":{"89c22e76":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","571b8c04":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom mlxtend.classifier import EnsembleVoteClassifier\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter('ignore')","f94e7e66":"df = pd.read_csv(\"..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv\")\ndf.head()","c3b2799c":"df.drop('id', axis=1, inplace=True)  # dropping ID Column\ndf.isnull().sum()                    # Checking Null values","914107ee":"corr = df.corr()\nsns.heatmap(corr, annot=True)","649d8706":"labels = [1,2,3,4,5]\ndf[\"age_mean\"] = pd.cut(df[\"age\"], 5, labels=labels)\ndf[\"age_mean\"].value_counts()","076a37cb":"grp_bmi = df.groupby(\"age_mean\")[\"bmi\"].mean()\ngrp_bmi","f7220072":"def bmi_val(cols):\n    bmi = cols[0]\n    age_mean = cols[1]\n    \n    if pd.isnull(bmi):\n        if age_mean == 1:\n            return 20.7\n        elif age_mean == 2:\n            return 28.6\n        elif age_mean == 3:\n            return 31.4\n        elif age_mean == 4:\n            return 31.6\n        elif age_mean == 5:\n            return 29.4\n    else:\n        return bmi\ndf[\"bmi\"] = df[[\"bmi\",\"age_mean\"]].apply(bmi_val, axis=1)","41d070dc":"df.isnull().sum()  ","59bf2408":"x = df[['age','hypertension','heart_disease','avg_glucose_level','bmi']]\ny = df['stroke']\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 3)","e9ba8af3":"k_neigh = np.arange(1,31,1)\n\nknn = KNeighborsClassifier()\nhyperParam = [{'n_neighbors':k_neigh}]\n\ngsv = GridSearchCV(knn,hyperParam,cv=5,verbose=1)\nbest_model = gsv.fit(x_train, y_train)                      # Fitting model with x_train and y_train\nknn_pred = best_model.best_estimator_.predict(x_test)           # Predicting the results\n\nprint(\"Best HyperParameter: \",gsv.best_params_)\nprint(\"Best Accuracy:\",best_model.score(x_test, y_test))","18fab1cb":"plot_confusion_matrix(gsv,x_test, y_test)\n\nconf_metr = confusion_matrix(y_test, knn_pred)\n\nprint(\"Confusion Matrix: \\n {}\".format(conf_metr))\nprint(metrics.classification_report(y_test,knn_pred))\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, knn_pred))\nprint(\"Recall\/Sensitivity\/True Positive Rate:\",metrics.recall_score(y_test, knn_pred))\nprint(\"Precision:\",metrics.precision_score(y_test, knn_pred))","8fc62b30":"# ROC Curve:\nmetrics.plot_roc_curve(gsv, x_test, y_test)","b5d7847c":"c_val = [0.001,0.01,0.1,0.5,1.0]\n\nlogr = LogisticRegression(solver='liblinear')\nhyperParam = [{'C':c_val}]\n\ngsv = GridSearchCV(logr,hyperParam,cv=5,verbose=1)\nbest_model = gsv.fit(x_train,y_train)                   # Fitting model with x_train and y_train\nlogr_pred = best_model.best_estimator_.predict(x_test)  # Predicting the results\n\nprint(\"Best HyperParameter: \", gsv.best_params_)\nprint(\"Best Accuracy:\",best_model.score(x_test,y_test))","ffa36a2c":"plot_confusion_matrix(gsv,x_test, y_test)\n\nconf_metr = confusion_matrix(y_test, logr_pred)\n\nprint(\"Confusion Matrix: \\n {}\".format(conf_metr))\nprint(metrics.classification_report(y_test,logr_pred))\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, logr_pred))\nprint(\"Recall\/Sensitivity\/True Positive Rate:\",metrics.recall_score(y_test, logr_pred))\nprint(\"Precision:\",metrics.precision_score(y_test, logr_pred))","ee9927da":"# ROC Curve\nmetrics.plot_roc_curve(gsv, x_test, y_test)","dddcfd39":"dtree_up = DecisionTreeClassifier()\ndtree_up.fit(x_train, y_train)                  # Fitting model with x_train and y_train\ndtree_pred_up = dtree_up.predict(x_test)        # Predicting the results\nprint(\"Accuracy is: \",metrics.accuracy_score(y_test, dtree_pred_up))","79698645":"depth = np.arange(1, 20, 1)\n\ndtree_pr = DecisionTreeClassifier()\nhyperParam = [{'max_depth':depth}]\n\ngsv = GridSearchCV(dtree_pr,hyperParam,cv=5,verbose=1)\nbest_model = gsv.fit(x_train,y_train)                          # Fitting model with x_train and y_train\ndtree_pred_pr = best_model.best_estimator_.predict(x_test)     # Predicting the results\n\nprint(\"Best HyperParameter: \", gsv.best_params_)\nprint(\"Best Accuracy:\",best_model.score(x_test,y_test))","0833c0e5":"plot_confusion_matrix(gsv,x_test, y_test)\n\nconf_metr = confusion_matrix(y_test, dtree_pred_pr)\n\nprint(\"Confusion Matrix: \\n {}\".format(conf_metr))\nprint(metrics.classification_report(y_test,dtree_pred_pr))\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, dtree_pred_pr))\nprint(\"Recall\/Sensitivity\/True Positive Rate:\",metrics.recall_score(y_test, dtree_pred_pr))\nprint(\"Precision:\",metrics.precision_score(y_test, dtree_pred_pr))","ea54e8be":"# ROC Curve\nmetrics.plot_roc_curve(gsv, x_test, y_test)","83cb0928":"estimators = [10,50,80,100,150,200,250,300]\n\nrf = RandomForestClassifier(max_depth=3,random_state=5)\nhyperParam = [{'n_estimators':estimators}]\n\ngsv = GridSearchCV(rf,hyperParam,cv=5,verbose=1)\nbest_model = gsv.fit(x_train,y_train)                   # Fitting model with x_train and y_train\nrf_pred = best_model.best_estimator_.predict(x_test)    # Predicting the results\n\nprint(\"Best HyperParameter: \", gsv.best_params_)\nprint(\"Best Accuracy:\",best_model.score(x_test,y_test))","76eea24a":"plot_confusion_matrix(gsv,x_test, y_test)\n\nconf_metr = confusion_matrix(y_test, rf_pred)\n\nprint(\"Confusion Matrix: \\n {}\".format(conf_metr))\nprint(metrics.classification_report(y_test,rf_pred))\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, rf_pred))\nprint(\"Recall\/Sensitivity\/True Positive Rate:\",metrics.recall_score(y_test, rf_pred))\nprint(\"Precision:\",metrics.precision_score(y_test, rf_pred))","95f4834e":"# ROC Curve\nmetrics.plot_roc_curve(gsv, x_test, y_test)","61989691":"kernels = ['rbf','linear','poly','sigmoid']\n\nsvc = SVC()\nhyperParam = [{'kernel':kernels}]\n\ngsv = GridSearchCV(svc,hyperParam,cv=5,verbose=1)\nbest_model = gsv.fit(x_train, y_train)                       # Fitting model with x_train and y_train\nsvc_pred = best_model.best_estimator_.predict(x_test)        # Predicting the results\n\nprint(\"Best HyperParameter: \",gsv.best_params_)\nprint(\"Best Accuracy :\",best_model.score(x_test, y_test))","37a92b1e":"plot_confusion_matrix(gsv,x_test, y_test)\n\nconf_metr = confusion_matrix(y_test, svc_pred)\n\nprint(\"Confusion Matrix: \\n {}\".format(conf_metr))\nprint(metrics.classification_report(y_test,svc_pred))\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, svc_pred))\nprint(\"Recall\/Sensitivity\/True Positive Rate:\",metrics.recall_score(y_test, svc_pred))\nprint(\"Precision:\",metrics.precision_score(y_test, svc_pred))","f2b34c3c":"# ROC Curve\nmetrics.plot_roc_curve(gsv, x_test, y_test)","72aa1624":"# Creating classifiers\nknn = KNeighborsClassifier()\nlg = LogisticRegression()\ndt = DecisionTreeClassifier()\nrf = RandomForestClassifier()\nsvc = SVC()\n\nclf_array = [knn, lg, dt, rf,svc]\n\nfor clf in clf_array:\n    cc_scores = cross_val_score(clf, x, y, cv=10, n_jobs=-1)\n    bagging_clf = BaggingClassifier(clf, max_samples=0.25, max_features=10, random_state=3)\n    bagging_scores = cross_val_score(bagging_clf, x, y, cv=10, n_jobs=-1)\n    \n    print(\"Accuracy of: {1:.3f}, std: (+\/-) {2:.3f} [{0}]\".format(clf.__class__.__name__,cc_scores.mean(), cc_scores.std()))\n    print(\"Accuracy of: {1:.3f}, std: (+\/-) {2:.3f} [Bagging {0}]\\n\".format(clf.__class__.__name__,bagging_scores.mean(), bagging_scores.std()))","4c5b7303":"clf = [knn, lg, dt, rf,svc]\neclf = VotingClassifier(estimators=[('KNN', knn), ('Logistic Regression', lg), ('Decision Tree', dt), ('Random Forest', rf), ('SVC', svc)], voting='hard')\nfor clf, label in zip([knn, lg, dt, rf,svc, eclf], ['KNN', 'Logistic Regression', 'Decision Tree', 'Random Forest', 'SVC', 'Ensemble']):\n    scores = cross_val_score(clf, x_train, y_train, cv=10, scoring='accuracy')\n    print(\"Accuracy: %0.3f (+\/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))","52315e77":"# Creating classifiers\nknn = KNeighborsClassifier()\nlg = LogisticRegression()\ndt = DecisionTreeClassifier()\nrf = RandomForestClassifier()\nsvc = SVC()\nada_boost = AdaBoostClassifier()\ngrad_boost = GradientBoostingClassifier()\nxgb_boost = XGBClassifier()\nboost_array = [ada_boost, grad_boost, xgb_boost]\nclf = [knn, lg, dt, rf,svc]\neclf = EnsembleVoteClassifier(clfs=[ada_boost, grad_boost, xgb_boost], voting='hard')\nlabels = ['Ada Boost', 'Grad Boost', 'XG Boost', 'Ensemble']\nfor clf, label in zip([ada_boost, grad_boost, xgb_boost, eclf], labels):\n    scores = cross_val_score(clf, x, y, cv=10, scoring='accuracy')\n    print(\"Accuracy: {0:.3f}, std: (+\/-) {1:.3f} [{2}]\".format(scores.mean(), scores.std(), label))","5b613cc9":"**We can see there are 201 records which does not have BMI values. Now, removing those records is not a good idea bcoz its around 4 % of our total data.\nLet's plot confusion matrix and check which variable is having best correlation with BMI.**","12673895":"**Confusion Matrix:**","1aba4393":"**Confusion Matrix:** Pruned Tree (Because Pruned tree has good accuracy than unpruned tree)","c5203e6d":"**Confusion Matrix:**\nPlotting confusion matrix for KNN. We can find results from confusion matrix:","cbc4161d":"**Confusion Matrix:**","2fe68283":"# **Boosting with all classifiers using Cross Validation:**\nAgain, we will use all 5 models with boosting method. After that, we will apply EnsembleVoteClassifier to vote the best boosting methods.\nHere, three boosting methods are used: Ada Boost, Gradient Boost, & XG Boost.","d7a0798b":"# **5. SVC (Support Vector Classifier)**\nThis is hyperparameter tunned model. Various values of Kernels has been used for tunning.","2c23769b":"## **2. Logistic Regression:**\nThis model is hyperparameter tunned. LogisticRegression is using solver as 'liblinear' and C has few values.","01ea6633":"### **Conclusion:**\nConclusively, we have a very small dataset of 5100 records. Out of which around 4% records had missing BMI values. We used mean avg age of the same age group for missing values.\nAfter that, 5 machine learning models have been used: KNN, Logistic Regression, Decision Tree, Random Forest and SVC. These models are hyperparameter tunned. This project also used bagging and boosting methods with all 5 ML models. VoteClassifier's has been used to cast a vote between all 5 models and will select best model with best results.\nLogistic Regression and SVC turns out to be best for this dataset with 95.1% Accuracy and 0.00 of Standard Deviation. Whereas, when looking at ROC Curve, it can be clearly observed that AUC (Area Under Curve value should be max for good results) of Logistic Regressions is 0.80 whereas AUC of SVC model is 0.56. Therefore, Logistic Regression turns out to be best model for this dataset when compared with AUC value after checking final Accuracy and Standard Deviation.\n3 boosting methods are used: Ada Boost, Gradient Boost, & XG Boost. Ada Boost was best with Accuracy 95% and Standard Deviation of 0.001.","50700198":"***Pruned Tree: HyperParameter Tunning***","44853a01":"Above, Logistic Regression and SVC turns out to be best for our dataset. These 2 are showing best result with Accuracy 95.1 % and Standard Deviation is 0.00 (Very close to 0).","67d350f8":"This model is hyperparameter tuned. This model will use 'n_neighbors' in range 1 to 30 and will try to find the best value of 'n_neighbors', which will provide the best result.","9ac3de63":"# **Bagging with all classifiers using Cross Validation:**\nNow, we will use all 5 Models we used before and after applying VotingClassifier we can get the best result by voting between all models.","722f0c15":"**Replacing missing values of BMI with age_mean values based on the age group.**","8cb391b2":"Above, Ada Boost came out to be the best boosting method for this data set. For Ada Boost, Accuracy is 95 % and Standard Deviation is 0.001.","d831201f":"## **1. KNN (K-Nearest Neighbors Algorithm)**","7db41e56":"**Confusion Matrix:**","583681e8":"**It can be observed that AGE is having best correlation 0.33 with BMI. So, I'm using age and will try to find mean age each age group arange. Now, fow missing values, I will assign those mean age values to those 201 records.\nSo, we will replace NULL values with mean values of age. Now, for each age group, I will assign a mean value to it. So, its better to have them with mean value, or else we may lose some important insights.**","67dac45b":"## **3. Decision Tree:**\nThis model is hyperparameter tunned. Unpruned Tree is basic model, however, Pruned tree is the one with tunning parameters.","42485aa4":"# **Machine Learning Models:**\nThere are 5 machine learning models used for this dataset. In the end, will compare results of each model and will try to find the best model for this dataset.","b63d7df6":"***Unpruned Treee:***","bce77d98":"## **4. Random Forest:**\nThis is hyperparameter tunned model. max_depth, n_estimators have been used as tunning parameters.","ae4272c1":"**Splitting data into Train and Test:**"}}