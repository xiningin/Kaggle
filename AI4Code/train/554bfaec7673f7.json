{"cell_type":{"f01e3817":"code","bb26f8e3":"code","1f9720a5":"code","95528855":"code","f84f683f":"code","3f261b28":"code","32330040":"code","bf128a4f":"code","57f2fb21":"code","6eab7e98":"code","cf19225b":"code","9c2ac6b3":"code","02fb0a0b":"code","17d81ae1":"code","d6ce4843":"code","bcf8804f":"code","0ac7de4d":"code","74e2830a":"code","df013b39":"code","de355b50":"code","22a33d70":"code","aebea6ab":"code","cbe9cd32":"code","9397b802":"code","1599ba80":"code","1e44d4f6":"code","7aca1774":"code","11a0dbe2":"code","0fa52748":"code","4bcf37f8":"code","5bd94a43":"code","8d862cb2":"code","a395d680":"code","8e47b2ca":"code","e7d378ef":"code","d83c2762":"code","77c6cb0c":"code","7076b65a":"code","30bcfcff":"code","507103e9":"code","b7e6b1c9":"code","3709214b":"code","50ef0c7c":"code","941dd66d":"code","c0c0245e":"markdown","869a188d":"markdown","cb85a41c":"markdown","98b833f5":"markdown","63193c4a":"markdown","240c1920":"markdown","c78d2b06":"markdown","92201cf4":"markdown","6bbec2d8":"markdown","c52c965b":"markdown","c6fa160e":"markdown","6cd7ccd6":"markdown","07f73fc8":"markdown","844b11e1":"markdown","6e6c1689":"markdown","a7263cd3":"markdown","556cc5f1":"markdown","2d071d23":"markdown","e3d8cbe9":"markdown","366999f8":"markdown","4ad23883":"markdown","5e8fa914":"markdown","e38d302d":"markdown","4ca73cb5":"markdown","f98be05f":"markdown","439c78e8":"markdown","a61d7001":"markdown","0e0acbd1":"markdown","58c9de6e":"markdown","e92e5c99":"markdown","8bedf253":"markdown","75ee16ed":"markdown","cb5dfe62":"markdown"},"source":{"f01e3817":"import numpy as np\nimport pandas as pd\nimport os\nimport gc\nimport matplotlib.pyplot as plt\n# %matplotlib inline\nfrom sklearn.preprocessing import StandardScaler\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom seaborn import countplot,lineplot, barplot\nfrom sklearn import model_selection\nfrom sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\nimport itertools\nimport json\nimport time\nfrom sklearn import linear_model\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\nimport shap\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.feature_selection import GenericUnivariateSelect, SelectPercentile, SelectKBest, f_classif, mutual_info_classif, RFE\nimport statsmodels.api as sm\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom catboost import CatBoostClassifier\nfrom collections import Counter\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.metrics import confusion_matrix\npd.set_option('max_columns', None)\nimport datetime\nimport seaborn as sns\nimport lightgbm as lgb\nfrom scipy import stats\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold, cross_val_score, GridSearchCV, RepeatedStratifiedKFold\n","bb26f8e3":"IS_LOCAL = False\nif (IS_LOCAL):\n    location = \"..\/input\/careercon\/\"\nelse:\n    location = \"..\/input\/\"\nos.listdir(location)","1f9720a5":"%%time\nX_train = pd.read_csv(os.path.join(location, 'X_train.csv'))\nX_test = pd.read_csv(os.path.join(location, 'X_test.csv'))\ny_train = pd.read_csv(os.path.join(location, 'y_train.csv'))","95528855":"print(\"Xtrain: {}\\nXtest: {}\\nytrain: {}\".format(X_train.shape, X_test.shape, y_train.shape))","f84f683f":"print('Size of the Xtrain')\nprint('Numbers of Measurements: {0}\\nNumbers of columns: {1}'.format(X_train.shape[0], X_train.shape[1]))","3f261b28":"print('Size of the Xtest')\nprint('Numbers of Measurements: {0}\\nNumbers of columns: {1}'.format(X_test.shape[0], X_test.shape[1]))","32330040":"print('Size of the Labels')\nprint('Numbers of Measurements: {0}\\nNumbers of columns: {1}'.format(y_train.shape[0], y_train.shape[1]))","bf128a4f":"def show_head(data):\n    return(data.head())","57f2fb21":"show_head(X_train)","6eab7e98":"show_head(X_test)","cf19225b":"show_head(y_train)","9c2ac6b3":"def missing_data(data):\n    total = data.isnull().sum()\n    percent = (total\/data.isnull().count()*100)\n    miss_column = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    miss_column['Types'] = types\n    return(np.transpose(miss_column))            ","02fb0a0b":"missing_data(X_train)","17d81ae1":"missing_data(X_test)","d6ce4843":"missing_data(y_train)","bcf8804f":"def describe_data(data):\n    return(data.describe())","0ac7de4d":"describe_data(X_train)","74e2830a":"describe_data(X_test)","df013b39":"describe_data(y_train)","de355b50":"Surface_count = y_train['surface'].value_counts().reset_index().rename(columns = {'index' : 'Labels'})\nSurface_count","22a33d70":"countplot(y = 'surface', data = y_train)\nplt.show()","aebea6ab":"trace = go.Pie(labels = y_train['surface'].value_counts().index,\n              values = y_train['surface'].value_counts().values,\n              domain = {'x':[0.55,1]})\n\ndata = [trace]\nlayout = go.Layout(title = 'PieChat Distribution of Floors')\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig)","cbe9cd32":"def plot_columns_distribution(df1, df2, label1, label2, columns):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(2,5,figsize=(16,8))\n\n    for col in columns:\n        i += 1\n        plt.subplot(2,5,i)\n        sns.kdeplot(df1[col], bw=0.5,label=label1)\n        sns.kdeplot(df2[col], bw=0.5,label=label2)\n        plt.xlabel(col, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=8)\n        plt.tick_params(axis='y', which='major', labelsize=8)\n    plt.show();\n    \ncolumns = X_train.columns.values[3:]\nplot_columns_distribution(X_train, X_test, 'train', 'test', columns)","9397b802":"def plot_columns_class_distribution(classes,series_group, columns):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(5,2,figsize=(16,24))\n\n    for col in columns:\n        i += 1\n        plt.subplot(5,2,i)\n        for clas in classes:\n            series_groups = series_group[series_group['surface']==clas]\n            sns.kdeplot(series_groups[col], bw=0.5,label=clas)\n        plt.xlabel(col, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=8)\n        plt.tick_params(axis='y', which='major', labelsize=8)\n    plt.show();\n    \nclasses = (y_train['surface'].value_counts()).index\nseries_group = X_train.merge(y_train, on='series_id', how='inner')\nplot_columns_class_distribution(classes, series_group, columns)","1599ba80":"def quaternion_to_euler(x, y, z, w):\n    import math\n    t0 = +2.0 * (w * x + y * z)\n    t1 = +1.0 - 2.0 * (x * x + y * y)\n    X = math.atan2(t0, t1)\n\n    t2 = +2.0 * (w * y - z * x)\n    t2 = +1.0 if t2 > +1.0 else t2\n    t2 = -1.0 if t2 < -1.0 else t2\n    Y = math.asin(t2)\n\n    t3 = +2.0 * (w * z + x * y)\n    t4 = +1.0 - 2.0 * (y * y + z * z)\n    Z = math.atan2(t3, t4)\n\n    return X, Y, Z\n\ndef data_engineering(actual):\n    new = pd.DataFrame()\n    actual['total_angular_velocity'] = (actual['angular_velocity_X'] ** 2 + actual['angular_velocity_Y'] ** 2 + actual['angular_velocity_Z'] ** 2) ** 0.5\n    actual['total_linear_acceleration'] = (actual['linear_acceleration_X'] ** 2 + actual['linear_acceleration_Y'] ** 2 + actual['linear_acceleration_Z'] ** 2) ** 0.5\n    \n    actual['acc_vs_vel'] = actual['total_linear_acceleration'] \/ actual['total_angular_velocity']\n    \n    x, y, z, w = actual['orientation_X'].tolist(), actual['orientation_Y'].tolist(), actual['orientation_Z'].tolist(), actual['orientation_W'].tolist()\n    nx, ny, nz = [], [], []\n    for i in range(len(x)):\n        xx, yy, zz = quaternion_to_euler(x[i], y[i], z[i], w[i])\n        nx.append(xx)\n        ny.append(yy)\n        nz.append(zz)\n    \n    actual['euler_x'] = nx\n    actual['euler_y'] = ny\n    actual['euler_z'] = nz\n    \n    actual['total_angle'] = (actual['euler_x'] ** 2 + actual['euler_y'] ** 2 + actual['euler_z'] ** 2) ** 5\n    actual['angle_vs_acc'] = actual['total_angle'] \/ actual['total_linear_acceleration']\n    actual['angle_vs_vel'] = actual['total_angle'] \/ actual['total_angular_velocity']\n    \n    def f1(x):\n        return np.mean(np.diff(np.abs(np.diff(x))))\n    \n    def f2(x):\n        return np.mean(np.abs(np.diff(x)))\n    \n    for col in actual.columns:\n        if col in ['row_id', 'series_id', 'measurement_number']:\n            continue\n        new[col + '_mean'] = actual.groupby(['series_id'])[col].mean()\n        new[col + '_min'] = actual.groupby(['series_id'])[col].min()\n        new[col + '_max'] = actual.groupby(['series_id'])[col].max()\n        new[col + '_std'] = actual.groupby(['series_id'])[col].std()\n        new[col + '_max_to_min'] = new[col + '_max'] \/ new[col + '_min']\n        \n        # Change. 1st order.\n        new[col + '_mean_abs_change'] = actual.groupby('series_id')[col].apply(f2)\n        \n        # Change of Change. 2nd order.\n        new[col + '_mean_change_of_abs_change'] = actual.groupby('series_id')[col].apply(f1)\n        \n        new[col + '_abs_max'] = actual.groupby('series_id')[col].apply(lambda x: np.max(np.abs(x)))\n        new[col + '_abs_min'] = actual.groupby('series_id')[col].apply(lambda x: np.min(np.abs(x)))\n\n    return new","1e44d4f6":"%%time\nxtrain = data_engineering(X_train)\nxtest = data_engineering(X_test)","7aca1774":"describe_data(xtrain)","11a0dbe2":"print(\"Xtrain: {}\\nXtest: {}\".format(X_train.shape, X_test.shape))","0fa52748":"print(\"Xtrain: {}\\nXtest: {}\".format(xtrain.shape, xtest.shape))","4bcf37f8":"show_head(xtest)","5bd94a43":"corr_xtrain = xtrain.corr()\ncorr_xtrain","8d862cb2":"colormap = plt.cm.RdBu\nplt.figure(figsize=(24,18))\nplt.title('Pearson Correlation of Features', y=1.05, size=20)\nsns.heatmap(X_train.astype(float).corr(),linewidths=0.05,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","a395d680":"le = LabelEncoder()\ny_train['surface'] = le.fit_transform(y_train['surface'])","8e47b2ca":"xtrain.fillna(0, inplace = True)\nxtrain.replace(-np.inf, 0, inplace = True)\nxtrain.replace(np.inf, 0, inplace = True)\nxtest.fillna(0, inplace = True)\nxtest.replace(-np.inf, 0, inplace = True)\nxtest.replace(np.inf, 0, inplace = True)","e7d378ef":"\nfolds = StratifiedKFold(n_splits=10, shuffle=True, random_state=11)\ndef feed_model(train,test,label,folds=folds,averaging='usual',clf=None,clf_type='rfc',params=None,\n               plot_feature_importance=False,groups=y_train['group_id']):\n    forcast = np.zeros((test.shape[0], 9))\n    con_pred = np.zeros((train.shape[0]))\n    scores = []\n    feature_importance = pd.DataFrame()\n    for fold_, (train_index, valid_index) in enumerate(folds.split(train, label, groups)):\n        print('Fold', fold_, 'started at', time.ctime())\n        x_train, x_valid = train.iloc[train_index], train.iloc[valid_index]\n        y_train, y_valid = label.iloc[train_index], label.iloc[valid_index]\n        \n        if clf_type == 'rfc':\n            clf = clf\n            clf.fit(x_train, y_train)\n            Valid_pred = clf.predict(x_valid).reshape(-1,)\n            score = accuracy_score(y_valid, Valid_pred)\n            Real_pred = clf.predict_proba(test)\n            \n        con_pred[valid_index] = clf.predict(x_valid).reshape(-1,)\n        scores.append(accuracy_score(y_valid, Valid_pred))\n        if averaging == 'usual':\n            forcast += Real_pred\n        elif averaging == 'rank':\n            forcast += pd.Series(Real_pred).rank().values\n        \n    forcast \/= folds.n_splits\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    if clf_type == 'lgb':\n        feature_importance[\"importance\"] \/= folds.n_splits\n        if plot_feature_importance:\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\",\n                                                                                                       ascending=False)[:50].index\n\n            best_features = feature_importance[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n        \n            return Valid_pred, forcast, feature_importance\n        return Valid_pred, forcast, scores\n    \n    else:\n        return Valid_pred, forcast, scores\n","d83c2762":"clf = RandomForestClassifier(n_estimators=500,n_jobs=-1,random_state=0)\ncon_pred_rfc1, forcast_rfc1, scores_rfc1 = feed_model(train=xtrain, test=xtest, label=y_train['surface'],folds=folds,clf_type='rfc',\n                                                   plot_feature_importance=True,clf=clf)","77c6cb0c":"FeatureImportance = clf.feature_importances_\nindices = np.argsort(FeatureImportance)\nfeatures = xtrain.columns\n\nhm = 30\nplt.figure(figsize=(16, 12))\nplt.title('RFC Features Avg Over Folds')\nplt.barh(range(len(indices[:hm])), FeatureImportance[indices][:hm], color='b', align='center')\nplt.yticks(range(len(indices[:hm])), [features[i] for i in indices])\nplt.xlabel('Importance')\nplt.show()","7076b65a":"feat_labels = xtrain.columns","30bcfcff":"for feature in zip(feat_labels, clf.feature_importances_):\n    print(feature)","507103e9":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import datasets\nsfm = SelectFromModel(clf, threshold=0.001)\nsfm.fit(xtrain, y_train['surface'])\nfor feature_list_index in sfm.get_support(indices=True):\n    print(feat_labels[feature_list_index])\n    \nxtrain_importance = sfm.transform(xtrain)\nxtest_importance = sfm.transform(xtest)","b7e6b1c9":"folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=11)\ndef feed_model_importance(train,test,label,folds=folds,averaging='usual',clf=None,clf_type='rfc',params=None,\n               plot_feature_importance=False,groups=y_train['group_id']):\n    forcast = np.zeros((test.shape[0], 9))\n    con_pred = np.zeros((train.shape[0]))\n    scores = []\n    feature_importance = pd.DataFrame()\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(train, label, groups)):\n        print('Fold', fold_n, 'started at', time.ctime())\n        x_train, x_valid = train[train_index], train[valid_index]\n        y_train, y_valid = label[train_index], label[valid_index]\n        \n        if clf_type == 'rfc':\n            clf = clf\n            clf.fit(x_train, y_train)\n            Valid_pred = clf.predict(x_valid).reshape(-1,)\n            score = accuracy_score(y_valid, Valid_pred)\n            Real_pred = clf.predict_proba(test)\n            \n        con_pred[valid_index] += (Valid_pred).reshape(-1,)\n        scores.append(accuracy_score(y_valid, Valid_pred))\n        if averaging == 'usual':\n            forcast += Real_pred\n        elif averaging == 'rank':\n            forcast += pd.Series(Real_pred).rank().values\n        \n    forcast \/= folds.n_splits\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    if clf_type == 'lgb':\n        feature_importance[\"importance\"] \/= n_folds\n        if plot_feature_importance:\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n        \n            return Valid_pred, forcast, feature_importance\n        return Valid_pred, forcast, scores\n    \n    else:\n        return Valid_pred, forcast, scores\n","3709214b":"clf1 = RandomForestClassifier(n_estimators=500,n_jobs=-1,random_state=0)\ncon_pred_rfc, forcast_rfc, scores_rfc = feed_model_importance(train=xtrain_importance, test=xtest_importance, label=y_train['surface'],folds=folds,clf_type='rfc',\n                                                   plot_feature_importance=False,clf=clf1)","50ef0c7c":"from keras import models\nfrom keras import layers\nfrom keras.callbacks import EarlyStopping\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nnp.random.seed(0)\n\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=11)\n\ndef create_network():\n    network = models.Sequential()\n    network.add(layers.Dense(units=16, activation='relu', input_dim= 171))\n    network.add(layers.Dense(units=16, activation='relu'))\n    network.add(layers.Dense(units=1, activation='sigmoid'))\n\n    network.compile(loss='binary_crossentropy',\n                    optimizer='adam',\n                    metrics=['accuracy'])\n    \n    # Return compiled network\n    return network\n# Wrap Keras model so it can be used by scikit-learn\nes = EarlyStopping(monitor='val_loss',mode='auto',verbose=1,patience=20)\nmc = ModelCheckpoint('best_model.h5',monitor='val_acc',mode='max',verbose=1,save_best_only=True)\nneural_network = KerasClassifier(build_fn=create_network,epochs=10,batch_size=100,verbose=0)\ncross_val_score(neural_network, xtrain, y_train['surface'],fit_params={'callbacks': [es,mc]}, cv=folds)","941dd66d":"sub = pd.read_csv(os.path.join(location,'sample_submission.csv'))\nsub['surface'] = le.inverse_transform(forcast_rfc.argmax(axis=1))\nsub.to_csv('Forcasting.csv', index=False)\nsub.head(20)","c0c0245e":"**Refrences**\n\n1. https:\/\/www.kaggle.com\/prashantkikani\/help-humanity-by-helping-robots\n2. https:\/\/www.kaggle.com\/artgor\/where-do-the-robots-drive ","869a188d":"The scores shows the importance of each feature. The higher the scores the higher the importance of the feature. The score are addedup to 100%.\n\nTo identify and select the most important features we need to select a range of threshold as a cut off point for the importance and compare. this is what we will use to train the model again and compare the importance. \n\nI ranges the cut off from 0.004 to 0.006, i noticed that the score at 0.005 was the highers. Above 0.005, the score begins to reduce.","cb85a41c":"I will like to print the shape of the files to know the topography of my data.","98b833f5":"In this code we rename the surface with Labels and also list out the numbers of floors and the type of floors. Thus shows that we are dealing with 9 classification model problem. We can also see the level of imbalance in this data","63193c4a":"I will try to use Neural Network model.\n\nThe validation as seen in the output below is low....still working on how to use cross val in a NN.\nIdeas on this will be acceted. The can also be posted in the comment section.","240c1920":"This shows that there are no missing values in both train and labels.\nI will now creat a function called describe_data, this will describe our data. We will know waht exactly the problem is in this function. this function will show us the mean, std, counts and max and min of each measurement. it seems the measurement are categorised into series.","c78d2b06":"**COUNTPLOT**","92201cf4":"We noticed that the Xtrain and Xtest have the same column but ytrain (the label) is different to Xtrain. Let us dig deep","6bbec2d8":"There is the same number of series in X_train and y_train, numbered from 0 to 3809 (total 3810). Each series have 128 measurements.\nEach series in train dataset is part of a group (numbered from 0 to 72, 72 being the half of 128).\nThe number of rows in X_train and X_test differs with 6 x 128, 128 being the number of measurements for each group.","c52c965b":"**About The Data:\n\nThe data was collected by IMU sensor data while driving a small mobile Robot over different surface in a university premises. We are asked to predict the type of floor the Robot is walking on through the test set data provided. \nThe data provided are:\n1. X_train having 13 columns\n2. y_train having 3 columns \n3. Sample_submission\n\n**Kernel\n\nIn this kernel, I will perform data preparation:\ndata visualization,\nfeatures engineering ,\nfeature selections, \nhyper-parameter optimization,\nmodelling, \nchoice of model,\npredictions and \nsubmission.\n\n","c6fa160e":"**Data Engineering**\n\nThis was borrowed from: https:\/\/www.kaggle.com\/prashantkikani\/help-humanity-by-helping-robots and https:\/\/www.kaggle.com\/artgor\/where-do-the-robots-drive ","6cd7ccd6":"**PieChartPlot**","07f73fc8":"In the plot below, we represent each counts of the surface on a barchat to clearly see the most common floor. The concrete floor is the most common this may be as a result of little funding allocated to universities lol.....","844b11e1":"Wow this is a drop. If we can tune parameters and check again","6e6c1689":"**Data Exploration:\n\nWe are to check the datas and have a better understanding on how the data are distributed. We will check the train and the test set by printing them.","a7263cd3":"From the data engineering we generated about 9 more columns making 22 columns in our train and test set","556cc5f1":"**Data Visualization:\n\nIn this section, we will visualize the label data to know which floor is most common. \n\nDensity plot, count_plot, Boxplot, frequency distribution, pie_chart, histogram_plot etc......","2d071d23":"Encoding The  Labels using labelEncoder","e3d8cbe9":"**Preparing Data for Analysis**\n\nLoading Packages","366999f8":"**INTRODUCTION:\n\nThe Humanoid concept was first introduced in the early ages through comlplex mechanics and electricity (Tesla). This concept has its debut not in the 90's but dated back to the ancient world. They can be errorniously termed the 'The Magic of The Ages'. The first use of this mechakraft was in industries to boost production with less human efforts. Digital control with AI developed machines were introduced in 2000's based on volume of data presently available.\n\nThe three-legged walking table of the Greeks, the humanoid text reader of the Buddhist, the stolen technology by the the Indians from Rome to build the automated soldiers which protects the relics of Buddha, Albertus android which has the capacity to perform domestic task, mechanical imitations of animals and demons invented by the early chinese and so on are prooves of existence of this technical demons as some fanatics might have called it. An holistic study of Robot mophology left me with the memories of the gods. How strong is our assurance that some of this inventions are not worshiped as gods in the primitive era? If doubtful, then Robots and gods can be lyrically alike or synonymous from human perception (Let me leave this for the deep thinkers and readers of history to phantom that out).\n\nThe almighty Tesla advanced this mechakraft inventions to a point of domestic use through the ability to remotely control them in 1910's. I still remember the great unveiling of ELEKTRO 'the seven feet tall Robot that can walk by voice command in USA'. It has ability to speak 700 words. The giant toys for the rich kids. It can even smokes cigar lol....By reflections through my knowledge, this gives me a great feeling about this competition. A chance to be part of Robot creation or can I say gods creation? \nFor more deep thoughts write me or vote this kernel.\nNow lets get started with the assignment before us.\nI hope this kernel will be of a great help.\n        ","4ad23883":"**Submission**\n\nI will submit this values for now. \n\nwork will continue on the hyper parameters and feature selections.......","5e8fa914":"**Density Plot**\n\nI will show the density plot of the variables in train abd test set. These plots will be represented in different colors of different surface values.\n\nThis plot shows how the surface are distributed over the columns and how columns are distributed over the test and train dataset.\n\nFrom the density plot below, the following are noticed:\n\n1. Orientation x, y are not normally distributed\n2. Orientation z, w are normally distributed\n3. linear_accelaration are normally distributed","e38d302d":"I built a function called show_head to show us the first five elements of our data","4ca73cb5":"**Correlation Matrix**","f98be05f":"**Pearson Correlation**","439c78e8":"**Feature Importance and Visualization**\n\nThis graph shows the level of importance of each features. It will help to determine feature selection later.","a61d7001":"I will like to check if there are missing values because the ytrain and the xtrain are not equal. So i will build a function for that called missing_data comprising of total, percentage of missing data and data type of each column","0e0acbd1":"**Introducing The Model**\n\nFirstly, RandomForestClassifier model will be used because i believe is the best random for this type od classification.\n\nI will run trough feature selection process and four other models will be introduced \n\nThe choice of model will later be made base on some few creterials that will be listed later.","58c9de6e":"**Confusion Matrix**\n\nThis helps to know the effective of the model. The true positive and true negative are important. \n\nHeavily copied form :https:\/\/www.kaggle.com\/prashantkikani\/help-humanity-by-helping-robots","e92e5c99":"Work Continues.............","8bedf253":"If you like this kernel kindly upvote \n\nAny help on using Kfolds validation in Neural Network will be highly appreciated.","75ee16ed":"In this code, we are to load the data into our working environment for further engineering and predictions.","cb5dfe62":"**Loading Data**\n\nWe will check the data files that are available and also print its formats. This helps us to know the real data we are working with. The nature of the data and how the data was collated.\nThis is very crucial in for data cleansing and visualisation.\nThe first code below is to access the location of the file and to check for the available file."}}