{"cell_type":{"d0842ad0":"code","51344793":"code","fbb06ebe":"code","4e952d8e":"code","4a9b5a91":"code","b908b1f0":"code","a95f7b8f":"code","da3019e9":"code","f62de416":"code","a4242344":"code","4e8da724":"code","a3bc762b":"code","44fc1425":"code","5c273e4c":"code","753ba23e":"code","a26f6d9f":"code","464b3ffd":"code","2b0f3625":"code","5b4a8405":"code","70040d5e":"code","071967ff":"code","5d73ef90":"code","8cea676b":"code","16138db1":"markdown","a63c21a9":"markdown","a52d9a24":"markdown","f18b96de":"markdown","937767b6":"markdown","361e8f0c":"markdown","38dc06aa":"markdown","5758af02":"markdown","6c83e405":"markdown","d61fa08c":"markdown","55f68f7b":"markdown","37b3d4b1":"markdown","520dcbf3":"markdown"},"source":{"d0842ad0":"# load dependencies\nfrom fastai.imports import *\nfrom fastai.text import *\nfrom fastai.tabular import *\n\n# extend torch.utils.data.dataset.Dataset to provide us with x1 and x2\n# x1 will be used for the tabular submodule and x2 for the text submodule\nclass ConcatDataset(Dataset):\n    def __init__(self, x1, x2, y): self.x1, self.x2, self.y = x1, x2, y\n    def __len__(self): return len(self.y)\n    def __getitem__(self, i): return (self.x1[i], self.x2[i]), self.y[i]\n\n    \n# define a custom collate function to be used to create a Concat DataBunch later\ndef tabtext_collate(batch):\n    x, y = list(zip(*batch))\n    # x1 is (cat,cont), x2 is numericalized ids for text\n    x1, x2 = list(zip(*x))\n    x1 = to_data(x1)\n    x1 = list(zip(*x1))\n    x1 = torch.stack(x1[0]), torch.stack(x1[1])\n    x2, y = pad_collate(list(zip(x2, y)), pad_idx=1, pad_first=True)\n    return (x1, x2), y\n\n\n# custom module to concatentate the outputs of the tabular and text submodules and add the Multilayer Perceptron on top\nclass ConcatModel(nn.Module):\n    def __init__(self, mod_tab, mod_nlp, layers, drops):\n        super().__init__()\n        self.mod_tab = mod_tab\n        self.mod_nlp = mod_nlp\n        lst_layers = []\n        activs = [nn.ReLU(inplace=True), ] * (len(layers)-2) + [None]\n        for n_in, n_out, p, actn in zip(layers[:-1], layers[1:], drops, activs):\n            lst_layers += bn_drop_lin(n_in, n_out, p=p, actn=actn)\n        self.layers = nn.Sequential(*lst_layers)\n\n    def forward(self, *x):\n        x_tab = self.mod_tab(*x[0])\n        x_nlp = self.mod_nlp(x[1])[0]\n        x = torch.cat([x_tab, x_nlp], dim=1)\n        return self.layers(x)","51344793":"# setup path variables to load data and save intermediate files\nDATA_PATH = Path('..\/input')\nMODEL_LM_PATH = Path('models_lm')\nMODEL_LM_PATH.mkdir(exist_ok=True)\nMODEL_TEXT_TAB_PATH = Path('models_text_tab')\nMODEL_TEXT_TAB_PATH.mkdir(exist_ok=True)","fbb06ebe":"## load data\ndf_train = pd.read_csv(DATA_PATH\/'train.csv')\ndf_valid = pd.read_csv(DATA_PATH\/'valid.csv')\nprint(f'{df_train.shape[0]} datapoints in training set')\nprint(f'{df_valid.shape[0]} datapoints in validation set')\n\n# bring data in the right format to convert it to a fastai Databunch\ndf_all = pd.concat([df_train, df_valid]).reset_index(drop=True)\n# save indexes of validation datapoints\nval_idxs = np.arange(df_train.shape[0], df_all.shape[0])\n\n# show one exmple for each label type\ndf_all.groupby('label', group_keys=False).apply(lambda df: df.sample(1))","4e952d8e":"# set batch size\nbs = 32\n\n# create Databunch (https:\/\/docs.fast.ai\/text.data.html)\ndata_lm = (TextList.from_df(df_all, cols=['text'])\n                           .split_by_idx(val_idxs)\n                           .label_for_lm()  \n                           .databunch(path=MODEL_LM_PATH, bs=bs))","4a9b5a91":"# inspect the text after tokenization \ndata_lm.show_batch()","b908b1f0":"# save for later use\ndata_lm.save('data_lm.pkl')","a95f7b8f":"# create learner (https:\/\/docs.fast.ai\/text.learner.html)\nlearn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.5, path=MODEL_LM_PATH)","da3019e9":"# use the learning rate finder \nlearn.lr_find()\nlearn.recorder.plot()","f62de416":"learn.fit_one_cycle(1, 1e-2, moms=(0.8,0.7)) ","a4242344":"# To complete the fine-tuning, we can then unfeeze and launch a new training.\nlearn.unfreeze()","4e8da724":"# use the learning rate finder again\nlearn.lr_find()\nlearn.recorder.plot()","a3bc762b":"learn.fit_one_cycle(1, 1e-3, moms=(0.8,0.7)) ","44fc1425":"learn.save_encoder('fine_tuned_enc')","5c273e4c":"cat_names = ['fake_cats'] # categorical\ncont_names = [] # continous variables (in our case we don't have any) \ndep_var = 'label' # label\nprocs = [FillMissing, Categorify, Normalize] # preprocessing steps\ntxt_cols=['text'] # text\nbs = 64 # batch size","753ba23e":"# get tabular Databunch from the fast.ai Tabular module (docs: https:\/\/docs.fast.ai\/tabular.data.html)\ndef get_tab_databunch(df,bs=bs,val_idxs=val_idxs, path = MODEL_TEXT_TAB_PATH):\n    return (TabularList.from_df(df, cat_names, cont_names, procs=procs, path=path)\n                            .split_by_idx(val_idxs)\n                            .label_from_df(cols=dep_var)\n                            .databunch(bs=bs))\n\n# get Text Databunch from the fast.ai Text module (docs: https:\/\/docs.fast.ai\/text.data.html)\ndef get_text_databunch(df,bs=bs,val_idxs=val_idxs, path = MODEL_TEXT_TAB_PATH):\n    # load fine-tuned vocab\n    data_lm =load_data(MODEL_LM_PATH, 'data_lm.pkl', bs=bs)\n    return (TextList.from_df(df, cols = txt_cols[0], vocab=data_lm.vocab, path=path)\n                            .split_by_idx(val_idxs)\n                            .label_from_df(cols=dep_var)\n                            .databunch(bs=bs))","a26f6d9f":"\n# get fast.ai tabular learner (docs: https:\/\/docs.fast.ai\/tabular.models.html)\ndef get_tabular_learner(data, params):\n    return tabular_learner(data,metrics=accuracy,**params)\n\n# get fast.ai text learner and load the fine-tuned weights (docs: https:\/\/docs.fast.ai\/text.learner.html)\ndef get_text_learner(data, params):\n    learn = text_classifier_learner(data,AWD_LSTM,metrics=accuracy,**params)\n    learn.path=MODEL_LM_PATH\n    learn.load_encoder('fine_tuned_enc')\n    learn.path=MODEL_TEXT_TAB_PATH\n    return learn","464b3ffd":"def get_data(bs=bs,path=MODEL_TEXT_TAB_PATH):\n    \n    # get databunches using fast.ai's implementation\n    tab_db = get_tab_databunch(df_all[cat_names + cont_names+ [dep_var]])\n    text_db = get_text_databunch(df_all[txt_cols +[dep_var]])\n    \n    # extend the databunches to work with our Concat Model\n    train_ds = ConcatDataset(tab_db.train_ds.x, text_db.train_ds.x, tab_db.train_ds.y)\n    valid_ds = ConcatDataset(tab_db.valid_ds.x, text_db.valid_ds.x, tab_db.valid_ds.y)\n    \n    # add sample startegies for both training and validation set\n    train_sampler = SortishSampler(text_db.train_ds.x, key=lambda t: len(text_db.train_ds[t][0].data), bs=bs\/\/2)\n    valid_sampler = SortSampler(text_db.valid_ds.x, key=lambda t: len(text_db.valid_ds[t][0].data))\n    \n    # create DataLoaders\n    train_dl = DataLoader(train_ds, bs\/\/2, sampler=train_sampler)\n    valid_dl = DataLoader(valid_ds, bs, sampler=valid_sampler)\n    \n    # create databunch to work with out Concat Model\n    data = DataBunch(train_dl, valid_dl, device=defaults.device, collate_fn=tabtext_collate, path=path)\n\n    return data,tab_db,text_db","2b0f3625":"# instantiate all necessary DataBunches\ndata,tab_db,text_db = get_data(bs=bs)","5b4a8405":"def get_concat_learner(data, tab_db, text_db):\n    \n    # output size of the last layer of the tabular module\n    tab_out_size = 100\n    # output size of the last layer of the text module (ULMFiT)\n    text_out_size = 300\n    \n    params = {\n        'layers': [tab_out_size], # output size of the last layer of the tabular module\n        'ps': [0.], # dropout before the last layer of the tabular module\n        'emb_drop': 0., # embedding dropout for categorical features\n        'use_bn': True, # use batchnorm\n    }\n    \n    # instantiate the tabular learner\n    tab_learner = get_tabular_learner(tab_db, params)\n    # cut off layers so we can put the Concat head on top\n    tab_learner.model.layers = tab_learner.model.layers[:-2]\n\n    params = {\n        'lin_ftrs': [text_out_size],  # output size of the last layer of the text module (ULMFiT)\n        'bptt': 80, # backpropagation through time (sequence length)\n        'max_len': 20*80, # last max_len activations to be considered (see https:\/\/docs.fast.ai\/text.learner.html)\n        'drop_mult': 1. # dropout rate\n    }\n    \n    # instantiate the text learner\n    text_learner = get_text_learner(text_db, params)\n    # cut off layers so we can put the Concat head on top\n    text_learner.model[-1].layers = text_learner.model[-1].layers[:-3]\n    \n    # size of fully connected layers in Concat Head\n    lin_layers = [tab_out_size + text_out_size, 200, 100]\n    # dropout rate for fully connected layers in Concat Head\n    ps = [0.3, 0.2, 0.1]\n    model = ConcatModel(tab_learner.model, text_learner.model, lin_layers, ps)\n    \n    # Use Mean Squared Error here if you want to use this for a regression task\n    loss_func = CrossEntropyFlat()\n   \n    learn = Learner(data, model, loss_func=loss_func, metrics=accuracy)\n    \n    return learn","70040d5e":"# instantiate the learner object\nlearner = get_concat_learner(data,tab_db,text_db)","071967ff":"learner.model","5d73ef90":"# use the learning rate finder \nlearner.lr_find()\nlearner.recorder.plot(skip_end=1)","8cea676b":"learner.fit_one_cycle(2, 1e-2, moms=(0.8,0.7)) ","16138db1":"# Metadata Enhanced Text Classification\n\nIn this Kernel we explain how we extended tha fastai library to create a **Concat Model** that combines both text and categorical\/continous input features to perform the task of classification. If you want to know how we used this at Reply.ai to make a real-time Assistant to increase the efficiency of Customer Service teams, please check out [this article](https:\/\/medium.com\/reply-ai\/next-best-action-prediction-with-text-and-metadata-building-an-agent-assistant-81117730be6b?source=friends_link&sk=8c8a7e1fcc76468649431e8a1c792235).\n\n# Concat Model Arquitecture\n![](https:\/\/cdn-images-1.medium.com\/max\/1600\/1*cwBGKvmxJYldV2wX0kLo-Q.png)\n\n\nHere is a very high-level explanation of the model architecture we propose. (Feel free to [hit us up](mailto:andy@reply.ai) for details in the comments or let\u2019s have a more technical conversation in [this thread](https:\/\/forums.fast.ai\/t\/build-mixed-databunch-and-train-end-to-end-model-for-tabular-categorical-continuous-data-and-text-data\/43155\/11) on the fast.ai forum.)\n\nNeural Nets (NN) can be composed of other smaller NNs. This type of modular architecture consists of multiple different networks that function to perform sub-tasks. They can interact with each other or work independently towards achieving the output. The proposed implementation makes use of this property to create an end-t0-end solution in three stages:\n\n1. **Feature Extraction:** The input layer consists of two submodules. The text data is passed to a cutting edge NN solution for text classification called ULMFiT and the tabular metadata is handled by the fast.ai tabular NN standard implementation. These two modules are not connected with each other and work independently to extract features from their respective input data. Each of them outputs a vector to represent those features.\n2. **Feature Combination:** In a second step, we combine the output from the previous modules into one by simply concatenating the two vectors.\n3. **Classification:** Finally, we feed the concatenated vector into a third module to decode the represented features and classify them. For this task, a Multilayer Perceptron is used, which is the standard approach.\n\n","a63c21a9":"## Combine both Models into one Concat Model","a52d9a24":"## Text Tabular Model\n\nOnce we have a fine-tuned language model we can go on with our Text Tabular Model.","f18b96de":"# Code to extend Fast.ai\n\nHere we define all necessary classes and functions to extend the fastai library. Fastai contains two separate modules for each text and tabular metadata. The goal is to combine both maintaining as much functionality already implemented as possible and create a wrapper that adds the Concat head.\n","937767b6":"# Let's try this out!\n\nWe created a short experiment so you can convince yourself that our Concat Model actually works and run some code yourself. To keep things simple we generated synthetic data based on the IMDB movie reviews dataset. But feel free to plug in your own dataset and [tell us](mailto:andy@reply.ai) how it goes. Our solution is built on top of the fast.ai library. We will not go into the details of the workings of fast.ai, but provide you with links to their excellent [documentation ](https:\/\/docs.fast.ai\/)in case you want to dive deeper.\n","361e8f0c":"### Inspect resulting Model","38dc06aa":"## Setup Tabular and Text Models following the Fast.ai Standard Approaches\n\nWe define functions to easly create Databunches and Learner objects for later use in the combined Concate Model.","5758af02":"### Training","6c83e405":"## Finetune the Language Model\n\nNow we need to finetune a Language Model for our IMDB corpus using transfer learning. If this sounds strange to you I recommend you check out [Fast.ai lesson 4](https:\/\/course.fast.ai\/videos\/?lesson=4). The code is very similar to what you can find in the [lesson notebook](https:\/\/github.com\/fastai\/course-v3\/blob\/master\/nbs\/dl1\/lesson3-imdb.ipynb). \n\n","d61fa08c":"### Create Concat Learner\n\nThis is where the real meat is. Here you can set all the parameter to create the Concat Model arquitecture.","55f68f7b":"## Generate synthetic data set based on IMDB\nFor the experiment a synthetic dataset based on IMDB movie review data (the dataset comes preinstalled with fast.ai) was created. IMDB is one of the most popular datasets for text classification and consists of movie reviews written by users. The reviews are labeled as positive or negative depending on the user\u2019s rating. \u201cFake\u201d tabular metadata in form of randomly chosen boolean values was added to see how the model would perform. The target labels of the new synthetic dataset IMDB + fake meta data are a combination of the review sentiment and the boolean value. E.g a datapoint with a positive review and a false boolean value would have the new target label \u201cpositive_false\u201d. \n","37b3d4b1":"### Train","520dcbf3":"### Create Databunches"}}