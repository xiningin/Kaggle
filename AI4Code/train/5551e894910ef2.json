{"cell_type":{"2ae11801":"code","5fa0ce91":"code","acbd74bd":"code","0668b057":"code","fd0ffdc8":"code","cd55b3bd":"code","5267144d":"code","7e24bbad":"code","e2672c22":"code","ace29170":"code","89688932":"code","c2c98080":"code","4e36c15a":"code","3680752e":"markdown"},"source":{"2ae11801":"#All imports her\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings(\"ignore\")","5fa0ce91":"data_asm_byte_final = pd.read_csv(\"..\/input\/data_asm_byte_final.csv\", index_col = 0)\ndata_asm_byte_final.head()","acbd74bd":"final_y = data_asm_byte_final[\"Class\"]\ndata_asm_byte_final = data_asm_byte_final.drop(\"Class\", axis=1)","0668b057":"#Let's normalize the data.\ndef normalize(dataframe):\n    #print(\"Here\")\n    test = dataframe.copy()\n    for col in tqdm(test.columns):\n        if(col != \"Id\" and col !=\"Class\"):\n            max_val = max(dataframe[col])\n            min_val = min(dataframe[col])\n            test[col] = (dataframe[col] - min_val) \/ (max_val-min_val)\n    return test","fd0ffdc8":"data_asm_byte_final = normalize(data_asm_byte_final)","cd55b3bd":"data_asm_byte_final.head()","5267144d":"data_y = final_y\n# split the data into test and train by maintaining same distribution of output varaible 'y_true' [stratify=y_true]\nx_train, x_test, y_train, y_test = train_test_split(data_asm_byte_final.drop(['Id'], axis=1), data_y,stratify=data_y,test_size=0.20)\n# split the train data into train and cross validation by maintaining same distribution of output varaible 'y_train' [stratify=y_train]\nx_train, x_cv, y_train, y_cv = train_test_split(x_train, y_train,stratify=y_train,test_size=0.20)","7e24bbad":"x_cfl=XGBClassifier()\n\nprams={\n    'learning_rate':[0.01,0.03,0.05,0.1,0.15,0.2],\n     'n_estimators':[100,200,500,1000,2000],\n     'max_depth':[3,5,10],\n    'colsample_bytree':[0.1,0.3,0.5,1],\n    'subsample':[0.1,0.3,0.5,1]\n}\nrandom_cfl=RandomizedSearchCV(x_cfl,param_distributions=prams,verbose=10,n_jobs=-1,)\nrandom_cfl.fit(x_train, y_train)","e2672c22":"print (random_cfl.best_params_)","ace29170":"best_param_dict = random_cfl.best_params_","89688932":" best_param_dict['subsample']","c2c98080":"x_cfl=XGBClassifier(n_estimators = best_param_dict['n_estimators'], max_depth = best_param_dict['max_depth'], learning_rate = best_param_dict['learning_rate'], colsample_bytree = best_param_dict['colsample_bytree'], subsample = best_param_dict['subsample'],nthread=-1)\nx_cfl.fit(x_train,y_train,verbose=True)","4e36c15a":"predict_y = x_cfl.predict_proba(x_train)\nprint (\"The train log loss is:\",log_loss(y_train, predict_y))\npredict_y = x_cfl.predict_proba(x_cv)\nprint(\"The cross validation log loss is:\",log_loss(y_cv, predict_y))\npredict_y = x_cfl.predict_proba(x_test)\nprint( \"The test log loss is:\",log_loss(y_test, predict_y))\n#plot_confusion_matrix(y_test,x_cfl.predict(x_test))","3680752e":"# XgBoost Classifier on final features with best hyper parameters using Random search"}}