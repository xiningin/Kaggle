{"cell_type":{"c897204f":"code","3dbe7818":"code","f962b797":"code","72bc4292":"code","35f3f9d0":"code","edd39aec":"code","df609457":"code","aa0764bb":"code","10a24ff4":"code","a15615c9":"code","da7ea21a":"code","9a425d81":"code","dafca358":"code","1f178661":"code","9db07f4f":"code","bdeb260d":"code","2cb9e5c1":"code","06671d9e":"code","c9d7d365":"code","3350c39c":"code","3cff35a1":"code","9d3365ca":"code","cefb5584":"code","f7e5f984":"code","9960aa07":"code","8160cfb8":"code","e8b1af41":"code","24fb844e":"code","9588bc01":"code","1bb8943a":"markdown","d5baeb78":"markdown","8f49db06":"markdown","11900005":"markdown","3ebfe493":"markdown","45dfbd7e":"markdown"},"source":{"c897204f":"!pip install git+https:\/\/download.radtorch.com\/ -q\n!pip install torchcam\n# !mkdir models","3dbe7818":"import os\nimport cv2\nimport glob\nimport tqdm\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image, ImageOps, ImageEnhance\n\nimport matplotlib.pyplot as plt\n\nfrom radtorch import pipeline, core\nfrom radtorch.settings import *\nfrom radtorch.utils.utils import *\n\n%matplotlib inline","f962b797":"# part = 'ELBOW'\n# part = 'FINGER'\n# part = 'FOREARM'\n# part = 'HAND'\n# part = 'HUMERUS'\npart = 'SHOULDER'\n# part = 'WRIST'\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","72bc4292":"train_data_root = '..\/input\/mura-v11\/MURA-v1.1\/train_image_paths.csv'\nx = pd.read_csv(train_data_root, header=None, names=['PATH'])\n\n\ndata_files = []\ndata_labels = []\nfor i, r in x.iterrows():\n    if part in r['PATH']:\n        data_files.append('..\/input\/mura-v11\/'+r['PATH'])\n        if 'positive' in r['PATH']:\n            data_labels.append('POSITIVE')\n        else:\n            data_labels.append('NEGATIVE')\n\ntrain_data = pd.DataFrame(zip(data_files, data_labels), columns=['PATH', 'LABEL'])\ntrain_data","35f3f9d0":"# tmp_clf = pipeline.Image_Classification(data_directory = '..\/input\/mura-v11\/MURA-v1.1\/train', \n#                                     table=train_data, \n#                                     image_path_column='PATH', \n#                                     image_label_column='LABEL',\n#                                     model_arch='resnet50',\n#                                     type='nn_classifier',\n#                                     epochs=20,\n#                                     custom_resize=256,\n#                                     balance_class=True,\n#                                     auto_save=True)","edd39aec":"# tmp_clf.run()","df609457":"# model = tmp_clf.classifier.trained_model\n# model","aa0764bb":"# torch.save(model, '.\/models\/'+part+'_resnet152.pth')","10a24ff4":"\"\"\"\n@author: Yuchi Ishikaw - https:\/\/github.com\/yiskw713\n\"\"\"\n\n\"\"\"\nCode obtained from Source (https:\/\/github.com\/yiskw713\/ScoreCAM) with modifications.\n\"\"\"\n\nclass SaveValues():\n    def __init__(self, m):\n        # register a hook to save values of activations and gradients\n        self.activations = None\n        self.gradients = None\n        self.forward_hook = m.register_forward_hook(self.hook_fn_act)\n        self.backward_hook = m.register_backward_hook(self.hook_fn_grad)\n\n    def hook_fn_act(self, module, input, output):\n        self.activations = output\n\n    def hook_fn_grad(self, module, grad_input, grad_output):\n        self.gradients = grad_output[0]\n\n    def remove(self):\n        self.forward_hook.remove()\n        self.backward_hook.remove()\n\nclass CAM(object):\n    \"\"\" Class Activation Mapping \"\"\"\n\n    def __init__(self, model, target_layer, device):\n        \"\"\"\n        Args:\n            model: a base model to get CAM which have global pooling and fully connected layer.\n            target_layer: conv_layer before Global Average Pooling\n        \"\"\"\n\n        self.model = model\n        self.target_layer = target_layer\n\n        # save values of activations and gradients in target_layer\n        self.values = SaveValues(self.target_layer)\n        self.device=device #<<<<<<<<<<<<<<<<\n\n    def forward(self, x, idx=None):\n        \"\"\"\n        Args:\n            x: input image. shape =>(1, 3, H, W)\n        Return:\n            heatmap: class activation mappings of the predicted class\n        \"\"\"\n\n        # object classification\n        score = self.model(x)\n\n        prob = F.softmax(score, dim=1)\n\n        if idx is None:\n            prob, idx = torch.max(prob, dim=1)\n            idx = idx.item()\n            prob = prob.item()\n            print(\"predicted class ids {}\\t probability {}\".format(idx, prob))\n\n        # cam can be calculated from the weights of linear layer and activations\n        weight_fc = list(\n            self.model._modules.get('fc').parameters())[0].to(self.device).data #<<<<<<<<<<<<<<<<\n\n        cam = self.getCAM(self.values, weight_fc, idx)\n\n        return cam, idx\n\n    def __call__(self, x):\n        return self.forward(x)\n\n    def getCAM(self, values, weight_fc, idx):\n        '''\n        values: the activations and gradients of target_layer\n            activations: feature map before GAP.  shape => (1, C, H, W)\n        weight_fc: the weight of fully connected layer.  shape => (num_classes, C)\n        idx: predicted class id\n        cam: class activation map.  shape => (1, num_classes, H, W)\n        '''\n\n        cam = F.conv2d(values.activations, weight=weight_fc[:, :, None, None])\n        _, _, h, w = cam.shape\n\n        # class activation mapping only for the predicted class\n        # cam is normalized with min-max.\n        cam = cam[:, idx, :, :]\n        cam -= torch.min(cam)\n        cam \/= torch.max(cam)\n        cam = cam.view(1, 1, h, w)\n\n        return cam.data\n    \nclass GradCAM(CAM):\n    \"\"\" Grad CAM \"\"\"\n\n    def __init__(self, model, target_layer, device):\n        super().__init__(model, target_layer, device)\n\n        \"\"\"\n        Args:\n            model: a base model to get CAM, which need not have global pooling and fully connected layer.\n            target_layer: conv_layer you want to visualize\n        \"\"\"\n\n    def forward(self, x, idx=None):\n        \"\"\"\n        Args:\n            x: input image. shape =>(1, 3, H, W)\n            idx: the index of the target class\n        Return:\n            heatmap: class activation mappings of the predicted class\n        \"\"\"\n\n        # anomaly detection\n        score = self.model(x)\n\n        prob = F.softmax(score, dim=1)\n\n        if idx is None:\n            prob, idx = torch.max(prob, dim=1)\n            idx = idx.item()\n            prob = prob.item()\n#             print(\"predicted class ids {}\\t probability {}\".format(idx, prob))\n\n        # caluculate cam of the predicted class\n        cam = self.getGradCAM(self.values, score, idx)\n\n        return cam, idx\n\n    def __call__(self, x):\n        return self.forward(x)\n\n    def getGradCAM(self, values, score, idx):\n        '''\n        values: the activations and gradients of target_layer\n            activations: feature map before GAP.  shape => (1, C, H, W)\n        score: the output of the model before softmax\n        idx: predicted class id\n        cam: class activation map.  shape=> (1, 1, H, W)\n        '''\n\n        self.model.zero_grad()\n\n        score[0, idx].backward(retain_graph=True)\n\n        activations = values.activations\n        gradients = values.gradients\n        n, c, _, _ = gradients.shape\n        alpha = gradients.view(n, c, -1).mean(2)\n        alpha = alpha.view(n, c, 1, 1)\n\n        # shape => (1, 1, H', W')\n        cam = (alpha * activations).sum(dim=1, keepdim=True)\n        cam = F.relu(cam)\n        cam -= torch.min(cam)\n        cam \/= torch.max(cam)\n\n        return cam.data\n\n\nclass GradCAMpp(CAM):\n    \"\"\" Grad CAM plus plus \"\"\"\n\n    def __init__(self, model, target_layer, device):\n        super().__init__(model, target_layer, device)\n        \"\"\"\n        Args:\n            model: a base model\n            target_layer: conv_layer you want to visualize\n        \"\"\"\n\n    def forward(self, x, idx=None):\n        \"\"\"\n        Args:\n            x: input image. shape =>(1, 3, H, W)\n            idx: the index of the target class\n        Return:\n            heatmap: class activation mappings of predicted classes\n        \"\"\"\n\n        # object classification\n        score = self.model(x)\n\n        prob = F.softmax(score, dim=1)\n\n        if idx is None:\n            prob, idx = torch.max(prob, dim=1)\n            idx = idx.item()\n            prob = prob.item()\n            print(\"predicted class ids {}\\t probability {}\".format(idx, prob))\n\n        # caluculate cam of the predicted class\n        cam = self.getGradCAMpp(self.values, score, idx)\n\n        return cam, idx\n\n    def __call__(self, x):\n        return self.forward(x)\n\n    def getGradCAMpp(self, values, score, idx):\n        '''\n        values: the activations and gradients of target_layer\n            activations: feature map before GAP.  shape => (1, C, H, W)\n        score: the output of the model before softmax. shape => (1, n_classes)\n        idx: predicted class id\n        cam: class activation map.  shape=> (1, 1, H, W)\n        '''\n\n        self.model.zero_grad()\n\n        score[0, idx].backward(retain_graph=True)\n\n        activations = values.activations\n        gradients = values.gradients\n        n, c, _, _ = gradients.shape\n\n        # calculate alpha\n        numerator = gradients.pow(2)\n        denominator = 2 * gradients.pow(2)\n        ag = activations * gradients.pow(3)\n        denominator += ag.view(n, c, -1).sum(-1, keepdim=True).view(n, c, 1, 1)\n        denominator = torch.where(\n            denominator != 0.0, denominator, torch.ones_like(denominator))\n        alpha = numerator \/ (denominator + 1e-7)\n\n        relu_grad = F.relu(score[0, idx].exp() * gradients)\n        weights = (alpha * relu_grad).view(n, c, -1).sum(-1).view(n, c, 1, 1)\n\n        # shape => (1, 1, H', W')\n        cam = (weights * activations).sum(1, keepdim=True)\n        cam = F.relu(cam)\n        cam -= torch.min(cam)\n        cam \/= torch.max(cam)\n\n        return cam.data\n\nclass SmoothGradCAMpp(CAM):\n    \"\"\" Smooth Grad CAM plus plus \"\"\"\n\n    def __init__(self, model, target_layer, device, n_samples=25, stdev_spread=0.15):\n        super().__init__(model, target_layer, device)\n        \"\"\"\n        Args:\n            model: a base model\n            target_layer: conv_layer you want to visualize\n            n_sample: the number of samples\n            stdev_spread: standard deviation\u00df\n        \"\"\"\n\n        self.n_samples = n_samples\n        self.stdev_spread = stdev_spread\n\n    def forward(self, x, idx=None):\n        \"\"\"\n        Args:\n            x: input image. shape =>(1, 3, H, W)\n            idx: the index of the target class\n        Return:\n            heatmap: class activation mappings of predicted classes\n        \"\"\"\n\n        stdev = self.stdev_spread \/ (x.max() - x.min())\n        std_tensor = torch.ones_like(x) * stdev\n\n        indices = []\n        probs = []\n\n        for i in range(self.n_samples):\n            self.model.zero_grad()\n\n            x_with_noise = torch.normal(mean=x, std=std_tensor)\n            x_with_noise.requires_grad_()\n\n            score = self.model(x_with_noise)\n\n            prob = F.softmax(score, dim=1)\n\n            if idx is None:\n                prob, idx = torch.max(prob, dim=1)\n                idx = idx.item()\n                probs.append(prob.item())\n\n            indices.append(idx)\n\n            score[0, idx].backward(retain_graph=True)\n\n            activations = self.values.activations\n            gradients = self.values.gradients\n            n, c, _, _ = gradients.shape\n\n            # calculate alpha\n            numerator = gradients.pow(2)\n            denominator = 2 * gradients.pow(2)\n            ag = activations * gradients.pow(3)\n            denominator += \\\n                ag.view(n, c, -1).sum(-1, keepdim=True).view(n, c, 1, 1)\n            denominator = torch.where(\n                denominator != 0.0, denominator, torch.ones_like(denominator))\n            alpha = numerator \/ (denominator + 1e-7)\n\n            relu_grad = F.relu(score[0, idx].exp() * gradients)\n            weights = \\\n                (alpha * relu_grad).view(n, c, -1).sum(-1).view(n, c, 1, 1)\n\n            # shape => (1, 1, H', W')\n            cam = (weights * activations).sum(1, keepdim=True)\n            cam = F.relu(cam)\n            cam -= torch.min(cam)\n            cam \/= torch.max(cam)\n\n            if i == 0:\n                total_cams = cam.clone()\n            else:\n                total_cams += cam\n\n        total_cams \/= self.n_samples\n        idx = mode(indices)\n        prob = mean(probs)\n\n        print(\"predicted class ids {}\\t probability {}\".format(idx, prob))\n\n        return total_cams.data, idx\n\n    def __call__(self, x):\n        return self.forward(x)\n\n\nclass ScoreCAM(CAM):\n    \"\"\" Score CAM \"\"\"\n\n    def __init__(self, model, target_layer, device, n_batch=32):\n        super().__init__(model, target_layer, device)\n        \"\"\"\n        Args:\n            model: a base model\n            target_layer: conv_layer you want to visualize\n        \"\"\"\n        self.n_batch = n_batch\n        self.device=device #<<<<<<<<<<<<<<<<\n\n    def forward(self, x, idx=None):\n        \"\"\"\n        Args:\n            x: input image. shape =>(1, 3, H, W)\n            idx: the index of the target class\n        Return:\n            heatmap: class activation mappings of predicted classes\n        \"\"\"\n\n        with torch.no_grad():\n            _, _, H, W = x.shape\n            device = x.device\n\n            self.model.zero_grad()\n            score = self.model(x)\n            prob = F.softmax(score, dim=1)\n\n            if idx is None:\n                p, idx = torch.max(prob, dim=1)\n                idx = idx.item()\n                # print(\"predicted class ids {}\\t probability {}\".format(idx, p))\n\n            # # calculate the derivate of probabilities, not that of scores\n            # prob[0, idx].backward(retain_graph=True)\n\n            self.activations = self.values.activations.to(self.device).clone() #<<<<<<<<<<<<<<<<\n            # put activation maps through relu activation\n            # because the values are not normalized with eq.(1) without relu.\n            self.activations = F.relu(self.activations)\n            self.activations = F.interpolate(\n                self.activations, (H, W), mode='bilinear', align_corners=True)  #<<<<< Added Align Corerner to Code.\n            _, C, _, _ = self.activations.shape\n\n            # normalization\n            act_min, _ = self.activations.view(1, C, -1).min(dim=2)\n            act_min = act_min.view(1, C, 1, 1)\n            act_max, _ = self.activations.view(1, C, -1).max(dim=2)\n            act_max = act_max.view(1, C, 1, 1)\n            denominator = torch.where(\n                (act_max - act_min) != 0., act_max - act_min, torch.tensor(1.).to(self.device)  #<<<<<<<<<<<<<<<<\n            )\n\n            self.activations = self.activations \/ denominator\n\n            # generate masked images and calculate class probabilities\n            probs = []\n            for i in range(0, C, self.n_batch):\n                mask = self.activations[:, i:i+self.n_batch].transpose(0, 1)\n                mask = mask.to(device)\n                masked_x = x * mask\n                score = self.model(masked_x)\n                probs.append(F.softmax(score, dim=1)[:, idx].to(self.device).data) #<<<<<<<<<<<<<<<<\n\n            probs = torch.stack(probs)\n            weights = probs.view(1, C, 1, 1)\n\n            # shape = > (1, 1, H, W)\n            cam = (weights * self.activations).sum(1, keepdim=True)\n            cam = F.relu(cam)\n            cam -= torch.min(cam)\n            cam \/= torch.max(cam)\n\n        return cam.data, idx\n\n    def __call__(self, x):\n        return self.forward(x)","a15615c9":"\nmodel = torch.load('..\/input\/mura-inpainting\/models\/'+part+'_resnet152.pth', device)\npart","da7ea21a":"valid_data_root = '..\/input\/mura-v11\/MURA-v1.1\/valid_image_paths.csv'\nx = pd.read_csv(valid_data_root, header=None, names=['PATH'])\n\nvalid_data_files = []\nvalid_data_labels = []\nfor i, r in x.iterrows():\n    if part in r['PATH']:\n        valid_data_files.append('..\/input\/mura-v11\/'+r['PATH'])\n        if 'positive' in r['PATH']:\n            valid_data_labels.append('POSITIVE')\n        else:\n            valid_data_labels.append('NEGATIVE')\n            \nvalid_data = pd.DataFrame(zip(valid_data_files, valid_data_labels), columns=['PATH', 'LABEL'])\nvalid_data","9a425d81":"os.mkdir(part)\nos.mkdir(part+\"\/positive\")\nos.mkdir(part+\"\/negative\")\n\nos.mkdir(part+\"\/positive\/original\")\nos.mkdir(part+\"\/positive\/original\/cam_heatmap\")\n\nos.mkdir(part+\"\/positive\/inpaint\")\n\nos.mkdir(part+\"\/positive\/inpaint\/cam\")\nos.mkdir(part+\"\/positive\/inpaint\/grabcut\")\nos.mkdir(part+\"\/positive\/inpaint\/intersect\")\n\nos.mkdir(part+\"\/positive\/inpaint\/cam\/flipped\")\nos.mkdir(part+\"\/positive\/inpaint\/cam\/not_flipped\")\nos.mkdir(part+\"\/positive\/inpaint\/grabcut\/flipped\")\nos.mkdir(part+\"\/positive\/inpaint\/grabcut\/not_flipped\")\nos.mkdir(part+\"\/positive\/inpaint\/intersect\/flipped\")\nos.mkdir(part+\"\/positive\/inpaint\/intersect\/not_flipped\")\n\nos.mkdir(part+\"\/negative\/original\")\nos.mkdir(part+\"\/negative\/inpaint\")\n\nos.mkdir(part+\"\/negative\/inpaint\/cam\")\nos.mkdir(part+\"\/negative\/inpaint\/grabcut\")\nos.mkdir(part+\"\/negative\/inpaint\/intersect\")\n\nos.mkdir(part+\"\/negative\/inpaint\/cam\/flipped\")\nos.mkdir(part+\"\/negative\/inpaint\/cam\/not_flipped\")\nos.mkdir(part+\"\/negative\/inpaint\/grabcut\/flipped\")\nos.mkdir(part+\"\/negative\/inpaint\/grabcut\/not_flipped\")\nos.mkdir(part+\"\/negative\/inpaint\/intersect\/flipped\")\nos.mkdir(part+\"\/negative\/inpaint\/intersect\/not_flipped\")","dafca358":"!rm -r ELBOW && rm ELBOW.zip","1f178661":"transform = transforms.Compose([\n                transforms.Resize((256, 256)),\n                transforms.ToTensor(),\n                transforms.Normalize([0,0,0], [1,1,1])\n])","9db07f4f":"def save_original(img, loc):\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    n_white_pix = np.sum(img >= 160)\n    n_black_pix = np.sum(img <= 140)\n    if n_white_pix > n_black_pix:\n        # Check if the image has more white or black\n        img = cv2.bitwise_not(img)\n    img = cv2.resize(img, (256,256))\n    cv2.imwrite(loc, img)\n#     cv2.imwrite(loc2, img)","bdeb260d":"def check_circle(img):\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    circles = cv2.HoughCircles(gray, cv2.HOUGH_GRADIENT, 1.2, 100)\n    return False if circles is None else True","2cb9e5c1":"images","06671d9e":"images = set()\nfor img_path in glob.glob('..\/input\/mura-inpainting\/SHOULDER 2\/SHOULDER\/positive\/original\/*.png'):\n        \n#     print(img_path.split('\/')[-1])\n    img = Image.open(img_path).convert('RGB')\n    img_cv2 = cv2.cvtColor(np.asarray(img),cv2.COLOR_RGB2BGR)  \n    img = transform(img)\n    img = img.unsqueeze(0)\n    img = img.to(device=device)\n\n    score = model(img)\n    prob = F.softmax(score, dim=1)\n\n    prob, idx = torch.max(prob, dim=1)\n    idx = idx.item()\n\n    if idx == 1:\n        if check_circle(img_cv2) or prob.item() < 0.85:\n            continue\n            \n#         if 'Train' in img_path:\n        loc = '.\/'+part+'\/positive\/original\/'+img_path.split('\/')[-1]\n#         elif 'Valid' in img_path:\n#             loc = '.\/'+part+'\/positive\/original\/'+img_path.split('\/')[-1]\n        save_original(img_cv2, loc)\n#         images.add(loc)\n        images.add(loc)\n    ","c9d7d365":"lab = {'NEGATIVE': 0, 'POSITIVE': 1}\nclasses = {'Train': train_data, 'Valid': valid_data}\ncam_types = ['CAM', 'ScoreCAM', 'GradCAM', 'GradCAMpp', 'SmoothGradCAMpp']\n\ndef check_accuracy(classes, lab):\n    Train_Normal=0 \n    Valid_Normal=0\n    Train_Abnormal=0\n    Valid_Abnormal=0\n    images = set()\n    \n    for key, data in classes.items():\n        correct = 0\n        total = 0\n        i = 0\n        image_sub = set()\n        for ind, dat in tqdm(data.iterrows(), total=data.shape[0]):\n#             print(len(images))\n            print('Number of images %d' % len(images), end='\\r')\n            print('Number %d' % int(Train_Normal + Valid_Normal + Train_Abnormal + Valid_Abnormal), end='\\r')\n            if len(image_sub)>=100:\n                continue\n            inputs, labels = dat\n\n            img = Image.open(inputs).convert('RGB')\n            img_cv2 = cv2.cvtColor(np.asarray(img),cv2.COLOR_RGB2BGR)  \n            img = transform(img)\n            img = img.unsqueeze(0)\n            img = img.to(device=device)\n    \n            score = model(img)\n            prob = F.softmax(score, dim=1)\n\n            prob, idx = torch.max(prob, dim=1)\n            idx = idx.item()\n            total += 1\n\n            \n                \n            if idx == lab[labels]:\n                correct += 1\n                if check_circle(img_cv2) or prob.item() < 0.85:\n                    continue\n#                 print(prob.item())\n                \n                if lab[labels] == 0:\n                    #Normal Images\n                    if key == 'Train' and Train_Normal<25:\n                        loc = '.\/'+part+'\/negative\/original\/Train'+str(Train_Normal+1)+'.png'\n#                         loc2 = '.\/'+part+'\/negative\/inpaint\/Train'+str(Train_Normal+1)+'.png'\n#                         save_original(img_cv2, loc)\n                        Train_Normal += 1\n                    elif key == 'Valid' and Valid_Normal<25:\n                        loc = '.\/'+part+'\/negative\/original\/Valid'+str(Valid_Normal+1)+'.png'\n#                         loc2 = '.\/'+part+'\/negative\/inpaint\/Valid'+str(Valid_Normal+1)+'.png'\n#                         save_original(img_cv2, loc)\n                        Valid_Normal+=1\n                else:\n                    #Abnormal Images\n                    if key == 'Train' and Train_Abnormal<75:\n                        loc = '.\/'+part+'\/positive\/original\/Train'+str(Train_Abnormal+1)+'.png'\n#                         loc2 = '.\/'+part+'\/positive\/inpaint\/Train'+str(Train_Abnormal+1)+'.png'\n#                         save_original(img_cv2, loc)\n                        Train_Abnormal+=1\n                    elif key == 'Valid' and Valid_Abnormal<75:\n                        loc = '.\/'+part+'\/positive\/original\/Valid'+str(Valid_Abnormal+1)+'.png'\n#                         loc2 = '.\/'+part+'\/positive\/inpaint\/Valid'+str(Valid_Abnormal+1)+'.png'\n                        Valid_Abnormal+=1\n                print(loc)\n                plt.imshow(img_cv2)\n                plt.show()\n                save_original(img_cv2, loc)\n                image_sub.add(loc)\n        \n        images.update(image_sub)\n\n        accuracy = correct \/ float(total)\n        print('Accuracy over '+key+' Dataset: ',accuracy)\n    return images\n\nimages = check_accuracy(classes, lab)\nprint(\"Number of images: \", len(images))","3350c39c":"def get_intersect(A,B):\n    dilate = 10\n    nrows, ncols = A.shape\n    x = []\n    y = []\n    for i in range(0, nrows):\n        for j in range(0, ncols):\n            if A[i,j] == True and B[i,j] == True:\n                for dilute in range(-dilate, dilate):\n                    new_x = i+dilute\n                    new_y = j+dilute\n                    \n                    if new_x>255 or new_y>255 or new_x<0 or new_x<0:\n                        continue\n                    x.append(new_x)\n                    y.append(new_y)\n    \n    return None if len(x)==0 or len(y)==0 else (np.array(x), np.array(y))","3cff35a1":"positive_images = glob.glob(\"..\/input\/mura-inpainting\/\"+part.lower()+\"\/positive\/original\/*.png\")\nnegative_images = glob.glob(\"..\/input\/mura-inpainting\/\"+part.lower()+\"\/negative\/original\/*.png\")\nprint(len(positive_images), len(negative_images))","9d3365ca":"for i, img in enumerate(tqdm(list(images)[:20])):\n# for i, img in enumerate(tqdm(images)):\n\n    if 'negative' in img.lower():\n        img_path = '.\/'+part+'\/negative\/'\n    if 'positive' in img.lower():\n        img_path = '.\/'+part+'\/positive\/'\n    \n    img_name = img.split('\/')[-1]\n    print(img_name)\n    image=Image.open(img).convert('RGB')\n    \n    enhancer = ImageEnhance.Contrast(image)\n    factor = 6 #increase contrast\n    new_image=image.copy()\n    new_image=enhancer.enhance(factor)\n    new_image=transform(new_image)\n    new_image=new_image.unsqueeze(0)\n    new_image=new_image.to(device)\n    \n#     image=enhancer.enhance(factor)\n    prep_img=transform(image)\n    prep_img=prep_img.unsqueeze(0)\n    prep_img=prep_img.to(device)\n    \n    wrapped_model = GradCAM(model, target_layer=model.layer4[2].conv3, device=device)\n    cam, idx = wrapped_model(prep_img)\n\n    _, _, H, W = prep_img.shape\n\n    cam = F.interpolate(cam, size=(H, W), mode='bilinear', align_corners=True)\n\n    output_image=prep_img.squeeze(0).squeeze(0).cpu().numpy()\n    output_image=np.moveaxis(output_image, 0, -1)\n    output_image=cv2.resize(output_image, (256,256))\n    \n    new_image=new_image.squeeze(0).squeeze(0).cpu().numpy()\n    new_image=np.moveaxis(new_image, 0, -1)\n    new_image=cv2.resize(new_image, (256,256))\n\n    cam_img = cam.squeeze(0).squeeze(0).cpu().numpy()\n    new_cam_img = cam_img.copy()\n    \n    if idx == 0:\n        cam_threshold = 1.1 # Between 1-2\n        original_threshold = 1.1\n        \n    else:\n        original_threshold = 1.2\n        cam_threshold = 5 # Between 1-2\n        \n    img_idx = new_cam_img[:,:] > (new_cam_img.max()\/cam_threshold)\n    new_cam_img[:,:] = 0\n    new_cam_img[img_idx] = 255\n    \n    mask_img = new_image.copy()\n\n    img_idx_org = mask_img[:,:] > (mask_img.max()\/original_threshold)\n\n    inter = get_intersect(img_idx, img_idx_org[:,:,-1])\n    if inter:\n        mask_img[:,:] = 0\n        mask_img[inter] = 255\n\n#     plt.figure(figsize=(10,10))\n\n# *****************************************************************************************************\n\n#     plt.subplot(1, 2, 1)\n#     plt.axis('off')\n#     plt.title(\"Original\")\n#     plt.imshow(output_image, cmap=plt.cm.gray)\n\n# *****************************************************************************************************\n\n#     plt.subplot(1, 6, 2)\n#     plt.axis('off')\n    cam_img = cam_img*255\n    cam_img = cam_img.astype('uint8')\n#     plt.title(\"CAM Heatmap\")\n#     plt.imshow(cam_img, cmap=plt.cm.gray)\n#     cv2.imwrite(img_path+'cam_heatmap\/'+img_name, cam_img)\n    \n# *****************************************************************************************************\n    \n#     plt.subplot(1, 6, 3)\n#     plt.axis('off')\n#     plt.title(\"CAM\")\n#     plt.imshow(new_cam_img, cmap=plt.cm.gray)\n    cv2.imwrite(img_path+'cam\/'+img_name, new_cam_img)\n\n# *****************************************************************************************************\n\n#     plt.subplot(1, 6, 4)\n#     plt.axis('off')\n    kernel = np.ones((10,10), np.uint8)\n    mask_img = cv2.morphologyEx(mask_img, cv2.MORPH_CLOSE, kernel)\n#     plt.title(\"Intersect CAM\")\n    \n    if idx == 0:\n        mask = new_cam_img\n    elif mask_img.max()<255:\n        mask = new_cam_img\n    else:\n        mask = mask_img\n#     plt.imshow(mask, cmap=plt.cm.gray)\n#     cv2.imwrite(img_path+'intersect\/'+img_name, mask)\n    \n\n# *****************************************************************************************************\n    \n#     plt.subplot(1, 6, 5)\n#     plt.axis('off')\n#     plt.title(\"Grabcut\")\n    grabcut_img = get_mask(img, img_path, img_name)\n    if grabcut_img is None:\n        print(\"Grabcut came empty.\")\n        grabcut_img = mask\n#     plt.imshow(grabcut_img, cmap=plt.cm.gray)\n#     cv2.imwrite(img_path+'grabcut\/'+img_name, grabcut_img)\n    \n# *****************************************************************************************************\n    \n#     plt.subplot(1, 2, 2)\n    plt.axis('off')\n#     plt.title(\"Overlay\")\n    \n#     new_image = cv2.cvtColor(new_image, cv2.COLOR_BGR2GRAY)\n\n    if idx == 0:\n        overlay = new_cam_img\n    else:\n#         overlay = cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR)\n#         overlay = cv2.cvtColor(overlay, cv2.COLOR_BGR2GRAY)\n        \n        grabcut_img_white = np.sum(grabcut_img >= 10)\n        mask_img_white = np.sum(mask_img >= 10)\n#         print(\"White pixels of Grabcut: \", grabcut_img_white)\n#         print(\"White pixels of MaskImage: \", mask_img_white)\n        \n        if grabcut_img_white == mask_img_white == 0:\n            overlay = new_cam_img\n        elif grabcut_img_white > mask_img_white:\n            overlay = grabcut_img\n        else:\n            overlay = mask_img\n    \n    \n#     print(new_image.shape)\n#     print(overlay.shape)\n#     added_image = cv2.addWeighted(new_image, 1, overlay, 0.5, 0)\n    \n#     cv2.imwrite(img_path+'overlay\/'+img_name, added_image)\n#     cv2.imwrite(img_path+'mask\/'+img_name, overlay)\n\n#     plt.imshow(added_image, cmap=plt.cm.gray)\n    plt.imshow(new_image, cmap=plt.cm.gray)\n    plt.imshow(mask, alpha=0.5, cmap=plt.cm.gray)\n\n    plt.show()","cefb5584":"for dat_type in ['positive\/', 'negative\/']:\n    dat_dir = \"..\/input\/mura-inpainting\/\"+part+\"\/\"+dat_type\n    if 'positive' in dat_type:\n        res = 'Flipped'\n    else:\n        res = 'Not Flipped'\n    print(\"**************************** \"+part+\" ---- \"+dat_type[:-1]+\" ****************************\")\n    for f in sorted(os.listdir(dat_dir)):\n        correct = 0\n        total = 0\n        print(\"Working on folder: \", f)\n\n        with open(part.lower()+\"_\"+dat_type[:-1]+\"_\"+f+\"_results.txt\", 'w') as txtfile:\n\n            for img in glob.glob(dat_dir+f+\"\/*.png\"):\n                \n                img_path = img\n                img = Image.open(img).convert('RGB')\n                img_cv2 = cv2.cvtColor(np.asarray(img),cv2.COLOR_RGB2BGR)  \n                img = transform(img)\n                img = img.unsqueeze(0)\n                img = img.to(device=device)\n\n                score = model(img)\n                prob = F.softmax(score, dim=1)\n\n                prob, idx = torch.max(prob, dim=1)\n                idx = idx.item()\n                total += 1\n\n                if idx == 0:\n                    correct += 1\n                    txtfile.write(\"Normal: \"+img_path+\"\\n\")\n                else:\n                    txtfile.write(\"Abnormal: \"+img_path+\"\\n\")\n\n\n            accuracy = correct \/ float(total)\n            print(res+' '+str(accuracy*100)+'% for '+str(total)+' images using '+f+' technique.')\n    print('\\n')","f7e5f984":"!zip -r wrist.zip *.txt\n!rm *.txt","9960aa07":"def get_threshold(average):\n#     print(average)\n    if average < 90:\n        return average * 2.2\n    elif average < 100:\n        return average * 2\n    elif average <140:\n        return average * 1.8\n    elif average <160:\n        return average * 1.6\n    elif average <180:\n        return average * 1.4","8160cfb8":"def get_mask(img, img_path, img_name):\n    try:\n        original = cv2.imread(img)\n        original = cv2.resize(original, (256,256))\n\n        cam_img = cv2.imread(img_path+'cam\/'+img_name, cv2.IMREAD_GRAYSCALE)\n\n        th, threshed = cv2.threshold(cam_img, 127, 255, cv2.THRESH_BINARY|cv2.THRESH_OTSU)\n\n        cnts = cv2.findContours(threshed, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)[-2]\n\n        ## sorted by area\n        cnts = sorted(cnts, key=cv2.contourArea)\n\n        ## get the maximum's boundinRect\n        cnt = cnts[-1]\n        bbox = x,y,w,h = cv2.boundingRect(cnt)\n\n        crop_mask = threshed[y:y+h, x:x+w]\n\n        crop_img = original[y:y+h, x:x+w]\n\n        crop_img_gray = cv2.cvtColor(crop_img, cv2.COLOR_BGR2GRAY)\n\n        # norm image\n        crop_img_gray = cv2.normalize(crop_img_gray, None, alpha = 0, beta = 255, norm_type = cv2.NORM_MINMAX)\n        ret,threshed_bin = cv2.threshold(crop_img_gray,get_threshold(np.mean(crop_img_gray)),255,cv2.THRESH_BINARY)\n\n        mask = np.ones_like(threshed_bin, np.uint8)*cv2.GC_PR_BGD\n        cnts = cv2.findContours(threshed_bin, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)[-2]\n        if len(cnts)==0:\n            return None\n        cnts = sorted(cnts, key=cv2.contourArea)\n        cnt = cnts[-1]\n        bbox = x2,y2,w2,h2 = cv2.boundingRect(cnt)\n        cv2.drawContours(mask, [cnt], -1, cv2.GC_FGD, -1)\n\n        # grabcut\n        bgdModel = np.zeros((1, 65), np.float64)\n        fgdModel = np.zeros((1, 65), np.float64)\n        rect = bbox\n        (mask, bgModel, fgModel) = cv2.grabCut(crop_img, mask, rect, bgdModel, fgdModel, 5, cv2.GC_INIT_WITH_MASK)\n        mask = np.where((mask==2)|(mask==0),0,1).astype(\"uint8\")\n\n        # Uncrop image and only use cam selected pixels for mask\n        grabcut = crop_mask*mask\n        shape = original.shape\n        mask_final = np.zeros((shape[0],shape[1]))\n        mask_final[y:y+h, x:x+w] = grabcut\n        kernel = np.ones((10,10),np.uint8)\n\n        # Morphology operations\n        mask_final = cv2.dilate(mask_final, kernel)\n        mask_final = cv2.morphologyEx(mask_final, cv2.MORPH_CLOSE, kernel)\n\n        return mask_final\n    except:\n        return None","e8b1af41":"flipped = 0\nnot_flipped = 0\n\n# for i, img in enumerate(tqdm(list(images)[:5])):\nfor i, img in enumerate(tqdm(images)):\n\n#     if flipped == 20 and not_flipped == 10:\n#         break\n    \n    if 'negative' in img.lower():\n        img_pred = '\/negative\/'\n        img_path = '.\/' + part + img_pred\n    if 'positive' in img.lower():\n        img_pred = '\/positive\/'\n        img_path = '.\/' + part + img_pred\n    \n    img_name = img.split('\/')[-1]\n    print(img)\n    \n    wrapped_model = GradCAM(model, target_layer=model.layer4[2].conv3, device=device)\n    \n    def get_img(path):\n        img=Image.open(path).convert('RGB')    \n        img=transform(img)\n        img=img.unsqueeze(0)\n        img=img.to(device)\n        return img\n    \n    original_img = get_img(img)    \n    \n    inpaint_cam_path = '..\/input\/mura-inpainting\/SHOULDER 2\/' + part + img_pred + 'cam\/' + img_name\n    inpaint_cam_img = get_img(inpaint_cam_path)\n    \n    inpaint_grabcut_path = '..\/input\/mura-inpainting\/SHOULDER 2\/' + part + img_pred + 'grabcut\/' + img_name\n    inpaint_grabcut_img = get_img(inpaint_grabcut_path)\n    \n    inpaint_intersect_path = '..\/input\/mura-inpainting\/SHOULDER 2\/' + part + img_pred + 'intersect\/' + img_name\n    inpaint_intersect_img = get_img(inpaint_intersect_path)\n    \n    def get_results(img, idx_original=None):\n        cam, idx = wrapped_model(img)\n        _, _, H, W = img.shape\n        cam = F.interpolate(cam, size=(H, W), mode='bilinear', align_corners=True)\n        cam = cam.squeeze(0).squeeze(0).cpu().numpy()\n        cam = cam*255\n        cam = cam.astype('uint8')\n        if idx_original in [0,1]:\n            if idx == idx_original:\n                return cam, 'not_flipped'\n            else:\n                return cam, 'flipped'                \n        else:\n            return cam, idx\n        \n        \n    cam_original, idx_original = get_results(original_img)\n    \n    cam_inpaint_cam, status_inpaint_cam = get_results(inpaint_cam_img, idx_original)\n    cam_inpaint_grabcut, status_inpaint_grabcut = get_results(inpaint_grabcut_img, idx_original)\n    cam_inpaint_intersect, status_inpaint_intersect = get_results(inpaint_intersect_img, idx_original)\n    \n        \n#     plt.figure(figsize=(10,10))\n    \n# *****************************************************************************************************\n\n#     plt.subplot(1, 2, 1)\n#     plt.axis('off')\n#     plt.title(\"Original\")\n#     plt.imshow(output_image, cmap=plt.cm.gray)\n\n    original_img=original_img.squeeze(0).squeeze(0).cpu().numpy()\n    original_img=np.moveaxis(original_img, 0, -1)\n    original_img=cv2.resize(original_img, (256,256))\n    original_img=cv2.convertScaleAbs(original_img, alpha=(255.0))\n    cv2.imwrite(img_path+'original\/'+img_name, original_img)\n    \n\n# *****************************************************************************************************\n\n# CAM Heatmap\n\n#     plt.subplot(1, 6, 2)\n#     plt.axis('off')\n#     plt.title(\"CAM Heatmap\")\n#     plt.imshow(cam_img, cmap=plt.cm.gray)\n    cam_original = cv2.applyColorMap(cam_original, cv2.COLORMAP_JET)\n    new_overlay_img = cv2.addWeighted(original_img, 1, cam_original, 0.3, 0)\n    cv2.imwrite(img_path+'original\/cam_heatmap\/'+img_name, new_overlay_img)\n    \n# *****************************************************************************************************\n    \n    inpaint_cam_img=inpaint_cam_img.squeeze(0).squeeze(0).cpu().numpy()\n    inpaint_cam_img=np.moveaxis(inpaint_cam_img, 0, -1)\n    inpaint_cam_img=cv2.resize(inpaint_cam_img, (256,256))\n    inpaint_cam_img=cv2.convertScaleAbs(inpaint_cam_img, alpha=(255.0))\n    cv2.imwrite(img_path+'inpaint\/cam\/'+img_name, inpaint_cam_img)\n    \n# Inpainted CAM Image\n\n#     plt.subplot(1, 6, 2)\n#     plt.axis('off')\n#     plt.title(\"CAM Heatmap\")\n#     plt.imshow(cam_img, cmap=plt.cm.gray)\n    cam_inpaint_cam = cv2.applyColorMap(cam_inpaint_cam, cv2.COLORMAP_JET)\n    new_overlay_img = cv2.addWeighted(inpaint_cam_img, 1, cam_inpaint_cam, 0.3, 0)\n    cv2.imwrite(img_path+'inpaint\/cam\/'+status_inpaint_cam+'\/'+img_name, new_overlay_img)\n    \n# *****************************************************************************************************\n    \n    inpaint_grabcut_img=inpaint_grabcut_img.squeeze(0).squeeze(0).cpu().numpy()\n    inpaint_grabcut_img=np.moveaxis(inpaint_grabcut_img, 0, -1)\n    inpaint_grabcut_img=cv2.resize(inpaint_grabcut_img, (256,256))\n    inpaint_grabcut_img=cv2.convertScaleAbs(inpaint_grabcut_img, alpha=(255.0))\n    cv2.imwrite(img_path+'inpaint\/grabcut\/'+img_name, inpaint_grabcut_img)\n    \n# # Grabcut Heatmap\n\n# #     plt.subplot(1, 6, 2)\n# #     plt.axis('off')\n# #     plt.title(\"CAM Heatmap\")\n# #     plt.imshow(cam_img, cmap=plt.cm.gray)\n#     plt.imshow(inpaint_cam_img, cmap=plt.cm.gray)\n#     plt.imshow(cam_inpaint_cam, cmap='jet', alpha = 0.3)\n#     plt.savefig(img_path+'inpaint\/cam\/'+status_inpaint_cam+'\/'+img_name, bbox_inches='tight', pad_inches=0)\n    cam_inpaint_grabcut = cv2.applyColorMap(cam_inpaint_grabcut, cv2.COLORMAP_JET)\n    new_overlay_img = cv2.addWeighted(inpaint_grabcut_img, 1, cam_inpaint_grabcut, 0.3, 0)\n    cv2.imwrite(img_path+'inpaint\/grabcut\/' + status_inpaint_grabcut + '\/' + img_name, new_overlay_img)\n\n    \n# *****************************************************************************************************\n    \n    inpaint_intersect_img=inpaint_intersect_img.squeeze(0).squeeze(0).cpu().numpy()\n    inpaint_intersect_img=np.moveaxis(inpaint_intersect_img, 0, -1)\n    inpaint_intersect_img=cv2.resize(inpaint_intersect_img, (256,256))\n    inpaint_intersect_img=cv2.convertScaleAbs(inpaint_intersect_img, alpha=(255.0))\n    cv2.imwrite(img_path+'inpaint\/intersect\/'+img_name, inpaint_intersect_img)\n    \n# # Intersect heatmap\n\n# #     plt.subplot(1, 6, 2)\n# #     plt.axis('off')\n# #     plt.title(\"CAM Heatmap\")\n# #     plt.imshow(cam_img, cmap=plt.cm.gray)\n#     plt.imshow(inpaint_cam_img, cmap=plt.cm.gray)\n#     plt.imshow(cam_inpaint_cam, cmap='jet', alpha = 0.3)\n#     plt.savefig(img_path+'inpaint\/cam\/'+status_inpaint_cam+'\/'+img_name, bbox_inches='tight', pad_inches=0)\n    cam_inpaint_intersect = cv2.applyColorMap(cam_inpaint_intersect, cv2.COLORMAP_JET)\n    new_overlay_img = cv2.addWeighted(inpaint_intersect_img,1,cam_inpaint_intersect,0.3, 0)\n    cv2.imwrite(img_path+'inpaint\/intersect\/' + status_inpaint_intersect + '\/' + img_name, new_overlay_img)\n\n    \n# *****************************************************************************************************\n    \n\n#     plt.show()\n","24fb844e":"!zip -r SHOULDER.zip SHOULDER","9588bc01":"\n# images = ['..\/input\/mura-v11\/MURA-v1.1\/train\/XR_HUMERUS\/patient00577\/study1_positive\/image1.png',\n#           '..\/input\/mura-v11\/MURA-v1.1\/train\/XR_HUMERUS\/patient01344\/study1_positive\/image3.png',\n#           '..\/input\/mura-v11\/MURA-v1.1\/train\/XR_HUMERUS\/patient00438\/study1_positive\/image1.png',\n#           '..\/input\/mura-v11\/MURA-v1.1\/valid\/XR_HUMERUS\/patient11231\/study1_positive\/image2.png',\n#           '..\/input\/mura-v11\/MURA-v1.1\/valid\/XR_HUMERUS\/patient11214\/study1_positive\/image1.png',\n          \n#           '..\/input\/mura-v11\/MURA-v1.1\/train\/XR_HUMERUS\/patient00077\/study1_negative\/image2.png',\n#           '..\/input\/mura-v11\/MURA-v1.1\/train\/XR_HUMERUS\/patient00119\/study1_negative\/image2.png',\n#           '..\/input\/mura-v11\/MURA-v1.1\/train\/XR_HUMERUS\/patient00246\/study1_negative\/image2.png',\n#           '..\/input\/mura-v11\/MURA-v1.1\/valid\/XR_HUMERUS\/patient11289\/study1_negative\/image2.png',\n#           '..\/input\/mura-v11\/MURA-v1.1\/valid\/XR_HUMERUS\/patient11225\/study1_negative\/image2.png'\n          \n#          ]\n\n# # CAM\/GradDCAM\/GradCAMpp\/SmoothGradCAMpp\/ScoreCAM\n\n# for i, img in enumerate(images):\n    \n#     image=Image.open(img).convert('RGB')\n\n#     prep_img=transform(image)\n#     prep_img=prep_img.unsqueeze(0)\n#     prep_img=prep_img.to(device)\n    \n    \n#     wrapped_model_CAM = CAM(model, target_layer=model.layer4[2].conv3, device=device)\n#     cam_CAM, idx = wrapped_model_CAM(prep_img)\n    \n#     wrapped_model_GradCAM = GradCAM(model, target_layer=model.layer4[2].conv3, device=device)\n#     cam_GradCAM, idx = wrapped_model_GradCAM(prep_img)\n    \n#     wrapped_model_GradCAMpp = GradCAMpp(model, target_layer=model.layer4[2].conv3, device=device)\n#     cam_GradCAMpp, idx = wrapped_model_GradCAMpp(prep_img)\n    \n#     wrapped_model_SmoothGradCAMpp = SmoothGradCAMpp(model, target_layer=model.layer4[2].conv3, device=device)\n#     cam_SmoothGradCAMpp, idx = wrapped_model_SmoothGradCAMpp(prep_img)\n    \n#     wrapped_model_ScoreCAM = ScoreCAM(model, target_layer=model.layer4[2].conv3, device=device)\n#     cam_ScoreCAM, idx = wrapped_model_ScoreCAM(prep_img)\n    \n# #     print(\"Working on img: \", prep_img)\n#     _, _, H, W = prep_img.shape\n    \n#     output_image=prep_img.squeeze(0).squeeze(0).cpu().numpy()\n#     output_image=np.moveaxis(output_image, 0, -1)\n    \n    \n    \n#     cam_CAM = F.interpolate(cam_CAM, size=(H, W), mode='bilinear', align_corners=True)\n#     cam_CAM_img = cam_CAM.squeeze(0).squeeze(0).cpu().numpy()\n    \n#     cam_GradCAM= F.interpolate(cam_GradCAM, size=(H, W), mode='bilinear', align_corners=True)\n#     cam_GradCAM_img = cam_GradCAM.squeeze(0).squeeze(0).cpu().numpy()\n    \n#     cam_GradCAMpp = F.interpolate(cam_GradCAMpp, size=(H, W), mode='bilinear', align_corners=True)\n#     cam_GradCAMpp_img = cam_GradCAMpp.squeeze(0).squeeze(0).cpu().numpy()\n    \n#     cam_SmoothGradCAMpp = F.interpolate(cam_SmoothGradCAMpp, size=(H, W), mode='bilinear', align_corners=True)\n#     cam_SmoothGradCAMpp_img = cam_SmoothGradCAMpp.squeeze(0).squeeze(0).cpu().numpy()\n    \n#     cam_ScoreCAM = F.interpolate(cam_ScoreCAM, size=(H, W), mode='bilinear', align_corners=True)\n#     cam_ScoreCAM_img = cam_ScoreCAM.squeeze(0).squeeze(0).cpu().numpy()\n    \n    \n#     plt.figure(figsize=(20,20))\n# # overlay  original  mask  cam\n    \n#     plt.subplot(1, 6, 1)\n#     plt.axis('off')\n#     plt.title(\"Original\")\n#     plt.imshow(output_image, cmap=plt.cm.gray)\n   \n#     plt.subplot(1, 6, 2)\n#     plt.axis('off')\n#     plt.title(\"CAM\")\n#     plt.imshow(output_image, cmap=plt.cm.gray)\n#     plt.imshow(cam_CAM_img, cmap='jet', alpha=0.5)\n    \n#     plt.subplot(1, 6, 3)\n#     plt.axis('off')\n#     plt.title(\"GradCAM\")\n#     plt.imshow(output_image, cmap=plt.cm.gray)\n#     plt.imshow(cam_GradCAM_img, cmap='jet', alpha=0.5)\n    \n#     plt.subplot(1, 6, 4)\n#     plt.axis('off')\n#     plt.title(\"GradCAMpp\")\n#     plt.imshow(output_image, cmap=plt.cm.gray)\n#     plt.imshow(cam_GradCAMpp_img, cmap='jet', alpha=0.5)\n    \n#     plt.subplot(1, 6, 5)\n#     plt.axis('off')\n#     plt.title(\"SmoothGradCAMpp\")\n#     plt.imshow(output_image, cmap=plt.cm.gray)\n#     plt.imshow(cam_SmoothGradCAMpp_img, cmap='jet', alpha=0.5)\n    \n#     plt.subplot(1, 6, 6)\n#     plt.axis('off')\n#     plt.title(\"ScoreCAM\")\n#     plt.imshow(output_image, cmap=plt.cm.gray)\n#     plt.imshow(cam_ScoreCAM_img, cmap='jet', alpha=0.5)\n\n#     plt.show()","1bb8943a":"## Test Inpainting","d5baeb78":"## GrabCut Technique","8f49db06":"## Analysis","11900005":"### Testing Models.","3ebfe493":"### Test CAM","45dfbd7e":"### CAM"}}