{"cell_type":{"228c3520":"code","747e6c33":"code","1f871ce9":"code","2363acca":"code","98cddcf9":"code","7b6e4394":"code","66c4a6e7":"code","f154dd91":"code","2a8ead0b":"code","5a545d47":"code","97948131":"code","a746390a":"code","2f9d05ed":"code","5c98c482":"code","be14e349":"code","4a99be83":"code","28d6bcb8":"code","9a5a68ef":"code","f14f0302":"code","42fbb538":"code","02e043bf":"code","8394e7ae":"code","5fafcf45":"code","7e428541":"code","f16e75aa":"code","4062ac24":"code","0c9bfc6d":"code","ebb7f010":"code","4314d8c0":"code","0f349479":"code","8972bf50":"code","c666b56b":"code","dd25b2c4":"code","19231bf1":"code","2f3756c0":"code","da3fdbd4":"code","dc079298":"code","1603f8e7":"code","c3a1ebbb":"code","9fef3257":"code","6e872235":"code","a24ce536":"code","67b5389d":"code","4a1e42c0":"code","f432a61e":"code","6226ad48":"code","0e292761":"code","01a0f01f":"code","1cbd3739":"code","9c78dcdb":"code","326c4938":"code","6f14e82a":"code","960d86b0":"code","7f3419cf":"markdown","5a506bcd":"markdown","669a92fb":"markdown","b5c93d7f":"markdown","61184aa1":"markdown","4357c457":"markdown","19a1dfe7":"markdown","19db17f1":"markdown","e784df17":"markdown","36db4699":"markdown","39d0c089":"markdown","fb24dbe4":"markdown","5cec3f95":"markdown","22edb524":"markdown","20c95bb9":"markdown","4c95558e":"markdown","f32217d7":"markdown"},"source":{"228c3520":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","747e6c33":"import numpy as np\nimport pandas as pd\nfrom os import path\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport matplotlib.pyplot as plt\nfrom textblob import TextBlob\nfrom sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn import decomposition, ensemble\n\nimport pandas, xgboost, numpy, textblob, string\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, mean_squared_error, r2_score, roc_auc_score, roc_curve, classification_report\nfrom sklearn.model_selection import train_test_split\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","1f871ce9":"\ntrain  =pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")","2363acca":"train.head()","98cddcf9":"train.isnull().sum()","7b6e4394":"#Dropping unncessary columns\ntrain.drop([\"keyword\",\"location\"],axis = 1,inplace = True)\n","66c4a6e7":"train[\"text\"] = train[\"text\"].apply(lambda x : \" \".join(x.lower() for x in x.split()))","f154dd91":"train.head()","2a8ead0b":"train[\"text\"] = train[\"text\"].str.replace(\"\\d\",\"\")","5a545d47":"train.head()","97948131":"train[\"text\"] = train[\"text\"].str.replace(\"[^\\w\\s]\",\"\")","a746390a":"train.head()","2f9d05ed":"import nltk ","5c98c482":"nltk.download(\"stopwords\")","be14e349":"from nltk.corpus import stopwords\nsw = stopwords.words(\"english\")","4a99be83":"sw.append(\"u\")\nsw.append(\"im\")","28d6bcb8":"train[\"text\"] = train[\"text\"].apply(lambda x: \" \".join(x for x in x.split() if x not in sw))","9a5a68ef":"from textblob import Word\nnltk.download(\"wordnet\")","f14f0302":"train[\"text\"] = train[\"text\"].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))","42fbb538":"train.head()","02e043bf":"#regex\ntrain[\"text\"] = train[\"text\"].str.replace('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',\"\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"We're\", \"We are\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"That's\", \"That is\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"won't\", \"will not\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"they're\", \"they are\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"Can't\", \"Cannot\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"wasn't\", \"was not\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"don\\x89\u00db\u00aat\", \"do not\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"aren't\", \"are not\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"isn't\", \"is not\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"You're\", \"You are\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"I'M\", \"I am\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"shouldn't\", \"should not\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"wouldn't\", \"would not\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"i'm\", \"I am\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"We've\", \"We have\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"Didn't\", \"Did not\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"it's\", \"it is\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"can't\", \"cannot\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"don't\", \"do not\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"you're\", \"you are\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"I've\", \"I have\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"Don't\", \"do not\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"I'll\", \"I will\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"Let's\", \"Let us\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"Could've\", \"Could have\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"youve\", \"you have\")\ntrain[\"text\"] = train[\"text\"].str.replace(r\"It's\", \"It is\")","8394e7ae":"from sklearn.model_selection import train_test_split","5fafcf45":"freq = (train[\"text\"][0:1000]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()","7e428541":"freq.columns = [\"words\",\"tf\"]","f16e75aa":"x = freq[freq[\"tf\"] > 30].sort_values(by = \"tf\" ,ascending = False)","4062ac24":"x.plot.bar(x = \"words\", y = \"tf\",color = \"pink\");","0c9bfc6d":"#WordCloud for the first 5 row\nfor i in range (0,5):\n    text = train[\"text\"][i]\n    wordcloud = WordCloud().generate(text)\n    plt.imshow(wordcloud, interpolation = \"bilinear\")\n    plt.axis(\"off\")\n    plt.show()\n    print(\"***********************************************\")","ebb7f010":"from nltk.tokenize import sent_tokenize, word_tokenize \nimport warnings \n  \nwarnings.filterwarnings(action = 'ignore') \n  \nimport gensim \nfrom gensim.models import Word2Vec ","4314d8c0":"import numpy as np\nimport pandas as pd\nfrom os import path\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport matplotlib.pyplot as plt","0f349479":"x_train, x_test, y_train, y_test = train_test_split(train[\"text\"], train[\"target\"],\n                                                    test_size = 0.3,\n                                                    random_state = 18)","8972bf50":"vectorizer = CountVectorizer()\nvectorizer.fit(x_train)","c666b56b":"x_train_count = vectorizer.transform(x_train)\nx_test_count = vectorizer.transform(x_test)","dd25b2c4":"loj = linear_model.LogisticRegression()\nloj_model = loj.fit(x_train_count, y_train)\ny_pred = loj_model.predict(x_test_count)\n\n\naccuracy_score(y_test,y_pred)","19231bf1":"x_train_count","2f3756c0":"nb = naive_bayes.MultinomialNB()\nnb_model = nb.fit(x_train_count,y_train)\ny_pred = nb_model.predict(x_test_count)\naccuracy_score(y_test,y_pred)","da3fdbd4":"rf = ensemble.RandomForestClassifier()\nrf_model = rf.fit(x_train_count,y_train)\ny_pred = rf_model.predict(x_test_count)\naccuracy_score(y_test,y_pred)","dc079298":"xgb = xgboost.XGBClassifier()\nxgb_model = xgb.fit(x_train_count,y_train)\ny_pred = xgb_model.predict(x_test_count)\naccuracy_score(y_test,y_pred)","1603f8e7":"tf_idf_word_vectorizer = TfidfVectorizer()\n\ntf_idf_word_vectorizer.fit(x_train)","c3a1ebbb":"x_train_tf_idf_word = tf_idf_word_vectorizer.transform(x_train)\nx_test_tf_idf_word = tf_idf_word_vectorizer.transform(x_test)","9fef3257":"loj = linear_model.LogisticRegression()\nloj_model = loj.fit(x_train_tf_idf_word, y_train)\ny_pred = loj_model.predict(x_test_tf_idf_word)\naccuracy_score(y_test,y_pred)","6e872235":"nb = naive_bayes.MultinomialNB()\nnb_model = nb.fit(x_train_tf_idf_word,y_train)\ny_pred = nb.predict(x_test_tf_idf_word)\naccuracy_score(y_test,y_pred)","a24ce536":"test = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\ntest.drop([\"keyword\",\"location\"],axis = 1,inplace = True)","67b5389d":"def prep(test):\n    \n    \n    test[\"text\"] = test[\"text\"].apply(lambda x : \" \".join(x.lower() for x in x.split()))\n    \n    test[\"text\"] = test[\"text\"].str.replace(\"\\d\",\"\")\n    \n    test[\"text\"] = test[\"text\"].str.replace(\"[^\\w\\s]\",\"\")\n    \n    test[\"text\"] = test[\"text\"].apply(lambda x: \" \".join(x for x in x.split() if x not in sw))\n    \n    test[\"text\"] = test[\"text\"].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n    \n    test[\"text\"] = test[\"text\"].str.replace('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',\"\")\n    \n    test[\"text\"] = test[\"text\"].str.replace(r'(((http)(s)?|www(.)?)(:\/\/)?\\S+)',\"\")\n    \n    test[\"text\"] = test[\"text\"].str.replace(r\"\\x89\u00db\u00d3\", \"\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"he's\", \"he is\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"there's\", \"there is\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"We're\", \"We are\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"That's\", \"That is\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"won't\", \"will not\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"they're\", \"they are\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"Can't\", \"Cannot\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"wasn't\", \"was not\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"don\\x89\u00db\u00aat\", \"do not\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"aren't\", \"are not\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"isn't\", \"is not\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"You're\", \"You are\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"I'M\", \"I am\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"shouldn't\", \"should not\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"wouldn't\", \"would not\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"i'm\", \"I am\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"We've\", \"We have\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"Didn't\", \"Did not\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"it's\", \"it is\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"can't\", \"cannot\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"don't\", \"do not\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"you're\", \"you are\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"I've\", \"I have\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"Don't\", \"do not\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"I'll\", \"I will\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"Let's\", \"Let us\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"Could've\", \"Could have\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"youve\", \"you have\")\n    test[\"text\"] = test[\"text\"].str.replace(r\"It's\", \"It is\")\n    \n\n    \n    return test\n  \n\n","4a1e42c0":"df = prep(test)\n","f432a61e":"test_x = df[\"text\"]","6226ad48":"vectorizer = CountVectorizer()\nvectorizer.fit(test_x)\nvectorizer.transform(test_x)","0e292761":" vectorizer.transform(test_x)","01a0f01f":"x_test_co= vectorizer.transform(test_x)","1cbd3739":"x_train_count = vectorizer.transform(x_train)","9c78dcdb":"nb = naive_bayes.MultinomialNB()\nnb_model = nb.fit(x_train_count,y_train)\ny_pred = nb_model.predict(x_test_co)\ny_pred","326c4938":"dictt = {}\ndictt['id'] = test.id\ndictt['target'] = y_pred\nsubmission = pd.DataFrame(dictt)","6f14e82a":"submission","960d86b0":"#submission.to_csv(\"submission3.csv\" , index = None)","7f3419cf":"## Logistic Regression","5a506bcd":"## Count Vector\n\n\n![count%20vector.png](attachment:count%20vector.png)\n\nBy word frequency we indicate the number of times each token occurs in a text. When talking about word frequency, we distinguish between types and tokens. Types are the distinct words in a corpus, whereas tokens are the words, including repeats. Let's see how this works in practice.","669a92fb":"## Naive Bayes","b5c93d7f":"## Dropping punctuation marks","61184aa1":"## Lemmatization\n\n**Lemmatization :** In linguistics, it is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item. Putting an example to the definition, \u201ccomputers\u201d is an inflected form of \u201ccomputer\u201d, the same logic as \u201cdogs\u201d being an inflected form of \u201cdog\u201d.","4357c457":"# Prediction","19a1dfe7":"## Visualisation","19db17f1":"## Stopwords\n\n**Stopwords** : Words that are filtered out by Web search engines and other enterprise searching and indexing platforms. Stop words are natural language words which have very little meaning, such as \"and\", \"the\", \"a\", \"an\", and similar words.","e784df17":"## Dropping Numbers","36db4699":"## TF-IDF Word Level\n\n![tf%20idf.png](attachment:tf%20idf.png)\n\n\n\n**Term-frequency-inverse document frequency (TF-IDF)** is another way to judge the topic of an article by the words it contains. With TF-IDF, words are given weight \u2013 TF-IDF measures relevance, not frequency. That is, wordcounts are replaced with TF-IDF scores across the whole dataset.\n\n\n","39d0c089":"## XGBOOST","fb24dbe4":"## RF","5cec3f95":"## Logistic Regression","22edb524":"# Cleaning  & Preprocessing","20c95bb9":"## For the test\n","4c95558e":"## Naive Bayes","f32217d7":"# Credits\n\n\nhttps:\/\/www.webopedia.com\/TERM\/S\/stop_words.html\n\nhttps:\/\/www.twinword.com\/blog\/what-is-lemmatization\/\n\nhttps:\/\/port.sas.ac.uk\/mod\/book\/view.php?id=583&chapterid=381#:~:text=2.3%20Word%20count,-After%20tokenising%20a&text=By%20word%20frequency%20we%20indicate,how%20this%20works%20in%20practice.\n\nhttps:\/\/wiki.pathmind.com\/bagofwords-tf-idf\n\nhttps:\/\/medium.com\/deep-math-machine-learning-ai\/chapter-9-1-nlp-word-vectors-d51bff9628c1"}}