{"cell_type":{"c505f52a":"code","20c1a703":"code","92fef83e":"code","ed8b37dc":"code","1d290295":"code","47abb409":"code","d18bbad8":"code","50ed83d0":"code","f87ab7e7":"code","ea3b872d":"code","74e91466":"code","30148597":"code","542a3190":"code","d9ebe62f":"code","069d4f59":"code","b1b589b0":"code","96ed438e":"code","7e11786c":"code","aec508bd":"code","7b2fd1a7":"code","a7badfbf":"code","9b3e9d95":"code","80a5a83e":"code","95e63381":"code","aa698cf3":"code","5e5fe5d8":"code","5fb1b8ed":"code","858b3a6a":"code","54600736":"code","b76a2f85":"code","136818d4":"code","8a5b766a":"code","15716783":"code","458cef43":"code","f1799916":"code","336c1f3f":"code","d5257b8a":"code","23b362f3":"code","b0ed5877":"code","fce817c0":"code","20c692f6":"code","20619352":"code","589b87c4":"code","79fe3782":"code","aa425926":"code","addda0cb":"code","90e6c528":"code","d772bec4":"code","e822f171":"code","30cb54e2":"code","67075ada":"markdown","45fe5f03":"markdown","9aa236f5":"markdown","3d783d6e":"markdown","184e3d06":"markdown","09b7c70b":"markdown","1e78e18a":"markdown","c1a66784":"markdown","7014a921":"markdown","b622be6a":"markdown","d2831cbf":"markdown","6ebd0da9":"markdown"},"source":{"c505f52a":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","20c1a703":"!pip install pyspark","92fef83e":"import pyspark\nsc = pyspark.SparkContext(appName=\"intro_to_spark\")\n\nlog_of_songs = [\n        \"Despacito\",\n        \"Nice for what\",\n        \"No tears left to cry\",\n        \"Despacito\",\n        \"Havana\",\n        \"In my feelings\",\n        \"Nice for what\",\n        \"despacito\",\n        \"All the stars\"\n]\n\n# parallelize the log_of_songs to use with Spark\ndistributed_song_log = sc.parallelize(log_of_songs)","ed8b37dc":"def convert_song_to_lowercase(song):\n    return song.lower()","1d290295":"distributed_song_log.map(convert_song_to_lowercase)","47abb409":"distributed_song_log.map(convert_song_to_lowercase).collect()","d18bbad8":"distributed_song_log.collect()","50ed83d0":"from pyspark import SparkConf\nfrom pyspark.sql import SparkSession","f87ab7e7":"spark = SparkSession \\\n    .builder \\\n    .appName(\"intro_to_spark\") \\\n    .getOrCreate()","ea3b872d":"spark.sparkContext.getConf().getAll()","74e91466":"spark","30148597":"path = \"..\/input\/sparkify-log-small\"\nuser_log = spark.read.json(path)","542a3190":"user_log.printSchema()","d9ebe62f":"user_log.describe()","069d4f59":"user_log.show(n=1)","b1b589b0":"user_log.take(5)","96ed438e":"out_path = \".\/\/sparkify_log_small.csv\"\nuser_log.write.save(out_path, format=\"csv\", header=True)","7e11786c":"user_log_2 = spark.read.csv(out_path, header=True)","aec508bd":"user_log_2.printSchema()","7b2fd1a7":"user_log_2.take(2)","a7badfbf":"user_log_2.select(\"userID\").show()","9b3e9d95":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\nfrom pyspark.sql.types import IntegerType\nfrom pyspark.sql.functions import desc\nfrom pyspark.sql.functions import asc\nfrom pyspark.sql.functions import sum as Fsum\n\nimport datetime\n\nimport numpy as np\nimport pandas as pd\n%matplotlib inline\nimport matplotlib.pyplot as plt","80a5a83e":"path = \"..\/input\/sparkify-log-small\/sparkify_log_small.json\"\nuser_log = spark.read.json(path)","95e63381":"user_log.take(5)","aa698cf3":"user_log.printSchema()","5e5fe5d8":"user_log.describe().show()","5fb1b8ed":"user_log.describe(\"artist\").show()","858b3a6a":"user_log.describe(\"sessionId\").show()","54600736":"user_log.count()","b76a2f85":"user_log.select(\"page\").dropDuplicates().sort(\"page\").show()","136818d4":"user_log.select([\"userId\", \"firstname\", \"page\", \"song\"]).where(user_log.userId == \"1046\").collect()","8a5b766a":"get_hour = udf(lambda x: datetime.datetime.fromtimestamp(x \/ 1000.0). hour)","15716783":"user_log = user_log.withColumn(\"hour\", get_hour(user_log.ts))","458cef43":"user_log.head()","f1799916":"songs_in_hour = user_log.filter(user_log.page == \"NextSong\").groupby(user_log.hour).count().orderBy(user_log.hour.cast(\"float\"))\nsongs_in_hour.show()","336c1f3f":"songs_in_hour_pd = songs_in_hour.toPandas()\nsongs_in_hour_pd.hour = pd.to_numeric(songs_in_hour_pd.hour)\n\nplt.scatter(songs_in_hour_pd[\"hour\"], songs_in_hour_pd[\"count\"])\nplt.xlim(-1, 24);\nplt.ylim(0, 1.2 * max(songs_in_hour_pd[\"count\"]))\nplt.xlabel(\"Hour\")\nplt.ylabel(\"Songs played\");","d5257b8a":"user_log_valid = user_log.dropna(how = \"any\", subset = [\"userId\", \"sessionId\"])","23b362f3":"user_log_valid.count()","b0ed5877":"user_log.select(\"userId\").dropDuplicates().sort(\"userId\").show()","fce817c0":"user_log_valid = user_log_valid.filter(user_log_valid[\"userId\"] != \"\")","20c692f6":"user_log_valid.count()","20619352":"user_log_valid.filter(\"page = 'Submit Downgrade'\").show()","589b87c4":"user_log.select([\"userId\", \"firstname\", \"page\", \"level\", \"song\"]).where(user_log.userId == \"1138\").collect()","79fe3782":"flag_downgrade_event = udf(lambda x: 1 if x == \"Submit Downgrade\" else 0, IntegerType())","aa425926":"user_log_valid = user_log_valid.withColumn(\"downgraded\", flag_downgrade_event(\"page\"))","addda0cb":"user_log_valid.head()","90e6c528":"from pyspark.sql import Window","d772bec4":"windowval = Window.partitionBy(\"userId\").orderBy(desc(\"ts\")).rangeBetween(Window.unboundedPreceding, 0)","e822f171":"user_log_valid = user_log_valid.withColumn(\"phase\", Fsum(\"downgraded\").over(windowval))","30cb54e2":"user_log_valid.select([\"userId\", \"firstname\", \"ts\", \"page\", \"level\", \"phase\"]).where(user_log.userId == \"1138\").sort(\"ts\").collect()","67075ada":"Note as well that Spark is not changing the original data set: Spark is merely making a copy. You can see this by running collect() on the original dataset.","45fe5f03":"Now let's save this as a csv file","9aa236f5":"You'll notice that this code cell ran quite quickly. This is because of lazy evaluation. Spark does not actually execute the map step unless it needs to.\n\n\"RDD\" in the output refers to resilient distributed dataset. RDDs are exactly what they say they are: fault-tolerant datasets distributed across a cluster. This is how Spark stores data.\n\nTo get Spark to actually run the map step, you need to use an \"action\". One available action is the collect method. The collect() method takes the results from all of the clusters and \"collects\" them into a single list on the master node.","3d783d6e":"map step will go through each song in the list and apply the convert_song_to_lowercase() function.","184e3d06":"# Data Exploration\n","09b7c70b":"# Users Downgrade Their Accounts\n\nLet's find when users downgrade their accounts and then flag those log entries.","1e78e18a":"# Reading and Writing Data with Spark\n","c1a66784":"# PySpark tutorial for beginners\n\nThis notebook is a part of my learning journey which I've been documenting from Udacity's Data Scientist Nanodegree program, which helped me a lot to learn and excel advanced data science stuff such as PySpark. Thank you so much Udacity for providing such quality content. \n\n![pyspark.jpg](attachment:66ee4703-f3ec-4eb2-a789-947c5f5ebdf7.jpg)\n\nSpark is a  big data framework which contains libraries for data analysis, machine learning, graph analysis, and streaming live data. Spark is generally faster than Hadoop. This is because Hadoop writes intermediate results to disk whereas Spark tries to keep intermediate results in memory whenever possible.\n\nThe Hadoop ecosystem includes a distributed file storage system called HDFS (Hadoop Distributed File System). Spark, on the other hand, does not include a file storage system. You can use Spark on top of HDFS but you do not have to. Spark can read in data from other sources as well such as Amazon S3.\n\nSpark doesn't implement MapReduce, you can write Spark programs that behave in a similar way to the map-reduce pattern.\n\n**Limitations of spark:**\n\n- Spark Streaming\u2019s latency is at least 500 milliseconds since it operates on micro-batches of records, instead of processing one record at a time. Native streaming tools such as Storm, Apex, or Flink can push down this latency value and might be more suitable for low-latency applications. Flink and Apex can be used for batch computation as well, so if you're already using them for stream processing, there's no need to add Spark to your stack of technologies.\n\n- Another limitation of Spark is its selection of machine learning algorithms. Currently, Spark only supports algorithms that scale linearly with the input data size. In general, deep learning is not available either, though there are many projects integrate Spark with Tensorflow and other deep learning tools.\n\nSource:- Spark and Pyspark documentation","7014a921":"# Maps \nMaps take data as input and then transform that data with whatever function you put in the map. \nThey are like directions for the data telling how each input should get to the output.\n\nFirst, we need to create a SparkContext object. With the SparkContext, you can input a dataset and parallelize the data across a cluster.","b622be6a":"# Calculating Statistics by Hour","d2831cbf":"# Drop Rows with Missing Values","6ebd0da9":"Now, let's create our first dataframe from a small sample data set which is a log file data set that describes user interactions with a music streaming service. The records describe events such as logging in to the site, visiting a page, listening to the next song, seeing an ad."}}