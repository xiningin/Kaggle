{"cell_type":{"5b12c35e":"code","2cfc71c2":"code","67f2165f":"code","5f7064ef":"code","06f93fab":"code","d494716d":"code","7dad0224":"code","68cf5c8e":"code","d9cc853c":"code","3450cf54":"code","78285314":"code","df2fd0c2":"code","50c1c7c4":"markdown","008a610c":"markdown","acdd762c":"markdown","12f7bf5e":"markdown","af068768":"markdown","d3a780ec":"markdown","11d6aeea":"markdown","16dd6709":"markdown","f7fa6b06":"markdown","3cf2e99e":"markdown","41f9289a":"markdown","846c84bb":"markdown"},"source":{"5b12c35e":"import torch\nfrom transformers import BertTokenizer, BertModel\nimport logging\nimport matplotlib.pyplot as plt\nimport transformers\nfrom transformers import *\nfrom tokenizers import BertWordPieceTokenizer\nimport pandas as pd\nimport numpy as np\nfrom scipy.spatial.distance import cosine\nimport cudf, cuml, cupy\nimport gc\nimport numpy as np\nfrom sklearn.preprocessing import normalize\nfrom scipy.spatial.distance import hamming\nfrom IPython.display import clear_output\nimport imagehash","2cfc71c2":"tokenizer = BertTokenizer.from_pretrained('..\/input\/bert-large-tokenizer')\nfast_tokenizer = BertWordPieceTokenizer('..\/input\/bert-large-tokenizer\/vocab.txt', lowercase=True )\nfast_tokenizer","67f2165f":"test=pd.read_csv('..\/input\/shopee-product-matching\/test.csv')","5f7064ef":"test.head()","06f93fab":"def fast_encode(texts, tokenizer, chunk_size=35000, maxlen=200):\n    tokenizer.enable_truncation(max_length=maxlen)\n    tokenizer.enable_padding()\n    all_ids = []\n    \n    for i in range(0, len(texts), chunk_size):\n        text_chunk = texts[i:i+chunk_size].tolist()\n        encs = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([enc.ids for enc in encs])\n    \n    return np.array(all_ids)","d494716d":"embeddings = fast_encode(test['title'].values, fast_tokenizer, maxlen=200)\nembeddings=normalize(embeddings)\npreds = []\nCHUNK = 1024*4\ntext_embeddings=cupy.array(embeddings)\nprint('Finding similar titles...')\nCTS = len(test)\/\/CHUNK\nif len(test)%CHUNK!=0: CTS += 1\nfor j in range( CTS ):\n    \n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b,len(test))\n    print('chunk',a,'to',b)\n    \n    # COSINE SIMILARITY DISTANCE\n    cts = cupy.matmul( text_embeddings, text_embeddings[a:b].T).T\n    \n    for k in range(b-a):\n        IDX = cupy.where(cts[k,]>0.995)[0]\n        o = test.iloc[cupy.asnumpy(IDX)].posting_id.values\n        preds.append(o)\ndel text_embeddings,embeddings\n_=gc.collect()","7dad0224":"posting=test['posting_id'].values\nans=[]\nfor i in range(len(test)):\n    s=posting[i]\n    if len(preds[i])>1 :\n        for j in preds[i]:\n            if j!=posting[i]:\n                s+=' '+j\n    ans.append(s)\npreds=[]\nfor i in ans:\n    preds.append(i.split(' '))","68cf5c8e":"hash_codes=np.array([imagehash.hex_to_flathash(i,64).hash for i in test['image_phash'].values])\nhash_preds=[]\ncount=1\nfor i in range(len(hash_codes)) :\n    print(count*100\/len(hash_codes),' % is done')\n    clear_output(wait=True)\n    count+=1\n    x=sum((hash_codes[i]*hash_codes).T)\n    y=sum((~hash_codes[i]*~hash_codes).T)\n    t=x+y\n    t=64-t\n    IDX = np.where(t<10)[1]\n    o = test.iloc[IDX].posting_id.values\n    hash_preds.append(o)\nhash_preds=[list(i) for i in hash_preds]","d9cc853c":"hash_preds","3450cf54":"%%time\ntest_list = test[['posting_id', 'title', 'image', 'image_phash']].reset_index(drop=True).values.tolist()\nsubmission_list = []\n# ==========================================\n# Case. title[1] & image[2] & image_phash[3]\n# ==========================================\ncount=0\nfor item in test_list:\n    res1 = test[test['title']==item[1]]['posting_id'].unique().tolist()\n    res2 = test[test['image']==item[2]]['posting_id'].unique().tolist()\n    res2=list(set(res2+preds[count]))\n    res3 = test[test['image_phash']==item[3]]['posting_id'].unique().tolist()\n    res3=list(set(res3+hash_preds[count]))\n    res = list(set(res1 + res2 + res3))\n    res.remove(item[0])\n    matches = ' '.join(map(str, res))\n    matches = f'{item[0]} {matches}'\n    submission_list.append([item[0], matches])\n    count+=1\n\n","78285314":"submission = pd.DataFrame(submission_list, columns = ['posting_id','matches'])\nsubmission.to_csv('submission.csv', index=False)","df2fd0c2":"submission.head()","50c1c7c4":"# Reference For Baseline model from : https:\/\/www.kaggle.com\/cdeotte\/part-2-rapids-tfidfvectorizer-cv-0-700 by chris deotte :)","008a610c":"<p style = \"font-family:courier,arial,helvetica;font-size:300%;\">\nRecent Update :\n<ol style=\"font-family:courier,arial,helvetica;font-size:200%\"><li>I added Phash Similarity Checker With a metric which is fast :)<\/li><\/ol>\n<\/p>","acdd762c":"<p style = \"font-family:courier,arial,helvetica;font-size:300%;\">\nBest Results Using PHASH<\/p>","12f7bf5e":"<p style = \"font-family:courier,arial,helvetica;font-size:300%;\">\nFinding Best Results<\/p>","af068768":"<p style = \"font-family:courier,arial,helvetica;font-size:300%;\">\nImporting Packages<\/p>","d3a780ec":"<p style = \"font-family:courier,arial,helvetica;font-size:300%;\">\nImporting Saved Tokenizer<\/p>","11d6aeea":"<p style = \"font-family:courier,arial,helvetica;font-size:300%;\">\nResult of Both Tokenizer and BaseLine Model Combined<\/p>","16dd6709":"![](https:\/\/img.lovepik.com\/original_origin_pic\/18\/05\/29\/d49c1a025873dc18ca5919263197a3cc.png_wh300.png)","f7fa6b06":"<p style = \"font-family:courier,arial,helvetica;font-size:300%;\">\nEncoding The Data<\/p>","3cf2e99e":"<p style = \"font-family:courier,arial,helvetica;font-size:300%;\">\nImporting Test Data<\/p>","41f9289a":"<p style = \"font-family:courier,arial,helvetica;font-size:200%;\">\nHow is this working?? \ud83e\udd14\ud83e\udd14\n<\/p>\n<p style = \"font-family:courier,arial,helvetica;font-size:100%;\">\n    Well i had a basic idea of how i can filter True-False or False-True from the arrays so i used the XNOR function like this and took out the sum of them and subtracted them from 64 which is the highest value which made a distance . So if is closer to 0 means that the hash code has high similarity from the choosen hash code otherwise it is different from it :)\n    <br>\n    It was better than the time it took for hamming distance to run and seems to be good :)\n<\/p>","846c84bb":"<p style = \"font-family:courier,arial,helvetica;font-size:300%;\">\nShopee - Price Match Gaurentee\n<\/p>"}}