{"cell_type":{"0024e9c2":"code","c03b9e58":"code","bbc17631":"code","5508e2f9":"code","d75977b1":"code","16d07be6":"code","d6983229":"code","d972415f":"code","8abff744":"code","0be613ee":"code","860087fa":"code","ccc4b03f":"code","65c791aa":"code","ae8e66c0":"code","1b50915c":"code","86271d22":"code","22a50746":"code","652b17b5":"code","50535e81":"code","56a2a89f":"code","16f97053":"code","d72c1e30":"code","578fad32":"code","ccc026e6":"code","cbddbf64":"code","d97e739a":"code","f4009f6e":"code","28913e5f":"code","6d84830c":"code","4a9266f1":"code","3875f927":"code","44e3539f":"code","798c6435":"code","f70c0c83":"code","4d76b789":"code","eb951bbb":"code","8838aaab":"code","70f6025b":"code","870526ac":"code","c5a8272e":"code","e96f38ec":"code","6f9ba409":"code","a2b2cb97":"code","6b1a824b":"code","d488016f":"code","fda12c63":"code","76f7c012":"code","776d88af":"code","176e2452":"code","30826009":"code","dd5a3a92":"code","b7c40e71":"code","0537fe67":"code","76ba98ab":"code","c6c016ee":"code","298cb71d":"code","cb253a2d":"code","c05eb165":"code","6b01dea0":"code","7c33b58b":"code","27b3ab23":"code","9323c521":"code","aae2aab2":"code","e1ada1cf":"code","6faeb7d9":"code","a467fed8":"code","d00a07b0":"code","406fec3e":"code","527d287a":"code","508d15b7":"code","c35f4cab":"code","cd3e4ca1":"code","9f9202cf":"code","ea9b83ce":"code","5f21ee7e":"code","24496639":"code","0d1323a7":"code","a42632fe":"code","68642b7a":"code","7f0dbc41":"code","28b6fbf5":"code","7971c1dd":"code","e5859508":"code","455b73e4":"code","2b219da5":"code","ab23b74f":"code","dfd3c90c":"code","4421686a":"code","30240b73":"code","4f09d132":"code","cb3dcaaf":"code","4c95ab78":"code","d00d0c38":"code","4863c829":"code","b668520e":"code","8b65cf2e":"markdown","1946f5ed":"markdown","9cc349e3":"markdown","1b1a9b04":"markdown","17454d4f":"markdown","1c3fdaee":"markdown","5adcff22":"markdown","4abd2de8":"markdown","e4c6af8e":"markdown","5916fdd5":"markdown","87a1ffc4":"markdown","1858dc09":"markdown","e00bd40a":"markdown","e24b5434":"markdown","c0f94731":"markdown","71125cbb":"markdown","4b0eea1a":"markdown","384c0ce8":"markdown","964a45ea":"markdown","4d2960e7":"markdown","c5c501c1":"markdown","5fd99823":"markdown","45524a6c":"markdown","4fc6c052":"markdown","374bf3e8":"markdown","566d9f51":"markdown","a32a5dc2":"markdown","87b25b64":"markdown","8b0ed53a":"markdown","85cd82b8":"markdown","36fd08f1":"markdown","4c0d2b00":"markdown","d0da6074":"markdown","2640d9e2":"markdown","0337c2b6":"markdown","fdd6d05c":"markdown","8029e218":"markdown","3ab02dac":"markdown","8d16417f":"markdown"},"source":{"0024e9c2":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\n\nfrom datetime import datetime\nfrom category_encoders import TargetEncoder\n\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\nfrom scipy import stats\nfrom scipy.stats import skew, boxcox_normmax, norm\nfrom scipy.special import boxcox1p\nfrom lightgbm import LGBMRegressor\n\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.ticker import MaxNLocator\n\nimport warnings\nwarnings.simplefilter('ignore')\nprint('Setup complete')","c03b9e58":"# Read data\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nprint(\"Train set size:\", train.shape)\nprint(\"Test set size:\", test.shape)","bbc17631":"train.head()","5508e2f9":"# Helper Functions\n# https:\/\/www.kaggle.com\/mviola\/house-prices-eda-lasso-lightgbm-0-11635 special thanks to this notebook its helped me a lot, for eda part.\ndef plot_numerical(col, discrete=False):\n    if discrete:\n        fig, ax = plt.subplots(1,2,figsize=(12,6))\n        sns.stripplot(x=col, y='SalePrice', data=train, ax=ax[0])\n        sns.countplot(train[col], ax=ax[1])\n        fig.suptitle(str(col) + ' analysis')\n    else:\n        fig, ax = plt.subplots(1,2,figsize=(12,6))\n        sns.scatterplot(x=col, y='SalePrice', data=train, ax=ax[0])\n        sns.distplot(train[col], kde=False, ax=ax[1])\n        fig.suptitle(str(col) + ' analysis')\n\ndef plot_categorical(col):\n    fig, ax = plt.subplots(1,2,figsize=(12,6), sharey=True)\n    sns.stripplot(x=col, y='SalePrice', data=train, ax=ax[0])\n    sns.boxplot(x=col, y='SalePrice', data=train, ax=ax[1])\n    fig.suptitle(str(col) + ' analysis')\n    \nprint('Plot functions are ready to use')","d75977b1":"plt.figure(figsize=(8,5))\na = sns.distplot(train.SalePrice, kde=False)\nplt.title('SalePrice distribution')\na = plt.axvline(train.SalePrice.describe()['25%'], color='b')\na = plt.axvline(train.SalePrice.describe()['75%'], color='b')\nprint('SalePrice description:')\nprint(train.SalePrice.describe().to_string())","16d07be6":"# Select numerical features only\nnum_features = [col for col in train.columns if train[col].dtype in ['int64', 'float64']]\n# Remove Id & SalePrice \nnum_features.remove('Id')\nnum_features.remove('SalePrice')\n# Create a numerical columns only dataframe\nnum_analysis = train[num_features].copy()\n# Impute missing values with the median just for the moment\nfor col in num_features:\n    if num_analysis[col].isnull().sum() > 0:\n        num_analysis[col] = SimpleImputer(strategy='median').fit_transform(num_analysis[col].values.reshape(-1,1))\n# Train a model   \nclf = ExtraTreesRegressor(random_state=42)\nh = clf.fit(num_analysis, train.SalePrice)\nfeature_imp = pd.DataFrame(sorted(zip(clf.feature_importances_,num_features)), columns=['Value','Feature'])\nplt.figure(figsize=(16,10))\nsns.barplot(x='Value', y='Feature', data=feature_imp.sort_values(by='Value', ascending=False))\nplt.title('Most important numerical features with ExtraTreesRegressor')\ndel clf, h;","d6983229":"plt.figure(figsize=(8,8))\nplt.title('Correlation matrix with SalePrice')\nselected_columns = ['OverallQual', 'GarageCars', 'GrLivArea', 'YearBuilt', 'FullBath', '1stFlrSF', 'TotalBsmtSF', 'GarageArea']\na = sns.heatmap(train[selected_columns + ['SalePrice']].corr(), annot=True, square=True)","d972415f":"plot_numerical('OverallQual', True)","8abff744":"plot_numerical('GarageCars', True)","0be613ee":"plot_numerical('GrLivArea')","860087fa":"plot_numerical('YearBuilt')","ccc4b03f":"plot_numerical('FullBath', True)","65c791aa":"plot_numerical('1stFlrSF')","ae8e66c0":"plot_numerical('TotalBsmtSF')","1b50915c":"plot_numerical('GarageArea')","86271d22":"# Select categorical features only\ncat_features = [col for col in train.columns if train[col].dtype =='object']\n# Create a categorical columns only dataframe\ncat_analysis = train[cat_features].copy()\n# Impute missing values with NA just for the moment\nfor col in cat_analysis:\n    if cat_analysis[col].isnull().sum() > 0:\n        cat_analysis[col] = SimpleImputer(strategy='constant').fit_transform(cat_analysis[col].values.reshape(-1,1))\n# Target encoding\ntarget_enc = TargetEncoder(cols=cat_features)\ncat_analysis = target_enc.fit_transform(cat_analysis, train.SalePrice) \n# Train a model \nclf = ExtraTreesRegressor(random_state=42)\nh = clf.fit(cat_analysis, train.SalePrice)\nfeature_imp = pd.DataFrame(sorted(zip(clf.feature_importances_,cat_features)), columns=['Value','Feature'])\nplt.figure(figsize=(16,10))\nsns.barplot(x='Value', y='Feature', data=feature_imp.sort_values(by='Value', ascending=False))\nplt.title('Most important categorical features with ExtraTreesRegressor')\ndel clf, h;","22a50746":"fig, ax = plt.subplots(1,2,figsize=(16,6), sharey=True)\nsns.stripplot(x='Neighborhood', y='SalePrice', data=train, ax=ax[0])\nsns.boxplot(x='Neighborhood', y='SalePrice', data=train, ax=ax[1])\nax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=90)\nax[1].set_xticklabels(ax[1].get_xticklabels(), rotation=90)\nfig.suptitle('Neighborhood analysis')\nplt.show()","652b17b5":"plot_categorical('ExterQual')","50535e81":"plot_categorical('BsmtQual')","56a2a89f":"plot_categorical('KitchenQual')","16f97053":"fig, ax = plt.subplots(1,2,figsize=(16,6), sharey=True)\ntrain_missing = round(train.isnull().mean()*100, 2)\ntrain_missing = train_missing[train_missing > 0]\ntrain_missing.sort_values(inplace=True)\nsns.barplot(train_missing.index, train_missing, ax=ax[0], color='orange')\nax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=90)\nax[0].set_ylabel('Percentage of missing values')\nax[0].set_title('Train set')\ntest_missing = round(test.isnull().mean()*100, 2)\ntest_missing = test_missing[test_missing > 0]\ntest_missing.sort_values(inplace=True)\nsns.barplot(test_missing.index, test_missing, ax=ax[1], color='orange')\nax[1].set_xticklabels(ax[1].get_xticklabels(), rotation=90)\nax[1].set_title('Test set')\nplt.show()","d72c1e30":"plot_numerical('LotFrontage')","578fad32":"print('LotFrontage minimum:', train.LotFrontage.min())","ccc026e6":"plot_categorical('FireplaceQu')","cbddbf64":"fig, ax = plt.subplots(2,2,figsize=(12,10), sharey=True)\nsns.stripplot(x='Fence', y='SalePrice', data=train, ax=ax[0][0])\nsns.stripplot(x='Alley', y='SalePrice', data=train, ax=ax[0][1])\nsns.stripplot(x='MiscFeature', y='SalePrice', data=train, ax=ax[1][0])\nsns.stripplot(x='PoolQC', y='SalePrice', data=train, ax=ax[1][1])\nfig.suptitle('Analysis of columns with more than 80% of missing values')\nplt.show()","d97e739a":"# Gereksiz olan ID s\u00fctununu \u00e7\u0131kar\u0131yorum.\n\ntrain.drop('Id', axis=1, inplace=True)\ntest.drop('Id', axis=1, inplace=True)\n\ntrain_features = train\ntest_features = test","f4009f6e":"# \u00d6n i\u015fleme i\u015flemleri i\u00e7in train ve test veri setini birle\u015ftirme.\n\nfeatures = pd.concat([train, test]).reset_index(drop=True)\nprint(features.shape)","28913e5f":"def missing_values_table(dataframe):\n    variables_with_na = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n    n_miss = dataframe[variables_with_na].isnull().sum().sort_values(ascending=False)\n    ratio = (dataframe[variables_with_na].isnull().sum() \/ dataframe.shape[0] * 100).sort_values(ascending=False)\n    dtypes = dataframe.dtypes\n    dtypesna = dtypes.loc[(np.sum(features.isnull()) != 0)]\n    missing_df = pd.concat([n_miss, np.round(ratio, 2), dtypesna], axis=1, keys=['n_miss', 'ratio', 'type'])\n    if len(missing_df)>0:\n        print(missing_df)\n        missing = train.isnull().sum()\n        missing = missing[missing > 0]\n        missing.sort_values(inplace=True)\n        missing.plot.bar()\n        print(\"\\nThere are {} columns with missing values\\n\".format(len(missing_df)))\n    else:\n        print(\"\\nThere is no missing value\")\nmissing_values_table(features)","6d84830c":"# Buradaki eksik g\u00f6zlemler o \u00f6zelli\u011fin olmad\u0131\u011f\u0131 anlam\u0131na gelmekte. Bu y\u00fczden None atayaca\u011f\u0131m.\n\nnone_cols = ['Alley', 'PoolQC', 'MiscFeature', 'Fence', 'FireplaceQu', 'GarageType',\n             'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond',\n             'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType']\n\n# Say\u0131sal de\u011fi\u015fkenlerdeki eksik g\u00f6zlemler de ayn\u0131 \u015fekilde, bunlara 0 at\u0131yorum.\n\nzero_cols = ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath','BsmtHalfBath', 'GarageYrBlt', 'GarageArea', 'GarageCars', 'MasVnrArea']\n\n# Bu de\u011fi\u015fkenlerdeki eksik g\u00f6zlemlere mod atayaca\u011f\u0131m.\n\nfreq_cols = ['Electrical', 'Exterior1st', 'Exterior2nd', 'Functional', 'KitchenQual','SaleType', 'Utilities']\n\n\nfor col in zero_cols:\n    features[col].replace(np.nan, 0, inplace=True)\n\nfor col in none_cols:\n    features[col].replace(np.nan, 'None', inplace=True)\n\nfor col in freq_cols:\n    features[col].replace(np.nan, features[col].mode()[0], inplace=True)","4a9266f1":"# MsZoning de\u011fi\u015fkenindeki bo\u015f de\u011ferleri MSSubClassa g\u00f6re doldurma.\n\nfeatures['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].apply(\n    lambda x: x.fillna(x.mode()[0]))\n\n# LotFrontage m\u00fclkiyetin cadde ile ba\u011flant\u0131s\u0131n\u0131 g\u00f6steren bir de\u011fi\u015fken, her mahallenin cadde ba\u011flant\u0131s\u0131n\u0131n birbirine benzeyebilece\u011finden bunu Neighborhood'a a g\u00f6re doldurdum.\n\nfeatures['LotFrontage'] = features.groupby(\n    ['Neighborhood'])['LotFrontage'].apply(lambda x: x.fillna(x.median()))\n\n# Say\u0131sal de\u011fi\u015fken olup asl\u0131nda kategorik de\u011fi\u015fken olmas\u0131 gerekenleri d\u00fczeltme\n\nfeatures['MSSubClass'] = features['MSSubClass'].astype(str)\nfeatures['YrSold'] = features['YrSold'].astype(str)\nfeatures['MoSold'] = features['MoSold'].astype(str)","3875f927":"missing_values_table(features)","44e3539f":"def stalk(dataframe, var, target=\"SalePrice\"):\n    print(\"{}  | type: {}\\n\".format(var, dataframe[var].dtype))\n    print(pd.DataFrame({\"n\": dataframe[var].value_counts(),\n                                \"Ratio\": 100 * dataframe[var].value_counts() \/ len(dataframe),\n                                \"TARGET_MEDIAN\": dataframe.groupby(var)[target].median(),\n                                \"Target_MEAN\": dataframe.groupby(var)[target].mean()}), end=\"\\n\\n\\n\")\n    \n    plt.figure(figsize=(15,5))\n    chart = sns.countplot(\n    data=features,\n    x=features[var])\n    chart.set_xticklabels(chart.get_xticklabels(), rotation=45, horizontalalignment='right')\n    plt.show();","798c6435":"stalk(features, \"Neighborhood\")","f70c0c83":"# Neighboor i\u00e7erisindeki benzer de\u011ferde olanlar\u0131 birbiri ile gruplad\u0131m.\n\nneigh_map = {'MeadowV': 1,'IDOTRR': 1,'BrDale': 1,'BrkSide': 2,'OldTown': 2,'Edwards': 2,\n             'Sawyer': 3,'Blueste': 3,'SWISU': 3,'NPkVill': 3,'NAmes': 3,'Mitchel': 4,\n             'SawyerW': 5,'NWAmes': 5,'Gilbert': 5,'Blmngtn': 5,'CollgCr': 5,\n             'ClearCr': 6,'Crawfor': 6,'Veenker': 7,'Somerst': 7,'Timber': 8,\n             'StoneBr': 9,'NridgHt': 10,'NoRidge': 10}\n\nfeatures['Neighborhood'] = features['Neighborhood'].map(neigh_map).astype('int')","4d76b789":"# Derecelendirme i\u00e7eren de\u011fi\u015fkenleri ordinal yap\u0131ya getirdim.\n\next_map = {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\nfeatures['ExterQual'] = features['ExterQual'].map(ext_map).astype('int')\n\next_map = {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\nfeatures['ExterCond'] = features['ExterCond'].map(ext_map).astype('int')\n\nbsm_map = {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\nfeatures['BsmtQual'] = features['BsmtQual'].map(bsm_map).astype('int')\nfeatures['BsmtCond'] = features['BsmtCond'].map(bsm_map).astype('int')\n\nbsmf_map = {'None': 0,'Unf': 1,'LwQ': 2,'Rec': 3,'BLQ': 4,'ALQ': 5,'GLQ': 6}\nfeatures['BsmtFinType1'] = features['BsmtFinType1'].map(bsmf_map).astype('int')\nfeatures['BsmtFinType2'] = features['BsmtFinType2'].map(bsmf_map).astype('int')\n\nheat_map = {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\nfeatures['HeatingQC'] = features['HeatingQC'].map(heat_map).astype('int')\nfeatures['KitchenQual'] = features['KitchenQual'].map(heat_map).astype('int')\nfeatures['FireplaceQu'] = features['FireplaceQu'].map(bsm_map).astype('int')\nfeatures['GarageCond'] = features['GarageCond'].map(bsm_map).astype('int')\nfeatures['GarageQual'] = features['GarageQual'].map(bsm_map).astype('int')","eb951bbb":"# RARE ANALYZER\ndef rare_analyser(dataframe, target, rare_perc):\n    rare_columns = [col for col in dataframe.columns if len(dataframe[col].value_counts()) <= 20\n                    and (dataframe[col].value_counts() \/ len(dataframe) < rare_perc).any(axis=None)]\n    for var in rare_columns:\n        print(var, \":\", len(dataframe[var].value_counts()))\n        print(pd.DataFrame({\"COUNT\": dataframe[var].value_counts(),\n                            \"RATIO\": dataframe[var].value_counts() \/ len(dataframe),\n                            \"TARGET_MEDIAN\": dataframe.groupby(var)[target].median()}), end=\"\\n\\n\\n\")\n\n\nrare_analyser(features, \"SalePrice\", 0.01)","8838aaab":"def stalk(dataframe, var, target=\"SalePrice\"):\n    print(\"{}  | type: {}\\n\".format(var, dataframe[var].dtype))\n    print(pd.DataFrame({\"n\": dataframe[var].value_counts(),\n                                \"Ratio\": 100 * dataframe[var].value_counts() \/ len(dataframe),\n                                \"TARGET_MEDIAN\": dataframe.groupby(var)[target].median(),\n                                \"Target_MEAN\": dataframe.groupby(var)[target].mean()}), end=\"\\n\\n\\n\")\n    \n    plt.figure(figsize=(10,5))\n    chart = sns.countplot(\n    data=features,\n    x=features[var])\n    chart.set_xticklabels(chart.get_xticklabels(), rotation=45, horizontalalignment='right')\n    plt.show();","70f6025b":"stalk(features,\"LotShape\")","870526ac":"features.loc[(features[\"LotShape\"] == \"Reg\"), \"LotShape\"] = 1\nfeatures.loc[(features[\"LotShape\"] == \"IR1\"), \"LotShape\"] = 2\nfeatures.loc[(features[\"LotShape\"] == \"IR2\"), \"LotShape\"] = 3 \nfeatures.loc[(features[\"LotShape\"] == \"IR3\"), \"LotShape\"] = 3 \n\nfeatures[\"LotShape\"] = features[\"LotShape\"].astype(\"int\")","c5a8272e":"stalk(features,\"GarageCars\")","e96f38ec":"features.loc[(features[\"GarageCars\"] == \"4\"), \"GarageCars\"] = 3","6f9ba409":"stalk(features,\"LotConfig\")","a2b2cb97":"features.loc[(features[\"LotConfig\"]==\"Inside\"),\"LotConfig\"] = 1\nfeatures.loc[(features[\"LotConfig\"]==\"FR2\"),\"LotConfig\"] = 1\nfeatures.loc[(features[\"LotConfig\"]==\"Corner\"),\"LotConfig\"] = 1\n\nfeatures.loc[(features[\"LotConfig\"]==\"FR3\"),\"LotConfig\"] = 2\nfeatures.loc[(features[\"LotConfig\"]==\"CulDSac\"),\"LotConfig\"] = 2","6b1a824b":"stalk(features, \"LandSlope\")","d488016f":"features.loc[features[\"LandSlope\"] == \"Gtl\", \"LandSlope\"] = 1\n\nfeatures.loc[features[\"LandSlope\"] == \"Sev\", \"LandSlope\"] = 2\nfeatures.loc[features[\"LandSlope\"] == \"Mod\", \"LandSlope\"] = 2\nfeatures[\"LandSlope\"]= features[\"LandSlope\"].astype(\"int\")","fda12c63":"stalk(features,\"OverallQual\")","76f7c012":"features.loc[features[\"OverallQual\"] == 1, \"OverallQual\"] = 1\nfeatures.loc[features[\"OverallQual\"] == 2, \"OverallQual\"] = 1\nfeatures.loc[features[\"OverallQual\"] == 3, \"OverallQual\"] = 1\nfeatures.loc[features[\"OverallQual\"] == 4, \"OverallQual\"] = 2\nfeatures.loc[features[\"OverallQual\"] == 5, \"OverallQual\"] = 3\nfeatures.loc[features[\"OverallQual\"] == 6, \"OverallQual\"] = 4\nfeatures.loc[features[\"OverallQual\"] == 7, \"OverallQual\"] = 5\nfeatures.loc[features[\"OverallQual\"] == 8, \"OverallQual\"] = 6\nfeatures.loc[features[\"OverallQual\"] == 9, \"OverallQual\"] = 7\nfeatures.loc[features[\"OverallQual\"] == 10, \"OverallQual\"] = 8","776d88af":"stalk(features,\"Exterior1st\")","176e2452":"stalk(features,\"MasVnrType\")","30826009":"features.loc[features[\"MasVnrType\"] == \"BrkCmn\" , \"MasVnrType\"] = \"None\" ","dd5a3a92":"stalk(features,\"Foundation\")","b7c40e71":"features.loc[features[\"Foundation\"] == \"Stone\", \"Foundation\"] = \"BrkTil\"\nfeatures.loc[features[\"Foundation\"] == \"Wood\", \"Foundation\"] = \"CBlock\"","0537fe67":"stalk(features,\"Fence\")","76ba98ab":"features.loc[features[\"Fence\"] == \"MnWw\", \"Fence\"] = \"MnPrv\"\nfeatures.loc[features[\"Fence\"] == \"GdWo\", \"Fence\"] = \"MnPrv\"","c6c016ee":"# RARE ANALYZER\ndef rare_analyser(dataframe, target, rare_perc):\n    rare_columns = [col for col in dataframe.columns if len(dataframe[col].value_counts()) <= 20\n                    and (dataframe[col].value_counts() \/ len(dataframe) < rare_perc).any(axis=None)]\n    for var in rare_columns:\n        print(var, \":\", len(dataframe[var].value_counts()))\n        print(pd.DataFrame({\"COUNT\": dataframe[var].value_counts(),\n                            \"RATIO\": dataframe[var].value_counts() \/ len(dataframe),\n                            \"TARGET_MEDIAN\": dataframe.groupby(var)[target].median()}), end=\"\\n\\n\\n\")\n\n\nrare_analyser(features, \"SalePrice\", 0.01)","298cb71d":"def rare_encoder(dataframe, rare_perc):\n    temp_df = dataframe.copy()\n    rare_columns = [col for col in temp_df.columns if temp_df[col].dtypes == 'O'\n                    and (temp_df[col].value_counts() \/ len(temp_df) < rare_perc).any(axis=None)]\n    for var in rare_columns:\n        tmp = temp_df[var].value_counts() \/ len(temp_df)\n        rare_labels = tmp[tmp < rare_perc].index\n        temp_df[var] = np.where(temp_df[var].isin(rare_labels), 'Rare', temp_df[var])\n    return temp_df\n\nfeatures = rare_encoder(features, 0.01)\n","cb253a2d":"# Plotting numerical features with polynomial order to detect outliers by eye.\n\ndef srt_reg(y, df):\n    fig, axes = plt.subplots(12, 3, figsize=(25, 80))\n    axes = axes.flatten()\n\n    for i, j in zip(df.select_dtypes(include=['number']).columns, axes):\n\n        sns.regplot(x=i,\n                    y=y,\n                    data=df,\n                    ax=j,\n                    order=3,\n                    ci=None,\n                    color='#25B89B',\n                    line_kws={'color': 'grey'},\n                    scatter_kws={'alpha':0.4})\n        j.tick_params(labelrotation=45)\n        j.yaxis.set_major_locator(MaxNLocator(nbins=8))\n\n        plt.tight_layout()","c05eb165":"srt_reg('SalePrice', train)","6b01dea0":"features.shape","7c33b58b":"# Dropping outliers after detecting them by eye.\nfeatures.loc[2590, 'GarageYrBlt'] = 2007 # missing value it was 2207\n\nfeatures = features.drop(features[(features['OverallQual'] < 5)\n                                  & (features['SalePrice'] > 200000)].index)\nfeatures = features.drop(features[(features['GrLivArea'] > 4000)\n                                  & (features['SalePrice'] < 200000)].index)\nfeatures = features.drop(features[(features['GarageArea'] > 1200)\n                                  & (features['SalePrice'] < 200000)].index)\nfeatures = features.drop(features[(features['TotalBsmtSF'] > 3000)\n                                  & (features['SalePrice'] > 320000)].index)\nfeatures = features.drop(features[(features['1stFlrSF'] < 3000)\n                                  & (features['SalePrice'] > 600000)].index)\nfeatures = features.drop(features[(features['1stFlrSF'] > 3000)\n                                  & (features['SalePrice'] < 200000)].index)\n","27b3ab23":"# Dropping target value\ny = features['SalePrice']\ny.dropna(inplace=True)\nfeatures.drop(columns='SalePrice', inplace=True)","9323c521":"features.shape","aae2aab2":"# Creating new features  based on previous observations. There might be some highly correlated features now. You cab drop them if you want to...\n\nfeatures['TotalSF'] = (features['BsmtFinSF1'] + features['BsmtFinSF2'] +\n                       features['1stFlrSF'] + features['2ndFlrSF'])\nfeatures['TotalBathrooms'] = (features['FullBath'] +\n                              (0.5 * features['HalfBath']) +\n                              features['BsmtFullBath'] +\n                              (0.5 * features['BsmtHalfBath']))\n\nfeatures['TotalPorchSF'] = (features['OpenPorchSF'] + features['3SsnPorch'] +\n                            features['EnclosedPorch'] +\n                            features['ScreenPorch'] + features['WoodDeckSF'])\n\nfeatures['YearBlRm'] = (features['YearBuilt'] + features['YearRemodAdd'])\n\n# Merging quality and conditions.\n\nfeatures['TotalExtQual'] = (features['ExterQual'] + features['ExterCond'])\nfeatures['TotalBsmQual'] = (features['BsmtQual'] + features['BsmtCond'] +\n                            features['BsmtFinType1'] +\n                            features['BsmtFinType2'])\nfeatures['TotalGrgQual'] = (features['GarageQual'] + features['GarageCond'])\nfeatures['TotalQual'] = features['OverallQual'] + features[\n    'TotalExtQual'] + features['TotalBsmQual'] + features[\n        'TotalGrgQual'] + features['KitchenQual'] + features['HeatingQC']\n\n# Creating new features by using new quality indicators.\n\nfeatures['QualGr'] = features['TotalQual'] * features['GrLivArea']\nfeatures['QualBsm'] = features['TotalBsmQual'] * (features['BsmtFinSF1'] +\n                                                  features['BsmtFinSF2'])\nfeatures['QualPorch'] = features['TotalExtQual'] * features['TotalPorchSF']\nfeatures['QualExt'] = features['TotalExtQual'] * features['MasVnrArea']\nfeatures['QualGrg'] = features['TotalGrgQual'] * features['GarageArea']\nfeatures['QlLivArea'] = (features['GrLivArea'] -\n                         features['LowQualFinSF']) * (features['TotalQual'])\nfeatures['QualSFNg'] = features['QualGr'] * features['Neighborhood']\n\nfeatures[\"new_home\"] = features[\"YearBuilt\"]\nfeatures.loc[features[\"new_home\"] == features[\"YearRemodAdd\"], \"new_home\"] = 0\nfeatures.loc[features[\"new_home\"] != features[\"YearRemodAdd\"], \"new_home\"] = 1\n\n# Creating some simple features.\n\nfeatures['HasPool'] = features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['Has2ndFloor'] = features['2ndFlrSF'].apply(lambda x: 1\n                                                     if x > 0 else 0)\nfeatures['HasGarage'] = features['QualGrg'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['HasBsmt'] = features['QualBsm'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['HasFireplace'] = features['Fireplaces'].apply(lambda x: 1\n                                                        if x > 0 else 0)\nfeatures['HasPorch'] = features['QualPorch'].apply(lambda x: 1 if x > 0 else 0)","e1ada1cf":"# Observing the effects of newly created features on sale price.\n\ndef srt_reg(feature):\n    merged = features.join(y)\n    fig, axes = plt.subplots(5, 3, figsize=(25, 40))\n    axes = axes.flatten()\n\n    new_features = [\n        'TotalSF', 'TotalBathrooms', 'TotalPorchSF', 'YearBlRm',\n        'TotalExtQual', 'TotalBsmQual', 'TotalGrgQual', 'TotalQual', 'QualGr',\n        'QualBsm', 'QualPorch', 'QualExt', 'QualGrg', 'QlLivArea', 'QualSFNg'\n    ]\n\n    for i, j in zip(new_features, axes):\n\n        sns.regplot(x=i,\n                    y=feature,\n                    data=merged,\n                    ax=j,\n                    order=3,\n                    ci=None,\n                    color='#25B89B',\n                    line_kws={'color': 'grey'},\n                    scatter_kws={'alpha':0.4})\n        j.tick_params(labelrotation=45)\n        j.yaxis.set_major_locator(MaxNLocator(nbins=10))\n\n        plt.tight_layout()\n\n\n","6faeb7d9":"srt_reg('SalePrice')","a467fed8":"\nskewed = [\n    'LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2',\n    'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea',\n    'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch',\n    'ScreenPorch', 'PoolArea', 'LowQualFinSF', 'MiscVal'\n]","d00a07b0":"# Skewnessl\u0131k derecesini bulma\n\nskew_features = np.abs(features[skewed].apply(lambda x: skew(x)).sort_values(\n    ascending=False))\n\n# Skewnessl\u0131k derecesine g\u00f6re filtreleme\n\nhigh_skew = skew_features[skew_features > 0.3]\n\n# Y\u00fcksek skewnessl\u0131\u011fa sahip olanlar\u0131n indexini alma\n\nskew_index = high_skew.index\n\n# Y\u00fcksek skewnessa sahip de\u011fi\u015fkenlere Box-Cox Transformation uygulanmas\u0131\n\nfor i in skew_index:\n    features[i] = boxcox1p(features[i], boxcox_normmax(features[i] + 1))","406fec3e":"# At\u0131lacaklar listesi\n\nto_drop = ['Utilities','PoolQC','YrSold','MoSold','ExterQual','BsmtQual','GarageQual','KitchenQual','HeatingQC',]\n\nfeatures.drop(columns=to_drop, inplace=True)","527d287a":"# Kategorik de\u011fi\u015fkenlere O.H.E uyguluyorum.\n# Normalde regresyon modellerinde First-Drop = True yap\u0131lmas\u0131 gerekiyor fakat, baz\u0131 a\u011fa\u00e7 modelleri de kullanaca\u011f\u0131m i\u00e7in ilk dummy'leri atm\u0131yorum. \n# Hatta bunun biraz puan\u0131m\u0131 artt\u0131rd\u0131\u011f\u0131m\u0131 da s\u00f6yleyebilirim.\n\nfeatures = pd.get_dummies(data=features)","508d15b7":"print(f'Toplam eksik g\u00f6zlem say\u0131s\u0131: {features.isna().sum().sum()}')","c35f4cab":"features.shape","cd3e4ca1":"# train ve test ay\u0131rma\nsubmission_model = features.copy()\ntrain = features.iloc[:len(y), :]\ntest = features.iloc[len(train):, :]","9f9202cf":"correlations = train.join(y).corrwith(train.join(y)['SalePrice']).iloc[:-1].to_frame()\ncorrelations['Abs Corr'] = correlations[0].abs()\nsorted_correlations = correlations.sort_values('Abs Corr', ascending=False)['Abs Corr']\nfig, ax = plt.subplots(figsize=(12,12))\nsns.heatmap(sorted_correlations.to_frame()[sorted_correlations>=.5], cmap='coolwarm', annot=True, vmin=-1, vmax=1, ax=ax);","ea9b83ce":"def plot_dist3(df, feature, title):\n    \n    # Creating a customized chart. and giving in figsize and everything.\n    \n    fig = plt.figure(constrained_layout=True, figsize=(12, 8))\n    \n    # creating a grid of 3 cols and 3 rows.\n    \n    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n\n    # Customizing the histogram grid.\n    \n    ax1 = fig.add_subplot(grid[0, :2])\n    \n    # Set the title.\n    \n    ax1.set_title('Histogram')\n    \n    # plot the histogram.\n    \n    sns.distplot(df.loc[:, feature],\n                 hist=True,\n                 kde=True,\n                 fit=norm,\n                 ax=ax1,\n                 color='#e74c3c')\n    ax1.legend(labels=['Normal', 'Actual'])\n\n    # customizing the QQ_plot.\n    \n    ax2 = fig.add_subplot(grid[1, :2])\n    \n    # Set the title.\n    \n    ax2.set_title('Probability Plot')\n    \n    # Plotting the QQ_Plot.\n    stats.probplot(df.loc[:, feature].fillna(np.mean(df.loc[:, feature])),\n                   plot=ax2)\n    ax2.get_lines()[0].set_markerfacecolor('#e74c3c')\n    ax2.get_lines()[0].set_markersize(12.0)\n\n    # Customizing the Box Plot:\n    \n    ax3 = fig.add_subplot(grid[:, 2])\n    # Set title.\n    \n    ax3.set_title('Box Plot')\n    \n    # Plotting the box plot.\n    \n    sns.boxplot(df.loc[:, feature], orient='v', ax=ax3, color='#e74c3c')\n    ax3.yaxis.set_major_locator(MaxNLocator(nbins=24))\n\n    plt.suptitle(f'{title}', fontsize=24)","5f21ee7e":"# Checking target variable.\n\nplot_dist3(train.join(y), 'SalePrice', 'Log D\u00f6n\u00fc\u015f\u00fcm \u00d6ncesi Sale Price')","24496639":"# Setting model data.\n\nX_my = train\nX_test_my = test\ny_ = y\ny_log =y\ny_log = np.log1p(y_log)","0d1323a7":"plot_dist3(train.join(y_log), 'SalePrice', 'Log D\u00f6n\u00fc\u015f\u00fcm Sonras\u0131 Sale Price')","a42632fe":"X_my = RobustScaler().fit_transform(X_my)\nX_test_my = RobustScaler().fit_transform(X_test_my)","68642b7a":"X_train, X_test, y_train, y_test = train_test_split(X_my, y_log, test_size=0.20, random_state=46)","7f0dbc41":"### NON LINEAR MODELS\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nimport pickle\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\n\nimport warnings\nfrom sklearn.exceptions import ConvergenceWarning\n\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n\nmodels = [('LR', LinearRegression()),\n          (\"Ridge\", Ridge()),\n          (\"Lasso\", Lasso()),\n          (\"ElasticNet\", ElasticNet()),\n          ('RF', RandomForestRegressor()),\n          ('GBM', GradientBoostingRegressor()),\n          (\"XGBoost\", XGBRegressor()),\n          (\"LightGBM\", LGBMRegressor()),\n          (\"CatBoost\", CatBoostRegressor(verbose=False))]\n\n# Base modellerin test hatalar\u0131\nprint(\"Base Test RMSE\")\nfor name, model in models:\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    rmsLe = np.sqrt(mean_squared_error(y_test, y_pred))\n    msg = \"%s: %f\" % (name, rmsLe)\n    print(msg)","28b6fbf5":"# Base modellerin test hatalar\u0131\nprint(\"Base Train RMSE\")\nfor name, model in models:\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_train)\n    rmsLe = np.sqrt(mean_squared_error(y_train, y_pred))\n    msg = \"%s: %f\" % (name, rmsLe)\n    print(msg)","7971c1dd":"X_train, X_test, y_train, y_test = train_test_split(X_my, y_, test_size=0.20, random_state=46)","e5859508":"models = [('LR', LinearRegression()),\n          (\"Ridge\", Ridge()),\n          (\"Lasso\", Lasso()),\n          (\"ElasticNet\", ElasticNet()),\n          ('RF', RandomForestRegressor()),\n          ('GBM', GradientBoostingRegressor()),\n          (\"XGBoost\", XGBRegressor()),\n          (\"LightGBM\", LGBMRegressor()),\n          (\"CatBoost\", CatBoostRegressor(verbose=False))]\n\n# Base modellerin test hatalar\u0131\nprint(\"Test RMSE\")\nfor name, model in models:\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    rmsLe = np.sqrt(mean_squared_error(y_test, y_pred))\n    msg = \"%s: %f\" % (name, rmsLe)\n    print(msg)","455b73e4":"# Base modellerin train hatalar\u0131\nprint(\"Train RMSE\")\nfor name, model in models:\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_train)\n    rmsLe = np.sqrt(mean_squared_error(y_train, y_pred))\n    msg = \"%s: %f\" % (name, rmsLe)\n    print(msg)","2b219da5":"X = train\nX_test = test\ny = np.log1p(y)","ab23b74f":"# Loading neccesary packages for modelling.\n\nfrom sklearn.model_selection import cross_val_score, KFold, cross_validate\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV, TweedieRegressor\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom mlxtend.regressor import StackingCVRegressor # This is for stacking part, works well with sklearn and others...","dfd3c90c":"# Setting kfold for future use.\n\nkf = KFold(10, random_state=42)","4421686a":"# Some parameters for ridge, lasso and elasticnet.\n\nalphas_alt = [15.5, 15.6, 15.7, 15.8, 15.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [\n    5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008\n]\ne_alphas = [\n    0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007\n]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n\n# ridge_cv\n\nridge = make_pipeline(RobustScaler(), RidgeCV(\n    alphas=alphas_alt,\n    cv=kf,\n))\n\n# lasso_cv:\n\nlasso = make_pipeline(\n    RobustScaler(),\n    LassoCV(max_iter=1e7, alphas=alphas2, random_state=42, cv=kf))\n\n# elasticnet_cv:\n\nelasticnet = make_pipeline(\n    RobustScaler(),\n    ElasticNetCV(max_iter=1e7,\n                 alphas=e_alphas,\n                 cv=kf,\n                 random_state=42,\n                 l1_ratio=e_l1ratio))\n\n# svr:\n\nsvr = make_pipeline(RobustScaler(),\n                    SVR(C=21, epsilon=0.0099, gamma=0.00017, tol=0.000121))\n\n# gradientboosting:\n\ngbr = GradientBoostingRegressor(n_estimators=2900,\n                                learning_rate=0.0161,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=17,\n                                loss='huber',\n                                random_state=42)\n\n# lightgbm:\n\nlightgbm = LGBMRegressor(objective='regression',\n                         n_estimators=3500,\n                         num_leaves=5,\n                         learning_rate=0.00721,\n                         max_bin=163,\n                         bagging_fraction=0.35711,\n                         n_jobs=-1,\n                         bagging_seed=42,\n                         feature_fraction_seed=42,\n                         bagging_freq=7,\n                         feature_fraction=0.1294,\n                         min_data_in_leaf=8)\n\n# xgboost:\n\nxgboost = XGBRegressor(\n    learning_rate=0.0139,\n    n_estimators=4500,\n    max_depth=4,\n    min_child_weight=0,\n    subsample=0.7968,\n    colsample_bytree=0.4064,\n    nthread=-1,\n    scale_pos_weight=2,\n    seed=42,\n)\n\n\n# hist gradient boosting regressor:\n\nhgrd= HistGradientBoostingRegressor(    loss= 'least_squares',\n    max_depth= 2,\n    min_samples_leaf= 40,\n    max_leaf_nodes= 29,\n    learning_rate= 0.15,\n    max_iter= 225,\n                                    random_state=42)\n\n# tweedie regressor:\n \ntweed = make_pipeline(RobustScaler(),TweedieRegressor(alpha=0.005))\n\n\n# stacking regressor:\n\nstack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, gbr,\n                                            xgboost, lightgbm,hgrd, tweed),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)","30240b73":"def model_check(X, y, estimators, cv):\n    \n    ''' A function for testing multiple estimators.'''\n    \n    model_table = pd.DataFrame()\n\n    row_index = 0\n    for est, label in zip(estimators, labels):\n\n        MLA_name = label\n        model_table.loc[row_index, 'Model Name'] = MLA_name\n\n        cv_results = cross_validate(est,\n                                    X,\n                                    y,\n                                    cv=cv,\n                                    scoring='neg_root_mean_squared_error',\n                                    return_train_score=True,\n                                    n_jobs=-1)\n\n        model_table.loc[row_index, 'Train RMSE'] = -cv_results[\n            'train_score'].mean()\n        model_table.loc[row_index, 'Test RMSE'] = -cv_results[\n            'test_score'].mean()\n        model_table.loc[row_index, 'Test Std'] = cv_results['test_score'].std()\n        model_table.loc[row_index, 'Time'] = cv_results['fit_time'].mean()\n\n        row_index += 1\n\n    model_table.sort_values(by=['Test RMSE'],\n                            ascending=True,\n                            inplace=True)\n\n    return model_table","4f09d132":"# Setting list of estimators and labels for them:\n\nestimators = [ridge, lasso, elasticnet, gbr, xgboost, lightgbm, svr, hgrd, tweed]\nlabels = [\n    'Ridge', 'Lasso', 'Elasticnet', 'GradientBoostingRegressor',\n    'XGBRegressor', 'LGBMRegressor', 'SVR', 'HistGradientBoostingRegressor','TweedieRegressor'\n]","cb3dcaaf":"# Executing cross validation.\n\nraw_models = model_check(X, y, estimators, kf)\ndisplay(raw_models.style.background_gradient(cmap='summer'))","4c95ab78":"# Fitting the models on train data.\n\nprint('=' * 20, 'START Fitting', '=' * 20)\nprint('=' * 55)\n\nprint(datetime.now(), 'StackingCVRegressor')\nstack_gen_model = stack_gen.fit(X.values, y.values)\nprint(datetime.now(), 'Elasticnet')\nelastic_model_full_data = elasticnet.fit(X, y)\nprint(datetime.now(), 'Lasso')\nlasso_model_full_data = lasso.fit(X, y)\nprint(datetime.now(), 'Ridge')\nridge_model_full_data = ridge.fit(X, y)\nprint(datetime.now(), 'SVR')\nsvr_model_full_data = svr.fit(X, y)\nprint(datetime.now(), 'GradientBoosting')\ngbr_model_full_data = gbr.fit(X, y)\nprint(datetime.now(), 'XGboost')\nxgb_model_full_data = xgboost.fit(X, y)\nprint(datetime.now(), 'Lightgbm')\nlgb_model_full_data = lightgbm.fit(X, y)\nprint(datetime.now(), 'Hist')\nhist_full_data = hgrd.fit(X, y)\nprint(datetime.now(), 'Tweed')\ntweed_full_data = tweed.fit(X, y)\nprint('=' * 20, 'FINISHED Fitting', '=' * 20)\nprint('=' * 58)","d00d0c38":"# Blending models by assigning weights:\n\ndef blend_models_predict(X):\n    return ((0.1 * elastic_model_full_data.predict(X)) +\n            (0.1 * lasso_model_full_data.predict(X)) +\n            (0.1 * ridge_model_full_data.predict(X)) +\n            (0.1 * svr_model_full_data.predict(X)) +\n            (0.05 * gbr_model_full_data.predict(X)) +\n            (0.1 * xgb_model_full_data.predict(X)) +\n            (0.05 * lgb_model_full_data.predict(X)) +\n            (0.05 * hist_full_data.predict(X)) +\n            (0.1 * tweed_full_data.predict(X)) +\n            (0.25 * stack_gen_model.predict(X.values)))","4863c829":"submission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\n# Inversing and flooring log scaled sale price predictions\nsubmission['SalePrice'] = np.floor(np.expm1(blend_models_predict(X_test)))\n# Defining outlier quartile ranges\nq1 = submission['SalePrice'].quantile(0.0050)\nq2 = submission['SalePrice'].quantile(0.99)\n\n# Applying weights to outlier ranges to smooth them\nsubmission['SalePrice'] = submission['SalePrice'].apply(\n    lambda x: x if x > q1 else x * 0.77)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x\n                                                        if x < q2 else x * 1.1)\nsubmission = submission[['Id', 'SalePrice']]","b668520e":"# Saving submission file\n\nsubmission.to_csv('mysubmission.csv', index=False)\nprint(\n    'Save submission',\n    datetime.now(),\n)\nsubmission.head()","8b65cf2e":"### BsmtQual\n\n```\nEx\tExcellent (100+ inches)\t\nGd\tGood (90-99 inches)\nTA\tTypical (80-89 inches)\nFa\tFair (70-79 inches)\nPo\tPoor (<70 inches)\n```\n","1946f5ed":"# Outliers\n\nIn this part special thanks to Dear Ertu\u011frul. He and his notebook helped me a lot. https:\/\/www.kaggle.com\/datafan07\/top-1-approach-eda-new-models-and-stacking\n","9cc349e3":"Trend g\u00fczel g\u00f6r\u00fcn\u00fcyor fakat 0 ve 3 teki de\u011ferler biraz garip.\n### 1stFlrSF\nBu s\u00fctun bize fit kare cinsinden birinci kat\u0131n boyutunu anlat\u0131yor ve a\u011fa\u00e7 modelimiz i\u00e7in \u00f6nemli olmas\u0131 tamamen mant\u0131kl\u0131.  \n`SalePrice` ile 0,61'lik bir korelasyonu var, hi\u00e7 fena de\u011fil.  \nAsl\u0131nda `GrLivArea` alan bak\u0131m\u0131ndan zaten ayn\u0131 g\u00f6revi g\u00f6rmekte yine de modelde tutaca\u011f\u0131m.   \n\u0130ki ayk\u0131r\u0131 de\u011ferin burada da mevcut oldu\u011funu ve \u00f6ncekiyle ayn\u0131 oldu\u011funu not ediyoruz.","1b1a9b04":"<a href=\"https:\/\/www.oguzerdogan.com\/\">\n    <img src=\"https:\/\/www.oguzerdogan.com\/wp-content\/uploads\/2020\/10\/logo_oz.png\" width=\"200\" align=\"right\">\n<\/a>","17454d4f":"`GarageArea` - `GarageCars` ve `1stFlSF` - `TotalBsmtSF` aras\u0131nda y\u00fcksek korelasyon g\u00f6ze \u00e7arp\u0131yor.\n###  OverallQual","1c3fdaee":"### GarageArea\n","5adcff22":"## Stacking & Blending\n","4abd2de8":"### TotalBsmtSF\nBu s\u00fctun, bodrum alan\u0131n\u0131n toplam fit karesini i\u00e7eriyor.   \nBa\u011f\u0131ml\u0131 de\u011fi\u015fken ile g\u00fczel bir korelasyonu var % 61.   \nG\u00f6rd\u00fc\u011f\u00fcm\u00fcz \u00fczere baz\u0131 0 de\u011ferleri var, her evin bodrum kat\u0131 olmad\u0131\u011f\u0131 i\u00e7in bu do\u011fal bir durum.   \nBurada da outlierlar var gibi g\u00f6r\u00fcn\u00fcyor.","e4c6af8e":"# En \u0130yi Kategorik De\u011fi\u015fkenler\n","5916fdd5":"# Model Sonu\u00e7lar\u0131\n","87a1ffc4":"# MODELLEME","1858dc09":"# Say\u0131sal De\u011fi\u015fken Analizi ","e00bd40a":" `GarageCars` 4 ara\u00e7 kapasiteli garajlar\u0131n \u00e7ok az oldu\u011funu ve 3 kapasiteli ara\u00e7lardan daha ucuz oldu\u011funu g\u00f6rebiliyoruz. Buraya ilerleyen k\u0131s\u0131mlarda el ataca\u011f\u0131m.\nGeri kalan trendler g\u00fczel  g\u00f6r\u00fcn\u00fcyor. `GarageCars` 3 ara\u00e7 kapasiteli garajlarda \u00e7ok pahal\u0131 2 tane l\u00fcks evin oldu\u011fu g\u00f6zlemleniyor.\n### GrLivArea\nBu de\u011fer, evin fit kare cinsinden (zemin) ya\u015fam alan\u0131n\u0131 temsil eder ve insanlar\u0131n sat\u0131n al\u0131rken dikkate ald\u0131klar\u0131 en yayg\u0131n \u00f6zelliklerden biridir.  \n\u0130nsanlar\u0131n \u00f6nem verdi\u011fi bir \u00f6zelli\u011fin sat\u0131\u015f fiyat\u0131yla bu kadar korelasyonunun olmas\u0131 \u015fa\u015f\u0131rt\u0131c\u0131 de\u011fil. Ba\u011f\u0131ml\u0131 de\u011fi\u015fken ile aras\u0131nda % 71 korelasyon var.  \nSa\u011f alt tarafta iki tane outlier g\u00f6zlemleniyor.","e24b5434":"### KitchenQual\n\n\n```\nEx\tExcellent\nGd\tGood\nTA\tTypical\/Average\nFa\tFair\nPo\tPoor\n```  ","c0f94731":"# Data Preprocessing","71125cbb":"# Feature Engineering","4b0eea1a":"# Son \u0130\u015flemler","384c0ce8":"Derecelendirmeye uygun g\u00fczel bir trend g\u00f6r\u00fcn\u00fcyor. \nFakat `OverallQual` 10 derecesinde 2 tane \u00e7ok ucuza ev bulunuyor, bunlar b\u00fcy\u00fck ihtimalle outlier.\n### GarageCars\nGaraj say\u0131s\u0131 ile beraber garaj kapasitesini \u00f6l\u00e7en bir de\u011fi\u015fken.","964a45ea":"<a href=\"\">\n    <img src=\"https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/5407\/media\/housesbanner.png\" align=\"center\">\n<\/a>","4d2960e7":"**Buraya kadar olan modelleme projem i\u00e7indi, submisson i\u00e7in blending models uygulayaca\u011f\u0131m.**","c5c501c1":"Daha yeni evlerin genellikle daha y\u00fcksek bir fiyata sahip oldu\u011funu g\u00f6rebiliyoruz.  \nOrta-y\u00fcksek fiyatl\u0131 baz\u0131 tarihi evler olsa da, pahal\u0131 evlerden (400k) bahsetti\u011fimizde, biri hari\u00e7 hepsi son 20 y\u0131lda in\u015fa edildi\u011fini g\u00f6r\u00fcyoruz.  \nAyr\u0131ca bir evin asgari fiyat\u0131n\u0131n, sava\u015f sonras\u0131 ikinci d\u00f6nemden sonra olduk\u00e7a g\u00fc\u00e7l\u00fc bir e\u011filimle artt\u0131\u011f\u0131n\u0131 g\u00f6r\u00fcyoruz.\n### FullBath\nTam banyolar\u0131n toplam say\u0131s\u0131n\u0131 g\u00f6sterir. \u00d6nem bak\u0131m\u0131ndan 5. s\u0131rada g\u00f6r\u00fcn\u00fcyor.","5fd99823":"## Hedef De\u011fi\u015fken SalePrice","45524a6c":"# Cross Validation","4fc6c052":"# En \u00d6nemli Say\u0131sal De\u011fi\u015fkenler\n\nB\u00fcy\u00fck ve \u00e7ok fazla de\u011fi\u015fkenin oldu\u011fu bir veri setinde ilk olarak \u00f6nemli de\u011fi\u015fkenleri g\u00f6zlemlemek bize zaman ve \u00f6nfikir sa\u011flayacakt\u0131r.  \nBa\u011f\u0131ml\u0131 De\u011fi\u015fkeni en iyi a\u00e7\u0131klayan say\u0131sal de\u011fi\u015fkenlere bakmak i\u00e7in ExtraTreesRegressor kullanaca\u011f\u0131m.","374bf3e8":"\u0130lk 8 de\u011fi\u015fkene bakmak veriyi anlamak i\u00e7in yeterli diye d\u00fc\u015f\u00fcn\u00fcyorum.  \nBu 8 de\u011fi\u015fkenin ba\u011f\u0131ml\u0131 de\u011fi\u015fken ve kendi aralar\u0131ndaki korelasyonuna da bakmak iyi olacakt\u0131r.","566d9f51":"\n## Eksik G\u00f6zlemler\n","a32a5dc2":"### ExterQual\n\n```\nEx\tExcellent\nGd\tGood\nTA\tAverage\/Typical\nFa\tFair\nPo\tPoor\n```\n","87b25b64":"Baz\u0131 eksik de\u011ferler kas\u0131tl\u0131 olarak bo\u015f b\u0131rak\u0131lm\u0131\u015ft\u0131r.Bu o \u00f6zelli\u011fin olmad\u0131\u011f\u0131 anlam\u0131na geliyor. Yani bodrumkat\u0131 olmayan bir evin bodrum kat\u0131 ile ilgili k\u0131s\u0131mlar\u0131 NA","8b0ed53a":"## Submission\n","85cd82b8":"### Neighborhood\nBu de\u011fi\u015fken Ames \u015fehir s\u0131n\u0131rlar\u0131 i\u00e7indeki fiziksel konumlar\u0131 temsil eder ve modelimize g\u00f6re en kullan\u0131\u015fl\u0131 kategorik de\u011fi\u015fken g\u00f6r\u00fcn\u00fcyor.  \nSan\u0131r\u0131m bunun nedeni b\u00f6lgelerin yoksullu\u011fu ve zenginli\u011fi ile alakal\u0131 olabilir.","36fd08f1":"### YearBuilt\nEvin orjinal in\u015fa tarihi.","4c0d2b00":"## Rare Analyze","d0da6074":"## Yeni De\u011fi\u015fkenler\n\nBu k\u0131s\u0131mda olan de\u011fi\u015fkenlerden yeni de\u011fi\u015fkenler t\u00fcretiliyor. \u0130lk \u00e7al\u0131\u015fmam oldu\u011fu i\u00e7in kendi d\u00fc\u015f\u00fcnd\u00fcklerimi ve \u00e7o\u011funlukla toplulukta kullan\u0131lan de\u011fi\u015fkenleri derledim.  ","2640d9e2":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Exploratory-Data-Analysis\" data-toc-modified-id=\"Exploratory-Data-Analysis-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Exploratory Data Analysis<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Hedef-De\u011fi\u015fken-SalePrice\" data-toc-modified-id=\"Hedef-De\u011fi\u015fken-SalePrice-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;<\/span>Hedef De\u011fi\u015fken SalePrice<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#En-\u00d6nemli-Say\u0131sal-De\u011fi\u015fkenler\" data-toc-modified-id=\"En-\u00d6nemli-Say\u0131sal-De\u011fi\u015fkenler-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>En \u00d6nemli Say\u0131sal De\u011fi\u015fkenler<\/a><\/span><\/li><li><span><a href=\"#Say\u0131sal-De\u011fi\u015fken-Analizi\" data-toc-modified-id=\"Say\u0131sal-De\u011fi\u015fken-Analizi-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>Say\u0131sal De\u011fi\u015fken Analizi<\/a><\/span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#OverallQual\" data-toc-modified-id=\"OverallQual-3.0.1\"><span class=\"toc-item-num\">3.0.1&nbsp;&nbsp;<\/span>OverallQual<\/a><\/span><\/li><li><span><a href=\"#GarageCars\" data-toc-modified-id=\"GarageCars-3.0.2\"><span class=\"toc-item-num\">3.0.2&nbsp;&nbsp;<\/span>GarageCars<\/a><\/span><\/li><li><span><a href=\"#GrLivArea\" data-toc-modified-id=\"GrLivArea-3.0.3\"><span class=\"toc-item-num\">3.0.3&nbsp;&nbsp;<\/span>GrLivArea<\/a><\/span><\/li><li><span><a href=\"#YearBuilt\" data-toc-modified-id=\"YearBuilt-3.0.4\"><span class=\"toc-item-num\">3.0.4&nbsp;&nbsp;<\/span>YearBuilt<\/a><\/span><\/li><li><span><a href=\"#FullBath\" data-toc-modified-id=\"FullBath-3.0.5\"><span class=\"toc-item-num\">3.0.5&nbsp;&nbsp;<\/span>FullBath<\/a><\/span><\/li><li><span><a href=\"#1stFlrSF\" data-toc-modified-id=\"1stFlrSF-3.0.6\"><span class=\"toc-item-num\">3.0.6&nbsp;&nbsp;<\/span>1stFlrSF<\/a><\/span><\/li><li><span><a href=\"#TotalBsmtSF\" data-toc-modified-id=\"TotalBsmtSF-3.0.7\"><span class=\"toc-item-num\">3.0.7&nbsp;&nbsp;<\/span>TotalBsmtSF<\/a><\/span><\/li><li><span><a href=\"#GarageArea\" data-toc-modified-id=\"GarageArea-3.0.8\"><span class=\"toc-item-num\">3.0.8&nbsp;&nbsp;<\/span>GarageArea<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#En-\u0130yi-Kategorik-De\u011fi\u015fkenler\" data-toc-modified-id=\"En-\u0130yi-Kategorik-De\u011fi\u015fkenler-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>En \u0130yi Kategorik De\u011fi\u015fkenler<\/a><\/span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Neighborhood\" data-toc-modified-id=\"Neighborhood-4.0.1\"><span class=\"toc-item-num\">4.0.1&nbsp;&nbsp;<\/span>Neighborhood<\/a><\/span><\/li><li><span><a href=\"#ExterQual\" data-toc-modified-id=\"ExterQual-4.0.2\"><span class=\"toc-item-num\">4.0.2&nbsp;&nbsp;<\/span>ExterQual<\/a><\/span><\/li><li><span><a href=\"#BsmtQual\" data-toc-modified-id=\"BsmtQual-4.0.3\"><span class=\"toc-item-num\">4.0.3&nbsp;&nbsp;<\/span>BsmtQual<\/a><\/span><\/li><li><span><a href=\"#KitchenQual\" data-toc-modified-id=\"KitchenQual-4.0.4\"><span class=\"toc-item-num\">4.0.4&nbsp;&nbsp;<\/span>KitchenQual<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#Data-Preprocessing\" data-toc-modified-id=\"Data-Preprocessing-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>Data Preprocessing<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Eksik-G\u00f6zlemler\" data-toc-modified-id=\"Eksik-G\u00f6zlemler-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;<\/span>Eksik G\u00f6zlemler<\/a><\/span><\/li><li><span><a href=\"#Eksik-G\u00f6zlemleri-Doldurma\" data-toc-modified-id=\"Eksik-G\u00f6zlemleri-Doldurma-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;<\/span>Eksik G\u00f6zlemleri Doldurma<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Feature-Engineering\" data-toc-modified-id=\"Feature-Engineering-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;<\/span>Feature Engineering<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Rare-Analyze\" data-toc-modified-id=\"Rare-Analyze-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;<\/span>Rare Analyze<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Outliers\" data-toc-modified-id=\"Outliers-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;<\/span>Outliers<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Yeni-De\u011fi\u015fkenler\" data-toc-modified-id=\"Yeni-De\u011fi\u015fkenler-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;<\/span>Yeni De\u011fi\u015fkenler<\/a><\/span><\/li><li><span><a href=\"#Box-Cox-d\u00f6n\u00fc\u015f\u00fcm\u00fc\" data-toc-modified-id=\"Box-Cox-d\u00f6n\u00fc\u015f\u00fcm\u00fc-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;<\/span>Box-Cox d\u00f6n\u00fc\u015f\u00fcm\u00fc<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Son-\u0130\u015flemler\" data-toc-modified-id=\"Son-\u0130\u015flemler-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;<\/span>Son \u0130\u015flemler<\/a><\/span><\/li><li><span><a href=\"#MODELLEME\" data-toc-modified-id=\"MODELLEME-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;<\/span>MODELLEME<\/a><\/span><\/li><li><span><a href=\"#Cross-Validation\" data-toc-modified-id=\"Cross-Validation-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;<\/span>Cross Validation<\/a><\/span><\/li><li><span><a href=\"#Model-Results\" data-toc-modified-id=\"Model-Results-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;<\/span>Model Results<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Stacking-&amp;-Blending\" data-toc-modified-id=\"Stacking-&amp;-Blending-11.1\"><span class=\"toc-item-num\">11.1&nbsp;&nbsp;<\/span>Stacking &amp; Blending<\/a><\/span><\/li><li><span><a href=\"#Submission\" data-toc-modified-id=\"Submission-11.2\"><span class=\"toc-item-num\">11.2&nbsp;&nbsp;<\/span>Submission<\/a><\/span><\/li><\/ul><\/li><\/ul><\/div>","0337c2b6":"Important note: I'll update this notebook for English soon!","fdd6d05c":"## Box-Cox d\u00f6n\u00fc\u015f\u00fcm\u00fc\nBaz\u0131 say\u0131sal de\u011fi\u015fkenler \u00e7ok fazla sa\u011fa \u00e7arp\u0131k, bunlar\u0131 biraz d\u00fczeltmek i\u00e7in Box-Cox Transformation uygulad\u0131m","8029e218":"**Rare Analysis sonras\u0131 g\u00f6z\u00fcme \u00e7arpan baz\u0131 gereksiz de\u011fi\u015fkenleri at\u0131yorum.**","3ab02dac":"## Eksik G\u00f6zlemleri Doldurma","8d16417f":"# Exploratory Data Analysis"}}