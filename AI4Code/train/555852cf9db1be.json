{"cell_type":{"8083cfaa":"code","42ac7c56":"code","dc60cfbe":"code","fb48b4de":"code","d7a36d95":"code","70798366":"code","8b9f07aa":"code","fac370ff":"code","fe0ad695":"code","553b3c80":"code","616c2343":"code","2f9c636b":"code","5a323370":"code","edcd15c1":"code","f172cea0":"code","cf55f176":"code","2bd03c36":"code","fbd6f21b":"code","f6e2ca85":"code","3f55bbcd":"code","426ea9d7":"code","9ae6e12b":"code","a15a7062":"code","20dc5f66":"code","04532a22":"code","72df0932":"code","5a109f87":"code","a753c08a":"code","ee31dbbe":"code","d3ee4b23":"code","41f0c366":"code","2a8a7f8a":"code","06e407b0":"code","3e0fabac":"code","fd7ad04b":"code","d34a2695":"code","af70f779":"code","e41ccf63":"code","bc5d0ef4":"code","f6d1a84b":"code","68a681f8":"code","1465ab1f":"code","712d912d":"code","327f3445":"code","9ef9c5c4":"code","92d5be0c":"code","b86c2772":"code","3378dfe7":"code","716850f2":"code","1407704a":"code","93644ad3":"code","81a64158":"code","54b80ff4":"code","ea14929a":"code","740a3033":"code","5c181345":"code","30467a1a":"code","469c995a":"code","5888daf8":"code","bce8f99c":"code","8cc5215a":"code","59fd7c5c":"code","3467f674":"code","0c633dd3":"code","dcf2ed15":"code","adf311fa":"code","173fe96b":"code","5352c9af":"code","8f159b54":"markdown","fe04138e":"markdown","8a428cbb":"markdown","95aa8b70":"markdown","913dcbf2":"markdown","ed659caa":"markdown","5a63217f":"markdown","fb99dab1":"markdown","ba55d2c5":"markdown","22f9cf9e":"markdown","069323db":"markdown","6544ce68":"markdown","20d74f02":"markdown","96d604b9":"markdown","3e99727d":"markdown","677cab16":"markdown","60e6737b":"markdown","2a11e935":"markdown","01a72ffa":"markdown","9e3646ca":"markdown","e3fe0b31":"markdown","7816f00e":"markdown","773f1767":"markdown","26df8718":"markdown","b6143e43":"markdown","51e6a7a3":"markdown","9c20f46e":"markdown","84ef628a":"markdown","255eba1c":"markdown","5e6d3b2f":"markdown","69a7843a":"markdown","87c8d395":"markdown","9aac5788":"markdown","0767f8ce":"markdown","d24d8be6":"markdown","e07c677c":"markdown"},"source":{"8083cfaa":"#importing libraries\nimport warnings\nwarnings.filterwarning='ignore'\n\nimport pandas as pd,numpy as np,matplotlib.pyplot as plt,seaborn as sns","42ac7c56":"#Read data\ndata=pd.read_csv('..\/input\/globalsuperstoredata\/GlobalSuperstoreData.csv')\ndata['Order Date']=pd.to_datetime(data['Order Date'],format='%d-%m-%Y')\ndata.head()","dc60cfbe":"#checking  columns details and data types\ndata.info()\n#no null values present. order date data type needs to be changed.","fb48b4de":"#checking column and rows \ndata.shape","d7a36d95":"#creating a new column Market Segments by combining two columnsmarkets and segments seperated by '-'\ndata['Market Segments']=data['Segment']+'-'+data['Market']\ndata.head()","70798366":"#droping the old columns \ndata.drop(['Segment','Market'],axis=1,inplace =True)\ndata.head()","8b9f07aa":"#Convert Order date column into year month format\ndata['Order Date'] = pd.to_datetime(data['Order Date']).dt.to_period('m')\n#data= data.sort_values(by=['Order Date'])\ndata.head()","fac370ff":"#Aggregate data as per month\ndata_profit=data.pivot_table(index='Order Date',columns='Market Segments',aggfunc='sum',values='Profit')\ndata_profit.head()","fe0ad695":"#check shape rows\/columns\ndata_profit.shape\n#48 months and 21 market segments present ","553b3c80":"# Train testdata split - 42 months for train data and 6 months for test data\ntrain_len=42\ntrain=data_profit[0:train_len]\ntest=data_profit[train_len:]\ntrain.shape","616c2343":"#checking most consistent market segment by checkingthe COV on profit. \nmean=np.mean(train)\nstd= np.std(train)\n\nCoV_df= pd.DataFrame(std\/mean)\nCoV_df= CoV_df.reset_index()\nCoV_df.columns= ['Market_Segment','CoV']\nCoV_df.sort_values(by='CoV', ascending= True, inplace = True)\nCoV_df","2f9c636b":"#Change date format into timestamps\ndata['Order Date'] = data['Order Date'].astype(str)\ndata['Order Date']=pd.to_datetime(data['Order Date'])","5a323370":"#filter the main data set to Consumer-APAC which is the most profitable market segment.\ndata1=data[data['Market Segments']=='Consumer-APAC']\ndata1.head()","edcd15c1":"#group data for using it for forecasting and applying timeseries modelling\ndata1=data1[['Order Date','Sales']].groupby('Order Date').sum()\ndata1.head()","f172cea0":"#plot time series data\ndata1.plot(figsize=(12, 4))\nplt.legend(loc='best')\nplt.title('Retail Giant Sales')\nplt.show(block=False)","cf55f176":"train_len=42\ntrain=data1[0:train_len]\ntest=data1[train_len:]","2bd03c36":"#Additive method\n\nfrom pylab import rcParams\nimport statsmodels.api as sm\nrcParams['figure.figsize'] = 12, 8\ndecomposition = sm.tsa.seasonal_decompose(data1.Sales, model='additive') \nfig = decomposition.plot()\nplt.show()","fbd6f21b":"##Multiplicative method\ndecomposition = sm.tsa.seasonal_decompose(data1.Sales, model='multiplicative') \nfig = decomposition.plot()\nplt.show()","f6e2ca85":"y_hat_naive = test.copy()\ny_hat_naive['naive_forecast'] = train['Sales'][train_len-1]","3f55bbcd":"#Plot train test and forecast\n\nplt.figure(figsize=(12,4))\nplt.plot(train['Sales'], label='Train')\nplt.plot(test['Sales'], label='Test')\nplt.plot(y_hat_naive['naive_forecast'], label='Naive forecast')\nplt.legend(loc='best')\nplt.title('Naive Method')\nplt.show()","426ea9d7":"#Calculate RMSE and MAPE\nfrom sklearn.metrics import mean_squared_error\nrmse = np.sqrt(mean_squared_error(test['Sales'], y_hat_naive['naive_forecast'])).round(2)\nmape = np.round(np.mean(np.abs(test['Sales']-y_hat_naive['naive_forecast'])\/test['Sales'])*100,2)\n\nresults = pd.DataFrame({'Method':['Naive method'], 'MAPE': [mape], 'RMSE': [rmse]})\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","9ae6e12b":"y_hat_avg = test.copy()\ny_hat_avg['avg_forecast'] = train['Sales'].mean()","a15a7062":"#Plot train test and forecast\n\nplt.figure(figsize=(12,4))\nplt.plot(train['Sales'], label='Train')\nplt.plot(test['Sales'], label='Test')\nplt.plot(y_hat_avg['avg_forecast'], label='Simple average forecast')\nplt.legend(loc='best')\nplt.title('Simple Average Method')\nplt.show()","20dc5f66":"#Calculate RMSE and MAPE\n\nrmse = np.sqrt(mean_squared_error(test['Sales'], y_hat_avg['avg_forecast'])).round(2)\nmape = np.round(np.mean(np.abs(test['Sales']-y_hat_avg['avg_forecast'])\/test['Sales'])*100,2)\n\ntempResults = pd.DataFrame({'Method':['Simple average method'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","04532a22":"y_hat_sma = data1.copy()\nma_window = 3\ny_hat_sma['sma_forecast'] = data1['Sales'].rolling(ma_window).mean()\ny_hat_sma['sma_forecast'][train_len:] = y_hat_sma['sma_forecast'][train_len-1]","72df0932":"#Plot train test and forecast\n\nplt.figure(figsize=(12,4))\nplt.plot(train['Sales'], label='Train')\nplt.plot(test['Sales'], label='Test')\nplt.plot(y_hat_sma['sma_forecast'], label='Simple moving average forecast')\nplt.legend(loc='best')\nplt.title('Simple Moving Average Method')\nplt.show()","5a109f87":"#Calculate RMSE and MAPE\nrmse = np.sqrt(mean_squared_error(test['Sales'], y_hat_sma['sma_forecast'][train_len:])).round(2)\nmape = np.round(np.mean(np.abs(test['Sales']-y_hat_sma['sma_forecast'][train_len:])\/test['Sales'])*100,2)\n\ntempResults = pd.DataFrame({'Method':['Simple moving average forecast'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","a753c08a":"from statsmodels.tsa.holtwinters import SimpleExpSmoothing\nmodel = SimpleExpSmoothing(train['Sales'])\nmodel_fit = model.fit(optimized=True)\nmodel_fit.params\ny_hat_ses = test.copy()\ny_hat_ses['ses_forecast'] = model_fit.forecast(6)","ee31dbbe":"#Plot train test and forecast\nplt.figure(figsize=(12,4))\nplt.plot(train['Sales'], label='Train')\nplt.plot(test['Sales'], label='Test')\nplt.plot(y_hat_ses['ses_forecast'], label='Simple exponential smoothing forecast')\nplt.legend(loc='best')\nplt.title('Simple Exponential Smoothing Method')\nplt.show()","d3ee4b23":"#Calculate RMSE and MAPE\nrmse = np.sqrt(mean_squared_error(test['Sales'], y_hat_ses['ses_forecast'])).round(2)\nmape = np.round(np.mean(np.abs(test['Sales']-y_hat_ses['ses_forecast'])\/test['Sales'])*100,2)\n\ntempResults = pd.DataFrame({'Method':['Simple exponential smoothing forecast'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults","41f0c366":"from statsmodels.tsa.holtwinters import ExponentialSmoothing\nmodel = ExponentialSmoothing(np.asarray(train['Sales']) ,seasonal_periods=12 ,trend='additive', seasonal=None)\nmodel_fit = model.fit(smoothing_level=0.2, smoothing_slope=0.01, optimized=False)\nprint(model_fit.params)\ny_hat_holt = test.copy()\ny_hat_holt['holt_forecast'] = model_fit.forecast(len(test))","2a8a7f8a":"#Plot train test and forecast\nplt.figure(figsize=(12,4))\nplt.plot( train['Sales'], label='Train')\nplt.plot(test['Sales'], label='Test')\nplt.plot(y_hat_holt['holt_forecast'], label='Holt\\'s exponential smoothing forecast')\nplt.legend(loc='best')\nplt.title('Holt\\'s Exponential Smoothing Method')\nplt.show()","06e407b0":"#Calculate RSME and MAPE\nrmse = np.sqrt(mean_squared_error(test['Sales'], y_hat_holt['holt_forecast'])).round(2)\nmape = np.round(np.mean(np.abs(test['Sales']-y_hat_holt['holt_forecast'])\/test['Sales'])*100,2)\n\ntempResults = pd.DataFrame({'Method':['Holt\\'s exponential smoothing method'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","3e0fabac":"y_hat_hwa = test.copy()\nmodel = ExponentialSmoothing(np.asarray(train['Sales']) ,seasonal_periods=12 ,trend='add', seasonal='add')\nmodel_fit = model.fit(optimized=True)\nprint(model_fit.params)\ny_hat_hwa['hw_forecast'] = model_fit.forecast(6)","fd7ad04b":"#Plot train test and forecast\nplt.figure(figsize=(12,4))\nplt.plot( train['Sales'], label='Train')\nplt.plot(test['Sales'], label='Test')\nplt.plot(y_hat_hwa['hw_forecast'], label='Holt Winters\\'s additive forecast')\nplt.legend(loc='best')\nplt.title('Holt Winters\\' Additive Method')\nplt.show()","d34a2695":"#Calculate RMSE and MAPE\nrmse = np.sqrt(mean_squared_error(test['Sales'], y_hat_hwa['hw_forecast'])).round(2)\nmape = np.round(np.mean(np.abs(test['Sales']-y_hat_hwa['hw_forecast'])\/test['Sales'])*100,2)\n\ntempResults = pd.DataFrame({'Method':['Holt Winters\\' additive method'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","af70f779":"y_hat_hwm = test.copy()\nmodel = ExponentialSmoothing(np.asarray(train['Sales']) ,seasonal_periods=12 ,trend='add', seasonal='mul')\nmodel_fit = model.fit(optimized=True)\nprint(model_fit.params)\ny_hat_hwm['hw_forecast'] = model_fit.forecast(6)","e41ccf63":"#Plot train, test and forecast\nplt.figure(figsize=(12,4))\nplt.plot( train['Sales'], label='Train')\nplt.plot(test['Sales'], label='Test')\nplt.plot(y_hat_hwm['hw_forecast'], label='Holt Winters\\'s mulitplicative forecast')\nplt.legend(loc='best')\nplt.title('Holt Winters\\' Mulitplicative Method')\nplt.show()","bc5d0ef4":"#Calculate RMSE and MAPE\nrmse = np.sqrt(mean_squared_error(test['Sales'], y_hat_hwm['hw_forecast'])).round(2)\nmape = np.round(np.mean(np.abs(test['Sales']-y_hat_hwm['hw_forecast'])\/test['Sales'])*100,2)\n\ntempResults = pd.DataFrame({'Method':['Holt Winters\\' multiplicative method'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","f6d1a84b":"# Stationarity vs non-stationary time series\ndata1['Sales'].plot(figsize=(12, 4))\nplt.legend(loc='best')\nplt.title('Retail Giant Sales')\nplt.show(block=False)","68a681f8":"#Augmented Dickey-Fuller (ADF) test\nfrom statsmodels.tsa.stattools import adfuller\nadf_test = adfuller(data1['Sales'])\n\nprint('ADF Statistic: %f' % adf_test[0])\nprint('Critical Values @ 0.05: %.2f' % adf_test[4]['5%'])\nprint('p-value: %f' % adf_test[1])","1465ab1f":"# Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test\nfrom statsmodels.tsa.stattools import kpss\nkpss_test = kpss(data1['Sales'])\n\nprint('KPSS Statistic: %f' % kpss_test[0])\nprint('Critical Values @ 0.05: %.2f' % kpss_test[3]['5%'])\nprint('p-value: %f' % kpss_test[1])\n","712d912d":"#Box Cox transformation to make variance constant\nfrom scipy.stats import boxcox\ndata_boxcox = pd.Series(boxcox(data1['Sales'], lmbda=0), index = data1.index)\n\nplt.figure(figsize=(12,4))\nplt.plot(data_boxcox, label='After Box Cox tranformation')\nplt.legend(loc='best')\nplt.title('After Box Cox transform')\nplt.show()","327f3445":"#Differencing to remove trend\ndata_boxcox_diff = pd.Series(data_boxcox - data_boxcox.shift(), data1.index)\nplt.figure(figsize=(12,4))\nplt.plot(data_boxcox_diff, label='After Box Cox tranformation and differencing')\nplt.legend(loc='best')\nplt.title('After Box Cox transform and differencing')\nplt.show()","9ef9c5c4":"data_boxcox_diff.dropna(inplace=True)","92d5be0c":"#Augmented Dickey-Fuller (ADF) test\nfrom statsmodels.tsa.stattools import adfuller\nadf_test = adfuller(data_boxcox_diff)\n\nprint('ADF Statistic: %f' % adf_test[0])\nprint('Critical Values @ 0.05: %.2f' % adf_test[4]['5%'])\nprint('p-value: %f' % adf_test[1])","b86c2772":"# Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test\nfrom statsmodels.tsa.stattools import kpss\nkpss_test = kpss(data_boxcox_diff)\n\nprint('KPSS Statistic: %f' % kpss_test[0])\nprint('Critical Values @ 0.05: %.2f' % kpss_test[3]['5%'])\nprint('p-value: %f' % kpss_test[1])","3378dfe7":"train_data_boxcox = data_boxcox[:train_len]\ntest_data_boxcox = data_boxcox[train_len:]\ntrain_data_boxcox_diff = data_boxcox_diff[:train_len-1]\ntest_data_boxcox_diff = data_boxcox_diff[train_len-1:]","716850f2":"from statsmodels.tsa.arima_model import ARIMA\nmodel = ARIMA(train_data_boxcox_diff, order=(1, 0, 0)) \nmodel_fit = model.fit()\nprint(model_fit.params)","1407704a":"#Recover original time series\ny_hat_ar = data_boxcox_diff.copy()\ny_hat_ar['ar_forecast_boxcox_diff'] = model_fit.predict(data_boxcox_diff.index.min(), data_boxcox_diff.index.max())\ny_hat_ar['ar_forecast_boxcox'] = y_hat_ar['ar_forecast_boxcox_diff'].cumsum()\ny_hat_ar['ar_forecast_boxcox'] = y_hat_ar['ar_forecast_boxcox'].add(data_boxcox[0])\ny_hat_ar['ar_forecast'] = np.exp(y_hat_ar['ar_forecast_boxcox'])","93644ad3":"#Plot train, test and forecast\nplt.figure(figsize=(12,4))\nplt.plot(train['Sales'], label='Train')\nplt.plot(test['Sales'], label='Test')\nplt.plot(y_hat_ar['ar_forecast'][test.index.min():], label='Auto regression forecast')\nplt.legend(loc='best')\nplt.title('Auto Regression Method')\nplt.show()","81a64158":"#Calculate RMSE and MAPE\nrmse = np.sqrt(mean_squared_error(test['Sales'], y_hat_ar['ar_forecast'][test.index.min():])).round(2)\nmape = np.round(np.mean(np.abs(test['Sales']-y_hat_ar['ar_forecast'][test.index.min():])\/test['Sales'])*100,2)\n\ntempResults = pd.DataFrame({'Method':['Autoregressive (AR) method'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","54b80ff4":"model = ARIMA(train_data_boxcox_diff, order=(0, 0, 1)) \nmodel_fit = model.fit()\nprint(model_fit.params)","ea14929a":"#Recover original time series\ny_hat_ma = data_boxcox_diff.copy()\ny_hat_ma['ma_forecast_boxcox_diff'] = model_fit.predict(data_boxcox_diff.index.min(), data_boxcox_diff.index.max())\ny_hat_ma['ma_forecast_boxcox'] = y_hat_ma['ma_forecast_boxcox_diff'].cumsum()\ny_hat_ma['ma_forecast_boxcox'] = y_hat_ma['ma_forecast_boxcox'].add(data_boxcox[0])\ny_hat_ma['ma_forecast'] = np.exp(y_hat_ma['ma_forecast_boxcox'])","740a3033":"#Plot train, test and forecast\nplt.figure(figsize=(12,4))\nplt.plot(data1['Sales'][:train_len], label='Train')\nplt.plot(data1['Sales'][train_len:], label='Test')\nplt.plot(y_hat_ma['ma_forecast'][test.index.min():], label='Moving average forecast')\nplt.legend(loc='best')\nplt.title('Moving Average Method')\nplt.show()","5c181345":"#Calculate RMSE and MAPE\nrmse = np.sqrt(mean_squared_error(test['Sales'], y_hat_ma['ma_forecast'][test.index.min():])).round(2)\nmape = np.round(np.mean(np.abs(test['Sales']-y_hat_ma['ma_forecast'][test.index.min():])\/test['Sales'])*100,2)\n\ntempResults = pd.DataFrame({'Method':['Moving Average (MA) method'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","30467a1a":"model = ARIMA(train_data_boxcox_diff, order=(1, 0, 1))\nmodel_fit = model.fit()\nprint(model_fit.params)","469c995a":"#Recover original time series\ny_hat_arma = data_boxcox_diff.copy()\ny_hat_arma['arma_forecast_boxcox_diff'] = model_fit.predict(data_boxcox_diff.index.min(), data_boxcox_diff.index.max())\ny_hat_arma['arma_forecast_boxcox'] = y_hat_arma['arma_forecast_boxcox_diff'].cumsum()\ny_hat_arma['arma_forecast_boxcox'] = y_hat_arma['arma_forecast_boxcox'].add(data_boxcox[0])\ny_hat_arma['arma_forecast'] = np.exp(y_hat_arma['arma_forecast_boxcox'])","5888daf8":"#Plot train, test and forecast\nplt.figure(figsize=(12,4))\nplt.plot( data1['Sales'][:train_len-1], label='Train')\nplt.plot(data1['Sales'][train_len-1:], label='Test')\nplt.plot(y_hat_arma['arma_forecast'][test.index.min():], label='ARMA forecast')\nplt.legend(loc='best')\nplt.title('ARMA Method')\nplt.show()","bce8f99c":"#Calculate RMSE and MAPE\nrmse = np.sqrt(mean_squared_error(test['Sales'], y_hat_arma['arma_forecast'][train_len-1:])).round(2)\nmape = np.round(np.mean(np.abs(test['Sales']-y_hat_arma['arma_forecast'][train_len-1:])\/test['Sales'])*100,2)\n\ntempResults = pd.DataFrame({'Method':['Autoregressive moving average (ARMA) method'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","8cc5215a":"model = ARIMA(train_data_boxcox, order=(1, 1, 1))\nmodel_fit = model.fit()\nprint(model_fit.params)","59fd7c5c":"#Recover original time series forecast\ny_hat_arima = data_boxcox_diff.copy()\ny_hat_arima['arima_forecast_boxcox_diff'] = model_fit.predict(data_boxcox_diff.index.min(), data_boxcox_diff.index.max())\ny_hat_arima['arima_forecast_boxcox'] = y_hat_arima['arima_forecast_boxcox_diff'].cumsum()\ny_hat_arima['arima_forecast_boxcox'] = y_hat_arima['arima_forecast_boxcox'].add(data_boxcox[0])\ny_hat_arima['arima_forecast'] = np.exp(y_hat_arima['arima_forecast_boxcox'])","3467f674":"#Plot train, test and forecast\nplt.figure(figsize=(12,4))\nplt.plot(train['Sales'], label='Train')\nplt.plot(test['Sales'], label='Test')\nplt.plot(y_hat_arima['arima_forecast'][test.index.min():], label='ARIMA forecast')\nplt.legend(loc='best')\nplt.title('Autoregressive integrated moving average (ARIMA) method')\nplt.show()","0c633dd3":"#Calculate RMSE and MAPE\nrmse = np.sqrt(mean_squared_error(test['Sales'], y_hat_arima['arima_forecast'][test.index.min():])).round(2)\nmape = np.round(np.mean(np.abs(test['Sales']-y_hat_arima['arima_forecast'][test.index.min():])\/test['Sales'])*100,2)\n\ntempResults = pd.DataFrame({'Method':['Autoregressive integrated moving average (ARIMA) method'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","dcf2ed15":"from statsmodels.tsa.statespace.sarimax import SARIMAX\n\nmodel = SARIMAX(train_data_boxcox, order=(1, 1, 1), seasonal_order=(1, 1, 1, 12)) \nmodel_fit = model.fit()\nprint(model_fit.params)","adf311fa":"#Recover original time series forecast\ny_hat_sarima = data_boxcox_diff.copy()\ny_hat_sarima['sarima_forecast_boxcox'] = model_fit.predict(data_boxcox_diff.index.min(), data_boxcox_diff.index.max())\ny_hat_sarima['sarima_forecast'] = np.exp(y_hat_sarima['sarima_forecast_boxcox'])","173fe96b":"#Plot train, test and forecast\nplt.figure(figsize=(12,4))\nplt.plot(train['Sales'], label='Train')\nplt.plot(test['Sales'], label='Test')\nplt.plot(y_hat_sarima['sarima_forecast'][test.index.min():], label='SARIMA forecast')\nplt.legend(loc='best')\nplt.title('Seasonal autoregressive integrated moving average (SARIMA) method')\nplt.show()","5352c9af":"#Calculate RMSE and MAPE\nrmse = np.sqrt(mean_squared_error(test['Sales'], y_hat_sarima['sarima_forecast'][test.index.min():])).round(2)\nmape = np.round(np.mean(np.abs(test['Sales']-y_hat_sarima['sarima_forecast'][test.index.min():])\/test['Sales'])*100,2)\n\ntempResults = pd.DataFrame({'Method':['Seasonal autoregressive integrated moving average (SARIMA) method'], 'RMSE': [rmse],'MAPE': [mape] })\nresults = pd.concat([results, tempResults])\nresults = results[['Method', 'RMSE', 'MAPE']]\nresults","8f159b54":"## 4. Build and Evaluate Time Series forecasting model","fe04138e":"### Comment:\n- This model captures the level and the trend along with the seasonality very well. ","8a428cbb":"### Comments: \n - P value <0.5 hence we rejct the null hypothesis . The series is stationary.","95aa8b70":"### Comments:\n-The trend captured is under forecasting. ","913dcbf2":"### Comments:\n- This model captures the trend component along with the level component but still does not capture the seasonality component.","ed659caa":"### Comments: \n - P value >0.5 hence we fail to rejct the null hypothesis . The series is stationary.","5a63217f":"### Comments:\n- Forecast = Rolling Average of past 3 months\u2019 sales. This method works well if the number of observations is fewer and the data is noisy since it  is able to predict the forecast better as it takes the variation of very few data points. ","fb99dab1":"### Comments:\n    - As seen in the plot,the naive method uses the last or previous month data which is 2014-06. We can see that the forecast for the next six months is the same value as the last observation .","ba55d2c5":"### Comments:\n    - From the plot we can see that we are able to capture trend in the forecast but could not cature the seasonality.","22f9cf9e":"### 4. Simple exponential smoothing","069323db":"## 3. Time Series Decomposition","6544ce68":"### 5. Holt's Exponential Smoothing","20d74f02":"### Comments:\n- The model captures the level of a time series which it does as seen in this graph.","96d604b9":"### Comments:\n- Forecast = Average of past months\u2019 sales i.e average of  the 42 months sales data which is not showing any trend or seasonality while train and test data has both trend and seasonality.","3e99727d":"### 3. Simple moving average method","677cab16":"### 3. Auto regression moving average method (ARMA)","60e6737b":"### 7. Holt Winter's multiplicative method","2a11e935":"### 4. Auto regressive integrated moving average (ARIMA)","01a72ffa":"### 1. Naive method\n","9e3646ca":"### 2. Simple Average method","e3fe0b31":"### 5. Seasonal auto regressive integrated moving average (SARIMA)","7816f00e":"## Auto Regressive methods","773f1767":"### 1. Auto regression method (AR)","26df8718":"## 1. Data Preparation","b6143e43":"### Comment:\n    - Here we are able to capture under forecasting trend but not seasonality in the forecast","51e6a7a3":"### Comments: \n - P value <0.5 hence we rejct the null hypothesis . The series is not stationary. so lets make non-stationary series into stationary series","9c20f46e":"### Comments:\n- The forecast captured both trend and seasonality very well. ","84ef628a":"### Simple Time series method","255eba1c":"### Conclusion:\n- Thus we can conclude that, Holt Winters additive method is the best forecasting method in the smoothing technique\n- And SARIMA - Seasonal Autoregressive Integrated moving average is the best method in ARIMA set of techniques.\n- Both these techniques could capture the trend very well and also the seasonality which the rest of the models could not. ","5e6d3b2f":"## Comments: \n - Consumer-APAC has the least COV. which means this has least variance and has more stability. Hence we select this most profitable market segment for sales forcasting. ","69a7843a":"### 6. Holt Winters' additive method","87c8d395":"### Comments:\n-ARMA model captured Trend which is under forecasting and also  no seasonality","9aac5788":"## 2. Split data Train-Test","0767f8ce":"### 2. Moving average method (MA)\n","d24d8be6":"### Comments: \n - P value <0.5 hence we rejct the null hypothesis . The series is stationary.","e07c677c":"### Comments:\n- Although this model captures level , trend and seasonality , the additive model has a better MAPE value. If the difference between subsequent troughs of the time series data does not increase as you progress in the graph, then the Holt-Winters' additive method works best"}}