{"cell_type":{"607b3c4a":"code","ba5fdeb5":"code","3c7c7a5b":"code","5e264e69":"code","ed3e3189":"code","8e0b08ac":"code","67ceda0e":"code","a7542b69":"code","25a1d4d1":"code","e1f67b26":"code","2eec529d":"code","9dc7925f":"code","c5a3c32a":"code","7b27845f":"code","5e7faf4c":"code","0551f4bb":"code","f3c81d57":"code","b69fc463":"code","c5abcbdd":"code","a51d87be":"code","55256f79":"code","ad4567dd":"code","029519f4":"code","8b73402d":"code","433c1100":"code","d2154978":"code","9592a5de":"code","54779d06":"code","9524110a":"code","7607480d":"code","a9cee102":"code","70ae4c50":"code","5d048714":"code","4ee9fe2f":"code","b955b83f":"code","1d628ff6":"code","edeb669e":"code","d35f2834":"code","3e8eb53c":"code","5feb6fbf":"code","5d44c69e":"code","530920b6":"code","19b0d346":"markdown","bd698367":"markdown","48c2c5d6":"markdown","eb0b8108":"markdown","a4f81fda":"markdown","aaa4df41":"markdown","846dc20f":"markdown","ac23002e":"markdown","ce221c8e":"markdown","5b2aba1d":"markdown","fb280893":"markdown","eb6fa53b":"markdown","99d853b7":"markdown","bfef0125":"markdown","18e65414":"markdown","32970c78":"markdown","8b01dbc2":"markdown","96070a0a":"markdown","7a48e87b":"markdown"},"source":{"607b3c4a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n\n# built-in imports\nimport sys\nimport os\n\n# data handling imports\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# machine learning imports\n# keras\nfrom keras.models import Model\nfrom keras import layers\nfrom keras import optimizers\nfrom keras.callbacks import EarlyStopping\nimport tensorflow as tf\nimport keras\n\n# sklearn\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import mean_squared_error\n\n\n# show version of imported packages\nprint(\"python:{}, keras:{}, tensorflow: {}\".format(sys.version, keras.__version__, tf.__version__))","ba5fdeb5":"# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","3c7c7a5b":"# set input dirname\ninput_dir = os.path.join('\/kaggle\/input', 'seleccion-datos-1-estacion-1-contaminante')","5e264e69":"# global variables initialization\nhistory_rmse = []","ed3e3189":"# make it reproducible\nnp.random.seed(42)","8e0b08ac":"# 1. Cargar datos\ndf = pd.read_csv(os.path.join(input_dir, 'pre_feb-may_2019+date.csv'), index_col = 0)\ndf.head()\n","67ceda0e":"df.info()","a7542b69":"window_width = 23\ntest_prop = 0.1\nvalidation_prop = 0.1\ntraining_size = round(len(df) * (1 - test_prop) * (1 - validation_prop)) - window_width\nvalidating_size = round(len(df) * (1 - test_prop) * validation_prop) - window_width\ntesting_size = round(len(df) * test_prop) - window_width","25a1d4d1":"# Creamos conjunto de entrada desde los datos origen\nX = df['NO2'].values\nnp.shape(X)","e1f67b26":"def normalize_data(data, training_size):\n    '''\n    Normalizing the data by mean-centering the data.\n    output within the training set have std = 1 and mean = 0\n    '''    \n    training_data = data[:training_size]\n    \n    mean = training_data.mean(axis=0)\n    std = training_data.std(axis=0)\n    \n    def _center_data(data, mean, std):\n        return (data - mean) \/ std\n    \n    data = _center_data(data, mean, std)\n    \n    return (data, mean, std)","2eec529d":"# X_norm, mean, std = normalize_data(X, training_size) # uncoment to normalize data","9dc7925f":"def normalize_data_sklearn(data, training_size):\n    '''\n    Normalizing the data by mean-centering the data.\n    output within the training set have std = 1 and mean = 0\n    '''    \n    training_data = data[:training_size]\n    training_data = np.reshape(training_data, (training_data.shape[0], 1))\n    \n    scaler = StandardScaler()\n    scaler.fit(training_data)\n    \n    data = scaler.transform(np.reshape(data, (data.shape[0], 1)))\n    \n    return (data[:,0], scaler)","c5a3c32a":"#from sklearn.preprocessing import StandardScaler\n#X_norm, scaler = normalize_data_sklearn(X, training_size)\n#print('Valores normalizaci\u00f3n:', scaler.mean_, np.sqrt(scaler.var_))","7b27845f":"def _get_chunk(data, seq_len = window_width):  \n    \"\"\"\n    data should be pd.DataFrame()\n    \"\"\"\n\n    chunk_X, chunk_Y = [], []\n    for i in range(len(data)-seq_len):\n        chunk_X.append(data[i:i+seq_len])\n        chunk_Y.append(data[i+seq_len])\n    chunk_X = np.array(chunk_X)\n    chunk_Y = np.array(chunk_Y)\n\n    return chunk_X, chunk_Y\n\ndef train_test_validation_split(data):  \n    \"\"\"\n    This just splits data to training, testing and validation parts\n    \"\"\"\n    \n    ntrn1 = round(len(data) * (1 - test_prop)) # get size for train set (including validation set)\n    ntrn2 = round(ntrn1 * (1 - validation_prop)) # get size for pure train set (without validation set)\n\n    train_set = data[:ntrn2]\n    val_set = data[ntrn2:ntrn1]\n    test_set = data[ntrn1:]\n    \n    X_train, y_train = _get_chunk(train_set)\n    X_val, y_val = _get_chunk(val_set)\n    X_test, y_test = _get_chunk(test_set)\n\n    return (X_train, y_train), (X_test, y_test), (X_val, y_val)","5e7faf4c":"(X_train, y_train), (X_test, y_test), (X_val, y_val) = train_test_validation_split(X)","0551f4bb":"# So we reshape the inputs to have dimensions (#examples, #values in sequences, dim. of each value)\nX_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\nX_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\nX_val = np.reshape(X_val, (X_val.shape[0], X_val.shape[1], 1))","f3c81d57":"print(np.shape(X_train))\nprint(X_train[:2])","b69fc463":"#print(np.shape(y_train))\nprint(y_train[:2])\n#print(np.shape(y_test))\nprint(y_test[:2])","c5abcbdd":"print(np.shape(X_train))\nprint(np.shape(X_val))\nprint(np.shape(X_test))\nprint(np.shape(y_train))\nprint(np.shape(y_val))\nprint(np.shape(y_test))","a51d87be":"aux_date_inputs = df[['hora', 'dia_semana']]\nprint(np.shape(aux_date_inputs))\nenc = OneHotEncoder()\naux_date_inputs = enc.fit_transform(aux_date_inputs) # categories = [aux_date_inputs.iloc[:,0].unique().sorted(), aux_date_inputs.iloc[:,1].unique().sorted()]\nprint(np.shape(aux_date_inputs))","55256f79":"aux_date_inputs = aux_date_inputs.toarray()\n#print(aux_date_inputs[:25])","ad4567dd":"aux_train_inputs = aux_date_inputs[:training_size]\nhead_val = training_size+window_width # we have skip the first window_width before to grab val inputs because they are missed in the sequence creation for (train, val, test) sets\naux_val_inputs = aux_date_inputs[head_val:head_val+validating_size]\nhead_test = head_val+validating_size+window_width\naux_test_inputs = aux_date_inputs[head_test:head_test+testing_size]\n\n# Alternative method\nno_test = round(len(aux_date_inputs) * (1 - test_prop))\nno_aux_test_inputs = aux_date_inputs[:no_test]\naux_train_inputs2 = no_aux_test_inputs[:training_size]\nval = round(len(no_aux_test_inputs) * validation_prop)\naux_val_inputs2 = no_aux_test_inputs[-val:-window_width]\naux_test_inputs2 = aux_date_inputs[no_test:-window_width]\n\n\nprint(np.shape(aux_train_inputs))\nprint(np.shape(aux_val_inputs))\nprint(np.shape(aux_test_inputs))\n#print(np.shape(aux_train_inputs2))\n#print(np.shape(aux_val_inputs2))\n\nnp.testing.assert_array_equal(aux_train_inputs2, aux_train_inputs)\nnp.testing.assert_array_equal(aux_val_inputs2, aux_val_inputs)\nnp.testing.assert_array_equal(aux_test_inputs2, aux_test_inputs)","029519f4":"def define_LSTM(seq_in_dim, hidden_layer_1, hidden_layer_2, dropout, rnn_dropout):\n    \n    lstm_input = keras.Input(shape = (window_width, seq_in_dim), name='lstm_input')\n    \n    # first layer\n    lstm = layers.LSTM(\n        input_shape=(window_width, seq_in_dim),\n        units=hidden_layer_1,\n        dropout=dropout,\n        recurrent_dropout=rnn_dropout,\n        return_sequences=True) (lstm_input)\n        \n    # second layer\n    lstm = layers.LSTM(\n        units=hidden_layer_2,\n        dropout=dropout,\n        recurrent_dropout=rnn_dropout,\n        return_sequences=False) (lstm)\n\n    #lstm = layers.Dense(units = seq_in_dim)(lstm) # use linear activation by default\n    \n    return lstm_input, lstm","8b73402d":"def define_model(aux_in_shape: tuple, in_out_dim: int, params: dict):\n    \n    units_fc_layer = params['units_fc_layer']\n    \n    lstm_input, lstm_model = define_LSTM(in_out_dim, params['first_layer'], params['second_layer'], params['dropout'], params['rnn-dropout'])\n    \n    # layer for auxiliar inputs\n    aux_input_layer = layers.Input(shape=aux_in_shape, name='aux_input_layer')\n    \n    # concatenate with LSTM\n    concatenated = layers.concatenate([lstm_model, aux_input_layer], axis=-1)\n    \n    # combine inputs with dense layer\n    combined = layers.Dense(units = units_fc_layer, activation='relu')(concatenated)\n    \n    # output layer\n    prediction = layers.Dense(units = in_out_dim)(combined) # use linear activation by default\n    \n    model = Model( inputs = [lstm_input, aux_input_layer], outputs = prediction, name = 'lstme')\n    \n    return model","433c1100":"def compile_model(model, optimizer):\n    model.compile( loss=\"mse\", metrics=['mape'], optimizer=optimizer)\n    model.summary()\n    keras.utils.plot_model(model, \"keras_prototipo1.png\", show_shapes=True)\n    return model\n    ","d2154978":"def time_series_forecasting_model(X_train, y_train, X_val, y_val, params, aux_train_inputs, aux_val_inputs,\n                                  do_early_stopping = False):\n    \n    in_out_dim = 1\n    aux_in_shape = np.shape(aux_train_inputs)[-1:] # should be 31\n    \n    # replace the hyperparameter inputs with references to params dictionary \n    model = define_model(aux_in_shape, in_out_dim, params)\n    \n    # Compile model\n    compile_model(model = model, optimizer=params['optimizer'](lr=params['lr']))\n    \n    # make sure history object is returned by model.fit()\n    \n    if do_early_stopping:\n        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights = True)\n        history = model.fit(\n                {'lstm_input': X_train, 'aux_input_layer': aux_train_inputs},\n                y_train,\n                shuffle = False,\n                batch_size=params['batch_size'], epochs=params['epochs'],\n                validation_data=({'lstm_input': X_val, 'aux_input_layer': aux_val_inputs}, y_val),\n                verbose = 1,\n                callbacks=[early_stopping])\n    else:\n        history = model.fit(\n                {'lstm_input': X_train, 'aux_input_layer': aux_train_inputs},\n                y_train,\n                shuffle = False,\n                batch_size=params['batch_size'], epochs=params['epochs'],\n                validation_data=({'lstm_input': X_val, 'aux_input_layer': aux_val_inputs}, y_val),\n                verbose = 1)\n    \n    # modify the output model\n    return history, model","9592a5de":"params = {\n    'first_layer': 128,\n    'second_layer': 128,\n    'optimizer': optimizers.Adam,\n    'batch_size': 32,\n    'epochs': 100,\n    'lr': 0.001,\n    'dropout': 0.01,\n    'rnn-dropout': 0.01,\n    'units_fc_layer': 128\n}","54779d06":"%%time\nhistory, model = time_series_forecasting_model(X_train, y_train, X_val, y_val, params, aux_train_inputs, aux_val_inputs, do_early_stopping = True)\nhistory_dict = history.history","9524110a":"# Lowest val_loss obtained\nbest_losses = sorted(history_dict['val_loss'])\nprint(f'Lowest val_loss obtained: {best_losses[:5]}')\nbest_losses_pos = np.argsort(history_dict['val_loss']) + 1\nprint(f'Corresponding epochs for those values: {best_losses_pos[:5]}')","7607480d":"def plot_history_loss(history_dict, offset = 0):\n    \n    if (offset > len(history_dict['mape'])):\n        return 'Offset greater than number of epochs'\n    \n    # Plotting the training and validation loss\n    loss_values = history_dict['loss'][offset:]\n    val_loss_values = history_dict['val_loss'][offset:]\n\n    epochs = range(offset + 1, len(loss_values) + offset + 1)\n\n    plt.plot(epochs, loss_values, 'bo', label='Training loss')\n    plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n\n    plt.show()","a9cee102":"plot_history_loss(history_dict, offset = 0)","70ae4c50":"plot_history_loss(history_dict, offset = 20)","5d048714":"def plot_history_acc(history_dict, offset = 0):\n    \n    if (offset > len(history_dict['mape'])):\n        return 'Offset greater than number of epochs'\n    \n    acc = [100 - mape for mape in history_dict['mape']][offset:]\n    val_acc = [100 - mape for mape in history_dict['val_mape']][offset:]\n    \n    epochs = range(offset + 1, len(acc) + offset + 1)\n    \n    plt.plot(epochs, acc, 'bo', label='Training accuracy (MAPE)')\n    plt.plot(epochs, val_acc, 'b', label='Validation accuracy (MAPE)')\n    plt.title('Training and validation accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.show()","4ee9fe2f":"plot_history_acc(history_dict, offset = 0)","b955b83f":"plot_history_acc(history_dict, offset = 20)","1d628ff6":"# Evaluate the model on the test data using `evaluate`\nresults = model.evaluate({'lstm_input': X_test, 'aux_input_layer': aux_test_inputs}, y_test, batch_size=24)\nprint(results)\nrmse = np.sqrt(results[0])\nprint(f'Valor de RMSE: {rmse} (\u00b5g \/ m\u00b3)\\nValor de MAPE: {results[1]} (%)')","edeb669e":"history_rmse.append(rmse)\nhistory_rmse","d35f2834":"# TODO: before last evaluation: train for last time model joining X_train and X_val data using best hyperparams found\npredictions = model.predict({'lstm_input': X_test, 'aux_input_layer': aux_test_inputs}, batch_size = 24)","3e8eb53c":"def get_performance(y_test, predictions):\n    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n    print('RMSE: %.3f' % rmse)\n    return rmse","5feb6fbf":"rmse = get_performance(y_test, predictions)","5d44c69e":"def plot_predvsactual(y_test, predictions):\n    # Plot predicted data vs actual data\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.plot(y_test, label='Recorded data')\n    plt.plot(predictions, label='Predicted data')\n    plt.title('Actual vs Predicted values')\n    plt.xlabel('Horas (en secuencia)')\n    plt.ylabel('NO\u00b2 (en \u00b5g\/m\u00b3)')\n    plt.legend()\n    plt.show()","530920b6":"plot_predvsactual(y_test, predictions)","19b0d346":"### An\u00e1lisis sobre la tasa de error","bd698367":"# Preparaci\u00f3n de los datos de entrada al modelo","48c2c5d6":"## Extracci\u00f3n de los datos desde el dataframe a objetos de numpy","eb0b8108":"# Carga de datos","a4f81fda":"## Creaci\u00f3n de las secuencias","aaa4df41":"# Definici\u00f3n de par\u00e1metros para el modelo","846dc20f":"# Evaluaci\u00f3n del modelo","ac23002e":"## An\u00e1lisis de la predicci\u00f3n del modelo","ce221c8e":"## Definici\u00f3n de variables para la preparaci\u00f3n de datos","5b2aba1d":"## Definici\u00f3n modelo","fb280893":"TODO: Para evitar errores comunes a la hora de partir los datos, deber\u00eda refactorizar el c\u00f3digo y unir esta parte con la anterior","eb6fa53b":"### An\u00e1lisis sobre la precisi\u00f3n","99d853b7":"## An\u00e1lisis del entrenamiento","bfef0125":"## Preparaci\u00f3n entradas auxiliares","18e65414":"# Entrenamiento de la red","32970c78":"We are therefore going to have a network with 1-dimensional input, two hidden layers of sizes 50 and 100 and eventually a 1-dimensional output layer.","8b01dbc2":"# Modelo RNN","96070a0a":"Cogemoslos valores en forma de numpy  para trabajar con ellos en lo sucesivo","7a48e87b":"## Normalizaci\u00f3n de los datos"}}