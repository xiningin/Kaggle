{"cell_type":{"5668e771":"code","1101850a":"code","ed6c28cc":"code","59ceef48":"code","e7049785":"code","e6e75a62":"code","9df3cbf5":"code","87ee2ae9":"code","c98b6dec":"code","41e8391b":"code","b7c97e16":"code","5b23d6df":"code","7c5b5710":"code","514cb662":"code","2f77bfb6":"code","426612e9":"markdown","4bdf5a35":"markdown","fa9e2e48":"markdown","216dc192":"markdown","be510f95":"markdown","5efda464":"markdown"},"source":{"5668e771":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfilepaths = []\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        filepaths.append(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1101850a":"df = pd.read_csv(filepaths[0])\ndf.head()","ed6c28cc":"news = df[['text', 'rubric']]\nnews = news[news['rubric'].isin(['\u042d\u043a\u043e\u043d\u043e\u043c\u0438\u043a\u0430', '\u0421\u043f\u043e\u0440\u0442'])]\nnews = news.reset_index(drop=True)\nnews","59ceef48":"!pip install wget\n!pip install ufal.udpipe","e7049785":"import wget\nfrom ufal.udpipe import Model, Pipeline\nimport os, re, sys\n\nudp_model_url = 'https:\/\/rusvectores.org\/static\/models\/udpipe_syntagrus.model'\nudp_model_filename = udp_model_url.split('\/')[-1]\n\nif not os.path.isfile(udp_model_filename):\n    print('UDPipe model not found. Downloading...', file=sys.stderr)\n    udp_model_filename = wget.download(udp_model_url)\n\nprint('\\nLoading the model...', file=sys.stderr)\nudp_model = Model.load(udp_model_filename)\nudp_process_pipeline = Pipeline(udp_model, 'tokenize', Pipeline.DEFAULT, Pipeline.DEFAULT, 'conllu')\n\n\ndef tag(sentance='\u0422\u0435\u043a\u0441\u0442 \u043d\u0443\u0436\u043d\u043e \u043f\u0435\u0440\u0435\u0434\u0430\u0442\u044c \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u0432 \u0432\u0438\u0434\u0435 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u044f!'):\n    modelfile='udpipe_syntagrus.model'\n    processed = udp_process_pipeline.process(sentance)\n    \n    result = []\n    processed = processed.split('\\n')\n    for line in processed:\n        if line.startswith('#') or not line:\n            continue\n        line = line.split('\\t')\n        if line[3] not in ['PUNCT', 'X']:\n            result.append(line[2] + '_' + line[3])\n    \n    return result","e6e75a62":"#\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u043c \u0440\u0430\u0431\u043e\u0442\u0443 \u0440\u0430\u0437\u043c\u0435\u0447\u0430\u044e\u0449\u0435\u0439 \u0444\u0443\u043d\u043a\u0446\u0438\u0438\nprint(tag(\"\u0421\u044a\u0435\u0448\u044c \u0435\u0449\u0451 \u044d\u0442\u0438\u0445 \u043c\u044f\u0433\u043a\u0438\u0445 \u0444\u0440\u0430\u043d\u0446\u0443\u0437\u0441\u043a\u0438\u0445 \u0431\u0443\u043b\u043e\u043a, \u0434\u0430 \u0432\u044b\u043f\u0435\u0439 \u0436\u0435 \u0447\u0430\u044e.\"))","9df3cbf5":"news['text'] = news['text'].apply(tag)\nnews['rubric'] = news['rubric'].apply(lambda x: 0 if x == '\u042d\u043a\u043e\u043d\u043e\u043c\u0438\u043a\u0430' else 1)\nprint(news)","87ee2ae9":"!pip install gensim","c98b6dec":"import sys\nimport gensim, logging\n\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)","41e8391b":"import zipfile\nmodel_url = 'http:\/\/vectors.nlpl.eu\/repository\/11\/180.zip'\nm = wget.download(model_url)\nmodel_file = model_url.split('\/')[-1]\nwith zipfile.ZipFile(model_file, 'r') as archive:\n    stream = archive.open('model.bin')\n    model = gensim.models.KeyedVectors.load_word2vec_format(stream, binary=True)","b7c97e16":"#\u041f\u0435\u0440\u0432\u0435\u0434\u0451\u043c \u043a\u0430\u0436\u0434\u043e\u0435 \u0441\u043b\u043e\u0432\u043e \u0432 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0438 \u0432 \u0432\u0435\u043a\u0442\u043e\u0440, \u0437\u0430\u0442\u0435\u043c \u043f\u043e \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0435\u043c \u0441\u0440\u0435\u0434\u043d\u0435\u0435\ndef sent2vec(sent):\n    sent = list(filter(lambda x: x in model.vocab, sent))\n    vects = [model.wv[word] for word in sent]\n    return np.mean(vects, axis=0)\n    \nnews['text'] = news['text'].apply(sent2vec)\nprint(news)","5b23d6df":"features = pd.DataFrame(news['text'][0].reshape(-1, len(news['text'][0])))\nfor i in range(1, news['text'].shape[0]):\n    features = features.append(pd.DataFrame(news['text'][i].reshape(-1, len(news['text'][i]))))\nfeatures = features.reset_index(drop=True)\n\ntargets = pd.Series(news['rubric'][0])\nfor i in range(1, news['rubric'].shape[0]):\n    targets = targets.append(pd.Series(news['rubric'][i]))\ntargets = targets.reset_index(drop=True)","7c5b5710":"import matplotlib.pyplot as plt\nfrom matplotlib.ticker import NullFormatter\nfrom sklearn import manifold, datasets","514cb662":"vecs_embedded = manifold.TSNE(n_components=2).fit_transform(features)\nclrs = np.asarray(list(targets.apply(lambda x: [1., 0., 0.] if x == 1 else [0., 1., 0.]).to_numpy()))","2f77bfb6":"plt.scatter(vecs_embedded.T[0], vecs_embedded.T[1], c=clrs)\nplt.show()","426612e9":"# \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c","4bdf5a35":"# \u041f\u0440\u0438\u043c\u0435\u043d\u0438\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u043e\u0442 RusVectores \u0434\u043b\u044f \u0432\u0435\u043a\u0442\u043e\u0440\u0438\u0437\u0430\u0446\u0438\u0438","fa9e2e48":"## \u0417\u0435\u043b\u0451\u043d\u044b\u0435 - \u044d\u043a\u043e\u043d\u043e\u043c\u0438\u043a\u0430, \u043a\u0440\u0430\u0441\u043d\u044b\u0435 - \u0441\u043f\u043e\u0440\u0442","216dc192":"## \u0412\u044b\u0431\u0435\u0440\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u0442\u0435\u043a\u0441\u0442\u044b \u0438 \u0440\u0443\u0431\u0440\u0438\u043a\u0438 \u042d\u043a\u043e\u043d\u043e\u043c\u0438\u043a\u0430 \u0438 \u0421\u043f\u043e\u0440\u0442","be510f95":"# \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c UDP \u0434\u043b\u044f \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438","5efda464":"# \u0418\u043c\u043f\u043e\u0440\u0442 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430"}}