{"cell_type":{"1baa4153":"code","e849f153":"code","f5035206":"code","1a621009":"code","5942c2bf":"code","a8ef76bf":"code","9a6f685a":"code","77015946":"code","b6d8b79c":"code","a1b347b4":"code","d039811b":"code","d5afaefb":"code","b8265747":"code","0f662aa3":"code","bc88969c":"code","ccb41696":"code","bda23a07":"code","53a039f3":"code","fffbf1a3":"code","444b211d":"code","c018bc96":"code","00a9b197":"code","3e31b629":"code","00d397a2":"code","4f9a5b98":"code","59f22c02":"code","415c1b55":"markdown","9195f560":"markdown","49b0bbed":"markdown","2e46245b":"markdown","a3c27160":"markdown","7e1b4154":"markdown","64b1c3b5":"markdown","4d38a839":"markdown","ed1eeb86":"markdown","05932242":"markdown","f4cb000c":"markdown","4f2d6ae7":"markdown","1b74d611":"markdown","15e6d1ed":"markdown","90c96214":"markdown","4a913d72":"markdown","efbecff9":"markdown","77697a10":"markdown","d037e6fb":"markdown","6873212a":"markdown","7d546a5f":"markdown","389bef95":"markdown","83300964":"markdown","128688dd":"markdown","289dcfe1":"markdown","3a9a190d":"markdown","578e9537":"markdown","79d43892":"markdown","c296991f":"markdown","318d195f":"markdown","3b67d209":"markdown","728e986e":"markdown","c62d5f00":"markdown","20e16e8c":"markdown","2fa571be":"markdown","2840c5cc":"markdown","d0abcc75":"markdown","07a9e438":"markdown","8afbaf5b":"markdown","7cf20cc4":"markdown","454055f2":"markdown"},"source":{"1baa4153":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt                                      # to plot graph\nimport seaborn as sns                                                # for intractve graphs\n%matplotlib inline\nfrom sklearn.linear_model import LogisticRegression                  # for Logistic regression\nfrom sklearn.cross_validation import train_test_split                # to split the data\nfrom sklearn.metrics import accuracy_score                           #Accuracy score calculation\nfrom sklearn.metrics import classification_report, confusion_matrix  # Classification report and confusion matrix\nfrom sklearn.neighbors import KNeighborsClassifier                   # for K nearest neighbours\nfrom sklearn import svm                                              #for Support Vector Machine (SVM) Algorithm\nfrom sklearn.tree import DecisionTreeClassifier                      #for using Decision Tree Algoithm\n\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","e849f153":"# df = pd.read_csv('..\/input\/train.csv')\ndf = pd.read_csv('..\/input\/Iris.csv')\ndf.head()","f5035206":"print(\"the total nuumber of rows {} and columns {} are present in this dataset :\".format(df.shape[0], df.shape[1]))","1a621009":"# First we will identify the predictor and target variable\ndf.columns","5942c2bf":"#Check the data types of each column\ndf.dtypes","a8ef76bf":"num_cols = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\ncat_cols = ['Species']\n# Check the levels in Speciies\ndf['Species'].value_counts()","9a6f685a":"#Histogram plots for knowing the data distribution\nplt.figure(1)\nplt.figure(figsize = (15,10))\nnum_cols = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\nfor col in num_cols:\n    plt.subplot(int(str(22)+str((num_cols.index(col)+1))))\n    sns.distplot(df[col])      \n   ","77015946":"#Box plots - shows visual outliers in the data\nplt.figure(1)\nplt.figure(figsize = (15,10))\nnum_cols = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\nfor col in num_cols:\n    plt.subplot(int(str(22)+str((num_cols.index(col)+1))))  \n    sns.boxplot(x=col,  data=df)\n\n   ","b6d8b79c":"# Check for any class Imbalance\ndf['Species'].value_counts(normalize = True)","a1b347b4":"df['Species'].value_counts().plot.bar(figsize=(10,4),title='Species - Split for Dataset')\nplt.xlabel('Species')\nplt.ylabel('Count')","d039811b":"plt.figure(1)\nplt.figure(figsize = (12,8))\nplt.scatter(df['SepalLengthCm'], df['SepalWidthCm'], s=np.array(df.Species == 'Iris-setosa'), marker='^', c='green', linewidths=5)\nplt.scatter(df['SepalLengthCm'], df['SepalWidthCm'], s=np.array(df.Species == 'Iris-versicolor'), marker='^', c='orange', linewidths=5)\nplt.scatter(df['SepalLengthCm'], df['SepalWidthCm'], s=np.array(df.Species == 'Iris-virginica'), marker='o', c='blue', linewidths=5)\nplt.xlabel('SepalLengthCm')\nplt.ylabel('SepalWidthCm')\nplt.legend(loc = 'upper left', labels =['Setosa', 'versicolor', 'virginica'])\nplt.show()","d5afaefb":"plt.figure(1)\nplt.figure(figsize = (12,8))\nplt.scatter(df['PetalLengthCm'], df['PetalWidthCm'], s=np.array(df.Species == 'Iris-setosa'), marker='^', c='green', linewidths=5)\nplt.scatter(df['PetalLengthCm'], df['PetalWidthCm'], s=np.array(df.Species == 'Iris-versicolor'), marker='^', c='orange', linewidths=5)\nplt.scatter(df['PetalLengthCm'], df['PetalWidthCm'], s=np.array(df.Species == 'Iris-virginica'), marker='o', c='blue', linewidths=5)\nplt.xlabel('PetalLengthCm')\nplt.ylabel('PetalWidthCm')\nplt.legend(loc = 'upper left', labels = ['Setosa', 'versicolor', 'virginica'])\nplt.show()","b8265747":"# Correlation between numerical variables\nnum_cols_data = (df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']])\nmatrix = num_cols_data.corr()\nf, ax = plt.subplots(figsize=(14, 4))\nsns.heatmap(matrix, vmax=.8, square=True, cmap=\"YlGnBu\", annot = True);","0f662aa3":"#Pair Plot\nsns.set()\ncolumns = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\nsns.pairplot(df[columns],size = 3 ,kind ='scatter',diag_kind='kde')\nplt.show()","bc88969c":"plt.figure(1)\nplt.figure(figsize = (15,10))\nnum_cols = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\nfor col in num_cols:\n    plt.subplot(int(str(22)+str((num_cols.index(col)+1))))\n    sns.violinplot(x='Species', y = col, data = df)   ","ccb41696":"#Check for missing values \ndf.isnull().sum()","bda23a07":"X = df.drop(['Id','Species'],1)\ny = df.Species","53a039f3":"# TODO: Shuffle and split the data into training and testing subsets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100)\n\n# Success\nprint (\"Training and testing split was successful.\")\n","fffbf1a3":"model_log = LogisticRegression()\nmodel_log.fit(X_train, y_train)","444b211d":"pred_cv_train = model_log.predict(X_train)\npred_cv_test = model_log.predict(X_test)","c018bc96":"print(\"the accuracy for train data is {}\".format(accuracy_score(y_train,pred_cv_train)))\nprint(\"the accuracy for test data is {}\".format(accuracy_score(y_test,pred_cv_test)))","00a9b197":"confusion_matrix = confusion_matrix( y_test,pred_cv_test)\nprint(\"the recall for this model is :\",confusion_matrix[1,1]\/(confusion_matrix[1,1]+confusion_matrix[1,0]))\n\nfig= plt.figure(figsize=(6,3))# to plot the graph\nprint(\"TP\",confusion_matrix[1,1,]) \nprint(\"TN\",confusion_matrix[0,0]) \nprint(\"FP\",confusion_matrix[0,1]) \nprint(\"FN\",confusion_matrix[1,0]) \nsns.heatmap(confusion_matrix,cmap=\"coolwarm_r\",annot=True,linewidths=0.5)\nplt.title(\"Confusion_matrix\")\nplt.xlabel(\"Predicted_class\")\nplt.ylabel(\"Real class\")\nplt.show()\nprint(confusion_matrix)\nprint(\"\\n--------------------Classification Report------------------------------------\")\nprint(classification_report(y_test, pred_cv_test)) ","3e31b629":"model_svm = svm.SVC() \nmodel_svm.fit(X_train,y_train) \npred_cv_train = model_svm.predict(X_train)\npred_cv_test = model_svm.predict(X_test)\nprint(\"the accuracy for train data is {}\".format(accuracy_score(y_train,pred_cv_train)))\nprint(\"the accuracy for test data is {}\".format(accuracy_score(y_test,pred_cv_test)))","00d397a2":"model_dt=DecisionTreeClassifier()\nmodel_dt.fit(X_train,y_train)\npred_cv_train = model_dt.predict(X_train)\npred_cv_test = model_dt.predict(X_test)\nprint(\"the accuracy for train data is {}\".format(accuracy_score(y_train,pred_cv_train)))\nprint(\"the accuracy for test data is {}\".format(accuracy_score(y_test,pred_cv_test)))","4f9a5b98":"model_knn=KNeighborsClassifier(n_neighbors=3) #this examines 3 neighbours as one pool, for putting the new data into a class\nmodel_knn.fit(X_train,y_train)\npred_cv_train = model_knn.predict(X_train)\npred_cv_test = model_knn.predict(X_test)\nprint(\"the accuracy for train data is {}\".format(accuracy_score(y_train,pred_cv_train)))\nprint(\"the accuracy for test data is {}\".format(accuracy_score(y_test,pred_cv_test)))","59f22c02":"# We can identify the accuracy for which K-value this model gives best accuracy\nindex=list(range(1,15))\naccur=pd.Series()\nx=[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\nfor i in list(range(1,15)):\n    model=KNeighborsClassifier(n_neighbors=i) \n    model.fit(X_train,y_train)\n    pred_test=model.predict(X_test)\n    accur=accur.append(pd.Series(accuracy_score(pred_test,y_test)))\nplt.plot(index, accur)\nplt.xticks(x);","415c1b55":"> Below are the steps involved to understand, clean and prepare our data for data exploration or building a predictive model:\n\n * Variable Identification\n * Univariate Analysis\n * Bi-variate Analysis\n * Missing values treatment\n * Outlier treatment\n * Variable transformation\n * Variable creation","9195f560":"* As we have only 4 attributes to do classification, PetalLenghthCm and PetalWidthCm, seem to be able to clearly separate the flowers clusters.\n  Hence these are very important attributes for final classification\n* By just running through only basic machine learning classification algorithms, we observe that almost all the models gave a good acuracy!\n\nFor any advanced machine learining algorithms like XGBoost, LightGBM, Neural Networks etc., I will implement them on separate project which I will notify.\n\nIf you have liked and found helpful about this kernel. **Please Upvote**.\n\n","49b0bbed":"* So from above plot it is evident that the K-value = 7 is the best we can have for a better accuracy","2e46245b":"* From our present dataset , we only have one categorical variable. Hence we don't require to plot above\n  Two-way table or Stacked Column Chart","a3c27160":"* Species is a target variable and all other are predictor variables","7e1b4154":"#### K-Nearest Neighbours","64b1c3b5":"#### Decision Trees Classifier","4d38a839":"#### Logistic Regression","ed1eeb86":"**Categorical & Continuous:** While exploring relation between categorical and continuous variables, we can draw box plots for each level of categorical variables.","05932242":"**Categorical & Categorical:** To find the relationship between two categorical variables, we can use following methods:\n\n**Two-way table:** We can start analyzing the relationship by creating a two-way table of count and count%. The rows represents the category of one variable and the columns represent the categories of the other variable. We show count or count% of observations available in each combination of row and column categories.\n\n**Stacked Column Chart: **  This method is more of a visual form of Two-way table.","f4cb000c":"Bi-variate Analysis finds out the relationship between two variables. \nHere, we look for association and disassociation between variables at a pre-defined significance level. We can perform bi-variate analysis for any combination of categorical and continuous variables. The combination can be: Categorical & Categorical, Categorical & Continuous and Continuous & Continuous. Different methods are used to tackle these combinations during analysis process.\n    \nLet\u2019s understand the possible combinations in detail:\n\n**Continuous & Continuous:** While doing bi-variate analysis between two continuous variables, we should look at scatter plot. It is a nifty way to find out the relationship between two variables. The pattern of scatter plot indicates the relationship between variables. The relationship can be linear or non-linear.","4f2d6ae7":"* It seems we have some outleirs for SepalWidthCm","1b74d611":"* I don't see any major surprises in the way the data is distributed","15e6d1ed":" ###  Load the data","90c96214":"*  By using SepalLenghthCm and SepalWidthCm, we are NOT able to clearly separate the flowers clusters ","4a913d72":"### Bi-variate Analysis","efbecff9":"<a id='models'><\/a>\n## Modelling","77697a10":"<a id='wrangling'><\/a>\n## Data Wrangling","d037e6fb":"* SepalLength and PetalLength are highly correlated\n* PetalLength and PetalWidth are highly correlated\n\nHence when considering for modelling , we can test by considering all the attributes and check accuracy and after we \ncan remove one of the attribute (May be SepalLength) because the clustering groups are nicely formed by considering the Petal attributes","6873212a":"**Categorical Variables:-** For categorical variables, we\u2019ll use frequency table to understand distribution of each category. We can also read as percentage of values under each category. It can be be measured using two metrics, \n    Count and Count% against each category. Bar chart can be used as visualization.","7d546a5f":"### Univariate Analysis\n\nAt this stage, we explore variables one by one. Method to perform uni-variate analysis will depend on whether the variable type is categorical or continuous. Let\u2019s look at these methods and statistical measures for categorical and continuous variables individually:\n\n**Continuous Variables:-**  In case of continuous variables, we need to understand the central tendency and spread of the variable. These are measured using various statistical metrics visualization methods as shown below:\n\n","389bef95":"* All the 3 categories are of equal proportion","83300964":"### Feature Engineering\n\n* Variable transformation\n* Variable creation\n\nFeature engineering itself can be divided in 2 steps:\n\nVariable transformation.\nVariable \/ Feature creation.\nThese two techniques are vital in data exploration and have a remarkable impact on the power of prediction.\n\nFrom our dataset, we do not need to do any feauture enginnering of variables. The given attributes will help us giving the predicttions during the modelling process\n","128688dd":"####  Variabel Identification","289dcfe1":"# Project: Iris Species\n###         Classify iris plants into three species in this classic dataset\n\n## Table of Contents\n<ul>\n<li><a href=\"#intro\">Introduction<\/a><\/li>\n<li><a href=\"#wrangling\">Data Wrangling<\/a><\/li>\n<li><a href=\"#eda\">Exploratory Data Analysis<\/a><\/li> \n<li><a href=\"#models\">Modelling<\/a><\/li> \n <li><a href=\"#conclusions\">Conclusions<\/a><\/li>\n<\/ul>","3a9a190d":"Though we have already plotted each of the individual charts for all the attribtues, it makes good sense to check the pair plot for the graphical view of correlations among the attributes. ( The correlation values can be seen in the correlation matrix obtained from Corr Plot)","578e9537":"Outlier can be of two types: \nUnivariate and Multivariate. Above, we have discussed the example of univariate outlier. These outliers can be found when we look at distribution of a single variable. Multi-variate outliers are outliers in an n-dimensional space.\n \n In order to find them, you have to look at distributions in multi-dimensions.\n \n From the above charts done in Univariate analysis plots and Bi-variate analysis plots - I do not see any major outliers which can impact the model.\n    ","79d43892":"Read about the confusion matrix in below links:\n   *  https:\/\/machinelearningmastery.com\/confusion-matrix-machine-learning\/\n   * https:\/\/towardsdatascience.com\/understanding-confusion-matrix-a9ad42dcfd62\n        ","c296991f":"#### Data Type\n\n* Character -\n     'Species'\n* Numeric -\n     'SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm'\n\n#### Variable Category\n* Categorical -\n    'Species'\n* Continuous -\n    'SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm'   \n    ","318d195f":"### Missing Value Treatment","3b67d209":"* As the k-value is hyperparameter here, we can actually find out the best K-value which can help us a best fit model.","728e986e":"### Outlier treatment","c62d5f00":"<a id='intro'><\/a>\n## Introduction\n\n> ### Understanding the Problem statement is the first step for us. This will help us to validate our hypothesis and clears our business understanding.\n  \n  * Now let us understand the business objective","20e16e8c":"Scatter plot shows the relationship between two variable but does not indicates\nthe strength of relationship amongst them. To find the strength of the relationship,\nwe use Correlation. Correlation varies between -1 and +1.\n\n* -1: perfect negative linear correlation\n* +1:perfect positive linear correlation and \n* 0: No correlation","2fa571be":"* There are no missing values","2840c5cc":"The dataset  includes three iris species with 50 samples each as well as some \nproperties about each flower. \nOne flower species is linearly separable from the other two, \nbut the other two are not linearly separable from each other.\n\nThe columns in this dataset are:\n\n* Id\n* SepalLengthCm\n* SepalWidthCm\n* PetalLengthCm\n* PetalWidthCm\n* Species\n\n\nSo our main objective is to build Machine Learning Model which will predict\nwhether the attributes given clearly classify the species.\n\nThis is a **Multi-class classification problem**.\n","d0abcc75":"![](http:\/\/)<a id='conclusions'><\/a>\n## Conclusions","07a9e438":"#### Support Vector Machine (SVM)","8afbaf5b":"*  By using PetalLenghthCm and PetalWidthCm, we are able to clearly separate the flowers clusters. Hence these are very important attributes for final classification","7cf20cc4":"* There is no class - imbalance in the target column","454055f2":"<a id='eda'><\/a>\n## Exploratory Data Analysis"}}