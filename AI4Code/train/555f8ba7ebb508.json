{"cell_type":{"50e8cc4b":"code","7211c006":"code","90a34bca":"code","6ff81fa7":"code","de647c9e":"code","5d280d20":"code","e11d2323":"code","6e28ca62":"code","d6f51f2f":"code","ca8c5267":"code","1f282b6d":"code","a28422d2":"code","57e69f8c":"code","67a1da85":"code","b51dc160":"code","209a204e":"code","cceaaf65":"code","f3e4d09e":"code","102a482a":"code","3d4fff86":"code","1defeb15":"code","3474e752":"code","f7b4c012":"code","79155adf":"code","3bf1009d":"code","544534f1":"code","7851fc54":"code","4d4ac4e3":"code","4e9f6718":"code","393a06c6":"code","9ec18cab":"code","b4bae5da":"code","7548dc5f":"code","ce06b3e6":"code","e4955814":"code","948aee81":"code","77af9c29":"code","c7664ebc":"code","376873c9":"code","39817c8d":"code","c7549e38":"code","728b5a3a":"markdown","598f5868":"markdown","e8fd7ea7":"markdown","60554a58":"markdown","b803b151":"markdown","255485c3":"markdown","63a0ec38":"markdown","796f6892":"markdown","c3fc4da9":"markdown","50105e0d":"markdown","6dc45917":"markdown","4ed0756a":"markdown","839b8e67":"markdown","3bf6f8db":"markdown","73e5cee9":"markdown","455575f6":"markdown","16212f4d":"markdown","09e8a836":"markdown","51c4f669":"markdown","cf5dddcb":"markdown"},"source":{"50e8cc4b":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\nweather_aus = pd.read_csv(\"..\/input\/weather-dataset-rattle-package\/weatherAUS.csv\")\n\nweather_aus.head()","7211c006":"# Look at the statistics of the continous variables\nweather_aus.describe()","90a34bca":"# Check datatypes\nweather_aus.dtypes","6ff81fa7":"plt.rcParams.update({'font.size': 5.5})\nweather_aus.hist()\nplt.show()","de647c9e":"plt.rcParams.update({'font.size': 10 })\nplt.style.use('fivethirtyeight') ","5d280d20":"# Relation between 3pm measures and raining the next day\nplt.subplots(figsize=(8, 5))\nsns.boxplot(data=weather_aus, x=\"RainTomorrow\", y = \"WindSpeed3pm\")","e11d2323":"plt.rcParams.update({'font.size': 10})\nplt.style.use('fivethirtyeight') ","6e28ca62":"plt.subplots(figsize=(8, 5))\nsns.boxplot(data=weather_aus, x='RainTomorrow', y = \"Temp3pm\")","d6f51f2f":"plt.rcParams.update({'font.size': 10})\nplt.style.use('fivethirtyeight')","ca8c5267":"plt.subplots(figsize=(8, 5)) \nsns.boxplot(data=weather_aus, x='RainTomorrow', y = \"Pressure3pm\")","1f282b6d":"plt.rcParams.update({'font.size': 10})\nplt.style.use('fivethirtyeight')","a28422d2":"plt.subplots(figsize=(8, 5)) \nsns.boxplot(data=weather_aus, x='RainTomorrow', y = \"Cloud3pm\")","57e69f8c":"plt.rcParams.update({'font.size': 10})\nplt.style.use('fivethirtyeight')","67a1da85":"plt.subplots(figsize=(8, 5)) \nsns.boxplot(data=weather_aus, x='RainTomorrow', y = \"Humidity3pm\")","b51dc160":"# Explore predictor variable\nweather_aus['RainTomorrow'].value_counts()","209a204e":"# Plot distribution of predictor variable\nplt.style.use('fivethirtyeight') \n\ncount_plot_predictor = sns.countplot(x = \"RainTomorrow\", data = weather_aus)\ncount_plot_predictor.set_title(\"Did it rain the next day?\")","cceaaf65":"weather_aus.isna().sum()","f3e4d09e":"# Remove missing values. \nweather_aus.dropna(inplace = True)","102a482a":"# Check again missing values\nweather_aus.isna().sum()","3d4fff86":"weather_aus['RainTomorrow'].value_counts()","1defeb15":"# Distribution of yes no after dropping missing values\nplt.subplots(figsize=(8, 5))\nplt.style.use('fivethirtyeight') \ncount_plot_predictor = sns.countplot(x = \"RainTomorrow\", data = weather_aus)\ncount_plot_predictor.set_title(\"Did it rain the next day? (after removing NA values)\")","3474e752":"# Add month variable\nweather_aus['Date']= pd.to_datetime(weather_aus['Date'])\nweather_aus['Month'] = pd.DatetimeIndex(weather_aus['Date']).month","f7b4c012":"# Make month variable a category \/ object type in order to get dummy variables later\nweather_aus = weather_aus.astype({\"Month\": object})","79155adf":"# Remove date variable\nweather_aus = weather_aus.drop(columns = \"Date\")","3bf1009d":"# Standardize continous variables with Robust scaler \n# Value = (value \u2013 median) \/ (p75 \u2013 p25)\nfrom sklearn.preprocessing import RobustScaler\n\n# Retrieve numerical columns to standardize\nnumerical_cols = weather_aus.select_dtypes(include=np.number).columns.tolist()\n\n# Transform numerical values with robust scaler \ntransformer = RobustScaler()\nweather_aus[numerical_cols] = transformer.fit_transform(weather_aus[numerical_cols])","544534f1":"# Look at the distribution again\nplt.rcParams.update({'font.size': 5.5})\nweather_aus.hist()\nplt.show()","7851fc54":"# Create dummy variables for raintoday and tomorrow\nweather_aus['RainToday'] = weather_aus['RainToday'].replace({'No':0, 'Yes':1})\nweather_aus['RainTomorrow'] = weather_aus['RainTomorrow'].replace({'No':0, 'Yes':1})","4d4ac4e3":"# Create K-1 dummy variables of categorical variables with the first value as a reference\nweather_aus = pd.get_dummies(weather_aus, drop_first=True)","4e9f6718":"pd.set_option('display.max_columns', None)\n\nweather_aus.head()","393a06c6":"from sklearn.model_selection import train_test_split\n\n# create feature variables(X)\nX = weather_aus.drop([\"RainTomorrow\"], axis=1)\n\n# create predictor\ny = weather_aus['RainTomorrow']\n\n# create training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=3)","9ec18cab":"len(X_train)","b4bae5da":"len(X_test)","7548dc5f":"# Import machine learning algorithmn and evaluation reports\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\n# Instantiate the classifier \nlr = LogisticRegression(solver=\"liblinear\", random_state=0) # liblinear works better for larger datasets\n\n# Fit to the training data\nlr.fit(X_train, y_train)\n\n# Predict the labels of the test set\ny_pred_lr = lr.predict(X_test)\n\n# Print classification report of test dataset\nprint(classification_report(y_test,y_pred_lr))","ce06b3e6":"# Import machine learning algorithmn\nfrom sklearn.svm import SVC\n\n# Instantiate the classifier\nsvc = SVC(random_state=0)\n\n# Fit the model\nsvc.fit(X_train, y_train)\n\n# Predict the labels on the test dataset\ny_pred_svc = svc.predict(X_test)\n\n# Print classification report of test dataset\nprint(classification_report(y_test,y_pred_svc))","e4955814":"# Import machine learning algorithmn\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Instantiatie the classifier\nrandom_forest = RandomForestClassifier(random_state=0)\n\n# Fit the model\nrandom_forest.fit(X_train, y_train)\n\n# Predict on test dataset\ny_pred_random_forest = random_forest.predict(X_test)\n\n# Print classification report on test dataset\nprint(classification_report(y_test,y_pred_random_forest))","948aee81":"# Balanced logistic regression\nlr_b = LogisticRegression(class_weight = \"balanced\", solver=\"liblinear\", random_state=0)\n\n# Fit to the training data\nlr_b.fit(X_train, y_train)\n\n# Predict the labels of the test set\ny_pred_lr_b = lr_b.predict(X_test)\n\n# Print classification report\nprint(classification_report(y_test,y_pred_lr_b))","77af9c29":"plt.rcParams[\"figure.figsize\"] = (6, 6)\nplt.rcParams.update({'font.size': 10})","c7664ebc":"from sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(lr_b, X_test, y_test)  ","376873c9":"# logistic regression feature importance\n\n# Get importance\nimportance = lr_b.coef_[0]\n\n# Sort importances\nsorted_index = np.argsort(importance)\n\n# Create labels\nlabels = X.columns[sorted_index] \n\n# make dataframe of importances\nimportance_df = pd.DataFrame({\"Feature importance\": importance[sorted_index], \"Feature name\": labels})\n\nimportance_df = importance_df.sort_values(by=['Feature importance'], ascending=False)\n\npd.set_option('display.max_rows', None)\nprint(importance_df)","39817c8d":"from sklearn.metrics import roc_curve\nplt.rcParams.update({'font.size': 10})\n\n# Get probabilities\nlr_b.predict_proba(X_test)[:,1]\ny_pred_prob = lr_b.predict_proba(X_test)[:,1]\n\n# Create ROC Curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n\nplt.plot(fpr, tpr)\n\nplt.xlabel(\"False Positive Rate\")\n\nplt.ylabel(\"True Positive Rate\")\n\nplt.plot([0, 1], [0, 1], \"k--\")\n\nplt.title(\"ROC Curve of (balanced) Logistic Regression\")\n\nplt.show()","c7549e38":"from sklearn.metrics import roc_auc_score\n\nauc = roc_auc_score(y_test, y_pred_lr_b)\nprint(auc)","728b5a3a":"**Comments**\n* The variables Date, Location, WindsGustDir, WindDir9am, WindDir3pm, RainToday, and RainTomorrow are categorical. I will later create dummy variables for these variables to meet the criteria of the machine learning algorithms. \n* I will transform the date variable to monthly's values, to prevent the low cardinality of the observations.","598f5868":"**Comments**\n* Some of the variables are skewed (Rainfall, Evaporation, Pressure). I will standardize them. They need to be standardized because logistic regression is based on a linear relationship. Furthermore, the algorithms will perform better when they are scaled to a standard range. \n* I will use robust standardization. It ignores outliers and uses the median. This is the formula: value = (value \u2013 median) \/ (p75 \u2013 p25)","e8fd7ea7":"**Comments**\n* The distributions still look a bit skewed. However, this robust transformation should lower the model bias and increase the accuracy. ","60554a58":"# Evaluating\nThe three models are doing quite well with accuracy around 85 percent. However, the recall score stays behind. The recall represents the number of correct predictions that it will rain tomorrow (true positives) divided by the number of correct predictions that it will rain (true positives) + the number of times it predicted no rainfall but it did rain (false negative). The latter seems to be frustrating and should be kept low. I've remarked earlier on the class imbalance of the predictor variable. There are not so many rain days to train on. Therefore, I am going to perform a logistic regression again with balanced classes. Balanced classes will have higher weights to the minority class (rain) and lower weights to the majority class (no rain.) The focus will be on the recall.","b803b151":"![water-drops-275938_640.jpg](attachment:3860a0b1-ce17-41c9-9eea-deea5d5afa4d.jpg)\n\nRainfall has a significant impact on society. Festivals, all sorts of activities, and sports matches are examples of events that could be heavily influenced by rainfall. Therefore, this topic is studied heavily. The goal of this analysis is to predict if it is going to rain the next day, based on the weather measures of the day before. This creates a binary classification problem (1 = Rain and 0 = No Rain). The dataset consists of weather data in Australia over 10 years (1 Nov 2007 to 25 Jun 2017). ","255485c3":"Let's look at the relationship between the weather measures on 3PM, the latest measure point, and Rainfall the next day. ","63a0ec38":"Now, let's train our model and see if we can predict rainfall based on the weather parameters of the day before.","796f6892":"**Comments**\n* The percentage of humidity at 3 PM, which is the amount of water vapor in the air, clearly is different between days where it rained the next days or did not.  \n* The median percentage of clouds is quite higher when it has rained. However, there is a large distribution of clouds when it didn't rain. \n* Pressure and Temperature seem to be very close to each other with some overlap. \n* There are a lot of outliers.","c3fc4da9":"**Comments**\n* The recall did improve, to 79 percent! It looks like that the model balancing has worked. \n* The accuracy lowered a bit to 80 percent, but that's isn't a big deal because my focus is on keeping the false negatives low which represents a higher recall.","50105e0d":"# Exploring ","6dc45917":"**Commments**\n* Classes are still imbalanced\n* 43993 observations of no rain and 12427 observations of rain are left to train and test the model on.","4ed0756a":"# Fitting","839b8e67":"# Conclusion\n\n* Humidity on 3 PM is the most important positive feature that the logistic regression model uses to make predictions. This was already clear in the explorative analysis, where the median of the two classes of rain and not rain were widely separated. \n* Clouds on 3PM and Wind gust speed are strong positive predictors of Rainfall next day.\n* The pressure on 3 PM is a strong negative predictor of rainfall the next day. That sounds reasonable because when the pressure is low, the air rises in the atmosphere where they meet. As the air rises, the water vapor within it condenses, forming clouds and often precipitation (Center for science education, 2021)\n* Sunshine is a strong negative predictor of rainfall the next day. Again, sounds logical because when there is a lot of sunshine there are fewer clouds. \n* Something that isn't explaining if it will rain tomorrow is the maximum temperature, wind direction, and Rainfall. Rainfall is the amount of rainfall recorded for the day in mm.\n* The area under the curve of the logistic regression is 0.79787","3bf6f8db":"**Comments**\n* We have 45136 rows (80%) of training data and 11284 rows (20%) of test data","73e5cee9":"**Comments**\n* 110316 times it didn't rain the next day and 31877 times it did rain. \n* This predictor is imbalanced with a ratio of 1: 3,46. An imbalanced predictor could bias our predictive accuracy. I may balance our classes later, but first, see how the basic model performs.","455575f6":"# Introduction","16212f4d":"# References\n1. https:\/\/scied.ucar.edu\/learning-zone\/how-weather-works\/highs-and-lows-air-pressure","09e8a836":"**Comments**\n* There are quite a lot of missing values. Especially in the sunshine, evaporation, and clouds variables. Dealing with missing values is kind of arbitrary, but I will remove them for now.","51c4f669":"# Transforming","cf5dddcb":"**Comments**\n* There is a wide range of minimum and maximum temperature. \n* The cloud variables have a high standard deviation compared to the mean. I will take a look a the distributions later and may standardize the variables. \n* The pressure variables have a low standard deviation. The distribution is close to their mean."}}