{"cell_type":{"98d49e18":"code","f42ac5ce":"code","987c1c38":"code","9d309d4e":"code","3dfcd89c":"code","ef4f86b5":"code","a6ef8fbd":"code","9786a28d":"code","70f789ea":"code","3647512c":"code","3389d078":"code","9d89c66c":"markdown","7d24419b":"markdown","d6cac703":"markdown","3a7499c9":"markdown","152bf8c7":"markdown","88c59385":"markdown","bb784836":"markdown","57e968a2":"markdown","437e2331":"markdown"},"source":{"98d49e18":"import pandas as pd\nimport numpy as np","f42ac5ce":"df_train = pd.read_csv(\"\/kaggle\/input\/widsdatathon2022\/train.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/widsdatathon2022\/test.csv\")","987c1c38":"## Drop irrelevant columns\n\ndf_train = df_train.drop('Year_Factor', axis=1)\ndf_train = df_train.drop('id', axis=1)\n\n## Handling null values\n\nfor col in ['year_built', 'energy_star_rating', 'direction_max_wind_speed', 'direction_peak_wind_speed', 'max_wind_speed', 'days_with_fog']:\n    df_train[col] = df_train[col].fillna(df_train[col].median())\ndf_train.isna().sum().sum() # check if there is any null values left\n\n## Encode categorical features (label encoding)\nfrom sklearn.preprocessing import OrdinalEncoder\ndf_train_LE = df_train.copy()\n\nordinalencoder = OrdinalEncoder()\ndf_train_LE['State_Factor_Cat'] = ordinalencoder.fit_transform(df_train_LE['State_Factor'].to_numpy().reshape(-1, 1))\ndf_train_LE['building_class_Cat'] = ordinalencoder.fit_transform(df_train_LE['building_class'].to_numpy().reshape(-1, 1))\ndf_train_LE['facility_type_Cat'] = ordinalencoder.fit_transform(df_train_LE['facility_type'].to_numpy().reshape(-1, 1))\n\ndf_train_LE = df_train_LE.drop('State_Factor', axis=1)\ndf_train_LE = df_train_LE.drop('building_class', axis=1)\ndf_train_LE = df_train_LE.drop('facility_type', axis=1)\n\ndf_train_LE.head()","9d309d4e":"# Drop irrelevant columns\nX_test = df_test.drop('Year_Factor', axis=1)\nX_test = X_test.drop('id', axis=1)\n\n# Handle null values\nfor col in ['year_built', 'energy_star_rating', 'direction_max_wind_speed', 'direction_peak_wind_speed', 'max_wind_speed', 'days_with_fog']:\n    X_test[col] = X_test[col].fillna(df_train[col].median())\n\n# Encode categorial features\nX_test['State_Factor_Cat'] = ordinalencoder.fit_transform(X_test['State_Factor'].to_numpy().reshape(-1, 1))\nX_test['building_class_Cat'] = ordinalencoder.fit_transform(X_test['building_class'].to_numpy().reshape(-1, 1))\nX_test['facility_type_Cat'] = ordinalencoder.fit_transform(X_test['facility_type'].to_numpy().reshape(-1, 1))\n\nX_test = X_test.drop('State_Factor', axis=1)\nX_test = X_test.drop('building_class', axis=1)\nX_test = X_test.drop('facility_type', axis=1)","3dfcd89c":"from sklearn.model_selection import train_test_split\n\ny = df_train_LE['site_eui']\nX = df_train_LE.drop('site_eui', axis=1)\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)","ef4f86b5":"from lightgbm import LGBMRegressor\n\nparams = {'learning_rate': 0.1, 'max_depth': 50, 'n_estimators': 20000, 'num_leaves': 65}\nmodel_1 = LGBMRegressor(**params)\n\nmodel_1.fit(X_train, y_train)","a6ef8fbd":"from catboost import CatBoostRegressor\n\nSEED = 42\nMODEL_MAX_DEPTH = 12\nMODEL_TASK_TYPE = 'GPU'\nMODEL_RL = 0.025\nMODEL_EVAL_METRIC ='RMSE'\nMODEL_LOSS_FUNCTION = 'RMSE'\nMODEL_ESR = 10\nMODEL_VERBOSE = 1000\nMODEL_ITERATIONS = 28000\n\nmodel_2 = CatBoostRegressor(\n    verbose=MODEL_VERBOSE,\n    early_stopping_rounds=MODEL_ESR,\n    random_seed=SEED,\n    max_depth=MODEL_MAX_DEPTH,\n    task_type=MODEL_TASK_TYPE,\n    learning_rate=MODEL_RL,\n    iterations=MODEL_ITERATIONS,\n    loss_function=MODEL_LOSS_FUNCTION,\n    eval_metric= MODEL_EVAL_METRIC\n)\nmodel_2.fit(X_train, y_train)","9786a28d":"models = [model_1, model_2]","70f789ea":"from sklearn.linear_model import LinearRegression\n\nX_meta = []\nfor model in models:\n    y_pred = model.predict(X_val) # predict on hold-out set\n    y_pred = y_pred.reshape(len(y_pred), 1) # reshape predictions into a matrix with one column\n    X_meta.append(y_pred)\n# create 2d array from predictions, each set is an input feature\nX_meta = np.hstack(X_meta)\n# define blending model\nblender = LinearRegression()\n# fit on predictions from base models\nblender.fit(X_meta, y_val)","3647512c":"X_meta = []\nfor model in models:\n    y_pred = model.predict(X_test) # predict on hold-out set\n    y_pred = y_pred.reshape(len(y_pred), 1) # reshape predictions into a matrix with one column\n    X_meta.append(y_pred)\n# create 2d array from predictions, each set is an input feature\nX_meta = np.hstack(X_meta)\n\ny_pred_b = blender.predict(X_meta)\nresults = pd.DataFrame(df_test['id'])\nresults['site_eui'] = y_pred_b\nresults.head()","3389d078":"# write predictions to CSV\nresults.to_csv(\"submission.csv\", header=True, index=False)","9d89c66c":"### Hold-out set for blending\nWe need to split a portion the training set to serve as training dataset for the blending step.","7d24419b":"### Base model: CatBoost\n\nReference: https:\/\/www.kaggle.com\/rhythmcam\/wids-2022-catboost-rmse","d6cac703":"### Data Preprocessing\n\nFor details of this section, refer to [my other notebook](https:\/\/www.kaggle.com\/yunlinlew\/wids-22-eda-lightgbm-comparecatencoding-shap).\n\n#### Preprocess Train set","3a7499c9":"# Model\n\n## Train base models\nWe need to create a number of base models. These can be any models we like for a regression or classification problem. In this datathon, we are tackling a regression problem. Here we experiment with LightGBM and CatBoost models.\n\n### Base model: LightGBM","152bf8c7":"## Data","88c59385":"#### Preprocess Test set","bb784836":"# Intro to blending\n\n**Blending** is an ensemble machine learning algorithm.\n\nIt is a colloquial name for **stacked generalization** or stacking ensemble where instead of fitting the meta-model on out-of-fold predictions made by the base model, it is fit on predictions made on a holdout dataset.\n\nBlending was used to describe stacking models that combined many hundreds of predictive models by competitors in the 1M Netflix machine learning competition, and as such, remains a popular technique and name for stacking in competitive machine learning circles, such as the Kaggle community.\n\n*Reference:* https:\/\/machinelearningmastery.com\/blending-ensemble-machine-learning-with-python\/","57e968a2":"# Submission\n### Make predictions on the test set","437e2331":"## Fit blending ensemble\nNext, we need to fit the blending model. The base models are previously fit on the training dataset. The meta-model is fit on the predictions made by each base model on the holdout dataset."}}