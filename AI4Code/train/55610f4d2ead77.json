{"cell_type":{"4462e815":"code","8b151e38":"code","6d46bde3":"code","4f707b73":"code","6989a521":"code","f8e45527":"code","5aa4872f":"code","de551186":"code","1256675e":"code","64202ea7":"code","871d5d56":"code","19872627":"code","7135aa75":"code","57b1ed0f":"code","7cce8af1":"code","b75bfabd":"markdown","9490accf":"markdown","e69bf3a1":"markdown","c8b2c668":"markdown","abec6efe":"markdown","89ef2e1f":"markdown","22b70569":"markdown","28750dbd":"markdown","87a6eeb6":"markdown","479a2eda":"markdown"},"source":{"4462e815":"import numpy as np\nimport pandas as pd\n\ntrain = pd.read_csv(\"..\/input\/bike-sharing-demand\/train.csv\")\ntest = pd.read_csv(\"..\/input\/bike-sharing-demand\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/bike-sharing-demand\/sampleSubmission.csv\")","8b151e38":"all_data_temp = pd.concat([train, test])\nall_data_temp","6d46bde3":"all_data = pd.concat([train, test], ignore_index=True)\nall_data","4f707b73":"from datetime import datetime\n\nall_data['date'] = all_data['datetime'].apply(lambda x: x.split()[0]) # Create date feature\nall_data['year'] = all_data['datetime'].apply(lambda x: x.split()[0].split('-')[0]) # Create year feature\nall_data['month'] = all_data['datetime'].apply(lambda x: x.split()[0].split('-')[1]) # Create month feature\nall_data['hour'] = all_data['datetime'].apply(lambda x: x.split()[1].split(':')[0]) # Create hour feature\nall_data[\"weekday\"] = all_data['date'].apply(lambda dateString : datetime.strptime(dateString,\"%Y-%m-%d\").weekday()) # Create weekday feature","6989a521":"categorical_features = ['season', 'holiday', 'workingday', 'weather', 'weekday', 'month', 'year', 'hour']\n\nfor feature in categorical_features:\n    all_data[feature] = all_data[feature].astype(\"category\")","f8e45527":"train = all_data[pd.notnull(all_data['count'])]\ntest = all_data[~pd.notnull(all_data['count'])]\ny = train['count']","5aa4872f":"drop_features = ['count', 'casual', 'registered', 'datetime', 'date', 'datetime', 'windspeed', 'month']\n\nX_train = train.drop(drop_features, axis=1)\nX_test = test.drop(drop_features, axis=1)","de551186":"X_train.info()","1256675e":"def rmsle(y_true, y_pred, convertExp=True):\n    # Apply exponential transformation function\n    if convertExp:\n        y_true = np.exp(y_true)\n        y_pred = np.exp(y_pred)\n        \n    # Convert missing value to zero after log transformation\n    log_true = np.nan_to_num(np.array([np.log(y+1) for y in y_true]))\n    log_pred = np.nan_to_num(np.array([np.log(y+1) for y in y_pred]))\n    \n    # Compute RMSLE\n    output = np.sqrt(np.mean((log_true - log_pred)**2))\n    return output","64202ea7":"from sklearn.linear_model import LinearRegression\n\n# Step 1: Create Model\nlinear_reg_model = LinearRegression()\n\n# Step 2: Train Model\nlog_y = np.log1p(y)  # Log Transformation of Target Value y\nlinear_reg_model.fit(X_train, log_y) \n\n# Step 3 : Predict\npreds = linear_reg_model.predict(X_train)\n\n# Step 4 : Evaluate\nprint ('Linear Regression RMSLE:', rmsle(log_y, preds, True))","871d5d56":"from sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics\n\n# Step 1: Create Model\nridge_model = Ridge()\n\n# Step 2-1 : Create GridSearchCV Object\n# Hyper-parameter List\nridge_params = {'max_iter':[3000], 'alpha':[0.1, 1, 2, 3, 4, 10, 30, 100, 200, 300, 400, 800, 900, 1000]}\n# Evaluate Function for Cross-Validation (RMSLE score)\nrmsle_scorer = metrics.make_scorer(rmsle, greater_is_better=False) \n# Create GridSearchCV Object (with Ridge)\ngridsearch_ridge_model = GridSearchCV(estimator=ridge_model,\n                                      param_grid=ridge_params,\n                                      scoring=rmsle_scorer,\n                                      cv=5)\n\n# Step 2-2 : Perform Grid Search\nlog_y = np.log1p(y) # Log Transformation of Target Value y\ngridsearch_ridge_model.fit(X_train, log_y) # Train (Grid Search)\n\nprint('Best Parameter:', gridsearch_ridge_model.best_params_)\n# Step 3 : Predict\npreds = gridsearch_ridge_model.best_estimator_.predict(X_train)\n\n# Step 4 : Evaluate\nprint('Ridge Regression RMSLE:', rmsle(log_y, preds, True))","19872627":"from sklearn.linear_model import Lasso\n\n# Step 1: Create Model\nlasso_model = Lasso()\n\n# Step 2-1 : Create GridSearchCV Object\n# Hyper-parameter List\nlasso_alpha = 1\/np.array([0.1, 1, 2, 3, 4, 10, 30, 100, 200, 300, 400, 800, 900, 1000])\nlasso_params = {'max_iter':[3000], 'alpha':lasso_alpha}\n# Create GridSearchCV Object (with Lasso)\ngridsearch_lasso_model = GridSearchCV(estimator=lasso_model,\n                                      param_grid=lasso_params,\n                                      scoring=rmsle_scorer,\n                                      cv=5)\n\n\n# Step 2-2 : Perform Grid Search\nlog_y = np.log1p(y)\ngridsearch_lasso_model.fit(X_train, log_y) # Train (Grid Search)\n\nprint('Best Parameter:', gridsearch_lasso_model.best_params_)\n# Step 3 : Predict\npreds = gridsearch_lasso_model.best_estimator_.predict(X_train)\n\n# Step 4 : Evaluate\nprint('Lasso Regression RMSLE:', rmsle(log_y, preds, True))","7135aa75":"from sklearn.ensemble import RandomForestRegressor\n\n# Step 1: Create Model\nrandomforest_model = RandomForestRegressor()\n\n# Step 2-1 : Create GridSearchCV Object\n# Hyper-parameter List\nrf_params = {'random_state':[42], 'n_estimators':[100, 120, 140]}\n# Create GridSearchCV Object (with Random Forest Regression)\ngridsearch_random_forest_model = GridSearchCV(estimator=randomforest_model,\n                                              param_grid=rf_params,\n                                              scoring=rmsle_scorer,\n                                              cv=5)\n\n# Step 2-2 : Perform Grid Search\nlog_y = np.log1p(y)\ngridsearch_random_forest_model.fit(X_train, log_y)\n\nprint('Best Parameter:', gridsearch_random_forest_model.best_params_)\n\n# \uc2a4\ud15d 3 : \uc608\uce21\npreds = gridsearch_random_forest_model.best_estimator_.predict(X_train)\n\n# \uc2a4\ud15d 4 : \ud3c9\uac00\nprint('Random Forest Regression RMSLE:', rmsle(log_y, preds, True))","57b1ed0f":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nrandomforest_preds = gridsearch_random_forest_model.best_estimator_.predict(X_test)\n\nfigure, axes = plt.subplots(ncols=2)\nfigure.set_size_inches(10, 4)\n\nsns.distplot(y, ax=axes[0], bins=50)\naxes[0].set_title('Train Data Distribution')\nsns.distplot(np.exp(randomforest_preds), ax=axes[1], bins=50)\naxes[1].set_title('Predicted Test Data Distribution');","7cce8af1":"submission['count'] = np.exp(randomforest_preds)\nsubmission.to_csv('submission.csv', index=False)","b75bfabd":"Feature Engineering","9490accf":"Random Forest Regression Model (Apply Grid Search)","e69bf3a1":"Ridge Model (Apply Gridsearch)","c8b2c668":"Evaluation score(RMSLE) function","abec6efe":"Lasso Model (Apply Gridsearch)","89ef2e1f":"Linear regression model","22b70569":"Check final features and types","28750dbd":"Change categorical data type for memory reduction","87a6eeb6":"Drop useless features","479a2eda":"Compare train data vs predicted test data distribution"}}