{"cell_type":{"e177c5ef":"code","fd92a649":"code","89203bf9":"code","11814018":"code","600e5f05":"code","59a07f5e":"code","a1eb5ba8":"code","e522b1c8":"code","0e28568b":"code","836f7c59":"code","a02403ca":"code","bb7e8889":"code","ecea07d7":"code","01c07704":"code","5bba4cb9":"code","ebe42bb5":"code","95b8fce0":"code","b11d752f":"code","f6f2beee":"code","b3df2209":"code","7bab4bba":"code","aafa7b31":"code","381d049f":"code","581e010a":"code","233688b3":"code","08797fd1":"code","d0725473":"code","2cc3f3a5":"code","70173f36":"code","beb7f1c4":"code","3b99e9f0":"code","74e27051":"code","b5548d1c":"code","37d86d71":"code","255ac2fb":"code","8e96b993":"code","5e94e221":"code","e77d5bf4":"code","1e8d6481":"code","17b96efb":"code","24a4fac5":"code","99feb2e7":"code","0af35709":"code","40000828":"code","3cd0f1e2":"code","7e0d49b3":"code","abd2eee6":"code","dd043b28":"markdown","5c14a941":"markdown","796a0829":"markdown"},"source":{"e177c5ef":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","fd92a649":"import pandas as pd\ncancer = pd.read_csv(\"..\/input\/cancer.csv\")","89203bf9":"cancer.head()","11814018":"cancer.shape","600e5f05":"types = cancer.dtypes\ntypes","59a07f5e":"cancer.isnull().sum()","a1eb5ba8":"cancer[\"radius_mean\"].fillna(cancer[\"radius_mean\"].mean(),inplace=True)","e522b1c8":"cancer.info()","0e28568b":"cancer.drop([\"id\"],axis=1,inplace=True)","836f7c59":"cancer.head()","a02403ca":"cancer['diagnosis'].replace(['M','B'],[0,1],inplace=True)","bb7e8889":"cancer.head()","ecea07d7":"#dividig the data set independent variable and depended variable\n#my target variable is diagnosis\n\nX = cancer.iloc[1:31]\ny = cancer.iloc[0]","01c07704":"X.head()","5bba4cb9":"y.head()","ebe42bb5":"X.describe()","95b8fce0":"## it used to histograms \nfrom matplotlib import pyplot\ncancer.hist(figsize=(12,10))\npyplot.show()","b11d752f":"correlations = cancer.corr()\nfig = pyplot.figure(figsize=(12,10))\nax = fig.add_subplot(111)\ncax = ax.matshow(correlations, vmin=-1, vmax=1)\nfig.colorbar(cax)\npyplot.show()","f6f2beee":"import pandas  \nfrom pandas.plotting import scatter_matrix\n\ndataCorr = cancer.corr()\npandas.plotting.scatter_matrix(dataCorr,figsize=(12,10))\npyplot.show()","b3df2209":"from sklearn.preprocessing import MinMaxScaler\nfrom numpy import set_printoptions\narray = cancer.values\nX = array[:,1:31]\ny = array[:,0]\n\nscaler = MinMaxScaler(feature_range=(0,1))\nrescaledX = scaler.fit_transform(X)\nprint(rescaledX[0:5,:])","7bab4bba":"from sklearn.preprocessing import StandardScaler\nscalar = StandardScaler().fit(X)\nscaled_X = scalar.transform(X)\nprint(scaled_X)","aafa7b31":"\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n# feature extraction\nmodel = LogisticRegression()\nrfe = RFE(model, 3)\nfit = rfe.fit(X, y)\nprint(fit.n_features_)\nprint(fit.support_)  # Selected Features:\nprint(fit.ranking_)  # Feature Ranking","381d049f":"# feature extraction model\nfrom sklearn.ensemble import ExtraTreesClassifier\nmodel = ExtraTreesClassifier()\nmodel.fit(X, y)\nprint(model.feature_importances_)","581e010a":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\ntest_size = 0.33\nseed = 7\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\nresult = model.score(X_test, y_test)\nprint(result*100.0)","233688b3":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=0)","08797fd1":"from sklearn.ensemble import RandomForestClassifier\nmodel=RandomForestClassifier()\nmodel.fit(X_train,y_train)\n","d0725473":"pred=model.predict(X_test)\npred","2cc3f3a5":"y_pred=model.predict(X_test)\ny_pred","70173f36":"from sklearn.metrics import accuracy_score\nprint(accuracy_score(y_test,pred)*100,\"%\")","beb7f1c4":"from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(y_test, y_pred))","3b99e9f0":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","74e27051":"from sklearn.neighbors import KNeighborsClassifier\nmodel = KNeighborsClassifier()\nmodel.fit(X_train,y_train)","b5548d1c":"pred=model.predict(X_test)\npred","37d86d71":"y_pred=model.predict(X_test)","255ac2fb":"from sklearn.metrics import accuracy_score\nprint(accuracy_score(y_test,pred)*100,\"%\")","8e96b993":"from sklearn.metrics import confusion_matrix\n\nprint(confusion_matrix(y_test, y_pred)) ","5e94e221":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","e77d5bf4":"import matplotlib.pyplot as plt  \nfrom sklearn.metrics import roc_curve, auc\nfpr, tpr, thresholds = roc_curve(y_pred, y_test)\nroc_auc = auc(fpr, tpr)\n\n#plt.figure()\nplt.plot(fpr, tpr, color='darkorange', \n         label='ROC curve (area = %0.3f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy',  linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","1e8d6481":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline","17b96efb":"pipe_lr = make_pipeline(StandardScaler(),PCA(n_components=2),LogisticRegression(random_state=1))\npipe_lr.fit(X_train,y_train)","24a4fac5":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import learning_curve\npipe_lr = make_pipeline(StandardScaler(), LogisticRegression(penalty='l2', random_state=1))\ntrain_sizes, train_scores, test_scores = learning_curve(estimator=pipe_lr, \n                                                          X=X_train, y=y_train, \n                                                train_sizes=np.linspace(0.1, 1.0, 10), cv=10, n_jobs=1)\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\nplt.plot(train_sizes, train_mean, color='blue', marker='o', markersize=5, label='training accuracy')\nplt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color='blue')\nplt.plot(train_sizes, test_mean, color='green', linestyle='--', marker='s', markersize=5, label='validation accuracy')\nplt.fill_between(train_sizes, test_mean + test_std, test_mean - test_std, alpha=0.15, color='green')\nplt.grid()\nplt.xlabel('Number of training samples')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.ylim([0.8, 1.0])\nplt.show()","99feb2e7":"from sklearn.model_selection import validation_curve\nparam_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\ntrain_scores, test_scores = validation_curve(estimator=pipe_lr,X=X_train,y=y_train,param_name='logisticregression__C',param_range=param_range,cv=10)\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\nplt.plot(param_range, train_mean,color='blue', marker='o', markersize=5, label='training accuracy')\nplt.fill_between(param_range, train_mean + train_std, train_mean - train_std, alpha=0.15,color='blue')\nplt.plot(param_range, test_mean, color='green', linestyle='--', marker='s', markersize=5, label='validation accuracy')\nplt.fill_between(param_range,test_mean + test_std, test_mean - test_std, alpha=0.15, color='green')\nplt.grid()\nplt.xscale('log')\nplt.legend(loc='lower right')\nplt.xlabel('Parameter C')\nplt.ylabel('Accuracy')\nplt.ylim([0.8, 1.03])\nplt.show()","0af35709":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\npipe_svc = make_pipeline(StandardScaler(),SVC(random_state=1))\nparam_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\nparam_grid = [{'svc__C': param_range, 'svc__kernel': ['linear']}, {'svc__C': param_range, 'svc__gamma': param_range,'svc__kernel': ['rbf']}]\ngs = GridSearchCV(estimator=pipe_svc, param_grid=param_grid, scoring='accuracy', cv=10, n_jobs=-1)\ngs = gs.fit(X_train, y_train)\nprint(gs.best_score_)\nprint(gs.best_params_)","40000828":"clf = gs.best_estimator_\nclf.fit(X_train, y_train)\nprint('Test accuracy: %.3f' % clf.score(X_test, y_test))","3cd0f1e2":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","7e0d49b3":"#SAVING MODEL\nfrom pandas import read_csv\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.externals.joblib import dump\nfrom sklearn.externals.joblib import load","abd2eee6":"data = pd.read_csv(\"..\/input\/cancer.csv\")\n#print(data.shape)\n\n#array = data.values\n\nX = array[:,1:31]\nY = array[:,0]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=7)\n# Fit the model on 33%\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\nfilename = 'finalized_model.sav'\ndump(model, filename)\n\nloaded_model = load(filename)\nresult = loaded_model.score(X_test, y_test)\nprint(result)","dd043b28":"The grid search used to improve accuracy ","5c14a941":"UNDER FITTING DATA SET","796a0829":"    OVER FITTING data set\n"}}