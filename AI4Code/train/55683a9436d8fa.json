{"cell_type":{"75bd0abb":"code","fa75c405":"code","0204ee83":"code","5be9863b":"code","6210a70d":"code","151b9c6d":"code","825424fe":"code","7d91a8e3":"code","353db1eb":"code","006148b9":"code","19fde819":"code","48ed2e1e":"code","a3c8c338":"code","0fda2fa0":"code","e4339f3f":"code","4c7e6e3f":"code","79759527":"code","5ebccc2b":"code","110e95d5":"code","ef8fda78":"code","7d8b71ea":"code","94b10c03":"code","e0c5ec96":"markdown","87e036e9":"markdown","4043d162":"markdown","0a380233":"markdown","9170391c":"markdown","c9c0afdb":"markdown","4b0b4671":"markdown","97db2af1":"markdown","05a7f908":"markdown","fdba312e":"markdown","2f01bee0":"markdown","d7678a4e":"markdown","18664bd3":"markdown","ca996fa9":"markdown","6fd9f60b":"markdown","c0f17742":"markdown","1a22060e":"markdown","2f8ed5d1":"markdown","96e5b06e":"markdown","56eb2d30":"markdown","8d49156c":"markdown","fdd88b1a":"markdown","0ae99ecb":"markdown","b9ed7f11":"markdown","cf966bba":"markdown","99df7658":"markdown","aa80f3f1":"markdown","aaefc359":"markdown","7fbc23f8":"markdown","e0202388":"markdown","5151d1fd":"markdown","92b7334e":"markdown","2c98b101":"markdown","81a74527":"markdown","b5afc18f":"markdown"},"source":{"75bd0abb":"!pip install tensorflow==2.0.0-alpha0","fa75c405":"import tensorflow as tf\nfrom tensorflow import keras\nprint(\"The TensorFlow version installed in the Notebook is TensorFlow {}\".format(tf.__version__))\nprint(\"The TensorFlow version of Keras installed in the Notebook is Keras {}\".format(keras.__version__))","0204ee83":"#Defining in and out\nimport numpy as np\nx_in = np.array([-1.0,  0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\ny_out = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)","5be9863b":"model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])","6210a70d":"#Compiling the model\nmodel.compile(optimizer='sgd', loss='mean_squared_error')","151b9c6d":"#Training the network\nmodel.fit(x_in, y_out, epochs=50)","825424fe":"print(\"For the x_in of 10, the value of y_out is {}\".format(model.predict([10.0])))","7d91a8e3":"#Installing the datasets inbuilt in TensorFLow\n!pip install -U tensorflow_datasets\n\n#Loading the TensorFLow datasets\nimport tensorflow_datasets as tfds\n\n#Plotting packages\nimport matplotlib.pyplot as plt\n\n#Improve progress bar display\nimport tqdm\nimport tqdm.auto\ntqdm.tqdm = tqdm.auto.tqdm","353db1eb":"#Loading the Fashion MNIST dataset\nmnist = tf.keras.datasets.fashion_mnist\n\n#Getting training & testing datasets\n(training_images, training_labels), (test_images, test_labels) = mnist.load_data()","006148b9":"plt.imshow(training_images[4])\nprint(training_labels[4])\nprint(training_images[4])","19fde819":"#Normalizing the pixiel values\ntraining_images  = training_images \/ 255.0\ntest_images = test_images \/ 255.0","48ed2e1e":"model = tf.keras.Sequential([tf.keras.layers.Flatten(), #Flattening the image into numpy array\n                             tf.keras.layers.Dense(128, activation=tf.nn.relu), #First hidden layer with 128 units & relu activation function\n                             tf.keras.layers.Dense(10, activation=tf.nn.softmax) #Output layer with 10 units to give the probability of 10 output classes\n                           ])","a3c8c338":"#Compiling the model\nmodel.compile(optimizer='adam', \n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])","0fda2fa0":"#Training the model\nmodel.fit(training_images, training_labels, epochs=10, batch_size=6000)","e4339f3f":"model.evaluate(test_images, test_labels)","4c7e6e3f":"classifications = model.predict(test_images)\n\nprint(classifications[0])","79759527":"print(test_labels[0])","5ebccc2b":"model = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu,\n                           input_shape=(28, 28, 1)),\n    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation=tf.nn.relu),\n    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dense(10,  activation=tf.nn.softmax)\n])","110e95d5":"model.compile(optimizer='adam', \n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])","ef8fda78":"#Data reshaping for training\n(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\ntraining_images=training_images.reshape(60000, 28, 28, 1)\ntraining_images=training_images \/ 255.0\ntest_images = test_images.reshape(10000, 28, 28, 1)\ntest_images=test_images\/255.0","7d8b71ea":"#Training the model\nmodel.fit(training_images, training_labels, epochs=10, batch_size=6000)","94b10c03":"test_loss, test_accuracy = model.evaluate(test_images, test_labels)\nprint('Accuracy on test dataset:', test_accuracy)\nprint('Loss on the test dataset:', test_loss)","e0c5ec96":"So, on the test data the model has the accuray of 84% and when we use unseen data for predicting then the model may still fall short of accuracy. Therefore, the parameters should be tuned while training the model in order to get high accuracy on unseen data.","87e036e9":"**Time to put the model to test**  \nLet us try with some new value of x_in which the model has not seen and let us see what the model predicts. Using the predict functions we can make the model to predict for any new values of x_in.","4043d162":"**Training the model**  \nTraining is performed using *fit* method.  In order for the model to learn from the data there are various parameters that should be incorporated while training the model.  \nSome of the main parameters are *Batch_Size* and *epochs* as they help in efficient training of the network.","0a380233":"![](https:\/\/d20vrrgs8k4bvw.cloudfront.net\/images\/courses\/logos\/logo-color-tensorflow.png)\nTensorFlow is a free and open-source software library for dataflow and differentiable programming across a range of tasks. It is a symbolic math library, and is also used for machine learning applications such as neural networks. It is used for both research and production at Google. It is a standard expectation in the industry to have experience in TensorFlow to work in machine learning.\n\nTensorFlow was developed by the Google Brain team for internal Google use. It was released under the Apache 2.0 open-source license on November 9, 2015.\n\nSource: [wiki\/TensorFlow](https:\/\/en.wikipedia.org\/wiki\/TensorFlow)\n\nGoogle has released the latest version of TensorFlow which is TensorFlow 2.0 alpha in the TensorFlow Dev Submit 2019 which held on between 6 March, 2019 and 7 March 2019.  \n\nKindly, find the details of the TensorFlow latest release in the below link  \n[TensorFlow Latest](https:\/\/www.youtube.com\/channel\/UC0rqucBdTuFTjJiefW5t-IQ\/videos)","9170391c":"Epochs is basically when an ENTIRE dataset is passed forward and backward through the neural network only ONCE. In our case we have defined Epochs as 500 which is the network is trained over 300 times (50 Epochs * 6 observations).  \n\n*Details on Epochs:*  \n[Epochs Vs Iterations Vs Batch size](https:\/\/towardsdatascience.com\/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9)","c9c0afdb":"**Importing the Fashion MNIST dataset**  \nFashion MNIST dataset contains 70,000 grayscale images in 10 categories. The images show individual articles of clothing at low resolution (28  \u00d7  28 pixels).  \nWe will use 60,000 images to train the network and 10,000 images to evaluate how accurately the network learned to classify images. We can access the Fashion MNIST directly from TensorFlow, using the Datasets API.","4b0b4671":"**Time to get our hands dirty!!**  \nFirst Let us try some simple things in the new version of TensorFlow which uses Keras for building the neural network.  \nConsider an example where x_in is the input and y_out is the output. So, Now we are going to use numpy to define x_in and y_out as numpy array.  \n","97db2af1":"The above pixiel values are shown for the class 0 which is T-shirt\/top. These pixiel values ranges between 0 and 255 which should be normalized between 0 and 1 before building the model.","05a7f908":"The images are 28  \u00d7  28 arrays, with pixel values in the range [0, 255]. The labels are an array of integers, in the range [0, 9]. These correspond to the class of clothing the image represents:\n\n<table>\n  <tr>\n    <th>Label<\/th>\n    <th>Class<\/th> \n  <\/tr>\n  <tr>\n    <td>0<\/td>\n    <td>T-shirt\/top<\/td> \n  <\/tr>\n  <tr>\n    <td>1<\/td>\n    <td>Trouser<\/td> \n  <\/tr>\n    <tr>\n    <td>2<\/td>\n    <td>Pullover<\/td> \n  <\/tr>\n    <tr>\n    <td>3<\/td>\n    <td>Dress<\/td> \n  <\/tr>\n    <tr>\n    <td>4<\/td>\n    <td>Coat<\/td> \n  <\/tr>\n    <tr>\n    <td>5<\/td>\n    <td>Sandal<\/td> \n  <\/tr>\n    <tr>\n    <td>6<\/td>\n    <td>Shirt<\/td> \n  <\/tr>\n    <tr>\n    <td>7<\/td>\n    <td>Sneaker<\/td> \n  <\/tr>\n    <tr>\n    <td>8<\/td>\n    <td>Bag<\/td> \n  <\/tr>\n    <tr>\n    <td>9<\/td>\n    <td>Ankle boot<\/td> \n  <\/tr>\n<\/table>\n","fdba312e":"**Highlights of TensorFlow 2.0 alpha**  \n* Eager execution as a central feature of 2.0. It aligns users\u2019 expectations about the programming model better with TensorFlow practice and should make TensorFlow easier to learn and apply.  \n* Keras tightly integrated with the TensorFlow ecosystem, and has support for Eager execution, tf.data API, tf.distribute.MirroredStrategy for multi-GPU training, TensorBoard visualization, and TF Lite and TF.js conversion.  \n* Starter list of TF-Hub models loadable in TF 2.0.  \n* Autograph making it easier to write models with custom control flow ops and getting graph performance with tf.function.  \n* Flexible API choices that let users build models quickly using high level building blocks, or take full control by writing custom models and custom training loops using lower level constructs.  \n* Support for TPUs using the TPUStrategy with tf.estimator.  \n* Support for multi worker synchronous training using tf.distribute.Strategy with tf.estimator.  \n* New end-to-end ML framework for building ML pipelines with TFX.  \n* Simplified and cleaned-up API by removing deprecated APIs, reducing redundancies, renaming symbols to more intuitive names, simplifying how variables are treated.  \n* Removal of tf.contrib - These features have been either moved to TensorFlow Core, or to tensorflow\/addons, or are no longer part of the TensorFlow build but are developed and maintained by their respective owners.  \n* Updated and revised documentation, examples, and website, including migration docs and TF 1.x to 2.0 converter guide.  \n* New releases of TensorFlow.js and Swift for TensorFlow packages.  \n* Many more add-ons and extensions (eg. TF Datasets, TF Federated, TF Privacy, etc).  \n\nSource: [Official Link](https:\/\/www.tensorflow.org\/community\/roadmap)  \n\nAdditional Source on TensorFlow Release: [medium\/tensorflow](https:\/\/medium.com\/@tensorflow)","2f01bee0":"**Compiling the model**  \nAs we build the network with convolution layers, now the next step is to compile the model with Loss function, Optimizer functions and Metrics.","d7678a4e":"**Version Check**  \nNow, Let us check the version of TensorFlow which is installed to get the confirmation as whether the cpu version of TensorFlow 2.0 alpha is installed correctly.  \n**Note:**  \nOne catch while importing Keras is that we need to import Keras from TensorFlow","18664bd3":"**Compiling the model**  \nCompiling the model involves declaring the Loss function, Optimizer function and Metrics to monitor the training & testing steps based on the problem.  \nIn our case, as we are trying the solve the multi label classification problem we will be using *'adam'* as *optimizer*, *'sparse_categorical_crossentropy'* as *loss* and *'accuracy'* as *metrics*.","ca996fa9":"**Time to Play with Images**  \nNow, let us try some thing on images. One of the famous dataset when we start with images in neural network is the Fashion MNIST dataset.  \nThe details about the Fashion MNIST dataset can be found in the below link:  \n[Fashion MNIST](https:\/\/github.com\/zalandoresearch\/fashion-mnist)","6fd9f60b":"So, the model correctly predicts the correct class 9 which is \"Ankle boot\".","c0f17742":"Parameter \"padding\" is given as \"same\" to retain the original image data.","1a22060e":"**Evaluating the model**  \nNow, let us evaluate our model and see how good our model is able to caputre the underlying information on the data using CCN.","2f8ed5d1":"**Building the Network using CNN**   \nThis time we are going to build the network with CNN layers to capture the maximum information the image data.","96e5b06e":"**Defining the Network**  \nIn the latest version of TensorFlow with Keras, Sequential function is used to build the network and Dense function is used to build the layers of the network.","56eb2d30":"**Getting the Data for CNN based model**  \nFor training the CNN model, the training and testing data needs to reshaped to fit into the network.","8d49156c":"**Training the Network**  \nNow, the time has come to train the network. Training the network is where the network learns the relationship between the input and the output. During the training with multiple iteration the network tries to make a guess and measures the loss with respect to correct know values from the guess. With the help of optimizer at each guess the loss function tries to reduce the loss to its least value.  \n\nBy using the fit command the network can be trained.","fdd88b1a":"**Exploring the Datasets**   \nWe will be using 60K images for training and 10K images for testing. Let us have a look as how the data is in pixiel values.","0ae99ecb":"**Training the model**  \nNow, it's time to train the network with the training data so that the network learns to get the maximum information out of the training data.","b9ed7f11":"**Installation of TensorFlow 2.0 alpha version**  \nI have used the pip command to install the TensorFlow 2.0 alpha version. ","cf966bba":"**Compiling the Network**  \nOnce the network is created, the next step is to compile the network. Compiling the network involves defining two functions which are \"Loss Function\" and \"Optimizing Function\".  \n\n*Loss Function* - Loss function measures the guessed answers against the known correct answers and measures how well or how badly it did.  \n*Optimizing Function* - Optimizing functions helps the loss function to reduce the loss at every iteration of training the network so that the loss is reduced to its least value.\n\nIn our case, we are defining the loss functions as \"mean_squared_error\" as we are going to work on regression problem and the optimizing functions as \"sgd\" which is basically \"Stochastic gradient descent\".  \n\n*Details on Loss Functions:*  \n[Loss Functions](https:\/\/isaacchanghau.github.io\/post\/loss_functions\/)  \n*Details on Optimizing Functions*  \n[Optimizing Functions](https:\/\/towardsdatascience.com\/types-of-optimization-algorithms-used-in-neural-networks-and-ways-to-optimize-gradient-95ae5d39529f)  \n*Additional Resource*  \n[Loss & Optimizers](https:\/\/medium.com\/data-science-group-iitr\/loss-functions-and-optimization-algorithms-demystified-bb92daff331c)","99df7658":"**Objective**  \nThe main objective of this notebook is to explore the new features of TensorFlow 2.0 and see how Keras play a vital role inside the TensorFlow framework as Keras is very popular with respect to ease of use when compared to Tensorflow.","aa80f3f1":"The final epochs gives the training accuracy of 86% considering we have trained on only 10 epochs. Let us put our model to test.","aaefc359":"The model predicted for the class 9 for the first image of the test dataset which is \"Ankle boot\". Let us see as what is the origianl label for that image in the test dataset.","7fbc23f8":"**Time for Prediction**  \nLet us see as how well our model gets with the prediction.","e0202388":"**Stay Tuned!!!! Will Update!!!**","5151d1fd":"**Testing the Model**  \nNow its the exam time for the model to show us how much it has learnt and gained knowledge on the data. Let us evauate the model based on the test dataset.","92b7334e":"**Time to build the model**  \nNow, we have the training and testing datasets ready we will start to build the model.  \n*Layers*  are the basic building blocks of the neural networks. We will be using Sequential and Layers functions to build the network.","2c98b101":"**Data Preprocessing - Normalizing the input**  \nNormalizing the input between 0 and 1 by dividing all the input values by the maximum pixel value which is 255.","81a74527":"**Time to bring CNN**  \nConvolution Neural Network is basically referred as CNN. A Convolutional Neural Network (ConvNet\/CNN) is a Deep Learning algorithm which can take in an input image, assign importance (learnable weights and biases) to various aspects\/objects in the image and be able to differentiate one from the other.  \n\n\n**Key terms in CNN**\n*   *Convolution*  \n     Convolution is the first layer to extract features from an input image. Convolution preserves the relationship between pixels by learning image features using small squares of input data. It is a mathematical operation that takes two inputs such as image matrix and a filter or kernal.\n*   *Kernel \/ Filter*  \n    A filter (or kernel) is an integral component of the layered architecture.\n\n    Generally, it refers to an operator applied to the entirety of the image such that it transforms the information encoded in the pixels. In practice, however, a kernel is a smaller-sized matrix in comparison to the input dimensions of the image, that consists of real valued entries.\n\n    The kernels are then convolved with the input volume to obtain so-called \u2018activation maps\u2019. Activation maps indicate \u2018activated\u2019 regions, i.e. regions where features specific to the kernel have been detected in the input. The real values of the kernel matrix change with each learning iteration over the training set, indicating that the network is learning to identify which regions are of significance for extracting features from the data.\n    \n*  *Stride*  \n    Stride is the number of pixels shifts over the input matrix. When the stride is 1 then we move the filters to 1 pixel at a time. When the stride is 2 then we move the filters to 2 pixels at a time and so on. The below figure shows convolution would work with a stride of 2.\n    \n*  *Pooling*  \n    The pooling layer is usually placed after the Convolutional layer. Its primary utility lies in reducing the spatial dimensions (Width x Height) of the Input Volume for the next Convolutional Layer. It does not affect the depth dimension of the Volume.  \n\n  The operation performed by this layer is also called \u2018down-sampling\u2019, as the reduction of size leads to loss of information as well. However, such a loss is beneficial for the network for two reasons:\n\n    1. The decrease in size leads to less computational overhead for the upcoming layers of the network.\n    2. It work against over-fitting.  \n    \n    \n**Additional information on CNN:**  \n[Understanding CNN](https:\/\/medium.com\/@RaghavPrabhu\/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148)  \n[CNN Illustration](https:\/\/blog.xrds.acm.org\/2016\/06\/convolutional-neural-networks-cnns-illustrated-explanation\/)  \n[CNN Guide](https:\/\/medium.freecodecamp.org\/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050)  \n[Begineers guide to CNN](https:\/\/adeshpande3.github.io\/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks\/)  ","b5afc18f":"Now we have created a neural network with one input layer which can take one input at a time connected to the next layer which is the output layer which has one neuron.\nHere, the layers are constructed as \"Dense\" layers which basically means that neurons in one layer are completely connected to all the neurons in the next layer irrespetive of the number of neurons."}}