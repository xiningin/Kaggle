{"cell_type":{"fd0d6adc":"code","1c62836e":"code","84788bfd":"code","e473bbe7":"code","f85157c9":"code","a9dc0ce8":"code","f96a5752":"code","a23260d6":"code","5c7218f5":"code","e5d2f2e1":"code","febd6e00":"code","b823b2f8":"code","469cc59b":"code","030ed97b":"code","8b5e8fbd":"code","52bfdec2":"code","de70b57f":"code","4f66a0c7":"code","1ec252e0":"code","58455f66":"code","76a18bce":"code","040157a0":"code","7abe495a":"code","0415191e":"code","03fe1b46":"code","510f1667":"code","f10e75eb":"code","11e93c3f":"code","392124a0":"code","bfac979f":"code","f7327aae":"code","18c53008":"code","34f4c092":"markdown","08b56950":"markdown","8604ba3d":"markdown","7f6729e7":"markdown","cb1f9506":"markdown","e5f929ad":"markdown","aff07361":"markdown","a0e232ee":"markdown"},"source":{"fd0d6adc":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport tensorflow as tf \nimport pathlib\nfrom sklearn.metrics import confusion_matrix ,accuracy_score","1c62836e":"data_apth = \"..\/input\/water-potability\/water_potability.csv\"\n\ndata = pd.read_csv(data_apth)","84788bfd":"data.head()","e473bbe7":"print(\"The number of examples: {}\".format(len(data)))","f85157c9":"data.isnull().sum()","a9dc0ce8":"print(data.info())","f96a5752":"# plotting the output to see if the data is bananced or not.\n# I will use accuracy and confusion matrix to evaluat the results \nprint(data['Potability'].value_counts())\nprint(\"\\n\")\nprint(\"{:.2F}% of the data is from the Not potable class and {:.2F}% is from the potable class\".format(1998\/len(data) * 100,\n                                                                                                       1278\/len(data) * 100))\ndata['Potability'].plot(kind='hist')","a23260d6":"data['Potability'] = data['Potability'].astype('category')\ndata.info()","5c7218f5":"#There are no duplicated rows\ndata.duplicated().sum()","e5d2f2e1":"data.fillna(value=data.median(), inplace=True)","febd6e00":"data.isnull().sum()","b823b2f8":"examples = data.drop('Potability', axis=1)\ntarget = data['Potability']","469cc59b":"x_train_full, x_test,  y_train_full, y_test = train_test_split(examples,\n                                                               target,\n                                                               shuffle=True, test_size=0.2)\n\nx_train, x_val, y_train, y_val = train_test_split(x_train_full,\n                                                  y_train_full,\n                                                  shuffle=True, test_size=0.2)\n\nprint(\"Training data shape: {}\".format(x_train.shape))\nprint(\"Validation data shape: {}\".format(x_val.shape))\nprint(\"Testing data shape: {}\".format(x_test.shape))","030ed97b":"def preprocessing(data, label):\n    \n    \"\"\"function that takes the data and labels, \n    standarize the data and return the standarized\n    data and the labels\"\"\"\n    \n    mean = tf.reduce_mean(data, axis=0)\n    std = tf.math.reduce_std(data, axis=0)\n    \n    preprocessed_data = (data - mean) \/ std\n    \n    return preprocessed_data, label","8b5e8fbd":"def get_batches(x, y, buffer_size, batch_size, shuffle=False):\n    \n    #Building a pipeline from a data that exists in memory\n    data_ds = tf.data.Dataset.from_tensor_slices((x, y))\n    #mapping the fuction we alredy made to the data\n    preprocessed_data = data_ds.map(preprocessing)\n    \n    \n    if shuffle:\n        # if we want to shuffle the dataset given a buffer size\n        shuffled_data = preprocessed_data.shuffle(buffer_size)\n        data_batches = shuffled_data.batch(batch_size).prefetch(1)\n        \n    else: \n        \n        data_batches = preprocessed_data.batch(batch_size).prefetch(1)\n    \n    \n    return data_batches","52bfdec2":"training_batches = get_batches(x_train, y_train, 1024, 64, shuffle=True)\nvalidation_batches = get_batches(x_val, y_val, 256, 64, shuffle=False)\ntesting_batches = get_batches(x_test, y_test, 256, 64, shuffle=False)","de70b57f":"# learning rate scheduling\ndef exponential_decay(lr_0, s):\n    def exponential_decay_func(epoch):\n        return lr_0 * 0.1 ** (epoch \/ s)\n    return exponential_decay_func\n\nexponential_decay_func = exponential_decay(lr_0=0.01, s=20)\n\nlr_scheduler = tf.keras.callbacks.LearningRateScheduler(exponential_decay_func)","4f66a0c7":"def leaky_relu(x):\n    return tf.maximum(0.01*x, x)","1ec252e0":"model_input = tf.keras.Input(shape=(9,))\n\nx = tf.keras.layers.Dense(256,\n                          kernel_regularizer=tf.keras.regularizers.L2(),\n                          kernel_initializer='he_normal')(model_input)\n\nx = tf.keras.layers.Lambda(leaky_relu)(x)\n\nx = tf.keras.layers.BatchNormalization()(x)\n\nx = tf.keras.layers.Dense(128,\n                          kernel_regularizer=tf.keras.regularizers.L2(),\n                          kernel_initializer='he_normal')(x)\n\nx = tf.keras.layers.Lambda(leaky_relu)(x)\n\nx = tf.keras.layers.BatchNormalization()(x)\n\nmodel_output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = tf.keras.models.Model(inputs=model_input, outputs=model_output)","58455f66":"model.compile(loss='binary_crossentropy',\n             optimizer=tf.keras.optimizers.RMSprop(),\n             metrics=['acc'])\n\nmodel.fit(training_batches, \n         epochs=500, \n         validation_data=validation_batches,\n         callbacks=[lr_scheduler])","76a18bce":"model.evaluate(testing_batches)","040157a0":"def from_continuous_to_category(datafrmae, num_bins):\n    \n    features = list(datafrmae.columns)\n    features_list = []\n    \n    for num, column in enumerate(features):\n        feature = datafrmae[column]\n        \n        bins = np.linspace(feature.min(), feature.max(), num_bins)\n        which_bin = np.digitize(feature, bins=bins)\n        \n        encoder = OneHotEncoder(sparse=False)\n        encoder.fit(which_bin.reshape(-1, 1))\n        x_binned = encoder.transform(which_bin.reshape(-1, 1))\n        \n        df = pd.DataFrame(x_binned)\n        features_list.append(df)\n        \n    return pd.concat(features_list, axis=1)","7abe495a":"x_train_binned = from_continuous_to_category(x_train, 5)\nx_val_binned = from_continuous_to_category(x_val, 5)\nx_test_binned = from_continuous_to_category(x_test, 5)","0415191e":"model = tf.keras.models.Sequential()\n\nmodel.add(tf.keras.layers.Dense(256, activation='relu', input_shape=(x_train_binned.shape[1],),\n                               kernel_regularizer=tf.keras.regularizers.L2()))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Dense(128, activation='relu',\n                               kernel_regularizer=tf.keras.regularizers.L2()))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n             optimizer='rmsprop',\n             metrics=['acc'])\n\n\nmodel.fit(x_train_binned,  y_train,\n         epochs=10, \n         validation_data=(x_val_binned, y_val))","03fe1b46":"model.evaluate(x_test_binned, y_test)","510f1667":"from sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.pipeline import Pipeline","f10e75eb":"pipe = Pipeline([('poly', PolynomialFeatures()),\n               ('scaling', StandardScaler())])\n\npipe.fit(x_train)\n\nx_train_ready = pipe.transform(x_train)\nx_val_ready = pipe.transform(x_val)\nx_test_ready = pipe.transform(x_test)","11e93c3f":"class stop_training(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if (logs.get('val_acc')>0.69):\n            print(\"\\nReached 69% accuracy so canceling training!\")\n            self.model.stop_training = True\n\nmy_callbacks = stop_training()\n\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss',\n                                                  patience=5,\n                                                  restore_best_weights=True)","392124a0":"model = tf.keras.models.Sequential()\n\n        \nmodel.add(tf.keras.layers.Dense(512, activation='relu',\n                               kernel_regularizer=tf.keras.regularizers.L2()))\nmodel.add(tf.keras.layers.BatchNormalization())\n          \nmodel.add(tf.keras.layers.Dense(256, activation='relu',\n                               kernel_regularizer=tf.keras.regularizers.L2()))\nmodel.add(tf.keras.layers.BatchNormalization())\n          \nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))","bfac979f":"model.compile(loss='binary_crossentropy',\n             optimizer='rmsprop',\n             metrics=['acc'])","f7327aae":"model.fit(x_train_ready,  y_train,\n         epochs=5, \n         validation_data=(x_val_ready, y_val),\n         callbacks=[early_stopping, my_callbacks])","18c53008":"model.evaluate(x_test_ready, y_test)","34f4c092":"## 5- Creating and training a model","08b56950":"## Hndling missing values","8604ba3d":"## Binning Data","7f6729e7":"## 1- loading the dataset","cb1f9506":"## sklearn scalling, PolynomialFeatures","e5f929ad":"## 4-  Input pipeline","aff07361":"## 3- Split the dataset ","a0e232ee":"## 2- summarizing and cleaning the data "}}