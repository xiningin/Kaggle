{"cell_type":{"dd15f4dd":"code","8a299a3c":"code","1978070f":"code","702f092f":"code","0dbd372b":"code","b19ba12f":"code","a81dd9a3":"code","64bd8175":"code","010fb13a":"code","3471db3d":"code","bb0307f8":"code","269b8ea6":"code","6de431a7":"code","db56ed60":"code","e0827f85":"code","1ed51da4":"code","29b1387d":"code","29569e20":"code","200becbd":"code","bfec82aa":"markdown","1557cd08":"markdown","55e8de5f":"markdown"},"source":{"dd15f4dd":"# Installing bottleneck transformer library\n!pip install -q bottleneck-transformer-pytorch","8a299a3c":"# Software library written for data manipulation and analysis.\nimport pandas as pd\n\n# Python library to interact with the file system.\nimport os\n\n# Python library for image augmentation\nimport albumentations as A\n\n# fastai library for computer vision tasks\nfrom fastai.vision.all import *\n\n# Developing and training neural network based deep learning models.\nimport torch\nfrom torch import nn\nfrom torchvision.models import resnet101\n\n# BotNet\nfrom bottleneck_transformer_pytorch import BottleStack","1978070f":"# Define path to dataset, whose benefit is that this sample is more balanced than original train data.\npath = Path('..\/input\/hpa-cell-tiles-sample-balanced-dataset')","702f092f":"df = pd.read_csv(path\/'cell_df.csv')\ndf.head()","0dbd372b":"# extract the the total number of target labels\nlabels = [str(i) for i in range(19)]\nfor x in labels: df[x] = df['image_labels'].apply(lambda r: int(x in r.split('|')))","b19ba12f":"# Here a sample of the dataset has been taken, change frac to 1 to train the entire dataset!\ndfs = df.sample(frac=0.1, random_state=42)\ndfs = dfs.reset_index(drop=True)\nlen(dfs)","a81dd9a3":"# obtain the input images.\ndef get_x(r): \n    return path\/'cells'\/(r['image_id']+'_'+str(r['cell_id'])+'.jpg')\n\n# obtain the targets.\ndef get_y(r): \n    return r['image_labels'].split('|')","64bd8175":"'''AlbumentationsTransform will perform different transforms over both\n   the training and validation datasets ''' \nclass AlbumentationsTransform(RandTransform):\n    \n    '''split_idx is None, which allows for us to say when we're setting our split_idx.\n       We set an order to 2 which means any resize operations are done first before our new transform. '''\n    split_idx, order = None, 2\n    \n    def __init__(self, train_aug, valid_aug): store_attr()\n    \n    # Inherit from RandTransform, allows for us to set that split_idx in our before_call.\n    def before_call(self, b, split_idx):\n        self.idx = split_idx\n    \n    # If split_idx is 0, run the trainining augmentation, otherwise run the validation augmentation. \n    def encodes(self, img: PILImage):\n        if self.idx == 0:\n            aug_img = self.train_aug(image=np.array(img))['image']\n        else:\n            aug_img = self.valid_aug(image=np.array(img))['image']\n        return PILImage.create(aug_img)","010fb13a":"def get_train_aug(size): \n    \n    return A.Compose([\n            # allows to combine RandomCrop and RandomScale\n            A.RandomResizedCrop(size,size),\n            \n            # Transpose the input by swapping rows and columns.\n            A.Transpose(p=0.5),\n        \n            # Flip the input horizontally around the y-axis.\n            A.HorizontalFlip(p=0.5),\n        \n            # Flip the input horizontally around the x-axis.\n            A.VerticalFlip(p=0.5),\n        \n            # Randomly apply affine transforms: translate, scale and rotate the input.\n            A.ShiftScaleRotate(p=0.5),\n        \n            # Randomly change hue, saturation and value of the input image.\n            A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n        \n            # Randomly change brightness and contrast of the input image.\n            A.RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n        \n            # CoarseDropout of the rectangular regions in the image.\n            A.CoarseDropout(p=0.5),\n        \n            # CoarseDropout of the square regions in the image.\n            A.Cutout(p=0.5) ])\n\ndef get_valid_aug(size): \n    \n    return A.Compose([\n    # Crop the central part of the input.   \n    A.CenterCrop(size, size, p=1.),\n    \n    # Resize the input to the given height and width.    \n    A.Resize(size,size)], p=1.)","3471db3d":"'''The first step item_tfms resizes all the images to the same size (this happens on the CPU) \n   and then batch_tfms happens on the GPU for the entire batch of images. '''\n# Transforms we need to do for each image in the dataset\nitem_tfms = [Resize(224), AlbumentationsTransform(get_train_aug(224), get_valid_aug(224))]\n\n# Transforms that can take place on a batch of images\nbatch_tfms = [Normalize.from_stats(*imagenet_stats)]\n\nbs=6","bb0307f8":"dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock(vocab=labels)), # multi-label target\n                splitter=RandomSplitter(seed=42), # split data into training and validation subsets.\n                get_x=get_x, # obtain the input images.\n                get_y=get_y,  # obtain the targets.\n                item_tfms=item_tfms,\n                batch_tfms=batch_tfms\n                )\n\ndls = dblock.dataloaders(dfs, bs=bs)","269b8ea6":"# We can call show_batch() to see what a sample of a batch looks like.\ndls.show_batch()","6de431a7":"layer = BottleStack(\n    # channels in\n    dim = 256, \n    \n    # feature map size\n    fmap_size = 56,  \n    \n    # channels out\n    dim_out = 2048, \n    \n    # projection factor\n    proj_factor = 4,\n    \n    # downsample on first layer or not\n    downsample = True, \n    \n    # number of heads\n    heads = 4, \n    \n    # dimension per head, defaults to 128\n    dim_head = 128,    \n    \n    # use relative positional embedding - uses absolute if False\n    rel_pos_emb = False, \n    \n    # activation throughout the network\n    activation = nn.ReLU()  \n)","db56ed60":"# define the backbone architecture\nresnet = resnet101()\n\n# extract the backbone layers\nbackbone = list(resnet.children())","e0827f85":"# define the model architecture for BotNet\nmodel = nn.Sequential(*backbone[:5],\n                      layer,\n                      nn.AdaptiveAvgPool2d((1, 1)),\n                      nn.Flatten(1),\n                      nn.Linear(2048, 19))","1ed51da4":"# Group together some dls, a model, and metrics to handle training\nlearn = Learner(dls, model, metrics= accuracy_multi)","29b1387d":"# Choosing a good learning rate\nlearn.lr_find()","29569e20":"# We can use the fine_tune function to train a model with this given learning rate\nlearn.fine_tune(4,0.0008317637839354575)","200becbd":"# Plot training and validation losses.\nlearn.recorder.plot_loss()","bfec82aa":"# Model Definition","1557cd08":"# Fetch the required libraries","55e8de5f":"# Data Preprocessing"}}