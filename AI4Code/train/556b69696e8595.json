{"cell_type":{"fc32e5fa":"code","22c7cd7e":"code","3c102fad":"code","39033704":"code","107ec61c":"code","3e3c741b":"code","e6b60fc9":"code","a66042be":"code","a10db38d":"code","06c34065":"code","91474f72":"code","cf3b2380":"code","ba035e2b":"markdown","9843256d":"markdown","a2d98e1f":"markdown","9675f360":"markdown","365c1921":"markdown"},"source":{"fc32e5fa":"!pip install --no-index --find-links \/kaggle\/input\/pytorchtabnet\/pytorch_tabnet-2.0.0-py3-none-any.whl pytorch-tabnet\n!pip install \/kaggle\/input\/iterative-stratification\/iterative-stratification-master\/","22c7cd7e":"import torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.model_selection import StratifiedKFold\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nimport numpy as np\nimport pandas as pd \n\nimport os\nimport random\nimport sys\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\nfrom tqdm import tqdm\nfrom sklearn.metrics import log_loss\n","3c102fad":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        \nseed_everything(42)","39033704":"data_path = \"..\/input\/lish-moa\/\"\ntrain = pd.read_csv(data_path+'train_features.csv')\ntrain.drop(columns=[\"sig_id\"], inplace=True)\n\ntrain_targets = pd.read_csv(data_path+'train_targets_scored.csv')\ntrain_targets.drop(columns=[\"sig_id\"], inplace=True)\n\ntest = pd.read_csv(data_path+'test_features.csv')\ntest.drop(columns=[\"sig_id\"], inplace=True)\n\nsubmission = pd.read_csv(data_path+'sample_submission.csv')\n\nremove_vehicle = True\nif remove_vehicle:\n    kept_index = train['cp_type']=='trt_cp'\n    train = train.loc[kept_index].reset_index(drop=True)\n    train_targets = train_targets.loc[kept_index].reset_index(drop=True)\n\ntrain[\"cp_type\"] = (train[\"cp_type\"]==\"trt_cp\") + 0\ntrain[\"cp_dose\"] = (train[\"cp_dose\"]==\"D1\") + 0\n\ntest[\"cp_type\"] = (test[\"cp_type\"]==\"trt_cp\") + 0\ntest[\"cp_dose\"] = (test[\"cp_dose\"]==\"D1\") + 0\n\nX_test = test.values","107ec61c":"from iterstrat.ml_stratifiers import MultilabelStratifiedKFold, MultilabelStratifiedShuffleSplit\nfrom sklearn.metrics import roc_auc_score","3e3c741b":"MAX_EPOCH=200\ntabnet_params = dict(n_d=24, n_a=24, n_steps=1, gamma=1.3,\n                     lambda_sparse=0, optimizer_fn=torch.optim.Adam,\n                     optimizer_params=dict(lr=2e-2, weight_decay=1e-5),\n                     mask_type='entmax',\n                     scheduler_params=dict(mode=\"min\",\n                                           patience=5,\n                                           min_lr=1e-5,\n                                           factor=0.9,),\n                     scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n                     verbose=10,\n                     )","e6b60fc9":"from sklearn.metrics import log_loss\nfrom pytorch_tabnet.metrics import Metric\nfrom sklearn.metrics import roc_auc_score, log_loss\n\nclass LogitsLogLoss(Metric):\n    def __init__(self):\n        self._name = \"logits_ll\"\n        self._maximize = False\n\n    def __call__(self, y_true, y_pred):        \n        logits = 1 \/ (1 + np.exp(-y_pred))\n        ll = (1-y_true)*np.log(1-logits + 1e-15) + y_true*np.log(logits + 1e-15)\n        return np.mean(-ll)\n\nclass PartialLogitsLogLoss(Metric):\n    def __init__(self):\n        self._name = \"logits_ll(partial)\"\n        self._maximize = False\n\n    def __call__(self, y_true, y_pred):\n        y_true = y_true\n        y_pred = y_pred\n        \n        logits = 1 \/ (1 + np.exp(-y_pred))\n        logloss = (1-y_true)*np.log(1-logits + 1e-15) + y_true*np.log(logits + 1e-15)\n        return np.mean(-logloss)","a66042be":"use_cols = [col for col in train_targets.columns if train_targets[col].mean() > 0.01]","a10db38d":"scores_auc_all= []\ntest_cv_preds = []\n\nNB_SPLITS = 10\nmskf = MultilabelStratifiedKFold(n_splits=NB_SPLITS, random_state=0, shuffle=True)\noof_preds = []\noof_targets = []\nscores = []\nscores_auc = []\nfor fold_nb, (train_idx, val_idx) in enumerate(mskf.split(train, train_targets)):\n    print(\"FOLDS : \", fold_nb)\n\n    ## model\n    X_train, y_train = train.values[train_idx, :], train_targets[use_cols].values[train_idx, :]\n    X_val, y_val = train.values[val_idx, :], train_targets[use_cols].values[val_idx, :]\n    model = TabNetRegressor(**tabnet_params)\n\n    model.fit(X_train=X_train,\n              y_train=y_train,\n              eval_set=[(X_val, y_val)],\n              eval_name = [\"val\"],\n              eval_metric = ['logits_ll', 'logits_ll(partial)'],\n              max_epochs=MAX_EPOCH,\n              patience=20, batch_size=1024, virtual_batch_size=128,\n              num_workers=1, drop_last=False,\n              # use binary cross entropy as this is not a regression problem\n              loss_fn=torch.nn.functional.binary_cross_entropy_with_logits)\n\n    preds_val = model.predict(X_val)\n    # Apply sigmoid to the predictions\n    preds =  1 \/ (1 + np.exp(-preds_val))\n    score = np.min(model.history[\"val_logits_ll(partial)\"])\n#     name = cfg.save_name + f\"_fold{fold_nb}\"\n#     model.save_model(name)\n    ## save oof to compute the CV later\n    oof_preds.append(preds_val)\n    oof_targets.append(y_val)\n    scores.append(score)\n\n    # preds on test\n    preds_test = model.predict(X_test)\n    test_cv_preds.append(1 \/ (1 + np.exp(-preds_test)))\n\noof_preds_all = np.concatenate(oof_preds)\noof_targets_all = np.concatenate(oof_targets)\ntest_preds_all = np.stack(test_cv_preds)","06c34065":"aucs = []\nfor task_id in range(oof_preds_all.shape[1]):\n    aucs.append(roc_auc_score(y_true=oof_targets_all[:, task_id],\n                              y_score=oof_preds_all[:, task_id]))\nprint(f\"Overall AUC : {np.mean(aucs)}\")\nprint(f\"Average CV : {np.mean(scores)}\")","91474f72":"all_feat = [col for col in submission.columns if col not in [\"sig_id\"]]\nother = [col for col in all_feat if col not in use_cols]\nsubmission[use_cols] = test_preds_all.mean(axis=0)\nsubmission[other] = train_targets[other].mean().values\n# set control to 0\nsubmission.loc[test['cp_type']==0, submission.columns[1:]] = 0\nsubmission.to_csv('submission.csv', index=None)","cf3b2380":"submission","ba035e2b":"# Model parameters \n\nHappy tuning! ;)","9843256d":"# Data and minimal preprocessing","a2d98e1f":"# Submission","9675f360":"# Define custom metric for valdidation","365c1921":"This is a fork of this notebook: [https:\/\/www.kaggle.com\/optimo\/tabnetregressor-2-0-train-infer](https:\/\/www.kaggle.com\/optimo\/tabnetregressor-2-0-train-infer)"}}