{"cell_type":{"4299a31a":"code","d1cc1dee":"code","9fd603f4":"code","c77aabc5":"code","ed191448":"code","8a4ae40b":"code","36643d4e":"code","323ecd55":"code","b2cbdd9c":"code","3ac29a65":"code","d0edaa6e":"code","f8f890f7":"code","88aaafb0":"code","ec2f3f22":"code","16a10bed":"code","0f15f0ab":"code","6de43ad6":"code","45a7d5ed":"code","a9807641":"code","b0b46989":"code","e2c5ff66":"code","bf72c65b":"code","4431d7db":"code","c32da558":"code","b0eaccb8":"code","cc7a4eae":"code","68d19659":"code","5fe2fe63":"code","bad4f8e8":"code","133cc664":"code","6d2aa5de":"code","32c1f177":"code","1db0ad21":"code","dec70290":"code","99b386d0":"code","899652c1":"code","a96dbd66":"code","e1aa3bee":"code","9f336101":"code","30c2e5bf":"code","0bfb8d76":"code","94e12fc5":"code","f681b0f9":"code","54c4927b":"code","6e89c52c":"code","0bb9fd45":"code","08fc0162":"code","c72fe60e":"code","7729966a":"code","2179fbdf":"code","7d8f515e":"code","1423c829":"code","5336dff5":"code","ca0071cb":"code","2dc57626":"code","019e0a1a":"code","de0d215d":"code","5a180c85":"markdown","7ab4b269":"markdown","739f074d":"markdown","8fa783e7":"markdown","d672d4aa":"markdown","43b7f062":"markdown","3c5e65fc":"markdown","16df7e5d":"markdown","c007df5b":"markdown","26cd6de6":"markdown","59072182":"markdown","1071fa42":"markdown","6e51b628":"markdown","5fb3ddf3":"markdown","ce269871":"markdown","983783e1":"markdown"},"source":{"4299a31a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d1cc1dee":"# import modules \n\nimport os\nimport calendar\nimport numpy as np \nimport networkx as nx\nimport pandas as pd\nfrom pandas.plotting import scatter_matrix, parallel_coordinates\nimport seaborn as sns\nfrom sklearn import preprocessing \nimport matplotlib.pylab as plt","9fd603f4":"# Purpose of this notebook is finding some key factors that decide housing price","c77aabc5":"houseprice_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\nhouseprice_df.shape\nprint(houseprice_df)","ed191448":"test= pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntest.head()","8a4ae40b":"# Descriptive statistics\n\nprint('Number of rows', len(houseprice_df['MSSubClass']))\nprint('Mean of MSSubClass', houseprice_df['MSSubClass'].mean())\nhouseprice_df.describe()","36643d4e":"# Sampling\nhouseprice_df.sample(10)\n\n# Oversampling\nweights = [0.9 if years>2008 else 0.01 for years in houseprice_df.YearBuilt]\nhouseprice_df.sample(10, weights=weights)","323ecd55":"# Search for all of columns\nhouseprice_df.columns","b2cbdd9c":"# heatmap of correlations \ncorr = houseprice_df.corr()\nsns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, vmin=-1, vmax=1, cmap=\"RdBu\")\n\n## Include information about values (example demonstrates how to control the size of the plot)\nfig, ax = plt.subplots()\nfig.set_size_inches(20, 15)\nsns.heatmap(corr, annot=True, fmt=\".1f\", cmap=\"RdBu\", center=0, ax=ax)","3ac29a65":"# Histogram of SalePrice\nax = houseprice_df.SalePrice.hist()\nax.set_xlabel('SalePrice'); ax.set_ylabel('count')","d0edaa6e":"# Analyze high-correlation factors in detail\n\nsaleprice_ts = pd.Series(houseprice_df.SalePrice.values, index=houseprice_df.YearBuilt)\nhouseprice_df.plot.scatter(x='OverallQual',y='SalePrice', legend=False)\nhouseprice_df.plot.scatter(x='YearBuilt',y='SalePrice', legend=False)\nhouseprice_df.plot.scatter(x='GrLivArea',y='SalePrice', legend=False)\nhouseprice_df.plot.scatter(x='GarageCars',y='SalePrice', legend=False)\nhouseprice_df.plot.scatter(x='OverallCond',y='SalePrice', legend=False)","f8f890f7":"#box plot overallqual\/saleprice\n## Reference_notebook by  Pedro Marcello, 2017\n\nvar = 'OverallQual'\ndata = pd.concat([houseprice_df['SalePrice'], houseprice_df[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000)","88aaafb0":"# Covariance Matrix among key factors\ndf=pd.DataFrame(houseprice_df, columns=['OverallQual', 'YearBuilt', 'GrLivArea','GarageCars'])\ncovMatrix = pd.DataFrame.cov(df).round(1)\ncovMatrix","ec2f3f22":"# barchart of mean SalePrice \nhouseprice_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf=pd.DataFrame(houseprice_df)\n\n## compute mean SalePrice per MSZoning (C,FV,RH,RL,RM)\nax = houseprice_df.groupby('MSZoning').mean().SalePrice.plot(kind='bar')\nax.set_ylabel('Avg. SalePrice')","16a10bed":"## compute mean SalePrice per BldgType (1Fam,2fmCon,Duplex,Twins,TwnhsE)\n\nax = houseprice_df.groupby('BldgType').mean().SalePrice.plot(kind='bar')\nax.set_ylabel('Avg. SalePrice')","0f15f0ab":"## compute mean SalePrice per BldgType (Ex, Fa, Gd, Po, TA)\n\nax = houseprice_df.groupby('GarageCond').mean().SalePrice.plot(kind='bar')\nax.set_ylabel('Avg. SalePrice')","6de43ad6":"# group by MSZoning and Building Condition\n\nhouseprice_df.groupby(['MSZoning', 'BldgType'])['SalePrice'].mean()","45a7d5ed":"# Analyzed by group (MSZoning - GarageCond - SalePrice)\nhouseprice_df.groupby(['MSZoning', 'GarageCond'])['SalePrice'].mean()","a9807641":"# Analyzed by group (BldgType - GarageCond - SalePrice)\nhouseprice_df.groupby(['BldgType', 'GarageCond'])['SalePrice'].mean()","b0b46989":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nfrom sklearn.linear_model import LinearRegression\nfrom scipy.stats import norm\nfrom scipy import stats\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n# SalePrice according to the neighborhood\nax = houseprice_df.plot.scatter(x='OverallQual', y='SalePrice', figsize=(20, 15))\npoints = houseprice_df[['OverallQual','SalePrice','Neighborhood']]\n_ = points.apply(lambda x: \n             ax.text(*x, rotation=20, horizontalalignment='left',\n                     verticalalignment='bottom', fontsize=9), axis=1)","e2c5ff66":"# the number of data each neighborhood condition has\nhouseprice_df.Neighborhood.value_counts()","bf72c65b":"# Neighborhood Option between OverallQual 1 ~ 3\nhouseprice_df['Qual_bin'] = pd.cut(houseprice_df.OverallQual, range(0, 4), \n   labels=False)\nhouseprice_df.groupby(['Qual_bin', 'Neighborhood'])['SalePrice'].mean()","4431d7db":"# Neighborhood Option between Overall Qual 4 ~ 6\nhouseprice_df['Qual_bin2'] = pd.cut(houseprice_df.OverallQual, range(3, 7), \n   labels=False)\nhouseprice_df.groupby(['Qual_bin2', 'Neighborhood'])['SalePrice'].mean()","c32da558":"# Neighborhood Option between OverallQual 8 ~ 10\nhouseprice_df['Qual_bin3'] = pd.cut(houseprice_df.OverallQual, range(7, 11), \n   labels=False)\nhouseprice_df.groupby(['Qual_bin3', 'Neighborhood'])['SalePrice'].mean()","b0eaccb8":"# in order to remove rows with missing values \nreduced_df = houseprice_df.dropna()\nprint('Number of rows after removing rows with missing values: ', houseprice_df['GarageCars'].count())","cc7a4eae":"# Prior to linear regression, sort out numeric data (it's because infinite or NaN data cannot be recognized in Python)\n\ndf=pd.DataFrame(houseprice_df, columns=['OverallQual','YearBuilt', 'YearRemodAdd', 'MasVnrArea','TotalBsmtSF', \n                                        '1stFlrSF', 'GrLivArea','TotRmsAbvGrd', 'Fireplaces',\n                                        'GarageYrBlt','GarageCars', 'GarageArea','SalePrice'])\ndf.isna()","68d19659":"df.dropna(how='all').all","5fe2fe63":"df[~df.isin([np.nan, np.inf, -np.inf]).any(1)].astype(np.float64)","bad4f8e8":"pip install dmba","133cc664":"import matplotlib.pylab as plt\nfrom dmba import regressionSummary, classificationSummary, liftChart, gainsChart\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\n\ndf=df.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n\n# create list of predictors and outcome\nexcludeColumns = ('SalePrice')\npredictors = [s for s in df.columns if s not in excludeColumns]\noutcome = 'SalePrice'\n                  \n# partition data\nX = df[predictors]\ny = df[outcome]\ntrain_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.4, random_state=1)\nmodel = LinearRegression()\nmodel.fit(train_X, train_y)\ntrain_pred = model.predict(train_X)\ntrain_results = pd.DataFrame({'SalePrice':train_y, 'predicted':train_pred, 'residual':train_y - train_pred})\ntrain_results.head() ","6d2aa5de":"# partition data\nX = df[predictors]\ny = df[outcome]\ntrain_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.4, random_state=1)\nmodel = LinearRegression()\nmodel.fit(train_X, train_y)\ntrain_pred = model.predict(train_X)\ntrain_results = pd.DataFrame({'SalePrice':train_y, 'predicted':train_pred, 'residual':train_y - train_pred})\ntrain_results.head() \n\nmodel = LinearRegression()\nmodel.fit(train_X, train_y)\n\ntrain_pred = model.predict(train_X)\ntrain_results = pd.DataFrame({'TOTAL_VALUE': train_y, 'predicted': train_pred, 'residual': train_y - train_pred})\n\n# show sample of predictions\ntrain_results.head()","32c1f177":"valid_pred = model.predict(valid_X)\nvalid_results = pd.DataFrame({'SalePrice': valid_y, 'predicted': valid_pred, 'residual': valid_y - valid_pred})\nvalid_results.head()","1db0ad21":"# import the utility function regressionSummary\nfrom dmba import regressionSummary\n\nX = df[predictors]\ny = df[outcome]\ntrain_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.4, random_state=1)\nmodel = LinearRegression()\nmodel.fit(train_X, train_y)\ntrain_pred = model.predict(train_X)\ntrain_results = pd.DataFrame({'SalePrice':train_y, 'predicted':train_pred, 'residual':train_y - train_pred})\ntrain_results.head() \n\n\n# training set\nregressionSummary(train_results.SalePrice, train_results.predicted)\n\n# validation set\nregressionSummary(valid_results.SalePrice, valid_results.predicted)","dec70290":"pip install pydotplus","99b386d0":"import math\nfrom pathlib import Path\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom dmba import plotDecisionTree, classificationSummary, regressionSummary\nfrom sklearn.metrics import accuracy_score, roc_curve, auc\nimport pydotplus","899652c1":"print(pd.DataFrame({'Predictor':X.columns,'coefficient':model.coef_}))","a96dbd66":"valid_pred = model.predict(valid_X)\nresult=pd.DataFrame({'Predicted':valid_pred, 'Actual':valid_y, 'Residual':valid_y-valid_pred})\nprint(result.head(20))","e1aa3bee":"pred_error_train = pd.DataFrame({\n    'residual': train_y - model.predict(train_X), \n    'data set': 'training'\n})\npred_error_valid = pd.DataFrame({\n    'residual': valid_y - model.predict(valid_X), \n    'data set': 'validation'\n})\nboxdata_df = pred_error_train.append(pred_error_valid, ignore_index=True)\n\nfig, axes = plt.subplots(nrows=1, ncols=3)\nfig.set_size_inches(9, 4)\ncommon = {'bins': 100, 'range': [-100000, 100000]}\npred_error_train.hist(ax=axes[0], **common)\npred_error_valid.hist(ax=axes[1], **common)\nboxdata_df.boxplot(ax=axes[2], by='data set')\n\naxes[0].set_title('training')\naxes[1].set_title('validation')\naxes[2].set_title(' ')\naxes[2].set_ylim(-100000, 100000)\nplt.suptitle('Prediction errors') \nplt.subplots_adjust(bottom=0.15, top=0.85, wspace=0.35)\n\nplt.show()","9f336101":"neighbortest_df=pd.DataFrame(houseprice_df, columns=['OverallQual','YearBuilt', 'YearRemodAdd', 'MasVnrArea','TotalBsmtSF', \n                                        '1stFlrSF', 'GrLivArea','TotRmsAbvGrd', 'SalePrice','Fireplaces',\n                                        'GarageYrBlt','GarageCars', 'GarageArea','Neighborhood'])\nneighbortest_df.set_index('Neighborhood', inplace=True)\n\n# apply scale function\nneighbortest_df = neighbortest_df.apply(lambda x: x.astype('float64'))\nneighbortest_df.head(20)","30c2e5bf":"%matplotlib inline\n\nfrom pathlib import Path\n\nimport pandas as pd\nfrom sklearn import preprocessing\nfrom sklearn.metrics import pairwise\nfrom scipy.cluster.hierarchy import dendrogram, linkage, fcluster\nfrom sklearn.cluster import KMeans\nimport matplotlib.pylab as plt\nimport seaborn as sns\nfrom pandas.plotting import parallel_coordinates\n\n# scikit-learn\nneighbortest_df_norm = neighbortest_df.apply(preprocessing.scale, axis=0)\n\n# pandas uses sample standard deviation\nneighbortest_df_norm = (neighbortest_df - neighbortest_df.mean())\/neighbortest_df.std()\n\n# compute normalized distance based on SalePrice and OverallQual\nd_norm = pairwise.pairwise_distances(neighbortest_df_norm[['SalePrice', 'OverallQual']], \n                                     metric='euclidean')\npd.DataFrame(d_norm, columns=neighbortest_df.index, index=neighbortest_df.index).head(20)","0bfb8d76":"neighbortest_df=neighbortest_df.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\nneighbortest_df_norm = neighbortest_df.apply(preprocessing.scale, axis=0)\nneighbortest_df_norm = (neighbortest_df - neighbortest_df.mean())\/neighbortest_df.std()\n\n\nZ = linkage(neighbortest_df_norm.head(60), method='average')\n\nfig = plt.figure(figsize=(20, 15))\nfig.subplots_adjust(bottom=0.23)\nplt.title('Hierarchical Clustering Dendrogram (Average linkage)')\nplt.xlabel('Neighborhood')\ndendrogram(Z, labels=neighbortest_df_norm.index, color_threshold=2.75, leaf_rotation=45, leaf_font_size=12)\nplt.axhline(y=2.75, color='black', linewidth=0.4, linestyle='dashed')\nplt.show()","94e12fc5":"memb = fcluster(linkage(neighbortest_df_norm, 'average'), 15, criterion='maxclust')\nmemb = pd.Series(memb, index=neighbortest_df_norm.index)\nfor key, item in memb.groupby(memb):\n    print(key, ': ', ', '.join(item.index))","f681b0f9":"neighbortest_df_norm.index = ['{}: {}'.format(cluster, Neighborhood) for cluster, Neighborhood\n                              in zip(memb, neighbortest_df_norm.index)]\nsns.clustermap(neighbortest_df_norm.head(60), method='average', col_cluster=False,  cmap=\"mako_r\")\nplt.show()","54c4927b":"kmeans = KMeans(n_clusters=6, random_state=0).fit(neighbortest_df_norm)\ncentroids = pd.DataFrame(kmeans.cluster_centers_, columns=neighbortest_df_norm.columns)\npd.set_option('precision', 3)\nprint(centroids)\npd.set_option('precision', 6)","6e89c52c":"withinClusterSS = [0] * 6\nclusterCount = [0] * 6\nfor cluster, distance in zip(kmeans.labels_, kmeans.transform(neighbortest_df_norm)):\n    withinClusterSS[cluster] += distance[cluster]**2\n    clusterCount[cluster] += 1\nfor cluster, withClustSS in enumerate(withinClusterSS):\n    print('Cluster {} ({} members): {:5.2f} within cluster'.format(cluster, \n        clusterCount[cluster], withinClusterSS[cluster]))","0bb9fd45":"# calculate the distances of each data point to the cluster centers\ndistances = kmeans.transform(neighbortest_df_norm)\n\n# reduce to the minimum squared distance of each data point to the cluster centers\nminSquaredDistances = distances.min(axis=1) ** 2\n\n# combine with cluster labels into a data frame\ndf = pd.DataFrame({'squaredDistance': minSquaredDistances, 'cluster': kmeans.labels_}, \n    index=neighbortest_df_norm.index)\n\n# Group by cluster and print information\nfor cluster, data in df.groupby('cluster'):\n    count = len(data)\n    withinClustSS = data.squaredDistance.sum()\n    print(f'Cluster {cluster} ({count} members): {withinClustSS:.2f} within cluster ')","08fc0162":"centroids['cluster'] = ['Cluster {}'.format(i) for i in centroids.index]\n\nplt.figure(figsize=(10,7))\nfig.subplots_adjust(right=3)\nax = parallel_coordinates(centroids, class_column='cluster', colormap='Dark2', linewidth=4)\nplt.legend(loc='center left', bbox_to_anchor=(0.95, 0.5))\nplt.xlim(-0.5,7.5)\ncentroids","c72fe60e":"centroids","7729966a":"centroids_df=centroids\ncentrodis_df=centroids.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n\n# create list of predictors and outcome\nexcludeColumns = ('SalePrice', 'cluster')\npredictors = [s for s in centroids_df.columns if s not in excludeColumns]\noutcome = 'SalePrice'\n                  \n\nX2 = centroids_df[predictors]\ny2 = centroids_df[outcome]\ntrain_X2, valid_X2, train_y2, valid_y2 = train_test_split(X2, y2, test_size=0.3, random_state=1)\nmodel2 = LinearRegression()\nmodel2.fit(train_X2, train_y2)\ntrain2_pred = model2.predict(train_X2)\ntrain2_results = pd.DataFrame({'SalePrice':train_y2, 'predicted':train2_pred, 'residual':train_y2 - train2_pred})\ntrain2_results.head() ","2179fbdf":"valid2_pred = model2.predict(valid_X2)\nvalid2_results = pd.DataFrame({'SalePrice': valid_y2, 'predicted': valid2_pred, 'residual': valid_y2 - valid2_pred})\nvalid2_results.head()","7d8f515e":"# training set\nregressionSummary(train2_results.SalePrice, train2_results.predicted)\n\n# validation set\nregressionSummary(valid2_results.SalePrice, valid2_results.predicted)","1423c829":"centroids_df['cluster'] = centroids_df.cluster.apply(lambda x: x.split('r')[-1])\n\nprint(centroids_df)","5336dff5":"from sklearn.cluster import AgglomerativeClustering\n\ndf2 = pd.DataFrame(centroids_df)\nmodel2 = AgglomerativeClustering(n_clusters=6)\n\nmodel2.fit(df2)\n\ny2_predict = model2.fit_predict(df2)\nprint(y2_predict) \n\ndf2['cluster'] = y2_predict\nprint(df2)","ca0071cb":"test_df=pd.DataFrame(test, columns=['OverallQual','YearBuilt', 'YearRemodAdd', 'MasVnrArea','TotalBsmtSF', \n                                        '1stFlrSF', 'GrLivArea','TotRmsAbvGrd','Fireplaces',\n                                        'GarageYrBlt','GarageCars', 'GarageArea'])\ntest_df=test_df.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n\ny3_predict=model2.fit_predict(test_df)\nsubmission = y3_predict","2dc57626":"print(submission)","019e0a1a":"submission=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')","de0d215d":"submission.to_csv('First_Kaggle_Submission.csv')","5a180c85":"From this, we are able to know that speicific neighborhood requirements are only existed in high overall quality. Also, those high-grade neighborhood conditions lead to high price.","7ab4b269":"We know key factors superficially, but cannot assure the correctness. Thus, we should verify the correctness of our estimation. For this verification, I utilize 'linear regression model'.","739f074d":"At first, make data frame named 'neighbortest_df' as I make 'df' in previous step (linear regression).\nPlus, use 'Neighborhood' column as an index.","8fa783e7":"Those string factors are quite influential to SalePrice, but I feel insufficiency of the level of correlation. According to 'Korea Association of Property Appraisers', location requirements is one of the most influential constituents deciding housing price. Therefore, I think that 'Neighborhood' can be the key factor among string data information.","d672d4aa":"Now, we know which factors have high correlation with SalePrice","43b7f062":"# **Cluster Analysis**","3c5e65fc":"# **String Data Analysis**\n","16df7e5d":"It is difficult to clearly classify and make links among neighborhoods factor, since the number of data is too big to deal with handily. (1460rows)","c007df5b":"# Firtstly, bring data from csv file\n","26cd6de6":"As a result of this normalized distance chart, the distance between each neighborhood followed the difference in overall quality. For example, neighborhood group,{NoRidge, NridgHt, Somerst, etc}, which shows less distance toward each other belongs to the group of 'OverallQual 10'","59072182":"Fortunately, this chart shows us the correlation between 'neighborhood' and other numeric factors.\nAs we see, 'StoneBr', 'NoRidge', 'NridgHt', which belongs to high quality group has strong-positive relationship with SalePrice.","1071fa42":"**Numeric Data Analysis**","6e51b628":"After dropping inappropriate value, we finally make train and valid data set.\nThen, we can assess whether the previous etimation is valid or not.\nIn this part, I try to show the train model and valid model respectively.","5fb3ddf3":"# **Linear Regression Model**","ce269871":"Throught the linear regression model, We can identify that our estimation is valid.\nHowever, as you see in graphs above, there are still a few errors. In my opinion, the reason is that\nstring data were not used in this linear regression model, despite their importance and impact.\nAccordingly, it is more accurate approach to consider the string data.\nIn order to do this, I make another data frame and utilize it with Cluster analysis.","983783e1":"Until now, we analyze numeric data. However, I guess there would be another key factor that is not expressed in numbers"}}