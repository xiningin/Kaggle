{"cell_type":{"16506532":"code","6f9c0b78":"code","43a56226":"code","423a01e7":"code","2ae7d7e3":"code","30b8d684":"code","14dcd788":"code","d29c2d82":"code","c41b90c3":"code","da39f8b9":"code","8ab28bf1":"code","3ecc5662":"code","3cced0fe":"code","c3bf0948":"code","fcd8a3c6":"code","e16673d0":"code","3d685027":"code","3a98051e":"code","0733773b":"code","148ceb1c":"code","30fb0928":"code","babea8e2":"code","6b3b337b":"code","d8f7a6b1":"code","dfe0f42a":"code","d4595146":"code","b63178b0":"code","8f98607a":"code","6d35619a":"code","cabca290":"code","17a544e5":"code","c30059d3":"code","60fdd74e":"code","2043f877":"code","dafec3be":"code","d152f975":"code","fb21ad66":"code","ddd33031":"code","2c6e0d4f":"code","e55b838d":"code","c0e09553":"code","c7401c7c":"code","3edd2034":"code","dffd29f8":"code","02388743":"markdown","6b1e91a1":"markdown","e1a8acf1":"markdown","1f899e0c":"markdown","2f97ed89":"markdown","aee9e440":"markdown","3bae414e":"markdown","ce537d42":"markdown","8c29ba65":"markdown","8db3b410":"markdown","8dd98c0a":"markdown","f44c7b4a":"markdown","6f91a01a":"markdown","8c4fe890":"markdown","4e93b1bb":"markdown","5619bde6":"markdown","7d78569d":"markdown","945fbc4a":"markdown"},"source":{"16506532":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport warnings\nfrom sklearn.metrics import r2_score , mean_absolute_error, mean_squared_error , median_absolute_error\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline","6f9c0b78":"# We start by reading the data\ndf = pd.read_csv('..\/input\/autotel-shared-car-locations\/sample_table.csv')\ndf.sample(5)","43a56226":"df.describe()","423a01e7":"df = df[df['total_cars'] > 0]","2ae7d7e3":"df_cars_by_time = df.groupby('timestamp').agg({'total_cars':'sum'}).reset_index()\ndf_cars_by_time.sample(5)","30b8d684":"df_cars_by_time['timestamp'] = df_cars_by_time['timestamp'].apply(pd.Timestamp)\ndf_cars_by_time.set_index('timestamp').sort_index().rolling('60min').mean().plot(figsize=(20,6), c='salmon', lw=1.6)\nplt.grid()\nplt.show()","14dcd788":"df_cars_by_time['usage_rate'] = (260 - df_cars_by_time['total_cars']) \/ 260\ndf_cars_by_time.set_index('timestamp').sort_index()['usage_rate'].rolling('60min').mean().plot(figsize=(20,6), c='mediumslateblue', lw=1.6)\nplt.grid()\nplt.show()","d29c2d82":"df_cars_by_time['usage_rate'] = (260 - df_cars_by_time['total_cars']) \/ 260\ndf_cars_by_time.set_index('timestamp').sort_index()['usage_rate'].rolling('3D').mean().plot(figsize=(20,6), c='navy', lw=1.6)\nplt.grid()\nplt.show()","c41b90c3":"# Convert timezone\ntimestamps = pd.DatetimeIndex(df_cars_by_time['timestamp'])\ntimestamps = timestamps.tz_convert('Asia\/Jerusalem')\n\ndf_cars_by_time['local_time'] = timestamps\n\n#Extract time features\ndf_cars_by_time['weekday'] = df_cars_by_time['local_time'].dt.weekday_name\ndf_cars_by_time['hour'] = df_cars_by_time['local_time'].dt.hour","da39f8b9":"df_cars_by_time.head() # Looks right!","8ab28bf1":"plt.figure(figsize=(20,6))\nplt.subplot(121)\nsns.barplot(x='hour', y='total_cars', data=df_cars_by_time)\nplt.subplot(122)\nsns.boxplot(x='weekday', y='total_cars', data=df_cars_by_time, showfliers=False)\n\nplt.show()","3ecc5662":"df_cars_by_time['usage_rate'] = (260 - df_cars_by_time['total_cars']) \/ 260","3cced0fe":"plt.figure(figsize=(20,6))\nplt.subplot(121)\nsns.barplot(x='hour', y='usage_rate', data=df_cars_by_time)\nplt.title('Cars usage rate by hour of day')\nplt.subplot(122)\nsns.boxplot(x='weekday', y='usage_rate', data=df_cars_by_time, showfliers=False)\nplt.title('Cars usage rate by day of week')\n\nplt.show()","c3bf0948":"import folium\nfrom folium.plugins import HeatMap","fcd8a3c6":"df_locations = df.groupby(['latitude', 'longitude', 'timestamp']).sum().reset_index().sample(1500)\ndf_locations.head()","e16673d0":"m = folium.Map([df_locations.latitude.mean(), df_locations.longitude.mean()], zoom_start=11)\nfor index, row in df_locations.iterrows():\n    folium.CircleMarker([row['latitude'], row['longitude']],\n                        radius=row['total_cars'] * 6,\n                        fill_color=\"#3db7e4\", \n                       ).add_to(m)\n    \npoints = df_locations[['latitude', 'longitude']].as_matrix()\nm.add_children(HeatMap(points, radius=15)) # plot heatmap\n\nm","3d685027":"from shapely.geometry import Point, Polygon\nfrom shapely import wkt","3a98051e":"df_neighborhood = pd.read_csv('..\/input\/tel-aviv-neighborhood-polygons\/tel_aviv_neighborhood.csv')\ndf_neighborhood.head()","0733773b":"def load_and_close_polygon(wkt_text):\n    poly = wkt.loads(wkt_text)\n    point_list = poly.exterior.coords[:]\n    point_list.append(point_list[0])\n    \n    return Polygon(point_list)","148ceb1c":"# Lets transform the WKS's to Polygon Objects and save it to a GeoPandas DataFrame\ndf_neighborhood['polygon'] = df_neighborhood['area_polygon'].apply(load_and_close_polygon)\nneighborhood_map = df_neighborhood.set_index('neighborhood_name')['polygon'].to_dict()","30fb0928":"sample_df = df.sample(10000)\nsample_df['points'] = sample_df.apply(lambda row : Point([row['longitude'], row['latitude']]), axis=1)\nsample_df.head()","babea8e2":"poly_idxs = sample_df['points'].apply(lambda point : np.argmax([point.within(polygon) for polygon in list(neighborhood_map.values())]))\npoly_idxs = poly_idxs.apply(lambda x: list(neighborhood_map.keys())[x])\nsample_df['neighborhood'] = poly_idxs.values\nsample_df.head()","6b3b337b":"plt.figure(figsize=(20,7))\nsns.barplot(x='neighborhood', y='total_cars', data=sample_df.groupby('neighborhood').count().reset_index())\nplt.xticks(rotation=45)\nplt.show()","d8f7a6b1":"import lightgbm as lgb","dfe0f42a":"df_sample = df.copy()","d4595146":"df_timestamps = pd.DataFrame()\ndf_timestamps['timestamp'] = df_sample.timestamp.drop_duplicates()\ntimestamps = pd.DatetimeIndex(df_timestamps['timestamp']).tz_localize('UTC')\ndf_timestamps['local_time'] = timestamps.tz_convert('Asia\/Jerusalem')","b63178b0":"df_sample = df_sample.merge(df_timestamps, on='timestamp', how='left')","8f98607a":"# Again no reason to calculate on duplicate points, it's very expensive!\ndf_points = df_sample[['longitude','latitude']].drop_duplicates()\ndf_points['points'] = df_points.apply(lambda row : Point([row['longitude'], row['latitude']]), axis=1)\npoly_idxs = df_points['points'].apply(lambda point : np.argmax([point.within(polygon) for polygon in list(neighborhood_map.values())]))\npoly_idxs = poly_idxs.apply(lambda x: list(neighborhood_map.keys())[x])\ndf_points['neighborhood'] = poly_idxs.values","6d35619a":"df_sample = df_sample.merge(df_points[['longitude', 'latitude', 'neighborhood']], on=['longitude', 'latitude'], how='left')","cabca290":"df_sample['time_in_seconds'] = pd.to_datetime(df_sample['local_time']).values.astype(np.int64) \/\/ 10**6\n\nseconds_in_day = 24 * 60 * 60\nseconds_in_week = 7 * seconds_in_day\n\ndf_sample['sin_time_day'] = np.sin(2*np.pi*df_sample['time_in_seconds']\/seconds_in_day)\ndf_sample['cos_time_day'] = np.cos(2*np.pi*df_sample['time_in_seconds']\/seconds_in_day)\n\ndf_sample['sin_time_week'] = np.sin(2*np.pi*df_sample['time_in_seconds']\/seconds_in_week)\ndf_sample['cos_time_week'] = np.cos(2*np.pi*df_sample['time_in_seconds']\/seconds_in_week)\n\ndf_sample['weekday'] = df_sample['local_time'].dt.weekday\ndf_sample['hour'] = df_sample['local_time'].dt.hour\n\ndf_sample.sample(5)","17a544e5":"df_sample[['longitude', 'latitude', 'neighborhood']].groupby('neighborhood').nunique()","c30059d3":"df_sample[\"LL2\"] = df_sample['longitude'].round(2).astype(str)+ df_sample['latitude'].round(2).astype(str)","60fdd74e":"df_sample[['LL2', 'neighborhood']].groupby('neighborhood').nunique()","2043f877":"## Sde Dov looks weird. Let's check for odd values in latlongs , based on frequency\n#### TODO \n\n# df_sample[['longitude', 'latitude', 'neighborhood']]['longitude'].value_counts()","dafec3be":"aggs = {}\naggs['total_cars'] = 'sum'\naggs['sin_time_day'] = 'mean'\naggs['cos_time_day'] = 'mean'\naggs['sin_time_week'] = 'mean'\naggs['cos_time_week'] = 'mean'\naggs['weekday'] = 'first'\naggs['hour'] = 'first'\n# Mode is problematic with agg\naggs['latitude'] =   'first' # pd.Series.mode()#lambda x: x.mode #pd.Series.mode()#\naggs['longitude'] =  'first' # pd.Series.mode() #lambda x: x.mode #pd.Series.mode() # 'first'\n\n# 30 minute resample\ndf_sample = df_sample.set_index('local_time').groupby([pd.Grouper(freq='1800s'), 'neighborhood']).agg(aggs).reset_index()\n\n# df_sample.set_index('local_time').groupby([pd.Grouper(freq='60s'), 'neighborhood']).agg(aggs).reset_index().tail()\n\nprint(df_sample.shape)\n\ndf_sample.sample(6)","d152f975":"df_sample.to_csv(\"autoTel_30m_Neighborhoods_v1.csv.gz\",index=False,compression=\"gzip\")","fb21ad66":"df_sample['neighborhood'] = df_sample['neighborhood'].astype('category')\ndf_sample['weekday'] = df_sample['weekday'].astype('category')\ndf_sample['hour'] = df_sample['hour'].astype('category')","ddd33031":"df_train = df_sample[df_sample['local_time'] < '2019-01-04']\ndf_test = df_sample[df_sample['local_time'] >= '2019-01-04']\n\nprint('train_shape: ', df_train.shape)\nprint('test_shape: ', df_test.shape)\n\n# df_train.to_csv(\"autoTel_train_30m_Neighborhoods.csv.gz\",index=False,compression=\"gzip\")\n# df_test.to_csv(\"autoTel_test_30m_Neighborhoods.csv.gz\",index=False,compression=\"gzip\")","2c6e0d4f":"features = ['neighborhood', 'sin_time_day', 'cos_time_day', 'sin_time_week', 'cos_time_week', 'weekday', 'hour']\ntarget = 'total_cars'","e55b838d":"gbm = lgb.LGBMRegressor(num_leaves=31,\n                        learning_rate=0.05,\n                        n_estimators=250)","c0e09553":"gbm.fit(df_train[features], df_train[target],\n        eval_set=[(df_test[features], df_test[target])],\n        eval_metric='mse',\n        early_stopping_rounds=5,\n      )","c7401c7c":"df_test['prediction'] = gbm.predict(df_test[features])","3edd2034":"print(\"evaluation:\")\nprint(\"r2_score:\",r2_score(y_true=df_test['total_cars'], y_pred=df_test['prediction']))\nprint(\"mean_absolute_error:\",mean_absolute_error(y_true=df_test['total_cars'], y_pred=df_test['prediction']))\nprint(\"mean_squared_error:\",mean_squared_error(y_true=df_test['total_cars'], y_pred=df_test['prediction']))\nprint(\"median_absolute_error:\",median_absolute_error(y_true=df_test['total_cars'], y_pred=df_test['prediction']))\n","dffd29f8":"df_test.plot(kind='scatter', x='total_cars', y='prediction', lw=0, s=0.4, figsize=(20,6))\nplt.show()","02388743":"## Predicting the number of available cars per neighborhood ","6b1e91a1":"In order to analyze the data by time we need to consider another issu. We notice is the time zone, Tel Aviv is not in UTC timezome, making all analysis shifted. Let's shift back!","e1a8acf1":"Lets Start by loading the data","1f899e0c":"we peovide additional data of neighborhood polygons of the city of Tel Aviv. This data will enable us to group the data by neighborhood and later predict the number of available cars per neighborhood!\n* Disclaimer: Kaggle Kernels do not support GeoPandas, so I implemented my own geo Joins wich are not efficient. Hopefully they will add it soon!","2f97ed89":"## Analyzing Usage Patterns by Time","aee9e440":"* We can see that tge max available cars is 260, so we can assume that this is the total number of cars available in AutoTel.\n* By assuming this we will calculate the usage rate.","3bae414e":"* We see that looking at the \"parking lot\" level would mean roughly Tripling + the amount of rows\/samples in our data to predict on. This might be a bit too much, although it would be more relevant for the level of taking action, i.e \"where are there missing cars + a demand for cars\"\n","ce537d42":"### Does our data  contain multiple parking spots per Neighborhood?\n* i.e multiple rounded latLongs per neighborhood? \n* We may want to roundup the latlongs , in case reporting comes from the car level, rather than the parking spot(s)","8c29ba65":"# AutoTel Shared Cars Availability\n* This notebook explores the AutoTel Dataset and aggregates it for demand forecasting prediction. \n* The dataset contains points of parked cars collected from [autotel](https:\/\/www.autotel.co.il\/en\/).\n* A [blog post](https:\/\/blog.doit-intl.com\/using-bigquery-ml-to-predict-car-share-availability-f929310ec662) on processing the data using BigQuery ML\n* [the original BigQuery dataset](https:\/\/bigquery.cloud.google.com\/table\/gad-playground-212407:doit_intl_autotel_public.car_locations)  updates frequently","8db3b410":"## Plotting the data on a map\nThis geographical data can be presented on a map, showing where people park the cars","8dd98c0a":"# Featurize for ML\n* Temporal split","f44c7b4a":"## Analyze usage patterns by day and hour","6f91a01a":"## Predicting Car Availability using LightGBM","8c4fe890":"Some of the rows indicate empty parking spots. These spots are reserved for AutoTel cars, but no car was parked there art the time. Since we are only interested in the cars location, we will filter these rows to save computation and memory","4e93b1bb":"Now lets imagine that we'd like to generate predictions to how cars will be distributed between neighborhoods in the city. So in this sample code we will try to use LightGBM to predict the number of available car in a neighborhood","5619bde6":"That's nice, the usage varies between 2.5% at night and 25% at peak times! now lets look at the trend, is the usage growing or decreasing over time?","7d78569d":"* Looks much better isn't it? during the night the usage drops to as much as 2.5%, while at peak times almost 20% of the cars are in use.\n* Weekdays in israel are Sunday to Thursday, the weekend is Friday and Saturday. This may explain why Thursday has the highest usage rate on average","945fbc4a":"## Back to merging + aggregation by Neighborhood: \n*  *Changed* : we will look at the hourly level, not minute level. (Could also do every half hour maybe?)\n\n* Alternative target: Per \"parking lot\" = by LatLong2 `LL2`"}}