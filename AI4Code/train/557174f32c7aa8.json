{"cell_type":{"e776b685":"code","0757d08f":"code","1e0095db":"code","cd8d051e":"code","daa8f9c4":"code","75c6c1be":"code","e4a4a5f9":"code","13c064c0":"code","e4ea9baa":"code","59e6e7b5":"code","e182eb54":"code","3aebfc11":"code","4630f83c":"code","290efd10":"code","9d4daf7e":"code","76bb2b3f":"code","4c8766b9":"code","23d8c279":"code","47375037":"code","d9fbfba3":"code","a0b0561e":"code","a102e4af":"code","fc04d34a":"code","c6b84d33":"code","2eb41ab4":"code","861c2c58":"code","08eb4e9f":"code","9b5ca778":"code","5f8fd710":"code","005455e3":"code","fc8bfa68":"code","b92ce006":"code","b45f0999":"markdown","85d1bcbc":"markdown","10b33feb":"markdown","b85c533c":"markdown","3057a360":"markdown","e2c7826d":"markdown","0f7249e9":"markdown","73ab5df1":"markdown","e5c1c3d8":"markdown","fc6a62fb":"markdown","c5e2ba0c":"markdown","dfa483a7":"markdown","04aa7f7a":"markdown","711d1a03":"markdown","be8a78eb":"markdown","76a5c47f":"markdown","2d414a06":"markdown","71ff3123":"markdown","74ea48d8":"markdown","ee8f4216":"markdown","90a1d31f":"markdown","7ab35505":"markdown","8f494de0":"markdown","0088f025":"markdown","72c69f06":"markdown","4af02f09":"markdown","7536cf6e":"markdown","87c53602":"markdown","f30e91b0":"markdown","449e8e34":"markdown","0507e5c9":"markdown","7a199dfb":"markdown","ce878481":"markdown","354c8621":"markdown","8a44691a":"markdown","cf6d15e7":"markdown","0fcf6252":"markdown","9f5018aa":"markdown"},"source":{"e776b685":"%matplotlib inline\nimport cv2\nimport os\nimport numpy as np\nimport keras\nimport matplotlib.pyplot as plt\n# import download\nfrom random import shuffle\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense, Activation\nimport sys\nimport h5py","0757d08f":"keras.__version__","1e0095db":"def print_progress(count, max_count):\n    # Percentage completion.\n    pct_complete = count \/ max_count\n\n    # Status-message. Note the \\r which means the line should\n    # overwrite itself.\n    msg = \"\\r- Progress: {0:.1%}\".format(pct_complete)\n\n    # Print it.\n    sys.stdout.write(msg)\n    sys.stdout.flush()","cd8d051e":"in_dir = \"..\/input\/hockey-fight-vidoes\/data\"","daa8f9c4":"# Frame size  \nimg_size = 224\n\nimg_size_touple = (img_size, img_size)\n\n# Number of channels (RGB)\nnum_channels = 3\n\n# Flat frame size\nimg_size_flat = img_size * img_size * num_channels\n\n# Number of classes for classification (Violence-No Violence)\nnum_classes = 2\n\n# Number of files to train\n_num_files_train = 1\n\n# Number of frames per video\n_images_per_file = 20\n\n# Number of frames per training set\n_num_images_train = _num_files_train * _images_per_file\n\n# Video extension\nvideo_exts = \".avi\"","75c6c1be":"def get_frames(current_dir, file_name):\n    \n    in_file = os.path.join(current_dir, file_name)\n    \n    images = []\n    \n    vidcap = cv2.VideoCapture(in_file)\n    \n    success,image = vidcap.read()\n        \n    count = 0\n\n    while count<_images_per_file:\n                \n        RGB_img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    \n        res = cv2.resize(RGB_img, dsize=(img_size, img_size),\n                                 interpolation=cv2.INTER_CUBIC)\n    \n        images.append(res)\n    \n        success,image = vidcap.read()\n    \n        count += 1\n        \n    resul = np.array(images)\n    \n    resul = (resul \/ 255.).astype(np.float16)\n        \n    return resul","e4a4a5f9":"def label_video_names(in_dir):\n    \n    # list containing video names\n    names = []\n    # list containin video labels [1, 0] if it has violence and [0, 1] if not\n    labels = []\n    \n    \n    for current_dir, dir_names,file_names in os.walk(in_dir):\n        \n        for file_name in file_names:\n            \n            if file_name[0:2] == 'fi':\n                labels.append([1,0])\n                names.append(file_name)\n            elif file_name[0:2] == 'no':\n                labels.append([0,1])\n                names.append(file_name)\n                     \n            \n    c = list(zip(names,labels))\n    # Suffle the data (names and labels)\n    shuffle(c)\n    \n    names, labels = zip(*c)\n            \n    return names, labels","13c064c0":"# First get the names and labels of the whole videos\nnames, labels = label_video_names(in_dir)","e4ea9baa":"names[12]","59e6e7b5":"frames = get_frames(in_dir, names[12])","e182eb54":"visible_frame = (frames*255).astype('uint8')","3aebfc11":"plt.imshow(visible_frame[3])","4630f83c":"plt.imshow(visible_frame[15])","290efd10":"image_model = VGG16(include_top=True, weights='imagenet')","9d4daf7e":"image_model.summary()\n","76bb2b3f":"# We will use the output of the layer prior to the final\n# classification-layer which is named fc2. This is a fully-connected (or dense) layer.\ntransfer_layer = image_model.get_layer('fc2')\n\nimage_model_transfer = Model(inputs=image_model.input,\n                             outputs=transfer_layer.output)\n\ntransfer_values_size = K.int_shape(transfer_layer.output)[1]\n\n\nprint(\"The input of the VGG16 net have dimensions:\",K.int_shape(image_model.input)[1:3])\n\nprint(\"The output of the selecter layer of VGG16 net have dimensions: \", transfer_values_size)","4c8766b9":"def get_transfer_values(current_dir, file_name):\n    \n    # Pre-allocate input-batch-array for images.\n    shape = (_images_per_file,) + img_size_touple + (3,)\n    \n    image_batch = np.zeros(shape=shape, dtype=np.float16)\n    \n    image_batch = get_frames(current_dir, file_name)\n      \n    # Pre-allocate output-array for transfer-values.\n    # Note that we use 16-bit floating-points to save memory.\n    shape = (_images_per_file, transfer_values_size)\n    transfer_values = np.zeros(shape=shape, dtype=np.float16)\n\n    transfer_values = \\\n            image_model_transfer.predict(image_batch)\n            \n    return transfer_values","23d8c279":"def proces_transfer(vid_names, in_dir, labels):\n    \n    count = 0\n    \n    tam = len(vid_names)\n    \n    # Pre-allocate input-batch-array for images.\n    shape = (_images_per_file,) + img_size_touple + (3,)\n    \n    while count<tam:\n        \n        video_name = vid_names[count]\n        \n        image_batch = np.zeros(shape=shape, dtype=np.float16)\n    \n        image_batch = get_frames(in_dir, video_name)\n        \n         # Note that we use 16-bit floating-points to save memory.\n        shape = (_images_per_file, transfer_values_size)\n        transfer_values = np.zeros(shape=shape, dtype=np.float16)\n        \n        transfer_values = \\\n            image_model_transfer.predict(image_batch)\n         \n        labels1 = labels[count]\n        \n        aux = np.ones([20,2])\n        \n        labelss = labels1*aux\n        \n        yield transfer_values, labelss\n        \n        count+=1","47375037":"def make_files(n_files):\n    \n    gen = proces_transfer(names_training, in_dir, labels_training)\n\n    numer = 1\n\n    # Read the first chunk to get the column dtypes\n    chunk = next(gen)\n\n    row_count = chunk[0].shape[0]\n    row_count2 = chunk[1].shape[0]\n    \n    with h5py.File('prueba.h5', 'w') as f:\n    \n        # Initialize a resizable dataset to hold the output\n        maxshape = (None,) + chunk[0].shape[1:]\n        maxshape2 = (None,) + chunk[1].shape[1:]\n    \n    \n        dset = f.create_dataset('data', shape=chunk[0].shape, maxshape=maxshape,\n                                chunks=chunk[0].shape, dtype=chunk[0].dtype)\n    \n        dset2 = f.create_dataset('labels', shape=chunk[1].shape, maxshape=maxshape2,\n                                 chunks=chunk[1].shape, dtype=chunk[1].dtype)\n    \n         # Write the first chunk of rows\n        dset[:] = chunk[0]\n        dset2[:] = chunk[1]\n\n        for chunk in gen:\n            \n            if numer == n_files:\n            \n                break\n\n            # Resize the dataset to accommodate the next chunk of rows\n            dset.resize(row_count + chunk[0].shape[0], axis=0)\n            dset2.resize(row_count2 + chunk[1].shape[0], axis=0)\n\n            # Write the next chunk\n            dset[row_count:] = chunk[0]\n            dset2[row_count:] = chunk[1]\n\n            # Increment the row count\n            row_count += chunk[0].shape[0]\n            row_count2 += chunk[1].shape[0]\n            \n            print_progress(numer, n_files)\n        \n            numer += 1","d9fbfba3":"def make_files_test(n_files):\n    \n    gen = proces_transfer(names_test, in_dir, labels_test)\n\n    numer = 1\n\n    # Read the first chunk to get the column dtypes\n    chunk = next(gen)\n\n    row_count = chunk[0].shape[0]\n    row_count2 = chunk[1].shape[0]\n    \n    with h5py.File('pruebavalidation.h5', 'w') as f:\n    \n        # Initialize a resizable dataset to hold the output\n        maxshape = (None,) + chunk[0].shape[1:]\n        maxshape2 = (None,) + chunk[1].shape[1:]\n    \n    \n        dset = f.create_dataset('data', shape=chunk[0].shape, maxshape=maxshape,\n                                chunks=chunk[0].shape, dtype=chunk[0].dtype)\n    \n        dset2 = f.create_dataset('labels', shape=chunk[1].shape, maxshape=maxshape2,\n                                 chunks=chunk[1].shape, dtype=chunk[1].dtype)\n    \n         # Write the first chunk of rows\n        dset[:] = chunk[0]\n        dset2[:] = chunk[1]\n\n        for chunk in gen:\n            \n            if numer == n_files:\n            \n                break\n\n            # Resize the dataset to accommodate the next chunk of rows\n            dset.resize(row_count + chunk[0].shape[0], axis=0)\n            dset2.resize(row_count2 + chunk[1].shape[0], axis=0)\n\n            # Write the next chunk\n            dset[row_count:] = chunk[0]\n            dset2[row_count:] = chunk[1]\n\n            # Increment the row count\n            row_count += chunk[0].shape[0]\n            row_count2 += chunk[1].shape[0]\n            \n            print_progress(numer, n_files)\n        \n            numer += 1","a0b0561e":"training_set = int(len(names)*0.8)\ntest_set = int(len(names)*0.2)\n\nnames_training = names[0:training_set]\nnames_test = names[training_set:]\n\nlabels_training = labels[0:training_set]\nlabels_test = labels[training_set:]","a102e4af":"make_files(training_set)","fc04d34a":"make_files_test(test_set)","c6b84d33":"def process_alldata_training():\n    \n    joint_transfer=[]\n    frames_num=20\n    count = 0\n    \n    with h5py.File('prueba.h5', 'r') as f:\n            \n        X_batch = f['data'][:]\n        y_batch = f['labels'][:]\n\n    for i in range(int(len(X_batch)\/frames_num)):\n        inc = count+frames_num\n        joint_transfer.append([X_batch[count:inc],y_batch[count]])\n        count =inc\n        \n    data =[]\n    target=[]\n    \n    for i in joint_transfer:\n        data.append(i[0])\n        target.append(np.array(i[1]))\n        \n    return data, target","2eb41ab4":"def process_alldata_test():\n    \n    joint_transfer=[]\n    frames_num=20\n    count = 0\n    \n    with h5py.File('pruebavalidation.h5', 'r') as f:\n            \n        X_batch = f['data'][:]\n        y_batch = f['labels'][:]\n\n    for i in range(int(len(X_batch)\/frames_num)):\n        inc = count+frames_num\n        joint_transfer.append([X_batch[count:inc],y_batch[count]])\n        count =inc\n        \n    data =[]\n    target=[]\n    \n    for i in joint_transfer:\n        data.append(i[0])\n        target.append(np.array(i[1]))\n        \n    return data, target","861c2c58":"data, target = process_alldata_training()","08eb4e9f":"data_test, target_test = process_alldata_test()","9b5ca778":"chunk_size = 4096\nn_chunks = 20\nrnn_size = 512\n\nmodel = Sequential()\nmodel.add(LSTM(rnn_size, input_shape=(n_chunks, chunk_size)))\nmodel.add(Dense(1024))\nmodel.add(Activation('relu'))\nmodel.add(Dense(50))\nmodel.add(Activation('sigmoid'))\nmodel.add(Dense(2))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='mean_squared_error', optimizer='adam',metrics=['accuracy'])","5f8fd710":"epoch = 200\nbatchS = 500\n\nhistory = model.fit(np.array(data[0:720]), np.array(target[0:720]), epochs=epoch,\n                    validation_data=(np.array(data[720:]), np.array(target[720:])), \n                    batch_size=batchS, verbose=2)","005455e3":"result = model.evaluate(np.array(data_test), np.array(target_test))","fc8bfa68":"for name, value in zip(model.metrics_names, result):\n    print(name, value)","b92ce006":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.savefig('destination_path.eps', format='eps', dpi=1000)\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.savefig('destination_path1.eps', format='eps', dpi=1000)\nplt.show()","b45f0999":"The video has violence, look at the name of the video, starts with 'fi'","85d1bcbc":"### Load the cached transfer values into memory\nWe have already saved all the videos transfer values into disk. But we have to load those transfer values into memory in order to train the LSTM net. One question would be: why not process transfer values and load them into RAM memory? Yes is a more eficient way to train the second net. But if you have to train the LSTM in different ways in order to see which way gets the best accuracy, if you didn't save the transfer values into disk you would have to process the whole videos each training. It's very time consuming processing the videos through VGG16 net. \n","10b33feb":"Today, the amount of public violence has increased dramatically. As much in high schools as in the street. This has resulted in the ubiquitous use of surveillance cameras. This has helped the authorities to identify these events and take the necessary measures. But almost all systems today require the human-inspection of these videos to identify such events, which is virtually inefficient. It is therefore necessary to have such a practical system that can automatically monitor and identify the surveillance videos.\nThe development of various deep learning techniques, thanks to the availability of large data sets and computational resources, has resulted in a historic change in the community of computer vision. Various techniques have been developed to address problems such as object detection, recognition, tracking, action recognition, legend generation, etc. However, despite recent developments in deep learning, very few techniques based on deep learning have been proposed to address the problem of detecting violence from videos.","b85c533c":"Then we are going to process all video frames through VGG16 and save the transfer values.","3057a360":"Copy some of the data-dimensions for convenience.","e2c7826d":"## Helper Functions","0f7249e9":"### Generator that process one video through VGG16 each function call","73ab5df1":"Convert back the frames to uint8 pixel format to plot the frame","e5c1c3d8":"### Split the dataset into training set and test set\nWe are going to split the dataset into training set and testing. The training set is used to train the model and the test set to check the model accuracy.","fc6a62fb":"### Plot a video frame to see if data is correct","c5e2ba0c":"The basic building block in a Recurrent Neural Network (RNN) is a Recurrent Unit (RU). There are many different variants of recurrent units such as the rather clunky LSTM (Long-Short-Term-Memory) and the somewhat simpler GRU (Gated Recurrent Unit) which we will use in this tutorial. Experiments in the literature suggest that the LSTM and GRU have roughly similar performance. Even simpler variants also exist and the literature suggests that they may perform even better than both LSTM and GRU, but they are not implemented in Keras which we will use in this tutorial.\n\nA recurrent neuron has an internal state that is being updated every time the unit receives a new input. This internal state serves as a kind of memory. However, it is not a traditional kind of computer memory which stores bits that are either on or off. Instead the recurrent unit stores floating-point values in its memory-state, which are read and written using matrix-operations so the operations are all differentiable. This means the memory-state can store arbitrary floating-point values (although typically limited between -1.0 and 1.0) and the network can be trained like a normal neural network using Gradient Descent.\n\n","dfa483a7":"Then we are going to load 20 frames of one video, for example","04aa7f7a":"## Pre-Trained Model: VGG16","711d1a03":"We can observe the shape of the tensors expected as input by the pre-trained VGG16 model. In this case it is images of shape 224 x 224 x 3. Note that we have defined the frame size as 224x224x3. The video frame will be the input of the VGG16 net.","be8a78eb":"Let's see the model summary","76a5c47f":"## Model training\n","2d414a06":"### Function to process 20 video frames through VGG16 and get transfer values","71ff3123":"## Load Data\n\nFirstly, we define the directory to place the video dataset","74ea48d8":"### Helper-function for getting video frames\nFunction used to get 20 frames from a video file and convert the frame to a suitable format for the neural net.","ee8f4216":"# Violence Detection using CNN + LSTM neural netowrk","90a1d31f":"We will use the function ```print_progress``` to print the amount of videos processed the datasets","7ab35505":"##Recurrent Neural Network","8f494de0":"## Test the model","0088f025":"We are going to test the model with 20 % of the total videos. This videos have not been used to train the network. ","72c69f06":"The first input dimension of LSTM neurons is the temporal dimension, in our case it is 20. The second is the size of the features vector (transfer values).\n","4af02f09":"## Imports","7536cf6e":"## Flowchart","87c53602":"### Define LSTM architecture","f30e91b0":"### Helper function to get the names of the data downloaded and label it","449e8e34":"In order to load the saved transfer values into RAM memory we are going to use this two functions:","0507e5c9":"When defining the LSTM architecture we have to take into account the dimensions of the transfer values. From each frame the VGG16 network obtains as output a vector of 4096 transfer values. From each video we are processing 20 frames so we will have 20 x 4096 values per video. The classification must be done taking into account the 20 frames of the video. If any of them detects violence, the video will be classified as violent.\n","7a199dfb":"### VGG16 model flowchart","ce878481":"## Print the model accuracy","354c8621":"The following creates an instance of the pre-trained VGG16 model using the Keras API. This automatically downloads the required files if you don't have them already.\n\nThe VGG16 model contains a convolutional part and a fully-connected (or dense) part which is used for classification. If include_top=True then the whole VGG16 model is downloaded which is about 528 MB. If include_top=False then only the convolutional part of the VGG16 model is downloaded which is just 57 MB.","8a44691a":"### Functions to save transfer values from VGG16 to later use\nWe are going to define functions to get the transfer values from VGG16 with defined number of files. Then save the transfer values files used from training in one file and the ones uses for testing in another one. ","cf6d15e7":"The following chart shows how the data flows when using the VGG16 model for Transfer Learning. First we input and process 20 video frames in batch with the VGG16 model. Just prior to the final classification layer of the VGG16 model, we save the so-called Transfer Values to a cache-file.\n\nThe reason for using a cache-file is that it takes a long time to process an image with the VGG16 model. If each image is processed more than once then we can save a lot of time by caching the transfer-values.\n\nWhen all the videos have been processed through the VGG16 model and the resulting transfer-values saved to a cache file, then we can use those transfer-values as the input to LSTM neural network. We will then train the second neural network using the classes from the violence dataset (Violence, No-Violence), so the network learns how to classify images based on the transfer-values from the VGG16 model.","0fcf6252":"## Introduction","9f5018aa":"The method consists of extracting a set of frames belonging to the video, sending them to a pretrained network called VGG16, obtaining the output of one of its final layers and from these outputs train another network architecture with a type of special neurons called LSTM. These neurons have memory and are able to analyze the temporal information of the video, if at any time they detect violence, it will be classified as a violent video.\n\n\n\n"}}