{"cell_type":{"d2a8e288":"code","4b658eb3":"code","01d69759":"code","6a7d9a74":"code","f6bf6aae":"code","9b44e4e2":"code","38e62166":"code","e472c6d5":"code","2bbfff6c":"code","7de30bc9":"code","757b55f5":"code","15d4bad3":"code","14252000":"code","ab2274b4":"code","a8b1be47":"code","db3c9699":"code","f0d72b8a":"code","f0827161":"code","8f32bd3c":"markdown","02ded760":"markdown","75f4b9e6":"markdown","c3c320a9":"markdown","f6aa7f40":"markdown","cdaee38b":"markdown","12ce9131":"markdown","42b9bba1":"markdown","bb4bde33":"markdown","9715e40e":"markdown","761d73b9":"markdown","3b83bac2":"markdown","f687ba04":"markdown","52b8cc94":"markdown","7032c764":"markdown","4d27a3f2":"markdown"},"source":{"d2a8e288":"# linear algebra\nimport numpy as np \n\n# data processing\nimport pandas as pd \n\n# data visualization\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\n\n# Algorithms\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","4b658eb3":"train_df=pd.read_csv(\"..\/input\/heart.csv\")","01d69759":"a = pd.get_dummies(train_df['cp'], prefix = \"cp\")\nb = pd.get_dummies(train_df['thal'], prefix = \"thal\")\nc = pd.get_dummies(train_df['slope'], prefix = \"slope\")","6a7d9a74":"frames = [train_df, a, b, c]\ntrain_df = pd.concat(frames, axis = 1)\ntrain_df = train_df.drop(columns = ['cp', 'thal', 'slope'])\ntrain_df.head()","f6bf6aae":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(train_df.drop('target',axis=1), \n                                                    train_df['target'], test_size=0.20, \n                                                    random_state=0)","9b44e4e2":"from sklearn.linear_model import LogisticRegression","38e62166":"# Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\n\nY_pred = logreg.predict(X_test)\n\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nprint(round(acc_log,2,), \"%\")","e472c6d5":"# stochastic gradient descent (SGD) learning\nsgd = linear_model.SGDClassifier(max_iter=4, tol=None)\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\n\nsgd.score(X_train, Y_train)\n\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\n\n\nprint(round(acc_sgd,2,), \"%\")","2bbfff6c":"# Random Forest\nrandom_forest = RandomForestClassifier(n_estimators=1)\nrandom_forest.fit(X_train, Y_train)\n\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nprint(round(acc_random_forest,2,), \"%\")","7de30bc9":"# KNN\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\n\nY_pred = knn.predict(X_test)\n\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nprint(round(acc_knn,2,), \"%\")","757b55f5":"# Gaussian Naive Bayes\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\n\nY_pred = gaussian.predict(X_test)\n\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nprint(round(acc_gaussian,2,), \"%\")","15d4bad3":"# Perceptron\nperceptron = Perceptron(max_iter=5)\nperceptron.fit(X_train, Y_train)\n\nY_pred = perceptron.predict(X_test)\n\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\nprint(round(acc_perceptron,2,), \"%\")","14252000":"# Linear SVC\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\n\nY_pred = linear_svc.predict(X_test)\n\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\nprint(round(acc_linear_svc,2,), \"%\")","ab2274b4":"# Decision Tree\ndecision_tree = DecisionTreeClassifier(criterion='gini',splitter='best',max_depth=3,min_samples_split=3,min_samples_leaf=1,\n                                       min_weight_fraction_leaf=0.0,max_features=None,random_state=None,max_leaf_nodes=None,\n                                       min_impurity_decrease=0.0,min_impurity_split=None)\ndecision_tree.fit(X_train, Y_train)\n\nY_pred = decision_tree.predict(X_test)\n\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nprint(round(acc_decision_tree,2,), \"%\")","a8b1be47":"results = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', \n              'Decision Tree'],\n    'Score': [acc_linear_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_decision_tree]})\nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Score')\nresult_df.head(9)","db3c9699":"# Predicted values\ny_lr = logreg.predict(X_test)\nknn3 = KNeighborsClassifier(n_neighbors = 3)\nknn3.fit(X_train, Y_train)\ny_knn = knn3.predict(X_test)\ny_svm = linear_svc.predict(X_test)\ny_nb = gaussian.predict(X_test)\ny_dtc = decision_tree.predict(X_test)\ny_rf = random_forest.predict(X_test)","f0d72b8a":"from sklearn.metrics import confusion_matrix\n\ncm_lr = confusion_matrix(Y_test,y_lr)\ncm_knn = confusion_matrix(Y_test,y_knn)\ncm_svm = confusion_matrix(Y_test,y_svm)\ncm_nb = confusion_matrix(Y_test,y_nb)\ncm_dtc = confusion_matrix(Y_test,y_dtc)\ncm_rf = confusion_matrix(Y_test,y_rf)","f0827161":"plt.figure(figsize=(24,12))\n\nplt.suptitle(\"Confusion Matrixes\",fontsize=24)\nplt.subplots_adjust(wspace = 0.4, hspace= 0.4)\n\nplt.subplot(2,3,1)\nplt.title(\"Logistic Regression Confusion Matrix\")\nsns.heatmap(cm_lr,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False)\n\nplt.subplot(2,3,2)\nplt.title(\"K Nearest Neighbors Confusion Matrix\")\nsns.heatmap(cm_knn,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False)\n\nplt.subplot(2,3,3)\nplt.title(\"Support Vector Machine Confusion Matrix\")\nsns.heatmap(cm_svm,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False)\n\nplt.subplot(2,3,4)\nplt.title(\"Naive Bayes Confusion Matrix\")\nsns.heatmap(cm_nb,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False)\n\nplt.subplot(2,3,5)\nplt.title(\"Decision Tree Classifier Confusion Matrix\")\nsns.heatmap(cm_dtc,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False)\n\nplt.subplot(2,3,6)\nplt.title(\"Random Forest Confusion Matrix\")\nsns.heatmap(cm_rf,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False)\n\nplt.show()","8f32bd3c":"# **Import Libraries**","02ded760":"# 6 Perceptron","75f4b9e6":"# About this file\nData Set Information:\n\nThis database contains 76 attributes, but all published experiments refer to using a subset of 14 of them. In particular, the Cleveland database is the only one that has been used by ML researchers to this date. The \"goal\" field refers to the presence of heart disease in the patient. It is integer valued from 0 (no presence) to 4. Experiments with the Cleveland database have concentrated on simply attempting to distinguish presence (values 1,2,3,4) from absence (value 0).\n\nThe names and social security numbers of the patients were recently removed from the database, replaced with dummy values.\n\nOne file has been \"processed\", that one containing the Cleveland database. All four unprocessed files also exist in this directory.\n\nTo see Test Costs (donated by Peter Turney), please see the folder \"Costs\"\n\nAttribute Information:\n\nOnly 14 attributes used: \n- 1. (age) : age in years\n\n- 2. (sex) : (1 = male; 0 = female)\n\n- 3. (cp) : chest pain type chest pain type \n\n     -- Value 1: typical angina\n     \n     -- Value 2: atypical angina \n     \n     -- Value 3: non-anginal pain \n     \n     -- Value 4: asymptomatic\n     \n\n- 4. (trestbps) : resting blood pressure (in mm Hg on admission to the hospital)\n\n- 5. (chol) : serum cholestoral in mg\/dl\n\n- 6. (fbs) : (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n\n- 7. (restecg) : resting electrocardiographic results resting electrocardiographic results\n\n     -- Value 0: normal \n     \n     -- Value 1: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV) \n     \n     -- Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria \n     \n\n- 8. (thalach) : maximum heart rate achieved\n\n- 9. (exang) : exercise induced angina (1 = yes; 0 = no)\n\n- 10. (oldpeak) : ST depression induced by exercise relative to rest\n\n- 11. (slope) : the slope of the peak exercise ST segment \n\n       -- Value 1: upsloping \n       \n       -- Value 2: flat \n       \n       -- Value 3: downsloping\n       \n\n- 12. (ca) : number of major vessels (0-3) colored by flourosopy\n\n- 13. (thal) : 3 = normal; \n          6 = fixed defect; \n          7 = reversable defect\n\n- 14. (num) (the predicted attribute) target :1 or 0\n","c3c320a9":"# 1 Logistic Regression","f6aa7f40":"# 7 Linear SVC","cdaee38b":"# 4 KNN","12ce9131":"# Which is the best Model ?","42b9bba1":"# 8 Decision Tree","bb4bde33":"# 5 Gaussian Naive Bayes","9715e40e":"# **Getting the Data**","761d73b9":"# Confusion Matrix","3b83bac2":"# 3 Random Forest","f687ba04":"# 2 Stochastic Gradient Descent (SGD) learning","52b8cc94":"# Since 'cp', 'thal' and 'slope' are categorical variables we'll turn them into dummy variables.","7032c764":"# Building Machine Learning Models","4d27a3f2":"**better approach\/feedback appricated.. : )**"}}