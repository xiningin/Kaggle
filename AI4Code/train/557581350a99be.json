{"cell_type":{"d4a754ff":"code","98d03e9c":"code","bf7af4bc":"code","2800a682":"code","7c8e7870":"code","f9df8590":"code","1937cdb2":"code","642cab74":"code","0f97b3bb":"code","b29b3247":"code","8f30f801":"code","c657d0b6":"code","928f748a":"code","5e06e511":"code","611126ab":"code","52c072b5":"code","0a68d2e2":"code","182bd8b0":"code","754aa93a":"code","efb68905":"code","ba252ef3":"code","f965adae":"code","39aea57b":"code","42be702c":"code","d1585d7d":"code","959c79e4":"code","2dbdea03":"code","1b5c63c7":"code","65d03750":"code","ebaddc47":"code","363c109d":"code","1f6b5f18":"code","36ba819f":"code","19eb7de7":"code","88b9495f":"code","cad23ce7":"code","ea2d742f":"code","051dde02":"code","0af1ef0e":"code","96d255ca":"code","f017adbf":"code","40e48dc2":"code","3154fb32":"code","a4d58020":"code","19b9ff04":"code","94f12a07":"code","64755d5d":"code","934290e3":"code","6c163239":"code","343a1ef3":"code","a1aa9506":"code","a2acd962":"code","8da837f1":"code","54f8773a":"code","44ca5adf":"code","e28105fe":"code","cf3a88de":"code","7828c128":"code","93df68f9":"code","8bf80e6c":"code","5a63e9a0":"code","b48ebc2e":"code","eb78e269":"code","8861344e":"code","4c449462":"code","e5a53f21":"code","fcd9ca56":"code","a47a19d7":"code","1334b651":"code","a2339440":"code","f903c93e":"code","bc32bbf5":"code","6e9d941e":"code","bf518d64":"code","5db25d5d":"code","a3173cde":"code","8cbd323e":"code","eb2d5c82":"code","11ae70ac":"code","a8862fc0":"code","1d467088":"code","ac25f0f5":"code","2c079097":"code","a2c4d6d7":"code","cc82f41b":"code","f03444e6":"code","5f62a4e5":"code","ab97a6c1":"code","5cf10d7e":"code","aefc430f":"code","1b18e393":"code","8a0bcffd":"code","d5a5df8d":"code","103d9973":"code","176ab38a":"code","bd17788c":"code","cea3f96f":"markdown","0e4e6035":"markdown","8ae3c6fc":"markdown","6e69158f":"markdown","e709fe8b":"markdown","881d8af8":"markdown","80b58574":"markdown","d4526ee2":"markdown","aaa55e9a":"markdown","6837b85c":"markdown","5fe2271c":"markdown","d3d1bf59":"markdown","41b3e74c":"markdown","da5c567f":"markdown","76311c36":"markdown","2d02aef2":"markdown","b863ce05":"markdown","3debb1ac":"markdown","e325b8ec":"markdown","9d2df6e6":"markdown","1dcf117b":"markdown","8309a195":"markdown","42bf2625":"markdown","8a6b4b2b":"markdown","df9ff6f7":"markdown","7f940b45":"markdown","7967b837":"markdown","e6007109":"markdown","7ea33022":"markdown","75cf7831":"markdown","9adf2161":"markdown","c10e5f49":"markdown","5f314943":"markdown","a3c3d806":"markdown","e5cd74be":"markdown","10de47fe":"markdown","728e7ba9":"markdown","4207b7eb":"markdown","0b3744c5":"markdown","46cc687b":"markdown","e3e382af":"markdown","b986e215":"markdown","d12fb03a":"markdown","b3c4e79d":"markdown","255a6680":"markdown","f486da08":"markdown","e981ec09":"markdown","fea0c76b":"markdown","a7ac07b0":"markdown","ebb8ebe1":"markdown","f4bd4df4":"markdown","689af81e":"markdown","bd953c18":"markdown","d35c931f":"markdown","91d67841":"markdown","16d13561":"markdown","b61b2436":"markdown","f750697b":"markdown","d8a0a371":"markdown","c17d1fc4":"markdown","719d1a6f":"markdown","b0f1caa6":"markdown","335ec22a":"markdown","6cc8a68d":"markdown","6833e1ad":"markdown","00a7a9e3":"markdown","55bdf5b3":"markdown","86522fb2":"markdown","e9f656ac":"markdown","019be259":"markdown","c4606eb8":"markdown","ebaf5b27":"markdown","6af016f9":"markdown","29058b29":"markdown","896f8ee6":"markdown","57540650":"markdown","34a961e8":"markdown","455969db":"markdown"},"source":{"d4a754ff":"import numpy as np\nimport pandas as pd\nimport matplotlib.pylab as plt\nimport seaborn as sns\nimport os\nimport sys\nsys.path.append(os.path.realpath('..')) #note to self: this works, only when notebook is alrdy saved in directory. So, first save notebook and then use this line of code.\nfrom scipy.stats import normaltest\nfrom scipy.stats import anderson\nfrom scipy.stats import kendalltau\nfrom scipy.stats import pearsonr\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import classification_report\nimport imblearn\nfrom imblearn.over_sampling import BorderlineSMOTE\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom xgboost import XGBClassifier","98d03e9c":"# Loading application records into a pandas dataframe\nappl_df = pd.read_csv( '..\/input\/credit-card-approval-prediction\/application_record.csv', index_col= 'ID' )","bf7af4bc":"\n# Loading credit records into a pandas dataframe\ncred_df = pd.read_csv( '..\/input\/credit-card-approval-prediction\/credit_record.csv', index_col= 'ID' )","2800a682":"appl_df.head().T","7c8e7870":"cred_df.head().T","f9df8590":"print( appl_df.shape )\nprint('')\nappl_df.info()","1937cdb2":"print( cred_df.shape )\nprint('')\ncred_df.info()","642cab74":"appl_df.loc[[5008804, 5008805]].T","0f97b3bb":"valid_indexes = list( set(appl_df.index).intersection(set(cred_df.index)) ) \nlen( valid_indexes )","b29b3247":"#appl_df_clean =  appl_df.loc[valid_indexes]\n#cred_df_clean = cred_df.loc[valid_indexes]\n\nappl_df = appl_df.loc[valid_indexes]\ncred_df = cred_df.loc[valid_indexes]","8f30f801":"print(appl_df.shape)\nprint('')\nappl_df.head().T","c657d0b6":"appl_df_clean = appl_df.sort_values(by = appl_df.columns.to_list()) #safety step - performance doesen't seem to be affected that much.\ngrouped_cust = appl_df.sum(axis=1).map(hash).reset_index().rename(columns={0:'customer_id'})\n\nid_counts_df = pd.DataFrame( grouped_cust.groupby('customer_id').size().sort_values(ascending=False), columns = ['id_count'] )\nappl_df_clean['cust_id'] = appl_df.sum(axis=1).map(hash)","928f748a":"appl_df_clean.head()","5e06e511":"grouped_cust = grouped_cust.set_index('ID')\ncred_df_trsf = cred_df.merge(grouped_cust, how = 'inner', on = 'ID').reset_index()[['customer_id','ID', 'MONTHS_BALANCE', 'STATUS']]\n\ncred_df_g = cred_df_trsf.sort_values(by=['customer_id', 'ID', 'MONTHS_BALANCE'], ascending = [True, True, False]).reset_index(drop=True)\ncred_df_g['interaction_ID'] = cred_df_g.groupby(['customer_id','ID'], sort = False).ngroup().add(1) # not woring as I wanted, but it's good enough for what needs to be done\ncred_df_g.drop(columns = ['ID'], inplace=True)\ncred_df_g = cred_df_g[['customer_id', 'interaction_ID', 'MONTHS_BALANCE', 'STATUS']]","611126ab":"cred_df_g['month_behav'] = np.where( cred_df_g.STATUS.isin(['2','3','5']), 'b', 'g' )\n\ncust_beh = pd.DataFrame( round( cred_df_g.groupby(['customer_id', 'month_behav']).size() \/ cred_df_g.groupby(['customer_id']).size() * 100, 2), columns = ['behav_kpi']).reset_index().set_index('customer_id')\n\nbad_cust = \\\ncust_beh[\n    ( (cust_beh.month_behav=='g') & (cust_beh.behav_kpi <= 50) ) | \n    ( (cust_beh.month_behav=='b') & (cust_beh.groupby('customer_id').size()==1) )\n        ]\nbad_cust['customer_type'] = 'bad'\nbad_cust.drop(columns=['month_behav', 'behav_kpi'], inplace=True)\n\ngood_cust = \\\n    cust_beh[\n        ( (cust_beh.month_behav=='g') & (cust_beh.behav_kpi > 50) ) | \n        ( (cust_beh.month_behav=='g') & (cust_beh.groupby('customer_id').size()==1) )\n            ]\ngood_cust['customer_type'] = 'good'\ngood_cust.drop(columns=['month_behav', 'behav_kpi'], inplace=True)\n\ncred_df_clean = pd.concat([bad_cust, good_cust])\ncred_df_clean['months_in_book'] = cred_df_g.groupby('customer_id').size()\ncred_df_clean['contracts_nr'] = cred_df_g.groupby(['customer_id'])['interaction_ID'].nunique()","52c072b5":"# Checking how many values are missing in credit records dataset\ncred_df_clean.isnull().sum()","0a68d2e2":"# Checking how many values are missing in application dataset\nappl_df_clean.isnull().sum()","182bd8b0":"appl_df_clean.OCCUPATION_TYPE.unique()","754aa93a":"appl_df_clean['OCCUPATION_TYPE'] = appl_df_clean['OCCUPATION_TYPE'].fillna('Not Available')","efb68905":"dic = {\n    'Y' : 1,\n    'N' : 0\n}\n\nappl_df_clean['FLAG_OWN_CAR'] = appl_df_clean['FLAG_OWN_CAR'].replace(dic)\nappl_df_clean['FLAG_OWN_REALTY'] = appl_df_clean['FLAG_OWN_REALTY'].replace(dic)\n\nappl_df_clean.head()","ba252ef3":"flag_cols = [x for x in appl_df_clean.columns if x.startswith('FLAG_')]                                                          \ncat_cols  = [x for x in appl_df_clean.columns if x.startswith('CODE_') or x.startswith('NAME_') or x.startswith('OCCUPATION_') ] \nnum_cols  = [x for x in appl_df_clean.columns if x.startswith('CNT_')] + [x for x in appl_df_clean.columns if x.startswith('AMT_') or x.startswith('DAYS_')]          \n\n#checkzone:\n# \n# +1 accounts for cust_id column who does not fit in any column category because it's a \"dummy\" column just to join with cred_df later on\n#\nlen(flag_cols) + len( cat_cols ) + len( num_cols ) + 1 == len( appl_df_clean.columns )  ","f965adae":"appl_df_clean = appl_df_clean[flag_cols + cat_cols + num_cols + ['cust_id']]\n\nappl_df_clean.head().T","39aea57b":"cred_df_clean.reset_index(inplace=True)\n\ndf = \\\nappl_df_clean.reset_index().merge(\n    cred_df_clean,\n    left_on = appl_df_clean.cust_id,\n    right_on = cred_df_clean.customer_id,\n    how = 'inner'\n).drop(columns = ['key_0','cust_id', 'customer_id']).set_index('ID')","42be702c":"fig, axes = plt.subplots(ncols=len( flag_cols ), figsize=(20,5))\nfor col, ax in zip(df[flag_cols], axes):\n    df[col].value_counts().sort_values().plot.barh(ax=ax, title=col + ' histogram')\n\nplt.tight_layout()    \nplt.show()","d1585d7d":"#removing FLAG_MOBIL from the app_df_clean dataset since it holds no value for this study\ndf.drop(columns=['FLAG_MOBIL'], inplace=True)\n#removing FLAG_MOBIL from flag_cols aswell\nflag_cols.remove('FLAG_MOBIL')\n\ndf.head()","959c79e4":"#fig, axes = plt.subplots(ncols=len(flag_cols), figsize=(20,5))\n#\n#for col, ax in zip(appl_df_clean[flag_cols], axes):\n#    n, bins, patches = \\\n#        plt.hist(\n#            appl_df_clean[col],\n#            orientation='horizontal',\n#            bins = 3,\n#            align= 'mid'\n#            )\n#    \n#    plt.title(col + ' histogram of frequencies')\n#    plt.ylabel(col)\n#    plt.yticks(ticks=[1,0])\n#    plt.ylim(0,1)\n#    plt.xlabel('Frequency')\n#\n#    #plt.xlim([-1,1])\n#\n#    # Make some labels.\n#\n#    for rect in patches: # https:\/\/stackoverflow.com\/questions\/28931224\/adding-value-labels-on-a-matplotlib-bar-chart\n#        # Get X and Y placement of label from rect.\n#        x_value = rect.get_width()\n#        y_value = rect.get_y() + rect.get_height() \/ 2\n#\n#        # Number of points between bar and label. Change to your liking.\n#        space = -100\n#        # Vertical alignment for positive values\n#        ha = 'left'\n#\n#        # Use X value as label and format number with one decimal place\n#        #label = '{:,.0f}'.format(x_value)\n#        label = f\"{x_value:,.0f}\"\n#\n#        # Create annotation\n#        plt.annotate(\n#            label,                      # Use `label` as label\n#            (x_value, y_value),         # Place label at end of the bar\n#            xytext=(space, 0),           # Horizontally shift label by `space`\n#            textcoords=\"offset points\", # Interpret `xytext` as offset in points\n#            va='center',                # Vertically center label\n#            ha=ha)                      # Horizontally align label differently for\n#                                        # positive and negative values.\n#    ;","2dbdea03":"fig, axes = plt.subplots( nrows= len(cat_cols),  figsize=(10,20))\nfor col, ax in zip(df[cat_cols], axes):\n    df[col].value_counts().sort_values().plot.barh(ax=ax, title=col + ' histogram')\n\nplt.tight_layout()    \nplt.show()","1b5c63c7":"pd.DataFrame( df[cat_cols].groupby(cat_cols).size().sort_values(ascending=False), columns = ['Value']).reset_index().head().T","65d03750":"num_cols = num_cols + ['months_in_book', 'contracts_nr']","ebaddc47":"fig, axes = plt.subplots( nrows= len(num_cols), figsize=(10,20))\n\nfor col, ax in zip(df[num_cols], axes):\n    sns.distplot( df[col], ax=ax )\n\nplt.tight_layout()    \nplt.show()\n;","363c109d":"desc_num = round( df[num_cols].describe(), 0)\ndesc_num    ","1f6b5f18":"df[df.DAYS_EMPLOYED > 0 ]['DAYS_EMPLOYED'].min()","36ba819f":"appl_df[appl_df.DAYS_EMPLOYED > 0 ]['DAYS_EMPLOYED'].min()","19eb7de7":"appl_df_unem = df[appl_df_clean.DAYS_EMPLOYED > 0 ]\nappl_df_unem.head().T","88b9495f":"print( appl_df_unem['NAME_INCOME_TYPE'].unique() )\nprint('')\nprint( appl_df_unem['OCCUPATION_TYPE'].unique() )","cad23ce7":"print( appl_df[appl_df.DAYS_EMPLOYED == 365243]['NAME_INCOME_TYPE'].unique() )\nprint('')\nprint(appl_df[appl_df.DAYS_EMPLOYED == 365243]['OCCUPATION_TYPE'].unique())","ea2d742f":"fig, axes = plt.subplots( nrows= len(num_cols), figsize=(10,20))\n\nfor col, ax in zip(df[num_cols], axes):\n    sns.boxplot( y = df[col], ax=ax, orient = 'h' )\n\nplt.tight_layout()    \nplt.show()\n;","051dde02":"#fig, axes = plt.subplots( nrows= len(num_cols), figsize=(10,20))\n#\n#for col, ax in zip(appl_df_clean[num_cols], axes):\n#    series_smoothed = appl_df_clean[col]\n#    series_smoothed = series_smoothed[ series_smoothed <= series_smoothed.quantile(0.75) ]\n#    sns.boxplot( y = series_smoothed, ax=ax, orient = 'h' )\n#\n#plt.tight_layout()    \n#plt.show()\n#;","0af1ef0e":"# D\u2019Agostino\u2019s K^2 Test\nstat, p = normaltest(df['DAYS_BIRTH'])\nprint('Statistics=%.3f, p=%.3f' % (stat, p))\n# interpret\nalpha = 0.05\nif p > alpha:\n\tprint('Sample looks Gaussian (fail to reject H0)')\nelse:\n\tprint('Sample does not look Gaussian (reject H0)')","96d255ca":"# Anderson-Darling Test\nresult = anderson(df['DAYS_BIRTH'])\nprint('Statistic: %.3f' % result.statistic)\np = 0\nfor i in range(len(result.critical_values)):\n\tsl, cv = result.significance_level[i], result.critical_values[i]\n\tif result.statistic < result.critical_values[i]:\n\t\tprint('%.3f: %.3f, data looks normal (fail to reject H0)' % (sl, cv))\n\telse:\n\t\tprint('%.3f: %.3f, data does not look normal (reject H0)' % (sl, cv))","f017adbf":"help(pearsonr)","40e48dc2":"help(kendalltau)","3154fb32":"appl_corr = round(df[num_cols].corr('kendall'), 2)\n\nplt.figure(figsize=(10, 8))\n\nmask = np.zeros_like(appl_corr)\nmask[np.triu_indices_from(mask)] = True\n\nax = sns.heatmap(\n    appl_corr, \n    annot=True, \n    square=True,\n    mask=mask,\n    xticklabels=True, \n    yticklabels=True    \n    )\n\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n)\nax.set_ylim([0,7])\nplt.show()\n;","a4d58020":"def kendall_pvalue(pandas_series_1, pandas_series_2):\n    \"\"\"\n    Takes in 2 pandas series, and calculates p-value for hypotesis testing about dependence between those 2 data series.\n    \"\"\"\n    stat, p = kendalltau(pandas_series_1, pandas_series_2)\n    if p > 0.05:\n        return print( 'There is not enough statistical evidence to prove that: ' + pandas_series_1.name + ' is dependent of ' + pandas_series_2.name + '.'+'\\n'+ 'Kendall Corr. value is: ' + str( round(stat,2) ) +' | p-value is: ' + str( round(p,4) )  + '.' )\n    else:\n        return print( 'There is enough statistical evidence to prove that: ' + pandas_series_1.name + ' is dependent of ' + pandas_series_2.name +'.' +'\\n'+ 'Kendall Corr. value is: ' + str( round(stat,2) ) +' | p-value is: ' + str( round(p,4) )  + '.' )","19b9ff04":"kendall_pvalue(df['CNT_CHILDREN'], df['CNT_FAM_MEMBERS'])\nprint()\nkendall_pvalue(df['months_in_book'], df['contracts_nr'])","94f12a07":"plt.scatter(df['CNT_CHILDREN'], df['CNT_FAM_MEMBERS']);","64755d5d":"plt.scatter(df['months_in_book'], df['contracts_nr']);","934290e3":"num_cols.remove('CNT_CHILDREN')\nnum_cols.remove('contracts_nr')\ndf.drop(columns=['CNT_CHILDREN','contracts_nr'], inplace = True)\ndf.head()","6c163239":"cred_df['STATUS'].value_counts().sort_values().plot.barh(title= 'STATUS histogram')\n\nplt.tight_layout()    \nplt.show()","343a1ef3":"cred_df_clean.head()","a1aa9506":"percent_bad_customers = 100 * len( cred_df_clean[cred_df_clean.customer_type=='bad'] ) \/ cred_df_clean.shape[0] \n\nprint( \"{0:.3f}%\".format( percent_bad_customers) )","a2acd962":"#example of a \"bad\" customer\ncustomer_id = 5142361\nprint( appl_df_clean.loc[customer_id] )\nprint('')\nprint( cred_df.loc[customer_id].sort_values(by=['MONTHS_BALANCE'], ascending=False) )","8da837f1":"cat_cols","54f8773a":"# Variable list to transform\ncat_col_t = [ col for col in cat_cols if 'GENDER' not in col ] # we exclude gender as it only contains 2 categories\ncat_col_t","44ca5adf":"df['NAME_INCOME_TYPE'].value_counts()","e28105fe":"dic = {\n    'Commercial associate' : 'Working',\n    'State servant' : 'Working',\n}\ndf['NAME_INCOME_TYPE'] = df['NAME_INCOME_TYPE'].replace(dic)","cf3a88de":"df['NAME_EDUCATION_TYPE'].value_counts()","7828c128":"dic = {\n    'Incomplete higher' : 'Secondary \/ secondary special',\n    'Academic degree' : 'Higher education',\n    'Lower secondary' : 'Basic'\n}\ndf['NAME_EDUCATION_TYPE'] = df['NAME_EDUCATION_TYPE'].replace(dic)","93df68f9":"df['NAME_FAMILY_STATUS'].value_counts()","8bf80e6c":"dic = {\n    'Civil marriage' : 'Married'\n}\n\n#df['NAME_FAMILY_STATUS'].replace(dic).value_counts()\ndf['NAME_FAMILY_STATUS'] = df['NAME_FAMILY_STATUS'].replace(dic)","5a63e9a0":"df['NAME_HOUSING_TYPE'].value_counts()","b48ebc2e":"dic = {\n    'House \/ apartment' : 'Rented apartment',\n    'Co-op apartment' : 'Rented apartment',\n    'Municipal apartment': 'Municipal or Office apartment',\n    'Office apartment': 'Municipal or Office apartment'\n}\n\ndf['NAME_HOUSING_TYPE'] = df['NAME_HOUSING_TYPE'].replace(dic)","eb78e269":"df['OCCUPATION_TYPE'].value_counts()","8861344e":"# https:\/\/stackoverflow.com\/questions\/21912634\/how-can-i-sort-a-boxplot-in-pandas-by-the-median-values\ndata = pd.DataFrame(df.groupby(['OCCUPATION_TYPE'])['AMT_INCOME_TOTAL'].mean()).reset_index().sort_values(by=['AMT_INCOME_TOTAL'], ascending=False)\n\nplt.figure(figsize=(20,5))\nax = sns.boxplot(data=df.sort_values(by='AMT_INCOME_TOTAL', ascending = False), x='OCCUPATION_TYPE', y='AMT_INCOME_TOTAL', order = data['OCCUPATION_TYPE'], linewidth= 1)\n\nax.set_xticklabels(ax.get_xticklabels(),rotation=85)\n\n#ax.set_xlim(0,20)\n;","4c449462":"dic = {\n    'Managers' : 'Group 1',\n    'Realty agents' : 'Group 1',\n    'Drivers' : 'Group 1',\n    'Accountants' : 'Group 1',\n    'IT staff' : 'Group 2',\n    'Private service staff' : 'Group 2',\n    'High skill tech staff' : 'Group 2',\n    'HR staff' : 'Group 2',\n    'Core staff' : 'Group 2',\n    'Laborers' : 'Group 3',\n    'Security staff' : 'Group 3',\n    'Sales staff' : 'Group 3',\n    'Not Available' : 'Group 3',\n    'Secretaries' : 'Group 3',\n    'Medicine staff' : 'Group 4',\n    'Waiters\/barmen staff' : 'Group 4',\n    'Cleaning staff' : 'Group 4',\n    'Cooking staff' : 'Group 4',\n    'Low-skill Laborers' : 'Group 4'\n}\n\n#df['OCCUPATION_TYPE'].replace(dic)\ndf['OCCUPATION_TYPE'] = df['OCCUPATION_TYPE'].replace(dic)","e5a53f21":"data = pd.DataFrame(df.groupby(['OCCUPATION_TYPE'])['AMT_INCOME_TOTAL'].mean()).reset_index().sort_values(by=['AMT_INCOME_TOTAL'], ascending=False).round(1)\n\nplt.figure(figsize=(8,6))\nax = sns.boxplot(data=df.sort_values(by='AMT_INCOME_TOTAL', ascending = False), x='OCCUPATION_TYPE', y='AMT_INCOME_TOTAL', order = data['OCCUPATION_TYPE'], linewidth= 1)\n\nax.set_xticklabels(ax.get_xticklabels(),rotation=85);","fcd9ca56":"rand_st = 123\ntest_size =0.3","a47a19d7":"df.head()\n#df.to_csv('C:\\\\ML Analytics\\\\05 - Kaggle\\\\02 - Credit Card Aproval\\\\00 - dataset\\\\df.csv')","1334b651":"df_cat = df[cat_cols]\ndf_dumm = pd.get_dummies(df_cat,  prefix_sep='==')\n\ndf_dumm.head()","a2339440":"df.drop(columns = cat_cols, inplace= True)","f903c93e":"df_dumm.head()","bc32bbf5":"df = pd.concat([df_dumm, df], axis = 1)\ndf.head()","6e9d941e":"df[flag_cols] = df[flag_cols].astype('uint8')\n\ndf.head()","bf518d64":"X = df.drop(columns=['customer_type'])\nY = df['customer_type']\n\nX_train, X_test, Y_train, Y_test = train_test_split( X, Y, test_size = test_size, stratify = Y, random_state = rand_st )","5db25d5d":"#num_cols = num_cols + ['months_in_book']\ndf_num_X_train = X_train[num_cols]\ndf_num_X_test = X_test[num_cols]\n\n# perform a robust scaler transform of the dataset\ntrans = RobustScaler(with_centering=False, with_scaling=True)\n\ndf_num_X_train_s = trans.fit_transform(df_num_X_train)\ndf_num_X_train = pd.DataFrame( df_num_X_train_s, columns = df_num_X_train.columns, index = df_num_X_train.index)\n\ndf_num_X_test_s = trans.fit_transform(df_num_X_test)\ndf_num_X_test = pd.DataFrame( df_num_X_test_s, columns = df_num_X_test.columns, index = df_num_X_test.index)","a3173cde":"print( \"train set size: {0:,}\".format( len( df_num_X_train ) ) )\nprint('')\nprint( \"test set size: {0:,}\".format( len( df_num_X_test ) ) )","8cbd323e":"X_train.drop(columns = num_cols, inplace = True)\nX_train = pd.concat([X_train, df_num_X_train] ,axis=1)\n\nX_train.head()","eb2d5c82":"X_test.drop(columns = num_cols, inplace = True)\nX_test = pd.concat([X_test, df_num_X_test] ,axis=1)\n\nX_test.head()","11ae70ac":"cust_t_dic = { 'good':0, 'bad': 1 }\n\nY_train = Y_train.replace(cust_t_dic).astype('uint8')\nY_test = Y_test.replace(cust_t_dic).astype('uint8')","a8862fc0":"print(Y_train.value_counts())\nprint()\nprint( Y_test.value_counts())","1d467088":"\noversample = BorderlineSMOTE(sampling_strategy=0.1)\nundersample = RandomUnderSampler(sampling_strategy=0.5)","ac25f0f5":"print( len(Y_train) )\nprint()\nprint( Y_train.value_counts() )","2c079097":"weightings_0 = len(Y_train) \/ ( len( Y_train.unique() ) * Y_train.value_counts()[0] )\nweightings_0 #weight of observations classified as 0","a2c4d6d7":"weightings_1 = len(Y_train) \/ ( len( Y_train.unique() ) * Y_train.value_counts()[1] )\nweightings_1 #weight of observations classified as 1","cc82f41b":"def fit_pipeline(pipeline, X_train_data, Y_train_data, X_test_data, param_grid, cv, scoring_grid, scoring_fit):\n    \"\"\"\n    Defines a brute force pipeline to evaluate model, according to defined parameter and scoring grids.\n    \"\"\"\n    \n    grid = GridSearchCV(\n        estimator=pipeline,\n        param_grid=param_grid, \n        cv=cv, \n        n_jobs=-1, \n        scoring= scoring_grid,\n        refit = scoring_fit,\n        verbose=2\n        )\n\n    fitted_model = grid.fit(X_train_data, Y_train_data)\n    pred = fitted_model.predict(X_test_data)\n    \n    return fitted_model, pred","f03444e6":"#Defining classifier parameters\nweighting = [{0: 0.50, 1: 981}, {0: 0.05, 1: 981}, {0: 0.005, 1: 981}, {0: 0.005, 1:9810}, {0:0.005,1:98100}]\n\nparam_grid = dict( \n    model__penalty = ['l1','l2'], \n    model__class_weight = ['balance'] + weighting,\n    #model__C = [1,10,100,1000], #first attempt\n    #model__C = [0.0001, 0.001, 0.01, 1], #second attempt\n    model__C = [1, 1.05, 1.1 ], #third attempt\n    model__random_state = [rand_st]\n    ) \n\n\n#Defining classifier function\nlr = LogisticRegression()\n\n#Defining Pipeline\n#Note to self: Whenever using the pipeline, you will need to send the parameters in a way so that pipeline can understand which parameter is for which of the step in the list. For that it uses the name you provided during Pipeline initialisation. \n#https:\/\/stackoverflow.com\/questions\/58815016\/cross-validating-with-imblearn-pipeline-and-gridsearchcv\nsteps = [('over', oversample), ('under', undersample), ('model', lr)]\npipeline =  Pipeline(steps=steps)","5f62a4e5":"# define evaluation procedure\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=rand_st)\n# define grid search\n#scoring = {'AUC': 'roc_auc', 'Precision': make_scorer(precision_score), 'F1': make_scorer(f1_score), 'Recall': make_scorer(recall_score)}\nscoring = { 'F1': make_scorer(f1_score)}","ab97a6c1":"fitted_model, pred = fit_pipeline(pipeline, X_train, Y_train, X_test, param_grid, cv, scoring, 'F1')","5cf10d7e":"print('Best Penalty:', fitted_model.best_estimator_.get_params()['model__penalty'])\nprint('Best C:', fitted_model.best_estimator_.get_params()['model__C'])\nprint('Best Class_weight:', fitted_model.best_estimator_.get_params()['model__class_weight'])\nprint('Best F1:', fitted_model.best_score_.round(2))\n","aefc430f":"#Pred = lr_optimized.predict(X_test)\n\nclass_report = classification_report(Y_test, pred, output_dict=True)\n\ncr_df = round( pd.DataFrame(class_report).transpose(), 2)\n\ncr_df","1b18e393":"# Confusion Matrix\n\ncm = metrics.confusion_matrix(Y_test, pred)\n\nfig, ax = plt.subplots(figsize=(8,6))\n\nsns.heatmap(cm, annot=True, fmt=\",.0f\", linewidths=.5, square = True);\n\nplt.ylabel('Actual label')\nax.set_ylim([0,2])\n\nplt.xlabel('Predicted label');\n\nall_sample_title = 'F1: {:.2f}'.format(fitted_model.best_score_)\nplt.title(all_sample_title, size = 15);","8a0bcffd":"#Defining classifier parameters\n#brute force scan for all parameters, here are the tricks: https:\/\/www.kaggle.com\/phunter\/xgboost-with-gridsearchcv\n#usually max_depth is 6,7,8\n#learning rate is around 0.05, but small changes may make big diff\n#tuning min_child_weight subsample colsample_bytree can have \n#much fun of fighting against overfit \n#n_estimators is how many round of boosting\n#finally, ensemble xgboost with multiple seeds may reduce variance\nparam_grid = dict( \n    model__random_state = [rand_st],\n    model__nthread = [4], #when use hyperthread, xgboost may become slower\n    model__objective = ['binary:logistic'],\n    #model__learning_rate = [0.049, 0.050, 0.051], #so called `eta` value\n    model__learning_rate = [0.051], \n    #model__max_depth = [6, 7, 8],\n    model__min_child_weight= [11],\n    #model__silent=[1],\n    #model__subsample=[0.7, 0.8, 0.9],\n    #model__colsample_bytree=[0.7, 0.8] ,\n    #model__n_estimators=[400, 700, 1000], #number of trees, change it to 1000 for better results\n    model__n_estimators=[1000], \n    #model__missing=[-999],\n    model__seed=[1337]\n    ) \n\n\n#Defining classifier function\nxgb_model = XGBClassifier()\n\n#Defining Pipeline\n\nsteps = [('over', oversample), ('under', undersample), ('model', xgb_model)]\npipeline =  Pipeline(steps=steps)","d5a5df8d":"fitted_xgb_model, pred_xgb = fit_pipeline(pipeline, X_train, Y_train, X_test, param_grid, cv, scoring, 'F1')","103d9973":"print('Best learning rate:', fitted_xgb_model.best_estimator_.get_params()['model__learning_rate'])\nprint('Best max depth:', fitted_xgb_model.best_estimator_.get_params()['model__max_depth'])\nprint('Best nr. estimators:', fitted_xgb_model.best_estimator_.get_params()['model__n_estimators'])\nprint('Best F1:', fitted_xgb_model.best_score_.round(2))","176ab38a":"class_report = classification_report(Y_test, pred_xgb, output_dict=True)\n\ncr_df = round( pd.DataFrame(class_report).transpose(), 2)\n\ncr_df","bd17788c":"# Confusion Matrix\n\ncm = metrics.confusion_matrix(Y_test, pred_xgb)\n\nfig, ax = plt.subplots(figsize=(8,6))\n\nsns.heatmap(cm, annot=True, fmt=\",.0f\", linewidths=.5, square = True);\n\nplt.ylabel('Actual label')\nax.set_ylim([0,2])\n\nplt.xlabel('Predicted label');\n\nall_sample_title = 'F1: {:.2f}'.format(fitted_xgb_model.best_score_)\nplt.title(all_sample_title, size = 15);","cea3f96f":"WoW! \nCheck out that maximum value for DAYS_EMPLOYED!  \n365.243 days unemployed translates into about 1.001 years unemployed! (according to data dic, positive values for this feature mean the person is unemployed)  \nThis is impossible :)  \n  \nActually, if we filter app_df_clean by positive values in DAYS_EMPLOYED and check the minimum value for the resulting DAYS_EMPLOYED sample, we get 365243 days  \nwhich means that all of the positive values in app_df_clean are, most defenetly, wrongly registered.  \n","0e4e6035":"Seems like they are mostly pensionists...  \nLet's confirm that fact:","8ae3c6fc":"https:\/\/machinelearningmastery.com\/cost-sensitive-logistic-regression\/  \nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.utils.class_weight.compute_class_weight.html  \n\nThe scikit-learn library provides an implementation of the best practice heuristic for the class weighting.\n\nIt is implemented via the compute_class_weight() function and is calculated as:\n\n        n_samples \/ (n_classes * n_samples_with_class)\n\nWe can test this calculation manually on our dataset.\n  \nFor example, we have 10,000 examples in the dataset, 9900 in class 0, and 100 in class 1.  \nThe weighting for class 0 is calculated as:\n\n    weighting = n_samples \/ (n_classes * n_samples_with_class)\n    weighting = 10000 \/ (2 * 9900)\n    weighting = 10000 \/ 19800\n    weighting = 0.05\n\nThe weighting for class 1 is calculated as:\n\n    weighting = n_samples \/ (n_classes * n_samples_with_class)\n    weighting = 10000 \/ (2 * 100)\n    weighting = 10000 \/ 200\n    weighting = 50\n\n------------------------------  \n\nLogisticRgression has a parameter that alows to automatically apply this heuristic method, by assigning class_weight = 'balanced' in the model parameters.\n\n","6e69158f":"## Logistic Regression  \nhttps:\/\/en.wikipedia.org\/wiki\/Logistic_regression  \n\nhttps:\/\/scikit-learn.org\/stable\/modules\/linear_model.html#logistic-regression\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html","e709fe8b":"Now that we have our datasets cleaned we can prepare some descriptive analytics about them","881d8af8":"### OCCUPATION_TYPE","80b58574":"Percentage of bad clients in the data:","d4526ee2":"## Reclassifying some Flags\nIn application dataset there are 2 flag columns that have Y\/N labels, while other flags in this dataset are binary variables.   \nFor congruence, let's pass those Y\/N labels into 1 and 0's, respectively. ","aaa55e9a":"Filtering both dataframes by common indexes","6837b85c":"Adding the same unique customer ID to cred_df","5fe2271c":"### NAME_HOUSING_TYPE","d3d1bf59":"As we can see, from the box plots, there are alot of outliers in most of numeric features of our dataset.  \nOne common strategy to deal with outliers, is to replace them with mean, median, mode, etc. However, if we do that I belive it would not be beneficial for our model since it would biase the data.  \nI guess one solution would be to scale this variables with robust scaling:  https:\/\/machinelearningmastery.com\/robust-scaler-transforms-for-machine-learning\/","41b3e74c":"### Logistic Regression | Performance","da5c567f":"From the above table, we can check what the most frequent customer profiles are.","76311c36":"From the above chart sequence we can imediatly see that all the customers, recorded in the application dataset, have a mobile phone.  \nThis fact implies that it is irrelevant to use this feature for deault modeling, since it won't help to find differences amongst customers.  \n\nOther fact to note, is that in every flag feature, except FLAG_MOBIL, there is a notable difference between customers who have the caracteristic and the ones who don't.  ","2d02aef2":"Transforming cred_df in order to return a list of customers labeled by theyr behaviour type.  \nThis will help with getting our Y label","b863ce05":"Dropping categorical columns from df dataframe, and joining the \"dummy\" versions","3debb1ac":"### Numeric features | Outliers detection","e325b8ec":"Safe to conclude that DAYS_BIRTH feature is not coming from a purely Gaussian\/Normal probability distribution. ","9d2df6e6":"Adding a unique customer ID in appl_df","1dcf117b":"From the above charts we can immediately notice that there are notable differences in all categorical features.  \n","8309a195":"Let's divide this variable as 'Secondary \/ secondary special', 'Higher education' and 'Lower secondary'.","42bf2625":"## Credit Records Data Set","8a6b4b2b":"Using drop_duplicates in this case it's not possible, since there might be cases where one duplicate ID in application dataset might not be duplicated in credit records dataset.  \nSo we need to account ID's common to both dataframes \n","df9ff6f7":"We will use Kendal correlation coeficients, since all numeric variables are not coming from a Normal distribution (Pearson correlation coeficient requires that data is coming from Normal distributions).  \nKendall correlation coeficients are robust to lack of normality, and for that reason we hope to derive more accurate correlation values.  \nhttps:\/\/en.wikipedia.org\/wiki\/Kendall_rank_correlation_coefficient","7f940b45":"This is bad...  \nThis leave us with no idea for how long people are unemployed :(","7967b837":"Let's check who are the customers who have that much time unemployed...","e6007109":"https:\/\/scikit-learn.org\/stable\/modules\/grid_search.html","7ea33022":"## Removing duplicated data\nIn appl_df, althought there are different customer ID's, there is equal information over some disticnt ID values.  \nAn example of this is the customers with ID's 5008804 and 5008805:","75cf7831":"Let's plot some box-whiskers plots in ordert o see better the outliers in our data samples","9adf2161":"https:\/\/www.mathbootcamps.com\/how-to-make-a-boxplot-box-and-whiskers-plot-by-hand\/","c10e5f49":"### NAME_FAMILY_STATUS","5f314943":"https:\/\/en.wikipedia.org\/wiki\/XGBoost  \nhttps:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_intro.html#data-interface","a3c3d806":"## Encoding Categorical Features","e5cd74be":"### NAME_INCOME_TYPE","10de47fe":"### Flag features distributions","728e7ba9":"## Numeric Features Robust Scaling","4207b7eb":"Configuring parameters  \nhttps:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html","0b3744c5":"Choosing F1 score here, since its a better scorer  when data is imbalanced, which is the case.  \nSome references:  \nhttps:\/\/blog.exsilio.com\/all\/accuracy-precision-recall-f1-score-interpretation-of-performance-measures\/  \nhttps:\/\/en.wikipedia.org\/wiki\/F1_score  \nhttps:\/\/datascience.stackexchange.com\/questions\/65341\/f1-score-vs-accuracy-which-metric-is-more-important  \n\n","46cc687b":"### Numeric Feature correlations","e3e382af":"Bam! There's our missing value right there!  \nLet's replace the missing values by 'Not Available'.","b986e215":"### Building X and Y dataframes to feed the models","d12fb03a":"Let's divide this variable as 'Working', 'Pensionioner' and 'Student', making 'State servant', 'Commercial associate' merge into 'Working' category. ","b3c4e79d":"From the below plot we can imediatly see that most customers, that have loans to pay on a given month, pay them on time with max 59 days overdue.  \nHowever, there is a slight portion of them that either:  \n- have bad debts;  \n- pay within 60 or 149 days overdue.  \n\nThose last ones can be classified as the 'Bad' customers, because if we think (as a bank) of not receiveing the due amounts for more then 60 days it will have a very negative impact in the bank treasury.","255a6680":"# Dealing with Target classes Imbalance  \n  \nIf we count the differences between classes from the target variable, we can see that there is a hudge difference between them.  \n","f486da08":"### Sorting application dataset's columns  \nWe really don't need this extra step, but i like organized tables. Things get cleaner in my head that way.","e981ec09":"## Defining a couple constants first","fea0c76b":"## XGBoost","a7ac07b0":"And now we apply the sclaing to numeric features:","ebb8ebe1":"## Setting Flag Features to same data type and splitting for train and test sets","f4bd4df4":"### Categorical Features Distributions","689af81e":"Transforming the target variable into bynary format","bd953c18":"We can see that the follwing features might have strong relationships: \n* CNT_FAM_MEMBERS and CNT_CHILDREN,  \n* months_in_book and contracts_nr.  \n\nLet's use Hypotesis testing to check if there's dependence or not between those pairs of features:   ","d35c931f":"## Joining application records dataset with customer labels dataset","91d67841":"# Packages Imports","16d13561":"So... this lead us to conclude that whoever built this dataframe, used 365.243 days figure to register pensionists who have no occupation.  \nPerhaps one solution is to convert this feature into intervals.  \nIt will help our model to understand that some people are not employed (and neither they are employed) because they are already pensionists.","b61b2436":"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.RobustScaler.html  \n\nScale features using statistics that are robust to outliers.\n\nThis Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile).\n\nCentering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method.\n\nStandardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean \/ variance in a negative way. In such cases, the median and the interquartile range often give better results.","f750697b":"This means we only need to clean missing data in application dataset -> Ocupation_type column.  \nLet's have a peek at it:","d8a0a371":"To try to deal with this difference, we will use SMOTE mixing oversampling with undersampling for better results:  \nhttps:\/\/machinelearningmastery.com\/smote-oversampling-for-imbalanced-classification\/  \nhttps:\/\/machinelearningmastery.com\/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset\/","c17d1fc4":"So, firstly we separate out datsets into train and test:","719d1a6f":"### NAME_EDUCATION_TYPE","b0f1caa6":"## Applications Dataset","335ec22a":"# Descriptive Analytics","6cc8a68d":"Below dataframe give us the customers labeled as 'good' or 'bad' based on classification done previously","6833e1ad":"# Data Cleaning","00a7a9e3":"### Testing the \"Normality\" of DAYS_BIRTH feature.  \nWe will use 2 tests for this evaluation, in order to be sure of the results: \n\nhttps:\/\/machinelearningmastery.com\/a-gentle-introduction-to-normality-tests-in-python\/","55bdf5b3":"First attempt (xgboost configured for maximizing roc_auc)  \n   \nBest learning rate:  0.051  \nBest max depth:  7  \nBest nr. estimators:  1000  \nBest roc_auc:  0.78  ","86522fb2":"### Numeric Features Distributions","e9f656ac":"Aparently the idea of creating contract_nr feature wasn't so furtonate.  \nWe'll drop this feature along with CNT_CHILDREN given the high correlations with months_in_book and contract_nr, respectively.","019be259":"In this section we are going to pre-process the dataset to feed the model.  \nWe will split features by theyr type first: flag (or binary), numeric and categorical, and in each of those splits we are doing train and test splits to guarantee that numeric features are not scaled\/normalized before the train\/test split operation.","c4606eb8":"# Modelling\n","ebaf5b27":"### XGB | Performance","6af016f9":"Hmm... grouping professions type will be though without any grouping criteria.  \nLet's try to group by income.  \n\nEven though we have a criteria, is it possible that we introduce a biase with this step?  \nI'm talking about spurius relationships:  https:\/\/en.wikipedia.org\/wiki\/Spurious_relationship","29058b29":"## Shortening Categorical Feature Classes","896f8ee6":"# Data Pre Processing","57540650":"# Data Loading","34a961e8":"## Checking and Cleaning Missing Data","455969db":"Analysing the numeric features distribution plots, we can imediatly see that all of this features are assimetric over the lower values, except DAYS_BITH (perhaps its normal distibuted? We'll check this ahead).  \n  \nThis says alot about the majority of the profiles:  \nMost customers have:   \n- none or few children,  \n- have small families,  \n- have low incomes,  \n- have about 43 years old  \n- are emplyoed for not too long, although there is a significant proportion who are unemployed ( and yes, those positive days are way too high!!! ) \n\n\nFrom here we can also see that we, probably, will have serious problems with outliers lying in the data.  "}}