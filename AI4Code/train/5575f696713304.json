{"cell_type":{"c613f9aa":"code","0d92714d":"code","a57121f3":"code","ec77aef6":"code","4460c805":"code","5e83dae5":"code","602c0b0a":"code","ca727a98":"code","9d307c65":"code","0b9aa559":"code","f0a1a873":"code","c480a63b":"code","8824fc08":"code","d12a9b6b":"code","97dd3e6c":"code","dabd05bc":"code","816230d2":"markdown","1e502c5a":"markdown","d866bfa8":"markdown","22db94ee":"markdown","c93be514":"markdown","556b8a89":"markdown"},"source":{"c613f9aa":"#Import Statements\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport spacy\n\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import LinearSVC","0d92714d":"#Extract reddit data\nreddit_data = pd.read_csv('..\/input\/twitter-and-reddit-sentimental-analysis-dataset\/Reddit_Data.csv')\nreddit_data.rename(columns = {'clean_comment': 'text'}, inplace = True)","a57121f3":"#Extract twitter data\ntwitter_data = pd.read_csv('..\/input\/twitter-and-reddit-sentimental-analysis-dataset\/Twitter_Data.csv')\ntwitter_data.rename(columns = {'clean_text': 'text'}, inplace = True)","ec77aef6":"reddit_data.head(), twitter_data.head()","4460c805":"#Combine both dataframes into one master dataframe\ndata = pd.concat([reddit_data, twitter_data], ignore_index = True)","5e83dae5":"#Check for any null values\ndata.isna().sum()","602c0b0a":"#Drop rows with null values\ndata.dropna(axis = 0, inplace = True)","ca727a98":"#Check for any null values\ndata.isna().sum()","9d307c65":"#Checking the shape of the data to ensure nothing is broken\ndata.shape","0b9aa559":"#Getting Stopwords\nfrom spacy.lang.en.stop_words import STOP_WORDS\nstopwords = list(STOP_WORDS)\n\n#Getting a list of punctuations\nfrom string import punctuation\npunct = list(punctuation)\n\nprint(\"Length of punctuations:\\t {} \\nLength of stopwords:\\t {}\".format(len(punct), len(stopwords)))","f0a1a873":"fig , ax = plt.subplots(figsize = (10,10))\nax = data['category'].value_counts().plot(kind = 'bar')\n\nplt.xticks(rotation = 0, size = 14)\nplt.yticks(size = 14, color = 'white')\nplt.title('Distribution of Sentiment', size = 20)\n\nax.annotate(text = data['category'].value_counts().values[0], xy = (-0.13,88079), size = 18)\nax.annotate(text = data['category'].value_counts().values[1], xy = (0.87,68253), size = 18)\nax.annotate(text = data['category'].value_counts().values[2], xy = (1.87,43786), size = 18)\n\nplt.show()","c480a63b":"#Load spacy model\nnlp = spacy.load('en_core_web_sm')","8824fc08":"#Define function to find and drop blank entries in dataframe\ndef drop_blank(dataframe, column):\n    blank_index = dataframe[dataframe[column] == ''].index\n    dataframe.drop(blank_index, inplace = True)\n    dataframe.reset_index(inplace = True, drop = True)\n    \n    return dataframe","d12a9b6b":"def data_cleaning_tokenizer(sentence):\n    \"\"\"\n    This function cleans up a sentence. It first tokenizes it, removes stopewords, and then performs lemmatization on the sentence. \n    The function returns a list of tokenized words so that it can be converted into a doc vector directly. \n    \"\"\"\n    \n    doc = nlp(sentence)\n    tokens = []\n    \n    #Creates a list of lemmatized tokens\n    for token in doc:\n        try:\n            temp = token.lemma_.lower().strip()\n            \n        except:\n            temp = token.lower_\n            \n        tokens.append(temp)\n        \n    cleaned_tokens = []\n    \n    #Removes stopwords and punctuations\n    for token in tokens:\n        if token not in stopwords and token not in punct:\n            cleaned_tokens.append(token)\n        \n    #cleaned_tokens = \" \".join(cleaned_tokens)\n    \n    return cleaned_tokens","97dd3e6c":"#Getting X and y\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(tokenizer=data_cleaning_tokenizer)\nX = vectorizer.fit_transform(data['text'])\ny = data.category\n\n#Splitting the data into training and testing\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    test_size=0.1)","dabd05bc":"#Creating, fitting and scoring classifier\nclassifier = LinearSVC()\nclassifier.fit(X_train, y_train)\nprint(f\"Accuracy: {classifier.score(X_test, y_test) * 100:.3f}%\", )","816230d2":"---\n\nWe achieved 81% accuracy on our model. This is not bad, but it can be imporoved upon. The most efficient way to imporove our accuracy would be to implement an LSTM model. Neural networks often achieve over 90% on datasets like these.\n\nReferences:    \n- https:\/\/www.kaggle.com\/kritanjalijain\/twitter-sentiment-analysis-lstm\n- https:\/\/www.youtube.com\/watch?v=cd51nXNpiiU\n\nThankyou for going through my notebook! Please comment if you have any questions or suggestions!\n\n---","1e502c5a":"## Data Cleaning\n\n---\nBefore working with the data, we must first clean it. This includes removing all the null data. With NLP, we also need to lemmatize the data and remove stopwords and punctuations so that our model performs better. We do this as well as tokenize the data in the modelling section with the function 'data_cleaning_tokenizer'\n\n---","d866bfa8":"## Modelling: TF-IDF and LinearSVM\n\n---\nIn addition to tokenizing our data, the function 'data_cleaning_tokenizer' also removes stop words, converts all the data to lower and lemmatizes all the data.    \n\nThe vectorization of our data is done in the TF-IDF format through TfidfVectorizer().    \n\nAfter our data is vectorized, it is read to be used in a model. We use LinearSVM in this case beacuse of its effectiveness and ease of implementation. \n\n---","22db94ee":"# Twitter and Reddit Sentiment Analysis\n\n---\nThe following notebook deals with text data acquired from twitter and reddit. All these Tweets and Comments were extracted using there Respective Apis Tweepy and PRAW.\nThese tweets and Comments Were Made on Narendra Modi and Other Leaders as well as Peoples Opinion Towards the Next Prime Minister of The Nation ( In Context with General Elections Held In India - 2019).\nAll the Tweets and Comments From twitter and Reddit are Cleaned using Pythons re and also NLP with a Sentimental Label to each ranging from -1 to 1.   \n\n-1 -> Negative sentiment    \n0 -> Neutral sentiment    \n+1 -> Positive sentiment    \n    \nThe data and more information about it can be found here: https:\/\/www.kaggle.com\/cosmos98\/twitter-and-reddit-sentimental-analysis-dataset    \nAll credit in regards to the aquisition of the data goes to to the original author of the above post. \n\n---","c93be514":"## Data Visualization\n\n---\nIn order to better understand our data, we visualize it. Our main point of interest is to note how the sentiments are distributed and weather any of the categories are skewed.\n\nSince the data isnt skewed, we can proceed normally with our classification. Note, however, that there is approximately twice as much data with positive sentiment than there is with negative sentiment. \n\n---","556b8a89":"## Data Acquisition\n\n---\nThe first step is gathering the data into a singular dataset. We do this by first extracting the Twitter and Reddit data from their respective .csv files and placing them into dataframes. Then, we concatinate both those dataframes into one main dataframe.   \n\nI've renamed the columns tited 'clean_comment' to 'text' to make the dataframe more intuitive and slightly easier to work with.\n\n---"}}