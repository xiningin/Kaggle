{"cell_type":{"42064561":"code","2b5bead7":"code","915e97fa":"code","aa1c9030":"code","628d7ed3":"code","60de77bc":"code","e3d38bd8":"code","6f5a7a34":"code","30afa437":"code","4cd06173":"code","55d78529":"code","08b892ab":"code","8aaa4c96":"code","4248fb2a":"code","d1725e94":"code","8b6771f1":"code","fcbda915":"code","5397df39":"code","110b63d9":"code","df24c7bd":"code","4ca58ee1":"code","6fa78b4d":"code","8ef0017b":"code","b907a8aa":"code","068cd008":"code","9a529ca3":"code","b74b6d15":"code","b362ead4":"code","89c7d07f":"code","44ccb654":"code","c7a53f0d":"code","9b90689e":"code","68967586":"code","b8d8a677":"code","6239154d":"code","504c91e0":"code","16d3e10a":"code","e770e2cf":"code","c986e575":"code","6d7caf25":"code","cf231f4b":"code","d9ea6b2c":"code","32dcd61e":"code","a3e7bf6a":"code","4d555825":"code","a6121b25":"code","fbcb2157":"code","80c51748":"code","5fc94bfb":"code","fd25fb8a":"code","5624fe8f":"code","6a189021":"code","4d09e767":"code","11e17edc":"code","42d0027e":"code","bf765d0d":"code","b6e0e7c5":"code","d6f5e044":"code","eb0dcfe6":"code","0310bbd3":"code","b2668949":"code","1a527804":"code","d9628478":"code","cc23ecb1":"code","aaa3b63b":"code","4f63826a":"code","09588abd":"code","145f59ac":"code","b630ee4d":"code","d6767e45":"code","ae46bbc3":"code","e46518b3":"code","5eb5a2d6":"code","338c7b9b":"code","a3fa38c1":"code","d5f01784":"code","8fa61faa":"code","7d48d745":"code","6d9ef1ad":"code","d2bc812c":"markdown","e3dd5978":"markdown","a30c183f":"markdown","eaa6f6e1":"markdown","e42ae29f":"markdown","1accc4e7":"markdown","f2ed7161":"markdown"},"source":{"42064561":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ntrain=pd.read_csv(\"..\/input\/black-friday-sale-prediction\/train.csv\")\ntest=pd.read_csv(\"..\/input\/black-friday-sale-prediction\/test.csv\")\n","2b5bead7":"from sklearn.model_selection import train_test_split,KFold,StratifiedKFold,GridSearchCV,RandomizedSearchCV,cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier,BaggingClassifier,AdaBoostClassifier,GradientBoostingClassifier,RandomForestRegressor,BaggingRegressor,AdaBoostRegressor,GradientBoostingRegressor\nfrom sklearn.linear_model import LinearRegression,LogisticRegression,Lasso, Ridge\nfrom sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier,KNeighborsRegressor\nimport sklearn.metrics as metrics\nfrom sklearn.metrics import r2_score,roc_auc_score,classification_report,mean_squared_error,accuracy_score,confusion_matrix","915e97fa":"train.head()","aa1c9030":"train_cpy=train.copy()\ntest_cpy=test.copy()","628d7ed3":"train.shape","60de77bc":"train.info()","e3d38bd8":"train.Product_ID.nunique()","6f5a7a34":"train.User_ID.nunique()","30afa437":"train_cat=train.select_dtypes(include='object')\ntrain_cat.drop(['Product_ID'],axis=1,inplace=True)\ntrain_cat.columns","4cd06173":"for i in train_cat.columns:\n    train[i].value_counts().plot.bar()\n    plt.title('{0}'.format(i))\n    plt.show()","55d78529":"train_numeric=train.select_dtypes(include=['int64','float64'])\ntrain_numeric.drop(['User_ID'],axis=1,inplace=True)\ntrain_numeric.columns","08b892ab":"for i in train_numeric.columns:\n    plt.hist(train[i])\n    plt.title('{0}'.format(i))\n    plt.show()","8aaa4c96":"train_numeric.corr()","4248fb2a":"# bar plot with default statistic=mean\nsns.barplot(x='Gender', y='Purchase', data=train)\nplt.show()\n\n# though majorly males shop but the purchase amount is quite close","d1725e94":"# bar plot with default statistic=mean\nsns.barplot(x='Age', y='Purchase', data=train)\nplt.show()\n\n# purchase amount is same for almost all age groups","8b6771f1":"# bar plot with default statistic=mean\nsns.barplot(x='City_Category', y='Purchase', data=train)\nplt.show()\n\n# purchase amount is higher for C","fcbda915":"# bar plot with default statistic=mean\nsns.barplot(x='Stay_In_Current_City_Years', y='Purchase', data=train)\nplt.show()\n\n# amount nearly same for all groups","5397df39":"# bar plot with default statistic=mean\nsns.barplot(x='Marital_Status', y='Purchase', data=train)\nplt.show()\n\n# amount nearly same for all groups","110b63d9":"train[\"Product_Category_1_Count\"] = train.groupby(['Product_Category_1'])['Product_Category_1'].transform('count')\npc1_count_dict = train.groupby(['Product_Category_1']).size().to_dict()\ntest['Product_Category_1_Count'] = test['Product_Category_1'].apply(lambda x:pc1_count_dict.get(x,0))\n\ntrain[\"Product_Category_2_Count\"] = train.groupby(['Product_Category_2'])['Product_Category_2'].transform('count')\npc2_count_dict = train.groupby(['Product_Category_2']).size().to_dict()\ntest['Product_Category_2_Count'] = test['Product_Category_2'].apply(lambda x:pc2_count_dict.get(x,0))\n\ntrain[\"Product_Category_3_Count\"] = train.groupby(['Product_Category_3'])['Product_Category_3'].transform('count')\npc3_count_dict = train.groupby(['Product_Category_3']).size().to_dict()\ntest['Product_Category_3_Count'] = test['Product_Category_3'].apply(lambda x:pc3_count_dict.get(x,0))\n\ntrain[\"User_ID_Count\"] = train.groupby(['User_ID'])['User_ID'].transform('count')\nuserID_count_dict = train.groupby(['User_ID']).size().to_dict()\ntest['User_ID_Count'] = test['User_ID'].apply(lambda x:userID_count_dict.get(x,0))\n\ntrain[\"Product_ID_Count\"] = train.groupby(['Product_ID'])['Product_ID'].transform('count')\nproductID_count_dict = train.groupby(['Product_ID']).size().to_dict()\ntest['Product_ID_Count'] = test['Product_ID'].apply(lambda x:productID_count_dict.get(x,0))","df24c7bd":"train[\"User_ID_MinPrice\"] = train.groupby(['User_ID'])['Purchase'].transform('min')\nuserID_min_dict = train.groupby(['User_ID'])['Purchase'].min().to_dict()\ntest['User_ID_MinPrice'] = test['User_ID'].apply(lambda x:userID_min_dict.get(x,0))\n\ntrain[\"User_ID_MaxPrice\"] = train.groupby(['User_ID'])['Purchase'].transform('max')\nuserID_max_dict = train.groupby(['User_ID'])['Purchase'].max().to_dict()\ntest['User_ID_MaxPrice'] = test['User_ID'].apply(lambda x:userID_max_dict.get(x,0))\n\ntrain[\"User_ID_MeanPrice\"] = train.groupby(['User_ID'])['Purchase'].transform('mean')\nuserID_mean_dict = train.groupby(['User_ID'])['Purchase'].mean().to_dict()\ntest['User_ID_MeanPrice'] = test['User_ID'].apply(lambda x:userID_mean_dict.get(x,0))\n\n\ntrain[\"Product_ID_MinPrice\"] = train.groupby(['Product_ID'])['Purchase'].transform('min')\nproductID_min_dict = train.groupby(['Product_ID'])['Purchase'].min().to_dict()\ntest['Product_ID_MinPrice'] = test['Product_ID'].apply(lambda x:productID_min_dict.get(x,0))\n\ntrain[\"Product_ID_MaxPrice\"] = train.groupby(['Product_ID'])['Purchase'].transform('max')\nproductID_max_dict = train.groupby(['Product_ID'])['Purchase'].max().to_dict()\ntest['Product_ID_MaxPrice'] = test['Product_ID'].apply(lambda x:productID_max_dict.get(x,0))\n\ntrain[\"Product_ID_MeanPrice\"] = train.groupby(['Product_ID'])['Purchase'].transform('mean')\nproductID_mean_dict = train.groupby(['Product_ID'])['Purchase'].mean().to_dict()\ntest['Product_ID_MeanPrice'] = test['Product_ID'].apply(lambda x:productID_mean_dict.get(x,0))\n","4ca58ee1":"userID_25p_dict = train.groupby(['User_ID'])['Purchase'].apply(lambda x:np.percentile(x,25)).to_dict()\ntrain['User_ID_25PercPrice'] = train['User_ID'].apply(lambda x:userID_25p_dict.get(x,0))\ntest['User_ID_25PercPrice'] = test['User_ID'].apply(lambda x:userID_25p_dict.get(x,0))\n\nuserID_75p_dict = train.groupby(['User_ID'])['Purchase'].apply(lambda x:np.percentile(x,75)).to_dict()\ntrain['User_ID_75PercPrice'] = train['User_ID'].apply(lambda x:userID_75p_dict.get(x,0))\ntest['User_ID_75PercPrice'] = test['User_ID'].apply(lambda x:userID_75p_dict.get(x,0))\n\nproductID_25p_dict = train.groupby(['Product_ID'])['Purchase'].apply(lambda x:np.percentile(x,25)).to_dict()\ntrain['Product_ID_25PercPrice'] = train['Product_ID'].apply(lambda x:productID_25p_dict.get(x,0))\ntest['Product_ID_25PercPrice'] = test['Product_ID'].apply(lambda x:productID_25p_dict.get(x,0))\n\nproductID_75p_dict = train.groupby(['Product_ID'])['Purchase'].apply(lambda x:np.percentile(x,75)).to_dict()\ntrain['Product_ID_75PercPrice'] = train['Product_ID'].apply(lambda x:productID_75p_dict.get(x,0))\ntest['Product_ID_75PercPrice'] = test['Product_ID'].apply(lambda x:productID_75p_dict.get(x,0))\n","6fa78b4d":"round((train.isnull().sum()\/len(train.index))*100,2)","8ef0017b":"round((test.isnull().sum()\/len(test.index))*100,2)","b907a8aa":"train.info()","068cd008":"from sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\n\ntrain['Age']=le.fit_transform(train['Age'])\ntest['Age']=le.fit_transform(test['Age'])\n\ntrain['City_Category']=le.fit_transform(train['City_Category'])\ntest['City_Category']=le.fit_transform(test['City_Category'])\n\ntrain['Stay_In_Current_City_Years']=le.fit_transform(train['Stay_In_Current_City_Years'])\ntest['Stay_In_Current_City_Years']=le.fit_transform(test['Stay_In_Current_City_Years'])\n","9a529ca3":"pd.set_option('display.max_columns', 100)\ntrain.head(10)","b74b6d15":"train['Gender']=train['Gender'].map({'M':1, 'F':0})\ntest['Gender']=test['Gender'].map({'M':1, 'F':0})","b362ead4":"train.head()","89c7d07f":"#filling missing values in product categories 2 & 3 by by any constant number say 0\ntrain['Product_Category_2']=train['Product_Category_2'].fillna(0)\ntest['Product_Category_2']=test['Product_Category_2'].fillna(0)\n\ntrain['Product_Category_3']=train['Product_Category_3'].fillna(0)\ntest['Product_Category_3']=test['Product_Category_3'].fillna(0)\n\ntrain['Product_Category_2_Count']=train['Product_Category_2_Count'].fillna(0)\ntest['Product_Category_2_Count']=test['Product_Category_2_Count'].fillna(0)\n\ntrain['Product_Category_3_Count']=train['Product_Category_3_Count'].fillna(0)\ntest['Product_Category_3_Count']=test['Product_Category_3_Count'].fillna(0)","44ccb654":"round((test.isnull().sum()\/len(test.index))*100,2)","c7a53f0d":"train=train.drop(['User_ID','Product_ID'],axis=1)\ntest=test.drop(['User_ID','Product_ID'],axis=1)","9b90689e":"train.head()","68967586":"q1 = train['Purchase'].quantile(0.25)\nq3 = train['Purchase'].quantile(0.75)\niqr = q3-q1 #Interquartile range\nfence_low  = q1-1.5*iqr\nfence_high = q3+1.5*iqr\ntrain = train[(train['Purchase'] > fence_low) & (train['Purchase'] < fence_high)]","b8d8a677":"\nX=train.drop('Purchase',1)\ny=train['Purchase']\n","6239154d":"# # Create the parameter grid based on the results of random search \n# param_grid = {\n# 'max_depth': [10], 'max_features': [10], 'min_samples_leaf': [100], \n# 'min_samples_split': [200], 'n_estimators': [200]\n# }\n# # Create a based model\n# rf = RandomForestRegressor()\n# # Instantiate the grid search model\n# grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n#                           cv = 3, n_jobs = -1,verbose = 1)","504c91e0":"# Fit the grid search to the data\n# grid_search.fit(X, y)","16d3e10a":"# printing the optimal accuracy score and hyperparameters\n# print('We can get accuracy of',grid_search.best_score_,'using',grid_search.best_params_)\n# We can get accuracy of 0.6598673565353703 using {'max_depth': 10, 'max_features': 10, 'min_samples_leaf': 100, 'min_samples_split': 200, 'n_estimators': 200}","e770e2cf":"# # plotting accuracies with max_depth\n# plt.figure()\n# plt.plot(scores[\"param_max_depth\"], \n#          scores[\"mean_train_score\"], \n#          label=\"training accuracy\")\n# plt.plot(scores[\"param_max_depth\"], \n#          scores[\"mean_test_score\"], \n#          label=\"test accuracy\")\n# plt.xlabel(\"max_depth\")\n# plt.ylabel(\"Accuracy\")\n# plt.legend()\n# plt.show()\n","c986e575":"# y_test_pred=grid_search.predict(test)","6d7caf25":"# finalpred=pd.concat([test_cpy['User_ID'],test_cpy['Product_ID'],pd.DataFrame(y_test_pred,columns=['Purchase'])],1)\n# finalpred.to_csv(\"RF.csv\",index=False)","cf231f4b":"import xgboost as xgb \nfrom xgboost.sklearn import XGBRegressor","d9ea6b2c":"params = {}\nparams[\"eta\"] = 0.03\nparams[\"min_child_weight\"] = 10\nparams[\"subsample\"] = 0.8\nparams[\"colsample_bytree\"] = 0.7\nparams[\"max_depth\"] = 10\nparams[\"seed\"] = 0\nplst = list(params.items())\nnum_rounds = 1100","32dcd61e":"xgb=XGBRegressor()","a3e7bf6a":"xgb.fit(X,y)","4d555825":"y_test_pred_x=xgb.predict(test)","a6121b25":"finalpred=pd.concat([test_cpy['User_ID'],test_cpy['Product_ID'],pd.DataFrame(y_test_pred_x,columns=['Purchase'])],1)\nfinalpred.to_csv(\"xgb.csv\",index=False)","fbcb2157":"import lightgbm as lgb","80c51748":"lgbm=lgb.LGBMRegressor()","5fc94bfb":"# params={'num_leaves':[200], 'objective':['regression'],'max_depth':[15],'learning_rate':[.1],'max_bin':[200]}","fd25fb8a":"# model = GridSearchCV(lgbm,\n#                         params,\n#                         cv = 3,\n#                         n_jobs = 5,\n#                         verbose=True)\n","5624fe8f":"# model.fit(X,y)","6a189021":"# y_test_pred_l=model.predict(test)","4d09e767":"# finalpred=pd.concat([test_cpy['User_ID'],test_cpy['Product_ID'],pd.DataFrame(y_test_pred_l,columns=['Purchase'])],1)\n# finalpred.to_csv(\"lgbm.csv\",index=False)","11e17edc":"import catboost as cb","42d0027e":"model=cb.CatBoostRegressor()","bf765d0d":"grid = {'learning_rate': [0.1],\n        'depth': [10],\n        'l2_leaf_reg': [15]}\n\nmodel = GridSearchCV(model,\n                        grid,\n                        cv = 3,\n                        n_jobs = 5,\n                        verbose=True)","b6e0e7c5":"model.fit(X,y)","d6f5e044":"y_test_predict_c=model.predict(test)","eb0dcfe6":"finalpred=pd.concat([test_cpy['User_ID'],test_cpy['Product_ID'],pd.DataFrame(y_test_predict_c,columns=['Purchase'])],1)\nfinalpred.to_csv(\"catb_1.csv\",index=False)","0310bbd3":"alist = ['Gender',\n'Age',\n'Occupation',\n'City_Category',\n'Stay_In_Current_City_Years',\n'Marital_Status',\n'Product_Category_1',\n'Product_Category_2',\n'Product_Category_3',\n'User_ID_Count',\n'Product_ID_Count']\n         \nblist = ['User_ID_MinPrice',\n'User_ID_MaxPrice',\n'User_ID_MeanPrice',\n'Product_ID_MinPrice',\n'Product_ID_MaxPrice',\n'Product_ID_MeanPrice']\n\nclist = ['User_ID_25PercPrice',\n'User_ID_75PercPrice',\n'Product_ID_25PercPrice',\n'Product_ID_75PercPrice',\n'Product_Category_1_Count',\n'Product_Category_2_Count',\n'Product_Category_3_Count',]\n","b2668949":"#XGB model 1 dataframe\ntrain1 = train[alist+blist]\ntest1 = test[alist+blist]\n\n#XGB model 2 dataframe \ntrain2 = train[alist+clist]\ntest2 = test[alist+clist]","1a527804":"mod_1=lgb.LGBMRegressor(learning_rate=[.2],importance_type='gain')\nmod_2=lgb.LGBMRegressor(learning_rate=[.4],importance_type='gain')","d9628478":"X_train,X_test,Y_train,Y_test = train_test_split(train1,y,test_size=0.2,random_state=42)","cc23ecb1":"mod_1.fit(X_train,Y_train)","aaa3b63b":"y_test=mod_1.predict(X_test)","4f63826a":"from sklearn.metrics import mean_squared_error\nmse=mean_squared_error(Y_test,y_test)\nprint('Root Mean Square Value: ',np.sqrt(mse))","09588abd":"# feature_important \n\nfeature_important = pd.DataFrame({'Features':X_train.columns,'Importance':mod_1.feature_importances_})\n\nkeys = list(X_train.columns)\nvalues = list(mod_1.feature_importances_)\ntotal = sum(values)\nnew = [value * 100. \/ total for value in values]\nnew = np.round(new,2)\n\nfeature_importances = pd.DataFrame()\nfeature_importances['Features'] = keys\nfeature_importances['Importance (%)'] = new\n\n\nfeature_importances = feature_importances.sort_values(['Importance (%)'],ascending=False).reset_index(drop=True)\nfeature_importances\nfeature_importances.style.set_properties(**{'font-size':'10pt'})","145f59ac":"plt.figure(figsize=(20, 8))\nsns.barplot(data=feature_importances, x='Importance (%)', y='Features');\nplt.title('Feature importance',fontsize=24)\nplt.xlabel('Importance (%)',fontsize=20)\nplt.yticks(fontsize=15)\nplt.xticks(fontsize=15)\nplt.ylabel('Features',fontsize=20)","b630ee4d":"#prediction1\npred_lgbm_m1 = mod_1.predict(test1)\nsub=pd.concat([test_cpy['User_ID'],test_cpy['Product_ID'],pd.DataFrame(pred_lgbm_m1,columns=['Purchase'])],1)\nsub.to_csv('lgbm_mod1.csv',index=False)","d6767e45":"X_train,X_test,Y_train,Y_test = train_test_split(train2,y,test_size=0.2,random_state=42)","ae46bbc3":"mod_2.fit(X_train,Y_train)","e46518b3":"y_test=mod_2.predict(X_test)","5eb5a2d6":"from sklearn.metrics import mean_squared_error\nmse=mean_squared_error(Y_test,y_test)\nprint('Root Mean Square Value: ',np.sqrt(mse))","338c7b9b":"# feature_important \n\nfeature_important = pd.DataFrame({'Features':X_train.columns,'Importance':mod_2.feature_importances_})\n\nkeys = list(X_train.columns)\nvalues = list(mod_2.feature_importances_)\ntotal = sum(values)\nnew = [value * 100. \/ total for value in values]\nnew = np.round(new,2)\n\nfeature_importances = pd.DataFrame()\nfeature_importances['Features'] = keys\nfeature_importances['Importance (%)'] = new\n\n\nfeature_importances = feature_importances.sort_values(['Importance (%)'],ascending=False).reset_index(drop=True)\nfeature_importances\nfeature_importances.style.set_properties(**{'font-size':'10pt'})","a3fa38c1":"plt.figure(figsize=(20, 8))\nsns.barplot(data=feature_importances, x='Importance (%)', y='Features');\nplt.title('Feature importance',fontsize=24)\nplt.xlabel('Importance (%)',fontsize=20)\nplt.yticks(fontsize=15)\nplt.xticks(fontsize=15)\nplt.ylabel('Features',fontsize=20)","d5f01784":"#prediction2\npred_lgbm_m2 = mod_2.predict(test2)\nsub=pd.concat([test_cpy['User_ID'],test_cpy['Product_ID'],pd.DataFrame(pred_lgbm_m2,columns=['Purchase'])],1)\nsub.to_csv('lgbm_mod2.csv',index=False)","8fa61faa":"## Weighted average of above two models","7d48d745":"sub['Purchase'] = 0.5*pred_lgbm_m1 + 0.5*pred_lgbm_m2\nsub.to_csv('final.csv',index=False)","6d9ef1ad":"### CATBoost is the final model, as it has performed better than the stacked model","d2bc812c":"\n\nCreating 2 LightBoost Models both with different learning rates and different set of features for both of them\n\n   Light Boost model 1 (learning rate = 0.5), and features set = alist + blist (mentioned below)\n   \n   Light Boost model 2 (learinng rate = 0.8), and features set = alist + clist** (mentioned below)\n\n","e3dd5978":"## Stacked Models","a30c183f":"we can say higher number of males purchase.\n\nMaximum buyers are in the age group 18-45.\n\nCity category B type has max buyers than C and least A.\n\nThe ones new in the city are heavy buyers.","eaa6f6e1":"## Random Forest","e42ae29f":"### Catboost","1accc4e7":"### XGBOOST ","f2ed7161":"### Light GBM"}}