{"cell_type":{"8a292fcb":"code","ddfad3b0":"code","28612296":"code","0d259b75":"code","f913c2aa":"code","a9b69444":"code","9f84626e":"code","b0cad200":"code","80ef3ed4":"code","8e4001bd":"code","1bb7eb82":"code","81bca02c":"code","84262a71":"code","2276bd39":"code","16568efe":"code","d442376a":"code","fe917542":"code","d63918e6":"code","e70e5180":"code","ec95866e":"code","3045c489":"code","89e53e94":"code","de4676a5":"code","ad4ebac8":"code","b5125dd6":"code","8377601d":"code","d1866432":"code","dc062144":"code","33de2445":"code","e51dd913":"code","10db1bcd":"code","eae8b96d":"code","6010e061":"code","67bc5590":"code","79cf2c16":"code","c0bfd610":"code","f67e17a2":"code","16ee2eea":"code","46ef7e5f":"code","7e8fafcc":"code","a4b1a07a":"code","4577f4f0":"code","47ef7733":"code","85303d5a":"code","b4e4f7e9":"code","201330b2":"code","247af3e3":"code","e213d349":"code","35d89b4e":"code","23fa1ef9":"code","c0737387":"code","6029f05a":"code","d5667286":"code","4f446ede":"code","f8f2ce37":"code","fde52a32":"code","b7679291":"code","2d6b953c":"code","3317ae68":"code","30815555":"code","8231665a":"code","806bdecd":"code","e6552fd7":"code","ec3335d6":"code","dc373df8":"code","ea61ae0f":"code","bfaf4137":"code","b46940c0":"code","35596b95":"code","0d7ead74":"code","b5ef111f":"code","20f40569":"code","d580f6e7":"code","a68b47e8":"code","471e36ff":"code","0a37abce":"markdown","c2833efa":"markdown","45722845":"markdown","344e0bef":"markdown","c7133fb6":"markdown"},"source":{"8a292fcb":"# Competition description\n\n'''\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling \nor the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences \nprice negotiations than the number of bedrooms or a white-picket fence.\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition \nchallenges you to predict the final price of each home.\n'''\n\n# Importing libraries\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\nimport sklearn.linear_model as linear_model\nfrom sklearn.linear_model import Lasso\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import make_scorer, r2_score\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import BaggingRegressor, GradientBoostingRegressor\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\n\n\n\n# Ignorar warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n'''\nSteps:\n\n1. Collecting data\n2. Cleaning data\n3. Exploratory data analysis\n4. Model building\n\n'''","ddfad3b0":"%matplotlib inline","28612296":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","0d259b75":"SalePrice = train['SalePrice']","f913c2aa":"SalePrice.shape","a9b69444":"## concatenating train and test\n\ndf = pd.concat((train, test))\nprint(\"Shape of df: \", df.shape)","9f84626e":"df.head(10)","b0cad200":"df.shape","80ef3ed4":"df.columns","8e4001bd":"## Verificando n\u00famero de vari\u00e1veis num\u00e9ricas\n\nnumericalFeatures = df.select_dtypes(include = [np.number])\nprint(\"The number of numerical features is: {}\".format(numericalFeatures.shape[1]))","1bb7eb82":"numericalFeatures.columns","81bca02c":"## Verificando n\u00famero de vari\u00e1veis categ\u00f3ricas\n\ncategoricalFeatures = df.select_dtypes(exclude = [np.number])\nprint(\"The number of categorical features is: {}\".format(categoricalFeatures.shape[1]))","84262a71":"categoricalFeatures.columns","2276bd39":"## Checking data distribution only in the training set\n\nplt.subplots(figsize=(12,9))\nsns.distplot(train['SalePrice'], fit=stats.norm)\n\n# Get the fitted parameters used by the function\n(mu, sigma) = stats.norm.fit(train['SalePrice'])\n\n# Plot with the distribution\nplt.legend(['Normal dist. ($\/mu=$ {:.2f} and $\/sigma=$ {:.2f})'.format(mu, sigma)], loc='best')\n\n# Probability plot\nfig=plt.figure()\nstats.probplot(train['SalePrice'], plot=plt)\nplt.show()","16568efe":"corr = numericalFeatures.corr()\n\nsns.set(style=\"white\")\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(20, 10))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","d442376a":"## Correlation greater than 0.5\n\ntop_feature = corr.index[abs(corr['SalePrice']>0.5)]\nplt.subplots(figsize=(12,8))\ntop_corr = df[top_feature].corr()\nsns.heatmap(top_corr, annot=True)\nplt.show()","fe917542":"catFeatures = categoricalFeatures.columns\ntrain[catFeatures] = train[catFeatures].fillna('Missing')\n\n# Onward...\nanova = {'feature':[], 'f':[], 'p':[]}\nfor cat in catFeatures:\n  group_prices = []\n  for group in train[cat].unique():\n      group_prices.append(train[train[cat] == group]['SalePrice'].values)\n  f, p = stats.f_oneway(*group_prices)\n  anova['feature'].append(cat)\n  anova['f'].append(f)\n  anova['p'].append(p)\nanova = pd.DataFrame(anova)\nanova = anova[['feature','f','p']]\nanova.sort_values('p', inplace = True)","d63918e6":"anova","e70e5180":"df = df.drop(['SalePrice', 'Id'], axis=1)","ec95866e":"# Checando colunas com valores nulos\nnullValues = (df.isnull().sum() \/ len(df)) * 100\nnullValues = round(nullValues.drop(nullValues[nullValues == 0].index).sort_values(ascending=False)[:30],2)\nmissingData = pd.DataFrame({'Percente of null values' :nullValues})\nmissingData.head(30)","3045c489":"df.shape","89e53e94":"### Percentage of null values\n\nf, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=nullValues.index, y=nullValues)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of null values', fontsize=15)\nplt.title('Percent null values by feature', fontsize=15)\nplt.show()","de4676a5":"## Change the categorical features related to the quality of the house\n\nnumber = LabelEncoder()\ndf['Alley'] = number.fit_transform(df['Alley'].astype('str'))\ndf['LotShape'] = number.fit_transform(df['LotShape'].astype('str'))\ndf['LandContour'] = number.fit_transform(df['LandContour'].astype('str'))\ndf['Utilities'] = number.fit_transform(df['Utilities'].astype('str'))\ndf['LandSlope'] = number.fit_transform(df['LandSlope'].astype('str'))\ndf['ExterQual'] = number.fit_transform(df['ExterQual'].astype('str'))\ndf['BsmtQual'] = number.fit_transform(df['BsmtQual'].astype('str'))\ndf['BsmtCond'] = number.fit_transform(df['BsmtCond'].astype('str'))\ndf['BsmtExposure'] = number.fit_transform(df['BsmtExposure'].astype('str'))\ndf['BsmtFinType1'] = number.fit_transform(df['BsmtFinType1'].astype('str'))\ndf['BsmtFinType2'] = number.fit_transform(df['BsmtFinType2'].astype('str'))\ndf['HeatingQC'] = number.fit_transform(df['HeatingQC'].astype('str'))\ndf['KitchenQual'] = number.fit_transform(df['KitchenQual'].astype('str'))\ndf['Functional'] = number.fit_transform(df['Functional'].astype('str'))\ndf['FireplaceQu'] = number.fit_transform(df['FireplaceQu'].astype('str'))\ndf['GarageFinish'] = number.fit_transform(df['GarageFinish'].astype('str'))\ndf['GarageQual'] = number.fit_transform(df['GarageQual'].astype('str'))\ndf['GarageFinish'] = number.fit_transform(df['GarageFinish'].astype('str'))\ndf['GarageCond'] = number.fit_transform(df['GarageCond'].astype('str'))\ndf['PavedDrive'] = number.fit_transform(df['PavedDrive'].astype('str'))\ndf['PoolQC'] = number.fit_transform(df['PoolQC'].astype('str'))\n","ad4ebac8":"train.groupby(['YrSold', 'MoSold']).Id.count().plot(kind='bar', figsize=(14,4))\nplt.title(\"Sale date\")\nplt.show()","b5125dd6":"# Conversion from numeric feature to Category features\n\ndf['MSSubClass'] = df.MSSubClass.apply(lambda x: str(x))\ndf['MoSold'] = df.MoSold.apply(lambda x: str(x))\ndf['YrSold'] = df.YrSold.apply(lambda x: str(x))","8377601d":"df['MSSubClass'] = number.fit_transform(df['MSSubClass'].astype('str'))\ndf['MoSold'] = number.fit_transform(df['MoSold'].astype('str'))\ndf['YrSold'] = number.fit_transform(df['YrSold'].astype('str'))","d1866432":"df.columns[df.isnull().any()]","dc062144":"# Lot Frontage (how can there be no street infront of the lot) Hence we replace it with the median value\ndf.LotFrontage = df.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\n# Garage Year Built, if missing we can set it to zero\ndf.GarageYrBlt.fillna(0, inplace=True)\n\n# Masonary Veneer Area here most values are zero\ndf.MasVnrArea.fillna(0, inplace=True)","33de2445":"df.columns[df.isnull().any()]","e51dd913":"df.Electrical.value_counts()","10db1bcd":"df.Electrical.fillna(df.Electrical.mode()[0], inplace=True)","eae8b96d":"df.MasVnrType.value_counts()","6010e061":"# First let's correct our assignment\ndf.MasVnrType.replace({'Missing':'None'}, inplace=True)\n\n# Second, we are going to replace them with the mean value\ndf.loc[(df.MasVnrType == 'None') & (df.MasVnrArea > 1), 'MasVnrType'] = 'BrkFace' # Most common\ndf.loc[(df.MasVnrType == 'None') & (df.MasVnrArea == 1), 'MasVnrType'] = 0  # M1 sq ft is basically 0\n\nfor vnr_type in df.MasVnrType.unique():\n    # so here we set area equal to the mean of the given veneer type\n    df.loc[(df.MasVnrType == vnr_type) & (df.MasVnrArea == 0), 'MasVnrArea'] = df[df.MasVnrType == vnr_type].MasVnrArea.mean()\n    \ndf.MasVnrType.fillna(df.MasVnrType.mode()[0], inplace=True)    ","67bc5590":"df.GarageType.value_counts()","79cf2c16":"df.GarageType.fillna(df.GarageType.mode()[0], inplace=True)","c0bfd610":"df.Fence.value_counts()","f67e17a2":"df.Fence.fillna(0, inplace=True)","16ee2eea":"df.MiscFeature.value_counts()","46ef7e5f":"df.MiscFeature.fillna(0, inplace=True)","7e8fafcc":"df.GarageArea.fillna(df.GarageArea.mean(), inplace=True)","a4b1a07a":"df.SaleType.value_counts()","4577f4f0":"df.SaleType.fillna(df.SaleType.mode()[0], inplace=True)","47ef7733":"df.GarageCars.value_counts()","85303d5a":"df.GarageCars.fillna(df.GarageCars.mode()[0], inplace=True)","b4e4f7e9":"df.BsmtFinSF1.value_counts()","201330b2":"df.BsmtFinSF1.fillna(df.BsmtFinSF1.mean(), inplace=True)\ndf.BsmtFinSF2.fillna(df.BsmtFinSF2.mean(), inplace=True)","247af3e3":"df.BsmtFullBath.value_counts()","e213d349":"df.BsmtFullBath.fillna(df.BsmtFullBath.mode()[0], inplace=True)\ndf.BsmtHalfBath.fillna(df.BsmtHalfBath.mode()[0], inplace=True)\ndf.Exterior1st.fillna(df.Exterior1st.mode()[0], inplace=True)\ndf.Exterior2nd.fillna(df.Exterior2nd.mode()[0], inplace=True)\ndf.BsmtUnfSF.fillna(df.BsmtUnfSF.mode()[0], inplace=True)\ndf.MSZoning.fillna(df.MSZoning.mode()[0], inplace=True)\ndf.TotalBsmtSF.fillna(df.TotalBsmtSF.mean(), inplace=True)","35d89b4e":"df.columns[df.isnull().any()]","23fa1ef9":"# Calculating total square feet (area)\n\ndf['Total_SF'] = df.TotalBsmtSF + df.GrLivArea\ndf['TotalFloorSF'] = df['1stFlrSF'] + df['2ndFlrSF']\ndf['TotalPorchSF'] = df.OpenPorchSF + df.EnclosedPorch + df['3SsnPorch'] + df['ScreenPorch']","c0737387":"# Now let's create some boolean features (Yes-No type)\n\ndf['HasBasement'] = df.TotalBsmtSF.apply(lambda x: 1 if x>0 else 0)\ndf['HasGarage'] = df.GarageArea.apply(lambda x: 1 if x>0 else 0)\ndf['HasPorch'] = df.TotalPorchSF.apply(lambda x: 1 if x>0 else 0)\ndf['HasPool'] = df.PoolArea.apply(lambda x: 1 if x>0 else 0)\ndf['WasRemodeled'] = (df.YearRemodAdd != df.YearBuilt).astype(np.int64)\ndf['IsNew'] = (df.YearBuilt > 2000).astype(np.int64)\ndf['WasCompleted'] = (df.SaleCondition != 'Partial').astype(np.int64)","6029f05a":"booleanFeatures = ['HasBasement','HasGarage','HasPorch','HasPool','WasRemodeled','IsNew','WasCompleted']","d5667286":"numericalFeatures = numericalFeatures.drop(['Id','SalePrice'], axis=1)\nnumFeatures = numericalFeatures.columns\ncatFeatures = categoricalFeatures.columns","4f446ede":"numFeatures = [f for f in numFeatures if f not in booleanFeatures]","f8f2ce37":"# Total Bathrooms\n\ndf['TotalBathrooms'] = df.FullBath + 0.5*df.HalfBath + df.BsmtFullBath + 0.5*df.BsmtHalfBath","fde52a32":"for f in numFeatures:\n  df.loc[:,f] = np.log1p(df[f])","b7679291":"SalePrice = np.log1p(SalePrice)","2d6b953c":"df = pd.get_dummies(df).copy()","3317ae68":"dfColumns = df.columns","30815555":"df.head()","8231665a":"## Checando distribui\u00e7\u00e3o dos dados no dataset de treinamento\n\nplt.subplots(figsize=(12,9))\nsns.distplot(SalePrice, fit=stats.norm)\n\n# Get the fitted parameters used by the function\n(mu, sigma) = stats.norm.fit(SalePrice)\n\n# Plot with the distribution\nplt.legend(['Normal dist. ($\/mu=$ {:.2f} and $\/sigma=$ {:.2f})'.format(mu, sigma)], loc='best')\n\n# Probability plot\nfig=plt.figure()\nstats.probplot(SalePrice, plot=plt)\nplt.show()","806bdecd":"# scaling dataset with robust scaler\n\nscaler = StandardScaler()\n\ndf.loc[:, numFeatures] = scaler.fit_transform(df[numFeatures])","e6552fd7":"trainLen = len(train)\ny_train = SalePrice\nx_train = df[:trainLen]\nx_test = df[trainLen:]\n\n\nprint(x_train.shape)\nprint(x_test.shape)\nprint(len(y_train))","ec3335d6":"def test_model(model, x_train, y_train):\n    cv = KFold(n_splits = 3, shuffle=True, random_state = 45)\n    r2 = make_scorer(r2_score)\n    r2_val_score = cross_val_score(model, x_train, y_train, cv=cv, scoring = r2)\n    score = [r2_val_score.mean()]\n    return score","dc373df8":"def rsme(model, x, y):\n  cv_scores = -cross_val_score(model, x, y, scoring='neg_mean_squared_error', cv=10)\n  return np.sqrt(cv_scores)","ea61ae0f":"## Tuning parameters\n\nparam_grid = {'alpha':[0.0001,0.001,0.01,1.,5.,10.,25.],'max_iter':[50000]}\nlasso = GridSearchCV(Lasso(), cv=5, param_grid=param_grid, scoring='neg_mean_squared_error')\nlasso.fit(x_train, y_train)\nalpha = lasso.best_params_['alpha']\n\n# Home in\nparam_grid = {'alpha':[x\/100. * alpha for x in range(50,150,5)],'max_iter':[50000]}\nlasso = GridSearchCV(Lasso(), cv=5, param_grid=param_grid, scoring='neg_mean_squared_error')\nlasso.fit(x_train, y_train)\nalpha = lasso.best_params_['alpha']\nlasso = lasso.best_estimator_\n\nprint('Lasso -> Train RSME: {:,.5f}| alpha {:,.5f}'.format(rsme(lasso,x_train,y_train).mean(),alpha))","bfaf4137":"coefs = pd.DataFrame({'coefs':lasso.coef_,'Positive':lasso.coef_>0}, index=dfColumns)\ncoefs['coefs_abs'] = np.abs(coefs.coefs)\nprint('Lasso dropped {} of {} features.'.format(sum(coefs.coefs==0), coefs.shape[0]))\n\ntop_coefs = coefs.sort_values('coefs_abs', ascending=False).head(30)\nplt.figure(figsize=(8,10))\nsns.barplot(top_coefs.coefs_abs, top_coefs.index, orient='h', hue=top_coefs.Positive)\nplt.title=('Lasso Regression: Top Features')\nplt.xlabel('Absolute Coeficient')\nplt.show()","b46940c0":"# Linear Regression\n\nLR = linear_model.LinearRegression()\nacc_LR = test_model(LR, x_train, y_train)\n\nLR_rsme = rsme(LR, x_train, y_train)\n\nprint('Score: {:.5f}'.format((acc_LR[0])))\nprint('RSME: {:.5f}'.format(LR_rsme.mean()))","35596b95":"# Support Vector Regressor\n\nsvr_reg = SVR(kernel='rbf')\nacc_SVR = test_model(svr_reg, x_train, y_train)\n\nsvr_rsme = rsme(svr_reg, x_train, y_train)\nprint('Score: {:.5f}'.format((acc_SVR[0])))\nprint('RSME: {:.5f}'.format(svr_rsme.mean()))","0d7ead74":"#Decision Tree\ndt_reg = DecisionTreeRegressor(random_state=21)\nacc_tree = test_model(dt_reg, x_train, y_train)\n\ndt_rsme = rsme(dt_reg, x_train, y_train)\nprint('Score: {:.5f}'.format((acc_tree[0])))\nprint('RSME: {:.5f}'.format(dt_rsme.mean()))","b5ef111f":"# Random Forest\nrf_reg = RandomForestRegressor(n_estimators = 1000, random_state=51)\nacc_rf = test_model(rf_reg, x_train, y_train)\n\nrf_rsme = rsme(rf_reg, x_train, y_train)\nprint('Score: {:.5f}'.format((acc_rf[0])))\nprint('RSME: {:.5f}'.format(rf_rsme.mean()))","20f40569":"# Bagging Regressor\nbr_reg = BaggingRegressor(n_estimators=1000, random_state=51)\nacc_br = test_model(br_reg, x_train, y_train)\n\nbr_rsme = rsme(br_reg, x_train, y_train)\nprint('Score: {:.5f}'.format((acc_br[0])))\nprint('RSME: {:.5f}'.format(br_rsme.mean()))","d580f6e7":"# Gradient Boosting Regressor\ngbr_reg = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.1, loss='ls', random_state=51)\nacc_gbr = test_model(gbr_reg, x_train, y_train)\n\ngbr_rsme = rsme(gbr_reg, x_train, y_train)\nprint('Score: {:.5f}'.format((acc_gbr[0])))\nprint('RSME: {:.5f}'.format(gbr_rsme.mean()))","a68b47e8":"# XGBoost\n\nxgb_reg = xgb.XGBRegressor(colsample_bytree=0.2, \n                        gamma=0.0,\n                        learning_rate=0.05,\n                        max_depth=6,\n                        min_child_weight=1.5,\n                        n_estimators=7200,\n                        reg_alpha=0.9,\n                        reg_lambda=0.6,\n                        subsample=0.2,\n                        seed=42,\n                        silent=1)\n\nacc_xgb = test_model(xgb_reg,x_train[top_coefs.index], y_train)\nxgb_rsme = rsme(xgb_reg, x_train[top_coefs.index], y_train)\n\nprint('Score: {:.5f}'.format((acc_xgb[0])))\nprint('RSME: {:.5f}'.format(xgb_rsme.mean()))","471e36ff":"results = pd.DataFrame({\n    'Model': ['Linear Regression', 'Support Vector Regressor', \n              'Decision Tree', 'Random Forest', 'Bagging Regressor', 'Gradient Boosting Regressor ','XGBoost'],\n    'Score': [acc_LR[0], acc_SVR[0], acc_tree[0], acc_rf[0], acc_br[0], acc_gbr[0], acc_xgb[0]],\n    'RSME': [LR_rsme[0], svr_rsme[0], dt_rsme[0], rf_rsme[0], br_rsme[0], gbr_rsme[0], xgb_rsme[0]]\n})\n\nresult = results.sort_values(by='RSME', ascending=True)\nresult = result.set_index('Model')\ndisplay(result.head(8))","0a37abce":"# Loading data","c2833efa":"# Building models","45722845":"# Correlation\n","344e0bef":"Null hypothesis (H0): There is no difference\n\nIf P<0.05 we can reject H0\n\nThe features Street, LandSlope and Utilities have P>0.05, which means, they make difference in sales price.","c7133fb6":"# Data distribution"}}