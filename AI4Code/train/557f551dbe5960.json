{"cell_type":{"47729838":"code","e76d5519":"code","a46a76ba":"code","53a1cc14":"code","719a9725":"code","8e1885a8":"code","57c5fe72":"code","3424d53f":"code","cc5257dd":"code","8841a704":"code","f3a89b3d":"code","d2eb5ae8":"code","db2d15af":"code","aa7602bc":"code","ed7016a8":"code","427662e7":"code","d4ead284":"code","cfb3940b":"code","4c0db4d7":"code","90cef350":"code","2660ca44":"code","a1afe60f":"code","542a3b39":"code","f0a20195":"code","d56901bd":"code","48f7b4b6":"code","550d76b7":"code","8f2dee7d":"code","5eadd88b":"code","aebe32da":"code","6f01918f":"code","a06e6f89":"code","43ef5dc8":"code","1aeda186":"code","4ab7e831":"code","59e577e0":"code","78e0b424":"code","7b60b9a7":"code","74bee7b3":"code","7a506f39":"code","f63f06dc":"code","3892a2e8":"code","2b6f6210":"code","72008076":"code","4af7d939":"code","5b784ca5":"code","dff65b0a":"code","7562f605":"code","2e48d747":"code","5834856e":"code","2af6c2f8":"code","84953734":"code","3337bf93":"code","9cda3997":"code","2fd0e984":"code","2372ea86":"code","30a6e67b":"code","17704c75":"code","e51eda4f":"code","7d89430b":"code","56f4c6b0":"code","c606fa80":"code","1eea50a4":"code","a718db0f":"code","d4d62f8d":"code","bf784145":"code","b245bde9":"code","a80d26c4":"code","20da7112":"code","4ed45ae2":"code","8b299e33":"code","4f38030b":"code","5e4023fa":"code","08bfc783":"code","519cc6d8":"code","23569da9":"code","761d5d78":"code","3b9a1309":"code","d59de226":"code","837ad476":"code","9467173f":"code","ba42b29e":"code","ff257ab4":"code","83e5e113":"code","f3cdb0a7":"code","46a4365f":"code","c9fb17b4":"code","ff326de0":"code","a4b8b46a":"code","637921ce":"code","3ecb8d92":"code","c22b7bdd":"code","75169748":"code","5be22f38":"code","b8b82163":"code","eef3ef1f":"code","27b82601":"code","500d0333":"code","3196fd32":"markdown","222cdee4":"markdown","2a6db21f":"markdown","ad590a88":"markdown","0d8f188f":"markdown","e759ca3a":"markdown","940b7040":"markdown","c1cbd5a6":"markdown","c3f8d953":"markdown","abe0c1dd":"markdown","3076b9f3":"markdown","d150ce01":"markdown","9250137d":"markdown","fa2a2001":"markdown","9ed23661":"markdown","2bbdcc5f":"markdown","fe2f2ad5":"markdown","a673e44a":"markdown","c1eeccf7":"markdown","756d30c2":"markdown","f157b679":"markdown","1d87b649":"markdown","65941373":"markdown","422e3294":"markdown","a11c90e2":"markdown","dfc036b9":"markdown","1a55dd4d":"markdown","d48789db":"markdown","c622b535":"markdown","1c52b6a6":"markdown","467ba49d":"markdown","0095b87c":"markdown","162ca35e":"markdown","7885b209":"markdown","44918cce":"markdown","92b487d8":"markdown","9b17e299":"markdown","41910814":"markdown","d528baad":"markdown","f7641b9e":"markdown","74205296":"markdown","b17b581f":"markdown"},"source":{"47729838":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e76d5519":"!pip3 install catboost","a46a76ba":"!pip3 install xgboost==0.71\n!pip install mlxtend==0.18.0","53a1cc14":"# Essentials\nimport numpy as np\nimport pandas as pd\nimport datetime\nimport random\n\n# Plots\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Models\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor, ExtraTreesClassifier, StackingClassifier\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import RidgeClassifier, RidgeCV\nfrom sklearn.linear_model import ElasticNet, ElasticNetCV, LogisticRegressionCV\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\n#from mlxtend.classifier import StackingCVClassifier\nimport six\nimport sys\nsys.modules['sklearn.externals.six'] = six\nfrom mlxtend.classifier import StackingCVClassifier\n\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import Pool, CatBoostClassifier\n\n# Stats\nfrom scipy.stats import skew, norm\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\n# Misc\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, validation_curve\nfrom sklearn.metrics import log_loss, confusion_matrix\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import scale\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import mutual_info_classif\n\npd.set_option('display.max_columns', None)\n\n# Ignore useless warnings\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\npd.options.display.max_seq_items = 8000\npd.options.display.max_rows = 8000\n\nimport os\nos.listdir(\"..\/input\/\")","719a9725":"# Read in the dataset as a dataframe\ntrain = pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/sample_submission.csv\")\n\ntrain.info()\n#test.info()\n#submission.info()","8e1885a8":"# Split features and labels\ntrain_labels = train['target'].reset_index(drop=True)\ntrain_features = train.drop(['id','target'], axis=1)\ntest_features = test.drop(['id'], axis=1)\ntrain_labels.head()","57c5fe72":"\nsns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n#Check the new distribution \nsns.histplot(train['target'].sort_values(), color=\"b\");\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"Target\")\nax.set(title=\"Target distribution\")\nsns.despine(trim=True, left=True)\nplt.show()\n","3424d53f":"train['target'].value_counts().sort_values(ascending=False)\/sum(train['target'].value_counts())","cc5257dd":"\n# visualising some more outliers in the data values\nfig, axs = plt.subplots(ncols=2, nrows=1, figsize=(12, 120))\nplt.subplots_adjust(right=2)\nplt.subplots_adjust(top=2)\nsns.color_palette(\"husl\", 8)\nfor i, feature in enumerate(list(train_features), 1):\n    plt.subplot(len(list(train_features)), 3, i)\n    sns.boxplot(x=feature, y=train_labels, hue=train_labels, palette='Blues', data=train_features)\n        \n    plt.xlabel('{}'.format(feature), size=15,labelpad=12.5)\n    plt.ylabel('Target', size=15, labelpad=12.5)\n    \n    for j in range(2):\n        plt.tick_params(axis='x', labelsize=12)\n        plt.tick_params(axis='y', labelsize=12)\n    \n    plt.legend(loc='best', prop={'size': 10})\n        \nplt.show()\n","8841a704":"\n# Random Forest Classifier\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\n\nrf_model = rf.fit(train_features, train_labels)\n#rf_pred = rf_model.predict_proba(test_features)\n\nforest_importances = pd.Series(rf.feature_importances_, index=train_features.columns)\ntop_feat = forest_importances.sort_values(ascending = False).head(20)\ntop_feat\n\ntrain_features[top_feat.index]\n","f3a89b3d":"\n#corr = train_features[top_feat.index].corr()\n#corr\ncorr = train.corr()\nplt.subplots(figsize=(15,12))\nsns.heatmap(corr, vmax=0.9, cmap=\"Blues\", square=True)\n","d2eb5ae8":"\ndata = pd.concat([train['feature_38'], train['target']], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=train['feature_38'], y=\"target\", data=data)\n#fig.axis(ymin=0, ymax=800000);\n","db2d15af":"'''\n# feature of zero or nonzero values\n\ndef zeroornot(res, ls):\n    m = res.shape[1]\n    for l in ls:\n        res = res.assign(newcol=pd.Series(res[l] == 0).astype(int)) \n        res.columns.values[m] = l + '_zero'\n        m += 1\n    return res\n\ntrain_features_zero = zeroornot(train_features, train_features.columns.tolist())\ntest_features_zero = zeroornot(test_features, test_features.columns.tolist())\n'''","aa7602bc":"#train_features_zero = train_features_zero.drop(train_features_zero.iloc[:,0:50], axis=1)\n#test_features_zero = test_features_zero.drop(test_features_zero.iloc[:,0:50], axis=1)","ed7016a8":"'''\nX=train_features\n# Standardize\nX_scaled = (X - X.mean(axis=0)) \/ X.std(axis=0)\n\n# Create principal components\npca = PCA()\nX_pca = pca.fit_transform(X_scaled)\n\n# Convert to dataframe\ncomponent_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\nX_pca = pd.DataFrame(X_pca, columns=component_names)\n\nX_pca.head()\n'''","427662e7":"'''\nloadings = pd.DataFrame(\n    pca.components_.T,  # transpose the matrix of loadings\n    columns=component_names,  # so the columns are the principal components\n    index=X.columns,  # and the rows are the original features\n)\nloadings\n'''","d4ead284":"'''\ndef plot_variance(pca, width=8, dpi=100):\n    # Create figure\n    fig, axs = plt.subplots(1, 2)\n    n = pca.n_components_\n    grid = np.arange(1, n + 1)\n    # Explained variance\n    evr = pca.explained_variance_ratio_\n    axs[0].bar(grid, evr)\n    axs[0].set(\n        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 0.05)\n    )\n    # Cumulative Variance\n    cv = np.cumsum(evr)\n    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n    axs[1].set(\n        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n    )\n    # Set up figure\n    fig.set(figwidth=8, dpi=100)\n    return axs\n\n# Look at explained variance\nplot_variance(pca);\n'''","cfb3940b":"'''\ndef make_mi_scores(X, y, discrete_features):\n    mi_scores = mutual_info_classif(X, y, discrete_features=discrete_features)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\nmi_scores = make_mi_scores(X_pca, train_labels, discrete_features=False)\nmi_scores\n'''","4c0db4d7":"'''\nmiindex = mi_scores.index[mi_scores.values>0]\nmiload = loadings[miindex]\n'''","90cef350":"'''\ntrain_features_pca = pd.DataFrame(data = np.matmul(train_features,miload))\ntrain_features_pca.columns=miindex\n'''","2660ca44":"'''\ntest_features_pca = pd.DataFrame(data = np.matmul(test_features,miload))\ntest_features_pca.columns=miindex\n'''","a1afe60f":"# Set up cross validation folds\nkf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n\n# Define error metrics\ndef loss(y, y_pred):\n    return np.sqrt(log_loss(y, y_pred))\n\ndef cv_loss(model, X = train_features):\n    loss = np.sqrt(-cross_val_score(model, X, train_labels, scoring=\"neg_log_loss\", cv=kf, n_jobs=-1))\n    return (loss)","542a3b39":"\n# XGBoost Classifier\nxgb =  XGBClassifier(learning_rate = 0.1,\n                        colsample_bytree = 0.5,\n                        max_depth = 10,\n                        min_child_weight=5,\n                       gamma=0.001,\n                       subsample=0.9,\n                       objective='multi:softprob',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       reg_alpha=0.00006,\n                       random_state=42)\n\n#xgb_model = xgb.fit(train_features, train_labels)\n#xgb_pred = xgb_model.predict_proba(test_features)\n","f0a20195":"\n# XGBoost Classifier2\nxgb2 = XGBClassifier(n_estimators=110,\n                        learning_rate = 0.5,\n                        colsample_bytree = 0.13,\n                       max_depth = 2,\n                        min_child_weight=5,\n                       gamma=0.001,\n                       subsample=0.7,\n                       objective='multi:softprob',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       reg_alpha=0.00006,\n                       random_state=42)\n\n#xgb2_model = xgb2.fit(train_features, train_labels)\n#xgb2_pred = xgb2_model.predict_proba(test_features)","d56901bd":"\n# XGBoost Classifier3\nxgb3 =XGBClassifier(n_estimators=250,\n                        learning_rate = 0.06,\n                        colsample_bytree = 0.4,\n                       max_depth = 6,\n                        min_child_weight=5,\n                       subsample=0.75,\n                       objective='multi:softprob',\n                       nthread=-1,\n                       random_state=42)\n                       \n\n#xgb3_model = xgb3.fit(train_features, train_labels)\n#xgb3_pred = xgb3_model.predict_proba(test_features)","48f7b4b6":"# XGBoost Classifier4\nxgb4 = XGBClassifier(n_estimators=130, \n                        learning_rate = 0.5,\n                        colsample_bytree = 0.13,\n                        max_depth = 2,\n                        min_child_weight=7, \n                       gamma=0.001,\n                       subsample=0.7,\n                       objective='multi:softprob',\n                       nthread=-1,\n                       reg_alpha=0.00006,\n                       random_state=42)\n                       \n\n#xgb4_model = xgb4.fit(train_features, train_labels)\n#xgb4_pred = xgb4_model.predict_proba(test_features)","550d76b7":"# XGBoost Classifier5\nxgb5 = XGBClassifier( n_estimators=200,\n                        learning_rate = 0.5,\n                       colsample_bytree = 0.1,\n                       max_depth = 2,\n                        min_child_weight=5,\n                      gamma=0.005,\n                       subsample=0.7,\n                       objective='multi:softprob',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       reg_alpha=5,\n                         reg_lambda = 0.3,\n                         eval_metric = 'mlogloss',\n                       random_state=42)\n                       \n\n#xgb5_model = xgb5.fit(train_features, train_labels)\n#xgb5_pred = xgb5_model.predict_proba(test_features)","8f2dee7d":"\n# Random Forest Classifier\nrf = RandomForestClassifier(min_samples_split = 5,\n                            min_samples_leaf = 5,\n                            max_depth = None,\n                            bootstrap = True,\n                            n_jobs=-1,\n                            criterion = \"entropy\",\n                            n_estimators=500,\n                            max_features = 12,\n                            random_state = 42)\n'''\nrf_model = rf.fit(train_features, train_labels)\nrf_pred = rf_model.predict_proba(test_features)\n'''","5eadd88b":"\n# Light Gradient Boosting Regressor\nlgb =  LGBMClassifier(objective='multiclass', \n                       num_leaves=6,\n                    max_depth=6,\n                       learning_rate=0.1, \n                       n_estimators=220,\n                       max_bin=200, \n                       bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.7,\n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n\nlgb_model = lgb.fit(train_features, train_labels)\nlgb_pred = lgb_model.predict_proba(test_features)\n","aebe32da":"\n# Light Gradient Boosting Regressor\nlgb2 =  LGBMClassifier(objective='multiclass', \n                       num_leaves=2,\n                    max_depth=1,\n                       learning_rate=0.5, \n                       n_estimators=550,\n                      max_bin=25, \n                       bagging_fraction=0.6,\n                       bagging_freq=8, \n                       bagging_seed=8,\n                      feature_fraction=0.3,\n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n\n#lgb2_model = lgb2.fit(train_features, train_labels)\n#lgb2_pred = lgb2_model.predict_proba(test_features)","6f01918f":"\n# Light Gradient Boosting Regressor\nlgb3 = LGBMClassifier(objective='multiclass', \n                      min_data_in_leaf=7, \n                    max_depth=4,\n                       learning_rate=0.066, \n                       n_estimators=200,\n                       bagging_fraction=0.75,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.3,\n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n\n#lgb3_model = lgb3.fit(train_features, train_labels)\n#lgb3_pred = lgb3_model.predict_proba(test_features)","a06e6f89":"\n# Light Gradient Boosting Regressor\nlgb4 = LGBMClassifier(objective='multiclass', \n                       num_leaves=6,\n                    max_depth=6,\n                       learning_rate=0.1, \n                       n_estimators=250, \n                       max_bin=200, \n                       bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.5,\n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n\n#lgb4_model = lgb4.fit(train_features, train_labels)\n#lgb4_pred = lgb4_model.predict_proba(test_features)","43ef5dc8":"\n# Light Gradient Boosting Regressor\nlgb5 = LGBMClassifier(objective='multiclass', \n                       num_leaves=6,\n                    min_data_in_leaf=19, \n                    max_depth=6,\n                       learning_rate=0.1, \n                      n_estimators=310, \n                       max_bin=80, \n                      bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.1, \n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n\n#lgb5_model = lgb5.fit(train_features, train_labels)\n#lgb5_pred = lgb5_model.predict_proba(test_features)","1aeda186":"\n# Light Gradient Boosting Regressor\nlgb6 = LGBMClassifier(objective='multiclass', \n                       num_leaves=6,\n                    min_data_in_leaf=19, \n                    max_depth=5,\n                      learning_rate=0.1, \n                      n_estimators=310, \n                       max_bin=20, \n                      bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                      feature_fraction=0.1, \n                       feature_fraction_seed=8,\n                           lambda_l2 = 0.003,\n                           min_gain_to_split = 0.0001,\n                           metric = 'multi_logloss',\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n\n#lgb6_model = lgb6.fit(train_features, train_labels)\n#lgb6_pred = lgb6_model.predict_proba(test_features)","4ab7e831":"\n# Light Gradient Boosting Regressor\nlgb7 = LGBMClassifier(objective='multiclass', \n                       num_leaves=6,\n                    min_data_in_leaf=19, \n                    max_depth=5,\n                      learning_rate=0.1, \n                     n_estimators=500, \n                       max_bin=20, \n                      bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                      feature_fraction=0.1, \n                       feature_fraction_seed=8,\n                          lambda_l1 = 9,\n                           lambda_l2 = 0.003,\n                           min_gain_to_split = 0.0001,\n                           metric = 'multi_logloss',\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n\n#lgb7_model = lgb7.fit(train_features, train_labels)\n#lgb7_pred = lgb7_model.predict_proba(test_features)","59e577e0":"\n# Extra Trees Classifier\next =ExtraTreesClassifier(  min_samples_split = 5,\n                            min_samples_leaf = 5,\n                            max_depth = 15,\n                            bootstrap = True,\n                            n_jobs=-1,\n                            n_estimators=10,\n                            max_features = 20,\n                            random_state = 42,\n                            criterion = 'entropy')\n'''\next_model = ext.fit(train_features, train_labels)\next_pred = ext_model.predict_proba(test_features)\n'''","78e0b424":"\n#CatBoost\ncat_features = train_features.columns.values.tolist()\n\ntrain_dataset = Pool(data=train_features,\n                     label=train_labels,\n                     cat_features=cat_features)\n\neval_dataset = Pool(data=test_features,\n                    cat_features=cat_features)\n\n# Initialize CatBoostClassifier\ncat =  CatBoostClassifier(n_estimators=500,\n                           learning_rate=0.3,\n                           max_depth=2,\n                           loss_function='MultiClass',\n                          random_state=17,\n                          thread_count=-1,\n                            colsample_bylevel=0.5,\n                            min_data_in_leaf=5,\n                            verbose=0)\n# Fit model\n#cat.fit(train_dataset)\n# Get predicted probabilities for each class\n#cat_pred = cat.predict_proba(eval_dataset)\n","7b60b9a7":"\n#CatBoost2\n\n\n# Initialize CatBoostClassifier\ncat2 = CatBoostClassifier(n_estimators=550,\n                           learning_rate=0.5,\n                           max_depth=1,\n                           loss_function='MultiClass',\n                          random_state=17,\n                          thread_count=-1,\n                            colsample_bylevel=0.4,\n                            min_data_in_leaf=5,\n                            verbose=0)\n# Fit model\n#cat2.fit(train_dataset)\n# Get predicted probabilities for each class\n#cat2_pred = cat2.predict_proba(eval_dataset)","74bee7b3":"\n#CatBoost3\n\n\n# Initialize CatBoostClassifier\ncat3 = CatBoostClassifier(n_estimators=250,\n                           learning_rate=0.1,\n                           max_depth=6,\n                           loss_function='MultiClass',\n                          random_state=17,\n                          thread_count=-1,\n                            colsample_bylevel=0.4,\n                            min_data_in_leaf=3,\n                            verbose=0)\n\n# Fit model\n#cat3.fit(train_dataset)\n# Get predicted probabilities for each class\n#cat3_pred = cat3.predict_proba(eval_dataset)","7a506f39":"#CatBoost4\n\n# Initialize CatBoostClassifier\n\ncat4 = CatBoostClassifier(n_estimators=600,\n                           learning_rate=0.6,\n                           max_depth=1,\n                           loss_function='MultiClass',\n                          random_state=17,\n                          thread_count=-1,\n                            colsample_bylevel=0.4,\n                            min_data_in_leaf=5,\n                            verbose=0)\n\n\n# Fit model\n#cat4.fit(train_dataset)\n# Get predicted probabilities for each class\n#cat4_pred = cat4.predict_proba(eval_dataset)","f63f06dc":"#CatBoost5\n\n# Initialize CatBoostClassifier\n\ncat5 = CatBoostClassifier(n_estimators=600,\n                           learning_rate=0.6,\n                           max_depth=1,\n                           loss_function='MultiClass',\n                          random_state=17,\n                          thread_count=-1,\n                            colsample_bylevel=0.4,\n                            min_data_in_leaf=5,\n                               bagging_temperature = 0.2,\n                             sampling_frequency = 'PerTreeLevel',\n                               reg_lambda = 3,\n                            verbose=0)\n\n# Fit model\n#cat5.fit(train_dataset)\n# Get predicted probabilities for each class\n#cat5_pred = cat5.predict_proba(eval_dataset)","3892a2e8":"# %%time\n# Stack up all the models above, optimized using lgb\nstack_gen = StackingCVClassifier(classifiers = (rf, lgb, xgb, ext),\n                                meta_classifier = lgb,\n                                use_probas= True,\n                                use_features_in_secondary=True, \n                                verbose=2,\n                                n_jobs=-1,\n                                random_state=17\n                                )\n\n#stack_gen_model = stack_gen.fit(np.array(train_features), np.array(train_labels))\n#stack_pred = stack_gen_model.predict_proba(np.array(test_features))\n","2b6f6210":"\n%%time\n# Stack2\n# Stack up all the models above, optimized using cat\nstack_gen2 = StackingCVClassifier(classifiers = (xgb2, lgb, cat),\n                                meta_classifier = cat,\n                                 use_probas= True,\n                                use_features_in_secondary=True, \n                                verbose=2,\n                                n_jobs=-1,\n                                random_state=17)\n\n#stack_gen2_model = stack_gen2.fit(np.array(train_features), np.array(train_labels))\n#stack2_pred = stack_gen2_model.predict_proba(np.array(test_features))\n","72008076":"%%time\n# Stack4\n# Stack up all the models above, optimized using cat2\nstack_gen4 = StackingCVClassifier(classifiers = (xgb2,lgb, cat2),\n                                meta_classifier = cat,\n                                 use_probas= True,\n                                use_features_in_secondary=True, \n                                verbose=2,\n                                n_jobs=-1,\n                                random_state=17\n                                )\n\n#stack_gen4_model = stack_gen4.fit(np.array(train_features), np.array(train_labels))\n#stack4_pred = stack_gen4_model.predict_proba(np.array(test_features))","4af7d939":"%%time\n# Stack5\n# Stack up all the models above, optimized using cats5\n\ncats5 = CatBoostClassifier(n_estimators=450, \n                           learning_rate=0.1, \n                           max_depth=1,\n                           loss_function='MultiClass',\n                          random_state=17,\n                          thread_count=-1,\n                            colsample_bylevel=0.5,\n                            min_data_in_leaf=5,\n                            verbose=0)\n                            \n\nstack_gen5 = StackingCVClassifier(classifiers = (xgb2,lgb, cat2),\n                                meta_classifier = cats5,\n                                 use_probas= True,\n                                use_features_in_secondary=True, \n                                 verbose=2,\n                                 n_jobs=-1,\n                                random_state=17,\n                                   stratify = True)\n\n\n#stack_gen5_model = stack_gen5.fit(np.array(train_features), np.array(train_labels))\n#stack5_pred = stack_gen5_model.predict_proba(np.array(test_features))","5b784ca5":"%%time\n# Stack6\n# Stack up all the models above, optimized using lgbs6\n\nlgbs6 = LGBMClassifier(objective='multiclass', \n                       num_leaves=6,\n                    max_depth=6,\n                       learning_rate=0.03, \n                       n_estimators=160, \n                       max_bin=200, \n                       bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.7,\n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n                            \n\nstack_gen6 = StackingCVClassifier(classifiers = (lgb, xgb, cat),\n                                meta_classifier = lgbs6,\n                                 use_probas= True,\n                                use_features_in_secondary=True, \n                                 verbose=2,\n                                 n_jobs=-1,\n                                random_state=17,\n                                   stratify = True)\n\n\n#stack_gen6_model = stack_gen6.fit(np.array(train_features), np.array(train_labels))\n#stack6_pred = stack_gen6_model.predict_proba(np.array(test_features))","dff65b0a":"%%time\n# Stack7\n# Stack up all the models above, optimized using lgbs7\n\n\nlgbs7 = LGBMClassifier(objective='multiclass', \n                      min_data_in_leaf=7, \n                    max_depth=4,\n                       learning_rate=0.05, \n                       n_estimators=100,\n                       bagging_fraction=1,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.3,\n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1\n                   )\n\nstack_gen7 =  StackingCVClassifier(classifiers = (lgb3, xgb3, cat),\n                                meta_classifier = lgbs7,\n                                 use_probas= True,\n                                use_features_in_secondary=True, \n                                 verbose=2,\n                                 n_jobs=-1,\n                                random_state=17,\n                                   stratify = True)\n\n\n#stack_gen7_model = stack_gen7.fit(np.array(train_features), np.array(train_labels))\n#stack7_pred = stack_gen7_model.predict_proba(np.array(test_features))","7562f605":"%%time\n# Stack8\n# Stack up all the models above, optimized using lgbs8\n\n\nlgbs8 = LGBMClassifier(objective='multiclass', \n                     min_data_in_leaf=3, \n                    max_depth=4,\n                       learning_rate=0.05, \n                       n_estimators=100,\n                       bagging_fraction=0.75,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.3,\n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1\n                   )\n\nstack_gen8 = StackingCVClassifier(classifiers = (xgb2, lgb, cat2),\n                                meta_classifier = lgbs8,\n                                 use_probas= True,\n                                use_features_in_secondary=True, \n                                 verbose=2,\n                                 n_jobs=-1,\n                                random_state=17,\n                                   stratify = True)\n\n#stack_gen8_model = stack_gen8.fit(np.array(train_features), np.array(train_labels))\n#stack8_pred = stack_gen8_model.predict_proba(np.array(test_features))","2e48d747":"%%time\n# Stack9\n# Stack up all the models above, optimized using lgbs9\n\n\nlgbs9 = LGBMClassifier(objective='multiclass', \n                       num_leaves=6,\n                   min_data_in_leaf=23, \n                    max_depth=3,\n                       learning_rate=0.05, \n                    n_estimators=270, \n                      max_bin=60, \n                    bagging_fraction=0.7, \n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.1, \n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1\n                   )\n\nstack_gen9 = StackingCVClassifier(classifiers = (xgb4, lgb5, cat4),\n                                meta_classifier = lgbs9,\n                                 use_probas= True,\n                                use_features_in_secondary=True, \n                                 verbose=2,\n                                 n_jobs=-1,\n                                random_state=17,\n                                   stratify = True)\n\n#stack_gen9_model = stack_gen9.fit(np.array(train_features), np.array(train_labels))\n#stack9_pred = stack_gen9_model.predict_proba(np.array(test_features))","5834856e":"%%time\n# Stack10\n# Stack up all the models above, optimized using lgbs10\n\n\nlgbs10 = LGBMClassifier(objective='multiclass', \n                       num_leaves=4,\n                  min_data_in_leaf=400, \n                    max_depth=1, \n                     learning_rate=0.05, \n                   n_estimators=270, \n                      max_bin=60, \n                    bagging_fraction=0.5, \n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.15, \n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1\n                   )\n\n                            \n\nstack_gen10 = StackingCVClassifier(classifiers = (xgb4, lgb5, cat4),\n                                meta_classifier = lgbs10,\n                                 use_probas= True,\n                                use_features_in_secondary= False, \n                                 verbose=2,\n                                 n_jobs=-1,\n                                random_state=17,\n                                   stratify = True)\n\n\n#stack_gen10_model = stack_gen10.fit(np.array(train_features), np.array(train_labels))\n#stack10_pred = stack_gen10_model.predict_proba(np.array(test_features))","2af6c2f8":"%%time\n# Stack11\n# Stack up all the models above, optimized using lgbs11\n\n\nlgbs11 = LGBMClassifier(objective='multiclass', \n                       num_leaves=5,\n                    min_data_in_leaf=19, \n                    max_depth=4,\n                      learning_rate=0.1, \n                     n_estimators=180, \n                       max_bin=20, \n                      bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                      feature_fraction=0.1, \n                       feature_fraction_seed=8,\n                          lambda_l1 = 7,\n                           lambda_l2 = 0.1,\n                           min_gain_to_split = 0.01,\n                           metric = 'multi_logloss',\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n                       \nstack_gen11 = StackingCVClassifier(classifiers = (xgb5, lgb7, cat5),\n                                meta_classifier = lgbs11,\n                                 use_probas= True,\n                                use_features_in_secondary= True, \n                                 verbose=2,\n                                 n_jobs=-1,\n                                random_state=17,\n                                   stratify = True)\n\n\nstack_gen11_model = stack_gen11.fit(np.array(train_features), np.array(train_labels))\nstack11_pred = stack_gen11_model.predict_proba(np.array(test_features))","84953734":"scores = {}\n\nscore = cv_loss(stack_gen11)\nprint(\"rf: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['rf'] = (score.mean(), score.std())\n","3337bf93":"'''\ndef cvpca_loss(model, X = train_features_pca):\n    loss = np.sqrt(-cross_val_score(model, X, train_labels, scoring=\"neg_log_loss\", cv=kf, n_jobs=-1))\n    return (loss)\n\nscores = {}\n\nscore = cvpca_loss(cat2)\nprint(\"rf: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['rf'] = (score.mean(), score.std())\n'''","9cda3997":"lgb_pred = lgb_model.predict(np.array(train_features))","2fd0e984":"unique, counts = np.unique(lgb_pred, return_counts=True)\nnp.asarray((unique, counts)).T","2372ea86":"np.around(confusion_matrix(train_labels,lgb_pred, normalize = 'true'),3)","30a6e67b":"'''\n%%time\nnum_est = [100,200,300]\n\nvc_model = CatBoostClassifier(n_estimators=600,\n                           learning_rate=0.6,\n                           max_depth=1,\n                           loss_function='MultiClass',\n                          random_state=17,\n                          thread_count=-1,\n                            colsample_bylevel=0.4,\n                           # min_data_in_leaf=5,\n                               bagging_temperature = 0.2,\n                             sampling_frequency = 'PerTreeLevel',\n                               reg_lambda = 3,\n                            verbose=0)\n\n\n\n# Calculate accuracy on training and test set using the\n# parameter with 5-fold cross validation\ntrain_score, test_score = validation_curve( vc_model,\n                                X = train_features, y = train_labels, \n                                param_name = 'min_data_in_leaf', \n                                param_range = num_est, cv = kf, scoring=\"neg_log_loss\", n_jobs=-1\n                            )\n \n# Calculating mean and standard deviation of training score\nmean_train_score = -np.mean(train_score, axis = 1)\nstd_train_score = np.std(train_score, axis = 1)\n \n# Calculating mean and standard deviation of testing score\nmean_test_score = -np.mean(test_score, axis = 1)\nstd_test_score = np.std(test_score, axis = 1)\n \n# Plot mean accuracy scores for training and testing scores\nplt.plot(num_est, mean_train_score,\n     label = \"Training Score\", color = 'b')\nplt.plot(num_est, mean_test_score,\n   label = \"Cross Validation Score\", color = 'g')\n \n# Creating the plot\nplt.title(\"Validation Curve\")\nplt.xlabel(\"param\")\nplt.ylabel(\"LogLoss\")\nplt.tight_layout()\nplt.legend(loc = 'best')\nplt.show()\n'''","17704c75":"#mean_test_score","e51eda4f":"'''\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 10, stop = 500, num = 10)]\n# Number of features to consider at every split\nmax_features = [int(x) for x in np.linspace(5, 15, num = 10)]\nmax_features.append(\"auto\")\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(2, 10, num = 5)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = 5\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = 5\n# Method of selecting samples for training each tree\nbootstrap = True\n\n#XGB\nlearning_rate = [0.01,0.1,0.3,0.5, 0.7,1]\ncolsample_bytree = [0.05,0.1, 0.3, 0.5,0.7,1]\n\n#LGB\nfeature_fraction = [0.1, 0.3, 0.5, 0.7, 0.9, 1]\n\n#CAT\ncolsample_bylevel =[0.1, 0.3, 0.5, 0.7, 0.9, 1]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               #'max_features': max_features,\n               'max_depth': max_depth,\n               #'min_samples_split': min_samples_split,\n               #'min_samples_leaf': min_samples_leaf,\n               #'bootstrap': bootstrap,\n               'learning_rate': learning_rate,\n               #'colsample_bytree': colsample_bytree\n               #'feature_fraction':feature_fraction,\n               'colsample_bylevel':colsample_bylevel\n              }\n'''","7d89430b":"'''\n# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = CatBoostClassifier(#n_estimators=550,\n                           #learning_rate=0.5,\n                           #max_depth=1,\n                           loss_function='MultiClass',\n                          random_state=17,\n                          thread_count=-1,\n                            #colsample_bylevel=0.4,\n                            min_data_in_leaf=5,\n                            reg_lambda = 3,\n                            subsample = 0.8,\n                            bootstrap_type = 'Bernoulli'\n\n                            )\n# Random search of parameters, using 2 fold cross validation, \n# search across 30 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 33, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(train_features, train_labels)\n'''","56f4c6b0":"#rf_random.best_params_","c606fa80":"'''\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    #'max_depth': [5,7],\n    'n_estimators': [220,240,260],\n    #'learning_rate':[0.1,0.3,0.5],\n    #'feature_fraction':[0.1,0.3],\n    #'num_leaves':[2,6,10],\n    #'bagging_fraction':[0.4,0.6,0.8],\n    #'bagging_freq':[6,8,10],\n    #'max_bin':[10,15,25],\n    #'colsample_bylevel':[0.05,0.1,0.3],\n    'min_data_in_leaf':[3,5,7],\n    'reg_lambda':[1,3,5],\n    'subsample':[0.6,0.8,1]\n    \n}\n# Create a based model\nrf = XGBClassifier(n_estimators=100,\n                        learning_rate = 0.5,\n                        colsample_bytree = 0.4,\n                       max_depth = 6,\n                        min_child_weight=5,\n                       gamma=0.001,\n                       subsample=0.7,\n                       objective='multi:softprob',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       reg_alpha=0.00006,\n                       random_state=42)\n)\n\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)\n'''","1eea50a4":"'''\n# Fit the grid search to the data\ngrid_search.fit(train_features, train_labels)\n#best_grid = \ngrid_search.best_estimator_\n'''","a718db0f":"#grid_search.best_params_","d4d62f8d":"#stack_gen.get_params().keys()","bf784145":"'''\n%%time\nnum_est = [0.08,0.1,0.12]\n\nvc_model = LGBMClassifier(objective='multiclass', \n                       num_leaves=5,\n                    min_data_in_leaf=19, \n                    max_depth=4,\n                      learning_rate=0.1, \n                     n_estimators=180, \n                       max_bin=20, \n                      bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                      feature_fraction=0.1, \n                       feature_fraction_seed=8,\n                          lambda_l1 = 7,\n                           lambda_l2 = 0.1,\n                           min_gain_to_split = 0.01,\n                           metric = 'multi_logloss',\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n                       \nstack_gen_vc = StackingCVClassifier(classifiers = (xgb5, lgb7, cat5),\n                                meta_classifier = vc_model,\n                                 use_probas= True,\n                                use_features_in_secondary= True, \n                                 verbose=2,\n                                 n_jobs=-1,\n                                random_state=17,\n                                   stratify = True)\n\n\n# Calculate accuracy on training and test set using the\n# parameter with 3-fold cross validation\ntrain_score, test_score = validation_curve( stack_gen_vc,\n                                X = train_features, y = train_labels, \n                                param_name = 'meta_classifier__learning_rate', \n                                param_range = num_est, cv = kf, scoring=\"neg_log_loss\", n_jobs=-1\n                            )\n \n# Calculating mean and standard deviation of training score\nmean_train_score = -np.mean(train_score, axis = 1)\nstd_train_score = np.std(train_score, axis = 1)\n \n# Calculating mean and standard deviation of testing score\nmean_test_score = -np.mean(test_score, axis = 1)\nstd_test_score = np.std(test_score, axis = 1)\n \n# Plot mean accuracy scores for training and testing scores\nplt.plot(num_est, mean_train_score,\n     label = \"Training Score\", color = 'b')\nplt.plot(num_est, mean_test_score,\n   label = \"Cross Validation Score\", color = 'g')\n \n# Creating the plot\nplt.title(\"Validation Curve\")\nplt.xlabel(\"param\")\nplt.ylabel(\"LogLoss\")\nplt.tight_layout()\nplt.legend(loc = 'best')\nplt.show()\n'''","b245bde9":"#mean_test_score","a80d26c4":"\n# Read in sample_submission dataframe\nsubmission[['Class_1', 'Class_2', 'Class_3', 'Class_4','Class_5', 'Class_6', 'Class_7', 'Class_8', 'Class_9']] = stack11_pred\nsubmission.head()\n","20da7112":"submission.to_csv(\"submission_stack11.csv\", index=False)","4ed45ae2":"from IPython.display import FileLink\nFileLink('submission_stack11.csv')","8b299e33":"from google.colab import files\nfiles.download('submission_stack11.csv') ","4f38030b":"'''\nPerformance\n# CV scores have taken square root by careless setting, but would not affect the comparison of results.\n\nBasic rf (n=100)\nCV 1.0633 (0.0022)\npublic 1.12521\n\nFeatured rf (n=100)\nCV 1.0643 (0.0024)\npublic 1.12140\n\nTuned rf\nCV 1.0506 (0.0005)\npublic 1.09957\n\nTuned rf with criterion = \"entropy\"\nCV 1.0501 (0.0007)\npublic 1.09912\n\nBasic xgb\npublic 1.09495\n\nTuned xgb\nCV 1.0473 (0.0007)\npublic 1.08944\n\nTuned2 xgb\nCV 1.0453 (0.0006)\npublic 1.08896\n\nTuned3 xgb\nCV 1.0455 (0.0005)\npublic 1.08767\n\nTuned4 xgb\nCV 1.0453 (0.0007)\npublic 1.08834\n\nTuned5 xgb\nCV 1.0445 (0.0007)\npublic 1.08785\n\nBasic lgb\nCV 1.0470 (0.0006)\npublic 1.09036\n\nTuned lgb\nCV 1.0454 (0.0007)\npublic 1.08871\n\nTuned2 lgb\nCV 1.0459 (0.0007)\npublic 1.08916\n\nTuned3 lgb\nCV 1.0455 (0.0007)\npublic 1.08872\n\nTuned4 lgb\nCV 1.0452 (0.0007)\npublic 1.08789\n\nTuned5 lgb\nCV 1.0447 (0.0006)\npublic 1.08785\n\nTuned6 lgb\nCV 1.0447 (0.0006)\npublic 1.08790\n\nTuned7 lgb\nCV 1.0441 (0.0008)\npublic 1.08723\n\nBasic ext\nCV 1.0643 (0.0011)\npublic 1.12962\n\nTuned ext\nCV 1.0550 (0.0004)\npublic 1.10729\n\nTuned2 ext\nCV 1.0536 (0.0005)\npublic 1.10495\n\nBasic cat\nCV 1.0516 (0.0009)\npublic 1.10367\n\nTuned cat\nCV 1.0450 (0.0008)\npublic 1.09039\n\nTuned2 cat\nCV 1.0449 (0.0007)\npublic 1.09092\n\nTuned3 cat\nCV 1.0456 (0.0006)\npublic 1.09146\n\nTuned4 cat\nCV 1.0448 (0.0009)\npublic 1.09089\n\nTuned5 cat\nCV 1.0446 (0.0007)\npublic 1.09073\n\nStacking (rf, xgb, lgb, ext)>lgb\nCV 1.0455 (0.0007)\npublic 1.08811\n\nStacking1 (xgb, lgb, cat)>lgb\nCV 1.0450 (0.0008)\npublic 1.08605\n\nStacking2 (xgb2, lgb, cat)>cat\nCV 1.0448 (0.0008)\npublic 1.08766\n\nStacking3 (xgb2, lgb, cat)>xgb2\nCV 1.0458 (0.0008)\npublic 1.08729\n\nStacking4 (xgb2, lgb, cat2)>cat\nCV 1.0447 (0.0007)\npublic 1.08679\n\nStacking5 (xgb2, lgb, cat2)>cats5\nCV 1.0443 (0.0008)\npublic 1.08685\n\nStacking6 (xgb, lgb, cat)>lgbs6\nCV 1.0443 (0.0007)\npublic 1.08604\n\nStacking7 (xgb3, lgb3, cat)>lgbs7\nCV 1.0443 (0.0008)\npublic 1.08641\n\nStacking8 (xgb2, lgb, cat2)>lgbs8\nCV 1.0444 (0.0007)\npublic 1.08592\n\nStacking9 (xgb4, lgb5, cat4)>lgbs9\nCV 1.0439 (0.0008)\npublic 1.08613\n\nStacking10 (Stacking9 with no features input to meta model)\nCV 1.0440 (0.0007)\npublic 1.08674\n\nStacking11 (xgb5, lgb7, cat5)>lgbs11\nCV 1.0436 (0.0008)\npublic 1.08627\n\n\nrf = RandomForestClassifier(min_samples_split = 5,\n                            min_samples_leaf = 5,\n                            max_depth = None,\n                            bootstrap = True,\n                            n_jobs=-1,\n                            criterion = \"entropy\",\n                            n_estimators=500,\n                            max_features = 12,\n                            random_state = 42)\n\nxgb1 = XGBClassifier(learning_rate = 0.1,\n                        colsample_bytree = 0.5,\n                        max_depth = 10,\n                        min_child_weight=5,\n                       gamma=0.001,\n                       subsample=0.9,\n                       objective='multi:softprob',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       reg_alpha=0.00006,\n                       random_state=42)\n                       \nxgb2 = XGBClassifier(n_estimators=110,\n                        learning_rate = 0.5,\n                        colsample_bytree = 0.13,\n                       max_depth = 2,\n                        min_child_weight=5,\n                       gamma=0.001,\n                       subsample=0.7,\n                       objective='multi:softprob',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       reg_alpha=0.00006,\n                       random_state=42)\n                       \n                       \nxgb3 = XGBClassifier(n_estimators=250,\n                        learning_rate = 0.06,\n                        colsample_bytree = 0.4,\n                       max_depth = 6,\n                        min_child_weight=5,\n                       subsample=0.75,\n                       objective='multi:softprob',\n                       nthread=-1,\n                       random_state=42)\n\nxgb4 = XGBClassifier(n_estimators=130, \n                        learning_rate = 0.5,\n                        colsample_bytree = 0.13,\n                        max_depth = 2,\n                        min_child_weight=7, \n                       gamma=0.001,\n                       subsample=0.7,\n                       objective='multi:softprob',\n                       nthread=-1,\n                       reg_alpha=0.00006,\n                       random_state=42)\n\nxgb5 = XGBClassifier( n_estimators=200,\n                        learning_rate = 0.5,\n                       colsample_bytree = 0.1,\n                       max_depth = 2,\n                        min_child_weight=5,\n                      gamma=0.005,\n                       subsample=0.7,\n                       objective='multi:softprob',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       reg_alpha=5,\n                         reg_lambda = 0.3,\n                         eval_metric = 'mlogloss',\n                       random_state=42)\n                       \nlgb = LGBMClassifier(objective='multiclass', \n                       num_leaves=6,\n                    max_depth=6,\n                       learning_rate=0.1, \n                       n_estimators=220,\n                       max_bin=200, \n                       bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.7,\n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n                   \nlgb2 = LGBMClassifier(objective='multiclass', \n                       num_leaves=2,\n                    max_depth=1,\n                       learning_rate=0.5, \n                       n_estimators=550,\n                      max_bin=25, \n                       bagging_fraction=0.6,\n                       bagging_freq=8, \n                       bagging_seed=8,\n                      feature_fraction=0.3,\n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n                   \nlgb3 =  LGBMClassifier(objective='multiclass', \n                      min_data_in_leaf=7, \n                    max_depth=4,\n                       learning_rate=0.066, \n                       n_estimators=200,\n                       bagging_fraction=0.75,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.3,\n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n\nlgb4 = LGBMClassifier(objective='multiclass', \n                       num_leaves=6,\n                    max_depth=6,\n                       learning_rate=0.1, \n                       n_estimators=250, \n                       max_bin=200, \n                       bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.5,\n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n\nlgb5 = LGBMClassifier(objective='multiclass', \n                       num_leaves=6,\n                    min_data_in_leaf=19, \n                    max_depth=6,\n                       learning_rate=0.1, \n                      n_estimators=310, \n                       max_bin=80, \n                      bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.1, \n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n\nlgb6 = LGBMClassifier(objective='multiclass', \n                       num_leaves=6,\n                    min_data_in_leaf=19, \n                    max_depth=5,\n                      learning_rate=0.1, \n                      n_estimators=310, \n                       max_bin=20, \n                      bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                      feature_fraction=0.1, \n                       feature_fraction_seed=8,\n                           lambda_l2 = 0.003,\n                           min_gain_to_split = 0.0001,\n                           metric = 'multi_logloss',\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n\nlgb7 = LGBMClassifier(objective='multiclass', \n                       num_leaves=6,\n                    min_data_in_leaf=19, \n                    max_depth=5,\n                      learning_rate=0.1, \n                     n_estimators=500, \n                       max_bin=20, \n                      bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                      feature_fraction=0.1, \n                       feature_fraction_seed=8,\n                          lambda_l1 = 9,\n                           lambda_l2 = 0.003,\n                           min_gain_to_split = 0.0001,\n                           metric = 'multi_logloss',\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n\n                   \next = ExtraTreesClassifier(  min_samples_split = 5,\n                            min_samples_leaf = 5,\n                            max_depth = 15,\n                            bootstrap = True,\n                            n_jobs=-1,\n                            n_estimators=10,\n                            max_features = 20,\n                            random_state = 42,\n                            criterion = 'entropy')\n                            \n\ncat = CatBoostClassifier(n_estimators=500,\n                           learning_rate=0.3,\n                           max_depth=2,\n                           loss_function='MultiClass',\n                          random_state=17,\n                          thread_count=-1,\n                            colsample_bylevel=0.5,\n                            min_data_in_leaf=5,\n                            verbose=0)\n                            \n                            \ncat2 = CatBoostClassifier(n_estimators=550,\n                           learning_rate=0.5,\n                           max_depth=1,\n                           loss_function='MultiClass',\n                          random_state=17,\n                          thread_count=-1,\n                            colsample_bylevel=0.4,\n                            min_data_in_leaf=5,\n                            verbose=0)\n                            \n                            \ncat3 =   CatBoostClassifier(n_estimators=250,\n                           learning_rate=0.1,\n                           max_depth=6,\n                           loss_function='MultiClass',\n                          random_state=17,\n                          thread_count=-1,\n                            colsample_bylevel=0.4,\n                            min_data_in_leaf=3,\n                            verbose=0)\n\ncat4 = CatBoostClassifier(n_estimators=600,\n                           learning_rate=0.6,\n                           max_depth=1,\n                           loss_function='MultiClass',\n                          random_state=17,\n                          thread_count=-1,\n                            colsample_bylevel=0.4,\n                            min_data_in_leaf=5,\n                            verbose=0)\n\ncat5 = CatBoostClassifier(n_estimators=600,\n                           learning_rate=0.6,\n                           max_depth=1,\n                           loss_function='MultiClass',\n                          random_state=17,\n                          thread_count=-1,\n                            colsample_bylevel=0.4,\n                            min_data_in_leaf=5,\n                               bagging_temperature = 0.2,\n                             sampling_frequency = 'PerTreeLevel',\n                               reg_lambda = 3,\n                            verbose=0)\n\n\ncats5 = CatBoostClassifier(n_estimators=450, \n                           learning_rate=0.1, \n                           max_depth=1,\n                           loss_function='MultiClass',\n                          random_state=17,\n                          thread_count=-1,\n                            colsample_bylevel=0.5,\n                            min_data_in_leaf=5,\n                            verbose=0)\n                            \n                            \nlgbs6 = LGBMClassifier(objective='multiclass', \n                       num_leaves=6,\n                    max_depth=6,\n                       learning_rate=0.03, \n                       n_estimators=160, \n                       max_bin=200, \n                       bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.7,\n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n                   \n                   \nlgbs7 = LGBMClassifier(objective='multiclass', \n                      min_data_in_leaf=7, \n                    max_depth=4,\n                       learning_rate=0.05, \n                       n_estimators=100,\n                       bagging_fraction=1,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.3,\n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n                   \nlgbs8 = LGBMClassifier(objective='multiclass', \n                     min_data_in_leaf=3, \n                    max_depth=4,\n                       learning_rate=0.05, \n                       n_estimators=100,\n                       bagging_fraction=0.75,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.3,\n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n                   \nlgbs9 =  LGBMClassifier(objective='multiclass', \n                       num_leaves=6,\n                   min_data_in_leaf=23, \n                    max_depth=3,\n                       learning_rate=0.05, \n                    n_estimators=270, \n                      max_bin=60, \n                    bagging_fraction=0.7, \n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.1, \n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n                   \nlgbs10 = LGBMClassifier(objective='multiclass', \n                       num_leaves=4,\n                  min_data_in_leaf=400, \n                    max_depth=1, \n                     learning_rate=0.05, \n                   n_estimators=270, \n                      max_bin=60, \n                    bagging_fraction=0.5, \n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.15, \n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n                   \nlgbs11 = LGBMClassifier(objective='multiclass', \n                       num_leaves=5,\n                    min_data_in_leaf=19, \n                    max_depth=4,\n                      learning_rate=0.1, \n                     n_estimators=180, \n                       max_bin=20, \n                      bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                      feature_fraction=0.1, \n                       feature_fraction_seed=8,\n                          lambda_l1 = 7,\n                           lambda_l2 = 0.1,\n                           min_gain_to_split = 0.01,\n                           metric = 'multi_logloss',\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n\n'''","5e4023fa":"'''\n# Split features and labels\ntrain2_labels = train_labels.apply(lambda x: 'Class_2'  if x == 'Class_2' else 'Others')\ntrain2_features = train_features\ntrain2_labels.unique()\n'''","08bfc783":"'''\ntrain134_labels = train.loc[train['target']!='Class_2']['target'].reset_index(drop=True)\ntrain134_features = train.loc[train['target']!='Class_2'].drop(['id','target'], axis=1).reset_index(drop=True)\ntrain134_labels.unique()\n'''","519cc6d8":"'''\n# Light Gradient Boosting Regressor\nlgbp2 =  LGBMClassifier(objective='binary', \n                      min_data_in_leaf=3, \n                    max_depth=4,\n                      learning_rate=0.066, \n                       n_estimators=250,\n                       bagging_fraction=0.75,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.3,\n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n\n#lgbp2_model = lgbp2.fit(train2_features, train2_labels)\n#lgbp2_pred = lgbp2_model.predict_proba(test_features)\n\n'''","23569da9":"'''\n# Light Gradient Boosting Regressor\nlgbp134 =  LGBMClassifier(objective='multiclass', \n                      min_data_in_leaf=3, \n                    max_depth=4,\n                      learning_rate=0.1, \n                       n_estimators=100,\n                       bagging_fraction=0.75,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                      feature_fraction=0.4,\n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n\n\n#lgbp134_model = lgbp134.fit(train134_features, train134_labels)\n#lgbp134_pred = lgbp134_model.predict_proba(test_features)\n'''","761d5d78":"'''\n%%time\nnum_est = [0.3,0.4,0.5]\n\nvc_model = LGBMClassifier(objective='multiclass', \n                      min_data_in_leaf=3, \n                    max_depth=4,\n                      learning_rate=0.1, \n                       n_estimators=100,\n                       bagging_fraction=0.75,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                      feature_fraction=0.4,\n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n\n\n# Calculate accuracy on training and test set using the\n# parameter with 3-fold cross validation\ntrain_score, test_score = validation_curve( vc_model,\n                                X = train134_features, y = train134_labels, \n                                param_name = 'feature_fraction', \n                                param_range = num_est, cv = 2, scoring=\"neg_log_loss\", n_jobs=-1\n                            )\n \n# Calculating mean and standard deviation of training score\nmean_train_score = -np.mean(train_score, axis = 1)\nstd_train_score = np.std(train_score, axis = 1)\n \n# Calculating mean and standard deviation of testing score\nmean_test_score = -np.mean(test_score, axis = 1)\nstd_test_score = np.std(test_score, axis = 1)\n \n# Plot mean accuracy scores for training and testing scores\nplt.plot(num_est, mean_train_score,\n     label = \"Training Score\", color = 'b')\nplt.plot(num_est, mean_test_score,\n   label = \"Cross Validation Score\", color = 'g')\n \n# Creating the plot\nplt.title(\"Validation Curve\")\nplt.xlabel(\"param\")\nplt.ylabel(\"LogLoss\")\nplt.tight_layout()\nplt.legend(loc = 'best')\nplt.show()\n'''","3b9a1309":"#mean_test_score","d59de226":"'''\n# Read in sample_submission dataframe\nsubmission[['Class_1']] = lgbp2_pred[:,1]*lgbp134_pred[:,0]\nsubmission[['Class_2']] =lgbp2_pred[:,0]\nsubmission[['Class_3']] = lgbp2_pred[:,1]*lgbp134_pred[:,1]\nsubmission[['Class_4']] = lgbp2_pred[:,1]*lgbp134_pred[:,2]\n\nsubmission.head()\n'''","837ad476":"#submission[['Class_1','Class_2','Class_3','Class_4']].sum(axis=1).unique()","9467173f":"#submission.to_csv(\"submission_p2model.csv\", index=False)","ba42b29e":"'''\nBasic lgb\npublic 1.09213\n\nTuned lgbp2>lgbp134\npublic 1.08981\n\nlgbp2 = LGBMClassifier(objective='binary', \n                      min_data_in_leaf=3, \n                    max_depth=4,\n                      learning_rate=0.066, \n                       n_estimators=250,\n                       bagging_fraction=0.75,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.3,\n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n                   \n                   \n lgbp134 =   LGBMClassifier(objective='multiclass', \n                      min_data_in_leaf=3, \n                    max_depth=4,\n                      learning_rate=0.1, \n                       n_estimators=100,\n                       bagging_fraction=0.75,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                      feature_fraction=0.4,\n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)                \n\n'''","ff257ab4":"'''\ntrainsum_features = train_features\ntestsum_features = test_features\n'''","83e5e113":"'''\n%%time\nfor i in range(0,49):\n    a='feature_'+str(i)\n    \n    for j in range(i+1,50): \n        b='feature_'+str(j)\n        trainsum_features[str(i)+\"_\"+str(j)+\"_sum\"] = train_features[[a,b]].sum(axis=1)\n        '''","f3cdb0a7":"'''\n%%time\nfor i in range(0,49):\n    a='feature_'+str(i)\n    \n    for j in range(i+1,50): \n        b='feature_'+str(j)\n        testsum_features[str(i)+\"_\"+str(j)+\"_sum\"] = test_features[[a,b]].sum(axis=1)\n        '''","46a4365f":"'''\n%%time\n# Light Gradient Boosting Regressor\nlgb =  LGBMClassifier(objective='multiclass',                     \n                            verbose=-1,\n                       random_state=17,\n                      importance_type='gain',\n                   n_jobs=-1)\n\nlgb_model = lgb.fit(trainsum_features, train_labels)\nlgb_pred = lgb_model.predict_proba(testsum_features)\n'''","c9fb17b4":"'''\narr = np.stack((lgb.feature_name_, lgb.feature_importances_) ,axis=1)\nm = [row[0] for row in arr if row[1] != \"0.0\"]\n'''","ff326de0":"'''\ntrainsum_features = trainsum_features[m]\ntestsum_features = testsum_features[m]\n'''","a4b8b46a":"'''\n# Read in the dataset as a dataframe\ntrainsum_features = pd.read_csv(\"..\/input\/tps-may-2021\/trainsum_features.csv\")\ntestsum_features = pd.read_csv(\"..\/input\/tps-may-2021\/testsum_features.csv\")\n'''","637921ce":"'''\ntrainsum_features.to_csv(\"trainsum_features.csv\", index=False)\ntestsum_features.to_csv(\"testsum_features.csv\", index=False)\n'''","3ecb8d92":"'''\n# Light Gradient Boosting Regressor\nlgbsum =  LGBMClassifier(objective='multiclass', \n                     min_data_in_leaf=1, \n                    max_depth=4,\n                      learning_rate=0.07, \n                       n_estimators=100,\n                       bagging_fraction=0.75,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                     feature_fraction=0.3,\n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n\nlgbsum_model = lgbsum.fit(trainsum_features, train_labels)\nlgbsum_pred = lgbsum_model.predict_proba(testsum_features)\n'''","c22b7bdd":"'''\n# Setup cross validation folds\nkf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n\n# Define error metrics\ndef loss(y, y_pred):\n    return np.sqrt(log_loss(y, y_pred))\n\ndef cvsum_loss(model, X = trainsum_features):\n    loss = np.sqrt(-cross_val_score(model, X, train_labels, scoring=\"neg_log_loss\", cv=kf, n_jobs=-1))\n    return (loss)\n    '''","75169748":"'''\nscores = {}\n\nscore = cvsum_loss(lgbsum)\nprint(\"rf: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['rf'] = (score.mean(), score.std())\n'''","5be22f38":"'''\n%%time\nnum_est = [0.05,0.07,0.1]\n\nvc_model = LGBMClassifier(objective='multiclass', \n                     min_data_in_leaf=1, \n                    max_depth=4,\n                      learning_rate=0.07, \n                       n_estimators=100,\n                       bagging_fraction=0.75,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                     feature_fraction=0.3,\n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n\n\n# Calculate accuracy on training and test set using the\n# parameter with 3-fold cross validation\ntrain_score, test_score = validation_curve( vc_model,\n                                X = trainsum_features, y = train_labels, \n                                param_name = 'learning_rate', \n                                param_range = num_est, cv = 2, scoring=\"neg_log_loss\", n_jobs=-1\n                            )\n \n# Calculating mean and standard deviation of training score\nmean_train_score = -np.mean(train_score, axis = 1)\nstd_train_score = np.std(train_score, axis = 1)\n \n# Calculating mean and standard deviation of testing score\nmean_test_score = -np.mean(test_score, axis = 1)\nstd_test_score = np.std(test_score, axis = 1)\n \n# Plot mean accuracy scores for training and testing scores\nplt.plot(num_est, mean_train_score,\n     label = \"Training Score\", color = 'b')\nplt.plot(num_est, mean_test_score,\n   label = \"Cross Validation Score\", color = 'g')\n \n# Creating the plot\nplt.title(\"Validation Curve\")\nplt.xlabel(\"param\")\nplt.ylabel(\"LogLoss\")\nplt.tight_layout()\nplt.legend(loc = 'best')\nplt.show()\n'''","b8b82163":"#mean_test_score","eef3ef1f":"'''\n# Read in sample_submission dataframe\nsubmission[['Class_1', 'Class_2', 'Class_3', 'Class_4']] = lgbsum_pred\nsubmission.head()\n'''","27b82601":"#submission.to_csv(\"submission_sumlgb.csv\", index=False)","500d0333":"'''\nBasic lgb\nCV 1.0478 (0.0006)\npublic 1.09072\n\nTuned lgbsum\nCV 1.0462 (0.0007)\npublic 1.09009\n\nlgbsum = LGBMClassifier(objective='multiclass', \n                     min_data_in_leaf=1, \n                    max_depth=4,\n                      learning_rate=0.07, \n                       n_estimators=100,\n                       bagging_fraction=0.75,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                     feature_fraction=0.3,\n                       feature_fraction_seed=8,\n                            verbose=-1,\n                       random_state=17,\n                   n_jobs=-1)\n'''","3196fd32":"# Data Exploration","222cdee4":"First of all, this solution is built on the idea to practise the complete workflow of machine learning prediction. Even though auto ML and blending other people's results are frequently used and legit in Kaggle, I think building a ground up solution is beneficial for gaining really solid understanding of the ML techniques.\n\nSince my computer is not powerful enough to handle the size of data here (100K times 50) with ease, I will strive for simple and efficient way to build the classification model on Kaggle hosting server. Again, the modelling process here is for learning the ML process rather than doing the fancy stuff or building specific solutions that are not ready for generalisation to other problems.\n\nBased on the background above, you will see a solution in favour of simple ML workflow and low computation cost, ready to be deployed for different problems. The presentation may be raw, but I will keep it to show how the result is improved gradually.\n\nWorkflow:\n1. Data Exploration\n2. Data Preprocessing\n3. Feature Engineering\n4. Feature Selection\n5. Model Validation And Selection\n6. Hyperparameter Tuning\n","2a6db21f":"## Features EDA\n\nNo specific pattern is observed in this case.","ad590a88":"## Recombine datasets\n\nNo treatment is needed in this case.","0d8f188f":"# Preparation","e759ca3a":"# Submission","940b7040":"# Model Validation and Selection","c1cbd5a6":"# Additional: Class 2 pre-modelling","c3f8d953":"## Correlation\n\nFilter by RF feature importance first when the number of features is too large.\n\nThe 50 features show no significant correlation with each other.","abe0c1dd":"### Extra Trees","3076b9f3":"## Stacking\n\nOnly one level of meta model is trained on the base model predictions and the original features. The type of meta models would still be chosen from the available types in the base models. ","d150ce01":"### Further exploration for high correlation to target\n\nThe most important features by RF is feature_38, but visually its standalone correlation with the target is insignificant.","9250137d":"### CatBoost","fa2a2001":"## Tuning for meta-model","9ed23661":"## Random Hyperparameter Grid\n\nThe random grid is mostly used as a preliminary to narrow down the range of hyperparameters for the finer grid search.","2bbdcc5f":"Redefine CV function for PCA","fe2f2ad5":"## Base Models","a673e44a":"## Target distribution\n\nAs observed, 57% of the target in the training set is of \"Class 2\", which is moderately imbalanced.","c1eeccf7":"Combinatorial summation of the 50 features as new features. The features dimension is exploding with high computation cost. Result is not satisfactory, and not good enough to justify the higher complexity.","756d30c2":"## **Code Imported from** [https:\/\/www.kaggle.com\/lilkaskitc\/tps-may-2021](http:\/\/)","f157b679":"## Performance Log","1d87b649":"# Hyperparameter Tuning","65941373":"## Read data","422e3294":"# Ground up solution","a11c90e2":"# Data Preprocessing","dfc036b9":"## Grid Search","1a55dd4d":"## PCA\n\nSince there are 50 features, the dimension reduction technique may help. I have tried PCA, but the result is not satisfactory. This is intuitive given the low features correlation shown in EDA, and the almost identical contributions from all the principal components.","d48789db":"# Additional2: Summation of features","c622b535":"## Imports","1c52b6a6":"## Validation Curves\n\nValidation curve is most frequently used in hyperparameters tuning for its lower computation cost. Past studies have also shown that its performance is not far from the grid search if used properly.","467ba49d":"No outliers or missing values observed from EDA.","0095b87c":"## CV result","162ca35e":"# Feature Engineering\n\nObserved that the training set has large amount of zero values in the original 50 features, I have tried to add 50 binary features depending on whether each of the original 50 features is zero or not. The trial does not provide significant improvement in preliminary RF model, so this idea is not adopted. There maybe information loss in grouping all nonzero values together, which leads to worse performance.","7885b209":"# Feature Selection","44918cce":"Try to do modelling in sequence that first trained on separating \"Class 2\" from others, and then classify Class 1, 3, 4 in the remaining block. Result not satisfactory.","92b487d8":"## Set up CV","9b17e299":"## Split datasets","41910814":"### XGBoost","d528baad":"## Recreate training and test sets\n\nNo treatment is needed in this case.","f7641b9e":"## Confusion Matrix\n\nThe result shows that the imbalanced target data has led to prediction highly in favour of \"Class 2\", which is worth noting for insights.","74205296":"### Light GBM","b17b581f":"### Random Forest"}}