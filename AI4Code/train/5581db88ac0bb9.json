{"cell_type":{"ddee91b8":"code","781af781":"code","1d1dba1a":"code","5ba72044":"code","8cfdd38a":"code","e106a099":"code","92b5c697":"code","65ad8f4b":"code","3c2aa574":"code","cbbbd965":"code","725c78ac":"code","9178e2bf":"code","3f56e125":"code","1fee0860":"code","b0908beb":"code","13b0f9cc":"code","bee5c2c2":"code","0ba0b05b":"code","46a27162":"code","7d01567a":"code","f17330d1":"code","8d943574":"code","4830e08c":"code","87be4806":"code","c7edfd25":"code","6ffef69c":"code","0fe9160f":"code","22be3ca8":"code","4401a07b":"code","17c2c997":"code","99c78291":"code","c5b551f8":"code","cf270fdb":"code","2949c6b0":"code","c2d53053":"code","d9830791":"code","60f4882a":"code","3b7cc269":"code","47739e51":"code","26559c33":"code","c77da51a":"code","c6b6b5ca":"code","af6279f9":"code","72595806":"code","18caa855":"code","f2bd89e8":"code","1bcd95f3":"code","81549f4c":"code","06afd355":"code","6ef8405c":"code","ea06ddba":"code","c64c0de2":"code","8f5c1221":"code","ce54f21c":"code","372d33ba":"code","e325abc1":"code","42dbba62":"code","5e87b346":"code","a0792200":"code","4340ac1f":"code","354a7d9e":"code","6dc7c9c8":"code","67de0187":"code","b9db3101":"code","f8029155":"code","18acc597":"code","709deb83":"code","bc2e7889":"code","7b60b2da":"code","06ad02ca":"code","197ed79e":"code","33e7c5ff":"code","c01635b6":"code","b0b98fa7":"code","5d3dc2e1":"code","706fb409":"code","9bbe6b98":"code","a7488565":"code","00852a2b":"code","6f04a5ce":"code","861a6681":"code","98e5c8df":"code","03e697de":"code","23f5624d":"code","758b4ec6":"markdown","39a3081e":"markdown","2334fc14":"markdown","7f29f62e":"markdown","22b996bc":"markdown","e8b32be7":"markdown","7f2ebde7":"markdown","f3b80e3b":"markdown","a1609eaf":"markdown","b931b027":"markdown","840303ef":"markdown","62e15126":"markdown","22a6b852":"markdown","738a986f":"markdown","ecc59f68":"markdown","af71f9e4":"markdown"},"source":{"ddee91b8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom statistics import *\nfrom plotly import tools\nimport plotly.offline as py\nimport plotly.figure_factory as ff\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport tensorflow as tf\nimport time \nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nfrom IPython.core.display import display, HTML\nimport plotly.graph_objects as go\nimport re\n# Natural Language Tool Kit \nimport nltk  \nnltk.download('stopwords') \nfrom nltk.corpus import stopwords \nfrom nltk.stem.porter import PorterStemmer \nfrom collections import Counter\nimport cufflinks as cf\nimport string \n!pip install simpletransformers\nfrom simpletransformers.classification import ClassificationModel\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","781af781":"train_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","1d1dba1a":"train_df[train_df[\"target\"] == 0][\"text\"].values[1]\n","5ba72044":"print(train_df)\n","8cfdd38a":"y_train=train_df['target']\nx_train = train_df.drop(labels=[\"target\"],axis=1)\ny_train.value_counts()\nsns.countplot(y_train)","e106a099":"x_train.isnull().any().describe()\ntest_df.isnull().any().describe()","92b5c697":"missing_cols = ['keyword', 'location']\n\nfig, axes = plt.subplots(ncols=2, figsize=(17, 4), dpi=100)\n\nsns.barplot(x=train_df[missing_cols].isnull().sum().index, y=train_df[missing_cols].isnull().sum().values, ax=axes[0])\nsns.barplot(x=test_df[missing_cols].isnull().sum().index, y=test_df[missing_cols].isnull().sum().values, ax=axes[1])\n\naxes[0].set_ylabel('Missing Value Count', size=15, labelpad=20)\naxes[0].tick_params(axis='x', labelsize=15)\naxes[0].tick_params(axis='y', labelsize=15)\naxes[1].tick_params(axis='x', labelsize=15)\naxes[1].tick_params(axis='y', labelsize=15)\n\naxes[0].set_title('Training Set', fontsize=13)\naxes[1].set_title('Test Set', fontsize=13)\n\nplt.show()\n","65ad8f4b":"print(f'Number of unique values in location = {train_df[\"location\"].nunique()} (Training) - {test_df[\"location\"].nunique()} (Test)')","3c2aa574":"cnt_ = train_df['location'].value_counts()\ncnt_.reset_index()\ncnt_ = cnt_[:20,]\ntrace1 = go.Bar(\n                x = cnt_.index,\n                y = cnt_.values,\n                name = \"Number of tweets in dataset according to location\",\n                marker = dict(color = 'rgba(200, 74, 55, 0.5)',\n                             line=dict(color='rgb(0,0,0)',width=1.5)),\n                )\n\ndata = [trace1]\nlayout = go.Layout(barmode = \"group\",title = 'Number of tweets in dataset according to location')\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig)","cbbbd965":"train1_df = train_df[train_df[\"target\"]==1]\ntrain0_df = train_df[train_df[\"target\"]==0]\ncnt_1 = train1_df['location'].value_counts()\ncnt_1.reset_index()\ncnt_1 = cnt_1[:20,]\n\ncnt_0 = train0_df['location'].value_counts()\ncnt_0.reset_index()\ncnt_0 = cnt_0[:20,]","725c78ac":"trace1 = go.Bar(\n                x = cnt_1.index,\n                y = cnt_1.values,\n                name = \"real disaste\",\n                marker = dict(color = 'rgba(255, 74, 55, 0.5)',\n                             line=dict(color='rgb(0,0,0)',width=1.5)),\n                )\ntrace0 = go.Bar(\n                x = cnt_0.index,\n                y = cnt_0.values,\n                name = \"fake disaster\",\n                marker = dict(color = 'rgba(79, 82, 97, 0.5)',\n                             line=dict(color='rgb(0,0,0)',width=1.5)),\n                )\n\n\ndata = [trace0,trace1]\nlayout = go.Layout(barmode = 'stack',title = 'Number of tweets in dataset according to location')\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig)","9178e2bf":"print(f'Number of unique values in keyword = {train_df[\"keyword\"].nunique()} (Training) - {test_df[\"keyword\"].nunique()} (Test)')","3f56e125":"## Distribution per keywords\ncnt2 = train_df['keyword'].value_counts()\ncnt2.reset_index()\ncnt2 = cnt_[:30,]\ntrace1 = go.Bar(\n                x = cnt2.index,\n                y = cnt2.values,\n                name = \"Number of tweets in dataset according to keyword\",\n                marker = dict(color = 'rgba(200, 74, 55, 0.5)',\n                             line=dict(color='rgb(0,0,0)',width=1.5)),\n                )\n\ndata = [trace1]\nlayout = go.Layout(barmode = \"group\",title = 'Number of tweets in dataset according to keyword')\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig)","1fee0860":"\ncnt_1 = train1_df['keyword'].value_counts()\ncnt_1.reset_index()\n#cnt_1 = cnt_1[:30,]\n\ncnt_0 = train_df['keyword'].value_counts()\ncnt_0.reset_index()\n#cnt_0 = cnt_0[:30,]","b0908beb":"trace1 = go.Bar(\n                x = cnt_1.index,\n                y = cnt_1.values,\n                name = \"real disaste\",\n                marker = dict(color = 'rgba(255, 74, 55, 0.5)',\n                             line=dict(color='rgb(0,0,0)',width=1.5)),\n                )\ntrace0 = go.Bar(\n                x = cnt_0.index,\n                y = cnt_0.values,\n                name = \"fake disaster\",\n                marker = dict(color = 'rgba(79, 82, 97, 0.5)',\n                             line=dict(color='rgb(0,0,0)',width=1.5)),\n                )\n\n\ndata = [trace0,trace1]\nlayout = go.Layout(barmode = 'stack',title = 'Number of tweets in dataset according to location')\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig)","13b0f9cc":"train_df['target_mean'] = train_df.groupby('keyword')['target'].transform('mean')\n\nfig = plt.figure(figsize=(8, 72), dpi=100)\n\nsns.countplot(y=train_df.sort_values(by='target_mean', ascending=False)['keyword'],\n              hue=train_df.sort_values(by='target_mean', ascending=False)['target'])\n\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=12)\nplt.legend(loc=1)\nplt.title('Target Distribution in Keywords')\n\nplt.show()\n\ntrain_df.drop(columns=['target_mean'], inplace=True)","bee5c2c2":"#Fill NA \n\nfor df in [train_df, test_df]:\n    for col in ['keyword', 'location']:\n        df[col] = df[col].fillna(f'no_{col}')","0ba0b05b":"#tweets lenght - characters count\n#train_df['length']= train_df['text'].apply(len)","46a27162":"train_df['tweet_len']= train_df['text'].apply(len)\ntrain_df['text']\ntest_df['tweet_len']= test_df['text'].apply(len)","7d01567a":"fig, ax = plt.subplots(figsize=(12,7))\nfor label, group in train_df.groupby('target'):\n    sns.distplot(group['text'].str.len(), label=str(label), ax=ax)\nplt.xlabel('# of characters')\nplt.ylabel('density')\nplt.legend()\nsns.despine()","f17330d1":"data = [\n    go.Box(\n        y=train_df[train_df['target']==0]['tweet_len'],\n        name='Fake'\n    ),\n    go.Box(\n        y=train_df[train_df['target']==1]['tweet_len'],\n        name='Real'\n    )\n]\nlayout = go.Layout(\n    title = 'Comparison of text length in Tweets '\n)\nfig = go.Figure(data=data, layout=layout)\nfig.show()","8d943574":"#word count\ntrain_df['word_count']= train_df['text'].apply(lambda x: len(str(x).split()))\ntest_df['word_count']= test_df['text'].apply(lambda x: len(str(x).split()))\n\n","4830e08c":"data = [\n    go.Box(\n        y=train_df[train_df['target']==0]['word_count'],\n        name='Fake'\n    ),\n    go.Box(\n        y=train_df[train_df['target']==1]['word_count'],\n        name='Real'\n    )\n]\nlayout = go.Layout(\n    title = 'Comparison of word_count in Tweets '\n)\nfig = go.Figure(data=data, layout=layout)\nfig.show()","87be4806":"# unique_word_count\ntrain_df['unique_word_count'] = train_df['text'].apply(lambda x: len(set(str(x).split())))\ntest_df['unique_word_count'] = test_df['text'].apply(lambda x: len(set(str(x).split())))","c7edfd25":"data = [\n    go.Box(\n        y=train_df[train_df['target']==0]['unique_word_count'],\n        name='Fake'\n    ),\n    go.Box(\n        y=train_df[train_df['target']==1]['unique_word_count'],\n        name='Real'\n    )\n]\nlayout = go.Layout(\n    title = 'Comparison of unique word_count in Tweets '\n)\nfig = go.Figure(data=data, layout=layout)\nfig.show()","6ffef69c":"# punctuation_count\ntrain_df['punctuation_count'] = train_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\ntest_df['punctuation_count'] = test_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))","0fe9160f":"# mean of word lenght\ntrain_df['mean_word_length'] = train_df['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ntest_df['mean_word_length'] = test_df['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","22be3ca8":"METAFEATURES = ['word_count', 'unique_word_count', 'tweet_len', 'mean_word_length','punctuation_count']\nDISASTER_TWEETS = train_df['target'] == 1\n\nfig, axes = plt.subplots(ncols=2, nrows=len(METAFEATURES), figsize=(20, 50), dpi=100)\n\nfor i, feature in enumerate(METAFEATURES):\n    sns.distplot(train_df.loc[~DISASTER_TWEETS][feature], label='Not Disaster', ax=axes[i][0], color='green')\n    sns.distplot(train_df.loc[DISASTER_TWEETS][feature], label='Disaster', ax=axes[i][0], color='red')\n\n    sns.distplot(train_df[feature], label='Training', ax=axes[i][1])\n    sns.distplot(test_df[feature], label='Test', ax=axes[i][1])\n    \n    for j in range(2):\n        axes[i][j].set_xlabel('')\n        axes[i][j].tick_params(axis='x', labelsize=12)\n        axes[i][j].tick_params(axis='y', labelsize=12)\n        axes[i][j].legend()\n    \n    axes[i][0].set_title(f'{feature} Target Distribution in Training Set', fontsize=13)\n    axes[i][1].set_title(f'{feature} Training & Test Set Distribution', fontsize=13)\n\nplt.show()","4401a07b":"def generate_ngrams (text,n=1):\n    token = [token for token in text.lower().split(' ') if token != '' if token not in STOPWORDS]\n    ngrams= zip(*[token[i:] for i in range(n)])\n    return[' '.join(ngram) for ngram in ngrams]\nN=100","17c2c997":"#uni\nfrom collections import defaultdict\ndisaster_unigrams= defaultdict(int)\nnondisaster_unigrams = defaultdict(int)\n\nfor tweet in train_df[DISASTER_TWEETS]['text']:\n    for word in generate_ngrams(tweet):\n        disaster_unigrams[word]+=1\n    \nfor tweet in train_df[~DISASTER_TWEETS]['text']:\n    for word in generate_ngrams(tweet):\n        nondisaster_unigrams[word]+=1\n        \ndf_disaster_unigrams = pd.DataFrame(sorted(disaster_unigrams.items(), key=lambda x: x[1])[::-1])\ndf_nondisaster_unigrams = pd.DataFrame(sorted(nondisaster_unigrams.items(), key=lambda x: x[1])[::-1])","99c78291":"# bigrams\ndisaster_bigrams = defaultdict(int)\nnondisaster_bigrams = defaultdict(int)\n\nfor tweet in train_df[DISASTER_TWEETS]['text']:\n    for word in generate_ngrams(tweet, n=2):\n        disaster_bigrams[word] += 1\n        \nfor tweet in train_df[~DISASTER_TWEETS]['text']:\n    for word in generate_ngrams(tweet, n=2):\n        nondisaster_bigrams[word] += 1\n        \ndf_disaster_bigrams = pd.DataFrame(sorted(disaster_bigrams.items(), key=lambda x: x[1])[::-1])\ndf_nondisaster_bigrams = pd.DataFrame(sorted(nondisaster_bigrams.items(), key=lambda x: x[1])[::-1])","c5b551f8":"#trigram\ndisaster_trigrams = defaultdict(int)\nnondisaster_trigrams = defaultdict(int)\n\nfor tweet in train_df[DISASTER_TWEETS]['text']:\n    for word in generate_ngrams(tweet, n=3):\n        disaster_trigrams[word] += 1\n        \nfor tweet in train_df[~DISASTER_TWEETS]['text']:\n    for word in generate_ngrams(tweet, n=3):\n        nondisaster_trigrams[word] += 1\n        \ndf_disaster_trigrams = pd.DataFrame(sorted(disaster_trigrams.items(), key=lambda x: x[1])[::-1])\ndf_nondisaster_trigrams = pd.DataFrame(sorted(nondisaster_trigrams.items(), key=lambda x: x[1])[::-1])","cf270fdb":"import re\n\ndef generate_ngrams(text, n=1):\n    # Convert to lowercases\n    s = s.str.lower()\n    \n    # Replace all none alphanumeric characters with spaces\n    s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s.str())\n    \n    # Break sentence in the token, remove empty tokens\n    tokens = [token for token in s.split(\" \") if token != \"\"]\n    \n    # Use the zip function to help us generate n-grams\n    # Concatentate the tokens into ngrams and return\n    ngrams = zip(*[token[i:] for i in range(n)])\n    return [\" \".join(ngram) for ngram in ngrams]","2949c6b0":"fig, axes = plt.subplots(ncols=2, figsize=(18, 50), dpi=100)\n\nplt.tight_layout()\n\nsns.barplot(y=df_disaster_unigrams[0].values[:N], x=df_disaster_unigrams[1].values[:N], ax=axes[0], color='red')\nsns.barplot(y=df_nondisaster_unigrams[0].values[:N], x=df_nondisaster_unigrams[1].values[:N], ax=axes[1], color='green')\n\nfor i in range(2):\n    axes[i].spines['right'].set_visible(False)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    axes[i].tick_params(axis='x', labelsize=13)\n    axes[i].tick_params(axis='y', labelsize=13)\n\naxes[0].set_title(f'Top {N} most common unigrams in Disaster Tweets', fontsize=15)\naxes[1].set_title(f'Top {N} most common unigrams in Non-disaster Tweets', fontsize=15)\n\nplt.show()","c2d53053":"fig, axes = plt.subplots(ncols=2, figsize=(18, 50), dpi=100)\nplt.tight_layout()\n\nsns.barplot(y=df_disaster_bigrams[0].values[:N], x=df_disaster_bigrams[1].values[:N], ax=axes[0], color='red')\nsns.barplot(y=df_nondisaster_bigrams[0].values[:N], x=df_nondisaster_bigrams[1].values[:N], ax=axes[1], color='green')\n\nfor i in range(2):\n    axes[i].spines['right'].set_visible(False)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    axes[i].tick_params(axis='x', labelsize=13)\n    axes[i].tick_params(axis='y', labelsize=13)\n\naxes[0].set_title(f'Top {N} most common bigrams in Disaster Tweets', fontsize=15)\naxes[1].set_title(f'Top {N} most common bigrams in Non-disaster Tweets', fontsize=15)\n\nplt.show()","d9830791":"fig, axes = plt.subplots(ncols=2, figsize=(20, 50), dpi=100)\n\nsns.barplot(y=df_disaster_trigrams[0].values[:N], x=df_disaster_trigrams[1].values[:N], ax=axes[0], color='red')\nsns.barplot(y=df_nondisaster_trigrams[0].values[:N], x=df_nondisaster_trigrams[1].values[:N], ax=axes[1], color='green')\n\nfor i in range(2):\n    axes[i].spines['right'].set_visible(False)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    axes[i].tick_params(axis='x', labelsize=13)\n    axes[i].tick_params(axis='y', labelsize=11)\n\naxes[0].set_title(f'Top {N} most common trigrams in Disaster Tweets', fontsize=15)\naxes[1].set_title(f'Top {N} most common trigrams in Non-disaster Tweets', fontsize=15)\n\nplt.show()\n","60f4882a":"def build_vocab(tweets):\n    vocab = {}        \n    for tweet in tweets:\n        for word in tweet:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab","3b7cc269":"train_tweets = train_df['text'].apply(lambda s: s.split()).values\ntrain_vocab = build_vocab(train_tweets)\ntest_tweets = test_df['text'].apply(lambda s: s.split()).values\ntest_vocab = build_vocab(test_tweets)\ncorpus_tweets=df['text'].apply(lambda s: s.split()).values\ncorpus_vocab=build_vocab(corpus_tweets)","47739e51":"#embedding Glove\nembeddings_glove = np.load('..\/input\/pickled-glove840b300d-for-10sec-loading\/glove.840B.300d.pkl', allow_pickle=True)\n","26559c33":"#checking coverage\ndef check_coverage(vocab, embeddings, embeddings_name, dataset_name):\n    covered = {}\n    oov = {}    \n    n_covered = 0\n    n_oov = 0\n    \n    for word in vocab:\n        try:\n            covered[word]= embeddings[word]\n            n_covered+=vocab[word]\n        except:\n            oov[word]=vocab[word]\n            n_oov+=vocab[word]\n            \n    vocab_coverage=len(covered)\/len(vocab)\n    text_covrage= (n_covered\/(n_covered+n_oov))\n    print('{} Embeddings cover {:.2%} of {} vocab'.format(embeddings_name, vocab_coverage, dataset_name))\n    #sorted_oov = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n    #return sorted_oov\n\ntrain_oov_glove = check_coverage(train_vocab, embeddings_glove, 'GloVe', 'Training')\ntest_oov_glove = check_coverage(test_vocab, embeddings_glove, 'GloVe', 'Test')\n","c77da51a":"def clean(tweet):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    # Punctuations at the start or end of words    \n    for punctuation in \"#@!?()[]*%\":\n        tweet = tweet.replace(punctuation, f' {punctuation} ').strip()\n    tweet = tweet.replace('...', ' ... ').strip()\n    tweet = tweet.replace(\"'\", \" ' \").strip()   \n    #webpages\n    tweet= url.sub(r'',tweet)\n    #emojis\n    tweet = emoji_pattern.sub(r'', tweet)\n    #spellchecker\n    #tweet=correct_spellings(tweet)\n    \n    url.sub(r'',tweet)\n    #https\n    if 'http' not in tweet:\n        tweet = tweet.replace(\":\", \" : \").strip() \n        tweet = tweet.replace(\".\", \" . \").strip() \n    return  tweet\n\n","c6b6b5ca":"train_df['text'] = train_df['text'].apply(lambda x : clean(x))\ntest_df['text'] = test_df['text'].apply(lambda x : clean(x))\ndf['text'] = df['text'].apply(lambda x : clean(x))","af6279f9":"train_tweets_cleaned = train_df['text'].apply(lambda s: s.split()).values\ntrain_vocab_cleaned = build_vocab(train_tweets_cleaned)\ntest_tweets_cleaned = test_df['text'].apply(lambda s: s.split()).values\ntest_vocab_cleaned = build_vocab(test_tweets_cleaned)\ncorpus_tweets=df['text'].apply(lambda s: s.split()).values\ncorpus_vocab=build_vocab(corpus_tweets)\n\ntrain_oov_glove = check_coverage(train_vocab_cleaned, embeddings_glove, 'GloVe', 'Training')\ntest_oov_glove = check_coverage(test_vocab_cleaned, embeddings_glove, 'GloVe', 'Test')\n","72595806":"df=pd.concat([train_df,test_df])\ndf.shape","18caa855":"def create_corpus(df):\n    corpus=[]\n    for tweet in tqdm(df['text']):\n        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n        corpus.append(words)\n    return corpus","f2bd89e8":"from tqdm import tqdm\nfrom nltk.tokenize import word_tokenize\nstop=set(stopwords.words('english'))\ncorpus=create_corpus(df)\n#corpus_test=create_corpus(test_df)","1bcd95f3":"embedding_dict={}\nwith open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt','r')  as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()\n\n","81549f4c":"MAX_LEN=50\ntokenizer_obj=Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences=tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')\n\n","06afd355":"word_index=tokenizer_obj.word_index\nprint('Number of unique words:',len(word_index))\n","6ef8405c":"num_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,100))\n\nfor word,i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n    \n    emb_vec=embedding_dict.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i]=emb_vec","ea06ddba":"tqdm.pandas()\n!pip install textstat\nimport textstat","c64c0de2":"def plot_readability(a,b,title,bins=0.1,colors=['mediumvioletred', 'goldenrod']):\n    trace1 = ff.create_distplot([a,b], [\" disaster\",\"Not disaster\"], bin_size=bins, colors=colors, show_rug=False)\n    trace1['layout'].update(title=title)\n    py.iplot(trace1, filename='Distplot')\n    table_data= [[\"Statistical Measures\",\" Not real disaster tweets\",\"real disaster tweets\"],\n                [\"Mean\",np.mean(a),np.mean(b)],\n                [\"Standard Deviation\",pstdev(a),pstdev(b)],\n                [\"Variance\",pvariance(a),pvariance(b)],\n                [\"Median\",median(a),median(b)],\n                [\"Maximum value\",max(a),max(b)],\n                [\"Minimum value\",min(a),min(b)]]\n    trace2 = ff.create_table(table_data)\n    py.iplot(trace2, filename='Table')","8f5c1221":"fre_notreal=np.array(train_df[\"text\"][train_df[\"target\"] == 0].progress_apply(textstat.flesch_reading_ease))\nfre_real = np.array(train_df[\"text\"][train_df[\"target\"] == 1].progress_apply(textstat.flesch_reading_ease))\nplot_readability(fre_notreal,fre_real,\"Flesch Reading Ease\",20)","ce54f21c":"fkg_notreal = np.array(train_df[\"text\"][train_df[\"target\"] == 0].progress_apply(textstat.flesch_kincaid_grade))\nfkg_real = np.array(train_df[\"text\"][train_df[\"target\"] == 1].progress_apply(textstat.flesch_kincaid_grade))\nplot_readability(fkg_notreal,fkg_real,\"Flesch Kincaid Grade\",4,['#C1D37F','#491F21'])","372d33ba":"import spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\nparser = English()\npunctuations = string.punctuation\nstopwords = list(STOP_WORDS)\n\ndef spacy_tokenizer(sentence):\n    mytokens = parser(sentence)\n    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n    mytokens = [ word for word in mytokens if word not in stopwords and word not in punctuations ]\n    mytokens = \" \".join([i for i in mytokens])\n    return mytokens","e325abc1":"notreal_text = train_df[\"text\"][train_df[\"target\"] == 0].progress_apply(spacy_tokenizer)\nreal_text = train_df[\"text\"][train_df[\"target\"] == 1].progress_apply(spacy_tokenizer)\n#count vectorization\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nvectorizer_notreal = CountVectorizer(min_df=5, max_df=0.9, stop_words='english', lowercase=True, token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\nnotreal_vectorized = vectorizer_notreal.fit_transform(notreal_text)\nvectorizer_real = CountVectorizer(min_df=5, max_df=0.9, stop_words='english', lowercase=True, token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\nreal_vectorized = vectorizer_real.fit_transform(real_text)","42dbba62":"from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n\n\nlda_notreal = LatentDirichletAllocation(n_components=10, max_iter=5, learning_method='online',verbose=True)\nnotreal_lda = lda_notreal.fit_transform(notreal_vectorized)\nlda_real = LatentDirichletAllocation(n_components=10, max_iter=5, learning_method='online',verbose=True)\nreal_lda = lda_real.fit_transform(real_vectorized)","5e87b346":"def selected_topics(model, vectorizer, top_n=10):\n    for idx, topic in enumerate(model.components_):\n        print(\"Topic %d:\" % (idx))\n        print([(vectorizer.get_feature_names()[i], topic[i])\n                        for i in topic.argsort()[:-top_n - 1:-1]]) ","a0792200":"\nselected_topics(lda_notreal, vectorizer_notreal)","4340ac1f":"import pyLDAvis.sklearn\npyLDAvis.enable_notebook()\ndash = pyLDAvis.sklearn.prepare(lda_notreal, notreal_vectorized, vectorizer_notreal, mds='tsne')\ndash","354a7d9e":"pyLDAvis.enable_notebook()\ndash = pyLDAvis.sklearn.prepare(lda_real, real_vectorized, vectorizer_real, mds='tsne')\ndash","6dc7c9c8":"tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\ntfidf_vec.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\ntrain_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\ntest_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())","67de0187":"from sklearn import model_selection, preprocessing, metrics, ensemble, naive_bayes, linear_model","b9db3101":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(train_df,train_df['target'].values,test_size=0.15)","f8029155":"#model = linear_model.LogisticRegression(C=5., solver='sag')\n#model.fit(X_train, y_train)","18acc597":"train_y = train_df[\"target\"].values\n\ndef runModel(train_X, train_y, test_X, test_y, test_X2):\n    model = linear_model.LogisticRegression(C=5., solver='sag')\n    model.fit(train_X, train_y)\n    pred_test_y = model.predict_proba(test_X)[:,1]\n    pred_test_y2 = model.predict_proba(test_X2)[:,1]\n    return pred_test_y, pred_test_y2, model\n\nprint(\"Building model.\")\ncv_scores = []\npred_full_test = 0\npred_train = np.zeros([train_df.shape[0]])\nkf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\nfor dev_index, val_index in kf.split(train_df):\n    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    pred_val_y, pred_test_y, model = runModel(dev_X, dev_y, val_X, val_y, test_tfidf)\n    pred_full_test = pred_full_test + pred_test_y\n    pred_train[val_index] = pred_val_y\n    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n    break","709deb83":"runModel(dev_X, dev_y, val_X, val_y, test_tfidf)","bc2e7889":"from tqdm import tqdm\ndef threshold_search(y_true, y_proba):\n#reference: https:\/\/www.kaggle.com\/hung96ad\/pytorch-starter\n    best_threshold = 0\n    best_score = 0\n    for threshold in tqdm([i * 0.001 for i in range(1000)]):\n        score = metrics.f1_score(y_true=y_true, y_pred=y_proba > threshold)\n        if score > best_score:\n            best_threshold = threshold\n            best_score = score\n    search_result = {'threshold': best_threshold, 'f1': best_score}\n    return search_result\nsearch_result = threshold_search(val_y, pred_val_y)\nsearch_result","7b60b2da":"#sample_sub=pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\n#y_pre=model.predict(test_tfidf)\n#y_pre=np.round(y_pre).astype(int).reshape(3263)\n#sub=pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_pre})\n#sub.to_csv('submission.csv',index=False)","06ad02ca":"print(\"F1 score at threshold {0} is {1}\".format(0.381, metrics.f1_score(val_y, (pred_val_y>0.381).astype(int))))\nprint(\"Precision at threshold {0} is {1}\".format(0.381, metrics.precision_score(val_y, (pred_val_y>0.381).astype(int))))\nprint(\"recall score at threshold {0} is {1}\".format(0.381, metrics.recall_score(val_y, (pred_val_y>0.381).astype(int))))\n","197ed79e":"import eli5\neli5.show_weights(model, vec=tfidf_vec, top=100, feature_filter=lambda x: x != '<BIAS>')\n","33e7c5ff":"!pip install simpletransformers\nimport os, re, string\nimport random\n\nimport numpy as np\nimport pandas as pd\nimport sklearn\n\nimport torch\n\nfrom simpletransformers.classification import ClassificationModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold","c01635b6":"\nseed = 1337\ntorch.cuda.manual_seed(seed)\nbert_uncased = ClassificationModel('bert', 'bert-large-uncased') \n","b0b98fa7":"custom_args = {'fp16': False, # not using mixed precision \n               'train_batch_size': 4, # default is 8\n               'gradient_accumulation_steps': 2,\n               'do_lower_case': True,\n               'learning_rate': 1e-05, # using lower learning rate\n               'overwrite_output_dir': True, # important for CV\n               'num_train_epochs': 2} # default is 1","5d3dc2e1":"train_df=train_df.iloc[:,3:5]","706fb409":"train_y = train_df[\"target\"].values\nmodel = ClassificationModel('bert', 'bert-base-uncased', args=custom_args) \nmodel.train_model(train_df)","9bbe6b98":"#test_df= test_df.iloc[:,0]\ntest_df.head()\n\npredictions, raw_outputs = model.predict(test_df['text'])\nsample_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\nsample_submission[\"target\"] = predictions\nsample_submission.to_csv(\"submission.csv\", index=False)","a7488565":"from keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom keras.optimizers import Adam\n\nmodel=Sequential()\n\nembedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False)\n\n\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(64, dropout=0.2,recurrent_dropout=0.2))\nmodel.add(Dense(1,activation='sigmoid'))\noptimizer= Adam(learning_rate=1e-5)\nmodel.compile(loss='binary_crossentropy',optimizer= optimizer,metrics=['accuracy'])","00852a2b":"model.summary()","6f04a5ce":"train=tweet_pad[:train_df.shape[0]]\ntest=tweet_pad[train_df.shape[0]:]","861a6681":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(train,train_df['target'].values,test_size=0.15)\nprint('Shape of train',X_train.shape)\nprint(\"Shape of Validation \",X_test.shape)","98e5c8df":"history=model.fit(X_train,y_train,batch_size=4,epochs=15,validation_data=(X_test,y_test),verbose=2)","03e697de":"test_df.shape\ntest.shape","23f5624d":"\n#sample_sub=pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\n#y_pre=model.predict(test)\n#y_pre=np.round(y_pre).astype(int).reshape(3263)\n#sub=pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_pre})\n#sub.to_csv('submission.csv',index=False)\n\n","758b4ec6":"Readability features","39a3081e":"Keywords\n","2334fc14":"Locations vs Targets","7f29f62e":"Topic modeling","22b996bc":"NGRAMs\noption1 :\n","e8b32be7":"Model deep learning seq\n\n","7f2ebde7":"import re\nfrom nltk.util import ngrams\n\ntrain_df['text'] = train_df['text'].str.lower()\ns = re.sub(r'[^a-zA-Z0-9\\s]', ' ',train_df['text'].str)\ntokens = [token for token in train_df.text.str.split(\" \") if token != \"\"]\noutput = list(ngrams(tokens, 1))\noutput","f3b80e3b":"Spellchecker\n","a1609eaf":"Locations\n","b931b027":"MetaData features\n","840303ef":"Prediction and submission","62e15126":"Model on TFidf","22a6b852":"#!pip install pyspellchecker\nfrom spellchecker import SpellChecker\n\nspell = SpellChecker()\ndef correct_spellings(tweet):\n    corrected_text = []\n    misspelled_words = spell.unknown(tweet.split())\n    for word in tweet.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)\n        \ntweet = \"corect me plese\"\ncorrect_spellings(tweet)","738a986f":"Simple transformers\n","ecc59f68":"LDA\n","af71f9e4":"> - GLOVE 1 **"}}