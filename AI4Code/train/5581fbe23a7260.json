{"cell_type":{"2981b780":"code","73b336f3":"code","eecab203":"code","3277f758":"code","5bfdb6a4":"code","fc4c99f3":"code","af5b2632":"code","0320f0b4":"code","0acf0c7f":"code","761007fa":"code","e86f4d88":"code","c812b149":"code","05501025":"code","0f065263":"code","4b3d6f86":"code","5c953ba9":"code","b3c2ff3b":"code","72ffe2e3":"code","5710c321":"code","aa91a0af":"code","8d9952fd":"code","34dd943f":"code","276aed1b":"code","c56a2509":"code","a4c02cd8":"code","49673626":"code","f89e9ce7":"code","dcdabefd":"markdown","3967fa9d":"markdown","43293015":"markdown","dbd9f8f5":"markdown","829c3883":"markdown","ccfb61f0":"markdown","378eba0a":"markdown","ba12f015":"markdown","035bd44b":"markdown","c0ea333d":"markdown","d01a9bf2":"markdown"},"source":{"2981b780":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n\n#install python & kaggle dependencies\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","73b336f3":"#load data which is sourced from previous kaggle contributions\ndata = pd.read_csv(\"..\/input\/urldataset\/data.csv\")","eecab203":"#old skewl data triage\ndata.shape","3277f758":"data.head(5)","5bfdb6a4":"import pandas_profiling\nfrom pandas_profiling import ProfileReport","fc4c99f3":"#the file is to large for full visual so we need to minimize\nprofile = profile = ProfileReport(data, minimal=True, title = \"Features to Evaluate Data Profile\")","af5b2632":"#Let's look at the data from within the notebook\nprofile.to_notebook_iframe()","0320f0b4":"#use this if you want a seperate html file outside of jupyter notebook.  \n#In Kaggle, look to the right panel in the 'output' folder to find the html file\nprofile = ProfileReport(data, minimal=True)\nprofile.to_file(\"output.html\")","0acf0c7f":"# install additional library\n!pip install tldextract -q\n\n# import library\nimport numpy as np\nimport pandas as pd\nimport re\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport plotly.io as pio\nfrom plotly.subplots import make_subplots\nimport seaborn as sns\nimport gc\nimport random\nimport os\nimport pickle\nimport tensorflow as tf\nfrom tensorflow.python.util import deprecation\nfrom urllib.parse import urlparse\nimport tldextract\n\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras import models, layers, backend, metrics\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom keras.utils.vis_utils import plot_model\nfrom PIL import Image\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n# set random seed\nos.environ['PYTHONHASHSEED'] = '0'\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nnp.random.seed(0)\nrandom.seed(0)\ntf.random.set_seed(0)\n\n# other setup\n%config InlineBackend.figure_format = 'retina'\npd.set_option('max_colwidth', 50)\npio.templates.default = \"presentation\"\npd.options.plotting.backend = \"plotly\"\ndeprecation._PRINT_DEPRECATION_WARNINGS = False","761007fa":"fig = go.Figure([go.Pie(labels=['Good', 'Bad'], values=data.label.value_counts())])\nfig.update_layout(title='Percentage of Class (Good vs. Bad)')\nfig.show()","e86f4d88":"#set value size as .2, aka 20%, which will be the test data\nval_size = 0.2\ntrain_data, val_data = train_test_split(data, test_size=val_size, stratify=data['label'], random_state=0)\nfig = go.Figure([go.Pie(labels=['Train Size', 'Validation Size'], values=[train_data.shape[0], val_data.shape[0]])])\nfig.update_layout(title='Train and Validation Size')\nfig.show()","c812b149":"\ndef parsed_url(url):\n    # extract subdomain, domain, and domain suffix from url\n    # if item == '', fill with '<empty>'\n    subdomain, domain, domain_suffix = ('<empty>' if extracted == '' else extracted for extracted in tldextract.extract(url))\n    return [subdomain, domain, domain_suffix]\n\ndef extract_url(data):\n    # pass the parsed_url(url) as a for loop and create new columns.  Create new df with results\n    extract_url_data = [parsed_url(url) for url in data['url']]\n    extract_url_data = pd.DataFrame(extract_url_data, columns=['subdomain', 'domain', 'domain_suffix'])\n    \n    # concat extracted feature with main data\n    data = data.reset_index(drop=True)\n    data = pd.concat([data, extract_url_data], axis=1)\n    \n    return data\n\n#verify changes occured\ndata.head(5)\n\n","05501025":"def get_frequent_group(data, n_group):\n    # get the most frequent\n    data = data.value_counts().reset_index(name='values')\n    \n    # scale log base 10\n    data['values'] = np.log10(data['values'])\n    \n    # calculate total values\n    # x_column (subdomain \/ domain \/ domain_suffix)\n    x_column = data.columns[1]\n    data['total_values'] = data[x_column].map(data.groupby(x_column)['values'].sum().to_dict())\n    \n    # get n_group data order by highest values\n    data_group = data.sort_values('total_values', ascending=False).iloc[:, 1].unique()[:n_group]\n    data = data[data.iloc[:, 1].isin(data_group)]\n    data = data.sort_values('total_values', ascending=False)\n    \n    return data\n\ndata","0f065263":"def plot(data, n_group, title):\n    data = get_frequent_group(data, n_group)\n    fig = px.bar(data, x=data.columns[1], y='values', color='label')\n    fig.update_layout(title=title)\n    fig.show()\n\n# extract url\ndata = extract_url(data)\ntrain_data = extract_url(train_data)\nval_data = extract_url(val_data)","4b3d6f86":"data","5c953ba9":"fig = go.Figure([go.Bar(\n    x=['domain', 'Subdomain', 'Domain Suffix'], \n    y = [data.domain.nunique(), data.subdomain.nunique(), data.domain_suffix.nunique()]\n)])\nfig.show()","b3c2ff3b":"#Let's look at the data from within the notebook\nprofile = ProfileReport(data, minimal=True)\nprofile.to_notebook_iframe()","72ffe2e3":"data.head(5)","5710c321":"plot(\n    data=data.groupby('label')['domain'], \n    n_group=20, \n    title='Top 20 Domains Grouped By Labels (Logarithmic Scale)'\n)","aa91a0af":"# tokenization on the url so that it can be used as input to the CNN model\ntokenizer = Tokenizer(filters='', char_level=True, lower=False, oov_token=1)\n\n# fit only on training data\ntokenizer.fit_on_texts(train_data['url'])\nn_char = len(tokenizer.word_index.keys())\n\ntrain_seq = tokenizer.texts_to_sequences(train_data['url'])\nval_seq = tokenizer.texts_to_sequences(val_data['url'])\n\nprint('Before tokenization: ')\nprint(train_data.iloc[0]['url'])\nprint('\\nAfter tokenization: ')\nprint(train_seq[0])","8d9952fd":"# Each url has a different length, therefore padding is needed to equalize each url length. \n# Next step we will do padding on url column that was just tokenized\nsequence_length = np.array([len(i) for i in train_seq])\nsequence_length = np.percentile(sequence_length, 99).astype(int)\nprint(f'Before padding: \\n {train_seq[0]}')\ntrain_seq = pad_sequences(train_seq, padding='post', maxlen=sequence_length)\nval_seq = pad_sequences(val_seq, padding='post', maxlen=sequence_length)\nprint(f'After padding: \\n {train_seq[0]}')","34dd943f":"#now lets tokenize the other columns\nunique_value = {}\nfor feature in ['subdomain', 'domain', 'domain_suffix']:\n    # get unique value\n    label_index = {label: index for index, label in enumerate(train_data[feature].unique())}\n    \n    # add unknown label in last index\n    label_index['<unknown>'] = list(label_index.values())[-1] + 1\n    \n    # count unique value\n    unique_value[feature] = label_index['<unknown>']\n    \n    # encode\n    train_data.loc[:, feature] = [label_index[val] if val in label_index else label_index['<unknown>'] for val in train_data.loc[:, feature]]\n    val_data.loc[:, feature] = [label_index[val] if val in label_index else label_index['<unknown>'] for val in val_data.loc[:, feature]]\n    \ntrain_data.head()","276aed1b":"#Encode the label \nfor data in [train_data, val_data]:\n    data.loc[:, 'label'] = [0 if i == 'good' else 1 for i in data.loc[:, 'label']]\n    \ntrain_data.head()","c56a2509":"def convolution_block(x):\n    conv_3_layer = layers.Conv1D(64, 3, padding='same', activation='elu')(x)\n    conv_5_layer = layers.Conv1D(64, 5, padding='same', activation='elu')(x)\n    conv_layer = layers.concatenate([x, conv_3_layer, conv_5_layer])\n    conv_layer = layers.Flatten()(conv_layer)\n    return conv_layer\n\ndef embedding_block(unique_value, size, name):\n    input_layer = layers.Input(shape=(1,), name=name + '_input')\n    embedding_layer = layers.Embedding(unique_value, size, input_length=1)(input_layer)\n    return input_layer, embedding_layer\n\ndef create_model(sequence_length, n_char, unique_value):\n    input_layer = []\n    \n    # sequence input layer\n    sequence_input_layer = layers.Input(shape=(sequence_length,), name='url_input')\n    input_layer.append(sequence_input_layer)\n    \n    # convolution block\n    char_embedding = layers.Embedding(n_char + 1, 32, input_length=sequence_length)(sequence_input_layer)\n    conv_layer = convolution_block(char_embedding)\n    \n    # entity embedding\n    entity_embedding = []\n    for key, n in unique_value.items():\n        size = 4\n        input_l, embedding_l = embedding_block(n + 1, size, key)\n        embedding_l = layers.Reshape(target_shape=(size,))(embedding_l)\n        input_layer.append(input_l)\n        entity_embedding.append(embedding_l)\n        \n    # concat all layer\n    fc_layer = layers.concatenate([conv_layer, *entity_embedding])\n    fc_layer = layers.Dropout(rate=0.5)(fc_layer)\n    \n    # dense layer\n    fc_layer = layers.Dense(128, activation='elu')(fc_layer)\n    fc_layer = layers.Dropout(rate=0.2)(fc_layer)\n    \n    # output layer\n    output_layer = layers.Dense(1, activation='sigmoid')(fc_layer)\n    model = models.Model(inputs=input_layer, outputs=output_layer)\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[metrics.Precision(), metrics.Recall()])\n    return model\n\n# reset session\nbackend.clear_session()\nos.environ['PYTHONHASHSEED'] = '0'\nnp.random.seed(0)\nrandom.seed(0)\ntf.random.set_seed(0)\n\n# create model\nmodel = create_model(sequence_length, n_char, unique_value)\n\n# show model architecture\nplot_model(model, to_file='model.png')\nmodel_image = mpimg.imread('model.png')\nplt.figure(figsize=(75, 75))\nplt.imshow(model_image)\nplt.show()","a4c02cd8":"# create train data\ntrain_x = [train_seq, train_data['subdomain'], train_data['domain'], train_data['domain_suffix']]\ntrain_y = train_data['label'].values\n\n# model training\nearly_stopping = [EarlyStopping(monitor='val_precision', patience=5, restore_best_weights=True, mode='max')]\nhistory = model.fit(train_x, train_y, batch_size=64, epochs=25, verbose=1, validation_split=0.2, shuffle=True, callbacks=early_stopping)\nmodel.save('model.h5')","49673626":"fig = make_subplots(3, 1, subplot_titles=('loss', 'precision', 'recall'))\n\nfor index, key in enumerate(['loss', 'precision', 'recall']):\n    # train score\n    fig.add_trace(go.Scatter(\n        x=list(range(len(history.history[key]))),\n        y=history.history[key],\n        mode='lines+markers',\n        name=key\n    ), index + 1, 1)\n    \n    # val score\n    fig.add_trace(go.Scatter(\n        x=list(range(len(history.history[f'val_{key}']))),\n        y=history.history[f'val_{key}'],\n        mode='lines+markers',\n        name=f'val {key}'\n    ), index + 1, 1)\n\nfig.show()","f89e9ce7":"val_x = [val_seq, val_data['subdomain'], val_data['domain'], val_data['domain_suffix']]\nval_y = val_data['label'].values\n\nval_pred = model.predict(val_x)\nval_pred = np.where(val_pred[:, 0] >= 0.5, 1, 0)\nprint(f'Validation Data:\\n{val_data.label.value_counts()}')\nprint(f'\\n\\nConfusion Matrix:\\n{confusion_matrix(val_y, val_pred)}')\nprint(f'\\n\\nClassification Report:\\n{classification_report(val_y, val_pred)}')","dcdabefd":"# Pandas Profiling Complete\n\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/","3967fa9d":"##Now lets introduce some pandas profiling ","43293015":"# Feature Engineering\nLet's do some feature engineering. First we want to find out whether the data is imbalance","dbd9f8f5":"#At this point, we have done EDA with Pandas Profiling.  We have replicated the original code and recieved similar results.  Now we must improve upon them.  In other words, we have met goals 1 & 2.  On to goal 3.","829c3883":"Let's download the dependencies first","ccfb61f0":"## Interpretting the results above:\nA False results will be accurately predicted with 98% correctness.\nInversley, a 'false' results will be wrong 2% of the time\nA 'postitive' result will be correct 96% of the time, with 'False positives 1.8% frequency and false negatives occuring 0.6%\n## So what?  Scale is the problem...\n97 or 98% accuracy seems pretty good at first.  Now assume you work in a bank and your blue coat system automatically blockes malicious traffic. JP Morgan moves 3 Trillion dollars a day, there are over 100k web domain connections a day.  That means you're system would incorrectly block ~2K websites a day.  WHAT IF, one of those domains was a money movement domain.  Imagine telling Jamie Dimon that 2% of 3 Trillion dollars flow correctly, because your code was wrong 2% of the time. $6 Billion dollars a DAY, because you were 'only' 98% correct.  \n\nThis is a good start, but we need better.","378eba0a":"# MALICIOUS WEB DETECTION WITH 1D CNN (Convolution Neural Network)\n## Author: ** Perlz **\n## Initial Author: ** Rakha Paleva Kawiswara **\n\nThis work is a branch of the orginial work done by Rakha Paleva Kawisara\n\nFeel free to use this notebook for your research as well as the original contribution by Rakha Kawisara and upvotes to this notebook :). Suggestion on this notebook is very expected, Thanks!\n\n*nb\n1. psst... do not open the url contained in the data, to avoid opening dangerous websites. Because you know.....they are malicious!\n\nWhat is Malicious Website?\nA malicious website is a site that attempts to install malware (a general term for anything that will disrupt computer operation, gather your personal information or, in a worst-case scenario, gain total access to your machine) onto your device. This usually requires some action on your part, however, in the case of a drive-by download, the website will attempt to install software on your computer without asking for permission first. (source: https:\/\/us.norton.com\/internetsecurity-malware-what-are-malicious-websites.html)\n\nNotebook Goals\n1. Demonstrate EDA of of the original data set using Pandas-Profiling\n2. Replicate the 1D Convolutional Neural Network used to detect malicious websites\n3. Improve upon the original CNN written by Kawisara\n\nNotebook methodology: This notebook will create a model that can detect malicious websites. Website url is used as a feature and 1D Convolutional Neural Network (CNN) is used as an algorithm for detection malicious websites. Model will be validated by holdout method","ba12f015":"# Exploratory Data Analysis with Pandas Profiling","035bd44b":"The above took 3 hours to run....so prepare\/plan ahead;)","c0ea333d":"# Duplicate and Create the original CNN Model","d01a9bf2":"## Pandas-Profiling EDA Findings\n* 411247 Distinct rows of data\n* 9216 duplicate rows of data, represents 2.2% of data\n** No value occurs more than 27 times\n* urls have 'high cardinality', which means they are unique and great candidates for analysis\n\n"}}