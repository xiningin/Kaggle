{"cell_type":{"10838d74":"code","33b857db":"code","6a334457":"code","b741dfcd":"code","9a335b30":"code","39c122d1":"code","43f6911b":"code","b637eefb":"code","5ad218a8":"code","582b70fd":"code","2144eb82":"code","8cb24352":"code","d46215d6":"code","7aeb1e01":"code","3bd0f252":"code","496fea5a":"code","1a49f0d1":"code","cc615785":"code","263a486c":"code","166b2cc8":"code","1526720b":"markdown","a7456e92":"markdown","82524a71":"markdown","a8a9d2b1":"markdown","b1a7b1c0":"markdown","718a9ae0":"markdown","66b2e8ef":"markdown","efb97dc5":"markdown","f4d8658b":"markdown"},"source":{"10838d74":"import numpy as np \nimport pandas as pd\nimport os, gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\n\nsns.set_style('whitegrid')\nfrom sklearn.model_selection import train_test_split, KFold, GridSearchCV\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","33b857db":"PATH = '..\/input\/jane-street-market-prediction\/'\n\ntrain = pd.read_csv(PATH + 'train.csv')\nprint(train.shape)","6a334457":"train.head(10)","b741dfcd":"train = train[train['weight'] != 0]\ntrain['action'] = (train['resp'] > 0).astype(int)\nsns.countplot(train['action'])","9a335b30":"train.describe()","39c122d1":"FEATURES = [x for x in train.columns if 'feature' in x]\nlen(FEATURES)","43f6911b":"missing_values = pd.DataFrame()\nmissing_values['column'] = FEATURES\nmissing_values['num_missing'] = [train[i].isna().sum() for i in FEATURES]\n\nmissing_values.T","b637eefb":"unique_vals = pd.DataFrame()\nunique_vals['column'] = FEATURES\nunique_vals['num_missing'] = [train[i].nunique() for i in FEATURES]\n\nunique_vals.T","5ad218a8":"fig, ax = plt.subplots(10, 10, figsize=(20,22))\nax = ax.flatten()\n\nfor k,i in enumerate(FEATURES[1:101]):\n    sns.distplot(train[train['action'] == 0][i], hist=False, label='0', ax=ax[k])\n    sns.distplot(train[train['action'] == 1][i], hist=False, label='1', ax=ax[k])","582b70fd":"fig, ax = plt.subplots(6, 5, figsize=(20,22))\nax = ax.flatten()\n\nfor k,i in enumerate(FEATURES[101:]):\n    sns.distplot(train[train['action'] == 0][i], hist=False, label='0', ax=ax[k])\n    sns.distplot(train[train['action'] == 1][i], hist=False, label='1', ax=ax[k])","2144eb82":"p = FEATURES\np.append('resp')\nlen(p)","8cb24352":"x = train[p].corr()\nx","d46215d6":"x = x.abs()\nupper = x.where(np.triu(np.ones(x.shape), k=1).astype(np.bool))\nto_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\nprint(to_drop)","7aeb1e01":"train.drop(to_drop, 1, inplace=True)\ntrain","3bd0f252":"FEATURES = [x for x in train.columns if 'feature' in x]\n\nX = train[FEATURES]\ny = train['action']\nprint(X.shape, y.shape)","496fea5a":"model = lgb.LGBMRegressor()\ncv = KFold(shuffle=True, n_splits=5, random_state=108)\nparams = {\n    'n_estimators':[500]\n#     'learning_rate':[0.1, 0.001, 0.5],\n#     'subsample':[1, 0.9],\n#     'feature_fraction':[1, 0.9]\n}\n\nclf = GridSearchCV(\n    estimator=model, \n    scoring='neg_mean_squared_error',\n    cv = cv,\n    param_grid=params, \n    verbose=10\n)","1a49f0d1":"clf.fit(X, y)","cc615785":"feature_imp = pd.DataFrame()\nfeature_imp['imp'] = clf.best_estimator_.feature_importances_\nfeature_imp['column'] = X.columns\n\nfeature_imp = feature_imp.sort_values(by='imp', ascending=False)\n\nplt.figure(figsize=(15,20))\nsns.barplot(feature_imp.imp[:30], feature_imp.column[:30])","263a486c":"import janestreet\nenv = janestreet.make_env() \niter_test = env.iter_test() ","166b2cc8":"for (test_df, sample_prediction_df) in iter_test:\n    wt = test_df.iloc[0].weight\n    if(wt == 0):\n        sample_prediction_df.action = 0 \n    else:\n        sample_prediction_df.action = (clf.predict(test_df.loc[:, FEATURES]) > 0.5).astype(int)\n    env.predict(sample_prediction_df)","1526720b":"For most features, the distributions are very similar for the two target values. In case of some features like 91, 94, 103, 115 the distributions are different. We can use this in feature selection.\n\nNow let's check the correlations b\/w features.","a7456e92":"The dataset is balanced w.r.t target 'action'. Now let's analyse the features.","82524a71":"## <a>Model training<\/a>","a8a9d2b1":"Dropping columns where correlation coeff. > 0.95","b1a7b1c0":"Here we have, \n\n1. date column which represents the day of the trade and ts_id represents a time ordering.\n2. 130 anonymized features - feature_{0...129}\n3. weight and resp - which together represents a return on the trade\n4. resp_{1,2,3,4} values that represent returns over different time horizons.\n5. The target 'action' is not present in the train set. \n\nIn the data section of the competition, it is mentioned that **\"Trades with weight = 0 were intentionally included in the dataset for completeness, although such trades will not contribute towards the scoring evaluation.\"** \n\nSo, we can remove rows where weight=0.\n\nTarget variable 'action' is not present in the train set, we can create one by setting action=1 where resp>0 and action=0 where resp<0. By doing this we are passing the trades(where resp<0) which decreases the utility score.\n\nTo better understand utility score, check this simple and detailed explanation https:\/\/www.kaggle.com\/renataghisloti\/understanding-the-utility-score-function","718a9ae0":"Here, \n1. Weight of the trade is always +ve, min value being 0.\n2. Resp varies from -0.54 to 0.44, never exceeding 1 in either direction.\n3. The mean and std values are low for many features and NaNs are present.\n\n\n","66b2e8ef":"## <a>Loading Packages and Data<\/a>","efb97dc5":"We've many columns with high number of missing values. 'feature_0' is float64 type but contains only '1' and '-1' as values. Let's see if there are any other such columns.","f4d8658b":"All other feature columns have continuous values. Now, let's compare the distribution of features w.r.t two target values."}}