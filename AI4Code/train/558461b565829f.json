{"cell_type":{"4c56570a":"code","1fe2cfe0":"code","735bf062":"code","6e5934f1":"code","07556c3e":"code","77c791fa":"code","3e8454c2":"code","b905d9eb":"code","c5bc24d4":"code","331e7079":"code","a3e12d46":"code","3a62349c":"code","91781339":"code","62364f6a":"code","fc6277f9":"code","47a7b51e":"code","76af4e0c":"code","84670a2a":"code","37f061cc":"code","bd69cfea":"code","3a83ca88":"code","5aea8c50":"code","b3f955a6":"code","d6857b42":"code","1698b6e3":"code","d62558b5":"code","3295d71a":"code","378984e6":"code","3c266750":"code","5d8af9a8":"markdown","1e10b809":"markdown","9cf50570":"markdown","8e765b99":"markdown","c8d8ee0d":"markdown","b0c22513":"markdown"},"source":{"4c56570a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nfrom sklearn import ensemble, linear_model, metrics, preprocessing, model_selection, feature_selection, pipeline\nimport lightgbm as lgb\nimport tsfresh\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport catboost as cb\nfrom skopt import gp_minimize, space, gbrt_minimize, dummy_minimize, forest_minimize\nfrom functools import partial","1fe2cfe0":"# Function to optimize hyperparameters using skopt. We will be tuning lightgbm\ndef optimize_sk(params, param_names, X, y, scoring, estimator, cv = model_selection.KFold(n_splits = 5)):\n    '''params: list of param values\n    param_names: param names\n    x: training exogs\n    y: training endogs\n    return: negative metric after k fold validation'''\n\n    params = dict(zip(param_names, params))\n\n    # Initialize the model\n    model = estimator(**params)\n\n    kf = cv\n\n    scores = []\n    for train_index, test_index in kf.split(X, y):\n        # Split Data\n        X_train, y_train = np.array(X)[train_index, :], y[train_index]\n        X_test, y_test = np.array(X)[test_index, :], y[test_index]\n\n        # Fit model\n        model.fit(X_train, y_train)\n\n        # Evaluate model\n        preds = model.predict(X_test)\n        scores.append(scoring(y_test, preds))\n\n    return np.mean(scores)\n\ndef rmse(y_true, y_pred):\n    return metrics.mean_squared_error(y_true, y_pred)\n\n# Parameter Space\nparam_space = [\n    space.Integer(500, 2500, name = 'n_estimators'),\n    space.Integer(2, 10, name = 'max_depth'),\n    space.Real(0, 1, name = 'feature_fraction'),\n    space.Integer(2, 25, name = 'min_data_in_leaf'),\n    space.Real(.001, .4, name = 'learning_rate')\n]\n\n# Param names\nnames = ['n_estimators', 'max_depth', 'feature_fraction', 'min_data_in_leaf', 'learning_rate']","735bf062":"# To Create aggregation function  - If required\ndef ts_agg_feature(data, groupby, column, func = np.mean, suffix = 'agg'):\n    return data.merge(data.groupby(groupby)[column].agg(func).reset_index(), on = groupby,\n                     suffixes = ('', '_'+suffix+'_'+groupby))","6e5934f1":"train = pd.read_csv('\/kaggle\/input\/demand-forecasting\/train_0irEZ2H.csv')\ntest = pd.read_csv('\/kaggle\/input\/demand-forecasting\/test_nfaJ3J5.csv')\nmpl.rcParams['font.size'] = 13","07556c3e":"# Visualize the train test time positions\ntr = train.copy()\nte = test.copy()\ntr['tt'] = 1\nte['tt'] = 0\n\nappended = pd.concat([tr[te.columns], te], axis = 0)\nappended['week'] = pd.to_datetime(appended['week'])\nappended = appended.set_index('week')\nappended = appended.sort_index()\n\nappended['tt'].plot.line(figsize = (16, 5), marker = 'o', linewidth = 0, grid = True)\nplt.yticks([0, 1], ['test', 'train'])\n","77c791fa":"train.head(1)","3e8454c2":"import datetime\ndef ts_features(data, col = 'week'):\n    data['day'] = data[col].dt.day\n    data['month'] = data[col].dt.month\n    data['year'] = data[col].dt.year\n    data['dayofweek'] = data[col].dt.dayofweek\n    data['weekofyear'] = data[col].dt.weekofyear\n    data['weekend'] = (data[col].dt.weekday >= 5).astype(int)\n    data['weekofmonth'] = data[col].dt.day\/\/7\n    data['quarter'] = data[col].dt.quarter\n    \n    start_date = datetime.datetime(2011,1, 17)\n    data['end_week_serial']  = [divmod((x-start_date).total_seconds(), 86400)[0]\/7 for x in data['week']]\n    return data\n","b905d9eb":"# Remove Outliers\ndef preprocess(data):\n    data = data[data['units_sold'] < 200]\n    return data\n\ntrain = preprocess(train)","c5bc24d4":"train2 = train.copy()\ntrain2['week'] = pd.to_datetime(train2['week'])\n\nx = 'base_price'\ny = 'units_sold'\n","331e7079":"appended = pd.concat([tr[te.columns], te], axis = 0, ignore_index = True)\nappended['week'] = pd.to_datetime(appended['week'])\nappended = ts_features(appended)\n\nappended = appended.set_index('week')\n\n#appended = pd.get_dummies(appended, columns = ['store_id', 'sku_id'], drop_first = True)\nappended['diff'] = appended['total_price'].values - appended['base_price'].values\nappended['div_by_base'] = appended['diff'].values\/appended['base_price'].values\nappended['div_by_total'] = appended['diff'].values\/appended['total_price'].values\n#appended['base_total'] = appended['total_price'].values*appended['base_price'].values\n\nappended\ntrain = appended.loc[appended['tt'] == 1, :]\ntest = appended.loc[appended['tt'] == 0, :]","a3e12d46":"to_drop = ['record_ID', 'tt']\ny_col = ['units_sold']\n\nX, y = train.drop(to_drop, axis = 1), tr[y_col[0]]\nxtest = test.drop(to_drop, axis = 1)\n\ntrain.head()","3a62349c":"X = X.fillna(X.median())","91781339":"X.head(2)","62364f6a":"# Tune Model\n# Define objective - reformat it in terms of what is required for skopt\nobjective_optimization = partial(optimize_sk, param_names = names, X = X, y = np.log1p(y), \n                                scoring = rmse, estimator = partial(lgb.LGBMRegressor, cat_columns = ['store_id', 'sku_id']))\n\n# Perform Optimization\n#gbrt_minimize, dummy_minimize, forest_minimize\nskopt_optimization = forest_minimize(func = objective_optimization, \n                                dimensions = param_space, n_calls = 35,  \n                                x0 = None, y0 = None, random_state = 10, \n                                verbose = 10)\n#print(skopt_optimization)","fc6277f9":"# Calculate k fold averaged MSE - we will use this while training the ensemble model\nsplitter = model_selection.KFold(5)\n\ndef validate(estimator, X, y, cv = splitter, split = None):\n    if split is None:\n        split = y\n        \n    scores = []\n    for train_index, test_index in splitter.split(X, split):\n        xtr, xte = np.array(X)[train_index, :], np.array(X)[test_index, :]\n        ytr, yte = np.array(y)[train_index], np.array(y)[test_index]\n        xtr = pd.DataFrame(xtr, columns = X.columns)\n        xtr['store_id'] = xtr['store_id'].astype(int)\n        xtr['sku_id'] = xtr['sku_id'].astype(int)\n        \n        xte = pd.DataFrame(xte, columns = X.columns)\n        xte['store_id'] = xte['store_id'].astype(int)\n        xte['sku_id'] = xte['sku_id'].astype(int)\n        model = estimator.fit(xtr, ytr)\n        \n        scores.append(metrics.mean_squared_error(yte, model.predict(xte)))\n        \n        \n    return np.mean(scores)\n\n","47a7b51e":"# Model 1 - Catboost\nmodel1 = cb.CatBoostRegressor(iterations = 2000, learning_rate = .02, max_depth = 8, \n                             cat_features = ['store_id', 'sku_id'], objective = 'RMSE', verbose = 0)\nwt1 = 1\/validate(model1, X, np.log1p(y), )\n\n# Model 2 - lightgbm\nmodel2 = lgb.LGBMRegressor(**dict(zip(names, skopt_optimization.x)))\nwt2 = 1\/validate(model2, X, np.log1p(y), )\n\n# Weigh the ensemble models by inverse of their mse\nmodel = ensemble.VotingRegressor([('catboost', model1), ('lgb', model2)], n_jobs = -1,\n                                 weights = [wt1, wt2]).fit(X, np.log1p(y))\n\n# Without weighing - ensemble simple average\n#model = ensemble.VotingRegressor([('catboost', model1), ('lgb', model2)], n_jobs = -1).fit(X, np.log1p(y))\npred = model.predict(xtest)\n\npreds = pd.DataFrame()\npreds['record_ID'] = te['record_ID']\npreds['units_sold'] = np.abs(np.exp(pred))\npreds.to_csv('SubA.csv', index = None)","76af4e0c":"import sys\n!{sys.executable} -m pip install rfpimp\n\nimport rfpimp\n\n# View Permutation importance of features\nimps = rfpimp.importances(model = model, X_valid = X, y_valid = y)\nrfpimp.plot_importances(imps)","84670a2a":"from tqdm.notebook import tqdm","37f061cc":"train = pd.read_csv('\/kaggle\/input\/demand-forecasting\/train_0irEZ2H.csv')\ntest = pd.read_csv('\/kaggle\/input\/demand-forecasting\/test_nfaJ3J5.csv')\nmpl.rcParams['font.size'] = 13","bd69cfea":"train['week'] = pd.to_datetime(train['week'])\ntest['week'] = pd.to_datetime(test['week'])","3a83ca88":"train = train.fillna(train.median())\n","5aea8c50":"import datetime\ndef ts_features(data, col = 'week'):\n    data['day'] = data[col].dt.day\n    data['month'] = data[col].dt.month\n    data['year'] = data[col].dt.year\n    data['dayofweek'] = data[col].dt.dayofweek\n    data['weekofyear'] = data[col].dt.weekofyear\n    data['weekend'] = (data[col].dt.weekday >= 5).astype(int)\n    data['weekofmonth'] = data[col].dt.day\/\/7\n    data['quarter'] = data[col].dt.quarter\n    \n    return data\n\ntraints = ts_features(train)\ntestts = ts_features(test)","b3f955a6":"rec_id_test = testts['record_ID']","d6857b42":"models = {}\n\n# Store store, sku combinations present in data\nindices = traints.groupby(['store_id', 'sku_id']).mean().index.tolist()","1698b6e3":"traints.head(1)","d62558b5":"xcols = ['total_price', 'base_price',\n       'is_featured_sku', 'is_display_sku', 'day', 'month',\n       'year', 'dayofweek', 'weekofyear', 'weekend','weekofmonth']\n\ntraints = traints.fillna(traints.median())\ntraints.head(2)","3295d71a":"# Define transform and Inverse transform for y\ndef y_transform(x):\n    return np.log(x)\n\ndef inverse_y(x):\n    return np.exp(x)","378984e6":"xs, ys = {}, {}\n\nfor count, combo in tqdm(enumerate(indices)):\n    temp = traints.loc[(traints['store_id'] == indices[count][0])&(traints['sku_id'] == indices[count][1]), :]\n    \n    # Define model\n    model = lgb.LGBMRegressor(n_estimators = 100, max_depth = 3, learning_rate = 0.05, max_features = .8)\n    Xtemp = temp[xcols]\n    ytemp = temp['units_sold']\n    \n    # Store x\n    xs[combo] = Xtemp\n    ys[combo] = ytemp\n    \n    # Store model for that store, sku combination\n    models[(indices[count][0], indices[count][1])] = model.fit(Xtemp, y_transform(ytemp))\n    ","3c266750":"preds = pd.DataFrame()\npreds['record_ID'] = rec_id_test\npreds['units_sold'] = np.abs(pred)\npreds.to_csv('SubA.csv', index = None)","5d8af9a8":"# Approach 2: Create separate models for each store, sku combination. \nThis would result in creating 1155 models. While predicting on test data, we note down the store, sku combination and fetch the corresponding model.\nThis approach performed worse than the approach mentioned above. One reason could be that this approach does not allow interaction effects between stores and skus between stores. ","1e10b809":"## Utility Functions","9cf50570":"## Use rfpimp package to Calculate Feature importance. The following importances can be calculated:\n* Permutation importance\n* Drop Column Importance        \n\nIt is recommended to use permutation importance and drop-column importance to calculate feature importances instead of using the default gini-index based measure. \n\nPlease view the following link to read more about the above methods and why they are preferable options:        \nhttps:\/\/github.com\/parrt\/random-forest-importances ","8e765b99":"# Approach 1:\n* Treat store_id and sku_id as categorical variables\n* Create an ensemble model with simple and weighted averages","c8d8ee0d":"# Approach and Contents:\nThe following notebook explains the approach I used for the Demand Forecasting problem Hosted on AnalyticsVidhya. Please check it out on https:\/\/datahack.analyticsvidhya.com\/contest\/janatahack-demand-forecasting\/True\/#About      \n\nThe problem needs us to predict the number of items sold for multiple products across multiple stores.       \n\n*As we can understand, each product, store combination can be modelled as a separate time series. However, we can also build one model, treating the store id and item id as categorical features.*            \n\nI have used both the approaches in the following notebook. For me, the latter approach(building one model treating store and item ids as categorical variables) gave the better result.          \n\n**I made submissions after the competition ended and obtained Rank #17.**","b0c22513":"# Simple Non Lagged Features"}}