{"cell_type":{"ea67d1ff":"code","01671190":"code","9c1953ad":"code","be477acf":"code","3b885ae6":"code","3179367f":"code","37817c9c":"code","1c24e34e":"code","d38f3f7f":"code","dbee3b11":"code","190a3332":"code","a15a0a55":"code","36b873f3":"code","ea809557":"code","166b3e67":"code","739711a1":"code","864ba9b2":"code","15f769bb":"code","6d84ccce":"code","0523838f":"code","63f9d76e":"code","f47e8d67":"code","a4148e1f":"code","190734c6":"code","197c798f":"code","bf60eee4":"markdown","df76e807":"markdown","453f1f47":"markdown","a33a171e":"markdown","0a57dbbb":"markdown"},"source":{"ea67d1ff":"import matplotlib.pyplot as plt\nimport numpy as np \nimport pandas as pd\nimport cv2,math,gc,sys\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\nfrom torch.nn import Parameter\n\n!pip install \"..\/input\/efficient-net\/dist\/efficientnet_pytorch-0.7.0.tar\"\nfrom efficientnet_pytorch import EfficientNet\n\n!pip install \"..\/input\/faissgpuwheel\/faiss_gpu-1.7.0-cp37-cp37m-manylinux2014_x86_64.whl\"\nimport faiss\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nimport cudf, cuml, cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors\n\nsys.path.append('..\/input\/timm-pytorch-image-models\/pytorch-image-models-master')\nimport timm\n\nimport warnings\nwarnings.simplefilter('ignore')\n\ntorch.backends.cudnn.benchmark = True\n\nimport transformers\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup,get_cosine_schedule_with_warmup\nfrom transformers import get_cosine_with_hard_restarts_schedule_with_warmup","01671190":"class cfg:\n    img_size = (380,380)\n    feavec_num1 = 512\n    feavec_num2 = 1280\n    fea_norm = 64\n    margin = 0.35\n    batch = 50\n    wpath = [\"..\/input\/shopee-weight\/w_eff6_s380_cl8812_fold1_v2.pt\",\n             \"..\/input\/shopee-weight\/w_effb3_s380_cl8811_fold2_0.80.pt\",\n             \"..\/input\/shopee-weight\/w_effb5_s380_cl8811_fold3.pt\",\n             \"..\/input\/shopee-weight\/w_effb4_s380_cl8811_fold4.pt\",\n             \"..\/input\/shopee-weight\/w_effb3_s380_cl8811_fold5_m0.35.pt\",\n             \"..\/input\/shopee-weight\/w_effb0_s380_cl11014_fullfold_fc512.pt\",\n             \"..\/input\/shopee-weight\/w_effb0_s380_cl11014_fullfold_acc0.980_fc512.pt\"]\n    mname = ['efficientnet-b6','efficientnet-b3','efficientnet-b5','efficientnet-b4','efficientnet-b3',\"efficientnet_b0\",\"efficientnet_b1\"]\n    clsize = [8812,8811,8811,8811,8811,11014,11014]","9c1953ad":"class textcfg:\n    NUM_WORKERS = 2\n    BATCH_SIZE = 8  \n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nTOKENIZER = transformers.AutoTokenizer.from_pretrained('..\/input\/bert-base-uncased')\n\nmodel_params = {\n    'n_classes':8812,\n    #'n_classes':11014,\n    'model_name':'..\/input\/bert-base-uncased',\n    'pooling':'clf',\n    'use_fc':False,\n    'fc_dim':1280,\n    'dropout':0.0,\n    #'loss_module':loss_module,\n    's':30.0,\n    'margin':0.50,\n    'ls_eps':0.0,\n    'theta_zero':0.785\n}\n\nclass ShopeeDataset(Dataset):\n    def __init__(self, csv):\n        self.csv = csv.reset_index()\n\n    def __len__(self):\n        return self.csv.shape[0]\n\n    def __getitem__(self, index):\n        row = self.csv.iloc[index]\n        text = row.title\n        text = TOKENIZER(text, padding='max_length', truncation=True, max_length=64, return_tensors=\"pt\")\n        input_ids = text['input_ids'][0]\n        attention_mask = text['attention_mask'][0]  \n        \n        return input_ids, attention_mask\n    \nclass ShopeeNet(nn.Module):\n\n    def __init__(self,\n                 n_classes,\n                 model_name='bert-base-uncased',\n                 pooling='mean_pooling',\n                 use_fc=False,\n                 fc_dim=512,\n                 dropout=0.0,\n                 loss_module='softmax',\n                 s=30.0,\n                 margin=0.50,\n                 ls_eps=0.0,\n                 theta_zero=0.785):\n\n        super(ShopeeNet, self).__init__()\n\n        self.transformer = transformers.AutoModel.from_pretrained('..\/input\/bert-base-uncased')\n        final_in_features = self.transformer.config.hidden_size\n        \n        self.pooling = pooling\n        self.use_fc = use_fc\n        self.fc = nn.Linear(final_in_features, fc_dim)\n        self._init_params()\n        self.loss_module = loss_module\n        #self.final = ArcMarginProduct(final_in_features, n_classes)\n        self.final = ArcMarginProduct(fc_dim, n_classes)\n        \n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n        #nn.init.constant_(self.bn.weight, 1)\n        #nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, input_ids,attention_mask, label=None):\n        feature = self.extract_feat(input_ids,attention_mask)\n        feature = self.fc(feature)\n        if label is not None:\n            logits = self.final(feature, label)\n        else:\n            logits = feature\n        return F.normalize(logits,dim=1)\n\n    def extract_feat(self, input_ids,attention_mask):\n        x = self.transformer(input_ids=input_ids,attention_mask=attention_mask)\n        features = x[0]\n        features = features[:,0,:]\n\n        if self.use_fc:\n            features = self.dropout(features)\n            features = self.fc(features)\n            features = self.bn(features)\n            features = self.relu(features)\n\n        return features\n    \ndef get_embedding_bert(df):\n    test_dataset = ShopeeDataset(csv=df)\n    loader = torch.utils.data.DataLoader(\n        test_dataset,\n        batch_size=textcfg.BATCH_SIZE,\n        shuffle=False,\n        pin_memory=True,\n        drop_last=False,\n        num_workers=textcfg.NUM_WORKERS)\n    model = ShopeeNet(**model_params).to(device)\n    model.load_state_dict(torch.load('..\/input\/shopee-weight\/bertmodel_acc0.813.pt'))\n    model.eval()\n    print('start collection')\n    embedded = np.empty((0,1280),dtype='float32')\n    with torch.no_grad():\n        for idx,d in enumerate(loader):\n            input_ids, attention_mask = d[0].to(device),d[1].to(device)\n            outputs = model(input_ids,attention_mask)\n            embedded = np.append(embedded, outputs.cpu().detach().numpy(),axis=0)\n\n            if idx%500==0:\n                print(idx,len(loader)) \n                print(embedded.shape)\n    print(embedded.shape)\n    return embedded\n\ndef f1_score(y_true, y_pred):\n    y_true = y_true.apply(lambda x: set(x.split()))\n    y_pred = y_pred.apply(lambda x: set(x.split()))\n    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n    len_y_pred = y_pred.apply(lambda x: len(x)).values\n    len_y_true = y_true.apply(lambda x: len(x)).values\n    f1 = 2 * intersection \/ (len_y_pred + len_y_true)\n    return f1\n\ndef predict_bert(df,embeddings,topk=50,threshold=0.63):\n    N,D = embeddings.shape\n    cpu_index = faiss.IndexFlatL2(D)\n    gpu_index = faiss.index_cpu_to_all_gpus(cpu_index)\n    gpu_index.add(embeddings)\n    cluster_distance,cluster_index = gpu_index.search(x=embeddings, k=topk)\n    \n    df['pred_bert'] = ''\n    pred = []\n    for k in range(embeddings.shape[0]):\n        idx = np.where(cluster_distance[k,] < threshold)[0]\n        ids = cluster_index[k,idx]\n        #posting_ids = ' '.join(df['posting_id'].iloc[ids].values)\n        posting_ids = df['posting_id'].iloc[ids].values\n        pred.append(posting_ids)\n    df['pred_bert'] = pred\n    if COMPUTE_CV:\n        df['pred_bertonly'] = df.pred_bert.apply(lambda x: ' '.join(x))\n        df['f1_bert'] = f1_score(df['target'], df['pred_bertonly'])\n        score = df['f1_bert'].mean()\n        print(f'Our f1 score for threshold {threshold} is {score}')\n    return df","be477acf":"COMPUTE_CV = False\n\n#make target clustering\nif COMPUTE_CV:\n    df = pd.read_csv(\"..\/input\/shopee-product-matching\/train.csv\")\n    tmp = df.groupby('label_group').posting_id.agg('unique').to_dict()\n    df['target'] = df.label_group.map(tmp)\n    df['target'] = df['target'].apply(lambda x: ' '.join(x))\n    df_cu = cudf.DataFrame(df)\nelse:\n    df = pd.read_csv(\"..\/input\/shopee-product-matching\/test.csv\")\n    df_cu = cudf.DataFrame(df)\n    if len(df)==3:\n        cfg.batch = 3\n    \nprint('df shape is', df.shape )\ndf.head()","3b885ae6":"class ArcMarginProduct(nn.Module):\n    def __init__(self, in_features, out_features, s=30.0, m=0.30, easy_margin=False):\n        super(ArcMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.s = s\n        self.m = m\n        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(m)\n        self.sin_m = math.sin(m)\n        self.th = math.cos(math.pi - m)\n        self.mm = math.sin(math.pi - m) * m\n\n    def forward(self, input, label):\n        # --------------------------- cos(theta) & phi(theta) ---------------------------\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt((1.0 - torch.pow(cosine, 2)).clamp(0, 1))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n        # --------------------------- convert label to one-hot ---------------------------\n        one_hot = torch.zeros(cosine.size(), device=device)\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)  # you can use torch.where if your torch.__version__ is 0.4\n        output *= self.s\n        return output\n\n\nclass Model(nn.Module):\n    def __init__(self,name,clustersize,feavec=512):\n        super(Model, self).__init__()\n        self.eff = EfficientNet.from_name(name)\n        self.out = nn.Linear(1000,feavec)\n        self.margin = ArcMarginProduct(in_features=feavec, \n                                       out_features = clustersize, \n                                       s=cfg.fea_norm, \n                                       m=cfg.margin)      \n\n    def forward(self, x, labels=None):\n        x = self.eff(x)\n        x = self.out(x)\n        if labels is not None:\n            return self.margin(x,labels)\n        return F.normalize(x,dim=1)\n\nclass timmModel(nn.Module):\n    def __init__(self,name,clustersize,feavec=512):\n        super(timmModel, self).__init__()\n        #self.eff = EfficientNet.from_pretrained('efficientn\uff14t-b2')\n        self.eff = timm.create_model(name,pretrained=False)\n        #self.eff = torchvision.models.resnet101(pretrained=True)\n        #self.eff._fc = nn.Linear(self.eff._fc.in_features,1280)\n        self.fc = nn.Linear(1000,feavec)\n        #self.out = nn.Linear(2048,512)\n        self.dropout = nn.Dropout(p=0.3)\n        self.margin = ArcMarginProduct(in_features=feavec, out_features = clustersize)  \n        self.bn = nn.BatchNorm1d(512)\n        self._init_params()    \n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, x, labels=None):\n        x = self.eff(x)\n        x = self.dropout(x)\n        x = self.fc(x)\n        #x = self.bn(x)\n        #x = F.softmax(x,dim=1)\n        #features = x\n        if labels is not None:\n            x = self.margin(x,labels)\n            #x = F.softmax(x,dim=1)\n            return x\n        return F.normalize(x,dim=1)","3179367f":"model1 = Model(name=cfg.mname[0],clustersize=cfg.clsize[0]).to(device).half()\nmodel1.load_state_dict(torch.load(cfg.wpath[0], map_location=device))\n\nmodel2 = Model(name=cfg.mname[1],clustersize=cfg.clsize[1]).to(device).half()\nmodel2.load_state_dict(torch.load(cfg.wpath[1], map_location=device))\n\nmodel3 = Model(name=cfg.mname[2],clustersize=cfg.clsize[2]).to(device).half()\nmodel3.load_state_dict(torch.load(cfg.wpath[2], map_location=device))\n\nmodel4 = Model(name=cfg.mname[3],clustersize=cfg.clsize[3]).to(device).half()\nmodel4.load_state_dict(torch.load(cfg.wpath[3], map_location=device))\n\nmodel5 = Model(name=cfg.mname[4],clustersize=cfg.clsize[4]).to(device).half()\nmodel5.load_state_dict(torch.load(cfg.wpath[4], map_location=device))\n\nmodel6 = timmModel(name=cfg.mname[5],clustersize=cfg.clsize[5]).to(device).half()\nmodel6.load_state_dict(torch.load(cfg.wpath[5], map_location=device))\n\nmodel7 = timmModel(name=cfg.mname[6],clustersize=cfg.clsize[6]).to(device).half()\nmodel7.load_state_dict(torch.load(cfg.wpath[6], map_location=device))","37817c9c":"# make image Datasets\ndef load_image(file_name):\n    if COMPUTE_CV:\n        file_path = f'\/kaggle\/input\/shopee-product-matching\/train_images\/{file_name}'\n    else:\n        file_path = f'\/kaggle\/input\/shopee-product-matching\/test_images\/{file_name}'\n\n    img = cv2.imread(file_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, cfg.img_size)\n    tensor_img = torch.tensor(img)\n    tensor_img = tensor_img.permute(( 2, 0, 1)).float()\/255.0\n    return tensor_img\n\nclass valDataset(Dataset):\n    def __init__(self, df):\n        self.img = df.image.values\n        \n    def __len__(self):\n        return len(self.img)\n\n    def __getitem__(self, idx):\n        img = self.img[idx]\n        img = load_image(img)\n        return img","1c24e34e":"def image_embeddings(df):\n    dataset = valDataset(df)\n    loader = DataLoader(dataset,\n                        batch_size=cfg.batch,\n                        shuffle=False,\n                        num_workers=2,\n                        pin_memory=True,\n                        drop_last=False)\n    \n    \n    print('start collection')\n    feavec = 512\n    embedded = np.empty((0,feavec),dtype='float32')\n    with torch.no_grad():\n        for idx,images in enumerate(loader):\n            images = images.to(device,non_blocking=True).half()\n            outputs1 = model1(images)\n            outputs2 = model2(images)\n            outputs3 = model3(images)\n            outputs4 = model4(images)\n            outputs5 = model5(images)\n            outputs6 = model6(images)\n            outputs7 = model7(images)\n            outputs = (outputs1 + outputs2 + outputs3 + outputs4 + outputs5 + outputs6 + outputs7)\/7\n            embedded = np.append(embedded, outputs.cpu().detach().numpy(),axis=0)\n\n            if idx%100==0:\n                print(idx,len(loader)) \n                print(embedded.shape)\n    #del model1,model2,model3,model4,model5.model6\n    return embedded","d38f3f7f":"def f1_score(y_true, y_pred):\n    y_true = y_true.apply(lambda x: set(x.split()))\n    y_pred = y_pred.apply(lambda x: set(x.split()))\n    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n    len_y_pred = y_pred.apply(lambda x: len(x)).values\n    len_y_true = y_true.apply(lambda x: len(x)).values\n    f1 = 2 * intersection \/ (len_y_pred + len_y_true)\n    return f1\n\ndef predict_img(df,embeddings,topk=50,threshold=0.63):\n    N,D = embeddings.shape\n    cpu_index = faiss.IndexFlatL2(D)\n    gpu_index = faiss.index_cpu_to_all_gpus(cpu_index)\n    gpu_index.add(embeddings)\n    cluster_distance,cluster_index = gpu_index.search(x=embeddings, k=topk)\n    \n    df['pred_images'] = ''\n    pred = []\n    for k in range(embeddings.shape[0]):\n        idx = np.where(cluster_distance[k,] < threshold)[0]\n        ids = cluster_index[k,idx]\n        #posting_ids = ' '.join(df['posting_id'].iloc[ids].values)\n        posting_ids = df['posting_id'].iloc[ids].values\n        pred.append(posting_ids)\n    df['pred_images'] = pred\n    if COMPUTE_CV:\n        df['pred_imgonly'] = df.pred_images.apply(lambda x: ' '.join(x))\n        df['f1_img'] = f1_score(df['target'], df['pred_imgonly'])\n        score = df['f1_img'].mean()\n        print(f'Our f1 score for threshold {threshold} is {score}')\n    return df\n\ndef predict_text(df,embeddings,topk=50,threshold=0.63):\n    N,D = embeddings.shape\n    cpu_index = faiss.IndexFlatL2(D)\n    gpu_index = faiss.index_cpu_to_all_gpus(cpu_index)\n    gpu_index.add(embeddings)\n    cluster_distance,cluster_index = gpu_index.search(x=embeddings, k=topk)\n    \n    df['pred_text'] = ''\n    pred = []\n    for k in range(embeddings.shape[0]):\n        idx = np.where(cluster_distance[k,] < threshold)[0]\n        ids = cluster_index[k,idx]\n        #posting_ids = ' '.join(df['posting_id'].iloc[ids].values)\n        posting_ids = df['posting_id'].iloc[ids].values\n        pred.append(posting_ids)\n    df['pred_text'] = pred\n    if COMPUTE_CV:\n        df['pred_textonly'] = df.pred_images.apply(lambda x: ' '.join(x))\n        df['f1_text'] = f1_score(df['target'], df['pred_textonly'])\n        score = df['f1_text'].mean()\n        print(f'Our f1 score for threshold {threshold} is {score}')\n    return df","dbee3b11":"def get_text_predictions(df, max_features = 25000,threshold=0.7):\n    from cuml.feature_extraction.text import TfidfVectorizer\n    model = TfidfVectorizer(stop_words = 'english', binary = True, max_features = max_features)\n    text_embeddings = model.fit_transform(df_cu.title).toarray()\n    #print(text_embeddings)\n    preds = []\n    CHUNK = 1024*4\n\n    print('Finding similar titles...')\n    CTS = len(df)\/\/CHUNK\n    if len(df)%CHUNK!=0: CTS += 1\n    for j in range( CTS ):\n\n        a = j*CHUNK\n        b = (j+1)*CHUNK\n        b = min(b,len(df))\n        print('chunk',a,'to',b)\n\n        # COSINE SIMILARITY DISTANCE\n        cts = cupy.matmul( text_embeddings, text_embeddings[a:b].T).T\n\n        for k in range(b-a):\n            IDX = cupy.where(cts[k,]>threshold)[0]\n            o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n            preds.append(o)\n    df['pred_text'] = preds\n    del model,text_embeddings\n    gc.collect()\n    if COMPUTE_CV:\n        df['pred_textonly'] = df.pred_text.apply(lambda x: ' '.join(x))\n        df['f1_text'] = f1_score(df['target'], df['pred_textonly'])\n        score = df['f1_text'].mean()\n        print(f'Our f1 score for threshold {threshold} is {score}')\n    return df","190a3332":"class textvalDataset(Dataset):\n    def __init__(self, textlist):\n        self.text = textlist\n        \n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, idx):\n        text = torch.tensor(self.text[idx])\n        text = text.float()\n        return text\n\nclass Model(nn.Module):\n    def __init__(self,clustersize,feavec=512):\n        super(Model, self).__init__()\n        self.linear1 = nn.Linear(24939,4000)\n        self.linear2 = nn.Linear(4000,feavec)\n        self.dropout = nn.Dropout(p=0.5)\n        self.relu = nn.ReLU()\n        self.margin = ArcMarginProduct(in_features=feavec, \n                                       out_features = clustersize, \n                                       s=64, \n                                       m=0.7)      \n\n    def forward(self, x, labels=None):\n        x = self.linear1(x)\n        #x = self.relu(x)\n        x = self.linear2(x)\n        #x = self.relu(x)\n        x = self.dropout(x)\n        if labels is not None:\n            return self.margin(x,labels)\n        return F.normalize(x,dim=1)\n    \n\ndef get_deeptext_predictions(df):\n    from sklearn.feature_extraction.text import TfidfVectorizer\n    df_t = pd.read_csv(\"..\/input\/shopee-product-matching\/train.csv\")\n    models = TfidfVectorizer(stop_words = 'english', binary = True, max_features = 24939)\n    models.fit(pd.concat([df,df_t],axis=0).title)\n    text = models.transform(df.title).toarray()\n    batch = 50\n    if len(df)==3:\n        batch=3\n    test_dataset = textvalDataset(text)\n    test_loader = DataLoader(test_dataset,\n                            batch_size=batch,\n                            shuffle=False,\n                            num_workers=2,\n                            pin_memory=True)\n    model_t1 = Model(8811)\n    model_t2 = Model(8811)\n    model_t1 = model_t1.to(device)\n    model_t2 = model_t2.to(device)\n    model_t1.load_state_dict(torch.load('..\/input\/shopee-weight\/w_lin_e5_fold1.pt'))\n    model_t2.load_state_dict(torch.load('..\/input\/shopee-weight\/w_lin_e5_fold2.pt'))\n    #model.load_state_dict(torch.load('..\/input\/shopee-weight-text\/w_lin_e5_fold0.pt'))\n    model_t1.eval()\n    model_t2.eval()\n    print('start collection')\n    embedded1 = np.empty((0,512),dtype='float32')\n    embedded2 = np.empty((0,512),dtype='float32')\n    with torch.no_grad():\n        for idx,(images) in enumerate(test_loader):\n            images = images.to(device,non_blocking=True)\n            outputs = model_t1(images)\n            embedded1 = np.append(embedded1, outputs.cpu().detach().numpy(),axis=0)\n            outputs = model_t2(images)\n            embedded2 = np.append(embedded2, outputs.cpu().detach().numpy(),axis=0)\n\n            if idx%100==0:\n                print(idx,len(test_loader)) \n                print(embedded1.shape)\n                print(embedded2.shape)\n    print(embedded1.shape,embedded2.shape)\n    return embedded1,embedded2","a15a0a55":"#bert_embeddings = get_embedding_bert(df)","36b873f3":"#text_embeddings = get_deeptext_predictions(df)\nmodel1.eval()\nmodel2.eval()\nmodel3.eval()\nmodel4.eval()\nmodel5.eval()\nmodel6.eval()\nmodel7.eval()\nimage_embeddings = image_embeddings(df)","ea809557":"del model1,model2,model3,model4,model5,model6,model7","166b3e67":"#text_embeddings1,text_embeddings2 = get_deeptext_predictions(df)","739711a1":"#text_embeddings = (text_embeddings1+text_embeddings2)\/2\n#img_text_embeddings = (image_embeddings + 0.4*text_embeddings)\/1.4","864ba9b2":"if COMPUTE_CV:\n    df = predict_img(df,image_embeddings,topk=50,threshold=0.137)\n    #df = predict_img(df,img_text_embeddings,topk=50,threshold=0.106)","15f769bb":"#2,3,4\ndf = predict_img(df,image_embeddings,topk=50,threshold=0.11)\n#df = predict_bert(df,bert_embeddings,topk=50,threshold=0.54)\n#3,4\n#df = predict_img(df,image_embeddings,topk=50,threshold=0.30)","6d84ccce":"theresholds=np.linspace(0.12,0.14,10)\nif COMPUTE_CV:\n    #for topk in [49,50,51,60]:\n    for threshold in theresholds:\n        df = predict_img(df,image_embeddings,topk=50,threshold=threshold)","0523838f":"df = get_text_predictions(df, max_features = 25000,threshold=0.75)\ndf.head()","63f9d76e":"tmp = df.groupby('image_phash').posting_id.agg('unique').to_dict()\ndf['oof_hash'] = df.image_phash.map(tmp)","f47e8d67":"def combine_predictions(row):\n    x = np.concatenate([row['pred_images'], row['pred_text']])\n    return ' '.join( np.unique(x) )\n\ndef combine_predictions_addphash(row):\n    x = np.concatenate([row['pred_images'], row['pred_text'], row['oof_hash'],row['pred_bert']])\n    return ' '.join( np.unique(x))\n\ndef combine_predictions_bert(row):\n    x = np.concatenate([row['pred_images'], row['pred_bert']])\n    return ' '.join( np.unique(x))","a4148e1f":"df.head()","190734c6":"df['matches'] = df.apply(combine_predictions, axis=1)\n#df['matches'] = df['pred_images'].apply(lambda x: ' '.join(x))\nif COMPUTE_CV:\n    df['f1'] = f1_score(df['target'], df['matches'])\n    score = df['f1'].mean()\n    print(f'Final f1 score is {score}')\nelse:\n    with open('submission.csv', 'w') as outf:\n        print('posting_id,matches', file=outf)\n        for i,(idnum,match) in enumerate(zip(df['posting_id'],df['matches'])):\n            print(f'{idnum},{match}', file=outf)","197c798f":"df_t = pd.read_csv(\"submission.csv\")\nprint(df_t)","bf60eee4":"# Carry out image prediction","df76e807":"# Carry out text predictions","453f1f47":"# Use Image Embeddings","a33a171e":"# combine_predictions","0a57dbbb":"# Use Text Embeddings"}}