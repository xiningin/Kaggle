{"cell_type":{"cd80cdb7":"code","65ef502d":"code","4189740c":"code","ece407e6":"code","41594572":"code","e8e09ed4":"code","1f69b1e8":"code","95454ff9":"code","452640e9":"code","174bf257":"code","9c0132b5":"code","53020576":"code","e3bd11f8":"code","d0b97bbe":"code","d3e2a516":"code","0941a7fe":"code","3f0a73f0":"code","3a45d9d8":"code","1b3062b5":"code","f0fb37f6":"code","ebb07aed":"code","e92300a4":"code","a303d4cc":"code","0d8900bc":"code","45e3109f":"code","cb1b99aa":"code","80f4cfae":"code","0041e2b4":"code","2164e89c":"markdown"},"source":{"cd80cdb7":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n\nfrom sklearn.metrics import classification_report, log_loss, accuracy_score\nfrom sklearn.model_selection import train_test_split","65ef502d":"directory = '..\/input\/hand-gesture-recognition\/HandGesture\/images'","4189740c":"Name=[]\nfor file in os.listdir(directory):\n    if file[-4:]!='pt.m' and file[-4:]!='.txt':\n        Name+=[file]\nprint(Name)\nprint(len(Name))","ece407e6":"N=[]\nfor i in range(len(Name)):\n    N+=[i]\n    \nnormal_mapping=dict(zip(Name,N)) \nreverse_mapping=dict(zip(N,Name)) \n\ndef mapper(value):\n    return reverse_mapping[value]","41594572":"File=[]\nfor file in os.listdir(directory):\n    File+=[file]\n    print(file)","e8e09ed4":"dataset=[]\ntestset=[]\ncount=0\nfor file in File:\n    path=os.path.join(directory,file)\n    t=0\n    for im in os.listdir(path):\n        if im[-4:]!='pt.m' and im[-4:]!='.txt':\n            image=load_img(os.path.join(path,im), grayscale=False, color_mode='rgb', target_size=(60,60))\n            image=img_to_array(image)\n            image=image\/255.0\n            if t<400:\n                dataset.append([image,count])\n            else:   \n                testset.append([image,count])\n            t+=1\n    count=count+1","1f69b1e8":"data,labels0=zip(*dataset)\ntest,tlabels0=zip(*testset)","95454ff9":"labels1=to_categorical(labels0)\ndata=np.array(data)\nlabels=np.array(labels1)","452640e9":"tlabels1=to_categorical(tlabels0)\ntest=np.array(test)\ntlabels=np.array(tlabels1)","174bf257":"trainx,testx,trainy,testy=train_test_split(data,labels,test_size=0.2,random_state=44)","9c0132b5":"print(trainx.shape)\nprint(testx.shape)\nprint(trainy.shape)\nprint(testy.shape)","53020576":"datagen = ImageDataGenerator(horizontal_flip=True,vertical_flip=True,rotation_range=20,zoom_range=0.2,\n                        width_shift_range=0.2,height_shift_range=0.2,shear_range=0.1,fill_mode=\"nearest\")","e3bd11f8":"pretrained_model3 = tf.keras.applications.DenseNet201(input_shape=(60,60,3),include_top=False,weights='imagenet',pooling='avg')\npretrained_model3.trainable = False","d0b97bbe":"inputs3 = pretrained_model3.input\nx3 = tf.keras.layers.Dense(128, activation='relu')(pretrained_model3.output)\noutputs3 = tf.keras.layers.Dense(10, activation='softmax')(x3)\nmodel = tf.keras.Model(inputs=inputs3, outputs=outputs3)","d3e2a516":"model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])","0941a7fe":"his=model.fit(datagen.flow(trainx,trainy,batch_size=32),validation_data=(testx,testy),epochs=20)","3f0a73f0":"y_pred=model.predict(testx)\npred=np.argmax(y_pred,axis=1)\nground = np.argmax(testy,axis=1)\nprint(classification_report(ground,pred))","3a45d9d8":"get_acc = his.history['accuracy']\nvalue_acc = his.history['val_accuracy']\nget_loss = his.history['loss']\nvalidation_loss = his.history['val_loss']\n\nepochs = range(len(get_acc))\nplt.plot(epochs, get_acc, 'r', label='Accuracy of Training data')\nplt.plot(epochs, value_acc, 'b', label='Accuracy of Validation data')\nplt.title('Training vs validation accuracy')\nplt.legend(loc=0)\nplt.figure()\nplt.show()","1b3062b5":"epochs = range(len(get_loss))\nplt.plot(epochs, get_loss, 'r', label='Loss of Training data')\nplt.plot(epochs, validation_loss, 'b', label='Loss of Validation data')\nplt.title('Training vs validation loss')\nplt.legend(loc=0)\nplt.figure()\nplt.show()","f0fb37f6":"load_img(\"..\/input\/hand-gesture-recognition\/HandGesture\/images\/rock_on\/1317.jpg\",target_size=(60,60))","ebb07aed":"image=load_img(\"..\/input\/hand-gesture-recognition\/HandGesture\/images\/rock_on\/1317.jpg\",target_size=(60,60))\n\nimage=img_to_array(image) \nimage=image\/255.0\nprediction_image=np.array(image)\nprediction_image= np.expand_dims(image, axis=0)","e92300a4":"prediction=model.predict(prediction_image)\nvalue=np.argmax(prediction)\nmove_name=mapper(value)\nprint(\"Prediction is {}.\".format(move_name))","a303d4cc":"print(test.shape)\nprediction2=model.predict(test)\nprint(prediction2.shape)\n\nPRED=[]\nfor item in prediction2:\n    value2=np.argmax(item)      \n    PRED+=[value2]","0d8900bc":"ANS=tlabels0","45e3109f":"accuracy=accuracy_score(ANS,PRED)\nprint(accuracy)","cb1b99aa":"model.save('handgest.hdf5')\nmodel2 = keras.models.load_model('handgest.hdf5')","80f4cfae":"prediction3=model2.predict(test)\n\nPRED3=[]\nfor item in prediction3:\n    value3=np.argmax(item)      \n    PRED3+=[value3]","0041e2b4":"accuracy3=accuracy_score(ANS,PRED3)\nprint(accuracy3)","2164e89c":"### Model save and load"}}