{"cell_type":{"ef671ea6":"code","0570c536":"code","6f8cdd8b":"code","85edb89c":"code","074cc2b9":"code","862da0ae":"code","c79ef539":"code","336d0d27":"code","198462c2":"code","028f0b3a":"code","f92db733":"code","85df6b2c":"code","e4c7ad7d":"code","17aa8f59":"code","8332305f":"code","ad69607f":"code","a47a4710":"code","74cf3921":"code","244ae830":"code","a087ee00":"code","e6d8fefd":"code","d4866ed0":"code","3b27f976":"code","834401ad":"code","a09072b6":"code","432e030c":"code","52769b81":"code","8b6af2c2":"markdown"},"source":{"ef671ea6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom shutil import copytree\nimport os\nimport matplotlib.pyplot as plt\nimport cv2\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0570c536":"# to copy the data into folders like we need for the image gen\nsrc=['\/kaggle\/input\/covid19xray\/COVID','\/kaggle\/input\/normalxray\/NORMALc' , '\/kaggle\/input\/pneumoniag\/PNEUMONIA']\ndst=['\/kaggle\/output\/working\/covid', '\/kaggle\/output\/working\/normal' , '\/kaggle\/output\/working\/pneumonia']\n\n[ copytree(s, d) for s,d in zip(src , dst)] # copies the directory of the data to other directory","6f8cdd8b":"os.listdir('\/kaggle\/output\/working\/')","85edb89c":"data_dir='\/kaggle\/output\/working\/'","074cc2b9":"os.listdir('\/kaggle\/output\/working\/covid')","862da0ae":"classes=['Covid','Normal','Pneumonia']\ndata_gen=ImageDataGenerator(\n                                samplewise_center=True,\n                                samplewise_std_normalization=True,\n                                rotation_range=0.2,\n                                width_shift_range=0.1,\n                                height_shift_range=0.1,\n                                shear_range=0.1,\n                                zoom_range=0.15,\n                                horizontal_flip=True,\n                                rescale=1\/255.,\n                                validation_split=0.1)\n\n\n\ntrain_data=data_gen.flow_from_directory(data_dir,\n                                        target_size=(224, 224),\n                                        class_mode='categorical',\n                                        batch_size=32,\n                                        subset=\"training\",\n                                        shuffle=True)\n\n\nval_data=data_gen.flow_from_directory(data_dir,\n                                        target_size=(224, 224),\n                                        class_mode='categorical',\n                                        batch_size=32,\n                                        subset=\"validation\",\n                                        shuffle=True)","c79ef539":"# len( next(train_data) )\n# len(train_data)\nx= train_data.next()\nx[0].shape\n","336d0d27":"from tensorflow.keras.models import Model\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.applications import vgg16\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nimport seaborn as sns","198462c2":"def dump_build_model():\n  # load the base VGG16 model\n  base_model = vgg16.VGG16(input_shape=(224, 224) + (3,), \n                      weights='imagenet', \n                      include_top=False)\n  \n  # add a GAP layer\n  output = layers.GlobalAveragePooling2D()(base_model.output)\n\n\n  output = layers.Dense(3, activation='softmax')(output)\n\n  # set the inputs and outputs of the model\n  model = Model( base_model.input , output )\n\n  # freeze the earlier layers and leave the last 4 layers to train\n  for layer in base_model.layers[:-4]:\n      layer.trainable=False\n\n  # choose the optimizer\n  optimizer = tf.keras.optimizers.RMSprop(0.001)\n\n  # configure the model for training\n\n  model.compile(loss='categorical_crossentropy', \n                optimizer=optimizer, \n                metrics=['accuracy'])\n  \n  # display the summary\n  model.summary()\n  \n  return model","028f0b3a":"model=dump_build_model()","f92db733":"model.fit(train_data , validation_data=val_data , epochs=20)","85df6b2c":"# select all the layers for which you want to visualize the outputs and store it in a list\noutputs = [ layer.output for layer in model.layers[1:18] ] # all layers except the input layer\n\n# Define a new model that generates the above output\nvis_model = Model(model.input , outputs)\n\n# store the layer names we are interested in\nlayer_names = []\nfor layer in outputs:\n    layer_names.append( layer.name.split(\"\/\")[0] )\n\n    \nprint(\"Layers that will be used for visualization: \")\nprint(layer_names)","e4c7ad7d":"def get_CAM(processed_image, actual_label, layer_name='block5_conv3'):\n    model_grad = Model( [model.inputs] ,   [model.get_layer(layer_name).output , model.output]  )\n    \n    with tf.GradientTape() as tape:\n        conv_output_values, predictions = model_grad(processed_image)\n\n        # watch the conv_output_values\n        tape.watch(conv_output_values)\n\n        ## Use binary cross entropy loss\n        ## actual_label is 0 if cat, 1 if dog\n        # get prediction probability of dog\n        # If model does well, \n        # pred_prob should be close to 0 if cat, close to 1 if dog\n        pred_prob = predictions[:,1] # [ batch , (cat_prob , dog_prob) ]\n        # we tale only one prbability to be able to use binary_crossentropy_loss not sparse_categorical_loss\n        \n        # make sure actual_label is a float, like the rest of the loss calculation\n        actual_label = tf.cast( actual_label , dtype=tf.float32 )\n        \n        # add a tiny value to avoid log of 0\n        smoothing = 0.00001 \n        \n        # Calculate loss as binary cross entropy\n        # we can use tf.keras in that too\n        # bce = tf.keras.losses.BinaryCrossentropy()\n        # bce(y_true, y_pred).numpy()\n\n\n        loss = -1 * ( actual_label * tf.math.log(pred_prob + smoothing) + (1 - actual_label) * tf.math.log(1 - pred_prob + smoothing) )\n        print(f\"binary loss: {loss}\")\n    \n    # get the gradient of the loss with respect to the outputs of the last conv layer\n    grads_values = tape.gradient(loss , conv_output_values)\n    grads_values = tf.keras.backend.mean(grads_values , axis=(0,1,2)) # mean over batch , hight , width --> num of channels\n    \n    conv_output_values = np.squeeze( conv_output_values.numpy() ) # will remove the 1 valued dimention which is the batch  --> (h , w )\n    grads_values = grads_values.numpy()\n    \n    # weight the convolution outputs with the computed gradients\n    for i in range(512): # num of filter channels\n        conv_output_values[ : , : , i ] *= grads_values[i] # multiply the gradient of the channels by the channels values\n    heatmap = np.mean(conv_output_values, axis=-1)# taking the mean over the channels , --> ( h , w )\n    \n    heatmap = np.maximum(heatmap, 0) # taking only the positive values\n    heatmap \/= heatmap.max()# regularizing the pixel values\n    \n    del model_grad, conv_output_values, grads_values, loss\n   \n    return heatmap","17aa8f59":"next(train_data)[0].shape","8332305f":"idx_class={idx : label for idx,label in enumerate(classes) }\nidx_class","ad69607f":"def show_sample():\n    \n\n    images, labels= next(val_data)\n    sample_image = images[0]  # batch 0 so that returns ( h , w , c) for the image, without the batch dimention\n    sample_label = labels[0] # takes batch of xs and ys # x= train_data.next() -> x[0].shape -> 32,224,224,3\n    \n    sample_image_processed = np.expand_dims(sample_image, axis=0) # adding back the batch dimention\n    \n    activations = vis_model.predict(sample_image_processed) # the output of each layer -features-\n    \n    pred_label = np.argmax( model.predict(sample_image_processed) , axis=-1 )[0]\n    pred_label = idx_class[pred_label]\n    \n    \n    sample_activation = activations[0] [0 , : , : , 16] # taking the first output , for image of batch 0, and for the last layer #16 , --> (h,w)\n    \n    sample_activation-=sample_activation.mean()\n    sample_activation\/=sample_activation.std()\n    \n    sample_activation *=255\n    sample_activation = np.clip( sample_activation , 0 , 255 ).astype(np.uint8)\n    \n    heatmap = get_CAM(sample_image_processed , sample_label )\n    heatmap = cv2.resize( heatmap, ( sample_image.shape[0], sample_image.shape[1 ]) )\n    heatmap = heatmap *255\n    heatmap = np.clip( heatmap , 0 , 255 ).astype(np.uint8)\n    heatmap = cv2.applyColorMap( heatmap , cv2.COLORMAP_HOT )\n    converted_img = sample_image\n    super_imposed_image = cv2.addWeighted( converted_img, 0.8, heatmap.astype('float32'), 2e-3, 0.0 )\n    \n    sample_label = idx_class[np.argmax(sample_label)]\n    \n    f,ax = plt.subplots(2,2, figsize=(15,8))\n\n    ax[0,0].imshow(sample_image)\n    ax[0,0].set_title(f\"True label: {sample_label} \\n Predicted label: {pred_label}\")\n    ax[0,0].axis('off')\n    \n    ax[0,1].imshow(sample_activation)\n    ax[0,1].set_title(\"Random feature map\")\n    ax[0,1].axis('off')\n    \n    ax[1,0].imshow(heatmap)\n    ax[1,0].set_title(\"Class Activation Map\")\n    ax[1,0].axis('off')\n    \n    ax[1,1].imshow(super_imposed_image)\n    ax[1,1].set_title(\"Activation map superimposed\")\n    ax[1,1].axis('off')\n    plt.tight_layout()\n    plt.show()\n  \n    return activations","a47a4710":"# Choose an image index to show, or leave it as None to get a random image\nactivations = show_sample()","74cf3921":"def decent_build_model():\n  # load the base VGG16 model\n  base_model = vgg16.VGG16(input_shape=(224, 224) + (3,), \n                      weights='imagenet', \n                      include_top=False)\n  \n  # add a GAP layer\n  output = layers.GlobalAveragePooling2D()(base_model.output)\n  output = layers.Dropout(0.2)(output) \n  output = layers.Dense(3, activation='softmax')(output)\n\n  # set the inputs and outputs of the model\n  model = Model( base_model.input , output )\n\n  # freeze the earlier layers and leave the last 4 layers to train\n  for layer in base_model.layers[:-2]:\n      layer.trainable=False\n\n  # choose the optimizer\n  optimizer = tf.keras.optimizers.RMSprop(0.001)\n\n  # configure the model for training\n  \n    \n  model.compile(loss='categorical_crossentropy', \n                optimizer=optimizer, \n                metrics=['accuracy','AUC'],\n                )\n  \n  # display the summary\n  model.summary()\n  \n  return model","244ae830":"model=decent_build_model()","a087ee00":"# best model\ncb = tf.keras.callbacks.ModelCheckpoint(\"my_model.h5\", save_best_only=True , )\nhistory= model.fit(train_data , validation_data=val_data, epochs=10 ,callbacks=[cb] )","e6d8fefd":"# select all the layers for which you want to visualize the outputs and store it in a list\noutputs = [ layer.output for layer in model.layers[1:18] ] # all layers except the input layer\n\n# Define a new model that generates the above output\nvis_model = Model(model.input , outputs)\n\n# store the layer names we are interested in\nlayer_names = []\nfor layer in outputs:\n    layer_names.append( layer.name.split(\"\/\")[0] )\n\n    \nprint(\"Layers that will be used for visualization: \")\nprint(layer_names)","d4866ed0":"def visualize_intermediate_activations(layer_names, activations):\n    assert len(layer_names)==len(activations), \"Make sure layers and activation values match\"\n    images_per_row=16\n    \n    for layer_name, layer_activation in zip(layer_names, activations):\n        nb_features = layer_activation.shape[-1]\n        size= layer_activation.shape[1]\n\n        nb_cols = nb_features \/\/ images_per_row\n        grid = np.zeros((size*nb_cols, size*images_per_row))\n\n        for col in range(nb_cols):\n            for row in range(images_per_row):\n                feature_map = layer_activation[ 0 , : , : , col*images_per_row + row]\n                feature_map -= feature_map.mean() \n                feature_map \/= feature_map.std()\n                feature_map *=255\n                feature_map = np.clip(feature_map, 0, 255).astype(np.uint8)\n\n                grid[col*size:(col+1)*size, row*size:(row+1)*size] = feature_map\n\n        scale = 1.\/size\n        plt.figure(figsize=(scale*grid.shape[1], scale*grid.shape[0]))\n        plt.title(layer_name)\n        plt.grid(False)\n        plt.axis('off')\n        plt.imshow(grid, aspect='auto', cmap='viridis')\n    plt.show()\n    ","3b27f976":"visualize_intermediate_activations(activations=activations, \n                                   layer_names=layer_names)    ","834401ad":"# Choose an image index to show, or leave it as None to get a random image\nactivations = show_sample()","a09072b6":"from tensorflow.keras.preprocessing import image\n#2- setting the path of the image\npath='..\/input\/nawwar\/1.jpeg'\n#3- uploading the image into a variable\n\nimg= image.load_img( path , target_size=( 224,224 ) )\n# don't forget the target size the model is expecting\n#4- processing the image variable to suit the model\n\nx= image.img_to_array( img )\nx= np.expand_dims( x , axis=0 )\nimages= np.vstack( [x] )\n \nplt.imshow(img) # to show the image","432e030c":"# to predict the image\nidx_class[np.argmax(model.predict(x))]","52769b81":"print( history.history['val_loss'] )\nhistory.history.keys()","8b6af2c2":"## As we can see the model performs poorly because it's training on much less data, with more than 7mil parameter, so in the next model we will try reducing the num of trained params "}}