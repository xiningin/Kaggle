{"cell_type":{"9b0e0054":"code","7b05f21b":"code","0880736b":"code","0ce22470":"code","f7525091":"code","97228338":"code","a9b60974":"code","3c5b6769":"code","95b1125b":"code","26a438a1":"code","6b8fd275":"code","d99c9d4f":"code","e961431c":"code","bf928ed1":"code","c43bca5a":"code","704e1ee5":"code","db431e4d":"code","bdde8407":"code","231135ff":"code","f195c3ff":"code","f7e888e3":"code","050d7a85":"code","53d9f005":"code","2c1f7fa3":"code","ea6bd02c":"code","544127d6":"code","768d5cc8":"code","37a8fe82":"code","67cd267d":"code","d61858e1":"code","70c82e91":"code","1eca4556":"code","6967a6a6":"code","cf96f44c":"code","40036e26":"code","8e452334":"code","54fe9ccb":"code","ace7dfc6":"code","d6025a74":"code","fbe24d26":"markdown","c79bb5e4":"markdown","8ba0e000":"markdown","3bcd2a1b":"markdown","c8169464":"markdown","6d84c3e2":"markdown","9de8e046":"markdown","2e7a472b":"markdown","843d13b0":"markdown","a3a1f0cd":"markdown","0768f858":"markdown","5ae78894":"markdown","e070812c":"markdown","8feb228b":"markdown","95e01156":"markdown","49805629":"markdown","1d8faa38":"markdown","d101c9be":"markdown","e05011a7":"markdown","bcf5f47d":"markdown","a719568e":"markdown","62333428":"markdown","596c7f5c":"markdown","c21402e9":"markdown","24f1db7b":"markdown","13dede70":"markdown","7ccc5082":"markdown","78ecd60b":"markdown","c34811e2":"markdown","f943d144":"markdown"},"source":{"9b0e0054":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.metrics import plot_confusion_matrix\nfrom scipy.stats import norm, boxcox\nfrom collections import Counter\nfrom scipy import stats\nfrom pandas_profiling import ProfileReport\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","7b05f21b":"dataset = pd.read_csv('..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')","0880736b":"dataset.head()","0ce22470":"dataset.shape","f7525091":"dataset.describe()","97228338":"dataset.info()","a9b60974":"dataset.isnull().values.any()","3c5b6769":"sns.set_style('whitegrid')\nplt.figure(figsize=(12, 6))\nsns.countplot(x=\"quality\", data=dataset, palette='husl');","95b1125b":"plt.figure(figsize=(20, 17))\nmatrix = np.triu(dataset.corr())\nsns.heatmap(dataset.corr(), annot=True,\n            linewidth=.8, mask=matrix, cmap=\"rocket\");","26a438a1":"sns.catplot(x=\"quality\", y=\"fixed acidity\", data=dataset, kind=\"box\")\nsns.catplot(x=\"quality\", y=\"volatile acidity\", data=dataset, kind=\"box\")\nsns.catplot(x=\"quality\", y=\"citric acid\", data=dataset, kind=\"box\")\nsns.catplot(x=\"quality\", y=\"residual sugar\", data=dataset, kind=\"box\")\nsns.catplot(x=\"quality\", y=\"chlorides\", data=dataset, kind=\"box\")\nsns.catplot(x=\"quality\", y=\"density\", data=dataset, kind=\"box\")\nsns.catplot(x=\"quality\", y=\"pH\", data=dataset, kind=\"box\")\nsns.catplot(x=\"quality\", y=\"sulphates\", data=dataset, kind=\"box\")\nsns.catplot(x=\"quality\", y=\"alcohol\", data=dataset, kind=\"box\");","6b8fd275":"acidity_count = dataset[\"fixed acidity\"].value_counts().reset_index()\nacidity_count","d99c9d4f":"plt.figure(figsize=(30, 10))\nplt.style.use(\"ggplot\")\nsns.barplot(x=acidity_count[\"index\"], y=acidity_count[\"fixed acidity\"])\nplt.title(\"TYPE OF ACIDITY WITH QUALITY\", fontsize=20)\nplt.xlabel(\"ACIDITY\", fontsize=20)\nplt.ylabel(\"COUNT\", fontsize=20)\nplt.show()","e961431c":"plt.style.use(\"ggplot\")\nsns.displot(dataset[\"pH\"]);  # using displot here\nplt.title(\"DISTRIBUTION OF pH FOR DIFFERENT QUALITIES\", fontsize=18)\nplt.xlabel(\"pH\", fontsize=20)\nplt.ylabel(\"COUNT\", fontsize=20)\nplt.show()","bf928ed1":"def skewnessCorrector(columnName):\n    print('''Before Correcting''')\n    (mu, sigma) = norm.fit(dataset[columnName])\n    print(\"Mu before correcting {} : {}, Sigma before correcting {} : {}\".format(\n        columnName.upper(), mu, columnName.upper(), sigma))\n    plt.figure(figsize=(20,10))\n    plt.subplot(1,2,1)\n    sns.distplot(dataset[columnName], fit=norm, color=\"orange\")\n    plt.title(columnName.upper() +\n              \" Distplot before Skewness Correction\", color=\"black\")\n    plt.subplot(1,2,2)\n    stats.probplot(dataset[columnName], plot=plt)\n    plt.show();\n    dataset[columnName], lam_fixed_acidity = boxcox(\n        dataset[columnName])\n    print('''After Correcting''')\n    print(\"Mu after correcting {} : {}, Sigma after correcting {} : {}\".format(\n        columnName.upper(), mu, columnName.upper(), sigma))\n    plt.figure(figsize=(20, 10))\n    plt.subplot(1,2,1)\n    sns.distplot(dataset[columnName], fit=norm, color=\"orange\")\n    plt.title(columnName.upper() +\n              \" Distplot After Skewness Correction\", color=\"black\")\n    plt.subplot(1,2,2)\n    stats.probplot(dataset[columnName], plot = plt)\n    plt.show();\n","c43bca5a":"skewColumnList = [\n    'fixed acidity', 'residual sugar', 'free sulfur dioxide', 'total sulfur dioxide', 'sulphates'\n]\nfor columns in skewColumnList:\n    skewnessCorrector(columns)\n","704e1ee5":"def detect_outliers(columns):\n    outlier_indices = []\n\n    for column in columns:\n        # 1st quartile\n        Q1 = np.percentile(dataset[column], 25)\n        # 3st quartile\n        Q3 = np.percentile(dataset[column], 75)\n        # IQR\n        IQR = Q3 - Q1\n        # Outlier Step\n        outlier_step = IQR * 1.5\n        # detect outlier and their indeces\n        outlier_list_col = dataset[(dataset[column] < Q1 - outlier_step)\n                              | (dataset[column] > Q3 + outlier_step)].index\n        # store indeces\n        outlier_indices.extend(outlier_list_col)\n\n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 1.5)\n\n    return multiple_outliers\n","db431e4d":"print(\"number of outliers detected --> \",\n      len(dataset.loc[detect_outliers(dataset.columns[:-1])]))","bdde8407":"dataset.loc[detect_outliers(dataset.columns[:-1])]","231135ff":"dataset = dataset.drop(detect_outliers(dataset.columns[:-1]),axis = 0).reset_index(drop = True)","f195c3ff":"!pip install pandas_profiling","f7e888e3":"ProfileReport(dataset)","050d7a85":"dataset['quality'] = np.where(dataset['quality'] > 6, 1, 0)\ndataset['quality'].value_counts()","53d9f005":"X = dataset.iloc[:, 0:-1].values\ny = dataset.iloc[:, -1].values","2c1f7fa3":"X","ea6bd02c":"y","544127d6":"X.shape","768d5cc8":"y.shape","37a8fe82":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=1)\n","67cd267d":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n","d61858e1":"accuracy_scores = {}\ndef predictor(predictor, params):\n    global accuracy_scores\n    if predictor == 'lr':\n        print('Training Logistic Regression on Training Set')\n        from sklearn.linear_model import LogisticRegression\n        classifier = LogisticRegression(**params)\n\n    elif predictor == 'svm':\n        print('Training Support Vector Machine on Training Set')\n        from sklearn.svm import SVC\n        classifier = SVC(**params)\n\n    elif predictor == 'knn':\n        print('Training K-Nearest Neighbours on Training Set')\n        from sklearn.neighbors import KNeighborsClassifier\n        classifier = KNeighborsClassifier(**params)\n\n    elif predictor == 'dt':\n        print('Training Decision Tree Classifier on Training Set')\n        from sklearn.tree import DecisionTreeClassifier\n        classifier = DecisionTreeClassifier(**params)\n\n    elif predictor == 'nb':\n        print('Training Naive Bayes Classifier on Training Set')\n        from sklearn.naive_bayes import GaussianNB\n        classifier = GaussianNB(**params)\n\n    elif predictor == 'rfc':\n        print('Training Random Forest Classifier on Training Set')\n        from sklearn.ensemble import RandomForestClassifier\n        classifier = RandomForestClassifier(**params)\n\n    classifier.fit(X_train, y_train)\n\n    print('''Predicting Single Cell Result''')\n    single_predict = classifier.predict(sc.transform([[\n        7.4, 0.7, 0.0, 1.9, 0.076, 11.0, 34.0, 0.9978, 3.51, 0.56, 9.4\n    ]]))\n    if single_predict > 0 :\n        print('High Quality Wine')\n    else:\n        print('Low Quality Wine')\n    print('''Prediciting Test Set Result''')\n    y_pred = classifier.predict(X_test)\n    \n    result = np.concatenate((y_pred.reshape(len(y_pred), 1),\n                             y_test.reshape(len(y_test), 1)),1)\n    print(result, '\\n')\n    print('''Making Confusion Matrix''')\n    from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n    y_pred = classifier.predict(X_test)\n    cm = confusion_matrix(y_test, y_pred)\n    print(cm, '\\n')\n    plot_confusion_matrix(classifier, X_test, y_test, cmap=\"pink\")\n    print('True Positives :', cm[0][0])\n    print('False Positives :', cm[0][1])\n    print('False Negatives :', cm[1][0])\n    print('True Negatives :', cm[0][1], '\\n')\n\n    print('''Classification Report''')\n    print(classification_report(y_test, y_pred,\n          target_names=['0', '1'], zero_division=1))\n\n    print('''Evaluating Model Performance''')\n    accuracy = accuracy_score(y_test, y_pred)\n    print(accuracy, '\\n')\n\n    print('''Applying K-Fold Cross validation''')\n    from sklearn.model_selection import cross_val_score\n    accuracies = cross_val_score(\n        estimator=classifier, X=X_train, y=y_train, cv=10)\n    print(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n    accuracy_scores[classifier] = accuracies.mean()*100\n    print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100), '\\n')\n","70c82e91":"predictor('lr', {'penalty': 'l1', 'solver': 'liblinear'})\n","1eca4556":"predictor('svm', {'C': .5, 'gamma': 0.8,\n          'kernel': 'linear', 'random_state': 0})\n","6967a6a6":"predictor('svm', {'C': .25, 'gamma': 0.1, 'kernel': 'rbf', 'random_state': 0})\n","cf96f44c":"predictor('knn', {'algorithm': 'auto', 'n_jobs': 1,\n          'n_neighbors': 8, 'weights': 'distance'})\n","40036e26":"predictor('dt', {'criterion': 'entropy', 'max_features': 'auto',\n          'splitter': 'best', 'random_state': 0})\n","8e452334":"predictor('nb', {})\n","54fe9ccb":"predictor('rfc', {'criterion': 'gini', 'max_features': 'log2', 'n_estimators': 100,'random_state':0})\n","ace7dfc6":"maxKey = max(accuracy_scores, key=lambda x: accuracy_scores[x])\nprint('The model with highest K-Fold Validation Accuracy score is  {0} with an accuracy of  {1:.2f}'.format(\n    maxKey, accuracy_scores[maxKey]))\n","d6025a74":"plt.figure(figsize=(12, 6))\nmodel_accuracies = list(accuracy_scores.values())\nmodel_names = ['LogisticRegression', 'SVC',\n               'K-SVC', 'KNN', 'Decisiontree', 'GaussianNB', 'RandomForest']\nsns.barplot(x=model_accuracies, y=model_names, palette='mako');\n","fbe24d26":"- If quality value is less than or eqaul to 6 then it will be in class 0\n- If quality value is greater than 6  then it will be in class 1","c79bb5e4":"## Training Random Forest Classifier on Training Set","8ba0e000":"## Training K-Nearest Neighbours on Training Set","3bcd2a1b":"## Plotting Bar Chart for Accuracies of different classifiers","c8169464":"# 2) Using Pandas Profiling","6d84c3e2":"## Training Logistic Regression on Training Set","9de8e046":"# Skewness Correction","2e7a472b":"We have detected several outliers in our dataset here we will try to correct them.\n","843d13b0":"# Exploratory Data Analysis\n","a3a1f0cd":"## Plotting Count for Qualities","0768f858":"There are no Null Values in the Dataset","5ae78894":"# Summary\n- Random Forest Classifier performed best on this data set with an accuracy of 90.81%\n- K-Nearest Classifier was just behind with an accuracy of an accuracy of 90.56% \n\n# **Please give your feedback by commenting below.**","e070812c":"## Visualising Numerical Data","8feb228b":"### Finding which Classifier performed best","95e01156":"## Acidity Type with Different Qualities of Wine","49805629":"## Training SVM on Training Set","1d8faa38":"# Loading Dataset","d101c9be":"## Training Naive Bayes on Training Set","e05011a7":"## 1) Using Manual Methods","bcf5f47d":"## Distribution of pH with Different Qualities of Wine","a719568e":"## Training Decision Tree on Training Set","62333428":"Dropping Outliers","596c7f5c":"# Training Classifiers on Training Set and drawing Inference","c21402e9":"## Training Kernel SVM on Training Set","24f1db7b":"# Data Preprocessing","13dede70":"# Outlier Correction\n","7ccc5082":"## Finding Correlation among the variables","78ecd60b":"Here we will try to correct Skewness in some independent varaibles of our dataset","c34811e2":"## Standardizing Independent Variables","f943d144":"## Splitting Dataset into Training Set and Test Set"}}