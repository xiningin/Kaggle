{"cell_type":{"d08ee253":"code","d0ef5fc8":"code","811727b3":"code","ea21e64d":"code","1254c895":"code","1f650b53":"code","03b39405":"code","6777c181":"code","39a665b3":"code","f6e612b7":"markdown","5175474e":"markdown","b3cca01d":"markdown","21347aef":"markdown","0b88fceb":"markdown","47159f50":"markdown","5a1d6eac":"markdown"},"source":{"d08ee253":"!pip install nnAudio","d0ef5fc8":"from nnAudio import Spectrogram\nfrom scipy.io import wavfile\nimport torch\nimport matplotlib.pyplot as plt","811727b3":"if torch.cuda.is_available():\n    device = \"cuda:0\"\n    torch.set_default_tensor_type('torch.cuda.FloatTensor')\nelse:\n    device = \"cpu\"","ea21e64d":"!wget \"https:\/\/file-examples-com.github.io\/uploads\/2017\/11\/file_example_WAV_1MG.wav\"\nsr, song = wavfile.read('file_example_WAV_1MG.wav') # Loading your audio\nx = song.mean(1) # Converting Stereo  to Mono\nx = torch.tensor(x, device=device).float() # casting the array into a PyTorch Tensor","1254c895":"# linear spectrograms\nspec_layer = Spectrogram.STFT(n_fft=2048, freq_bins=None, hop_length=512, \n                              window='hann', freq_scale='log', center=True, pad_mode='reflect', \n                              fmin=50,fmax=11025, sr=sr, device=device) # Initializing the model with device='cuda:0'\nspec_lin = spec_layer(x)","1f650b53":"# log spectrograms\nspec_layer = Spectrogram.STFT(n_fft=2048, freq_bins=None, hop_length=512, \n                              window='hann', freq_scale='linear', center=True, pad_mode='reflect', \n                              fmin=50,fmax=11025, sr=sr, device=device) # Initializing the model with device='cuda:0'\nspec_log = spec_layer(x)","03b39405":"# Mel spectrogram\nspec_layer = Spectrogram.MelSpectrogram(sr=22050, n_fft=2048, n_mels=128, \n                                        hop_length=512, window='hann', center=True, \n                                        pad_mode='reflect', htk=False, fmin=0.0, fmax=None, \n                                        norm=1, trainable_mel=False, trainable_STFT=False, device=device)  # Initializing the model with device='cuda:0'\nspec_mel = spec_layer(x)","6777c181":"# CQT\nspec_layer = Spectrogram.CQT2010v2(sr=22050, hop_length=512, fmin=55, fmax=None, n_bins=88,\n                                   bins_per_octave=12, norm=True, basis_norm=1, \n                                   window='hann', pad_mode='reflect', earlydownsample=True, device=device)  # Initializing the model with device='cuda:0'\n\nspec_cqt = spec_layer(x)","39a665b3":"fig, ax = plt.subplots(1,4, figsize=(16,3))\nlabels = ['spec_lin', 'spec_log', 'spec_mel', 'spec_cqt']\nfor i,spec in enumerate([spec_lin, spec_log, spec_mel, spec_cqt]):\n  ax[i].imshow(spec.cpu().numpy()[0], origin='lower', cmap='jet', aspect='auto')\n  ax[i].title.set_text(labels[i])","f6e612b7":"Source: https:\/\/github.com\/KinWaiCheuk\/nnAudio","5175474e":"# 3. CQT","b3cca01d":"# 2. Mel Spectrogram","21347aef":"Audio processing by using pytorch 1D convolution network. By doing so, spectrograms can be generated from audio on-the-fly during neural network training. Kapre and torch-stft have a similar concept in which they also use 1D convolution from keras adn PyTorch to do the waveforms to spectrogram conversions.\n\nOther GPU audio processing tools are torchaudio and tf.signal. But they are not using the neural network approach, and hence the Fourier basis can not be trained.\n\nThe name of nnAudio comes from torch.nn, since most of the codes are built from torch.nn.","0b88fceb":"# nnAudio","47159f50":"**Currently there are 3 models to generate various types of spectrograms**","5a1d6eac":"# 1. STFT"}}