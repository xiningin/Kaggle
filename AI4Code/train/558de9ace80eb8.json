{"cell_type":{"7bf73c16":"code","1127751c":"code","8f6e785a":"code","ccbce391":"code","e3d7301c":"code","f2d35850":"code","bfd3e1f2":"code","9115df05":"code","205d65c3":"code","4e293d49":"code","c2e3a698":"code","932b9be4":"code","5cbf941b":"code","02b84e11":"code","21f1a4f0":"code","9a677162":"code","6e99a130":"code","941d0b35":"code","e1037d1b":"markdown","93859e89":"markdown","2452c465":"markdown","bf278dbf":"markdown","ab73cc3d":"markdown","5dbbe590":"markdown","f85ffc2d":"markdown","bcc371bd":"markdown","4909dbe7":"markdown","37981e07":"markdown","6af18f1d":"markdown","b2721d16":"markdown","40d2d13c":"markdown","41b419a5":"markdown","2efd3803":"markdown","2991790b":"markdown","ab110285":"markdown","7a460932":"markdown","5b1c21dc":"markdown","c2abffee":"markdown","54acce72":"markdown","afbbc89d":"markdown","a8f2a8f1":"markdown","a79765da":"markdown","9e6541a4":"markdown","31ae8610":"markdown"},"source":{"7bf73c16":"! pip install split-folders","1127751c":"### TODO: Write data loaders for training, validation, and test sets\n## Specify appropriate transforms, and batch_sizes\nimport torch\nfrom torchvision import datasets, transforms\nimport numpy as np\nimport splitfolders\n\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n# Create Transforms\ntrain_transform = transforms.Compose([\n    transforms.RandomResizedCrop(224),\n    # Augmentation\n    transforms.RandomHorizontalFlip(p=0.3), \n    transforms.RandomRotation(10),\n    transforms.RandomChoice([\n            transforms.ColorJitter(hue=0.1),\n            transforms.ColorJitter(brightness=0.2),\n            transforms.ColorJitter(saturation=0.2),\n            transforms.ColorJitter(contrast=0.2),\n        ]),\n    transforms.ToTensor(),\n    transforms.Normalize(mean, std) # Imagenet standards\n])\n\ntest_transform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean, std) # Imagenet standards\n])\n\n# Split the training folder to new training and validation folders\nsplitfolders.ratio(\"..\/input\/landmark-images\/landmark_images\/train\", output=\"train_valid\", seed=1337, ratio=(.8, .2), group_prefix=None)\n# Load the image data\ntrain_data = datasets.ImageFolder('.\/train_valid\/train', transform=train_transform)\nvalid_data = datasets.ImageFolder('.\/train_valid\/val', transform=test_transform)\ntest_data = datasets.ImageFolder('..\/input\/landmark-images\/landmark_images\/test', transform=test_transform)\n# Save classes names \nn_classes = len(train_data.classes)\nclasses = [class_.split(\".\")[1].replace(\"_\", \" \") for class_ in train_data.classes]\nbatch_size = 20\n# Create data loaders\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size, shuffle=True)\nvalid_loader = torch.utils.data.DataLoader(valid_data, batch_size, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size, shuffle=True)\nloaders_scratch = {'train': train_loader, 'valid': valid_loader, 'test': test_loader}","8f6e785a":"def unnormlize(img, s, m):\n    return img * s[:, None, None] + m[:, None, None]\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n## TODO: visualize a batch of the train data loader\n\n## the class names can be accessed at the `classes` attribute\n## of your dataset object (e.g., `train_dataset.classes`)\n## the class names can be accessed at the `classes` attribute\n## of your dataset object (e.g., `train_dataset.classes`)\nimport random\nfig = plt.figure(figsize=(20,2*8))\nfor idx in range(8):\n    ax = fig.add_subplot(4, 4, idx+1, xticks=[], yticks=[], )\n    rand_img = random.randint(0, len(train_data))\n    img = unnormlize(train_data[rand_img][0], torch.Tensor(std), torch.Tensor(mean)) # unnormalize\n    plt.imshow(np.transpose(img.numpy(), (1, 2, 0))) # convert from Tensor image\n    class_name = classes[train_data[rand_img][1]]\n    ax.set_title(class_name)","ccbce391":"# useful variable that tells us whether we should use the GPU\nuse_cuda = torch.cuda.is_available()","e3d7301c":"## TODO: select loss function\nimport torch.nn as nn\nimport torch.optim as optim\ncriterion_scratch = nn.CrossEntropyLoss() \ndef get_optimizer_scratch(model):    \n    ## TODO: select and return an optimizer\n    optimizer = optim.SGD(model.parameters(), lr=0.01)\n    return optimizer   ","f2d35850":"import torch.nn.functional as F\n\n# define the CNN architecture\nclass Net(nn.Module):\n    ## TODO: choose an architecture, and complete the class\n    def __init__(self):\n        super(Net, self).__init__()\n        ## Define layers of a CNN        \n        # Convolutional layers\n        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)       \n        self.pool = nn.MaxPool2d(2, 2)        \n        self.fc1 = nn.Linear(28 * 28 * 64, 256)\n        self.fc2 = nn.Linear(256, n_classes) # n_classes = 50       \n        self.dropout = nn.Dropout(0.3)        \n        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2)        \n        self.batch_norm2d = nn.BatchNorm2d(32)\n        self.batch_norm1d = nn.BatchNorm1d(256)            \n    def forward(self, x):\n        ## Define forward behavior\n        # sequence of convolutional and max pooling layers\n        x = self.pool(self.leaky_relu(self.conv1(x)))\n        x = self.pool(self.leaky_relu(self.conv2(x)))\n        x = self.batch_norm2d(x)\n        x = self.pool(self.leaky_relu(self.conv3(x)))        \n        x = x.view(-1, 28 * 28 * 64)        \n        x = self.dropout(x)       \n        x = self.leaky_relu(self.fc1(x))        \n        x = self.batch_norm1d(x)       \n        x = self.dropout(x)     \n        x = self.fc2(x)        \n        return x\n\n#-#-# Do NOT modify the code below this line. #-#-#\n\n# instantiate the CNN\nmodel_scratch = Net()\n\n# move tensors to GPU if CUDA is available\nif use_cuda:\n    model_scratch.cuda()","bfd3e1f2":"def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n    \"\"\"returns trained model\"\"\"\n    # initialize tracker for minimum validation loss\n    valid_loss_min = np.Inf \n    \n    for epoch in range(1, n_epochs+1):\n        # initialize variables to monitor training and validation loss\n        train_loss = 0.0\n        valid_loss = 0.0\n        \n        ###################\n        # train the model #\n        ###################\n        # set the module to training mode\n        model.train()\n        for batch_idx, (data, target) in enumerate(loaders['train']):\n            # move to GPU\n            if use_cuda:\n                data, target = data.cuda(), target.cuda()\n\n            ## TODO: find the loss and update the model parameters accordingly\n            optimizer.zero_grad()\n            output = model(data)\n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n            \n            ## record the average training loss, using something like\n            train_loss = train_loss + ((1 \/ (batch_idx + 1)) * (loss.data.item() - train_loss))\n\n            \n        ######################    \n        # validate the model #\n        ######################\n        # set the model to evaluation mode\n        model.eval()\n        for batch_idx, (data, target) in enumerate(loaders['valid']):\n            # move to GPU\n            if use_cuda:\n                data, target = data.cuda(), target.cuda()\n\n            ## TODO: update average validation loss \n            output = model(data)\n            loss = criterion(output, target)\n            valid_loss += ((1 \/ (batch_idx + 1)) * (loss.data.item() - valid_loss))\n            \n            \n        # print training\/validation statistics \n        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch, \n                                                                                   train_loss,\n                                                                                   valid_loss))\n\n        ## TODO: if the validation loss has decreased, save the model at the filepath stored in save_path\n        if valid_loss <= valid_loss_min:\n            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,\n                                                                                            valid_loss))\n            torch.save(model.state_dict(), save_path)\n            valid_loss_min = valid_loss\n        \n        \n    return model","9115df05":"def custom_weight_init(m):\n    ## TODO: implement a weight initialization strategy\n    if isinstance(m, nn.Conv2d):\n        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        m.weight.data.normal_(0, np.sqrt(2. \/ n))\n        if m.bias is not None:\n            m.bias.data.zero_()\n    elif isinstance(m, nn.BatchNorm2d):\n        m.weight.data.fill_(1)\n        m.bias.data.zero_()\n    elif isinstance(m, nn.Linear):\n        n = m.in_features\n        y = 1.0\/np.sqrt(n)\n        m.weight.data.normal_(0, y)\n        m.bias.data.zero_()\n    \n#-#-# Do NOT modify the code below this line. #-#-#\n    \nmodel_scratch.apply(custom_weight_init)\nmodel_scratch = train(20, loaders_scratch, model_scratch, get_optimizer_scratch(model_scratch),\n                      criterion_scratch, use_cuda, 'ignore.pt')","205d65c3":"## TODO: you may change the number of epochs if you'd like,\nnum_epochs = 20 \n\n#-#-# Do NOT modify the code below this line. #-#-#\n\n# function to re-initialize a model with pytorch's default weight initialization\ndef default_weight_init(m):\n    reset_parameters = getattr(m, 'reset_parameters', None)\n    if callable(reset_parameters):\n        m.reset_parameters()\n\n# reset the model parameters\nmodel_scratch.apply(default_weight_init)\n\n# train the model\nmodel_scratch = train(num_epochs, loaders_scratch, model_scratch, get_optimizer_scratch(model_scratch), \n                      criterion_scratch, use_cuda, 'model_scratch.pt')","4e293d49":"def test(loaders, model, criterion, use_cuda):\n\n    # monitor test loss and accuracy\n    test_loss = 0.\n    correct = 0.\n    total = 0.\n\n    # set the module to evaluation mode\n    model.eval()\n\n    for batch_idx, (data, target) in enumerate(loaders['test']):\n        # move to GPU\n        if use_cuda:\n            data, target = data.cuda(), target.cuda()\n        # forward pass: compute predicted outputs by passing inputs to the model\n        output = model(data)\n        # calculate the loss\n        loss = criterion(output, target)\n        # update average test loss \n        test_loss = test_loss + ((1 \/ (batch_idx + 1)) * (loss.data.item() - test_loss))\n        # convert output probabilities to predicted class\n        pred = output.data.max(1, keepdim=True)[1]\n        # compare predictions to true label\n        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n        total += data.size(0)\n            \n    print('Test Loss: {:.6f}\\n'.format(test_loss))\n\n    print('\\nTest Accuracy: %2d%% (%2d\/%2d)' % (\n        100. * correct \/ total, correct, total))\n\n# load the model that got the best validation accuracy\nmodel_scratch.load_state_dict(torch.load('model_scratch.pt'))\ntest(loaders_scratch, model_scratch, criterion_scratch, use_cuda)","c2e3a698":"### TODO: Write data loaders for training, validation, and test sets\n## Specify appropriate transforms, and batch_sizes\n\n#loaders_transfer = {'train': None, 'valid': None, 'test': None}\n\nloaders_transfer = loaders_scratch.copy()","932b9be4":"## TODO: select loss function\ncriterion_transfer = nn.CrossEntropyLoss() \ndef get_optimizer_transfer(model):\n    ## TODO: select and return optimizer\n    optimizer = optim.SGD(model.classifier.parameters(), lr=0.01)\n    # more general solution:\n    # filter(lambda param: param.requires_grad, model_transfer.parameters())\n    return optimizer","5cbf941b":"## TODO: Specify model architecture\nfrom torchvision import models\nmodel_transfer = models.vgg16(pretrained=True)\n# Freeze training for all \"features\" layers\nfor parameter in model_transfer.features.parameters():\n    parameter.requires_grad = False    \n# replace the final layer with one of your own problem\nn_input = model_transfer.classifier[6].in_features\nmodel_transfer.classifier[6] = nn.Linear(n_input, n_classes) \n#-#-# Do NOT modify the code below this line. #-#-#\n\nif use_cuda:\n    model_transfer = model_transfer.cuda()","02b84e11":"# TODO: train the model and save the best model parameters at filepath 'model_transfer.pt'\ntrain(15, loaders_transfer, model_transfer, get_optimizer_transfer(model_transfer), criterion_transfer,\n      use_cuda, 'model_transfer.pt')\n#-#-# Do NOT modify the code below this line. #-#-#\n\n# load the model that got the best validation accuracy\nmodel_transfer.load_state_dict(torch.load('model_transfer.pt'))","21f1a4f0":"test(loaders_transfer, model_transfer, criterion_transfer, use_cuda)","9a677162":"import cv2\nfrom PIL import Image\n\n## the class names can be accessed at the `classes` attribute\n## of your dataset object (e.g., `train_dataset.classes`)\n\ndef predict_landmarks(img_path, k):\n    ## TODO: return the names of the top k landmarks predicted by the transfer learned CNN\n    img = Image.open(img_path).convert('RGB')\n    \n    transform = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n    \n    img = transform(img)\n    img.unsqueeze_(0)\n    \n    if use_cuda:\n        img = img.cuda()\n        \n    model_transfer.eval()\n    output = model_transfer(img)\n    top_values, top_idx = output.topk(k)\n\n    top_classes = [classes[class_id] for class_id in top_idx[0].tolist()]\n    return top_classes\n\n# test on a sample image\npredict_landmarks('..\/input\/landmark-images\/landmark_images\/test\/09.Golden_Gate_Bridge\/190f3bae17c32c37.jpg', 5)","6e99a130":"def suggest_locations(img_path):\n    # get landmark predictions\n    predicted_landmarks = predict_landmarks(img_path, 3)\n    \n    ## TODO: display image and display landmark predictions\n    img = Image.open(img_path).convert('RGB')\n    plt.imshow(img)\n    plt.show()\n\n    print(f\"Actual Label: {img_path.split('\/')[2][3:].replace('_',' ').split('.')[0]}\")\n    print(f\"Predicted Label in order: Is this picture of the\\n {predicted_landmarks[0]}, {predicted_landmarks[1]}, or {predicted_landmarks[2]}?\")\n    \n\n# test on a sample image\nsuggest_locations('..\/input\/landmark-images\/landmark_images\/test\/09.Golden_Gate_Bridge\/190f3bae17c32c37.jpg')","941d0b35":"## TODO: Execute the `suggest_locations` function on\n## at least 4 images on your computer.\n## Feel free to use as many code cells as needed.\nimport os\n\nfor img_path in os.listdir('..\/input\/landmark-images\/landmark_images\/test'):\n    img_path = os.path.join('..\/input\/landmark-images\/landmark_images\/test', img_path)\n    if img_path.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif','jfif')):\n        suggest_locations(img_path)","e1037d1b":"### (IMPLEMENTATION) Visualize a Batch of Training Data\n\nUse the code cell below to retrieve a batch of images from your train data loader, display at least 5 images simultaneously, and label each displayed image with its class name (e.g., \"Golden Gate Bridge\").\n\nVisualizing the output of your data loader is a great way to ensure that your data loading and preprocessing are working as expected.","93859e89":"### (IMPLEMENTATION) Experiment with the Weight Initialization\n\nUse the code cell below to define a custom weight initialization, and then train with your weight initialization for a few epochs. Make sure that neither the training loss nor validation loss is `nan`.\n\nLater on, you will be able to see how this compares to training with PyTorch's default weight initialization.","2452c465":"### (IMPLEMENTATION) Implement the Training Algorithm\n\nImplement your training algorithm in the code cell below.  [Save the final model parameters](http:\/\/pytorch.org\/docs\/master\/notes\/serialization.html) at the filepath stored in the variable `save_path`.","bf278dbf":"__Question 2:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  ","ab73cc3d":"**Question 1:** Describe your chosen procedure for preprocessing the data. \n- How does your code resize the images (by cropping, stretching, etc)?  What size did you pick for the input tensor, and why?\n- Did you decide to augment the dataset?  If so, how (through translations, flips, rotations, etc)?  If not, why not?","5dbbe590":"---\n<a id='step3'><\/a>\n## Step 3: Write Your Landmark Prediction Algorithm\n\nGreat job creating your CNN models! Now that you have put in all the hard work of creating accurate classifiers, let's define some functions to make it easy for others to use your classifiers.\n\n### (IMPLEMENTATION) Write Your Algorithm, Part 1\n\nImplement the function `predict_landmarks`, which accepts a file path to an image and an integer k, and then predicts the **top k most likely landmarks**. You are **required** to use your transfer learned CNN from Step 2 to predict the landmarks.\n\nAn example of the expected behavior of `predict_landmarks`:\n```\n>>> predicted_landmarks = predict_landmarks('example_image.jpg', 3)\n>>> print(predicted_landmarks)\n['Golden Gate Bridge', 'Brooklyn Bridge', 'Sydney Harbour Bridge']\n```","f85ffc2d":"__Answer:__  \n- i choosed VGG-16 becase it is better than DenseNet-161 .this arch gives us a good architecture and taking all preceding feature-maps as input.and that gives me higher scores and faster performace so my accuracy 78% after 15 epoches","bcc371bd":"### (IMPLEMENTATION) Test the Model\n\nRun the code cell below to try out your model on the test dataset of landmark images. Run the code cell below to calculate and print the test loss and accuracy.  Ensure that your test accuracy is greater than 20%.","4909dbe7":"### (IMPLEMENTATION) Write Your Algorithm, Part 2\n\nIn the code cell below, implement the function `suggest_locations`, which accepts a file path to an image as input, and then displays the image and the **top 3 most likely landmarks** as predicted by `predict_landmarks`.\n\nSome sample output for `suggest_locations` is provided below, but feel free to design your own user experience!\n![](images\/sample_landmark_output.png)","37981e07":"__Question 3:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  Describe why you think the architecture is suitable for the current problem.","6af18f1d":"---\n\n<a id='step1'><\/a>\n## Step 1: Create a CNN to Classify Landmarks (from Scratch)\n\nIn this step, you will create a CNN that classifies landmarks.  You must create your CNN _from scratch_ (so, you can't use transfer learning _yet_!), and you must attain a test accuracy of at least 20%.\n\nAlthough 20% may seem low at first glance, it seems more reasonable after realizing how difficult of a problem this is. Many times, an image that is taken at a landmark captures a fairly mundane image of an animal or plant, like in the following picture.\n\n<img src=\"images\/train\/00.Haleakala_National_Park\/084c2aa50d0a9249.jpg\" alt=\"Bird in Haleakal\u0101 National Park\" style=\"width: 400px;\"\/>\n\nJust by looking at that image alone, would you have been able to guess that it was taken at the Haleakal\u0101 National Park in Hawaii?\n\nAn accuracy of 20% is significantly better than random guessing, which would provide an accuracy of just 2%. In Step 2 of this notebook, you will have the opportunity to greatly improve accuracy by using transfer learning to create a CNN.\n\nRemember that practice is far ahead of theory in deep learning.  Experiment with many different architectures, and trust your intuition.  And, of course, have fun!","b2721d16":"### Initialize use_cuda variable","40d2d13c":"### (IMPLEMENTATION) Model Architecture\n\nUse transfer learning to create a CNN to classify images of landmarks.  Use the code cell below, and save your initialized model as the variable `model_transfer`.","41b419a5":"### (IMPLEMENTATION) Test Your Algorithm\n\nTest your algorithm by running the `suggest_locations` function on at least four images on your computer. Feel free to use any images you like.\n\n__Question 4:__ Is the output better than you expected :) ?  Or worse :( ?  Provide at least three possible points of improvement for your algorithm.","2efd3803":"### (IMPLEMENTATION) Test the Model\n\nTry out your model on the test dataset of landmark images. Use the code cell below to calculate and print the test loss and accuracy.  Ensure that your test accuracy is greater than 60%.","2991790b":"### (IMPLEMENTATION) Train and Validate the Model\n\nTrain and validate your model in the code cell below.  [Save the final model parameters](http:\/\/pytorch.org\/docs\/master\/notes\/serialization.html) at filepath `'model_transfer.pt'`.","ab110285":"**Answer**: \n- I randomly resized crop the images to 224x224 pixels for training and for testing, I did normal resized to 256, then center crop to 224x224 pixels size.then center crop to 224x224 pixels size. I have picked 224x224 pixels as the size of the input tensor.\n- Data augmentation has been applied on just the training subset of the dataset through random rotations of 10 degrees and random horizontal flips to the images, also with random and different color jitters.","7a460932":"__Answer:__ (Three possible points for improvement)\n- The outputs are pretty good\n- Possible points for improvement:\n    1- Providing more images to train\n    2- Doing more data augmentations techniques\n    3- Maybe Trying other transfer learning models","5b1c21dc":"### (IMPLEMENTATION) Train and Validate the Model\n\nRun the next code cell to train your model.","c2abffee":"# Convolutional Neural Networks\n\n## Project: Write an Algorithm for Landmark Classification\n\n---\n\nIn this notebook, some template code has already been provided for you, and you will need to implement additional functionality to successfully complete this project. You will not need to modify the included code beyond what is requested. Sections that begin with **'(IMPLEMENTATION)'** in the header indicate that the following block of code will require additional functionality which you must provide. Instructions will be provided for each section, and the specifics of the implementation are marked in the code block with a 'TODO' statement. Please be sure to read the instructions carefully! \n\n> **Note**: Once you have completed all the code implementations, you need to finalize your work by exporting the Jupyter Notebook as an HTML document. Before exporting the notebook to HTML, all the code cells need to have been run so that reviewers can see the final implementation and output. You can then export the notebook by using the menu above and navigating to **File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission.\n\nIn addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question X'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.\n\n>**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut.  Markdown cells can be edited by double-clicking the cell to enter edit mode.\n\nThe rubric contains _optional_ \"Stand Out Suggestions\" for enhancing the project beyond the minimum requirements. If you decide to pursue the \"Stand Out Suggestions\", you should include the code in this Jupyter notebook.\n\n---\n### Why We're Here\n\nPhoto sharing and photo storage services like to have location data for each photo that is uploaded. With the location data, these services can build advanced features, such as automatic suggestion of relevant tags or automatic photo organization, which help provide a compelling user experience. Although a photo's location can often be obtained by looking at the photo's metadata, many photos uploaded to these services will not have location metadata available. This can happen when, for example, the camera capturing the picture does not have GPS or if a photo's metadata is scrubbed due to privacy concerns.\n\nIf no location metadata for an image is available, one way to infer the location is to detect and classify a discernable landmark in the image. Given the large number of landmarks across the world and the immense volume of images that are uploaded to photo sharing services, using human judgement to classify these landmarks would not be feasible.\n\nIn this notebook, you will take the first steps towards addressing this problem by building models to automatically predict the location of the image based on any landmarks depicted in the image. At the end of this project, your code will accept any user-supplied image as input and suggest the top k most relevant landmarks from 50 possible landmarks from across the world. The image below displays a potential sample output of your finished project.\n\n![Sample landmark classification output](images\/sample_landmark_output.png)\n\n\n### The Road Ahead\n\nWe break the notebook into separate steps.  Feel free to use the links below to navigate the notebook.\n\n* [Step 0](#step0): Download Datasets and Install Python Modules\n* [Step 1](#step1): Create a CNN to Classify Landmarks (from Scratch)\n* [Step 2](#step2): Create a CNN to Classify Landmarks (using Transfer Learning)\n* [Step 3](#step3): Write Your Landmark Prediction Algorithm\n\n---\n<a id='step0'><\/a>\n## Step 0: Download Datasets and Install Python Modules\n\n**Note: if you are using the Udacity workspace, *YOU CAN SKIP THIS STEP*. The dataset can be found in the `\/data` folder and all required Python modules have been installed in the workspace.**\n\nDownload the [landmark dataset](https:\/\/udacity-dlnfd.s3-us-west-1.amazonaws.com\/datasets\/landmark_images.zip).\nUnzip the folder and place it in this project's home directory, at the location `\/landmark_images`.\n\nInstall the following Python modules:\n* cv2\n* matplotlib\n* numpy\n* PIL\n* torch\n* torchvision","54acce72":"### (IMPLEMENTATION) Specify Loss Function and Optimizer\n\nUse the next code cell to specify a [loss function](http:\/\/pytorch.org\/docs\/stable\/nn.html#loss-functions) and [optimizer](http:\/\/pytorch.org\/docs\/stable\/optim.html).  Save the chosen loss function as `criterion_scratch`, and fill in the function `get_optimizer_scratch` below.","afbbc89d":"__Answer:__  \n- CNN contains of 3 layers as follows \n- 1-Conv1 - 224x224 16 | Leaky relu activation function\n- 2-Conv2 - 112x112 32 | Leaky relu activation function\n- 3-Conv3 - 56x56 64 | Leaky relu activation function\n  and 2 fully connected layers and dropout of 30 % probability,\n  Dropout of 30% probability,Dropout has been used to avoid overfitting and Max-pooling layers to focus on the main target    features via dividing the image by a factor of 2. I attempted to speed up the training process and to perform some regularization to the model via Batch Normalization.\n\n","a8f2a8f1":"### (IMPLEMENTATION) Model Architecture\n\nCreate a CNN to classify images of landmarks.  Use the template in the code cell below.","a79765da":"### (IMPLEMENTATION) Specify Loss Function and Optimizer\n\nUse the next code cell to specify a [loss function](http:\/\/pytorch.org\/docs\/stable\/nn.html#loss-functions) and [optimizer](http:\/\/pytorch.org\/docs\/stable\/optim.html).  Save the chosen loss function as `criterion_transfer`, and fill in the function `get_optimizer_transfer` below.","9e6541a4":"### (IMPLEMENTATION) Specify Data Loaders for the Landmark Dataset\n\nUse the code cell below to create three separate [data loaders](http:\/\/pytorch.org\/docs\/stable\/data.html#torch.utils.data.DataLoader): one for training data, one for validation data, and one for test data. Randomly split the images located at `landmark_images\/train` to create the train and validation data loaders, and use the images located at `landmark_images\/test` to create the test data loader.\n\n**Note**: Remember that the dataset can be found at `\/data\/landmark_images\/` in the workspace.\n\nAll three of your data loaders should be accessible via a dictionary named `loaders_scratch`. Your train data loader should be at `loaders_scratch['train']`, your validation data loader should be at `loaders_scratch['valid']`, and your test data loader should be at `loaders_scratch['test']`.\n\nYou may find [this documentation on custom datasets](https:\/\/pytorch.org\/docs\/stable\/torchvision\/datasets.html#datasetfolder) to be a useful resource.  If you are interested in augmenting your training and\/or validation data, check out the wide variety of [transforms](http:\/\/pytorch.org\/docs\/stable\/torchvision\/transforms.html?highlight=transform)!","31ae8610":"---\n<a id='step2'><\/a>\n## Step 2: Create a CNN to Classify Landmarks (using Transfer Learning)\n\nYou will now use transfer learning to create a CNN that can identify landmarks from images.  Your CNN must attain at least 60% accuracy on the test set.\n\n### (IMPLEMENTATION) Specify Data Loaders for the Landmark Dataset\n\nUse the code cell below to create three separate [data loaders](http:\/\/pytorch.org\/docs\/stable\/data.html#torch.utils.data.DataLoader): one for training data, one for validation data, and one for test data. Randomly split the images located at `landmark_images\/train` to create the train and validation data loaders, and use the images located at `landmark_images\/test` to create the test data loader.\n\nAll three of your data loaders should be accessible via a dictionary named `loaders_transfer`. Your train data loader should be at `loaders_transfer['train']`, your validation data loader should be at `loaders_transfer['valid']`, and your test data loader should be at `loaders_transfer['test']`.\n\nIf you like, **you are welcome to use the same data loaders from the previous step**, when you created a CNN from scratch."}}