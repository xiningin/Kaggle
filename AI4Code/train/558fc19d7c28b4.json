{"cell_type":{"68b20ddf":"code","12da7d32":"code","26029833":"code","3127571a":"code","ba24a872":"code","e3fe7b5e":"code","b919fec4":"code","a8348555":"code","96486c00":"code","b6076ba8":"code","81f04598":"code","f6155d7c":"code","de480d3a":"code","e13afd7e":"code","9147ec13":"code","c6642f8e":"code","a6b5a7fa":"code","d30151c1":"code","f3a4cac1":"code","64e8a57f":"code","89bdf38d":"code","6dd8b279":"code","b6a10e5a":"code","eeba6985":"code","180f1537":"code","10a2c193":"code","4a063f33":"code","8b174a80":"code","b95b4ad3":"markdown","d2458b7b":"markdown","ada668c9":"markdown","39271ca6":"markdown","436a06f5":"markdown","fb025552":"markdown","831bcce2":"markdown","9c8d8f96":"markdown","1b94d409":"markdown","086fa26b":"markdown","e853db34":"markdown","c7f48653":"markdown","fd4e9c16":"markdown","ab08b8e5":"markdown","29ac2a74":"markdown"},"source":{"68b20ddf":"#load all needed libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sb\nimport pandas as pd\n%matplotlib inline","12da7d32":"#load data for train and test\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","26029833":"#view some train data\ntrain.head()","3127571a":"#view some test data\ntest.head()","ba24a872":"#shape of train dataset\ntrain.shape","e3fe7b5e":"#shape of test dataset\ntest.shape","b919fec4":"#check data type of each column\ntrain.info()","a8348555":"#check missing values with 'isnull' command \ntrain.isnull().any()","96486c00":"#count, how many missing value is in this attributes\ntrain['y'].isnull().sum()","b6076ba8":"# Here, I remove Row of with missing value\ntrain = train.dropna()","81f04598":"#Now check there is missing value or not\nprint(train.isnull().any())\n\n#plot\nsb.heatmap(train.isnull())\nplt.show()","f6155d7c":"#describe of data into statical form\ntrain.describe()","de480d3a":"#Histograme od train dataset\ntrain.hist()\nplt.show()","e13afd7e":"#Scatter plot\nplt.scatter(x=train.x, y=train.y, c='blue')\nplt.title('scatter plot')\nplt.xlabel('Independent variables')\nplt.ylabel('Dependent variables')\nplt.show()","9147ec13":"#BoxPlot\ntrain.plot(kind='box', subplots=True, layout=(2, 2), figsize=(12, 8))\nplt.show()","c6642f8e":"#Input Variable \nX = train.x.values\n#Output Variable\ny = train.y.values","a6b5a7fa":"#Calculate mean of list numbers with mean function\ndef mean(numbers):\n    return sum(numbers) \/ float(len(numbers))\n\n#Calculate varience of list numbers with varience function\ndef varience(numbers, mean):\n    return sum([abs(x-mean)**2 for x in numbers])","d30151c1":"X_mean, y_mean = mean(X), mean(y)\nX_varience = varience(X, X_mean)\ny_varience = varience(y, y_mean)","f3a4cac1":"#Calculate the covarience of these groups\ndef covarience(X, X_mean, y, y_mean):\n    ln = len(X)\n    cov = 0.0\n    for i in range(ln):\n        cov += ((X[i] - X_mean) * (y[i] - y_mean))\n    return cov","64e8a57f":"#Lets estimate with coefficient\ndef coefficients():\n    m = covarience(X, X_mean, y, y_mean) \/ varience(X, X_mean)\n    b = y_mean - (m*X_mean)\n    return [m,b]","89bdf38d":"#Let's seprate the test datasets and reshape it\nX_test = test['x'].values.reshape(-1, 1)\ny_test = test['y'].values.reshape(-1, 1)","6dd8b279":"# simple_linear_regression() function making here to prediction\ndef simple_linear_regression():\n    prediction = list()\n    m, c = coefficients()\n    for test in X_test:\n        y_pred = m*test[0] + c\n        prediction.append(y_pred)\n    return prediction","b6a10e5a":"predict = simple_linear_regression()","eeba6985":"# Ploting Line\nplt.plot(X_test, predict, c='red', label='Regression Line')\n# Ploting Scatter Points\nplt.scatter(X, y, label='data', c='blue')\n\nplt.xlabel('Independent variable')\nplt.ylabel('Dependent variable')\nplt.legend()\nplt.show()","180f1537":"def root_mean_sqaure_error():\n    rmse = 0.0\n    m, c = coefficients()\n    for i in range(len(X_test)):\n        yhat = m*X_test[i] + c\n        rmse += (y_test[i] - yhat)**2\n    rmse = np.sqrt(rmse\/len(X_test))\n    return rmse","10a2c193":"#Root Mean Sqare Error\nRMSE = root_mean_sqaure_error()\nprint(RMSE[0])","4a063f33":"def r_sqaure():\n    #sst is the total sum of squares and ssr is the total sum of squares of residuals\n    sst = 0\n    ssr = 0\n    m, c = coefficients()\n    for i in range(len(X_test)):\n        ypred = m*X_test[i] + c\n        ssr += (y_test[i] - ypred)**2\n        sst += (y_test[i] - y_mean)**2\n    return (1-(ssr\/sst))","8b174a80":"#R-Sqaure\nscore = r_sqaure()\nprint(score[0])","b95b4ad3":"### **Now, Problem is that how to handle missing values ?**\nSolution : For missing values, there is some approach to handle the missing values like:-\n\n    1. Remove Rows With Missing Values\n    2. Imputting missing values with Mean, Median or Mode values of the Column\n   And, for more infomation [click here](https:\/\/machinelearningmastery.com\/handle-missing-data-python\/)","d2458b7b":" Let's find the slop of regression line(m) and bais(y-intercept). we can find these using diffrente approaches. Like:- Ordinary Least Square approach and Gradient Descent approach.\n \n ## Ordinary Least Square\n Earlier in this we discussed that the relationship between independent(X) variable and dependent(y) variable is know as linear regression.\n \n **Orinary Least Sqaure** - the sum of the squares of the differences between an observation\u2019s actual and estimated values.\n![Regression image](https:\/\/d1jnx9ba8s6j9r.cloudfront.net\/blog\/wp-content\/uploads\/2018\/06\/Least-Square-Method-2-399x300.png) \nTotal error of this model is the sum of all errors of each point. ie.\n\n$$SE=\\sum_{i=1}^nDi^2$$\n\n    Di = Distance between line and ith point.\n    n = Total number of points\n## Let's start Implementation\n    1. Calculate Mean and Variance\n    2. Calculate Covariance.\n    3. Estimate Coefficients.\n    4. Make prediction\n    5. Evaluating Model with RMSE\n\n### **Calculate Mean and Variance**\nMean of numbers can be calculate as: -\n\n    mean(x) = sum(x) \/ count(x)\nVarience of numbers can be calculate as: -\n    \n    variance = sum((x - mean(x))^2)\n\n","ada668c9":"### **Make prediction**\nWe have been estimated the coefficients. Now, we can use them to make predictions follows by this eqation: -\n\n       y = mX + b\n ","39271ca6":"### **Estimate Coefficients**\nWe must estimate two coefficients in simple linear regression. which is m and b.\n\n    m = covarience() \/ varience()\n    b = mean(y) - (m*mean(X))","436a06f5":"#### ** Let's find RMSE with root_mean_sqaure_error funtion below**","fb025552":"### Analysis on train dataset","831bcce2":"#### Now, there is no any missing values","9c8d8f96":"#### There is 1 missing value available in 'y' column","1b94d409":"### **Calculate Covariance**\ncovariance can describe the relationship between two or more groups of numbers. It is a generalization of correlation. Correlation describes the relationship between two groups of numbers.\n \n we can calculate the covariance between two variables as follows: \n       \n       covariance = sum((x(i) - mean(x)) * (y(i) - mean(y)))\n","086fa26b":"### [What is box plot ?](http:\/\/whatis.techtarget.com\/definition\/box-plot)","e853db34":"### Let's check missing values","c7f48653":"## Visualization of train dataset","fd4e9c16":"## Linear Regression\nLinear Regressoin is the most basic and popular algorithm of machine learning. It is supervised machine learning algorithm which is predicted output is real values\/continuous. Or, Relationship between input\/independent variables and single output\/dependent variable. \nWe will get linear regression image look like : -\n![image](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/3\/3a\/Linear_regression.svg\/800px-Linear_regression.svg.png)\n#### Linear Regression has main two type: -\n    1. Simple Linear Regression\n    2. Multiple Linear Regression\n### Simple Linear Regression\nA linear line relationship between one input\/independent variable(X) and one output\/dependent variable(y).\n\n     y = mX + b\nWhere, y is dependent\/target variable; X is input\/independent variable; m is slop of regression line; b is y-intercept.\n### Multiple Linear Regression\nA relationship betweet two or more independent\/input variables and one continuous dependent\/output variable.\n    \n  ","ab08b8e5":"### **Evaluating Model with Root Mean Sqare Error**\nWe need to find how good is our model. There are many methods to evaluate models. Here we will use **Root Mean Sqare Error** and\n**Coefficient of Determination(R-Sqaure)**\n\n**Root Mean Sqare Error** is the square root of the sum of all errors divided by the number of values.\n![RMSE](https:\/\/cdn-images-1.medium.com\/max\/800\/1*SGBsn7WytmYYbuTgDatIpw.gif)\nHere yj^ is the ith predicted output values.\n\n**R-Square**: It determines how much of the total variation in Y (dependent variable) is explained by the variation in X (independent variable). Mathematically, it can be written as:-\n![R-sqaure](https:\/\/s3-ap-south-1.amazonaws.com\/av-blog-media\/wp-content\/uploads\/2017\/06\/05203905\/snip10.png)\nThe value of R-square is always between 0 and 1, where 0 means that the model does not model explain any variability in the target variable (y) and 1 meaning it explains full variability in the target variable.","29ac2a74":"#### **R-Sqaure with followed by r_sqaure function**"}}