{"cell_type":{"a338881e":"code","d3b16f6e":"code","755007f0":"code","5e891ea3":"code","6834f131":"code","e77f17f8":"code","c74f2fa8":"markdown","ec7597cf":"markdown","bef203f2":"markdown","76b03d30":"markdown","35ddcc64":"markdown"},"source":{"a338881e":"import torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data.sampler import *\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms, datasets\nimport torchvision\nimport numpy as np\nimport torchvision.models as models\n\ndataset_root = \"\/kaggle\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/\"\nbatch_size = 128\ntarget_size = (224,224)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","d3b16f6e":"# Get the transforms\ndef load_datasets():\n\n    # Transforms for the image.\n    transform = transforms.Compose([\n                        transforms.Grayscale(3),\n                        transforms.Resize(target_size),\n                        transforms.ToTensor(),\n                        transforms.Normalize((0.5,), (0.5,))\n                ])\n\n    # Define the image folder for each of the data set types\n    trainset = torchvision.datasets.ImageFolder(\n        root=dataset_root + 'train',\n        transform=transform\n    )\n    validset = torchvision.datasets.ImageFolder(\n        root=dataset_root + 'val',\n        transform=transform\n    )\n    testset = torchvision.datasets.ImageFolder(\n        root=dataset_root + 'test',\n        transform=transform\n    )\n\n\n    # Define indexes and get the subset random sample of each.\n    train_dataloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n    valid_dataloader = torch.utils.data.DataLoader(validset, batch_size=batch_size, shuffle=True)\n    test_dataloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)\n        \n    return train_dataloader, valid_dataloader, test_dataloader","755007f0":"\n# Transfer Learning Model for pneumonia\nclass TransferLearningModel(nn.Module):\n    def __init__(self):\n        super(TransferLearningModel, self).__init__()\n        \n        # Using VGG-16 as our transfer model\n        self.prevmodel = models.vgg16(pretrained=True)\n\n        # Not retraining the model. Instead, learning the output.\n        # Freeze all layers we do not want the gradient to update.\n        for param in self.prevmodel.parameters():\n            param.require_grad = False\n\n        # Delete last layer and add own final layer\n        self.prevmodel.features[-1] = nn.Linear(14, 2)\n\n\n    # The forward operation for the NN. Backward is auto computed.\n    def forward(self, x):\n        x = self.prevmodel(x)\n        x = F.log_softmax(x)\n        return x\n\n# Following code appears at:  https:\/\/lirnli.wordpress.com\/2017\/09\/03\/one-hot-encoding-in-pytorch\/\nclass One_Hot(nn.Module):\n    def __init__(self, depth):\n        super(One_Hot,self).__init__()\n        self.depth = depth\n        self.ones = torch.sparse.torch.eye(depth).to(device)\n    def forward(self, X_in):\n        X_in = X_in.long()\n        return self.ones.index_select(0,X_in.data)\n\n    def __repr__(self):\n        return self.__class__.__name__ + \"({})\".format(self.depth)","5e891ea3":"# This probably exists already\ndef printif(message, isPrint):\n    if isPrint:\n        print(message)\n\n# Test the model on the given dataset.\ndef test(model, dataset):\n    model.eval()\n\n    # Define loss, one_hot and variables\n    mloss = nn.CrossEntropyLoss()\n    one_hot = One_Hot(2).to(device)\n    correct = 0\n    loss_total = 0\n    \n    with torch.no_grad():\n        for data, target in dataset:\n\n            # Convert data and target to device\n            data = data.to(device)\n            target = target.to(device)\n\n            # Get network output, loss and correct\n            output = model(data)\n            loss = mloss(output, target)\n            _, pred = torch.max(output.data, dim=1)\n\n            # Update correct and loss\n            correct += pred.eq(target.data.view_as(pred)).sum().item()\n            loss_total += loss.item()\n\n    return 100. * correct \/ len(dataset.dataset), loss_total\/ len(dataset.dataset)\n\ndef train(model, trainset, validset, learning_rate=0.001, decay=0, epochs=10, valid_interval=1, log_interval=10, console_logging=True):\n\n    # Logging curves\n    loss_curve = []\n    accuracy_curve =[]\n    validation_accuracy_curve = []\n    validation_loss_curve = []\n    \n    # Loss and one hot\n    mloss = nn.CrossEntropyLoss()\n    one_hot = One_Hot(2).to(device)\n\n    # Optimizer\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=decay)\n\n    max_accuracy = -1\n    for epoch in range(1, epochs+1):\n\n        epoch_loss = 0\n        epoch_accuracy = 0\n\n        for batch_idx, (data, target) in enumerate(trainset):\n            model.train()\n\n            # Convert data to device\n            data = data.to(device)\n            target = target.to(device).long()\n\n            # Zerograd optimizer as pytorch will add gradients together if not cleared\n            optimizer.zero_grad()\n\n            # Get the output, calculate loss\n            output = model(data)\n            loss = mloss(output, target)\n\n            epoch_loss += loss.item()\n\n            # Compute the backwards update step, send it to optimizer\n            loss.backward()\n            optimizer.step()\n\n            # Get correct items for logging\n            _, pred = torch.max(output.data, dim=1)\n            #print(\"Pred: \" + str(pred))\n            #print(\"Correct: \" + str(target))\n            correct = pred.eq(target.data.view_as(pred)).sum()\n            epoch_accuracy += correct.item()\n\n            if batch_idx % log_interval == 0:\n                printif('Train Epoch: {} [{}\/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                                    epoch, batch_idx * batch_size, len(trainset.dataset),\n                                    100. * (batch_idx * batch_size) \/ len(trainset.dataset), loss.item()), console_logging)\n                \n        # Add logged items to their curves\n        loss_curve.append(epoch_loss \/ len(trainset.dataset))\n        accuracy_curve.append(epoch_accuracy \/ len(trainset.dataset))\n\n        # If we are a validation interval, validate and save best model.\n        if epoch % valid_interval == 0:\n            printif(\"Starting Validation Step:\", console_logging)\n            accuracy_val, loss_val = test(model, validset)\n            validation_accuracy_curve.append(accuracy_val)\n            validation_loss_curve.append(loss_val)\n            printif(\"Validation Dataset - Accuracy: %.2f, Loss: %.4f\" % (accuracy_val, loss_val), console_logging)\n            if accuracy_val > max_accuracy:\n                printif(\"New optimal validation value recieved!\", console_logging)\n                #torch.save(model, \"\/kaggle\/output\/model.pnt\")\n                max_accuracy = accuracy_val\n\n    return loss_curve, accuracy_curve, validation_loss_curve, validation_accuracy_curve","6834f131":"trainset, valset, testset = load_datasets()\nprint(\"Loaded train, val and test with sizes: %d, %d, %d\" % (len(trainset.dataset), len(valset.dataset), len(testset.dataset)))\nmodel = TransferLearningModel().to(device)\nprint(\"Input Size: %d\" % (target_size[0] * target_size[1]))\ntrainloss, trainaccuracy, validloss, validaccuracy = train(model, trainset, valset, epochs=10, decay=0.1, valid_interval=2, learning_rate=0.0001)","e77f17f8":"#lin = torch.load(\"\/kaggle\/output\/model.pnt\").to(device)\naccuracy, loss = test(model, testset)\nprint(\"Test Dataset - Accuracy: %.2f, Loss: %.4f\" % (accuracy, loss))\naccuracy, loss = test(model, valset)\nprint(\"Valid Dataset - Accuracy: %.2f, Loss: %.4f\" % (accuracy, loss))","c74f2fa8":"# Dataset Loading and Modifications\nThe following transformations are performed on the data:\n* Convert it to 3 channel greyscale as x-rays are black and white\n* Resize the image to the VGG-16 input size of (224, 224)\n* Convert the image to a tensor\n* Normalize with std=0.5, mean=0.5","ec7597cf":"# Models and Onehot Class","bef203f2":"# Training and Testing loops","76b03d30":"# Training and Testing the Model\n","35ddcc64":"# Objective\nThis notebook was created as a clone of my CMPUT 466 machine learning project on google colab. A majority of the primary data analysis is missing. One of the noted items was the lack of validation data which was modified. For the purpose of this kaggle notebook, the code is below to modify the data set but will not be used.\n\n```\ndef copy_files(PATH, NEWPATH, AMOUNT):\n    filelist = os.listdir(PATH)\n    for i in range(0, AMOUNT):\n        os.rename(PATH + filelist[i], NEWPATH + filelist[i])\n\n# This is to balance out the 16 validation by taking from test and train.\ntarget_path = \"\/kaggle\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/val\/\"\ntrain_path = \"\/kaggle\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/train\/\"\ntest_path = \"\/kaggle\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/test\/\"\n\ncopy_files(train_path + \"NORMAL\/\", target_path + \"NORMAL\/\", 200)\ncopy_files(train_path + \"PNEUMONIA\/\", target_path + \"PNEUMONIA\/\", 200)\ncopy_files(test_path + \"NORMAL\/\", target_path + \"NORMAL\/\", 50)\ncopy_files(test_path + \"PNEUMONIA\/\", target_path + \"PNEUMONIA\/\", 50)\n```\n\nThe goal of this notebook is to classify whether or not the user has pneumonia or not. This attempt uses transfer learning on the VGG-16 model by retraining the final layer of the model."}}