{"cell_type":{"a5557167":"code","8d50aee8":"code","47528148":"code","656b4cf8":"code","4aee8a72":"code","228031bc":"code","bafa6de2":"code","a5af3b85":"code","ae195d2d":"code","01b0b133":"code","4ed8e543":"code","c5df8b9e":"code","6da2f77d":"code","28aa257b":"code","4cb14cb4":"code","29add5f2":"code","3d81c447":"code","db38f80f":"markdown","738191e4":"markdown","db071c45":"markdown","41328ebd":"markdown","5a5bd42b":"markdown","a93d4939":"markdown","a4808edb":"markdown","a5fb9db1":"markdown","4194d46f":"markdown","35ee6dd4":"markdown","45d16edc":"markdown","99a0ea88":"markdown","af0ae770":"markdown","f502840e":"markdown","330871d8":"markdown","9a1d50e3":"markdown","e15cfea8":"markdown","e1901a71":"markdown","06db13bc":"markdown","62bfb094":"markdown","9462cd59":"markdown","2398cbf1":"markdown","1cc9bfd6":"markdown","0637ab14":"markdown","7568b6f6":"markdown","198af44c":"markdown","454f9593":"markdown","312fa193":"markdown","8584fd35":"markdown","f091b250":"markdown","5421a876":"markdown","dbc9f294":"markdown","25e669b3":"markdown","fd63ee89":"markdown","c908f2f9":"markdown"},"source":{"a5557167":"%%writefile submission.py\n\nimport random\n\ndef nash_equilibrium_agent(observation, configuration):\n    return random.randint(0, 2)","8d50aee8":"# Upgrade kaggle_environments using pip before import\n!pip install -q -U kaggle_environments","47528148":"from kaggle_environments import make","656b4cf8":"env = make(\n    \"rps\", \n    configuration={\"episodeSteps\": 1000}\n)","4aee8a72":"agent_copy_opponent_path = \"..\/input\/rock-paper-scissors-agents-comparison\/copy_opponent.py\"","228031bc":"# nash_equilibrium_agent vs copy_opponent_agent\nenv.run(\n    [\"submission.py\", agent_copy_opponent_path]\n)\n\nenv.render(mode=\"ipython\", width=500, height=400)","bafa6de2":"agent_reactionary_path = \"..\/input\/rock-paper-scissors-agents-comparison\/reactionary.py\"","a5af3b85":"# nash_equilibrium_agent vs reactionary\nenv.run(\n    [\"submission.py\", agent_reactionary_path]\n)\n\nenv.render(mode=\"ipython\", width=500, height=400)","ae195d2d":"# nash_equilibrium_agent vs nash_equilibrium_agent\nenv.run(\n    [\"submission.py\", \"submission.py\"]\n)\n\nenv.render(mode=\"ipython\", width=500, height=400)","01b0b133":"agent_rock_path = \"..\/input\/rock-paper-scissors-agents-comparison\/rock.py\"","4ed8e543":"# nash_equilibrium_agent vs rock\nenv.run(\n    [\"submission.py\", agent_rock_path]\n)\n\nenv.render(mode=\"ipython\", width=500, height=400)","c5df8b9e":"agent_scissors_path = \"..\/input\/rock-paper-scissors-agents-comparison\/scissors.py\"","6da2f77d":"# nash_equilibrium_agent vs scissors\nenv.run(\n    [\"submission.py\", agent_scissors_path]\n)\n\nenv.render(mode=\"ipython\", width=500, height=400)","28aa257b":"agent_paper_path = \"..\/input\/rock-paper-scissors-agents-comparison\/paper.py\"","4cb14cb4":"# nash_equilibrium_agent vs scissors\nenv.run(\n    [\"submission.py\", agent_paper_path]\n)\n\nenv.render(mode=\"ipython\", width=500, height=400)","29add5f2":"agent_hit_the_last_own_action_path = \"..\/input\/rock-paper-scissors-agents-comparison\/hit_the_last_own_action.py\"","3d81c447":"# nash_equilibrium_agent vs scissors\nenv.run(\n    [\"submission.py\", agent_hit_the_last_own_action_path]\n)\n\nenv.render(mode=\"ipython\", width=500, height=400)","db38f80f":"Simulating the battle between two nash_equilibrium_agent agents","738191e4":"Let's start simulating the battle nash_equilibrium_agent vs reactionary","db071c45":"<img style=\"height:400px\" src=\"https:\/\/i.imgur.com\/yjy0yCx.png\">\n<cite>The image from YouTube: <a href=\"https:\/\/www.youtube.com\/watch?v=-1GDMXoMdaY&ab_channel=CrashCourse\">Game Theory 101: Rock, Paper, Scissors<\/a><\/cite>","41328ebd":"We need to import the library for creating environments and simulating agent battles","5a5bd42b":"Let's take the agent that will hit our previous action from [Rock Paper Scissors - Agents Comparison](https:\/\/www.kaggle.com\/ihelon\/rock-paper-scissors-agents-comparison)","a93d4939":"The remaining option in order to be in equilibrium is that both players need to play a random strategy, then there is no point in changing their strategy - which is the Nash equilibrium","a4808edb":"Simulating the battle between nash_equilibrium_agent and paper","a5fb9db1":"Let's take the agent that will copy our previous action from [Rock Paper Scissors - Agents Comparison](https:\/\/www.kaggle.com\/ihelon\/rock-paper-scissors-agents-comparison)","4194d46f":"Let's take the agent that will always use Scissors from [Rock Paper Scissors - Agents Comparison](https:\/\/www.kaggle.com\/ihelon\/rock-paper-scissors-agents-comparison)","35ee6dd4":"Create a rock-paper-scissors environment (RPS), and set 1000 episodes for each simulation","45d16edc":"# Rock Paper Scissors - Nash Equilibrium Strategy\n\nExample of using Nash Equilibrium principle in Rock-Paper-Scissors game   \nCreating simple agent for the [Rock, Paper, Scissors](https:\/\/www.kaggle.com\/c\/rock-paper-scissors) competition","99a0ea88":"<a id=\"1\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>1. Nash Equilibrium Overview<center><h2>","af0ae770":"To create the agent for this competition, we must put its code in \\*.py file.   \nTo do this, we can use the [magic commands](https:\/\/ipython.readthedocs.io\/en\/stable\/interactive\/magics.html) of Jupyter Notebooks    \nOne of these commands is [writefile](https:\/\/ipython.readthedocs.io\/en\/stable\/interactive\/magics.html#cellmagic-writefile) which writes the contents of the cell to a file.","f502840e":"Let's create an agent that will generate a random number from 0 to 3 each time (Nash Equilibrium Strategy)   \n**You must also put all the necessary imports to the \\*.py file, in our example, this is a RANDOM module**","330871d8":"Let's start simulating the battle nash_equilibrium_agent vs copy_opponent_agent","9a1d50e3":"Let's take the agent that will always use Paper from [Rock Paper Scissors - Agents Comparison](https:\/\/www.kaggle.com\/ihelon\/rock-paper-scissors-agents-comparison)","e15cfea8":"If the opponent will play all the time Paper, then:\n- he wins a third of the time against Rock,\n- he ties a third of the time against Paper,\n- and he loses a third of the time against Scissors.\n\nThen he will get reward 1\/3 \\* 1 + 1\/3 \\* 0 + 1\/3 * (-1) = 0.    \n**But in this case, we can change our strategy to Scissors and win all the time.**","e1901a71":"Simulating the battle between nash_equilibrium_agent and scissors","06db13bc":"Simulating the battle between nash_equilibrium_agent and agent_hit_the_last_own_action_path","62bfb094":"If the opponent will play all the time Scissors, then:\n- he loses a third of the time against Rock,\n- he wins a third of the time against Paper,\n- and he ties a third of the time against Scissors.\n\nThen he will get reward 1\/3 \\* (-1) + 1\/3 \\* 1 + 1\/3 * (0) = 0.    \n**But in this case, we can change our strategy to Rock and win all the time.**","9462cd59":"If we played each action with equal probability 1\/3 then the opponent must do the same.   \nOtherwise if the opponent will play all the time Rock, then:\n- he ties a third of the time against Rock, \n- he loses a third of the time against Paper,\n- and he wins a third of the time against Scissors.\n\nThen he will get reward 1\/3 \\* 0 + 1\/3 \\* (-1) + 1\/3 * 1 = 0.    \n**But in this case, we can change our strategy to Paper and win all the time.**","2398cbf1":"In game theory, the Nash equilibrium, named after the mathematician John Forbes Nash Jr., is a proposed solution of a non-cooperative game involving two or more players in which each player is assumed to know the equilibrium strategies of the other players, and no player has anything to gain by changing only their own strategy. [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Nash_equilibrium#cite_note-Osborne-1)","1cc9bfd6":"Let's take the agent that will always use Rock from [Rock Paper Scissors - Agents Comparison](https:\/\/www.kaggle.com\/ihelon\/rock-paper-scissors-agents-comparison)","0637ab14":"This part is devoted to simulating and testing battles with other agents.","7568b6f6":"![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/22838\/logos\/header.png?t=2020-11-02-21-55-44)","198af44c":"<a id=\"2\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>2. Agent Code<center><h2>","454f9593":"Simulating the battle between nash_equilibrium_agent and rock","312fa193":"<img style=\"height:400px\" src=\"https:\/\/i.imgur.com\/5FYS8L4.png\">\n<cite>The image from YouTube: <a href=\"https:\/\/www.youtube.com\/watch?v=-1GDMXoMdaY&ab_channel=CrashCourse\">Game Theory 101: Rock, Paper, Scissors<\/a><\/cite>","8584fd35":"Consider Rock-Paper-Scissors awards matrix (our reward\/action is blue, the reward\/action of the opponent is red):","f091b250":"<img style=\"height:400px\" src=\"https:\/\/i.imgur.com\/aEL9IKd.png\">\n<cite>The image from YouTube: <a href=\"https:\/\/www.youtube.com\/watch?v=-1GDMXoMdaY&ab_channel=CrashCourse\">Game Theory 101: Rock, Paper, Scissors<\/a><\/cite>","5421a876":"<img style=\"height:400px\" src=\"https:\/\/i.imgur.com\/doHd5dP.png\">\n<cite>The image from YouTube: <a href=\"https:\/\/www.youtube.com\/watch?v=-1GDMXoMdaY&ab_channel=CrashCourse\">Game Theory 101: Rock, Paper, Scissors<\/a><\/cite>","dbc9f294":"Let's take the agent that will hit self last actions from [Rock Paper Scissors - Agents Comparison](https:\/\/www.kaggle.com\/ihelon\/rock-paper-scissors-agents-comparison)","25e669b3":"<a id=\"top\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:black; background:#FBE338; border:0' role=\"tab\" aria-controls=\"home\"><center>Quick Navigation<\/center><\/h3>\n\n* [1. Nash Equilibrium Overview](#1)\n* [2. Agent Code](#2)\n* [3. Battle Examples (Optional)](#3)","fd63ee89":"<a id=\"3\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>3. Battle Examples (Optional)<center><h2>","c908f2f9":"Slides and more information: [Game Theory 101: Rock, Paper, Scissors](https:\/\/www.youtube.com\/watch?v=-1GDMXoMdaY&ab_channel=CrashCourse)"}}