{"cell_type":{"9b7ef306":"code","27fbfc54":"code","5ab032c2":"code","8811a381":"code","c615e9b1":"code","0f68177e":"code","ae19bb23":"code","b2a0f3c4":"code","5ff2aec8":"code","c5814d75":"code","d6515296":"code","4ddb6524":"code","d5afa167":"code","2c8599ad":"code","557633dc":"code","4d1ce40b":"code","9c782122":"markdown","eff228d7":"markdown","5273bdae":"markdown","60f002b7":"markdown","1755b2e3":"markdown","8783cb95":"markdown"},"source":{"9b7ef306":"import os\nimport math\nimport random\nimport time\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModel\nfrom transformers import AutoConfig\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.svm import SVR\n\nimport gc\ngc.enable()","27fbfc54":"BATCH_SIZE = 32\nMAX_LEN = 248\nEVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\nROBERTA_PATH = \"..\/input\/huggingface-roberta\/roberta-base\"\nTOKENIZER_PATH = \"..\/input\/huggingface-roberta\/roberta-base\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"","5ab032c2":"test_df = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/test.csv\")\nsubmission_df = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/sample_submission.csv\")","8811a381":"tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)","c615e9b1":"class LitDataset(Dataset):\n    def __init__(self, df, inference_only=False):\n        super().__init__()\n\n        self.df = df        \n        self.inference_only = inference_only\n        self.text = df.excerpt.tolist()\n        #self.text = [text.replace(\"\\n\", \" \") for text in self.text]\n        \n        if not self.inference_only:\n            self.target = torch.tensor(df.target.values, dtype=torch.float32)        \n    \n        self.encoded = tokenizer.batch_encode_plus(\n            self.text,\n            padding = 'max_length',            \n            max_length = MAX_LEN,\n            truncation = True,\n            return_attention_mask=True\n        )        \n \n\n    def __len__(self):\n        return len(self.df)\n\n    \n    def __getitem__(self, index):        \n        input_ids = torch.tensor(self.encoded['input_ids'][index])\n        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n        \n        if self.inference_only:\n            return (input_ids, attention_mask)            \n        else:\n            target = self.target[index]\n            return (input_ids, attention_mask, target)","0f68177e":"class LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7})                       \n        \n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config)  \n            \n        self.attention = nn.Sequential(            \n            nn.Linear(768, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(768, 1)                        \n        )\n        \n\n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n\n        # There are a total of 13 layers of hidden states.\n        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n        # We take the hidden states from the last Roberta layer.\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n\n        # The number of cells is MAX_LEN.\n        # The size of the hidden state of each cell is 768 (for roberta-base).\n        # In order to condense hidden states of all cells to a context vector,\n        # we compute a weighted average of the hidden states of all cells.\n        # We compute the weight of each cell, using the attention neural network.\n        weights = self.attention(last_layer_hidden_states)\n                \n        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n        # Now we compute context_vector as the weighted average.\n        # context_vector.shape is BATCH_SIZE x 768\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n        \n        # Now we reduce the context vector to the prediction score.\n        return self.regressor(context_vector)","ae19bb23":"def predict(model, data_loader):\n    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n    model.eval()\n\n    result = np.zeros(len(data_loader.dataset))    \n    index = 0\n    \n    with torch.no_grad():\n        for batch_num, (input_ids, attention_mask) in enumerate(data_loader):\n            input_ids = input_ids.to(DEVICE)\n            attention_mask = attention_mask.to(DEVICE)\n                        \n            pred = model(input_ids, attention_mask)                        \n\n            result[index : index + pred.shape[0]] = pred.flatten().to(\"cpu\")\n            index += pred.shape[0]\n\n    return result","b2a0f3c4":"# test_dataset = LitDataset(test_df, inference_only=True)","5ff2aec8":"NUM_MODELS = 5\n\nall_predictions = np.zeros((NUM_MODELS, len(test_df)))\n\n\n\ntest_dataset = LitDataset(test_df, inference_only=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n                         drop_last=False, shuffle=False, num_workers=2)\n\nfor model_index in range(NUM_MODELS):            \n    model_path = f\"..\/input\/commonlit-roberta-0467\/model_{model_index + 1}.pth\"\n    print(f\"\\nUsing {model_path}\")\n                        \n    model = LitModel()\n    model.load_state_dict(torch.load(model_path, map_location=DEVICE))    \n    model.to(DEVICE)\n        \n    all_predictions[model_index] = predict(model, test_loader)\n            \n    del model\n    gc.collect()","c5814d75":"model1_predictions_0467 = all_predictions.mean(axis=0)","d6515296":"# # import os\n# # import gc\n# # import sys\n# # import cv2\n# # import math\n# # import time\n# # import tqdm\n# # import random\n# # import numpy as np\n# # import pandas as pd\n# # import seaborn as sns\n# # from tqdm import tqdm\n# # import matplotlib.pyplot as plt\n\n# # import warnings\n# # warnings.filterwarnings('ignore')\n\n# # from sklearn.svm import SVR\n# # from sklearn.linear_model import Ridge\n# # from sklearn.metrics import mean_squared_error\n# # from sklearn.model_selection import KFold,StratifiedKFold\n\n# # import torch\n# # import torchvision\n# # import torch.nn as nn\n# # import torch.optim as optim\n# # import torch.nn.functional as F\n# # from torch.optim import Adam, lr_scheduler\n# # from torch.utils.data import Dataset, DataLoader\n\n# # from transformers import (AutoModel, AutoTokenizer, AutoConfig,\n# #                           AutoModelForSequenceClassification)\n\n# # import plotly.express as px\n# # import plotly.graph_objs as go\n# # import plotly.figure_factory as ff\n# # from colorama import Fore, Back, Style\n# # y_ = Fore.YELLOW\n# # r_ = Fore.RED\n# # g_ = Fore.GREEN\n# # b_ = Fore.BLUE\n# # m_ = Fore.MAGENTA\n# # c_ = Fore.CYAN\n# # sr_ = Style.RESET_ALL\n\n# # train_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\n# # test_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\n# # sample = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\n\n# # num_bins = int(np.floor(1 + np.log2(len(train_data))))\n# # train_data.loc[:,'bins'] = pd.cut(train_data['target'],bins=num_bins,labels=False)\n\n# # target = train_data['target'].to_numpy()\n# # bins = train_data.bins.to_numpy()\n\n# # def rmse_score(y_true,y_pred):\n# #     return np.sqrt(mean_squared_error(y_true,y_pred))\n# # config = {\n# #     'batch_size':8,\n# #     'max_len':256,\n# #     'nfolds':5,\n# #     'seed':42,\n# # }\n\n# # def seed_everything(seed=42):\n# #     random.seed(seed)\n# #     os.environ['PYTHONASSEED'] = str(seed)\n# #     np.random.seed(seed)\n# #     torch.manual_seed(seed)\n# #     torch.cuda.manual_seed(seed)\n# #     torch.backends.cudnn.deterministic = True\n# #     torch.backends.cudnn.benchmark = True\n\n# # seed_everything(seed=config['seed'])\n\n# # class CLRPDataset(Dataset):\n# #     def __init__(self,df,tokenizer):\n# #         self.excerpt = df['excerpt'].to_numpy()\n# #         self.tokenizer = tokenizer\n    \n# #     def __getitem__(self,idx):\n# #         encode = self.tokenizer(self.excerpt[idx],return_tensors='pt',\n# #                                 max_length=config['max_len'],\n# #                                 padding='max_length',truncation=True)\n# #         return encode\n    \n# #     def __len__(self):\n# #         return len(self.excerpt)\n    \n    \n# # class Model(nn.Module): \n# #     def __init__(self):\n# #         super().__init__() \n\n\n# #         config = AutoConfig.from_pretrained('..\/input\/huggingface-roberta\/roberta-large')\n# #         self.model = AutoModel.from_pretrained('..\/input\/huggingface-roberta\/roberta-large', config=config)\n        \n        \n\n# #         self.layer_norm1 = nn.LayerNorm(1024)\n# #         self.l1 = nn.Linear(1024, 512)\n# #         self.l2 = nn.Linear(512, 1)\n\n# #         self._init_weights(self.layer_norm1)\n# #         self._init_weights(self.l1)\n# #         self._init_weights(self.l2)\n \n# #     def _init_weights(self, module):\n# #         if isinstance(module, nn.Linear):\n# #             module.weight.data.normal_(mean=0.0, std=0.02)\n# #             if module.bias is not None:\n# #                 module.bias.data.zero_()\n# #         elif isinstance(module, nn.Embedding):\n# #             module.weight.data.normal_(mean=0.0, std=0.02)\n# #             if module.padding_idx is not None:\n# #                 module.weight.data[module.padding_idx].zero_()\n# #         elif isinstance(module, nn.LayerNorm):\n# #             module.bias.data.zero_()\n# #             module.weight.data.fill_(1.0)\n    \n# #     def forward(self, input_ids, attention_mask):\n        \n# #         outputs = self.model(input_ids, attention_mask)\n# #         last_hidden_state = outputs[0]     \n# #         out = torch.mean(last_hidden_state, 1)\n# #         out = self.layer_norm1(out)\n# #         return out       \n\n# # def get_embeddings(df,path,plot_losses=True, verbose=True):\n# #     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# #     print(f\"{device} is used\")\n            \n# #     model = Model()\n# #     model.load_state_dict(torch.load(path))\n# #     model.to(device)\n# #     model.eval()\n    \n# #     tokenizer = AutoTokenizer.from_pretrained('..\/input\/huggingface-roberta\/roberta-large')\n    \n# #     ds = CLRPDataset(df,tokenizer)\n# #     dl = DataLoader(ds,\n# #                   batch_size = config[\"batch_size\"],\n# #                   shuffle=False,\n# #                   num_workers = 4,\n# #                   pin_memory=True,\n# #                   drop_last=False\n# #                  )\n        \n# #     embeddings = list()\n# #     with torch.no_grad():\n# #         for i, inputs in tqdm(enumerate(dl)):\n# #             inputs = {key:val.reshape(val.shape[0],-1).to(device) for key,val in inputs.items()}\n# #             outputs = model(**inputs)\n# #             outputs = outputs.detach().cpu().numpy()\n# #             embeddings.extend(outputs)\n# #     return np.array(embeddings)\n\n# # def get_preds_svm(X,y,X_test,RidgeReg=0,bins=bins,nfolds=10,C=8,kernel='rbf'):\n# #     scores = list()\n# #     preds = np.zeros((X_test.shape[0]))\n    \n# #     kfold = StratifiedKFold(n_splits=10,shuffle=True,random_state=config['seed'])\n# #     for k, (train_idx,valid_idx) in enumerate(kfold.split(X,bins)):\n# #         if(RidgeReg):\n# #             print(\"ridge...\")\n# #             model = Ridge(alpha=80.0)\n# #         else:\n# #             model = SVR(C=C,kernel=kernel,gamma='auto')\n# #         X_train,y_train = X[train_idx], y[train_idx]\n# #         X_valid,y_valid = X[valid_idx], y[valid_idx]\n        \n# #         model.fit(X_train,y_train)\n# #         prediction = model.predict(X_valid)\n# #         score = rmse_score(prediction,y_valid)\n# #         print(f'Fold {k} , rmse score: {score}')\n# #         scores.append(score)\n# #         preds += model.predict(X_test)\n        \n# #     print(\"mean rmse\",np.mean(scores))\n# #     return np.array(preds)\/nfolds\n\n# # train_embeddings0 =  get_embeddings(train_data,'..\/input\/pixx0459\/model0\/model0.bin')\n# # test_embeddings0 = get_embeddings(test_data,'..\/input\/pixx0459\/model0\/model0.bin')\n# # svm_preds0 = get_preds_svm(train_embeddings0,target,test_embeddings0)\n# # ridge_preds0 = get_preds_svm(train_embeddings0,target,test_embeddings0,RidgeReg=1)\n# # del train_embeddings0,test_embeddings0\n# # gc.collect()\n# # train_embeddings1 =  get_embeddings(train_data,'..\/input\/pixx0459\/model1\/model1.bin')\n# # test_embeddings1= get_embeddings(test_data,'..\/input\/pixx0459\/model1\/model1.bin')\n# # svm_preds1 = get_preds_svm(train_embeddings1,target,test_embeddings1)\n# # ridge_preds1 = get_preds_svm(train_embeddings1,target,test_embeddings1,RidgeReg=1)\n# # del train_embeddings1,test_embeddings1\n# # gc.collect()    \n# # train_embeddings2 =  get_embeddings(train_data,'..\/input\/pixx0459\/model2\/model2.bin')\n# # test_embeddings2 = get_embeddings(test_data,'..\/input\/pixx0459\/model2\/model2.bin')\n# # svm_preds2 = get_preds_svm(train_embeddings2,target,test_embeddings2)\n# # ridge_preds2 = get_preds_svm(train_embeddings2,target,test_embeddings2,RidgeReg=1)\n# # del train_embeddings2,test_embeddings2\n# # gc.collect()\n# # train_embeddings3 =  get_embeddings(train_data,'..\/input\/pixx0459\/model3\/model3.bin')\n# # test_embeddings3 = get_embeddings(test_data,'..\/input\/pixx0459\/model3\/model3.bin')\n# # svm_preds3 = get_preds_svm(train_embeddings3,target,test_embeddings3)\n# # ridge_preds3 = get_preds_svm(train_embeddings3,target,test_embeddings3,RidgeReg=1)\n# # del train_embeddings3,test_embeddings3\n# # gc.collect()\n\n# # train_embeddings4 =  get_embeddings(train_data,'..\/input\/pixx0459\/model4\/model4.bin')\n# # test_embeddings4 = get_embeddings(test_data,'..\/input\/pixx0459\/model4\/model4.bin')\n# # svm_preds4 = get_preds_svm(train_embeddings4,target,test_embeddings4)\n# # ridge_preds4 = get_preds_svm(train_embeddings4,target,test_embeddings4,RidgeReg=1)\n# # del train_embeddings4,test_embeddings4\n# # gc.collect()\n\n\n\n\n\n\n\n# # svm_preds = (svm_preds1 + svm_preds2 + svm_preds3 + svm_preds4 + svm_preds0)\/5\n\n# import numpy as np # linear algebra\n# import pandas as pd \n# import os\n# import gc\n# import sys\n# import cv2\n# import math\n# import time\n# import tqdm\n# import random\n# import numpy as np\n# import pandas as pd\n# import seaborn as sns\n# from tqdm import tqdm\n# import matplotlib.pyplot as plt\n\n# import warnings\n# warnings.filterwarnings('ignore')\n\n# from sklearn.svm import SVR\n# from sklearn.linear_model import Ridge\n# from sklearn.metrics import mean_squared_error\n# from sklearn.model_selection import KFold,StratifiedKFold\n\n# import torch\n# import torchvision\n# import torch.nn as nn\n# import torch.optim as optim\n# import torch.nn.functional as F\n# from torch.optim import Adam, lr_scheduler\n# from torch.utils.data import Dataset, DataLoader\n\n# from transformers import (AutoModel, AutoTokenizer, AutoConfig,\n#                           AutoModelForSequenceClassification)\n\n# import plotly.express as px\n# import plotly.graph_objs as go\n# import plotly.figure_factory as ff\n\n\n# from colorama import Fore, Back, Style\n# y_ = Fore.YELLOW\n# r_ = Fore.RED\n# g_ = Fore.GREEN\n# b_ = Fore.BLUE\n# m_ = Fore.MAGENTA\n# c_ = Fore.CYAN\n# sr_ = Style.RESET_ALL\n# train_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\n# test_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\n# sample = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\n\n# num_bins = int(np.floor(1 + np.log2(len(train_data))))\n# train_data.loc[:,'bins'] = pd.cut(train_data['target'],bins=num_bins,labels=False)\n\n# target = train_data['target'].to_numpy()\n# bins = train_data.bins.to_numpy()\n\n# def rmse_score(y_true,y_pred):\n#     return np.sqrt(mean_squared_error(y_true,y_pred))\n# config = {\n#     'batch_size':8,\n#     'max_len':256,\n#     'nfolds':5,\n#     'seed':42,\n# }\n\n# def seed_everything(seed=42):\n#     random.seed(seed)\n#     os.environ['PYTHONASSEED'] = str(seed)\n#     np.random.seed(seed)\n#     torch.manual_seed(seed)\n#     torch.cuda.manual_seed(seed)\n#     torch.backends.cudnn.deterministic = True\n#     torch.backends.cudnn.benchmark = True\n\n# seed_everything(seed=config['seed'])\n# class CLRPDataset(Dataset):\n#     def __init__(self,df,tokenizer):\n#         self.excerpt = df['excerpt'].to_numpy()\n#         self.tokenizer = tokenizer\n    \n#     def __getitem__(self,idx):\n#         encode = self.tokenizer(self.excerpt[idx],return_tensors='pt',\n#                                 max_length=config['max_len'],\n#                                 padding='max_length',truncation=True)\n#         return encode\n    \n#     def __len__(self):\n#         return len(self.excerpt)\n    \n# class Model(nn.Module): \n#     def __init__(self):\n#         super().__init__() \n\n\n#         config = AutoConfig.from_pretrained('..\/input\/huggingface-roberta\/roberta-large')\n#         self.model = AutoModel.from_pretrained('..\/input\/huggingface-roberta\/roberta-large', config=config)\n#         self.drop_out1 = nn.Dropout(0)\n#         self.drop_out2 = nn.Dropout(0.1)\n#         self.layer_norm = nn.LayerNorm(1024)\n#         self.layer_norm1 = nn.LayerNorm(1024)\n#         self.l1 = nn.Linear(1024, 512)\n#         self.l2 = nn.Linear(512, 1)\n\n#         self._init_weights(self.layer_norm1)\n#         self._init_weights(self.l1)\n#         self._init_weights(self.l2)\n \n#     def _init_weights(self, module):\n#         if isinstance(module, nn.Linear):\n#             module.weight.data.normal_(mean=0.0, std=0.02)\n#             if module.bias is not None:\n#                 module.bias.data.zero_()\n#         elif isinstance(module, nn.Embedding):\n#             module.weight.data.normal_(mean=0.0, std=0.02)\n#             if module.padding_idx is not None:\n#                 module.weight.data[module.padding_idx].zero_()\n#         elif isinstance(module, nn.LayerNorm):\n#             module.bias.data.zero_()\n#             module.weight.data.fill_(1.0) \n#     def forward(self, input_ids, attention_mask, labels=None):\n        \n#         outputs = self.model(input_ids, attention_mask)\n#         last_hidden_state = outputs[0]\n#         input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n#         sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n#         sum_mask = input_mask_expanded.sum(1)\n#         sum_mask = torch.clamp(sum_mask, min=1e-9)\n#         mean_embeddings = sum_embeddings \/ sum_mask\n#         out= self.layer_norm(mean_embeddings)\n        \n       \n\n#         return out    \n# def get_embeddings(df,path,plot_losses=True, verbose=True):\n#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n#     print(f\"{device} is used\")\n            \n#     model = Model()\n#     model.load_state_dict(torch.load(path))\n#     model.to(device)\n#     model.eval()\n    \n#     tokenizer = AutoTokenizer.from_pretrained('..\/input\/huggingface-roberta\/roberta-large')\n    \n#     ds = CLRPDataset(df,tokenizer)\n#     dl = DataLoader(ds,\n#                   batch_size = config[\"batch_size\"],\n#                   shuffle=False,\n#                   num_workers = 4,\n#                   pin_memory=True,\n#                   drop_last=False\n#                  )\n        \n#     embeddings = list()\n#     with torch.no_grad():\n#         for i, inputs in tqdm(enumerate(dl)):\n#             inputs = {key:val.reshape(val.shape[0],-1).to(device) for key,val in inputs.items()}\n#             outputs = model(**inputs)\n#             outputs = outputs.detach().cpu().numpy()\n#             embeddings.extend(outputs)\n#     return np.array(embeddings)\n# def get_preds_svm(X,y,X_test,RidgeReg=0,bins=bins,nfolds=10,C=8,kernel='rbf'):\n#     scores = list()\n#     preds = np.zeros((X_test.shape[0]))\n    \n#     kfold = StratifiedKFold(n_splits=10,shuffle=True,random_state=config['seed'])\n#     for k, (train_idx,valid_idx) in enumerate(kfold.split(X,bins)):\n#         if(RidgeReg):\n#             print(\"ridge...\")\n#             model = Ridge(alpha=80.0)\n#         else:\n#             model = SVR(C=C,kernel=kernel,gamma='auto')\n#         X_train,y_train = X[train_idx], y[train_idx]\n#         X_valid,y_valid = X[valid_idx], y[valid_idx]\n        \n#         model.fit(X_train,y_train)\n#         prediction = model.predict(X_valid)\n#         score = rmse_score(prediction,y_valid)\n#         print(f'Fold {k} , rmse score: {score}')\n#         scores.append(score)\n#         preds += model.predict(X_test)\n        \n#     print(\"mean rmse\",np.mean(scores))\n#     return np.array(preds)\/nfolds\n# # train_embeddings0 =  get_embeddings(train_data,'..\/input\/nomlmqudiaodrop\/model0\/model0.bin')\n# # test_embeddings0 = get_embeddings(test_data,'..\/input\/nomlmqudiaodrop\/model0\/model0.bin')\n# # svm_preds0 = get_preds_svm(train_embeddings0,target,test_embeddings0)\n# # ridge_preds0 = get_preds_svm(train_embeddings0,target,test_embeddings0,RidgeReg=1)\n# # del train_embeddings0,test_embeddings0\n# # gc.collect()\n# # train_embeddings1 =  get_embeddings(train_data,'..\/input\/nomlmqudiaodrop\/model1\/model1.bin')\n# # test_embeddings1= get_embeddings(test_data,'..\/input\/nomlmqudiaodrop\/model1\/model1.bin')\n# # svm_preds1 = get_preds_svm(train_embeddings1,target,test_embeddings1)\n# # ridge_preds1 = get_preds_svm(train_embeddings1,target,test_embeddings1,RidgeReg=1)\n# # del train_embeddings1,test_embeddings1\n# # gc.collect()    \n# # train_embeddings2 =  get_embeddings(train_data,'..\/input\/nomlmqudiaodrop\/model2\/model2.bin')\n# # test_embeddings2 = get_embeddings(test_data,'..\/input\/nomlmqudiaodrop\/model2\/model2.bin')\n# # svm_preds2 = get_preds_svm(train_embeddings2,target,test_embeddings2)\n# # ridge_preds2 = get_preds_svm(train_embeddings2,target,test_embeddings2,RidgeReg=1)\n# # del train_embeddings2,test_embeddings2\n# # gc.collect()\n\n\n\n# train_embeddings0 =  get_embeddings(train_data,'..\/input\/nomlmean\/model0\/model0.bin')\n# test_embeddings0 = get_embeddings(test_data,'..\/input\/nomlmean\/model0\/model0.bin')\n# svm0 = get_preds_svm(train_embeddings0,target,test_embeddings0)\n# ridge_preds0 = get_preds_svm(train_embeddings0,target,test_embeddings0,RidgeReg=1)\n# del train_embeddings0,test_embeddings0\n# gc.collect()\n# train_embeddings1 =  get_embeddings(train_data,'..\/input\/nomlmean\/model1\/model1.bin')\n# test_embeddings1= get_embeddings(test_data,'..\/input\/nomlmean\/model1\/model1.bin')\n# svm1 = get_preds_svm(train_embeddings1,target,test_embeddings1)\n# ridge_preds1 = get_preds_svm(train_embeddings1,target,test_embeddings1,RidgeReg=1)\n# del train_embeddings1,test_embeddings1\n# gc.collect()    \n# train_embeddings2 =  get_embeddings(train_data,'..\/input\/nomlmean\/model2\/model2.bin')\n# test_embeddings2 = get_embeddings(test_data,'..\/input\/nomlmean\/model2\/model2.bin')\n# svm2 = get_preds_svm(train_embeddings2,target,test_embeddings2)\n# ridge_preds2 = get_preds_svm(train_embeddings2,target,test_embeddings2,RidgeReg=1)\n# del train_embeddings2,test_embeddings2\n# gc.collect()\n\n\n# train_embeddings4 =  get_embeddings(train_data,'..\/input\/nomlmean\/model4\/model4.bin')\n# test_embeddings4 = get_embeddings(test_data,'..\/input\/nomlmean\/model4\/model4.bin')\n# svm4 = get_preds_svm(train_embeddings4,target,test_embeddings4)\n# ridge_preds4 = get_preds_svm(train_embeddings4,target,test_embeddings4,RidgeReg=1)\n# del train_embeddings4,test_embeddings4\n# gc.collect()\n\n\n\n\n\n\n\n\n\n# svm_preds_0_465 = (svm0+svm1+svm2+svm4)\/4","4ddb6524":"\nimport os\nfrom pathlib import Path\nin_folder_path = Path('..\/input\/clrpfinetunerobertalarge')\nscripts_dir = Path(in_folder_path \/ 'scripts')\n\n\nos.chdir(scripts_dir)\nexec(Path(\"imports.py\").read_text())\nexec(Path(\"config.py\").read_text())\nexec(Path(\"dataset.py\").read_text())\nexec(Path(\"model.py\").read_text())\nos.chdir('\/kaggle\/working')\n\n\ntest_df = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/test.csv\")\n\ntokenizer = torch.load('..\/input\/tokenizers\/roberta-tokenizer.pt')\nmodels_folder_path = Path(in_folder_path \/ 'models')\nmodels_preds = []\nn_models = 5\n\nfor model_num in range(n_models):\n    print(f'Inference#{model_num+1}\/{n_models}')\n    test_ds = CLRPDataset(data=test_df, tokenizer=tokenizer, max_len=Config.max_len, is_test=True)\n    test_sampler = SequentialSampler(test_ds)\n    test_dataloader = DataLoader(test_ds, sampler = test_sampler, batch_size=Config.batch_size)\n    model = torch.load(models_folder_path \/ f'best_model_{model_num}.pt').to(Config.device)\n\n    all_preds = []\n    model.eval()\n\n    for step,batch in enumerate(test_dataloader):\n        sent_id, mask = batch['input_ids'].to(Config.device), batch['attention_mask'].to(Config.device)\n        with torch.no_grad():\n            preds = model(sent_id, mask)\n            all_preds += preds.flatten().cpu().tolist()\n    \n    models_preds.append(all_preds)\n    \nmodels_preds = np.array(models_preds)\nprint(models_preds.shape)\nprint(models_preds)\nall_preds_0_464 = models_preds.mean(axis=0)\n# result_df = pd.DataFrame(\n#     {\n#         'id': test_df.id,\n#         'target': all_preds\n#     })\n\n\n# result_df.to_csv('submission.csv', index=False)\n# result_df.head(10)\n\n","d5afa167":"import os\nimport numpy as np\nimport pandas as pd\nimport random\n\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup, logging\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, TensorDataset, SequentialSampler, RandomSampler, DataLoader\n\nfrom tqdm.notebook import tqdm\n\nimport gc; gc.enable()\nfrom IPython.display import clear_output\n\nfrom sklearn.model_selection import StratifiedKFold\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style('whitegrid')\nlogging.set_verbosity_error()\n\nINPUT_DIR = '..\/input\/commonlitreadabilityprize'\nMODEL_DIR = '..\/input\/roberta-transformers-pytorch\/roberta-large'\nCHECKPOINT_DIR = '..\/input\/clrp-mean-pooling\/'\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nTOKENIZER = AutoTokenizer.from_pretrained(MODEL_DIR)\nMAX_LENGTH = 300\nMAX_LEN = 300\nTEST_BATCH_SIZE = 1\nHIDDEN_SIZE = 1024\nBATCH_SIZE = 8\n\nNUM_FOLDS = 5\nSEEDS = [113, 71]\n\n# Utility functions\nSEED =42\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \n    \nseed_everything(SEED)\n\ntest = pd.read_csv(os.path.join(INPUT_DIR, 'test.csv'))\ntest.head(2)\n\nclass MeanPoolingModel(nn.Module):\n    \n    def __init__(self, model_name):\n        super().__init__()\n        \n        config = AutoConfig.from_pretrained(model_name)\n        self.model = AutoModel.from_pretrained(model_name, config=config)\n        self.linear = nn.Linear(HIDDEN_SIZE, 1)\n        #self.layer_norm = nn.LayerNorm(HIDDEN_SIZE)\n        self.loss = nn.MSELoss()\n        \n    def forward(self, input_ids, attention_mask, labels=None):\n        \n        outputs = self.model(input_ids, attention_mask)\n        last_hidden_state = outputs[0]\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings \/ sum_mask\n        #norm_mean_embeddings = self.layer_norm(mean_embeddings)\n        logits = self.linear(mean_embeddings)\n        \n        preds = logits.squeeze(-1).squeeze(-1)\n        \n        if labels is not None:\n            loss = self.loss(preds.view(-1).float(), labels.view(-1).float())\n            return loss\n        else:\n            return preds\n        \n        \ndef get_test_loader(data):\n\n    x_test = data.excerpt.tolist()\n    \n    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n\n    encoded_test = tokenizer.batch_encode_plus(\n        x_test, \n        add_special_tokens=True, \n        return_attention_mask=True, \n        padding='max_length', \n        truncation=True,\n        max_length=MAX_LENGTH, \n        return_tensors='pt'\n    )\n\n    dataset_test = TensorDataset(\n        encoded_test['input_ids'],\n        encoded_test['attention_mask']\n    )\n\n    dataloader_test = DataLoader(\n        dataset_test,\n        sampler = SequentialSampler(dataset_test),\n        batch_size=TEST_BATCH_SIZE\n    )\n    \n    return dataloader_test\nclass CLRPDataset(Dataset):\n    def __init__(self, texts, tokenizer):\n        self.texts = texts\n        self.tokenizer = tokenizer\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        encode = self.tokenizer(\n            self.texts[idx],\n            padding='max_length',\n            max_length=MAX_LEN,\n            truncation=True,\n            add_special_tokens=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        ) \n        return encode\n\ntest_dataloader = get_test_loader(test)\ndef predict(df, model):\n    \n    ds = CLRPDataset(df.excerpt.tolist(), TOKENIZER)\n    dl = DataLoader(\n        ds,\n        batch_size=BATCH_SIZE,\n        shuffle=False,\n        pin_memory=False\n    )\n    \n    model.to(DEVICE)\n    model.eval()\n    model.zero_grad()\n    \n    predictions = []\n    for batch in tqdm(dl):\n        inputs = {key:val.reshape(val.shape[0], -1).to(DEVICE) for key,val in batch.items()}\n        outputs = model(**inputs)\n        predictions.extend(outputs.detach().cpu().numpy().ravel())\n        \n    return predictions\n\nall_predictions = []\nfor seed in SEEDS:\n    \n    fold_predictions = []\n    \n    for fold in tqdm(range(NUM_FOLDS)):\n        model_path = f\"model_{seed + 1}_{fold + 1}.pth\"\n        \n        print(f\"\\nUsing {model_path}\")\n        \n        model_path = CHECKPOINT_DIR + f\"model_{seed + 1}_{fold + 1}.pth\"\n        model = MeanPoolingModel(MODEL_DIR)\n        model.load_state_dict(torch.load(model_path))\n        \n        predictions = predict(test, model)\n        fold_predictions.append(predictions)\n        del model \n        gc.collect()\n    all_predictions.append(np.mean(fold_predictions, axis=0).tolist())\n    \n    \nfold_predictions_ = []\n\nfor fold in tqdm(range(NUM_FOLDS)):\n    model_path = f\"..\/input\/mean-pool-roberta-large-v\/mean_pool_roberta_large\/model_{fold}.pth\"\n\n    print(f\"\\nUsing {model_path}\")\n\n    model_path = CHECKPOINT_DIR + f\"model_{seed + 1}_{fold + 1}.pth\"\n    model = MeanPoolingModel(MODEL_DIR)\n    model.load_state_dict(torch.load(model_path))\n\n    predictions = predict(test, model)\n    fold_predictions_.append(predictions)\n    del model \n    gc.collect()\n\n\nmean_pool_0_464 = np.mean(fold_predictions, axis=0)    \nmodel_predictions_0_466 = np.mean(all_predictions,axis=0)","2c8599ad":"# predictions = model1_predictions_0467 *0.3 + (svm_preds_0_465 * 0.1+model_predictions_0_466 * 0.1 + all_preds_0_464 * 0.5)*1.04\npredictions = model1_predictions_0467 *0.3 + (mean_pool_0_464 * 0.5 + all_preds_0_464 * 0.5)*0.705","557633dc":"# 0.458\u5c0f = model1_predictions_0467 *0.3 + svm_preds_0_465 * 0.1+model_predictions_0_466 * 0.1 + all_preds_0_464 * 0.5\n# 0.458\u5927 = model1_predictions_0467 *0.3+model_predictions_0_466 * 0.2 + all_preds_0_464 * 0.5\n# 0.459 = model1_predictions_0467 * 0.4 + all_preds_0_464 * 0.6\n# 0.459 = model1_predictions_0467 *0.2 + svm_preds_0_465 * 0.2+model_predictions_0_466 * 0.2 + all_preds_0_464 * 0.4\n# 0.462 = svm_preds_0_465 * 0.3+model_predictions_0_466 * 0.2 + all_preds_0_464 * 0.5\n#\n#\n#","4d1ce40b":"submission_df.target = predictions\nprint(submission_df)\nsubmission_df.to_csv(\"submission.csv\", index=False)","9c782122":"# Model\nThe model is inspired by the one from [Maunish](https:\/\/www.kaggle.com\/maunish\/clrp-roberta-svm).","eff228d7":"# Model 1","5273bdae":"# Inference","60f002b7":"# model 3","1755b2e3":"# Dataset","8783cb95":"# Model 2\nImported from [https:\/\/www.kaggle.com\/rhtsingh\/commonlit-readability-prize-roberta-torch-infer-3](https:\/\/www.kaggle.com\/rhtsingh\/commonlit-readability-prize-roberta-torch-infer-3)"}}