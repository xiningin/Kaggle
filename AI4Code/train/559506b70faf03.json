{"cell_type":{"080e6e83":"code","1dbd4b52":"code","1f34964d":"code","3c60dabc":"code","32e529bc":"code","3d3dfee9":"code","823f92f2":"code","14d3d409":"code","29418be5":"code","5f6ced80":"code","e1f8ee65":"code","1a3fd5e3":"code","920b93a0":"code","5249fc69":"code","9f17ec76":"code","72f29d33":"code","5866dd5a":"code","14264ac2":"code","693ab04d":"code","618cbefc":"code","ea00db48":"code","977fb8f5":"code","ee1d1b2a":"code","b3016d84":"code","c566adc7":"code","47202495":"code","61355d93":"markdown","018e6ee8":"markdown","47618b3f":"markdown","40d9c3d0":"markdown","e3b0f1c3":"markdown","2837c12a":"markdown","caee7cfa":"markdown","c9a41c9a":"markdown","8714dc6d":"markdown","fd1ec3df":"markdown","89346b99":"markdown","ebbb786a":"markdown","e587b5d7":"markdown","1853af24":"markdown","b6050aae":"markdown","ffe1f5e8":"markdown","24783fce":"markdown","127ad984":"markdown","c32e6e72":"markdown"},"source":{"080e6e83":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","1dbd4b52":"# Set up code checking\nimport os","1f34964d":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\npath = '\/kaggle\/input\/house-prices-advanced-regression-techniques\/'\n\n# Read the data\nX = pd.read_csv(path + 'train.csv', index_col='Id') \n#X_test = pd.read_csv(path + 'test.csv', index_col='Id')\n#X_test_full = pd.read_csv(path + 'test.csv', index_col='Id')\n\n# Remove rows with missing target, separate target from predictors\nX.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X.SalePrice\nX.drop(['SalePrice'], axis=1, inplace=True)\n\n# To keep things simple, we'll drop columns with missing values\ncols_with_missing = [col for col in X.columns if X[col].isnull().any()] \nX.drop(cols_with_missing, axis=1, inplace=True)\n# TODO: \n# To keep things simple, we'll use only numerical predictors\n#X_test = X_test_full.select_dtypes(exclude=['object'])\n\n#cols_with_missing_test = [col for col in X_test.columns if X_test[col].isnull().any()] \n#X_test.drop(cols_with_missing_test, axis=1, inplace=True)\n\n# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y,\n                                                      train_size=0.8, test_size=0.2,\n                                                      random_state=0)","3c60dabc":"X_train.head()","32e529bc":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n# function for comparing different approaches\ndef score_dataset(X_train, X_valid, y_train, y_valid):\n    model = RandomForestRegressor(n_estimators=100, random_state=0)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    return mean_absolute_error(y_valid, preds)","3d3dfee9":"# Fill in the lines below: drop columns in training and validation data\n\ndrop_X_train = X_train.select_dtypes(exclude=['object'])\ndrop_X_valid = X_valid.select_dtypes(exclude=['object'])\n","823f92f2":"print(\"MAE from Approach 1 (Drop categorical variables):\")\nprint(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))","14d3d409":"print(\"Unique values in 'Condition2' column in training data:\", X_train['Condition2'].unique())\nprint(\"\\nUnique values in 'Condition2' column in validation data:\", X_valid['Condition2'].unique())","29418be5":"# All categorical columns\nobject_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n\n# Columns that can be safely label encoded\ngood_label_cols = [col for col in object_cols if \n                   set(X_train[col]) == set(X_valid[col])]\n        \n# Problematic columns that will be dropped from the dataset\nbad_label_cols = list(set(object_cols)-set(good_label_cols))\n        \nprint('Categorical columns that will be label encoded:', good_label_cols)\nprint('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)","5f6ced80":"from sklearn.preprocessing import LabelEncoder\n\n# Drop categorical columns that will not be encoded\nlabel_X_train = X_train.drop(bad_label_cols, axis=1)\nlabel_X_valid = X_valid.drop(bad_label_cols, axis=1)\n\n# Apply label encoder to each column with categorical data\nlabel_encoder = LabelEncoder()\nfor col in good_label_cols:\n    label_X_train[col] = label_encoder.fit_transform(X_train[col])\n    label_X_valid[col] = label_encoder.transform(X_valid[col])\n    ","e1f8ee65":"print(\"MAE from Approach 2 (Label Encoding):\") \nprint(score_dataset(label_X_train, label_X_valid, y_train, y_valid))","1a3fd5e3":"# Get number of unique entries in each column with categorical data\nobject_nunique = list(map(lambda col: X_train[col].nunique(), object_cols))\nd = dict(zip(object_cols, object_nunique))\n\n# Print number of unique entries by column, in ascending order\nsorted(d.items(), key=lambda x: x[1])","920b93a0":"# Fill in the line below: How many categorical variables in the training data\n# have cardinality greater than 10?\nhigh_cardinality_numcols = 3\n\n# Fill in the line below: How many columns are needed to one-hot encode the \n# 'Neighborhood' variable in the training data?\nnum_cols_neighborhood = 25","5249fc69":"# Fill in the line below: How many entries are added to the dataset by \n# replacing the column with a one-hot encoding?\nOH_entries_added = 1e4*100 - 1e4\n\n# Fill in the line below: How many entries are added to the dataset by\n# replacing the column with a label encoding?\nlabel_entries_added = 0\n","9f17ec76":"# Columns that will be one-hot encoded\nlow_cardinality_cols = [col for col in object_cols if X_train[col].nunique() < 10]\n\n# Columns that will be dropped from the dataset\nhigh_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))\n\nprint('Categorical columns that will be one-hot encoded:', low_cardinality_cols)\nprint('\\nCategorical columns that will be dropped from the dataset:', high_cardinality_cols)\n\n#low_cardinality_cols_test = [col for col in object_cols if X_test[col].nunique() < 10]\n#high_cardinality_cols_test = list(set(object_cols)-set(low_cardinality_cols_test))\n#print('\\nCategorical columns that will be one-hot encoded:', low_cardinality_cols_test)\n#print('\\nCategorical columns that will be dropped from the dataset:', high_cardinality_cols_test)","72f29d33":"from sklearn.preprocessing import OneHotEncoder\n\n# Use as many lines of code as you need!\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[low_cardinality_cols]))\nOH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[low_cardinality_cols]))\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = X_train.index\nOH_cols_valid.index = X_valid.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = X_train.drop(object_cols, axis=1)\nnum_X_valid = X_valid.drop(object_cols, axis=1)\n\n# Add one-hot encoded columns to numerical features\nOH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\nOH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n","5866dd5a":"print(\"MAE from Approach 3 (One-Hot Encoding):\") \nprint(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))","14264ac2":"# (Optional) Your code here y_train, y_valid\n#from sklearn.impute import SimpleImputer","693ab04d":"final_imputer = SimpleImputer(strategy='median')\nfinal_X_train = pd.DataFrame(final_imputer.fit_transform(OH_X_train))\nfinal_X_valid = pd.DataFrame(final_imputer.transform(OH_X_valid))","618cbefc":"# Define and fit model\nmodel = RandomForestRegressor(n_estimators=100, random_state=0)\nmodel.fit(final_X_train, y_train)\n\n# Get validation predictions and MAE\npreds_valid = model.predict(final_X_valid)\nprint(\"MAE (Your approach):\")\nprint(mean_absolute_error(y_valid, preds_valid))","ea00db48":"# Fill in the line below: preprocess test data\nfinal_X_test = pd.DataFrame(final_imputer.transform(X_test))","977fb8f5":"# Fill in the line below: get test predictions\npreds_test = model.predict(final_X_test)","ee1d1b2a":"# Save test predictions to file\noutput = pd.DataFrame({'Id': X_test.index,\n                       'SalePrice': preds_test})\noutput.to_csv('submission.csv', index=False)\n","b3016d84":"print(final_X_valid.index)","c566adc7":"print(OH_X_valid.index)","47202495":"# path to file you will use for predictions\ntest_data_path = path + 'test.csv'\n\n# read test data file using pandas\ntest_data = pd.read_csv(test_data_path)\n\n# create test_X which comes from test_data but includes only the columns you used for prediction.\n# The list of columns is stored in a variable called features\n#features = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\n#test_X = test_data[features]\n\n# make predictions which we will submit. \ntest_preds = model.predict(test_data)\n\n# The lines below shows how to save predictions in format used for competition scoring\n# Just uncomment them.\n\noutput = pd.DataFrame({'Id': test_data.Id,\n                       'SalePrice': test_preds})\noutput.to_csv('submission.csv', index=False)","61355d93":"If you now write code to: \n- fit a label encoder to the training data, and then \n- use it to transform both the training and validation data, \n\nyou'll get an error.  Can you see why this is the case?  (_You'll need  to use the above output to answer this question._)","018e6ee8":"Use the next code cell to print the first five rows of the data.","47618b3f":"In this exercise, you will work with data from the [Housing Prices Competition for Kaggle Learn Users](https:\/\/www.kaggle.com\/c\/home-data-for-ml-course). \n\n![Ames Housing dataset image](https:\/\/i.imgur.com\/lTJVG4e.png)\n\nRun the next code cell without changes to load the training and validation sets in `X_train`, `X_valid`, `y_train`, and `y_valid`.  The test set is loaded in `X_test`.","40d9c3d0":"**[Intermediate Machine Learning Home Page](https:\/\/www.kaggle.com\/learn\/intermediate-machine-learning)**\n\n---\n","e3b0f1c3":"Run the next code cell to get the MAE for this approach.","2837c12a":"Run the next code cell to get the MAE for this approach.","caee7cfa":"Run the next code cell to get the MAE for this approach.","c9a41c9a":"Notice that the dataset contains both numerical and categorical variables.  You'll need to encode the categorical data before training a model.\n\nTo compare different models, you'll use the same `score_dataset()` function from the tutorial.  This function reports the [mean absolute error](https:\/\/en.wikipedia.org\/wiki\/Mean_absolute_error) (MAE) from a random forest model.","8714dc6d":"# Step 4: One-hot encoding\n\nIn this step, you'll experiment with one-hot encoding.  But, instead of encoding all of the categorical variables in the dataset, you'll only create a one-hot encoding for columns with cardinality less than 10.\n\nRun the code cell below without changes to set `low_cardinality_cols` to a Python list containing the columns that will be one-hot encoded.  Likewise, `high_cardinality_cols` contains a list of categorical columns that will be dropped from the dataset.","fd1ec3df":"The output above shows, for each column with categorical data, the number of unique values in the column.  For instance, the `'Street'` column in the training data has two unique values: `'Grvl'` and `'Pave'`, corresponding to a gravel road and a paved road, respectively.\n\nWe refer to the number of unique entries of a categorical variable as the **cardinality** of that categorical variable.  For instance, the `'Street'` variable has cardinality 2.\n\nUse the output above to answer the questions below.","89346b99":"By encoding **categorical variables**, you'll obtain your best results thus far!\n\n# Setup\n\nThe questions below will give you feedback on your work. Run the following cell to set up the feedback system.","ebbb786a":"# Step 5: Generate test predictions and submit your results\n\nAfter you complete Step 4, if you'd like to use what you've learned to submit your results to the leaderboard, you'll need to preprocess the test data before generating predictions.\n\n**This step is completely optional, and you do not need to submit results to the leaderboard to successfully complete the exercise.**\n\nCheck out the previous exercise if you need help with remembering how to [join the competition](https:\/\/www.kaggle.com\/c\/home-data-for-ml-course) or save your results to CSV.  Once you have generated a file with your results, follow the instructions below:\n- Begin by clicking on the blue **COMMIT** button in the top right corner.  This will generate a pop-up window.  \n- After your code has finished running, click on the blue **Open Version** button in the top right of the pop-up window.  This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n- Click on the **Output** tab on the left of the screen.  Then, click on the **Submit to Competition** button to submit your results to the leaderboard.\n- If you want to keep working to improve your performance, select the blue **Edit** button in the top right of the screen. Then you can change your model and repeat the process.","e587b5d7":"# Step 1: Drop columns with categorical data\n\nYou'll get started with the most straightforward approach.  Use the code cell below to preprocess the data in `X_train` and `X_valid` to remove columns with categorical data.  Set the preprocessed DataFrames to `drop_X_train` and `drop_X_valid`, respectively.  ","1853af24":"# Step 3: Investigating cardinality\n\nSo far, you've tried two different approaches to dealing with categorical variables.  And, you've seen that encoding categorical data yields better results than removing columns from the dataset.\n\nSoon, you'll try one-hot encoding.  Before then, there's one additional topic we need to cover.  Begin by running the next code cell without changes.  ","b6050aae":"Use the next code cell to label encode the data in `X_train` and `X_valid`.  Set the preprocessed DataFrames to `label_X_train` and `label_X_valid`, respectively.  \n- We have provided code below to drop the categorical columns in `bad_label_cols` from the dataset. \n- You should label encode the categorical columns in `good_label_cols`.  ","ffe1f5e8":"Use the next code cell to one-hot encode the data in `X_train` and `X_valid`.  Set the preprocessed DataFrames to `OH_X_train` and `OH_X_valid`, respectively.  \n- The full list of categorical columns in the dataset can be found in the Python list `object_cols`.\n- You should only one-hot encode the categorical columns in `low_cardinality_cols`.  All other categorical columns should be dropped from the dataset. ","24783fce":"For large datasets with many rows, one-hot encoding can greatly expand the size of the dataset.  For this reason, we typically will only one-hot encode columns with relatively low cardinality.  Then, high cardinality columns can either be dropped from the dataset, or we can use label encoding.\n\nAs an example, consider a dataset with 10,000 rows, and containing one categorical column with 100 unique entries.  \n- If this column is replaced with the corresponding one-hot encoding, how many entries are added to the dataset?  \n- If we instead replace the column with the label encoding, how many entries are added?  \n\nUse your answers to fill in the lines below.","127ad984":"# Step 2: Label encoding\n\nBefore jumping into label encoding, we'll investigate the dataset.  Specifically, we'll look at the `'Condition2'` column.  The code cell below prints the unique entries in both the training and validation sets.","c32e6e72":"This is a common problem that you'll encounter with real-world data, and there are many approaches to fixing this issue.  For instance, you can write a custom label encoder to deal with new categories.  The simplest approach, however, is to drop the problematic categorical columns.  \n\nRun the code cell below to save the problematic columns to a Python list `bad_label_cols`.  Likewise, columns that can be safely label encoded are stored in `good_label_cols`."}}