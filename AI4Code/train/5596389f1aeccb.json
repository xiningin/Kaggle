{"cell_type":{"dd4f5868":"code","fec65f29":"code","a309e196":"code","4155e0d6":"code","ac83f1cc":"code","a248423b":"code","99d04885":"code","56e10fb8":"code","2162f560":"code","9ca79f4c":"code","8764f9e1":"code","298fcb49":"code","ef06498a":"code","8319bd30":"code","63b9e238":"code","9df1218b":"code","86ee756d":"code","f8167cf3":"code","3a310ab6":"code","8d3ef572":"code","08bc3d0a":"code","82fd4c09":"code","295394ab":"code","a6f01d59":"code","9a6d2087":"code","6cf9c884":"code","5ddbffb2":"code","04ff46f9":"code","a85a55c6":"code","5dc22292":"code","6c96638b":"code","bed7fc38":"code","0d06f51a":"code","5e43559e":"code","7b09797f":"code","fc042375":"code","4ccb8103":"code","8909d04c":"markdown"},"source":{"dd4f5868":"# Supress Warnings\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fec65f29":"heart_df = pd.read_csv('\/kaggle\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')","a309e196":"heart_df.info()","4155e0d6":"heart_df.describe().transpose()","ac83f1cc":"col = list(heart_df.columns)\nheart_df.head()","a248423b":"sns.pairplot(vars=col, diag_kind = 'kde', data = heart_df, hue= 'DEATH_EVENT')\nplt.show()","99d04885":"binary_vars = ['anaemia','diabetes','high_blood_pressure','sex','smoking','DEATH_EVENT']\nplt.figure(figsize=(8,24))\nfor i in enumerate(binary_vars):\n    plt.subplot(3,2,i[0]+1)\n    sns.countplot(heart_df[i[1]],hue='DEATH_EVENT', data = heart_df)\nplt.show()","56e10fb8":"cont_vars = ['age','creatinine_phosphokinase','ejection_fraction','platelets','serum_creatinine','serum_sodium','time']\nplt.figure(figsize=(10,24))\nfor i in enumerate(cont_vars):\n    plt.subplot(4,2,i[0]+1)\n    sns.boxplot(y=heart_df[i[1]], data = heart_df)\nplt.show()\n\n# plt.figure(figsize=(8,24))\n# for i in enumerate(cont_vars):\n#     plt.subplot(4,2,i[0]+1)\n#     sns.boxplot(y=i[1],x='DEATH_EVENT', data = heart_df)\n# plt.show()","2162f560":"# Data Preparation for Modeling\n# outlier treatment\ncapping =['creatinine_phosphokinase','ejection_fraction','platelets','serum_creatinine','serum_sodium']\n\nfor i in capping:\n    Q3  = heart_df[i].quantile(0.75)\n    Q1  = heart_df[i].quantile(0.25)\n    IQR = Q3-Q1\n    UW = Q3 + 1.5*IQR\n    LW = Q1 - 1.5*IQR\n    heart_df[i]= heart_df[i].apply(lambda x: x if x<=UW else UW)\n    heart_df[i] = heart_df[i].apply(lambda x: x if x>=LW else LW)\n    \ncont_vars = ['age','creatinine_phosphokinase','ejection_fraction','platelets','serum_creatinine','serum_sodium','time']\nplt.figure(figsize=(10,24))\nfor i in enumerate(cont_vars):\n    plt.subplot(4,2,i[0]+1)\n    sns.boxplot(y=heart_df[i[1]], data = heart_df)\nplt.show()","9ca79f4c":"plt.figure(figsize=(15,12))\nsns.heatmap(data=heart_df.corr(),cmap=\"YlGnBu\",annot=True)\nplt.show()","8764f9e1":"# Logistic Regression Model Preparation\n# Train and Test split\nimport sklearn \nfrom sklearn.model_selection import train_test_split\ntrain,test =train_test_split(heart_df, random_state=100, test_size=0.3)\n\ntrain.info()","298fcb49":"# Creating (X_train, y_train) and (X_test, y_test)\n\ny_train = train.pop('DEATH_EVENT')\nX_train = train\n\ny_test = test.pop('DEATH_EVENT')\nX_test = test\n\nX_train.head()","ef06498a":"# Performing Feature Scaling \nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train[cont_vars] = scaler.fit_transform(X_train[cont_vars])\nX_train.head()","8319bd30":"# Buliding the model\nimport statsmodels.api as sm\n# Constant to X_train\nX_train_sm = sm.add_constant(X_train)\n\n# Building a model\nlgr0 =sm.GLM(y_train,X_train_sm, families = sm.families.Binomial())\nlgr0.fit().summary()","63b9e238":"# checking VIF for the X_train data set\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_sm\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif\n","9df1218b":"# high_blood_pressure feature has P value = 0.97 can be dropped\nX_train_sm.drop('high_blood_pressure', axis=1,inplace =True)\nX_train_sm.head()","86ee756d":"# Building a model 1\nlgr1 = sm.GLM(y_train,X_train_sm, families = sm.families.Binomial())\nres1 = lgr1.fit().summary()\nres1","f8167cf3":"vif = pd.DataFrame()\nX = X_train_sm\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","3a310ab6":"# platelets feature has P value = 0.793 can be dropped\nX_train_sm.drop('platelets', axis=1,inplace =True)\nX_train_sm.head()\n\n# Building a model 2\nlgr2 = sm.GLM(y_train,X_train_sm, families = sm.families.Binomial())\nres2 = lgr2.fit().summary()\nres2","8d3ef572":"# diabetes feature has P value = 0.794 can be dropped\nX_train_sm.drop('diabetes', axis=1,inplace =True)\nX_train_sm.head()\n\n# Building a model 3\nlgr3 = sm.GLM(y_train,X_train_sm, families = sm.families.Binomial())\nres3 = lgr3.fit().summary()\nres3","08bc3d0a":"# anaemia feature has P value = 0.784 can be dropped\nX_train_sm.drop('anaemia', axis=1,inplace =True)\nX_train_sm.head()\n\n# Building a model 4\nlgr4 = sm.GLM(y_train,X_train_sm, families = sm.families.Binomial())\nres4 = lgr4.fit().summary()\nres4","82fd4c09":"# smoking feature has P value = 0.614 can be dropped\nX_train_sm.drop('smoking', axis=1,inplace =True)\nX_train_sm.head()\n\n# Building a model 5\nlgr5 = sm.GLM(y_train,X_train_sm, families = sm.families.Binomial())\nres5 = lgr5.fit().summary()\nres5","295394ab":"# serum_sodium feature has P value = 0.49 can be dropped\nX_train_sm.drop('serum_sodium', axis=1,inplace =True)\nX_train_sm.head()\n\n# Building a model 6\nlgr6 = sm.GLM(y_train,X_train_sm, families = sm.families.Binomial())\nres6 = lgr6.fit().summary()\nres6","a6f01d59":"# creatinine_phosphokinase feature has P value = 0.18 can be dropped\nX_train_sm.drop('creatinine_phosphokinase', axis=1,inplace =True)\nX_train_sm.head()\n\n# Building a model 7\nlgr7 = sm.GLM(y_train,X_train_sm, families = sm.families.Binomial())\nres7 = lgr7.fit().summary()\nres7","9a6d2087":"# sex feature has P value = 0.172 can be dropped\nX_train_sm.drop('sex', axis=1,inplace =True)\nX_train_sm.head()\n\n# Building a model 8\nlgr8 = sm.GLM(y_train,X_train_sm, families = sm.families.Binomial())\nres8 = lgr8.fit()\nres8.summary()","6cf9c884":"vif = pd.DataFrame()\nX = X_train_sm\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","5ddbffb2":"y_train_pred = res8.predict(X_train_sm)\ntrain_pred = pd.DataFrame(y_train_pred)\ntrain_pred.columns = [\"train_prob\"]\n","04ff46f9":"train_pred['DEATH_EVENT']= y_train.values.reshape(-1)\n","a85a55c6":"train_pred.head()","5dc22292":"# columns with different probability cutoffs \nnumbers = [x for x in range(100)]\nfor i in numbers:\n    k=train_pred.train_prob*100\n    train_pred[i]= k.map(lambda x: 1 if x > i else 0)\ntrain_pred.head()","6c96638b":"# calculate accuracy sensitivity and specificity for various Score (probability*100) cutoffs.\n# creating a cutoff dataframe \ncutoff_df = pd.DataFrame( columns = ['Score','accuracy','sensi','speci','Precision','F1_score'])\n\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\n\n# Actual\/Predicted     N       P\n        # N          TN        FP\n        # P          FN        TP\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [x for x in range(100)]\nfor i in num:\n    cm = metrics.confusion_matrix(train_pred.DEATH_EVENT, train_pred[i] )\n    total=sum(sum(cm))\n    accuracy = (cm[0,0]+cm[1,1])\/total\n    \n    speci = cm[0,0]\/(cm[0,0]+cm[0,1])\n    sensi = cm[1,1]\/(cm[1,0]+cm[1,1])\n    Precision = cm[1,1]\/(cm[1,1]+cm[0,1])\n    F1_score = 2*Precision*  sensi\/(Precision+sensi)\n    Score = i\n    cutoff_df.loc[i] =[Score,accuracy,sensi,speci,Precision,F1_score]\ncutoff_df.iloc[20:45]","bed7fc38":"#  Plot accuracy, sensitivity and specificity for various probabilities\nplt.figure(figsize=(12,8))\ncutoff_df.plot.line(x='Score', y=['accuracy','sensi','speci'])\nplt.show()\n\nplt.figure(figsize=(12,8))\ncutoff_df.plot.line(x='Score', y=['Precision','F1_score'])\nplt.show()","0d06f51a":"# with cutoff at 39 , sensivity: 83 Accuracy : 0.81 , specificity : 0.81 and Precision 0.66, F1 = 73\n# Model evaluation using test dataset\n\ncol = list(X_train_sm.columns)\n","5e43559e":"# Performing Feature Scaling for Test \nX_test[cont_vars] = scaler.transform(X_test[cont_vars])\nX_test.head()","7b09797f":"# adding constant to X_test\nX_test_sm = sm.add_constant(X_test)\n\n# Model Prediction res8\ntest_prob = res8.predict(X_test_sm[col])\n\ntest_prob","fc042375":"final_df = pd.DataFrame(test_prob)\nfinal_df.columns =['test_prob']\nfinal_df['predicted_test'] = final_df['test_prob'].apply(lambda x: 1 if x> 0.39 else 0)\nfinal_df['DEATH_EVENT']= y_test.values.reshape(-1)\nfinal_df.head()","4ccb8103":"# Confusion matrix for test\nct = metrics.confusion_matrix(final_df['DEATH_EVENT'],final_df['predicted_test'])\nTP = ct[1,1] # true positive \nTN = ct[0,0] # true negatives\nFP = ct[0,1] # false positives\nFN = ct[1,0] # false negative\n\nsen = round(TP\/(TP+FN),2)\nspec = round(TN\/(TN+FP),2)\naccu = round((TP+TN)\/sum(sum(ct)),2)\npre = round(TP\/(TP+FP),2)\nF1 = round(2*pre*sen\/(pre+sen),2)\nprint('Model Evaluation Parameters: \\n')\nprint('Sensitivity_Test: ',sen)\nprint('Specificity_Test: ',spec)\nprint('Accuracy_Test: ',accu)\nprint('Precision_Test: ',pre)\nprint('F1_Score: ',F1)","8909d04c":"y_test.reshape(-1,1)\ny_test"}}