{"cell_type":{"2429ecfe":"code","778a23d6":"code","19678ded":"code","61ee8efc":"code","4f260575":"code","eb469d2e":"markdown","d367e9da":"markdown","32c48196":"markdown","42a73b6c":"markdown","49b9cfbf":"markdown"},"source":{"2429ecfe":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n","778a23d6":"df = pd.read_csv('\/kaggle\/input\/dnd_monsters.csv')\ndf.info(verbose=True)\ndf.head(5)","19678ded":"# Convert 'cr' column to float in seperate column\ndf['cr_float'] = df['cr']\ndf['cr_float'] = df['cr_float'].replace('1\/8', 0.125)\ndf['cr_float'] = df['cr_float'].replace('1\/4', 0.25)\ndf['cr_float'] = df['cr_float'].replace('1\/2', 0.5)\ndf['cr_float'] = df['cr_float'].astype('float')\ndf[\"cr\"] = df[\"cr\"].astype('category')\n\n# Change the \"legendary\" column to a boolean 0\/1\ndf[\"legendary\"] = df[\"legendary\"].fillna(0)\ndf[\"legendary\"] = df[\"legendary\"].replace('Legendary', 1)\n\n# Change the \"size\" column to an integer\nsizes = [\"Tiny\", \"Small\", \"Medium\", \"Large\", \"Huge\", \"Gargantuan\"]\ndf[\"size\"] = df[\"size\"].astype(pd.CategoricalDtype(sizes, ordered=True))\ndf[\"size\"] = df[\"size\"].cat.codes\n\n# These are the features we'll use to train \nfeatures = [\n    'str', 'dex', 'con', 'int', 'wis', 'cha', 'hp', 'ac', 'size', 'legendary'\n]\n\n# Drop any rows with missing features or that are missing the target value (cr)\ndf = df.dropna(subset=features + ['cr'])\ndf.info(verbose=True)\ndf[features].head(5)","61ee8efc":"from sklearn import svm\nfrom sklearn import tree\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nX = df[features]\ny = df[\"cr\"]\n\n# Train\nmodels = {\n    \"SVM\": svm.SVC(),\n    \"SGD\": SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=500),\n    \"Tree\": tree.DecisionTreeClassifier(),\n    \"RandomForest\": RandomForestClassifier(max_depth=2, random_state=0)\n}\n\ndef train_and_evaluate(model_type):\n    # Train\n    test_size = 0.3\n    seed = 42\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)\n    model = model_type.fit(X_train, y_train)\n\n    # Evaluate\n    y_pred = model.predict(X_test)    \n    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n\nfor key in models:\n    print(\"\\n\")\n    print(key)\n    print(\"==========\")\n    train_and_evaluate(models[key])","4f260575":"from sklearn import svm\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn.metrics import max_error\nfrom sklearn.model_selection import train_test_split\n\nX = df[features]\ny = df[\"cr_float\"]\n\n# Train\nmodels = {\n    \"SVC\": svm.SVR(),\n    \"SGD\": linear_model.SGDRegressor(),\n    \"Bayesian Ridge\": linear_model.BayesianRidge(),\n    \"Lasso\": linear_model.LassoLars(),\n    \"ARD\": linear_model.ARDRegression(),\n    \"PassAggr\": linear_model.PassiveAggressiveRegressor(),\n    \"Theil\": linear_model.TheilSenRegressor(),\n    \"Linear\": linear_model.LinearRegression()\n}\n\ndef train_and_evaluate(model_type):\n    # Train\n    test_size = 0.3\n    seed = 42\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)\n    model = model_type.fit(X_train, y_train)\n\n    # Evaluate\n    y_pred = model.predict(X_test)\n\n    print(\"Mean absolute error:\", mean_absolute_error(y_test, y_pred))\n    print(\"Mean squared error:\", mean_squared_error(y_test, y_pred))\n    print(\"Explained Variance:\", explained_variance_score(y_test, y_pred))\n\nfor key in models:\n    print(\"\\n\")\n    print(key)\n    print(\"==========\")\n    train_and_evaluate(models[key])","eb469d2e":"# Classification approach\nIs this a classificaiton problem? Evidently not. The best accuracy I was able to achieve (without any hyperparameter tuning) was around 0.3 - so one in every 3-ish creatures were being given the wrong challenge rating.\n\nThe fact it was able to get that close is pretty good anyway, however I think this is almost definately more of a regression problem - if we get \"close\" to the challenge rating we want then that still counts as a win.","d367e9da":"# Regression approach\n\nSome ridiculously impressive scores for a few regression models.[](http:\/\/)","32c48196":"# Exploration","42a73b6c":"Adopting the following strategy;\n\n* Drop all rows with missing values\n* Encode the \"size\" column - we can safely do this using standard label encoding as the sizes have a very clear order \/ are all related to one another.","49b9cfbf":"# Preprocessig \/ Feature Engineering"}}