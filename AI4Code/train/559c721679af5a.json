{"cell_type":{"050b5a28":"code","a2b541cb":"code","0d86b294":"code","490f9929":"code","60062af8":"code","be4cf291":"code","aa256fed":"code","10b2542e":"code","d3a2f504":"code","10a3e0ed":"code","ec72d890":"code","98fb8cec":"code","3bf0e3e5":"code","a186b15f":"code","a395ba53":"code","6122c9e7":"code","672cbca1":"code","2aa97c46":"code","5cc5162f":"code","a8183b65":"code","10653f2e":"markdown","3c69e678":"markdown","43554235":"markdown","275b011e":"markdown","fedb546a":"markdown","84106cae":"markdown","34cbe98e":"markdown","55590364":"markdown","75b8de21":"markdown","2ceb4079":"markdown","31d5df81":"markdown","b878c5d4":"markdown","6b426e02":"markdown","b0dd3d80":"markdown","370cdf29":"markdown","0e0f85c8":"markdown","9faeb833":"markdown","5c7d8fd5":"markdown","127c3c92":"markdown","19fb1536":"markdown","ff341051":"markdown","5b2787e6":"markdown","ab376283":"markdown","8d4efeb4":"markdown","d0d543ac":"markdown","76a5d299":"markdown","50c76b90":"markdown","1f272ab0":"markdown","9bb52bf7":"markdown"},"source":{"050b5a28":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a2b541cb":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport scipy.stats as ss","0d86b294":"%matplotlib inline\nwarnings.filterwarnings(\"ignore\")\nsns.set_palette('deep')\nsns.set_color_codes()\nsns.set_style('dark')\n","490f9929":"df = pd.read_csv('..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')\nprint('Data is Loaded')","60062af8":"df.shape","be4cf291":"df.head()","aa256fed":"df.info()","10b2542e":"df.isna().apply(pd.value_counts, axis=0)","d3a2f504":"categorical = ['sex', 'cp', 'fbs', 'restecg', 'exng', 'thall', 'caa', 'slp']\ncontinous = ['age', 'trtbps', 'chol', 'thalachh', 'oldpeak']\nprint('Categorical Variables are:', ', '.join(categorical))\nprint('Continous Variables are:', ', '.join(continous))","10a3e0ed":"df.describe().T","ec72d890":"chart_count = len(continous) + 1\n\nfig = plt.figure(figsize=(20, 17))\naxes = [fig.add_subplot(3, 3, i) for i in range(1, chart_count + 1)]\nfig.tight_layout(pad=7)\nfig.patch.set_facecolor('#eaeaf2')\n\naxes[0].spines[\"bottom\"].set_visible(False)\naxes[0].spines[\"left\"].set_visible(False)\naxes[0].spines[\"top\"].set_visible(False)\naxes[0].spines[\"right\"].set_visible(False)\naxes[0].tick_params(left=False, bottom=False)\naxes[0].set_xticklabels([])\naxes[0].set_yticklabels([])\naxes[0].text(0.5, 0.5,\n             'Violin plot for the\\n continous features\\n_________________',\n             horizontalalignment='center', verticalalignment='center',\n             fontsize=20, fontweight='bold', fontfamily='serif')\n\nfor i in range(1, chart_count):\n    var = continous[i - 1]\n    ax = axes[i]\n    ax.grid(axis='y', linestyle=':')\n    ax.text(0.5, 1.05, var.title(),\n            horizontalalignment='center', verticalalignment='center',\n            fontsize=14, fontweight='bold', transform=ax.transAxes)\n    color = sns.color_palette('deep')[i - 1]\n    sns.violinplot(data=df, y=var, ax=ax, color=color)\n    ax.set_xlabel('')\n    ax.set_ylabel('')","98fb8cec":"chart_count = len(categorical) + 1\n\nfig = plt.figure(figsize=(20, 17))\naxes = [fig.add_subplot(3, 3, i) for i in range(1, chart_count + 1)]\nfig.tight_layout(pad=7)\nfig.patch.set_facecolor('#eaeaf2')\n\naxes[0].spines[\"bottom\"].set_visible(False)\naxes[0].spines[\"left\"].set_visible(False)\naxes[0].spines[\"top\"].set_visible(False)\naxes[0].spines[\"right\"].set_visible(False)\naxes[0].tick_params(left=False, bottom=False)\naxes[0].set_xticklabels([])\naxes[0].set_yticklabels([])\naxes[0].text(0.5, 0.5,\n             'Count plot for the\\n categorical features\\n_________________',\n             horizontalalignment='center', verticalalignment='center',\n             fontsize=20, fontweight='bold', fontfamily='serif')\n\nfor i in range(1, chart_count):\n    var = categorical[i - 1]\n    ax = axes[i]\n    ax.text(0.5, 1.05, var.title(),\n            horizontalalignment='center', verticalalignment='center',\n            fontsize=14, fontweight='bold', transform=ax.transAxes)\n    sns.countplot(data=df, x=var, ax=ax)\n    ax.set_xlabel('')\n    ax.set_ylabel('')","3bf0e3e5":"def cramers_corrected_stat(x, y):\n    \"\"\" calculate Cramers V statistic for categorial-categorial association.\n        uses correction from Bergsma and Wicher, \n        Journal of the Korean Statistical Society 42 (2013): 323-328\n    \"\"\"\n    result = -1\n\n    conf_matrix = pd.crosstab(x, y)\n\n    if conf_matrix.shape[0] == 2:\n        correct = False\n    else:\n        correct = True\n\n    chi2, p = ss.chi2_contingency(conf_matrix, correction=correct)[0:2]\n\n    n = sum(conf_matrix.sum())\n    phi2 = chi2\/n\n    r, k = conf_matrix.shape\n    phi2corr = max(0, phi2 - ((k-1)*(r-1))\/(n-1))\n    rcorr = r - ((r-1)**2)\/(n-1)\n    kcorr = k - ((k-1)**2)\/(n-1)\n    result = np.sqrt(phi2corr \/ min((kcorr-1), (rcorr-1)))\n    return round(result, 6), round(p, 6)\n\n\nfor var in categorical:\n    x = df[var]\n    y = df['output']\n    cramersV, p = cramers_corrected_stat(x, y)\n    print(f'For variable {var}, Cramer\\'s V: {cramersV} and p value: {p}')","a186b15f":"for var in continous:\n    gp = df[[var, 'output']].groupby(['output'])\n    gp_array = [group[var].to_numpy() for name, group in gp]\n    kstat, p = ss.kruskal(*gp_array)\n    kstat, p = round(kstat, 6), round(p, 6)\n    print(f'For variable {var}, Kruskal-Wallis H-test: {kstat} and p value: {p}')","a395ba53":"sns.pairplot(df, hue='output');","6122c9e7":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support, confusion_matrix\nfrom sklearn.preprocessing import RobustScaler, MinMaxScaler, StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\n","672cbca1":"X = df[['sex', 'restecg', 'cp', 'exng', 'thall', 'caa', 'slp', 'age',\n        'trtbps', 'chol', 'thalachh', 'oldpeak']]\ny = df['output']\n\nscaler = StandardScaler()\nX[continous] = scaler.fit_transform(X[continous])\n\nencode_columns = categorical.copy()\nencode_columns.remove('fbs')\n\nX = pd.get_dummies(X, columns=encode_columns)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n                                                    random_state=65)\n\nprint('Done Pre-processing')\nprint('Final No. of features: ', X.shape[1])","2aa97c46":"models = {\n          'SVM': SVC(),\n          'Decision Tree': DecisionTreeClassifier(),\n          'Random Forest': RandomForestClassifier(),\n          'Logistic Regression': LogisticRegression(),\n          'K-Nearest Neighbors': KNeighborsClassifier(),\n          'Gradient Boosting': GradientBoostingClassifier(),\n          'AdaBoost Classifier': AdaBoostClassifier(learning_rate=0.15, n_estimators=25),\n         }\n\naccuracy_dict, precision_dict, recall_dict, f1_dict = dict(), dict(), dict(), dict()\n\nfor name, model in models.items():\n    model.fit(X_train, y_train)\n    y_hat = model.predict(X_test)\n    print('---------------------------------------------------\\n',\n          name,\n          '\\n---------------------------------------------------')\n\n    acc = accuracy_score(y_test, y_hat)\n    precision, recall, f1, support = precision_recall_fscore_support(y_test, y_hat, average='binary')\n    acc, precision, recall, f1 = round(acc, 5), round(precision, 5), round(recall, 5), round(f1, 5)\n    \n    accuracy_dict[name] = acc\n    precision_dict[name] = precision\n    recall_dict[name] = recall\n    f1_dict[name] = f1\n\n    print(f'Accuracy: {acc}\\nPrecision: {precision}\\nRecall: {recall}\\nF1: {f1}')\n\n    cm = confusion_matrix(y_test, y_hat)\n    df_cm = pd.DataFrame(cm)\n    sns.heatmap(df_cm, annot=True, cmap='Blues', linewidths=2)\n    plt.title(f'Confusion Matrix for {name}', fontsize=15)\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.show()","5cc5162f":"level0 = [(name, model) for name, model in models.items()]\nlevel1 = LogisticRegression()\nstacked = StackingClassifier(estimators=level0, final_estimator=level1, n_jobs=-1)\nstacked.fit(X_train, y_train)\ny_hat = stacked.predict(X_test)\n\nname = 'Stacked Classifier'\nprint('---------------------------------------------------\\n',\n      name,\n      '\\n---------------------------------------------------')\n\nacc = accuracy_score(y_test, y_hat)\nprecision, recall, f1, support = precision_recall_fscore_support(y_test, y_hat, average='binary')\nacc, precision, recall, f1 = round(acc, 5), round(precision, 5), round(recall, 5), round(f1, 5)\n\naccuracy_dict[name] = acc\nprecision_dict[name] = precision\nrecall_dict[name] = recall\nf1_dict[name] = f1\n\nprint(f'Accuracy: {acc}\\nPrecision: {precision}\\nRecall: {recall}\\nF1: {f1}')\n\ncm = confusion_matrix(y_test, y_hat)\ndf_cm = pd.DataFrame(cm)\nsns.heatmap(df_cm, annot=True, cmap='Blues', linewidths=2)\nplt.title(f'Confusion Matrix for {name}', fontsize=15)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.show()","a8183b65":"scores_dicts = {\n                'Accuracy': accuracy_dict,\n                'Precision': precision_dict,\n                'Recall': recall_dict,\n                'F1 Score': f1_dict,\n              }\n\nfor name, scores_dict in scores_dicts.items():\n    index, values = zip(*scores_dict.items())\n    acc_df = pd.DataFrame(data=values, index=index, columns=[name])\n    plt.figure(figsize=(9, 10))\n    sns.barplot(y=acc_df.index, x=acc_df[name])\n    plt.title(f'Plot of {name} Score')","10653f2e":"#### Conclusion\n\n* Restecg, Thall, Caa, Slp are likely to impact few models sensitive to data distribution because value counts for some values is extremely low.\n\n* Fbs and Cp could possibly affect models sensitive to data distribution because their value counts is also not ideal.\n\n","3c69e678":"### Stacking","43554235":"#### Performing feature separation and Scaling and Encoding of features","275b011e":"#### Corrected Cramer's V for categorical variables","fedb546a":"#### Pair Plot","84106cae":"### Bivariate Analysis","34cbe98e":"### Loading the dataset","55590364":"#### Kruskal-Wallis H-test\n\nSince the distribution for some variables is non-Gaussian we would be using non-parametric test--specifically Kruskal-Wallis H Test","75b8de21":"age - Age of the patient\n\nsex - Sex of the patient\n\ncp - Chest pain type ~ 0 = Typical Angina, 1 = Atypical Angina, 2 = Non-anginal Pain, 3 = Asymptomatic\n\ntrtbps - Resting blood pressure (in mm Hg)\n\nchol - Cholestoral in mg\/dl fetched via BMI sensor\n\nfbs - (fasting blood sugar > 120 mg\/dl) ~ 1 = True, 0 = False\n\nrestecg - Resting electrocardiographic results ~ 0 = Normal, 1 = ST-T wave normality, 2 = Left ventricular hypertrophy\n\nthalachh - Maximum heart rate achieved\n\noldpeak - Previous peak\n\nslp - Slope\n\ncaa - Number of major vessels\n\nthall - Thalium Stress Test result ~ (0,3)\n\nexng - Exercise induced angina ~ 1 = Yes, 0 = No\n\noutput - Target variable","2ceb4079":"#### Conclusion\n\n* Suprisingly, all variables have correlation. Although chol and trtpbs cut very close to our alpha (which is 0.05).","31d5df81":"### Model Creation","b878c5d4":"#### Conclusion\n\n* The stats seem to agree with the conclusions we drew previously from graph along with a measure of the correlation.\n\n* Fbs is not related at all\n\n* Sex, Restecg have very weak correlation\n\n* Sp, Thall seem to have a moderately strong correlation\n\n* Exng, Caa, Slp have decent correlation","6b426e02":"We would get out X as the values we determined to have an impact, Standardize them, and get One-hot encoding for categorical variables.","b0dd3d80":"#### Conclusion\n\n* chol, trtbps, and oldpeak have decent amount of outliers that could affect certain models sensitive to them.\n\n* oldpeak and chol (moderately) are not uniformly distributed. This could affect models or analysis with uniform distribution as requirement.","370cdf29":"#### Setting up some basic paprmeters for plotting purpose","0e0f85c8":"### Training Various Models","9faeb833":"## Heart Attack Prediction with Extensive EDA\n\n","5c7d8fd5":"#### Checking Missing values","127c3c92":"### Thanks for reading up to this far.\n\n### If you liked the notebook, please consider upvoting.","19fb1536":"#### Explaning the dataset","ff341051":"### Performing Feature Engineering","5b2787e6":"#### Univariate Analysis :","ab376283":"### Data Analysis Part","8d4efeb4":"#### Categorical and Continuous Variables","d0d543ac":"### Importing Important Libraries","76a5d299":"### Exploratory Data Analysis","50c76b90":"### Looking up basic stats of dataset","1f272ab0":"### Plotting Scores","9bb52bf7":"### Conclusion \n\nHere's the conclusion from the entire EDA:\n\n#### Feature Insights\n\n* chol, trtbps, and oldpeak have decent amount of outliers. This could affect certain models sensitive to them.\n\n* oldpeak and chol (moderately) are not uniformly distributed. This could affect models or analysis with uniform distribution as requirement.\n\n* Restecg, Thall, Caa, Slp are likely to impact few models sensitive to data distribution because value counts for some values is extremely low.\n\n* Fbs and Cp could possibly affect models sensitive to data distribution because their value counts is also not ideal.\n\n* Relation to target variable\n\n* All categorical variables except Fbs are related to output, albeit to varying degrees. Especially, Restecg and sex have very weak relation.\n\n* All continous variables are related to output\n\n* Multi-collinearity\n\n* Variables do not have strong correlation and are weakly correlated"}}