{"cell_type":{"fe26b765":"code","d2d8a732":"code","5f4e4a0f":"code","f9877796":"code","611476e6":"code","de767725":"code","b7660eef":"code","8128490a":"code","2dbe6cb4":"code","83acc9a1":"code","966f2e11":"code","a590e1b3":"code","77cf0008":"code","0a72acd4":"code","72ac7bc4":"code","0647beb3":"code","fa1a5767":"code","84ce775e":"code","b744bda1":"code","d1489cf6":"code","7417223e":"code","4fff0329":"code","e9724b65":"code","5d374a49":"code","0a166256":"code","52770793":"code","bc473459":"code","9bae1358":"code","978a9e54":"code","ff009ad2":"code","efffd4fd":"code","d139b459":"code","ed25b4c2":"code","456595c2":"code","f6da333b":"code","17dbbf2a":"code","6ce9c84f":"code","394ef85a":"code","67b789ae":"code","2ff1835d":"code","07f27810":"code","c7a0410a":"code","237a0293":"code","a6386a3b":"code","a5d70f30":"code","8308cf3b":"code","0c8d9475":"code","99935c9f":"code","a974885d":"code","04a29983":"code","080bca25":"code","11bbeb2a":"code","bc415c5f":"code","032ccb90":"code","46b415e0":"code","e72c0997":"code","3a5679e3":"code","ac2a1a9e":"code","d5ce15a9":"code","e6b0c95d":"code","fa22a982":"code","b19b9340":"code","ae613443":"code","960c7231":"code","dfde9ef0":"code","39cf1b80":"code","4e8ab41e":"code","d461f75e":"code","c215225d":"code","02dd4185":"code","c36d5430":"code","d2fa6e9d":"code","056916ce":"code","f8090875":"code","a76aa96d":"code","4ab76678":"code","e149409e":"code","45461d30":"code","e8cec77b":"code","a7b5ef70":"code","bce6483f":"code","24ab5ee0":"code","e3ce295b":"code","eaddf5cc":"code","359604ba":"code","ce557c33":"code","56af8d74":"code","74a36884":"code","e1f6a1d1":"code","0f959ba2":"code","652c8f58":"code","3cce3c22":"code","22dee941":"code","6a105403":"code","c0cfac7f":"code","88eaf950":"code","31abbd7e":"code","b040a6f1":"code","4bc8d52e":"code","f16a6ea4":"code","7d4bd04e":"code","b2854fbe":"code","d234a95a":"code","a7388fd6":"code","38cb8211":"code","f9b6a8e0":"code","12d0e71d":"code","0cb9dcf9":"code","195e6fd2":"code","7e7644ae":"code","cf81a386":"code","cf517f36":"code","34367c17":"code","f50b1f5f":"markdown","cedfb188":"markdown","d36f25e6":"markdown","be5db7fd":"markdown","3c3f4639":"markdown","36f4b453":"markdown","23013417":"markdown","80d21bb9":"markdown","ad91bc5c":"markdown","097408fc":"markdown","c240ca00":"markdown","9391123f":"markdown","c0bc6426":"markdown","0b73f31b":"markdown","d1287789":"markdown","0d1868aa":"markdown","727007e7":"markdown","6eddb404":"markdown","cb09f9a8":"markdown","9d29504f":"markdown","51f1eb9a":"markdown","10a19d22":"markdown","29e53e7f":"markdown","d06cc210":"markdown","3d4b25d3":"markdown","dff4f1a5":"markdown","9f372c77":"markdown","b8154407":"markdown","ea49c92b":"markdown","917cbdf8":"markdown","5901d76a":"markdown","3b5fbbc7":"markdown","713b062d":"markdown","10c89737":"markdown","1a4ad9de":"markdown","91d6e8ea":"markdown","25334168":"markdown","8649c071":"markdown","354e7f5c":"markdown"},"source":{"fe26b765":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sqlite3\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import scale \nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nimport datetime as dt\nimport datetime\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression  \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn import model_selection\nfrom sklearn.metrics import accuracy_score\nimport timeit\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport math\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.metrics import silhouette_samples","d2d8a732":"# importing dataset and read it\ndatabase = '..\/input\/soccer\/database.sqlite'\nconn = sqlite3.connect(database)","5f4e4a0f":"#connent to datasets and pick up the table that we need\ntables = pd.read_sql(\"\"\"SELECT *\n                        FROM sqlite_master\n                        WHERE type='table';\"\"\", conn)\nprint(\"Connection Successful\",conn)","f9877796":"# df for examination 1 and 2  and df3 for the third scenario\ndf = pd.read_sql_query(\"SELECT * FROM Player_Attributes\", conn)\n\n\n","611476e6":"df3 = pd.read_sql_query(\"SELECT * FROM Match\" , conn)","de767725":"# check our column \nprint(\"df3 shape is:\",df3.shape)\nprint(\"df shape is:\",df.shape)\n","b7660eef":"# a owerview of our dataset \ndf.describe().transpose()","8128490a":"# a owerview of our dataset \ndf.describe()","2dbe6cb4":"# find any null value \ndf.isna().any().any()","83acc9a1":"# drop missing value \ndf = df.dropna()","966f2e11":"df3 = df3.dropna()","a590e1b3":"df.isnull().sum().any()","77cf0008":"# to check how many rows was removed for cleaning data\ndf.shape","0a72acd4":"# converting data to numerical value\n\nfoot = df['preferred_foot']\n\np = {'right':0, 'left':1}\ndf['preferred_foot'] = df['preferred_foot'].map(p)","72ac7bc4":"df.head(5)","0647beb3":"df.loc[~(df['attacking_work_rate'].isin(['medium','high','low'])\\\n         \n                       | df['defensive_work_rate'].isin(['medium','high','low'])),:].shape","fa1a5767":"\ndf_updated = df.loc[(df['attacking_work_rate'].\\\n                                                  isin(['medium','high','low'])\\\n                       & df['defensive_work_rate'].isin(['medium','high','low'])),:]\ndf_updated.shape","84ce775e":"df_updated[\"attacking_work_rate\"].unique()","b744bda1":"# corolation test \ndf_corr = df.drop([\"id\",\"player_fifa_api_id\", \"player_api_id\",\"date\", \"attacking_work_rate\",\"defensive_work_rate\" ], axis=1) \ncorrolation = df_corr.corr()\ncorrolation","d1489cf6":"# Data visualasiton\ncorrelation_matrix  = df.corr()\ncorrelation_matrix[\"overall_rating\"].sort_values(ascending=False)","7417223e":"# Heatmap to show the corrolation \ncorr1 = df_corr.select_dtypes(include =['float64','int64']).\\\nloc[:,df_corr.select_dtypes(include =['float64','int64']).columns[3:]].corr()","4fff0329":"fig2,ax2 = plt.subplots(nrows = 1,ncols = 1)\nfig2.set_size_inches(w=24,h=24)\nsns.heatmap(corr1,annot = True,linewidths=0.5,ax = ax2)","e9724b65":"df2 = df.copy(deep=True)\ndf2 = df.drop([\"player_fifa_api_id\",\"player_api_id\",'date',\"attacking_work_rate\",\"defensive_work_rate\",\"preferred_foot\", \"id\"   ], axis=1)\ncolumns = ['potential',  'crossing', 'finishing', 'heading_accuracy',\n       'short_passing', 'volleys', 'dribbling', 'curve', 'free_kick_accuracy',\n       'long_passing', 'ball_control', 'acceleration', 'sprint_speed',\n       'agility', 'reactions', 'balance', 'shot_power', 'jumping', 'stamina',\n       'strength', 'long_shots', 'aggression', 'interceptions', 'positioning',\n       'vision', 'penalties', 'marking', 'standing_tackle', 'sliding_tackle',\n       'gk_diving', 'gk_handling', 'gk_kicking', 'gk_positioning',\n       'gk_reflexes']\n","5d374a49":"correlations = [ df2['overall_rating'].corr(df2[f]) for f in columns ]\nlen(columns),len(correlations)","0a166256":"# the line graph to show correlation between overall_rating and other attributs\n# from matplotlib.pyplot import figure\nfrom pylab import rcParams\n\ndef plot_df_corrolation(df2, y_label):\n     color = \"Blue\"\n     fig = plt.gcf\n    \n     plt.ylabel(y_label)\n     rcParams['figure.figsize'] = 10, 3\n     ax = df2.correlation.plot(linewidth= 3.5, color = color)\n     ax.set_xticks(df2.index)\n     ax.set_xticklabels(df2.attributes, rotation=75); \n     plt.show()","52770793":"df1 = pd.DataFrame({'attributes': columns, 'correlation': correlations})\n\n#df1 = df1[1:36]\ndf1[\"correlation\"].sort_values(ascending=False,ignore_index= True).head(6)\n","bc473459":"\n\nplot_df_corrolation(df1, 'Player\\'s Overall Rating')\n","9bae1358":"players_data = df_updated.copy(deep=True)\nplayers_data = players_data.drop([\"date\"], axis=1)\nplayers_data[\"preferred_foot\"].unique()","978a9e54":"# creating plot to show a overview of foot preferer and attackinf and defensive work \n\nfig1, ax1 = plt.subplots(nrows = 1, ncols= 3)\nfig1.set_size_inches(12,3)\nsns.barplot(x ='preferred_foot', y = 'preferred_foot', data = players_data,\\\n             estimator = lambda x: len(x)\/len(players_data) * 100, ax = ax1[0],\\\n            orient = 'v')\nax1[0].set(ylabel = 'Percentage', title = \"Player's Foot\")\n\n\nsns.barplot(x= \"attacking_work_rate\", y = \"attacking_work_rate\", data= players_data,\\\n            estimator = lambda x: len(x)\/len(players_data)*100, ax = ax1[1],\\\n            orient = \"v\")\nax1[1].set( ylabel = \"Percentage\", title = 'Attacking Rate')\n\n\nsns.barplot(x= \"defensive_work_rate\", y = \"defensive_work_rate\", data= players_data,\\\n            estimator = lambda x: len(x)\/len(players_data)*100, ax = ax1[2],\\\n            orient = \"v\")\nax1[2].set( ylabel = \"Percentage\", title = 'Defensive Rate')","ff009ad2":"players_data2 =  pd.read_sql_query(\"SELECT * FROM Player\", conn)\n\ndef get_age_for_football_players(x):\n    date  =  x.split(\" \")[0]\n    today = datetime.datetime.strptime(\"2015-01-01\", \"%Y-%m-%d\").date()\n    born = datetime.datetime.strptime(date, \"%Y-%m-%d\").date()\n    return today.year - born.year - ((today.month, today.day) < (born.month, born.day))\n\n\n","efffd4fd":"sns.kdeplot(players_data.overall_rating, shade=True, color=\"r\")\n","d139b459":"sns.kdeplot(players_data.potential, shade=True, color=\"r\")","ed25b4c2":"players_data[\"Potential_Growth\"] = (players_data.potential) -  (players_data.overall_rating)\nsns.kdeplot(players_data.Potential_Growth, shade=True, color=\"r\")","456595c2":"\n# player_data_updated = players_data.loc[(players_data['attacking_work_rate'].\\\n#                                                   isin(['medium','high','low'])\\\n#                        & players_data['defensive_work_rate'].isin(['medium','high','low'])),:]\n# print(player_att_table_updated1.shape)\n# player_att_table_updated1.head()","f6da333b":"# df3 = pd.read_sql_query(\"SELECT * FROM Player_Attributes\", conn)\n# player_att_table_updated1 = df3.loc[(df3['attacking_work_rate'].\\\n#                                                   isin(['medium','high','low'])\\\n#                        & df3['defensive_work_rate'].isin(['medium','high','low'])),:]\n\n# pat = player_att_table_updated1.loc[:,player_att_table_updated1.columns.tolist()[3:]]","17dbbf2a":"dfn = pd.read_sql_query(\"SELECT * FROM Player_Attributes\", conn)\nplayer_att_table_updated1 = dfn.loc[(dfn['attacking_work_rate'].\\\n                                                  isin(['medium','high','low'])\\\n                        & dfn['defensive_work_rate'].isin(['medium','high','low'])),:]\n\npat = player_att_table_updated1.loc[:,player_att_table_updated1.columns.tolist()[3:]]","6ce9c84f":"fig2, ax2 = plt.subplots(nrows=5,ncols=7)\nfig2.set_size_inches(16,12)\nfor i,j in enumerate(player_att_table_updated1.select_dtypes(include = ['float64','int64']).columns[3:].tolist()):\n    sns.distplot(pat.loc[:,j],kde = False,hist = True, ax = ax2[int(i\/7)][i%7])\nfig2.tight_layout()\n    ","394ef85a":"fig6, ax6 = plt.subplots(nrows=5,ncols=7)\nfig6.set_size_inches(16,12)\nfor i,j in enumerate(df2.select_dtypes(include = ['float64','int64']).columns[3:].tolist()):\n    sns.boxplot(x = \"preferred_foot\", y = j, data= pat, ax = ax6[int(i\/7)][i%7])\nfig6.tight_layout()","67b789ae":"df_K = df.copy(deep = True)\ndf_K.shape","2ff1835d":"# features that we selected to cluster our player based on it\nfetures_selected = [\"gk_kicking\", 'potential', 'marking', 'standing_tackle', 'interceptions']\nfetures_selected2 = ['potential','reactions',\"gk_kicking\",'standing_tackle',\"vision\"]\nfetures_selected3 = [\"potential\",\"gk_kicking\",'standing_tackle','ball_control','heading_accuracy' ]","07f27810":"target = df_K[\"overall_rating\"]\nX = df_K[fetures_selected].values\ny = target","c7a0410a":"from sklearn.cluster import KMeans\n#finding obtimising number for clustering\nwcss = []\n\nfor i in range(1,11):\n\n    kmeans = KMeans(n_clusters = i, init = 'k-means++')\n    kmeans.fit(X)\n    wcss.append(kmeans.inertia_)\n\n\n#Plotting the results onto a line graph, allowing us to observe 'The elbow'\nplt.plot(range(1, 11), wcss)\nplt.title('The elbow method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS') #within cluster sum of squares\nplt.tick_params(axis='x', colors='white') \nplt.tick_params(axis='y', colors='white')\nplt.title(\"Min_Samples_split\", color = \"Red\")\nplt.grid(axis = 'x')\nplt.xlabel('min_samples_split', color= \"white\")\nplt.show()","237a0293":"# removed some labeled data that meaningless \n","a6386a3b":"#Applying kmeans to the dataset \/ Creating the kmeans classifier\nkmeans_model = KMeans(n_clusters = 4, init = 'k-means++',).fit(X)","a5d70f30":"y_kmeans = kmeans_model.fit_predict(X)\ny_kmeans","8308cf3b":"pd.value_counts(kmeans_model.labels_, sort=False)","0c8d9475":"\nfrom sklearn.metrics import accuracy_score\nscore = silhouette_score(X, kmeans_model.labels_, metric='euclidean')\nscore","99935c9f":"#Applying kmeans to the dataset \/ Creating the kmeans classifier\n\nkmeans_model = KMeans(n_clusters = 3, init = 'k-means++',).fit(X)\ny_kmeans = kmeans_model.fit_predict(X)","a974885d":"y_kmeans","04a29983":"pd.value_counts(kmeans_model.labels_, sort=False)","080bca25":"X_std = StandardScaler().fit_transform(X)","11bbeb2a":"start = timeit.default_timer()\nfor i, k in enumerate([ 3, 4]):\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    fig.set_size_inches(18, 7)\n    \n    # Run the Kmeans algorithm\n    km = KMeans(n_clusters=k)\n    labels = km.fit_predict(X_std)\n    centroids = km.cluster_centers_\n\n    # Get silhouette samples\n    silhouette_vals = silhouette_samples(X_std, labels)\n\n    # Silhouette plot\n    y_ticks = []\n    y_lower, y_upper = 0, 0\n    for i, cluster in enumerate(np.unique(labels)):\n        cluster_silhouette_vals = silhouette_vals[labels == cluster]\n        cluster_silhouette_vals.sort()\n        y_upper += len(cluster_silhouette_vals)\n        ax1.barh(range(y_lower, y_upper), cluster_silhouette_vals, edgecolor='none', height=1)\n        ax1.text(-0.03, (y_lower + y_upper) \/ 2, str(i + 1))\n        y_lower += len(cluster_silhouette_vals)\n\n    # Get the average silhouette score and plot it\n    avg_score = np.mean(silhouette_vals)\n    ax1.axvline(avg_score, linestyle='--', linewidth=2, color='green')\n    ax1.set_yticks([])\n    ax1.set_xlim([-0.1, 1])\n    ax1.set_xlabel('Silhouette coefficient values')\n    ax1.set_ylabel('Cluster labels')\n    ax1.set_title('Silhouette plot for the various clusters', y=1.02);\n    \n    # Scatter plot of data colored with labels\n    ax2.scatter(X_std[:, 0], X_std[:, 1], c=labels)\n    ax2.scatter(centroids[:, 0], centroids[:, 1], marker='*', c='r', s=250)\n    ax2.set_xlim([-3, 4])\n    ax2.set_xlim([-3, 4])\n    ax2.set_xlabel('features scatter', color ='white')\n    ax2.set_ylabel('features scatter', color ='white')\n    ax2.set_title('Visualization of clustered data', y=1.02,  color ='white')\n    ax2.set_aspect('equal')\n    plt.tight_layout()\n    plt.suptitle(f'Silhouette analysis using k = {k}',\n                 fontsize=16, fontweight='semibold', y=1.05);\n    stop = timeit.default_timer()\nprint('Time: ', stop - start)  ","bc415c5f":"km = kmeans_model\nkm.fit(X_std)\nlabels = y_kmeans\ncentroids = km.cluster_centers_\n\n# Plot the data\nfig, ax = plt.subplots(1, 2, figsize=(10, 10))\nax[0].scatter(X_std[:, 0], X_std[:, 1])\nax[0].scatter(X_std[:, 0], X_std[:, 1])\nax[0].scatter(X_std[:, 0], X_std[:, 1])\nax[0].set_aspect('equal')\nax[1].scatter(X_std[:, 0], X_std[:, 1], c=labels)\nax[1].scatter(centroids[:, 0], centroids[:, 1], marker='o',\n                c=\"white\", alpha=1, s=200, edgecolor='k')\nfor i, c in enumerate(centroids):\n    ax[1].scatter(c[0], c[1], marker='$%d$' % i, s=50, alpha=1, edgecolor='r')\nax[1].set_aspect('equal')\nplt.tight_layout()","032ccb90":"print(kmeans.cluster_centers_) #display cluster centers\nplt.scatter(X[y_kmeans   == 0, 0], X[y_kmeans == 0, 1], c = 'red', label = 'Iris-setosa')\nplt.scatter(X[y_kmeans   == 1, 0], X[y_kmeans == 1, 1], c = 'blue', label = 'Iris-versicolour')\nplt.scatter(X[y_kmeans   == 2, 0], X[y_kmeans == 2, 1], c = 'green', label = 'Iris-virginica')   #Visualising the clusters - On the first two columns\nplt.legend()\nplt.show()\n","46b415e0":"df.columns","e72c0997":"# call our dataset \ndf2 = df.copy(deep = True)\n#df_new = df_new.drop(labels=range(50000, 180354), axis=0)\ndf2 = df2.dropna()\n\n","3a5679e3":"df2.shape ","ac2a1a9e":"# remove meaningless and high correlation features\nfeatures_2 = df2.drop(['id', 'player_fifa_api_id','player_api_id',\n                        'date', 'overall_rating', 'preferred_foot','attacking_work_rate' \n                        ,'ball_control', 'sprint_speed',\"marking\" ,'sliding_tackle','gk_diving',\n                        'gk_handling','gk_positioning','gk_reflexes', 'defensive_work_rate'], axis=1)\nfeatures_2.shape\n","d5ce15a9":"# this features just remove the columns with meaningless value and non numerical\nfeatures_2_n = df2.drop(['id', 'player_fifa_api_id','player_api_id','date','overall_rating', 'defensive_work_rate','attacking_work_rate' ], axis=1)\nfeatures_2_n.shape","e6b0c95d":"features_2_n['preferred_foot'] = features_2_n['preferred_foot'].astype(float)","fa22a982":"target = df2[\"overall_rating\"]","b19b9340":"X2 = features_2\nX_2 = features_2_n\ny2 = target","ae613443":"# building  model and fit it with training data\nregressore = LinearRegression()","960c7231":"# split our data to train and test by using sklearn.model_selection function\n# with removing high corrolation\nxtrain, xtest, ytrain, ytest  = train_test_split(X2, y2, test_size = 0.30, random_state = 49)\nstart = timeit.default_timer()\nregressore.fit(xtrain, ytrain)\ny_pred = regressore.predict(xtest)\naccuracy = r2_score(ytest, y_pred)\nprint(f\"the score value is:  {accuracy:.4f}\")\n\nstop = timeit.default_timer()\n\nprint('Time: ', stop - start) ","dfde9ef0":"# split our data to train and test by using sklearn.model_selection function\nxtrain, xtest, ytrain, ytest  = train_test_split(X_2, y2, test_size = 0.30, random_state = 49)\nstart = timeit.default_timer()\nregressore.fit(xtrain, ytrain)\ny_pred = regressore.predict(xtest)\naccuracy = r2_score(ytest, y_pred)\nprint(f\"the score value is:  {accuracy:.4f}\")\n\nstop = timeit.default_timer()\n\nprint('Time: ', stop - start) ","39cf1b80":"print(ytrain.shape)\nxtrain.shape","4e8ab41e":"from sklearn.metrics import mean_absolute_error\nMAE = (mean_absolute_error(ytest, y_pred))\nprint(f\"the MAE value is:  {MAE:.4f}\")","d461f75e":"MSE = mean_squared_error(ytest, y_pred)\nprint(f'Mean squared error: {MSE:.4f}')\n","c215225d":"# RMSE (Root Mean Squared Error)\nRMSE = math.sqrt(MSE)\nprint(\"Root Mean Square Error:\\n\")\nprint(RMSE)","02dd4185":"# to make the RMSE to rebust to outliers we implement log from RMSE\nRMSElog = np.log(RMSE)\nRMSElog","c36d5430":"mini = min(df2[\"overall_rating\"])\nmaxi = max(df2[\"overall_rating\"])","d2fa6e9d":"NormalizedRMSE = RMSE \/ (maxi - mini)\nprint(f'Mean squared error: {NormalizedRMSE:.4f}')","056916ce":"# to make the RMSE to rebust to outliers we implement log from \nRMSElog = np.log(RMSE)\nRMSElog","f8090875":"# Model Accuracy, how often is the classifier correct?\naccuracy = r2_score(ytest, y_pred)\n#test_score = regressore.score(xtest,ytest)\n#print(train_score*100,'%')\nprint(f'Mean squared error: {accuracy:.4f}')","a76aa96d":"# define datafram\ndf_DR = df.copy(deep = True)\n# of features chosen divide it into two sections: target and features\nfeatures_DR = df_DR.drop(['id', 'player_fifa_api_id', 'player_api_id', 'date', 'overall_rating',\n      'preferred_foot', 'attacking_work_rate','defensive_work_rate'], axis=1)\nX_R = features_DR\ny_r = df['overall_rating']","4ab76678":" # define a Decision tree Regression model \n# model_DR =  DecisionTreeRegressor(max_depth = 10)\nmodel_DR =  DecisionTreeRegressor()\n# split data into training and testing data to train algorithm\nxtrain1, xtest1, ytrain1, ytest1  = train_test_split(X_R, y_r, test_size = 0.30, random_state = 49)","e149409e":"# define a time method to calculate the time consuming\nstart = timeit.default_timer()\n# fit the model with data\nmodel_DR.fit(xtrain1, ytrain1)\ny_pred2 = model_DR.predict(xtest1)\naccuracy = r2_score(ytest1, y_pred2)\nprint('Accuracy', round(accuracy, 3), '%.')\n      \nstop = timeit.default_timer()\nprint('Time: ', stop - start)      ","45461d30":"max_depth = []\nAccuracy = []\nmax_error = []\nfor i in range(1,30):\n    dtree = DecisionTreeRegressor( max_depth=i )\n    dtree.fit(xtrain, ytrain)\n    pred = dtree.predict(xtest)\n    accuracy = r2_score(ytest, pred)\n    Accuracy.append(accuracy)\n    max_depth.append(i)\n    #print(accuracy)\nd = pd.DataFrame({'Accuracy':pd.Series(Accuracy),\n                  \n                  'max_depth':pd.Series(max_depth)})\nplt.plot('max_depth','Accuracy', data=d, label='Accuracy')\n\nplt.xlabel('max_depth', color= \"white\")\nplt.ylabel('Accuracy', color = \"white\")\nplt.tick_params(axis='x', colors='white') \nplt.tick_params(axis='y', colors='white') \nplt.grid(axis = 'x')\nplt.legend()\n","e8cec77b":"min_samples_split = []\nAccuracy2 = []\nfor i in range(2,20,1):\n dtree = DecisionTreeRegressor( min_samples_split=i )\n dtree.fit(xtrain, ytrain)\n pred2 = dtree.predict(xtest)\n accuracy2 = r2_score(ytest, pred2)\n Accuracy2.append(accuracy2)\n ####\n min_samples_split.append(i)\nd = pd.DataFrame({'Accuracy2':pd.Series(Accuracy2), \n 'min_samples_split':pd.Series(min_samples_split)})\n# visualizing changes in parameters\nplt.plot('min_samples_split','Accuracy2', data=d, label='Accuracy2')\nplt.xlabel('min_samples_split')\nplt.ylabel('accuracy')\nplt.title(\"Min_Samples_split\", color = \"Red\")\nplt.grid(axis = 'x')\nplt.xlabel('min_samples_split', color= \"white\")\nplt.ylabel('Accuracy', color = \"white\")\nplt.tick_params(axis='x', colors='white') \nplt.tick_params(axis='y', colors='white')\nplt.legend()","a7b5ef70":"# Optimisation of model\nmodel_O =  DecisionTreeRegressor(min_samples_split = 2,max_depth= 11 )","bce6483f":"start = timeit.default_timer()\n# fit the model with data\nmodel_O.fit(xtrain1, ytrain1)\ny_pred_O = model_O.predict(xtest1)\naccuracy = r2_score(ytest1, y_pred_O)\nprint('Accuracy', round(accuracy, 3), '%.')\n      \nstop = timeit.default_timer()\nprint('Time: ', stop - start)  ","24ab5ee0":"from sklearn.ensemble import RandomForestRegressor\ndf_R = df.copy(deep = True)\n\n\n","e3ce295b":"# define random forest model\nmodel_RF = RandomForestRegressor(n_estimators = 10, random_state = 49)\n# removing some columns that are not numerical \nfeatures_R = df_R.drop(['id', 'player_fifa_api_id', 'player_api_id', 'date', 'overall_rating',\n      'preferred_foot', 'attacking_work_rate','defensive_work_rate'], axis=1)\nX = features_R\ny = df['overall_rating']\n# split our data to train and test by using sklearn.model_selection function\nxtrain, xtest, ytrain, ytest  = train_test_split(X, y, test_size = 0.30, random_state = 49)","eaddf5cc":"X.shape","359604ba":" # inport time to caculate time consuming of model\n\nstart = timeit.default_timer()\n# train model with dataset \nmodel_RF.fit(xtrain, ytrain)\n# evaluate the model and find accuracy\npredictions = model_RF.predict(xtest) \nerrors = abs(predictions - ytest)\nmape = np.mean(100 * (errors \/ ytest))\n\naccuracy = 100 - mape\nprint('Accuracy:', round(accuracy, 2), '%.')\n\nscore = model_RF.score(xtest,ytest)\nprint('Score:', round(score, 2), '%.')\nstop = timeit.default_timer()\n\nprint('Time: ', stop - start) ","ce557c33":"# finding the features that are more important for model \nfeature_imp = pd.Series(model_RF.feature_importances_,index=features_R.columns).sort_values(ascending=False)\n","56af8d74":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib import rcParams\n\n\n%matplotlib inline\n# Creating a bar plot\nsns.barplot(x=feature_imp, y=feature_imp.index)\n\n# Add labels to your graph\nplt.xlabel('Feature Importance Score', color = \"white\")\nplt.ylabel('Features', color = \"white\",)\nplt.title(\"Visualizing Important Features\", color = 'white')\nplt.tick_params(axis='x', colors='white') \nplt.gcf().set_size_inches(15, 10)\nfig.set_figheight(6)\nfig.set_figwidth(8)\nplt.tick_params(axis='y', colors='white')\n\nplt.legend()\n\nplt.show()","74a36884":"#  List of tuples with variable and importance\n#feature_importances = [(features, round(feature_imp,3)) for features, feature_imp in zip(feature_list, importances)]\n\ncumulative_importances = np.cumsum(feature_imp)\nprint('Number of features for 95% importance:', np.where(cumulative_importances > 0.95)[0][0] + 1)","e1f6a1d1":"importances = list(model_RF.feature_importances_)\nfeature_list = list(features_R)\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)","0f959ba2":"important_feature_names = [feature[0] for feature in feature_importances[0:14]]\nX2 = df_R[important_feature_names]\n\n\nxtrain2, xtest2, ytrain2, ytest2  = train_test_split(X2, y, test_size = 0.30, random_state = 49)\nregressor3 = RandomForestRegressor(n_estimators = 10, random_state = 49)","652c8f58":"X2.shape","3cce3c22":"start = timeit.default_timer()\nregressor3.fit(xtest2,ytest2)\nregressor3.score(xtest2,ytest2)\npredictions2 = regressor3.predict(xtest2) \nerrors2 = abs(predictions2 - ytest2)\nmape2 = np.mean(100 * (errors2 \/ ytest2))\naccuracy2 = 100 - mape2\nprint('Accuracy2:', round(accuracy2, 2), '%.')\n#Your statements here\nscore = regressor3.score(xtest2,ytest2)\nprint('Score:', round(score, 2), '%.')\nstop = timeit.default_timer()\n\nprint('Time: ', stop - start) ","22dee941":"df3.shape","6a105403":"# a owerview of our dataset \ndf3.describe().transpose()","c0cfac7f":"# a owerview of our dataset \ndf.describe()","88eaf950":"columns = ['id','country_id', 'league_id', 'stage', 'match_api_id', 'home_team_api_id', 'away_team_api_id', \n           'home_team_goal', 'away_team_goal', 'B365H', 'B365D', 'B365A', 'BWH', 'BWD', 'BWA', 'IWH', \n           'IWD', 'IWA', 'LBH', 'LBD', 'LBA', 'PSH', 'PSD', 'PSA', 'WHH', 'WHD', 'WHA', 'SJH', 'SJD', \n           'SJA', 'VCH', 'VCD', 'VCA', 'GBH', 'GBD', 'GBA', 'BSH', 'BSD', 'BSA' ]","31abbd7e":"X = df3.copy(deep = True)","b040a6f1":"X = X[columns]","4bc8d52e":"X.dropna(subset = columns, inplace = True)","f16a6ea4":"# define a function to determine the result of match with binary \ndef matchresult(homeScore, awayScore):\n    if(homeScore > awayScore):\n        return 1\n    elif(homeScore < awayScore):\n        return 2\n    else:\n        return 0","7d4bd04e":"X.columns","b2854fbe":"X['result'] = df3.apply(lambda r: matchresult(r['home_team_goal'], r['away_team_goal']), axis=1)\ny = X['result']\n#Discard the features, so as not to impair the accuracy of the model\nX = X.drop('result',1)\nX = X.drop('home_team_goal',1)\nX = X.drop('away_team_goal',1)","d234a95a":"#Discard the features, so as not to impair the accuracy of the model\nX = X.drop(['id', 'country_id', 'league_id', 'stage', 'match_api_id',\n       'home_team_api_id', 'away_team_api_id'], axis=1)","a7388fd6":"y.head(3)","38cb8211":"xtrain3,xtest3, ytrain3,ytest3 = train_test_split(X,y,test_size = 0.40, random_state= 41)","f9b6a8e0":"model = RandomForestClassifier()\nmodel.fit(xtrain3 ,ytrain3)","12d0e71d":"score = model.score(xtest3, ytest3)\nscore","0cb9dcf9":"feature_imp = pd.Series(model.feature_importances_,index=X.columns).sort_values(ascending=False)","195e6fd2":"# Represent the most representative characteristics for the model\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n# Creating a bar plot\nsns.barplot(x=feature_imp, y=feature_imp.index)\n# Add labels to your graph\nplt.xlabel('Feature Importance Score', color = \"white\")\nplt.ylabel('Features', color = \"white\")\nplt.title(\"Visualizing Important Features\", color = \"white\")\nplt.legend()\nplt.show()\n\n","7e7644ae":"def Evaluation_Models(X, y):\n   \n    #Name of the different classification algorithms used in the tests\n    names = [\"Decision Tree\", \"Random Forest\",  \"AdaBoost\", \"Naive Bayes\"]\n\n    classifiers = [\n        \n        DecisionTreeClassifier(),\n        RandomForestClassifier(),\n        AdaBoostClassifier(),\n        GaussianNB(),\n        ]\n    X_train_N, X_test_N, y_train_N, y_test_N = train_test_split(X, y, test_size=.40, random_state=0)\n    \n    # Iterar sobre clasificadores\n    for name, clf in zip(names, classifiers):\n        clf.fit(X_train_N, y_train_N)\n        score = clf.score(X_test_N, y_test_N)\n   \n        print(\"%s: \\n   %f\" % (name, score))","cf81a386":"Evaluation_Models(X, y)","cf517f36":"from sklearn import svm\nclf = svm.SVC(kernel='linear')\nclf.fit(xtrain3, ytrain3) \ny_pred = clf.predict(xtest3)\nfrom sklearn import metrics\n\n# Model Accuracy: how often is the classifier correct?\n","34367c17":"print(\"Accuracy:\",metrics.accuracy_score(ytest3, y_pred))","f50b1f5f":"# The Last way to evaluate is R Squared (R2):\n\nA score of R2 is a measure of your model's performance, which is known as the Coefficient of Determination.\n","cedfb188":"# First scenario\n\nThis study intends to uncover some players' skills that have a high association with overall rating and group them into comparable groups based on these characteristics. What if we could group players with 5 or 6 skills together and predict their groups only based on these skills? The first problem we encountered was determining how many groups are eligible for clustering. I used the Kmeans machine learning algorithm and the elbow approach to find the optimal number of clusters for the dataset to achieve this requirement.\n","d36f25e6":"The objective of this examnation is to identify competition outcomes based on variables in the match table that represent specific bookmarkers. The bookmarks website has a special rate and ratio for each match's result known odds. In this case, classifier methods are used to predict the outcome of each match based on these odds from different bookmarks. The match table has 115 components, including team information, match stats, and bookmark data. The bookmarks data and some team attributes were preserved to avoid a complex model. Furthermore, some features are removed in the data frame because they have many missed values.","be5db7fd":"### Optimasing DT model \n\nAccuracy is directly proportional to maximum depth; however, after 10 or 12 layouts, this line will not shift significantly, and Accuracy will be reduced by raising the \"min sample split.\"\nSo, in order to optimise our model, we choose 11 as the maximum depth and two as the minimum sample split , and the accuaracy of model is about 0.90, which is excellent which was far better than the previous model Linear Regression (0.78). The DT model works better and has more accuracy can be explained by its structure and employed Varian reduction and the \"squared_error\" as a criterion. Also, DT can use a technique such as a hiearachy system to split and separate input features into several isolated nodes to classify. Moreover, Linear regression generally assumes input features to be mutually independent, but DT has a different method. \n","3c3f4639":"Because the computer can only interpret numerical values from machine learning and training models, datasets must be labelled from text to numerical. In this case, the encoder function should be used to convert text values to numerical values, such as the 'preferred foot' columns, which have two distinct values, 'right' and 'left,' converted to 0 for right and 1 for left. ","36f4b453":"## cleaning data ","23013417":"# Conclusion\n\nThis project aimed to develop two different types of Machin learning algorithms(Supervised and Unsupervised) to analyse football players' abilities to predict overall rating and cluster players based on specific criteria connected to the overall rating and predict the outcome of matches bookmarks odds. For the first experiment, Linear regression was used to predict and demonstrate that we can easily predict overall rating with at least 15 talents. The model's accuracy was adequate (0.84), .but by removing the characteristics with high correlation, we were able to minimise the learning time. However, the accuracy goes down to (0.78). Different machine learning models were utilised for prediction in order to boost accuracy and comparison, the results of which can be found on page 18 of the Table. The k-mean algorithm splits the player into groups of 3 and 4 in the second experiment, based on the characteristics selected based on the overall ranking. The elbow method and Silhouette analysis aid to improve the model, and several features were tried, and finally, were decided to choose to select features from a different post of the player on the pitch with high correlation with an overall rating, and the features with high correlation to each other did not pick up. The final Analysis in the third scenario shows that we achieved a low accuracy (0.53) for estimating match outcomes based on bookmarks, indicating the necessity for extra information such as player traits or match data.","80d21bb9":"### Notice\nIn this case, as shown the Random forest was choose, and the model's accuracy and score were quite high. Random forest is also a strong feature selection method that may be used to identify the most essential and relevant input attributes for output. In other words, we can avoid employing too many features and lower not only the model's complexity but also the time it takes to analyse it.","ad91bc5c":"#### what MSE represent to us?\nactually this evaluate shows the squre distance between actual and predicted values. The MSE of zero (0) denotes flawless accuracy, which is not feasible. To avoid negative numbers in our output, we employ the square.\n","097408fc":"### Elbow Method \nin this method we can pich up the optimal number for k in the k-means which is really importance to reach a perfect model.","c240ca00":"**The K-means machine learning method and elbow strategy were employed to meet these criteria to discover the optimal number of clusters for the dataset. In addition, in this report, the Silhouette analysis is used to evaluate the k value we implement in this model.**","9391123f":"an example heatmap correlation between various input attributes, confirming that some statics are significantly connected (for example, Gk_handdling and Gk_possitioning and Marketing and sliding_tackle have Corr >.94  ). For deciding which features select to the input, we should guide from this map. The previous step aided me to reduce the complexity of the model and facilitated understanding. Figure 7 illustrate the relationship between Overall_rating and other features in a bar chart. This chart also needs to classify player features into similar groups based on the overall_rating.","c0bc6426":"##### in this examination I try to predict overall rating by using other features","0b73f31b":"# way 2  Random Forest\nRandom Forest is a prominent machine learning model that has been utilised in many academic articles for categorization challenges. Random forests can be utilised for regression work in addition to classification. A random forest's nonlinear nature might aid it with linear techniques, making it a fantastic option. In overall, when employed on large data sets, this technique produces superior results, and it can operate with missing data by estimating it. They do, however, provide a significant issue in that they are unable to extrapolate data that is not apparent ","d1287789":"# Secondly, Mean Squared Error(MSE) \nMSE is another way to evaluate our model which is mean squared error states that finding the squared difference between actual and predicted value.\n###### So, we calculated the absolute difference earlier, and now we're looking for the squared difference.\n\n","0d1868aa":"### what does means R2 score\nThe R2 score ranges from 0 to 1, with a value approaching zero indicating that the regression and mean lines are equal and that overlapping occurs. The model's performance is poor in this scenario. When the R2 is 1, the regression is perfect and error-free, however this is impossible in the actual world.","727007e7":"## implementing the Silhouette plot to analysis our model function","6eddb404":"I would be happy help me to improve with your comment \n\n> Github : (http:\/\/https:\/\/github.com\/amiraliz93)\n\n> Linkein: (http:\/\/linkedin.com\/in\/amir-alizadeh-041993)\n\nMy website: (http:\/\/https:\/\/amiralizadeh1992.wixsite.com\/user-guide)","cb09f9a8":"# Decision Tree Regression\n\nIn terms of machine learning, the Decision Tree algorithm is a classification and regression algorithm connected to the supervised learning algorithm. Regression trees are decision trees in which the target variable can take continuous values (Letham et al., 2015). It uses tagged datasets to train the algorithm to categorise data. Input data and proper outputs are included in training procedures, which allows the model to learn.\nID3 JR Quinlan's approach to building decision trees in regression, which uses the ID3 algorithm to construct decision trees by replacing information gain with standard deviation reduction, performs continuous top-down searches in stable and irreversible branch space.\n","9d29504f":"### Note:\n   - We train our model twice, the first time by deleting features with high correlation, in this case nine, to save time when analysing and avoid overfitting and a complex model.\n\n- In the second attempt, we used all characteristics save those that were of no use.","51f1eb9a":"#### What does  MAE represent to us?\nIt represents the average absolute difference between X and Y and also is comparable but not identical to root mean squared error (RMSE).\n\n##### Advantages of MAE\n- The MAE you get is in the same unit as the output variable.\n- It is most Robust to outliers.","10a19d22":"The output or target in this model is chosen \"overall_rating and\" the rest of the data as features. To reduce the complexity of the model, I remove some features with high correlation( corr>90). And some non-numerical features were removed, such as \"attacking_work_rate\", \"defensive_work_rate\", \"preferred_foot\". ","29e53e7f":"### Firstly, evaluate Mean Absolute Error(MAE) \nThe absolute difference between actual and predicted values is calculated using simple measures. One method for comparing predictions to their final results is to use mean absolute error(MAE). And well-established alternatives to the mean absolute scaled error (MASE) and the square mean error.","d06cc210":"# Correlation\n\nThe relationship among the attributes in datasets is significant and could be an excellent analysis guide. Reducing the high-correlation features in the data set is also one of the best strategies to improve a model. Related features increase the model's noise and inaccuracy, making it difficult to get the desired output. In the first examination, I aim to forecast the overall rating; thus, we should discover and delete features with solid correlations to avoid overcomplicating our model. \n","3d4b25d3":"# import the library that we need ","dff4f1a5":"the value 0.053 is shows that my model accuracy is good and my model working well","9f372c77":"# Now we want to evaluate our model in different ways","b8154407":"# Third Scenario","ea49c92b":"# Corrolation ","917cbdf8":"We'll use an arbitrary threshold of 95% here, but we can adjust it if it leads to poor performance. First, we must determine the exact number of traits that are more important than 95 percent of the time:","5901d76a":"\n##Distribution of Overall Rating","3b5fbbc7":"The elbow approach was used to identify the ideal number of clusters ","713b062d":"# Third evaluate are RMSE and RMSLE\nThe acronym RMSE indicates that it is a simple square root of mean squared error, This informs us the average gap between the model's predicted values and the dataset's actual values.The optimum score in evaluating is mainly reliant on the datasets we have available. The lower the RMSE, the better the model fits the data, although which number is lower depends on the dataset range.\n\nPros:\n- The output value is in the same unit as the desired output variable, which simplifies loss interpretation.\n\ncons: \n- It is not that robust to outliers as compared to MAE.","10c89737":"For example, the RSME number is 3.2, which is not bad given my range data. However, we can use this formula to normalise the RMSE number between 0 and 1 to have a better understanding of this value.\n### Normalized RMSE = RMSE \/ (max value \u2013 min value)","1a4ad9de":"# Visualasation\n","91d6e8ea":"# Second Scenario","25334168":"# Enhancing accuracy \nThere are three way to enhance a model and accuracy\n    - More (high-quality) data should be used, as well as feature engineering.\n    - Adjust the algorithm's hyperparameters.\n    - Experiment with different algorithms.\n       \nwe can impelement other model to enhance our model and accuracy such as SVM , DecisionTreeRegressor and RandomForestRegressor. Due to its vast size, the SVM model is not appropriate for this data set. This model takes too long to analyse and is only used for a small dataset on a regular basis.","8649c071":"### Features importance for third examination","354e7f5c":"# **ARTIFICIAL INTELLIGENCE IN SPORT**\n\n> Amir Alizadeh\n\n## Data description:\nThe advantage of having a high volume of data with many features is that we can look into various scenarios. This report's datasets are divided into seven tables: 'Player Attributes,' 'Country,' 'Match,' 'League,' 'Player,' 'Team,' and 'Team Attributes.' This database contains a collection of (players, teams) attributes* collected from EA Sports and spans ten seasons in European leagues between 2008 and 2016\n> This report picks up the Player Attributes to answer our queries. This Table includes 42 columns(attributes) and about 183000 rows.\n\n**Problems:**\n> -\tScenario 1:\n\nIn this scenario, I attend to train the regression model to predict the overall rating by utilising the other features and classifying players into different groups based on features that have a high correlation to Overall_rating. The linear regression was chosen for two main reasons. \n* Firstly, a Linear Regression algorithm is an excellent option for estimating the value of a variable based on the values of other attributes (About Linear Regression - United Kingdom | IBM, 2021). It means we have two types of variables: a dependent variable that is supposed to predict and an independent variable utilised to predict. \n* Second, the relationship between our features and Overall_rating is linear, appropriate for this model because features in linear regression should have a linear relationship.\n> Scenario 2 is:\n\nThe aim of the second examination is to categorise players into similar groups based on some characteristics. The initial question in this situation is which features are appropriate among the 42 input attributes in this data collection. My goal was to group players based on five characteristics that assigned them to similar groups with similar overall ratings.\n\n\n> -\tScenario 3:\n\nThe objective of this paper is to identify competition outcomes based on variables in the match table that represent specific bookmarkers. The bookmarks website has a special rate and ratio for each match's result known odds. In this case, classifier methods are used to predict the outcome of each match based on these odds from different bookmarks. "}}