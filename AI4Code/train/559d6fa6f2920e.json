{"cell_type":{"94a30c79":"code","4739123b":"code","4c85589f":"code","74584f17":"code","e69fe7a9":"code","eee8f1e2":"code","b5209288":"code","da7190e2":"code","63f138f4":"code","9c9924d5":"code","cebbe258":"code","93fede48":"code","59d1ccf0":"code","d8abfca9":"code","693a858f":"code","1b2a4434":"code","08dcf9ee":"code","bfa077e1":"code","1cb4997e":"code","08933103":"code","f725a48c":"code","c55dcb50":"code","f0c356a2":"code","635c5fbe":"code","181c5c2a":"code","bc6f4f36":"code","73bb6e73":"code","45b2836c":"code","63616959":"code","f2645be8":"code","1cffb566":"code","18053d46":"code","eb1d286b":"code","2da4f2c0":"code","46dc797a":"markdown","c92e488b":"markdown","44734937":"markdown","8abf7d6b":"markdown","694a2dfa":"markdown","5f3cc1a7":"markdown","4500faa4":"markdown","54ce90a1":"markdown","3cd82506":"markdown","87f36a88":"markdown","7f28ac71":"markdown","59255d38":"markdown","ba076401":"markdown","ab32be5e":"markdown","5b6b4a39":"markdown","4f2561ab":"markdown","ed62242b":"markdown"},"source":{"94a30c79":"# import numpy as np\n# from numba import jit\nclass DBSCAN:\n     \n    def __init__(self,epsilon,min_pts):\n        self.epsilon = epsilon\n        self.min_pts = min_pts\n    \n#     @jit(nopython = True)\n    def is_joint(self,clusters):\n        joint = np.zeros(shape=(len(clusters),len(clusters)))\n        for i in range(len(clusters)):\n            joint[i] = np.array([not clusters[i].isdisjoint(x) for x in clusters])\n        return np.unique(joint,axis=0)\n\n#     @jit(nopython = True)\n    def merge(self,clusters,joint):\n        classes = [set()] * joint.shape[0]\n        for i in range(joint.shape[0]):\n            classes[i] = set().union(*np.where(joint[i],clusters,set()))\n        return np.unique(classes)\n\n    def fit_transform(self,X,dist_mat):\n\n        # get neighbours of each point\n        neighbor = (dist_mat <= self.epsilon)\n\n        # calculate num of neighbours for each point\n        num_pts = neighbor.sum(axis=1,keepdims=True)\n\n        # get core pts\n        core_pts = (num_pts >= self.min_pts)\n\n        # get indecies of core pts\n        core_i = np.where(core_pts == True)[0]\n\n        # form local zones around each corepoint\n        clusters = []\n        for i in core_i:\n            cluster = set()\n            for j in range(neighbor.shape[0]):\n                if neighbor[i,j]:\n                    cluster.add(j)\n            clusters.append(cluster)\n\n\n        # merge local zones to form big clusters\n        ## calculate which zones are to be joined\n        joint_sets = self.is_joint(clusters)\n        # merge\n        temp = self.merge(clusters,joint_sets)\n        n_prev = len(clusters)\n        clusters = temp\n        while n_prev != len(clusters):\n            n_prev = clusters.shape[0]\n            j2 = self.is_joint(clusters)\n            clusters = self.merge(clusters,j2)\n\n        # label the data\n        y = np.zeros(X.shape[0])-1\n        for i in range(len(clusters)):\n            for j in clusters[i]:\n                y[j] = i\n        self.clusters = clusters\n        return y","4739123b":"class PCA:\n    def __init__(self, n_components):\n        self.n_components = n_components\n        self.M = 0\n    \n#     @jit(nopython = True)\n    def fit_transform(self, X):\n        self.M = np.mean(X,axis=0)\n        self.X_shifted = X - self.M\n        self.cov_mat = (self.X_shifted.T @ self.X_shifted)\/self.X_shifted.shape[1]\n        self.E_vals,self.E_V = eig(self.cov_mat)\n        Q = np.array([self.E_V[:,i] for i in np.argsort(self.E_vals)])\n        self.Q = np.real(Q[:,:self.n_components])\n        self.F_ = self.X_shifted @ self.Q\n        return self.F_","4c85589f":"import numpy as np\n# from sklearn.datasets import make_classification\nfrom sklearn.metrics import pairwise_distances\n# from sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import NearestNeighbors\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# from scipy.spatial import distance\nimport pandas as pd\nfrom pandas.plotting import parallel_coordinates\nfrom wordcloud import WordCloud\nfrom numpy.linalg import eig\n# Ahmed Rushdi 20180008\n# Mahmoud attia 20180256\n# %matplotlib widget\ndf = pd.read_csv('..\/input\/emails\/emails.csv')\n# df = pd.read_csv('emails.csv')\n# X, _ = make_classification(n_samples=2000,n_features=3,n_classes =4,n_informative=3,n_redundant=0,n_repeated=0,n_clusters_per_class=1,class_sep = 3,)#random_state=None)","74584f17":"cleanup_words = np.unique(np.array(list(map(lambda col: col * (len(col) < 3), df.columns))))[1:]\nprint(cleanup_words.shape)\ncleanup_words","e69fe7a9":"df = df.drop(cleanup_words, axis = 1)\ndf.head()","eee8f1e2":"# plt.figure(figsize=(900,5))\n# parallel_coordinates(df.drop(['Email No.'],axis=1),class_column='Prediction')","b5209288":"features = df.drop(['Email No.','Prediction'],axis = 1)\n# sns.pairplot(features.sample(30,axis=1,random_state=1))","da7190e2":"X = features.values\nX.shape","63f138f4":"features.sum(axis = 0).to_dict()\nwordcloud = WordCloud(width=8000, height=2000)\n\nwordcloud.generate_from_frequencies(features.sum(axis = 0).to_dict())\nplt.figure(figsize=(80,20))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","9c9924d5":"pca = PCA(5)\nX = pca.fit_transform(X)\nX.shape","cebbe258":"# scaler= StandardScaler()\n\n# scaler.fit(X)\n# X = scaler.transform(X)\n# plt.scatter(X[:,0],X[:,1],X[:,2],alpha = 0.6)","93fede48":"dist_matrix = np.zeros(shape=(X.shape[0],X.shape[0]))","59d1ccf0":"# %%time\n# for i in range(X.shape[0]):\n#     for j in range(X.shape[0]):\n#         dist_matrix[i][j] = distance.euclidean(X[i],X[j])","d8abfca9":"%%time\ndist_matrix = pairwise_distances(X)","693a858f":"plt.matshow(dist_matrix)","1b2a4434":"print('Mean dsitance: '+str(dist_matrix.mean()))\nprint('std dsitances: '+str(dist_matrix.std()))","08dcf9ee":"neigh = NearestNeighbors(n_neighbors=15)\nnbrs = neigh.fit(X)","bfa077e1":"distances, indices = nbrs.kneighbors(n_neighbors=15)\ndistances","1cb4997e":"distances = np.sort(distances.mean(axis = 1))\ndistances","08933103":"# %matplotlib widget\nplt.figure(figsize=(10,5))\nplt.plot(distances)\nplt.yticks(np.arange(min(distances), max(distances)+1, 1))\nplt.grid()\nplt.show() ","f725a48c":"dbscan = DBSCAN(epsilon=0.2,min_pts=15)\nlabels = (dbscan.fit_transform(X,dist_matrix) == -1).astype(int)\nnp.unique(labels)","c55dcb50":"df['labels'] = labels.astype(int)\nfeatures['labels'] = labels.astype(int)\ndf.head()","f0c356a2":"fig = plt.figure(figsize=(20,10),)\nax = fig.gca()\nax.set_xticks(np.unique(labels))\ndf.hist('labels',bins=np.unique(labels).shape[0],ax = ax)\ndf['labels'].value_counts()\n# sns.histplot(df['labels'], ax=ax)","635c5fbe":"cmap = sns.blend_palette(reversed([\"firebrick\", 'blue', 'yellow', \"green\", 'orange','black']), 2)\nsns.set_palette(cmap)\nsns.color_palette()","181c5c2a":"PCA_df = pd.DataFrame(X)\nPCA_df['Prediction'] = df['labels']\nplt.figure(figsize=(20,4))\nparallel_coordinates(PCA_df, class_column = 'Prediction',color=sns.color_palette())","bc6f4f36":"PCA_df.head()\n","73bb6e73":"# sns.scatterplot(x=0, y=1, data=PCA_df, hue=\"Prediction\")","45b2836c":"from sklearn.metrics import classification_report\nprint(classification_report(df['Prediction'],PCA_df['Prediction']))","63616959":"df['Prediction'].sum()","f2645be8":"labels.sum()","1cffb566":"cmap = sns.blend_palette(reversed([\"firebrick\", 'blue', 'yellow', \"green\", 'orange','black']), np.unique(labels).shape[0])\nsns.set_palette(cmap)\nsns.color_palette()","18053d46":"PCA_df = PCA_df.drop(['Prediction'],axis=1)\nPCA_df['labels'] = labels\nplt.figure(figsize=(20,4))\nparallel_coordinates(PCA_df, class_column = 'labels',color=sns.color_palette())","eb1d286b":"sns.pairplot(PCA_df,hue='labels',palette=sns.color_palette())","2da4f2c0":"for i in np.unique(labels):\n    print('Wordcloud: '+str(i))\n    cluster_wc = features.loc[features['labels'] == i].drop(['labels'],axis=1)\n    wordcloud_i = WordCloud(width=8000, height=2000)\n    wordcloud_i.generate_from_frequencies(cluster_wc.sum(axis = 0).to_dict())\n    plt.figure(figsize=(80,20))\n    plt.imshow(wordcloud_i)\n    plt.axis(\"off\")\n    plt.show()","46dc797a":"# Principle Component Analysis for dimensionality reduction.\n> reduce X to 15 vectors","c92e488b":"# Calculate distance matrix ","44734937":"> try values of epsilon where the curve's slope changes from low to high (near the elbow part)\n## Clustering","8abf7d6b":"# Cluster the data using DBSCAN","694a2dfa":"## Generate a wordcloud from the data","5f3cc1a7":"## Parallel Coordinates Plot with the PCA features","4500faa4":"## Pair plot with the PCA features","54ce90a1":"# Plotting\n> Here i try to make sense of all the different clusters","3cd82506":"# Imports and read file","87f36a88":"## PCA class","7f28ac71":"## Plot the distances\n","59255d38":"> Original labels","ba076401":"## Calculate mean distance between a point and it's neighbors","ab32be5e":"> Clustered data","5b6b4a39":"## Wordcloud for each cluster","4f2561ab":"## Extract relevant columns for clustering","ed62242b":"| Name         | ID       |\n|--------------|----------|\n| Ahmed Rushdi | 20180008 |\n| Mahmoud Atia | 20180256 |\n# DBSCAN implementation\n## DBSCAN class"}}