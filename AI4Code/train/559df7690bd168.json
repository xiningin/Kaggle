{"cell_type":{"760a9c63":"code","f6953d57":"code","807cfe20":"code","c889a4a2":"code","96e065cc":"code","6d02e275":"code","a1bbafdf":"code","ba0f1594":"code","b3b269c5":"code","98861bed":"code","a33af9bb":"code","8f5cc9ee":"code","3e672adc":"code","b2f27bf7":"code","78d6db1c":"code","b437fb33":"code","82670cb3":"code","5ae2ceb4":"code","d3b8edf0":"code","7777246e":"code","5dcccbe0":"code","a54e4a0b":"code","5d9b282b":"code","2ab39a97":"code","70b25b72":"markdown","07d6bfaf":"markdown","76120d60":"markdown","47049337":"markdown","277be5b8":"markdown","bbfff7fa":"markdown","b50617b0":"markdown","28ea29b2":"markdown","559868b0":"markdown","d956f532":"markdown","97763687":"markdown","d950b895":"markdown"},"source":{"760a9c63":"# Importing alll libraries and packages\n\nimport pandas as pd\nimport numpy as np\nimport scipy as sp\nimport sklearn\nimport sys\nfrom nltk.corpus import stopwords\nimport nltk\nfrom gensim.models import ldamodel\nimport gensim.corpora\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nimport pickle\nimport string\nfrom nltk.stem.wordnet import WordNetLemmatizer\nimport re\n# from gensim.test.utils import common_corpus, common_dictionary\n# from gensim.models.ldamodel import LdaModel\nfrom gensim.models.coherencemodel import CoherenceModel\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\nfrom gensim import matutils, models\nimport scipy.sparse\nfrom nltk import word_tokenize, pos_tag\n# from sklearn.feature_extraction import text","f6953d57":"import pandas as pd\nTrain=pd.read_csv('data.csv')\npd.set_option('display.max_colwidth',150)\nTrain.head(6)","807cfe20":"# removing userid email and numbers if present\nTrain['clean_text']=Train['text'].str.lower().apply(lambda x: re.sub(r'(@[\\S]+)|(\\w+:\\\/\\\/\\S+)|(\\d+)','',x))\n\n\n# removing stopwords and special character and returned lemmatized word\nstop = set(stopwords.words('english'))\nexclude = set(string.punctuation) \nlemma = WordNetLemmatizer()\ndef clean(doc):\n    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop and len(i)>1])\n    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n    return normalized\nTrain['clean_text']=Train['clean_text'].apply(lambda x: clean(x))\nTrain.head(6)","c889a4a2":"# We are going to create a document-term matrix using CountVectorizer, and exclude common English stop words\n\ncv = CountVectorizer(stop_words='english')\ndata_cv = cv.fit_transform(Train.clean_text)\ndata = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names(),index=Train['Id'])\n# data_dtm.index = Train['Id'].index","96e065cc":"# One of the required inputs is a term-document matrix\ntdm = data.transpose()\ndata.head()","6d02e275":"# We're going to put the term-document matrix into a new gensim format, from data --> sparse matrix --> gensim corpus\nsparse_counts = scipy.sparse.csr_matrix(tdm)\ncorpus = matutils.Sparse2Corpus(sparse_counts)","a1bbafdf":"# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\nid2word = dict((v, k) for k, v in cv.vocabulary_.items())\nid2word","ba0f1594":"# Fit the model for 5 topics\nlda = models.LdaModel(corpus=corpus, num_topics=5, id2word=id2word, passes=100,eta=.90)","b3b269c5":"# get the list of top 20 words in each topic after applying LDA model\ndef get_lda_topics(model, num_topics,num_words):\n    word_dict = {}\n    topics = model.show_topics(num_topics,num_words)\n    word_dict = {'Topic '+str(i):[x.split('*') for x in words.split('+')] \\\n                 for i,words in model.show_topics(num_topics,num_words)}\n    return pd.DataFrame.from_dict(word_dict)\n\nget_lda_topics(lda,5,20)","98861bed":"corpus_transformed = lda[corpus]\n# getting topic having maximum score for a document\ntopic=[]\nfor i in range(len(corpus_transformed)):\n    v=dict(corpus_transformed[i])\n    for top, score in v.items():  # for name, age in dictionary.iteritems():  (for Python 2.x)\n        if score == max(v.values()):\n            topic.append(top)","a33af9bb":"id_topics=pd.DataFrame([a for a in topic],index=data.index)\nid_topics.columns=['Topics']\n\n# \"glassdoor_reviews\"\n# \"tech_news\"\n# \"room_rentals\"\n# \"sports_news\"\n# \"Automobiles\"","8f5cc9ee":"id_topics['topic']=np.where(id_topics['Topics']==0,'tech_news',\n                                np.where(id_topics['Topics']==1,'glassdoor_reviews',\n                                        np.where(id_topics['Topics']==2,'sports_news',\n                                                np.where(id_topics['Topics']==3,'Automobiles','room_rentals'))))","3e672adc":"# id_topics.head(20)\nfinal=id_topics.reset_index()\nfinal=final[['Id','topic']]\nfinal.to_csv('final_output12.csv',index=False)\nfinal\n\n\n## score .94197\n## Score .9617\n\n# This is the final output csv having ID and Topic column in it","b2f27bf7":"# set Id as an index in data frame\nTrain=Train.set_index('Id')\nTrain.head(4)","78d6db1c":"# Let's create a function to pull out nouns from a string of text\ndef nouns_adj(text):\n    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n    is_noun_adj = lambda pos: pos[:2].startswith('N') or pos[:2].startswith('J')\n    tokenized = word_tokenize(text)\n    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n    return ' '.join(nouns_adj)","b437fb33":"# Apply the nouns function to the transcripts to filter only on nouns\ndata_nouns_adj = pd.DataFrame(Train.clean_text.apply(nouns_adj))\ndata_nouns_adj.head(4)","82670cb3":"# Creating sparse matrix with ttems as columns and ids as index\ncvna = CountVectorizer(stop_words=stop, max_features = 5000, max_df=.8)\ndata_cvna = cvna.fit_transform(data_nouns_adj.clean_text)\ndata_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names())\ndata_dtmna.index = data_nouns_adj.index\ndata_dtmna.head(4)","5ae2ceb4":"# Create the gensim corpus\ncorpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n\n# Create the vocabulary dictionary\nid2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())","d3b8edf0":"# Create LDA model with 5 topics, number of passes is given 50 to get fine tuned result\nldana = models.LdaModel(corpus=corpusna, num_topics=5, id2word=id2wordna, passes=100,eta=.90)\nget_lda_topics(ldana,5,20)","7777246e":"corpus_transformed = ldana[corpusna]\n# getting topic having maximum score for a document\ntopic=[]\nfor i in range(len(corpus_transformed)):\n    v=dict(corpus_transformed[i])\n    for top, score in v.items():  \n        if score == max(v.values()):\n            topic.append(top)","5dcccbe0":"# Get the topic the each document contains\n\nid_topics=pd.DataFrame([a for a in topic],index=data_dtmna.index)\nid_topics.columns=['Topics']\n# id_topics.head(6)\n\n# \"glassdoor_reviews\"\n# \"tech_news\"\n# \"room_rentals\"\n# \"sports_news\"\n# \"Automobiles\"","a54e4a0b":"id_topics['topic']=np.where(id_topics['Topics']==0,'sports_news',\n                                np.where(id_topics['Topics']==1,'glassdoor_reviews',\n                                        np.where(id_topics['Topics']==2,'Automobiles',\n                                                np.where(id_topics['Topics']==3,'room_rentals','tech_news'))))","5d9b282b":"id_topics.head(20)\nfinal=id_topics.reset_index()\nfinal=final[['Id','topic']]\nfinal.to_csv('final5.csv',index=False)\nfinal\n\n# 0.91267\n# 0.89135\n# Final output having Noun and Adjective in Document\n# After comparing both results in this notebook, publish the one having highest score ","2ab39a97":"# Perplexity, lower the better.\n# print('\\nPerplexity: ', lda.log_perplexity(corpusna))  \n# # Coherance score, higher is better\n# coherence_model_lda = CoherenceModel(model=lda, texts=Train['clean_text'], dictionary=id2word, coherence='c_v')\n# coherence_lda = coherence_model_lda.get_coherence()\n# print('\\nCoherence Score: ', coherence_lda)","70b25b72":"Since my data cleaning has done now we'll proceed further to apply topic modeling technique using LDA method.\n\n\n### 3. Create Document term metrics and dictionaru of terms","07d6bfaf":"*This order will change when we rerun thhis code*\n\nBy looking at top 20 words from each topic it seams\n\n* Words in Topic 0 belong to automobiles\n* words in Topic 1 related to sports news\n* Words in Topic 2 related to room rental\n* Words in topic 3 also seem seem related to room rental\n* Words in topic 4 seem related to glassdoor reviews\n\nWe don't have a very clear result about topic and it seems there is repetition of topic so to tune it more fine we would consider Nouns and Adjective only","76120d60":"### To get a better result select only Noun and Adjective and rerun above code again","47049337":"As we can see perplexity is pretty good low and cohererance is high so we are going in good direction","277be5b8":"### 4.Fitting the LDA model","bbfff7fa":"## Topic Modeling\n\n### Introduction\n\nAnother popular text analysis technique is called topic modeling. The ultimate goal of topic modeling is to find various topics that are present in your corpus. Each document in the corpus will be made up of at least one topic, if not multiple topics.\n\nIn this notebook, we will be predicting topic using Latent Dirichlet Allocation (LDA), which is one of many topic modeling techniques. It was specifically designed for text data.\n\nTo use a topic modeling technique, you need to provide (1) a document-term matrix and (2) the number of topics you would like the algorithm to pick up.","b50617b0":"### 2. Data cleaning process\n\nIn data cleaning process we'll be removing stopwords, special character,numbers and user email id (if present) which make our data clean to get the important words to decide the topic.","28ea29b2":"* Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term), we need to specify two other parameters - the number of topics and the number of passes. ****Since we have been given number of topics so we'll specify number of passes ****","559868b0":"*This order will change once we rerun this code*\n\nBy looking at words in all topics it seems we have more fine tned topics.\n\n* Topic 0 seems realted to glassdoor reviews\n* Topic 1 seems related to room rental\n* Topic 2 seems realted to sports news\n* Topic 3 seems related to automobile\n* Topic 4 seems related to tech news\n","d956f532":"To measure how good our model is, we can use metrics like 'Perplexity' and 'Coherence'\n\n#### Coherence\n* Topic Coherence measures score a single topic by measuring the degree of semantic similarity between high scoring words in the topic. These measurements help distinguish between topics that are semantically interpretable topics and topics that are artifacts of statistical inference\n\n#### Perplexity\n* perplexity is a measurement of how well a probability distribution or probability model predicts a sample. It may be used to compare probability models. A low perplexity indicates the probability distribution is good at predicting the sample.\n","97763687":"The objective of this competition is to identify the theme around the given corpus and categorize it accordingly.","d950b895":"### 1.Loading the dataset"}}