{"cell_type":{"77931ba0":"code","ea4fbf9f":"code","25efc9b5":"code","2e833741":"code","1005cec5":"code","2c3b6cc3":"code","a3eb8334":"code","5243ff47":"code","26eda557":"code","1360509b":"code","32478715":"markdown","72bf7b60":"markdown","a866f17f":"markdown","d260c87e":"markdown","b487d8b2":"markdown","f6954533":"markdown","7dbd17e6":"markdown","b22d2193":"markdown","a04a07b4":"markdown","b0985319":"markdown","a9d7bbf5":"markdown","ffcb4d23":"markdown"},"source":{"77931ba0":"import pandas as pd\n\nfrom tabulate import tabulate\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\n# we import the three main functions from the utility script for scoring, training and prediction\nfrom quick_classification import score_models\nfrom quick_classification import train_models\nfrom quick_classification import predict_from_models\n\nBASE = \"\/kaggle\/input\"","ea4fbf9f":"X, y = make_classification(n_samples=1_000, \n                           n_features=20,\n                           n_informative=10, \n                           n_classes=2)\n\nX, X_test, y, _ = train_test_split(X, y)\n\ndf = pd.DataFrame(X)\ndf[\"target_variable\"] = y\ndf_test = pd.DataFrame(X_test)\n\nscores = score_models(df, \"target_variable\", verbose=True)\ndisplay(scores)","25efc9b5":"pipelines = train_models(df, \"target_variable\")\npredictions = predict_from_models(df_test, pipelines)\npredictions.head()","2e833741":"data = load_breast_cancer()\nX = pd.DataFrame(data[\"data\"])\nX[\"target_variable\"] = pd.DataFrame(data[\"target\"])\nscore_models(X, \"target_variable\", verbose=False)","1005cec5":"data = load_iris()\nX = pd.DataFrame(data[\"data\"])\nX[\"target_variable\"] = pd.DataFrame(data[\"target\"])\nscore_models(X, \"target_variable\", verbose=False)","2c3b6cc3":"df = pd.read_csv(f\"{BASE}\/titanic\/train.csv\", )\nscore_models(df, \"Survived\")","a3eb8334":"pipelines = train_models(df, \"Survived\", verbose=False)\n\ndf_test = pd.read_csv(f\"{BASE}\/titanic\/test.csv\")\npredictions = predict_from_models(df_test, pipelines)\npredictions.head()","5243ff47":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score","26eda557":"df = pd.read_csv(f\"{BASE}\/titanic\/train.csv\")\nX = df.select_dtypes(\"number\").drop(\"Survived\", axis=1)\ny = df.Survived\n\n# using the convenience function make_pipeline() to build a whole data pipeline in just one line of code\npipe = make_pipeline(SimpleImputer(), RobustScaler(), LogisticRegression())\nprint(f\"The accuracy is: {cross_val_score(pipe, X, y).mean():.4f}\")","1360509b":"num_cols = df.drop(\"Survived\", axis=1).select_dtypes(\"number\").columns\ncat_cols = df.select_dtypes(\"object\").columns\n\n# we instantiate a first Pipeline, that processes our numerical values\nnumeric_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer()),\n        ('scaler', RobustScaler())])\n\n# the same we do for categorical data\ncategorical_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='most_frequent', fill_value='missing')),\n        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n    \n# a ColumnTransformer combines the two created pipelines\n# each tranformer gets the proper features according to \u00abnum_cols\u00bb and \u00abcat_cols\u00bb\npreprocessor = ColumnTransformer(\n        transformers=[\n            ('num', numeric_transformer, num_cols),\n            ('cat', categorical_transformer, cat_cols)])\n\npipe = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', LogisticRegression())])\n\nX = df.drop(\"Survived\", axis=1)\ny = df.Survived\nprint(f\"The accuracy score is: {cross_val_score(pipe, X, y).mean():.4f}\")\n","32478715":"Now on to training and prediction... With just one more line we train all classifiers on the full training set. The function returns the fitted scikit Pipelines.\n\nWe can use these in the next step to predict from the test data. Just be aware: The first column are the predictions from the DummyRegressor. This will very likely spoil your result... \ud83d\ude09","72bf7b60":"The same we can setup with two pipeline branches for numerical and categorical data.","a866f17f":"# Demo Notebook for quick_classification utility script","d260c87e":"<font color=\"darkred\">**The script simply abstracts all this away and in addition takes care of instantiating the classifiers, crossvalidation, training and prediction.**","b487d8b2":"## Behind the scenes\nAt the core of the util script I used scikit-learn's pipeline class. This allows to chain arbitrary transformers with a final estimator. Let's look at a simple example. ","f6954533":"Let's try a standard dataset from scikit-learn.","7dbd17e6":"We now can very easily fit all classifiers on the whole training set and make predictions.","b22d2193":"### References\n\n[Alexis' Kaggle Tutorial](https:\/\/www.kaggle.com\/alexisbcook\/pipelines)<br>\n[Dan Becker's Pipeline Tutorial](https:\/\/www.kaggle.com\/dansbecker\/pipelines)<br>\n[Using the Column Transformer](https:\/\/scikit-learn.org\/stable\/auto_examples\/compose\/plot_column_transformer_mixed_types.html)<br>","a04a07b4":"And a second one.","b0985319":"Let's see how robust our function is. We just throw the Titanic dataset at it \u2013 without any modification.","a9d7bbf5":"### <font color=\"darkred\">What if we could score 12+ common classification algorithms on our data with just one line of code? \n\nFor the Ames Housing Price competition I experimented with a scikit Pipeline to automically clean and prepare the data, handle numerical and categorical values and crossvalidate on the most common classifiers being used by fellow Kagglers. See the complete notebook here:\n\nhttps:\/\/www.kaggle.com\/chmaxx\/sklearn-pipeline-playground-for-10-classifiers\n\nThis experimental playground I extended to this small utility script that can be used on any training data for a regression problem:\n\nhttps:\/\/www.kaggle.com\/chmaxx\/quick-regression\n\nNow I adapted this function to **classification** problems as well:\n\nhttps:\/\/www.kaggle.com\/chmaxx\/quick-classification","ffcb4d23":"Let's try the script on a first synthetic classification."}}