{"cell_type":{"af6bc2ab":"code","48c28468":"code","12fea2fc":"code","2f5461b6":"code","762f7355":"code","4e89fbae":"code","1cd6b2d9":"code","410fac6f":"code","b74851c8":"code","4f31f970":"code","4d6c7c0b":"markdown","fee0c7d5":"markdown","1f9c3b59":"markdown","5c246df4":"markdown","de612f27":"markdown"},"source":{"af6bc2ab":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","48c28468":"%matplotlib inline\nplt.style.use('fivethirtyeight')","12fea2fc":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntrain.info()","2f5461b6":" def Preprocess(train):\n    try:\n        # Dropping Cabin and Ticket \n        train.drop('Cabin', axis = 1,inplace = True)\n        train.drop('Ticket', axis = 1, inplace = True)\n\n        # Converting SibSp and Parch into Family Size and dropping them\n        train['Family_Size'] = train['SibSp'] + train['Parch']\n        train.drop(['SibSp','Parch'],axis = 1,inplace= True)\n\n        # Extracting the title from the Name\n        train['Title'] = np.nan\n        for i in range(len(train)):\n            title = train['Name'][i].split(',')[1].split('.')[0].strip()\n            train['Title'][i] = title \n\n        # Dropping the Null Values and resetting the PassengerID\n        train = train.dropna().reset_index(drop = True)\n        train.drop('PassengerId',axis = 1,inplace = True)\n        cols = ['PassengerId','Title','Pclass', 'Sex', 'Age', 'Fare', 'Embarked','Family_Size','Survived']\n        train['PassengerId'] = train.index\n        train = train[cols]\n\n        # Converting gender to categorical\n        train['Sex'] = train['Sex'].map({'male':0,'female':1})\n\n        # Converting Embarked to categorical\n        train['Embarked'] = train['Embarked'].map({'S':0, 'C':1, 'Q':2})\n\n        # Converting Title to categorical\n        train['Title'] = train['Title'].map({'Mr':0, 'Mrs':1, 'Miss':2, 'Master':3, 'Don':4, 'Rev':5, 'Dr':6, 'Mme':7, 'Ms':8,'Major':9, 'Lady':10, 'Sir':11, 'Mlle':12, 'Col':13, 'Capt':14, 'the Countess':15,'Jonkheer':16})\n    \n        return train\n\n    except:\n        print('Error! \\n Check if the data is already processed ')\n\n    \ntrain_processed= Preprocess(train)\ntrain_processed.head()","762f7355":"X_train, X_test, y_train, y_test = train_test_split(train_processed.drop('Survived',axis = 1),\n                                                    train_processed['Survived'],\n                                                    test_size = 0.2, \n                                                    random_state = 42)","4e89fbae":"data = [X_train,X_test,y_train,y_test]\nfor x in data:\n    x = np.asarray(x).astype('float32') ","1cd6b2d9":"model = keras.models.Sequential([\n    keras.layers.Dense(64, activation = 'relu', input_shape = X_train.shape[1:]),\n    keras.layers.Dense(1,activation='sigmoid')\n])\n\nearly_stopping_cb = keras.callbacks.EarlyStopping(patience = 20, restore_best_weights = True)\n\nmodel.compile(loss = 'binary_crossentropy',optimizer='adam', metrics = ['accuracy'])\n\nhistory = model.fit(X_train,y_train,epochs = 500, validation_data = (X_test,y_test), callbacks = [early_stopping_cb])","410fac6f":"model.evaluate(X_test,y_test)","b74851c8":"#Function to plot the data from history(model)\n\ndef plot_graphs(history, string):\n  plt.plot(history.history[string])\n  plt.plot(history.history['val_'+string])\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(string)\n  plt.legend([string, 'val_'+string])\n  plt.show()","4f31f970":"plot_graphs(history, 'accuracy')\nplot_graphs(history, 'loss')","4d6c7c0b":"## Final Accuracy and loss plots","fee0c7d5":"## Reading the data and Preprocessing","1f9c3b59":"**In this notebook, I have tried to implement a Deep Neural Network on the famous Titanic Dataset with accuracy close to 80%.**\n\n* First I have preprocessed the dataset extracting and converting the required content from the data.\n* Next the data is passed through the model and trained on 500 epochs with Early Stopping Callback.\n\n*There might be many reasons for this accuracy including the low data availiability or more fine-tuning required, if you have any suggestion on how I can improve the accuracy please feel free to drop it in the comments and I'll try to implement it in this code*","5c246df4":"## Importing the required Files","de612f27":"## Training DNN"}}