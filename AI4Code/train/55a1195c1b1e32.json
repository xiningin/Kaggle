{"cell_type":{"484c9560":"code","6dc82411":"code","18d14fb5":"code","0e4cf9b3":"code","cfb52777":"code","66a23800":"code","0f975a78":"code","4c991589":"code","53f503d5":"code","b63a66b6":"code","36be8671":"code","a25ecaa0":"code","730ae01d":"code","161add17":"code","6f59c681":"code","8398a8ae":"code","154b1842":"code","15bde982":"code","02f362a8":"code","769321ce":"code","00ad8bc5":"code","889c6c5f":"code","b2485bb3":"code","caa911ce":"code","f38333b7":"code","4db74769":"code","2f9fa3ab":"code","ee5320fc":"code","e07534cf":"code","85a89305":"code","6a2bec8a":"code","f67a01a9":"code","c756a219":"code","a36e7cd6":"code","ca20953b":"code","e6391e7b":"code","aed34dea":"code","b3e4e058":"code","a52c7526":"code","21c918d0":"code","1616a1b2":"code","88e9c1d7":"code","1c7c5c5c":"code","29f9c183":"code","3372bf19":"code","9c23865c":"code","d38a728e":"code","0d7986e5":"code","66dd02e3":"code","78005e6f":"code","2fe4acdc":"code","ea958c8c":"code","0357ea08":"code","efba9ee1":"code","34494ea9":"code","702ae20c":"markdown","5bd99142":"markdown","96678fab":"markdown","f3afae89":"markdown","116ac8ef":"markdown","e5bebbe1":"markdown","6e6367c2":"markdown","9b9cdc14":"markdown","9b5125b8":"markdown","800c7f52":"markdown","971bb6d3":"markdown","6448704a":"markdown"},"source":{"484c9560":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom scipy import stats\nimport warnings\nwarnings. simplefilter(action='ignore', category=Warning) ","6dc82411":"data = pd.read_csv(\"..\/input\/wine-components-dataset\/wine.csv\")","18d14fb5":"data.head(5)","0e4cf9b3":"data.describe()","cfb52777":"df = data.copy()","66a23800":"df","0f975a78":"df = df[['Alcohol', 'Proline', 'Alcalinity']]","4c991589":"def plot_Before_logerthemic(df,var):\n    plt.figure(figsize = (10,4))\n    \n    val = np.random.randint(100000,999999)\n    col = \"#\" + str(val)\n\n    sns.distplot(df[var], color = col, label = var)\n    plt.legend()\n\n    plt.plot()","53f503d5":"for i in df.columns:\n    print(i, \":\", df[i].std())","b63a66b6":"for i in df.columns:\n    plot_Before_logerthemic(df,i)","36be8671":"df = np.log(df)","a25ecaa0":"for i in df.columns:\n    print(i, \":\", df[i].std())","730ae01d":"def plot_After_logerthemic(df,var):\n    plt.figure(figsize = (10,4))\n    \n    val = np.random.randint(100000,999999)\n    col = \"#\" + str(val)\n\n    sns.distplot(df[var], color = col, label = var)\n    plt.legend()\n\n    plt.plot()","161add17":"for i in df.columns:\n    plot_After_logerthemic(df,i)","6f59c681":"df = data.copy()","8398a8ae":"df = df[['Proline']]","154b1842":"def Trans(df,var):\n    df[var + \"_Reci_trans\"] = 1\/(df[var]+1)\n    df[var + \"_SquareRt_trans\"] = df[var] ** (1\/2)\n    df[var + \"_Exp_trans\"] = np.exp(df[var])\n    df[var + \"BoxCox\"], param = stats.boxcox(df[var]+1) ","15bde982":"Trans(df,'Proline')","02f362a8":"df","769321ce":"for i in df.columns:\n    print(i, \":\", df[i].std())","00ad8bc5":"def plot_Trans(df,var):\n    plt.figure(figsize = (10,4))\n    \n    val = np.random.randint(100000,999999)\n    col = \"#\" + str(val)\n\n    sns.distplot(df[var], color = col, label = var)\n    plt.legend()\n\n    plt.plot()","889c6c5f":"### Because of Proline_Exp_trans has inf values, ignore them","b2485bb3":"for i in df.columns:\n    if i == 'Proline_Exp_trans': \n        continue\n    plot_Trans(df,i)","caa911ce":"df = data.copy()","f38333b7":"df = df[['Alcohol', 'Malic', 'Hue', 'Nonflavanoids']]","4db74769":"df","2f9fa3ab":"from sklearn.preprocessing import MinMaxScaler","ee5320fc":"scaling = MinMaxScaler()","e07534cf":"def norm(df,var):\n    df[var + \"_norm\"] = scaling.fit_transform(df[[var]])","85a89305":"for i in df.columns:\n    norm(df,i)","6a2bec8a":"df.head(10)","f67a01a9":"def plot_for_norm(df,var):\n    plt.figure(figsize = (10,4))\n    \n    val = np.random.randint(100000,999999)\n    col = \"#\" + str(val)\n    plt.subplot(1,2,1)\n    sns.distplot(df[var], color = col, label = var)\n    plt.legend()\n    plt.subplot(1,2,2)\n    stats.probplot(df[var], dist = \"norm\", plot = plt)\n\n    plt.plot()","c756a219":"for i in df.columns:\n    plot_for_norm(df,i)","a36e7cd6":"## In some case after this transformation, scaling is needed. \n## In other cases scaling only needed. Based on the variable and the algorithm, transformation is made","ca20953b":"df = data.copy()","e6391e7b":"df","aed34dea":"df = df[['Ash', 'Magnesium', 'Flavanoids', 'Color']]","b3e4e058":"df","a52c7526":"from sklearn.preprocessing import StandardScaler","21c918d0":"scaler = StandardScaler()","1616a1b2":"def stan(df,var):\n    df.loc[: , var + \"_scale\"] = scaler.fit_transform(df[[var]])\n","88e9c1d7":"for i in df.columns:\n    stan(df,i)","1c7c5c5c":"def plot(df,var):\n    plt.plot(figsize = (10,4))\n    \n    val = np.random.randint(100000,999999)\n    col = \"#\" + str(val)\n    plt.subplot(1,2,1)\n    sns.distplot(df[var], color = col, label = var)\n    plt.legend()\n    plt.subplot(1,2,2)\n    stats.probplot(df[var], dist = \"norm\", plot = plt)\n    plt.show()    \n\n    ","29f9c183":"for i in df.columns:\n    plot(df,i)","3372bf19":"df","9c23865c":"df = data.copy()","d38a728e":"df","0d7986e5":"df = df[['Alcohol', 'Phenols', 'Flavanoids']]","66dd02e3":"from sklearn.preprocessing import RobustScaler","78005e6f":"scaler = RobustScaler()","2fe4acdc":"def robust(df,var):\n    df[var + \"_Median_quantile\"] = scaler.fit_transform(df[[var]])","ea958c8c":"for i in df.columns:\n    robust(df,i)","0357ea08":"df","efba9ee1":"def plot(df,var):\n    plt.plot(figsize = (10,4))\n    \n    val = np.random.randint(100000,999999)\n    col = \"#\" + str(val)\n    plt.subplot(1,2,1)\n    sns.distplot(df[var], color = col, label = var)\n    plt.legend()\n    plt.subplot(1,2,2)\n    stats.probplot(df[var], dist = \"norm\", plot = plt)\n    plt.show()    \n\n    ","34494ea9":"for i in df.columns:\n    plot(df,i)","702ae20c":"# Standardization","5bd99142":"### why we need Transformation?\n\nFor Linear Regression, Gradient Descent & for Global minima, it used\n\nAlgorithm like KNN, K means, Hieracial clustering --- impact on Euclidean distance \n\nD.L techniques like CNN, ANN and RNN","96678fab":"# Applying all Exponential, reciprocal, and square root transformation","f3afae89":"# In this, we see about Transformations\n\n1) Transformations:\n    \n    Logerthemic Transformation\n    \n    Exponential Transformation \n    \n    Reciprocal Transformation \n    \n    square root transformation\n\n2) Scaling:\n\n    Normalization\n\n    Standardisation\n    \n    Robost Scaling","116ac8ef":"# Normalization","e5bebbe1":"# Logerthemic Transformation","6e6367c2":"for more...\n\n**Handling Categorical data's (Feature Engineering)**\nhttps:\/\/www.kaggle.com\/ganeshbalaji1608\/handling-categorical-data-s-feature-engineering\n\n**Handling Missing Categories (Feature Engineering) **\nhttps:\/\/www.kaggle.comhandling-missing-categories-feature-engineering\n\n**Handling Missing Numerical Features(Feature Engineering)**\nhttps:\/\/www.kaggle.comhandling-missing-numerical-features-f-e\n\n**Handling Imbalanced Datasets **\nhttps:\/\/www.kaggle.comhandling-imbalanced-datasets\n\n**Handling Outliers**\nhttps:\/\/www.kaggle.comhandling-outliers-feature-engieering\n","9b9cdc14":"First analyze the dataset given","9b5125b8":"from all above boxcox plays a role in this variable...\n","800c7f52":"# Scaling to Median and Quantiles (Robust Scaling)","971bb6d3":"#Comparing Before and after Logerthemic Transformation, values become normalized and follows the bell curve transformation\n#for this, model will read the variables easily, and prediction accuracy become high","6448704a":"Before Scaling The variable, apply some transformation for better result"}}