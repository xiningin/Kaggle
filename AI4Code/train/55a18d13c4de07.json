{"cell_type":{"8f335bfe":"code","d0b60fd9":"code","5013d374":"code","0821a40d":"code","7b9c372b":"code","b81d89d1":"code","96056cc5":"code","83b7338f":"code","2bbb33ac":"code","1189aaa8":"code","bcab6bd0":"code","4d8b1354":"code","db15968f":"code","af97cf07":"code","b90c4358":"code","85c03cca":"code","ce8f19c1":"code","ccde95ae":"code","f04299d2":"code","3e147404":"code","287a7490":"code","3f58c62e":"code","24ce0644":"code","2e3ec826":"code","6323195a":"code","f67e45c8":"code","d413c0ba":"code","e52eb169":"code","6f9fdd86":"code","3e442296":"code","bf5994a5":"markdown","70362fe3":"markdown","b2d7b334":"markdown","a420ff4a":"markdown","eb75ae79":"markdown"},"source":{"8f335bfe":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d0b60fd9":"# Importing the data set\ndf=pd.read_csv('..\/input\/vehicle-dataset-from-cardekho\/car data.csv')\ndf.head()","5013d374":"# Checking for null values\ndf.isnull().sum()","0821a40d":"# Checking the unique values of categorical feature\nprint(df['Fuel_Type'].unique())\nprint(df['Seller_Type'].unique())\nprint(df['Transmission'].unique())\nprint(df['Owner'].unique())","7b9c372b":"# We will remove car name from our data set because it will not have any mathematical significance\nfinal_dataset=df[['Year', 'Selling_Price', 'Present_Price', 'Kms_Driven',\n       'Fuel_Type', 'Seller_Type', 'Transmission', 'Owner']]\n# Adding a column to show the age of the car. Will take reference year as 2020\nfinal_dataset['Age']=2020-final_dataset['Year']\n# Dropping the year column since we have captured that information in the Age column\nfinal_dataset.drop(['Year'],axis=1,inplace=True)\nfinal_dataset.head()","b81d89d1":"# Encoding the catgorical features\nfrom sklearn.preprocessing import LabelEncoder\nlb=LabelEncoder()\n\nfor col in final_dataset.columns:\n    if final_dataset[col].dtypes=='O':\n        final_dataset[col]=lb.fit_transform(final_dataset[col])\nfinal_dataset.head()","96056cc5":"# Finding correlation\nfinal_dataset.corr()","83b7338f":"# Visual representation of correlation\nimport seaborn as sns\nsns.pairplot(final_dataset)","2bbb33ac":"# Correlation heatmap\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.figure(figsize=(20,20))\nsns.heatmap(data=final_dataset.corr().round(2), annot=True)","1189aaa8":"# Seperating the dependent and independent features\nX=final_dataset.drop(['Selling_Price'],axis=1)\ny=final_dataset['Selling_Price']","bcab6bd0":"#Splitting into test and train data\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=355)","4d8b1354":"# RandomForest regressor\nfrom sklearn.ensemble import RandomForestRegressor\nmodel1=RandomForestRegressor()","db15968f":"#Hyper-parameters\n\n# No. of trees in Random Forest\nn_estimators=[int(x) for x in np.linspace(start=100,stop=1200,num=12)]\n# No. of forests to consider at every split\nmax_features=['auto','sqrt']\n# Maximum number of levels in tree\nmax_depth=[int(x) for x in np.linspace(start=5,stop=30,num=6)]\n# Minimum number of samples required to split a node\nmin_samples_split=[2,5,10,15,100]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf=[1,2,5,10]","af97cf07":"# Hyper Parameter tuning\nfrom sklearn.model_selection import RandomizedSearchCV\n# Create random grid\nrandom_grid={'n_estimators':n_estimators,'max_features':max_features,'max_depth':max_depth,\n            'min_samples_split':min_samples_split,'min_samples_leaf':min_samples_leaf}\nmodel1=RandomizedSearchCV(estimator=RandomForestRegressor(),\n                             param_distributions=random_grid,scoring='neg_mean_squared_error',\n                            n_iter=10,cv=5,random_state=42, n_jobs = 1)\nmodel1.fit(X_train,y_train)","b90c4358":"# listing out the best parameters & score\nprint(model1.best_params_)\nprint(model1.best_score_)\n","85c03cca":"rf_model=RandomForestRegressor(n_estimators=1000,min_samples_split=2, min_samples_leaf=1,\n                              max_features='sqrt',max_depth=25)\nrf_model.fit(X_train,y_train)\ny_pred=rf_model.predict(X_test)\n# Plotting the predictions\nplt.scatter(y_test,y_pred)","ce8f19c1":"# Displaying the model metrics\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nprint('MAE:', metrics.mean_absolute_error(y_test, y_pred))\nprint('MSE:', metrics.mean_squared_error(y_test, y_pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test,y_pred)))\nprint(\"Model Score:\",rf_model.score(X_test,y_test))","ccde95ae":"# Applying Linear Regresison\n\n# from sklearn.linear_model  import Ridge, Lasso, RidgeCV, LassoCV, ElasticNet, ElasticNetCV, \nfrom sklearn.linear_model import LinearRegression\nimport statsmodels.api as sm\nregression = LinearRegression()\nregression.fit(X_train,y_train)","f04299d2":"# Training & Testing data scores\n\nprint(\"R2 Score for Training dataset: \",regression.score(X_train,y_train))\nprint(\"Adjusted R2 Score for Training dataset: \",sm.OLS(y_train, X_train).fit().rsquared_adj)\n\nprint(\"R2 score for Test dataset: \",regression.score(X_test,y_test))\nprint(\"Adjusted R2 Score for Testing dataset: \",sm.OLS(y_test, X_test).fit().rsquared_adj)","3e147404":"# Lasso Regularization\nfrom sklearn.linear_model  import Lasso, LassoCV\n\n# LassoCV will return best alpha and coefficients after performing 10 cross validations\nlasscv = LassoCV(alphas = None,cv =5, max_iter = 100000, normalize = True)\nlasscv.fit(X_train, y_train)\n\n# best alpha parameter\nalpha = lasscv.alpha_\nalpha\n\n#now that we have best parameter, let's use Lasso regression and see how well our data has fitted before\n\nlasso_reg = Lasso(alpha)\nlasso_reg.fit(X_train, y_train)\nlasso_reg.score(X_test, y_test)","287a7490":"# Using Ridge regression model\n# RidgeCV will return best alpha and coefficients after performing 10 cross validations. \n# We will pass an array of random numbers for ridgeCV to select best alpha from them\nfrom sklearn.linear_model  import Ridge,RidgeCV\nalphas = np.random.uniform(low=0, high=10, size=(50,))\nridgecv = RidgeCV(alphas = alphas,cv=5,normalize = True)\nridgecv.fit(X_train, y_train)\n\nridge_model = Ridge(alpha=ridgecv.alpha_)\nridge_model.fit(X_train, y_train)\nridge_model.score(X_test, y_test)","3f58c62e":"# Elastic net\nfrom sklearn.linear_model  import ElasticNet,ElasticNetCV\nelasticCV = ElasticNetCV(alphas = None, cv =5)\n\nelasticCV.fit(X_train, y_train)\n\nelasticnet_reg = ElasticNet(alpha = elasticCV.alpha_,l1_ratio=0.5)\nelasticnet_reg.fit(X_train, y_train)\nelasticnet_reg.score(X_test, y_test)","24ce0644":"# plotting the y_test vs y_pred\n# ideally should have been a straight line\nplt.scatter(y_test, regression.predict(X_test))\nplt.show()","2e3ec826":"# Displaying the model metrics\nprint('MAE:', metrics.mean_absolute_error(y_test,  regression.predict(X_test)))\nprint('MSE:', metrics.mean_squared_error(y_test,  regression.predict(X_test)))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, regression.predict(X_test))))\nprint(\"Model Score:\",metrics.r2_score(y_test, regression.predict(X_test)))\n","6323195a":"# Applying XGBoost\n# Importing XG Boost libraries\n\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\n# fit model to training data\nxg_model = XGBRegressor()\n","f67e45c8":"#Learning Rate\nlearning_rate=[1,0.5,0.1,0.01,0.001]\n# No. of trees\nn_estimators=[int(x) for x in np.linspace(start=100,stop=1200,num=12)]\n# Maximum number of levels in tree\nmax_depth=[int(x) for x in np.linspace(start=5,stop=30,num=6)]","d413c0ba":"# Hyper Parameter tuning\n\n# Create random grid\nrandom_grid_xg={'n_estimators':n_estimators,'learning_rate':learning_rate,'max_depth':max_depth}\nxg_model=RandomizedSearchCV(XGBRegressor(objective='reg:squarederror'),\n                            param_distributions=random_grid_xg,n_iter=10,cv=5,random_state=42, \n                            n_jobs = 1)\nxg_model.fit(X_train,y_train)","e52eb169":"# listing out the best parameters & score\nprint(xg_model.best_params_)\nprint(xg_model.best_score_)","6f9fdd86":"xg_model=XGBRegressor(n_estimators=900,max_depth=5,learning_rate=0.01)\nxg_model.fit(X_train,y_train)\n#y_pred=rf_model.predict(X_test)\n# Plotting the predictions\nplt.scatter(y_test,xg_model.predict(X_test))","3e442296":"# Displaying the model metrics\n\nprint('MAE:', metrics.mean_absolute_error(y_test, xg_model.predict(X_test)))\nprint('MSE:', metrics.mean_squared_error(y_test, xg_model.predict(X_test)))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test,xg_model.predict(X_test))))\nprint(\"Model Score:\",xg_model.score(X_test,y_test))\n","bf5994a5":"Our r2_score for test data (67.98%) comes almost same as before using regularization. So, it is fair to say our OLS model did not overfit the data.","70362fe3":"Even after using different types of regularisation techniques we are getting the same r2 score approximately, Hence we can conclude that our model is not overfitted.","b2d7b334":"We see that the adjusted R2 score is slightly less for our test data. Let's see if our model is overfitting our training data.","a420ff4a":"# **We can clearly see that XGBoost Regression gives us the best model score.**","eb75ae79":"We got the almost same r2 square using Ridge regression as well. So, it's safe to say there is no overfitting."}}