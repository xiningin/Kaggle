{"cell_type":{"1e8338d9":"code","ff3e7bde":"code","3d5541bc":"code","64d16dac":"code","dea0a40b":"code","9bd48fb5":"code","3f67d350":"code","1401c84e":"code","d09cc17d":"code","d13dd6d3":"code","7d05b7b9":"code","85b1b013":"code","01ab5d0a":"code","0b91c1fb":"code","acc93d1b":"code","e8d7475a":"code","eca97024":"markdown","d06d882f":"markdown","7ddddb06":"markdown","49f25cba":"markdown","5f53f39f":"markdown","6210edf2":"markdown","854a3391":"markdown","6991fdc3":"markdown","c4ccf3b3":"markdown","033d054a":"markdown","f241c1fc":"markdown"},"source":{"1e8338d9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom math import sqrt, ceil\n\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, AvgPool2D, BatchNormalization, Reshape\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import LearningRateScheduler\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","ff3e7bde":"# Loading training data\ndata_train = pd.read_csv(\"..\/input\/train.csv\")\n\n# Showing some data\ndata_train.head()\n","3d5541bc":"# Getting first column with labels\ny_train = data_train['label']\n\n# Dropping column with labels\nx_train = data_train.drop(labels = [\"label\"], axis = 1)\n\n# Showing current shape of training data\nprint('Shape of whole data for training', data_train.shape)  # (42000, 785)\nprint('x_train:', x_train.shape)  # (42000, 784)\nprint('y_train:', y_train.shape)  # (42000,)\n\n# Showing some examples\n%matplotlib inline\n\n# Preparing function for ploting set of examples\n# As input it will take 4D tensor and convert it to the grid\n# Values will be scaled to the range [0, 255]\ndef convert_to_grid(x_input):\n    N, H, W = x_input.shape\n    grid_size = int(ceil(sqrt(N)))\n    grid_height = H * grid_size + 1 * (grid_size - 1)\n    grid_width = W * grid_size + 1 * (grid_size - 1)\n    grid = np.zeros((grid_height, grid_width)) + 255\n    next_idx = 0\n    y0, y1 = 0, H\n    for y in range(grid_size):\n        x0, x1 = 0, W\n        for x in range(grid_size):\n            if next_idx < N:\n                img = x_input[next_idx]\n                low, high = np.min(img), np.max(img)\n                grid[y0:y1, x0:x1] = 255.0 * (img - low) \/ (high - low)\n                next_idx += 1\n            x0 += W + 1\n            x1 += W + 1\n        y0 += H + 1\n        y1 += H + 1\n\n    return grid\n\n\n# Visualizing some examples of training data\nexamples = np.array(x_train.iloc[:81]).reshape(81, 28, 28)\nprint(examples.shape)  # (81, 28, 28)\n\n# Plotting\nfig = plt.figure()\ngrid = convert_to_grid(examples)\nplt.imshow(grid.astype('uint8'), cmap='gray')\nplt.axis('off')\nplt.gcf().set_size_inches(7, 7)\nplt.title('Some examples of training data', fontsize=24)\nplt.show()\nplt.close()\n\n# Saving plot\nfig.savefig('training_examples.png')\nplt.close()\n","64d16dac":"# Loading testing data\nx_test = pd.read_csv(\"..\/input\/test.csv\")\n\n# Showing some data\nx_test.head()\n","dea0a40b":"# Visualizing some examples of testing data\nexamples = np.array(x_test.iloc[:81]).reshape(81, 28, 28)\nprint(examples.shape)  # (81, 28, 28)\n\n# Plotting\nfig = plt.figure()\ngrid = convert_to_grid(examples)\nplt.imshow(grid.astype('uint8'), cmap='gray')\nplt.axis('off')\nplt.gcf().set_size_inches(7, 7)\nplt.title('Some examples of testing data', fontsize=24)\nplt.show()\nplt.close()\n\n# Saving plot\nfig.savefig('testing_examples.png')\nplt.close()\n","9bd48fb5":"# Making data as numpy array\n# Reshaping training and testing data\nx_train = np.array(x_train).reshape(-1, 28, 28, 1)\nx_test = np.array(x_test).reshape(-1, 28, 28, 1)\n\n# Showing current shape of training and testing data\nprint('x_train:', x_train.shape)  # (42000, 28, 28, 1)\nprint('x_test:', x_test.shape)  # (28000, 28, 28, 1)\n","3f67d350":"# Preparing datasets for further using\n\n# Preparing function for preprocessing MNIST datasets for further use in classifier\ndef pre_process_mnist(x_train, y_train, x_test):\n    # Normalizing whole data by dividing \/255.0\n    x_train = x_train \/ 255.0\n    x_test = x_test \/ 255.0  # Data for testing consists of 28000 examples from testing dataset\n\n    # Preparing data for training, validation and testing\n    # Data for validation is taken with 1000 examples from training dataset in range from 41000 to 42000\n    batch_mask = list(range(41000, 42000))\n    x_validation = x_train[batch_mask]  # (1000, 28, 28, 1)\n    y_validation = y_train[batch_mask]  # (1000,)\n    # Data for training is taken with first 41000 examples from training dataset\n    batch_mask = list(range(41000))\n    x_train = x_train[batch_mask]  # (41000, 28, 28, 1)\n    y_train = y_train[batch_mask]  # (41000,)\n\n    # Normalizing data by subtracting mean image and dividing by standard deviation\n    # Subtracting the dataset by mean image serves to center the data\n    # It helps for each feature to have a similar range and gradients don't go out of control\n    # Calculating mean image from training dataset along the rows by specifying 'axis=0'\n    mean_image = np.mean(x_train, axis=0)  # numpy.ndarray (28, 28, 1)\n\n    # Calculating standard deviation from training dataset along the rows by specifying 'axis=0'\n    std = np.std(x_train, axis=0)  # numpy.ndarray (28, 28, 1)\n    # Taking into account that a lot of values are 0, that is why we need to replace it to 1\n    # In order to avoid dividing by 0\n    for j in range(28):\n        for i in range(28):\n            if std[i, j, 0] == 0:\n                std[i, j, 0] = 1.0\n\n    # Subtracting calculated mean image from pre-processed datasets\n    x_train -= mean_image\n    x_validation -= mean_image\n    x_test -= mean_image\n\n    # Dividing then every dataset by standard deviation\n    x_train \/= std\n    x_validation \/= std\n    x_test \/= std\n    \n    # Preparing y_train and y_validation for using in Keras\n    y_train = to_categorical(y_train, num_classes=10)\n    y_validation = to_categorical(y_validation, num_classes=10)\n\n    # Returning result as dictionary\n    d_processed = {'x_train': x_train, 'y_train': y_train,\n                   'x_validation': x_validation, 'y_validation': y_validation,\n                   'x_test': x_test}\n\n    # Returning dictionary\n    return d_processed\n\n\n# Preprocessing data\ndata = pre_process_mnist(x_train, y_train, x_test)\nfor i, j in data.items():\n    print(i + ':', j.shape)\n\n# x_train: (41000, 28, 28, 1)\n# y_train: (41000, 10)\n# x_validation: (1000, 28, 28, 1)\n# y_validation: (1000, 10)\n# x_test: (28000, 28, 28, 1)\n","1401c84e":"model = Sequential()\n\nmodel.add(Conv2D(64, kernel_size=7, padding='same', activation='relu', input_shape=(28, 28, 1)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64, kernel_size=9, strides=2, padding='same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64, kernel_size=7, padding='same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.4))\n\nmodel.add(Conv2D(128, kernel_size=5, padding='same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(128, kernel_size=7, strides=2, padding='same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(128, kernel_size=5, padding='same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.4))\n\nmodel.add(Conv2D(256, kernel_size=3, padding='same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(256, kernel_size=5, strides=2, padding='same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(256, kernel_size=3, padding='same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.4))\n\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.4))\n\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","d09cc17d":"annealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** (x + 50))\nepochs = 25\n\nh = model.fit(data['x_train'], data['y_train'], batch_size=50, epochs = epochs,\n              validation_data = (data['x_validation'], data['y_validation']), callbacks=[annealer], verbose=1)\n","d13dd6d3":"print(\"Epochs={0:d}, Train accuracy={1:.5f}, Validation accuracy={2:.5f}\".format(epochs, max(h.history['acc']), \n                                                                                 max(h.history['val_acc'])))\n","7d05b7b9":"%matplotlib inline\nplt.rcParams['figure.figsize'] = (15.0, 5.0) # Setting default size of plots\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\n\nfig = plt.figure()\nplt.plot(h.history['acc'], '-o')\nplt.plot(h.history['val_acc'], '-o')\nplt.title('Model accuracy')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.xlabel('Epoch', fontsize=15)\nplt.ylabel('Accuracy', fontsize=15)\nplt.ylim(0.98, 1)\nplt.show()\n\n# Saving plot\nfig.savefig('model_accuracy.png')\nplt.close()\n","85b1b013":"model.save('my_model.h5')","01ab5d0a":"# # Saving model locally without commiting\n# from IPython.display import FileLink\n\n# FileLink('my_model.h5')\n","0b91c1fb":"results = model.predict(data['x_test'])\nresults = np.argmax(results, axis=1)\n\n# Loading sample template for submission and writing predicted labels into 'Label' column\nsubmission = pd.read_csv('..\/input\/sample_submission.csv')\n\nsubmission['Label'] = results\nsubmission.to_csv('sample_submission.csv', index=None)\n","acc93d1b":"# Cheking\ns = pd.read_csv('sample_submission.csv')\ns.head()\n","e8d7475a":"# # Saving resulted data locally without commiting\n# from IPython.display import FileLink\n\n# FileLink('sample_submission.csv')\n","eca97024":"# \ud83d\udcc8 Plotting model's accuracy","d06d882f":"# \ud83d\udcbe Saving model","7ddddb06":"# \ud83d\udcc2 Loading data","49f25cba":"# \ud83d\udce5 Importing needed libraries","5f53f39f":"# \ud83d\udd2e Predicting with images from test dataset","6210edf2":"# \ud83d\udd22 CNN for MNIST Competition","854a3391":"# \u27b0 Preprocessing data","6991fdc3":"# \ud83c\udf93 Related course for classification tasks","c4ccf3b3":"# \ud83c\udfd7\ufe0f Building model of CNN with Keras","033d054a":"# \u27bf Training the model","f241c1fc":"**Design**, **Train** & **Test** deep CNN for Image Classification.\n\n**Join** the course & enjoy new opportunities to get deep learning skills:\n\n\n[https:\/\/www.udemy.com\/course\/convolutional-neural-networks-for-image-classification\/](https:\/\/www.udemy.com\/course\/convolutional-neural-networks-for-image-classification\/?referralCode=12EE0D74A405BF4DDE9B)\n\n\n![](https:\/\/github.com\/sichkar-valentyn\/1-million-images-for-Traffic-Signs-Classification-tasks\/blob\/main\/images\/slideshow_classification.gif?raw=true)"}}