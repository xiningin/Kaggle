{"cell_type":{"6e50452a":"code","f497f049":"code","aa4e139e":"code","5b4de3f0":"code","414dfd34":"code","99acad1d":"code","498b11ac":"code","72adc6e2":"code","70d156e6":"code","16f11f68":"code","6f0251e3":"code","71bb3d63":"code","50bdb966":"code","5f162fe5":"code","55726d63":"code","fff358f2":"code","5664b577":"code","5d3ff4aa":"code","bb7e4aba":"code","0125246a":"code","37ea2332":"code","460eb141":"code","2c28ac60":"code","fdcfa893":"code","4b6d81fa":"code","264ec337":"code","f32a31ee":"code","4c8991a5":"code","30d35245":"code","8e34d2a4":"code","bbc829d6":"code","e92d7f3e":"code","c13de1a2":"code","1fa7c2df":"code","143a92d1":"code","00e47b58":"code","6748ca6e":"code","566b87bf":"code","bf7e0c8a":"code","24639830":"code","82e02c4b":"code","5159217e":"code","b515f4d8":"code","8a6ea29c":"markdown","2e23b4fa":"markdown","b028b3bb":"markdown","10567062":"markdown","1506ae1c":"markdown","f9738e27":"markdown","a102208c":"markdown","48e997eb":"markdown","06ffcb67":"markdown","3f1acbd3":"markdown","55f25b44":"markdown","f386f0d2":"markdown","5ea34e4a":"markdown","ba278eb4":"markdown","ade37bcf":"markdown","51fd3bda":"markdown","68fdc2d8":"markdown","58779902":"markdown","23c08842":"markdown","b3ae3daf":"markdown","b8f77a10":"markdown","858f9f01":"markdown","f3f6484e":"markdown","8c9ef670":"markdown","2ca6c5cf":"markdown","619e9978":"markdown","1a3f4108":"markdown","50d73a57":"markdown","d2f19e93":"markdown","a30f6a3b":"markdown","8d63c13c":"markdown","872cb5d1":"markdown","c50a4069":"markdown","d8801ee0":"markdown"},"source":{"6e50452a":"import numpy as np\nimport pandas as pd\nimport datetime as dt\nimport re\nfrom nltk.util import ngrams\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n!pip install pywaffle\nfrom pywaffle import Waffle\nimport matplotlib.dates as mdates\n\nfrom mlxtend.frequent_patterns import apriori\nfrom mlxtend.frequent_patterns import association_rules\n","f497f049":"customer = pd.read_csv('..\/input\/quantium-data-analytics-virtual-experience-program\/PurchaseBehaviour.csv')\ndat = pd.read_csv('..\/input\/quantium-data-analytics-virtual-experience-program\/Transactions.csv')\n\ncustomer.head()","aa4e139e":"# check for missing values\ncustomer.isnull().sum()","5b4de3f0":"dat.head()","414dfd34":"dat.isnull().sum()","99acad1d":"# Change excel date to real date\ndat['DATE'] = pd.TimedeltaIndex(dat['DATE'], unit='d') + dt.datetime(1899, 12, 30)","498b11ac":"dat.head(10)","72adc6e2":"# Extract product size\ndat['PROD_SIZE'] = [re.search(r\"[0-9]+(g|G)\", p).group(0).replace('G','').replace('g','') for p in dat['PROD_NAME']]\n","70d156e6":"# get unique products\ndat['PROD_NAME'].unique()[:10] ","16f11f68":"### Remove salsa dips\ndat = dat[~dat['PROD_NAME'].isin(['Old El Paso Salsa   Dip Tomato Mild 300g',\n'Old El Paso Salsa   Dip Chnky Tom Ht300g',\n'Woolworths Mild     Salsa 300g',\n'Old El Paso Salsa   Dip Tomato Med 300g',\n'Woolworths Medium   Salsa 300g',\n'Doritos Salsa Mild  300g',\n'Doritos Salsa       Medium 300g'])].reset_index(drop=True)","6f0251e3":"### Clean up product names\n# https:\/\/www.guru99.com\/python-regular-expressions-complete-tutorial.html\n\n# replace & with space and remove multiple spaces\ndat['PROD_NAME'] = [\" \".join(p.replace('&',' ').split()) for p in dat['PROD_NAME']]\n# remove digits that are followed by grams\ndat['PROD_NAME'] = [re.sub(r\"\\s*[0-9]+(g|G)\", r\"\", p) for p in dat['PROD_NAME']]\n","71bb3d63":"def replaceWords(string):\n    # specific\n    string = re.sub(r\"SeaSalt\", \"Sea Salt\", string)\n    string = re.sub(r\"Frch\/Onin\", \"French Onion\", string)\n    string = re.sub(r\"Cheddr Mstrd\", \"Cheddar Mustard\", string)\n    string = re.sub(r\"Jlpno Chili\", \"Jalapeno Chilli\", string)\n    string = re.sub(r\"Swt\/Chlli Sr\/Cream\", \"Sweet Chilli Sour Cream\", string)\n    string = re.sub(r\"SourCream\", \"Sour Cream\", string)\n    string = re.sub(r\"Tmato Hrb Spce\", \"Tomato Herb Spice\", string)\n    string = re.sub(r\"S\/Cream\", \"Sour Cream\", string)\n    string = re.sub(r\"ChipsFeta\", \"Chips Feta\", string)\n    string = re.sub(r\"ChpsHny\", \"Chips Honey\", string)\n    string = re.sub(r\"FriedChicken\", \"Fried Chicken\", string)\n    string = re.sub(r\"OnionDip\", \"Onion Dip\", string)\n    string = re.sub(r\"SweetChili\", \"Sweet Chilli\", string)\n    string = re.sub(r\"PotatoMix\", \"Potato Mix\", string)\n    string = re.sub(r\"Seasonedchicken\", \"Seasoned Chicken\", string)\n    string = re.sub(r\"CutSalt\/Vinegr\", \"Cut Salt Vinegar\", string)\n    string = re.sub(r\"ChpsBtroot\", \"Chips Beetroot\", string)\n    string = re.sub(r\"ChipsBeetroot\", \"Chips Beetroot\", string)\n    string = re.sub(r\"ChpsFeta\", \"Chips Feta\", string)\n    string = re.sub(r\"OnionStacked\", \"Onion Stacked\", string)\n    string = re.sub(r\"Ched\", \"Cheddar\", string)\n    string = re.sub(r\"Strws\", \"Straws\", string)\n    string = re.sub(r\"Slt\", \"Salt\", string)\n    string = re.sub(r\"Chikn\", \"Chicken\", string)\n    string = re.sub(r\"Rst\", \"Roast\", string)\n    string = re.sub(r\"Vinegr\", \"Vinegar\", string)\n    string = re.sub(r\"Mzzrlla\", \"Mozzarella\", string)\n    string = re.sub(r\"Originl\", \"Original\", string)\n    string = re.sub(r\"saltd\", \"Salted\", string)\n    string = re.sub(r\"Swt\", \"Sweet\", string)\n    string = re.sub(r\"Chli\", \"Chilli\", string)\n    string = re.sub(r\"Hony\", \"Honey\", string)\n    string = re.sub(r\"Chckn\", \"Chicken\", string)\n    string = re.sub(r\"Chp\", \"Chip\", string)\n    string = re.sub(r\"Btroot\", \"Beetroot\", string)\n    string = re.sub(r\"Chs\", \"Cheese\", string)\n    string = re.sub(r\"Crm\", \"Cream\", string)\n    string = re.sub(r\"Orgnl\", \"Original\", string)\n\n    return string\n\ndat['PROD_NAME'] = [replaceWords(s) for s in dat['PROD_NAME']]\n\ndat['PROD_NAME'].replace('Infzns Crn Crnchers Tangy Gcamole',\n'Infuzions Corn Crunchers Tangy Guacamole', inplace=True)","50bdb966":"dat['PROD_NAME'].unique()[:10]","5f162fe5":"def replaceBrands(string):\n    # specific\n    string = re.sub(r\"Red Rock Deli\", \"RRD\", string)\n    string = re.sub(r\"Dorito\", \"Doritos\", string)\n    string = re.sub(r\"Doritoss\", \"Doritos\", string)\n    string = re.sub(r\"Smith\", \"Smiths\", string)\n    string = re.sub(r\"Smithss\", \"Smiths\", string)\n    string = re.sub(r\"GrnWves\", \"Grain Waves\", string)\n    string = re.sub(r\"Woolworths\", \"WW\", string) \n    string = re.sub(r\"Snbts\", \"Sunbites\", string) \n\n    return string\n\n# standardize common brand names\ndat['PROD_NAME'] = [replaceBrands(s) for s in dat['PROD_NAME']]\n\n# get brand name from first word\ndat['brand'] = [s.split(' ')[0] for s in dat['PROD_NAME']]","55726d63":"dat.head()","fff358f2":"dat.describe()","5664b577":"# remove outlier\ndat = dat[dat['PROD_QTY'] < 200].reset_index(drop=True)","5d3ff4aa":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,4))\n\n# Product quantity sales by brand\nax1=plt.subplot(121)\ndat.groupby(['brand'], as_index=False).agg({'PROD_QTY': 'sum'}).sort_values('PROD_QTY').plot.barh(x='brand',legend=False, ax=ax1)\n\nax1.set_xlabel('Quantity Sold')\nax1.set_ylabel('Brand')\nax1.set_title('Quantities Sold by Brand')\n\nax2=plt.subplot(122)\ndat.groupby(['brand'], as_index=False)[['TXN_ID']].count().sort_values('TXN_ID').plot.barh(x='brand',color='#ff7f0e', legend=False, ax=ax2)\nax2.set_xlabel('Number of Transactions')\nax2.set_ylabel('Brand')\nax2.set_title('Transactions by Brand')\n\nplt.show()","bb7e4aba":"## Plot quantities sold by date\nbydate = dat.groupby('DATE').agg({'PROD_QTY': 'sum'}).reset_index()\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,4))\n\nax1=plt.subplot(121)\nsns.lineplot(x=\"DATE\", y=\"PROD_QTY\", data=bydate, ax=ax1)\nax1.xaxis.set_major_formatter(mdates.DateFormatter(\"%b-%Y\"))\nsns.lineplot(x=\"DATE\", y=\"PROD_QTY\", data=bydate[(bydate['DATE'] > '2018-11-30') & (bydate['DATE'] < '2019-01-01')], color='#2ca02c', ax=ax1)\nsns.lineplot(x=\"DATE\", y=\"PROD_QTY\", data=bydate[(bydate['DATE'] > '2018-08-10') & (bydate['DATE'] < '2018-08-24')], color='red', ax=ax1)\nsns.lineplot(x=\"DATE\", y=\"PROD_QTY\", data=bydate[(bydate['DATE'] > '2019-05-10') & (bydate['DATE'] < '2019-05-24')], color='red', ax=ax1)\nplt.ylabel('Quantities Sold')\nplt.title('Quantities Sold Throughout Whole Year')\n\n\n## Plot December quantities sold\n# filter december\ndecember = bydate[bydate['DATE'].isin(pd.date_range(start=\"2018-12-01\",end=\"2018-12-31\").tolist())]\n\n# fill in missing dec data\ndecember = december.set_index('DATE').reindex(pd.date_range(start=\"2018-12-01\",end=\"2018-12-31\"), fill_value=0)\n\nax2=plt.subplot(122)\nax2.bar(december.index,december['PROD_QTY'],color='#2ca02c')\nax2.set_xticks(december.index)\nax2.xaxis.set_major_formatter(mdates.DateFormatter(\"%b-%d\"))\nax2.xaxis.set_minor_formatter(mdates.DateFormatter(\"%b-%d\"))\nax2.tick_params(axis='x', rotation=90) \nplt.ylabel('Quantities Sold')\nplt.title('Quantities Sold in December')\nplt.show()","0125246a":"# Product Size\n\nfig, axes = plt.subplots(nrows=1, ncols=1, figsize=(15,4))\n\nax1=plt.subplot(121)\ndat.groupby('PROD_SIZE').agg({'PROD_QTY': 'sum'}).sort_values('PROD_QTY').reset_index().plot.barh(x='PROD_SIZE', legend=False, ax=ax1)\nax1.set_ylabel('Product Size (g)')\nax1.set_xlabel('Quantities Sold')\n\nplt.show()","37ea2332":"# Function for counting product keywords\ndef count_keywords(df):\n    words_freq = {}\n    for c,p in enumerate(df['PROD_NAME']):\n        for word in p.split():\n            if word in words_freq:\n                words_freq[word] += df['PROD_QTY'][c]\n            else:\n                words_freq[word] = df['PROD_QTY'][c]\n    \n    return words_freq\n\n# Function for generating ngrams\ndef generate_ngrams(text, n):\n    words = text.split()\n    return [' '.join(ngram) for ngram in list(ngrams(words, n))]\n\n# Function for counting product bigrams\ndef count_bigrams(df):\n    bigrams_freq = {}\n    for c,p in enumerate(df['PROD_NAME']):\n        for ngram in generate_ngrams(p, 2):\n            if ngram in bigrams_freq:\n                bigrams_freq[ngram] += df['PROD_QTY'][c]\n            else:\n                bigrams_freq[ngram] = df['PROD_QTY'][c]\n    return bigrams_freq\n\nwords_freq = count_keywords(dat)\nbigrams_freq = count_bigrams(dat)","460eb141":"# get top keywords\ntopwords = pd.DataFrame(words_freq.items(), columns=['word','freq']).sort_values('freq')\ntopwords = topwords[~topwords.word.isin(['Chips','Kettle','Smiths','Doritos','Pringles'])]\n\n# get top bigrams\ntopbigrams = pd.DataFrame(bigrams_freq.items(), columns=['bigram','freq']).sort_values('freq')\ntopbigrams = topbigrams[~topbigrams.bigram.isin(['Smiths Crinkle','Doritos Corn','Thins Chips','Cobs Popd','Kettle Tortilla','Grain Waves','Kettle Sensations','Kettle Sweet','Tyrrells Crisps','Twisties Cheese'])]\n\n# Plot\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8,5))\n\nax1 = plt.subplot(121)\ntopwords[-10:].plot.barh(x='word', legend=False, ax=ax1)\nax1.set_xlabel('Quantities Sold')\nax1.set_ylabel('Product Keyword')\n\nax2 = plt.subplot(122)\ntopbigrams[-10:].plot.barh(x='bigram', color='#ff7f0e', legend=False, ax=ax2)\nax2.set_xlabel('Quantities Sold')\nax2.set_ylabel('Product Bigram')\n\nfig.tight_layout(pad=3.0)\nplt.suptitle('Top 10 Most Popular Product Keywords')\nplt.show()","2c28ac60":"# Plot wordcloud of keywords and bigrams\nfrom wordcloud import WordCloud\n\nrem_list = ['Chips','Kettle','Smiths','Doritos','Pringles','Chip', 'Infuzions', 'RRD', 'Thins', 'Twisties', 'Grain', 'Waves']\n[words_freq.pop(key) for key in rem_list] \n\nrem_list = ['Smiths Crinkle','Doritos Corn','Thins Chips','Cobs Popd','Kettle Tortilla','Grain Waves','Kettle Sensations','Kettle Sweet','Tyrrells Crisps','Twisties Cheese']\n[bigrams_freq.pop(key) for key in rem_list] \n\nplt.figure(figsize=(15,4))\nplt.subplot(121)\nwc = WordCloud(background_color=\"black\").generate_from_frequencies(words_freq)\nplt.imshow(wc)\nplt.subplot(122)\nwc = WordCloud(background_color=\"black\").generate_from_frequencies(bigrams_freq)\nplt.imshow(wc)\nplt.suptitle('Most Popular Product Keywords')\nplt.show()","fdcfa893":"customer.head()","4b6d81fa":"## Waffle chart for customer class\npremium = dict(customer['PREMIUM_CUSTOMER'].value_counts()\/len(customer)*100)\n\nplt.figure(figsize=(7,5),\n    FigureClass=Waffle, \n    rows=5, \n    values=premium, \n    colors=[\"#1f77b4\", \"#ff7f0e\", \"green\"],\n    legend={'loc': 'upper left', 'bbox_to_anchor': (1, 1)},\n    icons='child', \n    font_size=15, \n    icon_legend=True)\n    \nplt.show()","264ec337":"# Plot customer lifestage counts\ncustomer.LIFESTAGE.value_counts().plot(kind='barh', alpha=.9, color=sns.color_palette(\"colorblind\"), title='Customer Lifestage Demographics').invert_yaxis()","f32a31ee":"# Merge\nalldat = dat.merge(customer, on='LYLTY_CARD_NBR')\n\nprint('No Duplicates:', len(alldat) == len(alldat)) # check same rows, no duplicates\nprint('Number of Nulls:', alldat.isnull().sum().sum()) # check for nulls","4c8991a5":"alldat.head()","30d35245":"# Sum up for each group \nlife_prem = alldat.groupby(['LIFESTAGE','PREMIUM_CUSTOMER']).agg({'TOT_SALES':'sum','PROD_QTY':'sum', 'TXN_ID':'count'}).reset_index().sort_values('TOT_SALES') # sort by TOT_SALES\nlife_prem['Group'] = life_prem['LIFESTAGE'] + '_' + life_prem['PREMIUM_CUSTOMER']\n\n# sort by PROD_QTY\nlife_prem_qty = life_prem.sort_values('PROD_QTY')","8e34d2a4":"# Function for Lollipop chart\ndef loll_plot(df1,x,y,xlabel,title,firstX):\n    \n    my_color=np.where(df1[x]==firstX, '#ff7f0e', '#1f77b4')\n    my_color[0] = 'red'\n    my_size=np.where(df1[x]==firstX, 70, 30)\n    my_size[0] = '70'\n\n    plt.hlines(y=np.arange(0,len(df1)),xmin=0,xmax=df1[y],color=my_color)\n    plt.scatter(df1[y], np.arange(0,len(df1)), color=my_color, s=my_size)\n    plt.yticks(np.arange(0,len(df1)), df1[x])\n    plt.xlabel(xlabel)\n    plt.title(title)","bbc829d6":"## Sales and Quantity Sold by each Segment\nfig = plt.figure(figsize=(14,6))\n\nax1 = plt.subplot(121)\nloll_plot(life_prem,'Group','TOT_SALES','Sales ($)','Total Sales','OLDER FAMILIES_Budget')\nplt.xticks(ticks=[0,50000,100000,150000])\n\nax2 = plt.subplot(122)\nloll_plot(life_prem_qty,'Group','PROD_QTY','Quantity Sold','Total Quantity Sold','OLDER FAMILIES_Budget')\n\nfig.tight_layout(pad=3.0)\nplt.show()","e92d7f3e":"# Get number of unique customers in each group\nlife_prem_pc = alldat[['LYLTY_CARD_NBR','LIFESTAGE','PREMIUM_CUSTOMER']].drop_duplicates('LYLTY_CARD_NBR').reset_index(drop=True).groupby(['LIFESTAGE','PREMIUM_CUSTOMER']).size().reset_index(name='Count').sort_values('Count').merge(life_prem, on=['LIFESTAGE','PREMIUM_CUSTOMER'])\n\nlife_prem_pc['SALES_PER_C'] = life_prem_pc['TOT_SALES']\/life_prem_pc['TXN_ID']\nlife_prem_pc['SALES_PER_UC'] = life_prem_pc['TOT_SALES']\/life_prem_pc['Count']\nlife_prem_pc = life_prem_pc.sort_values('SALES_PER_C')","c13de1a2":"## Sales by each Segment per Customer and per Unique Customer\nfig = plt.figure(figsize=(14,6))\n\nax1 = plt.subplot(121)\nloll_plot(life_prem_pc,'Group','SALES_PER_C','Sales ($)','Sales Per Customer','MIDAGE SINGLES\/COUPLES_Mainstream')\n\nlife_prem_pc = life_prem_pc.sort_values('SALES_PER_UC')\nax2 = plt.subplot(122)\nloll_plot(life_prem_pc,'Group','SALES_PER_UC','Sales ($)','Sales Per Unique Customer','OLDER FAMILIES_Mainstream')\n\nfig.tight_layout(pad=3.0)\nplt.show()","1fa7c2df":"life_prem_pc['QTY_PER_C'] = life_prem_pc['PROD_QTY']\/life_prem_pc['TXN_ID']\nlife_prem_pc['QTY_PER_UC'] = life_prem_pc['PROD_QTY']\/life_prem_pc['Count']\nlife_prem_pc = life_prem_pc.sort_values('QTY_PER_C')","143a92d1":"\nfig = plt.figure(figsize=(14,6))\n\nax1 = plt.subplot(121)\nloll_plot(life_prem_pc,'Group','QTY_PER_C','Quantity Sold','Quantity Purchased Per Customer','OLDER FAMILIES_Mainstream')\n\nlife_prem_pc = life_prem_pc.sort_values('QTY_PER_UC')\nax2 = plt.subplot(122)\nloll_plot(life_prem_pc,'Group','QTY_PER_UC','Quantity Sold','Quantity Purchased Per Unique Customer','OLDER FAMILIES_Mainstream')\n\nfig.tight_layout(pad=3.0)\nplt.show()","00e47b58":"life_prem_pc = life_prem_pc.sort_values('Count')\nfig = plt.figure(figsize=(10,6))\nax1 = plt.subplot(121)\nloll_plot(life_prem_pc,'Group','Count','Number of Unique Customers','Number of Unique Customers','YOUNG SINGLES\/COUPLES_Mainstream')","6748ca6e":"alldat['PRICE_PER_UNIT'] = alldat['TOT_SALES']\/alldat['PROD_QTY'] # get price per unit\n# get price per unit of each customer then groupby lifestage and premium_customer to get average per group\nprice_per_unit = alldat.groupby('LYLTY_CARD_NBR').agg({'PRICE_PER_UNIT':'mean'}).reset_index().merge(alldat[['LYLTY_CARD_NBR','LIFESTAGE','PREMIUM_CUSTOMER']], on='LYLTY_CARD_NBR').groupby(['LIFESTAGE','PREMIUM_CUSTOMER']).agg({'PRICE_PER_UNIT':'mean'}).reset_index().sort_values('PRICE_PER_UNIT')\nprice_per_unit['Group'] = price_per_unit['LIFESTAGE'] + '_' + price_per_unit['PREMIUM_CUSTOMER']\n\n\nfig = plt.figure(figsize=(10,6))\nax1 = plt.subplot(121)\nloll_plot(price_per_unit,'Group','PRICE_PER_UNIT','Price Per Qty ($)','Price Paid Per Quantity Per Unique Customer','YOUNG SINGLES\/COUPLES_Mainstream')\n","566b87bf":"basket = (alldat[(alldat['LIFESTAGE']=='YOUNG SINGLES\/COUPLES') & (alldat['PREMIUM_CUSTOMER']=='Mainstream')]\n        .groupby(['LYLTY_CARD_NBR','brand'])['PROD_QTY']\n        .sum().unstack().reset_index().fillna(0)\n        .set_index('LYLTY_CARD_NBR'))\n\ndef encode_units(x):\n    if x <= 0:\n        return 0\n    if x >= 1:\n        return 1\n\nbasket_sets = basket.applymap(encode_units)\nbasket_sets\n","bf7e0c8a":"frequent_itemsets = apriori(basket_sets, min_support=0.07, use_colnames=True)\nfrequent_itemsets","24639830":"rules = association_rules(frequent_itemsets, metric=\"lift\")\nrules.head()","82e02c4b":"## Affinity to Brand\n\nyoung_mainstream = alldat[(alldat['LIFESTAGE']=='YOUNG SINGLES\/COUPLES') & (alldat['PREMIUM_CUSTOMER']=='Mainstream')]\nquantity_bybrand = young_mainstream.groupby(['brand'])[['PROD_QTY']].sum().reset_index()\nquantity_bybrand.PROD_QTY = quantity_bybrand.PROD_QTY \/ young_mainstream.PROD_QTY.sum()\nquantity_bybrand = quantity_bybrand.rename(columns={\"PROD_QTY\": \"Targeted_Segment\"})\n\nother_segments = pd.concat([alldat, young_mainstream]).drop_duplicates(keep=False) # remove young_mainsream\nquantity_bybrand_other = other_segments.groupby(['brand'])[['PROD_QTY']].sum().reset_index()\nquantity_bybrand_other.PROD_QTY = quantity_bybrand_other.PROD_QTY \/ other_segments.PROD_QTY.sum()\nquantity_bybrand_other = quantity_bybrand_other.rename(columns={\"PROD_QTY\": \"Other_Segment\"})\n\nquantity_bybrand = quantity_bybrand.merge(quantity_bybrand_other, on ='brand')\nquantity_bybrand['Affinitytobrand'] = quantity_bybrand['Targeted_Segment'] \/ quantity_bybrand['Other_Segment']\nquantity_bybrand = quantity_bybrand.sort_values('Affinitytobrand')\n\nquantity_bybrand.head()\n","5159217e":"## Affinity to Size\n\nquantity_bysize = young_mainstream.groupby(['PROD_SIZE'])[['PROD_QTY']].sum().reset_index()\nquantity_bysize.PROD_QTY = quantity_bysize.PROD_QTY \/ young_mainstream.PROD_QTY.sum()\nquantity_bysize = quantity_bysize.rename(columns={\"PROD_QTY\": \"Targeted_Segment\"})\n\nquantity_bysize_other = other_segments.groupby(['PROD_SIZE'])[['PROD_QTY']].sum().reset_index()\nquantity_bysize_other.PROD_QTY = quantity_bysize_other.PROD_QTY \/ other_segments.PROD_QTY.sum()\nquantity_bysize_other = quantity_bysize_other.rename(columns={\"PROD_QTY\": \"Other_Segment\"})\n\nquantity_bysize = quantity_bysize.merge(quantity_bysize_other, on='PROD_SIZE')\nquantity_bysize['Affinitytosize'] = quantity_bysize['Targeted_Segment'] \/ quantity_bysize['Other_Segment']\nquantity_bysize = quantity_bysize.sort_values('Affinitytosize')","b515f4d8":"\nfig = plt.figure(figsize=(10,6))\nax1 = plt.subplot(121)\nloll_plot(quantity_bybrand,'brand','Affinitytobrand','Affinity','Affinity To Brand','Tyrrells')\n\nax2 = plt.subplot(122)\nloll_plot(quantity_bysize,'PROD_SIZE','Affinitytosize','Affinity','Affinity To Product Size','270')\n\nplt.suptitle('Young Singles\/Couples Mainstream')\nplt.show()\n\n","8a6ea29c":"<a id='AffinityBrandSize'><\/a>\n## 4.2. Affinity to Brand and Size\n\nAs a final analysis, let us compute an affinity score for each of the product brand and sizes to see which brand and size `Mainstream Young Singles\/Couples` tend to buy more often.","2e23b4fa":"Some of the products are not chips, most of them with the word **'Salsa'**. However, some salsa are actually chips with 'salsa' in their product names. We will need to remove those that are salsa and not chips.","b028b3bb":"<a id='Affinity'><\/a>\n## 4. Market Basket and Affinity Analysis\n\nAffinity analysis or market basket analysis, is a data mining technique used to understand the purchase behavior of customers. For example, market basket analysis might tell us that customers who purchase Doritos also tend to buy Tyrrells in a certain year or transaction, so putting both items on promotion at the same time would not create a significant increase in revenue, while a promotion involving just one of the items would likely drive sales of the other.\n\n**Conditional Rule:**\nIF {Kettle} THEN {Doritos}\n\n### Support\nA collection of items purchased by a customer is an **`itemset`**. The set of items on the left-hand side (Kettle in the example above) is the **`antecedent`** of the rule, while the one to the right (Doritos) is the **`consequent`**.  The probability that the antecedent event will occur, i.e., a customer will buy a sandwich and cookies, is the **`support`** of the rule. That simply refers to the relative frequency that an itemset appears in transactions. The support of an item or item combination helps to identify keystone products. Hence, if a sandwich and cookies have high support, then they can be priced to attract people to the store.\n\n### Confidence\nThe probability that a customer will purchase a drink on the condition of purchasing a sandwich and cookies is referred to as the **`confidence`** of the rule. Confidence can be used for product placement strategy and increasing profitability. Placing high margin items near associated high confidence (driver) items can increase the overall margin on purchases.\n\n### Lift\nThe **`lift`** of the rule is the ratio of the support of the left-hand side of the rule (Kettle) co-occurring with the right-hand side (Doritos), divided by the probability that the left-hand side and right-hand side co-occur if the two are independent. A **lift greater than 1** suggests that the presence of the antecedent increases the chances that the consequent will occur in a given transaction\n\n**Lift below 1** indicates that purchasing the antecedent reduces the chances of purchasing the consequent in the same transaction. Note: This could indicate that the items are seen by customers as alternatives to each other\n\nWhen the **lift is 1**, then purchasing the antecedent makes no difference on the chances of purchasing the consequent","10567062":"<a id='Outliers'><\/a>\n## 1.4. Remove Outliers","1506ae1c":"To conclude, given that the `Mainstream Young Singles\/Couples` contributed to the most sales, have the largest number of customers but tend to purchase lower quantities of chips, perhaps this group can be further targeted for more advertisements\/recommendations\/promotions on certain brands to encourage more purchase and for purchasing slightly more expensive chip brands.\n\n`Mainstream Retirees` tend to be the same as `Mainstream Young Singles\/Couples`, except that they tend to purchase more chips and slightly cheaper chips. The general strategy would be the same except to promote cheaper brands to them.\n\nMeanwhile, `Budget Older Families` also contributed to the most sales, and they tend to buy lots of chips, but there are not as much of them in the customer base. Therefore, the strategy would be to attract more of this segment to increase the pool size.\n\nThis can help to drive up total sales and profits.","f9738e27":"According to the waffle chart, the classes are quite evenly split, with more mainstreams followed by budget customers. ","a102208c":"<a id='EDA'><\/a>\n## 2. Exploratory Data Analysis\n\n<a id='Brand'><\/a>\n## 2.1. By Brand\n\nFirst we look at the number of packages sold as well as the number of transaction for each chip brand. It appears that **Kettle** is the brand that has the highest quantities sold, followed by **Smiths**. **French Fries** and **The Natural Chip Co.** came in last and second last respectively. However, this could be due to some people purchasing more of a certain brand than another brand, which means that having the most quantity sold doesn't mean that it will be the ***most popular*** among customers. Transactions by brand gives a better idea but it could also be that some people buy lots of a brand in a single transaction. We will have to control for these in subsequent analyses.","48e997eb":"<a id='PricePerQuantityPerCustomer'><\/a>\n## 3.5. Price Paid Per Quantity Per Unique Customer\n\n\nFinally, let us look at how much each customer in each of the segments are willing to pay for each packet of chips. We see that `Mainstream Young and Midage Singles\/Couples` are more willing to pay more for each pack of chips.","06ffcb67":"<a id='Analysis'><\/a>\n## 3. Data Analysis\n\nNow we can dive deeper into the data and investigate specific customer segments and how sales are contributed by these segments.\n","3f1acbd3":"<a id='TotalSalesPC'><\/a>\n## 3.2. Total Sales Per Customer and Per Unique Customer","55f25b44":"Retirees, older and younger singles and couples are also the majority of the customer pool.","f386f0d2":"<a id='Customer'><\/a>\n## 2.5. Customer Data\n\nWe now explore the customer data, which contains their lifestage and their premium class.","5ea34e4a":"We can see that the lift are all close to 1, suggesting that the antecedents of each of the itemsets makes no difference on the chances of purchasing the consequent. ","ba278eb4":"<a id='CleanDate'><\/a>\n## 1.1. Clean Date","ade37bcf":"<a id='Keywords'><\/a>\n## 2.4. By Product Keywords\n\nHere we look at words and bigrams in the product names that are the most popular.","51fd3bda":"<a id='Conclusion'><\/a>\n## 5. Conclusion\n\nIn conclusion:\n* EDA showed that **Kettle** and **Smiths** brand of chips brought in the largest revenue as well as quantities sold\n* Products with **Salt** and **Sour Cream** tend to sell the most\n* `Mainstream Young Singles\/Couples` contributed to one of the highest sales due to their large customer pool but tend to purchase lower quantity of chips, so advertistments\/recommendations\/promotions can be targeted towards this group to **encourage more purchase**\n* `Mainstream Retirees` tend to be the same as `Mainstream Young Singles\/Couples`, except that they tend to purchase more chips and slightly cheaper chips. The general strategy would be the same except to **promote cheaper brands** to them.\n* `Budget Older Families` also contributed to the most sales, and they tend to buy lots of chips, but there are not as much of them in the customer base. Therefore, the strategy would be to attract more of this segment to **increase the pool size**.\n* Market basket analysis **did not provide** any meaningful support\/relationship among brands from `Mainstream Young Singles\/Couples` customer purchase behavior.\n* Affinity analysis suggests that `Mainstream Young Singles\/Couples` have affinity towards **Tyrrells** brand and **270g** product size\n","68fdc2d8":"# **Quantium Data preparation and Customer Analytics** <br>\nTeYang Lau<br>\nCreated: 24\/7\/2020<br>\nLast update: 31\/8\/2020<br>\n\n<img src = 'https:\/\/assets.epicurious.com\/photos\/5d2dee06bcca5400095d4de4\/16:9\/w_2560%2Cc_limit\/Taste_Test_Thin_Chips_HERO_071119_025.jpg' width=800>\n\n## Project Goals\n1. *Explore* customer transaction data and find interesting insights to chips **purchase behavior**\n2. *Identify* **customer segments** that contribute to sales\n3. *Investigate* which brand and size customers have **affinity** to \n\n### About this dataset\nThis data is provided courtesy of [Quantium](https:\/\/quantium.com\/) via [InsideSherpa's Data Analytics Virtual Experience Program](https:\/\/www.insidesherpa.com\/virtual-internships\/prototype\/NkaC7knWtjSbi6aYv\/Data%20Analytics%20Virtual%20Experience%20Program). It contains customer segment data as well as customer transaction data on chips purchase.\n\n### Findings\n* EDA showed that **Kettle** and **Smiths** brand of chips brought in the largest revenue as well as quantities sold\n* Products with **Salt** and **Sour Cream** tend to sell the most\n* `Mainstream Young Singles\/Couples` contributed to one of the highest sales due to their large customer pool but tend to purchase lower quantity of chips, so advertistments can be targeted towards this group to encourage more purchase\n* Affinity analysis suggests that `Mainstream Young Singles\/Couples` have affinity towards **Tyrrells** brand and **270g** product size\n\n\n### What's in this notebook:\n1. [Data Loading and Cleaning](#Data_loading)<br>\n    1.1. [Clean Date](#CleanDate)<br>\n    1.2. [Clean Product Name](#ProductName)<br>\n    1.3. [Extract Product Brand](#ProductBrand)<br>\n    1.4. [Remove Outliers](#Outliers)<br>\n2. [Exploratory Data Analysis](#EDA)<br>\n    2.1. [By Brand](#Brand)<br>\n    2.2. [By Date](#Date)<br>\n    2.3. [By Product Size](#Size)<br>\n    2.4. [By Product Keywords](#Keywords)<br>\n    2.5. [Customer Data](#Customer)<br>\n    2.6. [Merge Data](#Merge)<br>\n3. [Data Analysis](#Analysis)<br>\n    3.1. [Total Sales and Quantity](#TotalSales)<br>\n    3.2. [Total Sales Per Customer and Per Unique Customer](#TotalSalesPC)<br>\n    3.3. [Total Quantity Purchased Per Customer and Per Unique Customer](#TotalQuantityPC)]<br>\n    3.4. [Number of Unique Customers in Each Segment](#NumberOfCustomers)<br>\n    3.5. [Price Paid Per Quantity Per Unique Customer](#PricePerQuantityPerCustomer)<br>\n    \n4. [Market Basket and Affinity Analysis](#Affinity)<br>\n    4.1. [Market Basket](#Basket)<br>\n    4.2. [Affinity to Brand and Size](#AffinityBrandSize)<br>\n5. [Conclusion](#Conclusion)\n","58779902":"We can see that **salt** and **cheese** have the most quantities sold. When looking at bigrams, **sour cream** is the most popular. Below is a word cloud for the keywords and bigrams.","23c08842":"<a id='Merge'><\/a>\n## 2.6. Merge Data\n\nWe merge the customer data and the transaction data to allow further and deeper analyses. We also check if there are any duplicates and null values after merging.","b3ae3daf":"<a id='Size'><\/a>\n## 2.3. By Product Size\n\nIt's also interesting to look at quantities sold by product size and it appears that **175g** is the most popular size.","b8f77a10":"<a id='TotalSales'><\/a>\n## 3.1. Total Sales and Quantity\n\nHere we separate the customer segments by lifestage and their premium class and look at the sales and quantities sold by each of these segments.","858f9f01":"When we look at quantities sold, the same pattern appears. `Older Families` and `Young Families` purchase the most chips in the given year.","f3f6484e":"We can see that `Budget Older Families`, `Mainstream Young Singles\/Couples` and `Mainstream Retirees` contribute the most to chip sales and quantities sold. However, this could be due to some segments having more customers who purchase chips. A better way is to compute the sales **by each customer**.","8c9ef670":"Here we clean the product names so that abbrieviations like **Slt** will be converted to their original form **Salt**, and we also split up words that are meant to be separated. `&` and product size are also removed.","2ca6c5cf":"<a id='Basket'><\/a>\n## 4.1 Market Basket\n\nHere we will focus on `Mainstream Young Singles\/Couples` and the purchases they made throughout the year to understand their affinity to the brands and product sizes of chips.","619e9978":"<a id='Data_loading'><\/a>\n## **1. Data Loading and Cleaning** ##","1a3f4108":"<a id='Date'><\/a>\n## 2.2. By Date\n\nHere we look at the number of chips sold throughout the whole year. We see an increase during December, possibly due to people stocking up for Christmas. There were also dips around August and May. Zooming in on **December**, we can see a trend in increasing sales coming up towards Christmas, which then dropped to 0 as the supermarket is closed on the holiday.\n","50d73a57":"We can see that `Mainstream Young Singles\/Couples` and `Mainstream Retirees` have a lot more customers, which might explain their higher sales, while `Budget Retirees` do not have as large a customer base, which might explain why the above `Per Unique Customer` results. \n\nSo what we can conclude is the there are more `Mainstream Young Singles\/Couples` and `Mainstream Retiress`, which lead to more sales by this segments, although *each of the customers* in these segments do not spend the highest total in a year on chips. On the other hand, while there are not as many `Budget Older Families`, *each of the customers* in this segment tend to purchase more chips, leading to high total sales. ","d2f19e93":"<a id='ProductBrand'><\/a>\n## 1.3. Extract Product Brand \n\nHere we standardize all the product brands (e.g, Red Rock Deli becomes RRD) and then we extract the brand name from the first word of the product name. ","a30f6a3b":"<a id='ProductName'><\/a>\n## 1.2. Clean Product Name ","8d63c13c":"<a id='NumberOfCustomers'><\/a>\n## 3.4. Number of Unique Customers in Each Segment","872cb5d1":"The plot shows that `Mainstream Young Singles\/Couples` tend to buy more `Tyrrells`, `Twisties`, and `Doritos`, and prefer product sizes of `270g` to `380g`. The store can perhaps place more of these products near the shelves that tend to attract younger and mainstream customers.","c50a4069":"We also computed the sales **per unique customer** as some segments could have higher number of sales due to repeat transactions by the same customers. We can see that they tell a different story. Notice that the `Sales Per Unique Customer` plot has a higher sales per segment and this is because their are multiple customers who made multiple transactions in the given year.  It seems that `Older Families` and `Young Families` pay the most for chips in the given year.","d8801ee0":"<a id='TotalQuantityPC'><\/a>\n## 3.3. Total Quantity Purchased Per Customer and Per Unique Customer"}}