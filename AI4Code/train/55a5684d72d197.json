{"cell_type":{"c2617d95":"code","1ded3b62":"code","77f4fa30":"code","207a73e5":"code","334370cf":"code","152caa9c":"code","7fadb112":"code","45013519":"code","78c70f54":"code","1e3def84":"code","49fbd11a":"code","b07f92c2":"code","f723a8fc":"code","58b7ff96":"code","33a2fcc7":"code","0ec3098f":"code","c80c4337":"code","6800e8e8":"code","1089dcea":"code","40998281":"code","529260fc":"code","f22ab471":"code","b1cf9916":"code","be558bce":"code","750d95ae":"code","07305d7e":"code","4bc7cf0b":"code","5e27a33c":"code","c876f5c4":"code","66f6a94f":"code","f7223c91":"code","e3628a9e":"code","4bf066b6":"code","0cd32562":"code","d190d5d9":"code","66192110":"code","1208808e":"code","acb04c35":"code","74647d29":"code","754736df":"code","e2fcabe0":"code","f824db25":"code","67484f73":"code","34b12b96":"code","ca03a314":"code","42497112":"code","9cc18fbb":"code","3638bb65":"code","e1ee3136":"code","2744ee63":"code","0fb750a2":"code","934f9ce1":"code","d2360c92":"code","a4cc2315":"code","c66bed08":"code","f123fef7":"code","89a21847":"code","8d3b686e":"code","c03e6b17":"code","88c0e23d":"code","91bb0ef5":"code","830fdca7":"code","9c342545":"code","6f95a784":"code","7b15326d":"code","4949ce8e":"code","105080c3":"code","4b48bf2f":"code","3c7ea5e3":"code","fb7267ad":"code","2a05f6b6":"code","b1e64908":"code","c1618c41":"code","fd7a291d":"code","01a8803f":"code","b4f2e4dc":"code","ec6a804c":"code","b4d78d91":"code","a75c5da0":"code","4044a716":"code","73960b63":"code","90c68c60":"code","e58663d7":"code","3c7b6854":"code","9dafa3fe":"code","c0fe569d":"code","80cd0d9b":"code","fe244702":"code","53685cb6":"code","aa7bc759":"code","b3435620":"code","07942524":"code","dc315557":"code","dc46f624":"code","6a4c9249":"code","f5bb0843":"code","50d5cc7d":"code","5e19edcf":"code","38f70579":"code","1cdd227a":"code","879d371c":"code","ebc119a8":"code","80de9458":"code","69c130d4":"code","3e1fa565":"code","929d1500":"code","a3cb7cbf":"code","041f5240":"code","1dfb8abd":"code","84bd411d":"code","0436473c":"code","d52c0acc":"code","de17426b":"code","7b55f460":"code","6b92d8df":"code","0f92ebb1":"code","5ada3727":"code","b05d5800":"code","2fa2bc94":"code","b41b2bef":"code","13112457":"code","f5eaaaae":"code","2b62afa5":"code","30aeaa86":"code","2be6ae69":"code","283fe69a":"code","df2b3fc3":"code","550a351e":"code","d388310d":"code","093f1f53":"code","1e87f049":"code","6d16eddd":"code","71c9e0fb":"code","7b891003":"code","3cc97ff8":"code","ee62a90e":"code","6c453188":"code","20774324":"code","24774587":"code","9c7c971d":"code","fccd1b60":"code","9a9081a7":"code","fb549144":"code","f5a15826":"code","ba2fe71f":"code","a87ab1d7":"code","88778c05":"code","e9c0c2d7":"code","32fd7d9f":"code","d8dc4d5f":"code","aa03d5ef":"code","e8963c61":"code","a10df5a8":"code","c6d1273a":"code","0eb03f0a":"code","6de31e5a":"code","c99013fd":"code","0f6edb7c":"code","89023362":"code","f3233cf5":"code","ad3dbf44":"code","49a338ce":"code","9a1663cc":"code","fcc7bae8":"code","46e5be30":"code","a99bce6e":"code","4326c506":"code","82fba8bd":"code","dfda12b0":"code","8562dad5":"code","13bc2778":"code","22b7f4ac":"code","d935f5fb":"code","88b65b0a":"code","fa7f4d21":"code","3811e7ba":"code","3583c2fb":"code","97b2fb0b":"code","6cbe0660":"code","807256b5":"code","cbbde5d4":"code","66a3038f":"code","fd769ef1":"code","8025f172":"code","d04c55e1":"code","cbf5e022":"code","98c6540b":"code","8ac311e2":"code","51599745":"code","13e9b64c":"code","fdf4251e":"markdown","b70b0882":"markdown","09ceeffd":"markdown","eaea09a2":"markdown","48eb2ba9":"markdown","543eda7b":"markdown","6792d7f9":"markdown","6385254d":"markdown","fdcfd801":"markdown","5aa8d5c1":"markdown","71485b41":"markdown","1525717c":"markdown","fa80ac1e":"markdown","096e1a96":"markdown","1fe5e897":"markdown","2c79be5d":"markdown","4c8d82d0":"markdown","6b4699cf":"markdown","663e3f6f":"markdown","a18d63a5":"markdown","2eee31ad":"markdown","a82b893a":"markdown","5851e9ad":"markdown","ff36678f":"markdown","88363a7a":"markdown","2c3c9330":"markdown","a4f30f50":"markdown","45969900":"markdown","b6c9ae11":"markdown","2db1c77c":"markdown","da1889ef":"markdown","6b489ba8":"markdown","0a9d24a8":"markdown","5482f160":"markdown","041dc729":"markdown","19d9f57b":"markdown","d3ec9a15":"markdown","69104f96":"markdown","c37c954a":"markdown","44c2e191":"markdown","f4a9ffb4":"markdown","717ea6a4":"markdown","285f8d2c":"markdown","ebff9541":"markdown","8c53bb0b":"markdown","95ffd879":"markdown","033c30f7":"markdown","fa512110":"markdown","ce3173ef":"markdown","32a1186a":"markdown","8c8342d1":"markdown","d8535cd9":"markdown","26d320f2":"markdown","e9fdc661":"markdown","60ccc84f":"markdown","3e15ddbf":"markdown","2a565340":"markdown","749594b3":"markdown","b85cc835":"markdown","e8e49b93":"markdown","edf0862a":"markdown","8117fe32":"markdown","1ec927b6":"markdown","823cff8b":"markdown","2fd6a244":"markdown","44e075aa":"markdown","c1e8fe01":"markdown","0ff84f31":"markdown","8f6abeb9":"markdown","17207d95":"markdown","dfba7081":"markdown","86636553":"markdown","c5035c52":"markdown","15aa920f":"markdown","ddc110c9":"markdown","6b65f487":"markdown","f807cfb8":"markdown","674c0aed":"markdown","d4bf8e11":"markdown","762200f2":"markdown","83784805":"markdown","a5a56201":"markdown","b7d5e0c3":"markdown","99357226":"markdown","717d9b2a":"markdown","c3ecee2b":"markdown","239b921e":"markdown","b8d3e932":"markdown","97aa6389":"markdown","532ae461":"markdown","a4534c6a":"markdown","b14c9fa1":"markdown","fc2c473b":"markdown","5e4e9569":"markdown","d84efda7":"markdown","b51ddfe4":"markdown","de987dcc":"markdown","9774c52c":"markdown","4c7b0181":"markdown","d11592a0":"markdown","8fd7907e":"markdown","34ed30dc":"markdown","4297e6e9":"markdown","b70ffd90":"markdown","525b5eff":"markdown","621f1708":"markdown","5f81046d":"markdown","364cd5df":"markdown","049b3de8":"markdown","8db34239":"markdown","c2e34ad8":"markdown","a5a324b3":"markdown","19b4c6fe":"markdown","a54ed9bd":"markdown","95cad72a":"markdown","a0e928d5":"markdown","76a0e36e":"markdown","434ea43e":"markdown","edc98c05":"markdown","a164262c":"markdown","3f232d9a":"markdown","6b0de219":"markdown","321710a8":"markdown","808f77ed":"markdown","5b3ce7ff":"markdown","548dc71b":"markdown","506ea0a7":"markdown","f9fa0b91":"markdown","de5685cc":"markdown","c791adbf":"markdown","097a627d":"markdown","c0134a78":"markdown","d5c2b9a9":"markdown","7f2bd5ce":"markdown","f4fb46ad":"markdown","0b1f9bcd":"markdown","9888df40":"markdown","6554b8f1":"markdown","e681d6de":"markdown","d9649f04":"markdown","928f9046":"markdown","c93923f2":"markdown","e447fced":"markdown","890420bd":"markdown","c58f617a":"markdown","590f0f44":"markdown","ec01f7ac":"markdown","ed430ca7":"markdown","64a52be3":"markdown","1e8f850f":"markdown","a0cac2e1":"markdown","66bd8a08":"markdown","c673c140":"markdown","8491b01e":"markdown","cf067f4d":"markdown","af307c7f":"markdown","259ad800":"markdown","c150b52e":"markdown","95cee318":"markdown","30a5b6c1":"markdown","fea8f4a9":"markdown","f5017955":"markdown","e77460fe":"markdown","927c94e6":"markdown","a193912e":"markdown","b8f77c67":"markdown","5e06dbd2":"markdown","bbdc0922":"markdown","cdc7dc9d":"markdown","182bf851":"markdown","48c7f169":"markdown","01d46cbb":"markdown","b4fdf5a4":"markdown","42e3c98f":"markdown","234bbae7":"markdown","cf5fe9ef":"markdown","84cd99d6":"markdown","189873e3":"markdown","5150aed2":"markdown","bcce3c7a":"markdown","cdefd558":"markdown","304ca424":"markdown","78360d61":"markdown","b82ec969":"markdown","0c2a8133":"markdown","e12a451c":"markdown","8dcc7bc4":"markdown","1835cb48":"markdown","1fd83591":"markdown","eeee7be3":"markdown","f1f15d7b":"markdown","7d130d53":"markdown","81dd78e3":"markdown","3855a960":"markdown","3aef2c9c":"markdown","18e7d3d3":"markdown","9a2034f6":"markdown","657ab341":"markdown","57ccb27a":"markdown","0eb91bcf":"markdown","774bc192":"markdown","518380e1":"markdown","6ee29e34":"markdown","2e54af8d":"markdown","3ac907f3":"markdown","00d10722":"markdown","e0917002":"markdown","210bb0b6":"markdown","3e1360d2":"markdown","4586654b":"markdown","990b505d":"markdown","2408d21a":"markdown","e95c42bc":"markdown","6c433d8c":"markdown","ae4535e5":"markdown","d5803411":"markdown","ac48ceef":"markdown","d6d600d2":"markdown","d95fa4db":"markdown","f1635062":"markdown","fd2f5ca2":"markdown","2b843511":"markdown","220b94a8":"markdown","68447f9b":"markdown","2d0e3b07":"markdown","be76f4d9":"markdown","5a25ed51":"markdown","f12eee2a":"markdown","69ff1f53":"markdown","8267e013":"markdown","f20fc7be":"markdown","e02efdfc":"markdown","bc4de74e":"markdown","abdb924a":"markdown","e7bd7a2b":"markdown","d6c5fabf":"markdown","27daa7da":"markdown","72557dce":"markdown","3b1f2092":"markdown","d3fe71f3":"markdown","e13a1931":"markdown","1695f5c6":"markdown","2029947e":"markdown","b2fa1278":"markdown","8ce59d8b":"markdown","65246dd8":"markdown","7584ce86":"markdown","a2feb421":"markdown","e392b583":"markdown","53338169":"markdown","72300cd7":"markdown","9f5ea40d":"markdown","6b6725a2":"markdown","c6ef932d":"markdown","82c3ddb9":"markdown","fd975bde":"markdown","f4663f3e":"markdown","a869e7ec":"markdown","0ff96877":"markdown","d33bff25":"markdown","ac5b5c24":"markdown","d27338e8":"markdown","820d800d":"markdown","834da44f":"markdown","ee34ef36":"markdown","2562d467":"markdown","bfe071ad":"markdown","8c8ff803":"markdown","b39457df":"markdown","3e491f01":"markdown","91836189":"markdown","4d7f8067":"markdown","52184098":"markdown","dac1fa97":"markdown","4dca4a3b":"markdown","1173f09f":"markdown","40319b4c":"markdown"},"source":{"c2617d95":"import numpy as np\nimport pandas as pd \nimport os\nimport random\nimport statistics\nfrom scipy import stats","1ded3b62":"random.seed(2021)\nnp.random.seed(2021)","77f4fa30":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","207a73e5":"from sklearn.datasets import load_boston\nboston_dataset = load_boston()","334370cf":"boston = pd.DataFrame(boston_dataset.data, columns = boston_dataset.feature_names)","152caa9c":"boston.head()","7fadb112":"boston['MEDV'] = boston_dataset.target","45013519":"names = boston_dataset.feature_names","78c70f54":"from sklearn.tree import DecisionTreeRegressor","1e3def84":"array = boston.values\n\nX = array[:, 0:13]\nY = array[:, 13]","49fbd11a":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.3, random_state = 1234)","b07f92c2":"model = DecisionTreeRegressor(max_leaf_nodes = 20)","f723a8fc":"model.fit(X_train, Y_train)","58b7ff96":"from sklearn.metrics import r2_score","33a2fcc7":"YHat = model.predict(X_test)","0ec3098f":"r2 = r2_score(Y_test, YHat)\nprint(\"R2 Score -> \", r2)","c80c4337":"import graphviz\nfrom sklearn import tree","6800e8e8":"fig = plt.figure(figsize=(25,20))\n_ = tree.plot_tree(model, \n                   feature_names=names,  \n                   class_names=boston_dataset.target,\n                   filled=True)","1089dcea":"plt.figure(figsize = (20,20))\ndot_data = tree.export_graphviz(model, out_file=None, \n                                feature_names=names,  \n                                class_names=boston_dataset.target,\n                                filled=True, rounded= True)\n\n# Draw graph\ngraph = graphviz.Source(dot_data, format=\"png\") \ngraph","40998281":"\"\"\"import pydotplus\ngraph = pydotplus.graph_from_dot_data(dot_data)\nnodes = graph.get_node_list()\n\nfor node in nodes:\n    if node.get_label():\n        print(node.get_label())\n        node.set_fillcolor('yellow')\n        \n\ngraph.write_png('colored_tree.png')\n\"\"\"","529260fc":"from sklearn.linear_model import LinearRegression","f22ab471":"X = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))\ny = np.array([5, 20, 14, 32, 22, 38])","b1cf9916":"model = LinearRegression()\n\nmodel.fit(X,y)\n\nr2_score = model.score(X,y)\nprint(\"Coefficient of Determination -> \" , r2_score)\n\nprint(\"Intercept -> \", model.intercept_)\nprint(\"Slope -> \", model.coef_)","be558bce":"x = [[0, 1], [5, 1], [15, 2], [25, 5], [35, 11], [45, 15], [55, 34], [60, 35]]\ny = [4, 5, 20, 14, 32, 22, 38, 43]\n\nX = np.array(x)\ny = np.array(y)\n\n\nmodel = LinearRegression()\n\nmodel.fit(X,y)\n\n\nr2_score = model.score(X,y)\n\nprint(\"Coefficient of Determination -> \", r2_score)\n\nprint(\"Model Intercept => \", model.intercept_)\n\nprint(\"Slope => \", model.coef_)","750d95ae":"import statsmodels.api as sm","07305d7e":"x = [[0,1], [5,1], [15,2], [25,5], [35,11], [45,15], [55,34], [60,35]]\ny = [4,5,20,14,32,22,38,43]\n\nx, y = np.array(x), np.array(y)\n\n\nx = sm.add_constant(x)\n\nmodel = sm.OLS(y, x)\n\n\nresults = model.fit()\n\nprint(results.summary())","4bc7cf0b":"print(\"Coefficient of Determination -> \", results.rsquared)\n\nprint(\"Adjusted Coefficient of Determination => \", results.rsquared_adj)\n\nprint(\"Regression coefficients => \", results.params)","5e27a33c":"from sklearn.preprocessing import PolynomialFeatures","c876f5c4":"x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))\ny = np.array([15, 11, 2, 8, 25, 32])\n\n\ntransformer = PolynomialFeatures(degree = 2, include_bias = False)\n\nx = transformer.fit_transform(x)\n\nmodel = LinearRegression().fit(x,y)\n\nr2_score = model.score(x,y)\n\nprint(\"Coefficient of Determination => \", r2_score)\n\nprint(\"Intercept -> \", model.intercept_)\nprint(\"Coefficients => \", model.coef_)","66f6a94f":"from sklearn.feature_selection import SelectKBest \nfrom sklearn.feature_selection import chi2 \n\n\ndata = pd.read_csv('..\/input\/cat-in-the-dat\/train.csv')","f7223c91":"data.head()","e3628a9e":"data.drop(['id'], axis = 1, inplace = True)","4bf066b6":"data.dtypes","0cd32562":"for col in data.columns:\n    print(col, data[col].nunique())","d190d5d9":"for col in data.columns:\n    print(col, '\\n\\n',data[col].value_counts())\n    print('-'*10)","66192110":"data['bin_3'] = data['bin_3'].map({\"T\" : 1, \"F\" : 0})\ndata['bin_4'] = data['bin_4'].map({\"Y\" : 1, \"N\" : 0})","1208808e":"data.head()","acb04c35":"for col in ['ord_1', 'ord_2', 'ord_3', 'ord_4']:\n    print(col, list(np.unique(data[col])))","74647d29":"m1_ord1 = {'Novice' : 0, 'Contributor' : 1, 'Expert' : 2, 'Master' : 3, 'Grandmaster' : 4}\n\ndata['ord_1'] = data['ord_1'].map(m1_ord1)","754736df":"data.head()","e2fcabe0":"m2_ord2 = {'Boiling Hot' : 0, 'Cold' : 1, 'Freezing' : 2, 'Hot' : 3, 'Lava Hot' : 4, 'Warm' : 5}\n\ndata['ord_2'] = data['ord_2'].map(m2_ord2)","f824db25":"data.head()","67484f73":"data['ord_3'] = data['ord_3'].apply(lambda x : ord(x) - ord('a'))\ndata['ord_4'] = data['ord_4'].apply(lambda x : ord(x) - ord('A'))","34b12b96":"data.head()","ca03a314":"data['ord_5a'] = data['ord_5'].str[0]\ndata['ord_5b'] = data['ord_5'].str[1]\n\ndata['ord_5a'] = data['ord_5a'].map({val : idx for idx, val in enumerate(np.unique(data['ord_5a']))})\ndata['ord_5b'] = data['ord_5b'].map({val : idx for idx, val in enumerate(np.unique(data['ord_5b']))})","42497112":"data.head()","9cc18fbb":"data[['nom_0', 'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']]","3638bb65":"data['nom_1'].value_counts()","e1ee3136":"data['nom_2'].value_counts()","2744ee63":"data['nom_3'].value_counts()","0fb750a2":"data['nom_4'].value_counts()","934f9ce1":"data['nom_5'].value_counts()","d2360c92":"data['nom_6'].value_counts()","a4cc2315":"data['nom_7'].value_counts()","c66bed08":"data.drop(['ord_5', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9'], axis = 1, inplace = True)","f123fef7":"\"\"\"data['day'] = data['day'] \/ 7.0\n\ndata['month'] = data['month'] \/ 12.0\"\"\"","89a21847":"data.head()","8d3b686e":"data['nom_1'].value_counts()","c03e6b17":"m1_nom1 = {'Trapezoid' : 0, 'Square' : 1, 'Star' : 2, 'Circle' : 3, 'Polygon' : 4, 'Triangle' : 5}\n\ndata['nom_1'] = data['nom_1'].map(m1_nom1)","88c0e23d":"data['nom_2'].value_counts()","91bb0ef5":"m2_nom2 = {'Lion' : 0, 'Cat' : 1, 'Snake' : 2, 'Dog' : 3, 'Axolotl' : 4, 'Hamster' : 5}\ndata['nom_2'] = data['nom_2'].map(m2_nom2)","830fdca7":"data['nom_3'].value_counts()","9c342545":"m3_nom3 = {'Russia' : 0, 'Canada' : 1, 'China' : 2, 'Finland' : 3, 'Costa Rica' : 4, 'India' : 5}\n\ndata['nom_3'] = data['nom_3'].map(m3_nom3)","6f95a784":"data['nom_4'].value_counts()","7b15326d":"m4_nom4 = {'Oboe' : 0, 'Piano' : 1, 'Bassoon' : 2, 'Theremin' : 3}\n\ndata['nom_4'] = data['nom_4'].map(m4_nom4)","4949ce8e":"data.head()","105080c3":"data['nom_0'].value_counts()","4b48bf2f":"m0_nom0 = {'Green' : 0, 'Blue' : 1, 'Red' : 2}\n\ndata['nom_0'] = data['nom_0'].map(m0_nom0)","3c7ea5e3":"df_copy = data.copy()\ndf_copy.drop(['target'], axis = 1, inplace = True)","fb7267ad":"df_copy = pd.get_dummies(df_copy, columns = df_copy.columns)\ndf_copy","2a05f6b6":"data.head()","b1e64908":"#X = data.drop(['target'], axis = 1)\nX = df_copy\ny = data.target","c1618c41":"# perform feature engineering to encode categorical variables so as to be processed by chi2_feature transform","fd7a291d":"chi2_features = SelectKBest(chi2, k = 10)\nX_kbest_features = chi2_features.fit_transform(X,y)\n\nprint(\"Original Number of Features -> (shape)\", X.shape[1])\n\nprint(\"K Best Features (shape)-> \",X_kbest_features.shape[1])\n\n","01a8803f":"X_kbest_features","b4f2e4dc":"z = (74 - 72 )\/ 2\n\nprint(z)","ec6a804c":"z1 = (13 - 12) \/ 1.5\n\nz2 = (15 - 14) \/ 1\n\nprint(\"Z Score for Fluffy -> \", z1)\nprint(\"Z Score for Mittens -> \", z2)","b4d78d91":"import random","a75c5da0":"random.seed(2021)","4044a716":"df = pd.DataFrame([random.sample(range(1, 1000), 4) , random.sample(range(1, 1000), 4), random.sample(range(1, 1000), 4), random.sample(range(1, 1000), 4)], columns = ['A', 'B', 'C', \"D\"])","73960b63":"df","90c68c60":"df_melt = pd.melt(df.reset_index(), id_vars = ['index'], value_vars = ['A','B','C','D'])\ndf_melt.columns = ['index', 'treatments', 'value']","e58663d7":"df_melt","3c7b6854":"sns.boxplot(x='treatments', y='value', data=df_melt, color='#99c2a2')\nsns.swarmplot(x=\"treatments\", y=\"value\", data=df_melt, color='#7d0013')\nplt.show()","9dafa3fe":"from scipy import stats","c0fe569d":"fvalue, pvalue = stats.f_oneway(df['A'], df['B'], df['C'], df['D'])\nprint(\"f Value -> \", fvalue)\nprint(\"p value -> \", pvalue)","80cd0d9b":"import statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\n\nmodel = ols('value ~ C(treatments)', data = df_melt).fit()\n\nanova_table = sm.stats.anova_lm(model, typ = 2)\nanova_table","fe244702":"data = pd.DataFrame(list(zip(['A','A','A','B','B','B', 'C', 'C', 'C', 'D', 'D', 'D'], [np.random.ranf() for _ in range(12)], [np.random.ranf() for _ in range(12)], [np.random.ranf() for _ in range(12)])), columns = ['Genotype', '1_year', '2_year', '3_year'])","53685cb6":"data","aa7bc759":"data_melt = pd.melt(data, id_vars = ['Genotype'], value_vars = ['1_year', '2_year', '3_year'])","b3435620":"data_melt.head()","07942524":"data_melt.columns = ['Genotype', 'years', 'value']","dc315557":"sns.boxplot(x = 'Genotype', y = 'value', hue = 'years', data = data_melt, palette = ['r', 'k', 'w'])","dc46f624":"model = ols('value ~ C(Genotype) + C(years) + C(Genotype) : C(years)', data = data_melt).fit()","6a4c9249":"anova_table = sm.stats.anova_lm(model, typ = 2)\n\nanova_table","f5bb0843":"!pip install -q bioinfokit\nfrom bioinfokit.analys import stat","50d5cc7d":"res = stat()\nres.tukey_hsd(df = df_melt, res_var = 'value', xfac_var = 'treatments', anova_model = 'value ~ C(treatments)')\noutput = res.tukey_summary","5e19edcf":"output","38f70579":"from scipy.stats import binom\n\nn = 6\np = 0.6\n\nr_values = list(range(n + 1))\n\nmean, var = binom.stats(n, p)\n\n\ndist = [binom.pmf(r, n, p) for r in r_values]\n\ndf = pd.DataFrame(list(zip(r_values, dist)), columns = ['r', 'p(r)'], index = None)\n\ndf","1cdd227a":"df['p(r)'].plot.bar()","879d371c":"mu, sigma = 0.5, 1","ebc119a8":"data = np.random.normal(mu, sigma, 10000)","80de9458":"count, bins, ignored = plt.hist(data, 20)\n\n","69c130d4":"from scipy.stats import poisson\ndata_poisson =poisson.rvs(mu = 3, size = 10000, random_state= 2021)","3e1fa565":"sns.distplot(data_poisson, bins = 30, kde = False, \n            color = 'skyblue',\n             hist_kws = {'linewidth' : 15, 'alpha' : 1}\n            )\n\n","929d1500":"\nmeans = [np.mean(np.random.randint(1, 7, 100)) for _ in range(1000)]\n\nplt.hist(means)\nplt.show()","a3cb7cbf":"def pvalue(mu, sigma, samp_size, samp_mean = 0, deltam = 0):\n    \n    np.random.seed(2021)\n    ","041f5240":"# It is a problem of binomial distribution. N = 20, p = 0.5, q = 0.5, x = 14\n\n\n# use the probability mass function to evaluate the probability\nprint(stats.binom.pmf(k = 14, n = 20, p = 0.5))","1dfb8abd":"## Again a question of binom distribution. N = 90, k = 22, p = 0.25\n\n\n# In this question, we'll be using cumulative distribution function and subtract the outcome from 1. \n# Find the cumulative probability till k = 21 and subtract it from 1 so that outcome is >= 22 marks\n1 - stats.binom.cdf(k = 21, n = 90, p = 0.25)","84bd411d":"### Binomial Distribution problem\n\n## Defective sample is to be considered as success. p = 0.05, N = 10, k = 2\n\nstats.binom.pmf(k = 2, n= 10, p = 0.05)","0436473c":"## Since we are dealing with discrete occurences over an interval, this is the case of poisson distribution.\n# x = 5, mu = 3\nstats.poisson.pmf(5, 3)","d52c0acc":"## Case ii. x = 0, mu = 3\n\nstats.poisson.pmf(0,3)","de17426b":"# Case iii. x <= 2, mu = 3\n\nstats.poisson.cdf(2, 3)","7b55f460":"## Case iv. x = 1, mu = 3\n\nstats.poisson.pmf(1,3)","6b92d8df":"# part 1\n# x = 241.25\n# loc = 200\n# scale = 25\n\n# P(z > 241.25)\n\n1 - stats.norm.cdf(241.25, 200, 25)","0f92ebb1":"# part 2\n\n# x = 250\n# loc = 200\n# scale = 25\n\n# P(z < 250)\n\nstats.norm.cdf(250, 200, 25)","5ada3727":"# Expected value of a binomial experiment is np\n\n0.5 * 100","b05d5800":"stats.binom.cdf(2, 5, 0.3)","2fa2bc94":"stats.binom.cdf(45, 100, 0.5)","b41b2bef":"stats.binom.pmf(2,5, 0.167)","13112457":"stats.norm.cdf(365, 300, 50)","f5eaaaae":"stats.norm.cdf(110, 100, 10) - stats.norm.cdf(90, 100, 10)","2b62afa5":"stats.poisson.cdf(3, 5)","30aeaa86":"stats.poisson.pmf(2,3)","2be6ae69":"random.seed(2021)\nlst1 = random.sample(range(100), 50)\nprint(\"Elements of 1st list -> \", lst1, \"\\n\")\n\n\nlst2 = random.sample(range(100), 50)\nprint(\"Elements of 2nd list -> \", lst2, \"\\n\")\n","283fe69a":"corr, _ = stats.pearsonr(lst1, lst2)\n\nprint(\"Pearsons correlation : \", corr)","df2b3fc3":"def variance(data):\n    \n    n = len(data)\n    \n    mean = sum(data)\/n\n    \n    deviations = [(x - mean) ** 2 for x in data]\n    \n    variance = sum(deviations) \/ n\n    \n    return variance","550a351e":"random.seed(2021)\ndata = random.sample(range(1000), 10)\n\nvariance(data)","d388310d":"def variance(data , dof = 0):\n    \n    n = len(data)\n    mean = sum(data) \/ n\n    \n    return sum((x - mean)** 2 for x in data) \/ (n - dof)\n\n","093f1f53":"variance(data, dof = 1)","1e87f049":"from sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans","6d16eddd":"X, y = make_blobs(n_samples = 300, centers = 4, cluster_std = 0.60, random_state = 0)\n\n\nplt.scatter(X[:,0], X[:,1])","71c9e0fb":"wcss = []\n\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\n    \n    kmeans.fit(X)\n    wcss.append(kmeans.inertia_)\n    \n    \nplt.plot(range(1,11), wcss)\nplt.title('Elbow method')\nplt.xlabel('Number of Clusters')\nplt.ylabel('WCSS')\nplt.show()","7b891003":"kmeans = KMeans(n_clusters=4, init='k-means++', max_iter=300, n_init=10, random_state=0)\npred_y = kmeans.fit_predict(X)\nplt.scatter(X[:,0], X[:,1])\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red')\nplt.show()","3cc97ff8":"from sklearn.metrics import silhouette_samples, silhouette_score","ee62a90e":"n_clusters = [2,3,4,5,6,7,8,9]\n\n\nfor n_cluster in n_clusters:\n    \n    clusterer = KMeans(n_clusters = n_cluster, random_state = 10)\n    cluster_labels = clusterer.fit_predict(X)\n    \n    silhouette_avg = silhouette_score(X, cluster_labels)\n    print(\"\\nFor n_clusters = \", n_cluster, \"\\nThe average silhouette_score is : \", silhouette_avg)\n    ","6c453188":"!pip install -q https:\/\/github.com\/scikit-learn-contrib\/scikit-learn-extra\/archive\/master.zip\n","20774324":"from sklearn_extra.cluster import KMedoids\n\nkmediods = KMedoids(n_clusters = 3, random_state = 0).fit(X)","24774587":"kmediods.labels_","9c7c971d":"from sklearn.cluster import AgglomerativeClustering \nimport scipy.cluster.hierarchy as shc","fccd1b60":"df = pd.read_csv('..\/input\/ccdata\/CC GENERAL.csv')\n\nX = df.drop(['CUST_ID'], axis = 1)\n\nX.fillna(method = 'ffill', inplace = True)","9a9081a7":"from sklearn.preprocessing import StandardScaler, normalize\n\nscaler = StandardScaler()\n\nX_scaled = scaler.fit_transform(X)\n\n\nX_normalized = normalize(X_scaled)\n\n\nX_normalized = pd.DataFrame(X_normalized)","fb549144":"X_normalized","f5a15826":"from sklearn.decomposition import PCA \n\n\npca = PCA(n_components = 2)\n\n\nx_pca = pca.fit_transform(X_normalized)\n\nx_pca = pd.DataFrame(x_pca)\nx_pca.columns = ['P1', 'P2']","ba2fe71f":"plt.figure(figsize =(8, 8))\nplt.title('Visualising the data')\nDendrogram = shc.dendrogram((shc.linkage(x_pca, method ='ward')))","a87ab1d7":"from statsmodels.stats import weightstats as stests\n\n\n","88778c05":"np.random.seed(2021)\nv = np.random.normal(size = 100)\n\nres = stats.kstest(v, 'norm')\n\nprint(res)","e9c0c2d7":"def mean_confidence_interval(data, confidence = 0.95):\n    a = 1.0 * np.array(data)\n    n = len(a)\n    \n    m, se = np.mean(a), stats.sem(a)\n    h = se * stats.t.ppf((1 + confidence) \/ 2., n - 1)\n    \n    return m, m - h, m + h","32fd7d9f":"# Let's assume the following was a confusion matrix obtained for a Binary Dataset","d8dc4d5f":"from sklearn.metrics import confusion_matrix","aa03d5ef":"y_true = [0,1,0,1,0,1]\ny_pred = [0,0,1,1,0,1]\n\nconfusion_matrix(y_true, y_pred)","e8963c61":"tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n\nprint(\"True Negatives -> \", tn)\nprint(\"False Positives -> \", fp)\nprint(\"False Negatives -> \", fn)\nprint(\"True Positives -> \", tp)","a10df5a8":"def calculate_performance(tn, fp, fn, tp):\n    \n    \n    accuracy = (tp + tn)\/ (tp + tn + fp + fn)\n    \n    precision = tp \/ (tp + fp)\n    \n    recall = tp \/ (tp + fn)\n    \n    f1 = ( 2 * precision * recall ) \/ (precision + recall)\n    \n    \n    return accuracy, precision, recall, f1","c6d1273a":"acc, precision, recall, f1 = calculate_performance(tn, tp, fn, tp)\n\nprint(\"Accuracy of the hypothetical model -> \", acc)\nprint(\"Precision of the hypothetical model -> \", precision)\nprint(\"Recall of the hypothetical model -> \", recall)\nprint(\"F1-score of the hypothetical model -> \", f1)","0eb03f0a":"from sklearn.metrics import ConfusionMatrixDisplay\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_true, y_pred),\n                               display_labels=[0,1])\n\ndisp.plot()","6de31e5a":"# For multi-class classifiers, let's understand how to obtain the metrics given the hypothetical confusion matrix.","c99013fd":"y_true = [0,1,2, 0,1,2,0,1,2]\ny_pred = [0,2,1,0,1,2,1,0,1]\n\nconfusion_matrix(y_true, y_pred)","0f6edb7c":"disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_true, y_pred),\n                               display_labels=[0,1,2])\n\ndisp.plot()","89023362":"def calculate_metrics_multi(cnf_matrix):\n    FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix) \n    FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n    TP = np.diag(cnf_matrix)\n    TN = cnf_matrix.sum() - (FP + FN + TP)\n    \n    FP = FP.astype(float)\n    FN = FN.astype(float)\n    TP = TP.astype(float)\n    TN = TN.astype(float)\n    \n    # Sensitivity, hit rate, recall, or true positive rate\n    TPR = TP\/(TP+FN)\n    # Specificity or true negative rate\n    TNR = TN\/(TN+FP) \n    # Precision or positive predictive value\n    PPV = TP\/(TP+FP)\n    # Negative predictive value\n    NPV = TN\/(TN+FN)\n    # Fall out or false positive rate\n    FPR = FP\/(FP+TN)\n    # False negative rate\n    FNR = FN\/(TP+FN)\n    # False discovery rate\n    FDR = FP\/(TP+FP)\n    # Overall accuracy for each class\n    ACC = (TP+TN)\/(TP+FP+FN+TN)\n    \n    f1 = 2 * precision * recall \/ (precision + recall)\n    \n    print(\"The calculated metrics are as follows -> \\n\\n\")\n    print(f\"\\n1. Accuracy = {ACC} \\n2. Recall or Sensitivity = {TPR} \\n3. Specificity or True Negative Rate = {TNR} \\n4. Precision = {PPV} \\n5. Negative Predictive Value = {NPV} \\n6. Fall out or False Positive Rate = {FPR} \\n7. False Negative Rate = {FNR} \\n8. False Discovery Rate = {FDR} \\n9. F1-Score = {f1}\")","f3233cf5":"calculate_metrics_multi(confusion_matrix(y_true, y_pred))","ad3dbf44":"data1 = np.arange(1, 10, 2)\ndata1","49a338ce":"print(\"Mean of the above data set is => \", np.mean(data1))","9a1663cc":"data1 = np.arange(1, 21, 2)\ndata1","fcc7bae8":"weights = np.random.random(len(data1))\nweights","46e5be30":"def weighted_mean(data_lst, weights_lst):\n    return np.average(data_lst, weights = weights_lst)","a99bce6e":"print(\"Weighted Mean is -> \", weighted_mean(data1, weights))","4326c506":"data1 = np.arange(1, 21, 2)\ndata1","82fba8bd":"np.median(data1)","dfda12b0":"data1 = np.arange(1, 10, 2)\ndata1","8562dad5":"print(\"Median of the above dataset is -> \", np.median(data1))","13bc2778":"data1 = random.sample(range(1000), 20)\ndata1","22b7f4ac":"print('25th Percentile -> ', np.percentile(data1, 25))","d935f5fb":"print('50th Percentile -> ', np.percentile(data1, 50))","88b65b0a":"print('75th Percentile -> ', np.percentile(data1, 75))","fa7f4d21":"random.seed(2021)\n\ndata = [random.sample(range(100), 4), \n       random.sample(range(100), 4),\n       random.sample(range(100), 4)]\ndata","3811e7ba":"print(\"25th percentile value for axis = None -> \", np.percentile(data, 25,))\nprint(\"25th percentile value for axis = 0 -> \", np.percentile(data, 25,axis = 0))\nprint(\"25th percentile value for axis = 1 -> \", np.percentile(data, 25,axis = 1))","3583c2fb":"print(\"50th percentile value for axis = None -> \", np.percentile(data, 50,))\nprint(\"50th percentile value for axis = 0 -> \", np.percentile(data, 50,axis = 0))\nprint(\"50th percentile value for axis = 1 -> \", np.percentile(data, 50,axis = 1))","97b2fb0b":"print(\"75th percentile value for axis = None -> \", np.percentile(data, 75,))\nprint(\"75th percentile value for axis = 0 -> \", np.percentile(data, 75,axis = 0))\nprint(\"75th percentile value for axis = 1 -> \", np.percentile(data, 75,axis = 1))","6cbe0660":"from scipy.stats import skew\nimport pylab\nx1 = np.linspace(-10, 10, 1000)\ny1 = 1.\/ (np.sqrt(2. * np.pi)) * np.exp(-.5 * (x1) ** 2)\n\npylab.plot(x1, y1)\n\nprint(\"Skewness of the data --> \", skew(y1))","807256b5":"x1 = np.linspace(-5, 10, 1000)\ny1 = 1.\/ (np.sqrt(2. * np.pi)) * np.exp(-.5 * (x1) ** 2)\n\npylab.plot(x1, y1)\n\nprint(\"Skewness of the data -> \", skew(y1))","cbbde5d4":"x = np.random.normal(0, 2, 10000)\n\n\nprint(\"X: \\n\", x)\n\nprint(\"\\nSkewness for data : \", skew(x))","66a3038f":"x = np.linspace(-10, 10, 1000)\ny1 = 1.\/(np.sqrt(2.*np.pi)) * np.exp( -.5*(x1)**2  )\n\n\npylab.plot(x,y1, '*')\n\nprint(\"Kurtosis for normal distribution : \", stats.kurtosis(y1))\n\nprint(\"Kurtosis for normal distribution : \", stats.kurtosis(y1, fisher = False))\n\nprint(\"Kurtosis for normal distribution : \", stats.kurtosis(y1, fisher = True))","fd769ef1":"random.seed(2021)\ndata1 = random.sample(range(-1000, 1000), 100)\n#data1","8025f172":"min_value = min(data1)\n\nmax_value = max(data1)\n\nprint(f\"Range of the dataset -> [{min_value},{max_value}]\")","d04c55e1":"\n\n\nrandom.seed(2021)\n\ndata = random.sample(range(1000), 30)\n\nprint(f\"Dataset -> {data}\\n\\n\")\nIQR = stats.iqr(data, interpolation = 'midpoint')\n\nprint(IQR)","cbf5e022":"print(f\"Variance of the sample set --> {statistics.variance(data)}\")","98c6540b":"import scipy.stats as stats","8ac311e2":"stats.zscore(data)","51599745":"np.random.seed(2021)\ndata1 = np.random.randn(5,5)\n\nprint(\"\\nVariation at axis = 0: \\n\", stats.variation(data1, axis = 0))\nprint(\"\\nVariation at axis = 1: \\n\", stats.variation(data1, axis = 1))","13e9b64c":"df = pd.read_csv(\"..\/input\/logistic-regression-stats-dataset\/logit_train1.csv\", index_col = 0)\n\nXtrain = df[['gmat', 'gpa', 'work_experience']]\nytrain = df[['admitted']]\n\n\nlog_reg = sm.Logit(ytrain, Xtrain).fit()\n\n\nprint(log_reg.summary())","fdf4251e":"### 2. Regressions","b70b0882":"Q2. A question paper contains 90 multiple choice questions. There are four alternatives answers to each question of which only one is correct. What is the probability to score atleast 22 marks without any preparation (random guessing).","09ceeffd":"### 1. CART Algorithms","eaea09a2":"SSE","48eb2ba9":"### 6. ANOVA Test","543eda7b":"We have two types of T-Tests -> <br>\n* 1 sample t-test\n* 2 sample t-test","6792d7f9":"Range is the span of values in the entire dataset. <br>\nRange is denoted by [min_value, max_value]","6385254d":"As we can see, for n = 4, we obtain the highest silhouette score. <br>\nThis is exactly the same number we obtained using elbow method as well as it is the number of clusters we have defined in our dataset.","fdcfd801":"KS Test","5aa8d5c1":"ANOVA Hypotheses -> <br>\n* Null Hypotheses = Group means are equal. No variation in the groups. \n* Alternative Hypothesis = At least, one group is different from other groups.","71485b41":"Q6. Assuming a binomial experiment with p = 0.5 and a sample size of 100. The expected value of this distribution is?","1525717c":"**MSR ->** <br><br>\n\nMean Square due to regression, <br>\nSSR \/ ","fa80ac1e":"**Important Terminologies ->** <br><br><br>\n**Level of Significance** => the degree of significance in which we accept or reject the nuol hypothesis. A 5% significance means the the result should be atleast 95% confident to give a similar result in each sample. ","096e1a96":"#### 6.2 Two-Way ANOVA Test","1fe5e897":"In a local teaching district a technology grant is available to teachers in order to install a cluster of four computers in their classrooms. From 6250 teachers in the district, 250 were randly selected and asked if they felt that computers were an essential teaching tool for their classroom. Of those selected, 142 teachers felt that computers were an essential teaching tool. <br>\n1. Calculate a 99% confidence interval for the propertion of teachers who felt that computers are an essential teaching tool. <br>\n2. How could the survey be changed to narrow the confidence interval but to maintain the 99% confidence interval?","2c79be5d":"![](https:\/\/media.geeksforgeeks.org\/wp-content\/uploads\/20200311233526\/formula6.png)","4c8d82d0":"analyze the performance based on the following ROC curve generated.","6b4699cf":"#### I. Python Implementation","663e3f6f":"How to read a multi-class confusion matrix?\n\n![](https:\/\/miro.medium.com\/max\/875\/1*uQDpo9iISx00ucl3gftLVA.png)","a18d63a5":"Q1. The grades on a physics midterm at Covington are roughly symmetric with \u03bc=72, \u03c3=2.0. <br>\nStephanie scored 74 on the exam. <br>\nZ-Score to stephanie's exam score. ","2eee31ad":"We're done with dealing of binary variables. <br>\nNow we're left to deal with the nominals & ordinals.","a82b893a":"ANOVA Assumptions -> <br><br>\n* Residuals(experimental error) are normally distributed.(Shapiro-Wilks Test)\n* Homogenity of variances (variances are equal between treatment groups) (Levene's or Bartlett's Test)\n* Observations are sampled independently from each other. ","5851e9ad":"### 24. Confusion Matrix, ROC & Regression Analysis","ff36678f":"You can never experiment with all your customers (population). However, to draw a conclusion for an experiment which is a good representaion of your customers, you need to perform repeated experiments on different set of customers (different samples of the not normally distributed population\/sample as per the context) and confirm your hypotheses. ","88363a7a":"**Thresholding in ROC**","2c3c9330":"Label Encoding multiple columns","a4f30f50":"#### 19. ii. K-Mediods","45969900":"SST = SSR + SSE <br>\nThe total variability = Explained Variability + Unexplained Variability","b6c9ae11":"Multiple or multivariate linear regression is linear regression with two or more independent variables. <br>\nPolynomial Regression is considered a generalized case of linear regression. You assume polynomial dependence between the output and the input.  Which means, the regression equation can now include non-linear terms. ","2db1c77c":"An F-Test based on MSR \/ MSE can be used to test the statistical significance of the overall relationship between the dependent variable and the set of independent variables. <br>\nLarge values of F support the conclusion that the overall is statistically significant. ","da1889ef":"### 9. Normal Distribution","6b489ba8":"### 8. Binomial Distribution","0a9d24a8":"Considering Proportions -> z test","5482f160":"Q9. Suppose a die is tossed 5 times. Probability of getting exactly 2 fours?","041dc729":"One-Sample Z-Test","19d9f57b":"It is the ratio of standard deviation to mean. ","d3ec9a15":"Types of T-Test <br>\n* Independent Samples t-test\n* Paired Sample t-test\n* One Sample t-test","69104f96":"With the help of confusion matrix, we can obtain values for TN, FP, FN, TP","c37c954a":"##### Tools Used\n\nDataset Used -> Boston Dataset (UCI Machine Learning Repository)","44c2e191":"Cluster Analysis vs Discriminant Analysis","f4a9ffb4":"Wald Test -> <br>\nMeasures an individual independent variable's significance","717ea6a4":"It is a statistical method used for making statistical decisions using experimental data. <br>\nUsed to evaluate two mutually exclusive statements in a population to determine which statement is best supported using the sample data.","285f8d2c":"Q7. Probability of acceptance of a student in a college is 0.3 <br>\nIf 5 students apply, probability that at most 2 are selected?","ebff9541":"### 14. Measuring Correlation","8c53bb0b":"**Assumptions of Logistic Regression ->** <br>\n* Binary Logistic Regression requires the dependent variable to be binary\n* The variables should be independent of each other. That is, the model should have little or no multicollinearity. \n* Samples sizes should be preferably large. \n* ","95ffd879":"T-Test <br>\nType of inferential statistics used to determine if there is a significant difference between means of two groups which may be related in certain features. <br>\nUsed when the datasets would follow a normal distribution and may have unknown variances. ","033c30f7":"One-Way F-Test (Anova) -> <br>\n","fa512110":"### 11. Bernoulli Distribution","ce3173ef":"V. Confidence Interval for 2 populations or (proportions)","32a1186a":"Type I Error","8c8342d1":"It helps in the measure of how heavy the tail is in compared to a normal distribution.","d8535cd9":"You can also display the output of your confusion matrix in a better visual format","26d320f2":"**Applications ->** <br>\nThe parameters of a logistic regression model can be estimated by the probabilistic framework called Maximum Likelihood Estimation.","e9fdc661":"1. Linear Regression Analysis","60ccc84f":"Variance -> Measures how far from their mean the individual observations in dataset are. <br>\nStd Deviation -> Square root of variance is std deviation which measures the amount of dispersion of the dataset.","3e15ddbf":"a. Understanding Contigency Tables (also known as crosstab)","2a565340":"Contigency tables are the pivot tables obtained by utilizing the categorical variable. The contigency here is whether a variable affects the values of the caegorical variable. <br>\n","749594b3":"f. Standard Score or Z-Score","b85cc835":"Variance estimate of the population using sample data","e8e49b93":"Given a set of numbers -> [n1,n2,n3,n4,n5]<br><br>\n\nAverage or **Arithmetic Mean** -> Sum of dataset \/ num of data items <br>\n-> (n1 + n2 + n3 + n4 + n5) \/ 5 \n","edf0862a":"![](https:\/\/media.geeksforgeeks.org\/wp-content\/uploads\/kurtosis.jpg)","8117fe32":"case 1. 1D dataset","1ec927b6":"![](https:\/\/miro.medium.com\/max\/603\/1*D05sMUrwZIgvwsQVF_CJdg.jpeg)","823cff8b":"**What is it?** <br>\nROC is a plot useful for predicting the probability of a binary outcome. <br>\nIt is the plot of the false positive rate (x-axis) versus the true positive rate (y-axis) for a number of different candidate threshold values between 0.0 and 1.0 <br>","2fd6a244":"d. Interquartile Range","44e075aa":"For calculating weighted mean, <br>\nyou would require two lists. <br>\n* The data items list\n* The corresponding weights list","c1e8fe01":"Concluding Remarks about ROC & AUC Curves -> <br>\n* can be used as a summary of the model skill\n* ROC Curves of different models can be compared directly in general or for different thresholds.\n* Shape of ROC curves contains info about the predictive power of the model.\n* for imbalanced class distribution, ROC curves are very helpful. Helps to visualize the trade-off between TPR and FPR and thus help us to arrive at a threshold that minimizes the mis-classification cost.","0ff84f31":"Q8. Probability of obtaining 45 or fewer heads in 100 tosses of a coin?","8f6abeb9":"P-Value","17207d95":"I. Binary Classifiers ","dfba7081":"q1. A survey of 500 respondents is depicted in the table below. <br> (In Making)\n\n\n\n|Type| No regular Exercise  | Sporadic Exercise | Regular Exercise | Total  |\n|----|-----|----|----|-----|\n|Dormitory| 32  | 30 | 28 | 90 |\n|On-Campus Apartment| 74 | 64 | 42 | 180 |\n|Off-Campus Apartment| 110  | 25  | 15  | 150  |\n|At home| 39 | 6 | 5 | 50 |\n|Total | 255 | 125 | 90 | 470\n\n\n<br>Based on the above data, is there any relationship between exercise and student's living arrangement? <br>\n\n\nSoln. \n\n\nH0 -> Living arrangement and exercise are independent <br><br>\nH1 -> Are dependent \n","86636553":"2. Multiple Regression Analysis","c5035c52":"### 7. F Stats Test","15aa920f":"One Sample t-test -> <br>\n","ddc110c9":"#### Note:- Used only for Categorical Features.","6b65f487":"Regression tries to search for relationships among variables. ","f807cfb8":"method 2","674c0aed":"### Notebook in Making.  <br>\nEst. Date of Completion - 28-03-2021","d4bf8e11":"G - statistic in logistic regression is -> <br>\n-2ln[ likelihood without variable\/ likelihood with variable]","762200f2":"Let's define a function to output all the metrics needed to gauge model performance. <br>","83784805":"SSR","a5a56201":"Useful Resources -> <br>\n\n* https:\/\/www.maths.usyd.edu.au\/u\/UG\/SM\/STAT3022\/r\/current\/Lecture\/lecture03_2020JC.html#1\n* https:\/\/towardsdatascience.com\/maximum-likelihood-estimation-explained-normal-distribution-6207b322e47f#:~:text=%E2%80%9CA%20method%20of%20estimating%20the,observed%20data%20is%20most%20probable.%E2%80%9D&text=By%20assuming%20normality%2C%20we%20simply,the%20popular%20Gaussian%20bell%20curve.\n* https:\/\/online.stat.psu.edu\/stat462\/node\/207\/\n* https:\/\/psychscenehub.com\/psychpedia\/odds-ratio-2\/\n* http:\/\/statkat.com\/stat-tests\/logistic-regression.php#:~:text=Logistic%20regression%20analysis%20tests%20the,%3D%CE%B2K%3D0","b7d5e0c3":"**Ways to choose optimal number of clusters** -> <br>\n1. Elbow Method\n2. Silhouette coefficient","99357226":"* r takes values -1 to +1 \n* r = 0 means no correlation\n* can't be applied to ordinal variables\n* the sample size should be moderate 20 to 30. \n* outliers can lead to misleading calculations","717d9b2a":"**(Bonus or Optional) Polynomial Regression with Scikit-Learn** <br>\nIn polynomial regression, you need to transform the inputs to include non-linear terms.","c3ecee2b":"##### Interpretation","239b921e":"## Table of Contents\n\n* Understanding Data types\n    * Interval Scale\n    * Binary \n    * Categorical\n    * Ordinal \n    * Ratio Scaled\n    * Mixed Type\n* Different types of distances\n* Simmilarity and Dissimilarity Matrix\n* Familiarizing with different types of Error Metrics\n* Handling Missing data values\n* Central Tendency & Dispersion\n* Descriptive Statistics\n* Summary Statistics\n    * Central Tendency Statistics\n        * Arithmetic Mean\n        * Weighted Mean\n        * Median\n        * Percentile\n    * Dispersion\n        * Skewness\n        * Kurtosis\n        * Range\n        * Interquartile Range\n        * Variance\n        * Standard Score\n        * Coefficient of Variation\n* [Sample](https:\/\/www.kaggle.com\/antoreepjana\/statistics-for-ml-data-analysis\/#16.-Sample-Statistics) vs [Population statistics](https:\/\/www.kaggle.com\/antoreepjana\/statistics-for-ml-data-analysis\/#17.-Population-Statistics)\n* Random Variables\n* Probability Distribution Functions\n    * Uniform Distribution\n    * Exponential Distribution\n    * [Binomial Distribution](https:\/\/www.kaggle.com\/antoreepjana\/statistics-for-ml-data-analysis\/#8.-Binomial-Distribution)\n    * [Normal Distributions](https:\/\/www.kaggle.com\/antoreepjana\/statistics-for-ml-data-analysis\/#9.-Normal-Distribution)\n    * [Poisson Distributions](https:\/\/www.kaggle.com\/antoreepjana\/statistics-for-ml-data-analysis\/#10.-Poisson-Distribution)\n    * [Bernoulli Distribution](https:\/\/www.kaggle.com\/antoreepjana\/statistics-for-ml-data-analysis\/#11.-Bernoulli-Distribution)\n* [Measuring p-value](https:\/\/www.kaggle.com\/antoreepjana\/statistics-for-ml-data-analysis\/#13.-Calculating-p-Value)\n* [Measuring Correlation](https:\/\/www.kaggle.com\/antoreepjana\/statistics-for-ml-data-analysis\/#14.-Measuring-Correlation)\n* [Measuring Variance](https:\/\/www.kaggle.com\/antoreepjana\/statistics-for-ml-data-analysis\/#15.-Measuring-Variance)\n* Expected Value\n\n* [z-score](https:\/\/www.kaggle.com\/antoreepjana\/statistics-for-ml-data-analysis\/#5.-Z-Test)\n* Hypothesis Testing\n    * Null & Alternate Hypothesis\n    * Type 1 Error; Type 2 Error\n    * Various Approaches\n        * p-value\n        * critical value\n        * confidence interval value\n* z-stats vs t-stats\n\n* Two Sample Tests\n* Confidence Interval\n* Similarity & Dissimilarity Matrices\n* [Central Limit Theorem](https:\/\/www.kaggle.com\/antoreepjana\/statistics-for-ml-data-analysis\/#12.-Central-Limit-Theorem)\n* [Chi Square Test](https:\/\/www.kaggle.com\/antoreepjana\/statistics-for-ml-data-analysis\/#3.-Chi-Square-Test)\n* [T Test](https:\/\/www.kaggle.com\/antoreepjana\/statistics-for-ml-data-analysis\/#4.-T-Test)\n* [ANOVA Test](https:\/\/www.kaggle.com\/antoreepjana\/statistics-for-ml-data-analysis\/#6.-ANOVA-Test)\n    * [One Way Anova Test](https:\/\/www.kaggle.com\/antoreepjana\/statistics-for-ml-data-analysis\/#6.1-One-Way-ANOVA-Test)\n        * F Test (LSD Test)\n        * Tukey Kramer Test\n    * [Two Way Anova Test](https:\/\/www.kaggle.com\/antoreepjana\/statistics-for-ml-data-analysis\/#6.2-Two-Way-ANOVA-Test)\n        * Interaction Effects\n* [F Stats](https:\/\/www.kaggle.com\/antoreepjana\/statistics-for-ml-data-analysis\/#7.-F-Stats-Test)\n* [Regressions (Linear, Multiple) + ROC](https:\/\/www.kaggle.com\/antoreepjana\/statistics-for-ml-data-analysis\/#2.-Regressions)\n* Logistic Regression\n    * Python Implementation\n    * Calculating G Statistics\n* Residual Analysis\n* Maximum Likelihood Estimation\n* Cluster Analysis\n    * Partitioning Cluster Methods\n        * K-Means\n        * K Mediods\n    * Hierarchial Cluster Methods\n        * Agglomerative\n    * Density Based Cluster Methods\n        * DBSCAN\n* [CART Algorithms](https:\/\/www.kaggle.com\/antoreepjana\/statistics-for-ml-data-analysis\/#1.-CART-Algorithms)\n    * Python Implementation\n    * various Calculations involved\n        * Information Gain\n        * Gain Ratio\n        * Gini Index\n* Confusion Metrics, ROC & Regression Analysis\n* Bonus Topics\n    * Classification Thresholding\n    * Prediction Bias\n    * Sampling Methods\n        * Simple\n        * Convenience\n        * Systematic\n        * Cluster\n        * Stratified","b8d3e932":"**Receiver Operating Characteristic** <br>\n","97aa6389":"**z-score** tells us how many standard deviations away a value is from the mean.\n\nz = (X \u2013 \u03bc) \/ \u03c3\n","532ae461":"II. Multi-Class Classifiers","a4534c6a":"**Use regression analysis to ->** <br><br>\n* Model multiple independent variables\n* use polynomial terms to model the curvature. \n* include continuous and categorical variables\n* assess interaction terms to determine affect of one independent variable on the value of another variable.","b14c9fa1":"|  S.No | Likelihood   | Probability  |\n|---|---|---|\n| 1  | Refers to past events with known outcomes  | Refers to the occurence of future outcomes  |\n| 2  | eg. A coin is flipped 10 times and 10 heads occur. Likelihood the coin is an unbiased coin?  | eg. A coin flipped n times. Probability of getting heads.| \n| 3  | Sum of Likelihoods != 1  | Sum of Probabilities = 1  |  ","fc2c473b":"**Regression Analysis**","5e4e9569":"**How it works?** <br>\nStatement (Null Hypothesis) -> ","d84efda7":"Understanding the influence of threshold on the ROC Curve -> <br>\nThere are 2 possible cases for ROC Curve's threshold movement -> <br>\n* Shifting the threshold to the right\n* Shifting the threshold to the left","b51ddfe4":"g. Coefficient of Variation","de987dcc":"I. Confidence interval for a sample","9774c52c":"### 10. Poisson Distribution","4c7b0181":"Two Tailed Test","d11592a0":"### 3. Chi Square Test","8fd7907e":"### 21. Type-I Error & Type-II Error","34ed30dc":"The error is the difference between the observed value and the predicted value. <br>\nWe want to minimize the error. The smaller the error, the better the estimation power of the regression. <br>\n","4297e6e9":"Mittens had a longer lifespan","b70ffd90":"Elbow Method","525b5eff":"Q16. Suppose we select 5 cards from an ordinary deck of playing cards. What is the probability of obtaining 2 or fewer hearts?","621f1708":"Q12. Suppose the average number of lions seen on a 1-day safari is 5. What is the probability that tourists will see fewer than four lions on the next 1-day safari?","5f81046d":"Variance is the square of the difference of a variable from its mean. <br>\nIt measures the spread of random data in a set from its mean or median value. <br>\n* Low value for variance indicates the data are clustered together.\n* High value for variance indicates the data are spread widely.","364cd5df":"We'll learn how to custom paint your graph from the default settings (coming soon)","049b3de8":"degrees of freedom for the chi-squared distribution -> (rows -1) * (cols -1)","8db34239":"Normal Curve -> \n![](https:\/\/miro.medium.com\/max\/626\/1*gBnxoTRwo9sDovvegHfm6g.png)","c2e34ad8":"COnsidering Means -> Use t-test","a5a324b3":"**K-Means** clustering algorithm is sensitive to **outliers** as mean value is easily influenced by **extreme values**. <br>\n**K-Mediods** is a variant of K-Means which is more robust to noises and outliers.","19b4c6fe":"SST ","a54ed9bd":"![](https:\/\/cdn.askpython.com\/wp-content\/uploads\/2020\/10\/Probability-density-function-of-Normal-Distribution.jpg.webp)","95cad72a":"**Assumption** -> each cell in the contigency table has expected frequency atleast **5**","a0e928d5":"It is a range of values. <br>\n95% confidence interval is the most common. <br>\nNote -> 95% confidence interval doesn't mean 95% probability. ","76a0e36e":"Here each z-score tells us how many std. deviations each value is away from mean.","434ea43e":"b. Kurtosis","edc98c05":"### 19. Cluster Analysis","a164262c":"**AUC is desirable for 2 reasons** -> <br>\n* Scale invariant \n* classification-model-threshold invariant","3f232d9a":"![](https:\/\/developers.google.com\/machine-learning\/crash-course\/images\/ROCCurve.svg)","6b0de219":"![](https:\/\/media.geeksforgeeks.org\/wp-content\/uploads\/20200503191805\/Annotation-2020-05-03-191654-300x92.png)","321710a8":"We'll be generating the following percentile values for the dataset -> <br>\n* 50th Percentile \/ Median\n* 25th Percentile\n* 75th Percentile","808f77ed":"![](https:\/\/miro.medium.com\/max\/481\/1*2vTwIrqdELKJY-tpheO7GA.jpeg)\n\nStandardized Normal Curve <br>\nWhich means it is a normal distribution curve which is standardized (mean = 0, std = 1).","5b3ce7ff":"All the values are in accordance to the condition p > 0.05 <br>\nHence, aren't statistically significant.","548dc71b":"b. Performing Chi-Square Tests","506ea0a7":"The strength of the association between two variables is known as correlation test. <br>\nIf we want to know the relation between height and weight of human beings, a dataset of the same is to be obtain and correlation is to be found to justify or reject the above hypothesis. ","f9fa0b91":"Minkowski Distance <br><br>\nA generalization of both the Euclidean and the manhattan metric is the Minowski distance given by :- <br>\n\n\n![](https:\/\/slideplayer.com\/slide\/5070455\/16\/images\/16\/Minkowski+Distance+Minkowski+distance%3A+a+generalization.jpg)\n\n![](https:\/\/www.researchgate.net\/publication\/349155159\/figure\/fig1\/AS:989596292767746@1612949550717\/Three-typical-Minkowski-distances-ie-Euclidean-Manhattan-and-Chebyshev-distances.png)","de5685cc":"**What it states?** <br><br>\nEven when a sample is not normally distributed, if you draw multiple samples and take each of their averages, the averages will represent a normal distribution.<br><br>\nWhich means repeated sampling from a not normally distributed sample and taking the means of those repeated samples will end up being a normally distributed sample. <br><br>\n\n100 samples in total which are not normally distributed. Take random 10 samples say 50 times and take the mean of these samples. It will come out to be a normally distributed sample.","c791adbf":"**Case I : Shifting the threshold to the left**<br>\nThis would case TP to increase, FP to increase, FN to decrease and TN to decrease. <br>\nBy doing so, TPR or Sensitivity (which is TP \/ Sum(+ves)) increases. FPR (which is (FP \/ Sum(-ves) ) increases. If FPR increases, specificity decreases as FPR = 1 - specificity.","097a627d":"ANOVA test or F-Test","c0134a78":"![](https:\/\/miro.medium.com\/max\/963\/1*0XXmFcatWBkagH3YeYdpig.png)","d5c2b9a9":"### 22. Z-Stats & T-Stats","7f2bd5ce":"also known as \n* Gaussian Distribution\n* Bell Curve\n\n\n<br><br> Below is the probability distribution function (pdf) for Normal Distribution -> ","f4fb46ad":"Two Sample Z-Test","0b1f9bcd":"**Degree of Freedom** -> <br>\n","9888df40":"When you reject the null hypothesis but thay hypothesis was true. Type I error is denoted by alpha. The region that shows the critical region , is called the alpha region. <br>","6554b8f1":"if SSR = SST, our regression model captures all the observed variablity and its perfect. <br>\nESS -> Explained sum of squares.","e681d6de":"Helps to determine whether the distribution of the test statistics can be approximated by a normal distribution. <br>\nHelps to determine whether two sample means are approximately the same when their variance is known and the sample size is large enough.","d9649f04":"Dataset used -> https:\/\/www.kaggle.com\/c\/cat-in-the-dat","928f9046":"**In p-value tests, our task might be to find the probability that a sample mean could be x, given the hypothesis that the population mean is y.** <br>\n\n<br>\nConclusion => <br>\nThe p-value gives us the probability of observing what we observed, given the hypothesis is true. It doesn't tell us the probability that the null hypothesis is true.","c93923f2":"The aim of the plot is to analyze the predictive power of the predictor ","e447fced":"**Key Assumptions ->** <br>\n* \u03b5 is a random variable with an expected value of 0\n* the variance of \u03b5 is the same for all values of x\n* the values of \u03b5 are independent,\n* \u03b5 is a normally distributed random variable.","890420bd":"* useful for multiple population proportions\n* classify sample observations according to two or more characterstics\n* also called cross-classification table","c58f617a":"Q13. The average number of homes sold by the Acme Realty company is 2 homes per day. What is the probability that exactly 3 homes will be sold tomorrow?","590f0f44":"Q11. Suppose scores on an IQ test are normally distributed. If the test has a mean of 100 and a standard deviation of 10, what is the probability that a person who takes the test will score between 90 and 110?","ec01f7ac":"background -> \n\n* used to analyze the frequencies of two variables with multiple categories to determine their independency\n* qualitative variables\n* nominal data","ed430ca7":"Neither regression nor correlation analyses can be interpreted as establishing cause-and-effect relationships. <br><br> They can indicate only how or to what extent variables are associated with each other.<br><br> The correlation coefficient measures only the degree of linear association between two variables. Any conclusions about a cause-and-effect relationship must be based on the judgment of the analyst.","64a52be3":"**Mean Squared Error** -> <br>\n1\/n * SSE <br><br>\n**Root Mean Squared Error** -> <br>\nsqrt(MSE) <br><br>\n**R Squared** -> <br>\n1 - SSE \/ SST <br><br>\n**Adjusted R-squared** -> <br>\n1 - (n + k \/ n - k) * (1 - R**2) ","1e8f850f":"We have 5 ordinal variables of which 4 have few unique values and can be dealt in a similar manner. <br>\nord_5 has multiple unique values and needs to be handled separately. ","a0cac2e1":"a. Arithmetic Mean","66bd8a08":"c. Chi-Square Tests for Feature Selection","c673c140":"Note :- For linear regression models, we use Ordinary Least Squares (OLS) to fit the regression model and estimate the parameters B0 & B1. <br>\n**MLE** is based on the data we observe, what are the model parameters that maximize the likelihood of the observed data occuring?","8491b01e":"Q3. On an average 5% items supplied by a manufacturer are defective. If a batch of 10 items is inspected, what is the probability that 2 items are defective. ","cf067f4d":"**Residual Analysis ->** <br><br>\nThe analysis of residuals plays an important role in validating the regression model. <br>\nIf the error term satisfies the above 4 assumptions, then the regression model is valid. ","af307c7f":"Now we need to calculate TP, FN, FP, TN values. ","259ad800":"* x -> input value\n* mu -> mean\n* sigma -> std deviation","c150b52e":"#### 19. i. K Means Clustering","95cee318":"**Widely Used hypothesis testing types ->** <br>\n* T Test (Student T Test)\n* Z Test\n* ANOVA Test\n* Chi-Square Test","30a5b6c1":"ANOVA -> Analysis of Variance. <br>\nHelps to compare the means of more than 2 groups. <br>\nANOVA F Test is also called omnibus test. <br><br><br>\n\nMain types of ANOVA Test -> \n* One-way or One-factor \n* Two-way or Two-factor","fea8f4a9":"Interquartile range also called as midspread, or middle 50% or technically H-spread. <br>\nTechnically, it is the Q3 - Q1 where Q3 is the third quartile and the first quartile. <br>\nIt covers the center of the distribution and contains 50% of the observations. <br><br>\n\n**IQR** = **Q3 - Q1**","f5017955":"![](https:\/\/miro.medium.com\/max\/601\/1*QkWHqoSHSBig31InTzr8TA.jpeg)","e77460fe":"ANOVA Working -> <br><br>\n* Check sample sizes, i.e., Equal number of observations in each group. \n* Calculate Mean Square for each group (MS) (SS of group\/degrees of freedom-1)\n* Calc Mean Sq. Error (SS Error \/ df of residuals)\n* Calc F value (MS of group \/ MSE)","927c94e6":"### 13. Calculating p-Value","a193912e":"When we accept null hypothesis but it is false. Denoted by Beta. The normal curve that shows the acceptance region is called the beta region. ","b8f77c67":"In Two-Way ANOVA Test, we have 2 independent variables and their different levels","5e06dbd2":"b. Weighted Mean","bbdc0922":"Q1. Calculate the probability of getting 14 heads in 20 attempts from a fair coin.","cdc7dc9d":"Cluster Analysis is an unsupervised technique of grouping objects together based on their properties. By doing so, the objects in one group are more similar to each other than to those in other groups.","182bf851":"#### 6.1 One-Way ANOVA Test","48c7f169":"A bernoulli distribution has only 2 possible outcomes, 1 (success) and 0 (failure). <br>\nEg. A coin toss. <br>","01d46cbb":"**t-score**","b4fdf5a4":"Percentile is calculated by assuming the highest or maximum value in a dataset as the upper limit and relative to that value other values are calculated indicating how far or near they are.","42e3c98f":"case 2: odd number of elements","234bbae7":"### 25. Summary Statistics","cf5fe9ef":"* skewness = 0, normally distributed\n* skewness > 0, more weight in the left tail of the distribution\n* skewness < 0, more weight in the right tail of the distribution","84cd99d6":"While selecting threshold, you should visualize the following graph for threshold selection and\n\n![](https:\/\/miro.medium.com\/max\/644\/1*P2qKi7w1UHF7zg6SnCGTag.png)","189873e3":"KS Test is used to check if given values follow a distribution. <br>\n","5150aed2":"A **two-tailed** test is a statistical **test** in which the critical area of distribution is **two-sided** and tests are adopted as whether the samples aare greater than or less than some critical values. If the sample being tested falls in either of the critical values, the alternate hypothesis is accepted instead of the null hypothesis. ","bcce3c7a":"### 16. Sample Statistics","cdefd558":"### 18. Maximum Likehood Estimation","304ca424":"**Paired sampled t-test** -> <br>\nAlso known as dependent sample t-test. <br>\nUnivariate test that tests for a significant difference between 2 related variables. ","78360d61":"Cluster Analysis assigns objects to various groups without any prior object labels whereas Discriminant Analysis uses such knowledge which was defined in advance.","b82ec969":"e. Variance","0c2a8133":"![](https:\/\/www.geeksforgeeks.org\/wp-content\/ql-cache\/quicklatex.com-7b0fdc0b3c4d7ef2aeeba85f690456c2_l3.svg)","e12a451c":"### 23. Confidence Interval","8dcc7bc4":"Regression is a problem where you need to find a function that maps some features or variables to others sufficiently well.<br>\nDependent features are called the dependent variables. <br>\nIndependent features are called the independent variables. <br>\n\nRegression is useful to forecast a response using a set of predictors. <br>\nWhen implementing linear regression of some dependent variable y on set of independent variables x, \ud835\udc66 = \ud835\udefd\u2080 + \ud835\udefd\u2081\ud835\udc65\u2081 + \u22ef + \ud835\udefd\u1d63\ud835\udc65\u1d63 + \ud835\udf00. is called the regression equation. \ud835\udf00 is the random error.","1835cb48":"**Calculation of confidence interval** <br>\n","1fd83591":"Output of a regression analysis is usually a summary statistic that includes: <br>\n* R \n* R squared\n* adjusted R-squared\n* standard error of the estimate","eeee7be3":"Understanding various distance functions -> <br>\nBoth euclidean and manhattan distance satisfy the following -> \n* d(i,j) >= 0\n* d(i,j) = d(j,i)\n* d(i,j) >= d(i,h) + d(h,j)\n* d(i,i) != 0","f1f15d7b":"### 4. T-Test","7d130d53":"Type II Error","81dd78e3":"method 1","3855a960":"Q4. A car distributor experiences on an average 3 car sales per day. Find the probability that on a randomly selected day they will sell \n1. 5 cars.\n2. 0 Cars\n3. At most 2 cars\n4. exactly 1 car","3aef2c9c":"a. Skewness","18e7d3d3":"In such a scenario, never remember the standard TPR vs FPR diagram for ROC Curve. Rather Refer to the following diagram -> <br>\n![](https:\/\/lukeoakdenrayner.files.wordpress.com\/2018\/01\/threshold2.png?w=656)\n\n**Case I : Shifting the threshold to the right**<br>\nThis would case TP to decrease, FP to decrease, FN to increase and TN to increase. <br>\nBy doing so, TPR or Sensitivity (which is TP \/ Sum(+ves)) decreases. FPR (which is (FP \/ Sum(-ves) ) decreases. If FPR decreases, specificity increases as FPR = 1 - specificity.","9a2034f6":"### plot the decision tree as a graph ","657ab341":"We can use silhouette scores to find the optimal number of clusters.","57ccb27a":"Inference -> A value close to 0 means there is no correlation between the values of the elements. We can say there is slight but insignificant correlation between the values","0eb91bcf":"One tailed Test","774bc192":"### 12. Central Limit Theorem","518380e1":"#### Linear Regression using statsmodels","6ee29e34":"## Summary => <br>\nThis notebook includes the following topics. <br><br>\n\nThe notebook will be constructed in two stages. <br>\n* 1st Stage -> Complete python implementations along with brief descriptions. (Est. Date of Completion - 28-03-2021)\n* 2nd Stage -> Solving questions on these topics using python. (Est. Date of Completion - 31-04-2021)\n* Continuous Development and Improvisations....","2e54af8d":"![](https:\/\/media.geeksforgeeks.org\/wp-content\/uploads\/20200503190751\/Annotation-2020-05-03-190733-300x92.png)","3ac907f3":"Let's deal the nominal variables.","00d10722":"![](https:\/\/miro.medium.com\/proxy\/0*-oGC3SE8sPCPdmxs.jpg)","e0917002":"**Steps to perform MLE:** <br>\n* Perform a certain experiment to collect data\n* Choose parametric model of the data\n* Formulate the likelihood as an objective function to be maximized \n* Maximize the objective func and derive the parameters of the model\n\n\n**Examples** -> <br>\n* Coin Toss to find the probabilities of heads and tails\n* Dart throwing","210bb0b6":"![](https:\/\/media.geeksforgeeks.org\/wp-content\/uploads\/skewness.jpg)","3e1360d2":"### 17. Population Statistics","4586654b":"p-value is all about answering the question with certain confidence level. <br>\neg. I am 90% confident that I will get that job. ","990b505d":"Median for **odd** number of elements is the middle most element. <br>\nMedian for **even** number of elements is the average of the middle two elements. <br>","2408d21a":"Perform One Hot Encoding of the ordinal features","e95c42bc":"Two-Way F-Test (Anova) -> ","6c433d8c":"## Solving Questions on Distributions using python","ae4535e5":"**Confusion Matrix** <br><br>\n\nWe'll learn confusion matrix for both Binary Classifiers as well as Multi Class Classifiers. ","d5803411":"**Brief Overview** -> <br>\n![](https:\/\/miro.medium.com\/max\/421\/1*ayxQCn3xz6sm41KRjf3Ygw.gif)\n\n![](https:\/\/miro.medium.com\/max\/721\/1*6MTXtB4zipiDMguZrlXSlA.gif)\n\nIn statistics, MLE is widely used to obtain the parameter for a distribution. In this paradigm, ","ac48ceef":"##### Applications -> <br>\n* helps in easy identification of outlier values\n* gives the central tendency of the data.\n* higher the IQR, higher the variablity.\n* lower the IQR, the preferable the dataset is.","d6d600d2":"Applications of G-Statistic -> <br>\n* Helps to verify the overall significance of the model\n* ","d95fa4db":"Chi-Sq Test","f1635062":"#### Relation between SST, SSR, SSE","fd2f5ca2":"Q15. Acme Corporation manufactures light bulbs. The CEO claims that an average Acme light bulb lasts 300 days. A researcher randomly selects 15 bulbs for testing. The bulbs last an average of 290 days, with a standard deviation of 50 days. If the CEO's claim were true, what is the probability that 15 randomly selected bulbs would have an average life of no more than 290 days?","2b843511":"* If you aim for very low FPR, you might pick up a ","220b94a8":"### 15. Measuring Variance","68447f9b":"Q5. The weight of football players is normally distributed with mean of 200 pounds and a standard deviation of 25 pounds. Find the probability of a player weighing\n1. more than 241.25 pounds\n2. less than 250 pounds.","2d0e3b07":"### 28. Logistic Regression","be76f4d9":"MLE is a method to find the most likely density function that would have generated the data. <br>\nThe likelihood function depends on mean 'mu' and variance \u03c32 which is found through an iterative process using calculators or computers.<br>\n","5a25ed51":"Note -> Normal Distribution is a limiting case of poisson distribution when lambda -> inf. ","f12eee2a":"Q2. Following are the summary statistics of life spans of two breeds of cats. <br>\n\n| Breed  |Mean Life Span (yrs)   | Standard Deviation  |  \n|---|---|---|\n| Abyssinian  | 12  | 1.5  | \n| Colorpoint shorthair  |  14 | 1  |   \n\nFluffy was an Abyssinian cat who lived for 13 years, and Mittens was a colorpoint shorthair cat who lived for 15 years.<br>\nRelative to the breed, which cat had a longer life span? <br><br>\n\n* Fluffy \n* Mittens\n* Fluffy and Mittens had equally long lifespans relative to their breeds.\n* It's impossible to say without seeing all of the individual lifespans.\n* It's impossible to say since we don't know the shape of either distribution.","69ff1f53":"Z-Test <br>\nUse Z-test if -> <br>\n* Sample Size > 30\n* Data points **independent** from each other. \n* Data should be normally distributed. Sometimes, if the sample size is large enough, this doesn't matter. \n* Data items have equal chance of getting selected. ","8267e013":"##### Post-Hoc Analysis (Tukey's Test)","f20fc7be":"The outputs of a logistic regression are class probabilities.","e02efdfc":"### 27. Simmilarity and Dissimilarity Index","bc4de74e":"In this paradigm, to maximize log likelihood, we need to minimize the cost function. <br>\n![](https:\/\/miro.medium.com\/max\/774\/1*VAb-6NSg2vwUtqCtfNdjrA.gif)\n\nGradient Descent algorithm is used to tweak the values of the cost function using MLE. ","abdb924a":"Sum of differences between the predicted value and the mean of the dependent variable.","e7bd7a2b":"So-called dummy variables are used to represent qualitative variables in regression analysis. In general, k - 1 dummy variables are needed to model the effect of a qualitative variable that may assume k values.","d6c5fabf":"##### solving chi2 test generic questions and numericals using python ","27daa7da":"bin_3, bin_4 has T\/F values. <br>\nnom_0, nom_1, nom_2, nom_3, nom_4 have 3-6 unique values. <br>\nnom_5, nom_6, nom_7, nom_8, nom_9 have many unique values <br>\nTHen comes the ordinal variables","72557dce":"**Area Under the ROC Curve** -> <br>\nAUC provides aggregate measure of performance across all possible classification thresholds. ","3b1f2092":"![](https:\/\/cdn.askpython.com\/wp-content\/uploads\/2020\/10\/Standard-deviation-around-mean.jpg.webp)","d3fe71f3":"Brief Description -> ","e13a1931":"#### II. Dispersion","1695f5c6":"**Key Terms ->** <br><br><br>\n* Coefficient of Determination -> R^2, tells you the amount of variation in y based on the dependence on x. Larger R^2 indicates a better fit and means that the model can \n* Coefficient of Correlation -> \n* SSE ->\n* SSR -> Sum of Squared Residuals\n* SST -> \n* Error Term \u03b5 -> \n* Regression Equation -> \n* Correlation Equation -> \n* Estimated Regression Equation -> \n* Regression Model ->\n* F Statistic -> \n* MSE\n* MSR\n* Dummy Variable -> A variable that takes values of 0 or 1 and is used to consider the effect of qualitative variables in a regression model\n* Correlation vs Causation -> \n* Standard Error ->\n* Confidence Interval Estimate -> The interval estimate of the mean value of y for a given value of x.\n* OLS -> Method of Ordinary Least Squares. To get the best model weights, we use the method of OLS to minimize the SSR. ","2029947e":"**Distance Functions**","b2fa1278":"II. Confidence Interval with a small sample","8ce59d8b":"2 Samples t-test -> <br>\n        Compares the means of two independent groups in order to determine whether there is statistical evidence that the associated population means are statistically different. <br>\n        Also known as **Independent t-test**","65246dd8":"c. Range","7584ce86":"p-value obtained from ANOVA Analysis is not significant (p > 0.05), and therefore, we conclude that there are no significant differences amongst the groups. ","a2feb421":"The **p-Value**, or calculated probability, of finding extreme results when the null hypothesis is true. <br>\nIf your p value is less than the chosen significance level, you reject the null hypothesis. ","e392b583":"III. Confidence Interval with the Normal Distribution \/ Z-Distribution","53338169":"IV. Confidence Interval for a proportion","72300cd7":"### 5. Z-Test","9f5ea40d":"The following is an experiment of dice roll for 1000 times. <br>\nfor 1000 times, we make samples of samples size 100 where possible outcomes are 1,2,3,4,5,6 <br><br>\nBy plotting the histogram of the sample means, we obtain a normally distributed plot. <br>\nThis is Central Limit Theorem","6b6725a2":"Sum of Squares Total - the squared difference between the observed dependent variable and its mean.<br>\nSST is also denoted as TSS or total sum of squares.","c6ef932d":"![](https:\/\/res.cloudinary.com\/dyd911kmh\/image\/upload\/f_auto,q_auto:best\/v1539784818\/output_39_0_knqrjh.png)","82c3ddb9":"Standard deviation is the square root of the variance value calculated. <br><br><br>\nValues that are within one standard deviation of the mean can be thought of as fairly typical. <br>\nThose values which are three or more standard deviations away from the mean can be considered as **outliers**. ","fd975bde":"### 26. Familiarizing with different error metrics","f4663f3e":"case 2. 2D dataset","a869e7ec":"Q14. Suppose scores on an IQ test are normally distributed, with a mean of 100. Suppose 20 people are randomly selected and tested. The standard deviation in the sample group is 15. What is the probability that the average test score in the sample group will be at most 110?","0ff96877":"#### I. Central Tendency Statistics","d33bff25":"#### II. Calculating G-Statistic","ac5b5c24":"### 20. Hypothesis Testing","d27338e8":"![](https:\/\/media.geeksforgeeks.org\/wp-content\/uploads\/Capture-214.png)","820d800d":"Regression analysis involves identifying the relationship between a dependent variable and one or more independent variables. <br>\nA model of the relationship is hypothesized, and estimates of the parameter values are used to develop an estimated regression equation. <br>\n<br><br>\n**Regression Model** <br>\nIn simple linear regression, the model used to describe the relationship between a single dependent variable y and a single independent variable x is y = \u03b20 + \u03b21x + \u03b5. <br>\n\u03b20 and \u03b21 are referred to as the model parameters,and \u03b5 is a probabilistic error term that accounts for the variability in y that cannot be explained by the linear relationship with x <br><br>\nThe difference between the observed value of y and the value of y predicted by the estimated regression equation is called a residual.<br>","834da44f":"Silhouette Analysis Method","ee34ef36":"#### 19. iii. Agglomerative Clustering","2562d467":"c. Median","bfe071ad":"A statistical test in which the region of **rejection** is only on **one** side of the sampling distribution. ","8c8ff803":"**Poor Classifier** <br><br>\n![](https:\/\/miro.medium.com\/max\/764\/1*HVvNkWufhzGj2s0sc4CyIw.png)","b39457df":"case 1 : even number of elements","3e491f01":"**T-Values vs P-Values**","91836189":"Q10. An average light bulb manufactured by the Acme Corporation lasts 300 days with a standard deviation of 50 days. Assuming that bulb life is normally distributed, what is the probability that an Acme light bulb will last at most 365 days?","4d7f8067":"d. Percentile","52184098":"**Q. How would multiplying all of the predictions from a given model by 2.0 (for example, if the model predicts 0.4, we multiply by 2.0 to get a prediction of 0.8) change the model's performance as measured by AUC?** <br><br>\nAns. No change. AUC only cares about relative prediction scores.","dac1fa97":"##### Key Takeaways :- <br><br>\n\n![](https:\/\/miro.medium.com\/max\/366\/1*RdIQG331j0tayi50asTOIw.png)\n\n![](https:\/\/miro.medium.com\/max\/418\/1*dCxzo7E6lmKxHLEg2xZSoQ.png)","4dca4a3b":"![](https:\/\/files.realpython.com\/media\/kmeans-algorithm.a94498a7ecd2.png)","1173f09f":"Let's encode the remaining of the nominal values","40319b4c":"t-test also known as Student's t-test compares the two averages (means) and tells you if they are different from each other. <br>\nCan also tell you how significant the differences are. "}}