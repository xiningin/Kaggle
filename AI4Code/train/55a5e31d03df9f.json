{"cell_type":{"5d51057e":"code","1e8eb293":"code","e42489a4":"code","cce68b79":"code","4f4bedf5":"code","30384e04":"code","39fc603c":"code","3dc230c9":"code","b79d077b":"code","bd64de45":"code","8a667501":"code","f50c3a22":"code","cc31a03c":"code","10bacc11":"code","60aa002e":"code","84310077":"code","7bbfddb2":"code","9ae0e56d":"code","79d0f7fb":"code","57783f21":"code","b4a3045a":"code","93e8d59e":"code","3689d991":"code","54a8a857":"code","8d355400":"code","cf15ac40":"code","b17b607b":"code","ec4d0f2d":"code","9e401bf5":"code","09a43d7f":"code","fbb82a9f":"code","d5f2a591":"code","d318342f":"code","73d86395":"code","b02a3738":"code","510fee1f":"code","c1732eff":"code","8f7330bd":"code","1552c947":"code","cfd2b22e":"code","2f1777cd":"code","54835d25":"code","37c8feb7":"code","e3408764":"code","ef85c1b3":"code","6f183176":"code","4c674859":"code","971eb2b9":"code","7b50ccce":"code","4bf255ac":"code","af241185":"code","55ec3a56":"code","346397d3":"code","45531341":"code","fe43413d":"code","95e839ff":"code","725d9563":"code","4392d9ed":"code","7ff9d426":"markdown","2b61968d":"markdown","e4130919":"markdown","2f5bc694":"markdown","a9977ab3":"markdown","52fadb50":"markdown","6cb02e91":"markdown","26b64167":"markdown","e4ec7020":"markdown","67305727":"markdown","91c75edf":"markdown","42e5b1bc":"markdown","d13303fd":"markdown","8c887ad8":"markdown","d54a0a69":"markdown","20bb6930":"markdown","82b7348c":"markdown","6425e7ea":"markdown","0fd21dc8":"markdown","c8430108":"markdown","5e831c3d":"markdown","7d2f0214":"markdown","a0976aa3":"markdown","e5d84499":"markdown","c0b96193":"markdown","5c926bae":"markdown","9880774c":"markdown","0a1ce273":"markdown","7d4f6455":"markdown","52520edc":"markdown","548d0f76":"markdown","de0a186c":"markdown","d1d748d5":"markdown","1d315502":"markdown","8364b863":"markdown","bfe7d939":"markdown","475daeb4":"markdown","0de97365":"markdown","c7e1350d":"markdown","41738392":"markdown","d19a3a0d":"markdown","bba6b94e":"markdown","2f5e3abc":"markdown","0b32b331":"markdown","03aa940a":"markdown","15ef56eb":"markdown","ee48b46e":"markdown"},"source":{"5d51057e":"# Loading the require libraries we will use along the notebook\n\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \nseed_value= 42\nos.environ['PYTHONHASHSEED']=str(seed_value)\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport random\nimport datetime\nimport tensorflow_hub as hub\n\n# Setting randomness\nrandom.seed(seed_value)\nnp.random.seed(seed_value)\ntf.random.set_seed(seed_value)","1e8eb293":"def walk_through_dir(dir_path):\n    \"\"\"\n    Walks through dir_path returning its contents.\n    Args:\n    dir_path (str): target directory\n\n    Returns:\n    A print out of:\n      number of subdiretories in dir_path\n      number of images (files) in each subdirectory\n      name of each subdirectory\n    \"\"\"\n    total_images = 0\n    for dirpath, dirnames, filenames in os.walk(dir_path):\n        print(f\"There are {len(dirnames)} directories and {len(filenames)} files in '{dirpath}'.\")\n        for file in filenames:\n            if file.endswith('.jpg'):\n                total_images += 1\n    print(f'There are a total of {total_images} images')\n            ","e42489a4":"PATH = '\/kaggle\/input\/lego-minifigures-classification\/'\nwalk_through_dir(PATH)","cce68b79":"# Loading all the CSV files\ntrain_data = pd.read_csv(PATH + 'index.csv')\nmetadata = pd.read_csv(PATH + 'metadata.csv')\ntest_data = pd.read_csv(PATH + 'test.csv')","4f4bedf5":"print(train_data.shape)\ntrain_data.sample(5)","30384e04":"print(test_data.shape)\ntest_data.sample(5)","39fc603c":"print(metadata.shape)\nmetadata.sample(5)","3dc230c9":"train_data = train_data.merge(metadata, on = 'class_id')\ntest_data = test_data.merge(metadata, on = 'class_id')","b79d077b":"train_data.head()","bd64de45":"def prepare_and_load_image(path, img_shape):\n    # Read in the image\n    img = tf.io.read_file(path)\n    # Decode the read file into a tensor\n    img = tf.image.decode_image(img)\n    # Resize the image\n    img = tf.image.resize(img, size = [img_shape,img_shape])\n    return img\n\ndef plot_random_images(df, number_of_samples = 9, img_shape = 240, nrows = 3, ncols = 3, figsize = (9,9)):\n    # Get random rows from the df\n    rand_df = df.sample(number_of_samples)\n    # Create a subplot \n    fig, axes = plt.subplots(nrows = nrows ,\n                        ncols = ncols,\n                       figsize=figsize)\n    for i, ax in enumerate(axes.flat):\n        # Obtain the i image\n        sample = rand_df.iloc[i]\n        figure_name = sample['minifigure_name']\n        figure_path = sample['path']\n        # Get the full path of image\n        img_path = os.path.join(PATH, figure_path)\n        img = prepare_and_load_image(img_path, img_shape)\n        # Divide the image by 255. to correctly display colors\n        ax.imshow(img\/255.)\n        # Creating tittle with the figure name\n        ax.set_title(f'Figure: {figure_name}')\n        # Removing the axis\n        ax.axis('off')\n    plt.tight_layout()","8a667501":"plot_random_images(train_data)","f50c3a22":"plot_random_images(test_data)","cc31a03c":"an_image = train_data.loc[2]['path']\n# Get the full path of image\nimg_path = os.path.join(PATH, an_image)\n# Read in the image\nimg = tf.io.read_file(img_path)\n# Decode the read file into a tensor\nimg = tf.image.decode_image(img)\nprint(f'Shape of the image form tf.io_read_file:{img.shape}')\nprint(f'A part of the tensor:{img[0][0]}')\n# Resize the image\nimg = tf.image.resize(img, size = [240,240])\nprint(f'Shape of the after tf.image.resize:{img.shape}')\nprint(f'A part of the tensor:{img[0][0]}')\n# Normalize image\nimg = img\/255.\nprint(f'Shape of the after normalized:{img.shape}')\nprint(f'A part of the tensor:{img[0][0]}')","10bacc11":"IMG_SIZE = 240\n\ndef load_data_in_tf(dataframe, batch_size = 32, img_size = IMG_SIZE, directory_path = PATH, rescale = True):\n    '''\n    Generates the TensorFlow Dataset and preprocess all the images in it\n    \n    Parameters\n    ----------\n    dataframe: pd.DataFrame\n        Dataframe that contains the paths to each images and the target name.\n        \n    batch_size: int\n        Size of batches.\n        \n    img_size: int\n        Size of the resizing for the images.\n        \n    directory_path: str\n        Main path that will be added to the dataframe path information.\n    \n    '''\n\n    # Data augmentation pipeline\n    if rescale:\n        datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.\/255)\n    else:\n        datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n\n    # Reading files from path in data frame\n    tf_dataset = datagen.flow_from_dataframe(dataframe,\n                                            directory = directory_path,\n                                            x_col = 'path',\n                                            y_col = 'minifigure_name',\n                                            target_size = (img_size, img_size),\n                                            batch_size = batch_size,)\n    return tf_dataset\n\n\n\ntrain_ds = load_data_in_tf(train_data)\ntest_ds = load_data_in_tf(test_data)","60aa002e":"# Set a random  seed for reproducibility\ntf.random.set_seed(42)\n\n# Create Model\nmodel_mlp = tf.keras.Sequential([\n        tf.keras.layers.Input(shape = (train_ds.image_shape), name = 'input'),\n        tf.keras.layers.Flatten(name= 'flatten'),\n        tf.keras.layers.Dense(100, activation='relu', name= 'first_hidd_layer'),\n        tf.keras.layers.Dense(64, activation='relu', name= 'second_hidd_layer'),\n        tf.keras.layers.Dense(36, activation='softmax', name='output_layer')\n], name = 'MLP')\n\n# Compile the model\nmodel_mlp.compile(loss = tf.keras.losses.CategoricalCrossentropy(),\n            optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3),\n             metrics = ['accuracy'])","84310077":"tf.keras.utils.plot_model(model_mlp)","7bbfddb2":"model_mlp.summary()","9ae0e56d":"# Creating toy tensor\ntoy_tensor = tf.cast(tf.range(-10, 10), tf.float32)\nprint(toy_tensor)\n# Visualize our toy tensor\nplt.plot(toy_tensor);","79d0f7fb":"# Using sigmoid \ntoy_sigmoid = tf.nn.sigmoid(toy_tensor)\n# Using relu\ntoy_relu = tf.nn.relu(toy_tensor)\n# Using tan-h\ntoy_tanh = tf.nn.tanh(toy_tensor)\n# Using softmax\ntoy_softmax = tf.nn.softmax(toy_tensor)","57783f21":"plt.figure(figsize=(20, 4))\nplt.subplot(1, 5, 1)\nplt.title('Original')\nplt.plot(toy_tensor)\nplt.subplot(1, 5, 2)\nplt.title('Sigmoid')\nplt.plot(toy_sigmoid)\nplt.subplot(1, 5, 3)\nplt.title('ReLU')\nplt.plot(toy_relu)\nplt.subplot(1, 5, 4)\nplt.title('Tan-H')\nplt.plot(toy_tanh)\nplt.subplot(1, 5, 5)\nplt.title('SoftMax')\nplt.plot(toy_softmax);","b4a3045a":"print(f'Softmax tensor:{toy_softmax}')\nprint(f'Sum softmax tensor: {sum(toy_softmax):2f}')","93e8d59e":"# Create Model Callbacks\n\ndef create_tensorboard_callback(dir_name, experiment_name):\n    \"\"\"\n    Creates a TensorBoard callback instand to store log files.\n    Stores log files with the filepath:\n    \"dir_name\/experiment_name\/current_datetime\/\"\n    Args:\n    dir_name: target directory to store TensorBoard log files\n    experiment_name: name of experiment directory (e.g. efficientnet_model_1)\n    \"\"\"\n    log_dir = dir_name + \"\/\" + experiment_name + \"\/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n    print(f\"Saving TensorBoard log files to: {log_dir}\")\n    return tensorboard_callback\n\ncheckpoint_path = \"model_checkpoints\/mlp.ckpt\"\n\nmodel_checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n                                                      monitor=\"val_acc\", # save the model weights with best validation accuracy\n                                                      save_best_only=True, # only save the best weights\n                                                      save_weights_only=True, # only save model weights (not whole model)\n                                                      verbose=0)","3689d991":"# Set a random  seed for reproducibility\ntf.random.set_seed(42)\n\n# Training\nhistory_model_mlp = model_mlp.fit(train_ds,\n                                 epochs = 12,\n                                 steps_per_epoch = len(train_ds),\n                                 validation_data = test_ds,\n                                 validation_steps= len(test_ds),\n                                 callbacks=[create_tensorboard_callback('training_logs',\n                                                                       'lego_mlp'),\n                                           model_checkpoint])","54a8a857":"# Evaluate model on whole test dataset\nresults_model_mlp = model_mlp.evaluate(test_ds)\nresults_model_mlp","8d355400":"# Recovered from: https:\/\/github.com\/mrdbourke\/tensorflow-deep-learning\/blob\/main\/extras\/helper_functions.py\n\ndef plot_loss_curves(history):\n    \"\"\"\n    Returns separate loss curves for training and validation metrics.\n    Args:\n    history: TensorFlow model History object (see: https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/callbacks\/History)\n    \"\"\" \n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    accuracy = history.history['accuracy']\n    val_accuracy = history.history['val_accuracy']\n\n    epochs = range(len(history.history['loss']))\n\n    # Plot loss\n    plt.plot(epochs, loss, label='training_loss')\n    plt.plot(epochs, val_loss, label='val_loss')\n    plt.title('Loss')\n    plt.xlabel('Epochs')\n    plt.legend()\n\n    # Plot accuracy\n    plt.figure()\n    plt.plot(epochs, accuracy, label='training_accuracy')\n    plt.plot(epochs, val_accuracy, label='val_accuracy')\n    plt.title('Accuracy')\n    plt.xlabel('Epochs')\n    plt.legend();","cf15ac40":"plot_loss_curves(history_model_mlp)","b17b607b":"# Set a random  seed for reproducibility\ntf.random.set_seed(42)\n\n# Create a model use Functional API\ninputs = tf.keras.layers.Input(shape = (train_ds.image_shape), name = 'input')\nx = tf.keras.layers.Conv2D(filters = 10, activation = 'relu', kernel_size = 3, name = 'conv_1_1')(inputs)\nx = tf.keras.layers.Conv2D(filters = 10, activation = 'relu', kernel_size = 3, name = 'conv_1_2')(x)\nx = tf.keras.layers.MaxPool2D(pool_size = 2, padding = 'valid', name = 'max_pool_1')(x)\nx = tf.keras.layers.Conv2D(filters = 10, activation = 'relu', kernel_size = 3, name = 'conv_2_1')(x)\nx = tf.keras.layers.Conv2D(filters = 10, activation = 'relu', kernel_size = 3, name = 'conv_2_2')(x)\nx = tf.keras.layers.MaxPool2D(pool_size = 2, padding = 'valid', name = 'max_pool_2')(x)\nx = tf.keras.layers.Flatten(name='flatten_layer')(x)\noutputs = tf.keras.layers.Dense(36, activation='softmax', name='output_layer')(x)\n\nmodel_tiny_vgg = tf.keras.Model(inputs, outputs)\n\n# Compile the model\nmodel_tiny_vgg.compile(loss = tf.keras.losses.CategoricalCrossentropy(),\n                        optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3),\n                         metrics = ['accuracy'])","ec4d0f2d":"tf.keras.utils.plot_model(model_tiny_vgg)","9e401bf5":"model_tiny_vgg.summary()","09a43d7f":"# Set a random  seed for reproducibility\ntf.random.set_seed(42)\n\n# Training\nhistory_tiny_vgg  = model_tiny_vgg.fit(train_ds,\n                                       epochs = 12,\n                                       steps_per_epoch=len(train_ds),\n                                       validation_data = test_ds,\n                                       validation_steps = len(test_ds),\n                                       callbacks=[create_tensorboard_callback('training_logs',\n                                                                            'lego_tiny_vgg'),\n                                                  model_checkpoint])","fbb82a9f":"# Evaluate model on whole test dataset\nresults_tiny_vgg = model_tiny_vgg.evaluate(test_ds)\nresults_tiny_vgg","d5f2a591":"plot_loss_curves(history_tiny_vgg)","d318342f":"# URL from model in Tf.hub\neffnet_url = \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet1k_b1\/classification\/2\"\n\n# Setting random seed\ntf.random.set_seed(42)\n\n# Getting the model trainable should be false\nfeature_extractor_layer = hub.KerasLayer(effnet_url,\n                                       trainable=False, # freeze the underlying patterns\n                                       name='feature_extraction_layer',\n                                       input_shape=(IMG_SIZE, IMG_SIZE, 3)) # define the input image shape\n\n# Creating model with our the dense layer and our number of classes\nefficnet_model = tf.keras.Sequential([\n        feature_extractor_layer, # use the feature extraction layer as the base\n        tf.keras.layers.Dense(36, activation='softmax', name='output_layer') # create our own output layer\n], name = 'effnet_model')\n\n\n# Compile the model\nefficnet_model.compile(loss='categorical_crossentropy',\n                     optimizer=tf.keras.optimizers.Adam(),\n                     metrics=['accuracy'])","73d86395":"efficnet_model.summary()","b02a3738":"# Setting random seed\ntf.random.set_seed(42)\n\nhistory_effnet_tfhub = efficnet_model.fit(train_ds,\n                                          epochs = 5,\n                                          steps_per_epoch=len(train_ds),\n                                          validation_data = test_ds,\n                                          validation_steps = len(test_ds),\n                                          callbacks=[create_tensorboard_callback('training_logs',\n                                                                                'lego_tfhub_effnet'),\n                                                      model_checkpoint])","510fee1f":"# Evaluate model on whole test dataset\nresults_efficnet_model = efficnet_model.evaluate(test_ds)\nresults_efficnet_model","c1732eff":"plot_loss_curves(history_effnet_tfhub)","8f7330bd":"# Create base model\ninput_shape = (240, 240, 3)\nbase_model = tf.keras.applications.EfficientNetB2(include_top=False)\nbase_model.trainable = False # freeze base model layers","1552c947":"for layer in base_model.layers[:5]:\n    print(layer.name, layer.trainable, layer.dtype,)\n\nfor layer in base_model.layers[-5:]:\n    print(layer.name, layer.trainable, layer.dtype,)","cfd2b22e":"train_ds = load_data_in_tf(train_data, rescale=False)\ntest_ds = load_data_in_tf(test_data, rescale=False)","2f1777cd":"# Setting random seed\ntf.random.set_seed(42)\n\n# Create Functional model \ninputs = tf.keras.layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3), name=\"input_layer\")\nx = base_model(inputs, training=False) # set base_model to inference mode only\nx = tf.keras.layers.GlobalAveragePooling2D(name=\"pooling_layer\")(x)\noutputs = tf.keras.layers.Dense(36, activation='softmax', name='output_layer')(x) # want one output neuron per class \nmodel_effnet_keras = tf.keras.Model(inputs, outputs)\n\n# Compile the model\nmodel_effnet_keras.compile(loss=\"categorical_crossentropy\",\n                          optimizer=tf.keras.optimizers.Adam(),\n                          metrics=[\"accuracy\"])","54835d25":"# Create a random tensor\ninput_tensor = tf.random.normal((1, 4, 4, 3)) # Defining same shape as our current images\nprint(f\"Random input tensor:\\n {input_tensor}\\n\")\n\n# Pass the random tensor through a global average pooling 2D layer\nglobal_average_pooled_tensor = tf.keras.layers.GlobalAveragePooling2D()(input_tensor)\nprint(f\"2D global average pooled random tensor:\\n {global_average_pooled_tensor}\\n\")\n\n# Check the shapes of the different tensors\nprint(f\"Shape of input tensor: {input_tensor.shape}\")\nprint(f\"Shape of 2D global averaged pooled input tensor: {global_average_pooled_tensor.shape} \\n\")\n\n# Flatten a tensor\nflatten_tensor = tf.keras.layers.Flatten()(input_tensor)\nprint(f\"Flatten random tensor:\\n {flatten_tensor}\\n\")\nprint(f\"Shape of flatten tensor: {flatten_tensor.shape}\")","37c8feb7":"model_effnet_keras.summary()","e3408764":"# Training\nhistory_effnet_kerasapp = model_effnet_keras.fit(train_ds,\n                                                  epochs = 5,\n                                                  steps_per_epoch=len(train_ds),\n                                                  validation_data = test_ds,\n                                                  validation_steps = len(test_ds),\n                                                  callbacks=[create_tensorboard_callback('training_logs',\n                                                                                        'lego_kerasapp_effnet'),\n                                                              model_checkpoint])","ef85c1b3":"# Evaluate model on whole test dataset\nresults_efficnet_keras_model = model_effnet_keras.evaluate(test_ds)\nresults_efficnet_keras_model","6f183176":"plot_loss_curves(history_effnet_tfhub)","4c674859":"class_names = {v: k for k, v in train_ds.class_indices.items()}\ncustom_lego_images = [\"\/kaggle\/input\/lego-test-images\/\" + img_path for img_path in os.listdir(\"\/kaggle\/input\/lego-test-images\")]","971eb2b9":"# Create a subplot \nfig, axes = plt.subplots(nrows = 2 ,\n                         ncols = 3,\n                         figsize=(10,12))\nfor i, ax in enumerate(axes.flat):\n    # load the image\n    img = prepare_and_load_image(custom_lego_images[i], img_shape=240) \n    # Get name of figure\n    figure_name = custom_lego_images[i][31:-4] \n    # make prediction on image with shape [None, 224, 224, 3]\n    pred_prob = model_effnet_keras.predict(tf.expand_dims(img, axis=0)) \n    # find the predicted class label\n    pred_class = class_names[pred_prob.argmax()] \n    # Divide the image by 255. to correctly display colors\n    ax.imshow(img\/255.)\n    # Creating tittle with the figure name\n    ax.set_title(f'Figure load: {figure_name} \\n Prediction:{pred_class}')\n    # Removing the axis\n    ax.axis('off')\n    plt.tight_layout()","7b50ccce":"train_ds = load_data_in_tf(train_data, rescale=False)\ntest_ds = load_data_in_tf(test_data, rescale=False)","4bf255ac":"images, labels = next(train_ds)\nimages.shape, labels.shape","af241185":"AUTOTUNE = tf.data.AUTOTUNE\ndatagen = tf.keras.preprocessing.image.ImageDataGenerator()\nBATCH_SIZE = 32\nIMG_SIZE = 240\n\ntrain_ds = tf.data.Dataset.from_generator(\n    lambda: datagen.flow_from_dataframe(train_data,\n                                        directory=PATH,\n                                        x_col = 'path',\n                                        y_col = 'minifigure_name',\n                                        target_size = (IMG_SIZE, IMG_SIZE),\n                                        batch_size = BATCH_SIZE,),\n    output_shapes=([None,IMG_SIZE,IMG_SIZE,3], [None,36]), # Not required if specify in flow\n    output_types=(tf.float32, tf.float32),\n    \n)\n\n","55ec3a56":"for images, label in train_ds.take(1):\n    print('images.shape: ', images.shape)\n    print('labels.shape: ', labels.shape)","346397d3":"test_ds = tf.data.Dataset.from_generator(\n    lambda: datagen.flow_from_dataframe(test_data,\n                                        directory=PATH,\n                                        x_col = 'path',\n                                        y_col = 'minifigure_name',\n                                        target_size = (IMG_SIZE, IMG_SIZE),\n                                        batch_size = BATCH_SIZE,),\n    output_shapes=([None,IMG_SIZE,IMG_SIZE,3], [None,36]),\n    output_types=(tf.float32, tf.float32),\n    \n)","45531341":"buffer_size = train_data.shape[0]\ntrain_ds = train_ds.shuffle(buffer_size).prefetch(buffer_size=AUTOTUNE)\ntest_ds = test_ds.prefetch(buffer_size=AUTOTUNE)","fe43413d":"# Compile the model -- Required for everytime you want to try something new\nmodel_tiny_vgg.compile(loss = tf.keras.losses.CategoricalCrossentropy(),\n                        optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3),\n                         metrics = ['accuracy'])","95e839ff":"# Training -- Same as before\nhistory_effnet_kerasapp = model_effnet_keras.fit(train_ds,\n                                                  epochs = 5,\n                                                  steps_per_epoch=1,\n                                                  validation_data = test_ds,\n                                                  validation_steps = 1,\n                                                  callbacks=[create_tensorboard_callback('training_logs',\n                                                                                        'lego_kerasapp_effnet'),\n                                                              model_checkpoint])","725d9563":"plot_loss_curves(history_effnet_tfhub)","4392d9ed":"# Create a subplot \nfig, axes = plt.subplots(nrows = 2 ,\n                         ncols = 3,\n                         figsize=(10,12))\nfor i, ax in enumerate(axes.flat):\n    # load the image\n    img = prepare_and_load_image(custom_lego_images[i], img_shape=240) \n    # Get name of figure\n    figure_name = custom_lego_images[i][31:-4] \n    # make prediction on image with shape [None, 224, 224, 3]\n    pred_prob = model_effnet_keras.predict(tf.expand_dims(img, axis=0)) \n    # find the predicted class label\n    pred_class = class_names[pred_prob.argmax()] \n    # Divide the image by 255. to correctly display colors\n    ax.imshow(img\/255.)\n    # Creating tittle with the figure name\n    ax.set_title(f'Figure load: {figure_name} \\n Prediction:{pred_class}')\n    # Removing the axis\n    ax.axis('off')\n    plt.tight_layout()","7ff9d426":"## <a name='tfhubcreate'> Creating models using TensorFlow Hub <\/a>\n\nUsing the TFHub webpage filtering by Image Classification - Fine Tunable models and TF2.0 we found EfficientNet V2 with input size 240x240 which perfectly suits our data (\ud83d\ude09 this was intended). You can find more information about this model [here](https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet1k_b1\/classification\/2)","2b61968d":"## <a name='tfhubresults'> Evaluating TF-Hub Model performance <\/a>","e4130919":"## ** New update: Optimizing the our dataset ** \nRecently I discover reading our documentation that we can optimize the performance on training using TF.AUTOTUNE (https:\/\/www.tensorflow.org\/guide\/data_performance), however we need to change the iterator to be a `tf.data.Dataset` instead `image.DataFrameIterator` that is the case with our current `train_ds` and `test_ds`.\n\n**Intially we did this:**\n\n","2f5bc694":"We can also generate a summary of our model and see the shapes with the summary function, as the following:","a9977ab3":"# <a name=\"cnn\">CNN - Convolutional Neural Network<\/a>\n\nThe convolutional neural network excels working with images. Why? Because it recognizes patterns in the data taking into consideration not the Flatten pixels in which finding patterns isn't that easy but rather take into consideration the pixels around using the convolution layers. \n\nWhat is a convolution?\n*\"The convolutional neuron performs an elementwise dot product with a unique kernel and the output of the previous layer\u2019s corresponding neuron.\"* This may sound complicated but imagine in other words it's to find the **filters** (which are the equivalent of the weights we discussed before) that minimize the losses. \n\nTo understand the process we need to define the following:\n1. Kernel size: (or filter size) refers to the dimensions of the sliding window over the input.\n2. Stride: indicates how many pixels the kernel should be shifted.\n3. Padding: Padding conserves data at the borders, for example adding zeros symmetrically around the edges of an input.\n\nNow how the process work is to start at the top left corner and place the extract the pixels from the size of our kernel size, we multiply this kernel by the filter (element-wise multiplication) and add all the outputs of this multiplication, after this is done with the stride that we define we move our kernel for example 1 step to the right and do the same process. You can visualize it as the following:\n\n<img src=\"https:\/\/www.researchgate.net\/profile\/Baptiste-Wicht\/publication\/322505397\/figure\/fig5\/AS:583063998308353@1516024698839\/A-valid-convolution-of-a-5x5-image-with-a-3x3-kernel-The-kernel-will-be-applied-to.png\" width=500px>\n\nYou can see the Full MIT Deep learning [here](https:\/\/youtu.be\/AjtX1N_VT9E), it covers all these topics precisely and in an unique way and also can read the [CNN Explainer](https:\/\/poloclub.github.io\/cnn-explainer\/) which is a great source.\n\nWe're actually gonna replicate the network architecture from the CNN explainer a Tiny VGG.\n\n**As we have done before lets implement it and discuss the code afterwards.**","52fadb50":"\ud83e\udd2f Wow! As you can see the results are amazing with just 5 epochs we already beat all our previous work that is the power of transfer learning! Now let's visualize the losses curves. ","6cb02e91":"Let's replicate the same for test","26b64167":"## <a name=\"mlpcompile\">Create and compile MLP model<\/a>","e4ec7020":"There seem to still overfit, however it's so much better than previous model. Let's see Keras Application model compare them and fix overfit from there. ","67305727":"## <a name=\"cnneval\">Evaluating CNN model performance<\/a>\n\nWe train sucesfully for the same epochs than the MLP now let's check the results we got.","91c75edf":"As you can see with Average Pooling we reduced the number of parameters we're going to train, and since we're taking the avg for the dimensions model is less likely to memorize and would be inclined to generalize more.","42e5b1bc":"We use the function of `walk_through_dir` to explore all the directories available at the `PATH`. From this we can conclude that we have a total of 483 images, these images are organized as the following:\n* 78 images in the test folder.\n* The remaining images are divided into 4 directories each with a theme (Star Wars, Marvel, Jurrasic World, and Harry Potter) and in each of these directories, we can find folders with numbers that range is between 0001 and 0017. Besides, Marvel and Star Wars appear to be the theme with more images.","d13303fd":"# <a name='transferlearn'> Transfer Learning <\/a>\n\n\ud83e\udd14 **What is transfer learning?**\n\nTransfer learning is why deep learning can solve complex problems without much data, we use the architecture and weights of a typically large model that was pre-trained on a large amount of data, and then we fine-tuned the model to a highly relevant data set.\n\nIn that sense, when a model created for one job is utilized as the basis for a model for a different purpose.\n\n\u2753 **Why this works?**\n\nMost of the time the first layers of a neural network understand basic things, in an image, for example, are vertical lines, horizontal lines, edges, borders, etc. so we can leverage that understanding from one domain and apply it to our specific use case. This allows the network to use the features it previously learned, mix and match them in new combinations.\n\n\ud83e\udd14 **Why use it?**\n1. You can leverage an already proven to work architecture on problems similar to your own (like what we did with the TinyVGG).\n2. You don't have much data, so you can leverage layers with weights already tuned in much more data than what you own.\n3. It's faster, you don't need to start to train from 0.\n\n\u2753 **How we can use transfer learning in TensorFlow?**\nYou can use two options:\n1. [Using TensorflowHub](https:\/\/www.tensorflow.org\/hub), which is a open-source repository from TensorFlow already trained deep learning models. For using it you need to install `tensorflow_hub` and find a model that suits your use case [here](https:\/\/tfhub.dev\/).\n2. [Using Keras Applications](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/applications), which are deep learning models that are made available alongside pre-trained weights. For using it you just need to call any of the models in `tf.keras.applications` from the tensorflow library, you can find the documentation [here](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/applications). \n\n\nYou can read more about in [this resource](https:\/\/machinelearningmastery.com\/transfer-learning-for-deep-learning\/) and check Andrew NG trasnfer learning explanation [here](https:\/\/youtu.be\/yofjFQddwHE)\n\n\n","8c887ad8":"We evaluate for our test set and the accuracy is *0.039*, it may seem low and it's but is a little better than random guessing, the probability of random guessing is 100% divided by 36 classes which is around 0.027. \n\nThe `history_model_mlp` contains the losses and accuracy for the training and validation, what we want to achieve is every epoch to reduce the loss in the training and validation. If for example we just improve the loss within the training but not on the validation it's a simptom that we're **overfitting**, if by the other hand the loss is not improving it's a simptom of **underfitting**. \n\n<img src=\"https:\/\/miro.medium.com\/max\/1125\/1*_7OPgojau8hkiPUiHoGK_w.png\" width=\"600px\" > \n\nWhat can we do when we are **underfitting**? Increase complexity of the model it could be as the following:\n- Adding more layers\n- Increase the number of hidden units\n- Change the activation function\n- Change the optimization function \n- Change the learning rate\n- Fitting on more data \n- Fitting for longer\n\nWhat we can do if we are **overfitting**?\n- Add more data, which gives the model more chance to learn between samples.\n- Data Augmentation, increasing the diversity of training without collecting more data.\n- Better data, removing poor samples for example an image where there isn't a clear LEGO figure. \n- Use transfer learning, take a model pretrained patterns from one problem and adapt it to your use case. \n\n[More in depth about this topic here](https:\/\/medium.com\/greyatom\/what-is-underfitting-and-overfitting-in-machine-learning-and-how-to-deal-with-it-6803a989c76)\n","d54a0a69":"## Visualizing our overall progress with TensorBoard\n\nWe have in all the models used the TensorBoard callback this is because TensorBoard can keep track of the experiments and you just need to use:\n```\n%load_ext tensorboard\n%tensorboard --logdir <log_directory>\n```\n\nUnfortunately, Kaggle doesn't support TensorBoard right now, however I download the logs and uploaded them \nin this link: https:\/\/tensorboard.dev\/experiment\/cDtJEN5fQue6Nac2H5mv1g\/#scalars \n\n**\ud83d\udcdd Note:** Keep in mind that whatever you upload to TensorBoard.dev becomes public. If there are training logs you don't want to share, don't upload them.","20bb6930":"As you can see the syntax for constructing this model was a little different than we did before, this was due to the use of the **Functional API** instead of the **Sequential API**. Functional API may seem a little more complicated but allows more flexibility while creating a model, i.e. non-linear topology, shared layers, and even multiple inputs or outputs.\n\nNow we used new layers while creating this model:\n- Conv2D: This layer allows the convolution we discussed before. We here specify the number of filters that we will use, kernel size, and more. \n- MaxPool: The objective of the MaxPool layers is to reduce the dimensionality, for a kernel that we specify we compare all the pixels inside that kernel and leave only the max value. \n- Flatten: We already do the convolution and the max pooling, now this allows us to at the end create the dense layer that will classify our lego images. \n\n> Please notice that the trainable parameters for this model are around 1M, this is ~15X less than our initial MLP. \ud83e\udd14 What do you think happens? Should it be performing worst or better than our previous model? Let's train and check it out.","82b7348c":"## <a name=\"dataprep\">Preparing the data<\/a>\n\n### 1. Standarization\n\nMany machine learning models, including neural networks, prefer the values they work with to be between 0 and 1. \n\nBut why? \n* If your features are in different scales, the ratio the cost function contours in space, i.e. if you have a dataset from houses with one feature as the number of bathrooms and the other as the price of the house. If you 2D plot those variables you will notice that they will be a long shape. \n* It's faster! This is because the gradient descent will need to take more steps to converge to the local minimum.\n* Scaling each feature to a similar range to prevent or reduce bias in the network.\n* Why not? If your algorithm doesn't get the benefit from scaling it doesn't harm either \ud83d\ude05.\n\n<img src=\"https:\/\/qph.fs.quoracdn.net\/main-qimg-afdfbc63c83e097b3d831777397d905d\" width=\"700px\">\n\n[Check this Video with a full explanation](<https:\/\/www.youtube.com\/watch?v=FDCfw-YqWTE&ab_channel=DeepLearningAI>)\n\n**For our image case is get the pixel values and divide them by 255.**\n\n\n\n### 2. Image Resize\n\nResizing images is a critical preprocessing step in computer vision.\n\nBut why?\n* Machine learning models train faster on smaller images\n* Many deep learning model architectures demand that our pictures be of the same size in all the batches. \n\n[Short blog here](<https:\/\/blog.roboflow.com\/you-might-be-resizing-your-images-incorrectly\/>)\n\n\n\u261d\ud83c\udffd Fortunately for us most this preprocessing steps are common and Tensorflow already implement them and package them. Let's review them.\n\n\n## <a name=\"dataload\">Loading Data<\/a>\n\nCurrently there are several ways to efficiently load the data let me mention some of them:\n* Using [`tf.keras.utils.image_dataset_from_directory`](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/utils\/image_dataset_from_directory)\n* Using an [`ImageDataGenerator`](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/image\/ImageDataGenerator#flow_from_dataframe) and then a `flow_from_dataframe` or `flow_from_directory`\n* Creating a [`tf.data.Dataset`](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data\/Dataset)\n* If you're using `tensorflow_datasets` using the [`tfds.load` ](https:\/\/www.tensorflow.org\/datasets\/api_docs\/python\/tfds\/load)\n\nAs you can see there are many functions from Tensorflow that can help us with this task, the selection of each function depends on the data and folder structure that is built around the data. Fortunately, in our case, the CSV contains all the paths from the imagery. \n\n","6425e7ea":"This will be the our toy tensor, we will see how different activation functions affect the tensor.","0fd21dc8":"# \ud83d\udcda Learning Deep Learning & Tensorflow with LEGO - Image Classification \ud83d\udda5\ud83d\udc41\ud83d\udc41\n\n[Daniel Beltr\u00e1n](https:\/\/www.linkedin.com\/in\/danielbeltranpardo\/) - Oct 2021\n\n----------\nMotivation: This notebook is motivated by the [TensorFlow Developer Certificate in 2021: Zero to Mastery](https:\/\/www.udemy.com\/course\/tensorflow-developer-certificate-machine-learning-zero-to-mastery\/) which covers almost all these topics, a fantastic way to understand a topic is to teach. This notebook then pretends to capture all the knowledge and teach in the kaggle environment about Tensorflow, Deep Learning, and Image classification.\n\n---------\n<center><img src=\"https:\/\/3.bp.blogspot.com\/-QZVBl08fmPk\/XhO909Ha1dI\/AAAAAAAACZI\/q1a1UykGKe0KDUZ_ZITtWmM7bBJFRrvPQCLcBGAsYHQ\/s1600\/tensorflowkeras.jpg\", width = 400px><\/center>\n\n# Introduction \n\nComputer vision has advanced in terms of recognition and tracking thanks to machine learning. There are some sets of problems that the most common like:\n* Image Classification: The use of computer vision allows machines to analyze and categorize what they \"see\" in photos or videos. A classification problem generally involves classifying images into 2 or more classes (single-label problem) or if there is no limit to how many classes an instance can be allocated to it's the multi-label problem.\n* Object Localization and Detection: Object localization and detection is a computer vision issue in which an algorithm is given an image and is asked to determine the positions of one or more target items in the image or video frame, producing bounding boxes for each.\n* Image segmentation: Image segmentation is used in a variety of applications (robotics, satellite imagery, medical imagery, and so on) to not only comprehend the positions of items in pictures and video frames but also to map the borders between distinct objects in the same image more precisely.\n\nIn this Notebook we will focus in the **image classification task** on single label problem.\n\n**<span style=\"color:Green\">\ud83d\udcdd Note: This kernel has been a work of more than 10 days If you find my kernel useful and my efforts appreciable, Please Upvote it, it motivates me to write more Quality content<\/span>**\n\n<a id=\"top\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Contents<\/center><\/h3>\n\n* [1. Problem statement](#problem-statement)\n* [2. Data Exploration](#eda)\n    * [2.1 Visualizing random images](#randomviz)\n* [3. Data preparation](#dataprep)\n    * [3.1 Loading data](#dataload)\n* [4. Multi Layer Perceptron](#mlp)\n    * [4.1 Create and compile a MLP](#mlpcompile)\n    * [4.2 Callbacks and fitting an MLP](#mlpcallbfit)\n    * [4.3 Evaluating the MLP model](#mlpeval)\n* [5. Convolutional Neural Network](#cnn)\n    * [5.1 Create and compile a CNN](#cnncreate)\n    * [5.2 Evaluate a CNN](#cnneval)\n* [6. Transfer learning](#transferlearn)\n    * [6.1 Creating a model with TensorflowHub](#tfhubcreate)\n    * [6.2 Visualizing results from TFHub model](#tfhubresults)\n    * [6.3 Creating a model with Keras Application](#kerasmodel)\n    * [6.4 Visualizing results from KerasApplication model](#kerasresult)\n\nIn each of these contents, I will try to make a brief explanation of the topics and concepts that get presented and immediately do a code implementation.  \n\n    \n## <a name=\"problem-statement\">Problem statement \ud83c\udfaf<\/a>\nBefore we start going we are going to write what we want to achieve: **We want to develop a Machine Learning algorithm that is capable to identify LEGO\u00ae Minifigures images and classify them correctly**, to accomplish this we will use the imagery data available from the LEGO Minifigures dataset.\n\n## <a name=\"eda\">Data exploration \ud83d\udd75\ud83c\udffb\u200d\u2642\ufe0f<\/a>\n\nFor this first part, we will use the motto of **Visualize, Visualize and Visualize** this will help us to understand what data we have available, what preprocessing steps we need to do, what lego Minifigures we have available, what are the classification can be done with our current data.\n\nLet's answering the following questions:\n* How many images we have available?\n* What information does the CSVs contain?\n* How are the folders organized? (This is extremely useful we will cover it later on)\n    \n\n","c8430108":"Wow! We did a ton, let me explain what just happened with this piece of code.\n\nFirst, we set a random seed with `tf.random.set_seed` this is for reproducibility and that when you run the numbers we have the same results, it's a good practice as scientist that other people can replicate our experiments. [You can read more about this topic here](https:\/\/www.kdnuggets.com\/2019\/11\/reproducibility-replicability-data-science.html).\n\nSecondly, we create a model we will introduce first the **SEQUENTIAL API**. Here we create the model layer-by-layer and as you see is very simple and easy to use. We created the following layers:\n* Input Layer: This will assert that our input is Tensor with the proper shapes, we set it to be the `train_ds` shape, which is (240, 240, 3), 240 from the resizing we did early on and 3 for the channels RGB. \n* Flatten: As we seen in the explanation of a MLP, it requires to be a single dimensional vector. But currently we have a 240, 240 vector. Flatten simply 'melt' the tensor and give us a single vector of lenght: 172800. Why this number? It's simply 240 x 240 x 3 ! \n* Dense: This is the heart of our MLP it means that we're going to connect the input that this layer recieves with a total of neurons that we refer at the beggining with an *int*. We can also here specifiy the activation function that we will use for our neurons.\n\n> **Note:** As you can see the first and second hidden layer are the same except we use a different number of units (int), but the last is different. Why? It's because it's the output layer, we explicitly have to specify the number of units as the different number of classes and use a `softmax` activation function, if we use a `relu`, `tan-h` or even a `sigmoid` the outputs wouldn't make sense for our final objective to classify the images.\n\nThirdly we compile the model, we always need to compile it, in this step we specify the how are we are going to calculate the loss in our case since it's a multi-classification we used the `CategoricalCrossEntropy()`, if we had a binary classification we could use the `BinaryCrossEntropy()`, if we instead have a regression problem the `MeanAbsolutError()` or `MeanSquareError()` could have been the regression metric. Check more [here](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/losses).\n\nFinally, we use an optimizer in this case we used Adam optimizer there are multiple you can try on and you can check in depth [here](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/optimizers), with these optimizer we calculate the derivate of error and use the learning rate on it. \n\n\nLet's visualize the model we just construct: ","5e831c3d":"As you can see there is no need to call an extra package, you just need to call the model of interes within keras application module.","7d2f0214":"## Predicting with a new image\n\nOur final objective was to predict any Lego image, with that mindset I created a folder with some extra lego-test-images to test with our new algorithm and show how we can work when we want to inference new images.","a0976aa3":"Amazing! We have now increase the train accuracy to almost 100% and test accuracy to almost 30% that is so much better than our previous model, but as you can see the accuracy from training and validation differs by a lot, event the losses are increasing in our validation set. This is a symptom of the overfitting we discussed previously and it means that our current model is memorizing the LEGO rather than generalizing.\n\nHow about if we introduce some data augmentation and see if this symptom it's better? \n\n**\ud83d\udcdd Note:** Here we already identify what is a technique that is worth spending some time optimizing, we can now call our inside scientist and start experimenting \ud83d\udc68\ud83c\udffd\u200d\ud83d\udd2c.","e5d84499":"### <a name=\"mlpcallbfit\">MLP Training - Callbacks and Fit function<\/a>\n\nLet's resume the training of the model, as we did before let me write code and later on explain.","c0b96193":"Here we used the same Functional API in the past BUT! we added a new layer that we hadn't seen before, `GlobalAveragePooling` is a pooling operation designed to replace Dense layers and they help to minimize overfitting by reducing the total number of parameters in the model. Why? It's because instead of taking all the pixels to flatten in a dense layer we're taking the Avg of the tensor. Let me show you about:","5c926bae":"As we can see we got around 8.2 Million parameters! We also in the summary can check the output shape of our feature extraction, with this information we concluded that it wasn't neccesary to add a Flatten layer. \n\n**Note:** In the summary we can also check the number of trainable parameters, we will just train ~36K this is the 1000 inputs from our feature extraction layer all connected to the output layer (36x1000) + 36 bias terms","9880774c":"**Optimal way:**","0a1ce273":"## <a name=\"randomviz\">Visualizing random images<\/a>","7d4f6455":"When we use the tf.hub most of the times we get a Keras Layer object back, then we add it to the Sequential API (or Functional, but it is more complicated) and add the last layer our own output layer. \n\n**Note:** We specify the input shape in the `hub.KerasLayer` method and we set the trainable parameter to false, this is mandatory if you wish to fine-tune to your case and not start training all the layers.","52520edc":"# <a name=\"mlp\">First model: Multilayer Perceptron (MLP)<\/a>\n\nThe architecture of a classification neural network can widely vary depending on the problem you're working on. However, there are some fundamentals all deep neural networks contain:\n* An input layer.\n* Some hidden layers.\n* An output layer.\n\nInput and output layers, as well as one or more hidden layers with numerous neurons layered together, make up a Multilayer Perceptron. Multilayer Perceptron can have whatever **activation function** they choose: ReLu, Tan-H, Sigmoid, etc. These activation functions are the fundamental piece required for an MLP to learn the relationship between linear and non-linear data, we will see more in the depth of these functions. \n\n<img src=\"https:\/\/www.researchgate.net\/profile\/Mohamed-Zahran-16\/publication\/303875065\/figure\/fig4\/AS:371118507610123@1465492955561\/A-hypothetical-example-of-Multilayer-Perceptron-Network.png\">\n\n**The main characteristic of the MLP then is that all the neurons in a layer are fully connected to the others as we can see in the image. **\n\nEach layer feeds the output of its computation, or internal representation of the data, to the next. This applies to all hidden levels as well as the output layer. This is known as **feedforward**, but what computation are we doing here? Inputs are combined with the initial weights in a weighted sum and subjected to the activation function, like the following:\n\n![perceptron.png](attachment:32d637f2-0ec9-4e29-b01a-20dbfb611c49.png)\n\nThis is equivalent to this in linear algebra:\n> $W^T X + b$ \n\nThe final goal of the algorithm then is to find the $W$ that minimizes an error (difference between prediction and real output). However, if we just made the feedforward computations and stopped there the algorithm wouldn\u2019t be able to learn the weights that minimize the cost function, and then there needs to be some feedback to the model to adjust the weights. This is when **backpropagation** is required. \n\nBackpropagation is a learning method that allows the Multilayer Perceptron to iteratively modify the network's weights in order to minimize the cost function. How? Using differential calculus, if we calculate the derivates we can know the direction in which we need to move to improve our predictions and once we calculate the direction we multiply by a **learning rate** as the following:\n\n<img src = \"http:\/\/hmkcode.github.io\/images\/ai\/bp_update_formula.png\">\n\nThen just see the **learning rate** as to how big is the step we are going to take in the direction of our minimum, this is why this parameter is fundamental. This parameter follows the Goldilocks rule: If you take big steps you miss the spot, if you take too little steps you take too much time, you need the right.\n\n<img src = \"https:\/\/www.jeremyjordan.me\/content\/images\/2018\/02\/Screen-Shot-2018-02-24-at-11.47.09-AM.png\" width = 700px>\n\nYou can see the Full MIT Deep learning [here](https:\/\/youtu.be\/5tvmMX8r_OM), it covers all these topics precisely and in an unique way.\n\n**Happily in our case Tensorflow already implements all this logic (thank you TensorFlow!) lets implement it and discuss the code afterwards.**","548d0f76":"We trained sucesfully our first model! \n\nWe cover a few things so let me start explaining, first of all let's check the **Callbacks**\n\nCallbacks are an extra functionality you can add to your models to be performed during or after training. Why we want to use callbacks? Well it can help you prevent overfitting, visualize training progress, debug your code, save checkpoints, generate logs, create a TensorBoard, etc.\n\nIn our case we used it for 2 purposes:\n1. Create a tensorboard to later on check visually all our experiments.\n2. Save the best model as a CheckPoint, this is not required for our current model since it trains fast enough but imagine if you have a model that takes 10 hours to train, you would want to save different checkpoints while training in case someone by error turns off the computer when it was missing 5 min.\n\nYou can check more callbacks from Tensorflow documentation [here](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/callbacks) \n\n\nNow let's check the arguments we used in the `fit` function:\n- We used the `train_ds` TF will detect in the `train_ds` that we have our X and Y.\n- We specify the number of epochs that we want to train for, an *epoch means training the neural network with all the training data for one complete pass. In an epoch, we use all of the data exactly once. A forward pass and a backward pass together are counted as one pass.*\n- The steps_per_epoch is the number of batches we have in our training data. In this case we have 361 images and we divide them by batches of 32, this will give us around 11.28  (last batch isn't from 32 but instead from 9) then we have 11 batches from 32 and a last batch of 9 for a total of 12 batches.\n- Validation data would be in this case our `test_ds` with this we will compute the metrics that we specify in the compilation of the model.\n- Validation steps: Same as steps for epoch but for validation data. \n- Callbacks: This would be a *list* that contains all the callbacks we want to have during the training.\n\nBut what is the performance of our current model?\n","de0a186c":"Let's finally train our model with the GlobalAveragePooling!","d1d748d5":"Wow! Now finally see some of the activation functions and how they transform a input tensor. So the first thing we noticed is that **activation functions are fundamental for the non-linearity power from neural networks** without activation functions we would be working on the linear space.\n\nNow let's review these graphs:\n* **Sigmoid**: Transform our data between the range of 0 and 1, all negatives get almost 0 as value and all positives get almost 1 as value. This is the why when we have a binary classifier instead of softmax we used a sigmoid, is either a thing or the other.\n* **ReLu**: For the relu all the negatives get 0 as value else it returns there current value.\n* **Tah-H**: Is similar to Sigmoid however values are between -1 and 1 range. \n* **Softmax**: Softmax is the hardest to see in graph, but it normalize the output of a network to a probability distribution over predicted output classes. In other words it transform whatever the input to the range of probabilities where the summ of all the tensor should be 1 (or very very close due to how computers calculate). \n\n","1d315502":"Nice! A very crucial step at the beginning of any machine learning project is becoming one with the data, visualizing random images is a great way to understand what we're working with. Now we have a general idea of what we have but after we proceed let's explain what we did especially on the TF code that we just wrote.\n\nComputers don't understand images as we humans do, for a computer an image is just a combination of generally speaking 3 layers of numbers (RGB).  As the following:\n\n<img src=\"https:\/\/www.researchgate.net\/profile\/Muhammad-Qureshi-41\/publication\/307091456\/figure\/fig2\/AS:407248779137024@1474107083220\/3rd-order-Tensor-representation-of-a-color-image.png\" width=\"500px\">\n\nThese numbers are between 0 and 255, 0 meaning zero light and 255 meaning maximum light. If we have a pixel with a value of red=255, green=0, blue=0, means full red pixel. \n\nFor being able to visualize the images we did the following:\n1. Get a random sample, for that we used the pandas sample and get the path on that.\n2. Read the image, we used `tf.io.read_file` method from the TensorFlow library (since we're learning TF) this method receives as input the filename to read from and outputs a tensor. We could use another library to read an image as CV2, mpimg from matplotlib, and more but the advantage is that we already got an object that can be interpreted by TensorFlow.\n3. We decode the IO from `tf.io.read_file` with `tf.image.decode_image` we transform that into a real tensor. This tensor is on the following shape: width, height, channel. Channels as we previously see most of the time are 3 (1 for greyscale images and 4 for png transparency). \n4. We use `tf.image.resize` to resize the image (no surprise!), this is transforming from the original width and height to a new width and height. This will help us to visualize more images if we cast a smaller shape than the original.\n5. After we resize the image the tensor numbers change a little and instead of the integer are now floats, we could use another argument `ResizeMethod`, and use nearest or we could get the float and divide it by 255. Implot can plot from the 0 to 255 range or from 0 to 1 range. \n\nLet's see it in code:\n","8364b863":"As we can see train and test data have a path and a class id, this class id can be matched with the metadata to know what is the Minifigure name.\n\nThere are 37 different Legos Minifigures, also we noticed something interesting is that the number of test images in the CSV is different from the folder (difference by 2), so maybe there are some images that aren't used or are corrupted. For the sake of simplicity, we will stick with the paths given by the CSV's.\n\nWe will merge the metadata with the train and test CSV, this will help us to have the Minifigure name and visualize some random images with that name.","bfe7d939":"## <a name=\"cnncreate\">Creating and compiling a CNN<\/a>","475daeb4":"### <a name=\"mlpeval\">Evaluating MLP model performance<\/a>","0de97365":"## Prefetching and caching\n\nWe will prefetching and shuffle. This will help us with computing time as the memory wouldn't not be sequential batching and before a batch is done the other one will get prepared.","c7e1350d":"Wonderful! Looks like our training dataset has 361 images belonging to 36 classes and our test dataset has 76 images also belonging to also 36 classes.\n\nLet's highlight the following:\n\n* Due to how our dataframe is written the PATH helps TF to find all the images. \n* The target_size parameter in `datagen.flow_from_dataframe`  defines the input size of our images in (height, width) format.\n* The batch_size defines how many images will be in each batch, normally in deep learning we don't want to pass all the data in a single batch but rather pass minibatches. [You can't trust me but trust Yann LeCun](https:\/\/twitter.com\/ylecun\/status\/989610208497360896?lang=en)","41738392":"## <a name='kerasmodel'> Creating models using Keras Applications <\/a>\n\nCreating models from Keras Application is my preferred way to transfer learn because you can start unfreezing some of the layers as your data increases and you can check more details like the top layers and you're working with a model object type, not a layer. This adds some functionality like checking if a specific layer is trainable, the name of that layer, and more we will see now.","d19a3a0d":"Here we can see the name of some layers, if that layer is trainable or not, and the dtype. \n\nIf you go through the documentation of EfficientNet model you can read that the RESCALING is in the model included as a layer, so far all the models that we have explored need that rescaling before passing it through the model, for proper use of EffNet we need to reload the data but without rescaling this time.","bba6b94e":"So far this has been our best model to perform with the pooling layer we reduce a little bit the overfitting and got around 67% performance on the test set. This is the way to go.","2f5e3abc":"This means every iteration is 32 batches long from a 240 height and 240 width by 3 RGB channels, and the labels are 32 batches by 36 dimensions (one for each possible class).","0b32b331":"As we saw earlier our flatten effectively outputs 150528 as shape and we have over 15M parameters! Wow!\n\n\nNevertheless, before we train our model you can see that activation function are a crucial step for our Deep Learning Model. Let's quickly wrap some of these activation functions, see what they do with a TOY tensor. ","03aa940a":"## End Notes\n\nWe completed the purpose of this notebook to teach about the very basics and built some intuiton around deep learning and pass very quickly through some of the most important contents to get started. We trained a set of models starting from the very elemental structure from an MLP to transfer learning, what are we missing then?\n\n* Finally fixing the overfiting, we discussed about ways to reduce the overfit but we did not walk through them except from the transfer learning. \n* Error analysis on the cases that the model is not perfoming well, this will help us guide what efforts need to be make.\n\nThis was my effort to share my learnings so that everyone can benefit from it, if you like the content don't doubt to take Daniel Bourcke course he has the first 14 hours for free, check it [here](https:\/\/youtu.be\/tpCFfeUEGs8).\n\nIf this kernel receives love,I plan to keep on bringing the best insights around Deep Learning with TF but this time for NLP and Time-Series. \n\n<span style=\"color:Green\">Please consider upvote it if my efforts help you, it motivates me to write more notebooks explaining future content<\/span>","15ef56eb":"## <a name='kerasresult'> Evaluating Keras Application Model performance <\/a>","ee48b46e":"We now made some predictions on a totally new set of images, as you can see it isn't perfect yet however we could precisely predict some new images like R2-D2, Darth-Vader, Harry Potter.\n\nFor the other images it appears that they're getting confused, in the case of Obi-wan the lightsaber and clothes are the same to Mace windu and also Cara Dune could be the % of black and some blue related to Emperor Palpatine.\n\nAs we have seen before there is much room to improve to our current 'best' model and we could even input more variety of images to generalize even more the set of images we have."}}