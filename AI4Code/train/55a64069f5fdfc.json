{"cell_type":{"e368fe34":"code","a968b726":"code","15fc764e":"code","7f6517a9":"code","b0a7f2b1":"code","29a05af9":"code","1649f27b":"code","6aa2ce59":"code","26166309":"code","48ff7bfb":"code","ea74dac1":"code","bd148073":"code","3467d428":"code","a10894d5":"code","ed6d970a":"code","6864f37a":"code","97a6ff63":"code","f9dfa19b":"code","f0e3908e":"code","f6c193d5":"code","6eee3fd0":"code","6c38e265":"code","79803b6d":"code","9244c0c8":"code","11f5148a":"code","81be584b":"code","cde92609":"markdown","74d6bd2d":"markdown","c53d1ad1":"markdown","8410dec5":"markdown","31ad4073":"markdown","c4b015a7":"markdown","ae6ba052":"markdown","4681eb62":"markdown","c148ffb0":"markdown","88ee5602":"markdown","bcb12c33":"markdown","c83b2e47":"markdown","a940380c":"markdown","c7cb31fd":"markdown","7892b9a1":"markdown","d682060f":"markdown","20feb5d0":"markdown","5396f8e5":"markdown"},"source":{"e368fe34":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a968b726":"#All the libralies used in this project\n\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom xgboost import XGBRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics","15fc764e":"data = pd.read_csv(\"..\/input\/diamonds\/diamonds.csv\")\ndata.head()","7f6517a9":"data.shape","b0a7f2b1":"data.info()","29a05af9":"#The first column seems to be just index\ndata = data.drop([\"Unnamed: 0\"], axis=1)\ndata.describe()","1649f27b":"#Dropping dimentionless diamonds\ndata = data.drop(data[data[\"x\"]==0].index)\ndata = data.drop(data[data[\"y\"]==0].index)\ndata = data.drop(data[data[\"z\"]==0].index)\ndata.shape","6aa2ce59":"#Let's have a look at data \nshade = [\"#835656\", \"#baa0a0\", \"#ffc7c8\", \"#a9a799\", \"#65634a\"]#shades for hue\nax = sns.pairplot(data, hue= \"cut\",palette=shade)# I chose \"cut\" as hue. We can also examine other attributes in hue with less value count. \n","26166309":"ax = sns.regplot(x=\"price\", y=\"y\", data=data, fit_reg=True, scatter_kws={\"color\": \"#a9a799\"}, line_kws={\"color\": \"#835656\"})\nax.set_title(\"Regression Line on Price vs 'y'\", color=\"#4e4c39\")","48ff7bfb":"ax= sns.regplot(x=\"price\", y=\"z\", data=data, fit_reg=True, scatter_kws={\"color\": \"#a9a799\"}, line_kws={\"color\": \"#835656\"})\nax.set_title(\"Regression Line on Price vs 'z'\", color=\"#4e4c39\")","ea74dac1":"ax= sns.regplot(x=\"price\", y=\"depth\", data=data, fit_reg=True, scatter_kws={\"color\": \"#a9a799\"}, line_kws={\"color\": \"#835656\"})\nax.set_title(\"Regression Line on Price vs Depth\", color=\"#4e4c39\")","bd148073":"ax=sns.regplot(x=\"price\", y=\"table\", data=data, fit_reg=True, scatter_kws={\"color\": \"#a9a799\"}, line_kws={\"color\": \"#835656\"})\nax.set_title(\"Regression Line on Price vs Table\", color=\"#4e4c39\")","3467d428":"#Dropping the outliers. \ndata = data[(data[\"depth\"]<75)&(data[\"depth\"]>45)]\ndata = data[(data[\"table\"]<80)&(data[\"table\"]>40)]\ndata = data[(data[\"x\"]<30)]\ndata = data[(data[\"y\"]<30)]\ndata = data[(data[\"z\"]<30)&(data[\"z\"]>2)]\ndata.shape","a10894d5":"ax=sns.pairplot(data, hue= \"cut\",palette=shade)","ed6d970a":"# Get list of categorical variables\ns = (data.dtypes ==\"object\")\nobject_cols = list(s[s].index)\nprint(\"Categorical variables:\")\nprint(object_cols)","6864f37a":"plt.figure(figsize=(12,8))\nax = sns.violinplot(x=\"cut\",y=\"price\", data=data, palette=shade,scale= \"count\")\nax.set_title(\"Violinplot For Cut vs Price\", color=\"#4e4c39\")\nax.set_ylabel(\"Price\", color=\"#4e4c39\")\nax.set_xlabel(\"Cut\", color=\"#4e4c39\")","97a6ff63":"plt.figure(figsize=(12,8))\nshade_1 = [\"#835656\",\"#b38182\", \"#baa0a0\",\"#ffc7c8\",\"#d0cd85\", \"#a9a799\", \"#65634a\"]\nax = sns.violinplot(x=\"color\",y=\"price\", data=data, palette=shade_1,scale= \"count\")\nax.set_title(\"Violinplot For Color vs Price\", color=\"#4e4c39\")\nax.set_ylabel(\"Price\", color=\"#4e4c39\")\nax.set_xlabel(\"Color\", color=\"#4e4c39\")","f9dfa19b":"plt.figure(figsize=(12,8))\nshade_2 = [\"#835656\",\"#b38182\", \"#baa0a0\",\"#ffc7c8\",\"#f1f1f1\",\"#d0cd85\", \"#a9a799\", \"#65634a\"]\nax = sns.violinplot(x=\"clarity\",y=\"price\", data=data, palette=shade_2,scale= \"count\")\nax.set_title(\"Violinplot For Clarity vs Price\", color=\"#4e4c39\")\nax.set_ylabel(\"Price\", color=\"#4e4c39\")\nax.set_xlabel(\"Clarity\", color=\"#4e4c39\")","f0e3908e":"# Make copy to avoid changing original data \nlabel_data = data.copy()\n\n# Apply label encoder to each column with categorical data\nlabel_encoder = LabelEncoder()\nfor col in object_cols:\n    label_data[col] = label_encoder.fit_transform(label_data[col])\nlabel_data.head()","f6c193d5":"data.describe()","6eee3fd0":"#correlation matrix\ncmap = sns.diverging_palette(70,20,s=50, l=40, n=6,as_cmap=True)\ncorrmat= label_data.corr()\nf, ax = plt.subplots(figsize=(12,12))\nsns.heatmap(corrmat,cmap=cmap,annot=True, )","6c38e265":"# Assigning the featurs as X and trarget as y\nX= label_data.drop([\"price\"],axis =1)\ny= label_data[\"price\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.25, random_state=7)","79803b6d":"# Building pipelins of standard scaler and model for varios regressors.\n\npipeline_lr=Pipeline([(\"scalar1\",StandardScaler()),\n                     (\"lr_classifier\",LinearRegression())])\n\npipeline_dt=Pipeline([(\"scalar2\",StandardScaler()),\n                     (\"dt_classifier\",DecisionTreeRegressor())])\n\npipeline_rf=Pipeline([(\"scalar3\",StandardScaler()),\n                     (\"rf_classifier\",RandomForestRegressor())])\n\n\npipeline_kn=Pipeline([(\"scalar4\",StandardScaler()),\n                     (\"rf_classifier\",KNeighborsRegressor())])\n\n\npipeline_xgb=Pipeline([(\"scalar5\",StandardScaler()),\n                     (\"rf_classifier\",XGBRegressor())])\n\n# List of all the pipelines\npipelines = [pipeline_lr, pipeline_dt, pipeline_rf, pipeline_kn, pipeline_xgb]\n\n# Dictionary of pipelines and model types for ease of reference\npipe_dict = {0: \"LinearRegression\", 1: \"DecisionTree\", 2: \"RandomForest\",3: \"KNeighbors\", 4: \"XGBRegressor\"}\n\n# Fit the pipelines\nfor pipe in pipelines:\n    pipe.fit(X_train, y_train)","9244c0c8":"cv_results_rms = []\nfor i, model in enumerate(pipelines):\n    cv_score = cross_val_score(model, X_train,y_train,scoring=\"neg_root_mean_squared_error\", cv=10)\n    cv_results_rms.append(cv_score)\n    print(\"%s: %f \" % (pipe_dict[i], cv_score.mean()))","11f5148a":"# Model prediction on test data\npred = pipeline_xgb.predict(X_test)","81be584b":"# Model Evaluation\nprint(\"R^2:\",metrics.r2_score(y_test, pred))\nprint(\"Adjusted R^2:\",1 - (1-metrics.r2_score(y_test, pred))*(len(y_test)-1)\/(len(y_test)-X_test.shape[1]-1))\nprint(\"MAE:\",metrics.mean_absolute_error(y_test, pred))\nprint(\"MSE:\",metrics.mean_squared_error(y_test, pred))\nprint(\"RMSE:\",np.sqrt(metrics.mean_squared_error(y_test, pred)))","cde92609":"<a id=\"2\"><\/a>\n<h1 style='background:#a9a799; border:0; color:black'><center>LOADING DATA<\/center><\/h1> \n\n# Loading Data\n\nThis classic dataset contains the prices and other attributes of almost 54,000 diamonds. There are 10 attributes included in the dataset including the target ie. price.\n\n**Feature description**:\n\n**price** price in US dollars (\\$326--\\$18,823)This is the target column containing tags for the features.\u00a0\n\n**The 4 Cs of Diamonds:-**\n\n**carat (0.2--5.01)**\nThe carat is the diamond\u2019s physical weight measured in metric carats.\u00a0\u00a0One carat equals 1\/5 gram and is subdivided into 100 points. Carat weight is the most objective grade of the 4Cs.\u00a0\n\n**cut (Fair, Good, Very Good, Premium, Ideal)**\nIn determining the quality of the cut, the diamond grader evaluates the cutter\u2019s skill in the fashioning of the diamond. The more precise the diamond is cut, the more captivating the diamond is to the eye.\u00a0\u00a0\n\n**color, from J (worst) to D (best)**\nThe colour of gem-quality diamonds occurs in many hues. In the range from colourless to light yellow or light brown. Colourless diamonds are the rarest. Other natural colours (blue, red, pink for example) are known as \"fancy,\u201d and their colour grading is different than from white colorless diamonds.\u00a0\u00a0\n\n**clarity (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))**\nDiamonds can have internal characteristics known as\u00a0inclusions\u00a0or external characteristics known as\u00a0blemishes. Diamonds without inclusions or blemishes are rare; however, most characteristics can only be seen with magnification.\u00a0\u00a0\n\n**Dimensions**\n\n**x length in mm (0--10.74)**\n\n**y width in mm (0--58.9)**\n\n**z depth in mm (0--31.8)**\n\n\n![diamands%20project%20%281%29.png](attachment:diamands%20project%20%281%29.png)\n\n**depth total depth percentage = z \/ mean(x, y) = 2 * z \/ (x + y) (43--79)**\nThe depth of the diamond is its height (in millimetres) measured from the culet (bottom tip) to the table (flat, top surface).\n\n**table width of the top of the diamond relative to widest point (43--95)**\n\nA diamond's table refers to the flat facet of the diamond seen when the stone is face up. The main purpose of a diamond table is to refract entering light rays and allow reflected light rays from within the diamond to meet the observer\u2019s eye. The ideal table cut diamond will give the diamond stunning fire and brilliance.\n","74d6bd2d":"The first column is an index (\"Unnamed: 0\") and thus we are going to remove it.","c53d1ad1":"We have three categorical variables. \nLet us have a look at them.","8410dec5":" # <h1 style='background:#a9a799; border:0; color:black'><center>Price Prediction Of Diamonds: Regression<\/center><\/h1> \n   ![diamands%20project.png](attachment:diamands%20project.png)\n   <a id='top'><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h1 style='background:#a9a799; border:0; color:black'><center>INDEX<\/center><\/h1>\n\n[1. IMPORTING LIBRARIES](#1)     \n[2. LOADING DATA](#2)    \n[3. DATA PREPROCESSING](#3)     \n[4. MODEL BUILDING](#4)     \n[5. FIN](#5)","31ad4073":"We can clearly spot outliers in these attributes. Next up, we will remove these data points.Now that we have removed regression outliers, let us have a look at the pair plot of data in our hand. ","c4b015a7":"That's a much cleaner dataset. \nNext, we will deal with the categorical variables. ","ae6ba052":"<a id=\"1\"><\/a>\n<h1 style='background:#a9a799; border:0; color:black'><center>LIBRARIES<\/center><\/h1> \n\n# Importing Libraries\n","4681eb62":"We can clearly spot outliers in these attributes. Next up, we will remove these data points. ","c148ffb0":"We lost 20 data points by deleting the dimensionless(2-D or 1-D) diamonds.","88ee5602":"<a id=\"3\"><\/a>\n <h2 style='background:#a9a799; border:0; color:black'><center>DATA PREPROCESSING<\/center><\/h2> \n# Data Preprocessing\n**Steps involved in Data Preprocessing**\n* Data cleaning\n* Identifying and removing outliers\n* Encoding categorical variables\n","bcb12c33":"**Testing the Model with the best score on the test set**\n\nIn the above scores, XGBClassifier appears to be the model with the best scoring on negative root mean square error. Let's test this model on a test set and evaluate it with different parameters. ","c83b2e47":"Lable encoding the data to get rid of object dtype.","a940380c":"**A few points to notice in these pair plots**\n\nThere are some features with datapoint that are far from the rest of the dataset which will affect the outcome of our regression model.\n\n* \"y\" and \"z\" have some dimensional outlies in our dataset that needs to be eliminated.\n* The \"depth\" should be capped but we must examine the regression line to be sure.\n* The \"table\" featured should be capped too.\n* Let's have a look at regression plots to get a close look at the outliers.","c7cb31fd":"\n<a id=\"4\"><\/a>\n <h1 style='background:#a9a799; border:0; color:black'><center>MODEL BUILDING<\/center><\/h1> \n# Model Building\n\n**Steps involved in Model Building**\n\n* Setting up features and target\n* Build a pipeline of standard scalar and model for five different regressors.\n* Fit all the models on training data\n* Get mean of cross-validation on the training set for all the models for negative root mean square error\n* Pick the model with the best cross-validation score\n* Fit the best model on the training set and get","7892b9a1":"**Pairplot Of Data**","d682060f":"**Points to notice:**\n* \"x\", \"y\" and \"z\" show a high correlation to the target column. \n* \"depth\", \"cut\" and \"table\" show low correlation. We could consider dropping but let's keep it. ","20feb5d0":"**Points to notice:**\n\nMin value of \"x\", \"y\", \"z\" are zero this indicates that there are faulty values in data that represents dimensionless or 2-dimensional diamonds. So we need to filter out those as it clearly faulty data points.","5396f8e5":"**<span style=\"color:#65634a;\"> If you liked this Notebook, please do upvote.<\/span>**\n\n**<span style=\"color:#65634a;\"> If you have any suggestions or questions, I am all ears!<\/span>**\n\n**<span style=\"color:#65634a;\">Best Wishes!<\/span>**\n\n<a id=\"5\"><\/a> \n# <h1 style='background:#a9a799; border:0; color:black'><center>FIN<\/center><\/h1> "}}