{"cell_type":{"4eac43e0":"code","120728c9":"code","b8293827":"code","0ec2446a":"code","72c71e78":"code","dd1d5f4d":"code","dcbeb6f8":"code","0c3dabde":"code","7df55291":"code","2bf4e955":"code","88050f12":"code","ec01c529":"code","9d1b3f1f":"code","e632b76d":"code","3ff5492e":"code","9656555b":"markdown","8fedfd98":"markdown","41ba26b6":"markdown","aa607b49":"markdown","4f5be6bf":"markdown","83b7610f":"markdown","8ebc1f5e":"markdown","ac83fc8f":"markdown","b041d515":"markdown"},"source":{"4eac43e0":"import tensorflow as tf,numpy as np,pandas as pd\nimport sklearn as sl,pylab as pl,os,h5py,urllib\nimport tensorflow.keras.callbacks as tkc,\\\ntensorflow.keras.layers as tkl\nfrom sklearn.datasets import load_digits\nfrom IPython.display import display,HTML","120728c9":"def tf_model(data_shape,num_classes):\n    model=tf.keras.Sequential()\n    model.add(tkl.Conv2D(32,(5,5),padding='same', \n                         input_shape=data_shape))\n    model.add(tkl.LeakyReLU(alpha=.2))\n    model.add(tkl.MaxPooling2D(pool_size=(2,2)))\n    model.add(tkl.Dropout(.25))\n    model.add(tkl.Conv2D(128,(5,5),padding='same', \n                         input_shape=x_train.shape[1:]))\n    model.add(tkl.LeakyReLU(alpha=.2))\n    model.add(tkl.MaxPooling2D(pool_size=(2,2)))\n    model.add(tkl.Dropout(.25))\n    model.add(tkl.GlobalMaxPooling2D())    \n    model.add(tkl.Dense(1024))\n    model.add(tkl.LeakyReLU(alpha=.2))\n    model.add(tkl.Dropout(.5))      \n    model.add(tkl.Dense(num_classes))\n    model.add(tkl.Activation('softmax'))\n    model.compile(loss='sparse_categorical_crossentropy', \n                  optimizer='nadam',metrics=['accuracy'])    \n    return model","b8293827":"def keras_history_plot(fit_history,fig_size=7,color='#ff36ff'):\n    keys=list(fit_history.history.keys())\n    list_history=[fit_history.history[keys[i]] \n                  for i in range(len(keys))]\n    dfkeys=pd.DataFrame(list_history).T\n    dfkeys.columns=keys\n    fig=pl.figure(figsize=(fig_size,fig_size))\n    ax1=fig.add_subplot(2,1,1)\n    dfkeys.iloc[:,[0,2]].plot(\n        ax=ax1,color=['slategray',color])\n    pl.grid(); ax2=fig.add_subplot(2,1,2)\n    dfkeys.iloc[:,[1,3]].plot(\n        ax=ax2,color=['slategray',color])\n    pl.grid(); pl.show()","0ec2446a":"def check_model(x_train,y_train,x_valid,y_valid,x_test,y_test,\n                batch_size=64,epochs=300):\n    num_classes=len(set(y_train))\n    data_shape=x_train.shape[1:]\n    model=tf_model(data_shape,num_classes)\n    checkpointer=tkc.ModelCheckpoint(\n        filepath='\/tmp\/checkpoint',verbose=0,save_weights_only=True,\n        monitor='val_accuracy',mode='max',save_best_only=True)\n    lr_reduction=tkc.ReduceLROnPlateau(\n        monitor='val_loss',verbose=0,patience=10,factor=.8)\n    early_stopping=tkc.EarlyStopping(\n        monitor='val_loss',patience=20,verbose=2)\n    history=model.fit(\n        x_train,y_train,batch_size=batch_size,epochs=epochs,verbose=0,\n        validation_data=(x_valid,y_valid),\n        callbacks=[checkpointer,lr_reduction,early_stopping])\n    keras_history_plot(history)\n    model.load_weights('\/tmp\/checkpoint')\n    print('train results:')\n    model.evaluate(x_train,y_train)\n    print('test results:')\n    model.evaluate(x_test,y_test)\n    return model","72c71e78":"images,labels,x_test,x_valid,x_train,y_test,y_valid,y_train=\\\n[],[],[],[],[],[],[],[]\ndef data_split():\n    global images,labels,x_test,x_valid,x_train,y_test,y_valid,y_train\n    N=labels.shape[0]; n=int(.1*N); shuffle_ids=np.arange(N)\n    np.random.RandomState(123).shuffle(shuffle_ids)\n    images=images[shuffle_ids]; labels=labels[shuffle_ids]\n    x_test,x_valid,x_train=\\\n    images[:n],images[n:2*n],images[2*n:]\n    y_test,y_valid,y_train=\\\n    labels[:n],labels[n:2*n],labels[2*n:]\n    df=pd.DataFrame(\n        [[x_train.shape,x_valid.shape,x_test.shape],\n         [x_train.dtype,x_valid.dtype,x_test.dtype],\n         [y_train.shape,y_valid.shape,y_test.shape],\n         [y_train.dtype,y_valid.dtype,y_test.dtype]],\n        columns=['train','valid','test'],\n        index=['image shape','image type','label shape','label type'])\n    display(df)","dd1d5f4d":"(images,labels)=load_digits(return_X_y=True)\nimages=images.reshape(-1,8,8,1)\ndata_split()","dcbeb6f8":"model_tr1=check_model(x_train,y_train,x_valid,y_valid,x_test,y_test)","0c3dabde":"path='https:\/\/raw.githubusercontent.com\/OlgaBelitskaya\/data_kitchen\/main\/'\nzf='Flowers128.h5'; input_file=urllib.request.urlopen(path+zf)\noutput_file=open(zf,'wb'); output_file.write(input_file.read())\noutput_file.close(); input_file.close()\nwith h5py.File(zf,'r') as f:\n    keys=list(f.keys())\n    display(HTML('<p>h5py.File keys: '+', '.join(keys)+'<\/p>'))\n    images=np.array(f[keys[0]])\n    labels=np.array(f[keys[1]])\n    names=[el.decode('utf-8')for el in f[keys[2]]]\n    f.close()","7df55291":"data_split()\nfig=pl.figure(figsize=(8,4))\nn=np.random.randint(1,100)\nfor i in range(n,n+6):\n    ax=fig.add_subplot(2,3,i-n+1,xticks=[],yticks=[])\n    ax.set_title(\n        names[labels[i]],color='slategray',\n        fontdict={'fontsize':'large'})\n    ax.imshow((images[i]))\npl.tight_layout(); pl.show()","2bf4e955":"model_tr2=check_model(x_train,y_train,x_valid,y_valid,x_test,y_test)","88050f12":"names=[['lowercase','uppercase'],\n       [s for s in u'\u0430\u0431\u0432\u0433\u0434\u0435\u0451\u0436\u0437\u0438\u0439\u043a\u043b\u043c\u043d\u043e\u043f\u0440\u0441\u0442\u0443\u0444\u0445\u0446\u0447\u0448\u0449\u044a\u044b\u044c\u044d\u044e\u044f'],\n       ['single-colored paper','striped paper',\n        'squared paper','graph paper']]\npath='..\/input\/handwritten-russian-letters\/'\nimg_size=32","ec01c529":"def file_name(lower_upper,letter,background,names=names):\n    file_pre='%02d_'%names[0].index(lower_upper)\n    file_pre+='%02d_'%names[1].index(letter)\n    file_pre+='%02d'%names[2].index(background)\n    return file_pre+'.h5'\nfile_names=[file_name(n0,n1,n2) for n2 in names[2] \n            for n1 in names[1] for n0 in names[0]]\nfile_names[:66]","9d1b3f1f":"images,nlabels,labels=[],[],[]\nfor h5f in file_names[:66]:\n    with h5py.File(path+h5f,'r') as f:\n        keys=list(f.keys()); nums=510\n        print('file name: %s; '%h5f+'number of images: %d'%nums)\n        images.append(np.array(f[keys[0]][:nums],dtype='float32'))\n        nlabels=[el.decode('utf-8') for el in f[keys[1]]]\n        labels.append(np.array(\n           nums*[[names[i].index(nlabels[i]) for i in range(3)]],\n           dtype='int32'))\n        f.close()\nimages=np.concatenate(images,axis=0)\n#images=tf.image.resize(images,[img_size,img_size]).numpy()\nlabels=np.concatenate(labels,axis=0)\nlabels=labels[:,1]\nimages.shape,images.dtype,nlabels,\\\nlabels.shape,labels.dtype","e632b76d":"data_split()\ndef display_images(n_images,cols,images,labels):\n    rows=n_images\/\/cols    \n    pl.figure(figsize=(cols,rows))\n    for i in range(n_images):\n        pl.subplot(rows,cols,i+1)\n        pl.imshow(images[i])\n        pl.title(str(labels[i]))\n        pl.xticks([]); pl.yticks([])\n    pl.tight_layout(); pl.show()\ndisplay_images(35,7,x_valid,y_valid)","3ff5492e":"model_tr3=check_model(x_train,y_train,x_valid,y_valid,x_test,y_test)","9656555b":"# Data Functions","8fedfd98":"The model trains well, but... \n\nThere are not enough data to achieve \"industrial\" classification results applicable to applications or sites.","41ba26b6":"# Model Building\nOne of the main tasks of machine learning is to quickly assess the capabilities of the constructed algorithms.\n\nLet's try to do this with some essential steps.","aa607b49":"The model works fine, but ... \n\nFor this data, it is too powerful and quickly becomes overtrained.","4f5be6bf":"# Data 1. \"Toy\" Classification\nCan the constructed model classify images in principle?\n\nThis will easily be tested on the \"game\" databases built into almost every machine learning library.","83b7610f":"# Code Modules","8ebc1f5e":"# Data 3. Massive Data\nLet's try applying the model to a more massive set of images?","ac83fc8f":"# Data 2. Real Photos\nLet's evaluate if the model is applicable to real life using a small database with photos.","b041d515":"Well done! It's exactly what we would like to see."}}