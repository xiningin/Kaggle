{"cell_type":{"5a6d37fb":"code","1bec3c1f":"code","fd5212f5":"code","57a4a55a":"code","11bbc918":"code","461c0ec6":"code","4b74ff3b":"code","de6f4b75":"code","3374c89a":"code","b400c20f":"code","ca2fee89":"code","e35353be":"code","989df87c":"code","6628c5a3":"code","312be71c":"code","c1d30431":"markdown","f6d3ead7":"markdown","b11c2137":"markdown","33ab3f7d":"markdown","9013815a":"markdown","02533477":"markdown","48fa65b7":"markdown","be571ffa":"markdown","faa1643a":"markdown","c703a74e":"markdown","d59a2bba":"markdown","df6e2de8":"markdown","c33c188c":"markdown","03d49209":"markdown","663a9ab7":"markdown","084c9938":"markdown"},"source":{"5a6d37fb":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense, Activation,Dropout\nfrom tensorflow.keras.optimizers import Adam, Adamax\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.models import Model, load_model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Ridge\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingRegressor\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow\nimport seaborn as sns\nsns.set_style('darkgrid')\nfrom IPython.core.display import display, HTML\n# stop annoying tensorflow warning messages\nimport logging\nlogging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\nimport warnings\nwarnings.filterwarnings(action='ignore')","1bec3c1f":"df= pd.read_csv(r'..\/input\/crowdedness-at-the-campus-gym\/data.csv')\nprint(df.head())\nprint(df.info())","fd5212f5":"def process(df, trsplit):\n    df=df.copy()\n    # extract information in date column using pandas to datetime\n    df['date']=pd.to_datetime(df['date'])\n    # create columns  minutes . Will not use the year data because if the model is used for predictions in the future\n    # the year would be a future year about which the model has no information \n    # not sure of significance of the timestamp column? will keep it for now but also later run model without it as it is somewhat redundant\n    df['minute']=df['date'].apply(lambda x: x.minute)\n    # drop date column as all useful information is contained in other columns     \n    df=df.drop('date', axis=1)\n    # split into X (data) and y (value to predict in model)\n    y=df['number_people']\n    X=df.drop('number_people', axis=1)\n    # split the data into train and test sets\n    X_train, X_test, y_train, y_test=train_test_split(X, y, train_size=trsplit, shuffle=True, random_state=123)\n    # scale the data using a standard scalar which makes each X column have a mean of 0 and a variance of 1\n    scaler= StandardScaler()\n    scaler.fit(X_train) # only fit the scalar to the train data\n    X_train_array=scaler.transform(X_train) # returns a numpy array - convert it back to a data frame\n    X_train=pd.DataFrame(X_train_array, index=X_train.index, columns=X_train.columns)\n    # do the same for the test data\n    X_test_array=scaler.transform(X_test) # returns a numpy array - convert it back to a data frame\n    X_test=pd.DataFrame(X_test_array, index=X_test.index, columns=X_test.columns)\n    return  X_train, X_test, y_train, y_test  ","57a4a55a":"trsplit=.8 # set percentage of data used for training or validation, .2 is used for test\nvsplit= .2  # percent of train data used for validation\nX_train, X_test, y_train, y_test=process(df, trsplit)\nprint (X_train.describe())\n# means are all near 0.0 and standard deviations are near 1.0","11bbc918":"print (X_train.shape)","461c0ec6":"Xshape= X_train.shape[1]\ninputs= tf.keras.Input(shape=Xshape,)\nx = Dense(256, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),\n                bias_regularizer=regularizers.l1(0.006) ,activation='relu')(inputs)\nx=Dropout(rate=.3, seed=123)(x)\nx=Dense(64, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),\n                bias_regularizer=regularizers.l1(0.006) ,activation='relu')(x)\noutput=Dense(1, activation='linear')(x)\nmodel=Model(inputs=inputs, outputs=output)\nmetrics=[tf.keras.metrics.RootMeanSquaredError( name=\"root_mean_squared_error\", dtype=None)]\nmodel.compile(Adamax(learning_rate=.001), loss='mse',metrics=metrics) \nmodel.summary()","4b74ff3b":"epochs=50\nrlronp=tf.keras.callbacks.ReduceLROnPlateau(  monitor=\"val_loss\", factor=0.5, patience=1, verbose=1, min_lr= 1.0e-05)\nestop=tf.keras.callbacks.EarlyStopping( monitor=\"val_loss\", patience=4,  verbose=1,restore_best_weights=True)\ncallbacks=[rlronp, estop]\nhistory=model.fit(X_train, y_train,validation_split= vsplit, batch_size=32, epochs=epochs, callbacks=callbacks )","de6f4b75":"start_epoch=0\ntr_loss=history.history['loss']\ntr_rmse=history.history['root_mean_squared_error']\nv_loss=history.history['val_loss']\nv_rmse=history.history['val_root_mean_squared_error']\nEpoch_count=len(tr_loss)\nEpochs=[]\nfor i in range (0 ,Epoch_count):\n    Epochs.append(i+1)   \nindex_loss=np.argmin(v_loss)#  this is the epoch with the lowest validation loss\nval_lowest=v_loss[index_loss]# this is the value of the lowest loss\nindex_acc=np.argmin(v_rmse)\nacc_highest=v_rmse[index_acc]\nplt.style.use('fivethirtyeight')\nsc_label='best epoch= '+ str(index_loss+1 +start_epoch)\nvc_label='best epoch= '+ str(index_acc + 1+ start_epoch)\nfig,axes=plt.subplots(nrows=1, ncols=2, figsize=(20,8))\naxes[0].plot(Epochs,tr_loss, 'r', label='Training loss')\naxes[0].plot(Epochs,v_loss,'g',label='Validation loss' )\naxes[0].scatter(index_loss+1 +start_epoch,val_lowest, s=150, c= 'blue', label=sc_label)\naxes[0].set_title('Training and Validation Loss')\naxes[0].set_xlabel('Epochs')\naxes[0].set_ylabel('Loss')\naxes[0].legend()\naxes[1].plot (Epochs,tr_rmse,'r',label= 'Training RMSE')\naxes[1].plot (Epochs,v_rmse,'g',label= 'Validation RMSE')\naxes[1].scatter(index_acc+1 +start_epoch,acc_highest, s=150, c= 'blue', label=vc_label)\naxes[1].set_title('Training and Validation RSME')\naxes[1].set_xlabel('Epochs')\naxes[1].set_ylabel('RMSE')\naxes[1].legend()\nplt.tight_layout\n#plt.style.use('fivethirtyeight')\nplt.show()","3374c89a":"def print_in_color(txt_msg,fore_tupple,back_tupple,):\n    #prints the text_msg in the foreground color specified by fore_tupple with the background specified by back_tupple \n    #text_msg is the text, fore_tupple is foregroud color tupple (r,g,b), back_tupple is background tupple (r,g,b)\n    rf,gf,bf=fore_tupple\n    rb,gb,bb=back_tupple\n    msg='{0}' + txt_msg\n    mat='\\33[38;2;' + str(rf) +';' + str(gf) + ';' + str(bf) + ';48;2;' + str(rb) + ';' +str(gb) + ';' + str(bb) +'m' \n    print(msg .format(mat), flush=True)\n    print('\\33[0m', flush=True) # returns default print color to back to black\n    return","b400c20f":"ypred=np.squeeze(model.predict(X_test))","ca2fee89":"def calc_rmse(y_test, ypred, verbose):\n    error= y_test-ypred\n    error_sq=error**2\n    mean_sq_error=np.mean(error_sq)\n    mean_error=np.sqrt(mean_sq_error)\n    msg=f'RMSE on the test set is {mean_error:6.4f} '\n    if verbose == 1:\n        print_in_color(msg, (0,255,0),(55,65,80))\n    return mean_error\n\ndef calc_Rsq(y_test, ypred, verbose):\n    sum_of_model_squared_errors=np.sum((y_test-ypred)**2)\n    sum_of_mean_squared_errors=np.sum((y_test-y_test.mean())**2)\n    R_sq=1-sum_of_model_squared_errors\/sum_of_mean_squared_errors\n    msg=f'R Squared Score on the test set is {R_sq:6.4f} '\n    if verbose == 1:\n        print_in_color(msg, (0,255,0),(55,65,80))\n    return R_sq\n    ","e35353be":"verbose=1\nRSME=calc_rmse(y_test, ypred, verbose)\nR_sq=calc_Rsq(y_test, ypred, verbose)","989df87c":"max_people=np.max(y_train)\nprint (max_people)","6628c5a3":"models = {\n    \"Linear Regression (Ridge)\": Ridge(),\n    \"                 ADABoost\": AdaBoostClassifier(),    \n    \"            Random Forest\": RandomForestRegressor()\n}\n\nfor name, model in models.items():\n    if name == 'ADABoost':\n        model.fit(X_train, y_train, n_estimators=100)\n    elif name == 'GradientBoostingRegressor':\n        model.fit(X_train, y_train)\n    else:\n        model.fit(X_train, y_train)\n    print(name + \" trained.\")\n         ","312be71c":"verbose=0\nfor name, model in models.items():\n    ypred=model.predict(X_test)\n    RSME= calc_rmse(y_test, ypred, verbose)\n    R_sq=calc_Rsq(y_test, ypred, verbose)    \n    msg=f'for model {name} RSME= {RSME:7.3f} and R Squared= {R_sq:6.4f}'\n    print_in_color(msg, (0,255,0),(55,65,80))\n    \n    ","c1d30431":"## Task: Given gym data set the task is to train a model to predict the number of people that will be in the gym at a given time","f6d3ead7":"### define an early stopping calback and a reduce learning rate on plateau callback then train model","b11c2137":"### with a max of 145 people in the gym an RMSE score of 12.96 is not a very good result\n### The maximum value of R squared score is 1, an R squared score of .6763 is OK but not great","33ab3f7d":"### all values are numeric with the exception of the data which is a string. It has information useful to a model\n### so we will extract the information. Create a processing function to process the data.","9013815a":"### the Random Forest Model produced the best results!!!!!!!!!!!","02533477":"### define function to calculate rmse and the R squared metrics","48fa65b7":"### Import needed modules - will use various model to see what works best","be571ffa":"### calculate the RMSE and R squared scores","faa1643a":"### first lets create a neural network model","c703a74e":"### what is the range of people in the gym? ","d59a2bba":"### Lets try some different models","df6e2de8":"### define a function that prints in RGB foreground and background colors","c33c188c":"### get the RMSE and R squared score for these models","03d49209":"### plot the training data","663a9ab7":"### make predictions on test set","084c9938":"### read in the csv file"}}