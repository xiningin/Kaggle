{"cell_type":{"c003bc22":"code","717bc25e":"code","44d8c832":"code","efc0f49e":"code","099e6acd":"code","8bc33676":"code","438dad9b":"code","9d77f2b2":"code","892f3aba":"code","b98acace":"code","6e1983eb":"code","47ea8f97":"code","bd7bae0c":"markdown","d0acbbc1":"markdown","af608049":"markdown","db560c6c":"markdown","0a917027":"markdown","3331bfef":"markdown","235bcd05":"markdown","5638e76a":"markdown","78b003b7":"markdown","04552e91":"markdown","929cd84e":"markdown","1c2b8fa1":"markdown","ef78896c":"markdown","1f5671b6":"markdown"},"source":{"c003bc22":"#importing the required libraries\nimport torch\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport torch.nn as nn\nimport torchvision.models as models\nimport torch.optim as optim\nfrom torchvision.utils import save_image\nimport matplotlib.pyplot as plt","717bc25e":"#Loadung the model vgg19 that will serve as the base model\nmodel=models.vgg19(pretrained=True).features\n# the vgg19 model has three components :\n    #features: containg all the conv, relu and maxpool\n    #avgpool: containing the avgpool layer\n    #classifier: contains the Dense layer(FC part of the model) ","44d8c832":"#Assigning the GPU to the variable device\ndevice=torch.device(\"cuda\")","efc0f49e":"#defing a function that will load the image and perform the required preprocessing and put it on the GPU\ndef image_loader(path):\n    image=Image.open(path)\n    #defining the image transformation steps to be performed before feeding them to the model\n    loader=transforms.Compose([transforms.Resize((512,512)),transforms.ToTensor()])\n    #The preprocessing steps involves resizing the image and then converting it to a tensor\n\n    image=loader(image).unsqueeze(0)\n    return image.to(device,torch.float)\n","099e6acd":"#Loading the original and the style image\noriginal_image=image_loader('..\/input\/image-nst\/Nikola-Tesla.jpg')\nstyle_image=image_loader('..\/input\/image-nst\/style Image.jpg')\n#Creating the generated image from the original image\ngenerated_image=original_image.clone().requires_grad_(True)","8bc33676":"#[0,5,10,19,28] are the index of the layers we will be using to calculate the loss as per the paper of NST\n#Defining a class that for the model\nclass VGG(nn.Module):\n    def __init__(self):\n        super(VGG,self).__init__()\n        #Here we will use the following layers and make an array of their indices\n        # 0: block1_conv1\n        # 5: block2_conv1\n        # 10: block3_conv1\n        # 19: block4_conv1\n        # 28: block5_conv1\n        self.req_features= ['0','5','10','19','28'] \n        #Since we need only the 5 layers in the model so we will be dropping all the rest layers from the features of the model\n        self.model=models.vgg19(pretrained=True).features[:29] #model will contain the first 29 layers\n    \n   \n    #x holds the input tensor(image) that will be feeded to each layer\n    def forward(self,x):\n        #initialize an array that wil hold the activations from the chosen layers\n        features=[]\n        #Iterate over all the layers of the mode\n        for layer_num,layer in enumerate(self.model):\n            #activation of the layer will stored in x\n            x=layer(x)\n            #appending the activation of the selected layers and return the feature array\n            if (str(layer_num) in self.req_features):\n                features.append(x)\n                \n        return features\n","438dad9b":"def calc_content_loss(gen_feat,orig_feat):\n    #calculating the content loss of each layer by calculating the MSE between the content and generated features and adding it to content loss\n    content_l=torch.mean((gen_feat-orig_feat)**2)#*0.5\n    return content_l","9d77f2b2":"def calc_style_loss(gen,style):\n    #Calculating the gram matrix for the style and the generated image\n    batch_size,channel,height,width=gen.shape\n\n    G=torch.mm(gen.view(channel,height*width),gen.view(channel,height*width).t())\n    A=torch.mm(style.view(channel,height*width),style.view(channel,height*width).t())\n        \n    #Calcultating the style loss of each layer by calculating the MSE between the gram matrix of the style image and the generated image and adding it to style loss\n    style_l=torch.mean((G-A)**2)#\/(4*channel*(height*width)**2)\n    return style_l","892f3aba":"def calculate_loss(gen_features, orig_feautes, style_featues):\n    style_loss=content_loss=0\n    for gen,cont,style in zip(gen_features,orig_feautes,style_featues):\n        #extracting the dimensions from the generated image\n        content_loss+=calc_content_loss(gen,cont)\n        style_loss+=calc_style_loss(gen,style)\n    \n    #calculating the total loss of e th epoch\n    total_loss=alpha*content_loss + beta*style_loss \n    return total_loss","b98acace":"#Load the model to the GPU\nmodel=VGG().to(device).eval() ","6e1983eb":"#initialize the paramerters required for fitting the model\nepoch=7000\nlr=0.004\nalpha=8\nbeta=70\n\n#using adam optimizer and it will update the generated image not the model parameter \noptimizer=optim.Adam([generated_image],lr=lr)\n","47ea8f97":"#iterating for 1000 times\nfor e in range (epoch):\n    #extracting the features of generated, content and the original required for calculating the loss\n    gen_features=model(generated_image)\n    orig_feautes=model(original_image)\n    style_featues=model(style_image)\n    \n    #iterating over the activation of each layer and calculate the loss and add it to the content and the style loss\n    total_loss=calculate_loss(gen_features, orig_feautes, style_featues)\n    #optimize the pixel values of the generated image and backpropagate the loss\n    optimizer.zero_grad()\n    total_loss.backward()\n    optimizer.step()\n    \n    #print the image and save it after each 100 epoch\n    if(not (e%100)):\n        print(total_loss)\n        \n        save_image(generated_image,\"gen.png\")","bd7bae0c":"Let\u2019s define a class that will provide the feature representations of the intermediate layers. Intermediate layers are used because these layers serve as a complex feature extractor. Hence these can describe the style and content of the input image. In this class, we will initialize a model by eliminating the unused layers( layers beyond conv5_1) of the vgg19 model and extract the activations or the feature representations of the \u2018conv1_1\u2019, \u2018conv2_1\u2019, \u2018conv3_1\u2019, \u2018conv4_1\u2019 and \u2018conv5_1\u2019 layers (index values [0, 5, 10, 19, 28]). Store these activations of 5 convolutional layers in an array and return the array.","d0acbbc1":"# **Neural Style Transfer Using PyTorch**","af608049":"Here, create an object for the class VGG. Initializing the object will call the constructor and it will return a model with the first 29 layers and load it to the device. Epoch is 1000, the learning rate is 0.01, alpha(weighting coefficient of content loss) is 1 and beta(weighting coefficient of style loss) is 0.01.\nAdam is used as an optimizer. The pixel data of the generated image will be optimized to pass the generated image as the optimizer parameter.\nUse a for loop to iterate over the number of epochs. Extract the feature representation of the intermediate layers of the content, style and the generated image using the model. On passing an image to the model, it will return an array of length 5. Each element corresponds to the feature representation of each intermediate layer.\nCalculate the total loss by using the above-defined functions. Set the gradients to zero with optimizer.zero_grads(), backpropagate the total loss with total_loss.backward() and then update the pixel values of the generated image using optimizer.step(). We will save the generated image after every 100 epochs and print the total loss.","db560c6c":"Neural style transfer is an artificial system based on the Deep Neural Network to generate artistic images. This approach uses two random images, the content and the style image. It extracts the structural features from the content image, whereas the style features from the style image.","0a917027":"## Importing the Libraries\nWe will begin by importing the required libraries. We will be importing torch, torchvision and PIL to implement the style transfer using PyTorch.","3331bfef":"Initialize the variables that we will need to train our model. So, before moving on to train, we need to set the hyperparameters.","235bcd05":"## Training","5638e76a":"Here, we use a pre-trained VGG19 network\u2019s convolutional neural network and perform the content and style reconstructions. By entangling the structural information from the content representation and the texture\/style information from the style representation, we generate the artistic image. We can emphasize either reconstructing the style or the content. A strong emphasis on style will result in images that match the artwork's appearance, effectively giving a texturized version of it, but hardly show any of the photograph\u2019s content. When placing a strong emphasis on content, one can identify the photograph, but the painting style is not as well-matched. We perform the gradient descent on the generated image to find another image that matches the original image's feature responses.","78b003b7":"## Get Feature Representations","04552e91":"## Load the model\nIn this case, we will load the pre-trained VGG19 model from the torchvision.models(). The vgg19 model has three components features, avgpool and classifier.\n* The feature holds all the convolutional, max pool and ReLu layers\n* avgpool holds the average pool layer.\n* Classifier holds the dense layers.\n\nWe will be using only the convolutional neural network to implement style transfer, therefore import vgg19 features. Don\u2019t forget to use GPU if available. It\u2019s going to save the training time.","929cd84e":"## Defining the Loss","1c2b8fa1":"To make an image compatible with the model, the image has to be preprocessed. Using the torch.transforms() we will perform some of the basic preprocessing that will include the following steps:\n\n* Resize: resize all the image to 512 x 512\n* ToTensor: to convert the image into a tensor\n\nYou can also Normalize the tensor with mean (0.485, 0.456, 0.406) and standard deviation of (0.229, 0.224, 0.225) for the pretrained vgg19 model. But don\u2019t forget to convert it back to its original scale. So, define a function that loads the image using the PIL library and preprocess it. Add an extra dimension at 0th index, using unsqueeze() for batch size and then load it to the device and return it.\nNow, use the image_loader function to load the style and the content image from the local disk. It\u2019s a general practice to use the content image clone as the input base image or the generated image. Since the gradient descent will alter the generated image's pixel values, we will pass the parameter true for require_grads().","ef78896c":"For complete tutorials you can read : \n[Neural Style Transfer Using PyTorch](https:\/\/amankumarmallik.medium.com\/implementing-neural-style-transfer-using-pytorch-fd8d43fb7bfahttp:\/\/)","1f5671b6":"## Image Preprocessing"}}