{"cell_type":{"ca365637":"code","3864b68a":"code","802d1273":"code","59746a38":"code","faaeb74d":"code","c350f383":"code","5473eced":"code","cbeff4f7":"code","731ea538":"code","0dff08fe":"code","60fdd7ee":"code","7bb0fd21":"code","b29f44b8":"code","2f6e0a41":"code","b283a56a":"markdown","d7b71844":"markdown","7d7a589a":"markdown"},"source":{"ca365637":"# Importing the libraries\nimport numpy as np # Used for Numerical operations on data\nimport pandas as pd # for reading input files by using CSV\/sqlite\nimport sqlite3 # to connect sqlite3 db\nimport nltk # Natural loanguage tool kit\nfrom nltk.corpus import stopwords\nimport re # regular expression used to replace\/Substitute specific words\/pattern\nfrom bs4 import BeautifulSoup # Used to replace html tags\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom gensim.models import Word2Vec\n\n\nconnection = sqlite3.connect(\"..\/input\/amazon-fine-food-reviews\/database.sqlite\") # connecting to data\n# read the data from sqlite3 db.\n# Instead of reading 568,454 records, we are reading only 5000 records for early results.\ndf = pd.read_sql_query(\"SELECT * FROM Reviews where Score != 3 LIMIT 5000\", connection) \ndf.head(5)","3864b68a":"df[\"Score\"].value_counts()\n# Data is imbalanced dataset as we have different number of categories","802d1273":"# Task is to find the review is positive or negative.\n# We are considering if the review is less than 3 it is negative and more than 3 the review is positive \ndef mappingfunction(x):\n    if x < 3:\n        return 0\n    return 1\n\ndf[\"Score\"] = df[\"Score\"].map(mappingfunction)\ndf.head(5)","59746a38":"# Sorting the data based on Product Id\nsorted_data = df.sort_values(\"ProductId\", axis=0, na_position=\"last\")\n# Remove the dulicates if the UserId, ProfileName, Time, Text have the same values.\n# Because one user can give only one review at particular time.\nfinal_df = sorted_data.drop_duplicates(subset=[\"UserId\",\"ProfileName\",\"Time\",\"Text\"])\nprint(\"After removal duplicates we have {} number of reviews\".format(final_df.shape[0]))","faaeb74d":"# removing the records if we have more number of Numerator than denominator\nfinal_df = final_df.loc[final_df[\"HelpfulnessNumerator\"] <= final_df[\"HelpfulnessDenominator\"]]\nprint(final_df.shape[0])","c350f383":"# NLP\nsent_0 = df[\"Text\"].values[0]\nprint(sent_0)\n\n# remove urls\nsentence = re.sub(r\"http\\S+\", \"\", sent_0)\n\n# remove html tags\nsoup = BeautifulSoup(sentence, \"lxml\")\nsentence = soup.get_text()\n\n\n# Expand contraction words\ndef decontracted(phrase):\n     # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\nsentence = decontracted(sentence)\n\n# remove alpha numeric words(words with numbers)\nsentence = re.sub(\"\\S*\\d\\S*\", \"\", sentence).strip()\n\n# remove special characters\nsentence = re.sub(\"[^A-Za-z0-9]+\", \" \", sentence)\n\n# remove stopwords. Basically stopwords contains not aswell. but inorder to find the polarity of the review we need the not. \n# So we have created our wen set of stopwords.\nstopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"])\n\nprint(sentence)\n","5473eced":"# Applying all the above NLP techniques to reviews\nfrom tqdm import tqdm # Creates progress bar and show how much percentage is completed\npreprocessed_reviews = []\nfor sentence in tqdm(final_df[\"Text\"].values):\n    sentence = re.sub(r\"http\\S+\", \"\", sentence)\n    sentence = BeautifulSoup(sentence, 'lxml').get_text()\n    sentence = decontracted(sentence)\n    sentence = re.sub(\"\\S*\\d\\S*\", \"\", sentence).strip()\n    sentence = re.sub('[^A-Za-z]+', ' ', sentence)\n    sentence = ' '.join(e.lower() for e in sentence.split() if e.lower() not in stopwords)\n    preprocessed_reviews.append(sentence.strip())","cbeff4f7":"# CountVectorizer: In this, We create set of unique words for all the text and \n# we fill the each cell of a review with the occurenrence of number of times of the word.\n#BOW\ncount_vect = CountVectorizer()\ncount_vect.fit(preprocessed_reviews)\nfinal_counts = count_vect.transform(preprocessed_reviews)\nprint(final_counts.shape)","731ea538":"# Basically Countvectorizer creates each dimension by taking each word into consideration and it is called uni-gram.\n# Bi-gram: it takes 2 pair of words for creating a each dimension.\n# if we create Countvectorizer by using ngram_range=(1,2) means: it creates the dimensions for both Unigram & bi-gram.\n# ngram_range=(2,2) means: it creates dimension by using bi-grams only.\ncount_vect = CountVectorizer(ngram_range=(1,2), min_df=10, max_features=5000)\ncount_vect.fit(preprocessed_reviews)\nfinal_bigram_counts = count_vect.transform(preprocessed_reviews)\nprint(final_bigram_counts.shape)\nprint(\"After Count Vectorizer, we have {0} rows and {1} columns\".format(final_bigram_counts.shape[0], final_bigram_counts.shape[0]))\nprint(\"We have {0} unique words\".format(final_bigram_counts.shape[0]))","0dff08fe":"# TfidfVectorizer\n# ---------------\n# TF(Term-Frequency): Number of times of occurences of word\/ total number of word\n# IDF(Inverse Document Frequency): logarthemic(Total Number of reviews\/Number of reviews in which the word contains)\n\ntf_idf_vect = TfidfVectorizer()\ntf_idf_vect.fit(preprocessed_reviews)\nfinal_tf_idf_vect = tf_idf_vect.transform(preprocessed_reviews)\nprint(final_tf_idf_vect.shape)","60fdd7ee":"# Both Count Vectorizer and TfIdf Vectorizer creates the vector by considering occurences of the word \n# and it doesnot take semantic meaning into consideration.\n\n# Word2Vec creates the d-dimenstional vector by considering semantic meaning.\n# We used google genism model provided by Google for creating the 50-dimensional vector for each word.\n\n\n# Implementation of word2 vector\n# W2V takes list of lists into consideration for creating word2vector model. \n# So we split the reviews into list of lists(words) in below step\nlist_of_sentence = []\nfor sentence in preprocessed_reviews:\n    list_of_sentence.append(sentence.split())\n\n#print(list_of_sentence[0])\n\nw2v_model = Word2Vec(list_of_sentence, min_count=5, size=50, workers=8) \n# min_count=5: To create the vector the word should appear atleast 5 times inthe document.\n# size=50: Creates 50 diensional vector.\n\n\nprint(w2v_model.wv.most_similar(\"great\"))\n\n#print(list(w2v_model.wv.vocab))\n","7bb0fd21":"#Avg-W2V\n\n# Avg-W2V is sum of all the word 2 vectos for each word divided by number of words in the review\nw2v_words = list(w2v_model.wv.vocab)\navg_w2v = []\nfor sentence in tqdm(list_of_sentence):\n    sent_vec = np.zeros(50)\n    cnt_words = 0\n    for word in sentence:\n        if word in w2v_words:\n            word_vec = w2v_model.wv[word]\n            sent_vec += word_vec\n            cnt_words+=1\n    if cnt_words != 0:\n        word_vec = word_vec\/len(sentence)\n        avg_w2v.append(word_vec)\n    \n\nprint(len(avg_w2v))\n","b29f44b8":"# TFIDF weighted W2v\n\n# TFIDF weighted W2v: sum of product of tf-idf and w2v divided by sum of tf-idf\n# formulae: (sum(tf-idf * w2v))\/(sum(tf-idf))\nmodel = TfidfVectorizer()\nmodel.fit(preprocessed_reviews)\n\n# Create dictionay with key: word and vlue: idf\ntf_idf_dict = dict(zip(model.get_feature_names(), model.idf_))\n\n# print(\"IDF's for the words: \", tf_idf_dict)","2f6e0a41":"# Implementation\ntf_idf_weighted_w2v = []\ntfidf_feat = model.get_feature_names()\nfor sentence in tqdm(list_of_sentence):\n    weight_sum = 0\n    tf_idf_w2v = np.zeros(50)\n    for word in sentence:\n        if word in w2v_words and word in tfidf_feat:\n            tf = sentence.count(word)\/len(sentence)\n            tf_idf = tf*tf_idf_dict[word]\n            w2v = w2v_model.wv[word]\n            tf_idf_w2v += (tf_idf*w2v)\n            weight_sum += tf_idf\n    if weight_sum != 0:\n        tf_idf_w2v \/= weight_sum\n    tf_idf_weighted_w2v.append(tf_idf_w2v)\n    ","b283a56a":"**BOW(Bag of Word Techniques)**","d7b71844":"<html><h2>Context<\/h2>\nThis dataset consists of reviews of fine foods from amazon. The data span a period of more than 10 years, including all ~500,000 reviews up to October 2012. Reviews include product and user information, ratings, and a plain text review. It also includes reviews from all other Amazon categories.\n<br><br>\n  <h2>Contents<\/h2>\n    <ul>\n        <li>Reviews.csv: Pulled from the corresponding SQLite table named Reviews in database.sqlite<\/li>\n        <li> database.sqlite: Contains the table 'Reviews'<\/li>\n    <\/ul>\n\n \n\n<h2>Data includes:<\/h2>\n<ul>\n    <li>Reviews from Oct 1999 - Oct 2012<\/li>\n    <li>568,454 reviews<\/li>\n    <li>256,059 users<\/li>\n    <li>74,258 products<\/li>\n    <li>260 users with > 50 reviews<\/li>\n\n<\/ul>\n\n<h2>Notebook Includes:<\/h2>\n<ul>\n    <li>Deduplication of reviews<\/li>\n    <li>NLP Technques<\/li>\n    <ul>\n        <li>Removal of urls in the reviews<\/li>\n        <li>Removal of html tags<\/li>\n        <li>Removal of alphanumeric characters<\/li>\n        <li>Removal of special characters<\/li>\n        <li>Removal of Stop words<\/li>\n    <\/ul>\n    <li>Converint Text to Word <\/li>\n    <ul>\n        <li>CountVectorizer<\/li>\n        <li>n-grams(uni-gram, bi-grams)<\/li>\n        <li>TfidVectorizer<\/li>\n        <li>Word2Vec<\/li>\n        <li>Average Word2Vec<\/li>\n        <li>Tfid weighted Word2Vec<\/li>\n    <\/ul>\n<\/ul><\/html>\n","7d7a589a":"### Bi-grams & n-grams"}}