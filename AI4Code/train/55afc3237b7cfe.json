{"cell_type":{"4ce08896":"code","28df51e1":"code","b9ea9eaf":"code","82e2370b":"code","dbafd210":"code","28534773":"code","ef84e354":"code","3a3edefc":"code","94f72c4b":"code","0b51f88c":"code","33520c06":"code","bdaa8f5c":"code","e686c34e":"code","067d977c":"code","6cee7852":"code","74962744":"code","7244c4a8":"code","a9fc4cd7":"code","06cc45b2":"code","63957397":"code","bddf975c":"code","e4dacb08":"code","61cd1603":"code","37ecc26a":"code","ab57c99f":"code","9a0142da":"code","05654295":"code","a08fdb12":"code","930469ba":"code","dd24c7e5":"code","a1764271":"code","5e53ed1f":"code","0b568491":"code","5c29e58b":"code","eb35ad62":"markdown","7fdc002c":"markdown","aaca60d8":"markdown","e7a85f65":"markdown","cdb40a5d":"markdown","57508f1a":"markdown","ac309bc9":"markdown","ec555e83":"markdown","c481da5a":"markdown","82e146f4":"markdown","b5dee2b2":"markdown","71ac35df":"markdown","11e4c0c9":"markdown","4a236556":"markdown","cd276746":"markdown","8289b05c":"markdown","6d09ea38":"markdown","acb836b1":"markdown","fab1b282":"markdown"},"source":{"4ce08896":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re, unicodedata, string\nimport pandas as pd\nimport tqdm\nfrom tqdm.auto import tqdm as tqdmp\ntqdmp.pandas()\n\nimport gensim\nfrom gensim.corpora.dictionary import Dictionary\nfrom gensim.models.tfidfmodel import TfidfModel\nfrom gensim.matutils import corpus2csc\n\nfrom cuml.neighbors import NearestNeighbors\n\nfrom joblib import dump, load\n\n# ignoring warnings\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\n# Some settings for visualizations\nsns.set(rc={'axes.facecolor':'black', 'figure.facecolor':'black', \n            'xtick.color': 'white', 'ytick.color': 'white', \n            'grid.color': 'white', 'axes.labelcolor': 'white',\n            'figure.dpi': 150, 'grid.linestyle': ':', 'grid.alpha': .6,\n            'font.family': 'fantasy'})","28df51e1":"all_data = pd.read_csv('..\/input\/reddit-data-science-posts\/reddit_database.csv')\nall_data['created_date'] = all_data['created_date'].astype('datetime64')\nall_data.head()","b9ea9eaf":"def text_cleaner(text):\n    \"\"\"\n    Function for cleaning text data from unnecessary characters.\n    \"\"\"\n    text = text.lower()\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    text = re.sub('\\[.*?\\]', ' ', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', ' ', text)\n    text = re.sub('\\r', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\ndef knn_viz(rows, k_neighbors = 15):\n    for i in rows:\n        plt.figure(figsize = (20, 3))\n        plt.plot(np.arange(k_neighbors), \n                 distances[i, :k_neighbors], 'o--', \n                 color = 'red', linewidth = 3, markersize = 12)\n        plt.title('Text Distance From Row %i to Other Rows'%i, \n                  size = 16, color = 'white')\n        plt.ylabel('Distance to Row %i'%i, size = 14)\n        plt.xlabel('Index Sorted by Distance to Row %i'%i, size = 14)\n        plt.show()\n\n        print(all_data.loc[indices[i, :k_neighbors], 'title'])\n        print()","82e2370b":"post_cleaned = all_data['title'].dropna().reset_index(drop = True)\npost_cleaned = post_cleaned.progress_apply(lambda x: text_cleaner(x).split())\npost_cleaned","dbafd210":"test_posts = post_cleaned[2:4].copy()\ntest_posts.iloc[0] = ['hi'] + test_posts.iloc[0]\ntest_posts.iloc[1] = ['great'] + test_posts.iloc[1]\ntest_posts","28534773":"dictionary = Dictionary(post_cleaned)\nnum_docs = dictionary.num_docs\nnum_terms = len(dictionary.keys())","ef84e354":"# the bag of words corpus\ncorpus_bow = [dictionary.doc2bow(doc) for doc in post_cleaned]","3a3edefc":"tfidf = TfidfModel(corpus_bow)\ncorpus_tfidf = tfidf[corpus_bow]","94f72c4b":"corpus_tfidf_sparse = corpus2csc(corpus_tfidf, num_terms, num_docs=num_docs).T\ncorpus_tfidf_sparse","0b51f88c":"model = NearestNeighbors(n_neighbors = 5)\nmodel.fit(corpus_tfidf_sparse)\n\ndistances, indices = model.kneighbors(corpus_tfidf_sparse)","33520c06":"knn_viz(range(5), 5)","bdaa8f5c":"# test corpus from created dictionary\ntest_corpus_bow = [dictionary.doc2bow(doc) for doc in test_posts]\n\n# test tfidf values from created tfidf model\ntest_corpus_tfidf = tfidf[test_corpus_bow]\n\n# test sparse matrix\ntest_corpus_tfidf_sparse = corpus2csc(test_corpus_tfidf, num_terms).T\ntest_corpus_tfidf_sparse","e686c34e":"distances, indices = model.kneighbors(test_corpus_tfidf_sparse)\nknn_viz(range(2), 5)","067d977c":"dictionary.save('TFIDF_dict_title')\ntfidf.save('TFIDF_title')\ndump(model, 'knn_title.model')","6cee7852":"from sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=5, random_state=0)\nkmeans.fit(corpus_tfidf_sparse)\n\nall_data['title_cluster'] = pd.Series(index=all_data['title'].dropna().index, \n                                      data=kmeans.labels_)","74962744":"sns.countplot(all_data['title_cluster']);","7244c4a8":"all_data['title'][all_data['title_cluster']==0].head(10)","a9fc4cd7":"all_data['title'][all_data['title_cluster']==4].head(10)","06cc45b2":"post_cleaned = all_data['post'].dropna().reset_index(drop = True)\npost_cleaned = post_cleaned.progress_apply(lambda x: text_cleaner(x).split())\npost_cleaned","63957397":"test_posts = post_cleaned[:2].copy()\ntest_posts.iloc[0] = test_posts.iloc[0][6:]\ntest_posts.iloc[1] = test_posts.iloc[1][6:]\ntest_posts","bddf975c":"dictionary = Dictionary(post_cleaned)\nnum_docs = dictionary.num_docs\nnum_terms = len(dictionary.keys())","e4dacb08":"# the bag of words corpus\ncorpus_bow = [dictionary.doc2bow(doc) for doc in post_cleaned]\n\n# tfidf\ntfidf = TfidfModel(corpus_bow)\ncorpus_tfidf = tfidf[corpus_bow]\n\n# sparse matrix\ncorpus_tfidf_sparse = corpus2csc(corpus_tfidf, num_terms, num_docs=num_docs).T\ncorpus_tfidf_sparse","61cd1603":"model = NearestNeighbors(n_neighbors = 5)\nmodel.fit(corpus_tfidf_sparse)\n\ndistances, indices = model.kneighbors(corpus_tfidf_sparse)","37ecc26a":"knn_viz(range(5), 5)","ab57c99f":"# test corpus from created dictionary\ntest_corpus_bow = [dictionary.doc2bow(doc) for doc in test_posts]\n\n# test tfidf values from created tfidf model\ntest_corpus_tfidf = tfidf[test_corpus_bow]\n\n# test sparse matrix\ntest_corpus_tfidf_sparse = corpus2csc(test_corpus_tfidf, num_terms).T\ntest_corpus_tfidf_sparse","9a0142da":"distances, indices = model.kneighbors(test_corpus_tfidf_sparse)\nknn_viz(range(2), 5)","05654295":"kmeans = KMeans(n_clusters=5, random_state=0)\nkmeans.fit(corpus_tfidf_sparse)\n\nall_data['post_cluster'] = pd.Series(index=all_data['post'].dropna().index, \n                                     data=kmeans.labels_)","a08fdb12":"sns.countplot(all_data['post_cluster']);","930469ba":"all_data['post'][all_data['post_cluster']==2].head(10)","dd24c7e5":"all_data['post'][all_data['post_cluster']==3].head(10)","a1764271":"dictionary.save('TFIDF_dict_text')\ntfidf.save('TFIDF_text')\ndump(model, 'knn_text.model')","5e53ed1f":"def similarity_analyzer(data, k_nearest = 5,\n                        post_object = 'title'):\n    assert post_object in {'title', 'text'}\n    if k_nearest > 5:\n        raise ValueError('\"k_nearest\" greater than 5!')\n        \n    if post_object == 'title':\n        dictionary = Dictionary.load('.\/TFIDF_dict_title')\n        tfidf = TfidfModel.load('.\/TFIDF_title')\n        model = load('.\/knn_title.model')\n        num_terms = len(dictionary.keys())\n    elif post_object == 'text':\n        dictionary = Dictionary.load('.\/TFIDF_dict_text')\n        tfidf = TfidfModel.load('.\/TFIDF_text')\n        model = load('.\/knn_text.model')\n        num_terms = len(dictionary.keys())\n    \n    data_tokenized = data.progress_apply(lambda x: text_cleaner(x).split())\n    \n    corpus_bow = [dictionary.doc2bow(doc) for doc in data_tokenized]\n    corpus_tfidf = tfidf[corpus_bow]\n    corpus_tfidf_sparse = corpus2csc(corpus_tfidf, num_terms).T\n    \n    distances, indices = model.kneighbors(corpus_tfidf_sparse)\n    return distances[:, :k_nearest], indices[:, :k_nearest]","0b568491":"K = 5\ntest_posts = all_data['post'].dropna().reset_index(drop = True)[:2]\ndistances, indices = similarity_analyzer(test_posts, \n                                         k_nearest = K,\n                                         post_object = 'text')\nknn_viz(range(2), K)","5c29e58b":"K = 3\ntest_posts = all_data['title'].dropna().reset_index(drop = True)[2:4]\ndistances, indices = similarity_analyzer(test_posts, \n                                         k_nearest = K,\n                                         post_object = 'title')\nknn_viz(range(2), K)","eb35ad62":"Our matrix has the shape of 527646x101304. As mentioned, this is too much for a dense matrix.\n\n## RAPIDS KNN\n\nLet's fit our RAPIDS KNN model to find 5 nearest neighbors and then visualize the results with the familiar function.","7fdc002c":"Finally, we create a sparse matrix with TF-IDF values.\n\nIt needs to be transposed to bring it to the form of posts*size of the dictionary","aaca60d8":"The similarity of entire posts is a bit difficult to interpret at a glance without detailed comparison, but everything seems to work as it should. The algorithm found posts from which test samples were created with a very small distance.\n\nAlso, based on the sparse matrix of tfidf values, we can cluster similar posts using any of the algorithms presented in sklearn - KMeans, DBSCAN, etc. A small example.","e7a85f65":"Fine! Let's save the necessary objects and start creating our analyzer.","cdb40a5d":"## RAPIDS KNN","57508f1a":"<h1 style='color:white; background:black; border:0'><center>Post Similarity Analyzer (GENSIM TF-IDF + RAPIDS KNN)<\/center><\/h1>\n\n![](https:\/\/storage.googleapis.com\/kaggle-datasets-images\/1226967\/2048152\/b7b21a6c2113257ec9ddf7221abfed1a\/dataset-cover.png?t=2021-03-23-12-43-24)\n\n<h3 style='color:white; background:black; border:0'><center>This dataset includes over 500,000 posts from 19 Date Science subreddits:<\/center><\/h3>\n\n[r\/analytics](https:\/\/www.reddit.com\/r\/analytics\/), [r\/deeplearning](https:\/\/www.reddit.com\/r\/deeplearning\/), [r\/datascience](https:\/\/www.reddit.com\/r\/datascience\/), [r\/datasets](https:\/\/www.reddit.com\/r\/datasets\/), [r\/kaggle](https:\/\/www.reddit.com\/r\/kaggle\/), [r\/learnmachinelearning](https:\/\/www.reddit.com\/r\/learnmachinelearning\/), [r\/MachineLearning](https:\/\/www.reddit.com\/r\/MachineLearning\/), [r\/statistics](https:\/\/www.reddit.com\/r\/statistics\/), [r\/artificial](https:\/\/www.reddit.com\/r\/artificial\/), [r\/AskStatistics](https:\/\/www.reddit.com\/r\/AskStatistics\/), [r\/computerscience](https:\/\/www.reddit.com\/r\/computerscience\/), [r\/computervision](https:\/\/www.reddit.com\/r\/computervision\/), [r\/dataanalysis](https:\/\/www.reddit.com\/r\/dataanalysis\/), [r\/dataengineering](https:\/\/www.reddit.com\/r\/dataengineering\/), [r\/DataScienceJobs](https:\/\/www.reddit.com\/r\/DataScienceJobs\/), [r\/datascienceproject](https:\/\/www.reddit.com\/r\/datascienceproject\/), [r\/data](https:\/\/www.reddit.com\/r\/data\/), [r\/MLQuestions](https:\/\/www.reddit.com\/r\/MLQuestions\/), [r\/rstats](https:\/\/www.reddit.com\/r\/rstats\/)\n\nData were collected from [pushshift.io API](https:\/\/pushshift.io) (maintained by Jason Baumgartner).\n\n<h3 style='color:white; background:black; border:0'><center>19 datasets (one per one subreddit) include the following data:<\/center><\/h3>\n\n`#` - row index;\n`created_date` - post publication date;\n`created_timestamp` - post publication timestamp;\n`subreddit` - subreddit name;\n`title` - post title;\n`id` - unique operation id;\n`author` - post author;\n`author_created_utc` - author registration date;\n`full_link` - hyperlink to post;\n`score` - ratio of likes and dislikes;\n`num_comments` - the number of comments;\n`num_crossposts` - the number of crossposts; \n`subreddit_subscribers` - the number of subreddit subscribers at the time the post was published;\n`post` - post text.\n\n**If you are interested, see**\n- [this dataset (Reddit Data Science Posts)](https:\/\/www.kaggle.com\/maksymshkliarevskyi\/reddit-data-science-posts) \n- [first notebook (How is Data Science on Reddit? (EDA, RAPIDS))](https:\/\/www.kaggle.com\/maksymshkliarevskyi\/how-is-data-science-on-reddit-eda-rapids#RAPIDS-and-post-title).\n\nFeel free to leave your comments on this notebook. I will try to make dataset better and much larger.\n\n<h2 style='color:white; background:black; border:0'><center>Work Motivation<\/center><\/h2>\n\nThe first thing that comes to mind (to me personally) when looking at a fairly large database with posts is checking their similarity. We already tried to do this in the first notebook ([How is Data Science on Reddit? (EDA, RAPIDS)](https:\/\/www.kaggle.com\/maksymshkliarevskyi\/how-is-data-science-on-reddit-eda-rapids#RAPIDS-and-post-title), but we ran into the problem of insufficient memory when training the RAPIDS KNN model. Not surprisingly, because we used a dense matrix, which takes up a LOT of memory. Instead, it makes sense to use a sparse matrix. Since sklearn knn does not support GPU computation, we will again use RAPIDS to speed up computations.\n\nGreat, 500,000 posts shouldn't be a problem for this approach. But just looking at the similarities is interesting, but not entirely practical. A more professional approach is to create an analyzer of new posts that will find posts in the database that are most similar to them.\n\nThis notebook will be dedicated to the solution of this problem.","ac309bc9":"# Necessary functions","ec555e83":"## TF-IDF Gensim\n\nFirst, we need to create a dictionary, which will then be used to create the bag of words corpus.","c481da5a":"# POST TITLES\n\n## Data preprocessing\n\nWe will not do EDA, since it was already in the [first notebook](https:\/\/www.kaggle.com\/maksymshkliarevskyi\/how-is-data-science-on-reddit-eda-rapids#RAPIDS-and-post-title), and therefore we will immediately focus on completing the task.\n\nLet's prepare our data for further work.","82e146f4":"Fine! We see the same result as in the first notebook, only now on all data. Let's try to find similar posts for our test data.","b5dee2b2":"# POST SIMILARITY ANALYZER","71ac35df":"Great! It looks like everything is working as it should. Let's repeat the same for the posts themselves.\n\nBut before that, let's save the dictionary, tfidf model, and knn model. We still need them.","11e4c0c9":"Excellent! We have written an analyzer that checks the uniqueness of the title or text of a new post against the existing database.\n\nThis is a fairly simple, not the best, but powerful approach. The analyzer can be extended by adding additional parameters, for example, the similarity threshold, or can be rewritten to return the text itself, rather than the distance values and indices. The space for imagination is huge!\n\nThis is not the end of my work. In the future, I will try to improve the results, for example by better data cleaning, or by using other algorithms. Before that I experimented with `word2vec` and `doc2vec` models, and, to be honest, I am a little disappointed with their instability and poor results for this task. Maybe I was doing something wrong. I would be grateful if someone proves that I am wrong and posts good working `word2vec` and `doc2vec` models for this dataset.\n\nIf you enjoyed this work, please leave your opinion in the comments or upvoted. \nAlso, If you are interested, see\n- [this dataset (Reddit Data Science Posts)](https:\/\/www.kaggle.com\/maksymshkliarevskyi\/reddit-data-science-posts) \n- [first notebook (How is Data Science on Reddit? (EDA, RAPIDS))](https:\/\/www.kaggle.com\/maksymshkliarevskyi\/how-is-data-science-on-reddit-eda-rapids#RAPIDS-and-post-title).\n\n**Good luck!**\n\n<h2 style='color:white; background:black; border:0'><center>Work in progress...<\/center><\/h2>","4a236556":"And we will immediately generate several test titles that are slightly different from the original ones.","cd276746":"Also, based on the sparse matrix of tfidf values, we can cluster similar post titles using any of the algorithms presented in sklearn - KMeans, DBSCAN, etc. A small example.","8289b05c":"## TF-IDF Gensim","6d09ea38":"Now the TF-IDF model itself. First, we train it, and then we extract the vector representations for all posts.","acb836b1":"# POST TEXTS\n\n## Data preprocessing","fab1b282":"# Load Libraries and Data"}}