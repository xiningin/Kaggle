{"cell_type":{"4ba9de3c":"code","2fdc06ca":"code","417c4bc6":"code","dcb78228":"code","36ca7c3c":"code","f6c7baf1":"code","dc4b251a":"code","a8488fc2":"code","94d9702d":"code","a8b62c39":"code","f676bf55":"code","26893199":"code","b1f35e26":"code","08dc2181":"code","79ec91d3":"code","95f0c339":"code","4b686a62":"code","de3a9239":"code","1cdbc028":"code","a16e5083":"code","df21676b":"code","cff451c0":"code","91b83f6c":"code","234bda9f":"code","8c4905e9":"code","1aac7c31":"code","d6fc49fa":"code","0eb78678":"code","b4a13bab":"code","9503b832":"code","b9992566":"code","d6d93138":"code","cf6c58ab":"code","3310afdc":"code","5b82974b":"code","8cf7fae8":"code","d85ab59e":"code","a567151e":"code","1672f84d":"code","ec4ad5be":"code","fa854310":"code","066b6992":"code","41f629da":"code","80bf3ecc":"code","ac8fafad":"code","998c8983":"code","49b60664":"code","47fc109a":"code","45f4d1c8":"code","9c3829a6":"code","3c981a6a":"code","04657ae1":"code","0c2f9367":"code","997f65fa":"code","9b94551f":"code","7c61a5c8":"code","ba8fc233":"code","55cf56f6":"code","657fd659":"code","9b7a51bc":"code","1cfe4255":"code","1c01ab4a":"code","679843d3":"code","1ef1d0c7":"code","69a1be58":"code","05920ec5":"code","4484f44c":"markdown","a4c49d5a":"markdown","4647093c":"markdown","10dc633b":"markdown","f2dc3066":"markdown","6876ec70":"markdown","b157870a":"markdown","b216dbc7":"markdown","61539595":"markdown","72c2f791":"markdown","8fb0d244":"markdown","86b5896e":"markdown","88a844e5":"markdown","a3670da8":"markdown","ad008911":"markdown","375b7c9b":"markdown","6cbf6ac0":"markdown","20f3da00":"markdown","29cfa1fd":"markdown","86767e98":"markdown","f32b3e98":"markdown","2bc93631":"markdown"},"source":{"4ba9de3c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport re\nimport missingno as msno\n\n# to avoid warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.warn(\"this will not show\")\n\n# sns.set(style='darkgrid')\n# %matplotlib inline","2fdc06ca":"data = pd.read_csv('..\/input\/weather-dataset-rattle-package\/weatherAUS.csv')\ndf=data.copy()\ndf.head()","417c4bc6":"df.shape","dcb78228":"def null_values(df):\n    \"\"\"a function to show null values with percentage\"\"\"\n    nv=pd.concat([df.isnull().sum(), 100 * df.isnull().sum()\/df.shape[0]],axis=1).rename(columns={0:'Missing_Records', 1:'Percentage (%)'})\n    return nv[nv.Missing_Records>0].sort_values('Missing_Records', ascending=False)\nnull_values(df)","36ca7c3c":"# import missingno as msno\n# msno.matrix(df);","f6c7baf1":"# msno.bar(df);","dc4b251a":"# msno.heatmap(df);","a8488fc2":"df.dropna(subset=['RainToday', 'RainTomorrow'], inplace=True)\ndf[['RainToday', 'RainTomorrow']].isnull().sum()","94d9702d":"def summary(df, pred=None):\n    obs = df.shape[0]\n    Types = df.dtypes\n    Counts = df.apply(lambda x: x.count())\n    Min = df.min()\n    Max = df.max()\n    Uniques = df.apply(lambda x: x.unique().shape[0])\n    Nulls = df.apply(lambda x: x.isnull().sum())\n    print('Data shape:', df.shape)\n\n    if pred is None:\n        cols = ['Types', 'Counts', 'Uniques', 'Nulls', 'Min', 'Max']\n        str = pd.concat([Types, Counts, Uniques, Nulls, Min, Max], axis = 1, sort=True)\n\n    str.columns = cols\n    print('___________________________\\nData Types:')\n    print(str.Types.value_counts())\n    print('___________________________')\n    display(str.sort_values(by='Nulls', ascending=False))\n\nsummary(df)","a8b62c39":"df[['RainToday','RainTomorrow']] = df[['RainToday','RainTomorrow']].replace({'Yes':1, 'No':0})","f676bf55":"df = df.dropna()","26893199":"df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')\n\ndf['Year'] = df['Date'].dt.year.astype('int16')\ndf['Month'] = df['Date'].dt.month.astype('int16')\ndf['Year_Month']=df['Date'].array.strftime('%Y-%m') # strftime works with array\ndf['DiffTemp']=df['MaxTemp']-df['MinTemp']\ndf.head()","b1f35e26":"# for all variables \nplt.figure(figsize=(20,10))\nsns.heatmap(df.corr(),annot=True, cmap=\"coolwarm\");","08dc2181":"plt.figure(figsize=(10,6))\ndf.corr()[\"RainTomorrow\"].sort_values().plot.barh();","79ec91d3":"def corrank(X, threshold=0):\n    import itertools\n    df = pd.DataFrame([[i,j,X.corr().abs().loc[i,j]] for i,j in list(itertools.combinations(X.corr().abs(), 2))],columns=['Feature1','Feature2','corr'])    \n    df = df.sort_values(by='corr',ascending=False).reset_index(drop=True)\n    return df[df['corr']>threshold]\n\n# prints a descending list of correlation pair (Max on top)\ncorrank(df, 0.7)","95f0c339":"# df.groupby('Location')['RainTomorrow','RainToday'].agg(['mean'])","4b686a62":"drop_list=[\n            'Date', \n#             'Location', \n#             'MinTemp', \n#            'MaxTemp', 'Rainfall', \n#            'Evaporation',\n#            'Sunshine', \n#            'WindGustDir', \n#            'WindGustSpeed', \n#            'WindDir9am', 'WindDir3pm',\n#            'WindSpeed9am', 'WindSpeed3pm', \n#            'Humidity9am',\n#            'Humidity3pm',\n#            'Pressure9am', \n#            'Pressure3pm', \n#            'Cloud9am', \n#            'Cloud3pm', \n#            'Temp9am','Temp3pm', \n#            'RainToday', \n#            'RainTomorrow', \n           'Year', 'Year_Month',\n#             'Month',\n#        'DiffTemp',\n          ]","de3a9239":"df = df.drop(drop_list, axis=1)","1cdbc028":"df.isnull().any().sum()","a16e5083":"df.columns","df21676b":"df = pd.get_dummies(df, drop_first=True, columns = ['Location','WindGustDir','WindDir9am','WindDir3pm'])","cff451c0":"summary(df)","91b83f6c":"from sklearn.ensemble import IsolationForest\nfrom sklearn.model_selection import train_test_split\n\ndf_isolation_forest = df.copy()\n\ny_iso = df_isolation_forest['RainTomorrow']\nX_iso = df_isolation_forest.drop(['RainTomorrow'], axis=1)\n\nclf = IsolationForest(n_estimators=100, max_samples='auto', contamination=0.10, random_state=42)\nclf.fit(X_iso)","234bda9f":"outliers_values = X_iso[clf.predict(X_iso) == -1]\nf\"{len(outliers_values)} rows are outliers\"","8c4905e9":"df_noOutlier = X_iso.join(y_iso)[clf.predict(X_iso) == 1]\ndf_noOutlier","1aac7c31":"summary(df_noOutlier)","d6fc49fa":"df_noOutlier = df_noOutlier.reset_index().drop(['index'], axis=1)","0eb78678":"df_noOutlier.to_csv('weather_cleandata.csv')","b4a13bab":"df=pd.read_csv('weather_cleandata.csv',index_col=[0])\ndf.head()","9503b832":"# for basic operations\nimport numpy as np \nimport pandas as pd \n\n# for visualizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pylab import rcParams\n# rcParams['figure.figsize'] = 4,4\n# plt.style.use('fivethirtyeight')\n\nfrom collections import Counter\n\n# for modeling \nimport sklearn\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report, plot_precision_recall_curve, precision_recall_curve\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score, train_test_split, KFold\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import datasets, metrics\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\n\nimport imblearn\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import SMOTE\n\n# to avoid warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.warn(\"this will not show\")","b9992566":"# separating the dependent and independent data\nX=df.drop([\"RainTomorrow\"], axis=1)\ny=df[\"RainTomorrow\"]\n\n# the function train_test_split creates random data samples (default: 75-25%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state =42)\n\n# getting the shapes\nprint(f\"\"\"shape of X_train: {X_train.shape}\nshape of X_test\\t: {X_test.shape}\nshape of y_train: {y_train.shape}\nshape of y_test\\t: {y_test.shape}\"\"\")","d6d93138":"ax = df['RainTomorrow'].value_counts(normalize=True).plot.bar(color=[\"blue\", \"red\"])\ndef labels(ax):\n    for p in ax.patches:\n        ax.annotate(f\"%{p.get_height()*100:.2f}\", (p.get_x() + 0.15, p.get_height() * 1.005),size=11)\nlabels(ax)","cf6c58ab":"# # pip install imblearn\n# from imblearn import under_sampling, over_sampling\n# from imblearn.over_sampling import SMOTE","3310afdc":"# oversmote = SMOTE()\n# X_train, y_train= oversmote.fit_resample(X_train, y_train)","5b82974b":"# ax = y_train.value_counts().plot.bar(color=[\"blue\", \"red\"])\n# def labels(ax):\n#     for p in ax.patches:\n#         ax.annotate(f\"{p.get_height()}\", (p.get_x() + 0.15, p.get_height()+200),size=8)\n# labels(ax)\n# plt.show()","8cf7fae8":"# X_train.shape","d85ab59e":"# creating a minmax scaler\nsc = MinMaxScaler()\n\n# fitting independent data to the model\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","a567151e":"cv_acc_train = {}\ncv_acc_test = {}\ncv_TPR = {}\ncv_FPR = {}\ncv_AUC = {}","1672f84d":"def plot_result(model, name:str):\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n\n    # Evaluation based on a 10-fold cross-validation\n    scoring = ['balanced_accuracy', 'recall_macro']\n    scores_train = cross_val_score(model, X_train, y_train, cv=10, scoring = 'balanced_accuracy')\n    scores_test = cross_val_score(model, X_test, y_test, cv=10, scoring = 'balanced_accuracy')  \n    cv_acc_train[name] = round(scores_train.mean(), 4)*100  # balanced accuracy\n    cv_acc_test[name] = round(scores_test.mean(), 4)*100  # balanced accuracy\n    cv_TPR[name] = (confusion_matrix(y_test, y_pred)[1][1]\/confusion_matrix(y_test, y_pred)[1].sum())*100  # recall (Max)\n    cv_FPR[name] = (confusion_matrix(y_test, y_pred)[0][1]\/confusion_matrix(y_test, y_pred)[0].sum())*100  # fallout (Min)\n    \n    # accuracy scores\n    print('Average Balanced Accuracy (CV=10), Test Set:', scores_test.mean())  \n    print('Average Balanced Accuracy (CV=10), Training Set: ', scores_train.mean())\n\n    # print classification report\n    print(classification_report(y_test, y_pred, zero_division=0))\n\n    # Plot Confusion Matrix\n    plot_confusion_matrix(model, X_test, y_test)\n    plt.show()","ec4ad5be":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import plot_confusion_matrix, classification_report, confusion_matrix","fa854310":"# lr = LogisticRegression()\n# plot_result(lr, \"lr\")","066b6992":"from sklearn.naive_bayes import GaussianNB","41f629da":"nb = GaussianNB()\nplot_result(nb, \"nb\")","80bf3ecc":"from sklearn.neighbors import KNeighborsClassifier","ac8fafad":"# knn = KNeighborsClassifier()\n# plot_result(knn, \"knn\")","998c8983":"# svc = SVC(probability=True)  # default values\n# plot_result(svc, \"svc\")","49b60664":"from sklearn.tree import DecisionTreeClassifier, plot_tree","47fc109a":"dtc = DecisionTreeClassifier()\nplot_result(dtc, \"dtc\")","45f4d1c8":"from sklearn.neighbors import NearestCentroid","9c3829a6":"nc = NearestCentroid()\nplot_result(nc, \"nc\")","3c981a6a":"from sklearn.ensemble import RandomForestClassifier","04657ae1":"rfc = RandomForestClassifier()\nplot_result(rfc, \"rfc\")","0c2f9367":"def plot_feature_importances(model):\n    feature_imp = pd.Series(model.feature_importances_,index=X.columns).sort_values(ascending=False)[:10]\n\n    sns.barplot(x=feature_imp, y=feature_imp.index)\n    plt.title(\"Feature Importance\")\n    plt.show()\n\n    print(f\"Top 10 Feature Importance for {str(model).split('(')[0]}\\n\\n\",feature_imp[:10],sep='')","997f65fa":"plot_feature_importances(rfc)","9b94551f":"from sklearn.ensemble import GradientBoostingClassifier","7c61a5c8":"# gbc = GradientBoostingClassifier(random_state=42)\n# plot_result(gbc, \"gbc\")","ba8fc233":"# plot_feature_importances(gbc)","55cf56f6":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score","657fd659":"xgb = XGBClassifier(eval_metric = \"logloss\")\nplot_result(xgb, \"xgb\")","9b7a51bc":"plot_feature_importances(xgb)","1cfe4255":"import lightgbm as lgb","1c01ab4a":"lgb = lgb.LGBMClassifier()\nplot_result(lgb, \"lgb\")","679843d3":"plot_feature_importances(lgb)","1ef1d0c7":"\ndef AUC(cv_AUC, X_test=X_test):\n    dtc_auc= roc_auc_score(y_test,dtc.predict(X_test)) #Decision Tree Classifier\n#     lr_auc= roc_auc_score(y_test, lr.decision_function(X_test))#logistic regression\n#     svc_auc= roc_auc_score(y_test, svc.decision_function(X_test))#Support Vector Classifier\n    nc_auc= roc_auc_score(y_test, nc.predict(X_test))#Nearest Centroid Classifier\n    rfc_auc= roc_auc_score(y_test, rfc.predict_proba(X_test)[:,1])#Randomforest Classifier\n#     gbc_auc= roc_auc_score(y_test, gbc.predict_proba(X_test)[:,1])#GradientBoosting Classifier\n    nb_auc= roc_auc_score(y_test, nb.predict_proba(X_test)[:,1])#Naive Bayes Classifier\n#     knn_auc= roc_auc_score(y_test, knn.predict(X_test))#KNeighbors Classifier\n    xgb_auc= roc_auc_score(y_test, xgb.predict_proba(X_test)[:,1])#XGBoost Classifier\n    lgb_auc= roc_auc_score(y_test, lgb.predict_proba(X_test)[:,1])#LightGBM Classifier\n\n    cv_AUC={'dtc': dtc_auc,\n#            'lr': lr_auc,\n#            'svc':svc_auc,\n           'nc':nc_auc,\n           'rfc':rfc_auc,\n#            'gbc':gbc_auc,\n           'nb':nb_auc,\n#            'knn':knn_auc,\n           'xgb':xgb_auc,\n           'lgb':lgb_auc}\n    \n    return cv_AUC","69a1be58":"cv_AUC = AUC(cv_AUC)\ndf_eval = pd.DataFrame(data={'model': list(cv_acc_test.keys()), \n                             'bal_acc_train':list(cv_acc_train.values()),\n                             'bal_acc_test': list(cv_acc_test.values()), \n                             'recall': list(cv_TPR.values()), \n                             'fallout':list(cv_FPR.values()),\n                              'AUC': list(cv_AUC.values())}).round(2)\ndf_eval","05920ec5":"sns.relplot(x=\"recall\", y=\"AUC\", hue=\"model\", size=\"bal_acc_test\", \n            sizes=(40, 400), alpha=1, palette=\"bright\", height=5, legend='full', data=df_eval);","4484f44c":"### Smooting","a4c49d5a":"### Data Scaling","4647093c":"### 1-Logistic regression","10dc633b":"while 'RainTomorrow' is the target label, others are the independent features","f2dc3066":"### 3-K-Nearest Neighbor (KNN)","6876ec70":"### 6-NearestCentroid","b157870a":"### 2-Naive Bayes","b216dbc7":"We drop all but one of the features that are highly correlated with each other.","61539595":"### Isolation Forest\nReturn the anomaly score of each sample using the IsolationForest algorithm\n\nThe IsolationForest \u2018isolates\u2019 observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.\n\nSince recursive partitioning can be represented by a tree structure, the number of splittings required to isolate a sample is equivalent to the path length from the root node to the terminating node.\n\nThis path length, averaged over a forest of such random trees, is a measure of normality and our decision function.\n\nRandom partitioning produces noticeably shorter paths for anomalies. Hence, when a forest of random trees collectively produce shorter path lengths for particular samples, they are highly likely to be anomalies.\n![IsolationForest.png](attachment:IsolationForest.png)","72c2f791":"![metrics.png](attachment:metrics.png)","8fb0d244":"### 5-Decision Tree Classifier","86b5896e":"## Modelling","88a844e5":"### 7-Random Forest Classifier","a3670da8":"### 9-XGBOOST","ad008911":"### 4-Support Vector Mechanism (SVM)","375b7c9b":"### Make Feature Engineering","6cbf6ac0":"# Rain Prediction\nDataset Resource : [Kaggle](https:\/\/www.kaggle.com\/jsphyg\/weather-dataset-rattle-package)\n![download.png](attachment:download.png)","20f3da00":"> **Most succesful model is `Nearest Centroid Classifier`**","29cfa1fd":"## Outliers","86767e98":"![coeff.png](attachment:coeff.png)","f32b3e98":"### 8-Gradient Boosting Classifier","2bc93631":"### 10-LightGBM"}}