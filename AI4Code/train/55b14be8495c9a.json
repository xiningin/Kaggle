{"cell_type":{"b37a963a":"code","cb51c959":"code","b1f31a9d":"code","bb80a37a":"code","8b10699d":"code","63c35ed0":"code","13acff33":"code","769c9469":"code","05215dcd":"code","17bfac24":"code","14b516e4":"code","2c93aa96":"code","349b156d":"code","6f7cf2a5":"code","3a43ff27":"code","df75131a":"code","df23ccd8":"code","4a849727":"code","6227938c":"code","9451bfc3":"code","e3c1b944":"code","505e3142":"code","e6a41443":"code","d1540c41":"code","6edbf760":"code","727710b8":"code","ad42faca":"code","9cd48822":"code","36bb568c":"code","0b935310":"code","23ac763e":"code","166250e0":"code","4aa18f9a":"code","a37512ca":"code","ec40022e":"code","f68a4c2f":"code","a2c6a7bb":"code","2014388c":"code","d26aba93":"code","b1f0d29a":"code","d8934faf":"code","6c6ecfd5":"code","15c3b58d":"code","61dd5984":"code","0ac0aad5":"code","6e48d580":"code","94258cc9":"code","5a23da85":"code","e31690dc":"code","2d4af804":"code","fd2085f6":"code","04a6b904":"code","abb6e9e7":"code","61ca82fb":"code","13556bd6":"code","c2cda935":"code","dbd2619b":"code","3bf17320":"code","62e294de":"code","7bf3e574":"code","9ac1cf2c":"code","f44f8faf":"code","7d4526e4":"code","81b84892":"code","0675039d":"code","ad97d2eb":"code","b2a7de43":"code","738fc676":"code","97367fc7":"code","5c387333":"code","592eb351":"code","84f6a161":"code","7e42334f":"code","ae3d5a5c":"code","dd4b827d":"code","020b153c":"code","dbd45707":"markdown","b0a88536":"markdown","5895cf78":"markdown","b954bc5c":"markdown","41f46dc9":"markdown","cff63aab":"markdown","1d739a72":"markdown","4b5b2e53":"markdown","1a5772a4":"markdown","0255c1cc":"markdown"},"source":{"b37a963a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cb51c959":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","b1f31a9d":"pd.set_option('display.max_rows',None,'display.max_columns',None)","bb80a37a":"#reading train_csv\nhousing_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\n#reading test_csv\nhousing_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n#reading submission_csv\nhousing_submission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')","8b10699d":"#getting the the shape of all data frames\nprint('The shape of train data set is = {}'.format(housing_train.shape))\nprint('The shape of test data set is = {}'.format(housing_test.shape))\nprint('The shape of submission data set is = {}'.format(housing_submission.shape))","63c35ed0":"#having a look at train data\nhousing_train.head()","13acff33":"#having a look at test data\nhousing_test.head()","769c9469":"#having a look ata submission data frame\nhousing_submission.head()","05215dcd":"#checking for null values in train data \nhousing_train.isnull().sum()","17bfac24":"#checking for null values in test data\nhousing_test.isna().sum()","14b516e4":"##creating a function which will drop the columns having greater number of null values\ndef NullColumnDrop(df):\n    cols = []\n    print('Columns having null values :')\n    for i in df.columns:\n        if df[i].isna().sum() != 0:\n            per = round(((df[i].isna().sum()\/len(df[i]))*100),2)\n            print(str(i),str(per)+'%')\n            #setting threshold as 45%\n            if per >= 45:\n                cols.append(i)\n    print('*'*15)\n    print('Columns to be droped = {}'.format(cols))\n    #dropping the columns having greater number of null values than threshold\n    df.drop(columns = cols, axis = 1,inplace = True)\n    print('The new shape of data frame after dropping columns is = {}'.format(df.shape))\n\n##segregating numerical and categorical variables \ndef var_classifier(df):\n    #creating the list of numerical variables\n    num_var = [var for var in df.columns if df[var].dtypes != 'object']\n    #creating the list of categorical variables\n    cat_var = [var for var in df.columns if df[var].dtypes == 'object']\n    return [num_var,cat_var]\n\n\n##function for plotting coorelation matrix\ndef corr_heatmap(df):\n    plt.figure(figsize=(20,5))\n    ##getting the correlation matrix of data frame \n    corr = df.corr()\n    cmap = sns.diverging_palette(20, 220, n=256)\n    #plotting the heat map\n    sns.heatmap(corr,center = 0,cmap=cmap,annot=True)\n    plt.show()\n\n##function for plotting histogram pairplots\ndef hist_plot(lst,df):\n    plt.figure(figsize=(35,80))\n    for i in enumerate(lst):\n        sns.set_style(\"whitegrid\")\n        plt.subplot(7,6,i[0]+1)\n        sns.histplot(data=df, x=i[1],bins=30, kde=True,alpha = .2)\n        plt.xticks(rotation = 70,fontsize=12)\n        plt.xlabel(i[1],fontsize = 15)\n        plt.title('Distribution of '+i[1],fontweight = 'bold',fontsize=18)\n     ","2c93aa96":"##dropping id colum\nhousing_train.drop('Id',axis=1,inplace=True)\nhousing_test.drop('Id',axis = 1, inplace=True)","349b156d":"#dropping columns of test data set \nNullColumnDrop(housing_test)","6f7cf2a5":"##dropping columns of train data set\nNullColumnDrop(housing_train)","3a43ff27":"housing_train.head(3)","df75131a":"##filling out the null values for training data set for numerical columns:\nfor var in [i for i in var_classifier(housing_train)[0] if housing_train[i].isnull().sum() > 0]:\n    if var == 'MasVnrArea':\n        housing_train[var].fillna(int(housing_train[var].mode()),inplace=True)\n    else:\n        housing_train[var].fillna(int(housing_train[var].mean()),inplace=True)","df23ccd8":"##filling out the null values for categorical variables in train data set:\n\nvalues = {'MasVnrType': str(housing_train['MasVnrType'].mode()) ,\n          'BsmtQual': 'No Basement',\n          'BsmtCond': 'No Basement',\n          'BsmtExposure': 'No Basement',\n          'BsmtFinType1': 'No Basement',\n          'BsmtFinType2': 'No Basement',\n          'Electrical':str(housing_train['Electrical'].mode()),\n          'GarageType': 'No Garage' ,\n          'GarageFinish': 'No Garage',\n          'GarageQual': 'No Garage',\n          'GarageCond': 'No Garage'}\n#df.fillna(value=values)\nhousing_train.fillna(value = values, inplace=True)","4a849727":"##filling the null values for categorical variable in test data set:\nfor i in [var for var in var_classifier(housing_test)[1] if housing_test[var].isna().sum()!=0]:\n    print(i,housing_test[i].isna().sum())\n","6227938c":"##making the list of variable where the missing values will be replaced by mode\nmod_lst = ['MasVnrType','MSZoning','Utilities','Exterior1st',\n           'Exterior2nd','KitchenQual','Functional']\nfor i in mod_lst:\n    housing_test[i].fillna(str(housing_test[i].mode()),inplace = True)","9451bfc3":"##making a dict for categorical columns for filling respective missing values \nval = {'BsmtQual': 'No Basement',\n          'BsmtCond': 'No Basement',\n          'BsmtExposure': 'No Basement',\n          'BsmtFinType1': 'No Basement',\n          'BsmtFinType2': 'No Basement',\n          'GarageType': 'No Garage' ,\n          'GarageFinish': 'No Garage',\n          'GarageQual': 'No Garage',\n          'GarageCond': 'No Garage'}\nhousing_test.fillna(value=val,inplace = True)","e3c1b944":"for i in [var for var in var_classifier(housing_test)[0] if housing_test[var].isna().sum() !=0]:\n    print(i,housing_test[i].isna().sum())","505e3142":"#variables to be replaced with mean \nlst_mean = ['BsmtUnfSF','TotalBsmtSF','GarageYrBlt','GarageArea', 'LotFrontage']\nlst_mode = ['MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtFullBath','BsmtHalfBath','GarageCars']\nfor i in lst_mean:\n    housing_test[i].fillna(int(housing_test[i].mean()),inplace=True)\nfor j in lst_mode:\n    housing_test[j].fillna(int(housing_test[j].mode()),inplace=True)","e6a41443":"##viewing the train data\nplt.subplots(figsize = (20,5))\nsns.heatmap(housing_train.isna(),yticklabels=False,cmap=\"Greens\")\nplt.show()","d1540c41":"#viewing test data\nplt.subplots(figsize = (20,5))\nsns.heatmap(housing_test.isna(),yticklabels=False,cmap=\"PiYG\")\nplt.show()","6edbf760":"##there are 4 year columns\nyear_list = ['YearBuilt','YearRemodAdd','GarageYrBlt','YrSold',]\nnumerical_var = var_classifier(housing_train)[0]\ncategorical_var = var_classifier(housing_train)[1]\nnumerical_discrete = [var for var in numerical_var if housing_train[var].nunique()<25 and var not in year_list]\nnumerical_cont = [var for var in numerical_var if var not in numerical_discrete +year_list]","727710b8":"##analysis of categorical variables \nplt.figure(figsize=(40,105))\nfor i in enumerate(categorical_var):\n    sns.set_style(\"whitegrid\")\n    plt.subplot(8,5,i[0]+1)\n    sns.countplot(i[1],data = housing_train)\n    plt.xticks(rotation = 35,fontsize=13)\n    plt.xlabel(i[1],fontsize = 15,fontweight = 'bold')\n    plt.title('Sale_Price'+'-'+i[1],fontweight = 'bold',fontsize=15)","ad42faca":"##plotting different type of years with mean sales price\nplt.figure(figsize=(20,8))\nfor col in year_list:\n    housing_train.groupby(col)['SalePrice'].mean().plot(label = col)\nplt.legend()\nplt.xlabel('Years',fontsize = 15)\nplt.ylabel('Mean Sale Price',fontsize = 15)\nplt.title('Year vs Mean Sale Price',fontsize = 18, fontweight = 'bold')\nplt.show()","9cd48822":"##plotting numerical descrete variables with the mean SalePrice\nplt.figure(figsize=(25,50))\nfor i in enumerate(numerical_discrete):\n    sns.set_style(\"whitegrid\")\n    plt.subplot(8,4,i[0]+1)\n    data = housing_train.copy()\n    data.groupby(i[1])['SalePrice'].mean().plot.bar()\n    plt.xticks(rotation = 35,fontsize=8)\n    plt.xlabel(i[1],fontsize = 10)\n    plt.title('Sale_Price'+'-'+i[1],fontweight = 'bold',fontsize=12)\nplt.show()\n","36bb568c":"##analysis of numerical_continuous variable with sale price\n\nplt.figure(figsize=(35,80))\nfor i in enumerate(numerical_cont):\n    sns.set_style(\"whitegrid\")\n    plt.subplot(7,6,i[0]+1)\n    sns.scatterplot(x = i[1],y = housing_train['SalePrice'],hue = 'SalePrice',data = housing_train,palette=\"rocket_r\")\n    plt.xticks(rotation = 70,fontsize=12)\n    plt.xlabel(i[1],fontsize = 15)\n    plt.title('Sale_Price'+'-'+i[1],fontweight = 'bold',fontsize=18)","0b935310":"##checking the coorelation of numerical continuous variable \ncorr_heatmap(housing_train[numerical_cont])","23ac763e":"hist_plot(lst = numerical_cont,df = housing_train)","166250e0":"##creating a list which will be containing 0 and thus log transformation cant be performed\nlst_zero = []\nfor var in numerical_cont:\n    if 0 in housing_train[var].unique():\n        lst_zero.append(var)\n    else:\n        housing_train[var] = np.log(housing_train[var])\nprint('The log transformation is performed on variables = {}'.format([var for var in numerical_cont if var not in lst_zero]))","4aa18f9a":"##checking the curves after log transform\nhist_plot(lst = [var for var in numerical_cont if var not in lst_zero], df = housing_train)","a37512ca":"##checking the correlation of the variables just to make sure if coorelation is not decreased due to log transformation\ncorr_heatmap(housing_train[numerical_cont])","ec40022e":"###performing log transform on test data set also \nhist_plot(lst=[x for x in numerical_cont if x != 'SalePrice'],df= housing_test)","f68a4c2f":"##performing log transform to the same column of test data set\nlst_test = ['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea']\nfor var in lst_test:\n        housing_test[var] = np.log(housing_test[var])\n","a2c6a7bb":"hist_plot(lst = lst_test,df = housing_test)","2014388c":"##importing packages\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.feature_selection import SelectPercentile\nfrom sklearn.linear_model import LinearRegression,Ridge, Lasso\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error,r2_score,mean_absolute_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor","d26aba93":"##asigning a list for variable on which label encoding will be applied\nlabel_col = [var for var in categorical_var if housing_train[var].nunique() == 2]\n##asigning a list for variable on which one hot encoding will be applied\none_hot_col = [var for var in categorical_var if housing_train[var].nunique()  > 2]\n\n","b1f0d29a":"#performing label encoding\nle = preprocessing.LabelEncoder()\nfor i in label_col:\n    ##label encoding for train\n    housing_train[i] = le.fit_transform(housing_train[i])\n    ##label encoding for test\n    housing_test[i] = le.fit_transform(housing_test[i])","d8934faf":"##one hot encoding for the columns of train data set \nfor i in one_hot_col:\n    df1 = pd.get_dummies(housing_train[i],drop_first=True)\n    housing_train = pd.concat([df1,housing_train],axis=1)\n    housing_train.drop(i,axis = 1,inplace=True)\n##one hot encoding for the columns of train data set \nfor i in one_hot_col:\n    df1 = pd.get_dummies(housing_test[i],drop_first=True)\n    housing_test = pd.concat([df1,housing_test],axis=1)\n    housing_test.drop(i,axis = 1,inplace=True)\n##getting the shape of data \nprint('The shape of train data after one hot encoding is = {} \\nand test data is = {}'.format(housing_train.shape,housing_test.shape))","6c6ecfd5":"##scaling train and test data set \nscaling_object = StandardScaler()\n\n##scaling train data set\nhousing_train[[x for x in numerical_var if x!= 'SalePrice']] = scaling_object.fit_transform(housing_train[[x for x in numerical_var if x!= 'SalePrice']])\n##scaling test data set\nhousing_test[[x for x in numerical_var if x!= 'SalePrice']] = scaling_object.transform(housing_test[[x for x in numerical_var if x!= 'SalePrice']])","15c3b58d":"##having a look at transformed data\nhousing_train[[x for x in numerical_var if x!= 'SalePrice']].head(3)","61dd5984":"##having a look at transformed test data\nhousing_test[[x for x in numerical_var if x!= 'SalePrice']].head(3)","0ac0aad5":"##getting the shape of train and test data \nhousing_train.shape,housing_test.shape","6e48d580":"##dropping the duplicated columns from train data set\nhousing_train = housing_train.loc[:,~housing_train.columns.duplicated()]\nhousing_test = housing_test.loc[:,~housing_test.columns.duplicated()]\n##checking if there are any duplicated columns left in train and test data frame\nprint('Duplicated cols in train and test= '+str((housing_test.columns.duplicated().sum(), housing_train.columns.duplicated().sum())))\n##getting the shape of data after removing duplicates\nprint('Shape of train and test after removing duplicates = '+ str((housing_train.shape,housing_test.shape)))","94258cc9":"X = housing_train.loc[:, housing_train.columns != 'SalePrice']\ny = housing_train['SalePrice']","5a23da85":"##perfroming train-test-split and getting their shape\nX_train,X_test,y_train,y_test = train_test_split(X,y,\n                                                test_size=.2,\n                                                random_state=0)\nprint('Train', X_train.shape, y_train.shape)\nprint('Test', X_test.shape, y_test.shape)","e31690dc":"#applying mutual_info_regression\nmutual_info = mutual_info_regression(X_train,y_train)\nmutual_info","2d4af804":"mutual_info = pd.Series(mutual_info)\nmutual_info.index = X_train.columns\nmutual_info.sort_values(ascending=False).head(50).plot.bar(figsize=(20,5))\n ","fd2085f6":"##selecting top 20 percentile \ntop25col = SelectPercentile(mutual_info_regression,percentile=20) \ntop25col.fit(X_train,y_train)\n","04a6b904":"##creating the final list of top 20 percentile columns\nfinal_list = X_train.columns[top25col.get_support()]\n##viewing the list\nfinal_list","abb6e9e7":"##dropping the variables which are not in final_list\nX_train = X_train[final_list]\nX_test = X_test[final_list]","61ca82fb":"##getting the shape of x_train and x_test\nprint('Shape of X_train = {}'.format(X_train.shape))\nprint('Shape of X_test = {}'.format(X_test.shape))\n","13556bd6":"#selecting the same columns from housing_test data set \nhousing_test = housing_test[final_list]\nprint('Shape of X_test = {}'.format(housing_test.shape))","c2cda935":"#creating the object of linear regression\nli = LinearRegression()\nli.fit(X_train, y_train)","dbd2619b":"##predicting the results and calculating the scores \ny_pred = li.predict(X_test)\nr2 = r2_score(y_test,y_pred)\nrmse = mean_squared_error(y_test,y_pred)\nmae = mean_absolute_error(y_test,y_pred)\nprint('The r2 value = '+str(r2))\nprint('The rmse value = '+str(rmse))\nprint('The mean absolute error = '+str(mae))","3bf17320":"##ridge regression\nridge=Ridge()\nparameters={'alpha':[1e-15,1e-10,1e-8,1e-3,1e-2,1,5,10,20,30,35,40,45,50,55,100]}\nridge_regressor=GridSearchCV(estimator = ridge,param_grid = parameters,cv=5, return_train_score = True)\nridge_regressor.fit(X_train,y_train)\n##getting the best params\nprint(ridge_regressor.best_params_)\nprint(ridge_regressor.best_score_)\n##getting the score by predicting on test data \nridge_regressor.score(X_test,y_test)","62e294de":"## working with lasso reg\nlasso = Lasso()\nparameters={'alpha':[5, 0.5, 0.05, 0.005, 0.0005, 1, 0.1, 0.01, 0.001,0.0001, 0 ]}\nlasso_regressor = GridSearchCV(estimator= lasso, param_grid=parameters,cv = 5,return_train_score = True)\nlasso_regressor.fit(X_train, y_train)\nprint(lasso_regressor.best_estimator_)\nprint(lasso_regressor.best_score_)\n## getting the score by predicting on test data \nlasso_regressor.score(X_test,y_test)","7bf3e574":"##trying XGBoostRegressor\n#creating a dcitonary for hyperparameter tuning \nparam_dict = {\n    'n_estimators' : [50,100,300,500,900,1100],\n    'max_depth' : [2,3,4,5,6,8,10],\n    'booster' : ['gbtree'],\n    'learning_rate': [0.05,0.07,0.09,0.1,0.2,0.3],\n    'min_child_weight':[1,2,3,4,5],\n    'base_score' : [0.20,0.40,0.60,0.80,1],\n    \n\n}","9ac1cf2c":"#creating the object of XGBRegressor \nxgbregressor = XGBRegressor()\nxgbr = RandomizedSearchCV(estimator = xgbregressor,\n                   param_distributions = param_dict,\n                   cv = 5,verbose = 5,\n                   return_train_score = True,random_state = 42 )\nxgbr.fit(X_train,y_train)","f44f8faf":"##getting the best params \nxgbr.best_estimator_","7d4526e4":"#getting best score\nxgbr.best_score_","81b84892":"##getting the score by predicting on test data \nxgbr.score(X_test,y_test)","0675039d":"## trying out RandomForestRegressor\nrandom_grid = {\n               'n_estimators': [int(x) for x in np.linspace(start = 50, stop = 1000, num = 10)],\n               'max_features': ['auto', 'sqrt'],\n               'max_depth': [int(x) for x in np.linspace(start = 10, stop = 110, num = 10)],\n               'min_samples_split':[2, 5, 10],\n               'min_samples_leaf': [1, 2, 4],\n               'bootstrap': [True, False]\n              }","ad97d2eb":"#creating object of RFregressor\nrf = RandomForestRegressor()\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100,\n                               cv = 5, verbose=2, random_state=42, n_jobs = -1)\nrf_random.fit(X_train, y_train)","b2a7de43":"##getting the best params \nrf_random.best_params_","738fc676":"#getting best score\nrf_random.best_score_","97367fc7":"#getting the score after predicting test data \nround((rf_random.score(X_test,y_test)*100),4)","5c387333":"#creating a dict for data frame\naccuracy = {'Model': ['Linear_Reg', 'Ridge_Reg', 'Lasso_Reg', 'XGB_reg','RandomForest_Reg'],\n        'Efficiency': [round((li.score(X_test,y_test)*100),4),\n                       round((ridge_regressor.score(X_test,y_test)*100),4),\n                       round((lasso_regressor.score(X_test,y_test)*100),4),\n                       round((xgbr.score(X_test,y_test)*100),4),\n                       round((rf_random.score(X_test,y_test)*100),4),]} \n#creating a dataframe of models and there efficiency\nacc_df = pd.DataFrame.from_dict(accuracy)\n  \n","592eb351":"acc_df","84f6a161":"##using the best model to make predicitions \nfinal_model = XGBRegressor(base_score=1, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n             importance_type='gain', interaction_constraints='',\n             learning_rate=0.07, max_delta_step=0, max_depth=6,\n             min_child_weight=4, missing=np.nan, monotone_constraints='()',\n             n_estimators=300, n_jobs=4, num_parallel_tree=1, random_state=0,\n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n             tree_method='exact', validate_parameters=1, verbosity=None)\nfinal_model.fit(X_train,y_train)\npred = final_model.predict(housing_test)","7e42334f":"##getting the final result by reversing the effect of log transform that was done before training \nresult = np.exp(pred)\nresult","ae3d5a5c":"final_df = pd.DataFrame({\n    housing_submission.columns[0] : housing_submission['Id'],\n    housing_submission.columns[1] : result\n})","dd4b827d":"final_df.head()","020b153c":"final_df.to_csv('sample_submission.csv',index = False)","dbd45707":"### ***Filling missing values***","b0a88536":"# ***Dealing with missing data***","5895cf78":"# Performing EDA","b954bc5c":"### ***Encoding of categorical variables:***\n1. columns with 2 categories -> label encoding\n1. columns with more than two and less than equal to 10 categories -> one hot encoding","41f46dc9":"### ***I have devided whole dataframe into four parts :*** \n\n1. categorical_var-> containing categorical data  \n1. numerical_var -> containing both descrete and continuous\n\n1.       *numerical_discrete-> containing descrete*\n1.       *numerical_cont-> containing continuous numerical variables*","cff63aab":"### ***Performing Feature Selection***","1d739a72":"***I also tried GridSearchCV but RandomizedSearchCV was faster and quicker***","4b5b2e53":"### ***Creating some useful functions***","1a5772a4":"# ***Having a look at data***","0255c1cc":"***Reading CSVs***"}}