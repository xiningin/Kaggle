{"cell_type":{"02199fcf":"code","a306e807":"code","68612b7d":"code","127d1376":"code","0ffd435f":"code","bfb147b2":"code","90223829":"code","56c50778":"code","41210ae9":"code","e8cd7fef":"code","180c487a":"code","713b53d5":"code","e363ad5f":"code","08358bd7":"code","320e2fa1":"code","fdf7e0d8":"code","e1e24db2":"code","c931a399":"code","2ff1c1aa":"code","25c9f340":"code","3d77c944":"code","f70aa382":"code","85cc6745":"code","7949be11":"code","1b4cfb96":"code","c0c843a0":"code","a4eca943":"code","dfda7fed":"code","89b5da90":"code","56bca16e":"code","9256da2e":"code","fff25804":"code","db9c4d82":"code","bafa0f28":"code","5a53e862":"code","279276be":"code","7e46e6a0":"code","8adf9ba2":"code","ff2db54e":"code","537c7cd3":"code","83297188":"code","9a73ecf0":"code","81449149":"code","a9b16dfe":"code","7a0b82eb":"markdown","a642ce91":"markdown","0142b7a6":"markdown","16f77327":"markdown","71e86698":"markdown","9a3bac77":"markdown","f065daf9":"markdown","c30e082b":"markdown","cd8d3f4e":"markdown","0d9e395c":"markdown","24de2b1e":"markdown","243116d9":"markdown","64f6b3c2":"markdown","bd140673":"markdown","6252ddb1":"markdown","144327c5":"markdown","17452128":"markdown","e103611c":"markdown","4f5be714":"markdown","acc9ee93":"markdown","97fbcb8e":"markdown","1acc9663":"markdown","7b8c2fb8":"markdown","58a56577":"markdown","f878b163":"markdown","42117fd5":"markdown","e759c9f5":"markdown","2c53e91e":"markdown","667884e8":"markdown","0c5adb7a":"markdown","14391974":"markdown","86c8311f":"markdown","b14158b6":"markdown","7d2e6588":"markdown","a71456c9":"markdown","2a21bedc":"markdown","7a11015e":"markdown","00ac7ad3":"markdown","d74fd247":"markdown","2298de71":"markdown","a254a4e5":"markdown","0fd02c4b":"markdown","c107b2ef":"markdown"},"source":{"02199fcf":"import pandas as pd \nimport numpy as np \n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Header Files for Data preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Header Files for finding optimal linkage for clustering\nfrom scipy.cluster.hierarchy import cophenet\nfrom scipy.spatial.distance import pdist\n\n# Header files for visualizing the clusters\nfrom scipy.cluster.hierarchy import linkage,dendrogram,cut_tree\nfrom yellowbrick.cluster import SilhouetteVisualizer\n\n# Header Files for Agglomerative Clustering\nfrom sklearn.cluster import AgglomerativeClustering\n\n# Header Files for KMeans Custering\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\n# Header files for dimensionality Reduction\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import KernelPCA\n\n# Header files for DBSCan\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.cluster import DBSCAN\n","a306e807":"df=pd.read_csv('..\/input\/customer-segmentation-tutorial-in-python\/Mall_Customers.csv')\ndf.head()","68612b7d":"df.isnull().sum()","127d1376":"for x in df.select_dtypes(np.number).columns:\n    sns.boxplot(x=df[x])\n    plt.show()","0ffd435f":"df.shape","bfb147b2":"df.dtypes","90223829":"df.info()","56c50778":"# Analysing range of numerical columns\ndf.describe().T[['min','max']]","41210ae9":"# Analysing categorical Variables\nfor x in df.select_dtypes(exclude=np.number):\n    print(df[x].value_counts())","e8cd7fef":"df=df.drop('CustomerID',axis=1)","180c487a":"# Maintaining a copy of the data\ndata=df.copy()\ndf_num=df.select_dtypes(np.number)\ndf_cat=df.select_dtypes(exclude=np.number)","713b53d5":"ss=StandardScaler()\ndf_nums=pd.DataFrame(ss.fit_transform(df_num),columns=df_num.columns)\ndf_nums","e363ad5f":"df1=pd.concat([df_nums,df_cat],axis=1)\ndf1.head(2)","08358bd7":"df_processed=pd.get_dummies(df1,columns=['Gender'])\ndf_processed.head()","320e2fa1":"# Method with highest cophenetic score is the optimal linkage method\n\ncoph=dict()\nfor method in ['ward','average','complete','single']:\n    mergings=linkage(df_processed,method=method)\n    c,d=cophenet(mergings,pdist(df_processed))\n    coph[method]=c\nprint(coph)\n\nprint('\\nOptimal Linkage Method:',max(coph))","fdf7e0d8":"df_cluster=df_processed.copy()","e1e24db2":"# Done to find an approx value of k\nmergings=linkage(df_processed,method='ward',metric='euclidean')\ndendrogram(mergings,truncate_mode='lastp')\nplt.show()","c931a399":"df_cluster['cluster']=cut_tree(mergings,n_clusters=4)\ndf_cluster.head()","2ff1c1aa":"#Analysing The Cluster\ndf_cluster.cluster.value_counts()","25c9f340":"model=AgglomerativeClustering(n_clusters=4)\nmodel.fit(df_processed)\ndf_cluster['ag_cluster']=model.labels_\nmodel.labels_","3d77c944":"df_cluster.head()","f70aa382":"df_cluster.ag_cluster.value_counts()","85cc6745":"df_processed.columns","7949be11":"# Visualizing the cluster\nsns.scatterplot(x=df['Annual Income (k$)'],y=df['Spending Score (1-100)'],hue=df_cluster.ag_cluster)\nplt.show()","1b4cfb96":"#wcss is within cluster sum of squared errors.\nwcsse=[]\n\nfor k in np.arange(2,8):\n    model=KMeans(n_clusters=k,random_state=5)\n    model.fit(df_processed)\n    wcsse.append(model.inertia_)\n    \nplt.plot(np.arange(2,8),wcsse)\nplt.axvline(4,c='red')\nplt.xlabel('No. Of Clusters')\nplt.ylabel('wcsse')\nplt.show()\n    ","c0c843a0":"score=[]\nfor k in np.arange(2,6):\n    model=KMeans(n_clusters=k,random_state=5)\n    cluster=model.fit_predict(df_processed)\n    score.append(silhouette_score(df_processed,cluster))\n\nplt.plot(np.arange(2,6),score)\nplt.axvline(4,c='red')\nplt.xlabel('No. Of Clusters')\nplt.ylabel('Silhoutte Score')\nplt.show()","a4eca943":"clust_mod=KMeans(n_clusters=3,random_state=5)\nviz=SilhouetteVisualizer(clust_mod)\nviz.fit(df_processed)\nplt.show()","dfda7fed":"clust_mod=KMeans(n_clusters=4,random_state=5)\nviz=SilhouetteVisualizer(clust_mod)\nviz.fit(df_processed)\nplt.show()","89b5da90":"clust_mod=KMeans(n_clusters=5,random_state=5)\nviz=SilhouetteVisualizer(clust_mod)\nviz.fit(df_processed)\nplt.show()","56bca16e":"model=KMeans(n_clusters=4,random_state=5)\ncluster=model.fit_predict(df_processed)","9256da2e":"# Visualizing the cluster\nsns.scatterplot(x=df['Annual Income (k$)'],y=df['Spending Score (1-100)'],hue=cluster)\nplt.show()\n","fff25804":"pca=PCA()\npca.fit(df_processed)\nnp.cumsum(pca.explained_variance_ratio_)*100","db9c4d82":"df_pca=pd.DataFrame(pca.transform(df_processed))\ncol=['PC'+str(x) for x in np.arange(1,6)]\ndf_pca.columns=col\ndf_pca.head()","bafa0f28":"# Finding optimal number of clusters\nwcsse=[]\n\nfor k in np.arange(2,8):\n    model=KMeans(n_clusters=k,random_state=5)\n    model.fit(df_pca)\n    wcsse.append(model.inertia_)\n    \nplt.plot(np.arange(2,8),wcsse)\nplt.axvline(4,c='red')\nplt.xlabel('No. Of Clusters')\nplt.ylabel('wcsse')\nplt.show()","5a53e862":"model=KMeans(n_clusters=4,random_state=5)\ncluster=model.fit_predict(df_pca)\n\n# Visualizing the cluster\n\nsns.scatterplot(x=df_pca['PC1'],y=df_pca['PC2'],hue=cluster)\nplt.show()","279276be":"kpca= KernelPCA(n_components=2)\nkpca.fit(df_processed)\ndf_kpca=pd.DataFrame(kpca.fit_transform(df_processed),columns=['PC1','PC2'])\ndf_kpca.head()","7e46e6a0":"# Finding optimal number of clusters\nwcsse=[]\n\nfor k in np.arange(2,8):\n    model=KMeans(n_clusters=k,random_state=5)\n    model.fit(df_kpca)\n    wcsse.append(model.inertia_)\n    \nplt.plot(np.arange(2,8),wcsse)\nplt.axvline(4,c='red')\nplt.xlabel('No. Of Clusters')\nplt.ylabel('wcsse')\nplt.show()","8adf9ba2":"model=KMeans(n_clusters=4,random_state=5)\ncluster=model.fit_predict(df_kpca)\nsns.scatterplot(x=df_kpca['PC1'],y=df_kpca['PC2'],hue=cluster)\nplt.show()","ff2db54e":"nn=NearestNeighbors(n_neighbors=4)\nnn.fit(df_processed)","537c7cd3":"distance,index=nn.kneighbors(df_processed)\nplt.plot(np.sort(distance[:,3]))\nplt.show()","83297188":"df_db=df_kpca.copy()","9a73ecf0":"model=DBSCAN(eps=0.9,min_samples=5)\ndf_db['cluster']=model.fit_predict(df_processed)\ndf_db.cluster.value_counts()","81449149":"# Visualizing the cluster to identify outliers\nsns.scatterplot(x=df_db['PC1'],y=df_kpca['PC2'],hue=df_db['cluster'],palette=['Red','Yellow','Pink'])\nplt.show()","a9b16dfe":"# Removing the identified outliers\ndf_db=df_db[df_db.cluster != -1]\nsns.scatterplot(x=df_db['PC1'],y=df_kpca['PC2'],hue=df_db['cluster'],palette=['Yellow','Pink'])\nplt.show()","7a0b82eb":"-1 represents outliers","a642ce91":"### 7.1.2 Silhoutte Method","0142b7a6":"The point at which silhouette score is highest is considered as the optimal value for number of clusters.","16f77327":"<a id='ol'><\/a>\n## 4.Determining Optimal Linkage Method","71e86698":"<a id='outliers'><\/a>\n### 3.3 Analysing Outliers","9a3bac77":"<a id='dt'><\/a>\n### 3.4 Analysing the data set","f065daf9":"<a id='kmeans'><\/a>\n## 7. KMeans Clustering (Lloyds Algorithm)","c30e082b":"In the middle there are is no clear seperation in the clusters(Could be because the two features selected at random doesnot explain maximum variance).\n\nDimensionality reduction techniques like PCA or KPCA can be used to find the best vectors to represent clusters in lower dimensions.","cd8d3f4e":"### 9.1 Finding optimal value of epsilon","0d9e395c":"### 7.1.3 Silhoutte Visualizer","24de2b1e":"The aim of ploting an elbow plot is to find an optimal value of k such that varience within clusters is lowest and the number of clusters is not too large to interpret.","243116d9":"Customer ID - Unique identification of customer\n\nGender - Sex of the customer\n\nAge - Age of customer\n\nAnnual Income - Income of salary in 1000's unit Dollars\n\nSpending Score - Readiness of customer to spend money","64f6b3c2":"### 7.1.4 KMeans Model","bd140673":"Note : \n\n1. Determining optimal number of clusters using dendrogram is confusing\n    \n2. High time complexity","6252ddb1":"<a id='PCA'><\/a>\n## 8. Principal Component Analysis","144327c5":"Logic: Clusters data by seperating data into groups of equal variance.","17452128":"When compared to the results of pca at the edges the clusters are not overlapping . But the main application of KPCA is when the points are not linearly seperable.","e103611c":"## Table of Content\n\n1. **[Header Files](#lib)**\n2. **[About Data Set](#about)**\n3. **[Data Preparation](#prep)**\n    - 3.1 - **[Read Data](#read)**\n    - 3.2 - **[Analysing Missing Values](#miss)**\n    - 3.3 - **[Analysing Outliers](#outliers)**\n    - 3.4 - **[Analysing the data set](#dt)**\n    - 3.5 - **[Scaling](#scale)**   \n    - 3.6 - **[Encoding](#encode)** \n    \n4. **[Determining Optimal Linkage Method](#ol)**\n5. **[Visualizing the clusters](#vis)**\n6. **[Agglomerative Clustering](#ag)**\n7. **[KMeans Clustering](#kmeans)**\n8. **[Principal Component Analysis](#PCA)**\n9. **[Kernel PCA](#kpca)**\n10. **[Density Based Clustering](#dbscan)**","4f5be714":"<a id='kpca'><\/a>\n## 9. Kernel PCA","acc9ee93":"<a id='miss'><\/a>\n### 3.2 Analysing Missing Values","97fbcb8e":"<a id='about'><\/a>\n## 2.About Data Set\n","1acc9663":"PCA is a method used to represent the data in lower dimensions by creating new features that capture maximum variance.","7b8c2fb8":"<a id='dbscan'><\/a>\n## 9. Density Based Clustering (DBScan)","58a56577":" <a id='scale'><\/a>\n### 3.5 Scaling","f878b163":"Points in Light Red are the outliers","42117fd5":"Logic - Each Observation is a unique cluster at the initial step then iteratively moves to add more similar points to the cluster.This process is continued till all observations are fused to a single cluster\n\nNote - Doesnt work well with very large data(Computational Cost is very high)","e759c9f5":"The value of k is selelected at the point where an elbow is formed, hence the name elbow plot.(Here 4)","2c53e91e":"Note- No missing Values in data","667884e8":"<a id='lib'><\/a>\n## 1. Header Files","0c5adb7a":"<a id='vis'><\/a>\n## 5.Visualizing the clusters","14391974":"### 9.2 DBScan ","86c8311f":"<a id='ag'><\/a>\n## 6.Agglomerative Clustering","b14158b6":"DBScan forms clusters of non linear shapes. The main application of DBScan is in outlier detection. The regions are not densely populated are considered to be outliers.","7d2e6588":"<a id='read'><\/a>\n### 3.1 Read the data","a71456c9":"Kernel PCA uses a function to project non linear data onto a higher dimension inorder to make it linearly seperable and then uses PCA.","2a21bedc":"<a id='prep'><\/a>\n## 3.Data Preperation","7a11015e":"<a id='encode'><\/a>\n### 3.6 Encoding","00ac7ad3":"### 7.1.1 Elbow Plot","d74fd247":"Note: A cluster is said to be a good cluster when\n\n1.Clusters are well packed\n\n2.Clusters are well seperated","2298de71":"Note: Very Few Outliers","a254a4e5":"There are two methods to calculate the optimal value of K \n\n1. Elbow Plot\n\n2. Silhoutte Method","0fd02c4b":"### 7.1 Optimal Value of K for Kmeans clustering","c107b2ef":"DBScan has 2 main parameters to be considered : \n\n1. eps - Radius of neighbourhood of a data point\n\n2. min_samples - Number of points inside epsilon neighborhood to be considered as a core point"}}