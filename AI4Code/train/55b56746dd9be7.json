{"cell_type":{"805bd530":"code","a8833d1b":"code","11072540":"code","12093bbd":"code","1cfb4951":"code","832c33a5":"code","94159e44":"code","f771e7c0":"code","99df7618":"code","cc762600":"code","4f252905":"code","c5994549":"code","bc4243c1":"code","892fdd55":"code","52981d42":"markdown","27c34dcf":"markdown","3cca79c6":"markdown","af91ef7b":"markdown","18f2ba72":"markdown","d97b283e":"markdown","ed2becea":"markdown","02d890fe":"markdown","bfdc93a5":"markdown","41acda7c":"markdown","c4dece37":"markdown","62239b91":"markdown","d0203548":"markdown","7b9fa57d":"markdown"},"source":{"805bd530":"import gym","a8833d1b":"import torch\nimport time\nimport matplotlib.pyplot as plt","11072540":"\nfrom gym.envs.registration import register\nregister(\n    id='FrozenLakeNotSlippery-v0',\n    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n    kwargs={'map_name' : '4x4', 'is_slippery': False},\n)\n\nenv = gym.make('FrozenLakeNotSlippery-v0')\n\n# Instantiate the Environment.\n# env = gym.make('FrozenLake-v0')\n\n# To check all environments present in OpenAI\n# print(envs.registry.all())\n\n\nenv.render()","12093bbd":"# Total number of States and Actions\nnumber_of_states = env.observation_space.n\nnumber_of_actions = env.action_space.n\nprint( \"States = \", number_of_states)\nprint( \"Actions = \", number_of_actions)\n\nnum_episodes = 1000\nsteps_total = []\nrewards_total = []\negreedy_total = []\n","1cfb4951":"# if learning_rate == 0:\n#      Pick value of new Q(s,a) based on past experience\n# elif learning_rate == 1:\n#      Pick value of new Q(s,a) based on current situtation\n\n# Value of learning_rate(alpha) varies from [0 - 1]","832c33a5":"# Discount rate accounts for the Reward the agent receive on an action\n\n# if discount_rate == 0:\n#     only current reward accounted\n# elif discount_rate == 1:\n#     future rewards also accounted\n    ","94159e44":"# PARAMS \n\n# Discount on reward\ngamma = 0.95\n\n# Factor to balance the ratio of action taken based on past experience to current situtation\nlearning_rate = 0.9\n\n","f771e7c0":"# exploit vs explore to find action\n# Start with 70% random actions to explore the environment\n# And with time, using decay to shift to more optimal actions learned from experience\n\negreedy = 0.7\negreedy_final = 0.1\negreedy_decay = 0.999","99df7618":"Q = torch.zeros([number_of_states, number_of_actions])\nQ\n","cc762600":"for i_episode in range(num_episodes):\n    \n    # resets the environment\n    state = env.reset()\n    step = 0\n\n    while True:\n        \n        step += 1\n        \n        random_for_egreedy = torch.rand(1)[0]\n        \n\n        if random_for_egreedy > egreedy:      \n            random_values = Q[state] + torch.rand(1,number_of_actions) \/ 1000      \n            action = torch.max(random_values,1)[1][0]  \n            action = action.item()\n        else:\n            action = env.action_space.sample()\n            \n        if egreedy > egreedy_final:\n            egreedy *= egreedy_decay\n        \n        new_state, reward, done, info = env.step(action)\n\n        # Filling the Q Table\n        Q[state, action] = reward + gamma * torch.max(Q[new_state])\n        \n        # Setting new state for next action\n        state = new_state\n        \n        # env.render()\n        # time.sleep(0.4)\n        \n        if done:\n            steps_total.append(step)\n            rewards_total.append(reward)\n            egreedy_total.append(egreedy)\n            if i_episode % 10 == 0:\n                print('Episode: {} Reward: {} Steps Taken: {}'.format(i_episode,reward, step))\n            break\n        \n\n","4f252905":"print(Q)\n        \nprint(\"Percent of episodes finished successfully: {0}\".format(sum(rewards_total)\/num_episodes))\nprint(\"Percent of episodes finished successfully (last 100 episodes): {0}\".format(sum(rewards_total[-100:])\/100))\n\nprint(\"Average number of steps: %.2f\" % (sum(steps_total)\/num_episodes))\nprint(\"Average number of steps (last 100 episodes): %.2f\" % (sum(steps_total[-100:])\/100))\n","c5994549":"plt.figure(figsize=(12,5))\nplt.title(\"Rewards\")\nplt.bar(torch.arange(len(rewards_total)), rewards_total, alpha=0.6, color='green', width=5)\nplt.show()\n\n","bc4243c1":"plt.figure(figsize=(12,5))\nplt.title(\"Steps \/ Episode length\")\nplt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color='red', width=5)\nplt.show()\n\n","892fdd55":"plt.figure(figsize=(12,5))\nplt.title(\"Egreedy value\")\nplt.bar(torch.arange(len(egreedy_total)), egreedy_total, alpha=0.6, color='blue', width=5)\nplt.show()\n","52981d42":"![image.png](attachment:image.png)\n- s : Current State of the agent.\n- a : Current Action Picked according to some policy.\n- s' : Next State where the agent ends up.\n- a' : Next best action to be picked using current Q-value estimation, i.e. pick the action with the maximum Q-value in the next state.\n- R : Current Reward observed from the environment in Response of current action.\n- \ud835\udefe (>0 and <=1) : Discounting Factor for Future Rewards. Future rewars are less valuable than current rewards so they must be discounted. Since Q-value is an estimation of expected rewards from a state, discounting rule applies here as well.\n- \ud835\udefc  : Step length taken to update the estimation of Q(S, A).","27c34dcf":"- Hoped you liked my notebook (upvote top right), my way to conribute back to this fantastic Kaggle platform and community.\n- Author - Arjit Sharma","3cca79c6":"## GYM\n\nGym is released by Open AI in 2016 (http:\/\/gym.openai.com\/docs\/). It is a toolkit for developing and comparing reinforcement learning algorithms. OpenAI\u2019s mission is to ensure that artificial general intelligence benefits all of humanity. \n\n![image.png](attachment:image.png)\n\nSource: OpenAI\n\nIn 2018 Gym-retro was released as its successor: https:\/\/blog.openai.com\/gym-retro\/","af91ef7b":" #### In the above graph, we can see how  egreedy ($\\epsilon$) value is reducing with time ","18f2ba72":"#### Here, we can see that the steps taken in an episode is large because we have high random action in the starting and later on...as we learn from experience, we start to take more informed steps , hence less noise ","d97b283e":"## FrozenLake Problem\n\nThe agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile.\n![image.png](attachment:image.png)\n\n\n* S=Start\n* F=Frozen\n* H=Hole\n* G=Goal","ed2becea":"## Exploration vs Exploitation\n![image.png](attachment:image.png)\n\n- Choosing the Action to take using  \ud835\udf16 -greedy policy:\n- \ud835\udf16 -greedy policy of is a very simple policy of choosing actions using the current Q-value estimations. It goes as follows :\n- With probability (1- $\\epsilon$ ) choose the action which has the highest Q-value.\n- With probability ($\\epsilon$) choose any action at random.\n\n\n\n","02d890fe":"### Discount Rate ($\\gamma$)","bfdc93a5":"## Reinforcement Learning\nReinforcement Learning briefly is a paradigm of Learning Process in which a learning agent learns, overtime, to behave optimally in a certain environment by interacting continuously in the environment. The agent during its course of learning experience various different situations in the environment it is in. These are called states. The agent while being in that state may choose from a set of allowable actions which may fetch different rewards(or penalties). The learning agent overtime learns to maximize these rewards so as to behave optimally at any given state it is in.","41acda7c":"###  Learning Rate ($\\alpha$)","c4dece37":"# Interacting with the Gym environment  \nSource: [OpenAI](https:\/\/openai.com\/)  \nOPenAI\/gym makes it relative straightforward to interact with the game.  \n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/800\/1*7Ae4mf9gVvpuMgenwtf8wA.png\">\n\nEach timestep, the agent chooses an action, and the environment returns an observation and a reward.  \n\n*observation, reward, done, info = env.step(action) *  \n* observation (object): an environment-specific object representing your observation of the environment. For example, pixel data from a camera, joint angles and joint velocities of a robot, or the board state in a board game line Taxi.\n* reward (float): amount of reward achieved by the previous action. The scale varies between environments, but the goal is always to increase your total reward.\n* done (boolean): whether it\u2019s time to reset the environment again. Most (but not all) tasks are divided up into well-defined episodes, and done being True indicates the episode has terminated. (For example, perhaps the pole tipped too far, or you lost your last life.)\n* info (dict): ignore, diagnostic information useful for debugging. Official evaluations of your agent are not allowed to use this for learning.  \n\nLet's first do some random steps in the game so you see how the game looks like","62239b91":"# Frozen Lake with OpenAI GYM\n\n","d0203548":"\n4x4=16 States \n\nLeft, Right, Up, Down 4 actions.","7b9fa57d":"## Q-Learning Equation\n#### Q-Learning is a basic form of Reinforcement Learning which uses Q-values (also called action values) to iteratively improve the behavior of the learning agent.\n![image.png](attachment:image.png)\n\n\n#### New Q(s,a) = (1 - learning_rate) x Q(s,a)  +  learning_rate x [ reward + discount_rate x maxQ(s',a')]  \n\n\n\n"}}