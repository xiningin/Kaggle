{"cell_type":{"31434516":"code","68355c44":"code","886c114d":"code","1540da34":"code","4fcc187f":"code","ba483c28":"code","269935bd":"code","1edfddce":"code","a41e8d72":"code","de445733":"code","9c2031ef":"code","176dac91":"code","d874c593":"code","0cef2490":"code","fa08a3f3":"code","8af5f8a3":"code","39a2ba3a":"code","35288fb6":"code","5e11e634":"code","187f6013":"code","422c22b2":"code","339cf02f":"code","941db2f6":"code","b405cf7a":"code","1fd2def7":"code","8fda1831":"code","504f5a7f":"code","88446005":"code","caf2de76":"code","a89be229":"code","adf7d6a0":"code","1b427664":"code","ea8871f7":"code","0c22277a":"code","618229e1":"code","2fef56ef":"code","cd510550":"code","2caf157d":"code","ff8035a4":"code","990bbd52":"code","7b3f9b20":"code","014853cb":"code","576b66ea":"code","a29df45d":"code","39bc0d49":"code","911b6563":"code","702d5db4":"code","819b1908":"code","e14abc81":"code","1ab2f37e":"code","78994a19":"code","f8b1586d":"markdown","72987186":"markdown","10ed4afd":"markdown","ce129ada":"markdown","817d765b":"markdown","e1c427f4":"markdown","91b24e6d":"markdown","ddbdc6fe":"markdown","d8dd64df":"markdown","7f2da56a":"markdown","15f0d884":"markdown","03e8b807":"markdown","26d0d53c":"markdown","257504cb":"markdown","526dcd98":"markdown","37600091":"markdown","df3171f6":"markdown","fecfdfc2":"markdown","a660505c":"markdown","1e3543b6":"markdown","eed930b3":"markdown","05f81fdc":"markdown","3fc89f68":"markdown","92bb24e8":"markdown","6b421584":"markdown","68982e05":"markdown","4783e4da":"markdown","1cf718c2":"markdown","116ed757":"markdown","1ef6af95":"markdown","18a5d51c":"markdown","494b77e8":"markdown","2cea4f88":"markdown"},"source":{"31434516":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport math\nimport shap\nimport xgboost","68355c44":"df_train=pd.read_csv(\"..\/input\/novartis-data\/Train.csv\")\ndf_test=pd.read_csv(\"..\/input\/novartis-data\/Test.csv\")","886c114d":"df_train.head()","1540da34":"df_train.isnull().sum()","4fcc187f":"df_test.isnull().sum()","ba483c28":"plt.figure(figsize=(20,18))\ndf_train.iloc[:,2:-1].boxplot()","269935bd":"corr = df_train.iloc[:,2:-1].corr()\ncorr.style.background_gradient(cmap='coolwarm')","1edfddce":"from sklearn.impute import KNNImputer","a41e8d72":"k=int(round(len(df_train)**0.5,0))\nif k%2==0:\n    k=k+1\nk","de445733":"imputer = KNNImputer(n_neighbors=k)","9c2031ef":"X=df_train.iloc[:,2:-1].values\nY=df_train.iloc[:,-1].values\nx_test=df_test.iloc[:,2:].values","176dac91":"X=imputer.fit_transform(X)\nx_test=imputer.transform(x_test)","d874c593":"from sklearn.preprocessing import StandardScaler","0cef2490":"sc=StandardScaler()\nX=sc.fit_transform(X)\nx_test=sc.transform(x_test)\n","fa08a3f3":"np.corrcoef(X[:,9],X[:,11])\nnp.corrcoef(X[:,1],X[:,2])","8af5f8a3":"X_reformed=np.delete(X,(2,11),axis=1)\nx_test_reformed=np.delete(x_test,(2,11),axis=1)","39a2ba3a":"from sklearn.cluster import KMeans","35288fb6":"iner=[]\ncount=[]","5e11e634":"for i in range(1,8):\n    kmeans=KMeans(n_clusters=i)\n    kmeans.fit(X_reformed)\n    inertia=kmeans.inertia_\n    count.append(i)\n    iner.append(inertia)\n    ","187f6013":"count=np.array(count)\niner=np.array(iner)","422c22b2":"plt.plot(count,iner)","339cf02f":"from sklearn.decomposition import PCA\nexplained_variance=[]\ncount1=[]","941db2f6":"pca=PCA(n_components=10)\npca.fit(X_reformed)\nexplained_variance1=pca.explained_variance_ratio_\n","b405cf7a":"plt.plot(explained_variance1)","1fd2def7":"from sklearn.linear_model import LogisticRegression\nlr=LogisticRegression()","8fda1831":"p=sum(Y)\/len(Y)\nprint(p)","504f5a7f":"from imblearn.over_sampling import RandomOverSampler\nros = RandomOverSampler(random_state=0)\nX_resampled, Y_resampled = ros.fit_resample(X_reformed, Y)","88446005":"p=sum(Y_resampled)\/len(Y_resampled)\nprint(p)","caf2de76":"lr.fit(X_resampled,Y_resampled)","a89be229":"y=lr.predict(X_resampled)","adf7d6a0":"from sklearn.metrics import accuracy_score, auc, confusion_matrix,f1_score, roc_curve, roc_auc_score","1b427664":"confusion_matrix(Y_resampled,y)","ea8871f7":"from sklearn.model_selection import cross_val_score\nfrom sklearn import metrics\nscores = cross_val_score(lr,X_resampled, Y_resampled, cv=10, scoring='recall')","0c22277a":"scores","618229e1":"fpr, tpr, thresholds = metrics.roc_curve(Y_resampled, y)\nroc_auc = metrics.auc(fpr, tpr)\ndisplay = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,estimator_name='logistic')\ndisplay.plot()  \nplt.show() ","2fef56ef":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV","cd510550":"param= {\n    'bootstrap': [True],\n    'max_depth': [3,4,5,6,7,8],\n    'max_features': [5,6,7,8,9,10],\n    'min_samples_leaf': [5,6,7,8,9,10],\n    'min_samples_split': [20,25,50],\n    'n_estimators': [500,1000],\n    'criterion':[\"gini\",\"entropy\"]\n}","2caf157d":"random=RandomizedSearchCV(estimator=RandomForestClassifier(),param_distributions=param,n_iter=10,cv=3,n_jobs=-1)\nrandom.fit(X_resampled,Y_resampled)","ff8035a4":"search=random.fit(X_resampled,Y_resampled)\nsearch.best_params_","990bbd52":"y=random.predict(X_resampled)","7b3f9b20":"scores = cross_val_score(random,X_resampled, Y_resampled, cv=10, scoring='recall',n_jobs=-1)\nscores","014853cb":"fpr, tpr, thresholds = metrics.roc_curve(Y_resampled, y)\nroc_auc = metrics.auc(fpr, tpr)\ndisplay = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,estimator_name='randomforest')\ndisplay.plot()  \nplt.show()  ","576b66ea":"from sklearn.ensemble import AdaBoostClassifier","a29df45d":"ada=AdaBoostClassifier(n_estimators=1000)","39bc0d49":"ada.fit(X_resampled,Y_resampled)","911b6563":"scores = cross_val_score(ada,X_resampled, Y_resampled, cv=10, scoring='recall',n_jobs=-1)\nscores","702d5db4":"y=ada.predict(X_resampled)\nfpr, tpr, thresholds = metrics.roc_curve(Y_resampled, y)\nroc_auc = metrics.auc(fpr, tpr)\ndisplay = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,estimator_name='Adaboost')\ndisplay.plot()  \nplt.show() ","819b1908":"from sklearn.ensemble import GradientBoostingClassifier","e14abc81":"gbc=GradientBoostingClassifier(n_estimators=1000)\ngbc.fit(X_resampled,Y_resampled)","1ab2f37e":"scores = cross_val_score(gbc,X_resampled, Y_resampled, cv=10, scoring='recall',n_jobs=-1)\nscores","78994a19":"y=ada.predict(x_test_reformed)\nxcv={\"INCIDENT_ID\":df_test.iloc[:,0],\"MULTIPLE_OFFENSE\":y}\nsample=pd.DataFrame(xcv)\nsample.to_csv(\"Sample.csv\",index=False)","f8b1586d":"#### Looking at the plot it is evident that X_2 and X_3 are highly correlated, and X_12 and X_10 are highly correlated.","72987186":"##### Creating the confusion matrix","10ed4afd":"##### The recall is 87% which can be improved.","ce129ada":"#### Trying to reduce the number of features using a scree plot","817d765b":"###### Cross Validation Score is 97% which can be further improved","e1c427f4":"# Importing the required train and test dataset","91b24e6d":"#### Using selected features for classification.","ddbdc6fe":"#### Looking for ideal number of clusters,since the ideal number of clusters cannot be determined that is why we sack this idea.","d8dd64df":"#### It is clear that n_estimators should be greater that 500 trees","7f2da56a":"# Importing the required libraries","15f0d884":"#### Checking the balance again","03e8b807":"#### Trying to reduce the number of variables using clustering if we can find clusters and then we can classify","26d0d53c":"#### Looking at all the features it is evident that there is a high presence of outliers in all the features , incident ID and Date has been removed from the data","257504cb":"##### Now the dataset has been balanced","526dcd98":"#### The AUC curve shows 88% coverage.","37600091":"#### Since it is highly imbalanced we have to resample the dataset","df3171f6":"##### Best recall till now using Adaboost Classifier","fecfdfc2":"### Imputing the dataset with KNN impute for test as well as train\n","a660505c":"### Feature X_12 has missing 182 values","1e3543b6":"#### Since the average score of Adaboost Classifier is greater than Gradient Boost Classifier we will go for Adaboost Classifier.","eed930b3":"#### Going for Randomized Search to find optimum paprameters for Random Forest","05f81fdc":"### Using adaboost to predict test case","3fc89f68":"#### Using Gradient Boosting algorithm to check if it is greater than ada boost.","92bb24e8":"# Lets look at the balance of the training set","6b421584":"\n\n#### Using Ada boost Classifier since we know that n_estimators should be greater than 500, therefore using 1000 trees","68982e05":"### Feature X_12 is also missing in test dataset","4783e4da":"#### Scaling all the features","1cf718c2":"## Looking for missing values","116ed757":"### Identifying square root of the number of observations and making it odd","1ef6af95":"#### Since the correlation is high we will drop the features X_3 and X_12","18a5d51c":"#### Importing Standard Scaler for pre processing of Data","494b77e8":"#### Since the number of components cannot be determined using a scree plot, we have to move to classification algorithms","2cea4f88":"### Using KNN imputer to impute missing values"}}