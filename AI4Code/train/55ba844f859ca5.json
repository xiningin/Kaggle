{"cell_type":{"c24657f8":"code","313af621":"code","56c436bd":"code","83477741":"code","e1e71e82":"code","00584983":"code","f944fa9a":"code","20f53621":"code","ffabf18e":"code","60570013":"code","894454b9":"code","dc8bf682":"code","50a54321":"code","4d0053dc":"code","42b8978a":"code","cbb467e8":"code","963aacb0":"code","c5608105":"code","18a66088":"code","85dcc3d7":"code","ed1a3b8e":"code","a1956911":"code","5bede4a8":"code","d58b9fa1":"code","e17481d3":"code","d7974058":"code","1a74ec40":"code","6461204e":"code","0f730e5f":"code","c60176f2":"code","c4d42a4d":"code","1ef594c4":"code","324abdf5":"code","b7c87f9d":"code","60c2835b":"code","777e4bc7":"code","3e928c5e":"code","977c54eb":"code","b751298d":"code","f67d7238":"code","3b231ad1":"code","5575a6f0":"code","e7a04143":"code","155e1d24":"code","5335d480":"code","adee77d4":"code","fb6c2441":"code","d55c4fa4":"code","3ab1a278":"code","8027d688":"code","0a609a62":"code","2b68d7b7":"code","f10e3427":"code","a2972027":"code","a9858a09":"code","f0dc5fd4":"code","f69ea10f":"code","d9ccc55d":"code","45f5c003":"code","9f4fcffb":"code","6df45c70":"code","d5e447e5":"code","44898ddb":"code","555604f2":"code","5d043749":"code","671e41d4":"code","9ea05a79":"code","660f2704":"code","da1f8f7d":"code","3ad341f7":"code","b5b0d0c4":"code","a211830c":"markdown","84925ab7":"markdown","1405adf2":"markdown","622a5c90":"markdown","dc9314dc":"markdown","7a5c605c":"markdown","c465d3cd":"markdown","a2e6198b":"markdown","3899d967":"markdown","570c3854":"markdown","61feeabc":"markdown","918d9d7f":"markdown","764bbe89":"markdown","fe3f7d5f":"markdown","2560326c":"markdown","290bcb7d":"markdown","531fa19d":"markdown","7d77217a":"markdown","07dbb417":"markdown","69bd905b":"markdown","e9bf6a77":"markdown","beeb4b73":"markdown","0f5b96fb":"markdown","cd61da94":"markdown","b327317c":"markdown","89d00644":"markdown","d9e51a1d":"markdown","67e57e8f":"markdown","1902c230":"markdown","f93101ea":"markdown","775e0db8":"markdown","a712b62b":"markdown","dde4612c":"markdown","65585a89":"markdown","6b80cc6f":"markdown","f572f7db":"markdown","109599e4":"markdown","01eea7e8":"markdown","6449a7ec":"markdown","8ec2b6e3":"markdown","4a2988cd":"markdown","7b17ea63":"markdown","c3d5ff6a":"markdown","4fd1fd2d":"markdown","b100b95d":"markdown","687cb8ed":"markdown","0f6d0785":"markdown","6b9d9b00":"markdown","4738c184":"markdown","08457e9e":"markdown","8ca1a2b4":"markdown","dbdfa41d":"markdown","2f27dadb":"markdown","84358fea":"markdown","4e52e31f":"markdown","6eb2a8e2":"markdown","e158ff3c":"markdown","4caa02f1":"markdown","c03c0909":"markdown","69573d69":"markdown","7fc77977":"markdown","3e5a7bc3":"markdown","2415f519":"markdown","869f6c70":"markdown","2aea377e":"markdown","7ae5d2f4":"markdown"},"source":{"c24657f8":"# To help with reading and manipulating data\nimport pandas as pd\nimport numpy as np\n\n# To help with data visualization\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# To help with model building\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom sklearn import preprocessing\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nimport keras\nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.optimizers import Adam\n\n# To get different metric scores\nimport sklearn.metrics as metrics\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve, auc\n\n# To define maximum number of columns to be displayed in a dataframe\npd.set_option(\"display.max_columns\", None)\n\n# To supress scientific notations for a dataframe\npd.set_option(\"display.float_format\", lambda x: \"%.3f\" % x)\n\n# To supress warnings\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n# This will help in making the Python code more structured automatically\n#!pip install nb-black\n#%load_ext nb_black","313af621":"#pip install nb-black","56c436bd":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","83477741":"#Defining the path of the dataset\ndataset_file = '\/kaggle\/input\/bank-churn-prediction\/bank.csv'","e1e71e82":"#reading dataset\ndata = pd.read_csv(dataset_file)","00584983":"data.head()","f944fa9a":"data.tail()","20f53621":"data.info()","ffabf18e":"data.duplicated().sum()","60570013":"dupe = data[\"CustomerId\"].duplicated()\ndupe[dupe == True].count()","894454b9":"for feature in data.columns:  # Loop through all columns in the dataframe\n    if (\n        data[feature].dtype == \"object\"\n    ):  # Only apply for columns with categorical strings\n        print(data[feature].value_counts())\n        print(\"-\" * 30)","dc8bf682":"data.describe().T","50a54321":"# This function takes the numerical column as the input and returns the boxplots\n# and histograms for the variable\ndef histogram_boxplot(feature, figsize=(15, 10), bins=None):\n    \"\"\"Boxplot and histogram combined\n    feature: 1-d feature array\n    figsize: size of fig (default (9,8))\n    bins: number of bins (default None \/ auto)\n    \"\"\"\n    f2, (ax_box2, ax_hist2) = plt.subplots(\n        nrows=2,  # Number of rows of the subplot grid= 2\n        sharex=True,  # x-axis will be shared among all subplots\n        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n        figsize=figsize,\n    )  # creating the 2 subplots\n    sns.boxplot(\n        feature, ax=ax_box2, showmeans=True, color=\"violet\"\n    )  # boxplot will be created and a star will indicate the mean value of the column\n    sns.distplot(\n        feature, kde=F, ax=ax_hist2, bins=bins, color=\"orange\"\n    ) if bins else sns.distplot(\n        feature, kde=False, ax=ax_hist2, color=\"tab:cyan\"\n    )  # For histogram\n    ax_hist2.axvline(\n        np.mean(feature), color=\"purple\", linestyle=\"--\"\n    )  # Add mean to the histogram\n    ax_hist2.axvline(\n        np.median(feature), color=\"black\", linestyle=\"-\"\n    )  # Add median to the histogram","4d0053dc":"histogram_boxplot(data.CreditScore)","42b8978a":"histogram_boxplot(data.Age)","cbb467e8":"histogram_boxplot(data.Balance)","963aacb0":"histogram_boxplot(data.EstimatedSalary)","c5608105":"# Function to create barplots that indicate percentage for each category.\n\n\ndef perc_on_bar(z):\n    total = len(data[z])  # length of the column\n    plt.figure(figsize=(15, 5))\n    # plt.xticks(rotation=45)\n    ax = sns.countplot(data[z], palette=\"Paired\")\n    for p in ax.patches:\n        percentage = \"{:.1f}%\".format(\n            100 * p.get_height() \/ total\n        )  # percentage of each class of the category\n        x = p.get_x() + p.get_width() \/ 2 - 0.05  # width of the plot\n        y = p.get_y() + p.get_height()  # hieght of the plot\n\n        ax.annotate(percentage, (x, y), size=12)  # annotate the percantage\n    plt.show()  # show the plot","18a66088":"perc_on_bar(\"Exited\")","85dcc3d7":"perc_on_bar(\"Tenure\")","ed1a3b8e":"perc_on_bar(\"NumOfProducts\")","a1956911":"perc_on_bar(\"HasCrCard\")","5bede4a8":"perc_on_bar(\"IsActiveMember\")","d58b9fa1":"plt.figure(figsize=(10, 5))\nsns.heatmap(data.corr(), annot=True, vmin=-1, vmax=1, fmt=\".2f\", cmap=\"Spectral\")\nplt.show()","e17481d3":"### Function to plot stacked bar charts for categorical columns\ndef stacked_plot(x):\n    sns.set()\n    ## crosstab\n    tab1 = pd.crosstab(x, data[\"Exited\"], margins=True).sort_values(\n        by=0, ascending=False\n    )\n    print(tab1)\n    print(\"-\" * 120)\n    ## visualising the cross tab\n    tab = pd.crosstab(x, data[\"Exited\"], normalize=\"index\").sort_values(\n        by=0, ascending=False\n    )\n    tab.plot(kind=\"bar\", stacked=True, figsize=(17, 7))\n    plt.legend(\n        loc=\"lower left\",\n        frameon=False,\n    )\n    plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n    plt.show()","d7974058":"stacked_plot(data[\"Tenure\"])","1a74ec40":"stacked_plot(data[\"NumOfProducts\"])","6461204e":"stacked_plot(data[\"HasCrCard\"])","0f730e5f":"stacked_plot(data[\"IsActiveMember\"])","c60176f2":"### Function to plot distributions and Boxplots of customers\ndef plot(x, target=\"Exited\"):\n    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n    axs[0, 0].set_title(\n        f\"Distribution of {x} of people who attrited\", fontsize=12, fontweight=\"bold\"\n    )\n    sns.distplot(data[(data[target] == 1)][x], ax=axs[0, 0], color=\"teal\")\n    axs[0, 1].set_title(\n        f\"Distribution of {x} of people who did not attrite\",\n        fontsize=12,\n        fontweight=\"bold\",\n    )\n    sns.distplot(data[(data[target] == 0)][x], ax=axs[0, 1], color=\"orange\")\n    axs[1, 0].set_title(\n        f\"Boxplot of {x} w.r.t Exited\", fontsize=12, fontweight=\"bold\"\n    )\n\n    line = plt.Line2D(\n        (0.1, 0.9), (0.5, 0.5), color=\"grey\", linewidth=1.5, linestyle=\"--\"\n    )\n    fig.add_artist(line)\n\n    sns.boxplot(\n        data[target], data[x], ax=axs[1, 0], palette=\"gist_rainbow\", showmeans=True\n    )\n    axs[1, 1].set_title(\n        f\"Boxplot of {x} w.r.t Exited - Without outliers\",\n        fontsize=12,\n        fontweight=\"bold\",\n    )\n    sns.boxplot(\n        data[target],\n        data[x],\n        ax=axs[1, 1],\n        showfliers=False,\n        palette=\"gist_rainbow\",\n        showmeans=True,\n    )  # turning off outliers from boxplot\n    plt.tight_layout(pad=4)\n    plt.show()","c4d42a4d":"plot(\"CreditScore\")","1ef594c4":"plot(\"Age\")","324abdf5":"plot(\"Balance\")","b7c87f9d":"plot(\"EstimatedSalary\")","60c2835b":"dist_cols = [\n    \"Age\",\n    \"Balance\",\n]\n\nfor col in dist_cols:\n    data[col + \"_log\"] = np.log(data[col] + 1)\n    sns.distplot(data[col + \"_log\"])\n    plt.show()\n\n","777e4bc7":"# dropping the original columns\ndata.drop(\"Age\", axis=1, inplace=True)\ndata.drop(\"Balance_log\", axis=1, inplace=True)\ndata.head()","3e928c5e":"oneHotCols = [\"Geography\", \"Gender\",\"Tenure\",\"NumOfProducts\"]\n\ndata = pd.get_dummies(data, columns=oneHotCols, drop_first=True)\ndata.head(10)","977c54eb":"data.info()","b751298d":"data1 = data.copy()\n\n# Separating target variable and other variables\nX_data = data1.drop(columns=[\"CustomerId\",\"Exited\", \"Surname\", \"RowNumber\"])\nY_data = data1[\"Exited\"]\n\nX_data.head()","f67d7238":"from sklearn.preprocessing import StandardScaler\n\n# Normalize in [-1,+1] range\n\nScale_cols = [\"CreditScore\",\"Age_log\",\"Balance\",\"EstimatedSalary\"]\n\nfor col in Scale_cols:\n    X_data['normalized'+col] = StandardScaler().fit_transform(X_data[col].values.reshape(-1,1))  \n    X_data= X_data.drop(col,axis=1)","3b231ad1":"X_train, X_test, y_train, y_test = train_test_split(\n    X_data, Y_data, test_size=0.25, random_state=1, stratify=Y_data\n)\nprint(X_train.shape, X_test.shape)","5575a6f0":"X_train.head()","e7a04143":"#initialize the model\nmodel = Sequential()","155e1d24":"# This adds the input layer (by specifying input dimension) AND the first hidden layer (units)\nmodel.add(Dense(units=22, input_dim = 22,activation='relu'))   # input of 22 columns as shown above\n# hidden layer\nmodel.add(Dense(units=24,activation='relu'))\n#Adding Dropout to prevent overfitting \nmodel.add(Dropout(0.5))\nmodel.add(Dense(20,activation='relu'))\n# model.add(Dense(24,activation='relu'))\n# Adding the output layer\n# we have an output of 1 node, which is the the desired dimensions of our output (fraud or not)\n# We use the sigmoid because we want probability outcomes\nmodel.add(Dense(1,activation='sigmoid'))                        # binary classification fraudulent or not","5335d480":"# Create optimizer with default learning rate\n# Compile the model\nmodel.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])","adee77d4":"model.summary()","fb6c2441":"#fitting the model\nhistory=model.fit(X_train,y_train,batch_size=200,epochs=20,validation_split=0.2)","d55c4fa4":"# Capturing learning history per epoch\nhist  = pd.DataFrame(history.history)\nhist['epoch'] = history.epoch\n\n# Plotting accuracy at different epochs\nplt.plot(hist['loss'])\nplt.plot(hist['val_loss'])\nplt.legend((\"train\" , \"valid\") , loc =0)","3ab1a278":"score1 = model.evaluate(X_test, y_test)","8027d688":"yprednn1=model.predict(X_test)\nyprednn1=yprednn1.round()\nprint('Neural Network with relu:\\n {}\\n'.format(\n    metrics.classification_report(yprednn1, y_test)))","0a609a62":"#initialize the model\nmodel_sig = Sequential()","2b68d7b7":"# This adds the input layer (by specifying input dimension) AND the first hidden layer (units)\nmodel_sig.add(Dense(units=22, input_dim = 22,activation='sigmoid'))   # input of 22 columns as shown above\n# hidden layer\nmodel_sig.add(Dense(units=24,activation='sigmoid'))\n#Adding Dropout to prevent overfitting \nmodel_sig.add(Dropout(0.5))\nmodel_sig.add(Dense(20,activation='sigmoid'))\n# model.add(Dense(24,activation='relu'))\n# Adding the output layer\n# we have an output of 1 node, which is the the desired dimensions of our output (fraud or not)\n# We use the sigmoid because we want probability outcomes\nmodel_sig.add(Dense(1,activation='sigmoid'))                        # binary classification fraudulent or not","f10e3427":"# Create optimizer with default learning rate\n# Compile the model\nmodel_sig.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])","a2972027":"model_sig.summary()","a9858a09":"\n#fitting the model\nhistory_sig=model_sig.fit(X_train,y_train,batch_size=15,epochs=10,validation_split=0.2)","f0dc5fd4":"# Capturing learning history per epoch\nhist_sig  = pd.DataFrame(history_sig.history)\nhist_sig['epoch'] = history_sig.epoch\n\n# Plotting accuracy at different epochs\nplt.plot(hist_sig['loss'])\nplt.plot(hist_sig['val_loss'])\nplt.legend((\"train\" , \"valid\") , loc =0)","f69ea10f":"score2 = model_sig.evaluate(X_test, y_test)","d9ccc55d":"yprednn2=model_sig.predict(X_test)\nyprednn2=yprednn2.round()\nprint('Neural Network with relu:\\n {}\\n'.format(\n    metrics.classification_report(yprednn2, y_test)))","45f5c003":"#initialize the model\nmodel_tanh = Sequential()","9f4fcffb":"# This adds the input layer (by specifying input dimension) AND the first hidden layer (units)\nmodel_tanh.add(Dense(units=30, input_dim = 22,activation='tanh'))   # input of 22 columns as shown above\n# hidden layer\nmodel_tanh.add(Dense(units=30,activation='tanh'))\n#Adding Dropout to prevent overfitting \nmodel_tanh.add(Dropout(0.5))\nmodel_tanh.add(Dense(20,activation='tanh'))\n# model.add(Dense(24,activation='relu'))\n# Adding the output layer\n# we have an output of 1 node, which is the the desired dimensions of our output (fraud or not)\n# We use the sigmoid because we want probability outcomes\nmodel_tanh.add(Dense(1,activation='sigmoid'))                        # binary classification exited or not","6df45c70":"# Create optimizer with default learning rate\n# Compile the model\nmodel_tanh.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])","d5e447e5":"model_tanh.summary()","44898ddb":"#fitting the model\nhistory_tanh=model_tanh.fit(X_train,y_train,batch_size=200,epochs=20,validation_split=0.2)","555604f2":"# Capturing learning history per epoch\nhist_tanh  = pd.DataFrame(history_tanh.history)\nhist_tanh['epoch'] = history_tanh.epoch\n\n# Plotting accuracy at different epochs\nplt.plot(hist_tanh['loss'])\nplt.plot(hist_tanh['val_loss'])\nplt.legend((\"train\" , \"valid\") , loc =0)","5d043749":"score3 = model_tanh.evaluate(X_test, y_test)","671e41d4":"yprednn3=model_tanh.predict(X_test)\nyprednn3=yprednn3.round()\nprint('Neural Network with tanh:\\n {}\\n'.format(\n    metrics.classification_report(yprednn3, y_test)))","9ea05a79":"from sklearn.metrics import auc\nfrom sklearn.metrics import plot_roc_curve","660f2704":"# calculate the fpr and tpr for all thresholds of the classification\npreds = model.predict(X_test)\nfpr, tpr, threshold = metrics.roc_curve(y_test, preds)\nroc_auc = metrics.auc(fpr, tpr)","da1f8f7d":"plt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n","3ad341f7":"\ndef make_confusion_matrix(cf,\n                          group_names=None,\n                          categories='auto',\n                          count=True,\n                          percent=True,\n                          cbar=True,\n                          xyticks=True,\n                          xyplotlabels=True,\n                          sum_stats=True,\n                          figsize=None,\n                          cmap='Blues',\n                          title=None):\n    '''\n    This function will make a pretty plot of an sklearn Confusion Matrix cm using a Seaborn heatmap visualization.\n    Arguments\n    '''\n\n\n    # CODE TO GENERATE TEXT INSIDE EACH SQUARE\n    blanks = ['' for i in range(cf.size)]\n\n    if group_names and len(group_names)==cf.size:\n        group_labels = [\"{}\\n\".format(value) for value in group_names]\n    else:\n        group_labels = blanks\n\n    if count:\n        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n    else:\n        group_counts = blanks\n\n    if percent:\n        group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten()\/np.sum(cf)]\n    else:\n        group_percentages = blanks\n\n    box_labels = [f\"{v1}{v2}{v3}\".strip() for v1, v2, v3 in zip(group_labels,group_counts,group_percentages)]\n    box_labels = np.asarray(box_labels).reshape(cf.shape[0],cf.shape[1])\n\n\n    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n    if sum_stats:\n        #Accuracy is sum of diagonal divided by total observations\n        accuracy  = np.trace(cf) \/ float(np.sum(cf))\n\n        #if it is a binary confusion matrix, show some more stats\n        if len(cf)==2:\n            #Metrics for Binary Confusion Matrices\n            precision = cf[1,1] \/ sum(cf[:,1])\n            recall    = cf[1,1] \/ sum(cf[1,:])\n            f1_score  = 2*precision*recall \/ (precision + recall)\n            stats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(\n                accuracy,precision,recall,f1_score)\n        else:\n            stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n    else:\n        stats_text = \"\"\n\n\n    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS\n    if figsize==None:\n        #Get default figure size if not set\n        figsize = plt.rcParams.get('figure.figsize')\n\n    if xyticks==False:\n        #Do not show categories if xyticks is False\n        categories=False\n\n\n    # MAKE THE HEATMAP VISUALIZATION\n    plt.figure(figsize=figsize)\n    sns.heatmap(cf,annot=box_labels,fmt=\"\",cmap=cmap,cbar=cbar,xticklabels=categories,yticklabels=categories)\n\n    if xyplotlabels:\n        plt.ylabel('True label')\n        plt.xlabel('Predicted label' + stats_text)\n    else:\n        plt.xlabel(stats_text)\n    \n    if title:\n        plt.title(title)","b5b0d0c4":"## Confusion Matrix on unsee test set\ny_pred1 = model.predict(X_test)\nfor i in range(len(y_test)):\n    if y_pred1[i]>0.4:\n        y_pred1[i]=1 \n    else:\n        y_pred1[i]=0\n\n\n\ncm2=confusion_matrix(y_test, y_pred1)\nlabels = ['True Negative','False Positive','False Negative','True Positive']\ncategories = [ 'Stayed','Exited']\nmake_confusion_matrix(cm2, \n                      group_names=labels,\n                      categories=categories, \n                      cmap='Blues')","a211830c":"### Conclusion and Key Takeaways","84925ab7":"* Attrited customers are older than the existing customers","1405adf2":"### CreditScore","622a5c90":"* Similar frequency of all salary buckets.","dc9314dc":"### Encoding Categorical variables","7a5c605c":"* Imbalanced variable. Model is likely to be biased towards value 0","c465d3cd":"### Using tanh function for improving performance","a2e6198b":"### Tenure","3899d967":"### Rescaling the data","570c3854":"### Using sigmoid activation function to see if performance improves","61feeabc":"1. *CustomerId* is the Primary key with no missing values\n2. *CreditScore* ranges from 350 to 718, with mean around 651. Since mean and median are very close, this seems like a normal distribution\n3. *Age* ranges from 18 to 92, with mean around 39. Since mean and median are very close, this seems like a normal distribution\n4. *Tenure* ranges from 0 to 10, with mean around 5. Since mean and median are same, this seems like a normal distribution\n5. *Balance* ranges from 0 to 251k, with mean around 76.5k. Std Deviation is high, depicting a skewed distribution\n6. *NumOfProducts* ranges from 1 to 4, with mean around 2. Data seems slighly skewed. Need to see the distribution to know more\n7. *HasCrCard* has values 0 or 1, more than 50% customers having 1 card\n8. *IsActiveMember* has values 0 or 1, with 50% customers having 1 card\n9. *EstimatedSalary* ranges from 12 to 200k, with mean around 100k. Since mean and median are very close, this seems like a normal distribution.\n10. *Exited* has values 0 or 1, and most customers are exited","918d9d7f":"### Adding layers [layers and activations]","764bbe89":"* Active customers churn less than dormant customers","fe3f7d5f":"### Goping ahead with Relu version of neural network","2560326c":"* over 70% customers have a credit card","290bcb7d":"* Most customers have either 1 or 2 products","531fa19d":"* No Improvement in performance","7d77217a":"### Checking for Duplicate records","07dbb417":"* over 50% customers are active customers","69bd905b":"### Transforming Skewed Variables","e9bf6a77":"* Age has normalised, but Balance variable had a better distribution in the original variable","beeb4b73":"### NumOfProducts","0f5b96fb":"### Balance","cd61da94":"* Age is right skewed","b327317c":"### HasCrCard","89d00644":"* Shape - 10000*14 \n* No missing values observed\n* Datatypes seem fine\n* Need to see unique values to identify hidden missing values","d9e51a1d":"* Almost similar distribution across all tenures, except 0 and 10","67e57e8f":"* Performance still the same","1902c230":"### EstimatedSalary","f93101ea":"## Description\n### Background and Context\n\nBusinesses like banks that provide service have to worry about the problem of 'Churn' i.e. customers leaving and joining another service provider. It is important to understand which aspects of the service influence a customer's decision in this regard. Management can concentrate efforts on the improvement of service, keeping in mind these priorities.\n\n### Objective\n\nGiven a Bank customer, build a neural network-based classifier that can determine whether they will leave or not in the next 6 months.\n\n### Data Description\n\nThe case study is from an open-source dataset from Kaggle. The dataset contains 10,000 sample points with 14 distinct features such as CustomerId, CreditScore, Geography, Gender, Age, Tenure, Balance, etc.\n\n### Data Dictionary\n\n* **CustomerId**: Unique ID which is assigned to each customer\n* **Surname**: Last name of the customer \n* **CreditScore**: It defines the credit history of the customer.  \n* **Geography**: A customer\u2019s location    \n* **Gender**: It defines the Gender of the customer   \n* **Age**: Age of the customer     \n* **Tenure**: Number of years for which the customer has been with the bank\n* **NumOfProducts**: It refers to the number of products that a customer has purchased through the bank.\n* **Balance**: Account balance\n* **HasCrCard**: It is a categorical variable that decides whether the customer has a credit card or not.\n* **EstimatedSalary**: Estimated salary \n* **isActiveMember**: It is a categorical variable that decides whether the customer is an active member of the bank or not ( Active member in the sense, using bank products regularly, making transactions, etc )\n* **Excited**: It is a categorical variable that decides whether the customer left the bank within six months or not. It can take two values \n                    **0=No ( Customer did not leave the bank )**\n                    **1=Yes ( Customer left the bank )**","775e0db8":"* Similar distribution of the target variable across the HasCrCard variable","a712b62b":"* performance with tanh activation is slightly better than sigmoid, still close to Relu","dde4612c":"* Data is fairly clean\n* Target Variable is Excited and CustomerID is the Primary Key\n* Further analysis required","65585a89":"Though it is very difficult to derive insights from the neural network, let us try and understand the same from the EDA done, to understand where the areas of focus should be for the business\n\n1. Try to cross sell more products to improve customer stickiness, as single product holder are shown to be more likely to churn\n2. Run campaigns to increase customer spends, and keeping them engaged\/active\n3. Try to use dedicated RMs and give better service to high balance customers, maybe create an elite segment to retain them","6b80cc6f":"* No hidden missing\/invalid values found\n* Surname has too many values and does not look like a relevant variable for the model building. Will be dropped from training dataset during the split","f572f7db":"### Summary of the dataset","109599e4":"## Training [Forward pass and Backpropagation]","01eea7e8":"### Exited(Target Variable)","6449a7ec":"### Load data","8ec2b6e3":"# Bank Churn Prediction","4a2988cd":"### To see distribution of target across the variables","7b17ea63":"### Evaluation","c3d5ff6a":"## EDA","4fd1fd2d":"### Splitting Data into Training, Validation and Test Set","b100b95d":"* On an average similar Credit Score for both attrited and existing customers","687cb8ed":"* Normal distribution, with few outliers towards the left. Small peaks at the right end","0f6d0785":"### IsActiveMember","6b9d9b00":"## Bivariate analysis","4738c184":"### Model compile [optimizers and loss functions]","08457e9e":"### Checking for Duplicates in CustomerID","8ca1a2b4":"* Similar trend of the target variable across various salary buckets","dbdfa41d":"### checking unique values counts in the Object variables","2f27dadb":"* No Duplicates found","84358fea":"* Since over 96% customers have either 1 or 2 products, above trend shows single product holding customers churn more","4e52e31f":"* The optimal threshold can be seen around 0.4","6eb2a8e2":"* No duplicate records found","e158ff3c":"* Variables are independent. No correlation seen, even for the target variable","4caa02f1":"* Higher Balance customers are exiting more than the lower balance customers. Probably takeovers by other banks.","c03c0909":"### Univariate Analysis","69573d69":"### Creating a model","7fc77977":"### Loading Libraries","3e5a7bc3":"### View the first and last 5 rows of the dataset.","2415f519":"### Age","869f6c70":"### ROC-AUC Curve","2aea377e":"* Normal distribution, except a peak in the left, causing the skewness","7ae5d2f4":"* Similar distribution of target across all the tenures"}}