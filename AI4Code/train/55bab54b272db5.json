{"cell_type":{"028a6fd9":"code","fc0ed92e":"code","0a06cfd0":"code","005025bc":"code","744aa7a3":"code","ff7f5778":"code","35ea9cee":"code","e3e1efbf":"code","e6e55ef0":"code","6fc0ad29":"code","c0d0221e":"code","b31e2251":"code","ef0c85f4":"code","97ef0528":"code","770b4f57":"code","c89f53e8":"code","76b8d9a2":"code","6eafd260":"code","85799e4a":"code","c8a4248c":"code","c5eb5793":"code","888617b8":"code","3e5c3ef9":"code","e3d56fd0":"code","6e1aefed":"code","811da67e":"code","4ea1f63f":"code","e21a557b":"code","1f4cccbe":"code","421801cb":"code","24a39297":"code","e388fa3c":"code","c8468f81":"code","9aeba3f3":"code","80c0a698":"code","ac6d68fc":"code","e3c70bb1":"code","94e26727":"code","bb2aa251":"markdown","af010a42":"markdown","a5e02d86":"markdown","8c012aa6":"markdown","99dfb34f":"markdown","f97e2cf3":"markdown","885ca6f9":"markdown","16f05e8b":"markdown","44bbeabe":"markdown","594091b5":"markdown","a66329b7":"markdown","067bc989":"markdown"},"source":{"028a6fd9":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport keras","fc0ed92e":"data = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')","0a06cfd0":"data.head()","005025bc":"data.isnull().sum()","744aa7a3":"data.describe()","ff7f5778":"print('No Frauds', round(data['Class'].value_counts()[0]\/len(data) * 100,2), '% of the dataset')\nprint('Frauds', round(data['Class'].value_counts()[1]\/len(data) * 100,2), '% of the dataset')\nprint('Fraudulent Transactions', data['Class'].value_counts()[1])","35ea9cee":"import seaborn as sns\n\ncolors = [\"#0101DF\", \"#DF0101\"]\n\nsns.countplot('Class', data=data, palette=colors)","e3e1efbf":"from sklearn.preprocessing import StandardScaler, RobustScaler\n\n# RobustScaler is less prone to outliers.\n\nstd_scaler = StandardScaler()\nrob_scaler = RobustScaler()\n\ndata['scaled_amount'] = rob_scaler.fit_transform(data['Amount'].values.reshape(-1,1))\ndata['scaled_time'] = rob_scaler.fit_transform(data['Time'].values.reshape(-1,1))\n\ndata.drop(['Time','Amount'], axis=1, inplace=True)\n\n","e6e55ef0":"amount = data['scaled_amount']\ntime = data['scaled_time']\n\ndata.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\ndata.insert(0, 'amount', amount)\ndata.insert(1, 'time', time)","6fc0ad29":"data.head()","c0d0221e":"data = data.sample(frac=1)\n\nfraud_data = data.loc[data['Class']==1]\nnfraud_data = data.loc[data['Class']==0][:492]\n\nnormal_distributed_df = pd.concat([fraud_data, nfraud_data])\n\n# Shuffle dataframe rows\nndata = normal_distributed_df.sample(frac=1, random_state=42)\n\nndata.head()","b31e2251":"print('Fraudulent Transactions', data['Class'].value_counts()[1])\nprint('Non-Fraudulent Transactions', data['Class'].value_counts()[0])\ncolors = [\"#0101DF\", \"#DF0101\"]\n\nsns.countplot('Class', data=ndata, palette=colors)","ef0c85f4":"X= data.iloc[:, data.columns != 'Class']\ny = data.iloc[:, data.columns == 'Class']","97ef0528":"X.corrwith(ndata.Class).plot.bar(figsize= (20,10),title=\"Corr\", fontsize=10, grid=True)","770b4f57":"plt.figure(figsize = (20,10))\nsns.heatmap(ndata.corr(), annot=True,cmap=\"YlGnBu\")","c89f53e8":"dataframe = pd.DataFrame(data=ndata)","76b8d9a2":"dataframe","6eafd260":"X= dataframe.iloc[:, ndata.columns != 'Class']\ny = dataframe.iloc[:, ndata.columns == 'Class']","85799e4a":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.25, random_state=1)","c8a4248c":"X_train = np.array(X_train)\nX_test = np.array(X_test)\ny_train = np.array(y_train)\ny_test = np.array(y_test)","c5eb5793":"from sklearn.tree import DecisionTreeClassifier\nclassifier = DecisionTreeClassifier(criterion = 'gini', random_state= 0 )\nclassifier.fit(X_train, y_train.ravel())","888617b8":"y_pre = classifier.predict(X_test)","3e5c3ef9":"from sklearn.metrics import classification_report, confusion_matrix\ncm_grid = confusion_matrix(y_test,y_pre)\nsns.heatmap(cm_grid, annot=True)","e3d56fd0":"print(classification_report(y_test,y_pre))","6e1aefed":"classifier.score(X_test,y_test)","811da67e":"from sklearn import tree\ntext_representation = tree.export_text(classifier)\nprint(text_representation)","4ea1f63f":"with open(\"decistion_tree.log\", \"w\") as fout:\n    fout.write(text_representation)","e21a557b":"fig = plt.figure(figsize=(100,40))\n_ = tree.plot_tree(classifier, filled=True)","1f4cccbe":"from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(n_estimators= 10, criterion= 'entropy', random_state=0)\nclf.fit(X_train, y_train.ravel())","421801cb":"y_pred = clf.predict(X_test)","24a39297":"from sklearn.metrics import classification_report, confusion_matrix\ncm_grid = confusion_matrix(y_test,y_pred)\nsns.heatmap(cm_grid, annot=True)","e388fa3c":"print(classification_report(y_test,y_pred))","c8468f81":"clf.score(X_test,y_test)","9aeba3f3":"from sklearn.neighbors import KNeighborsClassifier\nclsf= KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)\nclsf.fit(X_train, y_train.ravel())","80c0a698":"y_pr = clsf.predict(X_test)\ncm_grid = confusion_matrix(y_test,y_pr)\nsns.heatmap(cm_grid, annot=True)","ac6d68fc":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\n\n\n#Initializing ANN\nclassifier = Sequential()\n\n\n#Input Layer\nclassifier.add(Dense(30, activation='relu'))\n#2nd layer\nclassifier.add(Dense(16, activation='relu'))\nclassifier.add(Dense(16, activation='relu'))\n\n#Output layer\nclassifier.add(Dense(1, activation='sigmoid'))\n\n#Compling the ANN\nclassifier.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])\n\n#Fitting the dataset into ANN\nclassifier.fit(X_train, y_train, batch_size=100, epochs=100)\n\n\n#Predicting the results\ny_pred = classifier.predict(X_test)\n\n\n\n\n\n\n","e3c70bb1":"classifier.summary()","94e26727":"import pandas as pd\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\n\n\ndef plot_densities(data):\n    '''\n    Plot features densities depending on the outcome values\n    '''\n    # change fig size to fit all subplots beautifully \n    rcParams['figure.figsize'] = 15, 60\n\n    # separate data based on outcome values \n    outcome_0 = data[data['Class'] == 0]\n    outcome_1 = data[data['Class'] == 1]\n\n    # init figure\n    fig, axs = plt.subplots(30, 1)\n    fig.suptitle('Features densities for different outcomes 0\/1')\n    plt.subplots_adjust(left = 0.25, right = 0.9, bottom = 0.1, top = 0.95,\n                        wspace = 0.2, hspace = 0.9)\n\n    # plot densities for outcomes\n    for column_name in names[:-1]: \n        ax = axs[names.index(column_name)]\n        #plt.subplot(4, 2, names.index(column_name) + 1)\n        outcome_0[column_name].plot(kind='density', ax=ax, subplots=True, \n                                    sharex=False, color=\"red\", legend=True,\n                                    label=column_name + ' for Outcome = 0')\n        outcome_1[column_name].plot(kind='density', ax=ax, subplots=True, \n                                     sharex=False, color=\"green\", legend=True,\n                                     label=column_name + ' for Outcome = 1')\n        ax.set_xlabel(column_name + ' values')\n        ax.set_title(column_name + ' density')\n        ax.grid('on')\n    plt.show()\n    fig.savefig('densities.png')\n\n# load your data \n#data  = pd.read_csv('diabetes.csv')\nnames = list(data.columns)\n\n# plot correlation & densities\nplot_densities(ndata)","bb2aa251":"Checking for missing values in the dataset","af010a42":"### ****Using Random forest classifier","a5e02d86":"### ****KNN","8c012aa6":"From the last Epoch the accuracy of ANN is 60% which is not acceptable.","99dfb34f":"Training our model on the original dataset for creating a predictive model would lead to a lot of errors due to overfitting, misclassifing the fraudulent transactions as legitimate activity due to skewed data. \n\n### Preparing the dataset:\n1. Scaling the 'Time' and 'Amount' attributes\n2. Re-distribution of the dataset for equal class values","f97e2cf3":"### Credit Card Fraud detection using machine learning\nLooking at the target class, the dataset can be categorized into a classification problem. Hence, I've used decision tree classifier and ANN for classification.\n\nThe datasets contains transactions made by credit cards in September 2013 by european cardholders.\nThis dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\nIt contains only numerical input variables which are the result of a PCA transformation. The original features and background data cannot be provided due to confidentiality issues. \n\nMy approach to solving this problem is standardising the 'amount' and 'time' variables and equal distribution of Class of Fraud and Non- Fraudulent transactions. Then classification algorithms can be applied.\n\nHowever, we can lose a lot of information when the Non-Fraudulent transactions are ignored. Here, bagging method can be useful to improve the model.","885ca6f9":"### Using decision tree classifier","16f05e8b":"Visualizing the Decision tree classifier\n\nLooks beautiful!","44bbeabe":"## from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.25, random_state=1)","594091b5":"### Using ANN","a66329b7":"The amount and time attributes are not scaled with the rest of the features in the dataset. These can be scaled using standard scaler.\nHowever, the classes are heavily skewed.\n","067bc989":"From the above plots it is observed that greater overlapping between the parameters is associated with low correlation. "}}