{"cell_type":{"36b48514":"code","99e1dd9b":"code","c3316612":"code","95275324":"code","785e8abb":"code","0ae0021d":"code","9874e0ae":"code","c38fe371":"code","345663db":"code","b6adecb1":"code","b440744e":"code","d8c80f32":"code","b6f34540":"code","20a0551e":"code","9a767bc8":"code","66db7a74":"code","15b3cacd":"code","02287e46":"code","a1988d08":"code","51fc6729":"code","c60fb02b":"code","bc297603":"code","3468d9c9":"code","62a851cc":"code","6dfe342f":"code","8b3b14fa":"code","a8d80afa":"code","2024af6c":"code","bd48e60d":"markdown","42b2eaad":"markdown","fa83650d":"markdown","b77eb50f":"markdown","40f27a5e":"markdown","dad8f8b4":"markdown","962c8f7c":"markdown","578dfe6f":"markdown","6390071a":"markdown","068d22d3":"markdown","19ab9e51":"markdown","1440c6c7":"markdown","3784efaa":"markdown","42d1c8c6":"markdown","8fe8f97f":"markdown","a1e4f546":"markdown","9b9b51ad":"markdown","fb52db17":"markdown","d9ed1d8f":"markdown","020fb97c":"markdown","0508f169":"markdown","10eb0747":"markdown","5d45fd31":"markdown","6b2e1298":"markdown","e2cc4ea1":"markdown","d1c04971":"markdown","d6306249":"markdown","55aa97ea":"markdown","c8303812":"markdown","1ea92bbd":"markdown","a0348199":"markdown","aac8ab16":"markdown","eb52ecf3":"markdown"},"source":{"36b48514":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV, learning_curve, cross_val_score","99e1dd9b":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntestid = test['PassengerId']\ntrain_len = len(train)\n\ny = train['Survived']\nX = pd.concat([train, test])","c3316612":"def remove_outlier(df_in, col_name):\n    \n    mean = df_in[col_name].mean()\n    q1 = df_in[col_name].quantile(0.25)\n    q3 = df_in[col_name].quantile(0.75)\n    iqr = q3-q1\n    fence_low  = q1-1.5*iqr\n    fence_high = q3+1.5*iqr\n    for ind in range(120):\n        if (df_in[col_name].iloc[ind]>fence_high) or (df_in[col_name].iloc[ind]<fence_low):\n            df_in[col_name].iloc[ind] = mean\n            \n    return df_in[col_name]\n\ndef graphics(df, y, features):\n    for ind in features:\n        plt.boxplot(x=df[ind], vert=False)\n        plt.grid(True)\n        plt.title(ind + \" boxplot\")\n        plt.xlabel(ind+' value')\n        plt.ylabel('Survived')\n        plt.show()\n\n\n#graphics(train, y, [\"Pclass\",\"SibSp\",\"Fare\"])\n\n#train['Age'] = remove_outlier(train, \"Age\")\n#train['SibSp'] = remove_outlier(train, \"SibSp\")\n#train['Parch'] = remove_outlier(train, \"Parch\")\n#train['Fare'] = remove_outlier(train, \"Fare\")","95275324":"sns.heatmap(train[[\"Survived\",\"Pclass\",\"Age\",\"SibSp\",\"Parch\",\"Fare\"]].corr(), annot=True)","785e8abb":"sns.FacetGrid(train, col='Survived').map(sns.distplot, \"Age\")","0ae0021d":"sns.catplot(x=\"SibSp\", y=\"Survived\", data=train, kind=\"bar\")","9874e0ae":"sns.catplot(x=\"Pclass\", y=\"Survived\", data=train, kind=\"bar\")","c38fe371":"sns.catplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=train, kind=\"bar\")","345663db":"sns.catplot(x=\"Parch\", y=\"Survived\", data=train, kind=\"bar\")","b6adecb1":"sns.barplot(x=\"Sex\", y=\"Survived\", data=train)","b440744e":"sns.catplot(x=\"Embarked\", y=\"Survived\", data=train, kind=\"bar\")","d8c80f32":"X['Title'] = X['Name']\n\nfor name_string in X['Name']:\n    X['Title'] = X['Name'].str.extract('([A-Za-z]+)\\.', expand=True)\n\nmapping = {'Mlle': 'Miss', 'Major': 'Mr', 'Col': 'Mr', 'Sir': 'Mr', 'Don': 'Mr', 'Mme': 'Miss',\n          'Jonkheer': 'Mr', 'Lady': 'Mrs', 'Capt': 'Mr', 'Countess': 'Mrs', 'Ms': 'Miss', 'Dona': 'Mrs'}\n\nX.replace({'Title': mapping}, inplace=True)\ntitles = ['Dr', 'Master', 'Miss', 'Mr', 'Mrs', 'Rev']\n\nfor title in titles:\n    age_to_impute = X.groupby('Title')['Age'].median()[titles.index(title)]\n    X.loc[(X['Age'].isnull()) & (X['Title'] == title), 'Age'] = age_to_impute\n\ngr = sns.countplot(x=\"Title\",data=X)\ngr.set_xticklabels(gr.get_xticklabels(), rotation=75)\nplt.show(gr)\nsns.catplot(x=\"Title\",y=\"Survived\",data=X.iloc[:train_len],kind=\"bar\")\nplt.show(sns)    \n\nX.drop('Title', axis = 1, inplace = True)","b6f34540":"X['Family_Size'] = X['Parch'] + X['SibSp']","20a0551e":"X['Last_Name'] = X['Name'].apply(lambda x: str.split(x, \",\")[0])\nX['Fare'].fillna(X['Fare'].mean(), inplace=True)\n\nDEFAULT_SURVIVAL_VALUE = 0.5\nX['Family_Survival'] = DEFAULT_SURVIVAL_VALUE\n\nfor grp, grp_df in X[['Survived','Name', 'Last_Name', 'Fare', 'Ticket', 'PassengerId',\n                           'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['Last_Name', 'Fare']):\n    \n    if (len(grp_df) != 1):\n        # A Family group is found.\n        for ind, row in grp_df.iterrows():\n            smax = grp_df.drop(ind)['Survived'].max()\n            smin = grp_df.drop(ind)['Survived'].min()\n            passID = row['PassengerId']\n            if (smax == 1.0):\n                X.loc[X['PassengerId'] == passID, 'Family_Survival'] = 1\n            elif (smin==0.0):\n                X.loc[X['PassengerId'] == passID, 'Family_Survival'] = 0\n                \nfor _, grp_df in X.groupby('Ticket'):\n    if (len(grp_df) != 1):\n        for ind, row in grp_df.iterrows():\n            if (row['Family_Survival'] == 0) | (row['Family_Survival']== 0.5):\n                smax = grp_df.drop(ind)['Survived'].max()\n                smin = grp_df.drop(ind)['Survived'].min()\n                passID = row['PassengerId']\n                if (smax == 1.0):\n                    X.loc[X['PassengerId'] == passID, 'Family_Survival'] = 1\n                elif (smin==0.0):\n                    X.loc[X['PassengerId'] == passID, 'Family_Survival'] = 0\n                        \n\nsns.catplot(x=\"Family_Size\",y=\"Survived\",data = X.iloc[:train_len],kind=\"bar\")","9a767bc8":"X['Fare'].fillna(X['Fare'].median(), inplace = True)\n\n# Making Bins\nX['FareBin'] = pd.qcut(X['Fare'], 5)\n\nlabel = LabelEncoder()\nX['FareBin_Code'] = label.fit_transform(X['FareBin'])\n\nX.drop(['Fare'], 1, inplace=True)","66db7a74":"X['AgeBin'] = pd.qcut(X['Age'], 4)\n\nlabel = LabelEncoder()\nX['AgeBin_Code'] = label.fit_transform(X['AgeBin'])\n\nX.drop(['Age'], 1, inplace=True)","15b3cacd":"X['Sex'].replace(['male','female'],[0,1],inplace=True)\n\n\nX.drop(['Name', 'PassengerId', 'SibSp', 'Parch', 'Ticket', 'Cabin',\n               'Embarked', 'Last_Name', 'FareBin', 'AgeBin', 'Survived'], axis = 1, inplace = True)","02287e46":"X_train = X[:train_len]\nX_test = X[train_len:]\n\ny_train = y","a1988d08":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","51fc6729":"kfold = StratifiedKFold(n_splits=8)","c60fb02b":"RFC = RandomForestClassifier()\n\nrf_param_grid = {\"max_depth\": [None],\n              \"max_features\": [3,\"sqrt\", \"log2\"],\n              \"min_samples_split\": [2, 4],\n              \"min_samples_leaf\": [5, 7],\n              \"bootstrap\": [False, True],\n              \"n_estimators\" :[200, 500],\n              \"criterion\": [\"gini\", \"entropy\"]}\n\nrf_param_grid_best = {\"max_depth\": [None],\n              \"max_features\": [3],\n              \"min_samples_split\": [4],\n              \"min_samples_leaf\": [5],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[200],\n              \"criterion\": [\"gini\"]}\n\ngs_rf = GridSearchCV(RFC, param_grid = rf_param_grid_best, cv=kfold, scoring=\"roc_auc\", n_jobs= 4, verbose = 1)\n\ngs_rf.fit(X_train, y_train)\nRFC.fit(X_train, y_train)\nrf_best = gs_rf.best_estimator_\nprint(f'RandomForest GridSearch best params: {gs_rf.best_params_}')\nprint(f'RandomForest GridSearch best score: {gs_rf.best_score_}')","bc297603":"KNN = KNeighborsClassifier()\n\nknn_param_grid = {'algorithm': ['auto'],\n                 'weights': ['uniform', 'distance'], \n                 'leaf_size': [20, 25, 30], \n                 'n_neighbors': [12, 14, 16]}\ngs_knn = GridSearchCV(KNN, param_grid = knn_param_grid, cv=kfold, scoring = \"roc_auc\", n_jobs= 4, verbose = 1)\n\ngs_knn.fit(X_train, y_train)\nKNN.fit(X_train, y_train)\n\nknn_best = gs_knn.best_estimator_\nprint(f'KNN GridSearch best params: {gs_knn.best_params_}')\nprint(f'KNN GridSearch best score: {gs_knn.best_score_}')","3468d9c9":"GB = GradientBoostingClassifier()\n\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [1000],\n              'learning_rate': [0.02, 0.05],\n              'min_samples_split': [15, 20, 25],\n              'max_depth': [4, 6],\n              'min_samples_leaf': [50, 60],\n              'max_features': [\"sqrt\"] \n              }\n\ngb_param_grid_best = {'loss' : [\"deviance\"],\n              'n_estimators' : [1000],\n              'learning_rate': [0.02],\n              'min_samples_split': [25],\n              'max_depth': [4],\n              'min_samples_leaf': [60],\n              'max_features': [\"sqrt\"] \n              }\n\ngs_gb = GridSearchCV(GB, param_grid = gb_param_grid_best, cv=kfold, scoring=\"roc_auc\", n_jobs= 4, verbose = 1)\n\ngs_gb.fit(X_train,y_train)\nGB.fit(X_train, y_train)\n\ngb_best = gs_gb.best_estimator_\nprint(f'GradienBoost GridSearch best params: {gs_gb.best_params_}')\nprint(f'GradienBoost GridSearch best score: {gs_gb.best_score_}')","62a851cc":"XGB = XGBClassifier()\n\nxgb_param_grid = {'learning_rate':[0.05, 0.1], \n                  'reg_lambda':[0.3, 0.5],\n                  'gamma': [0.8, 1],\n                  'subsample': [0.8, 1],\n                  'max_depth': [2, 3],\n                  'n_estimators': [200, 300]\n              }\n\nxgb_param_grid_best = {'learning_rate':[0.1], \n                  'reg_lambda':[0.3],\n                  'gamma': [1],\n                  'subsample': [0.8],\n                  'max_depth': [2],\n                  'n_estimators': [300]\n              }\n\ngs_xgb = GridSearchCV(XGB, param_grid = xgb_param_grid_best, cv=kfold, scoring=\"roc_auc\", n_jobs= 4, verbose = 1)\n\ngs_xgb.fit(X_train,y_train)\nXGB.fit(X_train, y_train)\n\nxgb_best = gs_xgb.best_estimator_\nprint(f'XGB GridSearch best params: {gs_xgb.best_params_}')\nprint(f'XGB GridSearch best score: {gs_xgb.best_score_}')","6dfe342f":"knn1 = KNeighborsClassifier(algorithm='auto', leaf_size=26, metric='minkowski', \n                           metric_params=None, n_jobs=1, n_neighbors=6, p=2, \n                           weights='uniform')\n\nknn1.fit(X_train, y_train)","8b3b14fa":"def CVScore(classifiers):\n    cv_score = []\n    names = []\n    for n_classifier in range(len(classifiers)):\n        name = classifiers[n_classifier][0]\n        model = classifiers[n_classifier][1]\n        cv_score.append(cross_val_score(model, X_train, y_train, scoring = \"roc_auc\", cv = kfold, n_jobs=4))\n        names.append(name)\n    cv_means = []\n    for cv_result in cv_score:\n        cv_means.append(cv_result.mean())\n        \n    cv_res = pd.DataFrame({\"Model\":names,\"CVMeans\":cv_means})\n    cv_res.sort_values(\"CVMeans\", axis = 0, ascending = True, inplace = True)\n\n    print('----------CrossVal scores---------\\n', cv_res)\n\n    \nbest_class = [(\"RandomForest\", rf_best), (\"GradientBoost\", gb_best), (\"KNN\", knn_best), (\"XGB\", xgb_best), (\"KNN new\", knn1)]\ndef_class = [(\"RandomForest\", RFC), (\"GradientBoost\", GB), (\"KNN\", KNN), (\"XGB\", XGB), (\"KNN new\", knn1)]\n\n#CVScore(def_class)\nCVScore(best_class)","a8d80afa":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    plt.show()\n\nplot_learning_curve(rf_best,\"RF learning curves\", X_train, y_train, cv=kfold)\nplot_learning_curve(xgb_best,\"XGB learning curves\", X_train, y_train, cv=kfold)\nplot_learning_curve(gb_best,\"GradientBoosting learning curves\", X_train, y_train, cv=kfold)\nplot_learning_curve(knn_best,\"KNN learning curves\", X_train, y_train, cv=kfold)\nplot_learning_curve(knn1,\"KNN (NEW) learning curves\", X_train, y_train, cv=kfold)","2024af6c":"y_pred = knn1.predict(X_test)\n\ntest_Survived = pd.Series(y_pred, name=\"Survived\")\nresults = pd.concat([testid,test_Survived],axis=1)\nresults.to_csv(\"submit.csv\",index=False)","bd48e60d":"**SibSp**","42b2eaad":"The age distribution is probably a Gaussian distribution. It's good.\n\nThe graphs show a peak in survival among young people. Unfortunately, elderly people have less chances to survive. A jump in survival in children is also visible, which is quite logical. Most likely they were saved in the first place.","fa83650d":"**Mapping SEX and dropping columns**","b77eb50f":"**Sex**","40f27a5e":"**Family Sirvival**\n\nThis function is from the [S.Xu kernel](https:\/\/www.kaggle.com\/shunjiangxu\/blood-is-thicker-than-water-friendship-forever), it groups families and people with the same tickets and explores the information. You can read in more detail in the original. Thanks to him for the work done!\n\n**Spoiler:** this feature that helped improve the current result quite well. It is quite complicated in ideological understanding, but it is worth it.","dad8f8b4":"**Pclass**","962c8f7c":"**Parch**","578dfe6f":"Only Fare feature seems to have a significative correlation with the survival probability.  It doesn't mean that the other features are not usefull. All other hidden dependencies are found on the graphs below.","6390071a":"# Outliers: profit or not?\n\nSince outliers can have a significant impact on the forecast, I decided to find them, remove and compare the results with them and without them. I used numerical values (Age, SibSp, Sarch and Fare).\n\nThe function ***graphics*** is responsible for the search for outliers and the function ***remove_outlier*** is for the removal.\n\n**Spoiler:** results without outliers got worse. So in the problems of classification with this it is worth being very careful.","068d22d3":"# Feature analysis\n\n","19ab9e51":"# Engineering features\n\n**Title**\n\nI make a Title feature for imputing ages more precisely. Median is used because ages distribution is not always normal, so it's generally preferred over mean. But I don't think this matters a lot, you can use mean too. Next, I don't use Title feature for fitting models so it's discarded.","1440c6c7":"From the graph, it can be suggested that people with fewer relatives are more likely to survive. We will return to this later and examine in more detail.","3784efaa":"**Family Size**\n\nEverything is simple here)","42d1c8c6":"**Embarked**","8fe8f97f":"**Model Comparison**\n\nIn the course of work, there were many models with roc_auc of about 0.9-0.93, but when testing they mostly showed lower results. This does not mean that they are worse. Perhaps the dataset is not well formed.","a1e4f546":"A passenger arriving from Cherbourg (C) seems to be more likely to survive. But I think that this will not be a key function for our prediction.","9b9b51ad":"If you found this notebook helpful or you just liked it , some upvotes would be very much appreciated - **That will keep me motivated** :)","fb52db17":"The passenger survival is not the same in the 3 classes. First class passengers have more chance to survive than second class and third class passengers.  ","d9ed1d8f":"**Age**","020fb97c":"*The model below turned out to be a trump card from all over my set. I borrowed it from one of my colleagues. You can play around with the parameters, maybe you can do it even better.*","0508f169":"**Age Bin**\n\n*Similarly*","10eb0747":"# Some conclusions:#\n\nI speak only from my experience on the Titanic, so the following may not be true for you.\n\n* The **Has_Cabin** function does not help. I developed a function with 0 if the passenger does not have a Cabin (NaN), and with 1 if he received.\n* The **Deck** function does not help. Based on the letters found in the \u201cCabin\u201d column, we can develop a \u201cDeck\u201d function indicating which deck (A - G, T or U for \u201cUnknown\u201d) was on the passenger. But it is rather noisy, it does not help the score.\n* **Embarked** does not help. I tried to add it, but the results were only worse. This does not affect the chances of survival.\n* Some algorithms may work better if you turn categorical functions into ordinal ones. Pluses are more accurate in some cases, minuses - you lose the connection between classes.\n* Standard Scaler is our helper. This helps to increase the score. Scaling functions are useful for many ML algorithms, they really increase their estimation.\n* It doesn't make much sense to scale functions that are already 0 or 1, like Sex, but now I scale them all. You can try to select the functions to scale.","5d45fd31":"**Fare Bin**\n\nAt first I tried get dummies, but the results were bad. Later I saw such an option in one of the kernels, implemented it and got a boost to the rating.","6b2e1298":"**Plot learning curves**\n\nLearning curves are a good way to see the overfitting effect on the training set and the effect of the training size on the accuracy.","e2cc4ea1":"**Making submission**","d1c04971":"This trend is conserved when we look at both male and female passengers.","d6306249":"# Classifiers\n\nI compared several popular classifiers and estimated the average accuracy of each of them using cross-validation. For each model, I selected the parameters with the help of GridSarch.\n\n* RandomForest\n* KNeighbors\n* XGB\n* KNeighbors","55aa97ea":"Small families have more chance to survive, more than single (Parch 0), medium (Parch 3,4) and large families (Parch 5,6). ","c8303812":"Men are less likely to survive than women. This is quite logical, because women were also allowed to go forward on rescue boats. Perhaps this will play a key role later.","1ea92bbd":"# Intro\n\nThis is my second Kaggle kernel. I will show some analysis functions, and then concentrate on developing new functions, which then will probably help to achieve a good result. The last part deals with modeling and comparing various models.","a0348199":"**Scaling features**","aac8ab16":"# Training","eb52ecf3":"**Make train and test sets**"}}