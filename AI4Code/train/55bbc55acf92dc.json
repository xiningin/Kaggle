{"cell_type":{"077cc5f8":"code","855107aa":"code","24f1cede":"code","2c33793b":"code","fc4d6ff8":"code","dbd9a6e9":"code","63bec17c":"code","3531d2df":"code","5fc98d06":"code","5cde077e":"code","31d460a2":"code","1486d4be":"code","10ce5ae1":"code","00aaa28a":"code","6a1285cd":"code","3cf7fc93":"code","0c7e0cac":"code","2c4e10ba":"code","8f2e136f":"code","9e672d9e":"code","3820324d":"code","56f31123":"code","e0964514":"code","fcc2a99a":"code","813b59d8":"code","af441c61":"code","a5293734":"code","d1396d24":"code","c36e9f29":"code","c6d70f47":"code","4c132531":"code","85330dbb":"code","00294243":"code","215af7c0":"code","d80fd90a":"code","29fec968":"code","2984f64f":"code","ca22d0ba":"code","0517dd6e":"code","d81750d6":"code","991c4907":"code","cd0bfb9d":"code","4233aaf8":"code","fb91de3a":"code","7559656d":"code","795c5775":"code","7012daa5":"code","d19aeb2c":"code","d8f446a3":"code","f912f822":"code","ec38bb99":"code","7273f9c2":"code","ed6a3fb9":"code","4634e39d":"code","ce90e2b2":"code","97ee6496":"code","036d54ee":"code","934a1899":"code","8941802d":"code","2f1cbd07":"code","4c54a270":"code","622fa983":"code","7252b8b9":"code","507a8624":"code","ea9aca8a":"code","427ceb8c":"code","808d2fb6":"code","5eac2ff7":"code","3aef738e":"code","4b630c79":"code","e3e37a0f":"code","90cfb46b":"code","f80ef146":"markdown","b393888b":"markdown","0210b122":"markdown","b2e94cf1":"markdown","f588f95f":"markdown","226ff1dc":"markdown","0ce8832b":"markdown","d62db547":"markdown","fa472b03":"markdown","b68fd5e4":"markdown","41b320bb":"markdown","734a3ab9":"markdown","f98858f6":"markdown","e5f51f46":"markdown","c73faa2d":"markdown","3d0a60e4":"markdown","5e0626b4":"markdown","411f7288":"markdown","7f8ec06f":"markdown","7811adaf":"markdown","9d251cc8":"markdown","7802a861":"markdown","c811e82d":"markdown","5918d1a1":"markdown","d4706218":"markdown","e5af7d66":"markdown","953c7109":"markdown","21a33476":"markdown","ebca0642":"markdown","49e64117":"markdown","01a9817b":"markdown","ce3e5e9d":"markdown","7e37ece7":"markdown","5ccbec39":"markdown","d07fb303":"markdown","4e580e2f":"markdown","d5bd5c40":"markdown","705c834c":"markdown","8b6e6744":"markdown","76997ade":"markdown","29e1977c":"markdown","31f06e8f":"markdown","3686180e":"markdown","2faca7bf":"markdown","fd33c780":"markdown","1f2f9d39":"markdown","c10464c4":"markdown","f9969a48":"markdown","778ec337":"markdown","893df943":"markdown","54019256":"markdown","1a21570e":"markdown","28170d9b":"markdown","72a58f89":"markdown","49cf71ff":"markdown","1182dc55":"markdown","e7cd5c09":"markdown","8df1ec23":"markdown","5c680374":"markdown","a41b8a6c":"markdown","5493ea29":"markdown","d4e233d8":"markdown","fa559be7":"markdown","f638b4fb":"markdown","d6b8f2fb":"markdown","e28ffb0e":"markdown","158868db":"markdown","1b98c9ad":"markdown","e0b43961":"markdown","cef14721":"markdown","0496803c":"markdown","0d501e91":"markdown","dc8d448a":"markdown","fb7d9eb5":"markdown","0823742e":"markdown","340cd0fc":"markdown","9a72e1a6":"markdown"},"source":{"077cc5f8":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","855107aa":"! pip install pytorch-lightning","24f1cede":"import pandas as pd\npd.options.mode.chained_assignment = None  # default='warn'\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\n\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.impute import KNNImputer\n\nfrom sklearn.decomposition import PCA\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset,DataLoader\nimport torch.optim as optim\n\n\nimport optuna\nfrom optuna.trial import TrialState\n\n##BONUS: PYTORCH LIGHTNING\nfrom pytorch_lightning.core.lightning import LightningModule\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\n\n\n\nfrom sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score, auc, roc_curve, accuracy_score\n\nseed=42 \n\n\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\"axes\", labelweight=\"bold\", labelsize=\"large\", titleweight=\"bold\", titlesize=14, titlepad=10)\n","2c33793b":"def get_scores(y, y_pred):\n    data={'Accuracy': np.round(accuracy_score(y, y_pred),2),\n    'Precision':np.round(precision_score(y, y_pred),2),\n    'Recall':np.round(recall_score(y, y_pred),2),\n    'F1':np.round(f1_score(y, y_pred),2),\n    'ROC AUC':np.round(roc_auc_score(y, y_pred),2)}\n    scores_df = pd.Series(data).to_frame('scores')\n    return scores_df","fc4d6ff8":"def conf_matrix(y, y_pred):\n    fig, ax =plt.subplots(figsize=(3.5,3.5))\n    labels=['No','Yes']\n    ax=sns.heatmap(confusion_matrix(y, y_pred), annot=True, cmap=\"Blues\", fmt='g', cbar=False, annot_kws={\"size\":25})\n    plt.title('Heart Failure?', fontsize=20)\n    ax.xaxis.set_ticklabels(labels, fontsize=17) \n    ax.yaxis.set_ticklabels(labels, fontsize=17)\n    ax.set_ylabel('Test')\n    ax.set_xlabel('Predicted')","dbd9a6e9":"df = pd.read_csv('\/kaggle\/input\/heart-failure-prediction\/heart.csv', skipinitialspace=True)","63bec17c":"df.head()","3531d2df":"df.info()","5fc98d06":"df.isnull().sum()","5cde077e":"df[df.duplicated()]","31d460a2":"num_cols = ['Age','RestingBP','Cholesterol','FastingBS','MaxHR','Oldpeak']","1486d4be":"def num_plot(df, col):\n    fig = px.histogram(df, x=col, color=\"HeartDisease\",\n                       marginal=\"box\")\n    fig.update_layout(height=400, width=500, showlegend=True)\n    fig.update_traces(marker_line_width=1,marker_line_color=\"black\")\n    fig.show()","10ce5ae1":"for col in num_cols:\n    num_plot(df,col)","00aaa28a":"df[df['RestingBP'] == 0]","6a1285cd":"df = df[(df['RestingBP'] > 0)]","3cf7fc93":"df[df['Cholesterol'] == 0]","0c7e0cac":"num_plot(df,'Cholesterol')","2c4e10ba":"print('Mean',df.Cholesterol.mean())\nprint('Median',df.Cholesterol.median())\nprint('Mode',df.Cholesterol.mode()[0])","8f2e136f":"df_no_chol = df[df['Cholesterol']==0]","9e672d9e":"for col in num_cols:\n    num_plot(df_no_chol,col)","3820324d":"df['Cholesterol'] = df['Cholesterol'].replace({0:np.nan})","56f31123":"df.Sex = df.Sex.replace({'M':0,'F':1})","e0964514":"df.ChestPainType.value_counts()","fcc2a99a":"df.ExerciseAngina.value_counts()","813b59d8":"df.ExerciseAngina = df.ExerciseAngina.replace({'N':0,'Y':1})","af441c61":"df.ST_Slope.value_counts()","a5293734":"df.RestingECG.value_counts()","d1396d24":"encoded_df = pd.get_dummies(df, drop_first=True)","c36e9f29":"fig = px.histogram(df, x=\"HeartDisease\", color=\"HeartDisease\")\nfig.update_layout(height=400, width=500, showlegend=True)\nfig.update_traces(marker_line_width=1,marker_line_color=\"black\")\nfig.show()","c6d70f47":"X = encoded_df.drop('HeartDisease', axis = 1)","4c132531":"y = encoded_df['HeartDisease']","85330dbb":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15, random_state=seed, stratify=y)","00294243":"fig, ax = plt.subplots(2, 1, sharex=True, figsize=(8,5),gridspec_kw={\"height_ratios\": (.2, .8)})\nax[0].set_title('Cholesterol distribution',fontsize=18)\nsns.boxplot(x='Cholesterol', data=X_train, ax=ax[0])\nax[0].set(yticks=[])\nsns.histplot(x='Cholesterol', data=X_train, ax=ax[1])\nax[1].set_xlabel(col, fontsize=16)\nplt.axvline(X_train['Cholesterol'].mean(), color='darkgreen', linewidth=2.2, label='mean=' + str(np.round(X_train['Cholesterol'].mean(),1)))\nplt.axvline(X_train['Cholesterol'].median(), color='red', linewidth=2.2, label='median='+ str(np.round(X_train['Cholesterol'].median(),1)))\nplt.axvline(X_train['Cholesterol'].mode()[0], color='purple', linewidth=2.2, label='mode='+ str(X_train['Cholesterol'].mode()[0]))\nplt.legend(bbox_to_anchor=(1, 1.03), ncol=1, fontsize=17, fancybox=True, shadow=True, frameon=True)\nplt.tight_layout()\nplt.show()","215af7c0":"chol = 240 ","d80fd90a":"X_train['Cholesterol'] = X_train['Cholesterol'].fillna(chol)","29fec968":"X_test['Cholesterol'] = X_test['Cholesterol'].fillna(chol)","2984f64f":"scaler = StandardScaler()   \nX_train = scaler.fit_transform(X_train)          \nX_test = scaler.transform(X_test)  ","ca22d0ba":"pca = PCA()\npca.fit_transform(X_train);","0517dd6e":"cum_sum = np.cumsum(pca.explained_variance_ratio_)*100\ncomp= [n for n in range(len(cum_sum))]","d81750d6":"plt.figure(figsize=(5,5))\nplt.plot(comp, cum_sum, marker='.')\nplt.xlabel('PCA Components')\nplt.ylabel('Cumulative Explained Variance (%)')\nplt.title('PCA')\nplt.show()","991c4907":"X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=seed)","cd0bfb9d":"DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(DEVICE)","4233aaf8":"class CustomDataset:\n    def __init__(self, X_data, y_data, device=DEVICE):\n        self.X_data = X_data\n        self.y_data = y_data \n    \n    def __len__ (self):\n        return len(self.X_data)\n    \n    def __getitem__(self, index):\n        return self.X_data[index], self.y_data[index]","fb91de3a":"train_data = CustomDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train.values))","7559656d":"val_data = CustomDataset(torch.FloatTensor(X_valid), torch.FloatTensor(y_valid.values))","795c5775":"test_data = CustomDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test.values))","7012daa5":"BATCHSIZE = 16","d19aeb2c":"train_loader = DataLoader(dataset=train_data, batch_size=BATCHSIZE)\nvalid_loader = DataLoader(dataset=val_data, batch_size=1)\ntest_loader = DataLoader(dataset=test_data, batch_size=1)","d8f446a3":"def define_model(trial):\n    n_layers = trial.suggest_int(\"n_layers\", 1, 2)\n    layers = []\n\n    in_features = 15\n    for i in range(n_layers):\n        out_features = trial.suggest_int(\"n_units_{}\".format(i), 8, 25)\n        layers.append(nn.Linear(in_features, out_features))\n        layers.append(nn.ReLU())\n        p = trial.suggest_uniform(\"dropout_{}\".format(i), 0.2, 0.5)\n        layers.append(nn.Dropout(p))\n        in_features=out_features\n    layers.append(nn.Linear(out_features, 1))\n\n    return nn.Sequential(*layers)","f912f822":"EPOCHS = 40","ec38bb99":"def objective(trial):\n\n    # call the define_model method\n    model = define_model(trial).to(DEVICE)\n\n    # Optimizer and loss definition\n    lr = trial.suggest_float(\"lr\", 5e-4, 1e-2, log=True)\n    optimizer =  getattr(optim, 'Adam')(model.parameters(), lr=lr)\n    criterion = nn.BCEWithLogitsLoss() \n    # Using the logit binary CE, we include the sigmoid function in the prediction output during the loss calculation\n    \n    train_acc = []\n    train_loss = []\n    \n    valid_acc = []\n    valid_loss = []\n    \n    total_step = len(train_loader)\n    total_step_val = len(valid_loader)\n\n    for epoch in range(EPOCHS):\n        \n        running_loss=0\n        correct=0\n        total=0\n        \n        #TRAINING\n\n        model.train()\n\n        for batch_idx, (X_train_batch, y_train_batch) in enumerate(train_loader):\n            X_train_batch, y_train_batch = X_train_batch.to(DEVICE), y_train_batch.to(DEVICE)\n            optimizer.zero_grad()\n            output = model(X_train_batch)\n            y_pred = torch.round(torch.sigmoid(output))\n            #LOSS\n            loss = criterion(output, y_train_batch.unsqueeze(1))\n            loss.backward()\n            optimizer.step()\n            running_loss+=loss.item() #sum all batch losses\n            #ACCURACY\n            correct += torch.sum(y_pred==y_train_batch.unsqueeze(1)).item()\n            total += y_train_batch.size(0)\n        train_acc.append(100 * correct \/ total) \n        train_loss.append(running_loss\/total_step) #get average loss among all batches dividing total loss by the number of batches\n\n        # VALIDATION\n        correct_v = 0\n        total_v = 0\n        batch_loss = 0\n        with torch.no_grad():\n            model.eval()\n            for batch_idx, (X_valid_batch, y_valid_batch) in enumerate(valid_loader):\n                X_valid_batch,y_valid_batch=X_valid_batch.to(DEVICE),y_valid_batch.to(DEVICE)\n                #PREDICTION\n                output = model(X_valid_batch)\n                y_pred = torch.round(torch.sigmoid(output))\n                #LOSS\n                loss_v = criterion(output, y_valid_batch.unsqueeze(1))\n                batch_loss+=loss_v.item()\n                #ACCURACY\n                correct_v += torch.sum(y_pred==y_valid_batch.unsqueeze(1)).item()\n                total_v += y_valid_batch.size(0)\n            valid_acc.append(100 * correct_v \/ total_v)\n            valid_loss.append(batch_loss\/total_step_val)\n\n        trial.report(np.mean(valid_loss), epoch)\n\n        # Handle pruning based on the intermediate value\n        if trial.should_prune():\n            raise optuna.exceptions.TrialPruned()\n            \n    return np.mean(valid_loss)","7273f9c2":"study = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=100)\n\npruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\ncomplete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n\nprint(\"Study statistics: \")\nprint(\"  Number of finished trials: \", len(study.trials))\nprint(\"  Number of pruned trials: \", len(pruned_trials))\nprint(\"  Number of complete trials: \", len(complete_trials))\n\nprint(\"Best trial:\")\ntrial = study.best_trial\n\nprint(\"  Value: \", trial.value)\n\nprint(\"  Params: \")\n\nparams = []\n\nfor key, value in trial.params.items():\n    params.append(value)\n    print(\"    {}: {}\".format(key, value))","ed6a3fb9":"params","4634e39d":"n_layers = params[0]\n\nunits_1 = params[1]\ndropout_1 = np.round(params[2],5)\n\nlr = np.round(params[3],8)","ce90e2b2":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.layer_1 = nn.Linear(X_train.shape[1], units_1)\n        self.layer_out = nn.Linear(units_1, 1) \n        self.dropout1 = nn.Dropout(p=dropout_1)\n        \n    def forward(self, inputs):\n        x = F.relu(self.layer_1(inputs))\n        x = self.dropout1(x)\n        x = self.layer_out(x)\n        \n        return x","97ee6496":"model = Net()\nmodel.to(DEVICE)\nprint(model)","036d54ee":"criterion = nn.BCEWithLogitsLoss()\noptimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.0001)","934a1899":"EPOCHS = 100","8941802d":"# Model Training\n\nearly_stopping_patience = 15\nearly_stopping_counter = 0\n\ntrain_acc = []\ntrain_loss = []\n\nvalid_acc = []\nvalid_loss = []\n\ntotal_step = len(train_loader)\ntotal_step_val = len(valid_loader)\n\nvalid_loss_min=np.inf\n\nfor epoch in range(EPOCHS):\n    \n    running_loss=0\n    correct=0\n    total=0\n    \n    #TRAINING\n\n    model.train()\n\n    for batch_idx, (X_train_batch, y_train_batch) in enumerate(train_loader):\n        X_train_batch, y_train_batch = X_train_batch.to(DEVICE), y_train_batch.to(DEVICE)\n        optimizer.zero_grad()\n        output = model(X_train_batch)\n        y_pred = torch.round(torch.sigmoid(output))\n        #LOSS\n        loss = criterion(output, y_train_batch.unsqueeze(1))\n        loss.backward()\n        optimizer.step()\n        running_loss+=loss.item() #sum loss for every batch\n        #ACCURACY\n        correct += torch.sum(y_pred==y_train_batch.unsqueeze(1)).item()\n        total += y_train_batch.size(0)\n    train_acc.append(100 * correct \/ total) #calculate accuracy among all entries in the batches\n    train_loss.append(running_loss\/total_step)  #get average loss among all batches dividing total loss by the number of batches\n\n    # VALIDATION\n    correct_v = 0\n    total_v = 0\n    batch_loss = 0\n    with torch.no_grad():\n        model.eval()\n        for batch_idx, (X_valid_batch, y_valid_batch) in enumerate(valid_loader):\n            X_valid_batch,y_valid_batch=X_valid_batch.to(DEVICE),y_valid_batch.to(DEVICE)\n            #PREDICTION\n            output = model(X_valid_batch)\n            y_pred = torch.round(torch.sigmoid(output))\n            #LOSS\n            loss_v = criterion(output, y_valid_batch.unsqueeze(1))\n            batch_loss+=loss_v.item()\n            #ACCURACY\n            correct_v += torch.sum(y_pred==y_valid_batch.unsqueeze(1)).item()\n            total_v += y_valid_batch.size(0)\n        valid_acc.append(100 * correct_v \/ total_v) \n        valid_loss.append(batch_loss\/total_step_val)\n    \n    \n    if np.mean(valid_loss) <= valid_loss_min:\n        torch.save(model.state_dict(), '.\/state_dict.pt')\n        print(f'Epoch {epoch + 0:01}: Validation loss decreased ({valid_loss_min:.6f} --> {np.mean(valid_loss):.6f}).  Saving model ...')\n        valid_loss_min = np.mean(valid_loss)\n        early_stopping_counter=0 #reset counter if validation loss decreases\n    else:\n        print(f'Epoch {epoch + 0:01}: Validation loss did not decrease')\n        early_stopping_counter+=1\n\n    if early_stopping_counter > early_stopping_patience:\n        print('Early stopped at epoch :', epoch)\n        break\n\n    print(f'\\t Train_Loss: {np.mean(train_loss):.4f} Train_Acc: {(100 * correct \/ total):.3f} Val_Loss: {np.mean(valid_loss):.4f}  BEST VAL Loss: {valid_loss_min:.4f}  Val_Acc: {(100 * correct_v \/ total_v):.3f}\\n')","2f1cbd07":"y_pred_prob_list = []\ny_pred_list = []\n\n\n# Loading the best model\nmodel.load_state_dict(torch.load('.\/state_dict.pt'))\n\nwith torch.no_grad():\n        model.eval()\n        for batch_idx, (X_test_batch, y_test_batch) in enumerate(test_loader):\n            X_test_batch = X_test_batch.to(DEVICE)\n            #PREDICTION\n            output = model(X_test_batch)\n            y_pred_prob = torch.sigmoid(output)\n            y_pred_prob_list.append(y_pred_prob.cpu().numpy())\n            y_pred = torch.round(y_pred_prob)\n            y_pred_list.append(y_pred.cpu().numpy())","4c54a270":"y_pred_prob_list = [a.squeeze().tolist() for a in y_pred_prob_list]\ny_pred_list = [a.squeeze().tolist() for a in y_pred_list]","622fa983":"print(classification_report(y_test, y_pred_list))","7252b8b9":"conf_matrix(y_test, y_pred_list)","507a8624":"plt.figure(figsize = (5.5, 4))\n\nfpr, tpr, _ = roc_curve(y_test, y_pred_prob_list)\nroc_auc = auc(fpr, tpr)\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.title('ROC curve',fontsize=25)\nplt.ylabel('True Positive Rate',fontsize=18)\nplt.xlabel('False Positive Rate',fontsize=18)\nplt.legend(loc = 'lower right', fontsize=24, fancybox=True, shadow=True, frameon=True, handlelength=0)\nplt.show()","ea9aca8a":"class LightNet(LightningModule):\n    def __init__(self):\n        super(LightNet, self).__init__()\n        self.layer_1 = nn.Linear(15, units_1)\n        self.dropout1 = nn.Dropout(p=dropout_1)\n        self.layer_out = nn.Linear(units_1, 1) \n        \n    def forward(self, x):\n        x = F.relu(self.layer_1(x))\n        x = self.dropout1(x)\n        x = self.layer_out(x)\n        return x\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self.forward(x)\n        y = y.unsqueeze(1)\n        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n        self.log(\"loss\", loss)    \n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y = y.unsqueeze(1)\n        y_hat = self.forward(x)\n        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n        self.log(\"val_loss\", loss, on_epoch=True)\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        y = y.unsqueeze(1)\n        y_hat = self.forward(x)\n        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n        self.log(\"test_loss\", loss, on_epoch=True)\n        \n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=lr)\n\n    def train_dataloader(self):\n        return DataLoader(dataset=train_data, batch_size=BATCHSIZE)\n\n    def val_dataloader(self):\n        return DataLoader(dataset=val_data, batch_size=1)\n\n    def test_dataloader(self):\n        return DataLoader(dataset=test_data, batch_size=1)","427ceb8c":"light_model = LightNet()\nearly_stop_callback = EarlyStopping(monitor='val_loss', patience=5)\ntrainer = Trainer(callbacks=[early_stop_callback], log_every_n_steps=20)   \ntrainer.fit(light_model)","808d2fb6":"trainer.test()","5eac2ff7":"y_pred_prob_list = []\nprediction_list = []\n\nfor batch_idx, (X_test_batch, y_test_batch) in enumerate(test_loader):\n    output = light_model(X_test_batch)\n    y_pred_prob = torch.sigmoid(output)\n    y_pred_prob_list.append(y_pred_prob)\n    y_pred = torch.round(y_pred_prob)\n    prediction_list.append(y_pred)","3aef738e":"y_pred_prob_list = [a.squeeze().tolist() for a in y_pred_prob_list]\npreds = [a.squeeze().tolist() for a in prediction_list]","4b630c79":"print(classification_report(y_test, preds))","e3e37a0f":"conf_matrix(y_test, preds)","90cfb46b":"plt.figure(figsize = (5.5, 4))\n\nfpr, tpr, _ = roc_curve(y_test, y_pred_prob_list)\nroc_auc = auc(fpr, tpr)\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.title('ROC curve',fontsize=25)\nplt.ylabel('True Positive Rate',fontsize=18)\nplt.xlabel('False Positive Rate',fontsize=18)\nplt.legend(loc = 'lower right', fontsize=24, fancybox=True, shadow=True, frameon=True, handlelength=0)\nplt.show()","f80ef146":"## PyTorch NN ROC curve","b393888b":"## Create Custom Dataset class","0210b122":"## Classification Report","b2e94cf1":"There are two classes, we will encode it by 0 and 1.","f588f95f":"# Hyperoptimization by Optuna","226ff1dc":"There are only 3 classes, we can encode it by OHE","0ce8832b":"## Classification Report","d62db547":"# Data Analysis","fa472b03":"## Dataset Loading","b68fd5e4":"# Categorical Features Encoding","41b320bb":"Depending on the number of different possible values for each categorical variable, we will choose if encoding the variable by label encoding or one hot encoding (OHE).","734a3ab9":"It looks like there are no common features among these patients with missing cholesterol value.<br>\nWe will impute the missing values of Cholesterol after splitting the dataset into training and test sets. For now, we label these values as 'NaN'.","f98858f6":"## ST_Slope","e5f51f46":"We set the number of epochs:","c73faa2d":"**The results of the hyperoptimized Pytorch Neural Network are satisfying, since all the scores are around 90%.**<br>\n**In particular, the best Recall score I could get is 92%, which in medical applications is the best score to look at, since having false negatives is a crime**","3d0a60e4":"Useful links for Pytorch and optuna:\n- https:\/\/medium.com\/pytorch\/using-optuna-to-optimize-pytorch-hyperparameters-990607385e36\n- https:\/\/pytorch.org\/\n- https:\/\/www.pytorchlightning.ai\/\n- https:\/\/pytorch.org\/tutorials\/beginner\/blitz\/cifar10_tutorial.html\n- https:\/\/optuna.org\/","5e0626b4":"The most frequent value (mode) is cholesterol = 0.","411f7288":"There are 171 patients with a cholesterol value = 0, which is not possible.","7f8ec06f":"In order to train the PyTorch Lightning neural network, we must call define a Trainer object and call the fit method.","7811adaf":"## ROC curve","9d251cc8":"There are only 3 classes, we can encode it by OHE","7802a861":"# Dimensionality reduction by PCA","c811e82d":"By visual inspection, a good value for imputing missing cholesterol values could be 240.","5918d1a1":"After extracting the training set, we can study how to impute missing values of the cholesterol column. We perform the imputation only on the training set to avoid any data leakage: the best value for imputation obtained on the training set will be used also on the test set.","d4706218":"## Confusion Matrix","e5af7d66":"## What about 0 values of Cholesterol?","953c7109":"## Train - Validation split","21a33476":"**The results of the hyperoptimized Pytorch Neural Network are satisfying, since all the scores are around 90%.**<br>\n**In particular, the best Recall score I could get is 92%, which in medical applications is the best score to look at, since having false negatives is a crime**","ebca0642":"Finally we can start the training of the Neural Network with the optimized hyperparameters","49e64117":"# PyTorch + PyTorch Lightning \u26a1 Neural Network Hyperparameter Optimization with OPTUNA for a binary classification problem","01a9817b":"# Neural Network hypermodel","ce3e5e9d":"Sex has only two classes, so we can encode it by 0 and 1.","7e37ece7":"To simplify the following analysis, we will define a function to plot numerical features by plotly.","5ccbec39":"# OPTUNA","d07fb303":"We have 15 features\/columns in the dataset, is it possible to remove some of them and still keep a high explainability?","4e580e2f":"## Confusion Matrix","d5bd5c40":"**The following projects aims to use PyTorch Neural Net with Optuna hyperoptimization to predict if a patient will have a Heart Disease based on given medical features.**","705c834c":"In the following we will define a **n-layers Neural Network hypermodel**, where n is the number of hidden layers chosen by OPTUNA. The idea of a hypermodel is that some hyperparameters are defined in a range of values, where the optmization algorithm (OPTUNA) will search to find the 'best' ones by optimizing a certain metric, for example the validation accuracy of the model. <br>\nIn this work, the hyperparameters that will be optimized are:\n- number of layers\n- number of neurons for each layer\n- learning rate of Adam optimizer","8b6e6744":"## Parameters definition","76997ade":"Indeed, we see very similar results compareed to the the 'classical' PyTorch Neural network's performance.<br>\nNonetheless, PyTorch Lightning is becoming more and more famous and should be definitely be considered for future works !! :)","29e1977c":"# PyTorch Results Summary","31f06e8f":"It looks like the PCA curve does not have any clear elbow. We should not drop any of the features.","3686180e":"## Gender","2faca7bf":"## Train - Test split","fd33c780":"We will drop this value!","1f2f9d39":"# Main results Summary:","c10464c4":"# BONUS: Pytorch Lightning \u26a1","f9969a48":"Are there null values?","778ec337":"Now we can get the predictions by looping over the test loader.","893df943":"Then, we define a new Neural Network by Pytorch, where the hyperparameters will have the optimized values obtaiend with optuna.","54019256":"**Thanks for reading my notebook ! Let me know if you have questions or if you want me to check out your works !!**","1a21570e":"## ExerciseAngina","28170d9b":"So now we will encode the variables by OHE using the convenient get_dummies function from pandas library.","72a58f89":"<img src=\"https:\/\/i.imgur.com\/V9JVw0X.png\" width=\"900px\">","49cf71ff":"<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https:\/\/i.imgur.com\/2EXDywB.jpg\" alt=\"Heat beating\" style=\"height:320px;margin-top:3rem;\"> <\/div>","1182dc55":"# Pytorch Neural Network modeling","e7cd5c09":"# Pytorch Neural Network training","8df1ec23":"## Custom functions definition","5c680374":"### Install pytorch lightning","a41b8a6c":"Optuna is a useful optimization library that can be used with different Machine Learning libraries such as XGBoost, Tensorflow or Pytorch in order to tune the models' hyperparameters. In the specific application of Neural Networks, the most common hyperparameters than can be tuned are the number of neurons per layers, the number of layers, the dropoout values and the learning rate.","5493ea29":"There are only 3 classes, we can encode it by OHE","d4e233d8":"There are no duplicate values.","fa559be7":"Then, we need to define an objective function, including the train and evaluation of our model.","f638b4fb":"## Dataloader definition","d6b8f2fb":"## RestingECG","e28ffb0e":"Finally, we can start the optimization with Optuna.","158868db":"We can extract the best parameters from the list:","1b98c9ad":"The definition of the neural network by PyTorch Lightning is very similar to the PyTorch one, with the difference that the Neural Network class also inclused functions such as the dataloaders, the training, validation and test steps.","e0b43961":"In addition to the PyTorch Neural Network, we will also define a PyTorch Lightning \u26a1 Neural Network, adding the very useful EarlyStopping function defined in the library.","cef14721":"## Missing Cholesterol values imputation","0496803c":"Are there duplicate values?","0d501e91":"There are outliers values for Colesterol and restingBP=0","dc8d448a":"# Data preprocessing for Machine Learning modeling","fb7d9eb5":"There are no null values.","0823742e":"## ChestPainType","340cd0fc":"Good news, the target variable looks balanced.","9a72e1a6":"# PyTorch Lightning Results Summary"}}