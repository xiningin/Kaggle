{"cell_type":{"daf50fb4":"code","2d04849a":"code","4d2d326c":"code","7c353060":"code","5a6148ed":"code","9aeb2340":"code","629485fd":"code","a424e585":"code","7fc5f18f":"code","fc8cfa85":"code","50d4c842":"code","8746ae14":"code","6dccf777":"code","83fea5bb":"code","dc9c274c":"code","23510b80":"code","84b7757e":"code","c016f967":"code","b9f5c459":"code","26efa9f5":"code","595ab146":"code","ddebffa5":"code","c2c91ab9":"code","578d5fca":"code","28ceac5a":"code","62c86bd1":"code","23269764":"code","5857eedd":"code","36eabb96":"code","d30750c4":"code","bcebda66":"code","59127359":"code","89a9696b":"code","91caa971":"code","0d2347f9":"code","96677e3d":"code","fb3df605":"code","972ed344":"code","1a30b269":"code","2e3ead45":"code","d9f9fbde":"code","918fcde9":"code","3d4e66a4":"code","e5f61f23":"code","d79a9904":"code","8d774166":"code","bf497f81":"code","e7272cc8":"code","8c773af5":"code","6182379d":"code","d09fbc70":"code","df867d58":"code","d2832534":"code","b8f653b3":"code","6d30afd9":"code","11967a5e":"code","49a9515c":"code","d37b4dc6":"code","cd35b3c3":"code","95785586":"code","0ed2bae0":"code","15a0c426":"code","6a37c314":"code","f76fd946":"code","be5ec4b9":"code","544c36c1":"code","847b0178":"code","ab7f6ef3":"code","c2695a00":"code","6f91c505":"code","493d4f93":"code","7d9d736a":"code","687c07a6":"code","70a5f516":"code","2e8950f4":"code","8de5912e":"code","0a04acd3":"code","bafad031":"code","5dffe31a":"code","efc82411":"code","a41e99f0":"markdown","fbd03a40":"markdown","4015d525":"markdown","8e1d8558":"markdown","b71f2e7d":"markdown","bc2e64ce":"markdown","af311081":"markdown","cbd15f3e":"markdown","7fc233a1":"markdown"},"source":{"daf50fb4":"!pip install -q efficientnet","2d04849a":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport math\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn import model_selection\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport tensorflow_addons as tfa\nimport efficientnet.tfkeras as efn\nimport h5py","4d2d326c":"data_dir = Path('..\/input\/landmark-recognition-2021')\ntrain_data_dir = data_dir \/ 'train'\ntest_data_dir = data_dir \/ 'test'\ntrain_label_file = data_dir \/ 'train.csv'\nsample_file = data_dir \/ 'sample_submission.csv'","7c353060":"original_label = pd.read_csv(train_label_file, index_col='id')\nsub = pd.read_csv(sample_file, index_col='id')","5a6148ed":"original_label","9aeb2340":"original_label = original_label.sample(frac=1) # Shuffling the data","629485fd":"original_label # Exhibition of the shuffled Data","a424e585":"leny = len(original_label)\nborder = int(0.2*leny)\ntest_label = original_label[:border]\nlabel = original_label[border:]","7fc5f18f":"len(label)","fc8cfa85":"label['landmark_id'].value_counts()","50d4c842":"values = label['landmark_id'].value_counts().values\nindexes = label['landmark_id'].value_counts().index","8746ae14":"values = np.array(values)\nindexes = np.array(indexes)","6dccf777":"values","83fea5bb":"cutter = 50 # How many labels we are trying to predict correctly","dc9c274c":"values[:cutter]","23510b80":"indexes[:cutter]","84b7757e":"# get_rid = indexes[values < 1000]\nget_rid = indexes[cutter:]","c016f967":"get_rid","b9f5c459":"label","26efa9f5":"len(get_rid)","595ab146":"label_numpy = label['landmark_id'].to_numpy()","ddebffa5":"label_numpy","c2c91ab9":"mask = np.isin(label_numpy, get_rid)","578d5fca":"mask","28ceac5a":"label_numpy[mask == 1] = 0","62c86bd1":"label['landmark_id'] = label_numpy","23269764":"label","5857eedd":"label = label.groupby('landmark_id').tail(100)","36eabb96":"label.landmark_id.value_counts()","d30750c4":"label","bcebda66":"label.drop(label.index[label['landmark_id'] == 0], inplace = True)","59127359":"label.landmark_id.value_counts()","89a9696b":"indexes = label.landmark_id.value_counts().index","91caa971":"indexes","0d2347f9":"index_numpy = np.array(indexes)","96677e3d":"landmark_id_numpy = label.landmark_id.to_numpy()","fb3df605":"landmark_id_numpy","972ed344":"for i, index in enumerate(index_numpy):\n    landmark_id_numpy[landmark_id_numpy==index] = i","1a30b269":"label['landmark_id'] = landmark_id_numpy","2e3ead45":"label.landmark_id.value_counts()","d9f9fbde":"no_classes = len(label.landmark_id.value_counts())","918fcde9":"def id_to_path(s, train=True):\n    data_dir = train_data_dir if train else test_data_dir\n    return data_dir \/ s[0] \/ s[1] \/ s[2] \/ f'{s}.jpg'","3d4e66a4":"input_size = (300, 300, 3)\nbatch_size = 32\nn_epoch = 10\nseed = 42","e5f61f23":"newsize = (300, 300)\n# # x = np.asarray((Image.open(id_to_path('fd80b73a476ae8a1')).resize(newsize)))\n# x = np.array(Image.open(id_to_path('fd80b73a476ae8a1')).resize(newsize))","d79a9904":"# x.shape","8d774166":"# plt.imshow(x)","bf497f81":"class DataGenerator(keras.utils.Sequence):\n    def __init__(self, x_set, y_set=None, batch_size=32):\n        self.x , self.y = x_set, y_set\n        self.batch_size = batch_size\n        self.is_train = False if y_set is None else True\n        \n    def __len__(self):\n        return math.ceil(len(self.x) \/ self.batch_size)\n    \n    def __getitem__(self, idx):\n        batch_ids = self.x[idx * self.batch_size: (idx + 1) * self.batch_size]\n        if self.y is not None:\n            batch_y = self.y[idx * self.batch_size: (idx + 1) * self.batch_size]\n        \n#         list_x = [np.load(id_to_path(x, self.is_train))[::2] for x in batch_ids]\n        newsize = (300, 300)\n        list_x = np.array([np.asarray(Image.open(id_to_path(x, self.is_train)).resize(newsize)) for x in batch_ids])\n#         list_x = np.concatenate(list_x, axis=0)\n        batch_x = np.moveaxis(list_x,1,1)\n        batch_x = batch_x.astype(\"float\") \/ 255\n        \n        if self.is_train:\n            return batch_x, batch_y\n        else:\n            return batch_x","e7272cc8":"label.nunique()[0]","8c773af5":"model = tf.keras.Sequential([\n        efn.EfficientNetB3(input_shape=input_size,weights='imagenet',include_top=False),\n        keras.layers.GlobalAveragePooling2D(),\n        keras.layers.Dense(no_classes, activation='softmax')\n        ])\n\nmodel.summary()","6182379d":"# from tensorflow.keras.models import Sequential\n# from tensorflow.keras.layers import Dense, Dropout, Flatten\n# from tensorflow.keras.layers import Conv2D, MaxPooling2D\n\n# input_shape = x.shape\n\n# # Create the model\n# model = Sequential()\n# model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n# model.add(MaxPooling2D(pool_size=(2, 2)))\n# model.add(Dropout(0.25))\n# model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n# model.add(MaxPooling2D(pool_size=(2, 2)))\n# model.add(Dropout(0.25))\n# model.add(Flatten())\n# model.add(Dense(256, activation='relu'))\n# model.add(Dense(no_classes, activation='softmax'))","d09fbc70":"# model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n#               loss= tf.keras.losses.SparseCategoricalCrossentropy(\n#     from_logits=False, reduction=\"auto\", name=\"sparse_categorical_crossentropy\"\n# ), metrics=[keras.metrics.AUC()])","df867d58":"# model.compile(optimizer='adam',\n# #               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n#               loss=tfa.losses.SigmoidFocalCrossEntropy(),\n#               metrics=['accuracy'])","d2832534":"# Compile the model\nmodel.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\n              optimizer=tf.keras.optimizers.Adam(),\n              metrics=['accuracy'])","b8f653b3":"x0 = label.index.values\ny0 = label['landmark_id'].values\nx2 = test_label.index.values\ny2 = test_label['landmark_id'].values\n\nx1 = sub.index.values\n\nx_train, x_val, y_train, y_val = model_selection.train_test_split(x0, y0, test_size=.2, random_state=seed)\n# x_train, y_train = x0, y0\n# x_val, y_val = x2, y2\n\ntrain = DataGenerator(x_train, y_train, batch_size=batch_size)\nval = DataGenerator(x_val, y_val, batch_size=batch_size)\ntest = DataGenerator(x1, batch_size=batch_size)\n\n# history = model.fit(train, validation_data=val, epochs=n_epoch)\nhistory = model.fit(train, validation_data=val, epochs=5)","6d30afd9":"import matplotlib.pyplot as plt\n\nplt.figure(num=0)\nplt.title('loss')\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='validation')\nplt.legend()\n\nplt.figure(num=1)\nplt.title('accuracy')\nplt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='validation')\nplt.legend()","11967a5e":"model.save(\"model_name.h5\")","49a9515c":"label","d37b4dc6":"test_label","cd35b3c3":"valid_label = test_label[:1000]\nx2 = valid_label.index.values\ny2 = valid_label['landmark_id'].values\nx_val, y_val = x2, y2\nval = DataGenerator(x_val, y_val, batch_size=batch_size)","95785586":"# prediction = model.predict(test).flatten()\nprediction = model.predict(val)\npredictions = np.argmax(prediction, axis=1)\nfor i, index in enumerate(index_numpy):\n    predictions[predictions==i] = index","0ed2bae0":"valid_label.nunique()[0]","15a0c426":"1 \/ valid_label.nunique()[0]","6a37c314":"(predictions == y_val).sum()\/len(predictions)","f76fd946":"# prediction = model.predict(test).flatten()\nprediction = model.predict(test)","be5ec4b9":"scores = prediction.max(axis=1)","544c36c1":"scores.shape","847b0178":"predictions = np.argmax(prediction, axis=1)","ab7f6ef3":"predictions.shape","c2695a00":"predictions","6f91c505":"index_numpy","493d4f93":"for i, index in enumerate(index_numpy):\n    predictions[predictions==i] = index","7d9d736a":"predictions","687c07a6":"my_pred = str(predictions) + ' ' + str(scores)","70a5f516":"sub['landmarks'] = predictions\nsub['scores'] = scores\nsub['space'] = scores\nsub['space'] = sub['space'].apply(lambda x: ' ')\n\nsub['landmarks'] = sub['landmarks'].apply(lambda x: str(x))\nsub['scores'] = sub['scores'].apply(lambda x: str(\"%.2f\" % x))\nsub['landmarks'] = sub['landmarks'] + sub['space'] + sub['scores']\n\nsub.drop('scores', inplace=True, axis=1)\nsub.drop('space', inplace=True, axis=1)","2e8950f4":"sub","8de5912e":"sub.reset_index(inplace=True)","0a04acd3":"sub","bafad031":"sub.to_csv('submission.csv')","5dffe31a":"s = pd.read_csv('..\/input\/landmark-recognition-2021\/sample_submission.csv')\ns","efc82411":"file = h5py.File('data.h5', 'w')\nfile.create_dataset('dataset', data=index_numpy)\nfile.close()","a41e99f0":"# Exhibition of the Learning of the Model","fbd03a40":"# Architecture of the Model","4015d525":"# Imports","8e1d8558":"# Predictions on the validation set","b71f2e7d":"# Predictions on the test set","bc2e64ce":"# Directories","af311081":"# Fit of the Model","cbd15f3e":"# Preprocessing of the Data","7fc233a1":"# Compile of the Model"}}