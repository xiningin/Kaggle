{"cell_type":{"ec195034":"code","8948fbb3":"code","02a61331":"code","61652ed4":"code","a91fb854":"code","f141ed16":"code","f4f72af3":"code","7ca1b62c":"code","1209db8c":"code","1f87dbcd":"code","84cbeff0":"code","cbcd10db":"code","f13552bf":"code","f68447fb":"code","cfaab136":"code","d8c85a38":"code","1876554f":"code","085d3e7c":"code","74eefd81":"code","78472388":"code","cc7772b6":"code","a845fa3c":"code","27e7fb72":"code","7a449e58":"code","e334f437":"code","203116b4":"code","544bc328":"code","9b4dda50":"code","5b51af74":"code","23f5c980":"code","b71d6baf":"code","7f289ff5":"code","9d6109cd":"code","7ed45398":"code","16c31c74":"code","af530396":"code","9456bb92":"code","9f778814":"code","108384f7":"code","23b38c5a":"code","fee1f318":"code","58e32f72":"code","b236f898":"code","70de25a5":"code","47db7f54":"code","e08b8dc9":"code","89f97700":"code","faf6f22d":"code","8f6309eb":"code","66c53b6d":"code","05b73d65":"code","ff61749d":"code","cb0760e6":"code","02d9c146":"code","174fa186":"code","3d2a1f18":"code","8b4ad5b4":"code","61741ccc":"code","1d8362f1":"code","29828149":"code","ece82352":"code","92734eba":"code","9efd77e4":"code","7182af76":"code","b563b404":"code","708e3736":"code","49cbfa3b":"code","d3643cec":"code","95b370df":"code","b4cd8521":"code","ef295db6":"markdown","9e8c6d48":"markdown","d9a3647c":"markdown","281211db":"markdown","7f903a2d":"markdown","c95d3d86":"markdown","b923a9df":"markdown","8f09a909":"markdown","d7e8fd1a":"markdown","77aae498":"markdown","3d4500fe":"markdown","64ee35fd":"markdown","5bb122f0":"markdown","57834904":"markdown","3ab566b5":"markdown","99358cd4":"markdown","af1a8024":"markdown","13a86762":"markdown","1768f9b7":"markdown","09f1e580":"markdown","fcb9fdec":"markdown","b904a0e5":"markdown","d6adbd72":"markdown","41a60017":"markdown","ecd97802":"markdown","2ce1b5df":"markdown","ed0e1f04":"markdown","1e4a2653":"markdown","54ce87b9":"markdown","6417e361":"markdown","30f7df5c":"markdown","a39b0b6c":"markdown","aa6ea150":"markdown","2a4c9122":"markdown","6e1a0c0c":"markdown","c5238b68":"markdown","5dc4d3a5":"markdown","f8f89743":"markdown","a35ac69d":"markdown","2e25f511":"markdown","c5324f11":"markdown","3de24262":"markdown","aa270f5d":"markdown","d4ab6d2d":"markdown","e7f57f16":"markdown","229666ef":"markdown","cf4a0ee2":"markdown","d7049b8b":"markdown","73034124":"markdown","d0bc16ec":"markdown","c9fcc767":"markdown","cca576c2":"markdown","e8be37ec":"markdown","ae032d7b":"markdown","a278eec9":"markdown","af4869b9":"markdown","d75f6bcd":"markdown","9905f0b9":"markdown","b098e8d0":"markdown","bfa3615c":"markdown","edab97fa":"markdown","b7af2370":"markdown","cbfbba6d":"markdown","d4b89355":"markdown","f548ac01":"markdown","cebae9f9":"markdown","0805e3b0":"markdown","b5fd1c22":"markdown","ab9811bd":"markdown","d20549f3":"markdown","2085ac38":"markdown","da838f7c":"markdown","95b4fc7a":"markdown","689a9203":"markdown","88dfa333":"markdown","4f5e4470":"markdown"},"source":{"ec195034":"!pip install neologdn","8948fbb3":"!pip install \"https:\/\/github.com\/megagonlabs\/ginza\/releases\/download\/latest\/ginza-latest.tar.gz\"","02a61331":"!apt-get install -y mecab libmecab-dev mecab-ipadic-utf8\n!git clone --depth 1 https:\/\/github.com\/neologd\/mecab-ipadic-neologd.git \n!echo yes | mecab-ipadic-neologd\/bin\/install-mecab-ipadic-neologd -n -a \n!ln -s \/etc\/mecabrc \/usr\/local\/etc\/mecabrc","61652ed4":"!pip install oseti","a91fb854":"import os\nimport datetime\nimport codecs\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')\nimport pkg_resources, imp\nimp.reload(pkg_resources)\nimport numpy as np\nimport pandas as pd\nimport holoviews as hv\nfrom holoviews import opts\nhv.extension('bokeh')\nfrom matplotlib import pyplot as plt\nimport neologdn\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('vader_lexicon')\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nimport spacy\nfrom spacy.lang.ja import Japanese\nimport regex\nfrom wordcloud import WordCloud\nimport collections\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nimport shap\nshap.initjs()\nimport oseti","f141ed16":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f4f72af3":"df = pd.read_csv(\"\/kaggle\/input\/shinzo-abe-japanese-prime-minister-twitter-nlp\/Shinzo Abe Tweet 20171024 - Tweet.csv\",low_memory=False)\nprint(f'Shinzo Abe Tweet 20171024 - Tweet.csv : {df.shape}')\ndf.head(1)","7ca1b62c":"!wget http:\/\/svn.sourceforge.jp\/svnroot\/slothlib\/CSharp\/Version1\/SlothLib\/NLP\/Filter\/StopWord\/word\/Japanese.txt","1209db8c":"ja = open('.\/Japanese.txt','r')\nstopwords_ja = [i.replace('\\n','') for i in ja.readlines()]","1f87dbcd":"stopwords_en = stopwords.words(\"english\")","84cbeff0":"!wget -qO-  https:\/\/noto-website-2.storage.googleapis.com\/pkgs\/NotoSansCJKjp-hinted.zip | bsdtar -xvf- ","cbcd10db":"df_preprocessed = df.copy()\ndf_preprocessed = df_preprocessed[df['Full Name Show']=='\u5b89\u500d\u664b\u4e09']","f13552bf":"df_preprocessed['replies'] = df_preprocessed['Profile Tweet 1'].replace('[^0-9]', '', regex=True).astype(np.int64)\ndf_preprocessed['retweets'] = df_preprocessed['Profile Tweet 2'].replace('[^0-9]', '', regex=True).astype(np.int64)\ndf_preprocessed['likes'] = df_preprocessed['Profile Tweet 3'].replace('[^0-9]', '', regex=True).astype(np.int64)","f68447fb":"df_preprocessed.drop(['url','Full Name Show','Username Dir','Tweet Nav_link','Tweet Text Size Link','Tweet Text Size Link_link',\\\n                      'Profile Tweet 1','Profile Tweet 2','Profile Tweet 3','Reply','Re Tweet','Like'], axis=1, inplace=True)\ndf_preprocessed.rename(columns={'Tweet Nav':'date', 'Tweet Text Size Block': 'ja', 'English Translation':'en'}, inplace=True)","cfaab136":"df_preprocessed['date'] = df_preprocessed['date'].apply(lambda x: datetime.datetime.strptime(x, '%d %b %Y') if '2016' in x else datetime.datetime.strptime(f'2016 {x}', '%Y %b %d'))\ndf_preprocessed.sort_values(by='date', inplace=True)\ndf_preprocessed.reset_index(inplace=True, drop=True)","d8c85a38":"df_preprocessed['year'] = df_preprocessed['date'].apply(lambda x: x.year)\ndf_preprocessed['month'] = df_preprocessed['date'].apply(lambda x: x.month)\ndf_preprocessed['day'] = df_preprocessed['date'].apply(lambda x: x.month)\ndf_preprocessed['weekday'] = df_preprocessed['date'].apply(lambda x: x.day_name())","1876554f":"df_preprocessed.index = df_preprocessed.date\ndf_preprocessed.drop(['date'], axis=1, inplace=True)\ndf_preprocessed.head(1)","085d3e7c":"def ja_preprocess(text):\n    nlp = spacy.load('ja_ginza')\n    lowered = text.lower()\n    normalized = neologdn.normalize(lowered)\n    tokenized = []\n    for i in nlp(normalized):\n        word = i.lemma_\n        pos = i.pos_\n        if pos in [\"NOUN\", \"ADJ\", \"VERB\", \"ADV\", \"PROPN\"] and len(word) > 1 and i.text not in stopwords_ja:\n            tokenized.append(word)\n    preprocessed = \" \".join(tokenized)\n    return preprocessed","74eefd81":"stopwords_ja.extend(['pic','twitter','.com','https','http',':\/\/','\u306a\u308b','\u3044\u308b','\u3059\u308b','\u308c\u308b','\u4e0b\u3055\u308b','\u3044\u304f','\u3042\u308b','\u304a\u308b','\u3089\u308c\u308b','\u3044\u305f\u3060\u304f','\u304f\u3060\u3055\u308b','\u53c2\u308b','\u305b\u308b','\u304f\u308b'\\\n                    'special','jimin','jp','japan','\u307e\u3044\u308b','\u3044\u305f\u3059'])\n#there are some english words in japanese text, so we need to add english stop words. \nstopwords_ja.extend(stopwords_en)","78472388":"df_preprocessed['ja_preprocessed'] = df_preprocessed['ja'].apply(lambda x: ja_preprocess(x))","cc7772b6":"def ja_pos(text):\n    nlp = spacy.load('ja_ginza')\n    lowered = text.lower()\n    normalized = neologdn.normalize(lowered)\n    pos_tagged = []\n    for i in nlp(normalized):\n        pos = i.pos_\n        pos_tagged.append(pos)\n    tagged_str = \" \".join(pos_tagged)\n    return tagged_str","a845fa3c":"df_preprocessed['ja_pos'] = df_preprocessed['ja'].apply(lambda x: ja_pos(x))\ndf_preprocessed.head(1)","27e7fb72":"def en_preprocess(text):\n    nlp = spacy.load('en_core_web_sm')\n    lowered = text.lower()\n    tokenized = []\n    for i in nlp(lowered):\n        word = i.lemma_\n        pos = i.pos_\n        if pos in [\"NOUN\", \"ADJ\", \"VERB\", \"ADV\", \"PROPN\"] and len(word) > 1 and i.text not in stopwords_en:\n            tokenized.append(word)\n    preprocessed = \" \".join(tokenized)\n    return preprocessed","7a449e58":"stopwords_en.extend(['',',','.','(',')',':','#','``',\"''\",'pic','twitter','com','jp'])","e334f437":"df_preprocessed['en_preprocessed'] = df_preprocessed['en'].apply(lambda x: en_preprocess(x))","203116b4":"def en_pos(text):\n    nlp = spacy.load('en_core_web_sm')\n    lowered = text.lower()\n    pos_tagged = []\n    for i in nlp(lowered):\n        pos = i.pos_\n        pos_tagged.append(pos)\n    tagged_str = \" \".join(pos_tagged)\n    return tagged_str","544bc328":"df_preprocessed['en_pos'] = df_preprocessed['en'].apply(lambda x: en_pos(x))\ndf_preprocessed.head(1)","9b4dda50":"df_preprocessed['chr_length_ja'] = df_preprocessed['ja'].apply(lambda x: len(x))","5b51af74":"def word_length(text):\n    nlp = spacy.load('ja_ginza')\n    lowered = text.lower()\n    normalized = neologdn.normalize(lowered)\n    tokenized = []\n    for i in nlp(normalized):\n        word = i.lemma_\n        pos = i.pos_\n        # extract content words\n        if pos in [\"NOUN\", \"ADJ\", \"VERB\", \"ADV\", \"PROPN\", \"PRON\"]:\n            tokenized.append(word)\n    word_len = np.mean([len(i) for i in tokenized])\n    return word_len","23f5c980":"df_preprocessed['word_length_ja'] = df_preprocessed['ja'].apply(lambda x: word_length(x))","b71d6baf":"def sentence_num(text):\n    # Japanese text mostly end with \"\u3002\"\n    p = regex.compile(\"\u3002\")\n    punc_num = len(regex.findall(p, text))\n    return punc_num","7f289ff5":"df_preprocessed['sent_num_ja'] = df_preprocessed['ja'].apply(lambda x: sentence_num(x))","9d6109cd":"def kanji_count(text):\n    # get KANJI words by regular expression\n    p = regex.compile(r'\\p{Script_Extensions=Han}+')\n    kanji_list = regex.findall(p, text)\n    kanji_ratio = len(kanji_list) \/ len(text)\n    return kanji_ratio","7ed45398":"df_preprocessed['kanji_ratio_ja'] = df_preprocessed['ja'].apply(lambda x: kanji_count(x))","16c31c74":"def foreign_word_count(text):\n    # extract KATAKANA characters using regular expression\n    p = regex.compile(r'[\\u30A1-\\u30F4]+')\n    foreign_word_list = regex.findall(p, text)\n    foreign_word_ratio = len(foreign_word_list) \/ len(text)\n    return foreign_word_ratio","af530396":"df_preprocessed['foreign_word_ratio_ja'] = df_preprocessed['ja'].apply(lambda x: foreign_word_count(x))","9456bb92":"def ttr(text, N):\n    text = text.split()\n    # token frequency in the original text\n    N0 = len(text)\n    # type frequency in the original document\n    V0 = len(set(text))\n    \n    # total kind of words that appear m times in a document of size N0 : spectrum[m]\n    counter = collections.Counter(text)\n    m = set(counter.values())\n    spectrum = collections.defaultdict(int)\n    for w, f in counter.most_common():\n        spectrum[f] += 1\n    \n    # calculate corrected offset based on N, the corrected token frequency\n    offset = 0\n    for m in spectrum.keys():\n        if m%2 == 0:\n            offset -= ((N\/N0 -1)**m) * spectrum[m]\n        else:\n            offset += ((N\/N0 -1)**m) * spectrum[m]\n    # corrected type frequency\n    VN = V0 + offset\n    \n    # corrected TTR\n    TTR = VN \/ N\n    return TTR","9f778814":"df_preprocessed['ttr_ja'] = df_preprocessed['ja_preprocessed'].apply(lambda x : ttr(x,20))","108384f7":"def taigendome_count(text):\n    nlp = spacy.load('ja_ginza')\n    lowered = text.lower()\n    normalized = neologdn.normalize(lowered)\n    tokenized = []\n    # get the text into POS-tagged\n    for token in nlp(normalized):\n        s = token.lemma_ + \"----\" + token.pos_\n        tokenized.append(s)\n    # conut the TAIGENDOME pairs\n    taigen_cnt = 0\n    for ind, tok in enumerate(tokenized):\n        lemma = tok.split(\"----\")[0]\n        pos = tokenized[ind-1].split(\"----\")[1]\n        if \"\u3002\" in lemma and pos in ('NOUN','ADJ','PRON','PROPN'):\n            taigen_cnt += 1\n    return taigen_cnt","23b38c5a":"df_preprocessed['taigendome_num_ja'] = df_preprocessed['ja'].apply(lambda x: taigendome_count(x))","fee1f318":"def ne_count(text):\n    nlp = spacy.load('ja_ginza')\n    lowered = text.lower()\n    normalized = neologdn.normalize(lowered)\n    ne_cnt = 0\n    # count the number of named entities\n    for ent in nlp(normalized).ents:\n        ne_cnt += 1\n    return ne_cnt","58e32f72":"df_preprocessed['ne_count_ja'] = df_preprocessed['ja'].apply(lambda x: ne_count(x))","b236f898":"def pronoun_count(text):\n    nlp = spacy.load('ja_ginza')\n    lowered = text.lower()\n    normalized = neologdn.normalize(lowered)\n    pron_cnt = 0\n    for i in nlp(normalized):\n        pos = i.pos_\n        # extract pronouns\n        if pos == \"PRON\":\n            pron_cnt += 1\n    return pron_cnt","70de25a5":"df_preprocessed['pronoun_count_ja'] = df_preprocessed['ja'].apply(lambda x: pronoun_count(x))","47db7f54":"def pic_flag(text):\n    if \"pic.twitter.com\" in text:\n        return 1\n    else:\n        return 0","e08b8dc9":"df_preprocessed['pic_flag_ja'] = df_preprocessed['ja'].apply(lambda x: pic_flag(x))","89f97700":"def tfidf_features(text, _max_features=10, _max_ngrams=2):\n    tfidf = TfidfVectorizer(max_features=_max_features, use_idf=True, ngram_range=(1,_max_ngrams))\n    vec = tfidf.fit_transform(text).toarray()\n    tfidf_df = pd.DataFrame(vec, columns=[\"TFIDF_\" + n for n in tfidf.get_feature_names()])\n    return tfidf_df","faf6f22d":"tfidf_df = tfidf_features(df_preprocessed[\"ja_preprocessed\"])\ntfidf_df.index = df_preprocessed.index\n\noriginal_tfidf_cols = list(tfidf_df.columns)\ntfidf_df = tfidf_df.rename(columns=lambda s: s.split(\"_\")[0]+\"_\"+str(original_tfidf_cols.index(s)))\ndf_preprocessed = pd.concat([df_preprocessed,tfidf_df], axis=1)\n\ntfidf_col_dict = collections.defaultdict(str)\nfor original, replaced in zip(original_tfidf_cols,tfidf_df.columns):\n    tfidf_col_dict[replaced] = original","8f6309eb":"def sentiment2score_ja(text):\n    analyzer = oseti.Analyzer()\n    sent_score = np.mean(analyzer.analyze(text))\n    return float(sent_score)","66c53b6d":"def sentiment2score_en(text):\n    analyzer = SentimentIntensityAnalyzer()\n    sent_score = analyzer.polarity_scores(text)[\"compound\"]\n    return float(sent_score)","05b73d65":"df_preprocessed['sentiment_score_ja'] = df_preprocessed['ja'].apply(lambda x: sentiment2score_ja(x))\ndf_preprocessed['sentiment_score_en'] = df_preprocessed['en'].apply(lambda x: sentiment2score_en(x))","ff61749d":"v1 = hv.Curve(df_preprocessed.groupby('month')[\"ja\"].count()).opts(opts.Curve(xlabel=\"Month\", ylabel=\"Tweets\", width=380, height=300,tools=['hover'],show_grid=True,title='Month Average Count'))\nv2 = hv.Curve(df_preprocessed.groupby('weekday')[\"ja\"].count().reindex(index=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']))\\\n    .opts(opts.Curve(xlabel=\"Month\", ylabel=\"Tweets\", width=380, height=300,tools=['hover'],show_grid=True,title='Weekday Average Count',fontsize={'xticks':7}))\nv1 + v2","cb0760e6":"agg_1 = df_preprocessed.groupby('month').aggregate({'replies':['mean'], 'retweets':['mean'], 'likes':['mean']})\nv1 = (hv.Curve(agg_1['replies'], label='replies') * hv.Curve(agg_1['retweets'], label='retweets') * hv.Curve(agg_1['likes'], label='likes'))\\\n    .opts(opts.Curve(xlabel=\"Month\", width=380, height=300,tools=['hover'],show_grid=True)).opts(legend_position='bottom',title='Month Trend Average')\n\nagg_2 = df_preprocessed.groupby('weekday').aggregate({'replies':['mean'], 'retweets':['mean'], 'likes':['mean']})\\\n    .reindex(index=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\nv2 = (hv.Curve(agg_2['replies'], label='replies') * hv.Curve(agg_2['retweets'], label='retweets') * hv.Curve(agg_2['likes'], label='likes'))\\\n    .opts(opts.Curve(xlabel=\"Weekday\", width=380, height=300,tools=['hover'],show_grid=True,fontsize={'xticks':7})).opts(legend_position='bottom',title='Weekday Trend Average')\n\nv1 + v2","02d9c146":"v1 = hv.Curve(df_preprocessed.groupby('month')[\"sentiment_score_ja\"].mean())\\\n    .opts(opts.Curve(xlabel=\"Month\", ylabel=\"Sentiment Score\", width=380, height=300,tools=['hover'],show_grid=True,title='Month Average Sentiment Score'))\nv2 = hv.Curve(df_preprocessed.groupby('weekday')[\"sentiment_score_ja\"].mean().reindex(index=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']))\\\n    .opts(opts.Curve(xlabel=\"Weekday\", ylabel=\"Sentiment Score\", width=380, height=300,tools=['hover'],show_grid=True,title='Weekday Average Sentiment Score',fontsize={'xticks':7}))\nv1 + v2","174fa186":"def ngram_func(ngram, text_series):\n    string_filterd =  text_series.sum().split()\n    dic = nltk.FreqDist(nltk.ngrams(string_filterd, ngram)).most_common(30)\n    ngram_df = pd.DataFrame(dic, columns=['ngram','count'])\n    ngram_df.index = [' '.join(i) for i in ngram_df.ngram]\n    ngram_df.drop('ngram',axis=1, inplace=True)\n    return ngram_df","3d2a1f18":"uni_ja = hv.Bars(ngram_func(1, df_preprocessed['ja_preprocessed'])[::-1]).opts(title=\"Japanese Unigram Count top-30\", color=\"red\", xlabel=\"Unigrams\", ylabel=\"Count\")\nbi_ja = hv.Bars(ngram_func(2, df_preprocessed['ja_preprocessed'])[::-1]).opts(title=\"Japanese Bigram Count top-30\", color=\"blue\", xlabel=\"Bigrams\", ylabel=\"Count\")\n(uni_ja + bi_ja).opts(opts.Bars(width=380, height=600,tools=['hover'],show_grid=True,invert_axes=True,fontsize={'title':10})).opts(shared_axes=False)","8b4ad5b4":"uni_en = hv.Bars(ngram_func(1, df_preprocessed['en_preprocessed'])[::-1]).opts(title=\"English Unigram Count top-30\", color=\"red\", xlabel=\"Unigrams\", ylabel=\"Count\")\nbi_en = hv.Bars(ngram_func(2, df_preprocessed['en_preprocessed'])[::-1]).opts(title=\"English Bigram Count top-30\", color=\"blue\", xlabel=\"Bigrams\", ylabel=\"Count\")\n(uni_en + bi_en).opts(opts.Bars(width=380, height=600,tools=['hover'],show_grid=True,invert_axes=True,fontsize={'title':10})).opts(shared_axes=False)","61741ccc":"uni_ja = hv.Bars(ngram_func(1, df_preprocessed['ja_pos'])[::-1]).opts(title=\"Japanese POS-Unigram Count top-30\", color=\"red\", xlabel=\"Unigrams\", ylabel=\"Count\")\nbi_ja = hv.Bars(ngram_func(2, df_preprocessed['ja_pos'])[::-1]).opts(title=\"Japanese POS-Bigram Count top-30\", color=\"blue\", xlabel=\"Bigrams\", ylabel=\"Count\")\n(uni_ja + bi_ja).opts(opts.Bars(width=380, height=600,tools=['hover'],show_grid=True,invert_axes=True,fontsize={'title':10})).opts(shared_axes=False)","1d8362f1":"uni_en = hv.Bars(ngram_func(1, df_preprocessed['en_pos'])[::-1]).opts(title=\"English POS-Unigram Count top-30\", color=\"red\", xlabel=\"Unigrams\", ylabel=\"Count\")\nbi_en = hv.Bars(ngram_func(2, df_preprocessed['en_pos'])[::-1]).opts(title=\"English POS-Bigram Count top-30\", color=\"blue\", xlabel=\"Bigrams\", ylabel=\"Count\")\n(uni_en + bi_en).opts(opts.Bars(width=380, height=600,tools=['hover'],show_grid=True,invert_axes=True,fontsize={'title':10})).opts(shared_axes=False)","29828149":"wordcloud = WordCloud(font_path='\/kaggle\/working\/NotoSansCJKjp-Regular.otf', background_color='white',width=800, height=600, \\\n                      min_font_size=10, max_words=500, collocations=False, min_word_length=2, stopwords = stopwords_ja)\nwordcloud.generate(' '.join(df_preprocessed['ja_preprocessed']))\nplt.figure(figsize=(15, 10))\nplt.imshow(wordcloud)\nplt.show()","ece82352":"wordcloud = WordCloud(background_color='white',width=800, height=600, \\\n                      min_font_size=10, max_words=500, collocations=True, min_word_length=2, stopwords = stopwords_en)\nwordcloud.generate(' '.join(df_preprocessed['en_preprocessed']))\nplt.figure(figsize=(15, 10))\nplt.imshow(wordcloud)\nplt.show()","92734eba":"def lgb_modeling(target):\n    y_series = df_preprocessed[target]\n    x_df = df_preprocessed.drop([\"ja\", \"en\", \"replies\", \"retweets\", \"likes\", \"ja_preprocessed\", \"ja_pos\", \"en_preprocessed\", \"en_pos\"], axis=1)\n    x_df['weekday'] = LabelEncoder().fit_transform(x_df['weekday']).astype(np.int8)\n    X_train, X_valid, Y_train, Y_valid = train_test_split(x_df, y_series, test_size=0.2, random_state=0)\n\n    lgb_train = lgb.Dataset(X_train, Y_train)\n    lgb_valid = lgb.Dataset(X_valid, Y_valid, reference=lgb_train)\n    \n    params = {\n        'task' : 'train',\n        'boosting':'gbdt',\n        'objective' : 'regression',\n        'metric' : {'mse'},\n        'num_leaves':200,\n        'drop_rate':0.05,\n        'learning_rate':0.1,\n        'seed':0\n    }\n    model = lgb.train(\n                params,\n                lgb_train,\n                num_boost_round=100,\n                valid_sets=lgb_valid,\n                early_stopping_rounds=100\n            )\n    return model, X_train","9efd77e4":"model_1, X_train = lgb_modeling('replies')","7182af76":"print(tfidf_col_dict)\nexplainer = shap.TreeExplainer(model=model_1)\nshap_values_1 = explainer.shap_values(X=X_train)\nshap.summary_plot(shap_values=shap_values_1, features=X_train, feature_names=X_train.columns, plot_type=\"violin\", max_display=20)","b563b404":"model_2, X_train = lgb_modeling('retweets')","708e3736":"print(tfidf_col_dict)\nexplainer = shap.TreeExplainer(model=model_2)\nshap_values_2 = explainer.shap_values(X=X_train)\nshap.summary_plot(shap_values=shap_values_2, features=X_train, feature_names=X_train.columns, plot_type=\"violin\", max_display=20)","49cbfa3b":"model_3, X_train = lgb_modeling('likes')","d3643cec":"print(tfidf_col_dict)\nexplainer = shap.TreeExplainer(model=model_3)\nshap_values_3 = explainer.shap_values(X=X_train)\nshap.summary_plot(shap_values=shap_values_3, features=X_train, feature_names=X_train.columns, plot_type=\"violin\", max_display=20)","95b370df":"wordcloud = WordCloud(font_path='\/kaggle\/working\/NotoSansCJKjp-Regular.otf', background_color='white',width=800, height=600, \\\n                      min_font_size=10, max_words=500, collocations=False, min_word_length=2, stopwords = stopwords_ja)\nwordcloud.generate(' '.join(df_preprocessed[df_preprocessed.month>=8]['ja_preprocessed']))\nplt.figure(figsize=(15, 10))\nplt.title(\"WordCloud since August\")\nplt.imshow(wordcloud)\nplt.show()","b4cd8521":"!rm -rf \/kaggle\/working\/*.txt \/kaggle\/working\/*.otf \/kaggle\/working\/mecab-ipadic-neologd\/","ef295db6":">[neologdn](https:\/\/github.com\/ikegami-yukino\/neologdn) is tool for Japanese text normalization.","9e8c6d48":"## Word Length\n><div class=\"alert alert-info\" role=\"alert\">\n>Length of each word (especially <b>content words<\/b>) may affect <b>readability<\/b> of tweets, so we add mean of word length in texts.\n><\/div>","d9a3647c":"## Replies Prediction\n><div class=\"alert alert-success\" role=\"alert\">\n><ul>\n><li>Tweets that have <b>longer sentences<\/b> tend to get less replies. I think this is because <u>people have a little diffuculty or hesitation to read and reply against long tweets<\/u>.<\/li>\n><li>Tweets with <b>images<\/b> tend to be more replied, and I think <u>images help people to understand contents of tweets<\/u> and make it easy to reply.<\/li>\n><li>According to result of <b>TTR<\/b>, <u>moderate vocabulary size is probably one of the factors to keep the conversation going<\/u>, and replies increase.<\/li>\n><li>Too many <b>foreign words<\/b> avoid people from replying, and it can be said that <u>foreign words affect tweets' readability<\/u>.<\/li>\n><\/ul>\n><\/div>","281211db":"# 2. Import libraries","7f903a2d":"# 8. Conclusion\n><div class=\"alert alert-success\" role=\"alert\">\n><ul>\n><li>Tweets that have <b>longer sentences<\/b> tend to get less replies, retweets and likes. I think this is because <u>people have a little diffuculty or hesitation to read and reply against long tweets<\/u>.<\/li>\n><li>People tend to <u>think the tweet is more important and retweet or like it<\/u> when the tweet contains more <b>KANJI<\/b> characters.<\/li>\n><li><b>Positiveness of his tweets<\/b> affect the tendency of retweets and likes of them.<\/li>\n><li>Too many <b>named entities<\/b> may <u>prevent people from understanding the content of tweets and pushing retweet or like botton<\/u>.<\/li>\n><\/ul>\n>It is thought that <u>retweets and likes may receive some influence due to Mr. Abe's political action<\/u> later in the year.<br\/>\n>In the WordCloud of 8~12 in the year, words such as <u>election (\u9078\u6319, \u8846\u9662\u9078), speech (\u6f14\u8aac), candidates (\u5019\u88dc\u8005), policy (\u653f\u7b56), economics (\u7d4c\u6e08), diplomacy (\u5916\u4ea4), President (\u5927\u7d71\u9818) and USA (\u7c73\u56fd)<\/u> are shown.<br\/><br\/>\n>According to <a href='https:\/\/ja.wikipedia.org\/wiki\/%E7%AC%AC3%E6%AC%A1%E5%AE%89%E5%80%8D%E5%86%85%E9%96%A3_(%E7%AC%AC2%E6%AC%A1%E6%94%B9%E9%80%A0)'>this page<\/a>, his political actions at the time were as follows : \n><ul>\n><li>2016\/7 : The ruling party won the house of councillors election.<\/li>\n><li>2016\/8 : The 3rd Abe Cabinet was established.<\/li>\n><li>2016\/8 : He <a href='https:\/\/japan.kantei.go.jp\/97_abe\/statement\/201608\/1218775_11013.html'>announced<\/a> the continuation of Abenomics at a press conference when the 3rd Abe Cabinet was established.<\/li>\n><li>2016\/11 : He held a pre-inauguration meeting with President-elect Donald Trump, who won the US presidential election.<\/li>\n><li>2016\/12 : TPP approval bill and related bills were passed in Japan.<\/li>\n><li>2016\/12 : He visited Pearl Harbor in Hawaii, USA\u3000to memorialize victims of the Pearl Harbor attack.<\/li>\n><\/ul>\n><\/div>","c95d3d86":">Japanese stop words","b923a9df":"## KANJI Ratio\n><div class=\"alert alert-info\" role=\"alert\">\n>Too many <b>KANJI (\u6f22\u5b57)<\/b> words prevent people from reading.\n><\/div>","8f09a909":">Remove unnecessary columns for analysis and change column names","d7e8fd1a":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","77aae498":"## POS-Ngram","3d4500fe":">Since there are tweets other than Mr. Abe, we have to delete them.","64ee35fd":"## Retweets Prediction\n><div class=\"alert alert-success\" role=\"alert\">\n><ul>\n><li>According to result of <b>'month'<\/b>, it seems that <u>retweets increased due to <b>his some political activity<\/b> later in the year<\/u>.<\/li>\n><li>Tweets with more <b>KANJI<\/b> character get more retweets. I think this is because <u>the amount of KANJI character is expected to increase in Japanese sentences if official decisions or important matters are included<\/u>. So people tend to expect tweets with more KANJI character to have <b>more value worth retweet<\/b>.<\/li>\n><li><u>Higher <b>sentiment score<\/b> of tweets (in English) is<\/u>, more people tend to retweet.<\/li>\n><li><b>Too many named entities or too long sentence<\/b> may affect tweets' readability and ease of retweet.<\/li>\n><\/ul>\n><\/div>","5bb122f0":"## TF-IDF","57834904":"# 9. References\n>* Type-Token Ratio (TTR)  \n>https:\/\/www.sltinfo.com\/wp-content\/uploads\/2014\/01\/type-token-ratio.pdf\n>* Corrected TTR (Ja)  \n>https:\/\/qiita.com\/Hiroyuki1993\/items\/1d7d4baf046fbb4efd70  \n>* Measures of Lexical Richness (Ja)  \n>http:\/\/www.tufs.ac.jp\/ts\/personal\/corpuskun\/pdf\/2014\/LexMeasureHandOut.pdf\n>* Research on Readability of Japanese Document (Ja)  \n>https:\/\/www.anlp.jp\/proceedings\/annual_meeting\/2019\/pdf_dir\/C3-1.pdf\n>* spaCy 101: Everything you need to know  \n>https:\/\/spacy.io\/usage\/spacy-101\n>* spaCy 101: Everything you need to know (Ja)  \n>https:\/\/qiita.com\/miorgash\/items\/0eda4adcc8d9ecd143e6","3ab566b5":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","99358cd4":"### English POS-Ngrams","af1a8024":"## English Pre-Processing","13a86762":"## WordCloud","1768f9b7":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","09f1e580":"# 7. Modeling\n>We will build models to predict replies, retweets and likes, and know why these metrics behaves that way.","fcb9fdec":"## Likes Prediction\n><div class=\"alert alert-success\" role=\"alert\">\n><ul>\n><li>According to result of <b>'month'<\/b>, it seems that <u>likes increased due to <b>his some political activity<\/b> later in the year<\/u>.<\/li>\n><li><u>Higher <b>sentiment score<\/b> of tweets (in English) is<\/u>, more people tend to like the tweet.<\/li>\n><li><b>Too many named entities or too long sentence<\/b> may affect tweets' readability and ease of likes.<\/li>\n><\/ul>\n><\/div>","b904a0e5":"## Ngram","d6adbd72":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","41a60017":"## Japanese Pre-Processing","ecd97802":"# 5. Feature Engineering\n>Make Features based on Text Mining  ","2ce1b5df":"## Japanese POS-Tagging","ed0e1f04":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","1e4a2653":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","54ce87b9":">[oseti](https:\/\/github.com\/ikegami-yukino\/oseti) is python library for Japanese Sentiment Analysis","6417e361":"## Picture Flag\n><div class=\"alert alert-info\" role=\"alert\">\n>Containing image seems to be related to <b>readability<\/b> of tweets and <b>ease of pulling interest<\/b>.\n><\/div>","30f7df5c":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","a39b0b6c":">Convert original text into POS-tagged string for POS-Ngram analysis","aa6ea150":">[MeCab](https:\/\/taku910.github.io\/mecab\/) install for oseti","2a4c9122":"## Tweet Trend","6e1a0c0c":">Add new stop words","c5238b68":"### English WordCloud","5dc4d3a5":"## Pronoun Count\n><div class=\"alert alert-info\" role=\"alert\">\n>Too many use of pronouns seems to affect the difficulty of reading tweets.\n><\/div>","f8f89743":"## English POS-Tagging","a35ac69d":"### English Ngrams","2e25f511":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","c5324f11":"### Japanese WordCloud\n><div class=\"alert alert-success\" role=\"alert\">\n><ul>\n><li>Many occurrences of Mr. Abe (\u5b89\u500d, \u664b\u4e09), Japan (\u65e5\u672c), his political party (\u81ea\u6c11\u515a) and his title (\u7dcf\u88c1, \u7dcf\u7406)<\/li>\n><li><b>Diplomacy<\/b> related words : North Korea (\u5317\u671d\u9bae), abduction (\u62c9\u81f4), US (\u7c73\u56fd), President (\u5927\u7d71\u9818) and security (\u4fdd\u969c)<\/li>\n><li><b>\"Great East Japan Earthquake\"<\/b> related words : Fukushima (\u798f\u5cf6), safety (\u5b89\u5168), damage (\u88ab\u5bb3), support (\u652f\u63f4), Great East Japan Earthquake (\u6771\u65e5\u672c\u5927\u9707\u707d) and reconstruction (\u5fa9\u8208)<\/li>\n><\/ul>\n><\/div>","3de24262":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","aa270f5d":">English stop words","d4ab6d2d":"## Named Entity Count\n><div class=\"alert alert-info\" role=\"alert\">\n>The number of named entities seems to affect the difficulty of reading tweets.\n><\/div>","e7f57f16":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","229666ef":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","cf4a0ee2":"## Sentiment Analysis\n><div class=\"alert alert-info\" role=\"alert\">\n><b>Positiveness or negativeness<\/b> of tweets is thought to affect Like\/Retweets\/Reply of them.\n><\/div>","d7049b8b":"### Japanese POS-Ngrams","73034124":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","d0bc16ec":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","c9fcc767":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","cca576c2":">[ginza](https:\/\/github.com\/megagonlabs\/ginza) is NLP library for Japanese.","e8be37ec":"### Japanese Ngrams","ae032d7b":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","a278eec9":">Convert 'date' column into appropriate format","af4869b9":"## Tweet Count","d75f6bcd":"## Character Length\n><div class=\"alert alert-info\" role=\"alert\">\n>Length of each text may affect <b>readability<\/b> of tweets, so we add number of characters in texts. I think people tend to <b>avoid reading too long texts<\/b>.  \n><\/div>","9905f0b9":">Japanese font files for plotting WordCloud","b098e8d0":">Extract replies, retweets and likes.","bfa3615c":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","edab97fa":"# 1. Overview\n## Project Detail\n>In this notebook, we will practice text analysis using Twitter data of former Japanese Prime Minister Shinzo Abe.  \n>We will perform feature engineering using text mining and analyze the cause of the trend of his tweet.\n\n## Goal of this notebook\n>* Feature Engineering using Text Mining\n>* EDA technique\n>* Causal Analysis","b7af2370":"# Table of Contents<a id='top'><\/a>\n>1. [Overview](#1.-Overview)  \n>    * [Project Detail](#Project-Detail)\n>    * [Goal of this notebook](#Goal-of-this-notebook)\n>1. [Import libraries](#2.-Import-libraries)\n>1. [Load the dataset](#3.-Load-the-dataset)\n>1. [Pre-Processing](#4.-Pre-Processing)\n>    * [Date Information](#Date-Information)\n>    * [Japanese Pre-Processing](#Japanese-Pre-Processing)\n>    * [Japanese POS-Tagging](#Japanese-POS-Tagging)\n>    * [English Pre-Processing](#English-Pre-Processing)\n>    * [English POS-Tagging](#English-POS-Tagging)\n>1. [Feature Engineering](#5.-Feature-Engineering)\n>    * [Character Length](#Character-Length)\n>    * [Word Length](#Word-Length)\n>    * [Number of Sentences](#Number-of-Sentences)\n>    * [KANJI Ratio](#KANJI-Ratio)\n>    * [Foreign Words Ratio](#Foreign-Words-Ratio)\n>    * [TTR](#TTR)\n>    * [TAIGENDOME](#TAIGENDOME)\n>    * [Named Entity Count](#Named-Entity-Count)\n>    * [Pronoun Count](#Pronoun-Count)\n>    * [Picture Flag](#Picture-Flag)\n>    * [TF-IDF](#TF-IDF)\n>    * [Sentiment Analysis](#Sentiment-Analysis)\n>1. [EDA](#6.-EDA)\n>    * [Tweet Count](#Tweet-Count)\n>    * [Tweet Trend](#Tweet-Trend)\n>    * [Sentiment Trend](#Sentiment-Trend)\n>    * [Ngram](#Ngram)\n>    * [POS-Ngram](#POS-Ngram)\n>    * [WordCloud](#WordCloud)\n>1. [Modeling](#7.-Modeling)\n>    * [Replies Prediction](#Replies-Prediction)\n>    * [Retweets Prediction](#Retweets-Prediction)\n>    * [Likes Prediction](#Likes-Prediction)\n>1. [Conclusion](#8.-Conclusion)\n>1. [References](#9.-References)","cbfbba6d":"## Date Information","d4b89355":"## TAIGENDOME\n><div class=\"alert alert-info\" role=\"alert\">\n><b>TAIGENDOME (\u4f53\u8a00\u6b62\u3081)<\/b> is a style of writing that ends a sentence with a noun or pronoun.<br\/>\n>It is said that by inserting this appropriately, a rhythm is created in the sentence and it becomes easier to read.\n><\/div>","f548ac01":"## TTR\n><div class=\"alert alert-info\" role=\"alert\">\n>$TTR$ is an abbreviation for <b>Type Token Ratio<\/b>. <br\/>\n>$$ TTR = \\frac{V}{N} $$ <br\/>\n><ul>\n><li>$N (Tokens)$ : How many <b>total<\/b> words are used in the text?<\/li>\n><li>$V (Types)$ : How many <b>unique<\/b> words are used in the text?<\/li>\n><\/ul>\n>$TTR$ is a measure of vocabulary variation in the text (<a href='https:\/\/www.sltinfo.com\/wp-content\/uploads\/2014\/01\/type-token-ratio.pdf'>REF<\/a>). <br\/>\n>When we use conventional $TTR$, $V$ depends on $N$. So it is not suitable for comparing texts with different sentence lengths. <br\/>\n>Therefore, we need to adjust $N$ to some constant value and correct $V$ accordingly, and corrected-$TTR$ is calculated (<a href='https:\/\/www.springer.com\/gp\/book\/9780792370178'>REF<\/a>).\n><\/div>","cebae9f9":"## Sentiment Trend\n><div class=\"alert alert-success\" role=\"alert\">\n>Looking at Weekday plot, it can be said that <u>Sentiment Score (Positiveness\/Negativeness of his tweets) affect Tweet Trend<\/u>.\n><\/div>","0805e3b0":">Extract calendar-based features","b5fd1c22":"## Foreign Words Ratio\n><div class=\"alert alert-info\" role=\"alert\">\n>If there are too many <b>foreign words (\u5916\u6765\u8a9e)<\/b>, it will be difficult to read text fluently, or understand the meaning of the words.<br\/>\n>In Japanese, foreign words are written in <b>KATAKANA (\u30ab\u30bf\u30ab\u30ca)<\/b> character, so we add KATAKANA words frequency to the feature.\n><\/div>","ab9811bd":"# 6. EDA","d20549f3":"# 3. Load the dataset","2085ac38":"## Number of Sentences\n><div class=\"alert alert-info\" role=\"alert\">\n>Number of sentence may affect <b>readability<\/b> of tweets, so we add number of sentence.\n><\/div>","da838f7c":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","95b4fc7a":"<h2 style=\"text-align:center;font-size:200%;;\">NLP Feature Engineering for Twitter Analysis<\/h2>\n<h3  style=\"text-align:center;\">Keywords : <span class=\"label label-success\">NLP<\/span> <span class=\"label label-success\">EDA<\/span> <span class=\"label label-success\">Text Mining<\/span> <span class=\"label label-success\">Japanese<\/span> <span class=\"label label-success\">Feature Engineering<\/span> <span class=\"label label-success\">Causal Analysis<\/span> <span class=\"label label-success\">Sentiment Analysis<\/span><\/h3>","689a9203":"# 4. Pre-Processing","88dfa333":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>","4f5e4470":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>"}}