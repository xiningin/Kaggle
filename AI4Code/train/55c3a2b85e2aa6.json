{"cell_type":{"d6773b9a":"code","2f120ae1":"code","b76e0476":"code","f011384e":"code","978363b0":"code","088a56cf":"code","f5681b76":"code","c349959d":"code","f22a261a":"code","0257a477":"code","535c13ad":"code","d976a9eb":"code","551d2edb":"code","04cb4d2a":"code","110d1ed9":"code","102dd4ae":"code","e8f6de26":"code","085219c4":"code","e3c151f6":"code","cc45a390":"code","e7bfd788":"code","00238ee5":"code","0353ee0c":"code","3dea94c0":"code","d3797eaa":"code","d747e42a":"code","200330dc":"code","34c026c1":"code","b8115c68":"code","62238f15":"code","1a4664c1":"code","f9fc1820":"code","a734eb5a":"code","46adabf1":"code","a911b8ba":"code","1ef434ad":"code","a697542f":"code","ed091622":"code","9ef15ec9":"code","e82d01ac":"code","f4ee2cd3":"code","abb68d1a":"code","372ec382":"markdown","d897ae5a":"markdown","0cb795b3":"markdown","3e614b38":"markdown","86b35689":"markdown","5037778d":"markdown"},"source":{"d6773b9a":"# import initial libraries\nimport pandas as pd\nimport numpy as np\nfrom numpy import NaN\nimport seaborn as sns\nfrom scipy import stats\nnp.set_printoptions(threshold=np.inf)\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nfrom wordcloud import WordCloud\nimport nltk","2f120ae1":"train = pd.read_csv('..\/input\/mercari-price-suggestion-challenge\/train.tsv', sep = '\\t', low_memory=True)\ntrain.head()","b76e0476":"#function to get all info in one go\ndef full_info(df):\n    df_column=[]\n    df_dtype=[]\n    df_null=[]\n    df_nullc=[]\n    df_mean=[]\n    df_median=[]\n    df_std=[]\n    df_min=[]\n    df_max=[]\n    df_uniq=[]\n    df_count=[]\n    for col in df.columns: \n        df_column.append(  col )\n        df_dtype.append( df[col].dtype)\n        df_null.append( round(100 * df[col].isnull().sum(axis=0)\/len(df[col]),2))\n        df_nullc.append( df[col].isnull().sum(axis=0))\n        df_uniq.append( df[col].nunique()) if df[col].dtype == 'object' else df_uniq.append( NaN)\n        df_mean.append(  '{0:.2f}'.format(df[col].mean())) if df[col].dtype == 'int64' or df[col].dtype == 'float64' else df_mean.append( NaN)\n        df_median.append( '{0:.2f}'.format(df[col].median())) if df[col].dtype == 'int64' or df[col].dtype == 'float64' else df_median.append( NaN)\n        df_std.append( '{0:.2f}'.format(df[col].std())) if df[col].dtype == 'int64' or df[col].dtype == 'float64' else df_std.append( NaN)\n        df_max.append( '{0:.2f}'.format(df[col].max())) if df[col].dtype == 'int64' or df[col].dtype == 'float64' else df_max.append( NaN)\n        df_min.append( '{0:.2f}'.format(df[col].min())) if df[col].dtype == 'int64' or df[col].dtype == 'float64' else df_min.append( NaN)\n        df_count.append(df[col].count())\n    return pd.DataFrame(data = {'ColName':  df_column, 'ColType': df_dtype, 'NullCnt': df_nullc, 'NullCntPrcntg': df_null,  'Min': df_min, 'Max': df_max, 'Mean': df_mean, 'Med': df_median, 'Std': df_std, 'UniqCnt': df_uniq, 'ColValCnt': df_count})\n","f011384e":"# lets get full desciption of the data\nfull_info(train)","978363b0":"print(train['category_name'].str.count('\/').min())\nprint(train['category_name'].str.count('\/').max())","088a56cf":"# lets split the category into category splits\ntrain_sp = train.join(train['category_name'].str.split('\/', expand=True).add_prefix('category_split_'))\n\n# and lets see how the data looks\ntrain_sp.head()","f5681b76":"# lets get full desciption of the data again\ntrain_sp_info= full_info(train_sp)\ntrain_sp_info","c349959d":"# lets visualize the Null Count percentage graphically\ntrain_sp_info.plot.bar(x = 'ColName', y = 'NullCntPrcntg', figsize=(20, 5),rot=90, title='Missing (null) Feature Values')\nplt.show()","f22a261a":"# lets drop the category_split_3 and category_split_4 as they have most nulls. lets keep the brand for now.\ntrain_sp_trim=train_sp.drop(['category_split_3', 'category_split_4'],axis=1)\n# lets see how the data looks like now\ntrain_sp_trim","0257a477":"# lets remove the items with price of $0 as well as they are of no use in price prediction\ntrain_sp_trim = train_sp_trim[train_sp_trim.price != 0]\n","535c13ad":"# Create a function to impute missing values\ndef fill_missing_value(df):\n    df['category_split_0'].fillna(value = 'unknown', inplace=True)\n    df['category_split_1'].fillna(value = 'unknown', inplace=True)\n    df['category_split_2'].fillna(value = 'unknown', inplace=True)\n    df['brand_name'].fillna(value = 'unknown', inplace=True)\n    df['category_name'].fillna(value = 'unknown', inplace=True)\n    df['item_description'].fillna(value = 'No description yet', inplace=True)\n    \n    return df","d976a9eb":"# lets apply the fill_missing_value function on the data to fill the nulls\ntrain_fill = fill_missing_value(train_sp_trim)\n\n# lets get full desciption of the data again\nfull_info(train_fill)","551d2edb":"# lets see the price distribution visually\nsns.set()\nsns.distplot(train_fill['price'], bins = 50)\nplt.title('Price Distribution', fontsize=12);","04cb4d2a":"plt.figure(figsize=(20, 6))\nplt.hist(train_fill['price'], bins=50, range=[0,2010], label='price')\nplt.title('Price Distribution', fontsize=15)\nplt.xlabel('Price', fontsize=15)\nplt.ylabel('Samples', fontsize=15)\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.legend(fontsize=15)\nplt.show()","110d1ed9":"plt.figure(figsize=(20, 6))\nsns.distplot(np.log(train_fill['price']+1), fit = stats.norm)\nplt.xlabel('log(price+1)', fontsize=12)\nplt.title('Log Price Distribution', fontsize=12);","102dd4ae":"# lets see by brand name how its priced\nbrand = train_fill.groupby('brand_name').price.agg(['count','mean'])\nbrand = brand[brand['count']>1000].sort_values(by='mean', ascending=False)\nbrand.head(30)","e8f6de26":"# lets visualize the count by brand name\nbrand = train_fill['brand_name'].value_counts()\nfig = go.Figure([go.Pie(labels=brand.keys(), values=brand)])\nfig.update_traces( hoverinfo=\"label+percent\")\nfig.update_layout(title_text=\"% by Brand\")\nfig.show()","085219c4":"# lets visualize Top 75 Expensive Brands By Mean Price\nplt.figure(figsize=(25, 6))\ntop_brands = train_fill.groupby('brand_name', axis=0).mean()\ndf_expPrice = pd.DataFrame(top_brands.sort_values('price', ascending = False)['price'][0:75].reset_index())\n\n\nax = sns.barplot(x=\"brand_name\", y=\"price\", data=df_expPrice)\nax.set_xticklabels(ax.get_xticklabels(),rotation=90, fontsize=15)\nax.set_title('Top Expensive Brands', fontsize=15)\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.show()","e3c151f6":"# lets see by main category how its priced\nmaincat = train_fill.groupby('category_split_0').price.agg(['count','mean'])\nmaincat = maincat[maincat['count']>1000].sort_values(by='mean', ascending=False)\nmaincat.head(30)","cc45a390":"# lets visualize the count by main category or category_split_0\ncategories = train_fill['category_split_0'].value_counts()\nfig = go.Figure([go.Pie(labels=categories.keys(), values=categories)])\nfig.update_traces( hoverinfo=\"label+percent\")\nfig.update_layout(title_text=\"% by Main Category\")\nfig.show()","e7bfd788":"# lets visualize Top 75 category_split_0 By Mean Price\nplt.figure(figsize=(25, 6))\ncategory = train_fill.groupby('category_split_0', axis=0).mean()\ndf_expPrice = pd.DataFrame(category.sort_values('price', ascending = False)['price'][0:75].reset_index())\nresult = df_expPrice.groupby([\"category_split_0\"])['price'].aggregate(np.median).reset_index().sort_values('price')\n\nax = sns.barplot(x=\"category_split_0\", y=\"price\", data=df_expPrice)\nax.set_xticklabels(ax.get_xticklabels(),rotation=90, fontsize=15)\nax.set_title('Top Expensive Main Sub Category', fontsize=15)\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.show()","00238ee5":"#price comparison by item condition across 5 most frequent Main Categories\nplt.figure(figsize=(20, 6))\nsns.barplot(x='item_condition_id', y=\"price\", hue='category_split_0', data=train_fill[(train_fill['category_split_0'] == train_fill['category_split_0'].value_counts().index[0]) | (train_fill['category_split_0'] == train_fill['category_split_0'].value_counts().index[1]) | (train_fill['category_split_0'] == train_fill['category_split_0'].value_counts().index[2]) | (train_fill['category_split_0'] == train_fill['category_split_0'].value_counts().index[3]) | (train_fill['category_split_0'] == train_fill['category_split_0'].value_counts().index[4])])\nhandles, labels = plt.gca().get_legend_handles_labels()\nhandles = [handles[4], handles[3], handles[2], handles[0], handles[1]]\nlabels = [labels[4], labels[3], labels[2], labels[0], labels[1]]\nplt.legend(handles, labels, title='Top 5 categories1', loc='upper right');","0353ee0c":"# lets see by second main category or category_split_1 how its priced\nseccat = train_fill.groupby('category_split_1').price.agg(['count','mean'])\nseccat = seccat[seccat['count']>1000].sort_values(by='mean', ascending=False)\nseccat.head(30)","3dea94c0":"# Display Top 75 category_split_1 By Mean Price\nplt.figure(figsize=(25, 6))\ncategory = train_fill.groupby('category_split_1', axis=0).mean()\ndf_expPrice = pd.DataFrame(category.sort_values('price', ascending = False)['price'][0:75].reset_index())\n\nax = sns.barplot(x=\"category_split_1\", y=\"price\", data=df_expPrice)\nax.set_xticklabels(ax.get_xticklabels(),rotation=90, fontsize=15)\nax.set_title('Top Expensive category1', fontsize=15)\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.show()","d3797eaa":"#price comparison by item condition across 5 most frequent Second Main Categories\nplt.figure(figsize=(20, 6))\nsns.barplot(x='item_condition_id', y=\"price\", hue='category_split_1', data=train_fill[(train_fill['category_split_1'] == train_fill['category_split_1'].value_counts().index[0]) | (train_fill['category_split_1'] == train_fill['category_split_1'].value_counts().index[1]) | (train_fill['category_split_1'] == train_fill['category_split_1'].value_counts().index[2]) | (train_fill['category_split_1'] == train_fill['category_split_1'].value_counts().index[3]) | (train_fill['category_split_1'] == train_fill['category_split_1'].value_counts().index[4])])\nhandles, labels = plt.gca().get_legend_handles_labels()\nhandles = [handles[4], handles[3], handles[2], handles[0], handles[1]]\nlabels = [labels[4], labels[3], labels[2], labels[0], labels[1]]\nplt.legend(handles, labels, title='Top 5 categories2', loc='upper right');","d747e42a":"# lets see by third main category or category_split_2 how its priced\nthrdcat = train_fill.groupby('category_split_2').price.agg(['count','mean'])\nthrdcat = thrdcat[thrdcat['count']>1000].sort_values(by='mean', ascending=False)\nthrdcat.head(30)","200330dc":"# Display Top 75 category_split_2 By Mean Price\nplt.figure(figsize=(25, 6))\ntop_category2 = train_fill.groupby('category_split_2', axis=0).mean()\ndf_expPrice = pd.DataFrame(top_category2.sort_values('price', ascending = False)['price'][0:75].reset_index())\n\nax = sns.barplot(x=\"category_split_2\", y=\"price\", data=df_expPrice)\nax.set_xticklabels(ax.get_xticklabels(),rotation=90, fontsize=15)\nax.set_title('Top Expensive category2', fontsize=15)\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.show()","34c026c1":"#price comparison by item condition across 5 most frequent Third Main Categories\nplt.figure(figsize=(20, 6))\nsns.barplot(x='item_condition_id', y=\"price\", hue='category_split_2', data=train_fill[(train_fill['category_split_2'] == train_fill['category_split_2'].value_counts().index[0]) | (train_fill['category_split_2'] == train_fill['category_split_2'].value_counts().index[1]) | (train_fill['category_split_2'] == train_fill['category_split_2'].value_counts().index[2]) | (train_fill['category_split_2'] == train_fill['category_split_2'].value_counts().index[3]) | (train_fill['category_split_2'] == train_fill['category_split_2'].value_counts().index[4])])\nhandles, labels = plt.gca().get_legend_handles_labels()\nhandles = [handles[4], handles[3], handles[2], handles[0], handles[1]]\nlabels = [labels[4], labels[3], labels[2], labels[0], labels[1]]\nplt.legend(handles, labels, title='Top 5 categories3', loc='upper right');","b8115c68":"# lets visualize the count by item_condition_id\nconditioncnt = train_fill['item_condition_id'].value_counts()\nfig = go.Figure([go.Pie(labels=conditioncnt.keys(), values=conditioncnt)])\nfig.update_traces( hoverinfo=\"label+percent\")\nfig.update_layout(title_text=\"% by Item Condition\")\nfig.show()","62238f15":"# visualizing the price distribution by Item Condition\nplt.figure(figsize=(25, 8))\n\nsns.boxplot(x='item_condition_id', y=\"price\", data=train_fill)\nplt.ylim(0, 200);","1a4664c1":"#price comparison by item condition across 5 most frequent Category Names\nplt.figure(figsize=(20, 6))\nsns.barplot(x='item_condition_id', y=\"price\", hue='category_name', data=train_fill[(train_fill['category_name'] == train_fill['category_name'].value_counts().index[0]) | (train_fill['category_name'] == train_fill['category_name'].value_counts().index[1]) | (train_fill['category_name'] == train_fill['category_name'].value_counts().index[2]) | (train_fill['category_name'] == train_fill['category_name'].value_counts().index[3]) | (train_fill['category_name'] == train_fill['category_name'].value_counts().index[4])])\nhandles, labels = plt.gca().get_legend_handles_labels()\nhandles = [handles[4], handles[3], handles[2], handles[0], handles[1]]\nlabels = [labels[4], labels[3], labels[2], labels[0], labels[1]]\nplt.legend(handles, labels, title='Top 5 categories', loc='upper right');","f9fc1820":"# define function for text normalization\nimport string\n\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as stopwords\nfrom nltk.stem.porter import PorterStemmer\n\ndef text_normalization(text):\n    # lowercase words\n    text = text.lower()\n    # remove stopwords\n    text = ' '.join([i for i in text.split(' ') if i not in stopwords])\n    #remove digits\n    text = ''.join([i for i in text if not i.isdigit()])\n    # remove punctuation\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    # stemming\n    stemmer = PorterStemmer()\n    text = ' '.join([stemmer.stem(word) for word in text.split()])\n    return text","a734eb5a":"%%time\n\ntrain_fill['item_description_normalized'] = train_fill['item_description'].apply(text_normalization).copy()","46adabf1":"%%time\n\ntrain_fill['name_normalized'] = train_fill['name'].apply(text_normalization).copy()","a911b8ba":"pd.set_option('display.width', 1000)\n\n# check first item description\ntrain_fill['item_description'][:8]","1ef434ad":"# check first item description after normalization and compare with previous result\ntrain_fill['item_description_normalized'][:8]","a697542f":"train_fill['name'][:8]","ed091622":"train_fill['name_normalized'][:8]","9ef15ec9":"# Generate a word cloud image for name frequency\nwordcloud = WordCloud().generate((train_fill['name_normalized'].sample(100000) + ' ').sum())\nplt.figure(figsize=(20,8))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")","e82d01ac":"# Generate a word cloud image for description word frequency\nwordcloud = WordCloud().generate((train_fill['item_description_normalized'].sample(100000) + ' ').sum())\nplt.figure(figsize=(20,8))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")","f4ee2cd3":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\nvectorizer = CountVectorizer()","abb68d1a":"item_description_bow = vectorizer.fit_transform(train_fill['item_description_normalized'])\nitem_description_bow","372ec382":"looking at the graphs and data, as expected brand, categories and condition is influencing the pricing. \n\nnow lets look at the description and see what kinda effect it has on the pricing","d897ae5a":"## 2. Importing data","0cb795b3":"## 1. Importing libraries","3e614b38":"## 4. Tokenization & Text Cleaning","86b35689":"## 3. EDA & Feature Engineering","5037778d":"## 5. Vectorization"}}