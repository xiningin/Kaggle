{"cell_type":{"c99d8434":"code","dfeb9dfd":"code","5287ab56":"code","9b1e5369":"code","0a08bdd8":"code","b8ed20f1":"code","7d5229cd":"code","9a443aaa":"code","3b9b03b0":"code","6ac10875":"code","d1eb55fd":"code","aa2b77c5":"markdown","53428b81":"markdown","7723ad5e":"markdown","765dbe65":"markdown","6b232885":"markdown","501be2aa":"markdown","dcc06843":"markdown","f3bfb22c":"markdown","52204664":"markdown","2284cc6f":"markdown","d6e8833c":"markdown"},"source":{"c99d8434":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\ntrain_df = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/test.csv')\nprint(train_df.columns,test_df.columns)","dfeb9dfd":"train_text = list(train_df.text)\ntrain_sentiment = list(train_df.sentiment)\ntest_text = list(test_df.text)\ntest_sentiment = list(test_df.sentiment)\n","5287ab56":"import re\ntest_curated_text = []\ntrain_curated_text = []\nfor text in train_text:\n    train_curated_text.append( re.sub(r\"http\\S+\", \"\", str(text)))\nfor text in test_text:\n    test_curated_text.append(re.sub(r'http\\S+',\"\",str(text)))\n","9b1e5369":"from tensorflow.keras.preprocessing.text import Tokenizer\nclean_text = []\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(train_curated_text)\nword_index = tokenizer.word_index","0a08bdd8":"max_length = max([len(text) for text in train_curated_text])\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nsequences = tokenizer.texts_to_sequences(train_curated_text)\npadded = pad_sequences(sequences,maxlen=max_length, truncating='post')\n\ntesting_sequences = tokenizer.texts_to_sequences(test_curated_text)\ntesting_padded = pad_sequences(testing_sequences,maxlen=max_length)\nprint('sentence:',train_curated_text[2],'\\nencoding:',sequences[2],'\\npadded encoding:',padded[2])","b8ed20f1":"from sklearn import preprocessing\nlb = preprocessing.LabelBinarizer()\nlb.fit(train_sentiment)\ntraining_labels_final = lb.transform(train_sentiment)\ntesting_labels_final = lb.transform(test_sentiment)\nprint('Before Binarizing:',train_sentiment[4:7],'\\nAfter Binarizing',training_labels_final[4:7])","7d5229cd":"vocab_size = len(word_index)\nembedding_dim = 13\nimport tensorflow as tf\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length),\n    tf.keras.layers.GlobalAvgPool1D(),\n    tf.keras.layers.Dense(3, activation='softmax')\n])\nmodel.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()","9a443aaa":"num_epochs = 20\nhistory = model.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final))","3b9b03b0":"from matplotlib import pyplot as plt\nplt.plot(history.history['val_accuracy'])\nplt.plot(history.history['accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.show()","6ac10875":"inputs = np.append(padded,testing_padded,axis=0)\nlabels = np.append(training_labels_final,testing_labels_final,axis=0)\nnum_epochs = 8\nhistory = model.fit(inputs,labels, epochs=num_epochs)","d1eb55fd":"example = tokenizer.texts_to_sequences([\"i feel nausea\",'the model is doing pretty good','But not great'])\nexample = pad_sequences(example,maxlen=max_length)\npred = model.predict(example)\nprint(pred[0],pred[1],pred[2])","aa2b77c5":"## Architecture\n### Embedding Layer\nThese padded encodings are now fed into a embedding layer which is responsible for creating vector representation for each word based on the semantics. This helps the classifier in understanding the words more at the level of their meaning. The dimension of embedding to be used in this example is 13. The fourth root of vocabulary size (25330) is used as per the good rule of thumb. However, increase in dimensionality increases the quality of embedding. But this might go useless when large enough data is not available. So, I am using 13 dimension word embedding.\n### Dense Layer\nAfter Embedding layer, we have a global_average_pooling1d layer which is responsible for converting the 2d values from embedding layer into 1d by flattening them. This is done so that it can be fed into the next dense layer which requires one dimensional input. This Final Dense layer has three neurons each corresponding to the three possible categories, just like our binarized outputs.","53428b81":"## Output Preprocessing\nNow we have our input preprocessed. Turning to our output, it is still in textual format and must be converted into numbers for compuational purpose. This can be performed with the help of LabelBinarizer in sklearn library. It helps to encode categorical data into their binary matrix representation. ","7723ad5e":"## Inference:\n\nThe graph shows that the validation accuracy has flattened by 13th epoch. Training it further can cause overfitting of out model. However, we see jagged from 7-8 epoch. That again means overfitting and hence lets us fix our epoch number at 8 and retrain our model with both train and test data.","765dbe65":"## Text To Sequence\n\n{'i': 1, 'to': 2, 'the': 3, 'a': 4, 'my': 5, 'it': 6, 'you': 7, 'and': 8, 'is': 9, 'in': 10, 'for': 11, 's': 12, 'of': 13, 't': 14, 'that': 15, 'on': 16, 'me': 17, 'so': 18, 'have': 19, 'but': 20, 'm': 21, 'just': 22, 'day': 23, 'with': 24, 'be': 25, 'at': 26, 'not': 27, 'was': 28, 'all': 29, 'now': 30, 'can': 31, 'good': 32, 'this': 33, 'out': 34, 'up': 35, 'get': 36, 'no': 37, 'are': 38, 'like': 39, 'go': 40, 'your': 41, 'do': 42, 'work': 43, 'today': 44, 'love': 45, 'too': 46, 'going': 47, 'got': 48, 'we': 49, 'lol': 50, 'what': 51, 'happy': 52, 'one': 53, 'from': 54, 'time': 55, 'u': 56, 'know': 57, 'there': 58, 'really': 59, 'back': 60, 'will': 61, 'don': 62, 'about': 63, 'im': 64, 'had': 65, 'its': 66, 'am': 67, 'see': 68, 'some': 69, 'they': 70, 'if': 71, 'night': 72, 'new': 73, 'home': 74, '2': 75, 'want': 76, 'well': 77, 'how': 78, 'think': 79, 'as': 80, 'still': 81, 'when': 82, 'll': 83, 'more': 84, 'oh': 85, 'thanks': 86, 'off': 87, 'much': 88, 'here': 89, 'he': 90, 'great': 91, 'miss': 92, 'an': 93, 'hope': 94, 'has': 95, 'last': 96, 're': 97, 'morning': 98, 'need': 99, 'haha': 100, 'her': 101, 'been': 102, 'fun': 103, 'she': 104,...}\n\nSo, there are 25330 words found in the train_curated_text list, each assigned with a number from 0 to 25329.\nNow, each sentence must be encoded into list of numbers using this word_index generated with the tokenizer. These encode sentences must be of same length in order to be fed into a Neural Network. So we go in for padding of these encoded sentences. Shorter sentences as padded with zeros at the beginning while longer ones are truncated from at the end.","6b232885":"The above model shows a training accuracy of 72.49%\n## Prediction\nNow the trained model is used to make predictions on our new example","501be2aa":"## Training\nNow it is time to train our processed data into our model.","dcc06843":"## Visualization:\nLet us try to visualize the train and validation accuracy with each epoch to get better idea about the training process.","f3bfb22c":"Remember that the list shows probability of negative, neutral and positive respectively. The model is performing decently but one way to improve the model performance is by using Bigrams instead of single words separately. That way we could capture better contextual meaning. 'not great' and 'is great' cannot be put under positive just because they have the word 'great' in them. And similarly, 'not good' and 'not bad' can also get confusing when considering one word at a time. Whereas Bigrams can come in helpful in such scenarios. I am still a beginner and hope this helps others like me. I also look forward to know about my mistakes and areas to improve. Thanks for reaching the end.","52204664":"Now that we have cleaned the text, we can proceed to the second step of tokenizing the words.\n## Tokenizing\nThe tokenising part can be done manually. However there is an easier approach with the help of tensorflow's Tokenizer. This Tokenizer alots each word in the list of samples fed to it to a unique number. It generally creates a table containing the unique words and their corresponding numbers.","2284cc6f":"## Dataset\nThe dataset used is from a Kaggle competition - https:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction .I am going to perform Sentiment Analysis part alone using a very simple Neural Network. Since the data we are dealing with is chaotic and in the form of text, it needs few data preprocessing steps before feeding it into the Neural Network. Lets take a look at few of the text samples to get ideas about how to proceed from here.","d6e8833c":"## Data Preprocessing\n\n[' as much as i love to be hopeful, i reckon the chances are minimal =P i`m never gonna get my cake and stuff',\n 'I really really like the song Love Story by Taylor Swift',\n 'My Sharpie is running DANGERously low on ink',\n 'i want to go to music tonight but i lost my voice.',\n 'test test from the LG enV2',\n 'Uh oh, I am sunburned',\n ' S`ok, trying to plot alternatives as we speak *sigh*',\n 'i`ve been sick for the past few days  and thus, my hair looks wierd.  if i didnt have a hat on it would look... http:\/\/tinyurl.com\/mnf4kw',...]\n \nAs we can see from above list, it has urls embedded between the text contents. They can be removed using regular expressions as follows."}}