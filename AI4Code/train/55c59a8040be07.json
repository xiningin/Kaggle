{"cell_type":{"494cac5d":"code","a1a043bb":"code","7700e556":"code","255b9ea8":"code","029c45f7":"code","d018356d":"code","4c8ed053":"code","7ba0abd6":"code","9153c7f0":"code","31355359":"code","c182045c":"code","0a006fab":"code","a6e426d5":"code","6a62c302":"code","004433cb":"code","76703e22":"code","f732eb18":"code","49e2be6e":"code","17a1b7f7":"code","8628610a":"code","95402a49":"code","333e7db2":"code","4a493320":"code","a7fcba36":"code","9b13f165":"code","cf6129b9":"code","17516fbb":"code","3fe7a914":"code","8cc41f76":"code","d33f07b8":"code","7ec061fc":"code","ea61ccc3":"code","0ffaed84":"code","9e1b3034":"code","035f7cc4":"code","2b7219a5":"code","5d040866":"code","debf974b":"code","b48f1fae":"code","3d0e77f2":"code","050b2b49":"code","b38e1cac":"code","2484df16":"code","543f22f4":"markdown","e4994df0":"markdown","5960e60a":"markdown","b3c78626":"markdown","047a52f2":"markdown","3b6b85c5":"markdown","3f681e55":"markdown","b2ed4156":"markdown","46919469":"markdown","96966c5b":"markdown","1913f218":"markdown","9e9851e1":"markdown","ce4de67c":"markdown","5d95e32c":"markdown","b6cc291e":"markdown","3bfda5e5":"markdown"},"source":{"494cac5d":"!pip install yfinance","a1a043bb":"import yfinance as yf\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom datetime import datetime","7700e556":"sp_wiki = \"https:\/\/en.wikipedia.org\/wiki\/List_of_S%26P_500_companies\"\nsp_wiki_df_list = pd.read_html(sp_wiki)","255b9ea8":"len(sp_wiki_df_list)","029c45f7":"sp_df = sp_wiki_df_list[0]","d018356d":"sp_df.head()","4c8ed053":"sp_ticker_list = list(sp_df['Symbol'].values)","7ba0abd6":"df = yf.download(sp_ticker_list)","9153c7f0":"df.shape","31355359":"sp_volume = df['Volume']","c182045c":"sp_volume.shape","0a006fab":"sp_volume.to_csv('sp_volume.csv')","a6e426d5":"sp_volume = pd.read_csv('..\/input\/sp-5001\/sp_volume (1).csv',index_col=0)","6a62c302":"sp_volume.index = pd.to_datetime(sp_volume.index)","004433cb":"type(sp_volume.index)","76703e22":"df_example = sp_volume.loc['2017-01-01':'2019-01-31',['AAPL', 'MSFT', 'AMZN','GOOGL']]","f732eb18":"df_example.tail()","49e2be6e":"df_example.describe()","17a1b7f7":"df_example.isnull().sum()","8628610a":"df_example1 = df_example[df_example.isna().any(axis=1)]\ndf_example1","95402a49":"companies = ['Apple','Microsoft','Amazon','Google']\nplt.figure(figsize=(15, 6))\nplt.subplots_adjust(top=1.25, bottom=1.2)\n\nfor i, company in enumerate(df_example.columns, 1):\n    plt.subplot(2, 2, i)\n    df_example[company].plot()\n    plt.ylabel('Volume')\n    plt.xlabel(None)\n    plt.title(f\"Daily sales volume of {companies[i-1]}\")\n    \nplt.tight_layout()","333e7db2":"sns.jointplot('AMZN', 'AMZN', df_example, kind='scatter', color='seagreen')","4a493320":"sns.jointplot('GOOGL', 'MSFT', df_example, kind='scatter')","a7fcba36":"sns.pairplot(df_example, kind='reg')","9b13f165":"# Set up our figure by naming it returns_fig, call PairPLot on the DataFrame\nreturn_fig = sns.PairGrid(df_example.dropna())\n\n# Using map_upper we can specify what the upper triangle will look like.\nreturn_fig.map_upper(plt.scatter, color='purple')\n\n# We can also define the lower triangle in the figure, inclufing the plot type (kde) \n# or the color map (BluePurple)\nreturn_fig.map_lower(sns.kdeplot, cmap='cool_d')\n\n# Finally we'll define the diagonal as a series of histogram plots of the daily return\nreturn_fig.map_diag(plt.hist, bins=30)","cf6129b9":"# Let's go ahead and use sebron for a quick correlation plot for the daily returns\nsns.heatmap(df_example.corr(), annot=True, cmap='summer')","17516fbb":"plt.figure(figsize=(16,6))\nplt.title('Volume of Apple sales')\nplt.plot(df_example['AAPL'])\nplt.xlabel('Date', fontsize=18)\nplt.ylabel('Sales volume', fontsize=18)\nplt.show()","3fe7a914":"df_example.interpolate(method='linear', inplace=True)\ndata = pd.DataFrame(df_example['AAPL'])","8cc41f76":"data.tail(22)","d33f07b8":"training_data_len = 525-21","7ec061fc":"21\/525   #how much test data we have","ea61ccc3":"df_example.isnull().sum()","0ffaed84":"# Let's check an example for Apple sales\ndataset = df_example['AAPL'].values\ndataset = dataset.reshape(-1,1)","9e1b3034":"#5% of the data will be the test data\n#training_data_len = int(np.ceil( len(dataset) * .95 ))\n#training_data_len","035f7cc4":"# Scale the data\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range=(0,1))\nscaled_data = scaler.fit_transform(dataset.reshape(-1, 1))\n\n#scaled_data","2b7219a5":"# Create the training data set \n# Create the scaled training data set\ntrain_data = scaled_data[0:int(training_data_len), :]\n# Split the data into x_train and y_train data sets\nx_train = []\ny_train = []\n\n# We will set a coefficient as number of days, which will be as training data\ns = 20\nfor i in range(s, len(train_data)):\n    x_train.append(train_data[i-s:i, 0])\n    y_train.append(train_data[i, 0])\n    #if i<= s+1:\n    #    print(x_train)\n    #    print(y_train)\n    #    print()\n        \n# Convert the x_train and y_train to numpy arrays \nx_train, y_train = np.array(x_train), np.array(y_train)\n\n# Reshape the data\nx_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n# x_train.shape","5d040866":"from keras.models import Sequential\nfrom keras.layers import Dense, LSTM\nimport tensorflow as tf\n\n# Build the LSTM model\nmodel = Sequential()\nmodel.add(LSTM(128, return_sequences=True, input_shape= (x_train.shape[1], 1)))\nmodel.add(LSTM(64, return_sequences=False))\nmodel.add(Dense(25))\nmodel.add(Dense(1))\n\nopt = tf.keras.optimizers.Adam(learning_rate=0.001)\n# Compile the model\nmodel.compile(optimizer=opt, loss='mean_squared_error')\n\n# Train the model\nmodel.fit(x_train, y_train, batch_size=1, epochs=1)","debf974b":"x_train.shape","b48f1fae":"# Create the testing data set\n# Create a new array containing scaled values from index 1543 to 2002 \ntest_data = scaled_data[training_data_len - s: , :]\n# Create the data sets x_test and y_test\nx_test = []\ny_test = dataset[training_data_len:, :]\nfor i in range(s, len(test_data)):\n    x_test.append(test_data[i-s:i, 0])\n    \n# Convert the data to a numpy array\nx_test = np.array(x_test)\n\n# Reshape the data\nx_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1 ))\n\n# Get the models predicted price values \npredictions = model.predict(x_test)\npredictions = scaler.inverse_transform(predictions)\n\n# Get the root mean squared error (RMSE)\nrmse = np.sqrt(np.mean(((predictions - y_test) ** 2)))\nrmse","3d0e77f2":"# Plot the data\ntrain = data[:training_data_len]\nvalid = data[training_data_len:]\nvalid['Predictions'] = predictions\n# Visualize the data\nplt.figure(figsize=(16,6))\nplt.title('Model')\nplt.xlabel('Date', fontsize=18)\nplt.ylabel('Volume USD ($)', fontsize=18)\nplt.plot(train)\nplt.plot(valid[['AAPL','Predictions']])\nplt.legend(['Train', 'Val', 'Predictions'], loc='upper right')\n'''\nplt.figure(figsize=(16,6))\nplt.title('Volume of Apple sales')\nplt.plot(df_example['AAPL'])\nplt.xlabel('Date', fontsize=18)\nplt.ylabel('Sales volume', fontsize=18)\n'''\nplt.show()","050b2b49":"s = [i for i in range(15,30)]\nrmse_list = []\nfor j in s:\n    # Create the training data set \n    # Create the scaled training data set\n    train_data = scaled_data[0:int(training_data_len), :]\n    # Split the data into x_train and y_train data sets\n    x_train = []\n    y_train = []\n\n\n    for i in range(j, len(train_data)):\n        x_train.append(train_data[i-j:i, 0])\n        y_train.append(train_data[i, 0])\n        #if i<= s+1:\n        #    print(x_train)\n        #    print(y_train)\n        #    print()\n\n    # Convert the x_train and y_train to numpy arrays \n    x_train, y_train = np.array(x_train), np.array(y_train)\n\n    # Reshape the data\n    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n    \n    \n    # Build the LSTM model\n    model = Sequential()\n    model.add(LSTM(128, return_sequences=True, input_shape= (x_train.shape[1], 1)))\n    model.add(LSTM(64, return_sequences=False))\n    model.add(Dense(25))\n    model.add(Dense(1))\n\n    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n    # Compile the model\n    model.compile(optimizer=opt, loss='mean_squared_error')\n\n    # Train the model\n    model.fit(x_train, y_train, batch_size=1, epochs=1)\n    \n    # Create the testing data set\n    # Create a new array containing scaled values from index 1543 to 2002 \n    test_data = scaled_data[training_data_len - j: , :]\n    # Create the data sets x_test and y_test\n    x_test = []\n    y_test = dataset[training_data_len:, :]\n    for i in range(j, len(test_data)):\n        x_test.append(test_data[i-j:i, 0])\n\n    # Convert the data to a numpy array\n    x_test = np.array(x_test)\n\n    # Reshape the data\n    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1 ))\n\n    # Get the models predicted price values \n    predictions = model.predict(x_test)\n    predictions = scaler.inverse_transform(predictions)\n\n    # Get the root mean squared error (RMSE)\n    rmse = np.sqrt(np.mean(((predictions - y_test) ** 2)))\n    rmse_list.append(rmse)\n#     print(f'RMSE = {rmse} for {j} number of elements')\n\n#     # Plot the data\n#     train = data[:training_data_len]\n#     valid = data[training_data_len:]\n#     valid['Predictions'] = predictions\n#     # Visualize the data\n#     plt.figure(figsize=(16,6))\n#     plt.title('Model')\n#     plt.xlabel('Date', fontsize=18)\n#     plt.ylabel('Volume USD ($)', fontsize=18)\n#     plt.plot(train)\n#     plt.plot(valid[['AAPL','Predictions']])\n#     plt.legend(['Train', 'Val', 'Predictions'], loc='upper right')\n\n#     plt.show()","b38e1cac":"min(60832775, 61211015, 59058928, 59648552) #20 elements has the minimal 59058928 rmse","2484df16":"plt.plot(s,rmse_list)","543f22f4":"20.02.2017,05.12.2018  - are the dates, for which we don't have volume values.","e4994df0":"# Working with volumes","5960e60a":"# Correlation","b3c78626":"# Risk","047a52f2":"Histograms show lognormal distribution","3b6b85c5":"# Prediction of sales volume","3f681e55":"Judging by the heatmap shown above, all companies have high correlation between each other. <br>\nThe highest values are shown by **Microsoft** and **Amazon**.","b2ed4156":"# Data preparation","46919469":"## LSTM modelling","96966c5b":"## Moving average","1913f218":"Can be found by the link","9e9851e1":"We will interpolate 2 missing values.","ce4de67c":"Can be used from the same source","5d95e32c":"# Daily return ","b6cc291e":"Can be used from here https:\/\/www.kaggle.com\/faressayah\/stock-market-analysis-prediction-using-lstm","3bfda5e5":"We will work with the data for 2017 and 2018. The prediction will be done for January 2019.<br>\nThe observed companies are S&P 500 top 4: Apple, Google, Microsoft and Amazon."}}