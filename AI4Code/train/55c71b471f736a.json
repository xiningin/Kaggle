{"cell_type":{"73341df2":"code","0567f2b4":"code","afc8f3e2":"code","75422101":"code","ef107a5b":"code","d3a79a6c":"code","4be9cc7b":"code","c5847b7b":"code","f4e02259":"code","8073f503":"code","fce0a06d":"code","e9c7bd89":"code","bc4cba53":"code","c4b560ad":"code","56c70f04":"markdown","15c45f35":"markdown","223ebf0f":"markdown"},"source":{"73341df2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0567f2b4":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport category_encoders as ce\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_validate \n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neighbors import NeighborhoodComponentsAnalysis\n\nfrom sklearn.metrics import roc_auc_score\n\n%matplotlib inline","afc8f3e2":"pip install sliced","75422101":"class Optimization():\n    \"\"\"\n    Exemplo de codigo base para busca no hyperopt\n    \n#######  Inicializa\u00e7\u00e3o e defini\u00e7\u00e3o do que registrar nos resultados  ###########\n    \n    from hyperopt import STATUS_OK\n    from timeit import default_timer as timer\n    from sklearn.cluster import OPTICS, cluster_optics_dbscan\n    from hyperopt import Trials\n\n\n    # Keep track of results\n    bayes_trials = Trials()\n\n    out_file = 'hyperopt_trials.csv'\n    of_connection = open(out_file, 'w')\n    writer = csv.writer(of_connection)\n\n    # Write the headers to the file\n    writer.writerow(['loss', 'params', 'iteration', 'estimators', 'train_time'])\n    of_connection.close()\n    \n\n######  Definir busca por algoritmo de busca bayesiana  ##########\n    \n    from hyperopt import tpe\n    # optimization algorithm\n    tpe_algorithm = tpe.suggest\n\n\n######  Fun\u00e7\u00e3o objetivo para ser otimizada  ###########\n\n    def objective(params):\n    \n        # Keep track of evals\n        global ITERATION\n    \n        ITERATION += 1\n\n        start = timer()\n\n        loss = - Optimization.metodo_qualquer\n        \n        \n        print(ITERATION)\n    \n        #registers the time took\n        run_time = timer() - start\n    \n        # Write to the csv file ('a' means append)\n        of_connection = open(out_file, 'a')\n        writer = csv.writer(of_connection)\n        writer.writerow([loss, params, ITERATION, run_time])\n    \n        # Dictionary with information for evaluation\n        return {'loss': loss, 'params': params, 'iteration': ITERATION,\n                'train_time': run_time, 'status': STATUS_OK}\n\n\n\n######  Registro do resultado de cada intera\u00e7\u00e3o resultado  #######\n\n        from hyperopt import fmin\n        from hyperopt import Trials\n    \n        bayes_trials = Trials()\n        \n        \n###########################################################################        \n####      Rodar a otimizacao, ao ter todo o resto definido        #########        \n###########################################################################\n\n    %%capture\n\n    # Global variable\n    global  ITERATION\n\n    ITERATION = 0\n\n    # Run optimization\n    best = fmin(fn = objective, space = space, algo = tpe.suggest, \n                max_evals = 2000, trials = bayes_trials, rstate = np.random.RandomState(50))\n    \"\"\"\n    \n    def Oversampling(df, target, params, cv, df_test = None, classifier = None):\n        \"\"\"\n        Exemplo de espa\u00e7o a ser definido para busca\n\n        from hyperopt import hp\n        from hyperopt.pyll.stochastic import sample\n\n        space = {\n        'sampling_strategy' :  hp.uniform('sampling_strategy', 0.5, 1),\n        'k_neighbors' : hp.quniform('k_neighbors', 1, 5, 1),\n        'n_jobs' : -1}\n\n        params = sample(space)\n        \"\"\"\n\n        from imblearn.over_sampling import SMOTE\n\n        for parameter_name in ['k_neighbors']:\n            if parameter_name in set(params.keys()):\n                params[parameter_name] = int(params[parameter_name])\n\n        \n        print('ok')\n        sm = SMOTE(**params)\n\n        dataframe = df.drop([target], axis = 1)\n        target = df[target]\n\n        if classifier == None:\n\n            X_res, y_res = sm.fit_resample(dataframe, target)\n            return X_res.join(y_res)\n\n        elif (classifier != None) and (type(df_test) == type(None)):\n            skf = StratifiedKFold(n_splits = cv)\n            skf.get_n_splits(dataframe, target)\n\n            auc_scores = [] \n            for train_index, test_index in skf.split(X_train, y_train):\n                X_train_cv, X_test_cv = X_train.iloc[train_index], X_train.iloc[test_index]\n                y_train_cv, y_test_cv = y_train.iloc[train_index], y_train.iloc[test_index]\n\n                X_res, y_res = sm.fit_resample(X_train_cv, y_train_cv)\n\n                classifier.fit(nca.transform(X_res), y_res)\n                y_score = pd.DataFrame(classifier.predict_proba(nca.transform(X_test_cv)))\n                y_score = y_score[1]\n\n                auc_interaction_n = roc_auc_score(y_test_cv, y_score)\n                auc_scores.append(auc_interaction_n)\n\n\n            score = np.array(auc_scores).mean()\n            return score\n\n        elif (classifier != None) and (type(df_test) != type(None)):\n            print('certinho')\n            df_test_target = df_test[target.name]\n            df_test = df_test.drop([target.name], axis = 1)\n\n            \n            print('certinho2')\n            X_res, y_res = sm.fit_resample(dataframe, target)\n\n            classifier.fit(X_res, y_res)\n            y_score = pd.DataFrame( classifier.predict_proba(df_test) )[1]\n            score = roc_auc_score(df_test_target, y_score)\n\n            return score\n\n\n\n\n\n    def OutlierRemoval(df, target, target_class, params, cv = 5, df_test = None, classifier = None):\n        \"\"\"\n        Exemplo de espa\u00e7o a ser definido para busca\n\n        from hyperopt import hp\n        from hyperopt.pyll.stochastic import sample\n\n        space = {\n        'n_estimators' : hp.choice('n_estimators', [100, 150, 200]),\n        'max_samples' : hp.choice('max_samples', [128, 256, 512]),\n        'bootstrap' : hp.choice('bootstrap', [True, False]),\n        'contamination' : hp.uniform('contamination', 0.0001, 0.5),\n        'n_jobs' : -1}\n\n        params = sample(space)\n        \"\"\"\n\n        from sklearn.ensemble import IsolationForest\n\n        for parameter_name in ['max_samples', 'n_estimators']:\n            if parameter_name in set(params.keys()):\n                params[parameter_name] = int(params[parameter_name])\n\n        dataframe = df.drop([target], axis = 1)\n        target = df[target]\n\n        iso_for = IsolationForest(**params)\n\n        # Run the fit\n\n        target_query = target.name + ' == ' + str(target_class)\n        iso_for.fit(dataframe.join(target).query(target_query))\n\n        outliers = iso_for.predict(dataframe.join(target).query(target_query))\n        outliers_target_class = pd.Series(outliers).rename('outliers')\n\n\n        df_target_class = dataframe.join(target).query(target_query).reset_index()\n        df_target_class = df_target_class.join(outliers_target_class)\n        df_target_class = df_target_class.set_index('index')\n\n        outliers_index = df_target_class.query('outliers == -1').index\n\n        if classifier == None:\n\n            return dataframe.drop(outliers_index)\n\n        elif (type(classifier != None)) and (type(df_test) == type(None)):\n\n            skf = StratifiedKFold(n_splits = cv)\n            skf.get_n_splits(dataframe, target)\n\n            auc_scores = [] \n            for train_index, test_index in skf.split(X_train, y_train):\n                X_train_cv, X_test_cv = X_train.iloc[train_index], X_train.iloc[test_index]\n                y_train_cv, y_test_cv = y_train.iloc[train_index], y_train.iloc[test_index]\n\n                index = X_train.iloc[train_index].index\n                train_index_no_outlier = list(set(index) - set(outliers_index))\n\n                X_train_cv = X_train_cv.loc[train_index_no_outlier]\n                y_train_cv = y_train_cv.loc[train_index_no_outlier]\n\n                classifier.fit(nca.transform(X_train_cv), y_train_cv)\n                y_score = pd.DataFrame(classifier.predict_proba(nca.transform(X_test_cv)))\n                y_score = y_score[1]\n\n                auc_interaction_n = roc_auc_score(y_test_cv, y_score)\n                auc_scores.append(auc_interaction_n)\n\n\n            score = np.array(auc_scores).mean()\n            return score\n\n        elif (type(classifier) != type(None)) and (type(df_test) != type(None)):\n            df_test_target = df_test[target.name]\n            df_test = df_test.drop([target.name], axis = 1)\n\n            classifier.fit(nca.transform(dataframe), target)\n            y_score = classifier.predict_proba(nca.transform(df_test))\n            y_score = pd.DataFrame(y_score)[1]\n            score = roc_auc_score(df_test_target, y_score)\n\n            return score  \n\n\n\n\n\n\n    def SmoteReduction(df, target, params, cv = 5, df_test = None, classifier = None, undersampled_index = []):\n        \"\"\"\n        Exemplo de espa\u00e7o a ser definido para busca\n\n        from hyperopt import hp\n        from hyperopt.pyll.stochastic import sample\n\n        space = {\n\n            smote : {'sampling_strategy' :  hp.uniform('sampling_strategy', 0.5, 1),\n                     'k_neighbors' : hp.quniform('k_neighbors', 1, 5, 1),\n                     'n_jobs' : -1},\n\n            reduction : {'n_estimators' : hp.choice('n_estimators', [100, 150, 200]),\n                         'max_samples' : hp.choice('max_samples', [128, 256, 512]),\n                         'bootstrap' : hp.choice('bootstrap', [True, False]),\n                         'contamination' : hp.uniform('contamination', 0.0001, 0.5),\n                         'n_jobs' : -1}\n                }\n\n        params = sample(space)\n        \"\"\"\n        from sklearn.ensemble import IsolationForest\n        from imblearn.over_sampling import SMOTE\n\n\n        # Turning float to int parameter - Smote\n        for parameter_name in ['k_neighbors']:\n            if parameter_name in set(params['smote'].keys()):\n                params['smote'][parameter_name] = int(params['smote'][parameter_name])\n        sm = SMOTE(**params['smote'])\n\n\n        # Turning float to int parameter - Isolation Forest\n        for parameter_name in ['max_samples', 'n_estimators']:\n            if parameter_name in set(params['outliers'].keys()):\n                params['outliers'][parameter_name] = int(params['outliers'][parameter_name])\n\n\n        iso_for = IsolationForest(**params['outliers'])\n\n\n        dataframe = df.drop([target], axis = 1)\n        target = df[target]\n\n        target_query = target.name + ' == 1'\n        iso_for.fit(dataframe.join(target).query(target_query))\n\n\n        if classifier == None:\n\n\n            X_res, y_res = sm.fit_resample(dataframe, target)\n\n            outliers = iso_for.predict(X_res.join(y_res).query(target_query))\n            outliers_target_class = pd.Series(outliers).rename('outliers')\n\n\n            df_target_class = X_res.join(y_res).query(target_query).reset_index()\n            df_target_class = df_target_class.join(outliers_target_class)\n            df_target_class = df_target_class.set_index('index')\n\n            outliers_index = df_target_class.query('outliers == -1').index\n\n            df_return = X_res.join(y_res).drop([outliers_index])\n            return df_return\n\n\n\n        elif (classifier != None) and (df_test == None):\n\n\n            skf = StratifiedKFold(n_splits = cv)\n            skf.get_n_splits(dataframe, target)\n\n            auc_scores = [] \n            for train_index, test_index in skf.split(X_train, y_train):\n                X_train_cv, X_test_cv = X_train.iloc[train_index], X_train.iloc[test_index]\n                y_train_cv, y_test_cv = y_train.iloc[train_index], y_train.iloc[test_index]\n\n                X_res, y_res = sm.fit_resample(X_train_cv, y_train_cv)\n\n                outliers = iso_for.predict(X_res.join(y_res).query(target_query))\n                outliers_target_class = pd.Series(outliers).rename('outliers')\n\n\n                df_target_class = X_res.join(y_res).query(target_query).reset_index()\n                df_target_class = df_target_class.join(outliers_target_class)\n                df_target_class = df_target_class.set_index('index')\n\n                outliers_index = df_target_class.query('outliers == -1').index\n\n                # undersampled_index remove indices indicados por procedimentos de undersampling realizados anteriormente no dataset\n                # se undersampled_index n\u00e3o for declarado, os indices ser\u00e3o um conjunto vazio e nada ser\u00e1 retirado durante os treinos\n                undersampled_index = set(undersampled_index)\n\n                train_index = set(X_res.index)\n                index_sem_outliers = train_index - set(outliers_index) - undersampled_index\n\n                X_res = X_res.loc[index_sem_outliers]\n                y_res = y_res.loc[index_sem_outliers]\n\n                classifier.fit(nca.transform(X_res), y_res)\n                y_score = pd.DataFrame(classifier.predict_proba(nca.transform(X_test_cv)))\n                y_score = y_score[1]\n\n                auc_interaction_n = roc_auc_score(y_test_cv, y_score)\n                auc_scores.append(auc_interaction_n)\n\n\n            score = np.array(auc_scores).mean()\n            return score\n        \n        \n        \n        \n        \n        \n\n\n\n    def wrapper(df, features = None):\n        \"\"\"\n        Exemplo de espa\u00e7o a ser definido para busca\n\n        from hyperopt import hp\n        from hyperopt.pyll.stochastic import sample\n\n        space = {\n        \n                }\n\n        params = sample(space)\n        \"\"\"\n        if features == None:\n            return df\n\n        else:\n            active_features = []\n            for col in features.keys():\n                if features[col] == 1:\n                    active_features.append(col)\n\n        return df[active_features]        \n\n    \n\n\n    \n\n\n    def feature_transform(df, target, method, components, gamma = 1, rbf_components = 100):\n        \"\"\"\n        Exemplo de espa\u00e7o a ser definido para busca\n\n        from hyperopt import hp\n        from hyperopt.pyll.stochastic import sample\n\n        space = {\n\n                }\n\n        params = sample(space)\n        \"\"\"\n\n\n        #pip install sliced\n        from sklearn.decomposition import PCA\n        from sklearn.neighbors import NeighborhoodComponentsAnalysis\n        from sklearn.cross_decomposition import PLSRegression\n        from sliced import SlicedInverseRegression\n        from sliced import SlicedAverageVarianceEstimation\n        from sklearn.kernel_approximation import RBFSampler\n        from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n    ####################################################################################################################################    \n\n        def transformation(df, target, transformer):\n\n            if transformer == None:\n                return df\n\n            else:\n                fit_df =  (\n                            df.sample(n = 15000, random_state = 42) if type(transformer) == type(NeighborhoodComponentsAnalysis()) else \n                            df\n                          )\n\n                target = (\n                           target.loc[fit_df.index] if type(transformer) == type(NeighborhoodComponentsAnalysis()) else\n                           target\n                         )\n\n                transformer.fit(fit_df, np.ravel(target))    \n\n\n                df_transformed = pd.DataFrame( transformer.transform(df) , index = df.index )\n                df_transformed.columns = [method + '_' + str(i) for i in range (0, len(df_transformed.columns))]\n                return df_transformed\n\n    #####################################################################################################################################    \n\n        preprocess = (\n                        RBFSampler(gamma = gamma, n_components = rbf_components, random_state = 42) if method in ['ksir', 'ksave'] else\n                        NeighborhoodComponentsAnalysis(n_components = components, random_state = 42) if method == 'nca-klda' else\n                        None\n                     )\n\n        main_method  =      (\n                                PCA(n_components = components) if method == 'pca' else\n                                NeighborhoodComponentsAnalysis(n_components= components, random_state=42) if method == 'nca' else\n                                NeighborhoodComponentsAnalysis(n_components= components, random_state=42) if method == 'nca-lda' else\n                                RBFSampler(gamma = gamma, n_components = rbf_components, random_state = 42) if method == 'nca-klda' else\n                                PLSRegression(n_components = components) if method == 'pls' else\n                                PLSRegression(n_components = components) if method == 'pls-lda' else\n                                SlicedInverseRegression() if method == 'sir' else\n                                SlicedAverageVarianceEstimation() if method == 'save' else\n                                SlicedInverseRegression() if method == 'ksir' else\n                                SlicedAverageVarianceEstimation() if method == 'ksave' else\n                                None\n                            )\n\n\n        postprocess = (\n                        LinearDiscriminantAnalysis() if method in ['nca-klda', 'nca-lda', 'pls-lda'] else\n                        None\n                      )\n\n        print('oi')\n\n\n        preprocess_df = transformation(df = df, target = target, transformer = preprocess)\n        print('oi1')\n        transformed_df = transformation(df = preprocess_df, target = target, transformer = main_method)\n        print('oi2')\n        postprocess_df = transformation(df = transformed_df, target = target, transformer = postprocess)\n\n        return postprocess_df\n\n        \n        \n        \n        ","ef107a5b":"cd \/kaggle\/input\/creditcardfraud\n","d3a79a6c":"pandas_df = pd.read_csv('creditcard.csv')\n\ntarget = 'Class'","4be9cc7b":"print(pandas_df[target].value_counts())\n\npandas_df.describe()","c5847b7b":"X_train, X_test = pandas_df.query('Time < 120000').drop(['Time', target], axis =1) , pandas_df.query('Time >= 140000').drop(['Time', target], axis =1)\ny_train, y_test =  pandas_df.query('Time < 120000')[target], pandas_df.query('Time >= 140000')[target]","f4e02259":"TargetEncoder = ce.TargetEncoder()\n\n\nTargetEncoder.fit(X_train, y_train)\n\n\nX_train = TargetEncoder.transform(X_train)\nX_test = TargetEncoder.transform(X_test)\n\nX_train = X_train.fillna(-999)\nX_test = X_test.fillna(-999)","8073f503":"from sklearn.model_selection import cross_validate\n\nrf = RandomForestClassifier(n_estimators = 100, max_depth = 5, random_state = 42, n_jobs = -1, max_samples = 0.7)\n\nrf.fit(X_train, y_train)\n\npredicted_target = pd.DataFrame( rf.predict_proba(X_test) )[1]\n\n\nprint(roc_auc_score(y_test, predicted_target))","fce0a06d":"import csv\nfrom hyperopt import STATUS_OK\nfrom timeit import default_timer as timer\nfrom sklearn.cluster import OPTICS, cluster_optics_dbscan\nfrom hyperopt import Trials\n\n\n# Keep track of results\nbayes_trials = Trials()\n\nout_file = 'hyperopt_trials.csv'\nof_connection = open(out_file, 'w')\nwriter = csv.writer(of_connection)\n\n# Write the headers to the file\nwriter.writerow(['loss', 'params', 'iteration', 'estimators', 'train_time'])\nof_connection.close()\n\n\n\n\n\n####################################################################################################################\n\n\n\ndef objective(params):\n    \"\"\"Objective function for Hyperparameter Optimization\"\"\"\n    \n    # Keep track of evals\n    global ITERATION\n    \n    ITERATION += 1\n\n    start = timer()\n\n    rf = RandomForestClassifier(n_estimators = 100, max_depth = 5, random_state = 42, n_jobs = -1, max_samples = 0.7)\n    \n    loss = - Optimization.Oversampling(df = X_train.join(y_train), target = 'Class', params = params, cv = 5, df_test = X_test.join(y_test), classifier = rf)\n        \n        \n    print(ITERATION)\n    \n    #registers the time took\n    run_time = timer() - start\n    \n    \n    \n  ##############################################################################################################   \n    \n    \n    print(ITERATION)\n    \n    #registers the time took\n    run_time = timer() - start\n    \n    # Write to the csv file ('a' means append)\n    of_connection = open(out_file, 'a')\n    writer = csv.writer(of_connection)\n    writer.writerow([loss, params, ITERATION, run_time])\n    \n    # Dictionary with information for evaluation\n    return {'loss': loss, 'params': params, 'iteration': ITERATION,\n            'train_time': run_time, 'status': STATUS_OK}\n\n\n\n\n\n###################################################################################################################\n\n\n\n\nfrom hyperopt import hp\nfrom hyperopt.pyll.stochastic import sample\n\n# Define the search space\nspace = {\n    'sampling_strategy' :  hp.uniform('sampling_strategy', 0.5, 1),\n    'k_neighbors' : hp.quniform('k_neighbors', 1, 5, 1),\n    'n_jobs' : -1}\n    \nparams = sample(space)\nparams\n\n\n##################################################################################################################\n\n\n\n\nfrom hyperopt import tpe\n# optimization algorithm\ntpe_algorithm = tpe.suggest\n\n\n\n\n###################################################################################################################\n\n\n\nfrom hyperopt import fmin\n\n\nfrom hyperopt import Trials\n# Keep track of results\nbayes_trials = Trials()","e9c7bd89":"%%capture\n\n# Global variable\nglobal  ITERATION\n\nITERATION = 0\n\n# Run optimization\nbest = fmin(fn = objective, space = space, algo = tpe.suggest, \n            max_evals = 30, trials = bayes_trials, rstate = np.random.RandomState(50))","bc4cba53":"bayes_trials_results = sorted(bayes_trials.results, key = lambda x: x['loss'])\nbayes_trials_results","c4b560ad":"from imblearn.over_sampling import SMOTE\nsm = SMOTE(k_neighbors = 5, sampling_strategy = 0.873, n_jobs = -1)\n\nX_res, y_res = sm.fit_resample(X_train, y_train)\n\nrf.fit(X_res, y_res)\ny_score = pd.DataFrame(rf.predict_proba(X_test) )[1]\nscore = roc_auc_score(y_test, y_score)\n\nprint(score)","56c70f04":"# Busca bayesiana - Otimiza\u00e7\u00e3o de oversampling por Smote","15c45f35":"# Exemplo baseline, sem usar nenhuma t\u00e9cnica de oversample ou undersample","223ebf0f":"# Exemplo com otimiza\u00e7\u00e3o feita no Smote"}}