{"cell_type":{"0e47690d":"code","f86cddb6":"code","f088a823":"code","88be18a7":"code","71459e76":"code","c62a88ed":"code","b4147675":"code","648bb1e4":"code","e2bcbde8":"code","c4b8f295":"code","8b316db1":"code","5cb61612":"code","cda61c3f":"code","db98366a":"code","ccb875b5":"code","bbad1048":"code","4a61a532":"code","4ba07ac0":"code","c00893a3":"code","eacaad60":"code","cc811ecd":"code","007edd5a":"code","6e043dc2":"code","ba1cf733":"code","e433976b":"code","d08b592d":"code","797d7acb":"code","2e586542":"code","c91da4f6":"code","ff448c90":"code","cc9457e9":"code","f16f5453":"code","2360d936":"code","3dc31902":"code","6a85c43e":"code","62912b0c":"code","37dd84b7":"code","32fef730":"code","81a8fc7d":"code","87a2f62f":"code","588a0422":"code","26d15ba2":"code","43c09e8e":"code","5e5acdbb":"code","edc332d1":"code","9cf54d22":"code","6c1e6387":"code","5ef42a9f":"code","b0811fbd":"code","348c812b":"code","a1a93e60":"code","3cc3f72c":"markdown","7474f836":"markdown","cf23d6ba":"markdown","42adfd28":"markdown","a8417754":"markdown","aaa8cf68":"markdown","7a18d794":"markdown","d7f92295":"markdown","397a8ac8":"markdown","4a0e4650":"markdown","d8ba8d7d":"markdown","cb6fb6fa":"markdown","9f98dcc3":"markdown","289c4f68":"markdown","8fea3e40":"markdown","2b91c473":"markdown"},"source":{"0e47690d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f86cddb6":"# Step 0: Load dataset\nimport pandas as pd\nfrom time import time\nt0 = time()\ndata = pd.read_csv(\"\/kaggle\/input\/chicago-divvy-bicycle-sharing-data\/data.csv\")\nprint(\"Done in %0.3fs.\" % (time() - t0))","f088a823":"# Step 1: Data pre-processing\n# 1.1 First look into the dataset\ndata.head()","88be18a7":"print(\"The dataset contains {0[0]: ,.0f} rows and {0[1]: .0f} variables.\".format(data.shape))","71459e76":"data.info() # There is no missing value in the dataset","c62a88ed":"# 1.2 Distribution across the years\nimport matplotlib.pyplot as plt\nplt.style.use(\"seaborn\")\nfig = data[\"year\"].value_counts().sort_index().plot(kind=\"bar\")\nplt.title(\"Number of Trips in Each Year\")\nplt.show()","b4147675":"import matplotlib.pyplot as plt\nimport seaborn as sns\nyear_month_pivot = pd.pivot_table(data, index=\"year\", columns=\"month\", values=\"trip_id\", aggfunc=\"count\")\nfig, ax = plt.subplots(figsize=(10, 6))\nsns.heatmap(year_month_pivot)\nplt.title(\"Number of Trips per Year and per Month\")","648bb1e4":"# 1.2 Create weekend column\nt0 = time()\ndata[\"day\"].value_counts() # Confirm that this column indicates the day of week\ndata[[\"starttime\", \"day\"]].head() # Confirm the coding of DOW: 0 = Monday\ndowmapping = pd.DataFrame.from_dict({\"0\": \"Monday\", \"1\": \"Tuesday\", \"2\": \"Wednesday\", \n                                     \"3\": \"Thursday\", \"4\": \"Friday\", \"5\": \"Saturday\", \n                                     \"6\": \"Sunday\"}, \n                                    orient=\"index\")\ndowmapping.reset_index(inplace=True)\ndowmapping.columns = [\"num_coding\", \"dow\"]\ndowmapping[\"num_coding\"] = dowmapping[\"num_coding\"].astype(\"int64\")\ndata = pd.merge(data, dowmapping, how=\"left\", left_on=\"day\", right_on=\"num_coding\")\ndel(data[\"num_coding\"])\ndata[\"weekend\"] = data[\"day\"].apply(lambda x: x in (5, 6))\nprint(\"Done in %0.3fs.\" % (time()-t0))","e2bcbde8":"# 1.3 Create date column\n# This process can take 15 minutes.\n# t0 = time()\n# from datetime import datetime\n# data[\"starttime\"] = data[\"starttime\"].astype(\"str\")\n# data[\"starttime\"] = data[\"starttime\"].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\"))\n# data[\"startdate\"] = data[\"starttime\"].apply(lambda x: x.date())\n# data[\"starthour\"] = data[\"starttime\"].apply(lambda x: x.replace(minute=0))\n# data[\"startmonth\"] = data[\"startdate\"].apply(lambda x: x.replace(day=1))\n# # Since this step takes 15 minutes to complete this step I will save the file to avoid long runtime.\n# data.to_csv(\"data_preprocess.csv\")\n# print(\"Done in %0.3fs.\" % (time()-t0))","c4b8f295":"# data = pd.read_csv(\"data_preprocess.csv\")","8b316db1":"# 1.4 Creat starthour pivot table for prediction modelling\n# starthour_pivot = pd.pivot_table(data=data,\n#                                  index=[\"starthour\", \n#                                    \"year\", \"month\", \"day\", \"hour\", \"weekend\", \"startdate\",\n#                                    \"temperature\", \"events\", \n#                                    \"from_station_id\", \"from_station_name\"], \n#                                  values=\"trip_id\", aggfunc=\"count\")\n# starthour_pivot = pd.DataFrame(starthour_pivot)\n# starthour_pivot.to_csv(\"starthour_pivot.csv\")","5cb61612":"data.describe(include=\"all\")","cda61c3f":"# Step 2: Explanatory analysis\n# 2.1 Temporal trends\n# Monthly trend\nyear_month = pd.DataFrame(data.groupby([\"year\", \"month\"]).count()[\"trip_id\"])\nyear_month.reset_index(inplace=True)\nyear_month.columns = [\"year\", \"month\", \"count_trips\"]\ng = sns.catplot(x=\"month\", y=\"count_trips\", col=\"year\", data=year_month, kind=\"bar\")\nplt.subplots_adjust(top=0.9)\ng.fig.suptitle('Number of Trips per Month')\nplt.show()","db98366a":"# DOW trend\nyear_day = pd.DataFrame(data.groupby([\"year\", \"day\"]).count()[\"trip_id\"])\nyear_day.reset_index(inplace=True)\nyear_day.columns = [\"year\", \"day\", \"count_trips\"]\ng = sns.catplot(x=\"day\", y=\"count_trips\", col=\"year\", data=year_day, kind=\"bar\")\nplt.subplots_adjust(top=0.9)\ng.fig.suptitle('Number of Trips per DOW (0=Monday)')\nplt.show()","ccb875b5":"# Hourly trend\nday_hour = pd.DataFrame(data[data[\"year\"]==2017].groupby([\"day\", \"hour\"]).count()[\"trip_id\"])\nday_hour.reset_index(inplace=True)\nday_hour.columns = [\"day\", \"hour\", \"count_trips\"]\ng = sns.catplot(x=\"hour\", y=\"count_trips\", col=\"day\", data=day_hour, kind=\"bar\")\nplt.subplots_adjust(top=0.9)\ng.fig.suptitle('Number of Trips per Hour (day=0 means Monday)')\nplt.show()","bbad1048":"# 2.2 Geo patterns\n# Import packages\nimport plotly.graph_objects as go\nimport plotly.express as px","4a61a532":"# Generate the list of stations\nstart = data[[\"latitude_start\", \"longitude_start\", \"from_station_name\"]]\nstart.columns = [\"lat\", \"lon\", \"station_name\"]\nend = data[[\"latitude_end\", \"longitude_end\", \"to_station_name\"]]\nend.columns = [\"lat\", \"lon\", \"station_name\"]\nstation_info = (pd.concat([start, end])).groupby(\"station_name\").mean()[[\"lat\", \"lon\"]] # One station name can be mapped with multiple pairs of latitude and longitude\nstation_info.reset_index(inplace=True)\nstation_info.columns = [\"station_name\", \"lat\", \"lon\"]\n\nstation_start_trips = pd.DataFrame(data[data[\"year\"]==2017][\"from_station_name\"].value_counts())\nstation_start_trips.reset_index(inplace=True)\nstation_start_trips.columns = [\"station_name\", \"count_start_trips_2017\"]\nstation_info = station_info.merge(station_start_trips, how=\"left\")\n\nstation_end_trips = pd.DataFrame(data[data[\"year\"]==2017][\"to_station_name\"].value_counts())\nstation_end_trips.reset_index(inplace=True)\nstation_end_trips.columns = [\"station_name\", \"count_end_trips_2017\"]\nstation_info = station_info.merge(station_end_trips, how=\"left\")\n\nstation_info.info()","4ba07ac0":"station_info = station_info.dropna()","c00893a3":"station_info[\"count_start_trips_2017\"].describe()","eacaad60":"quantiles = []\nlevels = []\nfor i in range(11):\n    quantiles.append(np.percentile(station_info[\"count_start_trips_2017\"], i*10))\n    levels.append(\"level_\"+str(i+1))\nstation_info[\"count_start_trips_intervals\"] = pd.cut(station_info[\"count_start_trips_2017\"], quantiles, labels=levels[:-1])\nstation_info[station_info[\"count_start_trips_2017\"]==3][\"count_start_trips_intervals\"] = \"level_1\"\nstation_info = station_info.sort_values(by=\"count_start_trips_2017\", ascending=True)","cc811ecd":"plt.figure(figsize=(10,6))\nsns.scatterplot(x=\"lon\", y=\"lat\", hue=\"count_start_trips_intervals\", palette=\"RdBu\",\n                data=station_info, alpha=0.5)\nplt.title(\"Scatter Plot of Stations that were active in 2017 (colored by heat)\")","007edd5a":"# Plot all stations on Chicago map\n# px.set_mapbox_access_token(\"XXX\") # Replace XXX with your Mapbox Token\n# fig = px.scatter_mapbox(station_info.sort_values(by=\"count_start_trips_2017\", ascending=True),\n#                         lat=\"lat\", \n#                         lon=\"lon\",\n#                         color=\"count_start_trips_2017\",\n#                         color_continuous_scale=px.colors.sequential.Plasma,\n#                         text=\"station_name\",\n#                         opacity=0.7, \n#                         zoom=10)\n# fig.show()","6e043dc2":"station_info = station_info.sort_values(by=\"count_start_trips_2017\", ascending=False)\nstation_info[\"count_start_trips_cumsum\"] = station_info[\"count_start_trips_2017\"].cumsum()\n(station_info[\"count_start_trips_cumsum\"].reset_index(drop=True) \/ sum(station_info[\"count_start_trips_2017\"])).plot()\nplt.title(\"Cum. Density of Number of Trips Starting from Each Station\")\nplt.show()","ba1cf733":"# Plot an animation of scatter plot to see which are the most popular start stations in each hour\nstart_station_hour = data[data[\"year\"]==2017].groupby([\"from_station_name\", \"hour\"]).count()[\"trip_id\"].reset_index()\nstart_station_hour = start_station_hour.merge(station_info[[\"station_name\", \"lat\", \"lon\"]], how=\"left\", left_on=\"from_station_name\", right_on=\"station_name\")\ndel(start_station_hour[\"from_station_name\"])\nstart_station_hour = start_station_hour.rename({\"trip_id\": \"count_start_trips\"}, axis=\"columns\")","e433976b":"for hour in range(24):\n    # Slice the whole dataframe\n    station_start_trips_per_hour = start_station_hour[start_station_hour[\"hour\"]==hour]\n    # Take the hourly max\n    hourly_max = station_start_trips_per_hour[\"count_start_trips\"].max()\n    # Generate labels\n    start_station_hour.loc[start_station_hour[\"hour\"]==hour, \"start_trips_heat_per_hour\"] = start_station_hour.loc[start_station_hour[\"hour\"]==hour, \"count_start_trips\"].apply(lambda x: x\/hourly_max)","d08b592d":"fig = px.scatter(start_station_hour.sort_values(by=[\"hour\", \"count_start_trips\"], ascending=[True, True]),\n                 x=\"lon\", y=\"lat\", \n                 animation_frame=\"hour\",\n                 color=\"start_trips_heat_per_hour\", \n                 opacity=0.5,\n                 size=\"count_start_trips\", size_max=60,\n                 range_x=[-87.9,-87.5], range_y=[41.7, 42.1])\nfig.update_layout(title=\"Hourly Start Trips from Each Station (Year 2017)\")\nfig.show()","797d7acb":"station_info.info()","2e586542":"starthour = pd.read_csv(\"\/kaggle\/input\/starthour-pivot\/starthour_pivot.csv\")\nstarthour = starthour.rename({\"trip_id\": \"count_start_trips\"}, axis=\"columns\")\nstarthour.head()","c91da4f6":"print(\"We have {0[0]: ,.0f} records of hourly number of trips starting from each station.\".format(starthour.shape))","ff448c90":"# 2.3 Temparature\nstarthour[\"temperature\"].hist()\nplt.title(\"Histogram of Temperature\")\nplt.show()","cc9457e9":"starthour.head(10)","f16f5453":"starthour[\"events\"].value_counts()","2360d936":"# from datetime import datetime\nunknown_events = starthour[starthour[\"events\"]==\"unknown\"]\nunknown_events.groupby([\"starthour\"]).count()","3dc31902":"# Check the weather condition on \"201x-04-18 11:00:00\"\nstarthour[starthour[\"starthour\"].apply(lambda x: x in [\"2015-04-18 11:00:00\", \"2016-04-18 11:00:00\", \"2017-04-18 11:00:00\"])]                                               ","6a85c43e":"# Fill the weather condition for \"2014-04-18 11:00:00\" as \"cloudy\"\nstarthour.loc[starthour[\"starthour\"]==\"2014-04-18 11:00:00\", \"events\"] = \"cloudy\"","62912b0c":"# Check the weather condition on \"201x-06-30 10:00:00\"\nstarthour[starthour[\"starthour\"].apply(lambda x: x in [\"2014-06-30 10:00:00\", \"2015-06-30 10:00:00\", \"2017-06-130 10:00:00\"])]                                                                 ","37dd84b7":"# Fill the weather condition for \"2016-06-30 10:00:00\" as \"cloudy\"\nstarthour.loc[starthour[\"starthour\"]==\"2016-06-30 10:00:00\", \"events\"] = \"cloudy\"","32fef730":"starthour_agg = pd.pivot_table(starthour,\n                               index=['starthour', 'year', 'hour', 'temperature', 'events'],\n                               values=['count_start_trips'], \n                               aggfunc=np.sum)\nstarthour_agg.reset_index(inplace=True)","81a8fc7d":"hour_temp_sum_trips_2017 = pd.pivot_table(starthour_agg[starthour_agg[\"year\"]==2017],\n                                          index=['hour', 'temperature'],\n                                          values=['starthour', 'count_start_trips'], \n                                          aggfunc={\"starthour\": np.count_nonzero, \"count_start_trips\": np.sum})\nhour_temp_sum_trips_2017.reset_index(inplace=True)\nhour_temp_sum_trips_2017[\"avg_trips\"] = hour_temp_sum_trips_2017[\"count_start_trips\"] \/ hour_temp_sum_trips_2017[\"starthour\"]","87a2f62f":"g = sns.FacetGrid(hour_temp_sum_trips_2017,\n                  col=\"hour\", col_wrap=4,\n                  margin_titles=True)\ng.set(ylim=(0, 60))\ng = g.map(plt.scatter, \"temperature\", \"avg_trips\", s=25, color=\"c\", alpha=0.3)","588a0422":"# 2.4 Weather conditions\nhour_event_sum_trips_2017 = pd.pivot_table(starthour_agg[starthour_agg[\"year\"]==2017],\n                                           index=['hour', 'events'],\n                                           values=['starthour', 'count_start_trips'], \n                                           aggfunc={\"starthour\": np.count_nonzero, \n                                                    \"count_start_trips\": np.sum})\nhour_event_sum_trips_2017.reset_index(inplace=True)\nhour_event_sum_trips_2017[\"avg_trips\"] = hour_event_sum_trips_2017[\"count_start_trips\"] \/ hour_event_sum_trips_2017[\"starthour\"]","26d15ba2":"g = sns.FacetGrid(hour_event_sum_trips_2017,\n                  col=\"hour\", col_wrap=4, hue=\"events\",\n                  height=4, aspect=1.5,\n                  margin_titles=True)\ng.set(ylim=(0, 60))\ng = g.map(plt.bar, \"events\", \"avg_trips\")","43c09e8e":"starthour.info()","5e5acdbb":"starthour.head()","edc332d1":"starthour2017 = starthour[starthour[\"year\"]==2017]\nstarthour2017 = starthour2017.merge(station_info[[\"station_name\", \"lat\", \"lon\", \"count_start_trips_intervals\"]], \n                                    how = \"left\",\n                                    left_on =\"from_station_name\", right_on =\"station_name\")\ndel(starthour)","9cf54d22":"starthour2017.head()","6c1e6387":"# Step 3: Run prediction model\n# 3.1 Create dummy variables\nweekend = pd.get_dummies(starthour2017[\"weekend\"], prefix=\"weekend\", drop_first=True)\nevents = pd.get_dummies(starthour2017[\"events\"], prefix=\"event\", drop_first=True)\nstation_heat = pd.get_dummies(starthour2017[\"count_start_trips_intervals\"], prefix=\"station_heat\", drop_first=True)\nX = starthour2017[[\"month\", \"day\", \"hour\", \"temperature\", \"lat\", \"lon\"]]\nfor cols in [weekend, events, station_heat]:\n    X = pd.concat([X, cols], axis=1)\n    del(cols)","5ef42a9f":"# 3.2 Split train and test set\nfrom sklearn.model_selection import train_test_split\ny = starthour2017[['count_start_trips']]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\ndel(X)\ndel(y)","b0811fbd":"# 3.4 Linear Regression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score, mean_squared_error\nlr = LinearRegression()\nlr.fit(X_train, y_train)\ny_test_pred = lr.predict(X_test)\nprint(\"The R-square score of linear model on test set is {0: .4f}.\".format(r2_score(y_test, y_test_pred)))\nprint(\"The MSE of linear model on test set is {0: .4f}.\".format(mean_squared_error(y_test, y_test_pred)))","348c812b":"# 3.5 GradientBoostingRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import GridSearchCV\n# gbr = GradientBoostingRegressor()\n# parameters = {\n#     'n_estimators': [100, 120, 140],\n#     'max_depth': [3, 5, 7]\n# }\n# grid_search = GridSearchCV(estimator=gbr, param_grid=parameters, cv=3, n_jobs=1)\n# grid_search.fit(X_train, y_train)\n# print(\"The best parameters are: \\n\")\n# print(grid_search.best_params_)\nt0 = time()\ngbr = GradientBoostingRegressor(n_estimators=100, max_depth=3)\ngbr.fit(X_train, y_train)\nprint(\"Done in %0.3fs.\" % (time()-t0))\ny_test_pred = gbr.predict(X_test)\nprint(\"The R-square score of GBR model on test set is {0: .4f}.\".format(r2_score(y_test, y_test_pred)))\nprint(\"The MSE of GBR model on test set is {0: .4f}.\".format(mean_squared_error(y_test, y_test_pred)))\ngbr_feature_inportance = pd.DataFrame(gbr.feature_importances_, index=X_train.columns)\ngbr_feature_inportance.sort_values(by=0, ascending=True).tail(10).plot(kind=\"barh\")\nplt.title(\"Feature Importance by GBR Model\")\nplt.show()","a1a93e60":"# 3.3 XGBoost\nimport xgboost as xgb\nt0 = time()\nxgb_reg = xgb.XGBRegressor(n_estimators=100, max_depth=3)\nxgb_reg.fit(X_train, y_train)\nprint(\"Done in %0.3fs.\" % (time()-t0))\ny_test_pred = xgb_reg.predict(X_test)\nprint(\"The R-square score of XGBoost model on test set is {0: .4f}.\".format(r2_score(y_test, y_test_pred)))\nprint(\"The MSE of XGBoost model on test set is {0: .4f}.\".format(mean_squared_error(y_test, y_test_pred)))\nxgb_feature_inportance = pd.DataFrame(xgb_reg.feature_importances_, index=X_train.columns)\nxgb_feature_inportance.sort_values(by=0, ascending=True).tail(10).plot(kind=\"barh\")\nplt.title(\"Feature Importance by XGBoost Model\")\nplt.show()","3cc3f72c":"We find some events have value \"unknown\" and try to fill the missing value with approximate values.","7474f836":"# Appendix\n\nIntro to Animation in Plotly: https:\/\/plot.ly\/python\/animations\/\n\nClustering to Reduce Spatial Data Set Size: https:\/\/geoffboeing.com\/2014\/08\/clustering-to-reduce-spatial-data-set-size\/\n\nSeaborn \n* Plot color code: https:\/\/seaborn.pydata.org\/generated\/seaborn.color_palette.html#seaborn.color_palette\n* Facet grid plot: https:\/\/seaborn.pydata.org\/generated\/seaborn.FacetGrid.html\n\nXGBoost regressor: https:\/\/dask-ml.readthedocs.io\/en\/stable\/modules\/generated\/dask_ml.xgboost.XGBRegressor.html","cf23d6ba":"From the first 20 records we can see that temperature and events (i.e. weather conditions) are aggregated on hourly level across all stations. Therefore, to reduce run time we will generate a new pivot table only at hourly level and sum up trips from all stations during that hour.","42adfd28":"# Intro\n\nDivvy is a bike share program based in Chicago (https:\/\/www.divvybikes.com\/). With its app users can pay to use Divvy bikes from its 600+ stations across Chicago city. \n\nThe dataset contains 9 million riding trips that are powered by Divvy from 2014 to 2017. For each trip we know the start timestamp, the geogprahic locations of the start and end stations, temperature and weather conditions.\n\nThe goal of this project is to detect patterns in the usage of bike sharing servcies. By exploring the dataset we've managed to find something interesting:\n\n\ud83d\uddd3 **Monthly trend:** the usage of bike sharing services has a peak in summer seasons, i.e. from June to August.  \n\n\ud83d\udcbc **DOW trend:** in general people are more likely to ride shared bikes on weekdays thab over weekend.\n\n\u23f1 **Hourly trend:** we see different patterns on weekdays vs weekends. From Monday to Friday, the peak hours of Divvy services match with the commuting rush hours, i.e. 7am - 9am and 5pm - 6pm. On weekends the hourly patter is rather smooth without significant patterns, the usage is higher in the day than night.\n\n\ud83d\uddfa **Geo pattern:** by plotting all stations on the map and coloring them with \"heat\" (i.e. the total number of bike sharing trips starting from the station), we find that the closer the station is to the city center, the more biking sharing trips satrt from the station.\n\n\u23f1\/\ud83d\uddfa **Hourly geo parttern:** the centralisation of heat areas in city center is most obvious during peak hours (i.e. 7am - 9am and 5pm - 7pm). During other slots, the geo difference in traffic is less significant.\n\n\u2668\ufe0f **Temperatue:** there is a positive correlation beween temperature and the usage of biking riding service during daytime and eveninig (i.e. from 8am to approx. 10pm).\n\n\ud83c\udf26 **Weather conditions:** as one may expect, people are less likely to use bike sharing service in bad weathers like rain\/snow and thunderstorm.\n\nIn addition, we compare three models to predict the number of trips starting from a specific station during a specific hour. The XGBoost regress gives the best model (with the highest R2 score and the lowest MSE) and suggests the top3 most importan features are: months, whether the station is among the top10% in terms of heat (measured by the number of trips from the stations in 2017) and hours.\n\nSuggestion to further improve the prediction model: one can import the event calendar of Chicago city with must-have information like time, location and audience size and include this in the prediction model.","a8417754":"# Predict number of trips in a specific hour\n\nIn this section we will build a model to predict the number of trips in a specifc hour.","aaa8cf68":"### Weather Conditions\n\nAs expected, people are less likely to ride by bike in case of bad weather conditions like rain, snow or thunderstorms.","7a18d794":"### Hourly Trend\n\nOn weekdays the peak matches the rush hours: 8am and 5pm. On weekends the hourly trends are much smoother: the usage increased steadily since 9am and peaked at 12pm, then descreased gradually.\n\nTherefore, in later steps we would split weekends from weekdays.","d7f92295":"### Geographic Pattern\n\nAs one may expect, the closer the station is to the city center, the more riding trips come from the station.","397a8ac8":"### Temperature\n\nIn popular hours we can observe that a positive correation between temparature and the usage of bike sharing service.","4a0e4650":"# Explanatory Data Analysis","d8ba8d7d":"### Feature Importance\n\nWe test three models on our test set: linear regression, Gradient Boosting Regressor and XGBoost Regressor (with highest R2 score and lowest MSE).\n\nThe XGBoost mdoel suggests that the top3 features with highest explaining power are: months, whether the station is among the top10% stations in terms of \"heat\" and hours.","cb6fb6fa":"# Data Pre-processing\n","9f98dcc3":"From the cum. density we can see that 200 out of 592 stations account for 80% of all bike riding trips. ","289c4f68":"### Hourly Geographic Pattern\n\nThe animation above shows that the overall coverage across the city may vary from hour to hour. During peake hours the city center always has the highest usage.","8fea3e40":"### DOW Trend\n\nWe also observe that people are less likely to user Divvy biking serivce on weekends.","2b91c473":"## Temporal Trends\n### Monthly Trend\n\nOverall the biking sharing service provided by Divvy has been more and more popular over the years. Meanwhile, as one can expect, summmer time (June, July and August) is the peak season for bike riding trips."}}