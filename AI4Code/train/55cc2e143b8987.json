{"cell_type":{"cbe8085d":"code","1c2b9d81":"code","b23648de":"code","c9fb346c":"code","85d6158b":"code","eea94c93":"code","6c988847":"code","1bc121bd":"code","1d964920":"code","d2d150ce":"code","0e66c8eb":"code","63e79440":"code","a5b80a47":"code","c3ca4fdd":"code","5d632bd6":"code","b6f6eb0a":"code","78e90477":"code","947af928":"code","c5351cd2":"code","8972e7a6":"code","2746146e":"code","4583d55c":"code","6631b6fa":"code","43e1765f":"code","6354ea49":"code","807d0198":"code","cb6ddde8":"code","318b0762":"code","230d54c7":"code","4a2be0c0":"code","3983e642":"code","f16a16f9":"code","814df1a3":"code","e91c5835":"code","fa526509":"code","09080c6b":"code","84c928ee":"code","6042b161":"code","00a6024e":"code","cd1fe608":"code","93904778":"code","c27360ed":"code","5d34ab83":"code","b7df4d4d":"code","b272bb69":"code","cd6af634":"code","be6f179e":"code","883e6093":"markdown","23d3216e":"markdown","91f57e34":"markdown","c072f490":"markdown","9c49e2fd":"markdown","6cefd319":"markdown","13f2d381":"markdown","96a98e7f":"markdown","de53c163":"markdown","0329ae88":"markdown","37ca288d":"markdown","7aab4dab":"markdown","7a50e786":"markdown","b2be96f3":"markdown","10017267":"markdown","f0626df3":"markdown"},"source":{"cbe8085d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1c2b9d81":"train = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\ntrain['dataset'] = 'train'\ntest = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')\ntest['dataset'] = 'test'\ndf = pd.concat([train, test])","b23648de":"train.head()\n","c9fb346c":"test.head()","85d6158b":"print(\"train\",train.shape)\nprint(\"test\",test.shape)","eea94c93":"train.isnull().sum()","6c988847":"test.isnull().sum()","1bc121bd":"df.info()","1d964920":"import plotly.express as px\nimport plotly.offline as pf\nfrom plotly.subplots import make_subplots","d2d150ce":"ds = df.groupby(['cp_type', 'dataset'])['sig_id'].count().reset_index()\nds.columns = ['cp_type', 'dataset', 'count']\nfig = px.bar(\n    ds, \n    x='cp_type', \n    y=\"count\", \n    color = 'dataset',\n    barmode='group',\n    orientation='v', \n    title='cp_type train\/test counts', \n    width=600,\n    height=500\n)\nfig.show()","0e66c8eb":"ratio1 = train.groupby('cp_type').count()\nratio2 = test.groupby('cp_type').count()\n#groupby(['cp_type', 'dataset'])['sig_id'].count().reset_index()","63e79440":"ratio1","a5b80a47":"ratio2","c3ca4fdd":"ds = df.groupby(['cp_time', 'dataset'])['sig_id'].count().reset_index()\nds.columns = ['cp_time', 'dataset', 'count']\nfig = px.bar(\n    ds, \n    x='cp_time', \n    y=\"count\", \n    color = 'dataset',\n    barmode='group',\n    orientation='v', \n    title='cp_time train\/test counts', \n    width=600,\n    height=500\n)\nfig.show()","5d632bd6":"ds = df.groupby(['cp_dose', 'dataset'])['sig_id'].count().reset_index()\nds.columns = ['cp_dose', 'dataset', 'count']\nfig = px.bar(\n    ds, \n    x='cp_dose', \n    y=\"count\", \n    color = 'dataset',\n    barmode='group',\n    orientation='v', \n    title='cp_dose train\/test counts', \n    width=600,\n    height=500\n)\nfig.show()","b6f6eb0a":"train_columns = train.columns.to_list()\ng_list = [i for i in train_columns if i.startswith('g-')]\nc_list = [i for i in train_columns if i.startswith('c-')]","78e90477":"g_list","947af928":"import plotly.graph_objs as go\nimport random\n\nplot_list = [g_list[random.randint(0, len(g_list)-1)] for i in range(12)]\n#[g_list[random.randint(0, len(g_list)-1)] for i in range(12)]\n\n\nfig = make_subplots(rows=4, cols=3)\n\n#trace0 = px.histogram(train, x=plot_list[0], color='cp_type',opacity=0.4, marginal='box')\ntrace0 = go.Histogram(x=train[plot_list[0]], nbinsx=20, name=plot_list[0])\ntrace1 = go.Histogram(x=train[plot_list[1]], nbinsx=20, name=plot_list[1])\ntrace2 = go.Histogram(x=train[plot_list[2]], nbinsx=20, name=plot_list[2])\ntrace3 = go.Histogram(x=train[plot_list[3]], nbinsx=20, name=plot_list[3])\ntrace4 = go.Histogram(x=train[plot_list[4]], nbinsx=20, name=plot_list[4])\ntrace5 = go.Histogram(x=train[plot_list[5]], nbinsx=20, name=plot_list[5])\ntrace6 = go.Histogram(x=train[plot_list[6]], nbinsx=20, name=plot_list[6])\ntrace7 = go.Histogram(x=train[plot_list[7]], nbinsx=20, name=plot_list[7])\ntrace8 = go.Histogram(x=train[plot_list[8]], nbinsx=20, name=plot_list[8])\ntrace9 = go.Histogram(x=train[plot_list[9]], nbinsx=20, name=plot_list[9])\ntrace10 = go.Histogram(x=train[plot_list[10]], nbinsx=20, name=plot_list[10])\ntrace11 = go.Histogram(x=train[plot_list[11]], nbinsx=20, name=plot_list[11])\n\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig.append_trace(trace2, 1, 3)\nfig.append_trace(trace3, 2, 1)\nfig.append_trace(trace4, 2, 2)\nfig.append_trace(trace5, 2, 3)\nfig.append_trace(trace6, 3, 1)\nfig.append_trace(trace7, 3, 2)\nfig.append_trace(trace8, 3, 3)\nfig.append_trace(trace9, 4, 1)\nfig.append_trace(trace10, 4, 2)\nfig.append_trace(trace11, 4, 3)\n\nfig.update_layout(\n    title_text='Randomly selected gene expression features distributions'\n)\nfig.show()","c5351cd2":"fig = px.histogram(df, x='g-0', color='cp_type',opacity=0.4, marginal='box')\n                           #nbins=19, range_x=[4,8], width=600, height=350,\n                           #opacity=0.4, marginal='box')\nfig.update_layout(barmode='overlay')\nfig.update_yaxes(range=[0,20],row=1, col=1)","8972e7a6":"import matplotlib.pyplot as plt\n\ncolumns = g_list + c_list\nfor_correlation = [columns[random.randint(0, len(columns)-1)] for i in range(40)]\ndata = df[for_correlation]\n\nf = plt.figure(figsize=(19, 15))\nplt.matshow(data.corr(), fignum=f.number)\nplt.xticks(range(data.shape[1]), data.columns, fontsize=14, rotation=45)\nplt.yticks(range(data.shape[1]), data.columns, fontsize=14)\ncb = plt.colorbar()\ncb.ax.tick_params(labelsize=14)","2746146e":"import time\n\nstart = time.time()\ncols = ['cp_time'] + columns\nall_columns = []\nfor i in range(0, len(cols)):\n    for j in range(i+1, len(cols)):\n        if abs(train[cols[i]].corr(train[cols[j]])) > 0.9:\n            all_columns.append(cols[i])\n            all_columns.append(cols[j])\n\nprint(time.time()-start)","4583d55c":"all_columns = list(set(all_columns))","6631b6fa":"len(all_columns)","43e1765f":"data = df[all_columns]\n\nf = plt.figure(figsize=(19, 15))\nplt.matshow(data.corr(), fignum=f.number)\nplt.xticks(range(data.shape[1]), data.columns, fontsize=14, rotation=45)\nplt.yticks(range(data.shape[1]), data.columns, fontsize=14)\ncb = plt.colorbar()\ncb.ax.tick_params(labelsize=14)","6354ea49":"fig = make_subplots(rows=12, cols=3)\n\ntraces = [go.Histogram(x=train[col], nbinsx=20, name=col) for col in all_columns]\n\ni=1\nj=1\n\nfor trace in traces:\n    fig.append_trace(trace, i, j)\n    if j==3:\n        j=1\n        i+=1\n    else:\n        j+=1\n\nfig.update_layout(\n    title_text='Highly correlated features',\n    height=1200\n)\nfig.show()","807d0198":"train_target = pd.read_csv(\"..\/input\/lish-moa\/train_targets_scored.csv\")\n\nprint('Number of rows : ', train_target.shape[0])\nprint('Number of cols : ', train_target.shape[1])\ntrain_target.head()","cb6ddde8":"x = train_target.drop(['sig_id'], axis=1).sum(axis=0).sort_values().reset_index()\nx.columns = ['column', 'nonzero_records']\n\nfig = px.bar(\n    x.tail(50), \n    x='nonzero_records', \n    y='column', \n    orientation='h', \n    title='Columns with the higher number of positive samples (top 50)', \n    height=1000, \n    width=800\n)\nfig.show()","318b0762":"x = train_target.drop(['sig_id'], axis=1).sum(axis=0).sort_values(ascending=False).reset_index()\nx.columns = ['column', 'nonzero_records']\n\nfig = px.bar(\n    x.tail(50), \n    x='nonzero_records', \n    y='column', \n    orientation='h', \n    title='Columns with the lowest number of positive samples (top 50)', \n    height=1000, \n    width=800\n)\nfig.show()","230d54c7":"x = train_target.drop(['sig_id'], axis=1).sum(axis=0).sort_values(ascending=False).reset_index()\nx.columns = ['column', 'count']\nx['count'] = x['count'] * 100 \/ len(train_target)\nfig = px.bar(\n    x, \n    x='column', \n    y='count', \n    orientation='v', \n    title='Percent of positive records for every column in target', \n    height=800, \n    width=1200\n)\nfig.show()","4a2be0c0":"data = train_target.drop(['sig_id'], axis=1).astype(bool).sum(axis=1).reset_index()\ndata.columns = ['row', 'count']\ndata = data.groupby(['count'])['row'].count().reset_index()\nfig = px.bar(\n    data, \n    y=data['row'], \n    x=\"count\", \n    title='Number of activations in targets for every sample', \n    width=800, \n    height=500\n)\nfig.show()\n","3983e642":"data = train_target.drop(['sig_id'], axis=1).astype(bool).sum(axis=1).reset_index()\ndata.columns = ['row', 'count']\ndata = data.groupby(['count'])['row'].count().reset_index()\nfig = px.pie(\n    data, \n    values=100 * data['row']\/len(train_target), \n    names=\"count\", \n    title='Number of activations in targets for every sample (Percent)', \n    width=800, \n    height=500\n)\nfig.show()","f16a16f9":"train_target.describe()","814df1a3":"start = time.time()\n\ncorrelation_matrix = pd.DataFrame()\nfor t_col in train_target.columns:\n    corr_list = list()\n    if t_col == 'sig_id':\n        continue\n    for col in columns:\n        res = train[col].corr(train_target[t_col])\n        corr_list.append(res)\n    correlation_matrix[t_col] = corr_list\n    \nprint(time.time()-start)","e91c5835":"correlation_matrix['train_features'] = columns\ncorrelation_matrix = correlation_matrix.set_index('train_features')\ncorrelation_matrix","fa526509":"maxCol=lambda x: max(x.min(), x.max(), key=abs)\nhigh_scores = correlation_matrix.apply(maxCol, axis=0).reset_index()\nhigh_scores.columns = ['column', 'best_correlation']\n\nfig = px.bar(\n    high_scores, \n    x='column', \n    y=\"best_correlation\", \n    orientation='v', \n    title='Best correlation with train columns for every target column', \n    width=1200,\n    height=800\n)\nfig.show()","09080c6b":"col_df = pd.DataFrame()\ntr_cols = list()\ntar_cols = list()\nfor col in correlation_matrix.columns:\n    tar_cols.append(col)\n    tr_cols.append(correlation_matrix[col].abs().sort_values(ascending=False).reset_index()['train_features'].head(1).values[0])\n\ncol_df['column'] = tar_cols\ncol_df['train_best_column'] = tr_cols\n\ntotal_scores = pd.merge(high_scores, col_df)\ntotal_scores","84c928ee":"count_features = total_scores['train_best_column'].value_counts().reset_index().sort_values('train_best_column')\ncount_features.columns = ['column', 'count']\nfig = px.bar(\n    count_features.tail(33), \n    x='count', \n    y=\"column\", \n    orientation='h', \n    title='Columns from training set with number of high correlations with target columns', \n    width=800,\n    height=700\n)\nfig.show()","6042b161":"target_columns = train_target.columns.tolist()\ntarget_columns.remove('sig_id')\nfor_analysis = [target_columns[random.randint(0, len(target_columns)-1)] for i in range(5)]\ncurrent_corr = correlation_matrix[for_analysis]","00a6024e":"target_columns","cd1fe608":"col_df = pd.DataFrame()\ntr_first_cols = list()\ntr_second_cols = list()\ntar_cols = list()\nfor col in current_corr.columns:\n    tar_cols.append(col)\n    tr_first_cols.append(current_corr[col].abs().sort_values(ascending=False).reset_index()['train_features'].head(2).values[0])\n    tr_second_cols.append(current_corr[col].abs().sort_values(ascending=False).reset_index()['train_features'].head(2).values[1])\n\ncol_df['column'] = tar_cols\ncol_df['train_1_column'] = tr_first_cols\ncol_df['train_2_column'] = tr_second_cols\ncol_df","93904778":"def plot_scatter(col_df, index):\n    analysis = pd.DataFrame()\n    analysis['color'] = train_target[col_df.iloc[index]['column']]\n    analysis['x'] = train[col_df.iloc[index]['train_1_column']]\n    analysis['y'] = train[col_df.iloc[index]['train_2_column']]\n    analysis.columns = ['color', col_df.iloc[index]['train_1_column'], col_df.iloc[index]['train_2_column']]\n    analysis['size'] = 1\n    analysis.loc[analysis['color'] == 1, 'size'] = 10\n\n    fig = px.scatter(\n        analysis, \n        x=col_df.iloc[index]['train_1_column'], \n        y=col_df.iloc[index]['train_2_column'], \n        color=\"color\", \n        size='size', \n        height=800,\n        title='Scatter plot for ' + col_df.iloc[index]['column']\n    )\n    fig.show()","c27360ed":"plot_scatter(col_df, 0)","5d34ab83":"plot_scatter(col_df, 1)","b7df4d4d":"plot_scatter(col_df, 2)","b272bb69":"plot_scatter(col_df, 3)","cd6af634":"for_analysis = [target_columns[random.randint(0, len(target_columns)-1)] for i in range(5)]\ncurrent_corr = correlation_matrix[for_analysis]\n\ncol_df = pd.DataFrame()\ntr_first_cols = list()\ntr_second_cols = list()\ntr_third_cols = list()\ntar_cols = list()\nfor col in current_corr.columns:\n    tar_cols.append(col)\n    tr_first_cols.append(current_corr[col].abs().sort_values(ascending=False).reset_index()['train_features'].head(3).values[0])\n    tr_second_cols.append(current_corr[col].abs().sort_values(ascending=False).reset_index()['train_features'].head(3).values[1])\n    tr_third_cols.append(current_corr[col].abs().sort_values(ascending=False).reset_index()['train_features'].head(3).values[2])\n\n\ncol_df['column'] = tar_cols\ncol_df['train_1_column'] = tr_first_cols\ncol_df['train_2_column'] = tr_second_cols\ncol_df['train_3_column'] = tr_third_cols\ncol_df","be6f179e":"#\u30bf\u30fc\u30b2\u30c3\u30c8\u540d\u304b\u3089\u6700\u5f8c\u306e\u7279\u5fb4\u6587\u5b57\u3092\u62bd\u51fa\u3057\u3001\u30ab\u30a6\u30f3\u30c8\u3059\u308b\u3002\nlast_term = dict()\nfor item in target_columns:\n    try:\n        last_term[item.split('_')[-1]] += 1\n    except:\n        last_term[item.split('_')[-1]] = 1\n\n#1\u3088\u308a\u3082\u591a\u304f\u3042\u3063\u305f\u3068\u3053\u308d\u3092\nlast_term = pd.DataFrame(last_term.items(), columns=['group', 'count'])\nlast_term = last_term.sort_values('count')\nlast_term = last_term[last_term['count']>1]\nlast_term['count'] = last_term['count'] * 100 \/ 206\n\n\nfig = px.bar(\n    last_term, \n    x='count', \n    y=\"group\", \n    orientation='h', \n    title='Groups in target columns (Percent from all target columns)', \n    width=800,\n    height=500\n)\nfig.show()","883e6093":"cp_type\u306e\u7279\u5fb4\u306e\u56f3\u793a","23d3216e":"\u3067\u306f\u3001\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30bb\u30c3\u30c8\u304b\u3089\u3001\u3069\u306e\u30ab\u30e9\u30e0\u304c\u30bf\u30fc\u30b2\u30c3\u30c8\u30ab\u30e9\u30e0\u3068\u306e\u76f8\u95a2\u5024\u304c\u9ad8\u3044\u304b\u3092\u898b\u3066\u307f\u307e\u3057\u3087\u3046\u3002\u30c1\u30e3\u30fc\u30c8\u306e\u5404\u884c\u306f\u3001\u30ab\u30e9\u30e0 A N \u304c\u7570\u306a\u308b\u30bf\u30fc\u30b2\u30c3\u30c8\u30ab\u30e9\u30e0\u3068\u306e\u76f8\u95a2\u304c\u6700\u3082\u9ad8\u3044\u5024\u3092\u6301\u3063\u3066\u3044\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u3066\u3044\u307e\u3059\u3002","91f57e34":"https:\/\/www.kaggle.com\/isaienkov\/mechanisms-of-action-moa-prediction-eda\n\nKATSUAI","c072f490":"\u5168\u4f53\u7684\u306b\u3001\uff10\u3092\u4e2d\u5fc3\u3068\u3057\u305f\u6b63\u898f\u5206\u5e03\u306b\u306a\u3063\u3066\u3044\u308b\u3002","9c49e2fd":"\u30bf\u30fc\u30b2\u30c3\u30c8\u306e\u30ab\u30e9\u30e0\u540d\u304b\u3089\u30b0\u30eb\u30fc\u30d7\u3092\u62bd\u51fa\u3059\u308b\u3002\n\n","6cefd319":"\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306f877\u6b21\u5143\u3067\u69cb\u6210\u3055\u308c\u3066\u304a\u308a\n\u3059\u3079\u3066\u306e\u30c7\u30fc\u30bf\u304cfloat\u65b9\u5f0f\u3067\u3042\u3063\u305f\u3002","13f2d381":"4\u5272\u306e\u30ab\u30e9\u30e0\u304c1\u3092\u542b\u307e\u306a\u3044\u4e8b\u300152%\u304c1\u3064\u3057\u304b\u542b\u3093\u3067\u3044\u306a\u3044\u3002","96a98e7f":"# MoA\u30b3\u30f3\u30daEDA\u30d1\u30fc\u30c8","de53c163":"# train\u30c7\u30fc\u30bf\u306e\u76f8\u95a2\u306e\u78ba\u8a8d\n\n","0329ae88":"\u3067\u306f\u3001\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30bb\u30c3\u30c8\u304b\u3089\u3001\u3069\u306e\u30ab\u30e9\u30e0\u304c\u30bf\u30fc\u30b2\u30c3\u30c8\u30ab\u30e9\u30e0\u3068\u306e\u76f8\u95a2\u5024\u304c\u9ad8\u3044\u304b\u3092\u898b\u3066\u307f\u307e\u3057\u3087\u3046\u3002\n\n\u30c1\u30e3\u30fc\u30c8\u306e\u5404\u884c\u306f\u3001\u30ab\u30e9\u30e0 A N \u304c\u7570\u306a\u308b\u30bf\u30fc\u30b2\u30c3\u30c8\u30ab\u30e9\u30e0\u3068\u306e\u76f8\u95a2\u304c\u6700\u3082\u9ad8\u3044\u5024\u3092\u6301\u3063\u3066\u3044\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u3066\u3044\u307e\u3059\u3002","37ca288d":"cp_time \u30fb\u30fb\u30fb\u3000\u51e6\u7f6e\u6642\u9593\u307f\u305f\u3044\u306a\u3082\u306e\uff08\uff1f\uff09\n\n\uff12\uff14\uff0c\uff14\uff18\uff0c\uff17\uff12\u6642\u9593\u306e3\u901a\u308a\u3042\u308b\u3002\ntrain\u3068test\u306e\u5272\u5408\u306f\u540c\u3058\u3002","7aab4dab":"\u30c7\u30fc\u30bf\u657020000\u4ee5\u4e0a\u306b\u5bfe\u3057\u3066\u30011\u306e\u657020\u4ee5\u4e0b\u304c50\u4ee5\u4e0a\u3002","7a50e786":"cp_type\u306f2\u56e0\u5b50\u3057\u304b\u306a\u3044\u3002\n\ntrain,test\u306e\u5272\u5408\u306f\u3001\u3060\u3044\u305f\u30448:2","b2be96f3":"# \u76ee\u7684\u5909\u6570\u3068\u76f8\u95a2\u306e\u5f37\u3044\u7279\u5fb4\u91cf\u306b\u3064\u3044\u3066\n\n","10017267":"# \u30c7\u30fc\u30bf\u306e\u53ef\u8996\u5316","f0626df3":"# \u30bf\u30fc\u30b2\u30c3\u30c8\u306e\u78ba\u8a8d"}}