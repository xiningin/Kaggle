{"cell_type":{"2a01714c":"code","892dac56":"code","496f75c2":"code","d22acdf0":"code","9a9457c3":"code","dff9c2e3":"code","cc8dc4ae":"code","6ef20b41":"code","e8790d9c":"code","fea3d24e":"code","94ec1cfd":"code","9fe664e2":"code","fff49ff2":"code","1ba2d852":"code","2716955d":"code","ee5e31c3":"code","495e3224":"code","f3bb179c":"code","caaeb85f":"code","342c70b8":"code","c504c05a":"code","76d057d7":"code","11bdc08f":"code","4266a624":"code","6d09433e":"code","5eda8efe":"code","f60d15ff":"code","7efd31d2":"code","8832a147":"code","ee5a00c9":"code","5d0be7f1":"code","f2bdb0bc":"code","096821ea":"markdown","c5119a1a":"markdown","22061e0b":"markdown","099585ed":"markdown","f185032d":"markdown","95d82240":"markdown","ed79a497":"markdown","47e1b0f9":"markdown"},"source":{"2a01714c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Any results you write to the current directory are saved as output.","892dac56":"test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\nsample_submission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')","496f75c2":"pd.set_option('display.max_columns', 100)\ntrain.head()","d22acdf0":"f = open('..\/input\/house-prices-advanced-regression-techniques\/data_description.txt','r')\nmessage = f.read()\nprint(message)","9a9457c3":"# On train data\nNA_col = pd.DataFrame(train.isna().sum(), columns = ['NA_Count'])\nNA_col['% of NA'] = (NA_col.NA_Count\/len(train))*100\nNA_col.sort_values(by = ['% of NA'], ascending = False, na_position = 'first')\n\n# Dropping 'PoolQC', 'MiscFeature', 'Alley', 'Fence'","dff9c2e3":"pd.set_option('display.max_rows', 500)\nNA_row = pd.DataFrame(train.isnull().sum(axis = 1), columns = ['NA_Row_Count'])\nNA_row['% of NA'] = (NA_row.NA_Row_Count\/len(train.columns))*100\nNA_row.sort_values(by = ['% of NA'], ascending = False, na_position = 'first')\n\n# We are good row-wise","cc8dc4ae":"# On test data\nNA_col = pd.DataFrame(test.isna().sum(), columns = ['NA_Count'])\nNA_col['% of NA'] = (NA_col.NA_Count\/len(test))*100\nNA_col.sort_values(by = ['% of NA'], ascending = False, na_position = 'first')\n\n# Dropping 'PoolQC', 'MiscFeature', 'Alley', 'Fence'","6ef20b41":"pd.set_option('display.max_rows', 500)\nNA_row = pd.DataFrame(test.isnull().sum(axis = 1), columns = ['NA_Row_Count'])\nNA_row['% of NA'] = (NA_row.NA_Row_Count\/len(test.columns))*100\nNA_row.sort_values(by = ['% of NA'], ascending = False, na_position = 'first')\n\n# We are good row-wise","e8790d9c":"train_data = train.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence'], axis=1) \ntest_data = test.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence'], axis=1) ","fea3d24e":"train_wo_target = train_data.drop(['Id', 'SalePrice'], axis=1)","94ec1cfd":"cols = train_wo_target.columns\nnum_cols = train_wo_target._get_numeric_data().columns\ncat_cols = list(set(cols) - set(num_cols))","9fe664e2":"num_cols","fff49ff2":"cat_cols","1ba2d852":"X_train = train_data.copy().drop('SalePrice', axis = 1)\ny_train = train_data[['Id','SalePrice']]","2716955d":"from sklearn.preprocessing import Imputer\nfrom sklearn.impute import SimpleImputer","ee5e31c3":"# Imputing Numerical Columns\n\nmean_imputer = Imputer(missing_values=\"NaN\", strategy=\"mean\", axis=0)\nmean_imputer.fit(X_train[num_cols])\nX_train[num_cols] = pd.DataFrame(mean_imputer.fit_transform(X_train[num_cols]))\ntest_data[num_cols] = pd.DataFrame(mean_imputer.fit_transform(test_data[num_cols]))","495e3224":"# Imputing Categorical Columns on train data\n\ndef impute_with_mode(x):\n    max_x = x.value_counts()\n    mode = max_x[max_x == max_x.max()].index[0]\n    x[x.isna()] = mode\n    return x\n\nX_train[cat_cols] = X_train[cat_cols].apply(lambda x: impute_with_mode(x))","f3bb179c":"# Imputing Categorical Columns on test data\ntest_data[cat_cols] = test_data[cat_cols].apply(lambda x: impute_with_mode(x))","caaeb85f":"train_objs_num = len(X_train)\ndataset = pd.concat(objs=[X_train, test_data], axis=0)\ndataset_preprocessed = pd.get_dummies(dataset)\nX_train = dataset_preprocessed[:train_objs_num]\ntest_data = dataset_preprocessed[train_objs_num:]","342c70b8":"# Checking NAs\nX_train.isna().sum()","c504c05a":"test_data.isna().sum()","76d057d7":"# Setting Index\n\nX_train.set_index('Id', inplace = True)\ny_train.set_index('Id', inplace = True)\ntest_data.set_index('Id', inplace = True)","11bdc08f":"# Checking shape\nprint(X_train.shape)\nprint(test_data.shape)","4266a624":"from sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import r2_score, mean_squared_log_error, mean_squared_error\nfrom sklearn.ensemble import GradientBoostingRegressor","6d09433e":"# Grid Search \n\nfrom sklearn.model_selection import GridSearchCV\n\n# Model in use\nGBM = GradientBoostingRegressor()\nparam_grid = { \n           \"n_estimators\" : [80, 100, 150, 200, 230, 250, 270, 300],\n           \"max_depth\" : [1,2,3,4,5,8,10],\n           \"learning_rate\" : [0.01, 0.05, 0.08, 0.1]}\n \nCV_GBM = GridSearchCV(estimator=GBM, param_grid=param_grid, cv= 10)","5eda8efe":"%time CV_GBM.fit(X=X_train, y=y_train.values.ravel())","f60d15ff":"best_gbm_model = CV_GBM.best_estimator_\nprint (CV_GBM.best_score_, CV_GBM.best_params_)","7efd31d2":"# Predictions\n\npred_train_gbm = best_gbm_model.predict(X_train)\npred_test_gbm = best_gbm_model.predict(test_data)","8832a147":"print('Mean Squared Log Error: ', metrics.mean_squared_log_error(y_train, pred_train_gbm).round(5))\nprint('Mean Squared Error: ', metrics.mean_squared_error(y_train, pred_train_gbm).round(5))\nprint('R-squared: ', metrics.r2_score(y_train, pred_train_gbm).round(5))","ee5a00c9":"y_pred_test_gbm = pd.DataFrame(pred_test_gbm, columns = ['SalePrice'])\ny_pred_test_gbm['Id'] = test['Id']","5d0be7f1":"columnsTitles = ['Id', 'SalePrice']\n\nsubmission = y_pred_test_gbm.reindex(columns=columnsTitles)\nsubmission .head()","f2bdb0bc":"filename = 'House_Pricing_GB.csv'\n\nsubmission.to_csv(filename,index=False)\n\nprint('Saved file: ' + filename)","096821ea":"## Reading Files","c5119a1a":"## Imputation","22061e0b":"# Simple Analysis using Gradient Boosting","099585ed":"## Combining train and test for dummification and then splitting accordingly","f185032d":"## Dropping the 'Id' column and the target variable to segregate Numerical and Categorical columns","95d82240":"## Checking NAs","ed79a497":"## Model Building - Gradient Boosting","47e1b0f9":"### Dropping columns having >= 80% NAs"}}