{"cell_type":{"6c00081c":"code","8edc8ce4":"code","43a53f72":"code","83511ffa":"code","2a2151cf":"code","eef67d84":"code","bb89d08b":"code","9d9c2039":"code","8ca2d7b3":"code","1b22c154":"code","5c20d9b5":"code","3ad928ad":"code","93cb05be":"code","3ed4fc4d":"code","5bdf8623":"code","284e4c73":"code","69e88d07":"code","5d37b3e0":"code","df78b951":"code","f36e5d17":"code","3aa6c5ca":"code","80036b8e":"code","0b0ad30c":"code","158eefe5":"code","7f87b9b2":"code","d6c9c12a":"code","f2a8c767":"code","42a150b8":"code","e312afd9":"code","ac41c46a":"code","58e75c7a":"code","76838154":"code","b63e2973":"code","3d38793e":"code","f505a21f":"code","8398a27f":"code","9e996571":"markdown","73dc2b63":"markdown","f11b2e4c":"markdown","0ddbc1a8":"markdown","39f5debb":"markdown","31ab35b1":"markdown","1f808022":"markdown","79f0265f":"markdown","c19365ca":"markdown","76d212a1":"markdown","7cf4226e":"markdown","b6b8d9f9":"markdown","312e7915":"markdown","f3817827":"markdown","74a624db":"markdown","0675ad65":"markdown","6a4f0b58":"markdown","a1928311":"markdown","9d183234":"markdown","bf1d2424":"markdown","6c2fd757":"markdown","fa7fd621":"markdown","628f5927":"markdown","66f915f4":"markdown","045acd79":"markdown","9c314924":"markdown","700e6ea2":"markdown","4f67c938":"markdown","e893443e":"markdown","fdace271":"markdown","485359be":"markdown","cd4a7ac2":"markdown","27650b45":"markdown","bad4eb9b":"markdown","ba72fd7e":"markdown","5406a8cb":"markdown","ed2983b9":"markdown","e7ba7012":"markdown","bec59160":"markdown","5ba13801":"markdown","643d21cd":"markdown","4f8804b7":"markdown","700748e6":"markdown","4d196113":"markdown"},"source":{"6c00081c":"import random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom scipy import stats\nimport matplotlib.pyplot as plt","8edc8ce4":"Random_seed = random.randint(1, 1000)\nprint(Random_seed)","43a53f72":"data = pd.read_csv('\/kaggle\/input\/demographic-data\/Demographic_Data.csv')","83511ffa":"data.head()","2a2151cf":"data.describe()","eef67d84":"data.info()","bb89d08b":"data.isnull().sum()","9d9c2039":"data = data.drop_duplicates()\ndata.isnull().sum()","8ca2d7b3":"data.dtypes","1b22c154":"print(data.columns)","5c20d9b5":"data.groupby('in-store')['in-store'].count()","3ad928ad":"pd.DataFrame(data.groupby(data['region'])['in-store'].mean())","93cb05be":"import warnings\nwarnings.simplefilter(\"ignore\")\nsns.factorplot('in-store', data=data, kind='count', aspect=1)\nplt.title(\"Number of in store purchases\")","3ed4fc4d":"subset = data.drop_duplicates(subset='amount')\nsns.factorplot('in-store', data = subset, kind='count', aspect=1)\nplt.title(\"Number of in store purchases\")","5bdf8623":"data[['amount','items']].describe()","284e4c73":"Sorted = data.sort_values('items', inplace=False)\nSorted.drop_duplicates(subset='amount', inplace=True)\nSorted.tail()","69e88d07":"Sorted = data.groupby(data['items']).count()\nit = Sorted.iloc[:, 0:3]\nplt.hist(Sorted)\nplt.legend(Sorted.columns)\nplt.title('Counting items')\nprint(Sorted)\nprint(it)","5d37b3e0":"Sorted = data.sort_values('items', inplace=False)\nSorted.drop_duplicates(subset='amount', inplace=True)\n\nage = Sorted.groupby(Sorted['age']).count()\nplt.hist(age)\nplt.legend(age.columns)\nplt.title('Counting items')\nplt.xlabel('Values')\nplt.ylabel('age')","df78b951":"plt.figure(figsize=(10, 10))\nfor i, j in enumerate(data.columns):\n    plt.subplot(3, 2, i+1)\n    plt.hist(Sorted[j], color='teal', histtype='bar')\n    plt.title(str(data.columns[i]))\n    plt.ylabel(\"Values\") ","f36e5d17":"sample = Sorted.sample(frac=0.005, random_state=Random_seed)\nprint(sample.shape)\nsample.tail()","3aa6c5ca":"X = sample['age']\ny = sample['amount']\nprint(\"Age sample:\", X, '\\n',\n      \"Amount sample:\", y)","80036b8e":"plt.scatter(X, y, marker='o', c='lawngreen')\nplt.title(\"Scatter plot\")\nplt.xlabel(\"age\")\nplt.ylabel(\"amount\")\nplt.show()","0b0ad30c":"X = sample['region']\ny = sample['amount']\n\ndef sct(X, y, Xlabel=None, ylabel=None):\n    plt.scatter(X, y, marker='o', c='lawngreen')\n    plt.title(\"Scatter plot\")\n    plt.xlabel(Xlabel)\n    plt.ylabel(ylabel)\n    return plt.show()\nsct(X, y, 'age', 'amount')","158eefe5":"X = sample['items']\ny = sample['amount']\n\ndef sct(X, y, Xlabel=None, ylabel=None):\n    plt.scatter(X, y, marker='o', c='lawngreen')\n    plt.title(\"Scatter plot\")\n    plt.xlabel(Xlabel)\n    plt.ylabel(ylabel)\n    return plt.show()\nsct(X, y, 'items', 'amount')","7f87b9b2":"plt.figure(figsize=(10, 15))\nfor i, j in enumerate(data.columns):\n    plt.subplot(3, 2, i+1)\n    plt.boxplot(Sorted[j], 0, 'gD', showmeans=True,\n                meanline=True, autorange=True)\n    plt.title('Box plt - ' + str(data.columns[i]))","d6c9c12a":"outliers = np.where(Sorted['amount']>2000)\nprint('outliers indes:', outliers)\n(Sorted['amount']>2000).shape","f2a8c767":"sct(Sorted['region'], Sorted['amount'], 'region', 'amount')","42a150b8":"amount = (sample.amount).values\nm = []\nfor i in range(amount.shape[0]):\n    m.append(np.mean(amount))\nplt.plot(amount, label='Amount of purchase')\nplt.plot(m, linewidth=3, color='r', label='Median')\nplt.legend()\nplt.title(\"Checking amount outliers\")","e312afd9":"z = np.abs(stats.zscore(amount))\nplt.plot(z, c='g', alpha=0.3)\nplt.ylabel('Distance')\nplt.xlabel('index')\nplt.title('Distance of the amount value from the mean')","ac41c46a":"\nQ1 = np.percentile(amount, 25, interpolation='midpoint')\nQ3 = np.percentile(amount, 65, interpolation='midpoint')\nIQR = Q3 - Q1\namount.shape\nupper = np.where(amount>=(Q3+1.5*IQR))\nlower = np.where(amount<=(Q1-1.5*IQR))\nnew = pd.DataFrame(amount)\nnew.drop(upper[0], inplace=True)\nnew.drop(lower[0], inplace=True)\nprint(new.shape)\nnew.head()","58e75c7a":"plt.plot(amount, label='Amount of purchase')\nplt.plot(new, label='Clean amount', c='g', alpha=0.7)\nplt.plot(m, linewidth=3, color='r', label='Median')\nplt.legend()\nplt.title(\"Comparing the amount with and without outliers\")","76838154":"plt.figure(figsize=(10,10))\nplt.subplot(2,2,1)\nplt.boxplot(new, 0, 'gD', showmeans=True,\n                meanline=True, autorange=True)\nplt.title('Cleaned amount without outliers')\nplt.subplot(2,2,2)\nplt.boxplot(amount, 0, 'gD', showmeans=True,\n                meanline=True, autorange=True)\nplt.title('Real values of amount')","b63e2973":"Sorted.corr(\"pearson\")","3d38793e":"sns.heatmap(data.corr(\"pearson\"), annot=True, cmap='GnBu')","f505a21f":"Sorted.cov()","8398a27f":"sns.heatmap(Sorted.cov(), annot=True)","9e996571":"As we can see in the amount, we have outliers values.\n\nAfter seeing the box plot, I curious about the \"amount\" feature, so I decided to have a subsample of it and compare it with its median. ","73dc2b63":"Here we can find the average of in-store shopping regarding the sale zone","f11b2e4c":"Defining the bounds to remove the outliers","0ddbc1a8":"## Seyedsaman Emami\n### 02.Jul.2021\n\n# Table of contents\n* About the Notebook\n* Hypothesis\n* Importing Libraries\n* Overview of dataset\n    * Exploring the Data\n    * Data sampling\n    * Visualizing\n    * Outlier treatment\n    * Correlation\n* Conclusion","39f5debb":"## An overview of dataset","31ab35b1":"### The number of purchases","1f808022":"### Check missing values","79f0265f":"### Comparing \n> Let's compare the simple samples of two different features of our dataset","c19365ca":"# Scatter","76d212a1":"As we only have four attributes, it is easier to check the correlation over a heatmap","7cf4226e":"### Check our data type","b6b8d9f9":"## Dropping duplicates values by subsetting the amount\nFrom the previous cell, I understand that the plots do not make a sense so, I thought about duplicated values in a specific column.\n\nFirst of all, I sorted my data frame to have the ideal form of table which I was looking for.\n\nI stored the modified dataset in a *Sorted* DataFrame","312e7915":"### Check the columns' names","f3817827":"### Check the Correlation","74a624db":"Instead of printing plots one-by-one, we can print them out in one loop in the range of features of the dataset.","0675ad65":"* There would be a relationship between different features of the dataset.\n* There is a high correlation between one pair of columns.\n* There are duplicated values.\n* Do customers in different regions spend more per transaction? Which regions spend the most\/least?\n* Is there a relationship between number of items purchased and amount spent?","6a4f0b58":"I imported the libraries which were useful to my proposal of EDA the dataset. After entering the dataset, and have an overview of the data frame, I looked for missing values and I did not find any.\n\nTo have a summary of our features, I grouped the dataset into different parts and studied them separately. My studies included sorting, histogram, scatter plots, box plots, data frame, correlations, covariance size of the data, and shape of the arrays.\n\nFrom the previous studies, I found that we have duplicate values in one of our features which were \"amount\" so I removed them and continue with the new dataset.\n\nAlso, from different plots such as the box plot, I noted the outliers values, So I defined the various percentile of the relevant axis to drop them. After removing outliers, I compared the shape, size, and behavior of the removed outliers and the principal dataset.\n\nRegarding the questions and hypothesizes, I should say that;\n-\tI found a relationship and correlation between different features of the dataset.\n-\tThere were duplicate values in the dataset.\n- Customers in regions one and four, spent more money to buy their items. And region two has the lowers amount of purchase.\n-\tYes, there is a relationship between number of items purchased and amount spent.","a1928311":"# Importing Libraries","9d183234":"## Explore the Data","bf1d2424":"I chose the pearson method to return the Correlation Coefficient matrix","6c2fd757":" ## Joint variability","fa7fd621":"### Data sampling","628f5927":"Check the description of only two features","66f915f4":"## Visualizing the data ","045acd79":"#### Visualize the correlation","9c314924":"### feature dentification ","700e6ea2":"Many libraries in python during the implementation, print out the logs. To have a clear output, I ignore the printing of the outputs.","4f67c938":"# Conclusion","e893443e":"### Check Data description ","fdace271":"## Data cleaning","485359be":"We have the same scatter as the previous with this difference that I defined a python method here to return the scatter plot for the user.","cd4a7ac2":"I tried to group by the dataset to check different features in contrast to the rest.","27650b45":"# Outlier treatment","bad4eb9b":"### Plotting\n>  a histogram on the features","ba72fd7e":"# Hypothesis","5406a8cb":"# About this notebook\nIn the following notebook, I reviewed the demographic dataset and investigated different aspects of this simple dataset. \nIn the first part, I imported the libraries which I needed for my experiments.\nTo import the dataset, I considered the Panda's library and read the *CSV file by calling the Pandas's method. And to have an overview of our data frame, I called the five top rows of the dataset by the Head method. For the description, I studied the five features of the dataset (In terms of min, max, std, mean, and quartile). Also, one can find the size, type, and dimensions of the dataset in the related cell. \n\nTo have a clean dataset, I had to check the null or missing values, and if I find any, there are different approaches to deal with them. Hence, I checked the missing values and used another function to remove the null value in the case that we have any.\n\nSo it is time to dive into the details by exploring more. For this matter, I visualized the data by plotting the histogram of each feature, scatter plot to see the relationship between pair columns, and box plot to have a summary of quantile, max, min, median, and mean of each attribute.\n\nRegarding the outlier, I reviewed the box plot and scatter plot, and defined the lower and upper bound to drop the outliers.\n\nFinally, I checked the correlation by applying the Pearson method and print out the covariance.","ed2983b9":"# Obtaining Data","e7ba7012":"For the sake of sampling and other experiments, I added a random seed to generate a different seed for functions.","bec59160":"Same as previous without duplicated purchased amount","5ba13801":"### Measure of the joint variability by using the Covariance","643d21cd":"I generated a random sample from the main dataset to explore the dataset in a small dimension.","4f8804b7":"### Check the data types","700748e6":"> ","4d196113":"### Drop missing value\n> We do not have any"}}