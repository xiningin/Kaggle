{"cell_type":{"55e4f598":"code","e43c42cd":"code","a10f29c8":"code","0bedecf6":"code","92d7cb47":"code","e485be4b":"code","92e68fce":"code","6a8cc51f":"code","a89684cb":"code","294bba72":"code","2d76e485":"code","7e7a93c9":"code","bcdb9e56":"code","a663b568":"code","6c0a4175":"code","d6b5e82d":"code","4b2d97ba":"code","ec6d769c":"code","f7193f9b":"code","773d07f9":"markdown","0f77d292":"markdown","31ba2f8d":"markdown","d600fd40":"markdown","0cd961e0":"markdown","f37ee005":"markdown","65c527ec":"markdown","b3547e64":"markdown"},"source":{"55e4f598":"!pip install tez\n!pip install --upgrade efficientnet-pytorch","e43c42cd":"import glob\nimport os\nimport random\n\nimport albumentations\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport tez\nimport torch\nimport torch.nn as nn\n\nfrom efficientnet_pytorch import EfficientNet\nfrom sklearn import metrics, model_selection, preprocessing\nfrom tez.callbacks import EarlyStopping\nfrom tez.datasets import ImageDataset\nfrom torch.nn import functional as F\nfrom tqdm import tqdm","a10f29c8":"%mkdir \/kaggle\/working\/train_resized\n%mkdir \/kaggle\/working\/test_resized","0bedecf6":"IMG_SIZE = 256","92d7cb47":"# Training data\nfor file in tqdm(glob.glob('..\/input\/dlub-summer-school-challenge-2021\/train\/train\/*.jpg')):\n    filename = file.split('\/')[-1]\n    image = cv2.imread(file)\n    resized = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n    cv2.imwrite(f'\/kaggle\/working\/train_resized\/{filename}', resized)\n\n# Test data\nfor file in tqdm(glob.glob('..\/input\/dlub-summer-school-challenge-2021\/test\/test\/*.jpg')):\n    filename = file.split('\/')[-1]\n    image = cv2.imread(file)\n    resized = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n    cv2.imwrite(f'\/kaggle\/working\/test_resized\/{filename}', resized)","e485be4b":"class FoodNet(tez.Model):\n    def __init__(self, num_classes):\n        super().__init__()\n\n        self.effnet = EfficientNet.from_pretrained(\"efficientnet-b7\")\n        self.dropout = nn.Dropout(0.1)\n        self.out = nn.Linear(2560, num_classes)\n        self.step_scheduler_after = \"epoch\"\n        \n    def monitor_metrics(self, outputs, targets):\n        if targets is None:\n            return {}\n        outputs = torch.argmax(outputs, dim=1).cpu().detach().numpy()\n        targets = targets.cpu().detach().numpy()\n        accuracy = metrics.accuracy_score(targets, outputs)\n        return {\"accuracy\": accuracy}\n    \n    def fetch_optimizer(self):\n        opt = torch.optim.Adam(self.parameters(), lr=3e-4)\n        return opt\n    \n    def fetch_scheduler(self):\n        sch = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n            self.optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1\n        )\n        return sch\n\n    def forward(self, image, targets=None):\n        batch_size, _, _, _ = image.shape\n\n        x = self.effnet.extract_features(image)\n        x = F.adaptive_avg_pool2d(x, 1).reshape(batch_size, -1)\n        outputs = self.out(self.dropout(x))\n        \n        if targets is not None:\n            loss = nn.CrossEntropyLoss()(outputs, targets)\n            metrics = self.monitor_metrics(outputs, targets)\n            return outputs, loss, metrics\n        return outputs, None, None","92e68fce":"train_aug = albumentations.Compose([\n            albumentations.RandomResizedCrop(256, 256),\n            albumentations.Transpose(p=0.5),\n            albumentations.HorizontalFlip(p=0.5),\n            albumentations.VerticalFlip(p=0.5),\n            albumentations.ShiftScaleRotate(p=0.5),\n            albumentations.HueSaturationValue(\n                hue_shift_limit=0.2, \n                sat_shift_limit=0.2, \n                val_shift_limit=0.2, \n                p=0.5\n            ),\n            albumentations.RandomBrightnessContrast(\n                brightness_limit=(-0.1,0.1), \n                contrast_limit=(-0.1, 0.1), \n                p=0.5\n            ),\n            albumentations.Normalize(\n                mean=[0.485, 0.456, 0.406], \n                std=[0.229, 0.224, 0.225], \n                max_pixel_value=255.0, \n                p=1.0\n            ),\n            albumentations.CoarseDropout(p=0.5),\n            albumentations.Cutout(p=0.5)], p=1.)\n  \n        \nvalid_aug = albumentations.Compose([\n            albumentations.CenterCrop(256, 256, p=1.),\n            albumentations.Resize(256, 256),\n            albumentations.Normalize(\n                mean=[0.485, 0.456, 0.406], \n                std=[0.229, 0.224, 0.225], \n                max_pixel_value=255.0, \n                p=1.0\n            )], p=1.)","6a8cc51f":"df = pd.read_csv('..\/input\/dlub-summer-school-challenge-2021\/train.csv')\ndf_train, df_valid = model_selection.train_test_split(\n        df, test_size=0.1, random_state=42, stratify=df.category_id.values\n)\n\ndf_train = df_train.reset_index(drop=True)\ndf_valid = df_valid.reset_index(drop=True)\n\nimage_path = \"\/kaggle\/working\/train_resized\/\"\ntrain_image_paths = [os.path.join(image_path, x) for x in df_train.filename.values]\nvalid_image_paths = [os.path.join(image_path, x) for x in df_valid.filename.values]\ntrain_targets = df_train.category_id.values\nvalid_targets = df_valid.category_id.values\n\ntrain_dataset = ImageDataset(\n    image_paths=train_image_paths,\n    targets=train_targets,\n    augmentations=train_aug,\n)\n\nvalid_dataset = ImageDataset(\n    image_paths=valid_image_paths,\n    targets=valid_targets,\n    augmentations=valid_aug,\n)","a89684cb":"model = FoodNet(num_classes=df.category_id.nunique())\nes = EarlyStopping(\n    monitor=\"valid_loss\", model_path=\"model.bin\", patience=3, mode=\"min\"\n)\nmodel.fit(\n    train_dataset,\n    valid_dataset=valid_dataset,\n    train_bs=32,\n    valid_bs=64,\n    device=\"cuda\",\n    epochs=10,\n    callbacks=[es],\n    fp16=True,\n)\nmodel.save(\"model.bin\")","294bba72":"class FoodNet(tez.Model):\n    def __init__(self, num_classes):\n        super().__init__()\n\n        self.effnet = EfficientNet.from_name(\"efficientnet-b7\")\n        self.dropout = nn.Dropout(0.1)\n        self.out = nn.Linear(2560, num_classes)\n        self.step_scheduler_after = \"epoch\"\n\n    def forward(self, image, targets=None):\n        batch_size, _, _, _ = image.shape\n\n        x = self.effnet.extract_features(image)\n        x = F.adaptive_avg_pool2d(x, 1).reshape(batch_size, -1)\n        outputs = self.out(self.dropout(x))\n        return outputs, None, None","2d76e485":"test_aug = albumentations.Compose([\n    albumentations.RandomResizedCrop(256, 256),\n    albumentations.Transpose(p=0.5),\n    albumentations.HorizontalFlip(p=0.5),\n    albumentations.VerticalFlip(p=0.5),\n    albumentations.HueSaturationValue(\n        hue_shift_limit=0.2, \n        sat_shift_limit=0.2,\n        val_shift_limit=0.2, \n        p=0.5\n    ),\n    albumentations.RandomBrightnessContrast(\n        brightness_limit=(-0.1,0.1), \n        contrast_limit=(-0.1, 0.1), \n        p=0.5\n    ),\n    albumentations.Normalize(\n        mean=[0.485, 0.456, 0.406], \n        std=[0.229, 0.224, 0.225], \n        max_pixel_value=255.0, \n        p=1.0\n    )\n], p=1.)","7e7a93c9":"df_subm = pd.read_csv(\"..\/input\/dlub-summer-school-challenge-2021\/submission.csv\")\nimage_path = \"\/kaggle\/working\/test_resized\/\"\ntest_image_paths = [os.path.join(image_path, x) for x in df_subm.filename.values]\ntest_targets = df_subm.category_id.values\ntest_dataset = ImageDataset(\n    image_paths=test_image_paths,\n    targets=test_targets,\n    augmentations=test_aug,\n)","bcdb9e56":"model = FoodNet(num_classes=df.category_id.nunique())\nmodel.load(\"\/kaggle\/working\/model.bin\")","a663b568":"# run inference 5 times\nfinal_preds = None\nfor j in range(5):\n    preds = model.predict(test_dataset, batch_size=32, n_jobs=-1)\n    temp_preds = None\n    for p in preds:\n        if temp_preds is None:\n            temp_preds = p\n        else:\n            temp_preds = np.vstack((temp_preds, p))\n    if final_preds is None:\n        final_preds = temp_preds\n    else:\n        final_preds += temp_preds\nfinal_preds \/= 5","6c0a4175":"final_preds = final_preds.argmax(axis=1)","d6b5e82d":"df_subm.category_id = final_preds","4b2d97ba":"df_subm.head(10)","ec6d769c":"%rm -r \/kaggle\/working\/train_resized\n%rm -r \/kaggle\/working\/test_resized","f7193f9b":"df_subm.to_csv(\"submission.csv\", index=False)","773d07f9":"# Image Resize \nSource: https:\/\/www.kaggle.com\/mlubbilgee\/baseline","0f77d292":"https:\/\/github.com\/lukemelas\/EfficientNet-PyTorch <br\/>\nhttps:\/\/github.com\/albumentations-team\/albumentations <br\/>\nhttps:\/\/www.kaggle.com\/khyeh0719\/pytorch-efficientnet-baseline-train-amp-aug <br\/>\nhttps:\/\/github.com\/abhishekkrthakur\/tez <br\/>\nhttps:\/\/www.kaggle.com\/mlubbilgee\/baseline","31ba2f8d":"# Model","d600fd40":"# Read csv, split & create dataset","0cd961e0":"# Data Augmentations","f37ee005":"# Train & Save Model","65c527ec":"# Inference","b3547e64":"# References"}}