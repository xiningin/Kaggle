{"cell_type":{"9d9aefa2":"code","a406330e":"code","5a6f7241":"code","13a6775b":"code","b7a0b03e":"code","dcb367e5":"code","ba456c5e":"code","91a7bfa0":"code","8ac62b9b":"markdown","d8096c4f":"markdown","a4844a24":"markdown","56e2daa5":"markdown","03473df3":"markdown"},"source":{"9d9aefa2":"import pandas as pd, gc, wt_text_processing_utils as wtp_utils, numpy as np\nfrom tqdm import tqdm\ntqdm.pandas()\n\ndftest = pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/test.csv.zip\")\ndfpred = pd.read_csv(\"..\/input\/classifying-multi-label-comments-0-9741-lb\/submission_binary.csv\")\ndftest = pd.merge(dftest,dfpred, on=\"id\", how=\"left\")\n\n#dftest = dftest.sample(1000).reset_index(drop=True)\ndel dfpred\ngc.collect()","a406330e":"cols = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\nweights = [1, 2, 3, 4, 1, 5]\n\ndef generate_weighted_score(row):\n    numerator = 0\n    for col, wt in zip(cols, weights):\n        numerator += row[col]*wt\n    return numerator\/sum(weights) #Why weight normalise? Because HF gives 0-1 scores\/\n\ndftest[\"unweighted_score\"] = dftest.progress_apply(lambda row: np.mean([row[col] for col in cols]), axis=1)\ndftest[\"weighted_score\"] = dftest.progress_apply(generate_weighted_score, axis=1)","5a6f7241":"dftest[\"weighted_score\"].describe() \ndftest[\"bin\"] = dftest[\"weighted_score\"].round(1)#.astype(str) \ndftest[\"bin\"] = dftest[\"bin\"]>0.\n(dftest[\"weighted_score\"]>=0.5).value_counts()","13a6775b":"dftest[\"normed_text\"] = wtp_utils.preprocess_text(dftest[\"comment_text\"])","b7a0b03e":"cois = ['id', 'comment_text', 'normed_text', 'unweighted_score', 'weighted_score']\ndftest = dftest[cois]\ndftest[cois].to_csv(\"cleaned_train.csv\", index=False) #misnomer. This is test data with roc 0.97","dcb367e5":"from tqdm import tqdm\nimport gc, pandas as pd\ntqdm.pandas()\n\ntraindf = pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip\")\ncols = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ntraindf[\"bin\"] = traindf.apply(lambda row: any([row[c]==1 for c in cols]), axis=1)\ntraindf = traindf.groupby(\"bin\").sample(16225).reset_index(drop=True)\n\nwts = pd.DataFrame([(c, traindf[c].value_counts().iloc[1]) for c in cols])\nwts[1] = 1000\/wts[1]\nwts.set_index(0, inplace=True)\nwts = wts.T.iloc[0].to_dict()\n\ntot_wts = sum(wts.values())\n\ntraindf[\"new_wscore\"] = traindf.progress_apply(lambda row: sum([wts[c]*row[c] for c in cols]), axis=1)\ntraindf[\"new_wscore\"] = traindf[\"new_wscore\"].apply(lambda x: min(x, .25))\ntraindf[\"new_wscore\"] = (traindf[\"new_wscore\"]-traindf[\"new_wscore\"].min())\/(traindf[\"new_wscore\"].max()-traindf[\"new_wscore\"].min())\n\ncois = [\"id\", \"comment_text\", \"new_wscore\"]\ntraindf = traindf[cois]\n\ntraindf[\"normed_text\"] = wtp_utils.preprocess_text(traindf[\"comment_text\"])\ntraindf.to_csv(\"cleaned_train_actual.csv\", index=False)","ba456c5e":"from tqdm import tqdm\nimport gc, pandas as pd\ntqdm.pandas()\n\ntraindf = pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip\")\ncols = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\nis_specific_t = traindf.apply(lambda row: any([row[c]==1 for c in cols[1:]]), axis=1)","91a7bfa0":"#nontoxic < toxiconly < severe toxic only < (obscene\/insult\/both only) < (threat or identity hate or both only)\nis_non_toxic = traindf[\"toxic\"]==0\nfor c in cols[1:]:\n    is_non_toxic &= traindf[c]==0\n    \nis_toxic_only = traindf[\"toxic\"]==1\nfor c in [\"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]:\n    is_toxic_only &= traindf[c]==0\n\nis_severe_toxic_only = traindf[\"severe_toxic\"]==1\nfor c in [\"obscene\", \"threat\", \"insult\", \"identity_hate\"]:\n    is_severe_toxic_only &= traindf[c]==0\n\nis_oi_only = (traindf[\"obscene\"]==1)|(traindf[\"insult\"]==1)\nfor c in [\"toxic\", \"severe_toxic\", \"threat\", \"identity_hate\"]:\n    is_oi_only &= traindf[c]==0\n\nis_it_only = (traindf[\"threat\"]==1)|(traindf[\"identity_hate\"]==1)\nfor c in [\"toxic\", \"severe_toxic\", \"obscene\", \"insult\"]:\n    is_it_only &= traindf[c]==0\n\n    \ntraindf[\"rank\"] = -1 #Inconclusive\ntraindf.loc[is_non_toxic, \"rank\"] = 0\ntraindf.loc[is_toxic_only, \"rank\"] = 1\ntraindf.loc[is_severe_toxic_only, \"rank\"] = 2\ntraindf.loc[is_oi_only, \"rank\"] = 3\ntraindf.loc[is_it_only, \"rank\"] = 4\ntraindf[\"rank\"].value_counts()\n\ntraindf.to_csv(\"ranked_train_actual.csv\", index=False)","8ac62b9b":"# Pseudoranked train data\n\nObscene is usually swearwords. Insult and obscene are kinda hand-in-hand. So same league? threat and identity hate are at highest. So we have non-toxic < toxic < severe toxic < obscenity and insults < threat and identity hate\n","d8096c4f":"Not even a useful baseline. But will work to evaluate the model. Last competition's public NB that uses combined submission also has a binary output, and a score of 0.97. We'll use those probability scores with weights for each comment type to generate a final regressor score. Bertweet model will get preprocessed inputs (except url and twitter handle preprocessing) and a regressor head on top of its model is finetuned. \n\n\n# Test from some prediction\n## Score generation","a4844a24":"```python\ntraincs = set(dftest[\"comment_text\"].unique().tolist())\ndummy = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\")\ntestcs = set(dummy[\"less_toxic\"].unique().tolist()+dummy[\"more_toxic\"].unique().tolist())\nlen(traincs), len(testcs), len(traincs-testcs), len(testcs-traincs), len(traincs.intersection(testcs))\nOP: (153164, 14251, 153164, 14251, 0)\n```","56e2daa5":"# Train data","03473df3":"## Preprocessing\nShould be kinda similar [to source code](https:\/\/github.com\/VinAIResearch\/BERTweet#preprocess).  Don't need twitter like preprocessing. But following are required:\n<ol>\n    <li>Replacing \\n with spaces.<\/li>\n    <li>Replacing <a href=\"https:\/\/en.wikipedia.org\/wiki\/Help:Talk_pages#Indentation\">Wiki specific tags<\/a><\/li>\n    <li>emoji normalizing <\/li>\n<\/ol>"}}