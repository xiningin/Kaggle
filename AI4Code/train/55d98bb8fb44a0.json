{"cell_type":{"484c36ef":"code","2426a3d5":"code","3bac952e":"code","0e5d1162":"code","bb2744b2":"code","cace81c9":"code","61c20b21":"code","f0405040":"code","271a3fb9":"code","cde4bfe3":"code","652933df":"code","288fbf87":"code","4d3256a5":"code","e42abd41":"code","0d1eb046":"code","048e2c88":"code","c105898c":"code","f3d19d12":"code","afedc28a":"code","e20498ed":"code","0deb59ae":"code","b3d9100e":"code","50160fdd":"code","b0bc4687":"code","862b37a1":"code","bc029a27":"code","c966d973":"code","909c9a1b":"code","3650e421":"code","0025964a":"code","678d0c96":"code","0a92a6fe":"code","7b04cde7":"code","9d7db170":"code","71509120":"code","04020ac1":"code","97173ac0":"code","68200e45":"code","9669667f":"code","cb66bc50":"code","3c60254f":"code","7b07f871":"code","9d9dd2da":"code","f5e6d6f6":"code","b6ed725d":"code","6453dc9a":"code","11ad456c":"code","3ceba43f":"code","beb8d360":"code","3d17e9c1":"code","108b8a20":"code","3c9f44fe":"markdown","86012de1":"markdown","bcbc0e98":"markdown","aa9af6d1":"markdown"},"source":{"484c36ef":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns  # visualization tool\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n","2426a3d5":"data =pd.read_csv(\"..\/input\/heart.csv\")","3bac952e":"data.info()","0e5d1162":"data.head()","bb2744b2":"data.columns","cace81c9":"print(data['target'].value_counts(dropna =False))","61c20b21":"data.describe()","f0405040":"data.corr()","271a3fb9":"f,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\n","cde4bfe3":"outlierlower_thalach = 133.5-1.5*(166-133.5)\nprint('outlierlower_thalach =',outlierlower_thalach)\n\noutlierupper_thalach = 166+1.5*(166-133.5)\nprint('outlierupper_thalach =',outlierupper_thalach)","652933df":"data.boxplot(column='thalach',by = 'target')\n","288fbf87":"data[(data['target']==1)].info()","4d3256a5":"data[(data['target']==1)].describe()","e42abd41":"outlierlower_thalach1 = 149-1.5*(172-149)\nprint('outlierlower_thalach1 =',outlierlower_thalach1)\n\noutlierupper_thalach1 = 172+1.5*(172-149)\nprint('outlierupper_thalach1 =',outlierupper_thalach1)","0d1eb046":"data[(data['thalach']<114.5) & (data['target']==1)].head()","048e2c88":"data['thalach'] = data['thalach'].astype('float')","c105898c":"data.drop([17,95,136,139], axis=0, inplace=True)","f3d19d12":"data.info()","afedc28a":"data[(data['target']==1)].describe()","e20498ed":"outlierlower_thalach2 = 159-1.5*(172-150)\nprint('outlierlower_thalach2 =',outlierlower_thalach2)\n\noutlierupper_thalach2 = 172+1.5*(172-150)\nprint('outlierupper_thalach2 =',outlierupper_thalach2)","0deb59ae":"data.boxplot(column='thalach',by = 'target')","b3d9100e":"data.boxplot(column='cp',by = 'target')","50160fdd":"data.boxplot(column='slope',by = 'target')","b0bc4687":"data.plot(kind='scatter', x='thalach', y='slope',alpha = 0.5,color = 'red')\nplt.xlabel('thalach')              # label = name of label\nplt.ylabel('slope')\nplt.title('thalach slope Scatter Plot')  ","862b37a1":"data.plot(kind='scatter', x='target', y='cp',alpha = 0.5,color = 'red')\nplt.xlabel('target')              # label = name of label\nplt.ylabel('cp')\nplt.title('target cp Scatter Plot')  ","bc029a27":"data[(data['slope']>=1) & (data['thalach']>100) & (data['target']==1)]","c966d973":"data1= data[(data['slope']>=1) & (data['thalach']>100) & (data['target']==1)].head()\ndata2= data[(data['slope']>=1) & (data['thalach']>100) & (data['target']==1)].tail()\nconc_data_row = pd.concat([data1,data2],axis =0,ignore_index =False) # axis = 0 : adds dataframes in row\nconc_data_row","909c9a1b":"data1 = data[(data['slope']>=1) & (data['thalach']>100) & (data['target']==1)]['age']\ndata2= data[(data['slope']>=1) & (data['thalach']>100) & (data['target']==1)]['sex']\nconc_data_col = pd.concat([data1,data2],axis =1) # axis = 0 : adds dataframes in row\nconc_data_col","3650e421":"data[(data['slope']>=1) & (data['thalach']>100) & (data['target']==1)].info()","0025964a":"fig, axes = plt.subplots(nrows=2,ncols=1)\ndata[(data['slope']>=1) & (data['thalach']>100) & (data['target']==1)].plot(kind = \"hist\",y = \"cp\",bins = 50,normed = True,ax = axes[0])\ndata[(data['slope']>=1) & (data['thalach']>100) & (data['target']==1)].plot(kind = \"hist\",y = \"cp\",bins = 50,normed = True,ax = axes[1],cumulative = True)\nplt.savefig('graph.png')\nplt","678d0c96":"fig, axes = plt.subplots(nrows=2,ncols=1)\ndata[(data['slope']>=1) & (data['thalach']>100) & (data['target']==1)].plot(kind = \"hist\",y = \"slope\",bins = 50,normed = True,ax = axes[0])\ndata[(data['slope']>=1) & (data['thalach']>100) & (data['target']==1)].plot(kind = \"hist\",y = \"slope\",bins = 50,normed = True,ax = axes[1],cumulative = True)\nplt.savefig('graph.png')\nplt","0a92a6fe":"fig, axes = plt.subplots(nrows=2,ncols=1)\ndata[(data['slope']>=1) & (data['thalach']>100) & (data['target']==1)].plot(kind = \"hist\",y = \"thalach\",bins = 50,normed = True,ax = axes[0])\ndata[(data['slope']>=1) & (data['thalach']>100) & (data['target']==1)].plot(kind = \"hist\",y = \"thalach\",bins = 50,normed = True,ax = axes[1],cumulative = True)\nplt.savefig('graph.png')\nplt","7b04cde7":"y = data.target.values\nx_data = data.drop(['target'], axis = 1)","9d7db170":"x = (x_data - np.min(x_data)) \/ (np.max(x_data) - np.min(x_data)).values","71509120":"x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=0)","04020ac1":"x_train = x_train.T\ny_train = y_train.T\nx_test = x_test.T\ny_test = y_test.T\n","97173ac0":"knn = KNeighborsClassifier(n_neighbors = 2)  # n_neighbors means k\nknn.fit(x_train.T, y_train.T)\nprediction = knn.predict(x_test.T)\n\nprint(\"{} NN Score: {:.2f}%\".format(2, knn.score(x_test.T, y_test.T)*100))","68200e45":"scoreList = []\nfor i in range(1,20):\n    knn2 = KNeighborsClassifier(n_neighbors = i)  # n_neighbors means k\n    knn2.fit(x_train.T, y_train.T)\n    scoreList.append(knn2.score(x_test.T, y_test.T))\n    \nplt.plot(range(1,20), scoreList)\nplt.xticks(np.arange(1,20,1))\nplt.xlabel(\"K value\")\nplt.ylabel(\"Score\")\nplt.show()\n\n\nprint(\"Maximum KNN Score is {:.2f}%\".format((max(scoreList))*100))","9669667f":"svm = SVC(random_state = 1)\nsvm.fit(x_train.T, y_train.T)","cb66bc50":"print(\"Test Accuracy of SVM Algorithm: {:.2f}%\".format(svm.score(x_test.T,y_test.T)*100))","3c60254f":"nb = GaussianNB()\nnb.fit(x_train.T, y_train.T)\nprint(\"Accuracy of Naive Bayes: {:.2f}%\".format(nb.score(x_test.T,y_test.T)*100))","7b07f871":"from sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier()\ndtc.fit(x_train.T, y_train.T)\nprint(\"Decision Tree Test Accuracy {:.2f}%\".format(dtc.score(x_test.T, y_test.T)*100))","9d9dd2da":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators = 1000, random_state = 1)\nrf.fit(x_train.T, y_train.T)\nprint(\"Random Forest Algorithm Accuracy Score : {:.2f}%\".format(rf.score(x_test.T,y_test.T)*100))","f5e6d6f6":"def initialize(dimension):\n    \n    weight = np.full((dimension,1),0.01)\n    bias = 0.0\n    return weight,bias","b6ed725d":"def sigmoid(z):\n    \n    y_head = 1\/(1+ np.exp(-z))\n    return y_head","6453dc9a":"def forwardBackward(weight,bias,x_train,y_train):\n    # Forward\n    \n    y_head = sigmoid(np.dot(weight.T,x_train) + bias)\n    loss = -(y_train*np.log(y_head) + (1-y_train)*np.log(1-y_head))\n    cost = np.sum(loss) \/ x_train.shape[1]\n    \n    # Backward\n    derivative_weight = np.dot(x_train,((y_head-y_train).T))\/x_train.shape[1]\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]\n    gradients = {\"Derivative Weight\" : derivative_weight, \"Derivative Bias\" : derivative_bias}\n    \n    return cost,gradients","11ad456c":"def update(weight,bias,x_train,y_train,learningRate,iteration) :\n    costList = []\n    index = []\n    \n    #for each iteration, update weight and bias values\n    for i in range(iteration):\n        cost,gradients = forwardBackward(weight,bias,x_train,y_train)\n        weight = weight - learningRate * gradients[\"Derivative Weight\"]\n        bias = bias - learningRate * gradients[\"Derivative Bias\"]\n        \n        costList.append(cost)\n        index.append(i)\n\n    parameters = {\"weight\": weight,\"bias\": bias}\n    \n    print(\"iteration:\",iteration)\n    print(\"cost:\",cost)\n\n    plt.plot(index,costList)\n    plt.xlabel(\"Number of Iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n\n    return parameters, gradients","3ceba43f":"def predict(weight,bias,x_test):\n    z = np.dot(weight.T,x_test) + bias\n    y_head = sigmoid(z)\n\n    y_prediction = np.zeros((1,x_test.shape[1]))\n    \n    for i in range(y_head.shape[1]):\n        if y_head[0,i] <= 0.5:\n            y_prediction[0,i] = 0\n        else:\n            y_prediction[0,i] = 1\n    return y_prediction","beb8d360":"def logistic_regression(x_train,y_train,x_test,y_test,learningRate,iteration):\n    dimension = x_train.shape[0]\n    weight,bias = initialize(dimension)\n    \n    parameters, gradients = update(weight,bias,x_train,y_train,learningRate,iteration)\n\n    y_prediction = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    \n\n    print(\"Manuel Test Accuracy: {:.2f}%\".format((100 - np.mean(np.abs(y_prediction - y_test))*100)\/100*100))","3d17e9c1":"logistic_regression(x_train,y_train,x_test,y_test,1,100)","108b8a20":"methods = [\"Logistic Regression\", \"KNN\", \"SVM\", \"Naive Bayes\", \"Decision Tree\", \"Random Forest\"]\naccuracy = [86.89, 88.52, 86.89, 86.89, 78.69, 88.52]\ncolors = [\"purple\", \"green\", \"orange\", \"magenta\",\"#CFC60E\",\"#0FBBAE\"]\n\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(16,5))\nplt.yticks(np.arange(0,100,10))\nplt.ylabel(\"Accuracy %\")\nplt.xlabel(\"Algorithms\")\nsns.barplot(x=methods, y=accuracy, palette=colors)\nplt.show()","3c9f44fe":"people with heart disease with slope 2, thalach 160, cp 2 when I interpret all the graphs","86012de1":"The most powerful correlations  cp target, target thalach, thalach slope between.","bcbc0e98":"when the slope is 1 the most thalach values 110 170 between in all value\nwhen the slope is 2 the most thalach values 130 190 between in all value","aa9af6d1":"there is probability of heart disease in all values of cp"}}