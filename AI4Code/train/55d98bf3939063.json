{"cell_type":{"1bb6e4d9":"code","ec8530b1":"code","759c31a7":"code","8d2a4328":"code","6da74057":"code","aeaa34ab":"code","eb725009":"code","fc821d13":"code","dceae62b":"code","fb51fc35":"code","d2ccc247":"code","916f507e":"code","80c1f8e8":"code","d5cb36a5":"code","42249f30":"code","a89ea94e":"code","eaba57e9":"code","9b274675":"code","daa745c3":"code","55356afd":"code","d793abc3":"code","d9ec4ff6":"code","37ba46b9":"code","80ca8179":"code","36566b3a":"code","c623ea1e":"code","66dca698":"code","db35d87a":"code","68d2bfd8":"code","7c34a1e1":"code","06e4a86b":"code","89f4584b":"code","090f1c01":"code","0724b726":"code","b999c6f9":"code","48dcd616":"code","1188ad15":"code","48dfe3c7":"code","189f7855":"code","37617024":"markdown","723ae1a4":"markdown","db2b1b7b":"markdown","85891ffc":"markdown","98f58ee1":"markdown","0e6422a2":"markdown","6a9b9089":"markdown","2443ac9a":"markdown","90623922":"markdown","dfcdab28":"markdown","6784c726":"markdown","11e80a58":"markdown","7e464123":"markdown","f792e6b2":"markdown","b4a75f08":"markdown","b3bc17dd":"markdown","1c8c4a0d":"markdown","6dca1212":"markdown","cf58a649":"markdown","09d761af":"markdown","44c921e6":"markdown","1ec683cb":"markdown","ccac105e":"markdown","792464f3":"markdown","d5969f65":"markdown","9b74ec2a":"markdown","29e78301":"markdown","d40ba18b":"markdown","75e12489":"markdown","64bd64d9":"markdown"},"source":{"1bb6e4d9":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(action='ignore', category=FutureWarning)\npd.set_option('display.max_rows',500)","ec8530b1":"data=pd.read_csv('..\/input\/concrete.csv')\ndata.head()","759c31a7":"data.shape","8d2a4328":"data.info()","6da74057":"data.describe().T","aeaa34ab":"for i in data.columns:\n    sns.distplot(data[i])\n    plt.show()","eb725009":"data.describe()","fc821d13":"data.isnull().sum()","dceae62b":"q1=data.quantile(0.25)\nq3=data.quantile(0.75)\nIQR=q3-q1\ncwo=((data.iloc[:] <(q1-1.5*IQR))|(data.iloc[:]>(q3+1.5*IQR))).sum(axis=0)\nopdf=pd.DataFrame(cwo,index=data.columns,columns=['No. of Outliers'])\nopdf['Percentage Outliers']=round(opdf['No. of Outliers']*100\/len(data),2)\nopdf","fb51fc35":"rwo=(((data[:]<(q1-1.5*IQR))|(data[:]>(q3+1.5*IQR))).sum(axis=1))\nro005=(((rwo\/len(data.columns))<0.05).sum())*100\/len(data)\nro01=(((rwo\/len(data.columns))<0.1).sum())*100\/len(data)\nro015=(((rwo\/len(data.columns))<0.15).sum())*100\/len(data)\nro02=(((rwo\/len(data.columns))<0.2).sum())*100\/len(data)\nro025=(((rwo\/len(data.columns))<0.25).sum())*100\/len(data)\nro03=(((rwo\/len(data.columns))<0.30).sum())*100\/len(data)\nro035=(((rwo\/len(data.columns))<=0.35).sum())*100\/len(data)\nro04=(((rwo\/len(data.columns))<=0.4).sum())*100\/len(data)\nro045=(((rwo\/len(data.columns))<=0.45).sum())*100\/len(data)\nro05=(((rwo\/len(data.columns))<=0.50).sum())*100\/len(data)\nro055=(((rwo\/len(data.columns))<0.55).sum())*100\/len(data)\nro06=(((rwo\/len(data.columns))<0.6+0).sum())*100\/len(data)\nro=pd.DataFrame(np.round([ro005,ro01,ro015,ro02,ro025,ro03,ro035,ro04,ro045,ro05,ro055,ro06],2),\n             index=['5%','10%','15%','20%','25%','30%','35%','40%','45%','50%','55%','60%'],\n            columns=['% Data'])\nro.index.name='% Outlier'\nro","d2ccc247":"cp = data.corr()\nmask = np.zeros_like(cp)\nmask[np.triu_indices_from(mask)] = True\nplt.figure(figsize=(8,8))\nwith sns.axes_style(\"white\"):\n    sns.heatmap(cp,annot=True,linewidth=2,mask = mask,cmap=\"coolwarm\")\nplt.title(\"Correlation Plot\")\nplt.show()","916f507e":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(data[data.columns[:-1]],\n                                                    data[[data.columns[-1]]],\n                                                    test_size = .2,\n                                                    random_state = 1)\nimport seaborn as sns\nsns.heatmap(x_train.corr().abs())\nplt.show()","80c1f8e8":"import statsmodels.api as sm\nX=data.iloc[:,:8]\nY=data.iloc[:,8]","d5cb36a5":"ls=sm.OLS(Y,sm.add_constant(X))\nresults=ls.fit()\nresults.summary()","42249f30":"ls=sm.OLS(Y,X)\nresults=ls.fit()\nresults.summary()","a89ea94e":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split","eaba57e9":"X_train,X_test,y_train,y_test = train_test_split(X, Y, random_state=150, test_size=0.3 )","9b274675":"lr=LinearRegression()\nlr.fit(X_train,y_train)\nprint('Score: ',lr.score(X_train,y_train))\ny_pred_lrtr=lr.predict(X_train)\ny_pred_lrte=lr.predict(X_test)\nfrom sklearn.metrics import r2_score\nprint('Train R2 score: ',r2_score(y_train,y_pred_lrtr))\nprint('Test R2 score: ',r2_score(y_test,y_pred_lrte))","daa745c3":"from sklearn.preprocessing import PolynomialFeatures\npf = PolynomialFeatures(degree = 2)\nX_polytr = pf.fit_transform(X_train)\nlr.fit(X_polytr,y_train)\ny_pred_lr2tr = lr.predict(X_polytr)\nprint(\"Training R2 - degree 2 polynomial: \",r2_score(y_train, y_pred_lr2tr ))\nX_polyte = pf.fit_transform(X_test)\ny_pred_lr2te= lr.predict(X_polyte)\nprint(\"Test R2 - degree 2 polynomial: \",r2_score(y_test,y_pred_lr2te))","55356afd":"pf = PolynomialFeatures(degree = 3)\nX_polytr = pf.fit_transform(X_train)\nlr.fit(X_polytr,y_train)\ny_pred_lr2tr = lr.predict(X_polytr)\nprint(\"Training R2 - degree 2 polynomial: \",r2_score(y_train, y_pred_lr2tr ))\nX_polyte = pf.fit_transform(X_test)\ny_pred_lr2te= lr.predict(X_polyte)\nprint(\"Test R2 - degree 2 polynomial: \",r2_score(y_test,y_pred_lr2te))","d793abc3":"pf = PolynomialFeatures(degree = 4)\nX_polytr = pf.fit_transform(X_train)\nlr.fit(X_polytr,y_train)\ny_pred_lr2tr = lr.predict(X_polytr)\nprint(\"Training R2 - degree 2 polynomial: \",r2_score(y_train, y_pred_lr2tr ))\nX_polyte = pf.fit_transform(X_test)\ny_pred_lr2te= lr.predict(X_polyte)\nprint(\"Test R2 - degree 2 polynomial: \",r2_score(y_test,y_pred_lr2te))","d9ec4ff6":"from sklearn.tree import DecisionTreeRegressor","37ba46b9":"dt=DecisionTreeRegressor()\ndt.fit(X_train,y_train)\ndt.score(X_train,y_train)\ny_pred_dttr=dt.predict(X_train)\ny_pred_dtte=dt.predict(X_test)\nprint('Train R2 score: ',r2_score(y_train,y_pred_dttr))\nprint('Test R2 score: ',r2_score(y_test,y_pred_dtte))","80ca8179":"from sklearn.model_selection import GridSearchCV\nparam_grid = {'max_depth': np.arange(3, 8),\n             'criterion' : ['mse','mae'],\n             'max_leaf_nodes': [5,10,20,100],\n             'min_samples_split': [2, 5, 10, 20]}\n\ngrid_tree = GridSearchCV(DecisionTreeRegressor(), param_grid, cv = 5, scoring= 'r2')\ngrid_tree.fit(X_train, y_train)\nprint(grid_tree.best_estimator_)\nprint(np.abs(grid_tree.best_score_))","36566b3a":"dtpr=DecisionTreeRegressor(criterion='mse', max_depth=7, max_features=None,\n                      max_leaf_nodes=100, min_impurity_decrease=0.0,\n                      min_impurity_split=None, min_samples_leaf=1,\n                      min_samples_split=10, min_weight_fraction_leaf=0.0,\n                      presort=False, random_state=None, splitter='best')\ndtpr.fit(X_train,y_train)\ndtpr.score(X_train,y_train)\ny_pred_dtprtr=dtpr.predict(X_train)\ny_pred_dtprte=dtpr.predict(X_test)\nprint('Train R2 score: ',r2_score(y_train,y_pred_dtprtr))\nprint('Test R2 score: ',r2_score(y_test,y_pred_dtprte))","c623ea1e":"param_grid = {'max_depth': np.arange(3, 6),\n             'criterion' : ['mse','mae'],\n             'max_leaf_nodes': [100,105, 90,95],\n             'min_samples_split': [6,7,8,9,10],\n             'max_features':[2,3,4,5,6]}\n\ngrid_tree = GridSearchCV(DecisionTreeRegressor(), param_grid, cv = 5, scoring= 'r2')\ngrid_tree.fit(X_train, y_train)\nprint(grid_tree.best_estimator_)\nprint(np.abs(grid_tree.best_score_))","66dca698":"dtpr=DecisionTreeRegressor(criterion='mae', max_depth=5, max_features=6,\n                      max_leaf_nodes=95, min_impurity_decrease=0.0,\n                      min_impurity_split=None, min_samples_leaf=1,\n                      min_samples_split=8, min_weight_fraction_leaf=0.0,\n                      presort=False, random_state=None, splitter='best')\ndtpr.fit(X_train,y_train)\ndtpr.score(X_train,y_train)\ny_pred_dtprtr=dtpr.predict(X_train)\ny_pred_dtprte=dtpr.predict(X_test)\nprint('Train R2 score: ',r2_score(y_train,y_pred_dtprtr))\nprint('Test R2 score: ',r2_score(y_test,y_pred_dtprte))","db35d87a":"from sklearn.ensemble import AdaBoostRegressor\nabr = AdaBoostRegressor(random_state=0, n_estimators=100)\nabr.fit(X_train, y_train)\nabr.feature_importances_  \nabr.fit(X_train,y_train)\nabr.score(X_train,y_train)\ny_pred_abrtr=abr.predict(X_train)\ny_pred_abrte=abr.predict(X_test)\nprint('Train R2 score: ',r2_score(y_train,y_pred_abrtr))\nprint('Test R2 score: ',r2_score(y_test,y_pred_abrte))","68d2bfd8":"from sklearn.ensemble import RandomForestRegressor\nrfr=RandomForestRegressor()\nrfr.fit(X_train,y_train)\nrfr.score(X_train,y_train)\ny_pred_rfrtr=rfr.predict(X_train)\ny_pred_rfrte=rfr.predict(X_test)\nprint('Train R2 score: ',r2_score(y_train,y_pred_rfrtr))\nprint('Test R2 score: ',r2_score(y_test,y_pred_rfrte))","7c34a1e1":"param_grid = {'max_depth': np.arange(3, 8),\n             'criterion' : ['mse','mae'],\n             'max_leaf_nodes': [100,105, 90,95],\n             'min_samples_split': [6,7,8,9,10],\n             'max_features':['auto','sqrt','log2']}\n\ngrid_tree = GridSearchCV(RandomForestRegressor(), param_grid, cv = 5, scoring= 'r2')\ngrid_tree.fit(X_train, y_train)\nprint(grid_tree.best_estimator_)\nprint(np.abs(grid_tree.best_score_))","06e4a86b":"rfr=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=7,\n                      max_features='auto', max_leaf_nodes=90,\n                      min_impurity_decrease=0.0, min_impurity_split=None,\n                      min_samples_leaf=2, min_samples_split=7,\n                      min_weight_fraction_leaf=0.0, n_estimators=100,\n                      n_jobs=None, oob_score=False, random_state=None,\n                      verbose=0, warm_start=False)\nrfr.fit(X_train,y_train)\nrfr.score(X_train,y_train)\ny_pred_rfrtr=rfr.predict(X_train)\ny_pred_rfrte=rfr.predict(X_test)\nprint('Train R2 score: ',r2_score(y_train,y_pred_rfrtr))\nprint('Test R2 score: ',r2_score(y_test,y_pred_rfrte))","89f4584b":"from sklearn.ensemble import GradientBoostingRegressor\ngb=GradientBoostingRegressor()\ngb.fit(X_train,y_train)\ngb.score(X_train,y_train)\ny_pred_gbtr=gb.predict(X_train)\ny_pred_gbte=gb.predict(X_test)\nprint('Train R2 score: ',r2_score(y_train,y_pred_gbtr))\nprint('Test R2 score: ',r2_score(y_test,y_pred_gbte))","090f1c01":"param_grid = {'n_estimators': [230],\n              'max_depth': range(10,31,2), \n              'min_samples_split': range(50,501,10), \n              'learning_rate':[0.2]}\nclf = GridSearchCV(GradientBoostingRegressor(random_state=1), \n                   param_grid = param_grid, scoring='r2', \n                   cv=5).fit(X_train, y_train)\nprint(clf.best_estimator_) \nprint(\"R Squared:\",clf.best_score_)","0724b726":"gb=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n                          learning_rate=0.2, loss='ls', max_depth=14,\n                          max_features=None, max_leaf_nodes=None,\n                          min_impurity_decrease=0.0, min_impurity_split=None,\n                          min_samples_leaf=1, min_samples_split=150,\n                          min_weight_fraction_leaf=0.0, n_estimators=230,\n                          n_iter_no_change=None, presort='auto', random_state=1,\n                          subsample=1.0, tol=0.0001, validation_fraction=0.1,\n                          verbose=0, warm_start=False)\ngb.fit(X_train,y_train)\ngb.score(X_train,y_train)\ny_pred_gbtr=gb.predict(X_train)\ny_pred_gbte=gb.predict(X_test)\nprint('Train R2 score: ',r2_score(y_train,y_pred_gbtr))\nprint('Test R2 score: ',r2_score(y_test,y_pred_gbte))","b999c6f9":"import xgboost as xgb","48dcd616":"from xgboost import XGBRegressor\n\nxgb=XGBRegressor()\nxgb.fit(X_train,y_train)\nprint('Model Score: ', xgb.score(X_train,y_train))\ny_pred_xgbtr=xgb.predict(X_train)\ny_pred_xgbte=xgb.predict(X_test)\nprint('Train R2-Score: ', r2_score(y_train,y_pred_xgbtr))\nprint('Test R2-Score: ', r2_score(y_test,y_pred_xgbte))","1188ad15":"xgb=XGBRegressor(base_score=0.7, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=0.65, colsample_bytree=1, gamma=0.3,\n             importance_type='weight', learning_rate=0.2, max_delta_step=150,\n             max_depth=4, min_child_weight=0.5, missing=None, n_estimators=200,\n             n_jobs=1, nthread=None, objective='reg:squarederror', random_state=0,\n             reg_alpha=0.001, reg_lambda=1, scale_pos_weight=1, seed=None,\n             silent=None, subsample=1, verbosity=1)\nxgb.fit(X_train,y_train)\nprint('Model Score: ', xgb.score(X_train,y_train))\ny_pred_xgbtr=xgb.predict(X_train)\ny_pred_xgbte=xgb.predict(X_test)\nprint('Train R2-Score: ', r2_score(y_train,y_pred_xgbtr))\nprint('Test R2-Score: ', r2_score(y_test,y_pred_xgbte))","48dfe3c7":"import shap\nexplainer = shap.TreeExplainer(xgb)\nshap_values = explainer.shap_values(X_train)\nfor i in X_train.columns:\n    shap.dependence_plot(i,shap_values, X_train)","189f7855":"shap.summary_plot(shap_values, X_train)","37617024":"The fully grown tree is overfitting. This can be controlled by pruning the tree. Using grid search we find the optimum depth and the impurity criterion and other hyper parameters.","723ae1a4":"RandomForest Regressor","db2b1b7b":"This model is slightly overfit. XGBoost may or may not reduce it. Trying out XGBoost in the next step.","85891ffc":"AdaBoost Regressor","98f58ee1":"Here we can see that the constant term is having P value greater than 0.05 viz. the assumed level of significance, thus we remove the constant term from modelling","0e6422a2":"XGBoost Regressor","6a9b9089":"SKLEARN - Linear Regression","2443ac9a":"It's severely overfit even now. We still have to tune it.","90623922":"Bivariate Analysis","dfcdab28":"Outlier Treatment\n\nWe calculate the outliers in each columns.","6784c726":"The overfit has reduced but the model performance has nt imporoved on the test data. So we now move onto other models.","11e80a58":"Univariate Analysis","7e464123":"Adaboost has reduced the variance and improved the model performance as well.","f792e6b2":"# Project  Description -\n\nThis project on Featurization, Model Selection & Tuning is conducted on a data set of concrete.csv.\n\n#### Data Description:\n\nThe actual concrete compressive strength (MPa) for a given mixture under a specific age (days) was determined from laboratory. Data is in raw form (not scaled).The data has 8 quantitative input variables, and 1 quantitative output variable, and 1030 instances (observations).\n\n#### Domain:\n\nMaterial manufacturing\n\n#### Context:\n\nConcrete is the most important material in civil engineering. The concrete compressive strength is a highly nonlinear function of age and ingredients. These ingredients include cement, blast furnace slag, fly ash, water, superplasticizer, coarse aggregate, and fine aggregate .\n\n#### Attribute Information:\n\n\n    \u25cf Cement : measured in kg in a m3 mixture\n    \u25cf Blast : measured in kg in a m3 mixture\n    \u25cf Fly ash : measured in kg in a m3 mixture\n    \u25cf Water : measured in kg in a m3 mixture\n    \u25cf Superplasticizer : measured in kg in a m3 mixture\n    \u25cf Coarse Aggregate : measured in kg in a m3 mixture\n    \u25cf Fine Aggregate : measured in kg in a m3 mixture\n    \u25cf Age : day (1~365)\n    \u25cf Concrete compressive strength measured in MPa\n   \n\n#### Learning Outcomes:\n\n\n    \u25cf Exploratory Data Analysis\n    \u25cf Building ML models for regression\n    \u25cf Hyper parameter tuning\n   \n\n#### Objective:\n\nModeling of strength of high performance concrete using Machine Learning\n\n#### Steps and tasks:\n\nDeliverable -1 (Exploratory data quality report reflecting the following)\n    \na. Univariate analysis\n    \ni. Univariate analysis \u2013 data types and description of the independent attributes which should include (name, meaning, range \n   of values observed, central values (mean and median), standard deviation and quartiles, analysis of the body of \n   distributions \/ tails, missing values, outliers\n    \nb. Multivariate analysis\n    \ni. Bi-variate analysis between the predictor variables and between the predictor variables and target column. Comment\n   on your findings in terms of their relationship and degree of relation if any. Presence of leverage points. \n   Visualize the analysis using boxplots and pair plots, histograms or density curves. Select the most appropriate \n   attributes\n           \nc. Strategies to address the different data challenges such as data pollution, outliers and missing values\n\nDeliverable -2 (Feature Engineering techniques)\n\na. Identify opportunities (if any) to create a composite feature, drop a feature\n\nb. Decide on complexity of the model, should it be simple linear mode in terms of parameters or would a quadratic or higher \n   degree help\n    \nc. Explore for gaussians. If data is likely to be a mix of gaussians, explore individual clusters and present your findings \n   in terms of the independent attributes and their suitability to predict strength\n\nDeliverable -3 (create the model )\n\na. Obtain feature importance for the individual features using multiple methods and present your findings\n\nDeliverable -4 (Tuning the model)\n\na. Algorithms that you think will be suitable for this project\n    \nb. Techniques employed to squeeze that extra performance out of the model without making it overfit or underfit\n    \nc. Model performance range at 95% confidence level\n    ","b4a75f08":"Here we have achieved a model which performes well with both test and train data. This is very lightly overfit. It can be further adjusted. But this project will focus on the interpretability of the model.","b3bc17dd":"More than 10% outliers of each row is not present. ","1c8c4a0d":"Polynomial Regression - Degree 2","6dca1212":"Gradient Boosting","cf58a649":"Little correlation of ~0.6 between Superplasticizer and Water (which is negative as evident from scatter matrix), but lets move forward as is.","09d761af":"We can observe that there is a steady increase in the compressive strength of concrete with passing time.","44c921e6":"Beyond this, the model does not perform well. From this it is clear that the model is non linear. Thus we proceed to other non-linear models.","1ec683cb":"Decision Tree Regressor","ccac105e":"Linear Regression - OLS","792464f3":"Interpreting Black Box Models","d5969f65":"Without the constant term we can observe that the R-squared value has increased drastically.","9b74ec2a":"The random forest is overfitting but has improved the model performance. So we now tune the hyper parameters to reduce the overfit.","29e78301":"Then we calculate the columns wise outliers. To determine in each row what is the presence of outliers.","d40ba18b":"1) From the first plot we can see that Cement content and presence of super plasticizer has a linear impact on the model. When the cement content is less than 300 compressive strength decreases. As the cement content increases beyond 300, the compressive strength increases as well. The compressive strength increases with higher content of super plasticizers.\n\n2) From the second plot we observe, when the blast furnace slag is greater than 50 kg\/m3, the comrpessive strength increases. This feature in combination with age is responsible for the compressive strength.\n\n3) From the third plot we observe, when there is no fly ash present, but the mix with highest content of superplasticizers have a positive impact on the compressive strength. However the fly ash has an increasing followed by decreasing trend with the compressive strength. When the fly ash is in the range of 75-150 kg\/m3 range, with super plasticizer content of 10 kg\/m3 leads to highest compressive strength observed. Similary in the range of fly ash > 150 kg\/m3 the least compressive strength was observed.\n\n4) From plot 4, we observe that water and blast furnace slag along with their interactive effect contributes to the compressive strength. Water content less than 150 kg\/m3 with lower blast furnac slag provides the highest compressive strength. For water content greater than 150 kg\/m3, higher blast furnace slag is preferred to have greater compressive strength.\n\n5) From plot 5, superplasticizers in the range of 10-12kg\/m3 along with higher blast furnace slag increases the compressive strength.\n\n6) Plot 6 suggests that, with increasing coarse aggregate, keeping the coment content lower, has positive impact on the compressive strength.\n\n7) For fine aggregates less than around 650 kg\/m3, the water content should be greater than 180 kg\/m3. Further, if the fine aggregate content is greater than 650 kg\/m3, water content should be lesser than 170 kg\/m3.\n\n8) From plot 8, with increasing age, the compressive strength surely increases, however different amount of water content surely has an effect with age. For 28 days strength, having water content greater than 300kg\/m3 is desired.","75e12489":"EDA","64bd64d9":"No high correlation between any two features. Lets verify with Heatmap."}}