{"cell_type":{"22019bfc":"code","6abb823d":"code","c40524b4":"code","f224b8ae":"code","bee5a8d7":"code","5dc9387b":"code","1a2e9258":"code","8f1a20b0":"code","fa561e8b":"code","c81ddca4":"code","87752dbe":"code","37584b43":"code","a21481fc":"code","de01a371":"code","e10fa5bb":"code","ff18ffbb":"code","4eec246f":"code","ce86b455":"code","97b9a77a":"code","f5cd76e6":"code","85de2351":"code","31088a71":"code","cd322aa3":"code","a3eab6f4":"code","d139c19f":"code","58fcfa79":"code","35bd2ab9":"code","9e3d1c2c":"code","29e21a7a":"code","2345e814":"code","ef7473dc":"code","7dc0f9de":"code","4e4fb709":"code","ef05c9b4":"code","178be321":"code","413ad112":"code","92768da4":"code","ff04a8ba":"code","0ab423bb":"code","1e1ea156":"code","832c0127":"code","29c96651":"code","84f51516":"code","34855966":"code","e161d5b4":"code","1ffd1098":"code","6381e30a":"code","e534fe0f":"code","3b4d8bc8":"code","038f997d":"code","3f98f921":"code","15db2315":"code","4f112fc8":"code","d5555a16":"code","c088b606":"code","2829c879":"markdown","4a171b0a":"markdown","d4a2b16f":"markdown","e8c166ee":"markdown","0be1aa81":"markdown","fb6ab003":"markdown","ccc30b4d":"markdown","d55863c0":"markdown","6b0eeb31":"markdown","1ef6dba0":"markdown","cdaf6bf8":"markdown","c13adaa7":"markdown","b68a7b4b":"markdown","becc5c5d":"markdown","994e52dc":"markdown","53690626":"markdown","0689a511":"markdown","014ae37a":"markdown","69770cd9":"markdown","733c3307":"markdown","d1d71292":"markdown","9dc02ee4":"markdown","d51cd8d7":"markdown","7b9689ab":"markdown","6b7cd243":"markdown","0e311e60":"markdown","abef22cb":"markdown","b3ce1385":"markdown","8f19a8ef":"markdown","157359e8":"markdown","b5c06c74":"markdown","1289a5e1":"markdown","6fe3a47e":"markdown","320a8fe7":"markdown","8726a901":"markdown"},"source":{"22019bfc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker\n#image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\n# Modelling importations\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier,VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\n\nplt.style.use(\"seaborn-whitegrid\")\n\nimport seaborn as sns\nfrom collections import Counter\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing\n#Shift+Enter) will list all files under the input directory\n\n\n# Modelling importations\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier,VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\n# machine learning\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\n\n# Input data files are available in the \"..\/input\/\" directory.\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","6abb823d":"# apply head() command just to see if the file is readble and it is OK\n\ntrain_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()","c40524b4":"# apply head() command to see if the file is readble and it is OK\n\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()","f224b8ae":"#Get how many recoreds & Columns are there in each file\n\nprint(\"train.csv total num of records & columns:\", train_data.shape)\nprint(\"test.csv total num of records & columns:\",test_data.shape)","bee5a8d7":"# Apply describe() command to see if there is null values \n# pass (include='all') parameter to get non-numeric columns otherwise will not get Cabin Number and Embarked\n\ntrain_data.describe(include='all')\n","5dc9387b":"train_data.info()\nprint(\"Result of ells with null values\")","1a2e9258":"#fill empty values\ntrain_data[\"Age\"] = train_data[\"Age\"].fillna(-0.5)\ntest_data[\"Age\"] = test_data[\"Age\"].fillna(-0.5)\n","8f1a20b0":"train_data[\"Age\"].hist(bins=40,color='green' ,alpha=0.6)","fa561e8b":"sns.countplot(x='SibSp',data=train_data)","c81ddca4":"#sort the ages into logical categories\nbins = [-1, 0, 5, 12, 18, 24, 35, 60, np.inf]\nlabels = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\ntrain_data['AgeGroup'] = pd.cut(train_data[\"Age\"], bins, labels = labels)\ntest_data['AgeGroup'] = pd.cut(test_data[\"Age\"], bins, labels = labels)\n\n#draw a bar plot of Age vs. survival\nsns.barplot(x=\"AgeGroup\", y=\"Survived\", data=train_data)\nplt.show()","87752dbe":"def bar_plot(variable):\n    \"\"\"\n        input: variable e.g. Sex\n        output: bar plot & value count\n    \"\"\"\n    # get feature\n    var = train_data[variable]\n    #count numer of categorical variable(value\/sample)\n    varValue = var.value_counts()\n    \n    #visualize\n    plt.figure(figsize=(9,3))\n    plt.bar(varValue.index,varValue)\n    plt.xticks(varValue.index, varValue.index.values)\n    plt.ylabel(\"Frequency\")\n    plt.title(variable)\n    plt.show()\n    print(\"{}: \\n {}\".format(variable,varValue))","37584b43":"category1 = [\"Sex\"]\nfor cat in category1:\n    bar_plot(cat)","a21481fc":" men = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)\/len(men)\n\nprint(\"% of male who survived:\", rate_men)","de01a371":" female = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_female = sum(female)\/len(female)\n\nprint(\"% of female who survived:\", rate_female)","e10fa5bb":"def plot_hist(variable):\n    plt.figure(figsize = (9,3))\n    plt.hist(train_data[variable],bins=75)\n    plt.xlabel(variable)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"{} distribution with histogram\".format(variable))\n    plt.show()","ff18ffbb":"# Male \/ Femal Counts and Histogram on Titanic\ntrain_data['Sex'].value_counts()","4eec246f":"numericVariables = [\"Sex\"]\nfor numer in numericVariables:\n    plot_hist(numer)","ce86b455":"# Age exploration and Histogram on Titanic\ntrain_data['Age'].value_counts()","97b9a77a":"numericVariables = [\"Age\"]\nfor numer in numericVariables:\n    plot_hist(numer)","f5cd76e6":"# Ticket Classes exploration and Histogram on Titanic\ntrain_data['Pclass'].value_counts()","85de2351":"numericVariables = [\"Pclass\"]\nfor numer in numericVariables:\n    plot_hist(numer)","31088a71":"numericVariables = [\"Sex\", \"Age\", \"Pclass\"]\nfor numer in numericVariables:\n    plot_hist(numer)","cd322aa3":"# Get number of Survived \/ Died passangers grouped by sex\n\ntrain_data.groupby(['Sex','Survived'])['Sex'].count()\n\n","a3eab6f4":"# Get number of Survived \/ Died passangers base on sex\ndata = train_data.copy()\ndata['Died']= 1 - data['Survived']\ndata.groupby('Sex').agg('sum')[['Survived','Died']].plot(kind='bar',stacked=True)","d139c19f":"# Get number of Survived \/ Died passangers grouped by ticket class\ntrain_data.groupby(['Pclass','Survived'])['Pclass'].count()","58fcfa79":"# Get number of Survived \/ Died passangers base on sex\ndata = train_data.copy()\ndata['Died']= 1 - data['Survived']\ndata.groupby('Pclass').agg('sum')[['Survived','Died']].plot(kind='bar',stacked=True)","35bd2ab9":"# Get number of Survived \/ Died passangers grouped by Age\ntrain_data.groupby(['Age','Survived'])['Age'].count()","9e3d1c2c":"# Get number of Survived \/ Died passangers base on Age\ndata = train_data.copy()\ndata['Died']= 1 - data['Survived']\ndata.groupby('Age').agg('sum')[['Survived','Died']].plot(kind='bar',stacked=True)","29e21a7a":"# Get number of Survived \/ Died passangers grouped by Parch\ntrain_data.groupby(['Parch','Survived'])['Parch'].count()","2345e814":"# Get number of Survived \/ Died passangers base on Parch\ndata = train_data.copy()\ndata['Died']= 1 - data['Survived']\ndata.groupby('Parch').agg('sum')[['Survived','Died']].plot(kind='bar',stacked=True)","ef7473dc":"# Get number of Survived \/ Died passangers grouped by SibSp\ntrain_data.groupby(['SibSp','Survived'])['SibSp'].count()","7dc0f9de":"# Get number of Survived \/ Died passangers base on SibSp\ndata = train_data.copy()\ndata['Died']= 1 - data['Survived']\ndata.groupby('SibSp').agg('sum')[['Survived','Died']].plot(kind='bar',stacked=True)","4e4fb709":"#map each Sex value to a numerical value\nsex_mapping = {\"male\": 0, \"female\": 1}\ntrain_data['Sex'] = train_data['Sex'].map(sex_mapping)\ntest_data['Sex'] = test_data['Sex'].map(sex_mapping)\n\ntrain_data.head()","ef05c9b4":"#now we need to fill in the missing values in the Embarked feature\nprint(\"Number of people embarking in Southampton (S):\")\nsouthampton = train_data[train_data[\"Embarked\"] == \"S\"].shape[0]\nprint(southampton)\n\nprint(\"Number of people embarking in Cherbourg (C):\")\ncherbourg = train_data[train_data[\"Embarked\"] == \"C\"].shape[0]\nprint(cherbourg)\n\nprint(\"Number of people embarking in Queenstown (Q):\")\nqueenstown = train_data[train_data[\"Embarked\"] == \"Q\"].shape[0]\nprint(queenstown)","178be321":"#replacing the missing values in the Embarked feature with S\nfreq_port = train_data.Embarked.dropna().mode()[0]\nfreq_port\n","413ad112":"train_data['Embarked'] = train_data['Embarked'].fillna(freq_port)\ntrain_data[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","92768da4":"#map each Embarked value to a numerical value\n\ntrain_data['Embarked'] = train_data['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\ntrain_data.head()","ff04a8ba":"test_data['Fare'].fillna(test_data['Fare'].dropna().median(), inplace=True)\ntest_data.head()","0ab423bb":"#check test data \ntest_data.head()","1e1ea156":"#check test data \ntrain_data.head()","832c0127":"#Drop the unuseful column \ntrain_data = train_data.drop(['Name'], axis = 1)\ntest_data = test_data.drop(['Name'], axis = 1)\n\ntrain_data = train_data.drop(['Cabin'], axis = 1)\ntest_data = test_data.drop(['Cabin'], axis = 1)\n\ntrain_data = train_data.drop(['Ticket'], axis = 1)\ntest_data = test_data.drop(['Ticket'], axis = 1)\n\ntrain_data = train_data.drop(['AgeGroup'], axis = 1)\ntest_data = test_data.drop(['AgeGroup'], axis = 1)\n\ntrain_data = train_data.drop(['Embarked'], axis = 1)\ntest_data = test_data.drop(['Embarked'], axis = 1)\n\ntrain_data = train_data.drop(['Fare'], axis = 1)\ntest_data = test_data.drop(['Fare'], axis = 1)\n\n","29c96651":"#check test data \ntest_data.head()","84f51516":"\nfrom sklearn.model_selection import train_test_split\n\npredictors = train_data.drop(['Survived', 'PassengerId'], axis=1)\ntarget = train_data[\"Survived\"]\nx_train_data, x_val, y_train_data, y_val = train_test_split(predictors, target, test_size = 0.22, random_state = 0)","34855966":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\ngaussian = GaussianNB()\ngaussian.fit(x_train_data, y_train_data)\ny_pred = gaussian.predict(x_val)\nacc_gaussian = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gaussian)","e161d5b4":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(x_train_data, y_train_data)\ny_pred = logreg.predict(x_val)\nacc_logreg = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_logreg)","1ffd1098":"# Support Vector Machines\nfrom sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(x_train_data, y_train_data)\ny_pred = svc.predict(x_val)\nacc_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_svc)","6381e30a":"# Linear SVC\nfrom sklearn.svm import LinearSVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(x_train_data, y_train_data)\ny_pred = linear_svc.predict(x_val)\nacc_linear_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_linear_svc)","e534fe0f":"# Perceptron\nfrom sklearn.linear_model import Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(x_train_data, y_train_data)\ny_pred = perceptron.predict(x_val)\nacc_perceptron = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_perceptron)","3b4d8bc8":"#Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecisiontree = DecisionTreeClassifier()\ndecisiontree.fit(x_train_data, y_train_data)\ny_pred = decisiontree.predict(x_val)\nacc_decisiontree = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_decisiontree)","038f997d":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandomforest = RandomForestClassifier()\nrandomforest.fit(x_train_data, y_train_data)\ny_pred = randomforest.predict(x_val)\nacc_randomforest = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_randomforest)","3f98f921":"# KNN or k-Nearest Neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(x_train_data, y_train_data)\ny_pred = knn.predict(x_val)\nacc_knn = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_knn)","15db2315":"# Stochastic Gradient Descent\nfrom sklearn.linear_model import SGDClassifier\n\nsgd = SGDClassifier()\nsgd.fit(x_train_data, y_train_data)\ny_pred = sgd.predict(x_val)\nacc_sgd = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_sgd)","4f112fc8":"# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngbk = GradientBoostingClassifier()\ngbk.fit(x_train_data, y_train_data)\ny_pred = gbk.predict(x_val)\nacc_gbk = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gbk)","d5555a16":"models = pd.DataFrame({\n    'Model Name': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', 'Linear SVC', \n              'Decision Tree', 'Stochastic Gradient Descent', 'Gradient Boosting Classifier'],\n    'Score': [acc_svc, acc_knn, acc_logreg, \n              acc_randomforest, acc_gaussian, acc_perceptron,acc_linear_svc, acc_decisiontree,\n              acc_sgd, acc_gbk]})\nmodels.sort_values(by='Score', ascending=False)","c088b606":"# ids as PassengerId and predict survival \nids = test_data['PassengerId']\npredictions = gbk.predict(test_data.drop('PassengerId', axis=1))\n\n#set the output as a dataframe and convert it into csv file submission.csv\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\noutput.to_csv('submission.csv', index=False)","2829c879":"# Import the required Python libraries \n","4a171b0a":"**Support Vector Machines Model**","d4a2b16f":"** some of what we learned from the Titanic disaster data analytics:**\n\n\n* Number of Passengers with Ticket Class 1 Survived >  Class 2 and Class 3\n\n* % of female who survived (0.1889081455805892) > % of male who survived (0.18890814558058924)\n\n* Number of Male passengers (male: 577) > females (female: 314)\n","e8c166ee":"**Compare and sort all Models accouirding to score**","0be1aa81":"# Creating Submission File\n\ncreate a submission.csv file to upload to the Kaggle competition!","fb6ab003":"**Perceptron Model**","ccc30b4d":"Some Values Variable Analysis:\n","d55863c0":"**Gradient Boosting Classifier Model**","6b0eeb31":"# Deep analysis\n#Get number of Survived \/ Died passangers grouped by sex","1ef6dba0":"# Another way to Find Null valus using info()\nTotal valus should be ","cdaf6bf8":"**Decision Tree Model**","c13adaa7":"# Load Data from .csv files and make it ready for use","b68a7b4b":"1. **Import the required Python libraries**\n2. **Load and check data (Data Exploration)**\n3. **Data analysis, Visualization and histograms**\n4. **Cleaning Data**\n5. **Choosing the Best Model**\n6. **Creating Submission File and Submit**\n\n","becc5c5d":"After appling describe() The count variable row shows that thers is a 891-714= 177 values are missing in the Age column..also hters is missing values at Cabin and Port number","994e52dc":"**After appling info() thers is a 891-714= 177 \nvalues are missing in the Age column..also hters\nis missing values at Cabin and Port number**","53690626":"it is very Good to use and apply **value_counts()**  on columns and match the result with histogram\nbellow am counting how many tickets classes are there and number of pasengers with each ticket class","0689a511":"# Find Null valus in test and train using describe()\n\ncount value shuold be 891, any column withh less valus means there is null cells\nfor example count for \nage is 714. 891-714= 177 null valus in Age column","014ae37a":"# Introduction:\n\nThis is my first competition, I have choosed the **Titanic** disaster competition as recomennded by one of my firends to start practicing the data science and machine learning on Kaggle Platform. \n","69770cd9":"Please consider upvoting if you find it useful to you.\n\nThanks.","733c3307":"**Stochastic Gradient Descent Model**","d1d71292":"> Number of Passengers with Ticket Class 1 Survived: 136 ! (High rate)\n\n> Number of Passengers with Ticket Class 2 Survived: 87\n\n> Number of Passengers with Ticket Class 3 Survived: 119\n\n","9dc02ee4":"**KNN or k-Nearest Neighbors Model**","d51cd8d7":"The Cabin is missing approximately 77.1% of its values.","7b9689ab":"**Competition Steps:**","6b7cd243":"# Handle Age Emapty values and Mapping\n","0e311e60":"it look like that Gradient Boosting Classifier model is the best model for this case","abef22cb":"# **Predictors**","b3ce1385":"# Important Notes:\n\n\n**train.csv - Traning data set - (891 passengers \/ recored)**\n\ncontains the details of a subset of the passengers on board starting from passenger ID 1 till 891 out of 1308 Passenger\n\n**test.csv - Test data set - (417 passengers \/ recored)** \n\nUsing the patterns you find in train.csv, you have to predict whether the other 418 passengers on board (in test.csv) survived.\n\n\n**Fields (you can find on data tab):**\n\n**Survived** = Survived (0 = No, 1 = Yes)\n\n\"1\", the passenger survived. \/ \"0\", the passenger died.\n\n**Pclass** = Ticket class (1 = 1st, 2 = 2nd, 3 = 3rd)\n\n**Parch** = # of parents \/ children aboard the Titanic\n\n**SibSp** = # of siblings \/ spouses aboard the Titanic\n\n**cabin** = Cabin number\n\n**Embarked** = Port of Embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)\n","8f19a8ef":"**Linear SVC Model**","157359e8":"* Gaussian Naive Bayes\n* Logistic Regression\n* Support Vector Machines\n* Perceptron\n* Decision Tree Classifier\n* Random Forest Classifier\n* KNN or k-Nearest Neighbors\n* Stochastic Gradient Descent\n* Gradient Boosting Classifier","b5c06c74":"# Passengers Ticket Class, Sex, Age Count and Histogram","1289a5e1":"# Choosing the Best Model: ","6fe3a47e":"** Gaussian Naive Bayes Model**","320a8fe7":"**Logistic Regression Model**","8726a901":"Random Forest Model:"}}