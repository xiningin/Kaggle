{"cell_type":{"b4eb0624":"code","2c6465e6":"code","8d859041":"code","df36bae1":"code","7e12c096":"code","115bc408":"code","3038bb98":"code","80fa13fa":"code","37ffd14b":"code","8e27770f":"code","e0afac0b":"code","ce0b734c":"code","41e5f40e":"code","97686e18":"code","013c888e":"code","6de9ce79":"code","3afbde0b":"code","f3c99bdf":"code","0ec6a3d9":"code","4f0b752e":"code","9e765c88":"code","36b3b0a2":"code","c44e2bac":"code","78eec087":"code","554d24a6":"code","5a8f4510":"code","92242a6a":"code","2099ba90":"code","843181f4":"code","8ba46fe4":"code","3afdc18f":"code","33a95229":"code","d74ad5cf":"code","48968e63":"code","99ab6607":"code","12c0f48f":"code","9275b571":"code","76312acf":"code","9822bd34":"code","a698e2e6":"code","3719cda8":"code","039a657c":"markdown","50a6366c":"markdown","8eb2f60a":"markdown","161f510c":"markdown","4cab80dd":"markdown","8266080a":"markdown","8c1abf55":"markdown","6cfcf50a":"markdown","15c032d3":"markdown","8e66de9d":"markdown","e4a38cf3":"markdown","a497afdb":"markdown","e18d2fbd":"markdown","41f49d94":"markdown","db977aea":"markdown","b57b26d4":"markdown","47eb5720":"markdown","4304fd89":"markdown","06831a47":"markdown","b64ce026":"markdown","cfa46864":"markdown","d61bb016":"markdown","350f246b":"markdown","d5553e2d":"markdown","1f3f8a92":"markdown","a48356b6":"markdown","76bf17cd":"markdown","03109711":"markdown","087ed4ae":"markdown","bf1de872":"markdown"},"source":{"b4eb0624":"import os\nimport io\nimport re\nimport sys\nimport glob\nimport json\nimport string\nimport requests\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize, sent_tokenize\n\nstemmer = PorterStemmer()\nroot = '..\/input\/CORD-19-research-challenge\/'\nstop_words = list(set(stopwords.words('english')))\nstop_words.extend(['within', 'what', 'how', 'eg', 'ie'])","2c6465e6":"class PaperLoader():\n    \"\"\"\n    Loads, parses and merges metadata for papers\n    \"\"\"\n    \n    def __init__(self, root_dir, no_bib=True):\n        \"\"\"\n        Initializes PaperLoader class to read all .json files from root_directory\n            \n            no_bib: if true, clean noisy sections with bibliographies\n            root_dir: root directory for papers\n        \"\"\"\n        self.ROOT_DIR = root_dir\n        self.JSON_FILES = glob.glob(f'{root}\/**\/*.json', recursive=True)\n        self.PAPERS_COLUMN = {\n            \"doc_id\": [None],\n            \"title\": [None],\n            \"abstract\": [None],\n            \"text_body\": [None]\n        }\n        self.PAPERS_DF = None\n        self.NO_BIB = no_bib\n\n    \n    def __clean_bib(self, body_text, thres):\n        \"\"\"\n        Removes sections with more than 5 URL\/DOI\/HTTP instances\n            \n            body_text: array of dictionaries for text_body\n            thres: number of hyperlinks tolerated before removal \n        \"\"\"\n        # Sometimes, the text body has duplicate sections consecutively.\n        merged_body = []\n        for segment in body_text:\n            # We will combine these duplicate sections\n            if len(merged_body) > 0:\n                if merged_body[-1]['section'] == segment['section']:\n                    merged_body[-1]['text'] += '\\n' + segment['text']\n                    continue\n            merged_body.append(segment)\n\n        merged_body = [\n            segment for segment in merged_body\n            if len(re.findall(\"(http|doi|www)\", segment['text'])) <= thres\n        ]\n        return merged_body\n\n\n    def create_paper_df(self):\n        \"\"\"\n        Creates a Pandas DataFrame from all json files in root_directory\n        Each json file represents a paper. \n        Features extracted are: doc_id, title, abstract, text_body\n        \"\"\"\n        df_list = []\n        \n        for i in tqdm(range(len(self.JSON_FILES))):\n            file_name = self.JSON_FILES[i]\n            \n            #Initialize row for returned df. Each row represents a paper\n            row = {x: None for x in self.PAPERS_COLUMN}\n\n            with open(file_name) as json_data:\n                data = json.load(json_data)\n\n                row['doc_id'] = data['paper_id']\n                row['title'] = data['metadata']['title']\n                \n                # If title is empty, we skip the paper\n                if len(row['title']) <= 2:\n                    continue\n\n                # If a paper does not have an abstract of a body, we will skip it\n                if ('abstract' not in data or 'body_text' not in data):\n                    continue\n                else:\n                    # Now need all of the abstract. Put it all in\n                    # a list then use str.join() \n                    abstract_list = [abst['text'] for abst in data['abstract']]\n                    abstract = \"\\n \".join(abstract_list)\n\n                # Skip the paper if abstract is empty\n                if len(abstract) <= 2:\n                    continue\n\n                row['abstract'] = abstract\n\n                # And lastly the body of the text.\n                # These clauses check if the user wants to clean up references\n                if self.NO_BIB:\n                    body_list = self.__clean_bib(data['body_text'], 4)\n                else:\n                    body_list = [bt for bt in data['body_text']]\n\n                row['text_body'] = body_list\n\n                df_list.append(row)\n        # create final dataframe\n        self.PAPERS_DF = pd.DataFrame(df_list)\n\n\n    def merge_metadata(self, metadata = 'metadata.csv'):\n        \"\"\"\n            Joins paper information with information on journal for paper,\n            authors, doi and published date  \n                metadata: path to csv file containing metadata\n        \"\"\"\n        metadata_df = pd.read_csv(self.ROOT_DIR + metadata)\n        metadata_df = metadata_df.loc[:, \n                          ['sha', 'publish_time', 'authors', 'journal', 'doi']]\n        self.PAPERS_DF = self.PAPERS_DF.merge(metadata_df,\n                                              left_on='doc_id',\n                                              right_on='sha',\n                                              how='inner')\n\n\n    def get_df(self):\n        \"\"\"\n        Returns processed dataframe\n        \"\"\"\n        self.PAPERS_DF = self.PAPERS_DF.dropna(\n            subset=['abstract', 'text_body'])\n        return self.PAPERS_DF","8d859041":"paper_loader = PaperLoader(root)\npaper_loader.create_paper_df()\npaper_loader.merge_metadata()\npapers_df = paper_loader.get_df()","df36bae1":"papers_df.head(2)","7e12c096":"papers_df.shape","115bc408":"# List of keywords for covid-19\ncov_list = [\n    'novel coronavi',\n    'covid',\n    'cov_2',\n    'cord-19',\n    'cord 19',\n    '2019-nCoV',\n    '2019 ncov',\n    '2019 cov',\n    'wuhan coronavi',\n]","3038bb98":"class RelevantFilter():\n    \n    def __init__(self, keywords, year='2019'):\n        \"\"\"\n        constructor for RelevantFilter\n            keywords: keywords to filter for\n            year: papers written before this year will be discarded\n        \"\"\"\n        self.KEYWORDS = keywords\n        self.YEAR = year\n\n    def extract_recent(self, df):\n        \"\"\"\n        extracts documents published on or after self.YEAR\n        \"\"\"\n        return df[df['publish_time'] >= self.YEAR]\n\n    def filter_papers(self, df):\n        \"\"\"\n        Filters for papers whose title have mention of \n        any of the terms in self.KEYWORDS\n        \"\"\"\n        pattern = re.compile('(' + \"|\".join(self.KEYWORDS) + ')',\n                                 re.IGNORECASE)\n        # We will filter for rows with one or more matches \n        # for title and covid keywords\n        df = df[df['title'].apply(lambda x: \n                                  len(pattern.findall(x)) >= 1\n                                  if x else False)]\n        \n        return df","80fa13fa":"covid_filter = RelevantFilter(cov_list, '2019')\ncovid_df = covid_filter.filter_papers(papers_df)\ncovid_df = covid_filter.extract_recent(covid_df)","37ffd14b":"covid_df.shape","8e27770f":"covid_df.head(1)","e0afac0b":"list(covid_df.head(2)['title'].values)","ce0b734c":"risk_factors = [{\n    'name': 'smoking',\n    'pattern': '(smoki|smoker|cigar|nicotine|cannabis|marijuana)'\n}, {\n    'name': 'diabetes',\n    'pattern': '(diabet|insulin|blood sugar|blood glucose|ketoacidosis|hyperglycemi)'\n}, {\n    'name': 'pregnancy',\n    'pattern': 'pregnan'\n}, {\n    'name': 'tuberculosis',\n    'pattern': '(tubercul|mtb|\\btb[A-Za-z0-9]\\b)'\n}, {\n    'name': 'hypertension',\n    'pattern': '(hypertension|blood pressure|hbp)'\n}, {\n    'name': 'cancer',\n    'pattern': 'cancer'\n}, {\n    'name': 'neonates',\n    'pattern': '(baby|neonate|enfant)'\n},\n    {\n    'name': 'liver disease',\n    'pattern': 'liver disease'\n},{\n    'name': 'COPD',\n    'pattern': 'COPD'\n},{\n    'name': 'pulmonary disease',\n    'pattern': 'pulm'\n},{\n    'name': 'race\/ethnicity',\n    'pattern': 'ethn'\n}]","41e5f40e":"design_list = [\n    'mathemat', 'profil', 'cross sectional case control',\n    'matched case control', 'contact', 'surviv', 'time to event',\n    'time-to-event', 'risk factor analysis', 'logistic regression',\n    'cross-sectional case-control', 'matched case-control',\n    'observational case series', 'time series analysis', 'survival analysis',\n    'investigati', 'model', 'outbreak', 'stochast', 'statist', 'analysi',\n    'experiment', 'excret', 'investig',\n    'retrospective cohort', 'cross-sectional case-control',\n    'cross sectional case control', 'prevalence survey', 'systematic review ',\n    'meta-analysis', 'meta analysis', 'matched case-control',\n    'matched case control', 'medical record review',\n    'observational case series', 'time series analysis',\n    'pseudo-randomized controlled', 'pseudo randomized controlled',\n    'randomized controlled', 'retrospective analysis', 'retrospective study',\n    'retrospective studies'\n]","97686e18":"outcome_list = [\n    'risk', 'range', 'duration', 'asymptomatic', 'infecti', 'reproducti',\n    'route', 'age', 'transmm'\n    'stratifi', 'period,', 'health', 'r0', 'shedd', 'viral'\n    'period', 'incub', 'generat', 'factor', 'interval,', 'serial'\n]","013c888e":"fatality_list = ['icu', 'fatal', 'death', 'die', 'dead', 'dying', 'mortal']","6de9ce79":"evaluation_weights= {\n    'risk': 5,\n    'design': 0.5,\n    'outcome': 0.5,\n    'fatality': 0.5,\n    'section': 0.5,\n    'inverse_length': 1000\n}","3afbde0b":"def rank_design(design_keyword):\n    \"\"\"\n    This helper function ranks study designs. So far we have\n    confirmed rankings for only three study designs, but this\n    data will be expanded and improved further with time as we \n    speak to more epidimiologists\n    \"\"\"\n    design_rankings = {\n        'meta': 10,\n        'random': 8,\n        'pseudo': 6,\n    }\n    current_ranking = -1\n    for key in design_rankings.keys():\n        if key in design_keyword.lower():\n            current_ranking = min(current_ranking, design_rankings[key])\n    \n    if current_ranking == -1:\n        current_ranking = 4\n    return current_ranking    \n\ndef flatten(arr):\n    \"\"\"\n    Returns a single flat list from a list of lists\n    \"\"\"\n    return [item \n            for sublist in arr \n            for item in sublist]","f3c99bdf":"class PaperAnalyzer():    \n    \"\"\"\n    Takes in a dataframe of papers and sets it up for analysis\n    \"\"\"\n    # Setting up static constants\n    DEFAULT_RISKS = risk_factors\n    DEFAULT_DESIGNS = design_list\n    DEFAULT_OUTCOMES = outcome_list\n    DEFAULT_FATAL = fatality_list\n    DEFAULT_WEIGHTS = evaluation_weights\n    \n    def __init__(self, parent_df, weights = None):\n        \"\"\"\n        Explodes the passed dataframe on sections for more granular analysis\n        Sets up ranks to be updated later by methods. Client can supply their\n        own dictionary of weights for different features.\n        \"\"\"\n        # Section ratings\n        self.section_ratings = {\n                        'discus': 10,\n                        'concl': 10,\n                        'resul': 10,\n                        'analy': 9,\n                        'impli': 9,\n                        'valu': 9,\n                        'intro': 6\n                        }\n        # Extracting full text from array of dicts in \"text_body\"\n        parent_df['full_text'] = parent_df['text_body'].apply(lambda x: \n                                                              '\\n'.join([sec['text'] for sec in x]))\n        \n        # Exploding \"text_body\" into individual dicts, each representing a section\n        self.df = parent_df.explode('text_body')\n        # Extracting section headers\n        self.df['section'] = self.df['text_body'].apply(lambda x: \n                                                        x['section'] \n                                                        if type(x) == dict \n                                                        else None)\n        # Extracting section text\n        self.df['text_body'] = self.df['text_body'].apply(lambda x:\n                                                          x['text'] \n                                                          if type(x) == dict \n                                                          else None)\n        # Dropping rows where section text is empty\n        self.df = self.df[self.df['text_body'].notna()]\n        # Initializing total rank\n        self.df['total_rank'] = 0\n        # If manual weights have been provided, then updating weights\n        if weights:\n            self.weights = weights\n        else:\n            self.weights = PaperAnalyzer.DEFAULT_WEIGHTS\n        # TQDM is used for progress bars\n        tqdm.pandas()\n\n    def analyze_risks(self, risk_factors):\n        \"\"\"\n        Analyses papers in self.df for risk factors and returns a report df\n        with columns has_{risk_factor}?, {risk_factor}_count, \n        {risk_factor}_in_title and updates {total_rank} for each row.\n        The match_indices column is produced for ease of visualization\n        in the web app.\n        \"\"\"\n        # Fallback to default risks if risk factors haven't been provided\n        if risk_factors == None:\n            risk_factors = PaperAnalyzer.DEFAULT_RISKS\n        \n        # This clause accomodates for both risk factor strings and \n        # risk factor dicts with names and regex patterns\n        if type(risk_factors[0]) == dict:\n            patterns = [risk['pattern'] for risk in risk_factors]\n        elif type(risk_factors[0]) == str:\n            patterns = [risk for risk in risk_factors]\n        \n        # Dropping excerpts that do not contain info about risk factors\n        self.df = self.df[self.df['text_body'].apply(lambda x:\n                                                    any(re.compile(pattern, re.IGNORECASE).findall(x)\n                                                       for pattern in patterns)\n                                                    )]\n        \n        # These columns will have the risk factors and \n        # their locations in text for the web app\n        self.df['risk_factors'] = [[]] * len(self.df)\n        self.df['match_indices'] = [[]] * len(self.df)\n        \n        \n        for i in tqdm(range(len(risk_factors))):\n            factor = risk_factors[i]\n            # Accomodating for variation in risk factor input\n            if type(factor) == dict:\n                name = factor['name']\n                pattern = re.compile(factor['pattern'], re.IGNORECASE)\n            elif type(factor) == str:\n                name = factor\n                pattern = re.compile(factor, re.IGNORECASE)\n    \n            # Temporary column that stores match data\n            # (matched string and start index) for risk factors\n            self.df['_matches'] = self.df['text_body'].apply(lambda x: \n                                                                      [(m.start(), m.group()) \n                                                                       for m in pattern.finditer(x)])\n            # Count the number of occurances for risk factor\n            self.df[name + '_count'] = self.df['_matches'].apply(lambda x: len(x))\n            # Boolean to indicate if excerpt has the risk factor\n            self.df['has_' + name + '?'] = self.df[name + '_count'].apply(lambda x: x > 0)\n            # Boolean to indicate if paper title has the risk factor\n            self.df[name + '_in_title'] = self.df['title'].apply(lambda x:\n                                                                         len(pattern.findall(x)) > 0)\n            \n            # Update total rank\n            self.df['total_rank'] += self.weights['risk'] * self.df[name + '_count']\n            \n            # Append risk factors list\n            self.df['risk_factors'] = self.df.apply(lambda x: \n                                    x['risk_factors'] + [name] if x['has_' + name + '?']\n                                    else x['risk_factors'],\n                                   axis=1)\n            # Append start indices for matches\n            self.df['match_indices'] = self.df.apply(lambda x: \n                                    x['match_indices'] + [n[0] for n in x['_matches']] if x['has_' + name + '?']\n                                    else x['match_indices'],\n                                   axis=1)\n            # Drop temporary column\n            self.df.drop('_matches', axis=1, inplace=True)\n            \n\n    def analyze_designs(self, design_list):\n        \"\"\"\n        Analyses papers in self.df for study designs and returns a report df \n        with 'design' and 'design_rank'. 'design_rank' is decided upon from the \n        input in crowdsourced medical dictionary.\n        \"\"\"\n        # Fallback to default design_list if none provided\n        if design_list == None:\n            design_list = PaperAnalyzer.DEFAULT_DESIGNS\n        \n        # Extract design keywords in excerpt\n        self.df['design'] = self.df['text_body'].progress_apply(lambda x:\n                                                                      flatten([re.findall(des, x, re.IGNORECASE) \n                                                                       for des in design_list\n                                                                       if re.findall(des, x, re.IGNORECASE)\n                                                                      ]))\n        # Convert # of keywords into a ranking\n        self.df['design_rank'] = self.df['design'].apply(lambda x:\n                                                                len(x))\n        # Rate each study design for its quality\n        self.df['design_rank'] += self.df['design'].apply(lambda x: rank_design(' '.join(x)))\n        # Update total rank\n        self.df['total_rank'] += self.weights['design'] * self.df['design_rank']\n\n    def analyze_outcomes(self, outcomes):\n        \"\"\"\n        Analyses papers in self.df for outcomes and returns a report df \n        with 'outcomes' and 'outcome_rank'. 'outcome_rank' is decided upon \n        by the frequency of mentions of outcomes in the excerpt\n        \"\"\"\n        # Fallback to default outcomes if none provided\n        if outcomes == None:\n            outcomes = PaperAnalyzer.DEFAULT_OUTCOMES\n        # Extract outcome keywords in excerpts\n        self.df['outcomes'] = self.df['text_body'].progress_apply(lambda x:\n                                                                        flatten([re.findall(outc, x, re.IGNORECASE)\n                                                                         for outc in outcomes\n                                                                         if re.findall(outc, x, re.IGNORECASE)\n                                                                        ]))\n        # Convert # of keyword matches into ranking\n        self.df['outcome_rank'] = self.df['outcomes'].apply(lambda x: len(x))\n        # Update total rank\n        self.df['total_rank'] += self.weights['outcome']* self.df['outcome_rank']\n        \n    def analyze_fatality(self, fatality_list):\n        \"\"\"\n        Analyses papers in self.df for information on fatality \n        returns a report df with 'fatality_rank'. \n        'fatality_rank' is decided upon by the frequency of \n        mentions of fatality in the excerpt\n        \"\"\"\n        # Fallback to default outcomes if none provided\n        if fatality_list == None:\n            fatality_list = PaperAnalyzer.DEFAULT_FATAL\n        # Extract fatality keywords in excerpts\n        self.df['fatality_count'] = self.df['text_body'].progress_apply(lambda x:\n                                                                        len([re.findall(key, x, re.IGNORECASE)\n                                                                         for key in fatality_list]))\n        # Indicate whether an excerpt has info on fatality\n        self.df['fatal_info?'] = self.df['fatality_count'].apply(lambda x: x > 0)\n        # Update total ranking\n        self.df['total_rank'] += self.weights['fatality']* self.df['fatality_count']\n\n    def perform_analysis(self, risk_factors, design_list=None, outcomes=None, fatality_list = None):\n        \"\"\"\n        This function is a wrapper function that provides interface\n        to conduct analysis on all of risk factors, study designs and\n        outcomes. Users may specify their own design_list or outcomes. If not,\n        the default is used.\n        \"\"\"\n        print(\"Analyzing risks\")\n        self.analyze_risks(risk_factors)\n        print(\"Analyzing study designs\")\n        self.analyze_designs(design_list)\n        print(\"Analyzing outcomes\")\n        self.analyze_outcomes(outcomes)\n        print(\"Analyzing fatality\")\n        self.analyze_fatality(fatality_list)\n        print(\"Generating final rankings\")\n        self.generate_risk_rankings()\n\n        \n    def generate_risk_rankings(self):\n        \"\"\"\n        Appends columns in self.df that contain individual rankings for \n        each risk factor\n        \"\"\"\n        # Rank each excerpt based on what section it belongs to\n        self.df['section_rank'] = self.df['section'].apply(lambda x: self.section_ratings[x] \n                                                           if x in self.section_ratings else 5)\n        # Obtaining list of risk factors\n        risk_factors = [column for column in self.df.columns if 'has_' in column]\n        # Trim risk factors(has_smoking? to smoking)\n        risk_factors = [factor[4:-1] for factor in risk_factors]\n        \n        for i in tqdm(range(len(risk_factors))):\n            factor = risk_factors[i]\n            self.df[factor + '_rank'] = ( # The next few lines will aggregate rankings\n                            self.weights['risk']*self.df[factor + '_count'] + # No. of risk matches\n                            self.weights['section']*self.df['section_rank'] + # Section quality\n                            self.weights['design']*self.df['design_rank'] + # Study design quality\n                            self.weights['outcome']*self.df['outcome_rank'] + # No. of outcome matches\n                            self.weights['fatality']* self.df['fatality_count'] # No. of fatality matches\n            )\n            # Increase rank if paper contains the factor in title\n            self.df[factor + '_rank'] = self.df.apply(lambda x: x[factor + '_rank'] + 60\n                                                         if x[factor + '_in_title']\n                                                      else x[factor + '_rank'],\n                                                      axis=1\n                                                     )\n            # Normalizing risk rank for length of excerpts\n            self.df[factor + '_rank'] = self.df.apply(lambda x: x[factor + '_rank'] + \n                                                        (self.weights['inverse_length']\/\n                                                         (len(word_tokenize(x['text_body'])))), \n                                              axis=1)\n        \n        # This will be used to sort papers when no filters applied\n        self.df['max_rank'] = self.df[[factor + '_rank' for factor in risk_factors]].max(axis=1)\n        self.df['total_rank'] += self.weights['section'] * self.df['section_rank']\n        \n        # Normalizing total rank for length of excerpts\n        self.df['total_rank'] = self.df.apply(lambda x: x['total_rank'] + \n                                                        (self.weights['inverse_length']\/\n                                                         (len(word_tokenize(x['text_body'])))), \n                                              axis=1)\n    \n    def get_df(self, risk_factor=None):\n        \"\"\"\n        Applies section ratings, updates total ratings and returns reporting df\n            risk_factor: if specified, the returned df will only have excerpts\n                            that mention this risk factor\n        \"\"\"\n        if risk_factor:\n            if not self.__ANALYZED_RISKS:\n                raise ValueError(self.__ERROR_MESSAGE)\n            return self.df[self.df['has_' + risk_factor + '?'] == True]\n        return self.df","0ec6a3d9":"covid_analysis = PaperAnalyzer(covid_df)","4f0b752e":"covid_analysis.analyze_risks(risk_factors)\ncovid_analysis.analyze_designs(design_list)\ncovid_analysis.analyze_outcomes(outcome_list)\ncovid_analysis.analyze_fatality(fatality_list)\ncovid_analysis.generate_risk_rankings()","9e765c88":"risk_df = covid_analysis.get_df()\nrisk_df.shape","36b3b0a2":"diabetes_df = risk_df.sort_values(by='diabetes_rank', ascending=False).reset_index(drop=True)\n#temp['design']\nprint(diabetes_df.loc[:,['title','text_body','diabetes_rank']].iloc[0]['text_body'])","c44e2bac":"# There are some papers that appear more than once. This is because we rank excerpts within papers, and could simply mean that two different excerpts within the same paper were ranked high in relevance by our algorithm.\n\nsmoking_df = risk_df.sort_values(by='smoking_rank', ascending=False)\\\n            .loc[:,['title','doi','publish_time','design','text_body']]\\\n            .reset_index(drop=True).rename(columns={'text_body':'excerpt'})\nsmoking_df.head()","78eec087":"# Here is a title-deduplicated version of the previous dataframe\n# Each paper will only appear once with its highest rated excerpt\nsmoking_df.drop_duplicates('title', keep='first').reset_index(drop=True).head()","554d24a6":"diabetes_df = risk_df.sort_values(by='diabetes_rank', ascending=False)\\\n                .loc[:,['title','doi','publish_time','design','text_body']]\\\n                .reset_index(drop=True).rename(columns={'text_body':'excerpt'})\n# Each paper will only appear once with its highest rated excerpt\ndiabetes_df.drop_duplicates('title', keep='first').reset_index(drop=True).head()","5a8f4510":"pregnancy_df = risk_df.sort_values(by='pregnancy_rank', ascending=False)\\\n                .loc[:,['title','doi','publish_time','design','text_body']]\\\n                .reset_index(drop=True).rename(columns={'text_body':'excerpt'})\n# Each paper will only appear once with its highest rated excerpt\ndiabetes_df.drop_duplicates('title', keep='first').reset_index(drop=True).head()","92242a6a":"hypertension_df = risk_df.sort_values(by='hypertension_rank', ascending=False)\\\n                    .loc[:,['title','doi','publish_time','design','text_body']]\\\n                    .reset_index(drop=True).rename(columns={'text_body':'excerpt'})\n# Each paper will only appear once with its highest rated excerpt\nhypertension_df.drop_duplicates('title', keep='first').reset_index(drop=True).head()","2099ba90":"tuberculosis_df = risk_df.sort_values(by='tuberculosis_rank', ascending=False)\\\n                .loc[:,['title','doi','publish_time','design','text_body']]\\\n                .reset_index(drop=True).rename(columns={'text_body':'excerpt'})\n# Each paper will only appear once with its highest rated excerpt\ntuberculosis_df.drop_duplicates('title', keep='first').reset_index(drop=True).head()","843181f4":"#smoking_df.to_csv('smoking_risk_analysis.csv')\n#tuberculosis_df.to_csv('smoking_risk_analysis.csv')\n#hypertension_df.to_csv('smoking_risk_analysis.csv')\n#pregnancy_df.to_csv('smoking_risk_analysis.csv')\n#diabetes_df.to_csv('smoking_risk_analysis.csv')","8ba46fe4":"# !pip install transformers\n# !pip install bert-extractive-summarizer","3afdc18f":"# import numpy as np\n# import pandas as pd\n# from transformers import *\n# from tqdm.notebook import tqdm\n# from summarizer import Summarizer\n\n# # Loading the Scibert Model\n# scibert_link = 'allenai\/scibert_scivocab_uncased'\n# sci_config = AutoConfig.from_pretrained(scibert_link)\n# sci_config.output_hidden_states = True\n# sci_tokenizer = AutoTokenizer.from_pretrained(scibert_link)\n# sci_model = AutoModel.from_pretrained(scibert_link, config = sci_config)\n\n# # Setting up Bert-Extractive-Summarizer to use SciBert\n# sci_model = Summarizer(custom_model=sci_model,custom_tokenizer=sci_tokenizer)\n\n# # Extractive unique papers from dataframe\n# covid_papers = list(set(risk_df['doc_id'].values))\n\n# # Summarizing full text of papers\n# paper_summary_map = {}\n# for i in tqdm(range(len(covid_papers))):\n#   paper = covid_papers[i]\n#   text = risk_df[risk_df['doc_id'] == paper].iloc[0]['full_text']\n#   # Bert-Extractive-Summarizer max-limit exceeded\n#   if len(text) >= 1000000:\n#     paper_summary_map[paper] = \"Too long\"\n#     continue\n#   ratio = 0.20\n#   summary = sci_model(text, ratio = ratio)\n#   paper_summary_map[paper] = summary\n\n# # Adding summaries to original DataFrame\n# risk_df['scibert_summary'] = risk_df['doc_id'].apply(\n#                                           lambda x: paper_summary_map[x] \n#                                           if x in paper_summary_map \n#                                               else \"Not yet extracted\"\n#                                           )","33a95229":"risk_df.to_json(\"..\/..\/risk_df.json\", orient='records')\nrisk_df.to_csv(\"..\/..\/risk_df.csv\")","d74ad5cf":"class Question():\n    \"\"\"\n    The purpose of this class is to resolve a question for \n    keyword searching\n    \"\"\"\n    def __init__(self, question, design_list=None, outcomes=None):\n        \"\"\"\n        The constuctor does most of the method-calling for question resolution\n        \"\"\"\n        self.DESIGN_LIST = design_list\n        self.OUTCOMES = outcomes\n        self.RISK = question\n        self.risk_factors = None\n        self.design_list = None\n        self.outcome_list = None\n        self.__resolve_question()\n        if design_list:\n            self.__resolve_design()\n        if outcomes:\n            self.__resolve_outcomes()\n\n    def __question_tokenize(self, sent):\n        \"\"\"\n        Cleans the question string\n        \"\"\"\n        abbvr_pattern = re.compile('(e.g.|i.e.)')\n        sent = abbvr_pattern.sub('', sent)\n        remove_punct_dict = {key: \" \" for key in string.punctuation}\n        remove_punct_dict['.'] = ''\n        remove_punct = str.maketrans(remove_punct_dict)\n        sent = sent.translate(remove_punct)\n        return sent.replace('R', 'R0').replace('-', ' ')\n\n    def __resolve_question(self):\n        \"\"\"\n        stems and removes irreleavnt words from questions\n        to create keywords for keyword analysis\n        \"\"\"\n        subquestion = self.RISK\n        sub_q = self.__question_tokenize(subquestion)\n        keywords = set([\n            stemmer.stem(word) for word in word_tokenize(sub_q)\n            if word.lower() not in stop_words and 'cov' not in word.lower()\n            and word.lower().islower()  #This checks and removes numbers\n        ])\n        self.risk_factors = list(keywords)\n\n    def __resolve_design(self):\n        \"\"\"\n        Resolves study designs to allow for study-design evaluation\n        \"\"\"\n        design_keys = self.DESIGN_LIST.split(\",\")\n        self.design_list = list(set(design_keys))\n\n    def __resolve_outcomes(self):\n        \"\"\"\n        Resolves outcomes to allow for outcome evaluation\n        \"\"\"\n        outcome_keys = self.__question_tokenize(self.OUTCOMES)\n        outcome_keys = set([\n            stemmer.stem(word) for word in word_tokenize(outcome_keys)\n            if word.lower() not in stop_words\n            and word.lower().islower()  #This checks and removes numbers\n        ])\n        self.outcome_list = list(outcome_keys)\n\n    def get_keywords(self):\n        \"\"\"\n        Returns keywords from earlier methods\n        \"\"\"\n        result = {'risk': None, 'design': None, 'outcome': None}\n        result['risk'] = self.risk_factors\n        if self.design_list:\n            result['design'] = self.design_list\n        if self.outcome_list:\n            result['outcome'] = self.outcome_list\n        return result","48968e63":"def analyze_question(df, question):\n    \"\"\"\n    Function to take in a Question instance and a \n    dataframe with covid-excerpts to perform \n    evaluation and rankings on information relevancy\n    \"\"\"\n    reference_df = PaperAnalyzer(df)\n    keys = question.get_keywords()\n    reference_df.perform_analysis(keys['risk'], keys['design'],\n                                  keys['outcome'])\n    return reference_df.get_df()","99ab6607":"def get_google_sheet(url, sheet_name):\n    response=requests.get(url=url)\n    sample_file = io.BytesIO(response.content)\n    df = pd.read_excel(sample_file, sheet_name = sheet_name)\n    return df\n\ndict_url = 'https:\/\/docs.google.com\/spreadsheets\/d\/1t2e3CHGxHJBiFgHeW0dfwtvCG4x0CDCzcTFX7yz9Z2E\/export?format=xlsx&id=1t2e3CHGxHJBiFgHeW0dfwtvCG4x0CDCzcTFX7yz9Z2E'\nquestions_df = get_google_sheet(dict_url, 'sub.question.matching')","12c0f48f":"questions_df.head(2)","9275b571":"# Picking a sample question for analysis\nques = questions_df.iloc[127]['Subquestion']\nques = sent_tokenize(ques)[0]\nques","76312acf":"# Designs recommended for sample question\ndes = questions_df.iloc[127]['Design.list']\ndes","9822bd34":"# Outcomes recommended for sample question\noutc = questions_df.iloc[127]['Outcome.list']\noutc","a698e2e6":"report_df = analyze_question(covid_df, Question(ques, des, outc))","3719cda8":"for i in range(5, 10):\n    print(report_df.sort_values(by='droplet_rank', ascending = False).iloc[i]['text_body'][:1000])\n    print(\"-----------------------------\",\"\\n\")","039a657c":"#### We will now look into the results of the risk factor extraction","50a6366c":"#### This marks the end of phase 1. The resulting dataframe will be stored as a json to be served by the web app.","8eb2f60a":"5 Best Ranked Excerpts for Smoking","161f510c":"## Data Parsing and Extraction","4cab80dd":"Let's look at the best excerpt identified for diabetes","8266080a":"We will now try to conduct analysis on a sample question about transmission and droplets to answer the second subtask in the Risk Factor Task of the Dataset","8c1abf55":"<a id=\"ackn\"><\/a>\n### Acknowledgement\nWe would like to thank Brian Romer (https:\/\/www.linkedin.com\/in\/brianromer\/) for his valuable help in designing the visualization as well as Serge Myroshnychenko (https:\/\/www.linkedin.com\/in\/smyroshnychenko\/) for his valuable feedback on the interface.","6cfcf50a":"### `PaperAnalyzer` class will take in a DataFrame of papers and then analyze each paper. \nThe analysis is done with with its `analyze_risks()`, `analyze_designs()` and `analyze_outcomes()` methods that will analyze the risk factors, designs and outcomes respectively for excerpts in the paper. Finally, the `get_df()` method will return a new DataFrame with rankings for relevancy of excerpts. The rankings also factor in the `section` of the paper that the excerpt is from, with sections like **discussion** or **results** that seem to have pertinent, concise information ranked higher. Furthermore, these rankings are also normalized by the lenght of the excerpts\n\n*Note: These rankings for sections were determined through our interviews with epidimiologists.*","15c032d3":"We will filter through `papers_df` to get only covid-19 related papers in `covid_df`","8e66de9d":"## Table of Contents:\n* [Team](#team)\n* [Task](#task)\n* [How we approach the challenge](#approach)\n* [Our hypotheses\/why](#hypotheses)\n* [How we solve it](#solve)\n* [First approach](#first)\n* [Second approach](#second)\n* [Pros and cons of our approach and platform](#pros)\n* [Acknowledgement](#ackn)\n* [Code](#code)\n* [Keyword Analysis (Phase 1)](#phase1)\n* [Extractive Summarizaiton](#extractive-summarization)\n* [Question Search (Phase 2)](#phase2)\n\n<a id=\"team\"><\/a>\n## Team:\n#### We are a team of three: a senior data scientist, a front-end senior software engineer and a data science intern. If you are looking to contact us, here is our contact information:\n- Maria Kamali: maria.kamali@thomsonreuters.com\n- Keshav Vardachari: keshav.varadachari@thomsonreuters.com\n- Nikhil Budathoki : nbudatho@uwaterloo.ca\n\n<a id=\"task\"><\/a>\n# Task: \n## What do we know about COVID-19 risk factors? \n\n<a id=\"approach\"><\/a>\n## Purpose: \nOur platform was built based on the following principles: \n- Empower users (health researchers) to conduct literature survey efficiently. \n- Adaptable to future needs and challenges of health researchers.  \n- Modularized to have the capability of being improved and polished in a short time and in parallel.  \n\n<a id=\"hypotheses\"><\/a>\n## Hypotheses\/why? \n\nAI can benefit researchers by extracting and visualizing information in the most efficient and relevant manner at scale.  In the absence of expert feedback and annotations, we have built\/provided a platform that extracts and ranks relevant info with potential for improvement in the future by taking advantage of expert annotation. We believe that there is value in extracting excerpts that provide context on the facts presented.\n\n<a id=\"solve\"><\/a>\n## Our Solution \n\nThe project was done in two slightly different approaches.  \nThe first approach focused on developing an end to end pipeline to address the first subtask which was: \n- Data on potential risks factors \n- Smoking, pre-existing pulmonary disease \n- Co-infections (determine whether co-existing respiratory\/viral infections make the virus more transmissible or virulent) and other co-    morbidities \n- Neonates and pregnant women \n- Socio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences. \n\nWe have developed a web app to visually accompany the analysis carried out for this task. You can check out the app in our project's [GitHub](https:\/\/github.com\/mahtablci1\/C19_txt_xtractr)\n\nThe second approach was focused on question information retrieval but isn\u2019t accompanied by with a web-app yet due to time constraint. \n\n<a id=\"first\"><\/a>\n## First approach has the following recipe: \nThe platform is designed to visualize snippets of relevant topics through the following process: \n- Merge different sources of the data\n- Extract all COVID19 related papers \n- Expand and process the list of keywords\n- Find excerpts of papers that include the keywords \n- Rank the excerpts\n- Create an extractive summary for each paper\n- Serve it to the web-app for visualization\n\n### The platform is running and live. You can check it out [here](https:\/\/nikhilbudathoki.github.io\/react-viz\/)\n\nHere is a screenshot of our platform:\n![Demo](https:\/\/github.com\/mahtablci1\/Kaggle\/blob\/master\/platform_sample.png?raw=true)\n\n<a id=\"second\"><\/a>\n## Second approach has the following recipe: \nThe platform is designed to rank snippets of relevant topics through the following process: \n\n- Merge different sources of the data\n- Extract all COVID 19 related paper s\n- Convert a question into set of keywords (The questions are extracted from the [medical dictionary](https:\/\/docs.google.com\/spreadsheets\/d\/1NoiAFJoydk3zuc-G0qqROarkhaGpfgbQhTVYhbYtLCM\/edit#gid=0) shared with Kaggle participants.\n- Expand the list of keywords\n- Find excerpts of papers that include the keywords \n- Rank the excerpts\n- Create an extractive summary for each paper\n\n\n<a id=\"pros\"><\/a>\n\n### Pros and Cons of our approach and platform: \n\n#### Pros: \n\n- The code base is highly modular and simple.    \n- The code is very well documented \n- The visualization resonates with researchers. We interviewed an expert in epidemiology, since that is our target user. He liked all  aspects of it including the extractive summaries of the papers. He also noted that using this platform could reduce the time taken for a literature survey, which normally takes 3 to 4 months, to less than a month.  \n- The platform is taking a high recall approach(keyword search). It sets a well structured baseline to build and expand on. \n- The platform also includes extractive sumamries(SciBert) for each paper. Researchers can go through the article in a fraction of the time it would take to read the full paper.\n- With a little bit of effort, we can convert this platform to an expert annotation platform where experts can interactively click on relevant sentences in the snippets to convert a completely unsupervised\u202fapproach to a supervised learning task. \n\n#### Cons: \n- The output wasn't reviewed and validated by a subject matter expert\n- Keyword search is not the most efficient search. Word embedding is known to be a better approach for text analysis but there wasn\u2019t enough time to implement and evaluate that. \n- Our ranking follows a simple approach, where an expert should decide on the metric for ranking the importance of a snippet. \n- The second approach is not coming with a visualization. \n- There may be more risk factors than what we currently searched for and an algorithm should extract the unknown risks too. \n- The platform should ideally allow users to query articles questions; the implementation for which is not yet complete.","e4a38cf3":"We will now parse the papers from our data(root) directory and store them in `papers_df`.","a497afdb":"<a id=\"code\"><\/a>\n# Code","e18d2fbd":"### `RelevantFilter` class will filter the dataframe from `PaperLoader` and filter for covid-19 papers published on 2019 or later. \nWe will need to supply a list of covid-related keywords to filter from to the `constructor`\n","41f49d94":"#### Helper functions for `PaperAnalyzer` ","db977aea":"### The `Question` class will decompose and resolve a question about risk factors.\nThe result will then be piped to an instance of `PaperAnalyzer` to conduct similar analysis. Users will be able to specify their own list of outcomes. If not specified, the default set of outcomes will be used.","b57b26d4":"5 Best Ranked Excerpts for Pregnancy","47eb5720":"## Requirements","4304fd89":"## Filtering for covid-19 related papers released after 2019\nThere is a lot of noise in this dataset due to information about other strains of coronavirus so we will select only the papers that are related to Covid-19. \n\nWhile the older papers may contain some important insight on the variance among the  different strains of coronavirus, for our purposes, we will only be looking at papers published on 2019 or later because that is when Covid-19 was first discovered in humans.","06831a47":"<a id=\"extractive-summarization\"><\/a>\n### Extractive Summarization\nOur analysis also includes extractive summaries for Covid-19 papers to allow researchers to scan through important information in a paper without spending the time to read the full article. This inclusion was very well received by an expert that we had interviewed for feedback on our platform. We use the [SciBert](https:\/\/github.com\/allenai\/scibert)\nmodel trained on papers from the corpus of [semanticscholar.org](https:\/\/semanticscholar.org)\n\nWe used Google Colab to summarize the papers and merged the keyword ranks with the summaries for our platform. The code for summarization is included below. ","b64ce026":"We will now go through the papers to extract and rank excerpts that contain relevant information about risk factors for covid-19. \nWe will do this through an analysis of:\n\n- Risk factors for covid-19\n- Study designs\n    - We will use this to evaluate the quality of a paper's methodologies for our rankings\n- Outcomes\n    - We will incentivise excerpts to explicitly mention outcomes that we have found researchers look for(in our interviews)\n- Fatality\n    - We have determined that information on mortality and fatality would be of high value to researcehrs, and rightly so.\n\n**Note**: The list of keywords were all obtained from a crowdsourced medical dictionary that researchers had assembled. You can find more details [here](https:\/\/docs.google.com\/spreadsheets\/d\/1t2e3CHGxHJBiFgHeW0dfwtvCG4x0CDCzcTFX7yz9Z2E\/edit#gid=1217643351)","cfa46864":"<a id=\"phase2\"><\/a>\n## Question Search (Phase 2)\nWe will extend the capabilities from the `PaperAnalyzer` class and attempt to answer some questions.","d61bb016":"5 Best Ranked Excerpts for Hypertension","350f246b":"5 Best Ranked Excerpts for Diabetes","d5553e2d":"5 Best Ranked Excerpts to Answer the Question: _(Is COVID-19 transmitted on droplets?)_","1f3f8a92":"We will be trying out the question answering pipeline now with a few questions from the aforementionned [medical dictionary](https:\/\/docs.google.com\/spreadsheets\/d\/1t2e3CHGxHJBiFgHeW0dfwtvCG4x0CDCzcTFX7yz9Z2E\/edit#gid=1217643351) that was available to Kaggle competitors.","a48356b6":"<a id=\"phase1\"><\/a>\n## Keyword Analysis (Phase 1)","76bf17cd":"### `PaperLoader` class will load all papers for the challenge and provide an interface for us to obtain `DataFrames` to work with. The focus will be on:\n- Obtaining Paper title, Abstract, Body\n    - The text body is filtered to remove sections containing lots of citations and hyperlinks\n- Obtaining Authors, Journal of Publication, Publication Date and Publication Date","03109711":"#### The next cell will contain the default coeffecients for the algorithm's prioritization of different features. These coeffecients are a work in progress and we seek to constantly improve them with more expert feedback.","087ed4ae":"5 Best Ranked Excerpts for Tuberculosis","bf1de872":"#### Keyword Lists"}}