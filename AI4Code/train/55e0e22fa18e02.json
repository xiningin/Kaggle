{"cell_type":{"2ee1509b":"code","e464fca0":"code","03d3391e":"code","ed86618b":"code","1013c70b":"code","2a5764c1":"code","9b6111d0":"code","4a68faab":"code","15d597fc":"code","10e402f5":"code","45aa37ab":"code","878b6710":"code","a9d8f483":"code","31fb2d27":"code","7e17d7f1":"code","1af10224":"code","306f2945":"code","c7fb7c9b":"code","6ae19ef8":"code","ebd03178":"code","aaf6bd46":"code","8c796e70":"code","b3c36d9a":"code","9a14a9bc":"code","5c2783d7":"code","dac1e40d":"code","7875abde":"code","15eecf67":"code","f3022bed":"code","e427923b":"code","6e1ba9e2":"code","bda913d4":"markdown","78f75c1f":"markdown","a08d5653":"markdown","c60e5f06":"markdown","1fde2b3a":"markdown","7a209ef7":"markdown","62836e8e":"markdown","930bb138":"markdown","ff1a0813":"markdown","3c98908c":"markdown","b27cbaf4":"markdown","a9cfc0a9":"markdown","b2f8dcc5":"markdown","d1613dfd":"markdown","bd800cae":"markdown","d15f0e44":"markdown","95316b22":"markdown","610325c3":"markdown","a821b534":"markdown","8b61c125":"markdown"},"source":{"2ee1509b":"!pip install ktrain","e464fca0":"import spacy\nfrom spacy import displacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\n\nimport string\nimport tensorflow as tf\nfrom transformers import AutoTokenizer, TFAutoModel\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\nimport re\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom nltk.util import ngrams\n\nfrom collections import defaultdict\nfrom collections import Counter\n\nimport ktrain # For Bert Model Implementation.\nfrom ktrain import text # Preprocessing text for the Bert Model.","03d3391e":"MODEL_DIR = \"\/kaggle\/input\/\"\n\n# Getting the train data\ndataset = pd.read_csv(MODEL_DIR + 'nlp-getting-started\/train.csv')\n\n# Getting the submission entries\ndf_sub = pd.read_csv(MODEL_DIR + 'nlp-getting-started\/test.csv')\n\n## Copying the dataset for analysis\ndf_analysis = dataset.copy()","ed86618b":"print(f'Training dataset shape: {df_analysis.shape}')\nprint(f'Testing dataset shape: {df_sub.shape}')","1013c70b":"df_analysis.head(3)","2a5764c1":"counter = np.array(\n    [\n        len(df_analysis[df_analysis['target'] == 1]),\n        len(df_analysis[df_analysis['target'] == 0])\n    ]\n)\n\n\nfig = go.Figure(data=[go.Pie(\n    values=counter,\n    labels=['Disaster', 'Not Disaster']\n)])\n\nfig.update_layout(\n    height=450\n)\n\nfig.show()","9b6111d0":"tweet_len_is_disaster = df_analysis[df_analysis['target'] == 1]['text'].map(lambda x: len(x))\ntweet_len_not_disaster = df_analysis[df_analysis['target'] == 0]['text'].map(lambda x: len(x))\n\nfig = go.Figure()\n\nfig.add_trace(go.Histogram(\n    x=tweet_len_not_disaster,\n    xbins=dict( # bins used for histogram\n        size=1\n    ),\n    name='Not a Disaster'\n))\n\nfig.add_trace(go.Histogram(\n    x=tweet_len_is_disaster,\n    xbins=dict( # bins used for histogram\n        size=1\n    ),\n    name='Disaster'\n))\n\n\n# Overlay both histograms\nfig.update_layout(\n    height=450,\n    barmode='overlay'\n)\n\n# Reduce opacity to see both histograms\nfig.update_traces(opacity=0.75)\n\nfig.show()","4a68faab":"word_len_is_disaster = df_analysis[df_analysis['target'] == 1]['text'].str.split().map(lambda x: len(x))\nword_len_not_disaster = df_analysis[df_analysis['target'] == 0]['text'].str.split().map(lambda x: len(x))\n\nfig = go.Figure()\n\nfig.add_trace(go.Histogram(\n    x=word_len_not_disaster,\n    xbins=dict( # bins used for histogram\n        size=1\n    ),\n    name='Not a Disaster'\n))\n\nfig.add_trace(go.Histogram(\n    x=word_len_is_disaster,\n    xbins=dict( # bins used for histogram\n        size=1\n    ),\n    name='Disaster'\n))\n\n\n# Overlay both histograms\nfig.update_layout(\n    height=450,\n    barmode='overlay'\n)\n\n# Reduce opacity to see both histograms\nfig.update_traces(opacity=0.75)\n\nfig.show()","15d597fc":"# Reference : Pratiyush Mishra.\ndef missing_val_analysis(data):\n    missing_values = data.isnull().sum()\n    missing_values = missing_values[missing_values > 0].sort_values(ascending = False)\n    missing_values_data = pd.DataFrame(missing_values)\n    missing_values_data.reset_index(level=0, inplace=True)\n    missing_values_data.columns = ['Feature','Number of Missing Values']\n    missing_values_data['Percentage of Missing Values'] = (100.0*missing_values_data['Number of Missing Values'])\/len(data)\n    return missing_values_data","10e402f5":"missing_val_analysis(df_analysis) # Missing value analysis in the training data.","45aa37ab":"duplicate_entries = df_analysis[df_analysis.duplicated(['text','target'],keep=False)] # Duplicate records with same targets.\n\nprint(f'Entries having same text and targets: {len(duplicate_entries)}')\n\nduplicate_entries.head(5)","878b6710":"df_analysis.drop_duplicates(['text','target'],inplace=True)","a9d8f483":"contradicting_entries = df_analysis[df_analysis.duplicated(['text'], keep=False)] # Duplicate records with outliers.\n\nprint(f'Entries having same text but different targets: {len(contradicting_entries)}')\n\ncontradicting_entries.head(5)","31fb2d27":"#for index, row in contradicting_entries.iterrows(): print(f\"{row['id']} : {row['target']}  \\t\\t {row['text']}\\n\")\n#contradicting_entries","7e17d7f1":"entries_to_drop = [610, 1197, 1221, 1365, 2832, 3243, 3985, 4221, 4232, 4292, 4305, 4312, 4320, 4382, 4618, 5620, 6091, 6616]\n\ndf_analysis.drop(entries_to_drop,inplace=True) # Dropping the outliers.\ndf_analysis = df_analysis.reset_index(drop=True) # Resetting the indexes.","1af10224":"df_train = dataset.copy()","306f2945":"entries_to_drop = [610, 1197, 1221, 1365, 2832, 3243, 3985, 4221, 4232, 4292, 4305, 4312, 4320, 4382, 4618, 5620, 6091, 6616]\n\ndf_train.drop_duplicates(['text','target'],inplace=True) # Dropping the outliers.\ndf_train.drop(entries_to_drop,inplace=True) # Dropping the outliers.\n\ndf_train = df_train.reset_index(drop=True) # Resetting the indexes.","c7fb7c9b":"nlp = spacy.load(\"en_core_web_sm\") # Loading the NLP","6ae19ef8":"stopwords = list(STOP_WORDS)\npunct = string.punctuation","ebd03178":"# remove urls\ndef remove_URL(text): return re.compile(r'https?:\/\/\\S+|www\\.\\S+').sub(r'',text)\n\n# remove html tags\ndef remove_html(text): return re.compile(r'<.*?>').sub(r'',text)\n\n# Reference : https:\/\/gist.github.com\/slowkow\/7a7f61f495e3dbb7e3d767f97bd7304b\n# remove emojis\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndef pre_processor(text):\n    text = remove_emoji(text)\n    text = remove_html(text)\n    text = remove_URL(text)\n        \n    \"\"\"\n    text_doc = nlp(text) # creating the doc object\n    \n        tokens = []\n    for token in text_doc:\n        if token.lemma_ != \"-PRON-\":\n            temp = token.lemma_.lower().strip()\n        else:\n            temp = token.lower_\n        tokens.append(temp)\n        \n    cleaned_tokens = []\n    for token in tokens:\n        if token not in stopwords and token not in punct:\n            cleaned_tokens.append(token)\n            \n    return ' '.join(cleaned_tokens)\n    \n    \"\"\"\n    return text","aaf6bd46":"sentence = '<h2>hello<\/h2> this is a test sentence, you should click on this url : http:\/\/crazy-website'\npre_processor(sentence)","8c796e70":"df_train['text'] = df_train['text'].apply(lambda x: pre_processor(x))\ndf_sub['text'] = df_sub['text'].apply(lambda x: pre_processor(x))","b3c36d9a":"df_train.head(3)","9a14a9bc":"# Training Data\ndf_train['keyword'].fillna('',inplace=True)\n\ndf_train['text'] = df_train['text'] + ' ' + df_train['keyword']\ndf_train['text'] = df_train['text'].apply(lambda x: x.strip())\n\ndf_train.drop(['keyword'], axis=1, inplace=True)\ndf_train.drop(['location'], axis=1, inplace=True)\n\n## Submission Data\ndf_sub['keyword'].fillna('', inplace=True)\ndf_sub['text'] = df_sub['text'] + ' ' + df_sub['keyword']\ndf_sub['text'] = df_sub['text'].apply(lambda x: x.strip())\ndf_sub.drop(['keyword'], axis=1, inplace=True)\ndf_sub.drop(['location'], axis=1, inplace=True)","5c2783d7":"df_train.head(3)","dac1e40d":"dataset_is_disaster = df_train[df_train['target'] == 1]\ndataset_not_disaster = df_train[df_train['target'] == 0]\n\ndataset_not_disaster = dataset_not_disaster.sample(dataset_is_disaster.shape[0])\n\nprint(f\"IS_D Shape : {dataset_is_disaster.shape}\\nNOT_D Shape : {dataset_not_disaster.shape}\")\n\nXy_train = dataset_not_disaster.append(dataset_is_disaster, ignore_index=True)\n\nXy_train.head()","7875abde":"train_data, val_data = train_test_split(Xy_train, test_size=0.1, random_state=0)\n\nprint(f\"Train shape : {train_data.shape}\")\nprint(f\"Validation shape : {val_data.shape}\")","15eecf67":"(X_train, y_train), (X_val, y_val), preproc = text.texts_from_df(\n    train_df=train_data,\n    text_column = 'text',\n    label_columns = 'target',\n    val_df = val_data,\n    maxlen = 256,\n    preprocess_mode = 'bert'\n)","f3022bed":"model = text.text_classifier(\n    name = 'bert',\n    train_data = (X_train, y_train),\n    preproc = preproc\n)\n\nlearner = ktrain.get_learner(\n    model=model,\n    train_data=(X_train, y_train),\n    val_data = (X_val, y_val),\n    batch_size = 16\n)\n\nlearner.fit_onecycle(\n    lr = 2e-5,\n    epochs = 2\n)\n\npredictor = ktrain.get_predictor(learner.model, preproc)","e427923b":"result = pd.DataFrame()\n\nresult['id'] = df_sub['id']\nresult['target'] = predictor.predict(df_sub['text'].values)\nresult['target'] = result['target'].map(lambda x:1 if x=='target' else 0)\n\nresult.head()","6e1ba9e2":"result.to_csv('submission.csv', index=False)","bda913d4":"## The dataset\n### Getting","78f75c1f":"## Prediction & Submission","a08d5653":"##### Conclusion :\n\n> We've found the outliers from our dataset, we'll remove them on the training dataset later.\n\n## Pre-Processing\n\n### Removing outliers","c60e5f06":"### Shape analysis","1fde2b3a":"We can easily suppress the duplicates so there's only one left for each case.","7a209ef7":"##### Conclusion :\n> Again, no significant difference.\n\n### Missing Values Analysis","62836e8e":"# Disaster tweets analysis using BERT & TF\n---\nSpecial thanks to Pratiyush Mishra.\n\n## The problem\n\nTwitter has become an important communication channel in times of emergency.\nThe ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n\nBut, it\u2019s not always clear whether a person\u2019s words are actually announcing a disaster.\n\n## The objective\n\n>  Predicting whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0.\n\n## Importing libraries","930bb138":"Here, it's a bit different, we need to find the one that is true (if the tweet is about a disaster or not). Since there's just 36 entries we can search them manualy.\n\n*881, 1723, 1760, 1968, 4076, 4659, 5662, 5996, 6012, 6097, 6112, 6122, 6134, 6223, 6566, 8018, 8698, 9472*","ff1a0813":"### Split Train \/ Validation\n\n#### Equilibrate the dataset","3c98908c":"##### Conclusion :\n\n> We can see a difference of 7% in favor of 'Not a Disaster', we will correct that later during the pre-processing.\n\n### Tweet length \/ Target","b27cbaf4":"##### Conclusion :\n>**Training dataset shape**: (7613, 5)<br>\n>**Testing dataset shape**: (3263, 4)<br>\n> There seems to have a some NaN values for the features \"keyword\" and \"location\". We'll see this later.\n\n## Exploratory Data Analysis (EDA)\n\n### Distribution of target value","a9cfc0a9":"### Pre-processing using Ktrain\n\nConvert tweet into an appropriate form for the bert model.","b2f8dcc5":"##### Conclusion :\n> There's juste few **keywords** missing, so we can fit NaN values with empty strings before adding them to the 'text' feature for the processing. <br>\nFor the **location** feature it's a bit different since there are a lot of them missing, also, a lot of location are wrong (we can see it when looking to the dataset). So we can easily not use them.\n\n### Outliers\nOutliers in this case are duplicated data. So we'll search that.\n\n#### Same text & Target","d1613dfd":"### SpaCy Text Cleaning\nIn this part we need to :\n\n- remove stopwords\n- remove punctuation\n- remove urls\n- remove smileys\n- remove urls\n\nThose are useless information that won't help our model.","bd800cae":"#### Same text & Different Target","d15f0e44":"ktrain pre-processes the data and returns us training and validation dataset those can directly be used by our bert model.\n\n## Ktrain Model & Training","95316b22":"### Remove keywords & location features","610325c3":"### Overview","a821b534":"##### Conclusion :\n\n> There's not a significant difference between the two targets.\n\n### Tweet words len \/ Target","8b61c125":"#### Split the dataset"}}