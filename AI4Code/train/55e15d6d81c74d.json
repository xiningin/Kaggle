{"cell_type":{"a07d586a":"code","60ce3a96":"code","1052d4ad":"code","08e60561":"code","9b0a0f2a":"code","b7e0da72":"code","326542bd":"code","5a84f84e":"code","3117aab9":"code","07ad7c98":"code","86c03b9f":"code","1b4fb75c":"code","7aa88d36":"markdown","37abc9c4":"markdown","5d731ea3":"markdown","46e30e0a":"markdown"},"source":{"a07d586a":"import numpy as np \nimport pandas as pd \nimport plotly.express as px\nimport glob\n\ntrain = pd.read_csv('\/kaggle\/input\/optiver-realized-volatility-prediction\/train.csv')\nsub = pd.read_csv('\/kaggle\/input\/optiver-realized-volatility-prediction\/sample_submission.csv')\n\n# Some Constants\nTRAIN_BOOK_PATHS = glob.glob(\"\/kaggle\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/*\")\nTEST_BOOK_PATHS  = glob.glob(\"\/kaggle\/input\/optiver-realized-volatility-prediction\/book_test.parquet\/*\")\nTRAIN_TRADE_PATHS = glob.glob(\"\/kaggle\/input\/optiver-realized-volatility-prediction\/trade_train.parquet\/*\")\nTEST_TRADE_PATHS  = glob.glob(\"\/kaggle\/input\/optiver-realized-volatility-prediction\/trade_test.parquet\/*\")\n\ndef submit(prediction):\n    sub.drop(sub.index, inplace=True)\n    sub['row_id'] = test_data['row_id']\n    sub['target'] = prediction\n    sub.to_csv('\/kaggle\/working\/submission.csv', index=False)","60ce3a96":"class DataManager:\n    def __init__(self, train=True):\n        self._train = train\n        self._book_file_list = TRAIN_BOOK_PATHS if train else TEST_BOOK_PATHS\n        self._trade_file_list = TRAIN_TRADE_PATHS if train else TEST_TRADE_PATHS\n        self.measures_list = []\n        \n    def _log_return(self, stock_prices):\n        return np.log(stock_prices).diff()\n    \n    def _traverse_book(self):\n        \"\"\" Goes through each of the training files. \"\"\"\n        for book_file_path, trade_file_path in zip(self._book_file_list, self._trade_file_list):\n            stock_id = book_file_path.split(\"=\")[1]\n            \n            book = pd.read_parquet(book_file_path)\n            book['wap'] = (book['bid_price1'] * book['ask_size1'] + book['ask_price1'] * book['bid_size1']) \/ (book['bid_size1']+ book['ask_size1'])\n            book['log_return'] = book.groupby(['time_id'])['wap'].apply(self._log_return)\n            book = book[~book['log_return'].isnull()]\n            \n            trade = pd.read_parquet(trade_file_path)\n            \n            book_stock_slice = train[train['stock_id'] == int(stock_id)]\n            \n            for time_id in book['time_id'].unique():\n                book_slice = book[book['time_id'] == time_id]\n                \n                dic = {\n                    'row_id': f\"{stock_id}-{time_id}\", # Fixing row-id from here\n                    'wap_mean': book_slice['wap'].mean(),\n                    'wap_std':book_slice['wap'].std(),\n                    'log_return_mean': book_slice['log_return'].mean(),\n                    'log_return_std':book_slice['log_return'].std(),\n                    'ask_size_mean': book_slice['ask_size1'].mean(),\n                    'ask_size_std': book_slice['ask_size1'].std(),\n                    'ask_price_mean': book_slice['ask_price1'].mean(),\n                    'ask_price_std': book_slice['ask_price1'].std(),\n                    'bid_size_mean': book_slice['bid_size1'].mean(),\n                    'bid_size_std': book_slice['bid_size1'].std(),\n                    'bid_price_mean': book_slice['bid_price1'].mean(),\n                    'bid_price_std': book_slice['bid_price1'].std(),\n                    'actual_price_mean': trade['price'].mean(),\n                    'actual_price_std': trade['price'].std(),\n                    'size_mean': trade['size'].mean(),\n                    'size_std': trade['size'].std(),\n                    'order_count_mean': trade['order_count'].mean(),\n                    'order_count_std': trade['order_count'].std(),\n                }\n                \n                if self._train: dic['target'] = book_stock_slice[book_stock_slice['time_id'] == time_id]['target'].values[0]\n                \n                self.measures_list.append(dic)\n    \n    def get_processed(self):\n        self._traverse_book()\n        \n        return pd.DataFrame(self.measures_list)","1052d4ad":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error as mae\n\ndata = pd.read_csv('\/kaggle\/input\/processedbooktrade\/train_v1.csv')\n\ny = data['target']\nX = data.iloc[:,2:-1]\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\ntest_data = DataManager(train=False).get_processed()\nX_test = test_data.iloc[:,1:]","08e60561":"print(type(X_train)) # 6128row 18 columns \nprint(type(y_train))# 6128\n#\u56de\u5f52\u4efb\u52a1","9b0a0f2a":"import torch\nimport torch.nn as nn\n\n#data\u8f6ctensor\ndef pandas_to_tensor(x):\n    x=np.array(x)\n    x=torch.tensor(x)\n    return x\n\nX_train=pandas_to_tensor(X_train)\ny_train=pandas_to_tensor(y_train)","b7e0da72":"class Volatility_prediction(nn.Module):\n    def __init__(self):\n        super(Volatility_prediction,self).__init__()\n        self.predict = nn.Sequential(\n            nn.Linear(18, 200),\n            nn.ReLU(),\n            nn.Linear(200, 50),\n            nn.ReLU(),\n            nn.Linear(50,1),\n            nn.Softmax(dim=0)\n        )\n\n    def forward(self, x):\n        prediction = self.predict(x)\n        return prediction\n\n\nnet =Volatility_prediction()\noptimizer = torch.optim.SGD(net.parameters(), lr=0.05)\nloss_func = nn.MSELoss()\n\nfor epoch in range(50000):\n    out = net(X_train.to(torch.float32))\n    loss = loss_func(out.squeeze(1),y_train.to(torch.float32))\n#     print(out,y_train.to(torch.float32))\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \ntorch.save(net, '\\model.pkl')\n","326542bd":"def model_test(X):\n    model = torch.load('\\model.pkl')\n    model.eval()\n\n    output=0\n    times=0\n    for data in pandas_to_tensor(X):\n        output=model(data.to(torch.float32))\n        if times==0:\n            times+=1\n            outputs=output\n        else:\n            outputs=torch.cat((outputs,output),0)\n        \n    outputs=pd.Series(outputs.to(torch.float64).detach().numpy())\n    return outputs\n\noutputs=model_test(X_val)","5a84f84e":"mae(outputs, y_val)\n\n# from sklearn import tree\n# import matplotlib.pyplot as plt\n\n# fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (4,4), dpi=800)\n# tree.plot_tree(rfg.estimators_[10],\n#                feature_names = X.columns, \n#                filled = True);\n\n# fig.savefig('rf_individualtree.png')","3117aab9":"submit(model_test(X_test))","07ad7c98":"from joblib import dump, load\ndump(net, 'rfg_1000_10_train_v1.joblib') \n# clf = load('rfg_1000_10.joblib') ","86c03b9f":"# pd.read_csv('\/kaggle\/working\/submission.csv')","1b4fb75c":"# clf.get_params()","7aa88d36":"### Submissiong process","37abc9c4":"- The data file for each sotck_id will be imported from book_train folder, the ids go from 0 to 126.\n\n## Preprocessing\n\nData will be categorized by the time-id, meaning the modeling will happen for each time-id individually. Hence, statistical measures for a given time-id should be extracted and fed into a model.\n\n- Get the wap for each row\n- Drp seconds_in_bucket","5d731ea3":"### Testing different pre-processing techniques","46e30e0a":"### Modeling"}}