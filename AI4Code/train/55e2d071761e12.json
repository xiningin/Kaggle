{"cell_type":{"59d4cd5f":"code","d7c8f48b":"code","0e1c9716":"code","f0fd53b7":"code","b54318f2":"code","608b899f":"code","b7c9ab7f":"code","da627644":"code","ad402629":"code","5cd0f21a":"code","e06abebb":"code","adee6b23":"code","fb1e1c22":"code","181dcd01":"code","2e6fdd78":"code","68dd67fb":"code","61624228":"code","e679c0f0":"code","90318292":"code","349c7f7a":"code","b724bdae":"code","a558b8ca":"code","ff5556f9":"code","b0e860b9":"code","388734e9":"code","6a6a206f":"code","ad75304a":"code","a46aefaa":"code","5411dd3c":"code","5666670d":"markdown","aa400cfc":"markdown","683f4d90":"markdown","5ead9aa8":"markdown","37f086ae":"markdown","93f9bbcb":"markdown","d3d5fa03":"markdown","0645285a":"markdown","80d511a7":"markdown","909ddf6a":"markdown","e719e1ef":"markdown","96616813":"markdown","752b5e73":"markdown","5d65fae7":"markdown","1af65d07":"markdown","93062fe8":"markdown"},"source":{"59d4cd5f":"!pip install pydotplus lime","d7c8f48b":"import lime\nimport lime.lime_tabular\n\nfrom sklearn.tree import (\n    ExtraTreeClassifier,\n    DecisionTreeClassifier,\n    export_graphviz\n)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    classification_report,\n    confusion_matrix\n)\nfrom sklearn.externals.six import StringIO\nfrom sklearn.preprocessing import (\n    OneHotEncoder,\n    LabelEncoder\n)\nfrom sklearn.compose import ColumnTransformer\n\nimport matplotlib.pyplot as plt\n\nfrom IPython.display import Image\n\nimport pydotplus\n\nimport seaborn as sns\nimport pandas as pd \nimport numpy as np","0e1c9716":"df = pd.read_csv('\/kaggle\/input\/heart-disease-uci\/heart.csv')","f0fd53b7":"df.columns = [\n    'age',\n    'sex',\n    'chest_pain_type',\n    'resting_blood_pressure',\n    'cholesterol',\n    'fasting_blood_sugar',\n    'rest_ecg',\n    'max_heart_rate_achieved',\n    'exercise_induced_angina',\n    'st_depression',\n    'st_slope',\n    'num_major_vessels',\n    'thalassemia',\n    'target'\n]","b54318f2":"df.head(5)","608b899f":"df['sex'][df['sex'] == 0] = 'female'\ndf['sex'][df['sex'] == 1] = 'male'\n\ndf['chest_pain_type'][df['chest_pain_type'] == 1] = 'typical angina'\ndf['chest_pain_type'][df['chest_pain_type'] == 2] = 'atypical angina'\ndf['chest_pain_type'][df['chest_pain_type'] == 3] = 'non-anginal pain'\ndf['chest_pain_type'][df['chest_pain_type'] == 4] = 'asymptomatic'\n\ndf['fasting_blood_sugar'][df['fasting_blood_sugar'] == 0] = 'lower than 120mg\/ml'\ndf['fasting_blood_sugar'][df['fasting_blood_sugar'] == 1] = 'greater than 120mg\/ml'\n\ndf['rest_ecg'][df['rest_ecg'] == 0] = 'normal'\ndf['rest_ecg'][df['rest_ecg'] == 1] = 'ST-T wave abnormality'\ndf['rest_ecg'][df['rest_ecg'] == 2] = 'left ventricular hypertrophy'\n\ndf['exercise_induced_angina'][df['exercise_induced_angina'] == 0] = 'no'\ndf['exercise_induced_angina'][df['exercise_induced_angina'] == 1] = 'yes'\n\ndf['st_slope'][df['st_slope'] == 1] = 'upsloping'\ndf['st_slope'][df['st_slope'] == 2] = 'flat'\ndf['st_slope'][df['st_slope'] == 3] = 'downsloping'\n\ndf['thalassemia'][df['thalassemia'] == 1] = 'normal'\ndf['thalassemia'][df['thalassemia'] == 2] = 'fixed defect'\ndf['thalassemia'][df['thalassemia'] == 3] = 'reversable defect'","b7c9ab7f":"df.head(5)","da627644":"print(f\"{df['target'].sum()\/df.shape[0]:.2f} is the proportion of healthy and unhealthy patients\")","ad402629":"sns.set(style=\"whitegrid\")","5cd0f21a":"sns.barplot(x='sex', y='age', data=df, hue='target')","e06abebb":"sns.barplot(x='sex', y='st_depression', data=df, hue='target')","adee6b23":"df = pd.get_dummies(df, drop_first=True)","fb1e1c22":"X = df.drop('target', axis=1)\ny = df['target']","181dcd01":"seed = 42\nX_train, X_test, y_train, y_test = train_test_split(\n    df.drop('target', axis=1), \n    df['target'], \n    test_size=0.2, \n    stratify=df['target'], \n    random_state=seed\n)","2e6fdd78":"tree = DecisionTreeClassifier(random_state=seed, max_depth=3)  # we set max_depth to 3 using a rule of thumb for visualization\ntree.fit(X_train, y_train)","68dd67fb":"y_pred = tree.predict(X_test)","61624228":"target_labels = ['healthy', 'unhealthy']\n\ncm = confusion_matrix(y_test, y_pred)\ndf_cm = pd.DataFrame(cm, columns=target_labels, index=target_labels)\ndf_cm.index.name = 'Actual'\ndf_cm.columns.name = 'Predicted'\nsns.set(font_scale=1.4)\nsns.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16})","e679c0f0":"print(classification_report(y_test, y_pred))","90318292":"dot_data = StringIO()\n\nexport_graphviz(\n    tree,\n    out_file=dot_data,\n    filled=True,\n    rounded=True, \n    class_names=target_labels, \n    special_characters=True, \n    feature_names=df.drop('target', axis=1).columns\n)\n\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n\nImage(graph.create_png())","349c7f7a":"xtra_tree = ExtraTreeClassifier(random_state=seed, max_depth=3)\nxtra_tree.fit(X_train, y_train)","b724bdae":"y_pred = xtra_tree.predict(X_test)","a558b8ca":"target_labels = ['healthy', 'unhealthy']\n\ncm = confusion_matrix(y_test, y_pred)\ndf_cm = pd.DataFrame(cm, columns=target_labels, index=target_labels)\ndf_cm.index.name = 'Actual'\ndf_cm.columns.name = 'Predicted'\nsns.set(font_scale=1.4)\nsns.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16})","ff5556f9":"print(classification_report(y_test, y_pred))","b0e860b9":"df_feature_importance = pd.DataFrame({\"Feature\": X_train.columns, \"Feature importance\": xtra_tree.feature_importances_})","388734e9":"plt.figure(figsize=(16, 6))\nsns.barplot(\n    x='Feature importance',\n    y='Feature',\n    data=df_feature_importance.sort_values(by='Feature importance', ascending=False)\n)","6a6a206f":"explainer = lime.lime_tabular.LimeTabularExplainer(\n    X_train, \n    feature_names=X_train.columns, \n    discretize_continuous=False,\n    mode='classification'\n)","ad75304a":"instance = X_test.iloc[0]\nexp = explainer.explain_instance(instance.to_numpy(), xtra_tree.predict_proba)","a46aefaa":"print(f\"True value: {y_test.iloc[0]}\")","5411dd3c":"exp.show_in_notebook(show_table=True, show_all=True)","5666670d":"# Extreme Tree\n\nExtreme Trees remember Random Forests with two main differences:\n\n1. There is no bootstraping of data, that is, no data is replaced back when sampling\n2. The trees do not look for optimal splits, they select the splits randomly","aa400cfc":"# Interpretability Examples\n\n## Dataset - UCI Heart Disease\n\nThis database contains 76 attributes, but all published experiments refer to using a subset of 14 of them. In particular, the Cleveland database is the only one that has been used by ML researchers to this date. The \"goal\" field refers to the presence of heart disease in the patient.\n\n## Features\n\n> 1. age\n> 2. sex\n> 3. chest pain type (4 values)\n> 4. resting blood pressure\n> 5. serum cholestoral in mg\/dl\n> 6. fasting blood sugar > 120 mg\/dl\n> 7. resting electrocardiographic results (values 0,1,2)\n> 8. maximum heart rate achieved\n> 9. exercise induced angina\n> 10. oldpeak = ST depression induced by exercise relative to rest\n> 11. the slope of the peak exercise ST segment\n> 12. number of major vessels (0-3) colored by flourosopy\n> 13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect","683f4d90":"### What if I want to understand local predictions?\n\nHere we can see the global effect of features, but what if we want to explain a local prediction?","5ead9aa8":"## Decision Tree\n\nDecision trees are widely used in the medicine for structured data because they can be easily transformed into a set of if-else statements and, therefore, are very easy to interpret.\n\nC4.5 and CART are some widely used implementations of Decision Trees.\n\nBecause we are only focusing on building 'toy' models we will not focus on the metrics (like using Cross-Validation to optimize some parameters).","37f086ae":"## What if we want to learn about feature importance?\n\nLet's use an extreme tree model (more complex than a decision tree) to learn more about feature importance!","93f9bbcb":"## To Do\n\nSHAP values!","d3d5fa03":"You can see that the categorial features have been transformed to integers, this will not help us when dealing with the model, so let's transform the features into its nominal values.","0645285a":"# Building models and trying to interpret them...","80d511a7":"We will change the name of the columns to something that makes more sense:|","909ddf6a":"# Exploratory Data Analysis\n\nJust exploring a little bit the data to know how it looks like...","e719e1ef":"### Let's visualize how the decision tree looks like","96616813":"#### References:\n\nLime-Python: https:\/\/github.com\/marcotcr\/lime\n\n\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier: https:\/\/arxiv.org\/abs\/1602.04938\n\nExtremely randomized trees: https:\/\/link.springer.com\/article\/10.1007\/s10994-006-6226-1","752b5e73":"Because we have categorical data we must convert them to a 'vector' of values...","5d65fae7":"Let's try visualizing the data to see if there are some features that already look interesting...","1af65d07":"## We can use LIME!\n\nLIME explain local predictions using:\n\n1. explainable model - to explain the black box model we need to use a model that we can understand\n2. perturbations - LIME generates new instances perturbating the instance in interest (it uses a Gaussian distribution)\n3. kernel - estimate proximity to the instance in insterest","93062fe8":"Splitting data for training\/testing"}}