{"cell_type":{"9e5f70dd":"code","adadc173":"code","cfa0ca5c":"code","a7f81dcd":"code","b78152d2":"code","7c8fc36c":"code","63aba9c9":"code","5f84f88e":"code","9cdcef30":"code","fcb8bb12":"code","21c52963":"code","b7925c36":"code","a66cd882":"code","a27a7f9e":"code","683daca4":"code","5a36df41":"code","6888e22f":"code","1c52ea10":"code","aa6c712a":"code","9625abd0":"code","33b465bd":"code","0608952c":"code","05232856":"code","c61dfcc7":"markdown","b5f617ac":"markdown","6333bc40":"markdown","ecf073ff":"markdown","1d972717":"markdown","d5648587":"markdown","efc38003":"markdown","8c110a85":"markdown","35a21ec9":"markdown","9415b5eb":"markdown","8d5551c8":"markdown","d6536288":"markdown","edcb0c68":"markdown","70120817":"markdown","731b1d23":"markdown","38f3f8fb":"markdown","5b1133e9":"markdown"},"source":{"9e5f70dd":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd","adadc173":"#title input file is in tsv format\ndata=pd.read_csv('..\/input\/reviews\/Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3) \n#delimeter or sep='\\t' \n#quoting =3 no quote or ignore quotes while processing","cfa0ca5c":"import seaborn as sns\nsns.countplot(x='Liked', data=data)\nplt.show()","a7f81dcd":"good_reviews_count = len(data.loc[data['Liked'] == 1])\nbad_reviews_count=len(data.loc[data['Liked']==0])\n(good_reviews_count, good_reviews_count)","b78152d2":"#Data Cleaning\n# Cleaning the Text\nimport nltk\nimport re\nnltk.download('stopwords')\nnltk.download('wordnet')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\n\nstemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()","7c8fc36c":"corpus = []\nfor i in range(len(data)):\n    review = re.sub('[^a-zA-Z]', ' ', data['Review'][i])\n    review = review.lower()\n    review = review.split()\n    all_stopwords = stopwords.words('english')\n    all_stopwords.remove('not') \n    #remove negative word 'not' as it is closest word to help determine whether the review is good or not \n    review = [stemmer.stem(word) for word in review if not word in set(all_stopwords)]\n    review = ' '.join(review)\n    corpus.append(review)\nprint(corpus)","63aba9c9":"# Creating the Bag of Words model\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features = 1500)\nX = cv.fit_transform(corpus).toarray()\ny = data.iloc[:, -1].values","5f84f88e":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)","9cdcef30":"from sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import BernoulliNB\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nkfold = StratifiedKFold(n_splits=12)\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report","fcb8bb12":"#Logistic Regression\nlr = LogisticRegression(random_state=0, C=0.82286, max_iter=2000, solver='lbfgs')\ncv = cross_val_score(lr,X_train,y_train,cv=kfold)\nprint(cv)\nprint(cv.mean()*100)\nlr.fit(X_train,y_train)\ny_pred_lr=lr.predict(X_test)\nprint('The accuracy of the Logistic Regression is',metrics.accuracy_score(y_pred_lr,y_test)*100)\ncm=confusion_matrix(y_test, y_pred_lr)\nprint(cm)\nclassification_report(y_test, y_pred_lr)","21c52963":"#GaussianNB\ngnb = GaussianNB(var_smoothing=1e-2)\ncv = cross_val_score(gnb,X_train,y_train,cv=kfold)\nprint(cv)\nprint(cv.mean()*100)\ngnb.fit(X_train,y_train)\ny_pred_gnb=gnb.predict(X_test)\nprint('The accuracy of the Naive Bayes is', metrics.accuracy_score(y_pred_gnb,y_test)*100)\ncm=confusion_matrix(y_test, y_pred_gnb)\nprint(cm)\nclassification_report(y_test, y_pred_gnb)","b7925c36":"#MultinomialNB\nmnb = MultinomialNB(alpha=2)\ncv = cross_val_score(mnb,X_train,y_train,cv=kfold)\nprint(cv)\nprint(cv.mean()*100)\nmnb.fit(X_train,y_train)\ny_pred_mnb=mnb.predict(X_test)\nprint('The accuracy of the Naive Bayes is', metrics.accuracy_score(y_pred_mnb,y_test)*100)\ncm=confusion_matrix(y_test, y_pred_mnb)\nprint(cm)\nclassification_report(y_test, y_pred_mnb)","a66cd882":"#Bernoulli NB\nbnb = BernoulliNB(alpha=10)\ncv = cross_val_score(bnb,X_train,y_train,cv=kfold)\nprint(cv)\nprint(cv.mean()*100)\nmnb.fit(X_train,y_train)\ny_pred_bnb=mnb.predict(X_test)\nprint('The accuracy of the Naive Bayes is', metrics.accuracy_score(y_pred_bnb,y_test)*100)\ncm=confusion_matrix(y_test, y_pred_bnb)\nprint(cm)\nclassification_report(y_test, y_pred_bnb)","a27a7f9e":"#Random Forest Classifier\nrf = RandomForestClassifier(bootstrap=False, criterion='entropy', max_depth=30,\n                       max_features='log2', min_samples_leaf=2,\n                       n_estimators=500, random_state=0)\nrf.fit(X_train, y_train)\ncv = cross_val_score(rf,X_train,y_train,cv=kfold)\nprint(cv)\nprint(cv.mean()*100)\ny_pred_rf = rf.predict(X_test)\nprint('The accuracy of the RandomForestClassifier is',metrics.accuracy_score(y_pred_rf,y_test)*100)\ncm=confusion_matrix(y_test, y_pred_rf)\nprint(cm)\nclassification_report(y_test, y_pred_rf)","683daca4":"#Linear SVC\nsvcl = SVC(kernel = 'linear', random_state = 0, probability=True)\nsvcl.fit(X_train, y_train)\ncv = cross_val_score(svcl,X_train,y_train,cv=kfold)\nprint(cv)\nprint(cv.mean()*100)\ny_pred_svcl = svcl.predict(X_test)\nprint('The accuracy of the Linear SVC is',metrics.accuracy_score(y_pred_svcl,y_test)*100)\ncm=confusion_matrix(y_test, y_pred_svcl)\nprint(cm)\nclassification_report(y_test, y_pred_svcl)","5a36df41":"#rbf SVC\nfrom sklearn.svm import SVC\nsvck = SVC(kernel = 'rbf', random_state = 0, probability=True)\nsvck.fit(X_train, y_train)\ncv = cross_val_score(svck,X_train,y_train,cv=kfold)\nprint(cv)\nprint(cv.mean()*100)\ny_pred_svck = svck.predict(X_test)\nprint('The accuracy of the Kernel SVC is',metrics.accuracy_score(y_pred_svck,y_test)*100)\ncm=confusion_matrix(y_test, y_pred_svck)\nprint(cm)\nclassification_report(y_test, y_pred_svck)","6888e22f":"#Decision Tree Classifier\ndt = DecisionTreeClassifier(random_state=0, max_depth=30, min_samples_split=5, min_samples_leaf=1)\ndt.fit(X_train, y_train)\ncv = cross_val_score(dt,X_train,y_train,cv=kfold)\nprint(cv)\nprint(cv.mean()*100)\ny_pred_dt = dt.predict(X_test)\nprint('The accuracy of the Decision Tree Classifier is',metrics.accuracy_score(y_pred_dt,y_test)*100)\ncm=confusion_matrix(y_test, y_pred_dt)\nprint(cm)\nclassification_report(y_test, y_pred_dt)","1c52ea10":"#KNN\nknn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2, leaf_size = 20)\nknn.fit(X_train, y_train)\ncv = cross_val_score(knn,X_train,y_train,cv=kfold)\nprint(cv)\nprint(cv.mean()*100)\ny_pred_knn = knn.predict(X_test)\nprint('The accuracy of the K-Neighbors Classifier is',metrics.accuracy_score(y_pred_knn,y_test)*100)\ncm=confusion_matrix(y_test, y_pred_knn)\nprint(cm)\nclassification_report(y_test, y_pred_knn)","aa6c712a":"#VCLF 1\nvoting_clf = VotingClassifier(estimators = [('lr', lr),('gnb',gnb),('bnb',bnb),('mnb',mnb),\n                                            ('knn',knn),('dt',dt),\n                                            ('rf',rf),('svck',svck),('svcl',svcl)], voting = 'soft') \nvoting_clf.fit(X_train, y_train)\ncv = cross_val_score(voting_clf,X_train,y_train,cv=kfold)\nprint(cv)\nprint(cv.mean()*100)\ny_pred_vclf = voting_clf.predict(X_test)\nprint('The accuracy of the Voting Classifier is',metrics.accuracy_score(y_pred_vclf,y_test)*100)\ncm=confusion_matrix(y_test, y_pred_vclf)\nprint(cm)\nclassification_report(y_test, y_pred_vclf)","9625abd0":"#VCLF 2\nvoting_clf = VotingClassifier(estimators = [('lr', lr),('bnb',bnb),('mnb',mnb),('gnb', gnb),\n                                            ('rf',rf),('svck',svck),('svcl',svcl)], voting = 'soft') \nvoting_clf.fit(X_train, y_train)\ncv = cross_val_score(voting_clf,X_train,y_train,cv=kfold)\nprint(cv)\nprint(cv.mean()*100)\ny_pred_vclf = voting_clf.predict(X_test)\nprint('The accuracy of the Voting Classifier is',metrics.accuracy_score(y_pred_vclf,y_test)*100)\ncm=confusion_matrix(y_test, y_pred_vclf)\nprint(cm)\nclassification_report(y_test, y_pred_vclf)","33b465bd":"#VCLF 3\nvoting_clf = VotingClassifier(estimators = [('lr', lr),('bnb',bnb),('mnb',mnb),('gnb', gnb), ('dt',dt),\n                                            ('rf',rf),('svck',svck),('svcl',svcl)], voting = 'soft') \nvoting_clf.fit(X_train, y_train)\ncv = cross_val_score(voting_clf,X_train,y_train,cv=kfold)\nprint(cv)\nprint(cv.mean()*100)\ny_pred_vclf = voting_clf.predict(X_test)\nprint('The accuracy of the Voting Classifier is',metrics.accuracy_score(y_pred_vclf,y_test)*100)\ncm=confusion_matrix(y_test, y_pred_vclf)\nprint(cm)\nclassification_report(y_test, y_pred_vclf)","0608952c":"#VCLF 4\nvoting_clf = VotingClassifier(estimators = [('lr', lr),('gnb',gnb),('bnb',bnb),('mnb',mnb),\n                                            ('knn',knn),('dt',dt),\n                                            ('rf',rf),('svck',svck),('svcl',svcl)], voting = 'hard') \nvoting_clf.fit(X_train, y_train)\ncv = cross_val_score(voting_clf,X_train,y_train,cv=kfold)\nprint(cv)\nprint(cv.mean()*100)\ny_pred_vclf = voting_clf.predict(X_test)\nprint('The accuracy of the Voting Classifier is',metrics.accuracy_score(y_pred_vclf,y_test)*100)\ncm=confusion_matrix(y_test, y_pred_vclf)\nprint(cm)\nclassification_report(y_test, y_pred_vclf)","05232856":"#VCLF 5\nvoting_clf = VotingClassifier(estimators = [('lr', lr),('bnb',bnb),('mnb',mnb),('gnb', gnb),\n                                            ('rf',rf),('svck',svck),('svcl',svcl)], voting = 'hard') \nvoting_clf.fit(X_train, y_train)\ncv = cross_val_score(voting_clf,X_train,y_train,cv=kfold)\nprint(cv)\nprint(cv.mean()*100)\ny_pred_vclf = voting_clf.predict(X_test)\nprint('The accuracy of the Voting Classifier is',metrics.accuracy_score(y_pred_vclf,y_test)*100)\ncm=confusion_matrix(y_test, y_pred_vclf)\nprint(cm)\nclassification_report(y_test, y_pred_vclf)","c61dfcc7":"6. KNN","b5f617ac":"**Restaurant Reviews Sentiment Analysis using Stemming and Bag of Words**","6333bc40":"# Importing the Libraries and reading the tsv data file","ecf073ff":"# In this notebook, I have made an attempt to get a simple text classification model up and running. In this, restaurant review data from Kaggle (also available in UCI ML Library) was used to  perform Sentiment Analysis using Stemming and Bag of Words model to classify reviews into two sentiments \u2014 Liked(1) and Disliked(0).","1d972717":"4. SVC","d5648587":"5. Decision Tree Classifier","efc38003":"2. Naive Bayes","8c110a85":"**So, the dataset is a balanced dataset. The number of '1's and '0's are equal.**","35a21ec9":"# Train, Test Split","9415b5eb":"3. Random Forest Classifier","8d5551c8":"# Since, the data is fairly balanced, we are only concerned with accuracy_score. From above distinct classifiers and various Voting Classifier combinations, highest accuracy achieved was 81% with cross_val_score of 79.4% with Voting Classifier 2 (# VCLF2). ","d6536288":"# Classifier Model Training","edcb0c68":"1. Logistic Regression","70120817":"# Data Cleaning","731b1d23":"**Since, the data is fairly balanced, we are only concerned with accuracy_score. From various Classifiers, highest accuracy achieved was 81% with cross_val_score of 79.4% with Voting Classifier 2 (# VCLF2).**","38f3f8fb":"Voting Classifier","5b1133e9":"# Create a BOW Model"}}