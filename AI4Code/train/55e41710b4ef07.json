{"cell_type":{"5ff43467":"code","b8d34452":"code","1d511c65":"code","e00c860e":"code","0085ac3b":"code","6df38b6e":"code","9256240f":"code","4f9e9e57":"code","6c9c7465":"code","2451a09a":"code","25132aef":"code","10acb795":"code","d8a7e321":"code","2d3131ee":"code","2befb2ee":"code","1e6a2359":"code","aef638da":"code","66738b1e":"code","f6749cb1":"code","f571c468":"code","254e54a6":"code","93656f55":"code","d69bbc80":"code","3284ea96":"code","106e079e":"code","df262c23":"code","b7fe4e3f":"code","76545480":"code","144d833d":"code","ab519d22":"code","7fda5e7b":"code","f2b44254":"code","e2796711":"code","f1b37888":"code","59417c99":"code","ba3dbebc":"code","3071a43c":"code","10b6864f":"code","e1fd1035":"code","7598f7c6":"code","a1abf3aa":"code","1c2cbccd":"code","fe9a9fa6":"code","832a5d3d":"code","1f5c9245":"code","3d7b7f53":"code","e817a497":"code","d631df58":"code","e3849eaf":"code","93161356":"code","478199df":"code","88746426":"code","8a949200":"code","e7280133":"code","46f74c50":"code","49f55c6d":"code","96226012":"code","d1ac4692":"code","10711514":"code","e12c7a60":"code","33109290":"code","c2530af7":"code","aee06c78":"code","1342e1ef":"code","e9331cee":"code","1defbea6":"code","69ff8ab4":"code","11ec51db":"code","0ae676ed":"code","50fcadba":"code","bccb6848":"markdown","96760145":"markdown","1451d55b":"markdown","63d09c74":"markdown","e266daa5":"markdown","6ef66e7a":"markdown","fdbb4ecc":"markdown","1e1fc725":"markdown","b1234cde":"markdown","ac6aeada":"markdown","4c540082":"markdown","90a9675b":"markdown","2c727cc5":"markdown","cd3e92e8":"markdown","478dcf35":"markdown","072100ba":"markdown","e4d1507c":"markdown","e7b3b22d":"markdown","5182b33b":"markdown","828abd08":"markdown","637c9f78":"markdown","ccaa5519":"markdown","55f0d1ad":"markdown","7643b0e3":"markdown","74110b4b":"markdown","39142a00":"markdown","ca5ea035":"markdown","78655106":"markdown","6a6cffae":"markdown","810fdd72":"markdown","8dd9f129":"markdown","c7eff161":"markdown","5a197d6e":"markdown","ef686d2b":"markdown","00457617":"markdown","b503ccf9":"markdown","f89e0315":"markdown","2c36badf":"markdown","1fa7e43c":"markdown","4d95d196":"markdown","13caf035":"markdown","3814fb86":"markdown","f01c34f6":"markdown","0fe4e1b8":"markdown","d3ad5b30":"markdown","43b5d6fe":"markdown","d9e6c0e8":"markdown","6288c4fc":"markdown","a901d672":"markdown","e2079c91":"markdown","b72ba42e":"markdown","d9af0e2a":"markdown","481c3091":"markdown","cdb5c33c":"markdown","c8d30d22":"markdown","348ab1c3":"markdown","b67f2a21":"markdown","c5ea42f0":"markdown","02f0119f":"markdown","5e115a60":"markdown","4938d094":"markdown","ca0e418f":"markdown","291d9aa4":"markdown","1ad964d2":"markdown","95c1d4ed":"markdown","95694c45":"markdown","0cefb470":"markdown","e44bd4d7":"markdown","a6230135":"markdown"},"source":{"5ff43467":"#importing libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import ConnectionPatch\nimport matplotlib.mlab as mlab \n%matplotlib inline\n\n\nimport scipy.optimize as opt\nimport warnings\nwarnings.simplefilter(\"ignore\")\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n\n","b8d34452":"df=pd.read_csv(r\"..\/input\/heart-disease-prediction-using-logistic-regression\/framingham.csv\")\ndf","1d511c65":"df.head(10)","e00c860e":"df.tail()","0085ac3b":"df.columns","6df38b6e":"df.shape","9256240f":"df.columns.nunique()","4f9e9e57":"df['male'].value_counts()","6c9c7465":"df['education'].value_counts()","2451a09a":"df['currentSmoker'].value_counts()","25132aef":"df['BPMeds'].value_counts()","10acb795":"df['prevalentStroke'].value_counts()","d8a7e321":"df['diabetes'].value_counts()","2d3131ee":"df['TenYearCHD'].value_counts()","2befb2ee":"df.info()","1e6a2359":"df.isnull().sum()","aef638da":"# percentage of missing data per category\ntotal = df.isnull().sum().sort_values(ascending=False)\npercent_total = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)*100\nmissing = pd.concat([total, percent_total], axis=1, keys=[\"Total\", \"Percentage\"])\nmissing_data = missing[missing['Total']>0]\nmissing_data","66738b1e":"plt.figure(figsize=(9,6))\nsns.barplot(x=missing_data.index, y=missing_data['Percentage'], data = missing_data)\nplt.title('Percentage of missing data by feature')\nplt.xlabel('Features', fontsize=14)\nplt.ylabel('Percentage', fontsize=14)\nplt.show()","f6749cb1":"# we can drop education as it doesnt effect heart disease\ndf = df.drop(['education'], axis=1)","f571c468":"print(df.isnull().sum().sum())\ndf=df.dropna()\nprint(df.isnull().sum().sum())\ndf.shape","254e54a6":"df.isna().sum()","93656f55":"#Outliers\ncols =['age','BMI','heartRate','sysBP','totChol','diaBP']\nplt.title(\"OUTLIERS VISUALIZATION\")\nfor i in cols:\n    df[i]\n    sns.distplot(df[i],color='grey')\n    plt.show()","d69bbc80":"df.describe().T","3284ea96":"plt.figure(figsize=(20,10))\nsns.heatmap(df.corr(),linewidths=0.1,annot=True)\n# linewidths is white space between boxes and annot gives value\nplt.show()","106e079e":"sns.boxplot(y='age',x='TenYearCHD',data=df)","df262c23":"sns.violinplot(y='age',x='TenYearCHD',data=df)","b7fe4e3f":"sns.violinplot(y='cigsPerDay',x='TenYearCHD',data=df)","76545480":"sns.violinplot(y='sysBP',x='TenYearCHD',data=df)","144d833d":"sns.boxplot(y='diaBP',x='TenYearCHD',data=df)","ab519d22":"sns.violinplot(y='BMI',x='TenYearCHD',data=df)","7fda5e7b":"sns.boxplot(y='heartRate',x='TenYearCHD',data=df)","f2b44254":"sns.countplot(x=df['male'], hue=df['TenYearCHD'])","e2796711":"sns.countplot(x='currentSmoker',data=df,hue='TenYearCHD')","f1b37888":"sns.countplot(x='prevalentHyp',data=df,hue='TenYearCHD')","59417c99":"sns.countplot(x='BPMeds',data=df,hue='TenYearCHD')","ba3dbebc":"sns.countplot(x='diabetes',data=df,hue='TenYearCHD')","3071a43c":"sns.countplot(x='prevalentStroke',data=df,hue='TenYearCHD')","10b6864f":"plt.figure(figsize=(10,10))\nsns.boxplot(x='TenYearCHD', y='age', data=df, hue='currentSmoker')","e1fd1035":"plt.figure(figsize=(10,10))\nsns.violinplot(x='TenYearCHD', y='age', data=df, hue='currentSmoker', split=True)","7598f7c6":"sns.boxplot(y='sysBP',x='prevalentHyp',data=df)","a1abf3aa":"plt.figure(figsize=(20,10))\nsns.boxplot(y='diaBP',hue='prevalentHyp',data=df,x='TenYearCHD')\n#split=True combines two plots","1c2cbccd":"plt.figure(figsize=(20,10))\nsns.violinplot(y='glucose',hue='diabetes',data=df,x='TenYearCHD',split=True)","fe9a9fa6":"# plot histogram to see the distribution of the data\nfig = plt.figure(figsize = (15,20))\nax = fig.gca()\ndf.hist(ax = ax)\nplt.show()","832a5d3d":"# Identify the features with the most importance for the outcome variable Heart Disease\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\n\nX = df.iloc[:,0:14]  \ny = df.iloc[:,-1]    \n\nbestfeatures = SelectKBest(score_func=chi2, k=10)\nfit = bestfeatures.fit(X,y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n\nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  \nprint(featureScores.nlargest(11,'Score'))  ","1f5c9245":"featureScores = featureScores.sort_values(by='Score', ascending=False)\nfeatureScores","3d7b7f53":"plt.figure(figsize=(20,5))\nsns.barplot(x='Specs', y='Score', data=featureScores, palette = \"plasma\")\nplt.box(False)\nplt.title('Feature importance', fontsize=16)\nplt.xlabel('\\n Features', fontsize=14)\nplt.ylabel('Importance \\n', fontsize=14)\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nplt.show()","e817a497":"# selecting the 10 most impactful features for the target variable\nfeatures_list = featureScores[\"Specs\"].tolist()[:10]\nfeatures_list","d631df58":"df = df[['sysBP', 'glucose','age','totChol','cigsPerDay','diaBP','prevalentHyp','diabetes','BPMeds','male','TenYearCHD']]\ndf","e3849eaf":"# Checking for outliers again\ndf.describe()\nsns.pairplot(df)","93161356":"sns.boxplot(df.totChol)\noutliers = df[(df['totChol'] > 500)] \noutliers","478199df":"#Dropping 2 outliers in cholesterin\ndf = df.drop(df[df.totChol > 599].index)\nsns.boxplot(df.totChol)","88746426":"df_clean = df","8a949200":"scaler = MinMaxScaler(feature_range=(0,1)) \nscaled_df= pd.DataFrame(scaler.fit_transform(df_clean), columns=df_clean.columns)","e7280133":"scaled_df.describe()\ndf.describe()","46f74c50":"y = scaled_df['TenYearCHD']\nX = scaled_df.drop(['TenYearCHD'], axis = 1)\n\n# divide train test: 60 % - 40 %\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state=29)\n","49f55c6d":"print(len(X_train))\nprint(len(X_test))","96226012":"target_count = scaled_df.TenYearCHD.value_counts()\nprint('Class 0:', target_count[0])\nprint('Class 1:', target_count[1])\nprint('Proportion:', round(target_count[0] \/ target_count[1], 2), ': 1')\n\nsns.countplot(scaled_df.TenYearCHD, palette=\"OrRd\")\nplt.box(False)\nplt.xlabel('Heart Disease No\/Yes',fontsize=11)\nplt.ylabel('Patient Count',fontsize=11)\nplt.title('Count Outcome Heart Disease\\n')\nplt.savefig('Balance Heart Disease.png')\nplt.show()","d1ac4692":"# Shuffle df\nshuffled_df = scaled_df.sample(frac=1,random_state=4)\n\n# Put all the fraud class in a separate dataset.\nCHD_df = shuffled_df.loc[shuffled_df['TenYearCHD'] == 1]\n\n#Randomly select 492 observations from the non-fraud (majority class)\nnon_CHD_df = shuffled_df.loc[shuffled_df['TenYearCHD'] == 0].sample(n=611,random_state=42)\n\n# Concatenate both dataframes again\nnormalized_df = pd.concat([CHD_df, non_CHD_df])\n\n# check new class counts\nnormalized_df.TenYearCHD.value_counts()\n\n# plot new count\nsns.countplot(normalized_df.TenYearCHD, palette=\"OrRd\")\nplt.box(False)\nplt.xlabel('Heart Disease No\/Yes',fontsize=11)\nplt.ylabel('Patient Count',fontsize=11)\nplt.title('Count Outcome Heart Disease after Resampling\\n')\n#plt.savefig('Balance Heart Disease.png')\nplt.show()","10711514":"#initialize model\nlogreg = LogisticRegression()\n\n# fit model\nlogreg.fit(X_train, y_train)\n\n# prediction = knn.predict(x_test)\nnormalized_df_logreg_pred = logreg.predict(X_test)\n\n# check accuracy: Accuracy: Overall, how often is the classifier correct? Accuracy = (True Pos + True Negative)\/total\nacc = accuracy_score(y_test, normalized_df_logreg_pred)\nprint(f\"The accuracy score for LogisticRegression is: {round(acc,3)*100}%\")\n\n# f1 score: The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0.\nf1 = f1_score(y_test, normalized_df_logreg_pred)\nprint(f\"The f1 score for LogisticRegression is: {round(f1,3)*100}%\")\n\n# Precision score: When it predicts yes, how often is it correct? Precision=True Positive\/predicted yes\nprecision = precision_score(y_test, normalized_df_logreg_pred)\nprint(f\"The precision score for LogisticRegression is: {round(precision,3)*100}%\")\n\n# recall score: True Positive Rate(Sensitivity or Recall): When it\u2019s actually yes, how often does it predict yes? True Positive Rate = True Positive\/actual yes\nrecall = recall_score(y_test, normalized_df_logreg_pred)\nprint(f\"The recall score for LogisticRegression is: {round(recall,3)*100}%\")\n","e12c7a60":"knn = KNeighborsClassifier(n_neighbors = 2)\n\n#fit model\nknn.fit(X_train, y_train)\n\n# prediction = knn.predict(x_test)\nnormalized_df_knn_pred = knn.predict(X_test)\n\n\n# check accuracy: Accuracy: Overall, how often is the classifier correct? Accuracy = (True Pos + True Negative)\/total\nacc = accuracy_score(y_test, normalized_df_knn_pred)\nprint(f\"The accuracy score for KNN is: {round(acc,3)*100}%\")\n\nf1 = f1_score(y_test, normalized_df_knn_pred)\nprint(f\"The f1 score for KNN is: {round(f1,3)*100}%\")\n\n# Precision score: When it predicts yes, how often is it correct? Precision=True Positive\/predicted yes\nprecision = precision_score(y_test, normalized_df_knn_pred)\nprint(f\"The precision score for KNN is: {round(precision,3)*100}%\")\n\n# recall score: True Positive Rate(Sensitivity or Recall): When it\u2019s actually yes, how often does it predict yes? True Positive Rate = True Positive\/actual yes\nrecall = recall_score(y_test, normalized_df_knn_pred)\nprint(f\"The recall score for KNN is: {round(recall,3)*100}%\")","33109290":"#initialize model\ndtc_up = DecisionTreeClassifier()\n\n# fit model\ndtc_up.fit(X_train, y_train)\n\nnormalized_df_dtc_up_pred = dtc_up.predict(X_test)\n\n# check accuracy: Accuracy: Overall, how often is the classifier correct? Accuracy = (True Pos + True Negative)\/total\nacc = accuracy_score(y_test, normalized_df_dtc_up_pred)\nprint(f\"The accuracy score for DTC is: {round(acc,3)*100}%\")\n\n# f1 score: The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0.\nf1 = f1_score(y_test, normalized_df_dtc_up_pred)\nprint(f\"The f1 score for DTC is: {round(f1,3)*100}%\")\n\n# Precision score: When it predicts yes, how often is it correct? Precision=True Positive\/predicted yes\nprecision = precision_score(y_test, normalized_df_dtc_up_pred)\nprint(f\"The precision score for DTC is: {round(precision,3)*100}%\")\n\n# recall score: True Positive Rate(Sensitivity or Recall): When it\u2019s actually yes, how often does it predict yes? True Positive Rate = True Positive\/actual yes\nrecall = recall_score(y_test, normalized_df_dtc_up_pred)\nprint(f\"The recall score for DTC is: {round(recall,3)*100}%\")","c2530af7":"#initialize model\nsvm = SVC()\n\n#fit model\nsvm.fit(X_train, y_train)\n\nnormalized_df_svm_pred = svm.predict(X_test)\n\nprint('Observations:')\n# check accuracy: Accuracy: Overall, how often is the classifier correct? Accuracy = (True Pos + True Negative)\/total\nacc = accuracy_score(y_test, normalized_df_svm_pred)\nprint(f\"The accuracy score for SVM is: {round(acc,3)*100}%\")\n\n# f1 score: The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0.\nf1 = f1_score(y_test, normalized_df_svm_pred)\nprint(f\"The f1 score for SVM is: {round(f1,3)*100}%\")\n\n# Precision score: When it predicts yes, how often is it correct? Precision=True Positive\/predicted yes\nprecision = precision_score(y_test, normalized_df_svm_pred)\nprint(f\"The precision score for SVM is: {round(precision,3)*100}%\")\n\n# recall score: True Positive Rate(Sensitivity or Recall): When it\u2019s actually yes, how often does it predict yes? True Positive Rate = True Positive\/actual yes\nrecall = recall_score(y_test, normalized_df_svm_pred)\nprint(f\"The recall score for SVM is: {round(recall,3)*100}%\")","aee06c78":"rfc =  RandomForestClassifier()\n\n#fit model\nrfc.fit(X_train, y_train)\n\nnormalized_df_rfc_pred = rfc.predict(X_test)\n\nprint('Observations:')\n# check accuracy: Accuracy: Overall, how often is the classifier correct? Accuracy = (True Pos + True Negative)\/total\nacc = accuracy_score(y_test, normalized_df_rfc_pred)\nprint(f\"The accuracy score for RFC is: {round(acc,3)*100}%\")\n\n# f1 score: The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0.\nf1 = f1_score(y_test, normalized_df_rfc_pred)\nprint(f\"The f1 score for RFC is: {round(f1,3)*100}%\")\n\n# Precision score: When it predicts yes, how often is it correct? Precision=True Positive\/predicted yes\nprecision = precision_score(y_test, normalized_df_rfc_pred)\nprint(f\"The precision score for RFC is: {round(precision,3)*100}%\")\n\n# recall score: True Positive Rate(Sensitivity or Recall): When it\u2019s actually yes, how often does it predict yes? True Positive Rate = True Positive\/actual yes\nrecall = recall_score(y_test, normalized_df_rfc_pred)\nprint(f\"The recall score for RFC is: {round(recall,3)*100}%\")","1342e1ef":"nb =  GaussianNB()\n\n#fit model\nnb.fit(X_train, y_train)\n\nnormalized_df_nb_pred = nb.predict(X_test)\n\nprint('Observations:')\n# check accuracy: Accuracy: Overall, how often is the classifier correct? Accuracy = (True Pos + True Negative)\/total\nacc = accuracy_score(y_test, normalized_df_nb_pred)\nprint(f\"The accuracy score for Naive Bayes is: {round(acc,3)*100}%\")\n\n# f1 score: The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0.\nf1 = f1_score(y_test, normalized_df_nb_pred)\nprint(f\"The f1 score for Naive Bayes is: {round(f1,3)*100}%\")\n\n# Precision score: When it predicts yes, how often is it correct? Precision=True Positive\/predicted yes\nprecision = precision_score(y_test, normalized_df_nb_pred)\nprint(f\"The precision score for Naive Bayes is: {round(precision,3)*100}%\")\n\n# recall score: True Positive Rate(Sensitivity or Recall): When it\u2019s actually yes, how often does it predict yes? True Positive Rate = True Positive\/actual yes\nrecall = recall_score(y_test, normalized_df_nb_pred)\nprint(f\"The recall score for Naive Bayes is: {round(recall,3)*100}%\")\n\n\n","e9331cee":"data = {'Model':['Logistic Regression','KNN','Decision Tree','SVM','Random Forest','Naive Bayes'],\n        'F1 Score':[6.60,12.5,22.7,1.70,13.0,26.0],'Accuracies':[84.89,84.1,74.1,84.7,83.89,81.8],'Recall':[3.40,7.30,24.6,0.89,7.80,20.70],'Precision':[72.70,41.50,21.00,100.00,40.00,35.00]}\n\n# Create DataFrame\ndf = pd.DataFrame(data)\n \n# Print the output.\nprint(df)","1defbea6":"Accuracies=[84.89,84.1,73.2,84.7,83.89,81.8]\nAccuracies","69ff8ab4":"plt.figure(figsize=(9,6))\nsns.barplot(x='Model', y='Accuracies', data = df)\nplt.title('Comparison of accuracy of models')\nplt.xlabel('model algorithms', fontsize=14)\nplt.ylabel('Accuracy', fontsize=14)\nplt.show()","11ec51db":"acc_test = logreg.score(X_test, y_test)\nprint(\"The accuracy score of the test data is: \",acc_test*100,\"%\")\nacc_train = logreg.score(X_train, y_train)\nprint(\"The accuracy score of the training data is: \",round(acc_train*100,2),\"%\")\n","0ae676ed":"cnf_matrix_logreg = confusion_matrix(y_test, normalized_df_logreg_pred)\n\nsns.heatmap(pd.DataFrame(cnf_matrix_logreg), annot=True,cmap=\"Reds\" , fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix Logistic Regression\\n', y=1.1)","50fcadba":"fpr, tpr, _ = roc_curve(y_test, normalized_df_logreg_pred)\nauc = roc_auc_score(y_test, normalized_df_logreg_pred)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.box(False)\nplt.title ('ROC CURVE LOGREG')\nplt.show()\n\nprint(f\"The score for the AUC ROC Curve is: {round(auc,3)*100}%\")","bccb6848":"<h3>Observations:<\/h3>\n<ol>\n    <li>Same here as well, it seems as if 90% of stroke patients  get CHD<\/li>\n<\/ol>","96760145":"### Resampling imbalanced Dataset ","1451d55b":"At 9.15%, the blood glucose entry has the highest percentage of missing data. The otherfeatures have very few missing entries.\n","63d09c74":"### Observations:","e266daa5":"# CONCLUSION:","6ef66e7a":"<h3>Observations:<\/h3>\n<ol>\n    <li>Males are at higher risk of getting CHD<\/li>\n<\/ol>","fdbb4ecc":"<h3>Observations:<\/h3>\n<ol>\n    <li>If your heart rate is in range of 70-80 is safe, but if their heart rate goes above or below can cause CHD<\/li>\n<\/ol>","1e1fc725":"## 3.\tCalculate  statistics and EDA of data.","b1234cde":"### Observations:\n    ","ac6aeada":"### AU ROC CURVE LOGISTIC REGRESSION\n","4c540082":"We can say the sysBP , Glucose ,Age are the first three important features in the data .","90a9675b":"# Taining and Testing the Data","2c727cc5":"Feature selection is a technique where we choose those features in our data that contribute most to the target variable. In other words we choose the best predictors for the target variable.\n\nThe classes in the sklearn.feature_selection module can be used for feature selection\/dimensionality reduction on sample sets, either to improve estimators\u2019 accuracy scores or to boost their performance on very high-dimensional datasets.\n\n","cd3e92e8":"##### F1 score : 12.5%","478dcf35":"### Observations:","072100ba":"# Comparing the Models ","e4d1507c":"## 5. Random Forest Classification","e7b3b22d":"The scores for test and training data for the logistic regression model are similar. Therefore we do not expect the model to overfit","5182b33b":"### Observations:\n###### The accuracy score for SVM is: 84.7%\n###### The f1 score for SVM is: 1.70%","828abd08":"### Observations:\n###### The accuracy score for DTC is: 74.1%\n###### The f1 score for DTC is: 22.7%","637c9f78":"### Observations:\n###### Accuracy Score : 83.89%\n###### F1 score : 13.0%","ccaa5519":"## 3. Decision Trees","55f0d1ad":"<h3>Observations:<\/h3>\n<ol>\n    <li>We see that most of smokers having no risk of CHD are in age around 40 years<\/li>\n    <li>most of non-smokers having risk are in age around 65-70 years<\/li>\n    <li>most smokers having risk are in age around 50 years<\/li>\n<\/ol>","7643b0e3":"### Observations:\n###### Accuracy : 81.8%\n###### f1 score :  26.0%","74110b4b":"A Support Vector Machine (SVM) is a discriminative classifier formally defined by a separating hyperplane. In other words, given labeled training data (supervised learning), the algorithm outputs an optimal hyperplane which categorizes new examples. In two dimentional space this hyperplane is a line dividing a plane in two parts where in each class lay in either side.","39142a00":"<h3>Observations:<\/h3>\n<ol>\n    <li>It's weird that patients who didn't smoke suffered from CHD<\/li>\n    <li>More the cigarretes they smoke higher chance of getting CHD <\/li>\n<\/ol>","ca5ea035":"# Checking cross validation","78655106":"Logistic Regression is a Machine Learning classification algorithm that is used to predict the probability of a categorical dependent variable. In logistic regression, the dependent variable is a binary variable that contains data coded as 1 (yes, success, etc.) or 0 (no, failure, etc.).\nIn other words, the logistic regression model predicts P(Y=1) as a function of X.","6a6cffae":"### Observations :\n\n#####  KNearestNeighors performs best at n = 10  with a accuracy of 84.1%","810fdd72":"The data on the prevalent stroke, diabetes, and blood pressure meds are poorly balanced.\nThe no. of cases of CHD is  in patients  suffering from prevalant Stroke\/ diabetes\/taking BP meds is very low compared to those not suffering from it.There is a huge gap between the two extremes of suffering and not suffering\"\n\n","8dd9f129":"# Feature Scaling","c7eff161":"We have observed outliers in totChol and by specifying the range we have  dropped the 2 outliers in totChol.","5a197d6e":"<h3>Observations:<\/h3>\n<ol>\n    <li>Higher sysBP and diaBP, higher the risk of Hypertension, which means higher risk of CHD<\/li>\n<\/ol>","ef686d2b":"### Observations:","00457617":"Naive Bayes classifiers are a collection of classification algorithms based on Bayes\u2019 Theorem. It is not a single algorithm but a family of algorithms where all of them share a common principle, i.e. every pair of features being classified is independent of each other.","b503ccf9":"<h2>1. Read the file and display columns<\/h2>","f89e0315":"## 2. KNN (k-nearest neighbors)","2c36badf":"<h3>Observations:<\/h3>\n<ol>\n    <li>Higher percentage of people having hypertension suffer from CHD<\/li>\n<\/ol>","1fa7e43c":"## 2. Handle missing values, Outliers and Duplicate Data","4d95d196":"These features which have strongest relationship with the output variable are:\n1. Systolic Blood Pressure\n2. Glucose\n3. Age\n4. Cholesterin\n5. Cigarettes per Day\n6. Diastolic Blood Pressure\n7. Hypertensive\n8. Diabetes\n9. Blood Pressure Medication\n10. Gender","13caf035":"By the above visualization we see that all the six models are being compared to eachother with respect to their accuracies .\nLogistic Regression has the highest accuracy in all the models as per the observation in the above barplot.\n\n1. Logistic Regression : 84.89%\n2. KNN                 : 84.10%\n3. Decision Tree       : 73.20%\n4. SVM                 : 84.70%\n5. Random Forest        :83.89%\n6. Naive Bayes         : 81.80%","3814fb86":"## 6. Naive Bayes Algorithm","f01c34f6":"<h3>Observations:<\/h3>\n<ol>\n    <li>It seems as if 50-60% of patients taking BP meds get CHD<\/li>\n<\/ol>","0fe4e1b8":"<h2>Attributes:<\/h2>\n    <ol>\n    <li>male: male(0) or female(1);(Nominal)<\/li>\n    <li>age: age of the patient;(Continuous - Although the recorded ages have been truncated to whole numbers, the concept of age is continuous)<\/li>\n    <li>education<\/li>\n    <li>currentSmoker: whether or not the patient is a current smoker (Nominal)<\/li>\n    <li>cigsPerDay: the number of cigarettes that the person smoked on average in one day.(can be considered continuous as one can have any number of cigarretts, even half a cigarette.)<\/li>\n    <li>BPMeds: whether or not the patient was on blood pressure medication (Nominal)<\/li>\n    <li>prevalentStroke: whether or not the patient had previously had a stroke (Nominal)<\/li>\n    <li>prevalentHyp: whether or not the patient was hypertensive (Nominal)<\/li>\n    <li>diabetes: whether or not the patient had diabetes (Nominal)<\/li>\n    <li>totChol: total cholesterol level (Continuous)<\/li>\n    <li>sysBP: systolic blood pressure (Continuous)<\/li>\n    <li>diaBP: diastolic blood pressure (Continuous)<\/li>\n    <li>BMI: Body Mass Index (Continuous)<\/li>\n    <li>heartRate: heart rate (Continuous - In medical research, variables such as heart rate though in fact discrete, yet are considered continuous because of large number of possible values.)<\/li>\n    <li>glucose: glucose level (Continuous)<\/li>\n    <li>10 year risk of coronary heart disease CHD (binary: \u201c1\u201d, means \u201cYes\u201d, \u201c0\u201d means \u201cNo\u201d) - Target Variable<\/li>\n    <\/ol>","d3ad5b30":"## 4. Support vector Machine","43b5d6fe":"### The algorithms that we  will be used are:  \n1. Logistic Regression\n2. k-Nearest Neighbours\n3. Decision Trees\n4. Support Vector Machine\n5. Random Forest Classification\n6. Naive Bayes","d9e6c0e8":"<h3>Observations:<\/h3>\n<ol>\n    <li>Patients who got CHD are in the age group:50- 65<\/li>\n    <li>Patients around the age group:35- 45 does not suffer from CHD mostly<\/li>\n<\/ol>","6288c4fc":"## 1. Logistic Regression","a901d672":"Logistic regression has the highest accuracy.","e2079c91":"# Feature Selection ","b72ba42e":"# F1 SCORES","d9af0e2a":"#### The following results were found: \n\n#### DATASET :  \nWith the dataset that was provided, age was ranged 30-60 (majority), number of cigsperday() ranges 10-40(majority) ,sysBP ranges 100-200,glucose ranges 65-100(majority),totChol ranges 150-300. \n\nThe above mentioned are the important features that were given by the order of highest importance with the help of Feature Selection. \n\n#### FINAL RESULT :\nThe accuracy was observed the highest at Logistic Regression with -           \n 1. Accuracy of 84.89% \n 2. f1 score of 6.60%\n 3. Precision of 72.7%\n 4. Recall of 3.40%.\n \nTherefore  Logistic Regression model is the recommended model\n\nAs observed by the visualizations,\n1. Age is directly proportional to the target variable (TenYearCHD)\n2. No: of cigs per day  is a major factor for predicting the heart disease .\n3. Diabetic patients those having higher level of glucose ranging from 200-400, have        higher risk of getting CHD.\n4. 90% of stroke patients get CHD\n5. Patients who have higher systole BP have higher chances of getting CHD\n6. Patients whose diastole BP is around 75-80 are mostly safe\n","481c3091":"<h3>Observations:<\/h3>\n<ol>\n    <li>It seems BMI doesn't affect chance of getting CHD<\/li>\n<\/ol>","cdb5c33c":"### Observations:","c8d30d22":"<h3>Observations:<\/h3>\n<ol>\n    <li>\nIn diabetic patients those having higher level of glucose ranging from 200-400, have higher risk of getting CHD\".\n\n<\/li>\n<\/ol>","348ab1c3":"<h2>Objective<\/h2>\n\n<p>Build a classification model that predicts heart disease in a subject. (Note the target column to predict is 'TenYearCHD' where CHD = Coronary heart disease) <\/p>","b67f2a21":"<h3>Observations:<\/h3>\n<ol>\n    <li>It seems as if 60-80% of diabetic patients  get CHD<\/li>\n<\/ol>","c5ea42f0":"Feature scaling is a method used to normalize the range of independent variables or features of data. In data processing, it is also known as data normalization and is generally performed during the data preprocessing step.","02f0119f":"### Undersampling methods","5e115a60":"# Models","4938d094":"### Observations:\n###### The accuracy score for LogisticRegression is: 84.89%\n###### The f1 score for LogisticRegression is: 6.60%","ca0e418f":"## Heart Disease Prediction\n\n","291d9aa4":"### New dataframe with selected features","1ad964d2":"A decision tree is a flowchart-like structure in which each internal node represents a test on a feature (e.g. whether a coin flip comes up heads or tails) , each leaf node represents a class label (decision taken after computing all features) and branches represent conjunctions of features that lead to those class labels. ","95c1d4ed":"The Random Forest Classifier is a set of decision trees from randomly selected subset of training set. It aggregates the votes from different decision trees to decide the final class of the test object.","95694c45":"<h3>Observations:<\/h3>\n<ol>\n    <li>Patients who have higher diastole BP have higher chances of getting CHD<\/li>\n    <li>Patients whose diastole BP is around 75-80 are mostly safe<\/li>\n<\/ol>","0cefb470":"The k-nearest neighbors (KNN) algorithm is a simple, easy-to-implement supervised machine learning algorithm that can be used to solve both classification and regression problems.","e44bd4d7":"<h3>Observations:<\/h3>\n<ol>\n    <li>sysBP and diaBP,currentSmoker and cigsPerDay  are highly correlated with values around 0.8<\/li>\n    <li>sysBP and diaBP and prevalentHyp, diabetes and glucose are correlated to some extent with values arouund 0.62 <\/li>\n<\/ol>","a6230135":"<h3>Observations:<\/h3>\n<ol>\n    <li>Patients who have higher systole BP have higher chances of getting CHD<\/li>\n    <li>Patients whose systole BP is around 120 are mostly safe<\/li>\n<\/ol>"}}