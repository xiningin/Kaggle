{"cell_type":{"2e433231":"code","d43222b9":"code","7bbaab9a":"code","7d13f57a":"code","df9855c5":"code","a0f14dd3":"code","5f9b58e3":"code","8d18ca2e":"code","0b57d9a5":"code","5dcc09de":"code","6f377a68":"code","adcf3995":"code","b0b2009d":"code","21b3ce75":"code","d5834881":"code","23fee3d7":"markdown","510cb6c2":"markdown","1ee1f250":"markdown","496b475c":"markdown","3f9e7d53":"markdown","b6ab3a97":"markdown","c722e312":"markdown","cdbe35f7":"markdown","42cf19a1":"markdown","e5410e98":"markdown","4f182963":"markdown","b39debd6":"markdown"},"source":{"2e433231":"import pandas as pd\nimport missingno as msno\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.impute import SimpleImputer","d43222b9":"df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv', index_col='Id')\nprint(df.shape)\ndf.head()","7bbaab9a":"df.info()","7d13f57a":"df.isnull().sum()","df9855c5":"msno.matrix(df)","a0f14dd3":"missing = df.isnull().sum()\nmissing_percent = 100 * missing \/ len(df)\n# create a dataframe with columns missing, missing_percent\nmissing_table = pd.concat([missing, missing_percent], axis=1)\n# change the column names from 0 and 1 to 'missing_val' and '% of missing_val'\nmissing_table.columns = ['missing_val', '%of missing_val']\n\n# keeping the features with only missing values and sort them\nmissing_table = missing_table.loc[missing_table['missing_val'] != 0].sort_values('missing_val')\nprint('the dataset has total {} columns \\nThere are {} columns \\\nthat have missing values\\n\\n'.format(df.shape[1], missing_table.shape[0]))\nmissing_table","5f9b58e3":"df.select_dtypes(exclude=['object'])","8d18ca2e":"def data_split(df):\n    # split the dataset into train and validation data\n    # ignore the features with object data types\n    y = df.SalePrice\n    XTrain = df.drop(['SalePrice'], axis=1)\n    X = XTrain.select_dtypes(exclude=['object'])\n\n    x_train, x_valid, y_train,y_valid = train_test_split(X, y,test_size=0.3,\n                                                     random_state=42)\n    return x_train, x_valid, y_train,y_valid\n\ndef data_score(xtrain, ytrain,xvalid,yvalid):\n    # returns mean absolute socre on predicted on validation data\n    model = RandomForestRegressor()\n    model.fit(xtrain, ytrain)\n    preds = model.predict(xvalid)\n    return mean_absolute_error(preds, yvalid)","0b57d9a5":"# Droping the rows with missing values\ndf_not_null_rows = df.dropna()\ndf_not_null_rows.shape","5dcc09de":"nul_cols = ['LotFrontage',  'Alley','MasVnrType', 'MasVnrArea','BsmtQual',\n        'BsmtCond','BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Electrical',\n        'FireplaceQu', 'GarageType', 'GarageYrBlt','GarageFinish',  'GarageQual', 'GarageCond',\n        'PoolQC', 'Fence', 'MiscFeature']\n\ndf_not_null = df.drop(nul_cols, axis=1)\nreduced_X_train, reduced_X_valid, y_train, y_valid = data_split(df_not_null)\nprint(\"MAE from droping columns with missing values:\")\nprint(data_score(reduced_X_train, y_train, reduced_X_valid,  y_valid))","6f377a68":"imputer = SimpleImputer()\n\n#using features with only neumeric values for simplicity purpose only\ndf_numeric = df.select_dtypes(exclude=['object'])\nimputed_df = imputer.fit_transform(df_numeric)\n\n# since imputation converts the data frame into a numpy array, \n# let's convert it inot a dataframe back\nimputed_df = pd.DataFrame(imputed_df)\n\nimputed_df.columns = df_numeric.columns\nimputed_df.head()","adcf3995":"imputed_df.isnull().sum().sum()","b0b2009d":"imputed_X_train, imputed_X_valid, y_train, y_valid = data_split(imputed_df)\nprint('MAE from Simple Imputation Approach: ')\nprint(data_score(imputed_X_train,  y_train, imputed_X_valid, y_valid))","21b3ce75":"# making a copy to avoid changing the original data during imputatoin\ndf_copy = df.copy()\n\n#Make new columns with the imputation infromation\nfor col in nul_cols:\n    df_copy[col + '_missing'] = df_copy[col].isnull()\n\nprint(df_copy.columns)\ndf_copy.head()","d5834881":"#remove object features\ndf_copy = df_copy.select_dtypes(exclude=['object'])\n\nimputer = SimpleImputer()\ndf_copy_imputed = pd.DataFrame(imputer.fit_transform(df_copy))\n\n# puting back the column names after being removed by imputation\ndf_copy_imputed.columns = df_copy.columns\n\nimputed_X_train_plus, imputed_X_valid_plus, y_train, y_valid = data_split(df_copy_imputed)\nprint('MAE from an extension to Imputation: ')\nprint(data_score(imputed_X_train_plus, y_train, imputed_X_valid_plus,  y_valid))","23fee3d7":"This idea might be be not as beneficial if we have a dataset with one or two columns and for those columns most of the values (say more than 75%) are missing. Dropping rows in this case will lead to get rid of the most of the dataset. Like in this dataset, if we drop the rows with at least on missing vlaues we end up getting rid of the whole datset. So what should we do? Well, in that case we can drop that one or two specific columns with missing values.\n\n### Idea 1(b): Droping the columns with missing value\nAnother simple idea to deal with missing values is just drop the columns with missing values.\n\n![image.png](attachment:2e9f047e-318f-45ad-a8ee-a2eca30ae455.png)","510cb6c2":"The dataset has 1460 examples with 80 features. Let's take a look into the dataset, data type and if the dataset has any missing values.","1ee1f250":"There are many area ideas that we can use to deal with missing values. In this notebook, I will discuss four of the simplest ideas. To keep things simple, I will avoid additional data cleaning or feature engineering. In order to do that I will work with only numerical features and ignore all object features. \n\nNow let's define a helper function to split the dataset and another function to compute the score using the dataset, so we don't need to write it again and again.","496b475c":"The dataset seems to have a lot of missing values. Let's take a look at the percentage of the data that are missing.","3f9e7d53":"I will go over few simple techiniques to work with a dataset with missing values. I will use house-prices-advanced-regression-techniques dataset to explain the process. ","b6ab3a97":"More than 80% of the data is missing for the features Fence, Alley, MiscFeatures and PoolQC. Now let's see how can we work around this missing values to get a good ML model.\n\nSince the main goal of this notebook is just to work with missing values, I will ignore the features with object data types for the sake of simplicity.","c722e312":"We might want to avoid this approach if most of the columns in the dataset has very few missing values. For example, if more than half of the columns has only 5 or 10 missing values, dropping columns in this case will lead to get rid of the most features of the dataset, which may carry very useful information about the prediction feature. In this situation, we might want to either drop the rows with missing values or we can fill up those missing values with mean or median or mode values of the column. Which leads to the next idea.\n\n### Idea2: Imputation\n**SimpleImputer:** Replace the missing values with the mean vlaue along each column. This simple imptation generally performs very well for most of the dataset. Eventhough there are complex techniques to find the imputed values (e.g regression imputation), they might not bring additional benifits once applying to the machine learning models.\n\n![image.png](attachment:030c7c03-d4eb-4921-a542-da0fc97e9dcf.png)","cdbe35f7":"Now we don't have any missing values in the dataset. Now let's see the mae score for the dataset","42cf19a1":"Now the new dataset has one additional feature for each feature with missing values. The new features has Ture or False values if the data is missing or not missing respectively. Now we are going to impute the missing values and the compute the mae score.","e5410e98":"We see the extension of the imputation approach performs slightly better than the approach with simple imputation on this dataset and than the idea with dropping features with missing values.\n\nOverall we get better performance by removing the columns with null values for this dataset and the result might be totally different for another dataset. There is no rule of thumb about which approach will work best on a dataset. The best idea would be to take try to understand the dataset and check which one works best.\n \nHope this helps. If you have any suggestions please let me know. Thank you for reading. ","4f182963":"We see approach two has little lower mean absolute error than approach with dropping columns with missing values on this dataset.\n\nSince filling missing values with mean, mode or median might not give the exact number the dataset is needed. The exact value could be lower or higher than the imputed values and sometimes the missing rows might some unique similarities that can help to get a better ML predictor. So along with filling the missing values by imputation, it is also a good practice to keep tract of the features\/columns where the missing values was. The next idea will describes how to apply this technique.\n\n### Idea-3: An Extension to Imputation\nNow I will apply imputation technique as before but this time I will keep track of what which values were imputed. Meaning, I will add one additional column for each column with null values, which have False or True values just to keep track of where the value of original columns was null.\n\n![image.png](attachment:1ecb4e63-ef1c-4056-9a60-8b6a0ae99830.png)\n\nThe reason of doing this is, imputed values are not the exact values that the dataset is missing, could be larger or smaller than the actual value. Also there is a change that the rows with missing values are unique in some ways.\n\nUsing this imputation technique sometime helps the ML model to make better predictions.","b39debd6":"### Idea 1(a): Droping the Rows with missing values\nOne simple idea is just drop the rows with null values.\n\n![image.png](attachment:b81c0538-6f23-476e-a6ed-2302ece4988f.png)\n"}}