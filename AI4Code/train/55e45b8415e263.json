{"cell_type":{"8c12f0e3":"code","5ad0a893":"code","b6a20d1b":"code","f59b6957":"code","daebbed6":"code","75d965ef":"code","a180cae0":"code","a375ee29":"code","2e44cfbf":"code","b7b58fc4":"code","78167984":"code","a68a5fac":"code","0e6e1f75":"code","fe467f16":"code","e8aec18c":"code","71441c1b":"code","ee00cb86":"code","bb81a1ee":"code","2a51ccbc":"code","b5ec8519":"code","eb6b9827":"code","7b047722":"code","d5deb4ed":"code","a91a517c":"code","af76f88f":"code","3c3abf73":"code","48318104":"code","b745e479":"code","61d1b3cc":"code","96c5d8f2":"code","4aef6465":"code","c6e78c4e":"code","b27dc553":"code","df7a2f4c":"code","7d57d76c":"code","d2d41109":"code","a7e90a6b":"code","acd24eed":"code","07212a19":"code","9c6cc93b":"code","71eec908":"code","13f8ad1c":"code","1d188c30":"code","7fa2878b":"code","917128e5":"code","a444b575":"code","c92a6f58":"code","04a6e56c":"code","00c0986d":"code","66c20ac4":"code","86bd6c4e":"code","4d7359a4":"code","762d22e5":"markdown","82449750":"markdown","ca96c943":"markdown","1c7f59ab":"markdown","710f8702":"markdown","66e15233":"markdown","9ceed8e6":"markdown","29dc7935":"markdown","5c343e79":"markdown","427af3c6":"markdown","5044d1d4":"markdown","cc84d30e":"markdown","db26a401":"markdown","dbd61d57":"markdown","318b915d":"markdown","a7fec14c":"markdown","fb77268c":"markdown","d5bb39ce":"markdown","09392ec0":"markdown"},"source":{"8c12f0e3":"import os\nos.listdir(\"..\/input\/nlp-dataset\")\n\n# \ud30c\uc77c\uc758 \ub514\ub809\ud1a0\ub9ac\ub97c \ud655\uc778\ud55c\ub2e4. (\uc555\ucd95\ud574\uc81c \ud6c4 \ub2e4\uc2dc \uc5c5\ub85c\ub4dc\ud55c \ub370\uc774\ud130\uc14b)","5ad0a893":"import pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib import font_manager, rc\n\nsns.set_style('whitegrid')\n\nimport warnings \nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline","b6a20d1b":"plt.rcParams[\"axes.unicode_minus\"] = False\nfontpath = \"..\/input\/koreanfont\/a3.ttf\"\nfontprop = font_manager.FontProperties(fname = fontpath)","f59b6957":"df_train = pd.read_csv(\"..\/input\/nlp-dataset\/labeledTrainData.tsv\",\n                       header = 0, delimiter = \"\\t\", quoting = 3)\n\ndf_test = pd.read_csv(\"..\/input\/nlp-dataset\/testData.tsv\",\n                      header = 0, delimiter = \"\\t\", quoting = 3)\n\ndf_train.shape\n\n# header = 0 : \ud30c\uc77c\uc758 \uccab \ubc88\uc9f8 \uc904\uc5d0 \uc5f4 \uc774\ub984\uc774 \uc788\uc74c\uc744 \ub098\ud0c0\ub0b8\ub2e4.\n# delimiter = \"\\t\" : \\t\ub294 \ud544\ub4dc\uac00 tab\uc73c\ub85c \uad6c\ubd84\ub418\ub294 \uac83\uc744 \uc758\ubbf8\ud55c\ub2e4.\n# quoting = 3 : 3\uc740 \ud14d\uc2a4\ud2b8\uc758 \uc30d\ub530\uc634\ud45c\ub97c \ubb34\uc2dc\ud558\ub3c4\ub85d \ud55c\ub2e4.","daebbed6":"df_train.head()","75d965ef":"df_test.shape","a180cae0":"df_test.head()","a375ee29":"print(\"Train Columns: \")  \nprint(df_train.columns.values)\nprint(\"----------------------------\")\nprint(\"Test Columns: \")  \nprint(df_test.columns.values)\n\n# Test \ub370\uc774\ud130 \uc14b\uc5d0 \uc5c6\ub294 Sentiment\ub97c \uc608\uce21\ud55c\ub2e4.","2e44cfbf":"df_train.info()\n\n# null value\ub294 \uc5c6\ub2e4.","b7b58fc4":"df_train.describe()\n\n# sentiment\uc758 \ud1b5\uacc4\uac12 \ud655\uc778","78167984":"df_train[\"sentiment\"].value_counts()\n\n# sentiment\uc758 \ud074\ub798\uc2a4\uac00 \ub531 \uc808\ubc18\uc73c\ub85c \ub418\uc5b4\uc788\uc74c\uc744 \uc54c \uc218 \uc788\ub2e4. (\ubd80\uc815, \uae0d\uc815)","a68a5fac":"df_train[\"review\"][0][:700]\n\n# review \uceec\ub7fc\uc744 700\uc790 \uae4c\uc9c0\ub9cc \ud655\uc778\ud574\ubcf8\ub2e4.","0e6e1f75":"!pip install BeautifulSoup4","fe467f16":"from bs4 import BeautifulSoup\n\nexam1 = BeautifulSoup(df_train[\"review\"][0], \"html5lib\")\nprint(df_train[\"review\"][0][:700])\nexam1.get_text()[:700]\n\n# BeautifulSoup\uc744 \ubd88\ub7ec\uc640\uc11c review\ub97c \ud655\uc778\ud55c\ub2e4. \n# \uadf8\ub0e5 print \ud55c\uac83\uacfc exam\uc73c\ub85c \ubd88\ub7ec\uc628 \ud14d\uc2a4\ud2b8\ub97c \ube44\uad50\ud574\ubcf4\uba74 \n# <br \\>\uacfc \uac19\uc740 html \ud0dc\uadf8\ub4e4\uc774 \uc0ac\ub77c\uc9c4 \uac83\uc744 \ubcfc \uc218\uc788\ub2e4.","e8aec18c":"import re\n\nletters_only = re.sub(\"[^a-zA-Z]\", \" \", exam1.get_text())\nletters_only[:700]\n\n# re\ub97c \ubd88\ub7ec\uc640\uc11c \uc815\uaddc\ud45c\ud604\uc2dd\uc73c\ub85c \ud2b9\uc218\ubb38\uc790\ub97c \uc81c\uac70\ud55c\ub2e4.\n# \uc18c\ubb38\uc790\uc640 \ub300\ubb38\uc790\uac00 \uc544\ub2cc \uac83\uc740 \uacf5\ubc31\uc73c\ub85c \ub300\uccb4\ud55c\ub2e4 (re.sub(\"\ubc14\uafd4\uc57c\ud560\uac83\", \"\ubc14\uafb8\uace0\uc2f6\uc740\uac83\"))\n# output\uc744 \ubcf4\uba74 \ud2b9\uc218\ubb38\uc790\ub4e4\uc774 \uc804\ubd80 \uacf5\ubc31\uc73c\ub85c \ub300\uccb4\ub41c \uac83\uc744 \ubcfc \uc218 \uc788\ub2e4.","71441c1b":"lower_case = letters_only.lower()\n\nwords = lower_case.split()\nprint(len(words))\nwords[:10]\n\n# letters_only\ub97c \uc804\ubd80 \uc18c\ubb38\uc790\ub85c \ub300\uccb4\ud574\uc900\ub2e4.\n# split\uc744 \uc0ac\uc6a9\ud558\uc5ec \ub2e8\uc5b4\ub2e8\uc704\ub85c \ub098\ub208\ub2e4. (\ud1a0\ud070\ud654)\n# 437\uac1c\uc758 \ud1a0\ud070\uc73c\ub85c \uc774\ub8e8\uc5b4\uc838 \uc788\ub2e4.","ee00cb86":"import nltk\nfrom nltk.corpus import stopwords\nstopwords.words(\"english\")[:10]\n\n# NLTK\ub97c \ubd88\ub7ec\uc624\uace0, stopwords\uae4c\uc9c0 \ubd88\ub7ec\uc640\uc11c \ud655\uc778\ud574\ubcf8\ub2e4. ","bb81a1ee":"words = [w for w in words if not w in stopwords.words(\"english\")]\nprint(len(words))\nwords[:10]\n\n# words\uc5d0 \ub2f4\uaca8\uc838 \uc788\ub358 \ub2e8\uc5b4\uc5d0 Stopwords\uac00 \uc788\ub2e4\uba74 \uc81c\uac70\ud55c\ub2e4.\n# \uc81c\uac70\ud55c \ud1a0\ud070\ub4e4\uc744 \ud655\uc778\ud55c\ub2e4.\n# \ud1a0\ud070\uc774 437\uac1c\uc5d0\uc11c 219\uac1c\ub85c \uc904\uc5b4\ub4e4\uc5c8\uc74c\uc744 \uc54c \uc218 \uc788\ub2e4.","2a51ccbc":"# \ud3ec\ud130 \uc2a4\ud0dc\uba38\uc758 \uc0ac\uc6a9 \uc608\uc2dc\n\nstemmer = nltk.stem.PorterStemmer()\nprint(stemmer.stem(\"maximum\"))\nprint(\"The stemmed form of running is : {}\".format(stemmer.stem(\"running\")))\nprint(\"The stemmed form of runs is: {}\".format(stemmer.stem(\"runs\")))\nprint(\"Tje stemmed form of run is: {}\".format(stemmer.stem(\"run\")))\n\n# maximum\uc774 \uadf8\ub300\ub85c \ucd9c\ub825\ub41c\ub2e4. \n# run\uc758 \ubcc0\ud615\uc5b4\ub4e4\uc740 run\uc73c\ub85c \uc5b4\uac04\uc774 \ucd94\ucd9c\ub41c\ub2e4.","b5ec8519":"# \ub7ad\ucee4\uc2a4\ud130 \uc2a4\ud0dc\uba38\uc758 \uc0ac\uc6a9 \uc608\uc2dc\n\nfrom nltk.stem.lancaster import LancasterStemmer\n\nlanc_stemmer = LancasterStemmer()\nprint(lanc_stemmer.stem(\"maximum\"))\nprint(\"The stemmed form of running is : {}\".format(lanc_stemmer.stem(\"running\")))\nprint(\"The stemmed form of runs is: {}\".format(lanc_stemmer.stem(\"runs\")))\nprint(\"Tje stemmed form of run is: {}\".format(lanc_stemmer.stem(\"run\")))\n\n# maximum\uc758 \uc5b4\uac04\uc774 maxim\uc73c\ub85c \ucd94\ucd9c\ub41c\ub2e4.\n# run\uc758 \ubcc0\ud615\uc5b4\ub4e4\uc740 \ub9c8\ucc2c\uac00\uc9c0\ub85c run\uc73c\ub85c \uc5b4\uac04\uc774 \ucd94\ucd9c\ub41c\ub2e4.","eb6b9827":"words[:10]\n\n# \ucc98\ub9ac\ud558\uae30 \uc804 \ub2e8\uc5b4\ub4e4\uc744 \ud655\uc778\ud574\ubcf8\ub2e4.\n# going, started \ub4f1 \ubcc0\ud615\ub41c \ub2e8\uc5b4\uac00 \uc788\uc74c\uc744 \uc54c \uc218 \uc788\ub2e4.","7b047722":"from nltk.stem.snowball import SnowballStemmer\n\nstemmer = SnowballStemmer(\"english\")\nwords = [stemmer.stem(w) for w in words]\n\nwords[:10]\n\n# \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c\ub294 \uc2a4\ub178\uc6b0\ubcfc \uc2a4\ud0dc\uba38\ub97c \uc0ac\uc6a9\ud574\uc11c words\uc758 \uc5b4\uac04\uc744 \ucd94\ucd9c\ud574\ubcf8\ub2e4.\n# going, started\ub4f1 \uc5b4\uac04\uc774 \uc798 \ucd94\ucd9c\ub41c \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4. ","d5deb4ed":"from nltk.stem import WordNetLemmatizer\nwordnet_lem = WordNetLemmatizer()\n\nprint(wordnet_lem.lemmatize(\"fly\"))\nprint(wordnet_lem.lemmatize(\"flies\"))\n\nwords = [wordnet_lem.lemmatize(w) for w in words]\n\nwords[:10]\n\n# lemmatizer\ub97c \uc0ac\uc6a9\ud558\uc5ec fly, flies\ub97c \ucc98\ub9ac\ud558\uba74 \ub458 \ub2e4 fly\ub85c \ubc14\ub010\ub2e4.\n# words\ub97c lemmatization \ucc98\ub9ac\ud55c \ud6c4 \uacb0\uacfc\ub97c \ud655\uc778\ud574\ubcf8\ub2e4.\n# stemming\ud55c \uacb0\uacfc\uc640 \ub9c8\ucc2c\uac00\uc9c0\ub85c \ucd9c\ub825\ub41c\ub2e4.","a91a517c":"# \uc704\uc5d0\uc11c \ubc30\uc6b4 \ub0b4\uc6a9\uc744 \ubc14\ud0d5\uc744 \uc804\uccb4\uc801\uc73c\ub85c \uc218\ud589\ud560 \uc218 \uc788\ub3c4\ub85d \ud568\uc218\ub97c \ub9cc\ub4e4\uc5b4 \uc900\ub2e4.\n\ndef review_to_words(raw_review):\n    review_text = BeautifulSoup(raw_review, \"html.parser\").get_text()\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n    words = letters_only.lower().split()\n    stops = set(stopwords.words(\"english\"))\n    meaningful_words = [w for w in words if not w in stops]\n    stemming_words = [stemmer.stem(w) for w in meaningful_words]\n    return(\" \".join(stemming_words))\n\n# 0. def\ub85c \ud568\uc218 \uc120\uc5b8 \n# 1. HTML \uc81c\uac70\n# 2. \uc601\ubb38\uc790\uac00 \uc544\ub2cc \ubb38\uc790\ub294 \uacf5\ubc31\uc73c\ub85c \ubcc0\ud658\n# 3. \uc18c\ubb38\uc790\ub85c \uc804\uccb4 \ubcc0\ud658\n# 4. \ud30c\uc774\uc36c\uc5d0\uc11c\ub294 \ub9ac\uc2a4\ud2b8\ubcf4\ub2e4 \uc138\ud2b8\ub85c \ucc3e\ub294\uac8c \ud6e8\uc52c \ube60\ub974\ub2e4. stopwods\ub97c \uc138\ud2b8\ub85c \ubcc0\ud658\n# 5. Stopwords \ubd88\uc6a9\uc5b4 \uc81c\uac70\n# 6. Stemming\uc73c\ub85c \uc5b4\uac04\ucd94\ucd9c\n# 7. \uacf5\ubc31\uc73c\ub85c \uad6c\ubd84\ub41c \ubb38\uc790\uc5f4\ub85c \uacb0\ud569\ud558\uc5ec \uacb0\uacfc \ubc18\ud658","af76f88f":"clean_review = review_to_words(df_train[\"review\"][0])\nclean_review\n\n# review \ub370\uc774\ud130\uc758 \uccab\ubc88\uc9f8 \ub370\uc774\ud130\ub97c \ud568\uc218\uc5d0 \ub123\uace0 \uc2e4\ud589\ud574\ubcf8\ub2e4.\n# review \ubb38\uc7a5\ub4e4\uc774 \ud1a0\ud070\ud654\ub418\uc5b4 \uae54\ub054\ud558\uac8c \ucc98\ub9ac\ub418\uc5c8\uc74c\uc744 \ud655\uc778\ud55c\ub2e4.","3c3abf73":"num_reviews = df_train[\"review\"].size\nnum_reviews","48318104":"# \uc801\uc6a9\uc2dc\uac04\uc774 \uc624\ub798\uac78\ub9ac\ub294 \ubb38\uc81c\ub85c \uc778\ud574 multiprocessing\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud568\uc218\ub97c \uc801\uc6a9\uc2dc\ucf1c\uc900\ub2e4.\n# multiprocessing\uc744 \uc0ac\uc6a9\ud558\uba74 \ubcf5\uc7a1\ud558\uace0 \uc624\ub798\uac78\ub9ac\ub294 \uc791\uc5c5\uc744 \ubcc4\ub3c4\uc758 \ud504\ub85c\uc138\uc2a4\ub97c \uc0dd\uc131 \ud6c4\n# \ubcd1\ub82c\ucc98\ub9ac\ud574\uc11c \ubcf4\ub2e4 \ube60\ub978 \uc751\ub2f5\ucc98\ub9ac \uc18d\ub3c4\ub97c \uae30\ub300\ud560 \uc218 \uc788\ub294 \uc7a5\uc810\uc774 \uc788\ub2e4.\n# \ucd9c\ucc98: https:\/\/gist.github.com\/yong27\/7869662\n\nfrom multiprocessing import Pool\nimport numpy as np\n\ndef _apply_df(args):\n    df, func, kwargs = args\n    return df.apply(func, **kwargs)\n\ndef apply_by_multiprocessing(df, func, **kwargs):\n    # \ud0a4\uc6cc\ub4dc \ud56d\ubaa9 \uc911 workers \ud30c\ub77c\ubbf8\ud130\ub97c \uaebc\ub0c4\n    workers = kwargs.pop(\"workers\")\n    # \uc704\uc5d0\uc11c \uac00\uc838\uc628 workers \uc218\ub85c \ud504\ub85c\uc138\uc2a4 \ud480\uc744 \uc815\uc758\n    pool = Pool(processes = workers)\n    # \uc2e4\ud589\ud560 \ud568\uc218\uc640 \ub370\uc774\ud130\ud504\ub808\uc784\uc744 \uc6cc\ucee4\uc758 \uc218 \ub9cc\ud07c \ub098\ub220\uc11c \uc791\uc5c5\n    result = pool.map(_apply_df, [(d, func, kwargs)\n                                 for d in np.array_split(df, workers)])\n    pool.close()\n    #\uc791\uc5c5 \uacb0\uacfc\ub97c \ud569\uccd0\uc11c \ubc18\ud658\n    return pd.concat(list(result))","b745e479":"%time clean_train_reviews = apply_by_multiprocessing(df_train[\"review\"],review_to_words, workers = 4)","61d1b3cc":"%time clean_test_reviews = apply_by_multiprocessing(df_test[\"review\"],review_to_words, workers = 4)","96c5d8f2":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndef displayWordCloud(data = None, backgroundcolor = \"black\", width = 800, height = 600):\n    wordcloud = WordCloud(stopwords = STOPWORDS,\n                         background_color = backgroundcolor,\n                         width = width, height = height).generate(data)\n    plt.figure(figsize = (15, 10))\n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.show()","4aef6465":"displayWordCloud(\" \".join(clean_train_reviews))","c6e78c4e":"df_train[\"num_words\"] = clean_train_reviews.apply(lambda x : len(str(x).split()))\ndf_train[\"num_uniq_words\"] = clean_train_reviews.apply(lambda x: len(set(str(x).split())))\n\n# \ub2e8\uc5b4 \uac1c\uc218 \uceec\ub7fc \uc0dd\uc131\n# \uc911\ubcf5\uc744 \uc81c\uac70\ud55c unique \ub2e8\uc5b4 \uac1c\uc218 \uceec\ub7fc \uc0dd\uc131","b27dc553":"x = clean_train_reviews[0]\nx = str(x).split()\nprint(len(x))\nx[:10]\n\n# \uccab \ubc88\uc9f8 \ub9ac\ubdf0\uc758 \ub2e8\uc5b4\ub97c \uc138\uc5b4\ubcf4\uba74 219\uac1c\uc774\ub2e4. ","df7a2f4c":"import seaborn as sns\n\nfig, ax = plt.subplots(ncols = 2, figsize = (18, 6))\nprint(\"\ub9ac\ubdf0\ubcc4 \ub2e8\uc5b4 \ud3c9\uade0 \uac12: \", df_train[\"num_words\"].mean())\nprint(\"\ub9ac\ubdf0\ubcc4 \ub2e8\uc5b4 \uc911\uac04 \uac12: \", df_train[\"num_words\"].median())\nsns.distplot(df_train[\"num_words\"], bins = 100, ax = ax[0])\nax[0].axvline(df_train[\"num_words\"].median(), linestyle = \"dashed\")\nax[0].set_title(\"\ub9ac\ubdf0\ubcc4 \ub2e8\uc5b4 \uc218 \ubd84\ud3ec\", fontproperties = fontprop)\n\nprint(\"\ub9ac\ubdf0\ubcc4 \uace0\uc720 \ub2e8\uc5b4 \ud3c9\uade0 \uac12: \", df_train[\"num_uniq_words\"].mean())\nprint(\"\ub9ac\ubdf0\ubcc4 \uace0\uc720 \ub2e8\uc5b4 \uc911\uac04 \uac12: \", df_train[\"num_uniq_words\"].median())\nsns.distplot(df_train[\"num_uniq_words\"], bins = 100, color = \"g\", ax = ax[1])\nax[1].axvline(df_train[\"num_uniq_words\"].median(), linestyle = \"dashed\")\nax[1].set_title(\"\ub9ac\ubdf0\ubcc4 \uace0\uc720 \ub2e8\uc5b4 \uc218 \ubd84\ud3ec\", fontproperties = fontprop)","7d57d76c":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.pipeline import Pipeline\n\n# \ud29c\ud1a0\ub9ac\uc5bc\uacfc \ub2e4\ub974\uac8c \ud30c\ub77c\ubbf8\ud130 \uac12\uc744 \uc218\uc815\n# \ud30c\ub77c\ubbf8\ud130 \uac12\ub9cc \uc218\uc815\ud574\ub3c4 \ub9ac\ub354\ubcf4\ub4dc \uc2a4\ucf54\uc5b4 \ucc28\uc774\uac00 \ud07c \nvectorizer = CountVectorizer(analyzer = \"word\", \n                            tokenizer = None,\n                            preprocessor = None,\n                            stop_words = None,\n                            min_df = 2, # \ud1a0\ud070\uc774 \ub098\ud0c0\ub0a0 \ucd5c\uc18c \ubb38\uc11c \uac1c\uc218\n                            ngram_range = (1, 3), # \uc720\ub2c8\uadf8\ub7a8, \ubc14\uc774\uadf8\ub7a8 \ub4f1 \n                            max_features = 20000 # \ucd5c\ub300 \ud53c\uccd0\uc758 \uac1c\uc218\n                            )\nvectorizer","d2d41109":"# \uc18d\ub3c4 \uac1c\uc120\uc744 \uc704\ud574 \ud30c\uc774\ud504\ub77c\uc778\uc744 \uc0ac\uc6a9\ud558\ub3c4\ub85d \uac1c\uc120\n\npipeline = Pipeline([\n    (\"vect\", vectorizer),\n])","a7e90a6b":"%time train_data_features = pipeline.fit_transform(clean_train_reviews)\n\ntrain_data_features","acd24eed":"train_data_features.shape\n\n# 25000\uc758 \uad00\uce21\uce58\uc640 \uc704\uc5d0\uc11c \uc9c0\uc815\ud574\uc8fc\uc5c8\ub358 20000\uac1c\uc758 feature\ub85c \uc774\ub8e8\uc5b4\uc838 \uc788\uc74c.","07212a19":"vocab = vectorizer.get_feature_names()\nprint(len(vocab))\nvocab[:10]\n\n# feature\uc758 \uc774\ub984 (\ub2e8\uc5b4)\ub97c \ud655\uc778","9c6cc93b":"dist = np.sum(train_data_features, axis = 0)\n\nfor tag, count in zip(vocab, dist):\n    print(count, tag)\n    \npd.DataFrame(dist, columns = vocab)\n\n# \ub2e8\uc5b4\ub97c count\ud574\uc918\uc11c \ud55c\ubc88\uc5d0 \ud655\uc778 ","71eec908":"pd.DataFrame(train_data_features[:10].toarray(), columns = vocab).head()\n\n# \uac01\uac01\uc758 row\uac00 \uc5b4\ub5a4 \ub2e8\uc5b4\ub97c \ud3ec\ud568\ud558\uace0 \uc788\ub294\uc9c0 \ud655\uc778\ud558\uae30 \uc704\ud568 ","13f8ad1c":"from sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(n_estimators = 100, random_state = 42, n_jobs =1)\nmodel","1d188c30":"%time model = model.fit(train_data_features, df_train[\"sentiment\"])","7fa2878b":"from sklearn.model_selection import cross_val_score\n%time score = np.mean(cross_val_score(\\\n                                     model, train_data_features,\\\n                                     df_train[\"sentiment\"], cv = 10,\\\n                                     scoring = \"roc_auc\"))","917128e5":"clean_test_reviews[0]","a444b575":"%time test_data_features = pipeline.transform(clean_test_reviews)\ntest_data_features = test_data_features.toarray()\n\n# test \ub370\uc774\ud130\ub3c4 \ub611\uac19\uc774 \ud30c\uc774\ud504\ub77c\uc778\uc744 \uc0ac\uc6a9\ud558\uc5ec \ubca1\ud130\ud654 \uc2dc\ucf1c\uc900\ub2e4.","c92a6f58":"test_data_features","04a6e56c":"# \ubca1\ud130\ud654\ud558\uba70 \ub9cc\ub4e0 \uc0ac\uc804\uc5d0\uc11c \ud574\ub2f9 \ub2e8\uc5b4\uac00 \ubb34\uc5c7\uc778\uc9c0 \ucc3e\uc544\ubcfc \uc218 \uc788\ub2e4.\n# vocab = vectorizer.get_feature_names()\nvocab[8], vocab[2558], vocab[2559], vocab[2560]","00c0986d":"y_pred = model.predict(test_data_features)\ny_pred[:10]","66c20ac4":"sub = pd.DataFrame(data = {\"id\":df_test[\"id\"], \"sentiment\" : y_pred})\nsub.head()","86bd6c4e":"sub.to_csv(\".\/tutorial_1_LB{:.5f}.csv\".format(score), index = False, quoting = 3)","4d7359a4":"sub_sent = sub[\"sentiment\"].value_counts()\nprint(sub_sent[0] - sub_sent[1])\nsub_sent\n\n# submission\uc758 \ubd80\uc815\uacfc \uae0d\uc815\uc758 \ucc28\uc774\ub97c \uc0b4\ud3b4\ubcf8\ub2e4.","762d22e5":"### Sklearn\uc758 CountVectorizer\ub97c \ud1b5\ud574 \ud53c\ucc98 \uc0dd\uc131\n\n* \uc815\uaddc\ud45c\ud604\uc2dd\uc744 \uc0ac\uc6a9\ud574 \ud1a0\ud070\uc744 \ucd94\ucd9c\ud55c\ub2e4.\n* \ubaa8\ub450 \uc18c\ubb38\uc790\ub85c \ubcc0\ud658\uc2dc\ud0a4\uae30 \ub54c\ubb38\uc5d0 good, Good, gOOd\uc774 \ubaa8\ub450 \uac19\uc740 \ud2b9\uc131\uc774 \ub41c\ub2e4.\n* \uc758\ubbf8\uc5c6\ub294 \ud2b9\uc131\uc744 \ub9ce\uc774 \uc0dd\uc131\ud558\uae30 \ub54c\ubb38\uc5d0 \uc801\uc5b4\ub3c4 \ub450 \uac1c\uc758 \ubb38\uc11c\uc5d0 \ub098\ud0c0\ub09c \ud1a0\ud070\ub9cc\uc744 \uc0ac\uc6a9\ud55c\ub2e4.\n* min_df\ub85c \ud1a0\ud070\uc774 \ub098\ud0c0\ub0a0 \ucd5c\uc18c \ubb38\uc11c \uac1c\uc218\ub97c \uc9c0\uc815\ud560 \uc218 \uc788\ub2e4.","82449750":"## **\ud29c\ud1a0\ub9ac\uc5bc \uac1c\uc694**\n\n### Part 1. \n#### - \ucd08\ubcf4\uc790\ub97c \ub300\uc0c1\uc73c\ub85c \ud558\ub294 \uae30\ubcf8 \uc790\uc5f0\uc5b4 \ucc98\ub9ac\ub97c \ub2e4\ub8ec\ub2e4.\n\n### Part 2.\n#### - Word2Vec\uc744 \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378\uc744 \ud559\uc2b5\uc2dc\ud0a4\ub294 \ubc29\ubc95\uacfc \uac10\uc815\ubd84\uc11d\uc5d0 \ub2e8\uc5b4 \ubca1\ud130\ub97c \uc0ac\uc6a9\ud558\ub294 \ubc29\ubc95\uc744 \ubcf8\ub2e4.\n#### - Part 3\uc740 \ub808\uc2dc\ud53c\ub97c \uc81c\uacf5\ud558\uc9c0 \uc54a\uace0 Word2Vec\uc744 \uc0ac\uc6a9\ud558\ub294 \uba87\uac00\uc9c0 \ubc29\ubc95\uc744 \uc2e4\ud5d8\ud574 \ubcf8\ub2e4.\n#### - Part 3\uc5d0\uc11c\ub294 K-means \uc54c\uace0\ub9ac\uc998\uc744 \uc0ac\uc6a9\ud574 \uad70\uc9d1\ud654\ub97c \ud574\ubcf8\ub2e4.\n#### - \uae0d\uc815\uacfc \ubd80\uc815 \ub9ac\ubdf0\uac00 \uc11e\uc5ec\uc788\ub294 100,000\uac1c\uc758 IMDb \uac10\uc815\ubd84\uc11d \ub370\uc774\ud130 \uc14b\uc744 \ud1b5\ud574 \ubaa9\ud45c\ub97c \ub2ec\uc131\ud574 \ubcf8\ub2e4.\n\n## **\ud3c9\uac00 - ROC Curve**","ca96c943":"## Bag-of-words model\n\n### \ud14d\uc2a4\ud2b8 \ub370\uc774\ud130 \ubca1\ud130\ud654","1c7f59ab":"# Part 1. NLP\ub780? & Pandas \ub370\uc774\ud130 \ubd88\ub7ec\uc624\uae30, \uc815\uc81c\ud558\uae30 & \ubca1\ud130\ud654 & \uac10\uc815\ubd84\uc11d\n\n## NLP(\uc790\uc5f0\uc5b4\ucc98\ub9ac)\ub294 \ud14d\uc2a4\ud2b8 \ubb38\uc81c\uc5d0 \uc811\uadfc\ud558\uae30 \uc704\ud55c \uae30\uc220\uc9d1\ud569\uc774\ub2e4. \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c\ub294 IMDb \uc601\ud654 \ub9ac\ubdf0\ub97c \ub85c\ub529\ud558\uace0 \uc815\uc81c\ud558\uace0 \uac04\ub2e8\ud55c BOW \ubaa8\ub378\uc744 \uc801\uc6a9\ud558\uc5ec \ub9ac\ubdf0\uac00 \ucd94\ucc9c\uc778\uc9c0 \uc544\ub2cc\uc9c0\uc5d0 \ub300\ud55c \uc815\ud655\ub3c4\ub97c \uc608\uce21\ud55c\ub2e4.\n\n## Train \ub370\uc774\ud130\uc758 \ud14d\uc2a4\ud2b8\ub97c \ubc14\ud0d5\uc73c\ub85c Test \ub370\uc774\ud130\uc758 Setiment\uc744 \uc608\uce21\n## 0\uc774\uba74 \ubd80\uc815, 1\uc774\uba74 \uae0d\uc815","710f8702":"## BOW(Bag of Words)\n\n### - \uac00\uc7a5 \uac04\ub2e8\ud558\uc9c0\ub9cc \ud6a8\uacfc\uc801\uc774\ub77c \ub110\ub9ac \uc4f0\uc774\ub294 \ubc29\ubc95\n### - \uc7a5, \ubb38\ub2e8, \ubb38\uc7a5, \uc11c\uc2dd\uacfc \uac19\uc740 \uc785\ub825 \ud14d\uc2a4\ud2b8\uc758 \uad6c\uc870\ub97c \uc81c\uc678\ud558\uace0 \uac01 \ub2e8\uc5b4\uac00 \uc774 \ub9d0\ubb49\uce58\uc5d0 \uc5bc\ub9c8\ub098 \ub9ce\uc774 \ub098\ud0c0\ub098\ub294\uc9c0\ub9cc \ud5e4\uc5b4\ub9b0\ub2e4.\n### - \uad6c\uc870\uc640 \uc0c1\uad00\uc5c6\uc774 \ub2e8\uc5b4\uc758 \ucd9c\ud604\ud69f\uc218\ub9cc \uc138\uae30 \ub54c\ubb38\uc5d0 \ud14d\uc2a4\ud2b8\ub97c \ub2f4\ub294 \uac00\ubc29(Bag)\uc774\ub77c\uace0 \uc0dd\uac01\ud560 \uc218 \uc788\ub2e4.\n### - BOW\ub294 \ub2e8\uc5b4\uc758 \uc21c\uc11c\uacfc \uc644\uc804\ud788 \ubb34\uc2dc \ub41c\ub2e4\ub294 \ub2e8\uc810\uc774 \uc788\ub2e4. \uc608\ub97c \ub4e4\uc5b4 \uc758\ubbf8\uac00 \uc644\uc804\ud788 \ubc18\ub300\uc778 \ub450 \ubb38\uc7a5\uc774 \uc788\ub2e4\uace0 \ud558\uba74,\n* It's bad, not good at all.\n* It's good, not bad at all.\n\n### - \uc704 \ub450 \ubb38\uc7a5\uc740 \uc758\ubbf8\uac00 \uc804\ud600 \ubc18\ub300\uc9c0\ub9cc \uc644\uc804\ud788 \ub3d9\uc77c\ud558\uac8c \ubc18\ud658\ub41c\ub2e4.\n### - \uc774\ub97c \ubcf4\uc644\ud558\uae30 \uc704\ud574 n-gram\uc744 \uc0ac\uc6a9\ud558\ub294\ub370 BOW\ub294 \ud558\ub098\uc758 \ud1a0\ud070\uc744 \uc0ac\uc6a9\ud558\uc9c0\ub9cc n-gram\uc740 n\uac1c\uc758 \ud1a0\ud070\uc744 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub3c4\ub85d \ud55c\ub2e4.\n\n### [BOW - \uc704\ud0a4\ub3c5\uc2a4](https:\/\/wikidocs.net\/22650)","66e15233":"![](https:\/\/static.amazon.jobs\/teams\/53\/images\/IMDb_Header_Page.jpg?1501027252)","9ceed8e6":"### \uc6cc\ub4dc \ud074\ub77c\uc6b0\ub4dc\n - \ub2e8\uc5b4\uc758 \ube48\ub3c4 \uc218 \ub370\uc774\ud130\ub97c \uac00\uc9c0\uace0 \uc788\uc744 \ub54c \uc774\uc6a9\ud560 \uc218 \uc788\ub294 \uc2dc\uac01\ud654 \ubc29\ubc95\n - \ub2e8\uc21c\ud788 \ube48\ub3c4 \uc218\ub97c \ud45c\ud604\ud558\uae30 \ubcf4\ub2e4\ub294 \uc0c1\uad00\uad00\uacc4\ub098 \uc720\uc0ac\ub3c4 \ub4f1\uc73c\ub85c \ubc30\uce58\ud558\ub294 \uac8c \ub354 \uc758\ubbf8 \uc788\uae30 \ub54c\ubb38\uc5d0 \ud070 \uc815\ubcf4\ub97c \uc5bb\uae30\ub294 \uc5b4\ub835\ub2e4.","29dc7935":"## RandomForest\ub85c \uc608\uce21\ud574\ubcf4\uae30","5c343e79":"### \ubd88\uc6a9\uc5b4 \uc81c\uac70 (Stopword Removal)\n- \uc77c\ubc18\uc801\uc73c\ub85c \ucf54\ud37c\uc2a4\uc5d0\uc11c \uc790\uc8fc \ub098\ud0c0\ub098\ub294 \ub2e8\uc5b4\ub294 \ud559\uc2b5 \ubaa8\ub378\ub85c\uc11c \ud559\uc2b5\uc774\ub098 \uc608\uce21 \ud504\ub85c\uc138\uc2a4\uc5d0 \uc2e4\uc81c\ub85c \uae30\uc5ec\ud558\uc9c0 \uc54a\uc544 \ub2e4\ub978 \ud14d\uc2a4\ud2b8\uc640 \uad6c\ubcc4\ud558\uc9c0 \ubabb\ud55c\ub2e4.\n- \uc608\ub97c\ub4e4\uc5b4 \uc870\uc0ac, \uc811\ubbf8\uc0ac, i, me, it, this, that, is, are \ub4f1 \uac19\uc740 \ub2e8\uc5b4\ub294 \ube48\ubc88\ud558\uac8c \ub4f1\uc7a5\ud558\uc9c0\ub9cc \uc2e4\uc81c \uc758\ubbf8\ub97c \ucc3e\ub294\ub370 \ud070 \uae30\uc5ec\ub97c \ud558\uc9c0 \uc54a\ub294\ub2e4.\n- Stopwords\ub294 \"to\" \ub610\ub294 \"the\"\uc640 \uac19\uc740 \uc6a9\uc5b4\ub97c \ud3ec\ud568\ud558\ubbc0\ub85c \uc0ac\uc804 \ucc98\ub9ac \ub2e8\uacc4\uc5d0\uc11c \uc81c\uac70\ud558\ub294 \uac83\uc774 \uc88b\ub2e4.\n- NLTK\uc5d0\ub294 153\uac1c\uc758 \uc601\uc5b4 \ubd88\uc6a9\uc5b4\uac00 \ubbf8\ub9ac \uc815\uc758\ub418\uc5b4 \uc788\ub2e4. \n- 17\uac1c\uc758 \uc5b8\uc5b4\uc5d0 \ub300\ud574 \uc815\uc758\ub418\uc5b4 \uc788\uc73c\uba70 \ud55c\uad6d\uc5b4\ub294 \uc5c6\ub2e4. ","427af3c6":"## [\uc790\uc5f0\uc5b4 \ucc98\ub9ac\ub780? - \uc704\ud0a4\ubc31\uacfc](http:\/\/ko.wikipedia.org\/wiki\/\uc790\uc5f0\uc5b4_\ucc98\ub9ac)\n- \uc790\uc5f0\uc5b4 \ucc98\ub9ac(\u81ea\u7136\u8a9e\u8655\u7406) \ub610\ub294 \uc790\uc5f0 \uc5b8\uc5b4 \ucc98\ub9ac(\u81ea\u7136\u8a00\u8a9e\u8655\u7406)\ub294 \uc778\uac04\uc758 \uc5b8\uc5b4 \ud604\uc0c1\uc744 \ucef4\ud4e8\ud130\uc640 \uac19\uc740 \uae30\uacc4\ub97c \uc774\uc6a9\ud574\uc11c \ubaa8\uc0ac \ud560\uc218 \uc788\ub3c4\ub85d \uc5f0\uad6c\ud558\uace0 \uc774\ub97c \uad6c\ud604\ud558\ub294 \uc778\uacf5\uc9c0\ub2a5\uc758 \uc8fc\uc694 \ubd84\uc57c \uc911 \ud558\ub098 (\ucd9c\ucc98: \uc704\ud0a4\ud53c\ub514\uc544)","5044d1d4":"## \ud14d\uc2a4\ud2b8 \ub370\uc774\ud130 \uc804\ucc98\ub9ac \uc774\ud574\ud558\uae30\n### \uc815\uaddc\ud654 normalization\n - \ud55c\uad6d\uc5b4\ub97c \ucc98\ub9ac\ud558\ub294 \uc608\uc2dc (\uc785\ub2c8\ub2fc\u314b\u314b -> \uc785\ub2c8\ub2e4 \u314b\u314b \ub4f1)\n### \ud1a0\ud070\ud654 tokenization\n - \ud55c\uad6d\uc5b4\ub97c \ucc98\ub9ac\ud558\ub294 \uc608\uc2dc (\ud55c\uad6d\uc5b4 Noun, \ub97c Josa, \ucc98\ub9acNoun, \ud558\ub294 Adjective, \uc608\uc2dcNoun)","cc84d30e":"## \ub370\uc774\ud130 \uc815\uc81c Data Cleaning and Text Preprocessing\n\n### \uae30\uacc4\uac00 \ud14d\uc2a4\ud2b8\ub97c \uc774\ud574\ud560 \uc218 \uc788\ub3c4\ub85d \ud14d\uc2a4\ud2b8\ub97c \uc815\uc81c\ud574 \uc900\ub2e4.\n### \uc2e0\ud638\uc640 \uc18c\uc74c\uc744 \uad6c\ubd84\ud55c\ub2e4. \uc544\uc6c3\ub77c\uc774\uc5b4 \ub370\uc774\ud130\ub85c \uc778\ud55c \uc624\ubc84\ud53c\ud305\uc744 \ubc29\uc9c0\ud55c\ub2e4.\n\n1. BeatifulSoup\uc744 \ud1b5\ud574 HTML\ud0dc\uadf8\ub97c \uc81c\uac70\n2. \uc815\uaddc\ud45c\ud604\uc2dd\uc73c\ub85c \uc54c\ud30c\ubcb3 \uc774\uc678\uc758 \ubb38\uc790\ub97c \uacf5\ubc31\uc73c\ub85c \uce58\ud658\n3. NLTK \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud574 \ubd88\uc6a9\uc5b4\uc81c\uac70\n4. \uc5b4\uac04\ucd94\ucd9c(Stemming)\uacfc \uc74c\uc18c\ud45c\uae30\ubc95(Lemmatizing)\uc758 \uac1c\ub150\uc744 \uc774\ud574\ud558\uace0 \uc5b4\uac04\uc744 \ucd94\ucd9c","db26a401":"## Word2Vec\uc774\ub780?\n\n### - \uc790\uc5f0\uc5b4 \ud14d\uc2a4\ud2b8\ub97c \ubd84\uc11d\ud574\uc11c \ud2b9\uc815\ub2e8\uc5b4\ub97c \uc5bc\ub9c8\ub098 \uc0ac\uc6a9\ud588\ub294\uc9c0, \uc5bc\ub9c8\ub098 \uc790\uc8fc \uc0ac\uc6a9\ud588\ub294\uc9c0, \uc5b4\ub5a4 \uc885\ub958\uc758 \ud14d\uc2a4\ud2b8\uc778\uc9c0 \ubd84\ub958\ud558\uac70\ub098 \uae0d\uc815\uc778\uc9c0 \ubd80\uc815\uc778\uc9c0\uc5d0 \ub300\ud55c \uac10\uc815\ubd84\uc11d, \uadf8\ub9ac\uace0 \uc5b4\ub5a4 \ub0b4\uc6a9\uc778\uc9c0 \uc694\uc57d\ud558\ub294 \uc815\ubcf4\ub97c \uc5bb\uc744 \uc218 \uc788\ub2e4.\n\n### - \uac10\uc815\ubd84\uc11d\uc740 \uba38\uc2e0\ub7ec\ub2dd\uc5d0\uc11c \uc5b4\ub824\uc6b4 \uc8fc\uc81c\ub85c \ud48d\uc790, \uc560\ub9e4\ubaa8\ud638\ud55c \ub9d0, \ubc18\uc5b4\ubc95, \uc5b8\uc5b4 \uc720\ud76c\ub85c \ud45c\ud604\uc744 \ud558\ub294\ub370 \uc774\ub294 \uc0ac\ub78c\uacfc \ucef4\ud4e8\ud130\uc5d0\uac8c \ubaa8\ub450 \uc624\ud574\uc758 \uc18c\uc9c0\uac00 \uc788\ub2e4. \uc5ec\uae30\uc5d0\uc11c\ub294 Word2Vec\uc744 \ud1b5\ud55c \uac10\uc815\ubd84\uc11d\uc744 \ud574\ubcf4\ub294 \ud29c\ud1a0\ub9ac\uc5bc\uc744 \ud574\ubcf8\ub2e4.\n\n### - Google\uc758 Word2Vec\uc740 \ub2e8\uc5b4\uc758 \uc758\ubbf8\uc640 \uad00\uacc4\ub97c \uc774\ud574\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub41c\ub2e4.\n\n### - \uc0c1\ub2f9\uc218\uc758 NLP\uae30\ub2a5\uc740 NLTK\ubaa8\ub4c8\uc5d0 \uad6c\ud604\ub418\uc5b4 \uc788\ub294\ub370 \uc774 \ubaa8\ub4c8\uc740 \ucf54\ud37c\uc2a4, \ud568\uc218\uc640 \uc54c\uace0\ub9ac\uc998\uc73c\ub85c \uad6c\uc131\ub418\uc5b4 \uc788\ub2e4. [\ucf54\ud37c\uc2a4\ub780? \ub9d0\ubb49\uce58\ub97c \uc758\ubbf8 (\uc704\ud0a4\ud53c\ub514\uc544)](https:\/\/ko.wikipedia.org\/wiki\/%EB%A7%90%EB%AD%89%EC%B9%98)\n\n### [Word2Vec - \uc704\ud0a4\ub3c5\uc2a4](https:\/\/wikidocs.net\/22660)","dbd61d57":"### \uc2a4\ud14c\ubc0d(\uc5b4\uac04\ucd94\ucd9c, \ud615\ud0dc\uc18c \ubd84\uc11d)\n#### \ucd9c\ucc98: [\uc5b4\uac04\ucd94\ucd9c - \uc704\ud0a4\ud53c\ub514\uc544](https:\/\/ko.wikipedia.org\/wiki\/%EC%96%B4%EA%B0%84_%EC%B6%94%EC%B6%9C)\n\n- \uc5b4\uac04 \ucd94\ucd9c\uc740 \uc5b4\ud615\uc774 \ubcc0\ud615\ub41c \ub2e8\uc5b4\ub85c\ubd80\ud130 \uc811\uc0ac \ub4f1\uc744 \uc81c\uac70\ud558\uace0 \uadf8 \ub2e8\uc5b4\uc758 \uc5b4\uac04\uc744 \ubd84\ub9ac\ud574\ub0b4\ub294 \uac83\n- \"message\", \"messages\", \"messaging\"\uacfc \uac19\uc774 \ubcf5\uc218\ud615, \uc9c4\ud589\ud615 \ub4f1\uc758 \ubb38\uc790\ub97c \uac19\uc740 \uc758\ubbf8\uc758 \ub2e8\uc5b4\ub85c \ub2e4\ub8f0 \uc218 \uc788\ub3c4\ub85d \ub3c4\uc640\uc900\ub2e4.\n- stemming(\ud615\ud0dc\uc18c \ubd84\uc11d): \uc5ec\uae30\uc5d0\uc11c\ub294 NLTK\uc5d0\uc11c \uc81c\uacf5\ud558\ub294 \ud615\ud0dc\uc18c \ubd84\uc11d\uae30\ub97c \uc0ac\uc6a9\ud55c\ub2e4. \uc5ec\ub7ec\uac00\uc9c0 stemmer\ub4e4\uc774 \uc788\ub294\ub370, \ud3ec\ucee4 \ud615\ud0dc\uc18c \ubd84\uc11d\uae30\ub294 \ubcf4\uc218\uc801\uc774\uace0 \ub7ad\ucee4\uc2a4\ud130 \ud615\ud0dc\uc18c \ubd84\uc11d\uae30\ub294 \uc880 \ub354 \uc801\uadf9\uc801\uc774\ub2e4. \n- \ud615\ud0dc\uc18c \ubd84\uc11d \uaddc\uce59\uc758 \uc801\uadf9\uc131 \ub54c\ubb38\uc5d0 \ub7ad\ucee4\uc2a4\ud130 \ud615\ud0dc\uc18c \ubd84\uc11d\uae30\ub294 \ub354 \ub9ce\uc740 \ub3d9\uc74c\uc774\uc758\uc5b4 \ud615\ud0dc\uc18c\ub97c \uc0dd\uc0b0\ud55c\ub2e4. ","318b915d":"#### \ub2e4\uc74c\uc758 \ub450 \ubb38\uc7a5\uc774 \uc788\ub2e4\uace0 \ud558\uc790, \n1. John likes to watch movies. Mary likes movies too.\n2. John also likes to watch football games.\n\n#### \uc704 \ub450 \ubb38\uc7a5\uc744 \ud1a0\ud070\ud654\ud558\uc5ec \uac00\ubc29\uc5d0 \ub2f4\uc544\uc8fc\uba74 \ub2e4\uc74c\uacfc \uac19\ub2e4.\n[\n   \"John\",\n   \"likes\",\n   \"to\",\n   \"watch\",\n   \"movies\",\n   \"Mary\",\n   \"too\",\n   \"also\",\n   \"football\",\n   \"games\"\n]\n#### \uadf8\ub9ac\uace0 \ubc30\uc5f4\uc758 \uc21c\uc11c\ub300\ub85c \uac00\ubc29\uc5d0\uc11c \uac01 \ud1a0\ud070\uc774 \uba87 \ubc88 \ub4f1\uc7a5\ud558\ub294\uc9c0 \ud69f\uc218\ub97c \uc138\uc5b4\uc900\ub2e4. \n\n(1) [1,2,1,1,2,1,1,0,0,0]\n(2) [1,1,1,1,0,0,0,1,1,1]\n#### => \uba38\uc2e0\ub7ec\ub2dd \uc54c\uace0\ub9ac\uc998\uc774 \uc774\ud574\ud560 \uc218 \uc788\ub294 \ud615\ud0dc\ub85c \ubc14\uafd4\uc8fc\ub294 \uc791\uc5c5\uc774\ub2e4.\n#### \ub2e8\uc5b4 \uac00\ubc29\uc744 n-gram\uc744 \uc0ac\uc6a9\ud574 bigram\uc73c\ub85c \ub2f4\uc544\uc8fc\uba74 \ub2e4\uc74c\uacfc \uac19\ub2e4.\n[\n   \"John likes\",\n   \"likes to\",\n   \"to watch\",\n   \"watch movies\",\n   \"Mary likes\",\n   \"likes movies\",\n   \"movies too\"\n]\n#### => \uc5ec\uae30\uc5d0\uc11c\ub294 CountVectorizer\ub97c \ud1b5\ud574 \uc704 \uc791\uc5c5\uc744 \ud55c\ub2e4.","a7fec14c":"# IMDb Review Tutorial \n#### ***[\ucf54\ub4dc\ucd9c\ucc98: \uc624\ub298\ucf54\ub4dc(\ubc15\uc870\uc740 \ub2d8)](https:\/\/github.com\/corazzon)***","fb77268c":"## \ubb38\uc790\uc5f4 \ucc98\ub9ac\n#### \uc704\uc5d0\uc11c \uac04\ub7b5\ud558\uac8c \uc0b4\ud3b4\ubcf8 \ub0b4\uc6a9\uc744 \ubc14\ud0d5\uc73c\ub85c \ubb38\uc790\uc5f4\uc744 \ucc98\ub9ac\ud574 \ubcf8\ub2e4.","d5bb39ce":"### Lemmatization (\uc74c\uc18c\ud45c\uae30\ubc95)\n#### \uc5b8\uc5b4\ud559\uc5d0\uc11c \uc74c\uc18c\ud45c\uae30\ubc95\uc740 \ub2e8\uc5b4\uc758 \ubcf4\uc870 \uc815\ub9ac \ub610\ub294 \uc0ac\uc804 \ud615\uc2dd\uc5d0 \uc758\ud574 \uc2dd\ubcc4\ub418\ub294 \ub2e8\uc77c \ud56d\ubaa9\uc73c\ub85c \ubd84\uc11d \ub420 \uc218 \uc788\ub3c4\ub85d \uad74\uc808 \ub41c \ud615\ud0dc\uc758 \ub2e8\uc5b4\ub97c \uadf8\ub8f9\ud654\ud558\ub294 \uacfc\uc815\uc774\ub2e4. \uc608\ub97c \ub4e4\uc5b4 \ub3d9\uc74c\uc774\uc758\uc5b4\uac00 \ubb38\ub9e5\uc5d0 \ub530\ub77c \ub2e4\ub978 \uc758\ubbf8\ub97c \uac16\ub294\ub370, \n 1. \ubc30\uac00 \ub9db\uc788\ub2e4.\n 2. \ubc30\ub97c \ud0c0\ub294\uac83\uc774 \uc7ac\ubbf8\uc788\ub2e4.\n 3. \ud3c9\uc18c\ubcf4\ub2e4 \ub450 \ubc30\ub85c \ub9ce\uc774 \uba39\uc5b4\uc11c \ubc30\uac00 \uc544\ud504\ub2e4.\n\n#### \uc704\uc5d0 \uc788\ub294 3\uac1c\uc758 \ubb38\uc7a5\uc5d0 \uc788\ub294 \"\ubc30\"\ub294 \ubaa8\ub450 \ub2e4\ub978 \uc758\ubbf8\ub97c \uac16\ub294\ub2e4.\n#### Lemmatization\uc740 \uc774 \ub54c \uc55e\ub4a4 \ubb38\ub9e5\uc744 \ubcf4\uace0 \ub2e8\uc5b4\uc758 \uc758\ubbf8\ub97c \uc2dd\ubcc4\ud558\ub294 \uac83\uc774\ub2e4. \n#### \uc601\uc5b4\uc5d0\uc11c meet\ub294 meeting\uc73c\ub85c \uc4f0\uc600\uc744 \ub54c \ud68c\uc758\ub97c \ub73b\ud558\uc9c0\ub9cc meet\uc77c \ub54c\ub294 \ub9cc\ub098\ub2e4\ub294 \ub73b\uc744 \uac16\ub294\ub370, \uadf8 \ub2e8\uc5b4\uac00 \uba85\uc0ac\ub85c \uc4f0\uc600\ub294\uc9c0 \ub3d9\uc0ac\ub85c \uc4f0\uc600\ub294\uc9c0\uc5d0 \ub530\ub77c \uc801\ud569\ud55c \uc758\ubbf8\ub97c \uac16\ub3c4\ub85d \ucd94\ucd9c\ud558\ub294 \uac83\uc774\ub2e4. \n\n* \ucc38\uace0:\n * [Lemmatization - \uc704\ud0a4\ud53c\ub514\uc544](https:\/\/en.wikipedia.org\/wiki\/Lemmatisation)\n * [Stemming\uacfc Lemmatization\uc758 \ucc28\uc774](https:\/\/4four.us\/article\/2008\/05\/lemmatization)","09392ec0":"## \uc790\uc5f0\uc5b4\ucc98\ub9ac(NLP)\uc640 \uad00\ub828\ub41c \uce90\uae00 Competition\n- [Sentiment Analysis on Movie Review](https:\/\/www.kaggle.com\/c\/sentiment-analysis-on-movie-reviews)\n- [Jigsaw Toxic Comment Classification](https:\/\/www.kaggle.com\/c\/jigsaw-toxic-comment-classification-challenge)\n- [Spooky Author Identification](https:\/\/www.kaggle.com\/c\/spooky-author-identification)"}}