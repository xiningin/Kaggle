{"cell_type":{"98637eae":"code","be56ba8e":"code","a38aff08":"code","47fee201":"code","88a73703":"code","ce9297d3":"code","bec437bd":"code","3b1f0e0e":"code","2bab5e10":"code","6bbf1142":"code","a7bf993f":"code","e37d8ef5":"code","772b8aa5":"code","03a70b10":"code","99edb5dc":"code","d6409202":"code","21850d61":"code","acd22fd7":"code","5cce0c2c":"code","22ff0014":"code","8361c0ad":"code","15d8604d":"code","6c2d60e5":"code","42473639":"code","f588f25f":"code","fbfbc16b":"code","0ad9ca86":"code","479f4632":"code","d74c8c0b":"code","47469e6a":"code","2f7c817f":"code","df354493":"code","39dac307":"code","d260c07e":"code","0b7fcae4":"code","a5e3d82b":"code","830fe9f6":"code","c262d9a1":"code","c622316a":"code","de101541":"code","01a96249":"code","fb309da4":"code","37d0c145":"code","9accd1b0":"code","5b0fb450":"code","7c8254e3":"code","411b67fa":"code","06eb76d8":"code","759a1eca":"code","3f3daed7":"code","2dcdb391":"code","05e7f1a1":"code","a20ebab5":"code","9b326255":"code","baf576d4":"code","a1b459a1":"markdown","f683a4d6":"markdown","55bce01c":"markdown","b17c0ed7":"markdown","f817edb4":"markdown","8177d3cd":"markdown","b892de28":"markdown","560b7bfe":"markdown","06e5aaab":"markdown","8d825577":"markdown","77e5396d":"markdown","8bd8a140":"markdown","f10f865d":"markdown","ecf82780":"markdown","96ebd652":"markdown","ed5c68b3":"markdown","82f32b6e":"markdown","cdfff951":"markdown","5be22fe6":"markdown","9bfab66a":"markdown","680b4abb":"markdown","e4b8988b":"markdown","3c841246":"markdown"},"source":{"98637eae":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import cohen_kappa_score","be56ba8e":"train_df = pd.read_csv('\/kaggle\/input\/liverpool-ion-switching\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/liverpool-ion-switching\/test.csv')","a38aff08":"train_df.head()","47fee201":"train_df.info(null_counts=True)","88a73703":"test_df.head()","ce9297d3":"test_df.info(null_counts=True)","bec437bd":"train_df['open_channels'].value_counts().plot(kind='bar')\nplt.title('Open channels distribution')\nplt.show()","3b1f0e0e":"plt.hist(train_df.signal, bins=20)\nplt.hist(test_df.signal, bins=20)\nplt.title('Signal Distribution for Test and Train')\nplt.legend(labels=['Train', 'Test'])\nplt.show()\nprint('Train mean {}, median {}, standard deviation {}'.format(np.mean(train_df.signal), np.median(train_df.signal), np.std(train_df.signal)))\nprint('Test mean {}, median {}, standard deviation {}'.format(np.mean(test_df.signal), np.median(test_df.signal), np.std(test_df.signal)))\nprint('\\nTrain:', stats.normaltest(train_df.signal))\nprint('Test:', stats.normaltest(train_df.signal))","2bab5e10":"stats.ttest_ind(train_df.signal, test_df.signal)","6bbf1142":"plt.hist(train_df.time)\nplt.hist(test_df.time)\nplt.legend(labels=['Train', 'Test'])\nplt.title('Time Distribution (Just Checking)')\nplt.show()","a7bf993f":"train_df['batch'] = 0\nfor i in range(0, 10):\n    train_df.iloc[i * 500000: 500000 * (i + 1), 3] = i","e37d8ef5":"plt.figure(figsize=(20,10))\nplt.plot(train_df.signal[train_df.time < 3])\nplt.plot(train_df.open_channels[train_df.time < 3])\nplt.legend(labels=['Signal', 'Open Channels'], fontsize=16)\nplt.title('Signal and Open Channels', fontsize=20)\nplt.show()","772b8aa5":"fig, axes = plt.subplots(2, 5, figsize=(16, 8))\nnum_batches = len(train_df.batch.unique())\nfig.suptitle('Signal and Open Channels by Batch. Blue == signal, Orange == open_channels', fontsize=16)\naxis_on = True\nfor i in range(num_batches):\n    axes[i \/\/ (num_batches \/\/ 2), i % (num_batches \/\/ 2)].plot(train_df.signal[train_df.batch == i])\n    axes[i \/\/ (num_batches \/\/ 2), i % (num_batches \/\/ 2)].plot(train_df.open_channels[train_df.batch == i])\n    axes[i \/\/ (num_batches \/\/ 2), i % (num_batches \/\/ 2)].set_yticks(range(-4, 13))\n    if axis_on == False:\n        axes[i \/\/ (num_batches \/\/ 2), i % (num_batches \/\/ 2)].set_xticks([])\n        axes[i \/\/ (num_batches \/\/ 2), i % (num_batches \/\/ 2)].set_yticks([])\n    axis_on = False","03a70b10":"test_df['batch'] = 0\nfor i in range(0, 4):\n    test_df.iloc[i * 500000: 500000 * (i + 1), 2] = i\n\nfig, axes = plt.subplots(2, 2, figsize=(10, 10))\nnum_batches = len(test_df.batch.unique())\nfig.suptitle('Test Distributions', fontsize=16)\naxis_on = True\nfor i in range(num_batches):\n    axes[i \/\/ (num_batches \/\/ 2), i % (num_batches \/\/ 2)].plot(test_df.signal[test_df.batch == i])\n    axes[i \/\/ (num_batches \/\/ 2), i % (num_batches \/\/ 2)].set_yticks(range(-4, 13))\n    if axis_on == False:\n        axes[i \/\/ (num_batches \/\/ 2), i % (num_batches \/\/ 2)].set_xticks([])\n        axes[i \/\/ (num_batches \/\/ 2), i % (num_batches \/\/ 2)].set_yticks([])\n    axis_on = False","99edb5dc":"fig, axes = plt.subplots(4, 3, figsize=(15, 20))\nfig.suptitle('Signal Distributions at Number of Open Channels', fontsize=16)\nfor i in range(11):\n    n, bins, patches = axes[i \/\/ 3, i % 3].hist(train_df.signal[train_df.open_channels == i], bins=40)\n    ind = list(n).index(max(n))\n    mean = round(np.mean(train_df.signal[train_df.open_channels == i]), 2)\n    binned_mode = (bins[ind] + bins[ind + 1])\/2\n    axes[i \/\/ 3, i % 3].set_title('Channels {}, BinMode {}, Mean {}'.format(i, round(binned_mode, 2), mean))\n    axes[i \/\/ 3, i % 3].set_xticks([-5, -2.5, 0, 2.5, 5, 7.5, 10, 12.5])\n    axes[i \/\/ 3, i % 3].axvline(binned_mode , color='orange')\n    axes[i \/\/ 3, i % 3].axvline(mean , color='green')\nplt.show()","d6409202":"start = 0.72\nend = 0.727\nplt.figure(figsize=(20,10))\nplt.plot(train_df.signal[(train_df.time > start) & (train_df.time < end)])\nplt.plot(train_df.open_channels[(train_df.time > start) & (train_df.time < end)])\nplt.legend(['Signal', 'Open Channels'], fontsize=16)\nplt.show()","21850d61":"start = 200.07\nend = 200.08\nplt.figure(figsize=(20,10))\nplt.plot(train_df.signal[(train_df.time > start) & (train_df.time < end)])\nplt.plot(train_df.open_channels[(train_df.time > start) & (train_df.time < end)])\nplt.legend(['Signal', 'Open Channels'], fontsize=16)\nplt.show()","acd22fd7":"start = 310.07\nend = 310.08\nplt.figure(figsize=(20,10))\nplt.plot(train_df.signal[(train_df.time > start) & (train_df.time < end)])\nplt.plot(train_df.open_channels[(train_df.time > start) & (train_df.time < end)])\nplt.legend(['Signal', 'Open Channels'], fontsize=16)\nplt.show()","5cce0c2c":"rfc = RandomForestClassifier(n_estimators=100, max_depth=5)\nX = train_df.drop('open_channels', 1)\nY = train_df.open_channels\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.15)\nrfc.fit(X_train, y_train)\npreds = rfc.predict(X_test)\nprint('Results with just signal and time:', cohen_kappa_score(preds, y_test, weights='quadratic'))","22ff0014":"previous_signal = []\nfor batch in train_df.batch.unique():\n    previous_signal += [train_df[train_df.batch == batch].signal.iloc[0]]\n    previous_signal += list(train_df[train_df.batch == batch].signal.iloc[:-1])\ntrain_df['previous'] = previous_signal\ntrain_df['previous'] = train_df.previous - train_df.signal\n\nsecond_prev = []\nfor batch in train_df.batch.unique():\n    second_prev += list(train_df[train_df.batch == batch].signal.iloc[:2])\n    second_prev += list(train_df[train_df.batch == batch].signal.iloc[:-2])\ntrain_df['second'] = second_prev\ntrain_df['second'] = train_df.second - train_df.signal\n\nthird_prev = []\nfor batch in train_df.batch.unique():\n    third_prev += list(train_df[train_df.batch == batch].signal.iloc[:3])\n    third_prev += list(train_df[train_df.batch == batch].signal.iloc[:-3])\ntrain_df['third'] = third_prev\ntrain_df['third'] = train_df.third - train_df.signal\n\nchunk_size = 20\nbatch_size = len(train_df[train_df.batch == 0])\nmean_chunks = []\nif batch_size \/\/ chunk_size == batch_size \/ chunk_size:\n    for i in range(len(train_df) \/\/ chunk_size):\n        mean_chunks += [np.mean(train_df.signal.iloc[chunk_size * i : chunk_size * (i + 1)])] * chunk_size\nelse:\n    print('Error! Not an even split!')\ntrain_df['MicroMean'] = mean_chunks\n\nchunk_size = 500\nbatch_size = len(train_df[train_df.batch == 0])\nmean_chunks = []\nif batch_size \/\/ chunk_size == batch_size \/ chunk_size:\n    for i in range(len(train_df) \/\/ chunk_size):\n        mean_chunks += [np.mean(train_df.signal.iloc[chunk_size * i : chunk_size * (i + 1)])] * chunk_size\nelse:\n    print('Error! Not an even split!')\ntrain_df['LocalMean'] = mean_chunks\n\nchunk_size = 5000\nbatch_size = len(train_df[train_df.batch == 0])\nmean_chunks = []\nif batch_size \/\/ chunk_size == batch_size \/ chunk_size:\n    for i in range(len(train_df) \/\/ chunk_size):\n        mean_chunks += [np.mean(train_df.signal.iloc[chunk_size * i : chunk_size * (i + 1)])] * chunk_size\nelse:\n    print('Error! Not an even split!')\ntrain_df['MacroMean'] = mean_chunks","8361c0ad":"fig, axes = plt.subplots(3, 2, figsize=(10, 15))\naxes[0, 0].hist(train_df.previous, bins=30)\naxes[0, 0].set_title('Previous')\naxes[0, 1].hist(train_df.second, bins=30)\naxes[0, 1].set_title('Second')\naxes[1, 0].hist(train_df.third, bins=30)\naxes[1, 0].set_title('Third')\naxes[1, 1].hist(train_df.previous, bins=30)\naxes[1, 1].set_title('MicroMean')\naxes[2, 0].hist(train_df.previous, bins=30)\naxes[2, 0].set_title('LocalMean')\naxes[2, 1].hist(train_df.previous, bins=30)\naxes[2, 1].set_title('MacroMean')\nplt.show()","15d8604d":"def plot_features(col_names, start, stop, title, lw):\n    plt.figure(figsize=(20,10))\n    for col in range(len(col_names)):\n        plt.plot(train_df[col_names[col]].iloc[start:stop], lw=lw[col])\n    plt.legend(col_names, fontsize=16)\n    plt.title(title, fontsize=20)\n    plt.show()","6c2d60e5":"plot_features(['signal', 'previous', 'open_channels'], 0, 500000, 'Previous in Batch 0', [1,1,1])","42473639":"plot_features(['signal', 'previous', 'open_channels'], 7215, 7250, '\\\"Previous\\\" Close Up on a Bump', [3,3,3])","f588f25f":"plot_features(['signal', 'second', 'open_channels'], 7215, 7250, '\\\"Second\\\" Close Up on a Bump', [3,3,3])","fbfbc16b":"plot_features(['signal', 'third', 'open_channels'], 7215, 7250, '\\\"Third\\\" Close Up on a Bump', [3,3,3])","0ad9ca86":"plot_features(['signal', 'MicroMean'], 3000000, 3010000, 'Signal and MicroMean in Batch 6', [1, 2])","479f4632":"plot_features(['signal', 'LocalMean'], 3000000, 3010000, 'Signal and LocalMean in Batch 6', [1, 5])","d74c8c0b":"plot_features(['signal', 'MacroMean'], 3000000, 3010000, 'Signal and MacroMean in Batch 6', [1, 5])","47469e6a":"plot_features(['open_channels', 'MicroMean'], 472000, 478000, 'MicroMean in Batch 0', [1, 2])","2f7c817f":"plot_features(['open_channels', 'LocalMean'], 500000, 700000, 'LocalMean in Batch 1', [1, 2])","df354493":"plot_features(['open_channels', 'MacroMean'], 3000000, 3500000, 'MacroMean in Batch 1', [1, 2])","39dac307":"results = []\nfor batch in range(10):\n    batch_results = []\n    rfc = RandomForestClassifier(n_estimators=100, max_depth=4)\n    X = train_df.drop(['open_channels', 'time', 'batch'], 1)[train_df.batch == batch]\n    Y = train_df['open_channels'][train_df.batch == batch]\n    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.15)\n    rfc.fit(X_train, y_train)\n    \n    for feature in zip(X.columns, rfc.feature_importances_):\n        batch_results.append(feature[1])\n    \n    preds = rfc.predict(X_test)\n    batch_results.append(cohen_kappa_score(preds, y_test, weights='quadratic'))\n    results.append(batch_results)\n    \nresults_df = pd.DataFrame()\nresults_df['Signal'] = [item[0] for item in results]\nresults_df['Previous'] = [item[1] for item in results]\nresults_df['Second'] = [item[2] for item in results]\nresults_df['Third'] = [item[3] for item in results]\nresults_df['MicroMean'] = [item[4] for item in results]\nresults_df['LocalMean'] = [item[5] for item in results]\nresults_df['MacroMean'] = [item[6] for item in results]\nresults_df['Kappa'] = [item[7] for item in results]","d260c07e":"results_df","0b7fcae4":"rfc = RandomForestClassifier(n_estimators=100, max_depth=5)\nX = train_df[['signal', 'previous', 'second', 'third', 'MicroMean', 'LocalMean', 'MacroMean']]\nY = train_df['open_channels']\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.15)\nrfc.fit(X_train, y_train)\npreds = rfc.predict(X_test)\nprint('Results with original features:', cohen_kappa_score(preds, y_test, weights='quadratic'))","a5e3d82b":"avg = []\nwin_size = 100\ncs = 500000 #chunk size\nfor i in range(len(train_df)):\n    if (i % cs) - win_size <= 0:\n        avg.append(np.mean(train_df.signal.iloc[(i\/\/cs) * cs : ((i\/\/cs) * cs) + win_size]))\n    else:\n        avg.append(np.mean(train_df.signal.iloc[i - win_size : i]))\ntrain_df['PrevAvgLittle'] = avg\n\navg = []\nfor i in range(len(train_df)):\n    if (i % cs) > cs - win_size:\n        avg.append(np.mean(train_df.signal.iloc[((i\/\/cs + 1) * cs) - win_size : (i\/\/cs + 1) * cs]))\n    else:\n        avg.append(np.mean(train_df.signal.iloc[i : i + win_size]))\ntrain_df['FutAvgLittle'] = avg\n                                \ntrain_df['SlopeLittle'] = train_df['FutAvgLittle'] - train_df['PrevAvgLittle']\n\ntrain_df['PrevAvgLittle'].fillna(method='bfill', inplace=True)\ntrain_df['FutAvgLittle'].fillna(method='ffill', inplace=True)\ntrain_df['SlopeLittle'].fillna(method='bfill', inplace=True)\ntrain_df['SlopeLittle'].fillna(method='ffill', inplace=True)","830fe9f6":"avg = []\nwin_size = 1000\ncs = 500000 #chunk size\nfor i in range(len(train_df)):\n    if (i % cs) - win_size <= 0:\n        avg.append(np.mean(train_df.signal.iloc[(i\/\/cs) * cs : ((i\/\/cs) * cs) + win_size]))\n    else:\n        avg.append(np.mean(train_df.signal.iloc[i - win_size : i]))\ntrain_df['PrevAvgMedium'] = avg\n\navg = []\nfor i in range(len(train_df)):\n    if (i % cs) > cs - win_size:\n        avg.append(np.mean(train_df.signal.iloc[((i\/\/cs + 1) * cs) - win_size : (i\/\/cs + 1) * cs]))\n    else:\n        avg.append(np.mean(train_df.signal.iloc[i : i + win_size]))\ntrain_df['FutAvgMedium'] = avg\n                                \ntrain_df['SlopeMedium'] = train_df['FutAvgMedium'] - train_df['PrevAvgMedium']\n\ntrain_df['PrevAvgMedium'].fillna(method='bfill', inplace=True)\ntrain_df['FutAvgMedium'].fillna(method='ffill', inplace=True)\ntrain_df['SlopeMedium'].fillna(method='bfill', inplace=True)\ntrain_df['SlopeMedium'].fillna(method='ffill', inplace=True)","c262d9a1":"avg = []\nwin_size = 5000\ncs = 500000 #chunk size\nfor i in range(len(train_df)):\n    if (i % cs) - win_size <= 0:\n        avg.append(np.mean(train_df.signal.iloc[(i\/\/cs) * cs : ((i\/\/cs) * cs) + win_size]))\n    else:\n        avg.append(np.mean(train_df.signal.iloc[i - win_size : i]))\ntrain_df['PrevAvgBig'] = avg\n\navg = []\nfor i in range(len(train_df)):\n    if (i % cs) > cs - win_size:\n        avg.append(np.mean(train_df.signal.iloc[((i\/\/cs + 1) * cs) - win_size : (i\/\/cs + 1) * cs]))\n    else:\n        avg.append(np.mean(train_df.signal.iloc[i : i + win_size]))\ntrain_df['FutAvgBig'] = avg\n                                \ntrain_df['SlopeBig'] = train_df['FutAvgBig'] - train_df['PrevAvgBig']\n\ntrain_df['PrevAvgBig'].fillna(method='bfill', inplace=True)\ntrain_df['FutAvgBig'].fillna(method='ffill', inplace=True)\ntrain_df['SlopeBig'].fillna(method='bfill', inplace=True)\ntrain_df['SlopeBig'].fillna(method='ffill', inplace=True)","c622316a":"avg = []\nwin_size = 15000\ncs = 500000 #chunk size\nfor i in range(len(train_df)):\n    if (i % cs) - win_size <= 0:\n        avg.append(np.mean(train_df.signal.iloc[(i\/\/cs) * cs : ((i\/\/cs) * cs) + win_size]))\n    else:\n        avg.append(np.mean(train_df.signal.iloc[i - win_size : i]))\ntrain_df['PrevAvgRealBig'] = avg\n\navg = []\nfor i in range(len(train_df)):\n    if (i % cs) > cs - win_size:\n        avg.append(np.mean(train_df.signal.iloc[((i\/\/cs + 1) * cs) - win_size : (i\/\/cs + 1) * cs]))\n    else:\n        avg.append(np.mean(train_df.signal.iloc[i : i + win_size]))\ntrain_df['FutAvgRealBig'] = avg\n                                \ntrain_df['SlopeRealBig'] = train_df['FutAvgRealBig'] - train_df['PrevAvgRealBig']\n\ntrain_df['PrevAvgRealBig'].fillna(method='bfill', inplace=True)\ntrain_df['FutAvgRealBig'].fillna(method='ffill', inplace=True)\ntrain_df['SlopeRealBig'].fillna(method='bfill', inplace=True)\ntrain_df['SlopeRealBig'].fillna(method='ffill', inplace=True)","de101541":"plot_features(['signal', 'PrevAvgLittle', 'FutAvgLittle', 'SlopeLittle'],\n                     0, 500000, 'Is LocalSlope Showing What We Want?', [1, 5, 5, 1])","01a96249":"plot_features(['signal', 'PrevAvgLittle', 'FutAvgLittle', 'SlopeLittle', 'open_channels'],\n                     470000, 500000, 'Zoomed Once', [1, 3, 3, 1, 1])","fb309da4":"plot_features(['signal', 'PrevAvgLittle', 'FutAvgLittle', 'SlopeLittle', 'open_channels'],\n                     496000, 499000, 'Zoomed Twice', [1, 3, 3, 3, 1])","37d0c145":"plot_features(['signal', 'PrevAvgLittle', 'FutAvgLittle', 'SlopeLittle', 'open_channels'],\n                     498000, 502000, 'Different spot', [1, 3, 3, 3, 1])","9accd1b0":"plot_features(['signal', 'PrevAvgMedium', 'FutAvgMedium', 'SlopeMedium'],\n                     500000, 1000000, 'Medium Slope', [1, 3, 3, 3])","5b0fb450":"plot_features(['signal', 'PrevAvgMedium', 'FutAvgMedium', 'SlopeMedium'],\n                     495000, 505000, 'SlopeMedium', [1, 3, 3, 3])","7c8254e3":"plot_features(['signal', 'PrevAvgBig', 'FutAvgBig', 'SlopeBig'],\n                     500000, 550000, 'Macro Slope', [1, 3, 3, 3])","411b67fa":"plot_features(['signal', 'PrevAvgBig', 'FutAvgBig', 'SlopeBig'],\n                     3000000, 3500000, 'Macro Slope', [1, 3, 3, 3])","06eb76d8":"plot_features(['signal', 'PrevAvgRealBig', 'FutAvgRealBig', 'SlopeRealBig'],\n                     3000000, 3500000, 'Is RealBig Showing What We Want?', [1, 5, 5, 1])","759a1eca":"plt.figure(figsize=(20,10))\nplt.plot(train_df.SlopeRealBig.iloc[3000000:3500000], lw=3)\nplt.plot(train_df.SlopeBig.iloc[3000000:3500000], alpha=.8)\nplt.plot(train_df.SlopeMedium.iloc[3000000:3500000], alpha=.6)\nplt.axhline(0, color='red')\nplt.title('Comparison of Slope Features in Batch 6')\nplt.legend(labels=['SlopeRealBig', 'SlopeBig', 'SlopeMedium'])\nplt.show()","3f3daed7":"results = []\nfor batch in range(10):\n    batch_results = []\n    rfc = RandomForestClassifier(n_estimators=150, max_depth=5)\n    X = train_df.drop(['open_channels', 'time', 'batch'], 1)[train_df.batch == batch]\n    Y = train_df['open_channels'][train_df.batch == batch]\n    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.15)\n    rfc.fit(X_train, y_train)\n    \n    for feature in zip(X.columns, rfc.feature_importances_):\n        batch_results.append(feature[1])\n    \n    preds = rfc.predict(X_test)\n    batch_results.append(cohen_kappa_score(preds, y_test, weights='quadratic'))\n    results.append(batch_results)\n    \nresults_df = pd.DataFrame()\nfor column in range(len(X.columns)):\n    results_df[X.columns[column]] = [item[column] for item in results]","2dcdb391":"results_df","05e7f1a1":"train_df.drop(['SlopeLittle', 'SlopeMedium', 'SlopeBig', 'SlopeRealBig'], 1, inplace=True)","a20ebab5":"rfc = RandomForestClassifier(n_estimators=100, max_depth=5)\nX = train_df.drop(['open_channels', 'time', 'batch'], 1)\nY = train_df['open_channels']\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.15)\nrfc.fit(X_train, y_train)\npreds = rfc.predict(X_test)\nprint('Results with all features:', cohen_kappa_score(preds, y_test, weights='quadratic'))","9b326255":"for i in range(4, 9):\n    prev = []\n    for batch in train_df.batch.unique():\n        prev += list(train_df[train_df.batch == batch].signal.iloc[:i])\n        prev += list(train_df[train_df.batch == batch].signal.iloc[:-i])\n    train_df['{}_prev'.format(i)] = prev\n    train_df['{}_prev'.format(i)] -= train_df.signal","baf576d4":"rfc = RandomForestClassifier(n_estimators=100, max_depth=5)\nX = train_df.drop(['open_channels', 'time', 'batch'], 1)\nY = train_df['open_channels']\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.15)\nrfc.fit(X_train, y_train)\npreds = rfc.predict(X_test)\nprint('Results with all features:', cohen_kappa_score(preds, y_test, weights='quadratic'))","a1b459a1":"The time feature resets every 50. Thank you https:\/\/www.kaggle.com\/artgor\/eda-and-model-qwk-optimization for the following little batch code:","f683a4d6":"Just want to check the distributions of the new features. This result shouldn't be surprising:","55bce01c":"Improvement with these new features. Now I will go back and add more 'previous' features from the first part, since they have been heavy lifters so far.","b17c0ed7":"<h2>Feature Engineering pt. 1 Analysis<\/h2>\n\nIn the previous section I engineered 6 features: The past 3 signal datapoints, and then the localized mean at 3 different window sizes.\n\nSince this is time series data, and from looking at the signal plot it appeared that past datapoints affect the open_channels target variable. The localized means acts as a 'smoothing' for the data as well, which might add insight since the data is subject to quite a bit of random noise. I modeled each chunk of the data seperately using Random Forest and then used the feature importances attribute in scikit-learn.\n\n'Previous', 'Second', and 'Third' were all very low importance for 'flat' shapes of the signal. However, in curved or sloped batches, they increase in importance, with Third being the most important followed by Second and then Previous. In batch 6, Third is a more important feature than signal, which is pretty amazing. Based on this, I want to add features beyond the Third last datapoint. I will have to manage the size of my data however.\n\nThe three localized mean features were also somewhat important. MicroMean is very important in the batch 0, which is not surprising since it just acts as a very localized smoothing feature. In the second batch, all three were important features with MacroMean helping the most. Maybe it was most helpful because of the chaotic start of the data. Interestingly, MacroMean is extremely unimportant for batches 2 - 5\n\n![](http:\/\/)Overall, adding these features increases the kappa score by quite a bit and I will keep all of them.","f817edb4":"<h2> Feature Engineering pt. 2 Analysis<\/h2>\n\nI tried to engineer a rate of change feature, and sort of succeeded. The smaller window slopes are too volitile to be that useful, but the larger window slopes do pick up on the general slope of some of the non-flat batches. Are these features actually useful though? In the next cell I make a table of the feature importances broken down by batch.\n\nI feel like I still haven't gotten to the bottom of why certain batches have such different shapes despite having seemingly similar open_channels. I think I need to spend more time analyzing the open_channels target and look for patterns and how it indicates the overall shape of the signal.","8177d3cd":"Not sure how helpful this previous feature is. If I was just looking at the previous plot I'm not sure I could pinpoint where the open_channels bump would be.","b892de28":"It's possible that the MacroMean feature will help the model learn the overall shape of signal for some of the more interestingly shaped plots, but I'm not sure if that will actually translate to predicting open_channels","560b7bfe":"This problem is going to be harder than it initially looks. Different open_channel inputs can cause some weird looking shapes. There is also a lot of noise in this data to work with. Here are the test distributions:","06e5aaab":"That's all for now, I will continue to look for more features. For the future:\n1. Reconsider slope features and try to understand why they weren't helpful\n2. Engineer some measure of 'jitter' or how bouncy the signal data is\n3. Imagine new features\n4. Pursue RNN models in keras, probably without the extra features and just a souped up model with the signal data\n\nThanks for reading!","8d825577":"<h2>Feature Engineering pt. 1<\/h2>\n\n1. previous: Difference between signal and previous time increment. At the start of a chunk, just sets it to 0.\n2. second: Same but two time increments ago.\n3. third: three increments ago.\n4. MicroMean: mean signal of very closeby neighbors\n5. LocalMean: mean signal of semi-closeby neighbors\n6. MacroMean: mean signal in a large range\n\n![](http:\/\/)I'm going to ignore the test data from here on out in this notebook version, since this notebook is mostly exploratory and I don't feel ready to make submissions yet.","77e5396d":"There is a basic easy to visualize connection between our target variable open_channels and our only feature signal. When the open channels switch is flipped on, the signal spikes up. Here is a breakdown of the 10 different batches of data and their distributions:","8bd8a140":"Actually, our slope features are extremely poor. It's possible that I need a better algorithm for finding the localized slope, but I think it's also likely that slope doesn't actually affect the number of open_channels feature. For now, I will drop these features, but keep the previous and future average features since they seem to actually help.","f10f865d":"This is just a little helper function to automate plotting better","ecf82780":"<h2>Feature Engineering pt. 2<\/h2>\n    \n1. PrevAvgLittle \/ Medium \/ Big \/ Real Big\n2. FutAvgLittle \/ Medium \/ Big \/ Real Big\n3. SlopeLittle \/ Medium \/ Big \/ Real Big\n\nNow I add 12 features that are the average of a previous window, an average of a future window, and their difference in order to find a slope.","96ebd652":"Despite the similar shape, these definitely were NOT both randomly sampled from the same population.","ed5c68b3":"<h2>Initial EDA<\/h2>","82f32b6e":"These 3 zoom ins are from different batches. If you compare the first zoom in with the 3rd, they both are switching back and forth from 0 and 1 open_channels and yet the charge is much higher in the second zoom in. Maybe the signal is dependent on long-term open_channel settings rather than just the immediate number of open channels plus noise.\n\nI will now try to use a basic random forest classifier to predict open_channels using signal and time. I expect this to perform pretty poorly but I want to use it as a baseline for later after I engineer features.","cdfff951":"MicroMean, LocalMean, MacroMean are all acting as 'smoothing' at different window sizes. I think they might be useful for different chunks of the data. Here are some examples for when I think they will help predict open_channels best:","5be22fe6":"Distribution of the signal feature for the train and test data. They appear to have similar distributions, but the mean, median, and standard deviations are all different. Even though they are not normally distributed, both distributions have enough samples to make a t-test valid.","9bfab66a":"Now I will investigate these features visually","680b4abb":"The open_channels target is not evenly distributed, look like it decreases almost linearly.","e4b8988b":"They gave us some real doozies to predict here. Next, I am curious about the distribution of signal broken down by number of open_channels.","3c841246":"There is a large peak and an interesting secondary slope at each open_channels number. The mean does not go up linearly with number of open channels, and neither does the location of the peak. There is an overall trend towards higher signal with more open_channels, but it's clearly much more complex than that. I will now zoom in heavily to see how a change in open_channels affects the signal."}}