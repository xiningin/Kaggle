{"cell_type":{"255109dc":"code","e285326a":"code","2d71672c":"code","f29e495b":"code","1c7895d9":"code","7ddf9e55":"code","07347c00":"code","aa36fbfd":"code","74e61f54":"code","03e5e725":"code","9629d153":"code","59c369b5":"code","64431e87":"code","192bf61a":"code","b1b12523":"code","1306e8e2":"code","35f80ebd":"code","ef2b4936":"code","091bafa8":"code","7c0057dd":"code","7e258543":"code","ee6819ee":"code","1c6575f2":"markdown","8980f846":"markdown","cd3dcaf0":"markdown","ddc95de0":"markdown","746b277f":"markdown","70b77835":"markdown","3d9bea7a":"markdown","5eee50bd":"markdown"},"source":{"255109dc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e285326a":"# importing libraries\nimport pandas as pd\nimport numpy as np\n\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\n\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pl\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\n%matplotlib inline\n\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings(\"ignore\")","2d71672c":"# importing datasets\ncalender= pd.read_csv(\"\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv\")\nprice= pd.read_csv(\"\/kaggle\/input\/m5-forecasting-accuracy\/sell_prices.csv\")\ntrain_val= pd.read_csv(\"\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_validation.csv\")\nsubmission= pd.read_csv(\"\/kaggle\/input\/m5-forecasting-accuracy\/sample_submission.csv\")","f29e495b":"calender.head()","1c7895d9":"import pandas_profiling as npp\nprofile = npp.ProfileReport(calender)\nprofile","7ddf9e55":"train_val.head()","07347c00":"train_val.shape","aa36fbfd":"train_val_state= train_val.drop(labels= ['id', 'item_id', 'dept_id', 'cat_id', 'store_id'],axis=1)\ntrain_val_state= train_val_state.groupby(\"state_id\", as_index=False).sum()\ntrain_val_state= train_val_state.T\ntrain_val_state= train_val_state.rename(columns=train_val_state.iloc[0]).drop(train_val_state.index[0])\ntrain_val_state= train_val_state.reset_index()\ntrain_val_state= train_val_state.rename(columns={\"index\":\"d\"})\ntrain_val_state= pd.merge(train_val_state, calender, how=\"inner\", on=\"d\")\n\n# plotting\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=train_val_state[\"date\"], y=train_val_state[\"CA\"], name=\"CA\", line_color='gray', opacity=1.0))\nfig.add_trace(go.Scatter(x=train_val_state[\"date\"], y=train_val_state[\"TX\"], name=\"TX\", line_color='purple', opacity=1.0))\nfig.add_trace(go.Scatter(x=train_val_state[\"date\"], y=train_val_state[\"WI\"], name=\"WI\", line_color='salmon', opacity=0.8))\nfig.update_layout(title_text='Sales per State over the year', font_size=15)\nfig.show()","74e61f54":"train_val_1= train_val.copy()\ntrain_val_1[\"Total Sale\"]= train_val_1.sum(axis=1)\n\n# plotting\nplt.figure(figsize=(9,5))\nplt.style.use('seaborn-darkgrid')\n\nsns.barplot(train_val_1[\"store_id\"], train_val_1[\"Total Sale\"], palette=\"Reds\")\nplt.title(\"Sale per Store\", fontsize=25)\nplt.xlabel(\"Store Id\", fontsize=15)\nplt.ylabel(\"Total Sale\", fontsize=15)","03e5e725":"train_val_2= train_val.drop(labels= [\"id\", \"dept_id\", \"item_id\"], axis=1)\ntrain_val_2= train_val_2.groupby([\"state_id\", \"store_id\", \"cat_id\"], as_index=False).sum()\ntrain_val_2[\"Total Sale\"]=train_val_2.sum(axis=1)\ntrain_val_2= train_val_2[[\"state_id\", \"store_id\", \"cat_id\", \"Total Sale\"]]\n\n#plotting\nfig = px.bar(train_val_2, x= \"store_id\", y= \"Total Sale\",color= \"cat_id\",  barmode= \"group\",  facet_row= \"state_id\", \n             category_orders= {\"state_id\": [\"CA\", \"TX\", \"WI\"]}, height=600, color_discrete_map={'FOODS':'indigo', 'HOBBIES':'pink', 'HOUSEHOLD':'purple' }\n            )\nfig.update_traces(marker_line_color='peachpuff', marker_line_width=3, opacity=0.9)\n\nfig.update_layout(title_text= \"Category-wise Total Sale per Store per State :\",font_size=12,         \n                  annotations=[dict(text='CA', font_size=20, font_color=\"indianred\"),\n                               dict(text='TX', font_size=20,  font_color=\"indianred\"),\n                               dict(text='WI', font_size=20,  font_color=\"indianred\")])\nfig.show()","9629d153":"train_val_cat= train_val.drop(labels= ['id', 'item_id', 'dept_id', 'state_id', 'store_id'],axis=1)\ntrain_val_cat= train_val_cat.groupby(\"cat_id\", as_index=False).sum()\ntrain_val_cat= train_val_cat.T\ntrain_val_cat= train_val_cat.rename(columns=train_val_cat.iloc[0]).drop(train_val_cat.index[0])\ntrain_val_cat= train_val_cat.reset_index()\ntrain_val_cat= train_val_cat.rename(columns={\"index\":\"d\"})\ntrain_val_cat= pd.merge(train_val_cat, calender, how=\"inner\", on=\"d\")\ntrain_val_cat= train_val_cat[[\"FOODS\",\"HOBBIES\",\"HOUSEHOLD\",\"year\"]]\n# train_val_cat= train_val_cat.groupby(\"year\").sum()\ntrain_val_cat = train_val_cat.groupby('year')['FOODS','HOBBIES','HOUSEHOLD'].sum().T\n\n#plotting\nplt.style.use('seaborn-darkgrid')\ntrain_val_cat.plot(kind='bar',figsize=(12,7), width=0.6, color=[\"palegreen\", \"limegreen\", \"forestgreen\",\"dimgray\",\"black\",\"darkgray\"])\nplt.title(\"Category Sales by year\", fontsize=22)\nplt.show()","59c369b5":"plt.style.use('seaborn-darkgrid')\ncolors= [\"rosybrown\",\"rosybrown\",\"darkcyan\",\"rosybrown\",\"darkcyan\",\"rosybrown\",\"rosybrown\"]\ntrain_val.groupby('dept_id').count()[\"id\"].plot(kind='bar',figsize=(10,6),width= 0.6, edgecolor=\"darkcyan\", linewidth=2, color=colors, title= 'Sales by Department')\n\nplt.show()","64431e87":"CA= train_val_state[train_val_state[\"snap_CA\"]==0]\nCA_snap= train_val_state[train_val_state[\"snap_CA\"]==1]\n\nTX= train_val_state[train_val_state[\"snap_TX\"]==0]\nTX_snap= train_val_state[train_val_state[\"snap_TX\"]==1]\n\nWI= train_val_state[train_val_state[\"snap_WI\"]==0]\nWI_snap= train_val_state[train_val_state[\"snap_WI\"]==1]\n\n# plotting\nfig = make_subplots(rows=1, cols=3, column_widths=[0.4,0.4,0.4], specs=[[{'type':'domain'}, {'type':'domain'}, {'type':'domain'}]])\n\nfig.add_trace(go.Pie(labels=[\"Sale without SNAP\", \"Sale with SNAP\"], values=[CA[\"CA\"].sum(), CA_snap[\"CA\"].sum()]), 1,1)\nfig.add_trace(go.Pie(labels=[\"Sale without SNAP\", \"Sale with SNAP\"], values=[TX[\"TX\"].sum(), TX_snap[\"TX\"].sum()]), 1,2)\nfig.add_trace(go.Pie(labels=[\"Sale without SNAP\", \"Sale with SNAP\"], values=[WI[\"WI\"].sum(), WI_snap[\"WI\"].sum()]), 1,3)\n\nfig.update_traces(hole=.6, hoverinfo=\"label+percent\", textinfo='percent' ,marker=dict(colors=['aquamarine', 'darkturquoise'], line=dict(color='gray', width=2.5)))\n\nfig.update_layout(title_text= \"SNAP Purchase effect on overall sales per State :\",font_size=15,         \n                  annotations=[dict(text='CA', x=0.15, y=0.4, font_size=30, showarrow=True, font_color=\"indianred\"),\n                               dict(text='TX', x=0.50, y=0.4, font_size=30, showarrow=True, font_color=\"indianred\"),\n                               dict(text='WI', x=0.87, y=0.4, font_size=30, showarrow=True, font_color=\"indianred\")])\nfig.show()","192bf61a":"event1= calender[calender[\"event_name_1\"].notnull()]\nevent1.loc[85, \"event_name_1\"]=\"OrthodoxEaster + Easter\"\nevent1.loc[827, \"event_name_1\"]=\"OrthodoxEaster + Cinco De Mayo\"\nevent1.loc[1177, \"event_name_1\"]=\"Easter + OrthodoxEaster\"\nevent1.loc[1233, \"event_name_1\"]=\"NBAFinalsEnd + Father's day\"\nevent1.loc[1968, \"event_name_1\"]=\"NBAFinalsEnd + Father's day\"\n\nevent1= pd.merge(train_val_state[[\"d\",\"CA\",\"TX\",\"WI\"]], event1[[\"d\",\"event_name_1\"]], on=\"d\", how=\"inner\").drop(labels=[\"d\"], axis=1)\nevent1[\"Total Sale\"]= event1[\"CA\"] + event1[\"TX\"] + event1[\"WI\"]\nevent1= event1.groupby(\"event_name_1\", as_index=False).sum() \nevent1= event1.sort_values(\"Total Sale\",ascending=True)\n\n# plotting\nplt.figure(figsize=(10,14))\n\nplt.barh(event1[\"event_name_1\"],event1[\"Total Sale\"], color=\"olive\")\nplt.xlabel(\"Total Sale\", fontsize=18)\nplt.ylabel(\"Events\", fontsize=18)\nplt.title(\"Sale on Events\", fontsize=22)\n# plt.xticks(rotation=\"vertical\")","b1b12523":"# creating time-series of train_validation dataset\ntrain_val_series= train_val[train_val.columns[6:]]\n\n# plotting first four time-series\ngs=gridspec.GridSpec(2,2)\nplt.figure(figsize=(20,5))\n\nax=pl.subplot(gs[0,0])\nplt.plot(train_val_series.iloc[0])\nplt.title(\"First time-series\")\n\nax=pl.subplot(gs[0,1])\nplt.plot(train_val_series.iloc[1])\nplt.title(\"Second time-series\")\n\nax=pl.subplot(gs[1,0])\nplt.plot(train_val_series.iloc[2])\nplt.title(\"Third time-series\")\n\nax=pl.subplot(gs[1,1])\nplt.plot(train_val_series.iloc[3])\nplt.title(\"Fourth time-series\")\n\nplt.tight_layout()","1306e8e2":"# checking the stationarity of the time-series(first 15 time-series):\n\n# Augmented Dickey Fuller(ADF) test:\nfrom statsmodels.tsa.stattools import adfuller\n\nfor i in range(15):\n    result= adfuller(train_val_series.iloc[i])   \n    print(\"\\n\")\n    print(f\"Time-Series {i+1}\")\n    print(\"test statistics:\", result[0])\n    print(\"p-value:\", result[1])\n    print(\"critical values:\")\n    for key,value in result[4].items():\n        print(\"\\t\", key,value)\n\n    if (result[0]< result[4][\"5%\"]) & (result[1]< 0.05):\n        print(\"reject Null Hypothesis: time-series is Stationary\")\n    else:\n        print(\"failed to reject Null Hypothesis: time-series is Non-Stationary\")","35f80ebd":"# Making first time-series as Stationary \ntrain_val_series.iloc[0]= train_val_series.iloc[0] - train_val_series.iloc[0].shift(1)\ntrain_val_series.fillna(0,inplace=True)\n\n# Checking the stationarity of the first time-series\nresult= adfuller(train_val_series.iloc[0])\nprint(f\"Time-Series 1\")\nprint(\"test statistics:\", result[0])\nprint(\"p-value:\", result[1])\nprint(\"critical values:\")\nfor key,value in result[4].items():\n    print(\"\\t\", key,value)\n\nif (result[0]< result[4][\"5%\"]) & (result[1]< 0.05):\n    print(\"reject Null Hypothesis: time-series is Stationary\")\nelse:\n    print(\"failed to reject Null Hypothesis: time-series is Non-Stationary\")\n\n# Plotting    \nplt.figure(figsize=(20,5))    \nplt.plot(train_val_series.iloc[0])\nplt.title(\"First time-series\")","ef2b4936":"# Determining number of lags:\nimport statsmodels.graphics.tsaplots as sgt\n\nplt.figure(figsize=(30,5))\nsgt.plot_pacf(train_val_series.iloc[0], lags=40, zero= False)\nplt.title(\"Partial Auto Correlation\", fontsize=20)\n\nplt.figure(figsize=(30,5))\nsgt.plot_acf(train_val_series.iloc[0], lags=40, zero= False)\nplt.title(\"Auto Correlation\", fontsize=20)","091bafa8":"# MODEL\nimport statsmodels.api as sm\n\nmodel1= sm.tsa.statespace.SARIMAX(train_val_series.iloc[0], order=(1,1,1), seasonal_order=(1,1,1,30))\nmodel1_fit= model1.fit()\n\nmodel1_fit.summary()","7c0057dd":"#defining higher lag model\nmodel2= sm.tsa.statespace.SARIMAX(train_val_series.iloc[0], order=(2,1,1), seasonal_order=(2,1,1,30))\nmodel2_fit= model2.fit()\n\nmodel2_fit.summary()","7e258543":"# Comparing two models\n\n# Log Likelihood Ratio test:\nfrom scipy.stats.distributions import chi2\n \ndef LLR_test(model_1, model_2, DF=1): # takes model to be compared, DF= diff. in model number which is 1\n    L1= model1.fit().llf            # llr test for model 1  \n    L2= model2.fit().llf            # llr test for model 2\n    LR= (2*(L2-L1))                 # diffenence b\/w their llr test\n    p= chi2.sf(LR, DF).round(3)     # should be <0.05\n    return p\nLLR_test(model1, model2, DF=1)","ee6819ee":"plt.figure(figsize=(20,5))\n\nplt.plot(train_val_series.iloc[0])\nplt.plot(model2_fit.fittedvalues)\nplt.title(\"Actual vs Predicted\", fontsize=15)\nplt.legend([\"Actual\", \"Predicted\"])\n\nprint(\"RMSE:\", np.sqrt((sum((train_val_series.iloc[0] - model2_fit.fittedvalues)**2))\/len(train_val_series.iloc[0])))","1c6575f2":"#### *p-value of LLR test is less than 0.05. Also from summary we can observe that log-likelihood(model2)>log-likelihood(model1), AIC(model2)< AIC(model1) and BIC(model2)< BIC(model1). Hence we go for second model.* ","8980f846":"## <font color=blue>ARIMA MODEL<\/font>","cd3dcaf0":"### *The stores in California seems to have more variance in their sales as compared to stores in Texas and Wisconsin.*","ddc95de0":"#### *The first time-series has now become Stationary.*","746b277f":"# <font color=blue> **Exploratory Data analysis** <\/font>","70b77835":"#### *Observing Partial Auto Correlation and Auto Correlation we choose p=1 and q=1*","3d9bea7a":"#### *Checking first 15 time-series we observe that the first time-series is NON-STATIONARY while the rest are STATIONARY.*","5eee50bd":"#### *p-value for auto regression and moving average coefficient is less than 0.05 which shows that we can go for higher model with higher lag.*"}}