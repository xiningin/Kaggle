{"cell_type":{"fe0f9c9e":"code","4692a526":"code","496f94ce":"code","cc6074d2":"code","fb545586":"code","4ca2ebd1":"code","dd223d78":"code","6047e59e":"code","cead3dd8":"code","d55b12fa":"code","097b5e66":"code","eb0d1964":"code","bae146ae":"code","4309bd9e":"code","325d4cc6":"code","d760eac8":"code","ab529be1":"code","08b55093":"code","e8eb7904":"code","ef4423b2":"code","053cf680":"code","8963acdb":"code","0a397472":"code","a4dd593d":"code","b47a2898":"code","248d222a":"code","55d27ae6":"code","46bdd262":"code","b3418081":"code","057fd44a":"code","3a14b94a":"code","85e552d8":"code","b0670325":"code","a71b0184":"code","31b10f49":"code","0e552390":"code","c18d9320":"code","5d14d16c":"code","9c823df0":"code","f664acad":"code","801f017b":"code","0b6c7d0a":"code","b513a467":"code","22ed4dd0":"code","7da9f90f":"code","198c84cc":"code","20a13c45":"code","987f43c3":"code","c09d96a6":"code","b207b28f":"code","a3557d41":"code","0856b9f9":"code","7c51bb28":"markdown","1589670d":"markdown","a8c96382":"markdown","30f0f509":"markdown","261bf8dc":"markdown","b3fcc987":"markdown","a0747fe8":"markdown","0c514dc0":"markdown","d776d95c":"markdown","26275c02":"markdown","94b487ba":"markdown","b0123b36":"markdown","478d1938":"markdown","425bbdde":"markdown","45aa0083":"markdown","07205bc7":"markdown","b9808142":"markdown","9479bd17":"markdown","77372a0a":"markdown","f6a9f21a":"markdown","a505cc28":"markdown","ea4b6e4c":"markdown","1c3fcd71":"markdown","469708b7":"markdown","1e8614f8":"markdown","04ef2aa8":"markdown","86883d2e":"markdown","7d9503f7":"markdown","64ee5b8e":"markdown","bbf993ba":"markdown","5765a31a":"markdown","c0ee6948":"markdown","545b9688":"markdown","c0dadfc5":"markdown","298a7e81":"markdown","a52c5b5b":"markdown","08ba46ef":"markdown","c9288d16":"markdown","aaae4e43":"markdown","457970e6":"markdown","2d7554e8":"markdown","098cd215":"markdown","076a90e0":"markdown"},"source":{"fe0f9c9e":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.impute import SimpleImputer\n\nimport warnings\n\npd.set_option('max_columns', 500)","4692a526":"def make_test(train, test_size, random_state, strat_feat=None):\n    if strat_feat:\n        \n        split = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n\n        for train_index, test_index in split.split(train, train[strat_feat]):\n            train_set = train.loc[train_index]\n            test_set = train.loc[test_index]\n            \n    return train_set, test_set","496f94ce":"df_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\n\ndf_train.head()","cc6074d2":"df_train.info()","fb545586":"train_set, test_set = make_test(df_train, \n                                test_size=0.2, random_state=654, \n                                strat_feat='Neighborhood')","4ca2ebd1":"tmp = train_set[['GrLivArea', 'TotRmsAbvGrd']].copy()\ntmp.head()","dd223d78":"scaler = StandardScaler()  # initialize a StandardScaler object (more on this later)\n\ntmp = scaler.fit_transform(tmp)  # apply a fit and a transform method (more on this later)\n\ntmp","6047e59e":"class df_scaler(TransformerMixin):\n    def __init__(self, method='standard'):\n        self.scl = None\n        self.scale_ = None\n        self.method = method\n        if self.method == 'sdandard':\n            self.mean_ = None\n        elif method == 'robust':\n            self.center_ = None\n\n    def fit(self, X, y=None):\n        if self.method == 'standard':\n            self.scl = StandardScaler()\n            self.scl.fit(X)\n            self.mean_ = pd.Series(self.scl.mean_, index=X.columns)\n        elif self.method == 'robust':\n            self.scl = RobustScaler()\n            self.scl.fit(X)\n            self.center_ = pd.Series(self.scl.center_, index=X.columns)\n        self.scale_ = pd.Series(self.scl.scale_, index=X.columns)\n        return self\n\n    def transform(self, X):\n        # X has to be a dataframe\n        Xscl = self.scl.transform(X)\n        Xscaled = pd.DataFrame(Xscl, index=X.index, columns=X.columns)\n        return Xscaled","cead3dd8":"tmp = train_set[['GrLivArea', 'TotRmsAbvGrd']].copy()\ntmp.head()","d55b12fa":"scaler = df_scaler()  # initialize the oject\n\ntmp = scaler.fit_transform(tmp)  # apply a fit and a transform method we defined above\n\ntmp.head()  # this time it is a dataframe, we can use `head`","097b5e66":"scaler.mean_","eb0d1964":"scaler.scale_","bae146ae":"class general_cleaner(BaseEstimator, TransformerMixin):\n    '''\n    This class applies what we know from the documetation.\n    It cleans some known missing values\n    If flags the missing values\n\n    This process is supposed to happen as first step of any pipeline\n    '''\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        #LotFrontage\n        X.loc[X.LotFrontage.isnull(), 'LotFrontage'] = 0\n        #Alley\n        X.loc[X.Alley.isnull(), 'Alley'] = \"NoAlley\"\n        #MSSubClass\n        X['MSSubClass'] = X['MSSubClass'].astype(str)\n        #MissingBasement\n        fil = ((X.BsmtQual.isnull()) & (X.BsmtCond.isnull()) & (X.BsmtExposure.isnull()) &\n              (X.BsmtFinType1.isnull()) & (X.BsmtFinType2.isnull()))\n        fil1 = ((X.BsmtQual.notnull()) | (X.BsmtCond.notnull()) | (X.BsmtExposure.notnull()) |\n              (X.BsmtFinType1.notnull()) | (X.BsmtFinType2.notnull()))\n        X.loc[fil1, 'MisBsm'] = 0\n        X.loc[fil, 'MisBsm'] = 1 # made explicit for safety\n        #BsmtQual\n        X.loc[fil, 'BsmtQual'] = \"NoBsmt\" #missing basement\n        #BsmtCond\n        X.loc[fil, 'BsmtCond'] = \"NoBsmt\" #missing basement\n        #BsmtExposure\n        X.loc[fil, 'BsmtExposure'] = \"NoBsmt\" #missing basement\n        #BsmtFinType1\n        X.loc[fil, 'BsmtFinType1'] = \"NoBsmt\" #missing basement\n        #BsmtFinType2\n        X.loc[fil, 'BsmtFinType2'] = \"NoBsmt\" #missing basement\n        #BsmtFinSF1\n        X.loc[fil, 'BsmtFinSF1'] = 0 # No bsmt\n        #BsmtFinSF2\n        X.loc[fil, 'BsmtFinSF2'] = 0 # No bsmt\n        #BsmtUnfSF\n        X.loc[fil, 'BsmtUnfSF'] = 0 # No bsmt\n        #TotalBsmtSF\n        X.loc[fil, 'TotalBsmtSF'] = 0 # No bsmt\n        #BsmtFullBath\n        X.loc[fil, 'BsmtFullBath'] = 0 # No bsmt\n        #BsmtHalfBath\n        X.loc[fil, 'BsmtHalfBath'] = 0 # No bsmt\n        #FireplaceQu\n        X.loc[(X.Fireplaces == 0) & (X.FireplaceQu.isnull()), 'FireplaceQu'] = \"NoFire\" #missing\n        #MisGarage\n        fil = ((X.GarageYrBlt.isnull()) & (X.GarageType.isnull()) & (X.GarageFinish.isnull()) &\n              (X.GarageQual.isnull()) & (X.GarageCond.isnull()))\n        fil1 = ((X.GarageYrBlt.notnull()) | (X.GarageType.notnull()) | (X.GarageFinish.notnull()) |\n              (X.GarageQual.notnull()) | (X.GarageCond.notnull()))\n        X.loc[fil1, 'MisGarage'] = 0\n        X.loc[fil, 'MisGarage'] = 1\n        #GarageYrBlt\n        X.loc[X.GarageYrBlt > 2200, 'GarageYrBlt'] = 2007 #correct mistake\n        X.loc[fil, 'GarageYrBlt'] = 0\n        #GarageType\n        X.loc[fil, 'GarageType'] = \"NoGrg\" #missing garage\n        #GarageFinish\n        X.loc[fil, 'GarageFinish'] = \"NoGrg\" #missing\n        #GarageQual\n        X.loc[fil, 'GarageQual'] = \"NoGrg\" #missing\n        #GarageCond\n        X.loc[fil, 'GarageCond'] = \"NoGrg\" #missing\n        #Fence\n        X.loc[X.Fence.isnull(), 'Fence'] = \"NoFence\" #missing fence\n        #Pool\n        fil = ((X.PoolArea == 0) & (X.PoolQC.isnull()))\n        X.loc[fil, 'PoolQC'] = 'NoPool' \n        \n        del X['Id']\n        del X['MiscFeature']\n        del X['MSSubClass']\n        del X['Neighborhood']  # this should be useful\n        del X['Condition1']\n        del X['Condition2']\n        del X['ExterCond']  # maybe ordinal\n        del X['Exterior1st']\n        del X['Exterior2nd']\n        del X['Functional']\n        del X['Heating']\n        del X['PoolQC']\n        del X['RoofMatl']\n        del X['RoofStyle']\n        del X['SaleCondition']\n        del X['SaleType']\n        del X['Utilities']\n        del X['BsmtCond']\n        del X['Electrical']\n        del X['Foundation']\n        del X['Street']\n        del X['Fence']\n        del X['LandSlope']\n        \n        return X","4309bd9e":"tmp = train_set.copy()\n\ngt = general_cleaner()\n\ntmp = gt.fit_transform(tmp)\n\ntmp.head()","325d4cc6":"class df_imputer(BaseEstimator, TransformerMixin):\n    '''\n    Just a wrapper for the SimpleImputer that keeps the dataframe structure\n    '''\n    def __init__(self, strategy='mean'):\n        self.strategy = strategy\n        self.imp = None\n        self.statistics_ = None\n\n    def fit(self, X, y=None):\n        self.imp = SimpleImputer(strategy=self.strategy)\n        self.imp.fit(X)\n        self.statistics_ = pd.Series(self.imp.statistics_, index=X.columns)\n        return self\n\n    def transform(self, X):\n        # X is supposed to be a DataFrame\n        Ximp = self.imp.transform(X)\n        Xfilled = pd.DataFrame(Ximp, index=X.index, columns=X.columns)\n        return Xfilled\n    \n    \nclass df_scaler(BaseEstimator, TransformerMixin):\n    '''\n    Wrapper of StandardScaler or RobustScaler\n    '''\n    def __init__(self, method='standard'):\n        self.scl = None\n        self.scale_ = None\n        self.method = method\n        if self.method == 'sdandard':\n            self.mean_ = None\n        elif method == 'robust':\n            self.center_ = None\n        self.columns = None  # this is useful when it is the last step of a pipeline before the model\n\n    def fit(self, X, y=None):\n        if self.method == 'standard':\n            self.scl = StandardScaler()\n            self.scl.fit(X)\n            self.mean_ = pd.Series(self.scl.mean_, index=X.columns)\n        elif self.method == 'robust':\n            self.scl = RobustScaler()\n            self.scl.fit(X)\n            self.center_ = pd.Series(self.scl.center_, index=X.columns)\n        self.scale_ = pd.Series(self.scl.scale_, index=X.columns)\n        return self\n\n    def transform(self, X):\n        # assumes X is a DataFrame\n        Xscl = self.scl.transform(X)\n        Xscaled = pd.DataFrame(Xscl, index=X.index, columns=X.columns)\n        self.columns = X.columns\n        return Xscaled\n\n    def get_feature_names(self):\n        return list(self.columns)  # this is going to be useful when coupled with FeatureUnion\n    \n\nclass dummify(BaseEstimator, TransformerMixin):\n    '''\n    Wrapper for get dummies\n    '''\n    def __init__(self, drop_first=False, match_cols=True):\n        self.drop_first = drop_first\n        self.columns = []  # useful to well behave with FeatureUnion\n        self.match_cols = match_cols\n\n    def fit(self, X, y=None):\n        self.columns = []  # for safety, when we refit we want new columns\n        return self\n    \n    def match_columns(self, X):\n        miss_train = list(set(X.columns) - set(self.columns))\n        miss_test = list(set(self.columns) - set(X.columns))\n        \n        err = 0\n        \n        if len(miss_test) > 0:\n            for col in miss_test:\n                X[col] = 0  # insert a column for the missing dummy\n                err += 1\n        if len(miss_train) > 0:\n            for col in miss_train:\n                del X[col]  # delete the column of the extra dummy\n                err += 1\n                \n        if err > 0:\n            warnings.warn('The dummies in this set do not match the ones in the train set, we corrected the issue.',\n                         UserWarning)\n            \n        return X\n        \n    def transform(self, X):\n        X = pd.get_dummies(X, drop_first=self.drop_first)\n        if (len(self.columns) > 0): \n            if self.match_cols:\n                X = self.match_columns(X)\n            self.columns = X.columns\n        else:\n            self.columns = X.columns\n        return X\n    \n    def get_features_name(self):\n        return self.columns","d760eac8":"tmp = train_set[['HouseStyle']].copy()\n\ndummifier = dummify()\n\ntmp = dummifier.transform(tmp)  # no reason to call the fit method here\n\ntmp.sample(5)","ab529be1":"class feat_sel(BaseEstimator, TransformerMixin):\n    '''\n    This transformer selects either numerical or categorical features.\n    In this way we can build separate pipelines for separate data types.\n    '''\n    def __init__(self, dtype='numeric'):\n        self.dtype = dtype\n\n    def fit( self, X, y=None ):\n        return self \n\n    def transform(self, X, y=None):\n        if self.dtype == 'numeric':\n            num_cols = X.columns[X.dtypes != object].tolist()\n            return X[num_cols]\n        elif self.dtype == 'category':\n            cat_cols = X.columns[X.dtypes == object].tolist()\n            return X[cat_cols]","08b55093":"tmp = train_set.copy()\n\nselector = feat_sel()  # it is numeric by default\n\ntmp = selector.transform(tmp)  # no reason to fit again\n\ntmp.head()","e8eb7904":"tmp = train_set[['RoofMatl']].copy()\n\ndummifier = dummify()\n\ndummifier.fit_transform(tmp).sum()  # to get how many dummies are present","ef4423b2":"test_set.RoofMatl.value_counts()","053cf680":"tmp = test_set[['RoofMatl']].copy()\n\ndummifier.transform(tmp).sum()  # the same instance as before","8963acdb":"class tr_numeric(BaseEstimator, TransformerMixin):\n    def __init__(self, SF_room=True):\n        self.columns = []  # useful to well behave with FeatureUnion\n        self.SF_room = SF_room\n        \n\n    def fit(self, X, y=None):\n        return self\n    \n\n    def remove_skew(self, X, column):\n        X[column] = np.log1p(X[column])\n        return X\n\n\n    def SF_per_room(self, X):\n        if self.SF_room:\n            X['sf_per_room'] = X['GrLivArea'] \/ X['TotRmsAbvGrd']\n        return X\n    \n\n    def transform(self, X, y=None):\n        for col in ['GrLivArea', '1stFlrSF', 'LotArea']: # they can also be inputs\n            X = self.remove_skew(X, col)\n\n        X = self.SF_per_room(X)\n        \n        self.columns = X.columns \n        return X\n    \n\n    def get_features_name(self):  # again, it will be useful later\n        return self.columns","0a397472":"numeric_pipe = Pipeline([('fs', feat_sel(dtype='numeric')),  # select only the numeri features\n                         ('imputer', df_imputer(strategy='median')),  # impute the missing values with the median of each column\n                         ('transf', tr_numeric(SF_room=True)),  # remove skew and create a new feature\n                         ('scl', df_scaler(method='standard'))])  # scale the data\n\nfull_pipe = Pipeline([('gen_cl', general_cleaner()), ('num_pipe', numeric_pipe)])  # put the cleaner on top because we like it clean","a4dd593d":"tmp = train_set.copy()\n\ntmp = full_pipe.fit_transform(tmp)\n\ntmp.head()","b47a2898":"tmp.info()","248d222a":"tmp = test_set.copy()  # not ready to work on those sets yet\n\ntmp = full_pipe.transform(tmp)  # the fit already happened with the training set, we don't want to fit again\n\ntmp.head()","55d27ae6":"full_pipe.get_params()","46bdd262":"class make_ordinal(BaseEstimator, TransformerMixin):\n    '''\n    Transforms ordinal features in order to have them as numeric (preserving the order)\n    If unsure about converting or not a feature (maybe making dummies is better), make use of\n    extra_cols and include_extra\n    '''\n    def __init__(self, cols, extra_cols=None, include_extra=True):\n        self.cols = cols\n        self.extra_cols = extra_cols\n        self.mapping = {'Po':1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\n        self.include_extra = include_extra\n    \n\n    def fit(self, X, y=None):\n        return self\n    \n\n    def transform(self, X, y=None):\n        if self.extra_cols:\n            if self.include_extra:\n                self.cols += self.extra_cols\n            else:\n                for col in self.extra_cols:\n                    del X[col]\n        \n        for col in self.cols:\n            X.loc[:, col] = X[col].map(self.mapping).fillna(0)\n        return X\n\n    \nclass recode_cat(BaseEstimator, TransformerMixin):        \n    '''\n    Recodes some categorical variables according to the insights gained from the\n    data exploration phase. Not presented in this notebook\n    '''\n    def fit(self, X, y=None):\n        return self\n    \n    \n    def tr_GrgType(self, data):\n        data['GarageType'] = data['GarageType'].map({'Basment': 'Attchd',\n                                                  'CarPort': 'Detchd', \n                                                  '2Types': 'Attchd' }).fillna(data['GarageType'])\n        return data\n    \n    \n    def tr_LotShape(self, data):\n        fil = (data.LotShape != 'Reg')\n        data['LotShape'] = 1\n        data.loc[fil, 'LotShape'] = 0\n        return data\n    \n    \n    def tr_LandCont(self, data):\n        fil = (data.LandContour == 'HLS') | (data.LandContour == 'Low')\n        data['LandContour'] = 0\n        data.loc[fil, 'LandContour'] = 1\n        return data\n    \n    \n    def tr_LandSlope(self, data):\n        fil = (data.LandSlope != 'Gtl')\n        data['LandSlope'] = 0\n        data.loc[fil, 'LandSlope'] = 1\n        return data\n    \n    \n    def tr_MSZoning(self, data):\n        data['MSZoning'] = data['MSZoning'].map({'RH': 'RM', # medium and high density\n                                                 'C (all)': 'RM', # commercial and medium density\n                                                 'FV': 'RM'}).fillna(data['MSZoning'])\n        return data\n    \n    \n    def tr_Alley(self, data):\n        fil = (data.Alley != 'NoAlley')\n        data['Alley'] = 0\n        data.loc[fil, 'Alley'] = 1\n        return data\n    \n    \n    def tr_LotConfig(self, data):\n        data['LotConfig'] = data['LotConfig'].map({'FR3': 'Corner', # corners have 2 or 3 free sides\n                                                   'FR2': 'Corner'}).fillna(data['LotConfig'])\n        return data\n    \n    \n    def tr_BldgType(self, data):\n        data['BldgType'] = data['BldgType'].map({'Twnhs' : 'TwnhsE',\n                                                 '2fmCon': 'Duplex'}).fillna(data['BldgType'])\n        return data\n    \n    \n    def tr_MasVnrType(self, data):\n        data['MasVnrType'] = data['MasVnrType'].map({'BrkCmn': 'BrkFace'}).fillna(data['MasVnrType'])\n        return data\n\n\n    def tr_HouseStyle(self, data):\n        data['HouseStyle'] = data['HouseStyle'].map({'1.5Fin': '1.5Unf', \n                                                         '2.5Fin': '2Story', \n                                                         '2.5Unf': '2Story', \n                                                         'SLvl': 'SFoyer'}).fillna(data['HouseStyle'])\n        return data\n    \n    \n    def transform(self, X, y=None):\n        X = self.tr_GrgType(X)\n        X = self.tr_LotShape(X)\n        X = self.tr_LotConfig(X)\n        X = self.tr_MSZoning(X)\n        X = self.tr_Alley(X)\n        X = self.tr_LandCont(X)\n        X = self.tr_BldgType(X)\n        X = self.tr_MasVnrType(X)\n        X = self.tr_HouseStyle(X)\n        return X","b3418081":"cat_pipe = Pipeline([('fs', feat_sel(dtype='category')),\n                     ('imputer', df_imputer(strategy='most_frequent')), \n                     ('ord', make_ordinal(['BsmtQual', 'KitchenQual','GarageQual',\n                                           'GarageCond', 'ExterQual', 'HeatingQC'])), \n                     ('recode', recode_cat()), \n                     ('dummies', dummify())])\n\nfull_pipe = Pipeline([('gen_cl', general_cleaner()), ('cat_pipe', cat_pipe)])\n\n\ntmp = train_set.copy()\n\ntmp = full_pipe.fit_transform(tmp)\n\ntmp.head()","057fd44a":"tmp = test_set.copy()\n\ntmp = full_pipe.transform(tmp)\n\ntmp.head()","3a14b94a":"full_pipe.get_params()","85e552d8":"class FeatureUnion_df(TransformerMixin, BaseEstimator):\n    '''\n    Wrapper of FeatureUnion but returning a Dataframe, \n    the column order follows the concatenation done by FeatureUnion\n\n    transformer_list: list of Pipelines\n\n    '''\n    def __init__(self, transformer_list, n_jobs=None, transformer_weights=None, verbose=False):\n        self.transformer_list = transformer_list\n        self.n_jobs = n_jobs\n        self.transformer_weights = transformer_weights\n        self.verbose = verbose  # these are necessary to work inside of GridSearch or similar\n        self.feat_un = FeatureUnion(self.transformer_list, \n                                    self.n_jobs, \n                                    self.transformer_weights, \n                                    self.verbose)\n        \n    def fit(self, X, y=None):\n        self.feat_un.fit(X)\n        return self\n\n    def transform(self, X, y=None):\n        X_tr = self.feat_un.transform(X)\n        columns = []\n        \n        for trsnf in self.transformer_list:\n            cols = trsnf[1].steps[-1][1].get_features_name()  # getting the features name from the last step of each pipeline\n            columns += list(cols)\n\n        X_tr = pd.DataFrame(X_tr, index=X.index, columns=columns)\n        \n        return X_tr\n\n    def get_params(self, deep=True):  # necessary to well behave in GridSearch\n        return self.feat_un.get_params(deep=deep)","b0670325":"numeric_pipe = Pipeline([('fs', feat_sel('numeric')),\n                         ('imputer', df_imputer(strategy='median')),\n                         ('transf', tr_numeric())])\n\n\ncat_pipe = Pipeline([('fs', feat_sel('category')),\n                     ('imputer', df_imputer(strategy='most_frequent')), \n                     ('ord', make_ordinal(['BsmtQual', 'KitchenQual','GarageQual',\n                                           'GarageCond', 'ExterQual', 'HeatingQC'])), \n                     ('recode', recode_cat()), \n                     ('dummies', dummify())])\n\n\nprocessing_pipe = FeatureUnion_df(transformer_list=[('cat_pipe', cat_pipe),\n                                                 ('num_pipe', numeric_pipe)])\n\n\nfull_pipe = Pipeline([('gen_cl', general_cleaner()), \n                      ('processing', processing_pipe), \n                      ('scaler', df_scaler())])  # the scaler is here to have also the ordinal features scaled\n\ntmp = df_train.copy()\n\ntmp = full_pipe.fit_transform(tmp)\n\ntmp.head()","a71b0184":"tmp = test_set.copy()\n\ntmp = full_pipe.transform(tmp)\n\ntmp.head()","31b10f49":"full_pipe.get_params()","0e552390":"from sklearn.linear_model import Lasso\nfrom sklearn.model_selection import KFold, GridSearchCV\n\nfolds = KFold(5, shuffle=True, random_state=541)\n\ndf_train['Target'] = np.log1p(df_train.SalePrice)\n\ndel df_train['SalePrice']\n\ntrain_set, test_set = make_test(df_train, \n                                test_size=0.2, random_state=654, \n                                strat_feat='Neighborhood')\n\ny = train_set['Target'].copy()\ndel train_set['Target']\n\ny_test = test_set['Target']\ndel test_set['Target']\n\n\ndef grid_search(data, target, estimator, param_grid, scoring, cv):\n    \n    grid = GridSearchCV(estimator=estimator, param_grid=param_grid, \n                        cv=cv, scoring=scoring, n_jobs=-1, return_train_score=False)\n    \n    pd.options.mode.chained_assignment = None  # this is because the gridsearch throws a lot of pointless warnings\n    tmp = data.copy()\n    grid = grid.fit(tmp, target)\n    pd.options.mode.chained_assignment = 'warn'\n    \n    result = pd.DataFrame(grid.cv_results_).sort_values(by='mean_test_score', \n                                                        ascending=False).reset_index()\n    \n    del result['params']\n    times = [col for col in result.columns if col.endswith('_time')]\n    params = [col for col in result.columns if col.startswith('param_')]\n    \n    result = result[params + ['mean_test_score', 'std_test_score'] + times]\n    \n    return result, grid.best_params_","c18d9320":"lasso_pipe = Pipeline([('gen_cl', general_cleaner()),\n                       ('processing', processing_pipe),\n                       ('scl', df_scaler()), \n                       ('lasso', Lasso(alpha=0.01))])\n\nres, bp = grid_search(train_set, y, lasso_pipe, \n            param_grid={'processing__num_pipe__transf__SF_room': [True, False], \n                        'processing__num_pipe__imputer__strategy': ['mean', 'median'],\n                        'processing__cat_pipe__dummies__drop_first': [True, False],\n                        'lasso__alpha': [0.1, 0.01, 0.001]},\n            cv=folds, scoring='neg_mean_squared_error')\n\nres","5d14d16c":"bp","9c823df0":"from sklearn.metrics import mean_squared_error, mean_absolute_error\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\ndef cv_score(df_train, y_train, kfolds, pipeline):\n    oof = np.zeros(len(df_train))\n    train = df_train.copy()\n    \n    for train_index, test_index in kfolds.split(train.values):\n            \n        trn_data = train.iloc[train_index][:]\n        val_data = train.iloc[test_index][:]\n        \n        trn_target = y_train.iloc[train_index].values.ravel()\n        val_target = y_train.iloc[test_index].values.ravel()\n        \n        pipeline.fit(trn_data, trn_target)\n\n        oof[test_index] = pipeline.predict(val_data).ravel()\n            \n    return oof\n\n\ndef get_coef(pipe):\n    imp = pipe.steps[-1][1].coef_.tolist()\n    feats = pipe.steps[-2][1].get_feature_names()  # again, this is why we implemented that method\n    result = pd.DataFrame({'feat':feats,'score':imp})\n    result = result.sort_values(by=['score'],ascending=False)\n    return result\n\ndef _plot_diagonal(ax):\n    xmin, xmax = ax.get_xlim()\n    ymin, ymax = ax.get_ylim()\n    low = min(xmin, xmax)\n    high = max(xmin, xmax)\n    scl = (high - low) \/ 100\n    \n    line = pd.DataFrame({'x': np.arange(low, high ,scl), # small hack for a diagonal line\n                         'y': np.arange(low, high ,scl)})\n    ax.plot(line.x, line.y, color='black', linestyle='--')\n    \n    return ax\n\n\ndef plot_predictions(data, true_label, pred_label, feature=None, hue=None, legend=False):\n    \n    tmp = data.copy()\n    tmp['Prediction'] = pred_label\n    tmp['True Label'] = true_label\n    tmp['Residual'] = tmp['True Label'] - tmp['Prediction']\n    \n    diag = False\n    alpha = 0.7\n    label = ''\n    \n    fig, ax = plt.subplots(1,2, figsize=(15,6))\n    \n    if feature is None:\n        feature = 'True Label'\n        diag = True\n    else:\n        legend = 'full'\n        sns.scatterplot(x=feature, y='True Label', data=tmp, ax=ax[0], label='True',\n                         hue=hue, legend=legend, alpha=alpha)\n        label = 'Predicted'\n        alpha = 0.4\n\n    sns.scatterplot(x=feature, y='Prediction', data=tmp, ax=ax[0], label=label,\n                         hue=hue, legend=legend, alpha=alpha)\n    if diag:\n        ax[0] = _plot_diagonal(ax[0])\n    \n    sns.scatterplot(x=feature, y='Residual', data=tmp, ax=ax[1], \n                    hue=hue, legend=legend, alpha=0.7)\n    ax[1].axhline(y=0, color='r', linestyle='--')\n    \n    ax[0].set_title(f'{feature} vs Predictions')\n    ax[1].set_title(f'{feature} vs Residuals')","f664acad":"lasso_oof = cv_score(train_set, y, folds, lasso_pipe)\n\nlasso_oof[:10]","801f017b":"get_coef(lasso_pipe)  # it has been fitted in the cv_score function\n# to be fair, these coefficients refer only to the last of the 5 folds","0b6c7d0a":"plot_predictions(train_set, y, lasso_oof)","b513a467":"plot_predictions(train_set, y, lasso_oof, feature='GrLivArea')","22ed4dd0":"numeric_pipe = Pipeline([('fs', feat_sel('numeric')),\n                         ('imputer', df_imputer(strategy='mean')),  # tuned above\n                         ('transf', tr_numeric(SF_room=True))])  # tuned above\n\n\ncat_pipe = Pipeline([('fs', feat_sel('category')),\n                     ('imputer', df_imputer(strategy='most_frequent')), \n                     ('ord', make_ordinal(['BsmtQual', 'KitchenQual','GarageQual',\n                                           'GarageCond', 'ExterQual', 'HeatingQC'])), \n                     ('recode', recode_cat()), \n                     ('dummies', dummify(drop_first=True))])  # tuned above\n\n\nprocessing_pipe = FeatureUnion_df(transformer_list=[('cat_pipe', cat_pipe),\n                                                    ('num_pipe', numeric_pipe)])\n\nlasso_pipe = Pipeline([('gen_cl', general_cleaner()), \n                 ('processing', processing_pipe),\n                  ('scl', df_scaler()), ('lasso', Lasso(alpha=0.01))])  # tuned above\n\nlasso_oof = cv_score(train_set, y, folds, lasso_pipe)\n\nget_coef(lasso_pipe)","7da9f90f":"plot_predictions(train_set, y, lasso_oof)","198c84cc":"plot_predictions(train_set, y, lasso_oof, feature='GrLivArea')","20a13c45":"lasso_pred = lasso_pipe.predict(test_set)\n\nplot_predictions(test_set, y_test, lasso_pred)","987f43c3":"plot_predictions(test_set, y_test, lasso_pred, feature='GrLivArea')","c09d96a6":"print('Score in 5-fold cv')\nprint(f'\\tRMSE: {round(np.sqrt(mean_squared_error(y, lasso_oof)), 5)}')\nprint(f'\\tMAE: {round(mean_absolute_error(np.expm1(y), np.expm1(lasso_oof)), 2)} dollars')\nprint('Score on holdout test')\nprint(f'\\tRMSE: {round(np.sqrt(mean_squared_error(y_test, lasso_pred)), 5)}')\nprint(f'\\tMAE: {round(mean_absolute_error(np.expm1(y_test), np.expm1(lasso_pred)), 2)} dollars')","b207b28f":"df_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\nsub = df_test[['Id']].copy()\n\npredictions = lasso_pipe.predict(df_test)","a3557d41":"import df_pipeline as dfp","0856b9f9":"dummifier = dfp.dummify()\n\ntmp = train_set[['HouseStyle']].copy()\n\ntmp = dummifier.transform(tmp)\n\ntmp.sample(5)","7c51bb28":"For the fans of the numeric metrics","1589670d":"# Pipelines: what and why?\n\nIn the context of a ML project, we can define a pipeline as a sequence of objects that act on a set of data. The actions can include:\n\n* learning a relation\n* transform some feature\n* impute missing values\n* create new features\n* fit a model\n* predict on unseen data\n\nThe purpose is to apply sequentially its element to **validate a process**. On this point, we can take a moment to stress out how important is to validate the entire process rather than just the ML model itself. It is well known that different problems call for different models since it is not possible to say a priori that one model will perform better than another one on any given dataset. This is one (fairly sloppy) formulation of the so-called No free lunch theorem.\n\nTherefore, it is easy to imagine that choices in the processing stage (being this the imputation of missing entries, the scaling of the features or the feature engineered) will influence the choice of the final model, both in terms of algorithm or of hyperparameters. It is thus important to be able to assess the ability of a process to be better than another one. \n\nA trustable evaluation of your process is necessary to justify the choices you made, otherwise you can be less sure about, for example, creating a new feature as you don't know if you are increasing your cross-validation score because of a flawed process or because of the creation of a genuinely powerful predictor (or, more commonly, you are overestimating the effect of adding that new feature or not).\n\nWhile the use of pipelines is not the silver bullet to this issue, it certainly helps in excluding some common validation errors. The main one, at first, is to not apply the same transformations to the train and test sets, resulting most of the time in an error or a very weird performing model. A pipeline will take care of this issue by always applying the same transformations in sequence until the final result is achieved, no matter what dataset are you applying it on.\n\nTo see why it is not enough to carefully apply the same sequence by hand and instead relying on a Pipeline, let's have a look at a simple example. In this competition, the training set (the only one we should be looking at to simulate a real-life problem) looks like this\n","a8c96382":"This time, the parameters are a bit more complex","30f0f509":"The normal OneHotEncoding or the standard get_dummies would create a dataset with only 3 columns and, when a model is called, an error caused by the mismatch in shape. This custom dummifier takes care of it as follows","261bf8dc":"The more perceptive of you will notice that this output has a different order of columns of the input. This is because `FeatureUnion` is essentially concatenating the results of each transformer: first the categorical features, then the numeric ones. This is also why the column list in our version of this transformer was build in that way.\n\nAgain, we can now apply the pipeline to the test set","b3fcc987":"If you have never seen a class before, it looks intimidating (at least it was for me). Let me break it down a bit.\n\n* We use `self` a lot, this refers to the object itself (sorry for the repetition). This means that when you create an object of this type, it will have its properties (scale, method, etc) and its methods (fit, transform). The use of `self` is simply to refer to those properties or methods.\n* There is an `__init__` method, which initializes some variables, some of them can be external inputs (like the method for scaling) or properties that the methods can modify (and you can access to when you use it)\n* There is a `fit` method. In this case, it just calls the fit method of the selected pre-made scaler. It also finds the mean and scale of each column (by using the premade scaler as well)\n* There is a `transform` method. This applies the transformation of the scaler fitted above **and** rebuilds the dataframe with its structure.\n\nLet's see how to use this new scaler on the same data as before.","a0747fe8":"The pipeline for categorical features will then be","0c514dc0":"This notebook aims to make the use of pipelines a bit more user-friendly even for more unexperienced ML enthusiasts. In particular, most of the efforts here will be directed to the use of pipelines for cross-validation and model selection (for example, via GridSearch), which can get tricky when the pipeline becomes more complex.\n\nWe will touch the following topics:\n\n* What is a Pipeline and why it matters\n* What is tricky about a pipeline\n* How to make your own custom transformers\n* A pipeline step by step (or tube by tube?)\n\nWe will make use of this dataset as it gives enough variety in input and enough opportunities for data transformation. However, we won't try to make an high scoring model but rather a very understandable one.\n\nThe final goal is thus making the reader able to use the concepts here presented to make their own high scoring model.","d776d95c":"The coefficients are a bit different, but we did not solved much. This was expected since we were not changing too much from the default.\n\nWe can make further use of the fact that we are working with a pipeline and directly apply it to the test set and see if the behavior changes.","26275c02":"We thus get the same numbers as before, but we still have the nice DataFrame structure, we will see in the next section how important this is.\n\nAll those properties defined in the `__init__` method are accessible like in the normal Scaler","94b487ba":"More often than not, you will need to create transformers that do nothing while fitting the data and do a lot of things when they transform it. For example, by following the documentation about this dataset, we can implement a transformer that cleans up the data.","b0123b36":"We have seen the scaler in action already, we can easily test the other transformers.","478d1938":"And there we have it, some categories converted into numeric features, other first recoded and the dummified. This dataset is ready for a model and, as before, this pipeline is ready for the validation set as well","425bbdde":"However, in the test set we don't have all those values","45aa0083":"And it is easy to see that we have quite a few missing values.","07205bc7":"Now, let's suppose we want to fill the missing values with the average of each column. It is not hard to do it and we are now happy about the result.\n\nWe then split our data into training and validation sets and train some models to be evaluated.\n\n**This is wrong!**\n\nBy imputing the missing values using the mean of the entire dataset and *then* splitting the data for validation, we are leaking some information from the validation set into the training one. This will most likely boost our evaluation score but not necessarily the quality of our model.\n\nOne can then argue that it is sufficient to split the data first and impute after on training and validation sets separately. However, it is important to keep in mind that the mean values (the values you will use to impute the missing entries) have to be calculated only on the training set and applied to both training and validation. In this way, you will be correctly simulating the performance of your model in your evaluation process.\n\nA pipeline will take care of that, regardless of your validation strategy (k-fold, train_test split, both...)\n\n# Existing instruments and what is tricky about them\n\nBy going through the sklearn documentation, it is easy to find that most of the operations you will commonly apply have already a very efficient implementation. For example, the imputation mentioned above can be done easily by using the `SimpleImputer`. Or, if we want to rescale our features, we can make use of the `StandardScaler`.\n\nLet's thus see how to use one of these instruments. First, we prepare an evaluation set","b9808142":"and we want the data to be scaled before going to our model. If we apply the provided scaler, we get the following result","9479bd17":"There is a big outlier in our prediction and a visible pattern in the residual plot, both things that would require further investigation.\n\nWe can also plot the residuals against the most important features, for example","77372a0a":"I hope it is now evident why I kept implementing a `get_features_name` method in the previous classes. It was all for this moment.\n\nThe complete pipeline will then be","f6a9f21a":"The grid search (here in an utility function just to have better looking results) looks like this","a505cc28":"We thus see how the missing dummies were added with all 0's and the dummy for `Metal` got dropped. In this way, the pipeline will not break later on.\n\n## A pipeline for numeric features\n\nWe can finally explicitly build our first pipeline. Ideally, we want it as follows\n\n* Clean the data following the documentation\n* Impute the missing values with the mean or the median (nothing stops us from using other types of imputations\n* Apply some transformations on some features\n* Create new features\n* Scale the data\n\nWe already have every element but one, let's make a custom transformer","ea4b6e4c":"In other words, with the use of the sklearn `Pipeline`, we want to sequentially apply the transformations in the given list. The list is made of tuples, the first element is a label for that step, and the second element is the transformation (or the model, or another pipeline). The name is useful to identify every parameter of the Pipeline, as we will see later.\n\nThis pipeline, given the training data, acts as follows","1c3fcd71":"I hope this script can help you and that this notebook was useful for you to better understand the advantages and the functioning of a Pipeline. Please let me know if something is not clear or incorrect.\n\nCheers","469708b7":"## Using the pipeline in GridSearch\n\nHaving set up everything as we did, it is not difficult to tune our pipeline with GridSearch. We will put a simple model at the end of the pipeline just for the fun of it and tune both the hyperparameters of this model and the parameters of the pipeline. \n\nWe thus make use of `GridSearch` to pick the best model configuration by varying several parameters, namely\n\n* Whether or not we create the new feature describing the square feet per room\n* If we impute the numerical missing values with the mean or the median\n* If we drop one dummy or not\n* If we change the regularization parameter of the Lasso regression\n\nThanks to the fact that we have a pipeline, we are able to easily explore all these configurations without worrying too much about information leakage or by repeating the same steps over and over","1e8614f8":"The validation set is 20% of the full training set and it correctly reproduces the proportion of houses in each Neighborhood (see the implementation in one of the hidden cells at the beginning). Now, let's suppose we only have this dataset","04ef2aa8":"We have to worry about nothing, if some column had missing values in the validation set and not in the train one, the pipeline is still able to take care of it. In other words, we are sure that our models will get the same data format both during training and validation, making the validation phase more trustable.\n\nWe mentioned in the comments the usefulness of exposing the parameters, this is how our pipeline looks like","86883d2e":"### Short note: the custom dummifier\n\nOne common issue one encounter when working with categorical features is that some categories may be very rare, resulting into a mismatch between the columns in the train and test (or validation set).\n\nFor this reason, the implementation here proposed takes care of the issue by creating or deleting any column that is not present in both sets. In other words, we assing the attribute columns when we first transform the training set and, when the transform method is called again, we check for missing or extra columns. If a column is missing, we add it with all 0's, otherwise we drop it.\n\nA short demostration of this can be done if we take the following feature","7d9503f7":"Note: we implemented a `fit` and a `transform` method but somehow we manage to use a `fit_transform` method. The reason is that we are making these objects inherit the properties of the `TransformerMixin`, which knows what to do when it finds a `fit_transform`. \n\nWe are now ready to put all this knowledge together in the section we are all here for\n\n# A Pipeline step-by-step\n\nIt is now time to build our complete pipeline for this dataset. We start with some transformers that are nothing more than simple wrappers around known sklearn functions.","64ee5b8e":"As we can see, the fit method doesn't need to do anything, while the transform method fills in missing values, removes columns, and creates new columns. Moreover, we don't need to specify an `__init__` because there is nothing to be initialized in this case (it won't always be the case).\n\nThe usage is the same","bbf993ba":"## A pipeline for categorical features\n\nIn the same way, we can create a similar pipeline for all the categorical features. The difference will be only that we impute differently (there is no mean or median for categories), and we transform differently.\n\nLet's then create a simple custom transformer","5765a31a":"We can use these transformers in many places but the imputer will probably come before the others as we don't want to deal with missing values again.\n\nHowever, the imputation method changes a lot if we are dealing with categorical or numerical features, we need something that automatically selects the features to be fed to the right imputer. This is what the next class is for","c0ee6948":"We have our predictions and we can see the coefficients of our regression","545b9688":"*I am still working on making easy to set these parameters into the full pipeline*\n\nWe thus see very easily how some parameters matter more than others and consider if it is worth it to keep spending time in tuning them (because the more parameters you want to tune, the more running time will take).\n\nPlease note how we refer to a specific parameter by calling every step of the pipeline by its name and concatenating those names by the double underscore. I think this is why you can't use parameters that start with and underscore, they mess up this reference system. (To be checked)\n\n## Other evaluation approaches\n\nWe see from the GridSearch that, if we ignore the fact that these models are vey simple and not high-performing, the best configurations of parameters are scoring very similar results. One may want to be sure that the model is really the best possible one and\/or it is predicting reasonable prices.\n\nThanks to all that effort in preserving the feature names and making sure that everything happens inside of the pipeline, this model will fit pretty much in any validation approach you might want to adopt.\n\nFor example, I might be interested in seeing how the model performs in a 5-fold cross-validation setting, I might want to see how much the predictions are off, if I am missing something in the data, what are the most important features. With a few helper functions, we are going to do all of it.","c0dadfc5":"Please note that this transformer takes a parameter that determines whether or not to create a new feature. It is this kind of parameter that can be tuned with a GridSearch (more on this later).\n\nCreating a new feature in that way would be impossible if the previous steps were not returning a DataFrame. There is naturally an alternative that includes specifying the index of the columns you want to use, but I find this approach way more user-friendly and robust.\n\nA pipeline for numeric features would then look like this","298a7e81":"Naturally, even though so far we didn't even loaded the test set from Kaggle, we can apply our pipeline to truly unseen data ","a52c5b5b":"And we again have to put no effort to make our pipeline work on new data. This is important for 2 reasons:\n\n* we can put all our effort in making the model better rather than fighting with messy code\n* we are virtually ready to send our model to our client and it is ready to use\n\n\n## Using this code in your Notebooks\n\nAfter this notebook was first created, Kaggle implemented a very nice feature: import utility script. Therefore I created a utility script with the general parts of this pipeline (so nothing that refers to this specific competition). You find it here https:\/\/www.kaggle.com\/lucabasa\/df-pipeline\n\nA simple example of how to use it is the following:\n\n* add the utility script via the menu of your Notebook (File-> Add utility script)\n* import the script","08ba46ef":"Which can be tested very simply","c9288d16":"The structure of the data is very different now. The reason is that, for speed and memory usage, the sklearn instruments use the dataframe as numpy arrays. However, one may want to operate again on the data after one of these transformations and losing the DataFrame structure may lead to very cumbersome code or, probably worse, very limited pipelines. In the next sections, we will implement methods and pipelines that aim to be very flexible in order to adapt to a larger variety of problems, the downside is that we will be a bit less efficient in terms of speed and memory usage.\n\nA note before moving on: a pipeline with only 2 steps (scaler and model) will do just fine by not doing any further coding but I find it more useful to break things down and learn what happens inside.\n\n# Transformers and classes\n\nIn this section, we will see how to use the existing instruments to build something that better suits our needs (having a pipeline and maintain the DataFrame structure).\n\nFirst of all, we are going to be working with sklearn Pipelines. These will sequentially apply the `fit` and `transform` (or `predict`) methods at each step. We thus need to create objects that can do `fit` and `transform`. In python, the way to do so is to create a **class**. You can read about classes pretty much everywhere and everyone will explain them better than I could possibly do, let's just jump into an example.\n\nWe want a transformer that works like the scaler above but also maintain the DataFrame structure. It will look like this","aaae4e43":"As we wanted, the data flew through the pipeline, getting cleaned, transformed, and rescaled. Moreover, we still have a nice DataFrame structure.\n\nThe powerfulness of this pipeline is visible when we want to do the same thing to the validation set and it is evident when we implement it","457970e6":"Now we want to see if the predictions are too far off or if there is something odd in the residual plot (I suggest to read about them as they are very useful tools for diagnosing something wrong in your model)","2d7554e8":"## Putting everything together\n\nWe have a pipeline for numeric features, one for categorical one, now we want a complete pipeline for the entire dataset.\n\nSklearn again helps us with `FeatureUnion` that, sadly, again compromises the DataFrame structure we are very much fun of. By now, we are confident enough to create our own version of it.","098cd215":"And the best parameters are","076a90e0":"And there we find that our prediction so much off with respect to the real value was indeed a house too cheap for its size (to be fair, this is one of the outliers that everybody know about and they are documented in the official documentation).\n\nSo far, we have used the *default* parameters of our pipeline but we know that there is a better configuration thanks to our GridSearch. Let's see if something changes."}}