{"cell_type":{"8a39ac3a":"code","d87ffaf9":"code","e3b8f755":"code","26d3f9e0":"code","1db8f937":"code","03f15c60":"code","beccf74a":"code","b8734913":"code","25303f9a":"code","e937b5a5":"code","7eef39b6":"code","a54f7a0c":"code","885e5bad":"code","ec359f3a":"code","e9b1d262":"code","783db772":"code","13d44f1d":"code","87929702":"code","51f99a66":"markdown","abbd4199":"markdown","666debed":"markdown","32d310dd":"markdown","0f56aa7a":"markdown","9ad85e69":"markdown","66f76029":"markdown","dd8fc53c":"markdown"},"source":{"8a39ac3a":"from itertools import chain\n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.integrate\nsns.set_style('darkgrid')\n\nimport os\nfrom glob import glob\n\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications.densenet import DenseNet121\nfrom keras.layers import Dense, GlobalAveragePooling2D\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras.models import load_model\n","d87ffaf9":"# import data\nall_xray_df = pd.read_csv('\/kaggle\/input\/data\/Data_Entry_2017.csv')\n\n# obtain path to each image and add to dataframe\n# use glob module to intelligently parse all files\n# add to dictionary with key as filename\nall_image_paths = {os.path.basename(x): x for x in \n                   glob(os.path.join('..\/input\/data', 'images*', '*', '*.png'))}\n\nprint('Scans found:', len(all_image_paths), ', Total Rows', all_xray_df.shape[0])\n\nall_xray_df['path'] = all_xray_df['Image Index'].map(all_image_paths.get)\n\n\n\n# this is multi-task, multi-class classification\n# rename null finding\n# split the string description \nall_xray_df['Finding Labels']  = all_xray_df['Finding Labels'].replace('No Finding', '')\n#all_labels = np.unique(list(chain(*all_xray_df['Finding Labels'].map(lambda x: x.split('|')).tolist())))\nall_labels = list({x for l in all_xray_df['Finding Labels'].str.split('|') for x in l})\n\n# obtain list of unique diseases\nall_labels = [x for x in all_labels if len(x) > 0]\nprint('All Labels ({}): {}'.format(len(all_labels), all_labels))\n\n#perform one-hot encoding based on diseases extracted\nfor c_label in all_labels:\n    if len(c_label)> 1: # leave out empty labels\n        all_xray_df[c_label] = all_xray_df['Finding Labels'].map(lambda finding: 1.0 if c_label in finding else 0)\n\n# drop unused columns\nall_xray_df = all_xray_df.drop(['OriginalImagePixelSpacing[x', 'y]', 'Unnamed: 11', \n                               'Finding Labels',\n                               'Follow-up #',\n                               'OriginalImage[Width','Height]'\n                               ], axis=1)\n\nall_xray_df.sample(3)","e3b8f755":"print('Number of unique patients:' , all_xray_df['Patient ID'].nunique())\n#all_xray_df.columns","26d3f9e0":"#sns.distplot(all_xray_df['Patient Age'])\nidx = all_xray_df['Patient Age'].sort_values().index[-2]\nplt.imshow(plt.imread(all_xray_df.iloc[idx]['path']), cmap='bone')","1db8f937":"sns.countplot(all_xray_df['Patient Gender'])\nsns.countplot(all_xray_df['View Position'])","03f15c60":"#sns.countplot()\nsns.barplot(x=all_labels, \n            y=all_xray_df[all_labels].sum(), \n            order = all_xray_df[all_labels].sum().sort_values(ascending=False).index)\n\nplt.tick_params(axis='x', rotation=90)","beccf74a":"from sklearn.model_selection import GroupShuffleSplit\nfrom sklearn.model_selection import train_test_split\n\nn = None # use all data\ngss = GroupShuffleSplit(n_splits=1, train_size=.7, random_state=42)\n\nfor train_idx, test_idx in gss.split(all_xray_df[: n], groups = all_xray_df[: n]['Patient ID'].values):\n    train_df = all_xray_df.iloc[train_idx]\n    test_df, valid_df = train_test_split(all_xray_df.iloc[test_idx], \n                                   test_size = 0.5, \n                                   random_state = 42) #should add stratified sampling\n    \ntrain_df.head()\ntest_df.head()","b8734913":"def get_train_generator(df, image_dir, x_col, y_cols, shuffle=True, batch_size=32, seed=1, target_w = 320, target_h = 320):\n    \"\"\"\n    Return generator for training set, normalizing using batch\n    statistics.\n\n    Args:\n      train_df (dataframe): dataframe specifying training data.\n      image_dir (str): directory where image files are held.\n      x_col (str): name of column in df that holds filenames.\n      y_cols (list): list of strings that hold y labels for images.\n      sample_size (int): size of sample to use for normalization statistics.\n      batch_size (int): images per batch to be fed into model during training.\n      seed (int): random seed.\n      target_w (int): final width of input images.\n      target_h (int): final height of input images.\n    \n    Returns:\n        train_generator (DataFrameIterator): iterator over training set\n    \"\"\"        \n    print(\"getting train generator...\") \n    # normalize images\n    image_generator = ImageDataGenerator(\n        samplewise_center=True,\n        samplewise_std_normalization= True)\n    \n    # flow from directory with specified batch size\n    # and target image size\n    generator = image_generator.flow_from_dataframe(\n            dataframe=df,\n            directory=image_dir,\n            x_col=x_col,\n            y_col=y_cols,\n            class_mode=\"raw\",\n            batch_size=batch_size,\n            shuffle=shuffle,\n            seed=seed,\n            target_size=(target_w,target_h))\n    \n    return generator\n\ndef get_test_and_valid_generator(valid_df, test_df, train_df, image_dir, x_col, y_cols, sample_size=100, batch_size=32, seed=1, target_w = 320, target_h = 320):\n    \"\"\"\n    Return generator for validation set and test test set using \n    normalization statistics from training set.\n\n    Args:\n      valid_df (dataframe): dataframe specifying validation data.\n      test_df (dataframe): dataframe specifying test data.\n      train_df (dataframe): dataframe specifying training data.\n      image_dir (str): directory where image files are held.\n      x_col (str): name of column in df that holds filenames.\n      y_cols (list): list of strings that hold y labels for images.\n      sample_size (int): size of sample to use for normalization statistics.\n      batch_size (int): images per batch to be fed into model during training.\n      seed (int): random seed.\n      target_w (int): final width of input images.\n      target_h (int): final height of input images.\n    \n    Returns:\n        test_generator (DataFrameIterator) and valid_generator: iterators over test set and validation set respectively\n    \"\"\"\n    print(\"getting train and valid generators...\")\n    # get generator to sample dataset\n    raw_train_generator = ImageDataGenerator().flow_from_dataframe(\n        dataframe=train_df, \n        directory=IMAGE_DIR, \n        x_col=x_col, \n        y_col=y_cols, \n        class_mode=\"raw\", \n        batch_size=sample_size, \n        shuffle=True, \n        target_size=(target_w, target_h))\n    \n    # get data sample\n    batch = raw_train_generator.next()\n    data_sample = batch[0]\n\n    # use sample to fit mean and std for test set generator\n    image_generator = ImageDataGenerator(\n        featurewise_center=True,\n        featurewise_std_normalization= True)\n    \n    # fit generator to sample from training data\n    image_generator.fit(data_sample)\n\n    # get test generator\n    valid_generator = image_generator.flow_from_dataframe(\n            dataframe=valid_df,\n            directory=image_dir,\n            x_col=x_col,\n            y_col=y_cols,\n            class_mode=\"raw\",\n            batch_size=batch_size,\n            shuffle=False,\n            seed=seed,\n            target_size=(target_w,target_h))\n\n    test_generator = image_generator.flow_from_dataframe(\n            dataframe=test_df,\n            directory=image_dir,\n            x_col=x_col,\n            y_col=y_cols,\n            class_mode=\"raw\",\n            batch_size=batch_size,\n            shuffle=False,\n            seed=seed,\n            target_size=(target_w,target_h))\n    return valid_generator, test_generator","25303f9a":"IMAGE_DIR = None # our column contains the absolute paths\ntrain_generator = get_train_generator(train_df, IMAGE_DIR, \"path\", all_labels)\nvalid_generator, test_generator= get_test_and_valid_generator(valid_df, test_df, train_df, IMAGE_DIR, \"path\", all_labels)","e937b5a5":"# show example\nx, y = train_generator.__getitem__(2)\nplt.imshow(x[0]);","7eef39b6":"def compute_class_freqs(labels):\n    \"\"\"\n    Compute positive and negative frequences for each class.\n\n    Args:\n        labels (np.array): matrix of labels, size (num_examples, num_classes)\n    Returns:\n        positive_frequencies (np.array): array of positive frequences for each\n                                         class, size (num_classes)\n        negative_frequencies (np.array): array of negative frequences for each\n                                         class, size (num_classes)\n    \"\"\"\n    \n    # total number of patients (rows)\n    N = len(labels)\n    \n    positive_frequencies = (np.sum(labels, 0)) \/ N\n    negative_frequencies = (1- positive_frequencies)\n\n    return positive_frequencies, negative_frequencies\n\n\n\ndef get_weighted_loss(pos_weights, neg_weights, epsilon=1e-7):\n    \"\"\"\n    Return weighted loss function given negative weights and positive weights.\n\n    Args:\n      pos_weights (np.array): array of positive weights for each class, size (num_classes)\n      neg_weights (np.array): array of negative weights for each class, size (num_classes)\n    \n    Returns:\n      weighted_loss (function): weighted loss function\n    \"\"\"\n    def weighted_loss(y_true, y_pred):\n        \"\"\"\n        Return weighted loss value. \n\n        Args:\n            y_true (Tensor): Tensor of true labels, size is (num_examples, num_classes)\n            y_pred (Tensor): Tensor of predicted labels, size is (num_examples, num_classes)\n        Returns:\n            loss (float): overall scalar loss summed across all classes\n        \"\"\"\n        # initialize loss to zero\n        loss = 0.0\n\n        for i in range(len(pos_weights)):\n            # for each class, add average weighted loss for that class \n            loss += -1 * K.mean(pos_weights * y_true * K.log(y_pred + epsilon) + \n                          (1 - y_true) * neg_weights * K.log(1 - y_pred + epsilon))\n            \n        return loss\n    \n    return weighted_loss\n\nfreq_pos, freq_neg = compute_class_freqs(train_generator.labels)\npos_weights = freq_neg\nneg_weights = freq_pos\npos_contribution = freq_pos * pos_weights \nneg_contribution = freq_neg * neg_weights\n\n","a54f7a0c":"data = pd.DataFrame({\"Class\": all_labels, \"Label\": \"Positive\", \"Value\": pos_contribution})\ndata = data.append([{\"Class\": all_labels[l], \"Label\": \"Negative\", \"Value\": v} \n                        for l,v in enumerate(neg_contribution)], ignore_index=True)\nplt.xticks(rotation=90)\nsns.barplot(x=\"Class\", y=\"Value\", hue=\"Label\" ,data=data);","885e5bad":"# create the base pre-trained model\nbase_model = DenseNet121(\n    include_top=False,\n    weights=\"imagenet\",\n    input_tensor=None,\n    input_shape=None,\n)\n\nx = base_model.output\n\n# add a global spatial average pooling layer\nx = GlobalAveragePooling2D()(x)\n\n# and a logistic layer\npredictions = Dense(len(all_labels), activation=\"sigmoid\")(x)\n\nmodel = Model(inputs=base_model.input, outputs=predictions)\nmodel.compile(optimizer='adam', loss=get_weighted_loss(pos_weights, neg_weights),metrics=['accuracy'])\n\ncheckpoint_path = 'xray_class_weights.best.hdf5'\n#model.load_weights(checkpoint_path)\n#model.summary()","ec359f3a":"from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n#weight_path=\".\/{}_weights.best.hdf5\".format('xray_class')\n\ncheckpoint = ModelCheckpoint(checkpoint_path, monitor='val_loss', verbose=1, \n                             save_best_only=True, mode='min', save_weights_only = True)\n\nearly = EarlyStopping(monitor=\"val_loss\", \n                      mode=\"min\", \n                      patience=10)\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                              patience=5, min_lr=0.001)\n\ncallbacks_list = [checkpoint, early, reduce_lr]\n\n\nhistory = model.fit(train_generator, \n                    validation_data=valid_generator,\n                    steps_per_epoch=100, \n                    validation_steps=25, \n                    epochs = 10,\n                   callbacks = callbacks_list)","e9b1d262":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.ylabel(\"loss\")\nplt.xlabel(\"epoch\")\nplt.title(\"Training Loss Curve\")\nplt.show()","783db772":"pred_Y = model.predict(test_generator, batch_size = 32, verbose = True)","13d44f1d":"from sklearn.metrics import roc_curve, auc\n\ntest_Y = test_generator.labels\nfig, c_ax = plt.subplots(1,1, figsize = (9, 9))\n\nfor (idx, c_label) in enumerate(all_labels):\n    fpr, tpr, thresholds = roc_curve(test_Y[:,idx].astype(int), pred_Y[:,idx])\n    c_ax.plot(fpr, tpr, label = '%s (AUC:%0.2f)'  % (c_label, auc(fpr, tpr)))\n\nc_ax.legend()\nc_ax.set_xlabel('False Positive Rate')\nc_ax.set_ylabel('True Positive Rate')","87929702":"model.save('finaltry.h5')","51f99a66":"We use the weighted loss function to account for class imbalance.","abbd4199":"This link https:\/\/vijayabhaskar96.medium.com\/tutorial-on-keras-flow-from-dataframe-1fd4493d237c was useful for understanding the image generator.","666debed":"# 2. Preparing data for training\n\nThe `GroupShuffleSplit` function separates the entries based on the Patient_ID to avoid data leakage.\n\nAfter splitting into training, test and validation sets, I then use some pre-written generator functions to use for the training. The way that it is implemented allows for standard normalisation. ","32d310dd":"Evaluation metrics are a bit lower than the CheXNet paper published, can the model be trained in a better way?","0f56aa7a":"---\n# 1. Preprocessing Data\n\nThis section reads in the data and creates a dataframe of the paths and the labels to the images. Furthermore, the labels are split and one-hot encoded.","9ad85e69":"# 3. Training\nThis section deals with Transfer Learning with pretrained weights.","66f76029":"The data available of different diseases varies a lot.","dd8fc53c":"# Deep Neural Network\n\n"}}