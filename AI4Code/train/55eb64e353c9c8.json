{"cell_type":{"04f5948c":"code","61bd1076":"code","4ccd00fd":"code","c85e5636":"code","f61cce1c":"code","2f3f7e68":"code","38fec2f9":"code","eb0d1994":"code","d2909ea8":"code","290f9802":"code","86c296f9":"code","1e88b3f3":"code","d8056e90":"code","464470e1":"code","e56946c8":"code","621e0082":"code","d9b1a87c":"code","4eb19f51":"code","44197415":"markdown","88c39cdc":"markdown","89658729":"markdown","bf0cda69":"markdown","ba31060d":"markdown","f6482042":"markdown"},"source":{"04f5948c":"# Load in our libraries\nimport pandas as pd\nimport numpy as np\nimport re\nimport sklearn\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom collections import Counter\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, RobustScaler\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, KFold, RepeatedStratifiedKFold, StratifiedKFold\nfrom sklearn.model_selection import RepeatedKFold, train_test_split, RandomizedSearchCV\nfrom sklearn.neighbors import  KNeighborsClassifier as knn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nimport xgboost\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import auc, accuracy_score, confusion_matrix\n\nfrom IPython.core.display import HTML\n\nimport matplotlib as mpl\nmpl.rcParams['figure.dpi'] = 120\nmpl.rcParams['axes.spines.top'] = False\nmpl.rcParams['axes.spines.right'] = False","61bd1076":"# Load in the train and test datasets\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\n\n# Store our passenger ID for easy access\nPassengerId = test['PassengerId']\n\ntrain['Ticket_type'] = train['Ticket'].apply(lambda x: x[0:3])\ntrain['Ticket_type'] = train['Ticket_type'].astype('category')\ntrain['Ticket_type'] = train['Ticket_type'].cat.codes\n\ntest['Ticket_type'] = test['Ticket'].apply(lambda x: x[0:3])\ntest['Ticket_type'] = test['Ticket_type'].astype('category')\ntest['Ticket_type'] = test['Ticket_type'].cat.codes","4ccd00fd":"# Outlier detection\n\ndef detect_outliers(df,n,features):\n    \"\"\"\n    Takes a dataframe df of features and returns a list of the indices\n    corresponding to the observations containing more than n outliers according\n    to the Tukey method.\n    \"\"\"\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers   \n\n# detect outliers from Age, SibSp , Parch and Fare\nOutliers_to_drop = detect_outliers(train,2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])\n# Drop outliers\ntrain = train.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)","c85e5636":"full_data = [train, test]\n\n# Some extra features, not necessarily important\n# Gives the length of the name\n# train['Name_length'] = train['Name'].apply(len)\n# test['Name_length'] = test['Name'].apply(len)\ntrain['Words_Count'] = train['Name'].apply(lambda x: len(x.split()))\ntest['Words_Count'] = test['Name'].apply(lambda x: len(x.split()))\n\n# Feature that tells whether a passenger had a cabin on the Titanic\ntrain['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\ntest['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n\n# Feature engineering steps taken from Sina\n# Create new feature FamilySize as a combination of SibSp and Parch\nfor dataset in full_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n# Create new feature IsAlone from FamilySize\nfor dataset in full_data:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n# Remove all NULLS in the Embarked column\nfor dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n# Remove all NULLS in the Fare column and create a new feature CategoricalFare\nfor dataset in full_data:\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\ntrain['CategoricalFare'] = pd.qcut(train['Fare'], 4)\n# Create a New feature CategoricalAge\nfor dataset in full_data:\n    age_avg = dataset['Age'].mean()\n    age_std = dataset['Age'].std()\n    age_null_count = dataset['Age'].isnull().sum()\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n    dataset['Age'] = dataset['Age'].astype(int)\ntrain['CategoricalAge'] = pd.cut(train['Age'], 5)\n# Define function to extract titles from passenger names\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n# Create a new feature Title, containing the titles of passenger names\nfor dataset in full_data:\n    dataset['Title'] = dataset['Name'].apply(get_title)\n# Group all non-common titles into one single grouping \"Rare\"\nfor dataset in full_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\nfor dataset in full_data:\n    # Mapping Sex\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n    \n    # Mapping titles\n    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n    \n    # Mapping Embarked\n    freq_port = dataset.Embarked.dropna().mode()[0]\n    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n    \n    # Mapping Fare\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \t\t\t\t\t\t        = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] \t\t\t\t\t\t\t        = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n    \n    # Mapping Age\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] \t\t\t\t\t       = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4","f61cce1c":"train.info()","2f3f7e68":"# Feature selection\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin']\ntrain = train.drop(drop_elements, axis = 1)\ntrain = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\ntest  = test.drop(drop_elements, axis = 1)","38fec2f9":"X_train = pd.get_dummies(train)\nX_train.drop(['Survived'], axis=1, inplace=True)\n\n# Scale data \nfrom sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\n\nX_train_scaled = train.copy()\nX_train_scaled = X_train_scaled.drop(['Survived'], axis=1)\nX_test_scaled = test.copy()\n\nX_train_scaled[['Age','SibSp','Parch']]= scale.fit_transform(train[['Age','SibSp','Parch']])\nX_test_scaled[['Age','SibSp','Parch']]= scale.fit_transform(test[['Age','SibSp','Parch']])\n\nY_train = train.Survived","eb0d1994":"from imblearn.over_sampling import SMOTE\noversample = SMOTE()\nx_train1, y_train1 = oversample.fit_resample(X_train_scaled, Y_train)","d2909ea8":"# Hyperparameter tuning takes a lot of time. If this variable is False, the tuning process will be omitted and the learning will proceed \n# with the hyperparameters already obtained. If this variable is true, you can proceed with the tuning process directly.\nallow_tuning = True","290f9802":"# This function is a function created by myself to eliminate repeated code generated by tuning XGBoost.\n# params_grid_xgb: Combines fixed parameters for grid search in xgboost.\n# features: Target features to be tuned using this function\n# values: Search parameters for each feature\n# X,y: Datasets.\n# last: If this value is false, change each value of the GridSearchCV object's best_params to a list for immediate use in the next adjustment.\ndef xgb_gridsearch(params_grid_xgb, features, values, X, y, last=False):\n    x_train, x_test = train_test_split(X, test_size=.2, random_state=42)\n    y_train_tmp, y_test_tmp = train_test_split(y, test_size=.2, random_state=42)\n\n    cv = RepeatedStratifiedKFold(n_splits = 10, n_repeats = 3, random_state = 42)\n\n    model_xgb = XGBClassifier(use_label_encoder = False, \n                              objective = 'binary:logistic')\n    \n    for i in range(len(features)):\n        params_grid_xgb[features[i]] = values[i]\n    search_xgb = GridSearchCV(model_xgb, params_grid_xgb, verbose = 0,\n                              scoring = 'neg_log_loss', cv = cv).fit(x_train, y_train_tmp, early_stopping_rounds = 15, \n                                  eval_set = [[x_test, y_test_tmp]], \n                                  eval_metric = 'logloss', verbose = False)\n    for i in range(len(features)):\n        print(f\"{features[i]}: {search_xgb.best_params_[features[i]]}\")\n    if not last:\n        for k, v in search_xgb.best_params_.items():\n            search_xgb.best_params_[k] = [v]\n    return search_xgb, search_xgb.best_params_","86c296f9":"if allow_tuning:\n    params_knn = {\n        'n_neighbors' : range(1, 10),\n        'weights' : ['uniform', 'distance'],\n        'algorithm' : ['auto', 'ball_tree','kd_tree'],\n        'p' : [1,2]\n    }\n    model_knn = knn()\n    search_knn = GridSearchCV(model_knn, params_knn, cv=5, scoring='accuracy', n_jobs=-1, verbose=1).fit(X_train, Y_train)\n    print(search_knn.best_params_)\n    \n    params_logistic = {\n        'max_iter': [2000],\n        'penalty': ['l1', 'l2'],\n        'C': np.logspace(-4, 4, 20),\n        'solver': ['liblinear']\n    }\n    model_logistic = LogisticRegression()\n    search_logistic = GridSearchCV(model_logistic, params_logistic, cv=5, scoring='accuracy', n_jobs=-1, verbose=1).fit(X_train, Y_train)\n    print(search_logistic.best_params_)\n    \n    params_svc = {'kernel': ['rbf'], 'gamma': [i\/10000 for i in range(90, 110)], 'C': range(50, 80, 10), 'probability': [True]}\n    model_svc = SVC()\n    search_svc = GridSearchCV(model_svc, params_svc, cv=5, scoring='accuracy', n_jobs=-1, verbose=1).fit(X_train, Y_train)\n    print(search_svc.best_params_)\n    \n    params_rf = {\n        'n_estimators': [95, 100, 105],\n        'criterion':['entropy'],\n        'bootstrap': [True, False],\n        'max_depth': [40, 45, 50],\n        'max_features': [4, 5, 6],\n        'min_samples_leaf': [1, 2, 3],\n        'min_samples_split': [9, 10, 11],\n        'random_state': [734]}\n    model_rf = RandomForestClassifier()\n    search_rf = GridSearchCV(model_rf, params_rf, cv=5, scoring='accuracy', n_jobs=-1, verbose=1).fit(X_train, Y_train)\n    search_rf.best_params_['random_state']=242\n    search_rf.best_estimator_.random_state=242\n    print(search_rf.best_params_)\n ","1e88b3f3":"if allow_tuning:\n    model_knn = search_knn.best_estimator_\n    model_logistic = search_logistic.best_estimator_\n    model_svc = search_svc.best_estimator_\n    model_rf = search_rf.best_estimator_\n    model_xgb = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n                              colsample_bynode=1, colsample_bytree=0.8,\n                              enable_categorical=False, eval_metric='logloss', gamma=0.8,gpu_id=-1, importance_type=None, interaction_constraints='',\n                              learning_rate=0.15, max_delta_step=0, max_depth=5,\n                              min_child_weight=1, missing=np.nan, monotone_constraints='()',\n                              n_estimators=15, n_jobs=-1, num_parallel_tree=1, predictor='auto',\n                              random_state=0, reg_alpha=1e-05, reg_lambda=1, scale_pos_weight=1,\n                              subsample=0.8, tree_method='exact', use_label_encoder=False,\n                              validate_parameters=1, verbosity=0)\nelse:\n    model_knn = knn(algorithm='auto', \n                    n_neighbors=9,\n                    p=1, \n                    weights='uniform')\n    \n    model_logistic = LogisticRegression(C=0.08858667904100823,\n                                        max_iter=2000, \n                                        penalty='l2', \n                                        solver='liblinear')\n    model_svc = SVC(C=70,\n                    gamma=0.0106,\n                    kernel='rbf',\n                    probability=True)\n    \n    model_rf = RandomForestClassifier(bootstrap=True,\n                                      criterion='entropy',\n                                      max_depth=50, max_features=6, \n                                      min_samples_leaf=1, \n                                      min_samples_split=10, \n                                      n_estimators=100,\n                                      random_state=734)\n    \n    model_xgb = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n                              colsample_bynode=1, colsample_bytree=0.8,\n                              enable_categorical=False, eval_metric='logloss', gamma=0.8,gpu_id=-1, importance_type=None, interaction_constraints='',\n                              learning_rate=0.15, max_delta_step=0, max_depth=5,\n                              min_child_weight=1, missing=np.nan, monotone_constraints='()',\n                              n_estimators=15, n_jobs=-1, num_parallel_tree=1, predictor='auto',\n                              random_state=0, reg_alpha=1e-05, reg_lambda=1, scale_pos_weight=1,\n                              subsample=0.8, tree_method='exact', use_label_encoder=False,\n                              validate_parameters=1, verbosity=0)\n\nmodels = {\n    'knn': model_knn,\n    'logistic': model_logistic,\n    'svc': model_svc,\n    'rf': model_rf,\n    'xgb': model_xgb\n}","d8056e90":"import copy\n\n# goal: The number of models to combine.\n# estimaors: empty list.\n# voting: voting method.\ndef select_models(start, cnt, goal, estimators, voting):\n    if cnt == goal:\n        estimators_copy = copy.deepcopy(estimators)\n        voting_name = f'{voting}_' + '_'.join([i[0] for i in list(estimators_copy)])\n        models[voting_name] = VotingClassifier(estimators=estimators_copy, voting=voting)\n        return\n    for i in range(start, 5):\n        estimators.append(list(models.items())[i])\n        select_models(i + 1, cnt + 1, goal, estimators, voting)\n        estimators.pop()","464470e1":"# create voting models\nselect_models(0, 0, 2, [], 'hard')\nselect_models(0, 0, 3, [], 'hard')\nselect_models(0, 0, 4, [], 'hard')\nselect_models(0, 0, 5, [], 'hard')\n\nselect_models(0, 0, 2, [], 'soft')\nselect_models(0, 0, 3, [], 'soft')\nselect_models(0, 0, 4, [], 'soft')\nselect_models(0, 0, 5, [], 'soft')","e56946c8":"# Dictionary for storing results for each model.\nresult_by_model = pd.DataFrame({'model name': models.keys(), 'model': models.values(), 'score': 0})","621e0082":"# Cross-validation progresses for all models.\nfor name, model in models.items():\n    result_by_model.loc[result_by_model['model name'] == name, 'score'] = cross_val_score(model, X_train_scaled,Y_train,cv=5).mean()","d9b1a87c":"# Cross validation scores of all models.\nresult_by_model.sort_values('score', ascending=False).reset_index(drop=True)","4eb19f51":"model_name = 'soft_knn_rf_xgb'\nmodels[model_name].fit(X_train_scaled, Y_train)\ny_pred = models[model_name].predict(X_test_scaled).astype('int')\n\nsubmission = pd.DataFrame({'PassengerId': PassengerId, \n                              'Survived': y_pred})\n\nsubmission.to_csv('submission2.csv', index = False)","44197415":"<h3><b>Take a look at the training data:<\/b><\/h3>\n<b>----------------------------------------------------------------------------------------------------<\/b>\n<h4>Summary<h4\/>\n<li>Training set has 891 rows and test set has 418 rows<\/li>\n<li>Training set have 12 features and test set have 11 features<\/li>\n<li>One extra feature in training set is Survived feature, which is the target variable<\/li>\n<br>\n<li>Numerical Features: Age (Continuous), Fare (Continuous), SibSp (Discrete), Parch (Discrete)<\/li>\n<li>Categorical Features: Survived, Sex, Embarked, Pclass<\/li>\n<li>Alphanumeric Features: Ticket, Cabin<\/li>","88c39cdc":"<H2>Modeling<\/H2>","89658729":"<h2>Exploratory Data Analysis<\/h2>\n<h4><li>Data exploration process which aims to understand the contents and components of the data.<\/li><\/h4>","bf0cda69":"<h2>Final","ba31060d":"<H1>Introduction<\/H1>\n<h2>This is my first kernel at Kaggle. I choosed the Titanic competition which is a good way as starter for beginner like me <\/h2>","f6482042":"<h2>Data Preprocessing<\/h2>\n<h4>\n1) Drop null values from Embarked (only 2)\n<br><br>\n2) Include only relevant variables (Since we have limited data, I wanted to exclude things like name and passanger ID so that we could have a reasonable number of features for our models to deal with)\n<br><br>\n3) Do categorical transforms on all data. Usually we would use a transformer, but with this approach we can ensure that our traning and test data have the same colums. We also may be able to infer something about \nthe shape of the test data through this method. I will stress, this is generally not recommend outside of a competition (use onehot encoder).\n<br><br>\n4) Impute data with mean for fare and age (Should also experiment with median)\n<br><br>\n5) Normalized fare using logarithm to give more semblance of a normal distribution\n<br><br>\n6) Scaled data 0-1 with standard scaler\n<\/h4>"}}