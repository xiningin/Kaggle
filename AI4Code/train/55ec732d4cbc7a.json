{"cell_type":{"8e43912d":"code","15dbdf74":"code","3a386663":"code","428ef794":"code","4c7a8246":"code","ad219fa5":"code","75419027":"code","49476907":"code","6d2c5f42":"code","9d3e787f":"code","2f392970":"code","9b9f709b":"code","8e3c97bf":"code","9899aad3":"code","d4a604ed":"code","ded6db4e":"code","f66a659a":"code","4252ed09":"code","35450521":"code","89ba0f8d":"code","3dfdbfa2":"code","6f013cbc":"code","2a23fa35":"code","7b7ea5d3":"code","b2779750":"code","4e26b6c4":"code","fca49a9f":"code","ef7ec980":"code","92c9c46d":"code","dc85d4e5":"code","19da4cbb":"markdown","86d6b2c8":"markdown","d0072a46":"markdown","db382e8f":"markdown","edd622fc":"markdown","1fd705d6":"markdown","6509aead":"markdown","55947dc4":"markdown","d7e7f9c6":"markdown","9c2c336a":"markdown","a5c67db2":"markdown","5b179fa3":"markdown","70f477e4":"markdown","557e8809":"markdown","1cb9ee9b":"markdown"},"source":{"8e43912d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","15dbdf74":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","3a386663":"nRowsRead = None # 'None' if want to read whole file\n# avocado.csv has 18250 rows in reality, but we are only loading\/previewing the first 1000 rows\ndf = pd.read_csv('..\/input\/tsdata-1\/avocado.csv', delimiter=',', nrows = nRowsRead)\ndf.dataframeName = 'avocado.csv'\nnRow, nCol = df.shape\nprint(f'There are {nRow} rows and {nCol} columns')","428ef794":"df.head()","4c7a8246":"# Distribution graphs (histogram\/bar graph) of column data\ndef plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) \/ nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()","ad219fa5":"plotPerColumnDistribution(df, 10, 20)","75419027":"df.head()","49476907":"df.drop('Unnamed: 0', axis=1, inplace=True)","6d2c5f42":"df.head()","9d3e787f":"df.info()","2f392970":"df['Date'] = pd.to_datetime(df['Date'])\ndf['Month'] = df['Date'].apply(lambda x: x.month)\ndf['Day'] = df['Date'].apply(lambda x: x.day)","9b9f709b":"df.head()","8e3c97bf":"byDate = df.groupby('Date').mean()\nplt.figure(figsize=(12, 8))\nbyDate['AveragePrice'].plot()\nplt.title('Average Price')\nplt.show()","9899aad3":"plt.figure(figsize=(12, 6))\nsns.heatmap(df.corr(), cmap='coolwarm', annot=True)\nplt.show()","d4a604ed":"df['region'].nunique()\n","ded6db4e":"df['type'].nunique()","f66a659a":"df_final = pd.get_dummies(df.drop(['region', 'Date'], axis=1), drop_first=True)","4252ed09":"df_final.head()","35450521":"df_final.tail()","89ba0f8d":"X = df_final.iloc[:, 1:14]\ny = df_final['AveragePrice']\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","3dfdbfa2":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(X_train, y_train)\npred = lr.predict(X_test)","6f013cbc":"from sklearn import metrics\nprint('MAE:', metrics.mean_absolute_error(y_test, pred))\nprint('MSE:', metrics.mean_squared_error(y_test, pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, pred)))","2a23fa35":"plt.scatter(x=y_test, y=pred)\nplt.show()","7b7ea5d3":"from sklearn.tree import DecisionTreeRegressor\ndtr = DecisionTreeRegressor()\ndtr.fit(X_train, y_train)\npred = dtr.predict(X_test)","b2779750":"plt.scatter(x=y_test, y=pred)\nplt.xlabel('Y Test')\nplt.ylabel('Predicted Y')\nplt.show()\n","4e26b6c4":"print('MAE:', metrics.mean_absolute_error(y_test, pred))\nprint('MSE:', metrics.mean_squared_error(y_test, pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, pred)))","fca49a9f":"from sklearn.ensemble import RandomForestRegressor\nrdr = RandomForestRegressor()\nrdr.fit(X_train, y_train)\npred = rdr.predict(X_test)","ef7ec980":"print('MAE:', metrics.mean_absolute_error(y_test, pred))\nprint('MSE:', metrics.mean_squared_error(y_test, pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, pred)))\n","92c9c46d":"sns.displot((y_test-pred), bins=50)\nplt.show()","dc85d4e5":"data = pd.DataFrame({'Y Test': y_test, 'Pred': pred}, columns=['Y Test', 'Pred'])\nsns.lmplot(x='Y Test', y='Pred', data=data, palette='rainbow')\ndata.head()\nplt.show()","19da4cbb":"*As we can from the heatmap above, all the Features are not corroleted with the Average Price column, instead most of them are correlated with each other.So now I am bit worried because that will not help us get a good model. Lets try and see.First we have to do some Feature Engineering on the categorical Features : region and type*","86d6b2c8":"*Nice, here we can see that we nearly have a straigt line, so it's better than the Linear regression model, and to be more sure let's check the RMSE*","d0072a46":"*Very Nice, our RMSE is lower than the previous one we got with Linear Regression. Now I am going to try one last model to see if I can improve my predictions for this data which is the RandomForestRegressor*","db382e8f":"*As we can see that we don't have a straight line so I am not sure if this is the best model we can apply on our data. Let's try working with the Decision Tree Regression model*","edd622fc":"*Let's check our data head again to make sure that the  Unnamed:0 is removed*","1fd705d6":"*Well as we can see the RMSE is lower than the two previous models, so the RandomForest Regressor is the best model in this case*","6509aead":"*The\"Unnamed:0\" is just a representation of the indexes, so it's useless to keep it, let's remove it !*","55947dc4":"*Let's check the head to see what we have done:*","d7e7f9c6":"*Now our data is ready! let's apply our model which is going to be the Linear Regression because our Target variable 'AveragePrice'is continuous. Let's now begin to train out regression model. We will need to first split up our data into a X array that contains the features to train on, and a y array with the target variable*","9c2c336a":"*Notice here that our residuals looked to be normally distributed and that's really a good sign which means that our model was a correct choice for the data.*","a5c67db2":"***Creating and Training the Model:***","5b179fa3":"*Distribution graphs (histogram\/bar graph) of sampled columns:*","70f477e4":"*Well, as a first observation we can see that we are lucky, cuz we don't have any missing values (18249 complete data) and 13 columns.                                                                               Now let's do some Feature Engineering on the Date Feature so we can be able to use the day and the month columns in building our machine learning model later.*","557e8809":"*As we can see we have 54 regions and 2 unique types, so it's going to be easy to to transform the type feature to dummies, but for the region it's going to be a bit complex so I decided to drop the entire column. I will drop the Date Feature as well because I already have 3 other columns for the Year, Month and Day.*","1cb9ee9b":"*Now let's use the info() methode to get a general idea about our data:*"}}