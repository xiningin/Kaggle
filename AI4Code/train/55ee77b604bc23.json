{"cell_type":{"b9930247":"code","83721898":"code","3056f4a2":"code","f6aaba6b":"code","5e658871":"code","bc55b8f0":"code","8c514a35":"code","004fb751":"code","a02ef83c":"code","a084f0d9":"code","55d6dce7":"code","87fbe2c6":"code","a8707f0e":"code","13f7fd2c":"code","9a99f2b9":"code","87a89b06":"code","a44b1dd3":"code","d2f45f50":"code","a3a72f78":"code","57c3e303":"code","983ff613":"code","861d73ea":"code","ccf6b4de":"code","cd790945":"code","bff210d0":"code","a2496e25":"code","626207ea":"code","27cbe6f7":"code","2b7236ef":"code","0ac729e0":"code","d4de3dfb":"code","589eaaa7":"code","fad55f28":"markdown","f3b79224":"markdown","61c19a1d":"markdown","f3959eb0":"markdown","91fc79e1":"markdown","ee74a6c1":"markdown","9096758e":"markdown","e2aba733":"markdown","4b76bdc7":"markdown","34517ef4":"markdown","7dc5faba":"markdown","43699175":"markdown","bd778a33":"markdown","6a1ee22c":"markdown","a783e4cc":"markdown","6c8e60a3":"markdown","9b9b1b71":"markdown","2be478c4":"markdown","40c85606":"markdown","3e5fec3c":"markdown","6508097c":"markdown","496d64b5":"markdown","fb48037d":"markdown","9a0dc747":"markdown","138da60b":"markdown","02e57123":"markdown","0f5f25b0":"markdown","ae1b99f5":"markdown","7dad105f":"markdown","ab5482ca":"markdown","9f3988ac":"markdown","1091e984":"markdown","1200827f":"markdown"},"source":{"b9930247":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","83721898":"data = pd.read_csv('..\/input\/consumer_complaints.csv')","3056f4a2":"data","f6aaba6b":"verbatim_product = data[['consumer_complaint_narrative','product']]","5e658871":"verbatim_product.head(5)","bc55b8f0":"filtered_verbatim = verbatim_product.dropna()\nfiltered_verbatim.head(2)","8c514a35":"len(filtered_verbatim.consumer_complaint_narrative)","004fb751":"filtered_verbatim['product'].value_counts()","a02ef83c":"filtered_verbatim['product'].value_counts().plot(kind='bar')","a084f0d9":"complaint = filtered_verbatim.iloc[1]['consumer_complaint_narrative']\npd.options.display.max_colwidth = 1000\nprint(complaint)","55d6dce7":"import spacy #for our NLP processing\nimport nltk #to use the stopwords library\nimport string # for a list of all punctuation\nfrom nltk.corpus import stopwords # for a list of stopwords","87fbe2c6":"nlp = spacy.load('en_core_web_sm')\ntext = nlp(complaint)","a8707f0e":"text","13f7fd2c":"tokens = [tok for tok in text]\ntokens.head(5)","9a99f2b9":"tokens = [tok.lemma_ for tok in text]\ntokens","87a89b06":"tokens = [tok.lemma_.lower().strip() for tok in text]\ntokens","a44b1dd3":"tokens = [tok.lemma_.lower().strip() for tok in text if tok.lemma_ != '-PRON-']\ntokens","d2f45f50":"stop_words = stopwords.words('english')\npunctuations = string.punctuation\nstop_words","a3a72f78":"tokens = [tok for tok in tokens if tok not in stop_words and tok not in punctuations]","57c3e303":"tokens","983ff613":"def cleanup_text(complaint):\n    doc = nlp(complaint, disable=['parser', 'ner'])\n    tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']\n    tokens = [tok for tok in tokens if tok not in stop_words and tok not in punctuations]\n    return tokens\n","861d73ea":"limit = 100\ndoc_sample = filtered_verbatim.consumer_complaint_narrative\nprint('tokenized and lemmatized document: ')\n\nfor idx, complaint in enumerate(doc_sample):\n    print(cleanup_text(complaint))\n    if idx == limit:\n        break\n    ","ccf6b4de":"doc_sample = doc_sample[0:10000]\n#doc_sample = doc_sample[:]","cd790945":"processed_docs = doc_sample.map(cleanup_text)","bff210d0":"import gensim\ndictionary = gensim.corpora.Dictionary(processed_docs)","a2496e25":"dictionary.filter_extremes(no_below=10, no_above=0.5, keep_n=100000)","626207ea":"bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]","27cbe6f7":"bow_doc_4310 = bow_corpus[4310]\n\nfor i in range(len(bow_doc_4310)):\n    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n                                                     dictionary[bow_doc_4310[i][0]], \n                                                     bow_doc_4310[i][1]))","2b7236ef":"lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2 )","0ac729e0":"for idx, topic in lda_model.print_topics(-1):\n    print('Topic: {} \\nWords: {}'.format(idx, topic))","d4de3dfb":"import pyLDAvis\nimport pyLDAvis.gensim as gensimvis","589eaaa7":"vis_data = gensimvis.prepare(lda_model, bow_corpus, dictionary)\npyLDAvis.display(vis_data)","fad55f28":"Lets display our topics","f3b79224":"For this test case we will be using the US Consumer Finance Complaints, which holds verbatim complaints as well as product information among other fields.\nIn this exercise we will focus on the customer verbatim to distinguish topics following an unsupervised learning approach","61c19a1d":"# Topic Modelling Demo","f3959eb0":"The LDA algorithm has a number of parameters than can be used to calibrate the output:\n- num_topics: In this example we have prescribed a number 10, in a previous run without a prescribed number, the LDA produced 99 clusters which is not very informative for our usecase\n- id2word: The previously defined dictionary mapping from word IDs to Words\n- Workers: for parralelisation\n- chunksize: number of documents to use in each training chunk\n- passes: no. passes through the corpus during training\n- alpha: Can be set to an 1D array of length equal to the number of expected topics that expresses our a-priori belief for the each topics\u2019 probability.\n- decay: A number between (0.5, 1] to weight what percentage of the previous lambda value is forgotten when each new document is examined.\n- iterations: Maximum number of iterations through the corpus when inferring the topic distribution of a corpus.\n- gamma_threshold: Minimum change in the value of the gamma parameters to continue iterating.","91fc79e1":"In the interest of time (as we don't have much time in this interactive demo), we'll run the rest of the demo on 10k complaints","ee74a6c1":"Lets import the required libraries for our natural language processing","9096758e":"The dictionary is then converted to a bag of words format","e2aba733":"# LDA","4b76bdc7":"Let's start by tokenising our complaint -- Splitting it out into words","34517ef4":"Now let's get rid of all the -PRON- lemmas as they will add no value to our analysis","7dc5faba":"The dictionary is then filtered to remove extreme values using the following parameters:\n- *no_below* parameter is an absolute number - Words appearing less than 10 times in the entire corpus are removed from the analysis\n- *no_above* parameter is a fraction - Words appearing more than 50% of the time are removed from the analysis","43699175":"We'll look to see if this looks correct on the first 100 complaints - pre-check  \nWe'll also declare **doc_sample** as the complaints verbatim","bd778a33":"We will now be looking at the tokens in our tokens list (no longer using the spacy\/ nlp(text) object) - and remove any puntuation and stop_words","6a1ee22c":"We display an example of the bag-of-words format on a single complaint to ensure it worked (here on complaint 4310)","a783e4cc":"And there we have! We've tokenized, lemmatied, lowercased, stripped of white spaces + removed stopwords and punctuations","6c8e60a3":"Lets now use another library - NLTK to get a list of stopwords (think: I, me, you ,they etc.) Words that won't really add much value to our analysis, more like fillers between the important words","9b9b1b71":"Let's select a single complaint to start doing some NLP on","2be478c4":"Let's put it all together into a function so we can apply these steps to every complaint","40c85606":"We take a subset of our data","3e5fec3c":"We remove any complaints which don't have any verbatim to analyse, ","6508097c":"Latent Dirichlet allocation (LDA), is an  **unsupervised** algorithm: only the words in the documents are modeled. \n- The goal is to _infer topics that maximize the likelihood (or the posterior probability) of the collection_.","496d64b5":"We can also use pyLDA vis to inspect our outputs in a more interactive way","fb48037d":"## Running LDA on Bag of Words","9a0dc747":"The dictionary encapsulates the mapping between normalized words and their integer ids","138da60b":"We can now apply our function to doc_sample and process our 10k complaints using the .map function","02e57123":"Now we can load and use spacy to analyse our complaint","0f5f25b0":"# Bag of Words","ae1b99f5":"Exploring Topic Modelling using:\n    - Latent Dirichlet Allocation (LDA) following both Bag of words and TF-IDF approach","7dad105f":"What are the most popular products owned","ab5482ca":"How many complaints are there with Verbatim?","9f3988ac":"For our bag of words to have less overlap - lets lemmatize our words","1091e984":"Can we see this as a chart?","1200827f":"To ensure our words match up and there are no sneaky spaces let's strip any whitespace around the tokens, and lowercase our text to ensure words like 'Credit' and 'credit' are matched"}}