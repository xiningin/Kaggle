{"cell_type":{"10fa5780":"code","ab156fbe":"code","98b73bbe":"code","ff8e2f7b":"code","e0de4808":"code","736325ff":"code","7768ab26":"code","cc991bf7":"code","50d10b61":"code","102dd8be":"code","60ca0416":"code","c2807597":"code","b59f7b04":"code","098b42ff":"code","afadd3c9":"code","ce40a949":"code","d95d821b":"code","98f4b5b5":"code","7bb9e232":"code","d1306b4a":"code","473250fa":"code","169eeb3d":"code","1fcf93c8":"code","721b926d":"code","23eaa726":"code","3b3ab29f":"code","910b43e1":"code","735c073f":"code","10b55f36":"markdown","4b24b1e2":"markdown","8c0a2672":"markdown","c0515334":"markdown","38fca2cc":"markdown","b2f8879e":"markdown","fd1b161f":"markdown","50c93af7":"markdown","0a74174a":"markdown"},"source":{"10fa5780":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","ab156fbe":"from numpy import array\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import SimpleRNN\nfrom keras.layers import Flatten\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import LSTM\nfrom keras.layers import Bidirectional\n\n\nfrom numpy import asarray\nfrom numpy import zeros\n\nimport random","98b73bbe":"import pickle\nwith open('\/kaggle\/input\/patents-data\/abstracts_data.pkl', 'rb') as f:\n    abstract_data = pickle.load(f)\n\ndel abstract_data['A']    \ndel abstract_data['F']    \ndel abstract_data['G']    \ndel abstract_data['H']    \n\ndel abstract_data['D']    \ndel abstract_data['E']    \n\ncategories = list(abstract_data.keys())\n\n\ni = 0\nlabels = []\nfor category in categories:\n    for x in range(9996):\n        labels.append(i)\n    i+=1\n\nfrom keras.utils import np_utils\nlabels = np_utils.to_categorical(labels)    \nlabels = array(labels)\n\ndata = []\n\nfor category in categories:\n    for d in abstract_data[category][0:9996]:\n        data.append(d)\n        \n","ff8e2f7b":"for category in categories:\n    print(category+':'+str(len(abstract_data[category])))","e0de4808":"with open('\/kaggle\/input\/patents-data\/abstracts_testing_data.pkl', 'rb') as f:\n    abstract_testing_data = pickle.load(f)\n\ndel abstract_testing_data['A']    \ndel abstract_testing_data['F']    \ndel abstract_testing_data['G']    \ndel abstract_testing_data['H']  \n    \ndel abstract_testing_data['D']\ndel abstract_testing_data['E']    \n\ncategories = list(abstract_testing_data.keys())\n\ni = 0\ntesting_labels = []\nfor category in categories:\n    for x in range(300):\n        testing_labels.append(i)\n    i+=1\n\nfrom keras.utils import np_utils\ntesting_labels = np_utils.to_categorical(testing_labels)    \ntesting_labels = array(testing_labels)\n\n\ntesting_data = []\nfor category in categories:\n    for d in abstract_testing_data[category][0:300]:\n        testing_data.append(d)\n","736325ff":"l=0\ntotal_data=testing_data+data\nfor x in total_data:\n    l=l+len(x)\nlength_long_sentence = (l)\/(len(total_data))","7768ab26":"word_tokenizer = Tokenizer()\nword_tokenizer.fit_on_texts(data+testing_data)\n\nvocab_length = len(word_tokenizer.word_index) + 1\nvocab_length\n\nembedded_sentences = word_tokenizer.texts_to_sequences(data)\n\nembedded_testing_sentences = word_tokenizer.texts_to_sequences(testing_data)\n\n\nfrom nltk.tokenize import word_tokenize\n\nword_count = lambda sentence: len(word_tokenize(sentence))\nlongest_sentence = max(data+testing_data, key=word_count)\n#length_long_sentence = len(word_tokenize(longest_sentence))\n#length_long_sentence = 230\nlength_long_sentence = int(length_long_sentence + 10)\npadded_sentences = pad_sequences(embedded_sentences, length_long_sentence, padding='post')\n\npadded_testing_sentences = pad_sequences(embedded_testing_sentences, length_long_sentence, padding='post')","cc991bf7":"embeddings_dict = {}\nwith open(\"\/kaggle\/input\/patents-data\/glove.6B.100d.txt\", 'r') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vector = np.asarray(values[1:], \"float32\")\n        embeddings_dict[word] = vector\n\n\nembedding_matrix = zeros((vocab_length, 100))\nfor word, index in word_tokenizer.word_index.items():\n    embedding_vector = embeddings_dict.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector\n","50d10b61":"model = Sequential()\nembedding_layer = Embedding(vocab_length, 100, weights=[embedding_matrix], input_length=length_long_sentence, trainable=False)\nmodel.add(embedding_layer)\n#model.add(Flatten())\nmodel.add(Bidirectional(LSTM(100)))\n#model.add(SimpleRNN(100))\n#model.add(Dense(50, activation = 'relu'))\nmodel.add(Dense(len(categories), activation='softmax'))\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\nprint(model.summary())","102dd8be":"history = model.fit(padded_sentences, labels,\n                    epochs=10,\n                    batch_size=128,\n                    validation_split=0.2)","60ca0416":"scores = model.evaluate(padded_testing_sentences, testing_labels)\nprint(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n","c2807597":"import matplotlib.pyplot as plt\n%matplotlib inline \nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","b59f7b04":"patent_embeddings_dict = {}\nwith open(\"\/kaggle\/input\/patents-data\/patent-100.vec\/patent-100.vec\", 'r') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        try:\n            vector = np.asarray(values[1:101], \"float32\")\n            patent_embeddings_dict[word] = vector\n        except ValueError:\n            pass\n        \npatent_embedding_matrix = zeros((vocab_length, 100))\nfor word, index in word_tokenizer.word_index.items():\n    embedding_vector = patent_embeddings_dict.get(word)\n    if embedding_vector is not None:\n        patent_embedding_matrix[index] = embedding_vector","098b42ff":"model = Sequential()\nembedding_layer = Embedding(vocab_length, 100, weights=[patent_embedding_matrix], input_length=length_long_sentence, trainable=False)\nmodel.add(embedding_layer)\n#model.add(Flatten())\nmodel.add(Bidirectional(LSTM(100)))\n#model.add(SimpleRNN(100))\n#model.add(Dense(50, activation = 'relu'))\nmodel.add(Dense(len(categories), activation='softmax'))\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\nprint(model.summary())","afadd3c9":"history = model.fit(padded_sentences, labels,\n                    epochs=10,\n                    batch_size=128,\n                    validation_split=0.2)","ce40a949":"scores = model.evaluate(padded_testing_sentences, testing_labels)\nprint(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))","d95d821b":"%matplotlib inline \nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","98f4b5b5":"!pip install keras-bert\n!pip install keras-rectified-adam\n\n!wget -q https:\/\/storage.googleapis.com\/bert_models\/2018_10_18\/uncased_L-12_H-768_A-12.zip\n!unzip -o uncased_L-12_H-768_A-12.zip","7bb9e232":"import codecs\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom chardet import detect\nimport keras\nfrom keras_radam import RAdam\nfrom keras.optimizers import Adam\nfrom keras import backend as K\nfrom keras_bert import load_trained_model_from_checkpoint","d1306b4a":"SEQ_LEN = 150\nBATCH_SIZE = 50\nEPOCHS = 5\nLR = 1e-4","473250fa":"pretrained_path = 'uncased_L-12_H-768_A-12'\nconfig_path = os.path.join(pretrained_path, 'bert_config.json')\ncheckpoint_path = os.path.join(pretrained_path, 'bert_model.ckpt')\nvocab_path = os.path.join(pretrained_path, 'vocab.txt')","169eeb3d":"model = load_trained_model_from_checkpoint(\n      config_path,\n      checkpoint_path,\n      training=True,\n      trainable=True,\n      seq_len=SEQ_LEN,\n  )","1fcf93c8":"import codecs\nfrom keras_bert import Tokenizer\ntoken_dict = {}\nwith codecs.open(vocab_path, 'r', 'utf8') as reader:\n    for line in reader:\n        token = line.strip()\n        token_dict[token] = len(token_dict)\n        \ntokenizer = Tokenizer(token_dict)","721b926d":"i = 0\nlabels = []\nfor category in categories:\n    for x in range(9996):\n        labels.append(i)\n    i+=1\n\ni = 0\ntesting_labels = []\nfor category in categories:\n    for x in range(300):\n        testing_labels.append(i)\n    i+=1\n\ndata_indices = []\nfor d in data:\n    ids, segments = tokenizer.encode(d, max_len=SEQ_LEN)\n    data_indices.append(ids)\n\ntest_data_indices = []\nfor d in testing_data:\n    ids, segments = tokenizer.encode(d, max_len=SEQ_LEN)\n    test_data_indices.append(ids)    \n\nitems = list(zip(data_indices, labels))\ntest_items = list(zip(test_data_indices, testing_labels))\n\nnp.random.shuffle(items)\nnp.random.shuffle(test_items)","23eaa726":"indices_train, sentiments_train = zip(*items)\nindices_train = np.array(indices_train)\ntrain_x, train_y = [indices_train, np.zeros_like(indices_train)], np.array(sentiments_train)\n\nindices_test, sentiments_test = zip(*test_items)\nindices_test = np.array(indices_test)\ntest_x, test_y = [indices_test, np.zeros_like(indices_test)], np.array(sentiments_test)\n","3b3ab29f":"inputs = model.inputs[:2]\ndense = model.get_layer('NSP-Dense').output\noutputs = keras.layers.Dense(units=2, activation='softmax')(dense)\n\nmodel = keras.models.Model(inputs, outputs)\nmodel.compile(\n  Adam(learning_rate =LR),\n  loss='sparse_categorical_crossentropy',\n  metrics=['sparse_categorical_accuracy'],\n)","910b43e1":"history = model.fit(\n    train_x,\n    train_y,\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    validation_split=0.2\n)","735c073f":"predicts = model.predict(test_x, verbose=True).argmax(axis=-1)\nscore = (np.sum(test_y == predicts) \/ test_y.shape[0])\n\nprint(\"\\n%s: %.2f%%\" % ('Accuracy', score*100))","10b55f36":"Preparing Testing Data obtained from XML files but saved as a pickle.","4b24b1e2":"Training the model.","8c0a2672":"Loading the embedding of Prof Ralf.","c0515334":"BERT","38fca2cc":"Verificaion of the model.","b2f8879e":"Tokenzing and padding the training and testing data.","fd1b161f":"Setting up the model which utilizes Glove embedding.","50c93af7":"Preparing Training Data obtained from XML files but saved as a pickle.","0a74174a":"Importing and preparing the Glove embedding of each word in our dictionay."}}