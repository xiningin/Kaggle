{"cell_type":{"8ac26e02":"code","670151e0":"code","16128356":"code","e2bb291d":"code","2f416c0e":"code","df4e8d7e":"code","8845f967":"code","62bf5e16":"code","bb139986":"code","a90a0219":"code","b5b4f0e6":"code","867ea8c0":"code","ce7cb8c9":"code","01e6ff16":"code","73f899d3":"code","05a37b62":"code","43b72628":"code","bdfeccc1":"code","6a3fb75f":"code","27981d99":"code","7c827187":"code","bcb1668f":"code","23ed94a3":"code","35f8db41":"code","54fc6aaa":"code","f9e8dc44":"code","ad396e64":"code","b0300cc2":"code","b43cb3e5":"code","edd748f4":"code","ce9d72cf":"code","7af34b3f":"code","b16eb91f":"code","ef47b663":"code","521e7053":"code","d1cf4d2a":"code","14b8f45a":"markdown","0effead8":"markdown","ff94a1d8":"markdown","037488b0":"markdown","cfa224ce":"markdown","d6456f28":"markdown","7c00bb3d":"markdown","7feb6390":"markdown","aa706bd5":"markdown","04a27c32":"markdown","9a5dea83":"markdown","db2e2eed":"markdown","c361d38d":"markdown","a7496c55":"markdown","93ee6718":"markdown","8cef6ac0":"markdown","9b31ac97":"markdown","f139d66e":"markdown","b228514d":"markdown","658e5721":"markdown","cfa7a2c4":"markdown","e87c4884":"markdown"},"source":{"8ac26e02":"#importing\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.preprocessing import LabelBinarizer, MinMaxScaler\nfrom sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, MeanShift, estimate_bandwidth\n","670151e0":"df= pd.read_csv('..\/input\/unsupervised-learning-on-country-data\/Country-data.csv',sep=',')\ndf.head()","16128356":"df.shape","e2bb291d":"df.dtypes.value_counts()","2f416c0e":"df_new = df.drop(columns='country')","df4e8d7e":"for column in df_new:\n    df_new[column] =df_new[column].astype(float)\n\ndf_new.dtypes.value_counts()","8845f967":"# checking for missing values\ndf_new.isnull().sum()","62bf5e16":"# see the min and max of data excluding the target\nprint('min = ',df_new.iloc[:, :-1].min().value_counts())\nprint('max = ',df_new.iloc[:, :-1].max().value_counts())","bb139986":"skew_limit = 0.75 # define a limit above which we will log transform\nskew_vals = df_new[df_new.columns].skew()","a90a0219":"# Showing the skewed columns\nskew_cols = (skew_vals\n             .sort_values(ascending=False)\n             .to_frame()\n             .rename(columns={0:'Skew'})\n             .query('abs(Skew) > {}'.format(skew_limit)))\n\nskew_cols","b5b4f0e6":"# Perform the skew transformation:\nfor col in skew_cols.index.values:\n    df_new[col] = df_new[col].apply(np.log1p)","867ea8c0":"df_new.hist()","ce7cb8c9":"# The correlation matrix\ncorr_mat = df_new.corr()\n# Strip out the diagonal values for the next step\nfor x in range(len(df_new.columns)):\n    corr_mat.iloc[x,x] = 0.0\n    \ncorr_mat","01e6ff16":"corr_mat.abs().idxmax()","73f899d3":"#  let's look at distribution of health and income\nsns.set_context('notebook')\nsns.set_style('white')\n\nplt.figure(figsize=(14,6))\nplt.suptitle('Density Distribution of health and income', size=20)\n\n# create first histplot\nplt.subplot(1,2,1)\nsns.histplot(df_new['health'], kde=True, color='red')\nplt.ylabel('Health')\n\n# create second histplot\nplt.subplot(1,2,2)\nsns.histplot(df_new['income'], kde=True, color='green')\nplt.xlabel('Net income per person ')","05a37b62":"#scaling\nmms = MinMaxScaler()\n\nfor col in df_new.columns:\n    df_new[col] = mms.fit_transform(df_new[[col]]).squeeze()","43b72628":"df_new.describe().T","bdfeccc1":"df_new.isnull().sum()","6a3fb75f":"df_new[df_new['inflation'].isnull()]","27981d99":"df_new.fillna(0,inplace=True)","7c827187":"X = df_new[['health','income']]","bcb1668f":"inertia = []\nlist_num_clusters = list(range(1,11))\nfor num_clusters in list_num_clusters:\n    km = KMeans(n_clusters=num_clusters)\n    km.fit(X)\n    inertia.append(km.inertia_)","23ed94a3":"# elbow method\n\nfig, ax = plt.subplots(figsize=(14, 8))\nplt.plot(list_num_clusters,inertia, color='navy')\nplt.scatter(list_num_clusters,inertia)\nplt.xlabel('Number of Clusters')\nplt.ylabel('Inertia')\nplt.title('The Elbow Method', fontsize=20)\n\n# Annotate arrow\n# ax.annotate('Possible Elbow Point', xy=(5, 4), xytext=(5, 6), xycoords='data',          \n#              arrowprops=dict(arrowstyle='->', connectionstyle='arc3', color='red', lw=2))","35f8db41":"X = df_new[['health','income']]","54fc6aaa":"# silhouette index\nfrom sklearn.metrics import silhouette_score\n\nrange_n_clusters = list (range(2,10))\nfor n_clusters in range_n_clusters:\n    clusterer = KMeans(n_clusters=n_clusters).fit(X)\n    preds = clusterer.predict(X)\n    centers = clusterer.cluster_centers_\n\n    score = silhouette_score (X, preds, metric='euclidean')\n    print (\"For n_clusters = {}, silhouette score is {}\".format(n_clusters, score))","f9e8dc44":"# 2 cluster\nkm6 = KMeans(n_clusters=2).fit(X)\n\nX['Labels'] = km6.labels_\nplt.figure(figsize=(12, 8))\nsns.scatterplot(X['health'], X['income'], hue=X['Labels'], \n                palette=sns.color_palette('gist_rainbow', 2))\nplt.scatter(km6.cluster_centers_[:, 0], km6.cluster_centers_[:, 1], s = 50, c = 'black')\nplt.xlabel('health')\nplt.ylabel('income')\nplt.title('KMeans with 2 Clusters', fontsize=20)\nplt.legend(loc=6, bbox_to_anchor=(1, 0.5), ncol=1)","ad396e64":"df['Labels'] = km6.labels_\ndf[['country','Labels']].sample(5)","b0300cc2":"# silhouette index\nfrom sklearn.metrics import silhouette_score\n\nrange_n_clusters = list (range(2,10))\nfor n_clusters in range_n_clusters:\n    ward = AgglomerativeClustering(n_clusters=n_clusters)\n    ward = ward.fit(X)\n    preds = ward.fit_predict(X)\n    \n    score = silhouette_score (X, preds, metric='euclidean')\n    print (\"For n_clusters = {}, silhouette score is {}\".format(n_clusters, score))","b43cb3e5":"agglom = AgglomerativeClustering(n_clusters=2).fit(X)\n\nX['Labels'] = agglom.labels_\nplt.figure(figsize=(10, 6))\nsns.scatterplot(X['health'], X['income'], hue=X['Labels'], \n                palette=sns.color_palette('tab10', 2))\nplt.xlabel('health')\nplt.ylabel('income')\nplt.title('Agglomerative Clustering with 2 Clusters', fontsize=20)\nplt.legend(loc=6, bbox_to_anchor=(1, 0.5), ncol=1)","edd748f4":"from scipy.cluster import hierarchy\nHL = hierarchy.linkage(agglom.children_, method='ward')\n\nfig, ax = plt.subplots(figsize=(15,5))\n\nhierarchy.set_link_color_palette(['red', 'green'])\n\nden = hierarchy.dendrogram(HL, orientation='top', \n                           p=30, truncate_mode='lastp',\n                            show_leaf_counts=True, ax=ax,\n                           above_threshold_color='blue')\nplt.xlabel('Counrties')\nplt.ylabel('Euclidean distance')\nplt.title('Dendrogram', fontsize=20)","ce9d72cf":"# determine epsilon and min_samples values\nepsilon = 10\nmin_samples = 5","7af34b3f":"# DBSCAN\ndb = DBSCAN(eps=epsilon, min_samples=min_samples).fit(X)\n\nX['Labels'] = db.labels_\nplt.figure(figsize=(10, 6))\nsns.scatterplot(X['health'], X['income'], hue=X['Labels'], \n                palette='tab10')\nplt.xlabel('health')\nplt.ylabel('income')\nplt.title('DBSCAN with epsilon=10 and min_samples=5', fontsize=20)\nplt.legend(loc=6, bbox_to_anchor=(1, 0.5), ncol=1)","b16eb91f":"# the following bandwidth can be automatically detected using\nbandwidth = estimate_bandwidth(X, quantile=0.1)\n\n# MeanShift \nms = MeanShift(bandwidth=bandwidth).fit(X)\n\nX['Labels'] = ms.labels_\nplt.figure(figsize=(10, 6))\nsns.scatterplot(X['health'], X['income'], hue=X['Labels'], \n                palette=sns.color_palette('hls', np.unique(ms.labels_).shape[0]))\nplt.scatter(ms.cluster_centers_[:, 0], ms.cluster_centers_[:, 1], s = 50, c = 'black')\nplt.xlabel('health')\nplt.ylabel('income')\nplt.title('Mean Shift', fontsize=20)\nplt.legend(loc=6, bbox_to_anchor=(1, 0.5), ncol=1)","ef47b663":"X = df_new[['health','income']]\n\nplt.figure(figsize=(20,16))\nplt.suptitle('Clustering Results for health and income', fontsize=20)\n\nplt.subplot(221)\nkm5 = KMeans(n_clusters=2).fit(X)\nX['Labels'] = km5.labels_\nsns.scatterplot(X['health'], X['income'], hue=X['Labels'], \n                palette=sns.color_palette('tab10', 2))\nplt.xlabel('health')\nplt.ylabel('income')\nplt.title('KMeans with 2 Clusters', fontsize=15)\nplt.legend(loc=6, bbox_to_anchor=(1, 0.5), ncol=1)\n\n\nplt.subplot(222)\nagglom = AgglomerativeClustering(n_clusters=2).fit(X)\nX['Labels'] = agglom.labels_\nsns.scatterplot(X['health'], X['income'], hue=X['Labels'], \n                palette=sns.color_palette('tab10', 2))\nplt.xlabel('health')\nplt.ylabel('income')\nplt.title('Agglomerative Clustering with 2 Clusters', fontsize=15)\nplt.legend(loc=6, bbox_to_anchor=(1, 0.5), ncol=1)\n\n\nplt.subplot(223)\ndb = DBSCAN(eps=10, min_samples=5).fit(X)\nX['Labels'] = db.labels_\nsns.scatterplot(X['health'], X['income'], hue=X['Labels'], \n                palette='tab10')\nplt.xlabel('health')\nplt.ylabel('income')\nplt.title('DBSCAN with epsilon=10 and min_samples=5', fontsize=15)\nplt.legend(loc=6, bbox_to_anchor=(1, 0.5), ncol=1)\n\n\nplt.subplot(224)\nbandwidth = estimate_bandwidth(X, quantile=0.1)\nms = MeanShift(bandwidth=bandwidth).fit(X)\nX['Labels'] = ms.labels_\nsns.scatterplot(X['health'], X['income'], hue=X['Labels'], \n                palette=sns.color_palette('tab10', np.unique(ms.labels_).shape[0]))\nplt.xlabel('health')\nplt.ylabel('income')\nplt.title('Mean Shift', fontsize=15)\nplt.legend(loc=6, bbox_to_anchor=(1, 0.5), ncol=1)\n","521e7053":"gglom = AgglomerativeClustering(n_clusters=2).fit(X)\ndf['Clusterts'] = agglom.labels_","d1cf4d2a":"Results = df[['Clusterts','country']].groupby(['Clusterts','country']).mean()\nResults","14b8f45a":"## 3.DBSCAN\n","0effead8":"No matter what value of epsilon or min_samples I took, the algorithm performed in similar way - all data was classified as one group. The reason why DBSCAN doesn't perform very well is a fact that density in our data isn't so strong. Probably, if the dataset will be bigger DBSCAN will donne better job.","ff94a1d8":"Based on those two graphs above I can concluded that KMeans with 2 clusters gave better insights about clustering income and health","037488b0":"## 1.K-Means","cfa224ce":"## 4.Mean Shift","d6456f28":"# `Description of the data set`\n\nHere we use data from [kaggle](https:\/\/www.kaggle.com\/rohan0301\/unsupervised-learning-on-country-data?select=Country-data.csv) \n\n\n1. child_mort: Death of children under 5 years of age per 1000 live births\n\n2. exports: Exports of goods and services per capita. Given as %age of the GDP per capita \n\n3. health: Total health spending per capita. Given as %age of GDP per capita\n\n4. imports: Imports of goods and services per capita. Given as %age of the GDP per capita \n\n5. Income: Net income per person \n\n6. Inflation: The measurement of the annual growth rate of the Total GDP\n\n7. life_expec: The average number of years a new born child would live if the current mortality patterns are to remain the same\n\n8. total_fer The number of children that would be born to each woman if the current age-fertility rates remain the same.\n\n9. gdpp: The GDP per capita. Calculated as the Total GDP divided by the total population.","7c00bb3d":"## Transforming Skewed Features\n","7feb6390":"The data columns are all numeric except for the 'country' label","aa706bd5":"## 2.Hierarchical Clustering\n","04a27c32":"# `Clustering`\n\ncluster analysis of health and income","9a5dea83":"#### Objective:\n\nTo categorise the countries using socio-economic and health factors that determine the overall development of the country.\n\n#### About organization:\n\nHELP International is an international humanitarian NGO that is committed to fighting poverty and providing the people of backward countries with basic amenities and relief during the time of disasters and natural calamities.\n\n#### Problem Statement:\n\nHELPING International have been able to raise around $ 10 million. Now the CEO of the NGO needs to decide how to use this money strategically and effectively. So, CEO has to make decision to choose the countries that are in the direst need of aid. Hence, our Job is to categorise the countries using some socio-economic and health factors that determine the overall development of the country. Then to suggest the countries which the CEO needs to focus on the most.\n\n\n","db2e2eed":"## * Label 1: low income and low\/mid health --> target\n## * Label 0: high income and mid health --> avoid","c361d38d":"Both clustering algorithms: KMeans and Hierarchical Clustering did good job in labeling our countries into 2 different groups and both results are similar.","a7496c55":"we will drop the country column as it's not useful for us","93ee6718":"K-means and Agglomerative Clustering with 2 Clusters performed best according to the silhouette_score\n\n####  possible flaws:\nthis dataset from Kaggle was really small one. Because of it DBSCAN couldn't perform well.\n\n#### Next steps:\nAs a further suggestion, a DBSCAN could be implemented, following a Principal Component Analysis.","8cef6ac0":"## Clustering the Countries by using Unsupervised Learning for \"HELP International\" oraganization\n","9b31ac97":"# `Plan for data exploration`:\n\n1. Exploring and feature engineering \n    * Examine the data types and value_counts\n    * removing unimportant data if found \n    * dealing with missing (NaN) values if found\n    * Transforming Skewed Continuous Features\n    * see the data distribution\n    * feature scalling for continuous variables if needed\n    * encoding for categorical variables if found as to Encode the activity label as an integer\n2. Clustering\n    * K_Means\n    * AgglomerativeClustering\n    * DBSCAN\n    * MeanShift\n3. Selecting the best model\n4. Conclusion and Next Steps\n","f139d66e":"# `Exploring and feature engineering`","b228514d":"# `Conclusion`\n","658e5721":"we will select AgglomerativeClustering","cfa7a2c4":"# `All in one place and selecting the best model`\n","e87c4884":"## looking at correlation"}}