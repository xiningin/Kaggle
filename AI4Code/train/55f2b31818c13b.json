{"cell_type":{"019374fb":"code","d3512476":"code","005c5b65":"code","51d962af":"code","1ccc78e1":"code","981ec3e6":"code","1f3f8349":"code","368aaeb8":"code","a6179cea":"code","304d515d":"code","a91b432e":"code","4d4d4d5e":"code","c15ee5c4":"code","ed34ef71":"markdown","6b85984c":"markdown","1d95429b":"markdown","924272a5":"markdown","1d4a3f71":"markdown","8e898ba4":"markdown","7e9be8b5":"markdown","1547c8ca":"markdown","e67048c2":"markdown","d18358b0":"markdown"},"source":{"019374fb":"!pip install segmentation-models-pytorch","d3512476":"# -*- coding: utf-8 -*-\nimport torch.nn as nn\nfrom  torch.utils.data import Dataset,DataLoader\nimport torch\nimport torchvision\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import StratifiedKFold\nimport pandas as pd\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager as fm\nfrom matplotlib.collections import QuadMesh\nimport seaborn as sns\nfrom tqdm import tqdm,trange\nfrom time import sleep\nimport warnings\nfrom glob import glob\nimport os,sys\nimport cv2\nimport segmentation_models_pytorch as smp\nsys.path.append('..\/input\/coolshan-coding-extension-file')\nfrom confusion_matrix_pretty_print import pretty_plot_confusion_matrix\nwarnings.filterwarnings(\"ignore\")\nBASEPATH = '..\/input\/image-depth-estimation\/data'\n","005c5b65":"def read_csv(path):\n    df=pd.read_csv(path,names=['image','label'],header=None)\n    return df\ndef read_image(path):\n    image = cv2.imread(path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    return image\n\ndf = read_csv(os.path.join(BASEPATH,'nyu2_train.csv'))\ndf['image'] = df['image'].apply(lambda x: '..\/input\/image-depth-estimation\/'+x)\ndf['label'] = df['label'].apply(lambda x: '..\/input\/image-depth-estimation\/'+x)\ndf.info()","51d962af":"fig,axis = plt.subplots(1,2,figsize=(12,6))\naxis[0].imshow(read_image(df.iloc[0].image))\naxis[1].imshow(read_image(df.iloc[0].label))\nplt.show()","1ccc78e1":"class CFG():\n    def __init__(self):\n        self.lr = 3e-4\n        self.epoch=15\n        self.TTA_times=0\n        self.batch_size = 32\n        self.image_size = (192,256)\n        self.seed=31\n        self.fold = 5\n        self.run_fold=1\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        print(self.device)\n\n\ndef setSeed(seed=31,tor=True,tensorf=False):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)#only one time\n    pd.core.common.random_state(seed)\n    # tensorflow seed setting\n    if tensorf:\n        tf.random.set_seed(seed)\n        session_conf = tf.compat.v1.ConfigProto(\n            intra_op_parallelism_threads=1,\n            inter_op_parallelism_threads=1\n        )\n        sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n        tf.compat.v1.keras.backend.set_session(sess)\n        \n    # pytorch seed setting\n    if tor:\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed(seed)\n        torch.backends.cudnn.deterministic = True\n        \n        \n        \ncfg = CFG()\nsetSeed(cfg.seed)","981ec3e6":"class Logger():\n    logger_dict={}\n    logger_raw_dict={}\n    log_category=None\n    \n    def __init__(self,name,layer=0):\n        self.history=pd.DataFrame()\n        self.epoch_history=pd.DataFrame()\n        self.current_epoch_history=pd.DataFrame()\n        self.name=name\n        self.epoch=0\n        self.layer=str(layer)\n        if self.layer not in Logger.logger_dict.keys():\n            Logger.logger_dict[self.layer] = {}\n            Logger.logger_raw_dict[self.layer] = {}\n            \n    def __call__(self,**kwargs):\n        new_row = pd.DataFrame(kwargs,index=[0])\n        self.current_epoch_history=self.current_epoch_history.append(new_row)\n    \n    def __str__(self,r=3):\n        output_str=''\n        if len(self.current_epoch_history)>0:\n            avg_epoch = self.get_current_epoch_avg()\n            for log_c in avg_epoch.index:\n                output_str+= 'avg{}:{} \\t'.format(log_c,f'%.{r}f'%avg_epoch[log_c])\n            return output_str\n        else:\n            avg_epoch = self.epoch_history.iloc[-1]\n            for log_c in avg_epoch.index:\n                output_str+= 'avg{}:{} \\t'.format(log_c,f'%.{r}f'%avg_epoch[log_c])\n            return output_str\n        \n    def get_current_epoch_avg(self):\n        return self.current_epoch_history.mean()\n    \n    def get_best_record(self,category='loss',mode='min',unit='epoch'):\n        _history = self.epoch_history if unit == 'epoch' else self.history\n        if mode == 'min':\n            best_index = _history[category].idxmin()\n        else:\n            best_index = _history[category].idxmax()\n        return best_index, _history.iloc[best_index]\n\n    def check_best(self,category='loss',mode='min',unit='epoch'):\n        _history = self.epoch_history if unit == 'epoch' else self.history\n        best_index,best_record = self.get_best_record(category,mode,unit)\n        return (len(_history[category])-1)==best_index\n\n    def save_epoch(self):\n        if Logger.log_category==None:\n            Logger.log_category = self.current_epoch_history.columns.to_list()\n        self.epoch_history=self.epoch_history.append(self.get_current_epoch_avg().copy(),ignore_index=True).reset_index(drop=True)\n        self.current_epoch_history['epoch'] = self.epoch\n        self.epoch += 1\n        self.history = self.history.append(self.current_epoch_history.copy()).reset_index(drop=True)\n        self.current_epoch_history=pd.DataFrame(columns=Logger.log_category)\n        Logger.logger_dict[self.layer][self.name]=self.history\n        \n        \n    @staticmethod\n    def plot(unit='epoch',show_category=None,figsize=(6.2,3)):\n        \n        show_category=Logger.log_category if show_category == None else show_category\n        layer_list = sorted(list(Logger.logger_dict.keys()))\n        figers, axs = plt.subplots(len(layer_list),len(show_category),figsize=(len(show_category)*figsize[0],len(layer_list)*figsize[1]))\n        plt.subplots_adjust(hspace=0.5)\n\n        if len(layer_list)>1 and len(show_category)>1:\n            for lidx,l in enumerate(layer_list):\n                keys_list=list(Logger.logger_dict[l].keys())\n                for kidx,k in enumerate(keys_list):\n                    for lcidx,log_c in enumerate(show_category):\n                        _history=Logger.logger_dict[l][k]\n                        _history = _history.groupby('epoch').mean() if unit == 'epoch' else _history\n                        sns.lineplot(data=_history,x=range(len(_history)),y=log_c,ax=axs[lidx,lcidx],label=k).set(title = '{}'.format(log_c))\n                        axs[lidx,lcidx].legend(loc='upper left')\n\n        elif len(layer_list)>1:\n            for lidx,l in enumerate(layer_list):\n                keys_list=list(Logger.logger_dict[l].keys())\n                for kidx,k in enumerate(keys_list):\n                    for lcidx,log_c in enumerate(show_category):\n                        _history=Logger.logger_dict[l][k]\n                        _history = _history.groupby('epoch').mean() if unit == 'epoch' else _history\n                        sns.lineplot(data=_history,x=range(len(_history)),y=log_c,ax=axs[lidx],label=k).set(title = '{}'.format(log_c))\n                        axs[lidx].legend(loc='upper left')\n        elif len(show_category)>1:\n            for lidx,l in enumerate(layer_list):\n                keys_list=list(Logger.logger_dict[l].keys())\n                for kidx,k in enumerate(keys_list):\n                    for lcidx,log_c in enumerate(show_category):\n                        _history=Logger.logger_dict[l][k]\n                        _history = _history.groupby('epoch').mean() if unit == 'epoch' else _history\n                        sns.lineplot(data=_history,x=range(len(_history)),y=log_c,ax=axs[lcidx],label=k).set(title = '{}'.format(log_c))\n                        axs[lcidx].legend(loc='upper left')\n        else:\n            keys_list=list(Logger.logger_dict[layer_list[0]].keys())\n            for kidx,k in enumerate(keys_list):\n                for lidx,log_c in enumerate(show_category):\n                    _history=Logger.logger_dict[layer_list[0]][k]\n                    _history = _history.groupby('epoch').mean() if unit == 'epoch' else _history\n                    sns.lineplot(data=_history,x=range(len(_history)),y=log_c,ax=axs,label=k).set(title = '{}'.format(log_c))\n                    axs.legend(loc='upper left')\n        plt.show()\n#\u6559\u5b78\n# \u76f8\u540clayer\u7e6a\u756b\u518d\u4e00\u8d77\n# \u7121\u9808\u8a2d\u5b9a\u5b58\u5165\u8b8a\u6578\u7a2e\u985e\uff0c\u76f4\u63a5\u4e1f\u5373\u53ef\n# \u53ef\u4ee5\u5728plot\u6642\u8a2d\u5b9ashow_category\u4f86\u9078\u64c7\u986f\u793a\u53c3\u6578\n# \u5982\u679c\u60f3\u770biter\u7248\u672c\uff0cplot\u6642unit\u7d66iter(\u975eepoch)\n# save_epoch\u4e00\u5b9a\u8981call\u624d\u53ef\u4ee5\u756b\u5716\uff0c\u5373\u4f7f\u662fiter\u7248\u672c\n# \u8981\u5148save_epoch \u624d\u80fd check_best \u4ee5\u53ca get_best_record\n\n#example\n#training_logger=Logger('Training')#default 0\n#validation_logger=Logger('Validation')#default 0\n#for i in range(10):\n#    for i in range(10):\n#        training_logger(acc=np.random.rand(),loss=random.random(),f1score=random.random())\n#        validation_logger(acc=random.random(),loss=random.random(),f1score=random.random())\n#    validation_logger.save_epoch()\n#    training_logger.save_epoch()\n#    validation_logger.check_best(category='acc',mode='max',unit='epoch')\n#Logger.plot()\n#Logger.plot('iter',show_category= ['loss'])\n\n#display(validation_logger.history,validation_logger.epoch_history,validation_logger.history.groupby(by='epoch').mean())\n#display(validation_logger.get_best_record(category='acc',mode='max',unit='epoch'))\n","1f3f8349":"def split_train_valid(_df):\n    num_sample = len(_df)\n    num_split = int(num_sample*cfg.training_split)\n    _df = df.sample(frac=1,random_state = cfg.seed).reset_index(drop=True)\n    train_df = _df.iloc[:num_split]\n    valid_df = _df.iloc[num_split:]\n    return train_df, valid_df\n\nsample_df = df.sample(n=24800,random_state=1331)\ndf=sample_df.reset_index(drop=True)\nskf = StratifiedKFold(n_splits=cfg.fold, random_state=cfg.seed, shuffle=True)\nsplitlist = list(skf.split(df.index,[0]*len(df.index)))","368aaeb8":"class NYU2_Dataset(Dataset):\n    '''\n    \u4e3b\u8981\u662f\u628adataframe\u5168\u8f49\u6210numpy\u4f86\u8655\u7406\n    '''\n    def __init__(self,df, transform=None,test=False):\n        self.df =  df\n        self.transform = transform\n        self.test = test\n        self.data_id = self.df.index\n        self.labels = self.df.label.values\n        self.data = self.df.image.values\n        self.label_transform =  A.Compose([\n                                 A.Resize(cfg.image_size[0],cfg.image_size[1],always_apply=True),\n                                ])\n    def get_image_pair(self,idx):\n        source_image = cv2.imread(self.data[idx])\n        source_image = cv2.cvtColor(source_image, cv2.COLOR_BGR2RGB)\n        label_image = cv2.imread(self.labels[idx])\n        label_image = cv2.cvtColor(label_image, cv2.COLOR_BGR2GRAY)\n        return source_image,label_image\n        \n    def __getitem__(self,idx):\n        data_id = self.data_id[idx]\n        source_image,label_image = self.get_image_pair(idx)\n        \n        if self.transform:\n            source_image = self.transform(image=source_image)['image']\n            label_image = self.label_transform(image=label_image)['image']\n            label_image = label_image\/255\n            label_image = torch.from_numpy(np.transpose(label_image[...,None],(2,0,1))).float()\n\n        return data_id,source_image,label_image\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def _working_test(self):\n        _test_dataloader = DataLoader(self,\n                                      batch_size=1,\n                                      shuffle=False,\n                                      num_workers=1,\n                                      drop_last=True,\n                                      pin_memory=True)\n        return next(iter(_test_dataloader))\n#_test_dataset = NYU2_Dataset(df)\n#data_id,data,label=_test_dataset._working_test()\n#fig,axis = plt.subplots(1,2,figsize=(12,6))\n#axis[0].imshow(data[0,...])\n#axis[1].imshow(label[0,...])\n#plt.show()","a6179cea":"def get_transform():\n    transform = A.Compose([\n                                 A.Normalize(mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5]),\n                                 A.Resize(cfg.image_size[0],cfg.image_size[1],always_apply=True),\n                                 ToTensorV2()\n                                ])\n    return transform\n\n\ndef get_dataloader(train_df,valid_df):\n    \n    print('training data are processing..')\n    train_dataset = NYU2_Dataset(train_df,transform=transform)\n    train_dataloader = DataLoader(train_dataset,\n                                  batch_size=cfg.batch_size,\n                                  shuffle=True,\n                                  num_workers=4,\n                                  drop_last=True,\n                                  pin_memory=True)\n        \n    print('validation data are processing..')\n    valid_dataset = NYU2_Dataset(valid_df,transform=transform)\n    valid_dataloader = DataLoader(valid_dataset,\n                                  batch_size=cfg.batch_size,\n                                  shuffle=False,\n                                  num_workers=4,\n                                  drop_last=False,\n                                  pin_memory=True)\n    \n    return train_dataloader,valid_dataloader\ntransform = get_transform()","304d515d":"class ModelInstance():\n    def __init__(self,model,optimizer,loss_function,scheduler=None,clip_grad=None):\n        self.model = model\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.certification = loss_function\n        self.clip_grad = clip_grad\n        \n    def run_model(self,feature,label,update=True):\n        feature = feature.to(cfg.device)\n        label = label.to(cfg.device)\n        \n        if update:\n            pred = self.model(feature)\n        else:\n            with torch.no_grad():\n                pred = self.model(feature)\n            \n            \n        loss = self.certification(pred,label)\n        \n        if update:\n            loss.backward()\n            if self.clip_grad:\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=clip_grad)\n            self.optimizer.step()\n            self.optimizer.zero_grad()\n            \n        return  pred.cpu().detach(), loss.cpu().detach().item()\n    \n    def predict(self,feature):\n        feature = feature.to(cfg.device)\n        pred = self.model(feature)\n        return pred.cpu().detach()\n    \ndef create_model_instance():\n    \n    def certification(predict,label):\n        loss = nn.MSELoss()(predict,label)\n        return loss\n    \n    model = smp.UnetPlusPlus(\n                    encoder_name=\"resnet50\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n                    encoder_weights=None,#'imagenet',     # use `imagenet` pre-trained weights for encoder initialization\n                    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n                    classes=1,                      # model output channels (number of classes in your dataset)\n                    activation='sigmoid',\n    ).to(cfg.device)\n    \n    model.load_state_dict(torch.load('..\/input\/autofocus-weighted-tmp\/best_model.pkl'))\n    \n    optimizer = torch.optim.Adam(model.parameters(),lr=cfg.lr,amsgrad=True)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode='min', factor=0.4, patience=2, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=1e-7, eps=1e-08, verbose=True)\n    return ModelInstance(model,optimizer,certification,scheduler,None)\n    \n    ","a91b432e":"from sklearn.metrics import f1_score, precision_recall_fscore_support, accuracy_score\n\ndef onehot_encoding(pred):\n    return pred.argmax(dim=1)\n\ndef unnormalize(images):\n        return (0.5*np.array(images)+0.5)*255\ndef run_epoch(dataloader,epoch,logger,validmode):\n    \n    if validmode:\n        instance.model.eval()\n    else:\n        instance.model.train()\n        \n    trange = tqdm(enumerate(dataloader),total=len(dataloader), desc='Valid' if validmode else 'Train')\n    data_id,pred=None,None\n    for idx, (data_id,feature,label) in trange:\n        pred,loss = instance.run_model(feature,label,not validmode)\n        logger(loss=loss)\n        avg_log = logger.get_current_epoch_avg()\n        trange.set_postfix(loss = avg_log['loss'])\n    logger.save_epoch()\n    data_id = np.array(data_id)\n    pred[0] = pred[0]*255\n    fig,axis = plt.subplots(1,3,figsize=(12,4))\n    axis[0].imshow(read_image(df.iloc[data_id[0]].image))\n    axis[1].imshow(read_image(df.iloc[data_id[0]].label))\n    axis[2].imshow(np.transpose(pred[0],(1,2,0)),cmap='gray')\n    plt.show()\n    \n\ndef inference(dataloader,test=False):\n    \n    instance.model.eval()\n    \n    all_pred=[]\n    all_label=[]\n    all_id=[]\n    \n    trange = tqdm(enumerate(dataloader),total=len(dataloader), desc='')\n    if test:\n        for idx, (data_id,feature) in trange:\n            pred = instance.predict(feature)\n            all_pred.append(pred)\n            all_id.append(data_id)\n\n        all_pred = np.concatenate(all_pred,axis=0)\n        all_id = np.concatenate(all_id,axis=0)\n        return all_id,all_pred\n    \n    else:\n        for idx, (data_id,feature,label) in trange:\n            pred,loss = instance.run_model(feature,label,update=False)\n            all_pred.append(pred)\n            all_label.append(label)\n            all_id.append(data_id)\n\n        all_pred = np.concatenate(all_pred,axis=0)\n        all_label = np.concatenate(all_label,axis=0)\n        all_id = np.concatenate(all_id,axis=0)\n        return data_id,all_pred,all_label\n    \n    ","4d4d4d5e":"for fold,(train_index,valid_index) in enumerate(splitlist):\n    instance = create_model_instance()\n    training_logger=Logger('Training',fold)\n    validation_logger=Logger('Validation',fold)\n    \n    print('------------ Flod {} ------------'.format(fold))\n    train_df = df.iloc[train_index].copy()\n    valid_df = df.iloc[valid_index].copy()\n    train_dataloader,valid_dataloader =get_dataloader(train_df,valid_df)\n\n    for epoch in range(cfg.epoch):\n        print('Epoch {}\/{}:'.format(epoch,cfg.epoch))\n        run_epoch(train_dataloader,epoch,training_logger,False)\n        run_epoch(valid_dataloader,epoch,validation_logger,True)\n        if instance.scheduler:\n            instance.scheduler.step(validation_logger.history.iloc[-1]['loss'])\n        if validation_logger.check_best(category='loss',mode='min'):\n            print('--save mode--')\n            torch.save(instance.model.state_dict(),'best_model.pkl')\n    display(validation_logger.get_best_record(category='loss',mode='min'))\n    if cfg.run_fold == fold+1:\n        break","c15ee5c4":"Logger.plot(show_category=['loss'])","ed34ef71":"## Dataset","6b85984c":"## Main","1d95429b":"## Model Instance","924272a5":"## Split Training and Validation Data","1d4a3f71":"## Configer and Set Random Seed","8e898ba4":"## Model Structure","7e9be8b5":"## Run Unit","1547c8ca":"## DataLoader","e67048c2":"## Logger","d18358b0":"## Load Data"}}