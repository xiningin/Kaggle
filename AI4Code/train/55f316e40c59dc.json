{"cell_type":{"80f6ac29":"code","1108f5b7":"code","311a6946":"code","0598e905":"code","7ed4895c":"code","43be78fe":"code","56562633":"code","17da77ee":"code","b3758419":"code","beeb7f40":"code","58e03dcb":"code","a85763ca":"code","e0d568a1":"code","6dbaf680":"code","83b4dc94":"code","84146efa":"code","d5a26b95":"code","9c11d85c":"code","0d2adc65":"code","2c8af6f9":"code","c224a317":"code","e056bba9":"code","4a3c19ce":"code","67b8c360":"code","2c9f2983":"code","8eb36177":"code","50012d76":"code","39efcb17":"code","3b4b48b9":"code","16483bfa":"code","ad905073":"code","e2d8f62b":"code","bdaa1925":"code","e9c8be97":"code","5b70c6fc":"code","7ff29b4b":"code","b24c0827":"code","0d411982":"code","f41c06d9":"code","4a6f47ed":"code","c7e3a12c":"code","0bb87370":"code","6d38064d":"code","4310c966":"code","1a5e77ab":"code","7de54127":"code","ae6038e6":"code","821c3bc6":"code","edfa6fdb":"code","91da565e":"code","c47b2262":"code","2cda046e":"code","94824c99":"code","2880a4cb":"code","cfc6c468":"code","94e21e42":"code","d564951c":"code","2ed17025":"code","6198fcfc":"code","2aa8b9f9":"code","9164b858":"code","1275e686":"code","b293f8d5":"code","2b1262b8":"code","4c57be7c":"code","9fd9230a":"code","e44dd5c0":"code","15068272":"code","6c0d030f":"code","3250def7":"code","592bce58":"code","25941895":"code","b9175c68":"code","82b974cd":"code","45c1963f":"code","1fbd3f97":"code","11d582b3":"code","13672631":"code","015fca62":"code","0a9c45da":"code","4f90fc45":"code","1b83b657":"markdown","d93a33a2":"markdown","c944fdae":"markdown","a6819cec":"markdown","1bcd785a":"markdown","79149d87":"markdown","7667c789":"markdown","c7515d90":"markdown","d2666ae2":"markdown","69c764e9":"markdown","0509e087":"markdown","df51b34b":"markdown","1ab3b8ac":"markdown","9bf7bd52":"markdown","f350667c":"markdown","0fe84d2d":"markdown","dd1d96b9":"markdown","5764f2bb":"markdown","6c347683":"markdown","7cef6376":"markdown","bbb38702":"markdown","e71b0c88":"markdown","7892d08d":"markdown","2b1a6049":"markdown","05be1d31":"markdown","da6c9ae1":"markdown","b960580f":"markdown","5d34a932":"markdown","c4b30ef0":"markdown","08bf0060":"markdown","04cfd6b9":"markdown","e1ee851e":"markdown"},"source":{"80f6ac29":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np \nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","1108f5b7":"X_test = pd.read_csv(\"..\/input\/X_test.csv\")\nX_train = pd.read_csv(\"..\/input\/X_train.csv\")\ny_train = pd.read_csv(\"..\/input\/y_train.csv\")","311a6946":"X_train.head()","0598e905":"X_test.head()","7ed4895c":"print(X_train.shape)\nprint(X_test.shape)","43be78fe":"y_train.head(10)","56562633":"y_train.shape","17da77ee":"y_train.group_id.nunique()","b3758419":"y_train.groupby(['group_id', 'surface']).nunique().head(10)","beeb7f40":"df_temp = y_train.groupby(['surface']).count().reset_index()\nplt.barh(df_temp.surface, df_temp.series_id)\nplt.title(\"Number of 'series_id' in train data\")\nplt.show()","58e03dcb":"surface_list = y_train.surface.unique()","a85763ca":"X_70 = X_train[X_train.series_id==70]\n\nplt.figure(figsize=(30, 15))\nfor i, col in enumerate(X_70.columns[3:]):\n    plt.subplot(3, 4, i + 1)\n    plt.plot(X_70[col])\n    plt.title(col)","e0d568a1":"X_12 = X_test[X_test.series_id==12]\n\nplt.figure(figsize=(30, 15))\nfor i, col in enumerate(X_12.columns[3:]):\n    plt.subplot(3, 4, i + 1)\n    plt.plot(X_12[col])\n    plt.title(col)","6dbaf680":"X_train = pd.merge(y_train[['series_id', 'group_id', 'surface']], X_train, on='series_id')\nX_train.head()","83b4dc94":"sns.set_context(\"notebook\", font_scale=1.7)\nplt.figure(figsize=(30, 45))\nfor i, col in enumerate(X_train.columns[5:]):\n    plt.subplot(5, 2, i+1)\n    for surface in surface_list:\n        sns.distplot(X_train[X_train.surface==surface][col], label=surface)\n    plt.legend(fontsize='x-small')","84146efa":"train_X = pd.read_csv('..\/input\/X_train.csv').iloc[:,3:].values.reshape(-1,128,10)\ntest_X = pd.read_csv('..\/input\/X_test.csv').iloc[:,3:].values.reshape(-1,128,10)\nprint('train_X shape:', train_X.shape, ', test_X shape:', test_X.shape)","d5a26b95":"train_group = y_train['group_id'].values","9c11d85c":"sns.set_context(\"notebook\", font_scale=1.)\nfig, axes = plt.subplots(1,4)\nfig.set_size_inches(20,3)\n\nfor i in range(4):\n    axes[i].plot(train_X[train_group == 17][:,:,i].reshape(-1))\n    axes[i].grid(True)","0d2adc65":"def sq_dist(a,b):\n    ''' the squared euclidean distance between two samples '''\n    \n    return np.sum((a-b)**2, axis=1)\n\n\ndef find_run_edges(data, edge):\n    ''' examine links between samples. left\/right run edges are those samples which do not have a link on that side. '''\n\n    if edge == 'left':\n        border1 = 0\n        border2 = -1\n    elif edge == 'right':\n        border1 = -1\n        border2 = 0\n    else:\n        return False\n    \n    edge_list = []\n    linked_list = []\n        \n    for i in range(len(data)):\n        dist_list = sq_dist(data[i, border1, :4], data[:, border2, :4]) # distances to rest of samples\n        min_dist = np.min(dist_list)\n        closest_i   = np.argmin(dist_list) # this is i's closest neighbor\n        if closest_i == i: # this might happen and it's definitely wrong\n            print('Sample', i, 'linked with itself. Next closest sample used instead.')\n            closest_i = np.argsort(dist_list)[1]\n        dist_list = sq_dist(data[closest_i, border2, :4], data[:, border1, :4]) # now find closest_i's closest neighbor\n        rev_dist = np.min(dist_list)\n        closest_rev = np.argmin(dist_list) # here it is\n        if closest_rev == closest_i: # again a check\n            print('Sample', i, '(back-)linked with itself. Next closest sample used instead.')\n            closest_rev = np.argsort(dist_list)[1]\n        if (i != closest_rev): # we found an edge\n            edge_list.append(i)\n        else:\n            linked_list.append([i, closest_i, min_dist])\n            \n    return edge_list, linked_list\n\ndef find_runs(data, left_edges, right_edges):\n    ''' go through the list of samples & link the closest neighbors into a single run '''\n    \n    data_runs = []\n\n    for start_point in left_edges:\n        i = start_point\n        run_list = [i]\n        while i not in right_edges:\n            tmp = np.argmin(sq_dist(data[i, -1, :4], data[:, 0, :4]))\n            if tmp == i: # self-linked sample\n                tmp = np.argsort(sq_dist(data[i, -1, :4], data[:, 0, :4]))[1]\n            i = tmp\n            run_list.append(i)\n        data_runs.append(np.array(run_list))\n    \n    return data_runs","2c8af6f9":"train_left_edges, train_left_linked  = find_run_edges(train_X, edge='left')\ntrain_right_edges, train_right_linked = find_run_edges(train_X, edge='right')\nprint('Found', len(train_left_edges), 'left edges and', len(train_right_edges), 'right edges.')","c224a317":"train_runs = find_runs(train_X, train_left_edges, train_right_edges)","e056bba9":"y_train['run_id'] = 0\ny_train['run_pos'] = 0\n\nfor run_id in range(len(train_runs)):\n    for run_pos in range(len(train_runs[run_id])):\n        series_id = train_runs[run_id][run_pos]\n        y_train.at[ series_id, 'run_id'  ] = run_id\n        y_train.at[ series_id, 'run_pos' ] = run_pos\n\ny_train.to_csv('y_train_with_runs.csv', index=False)\ny_train.tail()","4a3c19ce":"flat_list = [series_id for run in train_runs for series_id in run]\nprint(len(flat_list), len(np.unique(flat_list)))","67b8c360":"surface_cnt_train = []\n\nfor i in range(len(train_runs)):\n    surface_cnt_train.append(len(np.unique(y_train['surface'].iloc[train_runs[i]])))","2c9f2983":"print('Id of runs with different surfaces:')\nfor i in range(len(surface_cnt_train)):\n    if surface_cnt_train[i] > 1:\n        print(str(i) + ', number of surfaces: ' + str(surface_cnt_train[i]))","8eb36177":"fig, axes = plt.subplots(10,1, sharex=True)\nfig.set_size_inches(20,15)\nfig.subplots_adjust(hspace=0)\n\nfor i in range(10):\n    axes[i].plot(train_X[train_runs[59]][:,:,i].reshape(-1))\n    axes[i].grid(True)","50012d76":"X_train.head()","39efcb17":"X_train = pd.merge(y_train[['series_id', 'run_id']], X_train, on='series_id')\nX_train.head()","3b4b48b9":"test_X  = pd.read_csv('..\/input\/X_test.csv' ).iloc[:,3:].values.reshape(-1,128,10)","16483bfa":"test_left_edges, test_left_linked  = find_run_edges(test_X, edge='left')\ntest_right_edges, test_right_linked = find_run_edges(test_X, edge='right')\nprint('Found', len(test_left_edges), 'left edges and', len(test_right_edges), 'right edges.')","ad905073":"test_runs = find_runs(test_X, test_left_edges, test_right_edges)","e2d8f62b":"flat_list = [series_id for run in test_runs for series_id in run]\nprint(len(flat_list), len(np.unique(flat_list)))","bdaa1925":"lost_samples = np.array([ i for i in range(len(test_X)) if i not in np.concatenate(test_runs) ])\nprint(lost_samples)\nprint(len(lost_samples))","e9c8be97":"find_run_edges(test_X[lost_samples], edge='left')[1][0]","5b70c6fc":"lost_run = np.array(lost_samples[find_runs(test_X[lost_samples], [0], [5])[0]])\ntest_runs.append(lost_run)","7ff29b4b":"len(test_runs)","b24c0827":"for j in [0, 4, 5, 9, 16, 45, 47, 48, 51, 75]:   #for j in range(len(test_runs)):\n    fig, axes = plt.subplots(10,1, sharex=True)\n    fig.set_size_inches(20,10)\n    fig.subplots_adjust(hspace=0)\n    fig.suptitle('test_run: ' + str(j))\n    \n    for i in range(10):\n        axes[i].plot(test_X[test_runs[j]][:,:,i].reshape(-1))\n        axes[i].grid(True)\n\n## test_runs(j), which look like they have 2 or more surfaces: 0, 4(?), 5, 9(?), 16, 45, 47, 48, 51, 75","0d411982":"X_test = pd.read_csv(\"..\/input\/X_test.csv\")","f41c06d9":"y_test = pd.DataFrame(columns = ['series_id', 'run_id', 'run_pos'])\ny_test['series_id'] = X_test.series_id.unique()","4a6f47ed":"y_test['run_id'] = 0\ny_test['run_pos'] = 0\n\nfor run_id in range(len(test_runs)):\n    for run_pos in range(len(test_runs[run_id])):\n        series_id = test_runs[run_id][run_pos]\n        y_test.at[ series_id, 'run_id'  ] = run_id\n        y_test.at[ series_id, 'run_pos' ] = run_pos\n\ny_test.head()","c7e3a12c":"X_test = pd.merge(y_test[['series_id', 'run_id']], X_test, on='series_id')\nX_test.head()","0bb87370":"#thanks to https:\/\/www.kaggle.com\/jesucristo\/1-smart-robots-complete-notebook-0-73\nimport math\n    \ndef quaternion_to_euler(x, y, z, w):\n    t0 = +2.0 * (w * x + y * z)\n    t1 = +1.0 - 2.0 * (x * x + y * y)\n    X = math.degrees(math.atan2(t0, t1))\n\n    t2 = +2.0 * (w * y - z * x)\n    t2 = +1.0 if t2 > +1.0 else t2\n    t2 = -1.0 if t2 < -1.0 else t2\n    Y = math.degrees(math.asin(t2))\n\n    t3 = +2.0 * (w * z + x * y)\n    t4 = +1.0 - 2.0 * (y * y + z * z)\n    Z = math.degrees(math.atan2(t3, t4))\n\n    return X, Y, Z","6d38064d":"def fe_step0 (data):\n    \n    # https:\/\/www.mathworks.com\/help\/aeroblks\/quaternionnorm.html\n    # https:\/\/www.mathworks.com\/help\/aeroblks\/quaternionmodulus.html\n    # https:\/\/www.mathworks.com\/help\/aeroblks\/quaternionnormalize.html\n        \n    data['norm_quat'] = data['orientation_X']**2 + data['orientation_Y']**2 + data['orientation_Z']**2 + data['orientation_W']**2 \n    data['mod_quat'] = (data['norm_quat'])**0.5\n    data['norm_X'] = data['orientation_X'] \/ data['mod_quat']\n    data['norm_Y'] = data['orientation_Y'] \/ data['mod_quat']\n    data['norm_Z'] = data['orientation_Z'] \/ data['mod_quat']\n    data['norm_W'] = data['orientation_W'] \/ data['mod_quat']\n    \n    return data","4310c966":"X_train = fe_step0(X_train)\nX_test = fe_step0(X_test)\nprint(X_train.shape)\nX_train.head(5)","1a5e77ab":"def fe_step1 (data):\n    \"\"\"\n    Quaternions to Euler Angles\n    Adding columns 'euler_x', 'euler_y', 'euler_z'\n    \"\"\"\n    \n    x, y, z, w = data['norm_X'].tolist(), data['norm_Y'].tolist(), data['norm_Z'].tolist(), data['norm_W'].tolist()\n    nx, ny, nz = [], [], []\n    for i in range(len(x)):\n        xx, yy, zz = quaternion_to_euler(x[i], y[i], z[i], w[i])\n        nx.append(xx)\n        ny.append(yy)\n        nz.append(zz)\n    \n    data['euler_x'] = nx\n    data['euler_y'] = ny\n    data['euler_z'] = nz\n    \n    return data","7de54127":"X_train = fe_step1(X_train)\nX_test = fe_step1(X_test)\n\nprint (X_train.shape)\nX_train.head(5)","ae6038e6":"feature_list = list(X_train.columns[6:])","821c3bc6":"def feat_eng(data, test_data=False):\n    \"\"\"\n    Feature engineering, step 2: \n    Grouping data, then calculating different statistical features.\n    \n    train data: is grouped by 'run_id' and 'surface'\n    test data: grouped only by 'run_id', since we don't know type of surface here\n    \"\"\"\n    \n    df = pd.DataFrame()\n    \n    data['totl_anglr_vel'] = (data['angular_velocity_X']**2 + data['angular_velocity_Y']**2 + data['angular_velocity_Z']**2)** 0.5\n    data['totl_linr_acc'] = (data['linear_acceleration_X']**2 + data['linear_acceleration_Y']**2 + data['linear_acceleration_Z']**2)**0.5\n    data['acc_vs_vel'] = data['totl_linr_acc'] \/ data['totl_anglr_vel']\n    \n    def mean_change_of_abs_change(x):\n        return np.mean(np.diff(np.abs(np.diff(x))))\n    \n    if test_data:\n        col_for_group_by = ['run_id']\n    else:\n        col_for_group_by = ['run_id', 'surface']\n    \n    for col in data.columns:\n        if col in ['row_id','series_id','measurement_number', 'group_id', 'run_id', 'run_pos', 'surface']:\n            continue\n        df[col + '_mean'] = data.groupby(col_for_group_by)[col].mean()\n        df[col + '_median'] = data.groupby(col_for_group_by)[col].median()\n        df[col + '_max'] = data.groupby(col_for_group_by)[col].max()\n        df[col + '_min'] = data.groupby(col_for_group_by)[col].min()\n        df[col + '_std'] = data.groupby(col_for_group_by)[col].std()\n        df[col + '_range'] = df[col + '_max'] - df[col + '_min']\n        df[col + '_maxtoMin'] = df[col + '_max'] \/ df[col + '_min']\n        df[col + '_mean_abs_chg'] = data.groupby(col_for_group_by)[col].apply(lambda x: np.mean(np.abs(np.diff(x))))\n        df[col + '_mean_change_of_abs_change'] = data.groupby(col_for_group_by)[col].apply(mean_change_of_abs_change)\n        df[col + '_abs_max'] = data.groupby(col_for_group_by)[col].apply(lambda x: np.max(np.abs(x)))\n        df[col + '_abs_min'] = data.groupby(col_for_group_by)[col].apply(lambda x: np.min(np.abs(x)))\n        df[col + '_abs_avg'] = (df[col + '_abs_min'] + df[col + '_abs_max'])\/2\n    return df","edfa6fdb":"X_train = feat_eng(X_train)\nX_test = feat_eng(X_test, test_data=True)\n\nprint (\"New features: \",X_train.shape)","91da565e":"feature_list.extend(['totl_anglr_vel', 'totl_linr_acc', 'acc_vs_vel'])","c47b2262":"X_train = pd.merge(y_train[['series_id', 'run_id', 'surface']], X_train, on=['run_id', 'surface'])\nX_train.drop(['run_id', 'surface'], axis=1, inplace=True)\nX_train.head()","2cda046e":"X_test = pd.merge(y_test[['series_id', 'run_id']], X_test, on='run_id')\nX_test.drop(['run_id'], axis=1, inplace=True)\nX_test.head()","94824c99":"y_gr = y_train[['group_id','surface','series_id']].groupby(['group_id','surface']).count().reset_index()\ny_gr.rename(columns={'series_id':'cnt'}, inplace=True)\ny_gr.head()","2880a4cb":"y_gr_temp = y_gr[y_gr.surface!='hard_tiles']\ny_gr_train, y_gr_val = train_test_split(y_gr_temp, test_size=0.15, random_state=11, stratify=y_gr_temp.surface)\ny_gr_train = y_gr_train.append(y_gr[y_gr.surface=='hard_tiles'])","cfc6c468":"y_gr_val","94e21e42":"y_gr_val[['group_id', 'surface']].groupby('surface').count()","d564951c":"#y_gr_train.head(3)\ny_gr_train[['group_id', 'surface']].groupby('surface').count()","2ed17025":"y_train_data = y_train.merge(y_gr_train.group_id, on='group_id')","6198fcfc":"y_val_data = y_train.merge(y_gr_val.group_id, on='group_id')","2aa8b9f9":"y_train_data[['series_id', 'surface']].groupby('surface').count()","9164b858":"y_val_data[['group_id', 'surface']].groupby('surface').count() ","1275e686":"X = y_train_data.merge(X_train, on='series_id')\nX = X.drop(['group_id', 'run_id', 'run_pos', 'series_id', 'surface'], axis=1)\nX = np.array(X)\n\nX_val = y_val_data.merge(X_train, on='series_id')\nX_val = X_val.drop(['group_id', 'run_id', 'run_pos', 'series_id', 'surface'], axis=1)\nX_val = np.array(X_val)","b293f8d5":"X_test = X_test.sort_values('series_id')\n\nX_t = X_test.drop(['series_id'], axis=1)\nX_t = np.array(X_t)","2b1262b8":"print(X.shape, X_val.shape, X_t.shape) ","4c57be7c":"n_featurs = X.shape[1]\nprint(n_featurs)","9fd9230a":"rf_clf = RandomForestClassifier(n_estimators=100, max_depth=11, verbose=1,\n                                class_weight='balanced',\n                                random_state=11, n_jobs=10)","e44dd5c0":"rf_clf.fit(X, y_train_data['surface'])","15068272":"print('Accuracy on train data: ', rf_clf.score(X, y_train_data['surface']))\nprint('Accuracy on validation data: ', rf_clf.score(X_val, y_val_data['surface']))","6c0d030f":"# thanks to the author of this kernel: https:\/\/www.kaggle.com\/artgor\/where-do-the-robots-drive\nimport itertools\ndef plot_confusion_matrix(truth, pred, classes, normalize=False, title=''):\n    cm = confusion_matrix(truth, pred)\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    \n    plt.figure(figsize=(10, 10))\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title(title, size=15)\n    plt.colorbar(fraction=0.046, pad=0.04)\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.grid(False)\n    plt.tight_layout()\n","3250def7":"pred_train = rf_clf.predict(X)\npred_val = rf_clf.predict(X_val) ","592bce58":"plot_confusion_matrix(y_train_data['surface'], pred_train, classes=rf_clf.classes_, title='Confusion matrix - Train data')","25941895":"plot_confusion_matrix(y_val_data['surface'], pred_val, classes=rf_clf.classes_, title='Confusion matrix - Validation data')","b9175c68":"feat_imp = pd.DataFrame(columns=['importance', 'feature'])\nfeat_imp['importance'] = rf_clf.feature_importances_\nfeat_imp['feature'] =  X_train.columns[1:]\nfeat_imp.sort_values(by='importance', ascending=False).head(10)","82b974cd":"color_list = list('kkkkbbbgggccyyyymmmrrr')\n\nfig, ax = plt.subplots(figsize=(14, 5))\nplt.plot(feat_imp.importance)\nplt.xticks(np.arange(0, 270, 12))\nplt.grid()\nfor i in range(len(feature_list)):\n    t = feature_list[i]\n    plt.text(6+12*i, -0.004, t, ha='center', va='top', rotation=90, color=color_list[i], fontsize=12)\n    \nplt.title(\"Feature importance and groups of features\")\nplt.show()\n","45c1963f":"X = y_train.merge(X_train, on='series_id')\nX = X.drop(['group_id', 'run_id', 'run_pos', 'series_id', 'surface'], axis=1)\nX = np.array(X)","1fbd3f97":"rf_clf = RandomForestClassifier(n_estimators=100, max_depth=11, verbose=1,\n                                class_weight='balanced',\n                                random_state=11, n_jobs=10)","11d582b3":"rf_clf.fit(X, y_train['surface'])","13672631":"subm_result = rf_clf.predict(X_t)","015fca62":"submission = pd.DataFrame(columns = ['series_id', 'surface'])\nsubmission['series_id'] = X_test.series_id.unique()\nsubmission['surface'] = subm_result","0a9c45da":"submission.to_csv('submission.csv', index=False)","4f90fc45":"submission.head()","1b83b657":"**5. Training Model (Random Forest)**","d93a33a2":"> The input data covers 10 sensor channels and 128 measurements per time series.\n> The orientation channels encode the current angles how the robot is oriented as a quaternion. Angular velocity describes the angle and speed of motion, and linear acceleration components describe how the speed is changing at different times. ","c944fdae":"So we have 73 groups in train data, each group has different number of series.","a6819cec":"**6. Submission**","1bcd785a":"**3. Feature engineering**","79149d87":"**4. Train-test-split**","7667c789":"Example 1 : series_id = 70 (Concrete)","c7515d90":"Confusion matrix for training and validation data:","d2666ae2":"Data has hierarchical ID system:\ngroup_id -> series_id -> measurement_number\n\nEach Group consists of several Series, while each Serie conrains 128 Measurements.","69c764e9":"8 of top-10 most important features is Mean Absolute Change. It seems to be the most usefull statistical characteristic here. ","0509e087":"Now, when we've chosen the model, we train it for submission on the whole dataset.","df51b34b":"**2. Looking for chains in data**","1ab3b8ac":"First we translate the quaternions into euler coordinates.","9bf7bd52":"*2.2 Test data*","f350667c":"*2.1 Train data*","0fe84d2d":"The idea is to find out the right order and reconstruct one large contionuous mesuarement. Then we can calculate features that would be more reliable and robust.","dd1d96b9":"An example of data (orientation_x, orientation_y, orientation_z and orientation_w) corresponding to one group_id (group_id=17).\nLooks like one long measurement was split into small parts and then shuffled. ","5764f2bb":"Many thanks to Nanashi and to Markus F, whose kernels ([#1 Smart Robots. Most Complete Notebook ](http:\/\/www.kaggle.com\/jesucristo\/1-smart-robots-most-complete-notebook) and [The Missing Link...](http:\/\/www.kaggle.com\/friedchips\/the-missing-link)) helped a lot.","6c347683":"This kernel had only 0.5788 score in Public Leaderboard (~1090 place).\nNevertheless, I choosed it for the final submission, as I believed it can generalize well.\n\nBut the result was even better then I expected! The kernel endet up at place 20 with score 0.7536.\n\nWell, I think it's a good example why we should not always trust Public Leaderboard :)\n","7cef6376":"**1. Data Exploration**","bbb38702":"Well, some differences are obvious, especially for orientation data. In case of carpet grafs are smoother, which makes sence:) ","e71b0c88":"Histograms and density curves:","7892d08d":"Some things we can see at a glance:\n- distribution of orientation data for hard tiles differs a lot\n- distribution of acceleration data is more or less simmetric with the same \"center\" for all types of surfaces\n- acceleration data for soft tiles has the lowest dispertion, for tiled - probably the highest dispertion","2b1a6049":"Example 2: series_id = 12 (Carpet)","05be1d31":"**Description of the task from the competion page**\n> \n> Robots are smart\u2026 by design. To fully understand and properly navigate a task, however, they need input about their environment.\n> \n> In this competition, you\u2019ll help robots recognize the floor surface they\u2019re standing on using data collected from Inertial Measurement Units (IMU sensors).\n> \n> We\u2019ve collected IMU sensor data while driving a small mobile robot over different floor surfaces on the university premises. The task is to predict which one of the nine floor types (carpet, tiles, concrete) the robot is on using sensor data such as acceleration and velocity. Succeed and you'll help improve the navigation of robots without assistance across many different surfaces, so they won\u2019t fall down on the job.\n\n\n\n","da6c9ae1":"*Examples of data with particular series_id*","b960580f":"Here train-test-split is used for model and hyperparameters selection. For the final training and submission we use the whole train dataset.\n\nTrain-test-split:\n- stratify  by surface type\n- groups are not divided into test and train (all mesurements of one group go either to train or to test set) \n\nThere is only one group_id corresponding to the surface 'Hard tiles'. So it goes to the train set.\n","5d34a932":"Here I tried to find test_runs, that have mesurements for 2 or more surfaces. As we can see, there are some of them (~10 runs).\nDividing these test_runs into separate runs could improve the final score.\nUnfortunately I didn't use this observation because the deadline of compentition was too close. ","c4b30ef0":"As we can see here, the most important groups of features are angular velocity and linear acceleration (coordinate wise as well as their absolute values and their ratio).  \nOrientation data are less usefull.","08bf0060":"As we can see, our model is overfitted. But the score on validation set is still quite good for this competion.\n\nActually, as we know now after the end of competition, the CV score is very close to the private leaderboard score, this means our CV-strategy works well.","04cfd6b9":"It's interesting to take a look at feature importance:","e1ee851e":"Ok, now the group numbers are good stratified."}}