{"cell_type":{"02debcd7":"code","d90bba83":"code","de1fe83f":"code","2d352d68":"code","5f0cc9d2":"code","c222f0fb":"code","ad7bf737":"code","02a96b5e":"code","93a7043b":"code","423d1681":"code","ff0019a6":"code","166c1fa2":"code","d3bc1f96":"code","dc65da71":"code","f3e4940d":"markdown","51f9f829":"markdown","ab235a13":"markdown","76eec01b":"markdown","7a53c3f9":"markdown","df90bc10":"markdown","23ba29eb":"markdown","acdbdcbb":"markdown","b0b9518e":"markdown","1a104a4f":"markdown"},"source":{"02debcd7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d90bba83":"#importing the libraries\nimport pandas as pd\nimport numpy as np","de1fe83f":"news_train_test = pd.read_csv('\/kaggle\/input\/odia-news-dataset\/train.csv')\nnews_validate = pd.read_csv('\/kaggle\/input\/odia-news-dataset\/valid.csv')\nprint(news_train_test.head(5))","2d352d68":"#importing the stopword dictionary. You can create your own dictionary.\nstopwords = pd.read_csv('\/kaggle\/input\/stopwords-odia\/stopwords.csv')\nsw_arr = stopwords.to_numpy()\nprint(sw_arr)","5f0cc9d2":"def removePunctuations(headline):\n    headline = headline.replace(',',' ')\n    headline = headline.replace(':',' ')\n    headline = headline.replace(';',' ')\n    headline = headline.replace('.',' ')\n    headline = headline.replace('\\'','')\n    headline = headline.replace('-',' ')\n    return headline;","c222f0fb":"news_arr = []\nfor headline in news_train_test['headings'] :\n    filtered_news_string = ''\n    headline=removePunctuations(headline)\n    for word in headline.split(' '):\n        if word not in sw_arr:\n            filtered_news_string = filtered_news_string+word+' '\n    news_arr.append(filtered_news_string)","ad7bf737":"#creating a new dataframe to store the filtered news array headlines\ndataset_new = pd.DataFrame(news_arr, columns=['filter_news'])\nprint(dataset_new.head(5))\n\n#Now we will concat the filtered news dataset with our original news corpus so that we can get the labels against filtered headlines\nnews_train_test = pd.concat([news_train_test, dataset_new], axis = 1)\nprint(news_train_test.columns)","02a96b5e":"#importing CountVectorizer to create vectors\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n#we will vectorize each words in a documents.these vectors will be our features to train the model\nvectorizer = CountVectorizer(analyzer = \"word\",max_features = 1700)\nx= vectorizer.fit_transform(news_train_test['filter_news']).toarray()\n\n#now we will store our features in x\nprint(x)","93a7043b":"#our target variable\nprint(news_train_test['label'].unique)","423d1681":"#importing library\nfrom sklearn.preprocessing import LabelEncoder\ncat_encoder = LabelEncoder()\n\n#encoding the label column from our news corpus as type category\ny = cat_encoder.fit_transform(news_train_test['label'].astype('category'))\nprint(y)","ff0019a6":"#importing the library\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)","166c1fa2":"print('Shape of train data')\nprint(x_train.shape)\nprint(y_train.shape)\n\nprint('Shape of test data')\nprint(x_test.shape)\nprint(y_test.shape)","d3bc1f96":"#Model building Naive Bayes Classifier\nfrom sklearn.naive_bayes import MultinomialNB\nclassifier = MultinomialNB()\n\n#fitting our train data with our classifier to create the model\nclassifier.fit(x_train, y_train)\n\nprint('Training data accuracy')\nprint(classifier.score(x_train , y_train))","dc65da71":"y_pred = classifier.predict(x_test)\nprint('Test data accuracy')\nprint(classifier.score(x_test , y_test))","f3e4940d":"Now we will split our corpus into train data and test data. To split the corpus we will use **train_test_split** from **sklearn.model_selection** package","51f9f829":"Now we will build our classification model. As we are working on text documents and we have 1700 features, it is not advisable to use any random classifier. When we have a comparatively huge number of columns we will use Multinomial Naive Bayes. So we need to import **MultinomialNB** classifier from **sklearn.naive_bayes**","ab235a13":"Now we will iterate the news corpus and call removePunctuations(headline) and then will remove the stopwords from each headlines. We will append the filtered texts to an array. ","76eec01b":"Importing the train, test and stopword datasets","7a53c3f9":"Our target variable i.e. label column is in text format. So we need to encode it. We will use **LabelEncoder** class from **sklearn.preprocessing** library to encode our target value.","df90bc10":"Now we have our training set as x_train, y_train and test set as x_test, y_test. We can check the shape of our train and test set. You can check the shape of your train and test test. Number of rows in x_train and y_train should always match.","23ba29eb":"Now we need to clean the headlines. We have to remove punctuations and symbols from our news corpus. We will create a method to remove punctuations and symbols. Later we will call this method.","acdbdcbb":"Now we have our news corpus got cleaned. We can further clean it using **Lemmatizing** long words or by **stemming** them.\nNow we are going to extract the vectors out of the words from our news corpus.","b0b9518e":"Now as we have the filtered texts in an array. Now we have to add this array to our news corpus to get the label mapping.","1a104a4f":"Now we will predict the label of our test dataset."}}