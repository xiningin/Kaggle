{"cell_type":{"1feff8e7":"code","49447557":"code","06a566e2":"code","4843fa02":"code","b2c8a4eb":"code","7e67ee9c":"code","146bc23d":"code","dc7481cf":"code","e5722317":"code","ddaebadc":"code","93e23ac6":"code","d6b4c71b":"code","ed5eb649":"code","62fc4466":"code","a36c815d":"code","7bcdb17d":"code","61d3d68b":"code","37009780":"code","ad0c3f25":"code","0ff58ac1":"code","19395dc6":"code","da28f4ea":"code","27b8dc91":"code","774b2f27":"code","933b40e4":"code","afea7f9c":"code","727ce8ee":"code","3ebf8bf8":"code","882de0f4":"code","b7788c5c":"code","a2f82608":"code","de938bb4":"code","7421957b":"code","84c57ad2":"code","929f08b5":"code","698d1f1b":"code","04d48dbc":"code","153474bb":"code","badfdcd1":"code","feedb56d":"markdown","40824958":"markdown","38f4a65b":"markdown","6c2e9d74":"markdown","cb140f66":"markdown","7ed5538e":"markdown","c6d21706":"markdown","3a4ab343":"markdown","4bda0c23":"markdown","442b7a93":"markdown","beaf59d4":"markdown","5dbe7ed3":"markdown","a6bcab81":"markdown","c74f91da":"markdown","a24b8d80":"markdown","cd40eb18":"markdown","4ddf811d":"markdown","d721f185":"markdown","7ecd2b78":"markdown","096768d9":"markdown","1f8276ec":"markdown","9c403fc1":"markdown","b43f0807":"markdown","017e5e9e":"markdown","bdca253f":"markdown","a139dc53":"markdown","7321b410":"markdown","9ad61eab":"markdown","369e246e":"markdown","51b5f390":"markdown","b3e3550e":"markdown","b272047c":"markdown","302d67aa":"markdown","7b4249e5":"markdown","5ad25826":"markdown","5f28a6eb":"markdown","b7eed43d":"markdown","e3d4d7ca":"markdown","bc20d59f":"markdown","07e8d713":"markdown","05545941":"markdown","d319c105":"markdown","01ac48be":"markdown","c0de2897":"markdown","c9055aec":"markdown","2f3a600f":"markdown"},"source":{"1feff8e7":"import os\nimport warnings\nwarnings.filterwarnings('ignore')\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","49447557":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"darkgrid\")","06a566e2":"from sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.cluster import KMeans\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV, cross_val_score,RandomizedSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import classification_report,accuracy_score","4843fa02":"iris_train = pd.read_csv(\"\/kaggle\/input\/usp-pj01\/train_Iris.csv\")\niris_train.head()","b2c8a4eb":"iris_train.shape","7e67ee9c":"iris_test = pd.read_csv(\"\/kaggle\/input\/usp-pj01\/test_Iris.csv\")\niris_test.head()","146bc23d":"iris_train = iris_train.loc[:, 'SepalLengthCm':]\niris_train.describe()","dc7481cf":"print(iris_train.info())","e5722317":"fig, axs = plt.subplots(ncols=2, nrows=2, figsize=(10, 10))\nplt.subplots_adjust(right=1.5)\n\n\nfor i, column in enumerate(iris_train.columns.values[:-1]):\n  ax = axs[int(i\/2)][i%2]\n  sns.boxplot(x='Species', y=column, data=iris_train, ax=ax);\n  ax.set_xlabel('')\n  ax.set_ylabel(column)\nfig.tight_layout();","ddaebadc":"sns.pairplot(iris_train, hue=\"Species\");","93e23ac6":"fig, axs = plt.subplots(ncols=2, nrows=2, figsize=(7, 7))\nplt.subplots_adjust(right=1.5)\n\n\nfor i, column in enumerate(iris_train.columns.values[:-1]):\n  sns.distplot(iris_train.loc[:,column], ax=axs[int(i\/2)][i%2]);","d6b4c71b":"test_id = iris_test.Id\niris_test = iris_test.loc[:, 'SepalLengthCm':]","ed5eb649":"fig, axs = plt.subplots(ncols=2, nrows=2, figsize=(7, 7))\nplt.subplots_adjust(right=1.5)\n\n\nfor i, column in enumerate(iris_test.columns.values):\n  sns.distplot(iris_test.loc[:,column], ax=axs[int(i\/2)][i%2]);","62fc4466":"sns.countplot(x='Species', data=iris_train);","a36c815d":"matrix_corr = iris_train.corr()\nmask = np.triu(np.ones_like(matrix_corr, dtype=bool))\ncmap = sns.diverging_palette(250, 15, s=75, l=40,\n                            n=9, center=\"light\", as_cmap=True)\nplt.figure(figsize=(8,6))\nsns.heatmap(matrix_corr, mask=mask, center=0, annot=True,\n            fmt='.2f', square=True, cmap=cmap)\nplt.show();","7bcdb17d":"target_to_int = {iris_train.Species.unique()[i]: i for i in range(iris_train.Species.nunique())}\niris_train.Species = iris_train.Species.map(target_to_int)","61d3d68b":"y = iris_train.Species\niris_train.drop(['Species'], axis=1, inplace=True)","37009780":"scaler = StandardScaler()\nX_scaled = scaler.fit_transform(iris_train)\ntsne = TSNE(random_state=17)\ntsne_repr = tsne.fit_transform(X_scaled)","ad0c3f25":"dict_target = {0: 'Iris-versicolor', 1: 'Iris-virginica', 2: 'Iris-setosa'}","0ff58ac1":"plt.scatter(tsne_repr[:, 0], tsne_repr[:, 1],\n            c=y.map({0: 'blue', 1: 'orange', 2:'green'}), alpha=.5);","19395dc6":"class Iris_Model(object):\n    \n    def __init__(self):\n        self.load_data()\n        return None\n    \n    def load_data(self, iris_train=iris_train, y=y, iris_test=iris_test):\n        self.X_train, self.X_holdout, self.y_train, self.y_holdout = train_test_split(iris_train.values, y, test_size=0.25,random_state=17)\n        self.X_test = iris_test.values\n        self.final_predictions = None\n        return 1\n    \n    def Kmeans(self, n_clusters, output):\n        \n        km = KMeans(n_clusters=n_clusters, random_state=17)\n        km.fit(np.concatenate([self.X_train, self.X_holdout], axis=0))\n\n        self.X_train = pd.DataFrame(self.X_train)\n        self.X_holdout = pd.DataFrame(self.X_holdout)\n        self.X_test = pd.DataFrame(self.X_test)\n      \n        if output == 'total':\n            \n            self.X_train['label kmeans'] = km.labels_[0: self.X_train.shape[0]]\n            self.X_holdout['label kmeans'] = km.labels_[self.X_train.shape[0]:]\n            self.X_test['label kmeans'] = km.predict(self.X_test)\n            \n            return self\n    \n    def model(self, model):\n        model.fit(self.X_train, self.y_train)\n        predictions = model.predict(self.X_holdout)\n        results = cross_val_score(model, self.X_train, self.y_train, cv=5)\n        train_score = model.score(self.X_train, self.y_train)\n        holdout_score = model.score(self.X_holdout, self.y_holdout)\n        mean_cv_score = np.mean(results)\n        std_cv_score = np.std(results)\n\n        print(f\"CV score: {mean_cv_score} +- {std_cv_score}\")\n        print(70*\"-\")\n        print(f\"Train score: {train_score}\")\n        print(f\"Holdout score: {holdout_score}\")\n        print(classification_report(self.y_holdout, predictions))\n        print(\"Tuned best model parameters: {}\".format(model.best_params_))\n        self.final_predictions = model.predict(self.X_test)\n        \n        model_results = {\"mean_cv_accuracy\": mean_cv_score, \"std_cv_accuracy\": std_cv_score, \"train accuracy\": train_score, \"holdout accuracy\": holdout_score}\n        return model_results","da28f4ea":"pipeline = Pipeline([('SVM', SVC())])\n\nclist = [0.01, 0.05, 0.1, 0.5, 1, 10, 100]\ngammalist = 10.0 ** np.array([i for i in range(-3,3,1)])\nparameters = {'SVM__C':clist,\n              'SVM__gamma':gammalist,\n              'SVM__kernel':['linear', 'rbf', 'poly', 'sigmoid']}\n\nsvm_pipe = RandomizedSearchCV(pipeline, parameters, cv=5, n_jobs=-1, verbose=True, random_state=17)","27b8dc91":"svm = Iris_Model()\nsvm_results = svm.model(model=svm_pipe)\nfinal_results = pd.DataFrame(data=svm_results, index=['Only SVM'])","774b2f27":"svm_with_km = Iris_Model()\nsvm_with_km_results = svm_with_km.Kmeans(n_clusters=3, output = 'total').model(model=svm_pipe)\nfinal_results = final_results.append(svm_with_km_results, ignore_index=True)","933b40e4":"knn_pipe = Pipeline([('scaler', MinMaxScaler()), ('knn', KNeighborsClassifier(n_jobs=-1))])\n\nknn_par = {'knn__n_neighbors': range(1, 20), 'knn__weights':[\"uniform\", \"distance\"], 'knn__metric':[\"euclidean\", \"minkowski\", \"manhattan\"]}\n\nknn_model = RandomizedSearchCV(knn_pipe, knn_par, cv=5, n_jobs=-1, verbose=True, random_state=17)","afea7f9c":"knn_min_max = Iris_Model()\nknn_min_max_results = knn_min_max.model(knn_model)\nfinal_results = final_results.append(knn_min_max_results, ignore_index=True)","727ce8ee":"knn_min_max_km = Iris_Model()\nknn_min_max_km_results = knn_min_max_km.Kmeans(n_clusters=3, output='total').model(knn_model)\nfinal_results = final_results.append(knn_min_max_km_results, ignore_index=True)","3ebf8bf8":"knn_pipe_ss = Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier(n_jobs=-1))])\n\nknn_par_ss = {'knn__n_neighbors': range(1, 20), \n              'knn__weights':[\"uniform\", \"distance\"], \n              'knn__metric':[\"euclidean\", \"minkowski\", \"manhattan\"]}\n\nknn_model_ss = RandomizedSearchCV(knn_pipe_ss, knn_par_ss, cv=5, n_jobs=-1, verbose=True, random_state=17)","882de0f4":"knn_std = Iris_Model()\nknn_std_results = knn_std.model(knn_model_ss)\nfinal_results = final_results.append(knn_std_results, ignore_index=True)","b7788c5c":"knn_std_km = Iris_Model()\nknn_std_km_results = knn_std_km.Kmeans(n_clusters=3, output='total').model(model=knn_model_ss)\nfinal_results = final_results.append(knn_std_km_results, ignore_index=True)","a2f82608":"rf_pipe = Pipeline([('RF', RandomForestClassifier())])\n\nrf_param = {'RF__max_depth':range(1, 4),\n            'RF__max_features':range(2, 4)}\n\nrf_model = RandomizedSearchCV(rf_pipe, rf_param, cv=5, n_jobs=-1, random_state=17)","de938bb4":"rf = Iris_Model()\nrf_results = rf.model(rf_model)\nfinal_results = final_results.append(rf_results, ignore_index=True)","7421957b":"rf_km = Iris_Model()\nrf_km_results = rf_km.Kmeans(n_clusters=3, output = 'total').model(rf_model)\nfinal_results = final_results.append(rf_km_results, ignore_index=True)","84c57ad2":"list_alg = [\"Only SVM\", \"SVM + KMeans\", \"MinMaxScaler + KNN\", \"MinMaxScaler + Kmeans + KNN\",\n\"StandardScaler + KNN\", \"StandardScaler + KMeans + KNN\", \"Only RF\", \"RF + Kmeans\"]\nfinal_results = final_results.set_axis(list_alg, axis=0)\n","929f08b5":"final_results","698d1f1b":"final_results['ci_lower_bound'] = final_results['mean_cv_accuracy'] - final_results['std_cv_accuracy']\nfinal_results['ci_upper_bound'] = final_results['mean_cv_accuracy'] + final_results['std_cv_accuracy']\nfinal_results['err'] = (final_results['ci_upper_bound'] - final_results['ci_lower_bound']) \/ 2\n\nfig, ax = plt.subplots(figsize=(8,8))\n\nx_bar = np.linspace(0.1, 1, final_results.shape[0], endpoint=True)\n\nfor i in range(final_results.shape[0]):\n    ax.errorbar(y=final_results.iloc[i,0], \n                x=x_bar[i], yerr=final_results.iloc[i,6], \n                fmt='o', label=list_alg[i])\n    \nplt.legend(loc ='lower left')\nplt.ylim(0.80,1)\nplt.show()\n","04d48dbc":"ax = final_results.plot.bar(y = [\"train accuracy\", \"holdout accuracy\"], rot=0, figsize=(30,14))\nax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\nplt.show()","153474bb":"final = {\"Id\": test_id, \"Category\": pd.Series(np.vectorize(dict_target.get)(svm_with_km.final_predictions))}\nsubmission = pd.DataFrame(final)\nsubmission.to_csv('submission.csv', index=False)","badfdcd1":"submission","feedb56d":"## Project 01 - Iris Dataset","40824958":"t-distributed Stohastic Neighbor Embedding: encontra uma proje\u00e7\u00e3o para um espa\u00e7o n- dimensional (geralmente a proje\u00e7\u00e3o \u00e9 em 2D) que preserva a dist\u00e2ncia dos exemplos no espa\u00e7o antigo. Basicamente, encontra um embedding (tradu\u00e7\u00e3o de um vetor de alta dimens\u00e3o em um espa\u00e7o de baixa dimens\u00e3o) que preserva a vizinhan\u00e7a entre os exemplos em um espa\u00e7o de menor dimens\u00e3o.","38f4a65b":"Vale ressaltar que o uso de um \u00fanico vizinho resulta em um limite de decis\u00e3o que segue de perto os dados de treinamento. Caso consideremos mais e mais vizinhos, esta a\u00e7\u00e3o nos leva a um limite de decis\u00e3o mais suave. Este limite mais suave corresponde a um modelo mais simples. Em outras palavras, usar poucos vizinhos corresponde a uma alta complexidade do modelo, e usar muitos vizinhos corresponde a uma baixa complexidade do mesmo.\n","6c2e9d74":"* A distribui\u00e7\u00e3o no conjunto de treinamento \u00e9 normal apenas para alguns atributos. Usar uma normaliza\u00e7\u00e3o MinMaxScaler (dados no intervalo [0,1]) para um modelo usando KNN pode ser mais interessante do que o StandardScaler (normal padr\u00e3o).","cb140f66":"### Matriz de correla\u00e7\u00e3o\n\nA correla\u00e7\u00e3o \u00e9 uma m\u00e9trica do qu\u00e3o relacionadas duas vari\u00e1veis s\u00e3o. A correla\u00e7\u00e3o utilizada \u00e9 a de Pearson, linear, onde os coeficientes da correla\u00e7\u00e3o variam entre -1 e 1. Quanto mais pr\u00f3ximo de 1 mais correlacionadas s\u00e3o essas vari\u00e1veis, enquanto as mais pr\u00f3ximas de -1 s\u00e3o inversamente correlacionadas.  \n\nNa matriz abaixo, podemos identificar que h\u00e1 uma forte correla\u00e7\u00e3o positiva entre as vari\u00e1veis:\n\"PetalLengthCm\" e \"SepalLengthCm\" , \"PetalWidthCm\" e \"SepalLengthCm\" e entre as vari\u00e1veis \"PetalLengthCm\" e \"PetalWidthCm\". ","7ed5538e":"### Gr\u00e1ficos de distribui\u00e7\u00e3o\n\n\u00c9 a combina\u00e7\u00e3o de um histograma com a densidade das probabilidades de cada vari\u00e1vel. A seguir, \u00e9 poss\u00edvel observar que as vari\u00e1vel referente as s\u00e9palas possui uma distribui\u00e7\u00e3o mais aproximada da normal. Enquanto que para as p\u00e9talas n\u00e3o \u00e9 poss\u00edvel identificar suas distribui\u00e7\u00f5es apenas visualizando os dados, mas sabemos que n\u00e3o se tratam de uma distribui\u00e7\u00e3o normal. ","c6d21706":"* N\u00e3o encontrando nenhum dado NaN.","3a4ab343":"O objetivo do algoritmo de Support Vector Machine \u00e9 encontrar um hiperplano em um espa\u00e7o N-dimensional (N = o n\u00famero de atributos) que classifica distintamente os pontos de cada classe, ou seja, um plano que tenha a margem m\u00e1xima para que assim a dist\u00e2ncia entre os pontos de dados de ambas as classes seja o maior poss\u00edvel. Para o modelo de SVM, utilizando a classe IrisModel, configuramos um pipeline com o tipo de Support Vector para classifica\u00e7\u00e3o e uma varia\u00e7\u00e3o de seus principais param\u00eatros:","4bda0c23":"### Contagem dos dados categoricos \n\nA seguir, \u00e9 poss\u00edvel visulizar o n\u00famero de cada uma das categorias de esp\u00e9cies presentes no dataset:","442b7a93":"## Visualiza\u00e7\u00e3o dos dados","beaf59d4":"Usamos o conjunto de dados Iris (train e test) para construir um modelo de classifica\u00e7\u00e3o para plantas da esp\u00e9cie \u00edris. Para este desafio utilizamos o ***Holdout*** como estrat\u00e9gia de divis\u00e3o de dados, decidimos experimentar tanto a estrat\u00e9gia de padroniza\u00e7\u00e3o \/ normaliza\u00e7\u00e3o de dados ***StandardScaler*** e ***MinMaxScaler***, e para avaliar qual dos classificadores tiveram os melhores resultados nos dados, tomamos como principal m\u00e9trica de desempenho a acur\u00e1cia.","5dbe7ed3":"* Observando os boxplots podemos distinguir a classe da planta com mais facilidade para PetalLength e PetalWidth. Isso demostra que no iris a complexidade para separar as classes \u00e9 baixa.","a6bcab81":"#### RF + Kmeans","c74f91da":"### Random Forest","a24b8d80":"### TSNE","cd40eb18":"#### StandardScaler + KNN ","4ddf811d":"* Atrav\u00e9s da descri\u00e7\u00e3o dos dados podemos identificar alguns comportamentos para cada uma das vari\u00e1veis. Um exemplo: Podemos destacar desses n\u00fameros, a vari\u00e2ncia da vari\u00e1vel \"PetalLengthCm\", sendo a maior dentre as demais.\n","d721f185":"* A distribui\u00e7\u00e3o no conjunto de teste varia um pouco com rela\u00e7\u00e3o ao conjunto de treinamento","7ecd2b78":"Para encontrar os melhores par\u00e2metros para o modelo usamos valida\u00e7\u00e3o cruzada estratificada com 5 folds. Ap\u00f3s, retreinamos o modelo usando 75% dos dados para treinamento com os par\u00e2metros encontrados no ajuste e usamos 25% para valida\u00e7\u00e3o. Esse processo dos experimentos pode ser observado na imagem acima.","096768d9":"#### MinMaxScaler + Kmeans + KNN ","1f8276ec":"### Gr\u00e1ficos de pares\n\nEsse tipo de visualiza\u00e7\u00e3o, exibe a distribui\u00e7\u00e3o de cada vari\u00e1vel e caracter\u00edstica, assim como as rela\u00e7\u00f5es entre pares de vari\u00e1veis e caracteristicas (na forma de scatter plots). \n\nA seguir, \u00e9 poss\u00edvel observar melhor como s\u00e3o as distribui\u00e7\u00f5es de cada uma das vari\u00e1veis atrav\u00e9s dos gr\u00e1ficos exibidos na diagonal principal.","9c403fc1":"#### Only RF ","b43f0807":"Come\u00e7amos utilizando apenas o algor\u00edtimo SVM:","017e5e9e":"* O problema \u00e9 praticamente balanceado, n\u00e3o \u00e9 necess\u00e1rio utilizar t\u00e9cnicas para manejar o desbalanceamento.","bdca253f":"#### SVM + KMeans","a139dc53":"#### StandardScaler + KMeans + KNN ","7321b410":"#### MinMaxScaler + KNN ","9ad61eab":"Ao projetarmos o dataset no novo espa\u00e7o 2D encontrado pelo TSNE, claramente vemos que os exemplos da esp\u00e9cie setosa est\u00e3o distantes dos exemplos das outras duas classes e formam um cluster. Apesar dos exemplos da esp\u00e9cie virginica e versicolor estarem pr\u00f3ximos no espa\u00e7o, as duas classes quase formam dois clusters. Por esse motivo, pode ser interessante criar uma feature com as labels aprendidas por um algoritmo como o KMeans (no caso gostariamos que 3 clusters sejam criados).","369e246e":"O algoritmo k-NN \u00e9 indiscutivelmente o algoritmo de aprendizado de m\u00e1quina mais simples. A maioria das t\u00e9cnicas de aprendizado de m\u00e1quina considera o conjunto de dados como um todo a fim de aprender padr\u00f5es nos dados. O KNN, por outro lado, rejeita muitas informa\u00e7\u00f5es conscientemente, uma vez que a previs\u00e3o para cada ponto novo depende somente de alguns pontos mais pr\u00f3ximos. Construir o modelo em KNN \u00e9 muito r\u00e1pido, mas quando seu conjunto de treinamento \u00e9 muito grande (em n\u00famero de features ou em n\u00famero de samples), a previs\u00e3o pode ser lenta.","51b5f390":"## Model","b3e3550e":"* A vari\u00e1vel \"SepalWidthCm\" exibe as distribui\u00e7\u00f5es de cada uma as esp\u00e9cies de forma mais pr\u00f3xima, ou seja, existe uma menor variabilidade dos dados, al\u00e9m de apresentarem m\u00e9dias entre 3,5 e 4. J\u00e1 a vari\u00e1vel \"PetalLengthCm\", possui uma variabilidade maior, sendo a esp\u00e9cie setosa com a maior m\u00e9dia puxada pelo valor m\u00e1ximo encontrado nesse conjunto. \n\n* No gr\u00e1fico de dispers\u00e3o, verificamos que cortar o espa\u00e7o dependendo do atributo pode facilitar a classifica\u00e7\u00e3o. A distribui\u00e7\u00e3o do PetalLength e PetalWidth facilita a classifica\u00e7\u00e3o, pois h\u00e1 uma menor complexidade na separa\u00e7\u00e3o de classes. Utilizar um modelos baseados em dist\u00e2ncia como o KNN pode ser interessante, j\u00e1 que os exemplos de uma mesma classe podem estar pr\u00f3ximos no espa\u00e7o. Uma SVM tamb\u00e9m pode ser interessante para separar as classes usando um hiperplano. Uma \u00e1rvore de decis\u00e3o pode aprender regras para cortar o espa\u00e7o e separar as classes.","b272047c":"## EDA","302d67aa":"#### Only SVM","7b4249e5":"<img src=\"https:\/\/scikit-learn.org\/stable\/_images\/grid_search_workflow.png\" width=\"600\" height=\"600\" \/>","5ad25826":"### **Verificamos agroa se n\u00e3o h\u00e1 valores NaN dentro do dataset:**","5f28a6eb":"### Come\u00e7amos obtendo as estat\u00edticas descritivas do dataset:","b7eed43d":"<h3> Aline Fernanda da Concei\u00e7\u00e3o <\/h3> |  <h3> Bruner Eduardo Augusto Albrehct <\/h3>| <h3> Claudemir Firmino Souza Cruz <\/h3>|  <h3> Juan Carlos Elias Obando Valdivia <\/h3>\n:---: | :---:| :--:| :--:\n9437275|9435846|10734011|7487156","e3d4d7ca":"### KNN","bc20d59f":"### SVM","07e8d713":"Como podemos observar nosso melhor modelo foi o SVM usando a feature gerada pelo KMeans pois a acur\u00e1cia no treino e holdout foram as maiores obtidas. Al\u00e9m disso, vemos que m\u00e9dia da acur\u00e1cia no Cross Validation foi a maior e o desvio padr\u00e3o foi baixo.","05545941":"### Boxplots\n\nEsse tipo de gr\u00e1fico \u00e9 utilizado na visualiza\u00e7\u00e3o da distribui\u00e7\u00e3o dos dados. Sendo composto pelo primeiro e terceiro quartil, mediana, e limites superiores e inferiores, al\u00e9m da facilidade de visualizar a presen\u00e7a de outliers (representados pelos pontos acima e abaixo dos limites). A seguir, os boxplots para cada uma das vari\u00e1veis de acordo com a esp\u00e9cie de Iris: ","d319c105":"A principal desvantagem das \u00e1rvores de decis\u00e3o \u00e9 que elas tendem a sobrecarregar os dados de treinamento. Ja as Random forests s\u00e3o uma forma de resolver esse problema, por serem uma cole\u00e7\u00e3o de \u00e1rvores de decis\u00e3o, em que cada uma \u00e9 ligeiramente diferente das demias e que todas conseguem fazer um trabalaho realativamente bom de previs\u00e3o. \n\nToamamos a decis\u00e3o de usar RF porque \u00e1rvores de decis\u00e3o n\u00e3o requerem muito pr\u00e9-processamento. Por exemplo, os atributos podem estar em diferentes escalas e magnitudes, o que pode ser vantajoso ajustar um modelo. Al\u00e9m disso, o RF \u00e9 um algoritmo de Bagging que reduz a vari\u00e2ncia do erro do classificador, o qual tamb\u00e9m permite que encontremos quais atributos de entrada contibuem mais para o resultado no atributo de sa\u00edda, ou seja, a import\u00e2ncia dos atributos para a previs\u00e3o.","01ac48be":"- Uma observa\u00e7\u00e3o interssante \u00e9 que as Random forests n\u00e3o tendem a ter um bom desempenho em dados esparsos de dimens\u00f5es muito altas, como dados de texto. Para este tipo de dados, modelos lineares podem ser mais apropriados. As Random forests geralmente funcionam bem mesmo em conjuntos de dados muito grandes, e o treinamento pode ser facilmente paralelizado em muitos n\u00facleos de CPU em um computador poderoso. No entanto, as Random forests requerem mais mem\u00f3ria e s\u00e3o mais lentas para treinar e prever do que os modelos lineares. Se o tempo e a mem\u00f3ria forem importantes em uma aplica\u00e7\u00e3o, pode fazer sentido usar um modelo linear.","c0de2897":"A classe Iris_Model faz todo o processo de modelagem: \n\n- Divide os dados: conjunto de treino (75%) e holdout(25%)\n- Testar da cria\u00e7\u00e3o de uma feature nova usando o KMeans para criar 3 clusters. A predi\u00e7\u00e3o de uma classe para cada exemplo usando Kmeans pode ajudar a previs\u00e3o final do nosso modelo. Dependendo, podemos usar a feature nova ao instanciarmos um objeto do tipo Iris_Model\n- Faz a modelagem usando RandomizedSearchCV para encontrar os melhores par\u00e2metros para o modelo utilizado. Como definimos os par\u00e2metros para SVM abaxo, eles ser\u00e3o usados como default.\n- Reporta medidas de desempenho do modelo: acur\u00e1cia no treino, acur\u00e1cia no holdout, mean cv score, std cv score, imprimi um report (precision, recall,f1-score e support) e faz a predi\u00e7\u00e3o do modelo","c9055aec":"Como foi explicado anteriormente no TSNE, usaremos o SVM com o Kmeans:","2f3a600f":"## Importing libraries and dataset"}}