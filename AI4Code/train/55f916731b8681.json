{"cell_type":{"1e737fdd":"code","3ceb3d71":"code","7140162f":"code","2d8c0c07":"code","88e4b774":"code","516ae60e":"code","cb7b3561":"code","6c6c6d11":"code","1f471c65":"code","4b7d010e":"code","7bd76bc1":"code","8c8eddb9":"code","2450dea1":"code","a7b2744c":"code","7994ab56":"code","ecb55e72":"code","bc1bf1f4":"code","65642070":"code","99a22145":"code","cfd49d57":"code","8858a946":"code","55a789f0":"code","f0a54e77":"code","a2214581":"markdown","8d3a3f38":"markdown","3807b4ca":"markdown","11285153":"markdown","fd35b0fa":"markdown","1b72063e":"markdown","f050b7de":"markdown","0e757a5f":"markdown","1d8094aa":"markdown","feea42eb":"markdown","d6003dd4":"markdown","ea87ada7":"markdown","d5bc1137":"markdown","f1e9dca4":"markdown","13707415":"markdown","f6db9dc3":"markdown","7be283be":"markdown","f443868e":"markdown","973d5671":"markdown","8cfb4dff":"markdown","7f2209df":"markdown"},"source":{"1e737fdd":"%%HTML\n<a id=\"Analysis\"><\/a>\n<center>\n<iframe width=\"700\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/cJukBLH4qP4\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" style=\"position: relative;top: 0;left: 0;\" allowfullscreen ng-show=\"showvideo\"><\/iframe>\n<\/center>","3ceb3d71":"import numpy as np\nimport pandas as pd\n\n# for vis\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n# anomaly and skewness detection \nfrom scipy import stats\nfrom scipy.stats import skew, norm\nfrom scipy.special import boxcox1p\nfrom numpy import mean, std\n\nfrom IPython.display import HTML\nfrom matplotlib import animation\nfrom termcolor import colored\n\n# Importing and concating train and test set\npath = '..\/input\/house-prices-advanced-regression-techniques'\ntrain = pd.read_csv(path + r'\/train.csv')\ntest = pd.read_csv(path + r'\/test.csv')\n\ntrain.name = 'train'\ntest.name = 'test'\n\n# keeping testing id for submission in the future\ntest_id = test.Id\nfor df in [train, test]:\n    df.drop(columns = ['Id'], inplace = True)\n    \ndf_concat = pd.concat([train, test], axis = 0).reset_index(drop = True)\ndf_concat.name = 'both dfs'\n\n\ndf_concat.loc[:train.shape[0], 'which'] = 'train'\ndf_concat.loc[train.shape[0]:, 'which'] = 'test'","7140162f":"# Dropping two unuseful columns\ndf_concat.drop(columns = ['PoolQC', 'Utilities'], inplace = True)\n\n# Filling missing values\n\n# Filling with zero\n# fields about the Garage\nfor field in ['GarageType', 'GarageFinish','GarageQual', 'GarageCond',\n              'BsmtFinType1','BsmtQual','BsmtCond', 'BsmtExposure', 'BsmtFinType1', \n              'BsmtFinType2','MiscFeature','Alley','Fence','FireplaceQu',\n               'MasVnrType' ] :\n    df_concat[field].fillna('None',inplace=True)\n    \nfor field in ['MasVnrArea','BsmtFullBath','BsmtHalfBath'\n              ,'BsmtFinSF1','GarageCars','GarageArea','TotalBsmtSF',\n             'BsmtUnfSF','BsmtFinSF2','GarageYrBlt','TotalBsmtSF']:\n    df_concat[field].fillna(0,inplace=True) \n\n\n# Filling with appropriate values\ndf_concat['LotFrontage'] = df_concat.groupby('Neighborhood')['LotFrontage']\\\n                          .transform(lambda x: x.fillna(x.mean()))\nfor feature in ['MSZoning', 'Electrical']:\n    df_concat[feature] = df_concat.groupby('Neighborhood')[feature]\\\n                        .transform(lambda x: x.fillna(x.mode()[0]))\n\nfor field in ['SaleType','Exterior1st','Exterior2nd',]:\n    df_concat[field].fillna(df_concat[field].mode()[0],inplace=True)\n    \ndf_concat.Functional.fillna('Typ',inplace=True)\ndf_concat.KitchenQual.fillna('TA',inplace=True)\n\n# Converting categorical data into numerical\n\n### ordinal \nordinal_fields_with_labelencoder=['LandSlope','YearBuilt','YearRemodAdd',\n                                  'CentralAir','GarageYrBlt','PavedDrive',\n                                  'YrSold']\n\n### ordinal with labelencoder...\nfor field in ordinal_fields_with_labelencoder:\n    le = LabelEncoder()\n    df_concat[field] = le.fit_transform(df_concat[field].values)\n\nfeatures_that_are_already_ordinal = ['OverallQual','OverallCond','MoSold',\n                                     'FullBath','KitchenAbvGr','TotRmsAbvGrd']\n\n\n### ordinal features that need to be sorted with ordinal encoder...\nfields_that_need_to_be_ordered = [\n              'MSSubClass','ExterQual','LotShape','BsmtQual','BsmtCond',\n              'BsmtExposure','BsmtFinType1', 'BsmtFinType2','HeatingQC',\n              'Functional','FireplaceQu','KitchenQual', 'GarageFinish',\n              'GarageQual','GarageCond','Fence'\n                                    ]\nfor field in  fields_that_need_to_be_ordered:\n    df_concat[field] = df_concat[field].astype(str)\n\n\norders=[#msclass\n    ['20','30','40','45','50','60','70','75','80','85', '90','120','150','160','180','190'],\n    #ExterQual\n    ['Po','Fa','TA','Gd','Ex'],\n    #LotShape\n    ['Reg','IR1' ,'IR2','IR3'],\n    #BsmtQual\n    ['None','Fa','TA','Gd','Ex'],\n    #BsmtCond\n    ['None','Po','Fa','TA','Gd','Ex'],\n    #BsmtExposure\n    ['None','No','Mn','Av','Gd'],\n    #BsmtFinType1\n    ['None','Unf','LwQ', 'Rec','BLQ','ALQ' , 'GLQ' ],\n    #BsmtFinType2\n   ['None','Unf','LwQ', 'Rec','BLQ','ALQ' , 'GLQ' ],\n    #HeatingQC\n    ['Po','Fa','TA','Gd','Ex'],\n    #Functional\n   ['Sev','Maj2','Maj1','Mod','Min2','Min1','Typ'],\n    #FireplaceQu\n    ['None','Po','Fa','TA','Gd','Ex'],\n    #KitchenQual\n    ['Fa','TA','Gd','Ex'],\n    #GarageFinish\n    ['None','Unf','RFn','Fin'],\n    #GarageQual\n    ['None','Po','Fa','TA','Gd','Ex'],\n    #GarageCond\n    ['None','Po','Fa','TA','Gd','Ex'],\n    #PoolQC\n    #['None','Fa','Gd','Ex'],\n    #Fence\n    ['None','MnWw','GdWo','MnPrv','GdPrv'] ]\n   \n\n### ordinal features with specific order.....\nfor i in range(len(orders)):\n\n    ord_en = OrdinalEncoder(categories = {0:orders[i]})\n    df_concat.loc[:,fields_that_need_to_be_ordered[i]] = ord_en.fit_transform(df_concat.loc[:,fields_that_need_to_be_ordered[i]].values.reshape(-1,1))\n\n# Finally one hot encoding categorical data that are not ordinal    \ndf_concat=pd.get_dummies(df_concat.drop(columns = ['which']))\ntrain = df_concat[:train.shape[0]]\ntest = df_concat[train.shape[0]:].drop(columns = ['SalePrice'])\ndf_concat.head()","2d8c0c07":"# One thing we add to the above code from the previous tutorial is \n# finding overfitted features in which more than 99 percent of \n# all the data is filled with one value and removing them.\n# This happens after one-hot encoding\n# when the appearance of one value is \n# rare between different values of one feature\n\ndef finding_over_fitting_features(df, percentage = 99.9):\n    overfit=[]\n    for feature in df.columns:\n        most_frequent=(df[feature] .value_counts().iloc[0])\n        if most_frequent\/len(df) *100 >percentage:\n            overfit.append(feature)\n    return(overfit)\noverfitted = finding_over_fitting_features(df_concat, percentage = 99.0)\ndf_concat.drop(columns = overfitted, inplace = True)","88e4b774":"!git clone https:\/\/github.com\/mitramir55\/mood_swings.git\ndf = pd.read_csv(r'mood_swings\/mood swings.csv', sep = '\u001b')","516ae60e":"trace1 = go.Scatter(x=df[\"day\"], y=df[\"mood\"],\n                    mode = 'lines', line = dict(width = 2))\nframes = [dict(data = [dict(type = 'scatter',\n                      x = df[\"day\"][:k],\n                      y = df[\"mood\"][:k])\n                      ], \n              traces = [0]) for k in range(1, len(df[\"day\"])-1)]\n\n\nlayout = go.Layout(width=1000,\n                   height=400,\n                   updatemenus=[\n                        dict(\n                            type='buttons', showactive=False,\n                            buttons=[dict(label='Play',\n                            method='animate',\n                            args=[None,\n                                  dict(frame=dict(duration=200),\n                                   transition=dict(duration=50),\n                                     mode='immediate'              )]\n                            )]\n                        ),\n                    ]              \n                  )\nlayout.update(xaxis=dict(range=[min(df.day),max(df.day)], autorange=False),\n              yaxis=dict(range=[0,100],  autorange=False));\n\ngo.Figure(data = [trace1], frames = frames,\n          layout = layout)","cb7b3561":"fig, axes = plt.subplots(1, 2, figsize=(20,8),)\nfig.suptitle('Before and After Manually removing outliers')\naxes[0].set_title('Before')\naxes[1].set_title('After')\n\ng = sns.regplot(x=train['GrLivArea'], y=train['SalePrice'], ax = axes[0])\ng.text(4500, 400000, \"Outliers\", horizontalalignment='left', size='large', color='black', weight='semibold')\ng.axvline(4000, ls='--', color = 'red')\n\nidx_manual = np.where(train['GrLivArea']>4000)[0]\ntrain_NoOutlier = train.drop(idx_manual)\ng = sns.regplot(x=train_NoOutlier['GrLivArea'], y=train_NoOutlier['SalePrice'], ax = axes[1])\n","6c6c6d11":"fig, axes = plt.subplots(figsize=(20,8),)\n\nsns.distplot(train[\"SalePrice\"], kde = True, )","1f471c65":"fig = plt.figure(figsize = (20,4))\nax = fig.add_subplot(111)\nstats.probplot(train.SalePrice, dist=stats.norm, plot = ax)\nplt.show()","4b7d010e":"sale = train['SalePrice']\nprint('Skewness before and after transformation:')\nprint('Before: ', sale.skew())\nsale_t_log = np.log(sale)\nprint('After log:', sale_t_log.skew())\nsale_t_box = boxcox1p(sale,stats.boxcox_normmax(sale+1))\nprint('After Box-Cox:', sale_t_box.skew())","7bd76bc1":"fig, axes = plt.subplots(figsize=(20,8),)\n\ng = sns.distplot(sale_t_box)\ng.set_title('After transforming the data we get a gussian-like distribution');","8c8eddb9":"def anomaly_detection_deletation(normal_dist, alpha):\n    list_detected_points=[]\n    \n    mean_, std_ = mean(normal_dist), std(normal_dist)\n    pdf=norm.pdf(normal_dist,mean_, std_)\n    anom_indices=np.where(pdf<alpha)[0]\n    \n    for idx in anom_indices:\n        list_detected_points.append((idx))\n    \n    # plot\n    fig, axes = plt.subplots(1, 2, figsize=(20,8),)\n    axes[0].set_title('Transformed data with anomalies in red')\n    axes[1].set_title('True values with anomalies in red')\n    axes[0].set(xlabel='Transformed SalePrice', ylabel='Probability Density of each point')\n    axes[1].set(xlabel='True values of SalePrice', ylabel='Probability Density of each point')\n    \n    g = sns.scatterplot(normal_dist, pdf, ax = axes[0])\n    \n    \n    sns.scatterplot([normal_dist[i] for i in anom_indices],\n                    [pdf[i] for i in anom_indices], color = 'red', ax = axes[0])\n    \n    g.axvline(mean_, ls='--', color = 'red')\n    g.text(mean_, 0.5, \"mean\", size='large', color='black', weight='semibold')\n    \n    g.axvline(mean_-3*std_, ls='--', color = 'green')    \n    g.axvline(mean_+3*std_, ls='--', color = 'green')\n    g.text(mean_+3*std_, 0.5, \"%99.7\", size='large', color='green')\n    \n    g.axvline(mean_+2*std_, ls='--', color = 'orange')\n    g.axvline(mean_-2*std_, ls='--', color = 'orange')\n    g.text(mean_+2*std_, 0.5, \"%95\", size='large', color='orange')\n    \n    g.axvline(mean_+1*std_, ls='--', color = 'gray')\n    g.axvline(mean_-1*std_, ls='--', color = 'gray')\n    g.text(mean_+1*std_, 0.5, \"%68\", size='large', color='gray')\n    \n    sns.scatterplot(train.SalePrice, pdf, ax = axes[1])\n    sns.scatterplot([train.loc[i,'SalePrice'] for i in anom_indices],\n                    [pdf[i] for i in anom_indices], color = 'red', ax = axes[1])\n    return list_detected_points\n\nidx_stat = anomaly_detection_deletation(sale_t_box,alpha = 0.003)# we choose to have 99.7 percent of our data","2450dea1":"# I comment these to have all the data for the next methods too.\n# train.drop(index = [i for (i,x) in anomaly_detection_deletation(sale_t_box ,alpha=0.003)],inplace=True)\n# train.reset_index(drop = True,inplace = True)","a7b2744c":"n = 1000\nx = np.random.normal(size = n)\ny = np.random.normal(size = n)\nz = np.random.normal(size = n)\nx_c = np.linspace(-3, 3, n)\ny_c = np.sqrt(9-x_c**2)\nc_1 = np.concatenate([x_c, -1* x_c])\nc_2 = np.concatenate([y_c, -1*y_c])\nc_3 = np.array([0]* 2 * n)\n\nfig = plt.figure(figsize = (10,8))\nax = fig.add_subplot(111, projection = '3d')\nax.scatter(x, y, z)\n\nax.plot(c_1, c_2, c_3, color = 'red')\nax.plot(c_2, c_3, c_1, color = 'red')\nax.plot(c_3, c_2, c_1, color = 'red');\nfor ii in range(0,360,1):\n        ax.view_init(elev=10, azim=20)\nax.set_title('Three features each having a Gaussian distribution');\n\n\n### ---------- for animating the graph add the following code--------###\ndef animate(frame):\n    ax.view_init(10, frame\/3)\n    return fig\n\n\nanim = animation.FuncAnimation(fig, animate, frames=200, interval=50)\nHTML(anim.to_html5_video())","7994ab56":"def detect_skewness(df):\n    skewed=pd.DataFrame(df[:].skew(axis=0).sort_values(ascending=False),columns=['skewness'])\n    \n    return skewed\ndetect_skewness(df_concat)","ecb55e72":"\n# transforming the label\nlabel_t = boxcox1p(train.SalePrice,stats.boxcox_normmax(train.SalePrice+1))\n# features\ndf_concat_features_t = df_concat.copy().drop(columns = ['SalePrice']) \nskew_df = detect_skewness(df_concat_features_t)\n\n# skewed more than 0.3?\nskewed_features = skew_df[(skew_df.skewness>0.3)|(skew_df.skewness<-0.3)].index\n\n# Transforming with boxcox normmax\nfor field in list(skewed_features):\n    df_concat_features_t[field]=boxcox1p(df_concat_features_t[field],stats.boxcox_normmax(df_concat_features_t[field]+1))\ndf_concat_features_t.head()","bc1bf1f4":"skew_df = detect_skewness(df_concat_features_t)\n\nnot_skewed = skew_df[(skew_df.skewness<0.1)&(skew_df.skewness>-0.1)].index\ntrain_t = df_concat_features_t.loc[:train.shape[0]-1, not_skewed]\ntrain_t_with_label = pd.concat([train_t, label_t], axis = 1)","65642070":"detect_skewness(train_t_with_label)","99a22145":"from sklearn.covariance import EllipticEnvelope\n\nMCD_model = EllipticEnvelope(contamination = 0.03)\n# contamination = the proportion of outliers in the dataset\n\n# model fitting\ny_pred = MCD_model.fit_predict(train_t_with_label)\n\n# filter outlier index\nidx_MCD = np.where(y_pred == -1)[0] # negative values are outliers\n\noutlier_values = train.iloc[idx_MCD]\n# plot data\nfigure = plt.figure(figsize = (10, 8))\nsns.scatterplot(train.GrLivArea, train.SalePrice, color = \"blue\", s = 6)\n# plot outlier values\nsns.scatterplot(outlier_values.GrLivArea, outlier_values.SalePrice, color = \"red\")","cfd49d57":"from sklearn.neighbors import LocalOutlierFactor\n\nlof_model = LocalOutlierFactor(n_neighbors = 150, metric = \"manhattan\",\n                            contamination = 0.01)\n# which features to take into account?\nselected_features = 'all'\n# model fitting\nif selected_features == 'all': y_pred = lof_model.fit_predict(train[:])\nelse: y_pred = lof_model.fit_predict(train[features])\n# filter outlier index\nidx_LOF = np.where(y_pred == -1)[0] # negative values are outliers\n\noutlier_values = train.iloc[idx_LOF]\n# plot data\nfigure = plt.figure(figsize = (10, 8))\nsns.scatterplot(train.GrLivArea, train.SalePrice, color = \"blue\", s = 6)\n# plot outlier values\nsns.scatterplot(outlier_values.GrLivArea, outlier_values.SalePrice, color = \"red\")","8858a946":"from sklearn.ensemble import IsolationForest\n\nisof = IsolationForest(contamination='auto')\n# which features to take into account?\nselected_features = 'all'\n# model fitting\nif selected_features == 'all': y_pred = isof.fit_predict(train[:])\nelse: y_pred = isof.fit_predict(train[features])\n\n# filter outlier index\nidx_isof = np.where(y_pred == -1)[0]\noutlier_values = train.iloc[idx_isof]\n\n# plot data\nfigure = plt.figure(figsize = (10, 8))\nsns.scatterplot(train.GrLivArea, train.SalePrice, color = \"blue\", s = 6)\n# plot outlier values\nsns.scatterplot(outlier_values.GrLivArea, outlier_values.SalePrice, color = \"red\")","55a789f0":"#Calculates the mean of cv and r2 scores\nn_splits=5\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\ndef tell_me(model,x,y):\n    kf=KFold(n_splits,shuffle=True)\n    cv_mse_errors=np.sqrt(-cross_val_score(model, x,y, \n                                           scoring=\"neg_mean_squared_error\",cv = kf))\n    cv_r2_scores=cross_val_score(model, x,y, scoring=\"r2\", cv =5 ) \n\n    print('Mean of all the MSE errors: ',np.mean(cv_mse_errors))\n    \n    print('Mean of all the r^2 scores ',np.mean(cv_r2_scores))\n    ","f0a54e77":"outliers = [[], idx_manual,idx_stat, idx_LOF, idx_isof, idx_MCD]\nfor i, idx in enumerate(outliers):\n    print(colored('method ', 'green'), i+1)\n    print('number of deleted points: ', colored(len(idx), 'red'))\n    \n    tell_me(Ridge(),df_concat_features_t[:train.shape[0]].drop(index = idx), label_t.drop(index = idx))","a2214581":"Thanks for reading this notebook! I hope it was useful :)\n# Also please Upvode if it was ;)","8d3a3f38":"What we are doing in the above mentioned function is that we defined a threshold(alpha also known as **p-value**) and we want to take out the points that have a smaller probability of happening than alpha. So when our alpha is 0.003, we color the ones that have smaller than 0.05 probability of happening and in other words, belonging to this distribution.\n\nOk so now we identified the ones that are not very common and usual and can easily take them out:","3807b4ca":"Some great resources! make sure to check them out for more info:\n* [Sergio Santoyo on TowardsDataScience](https:\/\/towardsdatascience.com\/a-brief-overview-of-outlier-detection-techniques-1e0b2c19e561)\n* [Data Skewness and Transformation by Nicholas J. Cox, Durham University](http:\/\/fmwww.bc.edu\/repec\/bocode\/t\/transint.html)\n* [Box-Cox Transformation-1](https:\/\/www.statisticshowto.com\/box-cox-transformation\/)\n* [Box-Cox Transformation-2](https:\/\/towardsdatascience.com\/box-cox-transformation-explained-51d745e34203)\n* [P-value intuition by Cassie Kozyrkov](https:\/\/youtu.be\/9jW9G8MO4PQ)\n* [Isolation Forests](https:\/\/cs.nju.edu.cn\/zhouzh\/zhouzh.files\/publication\/icdm08b.pdf?q=isolation-forest)\n* [Local Outlier Factor on TowardsDataScience](https:\/\/towardsdatascience.com\/local-outlier-factor-lof-algorithm-for-outlier-identification-8efb887d9843)\n* [Local Outlier Factor on GeeksForGeeks](https:\/\/www.geeksforgeeks.org\/local-outlier-factor\/)","11285153":"Besides all these functions there is one more that often seems a bit confusing but can help us alot in reducing skewness. It's **Box-Cox Transformation**.\n## What is Box-Cox Transformation?\nWhen you look at the equasion bellow you'll see we have a **lambda (\u03bb)** that decides for the shape of the transformation. We vary \u03bb from -5 to +5 so to see which one gives us the best approximation of a normal distribution. \n![image.png](attachment:image.png)\n\nYou can imagine how having 1 as \u03bb will have no effect and will simply return $Y-1$ as the new values.\n\nThe Box-Cox power transformation searches for various values of lambda by various methods until the best value is found. One of those methods is calculating the log-likelihood of each lambda and see which transformation gives us the biggest likelihood of being a normal distribution.\n\nSo for comparing these different functions we use a library to measure the skewness of our data.","fd35b0fa":"# Table of Content\n\n* [Handling Missing Values and Categorical Data- Notebook 1 in this series](https:\/\/www.kaggle.com\/mitramir5\/missing-values-ordinal-data-and-stories)\n* [Definition of Anomalies](#formal-def)\n* [Different Types of Anomalies](#types)\n* [Detecting Manually](#manual)\n* [Data Transformation and it's methods](#transform)\n* [Detecting Automatically](#automatic)\n* [Comparing methods](#compare)\n\n\nAfter handling null values and turning categories into digits and making them ready for our models, Now we want to transform the data for anomaly detection and models that need normaly distributed features. I did my best to thoroghly explain each part, so feel free to skip to the part you need to read. So let's get going \ud83d\udeb6\u200d\u2640\ufe0f \ud83d\udeb6\n<a id=\"formal-def\"><\/a>\n# Why do we need \"Anomaly Detection\"?\n\nThere are several reasons why someone would concider deleting few examples of their dataset, even when the dataset is small and we need every bit of information we can get. Outliers can be destructive to our model and our preception of reality. We want our model to predict the most probable label and not be affected by some random value in our dataset. The best way is to remove as little as possible, but make the models robust so that it can ignore or emulate their effect on our prediction.\n\n<div class=\"alert alert-warning\" role=\"alert\">\n\n<h5><span class=\"label label-warning\", align = \"middle\">Quick note about this competition<\/span><\/h5>\n   \nThe evaluation metric of this competition is RMSE(Root Mean Squared Error). This is a metric that is more sensitive to outliers than other methods such as MAE(Mean Absolute Error). Therefore it is important to construct a model based on non-outliers or make our models robust to them so that we won't get the values similar to them in the predictions for the test set. learn more about these metrics <a href = 'https:\/\/medium.com\/human-in-a-machine-world\/mae-and-rmse-which-metric-is-better-e60ac3bde13d'>here<\/a>.\n<\/div>\n  \n","1b72063e":"<a id=\"#compare\"><\/a>\n# Comparing Methods\ud83e\uddd8\ud83c\udffb\u200d\nNow we want to know which method is actually giving us the best results, so we're going to use some simple algorithms to determine which would work the best for us. Although keep in mind that for algorithms like linear regression, lasso, Ridge,etc. we need to have unskewed data, so we first check which ones are skewed and then transform them with the same methods as we did earlier.","f050b7de":"<a id=\"#automatic\"><\/a>\n# Automatic Detection \ud83e\udd16\n## Minimum Covariance Determination\nThis is the multivariate version of the method we just used in the previous section. In fact in this method we have to have several Gaussian or Gaussian-like features, and then form an ellipsoid shape with them in space. The features that fall out of this ellipsoid will be our outliers. Take a look at the graph bellow:\n![image.png](attachment:image.png)\n[image source](https:\/\/commons.wikimedia.org\/wiki\/File:Multivariate_normal_sample.svg)","0e757a5f":"<a id=\"transform\"><\/a>\nright now we can see that SalePrice data is right skewed. This means that for both regression models and outlier detection we will have some problems if we don't do something about it. Because As you can read in [Jason Brownlee says in his blog](https:\/\/machinelearningmastery.com\/how-to-use-statistics-to-identify-outliers-in-data\/):\n> If we know that the distribution of values in the sample is Gaussian or Gaussian-like, we can use the standard deviation of the sample as a cut-off for identifying outliers.\n\nSo we have to have a normal distribution to use variance and mean to detect outliers, but why? based [on this question on stackoverflow: ](https:\/\/stats.stackexchange.com\/questions\/129274\/outlier-detection-on-skewed-distributions)\n> (summarised)When the data is skewed we only tend to expect outliers to be on the skewed part (in our case on the right side). Therefore we forget about the other side unless when they're really extreme!\n\nSo without having a symetric bell-shaped distribution, we won't be able to identify outliers correctly. What we have to do is to transform the data. \n\n## What is Data Transformation and Its Methods?\n\nTransformation is the replacement of a variable by the function of that variable. We can replace our skewed feature, with the root or logarithm of it. See the graph below for more intuition.\n\n![image.png](attachment:image.png)\n\nThe graph makes it more easy for us to see how log and sqrt are shrinking the big xs to smaller ys so that the right skewed dataset with values spread out to the extremes shrink onto a smaller range.\n\nThere are so many other transformations like **cube root**($x^{1\/3}$), **reciprocal** ($1\/x$) or **negative reciprocal** ($-1\/x$), **square** ($x^2$).\n\nThe question is which one works best with your data and makes sense. After applying the function we apply a reverse function to bring the values back to their original state.\n<div class=\"alert alert-success\" role=\"alert\">\n  \nSometimes in the real world we tend not to apply the reverse function as the transformated values are simple and make sense to us:\n> If possible, we prefer measurement scales that are  easy to think about. The cube root of a volume and the square root of an area both have the dimensions of length, so far from complicating matters, such transformations may simplify them. \n    <a href = 'http:\/\/fmwww.bc.edu\/repec\/bocode\/t\/transint.html'>Nicholas J. Cox, Durham University<\/a>\n<\/div>","1d8094aa":"# Let's Use Some Statistics \ud83d\ude0e\nFirst let's see how the distribution of The SalePrice is. Now because we're only looking at the sale price this is a univariate outlier detection.","feea42eb":"Look at how data is less spread and more concentrated in a smaller range of Ground Living Area. Although you can still look a bit closer and see that there are some very small values in the bottom left corner of the plot that show properties sold with abnormally small values. You can go ahead and remove them and see what happens to your results. I've manually removed Prices lower than 40000 and indeed it helped in the accuracy. We will look at the scientific methods that would do this for us. ","d6003dd4":"\n## LOF(Local Outlier Factor)\ud83c\udf73\nThis method is very much like how we group points together in the K-Nearest Neighbors algorithm. we're calculating outliers based on their local neighboorhood density. Just keep in mind these two and their differences:\n\n**k-neighboors:** K points around one example\n\n**k-distance:** The distance between an example and its $K^{th}$ nearest neighbor.\n![image.png](attachment:image.png)\nimage: [LOF: Identifying Density-Based Local Outliers](https:\/\/www.researchgate.net\/publication\/221214719_LOF_Identifying_Density-Based_Local_Outliers)\n1. **Reachability-distance**: We calculate the Reachability-distance for each example.(If points be in a specific radius of the example, we'll put k-distance as the reachability distance, otherwise for points outside that  radius we'll put the distance between that point and our centeral example)\n2. **Local Reachability Density (LRD):** Calculate the average of all reachability distances for one example and reverse it (we want the density so the larger the average of distances, the less the dense around one point is)\n3. **Local Outlier Factor(LOF)**: We calculate the average LRD of all k-points around an example and compare it with the LRD of our example. (We divide them therefore the closer it is to 1, the more likely it is  to be similar to its neighbors and not an outlier.)\n\nFor equations and mathematical concepts [read this blog](https:\/\/towardsdatascience.com\/local-outlier-factor-lof-algorithm-for-outlier-identification-8efb887d9843).\nBecause we'll use different features to detect outlier, this will be a multivariate prediction.\nBellow we're calculating the density based on the euclidean distances between examples.","ea87ada7":"## What is the definition and why do we have them?\nOutliers are abnormal observations that diverge from other groups. They can have negative effects on our preception of the data and the construction of our model.\n\n**We might have outliers because of:**\nData entry or human errors, damaged or not qualified measurement instruments, data manipulation, dummies made to test detection methods or to add noise, and finally novelties in data.\n\nEven when you generate random numbers from a  distribution(e.g. gaussian), there will be some rare values that stand far away from the mean of all other examples. These are the ones we want to get rid of (or analyze in the real world to know why they are there).\n\n## Should we always remove them?\nWell interesting question, No! more precisely, not always! They are there for a reason. Why do we have them? are the measuring tools not working correctly? Are we observing a pattern in the time and place they occur? Maybe there is a bottleneck in our system that is generating them.\n\n  ","d5bc1137":"Interesting! We took all the features into account and some points that don't even seem like outliers while plotted on this graph, are distinguished as anomalies. This time the smaller the score, the more likely it is that we're detecting an outlier!\n\n## Isolation Forests\ud83c\udf33\nIn this method we try to isolate the anomalies based on their divergent features(multivariate) and their rare appearance.\n>  iForest also works well in high dimensional problems which have a large number of irrelevant attributes,\nand in situations where training set does not contain any\nanomalies. [Iforests Paper](https:\/\/cs.nju.edu.cn\/zhouzh\/zhouzh.files\/publication\/icdm08b.pdf?q=isolation-forest)\n\n**What happens in iforests?**\n1. We recursively partition data based on random features and make decision trees.\n2. Then we calculate a **path length** which is the average path length to get to the node of all the trees in our forest for each examples.\n3. We compare these lengths. The examples with the smallest length(score) are the anomalies. As they are very different in features and are less than other groups and required less partitioning in a high dimentional space to get to the node.\n\n![image.png](attachment:image.png)\nimage: [Sahand Harir-LSST Workshop 2018](file:\/\/\/D:\/Machine%20Learning\/Coursera_Deep\/hariri_forest.pdf)","f1e9dca4":"*** If you get a warning** just like I did, it is because of some features that have too many zeros or ones(constant values). *\n\nNow After all the transformation, our data is ready to be fed into our model with different anomaly detection methods. The first item in the outliers list is an empty list indicating the baseline performance.","13707415":"We can also make use of another statistical method called the QQ-plot: \n## QQ-plot?\nIn a QQ-plot, **we try to compare our data's distribution with a nother distribution(such as a normal bell-curved distribution) and see whether they match**.\n\nIn this plot we have two axis; Y axis that shows the quantiles generated from our dataset and X axis that is generated from a normal distribution. Then we simply plot data points based on which quantile they are in, in each our dataset and a normally distributed dataset. If the two distributions match, the datapoints will be in the same quantiles and therefore make up a streight ($X=Y$) line. But if they differ, we'll have points going up and down from this streight line. Learn more [here](https:\/\/www.youtube.com\/watch?v=okjYjClSjOg).","f6db9dc3":"***Well Box-Cox crushed it!*** As it turns out we have other features in this dataset that are skewed as well. Just keep in mind that we soon have to transform them as well.\n\nSo now let's calculate the variance and mean to see which ones are out of the normal range:","7be283be":"# So now let's see how we can detect them\ud83e\uddd0:\nSo this journey starts with two important questions. \n* Which and how many features are you going to take into account?(Univariate or multivariate)\n* whether or not we can assume a distribution for the selected features.\n","f443868e":"<a id=\"#types\"><\/a>\n# What are different types and how to understand them?\ud83c\udfa8\n## Univariate and Multivariate\n\n1. **Univariate outliers**: When we look at the values in a single feature space (for example only looking at the distribution of the SalePrice column).\n\n2. **Multivariate outliers**: When we look at an n-dimentional space with each dimention representing one feature. In this case because we have too many features to take into account, we cannot simply plot the data and detect which point is away from the normal groups, therefore we use models to do this detection for us.\n\n## Point, Contextual, Collective\n\nFor simplicity I'll tell you about my experiment on myself.\ud83e\udddb\u200d During the second half of the quarantine, I was loosing that motivation to stay focused and happy so I wanted to see which activities actually contributed to my happines and lightened the mood! Just keep in mind that the main theme here is depression and boredom;\n\n1. **Point\/Global Outliers**: While looking at the data, there are points in which I see one significant spike in my happiness. Well when I read my journals I see in that day my sister bought me a nice present. This does not happen very often(unfortunately) this is a random point that can't mean anything in particular \ud83d\ude0a\nThese are single points lying significantly away from the normal groups and distribution. They don't depend on any other feature and are randomly generated.\n2. **Contextual\/Conditional Outliers**: After a closer look I saw that there were a few day that my mood spiked up and I was very happy. In these days I notice that I've been doing alot of focused studying and also vigoures work outs. So the level of happiness is now related to other tasks of my day. When examples are not outside the usual range of features but are still abnormal compared to the most frequent values we have Contextual outliers. \n3. **Collective outliers**: This is when looking at the data helps us understand some novelties and end up in us making new discoveries! Like when there are some specific days in which I'm observing some high levels of happiness but no specific thing that I can point out to as the cause! Maybe it's publishing videos, maybe it's writing, drawing and all the other activities. Indeed these outliers are the ones that help austro physisists find out about new galaxies or comets!\nTo put it a bit more formally, they deviate from a non-parametric distribution.\n","973d5671":"\n<a id=\"#manual\"><\/a>\n## Manually and With Visual Tools\nWhen we read the discription of this dataset in [here](http:\/\/jse.amstat.org\/v19n3\/decock\/DataDocumentation.txt), we'll notice in the SPECIAL NOTES section, the collectors of this dataset mention that we have about 5 outliers in the data. Three of them are true outliers (Partial Sales that likely don\u2019t represent actual market values) and two of them are simply unusual sales (very large houses priced relatively appropriately). Then they recommend removing houses with more that 4000 square feet from the data. So let's plot it against the Ground Living Area and see what we get. I've highlighted the ones that the discription is talking about.","8cfb4dff":"Depending on how many and which features we want to use, we have to transform the ones that don't have a Gaussian-like disribution. So now we detect the ones that are skewed and apply box-cox transformation on each of them. This is the precedure:\n1. Print out the skewness of each feature\n2. Transform the ones with skewness>threshold or <-threshold (here threshold = 0.3)\n3. Get this transformed dataset and choose a few features who could turn into a Gaussian or Gaussian-like distribution with little skewness.\n\n We'll also use the transformed data at the end for feeding it to a Ridge regression and comparing different methods. (That's why I separated label and feature)","7f2209df":"Now we quickly handle missing values and ordinal data (for understanding the following code please refer to the [previous kernel](https:\/\/www.kaggle.com\/mitramir5\/missing-values-ordinal-data-and-stories) in this series.)"}}