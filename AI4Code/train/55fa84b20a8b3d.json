{"cell_type":{"8ccdeafd":"code","07ce19d2":"code","859f8169":"code","9b54f354":"code","bb8b2d44":"code","65d8beff":"code","4fa4e983":"code","37bf039e":"code","d2c0a870":"code","385d4a2b":"code","30121764":"code","aff65b77":"code","049fd40f":"code","3f2ebeef":"code","4db29749":"code","70ad4ffa":"code","e7f9dded":"code","6c786b87":"code","2a8ecaab":"code","add2ed0d":"code","98d0c157":"code","e4648ad2":"code","4666f4f0":"code","e0a9eceb":"code","555b0fdc":"code","ab1f9bbb":"code","fc9c66d9":"code","4d64237c":"code","5c194a4e":"code","1d85d744":"code","2c3f2e32":"code","4cb863bf":"markdown","95c5ef21":"markdown","02c07d15":"markdown","ece6e07f":"markdown","36502a6a":"markdown","322c3de4":"markdown","d40e275b":"markdown","aaa6a630":"markdown","cb99c34c":"markdown","96d88b11":"markdown","9728c98d":"markdown","a4cb1f21":"markdown","dbd61a02":"markdown"},"source":{"8ccdeafd":"import numpy as np\nimport pandas as pd\ndata_term_dep = pd.read_csv(\"..\/input\/bank-marketing-term-deposit\/bank_customer_survey.csv\")\n#Top 5 rows of the dataset is shown below.\ndata_term_dep.head()","07ce19d2":"#Dataset has 17columns and around 45k rows as shown below:\ndata_term_dep.shape","859f8169":"def memory_management(train_identity):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n    to reduce memory usage.\"\"\"\n\n    df=train_identity\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type in [int,float]:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n       # else:\n        #    df[col] = df[col].astype('category')\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    print(\"*******************************************************************************************\")\n    train_identity=df\n    return df","9b54f354":"# size reduced for data and named as data1\ndata_term_dep1=memory_management(data_term_dep)","bb8b2d44":"def null_values(base_dataset):\n    print(base_dataset.isna().sum())\n    ## null value percentage     \n    null_value_table=(base_dataset.isna().sum()\/base_dataset.shape[0])*100\n    ## null value percentage beyond threshold drop , else treat the columns    \n    retained_columns=null_value_table[null_value_table<30].index\n    # if any variable as null value greater than input(like 30% of the data) value than those variable are consider as drop\n    drop_columns=null_value_table[null_value_table>30].index\n    base_dataset.drop(drop_columns,axis=1,inplace=True)\n    len(base_dataset.isna().sum().index)\n    cont=base_dataset.describe().columns\n    cat=[i for i in base_dataset.columns if i not in base_dataset.describe().columns]\n    for i in cat:\n        base_dataset[i].fillna(base_dataset[i].value_counts().index[0],inplace=True)\n    for i in cont:\n        base_dataset[i].fillna(base_dataset[i].median(),inplace=True)\n    print(base_dataset.isna().sum())\n    return base_dataset,cat,cont","65d8beff":"data_term_dep2,cat,cont=null_values(data_term_dep1)","4fa4e983":"def outliers_transform(base_dataset):\n    for i in base_dataset.var().sort_values(ascending=False).index[0:10]:\n        x=np.array(base_dataset[i])\n        qr1=np.quantile(x,0.25)\n        qr3=np.quantile(x,0.75)\n        iqr=qr3-qr1\n        utv=qr3+(1.5*(iqr))\n        ltv=qr1-(1.5*(iqr))\n        y=[]\n        for p in x:\n            if p <ltv or p>utv:\n                y.append(np.median(x))\n            else:\n                y.append(p)\n        base_dataset[i]=y","37bf039e":"outliers_transform(data_term_dep2)","d2c0a870":"#Display the columns after outlier treatment\ndata_term_dep2.columns","385d4a2b":"dummy_columns=[]\nfor i in data_term_dep2.columns:\n    if (data_term_dep2[i].nunique()>=3) & (data_term_dep2[i].nunique()<5):\n        dummy_columns.append(i)","30121764":"dummy_columns","aff65b77":"#Dummy Variable\ndummies_tables=pd.get_dummies(data_term_dep2[dummy_columns])","049fd40f":"for i in dummies_tables.columns:\n    data_term_dep2[i]=dummies_tables[i]","3f2ebeef":"#Displaying columns after dummy variable creation\ndata_term_dep2.columns","4db29749":"#Drop the existing columns after the creation of dummy variable for those\ndata_term_dep2=data_term_dep2.drop(dummy_columns,axis=1)","70ad4ffa":"data_term_dep2.columns","e7f9dded":"from sklearn.preprocessing import LabelEncoder\ndef label_encoders(data,cat):\n    le=LabelEncoder()\n    for i in cat:\n        le.fit(data[i])\n        x=le.transform(data[i])\n        data[i]=x\n    return data","6c786b87":"data_new=data_term_dep2\ncat=data_term_dep2.describe(include='object').columns","2a8ecaab":"label_encoders(data_new,cat).head()","add2ed0d":"data_new.columns","98d0c157":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(20,10))\nfor i in data_new.var().index:\n    sns.distplot(data_new[i],kde=False)\n    plt.show()","e4648ad2":"plt.figure(figsize=(20,10))\nsns.heatmap(data_new.corr())","4666f4f0":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import BaggingClassifier","e0a9eceb":"y=data_new['y']\nx=data_new.drop('y',axis=1)","555b0fdc":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(x,y,test_size=0.20,random_state=43)","ab1f9bbb":"print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)","fc9c66d9":"models=[DecisionTreeClassifier(),RandomForestClassifier(),BaggingClassifier()]","4d64237c":"from sklearn.metrics import confusion_matrix,accuracy_score\nfinal_accuracy_scores=[]\nfor i in models:\n    dt=i\n    dt.fit(X_train,y_train)\n    dt.predict(X_test)\n    dt.predict(X_train)\n    print(confusion_matrix(y_test,dt.predict(X_test)))\n    print(accuracy_score(y_test,dt.predict(X_test)))\n    print(confusion_matrix(y_train,dt.predict(X_train)))\n    print(accuracy_score(y_train,dt.predict(X_train)))\n    print(i)\n    final_accuracy_scores.append([i,confusion_matrix(y_test,dt.predict(X_test)),accuracy_score(y_test,dt.predict(X_test)),confusion_matrix(y_train,dt.predict(X_train)),accuracy_score(y_train,dt.predict(X_train))])\n    from sklearn.model_selection import cross_val_score\n    print(cross_val_score(i,X_train,y_train,cv=10))","5c194a4e":"final_accuracy_scores1=pd.DataFrame(final_accuracy_scores)","1d85d744":"final_accuracy_scores1","2c3f2e32":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test,dt.predict(X_test))","4cb863bf":"## Supervised","95c5ef21":"## Dummy Variable Declaration","02c07d15":"### Regression","ece6e07f":"# Model Building","36502a6a":"## Univariate analysis (EDA) ","322c3de4":"# Problem definition:\nThe data is related with direct marketing campaigns of a banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) was subscribed or not. Data set has 17 predictor varaibles (features) and around 45K rows. ","d40e275b":"## Outlier treatment ","aaa6a630":"## Null value treatment ","cb99c34c":"Label Encoder","96d88b11":"# Creation of base dataset","9728c98d":" ### Classification","a4cb1f21":"## \tBivariate analysis (EDA)","dbd61a02":"## Memory management "}}