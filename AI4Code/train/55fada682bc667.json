{"cell_type":{"ef98a09b":"code","ea08a878":"code","df9727f7":"code","a93d947f":"code","67930359":"code","14e962de":"code","5b5a99de":"code","e770e170":"code","8fbc1fe2":"code","79513eb5":"code","5f9cbdf3":"code","9f140ee8":"code","901d13ed":"code","07098568":"code","85bfe6b2":"code","5c83d7ed":"code","dc8258da":"code","7a75eace":"code","4b7e0480":"code","73b507bc":"code","77a1ff0f":"code","b9ae5348":"code","02a03a3a":"markdown","73266413":"markdown","03ee9471":"markdown","d79a0014":"markdown","8fee1cbe":"markdown","a98bf3a4":"markdown","c7d32b91":"markdown","81b7a20f":"markdown","9c18c5c5":"markdown"},"source":{"ef98a09b":"import matplotlib.pyplot as plt\n\nimport numpy as np\n\nfrom l5kit.data import ChunkedDataset, LocalDataManager\nfrom l5kit.dataset import EgoDataset, AgentDataset\n\nfrom l5kit.rasterization import build_rasterizer\nfrom l5kit.configs import load_config_data\nfrom l5kit.visualization import draw_trajectory, TARGET_POINTS_COLOR\nfrom l5kit.geometry import transform_points\nfrom tqdm import tqdm\nfrom collections import Counter\nfrom l5kit.data import PERCEPTION_LABELS\nfrom prettytable import PrettyTable\n\nimport os\n\nfrom matplotlib import animation, rc\nfrom IPython.display import HTML\n\nrc('animation', html='jshtml')\n\ndef set_seed(seed):\n    # random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    # torch.manual_seed(seed)\n    # torch.cuda.manual_seed(seed)","ea08a878":"set_seed(42)","df9727f7":"# set env variable for data\nos.environ[\"L5KIT_DATA_FOLDER\"] = \"..\/input\/lyft-motion-prediction-autonomous-vehicles\"\n# get config\ncfg = load_config_data(\"..\/input\/lyft-config-files\/visualisation_config.yaml\")\ncfg['model_params']['history_num_frames'] = 3  # note when training model, we set this to 10\ncfg['raster_params']['disable_traffic_light_faces'] = False\nprint(cfg)","a93d947f":"dm = LocalDataManager()\ndataset_path = dm.require(cfg[\"val_data_loader\"][\"key\"])\nzarr_dataset = ChunkedDataset(dataset_path)\nzarr_dataset.open()\nprint(zarr_dataset)","67930359":"# Semantic view\ncfg['raster_params']['map_type'] = 'py_semantic'\nsemantic_rasterizer = build_rasterizer(cfg, dm)\n\n# Satellite view\ncfg['raster_params']['map_type'] = 'py_satellite'\nsatellite_rasterizer = build_rasterizer(cfg, dm)","14e962de":"semantic_dataset = EgoDataset(cfg, zarr_dataset, semantic_rasterizer)\nsatellite_dataset = EgoDataset(cfg, zarr_dataset, satellite_rasterizer)","5b5a99de":"# note raster_from_agent is actually a constant matrix for each raster once you fix the raster params\nraster_params = cfg['raster_params']\nraster_from_agent = np.array([\n    [2., 0.,  56.],\n    [0., 2., 112.],\n    [0., 0.,   1.],\n]) if (\n    raster_params['raster_size'] == [224, 224] and\n    raster_params['pixel_size'] == [0.5, 0.5] and\n    raster_params['ego_center'] == [0.25, 0.5]\n) else None\n    \ndef generate_image_trajectory(dataset, index):\n    data = dataset[index]\n    im = data['image'].transpose(1, 2, 0)\n    im = dataset.rasterizer.to_rgb(im)\n    rfg = raster_from_agent if raster_from_agent is not None else data['raster_from_agent']\n    target_positions_pixels = transform_points(data['target_positions'], rfg)\n    draw_trajectory(im, target_positions_pixels, yaws=data['target_yaws'], rgb_color=TARGET_POINTS_COLOR)\n    return im\n\ndef plot_trajectory(dataset, indices, width=12, height=4, n_cols=3, title=''):\n    if not isinstance(indices, (list, np.ndarray)):\n        indices = [indices]\n    n_rows = len(indices) \/\/ n_cols + len(indices) % n_cols\n    plt.figure(figsize=(width, height*n_rows))\n    for k, index in enumerate(indices):\n        plt.subplot(n_rows, n_cols, 1+k).set_title(str(index))\n        im = generate_image_trajectory(dataset, index)\n        plt.imshow(im, origin='lower')\n    if title:\n        plt.suptitle(title)\n    plt.show()","e770e170":"i_plots = np.random.randint(len(semantic_dataset), size=9)\ni_plots = (i_plots - i_plots[0] + 18552) % len(semantic_dataset)\ni_plots","8fbc1fe2":"plot_trajectory(semantic_dataset, i_plots)","79513eb5":"plot_trajectory(satellite_dataset, i_plots)","5f9cbdf3":"# note that the frame are continue. the green (history) and red (future) curves are from other frames\nplot_trajectory(semantic_dataset, list(range(18552, 18552+9)))","9f140ee8":"agent_semantic_dataset = AgentDataset(cfg, zarr_dataset, semantic_rasterizer)\nagent_satellite_dataset = AgentDataset(cfg, zarr_dataset, satellite_rasterizer)","901d13ed":"plot_trajectory(agent_semantic_dataset, i_plots)","07098568":"plot_trajectory(agent_satellite_dataset, i_plots)","85bfe6b2":"def animate(images):\n    fig = plt.figure()\n    ims = [(plt.imshow(im, animated=True, origin='lower'),) for im in images]\n    anim = animation.ArtistAnimation(fig, ims, interval=60, blit=True, repeat_delay=1000)\n    plt.close()\n    return HTML(anim.to_jshtml())\n\ndef plot_scene(dataset, scene_id):\n    indices = dataset.get_scene_indices(scene_id)\n    print('scene', scene_id, ':', indices[0], '-', indices[-1])\n    images = [generate_image_trajectory(dataset, i) for i in indices]\n    return animate(images)\n\nimport bisect\ndef get_scene_index_from_frame_id(dataset, frame_id):\n    return bisect.bisect_right(dataset.cumulative_sizes, frame_id)","5c83d7ed":"scene_id = get_scene_index_from_frame_id(semantic_dataset, i_plots[0])","dc8258da":"plot_scene(semantic_dataset, scene_id)","7a75eace":"plot_scene(satellite_dataset, scene_id)","4b7e0480":"agent_scene_id = get_scene_index_from_frame_id(agent_semantic_dataset, i_plots[4])","73b507bc":"plot_scene(agent_semantic_dataset, agent_scene_id)","77a1ff0f":"plot_scene(agent_satellite_dataset, agent_scene_id)","b9ae5348":"# see what each agent were doing\nplot_trajectory(agent_semantic_dataset, list(range(47930, 47930+9)))","02a03a3a":"# Scene Visualization\n\nI simplified the visualization codes from @jpbremer and @corochann","73266413":"#  Entire Scene\nA scene is just a lot of frames","03ee9471":"# Load the data","d79a0014":"### References\n* https:\/\/www.kaggle.com\/corochann\/lyft-comprehensive-guide-to-start-competition\/\n* https:\/\/www.kaggle.com\/jpbremer\/lyft-scene-visualisations","8fee1cbe":"# Build the Rasterizers","a98bf3a4":"### Agent\nNote one agent scene contains lots of agents. So you will see the image center jump around at each agent","c7d32b91":"## Autonomous Vehicle (Ego)","81b7a20f":"## Agent\nOther objects that are not the Ego","9c18c5c5":"# Frame"}}