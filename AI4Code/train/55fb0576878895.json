{"cell_type":{"c8295eef":"code","1b01d398":"code","5e65afd0":"code","09e3b116":"code","0f1879cc":"code","f309a477":"code","74800a7d":"code","a3809e0f":"code","a7633ed7":"code","7a5aa197":"markdown","1c657f9d":"markdown","5d1f9d08":"markdown","7e2738da":"markdown","bf83a2b5":"markdown","b7285a53":"markdown","7067e541":"markdown","f7e2c525":"markdown","96c22400":"markdown"},"source":{"c8295eef":"#Als erstes lesen wir die .arff-Datei ein, die unsere Trainingsdaten enth\u00e4lt\ndef read_data(filename):\n    f = open(filename)\n    data_line = False\n    data = []\n    for l in f:\n        l = l.strip() \n        if data_line:\n            content = [float(x) for x in l.split(',')]\n            if len(content) == 3:\n                data.append(content)\n        else:\n            if l.startswith('@DATA'):\n                data_line = True\n    return data\n\ntrain = read_data(\"..\/input\/kiwhs-comp-1-complete\/train.arff\")","1b01d398":"import numpy as np\nimport pandas as pd\ndf_data = pd.DataFrame({'x':[item[0] for item in train], 'y':[item[1] for item in train], 'Category':[item[2] for item in train]})\n\ndf_data.head()\n","5e65afd0":"from sklearn.model_selection import train_test_split\n\nX = df_data[[\"x\",\"y\"]].values\nY = df_data[\"Category\"].values\ncolors = {-1:'red',1:'blue'}\n\nX_Train, X_Test, Y_Train, Y_Test = train_test_split(X,Y, random_state=0, test_size = 0.2)","09e3b116":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaler.fit(X_Train)\n\nX_Train = scaler.transform(X_Train)\nX_Test = scaler.transform(X_Test)","0f1879cc":"%matplotlib inline\nimport matplotlib.pyplot as plt\n\nplt.scatter(X[:,0],X[:,1],c=df_data[\"Category\"].apply(lambda x: colors[x]))\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.show()","f309a477":"from sklearn.neighbors import KNeighborsClassifier\n\ntest_accuracy = []\n\nneighbors_range = range(1,50)\n\nfor n_neighbors in neighbors_range:\n    \n    clf = KNeighborsClassifier(n_neighbors=n_neighbors)\n    clf.fit(X_Train, Y_Train)\n    test_accuracy.append(clf.score(X_Test, Y_Test))    \n    \nplt.plot(neighbors_range, test_accuracy, label='Genauigkeit bei den Testdaten')\nplt.ylabel('Genauigkeit')\nplt.xlabel('Anzahl der Nachbarn')\nplt.legend()","74800a7d":"model = KNeighborsClassifier(n_neighbors = 14)\nmodel.fit(X_Train, Y_Train)\n\nprint(model.score(X_Test,Y_Test))","a3809e0f":"#Wird f\u00fcr externe .py-Datein ben\u00f6tigt.\nimport importlib.machinery\nmodulename = importlib.machinery.SourceFileLoader('helper','..\/input\/helper\/helper.py').load_module()\n\n# 1. Bild mit Trennlinie\nfrom helper import plot_classifier\n\nplot_classifier(model,X, Y, proba = False, xlabel = \"x\", ylabel=\"y\")\n\n# 2. Bild mit Wahrscheinlichkeiten\nfrom helper import plot_classifier\n\nplot_classifier(model,X, Y, proba = True, xlabel = \"x\", ylabel=\"y\")","a7633ed7":"######### hier versuchen wir nun das Vorhersagen#######\ntestdf = pd.read_csv(\"..\/input\/kiwhs-comp-1-complete\/test.csv\")\n\ntestX = testdf[[\"X\",\"Y\"]].values\nmodel.predict(testX)\n######################################################\n\n\n######## Anschlie\u00dfend Speichern wir unsere Vorhersage ab #######\nprediction = pd.DataFrame()\nid = []\nfor i in range(len(testX)):\n    id.append(i)\n    i = i + 1\nprediction[\"Id (String)\"] = id \nprediction[\"Category (String)\"] = model.predict(testX).astype(int)\nprint(prediction[:10])\nprediction.to_csv(\"predict.csv\", index=False)\n##################### ENDE ####################################","7a5aa197":"Zu guter Letzt versuchen wir nun, aus der Test.csv-Datei alle Punkte zu kategorisieren. Dazu nutzen wir unser erlerntes Modell. Anschlie\u00dfend schreiben wir die Prediction in eine csv-Datei zur Abgabe ;)","1c657f9d":"Nun haben wir unsere Daten gesplittet und m\u00fcssen diese skalieren, damit diese geordnet bei der Visualisierung aussehen.","5d1f9d08":"Jetzt haben wir ein Dataframe, welches unsere vorherigen Daten aus der Liste enth\u00e4lt. Als N\u00e4chstes k\u00f6nnen wir nun unsere Daten in Trainings- und Testdaten aufteilen. Zun\u00e4chst definieren wir aber ein Dataframe X, welches alle x und y-Variablen f\u00fcr alle Punkte enth\u00e4lt. Des Weiteren ein Y Dataframe, welches die Kategorien der Punkte enth\u00e4lt. Mit der Methode train_test_split splitten wir unsere Daten auf. Zus\u00e4tzlich erstellen ordnen wir den Kategorien eine Farbe zu, damit wir diese sp\u00e4ter visualisieren k\u00f6nnen.","7e2738da":"Nun fangen wir endlich an unser Model zu trainieren!! Dazu nutzen wir die von sklearn bereitgestellte Funktion KNeighborsClassifier, die es uns erm\u00f6glicht den k-nearest-neighbor-Algorithmus auf unsere Daten anzuwenden. Als Parameter geben wir die Variable n_neighbors mit, die angibt, wie viele Nachbarn ausgehend von einem Punkt aus beobachtet werden sollen. Dieser Wert ist extrem wichtig!! Ein zu kleiner Wert,kann dazu f\u00fchren, dass das Modell zu ungenau wird. Ein zu gro\u00dfer Wert kann dazu f\u00fchren, dass das Modell zu allgemein wird. \n \nDamit wir wissen, welche Anzahl der Nachbarn am besten f\u00fcr uns ist, testen wir die verschiedenen Modelle mit unterschiedlichen n_neighbor-Parametern und lassen uns danach die Werte als Graph ausgeben. Wir sehen, dass 14 den besten Wert f\u00fcr den Parameter n_neighbors hat. ","bf83a2b5":"**Klassifizieren von Daten mit Hilfe von k-Nearest-Neighbor**\n\nWir wollen mit Hilfe von k-Nearest-Neighbor verschiedene Punkte auf einer 2D-Ebene klassifizieren. Die Idee ist, dass wir uns die Nachbarn jedes einzelnen Punktes anschauen und anhand der Nachbarn bestimmen, zu welcher Kategorie der Punkt geh\u00f6rt. Daraus k\u00f6nnen wir dann ein Modell lernen und dieses auf unkategorisierte Daten anwenden.\n\nZun\u00e4chst definieren wir zwei Farben, in denen die Punkte aufgeteilt werden sollen. Wir entscheiden uns f\u00fcr rote und blaue Punkt. Der erste Schritt wird das Einlesen der Daten beinhalten:","b7285a53":"Jetzt, wo wir die Daten eingelesen haben, bauen wir uns ein Pandas-Dataframe aus den Daten zusammen, damit wir sp\u00e4ter mit sklearn unser Modell trainieren k\u00f6nnen. Zur \u00dcberpr\u00fcfung geben wir die ersten Zeilen des Dataframes mit dem Befehl .head() aus. Die Trainingsdaten bestehen jeweils aus einer x-Variable, einer y-Variable und der zugeordneten Kategorie. Die Kategorie wird in diesem Fall als -1 oder 1 angegeben. Wir definieren f\u00fcr unser besseres Verst\u00e4ndnis die -1 als roten Punkt und 1 als blauen Punkt.","7067e541":"Wir berechnen nun das Modell mit n_neighbors = 14\n\nIm Anschluss der Trainingsphase geben wir den Score aus. 0.9875. Klingt doch gut :-)","f7e2c525":"**Visualisierung der Trainingsdaten**\nWir visualisieren nun unsere Trainingsdaten und weisen den Punkten die oben definierte Farbe zu. Wir erkennen nun bereits ein Muster. Die roten Punkte scheinen sowohl f\u00fcr x und y \u00f6fter < 0 zu sein. Die blauen Punkte haben in der Mehrzahl, Werte f\u00fcr x und y oberhalb von 0.","96c22400":"Nun haben wir endlich unser Modell trainiert! Aber wie sieht das nun eigentlich aus? Was haben wir davon?\nNachfolgend werden zwei Graphen erzeugt.\n\nDer erste Graph zeigt die genaue Trennlinie, die unser Modell gelernt hat, um die Punkte in zwei \"Cluster\" einzuteilen. Wenn nun ein unkategorisierter Punkt innerhalb eines Bereiches liegt, wei\u00df das Modell, zu welchem Bereich der Punkt geh\u00f6rt.\n\nDer zweite Graph zeigt die Wahrscheinlichkeiten, mit der das Modell wom\u00f6glich unsere Punkte kategorisiert. Ein tiefrotes oder ein tiefblauer Bereich sagt aus, dass Punkte in diesen Bereichen mit hoher Wahrscheinlichkeit richtig kategorisiert werden. Hellere Farben zeigen lediglich die Tendenz zur Kategorisierung an. Diese Bereiche sind sch\u00f6n in dem Bereich zu sehen, wo vorher die Trennlinie (1. Bild) zu sehen war.\n"}}