{"cell_type":{"e58ec4d9":"code","0f29d9a0":"code","6ee41b2d":"code","1c35c313":"code","d3b5d333":"code","87f25297":"code","a1470d14":"code","6e5e31f4":"code","1cc46a4e":"code","655b149c":"code","c47df09a":"code","1028cc16":"code","660dd3de":"code","d2052748":"code","a8872d8e":"code","12b122d5":"code","08700508":"code","8a355382":"code","7c69bebc":"code","6c5bb01a":"code","7ab95549":"code","cc0d81c1":"code","cba3561d":"code","576e8119":"code","ada8e042":"code","d60be627":"code","b27df423":"code","3f3bfb6e":"code","7d2c36fc":"code","4a858d1a":"code","f839a5c1":"code","339781e3":"code","ed99cc90":"code","937bff4c":"code","70a41cb8":"code","6e79cdee":"code","ced87290":"code","0a410521":"code","8bcb1861":"code","f96eef9a":"code","124fd036":"code","f051c104":"code","1a060869":"code","7b9420fa":"code","2e4242a1":"code","d1aca09a":"code","b955cf12":"code","3569c71f":"code","a6db5a1c":"code","c468b587":"code","6b769cc7":"code","10e6c9c4":"code","c1fd9096":"code","34c2522b":"code","cf55e2f2":"code","d9aa1730":"code","96a8bbe0":"markdown","bcd6da0a":"markdown","1c71a831":"markdown","c3783728":"markdown","c7bbd74f":"markdown","c8454925":"markdown","cbb17eb3":"markdown","c5a48db4":"markdown","85a3d887":"markdown","94a0824f":"markdown","9fa97cb2":"markdown","f21f3586":"markdown","d3722914":"markdown","c09b4a01":"markdown","1ecb4d79":"markdown","78786be1":"markdown"},"source":{"e58ec4d9":"# Import Necessary Libraries\n\n#data analysis libraries \nimport numpy as np\nimport pandas as pd\n\n#visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n# sns.set_palette(\"GnBu_d\")\nsns.set_style('whitegrid')\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Importing Classifier Modules\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\n\n# Load the library for splitting the data\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics","0f29d9a0":"# Read in and Explore the Data\n\n# Import train and test CSV files\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")","6ee41b2d":"train.head()","1c35c313":"# Data Analysis\n# Observing summarized information of data like Data types, Missing values etc.\nprint(\"\\nTrain set summary\")\nprint(train.info())\n\nprint(\"\\nTest set summary\")\nprint(test.info())","d3b5d333":"print(\"\\nTrain set summary\")\ntrain.describe()","87f25297":"print('Shape before deleting duplicate values:', train.shape)\n\n# Removing duplicate rows if any\ntrain=train.drop_duplicates()\nprint('Shape After deleting duplicate values:', train.shape)","a1470d14":"# check for any missing values\nprint(\"\\n Check for any null values in Train set:\")\nprint(pd.isnull(train).sum())\n\nprint(\"\\n Check for any null values in Test set:\")\nprint(pd.isnull(test).sum())","6e5e31f4":"# sns.pairplot(train)","1cc46a4e":"# Observe the distribution of target variable\nprint(train['Survived'].value_counts())\nsns.countplot(x=\"Survived\", data=train)\nplt.title(\"Distribution of Survived\")\nplt.show()","655b149c":"# Finging unique values for each column\n# TO understand which column is categorical and which one is Continuous\n# Typically if the numer of unique values are < 20 then the variable is likely to be a category \n# otherwise continuous\ntrain.nunique()","c47df09a":"# Plotting multiple bar charts for categorical variables\n# Since there is no default function which can plot bar charts for multiple columns at once\n# we are defining our own function for the same\n\ndef PlotBarCharts(inpData, colsToPlot):\n    %matplotlib inline\n    \n    import matplotlib.pyplot as plt\n    \n    # Generating multiple subplots\n    fig, subPlot=plt.subplots(nrows=1, ncols=len(colsToPlot), figsize=(18,5))\n    fig.suptitle('Bar charts of: '+ str(colsToPlot))\n\n    for colName, plotNumber in zip(colsToPlot, range(len(colsToPlot))):\n        inpData.groupby(colName).size().plot(kind='bar',ax=subPlot[plotNumber])\n\n# Calling the function\nPlotBarCharts(inpData=train, colsToPlot=['Pclass', 'Sex','SibSp','Parch','Embarked'])","1028cc16":"# Plotting histograms of multiple columns together\n# Observe that Fare column has outliers\ntrain.hist(['Age', 'Fare'], figsize=(16,4))","660dd3de":"# Grouping by Categorical variable Survived to find the aggregated values\nGroupedData=train.groupby(['Survived'])\n\n# Printing the aggregated values\n#GroupedData.size()\n#GroupedData.sum()\n#GroupedData.count()\nGroupedData.mean()","d2052748":"# Creating the graph with Price\n# You can observice that many people who survived has paid higher fares!\nGroupedData.mean()['Fare'].plot(kind='bar', title='Average Fare for Each Survival Type')\nplt.show()","a8872d8e":"# Box plot for Categorical Variable Survived Vs Continuous Variable Fare\n# Observe the outlier in Fare for Survived=1\nsns.boxplot(x='Survived', y='Fare', data=train)","12b122d5":"sns.boxplot(y='Fare', x='Pclass', data=train)","08700508":"# Create a function to return index of outliers \ndef indicies_of_outliers(x): \n    q1, q3 = np.percentile(x, [25, 75]) \n    iqr = q3 - q1 \n    lower_bound = q1 - (iqr * 1.5) \n    upper_bound = q3 + (iqr * 1.5)\n    #print(upper_bound)\n    \n    return np.where((x > upper_bound) | (x < lower_bound)) ","8a355382":"\n# indicies_of_outliers(train[train['Pclass']==1]['Fare'])[0]\n\nlen(indicies_of_outliers(train[train['Pclass']==1]['Fare'])[0])","7c69bebc":"# Finding those rows where Fare column has outliers\n# All the outlier fares are coming from Pclass=1 which makes sense!\ntrain[train['Fare']>187].head()","6c5bb01a":"# checking the balance of outliers in each category\ntrain[train.index.isin(list(indicies_of_outliers(train[train['Pclass']==1]['Fare'])[0]))]['Survived'].value_counts()","7ab95549":"# Replacing the outlier records of Fare with value 187\n# train['Fare'][train['Fare']>187] = 187","cc0d81c1":"# train.drop(list(indicies_of_outliers(train[train['Pclass']==1]['Fare'])))\n\ntrain.drop(train.index[indicies_of_outliers(train[train['Pclass']==1]['Fare'])[0]], inplace=True)\ntrain.reset_index(inplace = True , drop = True)\ntrain.shape","cba3561d":"# Observing the relationship with Target variable again after Outlier treatment\n# You can see that the distribution has improved now\nsns.boxplot(y='Fare', x='Pclass', data=train)\nplt.show()","576e8119":"# f_oneway() function takes the group data as input and returns F-statistic and P-value\nfrom scipy.stats import f_oneway\n\n# Running the one-way anova test between Fare and Survived\n# Assumption(H0) is that Fare and Survived are NOT correlated with each other\nSurvived_0 = train['Fare'][train['Survived']==0]\nSurvived_1 = train['Fare'][train['Survived']==1]\n\n# Performing the ANOVA test\nAnovaResults = f_oneway(Survived_0, Survived_1)\n\nprint('P-Value for Anova is: ', AnovaResults[1])\n\n# We accept the Assumption(H0) only when P-Value > 0.05\n# Here the P-Value is almost Zero which means we will REJECT the Assumption(H0)\n# This means Fare and Survived ARE correlated with each other","ada8e042":"# Cross tablulation between two categorical variables\nCrossTabResult = pd.crosstab(index=train['Sex'], columns=train['Survived'])\nCrossTabResult","d60be627":"# Visual Inference using Grouped Bar chart\n# Notice that Male Suvival rate is very low as compared to Female\nsns.countplot( x='Sex', hue=\"Survived\", data=train)\nplt.show()","b27df423":"from scipy.stats import chi2_contingency\n\n# Performing Chi-sq test\nChiSqResult = chi2_contingency(CrossTabResult)\n\n# P-Value is the Probability of H0 being True\n# If P-Value>0.05 then only we Accept the assumption(H0)\n# In this case it is way way lower than 0.05 Hence, we reject H0\n# this means the two columns are correlated with each other and Gender of a person affects the Survival\nprint('The P-Value of the ChiSq Test is:', ChiSqResult[1])","3f3bfb6e":"cont_list=[\"SibSp\", \"Parch\", \"Age\", \"Fare\", \"Survived\"]\n\nsns.heatmap(train[cont_list].corr(), annot=True, fmt =\".2f\")\nplt.show()","7d2c36fc":"# concatenate train and test set\ntrain['TrainTest'] = 1\ntest['TrainTest'] = 0\ndata = pd.concat([train,test])\ndata.reset_index(inplace = True , drop = True)","4a858d1a":"# Drop useless columns\n# Remove those variables from data which have too many missing values (Missing Values > 30%)\n# Remove Qualitative variables which cannot be used in Machine Learning\ndata.drop(['Name','Ticket','Cabin'], axis=1, inplace=True)","f839a5c1":"#check for any missing values\nprint(\"\\n Check for any null values in combined set\")\nprint(pd.isnull(data).sum())","339781e3":"# Missing values imputation\ndata['Age'].fillna(data['Age'].median(skipna=True), inplace = True)\ndata['Embarked'].fillna(data['Embarked'].mode()[0], inplace = True)\ndata['Fare'].fillna(data['Fare'].median(skipna=True), inplace = True)","ed99cc90":"## Create categorical variable for traveling alone\ndata['TravelBuds']=data[\"SibSp\"]+data[\"Parch\"]\ndata['TravelAlone']=np.where(data['TravelBuds']>0, 0, 1)\n\ndata.drop('SibSp', axis=1, inplace=True)\ndata.drop('Parch', axis=1, inplace=True)\ndata.drop('TravelBuds', axis=1, inplace=True)","937bff4c":"# Converting Categorical Features\n# Treating all the nominal variables at once using dummy variables\n# data_Numeric=pd.get_dummies(data,drop_first=True)\n# data_Numeric = pd.get_dummies(data)\n# data_Numeric = pd.get_dummies(data, columns=[\"Pclass\"])\ndata_Numeric = pd.get_dummies(data, columns=[\"Sex\",\"Embarked\",\"Pclass\"],drop_first=True)\ndata_Numeric.shape","70a41cb8":"data_Numeric.head()","6e79cdee":"# Rearrange columns\n# data_Numeric.columns.tolist()\ndata_Numeric = data_Numeric[['PassengerId','Age','Fare','TravelAlone','Sex_male','Embarked_Q',\n 'Embarked_S','Pclass_2','Pclass_3','Survived','TrainTest']]","ced87290":"# Standardize the Variables\nscaler = StandardScaler()\n\ndata_Numeric.iloc[:,1:-2] = scaler.fit_transform(data_Numeric.iloc[:,1:-2])\ndata_Numeric.head()","0a410521":"# After data preprocess, again split the data into train and test set\ntrain = data_Numeric[data_Numeric['TrainTest'] == 1]\ntest = data_Numeric[data_Numeric['TrainTest'] == 0]\n\ntrain = train.drop(['PassengerId','TrainTest'], axis=1)\ntest = test.drop(['Survived','TrainTest'], axis=1)\n\nprint(train.shape, \"  \", test.shape)","8bcb1861":"X = train.drop('Survived', axis=1)\ny = train['Survived']","f96eef9a":"# Split the data into training and testing set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=101)\n\n# Quick sanity check with the shapes of Training and testing datasets\nprint(\"X_train - \",X_train.shape)\nprint(\"y_train - \",y_train.shape)\nprint(\"X_test - \",X_test.shape)\nprint(\"y_test - \",y_test.shape)","124fd036":"model_lst = []\naccuracy_lst = []","f051c104":"# Logistic Regression\nmodel_lst.append('Logistic Regression')\n\nlr = LogisticRegression(C=1,penalty='l2', solver='liblinear')\nlr.fit(X_train, y_train)\ny_pred_log_reg = lr.predict(X_test)\n\n# calculate accuracy\nacc_log_reg = round( metrics.accuracy_score(y_pred_log_reg , y_test) * 100, 2)\naccuracy_lst.append(acc_log_reg)\n\n# calculate auc\nAucs = metrics.roc_auc_score(y_test , y_pred_log_reg)\n\n# calculate precision\nPrecisionScore = metrics.precision_score(y_test , y_pred_log_reg)\n\n# calculate recall\nRecallScore = metrics.recall_score(y_test , y_pred_log_reg)\n\n# calculate f1 score\nF1Score = metrics.f1_score(y_test , y_pred_log_reg)\n\n# draw confusion matrix\ncnf_matrix = metrics.confusion_matrix(y_test , y_pred_log_reg)\n\nprint(\"Model Name : Logistic Regression\")\nprint('Accuracy :{0:0.2f} %'.format(acc_log_reg)) \nprint('AUC : {0:0.2f}'.format(Aucs))\nprint('Precision : {0:0.2f}'.format(PrecisionScore))\nprint('Recall : {0:0.2f}'.format(RecallScore))\nprint('F1 : {0:0.2f}'.format(F1Score))\nprint('Confusion Matrix : \\n', cnf_matrix)\nprint(\"\\n\")","1a060869":"# Support Vector Classifier\nmodel_lst.append('Support Vector Classifier')\n\nsvc = SVC()\nsvc.fit(X_train, y_train)\ny_pred_svc = svc.predict(X_test)\n\n# calculate accuracy\nacc_svc = round( metrics.accuracy_score(y_pred_svc , y_test) * 100, 2)\naccuracy_lst.append(acc_svc)\n\nprint(\"Model Name : SVC\")\nprint('Accuracy :{0:0.2f} %'.format(acc_svc)) ","7b9420fa":"# Linear Support Vector Classifier\nmodel_lst.append('Linear Support Vector Classifier')\n\nlsvc = LinearSVC()\nlsvc.fit(X_train, y_train)\ny_pred_linear_svc = lsvc.predict(X_test)\n\n# calculate accuracy\nacc_linear_svc = round( metrics.accuracy_score(y_pred_linear_svc , y_test) * 100, 2)\naccuracy_lst.append(acc_linear_svc)\n\nprint(\"Model Name : Linear SVC\")\nprint('Accuracy :{0:0.2f} %'.format(acc_linear_svc)) ","2e4242a1":"# K-Nearest Neighbors\nmodel_lst.append('K-Nearest Neighbors')\n                 \nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, y_train)\ny_pred_knn = knn.predict(X_test)\n\n# calculate accuracy\nacc_knn = round( metrics.accuracy_score(y_pred_knn , y_test) * 100, 2)\naccuracy_lst.append(acc_knn)\n\nprint(\"Model Name : K Neighbors Classifier\")\nprint('Accuracy :{0:0.2f} %'.format(acc_knn)) ","d1aca09a":"# Decision Tree\nmodel_lst.append('Decision Tree')\n                 \ndt = DecisionTreeClassifier()\ndt.fit(X_train, y_train)\ny_pred_decision_tree = dt.predict(X_test)\n\n# calculate accuracy\nacc_decision_tree = round( metrics.accuracy_score(y_pred_decision_tree , y_test) * 100, 2)\naccuracy_lst.append(acc_decision_tree)\n\nprint(\"Model Name : Decision Tree Classifier\")\nprint('Accuracy :{0:0.2f} %'.format(acc_decision_tree)) ","b955cf12":"# Random Forest\nmodel_lst.append('Random Forest')\n\nrf = RandomForestClassifier(n_estimators=100)\nrf.fit(X_train, y_train)\ny_pred_random_forest = rf.predict(X_test)\n\n# calculate accuracy\nacc_random_forest = round( metrics.accuracy_score(y_pred_random_forest , y_test) * 100, 2)\naccuracy_lst.append(acc_random_forest)\n\nprint(\"Model Name : Random Forest Classifier\")\nprint('Accuracy :{0:0.2f} %'.format(acc_random_forest)) ","3569c71f":"# Gaussian Naive Bayes\nmodel_lst.append('Gaussian Naive Bayes')\n                 \ngnb = GaussianNB()\ngnb.fit(X_train, y_train)\ny_pred_gnb = gnb.predict(X_test)\n\n# calculate accuracy\nacc_gnb = round( metrics.accuracy_score(y_pred_gnb , y_test) * 100, 2)\naccuracy_lst.append(acc_gnb)\n\nprint(\"Model Name : Gaussian NB\")\nprint('Accuracy :{0:0.2f} %'.format(acc_gnb)) ","a6db5a1c":"# Perceptron\nmodel_lst.append('Perceptron')\n                 \nprct = Perceptron(max_iter=5, tol=None)\nprct.fit(X_train, y_train)\ny_pred_perceptron = prct.predict(X_test)\n\n# calculate accuracy\nacc_perceptron = round( metrics.accuracy_score(y_pred_perceptron , y_test) * 100, 2)\naccuracy_lst.append(acc_perceptron)\n\nprint(\"Model Name : Perceptron\")\nprint('Accuracy :{0:0.2f} %'.format(acc_perceptron)) ","c468b587":"# Stochastic Gradient Descent\nmodel_lst.append('Stochastic Gradient Descent')\n\nsgd = SGDClassifier(max_iter=5, tol=None)\nsgd.fit(X_train, y_train)\ny_pred_sgd = sgd.predict(X_test)\n\n# calculate accuracy\nacc_sgd = round( metrics.accuracy_score(y_pred_sgd , y_test) * 100, 2)\naccuracy_lst.append(acc_sgd)\n\nprint(\"Model Name : SGD Classifier\")\nprint('Accuracy :{0:0.2f} %'.format(acc_sgd)) ","6b769cc7":"# Gradient Boosting Classifier\nmodel_lst.append('Gradient Boosting Classifier')\n\ngbk = GradientBoostingClassifier()\ngbk.fit(X_train, y_train)\ny_pred_gbk = gbk.predict(X_test)\n\n\n# calculate accuracy\nacc_gbk = round( metrics.accuracy_score(y_pred_gbk , y_test) * 100, 2)\naccuracy_lst.append(acc_gbk)\n\nprint(\"Model Name : GB Classifier\")\nprint('Accuracy :{0:0.2f} %'.format(acc_gbk)) ","10e6c9c4":"# Xtreme Gradient Boosting (XGBoost)\nmodel_lst.append('Xtreme Gradient Boosting')\n\nxgb=XGBClassifier(max_depth=2,learning_rate=0.01,n_estimators=400,objective='binary:logistic',booster='gbtree')\nxgb.fit(X_train, y_train)\ny_pred_xgb = xgb.predict(X_test)\n\n# calculate accuracy\nacc_xgb = round( metrics.accuracy_score(y_pred_xgb , y_test) * 100, 2)\naccuracy_lst.append(acc_xgb)\n\nprint(\"Model Name : XGBoost Classifier\")\nprint('Accuracy :{0:0.2f} %'.format(acc_xgb)) ","c1fd9096":"# Performance measures of various classifiers\ndata = {'Models':model_lst,\n       'Accuracy':accuracy_lst}\n\nprint(\"Performance measures of various classifiers: \\n\")\nmodels = pd.DataFrame(data) \nmodels.sort_values(['Accuracy'],ascending=False)","34c2522b":"sns.barplot(y='Models', x='Accuracy', data=models.sort_values(['Accuracy'],ascending=False))\nplt.show()","cf55e2f2":"# Creating Submission File\n\n#set ids as PassengerId and predict survival \nids = test['PassengerId']\npredictions = gbk.predict(test.drop('PassengerId', axis=1)).astype(int)\n\n\n#set the output as a dataframe and convert to csv file named submission.csv\nsubmission = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\nsubmission.head()","d9aa1730":"submission.to_csv('submission.csv', header=True, index=False)","96a8bbe0":"### Data Pre-Processing","bcd6da0a":"### Data Analysis","1c71a831":"###  Load Data","c3783728":"##### Visual exploration of Relationship : Continuous Vs Continuous using Scatter Plot\nWhen the target variable is Continuous and the predictor is also Continuous then we explore the correlation between them  visually using scatter plots and statistically using Correlation test.","c7bbd74f":"##### Visual exploration of Relationship : Categorical Vs Categorical using Bar Plot\nWhen the target variable is Categorical and the predictor is also Categorical then we explore the correlation between them  visually using barplots and statistically using Chi-square test","c8454925":"##### Visual exploration of Relationship : Categorical Vs Continuous using Bar Plot, Box Plot\nWhen the target variable is Categorical and the predictor variable is Continuous we analyze the relation using bar plots\/Boxplots and measure the strength of relation using Anova test","cbb17eb3":"### Distribution of all numeric Predictor","c5a48db4":"### Relationship between variables\n\n#### Visual exploration of relationship between variables\n* Continuous Vs Continuous ---- Scatter Plot\n* Categorical Vs Continuous---- Bar Plot, Box Plot\n* Categorical Vs Categorical---- Bar Plot\n\n#### Statistical measurement of relationship between variables\n* Continuous Vs Continuous ---- Correlation matrix\n* Categorical Vs Continuous---- ANOVA test\n* Categorical Vs Categorical--- Chi-Square test","85a3d887":"#### Start observing the Quantitative\/Categorical\/Qualitative variables","94a0824f":"##### Statistical exploration of Relationship : Categorical Vs Continuous using ANOVA test\n\nAnalysis of variance(ANOVA) is performed to check if there is any relationship between the given continuous and categorical variable\n* Assumption(H0): There is NO relation between the given variables (i.e. The average(mean) values of the numeric Predictor variable is same for all the groups in the categorical Target variable)\n* ANOVA Test result: Probability of H0 being true","9fa97cb2":"### Import Necessary Libraries","f21f3586":"### Distribution of categorical Predictor variables\nWe can spot a categorical variable in the data by looking at the unique values in them. Typically a categorical variable contains less than 20 Unique values AND there is repetition of values, which means the data can be grouped by those unique values.","d3722914":"##### Statistical exploration of Relationship : Categorical Vs Categorical using Chi-Square Test\n\nChi-Square test is conducted to check the correlation between two categorical variables\n\n* Assumption(H0): The two columns are NOT related to each other\n* Result of Chi-Sq Test: The Probability of H0 being True\n* More information on ChiSq: https:\/\/www.mathsisfun.com\/data\/chi-square-test.html","c09b4a01":"#### Looking at the variances in Fare by each Survival type in box plot\nIF the distribution of each type is similar, it gives a hint that there is no correlation Between the categorical and the numeric variable","1ecb4d79":"#### Outlier Treatment\n\n* Option-1: Delete the outlier Records\n* Option-2: Impute the outlier values with a logical business value\n* Below we are Finding out the most logical value to be replaced in place of outliers by looking at the box-plot of Fare Vs Pclass","78786be1":"### Build Various Models & choose best one"}}