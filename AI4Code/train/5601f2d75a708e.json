{"cell_type":{"ffc4c43e":"code","ed90d2ce":"code","65b09b0d":"code","df1f19a4":"code","91f78d93":"code","9c717ed4":"code","f999c3d8":"code","b39ec7d3":"code","461b9f93":"code","6566949c":"code","e698caad":"code","61940d42":"code","47c2219e":"code","63b929d8":"code","767887dd":"code","4dd3e8a1":"code","4bf2d70f":"code","f7866679":"code","7d0c13ef":"code","d30e4837":"code","aa62630d":"code","2136bbea":"code","e2cb1f7c":"code","fc8b9bd5":"code","89ab4aad":"code","c1ef8edd":"code","bf1be1f7":"code","04f307d1":"code","a328a7a7":"code","2ed7e23d":"code","15c8a2f9":"code","06c5358e":"code","be7bca98":"code","3268b8af":"code","48bc7dcc":"code","36d9363d":"code","cb3b22f5":"markdown","0ba46084":"markdown","fff72422":"markdown","191ba30b":"markdown","08546ecc":"markdown","72dc93e6":"markdown","b32296f8":"markdown","b0f01a53":"markdown","a3e3b2ab":"markdown","3834224b":"markdown","ca6c31ca":"markdown","49c3ff45":"markdown","2c1138da":"markdown","c54dd67f":"markdown","35a2f265":"markdown","5db2dc92":"markdown","90ef1ef3":"markdown","d1e3ba53":"markdown","786237af":"markdown","a36a486f":"markdown","049c1cdb":"markdown","934a38ef":"markdown","dbfe2b71":"markdown","3885cc0d":"markdown","9ceef6f1":"markdown","062478ab":"markdown","1832a68f":"markdown","66c9261e":"markdown","7951bf74":"markdown","124df322":"markdown","7420d2a4":"markdown","5e23fe98":"markdown","693c77cd":"markdown","41b9147f":"markdown","84cf254c":"markdown","5be0284c":"markdown","b810de6d":"markdown","b9700a5b":"markdown","ef4b0617":"markdown","1daf1cc9":"markdown","f69110aa":"markdown","84830aef":"markdown","e89b8af2":"markdown","6a580a31":"markdown","8ab8502c":"markdown","e763ef28":"markdown","f53f1de0":"markdown","e9bc9090":"markdown"},"source":{"ffc4c43e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nmpl.style.use('ggplot')\nimport random","ed90d2ce":"df = pd.read_csv(\"..\/input\/boston-ds\/crime.csv\", index_col = None, encoding='windows-1252', parse_dates = ['OCCURRED_ON_DATE', 'YEAR', 'DAY_OF_WEEK'], engine='python')\ndf.head()","65b09b0d":"print(f'The dataset contains %s rows and %s columns' % (df.shape[0],df.shape[1]), '\\n')\nprint('The columns and the its values types:\\n')\ndf.info()","df1f19a4":"codes = pd.read_csv(\"..\/input\/boston-ds\/offense_codes.csv\", index_col = None, encoding='windows-1252', engine='python')\ncodes.CODE.value_counts() #This line is for checking of whether codes are unique or not\ncodes.drop_duplicates(subset=['CODE'], keep='first', inplace=True) #Since there are duplicates, let's drop them","91f78d93":"codes.head()","9c717ed4":"print(f'The dataset contains %s rows and %s columns' % (codes.shape[0],codes.shape[1]), '\\n')\nprint('The columns and the its values types:\\n')\ncodes.info()","f999c3d8":"top = df.OFFENSE_CODE.value_counts().to_frame().reset_index(level=0)","b39ec7d3":"top.columns.values[0] = 'CODE'\ntop.columns.values[1] = 'TOTAL_AMOUNT'\ntop.head(5)","461b9f93":"code_top = top.merge(codes, on='CODE', how = 'left')","6566949c":"code_top.head(10)","e698caad":"gr = df.loc[:,['OFFENSE_CODE','OFFENSE_CODE_GROUP']]\ngr.info()\ncode_tg = pd.merge(code_top, gr, left_on='CODE', right_on='OFFENSE_CODE', how='inner')\ncode_tg.drop_duplicates(subset=['CODE'], keep='first', inplace=True)\ncode_tg.reset_index(drop=True,inplace=True)\ncode_tg.head(5)","61940d42":"code_tg.head(20).plot(kind = 'barh', x = 'NAME', y = 'TOTAL_AMOUNT', figsize=(12, 12))\n\nplt.gca().invert_yaxis() \n\nplt.xlabel('Number of Reports')\nplt.ylabel('Offense Type')\ndf.sort_values(['OCCURRED_ON_DATE'], ascending=True, inplace=True)\nplt.title('Boston Offense Rating: ' + str(df.OCCURRED_ON_DATE.dt.date.iloc[0]) + ' : ' + str(df.OCCURRED_ON_DATE.dt.date.iloc[-1]))\n\n# This loop automatically add the value of each position to the each bar:\nfor index, value in enumerate(code_tg.head(20)['TOTAL_AMOUNT']):\n    label = format(int(value), ',')\n    plt.annotate(label, xy=(value - 100, index + 0.10), ha='right', color='white')","47c2219e":"top_gr = code_tg.groupby(['OFFENSE_CODE_GROUP'], as_index=False).sum(axis=1)\ntop_gr = top_gr[['OFFENSE_CODE_GROUP','TOTAL_AMOUNT']].sort_values(['TOTAL_AMOUNT'], ascending=False).reset_index(drop=True)","63b929d8":"top_gr.head(10)","767887dd":"top_gr.head(10).plot(kind = 'barh', x = 'OFFENSE_CODE_GROUP', y = 'TOTAL_AMOUNT', figsize=(12, 5))\n\nplt.gca().invert_yaxis() \n\nplt.xlabel('Number of Reports')\nplt.ylabel('Offense Group')\nplt.title('Boston Offense Groups Rating: ' + str(df.OCCURRED_ON_DATE.dt.date.iloc[0]) + ' : ' + str(df.OCCURRED_ON_DATE.dt.date.iloc[-1]))\n\n\n# This loop automatically add the value of each position to the each bar:\nfor index, value in enumerate(top_gr.head(10)['TOTAL_AMOUNT']):\n    label = format(int(value), ',')\n    plt.annotate(label, xy=(value - 300, index + 0.13), ha='right', color='white')","4dd3e8a1":"shtng = df[(df.SHOOTING == 'Y') & (df.DISTRICT.notnull())]","4bf2d70f":"import folium\nimport folium.plugins as plugins\n\nlatitude = list(shtng.Lat)[1] # This is to initiate the latitude start point for the map\nlongitude = list(shtng.Long)[1] # This is to initiate the longitude start point for the map\n\nlatitudes = list(shtng.Lat) #create the list of all reported latitudes\nlongitudes = list(shtng.Long) #create the list of all reported longitudes\n\nshooting_map = folium.Map(location = [latitude, longitude], zoom_start = 12) # instantiate a folium.map object\n\nshooting = plugins.MarkerCluster().add_to(shooting_map) # instantiate a mark cluster object for the incidents in the dataframe\n\n# loop through the dataframe and add each data point to the mark cluster\nfor lat, lng, label, in zip(shtng.Lat, shtng.Long, shtng.DISTRICT):\n    if (not np.isnan(lat)) & (not np.isnan(lng)): # also, we check a non-nullness of the coordinates\n        folium.Marker(\n            location=[lat, lng],\n#             icon=None,\n            popup=label,\n            icon=folium.Icon(icon='exclamation-sign')\n        ).add_to(shooting)\n\n# display the map\nshooting_map","f7866679":"# re-assemble the dataset for the more convenient plotting process\ntop_sh = shtng.DISTRICT.value_counts().to_frame().reset_index(level=0)\ntop_sh.columns.values[0] = 'DISTRICT'\ntop_sh.columns.values[1] = 'NUMBER'\ntop_sh.plot(kind = 'barh', x = 'DISTRICT', y = 'NUMBER', figsize=(12, 7))\n\n# invert y-axis\nplt.gca().invert_yaxis()\n\n# Name axis and title\nplt.xlabel('Number of Shooting Reports')\nplt.ylabel('Districts')\n\nplt.title('Boston Top Shooting Districts: ' + str(df.OCCURRED_ON_DATE.dt.date.iloc[0]) + ' : ' + str(df.OCCURRED_ON_DATE.dt.date.iloc[-1]))\n\n# Lop for values plotting\nfor index, value in enumerate(top_sh['NUMBER']):\n    label = format(int(value), ',')\n    plt.annotate(label, xy=(value - 1, index + 0.11),\n                 ha='right', \n                 color='white'\n                )\n    \n# Loop for arrows plotting. Notice that the arrowhead will always point on the bottom-right bar's corner.\n# Also, here I separately defined a starting arrows' point to maximize the procedural plotting \nxy_label = (250,5)\nfor index, value in enumerate(top_sh['NUMBER']):\n    plt.annotate('',\n             xy=(value, index + 0.3),\n             xytext=xy_label,\n             xycoords='data',\n             arrowprops=dict(arrowstyle='fancy ,head_length=0.4,head_width=0.4,tail_width=0.2',\n                             connectionstyle='arc3', \n                             color='xkcd:blue', \n                             lw=2\n                            )\n            )\n    if index == 2: # We want to plot only top 3 the most shooting districts, so we need to interrupt the loop here.\n        break\n\n# This dictionary I built using googling method.\ndict0 = {'C11' : 'DORCHESTER', 'B3' : 'MATTAPAN', 'B2' : 'ROXBURY'} \n\n# Plot the district name decoding it using our dictionary.\nfor index, value in enumerate(top_sh['NUMBER']):\n    v = top_sh.loc[top_sh['NUMBER']==value]['DISTRICT'].astype('str')\n    plt.annotate('[ ' + dict0[v[index]] + ' ]',\n             xy=(value - 15, index + 0.13),\n             rotation=0,\n             va='bottom',\n             ha='right',\n             color = 'white'\n            )\n    if index == 2:\n        break\n        \n# Plot the annotation text. Here I used xy_label defined earlier for automation.\nplt.annotate('The Most Shooting Districts', # text to display\n             xy=(xy_label[0],xy_label[1] + 0.5),\n             rotation=0,\n             va='bottom',\n             ha='center',\n            )\n        \nplt.show()","7d0c13ef":"shooting_hmap = folium.Map(location=[df.Lat[100],df.Long[100]], \n                       tiles = \"Stamen Toner\",\n                      zoom_start = 12)\n\nfrom folium.plugins import HeatMap   \n\nhm = df.loc[:,['Lat','Long','SHOOTING']]\nhm.dropna(axis=0, inplace=True)\nhlimit = hm.shape[0]\nhm = hm.sample(hlimit)\nhdata = []\nfor ln, lt in zip(hm.Lat, hm.Long):\n    hdata.append((ln,lt))\nHeatMap(hdata, \n        gradient = {0.01: 'blue', 0.15: 'lime', 0.25: 'red'},\n        blur = 15,\n        radius=5).add_to(shooting_hmap)\n\nshooting_hmap","d30e4837":"shtngh = df[(df.SHOOTING == 'Y') & (df.HOUR.notnull())]\nshtngh.sort_values(['OCCURRED_ON_DATE'], ascending=True, inplace=True)\nshtngh.head()","aa62630d":"# plot the histogram\nplt.figure(figsize=(9, 5))\nplt.hist(shtngh.HOUR, bins=range(24))\nplt.title('Shooting Time Distribution in Boston: ' + str(shtngh.OCCURRED_ON_DATE.dt.date.iloc[0]) + ' : ' + str(shtngh.OCCURRED_ON_DATE.dt.date.iloc[-1]))\nplt.xticks(range(24))\n\n# Decrease Arrow\nplt.annotate('',\n             xy=(10, 25), # Arrow head\n             xytext=(1, 120), # Starting point\n             xycoords='data', # Use the coordinate system of the object being annotated \n             arrowprops=dict(arrowstyle='fancy ,head_length=0.4,head_width=0.4,tail_width=0.2', connectionstyle='angle3, angleA=110,angleB=0', color='xkcd:blue', lw=2)\n            ) # Arrow props\n\n# After Midday Arrow\nplt.annotate('',\n             xy=(16, 85),\n             xytext=(15, 100),\n             xycoords='data',\n             arrowprops=dict(arrowstyle='fancy ,head_length=0.4,head_width=0.4,tail_width=0.2', connectionstyle='arc3', color='xkcd:blue', lw=2)\n            )\n\n# Latenight Madness Arrow\nplt.annotate('',\n             xy=(21.5, 190),\n             xytext=(20, 65),\n             xycoords='data',\n             arrowprops=dict(arrowstyle='fancy ,head_length=0.4,head_width=0.4,tail_width=0.2', connectionstyle='arc3', color='xkcd:blue', lw=2)\n            )\n\n# Annotate Text\nplt.annotate('Gradual decrease ',\n             xy=(2.5, 38),\n             rotation=-40,\n             va='bottom',\n             ha='left',\n            )\n\n# Annotate Text\nplt.annotate('After midday peak', # text to display\n             xy=(15, 100),\n             rotation=0,\n             va='bottom',\n             ha='right',\n            )\n\n# Annotate Text\nplt.annotate('Latenight madness',\n             xy=(19.5, 90),\n             rotation=79,\n             va='bottom',\n             ha='left',    \n            )\n\nplt.show()","2136bbea":"vd = df[(df.OFFENSE_CODE_GROUP == 'Verbal Disputes') & (df.HOUR.notnull())]\nvd.sort_values(['OCCURRED_ON_DATE'], ascending=True, inplace=True)\nvd.shape","e2cb1f7c":"plt.figure(figsize=(9, 5))\nplt.hist(vd.HOUR, bins=range(24))\nplt.title('Verbal Disputes Rate in Boston:'  + str(vd.OCCURRED_ON_DATE.dt.date.iloc[0]) + ' : ' + str(vd.OCCURRED_ON_DATE.dt.date.iloc[-1]))\nplt.xticks(range(24))\nplt.annotate('',\n             xy=(21.5, 1400),\n             xytext=(5, 200),\n             xycoords='data',\n             arrowprops=dict(arrowstyle='->', connectionstyle='arc, angleA=90, angleB=-95, armA=40, armB=60, rad=45.0', color='xkcd:blue', lw=2)\n            )\nplt.annotate('Verbal disputes gradually increases', # text to display\n             xy=(3, 1200),\n             rotation=0,\n             va='bottom',\n             ha='left',\n            )\nplt.annotate('all day long and reaches its peak', # text to display\n             xy=(3, 1100),\n             rotation=0,\n             va='bottom',\n             ha='left'\n            )\nplt.annotate('at midnight', # text to display\n             xy=(3, 1000),\n             rotation=0,\n             va='bottom',\n             ha='left',\n            )\nplt.show()","fc8b9bd5":"pred = df.loc[:,['SHOOTING', 'OFFENSE_CODE_GROUP']]\npred.replace(np.nan, 0, inplace=True)\npred.replace('Y', 1, inplace=True)\ngroups_dummy = pd.get_dummies(pred['OFFENSE_CODE_GROUP'])\npred = pd.concat([pred,groups_dummy], axis=1)\npred.drop(['OFFENSE_CODE_GROUP'], inplace=True, axis=1)","89ab4aad":"y = pred[['SHOOTING']]\nX = pred.iloc[:,1:]\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nLR = LogisticRegression(C=0.01, solver='liblinear').fit(x_train,y_train)\nLR","c1ef8edd":"yhat = LR.predict(x_test)\nyhat","bf1be1f7":"# Evaluation using Jaccard Index\nfrom sklearn.metrics import jaccard_similarity_score\njaccard_similarity_score(y_test, yhat)","04f307d1":"yhat_prob = LR.predict_proba(x_test)\nyhat_prob","a328a7a7":"from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(y_test, yhat, labels=[1,0]))","2ed7e23d":"from sklearn.metrics import log_loss\nlog_loss(y_test, yhat_prob)","15c8a2f9":"feature_importance=pd.concat([pd.DataFrame(X.columns), pd.DataFrame(LR.coef_.T)], axis = 1)\nfeature_importance.columns = ['features', 'importance']\nfeature_importance.sort_values(['importance'], ascending=False, inplace=True)\nfeature_importance = feature_importance.reset_index(drop=True)\nfeature_importance.head(8)","06c5358e":"shtng_gr = shtng.loc[:,['OCCURRED_ON_DATE']]\nshtng_gr['Amount'] = 1\nshtng_gr['Date'] = pd.DatetimeIndex(shtng_gr.OCCURRED_ON_DATE).normalize()\nshtng_gr.drop(['OCCURRED_ON_DATE'], axis=1, inplace=True)\nshtng_gr['YM'] = pd.to_datetime(shtng_gr[\"Date\"], format='%Y00%m').apply(lambda x: x.strftime('%Y-%m'))\nshtng_gr['YM'] = pd.to_datetime(shtng_gr[\"YM\"])\nshtng_gr.drop(['Date'], axis=1, inplace=True)\nshtng_gr = shtng_gr.groupby(['YM'], as_index=False).sum()\nshtng_gr.reset_index(drop=False, inplace=True)\nshtng_gr.head()","be7bca98":"from numpy.polynomial.polynomial import polyfit\npx = np.asarray(shtng_gr.index)\nb, m = polyfit(shtng_gr.index, shtng_gr.Amount, 1)\nshtng_gr.plot(kind='scatter',x='index', y='Amount', rot='90', figsize=(10, 6), alpha = 1, c='xkcd:salmon')\nplt.plot(px, b + m * px, '-', c='xkcd:blue')\nplt.xticks(shtng_gr.index, shtng_gr.YM.dt.date, rotation=90)\nplt.ylabel('Amount of Shooting Reports \/ Month')\nplt.xlabel('Months')\nplt.title('Total Shooting Reports in Boston: ' + str(shtng_gr.YM.dt.date.iloc[0]) + ' : ' + str(shtng_gr.YM.dt.date.iloc[-1]))\n\nplt.annotate('Regression Line : Insignificant growth',                      # s: str. Will leave it blank for no text\n             xy=(20, 27),             # place head of the arrow at point (year 2012 , pop 70)\n             xytext=(12, 46),         # place base of the arrow at point (year 2008 , pop 20)\n             xycoords='data',         # will use the coordinate system of the object being annotated \n             arrowprops=dict(arrowstyle='fancy ,head_length=0.4,head_width=0.4,tail_width=0.2', connectionstyle='arc3', color='xkcd:blue', lw=2)\n            )\n\nplt.show()","3268b8af":"ma = df[(df.OFFENSE_CODE_GROUP == 'Medical Assistance') & (df.OCCURRED_ON_DATE.notnull())]","48bc7dcc":"ma_gr = ma.loc[:,['OCCURRED_ON_DATE']]\nma_gr['Amount'] = 1\nma_gr['Date'] = pd.DatetimeIndex(ma_gr.OCCURRED_ON_DATE).normalize()\nma_gr.drop(['OCCURRED_ON_DATE'], axis=1, inplace=True)\nma_gr['YM'] = pd.to_datetime(ma_gr[\"Date\"], format='%Y00%m').apply(lambda x: x.strftime('%Y-%m'))\nma_gr['YM'] = pd.to_datetime(ma_gr[\"YM\"])\nma_gr.drop(['Date'], axis=1, inplace=True)\nma_gr = ma_gr.groupby(['YM'], as_index=False).sum()\nma_gr.reset_index(drop=False, inplace=True)\nma_gr.head()","36d9363d":"import seaborn as sns\nplt.figure(figsize=(15, 10))\nax = sns.regplot(x='index', y='Amount', data=ma_gr, color='green', marker='+', scatter_kws={'s': 50,'color':'xkcd:salmon', 'alpha' : 1})\n\nax.set(xlabel='Months', ylabel='Amount of Medical Assistance Reports \/ Month')\nax.set_title('Total Medical Assistance Reports in Boston: ' + str(ma_gr.YM.dt.date.iloc[0]) + ' : ' + str(ma_gr.YM.dt.date.iloc[-1]))\nax.set_ylim(200)\nplt.xticks(ma_gr.index, ma_gr.YM.dt.date, rotation=90)\n\nplt.annotate('Steady Growth',\n             xy=(25, 600),\n             xytext=(12, 350),\n             xycoords='data',\n             arrowprops=dict(arrowstyle='fancy ,head_length=0.4,head_width=0.4,tail_width=0.2', connectionstyle='arc3', color='xkcd:salmon', lw=2)\n            )\n\n\nplt.show()","cb3b22f5":"What I see here is a clear uptrend. There is more than a 50% increase in 'the Medical Assistance reports' in just 3 years.\n<br>However, to investigate the reasons for this we need other datasets.","0ba46084":"Now, I want to plot just a regression line to see the trend briefly","fff72422":"Now let's see the probalility of the classes:","191ba30b":"Here we see a slightly different picture rather than comparing offense names individually. The most-reported here are incidents related to Motor Vehicle Incidents.","08546ecc":"After that, we can plot the incidents using folium library:","72dc93e6":"Let's take a look on this dataset briefly","b32296f8":"Now I want to range the districts to highlight those where I wouldn't recommend settling.","b0f01a53":"To finalize with folium library, I'd like to show how else we can depict the incidents using a heat map.<br>\nIn this case, all shooting incidents are colored depending on the intensity at any particular area. For the time-saving purpose, I limited the number of points to be pictured.","a3e3b2ab":"And now we are merging <i>top<\/i> dataset with <i>codes<\/i> on <i>'CODE'<\/i> column as a key","3834224b":"<h3>Task2 - Most Shooting Areas<\/h3>\n<br>This part is about the understanding of which boroughs are most dangerous in the meaning of shooting level. The library I used here is <b>folium<\/b> - it works with maps and coordinates, can combine the close locations into groups and many other things","ca6c31ca":"For this purpose, we need to transform our data, parse the date column and leave only year-month value because I don't want to plot everyday-dots.","49c3ff45":"I chose the Logistic Regression model since we need to predict the probability of 0 or 1 and it usually suits perfectly for this purpose.","2c1138da":"Let's bultd a dataset we want to examine:","c54dd67f":"Let's load the offense code dataset which will help us with code decoding","35a2f265":"To diverse the project, I use a seaborn library here to plot a regression line. From my perspective, it makes it more spectacular and informative.","5db2dc92":"Well, usually the shooting follows by the verbal disputes. I want to check, is the correlation between these two events or not.<br>\nFirst of all, I want to check it graphically, using the old good histogram.","90ef1ef3":"Great result, let's find the importance for the each feature.","d1e3ba53":"Firstly, let's extract the rows we are interested in. We need those with a 'Y' shooting mark and the filled district value.","786237af":"After a short time, we can see the map with all reported incidents with the shooting. The most frequently mentioned areas are colored in dark orange. <br>\nAccording to the plot, the most  \"dangerous\" boroughs are situated in the south part of the city, mainly along the railroad.","a36a486f":"Model Development","049c1cdb":"So, the model is trained. It's time to evaluate it.","934a38ef":"The most interesting part of Task1 - visualization! Look how the plot is built in code.<br>\nHere only first 20 codes are pictured.","dbfe2b71":"The dataset we need seems to be relatively small, so we will plot all the observations.<br>\nThis time I use histogram plot, which shows how frequently each particular hour, when the incident with the shooting was reported, appears in the dataset.<br>\nI am going to use different kinds of arrows to point the main features.","3885cc0d":"This is how I plot it:","9ceef6f1":"What I see here is that there is no place for Verbal Disputes! However, even these outcomes do not exclude the relationship between shooting and verbal disputes reported. They just could follow one after another since they are both in different observations.","062478ab":"Outstanding result! Seems like our model fits test data perfectly.","1832a68f":"Let's prepare the data first: we're interested only in the rows where 'Y' shooting mark is and the 'HOUR' values are filled.","66c9261e":"Great! Even though the number of shooting reports experiences slow growth, it still seems like a lateral trend.","7951bf74":"In comparison to the trend above, I would like to show realy disturbing tendency - the tendency in Medical Assistance reports.","124df322":"Data preparation step:","7420d2a4":"What we can conclude from the histogram is:\n1. Nobody shoots at 7 am (too sleepy, probably)\n2. There is a midday shooting peak (too hot?)\n3. And the late-night madness at midnight (everybody has fun)\n4. Then the fun gradually goes down and fades in the early morning.","5e23fe98":"<b>Visualization of Task1<\/b>. <br>\nHere, for the further exploratory purpuse, I added OFFENSE_CODE_GROUP column to our decoded top list.","693c77cd":"Let's find out how many entries there are in our dataset and what type the variables are.","41b9147f":"My theory of that incidents with shooting are preceded by verbal disputes is confirmed graphically.<br>But I need more evidence.<br>\nSuddenly, I've decided to check whether we could build a prediction model that could predict the probability of the shooting using only OFFENSE_CODE_GROUP features.<br>Then we will see which OFFENSE_CODE_GROUP codes are the most related to incidents with shooting.","84cf254c":"<h3>Task3 - Most Dangerous Time in Boston<\/h3>\n<br>To continue the previous chapter, let's consider the question of determining the time of the day when It is better to stay at home. In particular, I want to know, when the chance to catch a bullet is peaked.","5be0284c":"Then, lets' sumup the codes from the main dataset and sort them descending","b810de6d":"In addition, I'd also like to know the top reports rating, grouped by OFFENSE_CODE_GROUP.<br>\nFor this reason we need to make some additional manipulations:","b9700a5b":"It contains over 13k observations so that our data look statistically significant.","ef4b0617":"Here, <b>predict_proba<\/b> returns evaluations for all classes, ordered by the label of the classes. <br>\nSo, the first column is the probability of class 1, P(Y=1|X), and second column is probability of class 0, P(Y=0|X):","1daf1cc9":"Hare are some data manipulations:","f69110aa":"<h3>Task4 - The tendency in the incedents with shooting<\/h3>\n<br>Ok, now let's switch to a slightly different question - I want to know how is everything going with the number of shooting incidents - is it increasing or decreasing over the recent years?","84830aef":"Let's take a look at the first five items in our dataset.","e89b8af2":"<h3>Task1 - Top Offense Code<\/h3>\n<br>Here I am investigating the most \"popular\" report registered. This will help us to better understand the structure and dataset in general. Additionally, this will be useful for our further discoveries.","6a580a31":"<h3>Import Libraries & Data<\/h3>","8ab8502c":"Brief observation of that everything went smoothly:","e763ef28":"This is how confusion matrix looks like. All wee need to do here is to measure Log Loss for this model. The lower value, the better model.","f53f1de0":"Now let's plot it!","e9bc9090":"The figure above clearly shows that Roxbury, Mattapan and Dorchester are the most restless districts."}}