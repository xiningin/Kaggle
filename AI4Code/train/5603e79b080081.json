{"cell_type":{"f398737c":"code","e8f3c928":"code","3a8da689":"code","007c8623":"code","6089d8d6":"code","e7a18ac8":"code","11aea236":"code","9cc5f0ef":"code","512faae2":"code","1127a4e8":"code","b498b47d":"code","f69b7b28":"code","c8f2ae53":"code","2bb8b5d1":"code","3d80da7f":"code","024a0416":"code","f377d5e9":"code","59d7873e":"code","9568b34c":"markdown","155a199d":"markdown","aaf69b8b":"markdown","37d3c3f5":"markdown","4ae0f3a7":"markdown","03357b56":"markdown","9b76c450":"markdown","1907af38":"markdown","ff46e9cd":"markdown","60330a9a":"markdown","737e037f":"markdown","eb13992d":"markdown","97301805":"markdown","16ca5992":"markdown","8c5fb69c":"markdown","a81da648":"markdown","6526ed74":"markdown","479ca01d":"markdown","843842ea":"markdown","e4f86095":"markdown","cf90bcb2":"markdown","a9e1a172":"markdown","2bd62cf4":"markdown","3d816f39":"markdown","7c6374d4":"markdown","10b648b4":"markdown","bb1dd772":"markdown","6f593ab3":"markdown"},"source":{"f398737c":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import XGBRegressor\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#ignore annoying warning (from sklearn and seaborn)\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn ","e8f3c928":"data = pd.read_csv('..\/input\/feature-engineering-house-rent-prediction\/\/data_cleaned.csv')\ndata.shape","3a8da689":"data.head()","007c8623":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n                                    data.drop('price',axis=1),\n                                    data['price'],\n                                    test_size=0.25,\n                                    random_state=0)","6089d8d6":"y = data['price']\nX = data.drop(['price'], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=101)","e7a18ac8":"sc_X = StandardScaler()\n\nX2_train = sc_X.fit_transform(X_train)\nX2_test = sc_X.fit_transform(X_test)\ny2_train = y_train\ny2_test = y_test","11aea236":"lm1 = LinearRegression()\nlm1.fit(X_train,y_train)\n\nlm2 = LinearRegression()\nlm2.fit(X2_train,y2_train)","9cc5f0ef":"lm1_pred = lm1.predict(X_test)\nlm2_pred = lm2.predict(X2_test)","512faae2":"print('Linear Regression Performance:')\n\nprint('\\nall features, No scaling:')\nprint('MAE:', metrics.mean_absolute_error(y_test, lm1_pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, lm1_pred)))\nprint('R2_Score: ', metrics.r2_score(y_test, lm1_pred))\n\nprint('\\nall features, with scaling:')\nprint('MAE:', metrics.mean_absolute_error(y2_test, lm2_pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y2_test, lm2_pred)))\nprint('R2_Score: ', metrics.r2_score(y2_test, lm2_pred))\n\npred = pd.DataFrame(data={'Predicted_Rent': np.exp(lm2_pred), 'Actual_Rent': np.exp(y2_test)})\n\nfig = plt.figure(figsize=(12, 6))\nax1 = fig.add_subplot(111)\nax1.scatter(pred['Actual_Rent'], pred['Predicted_Rent'], color='tab:cyan', marker=\".\", label='Predicted', alpha=0.5)\nax1.scatter(pred['Actual_Rent'], pred['Actual_Rent'], s=10, color='tab:orange', marker=\"s\", label='Actual')\n\nplt.xlabel('Actual Rent')\nplt.ylabel('Predicted Rent')\nplt.legend(['R2_Score= {:.4f}\\nRMSE= {:.4f} '.format(\n                metrics.r2_score(y2_test, lm2_pred), \n                np.sqrt(metrics.mean_squared_error(y2_test, lm2_pred)))\n           ],\n           loc='best')\nplt.title('Linear Regression')\nplt.grid()\nplt.show()","1127a4e8":"rf1 = RandomForestRegressor(random_state=101, n_estimators=200)\nrf2 = RandomForestRegressor(random_state=101, n_estimators=200)\n\nrf1.fit(X_train, y_train)\nrf2.fit(X2_train, y2_train)","b498b47d":"rf1_pred = rf1.predict(X_test)\nrf2_pred = rf2.predict(X2_test)","f69b7b28":"print('Random Forest Performance:')\n\nprint('\\nall features, No scaling:')\nprint('MAE:', metrics.mean_absolute_error(y_test, rf1_pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, rf1_pred)))\nprint('R2_Score: ', metrics.r2_score(y_test, rf1_pred))\n\nprint('\\nall features, with scaling:')\nprint('MAE:', metrics.mean_absolute_error(y2_test, rf2_pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y2_test, rf2_pred)))\nprint('R2_Score: ', metrics.r2_score(y2_test, rf2_pred))\n\npred = pd.DataFrame(data={'Predicted_Rent': np.exp(rf1_pred), 'Actual_Rent': np.exp(y_test)})\n\nfig = plt.figure(figsize=(12, 6))\nax1 = fig.add_subplot(111)\nax1.scatter(pred['Actual_Rent'], pred['Predicted_Rent'], color='tab:cyan', marker=\".\", label='Predicted', alpha=0.5)\nax1.scatter(pred['Actual_Rent'], pred['Actual_Rent'], s=10, color='tab:orange', marker=\"s\", label='Actual')\n\nplt.xlabel('Actual Rent')\nplt.ylabel('Predicted Rent')\nplt.legend(['R2_Score= {:.4f}\\nRMSE= {:.4f} '.format(\n                metrics.r2_score(y_test, rf1_pred), \n                np.sqrt(metrics.mean_squared_error(y_test, rf1_pred)))\n           ],\n           loc='best')\nplt.title('Random Forest')\nplt.grid()\nplt.show()","c8f2ae53":"gbm1 = GradientBoostingRegressor(learning_rate=0.1, n_estimators=100, min_samples_split=2, \n                                 min_samples_leaf=1, max_depth=3, subsample=1.0, max_features= None, \n                                 random_state=101)\n\ngbm1.fit(X_train, y_train)","2bb8b5d1":"gbm1_pred = gbm1.predict(X_test)","3d80da7f":"print('Gradiant Boosting Performance:')\n\nprint('\\nall features, No scaling:')\nprint('MAE:', metrics.mean_absolute_error(y_test, gbm1_pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, gbm1_pred)))\nprint('R2_Score: ', metrics.r2_score(y_test, gbm1_pred))\n\npred = pd.DataFrame(data={'Predicted_Rent': np.exp(gbm1_pred), 'Actual_Rent': np.exp(y_test)})\n\nfig = plt.figure(figsize=(12, 6))\nax1 = fig.add_subplot(111)\nax1.scatter(pred['Actual_Rent'], pred['Predicted_Rent'], color='tab:cyan', marker=\".\", label='Predicted', alpha=0.5)\nax1.scatter(pred['Actual_Rent'], pred['Actual_Rent'], s=10, color='tab:orange', marker=\"s\", label='Actual')\n\nplt.xlabel('Actual Rent')\nplt.ylabel('Predicted Rent')\nplt.legend(['R2_Score= {:.4f}\\nRMSE= {:.4f} '.format(\n                metrics.r2_score(y_test, gbm1_pred), \n                np.sqrt(metrics.mean_squared_error(y_test, gbm1_pred)))\n           ],\n           loc='best')\nplt.title('Gradient Boosting')\nplt.grid()\nplt.show()","024a0416":"xgb1 = XGBRegressor(learning_rate=0.1, n_estimators=100, \n                    min_samples_split=2, min_samples_leaf=1, \n                    max_depth=3, subsample=1.0, random_state=101)\n\nxgb1.fit(X_train, y_train)","f377d5e9":"xgb1_pred = xgb1.predict(X_test)","59d7873e":"print('XgBoost Performance:')\n\nprint('\\nall features, No scaling:')\nprint('MAE:', metrics.mean_absolute_error(y_test, xgb1_pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, xgb1_pred)))\nprint('R2_Score: ', metrics.r2_score(y_test, xgb1_pred))\n\npred = pd.DataFrame(data={'Predicted_Rent': np.exp(xgb1_pred), 'Actual_Rent': np.exp(y_test)})\n\nfig = plt.figure(figsize=(12, 6))\nax1 = fig.add_subplot(111)\nax1.scatter(pred['Actual_Rent'], pred['Predicted_Rent'], color='tab:cyan', marker=\".\", label='Predicted', alpha=0.5)\nax1.scatter(pred['Actual_Rent'], pred['Actual_Rent'], s=10, color='tab:orange', marker=\"s\", label='Actual')\n\nplt.xlabel('Actual Rent')\nplt.ylabel('Predicted Rent')\nplt.legend(['R2_Score= {:.4f}\\nRMSE= {:.4f} '.format(\n                metrics.r2_score(y_test, xgb1_pred), \n                np.sqrt(metrics.mean_squared_error(y_test, xgb1_pred)))\n           ],\n           loc='best')\nplt.title('XgBoost')\nplt.grid()\nplt.show()","9568b34c":"***Training***","155a199d":"***Predictions***","aaf69b8b":"## XgBoost","37d3c3f5":"***Data Split 2: all of our final dataset, with scaling***","4ae0f3a7":"## Machine Learning Model Building Pipeline: Data Analysis\n\nIn this following notebook, we will go through the Data Analysis step in the Machine Learning model building pipeline. There will be a notebook for each one of the Machine Learning Pipeline steps:\n\n1. [Data Analysis](https:\/\/www.kaggle.com\/rkb0023\/exploratory-data-analysis-house-rent-prediction)\n2. [Feature Engineering](https:\/\/www.kaggle.com\/rkb0023\/feature-engineering-house-rent-prediction)\n3. Model Building\n\n**This is the notebook for step 3: Model Building**\n<hr>\n","03357b56":"## Linear Regression\n\nFrom our EDA, we can see that most features do not have a linear relationship with our labels (gdp_per_capita), yet we will try linear regression, and use the result as a reference (other methods should have better results).","9b76c450":"## Conclusion\n\nIn this project, we used house rent dataset to build a monthly rent predictor. 4 different learning regressors (Linear Regression, Random Forest, Gradiant Boosting, and XgBoost) were tested, and we have acheived the best prediction performance using Random Forest, followed by Gradiant Boosting, and then XgBoost, while Linear Regression, acheived the worst performance of the 4.\n\nThe best prediction performance was acheived using Random Forest regressor, using all features in the dataset, and resulted in the following metrics:\n\n- Mean Absolute Error (MAE): 0.0591\n- Root mean squared error (RMSE): 0.1717\n- R-squared Score (R2_Score): 0.8746","1907af38":"The orange line in the graph represents the actual rents. And the cyan circles plotted against actual rents, on the x-axis, are the predicted rents.","ff46e9cd":"***Data Split 1: all of our final dataset, no scaling***","60330a9a":"We will train the xgboost regressor with the default parameter values","737e037f":"***Predictions***","eb13992d":"## Random Forest\n\nLet's first try random forest with our data splits (with and without feature selection). Scaling the features should not affect this algorithm's performance, still we are going to test it. later we will try to improve its performance.","97301805":"***Evaluation***","16ca5992":"***Evaluation***","8c5fb69c":"***Model Training***","a81da648":"XgBoost also performed poorly. So will not be optimizing.","6526ed74":"***Model Training***","479ca01d":"***Predictions***","843842ea":"***Evaluation***","e4f86095":"## Gradient Boosting","cf90bcb2":"***Evaluation***","a9e1a172":"Linear Model worked terribly, which was expected as there were no linear relationship among the features. Feature scaling has a small positive effect on LR's prediction performance.","2bd62cf4":"It is clear that Gradiant Boosting gave us a very poor performance, compared to Random Forest. So not opting for an optimization.","3d816f39":"***Model Training***","7c6374d4":"Features without scaling gave a better result on random forest than with scaled features","10b648b4":"# Model Building","bb1dd772":"***Predictions***","6f593ab3":"## Importing Libraries"}}