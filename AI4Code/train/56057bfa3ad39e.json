{"cell_type":{"c7d8b6fc":"code","43db3d6c":"code","cf40d1be":"code","6448a42b":"code","e240a025":"code","0a6fa29f":"code","4bc9901f":"code","ca413212":"code","a68a62ed":"code","02925b45":"code","e96b7ab5":"code","7f71466c":"code","31554d05":"code","30afb28e":"code","dcc05045":"code","80842556":"code","92e72545":"code","6c93d879":"code","28413c55":"code","d4fba3e8":"code","37cde770":"code","b023fb52":"code","58f1d4e3":"code","30beddf8":"code","f6a86c1b":"code","8c88f144":"code","4b0c3e91":"code","74cb59c8":"code","c89b3f96":"code","6517c7c2":"code","7c9d3961":"code","b7b1d361":"code","e9ed6344":"code","83eb13c7":"code","7ec927aa":"code","489629d1":"code","372b47ee":"code","33dc2516":"code","35ab143b":"code","3ef5c327":"code","461d3f75":"code","340bfff3":"code","1b198577":"markdown","42c31668":"markdown"},"source":{"c7d8b6fc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n%matplotlib inline\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\nimport xgboost\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,SpatialDropout1D\nfrom tensorflow.keras.layers import LSTM,Dropout\nfrom keras.layers import Bidirectional\nfrom tensorflow.keras.optimizers import RMSprop,Adam\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom numpy import array\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","43db3d6c":"train=pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest=pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","cf40d1be":"train.head()","6448a42b":"test.head()","e240a025":"train[\"target\"].value_counts()","0a6fa29f":"sns.countplot(\"target\",data=train)","4bc9901f":"from textblob import TextBlob","ca413212":"tweettoken = TweetTokenizer(strip_handles=True, reduce_len=True)","a68a62ed":"lemmatizer=WordNetLemmatizer()","02925b45":"stemmer=PorterStemmer()","e96b7ab5":"collect=[]\ncollecttest=[]\ndef preprocess(t,kpc):\n    def form_sentence(tweet):\n        tweet_blob = TextBlob(tweet)\n        return ' '.join(tweet_blob.words)\n    t=form_sentence(t)\n    tee=re.sub('[^a-zA-Z]',\" \",t)\n    tee=tee.lower()\n    res=tweettoken.tokenize(tee)\n    for i in res:\n        if i in stopwords.words('english'):\n            res.remove(i)\n    rest=[]\n    for k in res:\n        rest.append(stemmer.stem(k))\n    ret=\" \".join(rest)\n    if kpc==1:\n        collect.append(ret)\n    elif kpc==0:\n        collecttest.append(ret)","7f71466c":"def splitpro(t,q,m):\n         for j in range(q):\n                 preprocess(t[\"text\"].iloc[j],m)","31554d05":"len(train)","30afb28e":"len(test)","dcc05045":"splitpro(train,7613,1)","80842556":"splitpro(test,3263,0)","92e72545":"len(collect)","6c93d879":"len(collecttest)","28413c55":"collect[:5]","d4fba3e8":"collecttest[:5]","37cde770":"val=train[\"target\"].values","b023fb52":"val","58f1d4e3":"oneh=[]\noneht=[]\ndef hot(cc,k):\n    for i in cc:\n        if k==1:\n            oneh.append(one_hot(i,500))\n        elif k==0:\n            oneht.append(one_hot(i,500))","30beddf8":"hot(collect,1)","f6a86c1b":"hot(collecttest,0)","8c88f144":"len(oneh[0])","4b0c3e91":"max=0\nfor i in oneh:\n    tq=len(i)\n    if tq>max:\n        max=tq\nprint(max)","74cb59c8":"sent=38\nemoneh=pad_sequences(oneh,padding=\"post\",maxlen=sent)\nemoneht=pad_sequences(oneht,padding=\"post\",maxlen=sent)","c89b3f96":"xtrain,xtest,ytrain,ytest=train_test_split(emoneh,val,train_size=0.80,random_state=42)","6517c7c2":"model=Sequential()\nmodel.add(Embedding(500,15,input_length=sent))\nmodel.add(SpatialDropout1D(0.4))\nmodel.add(LSTM(100))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(1, activation='softmax'))\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","7c9d3961":"model.fit(xtrain,ytrain,validation_data=(xtest,ytest),epochs=50,batch_size=300,verbose=2)","b7b1d361":"tttt=model.predict(emoneht)","e9ed6344":"tttt","83eb13c7":"ttttt=tttt.flatten()","7ec927aa":"r = pd.Series(ttttt,name=\"target\")","489629d1":"sample=pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","372b47ee":"sample.head()","33dc2516":"t=test[\"id\"]","35ab143b":"t","3ef5c327":"submiss = pd.concat([pd.Series(t,name = \"id\"),r],axis = 1)","461d3f75":"submiss","340bfff3":"submiss.to_csv(\"disasml20.csv\",index=False)","1b198577":"Let initiliaze our stemmer and tokenizer and do some data processing","42c31668":"# DATA PREPROCESSING"}}