{"cell_type":{"49e07607":"code","60523e82":"code","eacb96db":"code","5410e2c9":"code","627955dc":"code","eedf2d06":"code","4fe08e23":"code","e822b6c0":"code","244a86de":"code","46e5d81b":"code","89b3cf57":"code","a17db02d":"code","2b0cf63a":"code","bd328e3d":"code","d92da362":"code","b7807a14":"code","8c0315aa":"code","608718bf":"code","204846ea":"code","c1e26da9":"code","3c4b1321":"markdown","3493efd3":"markdown","ccb43a7d":"markdown","feb8b610":"markdown","f69c1ed2":"markdown","74ab28e6":"markdown","39350d57":"markdown","ba189a0a":"markdown","e2cf9c39":"markdown","fc213fac":"markdown","af1a8361":"markdown","41d88071":"markdown","58d777b2":"markdown","a49e7925":"markdown"},"source":{"49e07607":"# Importar os principais pacotes\nimport numpy as np\nimport pandas as pd\nimport itertools\nimport seaborn as sns\nsns.set()\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport time\nimport datetime\nimport gc\n\n# Evitar que aparece os warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Seta algumas op\u00e7\u00f5es no Jupyter para exibi\u00e7\u00e3o dos datasets\npd.set_option('display.max_columns', 200)\npd.set_option('display.max_rows', 200)\n\n# Variavel para controlar o treinamento no Kaggle\nTRAIN_OFFLINE = False","60523e82":"# Importa os pacotes de algoritmos\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\n\n# Importa pacotes do sklearn\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split, GridSearchCV, KFold\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, log_loss\nfrom sklearn.preprocessing import scale, MinMaxScaler, StandardScaler","eacb96db":"def read_data():\n    \n    if TRAIN_OFFLINE:\n        print('Carregando arquivo dataset_treino.csv....')\n        train = pd.read_csv('..\/dataset\/dataset_treino.csv')\n        print('dataset_treino.csv tem {} linhas and {} colunas'.format(train.shape[0], train.shape[1]))\n        \n        print('Carregando arquivo dataset_teste.csv....')\n        test = pd.read_csv('..\/dataset\/dataset_teste.csv')\n        print('dataset_teste.csv tem {} linhas and {} colunas'.format(test.shape[0], test.shape[1]))\n        \n    else:\n        print('Carregando arquivo dataset_treino.csv....')\n        train = pd.read_csv('\/kaggle\/input\/competicao-dsa-machine-learning-dec-2019\/dataset_treino.csv')\n        print('dataset_treino.csv tem {} linhas and {} colunas'.format(train.shape[0], train.shape[1]))\n        \n        print('Carregando arquivo dataset_treino.csv....')\n        test = pd.read_csv('\/kaggle\/input\/competicao-dsa-machine-learning-dec-2019\/dataset_teste.csv')\n        print('dataset_teste.csv tem {} linhas and {} colunas'.format(test.shape[0], test.shape[1]))\n    \n    return train, test","5410e2c9":"# Leitura dos dados\ntrain, test = read_data()","627955dc":"# Removendo todas as variaveis categoricas\ndrop_features = []\nfor col in train.columns:\n    if train[col].dtype =='object':\n        drop_features.append(col)\n\ntrain = train.drop(drop_features, axis=1)","eedf2d06":"# Preenche os dados missing com 0 (zero)\ntrain.fillna(train.mean(),inplace=True)","4fe08e23":"# Separando features preditoras e target\ntrain_x = train.drop(['ID','target'], axis=1)\ntrain_y = train['target']\n\n# Padronizando os dados\nscaler = StandardScaler()\ntrain_x = scaler.fit_transform(train_x)","e822b6c0":"# Criando uma funcao para cria\u00e7\u00e3o, execu\u00e7\u00e3o e valida\u00e7\u00e3o do modelo\ndef run_model(modelo, X_tr, y_tr, useTrainCV=True, cv_folds=5, early_stopping_rounds=10):\n    \n    # Utiliza\u00e7\u00e3o do Cross-Validation\n    if useTrainCV:\n        xgb_param = modelo.get_xgb_params()\n        xgtrain = xgb.DMatrix(X_tr, label=y_tr)\n        \n        print ('Start cross validation')\n        cvresult = xgb.cv(xgb_param, \n                          xgtrain, \n                          num_boost_round=modelo.get_params()['n_estimators'], \n                          nfold=cv_folds,\n                          metrics=['logloss'],\n                          stratified=True,\n                          seed=42,\n                          #verbose_eval=True,\n                          early_stopping_rounds=early_stopping_rounds)\n\n        modelo.set_params(n_estimators=cvresult.shape[0])\n        best_tree = cvresult.shape[0]\n        print('Best number of trees = {}'.format(best_tree))\n    \n    # Fit do modelo\n    modelo.fit(X_tr, y_tr, eval_metric='logloss')\n        \n    # Predi\u00e7\u00e3o no dataset de treino\n    train_pred = modelo.predict(X_tr)\n    train_pred_prob = modelo.predict_proba(X_tr)[:,1]\n    \n    # Exibir o relatorio do modelo\n    #print(\"Acur\u00e1cia : %.4g\" % accuracy_score(y_tr, train_pred))\n    #print(\"AUC Score (Treino): %f\" % roc_auc_score(y_tr, train_pred_prob))\n    print(\"Log Loss (Treino): %f\" % log_loss(y_tr, train_pred_prob))\n    print(\"Log Loss (Test): %f\" % cvresult['test-logloss-mean'][best_tree-1])\n    \n    feature_imp = pd.Series(modelo.feature_importances_.astype(float)).sort_values(ascending=False)\n    \n    plt.figure(figsize=(18,8))\n    feature_imp[:25].plot(kind='bar', title='Feature Importances')\n    plt.ylabel('Feature Importance Score')\n    plt.tight_layout()","244a86de":"%%time\n\n# Criando o primeiro modelo XGB\nmodeloXGB = XGBClassifier(learning_rate = 0.1,\n                          n_estimators = 200,\n                          max_depth = 5,\n                          min_child_weight = 1,\n                          gamma = 0,\n                          subsample = 0.8,\n                          colsample_bytree = 0.8,\n                          objective = 'binary:logistic',\n                          n_jobs = -1,\n                          scale_pos_weight = 1,\n                          seed = 42)\n\nrun_model(modeloXGB, train_x, train_y)","46e5d81b":"gc.collect()","89b3cf57":"'''%%time\n\n# Definindo os parametros que ser\u00e3o testados no GridSearch\nparam_v1 = {\n 'max_depth':range(2,5),\n 'min_child_weight':range(1,2)\n}\n\ngrid_1 = GridSearchCV(estimator = XGBClassifier(learning_rate = 0.1, \n                                                n_estimators = 200, \n                                                max_depth = 5,\n                                                min_child_weight = 1, \n                                                gamma = 0, \n                                                subsample = 0.8, \n                                                colsample_bytree = 0.8,\n                                                objective = 'binary:logistic', \n                                                nthread = 4, \n                                                scale_pos_weight = 1, \n                                                seed = 42),\n                      param_grid = param_v1, \n                      scoring = 'neg_log_loss',\n                      n_jobs = -1,\n                      iid = False, \n                      cv = 5)\n\n# Realizando o fit e obtendo os melhores parametros do grid\ngrid_1.fit(train_x, train_y)\ngrid_1.best_params_, grid_1.best_score_'''","a17db02d":"'''%%time\n\n# Definindo os parametros que ser\u00e3o testados no GridSearch\nparam_v2 = {\n 'gamma':[i\/10.0 for i in range(0,2)]\n}\n\ngrid_2 = GridSearchCV(estimator = XGBClassifier(learning_rate = 0.1, \n                                                n_estimators = 200, \n                                                max_depth = grid_1.best_params_['max_depth'],\n                                                min_child_weight = grid_1.best_params_['min_child_weight'], \n                                                gamma = 0, \n                                                subsample = 0.8, \n                                                colsample_bytree = 0.8,\n                                                objective = 'binary:logistic', \n                                                nthread = 4, \n                                                scale_pos_weight = 1, \n                                                seed = 42),\n                      param_grid = param_v2, \n                      scoring = 'neg_log_loss',\n                      n_jobs = -1,\n                      iid = False, \n                      cv = 5)\n\n# Realizando o fit e obtendo os melhores parametros do grid\ngrid_2.fit(train_x, train_y)\ngrid_2.best_params_, grid_2.best_score_'''","2b0cf63a":"'''%%time\n\n# Definindo os parametros que ser\u00e3o testados no GridSearch\nparam_v3 = {\n 'subsample':[i\/10.0 for i in range(6,8)],\n 'colsample_bytree':[i\/10.0 for i in range(6,8)]\n}\n\ngrid_3 = GridSearchCV(estimator = XGBClassifier(learning_rate = 0.1, \n                                                n_estimators = 200, \n                                                max_depth = grid_1.best_params_['max_depth'],\n                                                min_child_weight = grid_1.best_params_['min_child_weight'], \n                                                gamma = grid_2.best_params_['gamma'], \n                                                subsample = 0.8, \n                                                colsample_bytree = 0.8,\n                                                objective = 'binary:logistic', \n                                                nthread = 4, \n                                                scale_pos_weight = 1, \n                                                seed = 42),\n                      param_grid = param_v3, \n                      scoring = 'neg_log_loss',\n                      n_jobs = -1,\n                      iid = False, \n                      cv = 5)\n\ngrid_3.fit(train_x, train_y)\ngrid_3.best_params_, grid_3.best_score_'''","bd328e3d":"'''%%time\n\n# Definindo os parametros que ser\u00e3o testados no GridSearch\nparam_v4 = {\n 'reg_alpha':[0, 0.001, 0.005]\n}\n\ngrid_4 = GridSearchCV(estimator = XGBClassifier(learning_rate = 0.1, \n                                                n_estimators = 200, \n                                                max_depth = grid_1.best_params_['max_depth'],\n                                                min_child_weight = grid_1.best_params_['min_child_weight'], \n                                                gamma = grid_2.best_params_['gamma'], \n                                                subsample = grid_3.best_params_['subsample'], \n                                                colsample_bytree = grid_3.best_params_['colsample_bytree'],\n                                                objective = 'binary:logistic', \n                                                nthread = 4, \n                                                scale_pos_weight = 1, \n                                                seed = 42),\n                      param_grid = param_v4, \n                      scoring = 'neg_log_loss',\n                      n_jobs = -1,\n                      iid = False, \n                      cv = 5)\n\n# Realizando o fit e obtendo os melhores parametros do grid\ngrid_4.fit(train_x, train_y)\ngrid_4.best_params_, grid_4.best_score_'''","d92da362":"'''%%time\n\n# Criando o modelo XGB com todas as otimiza\u00e7\u00f5es\nmodeloXGB_v2 = XGBClassifier(learning_rate = 0.01, \n                             n_estimators = 1000, \n                             max_depth = 4,\n                             min_child_weight = 1,\n                             gamma = 0.04, \n                             subsample = 0.6,\n                             colsample_bytree = 0.8,\n                             reg_alpha = 0, \n                             objective = 'binary:logistic', \n                             n_jobs = -1,\n                             scale_pos_weight = 1, \n                             seed = 42)\n\nrun_model(modeloXGB_v2, train_x, train_y)'''","b7807a14":"# Visualizando o modelo XGBoost\nprint(modeloXGB)","8c0315aa":"# Colocando o dataset de teste conforme o modelo treinado\n# Neste caso \u00e9 necess\u00e1rio aplicar a Feature Engineering usada para gerar o modelo\ntext_x = test.drop(['ID'], axis=1)\n\n# Removendo todas as variaveis categoricas\ndrop_features = []\nfor col in text_x.columns:\n    if text_x[col].dtype =='object':\n        drop_features.append(col)\ntext_x = text_x.drop(drop_features, axis=1)\n\n# Preenche os dados missing com 0 (zero)\ntext_x.fillna(text_x.mean(),inplace=True)\n\n# Aplicando escala aos dados\ntext_x = scaler.fit_transform(text_x)\n\n# Realizando as previsoes\ntest_pred_prob = modeloXGB.predict_proba(text_x)[:,1]","608718bf":"# Criando dataset de submissao\nsubmission = pd.DataFrame({'ID': test[\"ID\"], 'PredictedProb': test_pred_prob.reshape((test_pred_prob.shape[0]))})\nprint(submission.head(10))","204846ea":"submission.to_csv('submission.csv', index=False)","c1e26da9":"plt.hist(submission.PredictedProb)\nplt.show()","3c4b1321":"## 5. Submissions","3493efd3":"#### Passo 05: otimiza\u00e7\u00e3o dos parametros: reg_alpha","ccb43a7d":"#### Passo 01: criando o modelo fixando alguns hyperparametros","feb8b610":"#### Passo 06: reduzindo Learning Rate e aumentar o numero de estimadores","f69c1ed2":"## 4.1. Algoritmo XGBoost - Extreme Gradient Boosting\n\nVamos dar uma olhada nas vantagens desse algoritmo:\n\n***Regulariza\u00e7\u00e3o:***\n- O XGBoost tamb\u00e9m \u00e9 conhecido como uma t\u00e9cnica de \"refor\u00e7o regularizado\", ajudando a reduzir o overfitting\n\n***Processamento paralelo:***\n- O XGBoost implementa o processamento paralelo e \u00e9 incrivelmente mais r\u00e1pido em compara\u00e7\u00e3o com o GBM. Mas, espere, sabemos que impulsionar \u00e9 um processo seq\u00fcencial; portanto, como ele pode ser paralelo? Sabemos que cada \u00e1rvore pode ser constru\u00edda somente ap\u00f3s a anterior, ent\u00e3o o que nos impede de fazer uma \u00e1rvore usando todos os n\u00facleos?\n\n***Alta flexibilidade***\n- O XGBoost permite que os usu\u00e1rios definam objetivos de otimiza\u00e7\u00e3o personalizados e crit\u00e9rios de avalia\u00e7\u00e3o. Isso adiciona uma nova dimens\u00e3o ao modelo e n\u00e3o h\u00e1 limite para o que podemos fazer.\n\n***Tratamento de valores ausentes***\n- O XGBoost possui uma rotina integrada para lidar com os valores ausentes. \u00c9 necess\u00e1rio que o usu\u00e1rio forne\u00e7a um valor diferente de outras observa\u00e7\u00f5es e passe isso como um par\u00e2metro. O XGBoost tenta coisas diferentes ao encontrar um valor ausente em cada n\u00f3 e descobre qual caminho seguir para valores ausentes no futuro.\n\n***Poda de \u00e1rvores:***\n- O XGBoost faz divis\u00f5es at\u00e9 a profundidade m\u00e1xima especificada e, em seguida, come\u00e7a a podar a \u00e1rvore para tr\u00e1s e remove as divis\u00f5es al\u00e9m das quais n\u00e3o h\u00e1 ganho positivo.\n\n***Valida\u00e7\u00e3o cruzada incorporada***\n- O XGBoost permite que o usu\u00e1rio execute uma valida\u00e7\u00e3o cruzada a cada itera\u00e7\u00e3o do processo de otimiza\u00e7\u00e3o e, portanto, \u00e9 f\u00e1cil obter o n\u00famero ideal exato de itera\u00e7\u00f5es de otimiza\u00e7\u00e3o em uma \u00fanica execu\u00e7\u00e3o.\n\n- Voc\u00ea pode come\u00e7ar a treinar um modelo XGBoost a partir da \u00faltima itera\u00e7\u00e3o da execu\u00e7\u00e3o anterior. Isso pode ser uma vantagem significativa em certas aplica\u00e7\u00f5es espec\u00edficas.","74ab28e6":"#### Passo 03: otimiza\u00e7\u00e3o dos parametros: gamma","39350d57":"#### Passo 04: otimiza\u00e7\u00e3o dos parametros: subsample e colsample_bytree","ba189a0a":"**Vers\u00e3o 1.0.0: **\n- modelo: XGBoost (com algumas otimiza\u00e7\u00f5es)\n- features categoricas: removido\n- dados missing: atribu\u00eddo o valor medio\n\n**Melhorias:**\n- Realizar Feature Engineering atrav\u00e9s da analise explorat\u00f3ria\n- Explorar os dados missing e vari\u00e1veis categ\u00f3ricas\n- Testar as otimiza\u00e7\u00f5es do XBoost (deixei comentado para explorar)\n\n**Se este notebook foi \u00fatil, realize o fork e implemente melhorias. Deixa um VOTO tamb\u00e9m ;)**","e2cf9c39":"# Kaggle\n## Competi\u00e7\u00e3o DSA de Machine Learning - Dezembro 2019","fc213fac":"## 1. Importando as bibliotecas","af1a8361":"#### Passo 02: otimiza\u00e7\u00e3o dos parametros: max_depth e min_child_weight","41d88071":"## 3. Feature Engineering","58d777b2":"## 2. Carregando os dados de treino e teste","a49e7925":"## 4. Criar e avaliar alguns algoritmos de Machine Learning"}}