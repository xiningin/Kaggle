{"cell_type":{"024d61a3":"code","c3fd3b2a":"code","577e7487":"code","a10183c3":"code","d84d25dc":"code","554503bd":"code","fa5554f5":"code","3dcd47c2":"code","adfdd23e":"code","dca1d3e8":"code","53bda68d":"code","d9409312":"code","94d0a967":"code","5605f56d":"code","ba0f0933":"code","2d4a2911":"code","b7fa89ea":"code","54f3dc78":"code","5b939466":"code","3acf4d23":"code","81531d99":"code","7ca95616":"code","015c03da":"code","95075886":"code","8ebb0139":"code","614e1b66":"code","25d95389":"code","46ff2127":"code","20ccec5c":"code","257690b0":"code","119b4756":"code","1311b80b":"code","51c31b70":"code","e001b3e5":"code","48a68d74":"code","60bb36c1":"code","0b1013cb":"code","927c7d61":"code","3fe3c7ba":"code","acfa3dec":"code","46ce2181":"code","af4dd0cd":"code","3479c156":"code","ad97664b":"markdown","11a83a6b":"markdown","5027f05c":"markdown","9e0fe69c":"markdown","4a42e30e":"markdown","b5b02af4":"markdown","9bd22101":"markdown","0dd9a5db":"markdown","cca44163":"markdown","0b7baab1":"markdown","1410e414":"markdown","25859893":"markdown","2ad89a2a":"markdown","50a71bd5":"markdown","10ec4628":"markdown","d7862bcb":"markdown","4402f4d7":"markdown","d25c7c86":"markdown","1f2548ec":"markdown","9088fcf7":"markdown","48ca444e":"markdown","06b99e30":"markdown","b8ac1e31":"markdown","e5bbe75a":"markdown","a96fc080":"markdown","1ef2d987":"markdown","f8562170":"markdown","3f6f6950":"markdown","aeb5d394":"markdown","13358e31":"markdown","d9cad34b":"markdown","5446ff69":"markdown","b9fb58a3":"markdown","abaed62a":"markdown","d42fd1c7":"markdown","e9152de6":"markdown","2bade556":"markdown","b16c445b":"markdown","3fa4972d":"markdown"},"source":{"024d61a3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c3fd3b2a":"import numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\npd.set_option('display.width', None)","577e7487":"# Train and test datasets are being concatenated, because I will need the past in order to predict the future based on \n# some possible patterns that occured in the past.\n\ntrain = pd.read_csv(\"..\/input\/demand-forecasting-kernels-only\/train.csv\", parse_dates=['date'])\ntest = pd.read_csv(\"..\/input\/demand-forecasting-kernels-only\/test.csv\", parse_dates=['date'])\ndf = pd.concat([train, test], sort=False)","a10183c3":"def check_df(dataframe):\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    print(\"##################### Head #####################\")\n    print(dataframe.head(3))\n    print(\"##################### Tail #####################\")\n    print(dataframe.tail(3))\n    print(\"##################### NA #####################\")\n    print(dataframe.isnull().sum())\n    print(\"##################### Quantiles #####################\")\n    print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)","d84d25dc":"check_df(train)","554503bd":"check_df(test)","fa5554f5":"def outlier_thresholds(dataframe, col_name, q1_perc=0.05, q3_perc=0.95):\n    \"\"\"\n    given dataframe, column name, q1_percentage and q3 percentage, function calculates low_limit and up_limit\n\n    \"\"\"\n    quartile1 = dataframe[col_name].quantile(q1_perc)\n    quartile3 = dataframe[col_name].quantile(q3_perc)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit\n\n\ndef check_outlier(dataframe, col_name, q1_perc=0.01, q3_perc=0.99):\n    outlier_list = []\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name, q1_perc=0.01, q3_perc=0.99)\n    if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):\n        return True\n\n    else:\n        return False","3dcd47c2":"check_outlier(df, 'sales')","adfdd23e":"# check the concatenated df\ndf.head()","dca1d3e8":"print(f\"total number of stores: {df['store'].nunique()}\\n\")\nprint(f\"total number of items: {df['item'].nunique()}\\n\")\nprint(f\"total number of items per each store: {df.groupby(['store'])['item'].nunique()}\")","53bda68d":"# before building an ML model, let's have a basic prediction by taking the mean values for each item on each store \n\ndf.groupby([\"store\", \"item\"]).agg({\"sales\": [\"sum\", \"mean\", \"median\", \"std\"]})","d9409312":"train_plot = train.set_index('date')\ny = train_plot['sales'].resample('MS').mean() \n\nresult = sm.tsa.seasonal_decompose(y, model='additive')\nfig = plt.figure()  \nfig = result.plot()  \nfig.set_size_inches(8, 6)","94d0a967":"def create_date_features(df):\n    df['month'] = df.date.dt.month\n    df['day_of_month'] = df.date.dt.day\n    df['day_of_year'] = df.date.dt.dayofyear\n    df['week_of_year'] = df.date.dt.weekofyear\n    # 1.1.2013 is Tuesday, so our starting point is the 2nd day of week\n    df['day_of_week'] = df.date.dt.dayofweek + 1\n    df['year'] = df.date.dt.year\n    df[\"is_wknd\"] = df.date.dt.weekday \/\/ 4\n    df['is_month_start'] = df.date.dt.is_month_start.astype(int)\n    df['is_month_end'] = df.date.dt.is_month_end.astype(int)\n    return df","5605f56d":"df = create_date_features(df)\ndf.head()","ba0f0933":"def random_noise(dataframe):\n\n    return np.random.normal(size=(len(dataframe),))","2d4a2911":"# sort the values per store, item and date so that values would be shifted equally\n\ndf.sort_values(by=['store', 'item', 'date'], axis=0, inplace=True)","b7fa89ea":"# the feature name will be created dynamically with regards to the lag value for a given list of lags\n\ndef lag_features(dataframe, lags):\n    dataframe = dataframe.copy()\n    for lag in lags:\n        dataframe['sales_lag_' + str(lag)] = dataframe.groupby([\"store\", \"item\"])['sales'].transform(\n            lambda x: x.shift(lag)) + random_noise(dataframe)\n    return dataframe","54f3dc78":"df = lag_features(df, [91, 98, 105, 112, 119, 126, 182, 364, 546, 728])","5b939466":"def roll_mean_features(dataframe, windows):\n    dataframe = dataframe.copy()\n    for window in windows:\n        dataframe['sales_roll_mean_' + str(window)] = dataframe.groupby([\"store\", \"item\"])['sales']. \\\n                                                          transform(\n            lambda x: x.shift(1).rolling(window=window, min_periods=10, win_type=\"triang\").mean()) + random_noise(dataframe)\n    return dataframe","3acf4d23":"df = roll_mean_features(df, [365, 546, 730])\ndf.head()","81531d99":"def ewm_features(dataframe, alphas, lags):\n    dataframe = dataframe.copy()\n    for alpha in alphas:\n        for lag in lags:\n            dataframe['sales_ewm_alpha_' + str(alpha).replace(\".\", \"\") + \"_lag_\" + str(lag)] = \\\n                dataframe.groupby([\"store\", \"item\"])['sales']. \\\n                    transform(lambda x: x.shift(lag).ewm(alpha=alpha).mean())\n    return dataframe","7ca95616":"alphas = [0.95, 0.9, 0.8, 0.7, 0.5]\nlags = [91, 98, 105, 112, 180, 270, 365, 546, 728]","015c03da":"df = ewm_features(df, alphas, lags)\ndf.tail()","95075886":"df = pd.get_dummies(df, columns=['store', 'item', 'day_of_week', 'month'])","8ebb0139":"df['sales'] = np.log1p(df[\"sales\"].values)\ndf['sales'].head() ","614e1b66":"def smape(preds, target):\n    n = len(preds)\n    masked_arr = ~((preds == 0) & (target == 0))\n    preds, target = preds[masked_arr], target[masked_arr]\n    num = np.abs(preds-target)\n    denom = np.abs(preds)+np.abs(target)\n    smape_val = (200*np.sum(num\/denom))\/n\n    return smape_val\n\ndef lgbm_smape(preds, train_data):\n    labels = train_data.get_label()\n    smape_val = smape(np.expm1(preds), np.expm1(labels))\n    return 'SMAPE', smape_val, False","25d95389":"train = df.loc[(df[\"date\"] < \"2017-01-01\"), :]\ntrain[\"date\"].min(), train[\"date\"].max()","46ff2127":"val = df.loc[(df[\"date\"] >= \"2017-01-01\") & (df[\"date\"] < \"2017-04-01\"), :]","20ccec5c":"# columns with no useful information or with information that is already derived will be dropped.\n\ncols = [col for col in train.columns if col not in ['date', 'id', \"sales\", \"year\"]]","257690b0":"Y_train = train['sales']\nX_train = train[cols]\n\nY_val = val['sales']\nX_val = val[cols]","119b4756":"lgb_params = {'metric': {'mae'},\n              'num_leaves': 10,\n              'learning_rate': 0.02,\n              'feature_fraction': 0.8,\n              'max_depth': 5,\n              'verbose': 0,\n              'num_boost_round': 15000,\n              'early_stopping_rounds': 200,\n              'nthread': -1}","1311b80b":"lgbtrain = lgb.Dataset(data=X_train, label=Y_train, feature_name=cols)\nlgbval = lgb.Dataset(data=X_val, label=Y_val, reference=lgbtrain, feature_name=cols)","51c31b70":"# lgbtrain and lgbval's datatype is LightGBM Dataset\n\ntype(lgbtrain)","e001b3e5":"model = lgb.train(lgb_params, lgbtrain,\n                  valid_sets=[lgbtrain, lgbval],\n                  num_boost_round=lgb_params['num_boost_round'],\n                  early_stopping_rounds=lgb_params['early_stopping_rounds'],\n                  feval=lgbm_smape,\n                  verbose_eval=200)","48a68d74":"# sales values in train dataset will be predicted, these are the log values\n\ny_pred_val = model.predict(X_val)","60bb36c1":"# log values are reversed and the predicted sales values are revealed. \n\nsmape(np.expm1(y_pred_val), np.expm1(Y_val))","0b1013cb":"def plot_lgb_importances(model,plot=True,num=10):\n    from matplotlib import pyplot as plt\n    import seaborn as sns\n    gain = model.feature_importance('gain')\n    feat_imp = pd.DataFrame({'feature': model.feature_name(),\n                             'split': model.feature_importance('split'),\n                             'gain': 100 * gain \/ gain.sum()}).sort_values('gain', ascending=False)\n    if plot:\n        plt.figure(figsize=(10, 10))\n        sns.set(font_scale=1)\n        sns.barplot(x=\"gain\", y=\"feature\", data=feat_imp[0:25])\n        plt.title('feature')\n        plt.tight_layout()\n        plt.show()\n    else:\n        print(feat_imp.head(num))\n    print(feat_imp.head(num))","927c7d61":"plot_lgb_importances(model,30)","3fe3c7ba":"# this one is the built-in plot function of LightGBM library\n\nlgb.plot_importance(model, max_num_features=20, figsize=(10, 10), importance_type=\"gain\")\nplt.show()","acfa3dec":"# train and validation values are concatenated \n\ntrain = df.loc[~df.sales.isna()]\nY_train = train['sales']\nX_train = train[cols]\n\ntest = df.loc[df.sales.isna()]\nX_test = test[cols]","46ce2181":"lgb_params = {'metric': {'mae'},\n              'num_leaves': 10,\n              'learning_rate': 0.02,\n              'feature_fraction': 0.8,\n              'max_depth': 5,\n              'verbose': 0,\n              'nthread': -1,\n              \"num_boost_round\": model.best_iteration}","af4dd0cd":"lgbtrain_all = lgb.Dataset(data=X_train, label=Y_train, feature_name=cols)\nfinal_model = lgb.train(lgb_params, lgbtrain_all, num_boost_round=model.best_iteration)\ntest_preds = final_model.predict(X_test, num_iteration=model.best_iteration)","3479c156":"submission_df = test.loc[:, ['id', 'sales']]\nsubmission_df['sales'] = np.expm1(test_preds)\nsubmission_df['id'] = submission_df.id.astype(int)\nsubmission_df.to_csv('submission.csv', index=False)","ad97664b":"Test dataset has 4 variables : id (which I'll need when submitting predicted sales values), store, item and date. There is no sales variable in test dataset for this is the variable that I will be predicting.\n\nTest dataset covers the period between 2018-01-01 and 2018-03-31, so I will be predicting 3 months of sales per each store and item per each day.\n\nThere are no missing values.\n","11a83a6b":"**Trend**:    \n\nA time series is said to have a trend when there is a persistent increasing or decreasing direction in the data.\n\n![TimeSeriesAnalysis_6.gif](attachment:TimeSeriesAnalysis_6.gif)","5027f05c":"<a id=\"section-ts\"><\/a>\n# 2.What is Time Series?\n\nTime series is a sequence of observations recorded at regular time intervals. It is composed of discrete-time periods that are ordered succesively. When each value of the time points are concatenated, these independent points will form a non-discrete data, reveal the correlations and by using this data it would be possible to have a prediction of what would be coming next. \n\nTime Series are mostly used for:\n\n* monitor sensor data\n* track assets (such as cars of a fleet, items in a store)\n* business forecasting\n* understand past behaviours (of customers, stocks, etc.)\n* predict and plan future\n* evaluate current accomplishment","9e0fe69c":"**SMAPE**:  \n\nSymmetric mean absolute percentage error (SMAPE or sMAPE) is an accuracy measure based on percentage (or relative) errors. Errors with higher values will be adjusted by diving it to the sum of the forecast and actual value's average.\n\n![smape.svg](attachment:smape.svg)","4a42e30e":"<a id=\"section-fifteen\"><\/a>\n## 3.11.Submit Prediction ","b5b02af4":"<a id=\"section-two\"><\/a>\n# 3.2.Exploratory Data Analysis","9bd22101":"# Table of Content\n\n1. [Introduction](#section-intro)\n2. [What is Time Series](#section-ts)\n3. [ML Project](#section-pro)\n    * 3.1.[Import Library and Load Dataset](#section-one)\n    * 3.2.[Exploratory Data Analysis](#section-two)\n    * 3.3.[Outlier Check](#section-three)    \n    * 3.4.[Time Series Decomposition](#section-four)\n    * 3.5.[Feature Engineering](#section-five)\n        * 3.5.1.[Random Noise](#section-six)  \n        * 3.5.2.[Lag\/Shifted Features](#section-seven)\n        * 3.5.3.[Rolling Mean\/Moving Average](#section-eight)\n        * 3.5.4.[Exponentially Weighted Mean Features](#section-nine)\n    * 3.6.[One Hot Encoding](#section-ten)\n    * 3.7.[Custom Cost Function](#section-eleven)\n    * 3.8.[Train-Validation Split](#section-twelve) \n    * 3.9.[Base Model](#section-thirteen)\n    * 3.10.[Final Model](#section-fourteen)\n    * 3.11.[Submit Prediction](#section-fifteen)","0dd9a5db":"**Feature Importances**:\n\nThese will be calculated based on split and gain values, that is how often they have been used for splitting the values and the gain of entropy.","cca44163":"<a id=\"section-four\"><\/a>\n# 3.4.Time Series Decomposition  \n\nIt is a statistical task that deconstructs a time series into several components, each representing one of the underlying categories of patterns\n","0b7baab1":"<a id=\"section-intro\"><\/a>\n# 1.Introduction","1410e414":"In this notebook I will be working with Kaggle's [Store Item Demand Forecasting Challange](https:\/\/www.kaggle.com\/c\/demand-forecasting-kernels-only\/overview) dataset in order to deep dive into Time Series Analysis.\n\nWhile exploring in the past in order to find clues for the future, I will be explaining the whole process step by step and also sharing useful theoretical informations that is needed in order to understand the nature of Time Series.","25859893":"<a id=\"section-ten\"><\/a>\n## 3.6.One-Hot Encoding\n\nI won't be using features such as \"day_of_year\" for one hot encoding, it would derive 365 features which won't be helpful for the model.","2ad89a2a":"# Time Series Using LightGBM with Explanations","50a71bd5":"id values are only present in test dataset whereas sales variables are only present in train dataset. \n\nThe reason for concatenating the train and test dataset is to gain insight and find patterns from the previous sales values and finally be able to have a better prediction.","10ec4628":"no outliers for sales variable.","d7862bcb":"<a id=\"section-six\"><\/a>\n## 3.5.1.Random Noise\n\nFor small datasets like this one, in order to avoid overfitting, random noise can be added to the values. I will add Gaussian random noise which is normally distributed with a standard deviation of 1 and mean of 0.","4402f4d7":"If the model begins to memorize the train dataset instead of learning it, the error will get lower but the model won't be able to have a good prediction of the validation set (because it didn't learn the patterns) so the error in validation will begin to increase.\n\nIn the below graphic, as variance increase, the errors in both train and dataset decrease until they reach to a certain point where the test errors no longer decrease but train errors are still decreasing. This is the point where overfitting begins. This is the point where the iteration process should be interrupted. This will be handled by a special hyperparameter within LightGBM.\n\nearly_stopping_rounds wil be given as a hyperparameter. In the specified time intervals, the iteration will be paused and the l1 and SMAPE of train and validation set will be calculated, at the point where the validation error will begin to increase while train error is still decreasing, the iteration process will be interrupted.","d25c7c86":"## Taking Log Values of Sales  \n\nIn regression problems using gradient descent optimisation, when the target value is higher, the number of iterations would decrease and so the computation time. In order to eliminate this problem, I will be taking the logaritmic values of the target value. ","1f2548ec":"<a id=\"section-fourteen\"><\/a>\n## 3.10.Final Model ","9088fcf7":"<a id=\"section-nine\"><\/a>\n## 3.5.4.Exponentially Weighted Mean Features  \n\nThe value in time t highly depends on the value in time t-1, so in order to have a better prediction, while computing the average value, the values should not be equally weighted.","48ca444e":"<a id=\"section-one\"><\/a>\n## 3.1.Import Library","06b99e30":"<a id=\"section-eleven\"><\/a>\n## 3.7.Custom Cost Function \n\nI will define a custom cost function which is based on SMAPE which reverses the log values and calculates the SMAPE.","b8ac1e31":"The values for the newly derived lag and rolling mean features will be NaN for most of the train part of the dataframe. This is normal as we are trying to find patterns in order to be able to predict the values in test dataset.","e5bbe75a":"<a id=\"section-eight\"><\/a>\n## 3.5.3.Rolling Mean \/ Moving Average  \n\nIn order to find out possible seasonalities, I will be creating moving averagesfor specified time intervals. This function takes the number of time given as window parameter and takes the average of the values, but one of the values is the value on this specific observation. In order to eliminate today's affect on moving average values, I will take 1 shift and use this function","a96fc080":"<a id=\"section-seven\"><\/a>\n## 3.5.2.Lag\/Shifted Features \n\nTime Series theory states that, the value in time: t highly depends on the value in time: t-1. That is why I will be shifting all the sales values by 1 and adding noise.","1ef2d987":"<a id=\"section-three\"><\/a>\n## 3.3.Outlier Check  \n\nTime for an outlier check!\n\nFor outlier detection, I will use IQR method with Q1 as 0.05% and Q3 as 0.95%. I will compute the low limit and up limit with IQR method and check if the sales variable contain values above\/below these limits. It will return boolean.\n\n","f8562170":"![1_UDF1AklqOs0zuCLr2dCV0Q.png](attachment:1_UDF1AklqOs0zuCLr2dCV0Q.png)","3f6f6950":"In this function, values will be shifted by the given lags (number of days to be used for calculation) and the values will be weighted (using the alpha value) and the mean weighted value is obtained.\nAlpha is a parameter that is between 0 and 1, when close to 1 the near past will be weighted more and oppositely when close to 0 the far past will be weighted more.","aeb5d394":"**Stationary**:   \n\nA time series is stationary if the statistical characteristics doen't change over time period. If so, it would be easy to predict the future because the mean, variance and covariance is present in our hands. In order to make the series stationary, different transformation approaches can be maintained.\n\n![stationary.png](attachment:stationary.png)","13358e31":"![model_complexity_error_training_test.jpeg](attachment:model_complexity_error_training_test.jpeg)","d9cad34b":"<a id=\"section-pro\"><\/a>\n# 3.ML Project","5446ff69":"## Overview of the Dataset  \n\nTrain dataset contains 5 years of sales data per 50 different items being sold in 10 different stores. It covers the period between 2013-01-01 and 2017-12-31. \n\nWe are expected to forecast the daily sales of each item in each store for the time interval of 2018-01-01 and 2018-03-31. I will use LightGBM where I will benefit from early stopping interval in order to avoid overfitting.","b9fb58a3":"<a id=\"section-twelve\"><\/a>\n##  3.8.Train-Validation Split\n\nValidation will be the exact same time period as test, but the year before. ","abaed62a":"<a id=\"section-thirteen\"><\/a>\n## 3.9.Base Model","d42fd1c7":"based on the:\n\n* 1st graph: the dataset is not stationary, it would be easier to have a future prediction simply by taking mean values if it was stationary,\n* 2nd graph: there is an increasing trend over time,\n* 3rd graph: a repeating pattern is observed, so there is seasonality- moving upwards on July.\n* 4th graph: residuals are decomposing randomly around 0, so the series is additive.","e9152de6":"**Seasonality**:    \n\nA seasonal pattern exists when a time series is influenced by seasonal factors. Seasonality occurs over a fixed and known period (e.g., the quarter of the year, the month, or day of the week).  \n\n![SV_Tmin.png](attachment:SV_Tmin.png)","2bade556":"## Load Dataset","b16c445b":"Train dataset has 4 variables : store, item, sales (target variable) and date. Date is the variable with which I will derive features to use for my ML model. This is why I parsed the date variable while reading csv.\n\nTrain dataset covers the period between 2013-01-01 and 2017-12-31, so it's 5 years' time.\n\nThere are no missing values, I'll check sales for outliers.","3fa4972d":"<a id=\"section-five\"><\/a>\n# 3.5.Feature Engineering\n\nIn order to search for seasonalities, date variable will be used to derive new features and different time periods will be created."}}