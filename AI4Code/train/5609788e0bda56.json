{"cell_type":{"3bd72be8":"code","f0a6d781":"code","43b640a8":"code","0b3b46c0":"code","c481e5d8":"code","4f6deeca":"code","3e3d7f2b":"code","9abb95ef":"code","9678a971":"code","afa5f693":"code","8d25d4ef":"code","cdc52bad":"code","4cb1cf52":"code","d289189d":"code","3c7837f0":"code","260ce065":"code","291fa788":"code","53b027b7":"code","b11efef0":"code","09d176cc":"code","2c46ca98":"code","96f96559":"code","9df03c46":"code","99cb4d1c":"code","e0cb57fb":"code","b4c058ea":"code","4a01ffc5":"code","2ff069d3":"code","7aaa4432":"code","a69362c3":"code","2cf53bfe":"code","2585dc31":"code","9f3ad333":"code","25927fb1":"code","b669734b":"code","2eaff194":"code","13706673":"code","1263f29b":"code","6ecf01f8":"code","b15d6b63":"code","58ae3f3f":"code","e5cd8f81":"code","1748cfe2":"code","a1c9bf50":"code","a0ef1c36":"code","6230af99":"code","1744ebf9":"code","84d1ca1a":"markdown","dfa5851a":"markdown","58d11037":"markdown","2253739a":"markdown","c52971ca":"markdown","d1515dad":"markdown","e5dcb32e":"markdown","9333a977":"markdown","410f05cf":"markdown"},"source":{"3bd72be8":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom pandas.plotting import parallel_coordinates\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn import metrics\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\n\nplt.style.use('ggplot') # make plots look better","f0a6d781":"df = pd.read_csv(\"..\/input\/iris\/Iris.csv\")","43b640a8":"df.head()","0b3b46c0":"df.describe()","c481e5d8":"df.info()","4f6deeca":"# class distribution\nprint(df.groupby('Species').size())","3e3d7f2b":"# Train-Test Split\ntrain, test = train_test_split(df, test_size = 0.4, stratify = df['Species'], random_state = 42)","9abb95ef":"# Hist\nfig, axs = plt.subplots(2, 2)\naxs[0,0].hist(train['SepalLengthCm'], bins = 10)\naxs[0,0].set_title('Sepal Length')\naxs[0,1].hist(train['SepalWidthCm'], bins = 10)\naxs[0,1].set_title('Sepal Width')\naxs[1,0].hist(train['PetalLengthCm'], bins = 10)\naxs[1,0].set_title('Petal Length')\naxs[1,1].hist(train['PetalWidthCm'], bins = 10)\naxs[1,1].set_title('Petal Width')\n# add some spacing between subplots\nfig.tight_layout(pad=1.0)","9678a971":"# Box Plot\nfig, axs = plt.subplots(2, 2, figsize=(10, 10))\nfeatures = [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]\nclasses = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\nsns.boxplot(x = 'Species', y = 'SepalLengthCm', data = train, order = classes, ax = axs[0,0], palette=\"Set1\")\nsns.boxplot(x = 'Species', y = 'SepalWidthCm', data = train, order = classes, ax = axs[0,1], palette=\"Set1\")\nsns.boxplot(x = 'Species', y = 'PetalLengthCm', data = train, order = classes, ax = axs[1,0], palette=\"Set1\")\nsns.boxplot(x = 'Species', y = 'PetalWidthCm', data = train,  order = classes, ax = axs[1,1], palette=\"Set1\")\n# add some spacing between subplots\nfig.tight_layout(pad=6.0)\n","afa5f693":"# Violin Plot SepalLengthCm\n# Violin Plot\nsns.violinplot(x=\"Species\", y=\"SepalLengthCm\", data=train, size=5, order = classes, palette=\"Set1\")","8d25d4ef":"sns.violinplot(x=\"Species\", y=\"PetalLengthCm\", data=train, size=5, order = classes, palette=\"Set1\")","cdc52bad":"sns.violinplot(x=\"Species\", y=\"PetalWidthCm\", data=train, size=5, order = classes, palette=\"Set1\")","4cb1cf52":"# Scatter plot\nsns.pairplot(train.iloc[1:, 1:], hue=\"Species\", height = 2, palette=\"Set1\");","d289189d":"# parallel coordinate plot\nparallel_coordinates(train.iloc[1:, 1:], \"Species\", color = ['red','blue', 'green']);","3c7837f0":"# Correlation Matrix\ncorrmat = train.corr()\nsns.heatmap(corrmat.iloc[1:, 1:], annot = True, square = True, )\nplt.xticks(rotation=45) \n\nplt.show()","260ce065":"# Separate class label and features\nX_train = train[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]\ny_train = train.Species\nX_test = test[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]\ny_test = test.Species","291fa788":"# Classification Tree\nmod_dt = DecisionTreeClassifier(max_depth = 3, random_state = 1)\nmod_dt.fit(X_train,y_train)\nprediction=mod_dt.predict(X_test)\nprint('The accuracy of the Decision Tree is',\"{:.3f}\".format(metrics.accuracy_score(prediction,y_test)))","53b027b7":"plt.figure(figsize = (10,8))\nplot_tree(mod_dt, feature_names = features, class_names = classes, filled = True);","b11efef0":"#Confusion Matrix\n# https:\/\/towardsdatascience.com\/exploring-classifiers-with-python-scikit-learn-iris-dataset-2bcb490d2e1b\n#Through this matrix, we see that there is one versicolor which we predict to be virginica.\ndisp = metrics.plot_confusion_matrix(mod_dt, X_test, y_test,\n                                 display_labels=classes,\n                                 cmap=plt.cm.Blues,\n                                 normalize=None)\ndisp.ax_.set_title('Decision Tree Confusion matrix, without normalization')","09d176cc":"sns.FacetGrid(df,  \n    hue=\"Species\", palette=\"Set1\").map(plt.scatter, \"SepalLengthCm\", \"SepalWidthCm\").add_legend()\n\nplt.show()","2c46ca98":"sns.FacetGrid(df,  \n    hue=\"Species\", palette=\"Set1\").map(plt.scatter, \"PetalLengthCm\", \"PetalWidthCm\").add_legend()\n\nplt.show()","96f96559":"labels = np.asarray(df.Species)","9df03c46":"# use LabelEncoder to label \"String\" Species as Numerical ones.\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(labels)\n\n# apply encoding to labels\nlabels = le.transform(labels)","99cb4d1c":"df.sample(5)","e0cb57fb":"labels","b4c058ea":"df_selected = df.drop(['SepalLengthCm', 'SepalWidthCm', \"Id\", \"Species\"], axis=1)","4a01ffc5":"df_features = df_selected.to_dict(orient='records')","2ff069d3":"df_features[:5]","7aaa4432":"from sklearn.feature_extraction import DictVectorizer\nvec = DictVectorizer()\nfeatures = vec.fit_transform(df_features).toarray()","a69362c3":"from sklearn.model_selection import train_test_split\n\nfeatures_train, features_test, labels_train, labels_test = train_test_split( features, labels, test_size=0.33, random_state=42)","2cf53bfe":"# Now Classify again\nfrom sklearn.ensemble import RandomForestClassifier\n\n# initialize\nclf = RandomForestClassifier()\n\n# train the classifier using the training data\nclf.fit(features_train, labels_train)","2585dc31":"# compute accuracy using test data\nacc_test = clf.score(features_test, labels_test)\n\nprint (\"Test Accuracy:\", acc_test)","9f3ad333":"# compute accuracy using training data\nacc_train = clf.score(features_train, labels_train)\n\nprint (\"Train Accuracy:\", acc_train)","25927fb1":"flower = [[5.2,0.9]]\nclass_code = clf.predict(flower)","b669734b":"class_code","2eaff194":"decoded_class = le.inverse_transform(class_code)\nprint (decoded_class) # ['Iris-versicolor']","13706673":"pred = clf.predict(features_test)","1263f29b":"#Evaluation\nfrom sklearn.metrics import recall_score, precision_score\n\nprecision = precision_score(labels_test, pred, average=\"weighted\")\nrecall = recall_score(labels_test, pred, average=\"weighted\")\n\nprint (\"Precision:\", precision) # Precision: 0.98125\nprint (\"Recall:\", recall) # Recall: 0.98","6ecf01f8":"clf = RandomForestClassifier(\n    min_samples_split=4,\n    criterion=\"entropy\"\n)\n","b15d6b63":"clf.fit(features_train, labels_train)","58ae3f3f":"acc_test = clf.score(features_test, labels_test)\nacc_train = clf.score(features_train, labels_train)\nprint (\"Train Accuracy:\", acc_train)\nprint (\"Test Accuracy:\", acc_test)","e5cd8f81":"pred = clf.predict(features_test)","1748cfe2":"precision = precision_score(labels_test, pred, average=\"weighted\")\nrecall = recall_score(labels_test, pred, average=\"weighted\")\n\nprint (\"Precision:\", precision)\nprint (\"Recall:\", recall)","a1c9bf50":"# Now SVC\nfrom sklearn.svm import SVC\nclf = SVC()","a0ef1c36":"clf.fit(features_train, labels_train)\n\n# find the accuracy of the model\nacc_test = clf.score(features_test, labels_test)\nacc_train = clf.score(features_train, labels_train)\nprint (\"Train Accuracy:\", acc_train)\nprint (\"Test Accuracy:\", acc_test)","6230af99":"# compute predictions on test features\npred = clf.predict(features_test)\n\n# predict our new unique iris flower\nflower = [[5.2,0.9]]\nclass_code = clf.predict(flower)\nclass_code","1744ebf9":"from sklearn.metrics import recall_score, precision_score\n\nprecision = precision_score(labels_test, pred, average=\"weighted\")\nrecall = recall_score(labels_test, pred, average=\"weighted\")\n\nprint (\"Precision:\", precision)\nprint (\"Recall:\", recall)","84d1ca1a":"### Let's enter some parameter and check again","dfa5851a":"## Classifiers","58d11037":"So Sepals features are not that affecting to classify 3 Species, but Petals are much. Though there seems some overlap in theboundary of iris-versicolor and iris-virginca. So keep that in mind during training.\n\nN.B. If correlating different features in order to select the best ones sounds like a lot of work, it should be noted that there are automated methods of doing this such as **kbest, and recursive feature elimination** both of which are available in sklearn.","2253739a":"## Exploratory Data Analysis","c52971ca":"so the Species are now changed as class: 0, 1 and 2. If we ever want to return to the string labels we can use **le.inverse_transform(labels)**.\nNow we drop the rest data.","d1515dad":"Maybe the model is slightly underfitting. But it's very little.<br>\nNow we predict for a new flower.","e5dcb32e":"Now we only have the columns PetalLengthCm and PetalWidthCm left.\n\nSince we want to use more than one column, we can't just simply use np.asarray(). Instead, we can use the to_dict() method together with sklearn's DictVectorizer.\n\n","9333a977":"## Let's go Deeper with Feature Selection\nWe want the least amount of features that gives us as much information about our data as possible.","410f05cf":"## Prepare Data\nAll the data is encoded in a DataFrame, but sklearn doesn't work with pandas' DataFrames, so we need to extract the features and labels and convert them into numpy arrays instead."}}