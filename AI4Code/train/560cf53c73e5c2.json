{"cell_type":{"b1ed20e6":"code","c853c4b1":"code","a0decfc7":"code","cc81742c":"code","708fa85a":"code","406d920c":"code","5193780f":"code","c3250229":"code","9fbed6ce":"code","f35befa6":"code","b3db16c3":"code","4af564f5":"code","be90942b":"code","d46159db":"markdown","45c8066d":"markdown","4e4821eb":"markdown","11c981ea":"markdown","b5901fb8":"markdown","c6e7ed26":"markdown","0c1237fa":"markdown","c4932ba4":"markdown","9e8fe9d9":"markdown","8a8eeaeb":"markdown","b04ea6f6":"markdown"},"source":{"b1ed20e6":"import numpy as np\nimport pandas as pd \nimport os\nimport cv2\nfrom matplotlib import pyplot as plt\nfrom kaggle_datasets import KaggleDatasets\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import layers as L\nfrom tensorflow.keras.utils import to_categorical\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn import metrics\n\n%matplotlib inline","c853c4b1":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU:', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nAUTO = tf.data.experimental.AUTOTUNE\nGCS_DS_PATH = KaggleDatasets().get_gcs_path()\n\nBATCH_SIZE = 8 * strategy.num_replicas_in_sync\nIMG_SIZE = 768\n\nprint('Batch size:', BATCH_SIZE)","a0decfc7":"train = pd.read_csv('\/kaggle\/input\/plant-pathology-2020-fgvc7\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/plant-pathology-2020-fgvc7\/test.csv')\nsub = pd.read_csv('\/kaggle\/input\/plant-pathology-2020-fgvc7\/sample_submission.csv')\n\nprint(train.head())\n\ntrain_path = train.image_id.apply(lambda x: f'{GCS_DS_PATH}\/images\/{x}.jpg').values\ntest_path = test.image_id.apply(lambda x: f'{GCS_DS_PATH}\/images\/{x}.jpg').values\ntrain_label = train.loc[:, 'healthy':].values\n\n# train_path, valid_path, train_label, valid_label = train_test_split(train_path, train_label,\n#                                                                     test_size=0.1, stratify=train_label)","cc81742c":"class_weight = compute_class_weight('balanced', np.unique(np.argmax(train_label, axis=1)), np.argmax(train_label, axis=1))\nplt.bar(range(4), class_weight)","708fa85a":"fig, ax = plt.subplots(2, 2)\nimg = cv2.imread('\/kaggle\/input\/plant-pathology-2020-fgvc7\/images\/Train_0.jpg')\nimg1 = cv2.imread('\/kaggle\/input\/plant-pathology-2020-fgvc7\/images\/Train_1.jpg')\nimg2 = cv2.imread('\/kaggle\/input\/plant-pathology-2020-fgvc7\/images\/Train_2.jpg')\nimg3 = cv2.imread('\/kaggle\/input\/plant-pathology-2020-fgvc7\/images\/Train_3.jpg')\nax[0, 0].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\nax[0, 1].imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))\nax[1, 0].imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))\nax[1, 1].imshow(cv2.cvtColor(img3, cv2.COLOR_BGR2RGB))","406d920c":"def decode_image(filename, label=None, image_size=(IMG_SIZE, IMG_SIZE)):\n    bits = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(bits, channels=3)\n    image = tf.cast(image, tf.float32) \/ 255.0\n    image = tf.image.resize(image, image_size)\n    \n    if label is None:\n        return image\n    else:\n        return image, label\n\ndef data_augment(image, label=None):\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    \n    if label is None:\n        return image\n    else:\n        return image, label","5193780f":"train_dataset = (\n    tf.data.TFRecordDataset\n    .from_tensor_slices((train_path, train_label))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .map(data_augment, num_parallel_calls=AUTO)\n    .cache()\n    .repeat()\n    .shuffle(1024)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\n# valid_dataset = (\n#     tf.data.TFRecordDataset\n#     .from_tensor_slices((valid_path, valid_label))\n#     .map(decode_image, num_parallel_calls=AUTO)\n#     .map(data_augment, num_parallel_calls=AUTO)\n#     .cache()\n#     .batch(BATCH_SIZE)\n#     .prefetch(AUTO)\n# )\n\ntest_dataset = (\n    tf.data.TFRecordDataset\n    .from_tensor_slices(test_path)\n    .map(decode_image, num_parallel_calls=AUTO)\n    .map(data_augment, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n)","c3250229":"EPOCHS = 40\nLR_START = 0.0001\nLR_MAX = 0.00005 * strategy.num_replicas_in_sync\nLR_MIN = 0.0001\nLR_RAMPUP_EPOCHS = 10\nLR_SUSTAIN_EPOCHS = 4\nLR_EXP_DECAY = .8\n\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) \/ LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n    return lr\n\nlr = tf.keras.callbacks.LearningRateScheduler(lrfn)\n\ny = [lrfn(x) for x in range(EPOCHS)]\nplt.plot(y)","9fbed6ce":"!pip install -q efficientnet","f35befa6":"from efficientnet.tfkeras import EfficientNetB7\n\nwith strategy.scope():\n    efn = EfficientNetB7(include_top=False, weights='noisy-student', pooling='avg', input_shape=(IMG_SIZE, IMG_SIZE, 3))\n\n    model = Sequential()\n    model.add(efn)\n    model.add(L.Dense(4, activation='softmax'))\n\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    print(model.summary())","b3db16c3":"mc = tf.keras.callbacks.ModelCheckpoint('weights.h5', monitor='loss', save_best_only=True, save_weights_only=True)\nhistory = model.fit(train_dataset, epochs=EPOCHS, callbacks=[lr, mc], steps_per_epoch=train_label.shape[0] \/\/ BATCH_SIZE)","4af564f5":"with strategy.scope():\n    model.load_weights('weights.h5')\n# valid_prob = model.predict(valid_dataset, verbose=1)\n# print(metrics.classification_report(np.argmax(valid_label, axis=1), np.argmax(valid_prob, axis=1)))\n# print(metrics.confusion_matrix(np.argmax(valid_label, axis=1), np.argmax(valid_prob, axis=1)))","be90942b":"probs = model.predict(test_dataset, verbose=1)\nsub.loc[:, 'healthy':] = probs\nsub.to_csv('submission.csv', index=False)\nsub.head()","d46159db":"# Decode images","45c8066d":"# Import Libraries","4e4821eb":"# Get class weights","11c981ea":"# Lets see some images","b5901fb8":"# Get train and test data","c6e7ed26":"# Model","0c1237fa":"# EfficientNetB7","c4932ba4":"# TPU setup","9e8fe9d9":"# Predict ","8a8eeaeb":"# Define the parameters","b04ea6f6":"# Plant Pathology 2020 - FGVC7\n\nThis is a fork of ndhpro's notebook: https:\/\/www.kaggle.com\/ndhpro\/plant-pathology-2020-efficientnet\n\nThis is practice for EfficientNetB7 on TPUs"}}