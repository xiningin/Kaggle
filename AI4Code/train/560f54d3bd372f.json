{"cell_type":{"fcec163b":"code","70d4dc97":"code","f4ecae1c":"code","5e847f0d":"code","ce5c4f71":"code","ac7473c8":"code","8b32eb3d":"code","c002ffe8":"code","86c2ce09":"code","c5824b9c":"code","e6cd431f":"code","598dbf96":"code","b661231b":"code","3076d4eb":"code","e44464a4":"code","eeac7ab6":"code","97b56892":"code","c2545bd1":"code","9465f08a":"code","a9e3013f":"code","4136dd62":"code","85cd1479":"code","246029e0":"code","193ac40e":"code","6d935655":"code","326cc576":"code","e38a6ba4":"code","528b9aa5":"code","1a81b467":"code","007a86ab":"markdown","565497de":"markdown","ae653493":"markdown","ae1528a9":"markdown","f0f91fa2":"markdown","88646269":"markdown","46dd47ed":"markdown","9cb802c2":"markdown","95818bf8":"markdown","b93e323c":"markdown"},"source":{"fcec163b":"from itertools import groupby\nimport numpy as np\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\nimport pandas as pd\nimport os\nimport pickle\nimport cv2\nfrom multiprocessing import Pool\nimport matplotlib.pyplot as plt\n# import cupy as cp\nimport ast\nimport glob\n\nimport shutil\nimport sys\nsys.path.append('..\/input\/tensorflow-great-barrier-reef')\n\nfrom joblib import Parallel, delayed\n\nfrom IPython.display import display, HTML\n\nfrom matplotlib import animation, rc\nrc('animation', html='jshtml')\n\n%matplotlib inline","70d4dc97":"os.listdir('\/kaggle\/input\/great-barrier-reef-yolov5-train\/')","f4ecae1c":"VAL_TXT = '\/kaggle\/input\/great-barrier-reef-yolov5-train\/val.txt'\nTRAIN_TXT = '\/kaggle\/input\/great-barrier-reef-yolov5-train\/train.txt'\nBEST_PT = '\/kaggle\/input\/great-barrier-reef-yolov5-train\/yolov5\/runs\/train\/exp\/weights\/best.pt' # best wegith of yolov5 model trained in great-barrier-reef\n","5e847f0d":"ROOT_DIR  = '\/kaggle\/input\/tensorflow-great-barrier-reef\/'\nIMAGE_DIR = '\/kaggle\/images\/' # directory to save images\nLABEL_DIR = '\/kaggle\/labels\/' # directory to save labels\n!mkdir -p {IMAGE_DIR}\n!mkdir -p {LABEL_DIR}","ce5c4f71":"def get_path(row):\n    row['old_image_path'] = f'{ROOT_DIR}\/train_images\/video_{row.video_id}\/{row.video_frame}.jpg'\n    row['image_path'] = f'{IMAGE_DIR}\/video_{row.video_id}_{row.video_frame}.jpg'\n    row['label_path'] = f'{LABEL_DIR}\/video_{row.video_id}_{row.video_frame}.txt'\n    return row","ac7473c8":"# Train Data\ndf = pd.read_csv(f'{ROOT_DIR}\/train.csv')\ndf = df.progress_apply(get_path, axis=1)\ndf['annotations'] = df['annotations'].progress_apply(lambda x: ast.literal_eval(x))\ndisplay(df.head(2))","8b32eb3d":"df['num_bbox'] = df['annotations'].progress_apply(lambda x: len(x))\ndata = (df.num_bbox>0).value_counts(normalize=True)*100\nprint(f\"No BBox: {data[0]:0.2f}% | With BBox: {data[1]:0.2f}%\")","c002ffe8":"REMOVE_NOBBOX=True\nif REMOVE_NOBBOX:\n    df = df.query(\"num_bbox>0\")","86c2ce09":"def make_copy(path):\n    data = path.split('\/')\n    filename = data[-1]\n    video_id = data[-2]\n    new_path = os.path.join(IMAGE_DIR,f'{video_id}_{filename}')\n    shutil.copy(path, new_path)\n    return","c5824b9c":"image_paths = df.old_image_path.tolist()\n_ = Parallel(n_jobs=-1, backend='threading')(delayed(make_copy)(path) for path in tqdm(image_paths))","e6cd431f":"def voc2yolo(image_height, image_width, bboxes):\n    \"\"\"\n    voc  => [x1, y1, x2, y1]\n    yolo => [xmid, ymid, w, h] (normalized)\n    \"\"\"\n    \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    bboxes[..., [0, 2]] = bboxes[..., [0, 2]]\/ image_width\n    bboxes[..., [1, 3]] = bboxes[..., [1, 3]]\/ image_height\n    \n    w = bboxes[..., 2] - bboxes[..., 0]\n    h = bboxes[..., 3] - bboxes[..., 1]\n    \n    print(img_name)\n    bboxes[..., 0] = bboxes[..., 0] + w\/2\n    bboxes[..., 1] = bboxes[..., 1] + h\/2\n    bboxes[..., 2] = w\n    bboxes[..., 3] = h\n    \n    return bboxes\n\ndef yolo2voc(image_height, image_width, bboxes):\n    \"\"\"\n    yolo => [xmid, ymid, w, h] (normalized)\n    voc  => [x1, y1, x2, y1]\n    \n    \"\"\" \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    bboxes[..., [0, 2]] = bboxes[..., [0, 2]]* image_width\n    bboxes[..., [1, 3]] = bboxes[..., [1, 3]]* image_height\n    \n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]\/2\n    bboxes[..., [2, 3]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]\n    \n    return bboxes\n\ndef coco2yolo(image_height, image_width, bboxes):\n    \"\"\"\n    coco => [xmin, ymin, w, h]\n    yolo => [xmid, ymid, w, h] (normalized)\n    \"\"\"\n    \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    # normolizinig\n    bboxes[..., [0, 2]]= bboxes[..., [0, 2]]\/ image_width\n    bboxes[..., [1, 3]]= bboxes[..., [1, 3]]\/ image_height\n    \n    # converstion (xmin, ymin) => (xmid, ymid)\n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]\/2\n    \n    return bboxes\n\ndef yolo2coco(image_height, image_width, bboxes):\n    \"\"\"\n    yolo => [xmid, ymid, w, h] (normalized)\n    coco => [xmin, ymin, w, h]\n    \n    \"\"\" \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    # denormalizing\n    bboxes[..., [0, 2]]= bboxes[..., [0, 2]]* image_width\n    bboxes[..., [1, 3]]= bboxes[..., [1, 3]]* image_height\n    \n    # converstion (xmid, ymid) => (xmin, ymin) \n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]\/2\n    \n    return bboxes\n\n\ndef load_image(image_path):\n    return cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n\n\ndef plot_one_box(x, img, color=None, label=None, line_thickness=None):\n    # Plots one bounding box on image img\n    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) \/ 2) + 1  # line\/font thickness\n    color = color or [random.randint(0, 255) for _ in range(3)]\n    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n    if label:\n        tf = max(tl - 1, 1)  # font thickness\n        t_size = cv2.getTextSize(label, 0, fontScale=tl \/ 3, thickness=tf)[0]\n        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n        cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled\n        cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl \/ 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n\ndef draw_bboxes(img, bboxes, classes, class_ids, colors = None, show_classes = None, bbox_format = 'yolo', class_name = False, line_thickness = 2):  \n     \n    image = img.copy()\n    show_classes = classes if show_classes is None else show_classes\n    colors = (0, 255 ,0) if colors is None else colors\n    \n    if bbox_format == 'yolo':\n        \n        for idx in range(len(bboxes)):  \n            \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            color = colors[cls_id] if type(colors) is list else colors\n            \n            if cls in show_classes:\n            \n                x1 = round(float(bbox[0])*image.shape[1])\n                y1 = round(float(bbox[1])*image.shape[0])\n                w  = round(float(bbox[2])*image.shape[1]\/2) #w\/2 \n                h  = round(float(bbox[3])*image.shape[0]\/2)\n\n                voc_bbox = (x1-w, y1-h, x1+w, y1+h)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(get_label(cls)),\n                             line_thickness = line_thickness)\n            \n    elif bbox_format == 'coco':\n        \n        for idx in range(len(bboxes)):  \n            \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            color = colors[cls_id] if type(colors) is list else colors\n            \n            if cls in show_classes:            \n                x1 = int(round(bbox[0]))\n                y1 = int(round(bbox[1]))\n                w  = int(round(bbox[2]))\n                h  = int(round(bbox[3]))\n\n                voc_bbox = (x1, y1, x1+w, y1+h)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(cls_id),\n                             line_thickness = line_thickness)\n\n    elif bbox_format == 'voc_pascal':\n        \n        for idx in range(len(bboxes)):  \n            \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            color = colors[cls_id] if type(colors) is list else colors\n            \n            if cls in show_classes: \n                x1 = int(round(bbox[0]))\n                y1 = int(round(bbox[1]))\n                x2 = int(round(bbox[2]))\n                y2 = int(round(bbox[3]))\n                voc_bbox = (x1, y1, x2, y2)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(cls_id),\n                             line_thickness = line_thickness)\n    else:\n        raise ValueError('wrong bbox format')\n\n    return image\n\ndef get_bbox(annots):\n    bboxes = [list(annot.values()) for annot in annots]\n    return bboxes\n\ndef get_imgsize(row):\n    row['width'], row['height'] = imagesize.get(row['image_path'])\n    return row\n\n\n# https:\/\/www.kaggle.com\/diegoalejogm\/great-barrier-reefs-eda-with-animations\ndef create_animation(ims):\n    fig = plt.figure(figsize=(16, 12))\n    plt.axis('off')\n    im = plt.imshow(ims[0])\n\n    def animate_func(i):\n        im.set_array(ims[i])\n        return [im]\n\n    return animation.FuncAnimation(fig, animate_func, frames = len(ims), interval = 1000\/\/12)\n\nnp.random.seed(32)\ncolors = [(np.random.randint(255), np.random.randint(255), np.random.randint(255))\\\n          for idx in range(1)]","598dbf96":"df['bboxes'] = df.annotations.progress_apply(get_bbox)\ndf.head(2)","b661231b":"df['width']  = 1280\ndf['height'] = 720\ndisplay(df.head(2))","3076d4eb":"cnt = 0\nall_bboxes = []\nfor row_idx in tqdm(range(df.shape[0])):\n    row = df.iloc[row_idx]\n    image_height = row.height\n    image_width  = row.width\n    bboxes_coco  = np.array(row.bboxes).astype(np.float32).copy()\n    num_bbox     = len(bboxes_coco)\n    names        = ['cots']*num_bbox\n    labels       = [0]*num_bbox\n    ## Create Annotation(YOLO)\n    with open(row.label_path, 'w') as f:\n        if num_bbox<1:\n            annot = ''\n            f.write(annot)\n            cnt+=1\n            continue\n        bboxes_yolo  = coco2yolo(image_height, image_width, bboxes_coco)\n        bboxes_yolo  = np.clip(bboxes_yolo, 0, 1)\n        all_bboxes.extend(bboxes_yolo)\n        for bbox_idx in range(len(bboxes_yolo)):\n            annot = [str(labels[bbox_idx])]+ list(bboxes_yolo[bbox_idx].astype(str))+(['\\n'] if num_bbox!=(bbox_idx+1) else [''])\n            annot = ' '.join(annot)\n            annot = annot.strip(' ')\n            f.write(annot)\nprint('Missing:',cnt)","e44464a4":"# images, labels folder check\nassert os.path.exists(IMAGE_DIR)\nassert os.path.exists(LABEL_DIR)","eeac7ab6":"%cd \/kaggle\/working\n!rm -r \/kaggle\/working\/yolov5\n!git clone https:\/\/github.com\/ultralytics\/yolov5 # clone\n%cd yolov5\n%pip install -qr requirements.txt  # install\n\nfrom yolov5 import utils\ndisplay = utils.notebook_init()  # check","97b56892":"!cd yolov5","c2545bd1":"os.listdir('\/kaggle\/input\/great-barrier-reef-yolov5-train\/yolov5\/runs\/train\/exp\/weights\/')","9465f08a":"# move files to \/kaggle\/working\n!cp \/kaggle\/input\/great-barrier-reef-yolov5-train\/yolov5\/runs\/train\/exp\/weights\/best.pt \/kaggle\/working\/\n!cp \/kaggle\/input\/great-barrier-reef-yolov5-train\/train.txt \/kaggle\/working\/\n!cp \/kaggle\/input\/great-barrier-reef-yolov5-train\/val.txt \/kaggle\/working\/\n!cp \/kaggle\/input\/great-barrier-reef-yolov5-train\/bgr.yaml \/kaggle\/working\/","a9e3013f":"os.listdir('\/kaggle\/working')","4136dd62":"import val\n!python val.py --data ..\/bgr.yaml\\\n    --weights ..\/best.pt\\\n    --imgsz 1280\\\n    --conf-thres 0.01\\\n    --iou-thres 0.3\\\n    --save-txt\\\n    --save-conf\\\n    --exist-ok","85cd1479":"# val bbox result directory\nPRD_BBOX_DIR = '\/kaggle\/working\/yolov5\/runs\/val\/exp\/labels\/'\nprint(f'made bounding box of {len(os.listdir(PRD_BBOX_DIR))} images in validation set ')","246029e0":"val_images = []\nwith open('\/kaggle\/working\/val.txt', 'r') as f:\n    while True:\n        r = f.readline().rstrip()\n        if not r:\n            break\n        val_images.append(os.path.basename(r))\nprint(f'{len(val_images)} image in validation set')","193ac40e":"not_processed_images = val_images.copy()\nfor file in os.listdir(PRD_BBOX_DIR):\n    img_name = file[:-4]+'.jpg'\n    if img_name in val_images:\n        not_processed_images.remove(img_name)\nprint(f\"yolov5 model doesn't create bounding box for {len(not_processed_images)} images\")","6d935655":"# model didn't detect starfish in \"not_processed_images\" - it will be calculated as False Negative(FN)\n# run code to know that there exist ground truth bounding boxs in \"not_processed_images\"\n# in fact, \/kaggle\/images\/ only include images which have bounding boxs\nfor image_name in not_processed_images[:20]:\n    img = cv2.imread('\/kaggle\/images\/'+image_name)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.imshow(img)\n    plt.title(image_name)\n    plt.show()\n    txt_name = image_name[:-4]+'.txt'\n    with open('\/kaggle\/labels\/'+txt_name, 'r') as f:\n        r = f.read()\n        count = r.count('\\n')+1\n        print(f\"{count} ground truth bounding box exits\")","326cc576":"def calc_iou(bboxes1, bboxes2, bbox_mode='xywh'):\n    assert len(bboxes1.shape) == 2 and bboxes1.shape[1] == 4\n    assert len(bboxes2.shape) == 2 and bboxes2.shape[1] == 4\n    \n    bboxes1 = bboxes1.copy()\n    bboxes2 = bboxes2.copy()\n    \n    if bbox_mode == 'xywh':\n        bboxes1[:, 2:] += bboxes1[:, :2]\n        bboxes2[:, 2:] += bboxes2[:, :2]\n\n    x11, y11, x12, y12 = np.split(bboxes1, 4, axis=1)\n    x21, y21, x22, y22 = np.split(bboxes2, 4, axis=1)\n    xA = np.maximum(x11, np.transpose(x21))\n    yA = np.maximum(y11, np.transpose(y21))\n    xB = np.minimum(x12, np.transpose(x22))\n    yB = np.minimum(y12, np.transpose(y22))\n    interArea = np.maximum((xB - xA + 1), 0) * np.maximum((yB - yA + 1), 0)\n    boxAArea = (x12 - x11 + 1) * (y12 - y11 + 1)\n    boxBArea = (x22 - x21 + 1) * (y22 - y21 + 1)\n    iou = interArea \/ (boxAArea + np.transpose(boxBArea) - interArea)\n    return iou\n\ndef f_beta(tp, fp, fn, beta=2):\n    return (1+beta**2)*tp \/ ((1+beta**2)*tp + beta**2*fn+fp)\n\ndef calc_is_correct_at_iou_th(gt_bboxes, pred_bboxes, iou_th, verbose=False):\n    gt_bboxes = gt_bboxes.copy()\n    pred_bboxes = pred_bboxes.copy()\n    \n    tp = 0\n    fp = 0\n    for k, pred_bbox in enumerate(pred_bboxes): # fixed in ver.7\n        ious = calc_iou(gt_bboxes, pred_bbox[None, 1:])\n        max_iou = ious.max()\n        if max_iou > iou_th:\n            tp += 1\n            gt_bboxes = np.delete(gt_bboxes, ious.argmax(), axis=0)\n        else:\n            fp += 1\n        if len(gt_bboxes) == 0:\n            fp += len(pred_bboxes) - (k + 1) # fix in ver.7\n            break\n\n    fn = len(gt_bboxes)\n    return tp, fp, fn\n\ndef calc_is_correct(gt_bboxes, pred_bboxes, iou_th=0.5):\n    \"\"\"\n    gt_bboxes: (N, 4) np.array in xywh format\n    pred_bboxes: (N, 5) np.array in conf+xywh format\n    \"\"\"\n    if len(gt_bboxes) == 0 and len(pred_bboxes) == 0:\n        tps, fps, fns = 0, 0, 0\n        return tps, fps, fns\n\n    elif len(gt_bboxes) == 0:\n        tps, fps, fns = 0, len(pred_bboxes), 0\n        return tps, fps, fns\n\n    elif len(pred_bboxes) == 0:\n        tps, fps, fns = 0, 0, len(gt_bboxes)\n        return tps, fps, fns\n\n    pred_bboxes = pred_bboxes[pred_bboxes[:,0].argsort()[::-1]] # sort by conf\n\n    tps, fps, fns = 0, 0, 0\n    tp, fp, fn = calc_is_correct_at_iou_th(gt_bboxes, pred_bboxes, iou_th)\n    tps += tp\n    fps += fp\n    fns += fn\n    return tps, fps, fns\n\ndef calc_f2_score(gt_bboxes_list, pred_bboxes_list, verbose=False):\n    \"\"\"\n    gt_bboxes_list: list of (N, 4) np.array in xywh format\n    pred_bboxes_list: list of (N, 5) np.array in conf+xywh format\n    \"\"\"\n    f2s = []\n    for iou_th in np.arange(0.3, 0.85, 0.05):\n        tps, fps, fns = 0, 0, 0\n        for gt_bboxes, pred_bboxes in zip(gt_bboxes_list, pred_bboxes_list):\n            tp, fp, fn = calc_is_correct(gt_bboxes, pred_bboxes, iou_th)\n            tps += tp\n            fps += fp\n            fns += fn\n            if verbose:\n                num_gt = len(gt_bboxes)\n                num_pred = len(pred_bboxes)\n                print(f'num_gt:{num_gt:<3} num_pred:{num_pred:<3} tp:{tp:<3} fp:{fp:<3} fn:{fn:<3}')\n        f2 = f_beta(tps, fps, fns, beta=2)    \n        print(f'f2@{iou_th}:{f2}')\n        f2s.append(f2)\n    return np.mean(f2s)","e38a6ba4":"gt_bboxs_list, prd_bboxs_list = [], []\ncount = 0\nfor image_file in val_images:\n    txt_name = image_file[:-4]+'.txt'\n    gt_bboxs = []\n    prd_bboxs = []\n    with open(LABEL_DIR+txt_name, 'r') as f:\n        while True:\n            r = f.readline().rstrip()\n            if not r:\n                break\n            r = r.split()[1:]\n            bbox = np.array(list(map(float, r)))\n            gt_bboxs.append(bbox)\n    if os.path.exists(PRD_BBOX_DIR+txt_name):\n        with open(PRD_BBOX_DIR+txt_name, 'r') as f:\n            while True:\n                r = f.readline().rstrip()\n                if not r:\n                    break\n                r = r.split()[1:]\n                r = [r[4], *r[:4]]\n                bbox = np.array(list(map(float, r)))\n                prd_bboxs.append(bbox)\n    gt_bboxs, prd_bboxs = np.array(gt_bboxs), np.array(prd_bboxs)\n    gt_bboxs_list.append(gt_bboxs)\n    prd_bboxs_list.append(prd_bboxs)\n    count += 1\nprint(f'{count} bound boxs appended to list')","528b9aa5":"score = calc_f2_score(gt_bboxs_list, prd_bboxs_list, verbose=False)","1a81b467":"print(f'f2 score for validation set is {score}')","007a86ab":"### why predicted bounding box txt file for some images doesn't exist?","565497de":"## Make images, labels directory on \/kaggle\nrefer to https:\/\/www.kaggle.com\/awsaf49\/great-barrier-reef-yolov5-train","ae653493":"## \ud83d\udcd2 inferecne Notebooks:\n* Train: [Great-Barrier-Reef: YOLOv5 [train] \ud83c\udf0a](https:\/\/www.kaggle.com\/awsaf49\/great-barrier-reef-yolov5-train)\n* Infer: [Great-Barrier-Reef: YOLOv5 [infer] \ud83c\udf0a](https:\/\/www.kaggle.com\/awsaf49\/great-barrier-reef-yolov5-infer)\n* F2 score : [competition metric implementation](https:\/\/www.kaggle.com\/bamps53\/competition-metric-implementation)","ae1528a9":"## Check how predicted bounding box is created","f0f91fa2":"## YOLOV5 install","88646269":"## Calculate F2 score on validation set\nreference : [competition metric implementation](https:\/\/www.kaggle.com\/bamps53\/competition-metric-implementation)","46dd47ed":"model didn't detect starfish in \"not_processed_images\" - it will be calculated as False Negative(FN)\n\nrun code to know that there exist ground truth bounding boxs in \"not_processed_images\"","9cb802c2":"## Import Library","95818bf8":"## Define model weight, validation images, labels","b93e323c":"## Run YOLOV5 model in validation images "}}