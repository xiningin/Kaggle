{"cell_type":{"41a6bbd6":"code","815ce9f3":"code","3c6849e7":"code","cc03b15e":"code","603491bd":"code","79cbbaf7":"code","302a3101":"code","5ee940bd":"code","4f021e70":"code","9224020a":"code","1afefff3":"code","16012066":"code","293ff6b6":"code","3c1b4001":"code","11426fcb":"code","0798b4c6":"code","e43e8351":"code","d7eb6de6":"code","ae5d74ad":"code","4c1f2353":"code","383128ef":"code","29bf408d":"code","9f231c0d":"code","c829d98f":"code","42a7274f":"code","27e2990b":"code","c03494fd":"code","fdccab16":"code","38a7dd09":"code","833bcc9f":"code","c8eda01a":"code","d0ee0944":"code","229c8373":"code","a78a5401":"code","2fc7e14e":"code","cbc97c7b":"code","00bcd420":"code","001d1d9a":"code","e71e22f0":"code","ab5ce084":"code","c9bfb1cd":"code","2e61105d":"code","19f70b12":"code","0ea80180":"code","e9606e9e":"code","91d2a7d7":"code","ce598c40":"code","e483a15c":"code","a8321949":"code","74c11493":"code","52ff8ab2":"code","f195cca7":"code","b335365a":"code","2f72e8ec":"code","c415f0ee":"code","9384e05f":"code","ccd98924":"code","cc2048c7":"code","4fb1cbd2":"code","a1a56aaa":"code","5093a831":"code","ac29b67e":"code","03c2139e":"code","f879881b":"code","1c1c5e3c":"code","fb3122b2":"code","9e3088d5":"code","60c01bea":"code","ea9cf7ae":"code","f03b58b6":"code","7211a218":"code","05dbd2f9":"code","819336e3":"code","15ebabad":"code","1f366085":"code","13cf1413":"code","7d85f87c":"code","5fa95320":"code","8d662bd2":"code","fcfaba4d":"code","35cfebe5":"code","c485af48":"code","ca4ab1bf":"code","5fab6e8d":"code","cfb04e55":"code","57ba5bea":"code","d4aac534":"code","5e8a98f9":"code","fa0f3542":"code","95a8b762":"code","6efc06ed":"code","19e2f9cf":"code","8cdcfb01":"code","5431f227":"code","1e6a4fb6":"code","7cdb180b":"code","61ba495d":"code","5e3d004f":"code","0429f59a":"code","83612522":"code","78c899d4":"code","c54585f2":"code","6d1d5182":"code","55115d4c":"code","4bd3122c":"code","fdc1721a":"code","5136bca0":"code","607b9151":"code","27907dba":"code","05333b6e":"code","0b8e9c88":"code","6d932a3f":"code","d8ea3a08":"code","6c26f573":"code","2dedb0f4":"code","789b0a04":"code","a464469f":"code","2072dcd6":"code","33f7a554":"code","cca853d9":"code","bb35a565":"code","56f7dd91":"code","00995e47":"code","4e98f356":"code","5d5e902a":"code","7061a9c7":"code","b0655a7e":"code","8fa0d2dc":"code","498a0837":"code","87473c1d":"code","2edc8ecf":"code","b9616ffe":"code","eb18a7ca":"code","38ce2583":"code","0495c858":"code","6874fa12":"code","c2b2ea1e":"code","e8f4eb70":"code","2891f295":"code","94c46a3b":"code","c94082c2":"code","c63480e2":"code","92f46854":"code","389ba615":"code","d60be5dd":"code","2c08b13f":"code","016c0ee6":"code","10fba7db":"code","706879d5":"code","0ff6a077":"code","149b7a81":"code","7b0867f9":"code","d96a79a7":"markdown","c8840ce6":"markdown","3c68eb21":"markdown","829d0a1f":"markdown","4631275b":"markdown","c09500b7":"markdown","3d259a0a":"markdown","f4d9ec7c":"markdown","cc325cec":"markdown","69834292":"markdown","3a6be935":"markdown","a67bed52":"markdown","b394c57f":"markdown","6f880963":"markdown","cd6beccd":"markdown","30e7c27e":"markdown","d90e53e6":"markdown","9696d90c":"markdown","b801a96b":"markdown","b1c42a59":"markdown","a477a8b2":"markdown","1e442d69":"markdown","92e163ca":"markdown","0481098c":"markdown","d938f8c4":"markdown","14531e00":"markdown","d670bbe6":"markdown","2ad95099":"markdown","968c3caa":"markdown","9ec42d66":"markdown","39780d94":"markdown","cd7b50c7":"markdown","6ca0ebbd":"markdown","5bcaf22e":"markdown","7733a6c6":"markdown","e4988e09":"markdown","c028177a":"markdown","bd008ea0":"markdown","44dd9f84":"markdown","4663b187":"markdown","d342b536":"markdown","f3930358":"markdown","0d4bed85":"markdown","5c4acf74":"markdown","a8f08c11":"markdown","187efacd":"markdown","7f2579f6":"markdown","a0787e81":"markdown","0faf4857":"markdown","9506e920":"markdown","9d0dfc4d":"markdown","9ef97a0b":"markdown","2f2c2a9c":"markdown","6f8e31a3":"markdown","d6c55265":"markdown","e929aeaf":"markdown","71018eb5":"markdown","075a1588":"markdown","fe37114b":"markdown","60b957f6":"markdown","f395b6d1":"markdown","cc90f191":"markdown","93fa85b2":"markdown","316e0bcb":"markdown","1e0d1e62":"markdown","827ea20d":"markdown","3ccddad3":"markdown","1b0e8fc6":"markdown","7b038be7":"markdown","5048297f":"markdown","a33c4104":"markdown","a63e9555":"markdown","6629eb8c":"markdown","e210706c":"markdown"},"source":{"41a6bbd6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nimport matplotlib.pyplot as plt\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\nplt.style.use('fivethirtyeight')\n\nplt.figure(figsize=(25,25))\n\nimport pandas_profiling as pp\n\n# Any results you write to the current directory are saved as output.","815ce9f3":"import gc\ngc.collect()","3c6849e7":"# !pip install pretrainedmodels\n\n%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\n!pip install fastai==1.0.57\nimport fastai\n\nfrom fastai import *\nfrom fastai.vision import *\nfrom fastai.tabular import *\n\n# from torchvision.models import *\n# import pretrainedmodels\n\nfrom utils import *\nimport sys\n\nfrom fastai.callbacks.hooks import *\n\nfrom fastai.callbacks.tracker import EarlyStoppingCallback\nfrom fastai.callbacks.tracker import SaveModelCallback","cc03b15e":"import pandas as pd \nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\nfrom sklearn.manifold import TSNE\n\nfrom sklearn.experimental import enable_hist_gradient_boosting \nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\nimport gc\nfrom datetime import datetime \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn import svm\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier  \nfrom xgboost.sklearn import XGBRegressor\n\nfrom scipy.special import erfinv\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch.utils.data import *\nfrom torch.optim import *\nfrom fastai.tabular import *\nimport torch.utils.data as Data\nfrom fastai.basics import *\nfrom fastai.callbacks.hooks import *\nfrom tqdm import tqdm_notebook as tqdm\n\nfrom hyperopt import hp, tpe\nfrom hyperopt.fmin import fmin\nfrom hyperopt import STATUS_OK\n\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom sklearn.metrics import make_scorer\n\nimport warnings\nwarnings.filterwarnings('ignore')","603491bd":"def to_gauss(x): return np.sqrt(2)*erfinv(x)  #from scipy\n\ndef normalize(data, exclude=None):\n    # if not binary, normalize it\n    norm_cols = [n for n, c in data.drop(exclude, 1).items() if len(np.unique(c)) > 2]\n    n = data.shape[0]\n    for col in norm_cols:\n        sorted_idx = data[col].sort_values().index.tolist()# list of sorted index\n        uniform = np.linspace(start=-0.99, stop=0.99, num=n) # linsapce\n        normal = to_gauss(uniform) # apply gauss to linspace\n        normalized_col = pd.Series(index=sorted_idx, data=normal) # sorted idx and normalized space\n        data[col] = normalized_col # column receives its corresponding rank\n    return data","79cbbaf7":"df_all = pd.read_csv('..\/input\/train.csv')","302a3101":"df_all.head()","5ee940bd":"df_all.shape","4f021e70":"df_all.columns","9224020a":"df_all['Response'].value_counts()","1afefff3":"sns.set_color_codes()\nplt.figure(figsize=(12,12))\nsns.countplot(df_all.Response).set_title('Dist of Response variables')","16012066":"df_all.describe()","293ff6b6":"df_all.dtypes","3c1b4001":"df_all.shape","11426fcb":"f, axes = plt.subplots(1, 2, figsize=(10,5))\nsns.boxplot(x = 'BMI', data=df_all,  orient='v' , ax=axes[0])\nsns.distplot(df_all['BMI'],  ax=axes[1])","0798b4c6":"f, axes = plt.subplots(1, 2, figsize=(10,5))\nsns.boxplot(x = 'Ins_Age', data=df_all,  orient='v' , ax=axes[0])\nsns.distplot(df_all['Ins_Age'],  ax=axes[1])","e43e8351":"f, axes = plt.subplots(1, 2, figsize=(10,5))\nsns.boxplot(x = 'Ht', data=df_all,  orient='v' , ax=axes[0])\nsns.distplot(df_all['Ht'],  ax=axes[1])","d7eb6de6":"f, axes = plt.subplots(1, 2, figsize=(10,5))\nsns.boxplot(x = 'Wt', data=df_all,  orient='v' , ax=axes[0])\nsns.distplot(df_all['Wt'],  ax=axes[1])","ae5d74ad":"#create a funtion to create new target variable based on conditions\n# 0 means reject\n# 1 means accept\n\ndef new_target(row):\n    if (row['Response']<=7):\n        val=0\n    elif (row['Response']==8):\n        val=1\n    else:\n        val=2\n    return val\n\n\n#create a copy of original dataset\nnew_data=df_all.copy()\n\n#create a new column\nnew_data['Final_Response']=new_data.apply(new_target,axis=1)\n\n#print unique values of target variable\nprint(\"Unique values in Target Variable: {}\".format(new_data.Final_Response.dtype))\nprint(\"Unique values in Target Variable: {}\".format(new_data.Final_Response.unique()))\nprint(\"Total Number of unique values : {}\".format(len(new_data.Final_Response.unique())))\n\n#distribution plot for target classes\nsns.countplot(x=new_data.Final_Response).set_title('Distribution of rows by response categories')","4c1f2353":"new_data.drop(['Response'], axis=1, inplace=True)\ndf_all = new_data\ndel new_data","383128ef":"df_all.rename(columns={'Final_Response': 'Response'}, inplace=True)","29bf408d":"#1\ndf_all['Product_Info_2_char'] = df_all.Product_Info_2.str[0]\n#2\ndf_all['Product_Info_2_num'] = df_all.Product_Info_2.str[1]\n\n#3\ndf_all['BMI_Age'] = df_all['BMI'] * df_all['Ins_Age']\n#4\ndf_all['Age_Wt'] = df_all['Ins_Age'] * df_all['Wt']\n#5\ndf_all['Age_Ht'] = df_all['Ins_Age'] * df_all['Ht']\n\nmed_keyword_columns = df_all.columns[df_all.columns.str.startswith('Medical_Keyword_')]\n#6\ndf_all['Med_Keywords_Count'] = df_all[med_keyword_columns].sum(axis=1)\n\n#7\ndf_all['Ins_Age_sq'] = df_all['Ins_Age'] * df_all['Ins_Age']\n#8\ndf_all['Ht_sq'] = df_all['Ht'] * df_all['Ht']\n#9\ndf_all['Wt_sq'] = df_all['Wt'] * df_all['Wt']\n#10\ndf_all['BMI_sq'] = df_all['BMI'] * df_all['BMI']\n\n#11\ndf_all['Ins_Age_cu'] = df_all['Ins_Age'] * df_all['Ins_Age'] * df_all['Ins_Age']\n#12\ndf_all['Ht_cu'] = df_all['Ht'] * df_all['Ht'] * df_all['Ht']\n#13\ndf_all['Wt_cu'] = df_all['Wt'] * df_all['Wt'] * df_all['Wt']\n#14\ndf_all['BMI_cu'] = df_all['BMI'] * df_all['BMI'] * df_all['BMI']\n\n# BMI Categorization\nconditions = [\n    (df_all['BMI'] <= df_all['BMI'].quantile(0.25)),\n    (df_all['BMI'] > df_all['BMI'].quantile(0.25)) & (df_all['BMI'] <= df_all['BMI'].quantile(0.75)),\n    (df_all['BMI'] > df_all['BMI'].quantile(0.75))]\n\nchoices = ['under_weight', 'average', 'overweight']\n#15\ndf_all['BMI_Wt'] = np.select(conditions, choices)\n\n# Age Categorization\nconditions = [\n    (df_all['Ins_Age'] <= df_all['Ins_Age'].quantile(0.25)),\n    (df_all['Ins_Age'] > df_all['Ins_Age'].quantile(0.25)) & (df_all['Ins_Age'] <= df_all['Ins_Age'].quantile(0.75)),\n    (df_all['Ins_Age'] > df_all['Ins_Age'].quantile(0.75))]\n\nchoices = ['young', 'average', 'old']\n#16\ndf_all['Old_Young'] = np.select(conditions, choices)\n\n# Height Categorization\nconditions = [\n    (df_all['Ht'] <= df_all['Ht'].quantile(0.25)),\n    (df_all['Ht'] > df_all['Ht'].quantile(0.25)) & (df_all['Ht'] <= df_all['Ht'].quantile(0.75)),\n    (df_all['Ht'] > df_all['Ht'].quantile(0.75))]\n\nchoices = ['short', 'average', 'tall']\n#17\ndf_all['Short_Tall'] = np.select(conditions, choices)\n\n# Weight Categorization\nconditions = [\n    (df_all['Wt'] <= df_all['Wt'].quantile(0.25)),\n    (df_all['Wt'] > df_all['Wt'].quantile(0.25)) & (df_all['Wt'] <= df_all['Wt'].quantile(0.75)),\n    (df_all['Wt'] > df_all['Wt'].quantile(0.75))]\n\nchoices = ['thin', 'average', 'fat']\n#18\ndf_all['Thin_Fat'] = np.select(conditions, choices)\n\n#19\ndf_all['min'] = df_all[med_keyword_columns].min(axis=1)\n#20\ndf_all['max'] = df_all[med_keyword_columns].max(axis=1)\n#21\ndf_all['mean'] = df_all[med_keyword_columns].mean(axis=1)\n#22\ndf_all['std'] = df_all[med_keyword_columns].std(axis=1)\n#23\ndf_all['skew'] = df_all[med_keyword_columns].skew(axis=1)\n#24\ndf_all['kurt'] = df_all[med_keyword_columns].kurtosis(axis=1)\n#25\ndf_all['med'] = df_all[med_keyword_columns].median(axis=1)","9f231c0d":"def new_target(row):\n    if (row['BMI_Wt']=='overweight') or (row['Old_Young']=='old')  or (row['Thin_Fat']=='fat'):\n        val='extremely_risky'\n    else:\n        val='not_extremely_risky'\n    return val\n\n#26\ndf_all['extreme_risk'] = df_all.apply(new_target,axis=1)","c829d98f":"df_all.extreme_risk.value_counts()","42a7274f":"# Risk Categorization\nconditions = [\n    (df_all['BMI_Wt'] == 'overweight') ,\n    (df_all['BMI_Wt'] == 'average') ,\n    (df_all['BMI_Wt'] == 'under_weight') ]\n\nchoices = ['risk', 'non-risk', 'risk']\n#27\ndf_all['risk_bmi'] = np.select(conditions, choices)","27e2990b":"df_all.risk_bmi.value_counts()","c03494fd":"def new_target(row):\n    if (row['BMI_Wt']=='average') or (row['Old_Young']=='average')  or (row['Thin_Fat']=='average') or (row['Short_Tall']=='average'):\n        val='average'\n    else:\n        val='non_average'\n    return val\n\n#28\ndf_all['average_risk'] = df_all.apply(new_target,axis=1)","fdccab16":"df_all.average_risk.value_counts()","38a7dd09":"def new_target(row):\n    if (row['BMI_Wt']=='under_weight') or (row['Old_Young']=='young')  or (row['Thin_Fat']=='thin') or (row['Short_Tall']=='short'):\n        val='low_end'\n    else:\n        val='non_low_end'\n    return val\n\n#29\ndf_all['low_end_risk'] = df_all.apply(new_target,axis=1)","833bcc9f":"df_all.low_end_risk.value_counts()","c8eda01a":"def new_target(row):\n    if (row['BMI_Wt']=='overweight') or (row['Old_Young']=='old')  or (row['Thin_Fat']=='fat') or (row['Short_Tall']=='tall'):\n        val='high_end'\n    else:\n        val='non_high_end'\n    return val\n\n#30\ndf_all['high_end_risk'] = df_all.apply(new_target,axis=1)","d0ee0944":"df_all.high_end_risk.value_counts()","229c8373":"plt.figure(figsize=(12,10))\nsns.countplot(x = 'extreme_risk', hue = 'Response', data = df_all)","a78a5401":"plt.figure(figsize=(12,10))\nsns.countplot(x = 'average_risk', hue = 'Response', data = df_all)","2fc7e14e":"plt.figure(figsize=(12,10))\nsns.countplot(x = 'low_end_risk', hue = 'Response', data = df_all)","cbc97c7b":"plt.figure(figsize=(12,10))\nsns.countplot(x = 'high_end_risk', hue = 'Response', data = df_all)","00bcd420":"plt.figure(figsize=(12,10))\nsns.countplot(x = 'BMI_Wt', hue = 'Response', data = df_all)","001d1d9a":"plt.figure(figsize=(12,10))\nsns.countplot(x = 'Old_Young', hue = 'Response', data = df_all)","e71e22f0":"plt.figure(figsize=(12,10))\nsns.countplot(x = 'Thin_Fat', hue = 'Response', data = df_all)","ab5ce084":"plt.figure(figsize=(12,10))\nsns.countplot(x = 'risk_bmi', hue = 'Response', data = df_all)","c9bfb1cd":"from sklearn.manifold import TSNE\n\ndef tsne_plot(x1, y1, name=\"graph.png\"):\n    tsne = TSNE(n_components=2)\n    X_t = tsne.fit_transform(x1)\n\n    plt.figure(figsize=(12, 8))\n    #plt.scatter(X_t[np.where(y1 == 8), 0], X_t[np.where(y1 == 8), 1], marker='o', color='red', linewidth='1', alpha=0.8, label='8')\n    plt.scatter(X_t[np.where(y1 == 0), 0], X_t[np.where(y1 == 0), 1], marker='o', color='red', linewidth='1', alpha=0.8, label='0')\n    plt.scatter(X_t[np.where(y1 == 1), 0], X_t[np.where(y1 == 1), 1], marker='o', color='green', linewidth='1', alpha=0.8, label='1')\n#     plt.scatter(X_t[np.where(y1 == 3), 0], X_t[np.where(y1 == 3), 1], marker='o', color='yellow', linewidth='1', alpha=0.8, label='3')\n#     plt.scatter(X_t[np.where(y1 == 4), 0], X_t[np.where(y1 == 4), 1], marker='o', color='blue', linewidth='1', alpha=0.8, label='4')\n#     plt.scatter(X_t[np.where(y1 == 5), 0], X_t[np.where(y1 == 5), 1], marker='o', color='magenta', linewidth='1', alpha=0.8, label='5')\n#     plt.scatter(X_t[np.where(y1 == 6), 0], X_t[np.where(y1 == 6), 1], marker='o', color='black', linewidth='1', alpha=0.8, label='6')\n#     plt.scatter(X_t[np.where(y1 == 7), 0], X_t[np.where(y1 == 7), 1], marker='o', color='brown', linewidth='1', alpha=0.8, label='7')\n\n    plt.legend(loc='best');\n    plt.savefig(name);\n    plt.show();\n\ngc.collect()","2e61105d":"df_all.shape","19f70b12":"df_train = df_all","0ea80180":"del df_all","e9606e9e":"df_train.head()","91d2a7d7":"df_train.drop(['Id'], axis=1, inplace=True)","ce598c40":"df_train.dtypes","e483a15c":"exclude = ['Product_Info_1', 'Product_Info_2', 'Product_Info_3', 'Product_Info_5', \n           'Product_Info_6', 'Product_Info_7', 'Employment_Info_2', 'Employment_Info_3', 'Employment_Info_5', \n           'InsuredInfo_1', 'InsuredInfo_2', 'InsuredInfo_3', 'InsuredInfo_4', 'InsuredInfo_5', 'InsuredInfo_6', \n           'InsuredInfo_7', 'Insurance_History_1', 'Insurance_History_2', 'Insurance_History_3', 'Insurance_History_4', \n           'Insurance_History_7', 'Insurance_History_8', 'Insurance_History_9', 'Family_Hist_1', 'Medical_History_2', 'Medical_History_3', \n           'Medical_History_4', 'Medical_History_5', 'Medical_History_6', 'Medical_History_7', 'Medical_History_8', 'Medical_History_9', \n           'Medical_History_11', 'Medical_History_12', 'Medical_History_13', 'Medical_History_14', 'Medical_History_16', 'Medical_History_17', \n           'Medical_History_18', 'Medical_History_19', 'Medical_History_20', 'Medical_History_21', 'Medical_History_22', 'Medical_History_23', \n           'Medical_History_25', 'Medical_History_26', 'Medical_History_27', 'Medical_History_28', 'Medical_History_29', 'Medical_History_30', \n           'Medical_History_31', 'Medical_History_33', 'Medical_History_34', 'Medical_History_35', \n           'Medical_History_36', 'Medical_History_37', 'Medical_History_38', 'Medical_History_39', 'Medical_History_40', 'Medical_History_41', \n           'Response', 'Product_Info_2_char', 'Product_Info_2_num', 'BMI_Wt', 'Old_Young', 'Thin_Fat', 'Short_Tall', 'risk_bmi',\n          'Medical_Keyword_1', 'Medical_Keyword_2', 'Medical_Keyword_3', 'Medical_Keyword_4',\n          'Medical_Keyword_5', 'Medical_Keyword_6', 'Medical_Keyword_7', 'Medical_Keyword_8',\n          'Medical_Keyword_9', 'Medical_Keyword_10', 'Medical_Keyword_11', 'Medical_Keyword_12',\n          'Medical_Keyword_13', 'Medical_Keyword_14', 'Medical_Keyword_15', 'Medical_Keyword_16',\n          'Medical_Keyword_17', 'Medical_Keyword_18', 'Medical_Keyword_19', 'Medical_Keyword_20',\n          'Medical_Keyword_21', 'Medical_Keyword_22', 'Medical_Keyword_23', 'Medical_Keyword_24', 'Medical_Keyword_25',\n          'Medical_Keyword_26', 'Medical_Keyword_27', 'Medical_Keyword_28', 'Medical_Keyword_29',\n          'Medical_Keyword_30', 'Medical_Keyword_31', 'Medical_Keyword_32', 'Medical_Keyword_33',\n          'Medical_Keyword_34', 'Medical_Keyword_35', 'Medical_Keyword_36', 'Medical_Keyword_37',\n          'Medical_Keyword_38', 'Medical_Keyword_39', 'Medical_Keyword_40', 'Medical_Keyword_41', \n          'Medical_Keyword_42', 'Medical_Keyword_43', 'Medical_Keyword_44',\n          'Medical_Keyword_45', 'Medical_Keyword_46', 'Medical_Keyword_47', 'Medical_Keyword_48', 'extreme_risk', \n           'average_risk', 'high_end_risk', 'low_end_risk']\n\nnorm_data = normalize(df_train, exclude=exclude)","a8321949":"cont_names = ['Product_Info_4', 'Ins_Age', 'Ht', 'Wt', 'BMI', 'Employment_Info_1', \n              'Employment_Info_4', 'Employment_Info_6', 'Insurance_History_5', 'Family_Hist_2', \n              'Family_Hist_3', 'Family_Hist_4', 'Family_Hist_5', 'Medical_History_1', \n              'Medical_History_10', 'Medical_History_15', 'Medical_History_24', 'Medical_History_32', 'BMI_Age', 'Med_Keywords_Count',\n             'min', 'max', 'mean', 'std', 'skew', 'med', 'kurt', 'Age_Wt', 'Age_Ht', \n              'Ins_Age_sq', 'Ht_sq','Wt_sq',\n              'Ins_Age_cu','Ht_cu','Wt_cu', 'BMI_sq', 'BMI_cu'\n             ]\n\ndep_var = 'Response'\nprocs = [FillMissing, Categorify]\n\ncat_names= ['Product_Info_1', 'Product_Info_2', 'Product_Info_3', 'Product_Info_5', \n           'Product_Info_6', 'Product_Info_7', 'Employment_Info_2', 'Employment_Info_3', 'Employment_Info_5', \n           'InsuredInfo_1', 'InsuredInfo_2', 'InsuredInfo_3', 'InsuredInfo_4', 'InsuredInfo_5', 'InsuredInfo_6', \n           'InsuredInfo_7', 'Insurance_History_1', 'Insurance_History_2', 'Insurance_History_3', 'Insurance_History_4', \n           'Insurance_History_7', 'Insurance_History_8', 'Insurance_History_9', 'Family_Hist_1', 'Medical_History_2', 'Medical_History_3', \n           'Medical_History_4', 'Medical_History_5', 'Medical_History_6', 'Medical_History_7', 'Medical_History_8', 'Medical_History_9', \n           'Medical_History_11', 'Medical_History_12', 'Medical_History_13', 'Medical_History_14', 'Medical_History_16', 'Medical_History_17', \n           'Medical_History_18', 'Medical_History_19', 'Medical_History_20', 'Medical_History_21', 'Medical_History_22', 'Medical_History_23', \n           'Medical_History_25', 'Medical_History_26', 'Medical_History_27', 'Medical_History_28', 'Medical_History_29', 'Medical_History_30', \n           'Medical_History_31', 'Medical_History_33', 'Medical_History_34', 'Medical_History_35', \n           'Medical_History_36', 'Medical_History_37', 'Medical_History_38', 'Medical_History_39', 'Medical_History_40', 'Medical_History_41', \n            'Product_Info_2_char', 'Product_Info_2_num', 'BMI_Wt', 'Old_Young', 'Thin_Fat', 'Short_Tall', 'risk_bmi','extreme_risk','average_risk','high_end_risk',\n          'Medical_Keyword_1', 'Medical_Keyword_2', 'Medical_Keyword_3', 'Medical_Keyword_4',\n          'Medical_Keyword_5', 'Medical_Keyword_6', 'Medical_Keyword_7', 'Medical_Keyword_8',\n          'Medical_Keyword_9', 'Medical_Keyword_10', 'Medical_Keyword_11', 'Medical_Keyword_12',\n          'Medical_Keyword_13', 'Medical_Keyword_14', 'Medical_Keyword_15', 'Medical_Keyword_16',\n          'Medical_Keyword_17', 'Medical_Keyword_18', 'Medical_Keyword_19', 'Medical_Keyword_20',\n          'Medical_Keyword_21', 'Medical_Keyword_22', 'Medical_Keyword_23', 'Medical_Keyword_24', 'Medical_Keyword_25',\n          'Medical_Keyword_26', 'Medical_Keyword_27', 'Medical_Keyword_28', 'Medical_Keyword_29',\n          'Medical_Keyword_30', 'Medical_Keyword_31', 'Medical_Keyword_32', 'Medical_Keyword_33',\n          'Medical_Keyword_34', 'Medical_Keyword_35', 'Medical_Keyword_36', 'Medical_Keyword_37',\n          'Medical_Keyword_38', 'Medical_Keyword_39', 'Medical_Keyword_40', 'Medical_Keyword_41', \n          'Medical_Keyword_42', 'Medical_Keyword_43', 'Medical_Keyword_44', 'low_end_risk',\n          'Medical_Keyword_45', 'Medical_Keyword_46', 'Medical_Keyword_47', 'Medical_Keyword_48'\n           ]","74c11493":"df_train.shape, norm_data.shape","52ff8ab2":"valid_sz = 5000\nvalid_idx = range(len(norm_data)-valid_sz, len(norm_data))\n\ndata = (TabularList.from_df(norm_data, procs = procs, cont_names=cont_names, cat_names=cat_names)\n        .split_by_idx(valid_idx)\n        .label_from_df(cols=dep_var)\n        .databunch(bs=1024))","f195cca7":"# data.add_test(TabularList.from_df(df_test, cont_names=cont_names))","b335365a":"data.show_batch()","2f72e8ec":"df_t = data.train_ds.inner_df\ndf_v = data.valid_ds.inner_df","c415f0ee":"df_t.shape, df_v.shape","9384e05f":"df = df_t.append(df_v, ignore_index=True)","ccd98924":"df.shape","cc2048c7":"pd.set_option('float_format', '{:f}'.format)\ndf.describe()","4fb1cbd2":"# Categorical boolean mask\ncategorical_feature_mask = df.dtypes=='category'\n# filter categorical columns using mask and turn it into a list\ncategorical_cols = df.columns[categorical_feature_mask].tolist()","a1a56aaa":"# import labelencoder\nfrom sklearn.preprocessing import LabelEncoder\n# instantiate labelencoder object\nle = LabelEncoder()","5093a831":"# apply le on categorical feature columns\ndf[categorical_cols] = df[categorical_cols].apply(lambda col: le.fit_transform(col))\ndf[categorical_cols].head(10)","ac29b67e":"sample_size=500\ndf_grp = df.groupby('Response').apply(lambda x: x.sample(sample_size))","03c2139e":"df_grp = df_grp.reset_index(drop=True)","f879881b":"X = df_grp.drop(['Response'], axis = 1).values\nY = df_grp[\"Response\"].values","1c1c5e3c":"tsne_plot(X, Y, 'graph')","fb3122b2":"var = df.columns.values\n\ni = 0\nt0 = df.loc[df['Response'] == 0]\nt1 = df.loc[df['Response'] == 1]\n\n\nsns.set_style('whitegrid')\nplt.figure()\nfig, ax = plt.subplots(30,6,figsize=(60,50))\n\nfor feature in var:\n    i += 1\n    plt.subplot(30,6,i)\n    sns.kdeplot(t0[feature], bw=0.5,label=\"Response = 0\")\n    sns.kdeplot(t1[feature], bw=0.5,label=\"Response = 1\")\n    \n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();","9e3088d5":"df['Response'].value_counts()","60c01bea":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom scipy.spatial.distance import pdist\n\nimport numpy as np\nimport numpy as np\nfrom pandas import *\nimport matplotlib.pyplot as plt\n#from hcluster import pdist, linkage, dendrogram\nfrom numpy.random import rand\n\nX_ = df.T.values #Transpose values \nY_ = pdist(X_)\nZ_ = linkage(Y_)\n\nplt.figure(figsize=(24,24))\n#dendrogram(Z, labels = df.columns, orientation='bottom')\nfig = ff.create_dendrogram(Z_, labels=df.columns, color_threshold=1.5)\nfig.update_layout(width=1500, height=1000)\nfig.show()","ea9cf7ae":"corr_df = pd.DataFrame(df.drop(\"Response\", axis=1).apply(lambda x: x.corr(df.Response)))","f03b58b6":"corr_df.columns = ['corr']","7211a218":"corr_df.sort_values(by='corr')","05dbd2f9":"df.head()","819336e3":"df_small = df[['BMI','Medical_Keyword_15', 'Medical_History_4','Medical_History_23', \n              'Product_Info_4','InsuredInfo_6', 'Ht', 'Wt', 'Ins_Age', 'Med_Keywords_Count',\n              'extreme_risk', 'high_end_risk', 'low_end_risk', 'Thin_Fat', 'BMI_Age', 'Age_Ht', 'Age_Wt', 'Medical_Keyword_15']]\n\nx = df_small.reset_index(drop=True)\n\nx.columns = ['BMI','Medical_Keyword_15', 'Medical_History_4','Medical_History_23', \n              'Product_Info_4','InsuredInfo_6', 'Ht', 'Wt', 'Ins_Age', 'Med_Keywords_Count',\n              'extreme_risk', 'high_end_risk', 'low_end_risk', 'Thin_Fat', 'BMI_Age', 'Age_Ht', 'Age_Wt', 'Medical_Keyword_15']","15ebabad":"from sklearn.mixture import GaussianMixture, BayesianGaussianMixture\ngmix = GaussianMixture(n_components=18, random_state=42, reg_covar=1e-3)\ngmix.fit(x)\ndf['test_cluster'] = gmix.predict(x)","1f366085":"from sklearn.cluster import KMeans,DBSCAN,Birch\ngc.collect()\nbrc = Birch(n_clusters=14)\n\ndf_small = df[['BMI','Medical_Keyword_15', 'Medical_History_4','Medical_History_23', \n              'Product_Info_4','InsuredInfo_6', 'Ht', 'Wt', 'Ins_Age', 'Med_Keywords_Count',\n              'extreme_risk', 'high_end_risk', 'low_end_risk', 'Thin_Fat']]\n\nx = df_small.reset_index(drop=True)\n\nx.columns = ['BMI','Medical_Keyword_15', 'Medical_History_4','Medical_History_23', \n              'Product_Info_4','InsuredInfo_6', 'Ht', 'Wt', 'Ins_Age', 'Med_Keywords_Count',\n              'extreme_risk', 'high_end_risk', 'low_end_risk', 'Thin_Fat']","13cf1413":"clustering = brc.fit(x).labels_","7d85f87c":"df['big_cluster'] = clustering","5fa95320":"gc.collect()\nfrom sklearn.cluster import KMeans,DBSCAN,Birch\n#from hdbscan import HDBSCAN\n\nx1=df[['BMI','Medical_Keyword_15']].reset_index(drop=True)\nx2=df[['Medical_History_4','Medical_History_23']].reset_index(drop=True)\nx3=df[['BMI','Med_Keywords_Count']].reset_index(drop=True)\nx4=df[['Product_Info_4','InsuredInfo_6']].reset_index(drop=True)\nx5=df[['BMI', 'Ins_Age']].reset_index(drop=True)\nx6=df[['Thin_Fat', 'Medical_History_15']].reset_index(drop=True)\nx7=df[['BMI_Age', 'Age_Ht']].reset_index(drop=True)\nx8=df[['BMI_Age', 'Age_Wt']].reset_index(drop=True)\nx9=df[['BMI', 'Wt']].reset_index(drop=True)\nx10=df[['BMI', 'Ht']].reset_index(drop=True)\n\nx11=df[['extreme_risk', 'Medical_History_23']].reset_index(drop=True)\nx12=df[['extreme_risk', 'Medical_History_4']].reset_index(drop=True)\nx13=df[['extreme_risk','Medical_Keyword_15']].reset_index(drop=True)\nx14=df[['extreme_risk','Med_Keywords_Count']].reset_index(drop=True)\n\nx15=df[['high_end_risk', 'Medical_History_23']].reset_index(drop=True)\nx16=df[['high_end_risk', 'Medical_History_4']].reset_index(drop=True)\nx17=df[['high_end_risk','Medical_Keyword_15']].reset_index(drop=True)\nx18=df[['high_end_risk','Med_Keywords_Count']].reset_index(drop=True)\n\nx19=df[['low_end_risk', 'Medical_History_23']].reset_index(drop=True)\nx20=df[['low_end_risk', 'Medical_History_4']].reset_index(drop=True)\nx21=df[['low_end_risk','Medical_Keyword_15']].reset_index(drop=True)\nx22=df[['low_end_risk','Med_Keywords_Count']].reset_index(drop=True)\n\nx23=df[['extreme_risk', 'Product_Info_4']].reset_index(drop=True)\nx24=df[['extreme_risk', 'InsuredInfo_6']].reset_index(drop=True)\nx25=df[['extreme_risk','BMI']].reset_index(drop=True)\nx26=df[['extreme_risk','Thin_Fat']].reset_index(drop=True)\n\nx27=df[['high_end_risk', 'Product_Info_4']].reset_index(drop=True)\nx28=df[['high_end_risk', 'InsuredInfo_6']].reset_index(drop=True)\nx29=df[['high_end_risk','BMI']].reset_index(drop=True)\nx30=df[['high_end_risk','Thin_Fat']].reset_index(drop=True)\n\nx31=df[['low_end_risk', 'Product_Info_4']].reset_index(drop=True)\nx32=df[['low_end_risk', 'InsuredInfo_6']].reset_index(drop=True)\nx33=df[['low_end_risk','BMI']].reset_index(drop=True)\nx34=df[['low_end_risk','Thin_Fat']].reset_index(drop=True)\n\nx1.columns=['bmi','m_k_15'];x2.columns=['m_h_4','m_h_23'];x3.columns=['bmi','med_key'];x4.columns=['i_i_6','p_i_4']\nx5.columns=['bmi', 'age']; x6.columns=['thinfat', 'mh15']; x7.columns = ['bmiage', 'ageht']; x8.columns = ['bmiage', 'agewt'];\nx9.columns=['bmi', 'wt']; x10.columns=['bmi', 'ht']; x11.columns=['xrisk', 'mh23']; x12.columns=['xrisk', 'mh4'];\nx13.columns=['xrisk', 'mk15']; x14.columns=['xrisk', 'mkc'];x15.columns=['hrisk', 'mh23']; x16.columns=['hrisk', 'mh4'];\nx17.columns=['hrisk', 'mk15']; x18.columns=['hrisk', 'mkc'];x19.columns=['lrisk', 'mh23']; x20.columns=['lrisk', 'mh4'];\nx21.columns=['lrisk', 'mk15']; x22.columns=['lrisk', 'mkc'];x23.columns=['xrisk', 'pi4']; x24.columns=['xrisk', 'ii6'];\nx25.columns=['xrisk', 'bmi']; x26.columns=['xrisk', 'tf'];x27.columns=['hrisk', 'pi4']; x28.columns=['hrisk', 'ii6'];\nx29.columns=['hrisk', 'bmi']; x30.columns=['hrisk', 'tf'];x31.columns=['lrisk', 'pi4']; x32.columns=['lrisk', 'ii6'];\nx33.columns=['lrisk', 'bmi']; x34.columns=['lrisk', 'tf']\n\nbrc = Birch(n_clusters=2)\n\nclustering1 = brc.fit(x1).labels_\nclustering2 = brc.fit(x2).labels_\nclustering3 = brc.fit(x3).labels_\nclustering4 = brc.fit(x4).labels_\nclustering5 = brc.fit(x5).labels_\nclustering6 = brc.fit(x6).labels_\nclustering7 = brc.fit(x7).labels_\nclustering8 = brc.fit(x8).labels_\nclustering9 = brc.fit(x9).labels_\nclustering10 = brc.fit(x10).labels_\nclustering11 = brc.fit(x11).labels_\nclustering12 = brc.fit(x12).labels_\nclustering13 = brc.fit(x13).labels_\nclustering14 = brc.fit(x14).labels_\nclustering15 = brc.fit(x15).labels_\nclustering16 = brc.fit(x16).labels_\nclustering17 = brc.fit(x17).labels_\nclustering18 = brc.fit(x18).labels_\nclustering19 = brc.fit(x19).labels_\nclustering20 = brc.fit(x20).labels_\nclustering21 = brc.fit(x21).labels_\nclustering22 = brc.fit(x22).labels_\nclustering23 = brc.fit(x23).labels_\nclustering24 = brc.fit(x24).labels_\nclustering25 = brc.fit(x25).labels_\nclustering26 = brc.fit(x26).labels_\nclustering27 = brc.fit(x27).labels_\nclustering28 = brc.fit(x28).labels_\nclustering29 = brc.fit(x29).labels_\nclustering30 = brc.fit(x30).labels_\nclustering31 = brc.fit(x31).labels_\nclustering32 = brc.fit(x32).labels_\nclustering33 = brc.fit(x33).labels_\nclustering34 = brc.fit(x34).labels_\n\ndf['bmi_mk15'] = clustering1\ndf['mh4_mh23'] = clustering2\ndf['bmi_medkey'] = clustering3\ndf['ii6_pi_4'] = clustering4\ndf['bmi_age'] = clustering5\ndf['thinfat_mh15'] = clustering6\ndf['bmiage_ageht'] = clustering7\ndf['bmiage_agewt'] = clustering8\ndf['bmiwt'] = clustering9\ndf['bmiht'] = clustering10\ndf['xrisk_mh23'] = clustering11\ndf['xrisk_mh4'] = clustering12\ndf['xrisk_mk15'] = clustering13\ndf['xrisk_mkc'] = clustering14\ndf['hrisk_mh23'] = clustering15\ndf['hrisk_mh4'] = clustering16\ndf['hrisk_mk15'] = clustering17\ndf['hrisk_mkc'] = clustering18\ndf['lrisk_mh23'] = clustering19\ndf['lrisk_mh4'] = clustering20\ndf['lrisk_mk15'] = clustering21\ndf['lrisk_mkc'] = clustering22\ndf['xrisk_pi4'] = clustering23\ndf['xrisk_ii6'] = clustering24\ndf['xrisk_bmi'] = clustering25\ndf['xrisk_tf'] = clustering26\ndf['hrisk_pi4'] = clustering27\ndf['hrisk_ii6'] = clustering28\ndf['hrisk_bmi'] = clustering29\ndf['hrisk_tf'] = clustering30\ndf['lrisk_pi4'] = clustering31\ndf['lrisk_ii6'] = clustering32\ndf['lrisk_bmi'] = clustering33\ndf['lrisk_tf'] = clustering34\n\ngc.collect()","8d662bd2":"df.head(3)","fcfaba4d":"df.shape","35cfebe5":"df.columns[df.isna().any()].tolist()","c485af48":"df.shape","ca4ab1bf":"df.columns","5fab6e8d":"def correlation(df, threshold):\n    col_corr = set() # Set of all the names of deleted columns\n    corr_matrix = df.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if (corr_matrix.iloc[i, j] >= threshold) and (corr_matrix.columns[j] not in col_corr):\n                colname = corr_matrix.columns[i] # getting the name of column\n                col_corr.add(colname)\n                if colname in df.columns:\n                    del df[colname] # deleting the column from the dataset\n\n    print(df.shape)","cfb04e55":"correlation(df, 0.95)\ndf.shape","57ba5bea":"df.columns","d4aac534":"X = df.drop(['Response'], axis=1).values\nY = df['Response'].values","5e8a98f9":"from sklearn.feature_selection import SelectFromModel\n\n\n\nforest_1 = SelectFromModel(LGBMClassifier( n_estimators=200, \n                          objective='binary', class_weight='balanced', \n                         ), \n                         threshold='2*median')\n\n\n\nforest_2 = SelectFromModel(ExtraTreesClassifier(bootstrap=True, criterion='gini', max_depth=10, max_features='auto',class_weight='balanced',\n                              \n            max_leaf_nodes=None, min_impurity_decrease=0.0,\n            min_impurity_split=None, min_samples_leaf=20,\n            min_samples_split=7, min_weight_fraction_leaf=0.0,\n            n_estimators=200, n_jobs=1, oob_score=False, random_state=42,\n            verbose=0, warm_start=False), \n                         threshold='2*median')\n\n\n\nforest_3 = SelectFromModel(XGBClassifier(objective=\"binary:logistic\", random_state=42, n_estimators=200,\n                       reg_alpha=1, colsample_bylevel=0.7, colsample_bytree=0.7, gamma=5), \n                         threshold='2*median')\n\nforest_1.fit(X, Y)\nforest_2.fit(X, Y)\nforest_3.fit(X, Y)","fa0f3542":"gc.collect()","95a8b762":"df_without_label = df.drop(['Response'], axis=1)\nselected_feat_1= df_without_label.columns[(forest_1.get_support())]\nselected_feat_2= df_without_label.columns[(forest_2.get_support())]\nselected_feat_3= df_without_label.columns[(forest_3.get_support())]","6efc06ed":"print(selected_feat_1), print(selected_feat_2), print(selected_feat_3)\nprint(len(selected_feat_1)), print(len(selected_feat_2)), print(len(selected_feat_3))\nprint(len(selected_feat_1) + len(selected_feat_2) + len(selected_feat_3))","19e2f9cf":"selected_feat = selected_feat_1.union(selected_feat_2)\nlen(selected_feat)","8cdcfb01":"selected_feat_new = selected_feat.union(selected_feat_3)\nlen(selected_feat_new)","5431f227":"importances = forest_1.estimator_.feature_importances_\n\ndata={'Feature_Name':df.drop(['Response'], axis=1).columns,\n      'Feature_Importance': importances\n     }\n\nfeature_df=pd.DataFrame(data)\n\nfeature_df.sort_values(by=['Feature_Importance'],ascending=False,inplace=True)\n\nfig, ax = plt.subplots(figsize=(20,25))\nsns.barplot(data=feature_df,y='Feature_Name',x='Feature_Importance')","1e6a4fb6":"importances = forest_2.estimator_.feature_importances_\n\ndata={'Feature_Name':df.drop(['Response'], axis=1).columns,\n      'Feature_Importance': importances\n     }\n\nfeature_df=pd.DataFrame(data)\n\nfeature_df.sort_values(by=['Feature_Importance'],ascending=False,inplace=True)\n\nfig, ax = plt.subplots(figsize=(20,25))\nsns.barplot(data=feature_df,y='Feature_Name',x='Feature_Importance')","7cdb180b":"importances = forest_3.estimator_.feature_importances_\n\ndata={'Feature_Name':df.drop(['Response'], axis=1).columns,\n      'Feature_Importance': importances\n     }\n\nfeature_df=pd.DataFrame(data)\n\nfeature_df.sort_values(by=['Feature_Importance'],ascending=False,inplace=True)\n\nfig, ax = plt.subplots(figsize=(20,25))\nsns.barplot(data=feature_df,y='Feature_Name',x='Feature_Importance')","61ba495d":"df[selected_feat_new].head()","5e3d004f":"feature_mask_1 = df[selected_feat_new].dtypes=='int64'\nfeature_mask_2 = df[selected_feat_new].dtypes == 'float64'\n\n\nint_cols = df[selected_feat_new].columns[feature_mask_1].tolist()\n#int_cols = int_cols.remove('Response')\nfloat_cols = df[selected_feat_new].columns[feature_mask_2].tolist()","0429f59a":"cont_names = float_cols\n\ndep_var = 'Response'\nprocs = [FillMissing, Categorify]\n\ncat_names = int_cols","83612522":"df.Response.value_counts()","78c899d4":"df_sel_feat = df[selected_feat_new]\ndf_sel_feat['Response'] = df['Response']\ndf_sel_feat.head()","c54585f2":"df_sel_feat.shape","6d1d5182":"var = df_sel_feat.columns.values\n\ni = 0\nt0 = df_sel_feat.loc[df_sel_feat['Response'] == 0]\nt1 = df_sel_feat.loc[df_sel_feat['Response'] == 1]\n\n\nsns.set_style('whitegrid')\nplt.figure()\nfig, ax = plt.subplots(24,4,figsize=(30,30), dpi=60)\n\nfor feature in var:\n    i += 1\n    plt.subplot(24,4,i)\n    sns.kdeplot(t0[feature], bw=0.5,label=\"Response = 0\")\n    sns.kdeplot(t1[feature], bw=0.5,label=\"Response = 1\")\n    \n    plt.xlabel(feature, fontsize=12,)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();","55115d4c":"df_sel_feat.shape","4bd3122c":"df_sel_feat.head(2)","fdc1721a":"df_sel_feat_wo_response = df_sel_feat.drop(['Response'], axis=1)\nX = df_sel_feat.drop(['Response'], axis=1)\nY = df_sel_feat['Response']\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, stratify = Y, random_state=42)","5136bca0":"model = XGBClassifier(objective=\"binary:logistic\", random_state=42, n_estimators=200,\n                       reg_alpha=1, colsample_bylevel=0.7, colsample_bytree=0.7, gamma=5)\n\nmodel_xgb = model.fit(X_train, y_train)","607b9151":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(model_xgb).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = df_sel_feat_wo_response.columns.tolist(), top=100)","27907dba":"from matplotlib import pyplot as plt\nfrom pdpbox import pdp, get_dataset, info_plots\n\n# Create the data that we will plot\npdp_goals = pdp.pdp_isolate(model=model_xgb, dataset=X_test, model_features=X_test.columns.tolist(), feature='BMI')\n\n# plot it\npdp.pdp_plot(pdp_goals, 'BMI')\nplt.show()","05333b6e":"pdp_goals = pdp.pdp_isolate(model=model_xgb, dataset=X_test, model_features=X_test.columns.tolist(), feature='Medical_History_15')\n\n# plot it\npdp.pdp_plot(pdp_goals, 'Medical_History_15')\nplt.show()","0b8e9c88":"\npdp_goals = pdp.pdp_isolate(model=model_xgb, dataset=X_test, model_features=X_test.columns.tolist(), feature='Medical_Keyword_15')\n\n# plot it\npdp.pdp_plot(pdp_goals, 'Medical_Keyword_15')\nplt.show()","6d932a3f":"pdp_goals = pdp.pdp_isolate(model=model_xgb, dataset=X_test, model_features=X_test.columns.tolist(), feature='Product_Info_4')\n\n# plot it\npdp.pdp_plot(pdp_goals, 'Product_Info_4')\nplt.show()","d8ea3a08":"pdp_goals = pdp.pdp_isolate(model=model_xgb, dataset=X_test, model_features=X_test.columns.tolist(), feature='Medical_History_4')\n\n# plot it\npdp.pdp_plot(pdp_goals, 'Medical_History_4')\nplt.show()","6c26f573":"pdp_goals = pdp.pdp_isolate(model=model_xgb, dataset=X_test, model_features=X_test.columns.tolist(), feature='Medical_History_23')\n\n# plot it\npdp.pdp_plot(pdp_goals, 'Medical_History_23')\nplt.show()","2dedb0f4":"import shap\n\nexplainer = shap.TreeExplainer(model_xgb)\nshap_values = explainer.shap_values(X_test)\n\nshap.summary_plot(shap_values, X_test, plot_type=\"bar\")","789b0a04":"shap.summary_plot(shap_values, X_test)","a464469f":"def policy_acceptance_factors(model, policyholder):\n\n    explainer = shap.TreeExplainer(model_xgb)\n    shap_values = explainer.shap_values(policyholder)\n    shap.initjs()\n    return shap.force_plot(explainer.expected_value, shap_values, policyholder)","2072dcd6":"data_for_prediction = X_test.iloc[1,:].astype(float)\npolicy_acceptance_factors(model_xgb, data_for_prediction)","33f7a554":"data_for_prediction = X_test.iloc[6,:].astype(float)\npolicy_acceptance_factors(model_xgb, data_for_prediction)","cca853d9":"data_for_prediction = X_test.iloc[12,:].astype(float)\npolicy_acceptance_factors(model_xgb, data_for_prediction)","bb35a565":"shap_values = shap.TreeExplainer(model_xgb).shap_values(X_test)\nshap.dependence_plot(\"BMI\", shap_values, X_test)","56f7dd91":"shap.dependence_plot(\"Medical_History_15\", shap_values, X_test)","00995e47":"shap.dependence_plot(\"Medical_Keyword_15\", shap_values, X_test)","4e98f356":"shap.dependence_plot(\"Medical_History_23\", shap_values, X_test)","5d5e902a":"shap.dependence_plot(\"Medical_History_4\", shap_values, X_test)","7061a9c7":"shap.dependence_plot(\"bmi_mk15\", shap_values, X_test)","b0655a7e":"shap.dependence_plot(\"Product_Info_4\", shap_values, X_test)","8fa0d2dc":"shap_values = explainer.shap_values(X_train.iloc[:100])\nshap.force_plot(explainer.expected_value, shap_values, X_test.iloc[:])","498a0837":"valid_sz = 5000\nvalid_idx = range(len(df_sel_feat)-valid_sz, len(df_sel_feat))\n\ndata = (TabularList.from_df(df_sel_feat, procs = procs, cont_names=cont_names, cat_names=cat_names)\n        .split_by_rand_pct(0.1, seed=42)\n        .label_from_df(cols=dep_var)\n        .databunch(bs=1024*4)) ","87473c1d":"from fastai.callbacks import *\n\nauroc = AUROC()\n\nlearn = tabular_learner(data, layers=[200, 100], metrics=[auroc], \n                        ps=[0.3, 0.3], emb_drop=0.3)","2edc8ecf":"learn.loss_func = LabelSmoothingCrossEntropy()","b9616ffe":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","eb18a7ca":"lr = 1e-2\nlearn.fit_one_cycle(7, max_lr=lr,  pct_start=0.5, wd = 0.75)","38ce2583":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","0495c858":"lr=1e-4\nlearn.fit_one_cycle(7, max_lr=lr,  pct_start=0.5, wd = 1)","6874fa12":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","c2b2ea1e":"lr=1e-5\nlearn.fit_one_cycle(5, max_lr=lr,  pct_start=0.5, wd = 1.)","e8f4eb70":"lr=1e-7\nlearn.fit_one_cycle(5, max_lr=lr,  pct_start=0.5, wd = 1.)","2891f295":"learn.recorder.plot_losses()","94c46a3b":"learn.save('1st-round')\nlearn.load('1st-round')","c94082c2":"interp = ClassificationInterpretation.from_learner(learn)\n\nlosses,idxs = interp.top_losses()\n\nlen(data.valid_ds)==len(losses)==len(idxs)","c63480e2":"interp.plot_confusion_matrix(figsize=(8,8), dpi=60)","92f46854":"gc.collect()","389ba615":"data_init = (TabularList.from_df(df_sel_feat, procs = procs, cont_names=cont_names, cat_names=cat_names)\n        .split_by_rand_pct(0.1, seed=42)\n        .label_from_df(cols=dep_var)\n        .databunch(bs=1024))","d60be5dd":"x = int(len(df_sel_feat)*.9)","2c08b13f":"train_df = df_sel_feat.iloc[:x]\ntest_df = df_sel_feat.iloc[x:]","016c0ee6":"skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)","10fba7db":"val_pct = []\ntest_pct = []\nroc_auc = AUROC()\n\nfor train_index, val_index in skf.split(train_df.index, train_df[dep_var]):\n    data_fold = (TabularList.from_df(train_df, cat_names=cat_names.copy(),\n                                  cont_names=cont_names.copy(), procs=procs,\n                                  processor=data_init.processor) # Very important\n              .split_by_idxs(train_index, val_index)\n              .label_from_df(cols=dep_var)\n              .databunch())\n    \n    data_test = (TabularList.from_df(test_df, cat_names=cat_names.copy(),\n                                  cont_names=cont_names.copy(), procs=procs,\n                                  processor=data_init.processor) # Very important\n              .split_none()\n              .label_from_df(cols=dep_var))\n    \n    data_test.valid = data_test.train\n    data_test = data_test.databunch()\n    \n    learn_f = tabular_learner(data_fold, layers=[200, 100], metrics=[auroc], \n                        ps=[0.3, 0.3], emb_drop=0.3)\n    \n    learn_f.fit_one_cycle(5, max_lr=1e-3,  pct_start=0.5, wd = 1)\n    \n    _, val = learn_f.validate()\n    \n    learn_f.data.valid_dl = data_test.valid_dl\n    \n    _, test = learn_f.validate()\n    \n    val_pct.append(val.numpy())\n    test_pct.append(test.numpy())","706879d5":"print(f'Validation\\nmean: {np.mean(val_pct)}\\nstd: {np.std(val_pct)}')","0ff6a077":"\nprint(f'Test\\nmean: {np.mean(test_pct)}\\nstd: {np.std(test_pct)}')","149b7a81":"class SaveFeatures():\n    features=None\n    def __init__(self, m): \n        self.hook = m.register_forward_hook(self.hook_fn)\n        self.features = None\n    def hook_fn(self, module, input, output): \n        out = output.detach().cpu().numpy()\n        if isinstance(self.features, type(None)):\n            self.features = out\n        else:\n            self.features = np.row_stack((self.features, out))\n    def remove(self): \n        self.hook.remove()","7b0867f9":"sf = SaveFeatures(learn.model.layers[4])\n_= learn.get_preds(data.train_ds)\n\nlabel = [data.classes[x] for x in (list(data.train_ds.y.items))]\ndf_new = pd.DataFrame({'label': label})\narray = np.array(sf.features)\nx=array.tolist()\ndf_new['img_repr'] = x\n\nd2 = pd.DataFrame(df_new.img_repr.values.tolist(), index = df_new.index).rename(columns = lambda x: 'img_repr{}'.format(x+1))\ndf_new_2 = df_new.join(d2)\ndf_new_2.drop(['img_repr'], axis=1, inplace=True)\n\nsample_size=500\ndf_grp = df_new_2.groupby('label').apply(lambda x: x.sample(sample_size))\nX = df_grp.drop(['label'], axis = 1).values\nY = df_grp[\"label\"].values\ntsne_plot(X, Y, \"original.png\")","d96a79a7":"# Loading Libraries and Data\n\nLet's first load few libraries","c8840ce6":"Above are top features who have contributed the most in classifying Green (clean policies) and Red (not clean policies)","3c68eb21":"> Now, lets run few classifiers wherein using SelectFromModel function in SKLearn, we will choose top features. I have chosen \"threshold\" as 2* median here.","829d0a1f":"Let's plot few variables. These will be helpful in doing some very important feature engineering.","4631275b":"We can see that Class 8 has the highest distribution. We will assume this as clean and accepted policies on standard underwriting terms. Rest other classes can be considered as policies rejected or accepted at extra terms and conditions","c09500b7":"This is perhaps the most important part of this notebook.\n\nBased on my industry knowledge, we know that these are high risk policies:\n\n1. Old Age\n2. Obese persons\n3. High BMI\n4. Extremely short or tall persons\n\nWe will therefore create few features such as:\n\n1. Person very old or very young or in middle\n2. Person very short or tall or in middle\n3. Person with very high BMI or low BMI or in middle\n4. Persons with obesity or are very thin or in middle\n\nWe will also create few more features such as:\n\n1. Multiplication of BMI and Age - higher the factor, higher the risk\n2. Multiplication of Weight and Age - higher the factor, higher the risk\n3. Multiplication of Height and Age\n4. Split of Product Info 2 into numericals and characters\n5. Few stats description of Medical Keywords\n6. Depending on the BMI, classification of lives into High Risk or Low Risk (very low and very high BMI both are risky factors)\n7. Depending on BMI, Ht, Wt, Age, creation of more features which basically categorizes the risk into different buckets","3d259a0a":"# Data\n\nLet's see how does our data look like.\n\nWe will see first few entries, its shape and its statistical description","f4d9ec7c":"# Introduction\n\nIn this Notebook, we will use Prudential Life Insurance's data of its policyholders and build a classifier which will try to predict different classes of its policyholders depending on the underwriting and risk assessment.\n\n![image.png](attachment:image.png)\n\nWe have only used Training data for the purpose of building classifier. In the training data, we have around 60k records and around 128 features. Most of the features are masked and normalized. This makes the task of feature engineering very difficult.\n\nFor the purpose of this analysis, we will turn the multiclass classification problem to binary classification problem. We can see (ref. below) that class 8 has the highest number of records which suggests that these are clean and accepted records (i.e. policy issued to these lives on standard terms). Rest other classes can be considered as policy issuance with some extra terms and conditions (i.e. not completely clean records).\n\nWe will use various packages to solve this challenge but primarily, we will use Fastai and Sklearn libraries.\n\nIn this Notebook, we will use following approaches to build the classifier:\n\n1. Feature engineering as much as possible\n2. Using clustering techniques, creation of more features\n3. Weeding out unwanted and highly correlated features\n\nIt will be soon found out that industry knowledge and feature engineering will play a major role in buildling a perfect and accurate classifier. So, let's dive in!\n\nPS: In my earlier committed kernel, I found one bug which I have corrected in this kernel (I accidentally included dependent variable in the list of independent variable which resulted in almost 100% accuracy. Extremely sorry for this oversight.","cc325cec":"Now we have created the Fastai Databunch, we will concatenate train and val data and train few Classifiers on entire dataset to see which features are the most important ones","69834292":"Now lets see TSNE plot after our training","3a6be935":"Using top features found by the model, lets create final Databunch which we can use for learning","a67bed52":"Now, lets look at individual cases and see how their variables affecting the outcome","b394c57f":"As discussed above, we will turn this Multiclass classification challenge into Binary classification challenge.","6f880963":"Let's see first Permutation Importance of the model. For this purpose, I have chosen XGB Classifier as my model.","cd6beccd":"# K fold cross validation\n\nhttps:\/\/github.com\/muellerzr\/fastai-Experiments-and-tips\/blob\/master\/K-Fold%20Cross%20Validation\/kfold.ipynb","30e7c27e":"The higher the medical history 4 and medical history 23, greater the chance of getting policy accepted.","d90e53e6":"# Feature Engineering","9696d90c":"As the value of Product Info 4 goes higher the chances that policy will be accepted becomes higher.","b801a96b":"The higher the value of medical keyword 15, the lower the chance that policy will be accepted.","b1c42a59":"# Selection of Important Features (i.e. weeding out unwanted features)","a477a8b2":"In this section, we will do the followings:\n\n1. Prepare the DataBunch for Fastai (this process will take care of missing values, categorization of categorical variables, normalization)\n\n2. We will then use XGBoost Classifier on the entire cleaned data to see which are the most predominant features in the data.","1e442d69":"Let's see SHAP values","92e163ca":"This shows that following features are extremely important from risk classification point of view:\n1. BMI\n2. Med History 15\n3. Medicak Keyword 15\n4. Product Info 4\n5. Med History 4\n6. Med History 23","0481098c":"# Data for Fastai Learning","d938f8c4":"Here, red ones are accepted risks and blue ones are rejected ones.","14531e00":"Considering categorical fields are not one-hot-encoded, we will use Label Encoder to transform these fields","d670bbe6":"# Model Interpretability","2ad95099":"Let's see the Partial Plots of these important features","968c3caa":"# Fastai Learning\n\nIn this section, we will use Fastai Tabular Deep Learning model to do the classification task. \n\nOne amazing thing with Fastai tabular model is that it creates categorical embeddings of categorical data. This is very helpful and works in almost similar ways embeddings work in NLP or other areas.","9ec42d66":"In this case, as medical history 15's values increases, chances of polices getting accepted increases.","39780d94":"Let's create Fastai Tabular Databunch. Please note that in \"procs\", I am not using Normalize since I already normalized the data above","cd7b50c7":"Let's see if these feature engineering makes sense","6ca0ebbd":"Below steps are very important.\n\nI basically created few more columns in the data using Unsupervised ML techniques.\n\nI picked up few pairs of most correlated features (+ve, -ve) in the data using above correlation coefficients. Then using BIRCH clustering technique, I grouped these pairs into different clusters and labels of these clusters were then used as a feature for the data.\n\nApart from this, I also used GaussianMixture algorithm to create clustering based feature.\n\nI did this on an experiment basis (inspired by few Kaggle Kernels) and it turned out that these features became very important features for my classification task.","5bcaf22e":"For this individual, many things are working in his favour (remember, class 0 is rejection, class 1 is acceptance). This person has lower BMI and other factors are also either 0 or in negative range. So most likely his proposal will be accepted. Similarly for the below one.","7733a6c6":"Nothing much can be made out from the Dendrogram since we have lots of features but looks like the new features we created have crucial information compared to other features we had.","e4988e09":"Under \"extreme risk\" category, lots of policies are getting either rejected or issued on substandard terms.","c028177a":"# Normalizing Data\n\nThis function will be used to normalize the data. Its called Rank - Gaussian Normalization technique. In very simple terms, for continious data in a column are sorted as per their values and ranks are determined. These ranks are then normalized using Gaussian distribution. \n\nI found this technique of normalizing the continious data in dataset really helpful. ","bd008ea0":"# Converting the Multiclass problem into Binary classes problem","44dd9f84":"Lets create the Tabular Databunch","4663b187":"It can be seen that Green (non clean policies) and Magenta (clean policies) are all jumbled up. It looks like the DL \/ ML model needs to do a very good job to classify Green and Magenta dots. Let's see how do we fare","d342b536":"Few important observations here:\n\n1. Features are sorted in descending order of its importance.\n2. BMI has High (red in colour) and negative (less than 0) effect on the target. This means higher the BMI, higher the rejection.\n3. Conversely, Med Hist 4 has High (red) and positive (greater than 0) effect on the target. This means that the higher the value of Med Hist 4, the chances are higher for policy getting accepted.","f3930358":"The higher the value of bmi_mk15, the lower the chance that policy will be accepted.","0d4bed85":"Lets create X and Y for our classification model","5c4acf74":"More often, risky lives are not offered standard terms","a8f08c11":"This dependence plot shows that higher the BMI, higher the chance is for rejection of policies.","187efacd":"Let's see how does our Response variable correlated with others","7f2579f6":"Now, lets see the rankings of these features in terms of their contribution in this classification task","a0787e81":"But, before we proceed for XGBoost classification training, lets see how does our concatenated dataset looks like in two dimensional space using TSNE function we created above","0faf4857":"Let's see how does Dendrogram look like for this data","9506e920":"We wil normalize only the continious variables in the data","9d0dfc4d":"The higher the value of medical keyword 23, the higher the chance that policy will be accepted.","9ef97a0b":"Although texts in above graph are not very clear but it can be seen that in last few graphs (reflecting new variables we created), there is a clear difference between the shapes of dependent variables. This means that these newly created features will play very important roles in our classification model.","2f2c2a9c":"We can see in above Confusion Matrix that the model is doing pretty good job. Accuracy is around 84% on validation dataset which is quite decent for this problem.","6f8e31a3":"Let's drop ID from the data","d6c55265":"* Here, as medical keyword 15 is moving from 0 to be 1, chances of policies getting rejected increases","e929aeaf":"Again, in high-end-risk category, lots of policies are either getting rejected or issued at substandard terms.","71018eb5":"More often, overweight policyholders are not offered standard terms.","075a1588":"It shows that once BMI is increasing (between 0 to +2), chances of getting policy rejected becomes higher but then saturates after a certain threshold of BMI","fe37114b":"Let's create a function to represent data points in 2d space using TSNE","60b957f6":"The higher the value of medical keyword 4, the higher the chance that policy will be accepted.","f395b6d1":"# Data preparation for Fastai","cc90f191":"Compared to young lives and average lives, more often, old lives were not offered standard terms","93fa85b2":"Under non-low-end risk category, lots of policies are either getting rejected or issued at substandard terms.","316e0bcb":"We will create the Fastai Databunch again and get the data ready for training processes","1e0d1e62":"Lets see which features got selected as most important ones by the model","827ea20d":"This has a completely different trend. Higher the value, higher the chance of policies getting accepted.","3ccddad3":"These are ROC AUC Score Benchmarking. Looks like our score is in Excellent category.\n\nhttp:\/\/gim.unmc.edu\/dxtests\/ROC3.htm\n\n.90-1 = excellent (A)\n\n.80-.90 = good (B)\n\n.70-.80 = fair (C)\n\n.60-.70 = poor (D)\n\n.50-.60 = fail (F)\n","1b0e8fc6":"\"Response\" is the target variable in the data. Let's see the value counts of the target variable","7b038be7":"# Removing highly correlated features","5048297f":"This case will most likely be rejected. His BMI is higher as well.","a33c4104":"More often, extremely fat people are not offered standard terms","a63e9555":"This does not indicate any behaviour","6629eb8c":"The higher the value of Product Info 4, the higher the chance of policy getting accepted","e210706c":"There are around 128 features and on a very broad level, these can be categorized into:\n\n1. Product Information (boundary conditions)\n2. Age\n3. Height\n4. Weight\n5. BMI\n6. Employment Information\n7. Other insured information\n8. Family History\n9. Medical History\n10. Medical Keywords"}}