{"cell_type":{"ae5bfa3d":"code","d2e21fce":"code","8328e41d":"code","6633bacc":"code","ee93fb8a":"code","601a43b3":"code","ad4e96c4":"code","5beed6db":"code","63c0fb01":"code","186530b0":"code","f6a23ee6":"code","70b8da04":"code","497d5fec":"code","371220b2":"code","933e7458":"code","ad77df39":"code","0ddeb1b6":"code","917c67c6":"code","c0b585d2":"code","890b3ade":"markdown","00406a5e":"markdown","d49aacad":"markdown","a0d35f83":"markdown","d10bcbef":"markdown","af130f40":"markdown","d0d6c177":"markdown","a177c381":"markdown","abd100fc":"markdown","1131a7c8":"markdown"},"source":{"ae5bfa3d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk(\"\/kaggle\/input\"):\n    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n        pass\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d2e21fce":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nimport matplotlib.style as style\nstyle.use(\"fivethirtyeight\")\n\nimport plotly.express as px\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\n\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# stats\nfrom scipy import stats\n\n# preprocessing packages\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\n\n# Modeling packages\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold, RepeatedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import make_scorer\n\n# XGB\nimport xgboost as xg\n\n# LGBM\nfrom lightgbm import LGBMRegressor\n\n# Keras\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nimport keras.backend as kb","8328e41d":"# set true to run feature generation, if false it will read prebuilt dataset\nrun_feature_generation = False\n\n# set path to the stored dataset you want to load\ntrain_dataset_path = \"..\/input\/optiver-feature-set-4\/train_df.csv\"\ntest_dataset_path = \"..\/input\/optiver-feature-set-4\/test_df.csv\"\n\n# set to True to perform model tuning\nrun_tuning = False\n\n# chose final model\n# options: \"Keras\" \"lgbm\"\n\nfinal_model = \"lgbm\"","6633bacc":"train = pd.read_csv(\"..\/input\/optiver-realized-volatility-prediction\/train.csv\")\ntest = pd.read_csv(\"..\/input\/optiver-realized-volatility-prediction\/test.csv\")","ee93fb8a":"# functions to access orderbook and tradebook by stock_id\n\ndef ffill(data_df):\n    \"\"\"\n    Forward fill the missing \"seconds_in_bucket\".\n    \"\"\"\n    data_df = data_df.set_index(['time_id', 'seconds_in_bucket'])\n    data_df = data_df.reindex(pd.MultiIndex.from_product([data_df.index.levels[0], np.arange(0,600)], names = ['time_id', 'seconds_in_bucket']), method='ffill')\n    return data_df.reset_index()\n\ndef ffill_fast(data_df):\n    \"\"\"\n    Forward fill the missing \"seconds_in_bucket\".\n    This method is about 7x faster than ffill\n    \"\"\"\n\n    time_ids = pd.DataFrame(pd.Series(data_df.time_id.unique()), columns=[\"time_id\"])\n    seconds_in_bucket = pd.DataFrame(pd.Series(np.arange(0,600)), columns=[\"seconds_in_bucket\"])\n\n    # create key to join on\n    time_ids['key'] = 1\n    seconds_in_bucket['key'] = 1\n\n    # merge dataframes and drop \"key\"\n    prod = pd.merge(time_ids, seconds_in_bucket, on ='key').drop(\"key\", 1)\n\n    # merge with orderbook\n    data_df = prod.merge(data_df, on=[\"time_id\",\"seconds_in_bucket\"], how=\"left\")\n    data_df.ffill(inplace=True)\n    \n    return data_df\n\ndef get_orderbook_df(dataset, stock_id):\n    book = pd.read_parquet(f\"..\/input\/optiver-realized-volatility-prediction\/book_{dataset}.parquet\/stock_id={stock_id}\")\n    book[\"stock_id\"] = stock_id\n    return book\n\ndef get_tradebook_df(dataset, stock_id):\n    trade = pd.read_parquet(f\"..\/input\/optiver-realized-volatility-prediction\/trade_{dataset}.parquet\/stock_id={stock_id}\")\n    trade[\"stock_id\"] = stock_id\n    return trade\n\n# functions for WAP and volatility calculations\n\ndef calculate_wap1(df):\n    \"\"\"\n    Calcualte WAP from price1\n    \"\"\"\n    return (df[\"bid_price1\"] * df[\"ask_size1\"] + df[\"ask_price1\"] * df[\"bid_size1\"]) \/ (df[\"bid_size1\"] + df[\"ask_size1\"])\n\ndef calculate_wap2(df):\n    \"\"\"\n    Calcualte WAP from price2\n    \"\"\"\n    return (df[\"bid_price2\"] * df[\"ask_size2\"] + df[\"ask_price2\"] * df[\"bid_size2\"]) \/ (df[\"bid_size2\"] + df[\"ask_size2\"])\n\ndef calculate_wap_full(df):\n    \"\"\"\n    Calcualte WAP from price1 and price2\n    \"\"\"\n    wf = (\n        df[\"bid_price1\"] * df[\"ask_size1\"] + df[\"ask_price1\"] * df[\"bid_size1\"] + \n        df[\"bid_price2\"] * df[\"ask_size2\"] + df[\"ask_price2\"] * df[\"bid_size2\"]\n    ) \/ (df[\"bid_size1\"] + df[\"ask_size1\"] + df[\"bid_size2\"] + df[\"ask_size2\"])\n    return wf\n\ndef log_return(wap):\n    return np.log(wap).diff().fillna(0)\n\ndef realized_volatility(log_return):\n    return np.sqrt(np.sum(log_return**2))\n\ndef rmspe(y_true, y_pred):\n    return (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))\n\ndef generate_feature_set_for_stock(stock_id, dataset):\n    \"\"\"\n    stock_id - id of the stock you want to generate features for\n    dataset - \"train\" or \"test\"\n    \"\"\"\n    \n    df = pd.read_csv(f\"..\/input\/optiver-realized-volatility-prediction\/{dataset}.csv\")\n    \n    df_sub = df[df[\"stock_id\"] == stock_id].copy(deep=True)\n\n    orderbook = get_orderbook_df(dataset, stock_id)\n    tradebook = get_tradebook_df(dataset, stock_id)\n\n    # takes about 7 seconds - reduced to about 1 second\n    orderbook = ffill_fast(orderbook)\n\n    # calculate realized volatility (only full) - 4-5 seconds - reduced to 1.5 seconds\n    orderbook[\"wap_full\"] = calculate_wap_full(orderbook)\n    orderbook[\"wap_full_log\"] = np.log(orderbook[\"wap_full\"])\n    orderbook['lr_full'] = orderbook.groupby(['time_id'])['wap_full_log'].diff().fillna(0)\n    orderbook[\"lr_full_sq\"] = orderbook[\"lr_full\"]**2\n    rv_full = orderbook.groupby([\"time_id\"])[\"lr_full_sq\"].sum().pow(1\/2).to_frame()\n    rv_full[\"stock_id\"] = stock_id\n    df_sub = df_sub.merge(rv_full, on=[\"time_id\",\"stock_id\"], how=\"left\")\n\n    # calculate mean spread1 - 50ms\n    orderbook[\"spread1\"] = orderbook[\"ask_price1\"] - orderbook[\"bid_price1\"]\n    mean_spread1 = orderbook.groupby([\"time_id\"])[\"spread1\"].mean().to_frame()\n    mean_spread1[\"stock_id\"] = stock_id\n    df_sub = df_sub.merge(mean_spread1, on=[\"time_id\",\"stock_id\"], how=\"left\")\n\n    # calculate mean size_balance1 -50ms\n    orderbook[\"size_balance1\"] = orderbook[\"bid_size1\"] - orderbook[\"ask_size1\"]\n    mean_size_balance1 = orderbook.groupby([\"time_id\"])[\"size_balance1\"].mean().to_frame()\n    mean_size_balance1[\"stock_id\"] = stock_id\n    df_sub = df_sub.merge(mean_size_balance1, on=[\"time_id\",\"stock_id\"], how=\"left\")\n\n    # calculate seconds_in_bucket bins - 60ms\n    orderbook[\"seconds_in_bucket_bin_30\"] = (orderbook[\"seconds_in_bucket\"] \/ 30).astype(int).to_frame()\n\n    # calculate realized volatility for each bin - 30 seconds - reduced to 0.5 seconds!\n    rv_full_bin_30 = orderbook.groupby([\"time_id\", 'seconds_in_bucket_bin_30'])[\"lr_full_sq\"].sum().pow(1\/2).to_frame()\n    rv_full_bin_30.reset_index(inplace=True)\n    rv_full_bin_30[\"bin_index\"] = rv_full_bin_30.groupby([\"time_id\"])[\"lr_full_sq\"].cumcount()\n\n    # calculate slope of the change in volatility - 2 seconds - reduced to <1 second\n    rv_full_bin_30_slope = rv_full_bin_30.groupby([\"time_id\"]).apply(lambda x: np.polyfit(x.bin_index, x.lr_full_sq, 1)[0]).reset_index(name='rv_full_bin_30_slope')\n    rv_full_bin_30_slope[\"stock_id\"] = stock_id\n    df_sub = df_sub.merge(rv_full_bin_30_slope, on=[\"time_id\",\"stock_id\"], how=\"left\")\n\n    # order count and order size from tradebook\n    order_count = tradebook.groupby([\"time_id\",\"stock_id\"])[\"order_count\"].sum()\n    order_count[\"stock_id\"] = stock_id\n    df_sub = df_sub.merge(order_count, on=[\"time_id\",\"stock_id\"], how=\"left\")\n\n    order_size = tradebook.groupby([\"time_id\",\"stock_id\"])[\"size\"].sum()\n    order_size[\"stock_id\"] = stock_id\n    df_sub = df_sub.merge(order_size, on=[\"time_id\",\"stock_id\"], how=\"left\")\n    \n    # calculate slope in change of log return\n    lr_full_slope = orderbook.groupby([\"time_id\"]).apply(lambda x: np.polyfit(x.seconds_in_bucket, x.lr_full_sq, 1)[0]).reset_index(name='lr_full_slope')\n    lr_full_slope[\"stock_id\"] = stock_id\n    df_sub = df_sub.merge(lr_full_slope, on=[\"time_id\",\"stock_id\"], how=\"left\")\n    \n    # calculate slope in change of order size\n    size_slope = tradebook.groupby([\"time_id\"]).apply(lambda x: np.polyfit(x.seconds_in_bucket+1, x[\"size\"], 1)[0]).reset_index(name='size_slope')\n    size_slope[\"stock_id\"] = stock_id\n    df_sub = df_sub.merge(size_slope, on=[\"time_id\",\"stock_id\"], how=\"left\")\n    \n    # calculate slope in change of order counts\n    order_count_slope = tradebook.groupby([\"time_id\"]).apply(lambda x: np.polyfit(x.seconds_in_bucket+1, x[\"order_count\"], 1)[0]).reset_index(name='order_count_slope')\n    order_count_slope[\"stock_id\"] = stock_id\n    df_sub = df_sub.merge(order_count_slope, on=[\"time_id\",\"stock_id\"], how=\"left\")\n    \n    return df_sub\n    \ndef generate_feature_set_1(dataset):\n    \"\"\"\n    Calculates realized volatility from both price levels\n    \"\"\"\n    \n    if dataset == \"train\":\n        stock_ids = list(train[\"stock_id\"].unique())\n    elif dataset == \"test\":\n        stock_ids = list(test[\"stock_id\"].unique())\n    else:\n        print(\"Please provide dataset parameter\")\n        return\n    \n    final_df = pd.DataFrame()\n\n    for stock_id in stock_ids:\n        \n        # append\n        df_sub = generate_feature_set_for_stock(stock_id, dataset)\n        final_df = final_df.append(df_sub)\n\n    final_df.rename({\"lr_full_sq\": \"rv_full\"}, axis=1, inplace=True)\n    final_df = final_df.reset_index(drop=True)\n    \n    return final_df","601a43b3":"# data transformation functions\ndef log_transform(df, list_of_features):\n    \"\"\"\n    Perform log tranform on a list of features.\n    Useful for e.g. exponentially distributed variables.\n    \"\"\"\n    df1 = df.copy(True)\n    df1[list_of_features] = np.log(df1[list_of_features])\n        \n    return df1\n\n\ndef scale(df, list_of_features):\n    \"\"\"\n    Scale variables using StandardScaler\n    \"\"\"\n    new_df = df.copy(True)\n        \n    new_df = new_df.fillna(0)\n    new_df = new_df.reset_index(drop=True)\n    \n    scaler = StandardScaler()\n    \n    new_df[list_of_features] = scaler.fit_transform(new_df[list_of_features])\n    \n    return new_df\n\n\ndef oh_encode(df, list_of_features):\n    \"\"\"\n    Performe one-hot encoding on categorical variables.\n    \"\"\"\n    new_df = df.copy(True)\n    \n    unenc_features = [item for item in df.columns if item not in list_of_features]\n    \n    enc = OneHotEncoder()\n    \n    df_enc = enc.fit_transform(new_df[list_of_features]).toarray()\n    column_names = enc.get_feature_names(list_of_features)\n    df_enc = pd.DataFrame(df_enc, columns=column_names)\n    \n    df_unenc = new_df[unenc_features]\n\n    new_df = df_enc.join(df_unenc)\n    \n    return new_df\n\ndef to_categorical(df, list_of_features):\n    \"\"\"\n    Change type of variables to \"category\".\n    \"\"\"\n    new_df = df.copy(True)\n    \n    for feature in list_of_features:\n        new_df[feature] = new_df[feature].astype('category').cat.as_ordered()\n    \n    return new_df","ad4e96c4":"%%time\n\n# Generate (or read) train and test dataset\n\nif run_feature_generation:\n    # Generate features and add them to train and test dataframes\n    train_df = generate_feature_set_1(\"train\")\n    test_df = generate_feature_set_1(\"test\")\n\n    # store datasets for future use\n    train_df.to_csv(\"train_df.csv\",index=False)\n    test_df.to_csv(\"test_df.csv\",index=False)\n\nelse:\n    train_df = pd.read_csv(train_dataset_path)\n    test_df = pd.read_csv(test_dataset_path) # change back before deploy\n    \ntrain_df_copy = train_df.copy(True)\ntest_df_copy = test_df.copy(True)","5beed6db":"## calculate time_id_rank\n\ndef calculate_time_id_rank(df):\n    \"\"\"\n    Calculate \"time id rank\" - rank time ids by volatility.\n    \"\"\"\n    mean_rv_full_by_time = df[[\"time_id\",\"rv_full\"]].groupby([\"time_id\"]).mean()\n    mean_rv_full_by_time = mean_rv_full_by_time.reset_index()\n    mean_rv_full_by_time.head()\n\n    mean_rv_full_by_time.sort_values(by=\"rv_full\", inplace=True, ignore_index=True)\n\n    mean_rv_full_by_time[\"time_id_rank\"] = mean_rv_full_by_time.index\n    mean_rv_full_by_time\n\n    df = df.merge(mean_rv_full_by_time[[\"time_id\",\"time_id_rank\"]], on=\"time_id\", how=\"left\")\n    \n    return df\n\ntrain_df = calculate_time_id_rank(train_df)\ntest_df = calculate_time_id_rank(test_df)","63c0fb01":"train_df.head()","186530b0":"# lets check variable distributions\nfig = plt.figure(figsize=[18,18])\n\nfor index, variable in enumerate(list(train_df.columns[2:])):\n    ax = fig.add_subplot(4, 3, index+1)\n    sns.distplot(train_df[variable], ax=ax)\n\nplt.show()","f6a23ee6":"sns.pairplot(train_df.drop([\"time_id\",\"stock_id\",\"time_id_rank\"], axis=1).sample(1000))","70b8da04":"fig, ax = plt.subplots(figsize=(14,10))\nsns.heatmap(train_df.drop([\"time_id\",\"stock_id\"], axis=1).corr(), annot=True, linewidths=.5, ax=ax)","497d5fec":"# Perform feature scaling, encoding, etc.\n\nlist_of_features_to_log = [\"rv_full\",\"order_count\",\"size\",\"spread1\",\"lr_full_slope\",\"size_slope\",\"order_count_slope\"]\nlist_of_features_to_scale = [\"rv_full\",\"order_count\",\"size\",\"rv_full_bin_30_slope\",\"spread1\",\"size_balance1\",\"lr_full_slope\",\"size_slope\",\"order_count_slope\"]\nlist_of_features_to_encode = [\"stock_id\"]\nlist_of_features_to_categorical = [\"stock_id\",\"time_id\",\"time_id_rank\"]\n\n# Log (not used)\ntrain_prep_log_df = log_transform(train_df, list_of_features_to_log)\ntest_prep_log_df = log_transform(test_df, list_of_features_to_log)\n\n# Scale\ntrain_prep_df = scale(train_df, list_of_features_to_scale)\ntest_prep_df = scale(test_df, list_of_features_to_scale)\n\n# OH encode\ntrain_prep_enc_df = scale(oh_encode(train_df, list_of_features_to_encode), list_of_features_to_scale)\ntest_prep_enc_df = scale(oh_encode(test_df, list_of_features_to_encode), list_of_features_to_scale)\n\n# To categorical (Keras)\ntrain_prep_keras_df = to_categorical(scale(train_df, list_of_features_to_scale), list_of_features_to_categorical)\ntest_prep_keras_df = to_categorical(scale(test_df, list_of_features_to_scale), list_of_features_to_categorical)\n\ntrain_prep_df.head()","371220b2":"# Split datasets into X and y part\n\nX_train = train_prep_df.drop([\"time_id\", \"target\"], axis=1)\ny_train = train_prep_df[\"target\"]\nX_test = test_prep_df.drop([\"time_id\",\"row_id\"], axis=1)\n\nX_train_enc = train_prep_enc_df.drop([\"time_id\", \"target\"], axis=1)\ny_train_enc = train_prep_enc_df[\"target\"]\nX_test_enc = test_prep_enc_df.drop([\"time_id\",\"row_id\"], axis=1)\n\nX_train_keras = train_prep_keras_df.drop([\"time_id\", \"target\"], axis=1)\ny_train_keras = train_prep_keras_df[\"target\"]\nX_test_keras = test_prep_keras_df.drop([\"time_id\",\"row_id\"], axis=1)\n\n# set up scorer\nscorer = make_scorer(rmspe, greater_is_better = False)","933e7458":"## Modeling functions\n\ndef test_model(model, X_train, n_jobs=1, trans=None):\n    \"\"\"\n    Test model using X-validation\n    \"\"\"\n    pipeline = Pipeline(steps=[('t', trans), ('m', model)])\n    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n    \n    scores = cross_val_score(\n        pipeline, \n        X_train, \n        y_train, \n        cv=cv, \n        scoring=scorer, \n        verbose=3, \n        n_jobs=n_jobs, \n        error_score='raise'\n    )\n    \n    print('RMSPE: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))\n    return model\n\n\n# hyper parameter optimization functions\n\n#simple performance reporting function\ndef reg_performance(model):\n    print('Best Score: ' + str(model.best_score_))\n    print('Best Parameters: ' + str(model.best_params_))\n    \n#tuning function using GridSearch\ndef tune_model(model, X_train, param_grid, trans=None, n_jobs=1):\n    \"\"\"\n    Tune model hyperparameters using GridSearchCV\n    \"\"\"\n    pipeline = Pipeline(steps=[('t', trans), ('model', model)])\n\n    grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5, verbose=2, n_jobs=n_jobs, scoring=scorer)\n    best_grid_search = grid_search.fit(X_train,y_train)\n    reg_performance(best_grid_search)","ad77df39":"# tunning\n# LGBM\n\nlgbm = LGBMRegressor()\n\nparam_grid_lgbm = {'model__learning_rate': [0.13, 0.15, 0.18, 0.20],\n                    'model__n_estimators': [70],\n                    'model__boosting_type': ['gbdt'],\n                    'model__objective': ['regression_l1'],\n                    'model__reg_alpha': [3],\n                    'model__reg_lambda': [3]\n                    }\n\nif run_tuning:\n    tune_model(lgbm, X_train, param_grid_lgbm, n_jobs=3)","0ddeb1b6":"# tunning\n# XGB\n\nxgb = xg.XGBRegressor(random_state = 0)\n\nparam_grid_xgb = {'model__learning_rate': [0.03, 0.05, 0.07, 0.09, 0.11, 0.13, 0.15, 0.18, 0.20],\n                  'model__n_estimators': [50,70,90],\n                  'model__booster': [\"gbtree\", \"gblinear\"],\n                  'model__reg_alpha': [1,2,3,5,10],\n                  'model__reg_lambda': [1,2,3,5,10]\n                 }\n\nif run_tuning:\n    tune_model(xgb, X_train_enc, param_grid_xgb, n_jobs=3)","917c67c6":"# Keras\nif final_model == \"Keras\":\n    def rmspe_keras(y_true, y_pred):\n        return (kb.sqrt(kb.mean(kb.square((y_true - y_pred) \/ y_true))))\n\n    def build_regressor():\n        regressor = Sequential()\n        regressor.add(Dense(units = len(X_train_keras.columns)**2, kernel_initializer = 'uniform', activation = 'relu', input_dim = len(X_train_keras.columns)))\n        regressor.add(Dense(units = len(X_train_keras.columns)*2, kernel_initializer = 'uniform', activation = 'relu'))\n        regressor.add(Dense(units = len(X_train_keras.columns)*2, kernel_initializer = 'uniform', activation = 'relu'))\n        regressor.add(Dense(units = len(X_train_keras.columns)*2, kernel_initializer = 'uniform', activation = 'relu'))\n        regressor.add(Dense(units = len(X_train_keras.columns)*2, kernel_initializer = 'uniform', activation = 'relu'))\n        regressor.add(Dense(units = len(X_train_keras.columns)*2, kernel_initializer = 'uniform', activation = 'relu'))\n        regressor.add(Dense(units = len(X_train_keras.columns)*2, kernel_initializer = 'uniform', activation = 'relu'))\n        regressor.add(Dense(units = len(X_train_keras.columns)*2, kernel_initializer = 'uniform', activation = 'relu'))\n        regressor.add(Dense(units = len(X_train_keras.columns)*2, kernel_initializer = 'uniform', activation = 'relu'))\n        regressor.add(Dense(units = len(X_train_keras.columns), kernel_initializer = 'uniform', activation = 'relu'))\n        regressor.add(Dense(units = 1, kernel_initializer = 'uniform'))\n        regressor.compile(optimizer = 'adam', loss = rmspe_keras, metrics = [rmspe_keras])\n        return regressor\n\n    keras_regressor = KerasRegressor(build_fn = build_regressor, batch_size = 2**5, epochs = 100)\n\n    keras_regressor.fit(X_train_keras,y_train_keras)\n    predictions = pd.DataFrame(keras_regressor.predict(X_test_keras), columns=[\"target\"])\n    submission = test_prep_keras_df.join(predictions)[[\"row_id\",\"target\"]]\n    submission.to_csv(\"submission.csv\", index=False)","c0b585d2":"# LGBM\nif final_model == \"lgbm\":\n    lgbm1 = LGBMRegressor(n_estimators=500)\n\n    lgbm1.fit(X_train,y_train)\n    predictions = pd.DataFrame(lgbm1.predict(X_test), columns=[\"target\"])\n    submission = test_prep_df.join(predictions)[[\"row_id\",\"target\"]]\n    submission.to_csv(\"submission.csv\", index=False)","890b3ade":"## Import packages","00406a5e":"### Log transformation, Scaling and Encoding","d49aacad":"## Settings","a0d35f83":"# Optiver - volatility prediction\n\nChallenge homepage: https:\/\/www.kaggle.com\/c\/optiver-realized-volatility-prediction\/overview\n\nIntro notebook: https:\/\/www.kaggle.com\/jiashenliu\/introduction-to-financial-concepts-and-data\n\n### Datasets\nThe goal of the challenge is to predict realized volatility for given stocks.\n\nDatasets contains data for 112 stocks. For each stock there are 3830 time intervals (time_id). Time intervals are 20 minutes long.\nThere are 3 datasets provided:\n1. train.csv - provides target variable used for training. It the actual value of realized volatility for each stock_id - time_id pair.\n2. orderbook - contains top 2 levels of order prices and sizes\n3. tradebook - contains data for all the trades that occured (price, size, order count)\n\nOrderbook and tradebook datasets contains data on a per second level and they only contain first 10 minutes of the data.\nThe goal is to use the data of the first 10 minutes of the time interval and predict the volatility of the full 20 minutes.","d10bcbef":"## Data preparation and feature engineering\n\n### Feature engineering plan\n\nThe folowing features were created:\n1. **rv_full** -> This is the realized volatilty calculated from the orderbook using both first and second level price\/size.\n2. **spread1** -> Mean spread between bid and ask price of the first level prices.\n3. **size_balance1** -> Mean difference between the first level bid and ask sizes.\n4. **rv_full_bin_30_slope** -> orderbook was sliced into 30 time intervals (bins). For each interval a realized volatility was calculated. Finally a line was fit to the datapoints to get the slope. The idea is that if the volatility in the first 10 minutes is falling it will also fall in the near future (second part of the interval).\n5. **order_count** -> Sum of all order counts.\n6. **size** -> Sum of size of the orders.\n7. **lr_full_slope** -> Slope of the calculated log returns. The idea is the same as with rv_full_bin_30_slope.\n8. **size_slope** -> Slope of the linear line fitted over trade order sizes.\n9. **order_count_slope** -> Slope of the linear line fitted over trade order counts.\n10. **time_id_rank** -> Given that time_ids are always representing the same time across stock_ids the idea is to rank them based on the average volatility. More volatile time_ids get higher rank.","af130f40":"### Ploting the data","d0d6c177":"## Modeling","a177c381":"Best Score: -0.2876800501071248\nBest Parameters: {'model__boosting_type': 'gbdt', 'model__learning_rate': 0.13, 'model__n_estimators': 70, 'model__objective': 'regression_l1', 'model__reg_alpha': 3, 'model__reg_lambda': 3}","abd100fc":"### Final build and submission","1131a7c8":"### Tuning"}}