{"cell_type":{"34dde1e7":"code","c425dd9d":"code","d0ee3817":"code","acf85ce0":"code","c9df7658":"code","2ee31b81":"code","0c455bd5":"code","4dad892a":"code","3c745f1e":"code","4f80aa30":"code","c5e158c0":"code","83a37c05":"code","5018f12a":"code","d1f312a3":"code","b8f691ca":"code","aa0d67ea":"code","f5df30ab":"code","96671564":"code","586ea452":"code","fed4c726":"code","97a11dab":"code","7d2f27a9":"code","f24fc221":"code","3c90c425":"code","09cc9ea0":"code","a3f0ee53":"code","ea471c1e":"code","b887d869":"code","6ccf34f3":"code","7f173387":"code","e94bf9f9":"code","d0f69648":"code","84162d7f":"code","7cd11f02":"code","0eec429a":"code","0d991159":"code","a4bb562c":"code","31334366":"code","831b4b41":"code","00132aa3":"code","d9eb339e":"code","3dfc36c1":"code","20f58021":"code","6a3a5b06":"code","66140642":"code","bf0da9c5":"code","e31e897e":"code","4f929d76":"code","e7098eb1":"code","b464cf4e":"code","cb7efc16":"code","aa1bc4a2":"code","f62ddf89":"code","0b88dfdc":"code","dc4f9bb9":"code","321ae768":"code","9b2b2031":"code","b53e4408":"code","92c195d7":"code","fa5d08c3":"code","09fba1df":"code","8bd773cb":"code","da61f478":"code","6f6e1eae":"code","c09388d1":"code","c672f81a":"code","e8e91baf":"code","8f502155":"code","5b7c7c6d":"code","12cdef87":"code","1d97ceb5":"code","2cabffe6":"code","24917d74":"markdown","bc71b88e":"markdown","c4876977":"markdown","9be59936":"markdown","9bae4ae1":"markdown","d3f795af":"markdown","05187ca5":"markdown","d7d07a7b":"markdown","61a37eca":"markdown","4e159db6":"markdown","aef755fd":"markdown"},"source":{"34dde1e7":"!pip install -U scikit-learn","c425dd9d":"import warnings\nwarnings.filterwarnings('ignore')","d0ee3817":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nimport string\nfrom nltk.stem import SnowballStemmer\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nimport os\nfrom textblob import TextBlob\nfrom nltk.stem import PorterStemmer\nfrom textblob import Word\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nimport sklearn.feature_extraction.text as text\nfrom sklearn import model_selection, preprocessing,linear_model, naive_bayes, metrics, svm\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import cross_val_score\nfrom io import StringIO\nimport seaborn as sns\nfrom tqdm import tqdm","acf85ce0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c9df7658":"tqdm.pandas()","2ee31b81":"df = pd.read_csv('..\/input\/us-consumer-finance-complaints\/consumer_complaints.csv', encoding='latin-1')","0c455bd5":"df.head()","4dad892a":"df[df['consumer_complaint_narrative'].notnull()]","3c745f1e":"df.shape","4f80aa30":"df.dtypes","c5e158c0":"df.isnull().sum()","83a37c05":"df.shape","5018f12a":"# We will extract the required columns for our predictions\n\ndf = df[['product', 'consumer_complaint_narrative']]\ndf = df[df['consumer_complaint_narrative'].notnull()]","d1f312a3":"df.shape","b8f691ca":"df.head()","aa0d67ea":"df['product'].nunique()","f5df30ab":"plt.figure(figsize=(15, 10))\nsns.histplot(x='product', data=df)\nplt.xticks(rotation=90)\nplt.title('Distribution of complaints')\nplt.show()","96671564":"def processRow(row):\n    import re\n    import nltk\n    from textblob import TextBlob\n    from nltk.corpus import stopwords\n    from nltk.stem import PorterStemmer\n    from textblob import Word\n    from nltk.util import ngrams\n    import re\n    from wordcloud import WordCloud, STOPWORDS\n    from nltk.tokenize import word_tokenize\n    tweet = row\n    #Lower case\n    tweet.lower()\n    #Removes unicode strings like \"\\u002c\" and \"x96\"\n    tweet = re.sub(r'(\\\\u[0-9A-Fa-f]+)',r\"\", tweet)\n    tweet = re.sub(r'[^\\x00-\\x7f]',r\"\",tweet)\n    #convert any url to URL\n    tweet = re.sub('((www\\.[^\\s]+)|(https?:\/\/[^\\s]+))','URL',tweet)\n    #Convert any @Username to \"AT_USER\"\n    tweet = re.sub('@[^\\s]+','AT_USER',tweet)\n    #Remove additional white spaces\n    tweet = re.sub('[\\s]+', ' ', tweet)\n    tweet = re.sub('[\\n]+', ' ', tweet)\n    print(\"VALUE OF S is\", tweet)\n    #Remove not alphanumeric symbols white spaces\n    tweet = re.sub(r'[^\\w]', ' ', tweet)\n    #Removes hastag in front of a word \"\"\"\n    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n    #Replace #word with word\n    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n    #Remove :( or :)\n    tweet = tweet.replace(':)',\"\")\n    tweet = tweet.replace(':(',\"\")\n    #remove numbers\n    tweet = \"\".join([i for i in tweet if not i.isdigit()])\n    #remove multiple exclamation\n    tweet = re.sub(r\"(\\!)\\1+\", ' ', tweet)\n    #remove multiple question marks\n    tweet = re.sub(r\"(\\?)\\1+\", ' ', tweet)\n    #remove multistop\n    tweet = re.sub(r\"(\\.)\\1+\", ' ', tweet)\n    #lemma\n    from textblob import Word\n    tweet =\" \".join([Word(word).lemmatize() for word in tweet.split()])\n    #stemmer\n    #st = PorterStemmer()\n    #tweet=\" \".join([st.stem(word) for word in tweet.split()])\n    #Removes emoticons from text\n    tweet = re.sub(':\\)|;\\)|:-\\)|\\(-:|:-D|=D|:P|xD|X-p|\\^\\^|:-*|\\^\\.\\^|\\^\\-\\^|\\^\\_\\^|\\,-\\)|\\)-:|:\\'\\(|:\\(|:-\\(|:\\S|T\\.T|\\.\\_\\.|:<|:-\\S|:-<|\\*\\-\\*|:O|=O|=\\-O|O\\.o|XO|O\\_O|:-\\@|=\/|:\/|X\\-\\(|>\\.<|>=\\(|D:', \"\", tweet)\n    #trim\n    tweet = tweet.strip('\\'\"')\n    row = tweet\n    return row","586ea452":"df.rename(columns={'consumer_complaint_narrative':'complaint'}, inplace=True)\ndf.head()","fed4c726":"df['complaint'] = df['complaint'].progress_apply(lambda x: processRow(x))","97a11dab":"df.head()","7d2f27a9":"# Working on splitting the data\nx_train, x_test, y_train, y_test = model_selection.train_test_split(df['complaint'], df['product'])","f24fc221":"le = preprocessing.LabelEncoder()\ny_train = le.fit_transform(y_train)\ny_test = le.fit_transform(y_test)","3c90c425":"tfidf = TfidfVectorizer()\ntfidf.fit(df['complaint'])\nx_train_t = tfidf.transform(x_train)\nx_test_t = tfidf.transform(x_test)","09cc9ea0":"x_train_t.shape","a3f0ee53":"x_test_t.shape","ea471c1e":"y_train","b887d869":"model = linear_model.LogisticRegression(solver='liblinear').fit(x_train_t, y_train)\naccuracy = metrics.accuracy_score(model.predict(x_test_t), y_test)\nprint(\"Accuracy: \", accuracy)","6ccf34f3":"print(metrics.classification_report(y_test, model.predict(x_test_t), target_names=df['product'].unique()))","7f173387":"matr = confusion_matrix(y_test, model.predict(x_test_t))","e94bf9f9":"df['category_id'] = df['product'].factorize()[0]","d0f69648":"df['category_id']","84162d7f":"cleaned_df = df[['product', 'category_id']].drop_duplicates().sort_values('category_id')\ncleaned_df.head()","7cd11f02":"conv = dict(cleaned_df.values)\ndict(cleaned_df.values)","0eec429a":"inverse_conv = dict(cleaned_df[['category_id','product']].values)\ndict(cleaned_df[['category_id','product']].values)","0d991159":"plt.figure(figsize=(10, 10))\nsns.heatmap(matr, annot=True, xticklabels=cleaned_df[['product']].values,\n           yticklabels=cleaned_df[['product']].values, cmap=\"BuPu\", fmt='d')\nplt.ylabel('True value')\nplt.xlabel('Predicted value')\nplt.show()","a4bb562c":"# Predicitng in real time\nsentence = [\"This company refuses to provide me verification andvalidation of debt\"+ \"per my right under the FDCPA.I do not believe this debt is mine.\"]\nfeature_sent = tfidf.transform(sentence)\npred = model.predict(feature_sent)\nprint(pred)","31334366":"print(sentence)\nprint(\"This has been predicted as:\",inverse_conv[pred[0]])","831b4b41":"df = pd.read_csv('..\/input\/amazon-fine-food-reviews\/Reviews.csv')\ndf.head()","00132aa3":"df.info()","d9eb339e":"df.dtypes","3dfc36c1":"df.shape","20f58021":"df.isnull().sum()","6a3a5b06":"# Column we are interested in:\n# Review summary\ndf['Summary'].head()","66140642":"# Review description\ndf['Text'].head()","bf0da9c5":"from textblob import TextBlob\nfrom textblob import Word\nfrom nltk.corpus import stopwords\n\n# Lowercase\ndf['Text'] = df['Text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n# Remove punctuation\ndf['Text'] = df['Text'].str.replace('[^\\w\\s]', \"\")\ndf['Text'].head()","e31e897e":"# Spelling corrections - very expensive process\n#df['Text'] = df['Text'].progress_apply(lambda x: str(TextBlob(x).correct()))","4f929d76":"df['Text'] = df['Text'].progress_apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))","e7098eb1":"df.dropna(inplace=True)\ndf.Score.hist(bins=5, grid=False)\nplt.show()","b464cf4e":"print(df.groupby('Score').count().Id)","cb7efc16":"score1 = df[df['Score']==1].sample(n=28000)\nscore2 = df[df['Score']==2].sample(n=28000)\nscore3 = df[df['Score']==3].sample(n=28000)\nscore4 = df[df['Score']==4].sample(n=28000)\nscore5 = df[df['Score']==5].sample(n=28000)","aa1bc4a2":"df_final = pd.concat([score1, score2, score3, score4, score5], axis=0)\ndf_final","f62ddf89":"df_final.reset_index(drop=True, inplace=True)","0b88dfdc":"df_final.head()","dc4f9bb9":"df_final.shape","321ae768":"print(df_final.groupby('Score').count().Id)","9b2b2031":"from wordcloud import WordCloud\nfrom wordcloud import STOPWORDS","b53e4408":"# Wordcloud input is a single string\ntotal_str = df_final.Summary.str.cat()\nwordcloud = WordCloud(background_color='white')\nwordcloud.generate(total_str)\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud,interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","92c195d7":"# Splitting our reviews\n\ndf_negative = df_final[df_final['Score'].isin([1, 2])]\ndf_positive = df_final[df_final['Score'].isin([4, 5])]\n# Transform into a single string\ndf_negative_s = df_negative.Summary.str.cat()\ndf_positive_s = df_positive.Summary.str.cat()","fa5d08c3":"neg_wc = WordCloud(background_color='white').generate(df_negative_s)\npos_wc = WordCloud(background_color='white').generate(df_positive_s) ","09fba1df":"fig = plt.figure(figsize=(10,10))\nplt.imshow(neg_wc,interpolation='bilinear')\nplt.axis(\"off\")\nplt.title('Reviews with Negative Scores',fontsize=20)\nplt.show()","8bd773cb":"fig = plt.figure(figsize=(10,10))\nplt.imshow(pos_wc,interpolation='bilinear')\nplt.axis(\"off\")\nplt.title('Reviews with Positive Scores',fontsize=20)\nplt.show()","da61f478":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport re\nimport os\nimport sys\nimport ast","6f6e1eae":"!pip install vaderSentiment","c09388d1":"from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nanalyzer = SentimentIntensityAnalyzer()","c672f81a":"listing = []\nfor row in tqdm(df['Text']):\n    vs = analyzer.polarity_scores(row)\n    listing.append(vs)\n    \ndf_results = pd.DataFrame(listing)\ndf_results.head()","e8e91baf":"df_with_sent = pd.concat([df.reset_index(drop=True), df_results], axis=1)\ndf_with_sent.head()","8f502155":"df_with_sent['Sentiment'] = np.where(df_results['compound']>=0, 'Positive', 'Negative')","5b7c7c6d":"df_with_sent.head()","12cdef87":"df_with_sent.head()","1d97ceb5":"sns.countplot(df_with_sent['Sentiment'])\nplt.show()","2cabffe6":"df_with_sent.groupby('ProductId')['Sentiment'].value_counts()","24917d74":"The VADER library returns 4 values such as:\n\n* pos: The probability of the sentiment to be positive\n* neu: The probability of the sentiment to be neutral\n* neg: The probability of the sentiment to be negative\n* compound: The normalized compound score which calculates the sum of all lexicon ratings and takes values from -1 to 1","bc71b88e":"<h3>Preprocessing","c4876977":"<h3>Feature engineering<\/h3>\n\nWe'll be using the Vader library here, so we do not have to engineer any features. We can do so, if we're building our model from scratch.","9be59936":"<h3>Sentiment definition<\/h3>\n\nLet us define our sentiments as follows (you can choose any criteria):\n\n* Score <= 2 : Negative\n* Score = 3 : Neutral\n* Score >=4 : Positive","9bae4ae1":"Some ways to improve the accuracy:\n\n* Use GridSearchCV for hyperparameter tuning. We can try out multiple combinations to see which one fits best for us\n\n* Using deep learning techniques like RNN and LSTMs\n\n* Reiterating the whole process using other vanilla ML models such SVM, Naive Bayes, MLP, GBM etc.","d3f795af":"<h1>Introduction<\/h1>\n\nThis notebook will be an introduction to implementing end-to-end solutions for a few NLP tasks. Tthe areas covered in this notebkook are:\n\n* Consumer complaint classification\n* Basic sentiment predictons using Vader\n\nThe others will cover more advanced topics like record linkage, text summarization etc.","05187ca5":"<h2>Multi-class classification - Consumer complaints<\/h2>\n\nWe have a database of thousands of consumer complaints about different financial products and services to the respective companies. We need to classify them into a product category, using the data available tous.","d7d07a7b":"<h3>Working with wordclouds<\/h3>\n\nWe will analyze the summary of each type of review.","61a37eca":"<h2>Sentiment analysis for Amazon food<\/h2>\n\nWe've seen sentiment analysis using vectorizers and TextBlob (in previous notebooks). Here we will work with the Vader library to automate our process:","4e159db6":"As we have a highly skewed dataset, we will pick up N-sample of values, for each class.","aef755fd":"Here we have it, we were able to get the total number of positive and negative sentiments for each product, using the Vader library. We can do the same process from scratch if we have labeled data, we'll see that in the next few notebooks."}}