{"cell_type":{"cfc45d6f":"code","aa13e9f1":"code","a52d7148":"code","ea54dc5b":"code","cc4ac871":"code","0c6e690f":"code","b22978fa":"code","c0867b03":"code","3006a49f":"code","8297c973":"code","f4b482b6":"code","04c18f55":"code","a89fa951":"code","47c7fa72":"code","6f4a3d51":"code","fea7c614":"code","a6b38d72":"code","3510433a":"code","593efa6c":"code","83053248":"code","127d880b":"code","7fad8a57":"code","0b9fff61":"code","1e573761":"code","fa8a94ff":"code","4163e8b4":"code","f725d0c5":"code","38550753":"code","6861e852":"code","f3d7aff0":"code","8b34b91b":"code","10db068d":"code","f7d7c34d":"code","d33d0a97":"code","408317b6":"code","f8bf5efd":"code","6a349826":"code","517c50b0":"code","f10a1a06":"markdown","886c0b3a":"markdown","d0452f1d":"markdown","e1075c64":"markdown","1b2e23af":"markdown","65cc3ca4":"markdown","b861c14e":"markdown","0800e850":"markdown","45e79933":"markdown","366f4de9":"markdown","354fc626":"markdown","214342df":"markdown","da74fe0e":"markdown","6fc4115d":"markdown","fb92f5d3":"markdown","f3f3e2b3":"markdown","6fa87173":"markdown","e80566ee":"markdown","94e24f01":"markdown","ee8a5438":"markdown","4eb17ea0":"markdown","a280fb42":"markdown","4fcb7693":"markdown","14a4f6e0":"markdown","02c84419":"markdown"},"source":{"cfc45d6f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","aa13e9f1":"data = pd.read_csv('..\/input\/otto-group-product-classification-challenge\/train.csv')","a52d7148":"data.describe()","ea54dc5b":"data.shape #(61878, 95)","cc4ac871":"import seaborn as sns\nsns.countplot(data.target)\nplt.show()","0c6e690f":"#Use random undersampling\n\ny = data['target']\nx = data.drop(['id','target'],axis=1)","b22978fa":"from imblearn.under_sampling import RandomUnderSampler#random undersampling api\n\nrus = RandomUnderSampler(random_state=0)\nX_resampled, y_resampled = rus.fit_resample(x,y)","c0867b03":"print(X_resampled.shape,y_resampled.shape) #17361 rows \u00d7 93 columns, (17361, 93) (17361,)","3006a49f":"sns.countplot(y_resampled)\nplt.show()","8297c973":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny_resampled = le.fit_transform(y_resampled)\ny_resampled","f4b482b6":"from sklearn.model_selection import train_test_split\nXtrain,Xcv,ytrain,ycv = train_test_split(X_resampled,y_resampled,test_size = 0.2)\n","04c18f55":"print(Xtrain.shape,Xcv.shape)#(13888, 93) (3473, 93)","a89fa951":"from sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier(oob_score=True)\nrfc.fit(Xtrain,ytrain)","47c7fa72":"rfc.score(Xcv,ycv)","6f4a3d51":"rfc.oob_score_","fea7c614":"sns.countplot(rfc.predict(Xcv))\nplt.show()","a6b38d72":"from sklearn.metrics import log_loss\n# log_loss(ycv,rfc.predict(Xcv),eps=1e-15,normalize=True)#eps is the input parameter during logloss model evaluation\uff0chere is 10^-15","3510433a":"ycv.reshape(-1,1)#reshape(-1,1) Convert to column vector","593efa6c":"from sklearn.preprocessing import OneHotEncoder\none_hot = OneHotEncoder(sparse=False)\nycv_onehot = one_hot.fit_transform(ycv.reshape(-1,1))\nypred_onehot = one_hot.fit_transform(rfc.predict(Xcv).reshape(-1,1))","83053248":"ycv_onehot#Check whether it is done","127d880b":"log_loss(ycv_onehot,ypred_onehot,eps=1e-15,normalize=True) #7.707328449771324","7fad8a57":"ypred_proba = rfc.predict_proba(Xcv)","0b9fff61":"ypred_proba #Similar to onehot output, no need to modify the shape","1e573761":"log_loss(ycv_onehot,ypred_proba,eps=1e-15,normalize=True) #0.7558184520641458 a little bit smaller 7.707->0.756","fa8a94ff":"## Determine which hyperparameters need to be adjusted\n# n_estimators, max_features, max_depth, min_samples_leaf","4163e8b4":"#Determine the value range of n_estimators\ntune_params = range(10,200,10)\n\n#Create a numpy array \"accuracy_t\" that adds accuracy\naccuracy_t = np.zeros(len(tune_params))\n\n#Create a numpy array \"error_t\" that adds log_loss\nerror_t = np.zeros(len(tune_params))\n\n#Tuning process realization\nfor i, param in enumerate(tune_params):\n    rfc2 = RandomForestClassifier(oob_score=True,\n                                 n_estimators = param,\n                                 max_depth=10,\n                                 max_features =10,\n                                 min_samples_leaf=10,\n                                 random_state =0,\n                                 n_jobs=-1)\n    rfc2.fit(Xtrain,ytrain)\n    #Output accuracy and log_loss\n    accuracy_t[i] = rfc2.oob_score_\n    \n    ypred = rfc2.predict_proba(Xcv)\n    error_t[i] = log_loss(ycv_onehot,ypred,eps=1e-15,normalize=True)\n    \n    print(error_t[i])\n    ","f725d0c5":"#Visualization of optimization results\nfig,axes = plt.subplots(nrows=1,ncols=2,figsize=(20,4))\naxes[0].plot(tune_params,error_t)\naxes[1].plot(tune_params,accuracy_t)\n\naxes[0].set_xlabel('n_estimators')\naxes[0].set_ylabel('error_t')\naxes[1].set_xlabel('n_estimators')\naxes[1].set_ylabel('accuracy_t')\n\naxes[0].grid(True)\naxes[1].grid(True)\n\nplt.show()","38550753":"#Determine the value range of max_features\ntune_params = range(5,40,5)\n\n#Create a numpy array \"accuracy_t\" that adds accuracy\naccuracy_t = np.zeros(len(tune_params))\n\n#Create a numpy array \"error_t\" that adds log_loss\nerror_t = np.zeros(len(tune_params))\n\n#Tuning process realization\nfor i, param in enumerate(tune_params):\n    rfc3 = RandomForestClassifier(oob_score=True,\n                                 n_estimators = 175,\n                                 max_depth=10,\n                                 max_features =param,\n                                 min_samples_leaf=10,\n                                 random_state =0,\n                                 n_jobs=-1)\n    rfc3.fit(Xtrain,ytrain)\n    #Output accuracy and log_loss\n    accuracy_t[i] = rfc3.oob_score_\n    \n    ypred = rfc3.predict_proba(Xcv)\n    error_t[i] = log_loss(ycv_onehot,ypred,eps=1e-15,normalize=True)\n    \n    print(error_t[i])\n\n#Visualization of optimization results\nfig,axes = plt.subplots(nrows=1,ncols=2,figsize=(20,4))\naxes[0].plot(tune_params,error_t)\naxes[1].plot(tune_params,accuracy_t)\n\naxes[0].set_xlabel('max_features')\naxes[0].set_ylabel('error_t')\naxes[1].set_xlabel('max_features')\naxes[1].set_ylabel('accuracy_t')\n\naxes[0].grid(True)\naxes[1].grid(True)\n\nplt.show()","6861e852":"#Determine the value range of max_depth\ntune_params = range(10,50,10)\n\n#Create a numpy array \"accuracy_t\" that adds accuracy\naccuracy_t = np.zeros(len(tune_params))\n\n#Create a numpy array \"error_t\" that adds log_loss\nerror_t = np.zeros(len(tune_params))\n\n#Tuning process realization\nfor i, param in enumerate(tune_params):\n    rfc4 = RandomForestClassifier(oob_score=True,\n                                 n_estimators = 175,\n                                 max_depth=param,\n                                 max_features =15,\n                                 min_samples_leaf=10,\n                                 random_state =0,\n                                 n_jobs=-1)\n    rfc4.fit(Xtrain,ytrain)\n    #Output accuracy and log_loss\n    accuracy_t[i] = rfc4.oob_score_\n    \n    ypred = rfc4.predict_proba(Xcv)\n    error_t[i] = log_loss(ycv_onehot,ypred,eps=1e-15,normalize=True)\n    \n    print(error_t[i])\n\n#Visualization of optimization results\nfig,axes = plt.subplots(nrows=1,ncols=2,figsize=(20,4))\naxes[0].plot(tune_params,error_t)\naxes[1].plot(tune_params,accuracy_t)\n\naxes[0].set_xlabel('max_depth')\naxes[0].set_ylabel('error_t')\naxes[1].set_xlabel('max_depth')\naxes[1].set_ylabel('accuracy_t')\n\naxes[0].grid(True)\naxes[1].grid(True)\n\nplt.show()","f3d7aff0":"#Determine the value range of min_samples_leaf\ntune_params = range(1,10,2)\n\n#Create a numpy array \"accuracy_t\" that adds accuracy\naccuracy_t = np.zeros(len(tune_params))\n\n#Create a numpy array \"error_t\" that adds log_loss\nerror_t = np.zeros(len(tune_params))\n\n#Tuning process realization\nfor i, param in enumerate(tune_params):\n    rfc5 = RandomForestClassifier(oob_score=True,\n                                 n_estimators = 175,\n                                 max_depth=30,\n                                 max_features =15,\n                                 min_samples_leaf=param,\n                                 random_state =0,\n                                 n_jobs=-1)\n    rfc5.fit(Xtrain,ytrain)\n    #Output accuracy and log_loss\n    accuracy_t[i] = rfc5.oob_score_\n    \n    ypred = rfc5.predict_proba(Xcv)\n    error_t[i] = log_loss(ycv_onehot,ypred,eps=1e-15,normalize=True)\n    \n    print(error_t[i])\n\n#Visualization of optimization results\nfig,axes = plt.subplots(nrows=1,ncols=2,figsize=(20,4))\naxes[0].plot(tune_params,error_t)\naxes[1].plot(tune_params,accuracy_t)\n\naxes[0].set_xlabel('min_samples_leaf')\naxes[0].set_ylabel('error_t')\naxes[1].set_xlabel('min_samples_leaf')\naxes[1].set_ylabel('accuracy_t')\n\naxes[0].grid(True)\naxes[1].grid(True)\n\nplt.show()","8b34b91b":"test_data = pd.read_csv('..\/input\/otto-group-product-classification-challenge\/test.csv')\nXtest = test_data.drop(['id'],axis=1)\nXtest.head","10db068d":"rfc_best = RandomForestClassifier(oob_score=True,\n                                 n_estimators = 175,\n                                 max_depth=30,\n                                 max_features =15,\n                                 min_samples_leaf=1,\n                                 random_state =0,\n                                 n_jobs=-1)\nrfc_best.fit(X_resampled,y_resampled)\n","f7d7c34d":"rfc_best.oob_score_    #0.763536866359447  ---->  0.7752433615575139","d33d0a97":"y_test_proba = rfc_best.predict_proba(Xtest)\ny_test_pred = rfc_best.predict(Xtest)","408317b6":"y_test_proba","f8bf5efd":"final = pd.DataFrame(y_test_proba,columns = ['Class_'+str(i) for i in range(1,10)])\nfinal","6a349826":"final.insert(loc=0,column='id',value = test_data.id)\nfinal","517c50b0":"final.to_csv('.\/final.csv',index=False)","f10a1a06":"**The current log_loss is 7.707, but the ypred here is directly obtained through rfc.predict, and the values are 0 and 1. We can use rfc.predict_proba to get the possibility of each category to reduce log_loss**","886c0b3a":"**Using Logloss to evaluate the model**","d0452f1d":"# Train an optimal rfc_best which min_samples_leaf=1, max_depth=30, max_features=15 and n_setimators=175.","e1075c64":"# Analysis:  According to the result, the max_depth is around 30, which is more appropriate. If it is too high, it will overfit.","1b2e23af":"# **Step6 Model tuning**\n# ","65cc3ca4":"add an id column","b861c14e":"# Analysis: According to the visualization results, determine n_setimators=175","0800e850":"min_samples_leaf := 1","45e79933":"# **Random Forest**","366f4de9":"# Author: Yicheng Jin","354fc626":"**Observe the data label distribution after undersampling**","214342df":"# **Step4 Split data to training set and cv set**","da74fe0e":"**An error will be reported here, because log_loss must require the output to be represented by one-hot\nThus we need to use OneHotEncoder to modify the output of multi-category problems**","6fc4115d":"\n# 6.3 max_depth","fb92f5d3":"**After statistically mapping the label column, it is found that the distribution of each label is uneven. This is because if all the data is used, the model will be biased, so the sampling method should be adopted.**","f3f3e2b3":"# **6.1. n_estimators**","6fa87173":"# **Step2 Data Visualization**","e80566ee":"# Analysis: error gradually decreases to stable as max_features increases, but accuracy first increases and then decreases, so I choose max_features as 15, at this time accuracy does not drop much.","94e24f01":"# 6.2 max_features","ee8a5438":"# **Step3 Processing of Data**","4eb17ea0":"# **Step1 Obtain the data**","a280fb42":"# **Step5 Model Training**","4fcb7693":"# 6.4 min_samples_leaf","14a4f6e0":"**After using undersampling, the data label distribution is very even, and the label value is converted into a numerical value, then using labelencoder**","02c84419":"# **Step7 Generate submission**"}}