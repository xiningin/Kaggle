{"cell_type":{"6305798e":"code","a6fe214c":"code","70f520bc":"code","8dc281ff":"code","a8c76c79":"code","c3ace57b":"code","f725e15e":"code","2a7ab601":"code","c445b2cf":"markdown","676ec3ef":"markdown","381da731":"markdown","26946485":"markdown","a1386923":"markdown","77eecaf3":"markdown","03c2bba6":"markdown"},"source":{"6305798e":"from sklearn.decomposition import PCA\nfrom sklearn import datasets\n\ndigits = datasets.load_digits()\nX = digits.data\ny = digits.target","a6fe214c":"print(digits.data.shape)","70f520bc":"import matplotlib.pyplot as plt \nplt.gray()\nplt.matshow(digits.images[25]) \nplt.show()\nprint(digits.target[25])","8dc281ff":"pca = PCA(n_components=0.95, whiten=True)\n\nX_pca = pca.fit_transform(X)\n\nprint('N\u00famero original de atributos:', X.shape[1])\nprint('N\u00famero reduzido de atributos:', X_pca.shape[1])","a8c76c79":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nmodel = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=2000)\n\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nprint('Acur\u00e1cia nos dados originais:', accuracy_score(y_test, y_pred))\n\n#######\n\nX_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.33, random_state=42)\n\nmodel = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=2000)\n\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nprint('Acur\u00e1cia nos dados reduzidos (PCA em tudo):', accuracy_score(y_test, y_pred))\n\n#######\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\npca = PCA(n_components=0.95, whiten=True)\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)\n\nmodel = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=2000)\n\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nprint('Acur\u00e1cia nos dados originais (PCA da parte certa):', accuracy_score(y_test, y_pred))","c3ace57b":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nfvalue_selector = SelectKBest(f_classif, k=20)\n\nX_kbest = fvalue_selector.fit_transform(X_train, y_train)\n\nprint('N\u00famero original de atributos:', X.shape[1])\nprint('N\u00famero reduzido de atributos:', X_kbest.shape[1])\n\n###\n\nselected_features = []\nmap_vector = []\nmask = fvalue_selector.get_support()\nfor m, feature in zip(mask, list(range(64))):\n    if m:\n        selected_features.append(feature)\n        map_vector.append(1)\n    else:\n        map_vector.append(0)\n\nprint(selected_features)\n\nmap_vector = np.asarray(map_vector)\n\nplt.matshow(map_vector.reshape(8,8)) \nplt.show()\n\n###\n\nmodel = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=2000)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nprint('Acur\u00e1cia nos dados originais:', accuracy_score(y_test, y_pred))\n\nmodel = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=2000)\nmodel.fit(X_kbest, y_train)\nX_test_kbest = fvalue_selector.transform(X_test)\ny_pred = model.predict(X_test_kbest)\nprint('Acur\u00e1cia nos dados Kbest:', accuracy_score(y_test, y_pred))","f725e15e":"pca = PCA(n_components=40, whiten=True, svd_solver='randomized')\n\nX_pca = pca.fit_transform(X)\n\nprint('N\u00famero original de atributos:', X.shape[1])\nprint('N\u00famero reduzido de atributos:', X_pca.shape[1])\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nmodel = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=2000)\n\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nprint('Acur\u00e1cia nos dados originais:', accuracy_score(y_test, y_pred))\n\n#######\n\nX_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.33, random_state=42)\n\nmodel = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=2000)\n\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nprint('Acur\u00e1cia nos dados reduzidos (PCA em tudo):', accuracy_score(y_test, y_pred))\n\n#######\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\npca = PCA(n_components=0.80, whiten=True)\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)\n\nmodel = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=2000)\n\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nprint('Acur\u00e1cia nos dados originais (PCA da parte certa):', accuracy_score(y_test, y_pred))","2a7ab601":"import seaborn as sns; sns.set()\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nplot_x = [1,2,3,4,5] # valores que v\u00e3o no eixo X\nplot_y = [80,85,90,83,70] # valores que v\u00e3o no eixo Y\n    \nax = sns.lineplot(x=np.array(plot_x), y=np.array(plot_y))\n","c445b2cf":"(2) Explore o n\u00famero de atributos na fun\u00e7\u00e3o KBest, variando sistematicamente, para provar que h\u00e1 ou n\u00e3o h\u00e1 um resultado melhor.","676ec3ef":"Os resultados devem ser comparados entre a acur\u00e1cia nos dados originais e a acur\u00e1cia com a redu\u00e7\u00e3o de dimensionalidade ou com a sele\u00e7\u00e3o dos atributos. Al\u00e9m de olhar para os resultados tamb\u00e9m \u00e9 importante notar a redu\u00e7\u00e3o de dimensionalidade, j\u00e1 que isso pode levar a processos mais r\u00e1pidos.","381da731":"---\n\n**Template para plotar gr\u00e1fico comparando os resultados, caso queira:**","26946485":"## Exerc\u00edcios\n\n(1) Explore as configura\u00e7\u00f5es do PCA, utilizando n\u00fameros fixos de componentes e outros valores cont\u00ednuos para a vari\u00e2ncia, e verifique se \u00e9 poss\u00edvel obter melhores resultados utilizando a regress\u00e3o log\u00edstica e a separa\u00e7\u00e3o de dados da forma como est\u00e1.","a1386923":"A fun\u00e7\u00e3o PCA importada de dentro do scikit-learn depende basicamente do atributo n_components. Esse atributo pode ser especificado como um inteiro, onde descrever\u00e1 o n\u00famero de componentes desejado, ou um valor cont\u00ednuo entre 0 e 1, indicando o percentual de vari\u00e2ncia que deseja ser mantido no resultado final. O par\u00e2metro whiten realiza uma opera\u00e7\u00e3o matem\u00e1tica no sinal dos vetores extra\u00eddos no processo do PCA, e pode melhorar o resultado.","77eecaf3":"## Redu\u00e7\u00e3o de Dimensionalidade e Sele\u00e7\u00e3o de atributos\n\nFoi visto anteriormente sobre a redu\u00e7\u00e3o de dimensionalidade, onde algoritmos n\u00e3o-supervisionados (como o PCA) s\u00e3o capazes de condensar informa\u00e7\u00f5es em um n\u00famero menor de dimens\u00e3o. No entanto, uma confus\u00e3o que normalmente acontece com as terminologias, \u00e9 a distin\u00e7\u00e3o entre selecionar atributos e reduzir dimensionalidade. Obviamente que a sele\u00e7\u00e3o de atributos acarretar\u00e1 em um n\u00famero menor de dimens\u00f5es, mas diferente da redu\u00e7\u00e3o de dimensionalidade, a sele\u00e7\u00e3o de atributos descarta atributos que n\u00e3o s\u00e3o julgados como relevante para o problema, enquanto a redu\u00e7\u00e3o condensa a informa\u00e7\u00e3o em um espa\u00e7o com menos dimens\u00f5es.\n\n---\n\nSer\u00e1 utilizado o dataset digits conhecido no meio de aprendizado de m\u00e1quina, importado diretamente do Scikit-Learn. Em seguida, ser\u00e3o aplicados PCA e sele\u00e7\u00e3o de atributos para analisar a diferen\u00e7a entre eles.","03c2bba6":"### Sele\u00e7\u00e3o de atributos\n\nA sele\u00e7\u00e3o de atributos n\u00e3o \u00e9 um processo n\u00e3o-supervisionado, igual a redu\u00e7\u00e3o de dimensionalidade. \u00c9 feita uma an\u00e1lise diretamente relacionada com o r\u00f3tulo da amostra, portanto \u00e9 supervisionado. Uma das estrat\u00e9gias mais comuns de sele\u00e7\u00e3o de atributos \u00e9 teste de valor ANOVA. Esse teste identifica se existe algum valor significamente diferente da rela\u00e7\u00e3o entre atributo e r\u00f3tulo. Os atributos que s\u00e3o muito diferentes nessa rela\u00e7\u00e3o, podem ser descartados.\n\nLembrando sempre de n\u00e3o dar informa\u00e7\u00f5es do teste para a sele\u00e7\u00e3o de atributos, para n\u00e3o virar um tipo de trapa\u00e7a."}}