{"cell_type":{"93a96fff":"code","6ef09188":"code","035e8197":"code","d878358b":"code","ef3c6770":"code","ba00821d":"code","334e2780":"code","4f3db516":"code","a9a71e41":"code","4d013a5e":"code","8902d92d":"code","560bb77f":"code","b939ead1":"code","67b6847a":"code","424732e6":"code","fac358ef":"code","e174bb19":"code","d1e4c65b":"code","cb96c27b":"code","c9aafb55":"code","95afa230":"code","e273f85b":"code","70e982dd":"code","c00b5f01":"code","88a4f80c":"code","b52f6e77":"code","e4d82f0f":"code","96e2253f":"code","7fe586cb":"code","97815146":"code","82b3c815":"code","4faa3cc4":"code","b2cd1415":"code","e2a06bd4":"code","0686baf2":"code","0c7e22e5":"code","ce83c4c1":"code","e7353f46":"code","8e7d4b95":"code","54805e38":"code","fb52aca1":"code","31b7e88d":"code","84582ba2":"markdown","f1c275fd":"markdown","d9f0dea7":"markdown","6fc70af8":"markdown","c4c9b2a0":"markdown","f14238b3":"markdown","04a68b71":"markdown"},"source":{"93a96fff":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6ef09188":"train_df = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\", usecols=[\"id\", \"text\", \"target\"])\ntest_df = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\", usecols=[\"id\", \"text\"])\nsample_df = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")","035e8197":"train_df.head()","d878358b":"train_df.isnull().sum()","ef3c6770":"sns.countplot(train_df.target)","ba00821d":"train_df['target'].value_counts()","334e2780":"print(f'{train_df.target[train_df.target==1].count()\/train_df.target.count()*100:.2f} % of tweets are labeled as disaster tweets in data')","4f3db516":"import re\nimport string\n\ndef tweets_processing(text):\n    # remove links and urls from text\n    text = re.sub(r'(https|http)?:\\\/\\\/(\\w|\\.|\\\/|\\?|\\=|\\&|\\%)*\\b', \"\", text, flags=re.MULTILINE)\n    \n    # convert text into lowercase\n    text = text.lower()\n\n    # remove new line characters in text\n    text = re.sub(r'\\n',' ', text)\n    \n    # remove punctuations from text\n    text = re.sub('[%s]' % re.escape(string.punctuation), \"\", text)\n    \n    # remove references and hashtags from text\n    text = re.sub(\"(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)|(\\w+:\\\/\\\/\\S+)\", \"\", text)\n    \n    # remove multiple spaces from text\n    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n    \n    # remove special characters from text\n    text = re.sub(r'\\W', ' ', text)\n    \n    return text","a9a71e41":"# make a copy of tweets\ntweets = train_df.copy()\n\n# apply processing techniques\ntweets.text = tweets.text.apply(lambda text: tweets_processing(text))","4d013a5e":"train_df.head(10)","8902d92d":"tweets.head()","560bb77f":"from transformers import AutoTokenizer, TFBertModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased\")\nbert_model = TFBertModel.from_pretrained(\"bert-large-uncased\")","b939ead1":"tokenizer(\"My Name is Mohamed\")","67b6847a":"tweets_list = [len(x.split()) for x in tweets.text]","424732e6":"import collections\n\ncounter=collections.Counter(tweets_list)\ncounter_keys = counter.keys()\ncounter_values = counter.values()\ncounter = dict(counter)\n# pd.DataFrame(counter, index=[0]).T.reset_index().rename(columns={'index': 'tweet_length', 0: 'freq'})\ntweets['length'] = tweets_list\n","fac358ef":"tweets.head(10)","e174bb19":"for i in range(1, 32):\n    sns.countplot(tweets[tweets['length'] == i].target)\n    plt.show()\n    print('Length of tweet: ', i)\n    print('============================================================================')","d1e4c65b":"tweets = tweets[tweets['length'] > 4]\ntweets = tweets[tweets['length'] < 28]","cb96c27b":"tweets.head()","c9aafb55":"print(\"Max length of tweets:\", max([len(x.split()) for x in tweets.text]))\nprint(\"Min length of tweets:\", min([len(x.split()) for x in tweets.text]))","95afa230":"tweets['target'].value_counts()","e273f85b":"X_train = tokenizer(\n    text=tweets.text.tolist(),\n    add_special_tokens=True,\n    max_length=27,\n    truncation=True,\n    padding=True,\n    return_tensors='tf',\n    return_token_type_ids=False,\n    return_attention_mask=True,\n    verbose=True\n)","70e982dd":"X_train['input_ids'].shape","c00b5f01":"y_train = tweets.target.values\ny_train.shape","88a4f80c":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.initializers import TruncatedNormal\nfrom tensorflow.keras.losses import CategoricalCrossentropy, BinaryCrossentropy\nfrom tensorflow.keras.metrics import CategoricalAccuracy, BinaryAccuracy\nfrom tensorflow.keras.utils import to_categorical, plot_model\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow as tf","b52f6e77":"MAX_LENGTH = 27\nINPUT_IDs = Input(shape=(MAX_LENGTH,), dtype=tf.int32, name=\"input_ids\")\nINPUT_MASK = Input(shape=(MAX_LENGTH,), dtype=tf.int32, name=\"attention_mask\")","e4d82f0f":"embeddings = bert_model([INPUT_IDs, INPUT_MASK])[1] # 0 is hidden state, 1 is pooler_output\n# shape of hidden state is (None, MAX_LENGTH, 1024) ==> out = tf.keras.layers.GlobalMaxPool1D(0.1)(embeddings)\n# shape of booled output is (None, 1024) ==> out = tf.keras.layers.Dropout(0.1)(embeddings)\n\nout = tf.keras.layers.Dropout(0.1)(embeddings)\nout = Dense(128, activation=\"relu\")(out)\nout = tf.keras.layers.Dropout(0.1)(out)\nout = Dense(32, activation=\"relu\")(out)\n\ny = Dense(1, activation='sigmoid')(out)\nmodel = tf.keras.Model(inputs=[INPUT_IDs, INPUT_MASK], outputs=y)\nmodel.layers[2].trainable = True\n\n\noptimizer = Adam(learning_rate=5e-06,\n                 epsilon=1e-08,\n                 decay=0.01,\n                 clipnorm=1.0)\n\nloss = BinaryCrossentropy(from_logits=True)\nmetric = BinaryAccuracy('accuracy')\n\nmodel.compile(\n    optimizer=optimizer,\n    loss=loss,\n    metrics=metric)","96e2253f":"model.summary()","7fe586cb":"# model.save('model.h5')","97815146":"plot_model(model, show_shapes=True)","82b3c815":"# define the checkpoint\nfilepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\ncallbacks_list = [checkpoint]\n\n# fit model\ntrain_history = model.fit(\n    x={'input_ids':X_train[\"input_ids\"], 'attention_mask':X_train[\"attention_mask\"]},\n    y=y_train,\n    validation_split=0.2,\n    epochs=10,\n    batch_size=24\n)","4faa3cc4":"# train_history.model.save('model.h5')","b2cd1415":"# import pickle\n\n# filename = 'model.h5'\n# pickle.dump(model, open(filename, 'wb'))","e2a06bd4":"test_df.head()","0686baf2":"# make a copy of tweets\ntest_tweets = test_df.copy()\n\n# apply processing techniques\ntest_tweets.text = test_tweets.text.apply(lambda text: tweets_processing(text))","0c7e22e5":"test_tweets.head()","ce83c4c1":"X_test = tokenizer(\n    text=test_tweets.text.tolist(),\n    add_special_tokens=True,\n    max_length=27,\n    truncation=True,\n    padding=True,\n    return_tensors='tf',\n    return_token_type_ids=False,\n    return_attention_mask=True,\n    verbose=True\n)","e7353f46":"prediction = model.predict(x={'input_ids':X_test[\"input_ids\"], 'attention_mask':X_test[\"attention_mask\"]})","8e7d4b95":"y_pred = np.where(prediction>0.5, 1, 0)","54805e38":"sample_df['id'] = test_tweets.id\nsample_df['text'] = test_tweets.text\nsample_df['target'] = y_pred","fb52aca1":"sample_df.head()","31b7e88d":"sample_df.to_csv('submission.csv', index=False)","84582ba2":"> No Null values in data","f1c275fd":"### Data Analysis","d9f0dea7":"### Test Model","6fc70af8":"### Build Model Architecture","c4c9b2a0":"> Data is almost balance","f14238b3":"### Text Preprocessing","04a68b71":"### Load the Model"}}