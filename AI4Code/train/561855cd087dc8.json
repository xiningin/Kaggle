{"cell_type":{"5144ee02":"code","b83c7c77":"code","1532e3f8":"code","4ea04de7":"code","f456a7c0":"code","444c7a11":"code","06e83ba2":"code","e1035ae4":"code","e58f4e09":"code","13724c8e":"code","7df33107":"code","7a9381b5":"code","2ac94d48":"code","22df5e8e":"code","101fb861":"code","cbafb272":"code","1d0fb4f9":"code","2643798b":"code","d0216563":"code","955c4600":"code","b3c08b1d":"code","10623e56":"code","2a3d6570":"code","e5fc5537":"code","3dff5d46":"code","eb04d216":"code","90b0ed95":"code","24062217":"code","2ee52726":"code","961e6926":"code","3f590b15":"code","80151314":"code","1119b2b4":"code","0fe59f44":"code","7ae8fc5e":"code","6d073295":"code","1cd36677":"code","d2f059ca":"code","69234319":"code","eec73b34":"code","69ce4e0d":"code","e6395899":"code","abf00299":"code","526eb11f":"code","e11298d7":"code","a840a144":"code","382bea7a":"code","92bf588e":"code","0bc49204":"code","dc52e107":"code","887cde3b":"markdown","3d9d223d":"markdown","303ee6f7":"markdown","71578531":"markdown","90a7ba20":"markdown","f4c78952":"markdown","ac6f566e":"markdown","8edfe2af":"markdown","8bfca832":"markdown","72625920":"markdown","ec23c381":"markdown","a7406287":"markdown","a50f0218":"markdown","5f1d2f8f":"markdown"},"source":{"5144ee02":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b83c7c77":"# Loading Essential libraries \nimport warnings\nwarnings.filterwarnings('ignore')\n%config InlineBackend.figure_format = 'retina'\n%config Completer.use_jedi = False # this to force autocompletion \nfrom matplotlib import pyplot as plt\nimport seaborn as sns","1532e3f8":"def read_data():\n    \"\"\"Read the data.\"\"\"\n    news1_df = pd.read_csv('..\/input\/news-summary\/news_summary.csv', encoding='latin-1', usecols=['headlines', 'text'])\n    news2_df = pd.read_csv('..\/input\/news-summary\/news_summary_more.csv', encoding='latin-1')\n    \n    return pd.concat([news1_df, news2_df], axis=0).reset_index(drop=True)","4ea04de7":"df = read_data()","f456a7c0":"df.head()","444c7a11":"df.tail()","06e83ba2":"# lowercasing all the words\ndf['headlines'] = df.headlines.apply(lambda x: x.lower())\ndf['text'] = df.text.apply(lambda x: x.lower())","e1035ae4":"!pip install text_hammer\nimport  text_hammer as th","e58f4e09":"# Remove quotes \ndef Text_cleaning(df, column):\n    column = column\n    print(column)\n    import re\n    \"\"\"column must be a string value\"\"\"\n    df[column] = df[column].progress_apply(lambda x: re.sub('\"',\"'\", x))\n    df[column] = df[column].progress_apply(lambda x:th.remove_special_chars(x))\n    df[column] = df[column].progress_apply(lambda x:th.remove_html_tags(x))\n    df[column] = df[column].progress_apply(lambda x: th.remove_urls(x))\n    df[column] = df[column].progress_apply(lambda x:th.cont_exp(x))\n    df[column] = df[column].progress_apply(lambda x: re.sub('[^a-zA-Z]+',' ', x))\n    df[column] = df[column].progress_apply(lambda x:' '.join([x for x in x.split() if len(x)>=2]) )\n    return df[column]\n\n","13724c8e":"cleaned_headlines= Text_cleaning(df, 'headlines')","7df33107":"cleaned_text = Text_cleaning(df, 'text')","7a9381b5":"cleaned_df = pd.DataFrame({'headlines':cleaned_headlines, 'text':cleaned_text})\ncleaned_df.head()","2ac94d48":"cleaned_df.to_csv('newgroup_cleaned.csv', index = False)","22df5e8e":"cleaned_df = pd.read_csv('.\/newgroup_cleaned.csv')","101fb861":"plt.figure(figsize=(12, 6))\nplt.style.use('ggplot')\nplt.subplot(1, 2, 1)\nsns.distplot(df['headlines'].str.split().apply(len))\nplt.title('Distribution of headlines sentences length')\nplt.xlabel('Length')\n\nplt.style.use('ggplot')\nplt.subplot(1, 2, 2)\nsns.distplot(df['text'].str.split().apply(len))\nplt.xlim(0,60)\nplt.title('Distribution of text sentences length')\nplt.xlabel('Length')\nplt.show()","cbafb272":"cleaned_df['headlines'] = cleaned_df.headlines.apply(lambda x: 'sostok '+str(x) + ' eostok')","1d0fb4f9":"cleaned_df.head()","2643798b":"cleaned_df['len_headlines'] = cleaned_df.headlines.apply(lambda x: len(x.split()))\ncleaned_df['len_text'] = cleaned_df.text.apply(lambda x:len(x.split()))","d0216563":"# calculation the word count of text and getting percentile vlaues\nfor i in range(80,100,5):\n    var = cleaned_df['len_headlines'].values\n    var = np.sort(var, axis = None)\n    print('{} percentile value is {}'.format(i, var[int(len(var)*(float(i)\/100))]))\n    print((\"100 percentile value is \", var[-1]))","955c4600":"max_len_text  = 60\n\nmax_len_headlines = 15\n","b3c08b1d":"cleaned_df = cleaned_df[(cleaned_df.len_text <=60)*(cleaned_df.len_headlines <= 15)]","10623e56":"cleaned_df.shape","2a3d6570":"cleaned_df.len_text.max()","e5fc5537":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(\ncleaned_df.text, cleaned_df.headlines, test_size = 0.2, random_state = 32, shuffle = True)","3dff5d46":"print(X_train.shape, y_test.shape)","eb04d216":"\nimport keras \nnum_words_x= 35000\nnum_words_y = 25000\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nx_tokenizer = Tokenizer(filters='|', num_words = num_words_x)\ny_tokenizer = Tokenizer(filters='|', num_words = num_words_y)\n\nx_tokenizer.fit_on_texts(cleaned_df.text)\ny_tokenizer.fit_on_texts(cleaned_df.headlines)\n\n","90b0ed95":"x_voc = len(x_tokenizer.word_index) + 1\ny_voc = len(y_tokenizer.word_index) + 1\n# this shows us the number of unique words in our text data ","24062217":"y_tr_seq    =   y_tokenizer.texts_to_sequences(y_train) \ny_val_seq   =   y_tokenizer.texts_to_sequences(y_test) \n\n#padding zero upto maximum length\ny_tr    =   pad_sequences(y_tr_seq, maxlen=max_len_headlines, padding='post')\ny_val   =   pad_sequences(y_val_seq, maxlen=max_len_headlines, padding='post')\n\n#convert text sequences into integer sequences (i.e one-hot encodeing all the words)\nx_tr_seq    =   x_tokenizer.texts_to_sequences(X_train) \nx_val_seq   =   x_tokenizer.texts_to_sequences(X_test)\n\n#padding zero upto maximum length\nx_tr    =   pad_sequences(x_tr_seq,  maxlen=max_len_text, padding='post')\nx_val   =   pad_sequences(x_val_seq, maxlen=max_len_text, padding='post')","2ee52726":"print(x_tr.shape, y_tr.shape)","961e6926":"## using 300 dimension word2vec model, since more dimension means more information\nglove_vectors = {}\nfile = open('..\/input\/glove6b\/glove.6B.300d.txt', encoding = 'utf-8')\nfor line in file:\n    values = line.split()\n    word = values[0]\n    vectors = np.asarray(values[1:])\n    glove_vectors[word] = vectors\nfile.close()   ","3f590b15":"glove_emb_matrix = np.zeros((num_words_x,300))\nglove_emb_matrix.shape","80151314":"%%time\nfor word, index in x_tokenizer.word_index.items():\n    if index < num_words_x: # since index starts with zero \n        embedding_vector = glove_vectors.get(word)\n        if embedding_vector is not None:\n            glove_emb_matrix[index] = embedding_vector\n    ","1119b2b4":"from tensorflow.keras import models\nimport numpy as np\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import LSTM, Bidirectional,Concatenate\nfrom tensorflow.keras.layers import Dense, Flatten,Embedding\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.layers import TimeDistributed\nfrom tensorflow.keras.layers import RepeatVector\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.utils import plot_model\nfrom keras.layers import Input,Dense, Activation, concatenate, Embedding, Flatten, Bidirectional, Concatenate\n \nfrom tensorflow.compat.v1.keras.layers import CuDNNLSTM\nfrom keras import optimizers\n","0fe59f44":"# now we will have to make attention classs\nimport tensorflow as tf\nimport os\nfrom tensorflow.python.keras.layers import Layer\nfrom tensorflow.python.keras import backend as K\n\n\nclass AttentionLayer(Layer):\n    \"\"\"\n    This class implements Bahdanau attention (https:\/\/arxiv.org\/pdf\/1409.0473.pdf).\n    There are three sets of weights introduced W_a, U_a, and V_a\n     \"\"\"\n\n    def __init__(self, **kwargs):\n        super(AttentionLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert isinstance(input_shape, list)\n        # Create a trainable weight variable for this layer.\n\n        self.W_a = self.add_weight(name='W_a',\n                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n                                   initializer='uniform',\n                                   trainable=True)\n        self.U_a = self.add_weight(name='U_a',\n                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n                                   initializer='uniform',\n                                   trainable=True)\n        self.V_a = self.add_weight(name='V_a',\n                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n                                   initializer='uniform',\n                                   trainable=True)\n\n        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n\n    def call(self, inputs, verbose=False):\n        \"\"\"\n        inputs: [encoder_output_sequence, decoder_output_sequence]\n        \"\"\"\n        assert type(inputs) == list\n        encoder_out_seq, decoder_out_seq = inputs\n        if verbose:\n            print('encoder_out_seq>', encoder_out_seq.shape)\n            print('decoder_out_seq>', decoder_out_seq.shape)\n\n        def energy_step(inputs, states):\n            \"\"\" Step function for computing energy for a single decoder state\n            inputs: (batchsize * 1 * de_in_dim)\n            states: (batchsize * 1 * de_latent_dim)\n            \"\"\"\n\n            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n\n            \"\"\" Some parameters required for shaping tensors\"\"\"\n            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n            de_hidden = inputs.shape[-1]\n\n            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n            # <= batch size * en_seq_len * latent_dim\n            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)\n\n            \"\"\" Computing hj.Ua \"\"\"\n            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n            if verbose:\n                print('Ua.h>', U_a_dot_h.shape)\n\n            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n            # <= batch_size*en_seq_len, latent_dim\n            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n            if verbose:\n                print('Ws+Uh>', Ws_plus_Uh.shape)\n\n            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n            # <= batch_size, en_seq_len\n            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n            # <= batch_size, en_seq_len\n            e_i = K.softmax(e_i)\n\n            if verbose:\n                print('ei>', e_i.shape)\n\n            return e_i, [e_i]\n\n        \n        def context_step(inputs, states):\n            \"\"\" Step function for computing ci using ei \"\"\"\n\n            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n\n            # <= batch_size, hidden_size\n            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n            if verbose:\n                print('ci>', c_i.shape)\n            return c_i, [c_i]\n\n        fake_state_c = K.sum(encoder_out_seq, axis=1)\n        fake_state_e = K.sum(encoder_out_seq, axis=2)  # <= (batch_size, enc_seq_len, latent_dim\n\n        \"\"\" Computing energy outputs \"\"\"\n        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n        last_out, e_outputs, _ = K.rnn(\n            energy_step, decoder_out_seq, [fake_state_e],\n        )\n\n        \"\"\" Computing context vectors \"\"\"\n        last_out, c_outputs, _ = K.rnn(\n            context_step, e_outputs, [fake_state_c],\n        )\n\n        return c_outputs, e_outputs\n\n    def compute_output_shape(self, input_shape):\n        \"\"\" Outputs produced by the layer \"\"\"\n        return [\n            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n        ]","7ae8fc5e":"y_tr.shape","6d073295":"from keras import backend as K \nK.clear_session() \nlatent_dim = 500 \n\n# Encoder \nencoder_inputs = Input(shape=(max_len_text,)) \nenc_emb = Embedding(num_words_x, 300,input_length = x_tr.shape[1],\n                    trainable = False, weights = [glove_emb_matrix])(encoder_inputs) \n\nencoder_lstm_layer = Bidirectional(LSTM(latent_dim, return_state=True, return_sequences = True, \n                                             name = 'lstm_encoder'), merge_mode = 'concat')\n\nencoder_output, forward_h, forward_c, backward_h, backward_c = encoder_lstm_layer(enc_emb)\n\nstate_h = Concatenate()([forward_h, backward_h])\nstate_c = Concatenate()([forward_c, backward_c])\nencoder_states = [state_h, state_c]\n\n# Set up the decoder. \ndecoder_inputs = Input(shape=(None,)) \ndec_emb_layer = Embedding(num_words_y,latent_dim, input_length=y_tr.shape[1],trainable  = True) \ndec_emb = dec_emb_layer(decoder_inputs) \n\n#LSTM using encoder_states as initial state\ndecoder_lstm_layer = LSTM(2*latent_dim, return_state=True, return_sequences = True, name = 'lstm_decoder')\ndecoder_output , decoder_h, decoder_c = decoder_lstm_layer(dec_emb,\n                                                   initial_state = encoder_states)\n\nattn_layer = AttentionLayer(name = 'attention_layer')\n\n######### -------------Attention layer---------------------------\nattn_out, attn_states = attn_layer([encoder_output, decoder_output])\ndecoder_concat_input = Concatenate(axis=-1, name='concat')([decoder_output, attn_out])\n\n\n\n#Dense layer\ndecoder_dense = TimeDistributed(Dense(num_words_y, activation='softmax')) \ndecoder_outputs = decoder_dense(decoder_concat_input) \n\n\nmodel = Model([encoder_inputs,decoder_inputs], decoder_outputs) \nmodel.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')                                                          \n# using sparse_categorical entropy will solve memory problem                                                          \nmodel.summary()","1cd36677":"plot_model(model, to_file='model.png', show_shapes=True)","d2f059ca":"## defining callbacks \nes = EarlyStopping(monitor='val_loss',patience = 10, mode='min', verbose=1)\nlr = keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', patience = 1, mode = 'min', verbose = 1)\n","69234319":"print(x_tr.shape, y_tr.shape)","eec73b34":"history=model.fit([x_tr,y_tr[:,:-1]], y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)[:,1:] ,epochs=10,\n                  callbacks=[es,lr],\n                  batch_size=128, validation_data=([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]))","69ce4e0d":"plt.plot(history.history['val_loss'], color = 'r')\nplt.plot(history.history['loss'], color = 'b')\nplt.legend(['val_loss','loss'])\nplt.xlim(0,10)\nplt.xlabel('epochs')\nplt.title('learning curve')\nplt.show()","e6395899":"# Define inference model\nencoder_model = Model(encoder_inputs, [encoder_output,state_h, state_c])\nplot_model(encoder_model,show_shapes=True)\n\n","abf00299":"\n# now lets design our decoder model \ndecoder_state_input_h = Input(shape=(2*latent_dim,))  # These states are required for feeding back to our next timestep decoder\ndecoder_state_input_c = Input(shape=(2*latent_dim,))\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\ndecoder_hidden_state_input = Input(shape=(max_len_text,2*latent_dim)) # since we are using bidirectional lstm\n\n\n# Get the embeddings of the decoder sequence\ndec_emb= dec_emb_layer(decoder_inputs) \n# To predict the next word in the sequence, set the initial states to the states from the previous time step\ndecoder_output, state_h2, state_c2 = decoder_lstm_layer(dec_emb, initial_state=decoder_states_inputs)\n\nattn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_output])\ndecoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_output, attn_out_inf])\n\n\n# A dense softmax layer to generate prob dist. over the target vocabulary\ndecoder_outputs = decoder_dense(decoder_inf_concat)\n\n# Final decoder model\ndecoder_model = Model(\n[decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n[decoder_outputs] + [state_h2, state_c2])","526eb11f":"plot_model(decoder_model,show_shapes = True)","e11298d7":"# defined a new variable to change words2index nd index2words\nreverse_target_word_index=y_tokenizer.index_word\nreverse_source_word_index=x_tokenizer.index_word\ntarget_word_index=y_tokenizer.word_index","a840a144":"# function for prediction of whole sentence by using loop\ndef decode_sequence(input_seq):\n    # Encode the input as state vectors.\n    e_out,e_h, e_c = encoder_model.predict(input_seq)\n    \n    # Generate empty target sequence of length 1.\n    target_seq = np.zeros((1,1))\n    \n    # Populate the first word of target sequence with the start word.\n    target_seq[0, 0] = target_word_index['sostok']\n\n    stop_condition = False\n    decoded_sentence = ''\n    while not stop_condition:\n      \n        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out,e_h, e_c])\n\n        # Sample a token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_token = reverse_target_word_index[sampled_token_index]\n        \n        if(sampled_token!='eostok'):\n            decoded_sentence += ' '+sampled_token\n\n        # Exit condition: either hit max length or find stop word.\n        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) >= (max_len_headlines-1)):\n            stop_condition = True\n\n        # Update the target sequence (of length 1).\n        target_seq = np.zeros((1,1))\n        target_seq[0, 0] = sampled_token_index\n\n        # Update internal states\n        e_h, e_c = h, c\n\n    return decoded_sentence","382bea7a":"x_val[0].shape","92bf588e":"## Testing phase \ndecode_sequence(np.array(x_val[0]).reshape(1,60))","0bc49204":"def seq2summary(input_seq):\n    newString=''\n    for i in input_seq:\n        if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\n            newString=newString+reverse_target_word_index[i]+' '\n    return newString\n\ndef seq2text(input_seq):\n    newString=''\n    for i in input_seq:\n        if(i!=0):\n            newString=newString+reverse_source_word_index[i]+' '\n    return newString","dc52e107":"for i in range(0,100):\n    print(\"Review:\",seq2text(x_val[i]))\n    print(\"Original summary:\",seq2summary(y_val[i]))\n    print(\"Predicted summary:\",decode_sequence(x_val[i].reshape(1,max_len_text)))\n    print(\"\\n\")","887cde3b":"### We are going to use word embedding (glove vectors ) ","3d9d223d":"### Inference Model ","303ee6f7":"### Time to prepare data ","71578531":"#### This is most important step to filter out those sentences who follows condition","90a7ba20":"#### here for deciding the length of the text we need to understand the distribution of the graph","f4c78952":"### Lets do some text preprocessing ","ac6f566e":"### Some analysis to decide the max_len_headlines and max_len_text","8edfe2af":"#### **i'm not applying word2vec in decoder phase becuase y_train have not that big corpus or not having that much big words vocabulary, and word embedding gives us more generalised and excellent model and accuracy **","8bfca832":"### This is attention layer and can be used in teacher forcing","72625920":"### Importing libraries for designing our architecture ","ec23c381":"stacked decoder more often need more epochs for training so for now we will not use them\n","a7406287":"### lets design our model architecture for training","a50f0218":"#### if word not available in our glove vector then that word will get 0 weightage","5f1d2f8f":"#### Dividing our data for train test split"}}