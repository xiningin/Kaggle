{"cell_type":{"51a3be8b":"code","a7ebaa27":"code","c4882680":"code","02556947":"code","7a8fe0f3":"code","6299facb":"code","ac4fdef5":"code","d44750a1":"code","95318c1e":"code","1a7c014b":"code","3dbcc07e":"code","3d9adfde":"code","d6d89cfb":"code","ef0919a1":"code","cbd40f5c":"code","b3e6742c":"code","ca374890":"code","c3353805":"code","89349e6a":"code","294e100c":"code","bea83811":"code","7a792ecd":"code","9637ddd2":"code","6f7084fd":"code","438e2272":"code","20bf3172":"code","ceb70b2c":"code","df96ca10":"code","57687703":"code","1333a7d2":"code","f865b053":"code","bf45a059":"code","e2b4d687":"code","532136d9":"code","a283ab2f":"code","544e25bb":"code","26d365ac":"code","d385958a":"code","f3d9fdd7":"code","e520f6ae":"code","14b9e938":"code","57920ca3":"code","c108ba32":"code","26a0aee3":"code","1c494635":"code","b5ed4706":"code","0f0d3ce3":"code","d7f14687":"code","5cfc5686":"markdown","55416fc5":"markdown","7b11657f":"markdown","f16bc22a":"markdown","1fd95ba1":"markdown","f71cae6f":"markdown","c31807a1":"markdown","34631103":"markdown","6bbbfaa4":"markdown","b21ebe1e":"markdown","3ce84f6a":"markdown","fbc2f260":"markdown","cb4f9207":"markdown","bf46e71d":"markdown","a322ca95":"markdown","0d0c05bd":"markdown","3f7728c0":"markdown","64fa0dff":"markdown","2b350a3f":"markdown","95d34926":"markdown","5b54d9c7":"markdown","3561917e":"markdown"},"source":{"51a3be8b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a7ebaa27":"!pip install pyspark","c4882680":"from pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession, SQLContext\n\nfrom pyspark.sql.types import *\nimport pyspark.sql.functions as F\nfrom pyspark.sql.functions import udf, col\n\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.mllib.evaluation import RegressionMetrics\n\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\nfrom pyspark.ml.feature import VectorAssembler, StandardScaler\nfrom pyspark.ml.evaluation import RegressionEvaluator","02556947":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew ","7a8fe0f3":"spark_session = SparkSession.builder.master(\"local[2]\").appName(\"HousingRegression\").getOrCreate()","6299facb":"spark_context = spark_session.sparkContext","ac4fdef5":"spark_sql_context = SQLContext(spark_context)","d44750a1":"TRAIN_INPUT = '..\/input\/house-prices-advanced-regression-techniques\/train.csv'\nTEST_INPUT = '..\/input\/house-prices-advanced-regression-techniques\/test.csv'","95318c1e":"pd_train = pd.read_csv(TRAIN_INPUT)\npd_test = pd.read_csv(TEST_INPUT)\nna_cols = pd_train.columns[pd_train.isna().any()].tolist()\n","1a7c014b":"# Let's Explore how SalePrice is distributed against normal theoretical quantiles\nfig = plt.figure()\nax = fig.add_subplot()\nres = stats.probplot(pd_train['SalePrice'], plot=plt)","3dbcc07e":"sns.distplot(pd_train['SalePrice'] , fit=norm);\n\n# parameters\n(mu, sigma) = norm.fit(pd_train['SalePrice'])\n\nplt.suptitle('Normal distribution with mu = {:.2f} and sigma = {:.2f}'.format(mu, sigma))\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#ax = plt.axes()\n","3d9adfde":"corr = pd_train.corr()","d6d89cfb":"corr[['SalePrice']].sort_values(by='SalePrice',ascending=False).style.background_gradient(cmap='viridis', axis=None)","ef0919a1":"fig, axes = plt.subplots(1, 2, sharex=True, figsize=(15,5))\naxes[0].set_xlim(0,10)\n\nsns.scatterplot(data=pd_train, ax=axes[0], x='OverallQual', y='SalePrice')\naxes[0].set_title('OverallQual vs SalePrice')\nsns.scatterplot(data=pd_train, ax=axes[1], x='GarageCars', y='SalePrice')\naxes[1].set_title('GarageCars vs SalePrice')","cbd40f5c":"fig, axes = plt.subplots(1, 2, sharex=True, figsize=(15,5))\naxes[0].set_xlim(0, 6000)\n\nsns.scatterplot(data=pd_train, ax=axes[0], x='GrLivArea', y='SalePrice')\naxes[0].set_title('GrLivArea vs SalePrice')\nsns.scatterplot(data=pd_train, ax=axes[1], x='GarageArea', y='SalePrice')\naxes[1].set_title('GarageArea vs SalePrice')","b3e6742c":"fig, axes = plt.subplots(1, 2, sharex=True, figsize=(15,5))\naxes[0].set_xlim(0, 6000)\n\nsns.scatterplot(data=pd_train, ax=axes[0], x='TotalBsmtSF', y='SalePrice')\naxes[0].set_title('TotalBsmtSF vs SalePrice')\nsns.scatterplot(data=pd_train, ax=axes[1], x='1stFlrSF', y='SalePrice')\naxes[1].set_title('1stFlrSF vs SalePrice')","ca374890":"total = pd_train.isnull().sum().sort_values(ascending=False)\npercent = (pd_train.isnull().sum()\/pd_train.shape[0]).sort_values(ascending=False)\n\nmissing = pd.concat([total, percent], axis=1, keys=['Total', 'Perc_missing'])\nmissing.head(15)","c3353805":"# We will remove features with missing proportion of more than 15% (thumb rule)\n\npd_train = pd_train.drop((missing[missing['Perc_missing'] >= 0.15]).index,1)\npd_train.head()","89349e6a":"pd_train['New'] = pd_train['OverallQual'] * pd_train['GarageArea'] * pd_train['GrLivArea']\npd_test['New'] = pd_test['OverallQual'] * pd_test['GarageArea'] * pd_test['GrLivArea']\n\n# As some of the contestants have noticed, this results in a spike in model performance later","294e100c":"train_cols = list(pd_train.columns)\ntrain_cols.remove('SalePrice')","bea83811":"#Make test ds feature set same as in train ds\npd_test = pd_test[train_cols]","7a792ecd":"pd_test.columns[pd_test.isna().any()].tolist()","9637ddd2":"# Althoug this is not the best solution to fill the NA-values with \"None\"\/0, for most of the features \n# in the particular datas, it literally means \"None\"\/0 (e.g. Garage Area, Garage Type, Condition) as the house\n# probably doesn't have the garage.\n\nfor col in ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']:\n    pd_train[col] = pd_train[col].fillna(\"None\")\n    pd_test[col] = pd_test[col].fillna(\"None\")\n    \nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    pd_train[col] = pd_train[col].fillna(\"None\")\n    pd_test[col] = pd_test[col].fillna(\"None\")\n    \nfor col in ['GarageYrBlt', 'GarageArea', 'GarageCars']:\n    pd_train[col] = pd_train[col].fillna(0)\n    pd_test[col] = pd_test[col].fillna(0)\n    \npd_train['MasVnrType'] = pd_train['MasVnrType'].fillna(\"None\")\npd_test['MasVnrType'] = pd_test['MasVnrType'].fillna(\"None\")\n\npd_train['MasVnrArea'] = pd_train['MasVnrArea'].fillna(0)\npd_test['MasVnrArea'] = pd_test['MasVnrArea'].fillna(0)\n\npd_train['Electrical'] = pd_train['Electrical'].fillna(pd_train['Electrical'].mode()[0])\npd_test['Electrical'] = pd_test['Electrical'].fillna(pd_test['Electrical'].mode()[0])\n\nprint(pd_train.isnull().sum().max()) # check if any missing values are left\nprint(pd_test.isnull().sum().max())","6f7084fd":"pd_test['BsmtFinSF1'] = pd_test['BsmtFinSF1'].fillna(pd_test['BsmtFinSF1'].mean())\npd_test['BsmtFinSF2'] = pd_test['BsmtFinSF2'].fillna(pd_test['BsmtFinSF2'].mean())\npd_test['BsmtUnfSF'] = pd_test['BsmtUnfSF'].fillna(pd_test['BsmtUnfSF'].mean())\npd_test['TotalBsmtSF'] = pd_test['TotalBsmtSF'].fillna(pd_test['TotalBsmtSF'].mean())\npd_test['BsmtFullBath'] = pd_test['BsmtFullBath'].fillna(pd_test['BsmtFullBath'].mean())\npd_test['BsmtHalfBath'] = pd_test['BsmtHalfBath'].fillna(pd_test['BsmtHalfBath'].mean())","438e2272":"# This is how fillna is done in PySpark\n\n# train_df = train_df.na.fill('NoData', subset=['MSZoning', 'Utilities', 'Exterior1st', 'Exterior2nd', 'BsmtFinSF1',\n#       'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath',\n#       'BsmtHalfBath', 'KitchenQual', 'Functional', 'GarageCars', 'GarageArea','SaleType'])\n# test_df = test_df.na.fill('NoData', subset=['MSZoning', 'Utilities', 'Exterior1st', 'Exterior2nd', 'BsmtFinSF1',\n#       'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath',\n#       'BsmtHalfBath', 'KitchenQual', 'Functional', 'GarageCars', 'GarageArea','SaleType'])","20bf3172":"cat_columns = pd_train.select_dtypes(include=['object']).columns\npd_train[cat_columns] = pd_train[cat_columns].fillna('NoData')\npd_test[cat_columns] = pd_test[cat_columns].fillna('NoData')","ceb70b2c":"print(\"Dropping outliers resulted in %d instances in the new dataset\" % len(pd_train))\npd_train = pd_train.drop(pd_train[(pd_train['GrLivArea']>4500) \n                                & (pd_train['SalePrice']<300000)].index)\nprint(\"Dropping outliers resulted in %d instances in the new dataset\" % len(pd_train))\npd_train = pd_train.drop(pd_train[(pd_train['GrLivArea']>5500) \n                                | (pd_train['SalePrice']>500000)].index)\nprint(\"Dropping outliers resulted in %d instances in the new dataset\" % len(pd_train))\npd_train = pd_train.drop(pd_train[pd_train['GarageArea']>1100].index)\nprint(\"Dropping outliers resulted in %d instances in the new dataset\" % len(pd_train))","df96ca10":"train_df = spark_session.createDataFrame(pd_train)\ntest_df = spark_session.createDataFrame(pd_test)","57687703":"train_df = train_df.select([c for c in train_df.columns if c not in na_cols])\ntrain_cols = train_df.columns\ntrain_cols.remove('SalePrice')\ntest_df = test_df.select(train_cols)","1333a7d2":"from pyspark.sql.types import IntegerType\n\n# As PySpark DFs can be finicky, sometimes your have to explicitly cast certain data types to columns\n\ntest_df = test_df.withColumn(\"BsmtFinSF1\", test_df[\"BsmtFinSF1\"].cast(IntegerType()))\ntest_df = test_df.withColumn(\"BsmtFinSF2\", test_df[\"BsmtFinSF2\"].cast(IntegerType()))\ntest_df = test_df.withColumn(\"BsmtUnfSF\", test_df[\"BsmtUnfSF\"].cast(IntegerType()))\ntest_df = test_df.withColumn(\"TotalBsmtSF\", test_df[\"TotalBsmtSF\"].cast(IntegerType()))\ntest_df = test_df.withColumn(\"BsmtFullBath\", test_df[\"BsmtFullBath\"].cast(IntegerType()))\ntest_df = test_df.withColumn(\"BsmtHalfBath\", test_df[\"BsmtHalfBath\"].cast(IntegerType()))\ntest_df = test_df.withColumn(\"GarageCars\", test_df[\"GarageCars\"].cast(IntegerType()))\ntest_df = test_df.withColumn(\"GarageArea\", test_df[\"GarageArea\"].cast(IntegerType()))\n\n# train_df.printSchema()","f865b053":"# Defining string columns to pass on to the String Indexer (= categorical feature encoding)\n\ntrain_string_columns = []\n\nfor col, dtype in train_df.dtypes:\n    if dtype == 'string':\n        train_string_columns.append(col)","bf45a059":"from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer\n\nindexers = [StringIndexer(inputCol=column, outputCol=column+'_index', handleInvalid='keep').fit(train_df) for column in train_string_columns]\n\n\npipeline = Pipeline(stages=indexers)\ntrain_indexed = pipeline.fit(train_df).transform(train_df)","e2b4d687":"print(len(train_indexed.columns))","532136d9":"test_string_columns = []\n\nfor col, dtype in test_df.dtypes:\n    if dtype == 'string':\n        test_string_columns.append(col)","a283ab2f":"indexers2 = [StringIndexer(inputCol=column, outputCol=column+'_index', handleInvalid='keep').fit(test_df) for column in test_string_columns]\n\npipeline2 = Pipeline(stages=indexers2)\ntest_indexed = pipeline2.fit(test_df).transform(test_df)","544e25bb":"print(len(test_indexed.columns))","26d365ac":"def get_dtype(df,colname):\n    return [dtype for name, dtype in df.dtypes if name == colname][0]\n\nnum_cols_train = []\nfor col in train_indexed.columns:\n    if get_dtype(train_indexed,col) != 'string':\n        num_cols_train.append(str(col))\n        \nnum_cols_test = []\nfor col in test_indexed.columns:\n    if get_dtype(test_indexed,col) != 'string':\n        num_cols_test.append(str(col))\n\ntrain_indexed = train_indexed.select(num_cols_train)\ntest_indexed = test_indexed.select(num_cols_test)","d385958a":"print(len(train_indexed.columns))\nprint(len(test_indexed.columns))","f3d9fdd7":"from pyspark.ml.feature import VectorAssembler\nvectorAssembler = VectorAssembler(inputCols = train_indexed.drop(\"SalePrice\").columns, outputCol = 'features').setHandleInvalid(\"keep\")\n\ntrain_vector = vectorAssembler.transform(train_indexed)","e520f6ae":"vectorAssembler2 = VectorAssembler(inputCols = test_indexed.columns, outputCol = 'features').setHandleInvalid(\"keep\")\n\ntest_vector = vectorAssembler2.transform(test_indexed)","14b9e938":"from pyspark.sql.functions import lit\n\ntest_vector = test_vector.withColumn(\"SalePrice\", lit(0))","57920ca3":"#You can use this to scale all instances, however, as I checked, this did not improve the performance\n\n#from pyspark.ml.feature import StandardScaler\n#scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False)\n#scalerModel = scaler.fit(train_vector)\n#scaled_train = scalerModel.transform(train_vector)\n\n#scalerModel2 = scaler.fit(test_vector)\n#scaled_test = scalerModel2.transform(test_vector)","c108ba32":"# Train-test split\n\nsplits = train_vector.randomSplit([0.7, 0.3])\ntrain = splits[0]\nval = splits[1]","26a0aee3":"# Simple baseline (linreg)\n\nfrom pyspark.ml.regression import LinearRegression\n\nlr = LinearRegression(featuresCol = 'features', labelCol='SalePrice', maxIter=10, \n                      regParam=0.8, elasticNetParam=0.1) # It is always a good idea to play with hyperparameters.\nlr_model = lr.fit(train)\n\ntrainingSummary = lr_model.summary\nprint(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\nprint(\"r2: %f\" % trainingSummary.r2)\n\nlr_predictions = lr_model.transform(val)\nlr_predictions.select(\"prediction\",\"SalePrice\",\"features\").show(5)\n\nfrom pyspark.ml.evaluation import RegressionEvaluator\nlr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n                 labelCol=\"SalePrice\",metricName=\"r2\")\nprint(\"R Squared (R2) on val data = %g\" % lr_evaluator.evaluate(lr_predictions))","1c494635":"# A more complex model with RF\n\nfrom pyspark.ml.regression import RandomForestRegressor\n\nrf = RandomForestRegressor(featuresCol = 'features', labelCol='SalePrice', \n                           maxDepth=20, \n                           minInstancesPerNode=2,\n                           bootstrap=True\n                          )\nrf_model = rf.fit(train)\n\nrf_predictions = rf_model.transform(val)\nrf_predictions.select(\"prediction\",\"SalePrice\",\"features\").show(5)\n\nfrom pyspark.ml.evaluation import RegressionEvaluator\nrf_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n                 labelCol=\"SalePrice\",metricName=\"r2\")\nprint(\"R Squared (R2) on val data = %g\" % rf_evaluator.evaluate(rf_predictions))","b5ed4706":"rf_predictions2 = rf_model.transform(test_vector)\n#rf_predictions2.printSchema()\npred = rf_predictions2.select(\"Id\",\"prediction\")\npred = pred.withColumnRenamed(\"prediction\",\"SalePrice\")\n\nfrom pyspark.sql.types import FloatType, IntegerType\n\n#pred.printSchema()\npred = pred.withColumn(\"Id\", pred[\"Id\"].cast(IntegerType()))\npred = pred.withColumn(\"SalePrice\", pred[\"SalePrice\"].cast(FloatType()))\n","0f0d3ce3":"pred_pd = pred.toPandas()\nsave = pred_pd.to_csv(\"submission.csv\", index=False)\nsave","d7f14687":"pred_pd","5cfc5686":"**Let's convert the predictions in the .csv file for submission**","55416fc5":"**We can see that the top-10 features have a high correlation level with the target variable.**","7b11657f":"# 3. Model building (MLlib)","f16bc22a":"## Handling missing data: \n**determining the proportion of missing data from overall dataset.**","1fd95ba1":"**A good idea would be to create a new feature with 3-4 of top-features combined, which we will try later. [see \"New\" Feature]**","f71cae6f":"# House Prices Prediction baseline (written on PySpark)\n**Task type:** Regression\n\n**Models used:** Linear Regression, RandomForest Regression\n\n<img src=\"http:\/\/www.freeiconspng.com\/uploads\/house-from-premier-builders-in-carthage-mo-64836--home-builders-5.png\" height=200 width=200>","c31807a1":"**QQ-plot**","34631103":"**Now let's build a correlation matrix**","6bbbfaa4":"*This notebook is a version of the solution to the Kaggle's **House Prices - Advanced Regression Techniques** competition. It was written using PySpark though the amount of data does not require using BigData techniques.*","b21ebe1e":"**First, let's us pandas to handle the missing data and do some visualizations.**","3ce84f6a":"# 5. Conclusion","fbc2f260":"**Now let's explore the dataset for outliers.**","cb4f9207":"**Before passing the data on to the PySpark model, we need to vectorize the data.**","bf46e71d":"# 4. Making predictions & submission","a322ca95":"**Dist plot (normal vs factual distribution of SalePrice)**","0d0c05bd":"## Creating Spark DataFrames","3f7728c0":"**However, we achieved quite good metrics of R^2~0.9 with Linear Regression and Random Forest Regression.**","64fa0dff":"# 2. Loading data & EDA","2b350a3f":"## Handling outliers: \n**the values are based on the scatterplots above.**","95d34926":"**PySpark offers way less flexibility in working with data & models. This notebook represents just an exercise in using it. As for this particular dataset, you'd better use classical instruments.**","5b54d9c7":"**Installing PySpark in cause you don't have it preinstalled on your environment.**","3561917e":"# 1. Initiate Spark environment"}}