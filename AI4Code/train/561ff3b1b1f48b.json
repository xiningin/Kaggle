{"cell_type":{"144e116a":"code","cb19bf6a":"code","fbbba976":"code","9c3920e3":"code","659f443d":"code","fc1b33a1":"code","a905f430":"code","11a8d172":"code","c3d7085a":"code","bba2ecef":"code","50ba6f8c":"code","83ff9094":"code","450c74e6":"markdown"},"source":{"144e116a":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\n\nplt.ion()","cb19bf6a":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device %r' % device)\nbatch_size = 64\nnum_workers = 4","fbbba976":"class MNIST(torch.utils.data.Dataset):\n\n    def __init__(self, path, train=True, transform=None):\n        self.train = train\n        # I don't like pandas ---\n        # It's all coarse, and\n        # Rough, and irritating.\n        # And it gets everywhere.\n        with open(path) as file:\n            _, *lines = file.readlines()\n        if train:\n            labels = []\n        datapoints = []\n        for line in lines:\n            if train:\n                label, *data = line.split(',')\n                label = int(label)\n                labels.append(label)\n            else:\n                data = line.split(',')\n            data = np.array([int(d) for d in data], dtype=np.float).reshape((28, 28))\n            if transform is not None:\n                data = transform(data)\n            datapoints.append(data.tolist())\n        if train:\n            self.labels = torch.tensor(labels, dtype=torch.long)\n        self.datapoints = torch.tensor(datapoints, dtype=torch.float)\n\n    def __len__(self):\n        return self.datapoints.size(0)\n\n    def __getitem__(self, key):\n        datapoint = self.datapoints[key]\n        if self.train:\n            label = self.labels[key]\n            return datapoint, label\n        else:\n            return datapoint","9c3920e3":"transform = torchvision.transforms.Compose([\n    torchvision.transforms.ToTensor(),\n    torchvision.transforms.Normalize(mean=[0.5], std=[0.5]),\n])\n\ntrainset = MNIST('\/kaggle\/input\/digit-recognizer\/train.csv', train=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n                                          shuffle=True, num_workers=num_workers,\n                                          pin_memory=True)\n\ntestset = MNIST('\/kaggle\/input\/digit-recognizer\/test.csv', train=False, transform=transform)\ntestloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n                                         shuffle=True, num_workers=num_workers,\n                                         pin_memory=True)","659f443d":"dataiter = iter(trainloader)\nimages, labels = dataiter.next()\nplt.imshow(torchvision.utils.make_grid(images).numpy().transpose((1, 2, 0)) \/ 2 + 0.5,\n           cmap='gray')\nplt.title(', '.join(str(label) for label in labels.tolist()))","fc1b33a1":"class Model(torch.nn.Module):\n    \n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv1 = torch.nn.Sequential(\n            torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            torch.nn.ReLU(),\n            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n        )\n        self.dense = torch.nn.Sequential(\n            torch.nn.Linear(14 * 14 * 128, 1024),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(p=0.5),\n            torch.nn.Linear(1024, 10),\n        )\n    def forward(self, x):\n        x = self.conv1(x)\n        x = x.view(-1, 14 * 14 * 128)\n        x = self.dense(x)\n        return x\n\nmodel = Model()\nmodel.to(device)\nmodel","a905f430":"criterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)","11a8d172":"# Somehow this is not fitting as good as my machine\n# [It works on my machine]\n# It can get as high as 99.9 or as low as 99.7\n# But no difference\n# Should be stable with more epochs\nfor epoch in range(20):\n    running_total = running_errors = running_loss = 0\n    for i, (images, labels) in enumerate(trainloader):\n        optimizer.zero_grad()\n        outputs = model(images.to(device))\n        loss = criterion(outputs, labels.to(device))\n        loss.backward()\n        optimizer.step()\n\n        running_total += labels.size(0)\n        running_errors += (outputs.cpu().max(1).indices != labels).sum().item()\n        running_loss += loss.item()\n        if i == 0 and epoch > 0:\n            print()\n        if i % 10 == 9:\n            print(f'\\rEpoch {epoch + 1}, sample {(i + 1) * batch_size:6d}, '\n                  f'loss {running_loss \/ running_total:.5f}, '\n                  f'acc {(1 - running_errors \/ running_total) * 100:2.2f}')\n            running_total = running_errors = running_loss = 0","c3d7085a":"torch.save(model.state_dict(), 'model.pt')\n!ls -lah","bba2ecef":"model.eval()\ntotal = 0\ncorrects = 0\nfor i, (images, labels) in enumerate(trainloader):\n    total += labels.size(0)\n    corrects += (model(images.to(device)).argmax(1) == labels.to(device)).sum().cpu().item()\n    print('\\r' + str(i), end='')\nprint()\nprint(corrects \/ total)","50ba6f8c":"dataiter = iter(testloader)\nimages, labels = dataiter.next()\nimages = images[:8, :, :, :]\nlabels = labels[:8]\nplt.imshow(torchvision.utils.make_grid(images).numpy().transpose((1, 2, 0)) \/ 2 + 0.5,\n           cmap='gray')\nplt.title(', '.join(str(label) for label in labels.tolist()))\npreds = ', '.join(str(pred.item()) for pred in model(images.to(device)).argmax(1).cpu())\nprint(f'Predictions: {preds}')","83ff9094":"with open('\/kaggle\/working\/predictions.csv', 'w') as file:\n    file.write('ImageId,Label\\n')\n    for i, image in enumerate(testset, 1):\n        file.write('%d,%d\\n' % (i, model(image.to(device).unsqueeze(0)).argmax().item()))\n        print('\\rWritten row %5d' % i, end='')","450c74e6":"Appears that it's not that stable. If you want, add more epochs to make it stable.\n\nTest accuracy can reach as low as 98% and as high as 99.8%. Maybe there was some bug in the code. I don't know."}}