{"cell_type":{"5cf68af4":"code","a47e6239":"code","8af7aaf7":"code","eede7c92":"code","08dd0f4b":"code","f13a4c61":"code","ae2d250e":"code","831ac767":"code","fb836d6f":"code","65fc6626":"code","d8fd57c5":"code","edfafc88":"code","3b50756f":"code","baf1e0f6":"code","04945648":"code","27c3ff5e":"code","31621215":"code","2522004e":"code","ccbae5cf":"code","a4a6d04c":"code","5e3c54b4":"code","f9156b3d":"code","bc8a5218":"code","5dadd36b":"code","042355a1":"code","937a38af":"code","bb03480e":"code","efe73b89":"markdown","17ecd1e7":"markdown"},"source":{"5cf68af4":"# import the libraries\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nimport numpy as np\n\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\n\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split","a47e6239":"# Load dataset\n\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\ngender_submission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')","8af7aaf7":"# training data description\nprint(train.shape)\ntrain.head()","eede7c92":"# test data description\nprint(test.shape)\ntest.head()","08dd0f4b":"# check the submission file format.\nprint(gender_submission.shape)\ngender_submission.head()","f13a4c61":"train.info()","ae2d250e":"test.info()","831ac767":"# combine dataset\ntrain_test = pd.concat([train, test], sort=True)","fb836d6f":"train_test.info()","65fc6626":"#Simply check the value counts\nprint(train_test['Pclass'].value_counts(), '\\n')\nprint(train_test['Sex'].value_counts(), '\\n')\nprint(train_test['SibSp'].value_counts(), '\\n')\nprint(train_test['Parch'].value_counts(), '\\n')\nprint(train_test['Ticket'].value_counts().head(10), '\\n')\nprint(train_test['Fare'].value_counts().head(10), '\\n')\nprint(train_test['Cabin'].value_counts().head(10), '\\n')\nprint(train_test['Embarked'].value_counts(), '\\n')","d8fd57c5":"# Split data for visualization\ndf_survived = train_test[train_test['Survived'] == 1.0]\ndf_nonsurvived = train_test[train_test['Survived'] == 0.0]\n\nlabels = ['Survived', 'Nonsurvived']\n\nfig, axs = plt.subplots(3, 2, figsize=(10, 10))\naxs[0,0].hist([df_survived['Pclass'], df_nonsurvived['Pclass']], color=['blue', 'red'], alpha=0.5, bins='auto', label=labels)\naxs[0,0].set_title('Pclass')\naxs[0,0].legend()\naxs[0,1].hist([df_survived['Sex'], df_nonsurvived['Sex']], color=['blue', 'red'], alpha=0.5, bins='auto', label=labels)\naxs[0,1].set_title('Sex')\naxs[0,1].legend()\naxs[1,0].hist([df_survived['Age'], df_nonsurvived['Age']], color=['blue', 'red'], alpha=0.5, range=(0, 90), label=labels)\naxs[1,0].set_title('Age')\naxs[1,0].legend()\naxs[1,1].hist([df_survived['SibSp'], df_nonsurvived['SibSp']], color=['blue', 'red'], alpha=0.5, bins='auto', label=labels)\naxs[1,1].set_title('SibSp')\naxs[1,1].legend()\naxs[2,0].hist([df_survived['Parch'], df_nonsurvived['Parch']], color=['blue', 'red'], alpha=0.5, bins='auto', label=labels)\naxs[2,0].set_title('Parch')\naxs[2,0].legend()\naxs[2,1].hist([df_survived['Embarked'].fillna('missval'), df_nonsurvived['Embarked'].fillna('missval')], color=['blue', 'red'], alpha=0.5, label=labels)\naxs[2,1].set_title('Embarked')\naxs[2,1].legend()","edfafc88":"# separate alphabetic character and cabin number\ndf_cabin = pd.DataFrame()\ndf_cabin['survived'] = train_test['Survived']\n\n# unavailable value settings\ndf_cabin['cab'] = train_test['Cabin'].fillna('N').apply(lambda x : x[:1])\ndf_cabin['cabnum'] = train_test['Cabin'].fillna('N').apply(lambda x : x[1:3])\ndf_cabin.loc[df_cabin['cabnum'].str.contains(' '), 'cabnum'] = -1\ndf_cabin.loc[df_cabin['cabnum'] == '', 'cabnum'] = -1\n\ndf_cabin['cabnum'].astype(int)\n#pd.set_option('display.max_rows', 90)\ndf_cabin['cabnum'].value_counts().head(10)","3b50756f":"df_cabin['cab'].value_counts()","baf1e0f6":"fig, ax = plt.subplots(1, 1, figsize=(10, 3))\nax.hist([df_cabin[df_cabin['survived']==0.0]['cab'], df_cabin[df_cabin['survived']==1.0]['cab']], alpha=0.5, bins='auto', color=['red', 'blue'])\nax.set_title('Cabin')\nax.legend(['Survived', 'Nonsurvived'])\n#ax[1].hist([temp[temp['survived']==0.0]['cabnum'], temp[temp['survived']==1.0]['cabnum']], alpha=0.5, color=['red', 'blue'], bins='auto')\n#ax[1].set_title('number')","04945648":"df_feature = train_test.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n\n# binary description about sex\nle = LabelEncoder()\ndf_feature['Sex'] = le.fit_transform(df_feature['Sex'])\n\n# one-hot description for embarked feature\ndf_feature = pd.get_dummies(df_feature, columns=['Embarked'], drop_first=True)\ndf_feature.head()","27c3ff5e":"df_feature.info()","31621215":"df_feature.describe()","2522004e":"df_feature['Age'].fillna(-1, inplace=True)","ccbae5cf":"# only one NaN value is in Fare feature\ndf_feature[df_feature['Fare'].isnull()]","a4a6d04c":"#\u3000Interpolate with mean value\n#feature.loc[feature['Fare'].isnull(), 'Fare'] = feature['Fare'].mean()\ndf_feature.loc[df_feature['Fare'].isnull(), 'Fare'] = df_feature.query('Pclass == 3 and Parch == 0 and SibSp == 0 and Embarked_Q == 0 and Embarked_S == 1 and Sex == 1')['Fare'].mean()","5e3c54b4":"train_X = df_feature[df_feature['Survived'].notnull()].drop(['Survived'], axis=1)\ntrain_y = df_feature.loc[df_feature['Survived'].notnull(), 'Survived']\ntest_X = df_feature[df_feature['Survived'].isnull()].drop(['Survived'], axis=1)\ntest_y = df_feature.loc[df_feature['Survived'].isnull(), 'Survived']","f9156b3d":"# AdaBoost Classifier\ntree = DecisionTreeClassifier(criterion='entropy', max_depth=4, random_state=1)\nmodel = AdaBoostClassifier(base_estimator=tree, n_estimators=200, learning_rate=00.1, random_state=1)\n\n# Cross validation\n#score = cross_val_score(estimator=model, X=train_X, y=train_y, cv=4, n_jobs=-1)\n#score\n\n'''\n# Grid Search\nparam_grid = {'base_estimator__max_depth': [2], \n                          'n_estimators': np.array([50, 100, 200, 300, 400, 500]), \n                          'learning_rate': [0.01]}\ngs = GridSearchCV(estimator=AdaBoostClassifier(base_estimator=DecisionTreeClassifier(criterion='entropy', random_state=1), random_state=1), \n                                  param_grid=param_grid, \n                                  cv=4)\ngs.fit(train_X, train_y)\nprint('Best cross validation score:', gs.best_score_)\nprint('Best parameters: ', gs.best_params_)\n'''\n\nmodel.fit(train_X, train_y)\nmodel.score(train_X, train_y)\n\nz = model.predict(test_X).astype(int)\n\ngender_submission['Survived'] = z\ngender_submission.to_csv('adaboost.csv', index=False)","bc8a5218":"# Random Forest Classifier\nmodel = RandomForestClassifier(random_state=1)\n\n# Cross validation\n#score = cross_val_score(estimator=model, X=train_X, y=train_y, cv=4, n_jobs=-1)\n#score\n\nmodel.fit(train_X, train_y)\nmodel.score(train_X, train_y)\n\nz = model.predict(test_X).astype(int)\n\ngender_submission['Survived'] = z\ngender_submission.to_csv('random_forest.csv', index=False)","5dadd36b":"# LightGBM Classifier\n# Cross validation with accuracy\n\nkfold = StratifiedKFold(n_splits=4, random_state=1, shuffle=True).split(train_X, train_y)\nscores = []\nlgbm_params = {'objective': 'binary', 'seed': 1, 'metric': 'binary_logloss'}\n\nfor k, (train_id, test_id) in enumerate(kfold):\n    \n    eval_X, ktest_X, eval_y, ktest_y = train_test_split(train_X.iloc[test_id], train_y.iloc[test_id], test_size=0.5, random_state=1, stratify=train_y.iloc[test_id])\n    \n    lgb_train = lgb.Dataset(train_X.iloc[train_id], train_y.iloc[train_id])\n    lgb_eval = lgb.Dataset(eval_X, eval_y)\n    \n    model = lgb.train(lgbm_params, lgb_train, num_boost_round=1000, valid_names=['train', 'valid'], valid_sets=[lgb_train, lgb_eval], early_stopping_rounds=5) \n    pred = model.predict(ktest_X, num_iteration=model.best_iteration)\n    \n    # convert class probability into binary\n    pred_class = np.empty((len(pred), ))\n    \n    for i, prob in enumerate(pred):\n        if prob > 0.5:\n            pred_class[i] = 1.0\n        else:\n            pred_class[i] = 0.0\n            \n    scores.append(accuracy_score(ktest_y, pred_class))\n    \nscores","042355a1":"# Light GBM Classifier \n# Cross validationb with logloss \n\nlgbm_params = {'objective': 'binary', 'seed': 1, 'metric': 'binary_logloss'}\n\nlgb_train = lgb.Dataset(train_X, train_y)\n\ncv_results = lgb.cv(lgbm_params, lgb_train, nfold=4)\ncv_logloss = cv_results['binary_logloss-mean']\nround_n = np.arange(len(cv_logloss))\n\nplt.xlabel('round')\nplt.ylabel('logloss')\nplt.plot(round_n, cv_logloss)\nplt.show()","937a38af":"# Training LightGBM\n\nlgbm_params = {'objective': 'binary', 'metric': 'binary_logloss', 'seed': 1}\n\nlgbm_train = lgb.Dataset(train_X, train_y)\nmodel = lgb.train(lgbm_params, lgbm_train, num_boost_round=30)\n\npred = model.predict(test_X)\n        \n# convert class probability into binary\npred_class = np.empty((len(pred), ), dtype=int)\n\nfor i, prob in enumerate(pred):\n    if prob > 0.5:\n        pred_class[i] = 1\n    else:\n        pred_class[i] = 0\n        \ngender_submission['Survived'] = pred_class\ngender_submission.to_csv('lightgbm.csv', index=False)","bb03480e":"# SVM Classifier\n\nsc = StandardScaler()\nsc.fit(train_X)\ntrain_X_std = sc.transform(train_X)\n\n# SVM Grid Search\n'''\nparam_grid = {'C': np.logspace(-3, 2, num=6), 'gamma': np.logspace(-3, 2, num=6)}\ngs = GridSearchCV(estimator=SVC(kernel='rbf', random_state=1), param_grid=param_grid, cv=5)\ngs.fit(train_X_std, train_y)\nprint('Best cross validation score:', gs.best_score_)\nprint('Best parameters: ', gs.best_params_)\n'''\n\n#svc = SVC(kernel='rbf', random_state=1, gamma=0.01, C=100.0)\n\n# Cross validation\n#svc_score = cross_val_score(estimator=svc, X=train_X_std, y=train_y, cv=4, n_jobs=-1)\n\n#svc.fit(train_X, train_y)\n#print(svc.score(train_X, train_y))\n\n#z = svc.predict(test_X).astype(int)","efe73b89":"Finally, cabin feature is dropped because it has a lot of unavailable values. ","17ecd1e7":"Ticket\u756a\u53f7\u306e\u793a\u3059\u610f\u5473\u304c\u4e0d\u660e \u2192 \u3042\u3068\u3067drop\u3059\u308b  \nCabin\u8981\u7d20\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u304c\u7d71\u4e00\u3055\u308c\u3066\u3044\u306a\u3044 \u2192 \u3042\u3068\u3067drop\u3059\u308b\n\nThe meaning of the ticket number is unknown. \u2192 It is dropped.  \nCabin features are not in the same format. \u2192 It is dropped."}}