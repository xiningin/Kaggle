{"cell_type":{"50239c49":"code","62b7f92a":"code","e99e9642":"code","db7c4b15":"code","b9aa5c75":"code","e4c7735b":"code","306ec80f":"code","8cd011c2":"code","535df26a":"code","a447e75d":"code","cebde8c5":"code","a7cfcfbd":"markdown","be5070db":"markdown","c648cf48":"markdown","15dddf3d":"markdown","3f58ba36":"markdown","cefaf923":"markdown","8bfedee6":"markdown","75c61e7e":"markdown"},"source":{"50239c49":"#Import libraries for data analysis, visualization, and ML model building\nimport numpy as np\nimport pandas as pd\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom scipy import stats\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","62b7f92a":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e99e9642":"train = pd.read_csv('\/kaggle\/input\/learn-together\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/learn-together\/test.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/learn-together\/sample_submission.csv')","db7c4b15":"#List of all non-binary columns\nnon_binary_cols = ['Elevation', 'Aspect', 'Slope',\n       'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology',\n       'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon',\n       'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points','Cover_Type']","b9aa5c75":"#The 'aspect' variable reflects the azimuth measurement in degrees (0-360).\n#Since a reading of 180 degrees indicates that the sun is directly overhead,\n#readings of 200 and 160 degrees would be equivalent. \n#Therefore, I updated the column for this variable to situate all readings between 0 and 180.\ntrain['aspect_updated']=np.absolute(180-train['Aspect'])\ntest['aspect_updated']=np.absolute(180-test['Aspect'])\nnon_binary_cols.append('aspect_updated')","e4c7735b":"#Create correlation table\ncorrelation_table = train[non_binary_cols].drop('Aspect',axis=1).corr().round(3)\n#Diagonally slice table to remove duplicates using np.tril method\ndata_viz_cols = correlation_table.columns\ndata_viz_index = correlation_table.index\ncorrelation_table = pd.DataFrame(np.tril(correlation_table),columns=data_viz_cols,index=data_viz_index).replace(0,np.nan).replace(1,np.nan)\n#Adjust plot size\na4_dims = (11.7, 8.27)\nfig, ax = plt.subplots(figsize=a4_dims)\nsns.heatmap(correlation_table, annot=True,cmap='coolwarm')\nplt.show()","306ec80f":"train['Cover_Type'] = train['Cover_Type'].astype('category')\ndummie_cats = pd.get_dummies(train['Cover_Type'])\ntrain_updated = pd.concat([train,dummie_cats],axis=1)\ntrain_updated = train_updated.drop(['Cover_Type','Id'],axis=1)\na4_dims = (10, 30)\nfig, ax = plt.subplots(figsize=a4_dims)\nsns.heatmap(abs(train_updated.corr()[[1,2,3,4,5,6,7]]).round(3).drop([1,2,3,4,5,6,7],axis=0),cmap='coolwarm',annot=True)\n","8cd011c2":"target_correlations = train.corr()['Cover_Type'].abs().round(3).sort_values(ascending=False)\ntarget_corr_series = pd.Series(target_correlations)\ntarget_corrs = target_corr_series[target_corr_series>.1]\ncorr_cols = target_corrs.index","535df26a":"#For the purposes of better understanding data, I ran the dataset through a K-Means model.\n#This type of unsupervised learning model can help see how features\n#naturally group into various \"clusters.\" \n\n#Scale the features between 0 and 1, so no feature disproportionately contributes to distance calculations\nfeatures = train.drop(['Id','Cover_Type'],axis=1)\nscaler = MinMaxScaler()\nfit = scaler.fit(features)\nnormalized_features = pd.DataFrame(scaler.transform(features),columns=features.columns)\n\n#Create a KMeans model where \ncluster_model = KMeans(n_clusters=7,random_state=1)\ncluster_model.fit_transform(normalized_features)\npredicted_clusters = cluster_model.labels_\nnormalized_features['cluster'] = predicted_clusters\nnormalized_features['cover_type'] = train['Cover_Type']\n\n#Print value counts for each cluster\nfor i in normalized_features['cluster'].unique():\n    df = normalized_features[normalized_features['cluster']==i]\n    counts = df['cover_type'].value_counts()\n    print('cluster '+str(i))\n    print('\\n')\n    print(counts)","a447e75d":"col_names = ['pca_'+str(i+1) for i in range(len(features.columns))]\npca = PCA(n_components=len(features.columns))\npca.fit_transform(features)\npca_df = pd.DataFrame(pca.components_,columns=col_names)\npca_df['cover_type'] = train['Cover_Type']\nx_lbls = [str(i+1) for i in range(len(features.columns))]\ny = pca.explained_variance_ratio_\nplt.figure(figsize=(20,5))\nplt.title('Scree Plot of Eigenvalues')\nplt.ylabel('% of Variance Explained')\nplt.xlabel('Principal Component')\nplt.bar(x_lbls,y)\nplt.show()","cebde8c5":"plt.figure(figsize=(5,5))\npca_two_feature = PCA(n_components=2)\npca_two_feature.fit_transform(features)\npca_two_feature_df = pd.DataFrame(pca_two_feature.components_)\npca_two_feature_df\n\n#plt.scatter(x=pca_two_feature_df['pca_1'],y=pca_two_feature_df['pca_2'],c=pca_df['cover_type'],cmap='prism')\n#plt.xlabel('pca_1')\n#plt.ylabel('pca_2')\n#plt.show()","a7cfcfbd":"Ideally, you'd want to see a one-to-one correspondence between each target label and the cluster label given by the algorithm, as this would indicate the distance metric used by the algorithm, when applied to all the available features, is a sufficient predictor of each row's cover type. As can be seen by the below output, certain cover types are distributed across clusters, while others only contain 3 or 4 cover types with one cover type being predominant (as regards frequency count). \n","be5070db":"### Feature Clustering","c648cf48":"### Principal Component Analysis (PCA)","15dddf3d":"### Correlation Matrix\n\nNotes to self on further steps:\n* make sure cover_type variable is converted to categorical dtype or is encoded as binary values in multiple columns to ensure regressions are run properly","3f58ba36":"The three non-binary variables with the biggest *positive* correlation coefficients are as follows:\n\n-----------\n| Feature 1 | Feature 2 | Correlation |\n|---|---|---|\n|Vertical Distance to Hydrology | Horizontal Distance to Hydrology|0.65|\n|Hillshade Noon|Hillshade 3pm |0.61|\n|Horizontal Distance to Roadways | Elevation |0.58|\n\n----------\n\nThe three non-binary variables with the biggest *negative* correlation coefficients are as follows:\n\n-----------\n| Feature 1 | Feature 2 | Correlation |\n|---|---|---|\n|Hillshade 9am| Hillshade 3pm|-0.78|\n|Hillshade Noon|Slope|-0.61|\n|Hillshade Noon|Aspect|-0.58|\n\n----------\nObservations Regarding Feature Correlations:\n\n* No features are strongly correlated with the target variable, Cover Type. Horizontal Distance to Roadways is negatively-correlated with the target with a weak -0.11 coefficient.\n* One would expect to see a positive correlation between Hillshade Noon and Slope as well as a negative  correlation between Hillshade Noon and Aspect, as Slope and Aspect would be the primary if not exclusive variables calculating the degree of hillshade when the Sun is directly overhead. \n* The strong correlation between the Vertical and Horizontal Distances to Hydrology seems to suggest that they are measuring proximity to the *same* body of water and can be combined using the Pythagorean theorem to calculate a direct distance to the water body. \n* Horizontal Distance to Roadways and Elevation is an intuitive correlation, as is the negative correlation between Hillshade 9am and Hillshade 3pm. ","cefaf923":"The above plot shows the explanatory power of each principal component as regards the variance in the data. Principal Component 1 (PC1) explains around 70 percent of the variance amongst all the data points, while PC2 explains around 20 percent. The elbow of the distribution lies between PC2 and PC3 where the y-axis values drop down to under 0.5 and quickly trail off with no values being present in the plot after PC6. Since PC1 and PC2 together capture just over 90 percent of the variance in the dataset, a 2-dimensional scatter plot <i>should<\/i> reveal statistically-significant clusters of data that would, hopefully, correspond to the target values we are looking to predict.","8bfedee6":"The scatterplot of the first two principal components does not show any clearly-identifiable clusters, meaning the cover types cannot be predicted by using these two features.","75c61e7e":"## Exploratory Data Analysis"}}