{"cell_type":{"a8be3e98":"code","0353666e":"code","181fe50b":"code","399a15a3":"code","77831c78":"code","c33f90f3":"code","dedb673a":"code","6585dba6":"code","5602ef17":"code","6140163a":"code","c6fc7b0d":"code","77253886":"code","914c2c2b":"code","fd225277":"code","0255856b":"code","ee32644d":"code","88ccd7d3":"code","987deac0":"code","2f3836bf":"code","91d4ab4d":"code","a2edf2aa":"code","99710bca":"code","940cbd77":"code","527f1c7e":"code","fd9a6e1b":"code","b06cd80f":"code","9735d7b2":"code","020290ae":"code","fc8667f9":"code","ab8146e9":"code","c92cebd7":"code","1b448ffe":"code","b55c3ea4":"code","8240ee99":"markdown","62d2df82":"markdown","6df62684":"markdown","05c5f986":"markdown","8b117caf":"markdown","6ef9fee7":"markdown","ce4a006c":"markdown","410d0311":"markdown","dd86014e":"markdown","f58bee7b":"markdown","8a50f96f":"markdown","f1cb7ccf":"markdown"},"source":{"a8be3e98":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\npd.options.display.float_format = '{:20,.2f}'.format\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom  warnings import simplefilter\nfrom sklearn.exceptions import ConvergenceWarning\nsimplefilter(\"ignore\", category=ConvergenceWarning)\n# Any results you write to the current directory are saved as output.","0353666e":"train = pd.read_csv('..\/input\/covid19-global-forecasting-week-1\/train.csv')\ntest = pd.read_csv('..\/input\/covid19-global-forecasting-week-1\/test.csv')\nsubmission = pd.read_csv('..\/input\/covid19-global-forecasting-week-1\/submission.csv')\n","181fe50b":"test['Date']=test.Date.astype('datetime64[ns]')","399a15a3":"train['key']=train['Province\/State'].astype('str')+ \" \" + train['Country\/Region'].astype('str')+ \" \" +train['Lat'].astype('str')+ \" \"  +train['Long'].astype('str')\n\ntest['key']=test['Province\/State'].astype('str')+ \" \" + test['Country\/Region'].astype('str')+ \" \" +test['Lat'].astype('str')+ \" \"  +test['Long'].astype('str')\n\ntrain.describe()","77831c78":"train.columns","c33f90f3":"daily_analysis=train.groupby(['Date']).sum()","dedb673a":"daily_analysis[['ConfirmedCases','Fatalities']].plot()","6585dba6":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n\ndef RMSLE(pred,actual):\n    return np.sqrt(np.mean(np.power((np.log(pred+1)-np.log(actual+1)),2)))","5602ef17":"train_lag_1=train.groupby(['key']).shift(periods=1)\ntrain_lag_2=train.groupby(['key']).shift(periods=2)\ntrain_lag_3=train.groupby(['key']).shift(periods=3)\ntrain['lag_1_ConfirmedCases']=train_lag_1['ConfirmedCases']\ntrain['lag_1_Fatalities']=train_lag_1['Fatalities']\ntrain['lag_2_ConfirmedCases']=train_lag_2['ConfirmedCases']\ntrain['lag_2_Fatalities']=train_lag_2['Fatalities']\ntrain['lag_3_ConfirmedCases']=train_lag_3['ConfirmedCases']\ntrain['lag_3_Fatalities']=train_lag_3['Fatalities']\ntrain","6140163a":"\ndef pred_ets(fcastperiod,fcastperiod1,actual,ffcast,type_ck='ConfirmedCases',verbose=False):\n    \n    actual=actual[actual[type_ck]>0]\n    index=pd.date_range(start=ffcast.index[0], end=ffcast.index[-1], freq='D')\n    data=ffcast[type_ck].values\n    ffcast1 = pd.Series(data, index)\n    index=pd.date_range(start=actual.index[0], end=actual.index[-1], freq='D')\n    data=actual[type_ck].values\n    daily_analysis_dat = pd.Series(data, index)\n    livestock2=daily_analysis_dat\n    fit=[]\n    fcast=[]\n    fname=[]\n    try:\n        fit1 = SimpleExpSmoothing(livestock2).fit()\n        fcast1 = fit1.forecast(fcastperiod1).rename(\"SES\")\n        fit.append(fit1)\n        fcast.append(fcast1)\n        fname.append('SES')\n    except:\n        1==1\n    try:\n        fit2 = Holt(livestock2).fit()\n        fcast2 = fit2.forecast(fcastperiod1).rename(\"Holt\")\n        fit.append(fit2)\n        fcast.append(fcast2)\n        fname.append('Holt')\n    except:\n        1==1\n    try:\n        fit3 = Holt(livestock2, exponential=True).fit()\n        fcast3 = fit3.forecast(fcastperiod1).rename(\"Exponential\")\n        fit.append(fit3)\n        fcast.append(fcast3)\n        fname.append('Exponential')\n    except:\n        1==1\n    try:\n        fit4 = Holt(livestock2, damped=True).fit(damping_slope=0.98)\n        fcast4 = fit4.forecast(fcastperiod1).rename(\"AdditiveDamped\")\n        fit.append(fit4)\n        fcast.append(fcast4)\n        fname.append('AdditiveDamped')\n    except:\n        1==1\n    try:\n        fit5 = Holt(livestock2, exponential=True, damped=True).fit()\n        fcast5 = fit5.forecast(fcastperiod1).rename(\"MultiplicativeDamped\")\n        fit.append(fit5)\n        fcast.append(fcast5)\n        fname.append('MultiplicativeDamped')\n    except:\n        1==1\n    try:\n        fit6 = Holt(livestock2, damped=True).fit()\n        fcast6 = fit6.forecast(fcastperiod1).rename(\"AdditiveDampedC\")\n        fit.append(fit6)\n        fcast.append(fcast6)\n        fname.append('AdditiveDampedC')\n    except:\n        1==1\n\n\n    def RMSLE(pred,actual):\n        return np.sqrt(np.mean(np.power((np.log(pred+1)-np.log(actual+1)),2)))\n    \n    pred_all_result=pd.concat([pd.DataFrame(k.fittedvalues) for k in fit],axis=1)\n    pred_all_result.columns=fname\n    all_result=pd.concat([pd.DataFrame(k) for k in fcast],axis=1)\n    col_chk=[]\n    for k in all_result.columns:\n        if verbose: print(\"actual value for method %s  is = %s\" % (k,RMSLE(all_result[k].values,ffcast[type_ck].values)))\n        if RMSLE(all_result[k].values,ffcast[type_ck].values) is not np.nan:\n            col_chk.append(k)\n    col_chk_f=[]\n    min_acc=-1\n    for k in col_chk:\n        acc=RMSLE(pred_all_result[k].values,actual[type_ck].values)\n#         if k =='AdditiveDamped' and acc>0.01:\n#             acc=acc-0.01\n        if verbose: print(\"pred value for method %s  is = %s\" % (k,acc))\n        if acc is not np.nan:\n            col_chk_f.append(k)\n            if min_acc==-1:\n                min_acc=acc\n                model_select=k\n            elif acc<min_acc:\n                min_acc=acc\n                model_select=k\n    all_result=all_result.append(pred_all_result,sort=False)\n\n    all_result['best_model']=model_select\n    all_result['best_pred']=all_result[model_select]\n    return all_result\n    #return pred_all_result,all_result\n","c6fc7b0d":"import sys\norig_stdout = sys.stdout\n\nFatalities_all_result_final=pd.DataFrame()\nConfirmedCases_all_result_Final=pd.DataFrame()\nfor keys in train['key'].unique():\n    chk=train[train['key']==keys]\n    chk.index=chk.Date\n    fcastperiod=0\n    fcastperiod1=35\n    actual=chk[:chk.shape[0]-fcastperiod]\n    ffcast=chk[chk.shape[0]-fcastperiod-1:]\n    ffcast\n    try:\n        Fatalities_all_result_1=pred_ets(fcastperiod,fcastperiod1,actual,ffcast,'Fatalities').reset_index()\n        \n        \n    except:\n        Fatalities_all_result_1=pd.DataFrame(pd.date_range(start=train.Date.min(), periods=60+fcastperiod1+1, freq='D')[1:])\n        Fatalities_all_result_1.columns=['index']\n        Fatalities_all_result_1['best_model']='naive'\n        Fatalities_all_result_1['best_pred']=0\n        \n    Fatalities_all_result_1['key']=keys\n    Fatalities_all_result_final=Fatalities_all_result_final.append(Fatalities_all_result_1,sort=True)\n    try:\n        ConfirmedCases_all_result_1=pred_ets(fcastperiod,fcastperiod1,actual,ffcast,'ConfirmedCases').reset_index()\n\n        \n    except:\n        ConfirmedCases_all_result_1=pd.DataFrame(pd.date_range(start=train.Date.min(), periods=60+fcastperiod1+1, freq='D')[1:])\n        ConfirmedCases_all_result_1.columns=['index']\n        ConfirmedCases_all_result_1['best_model']='naive'\n        ConfirmedCases_all_result_1['best_pred']=1\n    \n    ConfirmedCases_all_result_1['key']=keys\n    ConfirmedCases_all_result_Final=ConfirmedCases_all_result_Final.append(ConfirmedCases_all_result_1,sort=True)\n    print( ' done for %s' % keys)\nsys.stdout = orig_stdout","77253886":"ConfirmedCases_all_result_Final.rename(columns={'index':'Date'},inplace=True)\nFatalities_all_result_final.rename(columns={'index':'Date'},inplace=True)\nConfirmedCases_all_result_Final['best_pred']=np.where(ConfirmedCases_all_result_Final['best_pred'] is np.nan , 0,\n                                                       ConfirmedCases_all_result_Final['best_pred'] )\nFatalities_all_result_final['best_pred']=np.where(Fatalities_all_result_final['best_pred'] is np.nan , 0 ,\n                                                       Fatalities_all_result_final['best_pred'] )\nConfirmedCases_all_result_Final['best_pred']=np.where(ConfirmedCases_all_result_Final['best_pred'] <0 , 0,\n                                                       ConfirmedCases_all_result_Final['best_pred'] )\nFatalities_all_result_final['best_pred']=np.where(Fatalities_all_result_final['best_pred'] <0 , 0 ,\n                                                       Fatalities_all_result_final['best_pred'] )\nConfirmedCases_all_result_Final['best_pred_1']=np.where(ConfirmedCases_all_result_Final['AdditiveDamped'] is np.nan , ConfirmedCases_all_result_Final['best_pred'] ,\n                                                       ConfirmedCases_all_result_Final['AdditiveDamped'] )\nFatalities_all_result_final['best_pred_1']=np.where(Fatalities_all_result_final['AdditiveDamped'] is np.nan , Fatalities_all_result_final['best_pred'] ,\n                                                       Fatalities_all_result_final['AdditiveDamped'] )\nConfirmedCases_all_result_Final['best_pred_1']=np.where(ConfirmedCases_all_result_Final['best_pred'] is np.nan , 0,\n                                                       ConfirmedCases_all_result_Final['best_pred'] )\nFatalities_all_result_final['best_pred_1']=np.where(Fatalities_all_result_final['best_pred'] is np.nan , 0 ,\n                                                       Fatalities_all_result_final['best_pred'] )\nConfirmedCases_all_result_Final['best_pred_1']=np.where(ConfirmedCases_all_result_Final['best_pred'] <0 , 0,\n                                                       ConfirmedCases_all_result_Final['best_pred'] )\nFatalities_all_result_final['best_pred_1']=np.where(Fatalities_all_result_final['best_pred'] <0 , 0 ,\n                                                       Fatalities_all_result_final['best_pred'] )","914c2c2b":"test['Date']=test.Date.astype('datetime64[ns]')","fd225277":"\neval1 = ConfirmedCases_all_result_Final[['key','Date','best_pred','best_pred_1']].merge(test, how='right', on=['key','Date'])\neval1.rename(columns={'best_pred':'ConfirmedCases'},inplace=True)\neval1['ConfirmedCases']=eval1['ConfirmedCases'].fillna(0)\neval1","0255856b":"eval2 = Fatalities_all_result_final[['key','Date','best_pred','best_pred_1']].merge(test, how='right', on=['key','Date'])\n\neval2.rename(columns={'best_pred':'Fatalities'},inplace=True)\neval2['Fatalities']=eval2['Fatalities'].fillna(0)\neval2","ee32644d":"sub_prep = eval1[['ForecastId','ConfirmedCases','key']].merge(eval2[['ForecastId','Fatalities']], on=['ForecastId'],  how='left')\nsub_prep","88ccd7d3":"sub = sub_prep.merge(submission['ForecastId'], on=['ForecastId'],  how='right')\nsub","987deac0":"sub=sub[['ForecastId','ConfirmedCases','Fatalities']]\nsub=sub.sort_values('ForecastId')\nsub","2f3836bf":"sub.to_csv('submission.csv',header=['ForecastId','ConfirmedCases','Fatalities'],index=False)","91d4ab4d":"#sub.to_csv('submission.csv')\nsub","a2edf2aa":"train['Date']=train.Date.astype('datetime64[ns]')\nverify=train[['key','Date','ConfirmedCases','Fatalities']].merge(test[['key','Date','ForecastId']], how='inner', on=['key','Date'])\npred=verify[['ForecastId']].merge(sub, how='inner', on=['ForecastId'])","99710bca":"RMSLE(pred['Fatalities'].values,verify['Fatalities'].values)","940cbd77":"RMSLE(pred['ConfirmedCases'].values,verify['ConfirmedCases'].values)","527f1c7e":"ConfirmedCases_all_result_Final","fd9a6e1b":"best_model_key=ConfirmedCases_all_result_Final[['key','best_model']].drop_duplicates()\nmax_number_current=train.groupby('key').max()[['ConfirmedCases','Fatalities']].reset_index()\nbest_model_key=best_model_key.merge(max_number_current,on='key',how='left')\nbest_model_key=best_model_key.sort_values('ConfirmedCases',ascending=False).reset_index(drop=True)","b06cd80f":"for j in best_model_key.best_model.unique():\n    print('Top Countries\/District currently under %s growth rate' % str (j))\n    print(best_model_key[best_model_key['best_model']==j].head())\n    print('-'*30)\n    print('-'*30)","9735d7b2":"best_model_key.key.unique()","020290ae":"train","fc8667f9":"train_ck=train.groupby(['Country\/Region','Date']).sum().reset_index()\ntrain_ck['key']=train_ck['Country\/Region']","ab8146e9":"\nFatalities_all_result_final=pd.DataFrame()\nConfirmedCases_all_result_Final=pd.DataFrame()\nfor keys in train_ck['key'].unique():\n    chk=train_ck[train_ck['key']==keys]\n    chk.index=chk.Date\n    fcastperiod=0\n    fcastperiod1=35\n    actual=chk[:chk.shape[0]-fcastperiod]\n    ffcast=chk[chk.shape[0]-fcastperiod-1:]\n    ffcast\n    try:\n        Fatalities_all_result_1=pred_ets(fcastperiod,fcastperiod1,actual,ffcast,'Fatalities').reset_index()\n        \n        \n    except:\n        Fatalities_all_result_1=pd.DataFrame(pd.date_range(start=train.Date.min(), periods=60+fcastperiod1+1, freq='D')[1:])\n        Fatalities_all_result_1.columns=['index']\n        Fatalities_all_result_1['best_model']='naive'\n        Fatalities_all_result_1['best_pred']=0\n        \n    Fatalities_all_result_1['key']=keys\n    Fatalities_all_result_final=Fatalities_all_result_final.append(Fatalities_all_result_1,sort=True)\n    try:\n        ConfirmedCases_all_result_1=pred_ets(fcastperiod,fcastperiod1,actual,ffcast,'ConfirmedCases').reset_index()\n\n        \n    except:\n        ConfirmedCases_all_result_1=pd.DataFrame(pd.date_range(start=train.Date.min(), periods=60+fcastperiod1+1, freq='D')[1:])\n        ConfirmedCases_all_result_1.columns=['index']\n        ConfirmedCases_all_result_1['best_model']='naive'\n        ConfirmedCases_all_result_1['best_pred']=1\n    \n    ConfirmedCases_all_result_1['key']=keys\n    ConfirmedCases_all_result_Final=ConfirmedCases_all_result_Final.append(ConfirmedCases_all_result_1,sort=True)\n    print( ' done for %s' % keys)","c92cebd7":"ConfirmedCases_all_result_Final.rename(columns={'index':'Date'},inplace=True)\nFatalities_all_result_final.rename(columns={'index':'Date'},inplace=True)\nConfirmedCases_all_result_Final['best_pred']=np.where(ConfirmedCases_all_result_Final['best_pred'] is np.nan , 0,\n                                                       ConfirmedCases_all_result_Final['best_pred'] )\nFatalities_all_result_final['best_pred']=np.where(Fatalities_all_result_final['best_pred'] is np.nan , 0 ,\n                                                       Fatalities_all_result_final['best_pred'] )\nConfirmedCases_all_result_Final['best_pred']=np.where(ConfirmedCases_all_result_Final['best_pred'] <0 , 0,\n                                                       ConfirmedCases_all_result_Final['best_pred'] )\nFatalities_all_result_final['best_pred']=np.where(Fatalities_all_result_final['best_pred'] <0 , 0 ,\n                                                       Fatalities_all_result_final['best_pred'] )\nConfirmedCases_all_result_Final['best_pred_1']=np.where(ConfirmedCases_all_result_Final['AdditiveDamped'] is np.nan , ConfirmedCases_all_result_Final['best_pred'] ,\n                                                       ConfirmedCases_all_result_Final['AdditiveDamped'] )\nFatalities_all_result_final['best_pred_1']=np.where(Fatalities_all_result_final['AdditiveDamped'] is np.nan , Fatalities_all_result_final['best_pred'] ,\n                                                       Fatalities_all_result_final['AdditiveDamped'] )\nConfirmedCases_all_result_Final['best_pred_1']=np.where(ConfirmedCases_all_result_Final['best_pred'] is np.nan , 0,\n                                                       ConfirmedCases_all_result_Final['best_pred'] )\nFatalities_all_result_final['best_pred_1']=np.where(Fatalities_all_result_final['best_pred'] is np.nan , 0 ,\n                                                       Fatalities_all_result_final['best_pred'] )\nConfirmedCases_all_result_Final['best_pred_1']=np.where(ConfirmedCases_all_result_Final['best_pred'] <0 , 0,\n                                                       ConfirmedCases_all_result_Final['best_pred'] )\nFatalities_all_result_final['best_pred_1']=np.where(Fatalities_all_result_final['best_pred'] <0 , 0 ,\n                                                       Fatalities_all_result_final['best_pred'] )","1b448ffe":"best_model_key=ConfirmedCases_all_result_Final[['key','best_model']].drop_duplicates()\nmax_number_current=train_ck.groupby('key').max()[['ConfirmedCases','Fatalities']].reset_index()\nbest_model_key=best_model_key.merge(max_number_current,on='key',how='left')\nbest_model_key=best_model_key.sort_values('ConfirmedCases',ascending=False).reset_index(drop=True)","b55c3ea4":"best_model_key=best_model_key[~(best_model_key['key']=='China')].reset_index()\nfor j in best_model_key.best_model.unique():\n    print('Top Countries\/District currently under %s growth rate' % str (j))\n    print(best_model_key[best_model_key['best_model']==j].head(10))\n    print('-'*30)\n    print('\\n \\n')\n    print('-'*30)","8240ee99":"Converting Date stored as object to datetime dtype","62d2df82":"# Next Steps\nRun the models at weekly level starting mid feb and see the best ETS model picked for each country. \nWe can understand at which week of breakout a country is growing by how much","6df62684":"# Skip warnings","05c5f986":"# Global Total Numbers","8b117caf":"# Top country except china model state ","6ef9fee7":"# Forecast country level","ce4a006c":"Creating ky for easy joins ","410d0311":"# Skip for error","dd86014e":"# Accuracy function ","f58bee7b":"# Using Exponential and holt winter methods\n## Series only starts after first case is reported\nCurrently ConfirmedCases and Fatalities are sparately forecasted \n\n[Link to python ets](https:\/\/www.statsmodels.org\/stable\/examples\/notebooks\/generated\/exponential_smoothing.html) \n\n[Details about ets method used ](https:\/\/robjhyndman.com\/uwafiles\/3-ExponentialSmoothing.pdf)","8a50f96f":"# Country level","f1cb7ccf":"# Run prediction"}}