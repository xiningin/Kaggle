{"cell_type":{"71d428fd":"code","f391c12c":"code","8ebb7746":"code","c7c42829":"code","02f919a2":"code","5ae3d306":"code","e1ce3168":"code","976b3a27":"code","6c7a9b13":"code","6c265879":"code","fbe567c6":"code","15787d4d":"code","84fc3e74":"code","e0ffa5be":"code","a7144ebf":"code","3618ec27":"markdown"},"source":{"71d428fd":"# read the data\nimport pandas as pd\ntrain_df = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\ntrain_df.head()","f391c12c":"train_df[\"text\"][79]","8ebb7746":"# Shuffle training dataframe\ntrain_df_shuffled = train_df.sample(frac=1, random_state=42)\ntrain_df_shuffled.head()\n","c7c42829":"# How many classes of each class?\ntrain_df.target.value_counts()\n","02f919a2":"# Let's visualize some random training examples\nimport random\nrandom_index = random.randint(0, len(train_df)-5)\nfor row in train_df_shuffled[[\"text\", \"target\"]][random_index:random_index+5].itertuples():\n  _, text, target = row\n  print(f\"Target: {target}\", \"(real_disaster)\" if target > 0 else \"(not real disaster)\")\n  print(f\"Text:\\n{text}\\n\")\n  print(f\"---\\n\")","5ae3d306":"from sklearn.model_selection import train_test_split\n\n# Use train_test_split to split training data into training and validation sets\ntrain_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled[\"text\"].to_numpy(),\n                                                                            train_df_shuffled[\"target\"].to_numpy(),\n                                                                            test_size=0.1, # dedicate 10% of samples to validation set\n                                                                            random_state=42) # random state for reproducibility","e1ce3168":"# Getting a baseline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\n\n# Create tokenization and modelling pipeline\nmodel_0 = Pipeline([\n                    (\"tfidf\", TfidfVectorizer()), # convert words to numbers using tfidf\n                    (\"clf\", MultinomialNB()) # model the text\n])\n\n# Fit the pipeline to the training data\nmodel_0.fit(train_sentences, train_labels)","976b3a27":"baseline_score = model_0.score(val_sentences, val_labels)\nprint(f\"Our baseline model achieves an accuracy of: {baseline_score*100:.2f}%\")","6c7a9b13":"# Make some predictions\nbaseline_preds = model_0.predict(val_sentences)\nbaseline_preds[:20]","6c265879":"# Function to evaluate: accuracy, precision, recall, f1-score\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\ndef calculate_results(y_true, y_pred):\n  \"\"\"\n  Calculates model accuracy, precision, recall and f1 score of a binary classification model.\n  \"\"\"\n  # Calculate model accuracy\n  model_accuracy = accuracy_score(y_true, y_pred) * 100\n  # Calculate precision, recall an f-score using \"weighted\" average\n  model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n\n  model_results = {\"accuracy\": model_accuracy,\n                   \"precision\": model_precision,\n                   \"recall\": model_recall,\n                   \"f1\": model_f1}\n\n  return model_results","fbe567c6":"# Get baseline results\nbaseline_results = calculate_results(y_true=val_labels,\n                                     y_pred=baseline_preds)\nbaseline_results, baseline_preds.shape","15787d4d":"test_preds = model_0.predict(test_df[\"text\"])","84fc3e74":"submission = pd.DataFrame({\"id\": test_df[\"id\"],\"target\": test_preds})","e0ffa5be":"submission.head()","a7144ebf":"submission.to_csv(\"submission_baseline.csv\", index=False)","3618ec27":"## Baseline model\n\n* To create our baseline, we'll create a Scikit-Learn Pipeline using the TF-IDF (term frequency-inverse document frequency) formula to convert our words to numbers and then model them with the Multinomial Naive Bayes algorithm. "}}