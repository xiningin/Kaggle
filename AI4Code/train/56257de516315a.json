{"cell_type":{"a1f7677e":"code","4653dc1f":"code","eac53464":"code","7efd3b70":"code","d7f18a47":"code","b2afb9db":"code","09a95b6b":"code","49cceac6":"code","da3306e4":"code","d0b92c1a":"code","629079cb":"code","bcb81f49":"code","c77d61af":"markdown","b446ac27":"markdown","2c074508":"markdown","e0d35826":"markdown","292c4c3a":"markdown","35640ac7":"markdown","7ba9b42d":"markdown","a111231a":"markdown","87cdf3a1":"markdown","4856a13c":"markdown","24f0e5f1":"markdown"},"source":{"a1f7677e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport warnings\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.options.mode.chained_assignment = None  # default='warn'\nwarnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4653dc1f":"df = pd.read_csv('\/kaggle\/input\/ultimate-spotify-tracks-db\/SpotifyFeatures.csv')\n\ndf.loc[df.loc[:,'genre']=='Rap','genre'] = 'Hip-Hop'\n\ndf.head()","eac53464":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(context=\"notebook\", palette=\"Spectral\", style = 'darkgrid' ,font_scale = 1.5, color_codes=True)\n\nimport scipy.stats as st\n\ndata = df.loc[:, 'popularity'].values\n\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\n\nkde_xs = np.linspace(min(data), max(data), 100)\nkde = st.gaussian_kde(data)\n\nax.hist(\n    data,\n    alpha=0.2,\n    bins=100,\n    density=True\n)\nax.plot(\n    kde_xs, \n    kde.pdf(kde_xs)\n)\n\nax.axvline(x=60)\n\nplt.xlabel('popularity')\nplt.title('popularity distribution')\nplt.show()\n\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\n\nax.hist(\n    data,\n    alpha=0.2,\n    bins=100,\n    density=True,\n    cumulative=True\n)\nax.axvline(x=60)\n\nplt.xlabel('popularity')\nplt.title('cumulative popularity distribution')\nplt.show()","7efd3b70":"df = df.loc[df.loc[:, 'popularity']>= 60, :]\n\ndf.shape","d7f18a47":"from sklearn.decomposition import PCA\nimport seaborn as sns\n\nnum_components = 2\ncols = ['artist_name','track_name','track_id']\n\ndef prepro(df, cols, num_components):\n\n    dummies = pd.get_dummies(df.loc[:, ~df.columns.isin(cols)])\n    \n    merged = pd.merge(df.loc[:, cols], dummies, left_index=True, right_index=True)\n    \n    merged = merged.groupby(cols).max()\n    \n    genres = merged.loc[:, [i for i in merged.columns if i.startswith('genre')]]\n            \n    x = merged.iloc[:, 11:]\n\n    pca = PCA(n_components=num_components)\n    x_pca = pca.fit_transform(x)\n    \n    merged = merged.iloc[:, :11]\n    \n    for i in range(num_components):\n        merged.loc[:, f'pca_{i+1}'] = x_pca[:, i]\n        \n    for col in merged.columns:\n        mn, mx = min(merged.loc[:, col]), max(merged.loc[:, col])\n        \n        merged.loc[:, col] -= mn\n        merged.loc[:, col] \/= (mx - mn)\n\n    \n    return merged, genres\n    \n    \ntmp, genres = prepro(df, cols, num_components)\n\ntmp = tmp.sort_values(['popularity'], ascending=False)\n\nfor i, col in enumerate(tmp.columns):\n    print(i, col)\n\n    \nsns.pairplot(tmp.iloc[::10,:])\nplt.show()","b2afb9db":"for i in range(1000):\n    print(i, tmp.iloc[i, :].name)","09a95b6b":"# kmeans\n# input: 1 song\n# output: which cluster it came from (charts), and similar songs from the cluster (cosine similarity ranking top 10 similar from the same cluster)\n\nfrom sklearn.cluster import KMeans\n\nnum_clust = 30\n\nwcss = []\n\nfor i in range(1, num_clust+1):\n    print(f'k={i}')\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n    kmeans.fit(tmp)\n    # inertia method returns wcss for that model\n    wcss.append(kmeans.inertia_)\n\nplt.figure(figsize=(10,5))\n\nsns.lineplot(range(1, num_clust+1), wcss,marker='o',color='red')\n\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()","49cceac6":"num_clust = 15\n\nkmeans = KMeans(n_clusters = num_clust, init = 'k-means++', random_state = 42)\ny_kmeans = kmeans.fit_predict(tmp)\n\ntmp.loc[:, 'cluster'] = y_kmeans\n\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\n\nax.hist(\n    y_kmeans,\n    bins=num_clust,\n    density=True,\n    alpha=0.8\n)\n\nplt.title('Distribution by cluster')\nplt.show()","da3306e4":"col_nums = {}\n\nfor i, c in enumerate(tmp.columns[:-1]):\n    col_nums[c] = i\n    \ncol_nums\n\n# kmeans.cluster_centers_.shape (n_clusters, n_features)","d0b92c1a":"# cosine similarity\n\nfrom scipy import spatial\nimport matplotlib.patches as mpatches\nfrom matplotlib import transforms\n\n\ninds = {\n    'index': 20\n}\n\nfor i in inds.values():\n    print(f'{tmp.iloc[i, :].name[0]} - {tmp.iloc[i, :].name[1]}')\n    \npairs = [\n    ['popularity', 'loudness'],\n    ['danceability', 'energy'],\n    ['tempo', 'valence'],\n    ['pca_1', 'pca_2']\n]\n\n    \ndef charts(inds, tmp, genres):\n    \n    plotDf = tmp\n        \n    target_ind = inds['index']\n    target_vals = tmp.iloc[target_ind, :]\n    \n    genres = genres.reindex(tmp.index)\n    \n    tmp_genres = genres.iloc[target_ind, :]\n    \n    genres.loc[:, 'cluster'] = tmp.loc[:, 'cluster']\n\n    clust = tmp['cluster'].iloc[target_ind]\n    tmp = tmp.loc[tmp.loc[:, 'cluster'] == clust, :]\n    \n    tmp.loc[:, 'index'] = 0\n\n    for i in range(len(tmp)):\n        tmp.loc[:, 'index'].iloc[i] = spatial.distance.cosine(target_vals, tmp.iloc[i, :-1])\n\n    print('most similar songs:')\n    head = tmp.sort_values(['index']).head(10)\n    \n    for i in head.index:\n        print(f'    {i[0]} - {i[1]}')\n                \n    genres = genres.loc[genres.loc[:,'cluster']==clust,:].groupby(['cluster']).sum()\n    \n    genres = genres.T\n    \n    genres.loc[:, 'target'] = tmp_genres.T\n    \n    genres.columns = ['cluster', 'target']\n    \n    genres = genres.sort_values(['cluster'], ascending = True)\n    \n    genres.loc[:, 'cluster'] \/= genres.loc[:, 'cluster'].sum()\n    \n    fig = plt.figure()\n    ax = fig.add_axes([0,0,2,2])\n\n    ax.barh(\n        [i.split('_')[1] for i in genres.index],\n        genres.loc[:, 'cluster'],\n        color = ['blue' if i == 1 else 'red' for i in genres.loc[:, 'target']],\n        alpha = 0.5,\n        label=f''\n    )\n\n    plt.title(f'primary genres of cluster {clust}')\n    plt.xlabel(f'share of songs')\n    plt.ylabel(f'')\n    \n    red_patch = mpatches.Patch(color='red', label=f'Other genres')\n    blue_patch = mpatches.Patch(color='blue', label=f'Genre(s) of target song')\n    plt.legend(handles=[blue_patch, red_patch], loc='lower right')\n    plt.show()\n\n\n    for p in pairs:\n                \n        # the random data\n        x = plotDf.loc[plotDf.loc[:, 'cluster']!=clust, p[0]]\n        y = plotDf.loc[plotDf.loc[:, 'cluster']!=clust, p[1]]\n        \n        # definitions for the axes\n        left, width = 0.1, 0.65\n        bottom, height = 0.1, 0.65\n        spacing = 0.005\n\n\n        rect_scatter = [left, bottom, width, height]\n        rect_histx = [left, bottom + height + spacing, width, 0.2]\n        rect_histy = [left + width + spacing, bottom, 0.2, height]\n\n        # start with a rectangular Figure\n        plt.figure(figsize=(12, 8))\n\n        ax_scatter = plt.axes(rect_scatter)\n        ax_scatter.tick_params(direction='in', top=True, right=True)\n        ax_histx = plt.axes(rect_histx)\n        ax_histx.tick_params(direction='in', labelbottom=False)\n        ax_histy = plt.axes(rect_histy)\n        ax_histy.tick_params(direction='in', labelleft=False)\n\n        # the scatter plot:\n        ax_scatter.scatter(x, y,\n            color = 'red',\n            alpha = 0.1,\n            label=f'songs NOT in cluster {clust}'\n        )\n        \n        def get_kde(data):\n        \n            kde_xs = np.linspace(min(data), max(data), 100)\n            kde = st.gaussian_kde(data)\n            \n            return kde_xs, kde\n        \n        y_kde_xs, y_kde = get_kde(y)\n        x_kde_xs, x_kde = get_kde(x)\n\n\n        ax_histx.hist(x, \n                      bins=25, \n                      alpha=0.2, \n                      density=True\n        )\n        ax_histy.hist(y, \n                      bins=25, \n                      alpha=0.2, \n                      density=True, \n                      orientation='horizontal'\n        )\n        \n        base = plt.gca().transData\n        rot = transforms.Affine2D().rotate_deg(90)\n        \n        ax_histx.plot(\n            x_kde_xs, \n            x_kde.pdf(x_kde_xs), color='red'\n        )\n        ax_histy.plot(\n            y_kde_xs, \n            -y_kde.pdf(y_kde_xs), color='red', transform = rot + base\n        )\n        \n        x = tmp.loc[tmp.loc[:, 'cluster']==clust, p[0]]\n        y = tmp.loc[tmp.loc[:, 'cluster']==clust, p[1]]\n        \n        ax_scatter.scatter(x, y, \n            color = 'blue',\n            alpha = 0.5,\n            label=f'songs in cluster {clust}'\n        )        \n        ax_scatter.scatter(\n            kmeans.cluster_centers_[clust, col_nums[p[0]]],\n            kmeans.cluster_centers_[clust, col_nums[p[1]]],\n            color = 'lightblue',\n            edgecolors = 'black',\n            alpha = 0.8,\n            s=400,\n            marker='P',\n            label=f'centroid of cluster {clust}'\n        )\n        ax_scatter.scatter(\n            target_vals[col_nums[p[0]]],\n            target_vals[col_nums[p[1]]],\n            color = 'black',\n            alpha = 0.8,\n            s=400,\n            marker='P',\n            label=f'{head.index[0][1]} - {head.index[0][0]}'\n        )\n        ax_scatter.legend()\n                \n        ax_scatter.axvline(x=target_vals[col_nums[p[0]]], color='black', linestyle='dashed')\n        ax_scatter.axhline(y=target_vals[col_nums[p[1]]], color='black', linestyle='dashed')\n\n        y_kde_xs, y_kde = get_kde(y)\n        x_kde_xs, x_kde = get_kde(x)\n\n        ax_histx.hist(x, bins=25, alpha=0.2, density=True, color='blue')\n        ax_histy.hist(y, bins=25, alpha=0.2, density=True, color='blue', orientation='horizontal')\n        \n        ax_histx.plot(\n            x_kde_xs, \n            x_kde.pdf(x_kde_xs), color='blue'\n        )\n        ax_histy.plot(\n            y_kde_xs, \n            -y_kde.pdf(y_kde_xs), color='blue', transform = rot + base\n        )\n        \n        ax_histx.axes.xaxis.set_visible(False)\n        ax_histx.axes.yaxis.set_visible(False)\n        ax_histy.axes.xaxis.set_visible(False)\n        ax_histy.axes.yaxis.set_visible(False)\n        \n\n        ax_histx.set_title(f'x:{p[0]} - y:{p[1]}')\n        ax_scatter.set_xlabel(f'x:{p[0]}')\n        ax_scatter.set_ylabel(f'y:{p[1]}')\n\n        plt.show()\n\n    \ncharts(inds, tmp, genres)","629079cb":"inds = {\n    'index': 370\n}\n\nfor i in inds.values():\n    print(f'{tmp.iloc[i, :].name[0]} - {tmp.iloc[i, :].name[1]}')\n    \n    \ncharts(inds, tmp, genres)","bcb81f49":"inds = {\n    'index': 877\n}\n\nfor i in inds.values():\n    print(f'{tmp.iloc[i, :].name[0]} - {tmp.iloc[i, :].name[1]}')\n    \n\n    \ncharts(inds, tmp, genres)","c77d61af":"## 3.1 Creating a playlist around Travis Scott's SICKO MODE\n\n![](https:\/\/i.ebayimg.com\/images\/g\/qhUAAOSwOvRbYY8D\/s-l300.jpg)","b446ac27":"# 3. Cosine similarity recommendation\n\n### Below is how we define cosine similarity. Lower value = more similar.\n\n![](https:\/\/neo4j.com\/docs\/graph-algorithms\/current\/images\/cosine-similarity.png)\n\n### Recommendation logic is simple: in a loop, check similarity score between the input song and every other song in that cluster. Then rank output in ascending order by similarity value.","2c074508":"# 0. Purpose \/ Method\n\n### The purpose of this notebook is to showcase a potential content-based Spotify playlist generator, using cosine similarity and K-Means clustering. \n\n### Input - 1 song; Output - N most similar songs (ranked)\n\n### Bulk of the recommendation is done by checking cosine similarity between the input song and every other song.\n\n### Finding: While K-Means is superfluous in terms of the recommendations themselves, it does help speed up the recommendation process significantly. By limiting the cos. similarity checking to only songs from the input song's cluster, you get identical recommendations but with faster computation.","e0d35826":"# 1. EDA \/ Feature selection","292c4c3a":"### To utilize categorical values such as genre, mode, key and time signature, we will combine them into 2 continuous variables using PCA.","35640ac7":"# 2. Clustering\n\n### We are going to use K-Means clustering, on all parameters previously outlined.","7ba9b42d":"# 4. Conclusion\n\n### Cosine similarity works reasonably well in identifying similar songs, that adds value beyond simple genre filtering. In the real world, this combined with a collaborative filtering method could produce a reasonable recommendation system for songs. ","a111231a":"### We choose k = 15, it seems a good approximation of optimal # of clusters.","87cdf3a1":"## 3.2 Creating a playlist around Radiohead's Creep\n\n![](https:\/\/upload.wikimedia.org\/wikipedia\/en\/9\/9d\/Radiohead_original_creep_cover.jpg)","4856a13c":"## 3.3 Creating a playlist around John Lennon's Imagine\n\n![](https:\/\/upload.wikimedia.org\/wikipedia\/en\/6\/69\/ImagineCover.jpg)","24f0e5f1":"### We are only using songs with popularity >= 60, to speed up computation and increase relevance."}}