{"cell_type":{"76235453":"code","38516363":"code","cadce087":"code","d1827641":"code","b5d68e6c":"code","ad0ff9f3":"code","59e874ef":"code","e293819d":"code","787d8cc1":"code","9cbd38a0":"code","dc91a74b":"code","1858ebbc":"code","af01368b":"code","424a59db":"code","e441c4bc":"code","a83ddf8c":"code","d6162f91":"code","b8641228":"code","4107baaa":"code","2714100a":"code","5d2408f7":"code","d9f8ec67":"code","ae92496f":"code","42a4801e":"code","c028d2be":"code","c63e400b":"code","6d2c27bc":"code","4baff7fa":"code","5b6de00b":"code","a765e63d":"code","1735ec49":"code","f4ef6556":"code","647905e2":"code","016e59a8":"code","19aa944d":"code","e6a2c704":"code","4e1a111e":"code","17cba576":"code","405e9f6e":"code","f4b03bd1":"code","1bf383b7":"code","58bf03b4":"code","31fd2244":"code","d319ae18":"code","1aa1031d":"code","fa639a11":"code","01f2f2ae":"code","5ced3192":"markdown","cf6776c7":"markdown","1e9f3d55":"markdown","7db26edc":"markdown","e0f98bab":"markdown","f8c7a1ba":"markdown","6da06431":"markdown","f2b90fc1":"markdown","bc9c2ff6":"markdown","efc95fad":"markdown","ece9f869":"markdown","db8b4bd7":"markdown","1593620a":"markdown","f85f05d2":"markdown","cb080669":"markdown","59deff42":"markdown","5d83edf7":"markdown","bec1a136":"markdown","b3c7be54":"markdown","dd5dd87a":"markdown","cb141597":"markdown","103dd561":"markdown","b8403e60":"markdown"},"source":{"76235453":"from pandas.tseries.holiday import *\n\nimport math\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n!pip install plotly\n!pip install shap\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport seaborn as sns\nimport shap\nshap.initjs()\n\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.preprocessing import OrdinalEncoder, StandardScaler\n\nfrom sklearn.model_selection import cross_val_predict, train_test_split, KFold\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, BaggingRegressor, GradientBoostingRegressor\n!pip install xgboost\n!pip install lightgbm\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\n","38516363":"def merge_data(df):\n\n    df = df.merge(df_stores, 'left', 'Store')\n    \n    df = df.merge(df_features, 'left', ['Store', 'Date', 'IsHoliday'])\n    \n    df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')\n\n    return df","cadce087":"# Paths\n\npath_project = '..\/input\/walmart-recruiting-store-sales-forecasting\/'\n\npath_train = path_project + 'train.csv.zip'\n\npath_test = path_project + 'test.csv.zip'\n\npath_stores = path_project + 'stores.csv'\n\npath_features = path_project + 'features.csv.zip'\n\npath_sample_submission = path_project + 'sampleSubmission.csv.zip'","d1827641":"df_train = pd.read_csv(path_train)\n\ndf_test = pd.read_csv(path_test)\n\ndf_stores = pd.read_csv(path_stores)\n\ndf_features = pd.read_csv(path_features)\n\ndf_submission = pd.read_csv(path_sample_submission)","b5d68e6c":"df_train.head(1)","ad0ff9f3":"df_train.describe()","59e874ef":"df_train = merge_data(df_train)\n\ndf_test = merge_data(df_test)","e293819d":"df_train.head(1)","787d8cc1":"def get_percent_null(df):\n    '''\n    Esta fun\u00e7\u00e3o calcula a porcentagem de valores Null em dataframe\n    '''\n    return (\n        \n        (df.isnull().sum()\/len(df) * 100).sort_values(ascending=False)\n        \n    ).to_frame(name='percent_null')","9cbd38a0":"def plot_heatmap(df, title=''):\n    '''\n    Esta fun\u00e7\u00e3o faz o plot heatmap do seanborn\n    '''\n    corr = df.corr()\n    mask = np.zeros_like(corr)\n    mask[np.triu_indices_from(mask)] = True\n    with sns.axes_style(\"white\"):\n        f, ax = plt.subplots(figsize=(15, 15))\n        ax.set_title(title)\n        ax = sns.heatmap(corr, mask=mask, annot=True, vmax=.3, center=0, cmap=sns.color_palette(\"vlag\", as_cmap=True))","dc91a74b":"def plot_two_col(df, col_x='Type', col_y = 'count', type_plot='Bar', mode='markers'):\n\n    '''\n    Esta fun\u00e7\u00e3o faz o plot de duas vari\u00e1veis, onde: \n    - col_x: Representa vari\u00e1vel que assumiram o eixo x em cada um dos subplots\n    - col_y: Representa a vari\u00e1vel que assumira o eixo y nos dois subplots\n    - type_plot: Tipo do plot, apenas s\u00e3o consideradas duas op\u00e7\u00f5es 'Bar' ou 'Scatter'\n    - mode: Apenas utilizado quando type_plot \u00e9 'Scatter'\n    '''\n    \n    data = None\n    \n    if type_plot == 'Bar': \n            \n        if col_y is None: \n            col_y = 'count'\n\n        df_ = df.groupby(by=[col_x]).size().reset_index(name='count').copy()  \n        data = [go.Bar(x=df_[col_x], \n                       y=df_[col_y], \n                       marker=dict(color='#197278')\n                )]\n\n    elif type_plot == 'Scatter':\n\n        if col_y is None: \n            col_y = 'Weekly_Sales'\n\n        df = df_train.groupby(by=[col_x], as_index=False)[col_y].mean()\n        data = [go.Scatter(x=df[col_x], \n                           y=df[col_y], \n                           marker=dict(color='#197278'), \n                           mode=mode)]\n    else: \n\n        raise ValueError('Apenas duas op\u00e7\u00f5es s\u00e3o aceitas no par\u00e2metro type_plot: Bar ou Sacetter.')\n    \n    fig = go.Figure(data=data)\n\n    fig.update_layout(plot_bgcolor=\"white\",showlegend= False, \n                      xaxis_title=col_x, yaxis_title=col_y, title=f'{col_x} vs {col_y}')\n    fig.update_yaxes(showline=True, gridcolor='#dadae8')\n    \n    fig.show()","1858ebbc":"def plot_scatter_per_year(df, col_x='week', col_y='Weekly_Sales', filter_month=None):\n\n    '''\n    Esta fun\u00e7\u00e3o faz o plot Scatter sobre duas vari\u00e1veis, em cada ano, onde:\n    - df: Dataframe\n    - col_x: Coluna que ficar\u00e1 no eixo x\n    - col_y: Coluna que ficar\u00e1 no eixo y\n    - filter_month: Lista com o n\u00famero dos meses que se deseja filtrar\n    '''\n\n    df = df[['Date', 'Weekly_Sales']].copy()\n    df['week'] = df.Date.dt.isocalendar().week.astype('int64')\n    df['year'] = df.Date.dt.year\n    df['month'] = df.Date.dt.month\n\n    fig = go.Figure()\n\n    colors = ['#C17767', '#197278', '#512D38']\n\n    title = f'{col_y} per mean {col_x}'\n\n    if not filter_month is None:\n        df = df[df['month'].isin(filter_month)]\n        title += f' and only month: {filter_month}' \n    \n    for year, color in zip(df['year'].unique().tolist(), colors):\n\n        \n\n        weekly_sales_year = df[df['year'] == year].groupby(col_x)[col_y].mean()\n\n        fig.add_trace(\n            \n            go.Scatter(x=weekly_sales_year.index, y=weekly_sales_year.values,\n                          mode='lines',\n                          name= f'year {year}',\n                          line=dict(color=color))\n        )\n\n    fig.update_layout(title=title, plot_bgcolor=\"white\", yaxis_title=col_y, xaxis_title=col_x)\n\n    fig.update_xaxes(showline=True, gridcolor='#dadae8')\n    fig.update_yaxes(showline=True, gridcolor='#dadae8')\n\n    fig.show()","af01368b":"get_percent_null(df_train)","424a59db":"get_percent_null(df_test)","e441c4bc":"plot_heatmap(df_train, 'Dataframe Train')","a83ddf8c":"for col_x in ['Type', 'IsHoliday']:\n    \n    plot_two_col(df_train, col_x=col_x, col_y='count', type_plot='Bar')","d6162f91":"for col_x in ['Dept', 'Size']:\n    \n    plot_two_col(df_train, col_x=col_x, col_y='Weekly_Sales', type_plot='Scatter', mode='markers')","b8641228":"plot_scatter_per_year(df_train, col_x='week', col_y='Weekly_Sales')","4107baaa":"plot_scatter_per_year(df_train, col_x='week', col_y='Weekly_Sales', filter_month=[5, 6, 7, 8])","2714100a":"def get_df_holidays(df):\n    '''\n    Fun\u00e7\u00e3o que retorna um dataframe com as semanas de feriados do dataframe de treino\n    + os novos feriados\n    '''\n    holiday_list = [\n          '2010-02-12', '2011-02-11', '2012-02-10', '2013-02-08',\n          '2010-09-10','2011-09-09', '2012-09-07', '2013-09-06',\n          '2010-11-26', '2011-11-25', '2012-11-23', '2013-11-29',\n          '2010-12-31', '2011-12-30', '2012-12-28', '2013-12-27'\n    ]\n\n    df_holiday = pd.DataFrame(zip(holiday_list, [True] * len(holiday_list)), \n                              columns=['day', 'is_holiday'])\n\n    df_holiday['day'] = pd.to_datetime(df_holiday['day'], format='%Y-%m-%d')\n\n    df['year'] = df.Date.dt.year\n    df['week'] = df.Date.dt.isocalendar().week.astype('int64')\n\n    list_new_holidays = [\n                      \n                    # P\u00e1scoa \n                    (2010, 13), (2011, 16), (2012, 14), (2013, 13),\n\n                    # Dia da Independ\u00eancia Americana (04\/07)\n                    (2010, 26), (2011, 26), (2012, 27), (2013, 27),\n    ] \n\n    for year, week in list_new_holidays:\n        df.loc[((df.year == year) & (df.week == week)), 'is_holiday'] = True\n\n    df = df[df['is_holiday'] == True][['Date', 'is_holiday']]\n\n    df.columns = ['day', 'is_holiday']\n\n    return pd.concat([df_holiday, df], ignore_index=True).drop_duplicates().reset_index(drop=True) ","5d2408f7":"def count_holidays_per_key(df, keys, col_name):\n    '''\n    Fun\u00e7\u00e3o que calcula a quantidade de feriados em rela\u00e7\u00e3o a alguma outra feature.\n    Ex: Quantidade de feriados por m\u00eas, ou por semana\n    '''\n    df_grouped =  df.groupby(keys, as_index=False)['is_holiday'].count()\n\n    df_grouped.columns = keys + [col_name]\n    \n    return (\n    \n        df.merge(\n\n                df_grouped, how='left', on=keys\n\n            )\n    \n    ).fillna(0)","d9f8ec67":"def create_features_holidays(df, df_holidays, start, end):\n\n    '''\n    Fun\u00e7\u00e3o que cria features booleanas que informam se a semana \u00e9 do \n    - in\u00edcio do m\u00eas \n    - quizena do m\u00eas\n    - final do m\u00eas\n    '''\n        \n    all_days = pd.date_range(start, end, freq='D').to_series()\n    \n    df_all_days = pd.DataFrame(all_days, columns=['day'])\n\n    df_all_days.loc[\n        \n            df_all_days.day.dt.strftime('%d') == '15', 'is_fortnight'\n\n    ] = True\n\n    df_all_days['is_fortnight'] = df_all_days['is_fortnight'].fillna(value=False)\n    \n    df_all_days['year'] = df_all_days.day.dt.year\n    \n    df_all_days['week'] = df_all_days.day.dt.isocalendar().week.astype('int64')\n    \n    df_all_days['week_day'] = df_all_days.day.dt.dayofweek\n    \n    df_all_days['is_month_start'] = df_all_days.day.dt.is_month_start\n    \n    df_all_days['is_month_end'] = df_all_days.day.dt.is_month_end\n\n    df_all_days = df_all_days.merge(df_holidays, 'left', 'day')\n    \n    df_all_days['is_holiday'] = df_all_days['is_holiday'].fillna(value=False)\n    \n    df_week_year = df_all_days.groupby(['week', 'year'], as_index=False)\n\n    df_week_year = df_week_year.any()\n    \n    df_week_year = df_week_year[[\n    \n        'week', 'year', 'is_month_start', 'is_fortnight', \n        'is_month_end', 'is_holiday'\n    ]]\n    \n    df_week_year.columns = [\n    \n        'week', 'year', 'is_week_start_month', 'is_week_fortnight', \n        'is_week_end_month', 'is_holiday'\n    ]\n\n    return df.merge(\n        \n        df_week_year, 'left', ['week', 'year']\n\n    ).fillna(value=False)    ","ae92496f":"def create_features(df):\n\n    '''\n    Fun\u00e7\u00e3o que criar as features adicionadas ao dataframe de treinamento:\n    - year\n    - month\n    - week\n    - 'is_week_start_month'\n    - 'is_week_fortnight' \n    - 'is_week_end_month'\n    - 'is_holiday'\n    - 'qt_holiday_week'\n    '''\n       \n    start = pd.Timestamp(df.Date.dt.date.min()).to_pydatetime()\n    \n    end = pd.Timestamp(df.Date.dt.date.max()).to_pydatetime()\n    \n    df_holidays = get_df_holidays(df.copy())\n\n    df['year'] = df.Date.dt.year\n    \n    df['week'] = df.Date.dt.isocalendar().week.astype('int64')\n    \n    df['month'] = df.Date.dt.month\n    \n    df = create_features_holidays(df.copy(), df_holidays, start, end)\n\n    df = count_holidays_per_key(df.copy(), ['year', 'week'], 'qt_holiday_week')\n\n    return df.drop_duplicates().reset_index(drop=True)","42a4801e":"def convert_to_numeric(df):\n    '''\n    Fun\u00e7\u00e3o que converte todas as colunas pra float e preenche valores nan.\n    '''\n\n    list_col_numeric = [col for col in df if df[col].dtype != 'O']\n\n    list_col_cat = [col for col in df if df[col].dtype == 'O']\n\n    for col in df.columns:\n\n        if col in list_col_numeric:\n            \n            df[col] = df[col].fillna(-9999).astype('float64')\n            \n        elif col in list_col_cat:\n            \n            df[col] = df[col].fillna('ND')\n\n    return df","c028d2be":"def prepare_dataframe(df, is_test = False):\n\n    '''\n    Fun\u00e7\u00e3o que chama a cria\u00e7\u00e3o de feature, seleciona as features usadas no treinamento\n    '''\n    \n    df = create_features(df)\n    \n    col_selected = [\n    \n           'Store', 'Dept', 'Type', 'Size', 'is_holiday',\n           'year', 'week', 'is_week_start_month', 'month',\n           'is_week_fortnight', 'is_week_end_month',\n           'qt_holiday_week', 'Weekly_Sales'\n    ]\n    type_df = 'df_train'\n\n    if is_test:\n        \n        col_selected.remove('Weekly_Sales')\n        type_df = 'df_test'\n\n    plot_heatmap(df, f'{type_df} After add all the features')\n\n    df = df[col_selected]\n    \n    df = convert_to_numeric(df) \n\n    plot_heatmap(df, f'{type_df} After removing some features')\n\n    return df","c63e400b":"def get_make_col_transformer(df):\n\n    '''\n    Fun\u00e7\u00e3o que cria um pipeline, onde definimos a estrat\u00e9gia de transforma\u00e7\u00e3o das colunas \n    categ\u00f3ricas e a forma de normaliza\u00e7\u00e3o que ser\u00e1 utilizada\n    '''\n\n    list_col_numeric = [col for col in df if df[col].dtype != 'O']\n\n    list_col_cat = [col for col in df if df[col].dtype == 'O']\n\n    if 'Weekly_Sales' in list_col_numeric: list_col_numeric.remove('Weekly_Sales')\n\n    list_categories = [df[column].unique() for column in df[list_col_cat]]\n\n    encoder_col_cat = make_pipeline(\n        OrdinalEncoder(categories=list_categories)\n    )\n\n    normalize_col_numerics = make_pipeline(\n        StandardScaler()\n    )\n    \n    return make_column_transformer(\n        \n        (encoder_col_cat, list_col_cat),\n        (normalize_col_numerics, list_col_numeric)\n        \n    )","6d2c27bc":"df_train  = prepare_dataframe(df_train)\n\ndf_test = prepare_dataframe(df_test, is_test=True)\n\ntransform_columns = get_make_col_transformer(df_train)","4baff7fa":"X = df_train.drop(['Weekly_Sales'], axis=1)\ny = df_train['Weekly_Sales']","5b6de00b":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","a765e63d":"def WMAE(dataset, real, predicted, col_target='is_holiday'):\n    \"\"\"\n    M\u00e9trica da competi\u00e7\u00e3o\n    \"\"\"\n    weights = dataset[col_target].apply(lambda x: 5 if x else 1)\n    return np.round(np.sum(weights*abs(real-predicted))\/(np.sum(weights)), 2)","1735ec49":"models = {\n\n    'LinearRegression': LinearRegression(),\n    \n    'XGBRegressor': XGBRegressor(),\n    \n    'RandomForestRegressor': RandomForestRegressor(verbose = False, n_jobs = -1),\n       \n    'LGBMRegressor': LGBMRegressor(),\n    \n    'ExtraTreesRegressor' : ExtraTreesRegressor(verbose = False, n_jobs = -1),\n    \n    'BaggingRegressor' : BaggingRegressor(verbose = False, n_jobs = -1),\n    \n    'GradientBoostingRegressor': GradientBoostingRegressor()\n}\n\nbest_nome_model = None\nbest_model = None\nbest_error = math.inf\nlist_metrics = []","f4ef6556":"for name in models:\n  \n    print(f'\\n\\n\\n{name}')\n    regressor = models[name]\n                       \n    training_pipeline = make_pipeline(\n        transform_columns,\n        regressor\n    )\n\n    kfold = KFold(n_splits=3)\n    \n    y_pred = cross_val_predict(\n        training_pipeline,\n        X_train,\n        y_train,\n        cv = kfold,\n    )\n    \n    wmae_metric = WMAE(X_train, y_train, y_pred)\n    \n    print(f'\\nModel {name} Metrica WMAE', wmae_metric)\n    \n    list_metrics.append((name, wmae_metric))\n    \n    if wmae_metric < best_error:\n        best_error = wmae_metric\n        best_model = regressor\n        best_nome_model = name","647905e2":"list_metrics","016e59a8":"best_model","19aa944d":"best_error","e6a2c704":"def find_best_paramns(best_model, df_train, sample_train=0.5, param_grid=None, \n                      type_search='randomized', n_iter=None):\n    \n    '''\n    Fun\u00e7\u00e3o que encontra os melhores par\u00e2metros para o melhor modelo\n    \n    best_model: Inst\u00e2ncia do melhor modelo encontrado\n    df_train: Dataframe de treinamento\n    sample_train: Sample utilizada para fazer a otimiza\u00e7\u00e3o de par\u00e2metros, se 1 considera todo o dataframe\n    param_grid: Dict com os par\u00e2metros e valores a serem testados\n    type_search: Tipo da busca otimizada, pode ser: 'grid' ou 'randomized' \n    '''\n    \n    df_sample = df_train.sample(frac=sample_train)\n    \n    X, y = df_sample.drop(['Weekly_Sales'], axis=1), df_sample['Weekly_Sales']\n\n    X_train, _, y_train, _ = train_test_split(X, y, test_size=0.3)\n\n    sample_train_pipeline = Pipeline([\n        (\"transform_columns\", transform_columns),\n        (\"model\", best_model)\n    ])\n\n    kfold = KFold(n_splits=3)\n    \n    if param_grid is None:\n      \n      param_grid = {\n          \n          #'model__bootstrap': [True],\n          #'model__max_features': ['auto', 'log2'],\n          #'model__min_samples_split': [2, 3],\n          'model__max_depth': [None, 30],\n          'model__n_estimators': [60, 150, 200]\n\n      }\n\n    if type_search == 'randomized':\n        \n        model_search = RandomizedSearchCV(\n            sample_train_pipeline,\n            param_distributions = param_grid,\n            cv = kfold,\n            n_iter=n_iter,\n            n_jobs = -1\n        )\n        \n    elif type_search == 'grid':\n        \n        model_search = GridSearchCV(\n            sample_train_pipeline,\n            param_grid = param_grid,\n            cv = kfold,\n            n_jobs = -1\n        )\n    \n    else:\n        \n        raise ValueError('Apenas duas op\u00e7\u00f5es s\u00e3o aceitas no par\u00e2metro type_search: randomized ou grid.')\n\n    return model_search.fit(X_train, y_train).best_params_","4e1a111e":"best_params = find_best_paramns(best_model, df_train, sample_train=1, type_search='grid')","17cba576":"best_params","405e9f6e":"paramns_name = []\n\nfor key, value in best_params.items():\n    paramns_name.append(key.replace('model__', ''))\n\nbest_params_ = dict(zip(paramns_name, best_params.values()))","f4b03bd1":"best_params_","1bf383b7":"def fit_best_model(best_model, X_train, y_train, X_test, y_test):\n\n    model = best_model.set_params(**best_params_)\n\n    training_pipeline = Pipeline([\n              (\"transform_columns\", transform_columns),\n              (\"model\", model)\n    ])\n\n    training_pipeline.fit(X_train, y_train)\n\n    y_pred = training_pipeline.predict(X_test)\n\n    wmae_metric = WMAE(X_test, y_test, y_pred)\n\n    print(f'\\nBest Model WMAE: ', wmae_metric)\n\n    return training_pipeline, wmae_metric","58bf03b4":"best_model_, wmae_metric = fit_best_model(best_model, X_train, y_train, X_test, y_test)","31fd2244":"def plot_feature_importance(feature_importance, columns):\n    '''\n    Fun\u00e7\u00e3o que plota o feature importance do melhor modelo\n    '''\n    df_importance = pd.DataFrame(\n    \n        zip(columns, feature_importance), \n        columns=['Feature', 'Importance']\n\n    ).sort_values(by=['Importance'])\n\n    fig = go.Figure(data=[\n        go.Bar(y=df_importance['Feature'], \n               x=df_importance['Importance'], \n               marker=dict(color='#197278'),\n               orientation='h'\n              )\n    ])\n\n    fig.update_layout(plot_bgcolor=\"white\",showlegend= False, title='Feature Importance', \n                      xaxis_title='Importance', yaxis_title='Feature')\n    fig.update_yaxes(showline=True, gridcolor='#dadae8')\n\n    fig.show()","d319ae18":"plot_feature_importance(best_model_.steps[1][1].feature_importances_, X_train.columns)","1aa1031d":"y_pred = best_model_.predict(df_test)\n\ndf_test['Weekly_Sales'] = y_pred","fa639a11":"df_submission['Weekly_Sales'] = y_pred\n\ndf_submission.to_csv('submission.csv',index=False)","01f2f2ae":"df_submission","5ced3192":"Vamos observar tamb\u00e9m a rela\u00e7\u00e3o entre as vari\u00e1veis Dept e Size com nossa vari\u00e1vel resposta.","cf6776c7":"# 8. Pr\u00f3ximos passos","1e9f3d55":"1. Aumentar o n\u00famero de par\u00e2metros na otimiza\u00e7\u00e3o de par\u00e2metros\n2. Refinar o processo de engenharia de features, tentar encontrar mais features que expliquem as varia\u00e7\u00f5es de vendas entre as semanas\n3. Tentar utilizar outros algoritmos\n","7db26edc":"### Features Criadas\n\n- year\n- month\n- week\n- is_week_start_month\n- is_week_fortnight\n- is_week_end_month\n- is_holiday\n- qt_holiday_week","e0f98bab":"Treinando com os melhores par\u00e2metros","f8c7a1ba":"## Adicionando feriados\n\n> Como observado pelo participante [Avelino Caio](https:\/\/www.kaggle.com\/avelinocaio\/walmart-store-sales-forecasting), que notou que o pico das semanas 14 e 13 poderiam estar associadas ao feriado de p\u00e1scoa (feriado n\u00e3o mapeado no nosso dataframe de treino), nos traz a reflex\u00e3o de que outros feriado podem influenciar no aumentar das vendas tamb\u00e9m e que n\u00e3o est\u00e3o mapeados no nosso dataframe de treino. \n\nA patir de uma busca r\u00e1pida na internet [Public holidays in  the United States](https:\/\/en.wikipedia.org\/wiki\/Public_holidays_in_the_United_States), podemos encontrar outros feriados que poderiam influ\u00eanciar tamb\u00e9m nas vendas dos departamentos. Um ponto de refer\u00eancia para decidirmos se um feriado pode ou n\u00e3o ser relevante para adicionar no nosso problema, seria analisando o impacto financeiro desse feriados no pa\u00eds. Em [https:\/\/nrf.com\/insights\/retail-holiday-and-seasonal-trends](https:\/\/nrf.com\/insights\/retail-holiday-and-seasonal-trends) encontramos a informa\u00e7\u00e3o de vendas em bilh\u00f5es de dolares americanos nos feriados. \n\n> \u00c9 importante mencionar tamb\u00e9m que n\u00e3o foi fornecida as informa\u00e7\u00f5es da localiza\u00e7\u00e3o das lojas e departamentos do nosso dataframe de treino, apenas sabemos que s\u00e3o lojas da Walmart, uma empresa american multinacional. Por sabermos que a sede da empresa se localiza nos EUA, estou tomando como base o calend\u00e1rio nacional de feriados de l\u00e1. \n\nUm outro ponto importante que nos ajuda a escolher esses feriados adicionais, \u00e9 **observar os picos do gr\u00e1fico anterior e observar se as semanas de pico est\u00e3o relacionadas a algum feriado**. Seguindo essa linha encontramos um pico entre as semanas 26 e 27, mesma semana que ocorre o dia da independ\u00eancia americana (4 de julho). \n\n> Bom, com essas observa\u00e7\u00f5es vamos incluir os feriados\n> - **P\u00e1scoa** \n> - **Dia da independ\u00eancia Americana**","6da06431":"# 4. Buscando o melhor algoritmo\n\nAgora que definimos nossas features, precisamos encontrar o melhor algoritmo para fazermos a regress\u00e3o.","f2b90fc1":"# 2. An\u00e1lise Explorat\u00f3ria dos dados","bc9c2ff6":"# 6. Testes","efc95fad":"### Investigando valores Nan\n> df_train: Apenas as colunas de Markdown possuem valores Nan no dataframe de train\n\n> df_test: Al\u00e9m das colunas que possuem prefixo markdown, as colunas Unemployment e CPI tamb\u00e9m possuem valores Nan\n","ece9f869":"Com isso, conclu\u00edmos que alguns depatamentos possuem faturamento muito superior aos outros, e que quanto maior a vari\u00e1vel size tamb\u00e9m alcan\u00e7amos um maior valor de vendas semanais.","db8b4bd7":"Como descrito na Se\u00e7\u00e3o de m\u00e9tricas desse desafio\u00b9, utilizaremos a m\u00e9trica customizada WMAE, onde penalizamos mais os erros em feriados, visto que semanas com feriado possuem peso 5 vezes maior.\n\nCom isso percebemos que as informa\u00e7\u00f5es relacionadas aos feriados s\u00e3o muito importantes para o nosso modelo, vamos observar a representatividade dessa vari\u00e1vel na nossa base de treino. Um ponto importante \u00e9 que nem todos os feriados foram adicionados na base de treino, apenas os principais:\n\n- Super Bowl: 12-Feb-10, 11-Feb-11, 10-Feb-12, 8-Feb-13\n- Labor Day: 10-Sep-10, 9-Sep-11, 7-Sep-12, 6-Sep-13\n- Thanksgiving: 26-Nov-10, 25-Nov-11, 23-Nov-12, 29-Nov-13\n- Christmas: 31-Dec-10, 30-Dec-11, 28-Dec-12, 27-Dec-13\n\nTamb\u00e9m vamos observar a representatividade e categorias que existem na coluna Type.\n\n\u00b9https:\/\/www.kaggle.com\/c\/walmart-recruiting-store-sales-forecasting\/overview\/evaluation","1593620a":"Logo, vamos precisar desenvolver alguma estrat\u00e9gia para lidar com os valores Nan, temos algumas op\u00e7\u00f5es:\n\n1. Remover as colunas que possuem algum, ou todos os valores Nan\n2. Utilizar alguma estrat\u00e9gia de imputa\u00e7\u00e3o\n\nPara as colunas que possuem o prefixo markdown, no dataframe de treino observamos que elas possuem mais da das linhas com valores Nan, utilizar estrat\u00e9gia de imputa\u00e7\u00e3o de valores em colunas com baix\u00edssimo preenchimento \u00e9 muito arriscado, portanto, vamos excluir elas. Para as colunas Unemployment e CPI, vamos observar a correla\u00e7\u00e3o delas, para decidirmos se vale apena manter elas.","f85f05d2":"Analisando a matriz de correla\u00e7\u00e3o acima, observamos que as colunas Unemployment e CPI\npossuem baixa correla\u00e7\u00e3o com nossa vari\u00e1vel resposta Weekly_Sales. Observamos tamb\u00e9m que as colunas, Fuel_Price, Temperature apresentam baixa correla\u00e7\u00e3o com a vari\u00e1vel resposta. As vari\u00e1veis que possuem alguma correla\u00e7\u00e3o positiva com a vari\u00e1vel resposta s\u00e3o: Dept, Size e IsHoliday. \n\n> Logo, por enquanto manteremos apenas as colunas: \n> 1. Store\n> 2. Dept\n> 3. Size\n> 4. Type (categorical)\n> 5. IsHoliday","cb080669":"# 2. Engenharia de Features","59deff42":"Das vari\u00e1veis que foram criadas e tiveram algum impacto significativo, foram apenas:\n\n- week\n- qt_holiday_week\n- month\n- is_week_end_month\n- year\n","5d83edf7":"Buscando os melhores par\u00e2metros do melhor modelo ","bec1a136":"### Picos de vendas que n\u00e3o s\u00e3o feriados\n\n> Um comportamento de senso comum observado aqui no Brasil, \u00e9 o aumento do fluxo de pessoas em supermercados, lojas de conveni\u00eancia, shoppings, entre outros, no **come\u00e7o do m\u00eas devido ao pagamento do sal\u00e1rio mensal das pessoas**. \n\nIsso me levou a pensar se esse comportamento poderia acontecer em outros pa\u00edsese tamb\u00e9m. Utilizando como base os EUA, e uma r\u00e1pida busca na internet [https:\/\/www.patriotsoftware.com\/blog\/payroll\/pay-frequency-requirements-state-federal\/](https:\/\/www.patriotsoftware.com\/blog\/payroll\/pay-frequency-requirements-state-federal\/), observamos que a freq\u00eancia de pagamento dos funcion\u00e1rios \u00e9 bem variada, mensal, quizenal, entre outros. \n\nAbaixo observamos o nosso gr\u00e1fico anterior com um zoom nos meses Maio, Junho, Julho e Agosto, e vamos tentar observar algum padr\u00e3o.","b3c7be54":"As semans 22, 26, 27 e 31, que representam pico, s\u00e3o semanas do come\u00e7o do m\u00eas. As semanas 30 e 20, representam o final do m\u00eas.","dd5dd87a":"Agora vamos observar como nossa vari\u00e1vel resposta se comporta ao longo das semanas em cada ano.","cb141597":"# 7. Submission","103dd561":"# 1.  Leitura dos Dados","b8403e60":"# **Walmart - Store Sales Forecasting**\n\nA competi\u00e7\u00e3o Walmart - Store Sales Forecasting tem como principal objetivo utilizar dados hist\u00f3ricos de vendas de 45 lojas do Walmart localizadas em diferentes regi\u00f5es para fazer predi\u00e7\u00f5es de vendas por departamento, loja e semana observada.\n\nUm ponto relevante neste estudo, s\u00e3o os eventos promocionais que a Walmart realiza antes dos feriados. Os principais feriados destacados na competi\u00e7\u00e3o s\u00e3o: \n\n- Super Bowl: 12-Fev-10, 11-Fev-11, 10-Fev-12, 8-Fev-13\n- Dia do Trabalho: 10-Set-10, 9-Set-11, 7-Set-12, 6-Set -13\n- A\u00e7\u00e3o de Gra\u00e7as: 26-Nov-10, 25-Nov-11, 23-Nov-12, 29-Nov-13\n- Natal: 31-Dez-10, 30-Dez-11, 28-Dez-12, 27-Dez -13\n\n\n### Solu\u00e7\u00e3o\n\nA formula\u00e7\u00e3o do problema se caracteriza como um problema de s\u00e9rie temporal. Como estamos tentando fazer predi\u00e7\u00f5es de valores num\u00e9ricos, como por exemplo: valor das vendas, pre\u00e7os, um volume, e assim por diante, temos uma modelagem preditiva do tipo regress\u00e3o. Neste desafio nossa vari\u00e1vel resposta \u00e9 representada pela coluna Weekly_Sales.\n\n\nEste notebook est\u00e1 dividido nas seguintes se\u00e7\u00f5es:\n\n1.  **Leitura dos dados**\n2.  **An\u00e1lise Explorat\u00f3ria dos dados**\n3.  **Engenharia de Features**\n4.  **Buscar o melhor algoritmo**\n5.  **Buscar os melhores par\u00e2metros**\n6.  **Testes**\n7.  **Make Submission**\n8.  **Pr\u00f3ximos passos**\n\nCada se\u00e7\u00e3o inicialmente contem as fun\u00e7\u00f5es que seram utilizadas, e em seguida uma breve explica\u00e7\u00e3o.\n"}}