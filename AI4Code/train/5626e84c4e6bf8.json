{"cell_type":{"05df25fa":"code","6d968eca":"code","15bb8890":"code","705a3e08":"code","84bc9b80":"code","dfe53ac4":"code","a978e229":"code","4e6a59f1":"code","146df63a":"code","282360ca":"code","096a30f1":"code","b1ab6bc6":"code","e451ce7a":"code","08c57087":"code","875a6558":"code","57c2f036":"code","74c32810":"code","69cc463a":"code","d14edbc9":"code","d99a86f1":"code","b9a339e0":"code","654ac215":"markdown","8b3a2921":"markdown","96f24a3f":"markdown","85ee9674":"markdown","e9b4d009":"markdown","b5255f5c":"markdown","62d129f7":"markdown","7ed71425":"markdown","a970f6dc":"markdown","1a34ca27":"markdown","17d797ad":"markdown","ee979658":"markdown","fae52708":"markdown","9f79ad33":"markdown","0aaecfd2":"markdown","9402fae2":"markdown","e122239c":"markdown","17ee3ca2":"markdown","b29078d4":"markdown"},"source":{"05df25fa":"# Loading libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom minisom import MiniSom\nfrom hyperopt import fmin, hp, tpe, Trials, STATUS_OK\nimport concurrent.futures\nimport time\nfrom pylab import bone, pcolor, colorbar, plot, show, rcParams, savefig","6d968eca":"# Loading training and test set\ntrain = pd.read_csv('..\/input\/fashion-mnist_train.csv')\ntest = pd.read_csv('..\/input\/fashion-mnist_test.csv')\ntrain.head()","15bb8890":"# Combining training and test set to get over 70k samples\nnew_train = train.drop(columns=['label'])\nnew_test = test.drop(columns=['label'])\nsom_data = pd.concat([new_train, new_test], ignore_index=True).values\nlabels = pd.concat([train['label'], test['label']], ignore_index=True).values","705a3e08":"f, ax = plt.subplots(1,5)\nf.set_size_inches(80, 40)\nfor i in range(5):\n    ax[i].imshow(som_data[i].reshape(28, 28))\nplt.show()","84bc9b80":"#Initializing the map\nstart_time = time.time()\n# The map will have x*y = 50*50 = 2500 features  \nsom = MiniSom(x=50,y=50,input_len=som_data.shape[1],sigma=0.5,learning_rate=0.4)\n# There are two ways to train this data\n# train_batch: Data is trained in batches\n# train_random: Random samples of data are trained. Following line of code provides random weights as we are going to use train_random for training\nsom.random_weights_init(som_data)","dfe53ac4":"# Training data for 1000 iterations\nsom.train_random(data=som_data,num_iteration=1000)","a978e229":"# Finally plotting the map\nwith concurrent.futures.ProcessPoolExecutor() as executor:\n    rcParams['figure.figsize'] = 25, 20\n    bone()\n    pcolor(som.distance_map().T)\n    colorbar()\n    markers = ['o','s','p','*','^','1','h','x','+','d']\n    colors = ['#57B8FF','#B66D0D','#009FB7','#FBB13C','#FE6847','#4FB5A5','#670BE8','#F29F60','#8E1C4A','#85809B']\n    for i,x in enumerate(som_data):\n        w = som.winner(x)\n        plot(w[0]+0.5,w[1]+0.5,markers[labels[i]],markeredgecolor=colors[labels[i]],markerfacecolor='None',markersize=10,markeredgewidth=2)\n    savefig(\"map.png\")\n    show()\nend_time = time.time() - start_time","4e6a59f1":"print(int(end_time),\"seconds taken to complete the task.\")","146df63a":"start_time = time.time()\n# Returns a matrix where the element i,j is the number of time that the neuron i,j have been winner.\nact_res = som.activation_response(som_data)\n# Returns a dictionary wm where wm[(i,j)] is a list with all the patterns that have been mapped in the position i,j.\nwinner_map = som.win_map(som_data)\n# Returns a dictionary wm where wm[(i,j)] is a dictionary that contains the number of samples from a given label that have been mapped in position i,j.\nlabelmap = som.labels_map(som_data,labels)\nend_time = time.time() - start_time\nprint(int(end_time),\"seconds taken to extract data from results.\")","282360ca":"sns.heatmap(act_res)","096a30f1":"# Extracting outliers\nq75, q25 = np.percentile(act_res.flatten(), [75 ,25])\niqr = q75 - q25\nlower_fence = q25 - (1.5*iqr)\nupper_fence = q75 + (1.5*iqr)\ncondition = (act_res < lower_fence) | (act_res > upper_fence)\noutlier_neurons = np.extract(condition,act_res)","b1ab6bc6":"# Plotting the distribution of neurons and outliers\nf, (ax1, ax2) = plt.subplots(1, 2, sharex='col', sharey='row', figsize=(15,5))\nax1.set(xlabel='Distribution of all neurons')\nax2.set(xlabel='Distribution of outliers')\nsns.distplot(act_res.flatten(),ax=ax1)\nsns.distplot(outlier_neurons,ax=ax2)\nplt.close(2)\nplt.close(3)","e451ce7a":"f, ax = plt.subplots(1,5)\nf.set_size_inches(80, 40)\nfor i in range(5):\n    ax[i].imshow(winner_map[list(winner_map)[1]][i].reshape(28, 28))\nplt.show()","08c57087":"# Reorganizing the data\ntrain = train[(train.label == 4) | (train.label == 8)]\ntest = test[(test.label == 4) | (test.label == 8)]\n\nopt_train = train.drop(['label'],axis=1)\nopt_test = test.drop(['label'],axis=1)\nopt_data = pd.concat([opt_train, opt_test], ignore_index=True).values\nlabels = pd.concat([train['label'], test['label']], ignore_index=True).values","875a6558":"# Setting some parameters in advance\nx = y = 10\ninput_len = opt_data.shape[1]\nsigma = 1.0\nlearning_rate = 0.5\niterations = 1000","57c2f036":"# Now, a function to plot maps\ndef make_map(x,y,input_len,sigma,learning_rate,iterations):\n    som = MiniSom(x=x,y=y,input_len=input_len,sigma=sigma,learning_rate=learning_rate)\n    som.random_weights_init(opt_data)\n    som.train_random(data=opt_data,num_iteration=iterations)\n    with concurrent.futures.ProcessPoolExecutor() as executor:\n        rcParams['figure.figsize'] = 10, 8\n        bone()\n        pcolor(som.distance_map().T)\n        colorbar()\n        markers = ['o','s','p','*','^','1','h','x','+','d']\n        colors = ['#57B8FF','#B66D0D','#009FB7','#FBB13C','#FE6847','#4FB5A5','#8C9376','#F29F60','#8E1C4A','#85809B']\n        for i,x in enumerate(opt_data):\n            w = som.winner(x)\n            plot(w[0]+0.5,w[1]+0.5,markers[labels[i]],markeredgecolor=colors[labels[i]],markerfacecolor='None',markersize=12,markeredgewidth=2)\n        show()","74c32810":"# A simple unoptimized map\nmake_map(x,y,input_len,sigma,learning_rate,iterations)","69cc463a":"# This will optimize sigma to minimize the quantization error\nbest_params = fmin(\n    fn = lambda sig: MiniSom(x=x,y=y,input_len=input_len,sigma=sigma,learning_rate=learning_rate).quantization_error(opt_data),\n    space = hp.uniform(\"sig\",0.0009,x\/2.0001),\n    algo = tpe.suggest,\n    verbose=1,\n    max_evals = 50)\nprint(\"The best sigma value after 50 iterations is {}\".format(best_params['sig']))","d14edbc9":"# Let's see the new optimized map\nmake_map(x,y,input_len,best_params['sig'],learning_rate,iterations)","d99a86f1":"space = {\n    'sig': hp.uniform('sig',0.001,5.0),\n    'learning_rate': hp.uniform('learning_rate',0.001,0.5)\n}\n\ndef opt_map(space):\n    sig = space['sig']\n    learning_rate = space['learning_rate']\n    val = MiniSom(x=x,y=y,input_len=input_len,sigma=sigma,learning_rate=learning_rate).quantization_error(opt_data)\n    #print(\"Now,the quantization error is {}\\n\".format(val))\n    return {'loss':val, 'status':STATUS_OK}\n\ntrials = Trials()\nbest_params = fmin(fn=opt_map,space=space,algo=tpe.suggest,max_evals=50,trials=trials)\nprint(\"The best sigma value after 50 iterations is {}\".format(best_params['sig']))\nprint(\"The best learning_rate after 50 iterations is {}\".format(best_params['learning_rate']))","b9a339e0":"# The more optimized map \nmake_map(x,y,input_len,best_params['sig'],best_params['learning_rate'],iterations)","654ac215":"**Go to the end of the comments to see the interpretation of the first map**\n\n**Kindly upvote, comment, share and fork this kernel if you like my work. I am open to suggestions.**","8b3a2921":"**Well, we can certainly have more defined clusters through optimization.**","96f24a3f":"## Heatmap for performance of neurons\nWe will use *act_res* to generate a heatmap which indicates the neurons which perform better than others.\n**Colour given to a neuron represents the number of times it has been winner. Lighter colour shade is directly proportional to this frequency of winning.**","85ee9674":"# Analyzing the results\n**Minisom objects provide us with enough data to perform good analysis of our results and gain more insights**","e9b4d009":"## Minisom: A minimalistic implementation of Self Organizing Maps\nMiniSom is a minimalistic and Numpy based implementation of the Self Organizing Maps (SOM). SOM is a type of Artificial Neural Network able to convert complex, nonlinear statistical relationships between high-dimensional data items into simple geometric relationships on a low-dimensional display.\nSource: [GitHub](https:\/\/github.com\/JustGlowing\/minisom)","b5255f5c":"Now, we will plot the map. First, we will manually define labels with their markers.\n* 0 -> Light blue circle - T-shirt\/top\n* 1 -> Caramel square- Trouser\n* 2 -> Blue pentagon - Pullover\n* 3 -> Orange star - Dress\n* 4 -> Tomato red triangle - Coat\n* 5 -> Bright cyan tri_down - Sandal\n* 6 -> Electric indigo hexagon - Shirt\n* 7 -> Light orange x - Sneaker\n* 8 -> Raspberry plus - Bag\n* 9 -> Purple diamond - Ankle Boot","62d129f7":"## Bayesian Optimization\nBayesian optimization is a sequential design strategy for global optimization of black-box functions that doesn't require derivatives. Since the objective function is unknown, the Bayesian strategy is to treat it as a random function and place a prior over it. The prior captures our beliefs about the behaviour of the function. After gathering the function evaluations, which are treated as data, the prior is updated to form the posterior distribution over the objective function. The posterior distribution, in turn, is used to construct an acquisition function (often also referred to as infill sampling criteria) that determines what the next query point should be.\n\nSource: [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Bayesian_optimization)","7ed71425":"# Some sample images","a970f6dc":"## Distribution of outlier neurons","1a34ca27":"# About the dataset: Fashion MNIST\n## Context\nFashion-MNIST is a dataset of Zalando's article images\u2014consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. Zalando intends Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits.\n\nThe original MNIST dataset contains a lot of handwritten digits. Members of the AI\/ML\/Data Science community love this dataset and use it as a benchmark to validate their algorithms. In fact, MNIST is often the first dataset researchers try. \"If it doesn't work on MNIST, it won't work at all\", they said. \"Well, if it does work on MNIST, it may still fail on others.\" Zalando seeks to replace the original MNIST dataset.\n\n## Content\nEach image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255. The training and test data sets have 785 columns. The first column consists of the class labels (see above), and represents the article of clothing. The rest of the columns contain the pixel-values of the associated image. To locate a pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27. The pixel is located on row i and column j of a 28 x 28 matrix. For example, pixel31 indicates the pixel that is in the fourth column from the left, and the second row from the top, as in the ascii-diagram below. \n\n## Labels\nLabels\nEach training and test example is assigned to one of the following labels:\n* 0 T-shirt\/top\n* 1 Trouser\n* 2 Pullover\n* 3 Dress\n* 4 Coat\n* 5 Sandal\n* 6 Shirt\n* 7 Sneaker\n* 8 Bag\n* 9 Ankle boot \n","17d797ad":"# Things we can do with Self Organizing Maps\n* Visualizing high dimensional data into a low dimensional view which is usually 2D - In this case we have 784 columns because the Fashion MNIST dataset has images of dimensions 28x28 \n* Clustering - According to [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Self-organizing_map): It has been shown that while self-organizing maps with a small number of nodes behave in a way that is similar to K-means, larger self-organizing maps rearrange data in a way that is fundamentally topological in character.\n* Anomaly detection - We identify entities whose topological distance to its topological neighbors is significantly higher than all its topological neighbors amongst themselves as anomalies\n* Non-linear DImensionality Reduction - For visualization, we convert high-dimensional data into low-dimensional data","ee979658":"Maybe this is not the most optimized map but you can always fork this kernel to play with parameters and fully optimize this implementation.","fae52708":"<img src=\"https:\/\/i.imgflip.com\/2jxxia.jpg\" title=\"made at imgflip.com\">\n# Unsupervised Deep Learning\nWe, data scientists regularly use **DNNs, CNNs and RNNs** for most applications of deep learning but that's only the **supervised side** of the neural networks family but there is also the more sophisticated and less talked about **unsupervised side** which is just as or even more intriguing than the conventional supervised architectures. These unsupervised models enable the neural networks to perform tasks like **clustering, anomaly detection, feature selection, feature extraction, dimensionality reduction and recommender systems**. Some of these neural networks are: **Self organizing maps, Boltzmann machines, Autoencoders**\n\n### Self Organizing Maps\n<img src=\"https:\/\/www.researchgate.net\/profile\/Damian_Jankowski3\/publication\/291834232\/figure\/fig3\/AS:553741877481472@1509033759154\/Self-organizing-map-structure.png\">\n\n### Boltzmann Machines\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/7\/7a\/Boltzmannexamplev1.png\/330px-Boltzmannexamplev1.png\">\n\n### Autoencoders\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1000\/1*ZEvDcg1LP7xvrTSHt0B5-Q@2x.png\" height=\"400\" width=\"500\">","9f79ad33":"# The interpretation\n\nEach time the map is generated different but these are my observations from maps in different versions.\n\n - Clearly more optimization is required as there is much overlap of weights\n - Most anomalies\/outliers have the bright cyan three-pronged star(tri_down) label associated with them which is a Sandal\n - Most of the X, diamonds and three-pronged stars overlap with each other so they have similar features which is expected as these are all kinds of footwear i.e., sneakers, ankle boots and sandals respectively\n - Brown squares i.e., trousers have the most distinct and well-formed clusters with minimal overlapping and a significant number of features covered\n - Plus sign i.e., bag is not mapped on much features which means that many features have been extracted for bags \n - circles, pentagons and hexagons overlap with each other a lot and this is expected as they are T-shirts, pullovers and shirts respectively and so are expected to have similar features.\n - Star(dress) and triangle(coat) are the labels that overlap a lot with other labels\n - Much more than half of the extracted features have the following labels: Sneakers(X), Shirts(Hexagon), Trousers(Square), T-shirts\/tops(Circle)\n\nMany more things can be interpreted but these are just some easily discovered observations.","0aaecfd2":"As weights are random, every time the map is generated different. I got great distinguished clusters while I was coding with little overlapping. Overlapping should be minimal but minute to no overlapping means the features are very distinguished and images of both labels have little to nothing in common which is not really true in this case.\n\nNow, I will try optimizing two parameters with Hyperopt.","9402fae2":"## Hyperopt: A great library for Bayesian Optimization\nHyperopt is a Python library for serial and parallel optimization over awkward search spaces, which may include real-valued, discrete, and conditional dimensions. According to GitHub, it is \"Distributed Asynchronous Hyper-parameter Optimization\".\nSource: [GitHub](https:\/\/github.com\/hyperopt\/hyperopt)\n\n**The sigma parameter can have a lot of impact on the clustering in the map as it is essentially the spread of the neighborhood function. So, we will optimize the sigma function to minimize the quantization error(```quantization_error``` - A MiniSom method that returns the quantization error computed as the average distance between each input sample and its best matching unit.).**\n\n```fmin``` - For minimizing the parameter\n\n```Trials``` - To really see the purpose of returning a dictionary, let's modify the objective function to return some more things, and pass an explicit trials argument to fmin.\n\n```hp``` - Objective function for defining the search space\n\n```tpe``` - Algorithm used for optimization\n\n**We will have the training and test set contain only '4s' and '8s' as optimization for all labels will take a lot of time. At the end of the day, this is just a example to get you all to know about great libraries like 'MiniSom' and'Hyperopt'.**","e122239c":"## Visualizing patterns \n*winner_map* contains dominant features\/patterns generated by neurons. Some of these features if visualized look like amalgamation of different kinds of clothes.\nLike a generated feature may generate images that look like the combination of shirts, coats and pullovers. This may help in clustering","17ee3ca2":"# How to interpret and evaluate a Self Organizing Map\n* **Again, a Self Organizing Map creates a view that represents high dimensional data as low dimensional data preserving topological properties of the input space using a neighborhood function** \n* The heatmap in the background on which the clusters reside represent the topological properties of the input space. The colorbar() on the right represent the topological distance. The distance goes from **0(black) to 1(white) where lesser the distance, more is the correlation\/similarity of the feature with its immediate neighboring features**.\n* If the feature is white i.e., **topological distance close to 1, then they can be classified as anomalies**.\n* The markers(colored shapes) represent different labels and are clustered on the topological space on the basis of their topological properties.\n* Our goal is to have distinct clusters but that doesn't mean all the points of the cluster have to be close to each other because this is non-linear dimensionality reduction and not K-means clustering where points are located close to the centroids\n* For better evaluation, we have to take care that any **given feature should be occupied by only one label\/marker. We should optimize the map for the same.** \n* Overlap of mutiple labels on a feature means its uniqueness is compromised and there is a scope of improvement.","b29078d4":"# Aim\nThis is supposed to be a tutorial on **Self Organizing Maps** where we will perform clustering on Fashion MNIST using a neural network.\n# Concepts covered\n- Self Organizing Maps(For unsupervised deep learning)\n- Bayesian Optimization\n- Analysis of Self Organized Maps\n- Some image processing\n\n# Self Organizing Maps\nA self-organizing map (SOM) or self-organizing feature map (SOFM) is a type of artificial neural network (ANN) that is trained using unsupervised learning to produce a low-dimensional (typically two-dimensional), discretized representation of the input space of the training samples, called a map, and is therefore a method to do dimensionality reduction. Self-organizing maps differ from other artificial neural networks as they apply competitive learning as opposed to error-correction learning (such as backpropagation with gradient descent), and in the sense that they use a neighborhood function to preserve the topological properties of the input space.\n<img src=\"http:\/\/www.pitt.edu\/~is2470pb\/Spring05\/FinalProjects\/Group1a\/tutorial\/kohonen1.gif\">\nThis makes SOMs useful for visualization by creating low-dimensional views of high-dimensional data, akin to multidimensional scaling. The artificial neural network introduced by the Finnish professor Teuvo Kohonen in the 1980s is sometimes called a Kohonen map or network. The Kohonen net is a computationally convenient abstraction building on biological models of neural systems from the 1970s and morphogenesis models dating back to Alan Turing in the 1950s.\n<img src=\"https:\/\/www.nnwj.de\/uploads\/pics\/1_2-kohonon-feature-map.gif\">\nWhile it is typical to consider this type of network structure as related to feedforward networks where the nodes are visualized as being attached, this type of architecture is fundamentally different in arrangement and motivation. It has been shown that while self-organizing maps with a small number of nodes behave in a way that is similar to K-means, larger self-organizing maps rearrange data in a way that is fundamentally topological in character.\n\nSource: [Wikipedia](http:\/\/https:\/\/en.wikipedia.org\/wiki\/Self-organizing_map)"}}