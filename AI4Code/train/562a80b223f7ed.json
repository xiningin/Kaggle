{"cell_type":{"fdc60ca6":"code","e6dd86c9":"code","c37c8df6":"code","1e433f43":"code","7497655f":"code","9e87f2fc":"code","36ee712d":"code","f8b07437":"code","1ea9aefc":"code","6c8e40d0":"code","4730046d":"code","745ff291":"code","fa4ad5ca":"code","4271c9f0":"code","6958d234":"code","951cdcea":"markdown","a84d4a14":"markdown","9ede9dc9":"markdown","9d62480e":"markdown","46248290":"markdown","d2380027":"markdown","e72346f1":"markdown","7a152a05":"markdown","11ca4231":"markdown","f32e5f79":"markdown","7632670f":"markdown","a46d5ac8":"markdown","700fbb0c":"markdown","53207e82":"markdown"},"source":{"fdc60ca6":"#----------------------------\n##installing efficient net models\n#----------------------------\n!\/opt\/conda\/bin\/python3.7 -m pip install --upgrade pip\n! pip install -q efficientnet","e6dd86c9":"#-------------------------\n#importing necessary libraries\n#-------------------------\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport efficientnet.tfkeras as efn\n\nimport numpy as np\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport PIL\n\nimport os\n\nfrom kaggle_datasets import KaggleDatasets","c37c8df6":"AUTO = tf.data.experimental.AUTOTUNE\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","1e433f43":"GCS_DS_PATH = KaggleDatasets().get_gcs_path(\"shopee-product-matching\")\nTRAIN_PATH = GCS_DS_PATH + \"\/train_images\/\"\n\ntrain_df = pd.read_csv(\"..\/input\/shopee-product-matching\/train.csv\")\n\nlabel2id = dict(zip(range(train_df.label_group.nunique()),train_df.label_group.unique()))\nid2label = dict(zip(train_df.label_group.unique(),range(train_df.label_group.nunique())))\ntrain_df[\"label_group\"] = train_df[\"label_group\"].map(id2label)\ntrain_df.index = train_df[\"image\"]\n\n\ntmp = train_df.groupby('label_group').posting_id.agg('unique').to_dict()\ntrain_df['posting_ids'] = train_df.label_group.map(tmp)\ntrain_df['posting_ids'] = train_df['posting_ids'].apply(lambda x: list(x))\n\ntrain_df.head()","7497655f":"HEIGHT,WIDTH = 512,512\nCHANNELS = 3\n\n#--------------------\n#Display Samples\n#--------------------\n\ntrain_df.index = train_df[\"posting_id\"]\n\ndef filepath_to_arr(filepath):\n    img = tf.keras.preprocessing.image.load_img(filepath,target_size= (HEIGHT,WIDTH))\n    arr = tf.keras.preprocessing.image.img_to_array(img)\/255.\n    return arr\n\ndef display_img(training_ids):\n    num_imgs = len(training_ids)\n \n    plt.figure(figsize = (5*num_imgs,10))\n    for i,_id in enumerate(training_ids):\n        plt.subplot(1,num_imgs+1,i+1)\n        \n        filepath = os.path.join(\"..\/input\/shopee-product-matching\/train_images\",train_df.loc[_id][\"image\"])\n        plt.title(\"Image : \"+str(i+1))\n        arr = filepath_to_arr(filepath)\n        plt.imshow(arr)\n        plt.axis(\"off\")\n    plt.show()\n\nx = np.random.randint(0,30000,size=1)\nfor j in range(5):\n    display_img(train_df.iloc[x[0] + j][\"posting_ids\"])\n    \ntrain_df = train_df.drop_duplicates(subset=['image'])\ntrain_df.index = train_df[\"image\"]","9e87f2fc":"#------------------------\n#siamese dataframe\n#------------------------\n\nsiamese_df = pd.read_csv(\"..\/input\/shopee-siamese-training\/siamese_data.csv\")\nsiamese_df.replace(1,2,inplace = True)\nsiamese_df.replace(0,1, inplace = True)\nsiamese_df.replace(2,0, inplace = True)\nsiamese_df[\"label\"] = siamese_df[\"label\"].astype(\"float32\")\nsiamese_df.head()","36ee712d":"#------------------\n##processing image\n#------------------\ndef process_img(filepath):\n    image = tf.io.read_file(filepath)\n    image = tf.image.decode_jpeg(image, channels=CHANNELS)\n    image = tf.image.convert_image_dtype(image, tf.float32) \n    image = tf.image.resize(image, [HEIGHT,WIDTH])\n    return image\n\n#-----------------------------------\n##adding augmentations to image data\n#-----------------------------------\ndef data_augment(image):\n    p_spatial = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_rotate = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_pixel_1 = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_pixel_2 = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_pixel_3 = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_crop = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n            \n    # Flips\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    if p_spatial > .75:\n        image = tf.image.transpose(image)\n        \n    # Rotates\n    if p_rotate > .75:\n        image = tf.image.rot90(image, k=3) \n    elif p_rotate > .5:\n        image = tf.image.rot90(image, k=2) \n    elif p_rotate > .25:\n        image = tf.image.rot90(image, k=1) \n        \n    \n    if p_pixel_1 >= .4:\n        image = tf.image.random_saturation(image, lower=.7, upper=1.3)\n    if p_pixel_2 >= .4:\n        image = tf.image.random_contrast(image, lower=.8, upper=1.2)\n    if p_pixel_3 >= .4:\n        image = tf.image.random_brightness(image, max_delta=.1)\n        \n    \n    if p_crop > .7:\n        if p_crop > .9:\n            image = tf.image.central_crop(image, central_fraction=.7)\n        elif p_crop > .8:\n            image = tf.image.central_crop(image, central_fraction=.8)\n        else:\n            image = tf.image.central_crop(image, central_fraction=.9)\n    elif p_crop > .4:\n        crop_size = tf.random.uniform([], int(HEIGHT*.8), HEIGHT, dtype=tf.int32)\n        image = tf.image.random_crop(image, size=[crop_size, crop_size, CHANNELS])\n    \n    image = tf.image.resize(image, [HEIGHT,WIDTH])\n    return image\n\n\n#------------------\n#concat two arrays of image pairs\n#------------------\ndef process_img_pair(file_pair,label):\n    im1 = process_img(file_pair[0])\n    im1 = tf.expand_dims(im1,axis=-1)\n    \n    im2 = process_img(file_pair[1])\n    im2 = tf.expand_dims(im2,axis=-1)\n    \n    im_pair = tf.concat([im1,im2],axis=-1)\n    \n    return im_pair,label\n\n#------------------------\n#adding augmentation to image pair\n#------------------------\ndef augment_img_pair(image_pair,label):\n    im1 = image_pair[:,:,:,0]\n    im1 = data_augment(im1)\n    im1 = tf.expand_dims(im1,axis=-1)\n    \n    im2 = image_pair[:,:,:,1]\n    im2 = data_augment(im2)\n    im2 = tf.expand_dims(im2,axis=-1)\n    \n    im_pair = tf.concat([im1,im2],axis=-1)\n    return im_pair,label","f8b07437":"BATCH_SIZE = 8 * strategy.num_replicas_in_sync\nSPLIT = int(0.8*len(siamese_df))\n\nSTEPS_PER_EPOCH  = SPLIT\/\/BATCH_SIZE\nVALID_STEPS = (len(siamese_df)-SPLIT)\/\/BATCH_SIZE\nSEED = 143\n\nx = siamese_df.image_1.to_list()\ny = siamese_df.image_2.to_list()\nfilepairs = [[os.path.join(TRAIN_PATH,i),os.path.join(TRAIN_PATH,j)] for i,j in zip(x,y)]\n\nlabels = siamese_df.label.to_list()\n\ndataset = tf.data.Dataset.from_tensor_slices((filepairs,labels))\ndataset = dataset.map(process_img_pair,num_parallel_calls=AUTO)\ndataset = dataset.map(augment_img_pair,num_parallel_calls=AUTO)\n\ntrain_ds = dataset.take(SPLIT)\nval_ds = dataset.skip(SPLIT)\n\nAUTO = tf.data.experimental.AUTOTUNE\n\ntrain_ds = train_ds.cache().repeat().shuffle(BATCH_SIZE*20).batch(BATCH_SIZE).prefetch(AUTO)\nval_ds = val_ds.repeat().batch(BATCH_SIZE).prefetch(AUTO)\nprint(\"Data Pipeline\")","1ea9aefc":"for batch in val_ds.take(1):\n    im,label = batch\n    print(im.shape)\n    print(label)","6c8e40d0":"def create_model():\n    inputs = tf.keras.Input(shape = (HEIGHT,WIDTH,CHANNELS,2,))\n    \n    input_a = inputs[:,:,:,:,0]\n    input_b = inputs[:,:,:,:,1]\n    \n    pretrained = efn.EfficientNetB0(include_top=False, weights='noisy-student',input_shape=[HEIGHT,WIDTH, 3])\n            \n    x = pretrained.output\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n        \n    embed_model = tf.keras.Model(pretrained.input, x)\n    \n    embed_a = embed_model(input_a)\n    embed_b = embed_model(input_b)\n    \n    l1_layer = tf.keras.layers.Lambda(lambda tensors:tf.linalg.norm(tensors[0] - tensors[1], axis=1))\n    outputs = l1_layer([embed_a,embed_b])\n    \n    \n    model = tf.keras.Model(inputs,outputs)\n    \n    return model\n\nmodel = create_model()\nmodel.summary()","4730046d":"def compile_model(model, lr=0.0001):\n    \n    optimizer = tf.keras.optimizers.Adam(lr=lr)\n    \n    loss = tfa.losses.ContrastiveLoss(margin = 1.0,name = 'loss')\n    \n    metrics = [\n       tf.keras.metrics.Accuracy(name='acc')\n    ]\n\n    model.compile(optimizer=optimizer, loss=loss)\n\n    return model","745ff291":"metric = \"val_loss\"\nmode = \"min\"\n\ndef create_callbacks():\n    \n    cpk_path = '.\/best_model.h5'\n    \n    reducelr = tf.keras.callbacks.ReduceLROnPlateau(\n        monitor=metric,\n        mode=mode,\n        factor=0.1,\n        patience=3,\n        verbose=0\n    )\n    \n    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n        filepath=cpk_path,\n        monitor=metric,\n        mode=mode,\n        save_best_only=True,\n        verbose=1,\n    )\n\n    earlystop = tf.keras.callbacks.EarlyStopping(\n        monitor= metric,\n        mode=mode,\n        patience=10, \n        verbose=1\n    )\n    \n    callbacks = [checkpoint, reducelr, earlystop]         \n    \n    return callbacks","fa4ad5ca":"EPOCHS= 2\nVERBOSE =1\n\n\ntf.keras.backend.clear_session()\n\nwith strategy.scope():\n    \n    #model = create_model()\n    model = tf.keras.models.load_model(\"..\/input\/shopee-contrastiveloss-tensorflow-tpu-training\/best_model.h5\")\n    model = compile_model(model, lr=0.0001)\n   \n    callbacks = create_callbacks()\n    \n    history = model.fit(train_ds, \n                        epochs=EPOCHS,\n                        callbacks=callbacks,\n                        validation_data = val_ds,\n                        verbose=VERBOSE,\n                        steps_per_epoch = STEPS_PER_EPOCH,\n                        validation_steps=VALID_STEPS\n                       )","4271c9f0":"loss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs_range = range(len(history.history['val_loss']))\nplt.figure(figsize=(8, 8))\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()\nprint(\"Plotting the History\")","6958d234":"files_ls = tf.io.gfile.glob(TRAIN_PATH + '*.jpg')\nds = tf.data.Dataset.from_tensor_slices(files_ls)\nds = ds.map(process_img,num_parallel_calls=AUTO)\nds = ds.map(data_augment,num_parallel_calls=AUTO)\nds = ds.batch(BATCH_SIZE)\n\nwith strategy.scope():\n    model = tf.keras.models.load_model(\".\/best_model.h5\")\n    embed_model = tf.keras.Model(model.layers[-2].input,model.layers[-2].output)\n    embeddings = embed_model.predict(ds)\nnp.save(\".\/embeddings.npy\",embeddings)","951cdcea":"# Functions for preprocessing","a84d4a14":"# Training embeddings","9ede9dc9":"# History plotting","9d62480e":"# Create Model","46248290":"# Compile the Model","d2380027":"# Creating Callbacks","e72346f1":"# Training","7a152a05":"# Loading Siamese pairs","11ca4231":"# Introduction\n<img src = \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/f\/fe\/Shopee.svg\/1200px-Shopee.svg.png\" height = \"400\" width = \"400\">\n\n## Description of the competition\n\nTwo different images of similar wares may represent the same product or two completely different items. Retailers want to avoid misrepresentations and other issues that could come from conflating two dissimilar products. Currently, a combination of deep learning and traditional machine learning analyzes image and text information to compare similarity. But major differences in images, titles, and product descriptions prevent these methods from being entirely effective.\n\n## About Shopee\n\n**Website : [Shopee](https:\/\/shopee.com\/)** <br>\nShopee is the leading e-commerce platform in Southeast Asia and Taiwan. Customers appreciate its easy, secure, and fast online shopping experience tailored to their region. The company also provides strong payment and logistical support along with a 'Lowest Price Guaranteed' feature on thousands of Shopee's listed products.\n\n## What we need to do\nIn this competition, you\u2019ll apply your machine learning skills to build a model that predicts which items are the same products.\n\n## Other Details\n\n- Evaluation criteria : **F1 Score**\n- Accelerator used for Training : **TPU**\n- Technique : **Siamese model**\n- Loss Function  : **Contrastive Loss**\n\n**Note : Thanks to @tanulsingh077  for creating dataset for Siamese model <br>\nDataset : https:\/\/www.kaggle.com\/tanulsingh077\/shopee-siamese-training <br>\nPreparation notebook : https:\/\/www.kaggle.com\/tanulsingh077\/code-for-data-generation-for-siamese-training\/ <br>**","f32e5f79":"# Data Pipeline","7632670f":"# Import necessary Libraries","a46d5ac8":"# Data Visualization","700fbb0c":"## Hope this notebook is helpful. If you have any doubts or suggestions feel free to comment here.\n## An upvote will be very much encouraging for me.\n# Happy kaggling\u2764","53207e82":"# Checking TPU access"}}