{"cell_type":{"57f3e031":"code","aeb3a340":"code","13baca5e":"code","5aacc85e":"code","07c4dba0":"code","6177ac53":"code","718fe640":"code","439eb69d":"code","9e858c68":"code","913c64f4":"code","5434450e":"code","fe506d2c":"code","84109e4d":"code","bd49c91c":"code","101e17ee":"code","2276aac0":"code","ca819c92":"code","b8d5b38f":"code","24666a3b":"code","ef62ff4c":"code","3e0c60cf":"code","7a68cb70":"code","ec0f1ae3":"code","bd506f00":"code","824ffa6e":"code","bffbae70":"code","356d2bdc":"code","3f36b23c":"code","08766258":"code","f3ad1850":"code","232e105e":"code","1605935c":"code","cb0a08d7":"code","93d871fb":"markdown","d84a7d8d":"markdown","ae4a986f":"markdown","a3eaf230":"markdown"},"source":{"57f3e031":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# Import Dependencies\n%matplotlib inline\n\n# Start Python Imports\nimport math, time, random, datetime\n\n# Data Manipulation\nimport numpy as np\nimport pandas as pd\n\n# Visualization \nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nimport missingno as msno\nimport seaborn as sns\nplt.style.use('seaborn-whitegrid')\n\n# Let's ignore warnings for now\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","aeb3a340":"# Importing data and verfiying\n\ndf=pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndf.head()","13baca5e":"# General Analysis of the data\n\ndf.describe(),df.shape","5aacc85e":"# Checking the data types\n\ndf.dtypes","07c4dba0":"# Checking for missing values\n\ndf.isnull().sum()","6177ac53":"# While we can see above that the dataset does not have null values, we can see that it still has ZERO values which may not make a \n# lot of sense for fields such as Glucose, Blood Pressure, Skin Thickness, Insulin, BMI. We want to replace ZERO with Nan so that it\n# reflects as missing values\n\ndf[[\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"Insulin\",\"BMI\"]] = df[[\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"Insulin\",\"BMI\"]].replace({0:np.nan})\ndf.isnull().sum()","718fe640":"# Replacing null values with mean\n\ndf['Glucose'] = df['Glucose'].fillna(value=df['Glucose'].mean())\ndf['BloodPressure'] = df['BloodPressure'].fillna(value=df['BloodPressure'].mean())\ndf['SkinThickness'] = df['SkinThickness'].fillna(value=df['SkinThickness'].mean())\ndf['Insulin'] = df['Insulin'].fillna(value=df['Insulin'].mean())\ndf['BMI'] = df['BMI'].fillna(value=df['BMI'].mean())\ndf.isnull().sum()","439eb69d":"# Seeing the spread of people with diabetes (Our Target feature -> Outcome)\n\nfig = plt.figure(figsize=(20,1))\nsns.countplot(data=df,y='Outcome')\ndf['Outcome'].value_counts()","9e858c68":"# Checking correlation between various parameters\n\nrcParams[\"figure.figsize\"] = 20,10\nplt.title(\"Corellation between different features\")\nsns.heatmap(df.corr(),annot=True,cmap=\"YlGnBu\")","913c64f4":"df.hist(figsize = (20,20))","5434450e":"# Scaling the data\n\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX =  pd.DataFrame(sc_X.fit_transform(df.drop([\"Outcome\"],axis = 1),),columns=['Pregnancies', 'Glucose', 'BloodPressure', \n                                                                              'SkinThickness', 'Insulin','BMI',\n                                                                              'DiabetesPedigreeFunction', 'Age'])\nX.head()","fe506d2c":"# Target feature\ny=df['Outcome']\ny.head()","84109e4d":"## Test and train data\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=1\/3,random_state=42, stratify=y)","bd49c91c":"# Function that runs the requested algorithm and returns the accuracy metrics\ndef fit_ml_algo(algo, X_train, y_train, cv):\n    \n    # One Pass\n    model = algo.fit(X_train, y_train)\n    acc = round(model.score(X_train, y_train) * 100, 2)\n    \n    # Cross Validation \n    train_pred = model_selection.cross_val_predict(algo, \n                                                  X_train, \n                                                  y_train, \n                                                  cv=cv, \n                                                  n_jobs = -1)\n    # Cross-validation accuracy metric\n    acc_cv = round(metrics.accuracy_score(y_train, train_pred) * 100, 2)\n    \n    return train_pred, acc, acc_cv","101e17ee":"# Importing ML Libraries\n\nfrom sklearn import model_selection, tree, preprocessing, metrics, linear_model\nfrom sklearn.svm import LinearSVC,SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier","2276aac0":"# Logistic Regression\n\ntrain_pred_log, acc_log, acc_cv_log = fit_ml_algo(LogisticRegression(), \n                                                               X_train, \n                                                               y_train, \n                                                                    10)\n\nprint(\"Accuracy: %s\" % acc_log)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_log)","ca819c92":"# k-Nearest Neighbours\n\ntrain_pred_knn, acc_knn, acc_cv_knn = fit_ml_algo(KNeighborsClassifier(), \n                                                  X_train, \n                                                  y_train, \n                                                  10)\n\nprint(\"Accuracy: %s\" % acc_knn)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_knn)","b8d5b38f":"# Linear SVC\n\ntrain_pred_svc, acc_linear_svc, acc_cv_linear_svc = fit_ml_algo(LinearSVC(),\n                                                                X_train, \n                                                                y_train, \n                                                                10)\n\nprint(\"Accuracy: %s\" % acc_linear_svc)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_linear_svc)","24666a3b":"# Stochastic Gradient Descent\n\ntrain_pred_sgd, acc_sgd, acc_cv_sgd = fit_ml_algo(SGDClassifier(), \n                                                  X_train, \n                                                  y_train,\n                                                  10)\n\nprint(\"Accuracy: %s\" % acc_sgd)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_sgd)","ef62ff4c":"# SVM\n\ntrain_pred_svm, acc_linear_svm, acc_cv_linear_svm = fit_ml_algo(SVC(),\n                                                                X_train, \n                                                                y_train, \n                                                                10)\n\nprint(\"Accuracy: %s\" % acc_linear_svm)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_linear_svm)","3e0c60cf":"# Decision Tree Classifier\n\ntrain_pred_decision, acc_linear_decision, acc_cv_linear_decision = fit_ml_algo(DecisionTreeClassifier(),\n                                                                X_train, \n                                                                y_train, \n                                                                10)\n\nprint(\"Accuracy: %s\" % acc_linear_decision)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_linear_decision)","7a68cb70":"## Checking the importance of features\n\nfrom sklearn.ensemble import RandomForestClassifier \nmodel= RandomForestClassifier(n_estimators=100,random_state=0)\nX=df[df.columns[:8]]\nY=df['Outcome']\nmodel.fit(X,Y)\npd.Series(model.feature_importances_,index=X.columns).sort_values(ascending=False)","ec0f1ae3":"df=df[['Glucose','BMI','DiabetesPedigreeFunction','Age','Outcome']]\ndf.head()","bd506f00":"# Scaling the data\n\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\n\n\nX =  pd.DataFrame(sc_X.fit_transform(df.drop([\"Outcome\"],axis = 1),),columns=['Glucose','BMI','DiabetesPedigreeFunction','Age'])\nX.head()","824ffa6e":"## Test and train data\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=1\/3,random_state=42, stratify=y)","bffbae70":"# Logistic Regression\n\ntrain_pred_log, acc_log, acc_cv_log = fit_ml_algo(LogisticRegression(), \n                                                               X_train, \n                                                               y_train, \n                                                                    10)\n\nprint(\"Accuracy: %s\" % acc_log)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_log)","356d2bdc":"# k-Nearest Neighbours\n\ntrain_pred_knn, acc_knn, acc_cv_knn = fit_ml_algo(KNeighborsClassifier(), \n                                                  X_train, \n                                                  y_train, \n                                                  10)\n\nprint(\"Accuracy: %s\" % acc_knn)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_knn)","3f36b23c":"# Linear SVC\n\ntrain_pred_svc, acc_linear_svc, acc_cv_linear_svc = fit_ml_algo(LinearSVC(),\n                                                                X_train, \n                                                                y_train, \n                                                                10)\n\nprint(\"Accuracy: %s\" % acc_linear_svc)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_linear_svc)","08766258":"# Stochastic Gradient Descent\n\ntrain_pred_sgd, acc_sgd, acc_cv_sgd = fit_ml_algo(SGDClassifier(), \n                                                  X_train, \n                                                  y_train,\n                                                  10)\n\nprint(\"Accuracy: %s\" % acc_sgd)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_sgd)","f3ad1850":"# SVM\n\ntrain_pred_svm, acc_linear_svm, acc_cv_linear_svm = fit_ml_algo(SVC(),\n                                                                X_train, \n                                                                y_train, \n                                                                10)\n\nprint(\"Accuracy: %s\" % acc_linear_svm)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_linear_svm)","232e105e":"# Decision Tree Classifier\n\ntrain_pred_decision, acc_linear_decision, acc_cv_linear_decision = fit_ml_algo(DecisionTreeClassifier(),\n                                                                X_train, \n                                                                y_train, \n                                                                10)\n\nprint(\"Accuracy: %s\" % acc_linear_decision)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_linear_decision)","1605935c":"model_params = {\n    'svm': {\n        'model': SVC(gamma='auto'),\n        'params' : {\n            'C': [1,10,20],\n            'kernel': ['rbf','linear']\n        }  \n    },\n    'logistic_regression' : {\n        'model': LogisticRegression(solver='liblinear',multi_class='auto'),\n        'params': {\n            'C': [1,5,10]\n        }\n    }\n}","cb0a08d7":"from sklearn.model_selection import GridSearchCV\n\nscores = []\n\nfor model_name, mp in model_params.items():\n    clf =  GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False)\n    clf.fit(X_train,y_train)\n    scores.append({\n        'model': model_name,\n        'best_score': clf.best_score_,\n        'best_params': clf.best_params_\n    })\n    \nmodel_df = pd.DataFrame(scores,columns=['model','best_score','best_params'])\nmodel_df","93d871fb":"# **We now see that the models which are consistently having the greatest accurace are SVM (Linear and Radial) and Logistic Regression. So we will restrict to compare these. **","d84a7d8d":"# **Based on above, we can conclude that the best model is Logistic Regression with C as 1**","ae4a986f":"# **Data Descriptions:**\n\n* Pregnancies = No. of times a pregnancy has occurred\n* Glucose = Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n* BloodPressure = Diastolic blood pressure (mm Hg)\n* SkinThickness = Triceps skin fold thickness (mm)\n* Insuling = 2-Hour serum insulin (mu U\/ml)\n* BMI = Body mass index (weight in kg\/(height in m)^2)\n* DiabetesPedigreeFunction = Diabetes pedigree function\n* Age = Age (in years)\n* Outcome = Class variable (0 or 1) 268 of 768 are 1, the others are 0","a3eaf230":"# **We can see that the most important features are Glucose, BMI, Age, Diabates Pedigree Function. So we will repeat the steps above with the above features only**"}}