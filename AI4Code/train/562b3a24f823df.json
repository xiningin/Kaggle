{"cell_type":{"f3674d69":"code","4e7bc2a8":"code","a71dbd3f":"code","26534a80":"code","60dc39de":"code","c9c4f1a8":"code","b4ba8446":"code","c6487091":"code","1b7e26c1":"code","b72ba258":"code","79bfc5dd":"code","a12c7cc3":"code","042d6d6e":"code","f9805fcb":"code","841ec861":"code","4b00c89d":"code","95d75aca":"code","9757de94":"code","a6443b60":"code","065a61ca":"code","fd453032":"code","821dfe9b":"code","2734cfe8":"markdown","26c94478":"markdown","c3057974":"markdown","a9fcace3":"markdown","5b82c048":"markdown","c226ba95":"markdown","023b99f2":"markdown","b62242ee":"markdown","1ccdab24":"markdown","477238a0":"markdown"},"source":{"f3674d69":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4e7bc2a8":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as mn","a71dbd3f":"from warnings import filterwarnings\nfilterwarnings('ignore')","26534a80":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom lightgbm import LGBMClassifier","60dc39de":"df = pd.read_csv('\/kaggle\/input\/star-type-classification\/Stars.csv')\ndf","c9c4f1a8":"mn.matrix(df)","b4ba8446":"df.describe()","c6487091":"df['logL'] = np.log1p(df['L'])\ndf['logR'] = np.log1p(df['R'])","1b7e26c1":"df['Color'] = df['Color'].replace(['White','Whitish'],'white')\ndf['Color'] = df['Color'].replace(['Blue White','Blue-White','Blue white','Blue-white'],'blue_white')\ndf['Color'] = df['Color'].replace(['Red'],'red')\ndf['Color'] = df['Color'].replace(['Yellowish White','yellow-white','White-Yellow'],'yellow_white')\ndf['Color'] = df['Color'].replace(['yellowish','Yellowish'],'yellow')\ndf['Color'] = df['Color'].replace(['Blue'],'blue')\ndf['Color'] = df['Color'].replace(['Orange'],'orange')\ndf['Color'] = df['Color'].replace(['Pale yellow orange'],'pale_yellow_orange')\ndf['Color'] = df['Color'].replace(['Orange-Red'],'orange_red')","b72ba258":"color_dict = {'red':0, 'blue':1, 'blue_white':2, 'white':3, 'yellow_white':4, 'yellow':5, 'orange':6, \n              'pale_yellow_orange':5, 'orange_red':6}\ndf['Color'] = df['Color'].map(color_dict)","79bfc5dd":"spec_dict = {'M':1, 'B': 2, 'O':3, 'A':4, 'F':5, 'K':6, 'G':7}\ndf['Spectral_Class'] = df['Spectral_Class'].map(spec_dict)\ndf.drop(columns=['L','R'], inplace = True)\ndf","a12c7cc3":"fig,ax = plt.subplots(3,2, figsize=(10,10))\n\nax[0,0].plot(df['Temperature'],'r')\nax[0,0].set_title('Temperature')\nax[0,1].plot(df['logL'],'g')\nax[0,1].set_title('logL')\nax[1,0].plot(df['logR'],'y')\nax[1,0].set_title('logR')\nax[1,1].plot(df['A_M'],'b')\nax[1,1].set_title('A_M')\nax[2,0].plot(df['Color'],'grey')\nax[2,0].set_title('Color')\nax[2,1].plot(df['Spectral_Class'],'black')\nax[2,1].set_title('Spectral_Class')\n\nfig.tight_layout()\nplt.show()","042d6d6e":"p = sns.pairplot(data=df,hue='Type')","f9805fcb":"h = sns.heatmap(df.corr(), annot=True)","841ec861":"X = df.drop(['Type'], axis = 1)\nY = df['Type']\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=42)","4b00c89d":"models = {'LogisticRegression': LogisticRegression(max_iter=10000),\n          'KNeighborsClassifier': KNeighborsClassifier(),\n          'SVC': SVC(),\n          'DecisionTreeClassifier': DecisionTreeClassifier(),\n          'RandomForestClassifier': RandomForestClassifier(),\n          'AdaBoostClassifier': AdaBoostClassifier(),\n          'GradientBoostingClassifier': GradientBoostingClassifier(),\n          'LGBMClassifier':LGBMClassifier()}","95d75aca":"def fit_score(models, X_train, X_test, y_train, y_test):\n    np.random.seed(42)\n    model_scores = {}\n    \n    for model_name, model in models.items():\n        model.fit(X_train,y_train)\n        model_scores[model_name] = model.score(X_test,y_test)\n\n    model_scores = pd.DataFrame(model_scores, index=['Score']).transpose()\n    model_scores = model_scores.sort_values('Score')\n        \n    return model_scores","9757de94":"model_scores = fit_score(models, X_train, X_test, Y_train, Y_test)\nmodel_scores","a6443b60":"model = LogisticRegression(max_iter=10000)\nmodel.fit(X_train, Y_train)\ny_preds = model.predict(X_test)","065a61ca":"from sklearn.model_selection import cross_val_score","fd453032":"def cv_score(model, X, Y, cv=5):\n    np.random.seed(42)\n    cv_mean={}\n    cv_acc = cross_val_score(model,X,Y,cv=cv,scoring='accuracy')\n    cv_mean['cross_validation_mean'] = cv_acc.mean()\n    return cv_mean","821dfe9b":"cv_mean = cv_score(model, X_train, Y_train, cv=10)\ncv_mean","2734cfe8":"### Checking for null values\n\nDataset doesnt have any null values ","26c94478":"## Logistic regression model gives a cross validation accuracy of 98%","c3057974":"# Star Type Classification\n\n### Features\n\n1. Temperature (K)\n2. Relative Luminosity (L\/Lo)\n3. Relative Radius (R\/Ro)\n4. AM (Mv)\n5. Color => General Color of Spectrum\n6. Spectral_Class => O,B,A,F,G,K,M\n\n### Target\n\n* Red Dwarf - 0\n* Brown Dwarf - 1\n* White Dwarf - 2\n* Main Sequence - 3\n* Super Giants - 4\n* Hyper Giants - 5\n\n##### MATH:\n\n* Lo = 3.828 x 10^26 Watts (Avg Luminosity of Sun)\n* Ro = 6.9551 x 10^8 m (Avg Radius of Sun)","a9fcace3":"## Reading data and exploration","5b82c048":"## Model Training","c226ba95":"### Cross validiation on Logistic Regression","023b99f2":"## Data Visualization","b62242ee":"### Logistic Regression\n* From above data we know Logistic Regression has best score (97.5%). Models having score 1 are overfitting","1ccdab24":"### Data cleaning\n* Using logarithmic scale on L and R as they are very highly spreaded\n* Cleaning the 'Color' column\n* Assigning numerical values to 'Colors' and 'Spectral_Class' columns","477238a0":"## Import libraries"}}