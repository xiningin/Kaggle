{"cell_type":{"3610c752":"code","15915b96":"code","9be67ad4":"code","601ddfa9":"code","97d9f7e3":"code","923e7a06":"code","8f0923b2":"code","60f96f6e":"code","8a5ae1cf":"code","6d0ad327":"code","c3418684":"code","f623c7f7":"code","18bcd6d9":"code","9d77a859":"code","112ea97f":"code","79445083":"code","1afb4f17":"code","217c21ff":"code","9013b616":"code","7d860a9e":"code","18b1ac16":"code","73b6871b":"code","b6565473":"code","e67cbda3":"code","71e52492":"code","13c1c18a":"code","2658d021":"code","1e1b3e36":"code","0fd56ef0":"code","34e85e69":"code","943c5755":"code","f87dc0f2":"code","5da33bb4":"code","c2ddfa0d":"code","238174ca":"code","4314fb67":"code","a166301a":"code","e5b32b8e":"code","5e7e1bfb":"markdown","894098dd":"markdown","67a90fe2":"markdown","569f81aa":"markdown","cef5d1ef":"markdown","30ad3ef1":"markdown","45f10407":"markdown","86541e5a":"markdown","0dbc3587":"markdown","5a28bab6":"markdown","c1b63146":"markdown","b6af893d":"markdown","acfe83a7":"markdown","bc2b1feb":"markdown","c9a86395":"markdown","31d0a20c":"markdown","d6783c88":"markdown","c3cdef63":"markdown","269cb79e":"markdown","abe9b564":"markdown","2c9da63c":"markdown","04b0eac7":"markdown","9283a9bc":"markdown","60af747d":"markdown","a1d123f9":"markdown","8e52590c":"markdown","30ca64c1":"markdown"},"source":{"3610c752":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","15915b96":"import string","9be67ad4":"df = pd.read_json('..\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset_v2.json', \n                 lines = True)\ndf.head()","601ddfa9":"df.isnull().sum()","97d9f7e3":"df.shape","923e7a06":"s = df['headline'][0]\ns","8f0923b2":"(x.lower() for x in s.split())","60f96f6e":"df['headline'] = df['headline'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\ndf['headline']","8a5ae1cf":"string.punctuation","6d0ad327":"n = 'hello It is me Mario What is your name'\n[ch for ch in n]","c3418684":"# Replacing occurrences for sample data by replacing\ntext=['This is introduction to NLP','It is likely to be useful, to people',\n      'Machine learning is the new electrcity',\n      'There would be less hype around AI and more action going forward',\n      'python is the best tool!',\n      'R is good langauage',\n      'I like this book','I want more books like this']\nprint([\" \" if ch in string.punctuation else ch for n in text for ch in n])","f623c7f7":"# Mapping punctuation to none\nstr.maketrans('', '', string.punctuation)","18bcd6d9":"# Even better to work with\np = str.maketrans('', '', string.punctuation)\ntext[1].translate(p)","9d77a859":"# Applying it to the complete dataframe\ndf['headline'] = df['headline'].apply(lambda x: x.translate(p))","112ea97f":"df['headline']","79445083":"# Working on an example first - text\ndf_testing = pd.DataFrame({'tweet':text})\ndf_testing.head()","1afb4f17":"import nltk\nfrom nltk.corpus import stopwords\n\nstop = stopwords.words('english')\ndf_testing['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))","217c21ff":"df['headline'] = df['headline'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))","9013b616":"df['headline'].head()","7d860a9e":"# Working with our sample data:\nimport re\nlookups = {'nlp':'natural language processing',\n'ur':'your', \"wbu\" : \"what about you\"}","18b1ac16":"# Using puncutation removal regex\ns = \"I. like. This book!\"\ns1 = re.sub(r'[^\\w\\s]',\"\",s)\ns1","73b6871b":"#Creating a function that returns the standardized text:\ndef text_std(input_text):\n words = input_text.split()\n new_words = []\n for word in words:\n    # Removing the punctuation\n     word = re.sub(r'[^\\w\\s]',\"\",word)\n     if word.lower() in lookups:\n         word = lookups[word.lower()]\n         new_words.append(word)\n         new_text = \" \".join(new_words)\n return new_text","b6565473":"text_std(\"I like nlp it's ur choice\")","e67cbda3":"# Our sample data\ndf_testing['tweet']","71e52492":"from textblob import TextBlob\ndf_testing['tweet'].apply(lambda x: str(TextBlob(x).correct()))","13c1c18a":"# Very expensive process\n#df['headline'] = df['headline'].apply(lambda x: str(TextBlob(x).correct()))","2658d021":"from nltk.tokenize import LineTokenizer, SpaceTokenizer\nfrom nltk import word_tokenize","1e1b3e36":"s = \"\"\"My name is Maximus Decimus Meridius, commander of the Armies of the North, General of the Felix Legions and loyal servant to the true emperor, Marcus Aurelius. \\nFather to a murdered son, husband to a murdered wife. \\nAnd I will have my vengeance, in this life or the next.\"\"\"","0fd56ef0":"lTokenizer = LineTokenizer()\nlTokenizer.tokenize(s)","34e85e69":"sTokenizer = SpaceTokenizer()\nsTokenizer.tokenize(s)","943c5755":"word_tokenize(s)","f87dc0f2":"text=['I like fishing','I eat fish','There are many fishes in the pond', 'leaves and leaf']\n#convert list to dataframe\nimport pandas as pd\ndf1 = pd.DataFrame({'tweet':text})\nprint(df1)","5da33bb4":"from nltk.stem import PorterStemmer\nst = PorterStemmer()\ndf1['tweet'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))","c2ddfa0d":"from nltk.stem import WordNetLemmatizer\nwn_l = WordNetLemmatizer()\ndf1['tweet'].apply(lambda x: \" \".join([wn_l.lemmatize(word) for word in x.split()]))","238174ca":"def processRow(row):\n    import re\n    import nltk\n    from textblob import TextBlob\n    from nltk.corpus import stopwords\n    from nltk.stem import PorterStemmer\n    from textblob import Word\n    from nltk.util import ngrams\n    import re\n    from wordcloud import WordCloud, STOPWORDS\n    from nltk.tokenize import word_tokenize\n    tweet = row\n    #Lower case\n    tweet.lower()\n    #Removes unicode strings like \"\\u002c\" and \"x96\"\n    tweet = re.sub(r'(\\\\u[0-9A-Fa-f]+)',r\"\", tweet)\n    tweet = re.sub(r'[^\\x00-\\x7f]',r\"\",tweet)\n    #convert any url to URL\n    tweet = re.sub('((www\\.[^\\s]+)|(https?:\/\/[^\\s]+))','URL',tweet)\n    #Convert any @Username to \"AT_USER\"\n    tweet = re.sub('@[^\\s]+','AT_USER',tweet)\n    #Remove additional white spaces\n    tweet = re.sub('[\\s]+', ' ', tweet)\n    tweet = re.sub('[\\n]+', ' ', tweet)\n    #Remove not alphanumeric symbols white spaces\n    tweet = re.sub(r'[^\\w]', ' ', tweet)\n    #Removes hastag in front of a word \"\"\"\n    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n    #Replace #word with word\n    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n    #Remove :( or :)\n    tweet = tweet.replace(':)',\"\")\n    tweet = tweet.replace(':(',\"\")\n    #remove numbers\n    tweet = \"\".join([i for i in tweet if not i.isdigit()])\n    #remove multiple exclamation\n    tweet = re.sub(r\"(\\!)\\1+\", ' ', tweet)\n    #remove multiple question marks\n    tweet = re.sub(r\"(\\?)\\1+\", ' ', tweet)\n    #remove multistop\n    tweet = re.sub(r\"(\\.)\\1+\", ' ', tweet)\n    #lemma\n    from textblob import Word\n    tweet =\" \".join([Word(word).lemmatize() for word in tweet.split()])\n    #stemmer\n    #st = PorterStemmer()\n    #tweet=\" \".join([st.stem(word) for word in tweet.split()])\n    #Removes emoticons from text\n    tweet = re.sub(':\\)|;\\)|:-\\)|\\(-:|:-D|=D|:P|xD|X-p|\\^\\^|:-*|\\^\\.\\^|\\^\\-\\^|\\^\\_\\^|\\,-\\)|\\)-:|:\\'\\(|:\\(|:-\\(|:\\S|T\\.T|\\.\\_\\.|:<|:-\\S|:-<|\\*\\-\\*|:O|=O|=\\-O|O\\.o|XO|O\\_O|:-\\@|=\/|:\/|X\\-\\(|>\\.<|>=\\(|D:', \"\", tweet)\n    #trim\n    tweet = tweet.strip('\\'\"')\n    row = tweet\n    return row","4314fb67":"sample_text = \"How to take control of your #debt https:\/\/personal.vanguard.com\/us\/insights\/saving-investing\/debt-management.#Best advice for #family #financial #success(@PrepareToWin)\"","a166301a":"processRow(sample_text)","e5b32b8e":"df['headline'].apply(lambda x: processRow(x))","5e7e1bfb":"Stop words are common words that add very little or mostly no context to the sentence. We can remove these general words to focus on the lower frequency important keywords. Examples of some stop words include \"how\", \"and\", \"the\" etc. This can be done through the NLTK library","894098dd":"<h1>Introduction","67a90fe2":"This can be performed to correct any sort of typo errors or spelling errors committed in the dataset This will reduce multiple copies of the words, which represent the same meaning. \n\n**Note** - Abbreviations should be taken care of, before proceeding with spelling correction, otherwise there might be unexpected results. This is also a very expensive and time consuming process so it can be skipped, or done manually for large datasets:","569f81aa":"<h2>Removal of stop-words<\/h2>","cef5d1ef":"We will either be working with custom examples, or using the **news-headline-sarcasm dataset** for understanding real-world use cases.\n\nDataset link - https:\/\/www.kaggle.com\/rmisra\/news-headlines-dataset-for-sarcasm-detection?select=Sarcasm_Headlines_Dataset.json\n\nEach record consists of three attributes:\n\n* `is_sarcastic`: 1 if the record is sarcastic otherwise 0\n\n* `headline`: the headline of the news article\n\n* `article_link`: link to the original news article. Useful for collecting supplementary data","30ad3ef1":"<h2>Tokenization","45f10407":"This notebooks is a guide to preprocessing and cleaning before performing tasks such as Sentiment analysis, summary generation, next-word prediction etc. It will cover the basic concepts of NLP that include:\n<ul>\n    <li>Stemming<\/li>\n    <li>Lemmatization<\/li>\n    <li>Word embeddings<\/li>\n    <li>Bag of words models<\/li>\n   ","86541e5a":"This is what we mostly use, this breaks up words and punctuation marks separately","0dbc3587":"This step is usually performed after cleaning the text and removing any links, punctuation marks, spelling errors, emojis etc.","5a28bab6":"![image.png](attachment:9626d56e-cb4a-461d-b794-9d0fa9600ee4.png)","c1b63146":"<h2>1. Library and Data imports","b6af893d":"**Better method**\n\nUsing two main functions - `maketrans()` and `translate()`:\n\nHere we have the working of each\n\n1. The maketrans() method takes in three parameters and returns a mapping dictionary\n    * If only one is passed, then it should be a dictionary. It should contain a 1-1 mapping from a single character string to what it should translate to.\n    * If we have two arguments - it must be strings of equal length - First string is substituted by the positional characters in the other string.\n    * The third argument maps each character (supplied to it) as None\n    \n2. The translate() method returns a string where some specified characters are replaced with characters described in a dictionary or mapping table. The output of maketrans() is passed into this function.","acfe83a7":"One of the methods using `string.punctuation` and replacing occurrences","bc2b1feb":"<h2>Stop-word removal","c9a86395":"Our key column of interest is headlines - Checking missing data","31d0a20c":"<h2>Spelling correction","d6783c88":"Our text can contain short forms and abbreviations, this is even more common in customer reviews, blogs, tweets etc. where such slangs and acronyms are dominant. We should try reducing them back to their original form, to streamline the process better: It is always good to understand what acronyms may be present in your dataset before starting the process of cleaning","c3cdef63":"As we can see, it has successfully removed the stop-words. Repeating the same process on our original data now","269cb79e":"<h2>Standardization of text","abe9b564":"![image.png](attachment:7816aeea-129f-49e7-8f3b-f572ee4fe955.png)","2c9da63c":"**Lower-case conversion to prevent conflicts** - Using the apply function, we will first split the text using the whitespace character, convert each word into lower-case and then return a generator function to be joined together through whitespace.\n\nJoin is a method of strings. That method takes any iterable and iterates over it and joins the contents together. (The contents have to be strings, or it will raise an exception.) If you attempt to write the generator object directly to the file, you will just get the generator object itself, not its contents. join \"unrolls\" the contents of the generator.","04b0eac7":"We will be exploring the NLTK tokenizers for now, we'll be taking a look at the Keras tokenizer during our model creation and training process.","9283a9bc":"<h2>Basic Data cleaning","60af747d":"Tokenizing sentences","a1d123f9":"<h2>An example data pre-processing pipeline","8e52590c":"Tokenizing with space separator","30ca64c1":"<h2>Stemming and Lemmatization<\/h2>"}}