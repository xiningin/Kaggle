{"cell_type":{"78590704":"code","374e98b1":"code","c4b6f9ed":"code","2a052583":"code","dde75341":"code","71bc9567":"code","a7ee412d":"code","7e665c6c":"code","f628d3ee":"code","1a43500d":"code","695e3fdf":"code","006f5454":"code","50b1db0c":"code","88842552":"code","19ecc8e4":"code","218bfd06":"code","6d20a3e2":"code","adb61aa2":"code","7c14e957":"code","0cd1412b":"code","e4cda04c":"markdown","f4e63a61":"markdown","20e2cc07":"markdown","632a5bbb":"markdown","93f72618":"markdown","3e6ce83f":"markdown","bbd047cd":"markdown","ec654092":"markdown","ad6523cf":"markdown","43ded52f":"markdown"},"source":{"78590704":"import os, sys\nimport numpy as np\nimport torch\nimport cv2\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches","374e98b1":"print(\"PyTorch version:\", torch.__version__)\nprint(\"CUDA version:\", torch.version.cuda)\nprint(\"cuDNN version:\", torch.backends.cudnn.version())","c4b6f9ed":"gpu = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ngpu","2a052583":"def plot_detections(img, detections, with_keypoints=True):\n    fig, ax = plt.subplots(1, figsize=(10, 10))\n    ax.grid(False)\n    ax.imshow(img)\n    \n    if isinstance(detections, torch.Tensor):\n        detections = detections.cpu().numpy()\n\n    if detections.ndim == 1:\n        detections = np.expand_dims(detections, axis=0)\n\n    print(\"Found %d faces\" % detections.shape[0])\n        \n    for i in range(detections.shape[0]):\n        ymin = detections[i, 0] * img.shape[0]\n        xmin = detections[i, 1] * img.shape[1]\n        ymax = detections[i, 2] * img.shape[0]\n        xmax = detections[i, 3] * img.shape[1]\n\n        rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n                                 linewidth=1, edgecolor=\"r\", facecolor=\"none\", \n                                 alpha=detections[i, 16])\n        ax.add_patch(rect)\n\n        if with_keypoints:\n            for k in range(6):\n                kp_x = detections[i, 4 + k*2    ] * img.shape[1]\n                kp_y = detections[i, 4 + k*2 + 1] * img.shape[0]\n                circle = patches.Circle((kp_x, kp_y), radius=0.5, linewidth=1, \n                                        edgecolor=\"lightskyblue\", facecolor=\"none\", \n                                        alpha=detections[i, 16])\n                ax.add_patch(circle)\n        \n    plt.show()","dde75341":"import sys\nsys.path.insert(0, \"\/kaggle\/input\/blazeface-pytorch\")","71bc9567":"from blazeface import BlazeFace\n\nnet = BlazeFace().to(gpu)\nnet.load_weights(\"\/kaggle\/input\/blazeface-pytorch\/blazeface.pth\")\nnet.load_anchors(\"\/kaggle\/input\/blazeface-pytorch\/anchors.npy\")\n\n# Optionally change the thresholds:\nnet.min_score_thresh = 0.75\nnet.min_suppression_threshold = 0.3","a7ee412d":"input_dir = \"\/kaggle\/input\/deepfake-detection-challenge\/test_videos\"\nvideo_path = os.path.join(input_dir, np.random.choice(os.listdir(input_dir)))\nvideo_path","7e665c6c":"video_path = \"\/kaggle\/input\/deepfake-detection-challenge\/test_videos\/uhrqlmlclw.mp4\"","f628d3ee":"def read_frame(video_path):\n    capture = cv2.VideoCapture(video_path)\n    ret, frame = capture.read()\n    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame = cv2.resize(frame, (128, 128))\n    capture.release()\n    return frame","1a43500d":"frame = read_frame(video_path)","695e3fdf":"plt.imshow(frame)","006f5454":"%time detections = net.predict_on_image(frame)\ndetections.shape","50b1db0c":"detections","88842552":"plot_detections(frame, detections)","19ecc8e4":"frame1 = read_frame(\"\/kaggle\/input\/deepfake-detection-challenge\/test_videos\/jyfvaequfg.mp4\")\nframe2 = read_frame(\"\/kaggle\/input\/deepfake-detection-challenge\/test_videos\/gkutjglghz.mp4\")\n\nbatch = np.stack([frame1, frame2])\nbatch.shape","218bfd06":"%time detections = net.predict_on_batch(batch)","6d20a3e2":"len(detections)","adb61aa2":"[x.shape[0] for x in detections]","7c14e957":"plot_detections(frame1, detections[0])","0cd1412b":"plot_detections(frame2, detections[1])","e4cda04c":"Helper code for making plots:","f4e63a61":"## Load an image\n","20e2cc07":"## Make the prediction","632a5bbb":"# Face detection with BlazeFace\n\nThis notebook shows how to use the model from [BlazeFace PyTorch](https:\/\/www.kaggle.com\/humananalog\/blazeface-pytorch) for detecting faces in images.","93f72618":"## Load the model","3e6ce83f":"This returns a PyTorch tensor of shape `(num_faces, 17)`. \n\nHow to interpret these 17 numbers:\n\n- The first 4 numbers describe the bounding box corners: \n    - `ymin, xmin, ymax, xmax`\n    - These are normalized coordinates (between 0 and 1).\n    - Note that y comes before x here!\n- The next 12 numbers are the x,y-coordinates of the 6 facial landmark keypoints:\n    - `right_eye_x, right_eye_y`\n    - `left_eye_x, left_eye_y`\n    - `nose_x, nose_y`\n    - `mouth_x, mouth_y`\n    - `right_ear_x, right_ear_y`\n    - `left_ear_x, left_ear_y`\n    - Tip: these labeled as seen from the perspective of the person, so their right is your left.\n- The final number is the confidence score that this detection really is a face.\n\nIf no faces are found, the tensor has shape `(0, 17)`.","bbd047cd":"For the best results, enable GPU in the notebook. But CPU should work fine as well.","ec654092":"The batch prediction returns a list of PyTorch tensors, one for each image in the batch.","ad6523cf":"## Prediction on a batch","43ded52f":"The input image should be 128x128. BlazeFace will not automatically resize the image, you have to do this yourself!\n\nThe images from the Deepfake Detection Challenge dataset are 1920x1080, so resizing to 128x128 will squash them a bit. The version of BlazeFace we're using here doesn't work very well on small faces, so you may need to be more clever about how you resize\/crop the input images."}}