{"cell_type":{"740c3e8c":"code","1ecc305b":"code","1d034e42":"code","15ca552e":"code","202e4c6e":"code","70730ec6":"code","64af7bb6":"code","e1c72d00":"code","95a964ca":"code","275ae0bb":"code","e282130d":"code","251c025d":"code","911aa1a5":"code","fdfe80c6":"code","86faac90":"code","e17b2cbd":"code","f0a62bae":"code","0af34fd6":"code","dc65919d":"code","5dae2ab1":"code","34afdb1a":"code","5821f8e2":"markdown","7fd471f9":"markdown","859a43e9":"markdown","91dc7827":"markdown","6c2a2075":"markdown"},"source":{"740c3e8c":"import os\nimport numpy as np\nimport pandas as pd\nfrom time import time\nfrom tqdm import tqdm\nimport gc\n\nimport lightgbm as lgb\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LogisticRegression\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom __future__ import absolute_import, division, print_function\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\n%matplotlib inline","1ecc305b":"train = pd.read_csv('..\/input\/train.csv').fillna(' ')\ntest = pd.read_csv('..\/input\/test.csv').fillna(' ')","1d034e42":"test_text = test['question_text'].values.tolist()","15ca552e":"start_time = time()\nembed = hub.Module(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/2\")\ntime() - start_time","202e4c6e":"embeddings = embed(test_text)","70730ec6":"with tf.Session() as session:\n    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n    test_embeddings = session.run(embeddings)","64af7bb6":"test_embeddings.shape","e1c72d00":"train_text = train['question_text'].values.tolist()","95a964ca":"len(train_text)\/25","275ae0bb":"train_text = [train_text[i:i + 52250] for i in range(0, len(train_text), 52250)]\n","e282130d":"len(train_text)\n","251c025d":"embeddings_train = []\nfor i in tqdm(range(25)):\n    embeddings = embed(train_text[i])\n    embeddings_train.append(embeddings)","911aa1a5":"train_embeddings_all = []\nwith tf.Session() as session:\n    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n    for i in tqdm(range(25)):\n        train_embeddings = session.run(embeddings_train[i])\n        train_embeddings_all.append(train_embeddings)","fdfe80c6":"del train_text, test_text\ngc.collect()","86faac90":"train_embeddings_all = np.vstack(train_embeddings_all)","e17b2cbd":"train_embeddings_all.shape","f0a62bae":"train_target = train['target'].values\ndel train, test\ngc.collect()","0af34fd6":"kf = KFold(n_splits=5, shuffle=True, random_state=43)\ntest_pred_tf = 0\noof_pred_tf = np.zeros([train_embeddings_all.shape[0],])\n\nfor i, (train_index, val_index) in tqdm(enumerate(kf.split(train_embeddings_all))):\n    x_train, x_val = train_embeddings_all[train_index,:], train_embeddings_all[val_index,:]\n    y_train, y_val = train_target[train_index], train_target[val_index]\n    classifier = LogisticRegression(class_weight = \"balanced\", C=0.5, solver='sag')\n    classifier.fit(x_train, y_train)\n    val_preds = classifier.predict_proba(x_val)[:,1]\n    preds = classifier.predict_proba(test_embeddings)[:,1]\n    test_pred_tf += 0.2*preds\n    oof_pred_tf[val_index] = val_preds","dc65919d":"np.save('train_embeddings_all', train_embeddings_all)\nnp.save('test_embeddings', test_embeddings)\nnp.save('train_target', train_target)","5dae2ab1":"pred_train = (oof_pred_tf > 0.8).astype(np.int)\nf1_score(train_target, pred_train)","34afdb1a":"test = pd.read_csv('..\/input\/test.csv').fillna(' ')\npred_test = (test_pred_tf> 0.8).astype(np.int)\nsubmission = pd.DataFrame.from_dict({'qid': test['qid']})\nsubmission['prediction'] = pred_test\nsubmission.to_csv('submission.csv', index=False)","5821f8e2":"We'd like to chunk it into smaller segments, each of which is approximatley the same lenght as the test text. Turns out that if we aim for about 25 chunks we can get there:","7fd471f9":"Tensorflow hub provides a very nice [universal sentence encoder module](https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/2). Unfortunately, it can only bne accessed through an online connection, which is not permitted in this competition. I am still leaving the kernel here for educational purposes.","859a43e9":"And now we embed each chunk individually. This takes about 15 minutes. ","91dc7827":"Now we \"chunk\" it:","6c2a2075":"For train text we'll do somethign slightly different. In my previous attempts at emebdding large lists of texts in a Kaggle kernel I ran into mamory\/timout issues. So I decided to break the list into smaller chunks, end embed one of them at the time. We'll do something like that here too.\n\nFirst, let's extract the training text from the train file:"}}