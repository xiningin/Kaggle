{"cell_type":{"9b303337":"code","7a7dc371":"code","271710cb":"code","c4e41fda":"code","b5658861":"code","47172561":"code","25b564fe":"code","ff546706":"code","a52b274e":"code","65e79be2":"code","8e2242b3":"code","5b5735e0":"code","159910ae":"code","50f888d0":"code","2259012f":"code","063fd4e8":"code","35fc63c6":"code","4ac49111":"code","925c7b7f":"code","6e964ecd":"code","05875abd":"code","60316f6a":"code","7098080e":"code","6b072fe1":"code","196230fa":"code","b669faa1":"code","312b033d":"code","7eadcb65":"code","11ef2e07":"code","2f2ca8b9":"code","f83590b7":"code","d3d121cd":"code","cc627ff6":"code","07e45a81":"code","2e77ec1b":"code","fbd7cf42":"code","08b7f827":"code","342fc127":"code","6e209f72":"code","2e84de0d":"markdown","b9a5ea07":"markdown","c4413276":"markdown","54002633":"markdown","66f37c6a":"markdown","246a5fc7":"markdown","d824b2d9":"markdown","2e7e3104":"markdown","d165e8cc":"markdown","8766727a":"markdown","8c4dc5cb":"markdown","42994a84":"markdown"},"source":{"9b303337":"import warnings\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.max_columns', 100)\n%matplotlib inline","7a7dc371":"\"\"\"\nNote: \nmay functions included here are from Jeremy's MI courses (www.fastai.ai).\n\"\"\"\n\nimport os, io, platform, itertools, warnings\n\nimport pandas as pd\nimport numpy as np\nimport scipy\nfrom scipy.cluster import hierarchy as hc\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport plotly.figure_factory as ff\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\n\nfrom IPython.display import SVG,display\nfrom IPython.display import Image\nfrom PIL import  Image\n\n\nfrom sklearn.preprocessing import Imputer, PolynomialFeatures, StandardScaler\nfrom sklearn.feature_selection import VarianceThreshold, SelectFromModel\nfrom sklearn.model_selection import StratifiedKFold, KFold, cross_val_score, GridSearchCV, learning_curve, train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nimport lightgbm as lgb\nimport gc\nimport re\n\nfrom datetime import datetime\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype\n\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom graphviz import Source\n\n\npd.set_option('display.max_columns', 100)\n\n\ndef missing_data(df):\n    \"\"\"df: panda data frame\"\"\"\n    total = df.isnull().sum().sort_values(ascending=False)\n    percent = total \/ len(df) \n    return pd.concat([total,percent], axis=1, keys =['Total', 'Percent'])\n\ndef split_time(df, fldname, drop=True, date=True, time=False, errors='raise'):\n    fld = df[fldname]\n    fld_dtype = fld.dtype\n    if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n        fld_dtype = np.datetime64\n    if not np.issubdtype(fld_dtype, np.datetime64):\n        df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True, errors=errors)\n    targ_pre = re.sub('[Dd]ate$','',fldname)\n    attr = []\n    if date:\n        attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',\n            'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n    if time:\n        attr = attr + ['Hour', 'Minute', 'Second']\n    for n in attr:\n        df[targ_pre+n] = getattr(fld.dt, n.lower())\n    df[targ_pre+'Elapsed'] = fld.astype(np.int64) \/\/ 10**9\n    if drop:\n        df.drop(fldname, axis=1, inplace=True)\n\ndef is_date(x): return np.issubdtype(x.dtype, np.datetime64)\n\ndef cat_train(df):\n    for n, c in df.items():\n        if is_string_dtype(c):\n            df[n] = c.astype('category').cat.as_ordered()\n\ndef apply_cats(df, trn):    \n    for n,c in df.items():\n        if (n in trn.columns) and (trn[n].dtype.name=='category'):\n            df[n] = pd.Categorical(c, categories=trn[n].cat.categories, ordered=True)\n\n\ndef prep_df(df, y_fld=None, skip_flds=None, ignore_flds=None, do_scale=False, na_dict=None,\n            preproc_fn=None, max_n_cat=None, subset=None, mapper=None):    \n    if not ignore_flds: \n        ignore_flds=[]\n        \n    if not skip_flds: \n        skip_flds=[]\n        \n    if subset: \n        df = get_sample(df,subset)\n    else: \n        df = df.copy()\n        \n    ignored_flds = df.loc[:, ignore_flds]\n    df.drop(ignore_flds, axis=1, inplace=True)\n    \n    if preproc_fn: \n        preproc_fn(df)\n        \n    if y_fld is None: \n        y = None\n    else:\n        if not is_numeric_dtype(df[y_fld]): \n            df[y_fld] = df[y_fld].cat.codes\n            \n        y = df[y_fld].values\n        skip_flds += [y_fld]\n        \n    df.drop(skip_flds, axis=1, inplace=True)\n\n    if na_dict is None: \n        na_dict = {}\n    else: \n        na_dict = na_dict.copy()\n        \n    na_dict_initial = na_dict.copy()\n    \n    for n,c in df.items(): \n        na_dict = fix_missing(df, c, n, na_dict)\n    \n    if len(na_dict_initial.keys()) > 0:\n        df.drop([a + '_na' for a in list(set(na_dict.keys()) - set(na_dict_initial.keys()))], axis=1, inplace=True)\n    \n    if do_scale: \n        mapper = scale_vars(df, mapper)\n        \n    for n,c in df.items(): \n        numericalize(df, c, n, max_n_cat)\n    \n    df = pd.get_dummies(df, dummy_na=True)\n    df = pd.concat([ignored_flds, df], axis=1)\n    res = [df, y, na_dict]\n    \n    if do_scale: \n        res = res + [mapper]\n    \n    return res\n\ndef fix_missing(df, col, name, na_dict):    \n    if is_numeric_dtype(col):\n        if pd.isnull(col).sum() or (name in na_dict):\n            df[name+'_na'] = pd.isnull(col)\n            filler = na_dict[name] if name in na_dict else col.median()\n            df[name] = col.fillna(filler)\n            na_dict[name] = filler\n    return na_dict\n\ndef scale_vars(df, mapper):\n    warnings.filterwarnings('ignore', category=sklearn.exceptions.DataConversionWarning)\n    if mapper is None:\n        map_f = [([n],StandardScaler()) for n in df.columns if is_numeric_dtype(df[n])]\n        mapper = DataFrameMapper(map_f).fit(df)\n    df[mapper.transformed_names_] = mapper.transform(df)\n    return mapper\n\ndef numericalize(df, col, name, max_n_cat):\n    \n    if not is_numeric_dtype(col) and (max_n_cat is None or len(col.cat.categories)>max_n_cat):\n        df[name] = col.cat.codes+1\n\n\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\ndef rf_feat_importance(m, df):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}\n                       ).sort_values('imp', ascending=False)","271710cb":"churn_df = pd.read_csv('..\/input\/bigml_59c28831336c6604c800002a.csv')\nchurn_df.head()","c4e41fda":"print (\"Num of rows: \" + str(churn_df.shape[0])) # row count\nprint (\"Num of columns: \" + str(churn_df.shape[1])) # col count","b5658861":"churn_percent = churn_df['churn'].sum().astype(float)\/len(churn_df['churn'])\n'{:2.2%} of all customers tend to leave'.format(churn_percent)","47172561":"missing_data(churn_df)","25b564fe":"churn_df.dtypes","ff546706":"churn_df['area code'] = churn_df['area code'].astype(object)\nchurn_df['churn'] = np.where(churn_df['churn'] == True,1,0)","a52b274e":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n\nsb.distplot(churn_df['total intl charge'], kde=False)","65e79be2":"# Select the numeric columns\ncols = ['account length', 'number vmail messages',\n       'total day minutes', 'total day calls', 'total day charge',\n       'total eve minutes', 'total eve calls', 'total eve charge',\n       'total night minutes', 'total night calls', 'total night charge',\n       'total intl minutes', 'total intl calls', 'total intl charge',\n       'customer service calls','churn']      \n\n# Find correlations with the sale price \ncorrelations = churn_df[cols].corr()","8e2242b3":"sb.heatmap(correlations)","5b5735e0":"cat_train(churn_df)","159910ae":"X, y, na_dict = prep_df(churn_df, 'churn')","50f888d0":"X.shape, na_dict","2259012f":"## keep 20% for test later\nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\ntrain_index, test_index = next(kfold.split(X, y))\nlen(train_index), len(test_index)","063fd4e8":"def split_test(X, y):\n    X_train = X.iloc[train_index]\n    y_train = y[train_index]\n\n    X_test = X.iloc[test_index]\n    y_test = y[test_index]\n    return X_train, y_train, X_test, y_test","35fc63c6":"from sklearn.ensemble import RandomForestClassifier\nX_train, y_train, X_test, y_test = split_test(X, y)\nm = RandomForestClassifier(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)","4ac49111":"def confusion_df(y_test,pred):\n    return pd.DataFrame(confusion_matrix(y_test,pred),\n             columns=[\"Predicted Class \" + str(class_name) for class_name in [0,1]],\n             index = [\"Class \" + str(class_name) for class_name in [0,1]])","925c7b7f":"m.fit(X_train, y_train)\npred = m.predict(X_test)\nprint(confusion_df(y_test, pred))","6e964ecd":"fi = rf_feat_importance(m, X); fi[:10]","05875abd":"def plot_fi(fi): return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)","60316f6a":"plot_fi(fi);","7098080e":"to_keep = fi[fi.imp>0.005].cols; len(to_keep)\nX_keep = X[to_keep].copy()","6b072fe1":"X2, y, _ = prep_df(churn_df, 'churn', max_n_cat=7)\n\nX_train2, y_train, X_test2, y_test = split_test(X2, y)\n\nm.fit(X_train2, y_train)","196230fa":"pred = m.predict(X_test2)\nprint(confusion_df(y_test, pred))","b669faa1":"fi = rf_feat_importance(m, X2)\nplot_fi(fi);","312b033d":"corr = np.round(scipy.stats.spearmanr(X_keep).correlation, 4)\ncorr_condensed = hc.distance.squareform(1-corr)\nz = hc.linkage(corr_condensed, method='average')\nfig = plt.figure(figsize=(16,10))\ndendrogram = hc.dendrogram(z, labels=X_keep.columns, orientation='left', leaf_font_size=16)\nplt.show()","7eadcb65":"def get_oob(X, y):\n    m = RandomForestClassifier(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)   \n    m.fit(X, y)\n    return m.oob_score_","11ef2e07":"get_oob(X_keep, y)","2f2ca8b9":"for c in ('total day minutes','total day charge', 'total eve minutes','total eve charge',\n          'total intl minutes','total intl charge', 'total night minutes','total night charge', 'voice mail plan',\n       'number vmail messages'):\n    print(c, get_oob(X_keep.drop(c, axis=1), y))","f83590b7":"to_drop = ['total day minutes', 'total eve minutes', 'total intl minutes', 'total night minutes', 'voice mail plan']\nprint(get_oob(X_keep.drop(to_drop, axis=1), y))","d3d121cd":"X_keep.drop(to_drop, axis=1, inplace=True)\nX_keep.columns","cc627ff6":"X_train, y_train, X_test, y_test = split_test(X_keep, y)","07e45a81":"X_test.shape, X_train.shape","2e77ec1b":"### Find Optimal Parameters: Hyperopt\nfrom hyperopt import fmin, tpe, hp, STATUS_OK, Trials,  space_eval\n\nparam_space = {\n    'max_depth': hp.choice('max_depth', range(1,10)),\n    'max_features': hp.choice('max_features', ['auto', 'sqrt', 0.5, 0.6, None]),\n    'n_estimators': 100,\n    'criterion': hp.choice('criterion', [\"gini\", \"entropy\"]),\n    # Minimum number of samples required to split a node\n    'min_samples_split': hp.choice('min_samples_split', range(5, 25)),\n     # Minimum number of samples required at each leaf node\n    'min_samples_leaf': hp.choice('min_samples_leaf', range(4, 25)),\n    'bootstrap': hp.choice('bootstrap', [True, False])\n    }\n\nbest_score = 0\n\ndef RF_score(params):\n    global best_score\n    clf = RandomForestClassifier(**params)\n    score = cross_val_score(clf, X_train, y_train, cv=3, scoring='recall_weighted').mean()    \n    if score > best_score:\n        best_score = score\n        print('score:', score)\n        print('params:', params)\n    return {'loss': -score, 'status': STATUS_OK}   \n\n\ntrials = Trials()\nbest = fmin(RF_score, param_space, algo=tpe.suggest, max_evals=100, trials=trials)\nprint('best parameters:')\nbest_params = space_eval(param_space, best)\nprint(best_params)  ","fbd7cf42":"model = RandomForestClassifier(**best_params)\ng = plot_learning_curve(model, \"Learning curve\", X_train, y_train, cv=5)","08b7f827":"model.fit(X_train, y_train)\n\nprediction = model.predict(X_test)\nprint(confusion_df(y_test, prediction))","342fc127":"plot_fi(rf_feat_importance(model, X_keep));","6e209f72":"rf_feat_importance(model, X_keep)","2e84de0d":"### Removing redundant features","b9a5ea07":"## Part3: Important Feature Selection ","c4413276":"### Part 1.3: Understand the features","54002633":"### Part 1.2: Data cleaning \nmissing data, outliers, correct data types etc.","66f37c6a":"## Part 4: Final Model","246a5fc7":"### Part 1.1: Load Dataset","d824b2d9":"## Part 1: EDA","2e7e3104":"## Part 2: Model training and evaluation","d165e8cc":"# Customer Churn Prediction\n\nAs customer recruitment is normally expensive, losing customers might be a nightmare for a business. \n\nIn churn management, it is important to\n- identify customers who are likely to stop using a service\n- identify factors which trigger the decision\n\nIn this project, I build a model based on the random forest method which can be used to serve the two goals mentioned above. The accuracy of the model predictions reached up to 95%. Among all customers, 14.5% tend to churn. The model could successfully identify 70-75% of them. Important factors (ranked) are: total day charge, customer service calls, total evening charge, international plan.  ","8766727a":" ### Content\n- Part 1: Exploratory data analysis (EDA)\n- Part 2: Model training and evaluation\n- Part 3: Important features selection\n- Part 4: Final Model\n\nData source: https:\/\/www.kaggle.com\/becksddf\/churn-in-telecoms-dataset","8c4dc5cb":"### Model evaluation: confusion matrix","42994a84":"### check overfitting"}}