{"cell_type":{"6f2a033e":"code","0e8bb218":"code","9f8a645d":"code","5e2914ee":"code","4ad1cef6":"code","1af07b37":"code","371e5726":"code","464dd5a2":"code","591c70f7":"code","086468b8":"code","61c72d29":"code","87b4ed3e":"code","04a686f1":"code","50d79e71":"code","77eb3497":"code","bf9d4ed7":"code","10dab8c1":"code","d636020c":"code","4142cc55":"code","973786e9":"markdown"},"source":{"6f2a033e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","0e8bb218":"import matplotlib.pyplot as plt\nimport tensorflow as tf","9f8a645d":"train_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')\nprint(train_df.shape, test_df.shape)","5e2914ee":"train_df.head()","4ad1cef6":"X_train = train_df.iloc[:, 1:].values.T\ny_train = train_df.iloc[:, 0].values\nX_test = test_df.values.T","1af07b37":"index = 5\nplt.imshow(X_train[:, index].reshape(28, 28))\nprint(y_train[index])","371e5726":"y_train[:10]","464dd5a2":"#convert labels to one hot vector\ndef to_onehot(labels):\n    ones = tf.one_hot(labels, 10, axis=0)\n    with tf.Session() as sess:\n        ones = sess.run(ones)\n    return ones\n\n\ny_train = to_onehot(y_train)\ny_train[:, :10]","591c70f7":"X_train = X_train \/ 255\nX_test = X_test \/ 255","086468b8":"def initialize_parameters():\n    tf.set_random_seed(1)       \n    \n    W1 = tf.get_variable('W1', shape=[25, 784], initializer=tf.contrib.layers.xavier_initializer(seed=1),\n                          regularizer=tf.contrib.layers.l2_regularizer(0.8))\n    b1 = tf.get_variable('b1', shape=[25, 1], initializer=tf.contrib.layers.xavier_initializer(seed=1))\n    W2 = tf.get_variable('W2', shape=[15, 25], initializer=tf.contrib.layers.xavier_initializer(seed=1),\n                         regularizer=tf.contrib.layers.l2_regularizer(0.8))\n    b2 = tf.get_variable('b2', shape=[15, 1], initializer=tf.contrib.layers.xavier_initializer(seed=1))\n    W3 = tf.get_variable('W3', shape=[10, 15], initializer=tf.contrib.layers.xavier_initializer(seed=1),\n                         regularizer=tf.contrib.layers.l2_regularizer(0.8))\n    b3 = tf.get_variable('b3', shape=[10, 1], initializer=tf.contrib.layers.xavier_initializer(seed=1))\n\n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2,\n                  \"W3\": W3,\n                  \"b3\": b3}\n    \n    return parameters\n","61c72d29":"def forward_propagation(X, parameters):\n    W1 = parameters['W1']\n    b1 = parameters['b1']\n    W2 = parameters['W2']\n    b2 = parameters['b2']\n    W3 = parameters['W3']\n    b3 = parameters['b3']\n\n    Z1 = tf.add(tf.matmul(W1, X) , b1)                                          \n    A1 = tf.nn.relu(Z1)                                              \n    Z2 = tf.add(tf.matmul(W2, A1), b2)                                            \n    A2 = tf.nn.relu(Z2)                                              \n    Z3 = tf.add(tf.matmul(W3, A2), b3)                          \n    \n    return Z3","87b4ed3e":"def compute_cost(Z3, Y):\n    logits = tf.transpose(Z3)\n    labels = tf.transpose(Y)\n\n    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n    return cost\n","04a686f1":"def model(X_train, Y_train, learning_rate = 0.0005,\n          n_iterations = 2500, print_cost = True):\n    \n    tf.reset_default_graph()                         \n    tf.set_random_seed(1)                             \n    seed = 3                                          \n    (n_x, m) = X_train.shape                          \n    n_y = y_train.shape[0]                            \n    costs = []                                        \n    \n    X = tf.placeholder(tf.float32, shape=(n_x, None), name='X')\n    y = tf.placeholder(tf.float32, shape=(n_y, None), name='y')\n    parameters = initialize_parameters()\n \n    Z3 = forward_propagation(X, parameters)\n\n    cost = compute_cost(Z3, y)\n    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\n    init = tf.global_variables_initializer()\n    with tf.Session() as sess:\n        sess.run(init)\n        \n        # Do the training loop\n        for i in range(n_iterations):\n            _ , c = sess.run([optimizer, cost], feed_dict={X:X_train, y:y_train})\n            \n            # Print the cost every epoch\n            if print_cost == True and i % 100 == 0:\n                print (\"Cost after %i iterations: %f\" % (i, c))\n                costs.append(c)\n            \n                \n        # plot the cost\n        plt.plot(np.squeeze(costs))\n        plt.ylabel('cost')\n        plt.xlabel('iterations')\n        plt.title(\"Learning rate =\" + str(learning_rate))\n        plt.show()\n\n        # lets save the parameters in a variable\n        parameters = sess.run(parameters)\n        print (\"Parameters have been trained!\")\n\n        # Calculate the correct predictions\n        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(y))\n\n        # Calculate accuracy on the test set\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n\n        print (\"Train Accuracy:\", accuracy.eval({X: X_train, y: y_train}))\n        return parameters","50d79e71":"parameters = model(X_train, y_train)","77eb3497":"parameters","bf9d4ed7":"def predict(X, parameters):\n    \n    W1 = tf.convert_to_tensor(parameters[\"W1\"])\n    b1 = tf.convert_to_tensor(parameters[\"b1\"])\n    W2 = tf.convert_to_tensor(parameters[\"W2\"])\n    b2 = tf.convert_to_tensor(parameters[\"b2\"])\n    W3 = tf.convert_to_tensor(parameters[\"W3\"])\n    b3 = tf.convert_to_tensor(parameters[\"b3\"])\n    \n    params = {\"W1\": W1,\n              \"b1\": b1,\n              \"W2\": W2,\n              \"b2\": b2,\n              \"W3\": W3,\n              \"b3\": b3}\n    \n    x = tf.placeholder(\"float\", [784, None])\n    \n    z3 = forward_propagation(x, params)\n    p = tf.argmax(z3)\n    \n    with tf.Session() as sess:\n        prediction = sess.run(p, feed_dict = {x: X})\n        \n    return prediction","10dab8c1":"p = predict(X_test, parameters)","d636020c":"submission = pd.DataFrame({'ImageId':np.arange(1, X_test.shape[1]+1), \n                          'Label': p})","4142cc55":"from IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"forest.csv\"):  \n    csv = df.to_csv()\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n# create a random sample dataframe\n# create a link to download the dataframe\ncreate_download_link(submission)\n\n# \u2193 \u2193 \u2193  Yay, download link! \u2193 \u2193 \u2193 ","973786e9":"## Building a neural network with 2 hidden layers and 1 output layer"}}