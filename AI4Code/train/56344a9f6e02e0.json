{"cell_type":{"a4e5a212":"code","61ddd53b":"code","1fba48a6":"code","bed3df00":"code","42ade531":"code","e2a015e8":"code","8fac5e17":"code","1da8813f":"code","25155497":"code","1b813552":"code","ea624482":"code","52385d55":"code","e67e9322":"code","9bb47319":"code","2437cd66":"code","35f98b86":"code","209cb3af":"code","f3e61682":"code","9b754684":"code","bdf831b7":"code","224969dc":"code","97fb93ff":"code","ae8d827d":"code","f410133f":"code","8a365be3":"code","3b35797f":"code","c3e4e2ba":"code","532b7249":"code","e998d833":"code","a869ce46":"code","ed675451":"code","9c9003e3":"code","c7fae449":"code","61314259":"code","846a0640":"code","120adf3d":"code","0b3e6a84":"code","9a59fa92":"code","9834d72b":"code","d462fcee":"code","17e2b736":"code","dbd711b3":"code","812e8451":"code","1662ac98":"code","e9129282":"code","f05ec0b1":"code","8124d73a":"code","bf8835d6":"code","d44dcbf3":"code","242a0552":"markdown","eacba71d":"markdown","16e99637":"markdown","414f2dfc":"markdown","3fe9e634":"markdown","fbef812b":"markdown","164270bb":"markdown","90c54d00":"markdown"},"source":{"a4e5a212":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","61ddd53b":"df = pd.read_csv('..\/input\/predict-test-scores-of-students\/test_scores.csv')\ndf.head()","1fba48a6":"df.shape","bed3df00":"df.info()","42ade531":"df.isnull().sum()","e2a015e8":"y = df['posttest']\nX = df.drop('posttest', axis=1)","8fac5e17":"y.head()","1da8813f":"X.head()","25155497":"X.groupby('school').agg('count')","1b813552":"import seaborn as sns\nimport matplotlib.pyplot as plt","ea624482":"X.info()","52385d55":"object_columns = [col for col in X.columns if X[col].dtype == 'object']","e67e9322":"object_columns","9bb47319":"for i in object_columns:\n    print(i)\n    print(X[i].unique())\n    print('*'*60)","2437cd66":"X = X.drop('classroom', axis=1)","35f98b86":"X = X.drop('student_id', axis=1)\nX.head()","209cb3af":"object_cols = [col for col in X.columns if X[col].dtype=='object']","f3e61682":"from sklearn.preprocessing import LabelEncoder\nlabel_X = X.copy()\n\nlabel_encoder = LabelEncoder()\nfor col in object_cols:\n    label_X[col] = label_encoder.fit_transform(X[col])","9b754684":"label_X.head()","bdf831b7":"df2 = df.drop(['student_id', 'classroom'], axis=1)\ndf2.head()","224969dc":"label_encoder = LabelEncoder()\nfor col in object_cols:\n    df2[col] = label_encoder.fit_transform(X[col])","97fb93ff":"df2.head()","ae8d827d":"fig, ax = plt.subplots(figsize=(8,8))\nsns.heatmap(df2.corr(), annot=True, annot_kws={'size':9}, xticklabels=df2.columns, yticklabels=df2.columns, ax=ax)","f410133f":"df.head()","8a365be3":"plt.figure(figsize=(6, 6))\ndf['school_setting'].value_counts().plot(kind='bar')\nplt.title('School Setting')\n","3b35797f":"print(df['school_type'].value_counts())","c3e4e2ba":"import plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nlabels = df['school_type'].value_counts().keys()\nvalues = df['school_type'].value_counts()\n\n# # Use `hole` to create a donut-like pie chart\n# fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.5)])\n# fig.show()\n\nfig = make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]])\nfig.add_trace(go.Pie(labels=labels, values=values, name=\"School Type\"))\nfig.update_traces(hole=.5, hoverinfo=\"label+percent+name\")\n\nfig.update_layout(\n    title_text=\"School Type\")\nfig.show()","532b7249":"fig = make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]])\nfig.add_trace(go.Pie(labels=df['teaching_method'].value_counts().keys(), values=df['teaching_method'].value_counts(), name=\"Teaching Methods\"))\nfig.update_traces(hole=.5, hoverinfo=\"label+percent+name\")\n\nfig.update_layout(\n    title_text=\"Teaching Methods\")\nfig.show()","e998d833":"fig = make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]])\nfig.add_trace(go.Pie(labels=df['school_setting'].value_counts().keys(), values=df['school_setting'].value_counts(), name=\"School Setting\"))\nfig.update_traces(hole=.5, hoverinfo=\"label+percent+name\")\n\nfig.update_layout(\n    title_text=\"School Setting\")\nfig.show()","a869ce46":"fig = make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]])\nfig.add_trace(go.Pie(labels=df['lunch'].value_counts().keys(), values=df['lunch'].value_counts(), name=\"Lunch\"))\nfig.update_traces(hole=.5, hoverinfo=\"label+percent+name\")\n\nfig.update_layout(\n    title_text=\"Lunch\")\nfig.show()","ed675451":"# student frequency\nimport plotly.express as px\nfig = px.histogram(df['n_student'])\nfig.update_layout(bargap=0.2)\nfig.show()","9c9003e3":"from sklearn.model_selection import train_test_split\n\ntrain_x, test_x, train_y, test_y = train_test_split(label_X, y, random_state=0, test_size=0.25)","c7fae449":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\n\nmodel1 = DecisionTreeRegressor()\nmodel1.fit(train_x, train_y)\n","61314259":"pred_y1 = model1.predict(test_x)","846a0640":"mean_absolute_error(test_y, pred_y1)","120adf3d":"from sklearn.metrics import r2_score, mean_squared_error","0b3e6a84":"r2_score(test_y, pred_y1)","9a59fa92":"mean_squared_error(test_y, pred_y1)","9834d72b":"def getting_error_metric(max_leaf_nodes, train_x, test_x, train_y, test_y):\n    model2 = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model2.fit(train_x, train_y)\n    pred_y2 = model2.predict(test_x)\n    mae = mean_absolute_error(test_y, pred_y2)\n    mse = mean_squared_error(test_y, pred_y2)\n    r_square = r2_score(test_y, pred_y2)\n    return mae, mse, r_square\n\nfor i in [5, 50, 500, 5000]:\n    print('For max_leaf_nodes = ', i)\n    print('MAE, MSE, R_Square', getting_error_metric(i, train_x, test_x, train_y, test_y))\n    ","d462fcee":"label_X.head()","17e2b736":"plt.scatter(X['pretest'], y)","dbd711b3":"from sklearn.linear_model import LinearRegression\n\nmodel3 = LinearRegression()\n\nmodel3.fit(train_x, train_y)","812e8451":"pred_y3 = model3.predict(test_x)\nmean_absolute_error(test_y, pred_y3)\n","1662ac98":"r2_score(test_y, pred_y3)","e9129282":"from sklearn.ensemble import RandomForestRegressor\n\nmodel4 = RandomForestRegressor(random_state=0)\nmodel4.fit(train_x, train_y)\npred_y4 = model4.predict(test_x)\n\nprint('Mean Absolute Error = {}, Mean Squared Error = {}, R^2 = {}'.format(mean_absolute_error(test_y, pred_y4), mean_squared_error(test_y, pred_y4), r2_score(test_y, pred_y4)))","f05ec0b1":"def get_metrics_rf(n_estimator, train_x, test_x, train_y, test_y):\n    model5 = RandomForestRegressor(n_estimators = n_estimator, random_state=0)\n    model5.fit(train_x, train_y)\n    pred_y5 = model5.predict(test_x)\n    return mean_absolute_error(test_y, pred_y5), mean_squared_error(test_y, pred_y5), r2_score(test_y, pred_y5)\n\nestimators = [int(i) for i in np.linspace(start=100, stop=500, num=10)]\nfor i in estimators:\n    print('n_estimators value = ',i)\n    print('The MAE, MSE, R^2 values are ', get_metrics_rf(i, train_x, test_x, train_y, test_y))","8124d73a":"random_forest = RandomForestRegressor(n_estimators = 500, random_state=0)\nrandom_forest.fit(train_x, train_y)\npredictions_y = random_forest.predict(test_x)\nprint(r2_score(test_y, predictions_y))","bf8835d6":"import joblib\njoblib.dump(random_forest, 'predict_test_scores')","d44dcbf3":"!ls","242a0552":"## Feature and Target","eacba71d":"Linear Regression gave a better output than the decision tree","16e99637":"Using parameter n_estimators","414f2dfc":"# Linear Regression","3fe9e634":"Highest accuracy using max_leaf_nodes = 50","fbef812b":"# RandomForestRegressor","164270bb":"## LabelEncoding","90c54d00":"# DecisionTreeRegressor"}}