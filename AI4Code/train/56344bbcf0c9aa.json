{"cell_type":{"35a66d60":"code","e69b1f38":"code","6fbb7efe":"code","cb09f293":"code","9bf8e783":"code","acd50139":"code","60e0369f":"code","33a699c9":"code","655ea63c":"code","7da78886":"code","b64c6fab":"code","e54192b8":"code","6b60a845":"code","b3032736":"code","d33805ca":"code","9432ef23":"code","f0587ba7":"code","d33cb0a9":"code","2de4769c":"code","838bb069":"code","0c2aa020":"code","16c2744c":"code","cc8dd933":"code","db4408c7":"code","22a0f3b7":"code","09497eeb":"code","abb67877":"code","4ddb7688":"code","dbb56f32":"code","4e9a358a":"code","cd4e0c92":"code","248145de":"code","56e153a1":"code","b112b254":"code","26d7c208":"code","8c374f91":"code","e666d189":"code","b2d567b6":"code","c97fc223":"code","194f8c29":"code","9be4eab3":"code","4d67d4d8":"code","d5fc0ead":"markdown","20185739":"markdown","ac7be065":"markdown","bad6ef96":"markdown","49c84dc6":"markdown","aac1d817":"markdown","8a87fb8d":"markdown","47f269b3":"markdown","646d441d":"markdown","3fa31d85":"markdown","d967856b":"markdown","0e085e1e":"markdown","204e17e9":"markdown","f205ade1":"markdown","384c5e12":"markdown"},"source":{"35a66d60":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e69b1f38":"data=pd.read_csv('\/kaggle\/input\/stockmarket-sentiment-dataset\/stock_data.csv')\ndata.head()","6fbb7efe":"data.info()","cb09f293":"data.describe()","9bf8e783":"sns.heatmap(data.isnull(),cmap='Blues')","acd50139":"data['Sentiment'].value_counts()","60e0369f":"sns.countplot(x=data['Sentiment'])","33a699c9":"data.groupby('Sentiment').describe()","655ea63c":"data['Length']=data['Text'].apply(lambda x:len(x))","7da78886":"data['Length'].plot.hist(bins=200)","b64c6fab":"data['Length'].describe()","e54192b8":"plt.figure(figsize=(12,5))\ndata.hist(column='Length',by='Sentiment',bins=150)","6b60a845":"import string\nfrom nltk.corpus import stopwords","b3032736":"def clean(text):\n    a=[f for f in text if f not in string.punctuation]\n    a=''.join(a)\n    b=[w for w in a.split() if w.lower() not in stopwords.words('english')]\n    return b","d33805ca":"check=data['Text'].head(1).apply(clean)","9432ef23":"print(check[0])","f0587ba7":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score","d33cb0a9":"words=CountVectorizer(analyzer=clean).fit(data['Text']) # Cleaning all our data set from punctuations and stopwords","2de4769c":"print(len(words.vocabulary_))","838bb069":"sample=data['Text'][1]\nsample","0c2aa020":"trans=words.transform([sample])\nprint(trans)","16c2744c":"print(trans.shape) ","cc8dd933":"words.get_feature_names()[363]","db4408c7":"allmessgae=words.transform(data['Text'])","22a0f3b7":"print(allmessgae.shape)","09497eeb":"allmessgae.nnz # No of non - zero's value","abb67877":"sparsity = (100.0 * allmessgae.nnz \/ (allmessgae.shape[0] * allmessgae.shape[1]))\nprint('sparsity: {}'.format(sparsity))","4ddb7688":"tf=TfidfTransformer()\n","dbb56f32":"tf.fit(allmessgae)","4e9a358a":"tfidf=tf.transform(trans)\nprint(tfidf) ","cd4e0c92":"tf.idf_[words.vocabulary_['return']] # Checking the IDF value of particular word how imp a term is in whole dataset ","248145de":"final_transfrom=tf.transform(allmessgae)","56e153a1":"modelfitting=MultinomialNB().fit(final_transfrom,data['Sentiment'])","b112b254":"result=modelfitting.predict(final_transfrom)","26d7c208":"print(result)","8c374f91":"pipe=Pipeline([\n ('cv',CountVectorizer(analyzer=clean)),\n ('tfidf',TfidfTransformer()),\n ('Classifier',MultinomialNB())\n])","e666d189":"x_train,x_test,y_train,y_test=train_test_split(data['Text'],data['Sentiment'],test_size=0.3,random_state=101)","b2d567b6":"pipe.fit(x_train,y_train)","c97fc223":"pipe_predict=pipe.predict(x_test)\n","194f8c29":"print(classification_report(pipe_predict,y_test))","9be4eab3":"print(confusion_matrix(pipe_predict,y_test))","4d67d4d8":"print(accuracy_score(pipe_predict,y_test))","d5fc0ead":"Well no missing data that's great   :)","20185739":"L.H.S show's the position of a particular word and R.H.S tell the count of that word","ac7be065":"**Setting up tool for cleaning the data in required form**","bad6ef96":"Above shape telling us that we have in total 5791 documnets vs 13456 different words among them","49c84dc6":"Now comparing how length affect sentiment","aac1d817":"These values depict that how much a word is important in that particular document ","8a87fb8d":"Printing the particular word with the help of index :)","47f269b3":"No of non-zero value by total values ","646d441d":"**Another method instead of doing all above stuff**","3fa31d85":"Getting a view on our sentiment data ","d967856b":"**Now let's focus on our data prediction**","0e085e1e":"**Let's check if we are having missing data ??**","204e17e9":"These are the no of different no of words present in our data ","f205ade1":"Above curve shows how our length varies in our text data","384c5e12":"We have 1 documnet Vs 11425 Diffrent Words in total "}}