{"cell_type":{"b4bb7720":"code","7703da8f":"code","f6695ded":"code","ced5f598":"code","fbf92a73":"code","839e6e42":"code","88b30824":"code","3cc19bbc":"code","98fb883a":"code","92a35911":"code","e37170c6":"code","c2d33c53":"code","78e77685":"code","1d796559":"code","5a6687c9":"code","1d5bf663":"code","5f6d699d":"code","a03f63ed":"code","2d06fbde":"code","f69a9d69":"code","df0887f0":"code","094469a2":"code","d29c4a53":"code","5ac5e7a5":"code","dc2f608b":"code","b7a351c6":"code","5b690bd6":"code","b4299407":"code","1ec23f15":"code","26fdc0c5":"code","f72df042":"code","50257671":"code","fdabfa8f":"code","6175612c":"code","92aefc8b":"code","4436681a":"markdown","cd8fcd69":"markdown","c09037fd":"markdown","4b318baa":"markdown","de382748":"markdown","64b74e18":"markdown","6048adb3":"markdown","8baa9160":"markdown","e1b59691":"markdown","212a22a8":"markdown","9fcaed3e":"markdown","1894b4af":"markdown","96bcaeda":"markdown","ddd28daa":"markdown","c561f4e7":"markdown","a34faf11":"markdown","a38ecf1e":"markdown","2a5629f0":"markdown","6ecd162e":"markdown","1d239d79":"markdown","1047ba2c":"markdown","be0c4540":"markdown","1be167ef":"markdown","5c7831f7":"markdown","267bb508":"markdown"},"source":{"b4bb7720":"# Install the required packages (if not already done before)\n# !pip install numpy\n# !pip install pandas\n# !pip install sqlalchemy\n# !pip install mysqlclient","7703da8f":"# Import the packages\nimport pathlib\nimport pandas as pd\nimport numpy as np\nfrom sqlalchemy import create_engine\n\n# Optional adjustments of the default settings\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 500)\n%matplotlib inline","f6695ded":"# Create DataFrame\ncities_df = pd.DataFrame(data={'City': ['Berlin', 'Hamburg', 'London', 'Paris','Madrid', 'Copenhagen', 'Munich', 'Barcelona'],\n                               'Country': ['Germany', 'Germany', 'Great Britain', 'France','Spain', 'Denmark', 'Germany', 'Spain']})\n# Display DataFrame\ncities_df","ced5f598":"# Set url and load file\ncsv_url = 'https:\/\/lg4ml.org\/wp-content\/uploads\/2021\/11\/Kundendaten.csv'\ncustomer_df = pd.read_csv(csv_url)\ncustomer_df.head(10)","fbf92a73":"# Create database connection and load table (cannot be executed since no real database URI is set)\n# db_connection = create_engine('mysql:\/\/user:password@host:port\/database').connect()\n# table_df = pd.read_sql_table(table_name='Umsatz', con=db_connection)\n# table_df","839e6e42":"# Display general information (dtypes, non-null count, column names)\ncustomer_df.info()","88b30824":"# Get shape of dataframe\nprint(customer_df.shape)","3cc19bbc":"# Descriptive statistics of numerical columns (in this case PLZ is not a real numerical value)\ncustomer_df.describe()","98fb883a":"# Check value counts within one column\ncustomer_df['Branche'].value_counts()","92a35911":"# Access first rows of postal code column\ncustomer_df['PLZ'].head(10)","e37170c6":"# Access row of customer with index 555\ncustomer_df.loc[555]","c2d33c53":"# Access rows of customers with indices 123, 456, and 789. Furthermore just use Kunden_ID and Bedarf as columns\ncustomer_df.loc[[123, 456, 789], ['Kunden_ID', 'Bedarf']]","78e77685":"# Access the firstname of customer 100\ncustomer_df.at[100, 'Vorname']","1d796559":"# Display the first name and last name of the customers with the indices between 999 and 1010\ncustomer_df.loc[999:1010, ['Vorname', 'Nachname']]","5a6687c9":"# Access all rows of bakeries\ncustomer_df.loc[customer_df['Branche']=='B\u00e4ckerei']","1d5bf663":"# Select all bakery customers from customers\nbakery_customers = customer_df[customer_df['Branche']=='B\u00e4ckerei']\n# Save dataframe as csv without the index\nsave_path = pathlib.Path('Bakery_Customers.csv')\nbakery_customers.to_csv(save_path, index=False)","5f6d699d":"# Read in more data for following examples (internet connection required)\nkunden_df = pd.read_csv('https:\/\/lg4ml.org\/wp-content\/uploads\/2021\/11\/Kundendaten.csv')\nmaschinen_df = pd.read_csv('https:\/\/lg4ml.org\/wp-content\/uploads\/2021\/11\/machines.csv')\nteile_df = pd.read_csv('https:\/\/lg4ml.org\/wp-content\/uploads\/2021\/11\/parts_with_time.csv')\numsatz_df = pd.read_csv('https:\/\/lg4ml.org\/wp-content\/uploads\/2021\/11\/umsatz.csv')\nevent_df = pd.read_csv('https:\/\/lg4ml.org\/wp-content\/uploads\/2021\/11\/event.csv')\nzustand_df = pd.read_csv('https:\/\/lg4ml.org\/wp-content\/uploads\/2021\/11\/zustand.csv')","a03f63ed":"# Print out some basic information for each table (column names, dtypes, missing values)\nfor df in [kunden_df, maschinen_df, teile_df, umsatz_df, event_df, zustand_df]:\n    print(df.info())\n    print()","2d06fbde":"# Sort machines in descending order by price\nmaschinen_df.sort_values(by='Preis', ascending=False)","f69a9d69":"# Sort sales by customer id and date\numsatz_df.sort_values(by=['Kunden_ID', 'Datum']).head(25)","df0887f0":"# Set new properties for coffee machines\nnew_properties = pd.DataFrame(data={'Preis': [4850.99, pd.NA, pd.NA, 4999.99, 4750.00, 1950.00, 4050.49], 'TassenXTag': [pd.NA, 15, 30, 120, pd.NA, 25, 50]})\n# Copy machine df to keep the original data\nupdated_machines_df = maschinen_df.copy(deep=True)\n# Apply updated values and show the results\nupdated_machines_df.update(new_properties, overwrite=True)\nupdated_machines_df","094469a2":"# Get the first event for every machine ID\nevent_df.drop_duplicates(subset=['Maschinen_ID'], keep='first')","d29c4a53":"# Display the last machine of each customer\numsatz_df.drop_duplicates(subset=['Kunden_ID'], keep='last')","5ac5e7a5":"# Drop rows with missing values\nevent_df.dropna(axis=0)","dc2f608b":"# Fill missing values with the median of the column\nteile_df.fillna(value=teile_df.median())","b7a351c6":"# Merge tables with same column names in each table\nzustand_df.merge(right=event_df, how='left', on=['Maschinen_ID', 'Datum']).head(10)","5b690bd6":"# Merge tables with different column names in each table\nteile_df.merge(right=maschinen_df, how='left', left_on='Maschinen_Typ', right_on='Name').head(10)","b4299407":"# Create copy to keep the original data\nnew_umsatz_df = umsatz_df.copy(deep=True)\nnew_umsatz_df['Nutzung'] = np.where(new_umsatz_df['Ersatzdatum'].isna(), 'Aktiv', 'Inaktiv')\nnew_umsatz_df.sort_values(by=['Kunden_ID', 'Datum'])","1ec23f15":"# Count event types per part type\nzustand_df.pivot_table(values='Maschinen_ID', index='Fall', columns='Teil_Typ', aggfunc='count')","26fdc0c5":"# Merge tables and unstack the part age. Last column of index will turn into columns\nzustand_df.merge(right=event_df, how='left', on=['Maschinen_ID', 'Datum']).set_index(keys=['Maschinen_ID', 'Datum', 'Teil_Typ']).Teil_Alter.unstack()","f72df042":"# This pivot table does the same thing as the unstack method above\nzustand_df.merge(right=event_df, how='left', on=['Maschinen_ID', 'Datum']).pivot_table(values='Teil_Alter', index=['Maschinen_ID', 'Datum'], columns='Teil_Typ', aggfunc='sum')","50257671":"# First part\numsatz_df.drop_duplicates(subset=['Kunden_ID'], keep='first').merge(right=kunden_df, how='right', on='Kunden_ID')[['Kunden_ID', 'Vorname', 'Nachname', 'Maschinen_Typ', 'Datum']]","fdabfa8f":"# Get customers with more than three coffee machines\ncustomer_machine_counts = umsatz_df.groupby(by='Kunden_ID').count()\nrelevant_customer_ids = customer_machine_counts[customer_machine_counts['Maschinen_ID']>3].index\n# Select the rows from the sales df and do the same as for the first part\numsatz_df[umsatz_df['Kunden_ID'].isin(relevant_customer_ids)].drop_duplicates(subset=['Kunden_ID'], keep='first').\\\nmerge(right=kunden_df, how='left', on='Kunden_ID')[['Kunden_ID', 'Vorname', 'Nachname', 'Maschinen_Typ', 'Datum']].head(10)","6175612c":"# Merge tables to get all relevant data\nrelevant_data = zustand_df.merge(right=umsatz_df, how='left', on='Maschinen_ID', suffixes=('_event', '_umsatz')).merge(right=teile_df, how='left', on=['Teil_Typ', 'Maschinen_Typ'])\n# Filter for replacement events\nrelevant_data = relevant_data[relevant_data['Fall']=='Ersatz']\n# Create pivot table\nrelevant_data.pivot_table(values='Ersatzkosten', index='Maschinen_Typ', columns='Teil_Typ', aggfunc='sum')","92aefc8b":"# Merge tables to get all relevant data\nrelevant_data = zustand_df.merge(right=umsatz_df, how='left', on='Maschinen_ID', suffixes=('_event', '_umsatz')).merge(right=teile_df, how='left', on=['Teil_Typ', 'Maschinen_Typ'])\n# Filter for replacement events\nrelevant_data = relevant_data[relevant_data['Fall']=='Ersatz']\n# Add new column with the year of the event\nrelevant_data['year_event'] = relevant_data['Datum_event'].map(arg=lambda x: int(x.split('-')[0]))\n# Create pivot table\nrelevant_data.pivot_table(values='Maschinen_ID', index=['Teil_Typ', 'Maschinen_Typ'], columns='year_event', aggfunc='count').fillna(0)","4436681a":"### 3.6 Filling columns based on other columns\nIn certain cases, when calculating a new column, it is necessary to fill values depending on the values of another column. For example, when calculating the cost of a machine, it is necessary to access the machine type that is present in each column. In the example in the code, a new column is to be added that informs about the usage of a machine (Active if machine is still in use, Inactive otherwise). Accordingly, the `np.where()` function is used to see if there is no replacement date and the values are set accordingly.\n\n**Use:** Select new values depending on the value of a column.\n\n**Documentation:** [numpy.where()](https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.where.html) | [DataFrame.where](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.where.html)\n","cd8fcd69":"### 4.1 Customer & machine type\nThe management is interested in when the customers of the Kaffee-AG each received their first machine and which machine type this is in each case. Consider in which tables \/ DataFrames the required data can be found and how you can combine them.\n\n**Target data:** Customer ID, first name, last name, machine type, date.\n\n**Extension:** Management would like to have the data described above from customers who have already received more than three coffee machines.","c09037fd":"### 2.4 Executing descriptive methods\nAs soon as you encounter new, partly unknown data, you should first get a brief overview. But even if you think you already know the data, it doesn't hurt to check whether everything worked when you read it in and that no data types were incorrectly recognized. DataFrame objects have a number of methods that output information about the DataFrame itself, the data types of the individual columns, missing values and more.\n\n**Use:** Provide an overview of a DataFrame and its data.\n\n**Important methods:** *info()* - Overview of columns, data types and missing values | *shape* - Number of rows and columns | *describe()* - Descriptive statistics of numeric columns | *value_counts()* - Applied on a column\/series and gives frequency of values","4b318baa":"### 1.2 Importing packages\nAfter successful installation, the packages can be integrated or imported into the runtime of the Jupyter Notebook. In the data science area, some aliases for the most important packages have become accepted and established, so it is recommended to follow them. Pandas is imported as _pd_, Numpy as _np_ and the Pyplot package as _plt_.\nNote that the import has to be done every time the notebook is rebooted, because the runtime environment is reset. Besides the actual imports, it is recommended to adjust some parameters. These include the number of displayed rows of pandas or inline plots of matplotlib.","de382748":"### 3.7 Creating pivot tables\nPivot tables are an important tool for aggregating and extracting specific data from a DataFrame. The pivot function of a DataFrame groups the rows according to the specified index columns, and aggregates over the values of another column. In the process, the values of one column are converted to the columns of the resulting DataFrame. For example, Quality Assurance might be interested in how often replacement, repair, and maintenance are needed for each part type to pinpoint potential problems. The pivot function below counts (aggregation function count) the machine IDs (values) of the different cases and splits them by part type.\n\n**Use:** Aggregate data by the selected function over the set index.\n\n**Important keywords:** *values* - column with the values for aggregation | *index* - columns to group the rows | *columns* - column whose values will be used as new columns | *aggfunc* - aggregation function (e.g. count or sum).\n\n**Pandas documentation:** [pivot_table()](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.pivot_table.html)","64b74e18":"### 2.1 Creating a DataFrame from the Dictionary\nIn general, there are several ways to create a DataFrame. Pandas is able to extract or create a DataFrame from almost any two-dimensional structure in Python. These include, among others, numpy arrays and nested lists. Another possibility are dictionaries, where the keys are followed by a list of data (important: lists must be of equal length!). The keys are automatically used as column names if no other names are defined via the keyword `columns`.\n\n**Use:** Create a DataFrame from existing data or based on new data, such as in a dictionary.\n\n**Important keywords:** *data* - data the DataFrame should contain | *columns* - names of the columns as a list | *index* - index names as a list, otherwise they will be generated automatically\n\n**Pandas documentation:** [DataFrame](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.html)","6048adb3":"## 4. Application examples\nIn this part of the Notebok you have the possibility to deepen the different possibilities of data processing with Pandas at concrete questions to the data at hand. It can happen, as mostly in programming, that several ways lead to the goal - so there is not only one right solution!\n","8baa9160":"## 1. Getting started\n### 1.1 Installing packages\nBefore working with pandas, the library must first be installed into the Python interpreter. Generally, packages can be installed using the command `pip install paketname` - if Anaconda is used for Python administration, `conda install paketname` is also possible. After successful installation of the packages you can continue with the import.","e1b59691":"### 3.3 Remove duplicates\nEspecially for the use of Machine Learning algorithms, but also for other steps in Data Engineering, it is necessary to remove duplicates from the data. In most cases, this means removing rows with the same values from the data and keeping only one. The attribute `subset` can be used to specify in which columns the data should have the same values in order to be considered a duplicate.\n\n**Use:** Delete duplication within the data.\n\n**Important keywords:** *subset* - Subset of columns used for checking equal values | *keep* - Specify what to keep from duplication | *inplace* - If true DataFrame will be changed directly, otherwise new object will be returned.\n\n**Pandas documentation:** [drop_duplicates()](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.drop_duplicates.html)","212a22a8":"### 2.6 Select rows according to conditions\nIn many cases, not all rows or columns are relevant for the analysis or only a subset of the data is to be explicitly considered. Therefore, there are various ways to filter or select rows and columns in a DataFrame. For example, the management is interested in which areas of Germany the bakery customers come from. Accordingly, only rows that have _Bakery_ in the Industry column should be selected. In addition, only the customer ID and the postal code should be displayed in the columns.\nIndividual conditions according to which rows are to be selected can be linked with the logical operators 'and' and 'or'.\n\n**Use:** Selecting subsets from the DataFrame according to specific conditions.\n\n**Pandas Documentation:** [Selecting Subsets](https:\/\/pandas.pydata.org\/docs\/getting_started\/intro_tutorials\/03_subset_data.html)\n\n","9fcaed3e":"### 3.8 Converting columns to rows\nSimilar to the pivot table, the `unstack()` function can be used to convert values of a column to columns. To do this, the index of the DataFrame is first reset to group the rows. Then the column with the values is selected and unstacked. The same result can also be achieved with a pivot table by setting the appropriate columns for index and values and using the sum aggregate function. The two codes below produce the same output: the part age of each part at the time of a service.\n\n**Use:** Values of a column unstack based on the index set.\n\n**Pandas Documentation:** [unstack()](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.unstack.html)","1894b4af":"### Read in additional data\nFor the examples and methods in this section, several DataFrames with related data are required. This data is subsequently loaded from the LG4ML site, so an Internet connection is required (alternatively, use the Save CSV files locally links and replace link with path to file).","96bcaeda":"### 2.2 Reading a CSV file into a DataFrame\nPandas offers the option of reading CSV files directly into a DataFrame. CSV files are a popular format in the Data Science field for storing and exchanging data. For example, it is useful to save data after certain steps. Afterwards, this data can be read in again directly without having to perform all the previous steps again. With the `head()` method the first lines of the DataFrame can be displayed (number can be defined by parameters in the function call, default value is 5).\n\n**Use:** Creating a DataFrame from the contents of a CSV file.\n\n**Important keywords:** *path* - Path to the file | *separator* - Separator between individual values | *decimal* - Decimal character for numeric values | *index_col* - The specified column will be used as index.\n\n**Pandas documentation:** [read_csv()](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.read_csv.html)","ddd28daa":"### 4.3 Replacement events per part and machine type\nQuality Assurance is interested in how the number of replaced parts has developed since the beginning of the Coffee AG and has therefore turned to you. Compile the numbers for each year from 2010 to 2020 grouped by part type and machine type.\n\n**Target data:** Part type, machine type, 2010, ... , 2020","c561f4e7":"## 2. Pandas Basics\nThe basic and also most important data types in Pandas are 'Series' and 'DataFrame'. The Series stands for a single column with a name. A table - the DataFrame - results from several columns or Series.\nSeries and DataFrame are very similar in terms of operations, such as deleting\/filling missing values or calculating mean and standard deviation - but of course there are also differences. For example, unlike Series, DataFrames can be linked together via one or more column(s) (similar to an SQL Join) to combine information. In addition, DataFrames naturally have different methods for accessing the data, since it is a two-dimensional data structure.\n\nThe following blocks show the basics of Pandas:\n1. Create a DataFrame from a dictionary\n2. Read DataFrame from a CSV file\n3. Read SQL table into DataFrame\n4. Execute descriptive methods\n5. Access columns, rows and cells\n6. Select rows according to conditions\n7. Save DataFrame as CSV file","a34faf11":"### 3.4 Handling missing values\nSimilar to duplicates, missing values are also a problem that must be taken into account in data engineering and, with reference to machine learning, also in feature engineering. The `dropna()` method removes all rows (or columns if _axis=1_) with missing values in at least one column from the DataFrame. Depending on the number of missing values, it can happen that a lot of rows are removed and the dataset is no longer large enough to train good machine learning models. An alternative is the `fillna()` method, which fills missing values instead of deleting them. For example, as in the second example, the median of the respective column can be used in case of missing values (of course, mean value or similar key figures also work). A popular possibility are also so-called imputers, which estimate a value based on the other data (e.g. KNN imputer from Sklearn).\n\n**Use:** Delete rows with missing values or fill based on existing values.\n\n**Important keywords:** *value* - value to use for missing values | *axis* - specify whether to proceed row-wise or column-wise (default 0) | *inplace* - if true DataFrame will be changed directly, otherwise new object will be returned.\n\n**Pandas documentation:** [dropna()](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.dropna.html) | [fillna()](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.fillna.html)","a38ecf1e":"### 3.1 Sorting rows by attributes\nIn some cases, it is necessary for further steps to sort data by one or more attributes or columns. In the first example, the coffee machines are sorted in descending order by price; in the second example, sales of the coffee company are sorted by customer ID and date. The method `sort_values()` leaves the index of the DataFrame in its original form, so as you can see in the examples, the index is messed up.\n\n**Use:** Sorting the rows of a DataFrame by one or more columns.\n\n**Important keywords:** *by* - Column(s) to sort by | *axis* - Specify whether rows or columns should be sorted (default 0) | *ascencing* - Sort ascending or descending | *inplace* - If true DataFrame is changed directly, otherwise new object is returned\n\n**Pandas documentation:** [sort_values()](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.sort_values.html)","2a5629f0":"### 2.3 Reading SQL table into DataFrame\nPandas is able to load existing SQL tables directly into a DataFrame, taking over the corresponding column names. Likewise the results of queries on the database can be read in. The respective methods require a database connection in the form of a URI in text format or an SQL Alchemy Connection object. The URI consists of the following elements:\n1. _dbtype_ - type of database, e.g. mysql\n2. _user_ - user name, which should be used to log in to the server\n3. _password_ - password of the user (if set)\n4. _host_ - host name or IP address\n5. _port_ - port of the database, e.g. usually 3306 for MySQL\n6. _database_ - database or schema to be used for queries\n\n**Use:** Creating a DataFrame from an SQL table.\n\n**Important keywords:** *table_name* - name of the table to read | *con* - database connection.\n\n**Pandas documentation:** [read_sql_table()](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.read_sql_table.html)","6ecd162e":"### 4.2 Costs per machine type per part type\nIn order to be able to better calculate expenses in the future and possibly identify machines that are particularly susceptible, your colleagues in Accounting have asked you for a few figures. The accounting department would like to know how much has been spent on replacing parts for each type of machine per part type since the coffee company was first leased.\n\n**Target Data:** Totaled cost per machine type and part type for replacements.\n\n**Note:** First consider which tables need to be combined to bring all the relevant information together.","1d239d79":"## 3. Data Engineering\nBased on the basic concepts from the previous section of this Jupyter Notebook, the most important methods from the field of data engineering can be presented and explained below. Data Engineering is generally understood as the processing of data, e.g. to prepare raw data for Machine Learning models. Data Engineering thus has the task or goal of filtering the available data for relevant information and converting it into a usable form. For example, relevant data might exist in different tables in a database and need to be integrated and aligned before use.\n\nThe following blocks explain the following topics:\n1. Sort rows by attributes\n2. Update values of a DataFrame\n3. Remove duplicates\n4. Handle missing values\n5. Merge DataFrames\n6. Fill columns based on other columns\n7. Create pivot tables\n8. Convert columns to rows","1047ba2c":"### 2.7 Saving DataFrame as CSV file\nAs already mentioned, it is useful to save intermediate results in places so that you do not have to perform all the previous steps again when you edit them later. Therefore DataFrames can be saved as CSV files. Depending on how the DataFrame is saved (e.g. with index or without), appropriate parameters must be set within the `read_csv()` method when reading.\n\n**Use:** Saving a DataFrame into a CSV file.\n\n**Important keywords:** *path* - Path where the CSV file should be saved | *index* - Boolean whether the index should be saved as well | *sep* - Separator between the values of a row | *decimal* - Decimal point for numeric values\n\n**Pandas documentation:** [to_csv()](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.to_csv.html)","be0c4540":"### 3.2 Updating the values of a DataFrame\nIf changes need to be made to the values of a DataFrame, the update function can be used for this purpose. Smaller adjustments can also be made using the methods mentioned above for accessing the values, but if large parts of the data are to be updated, the update function is the best choice. A second DataFrame with the new values is placed on top of the existing DataFrame like a template and fills missing values and overwrites existing values if the overwrite parameter is set. Using column names and index, Pandas automatically determines which data should be updated with what.\n\n**Use:** Update values of single cells or whole columns\/rows.\n\n**Important keywords:** *other* - DataFrame or Series with the new data | *overwrite* - Option whether to overwrite existing values.\n\n**Pandas documentation:** [update()](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.update.html)","1be167ef":"### 2.5 Accessing columns, rows and cells\nOnce DataFrames have been created, the columns, rows and cells must be accessed in order to analyze and process the data. DataFrames offer various methods for accessing and updating the data. Depending on the method, only individual values or a selection of values can be updated. The most important methods for access are `loc[]`, `iloc[]` and `at[]`.\n\n**Use:** Accessing and updating rows and columns.\n\n**Pandas documentation:** [loc](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.loc.html) | [iloc](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.iloc.html) | [at](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.at.html)","5c7831f7":"### 3.5 Merging DataFrames\nAn important part of Data Engineering consists of combining data from different tables and finding rows that belong together. The `merge()` method of a DataFrame is comparable to a join in SQL and joins the columns of the DataFrames based on matching attributes in one or more columns. In the first example, the state table (information about affected parts in an event) is merged with the event table to combine corresponding information about the event with that about the part. In the second example, the column to be used for merging has different names in both DataFrames. Instead of the _on_ parameter, the _left_on_ and _right_on_ parameters are set to name the respective names of the column.\n\n**Use:** Merge columns of DataFrames based on common attributes.\n\n**Important Keywords:** *right* - Second DataFrame to concatenate | *how* - Specify how to concatenate columns | *on* - Column\/s to identify related rows | *suffixes* - Suffixes for columns if same column names exist.\n\n**Pandas documentation:** [merge()](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.merge.html)","267bb508":"# Data processing in Python with Pandas\nPandas is one of the most important Python packages for data processing. The Pandas package provides methods to read, process and transform data from files or databases and to create descriptive statistics or visualizations of the data. Often, much of the pre-processing prior to a machine learning model takes place as in Pandas.\n\nPandas has been developed on the basis of Numpy, which provides a data type for multidimensional arrays in the form of `NDArray` (hence the name). Numpy provides corresponding methods for matrix multiplication and the like, which are used by Pandas and machine learning algorithms, among others. Numpy is implemented to a large extent in C, which means that large amounts of data can be processed very quickly.\n\n\nThis notebook teaches the most important functions from the Pandas package. The individual chapters build on each other and start with the basics after installation. Subsequently, methods for data manipulation and transformation are introduced and used in the fourth part based on concrete application examples.\n1. Getting started (installing and importing packages)\n2. Pandas basics (read, retrieve and store data)\n3. Data engineering (manipulate and edit data)\n4. Application examples\n\n### Data used\nThe data from this notebook is taken from courses taught at Leuphana University of L\u00fcneburg and is part of the accompanying case study from the course _Business Analytics_ in the winter term 2021\/22. The data is available for download via the LG4ML platform using the links at the beginning of section 3. Alternatively, the CSV files can be downloaded directly from Pandas via the corresponding links."}}