{"cell_type":{"4a680884":"code","251517dd":"code","d5164477":"code","55d74213":"code","ac05798c":"code","090814ba":"code","00ba1c78":"code","959ae08b":"code","7bd912a0":"code","6ceb886d":"code","374c687e":"code","90dee433":"code","caeec765":"code","16ce04cc":"code","95dd914d":"code","635f5951":"code","4bde7ddc":"code","418cc977":"code","3086be59":"code","1ce3ff3a":"code","12126c12":"code","a7c71cab":"code","9f078ea9":"code","9106e1e8":"markdown","a7e62379":"markdown","c205ab0c":"markdown","5fe5db61":"markdown","a142d4b5":"markdown","446ca70c":"markdown","b6e6c99f":"markdown"},"source":{"4a680884":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","251517dd":"%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom colorama import Fore, Style\nsns.set()","d5164477":"import tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss, classification_report, confusion_matrix","55d74213":"train_features = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\ntrain_targets_scored = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_nonscored.csv')\ntest_features = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')","ac05798c":"train_features.head().T","090814ba":"train_targets_scored.head().T","00ba1c78":"train_targets_nonscored.head().T","959ae08b":"test_features.head().T","7bd912a0":"test_ids = test_features['sig_id']","6ceb886d":"for d in [train_features, test_features]:\n    d.drop(['sig_id', 'cp_type', 'cp_dose', 'cp_time'], axis=1, inplace=True)\n    \ntrain_features.head().T","374c687e":"train_targets_scored.drop(['sig_id'], axis=1, inplace=True)\ntrain_targets_scored.head().T","90dee433":"fig, axes = plt.subplots(3, 3, figsize=(8, 8))\nfor i, column in enumerate(train_features[train_features.columns[:9]].columns):\n    sns.distplot(train_features[column], ax=axes[i \/\/ 3, i % 3])\nplt.tight_layout()\nplt.show()","caeec765":"fig, axes = plt.subplots(3, 3, figsize=(8, 8))\nfor i, column in enumerate(train_features[train_features.columns[800:809]].columns):\n    sns.distplot(train_features[column], ax=axes[i \/\/ 3, i % 3], color='green')\nplt.tight_layout()\nplt.show()","16ce04cc":"print(Fore.YELLOW + 'Shape(x_train): ' + str(train_features.shape))\nprint(Fore.YELLOW + 'Shape(y_train): ' + str(train_targets_scored.shape))\nprint(Fore.BLUE + 'Shape(x_test): ' + str(test_features.shape))","95dd914d":"x_train, x_cv, y_train, y_cv = train_test_split(train_features, train_targets_scored, test_size=0.2) ","635f5951":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(512, input_dim=x_train.shape[1], activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(.25),\n    tf.keras.layers.Dense(1024, activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(.25),\n    tf.keras.layers.Dense(1024, activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(.25),\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(.25),\n    tf.keras.layers.Dense(y_train.shape[1], activation='sigmoid')\n])\n\nmodel.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\nmodel.summary()","4bde7ddc":"history = model.fit(\n    x_train, y_train, verbose=2, epochs=40,\n    validation_data=(x_cv, y_cv),\n    callbacks=[\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss', \n            factor=0.2, \n            patience=4\n        ),\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=10,\n            mode='auto',\n            verbose=1,\n            baseline=None,\n            restore_best_weights=True\n        )\n    ]\n)","418cc977":"loss_train = history.history['loss']\nloss_validation = history.history['val_loss']\nepochs = range(1, len(history.history['loss']) + 1)\nplt.plot(epochs, loss_train, 'g', label='Training')\nplt.plot(epochs, loss_validation, 'b', label='Validation')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Loss')\nplt.legend()\nplt.show()","3086be59":"acc_train = history.history['accuracy']\nacc_validation = history.history['val_accuracy']\nepochs = range(1, len(history.history['accuracy']) + 1)\nplt.plot(epochs, acc_train, 'g', label='Training')\nplt.plot(epochs, acc_validation, 'b', label='Validation')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Accuracy')\nplt.legend()\nplt.show()","1ce3ff3a":"print(Fore.BLUE + f'Average LogLoss: {log_loss(y_cv, model.predict(x_cv)) \/ 207}')","12126c12":"y_hat = model.predict(test_features)","a7c71cab":"submission = pd.concat(\n    [pd.DataFrame(test_ids, columns=['sig_id']),\n     pd.DataFrame(y_hat, columns=train_targets_scored.columns)],\n    axis=1\n)\nsubmission.head().T","9f078ea9":"submission.to_csv('submission.csv',index=False)","9106e1e8":"# Results","a7e62379":"# Dependencies","c205ab0c":"## Cell Data Distribution","5fe5db61":"## Gene Data Distribution","a142d4b5":"# Data Exploration","446ca70c":"# Model Building","b6e6c99f":"# Model Evaluation"}}