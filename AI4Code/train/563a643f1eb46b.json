{"cell_type":{"ab57bc98":"code","d4d7cbaf":"code","87e4a013":"code","244f7ec7":"code","5f5d912c":"code","340fbf63":"code","5616ad26":"code","2754b38c":"code","32a615f3":"code","ec1a9282":"code","f681b105":"code","24f7eced":"code","a9e56faf":"code","f58f5928":"code","42388b60":"code","5afc0296":"code","4e5e5ecd":"code","5e2b2ec8":"markdown"},"source":{"ab57bc98":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d4d7cbaf":"data = pd.read_csv('\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv')\nprint(data.shape)\ndata.head()","87e4a013":"data.info()","244f7ec7":"import seaborn as sns\nsns.heatmap(data.corr(),annot=True)","5f5d912c":"data.head(3)","340fbf63":"round(data.isnull().sum()\/len(data.index),2)","5616ad26":"from sklearn.model_selection import train_test_split\nX = data.iloc[:,:-1]\ny = data.iloc[:,-1]\nX_train,X_test,y_train,y_test = train_test_split(X,y,random_state = 100,test_size = 0.2,stratify = y)","2754b38c":"from xgboost import XGBClassifier\nfrom sklearn.metrics import log_loss\n","32a615f3":"my_model = XGBClassifier(n_estimators = 100)\nmy_model.fit(X_train,y_train,\n            early_stopping_rounds = 5,\n            eval_set=[(X_test,y_test)],\n            verbose = False)","ec1a9282":"y_pred1 = my_model.predict(X_test)","f681b105":"from sklearn.metrics import accuracy_score\nscore = accuracy_score(y_test,y_pred1)\n\nprint('The accuracy using XGBoost is: {}'.format(score))","24f7eced":"log_loss(y_test, y_pred1, eps=1e-15)\n","a9e56faf":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nlr = LogisticRegression()\nlr.fit(X_train,y_train)\ny_pred2 = lr.predict(X_test)\nscore = accuracy_score(y_test,y_pred2)\nprint('The accuracy using Logistic Regression is: {}'.format(score))\nlog_loss(y_test, y_pred2, eps=1e-15)\n","f58f5928":"from sklearn.ensemble import RandomForestClassifier\nRf = RandomForestClassifier()\nRf.fit(X_train,y_train)\ny_pred3 = Rf.predict(X_test)\nscore = accuracy_score(y_test,y_pred3)\nprint('The accuracy using RandomForest Classifier is: {}'.format(score))\nlog_loss(y_test, y_pred3, eps=1e-15)\n","42388b60":"y_pred3","5afc0296":"def inp(Pregnancies,Glucose,BloodPressure,SkinThickness,Insulin,BMI,DiabetesPedigreeFunction,Age):\n    x = np.zeros(len(X.columns))\n    x[0]=Pregnancies\n    x[1]=Glucose\n    x[2]=BloodPressure\n    x[3]=SkinThickness\n    x[4]=Insulin\n    x[5]=BMI\n    x[6]=DiabetesPedigreeFunction\n    x[7]=Age\n    out = Rf.predict([x])[0]\n    return out\n","4e5e5ecd":"inp(1,85,66,29,0,26.6,0.351,31)\n","5e2b2ec8":"- Pregnancies: Number of times pregnant\n- Glucose: Plasma glucose concentration over 2 hours in an oral glucose tolerance test\n- BloodPressure: Diastolic blood pressure (mm Hg)\n- SkinThickness: Triceps skin fold thickness (mm)\n- Insulin: 2-Hour serum insulin (mu U\/ml)\n- BMI: Body mass index (weight in kg\/(height in m)2)\n- DiabetesPedigreeFunction: Diabetes pedigree function (a function which scores likelihood of diabetes based on family history)\n- Age: Age (years)\n- Outcome: Class variable (0 if non-diabetic, 1 if diabetic)"}}